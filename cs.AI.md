# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Accelerating Transformer Inference for Translation via Parallel Decoding.](http://arxiv.org/abs/2305.10427) | 通过并行解码，本文提出了一种快速推断Transformer在翻译中的应用的方法，不需要修改现有模型并在保持翻译质量的同时加速了现有模型。 |
| [^2] | [SLiC-HF: Sequence Likelihood Calibration with Human Feedback.](http://arxiv.org/abs/2305.10425) | 本文提出了一种新方法，SLiC-HF，可以利用序列似然校准从人类偏好中学习，相较于过去的方法更加简单高效，并在TL;DR自动摘要任务中显著提高了监督微调基线。 |
| [^3] | [Scratch Copilot Evaluation: Assessing AI-Assisted Creative Coding for Families.](http://arxiv.org/abs/2305.10417) | 本研究评估了使用AI辅助家庭进行创意编码的潜力，使用大型语言模型（LLMs）帮助家庭理解代码、调试程序以及为未来项目生成新的想法，并实现了超过80\%的总体成功率。 |
| [^4] | [AI Friends: A Design Framework for AI-Powered Creative Programming for Youth.](http://arxiv.org/abs/2305.10412) | 本文调查了AI在家庭创意编程中的角色和作用，通过设计一个"人机交互"平台帮助家庭与AI朋友合作，发现AI朋友可以通过提问帮助家庭创作游戏，在AI朋友无法协助时，家长也发挥了独特的作用，AI支持的平台应强调儿童为主角的AI独特互动，并关注创意自我效能。 |
| [^5] | [Variational Classification.](http://arxiv.org/abs/2305.10406) | 提出一种新的变分分类方法，通过引入潜变量建模来优化训练，允许灵活的设计选择以改善校准和对抗鲁棒性，实验结果表明其对于域外数据的分类准确性得到了保持。 |
| [^6] | [PaLM 2 Technical Report.](http://arxiv.org/abs/2305.10403) | PaLM 2 是一种计算效率更高的最先进的语言模型，提供了更好的多语言和推理能力，并且通过使用多种目标进行训练，获得了在不同模型大小的下游任务上显着的改进质量。此外，PaLM 2 还展示了强大的推理能力和稳定的性能表现，使得模型能够更广泛地部署，并且可以控制毒性推理时间，而不会对其他能力产生影响。 |
| [^7] | [Explainable Multi-Agent Reinforcement Learning for Temporal Queries.](http://arxiv.org/abs/2305.10378) | 本文提出了一种可解释的多智能体强化学习方法，用于回答关于智能体行为发展的时间查询，该方法可以生成策略级对比解释，以解释智能体的实际和预期行为之间的差异。 |
| [^8] | [Human Choice Prediction in Non-Cooperative Games: Simulation-based Off-Policy Evaluation.](http://arxiv.org/abs/2305.10361) | 本文研究了语言游戏中的离线策略评估，并提出了一种结合真实和模拟数据的新方法。 |
| [^9] | [BIOT: Cross-data Biosignal Learning in the Wild.](http://arxiv.org/abs/2305.10351) | 该论文提出了一种可以在多个生物信号数据源上进行训练，并可在所有生物信号任务上进行微调的基础模型BIOT。通过将不同的生物信号令牌化为统一的“生物信号句子”，该模型可以实现跨数据源的学习，处理各种格式的生物信号，具有出色的性能。 |
| [^10] | [Interactive Learning of Hierarchical Tasks from Dialog with GPT.](http://arxiv.org/abs/2305.10349) | 该论文中，作者提出一种使用GPT模型作为对话前端，从对话中进行可解释的符号交互式任务学习的方法。通过将交互式对话转换为语义表示，并递归地要求未知步骤的定义，可以获取分层任务知识并在自然的对话环境中进行重复使用。 |
| [^11] | [MM-Fi: Multi-Modal Non-Intrusive 4D Human Dataset for Versatile Wireless Sensing.](http://arxiv.org/abs/2305.10345) | 提出了第一个多模态非侵入式的4D人体数据集MM-Fi，用于多种无线传感任务的支持。该数据集包含40名受试者的超过320K同步帧的五种模态，支持人体姿态估计和动作识别等任务的开展。 |
| [^12] | [Automatic Photo Orientation Detection with Convolutional Neural Networks.](http://arxiv.org/abs/2305.10319) | 本论文使用CNN解决了照片方向检测的问题，并在数据集上显著提高性能。使用Guided Backpropagation获得了CNN检测方向的见解。 |
| [^13] | [LeTI: Learning to Generate from Textual Interactions.](http://arxiv.org/abs/2305.10314) | LeTI是一种使用自然语言指令、LM生成的程序和错误消息进行串联迭代微调的技术，可以用于代码生成任务，并且在自然发生的Python指令数据集上表现最先进。 |
| [^14] | [Rethinking Data Augmentation for Tabular Data in Deep Learning.](http://arxiv.org/abs/2305.10308) | 本研究提出了一种新的表格数据增强方法“随机连续嵌入”（Random Continuous Embedding，RCE），能够提高 Transformer-based 预训练模型的自监督学习性能，大幅优于现有方法，并使得自监督学习模型能够在监督表格学习中优于树形方法。 |
| [^15] | [UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective.](http://arxiv.org/abs/2305.10306) | UniEX是一种能适用于各种模式格式的信息抽取框架，并能同时解决命名实体识别、关系抽取、事件提取和情感分析等任务，在性能和推理速度上优于其他通用信息抽取模型。 |
| [^16] | [Estimation of Remaining Useful Life and SOH of Lithium Ion Batteries (For EV Vehicles).](http://arxiv.org/abs/2305.10298) | 本文介绍不同方法估计锂电池剩余寿命，提出了一种利用机器学习技术的新方法，可以利用电池性能参数准确预测电池的寿命。 |
| [^17] | [Linear Query Approximation Algorithms for Non-monotone Submodular Maximization under Knapsack Constraint.](http://arxiv.org/abs/2305.10292) | 该研究提出了针对背包约束下的非单调子模最大化问题的两种新的线性查询逼近算法，其中$\mathsf{DLA}$和$\mathsf{RLA}$都有常数近似比，且查询复杂度为$O(n \log(1/\epsilon)/\epsilon)$。 |
| [^18] | [Explain Any Concept: Segment Anything Meets Concept-Based Explanation.](http://arxiv.org/abs/2305.10289) | 本文提出了一种名为Explain Any Concept（EAC）的有效而灵活的基于概念的解释方法，它利用Segment Anything Model（SAM）执行实例分割，并使用相关的概念生成自然语言解释。 |
| [^19] | [Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks.](http://arxiv.org/abs/2305.10284) | 本文提出了一种鲁棒的自然语言处理系统评估方法，可以解决基准测试中某些系统的得分缺失问题，并引入了一个规模更大的基准测试。 |
| [^20] | [Large-Scale Package Manipulation via Learned Metrics of Pick Success.](http://arxiv.org/abs/2305.10272) | 本文讨论了基于学习度量的大规模包裹操作，通过训练拾取成功预测器和学习拾取质量度量，实现了能够大规模部署的强力抓握策略。 |
| [^21] | [Improving Link Prediction in Social Networks Using Local and Global Features: A Clustering-based Approach.](http://arxiv.org/abs/2305.10257) | 该论文提出了一种基于聚类的方法，结合了相似性方法和学习方法来提高社交网络中的链接预测。 |
| [^22] | [SAM for Poultry Science.](http://arxiv.org/abs/2305.10254) | 本研究探究了 SAM 模型在禽业领域中的应用潜力，结果表明 SAM 模型性能优于其他模型，可以应用于基于部分的鸡类分割任务和红外热像的使用，并用于鸡类跟踪任务中。 |
| [^23] | [MemoryBank: Enhancing Large Language Models with Long-Term Memory.](http://arxiv.org/abs/2305.10250) | MemoryBank 提出了一种新型内存机制，旨在为大型语言模型提供类人的长期记忆。它可以召唤相关记忆，通过持续的记忆更新不断进化，通过合成过去的互动信息理解并适应用户个性。 |
| [^24] | [Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility.](http://arxiv.org/abs/2305.10235) | 本研究是一项关于大型语言模型方面的实证研究，对主流语言模型进行了大量查询和分析，结果发现这些模型存在着鲁棒性、一致性和可信性方面的潜在风险。 |
| [^25] | [Exploring Inductive Biases in Contrastive Learning: A Clustering Perspective.](http://arxiv.org/abs/2305.10229) | 本文比较了对比学习和监督学习方法形成的簇，揭示了对比学习可以生成具有局部密度但无全局密度的簇，而监督学习创建具有局部和全局密度的簇。同时，作者提出了使用图卷积网络分类器作为处理局部密集簇的线性分类器的替代方法，并利用t-SNE可视化证明了对比和监督学习方法产生的特征之间的差异。 |
| [^26] | [Separability and Scatteredness (S&S) Ratio-Based Efficient SVM Regularization Parameter, Kernel, and Kernel Parameter Selection.](http://arxiv.org/abs/2305.10219) | 该文通过分析数据的可分性和离散度，提出了一种基于S&S比的有效SVM正则化参数、核函数和核参数选择方法，表现较传统方法更优。 |
| [^27] | [Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection.](http://arxiv.org/abs/2305.10204) | 本文提出了一种名为迭代梯度基础投影（IGBP）的新方法，用于从神经表示中删除非线性编码的概念，以减轻模型的社会偏见。该方法通过迭代训练神经分类器来预测某个敏感属性，然后将表示投影到一个超平面上，使得分类器对目标属性变得无意识。实验证明，该方法在消除敏感属性方面是有效的，并且对下游任务的准确性影响很小。 |
| [^28] | [People Talking and AI Listening: How Stigmatizing Language in EHR Notes Affect AI Performance.](http://arxiv.org/abs/2305.10201) | 本文研究了电子病历中污名化语言对基于Transformer的深度学习模型和可解释AI(XAI)技术进行死亡预测的影响。发现临床医生所写的SL会对AI性能表现不利，尤其是在黑人患者中表现更为明显，强调了理解偏见对下游AI性能的影响的重要性，以开发更具公平和正义的医疗系统。 |
| [^29] | [Deep and Fast Approximate Order Independent Transparency.](http://arxiv.org/abs/2305.10197) | 本文提出了一种深度学习方法有效地计算无序透明度，具备快速性、占用内存少以及更为准确。该方法适用于所有场景，无需超前设置，且在运用普及GPU的所有平台上都有便捷的移植性。 |
| [^30] | [A Survey on Zero Pronoun Translation.](http://arxiv.org/abs/2305.10196) | 本文总结了零代词翻译（ZPT）领域神经网络全面推广后的重要工作，发现大型语言模型、多任务或迁移学习都可以实现ZPT的性能提升。 |
| [^31] | [Pragmatic Reasoning in Structured Signaling Games.](http://arxiv.org/abs/2305.10167) | 本文介绍了一个结构化传递博弈和理性言语行为框架的变体sRSA，应用于结构化领域中的实用推理问题。在颜色领域中，我们的研究表明采用sRSA的代理比传统RSA和仅基于强化学习的代理更接近于信息理论界限。 |
| [^32] | [Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model.](http://arxiv.org/abs/2305.10163) | 本研究通过在ChatGPT中集成医学领域知识和启用少样本学习的新方法，在中国国家医学执业医师资格考试中取得成功，这为建立在自然语言处理技术和医学领域知识的创新应用提供了可能。 |
| [^33] | [Collective Large-scale Wind Farm Multivariate Power Output Control Based on Hierarchical Communication Multi-Agent Proximal Policy Optimization.](http://arxiv.org/abs/2305.10161) | 本文提出了一种基于通讯的多代理深度强化学习大型风电场多变量控制方法，将风电场分成多个聚合器，并通过分层通讯进行信息交换来最大化风电场功率输出，并在仿真中证明了其有效性。 |
| [^34] | [Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks.](http://arxiv.org/abs/2305.10160) | 提出了三个适用策略：（1）公钥加密发布测试数据，仅允许特定派生发布；（2）对于API持有方，要求训练排除控制，保护测试数据，不停止评估直到达到要求；（3）如果测试数据来自互联网文本，需避免某些结果的使用。 |
| [^35] | [Personality Understanding of Fictional Characters during Book Reading.](http://arxiv.org/abs/2305.10156) | 本文提出了一个NLP领域内尚未研究的问题：情景和细致地理解小说人物个性，并提供了第一个标记数据集PersoNet来解决这个问题。 |
| [^36] | [Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models.](http://arxiv.org/abs/2305.10120) | 针对大规模文本到图像模型可能被误用生成有害内容的问题，该论文提出了一种选择性遗忘方法，即持续学习方法，可在深度生成模型中实现可控的遗忘，用户可指定消除哪些概念。 |
| [^37] | [Bridging the Gap: Enhancing the Utility of Synthetic Data via Post-Processing Techniques.](http://arxiv.org/abs/2305.10118) | 本文介绍了一种利用生成对抗网络生成合成数据集，并通过三种新颖的后处理技术改进合成数据集质量和多样性的方法。作者称其为Gap Filler (GaFi)流程并在真实图像上进行评估。 |
| [^38] | [Neuro-Symbolic AI for Compliance Checking of Electrical Control Panels.](http://arxiv.org/abs/2305.10113) | 本文提出了一种基于神经符号的AI方法，结合深度学习技术和答案集编程来自动化电气控制面板的合规性验证。该方法可以在只有非常有限的训练数据下识别可能存在的异常和错误，实验结果表明该方法具有很好的效果。 |
| [^39] | [Predicting Tweet Engagement with Graph Neural Networks.](http://arxiv.org/abs/2305.10103) | 本研究提出TweetGage，一个基于图神经网络的解决方案，通过表示发布帖子间的语义关联来预测用户在社交媒体上的互动，相对其他研究只考虑帖子文本和发布用户等因素，有效提高了预测准确性。 |
| [^40] | [Use of a Taxonomy of Empathetic Response Intents to Control and Interpret Empathy in Neural Chatbots.](http://arxiv.org/abs/2305.10096) | 本文提出了一种使用移情反应意图分类法来控制和解释神经聊天机器人中的共情回应的方法，能够产生可控和可解释的共情回应。 |
| [^41] | [Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges.](http://arxiv.org/abs/2305.10091) | 本文回顾了多智能体强化学习的方法与应用，并指出了未来十年的研究趋势与前景。我们认为，在未来十年中，可信赖的多智能体强化学习将成为一个热门的研究课题。此外，考虑人机交互是多智能体强化学习在各种社会中实际应用的重要考虑因素， 因此，本文还分析了应用于人机交互时所面临的挑战。 |
| [^42] | [A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization.](http://arxiv.org/abs/2305.10089) | 本文证明了Wasserstein反向强化学习模型适用于多目标优化问题，可让学习者的奖励值和最优解模仿专家，具有一定的实用价值。 |
| [^43] | [Cold PAWS: Unsupervised class discovery and the cold-start problem.](http://arxiv.org/abs/2305.10071) | 本文提出了一种新方法，通过结合自我监督、聚类和流形学习技术，解决冷启动或无监督选择标记问题，并在多个公共数据集上进行了测试，获得了更好的性能。 |
| [^44] | [Unveiling the Potential of Counterfactuals Explanations in Employability.](http://arxiv.org/abs/2305.10069) | 本研究展示了反事实解释在就业领域的多种应用，包括增强决策支持、遵守法律要求、引导受控变化和分析新颖见解。 |
| [^45] | [A hybrid feature learning approach based on convolutional kernels for ATM fault prediction using event-log data.](http://arxiv.org/abs/2305.10059) | 该论文提出了一种基于卷积核的混合特征学习方法，通过结合深度学习、核方法和基于规则的特征选择从ATM事件日志中提取相关特征，用于早期ATM故障预测，在真实的ATM事件日志数据上进行的广泛实验表明其优于现有技术。 |
| [^46] | [Finding an $\epsilon$-close Variation of Parameters in Bayesian Networks.](http://arxiv.org/abs/2305.10051) | 本文提出的算法基于参数马尔可夫链的"区域验证"技术，可以解决贝叶斯网络中的$\epsilon$-close参数调整问题，为处理参数化BN的子类提供了可能。 |
| [^47] | [Causal Discovery with Missing Data in a Multicentric Clinical Study.](http://arxiv.org/abs/2305.10050) | 本文扩展了最新的因果发现算法，利用专家知识从多中心临床研究的缺失数据中分析了不同缺失机制对恢复的因果图的影响，验证了所恢复因果图的临床相关性，并用图形分离来验证因果通路，讨论了因果图的拟合度和从临床决策角度的一致性。 |
| [^48] | [Risk Assessment of Lymph Node Metastases in Endometrial Cancer Patients: A Causal Approach.](http://arxiv.org/abs/2305.10041) | 子宫内膜癌淋巴结转移风险评估是一项具有挑战性的任务。由于质量问题、缺失值和高维度等限制，研究者采用因果性贝叶斯网络来学习评估模型，具有更好的先前知识利用和偏差缓解能力。 |
| [^49] | [Can Language Models Solve Graph Problems in Natural Language?.](http://arxiv.org/abs/2305.10037) | 本论文提出了自然语言图形(NLGraph)，这是一个全面的基于图形问题解决测试，旨在评估LLM在文本描述的图形结构和图形解决方案方面的处理能力。实验结果表明，LLM(GPT-3/4)具有相应的图形推理能力。 |
| [^50] | [A Survey on Causal Discovery: Theory and Practice.](http://arxiv.org/abs/2305.10032) | 该文综述了因果发现的理论、实践和最新进展，介绍了因果图恢复算法、实际应用及其重要性。 |
| [^51] | [An efficient solver for ASP(Q).](http://arxiv.org/abs/2305.10021) | 本文提出了一个基于qasp思想的ASP(Q)求解器，引入了更有效的编码过程和新的优化编码，并使用算法选择策略来提高性能，实验结果表明优于qasp和其他最先进的解决方案。 |
| [^52] | [Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers.](http://arxiv.org/abs/2305.10018) | 使用半监督学习技术对ViT模型进行微调，可用于缺乏注释数据的细粒度分类任务，比传统CNN和ViT都表现更好，有助于解决电子商务中图像标注问题。 |
| [^53] | [When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario.](http://arxiv.org/abs/2305.10013) | 本文介绍了一种新方法GDFO，将梯度下降和无导数优化结合在一起，协调地优化任务特定的连续提示。实验证明，该方法优于现有的无梯度和基于梯度的方法。 |
| [^54] | [Restoring Images Captured in Arbitrary Hybrid Adverse Weather Conditions in One Go.](http://arxiv.org/abs/2305.09996) | 提出了一种新的框架RAHC，可以一次性处理任意复杂恶劣天气条件下的图像恢复，并建立了一个新的数据集HAC，用于学习和基准测试混合条件的图像恢复。 |
| [^55] | [BASEN: Time-Domain Brain-Assisted Speech Enhancement Network with Convolutional Cross Attention in Multi-talker Conditions.](http://arxiv.org/abs/2305.09994) | 本文提出了一种基于听者脑电图信号的语音增强网络，可以在多说话人的情况下提取目标发言者，实验结果表明其优于最先进的方法。 |
| [^56] | [Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling.](http://arxiv.org/abs/2305.09993) | Reprompting是一种无需人类干预的算法，通过迭代采样新配方解决多步推理任务，比人类编写的思维链提示表现更好，还可以提高较弱模型的性能。 |
| [^57] | [A Fusion Model: Towards a Virtual, Physical and Cognitive Integration and its Principles.](http://arxiv.org/abs/2305.09992) | 本研究提出了一个融合模型--融合宇宙(FU)，它将虚拟、物理和认知世界融合在一起。研究了几个影响沉浸式和交互式体验的因素，并提出了一些融合宇宙的基本原则，可以使物理和虚拟世界无缝地融合起来。 |
| [^58] | [Stochastic Ratios Tracking Algorithm for Large Scale Machine Learning Problems.](http://arxiv.org/abs/2305.09978) | 本文提出了一种新的算法，在经典的SGD框架下实现自适应步长选择，在逻辑回归和深度神经网络上测试了所提出的算法，并证明了该算法可以生成与手动调整得到的最佳步长相当的步长。 |
| [^59] | [River of No Return: Graph Percolation Embeddings for Efficient Knowledge Graph Reasoning.](http://arxiv.org/abs/2305.09974) | 本文提出了一种基于图渗透的嵌入技术，通过维护最短路径和消除冗余路径来最小化熵的消息传递，从而显著提高了知识图谱推理的效率。 |
| [^60] | [HICO-DET-SG and V-COCO-SG: New Data Splits to Evaluate Systematic Generalization in Human-Object Interaction Detection.](http://arxiv.org/abs/2305.09948) | 本论文提出了两个新的HOI检测数据拆分，旨在评估系统性泛化。在新的数据拆分上测试结果表明，HOI检测模型对于未见过的对象和交互组合的泛化十分困难。 |
| [^61] | [Pittsburgh Learning Classifier Systems for Explainable Reinforcement Learning: Comparing with XCS.](http://arxiv.org/abs/2305.09945) | 本文介绍了两个新的匹兹堡学习分类器系统，与经典的密歇根系统 XCS 进行了比较，并在确定性和随机 FrozenLake 环境中获得了较好的表现。 |
| [^62] | [Demonstration-free Autonomous Reinforcement Learning via Implicit and Bidirectional Curriculum.](http://arxiv.org/abs/2305.09943) | 本文提出了一种无需演示的自主强化学习算法（IBC），通过辅助代理和基于最优输运的双向目标课程，能够在无需先前数据依赖的情况下，实现从非周期性交互中学习，并在稀疏任务相关交互的环境中取得更好的表现。 |
| [^63] | ["I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation.](http://arxiv.org/abs/2305.09941) | 本论文研究了如何以TGNB人群的声音为中心，评估开放式语言生成中的偏见。通过理解TGNB个体的经历，提出了以TGNB人群为中心的OLG系统评估框架，并且包括一个为TGNB人群设计的调查工具和分析方法。 |
| [^64] | [A Genetic Fuzzy System for Interpretable and Parsimonious Reinforcement Learning Policies.](http://arxiv.org/abs/2305.09922) | 提出了一种基于基因模糊系统的强化学习策略，可演化出可解释的简约策略，并能有效平衡策略性能与复杂性。 |
| [^65] | [Assessing the Impact of Context Inference Error and Partial Observability on RL Methods for Just-In-Time Adaptive Interventions.](http://arxiv.org/abs/2305.09913) | 本文探讨了在及时自适应干预中，强化学习方法如何学习干预选项选择策略，结果表明上下文推断误差和部分可观察性对学习有效策略的能力产生影响，通过在上下文不确定性增加时从上下文推断中传播的不确定性可以提高干预效果，而策略梯度算法可以提供对部分观察到的行为状态信息的非凡鲁棒性。 |
| [^66] | [Equivariant Few-Shot Learning from Pretrained Models.](http://arxiv.org/abs/2305.09900) | 本文提出了一种基于预训练模型的$\lambda$-\textit{equitune}方法，它使用\textit{重要性权重}$\lambda$对特征进行平均，可以显著提高等变小样本学习的表现。 |
| [^67] | [Clustering-Aware Negative Sampling for Unsupervised Sentence Representation.](http://arxiv.org/abs/2305.09892) | ClusterNS 是一种将聚类信息引入对比学习进行无监督句子表示学习的新方法，通过改进的 K 均值聚类算法提供难负例并识别错误负例，旨在通过一个统一的框架解决问题，实验结果表明其在无监督句子表示学习中表现优于基线。 |
| [^68] | [Semantic Similarity Measure of Natural Language Text through Machine Learning and a Keyword-Aware Cross-Encoder-Ranking Summarizer -- A Case Study Using UCGIS GIS&T Body of Knowledge.](http://arxiv.org/abs/2305.09877) | 本文提出了一种新方法，采用机器学习模型和关键词感知交叉编码器排序摘要程序，从文本内容中提取语义信息，并度量 GIS&T BoK 话题之间的语义相似度，以解决手动定义话题关系带来的不完整评估问题。该方法在准确度量话题关系方面表现良好，对 GIS&T 领域的研究和实践具有重要意义。 |
| [^69] | [Explaining black box text modules in natural language with language models.](http://arxiv.org/abs/2305.09863) | 本文介绍了一种名为Summarize and Score（SASC）的方法，该方法可以自动获取黑盒文本模块的自然语言解释以及解释可靠程度的分数。研究者们已经在合成模块和BERT模型中使用SASC，让我们可以解释模块的选择性，这对于增强大型语言模型的可解释性非常重要。 |
| [^70] | [Epsilon Sampling Rocks: Investigating Sampling Strategies for \\Minimum Bayes Risk Decoding for Machine Translation.](http://arxiv.org/abs/2305.09860) | 本文研究了用于机器翻译最小贝叶斯风险解码的不同采样策略，并发现了epsilon采样方式能够使得解码结果显著地优于其他所有已测试的采样方式和束搜索解码。 |
| [^71] | [Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs.](http://arxiv.org/abs/2305.09858) | 本文通过对知识图谱中关系标注的实证研究，发现大型语言模型具有强大的学习能力以及在少量标记数据下预测产品类型之间关系的有效性。 |
| [^72] | [CoEdIT: Text Editing by Task-Specific Instruction Tuning.](http://arxiv.org/abs/2305.09857) | CoEdIT是一种通过任务特定指令调整实现文本编辑的最先进模型，能够提高用户生成文本的质量和提高流程的效率。 |
| [^73] | [Keep It Simple: Fault Tolerance Evaluation of Federated Learning with Unreliable Clients.](http://arxiv.org/abs/2305.09856) | 本文评估了具有不可靠客户端的联邦学习的容错性，研究表明相对较简单的FL算法在此情境下也能表现良好。 |
| [^74] | [Scale-Adaptive Balancing of Exploration and Exploitation in Classical Planning.](http://arxiv.org/abs/2305.09840) | 本文提出了一种MCTS/THTS算法GreedyUCT-Normal，该算法能够通过采用奖励变化的尺度处理不同尺度的分布，以在经典计划中平衡探索和开发。 |
| [^75] | [Coagent Networks: Generalized and Scaled.](http://arxiv.org/abs/2305.09838) | 论文提出了一种强大而灵活的合作智能网络框架，可以异步计算网络不同部分、吸收反向传播不能使用的不可微组件、探索和/或时态抽象的分层网络，并使用高效算法进行分布式和并行学习。在基准问题上的模拟表明，该算法在性能上有显著提高。 |
| [^76] | [Revisiting the Minimalist Approach to Offline Reinforcement Learning.](http://arxiv.org/abs/2305.09836) | 这篇论文提出了一种名为ReBRAC的极简算法，它在TD3+BC方法的基础上整合了设计元素，通过对近期离线强化学习研究的回顾性分析，证明其在离线强化学习上的领先地位。 |
| [^77] | [A Deep RL Approach on Task Placement and Scaling of Edge Resources for Cellular Vehicle-to-Network Service Provisioning.](http://arxiv.org/abs/2305.09832) | 本文提出了一种基于深度强化学习的分散式方法，用于解决车联网服务提供中的任务部署和边缘资源的扩展问题。 |
| [^78] | [Mimetic Initialization of Self-Attention Layers.](http://arxiv.org/abs/2305.09828) | 本文介绍一种名为拟态初始化的方法，通过仅仅调整自注意力层的权重初始化，即可在视觉任务中大大提高Transformer的精度。 |
| [^79] | [A Method for Training-free Person Image Picture Generation.](http://arxiv.org/abs/2305.09817) | 本文提出一种无需训练的角色图像特征编码器模型，使得用户可以通过简单提供角色的图片，生成匹配期望的图像并调整各种细节，无需为每个个体/动画角色图像单独训练模型。 |
| [^80] | [Exploring outlooks towards generative AI-based assistive technologies for people with Autism.](http://arxiv.org/abs/2305.09815) | 本篇论文探讨了将深度伪造技术作为自闭症患者辅助技术的潜力，并通过 Reddit 评论的调查得出结论。 |
| [^81] | [Sasha: creative goal-oriented reasoning in smart homes with large language models.](http://arxiv.org/abs/2305.09802) | 本论文研究了在智能家居中使用大型语言模型实现用户命令的创意目标导向推理。实验结果显示，这种方法可以创造性地推理，以实现挑战性的目标。 |
| [^82] | [Reinforcement Learning for Safe Robot Control using Control Lyapunov Barrier Functions.](http://arxiv.org/abs/2305.09793) | 本研究利用控制李亚普诺夫屏障函数及LBAC算法，提出了一种模型无关的强化学习方法，实现了基于数据的安全性和可达性条件下机器人控制。在实际2D四旋翼导航任务中验证该方法的有效性，优于其他模型无关强化学习方法。 |
| [^83] | [Codesign of Edge Intelligence and Automated Guided Vehicle Control.](http://arxiv.org/abs/2305.09788) | 本文介绍了一种自主引导车辆（AGV）控制、边缘智能和人类输入的和谐设计，以实现工业环境中的自主运输，其核心技术是通过无线网络连接人工智能（AI）和AGV实现人机协同。 |
| [^84] | [Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned Language Models.](http://arxiv.org/abs/2305.09785) | 本论文通过对比学习策略，提高了语言模型的概念嵌入质量，并在各种基准测试中实现了最先进的结果。 |
| [^85] | [Analysis of Visual Question Answering Algorithms with attention model.](http://arxiv.org/abs/2305.09782) | 本文批评性地检查和审查了使用共同注意力方法的VQA算法的方法，重点关注文本语义生成、对象识别和答案分类技术。 |
| [^86] | [A Scalable Walsh-Hadamard Regularizer to Overcome the Low-degree Spectral Bias of Neural Networks.](http://arxiv.org/abs/2305.09779) | 本文提出了一种新型的可扩展Walsh-Hadamard正则化器，可避免神经网络学习低阶频率以及误识别低阶频率，从而提高了神经网络的泛化性能。 |
| [^87] | [Towards Modeling Human Attention from Eye Movements for Neural Source Code Summarization.](http://arxiv.org/abs/2305.09773) | 利用眼动数据对人类的注意力进行建模，并将模型应用于基于神经网络的源代码摘要中，预测源代码中最重要的单词并增强了基线模型的预测性能。 |
| [^88] | [ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing.](http://arxiv.org/abs/2305.09770) | ConvXAI是一个基于对话的XAI系统，它集成了多种XAI类型，并将实际用户需求嵌入设计中，以提高实用性。 |
| [^89] | [CQural: A Novel CNN based Hybrid Architecture for Quantum Continual Machine Learning.](http://arxiv.org/abs/2305.09738) | 本文展示了一种基于混合CNN的量子持续机器学习架构，可以通过解释哪些特征对于分类最重要来避免遗忘，并声称如果使用这些解释来训练模型，则会获得更好的性能。 |
| [^90] | [ADDSL: Hand Gesture Detection and Sign Language Recognition on Annotated Danish Sign Language.](http://arxiv.org/abs/2305.09736) | 本研究引入了一个新的丹麦手语注释数据集（ADDSL）并使用该数据集训练了一个基于YOLOv5的手势检测和字母数字识别模型，准确率最高可达92%。与同领域现有工作相比，该模型更高效和准确。 |
| [^91] | [FedHGN: A Federated Framework for Heterogeneous Graph Neural Networks.](http://arxiv.org/abs/2305.09729) | FedHGN是一种用于异构图神经网络的联邦学习框架，它采用模式权重解耦和系数对齐技术，使得不同客户端可以共享知识而不泄露隐私，相比于现有方法表现更加优秀。 |
| [^92] | [Dynamic Causal Explanation Based Diffusion-Variational Graph Neural Network for Spatio-temporal Forecasting.](http://arxiv.org/abs/2305.09703) | 本文提出了一种新颖的基于因果关系解释的扩散变分图神经网络，用于时空预测，在动态图构建上考虑了邻居节点之间的因果关系和不确定性，解决了动态图算法的可解释性和稳定性问题。 |
| [^93] | [Generative Table Pre-training Empowers Models for Tabular Prediction.](http://arxiv.org/abs/2305.09696) | 本文提出了TapTap，一种通过表格预训练生成高质量合成表格来提高表格预测性能的方法。在12个数据集实验中，TapTap在不同场景下优于16个基线，并可以与多个骨干模型结合使用。 |
| [^94] | [Evaluation Strategy of Time-series Anomaly Detection with Decay Function.](http://arxiv.org/abs/2305.09691) | 本文提出了带衰减函数的点调整协议（PAdf）以解决现有时间序列异常检测算法评估方式高估或低估性能的问题。通过在基准数据集上的重新评估，我们发现PAdf协议不仅考虑要查找尽可能多的段数，还考虑快速准确地检测异常。 |
| [^95] | [Data Bias Management.](http://arxiv.org/abs/2305.09686) | 本文讲述了数据偏差在机器学习中的应用、影响及可能的解决方案 |
| [^96] | [Decision-based iterative fragile watermarking for model integrity verification.](http://arxiv.org/abs/2305.09684) | 该论文提出了一种基于决策迭代算法的脆弱水印技术，能将普通训练样本转化为对模型更敏感的脆弱样本，用于模型完整性验证。 |
| [^97] | [Vulnerability Detection Using Two-Stage Deep Learning Models.](http://arxiv.org/abs/2305.09673) | 本文提出了一种双阶段解决方案，采用了两个深度学习模型用于漏洞检测，其可在识别和分类各种类型的漏洞方面达到高准确率，优于传统SAST和DAST方法。 |
| [^98] | [Satisfiability-Aided Language Models Using Declarative Prompting.](http://arxiv.org/abs/2305.09656) | 本文提出了一种利用自动定理证明器和声明性任务规范的可满足性辅助语言建模方法，可以提高大型语言模型的推理能力。 |
| [^99] | [Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer.](http://arxiv.org/abs/2305.09480) | 本文提出了一种深度生成模型，可以一次性地共同设计抗体CDR的1D序列和3D结构，解决几何建模和低效推断的问题。 |
| [^100] | [Unsupervised sequence-to-sequence learning for automatic signal quality assessment in multi-channel electrical impedance-based hemodynamic monitoring.](http://arxiv.org/abs/2305.09368) | 本论文提出了一种无监督序列到序列学习方法，用于自动评估多通道电阻抗血流动力学监测中心肺容积信号（CVS）的运动诱导可靠性降低。通过利用长短时记忆和变分自编码器结构，编码器 - 解码器模型可以捕捉到暂态 CVS 序列中存在的上下文知识。与现有方法相比，本方法不需要手动注释运动影响以及缺乏在 CVS 随时间出现上下文变化的情况下实现运动引起异常的显式机制，具有竞争性能。 |
| [^101] | [BERTTM: Leveraging Contextualized Word Embeddings from Pre-trained Language Models for Neural Topic Modeling.](http://arxiv.org/abs/2305.09329) | 本文提出了一种新颖的神经主题模型，利用来自预训练语言模型BERT的上下文化词嵌入，可以在不使用任何BoW信息的情况下推断出文档的主题分布，并直接从上下文化词嵌入中推断出文档中每个单词的主题分布。实验结果表明，该模型优于仅依赖BoW表示和其他神经主题模型的现有最先进方法。 |
| [^102] | [SuSana Distancia is all you need: Enforcing class separability in metric learning via two novel distance-based loss functions for few-shot image classification.](http://arxiv.org/abs/2305.09062) | 本文提出了两种新的基于距离的损失函数，通过考虑少量数据之间的类内距离和类间距离来考虑嵌入向量的重要性，以增强度量学习中的类别可分性，实现优秀的少样本图像分类表现。 |
| [^103] | [Motion Question Answering via Modular Motion Programs.](http://arxiv.org/abs/2305.08953) | 提出了一种新的HumanMotionQA任务，用于评估模型在复杂的人体运动序列上进行复杂、多步推理的能力。同时，提出了一种特殊的NSPose方法，利用符号化推理和模块化设计，成功地应用于该任务中，并超过所有基线方法。 |
| [^104] | [Neurosymbolic AI and its Taxonomy: a survey.](http://arxiv.org/abs/2305.08876) | 本文调查研究了神经符号人工智能的研究，探索了学习数据分布和推理先前和学习知识相结合的方法，以探索实现人工智能通用性的替代方案。 |
| [^105] | [A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects.](http://arxiv.org/abs/2305.07348) | 本文评估了基于深度学习的单目航天器姿态估计的现有方法，总结了部署该方法在现实任务中仍需克服的挑战。 |
| [^106] | [Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy.](http://arxiv.org/abs/2305.06360) | 本文综述了机器遗忘的现状和技术应用，包括数据删除、扰动和模型更新，讨论了MU在隐私、安全和公正性等领域的潜在益处，以及它在自然语言处理、计算机视觉和推荐系统中的未来发展方向。 |
| [^107] | [A proof of convergence of inverse reinforcement learning for multi-objective optimization.](http://arxiv.org/abs/2305.06137) | 本论文证明了多目标优化的逆强化学习方法在理论层面上的收敛性，包括Wasserstein逆强化学习和常规逆强化学习方法。 |
| [^108] | [A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness.](http://arxiv.org/abs/2305.03355) | 本研究对当前最先进的数据集压缩方法进行了全面评估，发现其存在隐私风险并可能放大模型的不公平性，提供了大规模的基准测试框架。 |
| [^109] | [Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory.](http://arxiv.org/abs/2305.02437) | 本文提出了一种新的检索增强文本生成模型Selfmem，通过迭代生成自我记忆池并采用记忆选择器，使检索更加自适应，提高了文本生成的质量和多样性。 |
| [^110] | [ChatGPT -- a Blessing or a Curse for Undergraduate Computer Science Students and Instructors?.](http://arxiv.org/abs/2304.14993) | 本文分析了ChatGPT在回答本科计算机科学问题上的不可靠性，并提供了在学术界使用ChatGPT的建议。 |
| [^111] | [Optimizing Energy Efficiency in Metro Systems Under Uncertainty Disturbances Using Reinforcement Learning.](http://arxiv.org/abs/2304.13443) | 本文提出了一种基于策略的强化学习方法，通过重新安排地铁时刻表和调整列车的停靠时间和巡航速度，优化扰动下的地铁系统能源效率，该方法在模拟环境下实验证明其优于基线方法，最高可达降低10.9%的牵引能量消耗和最高达提高47.9%的再生制动能量利用率，为城市轨道交通的节能问题提供了有效的解决方案。 |
| [^112] | [Segment Anything Model for Medical Image Analysis: an Experimental Study.](http://arxiv.org/abs/2304.10517) | 本研究对医学图像分割模型SAM在各种不同情况下的表现进行了广泛评估，结果表明在单点提示下其表现高度变化，是一项具有挑战性的工作。 |
| [^113] | [NetGPT: Generative Pretrained Transformer for Network Traffic.](http://arxiv.org/abs/2304.09513) | 本文提出了首个网络流量生成预训练变压器模型NetGPT，该模型可以优化网络任务的训练效率和有效性。 |
| [^114] | [Classification of US Supreme Court Cases using BERT-Based Techniques.](http://arxiv.org/abs/2304.08649) | 本文基于BERT技术探究了对美国最高法院案例进行分类的方法，比较了使用BERT模型与其他先进模型的准确性，最终在15个广泛类别上取得了80%的准确度，在279个细粒度类别上取得了60%的准确度。 |
| [^115] | [Attentive Q-Matrix Learning for Knowledge Tracing.](http://arxiv.org/abs/2304.08168) | 本文提出了一种基于Q-矩阵的注意力知识追踪模型，能够在不存在事先确定的技能标签的情况下应用于大规模的在线教育平台。 |
| [^116] | [AutoRL Hyperparameter Landscapes.](http://arxiv.org/abs/2304.02396) | 本文提出了一种方法，在训练期间多次建立和分析AutoRL超参数的景观，证明代表算法（DQN和SAC）在不同环境下的超参数景观会随时间而变化。 |
| [^117] | [Positive-Augmented Constrastive Learning for Image and Video Captioning Evaluation.](http://arxiv.org/abs/2303.12112) | 本论文提出一种新的图像标题评估指标PAC-S，可以更准确地评估图像和视频的标题，相比于现有的指标有更好的表现；源代码和训练模型已经公开。 |
| [^118] | [DeepSaDe: Learning Neural Networks that Guarantee Domain Constraint Satisfaction.](http://arxiv.org/abs/2303.01141) | 本文提出了一种学习神经网络的方法，该神经网络可以强制执行多样化的约束并且保证所有可能的预测都满足约束限制。 |
| [^119] | [Safe Multi-agent Learning via Trapping Regions.](http://arxiv.org/abs/2302.13844) | 本文通过引入定性理论中的捕获区域概念，创建出安全集，以确保分散式学习在联合策略空间中的收敛性。 |
| [^120] | [One Fits All:Power General Time Series Analysis by Pretrained LM.](http://arxiv.org/abs/2302.11939) | 本论文提出了一种称为 Frozen Pretrained Transformer (FPT) 的预训练模型，利用从数十亿标记训练出来的语言或 CV 模型进行时间序列分析的所有主要类型任务的微调，进而使其在所有任务中都具备着最先进的性能和泛化能力。 |
| [^121] | [Growing Steerable Neural Cellular Automata.](http://arxiv.org/abs/2302.10197) | 本文讨论了"Growing Steerable Neural Cellular Automata"，通过使每个细胞负责其自己的方向，产生了具有变化方向的细胞的模型。 |
| [^122] | [Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs.](http://arxiv.org/abs/2302.02865) | 本文提出利用概率对比学习方法可以恢复具有不确定性输入的正确估计，通过扩展InfoNCE目标和编码器以预测潜变量分布来实现，在计算已知查询图像的可信区间方面具有应用价值。 |
| [^123] | [Dual Self-Awareness Value Decomposition Framework without Individual Global Max for Cooperative Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2302.02180) | 提出了一种基于双重自我意识价值分解框架的协作多智体强化学习方法，完全摒弃了个体全局最大值的前提，具有良好的性能表现。 |
| [^124] | [Average-Constrained Policy Optimization.](http://arxiv.org/abs/2302.00808) | 本研究提出了一种新的基于函数逼近算法的带平均标准约束 MDP 的策略优化算法，具有较好的性能表现。 |
| [^125] | [Distillation Policy Optimization.](http://arxiv.org/abs/2302.00533) | 本文展示了一种演员-评论家的学习框架，该框架通过蒸馏优势在利用过去经验的同时遵循稳定的在线策略，实现了快速学习并可以适用于广泛的算法类别。 |
| [^126] | [Anti-Exploration by Random Network Distillation.](http://arxiv.org/abs/2301.13616) | 本文介绍了基于特征线性调制的随机网络碾压算法，可以有效防止探索，避免了区分度的问题，在 D4RL 基准测试中取得了可与集成方法相媲美的性能。 |
| [^127] | [Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2.](http://arxiv.org/abs/2301.11719) | 本论文研究了将事实知识纳入生成的摘要的可能性，具体采用前缀调整的方法，实验结果表明，此方法可以生成保留知识的摘要，而且可以提升整体性能。 |
| [^128] | [Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design.](http://arxiv.org/abs/2301.10774) | 本研究提出了一个基于层次数据有效表示学习的RNA设计流程，通过构建大型数据集并设计全面的结构建模方法，实现了更高效的RNA序列设计。 |
| [^129] | [A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference.](http://arxiv.org/abs/2212.12393) | 本文介绍了一种名为A-NeSI的新颖PNL框架，它使用神经网络实现了近似推理，能够保证概率逻辑语义的同时解决了PNL的可扩展性问题，能够在安全关键应用中保证逻辑约束的满足。 |
| [^130] | [Towards Causal Credit Assignment.](http://arxiv.org/abs/2212.11636) | 本论文研究了后见信用分配这种分配信用的方法，并提出了一种结合因果结构的状态表示来提高效率的方法。 |
| [^131] | [KGLM: Integrating Knowledge Graph Structure in Language Models for Link Prediction.](http://arxiv.org/abs/2211.02744) | 本文提出了 KGLM 架构，将新的实体/关系嵌入层整合进语言模型，使其能够学习知识图谱的结构。使用从知识图谱提取的三元组对模型进行预训练并进行链接预测任务得到了良好效果。 |
| [^132] | [Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning.](http://arxiv.org/abs/2207.09081) | 本文提出了一种增强的目标条件强化学习框架，它使用因果图来发现和表示因果关系来实现模型的泛化性。 |
| [^133] | [Super Images -- A New 2D Perspective on 3D Medical Imaging Analysis.](http://arxiv.org/abs/2205.02847) | 本文提出了一种在医学成像分析中处理3D数据的新方法，即将体积数据转换为2D超级图像，并使用2D网络进行分割，以提高效率。该方法可以在训练中有效地嵌入3D知识。 |
| [^134] | [Neighborhood Attention Transformer.](http://arxiv.org/abs/2204.07143) | 提出了针对视觉任务的高效和可扩展的滑动窗口注意力机制——邻域关注（NA）。基于NA，开发了NAT，NAT-Tiny在ImageNet上达到了83.2％的top-1准确率，能够提高图像分类和下游视觉性能。 |
| [^135] | [DiGS : Divergence guided shape implicit neural representation for unoriented point clouds.](http://arxiv.org/abs/2106.10811) | 本文提出了一种新的散度导向形状表示学习方法，无需输入法向量，以软约束倾向于平滑解决方案，可靠地定位每个点的未知法向量的梯度，甚至比使用真实法向量更好。此外，还引入了一种新的正弦INR几何初始化方法，以进一步提高收敛性能。 |

# 详细

[^1]: 并行解码加速Transformer在翻译中的应用

    Accelerating Transformer Inference for Translation via Parallel Decoding. (arXiv:2305.10427v1 [cs.CL])

    [http://arxiv.org/abs/2305.10427](http://arxiv.org/abs/2305.10427)

    通过并行解码，本文提出了一种快速推断Transformer在翻译中的应用的方法，不需要修改现有模型并在保持翻译质量的同时加速了现有模型。

    

    自回归解码限制了Transformer在机器翻译中的效率。社区提出了特定的网络架构和基于学习的方法来解决这个问题，但它们都很昂贵并且需要改变机器翻译模型，以推导出解码速度和翻译质量之间的平衡。本文从解码算法的角度提出了一个解决方案，将标准的贪心自回归解码转化为并行解码，并利用雅克比和高斯-塞德尔迭代方法实现快速推断。该算法不需要修改现有模型，并在保持翻译质量的同时加速了现有模型。我们提出了三种并行解码算法，并在不同语言和模型上进行了测试，证明并行化解码相对于标准自回归解码可提高达38％的速度，当扩展模型时，速度几乎提高了2倍。

    Autoregressive decoding limits the efficiency of transformers for Machine Translation (MT). The community proposed specific network architectures and learning-based methods to solve this issue, which are expensive and require changes to the MT model, trading inference speed at the cost of the translation quality. In this paper, we propose to address the problem from the point of view of decoding algorithms, as a less explored but rather compelling direction. We propose to reframe the standard greedy autoregressive decoding of MT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point iteration methods for fast inference. This formulation allows to speed up existing models without training or modifications while retaining translation quality. We present three parallel decoding algorithms and test them on different languages and models showing how the parallelization introduces a speedup up to 38% w.r.t. the standard autoregressive decoding and nearly 2x when scaling t
    
[^2]: SLiC-HF：人类反馈的序列似然校准

    SLiC-HF: Sequence Likelihood Calibration with Human Feedback. (arXiv:2305.10425v1 [cs.CL])

    [http://arxiv.org/abs/2305.10425](http://arxiv.org/abs/2305.10425)

    本文提出了一种新方法，SLiC-HF，可以利用序列似然校准从人类偏好中学习，相较于过去的方法更加简单高效，并在TL;DR自动摘要任务中显著提高了监督微调基线。

    

    已经证明，从人类反馈中学习可以有效地将语言模型与人类偏好对齐。过去的工作通常依赖于从人类偏好数据训练的奖励模型分配的奖励分数，利用人类反馈进行强化学习（RLHF）来优化语言模型。在本文中，我们展示了最近引入的序列似然校准（SLiC）如何有效地应用于从人类偏好中学习（SLiC-HF）。此外，我们证明这可以使用为不同模型收集的人类反馈数据来完成，类似于离线RL数据的离线学习。自动化和人类评估实验表明，SLiC-HF显著改进了监督微调基线。此外，SLiC-HF是过去工作中使用的PPO RLHF实现的竞争性替代，而且在实践中更简单、更易于调整，并具有更高的计算效率。

    Learning from human feedback has been shown to be effective at aligning language models with human preferences. Past work has often relied on Reinforcement Learning from Human Feedback (RLHF), which optimizes the language model using reward scores assigned from a reward model trained on human preference data. In this work we show how the recently introduced Sequence Likelihood Calibration (SLiC), can also be used to effectively learn from human preferences (SLiC-HF). Furthermore, we demonstrate this can be done with human feedback data collected for a different model, similar to off-policy, offline RL data. Automatic and human evaluation experiments on the TL;DR summarization task show that SLiC-HF significantly improves supervised fine-tuning baselines. Furthermore, SLiC-HF presents a competitive alternative to the PPO RLHF implementation used in past work while being much simpler to implement, easier to tune and more computationally efficient in practice.
    
[^3]: Scratch Copilot 评估：评估AI辅助的亲子创意编码

    Scratch Copilot Evaluation: Assessing AI-Assisted Creative Coding for Families. (arXiv:2305.10417v1 [cs.HC])

    [http://arxiv.org/abs/2305.10417](http://arxiv.org/abs/2305.10417)

    本研究评估了使用AI辅助家庭进行创意编码的潜力，使用大型语言模型（LLMs）帮助家庭理解代码、调试程序以及为未来项目生成新的想法，并实现了超过80\%的总体成功率。

    

    如何让人工智能增强家庭创意编码体验？本研究探讨了使用Scratch进行创意编码时，大型语言模型（LLM）的潜力。基于我们之前的用户研究，我们设计了三个评估方案，以确定LLM是否能帮助家庭理解游戏代码、调试程序和为未来项目生成新想法。我们使用了22个Scratch项目来完成每个方案，生成LLM的响应，包括有和没有练习任务的情况，从而产生了120个创意编码支持方案数据集。此外，作者还独立评估了LLM的精确性、教学价值和适宜年龄的语言。我们的研究结果显示，LLMs在不同的任务和评估标准下均实现了超过80\%的总体成功率。本研究提供了有关使用LLMs进行家庭创意编码的有价值信息，并提出了未来AI支持的编码应用的设计指南。我们的评估突出了LLMs在协助家庭使用Scratch进行创意编码方面的潜力。

    How can AI enhance creative coding experiences for families? This study explores the potential of large language models (LLMs) in helping families with creative coding using Scratch. Based on our previous user study involving a prototype AI assistant, we devised three evaluation scenarios to determine if LLMs could help families comprehend game code, debug programs, and generate new ideas for future projects. We utilized 22 Scratch projects for each scenario and generated responses from LLMs with and without practice tasks, resulting in 120 creative coding support scenario datasets. In addition, the authors independently evaluated their precision, pedagogical value, and age-appropriate language. Our findings show that LLMs achieved an overall success rate of more than 80\% on the different tasks and evaluation criteria. This research offers valuable information on using LLMs for creative family coding and presents design guidelines for future AI-supported coding applications. Our evalu
    
[^4]: AI朋友：面向青少年的AI创意编程设计框架

    AI Friends: A Design Framework for AI-Powered Creative Programming for Youth. (arXiv:2305.10412v1 [cs.HC])

    [http://arxiv.org/abs/2305.10412](http://arxiv.org/abs/2305.10412)

    本文调查了AI在家庭创意编程中的角色和作用，通过设计一个"人机交互"平台帮助家庭与AI朋友合作，发现AI朋友可以通过提问帮助家庭创作游戏，在AI朋友无法协助时，家长也发挥了独特的作用，AI支持的平台应强调儿童为主角的AI独特互动，并关注创意自我效能。

    

    AI能在家庭中发挥怎样的作用，支持和限制创意编程？为了调查这些问题，我们建立了一个"人机交互"平台，帮助家庭与由研究人员操控的AI朋友，进行创意编程的合作。我们与10个7至12岁的儿童家庭合作设计了一个为期3周的编程环节。通过创意自我效能理论，我们发现当AI朋友通过提问来引导游戏创意时，家庭更容易生成游戏创意；当AI朋友无法协助时，家长在指导孩子完成更复杂的编程任务方面起到了独特的作用；孩子们更加愿意使用AI朋友的帮助，写出新奇的编程创意。这些发现表明，AI支持的平台应强调家庭间以儿童为主角的AI独特互动，并关注创意自我效能。

    What role can AI play in supporting and constraining creative coding by families? To investigate these questions, we built a Wizard of Oz platform to help families engage in creative coding in partnership with a researcher-operated AI Friend. We designed a 3 week series of programming activities with ten children, 7 to 12 years old, and nine parents. Using a creative self efficacy lens, we observe that families found it easier to generate game ideas when prompted with questions by AI Friend; parents played a unique role in guiding children in more complex programming tasks when the AI Friend failed to help, and children were more encouraged to write code for novel ideas using the AI friend help. These findings suggest that AI supported platforms should highlight unique family AI interactions focused on children's agency and creative self-efficacy.
    
[^5]: 变分分类

    Variational Classification. (arXiv:2305.10406v1 [cs.LG])

    [http://arxiv.org/abs/2305.10406](http://arxiv.org/abs/2305.10406)

    提出一种新的变分分类方法，通过引入潜变量建模来优化训练，允许灵活的设计选择以改善校准和对抗鲁棒性，实验结果表明其对于域外数据的分类准确性得到了保持。

    

    我们提出了一种传统神经网络方法的新型扩展，称为变分分类 (VC)。通过引入潜变量建模，类似于变分自编码器和传统自编码器之间的关系，我们得到了一个基于证据下界 (ELBO) 的训练目标，采用对抗性方法优化。我们的VC模型允许在设计选择方面更加灵活，特别是类条件潜先验，而不是在现成的softmax分类器中做出的隐式假设。在图像和文本分类数据集上的实证评估表明，我们的方法在保持预测准确性的同时，改善了其他良好特性，如校准和对抗鲁棒性，即使应用于域外数据。

    We present a novel extension of the traditional neural network approach to classification tasks, referred to as variational classification (VC). By incorporating latent variable modeling, akin to the relationship between variational autoencoders and traditional autoencoders, we derive a training objective based on the evidence lower bound (ELBO), optimized using an adversarial approach. Our VC model allows for more flexibility in design choices, in particular class-conditional latent priors, in place of the implicit assumptions made in off-the-shelf softmax classifiers. Empirical evaluation on image and text classification datasets demonstrates the effectiveness of our approach in terms of maintaining prediction accuracy while improving other desirable properties such as calibration and adversarial robustness, even when applied to out-of-domain data.
    
[^6]: PaLM 2 技术报告

    PaLM 2 Technical Report. (arXiv:2305.10403v1 [cs.CL])

    [http://arxiv.org/abs/2305.10403](http://arxiv.org/abs/2305.10403)

    PaLM 2 是一种计算效率更高的最先进的语言模型，提供了更好的多语言和推理能力，并且通过使用多种目标进行训练，获得了在不同模型大小的下游任务上显着的改进质量。此外，PaLM 2 还展示了强大的推理能力和稳定的性能表现，使得模型能够更广泛地部署，并且可以控制毒性推理时间，而不会对其他能力产生影响。

    

    我们介绍了 PaLM 2，这是一种新的最先进的语言模型，比其前身 PaLM 在多语言和推理能力方面更加出色，并且计算效率更高。PaLM 2 是一种基于 Transformer 的模型，使用多种目标进行训练。通过对英语和多语言语言以及推理任务的广泛评估，我们展示了 PaLM 2 在不同模型大小的下游任务上具有显着的改进质量，同时展现了比 PaLM 更快和更有效的推理能力。这种改进的效率使得模型能够更广泛地部署，同时也使得模型能够更快地响应，以获得更自然的交互节奏。PaLM 2 展示了强大的推理能力，在 BIG-Bench 和其他推理任务上相对于 PaLM 有巨大的改进。PaLM 2 在一套负责人的 AI 评估中表现出稳定的性能，并且在没有附加运行开销或对其他能力产生影响的情况下，能够对毒性进行推理时间的控制。

    We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Over
    
[^7]: 可解释的多智能体强化学习用于时间查询

    Explainable Multi-Agent Reinforcement Learning for Temporal Queries. (arXiv:2305.10378v1 [cs.AI])

    [http://arxiv.org/abs/2305.10378](http://arxiv.org/abs/2305.10378)

    本文提出了一种可解释的多智能体强化学习方法，用于回答关于智能体行为发展的时间查询，该方法可以生成策略级对比解释，以解释智能体的实际和预期行为之间的差异。

    

    随着多智能体强化学习系统在社会中的广泛应用，用户理解复杂环境中多智能体行为的发展变得至关重要而具有挑战性。本研究提出了一种方法，生成多智能体强化学习策略级对比说明，以回答用户提出的时间查询，该查询指定了智能体完成任务的一系列任务，并可能进行合作。所提出的方法将时间查询编码为PCTL逻辑公式，并通过概率模型检查检查给定的MARL策略下是否可行。这种解释有助于协调实际和预期的多智能体行为之间的差异。所提出的方法还生成正确和完整的解释，以确定使用户查询不可行的原因。已成功将所提出的方法应用于四个基准MARL领域（一个领域最多有9个智能体）。此外，用户研究的结果显示，所提供的解释对于用户理解复杂的MARL行为模型非常有用。

    As multi-agent reinforcement learning (MARL) systems are increasingly deployed throughout society, it is imperative yet challenging for users to understand the emergent behaviors of MARL agents in complex environments. This work presents an approach for generating policy-level contrastive explanations for MARL to answer a temporal user query, which specifies a sequence of tasks completed by agents with possible cooperation. The proposed approach encodes the temporal query as a PCTL logic formula and checks if the query is feasible under a given MARL policy via probabilistic model checking. Such explanations can help reconcile discrepancies between the actual and anticipated multi-agent behaviors. The proposed approach also generates correct and complete explanations to pinpoint reasons that make a user query infeasible. We have successfully applied the proposed approach to four benchmark MARL domains (up to 9 agents in one domain). Moreover, the results of a user study show that the ge
    
[^8]: 非合作博弈中的人类选择预测：基于模拟的离线策略评估

    Human Choice Prediction in Non-Cooperative Games: Simulation-based Off-Policy Evaluation. (arXiv:2305.10361v1 [cs.LG])

    [http://arxiv.org/abs/2305.10361](http://arxiv.org/abs/2305.10361)

    本文研究了语言游戏中的离线策略评估，并提出了一种结合真实和模拟数据的新方法。

    

    说服游戏在经济和人工智能研究中具有重要意义并具有重要的实际应用。本文探讨了在基于语言的说服游戏中离线策略评估（OPE）的挑战性问题，提出了一种结合真实和模拟人类 - 机器人交互数据的新方法，并给出了一种深度学习训练算法，该算法有效地整合了真实交互和模拟数据。

    Persuasion games have been fundamental in economics and AI research, and have significant practical applications. Recent works in this area have started to incorporate natural language, moving beyond the traditional stylized message setting. However, previous research has focused on on-policy prediction, where the train and test data have the same distribution, which is not representative of real-life scenarios. In this paper, we tackle the challenging problem of off-policy evaluation (OPE) in language-based persuasion games. To address the inherent difficulty of human data collection in this setup, we propose a novel approach which combines real and simulated human-bot interaction data. Our simulated data is created by an exogenous model assuming decision makers (DMs) start with a mixture of random and decision-theoretic based behaviors and improve over time. We present a deep learning training algorithm that effectively integrates real interaction and simulated data, substantially im
    
[^9]: BIOT：在野外跨数据源生物信号学习

    BIOT: Cross-data Biosignal Learning in the Wild. (arXiv:2305.10351v1 [eess.SP])

    [http://arxiv.org/abs/2305.10351](http://arxiv.org/abs/2305.10351)

    该论文提出了一种可以在多个生物信号数据源上进行训练，并可在所有生物信号任务上进行微调的基础模型BIOT。通过将不同的生物信号令牌化为统一的“生物信号句子”，该模型可以实现跨数据源的学习，处理各种格式的生物信号，具有出色的性能。

    

    生物信号，如脑电图（EEG），在许多临床应用中发挥着至关重要的作用，显示出多种数据格式和质量特征。当前的生物信号深度学习模型通常专门针对特定数据集和临床设置进行了优化，从而限制了它们的应用范围。受到大型语言模型在文本处理方面的成功启发，我们探索开发基于多个数据源进行训练并可在不同基础生物信号任务上进行微调的基础模型。为了克服与各种格式的生物信号相关的独特挑战，例如不匹配的通道，可变的采样长度和普遍存在的缺失值，我们提出了一种Biosignal Transformer模型（简称\method）。所提出的\method模型可以通过将不同的生物信号令牌化为统一的“生物信号句子”来实现通道不匹配、长度可变和缺失值普遍存在的跨数据学习。具体来说，我们将每个通道令牌化为固定长度的特征，并在通道之间进行聚合，以形成完整的生物信号句子。在EEG分类和EEG转语音任务上的实验表明，与当前最先进的基线相比，我们的模型具有卓越的性能。

    Biological signals, such as electroencephalograms (EEG), play a crucial role in numerous clinical applications, exhibiting diverse data formats and quality profiles. Current deep learning models for biosignals are typically specialized for specific datasets and clinical settings, limiting their broader applicability. Motivated by the success of large language models in text processing, we explore the development of foundational models that are trained from multiple data sources and can be fine-tuned on different downstream biosignal tasks.  To overcome the unique challenges associated with biosignals of various formats, such as mismatched channels, variable sample lengths, and prevalent missing values, we propose a Biosignal Transformer (\method). The proposed \method model can enable cross-data learning with mismatched channels, variable lengths, and missing values by tokenizing diverse biosignals into unified "biosignal sentences". Specifically, we tokenize each channel into fixed-le
    
[^10]: 使用GPT从对话中交互学习分层任务

    Interactive Learning of Hierarchical Tasks from Dialog with GPT. (arXiv:2305.10349v1 [cs.HC])

    [http://arxiv.org/abs/2305.10349](http://arxiv.org/abs/2305.10349)

    该论文中，作者提出一种使用GPT模型作为对话前端，从对话中进行可解释的符号交互式任务学习的方法。通过将交互式对话转换为语义表示，并递归地要求未知步骤的定义，可以获取分层任务知识并在自然的对话环境中进行重复使用。

    

    我们提出了一个系统，利用GPT模型作为对话前端，从对话中进行可解释的符号交互式任务学习。学习到的任务被表示为谓词-参数结构的分层分解，具有作用域变量参数。通过使用GPT模型将交互式对话转换为语义表示，并递归地要求未知步骤的定义，我们展示了分层任务知识可以在自然和不受限制的对话环境中被获取和重复使用。我们将我们的系统与使用更传统的解析器的类似架构进行比较，并展示了我们的系统可以容忍更广泛的语言变异。

    We present a system for interpretable, symbolic, interactive task learning from dialog using a GPT model as a conversational front-end. The learned tasks are represented as hierarchical decompositions of predicate-argument structures with scoped variable arguments. By using a GPT model to convert interactive dialog into a semantic representation, and then recursively asking for definitions of unknown steps, we show that hierarchical task knowledge can be acquired and re-used in a natural and unrestrained conversational environment. We compare our system to a similar architecture using a more conventional parser and show that our system tolerates a much wider variety of linguistic variance.
    
[^11]: MM-Fi：用于多种无线传感的多模态非侵入式4D人体数据集

    MM-Fi: Multi-Modal Non-Intrusive 4D Human Dataset for Versatile Wireless Sensing. (arXiv:2305.10345v1 [eess.SP])

    [http://arxiv.org/abs/2305.10345](http://arxiv.org/abs/2305.10345)

    提出了第一个多模态非侵入式的4D人体数据集MM-Fi，用于多种无线传感任务的支持。该数据集包含40名受试者的超过320K同步帧的五种模态，支持人体姿态估计和动作识别等任务的开展。

    

    在家庭自动化和元宇宙人物模拟等众多应用中，4D人体感知起着至关重要的作用。然而，现有的解决方案主要依赖于摄像头和可穿戴设备，要么侵犯隐私，要么使用不便。为了解决这些问题，无线传感成为潜在的选择，利用激光雷达、毫米波雷达和WiFi信号进行非设备式人体感知。本文提出了MM-Fi，第一个包含27种日常或康复动作类别的多模态非侵入式4D人体数据集，以弥合无线传感和高级人体感知任务之间的差距。MM-Fi由40名人体主体的超过320K同步帧的五个模态组成。提供了各种注释以支持潜在的感知任务，例如人体姿态估计和动作识别。通过对多个任务的模态的感知能力进行比较，进行了大量实验。我们预计这些数据和所提出的评估协议将有助于开发新的无线传感算法，用于多种人体感知任务。

    4D human perception plays an essential role in a myriad of applications, such as home automation and metaverse avatar simulation. However, existing solutions which mainly rely on cameras and wearable devices are either privacy intrusive or inconvenient to use. To address these issues, wireless sensing has emerged as a promising alternative, leveraging LiDAR, mmWave radar, and WiFi signals for device-free human sensing. In this paper, we propose MM-Fi, the first multi-modal non-intrusive 4D human dataset with 27 daily or rehabilitation action categories, to bridge the gap between wireless sensing and high-level human perception tasks. MM-Fi consists of over 320k synchronized frames of five modalities from 40 human subjects. Various annotations are provided to support potential sensing tasks, e.g., human pose estimation and action recognition. Extensive experiments have been conducted to compare the sensing capacity of each or several modalities in terms of multiple tasks. We envision th
    
[^12]: 基于卷积神经网络的自动照片方向检测

    Automatic Photo Orientation Detection with Convolutional Neural Networks. (arXiv:2305.10319v1 [cs.CV])

    [http://arxiv.org/abs/2305.10319](http://arxiv.org/abs/2305.10319)

    本论文使用CNN解决了照片方向检测的问题，并在数据集上显著提高性能。使用Guided Backpropagation获得了CNN检测方向的见解。

    

    本文探讨了利用卷积神经网络(CNN)来解决确定消费者照片正确方向(0°, 90°, 180°和270°)的问题，特别对于模拟照片的数字化非常重要。我们在标准数据集上显著提高了性能，并在更困难的消费者照片大型数据集上进行了测试。我们使用引导反向传播(Guided Backpropagation)来获得关于CNN如何检测照片方向的见解，并解释其错误。

    We apply convolutional neural networks (CNN) to the problem of image orientation detection in the context of determining the correct orientation (from 0, 90, 180, and 270 degrees) of a consumer photo. The problem is especially important for digitazing analog photographs. We substantially improve on the published state of the art in terms of the performance on one of the standard datasets, and test our system on a more difficult large dataset of consumer photos. We use Guided Backpropagation to obtain insights into how our CNN detects photo orientation, and to explain its mistakes.
    
[^13]: LeTI：从文本交互中学习生成

    LeTI: Learning to Generate from Textual Interactions. (arXiv:2305.10314v1 [cs.CL])

    [http://arxiv.org/abs/2305.10314](http://arxiv.org/abs/2305.10314)

    LeTI是一种使用自然语言指令、LM生成的程序和错误消息进行串联迭代微调的技术，可以用于代码生成任务，并且在自然发生的Python指令数据集上表现最先进。

    

    微调预训练语言模型(LM)可以增强模型的能力。先前的技术通过输入输出对（例如指令微调）或用评估输出质量的数字奖励（例如从人类反馈中进行的强化学习）对预训练的LM进行微调。我们探索了LM从文本交互中学习的潜力(LeTI)，这不仅可以通过二进制标签检查其正确性，而且还可以通过文本反馈指出和解释其输出中的错误。我们的研究重点是代码生成任务，其中模型根据自然语言指令生成代码片段。这种设置可以自然且可扩展地获取文本反馈：使用Python解释器进行代码执行时的错误消息和堆栈跟踪。 LeTI使用LM目标对自然语言指令、LM生成的程序和文本反馈进行串联的迭代微调，只有在生成代码无法执行时才提供文本反馈。我们在一个包含58k个自然发生的Python指令，增加了错误消息和堆栈跟踪的数据集上评估了LeTI，在三种不同的评估指标上显著优于强基线模型，并取得了最先进的结果。

    Finetuning pre-trained language models (LMs) enhances the models' capabilities. Prior techniques fine-tune a pre-trained LM on input-output pairs (e.g., instruction fine-tuning), or with numerical rewards that gauge the quality of its outputs (e.g., reinforcement learning from human feedback). We explore LMs' potential to learn from textual interactions (LeTI) that not only check their correctness with binary labels, but also pinpoint and explain errors in their outputs through textual feedback. Our investigation focuses on the code generation task, where the model produces code pieces in response to natural language instructions. This setting invites a natural and scalable way to acquire the textual feedback: the error messages and stack traces from code execution using a Python interpreter. LeTI iteratively fine-tunes the model, using the LM objective, on a concatenation of natural language instructions, LM-generated programs, and textual feedback, which is only provided when the gen
    
[^14]: 深度学习中考虑表格数据数据增强的新思路

    Rethinking Data Augmentation for Tabular Data in Deep Learning. (arXiv:2305.10308v1 [cs.LG])

    [http://arxiv.org/abs/2305.10308](http://arxiv.org/abs/2305.10308)

    本研究提出了一种新的表格数据增强方法“随机连续嵌入”（Random Continuous Embedding，RCE），能够提高 Transformer-based 预训练模型的自监督学习性能，大幅优于现有方法，并使得自监督学习模型能够在监督表格学习中优于树形方法。

    

    表格数据是机器学习中最广泛使用的数据格式。虽然在有监督学习中，树形方法优于深度学习方法；但最近的文献报告称，Transformer-based 预训练模型的自监督学习优于树形方法。在关于表格数据的自监督学习的现有文献中，对比学习是主导方法。然而，由于表格数据的独特结构和高复杂性，表格数据的数据增强一直是困难的。此外，现有方法将模型结构、自监督学习方法和数据增强三个主要组成部分一起提出。因此，以往的研究在综合考虑这些组成部分的情况下进行对比，每个组成部分对实际性能的影响还不清楚。本研究关注数据增强，以解决这些限制。具体地，我们提出了一种新的数据增强方法“随机连续嵌入”（RCE），通过向连续变量注入噪声来生成增强的表格数据。我们在几个基准数据集上评估了我们的方法，并表明 RCE 在使用 Transformer-based 模型进行自监督学习时一致优于现有的数据增强方法。我们还进行筛选研究以显示 RCE 的有效性，并证明 RCE 使 Transformer-based 模型的自监督学习可在监督表格学习中优于树形方法。

    Tabular data is the most widely used data format in machine learning (ML). While tree-based methods outperform DL-based methods in supervised learning, recent literature reports that self-supervised learning with Transformer-based models outperforms tree-based methods. In the existing literature on self-supervised learning for tabular data, contrastive learning is the predominant method. In contrastive learning, data augmentation is important to generate different views. However, data augmentation for tabular data has been difficult due to the unique structure and high complexity of tabular data. In addition, three main components are proposed together in existing methods: model structure, self-supervised learning methods, and data augmentation. Therefore, previous works have compared the performance without comprehensively considering these components, and it is not clear how each component affects the actual performance.  In this study, we focus on data augmentation to address these 
    
[^15]: UniEX：一种基于跨度提取的统一信息抽取的有效高效框架

    UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective. (arXiv:2305.10306v1 [cs.CL])

    [http://arxiv.org/abs/2305.10306](http://arxiv.org/abs/2305.10306)

    UniEX是一种能适用于各种模式格式的信息抽取框架，并能同时解决命名实体识别、关系抽取、事件提取和情感分析等任务，在性能和推理速度上优于其他通用信息抽取模型。

    

    我们提出了一种新的通用信息抽取范式，它与任何模式格式兼容，并适用于一系列信息抽取任务，如命名实体识别、关系抽取、事件提取和情感分析。我们的方法将以文本为基础的信息抽取任务转化为 token-pair 问题，使用一种统一的提取框架 UniEX，将所有提取目标都统一分解为联合跨度检测、分类和关联问题。UniEX 可以同时编码基于模式的提示和文本信息，并使用自动编码器语言模型协同学习预定义信息的广义知识。我们开发了 traffine 注意机制，将包括任务、标签和内部 token 在内的异构因素集成起来，并通过评分矩阵获得提取目标。实验结果表明，UniEX 在 $14$个基准测试数据集上的表现和推理速度都优于基于生成的通用信息抽取模型。

    We propose a new paradigm for universal information extraction (IE) that is compatible with any schema format and applicable to a list of IE tasks, such as named entity recognition, relation extraction, event extraction and sentiment analysis. Our approach converts the text-based IE tasks as the token-pair problem, which uniformly disassembles all extraction targets into joint span detection, classification and association problems with a unified extractive framework, namely UniEX. UniEX can synchronously encode schema-based prompt and textual information, and collaboratively learn the generalized knowledge from pre-defined information using the auto-encoder language models. We develop a traffine attention mechanism to integrate heterogeneous factors including tasks, labels and inside tokens, and obtain the extraction target via a scoring matrix. Experiment results show that UniEX can outperform generative universal IE models in terms of performance and inference-speed on $14$ benchmar
    
[^16]: 针对锂离子电池（用于电动汽车）的剩余寿命和SOH估计

    Estimation of Remaining Useful Life and SOH of Lithium Ion Batteries (For EV Vehicles). (arXiv:2305.10298v1 [cs.LG])

    [http://arxiv.org/abs/2305.10298](http://arxiv.org/abs/2305.10298)

    本文介绍不同方法估计锂电池剩余寿命，提出了一种利用机器学习技术的新方法，可以利用电池性能参数准确预测电池的寿命。

    

    锂离子电池被广泛应用于各种领域，包括便携式电子设备、电动汽车和可再生能源存储系统。准确估计这些电池的剩余寿命对于确保其最佳性能，预防意外故障以及降低维护成本至关重要。本文对现有的估计锂离子电池剩余寿命的方法进行了综合评估，包括基于数据驱动方法、基于物理模型以及混合方法。我们还提出了一种基于机器学习技术的新方法，可以准确预测锂离子电池的剩余寿命。我们的方法利用电池性能参数，包括电压、电流和温度，训练了一个预测模型，可以准确地估计电池的剩余寿命。我们在一个锂离子电池周期的数据集上评估了我们的方法的性能。

    Lithium-ion batteries are widely used in various applications, including portable electronic devices, electric vehicles, and renewable energy storage systems. Accurately estimating the remaining useful life of these batteries is crucial for ensuring their optimal performance, preventing unexpected failures, and reducing maintenance costs. In this paper, we present a comprehensive review of the existing approaches for estimating the remaining useful life of lithium-ion batteries, including data-driven methods, physics-based models, and hybrid approaches. We also propose a novel approach based on machine learning techniques for accurately predicting the remaining useful life of lithium-ion batteries. Our approach utilizes various battery performance parameters, including voltage, current, and temperature, to train a predictive model that can accurately estimate the remaining useful life of the battery. We evaluate the performance of our approach on a dataset of lithium-ion battery cycles
    
[^17]: 针对背包约束下的非单调子模最大化的线性查询逼近算法

    Linear Query Approximation Algorithms for Non-monotone Submodular Maximization under Knapsack Constraint. (arXiv:2305.10292v1 [cs.DS])

    [http://arxiv.org/abs/2305.10292](http://arxiv.org/abs/2305.10292)

    该研究提出了针对背包约束下的非单调子模最大化问题的两种新的线性查询逼近算法，其中$\mathsf{DLA}$和$\mathsf{RLA}$都有常数近似比，且查询复杂度为$O(n \log(1/\epsilon)/\epsilon)$。

    

    该研究首次引入了两种具有线性查询复杂度的常数近似算法来解决地面集合大小为$n$，在背包约束下的非单调子模最大化问题，分别为$\mathsf{DLA}$和$\mathsf{RLA}$。其中$\mathsf{DLA}$是提供6+$\epsilon$近似比的确定性算法，而$\mathsf{RLA}$是具有4+$\epsilon$近似比的随机算法。两种算法的查询复杂度均为$O(n \log(1/\epsilon)/\epsilon)$。获取线性查询下的常数近似比的关键思想在于: (1)将地面集合分为两个合适的子集，用线性查询在这些子集中找到近似最优解，(2)将阈值贪心与两个不相交集合或随机选择进程的性质相结合，以提高解的质量。除了理论分析外，我们还使用三个应用程序评估了我们提出的解决方案：收入最大化，图像摘要。

    This work, for the first time, introduces two constant factor approximation algorithms with linear query complexity for non-monotone submodular maximization over a ground set of size $n$ subject to a knapsack constraint, $\mathsf{DLA}$ and $\mathsf{RLA}$. $\mathsf{DLA}$ is a deterministic algorithm that provides an approximation factor of $6+\epsilon$ while $\mathsf{RLA}$ is a randomized algorithm with an approximation factor of $4+\epsilon$. Both run in $O(n \log(1/\epsilon)/\epsilon)$ query complexity. The key idea to obtain a constant approximation ratio with linear query lies in: (1) dividing the ground set into two appropriate subsets to find the near-optimal solution over these subsets with linear queries, and (2) combining a threshold greedy with properties of two disjoint sets or a random selection process to improve solution quality. In addition to the theoretical analysis, we have evaluated our proposed solutions with three applications: Revenue Maximization, Image Summarizat
    
[^18]: 解释任何概念：Segment Anything 满足基于概念的解释

    Explain Any Concept: Segment Anything Meets Concept-Based Explanation. (arXiv:2305.10289v1 [cs.CV])

    [http://arxiv.org/abs/2305.10289](http://arxiv.org/abs/2305.10289)

    本文提出了一种名为Explain Any Concept（EAC）的有效而灵活的基于概念的解释方法，它利用Segment Anything Model（SAM）执行实例分割，并使用相关的概念生成自然语言解释。

    

    可解释性人工智能（XAI）是一个关键课题，可提高对神经网络黑盒内部的人类理解。对于计算机视觉任务，主流基于像素的 XAI 方法通过识别重要像素来解释 DNN 决策，而新兴的基于概念的 XAI 则探索使用概念（例如图像中的头部）来形成解释。然而，像素通常难以解释并对 XAI 方法的不准确性敏感，而在先前的工作中，“概念”要求人工注释或仅限于预定义的概念集。另一方面，受大规模预训练驱动，Segment Anything 模型（SAM）已经被证明是一种强大和可推广的框架，可执行精确而全面的实例分割，实现从给定图像自动准备概念集。本文首次探索使用 SAM 增强基于概念的 XAI。我们提出了一种有效而灵活的基于概念的解释方法，即 Explain Any Concept（EAC），它可以在图像中分割任何内容，并使用相关的概念形成解释。EAC首先使用SAM生成实例分割掩码，然后应用一组概念分类器来识别每个掩码的相关概念，最后使用识别出的概念生成自然语言解释。两个基准数据集上的实验结果表明，与最先进的XAI方法相比，EAC具有显着的效果。

    EXplainable AI (XAI) is an essential topic to improve human understanding of deep neural networks (DNNs) given their black-box internals. For computer vision tasks, mainstream pixel-based XAI methods explain DNN decisions by identifying important pixels, and emerging concept-based XAI explore forming explanations with concepts (e.g., a head in an image). However, pixels are generally hard to interpret and sensitive to the imprecision of XAI methods, whereas "concepts" in prior works require human annotation or are limited to pre-defined concept sets. On the other hand, driven by large-scale pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promotable framework for performing precise and comprehensive instance segmentation, enabling automatic preparation of concept sets from a given image. This paper for the first time explores using SAM to augment concept-based XAI. We offer an effective and flexible concept-based explanation method, namely Explain Any 
    
[^19]: 更鲁棒的自然语言处理系统评估方法：处理基准测试中的缺失得分问题

    Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks. (arXiv:2305.10284v1 [cs.CL])

    [http://arxiv.org/abs/2305.10284](http://arxiv.org/abs/2305.10284)

    本文提出了一种鲁棒的自然语言处理系统评估方法，可以解决基准测试中某些系统的得分缺失问题，并引入了一个规模更大的基准测试。

    

    自然语言处理系统的评估对推动该领域的发展至关重要，但目前的基准测试方法常常假设所有系统在所有任务上都有可用的得分，这并不总是切实可行的。在现实情况下，若干因素（例如运行基线，私有系统，计算限制或不完整的数据）可能会阻止某些系统在整个任务上进行评估。本文正式阐述了自然语言处理研究中的一个现有问题：如何在一些系统的任务得分缺失时进行基准测试，并提出了一种新的解决方法。我们的方法利用兼容的部分排名方法来填补缺失的数据，然后使用Borda计数方法进行聚合。它包括两个特定于任务级得分或实例级得分可用的场景的细化。我们还引入了一个扩展基准测试，其中包含超过1.31亿个得分，比现有基准测试大一个数量级。我们验证了我们的方法并证明其有效性。

    The evaluation of natural language processing (NLP) systems is crucial for advancing the field, but current benchmarking approaches often assume that all systems have scores available for all tasks, which is not always practical. In reality, several factors such as the cost of running baseline, private systems, computational limitations, or incomplete data may prevent some systems from being evaluated on entire tasks. This paper formalize an existing problem in NLP research: benchmarking when some systems scores are missing on the task, and proposes a novel approach to address it. Our method utilizes a compatible partial ranking approach to impute missing data, which is then aggregated using the Borda count method. It includes two refinements designed specifically for scenarios where either task-level or instance-level scores are available. We also introduce an extended benchmark, which contains over 131 million scores, an order of magnitude larger than existing benchmarks. We validate
    
[^20]: 基于学习度量的大规模包裹操作

    Large-Scale Package Manipulation via Learned Metrics of Pick Success. (arXiv:2305.10272v1 [cs.RO])

    [http://arxiv.org/abs/2305.10272](http://arxiv.org/abs/2305.10272)

    本文讨论了基于学习度量的大规模包裹操作，通过训练拾取成功预测器和学习拾取质量度量，实现了能够大规模部署的强力抓握策略。

    

    自动化仓储操作可以降低物流成本，最终降低消费品价格，提高交货速度，并增强对劳动力波动的抵抗能力。近年来，自动化重复任务的兴趣增加，但大多数是在受控环境中进行的。从杂乱的堆堆中挑选物品等任务直到最近才变得足够强大，可以在最小人工干预下进行大规模部署。本文展示了亚马逊机器人的Robot Induction（Robin）群的大规模包裹操作，该群利用在实际生产数据上训练的拾取成功预测器。具体而言，该系统在超过394K个拾取上进行了训练。它用于把每天高达5百万个包裹进行了分离，本文的评估期间操作了超过2亿个包裹。开发的学习拾取质量度量实时排名各种拾取替代方案，并采用高成功率的强力抓握策略。

    Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to workforce fluctuations. The past few years have seen increased interest in automating such repeated tasks but mostly in controlled settings. Tasks such as picking objects from unstructured, cluttered piles have only recently become robust enough for large-scale deployment with minimal human intervention.  This paper demonstrates a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which utilizes a pick success predictor trained on real production data. Specifically, the system was trained on over 394K picks. It is used for singulating up to 5~million packages per day and has manipulated over 200~million packages during this paper's evaluation period.  The developed learned pick quality measure ranks various pick alternatives in real-time and p
    
[^21]: 基于聚类的方法，利用局部和全局特征提高社交网络中的链接预测

    Improving Link Prediction in Social Networks Using Local and Global Features: A Clustering-based Approach. (arXiv:2305.10257v1 [cs.AI])

    [http://arxiv.org/abs/2305.10257](http://arxiv.org/abs/2305.10257)

    该论文提出了一种基于聚类的方法，结合了相似性方法和学习方法来提高社交网络中的链接预测。

    

    链接预测问题在社交网络分析、生物信息学实验、交通网络、刑事调查等诸多领域中日益突出。现有的文献缺乏利用每种方法的优势并将它们集成以实现更高效的方法的方法。为了解决这个问题，我们提出了一种基于第一和第二群体方法结合的方法。我们的两阶段开发方法首先确定了与节点之间关系相关的新特征。

    Link prediction problem has increasingly become prominent in many domains such as social network analyses, bioinformatics experiments, transportation networks, criminal investigations and so forth. A variety of techniques has been developed for link prediction problem, categorized into 1) similarity based approaches which study a set of features to extract similar nodes; 2) learning based approaches which extract patterns from the input data; 3) probabilistic statistical approaches which optimize a set of parameters to establish a model which can best compute formation probability. However, existing literatures lack approaches which utilize strength of each approach by integrating them to achieve a much more productive one. To tackle the link prediction problem, we propose an approach based on the combination of first and second group methods; the existing studied works use just one of these categories. Our two-phase developed method firstly determines new features related to the posit
    
[^22]: 面向家禽科学的 SAM 模型应用研究

    SAM for Poultry Science. (arXiv:2305.10254v1 [cs.CV])

    [http://arxiv.org/abs/2305.10254](http://arxiv.org/abs/2305.10254)

    本研究探究了 SAM 模型在禽业领域中的应用潜力，结果表明 SAM 模型性能优于其他模型，可以应用于基于部分的鸡类分割任务和红外热像的使用，并用于鸡类跟踪任务中。

    

    近年来，人工智能领域在建立大规模基础模型方面取得了显著进展。其中，Meta AI Research 推出的“Segment Anything Model”(SAM)作为物体分割任务的突破性解决方案而脱颖而出。虽然 SAM 在各种农业应用中表现出了成功的应用，但其在禽业，尤其是在无笼鸡的情况下的潜力仍未充分发掘。本研究旨在评估 SAM 在代表性的鸡类分割任务上的零样本分割性能，包括基于部分的分割和红外热像的使用，并试图探索使用 SAM 作为分割工具进行鸡类跟踪任务。研究结果表明，相比于 SegFormer 和 SETR，在整体和基于部分的鸡类分割中，SAM 的性能优于其他模型。基于 SAM 的对象跟踪也提供了有价值的行为数据。

    In recent years, the agricultural industry has witnessed significant advancements in artificial intelligence (AI), particularly with the development of large-scale foundational models. Among these foundation models, the Segment Anything Model (SAM), introduced by Meta AI Research, stands out as a groundbreaking solution for object segmentation tasks. While SAM has shown success in various agricultural applications, its potential in the poultry industry, specifically in the context of cage-free hens, remains relatively unexplored. This study aims to assess the zero-shot segmentation performance of SAM on representative chicken segmentation tasks, including part-based segmentation and the use of infrared thermal images, and to explore chicken-tracking tasks by using SAM as a segmentation tool. The results demonstrate SAM's superior performance compared to SegFormer and SETR in both whole and part-based chicken segmentation. SAM-based object tracking also provides valuable data on the beh
    
[^23]: MemoryBank: 用长期记忆增强大型语言模型

    MemoryBank: Enhancing Large Language Models with Long-Term Memory. (arXiv:2305.10250v1 [cs.CL])

    [http://arxiv.org/abs/2305.10250](http://arxiv.org/abs/2305.10250)

    MemoryBank 提出了一种新型内存机制，旨在为大型语言模型提供类人的长期记忆。它可以召唤相关记忆，通过持续的记忆更新不断进化，通过合成过去的互动信息理解并适应用户个性。

    

    大型语言模型的革命性进展极大地改变了我们与人工智能系统的互动方式。尽管如此，其中一个明显的不足之处是这些模型缺乏长期记忆机制。这在需要持续互动的情况下尤为明显，例如个人伴侣系统和心理咨询。因此，我们提出了MemoryBank，这是一种专为LLM量身定制的新型内存机制。MemoryBank可以召唤相关记忆，通过持续的记忆更新不断进化，通过合成过去的互动信息理解并适应用户个性。为了模仿人类行为并有选择地保存记忆，MemoryBank采用了受Ebbinghaus遗忘曲线理论启发的记忆更新机制，这样人工智能可以根据时间和记忆的相对重要性来遗忘和加强记忆，从而为LLM提供类似于人类的长期记忆。

    Revolutionary advancements in Large Language Models have drastically reshaped our interactions with artificial intelligence systems. Despite this, a notable hindrance remains-the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems and psychological counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user personality by synthesizing information from past interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory, which permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a hum
    
[^24]: 评估LLM的隐藏风险：关于鲁棒性、一致性和可信性的实证研究

    Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility. (arXiv:2305.10235v1 [cs.LG])

    [http://arxiv.org/abs/2305.10235](http://arxiv.org/abs/2305.10235)

    本研究是一项关于大型语言模型方面的实证研究，对主流语言模型进行了大量查询和分析，结果发现这些模型存在着鲁棒性、一致性和可信性方面的潜在风险。

    

    大型语言模型（LLMs）的普及对于许多领域产生了重大影响，特别是在其开放式环境（如API、开源模型和插件）中。然而，随着LLMs的广泛部署，缺乏全面讨论和分析潜在风险的研究。因此，我们进行了一项初步但开创性的研究，涵盖了LLMs系统的鲁棒性、一致性和可信性。我们提出了一个自动化工作流程来处理大量查询/响应。总体而言，我们对包括ChatGPT、LLaMA和OPT在内的主流LLMs进行了100多万个查询。我们的工作流核心包括数据原语，随后是自动解释器，评估这些LLMs在不同的对抗性度量系统下的表现。结果，我们得出了几个、也许是不幸的结论，这些结论相当不同

    The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite unco
    
[^25]: 探索对比学习中的归纳偏差：从聚类角度出发

    Exploring Inductive Biases in Contrastive Learning: A Clustering Perspective. (arXiv:2305.10229v1 [cs.LG])

    [http://arxiv.org/abs/2305.10229](http://arxiv.org/abs/2305.10229)

    本文比较了对比学习和监督学习方法形成的簇，揭示了对比学习可以生成具有局部密度但无全局密度的簇，而监督学习创建具有局部和全局密度的簇。同时，作者提出了使用图卷积网络分类器作为处理局部密集簇的线性分类器的替代方法，并利用t-SNE可视化证明了对比和监督学习方法产生的特征之间的差异。

    

    本文研究对比学习方法和监督学习方法之间数据组织的差异，重点关注局部密集簇的概念。我们引入一个新的度量指标，相对局部密度（RLD），用于定量测量簇内的局部密度。我们提供了视觉示例，以突出局部密集簇和全局密集簇之间的区别。通过对比对比学习和监督学习形成的簇，我们发现对比学习生成具有局部密度而无全局密度的簇，而监督学习创建具有局部和全局密度的簇。我们进一步探讨了使用图卷积网络（GCN）分类器作为处理局部密集簇的线性分类器的替代方法。最后，我们利用t-SNE可视化来证明对比和监督学习方法产生的特征之间的差异。我们提出了未来的研究方向来结束本文。

    This paper investigates the differences in data organization between contrastive and supervised learning methods, focusing on the concept of locally dense clusters. We introduce a novel metric, Relative Local Density (RLD), to quantitatively measure local density within clusters. Visual examples are provided to highlight the distinctions between locally dense clusters and globally dense ones. By comparing the clusters formed by contrastive and supervised learning, we reveal that contrastive learning generates locally dense clusters without global density, while supervised learning creates clusters with both local and global density. We further explore the use of a Graph Convolutional Network (GCN) classifier as an alternative to linear classifiers for handling locally dense clusters. Finally, we utilize t-SNE visualizations to substantiate the differences between the features generated by contrastive and supervised learning methods. We conclude by proposing future research directions, 
    
[^26]: 基于可分性和离散度比的SVM正则化参数、核函数和核参数选择方法

    Separability and Scatteredness (S&S) Ratio-Based Efficient SVM Regularization Parameter, Kernel, and Kernel Parameter Selection. (arXiv:2305.10219v1 [stat.ML])

    [http://arxiv.org/abs/2305.10219](http://arxiv.org/abs/2305.10219)

    该文通过分析数据的可分性和离散度，提出了一种基于S&S比的有效SVM正则化参数、核函数和核参数选择方法，表现较传统方法更优。

    

    支持向量机（SVM）是一种具有广泛应用的鲁棒机器学习算法，可用于分类、回归和异常值检测。SVM需要调整正则化参数（RP）来控制模型容量和泛化性能。传统上，通过交叉验证（CV）过程对一系列备选RP进行比较以找到最佳RP。此外，对于非线性可分数据，SVM使用核函数，在核函数的网格中选择一组具有一组参数的核函数。RP和核网格的最佳选择是通过CV的网格搜索获得的。通过随机分析正则化参数的行为，本文展示了SVM性能可以建模为数据的可分性和离散度（S&S）的函数。可分性是类别之间距离的度量，离散度是数据点的传播比率。特别地，对于铰链损失成本函数，S&S比可以有效地估计最优RP。此外，本文提出了一种基于S&S比的高效选择核函数及其参数方法。在各种基准数据集上比较了所提出方法与传统方法的性能，结果表明，所提出方法具有更少的需要调整的超参数且性能优异或可比。

    Support Vector Machine (SVM) is a robust machine learning algorithm with broad applications in classification, regression, and outlier detection. SVM requires tuning the regularization parameter (RP) which controls the model capacity and the generalization performance. Conventionally, the optimum RP is found by comparison of a range of values through the Cross-Validation (CV) procedure. In addition, for non-linearly separable data, the SVM uses kernels where a set of kernels, each with a set of parameters, denoted as a grid of kernels, are considered. The optimal choice of RP and the grid of kernels is through the grid-search of CV. By stochastically analyzing the behavior of the regularization parameter, this work shows that the SVM performance can be modeled as a function of separability and scatteredness (S&S) of the data. Separability is a measure of the distance between classes, and scatteredness is the ratio of the spread of data points. In particular, for the hinge loss cost fun
    
[^27]: 护盾式表示：通过迭代基于梯度的投影保护敏感属性

    Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection. (arXiv:2305.10204v1 [cs.CL])

    [http://arxiv.org/abs/2305.10204](http://arxiv.org/abs/2305.10204)

    本文提出了一种名为迭代梯度基础投影（IGBP）的新方法，用于从神经表示中删除非线性编码的概念，以减轻模型的社会偏见。该方法通过迭代训练神经分类器来预测某个敏感属性，然后将表示投影到一个超平面上，使得分类器对目标属性变得无意识。实验证明，该方法在消除敏感属性方面是有效的，并且对下游任务的准确性影响很小。

    

    自然语言处理模型倾向于学习和编码数据中存在的社会偏见。解决此类偏差的一种流行方法是消除模型表示中编码的信息。然而，当前的方法仅限于删除线性编码的信息。在这项工作中，我们提出了一种名为迭代梯度基础投影（IGBP）的新方法，用于从神经表示中删除非线性编码的概念。我们的方法包括通过迭代训练神经分类器来预测我们要消除的特定属性，然后将表示投影到一个超平面上，使得分类器对目标属性变得无意识。我们评估了我们的方法在消除性别和种族信息作为敏感属性的任务上的有效性。我们的结果表明，IGBP通过内在和外在评估在减轻偏见方面是有效的，并且对下游任务的准确性影响很小。

    Natural language processing models tend to learn and encode social biases present in the data. One popular approach for addressing such biases is to eliminate encoded information from the model's representations. However, current methods are restricted to removing only linearly encoded information. In this work, we propose Iterative Gradient-Based Projection (IGBP), a novel method for removing non-linear encoded concepts from neural representations. Our method consists of iteratively training neural classifiers to predict a particular attribute we seek to eliminate, followed by a projection of the representation on a hypersurface, such that the classifiers become oblivious to the target attribute. We evaluate the effectiveness of our method on the task of removing gender and race information as sensitive attributes. Our results demonstrate that IGBP is effective in mitigating bias through intrinsic and extrinsic evaluations, with minimal impact on downstream task accuracy.
    
[^28]: 人们交谈，AI倾听：电子病历中污名化语言对AI判断的影响

    People Talking and AI Listening: How Stigmatizing Language in EHR Notes Affect AI Performance. (arXiv:2305.10201v1 [cs.AI])

    [http://arxiv.org/abs/2305.10201](http://arxiv.org/abs/2305.10201)

    本文研究了电子病历中污名化语言对基于Transformer的深度学习模型和可解释AI(XAI)技术进行死亡预测的影响。发现临床医生所写的SL会对AI性能表现不利，尤其是在黑人患者中表现更为明显，强调了理解偏见对下游AI性能的影响的重要性，以开发更具公平和正义的医疗系统。

    

    电子病历(EHRs)是期望中的人工智能(AI)-驱动的医疗转型的重要数据来源。然而，反映在EHR笔记中的临床医师偏见可能会导致AI模型继承并放大这些偏见，从而不断加剧健康上的不平等。本研究调查了EHR笔记中污名化语言(SL)对基于Transformer的深度学习模型和可解释AI(XAI)技术进行死亡预测的影响。我们的研究发现，临床医生所写的SL不利于AI的性能表现，尤其是在黑人患者中表现更为明显，突出了SL作为AI模型发展中种族差异的一种来源。为探索一种操作上有效的缓解SL影响的方法，我们研究了临床医生协作网络中SL生成的模式，发现中央医生对AI模型中的种族差异具有更强的影响力。我们发现，删除中央临床医生撰写的SL是相对于随机选择临床医生而言，缓解SL对AI性能影响的更为有效的策略。我们的研究强调了理解反映在EHR笔记中的临床医师偏见对下游AI性能的影响的重要性，以开发更具公平和正义的医疗系统。

    Electronic health records (EHRs) serve as an essential data source for the envisioned artificial intelligence (AI)-driven transformation in healthcare. However, clinician biases reflected in EHR notes can lead to AI models inheriting and amplifying these biases, perpetuating health disparities. This study investigates the impact of stigmatizing language (SL) in EHR notes on mortality prediction using a Transformer-based deep learning model and explainable AI (XAI) techniques. Our findings demonstrate that SL written by clinicians adversely affects AI performance, particularly so for black patients, highlighting SL as a source of racial disparity in AI model development. To explore an operationally efficient way to mitigate SL's impact, we investigate patterns in the generation of SL through a clinicians' collaborative network, identifying central clinicians as having a stronger impact on racial disparity in the AI model. We find that removing SL written by central clinicians is a more 
    
[^29]: 深度快速的近似无序透明度

    Deep and Fast Approximate Order Independent Transparency. (arXiv:2305.10197v1 [cs.GR])

    [http://arxiv.org/abs/2305.10197](http://arxiv.org/abs/2305.10197)

    本文提出了一种深度学习方法有效地计算无序透明度，具备快速性、占用内存少以及更为准确。该方法适用于所有场景，无需超前设置，且在运用普及GPU的所有平台上都有便捷的移植性。

    

    我们提出了一种机器学习方法，用于有效计算无序透明度(OIT)。我们的方法快速、需要极少的内存(仅取决于屏幕分辨率而不是三角形或透明层数量)，与先前的近似方法相比更精确，适用于每个场景而无需设置，并且可在运行甚至使用商品GPU的所有平台上移植。我们需要渲染通行证来提取所有特征，随后使用预训练的神经网络预测整个OIT像素颜色。我们提供了比较性的实验评估和所有方法的着色器源代码以复制实验。

    We present a machine learning approach for efficiently computing order independent transparency (OIT). Our method is fast, requires a small constant amount of memory (depends only on the screen resolution and not on the number of triangles or transparent layers), is more accurate as compared to previous approximate methods, works for every scene without setup and is portable to all platforms running even with commodity GPUs. Our method requires a rendering pass to extract all features that are subsequently used to predict the overall OIT pixel color with a pre-trained neural network. We provide a comparative experimental evaluation and shader source code of all methods for reproduction of the experiments.
    
[^30]: 零代词翻译综述

    A Survey on Zero Pronoun Translation. (arXiv:2305.10196v1 [cs.CL])

    [http://arxiv.org/abs/2305.10196](http://arxiv.org/abs/2305.10196)

    本文总结了零代词翻译（ZPT）领域神经网络全面推广后的重要工作，发现大型语言模型、多任务或迁移学习都可以实现ZPT的性能提升。

    

    零代词（ZP）通常在类似中文、匈牙利语和印地语这样的丢省略，而在非丢失省份诸如英语中，应当进行回应。这一现象在机器翻译（MT）领域中得到了广泛的研究，因为它很难确定代词的正确先行词，这是MT系统面临的重要挑战。本文总结了神经网络全面推展之后在零代词翻译（ZPT）方面所做的重要工作，以便研究人员了解当前状态和未来方向。我们根据演变、数据集、方法和评估提供了一份文献组织形式。此外，我们还比较和分析了在不同基准测试上的竞争模型和评估指标。我们挖掘了一些有益的发现，例如：1）ZPT符合大型语言模型的发展趋势；2）数据限制会在不同语言和领域中产生学习偏差；3）通过多任务或迁移学习可以实现性能提升。

    Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g. Chinese, Hungarian, and Hindi), but should be recalled in non-pro-drop languages (e.g. English). This phenomenon has been studied extensively in machine translation (MT), as it poses a significant challenge for MT systems due to the difficulty in determining the correct antecedent for the pronoun. This survey paper highlights the major works that have been undertaken in zero pronoun translation (ZPT) after the neural revolution, so that researchers can recognise the current state and future directions of this field. We provide an organisation of the literature based on evolution, dataset, method and evaluation. In addition, we compare and analyze competing models and evaluation metrics on different benchmarks. We uncover a number of insightful findings such as: 1) ZPT is in line with the development trend of large language model; 2) data limitation causes learning bias in languages and domains; 3) performance improv
    
[^31]: 结构化传递博弈中的实用推理

    Pragmatic Reasoning in Structured Signaling Games. (arXiv:2305.10167v1 [cs.AI])

    [http://arxiv.org/abs/2305.10167](http://arxiv.org/abs/2305.10167)

    本文介绍了一个结构化传递博弈和理性言语行为框架的变体sRSA，应用于结构化领域中的实用推理问题。在颜色领域中，我们的研究表明采用sRSA的代理比传统RSA和仅基于强化学习的代理更接近于信息理论界限。

    

    本文引入了一种带有相似性结构的结构化传递博弈，并提出了一个理性言语行为框架的变体，称为结构化理性言语行为（sRSA），用于解决结构化领域的实用推理问题。我们探索了在颜色领域中采用sRSA的理性智能代理的行为，证明了采用World Color Survey得出的语义表示的结构化代理比传统RSA和仅基于强化学习的代理更接近于信息理论界限，且经过1或2次递归的训练就能够达到效率极限。此外，我们还研究了实用推理和多智能体强化学习框架中的学习过程和相互作用。结果表明，采用sRSA的人工智能代理发展出的通信策略更接近于信息理论界限。

    In this work we introduce a structured signaling game, an extension of the classical signaling game with a similarity structure between meanings in the context, along with a variant of the Rational Speech Act (RSA) framework which we call structured-RSA (sRSA) for pragmatic reasoning in structured domains. We explore the behavior of the sRSA in the domain of color and show that pragmatic agents using sRSA on top of semantic representations, derived from the World Color Survey, attain efficiency very close to the information theoretic limit after only 1 or 2 levels of recursion. We also explore the interaction between pragmatic reasoning and learning in multi-agent reinforcement learning framework. Our results illustrate that artificial agents using sRSA develop communication closer to the information theoretic frontier compared to agents using RSA and just reinforcement learning. We also find that the ambiguity of the semantic representation increases as the pragmatic agents are allowe
    
[^32]: 基于知识增强的生成预训练模型在中国医学执业医师资格考试上的应用研究

    Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model. (arXiv:2305.10163v1 [cs.CL])

    [http://arxiv.org/abs/2305.10163](http://arxiv.org/abs/2305.10163)

    本研究通过在ChatGPT中集成医学领域知识和启用少样本学习的新方法，在中国国家医学执业医师资格考试中取得成功，这为建立在自然语言处理技术和医学领域知识的创新应用提供了可能。

    

    生成式预训练模型（GPT），如ChatGPT，在各种自然语言处理任务中展现出了出色的性能。尽管ChatGPT已被整合到各个领域的工作流中以提高效率，但其微调过程的灵活性不足，阻碍了其在需要广泛领域专业知识和语义知识的领域，如医疗保健，的应用。在本文中，我们评估了ChatGPT在中国国家医学执业医师资格考试（CNMLE）中的表现，并提出了一种新的方法来改进ChatGPT，即从两个方面集成医学领域知识和启用少样本学习。通过使用简单但有效的检索方法，将医学背景知识提取为语义指令来指导ChatGPT的推断。类似地，相关的医疗问题被识别并作为演示输入给ChatGPT。实验结果表明，直接应用ChatGPT无法在CNMLE上获得合格分数（51分），只有基于知识增强训练的模型成功通过考试。

    Generative Pre-Training (GPT) models like ChatGPT have demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. Although ChatGPT has been integrated into the overall workflow to boost efficiency in many domains, the lack of flexibility in the finetuning process hinders its applications in areas that demand extensive domain expertise and semantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on the China National Medical Licensing Examination (CNMLE) and propose a novel approach to improve ChatGPT from two perspectives: integrating medical domain knowledge and enabling few-shot learning. By using a simple but effective retrieval method, medical background knowledge is extracted as semantic instructions to guide the inference of ChatGPT. Similarly, relevant medical questions are identified and fed as demonstrations to ChatGPT. Experimental results show that directly applying ChatGPT fails to qualify the CNMLE at a score of 51 (i.e., onl
    
[^33]: 基于分层通讯多代理PPO的集合大型风电场多变量功率输出控制

    Collective Large-scale Wind Farm Multivariate Power Output Control Based on Hierarchical Communication Multi-Agent Proximal Policy Optimization. (arXiv:2305.10161v1 [eess.SY])

    [http://arxiv.org/abs/2305.10161](http://arxiv.org/abs/2305.10161)

    本文提出了一种基于通讯的多代理深度强化学习大型风电场多变量控制方法，将风电场分成多个聚合器，并通过分层通讯进行信息交换来最大化风电场功率输出，并在仿真中证明了其有效性。

    

    风电成为全球越来越重要的可再生能源，但由于风电场的高系统复杂性，风电场功率控制面临巨大挑战。本文提出了基于通讯的多代理深度强化学习大型风电场多变量控制方法来处理这个问题，并最大化风电场功率输出。提出了一种风电场多变量功率模型来研究风力发电机对功率的影响。该多变量模型包括轴向感应因子、偏航角和倾斜角控制变量。提出了分层通讯多代理PPO算法来协调多变量大型风电场的连续控制。将大型风电场划分为多个风力发电机聚合器，并通过分层通讯进行信息交换以最大化风电场功率输出。仿真结果表明，所提出的HCMAPPO算法可以有效优化风电场功率输出，并优于现有的其他控制方法。

    Wind power is becoming an increasingly important source of renewable energy worldwide. However, wind farm power control faces significant challenges due to the high system complexity inherent in these farms. A novel communication-based multi-agent deep reinforcement learning large-scale wind farm multivariate control is proposed to handle this challenge and maximize power output. A wind farm multivariate power model is proposed to study the influence of wind turbines (WTs) wake on power. The multivariate model includes axial induction factor, yaw angle, and tilt angle controllable variables. The hierarchical communication multi-agent proximal policy optimization (HCMAPPO) algorithm is proposed to coordinate the multivariate large-scale wind farm continuous controls. The large-scale wind farm is divided into multiple wind turbine aggregators (WTAs), and neighboring WTAs can exchange information through hierarchical communication to maximize the wind farm power output. Simulation results
    
[^34]: 不要用明文上传测试数据：减轻数据外泄对于评估基准的持续影响的实用策略

    Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks. (arXiv:2305.10160v1 [cs.CL])

    [http://arxiv.org/abs/2305.10160](http://arxiv.org/abs/2305.10160)

    提出了三个适用策略：（1）公钥加密发布测试数据，仅允许特定派生发布；（2）对于API持有方，要求训练排除控制，保护测试数据，不停止评估直到达到要求；（3）如果测试数据来自互联网文本，需避免某些结果的使用。

    

    随着预训练模型在自动爬网资料库的大规模应用，数据外泄变得常见且部分难以应对。对于那些不会公开训练数据的模型，其数据成为了商业机密，即使在公开模型中，确定特定测试实例是否被泄露也不是一件容易的事情。本文提出三个可行的策略：（1）使用公钥加密发布的测试数据并限制派生发布的许可；（2）要求持有API训练数据的公司采用训练排除控制，并拒绝评估，直到训练排除控制无误为止；（3）如果测试数据来自互联网文本，那么需避免在网络搜索中出现包含正确提取部分的数据。

    Data contamination has become especially prevalent and challenging with the rise of models pretrained on very large, automatically-crawled corpora. For closed models, the training data becomes a trade secret, and even for open models, it is not trivial to ascertain whether a particular test instance has been compromised. Strategies such as live leaderboards with hidden answers, or using test data which is guaranteed to be unseen, are expensive and become fragile with time. Assuming that all relevant actors value clean test data and will cooperate to mitigate data contamination, what can be done? We propose three strategies that can make a difference: (1) Test data made public should be encrypted with a public key and licensed to disallow derivative distribution; (2) demand training exclusion controls from closed API holders, and protect your test data by refusing to evaluate until demands are met; (3) in case of test data based on internet text, avoid data which appears with its soluti
    
[^35]: 阅读过程中对小说人物个性的理解

    Personality Understanding of Fictional Characters during Book Reading. (arXiv:2305.10156v1 [cs.CL])

    [http://arxiv.org/abs/2305.10156](http://arxiv.org/abs/2305.10156)

    本文提出了一个NLP领域内尚未研究的问题：情景和细致地理解小说人物个性，并提供了第一个标记数据集PersoNet来解决这个问题。

    

    理解小说人物个性是阅读故事的关键。随着读者与故事的互动，他们对一个人物的理解会根据新的事件和信息而演变；并且可以感知到多个精细的个性方面。这导致了一个自然的问题：情境和精细的个性理解。这个问题在NLP领域中没有得到研究，主要是由于缺乏模仿阅读过程的适当数据集。我们提供了第一个标记数据集PersoNet来解决这个问题。我们的新型注释策略涉及用在线阅读应用程序的用户笔记作为原始书籍的代理进行注释。实验和人体研究表明，我们的数据集构建既有效又准确；我们的任务在很大程度上依赖于长期的上下文以实现对机器和人类的准确预测。数据集可在https://github.com/Gorov/personet_acl23获得。

    Comprehending characters' personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of situated and fine-grained personality understanding. The problem has not been studied in the NLP field, primarily due to the lack of appropriate datasets mimicking the process of book reading. We present the first labeled dataset PersoNet for this problem. Our novel annotation strategy involves annotating user notes from online reading apps as a proxy for the original books. Experiments and human studies indicate that our dataset construction is both efficient and accurate; and our task heavily relies on long-term context to achieve accurate predictions for both machines and humans. The dataset is available at https://github.com/Gorov/personet_acl23.
    
[^36]: 选择性遗忘：深度生成模型中的持续学习方法

    Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models. (arXiv:2305.10120v1 [cs.LG])

    [http://arxiv.org/abs/2305.10120](http://arxiv.org/abs/2305.10120)

    针对大规模文本到图像模型可能被误用生成有害内容的问题，该论文提出了一种选择性遗忘方法，即持续学习方法，可在深度生成模型中实现可控的遗忘，用户可指定消除哪些概念。

    

    近年来，大规模文本到图像模型的广泛使用引发了人们对这些模型可能被误用生成有害、误导或不当内容的担忧。受此问题的启发，我们提出了一种受持续学习启发的技术，用于有选择性地遗忘预训练的深度生成模型中的概念。我们的方法称为选择性遗忘，可以实现可控的遗忘，用户可以指定该如何遗忘一个概念。选择性遗忘可应用于变分似然模型，涵盖了各种流行的深度生成框架，包括变分自编码器和大规模文本到图像扩散模型。不同模型上的实验证明，我们的方法可以诱导遗忘各种概念，从标准数据集中的整个类别到文本到图像模型中的名人和裸体提示。我们的代码可公开访问，网址为https://github.com/clear-nus/selective-amnesia。

    The recent proliferation of large-scale text-to-image models has led to growing concerns that such models may be misused to generate harmful, misleading, and inappropriate content. Motivated by this issue, we derive a technique inspired by continual learning to selectively forget concepts in pretrained deep generative models. Our method, dubbed Selective Amnesia, enables controllable forgetting where a user can specify how a concept should be forgotten. Selective Amnesia can be applied to conditional variational likelihood models, which encompass a variety of popular deep generative frameworks, including variational autoencoders and large-scale text-to-image diffusion models. Experiments across different models demonstrate that our approach induces forgetting on a variety of concepts, from entire classes in standard datasets to celebrity and nudity prompts in text-to-image models. Our code is publicly available at https://github.com/clear-nus/selective-amnesia.
    
[^37]: 架起桥梁：通过后处理技术增强合成数据的实用性

    Bridging the Gap: Enhancing the Utility of Synthetic Data via Post-Processing Techniques. (arXiv:2305.10118v1 [cs.CV])

    [http://arxiv.org/abs/2305.10118](http://arxiv.org/abs/2305.10118)

    本文介绍了一种利用生成对抗网络生成合成数据集，并通过三种新颖的后处理技术改进合成数据集质量和多样性的方法。作者称其为Gap Filler (GaFi)流程并在真实图像上进行评估。

    

    获取和注释用于训练深度学习模型的合适数据集是具有挑战性的。生成模型已经成为一种有前途的解决方案，可生成替代或增强现实世界数据的合成数据集。尽管如此，合成数据的有效性受到其不能完全捕捉现实世界数据的复杂性和多样性的限制。为了解决这个问题，我们探索使用生成对抗网络生成用于训练分类器的合成数据集，随后在真实图像上进行评估。为了改进合成数据集的质量和多样性，我们提出了三种新颖的后处理技术：动态样本过滤，动态数据集回收和扩展技巧。此外，我们引入了一种名为“ Gap Filler (GaFi)”的流程，在最佳和协调的方式下应用这些技术，以最大程度地提高分类的准确性。

    Acquiring and annotating suitable datasets for training deep learning models is challenging. This often results in tedious and time-consuming efforts that can hinder research progress. However, generative models have emerged as a promising solution for generating synthetic datasets that can replace or augment real-world data. Despite this, the effectiveness of synthetic data is limited by their inability to fully capture the complexity and diversity of real-world data. To address this issue, we explore the use of Generative Adversarial Networks to generate synthetic datasets for training classifiers that are subsequently evaluated on real-world images. To improve the quality and diversity of the synthetic dataset, we propose three novel post-processing techniques: Dynamic Sample Filtering, Dynamic Dataset Recycle, and Expansion Trick. In addition, we introduce a pipeline called Gap Filler (GaFi), which applies these techniques in an optimal and coordinated manner to maximise classifica
    
[^38]: 基于神经符号的AI用于电气控制面板的合规检查

    Neuro-Symbolic AI for Compliance Checking of Electrical Control Panels. (arXiv:2305.10113v1 [cs.AI])

    [http://arxiv.org/abs/2305.10113](http://arxiv.org/abs/2305.10113)

    本文提出了一种基于神经符号的AI方法，结合深度学习技术和答案集编程来自动化电气控制面板的合规性验证。该方法可以在只有非常有限的训练数据下识别可能存在的异常和错误，实验结果表明该方法具有很好的效果。

    

    人工智能在支持和改善智能制造和工业4.0方面发挥着重要作用，通过使领域专家手动执行的不同类型的任务自动化。特别是，评估产品与相对原理图的符合性是一项耗时且容易出错的过程。在本文中，我们针对特定的工业场景解决此问题。具体而言，我们定义了一种神经符号方法来自动化电气控制面板的合规性验证。我们的方法基于深度学习技术和答案集编程（ASP）的组合，即使只有非常有限的训练数据，也可以识别出最终产品中可能存在的异常和错误。通过意大利一家从事电气控制面板生产的公司提供的实际测试案例进行的实验表明了所提出方法的有效性。

    Artificial Intelligence plays a main role in supporting and improving smart manufacturing and Industry 4.0, by enabling the automation of different types of tasks manually performed by domain experts. In particular, assessing the compliance of a product with the relative schematic is a time-consuming and prone-to-error process. In this paper, we address this problem in a specific industrial scenario. In particular, we define a Neuro-Symbolic approach for automating the compliance verification of the electrical control panels. Our approach is based on the combination of Deep Learning techniques with Answer Set Programming (ASP), and allows for identifying possible anomalies and errors in the final product even when a very limited amount of training data is available. The experiments conducted on a real test case provided by an Italian Company operating in electrical control panel production demonstrate the effectiveness of the proposed approach.
    
[^39]: 基于图神经网络的推特互动预测

    Predicting Tweet Engagement with Graph Neural Networks. (arXiv:2305.10103v1 [cs.SI])

    [http://arxiv.org/abs/2305.10103](http://arxiv.org/abs/2305.10103)

    本研究提出TweetGage，一个基于图神经网络的解决方案，通过表示发布帖子间的语义关联来预测用户在社交媒体上的互动，相对其他研究只考虑帖子文本和发布用户等因素，有效提高了预测准确性。

    

    社交网络是最重要的在线内容分享平台之一，预测发布内容的互动情况是利用社交媒体实现盈利的关键。本文认为发布的内容之间的语义关联也是互动数增长的关键因素。因此，我们提出了TweetGage，一个基于图的神经网络解决方案，通过新颖的基于图的模型来表示帖子间的关系，从而预测用户的互动。为了验证我们的提议，我们针对Twitter平台进行了全面的实验研究，证明了该方法的优良性能。

    Social Networks represent one of the most important online sources to share content across a world-scale audience. In this context, predicting whether a post will have any impact in terms of engagement is of crucial importance to drive the profitable exploitation of these media. In the literature, several studies address this issue by leveraging direct features of the posts, typically related to the textual content and the user publishing it. In this paper, we argue that the rise of engagement is also related to another key component, which is the semantic connection among posts published by users in social media. Hence, we propose TweetGage, a Graph Neural Network solution to predict the user engagement based on a novel graph-based model that represents the relationships among posts. To validate our proposal, we focus on the Twitter platform and perform a thorough experimental campaign providing evidence of its quality.
    
[^40]: 使用移情反应意图分类法来控制和解释神经聊天机器人中的共情。

    Use of a Taxonomy of Empathetic Response Intents to Control and Interpret Empathy in Neural Chatbots. (arXiv:2305.10096v1 [cs.CL])

    [http://arxiv.org/abs/2305.10096](http://arxiv.org/abs/2305.10096)

    本文提出了一种使用移情反应意图分类法来控制和解释神经聊天机器人中的共情回应的方法，能够产生可控和可解释的共情回应。

    

    在开放领域对话代理的领域中，一个最近的趋势是让它们能够对情感提示进行同情式的对话。目前的方法要么遵循端到端的方法，要么在相似的情感标签上进行条件反应以产生共情式的回答。但共情是一个广泛的概念，它指的是个体对另一个人观察到的经历的认知和情感反应，它比单纯的情感模仿更加复杂。因此，除了通用情感外，还需要识别复杂的人类对话策略和动态来控制和解释聊天机器人的共情回应能力。在这项工作中，我们使用了八种共情反应意图的分类法以及通用情感类别来建立一个对话响应生成模型，能够以可控和可解释的方式产生共情回应。它由两个模块组成：1）响应情感/意图预测模块；以及2）响应生成模块。

    A recent trend in the domain of open-domain conversational agents is enabling them to converse empathetically to emotional prompts. Current approaches either follow an end-to-end approach or condition the responses on similar emotion labels to generate empathetic responses. But empathy is a broad concept that refers to the cognitive and emotional reactions of an individual to the observed experiences of another and it is more complex than mere mimicry of emotion. Hence, it requires identifying complex human conversational strategies and dynamics in addition to generic emotions to control and interpret empathetic responding capabilities of chatbots. In this work, we make use of a taxonomy of eight empathetic response intents in addition to generic emotion categories in building a dialogue response generation model capable of generating empathetic responses in a controllable and interpretable manner. It consists of two modules: 1) a response emotion/intent prediction module; and 2) a res
    
[^41]: 多智能体强化学习：方法、应用、前景与挑战

    Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges. (arXiv:2305.10091v1 [cs.AI])

    [http://arxiv.org/abs/2305.10091](http://arxiv.org/abs/2305.10091)

    本文回顾了多智能体强化学习的方法与应用，并指出了未来十年的研究趋势与前景。我们认为，在未来十年中，可信赖的多智能体强化学习将成为一个热门的研究课题。此外，考虑人机交互是多智能体强化学习在各种社会中实际应用的重要考虑因素， 因此，本文还分析了应用于人机交互时所面临的挑战。

    

    多智能体强化学习是一种广泛应用于人工智能领域的技术。然而，当前的研究和应用需要解决可伸缩性、非静态性和可信性等问题。本文旨在回顾多智能体强化学习的方法与应用，并指出未来十年的研究趋势和前景。首先，本文总结了多智能体强化学习的基本方法和应用场景。其次，本文概述了相应的研究方法及其在安全性、鲁棒性、泛化性和道德约束方面的局限性，这些局限性需要在多智能体强化学习的实际应用中得到解决。我们认为，值得关注的是，在未来十年中，可信赖的多智能体强化学习将成为一个热门的研究课题。此外，我们建议在考虑多智能体强化学习在各种社会中的实际应用时应考虑人机互动，因此，本文还分析了在多智能体强化学习应用于人机交互时所面临的挑战。

    Multi-agent reinforcement learning (MARL) is a widely used Artificial Intelligence (AI) technique. However, current studies and applications need to address its scalability, non-stationarity, and trustworthiness. This paper aims to review methods and applications and point out research trends and visionary prospects for the next decade. First, this paper summarizes the basic methods and application scenarios of MARL. Second, this paper outlines the corresponding research methods and their limitations on safety, robustness, generalization, and ethical constraints that need to be addressed in the practical applications of MARL. In particular, we believe that trustworthy MARL will become a hot research topic in the next decade. In addition, we suggest that considering human interaction is essential for the practical application of MARL in various societies. Therefore, this paper also analyzes the challenges while MARL is applied to human-machine interaction.
    
[^42]: 一种多目标优化的Wasserstein反向强化学习模型的证明

    A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization. (arXiv:2305.10089v1 [cs.LG])

    [http://arxiv.org/abs/2305.10089](http://arxiv.org/abs/2305.10089)

    本文证明了Wasserstein反向强化学习模型适用于多目标优化问题，可让学习者的奖励值和最优解模仿专家，具有一定的实用价值。

    

    本文证明了Wasserstein反向强化学习模型可以在有限次迭代中让学习者的奖励值模仿专家的奖励值，并证明了在词典序的多目标优化中，Wasserstein反向强化学习模型可以让学习者的最优解模仿专家的最优解。

    We prove Wasserstein inverse reinforcement learning enables the learner's reward values to imitate the expert's reward values in a finite iteration for multi-objective optimizations. Moreover, we prove Wasserstein inverse reinforcement learning enables the learner's optimal solutions to imitate the expert's optimal solutions for multi-objective optimizations with lexicographic order.
    
[^43]: 冷启动问题：无监督的类别发现方法。

    Cold PAWS: Unsupervised class discovery and the cold-start problem. (arXiv:2305.10071v1 [cs.CV])

    [http://arxiv.org/abs/2305.10071](http://arxiv.org/abs/2305.10071)

    本文提出了一种新方法，通过结合自我监督、聚类和流形学习技术，解决冷启动或无监督选择标记问题，并在多个公共数据集上进行了测试，获得了更好的性能。

    

    在许多机器学习应用中，标记数据集常常是一项艰苦且耗时的任务。虽然研究表明半监督学习技术可以在计算机视觉领域中使用非常少的标签实现高准确性，但很少有人关注如何选择数据集中的图像进行标记。本文提出了一种基于自监督学习、聚类和流形学习技术的新方法，以解决首次选择信息图像子集进行标记的挑战，即冷启动或无监督选择标记问题。我们使用几个公共数据集（包括CIFAR10、Imagenette、DeepWeeds和EuroSAT）测试我们的方法，并观察到当使用我们的标签选择策略时，与随机抽样相比，在监督和半监督学习策略均表现出更好的性能。我们还在d方面获得了更优秀的性能

    In many machine learning applications, labeling datasets can be an arduous and time-consuming task. Although research has shown that semi-supervised learning techniques can achieve high accuracy with very few labels within the field of computer vision, little attention has been given to how images within a dataset should be selected for labeling. In this paper, we propose a novel approach based on well-established self-supervised learning, clustering, and manifold learning techniques that address this challenge of selecting an informative image subset to label in the first instance, which is known as the cold-start or unsupervised selective labelling problem. We test our approach using several publicly available datasets, namely CIFAR10, Imagenette, DeepWeeds, and EuroSAT, and observe improved performance with both supervised and semi-supervised learning strategies when our label selection strategy is used, in comparison to random sampling. We also obtain superior performance for the d
    
[^44]: 揭示反事实解释在就业领域的潜力

    Unveiling the Potential of Counterfactuals Explanations in Employability. (arXiv:2305.10069v1 [cs.AI])

    [http://arxiv.org/abs/2305.10069](http://arxiv.org/abs/2305.10069)

    本研究展示了反事实解释在就业领域的多种应用，包括增强决策支持、遵守法律要求、引导受控变化和分析新颖见解。

    

    在可解释的人工智能（XAI）中，反事实解释被认为可以为复杂的模型决策提供简单、简洁、易懂的理由。然而，我们还没有看到更多应用研究，将它们应用于真实世界情况。为了填补这一空白，本研究侧重于展示如何将反事实用于与就业相关的问题，其中涉及复杂的机器学习算法。对于这些用例，我们使用从比利时公共就业机构（VDAB）获得的真实数据。所提出的用例超越了仅将反事实用作解释的应用程序，展示了它们如何增强决策支持，遵守法律要求，引导受控变化并分析新颖见解。

    In eXplainable Artificial Intelligence (XAI), counterfactual explanations are known to give simple, short, and comprehensible justifications for complex model decisions. However, we are yet to see more applied studies in which they are applied in real-world cases. To fill this gap, this study focuses on showing how counterfactuals are applied to employability-related problems which involve complex machine learning algorithms. For these use cases, we use real data obtained from a public Belgian employment institution (VDAB). The use cases presented go beyond the mere application of counterfactuals as explanations, showing how they can enhance decision support, comply with legal requirements, guide controlled changes, and analyze novel insights.
    
[^45]: 基于卷积核的混合特征学习方法用于ATM故障预测的研究

    A hybrid feature learning approach based on convolutional kernels for ATM fault prediction using event-log data. (arXiv:2305.10059v1 [cs.LG])

    [http://arxiv.org/abs/2305.10059](http://arxiv.org/abs/2305.10059)

    该论文提出了一种基于卷积核的混合特征学习方法，通过结合深度学习、核方法和基于规则的特征选择从ATM事件日志中提取相关特征，用于早期ATM故障预测，在真实的ATM事件日志数据上进行的广泛实验表明其优于现有技术。

    

    预测性维护（PdM）方法旨在在设备故障之前安排维护工作。在这种情况下，检测自动取款机（ATM）的早期故障变得越来越重要，因为这些机器易受各种不可预测的故障影响。ATM通过生成大量的事件日志数据来跟踪执行状态，这些数据收集与故障事件无关的系统消息。 基于事件日志预测故障会导致额外的挑战，主要在于提取可能表示即将发生故障的事件序列的特征。因此，特征学习方法目前正在PdM中使用，其中从最小处理的传感器数据中自动学习有信息量的特征。但是，仍然存在如何利用这些方法来从基于事件日志的数据中导出相关特征的空白。为了填补这个空白，我们提出了基于卷积核的预测模型（MiniROCKET）用于早期ATM故障预测。我们提出的方法包括一种混合特征学习技术，通过结合深度学习、核方法和基于规则的特征选择从ATM事件日志中提取相关特征。我们对真实的ATM事件日志数据进行了广泛的实验，结果表明我们提出的方法在分类性能、稳定性、可解释性和可扩展性方面优于现有技术。

    Predictive Maintenance (PdM) methods aim to facilitate the scheduling of maintenance work before equipment failure. In this context, detecting early faults in automated teller machines (ATMs) has become increasingly important since these machines are susceptible to various types of unpredictable failures. ATMs track execution status by generating massive event-log data that collect system messages unrelated to the failure event. Predicting machine failure based on event logs poses additional challenges, mainly in extracting features that might represent sequences of events indicating impending failures. Accordingly, feature learning approaches are currently being used in PdM, where informative features are learned automatically from minimally processed sensor data. However, a gap remains to be seen on how these approaches can be exploited for deriving relevant features from event-log-based data. To fill this gap, we present a predictive model based on a convolutional kernel (MiniROCKET
    
[^46]: 在贝叶斯网络中找到$\epsilon$-close变分参数

    Finding an $\epsilon$-close Variation of Parameters in Bayesian Networks. (arXiv:2305.10051v1 [cs.AI])

    [http://arxiv.org/abs/2305.10051](http://arxiv.org/abs/2305.10051)

    本文提出的算法基于参数马尔可夫链的"区域验证"技术，可以解决贝叶斯网络中的$\epsilon$-close参数调整问题，为处理参数化BN的子类提供了可能。

    

    本文针对贝叶斯网络（BNs）中的$\epsilon$-close参数调整问题进行了研究：找到给定条件下最小的$\epsilon$-close概率修正，使BNs有效。我们提出了一种基于参数马尔可夫链的"区域验证"技术的算法，其能力超越任何现有技术。我们的实验表明，可以对最多有8个参数的大型BN基准进行$\epsilon$-close调整。特别地，通过允许（i）多个CPT中的不同参数和（ii）CPT之间的参数依赖关系，我们处理了迄今为止收到很少关注的参数化BN的子类。

    This paper addresses the $\epsilon$-close parameter tuning problem for Bayesian Networks (BNs): find a minimal $\epsilon$-close amendment of probability entries in a given set of (rows in) conditional probability tables that make a given quantitative constraint on the BN valid. Based on the state-of-the-art "region verification" techniques for parametric Markov chains, we propose an algorithm whose capabilities go beyond any existing techniques. Our experiments show that $\epsilon$-close tuning of large BN benchmarks with up to 8 parameters is feasible. In particular, by allowing (i) varied parameters in multiple CPTs and (ii) inter-CPT parameter dependencies, we treat subclasses of parametric BNs that have received scant attention so far.
    
[^47]: 在多中心临床研究中处理缺失数据的因果发现

    Causal Discovery with Missing Data in a Multicentric Clinical Study. (arXiv:2305.10050v1 [stat.ME])

    [http://arxiv.org/abs/2305.10050](http://arxiv.org/abs/2305.10050)

    本文扩展了最新的因果发现算法，利用专家知识从多中心临床研究的缺失数据中分析了不同缺失机制对恢复的因果图的影响，验证了所恢复因果图的临床相关性，并用图形分离来验证因果通路，讨论了因果图的拟合度和从临床决策角度的一致性。

    

    由于观测数据的数据生成模型和相关联的因果图通常不存在，因此从观测数据中检验临床假设的因果推断面临许多困难。此外，观测数据可能包含缺失的值，这会影响因果发现算法恢复因果图的效果，这是临床研究中经常被忽略的关键问题。针对这些问题，我们使用子宫内膜癌的多中心研究数据，分析不同缺失机制对恢复的因果图的影响。我们扩展了最先进的因果发现算法，利用专家知识而不损失理论的严谨性，验证了所恢复因果图的临床相关性。最后，我们使用图形分离来验证因果通路，讨论了因果图的拟合度和从临床决策角度的一致性。

    Causal inference for testing clinical hypotheses from observational data presents many difficulties because the underlying data-generating model and the associated causal graph are not usually available. Furthermore, observational data may contain missing values, which impact the recovery of the causal graph by causal discovery algorithms: a crucial issue often ignored in clinical studies. In this work, we use data from a multi-centric study on endometrial cancer to analyze the impact of different missingness mechanisms on the recovered causal graph. This is achieved by extending state-of-the-art causal discovery algorithms to exploit expert knowledge without sacrificing theoretical soundness. We validate the recovered graph with expert physicians, showing that our approach finds clinically-relevant solutions. Finally, we discuss the goodness of fit of our graph and its consistency from a clinical decision-making perspective using graphical separation to validate causal pathways.
    
[^48]: 子宫内膜癌患者淋巴结转移的风险评估：一种因果方法。

    Risk Assessment of Lymph Node Metastases in Endometrial Cancer Patients: A Causal Approach. (arXiv:2305.10041v1 [cs.AI])

    [http://arxiv.org/abs/2305.10041](http://arxiv.org/abs/2305.10041)

    子宫内膜癌淋巴结转移风险评估是一项具有挑战性的任务。由于质量问题、缺失值和高维度等限制，研究者采用因果性贝叶斯网络来学习评估模型，具有更好的先前知识利用和偏差缓解能力。

    

    对子宫内膜癌患者术前淋巴结转移风险进行评估是一项复杂而具有挑战性的任务。在原则上，机器学习和深度学习模型足够灵活和表达，可以捕捉临床风险评估动态。然而，在这种情况下，我们只能使用存在质量问题、缺失值、样本小和高维度的观测数据：我们不能从受到这些偏差影响的有限观测数据中可靠地学习这些模型。相反，我们选择学习因果性贝叶斯网络，以缓解上述问题，并利用临床医生和医师对子宫内膜癌的先前知识。我们引入了一种因果发现算法用于因果性贝叶斯网络，基于Bootstrap重抽样，而不是类似研究中使用的单一插补方法。此外，我们包括一个上下文变量，以评估选择偏差是否会导致学习虚假关联。最后，我们讨论了方法的优势。

    Assessing the pre-operative risk of lymph node metastases in endometrial cancer patients is a complex and challenging task. In principle, machine learning and deep learning models are flexible and expressive enough to capture the dynamics of clinical risk assessment. However, in this setting we are limited to observational data with quality issues, missing values, small sample size and high dimensionality: we cannot reliably learn such models from limited observational data with these sources of bias. Instead, we choose to learn a causal Bayesian network to mitigate the issues above and to leverage the prior knowledge on endometrial cancer available from clinicians and physicians. We introduce a causal discovery algorithm for causal Bayesian networks based on bootstrap resampling, as opposed to the single imputation used in related works. Moreover, we include a context variable to evaluate whether selection bias results in learning spurious associations. Finally, we discuss the strengt
    
[^49]: 语言模型能否用自然语言解决图问题？

    Can Language Models Solve Graph Problems in Natural Language?. (arXiv:2305.10037v1 [cs.CL])

    [http://arxiv.org/abs/2305.10037](http://arxiv.org/abs/2305.10037)

    本论文提出了自然语言图形(NLGraph)，这是一个全面的基于图形问题解决测试，旨在评估LLM在文本描述的图形结构和图形解决方案方面的处理能力。实验结果表明，LLM(GPT-3/4)具有相应的图形推理能力。

    

    大型语言模型越来越多地应用于一些具有隐式图形结构的任务，例如机器人规划、多跳问题回答或知识探索、结构化常识推理等等。虽然LLM在这些任务中已经取得了一定的进展，但是LLM是否能够显式处理图形的文本描述，将它们映射到基于概念的空间中，并执行结构化操作仍然尚未得到足够的研究。为此，我们提出了自然语言图形(NLGraph)，它是一个设计用于自然语言的基于图形问题解决全面测试。NLGraph包含29,370个问题，涵盖了八个图形推理任务，从简单的连接和最短路径到复杂的最大流和模拟图神经网络等任务不等。我们在NLGraph基准测试上评估了LLM(GPT-3/4)，并发现1)语言模型具有相应的图形推理能力；

    Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language
    
[^50]: 因果探索综述：理论与实践（arXiv:2305.10032v1 [cs.AI]）

    A Survey on Causal Discovery: Theory and Practice. (arXiv:2305.10032v1 [cs.AI])

    [http://arxiv.org/abs/2305.10032](http://arxiv.org/abs/2305.10032)

    该文综述了因果发现的理论、实践和最新进展，介绍了因果图恢复算法、实际应用及其重要性。

    

    理解控制现象的规律是科学进步的核心。特别是，在以因果方式建模不同方面的相互作用为目标时，这一点更为重要。事实上，因果推断专门设计用于量化导致其效应的基本关系。因果发现是更广泛的因果关系领域的一个分支，在其中从数据中恢复因果图（在可能的情况下），从而实现了因果效应的识别和估计。在本文中，我们以统一的方式探讨了最新进展，提供了对不同设置下已开发算法的一致概述，报告了有用的工具和数据，并提供实际应用以理解这些方法为什么以及如何得到丰富的利用。

    Understanding the laws that govern a phenomenon is the core of scientific progress. This is especially true when the goal is to model the interplay between different aspects in a causal fashion. Indeed, causal inference itself is specifically designed to quantify the underlying relationships that connect a cause to its effect. Causal discovery is a branch of the broader field of causality in which causal graphs is recovered from data (whenever possible), enabling the identification and estimation of causal effects. In this paper, we explore recent advancements in a unified manner, provide a consistent overview of existing algorithms developed under different settings, report useful tools and data, present real-world applications to understand why and how these methods can be fruitfully exploited.
    
[^51]: ASP(Q)的高效求解器

    An efficient solver for ASP(Q). (arXiv:2305.10021v1 [cs.AI])

    [http://arxiv.org/abs/2305.10021](http://arxiv.org/abs/2305.10021)

    本文提出了一个基于qasp思想的ASP(Q)求解器，引入了更有效的编码过程和新的优化编码，并使用算法选择策略来提高性能，实验结果表明优于qasp和其他最先进的解决方案。

    

    带量词的答案集编程ASP(Q)通过将问题模块化和声明化，将答案集编程（ASP）扩展到整个多项式层次结构中的问题建模。第一个实现ASP(Q)的工具qasp基于量化布尔公式（QBF）的翻译，旨在利用成熟的QBF求解技术。然而，qasp中采用的QBF编码实现非常通用，由于符号和子句的数量庞大，可能难以评估现有的QBF求解器。本文提出了一种新的实现方法，建立在qasp的思想基础上，具有更高效的编码过程和ASP(Q)程序在QBF中的新优化编码。新的编码产生更小的公式（在量词、变量和子句的数量方面），并导致更高效的评估过程。算法选择策略自动组合多个QBF求解器，以找到最佳的求解器来评估编码，从而进一步提高了我们求解器的性能。我们的实验评估表明，我们的求解器在运行时间和内存消耗方面优于qasp和其他ASP(Q)的最先进的求解器。

    Answer Set Programming with Quantifiers ASP(Q) extends Answer Set Programming (ASP) to allow for declarative and modular modeling of problems from the entire polynomial hierarchy. The first implementation of ASP(Q), called qasp, was based on a translation to Quantified Boolean Formulae (QBF) with the aim of exploiting the well-developed and mature QBF-solving technology. However, the implementation of the QBF encoding employed in qasp is very general and might produce formulas that are hard to evaluate for existing QBF solvers because of the large number of symbols and sub-clauses. In this paper, we present a new implementation that builds on the ideas of qasp and features both a more efficient encoding procedure and new optimized encodings of ASP(Q) programs in QBF. The new encodings produce smaller formulas (in terms of the number of quantifiers, variables, and clauses) and result in a more efficient evaluation process. An algorithm selection strategy automatically combines several Q
    
[^52]: 利用半监督学习和视觉Transformer进行细粒度分类的迁移学习

    Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers. (arXiv:2305.10018v1 [cs.CV])

    [http://arxiv.org/abs/2305.10018](http://arxiv.org/abs/2305.10018)

    使用半监督学习技术对ViT模型进行微调，可用于缺乏注释数据的细粒度分类任务，比传统CNN和ViT都表现更好，有助于解决电子商务中图像标注问题。

    

    细粒度分类是一项具有挑战性的任务，涉及识别同一类别内物体之间微小差异。在数据稀缺的情况下，这项任务尤其具有挑战性。由于其使用自注意力机制学习视觉数据高度表现力的能力，视觉Transformer（ViT）最近已成为图像分类的强大工具。在本研究中，我们探讨了半监督ViT，一种应用于缺乏注释数据情况下的ViT模型微调的半监督学习技术，这在电子商务中特别常见，其中图像易于获取但标签有噪音、不存在或获取昂贵。我们的结果表明，即使使用有限的注释数据进行微调，半监督ViT仍然优于传统的卷积神经网络（CNN）和ViT。这些发现表明，半监督ViT在需要精确和细粒度分类的应用中具有重要的前景。

    Fine-grained classification is a challenging task that involves identifying subtle differences between objects within the same category. This task is particularly challenging in scenarios where data is scarce. Visual transformers (ViT) have recently emerged as a powerful tool for image classification, due to their ability to learn highly expressive representations of visual data using self-attention mechanisms. In this work, we explore Semi-ViT, a ViT model fine tuned using semi-supervised learning techniques, suitable for situations where we have lack of annotated data. This is particularly common in e-commerce, where images are readily available but labels are noisy, nonexistent, or expensive to obtain. Our results demonstrate that Semi-ViT outperforms traditional convolutional neural networks (CNN) and ViTs, even when fine-tuned with limited annotated data. These findings indicate that Semi-ViTs hold significant promise for applications that require precise and fine-grained classifi
    
[^53]: 当梯度下降遇到无导数优化：黑盒场景下的完美组合

    When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario. (arXiv:2305.10013v1 [cs.CL])

    [http://arxiv.org/abs/2305.10013](http://arxiv.org/abs/2305.10013)

    本文介绍了一种新方法GDFO，将梯度下降和无导数优化结合在一起，协调地优化任务特定的连续提示。实验证明，该方法优于现有的无梯度和基于梯度的方法。

    

    大型预训练语言模型（PLMs）因其多功能性和解决广泛自然语言处理（NLP）任务的潜力而备受关注。然而，运行这些PLMs的成本可能是禁止的。此外，由于商业考虑和潜在的误用风险（例如GPT-3），PLMs可能未开放源代码。在这种情况下，无导数优化（DFO）提出了黑盒调整的解决方案，用于训练任务特定的连续提示，而不是使用梯度下降。然而，与基于梯度的方法相比，这些无梯度方法仍然存在显着差距。本文通过知识蒸馏将梯度下降引入黑盒调整场景，并提出了一种新的方法GDFO，将梯度下降和无导数优化融合到一起，以协调的方式优化任务特定的连续提示。我们在各种NLP任务上进行了广泛的实验，并展示了我们提出的方法优于现有的无梯度和基于梯度的方法。

    Large pre-trained language models (PLMs) have garnered significant attention for their versatility and potential for solving a wide spectrum of natural language processing (NLP) tasks. However, the cost of running these PLMs may be prohibitive. Furthermore, PLMs may not be open-sourced due to commercial considerations and potential risks of misuse, such as GPT-3. The parameters and gradients of PLMs are unavailable in this scenario. To solve the issue, black-box tuning has been proposed, which utilizes derivative-free optimization (DFO), instead of gradient descent, for training task-specific continuous prompts. However, these gradient-free methods still exhibit a significant gap compared to gradient-based methods. In this paper, we introduce gradient descent into black-box tuning scenario through knowledge distillation. Furthermore, we propose a novel method GDFO, which integrates gradient descent and derivative-free optimization to optimize task-specific continuous prompts in a harmo
    
[^54]: 一次性还原捕获于任意复杂恶劣天气条件下的图像

    Restoring Images Captured in Arbitrary Hybrid Adverse Weather Conditions in One Go. (arXiv:2305.09996v1 [cs.CV])

    [http://arxiv.org/abs/2305.09996](http://arxiv.org/abs/2305.09996)

    提出了一种新的框架RAHC，可以一次性处理任意复杂恶劣天气条件下的图像恢复，并建立了一个新的数据集HAC，用于学习和基准测试混合条件的图像恢复。

    

    在恶劣天气条件下，图像往往会受到随机混合的天气影响（例如，雨天和雾霾夜晚），而现有的图像恢复算法预计天气影响是相互独立的，因此可能无法处理复杂的现实情况。此外，由于缺乏全面对应的数据集来描述复杂的混合天气状况，监督训练不可行。为此，我们通过两个策略——框架和数据——来弥补上述限制。一方面，我们提出了一种新的统一框架，称为RAHC，可以舒适地处理混合情况，并通过单个训练模型灵活地恢复任意混合条件。另一方面，我们建立了一个新的数据集，称为HAC，用于学习和基准测试任意混合条件的图像恢复。HAC包含31种情景，包括任意组合的雨天、雪天、雾霾、雾天和薄雾天气条件。

    Adverse conditions typically suffer from stochastic hybrid weather degradations (e.g., rainy and hazy night), while existing image restoration algorithms envisage that weather degradations occur independently, thus may fail to handle real-world complicated scenarios. Besides, supervised training is not feasible due to the lack of comprehensive paired dataset to characterize hybrid conditions. To this end, we have advanced the forementioned limitations with two tactics: framework and data. On the one hand, we present a novel unified framework, dubbed RAHC, to Restore Arbitrary Hybrid adverse weather Conditions in one go, which can comfortably cope with hybrid scenarios with insufficient remaining background constituents and restore arbitrary hybrid conditions with a single trained model flexibly. On the other hand, we establish a new dataset, termed HAC, for learning and benchmarking arbitrary Hybrid Adverse Conditions restoration. HAC contains 31 scenarios composed of an arbitrary comb
    
[^55]: 采用跨通道卷积交叉注意力的基于时间域脑波辅助的语音增强网络

    BASEN: Time-Domain Brain-Assisted Speech Enhancement Network with Convolutional Cross Attention in Multi-talker Conditions. (arXiv:2305.09994v1 [eess.AS])

    [http://arxiv.org/abs/2305.09994](http://arxiv.org/abs/2305.09994)

    本文提出了一种基于听者脑电图信号的语音增强网络，可以在多说话人的情况下提取目标发言者，实验结果表明其优于最先进的方法。

    

    在多说话人的情况下，时间域单通道语音增强（SE）仍然难以提取目标发言者而不具备任何先前的信息。通过听觉关注解码已经表明，听者的大脑活动包含了所关注说话者的听觉信息。因此，本文提出了一种新的基于时间域脑波辅助的SE网络（BASEN），该网络包含从听者记录的脑电图（EEG）信号，可以从单声道语音混合物中提取目标说话者。所提出的BASEN基于全卷积时间域音频分离网络。为了充分利用脑电图信号中包含的互补信息，我们进一步提出了一个卷积多层交叉注意模块，以融合双分支特征。在公共数据集上的实验结果表明，所提出的模型在多个评估度量上优于最先进的方法。代码在接受后可用。

    Time-domain single-channel speech enhancement (SE) still remains challenging to extract the target speaker without any prior information on multi-talker conditions. It has been shown via auditory attention decoding that the brain activity of the listener contains the auditory information of the attended speaker. In this paper, we thus propose a novel time-domain brain-assisted SE network (BASEN) incorporating electroencephalography (EEG) signals recorded from the listener for extracting the target speaker from monaural speech mixtures. The proposed BASEN is based on the fully-convolutional time-domain audio separation network. In order to fully leverage the complementary information contained in the EEG signals, we further propose a convolutional multi-layer cross attention module to fuse the dual-branch features. Experimental results on a public dataset show that the proposed model outperforms the state-of-the-art method in several evaluation metrics. The reproducible code is availabl
    
[^56]: Reprompting: 通过吉布斯采样自动推断思维链的提示

    Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling. (arXiv:2305.09993v1 [cs.LG])

    [http://arxiv.org/abs/2305.09993](http://arxiv.org/abs/2305.09993)

    Reprompting是一种无需人类干预的算法，通过迭代采样新配方解决多步推理任务，比人类编写的思维链提示表现更好，还可以提高较弱模型的性能。

    

    我们引入了Reprompting，这是一种迭代采样算法，可以在没有人类干预的情况下搜索给定任务的思维链配方。通过吉布斯采样，我们推断适用于一组训练样例的思维链配方。我们的方法使用先前采样的解作为父提示，迭代地采样新的配方来解决其他训练问题。在需要多步推理的五个Big-Bench Hard任务中，Reprompting的表现始终优于零样本、少样本和人类编写的思维链基线。Reprompting还可以促进知识从一个更强的模型到一个较弱的模型的转移，从而大大提高了较弱模型的性能。总体而言，Reprompting相对于使用人类编写的思维链提示的先前最先进方法，带来了高达+17个性能改进。

    We introduce Reprompting, an iterative sampling algorithm that searches for the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, we infer CoT recipes that work consistently well for a set of training samples. Our method iteratively samples new recipes using previously sampled solutions as parent prompts to solve other training problems. On five Big-Bench Hard tasks that require multi-step reasoning, Reprompting achieves consistently better performance than the zero-shot, few-shot, and human-written CoT baselines. Reprompting can also facilitate transfer of knowledge from a stronger model to a weaker model leading to substantially improved performance of the weaker model. Overall, Reprompting brings up to +17 point improvements over the previous state-of-the-art method that uses human-written CoT prompts.
    
[^57]: 一个融合模型: 实现虚拟、物理和认知的一体化及其原理。

    A Fusion Model: Towards a Virtual, Physical and Cognitive Integration and its Principles. (arXiv:2305.09992v1 [cs.AI])

    [http://arxiv.org/abs/2305.09992](http://arxiv.org/abs/2305.09992)

    本研究提出了一个融合模型--融合宇宙(FU)，它将虚拟、物理和认知世界融合在一起。研究了几个影响沉浸式和交互式体验的因素，并提出了一些融合宇宙的基本原则，可以使物理和虚拟世界无缝地融合起来。

    

    虚拟现实(VR)、增强现实(AR)、混合现实(MR)、数字孪生、元宇宙和其他相关数字技术近年来引起了广泛关注。这些新兴技术正在显著改变着世界。本研究引入了一个融合模型——融合宇宙(FU)，在这个模型中，虚拟、物理和认知世界融合在一起。因此，建立一个符合我们物理宇宙规律和原则的融合模型的一套原则至关重要。本文研究了几个可能影响沉浸式和交互式体验的因素; 并提出了融合宇宙的基本原则，可以将物理和虚拟世界无缝地融合起来。

    Virtual Reality (VR), Augmented Reality (AR), Mixed Reality (MR), digital twin, Metaverse and other related digital technologies have attracted much attention in recent years. These new emerging technologies are changing the world significantly. This research introduces a fusion model, i.e. Fusion Universe (FU), where the virtual, physical, and cognitive worlds are merged together. Therefore, it is crucial to establish a set of principles for the fusion model that is compatible with our physical universe laws and principles. This paper investigates several aspects that could affect immersive and interactive experience; and proposes the fundamental principles for Fusion Universe that can integrate physical and virtual world seamlessly.
    
[^58]: 大规模机器学习问题的随机比率跟踪算法

    Stochastic Ratios Tracking Algorithm for Large Scale Machine Learning Problems. (arXiv:2305.09978v1 [cs.LG])

    [http://arxiv.org/abs/2305.09978](http://arxiv.org/abs/2305.09978)

    本文提出了一种新的算法，在经典的SGD框架下实现自适应步长选择，在逻辑回归和深度神经网络上测试了所提出的算法，并证明了该算法可以生成与手动调整得到的最佳步长相当的步长。

    

    许多机器学习应用和任务都依赖于随机梯度下降（SGD）算法及其变体。有效的步长选择对算法的成功至关重要，这促进了诸如ADAM或AdaGrad之类的算法的发展。在本文中，我们提出了一种新颖的算法，在经典的SGD框架下实现自适应步长选择，它可以轻松适应其他随机算法。我们的算法灵感来自传统的非线性优化技术，并受到分析发现的支持。我们展示了在合理条件下，该算法产生符合良好理论要求的步长，并在期望下生成收敛于解的静止邻域的迭代。我们在逻辑回归和深度神经网络上测试了所提出的算法，并证明了该算法可以生成与手动调整得到的最佳步长相当的步长。

    Many machine learning applications and tasks rely on the stochastic gradient descent (SGD) algorithm and its variants. Effective step length selection is crucial for the success of these algorithms, which has motivated the development of algorithms such as ADAM or AdaGrad. In this paper, we propose a novel algorithm for adaptive step length selection in the classical SGD framework, which can be readily adapted to other stochastic algorithms. Our proposed algorithm is inspired by traditional nonlinear optimization techniques and is supported by analytical findings. We show that under reasonable conditions, the algorithm produces step lengths in line with well-established theoretical requirements, and generates iterates that converge to a stationary neighborhood of a solution in expectation. We test the proposed algorithm on logistic regressions and deep neural networks and demonstrate that the algorithm can generate step lengths comparable to the best step length obtained from manual tu
    
[^59]: 无归河流：基于图渗透嵌入的高效知识图谱推理

    River of No Return: Graph Percolation Embeddings for Efficient Knowledge Graph Reasoning. (arXiv:2305.09974v1 [cs.AI])

    [http://arxiv.org/abs/2305.09974](http://arxiv.org/abs/2305.09974)

    本文提出了一种基于图渗透的嵌入技术，通过维护最短路径和消除冗余路径来最小化熵的消息传递，从而显著提高了知识图谱推理的效率。

    

    本文研究了基于图神经网络（GNN）的知识图谱（KG）推理嵌入技术。我们首次将路径编码和消息传递的最先进KG推理模型中的路径冗余问题与模型训练中的变换误差联系起来，这为我们带来了对KG推理的新的理论洞见，以及在实践中具有高效性。在理论方面，我们分析了KG路径变换误差的熵，并指出了查询特定冗余路径会引起熵的增加。这些发现指导我们维护最短路径，并消除冗余路径以最小化熵的消息传递。为了实现这一目标，在实践方面，我们提出了一种高效的图渗透过程，该过程受流体力学中渗透模型的启发，并设计了一个轻量级的基于GNN的KG推理框架，称为图渗透嵌入（GraPE）。GraPE在基准数据集WN18RR和FB15K237上的归纳式和传递式KG推理任务中均优于之前的最先进方法。

    We study Graph Neural Networks (GNNs)-based embedding techniques for knowledge graph (KG) reasoning. For the first time, we link the path redundancy issue in the state-of-the-art KG reasoning models based on path encoding and message passing to the transformation error in model training, which brings us new theoretical insights into KG reasoning, as well as high efficacy in practice. On the theoretical side, we analyze the entropy of transformation error in KG paths and point out query-specific redundant paths causing entropy increases. These findings guide us to maintain the shortest paths and remove redundant paths for minimized-entropy message passing. To achieve this goal, on the practical side, we propose an efficient Graph Percolation Process motivated by the percolation model in Fluid Mechanics, and design a lightweight GNN-based KG reasoning framework called Graph Percolation Embeddings (GraPE). GraPE outperforms previous state-of-the-art methods in both transductive and induct
    
[^60]: HICO-DET-SG和V-COCO-SG：新的数据拆分用于评估人-物交互检测中的系统性泛化

    HICO-DET-SG and V-COCO-SG: New Data Splits to Evaluate Systematic Generalization in Human-Object Interaction Detection. (arXiv:2305.09948v1 [cs.CV])

    [http://arxiv.org/abs/2305.09948](http://arxiv.org/abs/2305.09948)

    本论文提出了两个新的HOI检测数据拆分，旨在评估系统性泛化。在新的数据拆分上测试结果表明，HOI检测模型对于未见过的对象和交互组合的泛化十分困难。

    

    人-物交互检测是一种预测图像中人与物品之间交互的任务。在实际场景中，需要对HOI检测模型进行系统性的泛化，即泛化到新的对象和交互组合上，因为训练数据仅可能涵盖所有可能组合的一小部分。然而，据我们所知，没有开放的基准测试或现有工作评估HOI检测中的系统性泛化。为解决这个问题，我们基于HICO-DET和V-COCO数据集创建了两个名为HICO-DET-SG和V-COCO-SG的新的HOI检测数据拆分。我们在新的数据拆分上评估了代表性的HOI检测模型，并观察到与原始数据集上相比测试性能有很大的降低。这个结果表明系统性泛化是HOI检测中一个具有挑战性的目标。我们希望我们的新数据拆分能够鼓励更多的研究朝着这个目标努力。

    Human-Object Interaction (HOI) detection is a task to predict interactions between humans and objects in an image. In real-world scenarios, HOI detection models are required systematic generalization, i.e., generalization to novel combinations of objects and interactions, because it is highly probable that the train data only cover a limited portion of all possible combinations. However, to our knowledge, no open benchmark or existing work evaluates the systematic generalization in HOI detection. To address this issue, we created two new sets of HOI detection data splits named HICO-DET-SG and V-COCO-SG based on HICO-DET and V-COCO datasets. We evaluated representative HOI detection models on the new data splits and observed large degradation in the test performances compared to those on the original datasets. This result shows that systematic generalization is a challenging goal in HOI detection. We hope our new data splits encourage more research toward this goal.
    
[^61]: 可解释的强化学习中的匹兹堡学习分类器系统：与 XCS 的比较

    Pittsburgh Learning Classifier Systems for Explainable Reinforcement Learning: Comparing with XCS. (arXiv:2305.09945v1 [cs.LG])

    [http://arxiv.org/abs/2305.09945](http://arxiv.org/abs/2305.09945)

    本文介绍了两个新的匹兹堡学习分类器系统，与经典的密歇根系统 XCS 进行了比较，并在确定性和随机 FrozenLake 环境中获得了较好的表现。

    

    最近，由于深度学习技术的应用，对强化学习（RL）的兴趣有所增加，但是与象征性系统相比，这些连接主义方法不透明。学习分类器系统 (LCS) 是进化机器学习系统，可被归类为可解释的人工智能 (XAI)，因为它们基于规则的本质。密歇根 LCS 通常在 RL 领域中使用，而匹兹堡系统（例如 SAMUEL）由于复杂的算法设计和高计算要求而很少使用；然而，它们可以产生比密歇根系统更简洁/可解释的解决方案。我们旨在开发两个新的匹兹堡 LCS，以解决 RL 领域的问题：PPL-DL 和 PPL-ST。前者充当“零级”系统，后者重访了 SAMUEL 的核心蒙特卡罗学习机制，用于估计规则强度。我们将我们的两个匹兹堡系统与密歇根系统 XCS 在确定性和随机 FrozenLake 环境中进行比较。结果表明，PPL-ST在两种情况下表现良好。

    Interest in reinforcement learning (RL) has recently surged due to the application of deep learning techniques, but these connectionist approaches are opaque compared with symbolic systems. Learning Classifier Systems (LCSs) are evolutionary machine learning systems that can be categorised as eXplainable AI (XAI) due to their rule-based nature. Michigan LCSs are commonly used in RL domains as the alternative Pittsburgh systems (e.g. SAMUEL) suffer from complex algorithmic design and high computational requirements; however they can produce more compact/interpretable solutions than Michigan systems. We aim to develop two novel Pittsburgh LCSs to address RL domains: PPL-DL and PPL-ST. The former acts as a "zeroth-level" system, and the latter revisits SAMUEL's core Monte Carlo learning mechanism for estimating rule strength. We compare our two Pittsburgh systems to the Michigan system XCS across deterministic and stochastic FrozenLake environments. Results show that PPL-ST performs on-pa
    
[^62]: 无需演示的自主增强学习：隐式双向课程法

    Demonstration-free Autonomous Reinforcement Learning via Implicit and Bidirectional Curriculum. (arXiv:2305.09943v1 [cs.LG])

    [http://arxiv.org/abs/2305.09943](http://arxiv.org/abs/2305.09943)

    本文提出了一种无需演示的自主强化学习算法（IBC），通过辅助代理和基于最优输运的双向目标课程，能够在无需先前数据依赖的情况下，实现从非周期性交互中学习，并在稀疏任务相关交互的环境中取得更好的表现。

    

    虽然强化学习在仅通过与环境交互来获得复杂技能方面取得了巨大成功，但它假设在每个周期结束时都可以轻易地回到初始状态。这种假设妨碍了具身代理的自主学习，因为在物理世界中进行重置需要耗费时间和繁琐的解决方案。因此，对于能够从非周期性交互中学习的自主强化学习（ARL）方法越来越受到关注。然而，现有的ARL方法受到其对先前数据的依赖的限制，无法在任务相关交互稀疏的环境中学习。相反，我们提出了一种通过隐式和双向课程的无演示ARL算法（IBC）。通过辅助代理以及基于最优输运的双向目标课程，我们的方法表现优于以前的方法，甚至比利用演示的方法还要好。

    While reinforcement learning (RL) has achieved great success in acquiring complex skills solely from environmental interactions, it assumes that resets to the initial state are readily available at the end of each episode. Such an assumption hinders the autonomous learning of embodied agents due to the time-consuming and cumbersome workarounds for resetting in the physical world. Hence, there has been a growing interest in autonomous RL (ARL) methods that are capable of learning from non-episodic interactions. However, existing works on ARL are limited by their reliance on prior data and are unable to learn in environments where task-relevant interactions are sparse. In contrast, we propose a demonstration-free ARL algorithm via Implicit and Bi-directional Curriculum (IBC). With an auxiliary agent that is conditionally activated upon learning progress and a bidirectional goal curriculum based on optimal transport, our method outperforms previous methods, even the ones that leverage dem
    
[^63]: “我全然成为我自己”：以TGNB人群为中心，评估开放式语言生成中的偏见

    "I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])

    [http://arxiv.org/abs/2305.09941](http://arxiv.org/abs/2305.09941)

    本论文研究了如何以TGNB人群的声音为中心，评估开放式语言生成中的偏见。通过理解TGNB个体的经历，提出了以TGNB人群为中心的OLG系统评估框架，并且包括一个为TGNB人群设计的调查工具和分析方法。

    

    跨性别和非二元（TGNB）人群在日常生活中经历了不成比例的歧视和排斥。随着语言生成技术的日益普及和应用，进一步边缘化这一人群的可能性也在增加。虽然大量的NLP公平文献着重于阐明和解决性别偏见，但评估TGNB身份所带来的性别伤害需要理解这些身份如何独特地与社会性别规范互动以及与性别二元中心的视角相区分。这样的测量框架本质上需要以TGNB声音为中心，帮助指导包容性别的自然语言处理应该为谁服务。为实现这一目标，我们以TGNB社区和现有的跨学科文献为基础，评估了TGNB个体经历边缘化所形成的社会现实是如何影响和存在于开放式语言生成（OLG）中。首先理解TGNB个体的经历，我们提出了一个评估OLG系统的框架，旨在以TGNB人群为中心，度量与该人群相关的偏见。我们的框架包括特别为TGNB人群设计的调查工具，以及交叉分析结果的交叉方法。我们相信，这项工作将有助于实现更公平、更包容的自然语言处理社区，并潜在地解决NLP研究中广泛的交叉身份问题。

    Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
    
[^64]: 一种适用于可解释和简约强化学习策略的基因模糊系统

    A Genetic Fuzzy System for Interpretable and Parsimonious Reinforcement Learning Policies. (arXiv:2305.09922v1 [cs.LG])

    [http://arxiv.org/abs/2305.09922](http://arxiv.org/abs/2305.09922)

    提出了一种基于基因模糊系统的强化学习策略，可演化出可解释的简约策略，并能有效平衡策略性能与复杂性。

    

    强化学习（RL）正经历着研究兴趣的复苏，学习分类器系统（LCS）已经被应用多年。然而，传统的密歇根方法往往会演化成大量的规则库，这些规则库难以解释或扩展到超出标准迷宫之外的领域。提出了一种匹兹堡基因模糊系统，名为Fuzzy MoCoCo，其利用多目标和合作协同进化机制，为RL环境演化模糊规则策略。系统中的多目标与策略性能与复杂性有关。连续状态的RL环境Mountain Car被用作测试基础。结果表明，该系统能够有效探索策略性能与复杂性之间的权衡，并学习出可解释的，高性能的策略，并尽可能少地使用规则。

    Reinforcement learning (RL) is experiencing a resurgence in research interest, where Learning Classifier Systems (LCSs) have been applied for many years. However, traditional Michigan approaches tend to evolve large rule bases that are difficult to interpret or scale to domains beyond standard mazes. A Pittsburgh Genetic Fuzzy System (dubbed Fuzzy MoCoCo) is proposed that utilises both multiobjective and cooperative coevolutionary mechanisms to evolve fuzzy rule-based policies for RL environments. Multiobjectivity in the system is concerned with policy performance vs. complexity. The continuous state RL environment Mountain Car is used as a testing bed for the proposed system. Results show the system is able to effectively explore the trade-off between policy performance and complexity, and learn interpretable, high-performing policies that use as few rules as possible.
    
[^65]: 评估上下文推断误差和部分可观察性对及时自适应干预RL方法的影响

    Assessing the Impact of Context Inference Error and Partial Observability on RL Methods for Just-In-Time Adaptive Interventions. (arXiv:2305.09913v1 [cs.LG])

    [http://arxiv.org/abs/2305.09913](http://arxiv.org/abs/2305.09913)

    本文探讨了在及时自适应干预中，强化学习方法如何学习干预选项选择策略，结果表明上下文推断误差和部分可观察性对学习有效策略的能力产生影响，通过在上下文不确定性增加时从上下文推断中传播的不确定性可以提高干预效果，而策略梯度算法可以提供对部分观察到的行为状态信息的非凡鲁棒性。

    

    及时自适应干预(JITAIs)是行为科学界开发的一类个性化健康干预。 JITAIs旨在通过从预定义的组件集中迭代选择干预选项序列来响应每个个体的时间变化状态，以提供正确类型和数量的支持。在这项工作中，我们探讨了强化学习方法应用于学习干预选项选择策略的问题。我们研究了上下文推断误差和部分可观察性对学习有效策略的能力的影响。我们的结果表明，当上下文不确定性增加时，从上下文推断中传播的不确定性对提高干预效果至关重要，而策略梯度算法可以提供对部分观察到的行为状态信息的非凡鲁棒性。

    Just-in-Time Adaptive Interventions (JITAIs) are a class of personalized health interventions developed within the behavioral science community. JITAIs aim to provide the right type and amount of support by iteratively selecting a sequence of intervention options from a pre-defined set of components in response to each individual's time varying state. In this work, we explore the application of reinforcement learning methods to the problem of learning intervention option selection policies. We study the effect of context inference error and partial observability on the ability to learn effective policies. Our results show that the propagation of uncertainty from context inferences is critical to improving intervention efficacy as context uncertainty increases, while policy gradient algorithms can provide remarkable robustness to partially observed behavioral state information.
    
[^66]: 基于预训练模型的等变小样本学习

    Equivariant Few-Shot Learning from Pretrained Models. (arXiv:2305.09900v1 [cs.LG])

    [http://arxiv.org/abs/2305.09900](http://arxiv.org/abs/2305.09900)

    本文提出了一种基于预训练模型的$\lambda$-\textit{equitune}方法，它使用\textit{重要性权重}$\lambda$对特征进行平均，可以显著提高等变小样本学习的表现。

    

    高效的迁移学习算法是基础模型在有限数据情况下在各种下游任务上取得成功的关键。最近的作品 \cite{basu2022equi} 和 \cite{kaba2022equivariance} 分别提出了使用从群变换输入得到的特征的群平均值（\textit{equitune}）和基于优化的方法来从不等变的神经网络获取等变输出。虽然 \cite{kaba2022equivariance} 只关注从头开始训练，但我们发现即使在良好的微调结果下，\textit{equitune} 在等变零样本任务上表现不佳。我们认为这是因为预训练模型为某些转换提供了更高质量的特征，而对其进行简单平均会产生不良影响。因此，我们提出了一种使用\textit{重要性权重}$\lambda$对特征进行平均的$\lambda$-\textit{equitune} 方法。这些权重是使用一个小型神经网络直接从数据中学习的，从而导致出色的零样本和微调结果。

    Efficient transfer learning algorithms are key to the success of foundation models on diverse downstream tasks even with limited data. Recent works of \cite{basu2022equi} and \cite{kaba2022equivariance} propose group averaging (\textit{equitune}) and optimization-based methods, respectively, over features from group-transformed inputs to obtain equivariant outputs from non-equivariant neural networks. While \cite{kaba2022equivariance} are only concerned with training from scratch, we find that equitune performs poorly on equivariant zero-shot tasks despite good finetuning results. We hypothesize that this is because pretrained models provide better quality features for certain transformations than others and simply averaging them is deleterious. Hence, we propose $\lambda$-\textit{equitune} that averages the features using \textit{importance weights}, $\lambda$s. These weights are learned directly from the data using a small neural network, leading to excellent zero-shot and finetuned 
    
[^67]: 面向聚类的负采样用于无监督句子表示学习

    Clustering-Aware Negative Sampling for Unsupervised Sentence Representation. (arXiv:2305.09892v1 [cs.CL])

    [http://arxiv.org/abs/2305.09892](http://arxiv.org/abs/2305.09892)

    ClusterNS 是一种将聚类信息引入对比学习进行无监督句子表示学习的新方法，通过改进的 K 均值聚类算法提供难负例并识别错误负例，旨在通过一个统一的框架解决问题，实验结果表明其在无监督句子表示学习中表现优于基线。

    

    对比学习在句子表示学习中广泛研究，然而早期的研究主要集中在正例的构建上，而 batch 内的样本通常被视为负例。这种方法忽视了选择合适的负例的重要性，可能会导致难负例的稀缺性和错误负例的包含。为了解决这些问题，我们提出了 ClusterNS（面向聚类的负采样），一种将聚类信息引入对比学习进行无监督句子表示学习的新方法。我们应用改进的 K 均值聚类算法来提供难负例并识别训练过程中的错误负例，旨在通过一个统一的框架解决这两个问题。在语义文本相似度（STS）任务上的实验表明，我们提出的 ClusterNS 在无监督句子表示学习中表现优于基线。我们的代码已上传到 https://github.com/xxxxxx。

    Contrastive learning has been widely studied in sentence representation learning. However, earlier works mainly focus on the construction of positive examples, while in-batch samples are often simply treated as negative examples. This approach overlooks the importance of selecting appropriate negative examples, potentially leading to a scarcity of hard negatives and the inclusion of false negatives. To address these issues, we propose ClusterNS (Clustering-aware Negative Sampling), a novel method that incorporates cluster information into contrastive learning for unsupervised sentence representation learning. We apply a modified K-means clustering algorithm to supply hard negatives and recognize in-batch false negatives during training, aiming to solve the two issues in one unified framework. Experiments on semantic textual similarity (STS) tasks demonstrate that our proposed ClusterNS compares favorably with baselines in unsupervised sentence representation learning. Our code has been
    
[^68]: 基于机器学习和关键词感知交叉编码器排序摘要程序的自然语言文本语义相似度度量——以UCGIS GIS&T知识体系为案例研究

    Semantic Similarity Measure of Natural Language Text through Machine Learning and a Keyword-Aware Cross-Encoder-Ranking Summarizer -- A Case Study Using UCGIS GIS&T Body of Knowledge. (arXiv:2305.09877v1 [cs.CL])

    [http://arxiv.org/abs/2305.09877](http://arxiv.org/abs/2305.09877)

    本文提出了一种新方法，采用机器学习模型和关键词感知交叉编码器排序摘要程序，从文本内容中提取语义信息，并度量 GIS&T BoK 话题之间的语义相似度，以解决手动定义话题关系带来的不完整评估问题。该方法在准确度量话题关系方面表现良好，对 GIS&T 领域的研究和实践具有重要意义。

    

    GIS&T 知识体系是由地理信息科学与技术相关团体发起的一个社区项目，旨在定义、开发和记录地理信息科学与技术相关话题。本文提出了一种新方法，采用机器学习模型和关键词感知交叉编码器排序摘要程序，从文本内容中提取语义信息，并度量 BoK 话题之间的语义相似度。结果表明，我们的方法在识别 BoK 话题之间的语义相似度方面优于其他 NLP 技术。该方法能够自动且准确地度量话题之间的关系，从而使 GIS&T 领域的研究人员和实践者受益。

    Initiated by the University Consortium of Geographic Information Science (UCGIS), GIS&T Body of Knowledge (BoK) is a community-driven endeavor to define, develop, and document geospatial topics related to geographic information science and technologies (GIS&T). In recent years, GIS&T BoK has undergone rigorous development in terms of its topic re-organization and content updating, resulting in a new digital version of the project. While the BoK topics provide useful materials for researchers and students to learn about GIS, the semantic relationships among the topics, such as semantic similarity, should also be identified so that a better and automated topic navigation can be achieved. Currently, the related topics are either defined manually by editors or authors, which may result in an incomplete assessment of topic relationship. To address this challenge, our research evaluates the effectiveness of multiple natural language processing (NLP) techniques in extracting semantics from te
    
[^69]: 利用语言模型用自然语言解释黑盒文本模块

    Explaining black box text modules in natural language with language models. (arXiv:2305.09863v1 [cs.AI])

    [http://arxiv.org/abs/2305.09863](http://arxiv.org/abs/2305.09863)

    本文介绍了一种名为Summarize and Score（SASC）的方法，该方法可以自动获取黑盒文本模块的自然语言解释以及解释可靠程度的分数。研究者们已经在合成模块和BERT模型中使用SASC，让我们可以解释模块的选择性，这对于增强大型语言模型的可解释性非常重要。

    

    大型语言模型已经证明在各种任务中具有出色的预测性能。然而，它们的快速增长和不透明性已经引起了对可解释性的需求。本文询问是否可以自动获取黑盒文本模块的自然语言解释。一个“文本模块”是将文本映射到标量连续值的任何函数，例如LLM内的子模块或大脑区域的拟合模型。“黑盒”表示我们只能访问模块的输入/输出。我们引入了Summarize and Score（SASC）方法，它接受文本模块并返回模块选择性的自然语言解释以及解释可靠程度的分数。我们在三个上下文中研究SASC。首先，我们在合成模块上评估SASC，并发现它经常恢复基本真相说明。其次，我们使用SASC来解释预训练BERT模型中的模块，使得检查BERT的模块成为可能。

    Large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their rapid proliferation and increasing opaqueness have created a growing need for interpretability. Here, we ask whether we can automatically obtain natural language explanations for black box text modules. A "text module" is any function that maps text to a scalar continuous value, such as a submodule within an LLM or a fitted model of a brain region. "Black box" indicates that we only have access to the module's inputs/outputs.  We introduce Summarize and Score (SASC), a method that takes in a text module and returns a natural language explanation of the module's selectivity along with a score for how reliable the explanation is. We study SASC in 3 contexts. First, we evaluate SASC on synthetic modules and find that it often recovers ground truth explanations. Second, we use SASC to explain modules found within a pre-trained BERT model, enabling inspection of the 
    
[^70]: Epsilon Sampling Rocks: 研究用于机器翻译最小贝叶斯风险解码的采样策略

    Epsilon Sampling Rocks: Investigating Sampling Strategies for \\Minimum Bayes Risk Decoding for Machine Translation. (arXiv:2305.09860v1 [cs.CL])

    [http://arxiv.org/abs/2305.09860](http://arxiv.org/abs/2305.09860)

    本文研究了用于机器翻译最小贝叶斯风险解码的不同采样策略，并发现了epsilon采样方式能够使得解码结果显著地优于其他所有已测试的采样方式和束搜索解码。

    

    机器翻译中的最小贝叶斯风险（MBR）解码已经显示出是一种强大的替代束搜索解码的方法，尤其是与基于神经网络的效用函数相结合时。然而，MBR解码的性能严重依赖于从模型中采样的方法和数量。本文探讨了用于MBR解码的不同采样方法对性能的影响。我们评估了一些流行的采样方法，例如祖先采样，核采样和top-k采样。基于我们对它们局限性的认识，我们尝试了最近提出的epsilon采样方法，该方法通过修剪所有小于epsilon的标记，以确保样本中的每个标记获得公平的概率质量。通过广泛的人类评估，我们证明了基于epsilon采样的MBR解码显著优于不仅是束搜索解码，而且还优于所有其他已测试的采样方法的MBR解码。

    Recent advances in machine translation (MT) have shown that Minimum Bayes Risk (MBR) decoding can be a powerful alternative to beam search decoding, especially when combined with neural-based utility functions. However, the performance of MBR decoding depends heavily on how and how many candidates are sampled from the model. In this paper, we explore how different sampling approaches for generating candidate lists for MBR decoding affect performance. We evaluate popular sampling approaches, such as ancestral, nucleus, and top-k sampling. Based on our insights into their limitations, we experiment with the recently proposed epsilon-sampling approach, which prunes away all tokens with a probability smaller than epsilon, ensuring that each token in a sample receives a fair probability mass. Through extensive human evaluations, we demonstrate that MBR decoding based on epsilon-sampling significantly outperforms not only beam search decoding, but also MBR decoding with all other tested samp
    
[^71]: 知识图谱补全模型是少样本学习者：以 LLMS 在电商中的关系标注为例的经验研究

    Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs. (arXiv:2305.09858v1 [cs.IR])

    [http://arxiv.org/abs/2305.09858](http://arxiv.org/abs/2305.09858)

    本文通过对知识图谱中关系标注的实证研究，发现大型语言模型具有强大的学习能力以及在少量标记数据下预测产品类型之间关系的有效性。

    

    知识图谱在增强电子商务系统性能方面发挥着至关重要的作用，提供了关于实体及其关系的结构化信息，例如产品或产品类型之间的互补或替代关系，这些信息可以在推荐系统中利用。然而，由于电子商务领域的动态性和人力成本相关的原因，知识图谱中的关系标注仍然是一个具有挑战性的任务。最近，大型语言模型（LLM）的突破在许多自然语言处理任务中展示了出乎意料的结果。在本文中，我们进行了一个关于 LLM 在电子商务知识图谱中进行关系标注的实证研究，研究它们在自然语言方面强大的学习能力以及在有限标记数据下预测产品类型之间关系的有效性。我们评估了各种 LLM，包括 PaLM 和 GPT-3.5，在基准数据集上，证明它们能够达到与人类相当的关系性能水平。

    Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between products or product types, which can be utilized in recommender systems. However, relation labeling in KGs remains a challenging task due to the dynamic nature of e-commerce domains and the associated cost of human labor. Recently, breakthroughs in Large Language Models (LLMs) have shown surprising results in numerous natural language processing tasks. In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data. We evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets, demonstrating their ability to achieve competitive performance compared to humans on relation
    
[^72]: CoEdIT：通过任务特定指令调整实现文本编辑

    CoEdIT: Text Editing by Task-Specific Instruction Tuning. (arXiv:2305.09857v1 [cs.CL])

    [http://arxiv.org/abs/2305.09857](http://arxiv.org/abs/2305.09857)

    CoEdIT是一种通过任务特定指令调整实现文本编辑的最先进模型，能够提高用户生成文本的质量和提高流程的效率。

    

    文本编辑或修订是人类写作过程中必不可少的功能。理解LLMs在进行高质量修订和与人类写作者协作方面的能力是构建有效写作助手的关键步骤。在LLMs和指令调整的先前成功基础上，我们利用经过指令调整的LLMs进行文本修订，以提高用户生成文本的质量和提高流程的效率。我们引入了CoEdIT，这是一款用于写作辅助的最先进的文本编辑模型。CoEdIT从用户那里获取指令，指定所需文本的属性，例如“使句子更简单”或“以更中立的风格写作”，并输出编辑后的文本。我们提供了一个大型语言模型，该模型在各种文本编辑基准测试上实现了最先进的性能。我们的模型（1）在各种文本编辑基准测试上实现最先进的性能，（2）与公开可用的模型相比具有竞争力。

    Text editing or revision is an essential function of the human writing process. Understanding the capabilities of LLMs for making high-quality revisions and collaborating with human writers is a critical step toward building effective writing assistants. With the prior success of LLMs and instruction tuning, we leverage instruction-tuned LLMs for text revision to improve the quality of user-generated text and improve the efficiency of the process. We introduce CoEdIT, a state-of-the-art text editing model for writing assistance. CoEdIT takes instructions from the user specifying the attributes of the desired text, such as "Make the sentence simpler" or "Write it in a more neutral style," and outputs the edited text. We present a large language model fine-tuned on a diverse collection of task-specific instructions for text editing (a total of 82K instructions). Our model (1) achieves state-of-the-art performance on various text editing benchmarks, (2) is competitive with publicly availa
    
[^73]: 简单易用：具有不可靠客户端的联邦学习容错性评估

    Keep It Simple: Fault Tolerance Evaluation of Federated Learning with Unreliable Clients. (arXiv:2305.09856v1 [cs.LG])

    [http://arxiv.org/abs/2305.09856](http://arxiv.org/abs/2305.09856)

    本文评估了具有不可靠客户端的联邦学习的容错性，研究表明相对较简单的FL算法在此情境下也能表现良好。

    

    作为一种新兴的人工智能方法，联邦学习（FL）可以在多个设备上进行分散模型训练，而不泄露本地训练数据。虽然已经有研究提出了提高FL容错性的方法，但现实应用中不可靠设备（例如掉线、错误配置、差数据质量）的真实影响尚未得到充分调查。我们精心选择了两个具有有限客户端的代表性实际分类问题，以更好地分析FL容错性。与直觉相反，简单的FL算法在存在不可靠客户端的情况下可以出奇地表现良好。

    Federated learning (FL), as an emerging artificial intelligence (AI) approach, enables decentralized model training across multiple devices without exposing their local training data. FL has been increasingly gaining popularity in both academia and industry. While research works have been proposed to improve the fault tolerance of FL, the real impact of unreliable devices (e.g., dropping out, misconfiguration, poor data quality) in real-world applications is not fully investigated. We carefully chose two representative, real-world classification problems with a limited numbers of clients to better analyze FL fault tolerance. Contrary to the intuition, simple FL algorithms can perform surprisingly well in the presence of unreliable clients.
    
[^74]: 经典规划中探索和开发的自适应平衡

    Scale-Adaptive Balancing of Exploration and Exploitation in Classical Planning. (arXiv:2305.09840v1 [cs.AI])

    [http://arxiv.org/abs/2305.09840](http://arxiv.org/abs/2305.09840)

    本文提出了一种MCTS/THTS算法GreedyUCT-Normal，该算法能够通过采用奖励变化的尺度处理不同尺度的分布，以在经典计划中平衡探索和开发。

    

    在游戏树搜索和自动化规划中，平衡探索和开发一直是一个重要的问题。然而，虽然这个问题在多臂赌博机（MAB）文献中已经被广泛分析，但规划社区在试图应用这些结果时取得的成功有限。我们展示了MAB文献更详细的理论理解有助于改进基于蒙特卡罗树搜索（MCTS）/基于试验的启发式树搜索（THTS）的现有规划算法。具体而言，THTS在一种临时方法中使用UCB1 MAB算法，因为在启发式搜索中UCB1理论上需要有界支持奖励分布的要求在经典规划中不被满足。核心问题在于UCB1缺乏对不同奖励尺度的自适应。我们提出了GreedyUCT-Normal，这是一种具有UCB1-Normal赌博机的MCTS/THTS算法，用于敏捷经典计划，它通过采用奖励变化的尺度处理不同尺度的分布。

    Balancing exploration and exploitation has been an important problem in both game tree search and automated planning. However, while the problem has been extensively analyzed within the Multi-Armed Bandit (MAB) literature, the planning community has had limited success when attempting to apply those results. We show that a more detailed theoretical understanding of MAB literature helps improve existing planning algorithms that are based on Monte Carlo Tree Search (MCTS) / Trial Based Heuristic Tree Search (THTS). In particular, THTS uses UCB1 MAB algorithms in an ad hoc manner, as UCB1's theoretical requirement of fixed bounded support reward distributions is not satisfied within heuristic search for classical planning. The core issue lies in UCB1's lack of adaptations to the different scales of the rewards. We propose GreedyUCT-Normal, a MCTS/THTS algorithm with UCB1-Normal bandit for agile classical planning, which handles distributions with different scales by taking the reward vari
    
[^75]: 合作智能网络：泛化和扩展

    Coagent Networks: Generalized and Scaled. (arXiv:2305.09838v1 [cs.LG])

    [http://arxiv.org/abs/2305.09838](http://arxiv.org/abs/2305.09838)

    论文提出了一种强大而灵活的合作智能网络框架，可以异步计算网络不同部分、吸收反向传播不能使用的不可微组件、探索和/或时态抽象的分层网络，并使用高效算法进行分布式和并行学习。在基准问题上的模拟表明，该算法在性能上有显著提高。

    

    强化学习中的合作智能网络为任意随机神经网络的原则性学习提供了一种强大而灵活的框架。它不仅能够异步计算网络的不同部分，还能够吸收一些反向传播不能使用的不可微组件。此外，它还可以在动作空间级别以上进行探索，即可以设计为探索和/或时态抽象的分层网络。本文将协作理论和学习规则推广到任意网络拓扑，并通过使用分布式和并行学习的高效算法来扩展它。我们在基准问题上进行了模拟，证明了该算法在性能上的显著提高。

    Coagent networks for reinforcement learning (RL) [Thomas and Barto, 2011] provide a powerful and flexible framework for deriving principled learning rules for arbitrary stochastic neural networks. The coagent framework offers an alternative to backpropagation-based deep learning (BDL) that overcomes some of backpropagation's main limitations. For example, coagent networks can compute different parts of the network \emph{asynchronously} (at different rates or at different times), can incorporate non-differentiable components that cannot be used with backpropagation, and can explore at levels higher than their action spaces (that is, they can be designed as hierarchical networks for exploration and/or temporal abstraction). However, the coagent framework is not just an alternative to BDL; the two approaches can be blended: BDL can be combined with coagent learning rules to create architectures with the advantages of both approaches. This work generalizes the coagent theory and learning r
    
[^76]: 重新审视离线强化学习的极简方法

    Revisiting the Minimalist Approach to Offline Reinforcement Learning. (arXiv:2305.09836v1 [cs.LG])

    [http://arxiv.org/abs/2305.09836](http://arxiv.org/abs/2305.09836)

    这篇论文提出了一种名为ReBRAC的极简算法，它在TD3+BC方法的基础上整合了设计元素，通过对近期离线强化学习研究的回顾性分析，证明其在离线强化学习上的领先地位。

    

    近年来，离线强化学习取得了显着的进展，出现了许多具有不同复杂度的算法。虽然这些算法带来了显著的改进，但很多算法包含了看似微不足道的设计选择，这些选择对算法的有效性产生了影响，超出了核心算法的进步。然而，这些设计选择对于已有基线算法的影响尚未得到充分研究。在这项工作中，我们旨在通过对近期离线强化学习研究的回顾性分析，提出一种名为ReBRAC的极简算法，该算法在TD3+BC方法的基础上整合了这些设计元素。我们使用D4RL和V-D4RL基准测试评估了ReBRAC在51个具有自我感知和视觉状态空间的数据集上的性能，证明了其在不需要集成的方法中处于领先地位。为了进一步说明这些设计选择的有效性，我们进行了大规模消融研究和超参数敏感性分析，揭示了ReBRAC的成功源于其基于策略改进和评论家正则化的原则性设计选择。

    Recent years have witnessed significant advancements in offline reinforcement learning (RL), resulting in the development of numerous algorithms with varying degrees of complexity. While these algorithms have led to noteworthy improvements, many incorporate seemingly minor design choices that impact their effectiveness beyond core algorithmic advances. However, the effect of these design choices on established baselines remains understudied. In this work, we aim to bridge this gap by conducting a retrospective analysis of recent works in offline RL and propose ReBRAC, a minimalistic algorithm that integrates such design elements built on top of the TD3+BC method. We evaluate ReBRAC on 51 datasets with both proprioceptive and visual state spaces using D4RL and V-D4RL benchmarks, demonstrating its state-of-the-art performance among ensemble-free methods. To further illustrate the efficacy of these design choices, we perform a large-scale ablation study and hyperparameter sensitivity anal
    
[^77]: 基于深度强化学习的边缘资源任务部署和扩展方法用于车载网络服务提供

    A Deep RL Approach on Task Placement and Scaling of Edge Resources for Cellular Vehicle-to-Network Service Provisioning. (arXiv:2305.09832v1 [cs.AI])

    [http://arxiv.org/abs/2305.09832](http://arxiv.org/abs/2305.09832)

    本文提出了一种基于深度强化学习的分散式方法，用于解决车联网服务提供中的任务部署和边缘资源的扩展问题。

    

    “车联网”正处于我们社会数字化转型的前沿。本文提出了一种分散式方法用于提供车辆通联网（C-V2N）服务，解决服务任务部署和边缘资源的扩展问题。我们证明了这个联合问题的复杂性，并提出了一个两个问题的联接方式，采用了基于贪心算法的关于任务部署的方法和基于 Deep Deterministic Policy Gradient (DDPG) 的扩展方法。本文还对我们的方法进行了基准测试，重点关注了扩展代理与多个状态下最先进的扩展方法的性能比较。

    Cellular-Vehicle-to-Everything (C-V2X) is currently at the forefront of the digital transformation of our society. By enabling vehicles to communicate with each other and with the traffic environment using cellular networks, we redefine transportation, improving road safety and transportation services, increasing efficiency of traffic flows, and reducing environmental impact. This paper proposes a decentralized approach for provisioning Cellular Vehicular-to-Network (C-V2N) services, addressing the coupled problems of service task placement and scaling of edge resources. We formalize the joint problem and prove its complexity. We propose an approach to tackle it, linking the two problems, employing decentralized decision-making using (i) a greedy approach for task placement and (ii) a Deep Deterministic Policy Gradient (DDPG) based approach for scaling. We benchmark the performance of our approach, focusing on the scaling agent, against several State-of-the-Art (SoA) scaling approaches
    
[^78]: 自注意力层的拟态初始化

    Mimetic Initialization of Self-Attention Layers. (arXiv:2305.09828v1 [cs.CV])

    [http://arxiv.org/abs/2305.09828](http://arxiv.org/abs/2305.09828)

    本文介绍一种名为拟态初始化的方法，通过仅仅调整自注意力层的权重初始化，即可在视觉任务中大大提高Transformer的精度。

    

    在小规模数据集上训练Transformer十分困难。通常需要以大规模预训练模型作为起点。我们探索了这些预训练Transformer的权重（尤其是用于视觉任务），试图找到造成这种差异的原因。惊讶的是，我们发现仅仅通过初始化自注意力层的权重，使其“看起来”更像预训练模型，就能够更快且更高精度地训练普通Transformer，尤其是在像CIFAR-10和ImageNet分类这样的视觉任务上，我们的精度提高超过5％和4％。我们的初始化方案是闭式的、无需学习的、非常简单：我们将查询和键权重的乘积设置为近似于标识，将值和投影权重的乘积近似于负标识。由于这类似于我们在预训练Transformer中看到的模式，所以我们称为“拟态初始化”技术。

    It is notoriously difficult to train Transformers on small datasets; typically, large pre-trained models are instead used as the starting point. We explore the weights of such pre-trained Transformers (particularly for vision) to attempt to find reasons for this discrepancy. Surprisingly, we find that simply initializing the weights of self-attention layers so that they "look" more like their pre-trained counterparts allows us to train vanilla Transformers faster and to higher final accuracies, particularly on vision tasks such as CIFAR-10 and ImageNet classification, where we see gains in accuracy of over 5% and 4%, respectively. Our initialization scheme is closed form, learning-free, and very simple: we set the product of the query and key weights to be approximately the identity, and the product of the value and projection weights to approximately the negative identity. As this mimics the patterns we saw in pre-trained Transformers, we call the technique "mimetic initialization".
    
[^79]: 一种无需训练的人像图像生成方法

    A Method for Training-free Person Image Picture Generation. (arXiv:2305.09817v1 [cs.CV])

    [http://arxiv.org/abs/2305.09817](http://arxiv.org/abs/2305.09817)

    本文提出一种无需训练的角色图像特征编码器模型，使得用户可以通过简单提供角色的图片，生成匹配期望的图像并调整各种细节，无需为每个个体/动画角色图像单独训练模型。

    

    当前最先进的扩散模型已经在生成图像方面表现出色。然而，这些图片都是单调的，大多数都是训练集中人物图片的分布结果，难以为固定数量的个体生成多张图片。如果要解决这个问题，通常必须通过微调模型。这意味着，如果要绘制每个个体/动画角色图像，必须对其进行训练，而这种训练的硬件和成本常常超出了普通用户的能力，而普通用户实际上占了人数最多的一部分。为了解决这个问题，本文提出的角色图像特征编码器模型使得用户可以通过简单提供角色的图片，使生成的图像中的角色与期望匹配。此外，在过程中可以使用提示调整各种细节。与传统的图像到图像模型不同，本文提出的角色图像特征编码器模型不依赖于特定数据集的训练，因此是一种无需训练的人像图像生成方法。

    The current state-of-the-art Diffusion model has demonstrated excellent results in generating images. However, the images are monotonous and are mostly the result of the distribution of images of people in the training set, making it challenging to generate multiple images for a fixed number of individuals. This problem can often only be solved by fine-tuning the training of the model. This means that each individual/animated character image must be trained if it is to be drawn, and the hardware and cost of this training is often beyond the reach of the average user, who accounts for the largest number of people. To solve this problem, the Character Image Feature Encoder model proposed in this paper enables the user to use the process by simply providing a picture of the character to make the image of the character in the generated image match the expectation. In addition, various details can be adjusted during the process using prompts. Unlike traditional Image-to-Image models, the Ch
    
[^80]: 探索基于生成人工智能的辅助技术在自闭症患者中的应用前景

    Exploring outlooks towards generative AI-based assistive technologies for people with Autism. (arXiv:2305.09815v1 [cs.HC])

    [http://arxiv.org/abs/2305.09815](http://arxiv.org/abs/2305.09815)

    本篇论文探讨了将深度伪造技术作为自闭症患者辅助技术的潜力，并通过 Reddit 评论的调查得出结论。

    

    近年来，生成人工智能在全球范围内引起了广泛关注。生成人工智能的其中一个应用——深度伪造，是一种合成视频。但是，深度伪造的负面应用——虚假新闻和色情内容，却成为了数字生态系统中最流行的两种形式。人们已经开始思考在电影制作、教学等领域中使用深度伪造的优势应用。然而，对于残疾人士中的深度伪造技术潜力的研究却非常少或根本没有。本文探讨了深度伪造作为一项辅助技术的潜力。我们调查了有关 Nvdia 新的视频会议功能的 Reddit 交流，该功能允许参与者在在线会议期间保持目光接触。通过手动网页抓取和定性编码，我们发现了 162 条相关评论，讨论了这项技术对于自闭症人士的相关性和适用性。

    The last few years have significantly increased global interest in generative artificial intelligence. Deepfakes, which are synthetically created videos, emerged as an application of generative artificial intelligence. Fake news and pornographic content have been the two most prevalent negative use cases of deepfakes in the digital ecosystem. Deepfakes have some advantageous applications that experts in the subject have thought of in the areas of filmmaking, teaching, etc. Research on the potential of deepfakes among people with disabilities is, however, scarce or nonexistent. This workshop paper explores the potential of deepfakes as an assistive technology. We examined Reddit conversations regarding Nvdia's new videoconferencing feature which allows participants to maintain eye contact during online meetings. Through manual web scraping and qualitative coding, we found 162 relevant comments discussing the relevance and appropriateness of the technology for people with Autism. The the
    
[^81]: Sasha: 大型语言模型在智能家居中的创意目标导向推理

    Sasha: creative goal-oriented reasoning in smart homes with large language models. (arXiv:2305.09802v1 [cs.HC])

    [http://arxiv.org/abs/2305.09802](http://arxiv.org/abs/2305.09802)

    本论文研究了在智能家居中使用大型语言模型实现用户命令的创意目标导向推理。实验结果显示，这种方法可以创造性地推理，以实现挑战性的目标。

    

    智能家居的使用者与设备的交互都有明确或隐含的目标。现有的家庭助手能够轻松地实现明确的目标，例如"打开灯"。然而，在更自然的交流中，人们往往会描述隐含的目标。例如，我们可以让别人"让房间变得舒适"而不是描述具体的步骤。当前的系统很难解决这种歧义，因为需要将模糊的意图与具体的设备联系起来。我们从通用大型语言模型（LLMs）的角度来解决这个问题，这些模型经过大量语料库的训练，并通过灵活的方式适应下游任务。我们探讨了使用LLMs控制设备并创建自动化程序以满足用户命令的隐含目标。在以用户为中心的研究中，我们发现LLMs可以创造性地推理，以实现挑战性的目标，同时也揭示了降低其有用性的差距。我们使用Sasha解决了这些差距：这是一个在智能家居中使用LLMs进行灵活解释用户命令和设备控制的创意目标导向推理系统。

    Every smart home user interaction has an explicit or implicit goal. Existing home assistants easily achieve explicit goals, e.g., "turn on the light". In more natural communication, however, humans tend to describe implicit goals. We can, for example, ask someone to "make it cozy" rather than describe the specific steps involved. Current systems struggle with this ambiguity since it requires them to relate vague intent to specific devices. We approach this problem of flexibly achieving user goals from the perspective of general-purpose large language models (LLMs) trained on gigantic corpora and adapted to downstream tasks with remarkable flexibility. We explore the use of LLMs for controlling devices and creating automation routines to meet the implicit goals of user commands. In a user-focused study, we find that LLMs can reason creatively to achieve challenging goals, while also revealing gaps that diminish their usefulness. We address these gaps with Sasha: a system for creative, g
    
[^82]: 应用控制李亚普诺夫屏障函数的强化学习安全机器人控制

    Reinforcement Learning for Safe Robot Control using Control Lyapunov Barrier Functions. (arXiv:2305.09793v1 [cs.RO])

    [http://arxiv.org/abs/2305.09793](http://arxiv.org/abs/2305.09793)

    本研究利用控制李亚普诺夫屏障函数及LBAC算法，提出了一种模型无关的强化学习方法，实现了基于数据的安全性和可达性条件下机器人控制。在实际2D四旋翼导航任务中验证该方法的有效性，优于其他模型无关强化学习方法。

    

    当面对复杂的机器人控制任务时，强化学习（RL）展现了优异的性能。然而，由于缺乏强大的安全保障，其在物理机器人上的广泛应用受到了限制。为了克服这一挑战，本文探讨了控制李亚普诺夫屏障函数（CLBF），仅基于数据分析安全性和可达性，而无需明确使用动态模型。我们还提出了LyapunovBarrierActor-Critic（LBAC）算法，这是一种基于模型的强化学习算法，用于寻找满足基于数据的安全和可达性条件的控制器。我们通过仿真和真实机器人控制实验，即2D四旋翼导航任务，展示了所提出方法的有效性。实验结果表明，此方法在可达性和安全性方面的效果优于其他基于模型的强化学习方法。

    Reinforcement learning (RL) exhibits impressive performance when managing complicated control tasks for robots. However, its wide application to physical robots is limited by the absence of strong safety guarantees. To overcome this challenge, this paper explores the control Lyapunov barrier function (CLBF) to analyze the safety and reachability solely based on data without explicitly employing a dynamic model. We also proposed the Lyapunov barrier actor-critic (LBAC), a model-free RL algorithm, to search for a controller that satisfies the data-based approximation of the safety and reachability conditions. The proposed approach is demonstrated through simulation and real-world robot control experiments, i.e., a 2D quadrotor navigation task. The experimental findings reveal this approach's effectiveness in reachability and safety, surpassing other model-free RL methods.
    
[^83]: 边缘智能与自动引导车辆控制的协同设计

    Codesign of Edge Intelligence and Automated Guided Vehicle Control. (arXiv:2305.09788v1 [cs.CV])

    [http://arxiv.org/abs/2305.09788](http://arxiv.org/abs/2305.09788)

    本文介绍了一种自主引导车辆（AGV）控制、边缘智能和人类输入的和谐设计，以实现工业环境中的自主运输，其核心技术是通过无线网络连接人工智能（AI）和AGV实现人机协同。

    

    本文介绍了一种自主引导车辆（AGV）控制、边缘智能和人类输入的和谐设计，以实现工业环境中的自主运输。该AGV具有在源和目标之间导航并拾取/放置物品的能力。人类输入隐含地提供了目的地和准确的卸货点的偏好，这些偏好来自于网络边缘的人工智能（AI）模块，并通过无线网络与AGV共享。演示表明，所提出的硬件、软件和AI设计的综合设计达到了技术成熟度水平（TRL）4-5的范围。

    This work presents a harmonic design of autonomous guided vehicle (AGV) control, edge intelligence, and human input to enable autonomous transportation in industrial environments. The AGV has the capability to navigate between a source and destinations and pick/place objects. The human input implicitly provides preferences of the destination and exact drop point, which are derived from an artificial intelligence (AI) module at the network edge and shared with the AGV over a wireless network. The demonstration indicates that the proposed integrated design of hardware, software, and AI design achieve a technology readiness level (TRL) of range 4-5
    
[^84]: 从对比微调的语言模型中提取语义概念嵌入

    Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned Language Models. (arXiv:2305.09785v1 [cs.CL])

    [http://arxiv.org/abs/2305.09785](http://arxiv.org/abs/2305.09785)

    本论文通过对比学习策略，提高了语言模型的概念嵌入质量，并在各种基准测试中实现了最先进的结果。

    

    学习捕捉概念含义的向量仍然是一个基本挑战。令人惊讶的是，至今预训练的语言模型仅在对这种概念嵌入的质量方面产生了有限的提高。目前的使用语言模型的策略通常通过在某种语料库中平均表示一个概念在其提及中的语境化表示来表示一个概念。这在至少两个方面可能是次优的。首先，语境化的单词向量具有异常的几何性，这影响下游任务。其次，概念嵌入应该捕捉概念的语义属性，而语境化的单词向量也受到其他因素的影响。为了解决这些问题，我们提出了两种基于对比学习策略，基于这样的观点，每当两个句子显示相似的属性时，相应的语境化向量也应该相似。一种策略是完全无监督的，估计在一个句子中表达的属性。

    Learning vectors that capture the meaning of concepts remains a fundamental challenge. Somewhat surprisingly, perhaps, pre-trained language models have thus far only enabled modest improvements to the quality of such concept embeddings. Current strategies for using language models typically represent a concept by averaging the contextualised representations of its mentions in some corpus. This is potentially sub-optimal for at least two reasons. First, contextualised word vectors have an unusual geometry, which hampers downstream tasks. Second, concept embeddings should capture the semantic properties of concepts, whereas contextualised word vectors are also affected by other factors. To address these issues, we propose two contrastive learning strategies, based on the view that whenever two sentences reveal similar properties, the corresponding contextualised vectors should also be similar. One strategy is fully unsupervised, estimating the properties which are expressed in a sentence
    
[^85]: 带有注意力模型的视觉问答算法分析

    Analysis of Visual Question Answering Algorithms with attention model. (arXiv:2305.09782v1 [cs.CV])

    [http://arxiv.org/abs/2305.09782](http://arxiv.org/abs/2305.09782)

    本文批评性地检查和审查了使用共同注意力方法的VQA算法的方法，重点关注文本语义生成、对象识别和答案分类技术。

    

    视觉问答（VQA）使用图像处理算法处理图像，使用自然语言处理方法理解并回答问题。 VQA 对视觉受损者有帮助，可用于安全监控系统和从网络中学习的在线聊天机器人。 它使用自然语言处理方法学习问题的语义并提取文本特征。 计算机视觉技术用于以一种能够识别所问问题涉及的物体的方式生成图像表示。 注意力模型试图模仿人类根据语境关注图像不同区域的行为。 本文批评性地检查和审查了使用共同注意力方法的 VQA 算法的方法，例如生成文本语义，识别对象和答案分类技术。

    Visual question answering (VQA) usesimage processing algorithms to process the image and natural language processing methods to understand and answer the question. VQA is helpful to a visually impaired person, can be used for the security surveillance system and online chatbots that learn from the web. It uses NLP methods to learn the semantic of the question and to derive the textual features. Computer vision techniques are used for generating image representation in such a way that they can identify the objects about which question is asked. The Attention model tries to mimic the human behavior of giving attention to a different region of an image according to our understanding of its context. This paper critically examines and reviews methods of VQA algorithm such as generation of semantics of text, identification of objects and answer classification techniques that use the co-attention approach.
    
[^86]: 一种可扩展的Walsh-Hadamard正则化器，以克服神经网络的低阶谱偏差

    A Scalable Walsh-Hadamard Regularizer to Overcome the Low-degree Spectral Bias of Neural Networks. (arXiv:2305.09779v1 [cs.LG])

    [http://arxiv.org/abs/2305.09779](http://arxiv.org/abs/2305.09779)

    本文提出了一种新型的可扩展Walsh-Hadamard正则化器，可避免神经网络学习低阶频率以及误识别低阶频率，从而提高了神经网络的泛化性能。

    

    尽管神经网络具有学习任意函数的能力，但通过梯度下降训练的模型常常表现出对“更简单”函数的偏好。本文通过傅里叶（Walsh-Hadamard）变换，从离散（零一）输入的神经网络的角度探讨了简单性的概念，其中可以通过傅里叶系数的“阶”来捕捉简单性概念。我们实证表明神经网络有学习较低阶频率的趋势。我们展示了这种谱偏差向较简单特征的趋势实际上会损害神经网络在真实世界数据集上的泛化能力。为了解决这个问题，我们提出了一种新的可扩展的功能正则化方案，以帮助神经网络学习更高的阶频率。我们的正则化器还有助于避免对低阶频率的错误识别，从而进一步提高了泛化能力。我们在计算机视觉、自然语言处理和语音识别中应用各种神经网络架构进行分类任务的广泛评估。我们的实验结果表明，我们的正则化器在低数据量环境下显著提高了泛化性能。

    Despite the capacity of neural nets to learn arbitrary functions, models trained through gradient descent often exhibit a bias towards ``simpler'' functions. Various notions of simplicity have been introduced to characterize this behavior. Here, we focus on the case of neural networks with discrete (zero-one) inputs through the lens of their Fourier (Walsh-Hadamard) transforms, where the notion of simplicity can be captured through the \emph{degree} of the Fourier coefficients. We empirically show that neural networks have a tendency to learn lower-degree frequencies. We show how this spectral bias towards simpler features can in fact \emph{hurt} the neural network's generalization on real-world datasets. To remedy this we propose a new scalable functional regularization scheme that aids the neural network to learn higher degree frequencies. Our regularizer also helps avoid erroneous identification of low-degree frequencies, which further improves generalization. We extensively evaluat
    
[^87]: 基于眼动数据的人类注意力建模在神经源代码摘要中的应用

    Towards Modeling Human Attention from Eye Movements for Neural Source Code Summarization. (arXiv:2305.09773v1 [cs.SE])

    [http://arxiv.org/abs/2305.09773](http://arxiv.org/abs/2305.09773)

    利用眼动数据对人类的注意力进行建模，并将模型应用于基于神经网络的源代码摘要中，预测源代码中最重要的单词并增强了基线模型的预测性能。

    

    神经源代码摘要是使用神经网络生成源代码行为自然语言描述的任务。大多数神经模型的基本组成部分是注意机制。注意机制学习将源代码中的特征与生成自然语言描述时要使用的特定单词连接起来。人类在编码中也会更加关注某些特定的特征。这种人类关注反映了经验和高水平认知，远超任何当前神经模型的能力。本文利用已发布的眼动实验数据创建了人类注意力模型，并预测源代码中最重要的单词，以增强基线神经代码摘要方法。我们观察到增强方法的预测性能有所提升，这与其他生物启发式神经模型的表现相符。

    Neural source code summarization is the task of generating natural language descriptions of source code behavior using neural networks. A fundamental component of most neural models is an attention mechanism. The attention mechanism learns to connect features in source code to specific words to use when generating natural language descriptions. Humans also pay attention to some features in code more than others. This human attention reflects experience and high-level cognition well beyond the capability of any current neural model. In this paper, we use data from published eye-tracking experiments to create a model of this human attention. The model predicts which words in source code are the most important for code summarization. Next, we augment a baseline neural code summarization approach using our model of human attention. We observe an improvement in prediction performance of the augmented approach in line with other bio-inspired neural models.
    
[^88]: ConvXAI：通过对话提供异构的AI解释，支持人机科技写作

    ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])

    [http://arxiv.org/abs/2305.09770](http://arxiv.org/abs/2305.09770)

    ConvXAI是一个基于对话的XAI系统，它集成了多种XAI类型，并将实际用户需求嵌入设计中，以提高实用性。

    

    尽管已经提出了各种各样的人工智能解释（XAI）方法来解释AI系统，但目前的方法是否对人类实用仍存在不一致的发现。为了改善XAI方法的实用性，一系列研究确定了现实世界中多样化和动态的用户需求与现有XAI方法之间的差距。虽然之前的研究设想将多种XAI方法集成到通用XAI界面（例如，基于对话或GUI的XAI系统）中以减轻这些差距，但缺少针对这些系统如何设计以满足实际用户需求的研究。在本研究中，我们提出了ConvXAI，这是一个基于对话的XAI系统，它结合了多种XAI类型，并赋予用户通过通用的XAI对话界面提出各种XAI问题的能力。特别地，我们创新地将实际用户需求（即，基于格式研究的四个原则）嵌入ConvXAI设计中，以提高实用性。

    While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
    
[^89]: CQural：一种基于混合CNN的量子持续机器学习架构

    CQural: A Novel CNN based Hybrid Architecture for Quantum Continual Machine Learning. (arXiv:2305.09738v1 [cs.LG])

    [http://arxiv.org/abs/2305.09738](http://arxiv.org/abs/2305.09738)

    本文展示了一种基于混合CNN的量子持续机器学习架构，可以通过解释哪些特征对于分类最重要来避免遗忘，并声称如果使用这些解释来训练模型，则会获得更好的性能。

    

    在增量式学习中训练机器学习模型不仅很重要，而且是实现人工通用智能的有效方法。然而，当前的神经网络模型在持续学习方面很容易出现灾难性遗忘的问题。许多研究人员提出了许多技术来减少神经网络的遗忘影响，但是所有的技术都是在经典学习上研究的，很少有人关注机器学习模型结构的改变。在本研究中，我们展示了使用新的混合经典 - 量子神经网络可以避免持续学习中的灾难性遗忘，并解释了哪些功能对于分类最重要。此外，我们还声称如果使用这些解释来训练模型，则会获得更好的性能。

    Training machine learning models in an incremental fashion is not only important but also an efficient way to achieve artificial general intelligence. The ability that humans possess of continuous or lifelong learning helps them to not forget previously learned tasks. However, current neural network models are prone to catastrophic forgetting when it comes to continual learning. Many researchers have come up with several techniques in order to reduce the effect of forgetting from neural networks, however, all techniques are studied classically with a very less focus on changing the machine learning model architecture. In this research paper, we show that it is not only possible to circumvent catastrophic forgetting in continual learning with novel hybrid classical-quantum neural networks, but also explains what features are most important to learn for classification. In addition, we also claim that if the model is trained with these explanations, it tends to give better performance and
    
[^90]: ADDSL: 基于标注的丹麦手语的手势检测与识别

    ADDSL: Hand Gesture Detection and Sign Language Recognition on Annotated Danish Sign Language. (arXiv:2305.09736v1 [cs.CV])

    [http://arxiv.org/abs/2305.09736](http://arxiv.org/abs/2305.09736)

    本研究引入了一个新的丹麦手语注释数据集（ADDSL）并使用该数据集训练了一个基于YOLOv5的手势检测和字母数字识别模型，准确率最高可达92%。与同领域现有工作相比，该模型更高效和准确。

    

    长期以来，将手势检测并将其识别为字母或数字一直是一项具有挑战性的任务。这给残障人士带来了沟通障碍。本文介绍了一个新的数据集，即丹麦手语注释数据集（ADDSL）。使用开源工具LabelImg在YOLO格式中制作了数据集的注释。利用此数据集，使用CSP-DarkNet53骨干和YOLOv3头的单阶段目标检测器模型（YOLOv5）通过每类仅使用七个独特的图像（不进行数据增强）来训练，以识别字母（A-Z）和数字（0-9）。训练五个模型，共350个周期，得到每张图像的平均推断时间为9.02ms，与之前的研究相比，最佳准确率为92%。我们的结果表明，修改后的模型比同领域的现有工作更高效和准确。我们模型的代码库可在GitHub存储库https://github.com/s4nyam/pvt-addsl 上获得。

    For a long time, detecting hand gestures and recognizing them as letters or numbers has been a challenging task. This creates communication barriers for individuals with disabilities. This paper introduces a new dataset, the Annotated Dataset for Danish Sign Language (ADDSL). Annota-tions for the dataset were made using the open-source tool LabelImg in the YOLO format. Using this dataset, a one-stage ob-ject detector model (YOLOv5) was trained with the CSP-DarkNet53 backbone and YOLOv3 head to recognize letters (A-Z) and numbers (0-9) using only seven unique images per class (without augmen-tation). Five models were trained with 350 epochs, resulting in an average inference time of 9.02ms per image and a best accu-racy of 92% when compared to previous research. Our results show that modified model is efficient and more accurate than existing work in the same field. The code repository for our model is available at the GitHub repository https://github.com/s4nyam/pvt-addsl.
    
[^91]: FedHGN：异构图神经网络的联邦学习框架

    FedHGN: A Federated Framework for Heterogeneous Graph Neural Networks. (arXiv:2305.09729v1 [cs.LG])

    [http://arxiv.org/abs/2305.09729](http://arxiv.org/abs/2305.09729)

    FedHGN是一种用于异构图神经网络的联邦学习框架，它采用模式权重解耦和系数对齐技术，使得不同客户端可以共享知识而不泄露隐私，相比于现有方法表现更加优秀。

    

    与传统GNN相比，异构图神经网络（HGNN）可以更有效地从类型化和关系化图数据中学习。由于隐私法规（例如GDPR），实际应用中的训练数据往往很少，而使用更大的参数空间可能需要更多的训练数据。联邦图学习（FGL）使多个客户端共同训练GNN而不共享本地数据。然而，现有的FGL方法主要集中在同构GNN或知识图嵌入上；很少考虑异构图和HGNN。在联邦异构图学习中，客户端可能拥有私有图模式，尝试定义全局HGNN模型的传统FL/FGL方法会侵犯模式隐私。为了解决这些挑战，我们提出了FedHGN，一种新颖的HGNN FGL框架。FedHGN采用模式权重解耦来实现独立于模式的知识共享，并采用系数对齐来稳定训练过程和提高HGNN泛化能力。我们在合成和现实数据集上进行了广泛的实验，证明了FedHGN相对于现有的最先进方法的有效性。

    Heterogeneous graph neural networks (HGNNs) can learn from typed and relational graph data more effectively than conventional GNNs. With larger parameter spaces, HGNNs may require more training data, which is often scarce in real-world applications due to privacy regulations (e.g., GDPR). Federated graph learning (FGL) enables multiple clients to train a GNN collaboratively without sharing their local data. However, existing FGL methods mainly focus on homogeneous GNNs or knowledge graph embeddings; few have considered heterogeneous graphs and HGNNs. In federated heterogeneous graph learning, clients may have private graph schemas. Conventional FL/FGL methods attempting to define a global HGNN model would violate schema privacy. To address these challenges, we propose FedHGN, a novel and general FGL framework for HGNNs. FedHGN adopts schema-weight decoupling to enable schema-agnostic knowledge sharing and employs coefficients alignment to stabilize the training process and improve HGNN
    
[^92]: 基于因果关系解释的扩散变分图神经网络用于时空预测

    Dynamic Causal Explanation Based Diffusion-Variational Graph Neural Network for Spatio-temporal Forecasting. (arXiv:2305.09703v1 [cs.LG])

    [http://arxiv.org/abs/2305.09703](http://arxiv.org/abs/2305.09703)

    本文提出了一种新颖的基于因果关系解释的扩散变分图神经网络，用于时空预测，在动态图构建上考虑了邻居节点之间的因果关系和不确定性，解决了动态图算法的可解释性和稳定性问题。

    

    图神经网络，特别是动态图神经网络，已经成为时空预测问题中的研究热点。虽然许多动态图构建方法已经被开发，但其中相对较少的探索了邻居节点之间的因果关系。因此，由此产生的模型缺乏对动态生成图的邻居节点之间因果关系的强大可解释性，这很容易导致后续决策的风险。此外，很少有人考虑基于时间序列数据集的动态图的不确定性和噪声，而这在真实世界的图结构网络中是普遍存在的。在本文中，我们提出了一种新颖的基于扩散变分图神经网络（DVGNN）的时空预测方法。对于动态图构建，设计了一种无监督生成模型。在编码器阶段，应用两层图卷积网络（GCN）来计算潜在节点嵌入的后验分布。

    Graph neural networks (GNNs), especially dynamic GNNs, have become a research hotspot in spatio-temporal forecasting problems. While many dynamic graph construction methods have been developed, relatively few of them explore the causal relationship between neighbour nodes. Thus, the resulting models lack strong explainability for the causal relationship between the neighbour nodes of the dynamically generated graphs, which can easily lead to a risk in subsequent decisions. Moreover, few of them consider the uncertainty and noise of dynamic graphs based on the time series datasets, which are ubiquitous in real-world graph structure networks. In this paper, we propose a novel Dynamic Diffusion-Variational Graph Neural Network (DVGNN) for spatio-temporal forecasting. For dynamic graph construction, an unsupervised generative model is devised. Two layers of graph convolutional network (GCN) are applied to calculate the posterior distribution of the latent node embeddings in the encoder sta
    
[^93]: 生成式表格预训练增强了表格预测模型

    Generative Table Pre-training Empowers Models for Tabular Prediction. (arXiv:2305.09696v1 [cs.LG])

    [http://arxiv.org/abs/2305.09696](http://arxiv.org/abs/2305.09696)

    本文提出了TapTap，一种通过表格预训练生成高质量合成表格来提高表格预测性能的方法。在12个数据集实验中，TapTap在不同场景下优于16个基线，并可以与多个骨干模型结合使用。

    

    近年来，表格预训练已经成为研究的热点，但如何利用表格预训练来提高表格预测的性能仍然是一个开放性挑战。本文提出了TapTap，这是第一个利用表格预训练来增强表格预测模型的尝试。在对大量实际世界的表格数据进行预训练后，TapTap能够生成高质量的合成表格，以支持各种表格数据应用，包括隐私保护、低资源环境、缺失值插补和失衡分类。在12个数据集上的广泛实验表明，TapTap在不同场景下优于16个基线。同时，它可以轻松地与各种骨干模型结合使用，包括LightGBM、多层感知机（MLP）和Transformer。此外，在表格预训练的帮助下，使用TapTap生成的合成数据进行训练的模型甚至可以与使用原始真实数据的模型竞争，实现了相当甚至更好的性能。

    Recently, the topic of table pre-training has attracted considerable research interest. However, how to employ table pre-training to boost the performance of tabular prediction remains an open challenge. In this paper, we propose TapTap, the first attempt that leverages table pre-training to empower models for tabular prediction. After pre-training on a large corpus of real-world tabular data, TapTap can generate high-quality synthetic tables to support various applications on tabular data, including privacy protection, low resource regime, missing value imputation, and imbalanced classification. Extensive experiments on 12 datasets demonstrate that TapTap outperforms a total of 16 baselines in different scenarios. Meanwhile, it can be easily combined with various backbone models, including LightGBM, Multilayer Perceptron (MLP) and Transformer. Moreover, with the aid of table pre-training, models trained using synthetic data generated by TapTap can even compete with models using the or
    
[^94]: 带衰减函数的时间序列异常检测评估策略

    Evaluation Strategy of Time-series Anomaly Detection with Decay Function. (arXiv:2305.09691v1 [cs.LG])

    [http://arxiv.org/abs/2305.09691](http://arxiv.org/abs/2305.09691)

    本文提出了带衰减函数的点调整协议（PAdf）以解决现有时间序列异常检测算法评估方式高估或低估性能的问题。通过在基准数据集上的重新评估，我们发现PAdf协议不仅考虑要查找尽可能多的段数，还考虑快速准确地检测异常。

    

    近期，时间序列异常检测算法一般采用点调整协议来评估其性能。然而，这种协议容易高估检测算法的性能，因为它只考虑检测到的异常段数和大小。本文提出了一种新的评估协议——带衰减函数的点调整协议（PAdf），以评估时间序列异常检测算法的性能。该协议考虑了快速准确地检测异常和避免误报的理想要求。本文从理论和实验两个方面证明了PAdf协议解决了现有协议如PA和PA\%K等的高估和低估问题。通过在基准数据集上重新评估SOTA模型，我们发现PA协议只考虑查找尽可能多的异常段，而PAdf协议则不仅考虑查找尽可能多的段数，同时还考虑快速检测异常。

    Recent algorithms of time-series anomaly detection have been evaluated by applying a Point Adjustment (PA) protocol. However, the PA protocol has a problem of overestimating the performance of the detection algorithms because it only depends on the number of detected abnormal segments and their size. We propose a novel evaluation protocol called the Point-Adjusted protocol with decay function (PAdf) to evaluate the time-series anomaly detection algorithm by reflecting the following ideal requirements: detect anomalies quickly and accurately without false alarms. This paper theoretically and experimentally shows that the PAdf protocol solves the over- and under-estimation problems of existing protocols such as PA and PA\%K. By conducting re-evaluations of SOTA models in benchmark datasets, we show that the PA protocol only focuses on finding many anomalous segments, whereas the score of the PAdf protocol considers not only finding many segments but also detecting anomalies quickly witho
    
[^95]: 数据偏差管理

    Data Bias Management. (arXiv:2305.09686v1 [cs.LG])

    [http://arxiv.org/abs/2305.09686](http://arxiv.org/abs/2305.09686)

    本文讲述了数据偏差在机器学习中的应用、影响及可能的解决方案

    

    鉴于数据驱动系统在我们日常生活中的广泛应用，偏差和公平等概念在科研人员和从业人员，无论是在产业界还是学术界中，都受到了重视。这些问题通常源于用于训练机器学习系统的数据质量不同。随着这些系统被商业化和部署，有时被委托做出改变生活的决策，人们正在做出重大努力来确定和消除可能导致数据偏差的来源。本文提供了研究结果，展示数据偏见如何影响最终用户，偏差的起源以及我们应该如何解决该问题。我们认为，不必在所有情况下消除数据偏差，而是应将研究重点转向偏见的识别。

    Due to the widespread use of data-powered systems in our everyday lives, concepts like bias and fairness gained significant attention among researchers and practitioners, in both industry and academia. Such issues typically emerge from the data, which comes with varying levels of quality, used to train supervised machine learning systems. With the commercialization and deployment of such systems that are sometimes delegated to make life-changing decisions, significant efforts are being made towards the identification and removal of possible sources of data bias that may resurface to the final end user or in the decisions being made. In this paper, we present research results that show how bias in data affects end users, where bias is originated, and provide a viewpoint about what we should do about it. We argue that data bias is not something that should necessarily be removed in all cases, and that research attention should instead shift from bias removal towards the identification, m
    
[^96]: 基于决策迭代算法的脆弱水印技术用于模型完整性验证

    Decision-based iterative fragile watermarking for model integrity verification. (arXiv:2305.09684v1 [cs.CR])

    [http://arxiv.org/abs/2305.09684](http://arxiv.org/abs/2305.09684)

    该论文提出了一种基于决策迭代算法的脆弱水印技术，能将普通训练样本转化为对模型更敏感的脆弱样本，用于模型完整性验证。

    

    基于云服务器的服务需求，基础模型通常部署在云服务器上。然而，这也将模型暴露在安全风险中，攻击者可以在上传或传输到云服务器后修改它们。为了解决这个问题，我们提出了一种迭代的、基于决策的脆弱性水印算法，将普通训练样本转化为对模型更敏感的脆弱样本。在验证过程中，我们将原始模型的敏感样本输出与被攻击模型的输出进行比较，以评估模型的完整性。所提出的脆弱水印算法是一个优化问题，旨在最小化将转换样本输入到目标模型时预测概率分布的方差。我们通过多次迭代将普通样本转换为脆弱样本。我们的方法具有一些优点：（1）决策黑匣子方式下的迭代更新样本；（2）算法应用在模型完整性验证领域，而非加密领域。

    Typically, foundation models are hosted on cloud servers to meet the high demand for their services. However, this exposes them to security risks, as attackers can modify them after uploading to the cloud or transferring from a local system. To address this issue, we propose an iterative decision-based fragile watermarking algorithm that transforms normal training samples into fragile samples that are sensitive to model changes. We then compare the output of sensitive samples from the original model to that of the compromised model during validation to assess the model's completeness.The proposed fragile watermarking algorithm is an optimization problem that aims to minimize the variance of the predicted probability distribution outputed by the target model when fed with the converted sample.We convert normal samples to fragile samples through multiple iterations. Our method has some advantages: (1) the iterative update of samples is done in a decision-based black-box manner, relying s
    
[^97]: 使用双阶段深度学习模型进行漏洞检测

    Vulnerability Detection Using Two-Stage Deep Learning Models. (arXiv:2305.09673v1 [cs.CR])

    [http://arxiv.org/abs/2305.09673](http://arxiv.org/abs/2305.09673)

    本文提出了一种双阶段解决方案，采用了两个深度学习模型用于漏洞检测，其可在识别和分类各种类型的漏洞方面达到高准确率，优于传统SAST和DAST方法。

    

    应用程序安全是现代软件开发的重要组成部分，许多攻击取决于软件中的漏洞。由于技术进步，攻击数量正在全球范围内增加。公司必须在开发、测试和部署软件的每个阶段中都包含安全功能，以防止数据泄露。检测软件漏洞的方法有许多种，如非AI方法(如SAST和DAST)。然而，这些方法存在大量的误报和漏报。与此相对，研究人员一直致力于开发基于AI的漏洞检测系统，采用了BERT、BLSTM等深度学习模型。在本文中，我们提出了一种双阶段解决方案，提出了两个深度学习模型，用于C/C++源代码的漏洞检测。第一阶段是CNN，用于检测源代码是否包含任何漏洞(二元分类)，第二阶段是基于LSTM的模型，用于识别特定类型的漏洞(多类分类)。所提出的解决方案在检测和分类各种类型的漏洞方面实现了高精度率，优于传统的SAST和DAST方法。

    Application security is an essential part of developing modern software, as lots of attacks depend on vulnerabilities in software. The number of attacks is increasing globally due to technological advancements. Companies must include security in every stage of developing, testing, and deploying their software in order to prevent data breaches. There are several methods to detect software vulnerability Non-AI-based such as Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST). However, these approaches have substantial false-positive and false-negative rates. On the other side, researchers have been interested in developing an AI-based vulnerability detection system employing deep learning models like BERT, BLSTM, etc. In this paper, we proposed a two-stage solution, two deep learning models were proposed for vulnerability detection in C/C++ source codes, the first stage is CNN which detects if the source code contains any vulnerability (binary class
    
[^98]: 声明提示下的可满足性辅助语言模型

    Satisfiability-Aided Language Models Using Declarative Prompting. (arXiv:2305.09656v1 [cs.CL])

    [http://arxiv.org/abs/2305.09656](http://arxiv.org/abs/2305.09656)

    本文提出了一种利用自动定理证明器和声明性任务规范的可满足性辅助语言建模方法，可以提高大型语言模型的推理能力。

    

    本文提出了一种新的可满足性辅助语言建模方法，用于提高大型语言模型的推理能力。我们使用一个大型语言模型生成一个声明性任务规范，并利用一个现成的自动定理证明器得出最终答案。该方法具有两个关键优点：第一，声明性规范比推理步骤更接近问题描述，因此大型语言模型可以更准确地解析它；第二，通过将实际推理任务委托给自动定理证明器，我们的方法可以保证正确性。

    Prior work has combined chain-of-thought prompting in large language models (LLMs) with programmatic representations to perform effective and transparent reasoning. While such an approach works very well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving tasks that require more sophisticated planning and search. In this paper, we propose a new satisfiability-aided language modeling approach for improving the reasoning capabilities of LLMs. We use an LLM to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer. This approach has two key advantages. The declarative specification is closer to the problem description than the reasoning steps are, so the LLM can parse it more accurately. Furthermore, by offloading the actual reasoning task to an automated theorem prover, our approach can guarantee the correctness o
    
[^99]: 交叉门控多层感知机下的蛋白质复合物不变嵌入是一种一次性抗体设计器

    Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.09480](http://arxiv.org/abs/2305.09480)

    本文提出了一种深度生成模型，可以一次性地共同设计抗体CDR的1D序列和3D结构，解决几何建模和低效推断的问题。

    

    抗体是由免疫系统产生的针对外来物质或抗原的重要蛋白质。抗体的特异性由其互补决定区（CDR）决定，CDR位于抗体链的可变区域中，形成与抗原结合的位点。以往的研究利用复杂的技术生成CDR，但它们遭受了几何建模不足的问题。此外，常见的迭代精化策略导致了低效的推断。本文提出了一种深度生成模型，可以一次性地共同设计CDR的1D序列和3D结构。为了实现这一目标，我们将抗体CDR设计分为两个阶段：（i）蛋白质结构的几何建模和（ii）序列结构共学习。我们开发了一种蛋白质复合物不变嵌入，可捕捉蛋白质骨架原子（包括Cα、N、C和O原子）之间的内部和外部组分相互作用，以实现全面的几何建模。

    Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
    
[^100]: 多通道电阻抗血流动力学监测中的自动信号质量评估的无监督序列到序列学习

    Unsupervised sequence-to-sequence learning for automatic signal quality assessment in multi-channel electrical impedance-based hemodynamic monitoring. (arXiv:2305.09368v1 [eess.SP])

    [http://arxiv.org/abs/2305.09368](http://arxiv.org/abs/2305.09368)

    本论文提出了一种无监督序列到序列学习方法，用于自动评估多通道电阻抗血流动力学监测中心肺容积信号（CVS）的运动诱导可靠性降低。通过利用长短时记忆和变分自编码器结构，编码器 - 解码器模型可以捕捉到暂态 CVS 序列中存在的上下文知识。与现有方法相比，本方法不需要手动注释运动影响以及缺乏在 CVS 随时间出现上下文变化的情况下实现运动引起异常的显式机制，具有竞争性能。

    

    本研究提出了一种无监督的序列到序列学习方法，用于自动评估多通道电阻抗血流动力学监测中心肺容积信号（CVS）的运动诱导可靠性降低。该方法试图解决现有学习型评估方法的缺点，例如需要手动注释运动影响以及缺乏在 CVS 随时间出现上下文变化的情况下实现运动引起异常的显式机制。通过利用长短时记忆和变分自编码器结构，编码器 - 解码器模型不仅被训练成自我重现 CVS 的输入序列，而且还被训练成以平行方式外推未来。通过这样做，模型可以捕捉到暂态 CVS 序列中存在的上下文知识，同时被规范化以探索整个时间序列的一般关系。根据实际和预测 CVS 之间的差异，检测出低质量的受运动影响的 CVS。多通道 EIT 基心血管监测的大规模数据集实验表明，该方法在运动引起的测量质量评估方法的竞争性能上具有竞争力。

    This study proposes an unsupervised sequence-to-sequence learning approach that automatically assesses the motion-induced reliability degradation of the cardiac volume signal (CVS) in multi-channel electrical impedance-based hemodynamic monitoring. The proposed method attempts to tackle shortcomings in existing learning-based assessment approaches, such as the requirement of manual annotation for motion influence and the lack of explicit mechanisms for realizing motion-induced abnormalities under contextual variations in CVS over time. By utilizing long-short term memory and variational auto-encoder structures, an encoder--decoder model is trained not only to self-reproduce an input sequence of the CVS but also to extrapolate the future in a parallel fashion. By doing so, the model can capture contextual knowledge lying in a temporal CVS sequence while being regularized to explore a general relationship over the entire time-series. A motion-influenced CVS of low-quality is detected, ba
    
[^101]: BERTTM: 利用来自预训练语言模型的上下文化词向量进行神经主题建模

    BERTTM: Leveraging Contextualized Word Embeddings from Pre-trained Language Models for Neural Topic Modeling. (arXiv:2305.09329v1 [cs.CL])

    [http://arxiv.org/abs/2305.09329](http://arxiv.org/abs/2305.09329)

    本文提出了一种新颖的神经主题模型，利用来自预训练语言模型BERT的上下文化词嵌入，可以在不使用任何BoW信息的情况下推断出文档的主题分布，并直接从上下文化词嵌入中推断出文档中每个单词的主题分布。实验结果表明，该模型优于仅依赖BoW表示和其他神经主题模型的现有最先进方法。

    

    随着近年来神经主题模型的发展，主题建模在自然语言理解中扮演着日益重要的角色。然而，大多数现有的主题模型仍然依赖于词袋（BoW）信息，无论是作为训练输入还是训练目标。这限制了它们捕捉文档中的单词顺序信息的能力，并导致它们在处理新文档中的未观察到的单词时遇到困难。预训练语言模型中的上下文化词向量在词义消歧的能力上表现优越，并证明了它们在处理OOV单词时是有效的。在这项工作中，我们开发了一种新颖的神经主题模型，结合了预训练语言模型BERT的上下文化词嵌入。该模型可以在不使用任何BoW信息的情况下推断出文档的主题分布。此外，该模型可以直接从上下文化词嵌入中推断出文档中每个单词的主题分布。基准数据集的实验表明，我们的模型优于仅依赖BoW表示和其他神经主题模型的现有最先进方法。

    With the development of neural topic models in recent years, topic modelling is playing an increasingly important role in natural language understanding. However, most existing topic models still rely on bag-of-words (BoW) information, either as training input or training target. This limits their ability to capture word order information in documents and causes them to suffer from the out-of-vocabulary (OOV) issue, i.e. they cannot handle unobserved words in new documents. Contextualized word embeddings from pre-trained language models show superiority in the ability of word sense disambiguation and prove to be effective in dealing with OOV words. In this work, we developed a novel neural topic model combining contextualized word embeddings from the pre-trained language model BERT. The model can infer the topic distribution of a document without using any BoW information. In addition, the model can infer the topic distribution of each word in a document directly from the contextualize
    
[^102]: SuSana Distance是你所需要的：通过两种新的基于距离的损失函数实现度量学习中的类别可分性，用于少样本图像分类

    SuSana Distancia is all you need: Enforcing class separability in metric learning via two novel distance-based loss functions for few-shot image classification. (arXiv:2305.09062v1 [cs.CV])

    [http://arxiv.org/abs/2305.09062](http://arxiv.org/abs/2305.09062)

    本文提出了两种新的基于距离的损失函数，通过考虑少量数据之间的类内距离和类间距离来考虑嵌入向量的重要性，以增强度量学习中的类别可分性，实现优秀的少样本图像分类表现。

    

    少样本学习是一项具有挑战性的研究领域，旨在仅利用少量标记数据样本学习新的概念。基于度量学习方法的最近工作利用元学习方法，其中包括支持（训练）和查询集（测试），旨在学习这些集合之间的相似性比较度量。由于数据缺乏，嵌入网络的学习过程成为少样本任务的重要部分。先前的工作使用度量学习方法解决了这个问题，但底层潜在空间的属性和类别之间的可分性并未完全强制执行。在这项工作中，我们提出了两种不同的损失函数，通过考虑少量数据之间的类内距离和类间距离来考虑嵌入向量的重要性。第一个损失函数是Proto-Triplet Loss，它基于原始三元组损失，并添加了原型向量。第二个损失函数是SuSana Distance Loss，它考虑了同类样本和不同类样本之间的距离。我们在两个少样本图像分类的基准数据集上评估了我们提出的方法，实验结果证明了我们的方法在实现类别可分性和达到最先进性能方面的有效性。

    Few-shot learning is a challenging area of research that aims to learn new concepts with only a few labeled samples of data. Recent works based on metric-learning approaches leverage the meta-learning approach, which is encompassed by episodic tasks that make use a support (training) and query set (test) with the objective of learning a similarity comparison metric between those sets. Due to the lack of data, the learning process of the embedding network becomes an important part of the few-shot task. Previous works have addressed this problem using metric learning approaches, but the properties of the underlying latent space and the separability of the difference classes on it was not entirely enforced. In this work, we propose two different loss functions which consider the importance of the embedding vectors by looking at the intra-class and inter-class distance between the few data. The first loss function is the Proto-Triplet Loss, which is based on the original triplet loss with 
    
[^103]: 基于模块化动作程序的动作问答

    Motion Question Answering via Modular Motion Programs. (arXiv:2305.08953v1 [cs.CV])

    [http://arxiv.org/abs/2305.08953](http://arxiv.org/abs/2305.08953)

    提出了一种新的HumanMotionQA任务，用于评估模型在复杂的人体运动序列上进行复杂、多步推理的能力。同时，提出了一种特殊的NSPose方法，利用符号化推理和模块化设计，成功地应用于该任务中，并超过所有基线方法。

    

    为了构建能够感知和理解真实世界中的人类行为的人工智能系统，我们必须首先设计模型，对动作序列进行复杂的时空推理。为了实现这个目标，我们提出了HumanMotionQA任务，评估模型在长时间人类运动序列上进行复杂、多步推理的能力。我们生成了一个问答对数据集，需要在动作序列的小部分中检测运动线索，对事件发生的时间进行推理，并查询特定的运动属性。此外，我们还提出了NSPose，一种神经符号方法，用于处理该任务，它利用符号化推理和模块化设计，通过学习运动概念、属性神经操作符和时间关系，来处理动作。我们证明了NSPose在HumanMotionQA任务中的适用性，胜过了所有基线方法。

    In order to build artificial intelligence systems that can perceive and reason with human behavior in the real world, we must first design models that conduct complex spatio-temporal reasoning over motion sequences. Moving towards this goal, we propose the HumanMotionQA task to evaluate complex, multi-step reasoning abilities of models on long-form human motion sequences. We generate a dataset of question-answer pairs that require detecting motor cues in small portions of motion sequences, reasoning temporally about when events occur, and querying specific motion attributes. In addition, we propose NSPose, a neuro-symbolic method for this task that uses symbolic reasoning and a modular design to ground motion through learning motion concepts, attribute neural operators, and temporal relations. We demonstrate the suitability of NSPose for the HumanMotionQA task, outperforming all baseline methods.
    
[^104]: 神经符号人工智能及其分类法：一项调查研究

    Neurosymbolic AI and its Taxonomy: a survey. (arXiv:2305.08876v1 [cs.NE])

    [http://arxiv.org/abs/2305.08876](http://arxiv.org/abs/2305.08876)

    本文调查研究了神经符号人工智能的研究，探索了学习数据分布和推理先前和学习知识相结合的方法，以探索实现人工智能通用性的替代方案。

    

    神经符号人工智能涉及组合符号处理（如经典人工智能）和神经网络的模型，是一个非常成熟的领域。这些模型作为实现人工智能通用性的一种尝试，在探索除了增加数据集和模型尺寸以外的替代方案以及将学习数据分布和推理先前和学习知识相结合方面具有独特作用。本次调查研究了这一领域近年来的研究论文，并提供了这些模型的分类和比较，同时介绍了应用案例。

    Neurosymbolic AI deals with models that combine symbolic processing, like classic AI, and neural networks, as it's a very established area. These models are emerging as an effort toward Artificial General Intelligence (AGI) by both exploring an alternative to just increasing datasets' and models' sizes and combining Learning over the data distribution, Reasoning on prior and learned knowledge, and by symbiotically using them. This survey investigates research papers in this area during recent years and brings classification and comparison between the presented models as well as applications.
    
[^105]: 基于深度学习的单目航天器姿态估计综述：当前状态、限制和前景。

    A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects. (arXiv:2305.07348v1 [cs.CV])

    [http://arxiv.org/abs/2305.07348](http://arxiv.org/abs/2305.07348)

    本文评估了基于深度学习的单目航天器姿态估计的现有方法，总结了部署该方法在现实任务中仍需克服的挑战。

    

    估算不配合的航天器姿态是一项重要的计算机视觉问题，可以实现在轨自动视觉系统的部署，其应用范围从轨道维修到太空碎片清除。随着计算机视觉的趋势，越来越多的工作在利用深度学习（DL）方法来解决此问题。但是尽管在研究阶段获得了有希望的结果，仍存在阻止这种方法在现实任务中使用的主要挑战。特别是，部署这种计算密集型算法仍然受到少量研究，而在合成图像上进行训练和在真实图像上进行测试时性能下降仍然需要减轻。本文主要目的是全面描述当前基于DL的航天器姿态估计方法，辅助确定有效部署DL的航天器姿态估计在现实任务中的限制。

    Estimating the pose of an uncooperative spacecraft is an important computer vision problem for enabling the deployment of automatic vision-based systems in orbit, with applications ranging from on-orbit servicing to space debris removal. Following the general trend in computer vision, more and more works have been focusing on leveraging Deep Learning (DL) methods to address this problem. However and despite promising research-stage results, major challenges preventing the use of such methods in real-life missions still stand in the way. In particular, the deployment of such computation-intensive algorithms is still under-investigated, while the performance drop when training on synthetic and testing on real images remains to mitigate. The primary goal of this survey is to describe the current DL-based methods for spacecraft pose estimation in a comprehensive manner. The secondary goal is to help define the limitations towards the effective deployment of DL-based spacecraft pose estimat
    
[^106]: 探索机器遗忘的领域：一篇综述与分类

    Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])

    [http://arxiv.org/abs/2305.06360](http://arxiv.org/abs/2305.06360)

    本文综述了机器遗忘的现状和技术应用，包括数据删除、扰动和模型更新，讨论了MU在隐私、安全和公正性等领域的潜在益处，以及它在自然语言处理、计算机视觉和推荐系统中的未来发展方向。

    

    机器遗忘是一个越来越受关注的领域，因为需要删除或修改机器学习模型所做出的预测。虽然训练模型变得更加有效和准确，但在某些领域（如隐私、安全和公正性），遗忘先前学到的信息的重要性变得越来越显著。本文介绍了机器遗忘的综述，涵盖了当前最先进的技术和方法，包括数据删除、扰动和模型更新。此外，文中还介绍了常用的度量标准和数据集。文章还强调了需要解决的挑战，包括攻击复杂性、标准化、可转移性、可解释性、训练数据和资源限制。本文的贡献包括讨论MU的潜在益处以及它在自然语言处理、计算机视觉和推荐系统中的未来方向。

    Machine unlearning (MU) is a field that is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions in Natural Language Processing, Computer vision, and Recommender Systems. Additionally, 
    
[^107]: 多目标优化的逆强化学习的收敛性证明研究

    A proof of convergence of inverse reinforcement learning for multi-objective optimization. (arXiv:2305.06137v1 [cs.LG])

    [http://arxiv.org/abs/2305.06137](http://arxiv.org/abs/2305.06137)

    本论文证明了多目标优化的逆强化学习方法在理论层面上的收敛性，包括Wasserstein逆强化学习和常规逆强化学习方法。

    

    本文通过将等效于多目标优化的WIRL问题的逆问题与投影次梯度法相结合，证明了Wasserstein逆强化学习（WIRL）在多目标优化中的收敛性。此外，我们还证明了逆强化学习（最大熵逆强化学习，导引成本学习）在多目标优化中的收敛性。

    We show the convergence of Wasserstein inverse reinforcement learning (WIRL) for multi-objective optimizations with the projective subgradient method by formulating an inverse problem of the optimization problem that is equivalent to WIRL for multi-objective optimizations.  In addition, we prove convergence of inverse reinforcement learning (maximum entropy inverse reinforcement learning, guid cost learning) for multi-objective optimization with the projective subgradient method.
    
[^108]: 数据集压缩综合研究：性能、隐私、鲁棒性以及公平性

    A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness. (arXiv:2305.03355v1 [cs.LG])

    [http://arxiv.org/abs/2305.03355](http://arxiv.org/abs/2305.03355)

    本研究对当前最先进的数据集压缩方法进行了全面评估，发现其存在隐私风险并可能放大模型的不公平性，提供了大规模的基准测试框架。

    

    数据集压缩旨在将原始数据集的丰富特征编码成小型数据集，是一种加速神经网络训练和相关研究的有前途的方法。已经提出了不同的方法来改善压缩图像的信息性和泛化性能。然而，目前还没有从安全性角度全面分析这一技术的工作，并且对潜在风险缺乏系统理解。在本文中，我们进行了大量实验，评估了当前最先进的数据集压缩方法。我们成功使用成员推理攻击来显示仍然存在隐私风险。本文还表明，数据集压缩在模型鲁棒性方面可能会产生不同程度的影响，并在进行预测时放大类别间的模型不公平性。本研究为数据集压缩评估提供了大规模的基准测试框架。

    The aim of dataset distillation is to encode the rich features of an original dataset into a tiny dataset. It is a promising approach to accelerate neural network training and related studies. Different approaches have been proposed to improve the informativeness and generalization performance of distilled images. However, no work has comprehensively analyzed this technique from a security perspective and there is a lack of systematic understanding of potential risks. In this work, we conduct extensive experiments to evaluate current state-of-the-art dataset distillation methods. We successfully use membership inference attacks to show that privacy risks still remain. Our work also demonstrates that dataset distillation can cause varying degrees of impact on model robustness and amplify model unfairness across classes when making predictions. This work offers a large-scale benchmarking framework for dataset distillation evaluation.
    
[^109]: 运用自我记忆的检索增强文本生成模型

    Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory. (arXiv:2305.02437v1 [cs.CL])

    [http://arxiv.org/abs/2305.02437](http://arxiv.org/abs/2305.02437)

    本文提出了一种新的检索增强文本生成模型Selfmem，通过迭代生成自我记忆池并采用记忆选择器，使检索更加自适应，提高了文本生成的质量和多样性。

    

    相较于传统文本生成模型，检索增强文本生成模型能够直接迭代人类编写的参考库，并从中检索出相应的信息，以生成更优质的文本。但当前文献存在一个关键问题：检索到的记忆来自于固定的语料库，其质量存在一定局限性，可能会限制记忆增强模型的潜力。本文提出一种名为Selfmem的框架，该框架通过迭代地采用检索增强生成器自身以生成无限制的自我记忆池，并使用记忆选择器为下一轮生成选择一个生成的记忆。相结合，这两个主要问题提出了运用自我记忆的检索增强文本生成模型。

    With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation~(we define this as primal problem), previous works mainly focus on how to retrieve better memory. However, one fundamental limitation exists for current literature: the memory is retrieved from a fixed corpus and is bounded by the quality of the corpus. Due to the finite retrieval space, bounded memory would greatly limit the potential of the memory-augmented generation model. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a framework called Selfmem, which iteratively adopts a retrieval-augmented generator itself to generate an unbounded memory pool and uses a memory selector to pick one generated memory for the next generation round. By combining the primal and dual problem, a retrieval-augmented ge
    
[^110]: ChatGPT - 对于本科计算机科学学生和教师是福是祸?

    ChatGPT -- a Blessing or a Curse for Undergraduate Computer Science Students and Instructors?. (arXiv:2304.14993v1 [cs.HC])

    [http://arxiv.org/abs/2304.14993](http://arxiv.org/abs/2304.14993)

    本文分析了ChatGPT在回答本科计算机科学问题上的不可靠性，并提供了在学术界使用ChatGPT的建议。

    

    ChatGPT是由OpenAI开发的AI语言模型，可以理解和生成类人文本。它可用于语言生成、问答、文本摘要、聊天机器人开发、语言翻译、情感分析、内容创作、个性化、文本完成和故事叙述等多种用途。虽然ChatGPT受到了相当积极的关注，但在学术界也引起了一种担忧和不确定感。存在担忧学生可能会利用ChatGPT完成课外作业和考试，并获得有利的成绩，而不真正获得知识。本文采用定量方法，展示了ChatGPT在回答本科计算机科学范围内的各种问题上具有高度的不可靠性。我们的分析表明，学生盲目依赖ChatGPT完成作业和考试可能会自毁前程。我们在这个分析基础上提出了教师和学生如何在学术界使用ChatGPT的建议。

    ChatGPT is an AI language model developed by OpenAI that can understand and generate human-like text. It can be used for a variety of use cases such as language generation, question answering, text summarization, chatbot development, language translation, sentiment analysis, content creation, personalization, text completion, and storytelling. While ChatGPT has garnered significant positive attention, it has also generated a sense of apprehension and uncertainty in academic circles. There is concern that students may leverage ChatGPT to complete take-home assignments and exams and obtain favorable grades without genuinely acquiring knowledge. This paper adopts a quantitative approach to demonstrate ChatGPT's high degree of unreliability in answering a diverse range of questions pertaining to topics in undergraduate computer science. Our analysis shows that students may risk self-sabotage by blindly depending on ChatGPT to complete assignments and exams. We build upon this analysis to p
    
[^111]: 利用强化学习优化地铁系统能源效率在不确定扰动下的表现

    Optimizing Energy Efficiency in Metro Systems Under Uncertainty Disturbances Using Reinforcement Learning. (arXiv:2304.13443v1 [cs.AI])

    [http://arxiv.org/abs/2304.13443](http://arxiv.org/abs/2304.13443)

    本文提出了一种基于策略的强化学习方法，通过重新安排地铁时刻表和调整列车的停靠时间和巡航速度，优化扰动下的地铁系统能源效率，该方法在模拟环境下实验证明其优于基线方法，最高可达降低10.9%的牵引能量消耗和最高达提高47.9%的再生制动能量利用率，为城市轨道交通的节能问题提供了有效的解决方案。

    

    在城市交通领域，地铁系统是关键的可持续公共交通工具。然而，它们的巨大能源消耗对可持续性目标构成了挑战。延误和乘客流变化等扰动进一步加剧了这个问题，因为它们会对地铁系统的能源效率产生负面影响。为了解决这个问题，我们提出了一种基于策略的强化学习方法，通过调整列车的停靠时间和巡航速度，重新安排地铁时刻表，并优化受扰动影响的地铁系统的能源效率。在模拟环境中进行的实验表明，我们的方法优于基线方法，实现了高达10.9％的牵引能量消耗降低和最高达47.9％的再生制动能量利用率提高。本研究为城市轨道交通节能问题提供了有效的解决方案。

    In the realm of urban transportation, metro systems serve as crucial and sustainable means of public transit. However, their substantial energy consumption poses a challenge to the goal of sustainability. Disturbances such as delays and passenger flow changes can further exacerbate this issue by negatively affecting energy efficiency in metro systems. To tackle this problem, we propose a policy-based reinforcement learning approach that reschedules the metro timetable and optimizes energy efficiency in metro systems under disturbances by adjusting the dwell time and cruise speed of trains. Our experiments conducted in a simulation environment demonstrate the superiority of our method over baseline methods, achieving a traction energy consumption reduction of up to 10.9% and an increase in regenerative braking energy utilization of up to 47.9%. This study provides an effective solution to the energy-saving problem of urban rail transit.
    
[^112]: 医学图像分析中的任意分割模型：一项实验研究

    Segment Anything Model for Medical Image Analysis: an Experimental Study. (arXiv:2304.10517v1 [cs.CV])

    [http://arxiv.org/abs/2304.10517](http://arxiv.org/abs/2304.10517)

    本研究对医学图像分割模型SAM在各种不同情况下的表现进行了广泛评估，结果表明在单点提示下其表现高度变化，是一项具有挑战性的工作。

    

    由于数据注释的有限可用性和获取成本，训练医学图像分割模型仍然具有挑战性。Segment Anything Model（SAM）是一种基础模型，经过超过10亿个注释的训练，主要用于自然图像，旨在能够以交互方式分割用户定义的感兴趣的对象。尽管SAM在自然图像上表现出色，但不清楚该模型在转换到医学图像领域时会受到多大影响。在这里，我们对SAM在各种模态和解剖学的11个医学图像数据集上进行了广泛的评估。在我们的实验中，我们使用标准方法生成点提示来模拟交互分割。实验结果表明，SAM基于单点提示的表现在任务和数据集方面高度变化，即从脊柱MRI数据集的0.1135到髋关节X射线数据集的0.8650。

    Training segmentation models for medical images continues to be challenging due to the limited availability and acquisition expense of data annotations. Segment Anything Model (SAM) is a foundation model trained on over 1 billion annotations, predominantly for natural images, that is intended to be able to segment the user-defined object of interest in an interactive manner. Despite its impressive performance on natural images, it is unclear how the model is affected when shifting to medical image domains. Here, we perform an extensive evaluation of SAM's ability to segment medical images on a collection of 11 medical imaging datasets from various modalities and anatomies. In our experiments, we generated point prompts using a standard method that simulates interactive segmentation. Experimental results show that SAM's performance based on single prompts highly varies depending on the task and the dataset, i.e., from 0.1135 for a spine MRI dataset to 0.8650 for a hip x-ray dataset, eva
    
[^113]: NetGPT：网络流量生成预训练变压器模型

    NetGPT: Generative Pretrained Transformer for Network Traffic. (arXiv:2304.09513v1 [cs.NI])

    [http://arxiv.org/abs/2304.09513](http://arxiv.org/abs/2304.09513)

    本文提出了首个网络流量生成预训练变压器模型NetGPT，该模型可以优化网络任务的训练效率和有效性。

    

    预训练模型可以利用大规模的原始数据学习网络流量的基本特征，并为输入流量生成可区分的结果，而不考虑特定的下游任务。有效的预训练模型可以显著优化下游任务的训练效率和有效性，例如流量分类、攻击检测、资源调度、协议分析和流量生成。本文提出了NetGPT，旨在为网络流量构建预训练模型并解决多样的挑战。

    Pretrained models for network traffic can utilize large-scale raw data to learn the essential characteristics of network traffic, and generate distinguishable results for input traffic without considering specific downstream tasks. Effective pretrained models can significantly optimize the training efficiency and effectiveness of downstream tasks, such as traffic classification, attack detection, resource scheduling, protocol analysis, and traffic generation. Despite the great success of pretraining in natural language processing, there is no work in the network field. Considering the diverse demands and characteristics of network traffic and network tasks, it is non-trivial to build a pretrained model for network traffic and we face various challenges, especially the heterogeneous headers and payloads in the multi-pattern network traffic and the different dependencies for contexts of diverse downstream network tasks.  To tackle these challenges, in this paper, we make the first attemp
    
[^114]: 基于BERT的技术对美国最高法院案例进行分类

    Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v1 [cs.CL])

    [http://arxiv.org/abs/2304.08649](http://arxiv.org/abs/2304.08649)

    本文基于BERT技术探究了对美国最高法院案例进行分类的方法，比较了使用BERT模型与其他先进模型的准确性，最终在15个广泛类别上取得了80%的准确度，在279个细粒度类别上取得了60%的准确度。

    

    基于双向编码器表示来自变压器的模型（BERT）在许多自然语言处理（NLP）任务（如命名实体识别（NER），词性（POS）标记等）上产生了最新技术（SOTA）结果。当分类长文档（例如来自美国最高法院的文档）时，使用BERT模型可能比较困难。本文中，我们尝试了几种基于BERT的分类技术，用于对美国最高法院决定或最高法院数据库（SCDB）进行分类，并将其与先前的SOTA结果进行了比较。我们还将我们的结果与针对长文档的SOTA模型进行了比较。我们对两个分类任务进行了比较：（1）广泛的分类任务，具有15个类别；（2）细粒度的分类任务，具有279个类别。我们的最佳结果在15个广泛类别上产生80％的准确度，在279个细粒度类别上产生60％的准确度。

    Models based on bidirectional encoder representations from transformers (BERT) produce state of the art (SOTA) results on many natural language processing (NLP) tasks such as named entity recognition (NER), part-of-speech (POS) tagging etc. An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis. In this paper, we experiment with several BERT-based classification techniques for US supreme court decisions or supreme court database (SCDB) and compare them with the previous SOTA results. We then compare our results specifically with SOTA models for long documents. We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80\% on the 15 broad categories and 60\% on the fine-grained 279 categories 
    
[^115]: 知识追踪的注意力Q-矩阵学习

    Attentive Q-Matrix Learning for Knowledge Tracing. (arXiv:2304.08168v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2304.08168](http://arxiv.org/abs/2304.08168)

    本文提出了一种基于Q-矩阵的注意力知识追踪模型，能够在不存在事先确定的技能标签的情况下应用于大规模的在线教育平台。

    

    在智能教学系统的迅猛发展中，追踪学生的知识状态越来越重要，以便提供个性化的学习指导。这是知识追踪 (KT) 的主要思想，它根据学生在平台上的过去交互来建立模型，以模拟学生掌握知识概念 (KCs，解决问题所需的技能)。虽然已经提出了许多KT模型，并表现出了卓越的性能，但其中大部分模型使用概念来索引问题，这意味着需要预先确定每个问题所需的技能标签，以指示正确回答该问题所需的KC。这使得这些模型很难应用于大规模的在线教育平台，因为问题通常没有按照技能标签进行很好的组织。本文提出了基于Q-矩阵的注意力知识追踪 (QAKT)，这是一种端到端的模型，能够将注意力方法应用于场景中，其中不存在事先确定的技能标签。

    As the rapid development of Intelligent Tutoring Systems (ITS) in the past decade, tracing the students' knowledge state has become more and more important in order to provide individualized learning guidance. This is the main idea of Knowledge Tracing (KT), which models students' mastery of knowledge concepts (KCs, skills needed to solve a question) based on their past interactions on platforms. Plenty of KT models have been proposed and have shown remarkable performance recently. However, the majority of these models use concepts to index questions, which means the predefined skill tags for each question are required in advance to indicate the KCs needed to answer that question correctly. This makes it pretty hard to apply on large-scale online education platforms where questions are often not well-organized by skill tags. In this paper, we propose Q-matrix-based Attentive Knowledge Tracing (QAKT), an end-to-end style model that is able to apply the attentive method to scenes where n
    
[^116]: AutoRL超参数景观

    AutoRL Hyperparameter Landscapes. (arXiv:2304.02396v1 [cs.LG])

    [http://arxiv.org/abs/2304.02396](http://arxiv.org/abs/2304.02396)

    本文提出了一种方法，在训练期间多次建立和分析AutoRL超参数的景观，证明代表算法（DQN和SAC）在不同环境下的超参数景观会随时间而变化。

    

    强化学习（RL）在取得令人瞩目成果的同时，其超参数对性能的影响限制了其应用范围。这经常使得在实践中难以获得良好的结果。自动化RL（AutoRL）解决了这个难题，但有关超参数优化（HPO）方法在搜索最佳配置时所遍历的超参数景观动态变化的信息很少。鉴于现有AutoRL方法动态调整超参数配置的情况，我们提出了一种方法，在训练期间不仅在一个时间点，而且在多个时间点上建立和分析这些超参数景观。针对关于这种动态AutoRL方法合法性的一个重要开放问题，我们提供了充分的证据，表明在不同种类的环境（Cartpole和Pendulum）中，来自RL文献的代表算法（DQN和SAC）的超参数景观会随时间而强烈变化。

    Although Reinforcement Learning (RL) has shown to be capable of producing impressive results, its use is limited by the impact of its hyperparameters on performance. This often makes it difficult to achieve good results in practice. Automated RL (AutoRL) addresses this difficulty, yet little is known about the dynamics of the hyperparameter landscapes that hyperparameter optimization (HPO) methods traverse in search of optimal configurations. In view of existing AutoRL approaches dynamically adjusting hyperparameter configurations, we propose an approach to build and analyze these hyperparameter landscapes not just for one point in time but at multiple points in time throughout training. Addressing an important open question on the legitimacy of such dynamic AutoRL approaches, we provide thorough empirical evidence that the hyperparameter landscapes strongly vary over time across representative algorithms from RL literature (DQN and SAC) in different kinds of environments (Cartpole and
    
[^117]: 基于正样本增强对比学习的图像视频标题评估

    Positive-Augmented Constrastive Learning for Image and Video Captioning Evaluation. (arXiv:2303.12112v1 [cs.CV])

    [http://arxiv.org/abs/2303.12112](http://arxiv.org/abs/2303.12112)

    本论文提出一种新的图像标题评估指标PAC-S，可以更准确地评估图像和视频的标题，相比于现有的指标有更好的表现；源代码和训练模型已经公开。

    

    最近CLIP模型在很多跨模态任务上都非常有效，包括从视觉和语言结构中生成的标题评估。本文提出了一种新的基于对比度的图像标题评估指标配方，即正样本增强的对比度学习分数（PAC-S），以一种新颖的方式统一了对比度视觉-语义空间的学习和策展数据上生成的图像和文本的添加。跨越多个数据集的实验表明，我们的新指标在图像和视频上与人类判断的相关性最高，优于现有参考指标（如CIDEr和SPICE）和无参考指标（如CLIP-Score）。最后，我们考虑了流行的图像标题方法，并评估了采用不同跨模态特征的影响。我们的源代码和训练模型是公开的。

    The CLIP model has been recently proven to be very effective for a variety of cross-modal tasks, including the evaluation of captions generated from vision-and-language architectures. In this paper, we propose a new recipe for a contrastive-based evaluation metric for image captioning, namely Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way unifies the learning of a contrastive visual-semantic space with the addition of generated images and text on curated data. Experiments spanning several datasets demonstrate that our new metric achieves the highest correlation with human judgments on both images and videos, outperforming existing reference-based metrics like CIDEr and SPICE and reference-free metrics like CLIP-Score. Finally, we test the system-level correlation of the proposed metric when considering popular image captioning approaches, and assess the impact of employing different cross-modal features. Our source code and trained models are publicly availa
    
[^118]: DeepSaDe: 学习确保满足领域约束的神经网络

    DeepSaDe: Learning Neural Networks that Guarantee Domain Constraint Satisfaction. (arXiv:2303.01141v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01141](http://arxiv.org/abs/2303.01141)

    本文提出了一种学习神经网络的方法，该神经网络可以强制执行多样化的约束并且保证所有可能的预测都满足约束限制。

    

    随着机器学习模型的普及，尤其是神经网络，人们越来越关注它们的可信度，特别是在安全关键应用中，如自动驾驶汽车的行为必须是安全的。当前一些方法可以对神经网络进行约束，但它们不能保证所有可能的预测都满足约束限制（即使在未看过的数据上），或者它们对可强制执行的约束类型有限制。为了解决这些问题，本文提出了一种方法，用于训练可以强制执行广泛约束并保证所有可能预测都满足约束的神经网络。该方法基于以往将学习线性模型视为约束满足问题（CSP）的工作。为了将这个想法应用于神经网络，本文增加了两个关键的新元素：网络层上的约束传播和权重更新。

    As machine learning models, specifically neural networks, are becoming increasingly popular, there are concerns regarding their trustworthiness, specially in safety-critical applications, e.g. actions of an autonomous vehicle must be safe. There are approaches that can train neural networks where such domain requirements are enforced as constraints, but they either cannot guarantee that the constraint will be satisfied by all possible predictions (even on unseen data) or they are limited in the type of constraints that can be enforced. In this paper, we present an approach to train neural networks which can enforce a wide variety of constraints and guarantee that the constraint is satisfied by all possible predictions. The approach builds on earlier work where learning linear models is formulated as a constraint satisfaction problem (CSP). To make this idea applicable to neural networks, two crucial new elements are added: constraint propagation over the network layers, and weight upda
    
[^119]: 通过捕获区域实现安全的多智能体学习

    Safe Multi-agent Learning via Trapping Regions. (arXiv:2302.13844v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.13844](http://arxiv.org/abs/2302.13844)

    本文通过引入定性理论中的捕获区域概念，创建出安全集，以确保分散式学习在联合策略空间中的收敛性。

    

    多智能体学习的主要挑战之一在于确立算法的收敛性，因为在通常情况下，一组自私的个体智能体无法保证与其联合策略同时学习时会收敛。这与大多数单智能体环境形成鲜明对比，并为实际应用部署带来了禁锢障碍，因为它在系统的长期行为中引入了不确定性。在本文中，我们应用了从动态系统的定性理论中已知的捕获区域概念，为分散式学习在联合策略空间中建立安全集。我们提出了一种二进制分区算法来验证候选集是否形成了系统内已知学习动态的捕获区域，以及一种启发式采样算法，用于未知学习动态的情况下。我们演示了在规范化的Dirac生成式对抗网络和交通控制场景中的应用。

    One of the main challenges of multi-agent learning lies in establishing convergence of the algorithms, as, in general, a collection of individual, self-serving agents is not guaranteed to converge with their joint policy, when learning concurrently. This is in stark contrast to most single-agent environments, and sets a prohibitive barrier for deployment in practical applications, as it induces uncertainty in long term behavior of the system. In this work, we apply the concept of trapping regions, known from qualitative theory of dynamical systems, to create safety sets in the joint strategy space for decentralized learning. We propose a binary partitioning algorithm for verification that candidate sets form trapping regions in systems with known learning dynamics, and a heuristic sampling algorithm for scenarios where learning dynamics are not known. We demonstrate the applications to a regularized version of Dirac Generative Adversarial Network, a four-intersection traffic control sc
    
[^120]: 一站式解决方案：利用预训练 LM 进行强大的时间序列分析

    One Fits All:Power General Time Series Analysis by Pretrained LM. (arXiv:2302.11939v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11939](http://arxiv.org/abs/2302.11939)

    本论文提出了一种称为 Frozen Pretrained Transformer (FPT) 的预训练模型，利用从数十亿标记训练出来的语言或 CV 模型进行时间序列分析的所有主要类型任务的微调，进而使其在所有任务中都具备着最先进的性能和泛化能力。

    

    尽管预训练模型在自然语言处理 (NLP) 和计算机视觉 (CV) 领域取得了巨大成功，但在通用时间序列分析领域取得的进展有限。与 NLP 和 CV 不同的是，这些领域采用统一模型即可执行不同的任务，而在每个时间序列分析任务中，专门设计的方法仍然占据主导地位，如分类、异常检测、预测和少样本学习。阻碍预训练模型发展的主要挑战是缺乏大量用于训练的数据。在本文中，我们通过利用从数十亿标记训练出来的语言或 CV 模型，来解决这一挑战，用于时间序列分析。具体而言，我们避免改变预训练语言或图像模型中残差块中的自注意力和前向传递层。这种模型被称为冻结的预训练变压器 (FPT)，通过对涉及时间序列分析的所有主要类型的任务进行微调进行评估，包括分类、异常检测、预测和少样本学习等。实验结果证明，FPT 在所有任务中都具有最先进的性能和泛化能力。

    Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time s
    
[^121]: 生长可操纵神经元细胞自动机

    Growing Steerable Neural Cellular Automata. (arXiv:2302.10197v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2302.10197](http://arxiv.org/abs/2302.10197)

    本文讨论了"Growing Steerable Neural Cellular Automata"，通过使每个细胞负责其自己的方向，产生了具有变化方向的细胞的模型。

    

    神经元细胞自动机（NCA）模型显示出了惊人的模式形成能力和源于局部协调的复杂全局行为。然而，在原始的NCA实现中，细胞无法调整自己的方向，定向是由模型设计师在外部定向。最近的各向同性NCA变体（Growing Isotropic Neural Cellular Automata）通过消除对其邻域中空间状态梯度感知的依赖性，使模型与定向无关 - 细胞无法从上到下或从左到右，。在这项工作中，我们采用不同的方法重新审视NCA：我们使每个细胞负责其自己的定向，允许其根据可调整的内部状态“转向”。由此产生的可操纵NCA包含嵌入同一模式中具有不同方向的细胞。 我们观察到，虽然各向同性NCA没有定向，可操纵的NCA具有手性：它们有预定

    Neural Cellular Automata (NCA) models have shown remarkable capacity for pattern formation and complex global behaviors stemming from local coordination. However, in the original implementation of NCA, cells are incapable of adjusting their own orientation, and it is the responsibility of the model designer to orient them externally. A recent isotropic variant of NCA (Growing Isotropic Neural Cellular Automata) makes the model orientation-independent - cells can no longer tell up from down, nor left from right - by removing its dependency on perceiving the gradient of spatial states in its neighborhood. In this work, we revisit NCA with a different approach: we make each cell responsible for its own orientation by allowing it to "turn" as determined by an adjustable internal state. The resulting Steerable NCA contains cells of varying orientation embedded in the same pattern. We observe how, while Isotropic NCA are orientation-agnostic, Steerable NCA have chirality: they have a predete
    
[^122]: 概率对比学习恢复了不确定性输入的正确估计

    Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs. (arXiv:2302.02865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02865](http://arxiv.org/abs/2302.02865)

    本文提出利用概率对比学习方法可以恢复具有不确定性输入的正确估计，通过扩展InfoNCE目标和编码器以预测潜变量分布来实现，在计算已知查询图像的可信区间方面具有应用价值。

    

    最近，对比学习编码器被证明可以翻转数据生成过程：它们可以将每个输入（如图像）编码成生成该图像的真实潜变量（Zimmermann等人，2021）。然而，现实世界的观察结果通常存在内在的模糊性。例如，图像可能模糊或只显示3D物体的2D视图，因此可能有多个潜变量生成它们。这使得潜变量的真实后验概率具有异方差不确定性。在这种设置下，我们扩展了常见的InfoNCE目标和编码器，以预测潜变量分布而不是点。我们证明这些分布恢复了数据生成过程的正确后验分布，包括其不确定性水平的估计，该估计存在潜变量空间的旋转。除了提供校准的不确定性估计之外，这些后验分布还允许在图像检索中计算可信区间。它们包括具有与给定查询相同的潜变量的图像。

    Contrastively trained encoders have recently been proven to invert the data-generating process: they encode each input, e.g., an image, into the true latent vector that generated the image (Zimmermann et al., 2021). However, real-world observations often have inherent ambiguities. For instance, images may be blurred or only show a 2D view of a 3D object, so multiple latents could have generated them. This makes the true posterior for the latent vector probabilistic with heteroscedastic uncertainty. In this setup, we extend the common InfoNCE objective and encoders to predict latent distributions instead of points. We prove that these distributions recover the correct posteriors of the data-generating process, including its level of aleatoric uncertainty, up to a rotation of the latent space. In addition to providing calibrated uncertainty estimates, these posteriors allow the computation of credible intervals in image retrieval. They comprise images with the same latent as a given quer
    
[^123]: 基于双重自我意识价值分解框架的协作多智体强化学习

    Dual Self-Awareness Value Decomposition Framework without Individual Global Max for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2302.02180v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2302.02180](http://arxiv.org/abs/2302.02180)

    提出了一种基于双重自我意识价值分解框架的协作多智体强化学习方法，完全摒弃了个体全局最大值的前提，具有良好的性能表现。

    

    在协作多智体强化学习领域，价值分解方法已经变得越来越流行。然而，几乎所有现有方法都遵循个体全局最大值（IGM）或其变体的原则，这限制了它们的问题解决能力。为了解决这个问题，我们提出了一个基于心理学中双重自我意识概念的双重自我意识价值分解框架，完全摒弃了IGM前提。每个智能体包括自我策略以进行动作选择和替身价值函数以解决信用分配问题。价值函数分解可以利用显式搜索过程忽略IGM假设。在此基础上，我们还提出了一种新颖的反自我探索机制，以避免算法陷入局部最优。作为第一个完全不用IGM的价值分解方法，我们提出的框架在各种协作任务中实现了期望的性能。

    Value decomposition methods have gained popularity in the field of cooperative multi-agent reinforcement learning. However, almost all existing methods follow the principle of Individual Global Max (IGM) or its variants, which limits their problem-solving capabilities. To address this, we propose a dual self-awareness value decomposition framework, inspired by the notion of dual self-awareness in psychology, that entirely rejects the IGM premise. Each agent consists of an ego policy for action selection and an alter ego value function to solve the credit assignment problem. The value function factorization can ignore the IGM assumption by utilizing an explicit search procedure. On the basis of the above, we also suggest a novel anti-ego exploration mechanism to avoid the algorithm becoming stuck in a local optimum. As the first fully IGM-free value decomposition method, our proposed framework achieves desirable performance in various cooperative tasks.
    
[^124]: 平均限制策略优化

    Average-Constrained Policy Optimization. (arXiv:2302.00808v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00808](http://arxiv.org/abs/2302.00808)

    本研究提出了一种新的基于函数逼近算法的带平均标准约束 MDP 的策略优化算法，具有较好的性能表现。

    

    有限制条件的强化学习对于各种应用变得越来越重要。通常，平均标准比折扣标准更合适。然而，针对平均限制 CMDP 的强化学习仍然是一个具有挑战性的问题。针对折扣限制 RL 问题设计的算法通常在平均 CMDP 环境下表现不佳。在本文中，我们引入了一种新的基于函数逼近算法的带平均标准约束 MDP 的策略优化算法。平均限制策略优化（ACPO）算法的灵感来自基于信任区域方法的著名 PPO 类算法。我们发展了基本的平均 MDP 敏感性理论，然后在算法设计中使用相应的界限。我们提供了其性能的理论保证，并通过在各种具有挑战性的 MuJoCo 环境中进行大量实验工作，展示了该算法与其他常规算法相比的卓越表现。

    Reinforcement Learning (RL) with constraints is becoming an increasingly important problem for various applications. Often, the average criterion is more suitable than the discounted criterion. Yet, RL for average criterion-constrained MDPs remains a challenging problem. Algorithms designed for discounted constrained RL problems often do not perform well for the average CMDP setting. In this paper, we introduce a new policy optimization with function approximation algorithm for constrained MDPs with the average criterion. The Average-Constrained Policy Optimization (ACPO) algorithm is inspired by the famed PPO-type algorithms based on trust region methods. We develop basic sensitivity theory for average MDPs, and then use the corresponding bounds in the design of the algorithm. We provide theoretical guarantees on its performance, and through extensive experimental work in various challenging MuJoCo environments, show the superior performance of the algorithm when compared to other sta
    
[^125]: 蒸馏策略优化

    Distillation Policy Optimization. (arXiv:2302.00533v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00533](http://arxiv.org/abs/2302.00533)

    本文展示了一种演员-评论家的学习框架，该框架通过蒸馏优势在利用过去经验的同时遵循稳定的在线策略，实现了快速学习并可以适用于广泛的算法类别。

    

    本文提出了一个演员-评论家学习框架，它借鉴了分布式学习的视角和两种策略改进数据的交叉融合，实现了快速学习并可应用于广泛的算法类别。在该框架中，首先提出了方差减少机制，例如统一优势估计器 (UAE) 和一个学习的基线，不仅是连接到动作值函数的桥梁，还能提炼优势。

    On-policy algorithms are supposed to be stable, however, sample-intensive yet. Off-policy algorithms utilizing past experiences are deemed to be sample-efficient, nevertheless, unstable in general. Can we design an algorithm that can employ the off-policy data, while exploit the stable learning by sailing along the course of the on-policy walkway? In this paper, we present an actor-critic learning framework that borrows the distributional perspective of interest to evaluate, and cross-breeds two sources of the data for policy improvement, which enables fast learning and can be applied to a wide class of algorithms. In its backbone, the variance reduction mechanisms, such as unified advantage estimator (UAE), that extends generalized advantage estimator (GAE) to be applicable on any state-dependent baseline, and a learned baseline, that is competent to stabilize the policy gradient, are firstly put forward to not merely be a bridge to the action-value function but also distill the advan
    
[^126]: 随机网络碾压对防止探索的影响

    Anti-Exploration by Random Network Distillation. (arXiv:2301.13616v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13616](http://arxiv.org/abs/2301.13616)

    本文介绍了基于特征线性调制的随机网络碾压算法，可以有效防止探索，避免了区分度的问题，在 D4RL 基准测试中取得了可与集成方法相媲美的性能。

    

    尽管随机网络碾压 (RND) 在各种领域都取得了成功，但在用作离线强化学习中惩罚越界操作的不确定性估计器时，它被证明不具有足够的区分度。在本文中，我们重新审视了这些结果，并表明，通过对 RND 先验进行朴素的调节选择，演员有效地最小化反探索奖励变得不可行，并且区分度不再是问题。我们展示了可以通过基于特征线性调制 (FiLM) 的调节来避免这种局限性，从而得到一个简单而高效的基于软行为者-评论家算法的无集成算法。我们在 D4RL 基准测试上进行了评估，结果表明，它能够实现与基于集成的方法相当的性能，并显著优于无集成方法。

    Despite the success of Random Network Distillation (RND) in various domains, it was shown as not discriminative enough to be used as an uncertainty estimator for penalizing out-of-distribution actions in offline reinforcement learning. In this paper, we revisit these results and show that, with a naive choice of conditioning for the RND prior, it becomes infeasible for the actor to effectively minimize the anti-exploration bonus and discriminativity is not an issue. We show that this limitation can be avoided with conditioning based on Feature-wise Linear Modulation (FiLM), resulting in a simple and efficient ensemble-free algorithm based on Soft Actor-Critic. We evaluate it on the D4RL benchmark, showing that it is capable of achieving performance comparable to ensemble-based methods and outperforming ensemble-free approaches by a wide margin.
    
[^127]: 将知识纳入文档摘要生成中：基于GPT-2的前缀调整应用

    Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2. (arXiv:2301.11719v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11719](http://arxiv.org/abs/2301.11719)

    本论文研究了将事实知识纳入生成的摘要的可能性，具体采用前缀调整的方法，实验结果表明，此方法可以生成保留知识的摘要，而且可以提升整体性能。

    

    尽管现在文档摘要技术得到了很大的发展，但是生成的摘要和原始文本之间的事实不一致仍然时有发生。本研究探索了采用提示来将事实知识纳入生成的摘要的可能性。我们具体研究了前缀调整，它使用一组可训练的连续前缀提示和离散自然语言提示来帮助摘要生成。实验结果表明，可训练的前缀可以帮助摘要模型准确地从离散提示中提取信息，从而生成保留知识的摘要，这些摘要在事实上与离散提示一致。生成的摘要的ROUGE改进表明，将事实知识明确地添加到摘要生成过程中可以提升整体性能，显示出在其他自然语言处理任务中应用的巨大潜力。

    Despite the great development of document summarisation techniques nowadays, factual inconsistencies between the generated summaries and the original texts still occur from time to time. This study explores the possibility of adopting prompts to incorporate factual knowledge into generated summaries. We specifically study prefix-tuning that uses a set of trainable continuous prefix prompts together with discrete natural language prompts to aid summary generation. Experimental results demonstrate that the trainable prefixes can help the summarisation model extract information from discrete prompts precisely, thus generating knowledge-preserving summaries that are factually consistent with the discrete prompts. The ROUGE improvements of the generated summaries indicate that explicitly adding factual knowledge into the summarisation process could boost the overall performance, showing great potential for applying it to other natural language processing tasks.
    
[^128]: 基于层次数据有效表示学习的RNA三级结构设计

    Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design. (arXiv:2301.10774v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2301.10774](http://arxiv.org/abs/2301.10774)

    本研究提出了一个基于层次数据有效表示学习的RNA设计流程，通过构建大型数据集并设计全面的结构建模方法，实现了更高效的RNA序列设计。

    

    尽管人工智能已在揭示生物大分子的一级序列与三级结构之间的关系方面取得了显着进展，但基于特定三级结构设计RNA序列仍然具有挑战性。虽然蛋白质设计中的现有方法已经彻底探索了蛋白质中结构到序列的依赖性，但RNA设计仍面临结构复杂性和数据稀缺性的困难。与此同时，虽然RNA与蛋白质共享类似的结构组分，但直接将蛋白质设计方法移植到RNA设计中却无法取得令人满意的结果。本研究旨在系统构建数据驱动的RNA设计流程。我们构建了一个大型、精心策划的基准数据集，并设计了一个全面的结构建模方法来表示复杂的RNA三级结构。更重要的是，我们提出了一个层次数据有效表示学习框架，学习结构表示的多个层次特征，以实现更高效的RNA序列设计。

    While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules' primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Adding to the problem, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural repre
    
[^129]: A-NeSI: 一种可扩展的近似方法用于概率神经符号推理。

    A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference. (arXiv:2212.12393v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12393](http://arxiv.org/abs/2212.12393)

    本文介绍了一种名为A-NeSI的新颖PNL框架，它使用神经网络实现了近似推理，能够保证概率逻辑语义的同时解决了PNL的可扩展性问题，能够在安全关键应用中保证逻辑约束的满足。

    

    本文研究了将神经网络与符号推理相结合的问题。最近引入的概率神经符号学习（PNL）框架，如DeepProbLog，执行指数时间的精确推理，限制了PNL解决方案的可扩展性。我们介绍了近似神经符号推理（A-NeSI）：一种新的PNL框架，它使用神经网络进行可扩展的近似推理。A-NeSI 1) 在不改变概率逻辑语义的情况下，以多项式时间执行近似推理；2) 使用由背景知识生成的数据进行训练；3) 可以生成有关预测的符号解释；4) 可以在测试时间保证逻辑约束的满足，这在安全关键应用中非常重要。我们的实验表明，A-NeSI是第一个能够解决具有指数组合扩展的三种神经符号任务的端到端方法。最后，我们的实验表明，A-NeSI实现了可解释性和安全性，而没有惩罚。

    We study the problem of combining neural networks with symbolic reasoning. Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL), such as DeepProbLog, perform exponential-time exact inference, limiting the scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference (A-NeSI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NeSI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. Our experiments show that A-NeSI is the first end-to-end method to solve three neurosymbolic tasks with exponential combinatorial scaling. Finally, our experiments show that A-NeSI achieves explainability and safety without a penalty in p
    
[^130]: 朝向因果责任划分的探索

    Towards Causal Credit Assignment. (arXiv:2212.11636v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11636](http://arxiv.org/abs/2212.11636)

    本论文研究了后见信用分配这种分配信用的方法，并提出了一种结合因果结构的状态表示来提高效率的方法。

    

    在强化学习中，如何根据行为的贡献适当地将信用分配给未来的结果是一个长期存在的挑战。目前广泛使用的信用分配方法的假设在决策效果不立即显现的任务中具有劣势。此外，该方法只能评估代理已选择的动作，因此效率极低。本研究探讨后见信用分配的潜力和局限，并提出一种利用环境因果结构的分解状态表示来提高后见信用分配效率的方法。

    Adequately assigning credit to actions for future outcomes based on their contributions is a long-standing open challenge in Reinforcement Learning. The assumptions of the most commonly used credit assignment method are disadvantageous in tasks where the effects of decisions are not immediately evident. Furthermore, this method can only evaluate actions that have been selected by the agent, making it highly inefficient. Still, no alternative methods have been widely adopted in the field. Hindsight Credit Assignment is a promising, but still unexplored candidate, which aims to solve the problems of both long-term and counterfactual credit assignment. In this thesis, we empirically investigate Hindsight Credit Assignment to identify its main benefits, and key points to improve. Then, we apply it to factored state representations, and in particular to state representations based on the causal structure of the environment. In this setting, we propose a variant of Hindsight Credit Assignmen
    
[^131]: KGLM: 将知识图谱结构整合进语言模型以进行链接预测

    KGLM: Integrating Knowledge Graph Structure in Language Models for Link Prediction. (arXiv:2211.02744v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.02744](http://arxiv.org/abs/2211.02744)

    本文提出了 KGLM 架构，将新的实体/关系嵌入层整合进语言模型，使其能够学习知识图谱的结构。使用从知识图谱提取的三元组对模型进行预训练并进行链接预测任务得到了良好效果。

    

    知识图谱能够在大规模情况下表示复杂关系，已被广泛应用于知识表示、问答和推荐系统等领域。然而，知识图谱通常存在信息缺失的问题，需要进行知识图谱补全任务。预训练和微调的语言模型已经在这些任务中表现出出色的效果，尽管这些模型忽略了知识图谱所蕴含的固有信息，即实体和关系类型。因此，我们提出了KGLM（Knowledge Graph Language Model）架构，其中引入了一个新的实体/关系嵌入层，它学习区分不同的实体和关系类型，使得模型能够学习知识图谱的结构。在本文中，我们展示了使用从知识图谱提取的三元组进一步预训练这些额外嵌入层的语言模型，然后采用后续的 link prediction 任务表现出了良好的效果。

    The ability of knowledge graphs to represent complex relationships at scale has led to their adoption for various needs including knowledge representation, question-answering, and recommendation systems. Knowledge graphs are often incomplete in the information they represent, necessitating the need for knowledge graph completion tasks. Pre-trained and fine-tuned language models have shown promise in these tasks although these models ignore the intrinsic information encoded in the knowledge graph, namely the entity and relation types. In this work, we propose the Knowledge Graph Language Model (KGLM) architecture, where we introduce a new entity/relation embedding layer that learns to differentiate distinctive entity and relation types, therefore allowing the model to learn the structure of the knowledge graph. In this work, we show that further pre-training the language models with this additional embedding layer using the triples extracted from the knowledge graph, followed by the sta
    
[^132]: 引入变分因果推理的目标条件强化学习的泛化

    Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning. (arXiv:2207.09081v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.09081](http://arxiv.org/abs/2207.09081)

    本文提出了一种增强的目标条件强化学习框架，它使用因果图来发现和表示因果关系来实现模型的泛化性。

    

    推理作为人类智能中达成通用解决方案的关键部分，通过总结部分到整体的参数和发现因果关系，为强化学习（RL）代理人实现向各种目标的泛化提供了巨大的潜力。然而，如何发现和表示因果关系仍然是阻碍因果RL发展的重大障碍。在本文中，我们通过使用因果图结构来增强目标条件RL（GCRL），该结构建立在对象和事件之间的关系上。我们将GCRL问题新颖地制定为具有CG作为潜在变量的变分似然最大化。为了优化派生目标，我们提出了一个具有理论性能保证的框架，交替使用干预数据来估计CG的后验概率，使用CG来学习通用模型和可解释的策略。由于缺乏验证推理下泛化能力的公共基准，我们设计了...

    As a pivotal component to attaining generalizable solutions in human intelligence, reasoning provides great potential for reinforcement learning (RL) agents' generalization towards varied goals by summarizing part-to-whole arguments and discovering cause-and-effect relations. However, how to discover and represent causalities remains a huge gap that hinders the development of causal RL. In this paper, we augment Goal-Conditioned RL (GCRL) with Causal Graph (CG), a structure built upon the relation between objects and events. We novelly formulate the GCRL problem into variational likelihood maximization with CG as latent variables. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventional data to estimate the posterior of CG; using CG to learn generalizable models and interpretable policies. Due to the lack of public benchmarks that verify generalization capability under reasoning, we design 
    
[^133]: 超级图像——3D医学成像分析的全新二维视角

    Super Images -- A New 2D Perspective on 3D Medical Imaging Analysis. (arXiv:2205.02847v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2205.02847](http://arxiv.org/abs/2205.02847)

    本文提出了一种在医学成像分析中处理3D数据的新方法，即将体积数据转换为2D超级图像，并使用2D网络进行分割，以提高效率。该方法可以在训练中有效地嵌入3D知识。

    

    在医学成像分析中，深度学习表现出了很好的结果。我们经常依赖体积数据来分割医学图像，需要使用3D架构，这些架构因其捕捉interslice context的能力而受到赞赏。然而，由于这些网络中使用了3D卷积、最大池化、up-convolutions和其他操作，这些架构在时间和计算方面通常比它们的2D等价物更低效。此外，很少有3D预训练模型权重，预训练通常很难。我们提出了一种简单而有效的二维方法来有效地嵌入3D知识，同时处理3D数据。我们提出将体积数据转换为2D超级图像，并使用2D网络进行分割以解决这些挑战。我们的方法通过将3D图像的切片并排拼接来生成超分辨率图像。我们期望深度神经网络能够在空间上捕捉和学习这些特性，尽管可能会丢失interslice context信息。

    In medical imaging analysis, deep learning has shown promising results. We frequently rely on volumetric data to segment medical images, necessitating the use of 3D architectures, which are commended for their capacity to capture interslice context. However, because of the 3D convolutions, max pooling, up-convolutions, and other operations utilized in these networks, these architectures are often more inefficient in terms of time and computation than their 2D equivalents. Furthermore, there are few 3D pretrained model weights, and pretraining is often difficult. We present a simple yet effective 2D method to handle 3D data while efficiently embedding the 3D knowledge during training. We propose transforming volumetric data into 2D super images and segmenting with 2D networks to solve these challenges. Our method generates a super-resolution image by stitching slices side by side in the 3D image. We expect deep neural networks to capture and learn these properties spatially despite losi
    
[^134]: 《邻域注意力变换器》

    Neighborhood Attention Transformer. (arXiv:2204.07143v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2204.07143](http://arxiv.org/abs/2204.07143)

    提出了针对视觉任务的高效和可扩展的滑动窗口注意力机制——邻域关注（NA）。基于NA，开发了NAT，NAT-Tiny在ImageNet上达到了83.2％的top-1准确率，能够提高图像分类和下游视觉性能。

    

    我们提出了“邻域关注”（NA），这是第一种针对视觉任务的高效和可扩展的滑动窗口注意力机制。NA是一种像素级运算，将自注意力（SA）局限于最近的相邻像素，因此与SA的二次复杂度相比，具有线性的时间和空间复杂度。滑动窗口模式使NA的感受野能够增长而不需要额外的像素移位，并且保留了平移等变性，这与Swin Transformer的窗口自注意力（WSA）不同。我们开发了NATTEN（邻域关注扩展），这是一个具有高效的C++和CUDA内核的Python包，使NA的运行速度比Swin的WSA快高达40％，同时使用的内存少了25％。我们进一步提出了基于NA的新层次结构变换器设计——邻域关注变换器（NAT），以提高图像分类和下游视觉性能。在NAT上的实验结果表明具有竞争力，NAT-Tiny在ImageNet上达到了83.2％的top-1准确率和51.4％的mAP。

    We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on M
    
[^135]: DiGS：基于散度导向的未定向点云形状隐式神经表示

    DiGS : Divergence guided shape implicit neural representation for unoriented point clouds. (arXiv:2106.10811v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2106.10811](http://arxiv.org/abs/2106.10811)

    本文提出了一种新的散度导向形状表示学习方法，无需输入法向量，以软约束倾向于平滑解决方案，可靠地定位每个点的未知法向量的梯度，甚至比使用真实法向量更好。此外，还引入了一种新的正弦INR几何初始化方法，以进一步提高收敛性能。

    

    形状隐式神经表示（INR）最近已经在形状分析和重建任务中表现出有效性。现有的INR需要几何坐标来学习形状的隐式水平集。当每个点都有法向量时，可以学习到更高保真度的表示，然而通常原始数据中很少提供法向量。此外，已经表明方法的初始化对于表面重构起着至关重要的作用。在本文中，我们提出了一种不需要法向量作为输入的散度导向形状表示学习方法。我们展示了在距离函数的散度上添加软约束会倾向于平滑的解决方案，从而可靠地定位每个点的未知法向量的梯度，某些情况下甚至比直接使用地面真实法向量的方法表现得更好。此外，我们还引入了一种新的正弦INR几何初始化方法，进一步改善了收敛性能。

    Shape implicit neural representations (INRs) have recently shown to be effective in shape analysis and reconstruction tasks. Existing INRs require point coordinates to learn the implicit level sets of the shape. When a normal vector is available for each point, a higher fidelity representation can be learned, however normal vectors are often not provided as raw data. Furthermore, the method's initialization has been shown to play a crucial role for surface reconstruction. In this paper, we propose a divergence guided shape representation learning approach that does not require normal vectors as input. We show that incorporating a soft constraint on the divergence of the distance function favours smooth solutions that reliably orients gradients to match the unknown normal at each point, in some cases even better than approaches that use ground truth normal vectors directly. Additionally, we introduce a novel geometric initialization method for sinusoidal INRs that further improves conve
    

