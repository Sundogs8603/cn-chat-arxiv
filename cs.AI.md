# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ClothesNet: An Information-Rich 3D Garment Model Repository with Simulated Clothes Environment.](http://arxiv.org/abs/2308.09987) | ClothesNet是一个大规模的3D服装对象数据集，包含了服装特征、边界线和关键点的丰富注释，可用于促进计算机视觉和机器人交互任务。同时，通过模拟服装环境，ClothesNet还可以进行包括分类、边界线分割、关键点检测在内的服装感知任务以及包括重新摆放、折叠、挂衣和穿衣等在内的机器人交互任务的评估。在真实环境下的实验证明了ClothesNet的有效性。 |
| [^2] | [Artificial Intelligence across Europe: A Study on Awareness, Attitude and Trust.](http://arxiv.org/abs/2308.09979) | 本研究通过调查欧洲公民对人工智能的意识、态度和信任，发现尽管意识水平较低，但一半以上的人对人工智能持积极态度。 |
| [^3] | [Explicit Time Embedding Based Cascade Attention Network for Information Popularity Prediction.](http://arxiv.org/abs/2308.09976) | 这篇论文提出了一种显式时间嵌入的级联注意力网络，用于预测信息流行度。该网络通过时间嵌入方法将时间属性融入节点特征中，然后采用级联图注意力编码器和级联序列注意力编码器来充分学习级联图和级联序列的表示。 |
| [^4] | [Disposable Transfer Learning for Selective Source Task Unlearning.](http://arxiv.org/abs/2308.09971) | 本论文提出了一种称为可拆卸式迁移学习的新范式，通过引入梯度碰撞损失，该方法可以选择性地遗忘源任务而不降低目标任务的性能。 |
| [^5] | [Tackling Vision Language Tasks Through Learning Inner Monologues.](http://arxiv.org/abs/2308.09970) | 通过学习内心独白，提出了一种新方法（IMMO）来解决复杂的视觉语言任务，克服了混合融合和特征对齐方法所面临的优化和可解释性问题。 |
| [^6] | [Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation.](http://arxiv.org/abs/2308.09965) | 在自动驾驶中遇到未知对象是不可避免的，为了解决这个问题，我们提出了一种基于风格对齐的畸变感知语义分割方法。通过减小OoD数据和驾驶场景之间的域差异，我们改进了OoD合成过程，并利用预训练模型生成“给定类别之外”的预测结果进行异常分割，从而提高了性能。 |
| [^7] | [Data-to-text Generation for Severely Under-Resourced Languages with GPT-3.5: A Bit of Help Needed from Google Translate.](http://arxiv.org/abs/2308.09957) | 本文研究了在资源严重匮乏的语言中使用GPT-3.5进行数据到文本生成的任务。发现few-shot引导对于直接生成目标语言效果更好，而通过英语进行中转后，这种差异消失了。在WebNLG 2023共享任务中，这种few-shot + 翻译系统的变体在所有语言中的表现都优于竞争系统。 |
| [^8] | [Eva-KELLM: A New Benchmark for Evaluating Knowledge Editing of LLMs.](http://arxiv.org/abs/2308.09954) | Eva-KELLM是一个新的用于评估LLMs知识编辑的基准，提供了一个评估框架和数据集。该基准通过使用原始文档进行知识编辑和多角度的评估来解决了现有研究中收集成本高、表达复杂事实困难、评估视角受限等问题。 |
| [^9] | [Understanding Self-attention Mechanism via Dynamical System Perspective.](http://arxiv.org/abs/2308.09939) | 本文通过动力系统的视角，展示了自注意机制在神经网络性能提升中的重要作用，并且将其比喻为一种僵硬感知步长适配器。 |
| [^10] | [East: Efficient and Accurate Secure Transformer Framework for Inference.](http://arxiv.org/abs/2308.09923) | 我们提出了一个名为East的框架，以实现高效准确的安全Transformer推理。我们通过设计新的忘却分段多项式求值算法来优化激活函数的运行时间和通信量。此外，我们还设计了安全协议来处理softmax和层归一化。 |
| [^11] | [Recap: Detecting Deepfake Video with Unpredictable Tampered Traces via Recovering Faces and Mapping Recovered Faces.](http://arxiv.org/abs/2308.09921) | 提出了一种新型的Deepfake检测模型Recap，通过恢复面部并映射恢复的面部暴露了非特定的面部不一致性，扩大了真实和伪造之间的差异。 |
| [^12] | [Learning Multiscale Consistency for Self-supervised Electron Microscopy Instance Segmentation.](http://arxiv.org/abs/2308.09917) | 本文提出了一种利用多尺度视觉表示捕获电子显微镜实例分割中体素级和特征级一致性的新的预训练框架。 |
| [^13] | [Never Explore Repeatedly in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2308.09909) | 该论文提出了一种动态奖励缩放方法，以解决多智能体强化学习中的回访问题。实验结果表明，在复杂环境中，特别是在稀疏奖励设置下，该方法能够提高性能。 |
| [^14] | [LEGO: Learning and Graph-Optimized Modular Tracker for Online Multi-Object Tracking with Point Clouds.](http://arxiv.org/abs/2308.09908) | 本文提出了一个学习和图优化的模块化跟踪器LEGO，通过集成图优化和自注意力机制，提高了在线多目标跟踪中的数据关联性能。使用LiDAR单独进行跟踪的LEGO方法在KITTI目标跟踪评估中表现出了优秀的性能。 |
| [^15] | [RAH! RecSys-Assistant-Human: A Human-Central Recommendation Framework with Large Language Models.](http://arxiv.org/abs/2308.09904) | 提出了一个人类中心的推荐框架RAH，利用大型语言模型（LLMs）作为助手，实现用户满意度和个性化反馈，并成功应用于学习用户个性和调整推荐系统。 |
| [^16] | [SwinLSTM:Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM.](http://arxiv.org/abs/2308.09891) | SwinLSTM是一种将Swin Transformer和LSTM结合起来以提高时空预测准确性的新循环单元，在四个数据集上表现出优越的性能，特别是在预测准确性方面有显著提高。 |
| [^17] | [Inductive-bias Learning: Generating Code Models with Large Language Model.](http://arxiv.org/abs/2308.09890) | 这篇论文提出了一种新的学习方法，称为归纳偏差学习（IBL），它将上下文学习（ICL）和代码生成相结合，通过输入训练数据到提示中，输出相应的代码。 |
| [^18] | [Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks.](http://arxiv.org/abs/2308.09858) | 本文提出了一个完全无需反向传播的神经网络训练框架，并通过张量压缩的方差约减方法和混合梯度评估方法改进了优化和效率。同时，还扩展了框架用于物理信息的神经网络的估计。 |
| [^19] | [Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees.](http://arxiv.org/abs/2308.09842) | 通过epsilon-ProVe方法，我们提出了一种高效近似的方法来枚举深度神经网络中的安全区域，并提供了可证明概率保证的紧密下估计。 |
| [^20] | [Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis.](http://arxiv.org/abs/2308.09830) | 本文探索了将大型语言模型和认知架构相结合的替代方案，通过协同方法互补各自的弱点和限制，从而实现更稳健和复杂的人工智能系统。 |
| [^21] | [An Image is Worth a Thousand Toxic Words: A Metamorphic Testing Framework for Content Moderation Software.](http://arxiv.org/abs/2308.09810) | 本文提出了OASIS，一个用于内容审核软件的变形测试框架，以解决现代社交媒体平台上恶意内容的传播问题。 |
| [^22] | [VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control.](http://arxiv.org/abs/2308.09804) | 本文提出了一种名为VL-PET的视觉和语言参数高效调整框架，通过粒度控制机制对模块化修改进行有效控制，克服了现有技术在性能和功能差距方面的不足。 |
| [^23] | [Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction.](http://arxiv.org/abs/2308.09780) | 本研究提出了一种基于事件的动态图学习框架，用于准确预测专利申请趋势。该方法利用公司和分类代码的可记忆表示，通过历史记忆和当前信息更新来捕捉语义接近性。 |
| [^24] | [Taken by Surprise: Contrast effect for Similarity Scores.](http://arxiv.org/abs/2308.09765) | 提出了一种新的相似度度量方法，称为“惊喜分数”，该方法能够考虑对象的上下文信息并显著提高零样本和少样本文档分类任务的性能。 |
| [^25] | [The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation.](http://arxiv.org/abs/2308.09764) | 本研究通过去除时尚图像的背景，提高了数据质量和模型性能，在多个方面进行了广泛的比较实验，结果表明背景去除对于模型训练有积极的影响。 |
| [^26] | [A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments.](http://arxiv.org/abs/2308.09734) | 提出了一种适用于非静态环境中多目标强化学习的稳健策略引导算法，能够在线演化一个凸覆盖策略集，同时满足目标偏好空间的探索需求。 |
| [^27] | [Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes.](http://arxiv.org/abs/2308.09733) | 该论文提出了一种解决多目标马尔可夫决策过程中的问题的方法，通过学习通用的技能集，使得策略覆盖集能够在非稳态环境中持续演化，从而提高性能。 |
| [^28] | [Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm.](http://arxiv.org/abs/2308.09732) | 这篇论文解决了Baird反例上的收敛问题，通过一种具有收敛保证的算法，实现了线性收敛速度。 |
| [^29] | [ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT.](http://arxiv.org/abs/2308.09731) | 本研究介绍了一种创新的方法，利用ChatGPT在临床决策中应用大型语言模型，通过策略性设计上下文提示，并以领域知识为基础进行高质量的二元分类任务。通过将机器学习模型视为医疗专家，提取关键见解并辅助决策过程，这一领域知识和人工智能的结合在创建更具洞察力的诊断工具方面具有重要潜力。此外，研究还探索了基于ChatGPT的零样本和少样本提示学习的动态，并验证了ChatGPT在医疗决策支持中的优势。 |
| [^30] | [MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models.](http://arxiv.org/abs/2308.09729) | 本论文通过使用知识图谱来激发大型语言模型，解决了整合新知识、产生幻觉和决策过程不透明等问题，并通过生成思维导图展示了模型的推理路径，实验证明这种方法可以取得显著的实证增益。 |
| [^31] | [Equitable Restless Multi-Armed Bandits: A General Framework Inspired By Digital Health.](http://arxiv.org/abs/2308.09726) | 这项研究提出了不躁动多臂赌博机的公平目标，并开发了相应的算法，证明在数字健康等领域中可以提高数倍的公平性。 |
| [^32] | [MoCLIM: Towards Accurate Cancer Subtyping via Multi-Omics Contrastive Learning with Omics-Inference Modeling.](http://arxiv.org/abs/2308.09725) | 本论文介绍了一种名为MoCLIM的多组学对比学习框架，能够在癌症亚型划分中利用多组学数据的潜力，显著提高了数据的拟合度和亚型划分性能。 |
| [^33] | [Knowledge-inspired Subdomain Adaptation for Cross-Domain Knowledge Transfer.](http://arxiv.org/abs/2308.09724) | 知识启发的子领域适应（KISA）框架在交叉领域知识传递中提供了细粒度的领域适应能力。 |
| [^34] | [On the Unexpected Abilities of Large Language Models.](http://arxiv.org/abs/2308.09720) | 大型语言模型展示了与其训练任务不直接相关的广泛能力，通过间接获取过程使其拥有综合能力的发展，这些能力在一定程度上可预测，并且与人类认知有关。 |
| [^35] | [CIRO: COVID-19 infection risk ontology.](http://arxiv.org/abs/2308.09719) | 本研究通过COVID-19感染风险本体（CIRO）自动评估每个人的感染风险，以减轻公共卫生官员的负担。 |
| [^36] | [Graph of Thoughts: Solving Elaborate Problems with Large Language Models.](http://arxiv.org/abs/2308.09687) | 想法图（GoT）是一种新的框架，它超越了现有的提示范式，通过将大型语言模型（LLM）的信息建模为任意图形，将LLM想法组合成具有协同效应的结果，提炼整个思维网络的本质，或者使用反馈环路增强思维。GoT在不同任务上展示出优势，并可以通过新的想法转换进行扩展，使LLM的推理更接近人类思维。 |
| [^37] | [Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop Visual Reasoning.](http://arxiv.org/abs/2308.09658) | 该论文提出了一种树形混合思维的方法，通过将快速和慢速思考结合，用于解决多跳视觉推理任务中准确性和效率之间的权衡问题。 |
| [^38] | [Poison Dart Frog: A Clean-Label Attack with Low Poisoning Rate and High Attack Success Rate in the Absence of Training Data.](http://arxiv.org/abs/2308.09487) | 毒箭蛙是一种无标签攻击方法，不需要训练数据，只需要目标类别的知识。它具有低中毒率和高攻击成功率。 |
| [^39] | [V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models.](http://arxiv.org/abs/2308.09300) | 本研究提出了一种轻量级的解决方案，通过连接基础模型，特别是CLIP、CLAP和AudioLDM模型，解决了视听生成的问题。 |
| [^40] | [Enhancing Reasoning Capabilities of Large Language Models: A Graph-Based Verification Approach.](http://arxiv.org/abs/2308.09267) | 本研究引入了一种基于图形验证的方法，以进一步提高大型语言模型（LLM）的推理能力，通过将LLM生成的多个解决方案表示为推理图，从而增强推理能力。 |
| [^41] | [Lifted Algorithms for Symmetric Weighted First-Order Model Sampling.](http://arxiv.org/abs/2308.08828) | 本文研究了对称加权一阶模型抽样的提升算法，通过设计一种扩展了计数量词的一阶逻辑的“域可提升抽样”的内容，证明了加权模型抽样问题也存在可处理的情况。 |
| [^42] | [Exploring Demonstration Ensembling for In-context Learning.](http://arxiv.org/abs/2308.08780) | 本研究探索了上下文学习的演示集成方法，用于提高语言模型在给定任务的输入输出对中的预测性能。通过将演示分成子集并组合各子集的输出概率，我们得到了最终的预测结果。 |
| [^43] | [Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.](http://arxiv.org/abs/2308.08708) | 本论文提出了一种严谨的方法，通过对当前的人工智能系统进行详细评估来探讨人工智能的意识问题。研究中对几种科学意识理论进行概述，并通过计算方法推导出意识的“指示性特征”。分析结果表明目前的人工智能系统尚不具备意识，但建立具有意识的人工智能系统并无明显的障碍。 |
| [^44] | [TeCH: Text-guided Reconstruction of Lifelike Clothed Humans.](http://arxiv.org/abs/2308.08545) | 该论文提出了TeCH模型，通过文本引导的方法来重建逼真的服饰人物。模型可以准确恢复“未曾看见的区域”并添加高级细节，采用了基于DMTet的混合3D表示以达到更低的成本。 |
| [^45] | [AIGC In China: Current Developments And Future Outlook.](http://arxiv.org/abs/2308.08451) | 本文分析了中国在人工智能生成内容（AIGC）领域的现状，讨论了AIGC的基础技术、市场状况和发展轨迹，并重点强调了AIGC的生态建设。预测了行业未来的挑战和发展方向。 |
| [^46] | [OmniZoomer: Learning to Move and Zoom in on Sphere at High-Resolution.](http://arxiv.org/abs/2308.08114) | 本文提出了OmniZoomer，一种通过深度学习将Möbius变换整合到网络中，用于在全向图像上进行移动和缩放的方法。通过学习不同条件下的变换特征图，网络能够处理增加的边缘曲率并减轻模糊效果。此外，为了解决混叠问题，作者还提出了两个关键组成部分。 |
| [^47] | [Story Visualization by Online Text Augmentation with Context Memory.](http://arxiv.org/abs/2308.07575) | 本论文提出了一种通过在线文本增强和上下文记忆来进行故事可视化的方法。通过使用新颖的记忆架构和多个伪描述作为训练过程的补充监督，该方法在两个故事可视化基准测试中取得了显著优于现有方法的结果。 |
| [^48] | [Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents.](http://arxiv.org/abs/2308.07241) | 这项研究提出了一种称为CPEM的系统，它利用上下文信息和环境感知记忆来改进行为智能体的感知能力，从而提高视觉导航和物体交互的效果。 |
| [^49] | [Natural Language is All a Graph Needs.](http://arxiv.org/abs/2308.07134) | 本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。 |
| [^50] | [Prototypical Kernel Learning and Open-set Foreground Perception for Generalized Few-shot Semantic Segmentation.](http://arxiv.org/abs/2308.04952) | 本研究通过典型核学习和开放集前景感知，解决泛化少样本语义分割中的表示分割和嵌入偏见问题，并且在分割过程中使用了可学习的核以及典型学习和前景上下文感知模块来提高性能。 |
| [^51] | [Temporal DINO: A Self-supervised Video Strategy to Enhance Action Prediction.](http://arxiv.org/abs/2308.04589) | Temporal DINO是一种自监督视频策略，通过引入一个学生模型和一个教师模型，使得学生模型能够通过观察过去帧来学习未来帧的上下文，以增强动作预测。这种方法在行动预测任务上在ROAD数据集上进行了评估。 |
| [^52] | [Why We Don't Have AGI Yet.](http://arxiv.org/abs/2308.03598) | 本论文探讨了为什么尚未实现人工通用智能（AGI），并指出了纯粹的统计方法和资金推广不足是制约AGI发展的原因之一，同时还分析了实现人类适应能力和自主学习所需的关键认知能力。 |
| [^53] | [PaniniQA: Enhancing Patient Education Through Interactive Question Answering.](http://arxiv.org/abs/2308.03253) | PaniniQA是一个以患者为中心的交互式问答系统，旨在帮助患者理解他们的出院指导，提供及时的反馈以纠正患者的误解，通过有效的互动提高患者对医疗指导的掌握程度。 |
| [^54] | [Source-free Domain Adaptive Human Pose Estimation.](http://arxiv.org/abs/2308.03202) | 提出了无源域自适应的人体姿势估计任务，旨在解决在适应过程中无法访问源数据的跨域学习挑战。通过提出的新框架，源保护模块更有效地保留源信息并抵抗噪声。 |
| [^55] | [Solving Logistic-Oriented Bin Packing Problems Through a Hybrid Quantum-Classical Approach.](http://arxiv.org/abs/2308.02787) | 这项研究通过混合量子-经典方法解决了物流中以实际为导向的装箱问题，并扩展了解决不同维度问题和具有不同要求的箱子的能力。 |
| [^56] | [NeRFs: The Search for the Best 3D Representation.](http://arxiv.org/abs/2308.02751) | NeRFs是视图合成和相关问题中寻找最佳3D表示的结果，该方法利用神经网络查询获取体积参数来描述连续体积场景。 |
| [^57] | [Teaching Smaller Language Models To Generalise To Unseen Compositional Questions.](http://arxiv.org/abs/2308.00946) | 我们研究了如何教授较小的语言模型来推广到未见过的组合问题，通过多任务监督预训练和密集检索系统，我们建立了强大的基准，并展示了解决多个评估数据集上的问题的能力。 |
| [^58] | [Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?.](http://arxiv.org/abs/2308.00158) | 本研究探讨了使用Fine-Tuned的OpenAI LLM进行翻译质量估计的能力，实验证明可以通过Fine-Tuned的ChatGPT来预测机器翻译的质量，但仍有改进的空间。 |
| [^59] | [Getting pwn'd by AI: Penetration Testing with Large Language Models.](http://arxiv.org/abs/2308.00121) | 本文探讨了使用大型语言模型（如GPT3.5）作为AI助手来增强渗透测试人员的能力，实现了高级任务规划和低级漏洞寻找两种用例，取得了有前景的初步结果，并就提供该技术的伦理问题进行了讨论。 |
| [^60] | [TMR-RD: Training-based Model Refinement and Representation Disagreement for Semi-Supervised Object Detection.](http://arxiv.org/abs/2307.13755) | 在半监督目标检测中，本文提出了基于训练的模型精化(TMR)阶段和表示分歧(RD)策略，用来解决伪标签噪声和教师-学生模型的一致性问题。TMR阶段通过轻量级缩放操作优化模型权重，防止过度拟合或遗忘学到的模式；RD策略帮助保持模型的差异，鼓励学生模型探索互补的表示。 |
| [^61] | [On Solving the Rubik's Cube with Domain-Independent Planners Using Standard Representations.](http://arxiv.org/abs/2307.13552) | 本文将魔方的表示转换为PDDL语言，使其更具可访问性和易读性。实验结果表明，DeepCubeA可以解决所有不同复杂度的魔方问题，但只有18％是最优解。 |
| [^62] | [RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment.](http://arxiv.org/abs/2307.12950) | RLCD是一种用于语言模型对齐的强化学习方法，利用对比蒸馏训练偏好模型，可以使语言模型在不使用人类反馈的情况下遵循自然语言规则。在多个对齐任务和不同规模的模型上，RLCD优于其他基线方法。 |
| [^63] | [Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning.](http://arxiv.org/abs/2307.11897) | 本研究提出了Hindsight-DICE算法，利用重要抽样比率估计技术改善了深度强化学习中的信用分配问题。 |
| [^64] | [Ethosight: A Joint-Embedding Based System for Nuanced Perception Using Contextual Label Affinity Metric and Reasoning Based Iterative Learning.](http://arxiv.org/abs/2307.10577) | Ethosight是一种零样本计算机视觉算法，通过联合嵌入、上下文标签关联度计算和基于推理的迭代学习，实现对细微行为和场景细节的准确感知，同时消除了对预先存在符号知识的需求。 |
| [^65] | [Handwritten and Printed Text Segmentation: A Signature Case Study.](http://arxiv.org/abs/2307.07887) | 本研究旨在解决手写和打印文本分割的挑战，并提出了一种新的方法来完整地恢复不同类别的文本，特别是在重叠部分提高分割性能。同时，还引入了一个新的数据集SignaTR6K，用于支持该任务。 |
| [^66] | [Large Language Models as Superpositions of Cultural Perspectives.](http://arxiv.org/abs/2307.07870) | 大型语言模型被认为是具有个性或一套价值观的，但实际上它可以看作是具有不同价值观和个性特征的角度的叠加。通过角度可控性的概念，我们研究了大型语言模型在不同角度下展示的价值观和个性特征的变化。实验结果表明，即使在没有明显提示的情况下，大型语言模型也会表达出不同的价值观。 |
| [^67] | [SINC: Self-Supervised In-Context Learning for Vision-Language Tasks.](http://arxiv.org/abs/2307.07742) | 提出了一种名为SINC的自主上下文学习框架，可以在不依赖于大型语言模型的情况下实现上下文学习，并避免了模板敏感性和幻觉等问题。 |
| [^68] | [Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery.](http://arxiv.org/abs/2307.05182) | 这项研究提出了一种用于机器人手术中视觉问答定位的共同关注门控视觉-语言嵌入方法，可以为医学生和初级外科医生提供学习和理解手术视频的帮助。 |
| [^69] | [Continual Learning as Computationally Constrained Reinforcement Learning.](http://arxiv.org/abs/2307.04345) | 本文研究了连续学习作为计算受限的强化学习的主题，提出了一个框架和一套工具来解决人工智能领域长期以来的挑战并促进进一步的研究。 |
| [^70] | [Efficient Domain Adaptation of Sentence Embeddings using Adapters.](http://arxiv.org/abs/2307.03104) | 本论文提出了一种通过训练轻量级适配器来高效域自适应句子嵌入的方法，避免了微调整个句子嵌入模型的资源消耗。通过训练特定领域的适配器，可以在不同领域中使用同一模型获得良好的性能。 |
| [^71] | [Review helps learn better: Temporal Supervised Knowledge Distillation.](http://arxiv.org/abs/2307.00811) | 本文提出了一种基于时间的监督知识蒸馏方法，利用评论来帮助学生网络的学习。通过提取学生网络在不同训练阶段的时空特征，并通过动态目标进行训练，实现了对学生网络中旧知识的优化和利用，从而提高了网络的训练性能。 |
| [^72] | [Feasibility of Universal Anomaly Detection without Knowing the Abnormality in Medical Images.](http://arxiv.org/abs/2307.00750) | 本研究探讨了医学图像中通用异常检测的两个关键方面：比较不同的异常检测方法，解决使用正常图像进行验证过程中的偏差问题，并提出了一种通用异常检测方法。 |
| [^73] | [Rapid-INR: Storage Efficient CPU-free DNN Training Using Implicit Neural Representation.](http://arxiv.org/abs/2306.16699) | 本文提出了一种使用隐式神经表示进行高效的无CPU深度神经网络训练的新方法，通过在GPU上直接存储整个数据集以INR格式，减少了数据传输开销，从而加速训练过程。同时，采用高度并行化和实时执行的解码过程，进一步提升了压缩效果。 |
| [^74] | [Scalable Neural Contextual Bandit for Recommender Systems.](http://arxiv.org/abs/2306.14834) | 本研究提出了一种可扩展的神经上下文Bandit算法，通过设计Epistemic Neural Recommendation (ENR)网络结构，实现了大规模的Thompson抽样，显著提高了推荐系统的点击率和用户评分。 |
| [^75] | [Fault Identification of Rotating Machinery Based on Dynamic Feature Reconstruction Signal Graph.](http://arxiv.org/abs/2306.05281) | 本文提出了一种动态特征重构信号图法，在旋转机械故障诊断模型中取得了关键进展，能够动态地选择最优子带的特征系数矩阵，进行适应性信号重构，并从中提取深层特征。 |
| [^76] | [Adversarial Attacks and Defenses for Semantic Communication in Vehicular Metaverses.](http://arxiv.org/abs/2306.03528) | 本文提出了一种层级式SemCom实现的车载虚拟现实框架，可以显著缓解车载虚拟现实应用程序的通信资源压力，并阐述了该框架中SemCom模块的安全风险和可行的防御方法。 |
| [^77] | [Task Relation-aware Continual User Representation Learning.](http://arxiv.org/abs/2306.01792) | 本文提出了一种新的持续用户表示学习方法TERACON，它能够学习通用的用户表示，而不是为每个任务学习任务特定的用户表示，具有很强的实用性和学习能力。 |
| [^78] | [An Evaluation of Log Parsing with ChatGPT.](http://arxiv.org/abs/2306.01590) | 本文评估了ChatGPT在日志解析方面的能力，结果表明它可以通过适当的提示实现有前途的日志解析结果，特别是在few-shot提示下。 |
| [^79] | [Inverse Approximation Theory for Nonlinear Recurrent Neural Networks.](http://arxiv.org/abs/2305.19190) | 该论文证明了使用RNNs逼近非线性序列关系的逆近似定理，进一步将先前在线性RNNs中识别出的记忆难题推广到了一般的非线性情况，并提出了一个有原则的重新参数化方法来克服这些限制。 |
| [^80] | [EgoHumans: An Egocentric 3D Multi-Human Benchmark.](http://arxiv.org/abs/2305.16487) | EgoHumans是一个全面的自我中心多人基准数据集，旨在推进自我中心的人类三维姿态估计和跟踪的最新进展，可以支持各种任务，包括人类检测、跟踪、2D/3D姿态估计和网格恢复等，并且能够捕捉具有挑战性和无核编排的多人场景。 |
| [^81] | [Federated learning for secure development of AI models for Parkinson's disease detection using speech from different languages.](http://arxiv.org/abs/2305.11284) | 本论文利用联邦学习方法，无需共享患者数据，实现在德语、西班牙语和捷克语三种语言数据集上进行帕金森病检测，取得了优于本地模型的诊断准确性。 |
| [^82] | [What's the Problem, Linda? The Conjunction Fallacy as a Fairness Problem.](http://arxiv.org/abs/2305.09535) | 这篇论文探讨了“连接谬误”作为公平性问题在人工智能领域中的重要性，并提出了一些问题，以帮助AI研究人员和从业者避免类似情况在未来中复现。 |
| [^83] | [IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers.](http://arxiv.org/abs/2305.06741) | 本文提出了一种新的方法，在建模电子病历时间序列时，利用直接近似IVP的过程来消除递归计算，从而提高计算效率和训练速度。与目前基于IVP求解器和递归神经网络方法相比，本方法可以达到类似的分类和预测性能。 |
| [^84] | [Learning Decision Trees with Gradient Descent.](http://arxiv.org/abs/2305.03515) | 本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。 |
| [^85] | [Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs.](http://arxiv.org/abs/2305.00948) | 本研究展示了大型语言模型(LLMs)在语言任务上性能不断提高，且首次展示了它们能够生成连贯和有效的语言数据分析。分析和评估它们的元语言能力有助于我们理解它们的一般能力并对语言学理论模型提供新的认识。 |
| [^86] | [SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation.](http://arxiv.org/abs/2305.00795) | SelfDocSeg提出了一种完全基于视觉的自我监督文档分割方法，通过生成伪布局训练图像编码器学习文档对象的表示和定位，克服了标注数据稀缺的挑战。 |
| [^87] | [MASK-CNN-Transformer For Real-Time Multi-Label Weather Recognition.](http://arxiv.org/abs/2304.14857) | 本文提出了一种考虑天气条件复杂共现依赖关系的多标签识别模型MASK-CNN-Transformer，该模型结合了CNN和Transformer，并利用MASK机制以提高泛化能力。 |
| [^88] | [Supporting Human-AI Collaboration in Auditing LLMs with LLMs.](http://arxiv.org/abs/2304.09991) | 本论文通过对安全和公正人工智能专家的采访以及对人工智能协作和感知文献的研究，增强了“AdaTest”审计工具，这个工具可以通过利用人和生成模型的协同优势，进行更严格的大型语言模型审计。 |
| [^89] | [Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with Bridge Block Reconstruction.](http://arxiv.org/abs/2303.12557) | 本文针对视觉Transformer在移动设备上计算要求高的问题，提出了一种带桥块重构的混合视觉Transformer的后训练量化方法，提高其在移动设备上的加速效果。 |
| [^90] | [NeTO:Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing.](http://arxiv.org/abs/2303.11219) | 提出了一种名为NeTO的方法，通过体渲染从2D图像中捕捉固体透明物体的3D几何体。通过采用隐式有符号距离函数（SDF）作为表面表示，并通过自遮挡感知的折射追踪通过体渲染来优化SDF场，可以实现高质量的重建结果。 |
| [^91] | [GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models.](http://arxiv.org/abs/2303.10130) | 该研究调查了GPT（大语言模型）和相关技术对美国劳动力市场的潜在影响，发现大约80%的美国劳动力可能会受到10%的工作任务的影响，涵盖了所有工资水平和各行各业，预示着这些模型可能具有显著的经济、社会和政策影响。 |
| [^92] | [Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement.](http://arxiv.org/abs/2303.08983) | 提出了一种名为数据集增强的策略，一次性改进数据集，从而提高任何经过增强的数据集训练的模型的准确性、鲁棒性和校准性。例如，使用ImageDataNet+训练的ResNet-50在ImageNet验证集上的准确率提高了1.7％，在ImageNetV2上提高了3.5％，在ImageNet-R上提高了10.0％。 |
| [^93] | [Rule-based Out-Of-Distribution Detection.](http://arxiv.org/abs/2303.01860) | 本文提出了一种基于XAI的方法，通过不同指标鉴别in-distribution和out of-distribution之间的相似性，从而实现非参数统计模型的OoD检测。该方法在多个复杂场景下表现出良好的检测准确度和评估训练和操作条件的接近程度。 |
| [^94] | [Feature Affinity Assisted Knowledge Distillation and Quantization of Deep Neural Networks on Label-Free Data.](http://arxiv.org/abs/2302.10899) | 本文提出了一种特征亲和力辅助的知识蒸馏方法，通过结合logit损失和特征亲和力损失，可以在无标签数据上压缩深度神经网络模型。 |
| [^95] | [Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference.](http://arxiv.org/abs/2302.09582) | 本研究通过操纵大型语言模型中的语言衍生的情绪概念知识表示，探讨了语言是否会因果支持情绪推断。实验结果显示，属性特定的神经元操纵导致情绪推断任务的性能下降，这与人类心理空间中不同属性的重要性有关。这些发现为支持基于语言的情绪推断机制提供了因果证据，并凸显了情绪概念知识的贡献。 |
| [^96] | [Non-separable Covariance Kernels for Spatiotemporal Gaussian Processes based on a Hybrid Spectral Method and the Harmonic Oscillator.](http://arxiv.org/abs/2302.09580) | 该论文提出了一种基于混合谱方法和谐振子的非可分离协方差核的时空高斯过程研究，通过物理论证推导出一类新型的非可分离协方差核，能更好地捕捉观测到的时空相关性。 |
| [^97] | [Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image.](http://arxiv.org/abs/2302.02410) | 本文提出了一种分离迭代细化框架，用于从单个RGB图像中重建相互作用的双手。通过定义2D视觉特征空间和3D关节特征空间，实现了手部重建的像素对齐，并高效地建模手之间的空间关系。 |
| [^98] | [NeSyFOLD: Extracting Logic Programs from Convolutional Neural Networks.](http://arxiv.org/abs/2301.12667) | NeSyFOLD是一种神经符号框架，可以从CNN中提取逻辑规则并生成可解释的分类模型。它使用基于规则的FOLD-SE-M机器学习算法和自动映射算法来将CNN核映射到语义概念，并产生可解释的规则集。 |
| [^99] | [Generalisation Through Negation and Predicate Invention.](http://arxiv.org/abs/2301.07629) | 这篇论文介绍了一种结合了否定和谓词创造的归纳逻辑编程方法，通过学习仅具有全量化身体变量的规则来改善泛化能力，并实现在NOPI中。实验结果表明，这种方法可以提高预测准确性和学习效率。 |
| [^100] | [Towards Explainable Land Cover Mapping: a Counterfactual-based Strategy.](http://arxiv.org/abs/2301.01520) | 本论文提出了一种基于反事实的策略，用于土地覆盖分类任务中的卫星图像时间序列。该方法具有灵活性，能发现土地覆盖类别之间的有趣信息关系，同时通过鼓励时间连续的扰动来得到更稀疏且可解释的解决方案。 |
| [^101] | [RECOMED: A Comprehensive Pharmaceutical Recommendation System.](http://arxiv.org/abs/2301.00280) | RECOMED是基于患者和药物特征设计的一种综合性药物推荐系统，采用人工智能模型和自然语言处理方法进行实现，首次考虑了患者条件和历史来选择合适药物，以及药物相互作用。 |
| [^102] | [Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification.](http://arxiv.org/abs/2212.13939) | 本论文提出一种使用Transformer和相似度度量进行数据增强的方法，以改进阿拉伯文本分类，该方法利用AraGPT-2进行增强，并使用Euclidean、cosine、Jaccard和BLEU距离评估生成的句子。 |
| [^103] | [Learning Latent Representations to Co-Adapt to Humans.](http://arxiv.org/abs/2212.09586) | 机器人学习者在与非稳态人类共同互动时面临挑战，本文通过学习和推理人类策略和策略动态的高级表示，提出了一种算法形式以实现机器人与动态人类共适应。 |
| [^104] | [DeepCut: Unsupervised Segmentation using Graph Neural Networks Clustering.](http://arxiv.org/abs/2212.05853) | 本研究引入了一种轻量级的图神经网络（GNN）来解决图像分割中特征降维的问题，并采用无监督方法进行分割。与传统方法相比，该方法在构建图时同时使用局部特征和原始特征，从而能更好地进行聚类和分类分割。 |
| [^105] | [Beyond Object Recognition: A New Benchmark towards Object Concept Learning.](http://arxiv.org/abs/2212.02710) | 本文提出了一个挑战性的 Object Concept Learning (OCL) 任务，涉及对象属性、作用及其因果关系。作者构建了密集注释的知识库以支持 OCL，提出了 Object Concept Reasoning Network (OCRN) 作为基线，提升了对象认知的发展。 |
| [^106] | [PhysDiff: Physics-Guided Human Motion Diffusion Model.](http://arxiv.org/abs/2212.02500) | PhysDiff是一种物理引导的人体运动扩散模型，通过将物理约束融入扩散过程中，能够生成多样而逼真的人体动作，并解决了现有模型中存在的物理缺陷问题。 |
| [^107] | [One-shot Implicit Animatable Avatars with Model-based Priors.](http://arxiv.org/abs/2212.02469) | 本文提出了ELICIT，一种从单张图片学习人类特定神经辐射场的方法，同时利用3D几何先验和视觉语义先验实现了一次性数据高效的逼真可动3D人体的创建。 |
| [^108] | [PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial Networks for Radar-Based Precipitation Nowcasting.](http://arxiv.org/abs/2211.15046) | 本文提出了一种基于配对互补时间循环一致对抗网络的雷达降水预测方法，该方法包括两个生成器网络和循环一致性损失和对抗性损失。实验证明，该方法在准确性和推广能力方面优于现有的技术方法。 |
| [^109] | [Outcome-Oriented Prescriptive Process Monitoring Based on Temporal Logic Patterns.](http://arxiv.org/abs/2211.04880) | 提出了一种基于时间逻辑模式的结果导向的规范过程监控系统，推荐在过程执行期间必须保证的活动之间的时间关系，以实现期望的结果。 |
| [^110] | [Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis.](http://arxiv.org/abs/2211.02641) | 本文介绍了一种基于SPD流形的图神经网络用于运动想象分类，利用EEG的二阶统计量，相比传统方法具有更好的性能。 |
| [^111] | [FS-DETR: Few-Shot DEtection TRansformer with prompting and without re-training.](http://arxiv.org/abs/2210.04845) | 本文介绍了一种名为FS-DETR的少样本检测变形器，它基于视觉提示的方法实现了在少样本情况下进行目标检测的能力，且无需在测试时进行重新训练。 |
| [^112] | [Bayesian Prompt Learning for Image-Language Model Generalization.](http://arxiv.org/abs/2210.02390) | 本文提出了一种基于贝叶斯方法的提示学习框架，对提示空间进行正则化，提高了对未见提示的泛化能力。 |
| [^113] | [Dataset Distillation Using Parameter Pruning.](http://arxiv.org/abs/2209.14609) | 本文提出了一种使用参数修剪的数据集蒸馏方法，该方法可以在蒸馏过程中修剪难以匹配的参数，提高蒸馏性能。 |
| [^114] | [FiBiNet++: Reducing Model Size by Low Rank Feature Interaction Layer for CTR Prediction.](http://arxiv.org/abs/2209.05016) | FiBiNet++通过引入低秩特征交互层，成功减小了FiBiNet模型大小，并在三个公共数据集上实现了12倍至16倍的参数减小，同时显著提高了性能。 |
| [^115] | [Multipoint-BAX: A New Approach for Efficiently Tuning Particle Accelerator Emittance via Virtual Objectives.](http://arxiv.org/abs/2209.04587) | 本论文提出了一种名为多点-BAX的新方法，通过虚拟目标来高效调整粒子加速器的发射度。该方法避免了使用传统的黑盒优化器进行缓慢而低效的多点查询，并通过快速学习模型计算发射度目标。该方法在Linac相干光源(LCLS)和Facility for Adv中最小化发射度。 |
| [^116] | [Data Fusion in Neuromarketing: Multimodal Analysis of Biosignals, Lifecycle Stages, Current Advances, Datasets, Trends, and Challenges.](http://arxiv.org/abs/2209.00993) | 神经营销研究中的数据融合为实现特定目标提供了新的途径，并且需要先进的处理方法。 |
| [^117] | [Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics.](http://arxiv.org/abs/2208.10533) | 本文提出一种名为评判置信度引导探索的方法，用于将现有的神谕策略纳入标准的演员-评论家强化学习算法中，以提高探索效率。在不确定性高时，该方法会将神谕策略的行动作为建议纳入学习方案中，而在不确定性低时忽略它。 |
| [^118] | [Topical: Learning Repository Embeddings from Source Code using Attention.](http://arxiv.org/abs/2208.09495) | 这篇论文介绍了Topical，一种使用深度神经网络从源代码中学习存储库级嵌入的方法，以实现代码自动生成、代码推荐和代码自动标记等功能。 |
| [^119] | [Differentiable Inductive Logic Programming in High-Dimensional Space.](http://arxiv.org/abs/2208.06652) | 本研究提出了一种在高维空间中进行可微归纳逻辑编程的扩展方法，通过大规模谓词发明来充分利用高维梯度下降的效能，以学习超出现有神经符号ILP系统能力的任务的解决方案。 |
| [^120] | [Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks.](http://arxiv.org/abs/2207.13243) | 这篇综述调查了深度神经网络内部结构内部解释方法，并提出了一种分析方法的分类。这些解释方法对于帮助构建更可信赖的AI是至关重要的。 |
| [^121] | [Differentially Private Partial Set Cover with Applications to Facility Location.](http://arxiv.org/abs/2207.10240) | 本文提出了一个差分隐私算法，用于解决部分集覆盖问题和设施选址问题，该算法在给定输入集合系统的松弛条件下能够输出具有非平凡近似保证的显式集覆盖。 |
| [^122] | [On Specifying for Trustworthiness.](http://arxiv.org/abs/2206.11421) | 本论文识别了自主系统信任度规范化的关键挑战，跨越了不同领域的AS的弹性、信任度、功能、可验证性、安全性以及治理和监管，并强调了与环境不确定性相关的思维挑战。 |
| [^123] | [Generative Pretraining for Black-Box Optimization.](http://arxiv.org/abs/2206.10786) | 该论文提出了一种使用离线数据预训练黑盒优化器的生成框架BONET，使用自回归模型和样本策略合成轨迹以帮助在高维空间中优化昂贵的黑盒函数。 |
| [^124] | [Polynomial-Time Algorithms for Counting and Sampling Markov Equivalent DAGs with Applications.](http://arxiv.org/abs/2205.02654) | 本文提出了一种多项式时间算法，用于在马尔可夫等价类中计数和采样有向无环图。该算法解决了这一领域的长期未解决问题，并且在实验中得到验证，可以在活跃学习因果结构和因果效应识别方面实际应用。 |
| [^125] | [Suspected Object Matters: Rethinking Model's Prediction for One-stage Visual Grounding.](http://arxiv.org/abs/2203.05186) | 该论文研究了一阶视觉定位方法中物体之间关系建模的问题，提出了一种可疑物体转换机制（SOT），通过对可疑物体进行转换和关系建模，提高了模型的预测准确性。 |
| [^126] | [Unsupervised Multimodal Word Discovery based on Double Articulation Analysis with Co-occurrence cues.](http://arxiv.org/abs/2201.06786) | 本研究提出了一种无监督学习方法，基于语音学信息和对象信息，用于从语音信号中发现单词和音素，并同时利用多种模态的对象信息。实验结果表明，该方法在单词发现性能上优于基线方法。 |
| [^127] | [Safe Equilibrium.](http://arxiv.org/abs/2201.04266) | 这项研究提出了一种新的解决概念，安全均衡，该概念模拟了对手有一定概率进行理性行为，有剩余概率进行任意行为。研究证明在所有战略博弈中都存在安全均衡，并提供了计算安全均衡的算法。 |
| [^128] | [CausalAF: Causal Autoregressive Flow for Safety-Critical Driving Scenario Generation.](http://arxiv.org/abs/2110.13939) | CausalAF是一种用于安全关键驾驶场景生成的流模型，通过引入因果关系先验和新颖的因果掩蔽操作，将生成模型从仅从数据中学习相关性的情况扩展到学习生成场景如何引起风险情况的因果机制，以提高系统鲁棒性评估的多样性和效率。 |
| [^129] | [A Review on Deep Learning in UAV Remote Sensing.](http://arxiv.org/abs/2101.10861) | 该综述总结了近年来应用于UAV遥感中的深度学习算法，主要关注分类和回归技术。 |
| [^130] | [Deep Reinforcement Learning for Active High Frequency Trading.](http://arxiv.org/abs/2101.07107) | 本文介绍了一个基于深度强化学习的框架，用于在股票市场中进行活跃的高频交易。通过训练DRL代理来交易股票，并使用Proximal Policy Optimization算法进行优化。通过仅选择具有最大价格变动的训练样本来提高训练数据的信噪比。通过实验证明，代理能够创建对底层环境的动态表示，并能够识别偶尔出现的规律。 |
| [^131] | [One-Vote Veto: Semi-Supervised Learning for Low-Shot Glaucoma Diagnosis.](http://arxiv.org/abs/2012.04841) | 本文扩展了孪生网络，提出了一种可在有限标记数据和不平衡情况下进行低样本学习的训练方法，并引入了半监督学习策略，利用额外的未标记数据来提高准确性。 |
| [^132] | [Generalization in Deep Learning.](http://arxiv.org/abs/1710.05468) | 本文从理论上解释了为什么以及如何深度学习能够在容量大、复杂性高、可能存在算法不稳定性、非鲁棒性和尖锐极小值的情况下实现良好的泛化，提出了一些新的开放问题，并讨论了研究结果的局限性。 |
| [^133] | [DeepTransport: Learning Spatial-Temporal Dependency for Traffic Condition Forecasting.](http://arxiv.org/abs/1709.09585) | DeepTransport是一个利用CNN和RNN学习交通网络拓扑内时空交通信息的端到端框架，通过考虑周围位置的交通情况来解决效果衰减问题，并引入了注意力机制来对齐时空信息。他们还构建并发布了一个分辨率为5分钟的真实世界大型交通状况数据集。 |

# 详细

[^1]: ClothesNet：一个具有模拟服装环境的信息丰富的3D服装模型库

    ClothesNet: An Information-Rich 3D Garment Model Repository with Simulated Clothes Environment. (arXiv:2308.09987v1 [cs.RO])

    [http://arxiv.org/abs/2308.09987](http://arxiv.org/abs/2308.09987)

    ClothesNet是一个大规模的3D服装对象数据集，包含了服装特征、边界线和关键点的丰富注释，可用于促进计算机视觉和机器人交互任务。同时，通过模拟服装环境，ClothesNet还可以进行包括分类、边界线分割、关键点检测在内的服装感知任务以及包括重新摆放、折叠、挂衣和穿衣等在内的机器人交互任务的评估。在真实环境下的实验证明了ClothesNet的有效性。

    

    我们提出了ClothesNet：一个大规模的包含具有信息丰富注释的3D服装对象的数据集。我们的数据集包括约4400个涵盖11个类别的模型，这些模型标注了服装特征、边界线和关键点。ClothesNet可以用于促进各种计算机视觉和机器人交互任务。利用我们的数据集，我们建立了服装感知的基准任务，包括分类、边界线分割和关键点检测，并开发了用于机器人交互任务的模拟服装环境，包括重新摆放、折叠、挂衣和穿衣等。我们还展示了我们的ClothesNet在真实环境下的有效性。补充材料和数据集可在我们的项目网页上获得。

    We present ClothesNet: a large-scale dataset of 3D clothes objects with information-rich annotations. Our dataset consists of around 4400 models covering 11 categories annotated with clothes features, boundary lines, and keypoints. ClothesNet can be used to facilitate a variety of computer vision and robot interaction tasks. Using our dataset, we establish benchmark tasks for clothes perception, including classification, boundary line segmentation, and keypoint detection, and develop simulated clothes environments for robotic interaction tasks, including rearranging, folding, hanging, and dressing. We also demonstrate the efficacy of our ClothesNet in real-world experiments. Supplemental materials and dataset are available on our project webpage.
    
[^2]: 欧洲范围内的人工智能：对意识、态度和信任的研究

    Artificial Intelligence across Europe: A Study on Awareness, Attitude and Trust. (arXiv:2308.09979v1 [cs.CY])

    [http://arxiv.org/abs/2308.09979](http://arxiv.org/abs/2308.09979)

    本研究通过调查欧洲公民对人工智能的意识、态度和信任，发现尽管意识水平较低，但一半以上的人对人工智能持积极态度。

    

    本文介绍了一项广泛研究的结果，调查了来自八个不同国家（法国、德国、意大利、荷兰、波兰、罗马尼亚、西班牙和瑞典）的4,006名欧洲公民对人工智能的观点。研究旨在更好地了解欧洲上下文中人们的观点和认知，这已经受到重要政策行动和监管过程的影响。为了调查欧洲公民的观念，我们设计和验证了一个新的问卷（PAICE），围绕人们的意识、态度和信任三个维度进行构建。我们观察到，尽管意识水平自我评估较低，但对人工智能的态度在半数以上的人口中非常积极。通过对收集到的结果进行反思，我们强调了隐含的矛盾点，并确定了可能影响信任生态系统的建立和包容性人工智能发展的趋势。

    This paper presents the results of an extensive study investigating the opinions on Artificial Intelligence (AI) of a sample of 4,006 European citizens from eight distinct countries (France, Germany, Italy, Netherlands, Poland, Romania, Spain, and Sweden). The aim of the study is to gain a better understanding of people's views and perceptions within the European context, which is already marked by important policy actions and regulatory processes. To survey the perceptions of the citizens of Europe we design and validate a new questionnaire (PAICE) structured around three dimensions: people's awareness, attitude, and trust. We observe that while awareness is characterized by a low level of self-assessed competency, the attitude toward AI is very positive for more than half of the population. Reflecting upon the collected results, we highlight implicit contradictions and identify trends that may interfere with the creation of an ecosystem of trust and the development of inclusive AI po
    
[^3]: 显式时间嵌入的级联注意力网络用于信息流行度预测

    Explicit Time Embedding Based Cascade Attention Network for Information Popularity Prediction. (arXiv:2308.09976v1 [cs.SI])

    [http://arxiv.org/abs/2308.09976](http://arxiv.org/abs/2308.09976)

    这篇论文提出了一种显式时间嵌入的级联注意力网络，用于预测信息流行度。该网络通过时间嵌入方法将时间属性融入节点特征中，然后采用级联图注意力编码器和级联序列注意力编码器来充分学习级联图和级联序列的表示。

    

    在社交网络中，预测信息级联的流行度是一个基本问题。捕捉时间属性和级联角色信息（如级联图和级联序列）对于理解信息级联是必要的。当前的方法很少关注将这些信息统一起来进行流行度预测，这使得它们无法有效地对级联的全部属性进行建模，以实现令人满意的预测性能。在本文中，我们提出了一种显式的基于时间嵌入的级联注意力网络(TCAN)作为一种针对大规模信息网络的新型流行度预测架构。TCAN通过一种通用的时间嵌入方法(TC)将时间属性（周期性、线性和非线性缩放）融入节点特征中，然后采用级联图注意力编码器(CGAT)和级联序列注意力编码器(CSAT)来充分学习级联图和级联序列的表示。

    Predicting information cascade popularity is a fundamental problem in social networks. Capturing temporal attributes and cascade role information (e.g., cascade graphs and cascade sequences) is necessary for understanding the information cascade. Current methods rarely focus on unifying this information for popularity predictions, which prevents them from effectively modeling the full properties of cascades to achieve satisfactory prediction performances. In this paper, we propose an explicit Time embedding based Cascade Attention Network (TCAN) as a novel popularity prediction architecture for large-scale information networks. TCAN integrates temporal attributes (i.e., periodicity, linearity, and non-linear scaling) into node features via a general time embedding approach (TE), and then employs a cascade graph attention encoder (CGAT) and a cascade sequence attention encoder (CSAT) to fully learn the representation of cascade graphs and cascade sequences. We use two real-world dataset
    
[^4]: 可拆卸式迁移学习用于选择性源任务遗忘

    Disposable Transfer Learning for Selective Source Task Unlearning. (arXiv:2308.09971v1 [cs.LG])

    [http://arxiv.org/abs/2308.09971](http://arxiv.org/abs/2308.09971)

    本论文提出了一种称为可拆卸式迁移学习的新范式，通过引入梯度碰撞损失，该方法可以选择性地遗忘源任务而不降低目标任务的性能。

    

    迁移学习被广泛用于训练深度神经网络(DNN)来构建强大的表示。即使在预训练模型适应目标任务后，特征提取器的表示性能仍然保留在一定程度上。由于预训练模型的性能可以被视为所有者的私有财产，自然而然地寻求预训练权重的广义性能的独占权是合理的。为了解决这个问题，我们提出了一种称为可拆卸式迁移学习(DTL)的新范式，它只处理源任务而不降低目标任务的性能。为了实现知识的遗忘，我们提出了一种新的损失函数，称为梯度碰撞损失(GC损失)。GC损失通过引导不同mini-batches的梯度向量朝不同方向前进，选择性地遗忘源知识。模型是否成功遗忘源任务通过“背驮式学习准确性”(PL准确性)来衡量。

    Transfer learning is widely used for training deep neural networks (DNN) for building a powerful representation. Even after the pre-trained model is adapted for the target task, the representation performance of the feature extractor is retained to some extent. As the performance of the pre-trained model can be considered the private property of the owner, it is natural to seek the exclusive right of the generalized performance of the pre-trained weight. To address this issue, we suggest a new paradigm of transfer learning called disposable transfer learning (DTL), which disposes of only the source task without degrading the performance of the target task. To achieve knowledge disposal, we propose a novel loss named Gradient Collision loss (GC loss). GC loss selectively unlearns the source knowledge by leading the gradient vectors of mini-batches in different directions. Whether the model successfully unlearns the source task is measured by piggyback learning accuracy (PL accuracy). PL
    
[^5]: 通过学习内心独白解决视觉语言任务

    Tackling Vision Language Tasks Through Learning Inner Monologues. (arXiv:2308.09970v1 [cs.CL])

    [http://arxiv.org/abs/2308.09970](http://arxiv.org/abs/2308.09970)

    通过学习内心独白，提出了一种新方法（IMMO）来解决复杂的视觉语言任务，克服了混合融合和特征对齐方法所面临的优化和可解释性问题。

    

    视觉语言任务需要AI模型对视觉和文本内容进行理解和推理。基于大型语言模型（LLM）的强大力量，出现了两种主要方法：（1）LLM和视觉-语言模型（VLM）之间的混合融合，其中视觉输入首先被VLM转化为语言描述，成为LLM生成最终答案的输入；（2）语言空间中的视觉特征对齐，其中视觉输入被编码为嵌入向量，并通过进一步的监督微调将其投影到LLM的语言空间中。第一种方法具有轻量级的训练成本和可解释性，但很难以端到端的方式进行优化。第二种方法具有相当的性能，但特征对齐通常需要大量的训练数据，并且缺乏可解释性。为了解决这个困境，我们提出了一种新的方法，即内心独白多模态优化（IMMO），通过模拟思维过程来解决复杂的视觉语言问题。

    Visual language tasks require AI models to comprehend and reason with both visual and textual content. Driven by the power of Large Language Models (LLMs), two prominent methods have emerged: (1) the hybrid integration between LLMs and Vision-Language Models (VLMs), where visual inputs are firstly converted into language descriptions by VLMs, serving as inputs for LLMs to generate final answer(s); (2) visual feature alignment in language space, where visual inputs are encoded as embeddings and projected to LLMs' language space via further supervised fine-tuning. The first approach provides light training costs and interpretability but is hard to be optimized in an end-to-end fashion. The second approach presents decent performance, but feature alignment usually requires large amounts of training data and lacks interpretability. To tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal Optimization (IMMO), to solve complex vision language problems by simulating in
    
[^6]: 基于风格对齐的畸变感知语义分割

    Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation. (arXiv:2308.09965v1 [cs.CV])

    [http://arxiv.org/abs/2308.09965](http://arxiv.org/abs/2308.09965)

    在自动驾驶中遇到未知对象是不可避免的，为了解决这个问题，我们提出了一种基于风格对齐的畸变感知语义分割方法。通过减小OoD数据和驾驶场景之间的域差异，我们改进了OoD合成过程，并利用预训练模型生成“给定类别之外”的预测结果进行异常分割，从而提高了性能。

    

    在自动驾驶的背景下，在开放世界中遇到未知对象成为不可避免的。因此，将标准的语义分割模型配备异常感知是至关重要的。许多先前的方法利用合成的离群分布（OoD）数据增强来解决这个问题。本文通过减小OoD数据和驾驶场景之间的域差异，有效减少了培训过程中可能作为明显捷径的风格差异，从而改进了OoD合成过程。此外，我们提出了一种简单的微调损失函数，有效地导致预训练的语义分割模型生成“给定类别之外”的预测结果，利用每个像素的OoD分数进行异常分割。通过最小的微调工作，我们的流水线使得预训练模型能够用于异常分割，同时保持原任务的性能。

    Within the context of autonomous driving, encountering unknown objects becomes inevitable during deployment in the open world. Therefore, it is crucial to equip standard semantic segmentation models with anomaly awareness. Many previous approaches have utilized synthetic out-of-distribution (OoD) data augmentation to tackle this problem. In this work, we advance the OoD synthesis process by reducing the domain gap between the OoD data and driving scenes, effectively mitigating the style difference that might otherwise act as an obvious shortcut during training. Additionally, we propose a simple fine-tuning loss that effectively induces a pre-trained semantic segmentation model to generate a ``none of the given classes" prediction, leveraging per-pixel OoD scores for anomaly segmentation. With minimal fine-tuning effort, our pipeline enables the use of pre-trained models for anomaly segmentation while maintaining the performance on the original task.
    
[^7]: 使用GPT-3.5在资源严重匮乏的语言中进行数据到文本生成:需要谷歌翻译的一点帮助

    Data-to-text Generation for Severely Under-Resourced Languages with GPT-3.5: A Bit of Help Needed from Google Translate. (arXiv:2308.09957v1 [cs.CL])

    [http://arxiv.org/abs/2308.09957](http://arxiv.org/abs/2308.09957)

    本文研究了在资源严重匮乏的语言中使用GPT-3.5进行数据到文本生成的任务。发现few-shot引导对于直接生成目标语言效果更好，而通过英语进行中转后，这种差异消失了。在WebNLG 2023共享任务中，这种few-shot + 翻译系统的变体在所有语言中的表现都优于竞争系统。

    

    LLM（语言模型，如GPT）在处理英语相关任务时表现出色，因为训练数据中英语的比例占主导地位。本文研究了在数据到文本生成任务中，它们如何应对那些在训练数据中严重不足的语言，具体包括爱尔兰语、马耳他语、威尔士语和布里多尼语。我们在GPT-3.5和GPT-4上进行了引导式工程阶段的测试，使用了一小部分示例输入/输出对的各种引导类型和格式。然后我们在两种场景中对两个最有希望的引导进行了全面评估：（i）直接生成目标语言（资源匮乏的语言），（ii）生成英语后再翻译成目标语言。我们发现，在直接生成资源匮乏的语言方面，few-shot引导效果更好，但通过英语进行中转后，这种差异消失了。我们将few-shot + 翻译系统的变体提交到了WebNLG 2023共享任务中，在所有语言中，它们的表现都比竞争系统好得多。

    LLMs like GPT are great at tasks involving English which dominates in their training data. In this paper, we look at how they cope with tasks involving languages that are severely under-represented in their training data, in the context of data-to-text generation for Irish, Maltese, Welsh and Breton. During the prompt-engineering phase we tested a range of prompt types and formats on GPT-3.5 and~4 with a small sample of example input/output pairs. We then fully evaluated the two most promising prompts in two scenarios: (i) direct generation into the under-resourced language, and (ii) generation into English followed by translation into the under-resourced language. We find that few-shot prompting works better for direct generation into under-resourced languages, but that the difference disappears when pivoting via English. The few-shot + translation system variants were submitted to the WebNLG 2023 shared task where they outperformed competitor systems by substantial margins in all lan
    
[^8]: Eva-KELLM：评估LLMs的知识编辑的新基准

    Eva-KELLM: A New Benchmark for Evaluating Knowledge Editing of LLMs. (arXiv:2308.09954v1 [cs.CL])

    [http://arxiv.org/abs/2308.09954](http://arxiv.org/abs/2308.09954)

    Eva-KELLM是一个新的用于评估LLMs知识编辑的基准，提供了一个评估框架和数据集。该基准通过使用原始文档进行知识编辑和多角度的评估来解决了现有研究中收集成本高、表达复杂事实困难、评估视角受限等问题。

    

    大型语言模型（LLMs）的参数中存储着丰富的知识。然而，这些知识随着时间的推移可能变得过时或不合适。因此，对LLMs进行知识编辑并评估其效果引起了越来越多的关注。现有研究主要集中在使用事实三元组进行知识编辑，这不仅在收集上产生高成本，而且在表达复杂事实时也存在困难。此外，这些研究在评估视角上往往受到限制。在本文中，我们提出了Eva-KELLM，用于评估LLMs的知识编辑的新基准。该基准包括一个评估框架和相应的数据集。在我们的框架下，我们首先要求LLM使用原始文档进行知识编辑，与使用事实三元组相比，这提供了一种更方便、更通用的方法。然后我们从多个角度评估更新后的LLM的表现。

    Large language models (LLMs) possess a wealth of knowledge encoded in their parameters. However, this knowledge may become outdated or unsuitable over time. As a result, there has been a growing interest in knowledge editing for LLMs and evaluating its effectiveness. Existing studies primarily focus on knowledge editing using factual triplets, which not only incur high costs for collection but also struggle to express complex facts. Furthermore, these studies are often limited in their evaluation perspectives. In this paper, we propose Eva-KELLM, a new benchmark for evaluating knowledge editing of LLMs. This benchmark includes an evaluation framework and a corresponding dataset. Under our framework, we first ask the LLM to perform knowledge editing using raw documents, which provides a more convenient and universal approach compared to using factual triplets. We then evaluate the updated LLM from multiple perspectives. In addition to assessing the effectiveness of knowledge editing and
    
[^9]: 通过动力系统角度理解自注意机制

    Understanding Self-attention Mechanism via Dynamical System Perspective. (arXiv:2308.09939v1 [cs.CV])

    [http://arxiv.org/abs/2308.09939](http://arxiv.org/abs/2308.09939)

    本文通过动力系统的视角，展示了自注意机制在神经网络性能提升中的重要作用，并且将其比喻为一种僵硬感知步长适配器。

    

    自注意机制（SAM）广泛应用于人工智能的各个领域，并成功提升了不同模型的性能。然而，目前关于该机制的解释主要基于直觉和经验，而缺乏对SAM如何帮助性能的直接建模。为了解决这个问题，本文基于残差神经网络的动力系统视角，首先展示了在普通微分方程的高精度解中普遍存在的内在僵硬现象（SP）也存在于高性能神经网络（NN）中。因此，NN在特征层面上测量SP的能力对于获得高性能是必要的，并且是训练NN困难的重要因素。类似于解决僵硬ODE的自适应步长方法，我们展示了SAM也是一种能够增强模型表征能力的僵硬感知步长适配器。

    The self-attention mechanism (SAM) is widely used in various fields of artificial intelligence and has successfully boosted the performance of different models. However, current explanations of this mechanism are mainly based on intuitions and experiences, while there still lacks direct modeling for how the SAM helps performance. To mitigate this issue, in this paper, based on the dynamical system perspective of the residual neural network, we first show that the intrinsic stiffness phenomenon (SP) in the high-precision solution of ordinary differential equations (ODEs) also widely exists in high-performance neural networks (NN). Thus the ability of NN to measure SP at the feature level is necessary to obtain high performance and is an important factor in the difficulty of training NN. Similar to the adaptive step-size method which is effective in solving stiff ODEs, we show that the SAM is also a stiffness-aware step size adaptor that can enhance the model's representational ability t
    
[^10]: East: 高效准确的安全Transformer推理框架

    East: Efficient and Accurate Secure Transformer Framework for Inference. (arXiv:2308.09923v1 [cs.CR])

    [http://arxiv.org/abs/2308.09923](http://arxiv.org/abs/2308.09923)

    我们提出了一个名为East的框架，以实现高效准确的安全Transformer推理。我们通过设计新的忘却分段多项式求值算法来优化激活函数的运行时间和通信量。此外，我们还设计了安全协议来处理softmax和层归一化。

    

    Transformer已经成功应用于实际应用中，例如ChatGPT，由于其强大的优势。然而，在服务过程中，用户的输入会泄漏给模型提供商。随着人们对隐私的关注，隐私保护的Transformer推理在这类服务中需求量大。对于隐私保护的Transformer推理来说，非线性函数的安全协议至关重要，但研究得不多。因此，设计实用的非线性函数安全协议对于模型性能而言很困难但很重要。在这项工作中，我们提出了一个名为\emph{East}的框架，以实现高效准确的安全Transformer推理。首先，我们提出了一个新的忘却分段多项式求值算法，并应用于激活函数，与之前的方法相比，它将GELU的运行时间和通信量分别减少了1.5倍和2.5倍以上。其次，我们精心设计了用于softmax和层归一化的安全协议，以忠实地保持模型的性能。

    Transformer has been successfully used in practical applications, such as ChatGPT, due to its powerful advantages. However, users' input is leaked to the model provider during the service. With people's attention to privacy, privacy-preserving Transformer inference is on the demand of such services. Secure protocols for non-linear functions are crucial in privacy-preserving Transformer inference, which are not well studied. Thus, designing practical secure protocols for non-linear functions is hard but significant to model performance. In this work, we propose a framework \emph{East} to enable efficient and accurate secure Transformer inference. Firstly, we propose a new oblivious piecewise polynomial evaluation algorithm and apply it to the activation functions, which reduces the runtime and communication of GELU by over 1.5$\times$ and 2.5$\times$, compared to prior arts. Secondly, the secure protocols for softmax and layer normalization are carefully designed to faithfully maintain 
    
[^11]: 累计: 通过恢复面部并映射恢复的面部检测不可预测篡改痕迹的Deepfake视频

    Recap: Detecting Deepfake Video with Unpredictable Tampered Traces via Recovering Faces and Mapping Recovered Faces. (arXiv:2308.09921v1 [cs.CV])

    [http://arxiv.org/abs/2308.09921](http://arxiv.org/abs/2308.09921)

    提出了一种新型的Deepfake检测模型Recap，通过恢复面部并映射恢复的面部暴露了非特定的面部不一致性，扩大了真实和伪造之间的差异。

    

    恶意使用Deepfake技术已经引起了对Deepfake检测的重要研究兴趣。Deepfake操纵经常引入随机篡改痕迹，在不同的面部区域产生不可预测的结果。然而，现有的检测方法严重依赖于特定的伪造指标，随着伪造模式的改进，这些痕迹变得越来越随机化，导致依赖于特定伪造痕迹的方法的检测性能下降。为了解决这个限制，我们提出了一种新型的Deepfake检测模型Recap，通过恢复面部和映射恢复的面部，暴露了非特定的面部不一致性，扩大了真实和伪造之间的差异。在恢复阶段，模型专注于随机遮盖感兴趣区域，并在没有不可预测的篡改痕迹的情况下重建真实的面部，从而为真实的面部产生相对较好的恢复效果，而为伪造的面部产生较差的恢复效果。

    The exploitation of Deepfake techniques for malicious intentions has driven significant research interest in Deepfake detection. Deepfake manipulations frequently introduce random tampered traces, leading to unpredictable outcomes in different facial regions. However, existing detection methods heavily rely on specific forgery indicators, and as the forgery mode improves, these traces become increasingly randomized, resulting in a decline in the detection performance of methods reliant on specific forgery traces. To address the limitation, we propose Recap, a novel Deepfake detection model that exposes unspecific facial part inconsistencies by recovering faces and enlarges the differences between real and fake by mapping recovered faces. In the recovering stage, the model focuses on randomly masking regions of interest (ROIs) and reconstructing real faces without unpredictable tampered traces, resulting in a relatively good recovery effect for real faces while a poor recovery effect fo
    
[^12]: 学习多尺度一致性的自监督电子显微镜实例分割

    Learning Multiscale Consistency for Self-supervised Electron Microscopy Instance Segmentation. (arXiv:2308.09917v1 [cs.CV])

    [http://arxiv.org/abs/2308.09917](http://arxiv.org/abs/2308.09917)

    本文提出了一种利用多尺度视觉表示捕获电子显微镜实例分割中体素级和特征级一致性的新的预训练框架。

    

    由于实例的复杂形态和不充足的注释，电子显微镜（EM）体积中的实例分割面临着重大挑战。自监督学习最近出现作为一种有前途的解决方案，可以获取对于EM实例分割至关重要的细胞组织结构的先验知识。然而，现有的预训练方法往往缺乏捕捉复杂的视觉模式和体素之间的关系的能力，导致所获取的先验知识不足以应用于下游的EM分析任务。在本文中，我们提出了一个新的预训练框架，利用多尺度的视觉表示来捕捉EM体积中的体素级和特征级的一致性。具体地，我们的框架通过重构函数强制实施Siamese网络输出之间的体素级一致性，并结合交叉注意力机制进行软特征匹配，实现精细的特征级一致性。

    Instance segmentation in electron microscopy (EM) volumes poses a significant challenge due to the complex morphology of instances and insufficient annotations. Self-supervised learning has recently emerged as a promising solution, enabling the acquisition of prior knowledge of cellular tissue structures that are essential for EM instance segmentation. However, existing pretraining methods often lack the ability to capture complex visual patterns and relationships between voxels, which results in the acquired prior knowledge being insufficient for downstream EM analysis tasks. In this paper, we propose a novel pretraining framework that leverages multiscale visual representations to capture both voxel-level and feature-level consistency in EM volumes. Specifically, our framework enforces voxel-level consistency between the outputs of a Siamese network by a reconstruction function, and incorporates a cross-attention mechanism for soft feature matching to achieve fine-grained feature-lev
    
[^13]: 不再重复探索的多智能体强化学习方法

    Never Explore Repeatedly in Multi-Agent Reinforcement Learning. (arXiv:2308.09909v1 [cs.LG])

    [http://arxiv.org/abs/2308.09909](http://arxiv.org/abs/2308.09909)

    该论文提出了一种动态奖励缩放方法，以解决多智能体强化学习中的回访问题。实验结果表明，在复杂环境中，特别是在稀疏奖励设置下，该方法能够提高性能。

    

    在多智能体强化学习领域中，内在动机已经成为探索的重要工具。尽管计算许多内在奖励依赖于使用神经网络逼近器估计变分后验，但由于这些神经统计逼近器的表达能力有限，出现了一个明显的挑战。我们将这个挑战定为"回访"问题，即智能体反复探索任务空间中的有限区域。为了解决这个问题，我们提出了一种动态奖励缩放方法。这种方法旨在稳定先前探索区域内的内在奖励的明显波动，并促进更广泛的探索，有效地抑制回访现象。我们的实验结果强调了我们方法的有效性，在像Google Research Football和StarCraft II微操作任务这样的复杂环境中表现出了更好的性能，特别是在稀疏奖励设置下。

    In the realm of multi-agent reinforcement learning, intrinsic motivations have emerged as a pivotal tool for exploration. While the computation of many intrinsic rewards relies on estimating variational posteriors using neural network approximators, a notable challenge has surfaced due to the limited expressive capability of these neural statistics approximators. We pinpoint this challenge as the "revisitation" issue, where agents recurrently explore confined areas of the task space. To combat this, we propose a dynamic reward scaling approach. This method is crafted to stabilize the significant fluctuations in intrinsic rewards in previously explored areas and promote broader exploration, effectively curbing the revisitation phenomenon. Our experimental findings underscore the efficacy of our approach, showcasing enhanced performance in demanding environments like Google Research Football and StarCraft II micromanagement tasks, especially in sparse reward settings.
    
[^14]: LEGO: 对于基于点云的在线多目标跟踪的学习和图优化的模块化跟踪器

    LEGO: Learning and Graph-Optimized Modular Tracker for Online Multi-Object Tracking with Point Clouds. (arXiv:2308.09908v1 [cs.CV])

    [http://arxiv.org/abs/2308.09908](http://arxiv.org/abs/2308.09908)

    本文提出了一个学习和图优化的模块化跟踪器LEGO，通过集成图优化和自注意力机制，提高了在线多目标跟踪中的数据关联性能。使用LiDAR单独进行跟踪的LEGO方法在KITTI目标跟踪评估中表现出了优秀的性能。

    

    在线多目标跟踪（MOT）在自主系统中起着关键作用。现有的最先进方法通常采用跟踪-检测方法，数据关联起到了至关重要的作用。本文提出了一个学习和图优化（LEGO）的模块化跟踪器，以提高数据关联性能。所提出的LEGO跟踪器集成了图优化和自注意力机制，能够有效地制定关联评分图，从而实现准确高效的目标匹配。为了进一步增强状态更新过程，本文还添加了卡尔曼滤波器，通过将对象状态的时间连贯性纳入跟踪中，确保一致的跟踪。与其他在线跟踪方法（包括基于LiDAR和基于LiDAR-相机融合的方法）相比，我们提出的仅利用LiDAR的方法表现出了卓越性能。在提交结果至KITTI目标跟踪评估排行榜时，LEGO排名第一。

    Online multi-object tracking (MOT) plays a pivotal role in autonomous systems. The state-of-the-art approaches usually employ a tracking-by-detection method, and data association plays a critical role. This paper proposes a learning and graph-optimized (LEGO) modular tracker to improve data association performance in the existing literature. The proposed LEGO tracker integrates graph optimization and self-attention mechanisms, which efficiently formulate the association score map, facilitating the accurate and efficient matching of objects across time frames. To further enhance the state update process, the Kalman filter is added to ensure consistent tracking by incorporating temporal coherence in the object states. Our proposed method utilizing LiDAR alone has shown exceptional performance compared to other online tracking approaches, including LiDAR-based and LiDAR-camera fusion-based methods. LEGO ranked 1st at the time of submitting results to KITTI object tracking evaluation ranki
    
[^15]: RAH！RecSys-Assistant-Human：一个具有大型语言模型的人类中心推荐框架

    RAH! RecSys-Assistant-Human: A Human-Central Recommendation Framework with Large Language Models. (arXiv:2308.09904v1 [cs.IR])

    [http://arxiv.org/abs/2308.09904](http://arxiv.org/abs/2308.09904)

    提出了一个人类中心的推荐框架RAH，利用大型语言模型（LLMs）作为助手，实现用户满意度和个性化反馈，并成功应用于学习用户个性和调整推荐系统。

    

    推荐生态系统涉及到推荐系统（计算机）和用户（人类）之间的交互。与推荐系统的角度不同，我们尝试从用户的角度利用大型语言模型（LLMs），并提出一个更加人类中心的推荐框架，命名为RAH。该框架包括推荐系统、助手和人类。助手是一个基于LLMs的个人代理，用于实现用户满意度。助手扮演非侵入性的角色，RAH框架可以适应不同的推荐系统和用户群体。随后，我们实现并评估了RAH框架，用于学习用户个性和代理人类反馈。实验表明：（1）使用学习-行动-评论家和反思机制可以导致更加一致的个性，（2）我们的助手可以有效地代理人类反馈并帮助调整推荐系统。最后，我们讨论了在RAH框架中进一步解决人类中心问题的策略，包括用户``夺权''等问题。

    The recommendation ecosystem involves interactions between recommender systems(Computer) and users(Human). Orthogonal to the perspective of recommender systems, we attempt to utilize LLMs from the perspective of users and propose a more human-central recommendation framework named RAH, which consists of Recommender system, Assistant and Human. The assistant is a LLM-based and personal proxy for a human to achieve user satisfaction. The assistant plays a non-invasion role and the RAH framework can adapt to different recommender systems and user groups. Subsequently, we implement and evaluate the RAH framework for learning user personalities and proxy human feedback. The experiment shows that (1) using learn-action-critic and reflection mechanisms can lead more aligned personality and (2) our assistant can effectively proxy human feedback and help adjust recommender systems. Finally, we discuss further strategies in the RAH framework to address human-central concerns including user contr
    
[^16]: SwinLSTM：使用Swin Transformer和LSTM提高时空预测准确性

    SwinLSTM:Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM. (arXiv:2308.09891v1 [cs.CV])

    [http://arxiv.org/abs/2308.09891](http://arxiv.org/abs/2308.09891)

    SwinLSTM是一种将Swin Transformer和LSTM结合起来以提高时空预测准确性的新循环单元，在四个数据集上表现出优越的性能，特别是在预测准确性方面有显著提高。

    

    将CNN和RNN集成以捕捉时空依赖关系是时空预测任务中常用的策略。然而，CNN学习局部空间信息的属性降低了它们在捕捉时空依赖关系方面的效率，从而限制了它们的预测准确性。本文提出了一种新的循环单元SwinLSTM，它将Swin Transformer块和简化的LSTM集成在一起，将ConvLSTM中的卷积结构替换为自注意机制。此外，我们构建了一个以SwinLSTM单元为核心的时空预测网络。SwinLSTM在Moving MNIST，Human3.6m，TaxiBJ和KTH数据集上表现比最先进的方法更好，特别是在预测准确性方面有显著提高。我们的竞争性实验结果表明，学习全局空间依赖关系对于模型捕捉时空依赖关系更有优势。

    Integrating CNNs and RNNs to capture spatiotemporal dependencies is a prevalent strategy for spatiotemporal prediction tasks. However, the property of CNNs to learn local spatial information decreases their efficiency in capturing spatiotemporal dependencies, thereby limiting their prediction accuracy. In this paper, we propose a new recurrent cell, SwinLSTM, which integrates Swin Transformer blocks and the simplified LSTM, an extension that replaces the convolutional structure in ConvLSTM with the self-attention mechanism. Furthermore, we construct a network with SwinLSTM cell as the core for spatiotemporal prediction. Without using unique tricks, SwinLSTM outperforms state-of-the-art methods on Moving MNIST, Human3.6m, TaxiBJ, and KTH datasets. In particular, it exhibits a significant improvement in prediction accuracy compared to ConvLSTM. Our competitive experimental results demonstrate that learning global spatial dependencies is more advantageous for models to capture spatiotempo
    
[^17]: 归纳偏差学习: 用大语言模型生成代码模型

    Inductive-bias Learning: Generating Code Models with Large Language Model. (arXiv:2308.09890v1 [cs.LG])

    [http://arxiv.org/abs/2308.09890](http://arxiv.org/abs/2308.09890)

    这篇论文提出了一种新的学习方法，称为归纳偏差学习（IBL），它将上下文学习（ICL）和代码生成相结合，通过输入训练数据到提示中，输出相应的代码。

    

    大型语言模型(LLMs)因其在上下文学习(ICL)方面的能力而受到关注。ICL技术在不更新LLM参数的情况下，仅通过输入训练数据到提示中即可实现基于规则的高准确性推理。虽然ICL是一个发展中的领域，还有许多未解答的问题，但LLMs本身作为推理模型似乎实现了不需要明确指出"归纳偏差"的推理。另一方面，代码生成也是LLMs的一项重要应用。代码生成的准确性大大提高，使得即使非工程师也可以通过精心设计的提示来生成执行所需任务的代码。本文提出了一种新颖的“学习”方法，称为“归纳偏差学习（IBL）”，它结合了ICL和代码生成的技术。IBL的思想很直观。与ICL类似，IBL将训练数据输入到提示中，并输出相应的代码。

    Large Language Models(LLMs) have been attracting attention due to a ability called in-context learning(ICL). ICL, without updating the parameters of a LLM, it is possible to achieve highly accurate inference based on rules ``in the context'' by merely inputting a training data into the prompt. Although ICL is a developing field with many unanswered questions, LLMs themselves serves as a inference model, seemingly realizing inference without explicitly indicate ``inductive bias''. On the other hand, a code generation is also a highlighted application of LLMs. The accuracy of code generation has dramatically improved, enabling even non-engineers to generate code to perform the desired tasks by crafting appropriate prompts. In this paper, we propose a novel ``learning'' method called an ``Inductive-Bias Learning (IBL)'', which combines the techniques of ICL and code generation. An idea of IBL is straightforward. Like ICL, IBL inputs a training data into the prompt and outputs a code with 
    
[^18]: 张量压缩的反向传播免费训练（物理信息）的神经网络

    Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks. (arXiv:2308.09858v1 [cs.LG])

    [http://arxiv.org/abs/2308.09858](http://arxiv.org/abs/2308.09858)

    本文提出了一个完全无需反向传播的神经网络训练框架，并通过张量压缩的方差约减方法和混合梯度评估方法改进了优化和效率。同时，还扩展了框架用于物理信息的神经网络的估计。

    

    反向传播（BP）被广泛用于神经网络训练中计算梯度。然而，由于缺乏硬件和软件资源来支持自动微分，在边缘设备上实现BP是困难的。这大大增加了设备上训练加速器的设计复杂性和上市时间。本文提出了一个完全无需BP的框架，只需要前向传播就可以训练实际的神经网络。我们的技术贡献有三个方面。首先，我们提出了一种张量压缩的方差约减方法，极大提高了零阶（ZO）优化的可扩展性，使其能够处理大于以前ZO方法能力的网络尺寸。其次，我们提出了一种混合梯度评估方法，提高了ZO训练的效率。最后，我们通过提出一种稀疏格方法来扩展我们的BP-free训练框架，用于物理信息的神经网络（PINNs）的估计。

    Backward propagation (BP) is widely used to compute the gradients in neural network training. However, it is hard to implement BP on edge devices due to the lack of hardware and software resources to support automatic differentiation. This has tremendously increased the design complexity and time-to-market of on-device training accelerators. This paper presents a completely BP-free framework that only requires forward propagation to train realistic neural networks. Our technical contributions are three-fold. Firstly, we present a tensor-compressed variance reduction approach to greatly improve the scalability of zeroth-order (ZO) optimization, making it feasible to handle a network size that is beyond the capability of previous ZO approaches. Secondly, we present a hybrid gradient evaluation approach to improve the efficiency of ZO training. Finally, we extend our BP-free training framework to physics-informed neural networks (PINNs) by proposing a sparse-grid approach to estimate the 
    
[^19]: 用可证明概率保证在深度神经网络中枚举安全区域

    Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees. (arXiv:2308.09842v1 [cs.LG])

    [http://arxiv.org/abs/2308.09842](http://arxiv.org/abs/2308.09842)

    通过epsilon-ProVe方法，我们提出了一种高效近似的方法来枚举深度神经网络中的安全区域，并提供了可证明概率保证的紧密下估计。

    

    识别安全区域是保证基于深度神经网络（DNNs）系统的信任的关键点。为此，我们引入了AllDNN-Verification问题：给定一个安全属性和一个DNN，枚举属性输入域的所有安全区域，即属性成立的区域。由于问题的#P难度，我们提出了一种高效的近似方法叫做epsilon-ProVe。我们的方法通过统计预测容限限制获得可控低估的输出可达集，并能够提供一个具有可证明概率保证的安全区域的紧密下估计。我们在不同的标准基准测试上进行的实证评估显示了我们方法的可扩展性和有效性，为这种新型的DNN验证提供了有价值的见解。

    Identifying safe areas is a key point to guarantee trust for systems that are based on Deep Neural Networks (DNNs). To this end, we introduce the AllDNN-Verification problem: given a safety property and a DNN, enumerate the set of all the regions of the property input domain which are safe, i.e., where the property does hold. Due to the #P-hardness of the problem, we propose an efficient approximation method called epsilon-ProVe. Our approach exploits a controllable underestimation of the output reachable sets obtained via statistical prediction of tolerance limits, and can provide a tight (with provable probabilistic guarantees) lower estimate of the safe areas. Our empirical evaluation on different standard benchmarks shows the scalability and effectiveness of our method, offering valuable insights for this new type of verification of DNNs.
    
[^20]: 大型语言模型和认知架构的协同集成对于稳健人工智能的探索分析

    Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis. (arXiv:2308.09830v1 [cs.AI])

    [http://arxiv.org/abs/2308.09830](http://arxiv.org/abs/2308.09830)

    本文探索了将大型语言模型和认知架构相结合的替代方案，通过协同方法互补各自的弱点和限制，从而实现更稳健和复杂的人工智能系统。

    

    本文探讨了在构建表现出智能行为的人工智能代理时，将大型语言模型(LLMs)和认知架构(CAs) 进行集成的替代方案。在理论模型的指导下，通过初步的经验数据支持，我们假设不同的协同方法可以互补它们各自的弱点和限制，从而培育出更稳健和复杂的人工智能系统。此外，我们还讨论了每种方法所涉及的权衡和挑战。

    This paper explores alternatives for integrating two subdisciplines of AI in the construction of artificial agents that exhibit intelligent behavior: Large Language Models (LLMs) and Cognitive Architectures (CAs). Guided by theoretical models and supported by preliminary empirical data, we hypothesize how diverse synergistic approaches can mutually compensate for their respective weaknesses and limitations, ultimately fostering more robust and sophisticated artificial intelligence systems. Additionally, we discuss the tradeoffs and challenges associated with each approach.
    
[^21]: 一幅图像胜过千言万语：内容审核软件的变形测试框架

    An Image is Worth a Thousand Toxic Words: A Metamorphic Testing Framework for Content Moderation Software. (arXiv:2308.09810v1 [cs.SE])

    [http://arxiv.org/abs/2308.09810](http://arxiv.org/abs/2308.09810)

    本文提出了OASIS，一个用于内容审核软件的变形测试框架，以解决现代社交媒体平台上恶意内容的传播问题。

    

    社交媒体平台的指数增长为人类社会的沟通和内容传播带来了革命。然而，这些平台越来越多地被滥用来传播有害内容，包括仇恨言论、恶意广告和色情内容，导致严重的负面后果，如对青少年心理健康的伤害。尽管在开发和部署文本和图像内容审核方法方面已经做出了巨大努力，但恶意用户可以通过将文字嵌入图像中来规避审核，例如文字的截图，通常带有一些干扰。我们发现现代内容审核软件对此类恶意输入的性能仍然未被充分探索。在这项工作中，我们提出了OASIS，一个用于内容审核软件的变形测试框架。OASIS采用了21个转换规则，这些规则是从我们对来自4个流行社交媒体应用（包括Twitter、Instagram）收集的5000个真实有害内容的试验研究总结出来的。

    The exponential growth of social media platforms has brought about a revolution in communication and content dissemination in human society. Nevertheless, these platforms are being increasingly misused to spread toxic content, including hate speech, malicious advertising, and pornography, leading to severe negative consequences such as harm to teenagers' mental health. Despite tremendous efforts in developing and deploying textual and image content moderation methods, malicious users can evade moderation by embedding texts into images, such as screenshots of the text, usually with some interference. We find that modern content moderation software's performance against such malicious inputs remains underexplored. In this work, we propose OASIS, a metamorphic testing framework for content moderation software. OASIS employs 21 transform rules summarized from our pilot study on 5,000 real-world toxic contents collected from 4 popular social media applications, including Twitter, Instagram,
    
[^22]: VL-PET：通过粒度控制实现视觉和语言参数高效调整

    VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control. (arXiv:2308.09804v1 [cs.CV])

    [http://arxiv.org/abs/2308.09804](http://arxiv.org/abs/2308.09804)

    本文提出了一种名为VL-PET的视觉和语言参数高效调整框架，通过粒度控制机制对模块化修改进行有效控制，克服了现有技术在性能和功能差距方面的不足。

    

    随着预训练语言模型（PLM）的模型规模迅速增长，全面微调在模型训练和存储方面变得代价高昂。在视觉与语言（VL）中，提出了参数高效调整（PET）技术，将模块化修改（例如Adapter和LoRA）集成到编码器-解码器PLMs中。通过调整一小组可训练参数，这些技术的性能与全面微调相当。然而，过度的模块化修改和忽视编码器和解码器之间的功能差距可能导致性能降低，而现有的PET技术（例如VL-Adapter）忽视了这些关键问题。在本文中，我们提出了一种名为Vision-and-Language Parameter-Efficient Tuning（VL-PET）的框架，通过一种新颖的粒度控制机制对模块化修改进行有效控制。通过考虑由这种机制生成的不同粒度控制矩阵，可以实例化多种与模型无关的VL-PET模块。

    As the model size of pre-trained language models (PLMs) grows rapidly, full fine-tuning becomes prohibitively expensive for model training and storage. In vision-and-language (VL), parameter-efficient tuning (PET) techniques are proposed to integrate modular modifications (e.g., Adapter and LoRA) into encoder-decoder PLMs. By tuning a small set of trainable parameters, these techniques perform on par with full fine-tuning. However, excessive modular modifications and neglecting the functionality gap between the encoders and decoders can lead to performance degradation, while existing PET techniques (e.g., VL-Adapter) overlook these critical issues. In this paper, we propose a Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose effective control over modular modifications via a novel granularity-controlled mechanism. Considering different granularity-controlled matrices generated by this mechanism, a variety of model-agnostic VL-PET modules can be instantiated fr
    
[^23]: 基于事件的动态图表示学习在专利申请趋势预测中的应用

    Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction. (arXiv:2308.09780v1 [cs.AI])

    [http://arxiv.org/abs/2308.09780](http://arxiv.org/abs/2308.09780)

    本研究提出了一种基于事件的动态图学习框架，用于准确预测专利申请趋势。该方法利用公司和分类代码的可记忆表示，通过历史记忆和当前信息更新来捕捉语义接近性。

    

    准确预测公司在未来一段时间内将申请哪些类型的专利能够揭示出它们的发展战略，并帮助其提前发现潜在的合作伙伴或竞争对手。然而，由于对公司不断变化的偏好和对分类代码的语义关联的建模困难，这个问题在之前的研究中鲜有涉及。为了弥补这一空白，我们提出了一种基于事件的动态图学习框架，用于预测专利申请趋势。具体而言，我们的方法建立在公司和专利分类代码的可记忆表示基础上。当观察到一个新的专利时，相关公司和分类代码的表示根据历史记忆和当前编码的信息进行更新。此外，提供了一个层次化消息传递机制，以捕捉专利分类代码的语义接近性。

    Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modelling companies' continuously evolving preferences and capturing the semantic correlations of classification codes. To fill in this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by u
    
[^24]: 受冷落: 相似度分数的反差效应

    Taken by Surprise: Contrast effect for Similarity Scores. (arXiv:2308.09765v1 [cs.CL])

    [http://arxiv.org/abs/2308.09765](http://arxiv.org/abs/2308.09765)

    提出了一种新的相似度度量方法，称为“惊喜分数”，该方法能够考虑对象的上下文信息并显著提高零样本和少样本文档分类任务的性能。

    

    准确评估物体向量嵌入的相似度对于自然语言处理、信息检索和分类任务至关重要。流行的相似度分数（如余弦相似度）基于嵌入向量对，并忽略了从中提取对象的分布。人类对物体相似度的感知显著取决于对象出现的上下文。在这项工作中，我们提出了“惊喜分数”，这是一个对整体进行归一化的相似度度量，包括了人类感知的反差效应，并显著提高了零样本和少样本文档分类任务的性能。此分数量化了在两个元素之间找到给定相似度的惊喜，相对于成对的整体相似度。我们在零样本/少样本分类和聚类任务上评估了这个度量，通常发现与原始余弦相似度相比，性能提高了10-15\%。我们的代码...

    Accurately evaluating the similarity of object vector embeddings is of critical importance for natural language processing, information retrieval and classification tasks. Popular similarity scores (e.g cosine similarity) are based on pairs of embedding vectors and disregard the distribution of the ensemble from which objects are drawn. Human perception of object similarity significantly depends on the context in which the objects appear. In this work we propose the \emph{surprise score}, an ensemble-normalized similarity metric that encapsulates the contrast effect of human perception and significantly improves the classification performance on zero- and few-shot document classification tasks. This score quantifies the surprise to find a given similarity between two elements relative to the pairwise ensemble similarities. We evaluate this metric on zero/few shot classification and clustering tasks and typically find 10-15\% better performance compared to raw cosine similarity. Our cod
    
[^25]: 去除背景对于神经网络在时尚图像分类和分割中的性能影响

    The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation. (arXiv:2308.09764v1 [cs.CV])

    [http://arxiv.org/abs/2308.09764](http://arxiv.org/abs/2308.09764)

    本研究通过去除时尚图像的背景，提高了数据质量和模型性能，在多个方面进行了广泛的比较实验，结果表明背景去除对于模型训练有积极的影响。

    

    时尚理解是计算机视觉中的热门话题，在市场上具有很大的商业价值。由于服装的巨大多样性以及各种场景和背景的存在，时尚理解对于计算机视觉仍然是一个很大的挑战。在这项工作中，我们尝试去除时尚图像中的背景，以提高数据质量并提高模型性能。通过利用显著性物体检测，我们可以对时尚数据进行背景去除。被去除背景的时尚图像与时尚数据集中的原始图像形成对比。我们对这两种类型的图像进行了广泛的比较实验，包括模型架构、模型初始化、与其他训练技巧和数据增强的兼容性以及目标任务类型。实验证明，背景去除对于模型训练在多个方面都有影响。

    Fashion understanding is a hot topic in computer vision, with many applications having great business value in the market. Fashion understanding remains a difficult challenge for computer vision due to the immense diversity of garments and various scenes and backgrounds. In this work, we try removing the background from fashion images to boost data quality and increase model performance. Having fashion images of evident persons in fully visible garments, we can utilize Salient Object Detection to achieve the background removal of fashion data to our expectations. A fashion image with the background removed is claimed as the "rembg" image, contrasting with the original one in the fashion dataset. We conducted extensive comparative experiments with these two types of images on multiple aspects of model training, including model architectures, model initialization, compatibility with other training tricks and data augmentations, and target task types. Our experiments show that background 
    
[^26]: 一种适用于非静态环境中多目标强化学习的稳健策略引导算法

    A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments. (arXiv:2308.09734v1 [cs.LG])

    [http://arxiv.org/abs/2308.09734](http://arxiv.org/abs/2308.09734)

    提出了一种适用于非静态环境中多目标强化学习的稳健策略引导算法，能够在线演化一个凸覆盖策略集，同时满足目标偏好空间的探索需求。

    

    多目标马尔可夫决策过程是一类需要满足马尔可夫随机过程特性的序贯决策问题。多目标强化学习方法通过融合强化学习和多目标优化技术来解决这个问题。然而，这些方法在处理非静态环境时适应性不足。本文引入了一种发展性优化方法，可以在线探索定义的目标偏好空间，同时演化策略覆盖集。我们提出了一种新颖的多目标强化学习算法，可以在非静态环境中稳健地在线演化一个凸覆盖策略集。

    Multi-objective Markov decision processes are a special kind of multi-objective optimization problem that involves sequential decision making while satisfying the Markov property of stochastic processes. Multi-objective reinforcement learning methods address this problem by fusing the reinforcement learning paradigm with multi-objective optimization techniques. One major drawback of these methods is the lack of adaptability to non-stationary dynamics in the environment. This is because they adopt optimization procedures that assume stationarity to evolve a coverage set of policies that can solve the problem. This paper introduces a developmental optimization approach that can evolve the policy coverage set while exploring the preference space over the defined objectives in an online manner. We propose a novel multi-objective reinforcement learning algorithm that can robustly evolve a convex coverage set of policies in an online manner in non-stationary environments. We compare the prop
    
[^27]: 多目标马尔可夫决策过程中的内在动机层次策略学习

    Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes. (arXiv:2308.09733v1 [cs.LG])

    [http://arxiv.org/abs/2308.09733](http://arxiv.org/abs/2308.09733)

    该论文提出了一种解决多目标马尔可夫决策过程中的问题的方法，通过学习通用的技能集，使得策略覆盖集能够在非稳态环境中持续演化，从而提高性能。

    

    多目标马尔可夫决策过程是涉及多个相互冲突的奖励函数的顺序决策问题，这些函数无法在没有妥协的情况下同时进行优化。这种问题无法像传统情况下那样通过单个最优策略来解决。相反，多目标强化学习方法发展了一个可以满足解决问题中所有可能偏好的最优策略覆盖集。然而，许多这些方法无法将其覆盖集推广到在非稳态环境中工作。在这些环境中，状态转移和奖励分布的参数随时间变化。这限制导致了进化策略集的性能严重下降。为了克服这个限制，需要学习一组通用的技能，可以在环境动态变化时引导策略覆盖集的演变，从而促进持续学习

    Multi-objective Markov decision processes are sequential decision-making problems that involve multiple conflicting reward functions that cannot be optimized simultaneously without a compromise. This type of problems cannot be solved by a single optimal policy as in the conventional case. Alternatively, multi-objective reinforcement learning methods evolve a coverage set of optimal policies that can satisfy all possible preferences in solving the problem. However, many of these methods cannot generalize their coverage sets to work in non-stationary environments. In these environments, the parameters of the state transition and reward distribution vary over time. This limitation results in significant performance degradation for the evolved policy sets. In order to overcome this limitation, there is a need to learn a generic skill set that can bootstrap the evolution of the policy coverage set for each shift in the environment dynamics therefore, it can facilitate a continuous learning 
    
[^28]: Baird反例已解决：以调试两个时间尺度算法的示例。

    Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm. (arXiv:2308.09732v1 [cs.LG])

    [http://arxiv.org/abs/2308.09732](http://arxiv.org/abs/2308.09732)

    这篇论文解决了Baird反例上的收敛问题，通过一种具有收敛保证的算法，实现了线性收敛速度。

    

    Baird反例是由Leemon Baird在1995年提出的，首先用于证明Temporal Difference (TD(0))算法在这个例子上发散。从那时起，它经常被用来测试和比较离策略学习算法。梯度TD算法解决了TD在Baird反例上的发散问题。然而，它们在这个例子上的收敛仍然非常缓慢，而且缓慢的本质还不被很好地理解。本文旨在特别理解为什么TDC在这个例子上慢，并提供调试分析来理解这种行为。我们的调试技术可以用来研究两个时间尺度随机逼近算法的收敛行为。我们还提供了最近的Impression GTD算法在这个例子上的实证结果，表明收敛非常快，事实上是线性的。我们得出结论，Baird反例通过一种具有收敛保证的算法解决了，该算法收敛到TD解决方案。

    Baird counterexample was proposed by Leemon Baird in 1995, first used to show that the Temporal Difference (TD(0)) algorithm diverges on this example. Since then, it is often used to test and compare off-policy learning algorithms. Gradient TD algorithms solved the divergence issue of TD on Baird counterexample. However, their convergence on this example is still very slow, and the nature of the slowness is not well understood, e.g., see (Sutton and Barto 2018).  This note is to understand in particular, why TDC is slow on this example, and provide debugging analysis to understand this behavior. Our debugging technique can be used to study the convergence behavior of two-time-scale stochastic approximation algorithms. We also provide empirical results of the recent Impression GTD algorithm on this example, showing the convergence is very fast, in fact, in a linear rate. We conclude that Baird counterexample is solved, by an algorithm with convergence guarantee to the TD solution in gen
    
[^29]: ChatGPT-HealthPrompt. 利用ChatGPT在基于提示的医疗决策支持中发挥可解释人工智能的力量

    ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT. (arXiv:2308.09731v1 [cs.AI])

    [http://arxiv.org/abs/2308.09731](http://arxiv.org/abs/2308.09731)

    本研究介绍了一种创新的方法，利用ChatGPT在临床决策中应用大型语言模型，通过策略性设计上下文提示，并以领域知识为基础进行高质量的二元分类任务。通过将机器学习模型视为医疗专家，提取关键见解并辅助决策过程，这一领域知识和人工智能的结合在创建更具洞察力的诊断工具方面具有重要潜力。此外，研究还探索了基于ChatGPT的零样本和少样本提示学习的动态，并验证了ChatGPT在医疗决策支持中的优势。

    

    本研究提出了一种创新的方法，将大型语言模型（LLMs）应用于临床决策，重点关注OpenAI的ChatGPT。我们的方法引入了上下文提示的使用，策略性地设计包括任务描述、特征描述，并且关键地整合领域知识，以便在数据稀缺的情况下进行高质量的二元分类任务。我们工作的创新之处在于利用从高性能可解释机器学习模型获得的领域知识，并将其无缝地融入到提示设计中。通过将这些机器学习模型视为医疗专家，我们提取了关于特征重要性的关键见解，以帮助决策过程。领域知识和人工智能的相互作用在创建更具洞察力的诊断工具方面具有重要的潜力。此外，我们的研究探讨了基于LLMs的零样本和少样本提示学习的动态。通过比较OpenAI的ChatGPT与传统的supervised学习方法的表现，我们证明了ChatGPT在医疗决策支持方面的优势。

    This study presents an innovative approach to the application of large language models (LLMs) in clinical decision-making, focusing on OpenAI's ChatGPT. Our approach introduces the use of contextual prompts-strategically designed to include task description, feature description, and crucially, integration of domain knowledge-for high-quality binary classification tasks even in data-scarce scenarios. The novelty of our work lies in the utilization of domain knowledge, obtained from high-performing interpretable ML models, and its seamless incorporation into prompt design. By viewing these ML models as medical experts, we extract key insights on feature importance to aid in decision-making processes. This interplay of domain knowledge and AI holds significant promise in creating a more insightful diagnostic tool.  Additionally, our research explores the dynamics of zero-shot and few-shot prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT with traditional supe
    
[^30]: MindMap：知识图谱激发大型语言模型的思维图思考方法

    MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])

    [http://arxiv.org/abs/2308.09729](http://arxiv.org/abs/2308.09729)

    本论文通过使用知识图谱来激发大型语言模型，解决了整合新知识、产生幻觉和决策过程不透明等问题，并通过生成思维导图展示了模型的推理路径，实验证明这种方法可以取得显著的实证增益。

    

    通常，大型语言模型存在无法整合新知识、产生幻觉和决策过程不透明等限制。本文探讨了如何利用知识图谱（KG）来激发大型语言模型，以解决整合最新知识和引发模型思维路径的问题。具体来说，我们构建了一个提示管道，使大型语言模型能够理解KG输入并利用隐含知识和检索到的外部知识进行推理。此外，我们研究了引发大型语言模型执行推理和生成答案的思维导图。研究发现，生成的思维导图基于知识的本体论，展示了大型语言模型的推理路径，从而为生产环境中的推理提供了探索和评估的可能性。对三个问答数据集的实验证明，MindMap提示方法带来了显著的实证增益。

    LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question & answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
    
[^31]: 公平的不躁动多臂赌博机：受数字健康启发的通用框架

    Equitable Restless Multi-Armed Bandits: A General Framework Inspired By Digital Health. (arXiv:2308.09726v1 [cs.LG])

    [http://arxiv.org/abs/2308.09726](http://arxiv.org/abs/2308.09726)

    这项研究提出了不躁动多臂赌博机的公平目标，并开发了相应的算法，证明在数字健康等领域中可以提高数倍的公平性。

    

    不躁动多臂赌博机（RMABs）是一种在资源有限的连续环境中进行算法决策的流行框架。RMABs越来越被用于公共卫生、治疗安排、反偷猎等敏感决策，而本研究的动机正是数字健康。在这些高风险环境中，决策必须改善结果并避免不同群体之间的差距（例如，确保健康公平）。我们首次研究了RMABs的公平目标（ERMABs）。我们考虑了公平文献中两种公平性相关的目标，最小化最大化奖励和最大化纳什福利。我们为解决这两个目标开发了高效的算法——对于前者使用了水位填充算法，对于后者使用了理论上有动机的贪心算法来平衡不同群体大小。最后，我们通过三个模拟领域（包括一个新的数字健康模型）的实验证明，我们的方法在公平性方面可以比当前方法提高数倍。

    Restless multi-armed bandits (RMABs) are a popular framework for algorithmic decision making in sequential settings with limited resources. RMABs are increasingly being used for sensitive decisions such as in public health, treatment scheduling, anti-poaching, and -- the motivation for this work -digital health. For such high stakes settings, decisions must both improve outcomes and prevent disparities between groups (e.g., ensure health equity). We study equitable objectives for RMABs (ERMABs) for the first time. We consider two equity-aligned objectives from the fairness literature, minimax reward and max Nash welfare. We develop efficient algorithms for solving each -- a water filling algorithm for the former, and a greedy algorithm with theoretically motivated nuance to balance disparate group sizes for the latter. Finally, we demonstrate across three simulation domains, including a new digital health model, that our approaches can be multiple times more equitable than the curren
    
[^32]: MoCLIM: 用多组学对比学习和组学推理建模实现准确的癌症亚型划分

    MoCLIM: Towards Accurate Cancer Subtyping via Multi-Omics Contrastive Learning with Omics-Inference Modeling. (arXiv:2308.09725v1 [q-bio.GN])

    [http://arxiv.org/abs/2308.09725](http://arxiv.org/abs/2308.09725)

    本论文介绍了一种名为MoCLIM的多组学对比学习框架，能够在癌症亚型划分中利用多组学数据的潜力，显著提高了数据的拟合度和亚型划分性能。

    

    精准医学的目标是建立癌症亚型的生化机制与疾病之间的因果关系。基于组学的癌症亚型划分已经成为一种革命性的方法，因为不同级别的组学记录了癌症中多步骤过程的生化产物。本文旨在充分利用多组学数据的潜力来改善癌症亚型划分结果，因此开发了MoCLIM，一种表示学习框架。MoCLIM独立地从不同的组学模式中提取有信息量的特征。通过对不同组学模式之间的对比学习所得到的统一表示，在给定癌症情况下，我们能够将亚型很好地聚类到较低的潜空间中。这种对比可以被解释为在生物网络中观察到的组际推理的投影。在六个癌症数据集上的实验结果表明，我们的方法在较少的高维癌症数据拟合和亚型划分性能方面显著改善。

    Precision medicine fundamentally aims to establish causality between dysregulated biochemical mechanisms and cancer subtypes. Omics-based cancer subtyping has emerged as a revolutionary approach, as different level of omics records the biochemical products of multistep processes in cancers. This paper focuses on fully exploiting the potential of multi-omics data to improve cancer subtyping outcomes, and hence developed MoCLIM, a representation learning framework. MoCLIM independently extracts the informative features from distinct omics modalities. Using a unified representation informed by contrastive learning of different omics modalities, we can well-cluster the subtypes, given cancer, into a lower latent space. This contrast can be interpreted as a projection of inter-omics inference observed in biological networks. Experimental results on six cancer datasets demonstrate that our approach significantly improves data fit and subtyping performance in fewer high-dimensional cancer ins
    
[^33]: 知识启发的子领域适应用于交叉领域知识传递

    Knowledge-inspired Subdomain Adaptation for Cross-Domain Knowledge Transfer. (arXiv:2308.09724v1 [cs.LG])

    [http://arxiv.org/abs/2308.09724](http://arxiv.org/abs/2308.09724)

    知识启发的子领域适应（KISA）框架在交叉领域知识传递中提供了细粒度的领域适应能力。

    

    大多数最先进的深度领域适应技术以全局方式对齐源域和目标域样本。也就是说，对齐后，每个源样本都期望与任何目标样本相似。然而，在实践中，全局对齐可能并不总是最优或必要的。为了实现这种细粒度的领域适应，我们提出了一种新颖的知识启发的子领域适应（KISA）框架。具体来说，（1）我们提供了KISA最小化共享预期损失的理论洞见，这是领域适应方法成功的前提。（2）我们提出了知识启发的子领域划分问题，这在细粒度的领域适应中起着重要作用。

    Most state-of-the-art deep domain adaptation techniques align source and target samples in a global fashion. That is, after alignment, each source sample is expected to become similar to any target sample. However, global alignment may not always be optimal or necessary in practice. For example, consider cross-domain fraud detection, where there are two types of transactions: credit and non-credit. Aligning credit and non-credit transactions separately may yield better performance than global alignment, as credit transactions are unlikely to exhibit patterns similar to non-credit transactions. To enable such fine-grained domain adaption, we propose a novel Knowledge-Inspired Subdomain Adaptation (KISA) framework. In particular, (1) We provide the theoretical insight that KISA minimizes the shared expected loss which is the premise for the success of domain adaptation methods. (2) We propose the knowledge-inspired subdomain division problem that plays a crucial role in fine-grained doma
    
[^34]: 关于大型语言模型的意想不到能力

    On the Unexpected Abilities of Large Language Models. (arXiv:2308.09720v1 [cs.AI])

    [http://arxiv.org/abs/2308.09720](http://arxiv.org/abs/2308.09720)

    大型语言模型展示了与其训练任务不直接相关的广泛能力，通过间接获取过程使其拥有综合能力的发展，这些能力在一定程度上可预测，并且与人类认知有关。

    

    大型语言模型能展示出与其训练任务（预测人类书写文本的下一个单词）不直接相关的广泛能力。本文讨论了这种间接获取过程的性质及其与其他已知间接过程的关系。文章主张这种间接获取的一个重要副作用是综合能力的发展。本文还讨论了大型语言模型所开发的能力在多大程度上是可预测的。最后，文章简要讨论了这些系统所获得的认知技能与人类认知之间的关系。

    Large language models are capable of displaying a wide range of abilities that are not directly connected with the task for which they are trained: predicting the next words of human-written texts. In this article, I discuss the nature of this indirect acquisition process and its relation to other known indirect processes. I argue that an important side effect of such indirect acquisition is the development of integrated abilities. I discuss the extent to which the abilities developed by large language models are predictable. Finally, I briefly discuss the relation between the cognitive skills acquired by these systems and human cognition.
    
[^35]: CIRO: COVID-19感染风险本体论

    CIRO: COVID-19 infection risk ontology. (arXiv:2308.09719v1 [cs.AI])

    [http://arxiv.org/abs/2308.09719](http://arxiv.org/abs/2308.09719)

    本研究通过COVID-19感染风险本体（CIRO）自动评估每个人的感染风险，以减轻公共卫生官员的负担。

    

    公共卫生机构进行接触追踪以识别与感染病例的密切接触者。然而，在由2019年冠状病毒病（COVID-19）引起的大流行期间，高病例量国家没有采用这种操作。与此同时，日本政府进行了这项操作，以减少公共卫生官员的艰辛手工劳动，从而有助于控制感染。为减轻官员的负担，本研究尝试通过一种称为COVID-19感染风险本体（CIRO）的本体来自动评估每个人的感染风险。这个本体使用资源描述框架（RDF）和SPARQL（SPARQL协议和RDF查询语言）查询表达了日本政府对COVID-19的感染风险，以便自动评估个体的感染风险。通过评估，我们证明了构建的知识图可以推断出由t

    Public health authorities perform contact tracing for highly contagious agents to identify close contacts with the infected cases. However, during the pandemic caused by coronavirus disease 2019 (COVID-19), this operation was not employed in countries with high patient volumes. Meanwhile, the Japanese government conducted this operation, thereby contributing to the control of infections, at the cost of arduous manual labor by public health officials. To ease the burden of the officials, this study attempted to automate the assessment of each person's infection risk through an ontology, called COVID-19 Infection Risk Ontology (CIRO). This ontology expresses infection risks of COVID-19 formulated by the Japanese government, toward automated assessment of infection risks of individuals, using Resource Description Framework (RDF) and SPARQL (SPARQL Protocol and RDF Query Language) queries. For evaluation, we demonstrated that the knowledge graph built could infer the risks, formulated by t
    
[^36]: 想法图：用大型语言模型解决复杂问题

    Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.09687](http://arxiv.org/abs/2308.09687)

    想法图（GoT）是一种新的框架，它超越了现有的提示范式，通过将大型语言模型（LLM）的信息建模为任意图形，将LLM想法组合成具有协同效应的结果，提炼整个思维网络的本质，或者使用反馈环路增强思维。GoT在不同任务上展示出优势，并可以通过新的想法转换进行扩展，使LLM的推理更接近人类思维。

    

    我们介绍了一种名为想法图（Graph of Thoughts，GoT）的框架，它在大型语言模型（LLM）的提示能力上超越了Chain-of-Thought或Tree of Thoughts（ToT）等范式。GoT的关键思想和主要优势在于能够将LLM生成的信息建模为任意图形，其中信息单元（"LLM想法"）是顶点，边表示这些顶点之间的依赖关系。这种方法使得将任意LLM想法组合成具有协同效应的结果、提炼整个思维网络的本质或者使用反馈环路增强思维成为可能。我们证明GoT在不同任务上比最先进的方法有优势，例如在排序任务上质量提高了62%，同时成本降低了超过31%。我们确保GoT能够通过新的想法转换进行扩展，从而可以用于开创新的提示方案。这项工作使得LLM的推理更接近人类思维。

    We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinki
    
[^37]: 树形混合思维: 将快速和慢速思考结合用于多跳视觉推理

    Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop Visual Reasoning. (arXiv:2308.09658v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.09658](http://arxiv.org/abs/2308.09658)

    该论文提出了一种树形混合思维的方法，通过将快速和慢速思考结合，用于解决多跳视觉推理任务中准确性和效率之间的权衡问题。

    

    使用大型语言模型（LLMs）生成类似代码的计划，用于复杂推理任务（如视觉推理）正在成为一种有前途的趋势。这种被称为LLM-based planning的范式在问题解决的灵活性和可解释性方面具有优势。然而，当前的研究大多限于简单问题的基本情景，这些问题可以直接回答出来，只需要几个推理步骤。对于更具挑战性的多跳视觉推理任务的计划制订还未充分探索。在多跳推理的情况下，准确性和计划搜索复杂性之间的权衡变得显著。目前的算法要么通过采用快速一次性生成来解决效率问题，要么采用复杂的迭代生成方法来提高准确性。但两种方法都无法平衡效率和性能的需求。受到人脑中的双系统认知（快速思考和慢速思考）的启发，我们提出了一种树形混合思维的方法。

    There emerges a promising trend of using large language models (LLMs) to generate code-like plans for complex inference tasks such as visual reasoning. This paradigm, known as LLM-based planning, provides flexibility in problem solving and endows better interpretability. However, current research is mostly limited to basic scenarios of simple questions that can be straightforward answered in a few inference steps. Planning for the more challenging multi-hop visual reasoning tasks remains under-explored. Specifically, under multi-hop reasoning situations, the trade-off between accuracy and the complexity of plan-searching becomes prominent. The prevailing algorithms either address the efficiency issue by employing the fast one-stop generation or adopt a complex iterative generation method to improve accuracy. Both fail to balance the need for efficiency and performance. Drawing inspiration from the dual system of cognition in the human brain, the fast and the slow think processes, we pr
    
[^38]: 毒箭蛙：一种在没有训练数据的情况下，具有低中毒率和高攻击成功率的无标签攻击

    Poison Dart Frog: A Clean-Label Attack with Low Poisoning Rate and High Attack Success Rate in the Absence of Training Data. (arXiv:2308.09487v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2308.09487](http://arxiv.org/abs/2308.09487)

    毒箭蛙是一种无标签攻击方法，不需要训练数据，只需要目标类别的知识。它具有低中毒率和高攻击成功率。

    

    为了成功发动后门攻击，注入的数据需要被正确标记，否则，即使是基本的数据过滤器也能轻易检测出来。因此，引入了无标签攻击的概念，这种攻击更加危险，因为它不需要改变注入数据的标签。据我们所知，现有的无标签后门攻击在很大程度上依赖于对整个训练集或其中一部分的理解。然而，在实践中，攻击者很难拥有这些理解，因为训练数据集通常来自多个独立的来源。与所有当前的无标签攻击不同，我们提出了一种新颖的无标签方法，名为“毒箭蛙”。毒箭蛙不需要访问任何训练数据，只需要了解攻击目标类别，比如“蛙”。在CIFAR10、Tiny-ImageNet和TSRD上，仅需要分别占训练集大小的0.1%、0.025%和0.4%的中毒率，毒箭蛙就能取得成功。

    To successfully launch backdoor attacks, injected data needs to be correctly labeled; otherwise, they can be easily detected by even basic data filters. Hence, the concept of clean-label attacks was introduced, which is more dangerous as it doesn't require changing the labels of injected data. To the best of our knowledge, the existing clean-label backdoor attacks largely relies on an understanding of the entire training set or a portion of it. However, in practice, it is very difficult for attackers to have it because of training datasets often collected from multiple independent sources. Unlike all current clean-label attacks, we propose a novel clean label method called 'Poison Dart Frog'. Poison Dart Frog does not require access to any training data; it only necessitates knowledge of the target class for the attack, such as 'frog'. On CIFAR10, Tiny-ImageNet, and TSRD, with a mere 0.1\%, 0.025\%, and 0.4\% poisoning rate of the training set size, respectively, Poison Dart Frog achie
    
[^39]: V2A-Mapper：通过连接基础模型实现轻量级的视听生成解决方案

    V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models. (arXiv:2308.09300v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.09300](http://arxiv.org/abs/2308.09300)

    本研究提出了一种轻量级的解决方案，通过连接基础模型，特别是CLIP、CLAP和AudioLDM模型，解决了视听生成的问题。

    

    在人工智能研究中，基于一组基础模型（FMs）构建人工智能系统正在成为一种新的范式。这些模型通过大量数据学习得到的代表性和生成能力可以轻松地适应和迁移至各种下游任务，而无需额外的从头训练。然而，在涉及音频模态的跨模态生成中，利用FMs仍然是一个未充分研究的领域。另一方面，从视觉输入中自动生成语义相关的声音是跨模态生成研究中的一个重要问题。为了解决这个视听生成（V2A）问题，现有方法倾向于使用规模适中的数据集从头设计和构建复杂的系统。在本文中，我们提出了一个轻量级的解决方案，通过利用基础模型，具体来说是CLIP、CLAP和AudioLDM。我们首先研究了视觉CLIP模型和听觉CLAP模型的潜在空间之间的领域差距。然后我们提出了...

    Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose 
    
[^40]: 提升大型语言模型的推理能力：一种基于图形验证的方法

    Enhancing Reasoning Capabilities of Large Language Models: A Graph-Based Verification Approach. (arXiv:2308.09267v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.09267](http://arxiv.org/abs/2308.09267)

    本研究引入了一种基于图形验证的方法，以进一步提高大型语言模型（LLM）的推理能力，通过将LLM生成的多个解决方案表示为推理图，从而增强推理能力。

    

    大型语言模型（LLM）展现出了令人印象深刻的推理能力，在复杂的数学问题等推理任务中，特别是在受特定设计的提示指导下。这些模型通常使用思维链的方法来解决任务，这不仅增强了它们的推理能力，还提供了宝贵的洞察力，揭示了它们的问题求解过程。然而，提升LLM的推理能力还有很大的改进空间。一些研究表明，将LLM输出验证器集成到模型中可以提高推理准确性，而无需额外进行模型训练。在本文中，我们遵循这些研究，引入了一种新颖的基于图形的方法，进一步增强LLM的推理能力。我们设想一个推理任务的多个解决方案，由LLM生成，可以由推理图表示，因为不同推理路径的中间步骤之间存在逻辑连接。因此，我们提出了推理图方法。

    Large Language Models (LLMs) have showcased impressive reasoning capabilities, particularly when guided by specifically designed prompts in complex reasoning tasks such as math word problems. These models typically solve tasks using a chain-of-thought approach, which not only bolsters their reasoning abilities but also provides valuable insights into their problem-solving process. However, there is still significant room for enhancing the reasoning abilities of LLMs. Some studies suggest that the integration of an LLM output verifier can boost reasoning accuracy without necessitating additional model training. In this paper, we follow these studies and introduce a novel graph-based method to further augment the reasoning capabilities of LLMs. We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths. Therefore, we propose the Reasoning Graph 
    
[^41]: 对称加权一阶模型抽样的提升算法

    Lifted Algorithms for Symmetric Weighted First-Order Model Sampling. (arXiv:2308.08828v1 [cs.AI])

    [http://arxiv.org/abs/2308.08828](http://arxiv.org/abs/2308.08828)

    本文研究了对称加权一阶模型抽样的提升算法，通过设计一种扩展了计数量词的一阶逻辑的“域可提升抽样”的内容，证明了加权模型抽样问题也存在可处理的情况。

    

    加权模型计数（WMC）是计算命题公式的所有满足解（模型）的权重之和的任务。类似地，加权模型抽样（WMS）旨在以概率与它们相应的权重成比例地随机生成模型。WMC和WMS都难以精确求解，属于\#P-hard复杂度类别。然而，已知如果命题公式可以以紧凑的方式表示并用一阶逻辑表达，有时计数问题可能是可以处理的。在这种情况下，模型计数问题可以在域大小多项式时间内解决，并被称为“域可提升”。然后，就会出现以下问题：加权模型抽样是否也是如此？本文回答了这个问题，并肯定地回答了它。具体而言，我们在本文中通过设计一种扩展了计数量词的一阶逻辑的“域可提升抽样”的内容，证明了这个问题。

    Weighted model counting (WMC) is the task of computing the weighted sum of all satisfying assignments (i.e., models) of a propositional formula. Similarly, weighted model sampling (WMS) aims to randomly generate models with probability proportional to their respective weights. Both WMC and WMS are hard to solve exactly, falling under the \#P-hard complexity class. However, it is known that the counting problem may sometimes be tractable, if the propositional formula can be compactly represented and expressed in first-order logic. In such cases, model counting problems can be solved in time polynomial in the domain size, and are known as \textit{domain-liftable}. The following question then arises: Is it also the case for weighted model sampling? This paper addresses this question and answers it affirmatively. Specifically, we prove the \textit{domain-liftability under sampling} for the two-variables fragment of first-order logic with counting quantifiers in this paper, by devising an e
    
[^42]: 探索上下文学习的演示集成

    Exploring Demonstration Ensembling for In-context Learning. (arXiv:2308.08780v1 [cs.CL])

    [http://arxiv.org/abs/2308.08780](http://arxiv.org/abs/2308.08780)

    本研究探索了上下文学习的演示集成方法，用于提高语言模型在给定任务的输入输出对中的预测性能。通过将演示分成子集并组合各子集的输出概率，我们得到了最终的预测结果。

    

    上下文学习通过向语言模型展示输入-输出对的示例来进行操作，即演示。上下文学习的标准方法是将演示与测试输入连接起来提示给语言模型。然而，这种方法存在一些问题。首先，连接方法几乎无法控制每个演示对模型预测的贡献。当一些演示与测试示例无关时，这可能不是最优的。其次，由于某些变换器模型对输入长度有限制，将许多示例放入上下文中可能是不可行的，特别是在处理长输入任务时。在本研究中，我们探索了演示集成（DENSE）作为简单连接的替代方法。模型使用演示的子集（即bucket）来预测输出，然后将每个子集得到的输出概率组合起来生成最终预测结果。我们使用GPT-j研究了不同的集成方法，并进行了实验。

    In-context learning (ICL) operates by showing language models (LMs) examples of input-output pairs for a given task, i.e., demonstrations. The standard approach for ICL is to prompt the LM with concatenated demonstrations followed by the test input. This approach suffers from some issues. First, concatenation offers almost no control over the contribution of each demo to the model prediction. This can be sub-optimal when some demonstrations are irrelevant to the test example. Second, due to the input length limit of some transformer models, it might be infeasible to fit many examples into the context, especially when dealing with long-input tasks. In this work, we explore Demonstration Ensembling (DENSE) as an alternative to simple concatenation. \model predicts outputs using subsets (i.e., buckets) of the demonstrations and then combines the output probabilities resulting from each subset to produce the final prediction. We study different ensembling methods using GPT-j and experiment
    
[^43]: 人工智能中的意识：来自意识科学的洞见

    Consciousness in Artificial Intelligence: Insights from the Science of Consciousness. (arXiv:2308.08708v1 [cs.AI])

    [http://arxiv.org/abs/2308.08708](http://arxiv.org/abs/2308.08708)

    本论文提出了一种严谨的方法，通过对当前的人工智能系统进行详细评估来探讨人工智能的意识问题。研究中对几种科学意识理论进行概述，并通过计算方法推导出意识的“指示性特征”。分析结果表明目前的人工智能系统尚不具备意识，但建立具有意识的人工智能系统并无明显的障碍。

    

    当前或近期的人工智能系统是否能具有意识成为科学界关注的话题，也引起了公众的担忧。本报告提出并举例了一种严谨且经验基础的人工智能意识方法：根据我们目前最可信的神经科学理论对现有的人工智能系统进行详细评估。我们概述了几种广泛认可的科学意识理论，包括循环处理理论、全局工作空间理论、高阶理论、预测处理理论和注意模式理论。从这些理论中，我们推导出一些意识的“指示性特征”，并通过计算方法来评估人工智能系统是否具备这些特征。我们利用这些指示性特征来评估了几个近期的人工智能系统，并讨论了未来系统如何实现这些特征。我们的分析表明，目前没有现有的人工智能系统具有意识，但同时也显示出没有明显的建立具有意识的人工智能系统的障碍。

    Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.
    
[^44]: TeCH: 文本引导的逼真服饰人物重建

    TeCH: Text-guided Reconstruction of Lifelike Clothed Humans. (arXiv:2308.08545v1 [cs.CV])

    [http://arxiv.org/abs/2308.08545](http://arxiv.org/abs/2308.08545)

    该论文提出了TeCH模型，通过文本引导的方法来重建逼真的服饰人物。模型可以准确恢复“未曾看见的区域”并添加高级细节，采用了基于DMTet的混合3D表示以达到更低的成本。

    

    尽管最近在从单张图像中重建着装人物方面取得了研究进展，但准确恢复“未曾看见的区域”并添加高级细节仍然是一个未解决的挑战，缺乏关注。现有的方法往往会生成过于平滑的背面表面和模糊的纹理。本文提出了TeCH模型，通过利用1）通过服装解析模型和视觉问答（VQA）自动生成的描述性文本提示（例如，服饰、颜色、发型）；2）经过个性化微调的文本到图像扩散模型（T2I）来学习“无法描述”的外观，从而对3D人类形象进行重建。为了以更低的成本表示高分辨率的3D服饰人物，我们提出了基于DMTet的混合3D表示，它由明确的身体形状网格和一个……（摘要较长，省略部分内容）

    Despite recent research advancements in reconstructing clothed humans from a single image, accurately restoring the "unseen regions" with high-level details remains an unsolved challenge that lacks attention. Existing methods often generate overly smooth back-side surfaces with a blurry texture. But how to effectively capture all visual attributes of an individual from a single image, which are sufficient to reconstruct unseen areas (e.g., the back view)? Motivated by the power of foundation models, TeCH reconstructs the 3D human by leveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles) which are automatically generated via a garment parsing model and Visual Question Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusion model (T2I) which learns the "indescribable" appearance. To represent high-resolution 3D clothed humans at an affordable cost, we propose a hybrid 3D representation based on DMTet, which consists of an explicit body shape grid and an
    
[^45]: 中国AIGC的现状及未来展望

    AIGC In China: Current Developments And Future Outlook. (arXiv:2308.08451v1 [cs.AI])

    [http://arxiv.org/abs/2308.08451](http://arxiv.org/abs/2308.08451)

    本文分析了中国在人工智能生成内容（AIGC）领域的现状，讨论了AIGC的基础技术、市场状况和发展轨迹，并重点强调了AIGC的生态建设。预测了行业未来的挑战和发展方向。

    

    对于人工智能生成内容（AIGC）的日益关注，已经在日常生活、工业制造和学术界等各个方面产生了深远的影响。鉴于AIGC发展的全球趋势和竞争力，本研究旨在分析中国在该领域的现状。首先，研究概述了AIGC的基础技术和当前应用。随后，通过关键词搜索识别相关学术论文，深入研究了中国的AIGC市场状况、政策环境和发展轨迹。此外，本文全面审视了AIGC产品及其相应生态系统，并强调了AIGC的生态建设。最后，本文讨论了AIGC行业面临的挑战和风险，并根据AIGC的竞争性洞察展望了行业的未来。

    The increasing attention given to AI Generated Content (AIGC) has brought a profound impact on various aspects of daily life, industrial manufacturing, and the academic sector. Recognizing the global trends and competitiveness in AIGC development, this study aims to analyze China's current status in the field. The investigation begins with an overview of the foundational technologies and current applications of AIGC. Subsequently, the study delves into the market status, policy landscape, and development trajectory of AIGC in China, utilizing keyword searches to identify relevant scholarly papers. Furthermore, the paper provides a comprehensive examination of AIGC products and their corresponding ecosystem, emphasizing the ecological construction of AIGC. Finally, this paper discusses the challenges and risks faced by the AIGC industry while presenting a forward-looking perspective on the industry's future based on competitive insights in AIGC.
    
[^46]: OmniZoomer: 学习在高分辨率球面上移动和缩放

    OmniZoomer: Learning to Move and Zoom in on Sphere at High-Resolution. (arXiv:2308.08114v1 [cs.CV])

    [http://arxiv.org/abs/2308.08114](http://arxiv.org/abs/2308.08114)

    本文提出了OmniZoomer，一种通过深度学习将Möbius变换整合到网络中，用于在全向图像上进行移动和缩放的方法。通过学习不同条件下的变换特征图，网络能够处理增加的边缘曲率并减轻模糊效果。此外，为了解决混叠问题，作者还提出了两个关键组成部分。

    

    全向图像（ODI）变得越来越受欢迎，因为它们的大视野可以让观众在虚拟现实等沉浸式环境中自由选择视角。通常使用Möbius变换在ODI上提供移动和缩放的机会，但将其应用在图像级别上通常会导致模糊效果和混叠问题。在本文中，我们提出了一种新颖的基于深度学习的方法，称为OmniZoomer，将Möbius变换整合到网络中，用于在ODI上进行移动和缩放。通过学习在不同条件下的各种变换特征图，增强了网络处理增加的边缘曲率，从而减轻了模糊效果。此外，为了解决混叠问题，我们提出了两个关键部分。首先，为了弥补描述曲线所需像素的不足，我们增强了高分辨率（HR）空间中的特征图，并计算...

    Omnidirectional images (ODIs) have become increasingly popular, as their large field-of-view (FoV) can offer viewers the chance to freely choose the view directions in immersive environments such as virtual reality. The M\"obius transformation is typically employed to further provide the opportunity for movement and zoom on ODIs, but applying it to the image level often results in blurry effect and aliasing problem. In this paper, we propose a novel deep learning-based approach, called \textbf{OmniZoomer}, to incorporate the M\"obius transformation into the network for movement and zoom on ODIs. By learning various transformed feature maps under different conditions, the network is enhanced to handle the increasing edge curvatures, which alleviates the blurry effect. Moreover, to address the aliasing problem, we propose two key components. Firstly, to compensate for the lack of pixels for describing curves, we enhance the feature maps in the high-resolution (HR) space and calculate the
    
[^47]: 通过在线文本增强和上下文记忆的方式进行故事可视化

    Story Visualization by Online Text Augmentation with Context Memory. (arXiv:2308.07575v1 [cs.CV])

    [http://arxiv.org/abs/2308.07575](http://arxiv.org/abs/2308.07575)

    本论文提出了一种通过在线文本增强和上下文记忆来进行故事可视化的方法。通过使用新颖的记忆架构和多个伪描述作为训练过程的补充监督，该方法在两个故事可视化基准测试中取得了显著优于现有方法的结果。

    

    故事可视化是一个具有挑战性的文本到图像生成任务，难点在于不仅需要从文本描述中呈现视觉细节，还需要对跨多个句子的长期上下文进行编码。以往的工作主要关注为每个句子生成语义相关的图像，但在给定段落中编码上下文以生成具有上下文说服力的图像（例如，正确的角色或适当的场景背景）仍然是一个挑战。为此，我们提出了一种新颖的记忆架构，用于双向Transformer，并通过在线文本增强生成多个伪描述作为训练过程中的补充监督，以更好地适应推理中的语言变化。在两个流行的故事可视化基准测试中进行了大量实验证明，即Pororo-SV和Flintstones-SV，所提出的方法在包括FID、字符...

    Story visualization (SV) is a challenging text-to-image generation task for the difficulty of not only rendering visual details from the text descriptions but also encoding a long-term context across multiple sentences. While prior efforts mostly focus on generating a semantically relevant image for each sentence, encoding a context spread across the given paragraph to generate contextually convincing images (e.g., with a correct character or with a proper background of the scene) remains a challenge. To this end, we propose a novel memory architecture for the Bi-directional Transformers with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training, for better generalization to the language variation at inference. In extensive experiments on the two popular SV benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms the state of the arts in various evaluation metrics including FID, char
    
[^48]: 具有环境感知记忆的上下文感知规划用于指导行为智能体

    Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents. (arXiv:2308.07241v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2308.07241](http://arxiv.org/abs/2308.07241)

    这项研究提出了一种称为CPEM的系统，它利用上下文信息和环境感知记忆来改进行为智能体的感知能力，从而提高视觉导航和物体交互的效果。

    

    完成家务任务（例如“拿一杯水”）需要通过保持对空间对象的空间布局和先前行动的结果的知识来进行逐步的规划。然而，当前的行为智能体在感知模型方面经常出错，因为缺乏这种知识，而依赖于不完美的学习的模仿智能体或者没有关于先前行动对环境变化的知识的算法规划器。为了解决这个问题，我们提出了CPEM（上下文感知规划器和环境感知记忆），将先前行动的上下文信息与环境中物体的空间布局和状态（例如物体是否被移动）结合到感知模型中，以改进视觉导航和物体交互。我们观察到，CPEM在各种度量指标上实现了最先进的任务成功性能。

    Accomplishing household tasks such as 'bringing a cup of water' requires planning step-by-step actions by maintaining knowledge about the spatial arrangement of objects and the consequences of previous actions. Perception models of the current embodied AI agents, however, often make mistakes due to a lack of such knowledge but rely on imperfect learning of imitating agents or an algorithmic planner without knowledge about the changed environment by the previous actions. To address the issue, we propose CPEM (Context-aware Planner and Environment-aware Memory) to incorporate the contextual information of previous actions for planning and maintaining spatial arrangement of objects with their states (e.g., if an object has been moved or not) in an environment to the perception model for improving both visual navigation and object interaction. We observe that CPEM achieves state-of-the-art task success performance in various metrics using a challenging interactive instruction following ben
    
[^49]: 自然语言是图表所需要的全部内容

    Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.07134](http://arxiv.org/abs/2308.07134)

    本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。

    

    大规模预训练语言模型的出现，如ChatGPT，已经在人工智能的各个研究领域中引起了革命。基于Transformer的大型语言模型（LLMs）逐渐取代了CNN和RNN，将计算机视觉和自然语言处理领域统一起来。与相对独立存在的数据（如图像、视频或文本）相比，图表是一种包含丰富结构和关系信息的数据类型。同时，作为最具表现力的媒介之一，自然语言在描述复杂结构方面表现出色。然而，将图表学习问题纳入生成式语言建模框架的现有工作仍然非常有限。随着大型语言模型的重要性不断增长，探索LLMs是否也可以替代GNNs成为图表的基础模型变得至关重要。在本文中，我们提出了InstructGLM（结构化语言模型）算法，系统地设计高度可扩展的模型来处理图表学习问题。

    The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
    
[^50]: 泛化少样本语义分割中的典型核学习与开放集前景感知

    Prototypical Kernel Learning and Open-set Foreground Perception for Generalized Few-shot Semantic Segmentation. (arXiv:2308.04952v1 [cs.CV])

    [http://arxiv.org/abs/2308.04952](http://arxiv.org/abs/2308.04952)

    本研究通过典型核学习和开放集前景感知，解决泛化少样本语义分割中的表示分割和嵌入偏见问题，并且在分割过程中使用了可学习的核以及典型学习和前景上下文感知模块来提高性能。

    

    泛化少样本语义分割（GFSS）将少样本语义分割（FSS）扩展到评估过程中同时分割未见过的类别和已见过的类别。先前的研究利用额外的分支或典型聚合来消除FSS的约束设置。然而，表示分割和嵌入偏见，严重影响GFSS的性能，尚未综合考虑。我们通过联合典型核学习和开放集前景感知来解决上述问题。具体而言，我们提出了一组可学习的核来对每个类别进行分割。然后，我们将典型学习与基类核的更新相结合，这与少样本新类别的原型知识聚合相一致。此外，采用与条件偏差基于推理的前景上下文感知模块，用于执行与类别无关的分割。

    Generalized Few-shot Semantic Segmentation (GFSS) extends Few-shot Semantic Segmentation (FSS) to simultaneously segment unseen classes and seen classes during evaluation. Previous works leverage additional branch or prototypical aggregation to eliminate the constrained setting of FSS. However, representation division and embedding prejudice, which heavily results in poor performance of GFSS, have not been synthetical considered. We address the aforementioned problems by jointing the prototypical kernel learning and open-set foreground perception. Specifically, a group of learnable kernels is proposed to perform segmentation with each kernel in charge of a stuff class. Then, we explore to merge the prototypical learning to the update of base-class kernels, which is consistent with the prototype knowledge aggregation of few-shot novel classes. In addition, a foreground contextual perception module cooperating with conditional bias based inference is adopted to perform class-agnostic as 
    
[^51]: Temporal DINO:一种增强动作预测的自监督视频策略

    Temporal DINO: A Self-supervised Video Strategy to Enhance Action Prediction. (arXiv:2308.04589v1 [cs.CV])

    [http://arxiv.org/abs/2308.04589](http://arxiv.org/abs/2308.04589)

    Temporal DINO是一种自监督视频策略，通过引入一个学生模型和一个教师模型，使得学生模型能够通过观察过去帧来学习未来帧的上下文，以增强动作预测。这种方法在行动预测任务上在ROAD数据集上进行了评估。

    

    行动预测的新兴领域在各种计算机视觉应用中起着重要作用，如自动驾驶、活动分析和人机交互。尽管有重大进展，但由于视频数据中固有的高维度、复杂动态和不确定性，准确预测未来动作仍然是一个具有挑战性的问题。传统的监督方法需要大量的标记数据，而获取这些数据既昂贵又耗时。本文介绍了一种受DINO（无标签的自蒸馏）启发的增强动作预测的新型自监督视频策略，称为Temporal-DINO。该策略使用两个模型：一个“学生”处理过去的帧，一个“教师”同时处理过去和未来的帧，从而实现更广泛的时间上下文。在训练过程中，教师通过仅观察过去的帧来指导学生学习未来的上下文。该策略在ROAD数据集上进行了行动预测下游任务的评估。

    The emerging field of action prediction plays a vital role in various computer vision applications such as autonomous driving, activity analysis and human-computer interaction. Despite significant advancements, accurately predicting future actions remains a challenging problem due to high dimensionality, complex dynamics and uncertainties inherent in video data. Traditional supervised approaches require large amounts of labelled data, which is expensive and time-consuming to obtain. This paper introduces a novel self-supervised video strategy for enhancing action prediction inspired by DINO (self-distillation with no labels). The Temporal-DINO approach employs two models; a 'student' processing past frames; and a 'teacher' processing both past and future frames, enabling a broader temporal context. During training, the teacher guides the student to learn future context by only observing past frames. The strategy is evaluated on ROAD dataset for the action prediction downstream task usi
    
[^52]: 为什么我们尚未拥有AGI

    Why We Don't Have AGI Yet. (arXiv:2308.03598v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.03598](http://arxiv.org/abs/2308.03598)

    本论文探讨了为什么尚未实现人工通用智能（AGI），并指出了纯粹的统计方法和资金推广不足是制约AGI发展的原因之一，同时还分析了实现人类适应能力和自主学习所需的关键认知能力。

    

    2002年重新阐述了AI的原始愿景，称之为“人工通用智能”或AGI。这一愿景是构建能够像人类一样学习、推理和解决问题的“思考机器”计算机系统。这与几十年来几乎所有人在该领域实践的“狭义AI”方法形成鲜明对比。虽然有几个大规模的项目名义上在致力于AGI的研发（尤其是DeepMind），但在纯粹专注的AGI发展领域，资金和推广并不充足。这令人惊讶，因为真正的AGI可以为人类带来巨大的价值。除了在这个领域缺乏努力之外，还存在亏欠的理论和方法上的错误。我们强调纯粹的统计方法无法实现AGI，并确定了实现类似人类适应能力和自主学习的关键认知能力。最终，我们概述了社会技术发展方面的一些问题。

    The original vision of AI was re-articulated in 2002 via the term 'Artificial General Intelligence' or AGI. This vision is to build 'Thinking Machines' computer systems that can learn, reason, and solve problems similar to the way humans do. This is in stark contrast to the 'Narrow AI' approach practiced by almost everyone in the field over the many decades. While several large-scale efforts have nominally been working on AGI (most notably DeepMind), the field of pure focused AGI development has not been well funded or promoted. This is surprising given the fantastic value that true AGI can bestow on humanity. In addition to the dearth of effort in this field, there are also several theoretical and methodical missteps that are hampering progress. We highlight why purely statistical approaches are unlikely to lead to AGI, and identify several crucial cognitive abilities required to achieve human-like adaptability and autonomous learning. We conclude with a survey of socio-technical fa
    
[^53]: PaniniQA: 通过交互式问答提升患者教育

    PaniniQA: Enhancing Patient Education Through Interactive Question Answering. (arXiv:2308.03253v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03253](http://arxiv.org/abs/2308.03253)

    PaniniQA是一个以患者为中心的交互式问答系统，旨在帮助患者理解他们的出院指导，提供及时的反馈以纠正患者的误解，通过有效的互动提高患者对医疗指导的掌握程度。

    

    患者门户允许出院患者在电子健康记录中访问他们的个性化出院指导。然而，许多患者很难理解或记住他们的出院指导。在本论文中，我们提出了PaniniQA，一个以患者为中心的交互式问答系统，旨在帮助患者理解他们的出院指导。PaniniQA首先从患者的出院指导中识别重要的临床内容，然后提出患者特定的教育问题。此外，PaniniQA还配备了答案验证功能，可以及时反馈来纠正患者的误解。我们的全面自动和人工评估结果表明，PaniniQA能够通过有效的互动提高患者对医疗指导的掌握程度。

    Patient portal allows discharged patients to access their personalized discharge instructions in electronic health records (EHRs). However, many patients have difficulty understanding or memorizing their discharge instructions. In this paper, we present PaniniQA, a patient-centric interactive question answering system designed to help patients understand their discharge instructions. PaniniQA first identifies important clinical content from patients' discharge instructions and then formulates patient-specific educational questions. In addition, PaniniQA is also equipped with answer verification functionality to provide timely feedback to correct patients' misunderstandings. Our comprehensive automatic and human evaluation results demonstrate our PaniniQA is capable of improving patients' mastery of their medical instructions through effective interactions
    
[^54]: 无源域自适应的人体姿势估计

    Source-free Domain Adaptive Human Pose Estimation. (arXiv:2308.03202v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.03202](http://arxiv.org/abs/2308.03202)

    提出了无源域自适应的人体姿势估计任务，旨在解决在适应过程中无法访问源数据的跨域学习挑战。通过提出的新框架，源保护模块更有效地保留源信息并抵抗噪声。

    

    人体姿势估计广泛应用于运动分析、医疗保健和虚拟现实等领域。然而，标注实际场景的数据集的巨大开销对姿势估计构成了重要挑战。为了解决这个问题，一种方法是在合成数据集上训练姿势估计模型，然后在真实世界数据上进行域自适应(DA)。然而，现有的HPE的DA方法在适应过程中忽略了数据隐私和安全问题，因为使用了源数据和目标数据。为此，我们提出了一种新的任务，名为无源域自适应的HPE，旨在解决在适应过程中无法访问源数据的HPE的跨域学习挑战。我们进一步提出了一个由三个模型组成的新框架：源模型、中间模型和目标模型，从源数据和目标数据的角度探索该任务。源保护模块更有效地保留源信息并抵抗噪声。

    Human Pose Estimation (HPE) is widely used in various fields, including motion analysis, healthcare, and virtual reality. However, the great expenses of labeled real-world datasets present a significant challenge for HPE. To overcome this, one approach is to train HPE models on synthetic datasets and then perform domain adaptation (DA) on real-world data. Unfortunately, existing DA methods for HPE neglect data privacy and security by using both source and target data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process. We further propose a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives. The source-protect module preserves source information more effectively while resisting noise
    
[^55]: 通过混合量子-经典方法解决以物流为导向的装箱问题

    Solving Logistic-Oriented Bin Packing Problems Through a Hybrid Quantum-Classical Approach. (arXiv:2308.02787v1 [cs.AI])

    [http://arxiv.org/abs/2308.02787](http://arxiv.org/abs/2308.02787)

    这项研究通过混合量子-经典方法解决了物流中以实际为导向的装箱问题，并扩展了解决不同维度问题和具有不同要求的箱子的能力。

    

    装箱问题是一个具有广泛工业应用的经典问题。事实上，将物品高效地装箱是许多物流公司面临的最艰巨的挑战之一，也是降低存储成本或改善车辆空间分配的关键问题。本文采用了我们先前发表的量子-经典框架Q4RealBPP，并详细阐述了解决以实际为导向的装箱问题的方法。为此，本文重点关注以下特点：i）存在不同类型的箱子，ii）扩展框架以解决三维、二维和一维问题，iii）物品与箱子关联的要求，iv）交付优先级。本文对所有这些特点进行了测试，以及Q4RealBPP解决以实际为导向的装箱问题的能力。

    The Bin Packing Problem is a classic problem with wide industrial applicability. In fact, the efficient packing of items into bins is one of the toughest challenges in many logistic corporations and is a critical issue for reducing storage costs or improving vehicle space allocation. In this work, we resort to our previously published quantum-classical framework known as Q4RealBPP, and elaborate on the solving of real-world oriented instances of the Bin Packing Problem. With this purpose, this paper gravitates on the following characteristics: i) the existence of heterogeneous bins, ii) the extension of the framework to solve not only three-dimensional, but also one- and two-dimensional instances of the problem, iii) requirements for item-bin associations, and iv) delivery priorities. All these features have been tested in this paper, as well as the ability of Q4RealBPP to solve real-world oriented instances.
    
[^56]: NeRFs: 寻找最佳3D表示的研究

    NeRFs: The Search for the Best 3D Representation. (arXiv:2308.02751v1 [cs.CV])

    [http://arxiv.org/abs/2308.02751](http://arxiv.org/abs/2308.02751)

    NeRFs是视图合成和相关问题中寻找最佳3D表示的结果，该方法利用神经网络查询获取体积参数来描述连续体积场景。

    

    神经辐射场（NeRFs）已成为视图合成或基于图像渲染等问题的首选表示方法，也应用于计算机图形学和计算机视觉等多个领域。NeRFs通过查询神经网络获得视图相关辐射和体积密度等体积参数，将场景表示为连续的体积。该表示方法已广泛应用，每年有数千篇论文在其基础上扩展或相关研究，多位作者和网站提供概述和调研，并有众多工业应用和创业公司。本文简要回顾了NeRFs的表示方法，并描述了长达三十年的寻找最佳3D表示方法以及最终引出NeRFs论文的过程。

    Neural Radiance Fields or NeRFs have become the representation of choice for problems in view synthesis or image-based rendering, as well as in many other applications across computer graphics and vision, and beyond. At their core, NeRFs describe a new representation of 3D scenes or 3D geometry. Instead of meshes, disparity maps, multiplane images or even voxel grids, they represent the scene as a continuous volume, with volumetric parameters like view-dependent radiance and volume density obtained by querying a neural network. The NeRF representation has now been widely used, with thousands of papers extending or building on it every year, multiple authors and websites providing overviews and surveys, and numerous industrial applications and startup companies. In this article, we briefly review the NeRF representation, and describe the three decades-long quest to find the best 3D representation for view synthesis and related problems, culminating in the NeRF papers. We then describe n
    
[^57]: 教授较小的语言模型如何推广到未见过的组合问题

    Teaching Smaller Language Models To Generalise To Unseen Compositional Questions. (arXiv:2308.00946v1 [cs.CL])

    [http://arxiv.org/abs/2308.00946](http://arxiv.org/abs/2308.00946)

    我们研究了如何教授较小的语言模型来推广到未见过的组合问题，通过多任务监督预训练和密集检索系统，我们建立了强大的基准，并展示了解决多个评估数据集上的问题的能力。

    

    我们使一个较小的语言模型能够推广到回答具有挑战性的组合问题，这些问题在训练中没有出现。为此，我们提出了一种多任务监督预训练的组合方法，涵盖了最多93个任务，旨在培养多样的推理能力，并结合了一个密集的检索系统，旨在检索一组证据性的段落片段。在问答方面，最近的进展要么通过针对非常大的预训练语言模型的提示方法实现零或少样本学习，要么通过微调较小的模型，有时结合信息检索进行。我们关注较少探索的问题，即较小的模型在对于不存在足够信息来回答特定问题的语料库进行检索时，能否实现零样本推广。我们在这个设置中为多样的评估数据集（StrategyQA，CommonsenseQA，IIRC，DROP，Musique和ARC-DA）建立了强大的基准，并展示了...

    We equip a smaller Language Model to generalise to answering challenging compositional questions that have not been seen in training. To do so we propose a combination of multitask supervised pretraining on up to 93 tasks designed to instill diverse reasoning abilities, and a dense retrieval system that aims to retrieve a set of evidential paragraph fragments. Recent progress in question-answering has been achieved either through prompting methods against very large pretrained Language Models in zero or few-shot fashion, or by fine-tuning smaller models, sometimes in conjunction with information retrieval. We focus on the less explored question of the extent to which zero-shot generalisation can be enabled in smaller models with retrieval against a corpus within which sufficient information to answer a particular question may not exist. We establish strong baselines in this setting for diverse evaluation datasets (StrategyQA, CommonsenseQA, IIRC, DROP, Musique and ARC-DA), and show tha
    
[^58]: 使用Fine-Tuned的OpenAI LLM预测机器翻译输出中的完美质量段落：是否可以从历史数据中捕捉编辑距离模式？

    Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])

    [http://arxiv.org/abs/2308.00158](http://arxiv.org/abs/2308.00158)

    本研究探讨了使用Fine-Tuned的OpenAI LLM进行翻译质量估计的能力，实验证明可以通过Fine-Tuned的ChatGPT来预测机器翻译的质量，但仍有改进的空间。

    

    翻译质量估计（TQE）是将输出翻译部署到使用中之前的重要步骤。 TQE对于评估机器翻译（MT）和人工翻译（HT）的质量也是至关重要的，而不需要查看参考翻译。在这项工作中，我们检查了最先进的大型语言模型（LLMs）是否可以为TQE任务和它们的能力进行Fine-Tune。我们以ChatGPT为例，将TQE视为二元分类任务。使用英意和英德训练语料库，我们的实验结果显示，通过ChatGPT的API Fine-Tuned可以在预测翻译质量方面获得相对较高的得分，即是否需要编辑翻译，但肯定有改进准确性的空间。英意双语摘要可在论文中找到。

    Translation Quality Estimation (TQE) is an important step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. In this work, we examine if the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using English-Italian and English-German training corpus, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. if the translation needs to be edited, but there is definitely space to improve the accuracy. English-Italiano bilingual Abstract is available in the paper.
    
[^59]: 使用大型语言模型进行渗透测试：AI作为辅助

    Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v1 [cs.CL])

    [http://arxiv.org/abs/2308.00121](http://arxiv.org/abs/2308.00121)

    本文探讨了使用大型语言模型（如GPT3.5）作为AI助手来增强渗透测试人员的能力，实现了高级任务规划和低级漏洞寻找两种用例，取得了有前景的初步结果，并就提供该技术的伦理问题进行了讨论。

    

    软件安全测试领域，尤其是渗透测试是一项需要高水平专业知识的活动，并涉及许多手动测试和分析步骤。本文探讨了使用大型语言模型（如GPT3.5）来增强渗透测试人员的能力。我们研究了两种不同的用例：用于安全测试任务的高级任务规划和在易受攻击的虚拟机中进行低级漏洞寻找。对于后者，我们实现了一个闭环反馈，将由语言模型生成的低级操作与易受攻击的虚拟机（通过SSH连接）相连，并允许语言模型分析虚拟机状态以寻找漏洞，并提供具体的攻击向量。我们讨论了有前景的初步结果，详细介绍了改进的途径，并就提供该技术的伦理问题进行了讨论。

    The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providi
    
[^60]: TMR-RD: 用于半监督目标检测的基于训练的模型精化和表示分歧方法

    TMR-RD: Training-based Model Refinement and Representation Disagreement for Semi-Supervised Object Detection. (arXiv:2307.13755v1 [cs.CV])

    [http://arxiv.org/abs/2307.13755](http://arxiv.org/abs/2307.13755)

    在半监督目标检测中，本文提出了基于训练的模型精化(TMR)阶段和表示分歧(RD)策略，用来解决伪标签噪声和教师-学生模型的一致性问题。TMR阶段通过轻量级缩放操作优化模型权重，防止过度拟合或遗忘学到的模式；RD策略帮助保持模型的差异，鼓励学生模型探索互补的表示。

    

    半监督目标检测(SSOD)可以将有限的标记数据和大量的未标记数据结合起来，提高现有目标检测器的性能和泛化能力。尽管取得了许多进展，但是最近的SSOD方法仍然面临着伪标签噪声/误导、经典指数移动平均(EMA)策略和后期训练中教师-学生模型的一致性等挑战。本文提出了一种新颖的基于训练的模型精化(TMR)阶段和简单而有效的表示分歧(RD)策略，以解决经典EMA的局限性和一致性问题。教师-学生模型的TMR阶段优化了轻量级缩放操作，以精化模型的权重，并防止过度拟合或遗忘从未标记数据中学到的模式。同时，RD策略帮助保持这些模型的差异，鼓励学生模型探索互补的表示。此外，我们使用级连回归来生成... (摘要未完整提供)

    Semi-supervised object detection (SSOD) can incorporate limited labeled data and large amounts of unlabeled data to improve the performance and generalization of existing object detectors. Despite many advances, recent SSOD methods are still challenged by noisy/misleading pseudo-labels, classical exponential moving average (EMA) strategy, and the consensus of Teacher-Student models in the latter stages of training. This paper proposes a novel training-based model refinement (TMR) stage and a simple yet effective representation disagreement (RD) strategy to address the limitations of classical EMA and the consensus problem. The TMR stage of Teacher-Student models optimizes the lightweight scaling operation to refine the model's weights and prevent overfitting or forgetting learned patterns from unlabeled data. Meanwhile, the RD strategy helps keep these models diverged to encourage the student model to explore complementary representations. In addition, we use cascade regression to gene
    
[^61]: 用领域无关规划器和标准表示解决魔方的研究

    On Solving the Rubik's Cube with Domain-Independent Planners Using Standard Representations. (arXiv:2307.13552v1 [cs.AI])

    [http://arxiv.org/abs/2307.13552](http://arxiv.org/abs/2307.13552)

    本文将魔方的表示转换为PDDL语言，使其更具可访问性和易读性。实验结果表明，DeepCubeA可以解决所有不同复杂度的魔方问题，但只有18％是最优解。

    

    魔方是一个众所周知且计算上具有挑战性的谜题，已经激发了人工智能研究人员探索高效的替代表示和问题解决方法。本文首次将魔方的表示转换为流行的PDDL语言，使得领域对PDDL规划器、竞赛和知识工程工具更加可访问和易读。然后我们比较了现有方法的性能。我们发现，在一个可比较的实验中，DeepCubeA可以解决所有不同复杂度的问题，尽管只有18％是最优解。

    Rubik's Cube (RC) is a well-known and computationally challenging puzzle that has motivated AI researchers to explore efficient alternative representations and problem-solving methods. The ideal situation for planning here is that a problem be solved optimally and efficiently represented in a standard notation using a general-purpose solver and heuristics. The fastest solver today for RC is DeepCubeA with a custom representation, and another approach is with Scorpion planner with State-Action-Space+ (SAS+) representation. In this paper, we present the first RC representation in the popular PDDL language so that the domain becomes more accessible to PDDL planners, competitions, and knowledge engineering tools, and is more human-readable. We then bridge across existing approaches and compare performance. We find that in one comparable experiment, DeepCubeA solves all problems with varying complexities, albeit only 18\% are optimal plans. For the same problem set, Scorpion with SAS+ repre
    
[^62]: RLCD: 基于对比蒸馏的强化学习用于语言模型对齐

    RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment. (arXiv:2307.12950v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.12950](http://arxiv.org/abs/2307.12950)

    RLCD是一种用于语言模型对齐的强化学习方法，利用对比蒸馏训练偏好模型，可以使语言模型在不使用人类反馈的情况下遵循自然语言规则。在多个对齐任务和不同规模的模型上，RLCD优于其他基线方法。

    

    我们提出了一种称为Reinforcement Learning from Contrast Distillation (RLCD)的方法，用于无需使用人类反馈即可使语言模型遵循自然语言规则的对齐。RLCD使用模拟的偏好对进行训练，这些对包含了高质量和低质量的示例，其中使用对比的正负提示生成。然后，使用偏好模型通过强化学习来改进基础的无对齐语言模型。在实证上，RLCD在三个不同的对齐任务（无害性、有用性和故事大纲生成）以及7B和30B模型规模的偏好数据模拟上，都优于RLAIF (Bai等人，2022b)和上下文蒸馏 (Huang等人，2022) 的基准方法。

    We propose Reinforcement Learning from Contrast Distillation (RLCD), a method for aligning language models to follow natural language principles without using human feedback. RLCD trains a preference model using simulated preference pairs that contain both a high-quality and low-quality example, generated using contrasting positive and negative prompts. The preference model is then used to improve a base unaligned language model via reinforcement learning. Empirically, RLCD outperforms RLAIF (Bai et al., 2022b) and context distillation (Huang et al., 2022) baselines across three diverse alignment tasks--harmlessness, helpfulness, and story outline generation--and on both 7B and 30B model scales for preference data simulation.
    
[^63]: Hindsight-DICE：稳定信用分配用于深度强化学习

    Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning. (arXiv:2307.11897v1 [cs.LG])

    [http://arxiv.org/abs/2307.11897](http://arxiv.org/abs/2307.11897)

    本研究提出了Hindsight-DICE算法，利用重要抽样比率估计技术改善了深度强化学习中的信用分配问题。

    

    在顺序决策问题中，环境往往提供很少的评估反馈来指导强化学习代理。在极端情况下，行为的长时间轨迹仅以一个终止信号标记，导致观察到非平凡奖励和触发此类反馈的个体步骤之间存在显著的时间延迟。解决这种信用分配挑战是强化学习的重要特征之一，本研究利用现有的重要抽样比率估计技术来显著改善策略梯度方法中的信用分配处理。虽然使用所谓的事后策略为观察到的轨迹返回返回数据重新加权提供了一个有原则的机制，但是简单地应用重要抽样会导致不稳定或过度滞后的学习。

    Oftentimes, environments for sequential decision-making problems can be quite sparse in the provision of evaluative feedback to guide reinforcement-learning agents. In the extreme case, long trajectories of behavior are merely punctuated with a single terminal feedback signal, engendering a significant temporal delay between the observation of non-trivial reward and the individual steps of behavior culpable for eliciting such feedback. Coping with such a credit assignment challenge is one of the hallmark characteristics of reinforcement learning and, in this work, we capitalize on existing importance-sampling ratio estimation techniques for off-policy evaluation to drastically improve the handling of credit assignment with policy-gradient methods. While the use of so-called hindsight policies offers a principled mechanism for reweighting on-policy data by saliency to the observed trajectory return, naively applying importance sampling results in unstable or excessively lagged learning.
    
[^64]: Ethosight: 一种基于联合嵌入的系统，利用上下文标签关联度度量和基于推理的迭代学习进行细致感知

    Ethosight: A Joint-Embedding Based System for Nuanced Perception Using Contextual Label Affinity Metric and Reasoning Based Iterative Learning. (arXiv:2307.10577v1 [cs.CV])

    [http://arxiv.org/abs/2307.10577](http://arxiv.org/abs/2307.10577)

    Ethosight是一种零样本计算机视觉算法，通过联合嵌入、上下文标签关联度计算和基于推理的迭代学习，实现对细微行为和场景细节的准确感知，同时消除了对预先存在符号知识的需求。

    

    传统的计算机视觉模型通常需要大量的人工努力来进行数据获取和验证，特别是在检测细微的行为细节或事件时。在实际应用中，区分常规行为和潜在风险的困难，如区分常规购物和潜在扒窃，进一步复杂化了这一过程。我们提出了Ethosight，一种新颖的零样本计算机视觉算法。Ethosight消除了对预先存在的符号知识的需求，从用户需求和感兴趣的语义知识出发进行自主学习。通过使用局部标签关联度计算和基于推理的迭代学习循环，Ethosight推断场景细节并迭代地优化标签集。推理机制可以来自大型语言模型如GPT4、符号推理器如OpenNARS或混合系统。Ethosight还充分利用了预训练的多模态模型ImageBind的能力。

    Traditional computer vision models often require extensive manual effort for data acquisition and validation, particularly when detecting subtle behavioral nuances or events. The difficulty in distinguishing routine behaviors from potential risks in real-world applications, like differentiating routine shopping from potential shoplifting, further complicates the process.  We present Ethosight, a novel zero-shot computer vision algorithm. Ethosight eradicates the need for pre-existing symbolic knowledge, initiating from a clean slate based on user requirements and semantic knowledge of interest. Using localized label affinity calculations and a reasoning-guided iterative learning loop, Ethosight infers scene details and iteratively refines the label set. Reasoning mechanisms can be derived from large language models like GPT4, symbolic reasoners like OpenNARS, or hybrid systems.  Ethosight further capitalizes on the capabilities of a pre-trained multi-modal model, ImageBind, generating 
    
[^65]: 手写和打印文本分割：一个签名案例研究

    Handwritten and Printed Text Segmentation: A Signature Case Study. (arXiv:2307.07887v1 [cs.CV])

    [http://arxiv.org/abs/2307.07887](http://arxiv.org/abs/2307.07887)

    本研究旨在解决手写和打印文本分割的挑战，并提出了一种新的方法来完整地恢复不同类别的文本，特别是在重叠部分提高分割性能。同时，还引入了一个新的数据集SignaTR6K，用于支持该任务。

    

    在分析扫描文档时，手写文本可能覆盖打印文本。这在文档的光学字符识别（OCR）和数字化过程中造成困难，并且进而影响到下游的自然语言处理（NLP）任务。之前的研究要么仅关注手写文本的二分类，要么进行三类文档的分割，即手写、打印和背景像素的识别。这导致手写和打印重叠的像素只被分配到一个类别中，因此在另一个类别中不被考虑。因此，在这项研究中，我们开发了新的方法来解决手写和打印文本分割的挑战，目标是完整地恢复不同类别的文本，特别是提高重叠部分的分割性能。为了促进这项任务，我们介绍了一个新的数据集SignaTR6K，该数据集收集自真实的法律文件。

    While analyzing scanned documents, handwritten text can overlay printed text. This causes difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurts downstream NLP tasks. Prior research either focuses only on the binary classification of handwritten text, or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This results in the assignment of the handwritten and printed overlapping pixels to only one of the classes, and thus, they are not accounted for in the other class. Thus, in this research, we develop novel approaches for addressing the challenges of handwritten and printed text segmentation with the goal of recovering text in different classes in whole, especially improving the segmentation performance on the overlapping parts. As such, to facilitate with this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as
    
[^66]: 大型语言模型作为文化角度的叠加

    Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])

    [http://arxiv.org/abs/2307.07870](http://arxiv.org/abs/2307.07870)

    大型语言模型被认为是具有个性或一套价值观的，但实际上它可以看作是具有不同价值观和个性特征的角度的叠加。通过角度可控性的概念，我们研究了大型语言模型在不同角度下展示的价值观和个性特征的变化。实验结果表明，即使在没有明显提示的情况下，大型语言模型也会表达出不同的价值观。

    

    大型语言模型（LLMs）常常被错误地认为具有个性或一套价值观。我们认为LLMs可以看作是具有不同价值观和个性特征的角度叠加。LLMs表现出依赖于上下文的价值观和个性特征，这些特征基于产生的角度而改变（与人类相反，人类在不同情境下通常具有更一致的价值观和个性特征）。我们引入了“角度可控性”的概念，指的是模型采用不同具有不同价值观和个性特征的角度的能力。在我们的实验中，我们使用心理学问卷（PVQ、VSM、IPIP）来研究展示的价值观和个性特征如何基于不同角度而改变。通过定性实验，我们展示了当提示中（隐式或显式）暗示了某些价值观时，LLMs表达出不同的价值观，即使在没有明显暗示的情况下，LLMs也会表达出不同的价值观。

    Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrat
    
[^67]: SINC: 自主上下文学习用于视觉-语言任务

    SINC: Self-Supervised In-Context Learning for Vision-Language Tasks. (arXiv:2307.07742v1 [cs.CV])

    [http://arxiv.org/abs/2307.07742](http://arxiv.org/abs/2307.07742)

    提出了一种名为SINC的自主上下文学习框架，可以在不依赖于大型语言模型的情况下实现上下文学习，并避免了模板敏感性和幻觉等问题。

    

    大型预训练Transformers模型展示了在上下文学习中引人入胜的能力。这些模型可以在输入中呈现的演示中，迅速构建新的预测器，而无需梯度更新。最近的工作在视觉-语言领域中促进了这种能力，通过将视觉信息融入到已经能够进行上下文预测的大型语言模型中。然而，这些方法可能继承了语言领域的问题，如模板敏感性和产生幻觉。此外，这些语言模型的规模提高了计算需求，使得学习和操作这些模型资源密集。为此，我们提出了一个问题：“如何在不限制于大型语言模型的情况下，让通用模型能够进行上下文学习？”。为了回答这个问题，我们提出了一个简洁而通用的框架，自主上下文学习（SINC），它引入了一个元模型，以自我监督的提示为基础进行学习，这些提示包括量身定制的演示。

    Large Pre-trained Transformers exhibit an intriguing capacity for in-context learning. Without gradient updates, these models can rapidly construct new predictors from demonstrations presented in the inputs. Recent works promote this ability in the vision-language domain by incorporating visual information into large language models that can already make in-context predictions. However, these methods could inherit issues in the language domain, such as template sensitivity and hallucination. Also, the scale of these language models raises a significant demand for computations, making learning and operating these models resource-intensive. To this end, we raise a question: ``How can we enable in-context learning for general models without being constrained on large language models?". To answer it, we propose a succinct and general framework, Self-supervised IN-Context learning (SINC), that introduces a meta-model to learn on self-supervised prompts consisting of tailored demonstrations.
    
[^68]: Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery（用于机器人手术中视觉问答定位的共同关注门控视觉-语言嵌入）

    Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2307.05182v1 [cs.CV])

    [http://arxiv.org/abs/2307.05182](http://arxiv.org/abs/2307.05182)

    这项研究提出了一种用于机器人手术中视觉问答定位的共同关注门控视觉-语言嵌入方法，可以为医学生和初级外科医生提供学习和理解手术视频的帮助。

    

    医学生和初级外科医生在学习手术时通常依赖于高级外科医生和专家回答他们的问题。然而，专家们经常忙于临床和学术工作，没有多少时间提供指导。与此同时，现有的基于深度学习的外科视觉问答系统只能提供简单答案，而没有答案的位置信息。此外，在这类任务中，视觉语言嵌入仍然是一个较少探索的领域。因此，一种外科视觉问答定位系统对于医学生和初级外科医生从录制的手术视频中学习和理解是有帮助的。我们提出了一种适用于外科场景的端到端Transformer与共同关注门控视觉-语言（CAT-ViL）的VQLA方法，它不需要通过检测模型进行特征提取。CAT-ViL嵌入模块的设计旨在融合来自视觉和文本来源的异构特征。

    Medical students and junior surgeons often rely on senior surgeons and specialists to answer their questions when learning surgery. However, experts are often busy with clinical and academic work, and have little time to give guidance. Meanwhile, existing deep learning (DL)-based surgical Visual Question Answering (VQA) systems can only provide simple answers without the location of the answers. In addition, vision-language (ViL) embedding is still a less explored research in these kinds of tasks. Therefore, a surgical Visual Question Localized-Answering (VQLA) system would be helpful for medical students and junior surgeons to learn and understand from recorded surgical videos. We propose an end-to-end Transformer with Co-Attention gaTed Vision-Language (CAT-ViL) for VQLA in surgical scenarios, which does not require feature extraction through detection models. The CAT-ViL embedding module is designed to fuse heterogeneous features from visual and textual sources. The fused embedding 
    
[^69]: 连续学习作为计算受限的强化学习

    Continual Learning as Computationally Constrained Reinforcement Learning. (arXiv:2307.04345v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04345](http://arxiv.org/abs/2307.04345)

    本文研究了连续学习作为计算受限的强化学习的主题，提出了一个框架和一套工具来解决人工智能领域长期以来的挑战并促进进一步的研究。

    

    一种能够在漫长的生命周期内高效积累知识并发展越来越复杂技能的智能体可以推动人工智能能力的前沿。连续学习这一长期以来一直是人工智能领域的挑战，本文介绍了关于连续学习的概念并提出了一个框架和一套工具，以促进进一步的研究。

    An agent that efficiently accumulates knowledge to develop increasingly sophisticated skills over a long lifetime could advance the frontier of artificial intelligence capabilities. The design of such agents, which remains a long-standing challenge of artificial intelligence, is addressed by the subject of continual learning. This monograph clarifies and formalizes concepts of continual learning, introducing a framework and set of tools to stimulate further research.
    
[^70]: 使用适配器高效域自适应句子嵌入

    Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v1 [cs.CL])

    [http://arxiv.org/abs/2307.03104](http://arxiv.org/abs/2307.03104)

    本论文提出了一种通过训练轻量级适配器来高效域自适应句子嵌入的方法，避免了微调整个句子嵌入模型的资源消耗。通过训练特定领域的适配器，可以在不同领域中使用同一模型获得良好的性能。

    

    句子嵌入使我们能够捕捉短文本的语义相似性。大多数句子嵌入模型是针对一般语义文本相似性（STS）任务进行训练的。因此，要在特定领域中使用句子嵌入，必须将模型适应于该领域以获得良好的结果。通常，这是通过对感兴趣的域对整个句子嵌入模型进行微调来实现的。虽然这种方法能够产生最先进的结果，但在微调过程中更新了所有模型的权重，使该方法在资源上要求较高。因此，我们提出了训练轻量级适配器的方法，而不是单独为每个目标领域微调整个句子嵌入模型。这些特定领域的适配器不需要微调所有底层句子嵌入模型的参数。相反，我们只训练少量的额外参数，同时保持底层句子嵌入模型的权重不变。训练特定领域的适配器可以始终使用同一模型并在不同领域中获得良好的性能。

    Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity (STS) tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model's weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always 
    
[^71]: 评论帮助更好地学习：基于时间的监督知识蒸馏

    Review helps learn better: Temporal Supervised Knowledge Distillation. (arXiv:2307.00811v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.00811](http://arxiv.org/abs/2307.00811)

    本文提出了一种基于时间的监督知识蒸馏方法，利用评论来帮助学生网络的学习。通过提取学生网络在不同训练阶段的时空特征，并通过动态目标进行训练，实现了对学生网络中旧知识的优化和利用，从而提高了网络的训练性能。

    

    在学习知识时，评论发挥了重要作用。在某个时间点获取的知识可能在之前的经验帮助下得到极大的启发。因此，知识增长过程应该在时间维度上展现出强烈的关联性。在我们的研究中，我们发现在网络训练过程中，特征图的演化遵循时间序列特性。适当的时间监督可以进一步提高网络训练性能。受到这一观察的启发，我们提出了基于时间的监督知识蒸馏（TSKD）。具体而言，我们通过卷积长短期记忆网络（Conv-LSTM）提取学生网络在不同训练阶段的时空特征。然后，我们通过动态目标训练学生网络，而不是静态的教师网络特征。这个过程实现了学生网络中旧知识的优化，并将其用于辅助当前的学习。广泛的实验证实了该方法的有效性。

    Reviewing plays an important role when learning knowledge. The knowledge acquisition at a certain time point may be strongly inspired with the help of previous experience. Thus the knowledge growing procedure should show strong relationship along the temporal dimension. In our research, we find that during the network training, the evolution of feature map follows temporal sequence property. A proper temporal supervision may further improve the network training performance. Inspired by this observation, we propose Temporal Supervised Knowledge Distillation (TSKD). Specifically, we extract the spatiotemporal features in the different training phases of student by convolutional Long Short-term memory network (Conv-LSTM). Then, we train the student net through a dynamic target, rather than static teacher network features. This process realizes the refinement of old knowledge in student network, and utilizes it to assist current learning. Extensive experiments verify the effectiveness and 
    
[^72]: 不知道异常情况的情况下，医学图像的通用异常检测的可行性

    Feasibility of Universal Anomaly Detection without Knowing the Abnormality in Medical Images. (arXiv:2307.00750v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.00750](http://arxiv.org/abs/2307.00750)

    本研究探讨了医学图像中通用异常检测的两个关键方面：比较不同的异常检测方法，解决使用正常图像进行验证过程中的偏差问题，并提出了一种通用异常检测方法。

    

    近年来，许多异常检测方法，特别是深度学习方法，已经被开发出来，通过仅使用正常图像进行训练，来识别异常图像形态。不幸的是，许多先前的异常检测方法是为特定的“已知”异常（例如，脑肿瘤、骨折、细胞类型）进行优化的。此外，即使在训练过程中只使用正常图像，异常图像通常也在验证过程中使用（例如，时期选择、超参数调整），这可能会意外地泄漏“未知”异常。在本研究中，我们通过（1）比较四个医学数据集上的各种异常检测方法，（2）研究在验证阶段如何公正地选择仅使用正常图像来选择最佳异常检测模型时 inevitable but often neglected 问题，以及 （3）提出了一种在医学图像中进行通用异常检测的方法。

    Many anomaly detection approaches, especially deep learning methods, have been recently developed to identify abnormal image morphology by only employing normal images during training. Unfortunately, many prior anomaly detection methods were optimized for a specific "known" abnormality (e.g., brain tumor, bone fraction, cell types). Moreover, even though only the normal images were used in the training process, the abnormal images were often employed during the validation process (e.g., epoch selection, hyper-parameter tuning), which might leak the supposed ``unknown" abnormality unintentionally. In this study, we investigated these two essential aspects regarding universal anomaly detection in medical images by (1) comparing various anomaly detection methods across four medical datasets, (2) investigating the inevitable but often neglected issues on how to unbiasedly select the optimal anomaly detection model during the validation phase using only normal images, and (3) proposing a si
    
[^73]: 快速-INR: 使用隐式神经表示进行效率高的无CPU深度神经网络训练

    Rapid-INR: Storage Efficient CPU-free DNN Training Using Implicit Neural Representation. (arXiv:2306.16699v1 [cs.CV])

    [http://arxiv.org/abs/2306.16699](http://arxiv.org/abs/2306.16699)

    本文提出了一种使用隐式神经表示进行高效的无CPU深度神经网络训练的新方法，通过在GPU上直接存储整个数据集以INR格式，减少了数据传输开销，从而加速训练过程。同时，采用高度并行化和实时执行的解码过程，进一步提升了压缩效果。

    

    隐式神经表示(INR)是一种创新方法，用于表示复杂的形状或对象，而无需明确定义它们的几何形状或表面结构。相反，INR将对象表示为连续函数。先前的研究已经证明了将神经网络用作INR进行图像压缩的有效性，展示了与传统方法（如JPEG）相当的性能。然而，INR在图像压缩之外还具有各种应用潜力。本文介绍了Rapid-INR，一种利用INR对图像进行编码和压缩的新方法，从而加速计算机视觉任务中的神经网络训练。我们的方法在GPU上直接以INR格式存储整个数据集，减少了训练过程中CPU和GPU之间的数据传输开销。此外，从INR到RGB格式的解码过程高度并行化并实时执行。为了进一步提高压缩效果，我们提出了一种迭代的图像压缩算法。

    Implicit Neural Representation (INR) is an innovative approach for representing complex shapes or objects without explicitly defining their geometry or surface structure. Instead, INR represents objects as continuous functions. Previous research has demonstrated the effectiveness of using neural networks as INR for image compression, showcasing comparable performance to traditional methods such as JPEG. However, INR holds potential for various applications beyond image compression. This paper introduces Rapid-INR, a novel approach that utilizes INR for encoding and compressing images, thereby accelerating neural network training in computer vision tasks. Our methodology involves storing the whole dataset directly in INR format on a GPU, mitigating the significant data communication overhead between the CPU and GPU during training. Additionally, the decoding process from INR to RGB format is highly parallelized and executed on-the-fly. To further enhance compression, we propose iterativ
    
[^74]: 可扩展的神经上下文推荐系统中的Bandit算法

    Scalable Neural Contextual Bandit for Recommender Systems. (arXiv:2306.14834v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2306.14834](http://arxiv.org/abs/2306.14834)

    本研究提出了一种可扩展的神经上下文Bandit算法，通过设计Epistemic Neural Recommendation (ENR)网络结构，实现了大规模的Thompson抽样，显著提高了推荐系统的点击率和用户评分。

    

    高质量的推荐系统应通过与用户的有效和探索性互动提供创新和相关内容。然而，许多现有推荐系统中基于监督学习的神经网络仅利用已识别的用户兴趣，对于有效发现未知用户偏好存在不足。尽管神经上下文Bandit算法在通过神经网络实现在线探索方面取得了一些进展，但他们对计算的要求较高，限制了它在实际推荐系统中的广泛应用。在本研究中，我们提出了一种可扩展的样本效率高的神经上下文Bandit算法用于推荐系统。为此，我们设计了一种认知神经网络架构，Epistemic Neural Recommendation (ENR)，它能够在大规模上实现Thompson抽样。通过两个不同的真实任务的大规模实验，ENR显著提高了点击率和用户评分。

    High-quality recommender systems ought to deliver both innovative and relevant content through effective and exploratory interactions with users. Yet, supervised learning-based neural networks, which form the backbone of many existing recommender systems, only leverage recognized user interests, falling short when it comes to efficiently uncovering unknown user preferences. While there has been some progress with neural contextual bandit algorithms towards enabling online exploration through neural networks, their onerous computational demands hinder widespread adoption in real-world recommender systems. In this work, we propose a scalable sample-efficient neural contextual bandit algorithm for recommender systems. To do this, we design an epistemic neural network architecture, Epistemic Neural Recommendation (ENR), that enables Thompson sampling at a large scale. In two distinct large-scale experiments with real-world tasks, ENR significantly boosts click-through rates and user rating
    
[^75]: 基于动态特征重构信号图的旋转机械故障识别

    Fault Identification of Rotating Machinery Based on Dynamic Feature Reconstruction Signal Graph. (arXiv:2306.05281v1 [eess.SP])

    [http://arxiv.org/abs/2306.05281](http://arxiv.org/abs/2306.05281)

    本文提出了一种动态特征重构信号图法，在旋转机械故障诊断模型中取得了关键进展，能够动态地选择最优子带的特征系数矩阵，进行适应性信号重构，并从中提取深层特征。

    

    为了提高在旋转机械强噪声下识别故障的性能，本文提出了一种动态特征重构信号图法，它在所提出的端到端故障诊断模型中起着关键作用。具体来说，首先利用小波包分解（WPD）将原始机械信号分解为包括系数矩阵在内的多个子带。然后，利用最初定义的两个要素MDD和DDD，提出了一种基于L2能量范数的动态特征选择方法（DFSL），它可以根据能量范数分布的差异动态地选择WPD的特征系数矩阵，使每个子信号能够进行适应性信号重构。接下来，将最优特征子带的系数矩阵进行重构和重新组合，得到特征信号图。最后，利用2D-卷积神经网络（2D-CNN）从特征信号图中提取深层特征。

    To improve the performance in identifying the faults under strong noise for rotating machinery, this paper presents a dynamic feature reconstruction signal graph method, which plays the key role of the proposed end-to-end fault diagnosis model. Specifically, the original mechanical signal is first decomposed by wavelet packet decomposition (WPD) to obtain multiple subbands including coefficient matrix. Then, with originally defined two feature extraction factors MDD and DDD, a dynamic feature selection method based on L2 energy norm (DFSL) is proposed, which can dynamically select the feature coefficient matrix of WPD based on the difference in the distribution of norm energy, enabling each sub-signal to take adaptive signal reconstruction. Next the coefficient matrices of the optimal feature sub-bands are reconstructed and reorganized to obtain the feature signal graphs. Finally, deep features are extracted from the feature signal graphs by 2D-Convolutional neural network (2D-CNN). Ex
    
[^76]: 在车载虚拟现实中进行语义通信的对抗攻击和防御

    Adversarial Attacks and Defenses for Semantic Communication in Vehicular Metaverses. (arXiv:2306.03528v1 [cs.CR])

    [http://arxiv.org/abs/2306.03528](http://arxiv.org/abs/2306.03528)

    本文提出了一种层级式SemCom实现的车载虚拟现实框架，可以显著缓解车载虚拟现实应用程序的通信资源压力，并阐述了该框架中SemCom模块的安全风险和可行的防御方法。

    

    对于车载虚拟现实，优化从车上用户的沉浸式体验和服务质量是最终目标之一。语义通信（SemCom）被引入为一种革命性范例，可以显著缓解车载虚拟现实应用程序的通信资源压力，从而实现该目标。SemCom实现了高质量和超高效的车载通信，甚至在车辆之间的数据流量飞速增长时仍能如此。在本文中，我们提出了一个层级式SemCom实现的车载虚拟现实框架，包括全局虚拟现实、本地虚拟现实、SemCom模块和资源池。从分布角度考虑用户的服务质量，本文探讨了该框架的潜在安全漏洞。为了达到这个目的，本研究强调了该框架的SemCom模块的特定安全风险，并提供了一种可行的防御方法。

    For vehicular metaverses, one of the ultimate user-centric goals is to optimize the immersive experience and Quality of Service (QoS) for users on board. Semantic Communication (SemCom) has been introduced as a revolutionary paradigm that significantly eases communication resource pressure for vehicular metaverse applications to achieve this goal. SemCom enables high-quality and ultra-efficient vehicular communication, even with explosively increasing data traffic among vehicles. In this article, we propose a hierarchical SemCom-enabled vehicular metaverses framework consisting of the global metaverse, local metaverses, SemCom module, and resource pool. The global and local metaverses are brand-new concepts from the metaverse's distribution standpoint. Considering the QoS of users, this article explores the potential security vulnerabilities of the proposed framework. To that purpose, this study highlights a specific security risk to the framework's SemCom module and offers a viable de
    
[^77]: 任务关系感知的持续用户表示学习

    Task Relation-aware Continual User Representation Learning. (arXiv:2306.01792v1 [cs.IR])

    [http://arxiv.org/abs/2306.01792](http://arxiv.org/abs/2306.01792)

    本文提出了一种新的持续用户表示学习方法TERACON，它能够学习通用的用户表示，而不是为每个任务学习任务特定的用户表示，具有很强的实用性和学习能力。

    

    用户建模是基于其过去行为学习将用户表示为低维表示空间的方法，它受到了工业界提供个性化服务的兴趣激增。以往的用户建模工作主要集中在学习为单一任务而设计的任务特定用户表示上。然而，由于为每个任务学习任务特定用户表示是不可行的，因此最近的研究引入了通用用户表示的概念，即与多种任务相关的更广义用户表示。尽管这些方法非常有效，但由于数据需求、灾难性遗忘以及为持续添加的任务提供有限的学习能力，现有的学习通用用户表示的方法在实际应用中是不切实际的。本文提出了一种新颖的持续用户表示学习方法TERACON，其学习能力不受任务数量限制。

    User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number 
    
[^78]: 利用ChatGPT进行日志解析的评估

    An Evaluation of Log Parsing with ChatGPT. (arXiv:2306.01590v1 [cs.SE])

    [http://arxiv.org/abs/2306.01590](http://arxiv.org/abs/2306.01590)

    本文评估了ChatGPT在日志解析方面的能力，结果表明它可以通过适当的提示实现有前途的日志解析结果，特别是在few-shot提示下。

    

    软件日志在确保大规模软件系统的可靠性和可维护性方面扮演着至关重要的角色，因为它们通常是运行时信息的唯一来源。日志解析将原始日志消息转换为结构化数据，是向下游日志分析的重要初始步骤。在最近的研究中，当前最先进的大型语言模型（LLM）ChatGPT已广泛应用于各种软件工程任务。然而，它在自动化日志解析方面的性能仍不清楚。在本文中，我们通过解决两个研究问题评估了ChatGPT进行日志解析的能力。 （1）ChatGPT能否有效地解析日志？（2）ChatGPT在不同提示方法下的表现如何？我们的结果表明，ChatGPT可以通过适当的提示实现有前途的日志解析结果，特别是在few-shot提示下。基于我们的发现，我们概述了几个基于ChatGPT的日志解析的挑战和机遇。

    Software logs play an essential role in ensuring the reliability and maintainability of large-scale software systems, as they are often the sole source of runtime information. Log parsing, which converts raw log messages into structured data, is an important initial step towards downstream log analytics. In recent studies, ChatGPT, the current cutting-edge large language model (LLM), has been widely applied to a wide range of software engineering tasks. However, its performance in automated log parsing remains unclear. In this paper, we evaluate ChatGPT's ability to undertake log parsing by addressing two research questions. (1) Can ChatGPT effectively parse logs? (2) How does ChatGPT perform with different prompting methods? Our results show that ChatGPT can achieve promising results for log parsing with appropriate prompts, especially with few-shot prompting. Based on our findings, we outline several challenges and opportunities for ChatGPT-based log parsing.
    
[^79]: 非线性循环神经网络的逆近似理论

    Inverse Approximation Theory for Nonlinear Recurrent Neural Networks. (arXiv:2305.19190v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19190](http://arxiv.org/abs/2305.19190)

    该论文证明了使用RNNs逼近非线性序列关系的逆近似定理，进一步将先前在线性RNNs中识别出的记忆难题推广到了一般的非线性情况，并提出了一个有原则的重新参数化方法来克服这些限制。

    

    我们证明了使用RNNs来逼近非线性序列关系的逆近似定理。这是近似理论中的一种称为Bernstein型结果的结果，它在假设目标函数可以通过假设空间有效逼近的条件下推导出目标函数的属性。特别地，我们展示了非线性序列关系可以被具有hardtanh/tanh激活函数的RNNs稳定逼近的时候，必须具有一个指数衰减的记忆结构--这个概念可以被明确定义。这将先前在线性RNNs中识别出的记忆难题推广到了一般的非线性情况，并量化了RNN架构在学习具有长期记忆的序列关系时的重要限制。基于分析，我们提出了一个有原则的重新参数化方法来克服这些限制。我们的理论结果通过数值实验进行了确认。

    We prove an inverse approximation theorem for the approximation of nonlinear sequence-to-sequence relationships using RNNs. This is a so-called Bernstein-type result in approximation theory, which deduces properties of a target function under the assumption that it can be effectively approximated by a hypothesis space. In particular, we show that nonlinear sequence relationships, viewed as functional sequences, that can be stably approximated by RNNs with hardtanh/tanh activations must have an exponential decaying memory structure -- a notion that can be made precise. This extends the previously identified curse of memory in linear RNNs into the general nonlinear setting, and quantifies the essential limitations of the RNN architecture for learning sequential relationships with long-term memory. Based on the analysis, we propose a principled reparameterization method to overcome the limitations. Our theoretical results are confirmed by numerical experiments.
    
[^80]: EgoHumans:一种以自我为中心的三维多人基准数据集

    EgoHumans: An Egocentric 3D Multi-Human Benchmark. (arXiv:2305.16487v1 [cs.CV])

    [http://arxiv.org/abs/2305.16487](http://arxiv.org/abs/2305.16487)

    EgoHumans是一个全面的自我中心多人基准数据集，旨在推进自我中心的人类三维姿态估计和跟踪的最新进展，可以支持各种任务，包括人类检测、跟踪、2D/3D姿态估计和网格恢复等，并且能够捕捉具有挑战性和无核编排的多人场景。

    

    我们提出了EgoHumans，这是一个新的视角多人视频基准数据集，旨在推进自我中心的人类三维姿态估计和跟踪的最新进展。现有的自我中心基准数据集仅捕捉单个主体或仅限于室内场景，这限制了计算机视觉算法在现实世界应用中的泛化能力。我们提出了一种新的三维捕获设定，构建了一个全面的自我中心多人基准数据集，并提供注释支持各种任务，例如人类检测、跟踪、2D/3D姿态估计和网格恢复等。我们利用带摄像头的普通眼镜进行视角捕捉，并能够捕捉诸如踢足球、击剑、排球等动态活动。此外，我们的多视角设置在严重或完全遮挡下仍能生成准确的3D基准数据。该数据集包含超过125k个自我中心图像，跨越多种场景，特别关注具有挑战性和无核编排的多人场景。

    We present EgoHumans, a new multi-view multi-human video benchmark to advance the state-of-the-art of egocentric human 3D pose estimation and tracking. Existing egocentric benchmarks either capture single subject or indoor-only scenarios, which limit the generalization of computer vision algorithms for real-world applications. We propose a novel 3D capture setup to construct a comprehensive egocentric multi-human benchmark in the wild with annotations to support diverse tasks such as human detection, tracking, 2D/3D pose estimation, and mesh recovery. We leverage consumer-grade wearable camera-equipped glasses for the egocentric view, which enables us to capture dynamic activities like playing soccer, fencing, volleyball, etc. Furthermore, our multi-view setup generates accurate 3D ground truth even under severe or complete occlusion. The dataset consists of more than 125k egocentric images, spanning diverse scenes with a particular focus on challenging and unchoreographed multi-human 
    
[^81]: 基于联邦学习的多语言帕金森病检测模型的安全开发

    Federated learning for secure development of AI models for Parkinson's disease detection using speech from different languages. (arXiv:2305.11284v1 [eess.AS])

    [http://arxiv.org/abs/2305.11284](http://arxiv.org/abs/2305.11284)

    本论文利用联邦学习方法，无需共享患者数据，实现在德语、西班牙语和捷克语三种语言数据集上进行帕金森病检测，取得了优于本地模型的诊断准确性。

    

    帕金森病是一种影响人类说话的神经系统疾病。深度学习模型在自动化帕金森病评估中表现出了出色的性能，但严格的患者数据隐私法规阻碍了机构间共享数据。本文在不共享患者数据的前提下，利用联邦学习在德语、西班牙语和捷克语等三种不同语言的真实数据集上进行了帕金森病检测，并取得了优于本地模型的诊断准确性。

    Parkinson's disease (PD) is a neurological disorder impacting a person's speech. Among automatic PD assessment methods, deep learning models have gained particular interest. Recently, the community has explored cross-pathology and cross-language models which can improve diagnostic accuracy even further. However, strict patient data privacy regulations largely prevent institutions from sharing patient speech data with each other. In this paper, we employ federated learning (FL) for PD detection using speech signals from 3 real-world language corpora of German, Spanish, and Czech, each from a separate institution. Our results indicate that the FL model outperforms all the local models in terms of diagnostic accuracy, while not performing very differently from the model based on centrally combined training sets, with the advantage of not requiring any data sharing among collaborators. This will simplify inter-institutional collaborations, resulting in enhancement of patient outcomes.
    
[^82]: 《琳达，出了什么问题？》“连接谬误”作为公平性问题的探讨

    What's the Problem, Linda? The Conjunction Fallacy as a Fairness Problem. (arXiv:2305.09535v1 [cs.AI])

    [http://arxiv.org/abs/2305.09535](http://arxiv.org/abs/2305.09535)

    这篇论文探讨了“连接谬误”作为公平性问题在人工智能领域中的重要性，并提出了一些问题，以帮助AI研究人员和从业者避免类似情况在未来中复现。

    

    人工智能领域正在专注于创建尽可能接近人类智能的自动决策系统。这一努力推动人工智能研究人员探索心理学等认知领域。 Daniel Kahneman和已故的Amos Tversky在有偏见的人类决策制定方面的工作，包括对连接谬误的研究，因此进行了第二次复兴。 在连接谬误下，决策制定者会违反基本概率法则，认为连词比其中一个部分更有可能。通过一系列与琳达问题最为著名的实验，它已被证明是经得起时间考验的。虽然这种跨学科的努力受到欢迎，但我们担心，人工智能研究人员忽略了琳达问题所捕捉到的驱动力：琳达必须被刻板地描述为一个女性。 在本文中，我们重新审视琳达问题，并将其形式化为AI中的公平性问题。我们认为连接谬误是偏见数据集如何导致偏见结果的明显例子，从而延续和放大现有的系统性偏见。我们提出了一组问题供AI研究人员和从业人员使用，以避免类似情况在未来发生。

    The field of Artificial Intelligence (AI) is focusing on creating automated decision-making (ADM) systems that operate as close as possible to human-like intelligence. This effort has pushed AI researchers into exploring cognitive fields like psychology. The work of Daniel Kahneman and the late Amos Tversky on biased human decision-making, including the study of the conjunction fallacy, has experienced a second revival because of this. Under the conjunction fallacy a human decision-maker will go against basic probability laws and rank as more likely a conjunction over one of its parts. It has been proven overtime through a set of experiments with the Linda Problem being the most famous one. Although this interdisciplinary effort is welcomed, we fear that AI researchers ignore the driving force behind the conjunction fallacy as captured by the Linda Problem: the fact that Linda must be stereotypically described as a woman. In this paper we revisit the Linda Problem and formulate it as a
    
[^83]: IVP-VAE: 利用初值问题求解器对电子病历时间序列进行建模

    IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers. (arXiv:2305.06741v1 [cs.LG])

    [http://arxiv.org/abs/2305.06741](http://arxiv.org/abs/2305.06741)

    本文提出了一种新的方法，在建模电子病历时间序列时，利用直接近似IVP的过程来消除递归计算，从而提高计算效率和训练速度。与目前基于IVP求解器和递归神经网络方法相比，本方法可以达到类似的分类和预测性能。

    

    连续时间模型（例如神经ODE和神经流量）在分析电子病历中常见的不规则采样时间序列方面显示出有希望的结果。 基于这些模型，时间序列通常在变分自动编码器架构中通过初值问题（IVP）求解器和递归神经网络的混合处理。 顺序求解IVP使得这样的模型在计算效率上不够高。 本文提出了一种纯粹使用连续过程对时间序列进行建模的方法，其状态演变可以通过IVP直接近似。 这消除了递归计算的需要，并允许多个状态并行演变。 我们进一步通过一种基于其可逆性的IVP求解器融合编码器和解码器，这导致参数更少，收敛更快。 在三个真实世界的数据集上进行的实验表明，所提出的方法在获得更快的训练速度的同时，仍然可以获得较高的分类性能和预测性能。

    Continuous-time models such as Neural ODEs and Neural Flows have shown promising results in analyzing irregularly sampled time series frequently encountered in electronic health records. Based on these models, time series are typically processed with a hybrid of an initial value problem (IVP) solver and a recurrent neural network within the variational autoencoder architecture. Sequentially solving IVPs makes such models computationally less efficient. In this paper, we propose to model time series purely with continuous processes whose state evolution can be approximated directly by IVPs. This eliminates the need for recurrent computation and enables multiple states to evolve in parallel. We further fuse the encoder and decoder with one IVP solver based on its invertibility, which leads to fewer parameters and faster convergence. Experiments on three real-world datasets show that the proposed approach achieves comparable extrapolation and classification performance while gaining more 
    
[^84]: 使用梯度下降学习决策树

    Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])

    [http://arxiv.org/abs/2305.03515](http://arxiv.org/abs/2305.03515)

    本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。

    

    决策树是用于许多机器学习任务的常见工具，因为它们具有高度的解释性。然而，从数据中学习决策树是一个困难的优化问题，因为它是非凸和非可微的。因此，通常的方法是使用一种贪婪生长算法来学习决策树，在每个内部节点上局部最小化不纯度。不幸的是，这种贪心过程可能会导致次优的决策树。在本文中，我们提出了一种使用梯度下降学习难以处理的轴对齐决策树的新方法。所提出的方法使用反向传播和直通算子在密集的决策树表示上联合优化所有树的参数。我们的方法在二分类基准测试上优于现有方法，并在多类任务中实现了有竞争力的结果。

    Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
    
[^85]: 大型语言模型：分析LLM的理论语言能力

    Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs. (arXiv:2305.00948v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.00948](http://arxiv.org/abs/2305.00948)

    本研究展示了大型语言模型(LLMs)在语言任务上性能不断提高，且首次展示了它们能够生成连贯和有效的语言数据分析。分析和评估它们的元语言能力有助于我们理解它们的一般能力并对语言学理论模型提供新的认识。

    

    大型语言模型(LLMs)的性能最近已经提高到了能够在许多语言任务上表现良好的程度。我们在这里展示了，这些模型也可以生成连贯和有效的语言数据的形式分析，展示了大型语言模型对其元语言能力分析的巨大潜力。LLMs主要是通过文本形式的语言数据进行训练；分析和评估它们的元语言能力改进了我们对它们的一般能力的理解，并对语言学中的理论模型提供了新的认识。在本文中，我们通过专注于形式语言学的三个子领域：句法、音韵学和语义学，探究了GPT-4的元语言能力。我们提出了一个关于大型语言模型元语言分析的研究计划，提出了实验设计，提供了一般指导方针，讨论了限制，并为这个研究方向提供了未来的方向。这个研究还有助于揭示大型语言模型的潜在能力和理论模型的新视角。

    The performance of large language models (LLMs) has recently improved to the point where the models can perform well on many language tasks. We show here that for the first time, the models can also generate coherent and valid formal analyses of linguistic data and illustrate the vast potential of large language models for analyses of their metalinguistic abilities. LLMs are primarily trained on language data in the form of text; analyzing and evaluating their metalinguistic abilities improves our understanding of their general capabilities and sheds new light on theoretical models in linguistics. In this paper, we probe into GPT-4's metalinguistic capabilities by focusing on three subfields of formal linguistics: syntax, phonology, and semantics. We outline a research program for metalinguistic analyses of large language models, propose experimental designs, provide general guidelines, discuss limitations, and offer future directions for this line of research. This line of inquiry als
    
[^86]: SelfDocSeg: 一种自我监督视觉文档分割方法

    SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation. (arXiv:2305.00795v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.00795](http://arxiv.org/abs/2305.00795)

    SelfDocSeg提出了一种完全基于视觉的自我监督文档分割方法，通过生成伪布局训练图像编码器学习文档对象的表示和定位，克服了标注数据稀缺的挑战。

    

    文档布局分析是一个众所周知的问题，已经被广泛探索，涵盖了从文本挖掘、识别到基于图形的表示、视觉特征提取等多种解决方案。然而，现有的大部分工作都忽略了标注数据的稀缺性这一关键事实。我们使用自我监督来解决这一挑战，并且与现有的使用文本挖掘和文本标签的自我监督文档分割方法不同，我们使用了完全基于视觉的方法在预训练中生成伪布局，以在没有任何真实标签或其导出物的情况下训练图像编码器，以自我监督的框架中学习文档对象的表示和定位。

    Document layout analysis is a known problem to the documents research community and has been vastly explored yielding a multitude of solutions ranging from text mining, and recognition to graph-based representation, visual feature extraction, etc. However, most of the existing works have ignored the crucial fact regarding the scarcity of labeled data. With growing internet connectivity to personal life, an enormous amount of documents had been available in the public domain and thus making data annotation a tedious task. We address this challenge using self-supervision and unlike, the few existing self-supervised document segmentation approaches which use text mining and textual labels, we use a complete vision-based approach in pre-training without any ground-truth label or its derivative. Instead, we generate pseudo-layouts from the document images to pre-train an image encoder to learn the document object representation and localization in a self-supervised framework before fine-tun
    
[^87]: 一种用于实时多标签天气识别的MASK-CNN-Transformer模型

    MASK-CNN-Transformer For Real-Time Multi-Label Weather Recognition. (arXiv:2304.14857v1 [cs.CV])

    [http://arxiv.org/abs/2304.14857](http://arxiv.org/abs/2304.14857)

    本文提出了一种考虑天气条件复杂共现依赖关系的多标签识别模型MASK-CNN-Transformer，该模型结合了CNN和Transformer，并利用MASK机制以提高泛化能力。

    

    天气识别对于许多实际生活应用，包括交通安全、环境和气象方面都是至关重要的支持。然而，许多现有的相关工作由于复杂的共现依赖关系而无法全面描述天气条件。本文提出了一种考虑这些依赖关系的新型多标签天气识别模型。该模型名为MASK-CNN-Transformer (MASK-CT)，基于Transformer、卷积过程和MASK机制。

    Weather recognition is an essential support for many practical life applications, including traffic safety, environment, and meteorology. However, many existing related works cannot comprehensively describe weather conditions due to their complex co-occurrence dependencies. This paper proposes a novel multi-label weather recognition model considering these dependencies. The proposed model called MASK-Convolutional Neural Network-Transformer (MASK-CT) is based on the Transformer, the convolutional process, and the MASK mechanism. The model employs multiple convolutional layers to extract features from weather images and a Transformer encoder to calculate the probability of each weather condition based on the extracted features. To improve the generalization ability of MASK-CT, a MASK mechanism is used during the training phase. The effect of the MASK mechanism is explored and discussed. The Mask mechanism randomly withholds some information from one-pair training instances (one image an
    
[^88]: 支持人工智能协作审计LLM的LLM

    Supporting Human-AI Collaboration in Auditing LLMs with LLMs. (arXiv:2304.09991v1 [cs.HC])

    [http://arxiv.org/abs/2304.09991](http://arxiv.org/abs/2304.09991)

    本论文通过对安全和公正人工智能专家的采访以及对人工智能协作和感知文献的研究，增强了“AdaTest”审计工具，这个工具可以通过利用人和生成模型的协同优势，进行更严格的大型语言模型审计。

    

    大型语言模型通过部署在社会技术系统中变得越来越普遍和普及。然而，这些语言模型，无论是用于分类还是生成，都表现出有偏差和不负责任的行为，对人类造成了规模性的伤害。因此，对这些语言模型进行严格审计至关重要。现有的审计工具利用人和或AI来发现失败。在这项工作中，我们借鉴了人工智能协作和感知的文献，并采访了安全和公正人工智能的研究专家，以增强审计工具“AdaTest”（Ribeiro和Lundberg，2022），该工具由生成大型语言模型（LLM）驱动。通过设计过程，我们强调了感知和人工智能通信在协作审计中利用人与生成模型的互补优势的重要性。为了评估增强工具AdaTest ++的有效性，我们进行了用户研究，使参与者进行审计

    Large language models are becoming increasingly pervasive and ubiquitous in society via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased and behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously. Existing auditing tools leverage either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and conduct interviews with research experts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro and Lundberg, 2022), which is powered by a generative large language model (LLM). Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of the augmented tool, AdaTest++, we conduct user studies with participants audit
    
[^89]: Q-HyViT: 带桥块重构的混合视觉Transformer的后训练量化

    Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with Bridge Block Reconstruction. (arXiv:2303.12557v1 [cs.CV])

    [http://arxiv.org/abs/2303.12557](http://arxiv.org/abs/2303.12557)

    本文针对视觉Transformer在移动设备上计算要求高的问题，提出了一种带桥块重构的混合视觉Transformer的后训练量化方法，提高其在移动设备上的加速效果。

    

    最近，视觉Transformer （ViT）在许多任务中取代了卷积神经网络模型，包括分类、检测和分割。然而， ViT 的高计算要求阻碍了它们的广泛应用。为解决这个问题，研究人员提出了高效的混合变压器架构，结合卷积和变压器层，并优化注意力计算，使线性复杂度达到最大。此外，后训练量化被提出作为缓解计算要求的一种手段。将量化技术和高效的混合变压器结构相结合，对于在移动设备上加速视觉transformer至关重要。然而，以前没有研究将量化应用于高效的混合变压器。 在本文中，首先我们发现将现有的ViT PTQ方法直接应用于高效的混合transformer架构会导致严重的精度下降，由此我们提出了Q-HyViT。

    Recently, vision transformers (ViT) have replaced convolutional neural network models in numerous tasks, including classification, detection, and segmentation. However, the high computational requirements of ViTs hinder their widespread implementation. To address this issue, researchers have proposed efficient hybrid transformer architectures that combine convolutional and transformer layers and optimize attention computation for linear complexity. Additionally, post-training quantization has been proposed as a means of mitigating computational demands. Combining quantization techniques and efficient hybrid transformer structures is crucial to maximize the acceleration of vision transformers on mobile devices. However, no prior investigation has applied quantization to efficient hybrid transformers. In this paper, at first, we discover that the straightforward manner to apply the existing PTQ methods for ViT to efficient hybrid transformers results in a drastic accuracy drop due to the
    
[^90]: NeTO: 透明物体的神经重建与自遮挡感知折射追踪

    NeTO:Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing. (arXiv:2303.11219v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.11219](http://arxiv.org/abs/2303.11219)

    提出了一种名为NeTO的方法，通过体渲染从2D图像中捕捉固体透明物体的3D几何体。通过采用隐式有符号距离函数（SDF）作为表面表示，并通过自遮挡感知的折射追踪通过体渲染来优化SDF场，可以实现高质量的重建结果。

    

    我们提出了一种新颖的方法，称为NeTO，通过体渲染从2D图像中捕捉固体透明物体的3D几何体。透明物体的重建是一项非常具有挑战性的任务，不适合通用的重建技术，因为其困扰于镜面光传输现象。虽然现有的基于折射追踪的方法在这个任务上取得了令人印象深刻的结果，但它们仍然存在优化不稳定和细节缺失的问题，因为它们采用的显式表面表示难以优化，且忽略了自遮挡问题。在本文中，我们提出利用隐式有符号距离函数（SDF）作为表面表示，并通过自遮挡感知的折射追踪通过体渲染来优化SDF场。隐式表示使得我们的方法能够在有限的图像集合下重建出高质量的重建结果。

    We present a novel method, called NeTO, for capturing 3D geometry of solid transparent objects from 2D images via volume rendering. Reconstructing transparent objects is a very challenging task, which is ill-suited for general-purpose reconstruction techniques due to the specular light transport phenomena. Although existing refraction-tracing based methods, designed specially for this task, achieve impressive results, they still suffer from unstable optimization and loss of fine details, since the explicit surface representation they adopted is difficult to be optimized, and the self-occlusion problem is ignored for refraction-tracing. In this paper, we propose to leverage implicit Signed Distance Function (SDF) as surface representation, and optimize the SDF field via volume rendering with a self-occlusion aware refractive ray tracing. The implicit representation enables our method to be capable of reconstructing high-quality reconstruction even with a limited set of images, and the s
    
[^91]: GPT是GPT：大语言模型对劳动力市场影响的早期研究

    GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models. (arXiv:2303.10130v1 [econ.GN])

    [http://arxiv.org/abs/2303.10130](http://arxiv.org/abs/2303.10130)

    该研究调查了GPT（大语言模型）和相关技术对美国劳动力市场的潜在影响，发现大约80%的美国劳动力可能会受到10%的工作任务的影响，涵盖了所有工资水平和各行各业，预示着这些模型可能具有显著的经济、社会和政策影响。

    

    我们研究了生成预训练变压器（GPT）模型和相关技术对美国劳动力市场的潜在影响。使用新的标准，我们评估职业与GPT能力的对应关系，结合人类专业知识和GPT-4的分类。我们的研究结果表明，约80%的美国劳动力可能会至少有10%的工作任务受到GPT引入的影响，而约19%的工人可能会看到至少50%的任务受到影响。影响范围涵盖了所有工资水平，高收入工作可能面临更大的风险。值得注意的是，影响并不局限于最近生产率增长较高的行业。我们得出结论，生成预训练变压器具有通用技术（GPT）的特性，表明这些模型可能具有显著的经济、社会和政策影响。

    We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.
    
[^92]: 数据集增强：提高模型准确性和鲁棒性

    Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement. (arXiv:2303.08983v1 [cs.CV])

    [http://arxiv.org/abs/2303.08983](http://arxiv.org/abs/2303.08983)

    提出了一种名为数据集增强的策略，一次性改进数据集，从而提高任何经过增强的数据集训练的模型的准确性、鲁棒性和校准性。例如，使用ImageDataNet+训练的ResNet-50在ImageNet验证集上的准确率提高了1.7％，在ImageNetV2上提高了3.5％，在ImageNet-R上提高了10.0％。

    

    我们提出了一种名为数据集增强的策略，一次性改进数据集，从而提高任何经过增强的数据集训练的模型的准确性，对用户没有额外的训练成本。我们提出了一种基于数据增强和知识蒸馏的数据集增强策略。我们的通用策略是基于广泛的CNN和基于transformer的模型的分析，以及对带有各种数据增强的最先进模型进行大规模的蒸馏研究。我们创建了ImageDataNet+的增强版本，以及增强的数据集CIFAR-100+，Flowers-102+和Food-101+。使用ImageDataNet+训练的模型更准确、更有鲁棒性和校准性，并且对下游任务（例如分割和检测）具有很好的迁移能力。例如，ResNet-50在ImageNet验证集上的准确率提高了1.7％，在ImageNetV2上提高了3.5％，在ImageNet-R上提高了10.0％。在ImageDataNet+上测量的Expected Calibration Error（ECE）也有显著改进。

    We propose Dataset Reinforcement, a strategy to improve a dataset once such that the accuracy of any model architecture trained on the reinforced dataset is improved at no additional training cost for users. We propose a Dataset Reinforcement strategy based on data augmentation and knowledge distillation. Our generic strategy is designed based on extensive analysis across CNN- and transformer-based models and performing large-scale study of distillation with state-of-the-art models with various data augmentations. We create a reinforced version of the ImageNet training dataset, called ImageNet+, as well as reinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trained with ImageNet+ are more accurate, robust, and calibrated, and transfer well to downstream tasks (e.g., segmentation and detection). As an example, the accuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% on ImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on the Ima
    
[^93]: 基于规则的OoD检测

    Rule-based Out-Of-Distribution Detection. (arXiv:2303.01860v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.01860](http://arxiv.org/abs/2303.01860)

    本文提出了一种基于XAI的方法，通过不同指标鉴别in-distribution和out of-distribution之间的相似性，从而实现非参数统计模型的OoD检测。该方法在多个复杂场景下表现出良好的检测准确度和评估训练和操作条件的接近程度。

    

    检测数据集中是否存在OoD是部署机器学习的最关键问题之一。本文采用XAI解释性人工智能方法，通过考虑不同的指标鉴别出in-distribution和out of-distribution之间的相似性。该方法是非参数统计模型，不需要分布假设。在复杂应用场景下（如预测性维护、车队管制、网络安全追踪等）的验证显示了该方法在检测准确度和评估训练和操作条件的接近程度方面的优良性能。结果可在以下链接中的github上获得: https://github.com/giacomo97cnr/Rule-based-ODD。

    Out-of-distribution detection is one of the most critical issue in the deployment of machine learning. The data analyst must assure that data in operation should be compliant with the training phase as well as understand if the environment has changed in a way that autonomous decisions would not be safe anymore. The method of the paper is based on eXplainable Artificial Intelligence (XAI); it takes into account different metrics to identify any resemblance between in-distribution and out of, as seen by the XAI model. The approach is non-parametric and distributional assumption free. The validation over complex scenarios (predictive maintenance, vehicle platooning, covert channels in cybersecurity) corroborates both precision in detection and evaluation of training-operation conditions proximity. Results are available via open source and open data at the following link: https://github.com/giacomo97cnr/Rule-based-ODD.
    
[^94]: 特征亲和力辅助的知识蒸馏和深度神经网络无标签数据的量化

    Feature Affinity Assisted Knowledge Distillation and Quantization of Deep Neural Networks on Label-Free Data. (arXiv:2302.10899v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10899](http://arxiv.org/abs/2302.10899)

    本文提出了一种特征亲和力辅助的知识蒸馏方法，通过结合logit损失和特征亲和力损失，可以在无标签数据上压缩深度神经网络模型。

    

    本文提出了一种特征亲和力（FA）辅助的知识蒸馏（KD）方法，以改进深度神经网络（DNN）的量化感知训练。DNN的中间特征图上的FA损失起到了将中间步骤的解决方案教给学生的作用，而不仅仅是在传统的KD中作用于网络输出级别的logits损失。将logit损失和FA损失结合起来，我们发现量化的学生网络得到的监督比来自标记地面真实数据的监督更强。所得到的FAQD能够在无标签数据上压缩模型，这带来了即时的实际效益，因为预先训练好的教师模型是随时可用的，而无标签数据又是丰富的。相反，数据标记通常是费时费力的。最后，我们提出了一种快速特征亲和力（FFA）损失，它以较低的计算复杂度准确近似FA损失，有助于加快高分辨率训练的速度。

    In this paper, we propose a feature affinity (FA) assisted knowledge distillation (KD) method to improve quantization-aware training of deep neural networks (DNN). The FA loss on intermediate feature maps of DNNs plays the role of teaching middle steps of a solution to a student instead of only giving final answers in the conventional KD where the loss acts on the network logits at the output level. Combining logit loss and FA loss, we found that the quantized student network receives stronger supervision than from the labeled ground-truth data. The resulting FAQD is capable of compressing model on label-free data, which brings immediate practical benefits as pre-trained teacher models are readily available and unlabeled data are abundant. In contrast, data labeling is often laborious and expensive. Finally, we propose a fast feature affinity (FFA) loss that accurately approximates FA loss with a lower order of computational complexity, which helps speed up training for high resolution
    
[^95]: 语言特定的情绪概念知识表示对情绪推断的因果支持

    Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference. (arXiv:2302.09582v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.09582](http://arxiv.org/abs/2302.09582)

    本研究通过操纵大型语言模型中的语言衍生的情绪概念知识表示，探讨了语言是否会因果支持情绪推断。实验结果显示，属性特定的神经元操纵导致情绪推断任务的性能下降，这与人类心理空间中不同属性的重要性有关。这些发现为支持基于语言的情绪推断机制提供了因果证据，并凸显了情绪概念知识的贡献。

    

    在情绪科学中，如何理解语言支持情绪推断仍然是一个争议的话题。本研究通过操纵大型语言模型中的语言衍生的情绪概念知识表示，调查了语言是否会因果支持情绪推断。使用提示技术，发现了14个情绪概念的属性由不同的人工神经元群体表示。通过操纵这些属性相关的神经元，与随机操纵相比，大多数情绪推断任务的表现出现了下降。属性特定的表现下降与人类心理空间中不同属性的重要性有关。我们的发现提供了支持基于语言的情绪推断机制的因果证据，并强调了情绪概念知识的贡献。

    Understanding how language supports emotion inference remains a topic of debate in emotion science. The present study investigated whether language-derived emotion-concept knowledge would causally support emotion inference by manipulating the language-specific knowledge representations in large language models. Using the prompt technique, 14 attributes of emotion concepts were found to be represented by distinct artificial neuron populations. By manipulating these attribute-related neurons, the majority of the emotion inference tasks showed performance deterioration compared to random manipulations. The attribute-specific performance deterioration was related to the importance of different attributes in human mental space. Our findings provide causal evidence in support of a language-based mechanism for emotion inference and highlight the contributions of emotion-concept knowledge.
    
[^96]: 基于混合谱方法和谐振子的非可分离协方差核的时空高斯过程研究

    Non-separable Covariance Kernels for Spatiotemporal Gaussian Processes based on a Hybrid Spectral Method and the Harmonic Oscillator. (arXiv:2302.09580v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09580](http://arxiv.org/abs/2302.09580)

    该论文提出了一种基于混合谱方法和谐振子的非可分离协方差核的时空高斯过程研究，通过物理论证推导出一类新型的非可分离协方差核，能更好地捕捉观测到的时空相关性。

    

    高斯过程提供了一个灵活的非参数框架，用于近似高维空间中的函数。协方差核是高斯过程的主要引擎，包含了预测分布的相关性。对于具有时空数据集的应用，合适的核应该建模联合的时空依赖关系。可分离的时空协方差核提供了简单和计算效率较高的方案。然而，非可分离核包含了更好地捕捉观测到的相关性的时空交互作用。大多数具有显式表达式的非可分离核是基于数学考虑（可允许条件）而非基于第一原理导出的。我们提出了一种基于物理论证的混合谱方法来生成协方差核。我们使用这种方法推导了一类新型的物理动机的非可分离协方差核，它们的根源来自随机线性...

    Gaussian processes provide a flexible, non-parametric framework for the approximation of functions in high-dimensional spaces. The covariance kernel is the main engine of Gaussian processes, incorporating correlations that underpin the predictive distribution. For applications with spatiotemporal datasets, suitable kernels should model joint spatial and temporal dependence. Separable space-time covariance kernels offer simplicity and computational efficiency. However, non-separable kernels include space-time interactions that better capture observed correlations. Most non-separable kernels that admit explicit expressions are based on mathematical considerations (admissibility conditions) rather than first-principles derivations. We present a hybrid spectral approach for generating covariance kernels which is based on physical arguments. We use this approach to derive a new class of physically motivated, non-separable covariance kernels which have their roots in the stochastic, linear, 
    
[^97]: 单个RGB图像中相互作用的双手重建的分离迭代细化框架

    Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image. (arXiv:2302.02410v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.02410](http://arxiv.org/abs/2302.02410)

    本文提出了一种分离迭代细化框架，用于从单个RGB图像中重建相互作用的双手。通过定义2D视觉特征空间和3D关节特征空间，实现了手部重建的像素对齐，并高效地建模手之间的空间关系。

    

    从单个RGB图像中重建相互作用的双手是一项非常具有挑战性的任务。一方面，两只手之间严重的互相遮挡和类似的局部外观使得视觉特征的提取变得困难，导致估计的手模型和图像的错位。另一方面，相互作用的双手之间存在复杂的空间关系，这显著增加了手姿态的解空间，增加了网络学习的难度。在本文中，我们提出了一个分离迭代细化框架，以实现像素对齐的手部重建，并高效地建模手之间的空间关系。具体而言，我们定义了两个具有不同特征的特征空间，即2D视觉特征空间和3D关节特征空间。首先，我们从视觉特征图中获取关节特征，并利用图卷积网络和变形器在3D关节特征空间进行手内部和手之间的信息交互。

    Reconstructing interacting hands from a single RGB image is a very challenging task. On the one hand, severe mutual occlusion and similar local appearance between two hands confuse the extraction of visual features, resulting in the misalignment of estimated hand meshes and the image. On the other hand, there are complex spatial relationship between interacting hands, which significantly increases the solution space of hand poses and increases the difficulty of network learning. In this paper, we propose a decoupled iterative refinement framework to achieve pixel-alignment hand reconstruction while efficiently modeling the spatial relationship between hands. Specifically, we define two feature spaces with different characteristics, namely 2D visual feature space and 3D joint feature space. First, we obtain joint-wise features from the visual feature map and utilize a graph convolution network and a transformer to perform intra- and inter-hand information interaction in the 3D joint fea
    
[^98]: NeSyFOLD: 从卷积神经网络中提取逻辑程序

    NeSyFOLD: Extracting Logic Programs from Convolutional Neural Networks. (arXiv:2301.12667v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12667](http://arxiv.org/abs/2301.12667)

    NeSyFOLD是一种神经符号框架，可以从CNN中提取逻辑规则并生成可解释的分类模型。它使用基于规则的FOLD-SE-M机器学习算法和自动映射算法来将CNN核映射到语义概念，并产生可解释的规则集。

    

    我们提出了一种新的神经符号框架NeSyFOLD，从CNN中提取逻辑规则并创建一个NeSyFOLD模型来对图像进行分类。NeSyFOLD的学习流程如下：（i）我们首先在输入图像数据集上预训练CNN，并提取最后一层核的激活作为二进制值；（ii）接下来，我们使用基于规则的FOLD-SE-M机器学习算法生成能够分类图像的逻辑程序——表示为每个核对应的二进制激活向量，同时产生逻辑解释。由FOLD-SE-M算法生成的规则具有核编号作为谓词。我们设计了一种新的算法，用于自动将CNN核映射到图像中的语义概念。这个映射被用来将规则集中的谓词名（核编号）替换为对应的语义概念标签。结果产生了可解释的规则集，可以被人类直观地理解。我们将我们的NeSyFOLD框架与最先进的方法进行了比较，并表明它可以实现竞争性的分类性能，同时提供可解释的和解释性的知识。

    We present a novel neurosymbolic framework called NeSyFOLD to extract logic rules from a CNN and create a NeSyFOLD model to classify images. NeSyFOLD's learning pipeline is as follows: (i) We first pre-train a CNN on the input image dataset and extract activations of the last layer kernels as binary values; (ii) Next, we use the FOLD-SE-M rule-based machine learning algorithm to generate a logic program that can classify an image -- represented as a vector of binary activations corresponding to each kernel -- while producing a logical explanation. The rules generated by the FOLD-SE-M algorithm have kernel numbers as predicates. We have devised a novel algorithm for automatically mapping the CNN kernels to semantic concepts in the images. This mapping is used to replace predicate names (kernel numbers) in the rule-set with corresponding semantic concept labels. The resulting rule-set is interpretable, and can be intuitively understood by humans. We compare our NeSyFOLD framework with th
    
[^99]: 通过否定和谓词创造实现一般化

    Generalisation Through Negation and Predicate Invention. (arXiv:2301.07629v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.07629](http://arxiv.org/abs/2301.07629)

    这篇论文介绍了一种结合了否定和谓词创造的归纳逻辑编程方法，通过学习仅具有全量化身体变量的规则来改善泛化能力，并实现在NOPI中。实验结果表明，这种方法可以提高预测准确性和学习效率。

    

    在机器学习中，从少量的示例中进行泛化是一个基本挑战。为了解决这个挑战，我们引入了一种归纳逻辑编程（ILP）方法，结合了否定和谓词创造。结合这两个特性可以使ILP系统通过学习仅具有全量化身体变量的规则来更好地泛化。我们将这个想法实现在NOPI中，它可以学习具有谓词创造的正常逻辑程序，包括具有分层否定的Datalog程序。我们在多个领域上的实验结果表明，我们的方法可以提高预测准确性和学习时间。

    The ability to generalise from a small number of examples is a fundamental challenge in machine learning. To tackle this challenge, we introduce an inductive logic programming (ILP) approach that combines negation and predicate invention. Combining these two features allows an ILP system to generalise better by learning rules with universally quantified body-only variables. We implement our idea in NOPI, which can learn normal logic programs with predicate invention, including Datalog programs with stratified negation. Our experimental results on multiple domains show that our approach can improve predictive accuracies and learning times.
    
[^100]: 探索可解释的土地覆盖映射：一种基于反事实的策略

    Towards Explainable Land Cover Mapping: a Counterfactual-based Strategy. (arXiv:2301.01520v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01520](http://arxiv.org/abs/2301.01520)

    本论文提出了一种基于反事实的策略，用于土地覆盖分类任务中的卫星图像时间序列。该方法具有灵活性，能发现土地覆盖类别之间的有趣信息关系，同时通过鼓励时间连续的扰动来得到更稀疏且可解释的解决方案。

    

    反事实解释是提高深度学习模型解释性的新兴工具。在给定样本的情况下，这些方法寻找并向用户显示决策边界上类似的样本。在本文中，我们提出了一种用于土地覆盖分类任务的多类别设置中的卫星图像时间序列的对抗生成反事实方法。该方法的一个独特特点是在给定反事实解释的情况下对目标类别没有先验假设。这种固有的灵活性允许发现土地覆盖类别之间的有趣信息关系。另一个特点是鼓励反事实与原始样本之间仅在一个小而紧凑的时间段内有所不同。这些时间连续的扰动允许得到更稀疏且因此更可解释的解决方案。此外，通过强制生成的反事实解释的合理性/真实性。

    Counterfactual explanations are an emerging tool to enhance interpretability of deep learning models. Given a sample, these methods seek to find and display to the user similar samples across the decision boundary. In this paper, we propose a generative adversarial counterfactual approach for satellite image time series in a multi-class setting for the land cover classification task. One of the distinctive features of the proposed approach is the lack of prior assumption on the targeted class for a given counterfactual explanation. This inherent flexibility allows for the discovery of interesting information on the relationship between land cover classes. The other feature consists of encouraging the counterfactual to differ from the original sample only in a small and compact temporal segment. These time-contiguous perturbations allow for a much sparser and, thus, interpretable solution. Furthermore, plausibility/realism of the generated counterfactual explanations is enforced via the
    
[^101]: RECOMED: 一种综合性药物推荐系统

    RECOMED: A Comprehensive Pharmaceutical Recommendation System. (arXiv:2301.00280v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2301.00280](http://arxiv.org/abs/2301.00280)

    RECOMED是基于患者和药物特征设计的一种综合性药物推荐系统，采用人工智能模型和自然语言处理方法进行实现，首次考虑了患者条件和历史来选择合适药物，以及药物相互作用。

    

    基于从Drugs.com和Druglib.com提取的患者和药物特征，设计了一种综合性药物推荐系统。首先，将这些数据库的数据进行合并，建立了一个包含患者和药物信息的数据集。其次，对患者和药物进行了聚类，然后根据患者提供的不同评级以及从患者和药物规格中获取的知识，并考虑药物相互作用进行推荐。据我们所知，我们是第一组在所提出的方法中考虑患者的状况和历史来选择特定合适药物的。我们的方法使用人工智能（AI）模型来实现。在预处理中，采用自然语言处理方法进行情感分析，并使用基于神经网络的方法和推荐系统算法对系统建模。

    A comprehensive pharmaceutical recommendation system was designed based on the patients and drugs features extracted from Drugs.com and Druglib.com. First, data from these databases were combined, and a dataset of patients and drug information was built. Secondly, the patients and drugs were clustered, and then the recommendation was performed using different ratings provided by patients, and importantly by the knowledge obtained from patients and drug specifications, and considering drug interactions. To the best of our knowledge, we are the first group to consider patients conditions and history in the proposed approach for selecting a specific medicine appropriate for that particular user. Our approach applies artificial intelligence (AI) models for the implementation. Sentiment analysis using natural language processing approaches is employed in pre-processing along with neural network-based methods and recommender system algorithms for modeling the system. In our work, patients co
    
[^102]: 使用Transformer和相似度度量改进阿拉伯文本分类的数据增强

    Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification. (arXiv:2212.13939v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.13939](http://arxiv.org/abs/2212.13939)

    本论文提出一种使用Transformer和相似度度量进行数据增强的方法，以改进阿拉伯文本分类，该方法利用AraGPT-2进行增强，并使用Euclidean、cosine、Jaccard和BLEU距离评估生成的句子。

    

    学习模型的性能很大程度上依赖于训练数据的可用性和充足性。为了解决数据集充足性问题，研究人员广泛探索了数据增强（DA）作为一种有前景的方法。DA通过对现有数据应用转换来生成新的数据实例，从而增加数据集的大小和变化性。这种方法提高了模型的性能和准确性，尤其是在解决分类任务中的类别不平衡问题方面。然而，很少有研究探讨阿拉伯语的DA，而是依赖于传统的方法，如释义或基于噪声的技术。在本文中，我们提出了一种新的阿拉伯语DA方法，采用了最近强大的建模技术AraGPT-2来进行增强过程。利用欧氏距离、余弦距离、Jaccard距离和BLEU距离对生成的句子进行了上下文、语义、多样性和新颖性的评估。最后，使用AraBERT transformer对情感进行了预测。

    The performance of learning models heavily relies on the availability and adequacy of training data. To address the dataset adequacy issue, researchers have extensively explored data augmentation (DA) as a promising approach. DA generates new data instances through transformations applied to the available data, thereby increasing dataset size and variability. This approach has enhanced model performance and accuracy, particularly in addressing class imbalance problems in classification tasks. However, few studies have explored DA for the Arabic language, relying on traditional approaches such as paraphrasing or noising-based techniques. In this paper, we propose a new Arabic DA method that employs the recent powerful modeling technique, namely the AraGPT-2, for the augmentation process. The generated sentences are evaluated in terms of context, semantics, diversity, and novelty using the Euclidean, cosine, Jaccard, and BLEU distances. Finally, the AraBERT transformer is used on sentime
    
[^103]: 学习潜在表示以与人类共适应

    Learning Latent Representations to Co-Adapt to Humans. (arXiv:2212.09586v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2212.09586](http://arxiv.org/abs/2212.09586)

    机器人学习者在与非稳态人类共同互动时面临挑战，本文通过学习和推理人类策略和策略动态的高级表示，提出了一种算法形式以实现机器人与动态人类共适应。

    

    当机器人在家庭、道路或工厂中与人类互动时，人类的行为通常会因为机器人而改变。对于机器人学习者来说，非稳态的人类是一个挑战：机器人已经学会与原始人类协调一起执行的动作可能在人类适应机器人后失败。在本文中，我们提出了一个算法形式，使得机器人（即自我代理）能够与动态人类（即其他代理）共同适应，只利用机器人的低级状态、动作和奖励。一个核心挑战是，人类不仅会对机器人的行为做出反应，而且人类的反应方式随着时间和用户之间的变化而变化。为了应对这个挑战，我们的洞察力是，机器人不需要建立人类的精确模型，而是可以学习和推理人类策略和策略动态的高级表示。基于这个洞察，我们开发了RILI：稳健地影响潜在意图。RILI首先将低级机器人观察嵌入到高级表示中...

    When robots interact with humans in homes, roads, or factories the human's behavior often changes in response to the robot. Non-stationary humans are challenging for robot learners: actions the robot has learned to coordinate with the original human may fail after the human adapts to the robot. In this paper we introduce an algorithmic formalism that enables robots (i.e., ego agents) to co-adapt alongside dynamic humans (i.e., other agents) using only the robot's low-level states, actions, and rewards. A core challenge is that humans not only react to the robot's behavior, but the way in which humans react inevitably changes both over time and between users. To deal with this challenge, our insight is that -- instead of building an exact model of the human -- robots can learn and reason over high-level representations of the human's policy and policy dynamics. Applying this insight we develop RILI: Robustly Influencing Latent Intent. RILI first embeds low-level robot observations into 
    
[^104]: DeepCut: 使用图神经网络聚类的无监督分割技术

    DeepCut: Unsupervised Segmentation using Graph Neural Networks Clustering. (arXiv:2212.05853v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.05853](http://arxiv.org/abs/2212.05853)

    本研究引入了一种轻量级的图神经网络（GNN）来解决图像分割中特征降维的问题，并采用无监督方法进行分割。与传统方法相比，该方法在构建图时同时使用局部特征和原始特征，从而能更好地进行聚类和分类分割。

    

    图像分割是计算机视觉中的基本任务。为了训练监督方法需要进行数据注释，这是一项耗时耗力的工作，因此激励了无监督方法的发展。当前的方法通常依赖于从预训练网络中提取深度特征来构建图，然后再应用经典的聚类方法如k-means和归一化割作为后处理步骤。然而，这种方法将特征中的高维信息降维为一对一的标量亲和力。为了解决这个限制，本研究引入了一种轻量级的图神经网络（GNN），用它来替代经典的聚类方法，并在优化相同的聚类目标函数的同时。与现有的方法不同，我们的GNN同时接受局部图像特征之间的一对一亲和力和原始特征作为输入。原始特征与聚类目标之间的直接连接使得我们能够隐式地对不同图之间的聚类进行分类，从而获得更好的分割效果。

    Image segmentation is a fundamental task in computer vision. Data annotation for training supervised methods can be labor-intensive, motivating unsupervised methods. Current approaches often rely on extracting deep features from pre-trained networks to construct a graph, and classical clustering methods like k-means and normalized-cuts are then applied as a post-processing step. However, this approach reduces the high-dimensional information encoded in the features to pair-wise scalar affinities. To address this limitation, this study introduces a lightweight Graph Neural Network (GNN) to replace classical clustering methods while optimizing for the same clustering objective function. Unlike existing methods, our GNN takes both the pair-wise affinities between local image features and the raw features as input. This direct connection between the raw features and the clustering objective enables us to implicitly perform classification of the clusters between different graphs, resulting 
    
[^105]: 超越对象识别：面向对象概念学习的新基准

    Beyond Object Recognition: A New Benchmark towards Object Concept Learning. (arXiv:2212.02710v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02710](http://arxiv.org/abs/2212.02710)

    本文提出了一个挑战性的 Object Concept Learning (OCL) 任务，涉及对象属性、作用及其因果关系。作者构建了密集注释的知识库以支持 OCL，提出了 Object Concept Reasoning Network (OCRN) 作为基线，提升了对象认知的发展。

    

    理解对象是人工智能的核心，特别是对于具有体验的人工智能而言。虽然深度学习在对象识别方面表现出色，但当前机器仍然难以学习更高层次的知识，例如对象具有哪些属性，以及我们能够使用对象做什么。在本文中，我们提出了一项具有挑战性的 Object Concept Learning (OCL) 任务，以推动对象理解的发展。它要求机器推理出对象的作用，并同时给出原因：是哪些属性使得一个对象具有这些作用。为了支持 OCL，我们构建了一个密集注释的知识库，包括三个层次的对象概念（类别、属性、作用），以及三个层次的因果关系。通过分析 OCL 的因果结构，我们提出了一种基线：Object Concept Reasoning Network (OCRN)。它利用因果干预和概念实例化来推断三个层次，遵循它们之间的因果关系。

    Understanding objects is a central building block of artificial intelligence, especially for embodied AI. Even though object recognition excels with deep learning, current machines still struggle to learn higher-level knowledge, e.g., what attributes an object has, and what can we do with an object. In this work, we propose a challenging Object Concept Learning (OCL) task to push the envelope of object understanding. It requires machines to reason out object affordances and simultaneously give the reason: what attributes make an object possesses these affordances. To support OCL, we build a densely annotated knowledge base including extensive labels for three levels of object concept (category, attribute, affordance), and the causal relations of three levels. By analyzing the causal structure of OCL, we present a baseline, Object Concept Reasoning Network (OCRN). It leverages causal intervention and concept instantiation to infer the three levels following their causal relations. In ex
    
[^106]: PhysDiff: 物理引导的人体运动扩散模型

    PhysDiff: Physics-Guided Human Motion Diffusion Model. (arXiv:2212.02500v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02500](http://arxiv.org/abs/2212.02500)

    PhysDiff是一种物理引导的人体运动扩散模型，通过将物理约束融入扩散过程中，能够生成多样而逼真的人体动作，并解决了现有模型中存在的物理缺陷问题。

    

    降噪扩散模型为生成多样而逼真的人体动作提供了巨大的潜力。然而，现有的运动扩散模型在扩散过程中往往忽视了物理定律，并且经常生成出具有明显缺陷的不符合物理规律的动作，如浮空、滑脚和透地。这严重影响了生成动作的质量并限制了它们在现实世界中的应用。为了解决这个问题，我们提出了一种新的物理引导的运动扩散模型（PhysDiff），它将物理约束融入到扩散过程中。具体地，我们提出了一个基于物理模拟器中运动模仿的物理运动投影模块，用于将扩散步骤的去噪动作投影为符合物理规律的动作。投影后的动作进一步用于下一个扩散步骤，以引导去噪扩散过程。直观地说，我们模型中使用物理定律使得动作逐步趋向于符合物理规律。

    Denoising diffusion models hold great promise for generating diverse and realistic human motions. However, existing motion diffusion models largely disregard the laws of physics in the diffusion process and often generate physically-implausible motions with pronounced artifacts such as floating, foot sliding, and ground penetration. This seriously impacts the quality of generated motions and limits their real-world application. To address this issue, we present a novel physics-guided motion diffusion model (PhysDiff), which incorporates physical constraints into the diffusion process. Specifically, we propose a physics-based motion projection module that uses motion imitation in a physics simulator to project the denoised motion of a diffusion step to a physically-plausible motion. The projected motion is further used in the next diffusion step to guide the denoising diffusion process. Intuitively, the use of physics in our model iteratively pulls the motion toward a physically-plausib
    
[^107]: 带有基于模型先验的一次性隐式动画化头像制作方法

    One-shot Implicit Animatable Avatars with Model-based Priors. (arXiv:2212.02469v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02469](http://arxiv.org/abs/2212.02469)

    本文提出了ELICIT，一种从单张图片学习人类特定神经辐射场的方法，同时利用3D几何先验和视觉语义先验实现了一次性数据高效的逼真可动3D人体的创建。

    

    现有的创建人类头像的神经渲染方法通常要么需要稠密输入信号（如视频或多视角图像），要么利用从大规模特定3D人体数据集中学到的先验，使得可以使用稀疏视角输入进行重建。大多数这些方法在仅有一张图像时无法实现逼真重建。为了实现数据高效的逼真可动3D人体的创建，我们提出了ELICIT，这是一种从一张图片学习人体特定神经辐射场的新方法。受到人类可以轻松估计身体几何形状并从一张图片中想象造型完整的衣柜的启示，我们在ELICIT中利用了两个先验：3D几何先验和视觉语义先验。具体来说，ELICIT利用一个蒙皮顶点模板模型（即SMPL）的3D身体形状几何先验，并通过基于CLIP的预训练模型实现了视觉服装语义先验。这两个先验均用于从单个图像进行逼真的可动3D重建。

    Existing neural rendering methods for creating human avatars typically either require dense input signals such as video or multi-view images, or leverage a learned prior from large-scale specific 3D human datasets such that reconstruction can be performed with sparse-view inputs. Most of these methods fail to achieve realistic reconstruction when only a single image is available. To enable the data-efficient creation of realistic animatable 3D humans, we propose ELICIT, a novel method for learning human-specific neural radiance fields from a single image. Inspired by the fact that humans can effortlessly estimate the body geometry and imagine full-body clothing from a single image, we leverage two priors in ELICIT: 3D geometry prior and visual semantic prior. Specifically, ELICIT utilizes the 3D body shape geometry prior from a skinned vertex-based template model (i.e., SMPL) and implements the visual clothing semantic prior with the CLIP-based pre-trained models. Both priors are used 
    
[^108]: 基于配对互补时间循环一致对抗网络的雷达降水预测方法

    PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial Networks for Radar-Based Precipitation Nowcasting. (arXiv:2211.15046v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15046](http://arxiv.org/abs/2211.15046)

    本文提出了一种基于配对互补时间循环一致对抗网络的雷达降水预测方法，该方法包括两个生成器网络和循环一致性损失和对抗性损失。实验证明，该方法在准确性和推广能力方面优于现有的技术方法。

    

    降水预测方法已经在过去几个世纪里得到了很好的发展，因为雨水对人类生活有着至关重要的影响。现有的降水预测模型包括定量降水预测 (QPF) 模型、卷积长短期记忆 (ConvLSTM) 模型以及最新的 MetNet-2 等多种复杂的方法。本文提出了基于配对互补时间循环一致对抗网络 (PCT-CycleGAN) 的雷达降水预测方法，受对抗生成网络 (CycleGAN) 强大的图像转换性能启发。PCT-CycleGAN 使用两个具有向前和向后时间动态的生成器网络生成时序性，每个生成器网络学习一个庞大的一对一映射，以逼近表示每个方向上的时间动态的映射函数。为了创建配对互补循环之间的强健时间因果关系，我们应用了循环一致性损失和对抗性损失。广泛的实验证明，PCT-CycleGAN 在准确性和推广能力方面优于现有的技术方法。

    The precipitation nowcasting methods have been elaborated over the centuries because rain has a crucial impact on human life. Not only quantitative precipitation forecast (QPF) models and convolutional long short-term memory (ConvLSTM), but also various sophisticated methods such as the latest MetNet-2 are emerging. In this paper, we propose a paired complementary temporal cycle-consistent adversarial networks (PCT-CycleGAN) for radar-based precipitation nowcasting, inspired by cycle-consistent adversarial networks (CycleGAN), which shows strong performance in image-to-image translation. PCT-CycleGAN generates temporal causality using two generator networks with forward and backward temporal dynamics in paired complementary cycles. Each generator network learns a huge number of one-to-one mappings about time-dependent radar-based precipitation data to approximate a mapping function representing the temporal dynamics in each direction. To create robust temporal causality between paired 
    
[^109]: 基于时间逻辑模式的结果导向的规范过程监控

    Outcome-Oriented Prescriptive Process Monitoring Based on Temporal Logic Patterns. (arXiv:2211.04880v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.04880](http://arxiv.org/abs/2211.04880)

    提出了一种基于时间逻辑模式的结果导向的规范过程监控系统，推荐在过程执行期间必须保证的活动之间的时间关系，以实现期望的结果。

    

    规范过程监控系统在业务过程执行期间建议干预措施，以防止出现负面结果。这种干预措施必须可靠，即它们必须保证实现期望的结果或性能，并且它们必须灵活，即它们必须避免推翻正常的流程执行或强制执行给定的活动。然而，目前大部分现有的规范过程监控解决方案，虽然在推荐可靠性方面表现出色，但对用户提供了非常具体的（序列）活动的推荐，而并不关心这些推荐的可行性。为了解决这个问题，我们提出了一种新的结果导向的规范过程监控系统，推荐在过程执行期间必须保证的活动之间的时间关系，以实现期望的结果。

    Prescriptive Process Monitoring systems recommend, during the execution of a business process, interventions that, if followed, prevent a negative outcome of the process. Such interventions have to be reliable, that is, they have to guarantee the achievement of the desired outcome or performance, and they have to be flexible, that is, they have to avoid overturning the normal process execution or forcing the execution of a given activity. Most of the existing Prescriptive Process Monitoring solutions, however, while performing well in terms of recommendation reliability, provide the users with very specific (sequences of) activities that have to be executed without caring about the feasibility of these recommendations. In order to face this issue, we propose a new Outcome-Oriented Prescriptive Process Monitoring system recommending temporal relations between activities that have to be guaranteed during the process execution in order to achieve a desired outcome. This softens the mandat
    
[^110]: 基于SPD流形的图神经网络用于运动想象分类：来自时频分析的视角

    Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis. (arXiv:2211.02641v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.02641](http://arxiv.org/abs/2211.02641)

    本文介绍了一种基于SPD流形的图神经网络用于运动想象分类，利用EEG的二阶统计量，相比传统方法具有更好的性能。

    This paper introduces a graph neural network based on SPD manifolds for motor imagery classification, which utilizes second-order statistics of EEG signals and outperforms traditional methods.

    运动想象（MI）的分类是脑电图（EEG）基础脑机接口（BCI）领域中备受追捧的研究课题，具有巨大的商业价值。过去二十年，MI-EEG分类器的趋势发生了根本性的转变，其性能逐渐提高。 Tensor-CSPNet的出现是BCI研究中第一个几何深度学习（GDL）框架的必要性，其归因于信号的非欧几里德性质的特征化。从根本上讲，Tensor-CSPNet是一种基于深度学习的分类器，利用EEG的二阶统计量。与利用EEG信号的一阶统计量的传统方法相比，利用这些二阶统计量代表了经典的处理方法。这些统计量提供了足够的区分信息，使它们适用于MI-EEG分类。在本研究中，我们介绍了另一种GDL分类器，

    The classification of motor imagery (MI) is a highly sought-after research topic in the field of Electroencephalography (EEG)-based brain-computer interfaces (BCIs), with immense commercial value. Over the past two decades, there has been a fundamental shift in the trend of MI-EEG classifiers, resulting in a gradual increase in their performance. The emergence of Tensor-CSPNet, the first geometric deep learning (GDL) framework in BCI research, is attributed to the imperative of characterizing the non-Euclidean nature of signals. Fundamentally, Tensor-CSPNet is a deep learning-based classifier that capitalizes on the second-order statistics of EEGs. In contrast to the conventional approach of utilizing first-order statistics for EEG signals, the utilization of these second-order statistics represents the classical treatment. These statistics provide adequate discriminative information, rendering them suitable for MI-EEG classification. In this study, we introduce another GDL classifier,
    
[^111]: FS-DETR: 带提示和无需重新训练的少样本检测变形器

    FS-DETR: Few-Shot DEtection TRansformer with prompting and without re-training. (arXiv:2210.04845v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.04845](http://arxiv.org/abs/2210.04845)

    本文介绍了一种名为FS-DETR的少样本检测变形器，它基于视觉提示的方法实现了在少样本情况下进行目标检测的能力，且无需在测试时进行重新训练。

    

    本文研究少样本目标检测（FSOD），即在给定少量未在训练中见过的模板（示例）的情况下，目标是在一组图像中检测出该类别的所有实例。从实际角度考虑，FSOD系统必须满足以下要求：（a）在测试时不需要任何微调，即可以原样使用；（b）能够同时处理任意数量的新类别对象，并支持每个类别的任意数量的示例；（c）能够达到与封闭系统相当的准确性。为了满足（a）-（c），在本文中我们做出了以下贡献：首次引入了一种简单而强大的少样本检测变形器（FS-DETR），基于视觉提示实现了对（a）和（b）的处理。我们的系统基于DETR框架进行扩展，基于两个关键思想：（1）在测试过程中使用提供的新类别视觉模板作为视觉提示。

    This paper is on Few-Shot Object Detection (FSOD), where given a few templates (examples) depicting a novel class (not seen during training), the goal is to detect all of its occurrences within a set of images. From a practical perspective, an FSOD system must fulfil the following desiderata: (a) it must be used as is, without requiring any fine-tuning at test time, (b) it must be able to process an arbitrary number of novel objects concurrently while supporting an arbitrary number of examples from each class and (c) it must achieve accuracy comparable to a closed system. Towards satisfying (a)-(c), in this work, we make the following contributions: We introduce, for the first time, a simple, yet powerful, few-shot detection transformer (FS-DETR) based on visual prompting that can address both desiderata (a) and (b). Our system builds upon the DETR framework, extending it based on two key ideas: (1) feed the provided visual templates of the novel classes as visual prompts during test t
    
[^112]: 基于贝叶斯的提示学习用于图像-语言模型泛化

    Bayesian Prompt Learning for Image-Language Model Generalization. (arXiv:2210.02390v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.02390](http://arxiv.org/abs/2210.02390)

    本文提出了一种基于贝叶斯方法的提示学习框架，对提示空间进行正则化，提高了对未见提示的泛化能力。

    

    基础的图像-语言模型因其高效的适应下游任务的提示学习而引起了广泛关注。提示学习将语言模型输入的一部分视为可训练的，同时冻结其余部分，并优化经验风险最小化目标。然而，经验风险最小化已知受到分布偏移的影响，这影响了对训练过程中未见提示的泛化能力。通过利用贝叶斯方法的正则化能力，我们从贝叶斯角度考虑提示学习，并将其制定为变分推断问题。我们的方法对提示空间进行正则化，减少对已见提示的过度拟合，并提高了对未见提示的提示泛化能力。我们的框架通过以概率的方式对输入提示空间进行建模，作为先验分布，使我们的提议与基于图像无条件或有条件的提示学习方法兼容。我们进行了实验证明了本提出的方法的有效性。

    Foundational image-language models have generated considerable interest due to their efficient adaptation to downstream tasks by prompt learning. Prompt learning treats part of the language model input as trainable while freezing the rest, and optimizes an Empirical Risk Minimization objective. However, Empirical Risk Minimization is known to suffer from distributional shifts which hurt generalizability to prompts unseen during training. By leveraging the regularization ability of Bayesian methods, we frame prompt learning from the Bayesian perspective and formulate it as a variational inference problem. Our approach regularizes the prompt space, reduces overfitting to the seen prompts and improves the prompt generalization on unseen prompts. Our framework is implemented by modeling the input prompt space in a probabilistic manner, as an a priori distribution which makes our proposal compatible with prompt learning approaches that are unconditional or conditional on the image. We demon
    
[^113]: 参数修剪的数据集蒸馏方法

    Dataset Distillation Using Parameter Pruning. (arXiv:2209.14609v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.14609](http://arxiv.org/abs/2209.14609)

    本文提出了一种使用参数修剪的数据集蒸馏方法，该方法可以在蒸馏过程中修剪难以匹配的参数，提高蒸馏性能。

    

    在许多领域中，获得先进模型的方法取决于大型数据集，这使得数据存储和模型训练变得昂贵。作为解决方案，数据集蒸馏可以合成保留原始大型数据集大多数信息的小型数据集。最近提出的匹配网络参数的数据集蒸馏方法已被证明在几个数据集上有效。然而，网络参数的维度通常很大。此外，一些参数在蒸馏过程中难以匹配，降低了蒸馏性能。基于这个观察，本研究提出了一种基于参数修剪的新型数据集蒸馏方法来解决这个问题。该方法可以在蒸馏过程中修剪难以匹配的参数，从而合成更加稳健的蒸馏数据集并提高蒸馏性能。在三个数据集上的实验结果表明，该方法优于其他最先进的数据集蒸馏方法。

    In many fields, the acquisition of advanced models depends on large datasets, making data storage and model training expensive. As a solution, dataset distillation can synthesize a small dataset that preserves most information of the original large dataset. The recently proposed dataset distillation method by matching network parameters has been proven effective for several datasets. However, the dimensions of network parameters are typically large. Furthermore, some parameters are difficult to match during the distillation process, degrading distillation performance. Based on this observation, this study proposes a novel dataset distillation method based on parameter pruning that solves the problem. The proposed method can synthesize more robust distilled datasets and improve distillation performance by pruning difficult-to-match parameters during the distillation process. Experimental results on three datasets show that the proposed method outperforms other state-of-the-art dataset d
    
[^114]: FiBiNet++：通过低秩特征交互层减小CTR预测模型的大小

    FiBiNet++: Reducing Model Size by Low Rank Feature Interaction Layer for CTR Prediction. (arXiv:2209.05016v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2209.05016](http://arxiv.org/abs/2209.05016)

    FiBiNet++通过引入低秩特征交互层，成功减小了FiBiNet模型大小，并在三个公共数据集上实现了12倍至16倍的参数减小，同时显著提高了性能。

    

    点击率（CTR）估计已经成为许多现实世界应用中最基本的任务之一，并且已经提出了各种深度模型。一些研究证明FiBiNet是表现最佳的模型之一，并且在Avazu数据集上胜过了所有其他模型。然而，FiBiNet的大型模型大小限制了它的广泛应用。在本文中，我们提出了一种新的FiBiNet++模型来重新设计FiBiNet的模型结构，从而大大减小了模型大小，并进一步提高了性能。其中一个主要技术是我们提出的“低秩层”，专注于特征交互，它作为实现模型优越压缩比的关键驱动器。在三个公共数据集上进行的广泛实验证明，FiBiNet++有效地将FiBiNet的非嵌入式模型参数在三个数据集上减小了12倍至16倍。另一方面，与最先进的CTR方法（包括FiBiNet）相比，FiBiNet++能显著提高性能。

    Click-Through Rate (CTR) estimation has become one of the most fundamental tasks in many real-world applications and various deep models have been proposed. Some research has proved that FiBiNet is one of the best performance models and outperforms all other models on Avazu dataset. However, the large model size of FiBiNet hinders its wider application. In this paper, we propose a novel FiBiNet++ model to redesign FiBiNet's model structure, which greatly reduces model size while further improves its performance. One of the primary techniques involves our proposed "Low Rank Layer" focused on feature interaction, which serves as a crucial driver of achieving a superior compression ratio for models. Extensive experiments on three public datasets show that FiBiNet++ effectively reduces non-embedding model parameters of FiBiNet by 12x to 16x on three datasets. On the other hand, FiBiNet++ leads to significant performance improvements compared to state-of-the-art CTR methods, including FiBiN
    
[^115]: 多点-BAX: 一种通过虚拟目标高效调整粒子加速器发射度的新方法

    Multipoint-BAX: A New Approach for Efficiently Tuning Particle Accelerator Emittance via Virtual Objectives. (arXiv:2209.04587v4 [physics.acc-ph] UPDATED)

    [http://arxiv.org/abs/2209.04587](http://arxiv.org/abs/2209.04587)

    本论文提出了一种名为多点-BAX的新方法，通过虚拟目标来高效调整粒子加速器的发射度。该方法避免了使用传统的黑盒优化器进行缓慢而低效的多点查询，并通过快速学习模型计算发射度目标。该方法在Linac相干光源(LCLS)和Facility for Adv中最小化发射度。

    

    尽管束发射度对于高亮度加速器的性能至关重要，但优化通常会受到时间限制，因为发射度计算通常是通过四极扫描完成的，而四极扫描通常较慢。这种计算是一种多点查询，即每个查询都需要多个辅助测量。传统的黑盒优化器，如贝叶斯优化，在处理这样的目标时速度慢且效率低下，因为它们必须获取完整的测量序列，但每个查询仅返回发射度。我们提出将贝叶斯算法执行(BAX)应用于查询和建模单个束流尺寸测量。BAX通过使用快速学习模型而不是直接从加速器中获取发射度指标来避免在加速器上进行缓慢的多点查询。在这里，我们使用BAX来最小化Linac相干光源(LCLS)和Facility for Adv的发射度。

    Although beam emittance is critical for the performance of high-brightness accelerators, optimization is often time limited as emittance calculations, commonly done via quadrupole scans, are typically slow. Such calculations are a type of $\textit{multi-point query}$, i.e. each query requires multiple secondary measurements. Traditional black-box optimizers such as Bayesian optimization are slow and inefficient when dealing with such objectives as they must acquire the full series of measurements, but return only the emittance, with each query. We propose applying Bayesian Algorithm Execution (BAX) to instead query and model individual beam-size measurements. BAX avoids the slow multi-point query on the accelerator by acquiring points through a $\textit{virtual objective}$, i.e. calculating the emittance objective from a fast learned model rather than directly from the accelerator. Here, we use BAX to minimize emittance at the Linac Coherent Light Source (LCLS) and the Facility for Adv
    
[^116]: 神经营销中的数据融合：生物信号、生命周期阶段、当前进展、数据集、趋势和挑战的多模态分析

    Data Fusion in Neuromarketing: Multimodal Analysis of Biosignals, Lifecycle Stages, Current Advances, Datasets, Trends, and Challenges. (arXiv:2209.00993v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2209.00993](http://arxiv.org/abs/2209.00993)

    神经营销研究中的数据融合为实现特定目标提供了新的途径，并且需要先进的处理方法。

    

    任何公司的首要目标是通过提高产品的质量和广告方式来增加利润。在这个背景下，神经营销旨在提升产品的推广并在潜在买家中产生更大的认可度。传统上，神经营销研究依赖于单一生物信号来获得对呈现的刺激的反馈。然而，由于新设备和技术进步，近期趋势表明在这一领域的研究中出现了多种生物信号的融合。一个例子是使用脑电图来理解广告在神经水平上的影响，利用视觉跟踪来识别导致这种影响的刺激。这种新兴模式决定了为实现特定神经营销目标而使用哪些生物信号。此外，多源数据的融合需要先进的处理方法。尽管存在这些复杂性，但缺乏相关文献。

    The primary goal of any company is to increase its profits by improving both the quality of its products and how they are advertised. In this context, neuromarketing seeks to enhance the promotion of products and generate a greater acceptance on potential buyers. Traditionally, neuromarketing studies have relied on a single biosignal to obtain feedback from presented stimuli. However, thanks to new devices and technological advances studying this area of knowledge, recent trends indicate a shift towards the fusion of diverse biosignals. An example is the usage of electroencephalography for understanding the impact of an advertisement at the neural level and visual tracking to identify the stimuli that induce such impacts. This emerging pattern determines which biosignals to employ for achieving specific neuromarketing objectives. Furthermore, the fusion of data from multiple sources demands advanced processing methodologies. Despite these complexities, there is a lack of literature tha
    
[^117]: 一些监督是必须的：通过认知不确定性度量在强化学习中引入神谕策略

    Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics. (arXiv:2208.10533v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.10533](http://arxiv.org/abs/2208.10533)

    本文提出一种名为评判置信度引导探索的方法，用于将现有的神谕策略纳入标准的演员-评论家强化学习算法中，以提高探索效率。在不确定性高时，该方法会将神谕策略的行动作为建议纳入学习方案中，而在不确定性低时忽略它。

    

    强化学习的固有问题是通过随机行动探索环境，其中很大一部分可能是无效的。相反，可以通过使用现有的（先前学习的或硬编码的）神谕策略、离线数据或演示来改善探索。但在使用神谕策略的情况下，如何最大化学习样本效率地将神谕经验融入到学习策略中可能不清楚。本文提出了一种名为评判置信度引导探索（Critic Confidence Guided Exploration，CCGE）的方法，用于将这样的神谕策略纳入标准的演员-评论家强化学习算法中。具体而言，当不确定性高时，CCGE以神谕策略的行动为建议，并将此信息纳入学习方案中，而当不确定性低时忽略它。CCGE对不确定性估计方法不加区分，并且我们证明它与现有算法相当。

    An inherent problem of reinforcement learning is performing exploration of an environment through random actions, of which a large portion can be unproductive. Instead, exploration can be improved by initializing the learning policy with an existing (previously learned or hard-coded) oracle policy, offline data, or demonstrations. In the case of using an oracle policy, it can be unclear how best to incorporate the oracle policy's experience into the learning policy in a way that maximizes learning sample efficiency. In this paper, we propose a method termed Critic Confidence Guided Exploration (CCGE) for incorporating such an oracle policy into standard actor-critic reinforcement learning algorithms. More specifically, CCGE takes in the oracle policy's actions as suggestions and incorporates this information into the learning scheme when uncertainty is high, while ignoring it when the uncertainty is low. CCGE is agnostic to methods of estimating uncertainty, and we show that it is equa
    
[^118]: Topical: 使用Attention从源代码中学习存储库嵌入

    Topical: Learning Repository Embeddings from Source Code using Attention. (arXiv:2208.09495v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2208.09495](http://arxiv.org/abs/2208.09495)

    这篇论文介绍了Topical，一种使用深度神经网络从源代码中学习存储库级嵌入的方法，以实现代码自动生成、代码推荐和代码自动标记等功能。

    

    源代码的机器学习(MLOnCode)承诺改变软件交付的方式。通过挖掘软件工件之间的上下文和关系，MLOnCode通过代码自动生成、代码推荐、代码自动标记和其他数据驱动增强来增强软件开发人员的能力。对于许多任务来说，代码的脚本级表示已经足够，然而，在许多情况下，考虑到各种依赖关系和存储库结构的存储库级表示是必要的，例如，为存储库自动标记主题或自动记录存储库代码等。现有的计算存储库级表示的方法存在以下问题：(a) 依赖于代码的自然语言文档(例如README文件)；(b) 通过串联或平均等方法对方法/脚本级表示进行简单聚合。本文介绍了Topical，一种用于生成公共存储库级嵌入的深度神经网络。

    Machine learning on source code (MLOnCode) promises to transform how software is delivered. By mining the context and relationship between software artefacts, MLOnCode augments the software developers capabilities with code auto-generation, code recommendation, code auto-tagging and other data-driven enhancements. For many of these tasks a script level representation of code is sufficient, however, in many cases a repository level representation that takes into account various dependencies and repository structure is imperative, for example, auto-tagging repositories with topics or auto-documentation of repository code etc. Existing methods for computing repository level representations suffer from (a) reliance on natural language documentation of code (for example, README files) (b) naive aggregation of method/script-level representation, for example, by concatenation or averaging. This paper introduces Topical a deep neural network to generate repository level embeddings of publicly 
    
[^119]: 高维空间中可微归纳逻辑编程

    Differentiable Inductive Logic Programming in High-Dimensional Space. (arXiv:2208.06652v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.06652](http://arxiv.org/abs/2208.06652)

    本研究提出了一种在高维空间中进行可微归纳逻辑编程的扩展方法，通过大规模谓词发明来充分利用高维梯度下降的效能，以学习超出现有神经符号ILP系统能力的任务的解决方案。

    

    通过符号归纳逻辑编程（ILP）合成大型逻辑程序通常需要中间定义。然而，使用内涵谓词杂乱地占据假设空间通常会降低性能。相反，梯度下降提供了在这些高维空间中寻找解决方案的有效方法。到目前为止，神经符号ILP方法并没有充分利用这一点。我们提出扩展{\delta}ILP方法，以进行大规模谓词发明的归纳合成，从而允许我们利用高维梯度下降的效能。我们展示了大规模谓词发明通过梯度下降受益于可微归纳合成，并允许我们学习超出现有神经符号ILP系统能力的任务的解决方案。此外，我们在不指定解决方案的精确结构的语言偏差的情况下实现了这些结果。

    Synthesizing large logic programs through symbolic Inductive Logic Programming (ILP) typically requires intermediate definitions. However, cluttering the hypothesis space with intensional predicates typically degrades performance. In contrast, gradient descent provides an efficient way to find solutions within such high- dimensional spaces. Neuro-symbolic ILP approaches have not fully exploited this so far. We propose extending the {\delta}ILP approach to inductive synthesis with large-scale predicate invention, thus allowing us to exploit the efficacy of high-dimensional gradient descent. We show that large-scale predicate invention benefits differentiable inductive synthesis through gradient descent and allows one to learn solutions for tasks beyond the capabilities of existing neuro-symbolic ILP systems. Furthermore, we achieve these results without specifying the precise structure of the solution within the language bias.
    
[^120]: 走向透明AI: 对深度神经网络内部结构的解释的调查

    Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.13243](http://arxiv.org/abs/2207.13243)

    这篇综述调查了深度神经网络内部结构内部解释方法，并提出了一种分析方法的分类。这些解释方法对于帮助构建更可信赖的AI是至关重要的。

    

    过去十年的机器学习取得了巨大的规模和能力的增长，深度神经网络(DNNs)越来越多地被部署在现实世界中。然而，它们很难分析，这引发了对在不彻底理解其工作原理的情况下使用它们的担忧。解释它们的有效工具将对构建更可信赖的AI非常重要，通过帮助识别问题、修复错误和增进基本理解。特别是，"内部"可解释性技术，它们专注于解释DNNs的内部组件，非常适合于开发机械理解、指导手动修改和逆向工程解决方案。最近的研究主要集中在DNN可解释性上，迅速取得的进展使得对方法进行彻底系统化的困难。在这篇调查中，我们回顾了300多篇作品，重点关注内部可解释性工具。我们引入了一种分类方法，将方法按网络的哪个部分进行分类。

    The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, "inner" interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions.  Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the netwo
    
[^121]: 差分隐私下的部分集覆盖及其在设施选址中的应用

    Differentially Private Partial Set Cover with Applications to Facility Location. (arXiv:2207.10240v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2207.10240](http://arxiv.org/abs/2207.10240)

    本文提出了一个差分隐私算法，用于解决部分集覆盖问题和设施选址问题，该算法在给定输入集合系统的松弛条件下能够输出具有非平凡近似保证的显式集覆盖。

    

    在差分隐私下，观察到集覆盖问题存在强困难结果。本文中，我们观察到当我们转向部分集覆盖问题时，这些困难结果消失，其中我们只需要覆盖宇宙中的一部分元素$\rho$（$\rho \in (0,1)$）。我们展示了在输入集合系统的松弛条件下，我们给出了差分隐私算法，其输出具有非平凡近似保证的显式集覆盖。特别地，这是首个输出显式集覆盖的差分隐私算法。利用我们的部分集覆盖算法作为子程式，我们给出了一个差分隐私（双目标）近似算法，适用于广义的$k$-中心/$k$-供应问题，包括离群点。与集覆盖问题一样，目前无法给出非平凡的保证的算法。

    It was observed in \citet{gupta2009differentially} that the Set Cover problem has strong impossibility results under differential privacy. In our work, we observe that these hardness results dissolve when we turn to the Partial Set Cover problem, where we only need to cover a $\rho$-fraction of the elements in the universe, for some $\rho\in(0,1)$. We show that this relaxation enables us to avoid the impossibility results: under loose conditions on the input set system, we give differentially private algorithms which output an explicit set cover with non-trivial approximation guarantees. In particular, this is the first differentially private algorithm which outputs an explicit set cover.  Using our algorithm for Partial Set Cover as a subroutine, we give a differentially private (bicriteria) approximation algorithm for a facility location problem which generalizes $k$-center/$k$-supplier with outliers. Like with the Set Cover problem, no algorithm has been able to give non-trivial gua
    
[^122]: 关于信任度规范化的研究

    On Specifying for Trustworthiness. (arXiv:2206.11421v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.11421](http://arxiv.org/abs/2206.11421)

    本论文识别了自主系统信任度规范化的关键挑战，跨越了不同领域的AS的弹性、信任度、功能、可验证性、安全性以及治理和监管，并强调了与环境不确定性相关的思维挑战。

    

    随着自主系统（AS）越来越成为我们日常生活的一部分，确保其信任度至关重要。为了证明AS的信任度，我们首先需要指定AS被认为是可信的所需的条件。这篇路线图论文在英国研究和创新（UKRI）可信任自主系统（TAS）计划的一部分举办的“指定信任度”研讨会中，确定了在AS中为信任度指定的关键挑战。我们跨AS领域展望，考虑AS的弹性、信任度、功能、可验证性、安全性以及治理和监管，并确定了这些领域中的一些关键规范挑战。然后，我们重点介绍了在AS中为信任度指定所涉及的跨领域的思维挑战，并剖析了AS需要运行的环境所固有的不确定性带来的复杂性。

    As autonomous systems (AS) increasingly become part of our daily lives, ensuring their trustworthiness is crucial. In order to demonstrate the trustworthiness of an AS, we first need to specify what is required for an AS to be considered trustworthy. This roadmap paper identifies key challenges for specifying for trustworthiness in AS, as identified during the "Specifying for Trustworthiness" workshop held as part of the UK Research and Innovation (UKRI) Trustworthy Autonomous Systems (TAS) programme. We look across a range of AS domains with consideration of the resilience, trust, functionality, verifiability, security, and governance and regulation of AS and identify some of the key specification challenges in these domains. We then highlight the intellectual challenges that are involved with specifying for trustworthiness in AS that cut across domains and are exacerbated by the inherent uncertainty involved with the environments in which AS need to operate.
    
[^123]: 用于黑盒优化的生成预训练

    Generative Pretraining for Black-Box Optimization. (arXiv:2206.10786v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10786](http://arxiv.org/abs/2206.10786)

    该论文提出了一种使用离线数据预训练黑盒优化器的生成框架BONET，使用自回归模型和样本策略合成轨迹以帮助在高维空间中优化昂贵的黑盒函数。

    

    科学和工程中的许多问题涉及在高维空间中优化昂贵的黑盒函数。对于这样的黑盒优化 (BBO) 问题，我们通常假设在线函数评估的预算很小，但往往可以访问用于预训练的固定离线数据集。之前的方法试图利用离线数据来逼近函数或其反函数，但在离数据分布较远时不够精确。我们提出了BONET，这是一个利用离线数据集预训练黑盒优化器的生成框架。在BONET中，我们对来自离线数据集的定长轨迹训练一个自回归模型。我们设计了一种采样策略，使用从低保真度样本到高保真度样本的单调转换的简单启发式来合成来自离线数据的轨迹。在Design-Bench上使用被因果掩蔽的Transformer实例化BONET，并进行评估，我们在平均排名上排名第一。

    Many problems in science and engineering involve optimizing an expensive black-box function over a high-dimensional space. For such black-box optimization (BBO) problems, we typically assume a small budget for online function evaluations, but also often have access to a fixed, offline dataset for pretraining. Prior approaches seek to utilize the offline data to approximate the function or its inverse but are not sufficiently accurate far from the data distribution. We propose BONET, a generative framework for pretraining a novel black-box optimizer using offline datasets. In BONET, we train an autoregressive model on fixed-length trajectories derived from an offline dataset. We design a sampling strategy to synthesize trajectories from offline data using a simple heuristic of rolling out monotonic transitions from low-fidelity to high-fidelity samples. Empirically, we instantiate BONET using a causally masked Transformer and evaluate it on Design-Bench, where we rank the best on averag
    
[^124]: 多项式时间算法在计数和采样马尔可夫等价的有向无环图中的应用

    Polynomial-Time Algorithms for Counting and Sampling Markov Equivalent DAGs with Applications. (arXiv:2205.02654v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.02654](http://arxiv.org/abs/2205.02654)

    本文提出了一种多项式时间算法，用于在马尔可夫等价类中计数和采样有向无环图。该算法解决了这一领域的长期未解决问题，并且在实验中得到验证，可以在活跃学习因果结构和因果效应识别方面实际应用。

    

    在图形因果分析中，从马尔可夫等价类中计数和采样有向无环图是基本任务。本文展示了这些任务可以在多项式时间内完成，解决了这一领域的长期未解决问题。我们的算法有效且易于实现。正如我们在实验中展示的那样，这些突破使得在活跃学习因果结构和因果效应识别方面，对于马尔可夫等价类，原本认为不可行的策略实际可应用。

    Counting and sampling directed acyclic graphs from a Markov equivalence class are fundamental tasks in graphical causal analysis. In this paper we show that these tasks can be performed in polynomial time, solving a long-standing open problem in this area. Our algorithms are effective and easily implementable. As we show in experiments, these breakthroughs make thought-to-be-infeasible strategies in active learning of causal structures and causal effect identification with regard to a Markov equivalence class practically applicable.
    
[^125]: 可疑物体的重要性：重新思考一阶视觉定位模型的预测

    Suspected Object Matters: Rethinking Model's Prediction for One-stage Visual Grounding. (arXiv:2203.05186v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.05186](http://arxiv.org/abs/2203.05186)

    该论文研究了一阶视觉定位方法中物体之间关系建模的问题，提出了一种可疑物体转换机制（SOT），通过对可疑物体进行转换和关系建模，提高了模型的预测准确性。

    

    最近，由于其与两阶段定位方法相比具有可比较的准确性但显著更高的效率，一阶视觉定位方法受到了高度关注。然而，对于一阶视觉定位方法来说，物体之间的关系建模还没有得到很好的研究。虽然物体之间的关系建模很重要，但不一定需要在所有物体之间执行，因为只有其中一部分与文本查询相关，可能会困惑模型。我们称这些物体为可疑物体。然而，在一阶视觉定位方法中探索它们之间的关系并不容易，因为：首先，没有物体提议作为选择可疑物体和进行关系建模的基础。第二，可疑物体比其他物体更容易困惑，因为它们可能具有类似的语义，与特定关系纠缠在一起等，从而更容易误导模型的预测。为此，我们提出了一种可疑物体转换机制（SOT），它可以无缝地集成到现有的模型中。

    Recently, one-stage visual grounders attract high attention due to their comparable accuracy but significantly higher efficiency than two-stage grounders. However, inter-object relation modeling has not been well studied for one-stage grounders. Inter-object relationship modeling, though important, is not necessarily performed among all objects, as only part of them are related to the text query and may confuse the model. We call these objects suspected objects. However, exploring their relationships in the one-stage paradigm is non-trivial because: First, no object proposals are available as the basis on which to select suspected objects and perform relationship modeling. Second, suspected objects are more confusing than others, as they may share similar semantics, be entangled with certain relationships, etc, and thereby more easily mislead the model prediction. Toward this end, we propose a Suspected Object Transformation mechanism (SOT), which can be seamlessly integrated into exis
    
[^126]: 无监督双关节分析与共现线索的多模态单词发现

    Unsupervised Multimodal Word Discovery based on Double Articulation Analysis with Co-occurrence cues. (arXiv:2201.06786v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2201.06786](http://arxiv.org/abs/2201.06786)

    本研究提出了一种无监督学习方法，基于语音学信息和对象信息，用于从语音信号中发现单词和音素，并同时利用多种模态的对象信息。实验结果表明，该方法在单词发现性能上优于基线方法。

    

    人类婴儿在没有太多语言先验知识的情况下，通过声音分布的统计特性和其他感官刺激的共现来获取其语言词汇。本研究提出了一种基于语音学信息和对象信息的全新无监督学习方法，用于发现语音单元。该方法可以利用无监督学习从语音信号中获得单词和音素，并同时利用基于多种模态的对象信息，包括视觉、触觉和听觉。该方法基于非参数贝叶斯双关节分析器（NPB-DAA）从语音学特征中发现音素和单词，并且基于多模态潜在狄利克雷分配（MLDA）对从对象中获取的多模态信息进行分类。实验结果显示，该方法比基线方法具有更高的单词发现性能。表达了特性的单词包含了...

    Human infants acquire their verbal lexicon with minimal prior knowledge of language based on the statistical properties of phonological distributions and the co-occurrence of other sensory stimuli. This study proposes a novel fully unsupervised learning method for discovering speech units using phonological information as a distributional cue and object information as a co-occurrence cue. The proposed method can acquire words and phonemes from speech signals using unsupervised learning and utilize object information based on multiple modalities-vision, tactile, and auditory-simultaneously. The proposed method is based on the nonparametric Bayesian double articulation analyzer (NPB-DAA) discovering phonemes and words from phonological features, and multimodal latent Dirichlet allocation (MLDA) categorizing multimodal information obtained from objects. In an experiment, the proposed method showed higher word discovery performance than baseline methods. Words that expressed the characteri
    
[^127]: 安全均衡

    Safe Equilibrium. (arXiv:2201.04266v10 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2201.04266](http://arxiv.org/abs/2201.04266)

    这项研究提出了一种新的解决概念，安全均衡，该概念模拟了对手有一定概率进行理性行为，有剩余概率进行任意行为。研究证明在所有战略博弈中都存在安全均衡，并提供了计算安全均衡的算法。

    

    标准的博弈论解决概念纳什均衡假设所有玩家都会理性行为。如果我们按照纳什均衡进行决策而对手却是非理性的（或者按照另外一个纳什均衡的策略行动），那么我们可能会得到非常低的回报。另一方面，最大最小策略假设所有对手都在试图最小化我们的回报（即使这并不符合他们最佳利益），并确保了最差情况下的最大可能回报，但结果是过于保守的行动。我们提出了一个新的解决概念，称为安全均衡，它将对手建模为以指定的概率进行理性行为，而用剩余的概率进行任意行为。我们证明了在所有形式的战略博弈中都存在安全均衡（对于所有可能的理性参数值），并证明了其计算是PPAD-hard的。我们提供了在2人和n人博弈中计算安全均衡的精确算法，以及可扩展性算法。

    The standard game-theoretic solution concept, Nash equilibrium, assumes that all players behave rationally. If we follow a Nash equilibrium and opponents are irrational (or follow strategies from a different Nash equilibrium), then we may obtain an extremely low payoff. On the other hand, a maximin strategy assumes that all opposing agents are playing to minimize our payoff (even if it is not in their best interest), and ensures the maximal possible worst-case payoff, but results in exceedingly conservative play. We propose a new solution concept called safe equilibrium that models opponents as behaving rationally with a specified probability and behaving potentially arbitrarily with the remaining probability. We prove that a safe equilibrium exists in all strategic-form games (for all possible values of the rationality parameters), and prove that its computation is PPAD-hard. We present exact algorithms for computing a safe equilibrium in both 2 and $n$-player games, as well as scalab
    
[^128]: CausalAF: 用于安全关键驾驶场景生成的因果自回归流

    CausalAF: Causal Autoregressive Flow for Safety-Critical Driving Scenario Generation. (arXiv:2110.13939v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2110.13939](http://arxiv.org/abs/2110.13939)

    CausalAF是一种用于安全关键驾驶场景生成的流模型，通过引入因果关系先验和新颖的因果掩蔽操作，将生成模型从仅从数据中学习相关性的情况扩展到学习生成场景如何引起风险情况的因果机制，以提高系统鲁棒性评估的多样性和效率。

    

    生成关键安全场景是评估自动驾驶系统鲁棒性的有效方式，但是场景的多样性和生成方法的效率受限于关键安全场景的稀缺性和结构。因此，仅从观测数据估计分布的现有生成模型不能很好地解决这个问题。在本文中，我们将因果关系作为先验融入到场景生成中，提出了一种基于流的生成框架——因果自回归流（CausalAF）。CausalAF通过新颖的因果掩蔽操作，鼓励生成模型揭示和遵循生成对象之间的因果关系，而不仅仅是从观测数据中随机采样。通过学习生成场景如何引起风险情况的因果机制，而不仅仅是从数据中学习相关性，CausalAF显著提高了生成模型的能力。

    Generating safety-critical scenarios, which are crucial yet difficult to collect, provides an effective way to evaluate the robustness of autonomous driving systems. However, the diversity of scenarios and efficiency of generation methods are heavily restricted by the rareness and structure of safety-critical scenarios. Therefore, existing generative models that only estimate distributions from observational data are not satisfying to solve this problem. In this paper, we integrate causality as a prior into the scenario generation and propose a flow-based generative framework, Causal Autoregressive Flow (CausalAF). CausalAF encourages the generative model to uncover and follow the causal relationship among generated objects via novel causal masking operations instead of searching the sample only from observational data. By learning the cause-and-effect mechanism of how the generated scenario causes risk situations rather than just learning correlations from data, CausalAF significantly
    
[^129]: 《UAV遥感中深度学习的综述》

    A Review on Deep Learning in UAV Remote Sensing. (arXiv:2101.10861v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2101.10861](http://arxiv.org/abs/2101.10861)

    该综述总结了近年来应用于UAV遥感中的深度学习算法，主要关注分类和回归技术。

    

    深度神经网络（DNN）具有从数据中学习表征的出色能力，并在图像、时间序列、自然语言、音频、视频等处理方面取得了重大突破。在遥感领域，已经进行了有关DNN算法应用的调查和文献综述，试图总结其子领域中产生的大量信息。最近，基于无人机的应用在航空感知研究中占主导地位。然而，尚未进行将“深度学习”和“UAV遥感”两个主题结合起来的文献综述。我们的工作动机是对应用于基于UAV成像的深度学习（DL）的基本原理进行综合评述。我们主要关注描述用于最近UAV获取数据的分类和回归技术。为此，共筛选了232篇发表在国际科学期刊资料中的论文。

    Deep Neural Networks (DNNs) learn representation from data with an impressive capability, and brought important breakthroughs for processing images, time-series, natural language, audio, video, and many others. In the remote sensing field, surveys and literature revisions specifically involving DNNs algorithms' applications have been conducted in an attempt to summarize the amount of information produced in its subfields. Recently, Unmanned Aerial Vehicles (UAV) based applications have dominated aerial sensing research. However, a literature revision that combines both "deep learning" and "UAV remote sensing" thematics has not yet been conducted. The motivation for our work was to present a comprehensive review of the fundamentals of Deep Learning (DL) applied in UAV-based imagery. We focused mainly on describing classification and regression techniques used in recent applications with UAV-acquired data. For that, a total of 232 papers published in international scientific journal data
    
[^130]: 高频交易中的深度强化学习

    Deep Reinforcement Learning for Active High Frequency Trading. (arXiv:2101.07107v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2101.07107](http://arxiv.org/abs/2101.07107)

    本文介绍了一个基于深度强化学习的框架，用于在股票市场中进行活跃的高频交易。通过训练DRL代理来交易股票，并使用Proximal Policy Optimization算法进行优化。通过仅选择具有最大价格变动的训练样本来提高训练数据的信噪比。通过实验证明，代理能够创建对底层环境的动态表示，并能够识别偶尔出现的规律。

    

    我们介绍了第一个基于端到端深度强化学习（DRL）的框架，用于在股票市场中进行活跃的高频交易。我们训练DRL代理使用Proximal Policy Optimization算法来交易一单位的英特尔公司股票。训练是在连续三个月的高频限价委托簿数据上进行的，其中最后一个月是验证数据。为了最大化训练数据中的信噪比，我们通过仅选择具有最大价格变动的训练样本来组成后者。然后在接下来的一个月的数据上进行测试。使用顺序模型优化技术进行超参数调优。我们考虑了三种不同的状态特征化方式，它们在基于LOB的元特征上有所不同。通过分析代理在测试数据上的表现，我们认为代理能够创建对底层环境的动态表示。它们能够识别偶尔出现的规律。

    We introduce the first end-to-end Deep Reinforcement Learning (DRL) based framework for active high frequency trading in the stock market. We train DRL agents to trade one unit of Intel Corporation stock by employing the Proximal Policy Optimization algorithm. The training is performed on three contiguous months of high frequency Limit Order Book data, of which the last month constitutes the validation data. In order to maximise the signal to noise ratio in the training data, we compose the latter by only selecting training samples with largest price changes. The test is then carried out on the following month of data. Hyperparameters are tuned using the Sequential Model Based Optimization technique. We consider three different state characterizations, which differ in their LOB-based meta-features. Analysing the agents' performances on test data, we argue that the agents are able to create a dynamic representation of the underlying environment. They identify occasional regularities pre
    
[^131]: 一票否决权：用于低样本青光眼诊断的半监督学习

    One-Vote Veto: Semi-Supervised Learning for Low-Shot Glaucoma Diagnosis. (arXiv:2012.04841v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2012.04841](http://arxiv.org/abs/2012.04841)

    本文扩展了孪生网络，提出了一种可在有限标记数据和不平衡情况下进行低样本学习的训练方法，并引入了半监督学习策略，利用额外的未标记数据来提高准确性。

    

    卷积神经网络（CNN）是一种有前景的技术，用于自动诊断眼底图像中的青光眼，这些图像通常在眼科检查中获取。然而，CNN通常需要大量标记良好的数据进行训练，而在许多生物医学图像分类应用中可能无法获得足够的数据，特别是在罕见疾病或专家标记成本高昂的情况下。本文有两个贡献来解决这个问题：（1）它扩展了传统的孪生网络，引入一种在标记数据有限且不平衡的情况下进行小样本学习的训练方法；（2）它引入了一种新颖的半监督学习策略，利用额外的未标记训练数据来提高准确性。我们提出的多任务孪生网络（MTSN）可以使用任何骨干CNN，并且我们通过四个骨干CNN的实验证明，其在有限的训练数据上的准确性接近经过训练的骨干CNN的准确性。

    Convolutional neural networks (CNNs) are a promising technique for automated glaucoma diagnosis from images of the fundus, and these images are routinely acquired as part of an ophthalmic exam. Nevertheless, CNNs typically require a large amount of well-labeled data for training, which may not be available in many biomedical image classification applications, especially when diseases are rare and where labeling by experts is costly. This article makes two contributions to address this issue: (1) It extends the conventional Siamese network and introduces a training method for low-shot learning when labeled data are limited and imbalanced, and (2) it introduces a novel semi-supervised learning strategy that uses additional unlabeled training data to achieve greater accuracy. Our proposed multi-task Siamese network (MTSN) can employ any backbone CNN, and we demonstrate with four backbone CNNs that its accuracy with limited training data approaches the accuracy of backbone CNNs trained wit
    
[^132]: 深度学习的泛化问题

    Generalization in Deep Learning. (arXiv:1710.05468v8 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1710.05468](http://arxiv.org/abs/1710.05468)

    本文从理论上解释了为什么以及如何深度学习能够在容量大、复杂性高、可能存在算法不稳定性、非鲁棒性和尖锐极小值的情况下实现良好的泛化，提出了一些新的开放问题，并讨论了研究结果的局限性。

    

    本文从理论上解释了为什么以及如何深度学习能够在容量大、复杂性高、可能存在算法不稳定性、非鲁棒性和尖锐极小值的情况下实现良好的泛化，回应了文献中的一个开放问题。我们还讨论了提供深度学习非虚空泛化保证的方法。基于理论观察，我们提出了一些新的开放问题，并讨论了我们研究结果的局限性。

    This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. Based on theoretical observations, we propose new open problems and discuss the limitations of our results.
    
[^133]: DeepTransport: 学习时空依赖性进行交通情况预测

    DeepTransport: Learning Spatial-Temporal Dependency for Traffic Condition Forecasting. (arXiv:1709.09585v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/1709.09585](http://arxiv.org/abs/1709.09585)

    DeepTransport是一个利用CNN和RNN学习交通网络拓扑内时空交通信息的端到端框架，通过考虑周围位置的交通情况来解决效果衰减问题，并引入了注意力机制来对齐时空信息。他们还构建并发布了一个分辨率为5分钟的真实世界大型交通状况数据集。

    

    预测交通状况最近被探索作为缓解交通拥堵的一种方式。一些开创性的方法已经提出，基于目标位置及其相邻区域的交通观察，但由于缺乏道路拓扑的挖掘，它们的准确性有所限制。为了解决效果衰减问题，我们建议考虑周围位置（比邻接范围更广）的交通情况。我们提出了一种端到端的框架，称为DeepTransport，其中利用卷积神经网络（CNN）和循环神经网络（RNN）获取交通网络拓扑内的时空交通信息。此外，引入了注意力机制来对齐时空信息。此外，我们构建并发布了一个真实世界的大型交通状况数据集，分辨率为5分钟。我们对该数据集的实验表明，我们的方法捕捉到了时空关系的复杂性。

    Predicting traffic conditions has been recently explored as a way to relieve traffic congestion. Several pioneering approaches have been proposed based on traffic observations of the target location as well as its adjacent regions, but they obtain somewhat limited accuracy due to a lack of mining road topology. To address the effect attenuation problem, we suggest taking into account the traffic of surrounding locations(wider than the adjacent range). We propose an end-to-end framework called DeepTransport, in which Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) are utilized to obtain spatial-temporal traffic information within a transport network topology. In addition, an attention mechanism is introduced to align spatial and temporal information. Moreover, we constructed and released a real-world large traffic condition dataset with a 5-minute resolution. Our experiments on this dataset demonstrate our method captures the complex relationship in the temporal 
    

