# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Do large language models solve verbal analogies like children do?.](http://arxiv.org/abs/2310.20384) | 本文研究了大型语言模型是否像孩子一样通过联想来解决语言类比问题，实验证明荷兰语母语和多语言LLMs的表现与儿童相当，但当控制联想过程时，模型的性能下降1-2年。 |
| [^2] | [A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging.](http://arxiv.org/abs/2310.20381) | 本文对GPT-4V在医学影像中的多模态能力进行了全面研究和评估，发现其在生成描述性报告和医学VQA方面有潜力，但在某些评估指标上仍需改进。 |
| [^3] | [A Machine Learning-Based Framework for Clustering Residential Electricity Load Profiles to Enhance Demand Response Programs.](http://arxiv.org/abs/2310.20367) | 本文提出一种基于机器学习的框架，通过对住宅电力负荷轮廓进行聚类，增强需求响应计划。通过使用伦敦家庭的数据进行实证分析和多个评估指标，我们应用了四种聚类算法，并将问题重新定义为概率分类问题，利用可解释的人工智能（xAI）来增强解释性。 |
| [^4] | [Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory.](http://arxiv.org/abs/2310.20360) | 本书提供了对深度学习算法的数学介绍，包括不同的神经网络架构和优化算法，并涵盖了深度学习算法的理论方面。此外，还介绍了深度学习逼近偏微分方程的方法。希望对学生和科学家们有所帮助。 |
| [^5] | [Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model.](http://arxiv.org/abs/2310.20357) | 本论文提出了一种改进多模态大语言模型空间意识能力的方法，通过利用精确的物体空间位置信息指导模型，在用户查询中提供更准确的响应。 |
| [^6] | [Muscle volume quantification: guiding transformers with anatomical priors.](http://arxiv.org/abs/2310.20355) | 本研究提出了一种在三维磁共振图像中自动分割下肢18个肌肉的方法，以辅助形态测量分析。该方法基于混合架构，结合了卷积和视觉变压器模块，解决了肌肉组织无法分辨和轮廓难以检测的问题。 |
| [^7] | [Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand.](http://arxiv.org/abs/2310.20350) | 本文提出了一种结合形状完成和抓取预测的方法，实现了快速灵活的多指抓取。通过使用基于深度图像的形状完成模块和基于预测的抓取预测器，实现了在具有有限或无先验知识的情况下，对物体进行抓取的任务。 |
| [^8] | [Improving Entropy-Based Test-Time Adaptation from a Clustering View.](http://arxiv.org/abs/2310.20327) | 本文从聚类的角度解释了基于熵的测试时间自适应（EBTTA）方法，提出了一个迭代算法，并展示了对于EBTTA方法来说，熵损失会进一步增加最大的概率，从而为其在聚类任务上的较好性能提供了一个替代性解释。 |
| [^9] | [SemanticBoost: Elevating Motion Generation with Augmented Textual Cues.](http://arxiv.org/abs/2310.20323) | SemanticBoost是一个新颖的框架，通过增强文本提示和上下文信息来提升运动生成，解决了从复杂语义描述中生成运动的困难。 |
| [^10] | [Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests.](http://arxiv.org/abs/2310.20320) | 本研究通过将11种基础模型和调整指令的大型语言模型（LLMs）与7-10岁儿童在高级测试中进行比较，发现GPT系列的调整指令LLMs表现最佳，并且在某些任务上超过了儿童的表现。此外，基础LLMs大多无法解决ToM任务，而调整指令则通过奖励合作性沟通有助于提升LLM的性能。 |
| [^11] | [Causal Interpretation of Self-Attention in Pre-Trained Transformers.](http://arxiv.org/abs/2310.20307) | 本研究提出了一种对自注意力进行因果解释的方法，并利用已有的预训练Transformer进行零样本因果发现。通过计算最深注意层中相应表示之间的偏相关，我们可以学习输入序列上的因果结构。该方法在两个任务中为Transformer的结果提供了因果解释。 |
| [^12] | [Revolutionizing Global Food Security: Empowering Resilience through Integrated AI Foundation Models and Data-Driven Solutions.](http://arxiv.org/abs/2310.20301) | 通过集成AI基础模型和数据驱动解决方案，革命性的全球食品安全研究提供了多功能的方法来解决当前深度学习和机器学习方法的局限性，该研究通过提供准确预测、改善资源分配和支持明智决策，增强了食品安全倡议，实现了对全球食品安全限制的变革性突破。 |
| [^13] | [Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents.](http://arxiv.org/abs/2310.20287) | 这项研究提出一种基于重置的深度集合学习方法，以提高深度强化学习的样本效率和解决复位方法的局限性。实验结果表明，该方法在安全强化学习领域取得了良好的性能。 |
| [^14] | [AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data.](http://arxiv.org/abs/2310.20280) | 这篇论文提出了AutoMixer，一个基于时间序列基础模型的自动混合模型，通过通道压缩预训练和微调工作流技术，有效解耦了BizITOps数据中有用和嘈杂的跨通道交互，提高了多变量时间序列预测的性能。 |
| [^15] | [Constructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning.](http://arxiv.org/abs/2310.20268) | 本文提出了一种样本到类别图学习方法，用于解决少样本类增量学习中的过拟合和灾难性遗忘问题。通过构建样本之间的关系，该方法能够提取更精细的类别级特征，从而实现对新概念的学习和旧知识的保留。 |
| [^16] | [Beyond Average Return in Markov Decision Processes.](http://arxiv.org/abs/2310.20266) | 该论文研究了马尔可夫决策过程中超越平均回报的问题，总结了可以准确计算和优化的奖励函数的特征，并提供了针对这些特征的新规划方法。这些结果在马尔可夫决策过程的理论发展中具有重要意义。 |
| [^17] | [Artificial Intelligence for reverse engineering: application to detergents using Raman spectroscopy.](http://arxiv.org/abs/2310.20254) | 本研究使用人工智能和拉曼光谱法对洗涤剂进行逆向工程，以快速评估其潜在毒性，并为质量控制和监管提供精确的鉴定和定量分析。 |
| [^18] | [Diversified Node Sampling based Hierarchical Transformer Pooling for Graph Representation Learning.](http://arxiv.org/abs/2310.20250) | 本论文提出了一种基于多样化节点抽样的层次化Transformer池化方法，通过引入Transformer机制，能够有效捕捉图中节点之间的长距离依赖关系，并同时多样化采样节点，以解决现有节点丢弃方法中的限制。 |
| [^19] | [Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations.](http://arxiv.org/abs/2310.20246) | 本文首次探索并训练了强大的多语言数学推理模型，通过使用翻译构建了多语言数据集，并提出了各种训练策略来构建强大的模型。实验证实发现在多语言训练中，将目标语言的翻译与原始语言的表示结合起来以及交替训练和多语言模型的自举可以提高模型的性能。此外，模型在处理低频词和长句子方面仍面临挑战。 |
| [^20] | [Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape.](http://arxiv.org/abs/2310.20240) | 本论文提出了一个新的框架VividTalker，用于实现具有灵活头部姿态和自然面部细节的语音驱动的3D面部动画。该框架通过明确将面部动画分为头部姿态和嘴部运动来解决目前现有作品中的限制和挑战。 |
| [^21] | [VisPercep: A Vision-Language Approach to Enhance Visual Perception for People with Blindness and Low Vision.](http://arxiv.org/abs/2310.20225) | 本文提出了一种通过视觉语言方法增强盲人和视力低下人群的视觉感知能力的创新方法，能够提供周围环境的详细全面描述并提供潜在风险的警告。 |
| [^22] | [Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with Graphs for Passenger Trajectory Clustering.](http://arxiv.org/abs/2310.20224) | 本论文提出了一种新的张量狄利克雷过程多项式混合模型，利用图形结构和空间语义图对基于轨迹记录的乘客聚类进行了改进，能在一步中自动确定聚类数量，并保留了多维出行信息的分层结构。 |
| [^23] | [A Systematic Review for Transformer-based Long-term Series Forecasting.](http://arxiv.org/abs/2310.20218) | 基于Transformer的长期系列预测的系统综述，介绍了Transformer架构及其改进、公开可用的数据集和评估指标、有效训练Transformer的最佳实践和技术，并提出了潜在研究方向。 |
| [^24] | [Does GPT-4 Pass the Turing Test?.](http://arxiv.org/abs/2310.20216) | GPT-4通过了公开的在线图灵测试中的41%的游戏，在语言风格和社会情感特征方面表现较佳，但仍未能达到人类参与者的水平。图灵测试仍然是评估自然交流和欺骗的相关方法。 |
| [^25] | [Handover Protocol Learning for LEO Satellite Networks: Access Delay and Collision Minimization.](http://arxiv.org/abs/2310.20215) | 本研究提出了一种面向LEO卫星网络的深度强化学习切换协议(DHO)，通过预测能力跳过测量报告阶段，简化切换过程并消除访问延迟，同时在网络条件下表现优越，展示了实际应用价值。 |
| [^26] | [In Search of Lost Online Test-time Adaptation: A Survey.](http://arxiv.org/abs/2310.20199) | 本文展示了在线测试时间适应性（OTTA）的调研结果，重点研究了解决模糊设置、过时骨干结构和不一致超参数调整的挑战，并提出了有效的策略和新的评估指标。 |
| [^27] | [Generating Continuations in Multilingual Idiomatic Contexts.](http://arxiv.org/abs/2310.20195) | 本论文测试了生成性语言模型在多语种惯用语境中生成延续的能力，并发现模型在字面和惯用上下文中的表现相似，并且在两种语言中均具有鲁棒性。 |
| [^28] | [Self-supervised Pre-training for Precipitation Post-processor.](http://arxiv.org/abs/2310.20187) | 该论文提出了一种基于深度学习的降水后处理方法，使用自监督预训练和转移学习来提高数值天气预报模型的准确性。实验结果表明该方法在区域降水校正方面表现优于其他方法。 |
| [^29] | [Learning to Discover Skills through Guidance.](http://arxiv.org/abs/2310.20178) | 提出了一种名为DISCO-DANCE的无监督技能发现算法，通过引导学习提高探索效果，并在具有挑战性的环境中优于其他方法。 |
| [^30] | [GraphTransformers for Geospatial Forecasting.](http://arxiv.org/abs/2310.20174) | 本研究提出了一种新的基于图转换器的框架，用于改进地理空间序列轨迹预测。通过显式利用自动生成的图结构，可以显著提高地理空间轨迹预测的准确性。实验证明，该方法在飓风轨迹预测任务中表现优于基准模型。 |
| [^31] | [Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?.](http://arxiv.org/abs/2310.20162) | 本文研究了跨语言的多语言神经机器翻译中的鲁棒性是否可转移。通过一系列实验，我们发现从一个翻译方向获得的鲁棒性可以转移到其他翻译方向。 |
| [^32] | [Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts.](http://arxiv.org/abs/2310.20159) | 本文研究了知识增强的视觉问答任务，提出了一种利用语言引导的多模态框架，并通过实验证明了语言引导是一种简单但有效的策略，可以提高视觉问答的准确性。 |
| [^33] | [MLatom 3: Platform for machine learning-enhanced computational chemistry simulations and workflows.](http://arxiv.org/abs/2310.20155) | MLatom 3是一个机器学习平台，用于增强计算化学模拟和创建复杂工作流。用户可以选择多种模拟方法和预训练的ML模型进行能量计算、优化几何结构和模拟光谱等计算化学任务。 |
| [^34] | [Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision.](http://arxiv.org/abs/2310.20153) | 该论文提出了一种交互多重保真度学习框架，用于在有限的注释预算下开发小型特定领域的语言模型。该方法通过平衡低保真度自动注释和高保真度人类注释，以最大化模型性能。同时，还提出了一种增强注释多样性和信息性的查询策略。 |
| [^35] | [Unlearn What You Want to Forget: Efficient Unlearning for LLMs.](http://arxiv.org/abs/2310.20150) | 本论文提出了一种高效的遗忘框架来处理大语言模型（LLMs）中的隐私问题和数据保护违规。通过引入轻量级遗忘层到transformers中，并使用有选择的师生目标学习，我们能够在删除数据后有效地更新LLMs，而无需重新训练整个模型。实验证明了该方法的有效性。 |
| [^36] | [Decision-Making for Autonomous Vehicles with Interaction-Aware Behavioral Prediction and Social-Attention Neural Network.](http://arxiv.org/abs/2310.20148) | 本研究提出了一种自动驾驶车辆决策的行为模型，将驾驶员的互动意图编码为社交心理参数，并开发了基于优化的控制器和基于注意机制的神经网络架构来解决自动驾驶车辆在交通中的决策制定问题。 |
| [^37] | [EELBERT: Tiny Models through Dynamic Embeddings.](http://arxiv.org/abs/2310.20144) | EELBERT是一种通过动态嵌入实现微型模型的方法，具有最小的准确性回归和显著的模型尺寸缩小。最小的模型UNO-EELBERT在GLUE得分上与完全训练的BERT-tiny相差4%，并且体积只有其15倍之一（1.2MB）。 |
| [^38] | [Contrastive Difference Predictive Coding.](http://arxiv.org/abs/2310.20141) | 本文介绍了一种时间差异版本的对比预测编码，通过将不同时间序列数据的片段组合在一起，来减少学习预测未来事件所需的数据量。实验证明，与先前的方法相比，我们的方法在成功率上提高了2倍，并且对于随机环境有更好的适应能力。 |
| [^39] | [Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models.](http://arxiv.org/abs/2310.20105) | 本研究评估了GPT-3.5和GPT-4模型在分类编程课程中学生求助请求方面的性能，并发现它们可以通过大型语言模型的自动分类来提高教育系统的效能。 |
| [^40] | [Plagiarism and AI Assistance Misuse in Web Programming: Unfair Benefits and Characteristics.](http://arxiv.org/abs/2310.20104) | 这项研究关注编程教育中的抄袭和AI助手滥用问题，计划开发自动化工具帮助教师识别这些不当行为。实验结果显示，参与不当行为的学生在测试成绩上与独立完成时间短的学生相当。抄袭和使用AI助手的提交在细节上有所不同，后者更复杂且可读性较差。学生认为使用AI助手可能有益，但要得到适当承认。 |
| [^41] | [Data Market Design through Deep Learning.](http://arxiv.org/abs/2310.20096) | 这项研究介绍了使用深度学习进行收入最优数据市场设计的应用，旨在扩展前沿研究领域。 |
| [^42] | [Evaluating Neural Language Models as Cognitive Models of Language Acquisition.](http://arxiv.org/abs/2310.20093) | 本文评估了神经语言模型作为语言习得的认知模型的潜力。作者认为用于评估句法能力的基准不够严格，并提出了使用严选数据集来探索语法结构基础的建议。 |
| [^43] | [Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models.](http://arxiv.org/abs/2310.20081) | 个性化是NLP系统中用户体验的关键，本文通过大型语言模型将总结和检索整合，提出了一种利用任务感知的总结增强个性化方法。 |
| [^44] | [FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space.](http://arxiv.org/abs/2310.20071) | 本文提出了FOCAL框架，可以通过自监督训练从多模时间序列感知信号中提取全面的特征。它通过使每个模态都编码到一个因子化的潜空间中，同时突出共享特征和专属特征，从而有效处理感知模态之间的共享信息和专属信息。 |
| [^45] | [Vignat: Vulnerability identification by learning code semantics via graph attention networks.](http://arxiv.org/abs/2310.20067) | 本文提出了一种名为Vignat的漏洞识别框架，通过学习代码的图级语义表示，使用图注意力网络进行漏洞检测。实验结果表明，在C库的可靠数据集上，Vignat能够达到57.38%的准确率，同时提供了对漏洞模式的宝贵洞见。 |
| [^46] | [Concept Alignment as a Prerequisite for Value Alignment.](http://arxiv.org/abs/2310.20059) | 价值对齐对于构建安全可靠的人工智能系统至关重要。这篇论文分析了概念对齐问题，并展示了忽视概念对齐可能导致价值错位的情况。通过联合推理一个人的概念和价值，可以最小化这类失误。实验结果显示人类在有意行动时会考虑代理人所使用的概念。 |
| [^47] | [Constrained Hierarchical Monte Carlo Belief-State Planning.](http://arxiv.org/abs/2310.20054) | 有约束的层次蒙特卡洛信念状态规划（COBeTS）通过使用分层分解和约束选项控制器，将在线基于搜索的CPOMDP规划扩展到大型机器人问题，并能同时满足约束和奖励目标。 |
| [^48] | [Look At Me, No Replay! SurpriseNet: Anomaly Detection Inspired Class Incremental Learning.](http://arxiv.org/abs/2310.20052) | SurpriseNet提出了一个解决灾难性干扰和跨任务知识学习问题的方案，通过参数隔离方法和受异常检测启发的自编码器来实现。 |
| [^49] | [SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics.](http://arxiv.org/abs/2310.20049) | 提出了一个名为SURF的基准测试，用于评估和比较基于图的学习流体模拟器的泛化能力。SURF包括各种数据集和具体的性能和泛化度量指标。通过深入研究两种最先进的模型，我们证明了SURF的适用性。 |
| [^50] | [Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization.](http://arxiv.org/abs/2310.20033) | 本文提出了一种使用ChatGPT来生成高质量反馈数据以改善临床笔记总结的事实一致性的新方法。 |
| [^51] | [GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models.](http://arxiv.org/abs/2310.20025) | GOPlan是一个使用学习模型进行计划的目标条件下的离线强化学习方法，通过预训练先验策略和使用重新分析方法生成虚构轨迹，用以提高性能和处理有限数据预算和未见目标泛化的能力。 |
| [^52] | [Topology Recoverability Prediction for Ad-Hoc Robot Networks: A Data-Driven Fault-Tolerant Approach.](http://arxiv.org/abs/2310.20024) | 本论文提出了一种基于数据驱动的容错方法来预测自组织机器人网络的拓扑可恢复性。通过将问题转化为二分类问题，并使用贝叶斯高斯混合模型，该方法通过前故障和后故障的两个不同预测路径预测典型问题的解决方案。结果表明，与当前文献中最佳策略相比，该方法在解决拓扑可恢复性预测问题上取得了成功。 |
| [^53] | [Multiscale Feature Attribution for Outliers.](http://arxiv.org/abs/2310.20012) | 本论文提出了一种逆多尺度遮挡的特征归因方法，用于识别异常点，并且在Dark Energy Survey Instrument中的星系光谱异常点上取得了更易解释的结果。 |
| [^54] | [Evolutionary Tabletop Game Design: A Case Study in the Risk Game.](http://arxiv.org/abs/2310.20008) | 本研究提出了进化游戏设计方法的扩展，通过生成《风险》游戏的新变体来验证该方法。结果显示生成的新变体拥有更小的地图和更短的比赛时间，并产生更加平衡的游戏对局。 |
| [^55] | [Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning.](http://arxiv.org/abs/2310.20007) | 本文在多种情境下证明了汤普森采样在强化学习中的贝叶斯遗憾界，并通过对信息比的精确分析提出了一个基于时间不均匀强化学习问题的上界估计。同时，本文找到了各种设置中具体的界限，并讨论了这些结果是第一个其类别或改进了最先进方法的情况。 |
| [^56] | [Unveiling the Limits of Learned Local Search Heuristics: Are You the Mightiest of the Meek?.](http://arxiv.org/abs/2310.19990) | 本研究对学习的局部搜索启发式的限制进行了全面调查，结果表明，基于禁忌搜索的简单学习启发式超越了最先进的学习启发式方法。 |
| [^57] | [BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing.](http://arxiv.org/abs/2310.19975) | BioInstruct是一个用于生物医学自然语言处理的大型语言模型指令调整方法，通过引入针对性指令数据集BioInstruct，通过GPT-4语言模型进行精调，优化了模型在生物医学自然语言处理中的性能。 |
| [^58] | [ExPT: Synthetic Pretraining for Few-Shot Experimental Design.](http://arxiv.org/abs/2310.19961) | ExPT是一种用于少样本实验设计的合成预训练方法，通过将条件生成任务应用于少量带标签的样本和期望输出，生成最优的输入设计。这种方法在不依赖主动数据收集或大规模标记数据集的情况下，在现实场景中具有实用性。 |
| [^59] | [Deep Learning for Spatiotemporal Big Data: A Vision on Opportunities and Challenges.](http://arxiv.org/abs/2310.19957) | 本文介绍了深度学习在时空大数据中的应用，讨论了机遇和挑战，并指出了一些未来研究方向。 |
| [^60] | [Conditional Unscented Autoencoders for Trajectory Prediction.](http://arxiv.org/abs/2310.19944) | 本文提出了使用条件非线性自动编码器(CVAE)进行轨迹预测的方法，通过利用变分自动编码器(VAE)中的非线性采样过程和其他改进，超越了现有技术，为自动驾驶领域的轨迹预测提供了更好的性能。 |
| [^61] | [Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?.](http://arxiv.org/abs/2310.19936) | 本文提出了一种针对当前最先进目标检测器的半监督方法，使用师生架构在少标注学习设置中避免了依赖敏感的后处理步骤，并在标注稀缺的情况下表现优异。 |
| [^62] | [Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms.](http://arxiv.org/abs/2310.19927) | 研究者通过对基于模型的重新参数化策略梯度方法进行理论研究，发现在长期强化学习问题中可能会遇到优化困难，导致收敛速度较慢。他们提出了一种谱归一化方法来缓解梯度方差爆炸问题。 |
| [^63] | [Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents.](http://arxiv.org/abs/2310.19923) | Jina Embeddings 2是一个能够处理长篇文档的文本嵌入模型，突破了传统512个标记限制，提供了高达8192个标记的容量。 |
| [^64] | [Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records.](http://arxiv.org/abs/2310.19917) | 本综述对涉及利用电子健康记录数据的医疗人工智能研究中的偏见进行了系统综述，共涵盖了六种主要的偏见类型，同时总结了现有的偏见处理方法。 |
| [^65] | [Interpretable Prototype-based Graph Information Bottleneck.](http://arxiv.org/abs/2310.19906) | 这项工作提出了一种新颖的可解释的GNN框架，通过在信息瓶颈框架中将原型学习与输入图的关键子图相结合，为模型的解释能力和性能提供了改进。 |
| [^66] | [Herd: Using multiple, smaller LLMs to match the performances of proprietary, large LLMs via an intelligent composer.](http://arxiv.org/abs/2310.19902) | 使用智能组合器，一群开源模型可以达到或超过专有模型的性能。 |
| [^67] | [Exploring Geometry of Blind Spots in Vision Models.](http://arxiv.org/abs/2310.19889) | 本研究探索了在视觉模型中存在的不敏感现象，并提出了一种通过研究网络的等置信度级别集合的几何形状和范围的技术。通过提出的级别集遍历算法，可以找到与给定源图像相似但属于相同等置信度级别集的输入图像。 |
| [^68] | [Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone.](http://arxiv.org/abs/2310.19859) | Res-Tuning是一种新的调节范式，通过解绑调节器与主干模型，实现了灵活高效的调节。这种结构解离使得调节器的设计与网络架构无关，方便了各种调节策略的灵活组合。 |
| [^69] | [AI Alignment: A Comprehensive Survey.](http://arxiv.org/abs/2310.19852) | 本篇论文主要介绍了AI对齐的概念、方法和实践。研究围绕四个关键目标：健壮性、可解释性、可控性和道德性展开，并将其分为前向对齐和后向对齐两个部分。 AI对齐是为了构建符合人类意图和价值观的AI系统，并减轻由于系统不对齐带来的潜在风险。 |
| [^70] | [Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction.](http://arxiv.org/abs/2310.19845) | 本文提出了一种修改的遗传算法，用于同时降低维度和优化超参数，在不平衡数据集上进行垃圾邮件预测。实验结果表明，该模型在几何平均和准确率上表现良好。 |
| [^71] | [Modeling the Telemarketing Process using Genetic Algorithms and Extreme Boosting: Feature Selection and Cost-Sensitive Analytical Approach.](http://arxiv.org/abs/2310.19843) | 本研究提出了一个使用遗传算法和极限增强模型进行电话营销过程建模的方法，其中包括特征选择和成本敏感的分析方法。研究通过利用电话营销数据和社会经济指标对客户意愿进行建模，并构建出可解释的预测模型。 |
| [^72] | [AMIR: Automated MisInformation Rebuttal -- A COVID-19 Vaccination Datasets based Recommendation System.](http://arxiv.org/abs/2310.19834) | 本研究提出了基于COVID-19疫苗数据集的自动化虚假信息驳斥推荐系统，利用社交媒体信息和经过策划的事实核查数据，实现了大规模自动驳斥虚假信息。这为对抗虚假信息提供了一种成本效益高、可扩展的解决方案。 |
| [^73] | [GalliformeSpectra: A Hen Breed Dataset.](http://arxiv.org/abs/2310.19830) | GalliformeSpectra是一个包含十种不同鸡品种的数据集，共有1010张图像，展示了每个品种的独特特征，对于家禽科学、遗传学和农业研究有重要潜力。 |
| [^74] | [Machine Learning and Knowledge: Why Robustness Matters.](http://arxiv.org/abs/2310.19819) | 机器学习算法的鲁棒性对于信任和知识的形成至关重要，只有在正确特征下跨情景良好运行的算法才能提供可靠的知识。 |
| [^75] | [Training binary neural networks without floating point precision.](http://arxiv.org/abs/2310.19815) | 本研究提出了两种解决方案，通过拓扑变化和策略训练，实现了高效训练的二进制神经网络，具有低延迟和低能耗的特点。 |
| [^76] | [Enhancing Genetic Improvement Mutations Using Large Language Models.](http://arxiv.org/abs/2310.19813) | 本文研究了利用大型语言模型作为遗传改进的变异操作符来提高搜索过程，发现使用LLM编辑的补丁通过单元测试的数量高达75％，但相比较标准编辑，LLMs找到的补丁较少多样化。尽管LLM增强的GI找到了许多改进的补丁，但最好的改进补丁是通过标准GI找到的。 |
| [^77] | [Brain decoding: toward real-time reconstruction of visual perception.](http://arxiv.org/abs/2310.19812) | 本研究提出了一种基于脑磁图（MEG）的脑解码方法，通过训练一个具有预训练嵌入、MEG模块和图像生成器的模型，在实时应用中实现了对视觉知觉的高时间分辨率解码，并在图像检索上取得了7倍的改进。 |
| [^78] | [Automated Verification of Equivalence Properties in Advanced Logic Programs -- Bachelor Thesis.](http://arxiv.org/abs/2310.19806) | 这篇论文介绍了一种自动验证工具，用于验证优化的逻辑子程序是否可以替代原始子程序，在工业应用中具有重要意义。 |
| [^79] | [SERA:Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning.](http://arxiv.org/abs/2310.19805) | 这篇论文提出了一种称为SERA的奖励增强框架，用于改善离线到在线强化学习中的探索能力。它通过设计内在奖励来鼓励agent进行探索，并实现更好的在线微调效果。 |
| [^80] | [A Kernel Perspective on Behavioural Metrics for Markov Decision Processes.](http://arxiv.org/abs/2310.19804) | 本文从核的角度论述了马尔科夫决策过程行为度量的新视角，并提出了一种新的度量与MICo距离等价。此外，核的视角还使我们能够提供新的理论结果，包括界定价值函数差异和嵌入到低失真误差的欧氏空间中。这些结果对于使用行为度量构建强化学习表示至关重要。同时，我们通过实证结果证明了这些方法的实际有效性。 |
| [^81] | [SyMPox: An Automated Monkeypox Detection System Based on Symptoms Using XGBoost.](http://arxiv.org/abs/2310.19801) | SyMPox是一个基于症状的自动猴痘检测系统，利用XGBoost算法分析症状模式并提供准确的猴痘诊断，为用户提供了一个用户友好的平台。 |
| [^82] | [From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces.](http://arxiv.org/abs/2310.19786) | 通过新的约化方法，我们改进了经典的Swap遗憾最小化算法，并提供了一个无外部遗憾算法的替代方法。对于学习专家建议问题，我们的算法可以在较少的轮数和更低的复杂度下达到相同的Swap遗憾限制。 |
| [^83] | [Designing AI Support for Human Involvement in AI-assisted Decision Making: A Taxonomy of Human-AI Interactions from a Systematic Review.](http://arxiv.org/abs/2310.19778) | 通过对AI辅助决策文献的系统回顾，我们填补了人机交互协议的共同词汇的空白，并强调了解AI应该提供什么信息来帮助人类的重要性。 |
| [^84] | [Evaluating Large Language Models: A Comprehensive Survey.](http://arxiv.org/abs/2310.19736) | 本调查综述了对大型语言模型（LLMs）的评估，包括知识和能力评估、对齐评估和安全评估。对于充分利用LLMs的能力以及确保其安全和有益的发展至关重要。 |
| [^85] | [Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding.](http://arxiv.org/abs/2310.19671) | 当前的大型语言模型在生成文本方面表现出了出色的能力，但对其能力的辩论缺乏细致的考虑。这篇论文评估了三个常见批评观点，并提出了对LLMs理解和意图问题的务实观点。 |
| [^86] | [LLMaAA: Making Large Language Models as Active Annotators.](http://arxiv.org/abs/2310.19596) | LLMaAA是一种利用大型语言模型作为主动批注器的方法，通过在主动学习循环中使用LLM确定高效批注内容，以最大限度地利用LLM的潜力并利用大量未标记数据。 |
| [^87] | [Trust, Accountability, and Autonomy in Knowledge Graph-based AI for Self-determination.](http://arxiv.org/abs/2310.19503) | 基于知识图谱的人工智能技术带来了许多好处，但其普及可能导致市民的自主决策权受限。为了解决这个问题，一些地区提出了人工智能管制措施。 |
| [^88] | [Text-to-3D with Classifier Score Distillation.](http://arxiv.org/abs/2310.19415) | 本文提出了一种名为分类器分数蒸馏（CSD）的方法，它利用隐式分类模型进行文本到3D生成，证明了仅凭分类器无关指导就可以有效实现文本到3D生成任务，并在各种任务中取得了优于最先进方法的结果。 |
| [^89] | [TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise.](http://arxiv.org/abs/2310.19019) | TeacherLM-7.1B是一个小型模型，通过给自然语言处理样本进行注释，教会其他模型“为什么”而不仅仅是“什么”。它在MMLU上取得了52.3的零样本得分，同时具有出色的数据增强能力。发布TeacherLM系列模型和增强的数据集作为开源项目。 |
| [^90] | [AI for Open Science: A Multi-Agent Perspective for Ethically Translating Data to Knowledge.](http://arxiv.org/abs/2310.18852) | 这项研究提出了AI for Open Science (AI4OS)的概念，将其作为AI for Science (AI4Science)的多智能体扩展，旨在通过在科学企业中最大化开放知识转化来促进开放科学。研究使用知识发现与数据挖掘（KDD）的原则来形式化AI4OS，并提出了具体应用开放性的方法。 |
| [^91] | [Reboost Large Language Model-based Text-to-SQL, Text-to-Python, and Text-to-Function -- with Real Applications in Traffic Domain.](http://arxiv.org/abs/2310.18752) | 本论文提出了一种基于大型语言模型的文本到SQL、文本到Python和文本到函数的方法，通过查询重写和SQL增强等技术，将模糊信息转化为确切和精确的信息，并引入执行反馈和查询结果增强SQL本身。该方法在交通领域的实际应用中取得了显著的性能提升。 |
| [^92] | [LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery.](http://arxiv.org/abs/2310.18356) | LoRAShear是一种高效的大型语言模型结构剪枝和知识恢复方法，通过逐步剪枝和动态微调，有效减少LLMs的占用空间并且保持性能。 |
| [^93] | [BioImage.IO Chatbot: A Personalized Assistant for BioImage Analysis Augmented by Community Knowledge Base.](http://arxiv.org/abs/2310.18351) | BioImage.IO Chatbot 是一个根据用户个性化需求提供答案的AI聊天助手，通过汇集和解释多个数据库、工具文档和数据源的信息，以及社区贡献的知识库和优化的检索方法，为生物图像分析工具的使用者提供了个性化、上下文感知的体验，为可访问的科学研究设立了新的标准。 |
| [^94] | [AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models.](http://arxiv.org/abs/2310.18331) | AllTogether是一个标准化的提示模板，通过增强任务背景表示，提高了大型语言模型（LLMs）在基于HTML的Web导航中的性能。研究结果显示，像GPT-4这样的模型在这类任务中优于较小的模型，并且HTML代码片段的长度和历史轨迹对性能有显著影响。同时，在实时环境反馈方面，优于之前的逐步指导。这项工作为未来LLM驱动的Web代理研究提供了宝贵的见解。 |
| [^95] | [Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting.](http://arxiv.org/abs/2310.17811) | 该论文提出了一种使用RadGraph和少样本提示的风格感知放射学报告生成的方法。通过将报告的内容和风格分开处理，可以避免生成临床不准确的报告。定量评估和人工评估结果均表明该方法表现出良好的性能，并生成与个体放射科医生风格完全相同的报告。 |
| [^96] | [CodeFusion: A Pre-trained Diffusion Model for Code Generation.](http://arxiv.org/abs/2310.17680) | CodeFusion是一种预训练的代码生成模型，通过扩散的方式解决了自然语言代码生成中遇到的限制，实验表明其在准确率和多样性上优于最先进的自回归系统。 |
| [^97] | [FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language.](http://arxiv.org/abs/2310.17306) | FormaT5是一个基于转换器的模型，可以根据目标表格和自然语言描述生成数据相关的条件格式规则。为了解决描述不足的问题，FormaT5通过放弃目标的方式学习预测占位符。 |
| [^98] | [CosmosDSR -- a methodology for automated detection and tracking of orbital debris using the Unscented Kalman Filter.](http://arxiv.org/abs/2310.17158) | CosmosDSR是一种新颖的方法，将YOLOv3与无味卡尔曼滤波器结合起来，用于自动检测和跟踪轨道碎片。在使用SPARK数据集进行训练和测试时，YOLOv3表现出很高的准确度，并且CosmosDSR和LKF都能准确地跟踪卫星。 |
| [^99] | [Correction with Backtracking Reduces Hallucination in Summarization.](http://arxiv.org/abs/2310.16176) | 本文介绍了一种简单而有效的技术，CoBa，用于减少摘要中的幻觉。该方法通过测量条件词概率和上下文词距离的统计信息进行幻觉检测，并通过直观的回溯法进行减轻。实验证明，CoBa在减少摘要幻觉方面是有效且高效的。 |
| [^100] | [FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions.](http://arxiv.org/abs/2310.15421) | FANToM是一个新的基准，用于通过问答在信息不对称的对话环境中压力测试机器的心智理论。这个基准对最先进的大型语言模型来说具有挑战性，即使是具有思维链推理和微调的模型也比人类表现得差。 |
| [^101] | [Learning Interpretable Rules for Scalable Data Representation and Classification.](http://arxiv.org/abs/2310.14336) | 这项研究提出了一种名为RRL的新型分类器，通过自动学习可解释的非模糊规则，实现了数据表示和分类的良好可扩展性和解释性。 |
| [^102] | [Quality-Diversity through AI Feedback.](http://arxiv.org/abs/2310.13032) | 基于AI反馈的质量-多样性（QDAIF）算法利用语言模型来生成和评估创造性写作，比传统算法更广泛地覆盖高质量样本的搜索空间。 |
| [^103] | [Federated Multi-Objective Learning.](http://arxiv.org/abs/2310.09866) | 本研究提出了一种新的联邦多目标学习（FMOL）框架，在满足多代理多任务学习应用的分布式性质和数据隐私需求的同时，支持不同客户端上的不同目标函数集合。通过引入联邦学习的范式，将多目标优化（MOO）推广到联邦学习领域。 |
| [^104] | [MIR2: Towards Provably Robust Multi-Agent Reinforcement Learning by Mutual Information Regularization.](http://arxiv.org/abs/2310.09833) | MIR2提出了一种针对鲁棒多智能体强化学习的方法，通过在常规情况下训练策略并最小化互信息作为鲁棒正则化，实现了在不准备每种可能的最坏情况的情况下提升鲁棒性的目标。 |
| [^105] | [OptiMUS: Optimization Modeling Using mip Solvers and large language models.](http://arxiv.org/abs/2310.06116) | 介绍了OptiMUS，一种基于大规模语言模型的优化建模代理，用于解决MILP问题。该代理能够自动生成数学模型、编写和调试求解器代码，并具有较高的解决率。 |
| [^106] | [Leveraging Diffusion Disentangled Representations to Mitigate Shortcuts in Underspecified Visual Tasks.](http://arxiv.org/abs/2310.02230) | 本文提出了一种利用扩散分离表示来处理不完全规定的视觉任务中捷径学习问题的方法，通过生成合成反事实来促进模型的多样性，从而使模型能够忽略捷径线索并达到与其他方法相当的性能。 |
| [^107] | [NoxTrader: LSTM-Based Stock Return Momentum Prediction.](http://arxiv.org/abs/2310.00747) | NoxTrader是一种基于LSTM的股票收益动量预测模型，旨在实现中长期盈利。它通过对历史交易数据进行时间序列分析，并进行特征工程、预测建模、参数配置和回测验证，证明了算法交易模型在现实交易场景中的潜在可行性。 |
| [^108] | [D-Separation for Causal Self-Explanation.](http://arxiv.org/abs/2309.13391) | 本研究提出了一种新的准则，最小条件依赖（MCD）准则，来揭示因果解释。通过最小化选择理由候选项上未选择部分与目标标签的依赖，强制选择所有的标签原因。 |
| [^109] | [How is ChatGPT's behavior changing over time?.](http://arxiv.org/abs/2307.09009) | 本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。 |
| [^110] | [Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback.](http://arxiv.org/abs/2307.02770) | 该论文研究了通过3分钟的人类反馈来扩散模型的有限采样，并表明仅几分钟的人类反馈生成的标签就足够实现图像的审查任务。 |
| [^111] | [SAMAug: Point Prompt Augmentation for Segment Anything Model.](http://arxiv.org/abs/2307.01187) | SAMAug是一种用于增强交互式图像分割性能的方法，通过生成增强的点提示，结合初始提示，可以提高Segment Anything Model的分割结果。 |
| [^112] | [Soft Gripping: Specifying for Trustworthiness.](http://arxiv.org/abs/2307.01159) | 本研究讨论了在开发软机器人系统时制定规范的重要性，并针对购物杂货拾取任务的软夹持器提出了一个广泛的示例规范，涵盖了可靠性、安全性、适应性、可预测性、伦理和法规等功能和非功能要求。 |
| [^113] | [Towards Personalized Cold-Start Recommendation with Prompts.](http://arxiv.org/abs/2306.17256) | 本研究旨在解决个性化冷启动推荐问题，通过利用预训练语言模型的能力，将推荐过程转化为自然语言情感分析，提供适用于创业企业和用户参与历史不足的平台的个性化推荐。 |
| [^114] | [Separable Physics-Informed Neural Networks.](http://arxiv.org/abs/2306.15969) | 这项研究提出了一种可分离的物理信息神经网络（SPINN），通过逐个处理轴来显著减少了多维 PDE 中的网络传播数量，并使用正向模式自动微分降低了计算成本，使得可以在单个普通 GPU 上使用大量的配点。 |
| [^115] | [Simplifying and Empowering Transformers for Large-Graph Representations.](http://arxiv.org/abs/2306.10759) | 本文通过实验证明，在大型图上使用一层注意力即可获得令人惊讶的竞争性能，挑战了在语言和视觉任务中复杂模型的应用。这促使我们重新思考在大型图上设计Transformer的理念，以提高可扩展性。 |
| [^116] | [QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules.](http://arxiv.org/abs/2306.09549) | 该论文提出了一种新的量子哈密顿数据集QH9，用于为各种分子提供精确的哈密顿矩阵。通过设计基准任务，展示了当前机器学习模型有能力预测任意分子的哈密顿矩阵。 |
| [^117] | [Norm-guided latent space exploration for text-to-image generation.](http://arxiv.org/abs/2306.08687) | 本研究观察到当前训练过程中，扩散模型只观测到具有狭窄范值的输入，这对于图像生成方法和少样本学习任务具有重要影响。本文提出了一种新的插值方法，并定义了一种基于范数的非欧几里得度量，以解决这个问题。 |
| [^118] | [Identification of Nonlinear Latent Hierarchical Models.](http://arxiv.org/abs/2306.07916) | 本文提出了一种方法，可以在观测变量由因果相关的潜变量生成的非线性潜变量层次因果模型中实现因果结构和潜变量的可识别性。 |
| [^119] | [TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models.](http://arxiv.org/abs/2306.06815) | 本文开创性地研究了基于 prompt 学习的预训练语言模型 API 的特洛伊易感性，并提出了一种自动黑盒框架——TrojPrompt，用于生成通用和隐蔽的触发器，并将特洛伊木马插入硬提示。 |
| [^120] | [Multi-body SE(3) Equivariance for Unsupervised Rigid Segmentation and Motion Estimation.](http://arxiv.org/abs/2306.05584) | 本文提出了一种基于SE（3）等变结构和非监督训练策略的方法，可以实现刚性分割和运动估计，不需要类别信息且具有极高的模型效率。 |
| [^121] | [FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering.](http://arxiv.org/abs/2306.05523) | FACTIFY3M是一个以多模式虚假信息验证为目标的数据集。虚假信息如今已成为当下重大的社会问题，这一数据集旨在通过多模式验证来及时识别和缓解虚假信息。 |
| [^122] | [Structural Similarities Between Language Models and Neural Response Measurements.](http://arxiv.org/abs/2306.01930) | 本研究研究了语言模型和神经响应测量之间的结构相似性，发现神经语言模型越大，其表示越相似于脑成像的神经响应测量。 |
| [^123] | [Faith and Fate: Limits of Transformers on Compositionality.](http://arxiv.org/abs/2305.18654) | 研究了Transformer模型在三个代表性组合型任务中的表现，发现其通过线性子图匹配解决多步组合推理问题。 |
| [^124] | [Understanding Emotion Valence is a Joint Deep Learning Task.](http://arxiv.org/abs/2305.17422) | 研究通过多任务学习方法，探索情绪价值与情绪载体之间的相互依赖关系，并在联合预测设置中使用判别性模型取得了最佳平衡。 |
| [^125] | [Are Diffusion Models Vision-And-Language Reasoners?.](http://arxiv.org/abs/2305.16397) | 本文针对扩散-语言图像生成模型进行转换和评估，介绍了生成-鉴别评估基准(GDBench)基于7个视觉语言复杂任务，并发现转换后的模型在组合性任务方面的表现优于CLIP，通过微调可提高其组合性能。 |
| [^126] | [Decision-Aware Actor-Critic with Function Approximation and Theoretical Guarantees.](http://arxiv.org/abs/2305.15249) | 提出了一种具有函数逼近和理论保证的决策感知演员-评论家算法，通过设计联合目标来解决演员和评论家之间的不匹配，并且无论策略和评论家参数化的选择如何，该算法都保证单调策略改进。 |
| [^127] | [Flexible Grammar-Based Constrained Decoding for Language Models.](http://arxiv.org/abs/2305.13971) | 本文提出了一种使用形式语法约束丰富解码步骤的方法，有效生成符合特定语法的复杂输出结构，同时允许任何上下文无关语法集成。实验证明该方法在四个信息提取任务上实现了最先进的性能表现。 |
| [^128] | [Perception Test: A Diagnostic Benchmark for Multimodal Video Models.](http://arxiv.org/abs/2305.13786) | 该论文提出了一个名为“感知测试”的多模态视频基准测试，可以评估预训练模型的感知和推理能力，测试涵盖了记忆、抽象、物理、语义等技能和描述性、解释性、预测性、反事实性等推理类型。 |
| [^129] | [A Scalable Neural Network for DSIC Affine Maximizer Auction Design.](http://arxiv.org/abs/2305.12162) | 该论文提出了一种可扩展的神经网络AMenuNet来构造AMAs参数和生成候选分配，解决了现有方法在占优策略激励兼容性和可扩展性方面的限制，其在协商一致的价值和社会残余价值方面优于强基线模型。 |
| [^130] | [When Do Neural Nets Outperform Boosted Trees on Tabular Data?.](http://arxiv.org/abs/2305.02997) | 这项研究通过对176个数据集的比较分析发现，在许多数据集中，GBDT和NN之间的性能差异可以忽略不计，或者GBDT的轻微超参数调整比选择最佳算法更重要。此外，研究人员对965个元特征进行了分析，发现GBDT在高维稀疏数据上表现更好。 |
| [^131] | [Learning to Reason and Memorize with Self-Notes.](http://arxiv.org/abs/2305.00833) | 该论文提出了一种学习使用自注记进行推理和记忆的方法，通过允许模型明确思考、记录自己的想法，并整合先前的推理步骤，从而提高了多步推理的能力。 |
| [^132] | [Artificial General Intelligence (AGI) for Education.](http://arxiv.org/abs/2304.12479) | AGI技术具有革命教育领域潜力，可以建立e-learning平台、教育协作工具等，弥补传统AI模型因受限于数据和人际交互限制而无法满足教育需求的不足。 |
| [^133] | [OpenAssistant Conversations -- Democratizing Large Language Model Alignment.](http://arxiv.org/abs/2304.07327) | 释放了OpenAssistant Conversations，这是一个由全球超过1,000名参与者进行人工生成和人工注释的助手风格对话语料库，可以通过SFT和RLHF有效地用于LLM对齐，提高模型性能和可用性。 |
| [^134] | [RoboPianist: A Benchmark for High-Dimensional Robot Control.](http://arxiv.org/abs/2304.04150) | RoboPianist是一个新的高维机器人控制基准测试，旨在测试高精度、协调和规划，并通过反复接触的欠驱动系统进行钢琴演奏。该基准测试提供了性能特征的定量数据，并具有易于解释的结果。 |
| [^135] | [On the Pareto Front of Multilingual Neural Machine Translation.](http://arxiv.org/abs/2304.03216) | 本研究针对多语言神经机器翻译的数据不平衡问题，提出双重幂律方法用于预测独特的性能权衡前沿，并建立基于该方法的样本比例选择优化问题，取得更好的结果。 |
| [^136] | [Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning.](http://arxiv.org/abs/2304.01295) | 本文提出了一个平行大规模多语种会话数据集XSGD，开发了一种有效的基于提示调整的方法来学习对齐提示，同时研究了跨语言任务的NLI-based和vanilla分类器，并在插槽填充和意图分类任务上评估了模型的跨语言泛化能力。 |
| [^137] | [Time Series as Images: Vision Transformer for Irregularly Sampled Time Series.](http://arxiv.org/abs/2303.12799) | 本文提出了一种新颖的方法，将不规则采样的时间序列转换为线图像，并适应强大的视觉transformer进行时间序列分类。该方法简化了算法设计，具有通用性，并展示了在多个医疗和人体活动数据集上明显优于最先进的专业算法的表现。 |
| [^138] | [Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels.](http://arxiv.org/abs/2302.10586) | 本文介绍了一种名为双伪训练（DPT）的训练策略，该策略结合了强大的半监督学习器和扩散模型来进一步推进半监督生成和分类任务。实验结果表明，DPT在各种情况下都能实现半监督生成和分类任务的SOTA性能，特别是在每个类别只有一个或两个标签的情况下，超过了其他一些模型。 |
| [^139] | [Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey.](http://arxiv.org/abs/2302.10035) | 本文综合调查了大规模多模态预训练模型，介绍了背景、任务定义、关键挑战和优势，并讨论了数据、目标、网络架构和知识增强预训练等方面的相关内容。 |
| [^140] | [Machine Learning for Cutting Planes in Integer Programming: A Survey.](http://arxiv.org/abs/2302.09166) | 该论文调查了机器学习技术在混合整数线性规划中选择切割平面的最新研究，提出了使用数据进行优化切割选择的有前景的方法。 |
| [^141] | [Self-Supervised Temporal Graph learning with Temporal and Structural Intensity Alignment.](http://arxiv.org/abs/2302.07491) | 该论文提出了一种自监督的时间图学习方法，通过提取时间和结构信息来学习更具信息量的节点表示。 |
| [^142] | [Is Distance Matrix Enough for Geometric Deep Learning?.](http://arxiv.org/abs/2302.05743) | 本文证明了消息传递神经网络（MPNNs）不能学习几何信息，提出了$k$-DisGNNs可以利用距离矩阵中的信息，并建立了几何深度学习和传统图表示学习之间的联系。 |
| [^143] | [Better by you, better than me, chatgpt3 as writing assistance in students essays.](http://arxiv.org/abs/2302.04536) | 本研究比较了使用ChatGPT-3作为写作辅助工具和不使用对学生论文写作表现的影响，结果显示两组学生的平均分数相似，但实验组的文本不真实性稍高，整体样本中论文相似性较低。 |
| [^144] | [Learning List-Level Domain-Invariant Representations for Ranking.](http://arxiv.org/abs/2212.10764) | 本文提出了一种针对排名问题的列表级别对齐的学习方法，该方法利用列表的结构特性，在领域适应中实现从源领域到目标领域的知识转移。 |
| [^145] | [A Vision-free Baseline for Multimodal Grammar Induction.](http://arxiv.org/abs/2212.10564) | 本论文研究了在多模式设置下，只使用文本进行训练的大型语言模型（LLMs）是否能够提供强大的辅助来进行语法归纳。结果显示，基于LLM的纯文本方法在多种多模式数据集上优于先前的方法，并且在性能、参数数量和训练速度方面取得了最先进的结果。 |
| [^146] | [Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases.](http://arxiv.org/abs/2212.02648) | 这个论文提出了一种使用排序数据来测量和减少模型对虚假线索的偏见的简单有效方法。通过排名图像的虚假性，可以识别出少数子群体，并通过准确率差来评估模型的偏见。此外，通过在虚假性较低的图像上微调模型，可以在几乎不损失准确率的情况下消除模型的偏见，实现对样本的公正处理。 |
| [^147] | [Melting Pot 2.0.](http://arxiv.org/abs/2211.13746) | 研究工具Melting Pot 2.0为多智能体人工智能提供了评估协议，在一组典型测试场景中测量它们对新颖社交伙伴的泛化能力。 |
| [^148] | [Taming Reachability Analysis of DNN-Controlled Systems via Abstraction-Based Training.](http://arxiv.org/abs/2211.11127) | 本文提出了一种基于抽象的方法，用于绕过在DNN控制系统中对DNN进行过度近似的可达性分析问题。通过在传统的DNN中插入一个抽象层，将实数抽象化为一个区间进行训练，进而实现对DNN控制系统的黑盒可达性分析。 |
| [^149] | [Conceptor-Aided Debiasing of Large Language Models.](http://arxiv.org/abs/2211.11087) | 本论文提出一种基于概念器的大型语言模型去偏见方法。我们通过后处理和一种新架构CI-BERT将概念器投影纳入所有层中。概念器后处理方法取得了最先进的去偏见结果，同时保持或改善了模型的性能。 |
| [^150] | [Robust Reinforcement Learning in Continuous Control Tasks with Uncertainty Set Regularization.](http://arxiv.org/abs/2207.02016) | 该论文提出了一种新的正则化器USR，通过构建转换函数参数空间上的不确定性集合来提高连续控制任务中的强化学习性能。通过对值函数进行对抗生成未知不确定性集合，进一步增强了USR的灵活性。在真实世界强化学习基准测试中得到了改进的结果。 |
| [^151] | [A general class of surrogate functions for stable and efficient reinforcement learning.](http://arxiv.org/abs/2108.05828) | 本研究提出了一个基于函数镜像上升的普适框架(FMA-PG)，构建了一系列替代函数，这些函数可以实现策略改进，并且不受策略参数化选择的影响。通过实验证实，该方法具有良好的性能和理论保证。 |
| [^152] | [Revisiting Parameter Sharing in Multi-Agent Deep Reinforcement Learning.](http://arxiv.org/abs/2005.13625) | 本研究重访了多智能体深度强化学习中的参数共享方法。我们通过引入智能体指示信号实现了在不同策略网络共享参数的同时学习不同策略或任务的能力，并且证明了这些方法在异构观测和行动空间学习中可以收敛到最优策略。 |

# 详细

[^1]: 大型语言模型是否像孩子一样解决语言类比问题？

    Do large language models solve verbal analogies like children do?. (arXiv:2310.20384v1 [cs.CL])

    [http://arxiv.org/abs/2310.20384](http://arxiv.org/abs/2310.20384)

    本文研究了大型语言模型是否像孩子一样通过联想来解决语言类比问题，实验证明荷兰语母语和多语言LLMs的表现与儿童相当，但当控制联想过程时，模型的性能下降1-2年。

    

    类比思维是人类认知的核心。成年人通过映射关系并回答问题，如“马属于马厩，鸡属于...？”而解决语言类比问题。相反，孩子们经常使用联想作答，例如回答“蛋”。本文研究了大型语言模型（LLMs）是否像孩子一样通过联想来解决A:B::C:?形式的语言类比问题。我们使用从在线自适应学习环境中提取的语言类比问题，其中来自荷兰的14,002名7-12岁儿童解决了622个荷兰语的语言类比问题。六个测试的荷兰语母语和多语言LLMs的表现与儿童大致相当，MGPT表现最差，接近7岁水平，XLM-V和GPT-3表现最佳，略高于11岁水平。然而，当我们控制联想过程时，情况发生变化，每个模型的表现水平下降1-2年。进一步实验表明，联想过程的控制对模型的性能有显著影响。

    Analogy-making lies at the heart of human cognition. Adults solve analogies such as \textit{Horse belongs to stable like chicken belongs to ...?} by mapping relations (\textit{kept in}) and answering \textit{chicken coop}. In contrast, children often use association, e.g., answering \textit{egg}. This paper investigates whether large language models (LLMs) solve verbal analogies in A:B::C:? form using associations, similar to what children do. We use verbal analogies extracted from an online adaptive learning environment, where 14,002 7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six tested Dutch monolingual and multilingual LLMs performed around the same level as children, with MGPT performing worst, around the 7-year-old level, and XLM-V and GPT-3 the best, slightly above the 11-year-old level. However, when we control for associative processes this picture changes and each model's performance level drops 1-2 years. Further experiments demonstrate that associ
    
[^2]: GPT-4V在医学影像中的多模态能力的全面研究

    A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging. (arXiv:2310.20381v1 [cs.CV])

    [http://arxiv.org/abs/2310.20381](http://arxiv.org/abs/2310.20381)

    本文对GPT-4V在医学影像中的多模态能力进行了全面研究和评估，发现其在生成描述性报告和医学VQA方面有潜力，但在某些评估指标上仍需改进。

    

    本文对GPT-4V在不同医学影像任务中的能力进行了全面评估，包括放射学报告生成、医学视觉问答(VQA)和视觉定位。尽管先前的研究探索了GPT-4V在医学影像中的性能，但据我们所知，我们的研究是首个基于公开可用基准的定量评估。我们的研究发现，当给出结构良好的提示时，GPT-4V在胸部X射线图像的生成描述性报告方面具有潜力。然而，在MIMIC-CXR数据集基准上的表现揭示了某些评估指标(如CIDEr)的改进空间。在医学VQA领域，GPT-4V在区分问题类型方面表现出熟练，但在准确度方面不及现有基准。此外，我们的分析发现常规评估指标如BLEU分数的局限性，呼吁开发更好的评价指标。

    This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding. While prior efforts have explored GPT-4V's performance in medical imaging, to the best of our knowledge, our study represents the first quantitative evaluation on publicly available benchmarks. Our findings highlight GPT-4V's potential in generating descriptive reports for chest X-ray images, particularly when guided by well-structured prompts. However, its performance on the MIMIC-CXR dataset benchmark reveals areas for improvement in certain evaluation metrics, such as CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in distinguishing between question types but falls short of prevailing benchmarks in terms of accuracy. Furthermore, our analysis finds the limitations of conventional evaluation metrics like the BLEU score, advocating for the development of m
    
[^3]: 基于机器学习的框架用于对住宅电力负荷轮廓进行聚类以增强需求响应计划

    A Machine Learning-Based Framework for Clustering Residential Electricity Load Profiles to Enhance Demand Response Programs. (arXiv:2310.20367v1 [cs.LG])

    [http://arxiv.org/abs/2310.20367](http://arxiv.org/abs/2310.20367)

    本文提出一种基于机器学习的框架，通过对住宅电力负荷轮廓进行聚类，增强需求响应计划。通过使用伦敦家庭的数据进行实证分析和多个评估指标，我们应用了四种聚类算法，并将问题重新定义为概率分类问题，利用可解释的人工智能（xAI）来增强解释性。

    

    智能电表数据衍生出的负荷曲线经常被用于分析日常能源消耗模式，特别是在需求响应（DR）等应用领域。然而，这个工作最重要的挑战之一在于识别具有类似消耗行为的最合适的消费者群体。本文提出了一种新颖的基于机器学习的框架，通过一个实际案例研究，利用伦敦近5000户家庭的数据实现最佳负荷轮廓。具体应用了四种广泛使用的聚类算法，包括K-means、K-medoids、层次凝聚聚类和基于密度的空间聚类。还利用经验分析和多个评估指标来评估这些算法。之后，我们将问题重新定义为概率分类问题，分类器模拟聚类算法的行为，利用可解释的人工智能（xAI）来增强解释性。

    Load shapes derived from smart meter data are frequently employed to analyze daily energy consumption patterns, particularly in the context of applications like Demand Response (DR). Nevertheless, one of the most important challenges to this endeavor lies in identifying the most suitable consumer clusters with similar consumption behaviors. In this paper, we present a novel machine learning based framework in order to achieve optimal load profiling through a real case study, utilizing data from almost 5000 households in London. Four widely used clustering algorithms are applied specifically K-means, K-medoids, Hierarchical Agglomerative Clustering and Density-based Spatial Clustering. An empirical analysis as well as multiple evaluation metrics are leveraged to assess those algorithms. Following that, we redefine the problem as a probabilistic classification one, with the classifier emulating the behavior of a clustering algorithm,leveraging Explainable AI (xAI) to enhance the interpre
    
[^4]: 深度学习的数学介绍：方法、实现和理论

    Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory. (arXiv:2310.20360v1 [cs.LG])

    [http://arxiv.org/abs/2310.20360](http://arxiv.org/abs/2310.20360)

    本书提供了对深度学习算法的数学介绍，包括不同的神经网络架构和优化算法，并涵盖了深度学习算法的理论方面。此外，还介绍了深度学习逼近偏微分方程的方法。希望对学生和科学家们有所帮助。

    

    本书旨在介绍深度学习算法的主题。我们详细介绍了深度学习算法的基本组成部分，包括不同的人工神经网络架构（如全连接前馈神经网络、卷积神经网络、循环神经网络、残差神经网络和带有批归一化的神经网络）以及不同的优化算法（如基本的随机梯度下降法、加速方法和自适应方法）。我们还涵盖了深度学习算法的几个理论方面，如人工神经网络的逼近能力（包括神经网络的微积分）、优化理论（包括Kurdyka-Lojasiewicz不等式）和泛化误差。在本书的最后一部分，我们还回顾了一些用于偏微分方程的深度学习逼近方法，包括物理信息神经网络（PINNs）和深度Galerkin方法。希望本书能对学生和科学家们有所帮助。

    This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-{\L}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet 
    
[^5]: 提升多模态大语言模型的空间意识能力

    Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model. (arXiv:2310.20357v1 [cs.AI])

    [http://arxiv.org/abs/2310.20357](http://arxiv.org/abs/2310.20357)

    本论文提出了一种改进多模态大语言模型空间意识能力的方法，通过利用精确的物体空间位置信息指导模型，在用户查询中提供更准确的响应。

    

    多模态大语言模型（MLLM）是指扩展了大语言模型（LLM）的能力，可以接收和推断多模态数据。空间意识是MLLM的关键能力之一，包括了理解物体之间以及物体与场景之间的空间关系的多种技能。自动驾驶、智能医疗、机器人技术、虚拟现实和增强现实等行业对MLLM的空间意识能力有很大需求。然而，当前MLLM的空间意识能力与人类需求之间存在明显差距。为了解决这个问题，本文提出使用更精确的物体之间的空间位置信息来引导MLLM，从而提供更准确的用户查询响应。具体来说，针对特定的多模态任务，我们利用算法获取几何空间信息和场景图来获取相关的几何特征。

    The Multi-Modal Large Language Model (MLLM) refers to an extension of the Large Language Model (LLM) equipped with the capability to receive and infer multi-modal data. Spatial awareness stands as one of the crucial abilities of MLLM, encompassing diverse skills related to understanding spatial relationships among objects and between objects and the scene area. Industries such as autonomous driving, smart healthcare, robotics, virtual, and augmented reality heavily demand MLLM's spatial awareness capabilities. However, there exists a noticeable gap between the current spatial awareness capabilities of MLLM and the requirements set by human needs. To address this issue, this paper proposes using more precise spatial position information between objects to guide MLLM in providing more accurate responses to user-related inquiries. Specifically, for a particular multi-modal task, we utilize algorithms for acquiring geometric spatial information and scene graphs to obtain relevant geometric
    
[^6]: 肌肉体积定量化：基于解剖先验指导的变压器

    Muscle volume quantification: guiding transformers with anatomical priors. (arXiv:2310.20355v1 [cs.CV])

    [http://arxiv.org/abs/2310.20355](http://arxiv.org/abs/2310.20355)

    本研究提出了一种在三维磁共振图像中自动分割下肢18个肌肉的方法，以辅助形态测量分析。该方法基于混合架构，结合了卷积和视觉变压器模块，解决了肌肉组织无法分辨和轮廓难以检测的问题。

    

    肌肉体积是一种有用的定量生物标志物，不仅用于运动领域，还用于退行性肌肉骨骼疾病的随访。除了体积外，通过从医学图像中分割感兴趣的肌肉，还可以提取其他形状标志物。尽管非常耗时，但在这类测量中，手工分割仍然是金标准。我们提出了一种方法来自动分割三维磁共振图像中下肢的18个肌肉，以辅助形态测量分析。由于在MR图像中观察时，不同肌肉的组织无法分辨。因此，肌肉分割算法不能依赖外观，只能依赖轮廓线索。然而，这样的轮廓很难检测，并且其厚度在受试者之间变化。为了应对以上挑战，我们基于混合架构提出了一种分割方法，结合了卷积和视觉变压器模块。我们首次研究了这种混合架构的行为特点。

    Muscle volume is a useful quantitative biomarker in sports, but also for the follow-up of degenerative musculo-skelletal diseases. In addition to volume, other shape biomarkers can be extracted by segmenting the muscles of interest from medical images. Manual segmentation is still today the gold standard for such measurements despite being very time-consuming. We propose a method for automatic segmentation of 18 muscles of the lower limb on 3D Magnetic Resonance Images to assist such morphometric analysis. By their nature, the tissue of different muscles is undistinguishable when observed in MR Images. Thus, muscle segmentation algorithms cannot rely on appearance but only on contour cues. However, such contours are hard to detect and their thickness varies across subjects. To cope with the above challenges, we propose a segmentation approach based on a hybrid architecture, combining convolutional and visual transformer blocks. We investigate for the first time the behaviour of such hy
    
[^7]: 将形状完成和抓取预测结合，实现快速灵活的多指抓取

    Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand. (arXiv:2310.20350v1 [cs.RO])

    [http://arxiv.org/abs/2310.20350](http://arxiv.org/abs/2310.20350)

    本文提出了一种结合形状完成和抓取预测的方法，实现了快速灵活的多指抓取。通过使用基于深度图像的形状完成模块和基于预测的抓取预测器，实现了在具有有限或无先验知识的情况下，对物体进行抓取的任务。

    

    在辅助机器人中，对于具有有限或无先验知识的物体进行抓取是一项非常重要的技能。然而，在这种普适情况下，尤其是在观测能力有限和利用多指手进行灵活抓取时，仍然存在一个开放的问题。我们提出了一种新颖、快速和高保真度的深度学习流程，由基于单个深度图像的形状完成模块和基于预测的物体形状的抓取预测器组成。形状完成网络基于VQDIF，在任意查询点上预测空间占用值。作为抓取预测器，我们使用了两阶段架构，首先使用自回归模型生成手姿势，然后回归每个姿势的手指关节配置。关键因素是足够的数据真实性和增强，以及在训练过程中对困难情况的特殊关注。在物理机器人平台上进行的实验表明，成功地实现了抓取。

    Grasping objects with limited or no prior knowledge about them is a highly relevant skill in assistive robotics. Still, in this general setting, it has remained an open problem, especially when it comes to only partial observability and versatile grasping with multi-fingered hands. We present a novel, fast, and high fidelity deep learning pipeline consisting of a shape completion module that is based on a single depth image, and followed by a grasp predictor that is based on the predicted object shape. The shape completion network is based on VQDIF and predicts spatial occupancy values at arbitrary query points. As grasp predictor, we use our two-stage architecture that first generates hand poses using an autoregressive model and then regresses finger joint configurations per pose. Critical factors turn out to be sufficient data realism and augmentation, as well as special attention to difficult cases during training. Experiments on a physical robot platform demonstrate successful gras
    
[^8]: 从聚类视角改进基于熵的测试时间自适应

    Improving Entropy-Based Test-Time Adaptation from a Clustering View. (arXiv:2310.20327v1 [cs.AI])

    [http://arxiv.org/abs/2310.20327](http://arxiv.org/abs/2310.20327)

    本文从聚类的角度解释了基于熵的测试时间自适应（EBTTA）方法，提出了一个迭代算法，并展示了对于EBTTA方法来说，熵损失会进一步增加最大的概率，从而为其在聚类任务上的较好性能提供了一个替代性解释。

    

    在现实世界中，领域偏移是一个常见的问题，训练数据和测试数据遵循不同的数据分布。为了解决这个问题，完全的测试时间自适应（TTA）利用测试时间遇到的无标签数据来适应模型。特别是基于熵的测试时间自适应（EBTTA）方法，在测试样本上最小化预测的熵，取得了很大的成功。在本文中，我们从聚类的角度介绍了EBTTA的新视角和解释。这是一个迭代算法：1）在分配步骤中，EBTTA模型的前向过程是为这些测试样本分配标签；2）在更新步骤中，反向过程是通过已分配的样本来更新模型。根据这种解释，我们可以更深入地理解EBTTA，其中我们展示了熵损失会进一步增加最大的概率。因此，我们提供了一个替代性解释，解释了为什么现有的EBTTA方法在聚类任务上比在分类任务上表现更好。

    Domain shift is a common problem in the realistic world, where training data and test data follow different data distributions. To deal with this problem, fully test-time adaptation (TTA) leverages the unlabeled data encountered during test time to adapt the model. In particular, Entropy-Based TTA (EBTTA) methods, which minimize the prediction's entropy on test samples, have shown great success. In this paper, we introduce a new perspective on the EBTTA, which interprets these methods from a view of clustering. It is an iterative algorithm: 1) in the assignment step, the forward process of the EBTTA models is the assignment of labels for these test samples, and 2) in the updating step, the backward process is the update of the model via the assigned samples. Based on the interpretation, we can gain a deeper understanding of EBTTA, where we show that the entropy loss would further increase the largest probability. Accordingly, we offer an alternative explanation that why existing EBTTA 
    
[^9]: SemanticBoost：通过增强文本提示提升运动生成

    SemanticBoost: Elevating Motion Generation with Augmented Textual Cues. (arXiv:2310.20323v1 [cs.CV])

    [http://arxiv.org/abs/2310.20323](http://arxiv.org/abs/2310.20323)

    SemanticBoost是一个新颖的框架，通过增强文本提示和上下文信息来提升运动生成，解决了从复杂语义描述中生成运动的困难。

    

    目前的技术在从复杂的语义描述中生成运动时面临困难，主要原因是数据集中缺乏足够的语义注释和弱的上下文理解。为了解决这些问题，我们提出了SemanticBoost，一个新颖的框架，同时解决了这两个挑战。我们的框架包括语义增强模块和上下文调节运动去噪器（CAMD）。语义增强模块从运动数据中提取补充语义，丰富数据集的文本描述，并确保文本和运动数据之间的准确对齐，而不依赖于大型语言模型。另一方面，CAMD方法通过有效捕捉上下文信息并将生成的运动与给定的文本描述对齐，提供了一个全面的解决方案来生成高质量、语义一致的运动序列。与现有方法不同，我们的方法可以合成准确的定向运动。

    Current techniques face difficulties in generating motions from intricate semantic descriptions, primarily due to insufficient semantic annotations in datasets and weak contextual understanding. To address these issues, we present SemanticBoost, a novel framework that tackles both challenges simultaneously. Our framework comprises a Semantic Enhancement module and a Context-Attuned Motion Denoiser (CAMD). The Semantic Enhancement module extracts supplementary semantics from motion data, enriching the dataset's textual description and ensuring precise alignment between text and motion data without depending on large language models. On the other hand, the CAMD approach provides an all-encompassing solution for generating high-quality, semantically consistent motion sequences by effectively capturing context information and aligning the generated motion with the given textual descriptions. Distinct from existing methods, our approach can synthesize accurate orientational movements, combi
    
[^10]: 大型语言模型中的心灵理论：11种最新模型与7-10岁儿童在高级测试中的表现比较

    Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests. (arXiv:2310.20320v1 [cs.CL])

    [http://arxiv.org/abs/2310.20320](http://arxiv.org/abs/2310.20320)

    本研究通过将11种基础模型和调整指令的大型语言模型（LLMs）与7-10岁儿童在高级测试中进行比较，发现GPT系列的调整指令LLMs表现最佳，并且在某些任务上超过了儿童的表现。此外，基础LLMs大多无法解决ToM任务，而调整指令则通过奖励合作性沟通有助于提升LLM的性能。

    

    我们应该给予大型语言模型（LLM）多大的认知能力，例如理解意图和信念的理论心灵（ToM）能力？在这里，我们通过以下方式，为这场新兴辩论增加一些证据：（i）测试11个基础模型和调整指令的LLMs的ToM相关能力，超越主导的虚假信念范式，包括非文字的语言使用和递归的意图；（ii）使用新编写的标准化测试版本来评估LLMs的稳健性；（iii）提示并计分开放问题和封闭问题；（iv）将LLM的表现与7-10岁儿童在相同任务上的表现进行基准测试。我们发现，GPT系列的调整指令LLMs在其他模型中表现最佳，并且通常也超过了儿童的表现。基础LLMs大多无法解决ToM任务，即使使用了专门的提示。我们认为，语言和ToM的相互关联性可能有助于解释为什么调整指令会增加LLM的性能：奖励合作性沟通。

    To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that t
    
[^11]: 自训练Transformer中自注意力的因果解释

    Causal Interpretation of Self-Attention in Pre-Trained Transformers. (arXiv:2310.20307v1 [cs.AI])

    [http://arxiv.org/abs/2310.20307](http://arxiv.org/abs/2310.20307)

    本研究提出了一种对自注意力进行因果解释的方法，并利用已有的预训练Transformer进行零样本因果发现。通过计算最深注意层中相应表示之间的偏相关，我们可以学习输入序列上的因果结构。该方法在两个任务中为Transformer的结果提供了因果解释。

    

    我们提出了一种对Transformer神经网络架构中自注意力进行因果解释的方法。我们将自注意力解释为一种估计给定输入符号序列的结构方程模型的机制。结构方程模型可以解释为在特定上下文中输入序列上的因果结构。重要的是，在存在潜在混淆变量的情况下，这种解释仍然有效。根据这种解释，我们通过计算最深注意层中相应表示之间的偏相关来估计输入符号之间的条件独立关系。这使得可以使用现有的基于约束的算法学习输入序列上的因果结构。从这个意义上讲，现有的预训练Transformer可以用于零样本因果发现。我们通过为两个任务中Transformer的结果提供因果解释来示范这种方法。

    We propose a causal interpretation of self-attention in the Transformer neural network architecture. We interpret self-attention as a mechanism that estimates a structural equation model for a given input sequence of symbols (tokens). The structural equation model can be interpreted, in turn, as a causal structure over the input symbols under the specific context of the input sequence. Importantly, this interpretation remains valid in the presence of latent confounders. Following this interpretation, we estimate conditional independence relations between input symbols by calculating partial correlations between their corresponding representations in the deepest attention layer. This enables learning the causal structure over an input sequence using existing constraint-based algorithms. In this sense, existing pre-trained Transformers can be utilized for zero-shot causal-discovery. We demonstrate this method by providing causal explanations for the outcomes of Transformers in two tasks:
    
[^12]: 革命性的全球食品安全：通过集成AI基础模型和数据驱动解决方案增强韧性

    Revolutionizing Global Food Security: Empowering Resilience through Integrated AI Foundation Models and Data-Driven Solutions. (arXiv:2310.20301v1 [cs.AI])

    [http://arxiv.org/abs/2310.20301](http://arxiv.org/abs/2310.20301)

    通过集成AI基础模型和数据驱动解决方案，革命性的全球食品安全研究提供了多功能的方法来解决当前深度学习和机器学习方法的局限性，该研究通过提供准确预测、改善资源分配和支持明智决策，增强了食品安全倡议，实现了对全球食品安全限制的变革性突破。

    

    食品安全是一个全球性的关注点，需要精确多样的数据驱动解决方案来应对其多方面的挑战。本文探讨了将AI基础模型应用于各种食品安全应用的集成，利用不同类型的数据，以克服当前深度学习和机器学习方法的局限性。具体来说，我们研究了它们在作物类型映射、农田映射、田块划分和作物产量预测中的应用。通过利用多光谱图像、气象数据、土壤属性、历史记录和高分辨率卫星图像，AI基础模型提供了一种多功能的方法。研究表明，AI基础模型通过提供准确预测、改善资源分配和支持明智决策，增强了食品安全倡议。这些模型在解决全球食品安全限制方面起到了变革性的作用，标志着朝着可持续和安全的方向迈进了一大步。

    Food security, a global concern, necessitates precise and diverse data-driven solutions to address its multifaceted challenges. This paper explores the integration of AI foundation models across various food security applications, leveraging distinct data types, to overcome the limitations of current deep and machine learning methods. Specifically, we investigate their utilization in crop type mapping, cropland mapping, field delineation and crop yield prediction. By capitalizing on multispectral imagery, meteorological data, soil properties, historical records, and high-resolution satellite imagery, AI foundation models offer a versatile approach. The study demonstrates that AI foundation models enhance food security initiatives by providing accurate predictions, improving resource allocation, and supporting informed decision-making. These models serve as a transformative force in addressing global food security limitations, marking a significant leap toward a sustainable and secure f
    
[^13]: 通过重置深度集合代理实现高样本效率和安全的深度强化学习

    Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents. (arXiv:2310.20287v1 [cs.LG])

    [http://arxiv.org/abs/2310.20287](http://arxiv.org/abs/2310.20287)

    这项研究提出一种基于重置的深度集合学习方法，以提高深度强化学习的样本效率和解决复位方法的局限性。实验结果表明，该方法在安全强化学习领域取得了良好的性能。

    

    深度强化学习通过与深度神经网络集成作为函数逼近器，在解决复杂任务方面取得了显著的成功。然而，对深度神经网络的依赖引入了一种被称为“优先级偏见”的新挑战，即这些函数逼近器倾向于优先考虑早期的经验，导致过拟合。为了缓解这种优先级偏见，已经提出了一种重置方法，该方法以保留回放缓冲区的方式对深度强化学习代理的部分或全部进行周期性重置。然而，使用重置方法后可能出现性能崩溃，这从安全强化学习和遗憾最小化的角度来看可能是有害的。在本文中，我们提出了一种新的基于重置的方法，利用深度集合学习来解决普通重置方法的局限性，并增强样本效率。通过包括安全强化学习领域在内的各种实验对所提出的方法进行了评估。

    Deep reinforcement learning (RL) has achieved remarkable success in solving complex tasks through its integration with deep neural networks (DNNs) as function approximators. However, the reliance on DNNs has introduced a new challenge called primacy bias, whereby these function approximators tend to prioritize early experiences, leading to overfitting. To mitigate this primacy bias, a reset method has been proposed, which performs periodic resets of a portion or the entirety of a deep RL agent while preserving the replay buffer. However, the use of the reset method can result in performance collapses after executing the reset, which can be detrimental from the perspective of safe RL and regret minimization. In this paper, we propose a new reset-based method that leverages deep ensemble learning to address the limitations of the vanilla reset method and enhance sample efficiency. The proposed method is evaluated through various experiments including those in the domain of safe RL. Numer
    
[^14]: 改进的多变量时间序列预测的自动混合模型在BizITOps数据上的应用

    AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data. (arXiv:2310.20280v1 [cs.LG])

    [http://arxiv.org/abs/2310.20280](http://arxiv.org/abs/2310.20280)

    这篇论文提出了AutoMixer，一个基于时间序列基础模型的自动混合模型，通过通道压缩预训练和微调工作流技术，有效解耦了BizITOps数据中有用和嘈杂的跨通道交互，提高了多变量时间序列预测的性能。

    

    业务过程的效率依赖于业务关键绩效指标（Biz-KPIs），而IT故障可能对其产生负面影响。BizITOps数据将Biz-KPIs和IT事件通道融合成多变量时间序列数据。提前预测Biz-KPIs可以通过主动的纠正措施提高效率和收益。然而，BizITOps数据通常展示出Biz-KPIs和IT事件之间有用和嘈杂的跨通道交互，需要有效解耦。当使用现有的多变量预测模型时，这导致预测性能不佳。为了解决这个问题，我们引入了一个称为AutoMixer的时间序列基础模型（FM）方法，该方法基于新颖的通道压缩预训练和微调工作流技术。AutoMixer利用自动编码器进行通道压缩的预训练，并将其与先进的TSMixer模型集成，用于多变量时间序列预测。这种融合极大地增强了TSM

    The efficiency of business processes relies on business key performance indicators (Biz-KPIs), that can be negatively impacted by IT failures. BizITOps data fuses both Biz-KPIs and IT event channels together as multivariate time series data. Forecasting Biz-KPIs in advance can enhance efficiency and revenue through proactive corrective measures. However, BizITOps data generally exhibit both useful and noisy inter-channel interactions between Biz-KPIs and IT events that need to be effectively decoupled. This leads to suboptimal forecasting performance when existing multivariate forecasting models are employed. To address this, we introduce AutoMixer, a time-series Foundation Model (FM) approach, grounded on the novel technique of channel-compressed pretrain and finetune workflows. AutoMixer leverages an AutoEncoder for channel-compressed pretraining and integrates it with the advanced TSMixer model for multivariate time series forecasting. This fusion greatly enhances the potency of TSM
    
[^15]: 构建样本到类别图进行少样本类增量学习

    Constructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning. (arXiv:2310.20268v1 [cs.CV])

    [http://arxiv.org/abs/2310.20268](http://arxiv.org/abs/2310.20268)

    本文提出了一种样本到类别图学习方法，用于解决少样本类增量学习中的过拟合和灾难性遗忘问题。通过构建样本之间的关系，该方法能够提取更精细的类别级特征，从而实现对新概念的学习和旧知识的保留。

    

    少样本类增量学习旨在构建机器学习模型，能够从少量数据样本中不断学习新概念，同时不会遗忘旧类别的知识。该领域的挑战在于新类别的数据量有限，这不仅会导致严重的过拟合问题，还会加剧臭名昭著的灾难性遗忘问题。正如早期研究证明的那样，构建样本之间的关系对于从少样本中学习是有益的。本文将这一思想推广到增量场景，并提出了一种适用于少样本类增量学习的样本到类别（S2C）图学习方法。具体而言，本文提出了一个样本级图网络（SGN），专注于分析单个会话中样本之间的关系。该网络有助于聚合相似的样本，最终提取出更精细的类别级特征。然后，我们提出了一个类别级图网络（CGN），用于建立新类别和旧类别的类别级特征之间的连接。

    Few-shot class-incremental learning (FSCIL) aims to build machine learning model that can continually learn new concepts from a few data samples, without forgetting knowledge of old classes.  The challenges of FSCIL lies in the limited data of new classes, which not only lead to significant overfitting issues but also exacerbates the notorious catastrophic forgetting problems. As proved in early studies, building sample relationships is beneficial for learning from few-shot samples. In this paper, we promote the idea to the incremental scenario, and propose a Sample-to-Class (S2C) graph learning method for FSCIL.  Specifically, we propose a Sample-level Graph Network (SGN) that focuses on analyzing sample relationships within a single session. This network helps aggregate similar samples, ultimately leading to the extraction of more refined class-level features.  Then, we present a Class-level Graph Network (CGN) that establishes connections across class-level features of both new and 
    
[^16]: 马尔可夫决策过程中超越平均回报

    Beyond Average Return in Markov Decision Processes. (arXiv:2310.20266v1 [cs.AI])

    [http://arxiv.org/abs/2310.20266](http://arxiv.org/abs/2310.20266)

    该论文研究了马尔可夫决策过程中超越平均回报的问题，总结了可以准确计算和优化的奖励函数的特征，并提供了针对这些特征的新规划方法。这些结果在马尔可夫决策过程的理论发展中具有重要意义。

    

    在马尔可夫决策过程中，哪些奖励函数可以被准确地计算和优化？在有限时间段、无折扣设置中，动态规划只能高效处理某些统计类别的操作。我们总结了这些类别在策略评估中的特征，并提供了规划问题的新解。有趣的是，我们证明，即使在分布强化学习（DistRL）的更一般框架中，只有广义平均值可以被准确优化。然而，DistRL允许近似评估其他函数。我们给出了结果估计器的误差界限，并讨论了此方法的潜力及其局限性。这些结果通过研究回报的整体特征，特别是风险意识的策略，为马尔可夫决策过程的理论发展做出了贡献。

    What are the functionals of the reward that can be computed and optimized exactly in Markov Decision Processes? In the finite-horizon, undiscounted setting, Dynamic Programming (DP) can only handle these operations efficiently for certain classes of statistics. We summarize the characterization of these classes for policy evaluation, and give a new answer for the planning problem. Interestingly, we prove that only generalized means can be optimized exactly, even in the more general framework of Distributional Reinforcement Learning (DistRL).DistRL permits, however, to evaluate other functionals approximately. We provide error bounds on the resulting estimators, and discuss the potential of this approach as well as its limitations.These results contribute to advancing the theory of Markov Decision Processes by examining overall characteristics of the return, and particularly risk-conscious strategies.
    
[^17]: 逆向工程中的人工智能：应用于用拉曼光谱法检测洗涤剂

    Artificial Intelligence for reverse engineering: application to detergents using Raman spectroscopy. (arXiv:2310.20254v1 [cs.AI])

    [http://arxiv.org/abs/2310.20254](http://arxiv.org/abs/2310.20254)

    本研究使用人工智能和拉曼光谱法对洗涤剂进行逆向工程，以快速评估其潜在毒性，并为质量控制和监管提供精确的鉴定和定量分析。

    

    无论其性质如何，对复杂混合物进行逆向工程已经变得非常重要。能够快速评估与环境相关的新商业产品的潜在毒性对于分析学来说是一项真正的挑战。数字工具的发展（数据库、化学计量学、机器学习等）和分析技术（拉曼光谱法、近红外光谱法、质谱等）将允许鉴定潜在的有毒分子。本文以洗涤剂产品为例，其组成对人类或环境来说可能具有危险性，为了质量控制和监管目的，需要进行精确的鉴定和定量分析。通过结合各种数字工具（光谱数据库、混合数据库、实验设计、化学计量学/机器学习算法...）、不同的样品制备方法（原始样品或几种浓缩/稀释样品）和拉曼光谱法，我们可以实现这一目标。

    The reverse engineering of a complex mixture, regardless of its nature, has become significant today. Being able to quickly assess the potential toxicity of new commercial products in relation to the environment presents a genuine analytical challenge. The development of digital tools (databases, chemometrics, machine learning, etc.) and analytical techniques (Raman spectroscopy, NIR spectroscopy, mass spectrometry, etc.) will allow for the identification of potential toxic molecules. In this article, we use the example of detergent products, whose composition can prove dangerous to humans or the environment, necessitating precise identification and quantification for quality control and regulation purposes. The combination of various digital tools (spectral database, mixture database, experimental design, Chemometrics / Machine Learning algorithm{\ldots}) together with different sample preparation methods (raw sample, or several concentrated / diluted samples) Raman spectroscopy, has 
    
[^18]: 基于多样化节点抽样的层次化Transformer池化的图表示学习

    Diversified Node Sampling based Hierarchical Transformer Pooling for Graph Representation Learning. (arXiv:2310.20250v1 [cs.AI])

    [http://arxiv.org/abs/2310.20250](http://arxiv.org/abs/2310.20250)

    本论文提出了一种基于多样化节点抽样的层次化Transformer池化方法，通过引入Transformer机制，能够有效捕捉图中节点之间的长距离依赖关系，并同时多样化采样节点，以解决现有节点丢弃方法中的限制。

    

    图池化方法广泛用于降采样图形，在图级任务如图分类和图生成中取得了令人印象深刻的结果。一种重要的方法称为节点丢弃池化，旨在利用可学习的评分函数丢弃具有相对较低显著性得分的节点。然而，现有的节点丢弃方法存在两个限制：（1）对于每个池化节点，这些模型在主要以GNN为骨架的情况下难以捕捉长距离依赖关系；（2）仅汇集得分最高的节点往往会保留相似的节点，从而丢弃低得分节点的丰富信息。为了解决这些问题，我们提出了一种名为GTPool的图变换池化方法，将Transformer引入到节点丢弃池化中，以便高效地捕捉长距离的成对交互，并同时多样化采样节点。具体来说，我们设计了一个基于自我注意机制的评分模块，同时考虑了全局和局部信息，以指导节点的丢弃。

    Graph pooling methods have been widely used on downsampling graphs, achieving impressive results on multiple graph-level tasks like graph classification and graph generation. An important line called node dropping pooling aims at exploiting learnable scoring functions to drop nodes with comparatively lower significance scores. However, existing node dropping methods suffer from two limitations: (1) for each pooled node, these models struggle to capture long-range dependencies since they mainly take GNNs as the backbones; (2) pooling only the highest-scoring nodes tends to preserve similar nodes, thus discarding the affluent information of low-scoring nodes. To address these issues, we propose a Graph Transformer Pooling method termed GTPool, which introduces Transformer to node dropping pooling to efficiently capture long-range pairwise interactions and meanwhile sample nodes diversely. Specifically, we design a scoring module based on the self-attention mechanism that takes both globa
    
[^19]: 在多语言数学推理中打破语言障碍：见解与观察

    Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v1 [cs.CL])

    [http://arxiv.org/abs/2310.20246](http://arxiv.org/abs/2310.20246)

    本文首次探索并训练了强大的多语言数学推理模型，通过使用翻译构建了多语言数据集，并提出了各种训练策略来构建强大的模型。实验证实发现在多语言训练中，将目标语言的翻译与原始语言的表示结合起来以及交替训练和多语言模型的自举可以提高模型的性能。此外，模型在处理低频词和长句子方面仍面临挑战。

    

    现有研究主要集中在开发适用于单语言中的数学推理的强大语言学习模型（LLM），在多语言环境下保持效果的研究很少。为了弥补这一差距，本文首次探索和训练强大的多语言数学推理（xMR）LLM。首先，通过利用翻译，我们构建了第一个包含十种不同语言的多语言数学推理指导数据集MGSM8KInstruct，从而解决了xMR任务中训练数据稀缺的问题。根据收集的数据集，我们提出了不同的训练策略来构建强大的xMR LLMs，被命名为MathOctopus，在几次训练中表现出优于传统开源LLMs和ChatGPT的能力。值得注意的是，MathOctopus-13B在MGSM测试集上达到了47.6%的准确率，超过了ChatGPT的46.3%。除了显著的结果，我们还从大量的实验证实中发现了一些重要的观察和见解：（1）在多语言上进行训练时，最好将目标语言的翻译与原始语言的表示结合起来。 （2）交替训练和多语言模型的自举有助于提高模型的表现。 （3）模型对于低频词和长句子的处理是挑战的，需要进一步改进。

    Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When
    
[^20]: 为面部注入生命：具有自然头部姿态和详细形状的语音驱动的3D面部动画

    Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape. (arXiv:2310.20240v1 [cs.CV])

    [http://arxiv.org/abs/2310.20240](http://arxiv.org/abs/2310.20240)

    本论文提出了一个新的框架VividTalker，用于实现具有灵活头部姿态和自然面部细节的语音驱动的3D面部动画。该框架通过明确将面部动画分为头部姿态和嘴部运动来解决目前现有作品中的限制和挑战。

    

    创建逼真的语音驱动的3D面部动画需要音频输入和面部表情之间的自然和精确的同步。然而，现有作品仍无法渲染具有灵活头部姿态和自然面部细节（如皱纹）的形状。这种限制主要由两个方面导致：1）收集具有详细3D面部形状的训练集非常昂贵。详细形状注释的稀缺性阻碍了具有表情丰富的面部动画的模型训练。2）与嘴部运动相比，头部姿态与语音内容的相关性较小。因此，同时对嘴部运动和头部姿态进行建模导致了面部运动的可控性不足。为了解决这些挑战，我们引入了VividTalker框架，该框架旨在实现具有灵活头部姿态和自然面部细节的语音驱动的3D面部动画。

    The creation of lifelike speech-driven 3D facial animation requires a natural and precise synchronization between audio input and facial expressions. However, existing works still fail to render shapes with flexible head poses and natural facial details (e.g., wrinkles). This limitation is mainly due to two aspects: 1) Collecting training set with detailed 3D facial shapes is highly expensive. This scarcity of detailed shape annotations hinders the training of models with expressive facial animation. 2) Compared to mouth movement, the head pose is much less correlated to speech content. Consequently, concurrent modeling of both mouth movement and head pose yields the lack of facial movement controllability. To address these challenges, we introduce VividTalker, a new framework designed to facilitate speech-driven 3D facial animation characterized by flexible head pose and natural facial details. Specifically, we explicitly disentangle facial animation into head pose and mouth movement 
    
[^21]: VisPercep:一种通过视觉语言方法增强盲人和视力低下人群的视觉感知能力。

    VisPercep: A Vision-Language Approach to Enhance Visual Perception for People with Blindness and Low Vision. (arXiv:2310.20225v1 [cs.CV])

    [http://arxiv.org/abs/2310.20225](http://arxiv.org/abs/2310.20225)

    本文提出了一种通过视觉语言方法增强盲人和视力低下人群的视觉感知能力的创新方法，能够提供周围环境的详细全面描述并提供潜在风险的警告。

    

    盲人和视力低下人群在对陌生环境下的场景识别和精确的目标识别方面面临很大挑战。此外，由于视力丧失，盲人和视力低下人群很难自己访问和识别潜在的绊倒危险。本文提出了一种创新方法，利用大型视觉语言模型增强盲人和视力低下人群的视觉感知能力，提供周围环境的详细全面描述，并提供潜在风险的警告。我们的方法首先利用大型图像标记模型（即Recognize Anything (RAM)）识别捕获图像中的所有常见物体。然后将识别结果和用户查询整合成一个特定于盲人和视力低下人群的提示，并利用提示工程技术进行定制。通过将提示和输入图像相结合，一个大型视觉语言模型（即InstructBLIP）生成详细全面的描述。

    People with blindness and low vision (pBLV) encounter substantial challenges when it comes to comprehensive scene recognition and precise object identification in unfamiliar environments. Additionally, due to the vision loss, pBLV have difficulty in accessing and identifying potential tripping hazards on their own. In this paper, we present a pioneering approach that leverages a large vision-language model to enhance visual perception for pBLV, offering detailed and comprehensive descriptions of the surrounding environments and providing warnings about the potential risks. Our method begins by leveraging a large image tagging model (i.e., Recognize Anything (RAM)) to identify all common objects present in the captured images. The recognition results and user query are then integrated into a prompt, tailored specifically for pBLV using prompt engineering. By combining the prompt and input image, a large vision-language model (i.e., InstructBLIP) generates detailed and comprehensive desc
    
[^22]: 选择一个表：基于图的张量狄利克雷过程多项式混合模型用于乘客轨迹聚类

    Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with Graphs for Passenger Trajectory Clustering. (arXiv:2310.20224v1 [stat.ML])

    [http://arxiv.org/abs/2310.20224](http://arxiv.org/abs/2310.20224)

    本论文提出了一种新的张量狄利克雷过程多项式混合模型，利用图形结构和空间语义图对基于轨迹记录的乘客聚类进行了改进，能在一步中自动确定聚类数量，并保留了多维出行信息的分层结构。

    

    基于轨迹记录的乘客聚类对于交通运营商至关重要。然而，现有方法由于乘客出行信息的分层结构，包括每个乘客内部的多次出行以及每次出行的多维信息，无法轻松地聚类乘客。此外，现有方法依赖于准确指定聚类数量的起始值。最后，现有方法未考虑空间语义图，如地理邻近性和位置间的功能相似性。在本文中，我们提出了一种新颖的基于图的张量狄利克雷过程多项式混合模型，它能够保留多维出行信息的分层结构，并能以统一的一步方式对其进行聚类，具有自动确定聚类数量的能力。空间图被用于社区检测以连接语义邻居。我们进一步提出了张量版本的Coll...

    Passenger clustering based on trajectory records is essential for transportation operators. However, existing methods cannot easily cluster the passengers due to the hierarchical structure of the passenger trip information, including multiple trips within each passenger and multi-dimensional information about each trip. Furthermore, existing approaches rely on an accurate specification of the clustering number to start. Finally, existing methods do not consider spatial semantic graphs such as geographical proximity and functional similarity between the locations. In this paper, we propose a novel tensor Dirichlet Process Multinomial Mixture model with graphs, which can preserve the hierarchical structure of the multi-dimensional trip information and cluster them in a unified one-step manner with the ability to determine the number of clusters automatically. The spatial graphs are utilized in community detection to link the semantic neighbors. We further propose a tensor version of Coll
    
[^23]: 基于Transformer的长期系列预测的系统综述

    A Systematic Review for Transformer-based Long-term Series Forecasting. (arXiv:2310.20218v1 [cs.LG])

    [http://arxiv.org/abs/2310.20218](http://arxiv.org/abs/2310.20218)

    基于Transformer的长期系列预测的系统综述，介绍了Transformer架构及其改进、公开可用的数据集和评估指标、有效训练Transformer的最佳实践和技术，并提出了潜在研究方向。

    

    深度学习的出现在时间序列预测方面取得了显著进展。特别是Transformer架构在时间序列预测任务中得到了广泛的应用和采用。Transformer被证明是提取长序列内部元素之间语义相关性最成功的解决方案。各种变体使得Transformer架构能够有效处理长期时间序列预测任务。在本文中，我们首先对Transformer架构及其后续改进进行了全面概述，以解决各种长期时间序列预测任务。然后，我们总结了公开可用的长期时间序列预测数据集和相关的评估指标。此外，我们提供了关于在时间序列分析背景下有效训练Transformer的最佳实践和技术的有价值见解。最后，我们提出了这个快速发展领域的潜在研究方向。

    The emergence of deep learning has yielded noteworthy advancements in time series forecasting (TSF). Transformer architectures, in particular, have witnessed broad utilization and adoption in TSF tasks. Transformers have proven to be the most successful solution to extract the semantic correlations among the elements within a long sequence. Various variants have enabled transformer architecture to effectively handle long-term time series forecasting (LTSF) tasks. In this article, we first present a comprehensive overview of transformer architectures and their subsequent enhancements developed to address various LTSF tasks. Then, we summarize the publicly available LTSF datasets and relevant evaluation metrics. Furthermore, we provide valuable insights into the best practices and techniques for effectively training transformers in the context of time-series analysis. Lastly, we propose potential research directions in this rapidly evolving field.
    
[^24]: GPT-4 是否通过图灵测试？

    Does GPT-4 Pass the Turing Test?. (arXiv:2310.20216v1 [cs.AI])

    [http://arxiv.org/abs/2310.20216](http://arxiv.org/abs/2310.20216)

    GPT-4通过了公开的在线图灵测试中的41%的游戏，在语言风格和社会情感特征方面表现较佳，但仍未能达到人类参与者的水平。图灵测试仍然是评估自然交流和欺骗的相关方法。

    

    我们在一个公开的在线图灵测试中评估了 GPT-4。在表现最好的 GPT-4 提示中，在 41% 的游戏中通过了测试，超过了 ELIZA（27%）和 GPT-3.5（14%）设定的基准，但还不如人类参与者（63%）的机会和基准。参与者的决策主要基于语言风格（35%）和社会情感特征（27%），支持智能不足以通过图灵测试的观点。参与者的人口统计学特征，包括教育水平和对语言模型的熟悉度，并不能预测被识别率，这表明即使是深入了解系统并频繁与其交互的人，也会容易被欺骗。尽管图灵测试作为智能的测试具有已知的局限性，我们认为它在评估自然交流和欺骗方面仍然具有相关性。具有冒充人类能力的 AI 模型可能会对社会产生广泛的影响，我们分析了不同策略和标准的有效性。

    We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4 prompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and GPT-3.5 (14%), but falling short of chance and the baseline set by human participants (63%). Participants' decisions were based mainly on linguistic style (35%) and socio-emotional traits (27%), supporting the idea that intelligence is not sufficient to pass the Turing Test. Participants' demographics, including education and familiarity with LLMs, did not predict detection rate, suggesting that even those who understand systems deeply and interact with them frequently may be susceptible to deception. Despite known limitations as a test of intelligence, we argue that the Turing Test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and we analyse the effectiveness of different strategies and criteria fo
    
[^25]: LEO卫星网络的切换协议学习：访问延迟和碰撞最小化

    Handover Protocol Learning for LEO Satellite Networks: Access Delay and Collision Minimization. (arXiv:2310.20215v1 [cs.IT])

    [http://arxiv.org/abs/2310.20215](http://arxiv.org/abs/2310.20215)

    本研究提出了一种面向LEO卫星网络的深度强化学习切换协议(DHO)，通过预测能力跳过测量报告阶段，简化切换过程并消除访问延迟，同时在网络条件下表现优越，展示了实际应用价值。

    

    本研究介绍了一种新颖的基于深度强化学习 (DRL) 的切换 (HO) 协议，称为DHO，专门针对低轨道卫星网络的长传播延迟所带来的挑战。DHO在预定的LEO卫星轨迹模式下进行训练，在HO过程中利用其预测能力跳过测量报告(MR)阶段，简化了过程并消除了MR阶段产生的传播延迟，同时仍能提供有效的HO决策。所提出的DHO在不同网络条件下表现优于传统HO协议，包括访问延迟、碰撞率和切换成功率，展示了DHO在实际网络中的实际适用性。此外，本研究还探讨了访问延迟和碰撞率之间的平衡，并评估了使用各种DRL算法对DHO进行训练的性能和收敛性。

    This study presents a novel deep reinforcement learning (DRL)-based handover (HO) protocol, called DHO, specifically designed to address the persistent challenge of long propagation delays in low-Earth orbit (LEO) satellite networks' HO procedures. DHO skips the Measurement Report (MR) in the HO procedure by leveraging its predictive capabilities after being trained with a pre-determined LEO satellite orbital pattern. This simplification eliminates the propagation delay incurred during the MR phase, while still providing effective HO decisions. The proposed DHO outperforms the legacy HO protocol across diverse network conditions in terms of access delay, collision rate, and handover success rate, demonstrating the practical applicability of DHO in real-world networks. Furthermore, the study examines the trade-off between access delay and collision rate and also evaluates the training performance and convergence of DHO using various DRL algorithms.
    
[^26]: 追寻失落的在线测试时间适应性：一项调研

    In Search of Lost Online Test-time Adaptation: A Survey. (arXiv:2310.20199v1 [cs.AI])

    [http://arxiv.org/abs/2310.20199](http://arxiv.org/abs/2310.20199)

    本文展示了在线测试时间适应性（OTTA）的调研结果，重点研究了解决模糊设置、过时骨干结构和不一致超参数调整的挑战，并提出了有效的策略和新的评估指标。

    

    本文详细调研了在线测试时间适应性（OTTA）的综合概况，该范式专注于在批量到达时将机器学习模型调整到新数据分布上。尽管最近OTTA方法的增加，但该领域存在模糊的设置、过时的骨干结构和不一致的超参数调整等问题，这使得真正的挑战变得难以复现。为了清晰和严格的比较，我们将OTTA技术分为三个主要类别，并使用功能强大的Vision Transformer（ViT）骨干架构对它们进行基准测试，以发现真正有效的策略。我们的基准测试不仅涵盖传统的受损数据集，如CIFAR-10/100-C和ImageNet-C，还包括体现在CIFAR-10.1和CIFAR-10-Warehouse中的现实世界转变，涵盖了搜索引擎的变化和扩散模型合成数据的变化。为了衡量在线场景中的效率，我们引入了新的评估指标。

    In this paper, we present a comprehensive survey on online test-time adaptation (OTTA), a paradigm focused on adapting machine learning models to novel data distributions upon batch arrival. Despite the proliferation of OTTA methods recently, the field is mired in issues like ambiguous settings, antiquated backbones, and inconsistent hyperparameter tuning, obfuscating the real challenges and making reproducibility elusive. For clarity and a rigorous comparison, we classify OTTA techniques into three primary categories and subject them to benchmarks using the potent Vision Transformer (ViT) backbone to discover genuinely effective strategies. Our benchmarks span not only conventional corrupted datasets such as CIFAR-10/100-C and ImageNet-C but also real-world shifts embodied in CIFAR-10.1 and CIFAR-10-Warehouse, encapsulating variations across search engines and synthesized data by diffusion models. To gauge efficiency in online scenarios, we introduce novel evaluation metrics, inclusiv
    
[^27]: 在多语种惯用语境下生成延续

    Generating Continuations in Multilingual Idiomatic Contexts. (arXiv:2310.20195v1 [cs.CL])

    [http://arxiv.org/abs/2310.20195](http://arxiv.org/abs/2310.20195)

    本论文测试了生成性语言模型在多语种惯用语境中生成延续的能力，并发现模型在字面和惯用上下文中的表现相似，并且在两种语言中均具有鲁棒性。

    

    处理惯用或字面多词表达是理解和生成任何语言的关键方面。为包含惯用（或字面）表达的叙述生成具有上下文相关的延续的任务可以让我们测试生成性语言模型（LMs）理解非组合性比喻文本的纤细语言能力。我们使用两种不同语言（英语和葡萄牙语）的数据集在三种不同的训练设置下（零样本、少样本和微调）进行了一系列实验。我们的结果表明，模型在生成字面上下文的延续时略优于惯用上下文，但差距很小。此外，本研究中研究的模型在两种语言中表现出同样出色的性能，表明生成模型在执行此任务时具有鲁棒性。

    The ability to process idiomatic or literal multiword expressions is a crucial aspect of understanding and generating any language. The task of generating contextually relevant continuations for narratives containing idiomatic (or literal) expressions can allow us to test the ability of generative language models (LMs) in understanding nuanced language containing non-compositional figurative text. We conduct a series of experiments using datasets in two distinct languages (English and Portuguese) under three different training settings (zero-shot, few-shot, and fine-tuned). Our results suggest that the models are only slightly better at generating continuations for literal contexts than idiomatic contexts, with exceedingly small margins. Furthermore, the models studied in this work perform equally well across both languages, indicating the robustness of generative models in performing this task.
    
[^28]: 自监督预训练用于降水后处理

    Self-supervised Pre-training for Precipitation Post-processor. (arXiv:2310.20187v1 [cs.LG])

    [http://arxiv.org/abs/2310.20187](http://arxiv.org/abs/2310.20187)

    该论文提出了一种基于深度学习的降水后处理方法，使用自监督预训练和转移学习来提高数值天气预报模型的准确性。实验结果表明该方法在区域降水校正方面表现优于其他方法。

    

    为了预防危险天气事件，确保充足的局地降水预报提前时间至关重要。然而，全球变暖引起的气候变化增加了准确预测严重降水事件（如暴雨）的挑战。本工作提出了一种基于深度学习的降水后处理方法，用于数值天气预报（NWP）模型。降水后处理包括（i）自监督预训练，其中编码器的参数在大气物理领域的遮蔽变量重构上进行预训练，以及（ii）从预训练的编码器中转移学习到降水分割任务（目标领域）。我们还引入了一种启发式标记方法，以有效地训练类别不平衡的数据集。我们在区域NWP中的降水校正实验结果表明，所提出的方法优于其他方法。

    Securing sufficient forecast lead time for local precipitation is essential for preventing hazardous weather events. Nonetheless, global warming-induced climate change is adding to the challenge of accurately predicting severe precipitation events, such as heavy rainfall. In this work, we propose a deep learning-based precipitation post-processor approach to numerical weather prediction (NWP) models. The precipitation post-processor consists of (i) self-supervised pre-training, where parameters of encoder are pre-trained on the reconstruction of masked variables of the atmospheric physics domain, and (ii) transfer learning on precipitation segmentation tasks (target domain) from the pre-trained encoder. We also introduce a heuristic labeling approach for effectively training class-imbalanced datasets. Our experiment results in precipitation correction for regional NWP show that the proposed method outperforms other approaches.
    
[^29]: 通过引导学习发现技能

    Learning to Discover Skills through Guidance. (arXiv:2310.20178v1 [cs.LG])

    [http://arxiv.org/abs/2310.20178](http://arxiv.org/abs/2310.20178)

    提出了一种名为DISCO-DANCE的无监督技能发现算法，通过引导学习提高探索效果，并在具有挑战性的环境中优于其他方法。

    

    在无监督技能发现领域，主要挑战是有限的探索，主要是因为技能偏离其初始轨迹会受到重大惩罚。为了增强探索，最近的方法使用辅助奖励来最大化状态的认知不确定性或熵。然而，我们发现这些奖励的效果随着环境复杂性的增加而下降。因此，我们提出了一种新的无监督技能发现算法，DISCO-DANCE，它选择具有达到未探索状态潜力最高的引导技能，引导其他技能遵循引导技能，然后分散引导技能以最大化在未探索状态中的可区分性。实证评估表明，在具有挑战性的环境中，包括两个导航基准和一个连续控制基准，DISCO-DANCE优于其他无监督技能发现基线。DISCO-DANCE的定性可视化和代码。

    In the field of unsupervised skill discovery (USD), a major challenge is limited exploration, primarily due to substantial penalties when skills deviate from their initial trajectories. To enhance exploration, recent methodologies employ auxiliary rewards to maximize the epistemic uncertainty or entropy of states. However, we have identified that the effectiveness of these rewards declines as the environmental complexity rises. Therefore, we present a novel USD algorithm, skill discovery with guidance (DISCO-DANCE), which (1) selects the guide skill that possesses the highest potential to reach unexplored states, (2) guides other skills to follow guide skill, then (3) the guided skills are dispersed to maximize their discriminability in unexplored states. Empirical evaluation demonstrates that DISCO-DANCE outperforms other USD baselines in challenging environments, including two navigation benchmarks and a continuous control benchmark. Qualitative visualizations and code of DISCO-DANCE
    
[^30]: 基于图转换器的地理空间预测

    GraphTransformers for Geospatial Forecasting. (arXiv:2310.20174v1 [cs.AI])

    [http://arxiv.org/abs/2310.20174](http://arxiv.org/abs/2310.20174)

    本研究提出了一种新的基于图转换器的框架，用于改进地理空间序列轨迹预测。通过显式利用自动生成的图结构，可以显著提高地理空间轨迹预测的准确性。实验证明，该方法在飓风轨迹预测任务中表现优于基准模型。

    

    本文介绍了一种使用图转换器进行地理空间序列轨迹预测的新框架。通过观察多个序列，我们发现在这些序列之间会自动形成一个图结构，而这种结构在序列建模任务中通常被忽视。我们展示了通过显式利用这个图结构，地理空间轨迹预测可以显著改善。我们的图转换器方法在HURDAT数据集上，即对飓风轨迹进行6小时基础上的预测上，相比基于Transformer的基准模型有了显著的提升。

    In this paper we introduce a novel framework for trajectory prediction of geospatial sequences using GraphTransformers. When viewed across several sequences, we observed that a graph structure automatically emerges between different geospatial points that is often not taken into account for such sequence modeling tasks. We show that by leveraging this graph structure explicitly, geospatial trajectory prediction can be significantly improved. Our GraphTransformer approach improves upon state-of-the-art Transformer based baseline significantly on HURDAT, a dataset where we are interested in predicting the trajectory of a hurricane on a 6 hourly basis.
    
[^31]: 跨语言的多语言神经机器翻译中的鲁棒性是否可转移？

    Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?. (arXiv:2310.20162v1 [cs.AI])

    [http://arxiv.org/abs/2310.20162](http://arxiv.org/abs/2310.20162)

    本文研究了跨语言的多语言神经机器翻译中的鲁棒性是否可转移。通过一系列实验，我们发现从一个翻译方向获得的鲁棒性可以转移到其他翻译方向。

    

    鲁棒性是模型在面对干扰时保持性能的能力，对于开发可靠的自然语言处理系统至关重要。最近的研究通过对抗训练和数据增强在提高模型的鲁棒性方面取得了令人鼓舞的结果。然而，在机器翻译领域，大多数这些研究都集中在具有单一翻译方向的双语机器翻译上。在本文中，我们研究了跨不同语言的鲁棒性是否可转移于多语言神经机器翻译中。我们提出了一个鲁棒性转移分析协议，并进行了一系列实验。具体而言，我们使用字符、词和多级噪声来攻击多语言神经机器翻译模型的特定翻译方向，评估其他翻译方向的鲁棒性。我们的发现表明，从一个翻译方向获得的鲁棒性确实可以转移到其他翻译方向。

    Robustness, the ability of models to maintain performance in the face of perturbations, is critical for developing reliable NLP systems. Recent studies have shown promising results in improving the robustness of models through adversarial training and data augmentation. However, in machine translation, most of these studies have focused on bilingual machine translation with a single translation direction. In this paper, we investigate the transferability of robustness across different languages in multilingual neural machine translation. We propose a robustness transfer analysis protocol and conduct a series of experiments. In particular, we use character-, word-, and multi-level noises to attack the specific translation direction of the multilingual neural machine translation model and evaluate the robustness of other translation directions. Our findings demonstrate that the robustness gained in one translation direction can indeed transfer to other translation directions. Additionall
    
[^32]: 语言引导的视觉问答: 使用知识增强的提示来提升你的多模态语言模型

    Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts. (arXiv:2310.20159v1 [cs.CV])

    [http://arxiv.org/abs/2310.20159](http://arxiv.org/abs/2310.20159)

    本文研究了知识增强的视觉问答任务，提出了一种利用语言引导的多模态框架，并通过实验证明了语言引导是一种简单但有效的策略，可以提高视觉问答的准确性。

    

    视觉问答（VQA）是回答关于图像的问题的任务。这个任务假设了对图像和问题的理解，以提供自然语言回答。VQA近年来因其在机器人、教育和医疗等多个领域的潜在应用而受到关注。本文关注于知识增强的VQA，即回答问题需要常识知识、世界知识以及关于图像中不存在的思想和概念的推理能力。我们提出了一个多模态框架，利用语言引导（LG）形式的合理性、图像标题、场景图等来更准确地回答问题。我们在A-OKVQA、Science-QA、VSR和IconQA数据集的多选题问答任务上使用CLIP和BLIP模型进行了基准测试。结果表明，使用语言引导是一种简单而强大有效的视觉问答策略。我们的语言引导改进了问答的性能。

    Visual question answering (VQA) is the task of answering questions about an image. The task assumes an understanding of both the image and the question to provide a natural language answer. VQA has gained popularity in recent years due to its potential applications in a wide range of fields, including robotics, education, and healthcare. In this paper, we focus on knowledge-augmented VQA, where answering the question requires commonsense knowledge, world knowledge, and reasoning about ideas and concepts not present in the image. We propose a multimodal framework that uses language guidance (LG) in the form of rationales, image captions, scene graphs, etc to answer questions more accurately. We benchmark our method on the multi-choice question-answering task of the A-OKVQA, Science-QA, VSR, and IconQA datasets using CLIP and BLIP models. We show that the use of language guidance is a simple but powerful and effective strategy for visual question answering. Our language guidance improves
    
[^33]: MLatom 3: 用于增强计算化学模拟和工作流的机器学习平台

    MLatom 3: Platform for machine learning-enhanced computational chemistry simulations and workflows. (arXiv:2310.20155v1 [physics.chem-ph])

    [http://arxiv.org/abs/2310.20155](http://arxiv.org/abs/2310.20155)

    MLatom 3是一个机器学习平台，用于增强计算化学模拟和创建复杂工作流。用户可以选择多种模拟方法和预训练的ML模型进行能量计算、优化几何结构和模拟光谱等计算化学任务。

    

    机器学习在计算化学中越来越成为常用工具。MLatom 3 是一个程序包，旨在利用机器学习的强大能力增强典型的计算化学模拟，并创建复杂的工作流。这个开源包提供了多种选择，用户可以通过命令行选项、输入文件或使用MLatom作为Python包的脚本在他们的计算机上或通过XACS云计算平台运行模拟。计算化学家可以使用ML、量子力学和组合模型计算能量和热化学性质，优化几何结构，运行分子和量子动力学，并模拟(转动)振动，单光子UV/可见吸收和双光子吸收谱。用户可以从包含预训练ML模型的广泛方法库中进行选择。

    Machine learning (ML) is increasingly becoming a common tool in computational chemistry. At the same time, the rapid development of ML methods requires a flexible software framework for designing custom workflows. MLatom 3 is a program package designed to leverage the power of ML to enhance typical computational chemistry simulations and to create complex workflows. This open-source package provides plenty of choice to the users who can run simulations with the command line options, input files, or with scripts using MLatom as a Python package, both on their computers and on the online XACS cloud computing at XACScloud.com. Computational chemists can calculate energies and thermochemical properties, optimize geometries, run molecular and quantum dynamics, and simulate (ro)vibrational, one-photon UV/vis absorption, and two-photon absorption spectra with ML, quantum mechanical, and combined models. The users can choose from an extensive library of methods containing pre-trained ML models
    
[^34]: 与稀疏人类监督相适应的成本有效的交互多重保真度学习用于语言模型的适应性

    Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision. (arXiv:2310.20153v1 [cs.CL])

    [http://arxiv.org/abs/2310.20153](http://arxiv.org/abs/2310.20153)

    该论文提出了一种交互多重保真度学习框架，用于在有限的注释预算下开发小型特定领域的语言模型。该方法通过平衡低保真度自动注释和高保真度人类注释，以最大化模型性能。同时，还提出了一种增强注释多样性和信息性的查询策略。

    

    大型语言模型（LLMs）在各种任务中展示了卓越的能力。然而，由于它们在部署时的巨大规模、易受错误信息影响以及高昂的数据注释成本，它们在特定领域任务的适应性有限。我们提出了一种新颖的交互多重保真度学习（IMFL）框架，用于在有限的注释预算下开发小型特定领域的语言模型。我们的方法将领域特定的微调过程形式化为多重保真度学习问题，重点是确定平衡低保真度自动LLM注释和高保真度人类注释以最大化模型性能的最佳获取策略。我们进一步提出了一种增强注释多样性和信息性的探索-开发查询策略，结合了两个创新设计：1）从人类注释样本中选择上下文例子来改进LLM注释

    Large language models (LLMs) have demonstrated remarkable capabilities in various tasks. However, their suitability for domain-specific tasks, is limited due to their immense scale at deployment, susceptibility to misinformation, and more importantly, high data annotation costs. We propose a novel Interactive Multi-Fidelity Learning (IMFL) framework for the cost-effective development of small domain-specific LMs under limited annotation budgets. Our approach formulates the domain-specific fine-tuning process as a multi-fidelity learning problem, focusing on identifying the optimal acquisition strategy that balances between low-fidelity automatic LLM annotations and high-fidelity human annotations to maximize model performance. We further propose an exploration-exploitation query strategy that enhances annotation diversity and informativeness, incorporating two innovative designs: 1) prompt retrieval that selects in-context examples from human-annotated samples to improve LLM annotation
    
[^35]: 忘记你想忘记的：LLMs的高效遗忘方法

    Unlearn What You Want to Forget: Efficient Unlearning for LLMs. (arXiv:2310.20150v1 [cs.CL])

    [http://arxiv.org/abs/2310.20150](http://arxiv.org/abs/2310.20150)

    本论文提出了一种高效的遗忘框架来处理大语言模型（LLMs）中的隐私问题和数据保护违规。通过引入轻量级遗忘层到transformers中，并使用有选择的师生目标学习，我们能够在删除数据后有效地更新LLMs，而无需重新训练整个模型。实验证明了该方法的有效性。

    

    大语言模型（LLMs）通过预训练和记忆各种文本数据取得了重大进展，但这个过程可能面临隐私问题和数据保护规定的违规。因此，在不损害预测质量的情况下，能够轻松地从这些模型中删除与个人用户相关的数据变得越来越重要。为解决这些问题，本文提出了一种高效的遗忘框架，通过引入学习有选择的师生目标的轻量级遗忘层到transformers中，能够在数据删除后有效地更新LLMs，而无需对整个模型进行重新训练。此外，我们还引入了一种融合机制，以有效地组合不同的遗忘层，以处理一系列的遗忘操作。分类和生成任务的实验证明了我们方法的有效性。

    Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our 
    
[^36]: 具有交互感知行为预测和社交-注意力神经网络的自动驾驶车辆决策

    Decision-Making for Autonomous Vehicles with Interaction-Aware Behavioral Prediction and Social-Attention Neural Network. (arXiv:2310.20148v1 [cs.AI])

    [http://arxiv.org/abs/2310.20148](http://arxiv.org/abs/2310.20148)

    本研究提出了一种自动驾驶车辆决策的行为模型，将驾驶员的互动意图编码为社交心理参数，并开发了基于优化的控制器和基于注意机制的神经网络架构来解决自动驾驶车辆在交通中的决策制定问题。

    

    自动驾驶车辆需要在交通中与人类驾驶员互动来完成任务。因此，为了更好地理解周围交通的意图以促进任务的完成，将自动驾驶车辆配备人工推理是至关重要的。本文提出了一个行为模型，将驾驶员的互动意图编码为潜在的社交心理参数。利用贝叶斯滤波器，我们开发了一个基于优化的滚动地平面控制器，用于自动驾驶车辆的决策制定，考虑了交互驾驶员意图的不确定性。为了实现在线部署，我们设计了一个基于注意机制的神经网络架构，以在线估计参数先验来模仿行为模型。我们还提出了一个决策树搜索算法来解决在线决策问题。然后，我们评估了所提出的行为模型在实际轨迹方面的能力。

    Autonomous vehicles need to accomplish their tasks while interacting with human drivers in traffic. It is thus crucial to equip autonomous vehicles with artificial reasoning to better comprehend the intentions of the surrounding traffic, thereby facilitating the accomplishments of the tasks. In this work, we propose a behavioral model that encodes drivers' interacting intentions into latent social-psychological parameters. Leveraging a Bayesian filter, we develop a receding-horizon optimization-based controller for autonomous vehicle decision-making which accounts for the uncertainties in the interacting drivers' intentions. For online deployment, we design a neural network architecture based on the attention mechanism which imitates the behavioral model with online estimated parameter priors. We also propose a decision tree search algorithm to solve the decision-making problem online. The proposed behavioral model is then evaluated in terms of its capabilities for real-world trajector
    
[^37]: EELBERT:通过动态嵌入实现微型模型

    EELBERT: Tiny Models through Dynamic Embeddings. (arXiv:2310.20144v1 [cs.CL])

    [http://arxiv.org/abs/2310.20144](http://arxiv.org/abs/2310.20144)

    EELBERT是一种通过动态嵌入实现微型模型的方法，具有最小的准确性回归和显著的模型尺寸缩小。最小的模型UNO-EELBERT在GLUE得分上与完全训练的BERT-tiny相差4%，并且体积只有其15倍之一（1.2MB）。

    

    我们介绍了EELBERT，一种用于压缩基于Transformer的模型（例如BERT）的方法，对下游任务的准确性影响最小。这是通过将模型的输入嵌入层替换为动态的，即即时计算的嵌入实现来实现的。由于输入嵌入层占模型大小的重要部分，特别是对于较小的BERT变体，用嵌入计算函数替换该层有助于显著减小模型大小。在GLUE基准测试中的实证评估显示，我们的BERT变体（EELBERT）与传统BERT模型相比仅具有最小的回归。通过这种方法，我们能够开发出我们最小的模型UNO-EELBERT，其GLUE得分比完全训练的BERT-tiny高4％，同时体积小15倍（1.2MB）。

    We introduce EELBERT, an approach for compression of transformer-based models (e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is achieved by replacing the input embedding layer of the model with dynamic, i.e. on-the-fly, embedding computations. Since the input embedding layer accounts for a significant fraction of the model size, especially for the smaller BERT variants, replacing this layer with an embedding computation function helps us reduce the model size significantly. Empirical evaluation on the GLUE benchmark shows that our BERT variants (EELBERT) suffer minimal regression compared to the traditional BERT models. Through this approach, we are able to develop our smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully trained BERT-tiny, while being 15x smaller (1.2 MB) in size.
    
[^38]: 对比差异性预测编码

    Contrastive Difference Predictive Coding. (arXiv:2310.20141v1 [cs.LG])

    [http://arxiv.org/abs/2310.20141](http://arxiv.org/abs/2310.20141)

    本文介绍了一种时间差异版本的对比预测编码，通过将不同时间序列数据的片段组合在一起，来减少学习预测未来事件所需的数据量。实验证明，与先前的方法相比，我们的方法在成功率上提高了2倍，并且对于随机环境有更好的适应能力。

    

    预测和推理未来是许多时间序列问题的核心。例如，目标导向的强化学习可以被看作是学习表示以预测未来可能访问的状态。虽然先前的方法已经使用对比性预测编码来建模时间序列数据，但学习编码长期依赖通常需要大量的数据。在本文中，我们引入了一种时间差异版本的对比预测编码，将不同时间序列数据的片段组合在一起，以减少学习未来事件预测所需的数据量。我们将这种表示学习方法应用于导出目标导向的强化学习的离策略算法。实验证明，与先前的强化学习方法相比，我们的方法在成功率上实现了中位数提高2倍，并且可以更好地应对随机环境。在表格设置中，我们展示了我们的方法约为20倍。

    Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20
    
[^39]: 使用大型语言模型有效分类编程课程中学生求助请求

    Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models. (arXiv:2310.20105v1 [cs.CY])

    [http://arxiv.org/abs/2310.20105](http://arxiv.org/abs/2310.20105)

    本研究评估了GPT-3.5和GPT-4模型在分类编程课程中学生求助请求方面的性能，并发现它们可以通过大型语言模型的自动分类来提高教育系统的效能。

    

    准确分类与所寻求的帮助类型相关的学生求助请求可以实现针对性的有效响应。自动分类这类请求是非平凡的，但大型语言模型（LLMs）似乎提供了一种易于使用、经济实惠的解决方案。本研究评估了GPT-3.5和GPT-4模型在初级编程课程中对学生求助请求进行分类的性能。在零-shot测试中，GPT-3.5和GPT-4在大多数类别上表现出可比性，而GPT-4在与调试相关的子类别的分类上胜过GPT-3.5。对GPT-3.5模型进行微调可以提高其性能，以至于它在各类别之间的准确性和一致性上接近于两位人类评分者之间的观察结果。总体而言，本研究证明了使用LLMs通过自动分类学生需求来增强教育系统的可行性。

    The accurate classification of student help requests with respect to the type of help being sought can enable the tailoring of effective responses. Automatically classifying such requests is non-trivial, but large language models (LLMs) appear to offer an accessible, cost-effective solution. This study evaluates the performance of the GPT-3.5 and GPT-4 models for classifying help requests from students in an introductory programming class. In zero-shot trials, GPT-3.5 and GPT-4 exhibited comparable performance on most categories, while GPT-4 outperformed GPT-3.5 in classifying sub-categories for requests related to debugging. Fine-tuning the GPT-3.5 model improved its performance to such an extent that it approximated the accuracy and consistency across categories observed between two human raters. Overall, this study demonstrates the feasibility of using LLMs to enhance educational systems through the automated classification of student needs.
    
[^40]: 网页编程中的抄袭和AI助手滥用：不公平的利益和特点

    Plagiarism and AI Assistance Misuse in Web Programming: Unfair Benefits and Characteristics. (arXiv:2310.20104v1 [cs.AI])

    [http://arxiv.org/abs/2310.20104](http://arxiv.org/abs/2310.20104)

    这项研究关注编程教育中的抄袭和AI助手滥用问题，计划开发自动化工具帮助教师识别这些不当行为。实验结果显示，参与不当行为的学生在测试成绩上与独立完成时间短的学生相当。抄袭和使用AI助手的提交在细节上有所不同，后者更复杂且可读性较差。学生认为使用AI助手可能有益，但要得到适当承认。

    

    在编程教育中，抄袭和人工智能（AI）助手的滥用是新兴的问题。然而，关于网页编程方面的相关研究并不多。我们计划开发自动化工具，帮助教师识别这两种不当行为。为了充分了解问题，我们进行了一项控制实验，观察了不公平利益和特点。我们比较了学生独立完成网页编程任务、抄袭提交和使用AI助手（ChatGPT）的表现。我们的研究表明，参与这类不当行为的学生在测试成绩上与独立完成时间较短的学生相当。抄袭的提交与独立提交类似，只是在颜色和标识符名称等细节方面有所不同。使用AI助手的提交更加复杂，降低了可读性。学生认为使用AI助手可能有用，前提是得到适当的使用承认，尽管他们对可读性和...

    In programming education, plagiarism and misuse of artificial intelligence (AI) assistance are emerging issues. However, not many relevant studies are focused on web programming. We plan to develop automated tools to help instructors identify both misconducts. To fully understand the issues, we conducted a controlled experiment to observe the unfair benefits and the characteristics. We compared student performance in completing web programming tasks independently, with a submission to plagiarize, and with the help of AI assistance (ChatGPT). Our study shows that students who are involved in such misconducts get comparable test marks with less completion time. Plagiarized submissions are similar to the independent ones except in trivial aspects such as color and identifier names. AI-assisted submissions are more complex, making them less readable. Students believe AI assistance could be useful given proper acknowledgment of the use, although they are not convinced with readability and c
    
[^41]: 通过深度学习进行数据市场设计

    Data Market Design through Deep Learning. (arXiv:2310.20096v1 [cs.GT])

    [http://arxiv.org/abs/2310.20096](http://arxiv.org/abs/2310.20096)

    这项研究介绍了使用深度学习进行收入最优数据市场设计的应用，旨在扩展前沿研究领域。

    

    $\textit{数据市场设计}$问题是经济理论中的一个问题，旨在找到一组信号方案（统计实验），以最大化信息卖方的预期收入，其中每个实验揭示了卖方所知道的一些信息，并附带一个相应的价格[Bergemann et al., 2018]。每个买方在世界环境中都有自己的决策，并且他们对与特定实验相关联的信息的主观预期值来自于这个决策的改进，并且依赖于他们的先验和不同结果的价值。在具有多个买方的环境中，买方对实验的预期值也可能取决于卖给其他人的信息[Bonatti et al., 2022]。我们引入深度学习在收入最优数据市场设计中的应用，旨在扩展可以被理解和实现的边界。相对于之前关于拍卖设计的深度学习研究[D\"utting et al., 2023]，我们必须进行更多的研究来解决数据市场设计问题。

    The $\textit{data market design}$ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price [Bergemann et al., 2018]. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others [Bonatti et al., 2022]. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design [D\"utting et al., 2023], we must l
    
[^42]: 评估神经语言模型作为语言习得的认知模型

    Evaluating Neural Language Models as Cognitive Models of Language Acquisition. (arXiv:2310.20093v1 [cs.CL])

    [http://arxiv.org/abs/2310.20093](http://arxiv.org/abs/2310.20093)

    本文评估了神经语言模型作为语言习得的认知模型的潜力。作者认为用于评估句法能力的基准不够严格，并提出了使用严选数据集来探索语法结构基础的建议。

    

    尽管神经语言模型（LM）的训练方式与儿童语言习得存在明显的差异，但它们在许多技术任务上的成功为其作为语言科学理论的潜在相关性提供了支持。本文认为，评估LM的句法能力的一些主流基准可能不够严格。特别是，我们发现基于模板的基准缺乏语言理论和心理学研究中常见的结构多样性。当使用小规模数据来模拟儿童语言习得时，简单的基准模型可以轻松匹配LM。我们主张使用已经过大量母语者评估过梯度可接受性并设计用于探索语法结构基础的严选数据集。在其中一个这样的数据集LI-Adger上，LM评估句子的方式与人类语言处理不一致。

    The success of neural language models (LMs) on many technological tasks has brought about their potential relevance as scientific theories of language despite some clear differences between LM training and child language acquisition. In this paper we argue that some of the most prominent benchmarks for evaluating the syntactic capacities of LMs may not be sufficiently rigorous. In particular, we show that the template-based benchmarks lack the structural diversity commonly found in the theoretical and psychological studies of language. When trained on small-scale data modeling child language acquisition, the LMs can be readily matched by simple baseline models. We advocate for the use of the readily available, carefully curated datasets that have been evaluated for gradient acceptability by large pools of native speakers and are designed to probe the structural basis of grammar specifically. On one such dataset, the LI-Adger dataset, LMs evaluate sentences in a way inconsistent with hu
    
[^43]: 通过大型语言模型将总结和检索整合，增强个性化能力

    Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models. (arXiv:2310.20081v1 [cs.CL])

    [http://arxiv.org/abs/2310.20081](http://arxiv.org/abs/2310.20081)

    个性化是NLP系统中用户体验的关键，本文通过大型语言模型将总结和检索整合，提出了一种利用任务感知的总结增强个性化方法。

    

    个性化是自然语言处理(NLP)系统中用户体验的一个关键因素。我们提出了一种利用大型语言模型(LLM)来更好地个性化用户体验的方法。为了个性化语言模型的输出，一个直接的方法是将过去的用户数据并入语言模型的提示中，但这种方法会导致输入过长，超出输入长度限制，并且引起延迟和成本问题。现有方法通过选择性地提取相关的用户数据（即选择性检索）来解决这些挑战，以构建下游任务的提示。然而，基于检索的方法受限于潜在的信息丢失、缺乏更深入的用户理解和冷启动挑战。为了克服这些限制，我们提出了一种新颖的总结增强方法，通过扩展检索增强个性化与任务感知相结合。

    Personalization, the ability to tailor a system to individual users, is an essential factor in user experience with natural language processing (NLP) systems. With the emergence of Large Language Models (LLMs), a key question is how to leverage these models to better personalize user experiences. To personalize a language model's output, a straightforward approach is to incorporate past user data into the language model prompt, but this approach can result in lengthy inputs exceeding limitations on input length and incurring latency and cost issues. Existing approaches tackle such challenges by selectively extracting relevant user data (i.e. selective retrieval) to construct a prompt for downstream tasks. However, retrieval-based methods are limited by potential information loss, lack of more profound user understanding, and cold-start challenges. To overcome these limitations, we propose a novel summary-augmented approach by extending retrieval-augmented personalization with task-awar
    
[^44]: FOCAL: 在因子化正交潜空间中的多模时间序列感知信号中的对比学习

    FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space. (arXiv:2310.20071v1 [cs.AI])

    [http://arxiv.org/abs/2310.20071](http://arxiv.org/abs/2310.20071)

    本文提出了FOCAL框架，可以通过自监督训练从多模时间序列感知信号中提取全面的特征。它通过使每个模态都编码到一个因子化的潜空间中，同时突出共享特征和专属特征，从而有效处理感知模态之间的共享信息和专属信息。

    

    本文提出了一个名为FOCAL的新型对比学习框架，通过自监督训练从多模时间序列感知信号中提取全面的特征。现有的多模对比框架主要依赖于感知模态之间的共享信息，但没有明确考虑对理解底层感知物理学至关重要的专属模态信息。此外，针对时间序列的对比框架没有适当处理时间信息局部性。FOCAL解决了这些挑战，具体贡献如下：首先，对于给定的多模时间序列，将每个模态编码到一个因子化的潜空间中，该潜空间由共享特征和彼此正交的专属特征组成。共享空间通过模态匹配目标强调跨感知模态的特征模式一致性。相反，专属空间通过一个目标提取模态独占信息。

    This paper proposes a novel contrastive learning framework, called FOCAL, for extracting comprehensive features from multimodal time-series sensing signals through self-supervised training. Existing multimodal contrastive frameworks mostly rely on the shared information between sensory modalities, but do not explicitly consider the exclusive modality information that could be critical to understanding the underlying sensing physics. Besides, contrastive frameworks for time series have not handled the temporal information locality appropriately. FOCAL solves these challenges by making the following contributions: First, given multimodal time series, it encodes each modality into a factorized latent space consisting of shared features and private features that are orthogonal to each other. The shared space emphasizes feature patterns consistent across sensory modalities through a modal-matching objective. In contrast, the private space extracts modality-exclusive information through a tr
    
[^45]: Vignat: 通过图注意力网络学习代码语义的漏洞识别方法

    Vignat: Vulnerability identification by learning code semantics via graph attention networks. (arXiv:2310.20067v1 [cs.CR])

    [http://arxiv.org/abs/2310.20067](http://arxiv.org/abs/2310.20067)

    本文提出了一种名为Vignat的漏洞识别框架，通过学习代码的图级语义表示，使用图注意力网络进行漏洞检测。实验结果表明，在C库的可靠数据集上，Vignat能够达到57.38%的准确率，同时提供了对漏洞模式的宝贵洞见。

    

    漏洞识别对于保护软件系统免受网络攻击至关重要。然而，庞大的项目拥有超过数百万行代码，复杂的依赖关系使得传统的静态和动态方法难以实施。此外，不同类型的漏洞的语义结构差异很大，并且可能同时发生，使得通用的基于规则的方法难以扩展。在本文中，我们提出了一种名为Vignat的新型基于注意力机制的漏洞识别框架，通过学习代码的图级语义表示来识别漏洞。我们将代码表示为细粒度的代码属性图，并使用图注意力网络进行漏洞检测。实验结果表明，Vignat在来自流行的C库的可靠数据集上能够达到57.38%的准确率。此外，我们的图注意力网络的可解释性为漏洞模式提供了宝贵的洞见。

    Vulnerability identification is crucial to protect software systems from attacks for cyber-security. However, huge projects have more than millions of lines of code, and the complex dependencies make it hard to carry out traditional static and dynamic methods. Furthermore, the semantic structure of various types of vulnerabilities differs greatly and may occur simultaneously, making general rule-based methods difficult to extend. In this paper, we propose \textit{Vignat}, a novel attention-based framework for identifying vulnerabilities by learning graph-level semantic representations of code. We represent codes with code property graphs (CPGs) in fine grain and use graph attention networks (GATs) for vulnerability detection. The results show that Vignat is able to achieve $57.38\%$ accuracy on reliable datasets derived from popular C libraries. Furthermore, the interpretability of our GATs provides valuable insights into vulnerability patterns.
    
[^46]: 概念对齐作为价值对齐的先决条件

    Concept Alignment as a Prerequisite for Value Alignment. (arXiv:2310.20059v1 [cs.AI])

    [http://arxiv.org/abs/2310.20059](http://arxiv.org/abs/2310.20059)

    价值对齐对于构建安全可靠的人工智能系统至关重要。这篇论文分析了概念对齐问题，并展示了忽视概念对齐可能导致价值错位的情况。通过联合推理一个人的概念和价值，可以最小化这类失误。实验结果显示人类在有意行动时会考虑代理人所使用的概念。

    

    价值对齐对于构建可以安全可靠地与人类互动的人工智能系统至关重要。然而，一个人所重视的事物 - 甚至是他能够重视的事物 - 取决于他们当前用于理解和评估世界发生的事情的概念。价值依赖于概念，这意味着概念对齐是实现价值对齐的先决条件 - 代理需要将其对情境的表示与人类的情境表示进行对齐，以成功地对齐其价值观。在这里，我们在反向强化学习设置下对概念对齐问题进行了正式分析，展示了忽视概念对齐可能导致系统性的价值错位，并描述了一种能够通过联合推理一个人的概念和价值来最小化此类失误的方法。此外，我们报告了与人类参与者的实验结果，显示人类在有意行动时会考虑代理人所使用的概念，与我们的联合推理一致。

    Value alignment is essential for building AI systems that can safely and reliably interact with people. However, what a person values -- and is even capable of valuing -- depends on the concepts that they are currently using to understand and evaluate what happens in the world. The dependence of values on concepts means that concept alignment is a prerequisite for value alignment -agents need to align their representation of a situation with that of humans in order to successfully align their values. Here, we formally analyze the concept alignment problem in the inverse reinforcement learning setting, show how neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person's concepts and values. Additionally, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint re
    
[^47]: 有约束的层次蒙特卡洛信念状态规划

    Constrained Hierarchical Monte Carlo Belief-State Planning. (arXiv:2310.20054v1 [cs.AI])

    [http://arxiv.org/abs/2310.20054](http://arxiv.org/abs/2310.20054)

    有约束的层次蒙特卡洛信念状态规划（COBeTS）通过使用分层分解和约束选项控制器，将在线基于搜索的CPOMDP规划扩展到大型机器人问题，并能同时满足约束和奖励目标。

    

    有约束的部分可观察马尔可夫决策过程（CPOMDPs）中的最优规划在满足硬性成本约束的同时最大化奖励目标，推广了状态和过渡不确定性下的安全规划。然而，在大型或连续的问题域中进行在线CPOMDP规划非常困难。在许多大型机器人领域，通过使用高层动作原语（选项）为低层控制提供工具，分层分解可以简化规划。我们引入了有约束的选项信念树搜索（COBeTS）来利用这个层次结构，将在线基于搜索的CPOMDP规划扩展到大型机器人问题。我们证明如果原始选项控制器被定义为满足指定的约束预算，那么COBeTS将随时满足约束。否则，COBeTS将引导搜索朝着安全的选项原语序列，并且可以使用分层监控来实现运行时安全。我们在几个安全关键的约束问题中展示了COBeTS。

    Optimal plans in Constrained Partially Observable Markov Decision Processes (CPOMDPs) maximize reward objectives while satisfying hard cost constraints, generalizing safe planning under state and transition uncertainty. Unfortunately, online CPOMDP planning is extremely difficult in large or continuous problem domains. In many large robotic domains, hierarchical decomposition can simplify planning by using tools for low-level control given high-level action primitives (options). We introduce Constrained Options Belief Tree Search (COBeTS) to leverage this hierarchy and scale online search-based CPOMDP planning to large robotic problems. We show that if primitive option controllers are defined to satisfy assigned constraint budgets, then COBeTS will satisfy constraints anytime. Otherwise, COBeTS will guide the search towards a safe sequence of option primitives, and hierarchical monitoring can be used to achieve runtime safety. We demonstrate COBeTS in several safety-critical, constrain
    
[^48]: 看我，不是重播！SurpriseNet：受异常检测启发的类别增量学习的异常检测。

    Look At Me, No Replay! SurpriseNet: Anomaly Detection Inspired Class Incremental Learning. (arXiv:2310.20052v1 [cs.AI])

    [http://arxiv.org/abs/2310.20052](http://arxiv.org/abs/2310.20052)

    SurpriseNet提出了一个解决灾难性干扰和跨任务知识学习问题的方案，通过参数隔离方法和受异常检测启发的自编码器来实现。

    

    连续学习致力于创建人工神经网络，能够通过在一系列任务上的增量训练中积累知识和技能。连续学习的主要挑战是灾难性干扰，即新知识覆盖或干扰过去的知识，导致遗忘。与之相关的问题是学习“跨任务知识”，模型无法获取和保留有助于区分跨任务边界上的类别的知识。解决这两个问题的常见方法是“重播”，即利用有限的过去实例缓冲区来学习跨任务知识并减轻灾难性干扰。然而，这些方法的一个显著缺点是倾向于过度拟合有限的重播缓冲区。相比之下，我们提出的解决方案SurpriseNet通过采用参数隔离方法解决灾难性干扰，并使用受异常检测启发的自编码器学习跨任务知识。

    Continual learning aims to create artificial neural networks capable of accumulating knowledge and skills through incremental training on a sequence of tasks. The main challenge of continual learning is catastrophic interference, wherein new knowledge overrides or interferes with past knowledge, leading to forgetting. An associated issue is the problem of learning "cross-task knowledge," where models fail to acquire and retain knowledge that helps differentiate classes across task boundaries. A common solution to both problems is "replay," where a limited buffer of past instances is utilized to learn cross-task knowledge and mitigate catastrophic interference. However, a notable drawback of these methods is their tendency to overfit the limited replay buffer. In contrast, our proposed solution, SurpriseNet, addresses catastrophic interference by employing a parameter isolation method and learning cross-task knowledge using an auto-encoder inspired by anomaly detection. SurpriseNet is a
    
[^49]: SURF: GNN预测流体动力学的泛化性能评估

    SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics. (arXiv:2310.20049v1 [cs.LG])

    [http://arxiv.org/abs/2310.20049](http://arxiv.org/abs/2310.20049)

    提出了一个名为SURF的基准测试，用于评估和比较基于图的学习流体模拟器的泛化能力。SURF包括各种数据集和具体的性能和泛化度量指标。通过深入研究两种最先进的模型，我们证明了SURF的适用性。

    

    模拟流体动力学对于设计和开发过程至关重要，涵盖了从简单阀门到复杂涡轮机械的范围。准确求解潜在的物理方程具有计算成本高的特点。因此，基于学习的求解器在网格上建模相互作用并具有显著的加速优势。然而，目前尚不清楚这些模型在多大程度上真正理解潜在的物理原理，并能够实现泛化而非插值。泛化是通用流体模拟器的关键要求，它应该能够适应不同的拓扑结构、分辨率或热力学范围。我们提出了SURF，这是一个旨在测试学习的基于图的流体模拟器的泛化能力的基准测试。SURF包括各个数据集，并提供用于评估和比较不同模型的具体性能和泛化度量指标。我们通过深入研究两种最先进的模型，实证地证明了SURF的适用性。

    Simulating fluid dynamics is crucial for the design and development process, ranging from simple valves to complex turbomachinery. Accurately solving the underlying physical equations is computationally expensive. Therefore, learning-based solvers that model interactions on meshes have gained interest due to their promising speed-ups. However, it is unknown to what extent these models truly understand the underlying physical principles and can generalize rather than interpolate. Generalization is a key requirement for a general-purpose fluid simulator, which should adapt to different topologies, resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to test the \textit{generalization} of learned graph-based fluid simulators. SURF comprises individual datasets and provides specific performance and generalization metrics for evaluating and comparing different models. We empirically demonstrate the applicability of SURF by thoroughly investigating the two state-of-the
    
[^50]: 用于临床总结中事实对齐的合成模仿编辑反馈

    Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization. (arXiv:2310.20033v1 [cs.CL])

    [http://arxiv.org/abs/2310.20033](http://arxiv.org/abs/2310.20033)

    本文提出了一种使用ChatGPT来生成高质量反馈数据以改善临床笔记总结的事实一致性的新方法。

    

    大型语言模型（LLMs）如GPT和LLaMA系列在捕捉和浓缩关键上下文信息及在总结任务中实现最先进的性能方面表现出了异常能力。然而，社区对这些模型的虚构问题的担忧仍在不断上升。LLMs有时会生成虚构的摘要，这在临床领域的NLP任务（例如临床笔记总结）中可能会导致严重错误的诊断。使用人类反馈对LLMs进行微调已经显示出在生成过程中实现事实一致性的承诺，但这种训练过程需要高质量的人工注释数据，而在临床领域获取这样的数据可能非常昂贵。在这项工作中，我们提出了一种新的管道，使用ChatGPT代替人类专家生成高质量的反馈数据，以改善临床笔记总结的事实一致性。

    Large Language Models (LLMs) like the GPT and LLaMA families have demonstrated exceptional capabilities in capturing and condensing critical contextual information and achieving state-of-the-art performance in the summarization task. However, community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using human feedback has shown the promise of aligning LLMs to be factually consistent during generation, but such training procedure requires high-quality human-annotated data, which can be extremely expensive to get in the clinical domain. In this work, we propose a new pipeline using ChatGPT instead of human experts to generate high-quality feedback data for improving factual consistency in the clinical note summari
    
[^51]: GOPlan:通过学习模型进行计划的目标条件下的离线强化学习

    GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models. (arXiv:2310.20025v1 [cs.LG])

    [http://arxiv.org/abs/2310.20025](http://arxiv.org/abs/2310.20025)

    GOPlan是一个使用学习模型进行计划的目标条件下的离线强化学习方法，通过预训练先验策略和使用重新分析方法生成虚构轨迹，用以提高性能和处理有限数据预算和未见目标泛化的能力。

    

    离线目标条件下的强化学习（GCRL）为从多样化和多任务的离线数据集中学习通用策略提供了可行的范例。尽管近期取得了显著进展，但主导的离线GCRL方法仍然受限于无模型方法，限制了它们应对有限数据预算和未见目标泛化的能力。在这项工作中，我们提出了一种新的两阶段模型为基础的框架，Goal-conditioned Offline Planning（GOPlan），包括（1）预训练一个能够捕捉多目标数据集中多模态动作分布的先验策略；（2）利用规划的重新分析方法为微调策略生成虚构轨迹。具体而言，先验策略基于一个具有明显模式分离的带优势权重的条件生成对抗网络，以克服超出分布（OOD）动作的缺点。为进一步优化策略，重新分析方法通过规划生成高质量的虚构数据。

    Offline goal-conditioned RL (GCRL) offers a feasible paradigm to learn general-purpose policies from diverse and multi-task offline datasets. Despite notable recent progress, the predominant offline GCRL methods have been restricted to model-free approaches, constraining their capacity to tackle limited data budgets and unseen goal generalization. In this work, we propose a novel two-stage model-based framework, Goal-conditioned Offline Planning (GOPlan), including (1) pretraining a prior policy capable of capturing multi-modal action distribution within the multi-goal dataset; (2) employing the reanalysis method with planning to generate imagined trajectories for funetuning policies. Specifically, the prior policy is based on an advantage-weighted Conditioned Generative Adversarial Networks that exhibits distinct mode separation to overcome the pitfalls of out-of-distribution (OOD) actions. For further policy optimization, the reanalysis method generates high-quality imaginary data by
    
[^52]: 自组织机器人网络的拓扑可恢复性预测：一种数据驱动的容错方法

    Topology Recoverability Prediction for Ad-Hoc Robot Networks: A Data-Driven Fault-Tolerant Approach. (arXiv:2310.20024v1 [cs.RO])

    [http://arxiv.org/abs/2310.20024](http://arxiv.org/abs/2310.20024)

    本论文提出了一种基于数据驱动的容错方法来预测自组织机器人网络的拓扑可恢复性。通过将问题转化为二分类问题，并使用贝叶斯高斯混合模型，该方法通过前故障和后故障的两个不同预测路径预测典型问题的解决方案。结果表明，与当前文献中最佳策略相比，该方法在解决拓扑可恢复性预测问题上取得了成功。

    

    自组织机器人网络中的故障可能会严重扰乱其拓扑结构，导致其子集之间失去连接。通常情况下，对于大规模自组织机器人网络，进行最佳拓扑合成是资源密集型且耗时的，难以实时完成。只有当任何故障发生后拓扑恢复的概率超过不可恢复的概率时，才应该执行拓扑重新计算。我们将此问题定义为一个二分类问题，并基于贝叶斯高斯混合模型开发了一个基于数据驱动的模型，通过两个不同的故障前和故障后预测路径来预测典型问题的解决方案。通过整合这些路径的预测结果，与文献中最佳的当前策略相比，明确显示了我们的模型在解决拓扑（不）可恢复性预测问题方面的成功。

    Faults occurring in ad-hoc robot networks may fatally perturb their topologies leading to disconnection of subsets of those networks. Optimal topology synthesis is generally resource-intensive and time-consuming to be done in real time for large ad-hoc robot networks. One should only perform topology re-computations if the probability of topology recoverability after the occurrence of any fault surpasses that of its irrecoverability. We formulate this problem as a binary classification problem. Then, we develop a two-pathway data-driven model based on Bayesian Gaussian mixture models that predicts the solution to a typical problem by two different pre-fault and post-fault prediction pathways. The results, obtained by the integration of the predictions of those pathways, clearly indicate the success of our model in solving the topology (ir)recoverability prediction problem compared to the best of current strategies found in the literature.
    
[^53]: 多尺度特征归因的异常点检测

    Multiscale Feature Attribution for Outliers. (arXiv:2310.20012v1 [cs.LG])

    [http://arxiv.org/abs/2310.20012](http://arxiv.org/abs/2310.20012)

    本论文提出了一种逆多尺度遮挡的特征归因方法，用于识别异常点，并且在Dark Energy Survey Instrument中的星系光谱异常点上取得了更易解释的结果。

    

    机器学习技术能够快速且可靠地识别海量数据集中的异常点，比人工检查要快得多。但是，一旦发现这些异常点，很快就会产生一个问题：哪些特征使得这个输入成为异常？我们提出了一种新的特征归因方法，称为逆多尺度遮挡，专为异常点设计，因为我们对要识别的特征类型了解很少，并且预计模型性能可能不可靠，因为异常的测试数据很可能超过了训练数据的限制。我们在Dark Energy Survey Instrument检测到的星系光谱异常点上展示了我们的方法，并发现其结果比其他归因方法更易解释。

    Machine learning techniques can automatically identify outliers in massive datasets, much faster and more reproducible than human inspection ever could. But finding such outliers immediately leads to the question: which features render this input anomalous? We propose a new feature attribution method, Inverse Multiscale Occlusion, that is specifically designed for outliers, for which we have little knowledge of the type of features we want to identify and expect that the model performance is questionable because anomalous test data likely exceed the limits of the training data. We demonstrate our method on outliers detected in galaxy spectra from the Dark Energy Survey Instrument and find its results to be much more interpretable than alternative attribution approaches.
    
[^54]: 进化桌面游戏设计：以“风险游戏”为案例研究

    Evolutionary Tabletop Game Design: A Case Study in the Risk Game. (arXiv:2310.20008v1 [cs.AI])

    [http://arxiv.org/abs/2310.20008](http://arxiv.org/abs/2310.20008)

    本研究提出了进化游戏设计方法的扩展，通过生成《风险》游戏的新变体来验证该方法。结果显示生成的新变体拥有更小的地图和更短的比赛时间，并产生更加平衡的游戏对局。

    

    手动创造和评估游戏是一项艰巨而费时的任务。程序生成内容可以通过创建游戏元素来提供帮助，但通常不能生成完整的游戏。进化游戏设计将进化算法与自动化测试相结合，已用于创建具备简单设备的新颖桌面游戏；然而，原有方法并不包括带有骰子、卡牌和地图等复杂桌面游戏。本研究提出了将该方法扩展到桌面游戏的方式，并通过生成《风险》这款军事策略游戏的变体来评估该过程。我们使用遗传算法进化所选参数，并利用规则中心代理测试游戏，并使用多个质量标准评估生成的新变体。结果显示我们创造了原始游戏的新变体，这些变体具有较小的地图，导致比赛时间更短。此外，这些变体产生了更加平衡的比赛。

    Creating and evaluating games manually is an arduous and laborious task. Procedural content generation can aid by creating game artifacts, but usually not an entire game. Evolutionary game design, which combines evolutionary algorithms with automated playtesting, has been used to create novel board games with simple equipment; however, the original approach does not include complex tabletop games with dice, cards, and maps. This work proposes an extension of the approach for tabletop games, evaluating the process by generating variants of Risk, a military strategy game where players must conquer map territories to win. We achieved this using a genetic algorithm to evolve the chosen parameters, as well as a rules-based agent to test the games and a variety of quality criteria to evaluate the new variations generated. Our results show the creation of new variations of the original game with smaller maps, resulting in shorter matches. Also, the variants produce more balanced matches, main
    
[^55]: 提升强化学习中汤普森采样的贝叶斯遗憾界

    Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning. (arXiv:2310.20007v1 [stat.ML])

    [http://arxiv.org/abs/2310.20007](http://arxiv.org/abs/2310.20007)

    本文在多种情境下证明了汤普森采样在强化学习中的贝叶斯遗憾界，并通过对信息比的精确分析提出了一个基于时间不均匀强化学习问题的上界估计。同时，本文找到了各种设置中具体的界限，并讨论了这些结果是第一个其类别或改进了最先进方法的情况。

    

    本文证明了在多种情境下，汤普森采样在强化学习中的第一个贝叶斯遗憾界。我们利用离散的代理环境简化学习问题，并通过后验一致性对信息比进行了精确分析。这导致了一个基于时间不均匀强化学习问题的上界估计为$\widetilde{O}(H\sqrt{d_{l_1}T})$，其中$H$为回合长度，$d_{l_1}$为环境空间的Kolmogorov $l_1$维度。然后，我们在各种设置中找到了$d_{l_1}$的具体界限，比如表格、线性和有限混合，讨论了我们的结果是第一个其类别或改进了最先进方法的情况。

    In this paper, we prove the first Bayesian regret bounds for Thompson Sampling in reinforcement learning in a multitude of settings. We simplify the learning problem using a discrete set of surrogate environments, and present a refined analysis of the information ratio using posterior consistency. This leads to an upper bound of order $\widetilde{O}(H\sqrt{d_{l_1}T})$ in the time inhomogeneous reinforcement learning problem where $H$ is the episode length and $d_{l_1}$ is the Kolmogorov $l_1-$dimension of the space of environments. We then find concrete bounds of $d_{l_1}$ in a variety of settings, such as tabular, linear and finite mixtures, and discuss how how our results are either the first of their kind or improve the state-of-the-art.
    
[^56]: 揭示学习的局部搜索启发式的局限性: 你是最强大的温顺者吗？

    Unveiling the Limits of Learned Local Search Heuristics: Are You the Mightiest of the Meek?. (arXiv:2310.19990v1 [cs.AI])

    [http://arxiv.org/abs/2310.19990](http://arxiv.org/abs/2310.19990)

    本研究对学习的局部搜索启发式的限制进行了全面调查，结果表明，基于禁忌搜索的简单学习启发式超越了最先进的学习启发式方法。

    

    近年来，将神经网络与局部搜索启发式相结合已经在组合优化领域变得流行。尽管这种方法需要相当大的计算需求，但它在最小的人工工程投入下展现出了很有希望的结果。然而，我们发现了这些整合尝试的实证评估中存在三个关键限制。首先，中等复杂性和弱基线的情况在准确评估基于学习的方法的有效性时是一个挑战。其次，缺乏消融研究使得准确地量化和归因于深度学习架构的改进变得困难。最后，在不同分布下学习启发式的泛化性仍未被充分探索。在这项研究中，我们对这些被识别出的限制进行了全面的调查。令人惊讶的是，我们证明了基于禁忌搜索的简单学习启发式超越了最先进的学习启发式方法。

    In recent years, combining neural networks with local search heuristics has become popular in the field of combinatorial optimization. Despite its considerable computational demands, this approach has exhibited promising outcomes with minimal manual engineering. However, we have identified three critical limitations in the empirical evaluation of these integration attempts. Firstly, instances with moderate complexity and weak baselines pose a challenge in accurately evaluating the effectiveness of learning-based approaches. Secondly, the absence of an ablation study makes it difficult to quantify and attribute improvements accurately to the deep learning architecture. Lastly, the generalization of learned heuristics across diverse distributions remains underexplored. In this study, we conduct a comprehensive investigation into these identified limitations. Surprisingly, we demonstrate that a simple learned heuristic based on Tabu Search surpasses state-of-the-art (SOTA) learned heurist
    
[^57]: BioInstruct:用于生物医学自然语言处理的大型语言模型指令调整

    BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing. (arXiv:2310.19975v1 [cs.CL])

    [http://arxiv.org/abs/2310.19975](http://arxiv.org/abs/2310.19975)

    BioInstruct是一个用于生物医学自然语言处理的大型语言模型指令调整方法，通过引入针对性指令数据集BioInstruct，通过GPT-4语言模型进行精调，优化了模型在生物医学自然语言处理中的性能。

    

    大型语言模型通过在大量数据上进行预训练，然后进行特定领域的指令调整，在许多自然语言处理任务中取得了巨大成功。然而，在生物医学领域只发表了很少的指令。为了解决这个问题，我们引入了BioInstruct，这是一个定制的任务特定指令数据集，包含超过25,000个示例。通过使用三个人工筛选的指令样本，以GPT-4语言模型作为提示，精调大型语言模型，我们旨在优化其在生物医学自然语言处理中的性能。我们对LLaMA LLMs (1&2,7B&13B)进行了指令调整，并在生物医学自然语言处理应用中进行了评估，包括信息提取、问答和文本生成。我们还评估了指令如何对模型性能的贡献，使用了多任务学习原则。

    Large language models (LLMs) has achieved a great success in many natural language processing (NLP) tasks. This is achieved by pretraining of LLMs on vast amount of data and then instruction tuning to specific domains. However, only a few instructions in the biomedical domain have been published. To address this issue, we introduce BioInstruct, a customized task-specific instruction dataset containing more than 25,000 examples. This dataset was generated attractively by prompting a GPT-4 language model with a three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical natural language processing (BioNLP). We conducted instruction tuning on the LLaMA LLMs (1\&2, 7B\&13B) and evaluated them on BioNLP applications, including information extraction, question answering, and text generation. We also evaluated how instructions contributed to model performance using multi-tasking learning principl
    
[^58]: ExPT: 用于少样本实验设计的合成预训练方法

    ExPT: Synthetic Pretraining for Few-Shot Experimental Design. (arXiv:2310.19961v1 [cs.LG])

    [http://arxiv.org/abs/2310.19961](http://arxiv.org/abs/2310.19961)

    ExPT是一种用于少样本实验设计的合成预训练方法，通过将条件生成任务应用于少量带标签的样本和期望输出，生成最优的输入设计。这种方法在不依赖主动数据收集或大规模标记数据集的情况下，在现实场景中具有实用性。

    

    实验设计是许多科学和工程领域中的一个基本问题。在这个问题中，由于现实世界设计评估的时间、金钱和安全成本，样本效率非常重要。现有的方法要么依赖主动数据收集，要么依赖对过去实验的大规模标记数据集的访问，这使得它们在许多现实场景中不可行。在这项工作中，我们解决了少样本实验设计的更具挑战性、现实的环境，其中只有少量带标签的输入设计样本及其相应的数值可用。我们将这个问题看作是一个条件生成任务，在这个任务中，模型根据少量带标签的样本和期望的输出条件生成最优的输入设计。为此，我们引入了实验预训练变换器（ExPT），这是一个用于少样本实验设计的基础模型，它采用合成预训练与上下文学习的新颖组合。在ExPT中，我们只假设对有限的上下文知识有了解。

    Experimental design is a fundamental problem in many science and engineering fields. In this problem, sample efficiency is crucial due to the time, money, and safety costs of real-world design evaluations. Existing approaches either rely on active data collection or access to large, labeled datasets of past experiments, making them impractical in many real-world scenarios. In this work, we address the more challenging yet realistic setting of few-shot experimental design, where only a few labeled data points of input designs and their corresponding values are available. We approach this problem as a conditional generation task, where a model conditions on a few labeled examples and the desired output to generate an optimal input design. To this end, we introduce Experiment Pretrained Transformers (ExPT), a foundation model for few-shot experimental design that employs a novel combination of synthetic pretraining with in-context learning. In ExPT, we only assume knowledge of a finite co
    
[^59]: 深度学习在时空大数据中的应用：机遇和挑战展望

    Deep Learning for Spatiotemporal Big Data: A Vision on Opportunities and Challenges. (arXiv:2310.19957v1 [cs.LG])

    [http://arxiv.org/abs/2310.19957](http://arxiv.org/abs/2310.19957)

    本文介绍了深度学习在时空大数据中的应用，讨论了机遇和挑战，并指出了一些未来研究方向。

    

    随着全球定位系统、遥感和计算模拟的进步，来自各个应用领域的大量时空数据正在以越来越快的速度被收集。涵盖的应用领域包括地球科学、农业、智慧城市和公共安全等。这种新兴的地理空间和时空大数据，结合深度学习技术的最新进展，为解决以往无法实现的问题提供了新机会。例如，遥感研究人员可以利用地球图像大数据训练基础模型，用于众多土地覆盖和土地利用建模任务。沿海模拟学家可以训练人工智能替代模型加速数值模拟。然而，时空大数据的独特特征给深度学习技术带来了新的挑战。本文展望了时空大数据的各种类型，讨论了深度学习在时空大数据中的新研究机会，并列举了一些未来研究方向。

    With advancements in GPS, remote sensing, and computational simulation, an enormous volume of spatiotemporal data is being collected at an increasing speed from various application domains, spanning Earth sciences, agriculture, smart cities, and public safety. Such emerging geospatial and spatiotemporal big data, coupled with recent advances in deep learning technologies, foster new opportunities to solve problems that have not been possible before. For instance, remote sensing researchers can potentially train a foundation model using Earth imagery big data for numerous land cover and land use modeling tasks. Coastal modelers can train AI surrogates to speed up numerical simulations. However, the distinctive characteristics of spatiotemporal big data pose new challenges for deep learning technologies. This vision paper introduces various types of spatiotemporal big data, discusses new research opportunities in the realm of deep learning applied to spatiotemporal big data, lists the un
    
[^60]: 条件非线性自动编码器用于轨迹预测

    Conditional Unscented Autoencoders for Trajectory Prediction. (arXiv:2310.19944v1 [cs.RO])

    [http://arxiv.org/abs/2310.19944](http://arxiv.org/abs/2310.19944)

    本文提出了使用条件非线性自动编码器(CVAE)进行轨迹预测的方法，通过利用变分自动编码器(VAE)中的非线性采样过程和其他改进，超越了现有技术，为自动驾驶领域的轨迹预测提供了更好的性能。

    

    条件变分自动编码器(CVAE)是自动驾驶轨迹预测中最常用的模型之一。它将驾驶环境和真实未来关系建立在概率隐空间中，并利用此空间生成预测。本文对CVAE的关键组件提出了挑战。我们利用变分自动编码器(VAE)领域的最新进展，发现变化采样过程可以极大地提高性能。我们发现非线性采样能够更适合于轨迹预测，而随机采样可能带来潜在的问题。我们进一步提出了其他改进，包括更结构化的混合隐空间，以及一种新颖、可能更具表达力的CVAE推理方法。我们通过在INTERACTION预测数据集上进行评估，展示了我们模型的广泛适用性，超过了现有技术的表现。

    The \ac{CVAE} is one of the most widely-used models in trajectory prediction for \ac{AD}. It captures the interplay between a driving context and its ground-truth future into a probabilistic latent space and uses it to produce predictions. In this paper, we challenge key components of the CVAE. We leverage recent advances in the space of the VAE, the foundation of the CVAE, which show that a simple change in the sampling procedure can greatly benefit performance. We find that unscented sampling, which draws samples from any learned distribution in a deterministic manner, can naturally be better suited to trajectory prediction than potentially dangerous random sampling. We go further and offer additional improvements, including a more structured mixture latent space, as well as a novel, potentially more expressive way to do inference with CVAEs. We show wide applicability of our models by evaluating them on the INTERACTION prediction dataset, outperforming the state of the art, as well 
    
[^61]: 面向少标注学习的目标检测：基于Transformer的模型更有效吗？

    Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient ?. (arXiv:2310.19936v1 [cs.CV])

    [http://arxiv.org/abs/2310.19936](http://arxiv.org/abs/2310.19936)

    本文提出了一种针对当前最先进目标检测器的半监督方法，使用师生架构在少标注学习设置中避免了依赖敏感的后处理步骤，并在标注稀缺的情况下表现优异。

    

    对于专门的和密集的下游任务，如目标检测，标记数据需要专业知识，成本较高，因此少样本和半监督模型成为更具吸引力的替代方案。在少样本的设置中，我们观察到，在相似数量的参数下，基于Transformer的目标检测器比基于卷积的两阶段模型表现更好，但在最新的半监督设置中，它们的效果没有那么好。在本文中，我们提出了一种针对当前最先进的Deformable DETR目标检测器的半监督方法，使用师生架构在少标注学习设置中避免了依赖敏感的后处理步骤。我们在半监督目标检测基准COVO和Pascal VOC上评估了我们的方法，它在特别是标注稀缺的情况下优于先前的方法。我们相信我们的贡献将开启新的可能性。

    For specialized and dense downstream tasks such as object detection, labeling data requires expertise and can be very expensive, making few-shot and semi-supervised models much more attractive alternatives. While in the few-shot setup we observe that transformer-based object detectors perform better than convolution-based two-stage models for a similar amount of parameters, they are not as effective when used with recent approaches in the semi-supervised setting. In this paper, we propose a semi-supervised method tailored for the current state-of-the-art object detector Deformable DETR in the few-annotation learning setup using a student-teacher architecture, which avoids relying on a sensitive post-processing of the pseudo-labels generated by the teacher model. We evaluate our method on the semi-supervised object detection benchmarks COCO and Pascal VOC, and it outperforms previous methods, especially when annotations are scarce. We believe that our contributions open new possibilitie
    
[^62]: 基于模型的重新参数化策略梯度方法：理论和实践算法

    Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms. (arXiv:2310.19927v1 [cs.LG])

    [http://arxiv.org/abs/2310.19927](http://arxiv.org/abs/2310.19927)

    研究者通过对基于模型的重新参数化策略梯度方法进行理论研究，发现在长期强化学习问题中可能会遇到优化困难，导致收敛速度较慢。他们提出了一种谱归一化方法来缓解梯度方差爆炸问题。

    

    在机器人学和计算机图形学中，重新参数化策略梯度方法已被广泛应用于连续控制任务。然而，最近的研究发现，当应用于长期强化学习问题时，基于模型的重新参数化策略梯度方法可能会遇到混乱和非平滑的优化景观，导致梯度方差爆炸，从而导致收敛速度较慢。这与传统观念相反，即重新参数化方法在训练深度生成模型等问题中具有较低的梯度估计方差。为了理解这一现象，我们对基于模型的重新参数化策略梯度方法进行了理论研究，并寻找解决优化困难的方法。具体而言，我们分析了基于模型的重新参数化策略梯度方法的收敛性，并指出函数逼近器的平滑性是影响梯度估计质量的主要因素。根据我们的分析，我们提出了一种谱归一化方法来缓解爆炸方差问题。

    ReParameterization (RP) Policy Gradient Methods (PGMs) have been widely adopted for continuous control tasks in robotics and computer graphics. However, recent studies have revealed that, when applied to long-term reinforcement learning problems, model-based RP PGMs may experience chaotic and non-smooth optimization landscapes with exploding gradient variance, which leads to slow convergence. This is in contrast to the conventional belief that reparameterization methods have low gradient estimation variance in problems such as training deep generative models. To comprehend this phenomenon, we conduct a theoretical examination of model-based RP PGMs and search for solutions to the optimization difficulties. Specifically, we analyze the convergence of the model-based RP PGMs and pinpoint the smoothness of function approximators as a major factor that affects the quality of gradient estimation. Based on our analysis, we propose a spectral normalization method to mitigate the exploding var
    
[^63]: Jina Embeddings 2: 面向长篇文档的8192-Token通用文本嵌入模型

    Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])

    [http://arxiv.org/abs/2310.19923](http://arxiv.org/abs/2310.19923)

    Jina Embeddings 2是一个能够处理长篇文档的文本嵌入模型，突破了传统512个标记限制，提供了高达8192个标记的容量。

    

    文本嵌入模型已经成为将句子转化为固定大小特征向量的强大工具，这些向量包含了语义信息。尽管这些模型对于信息检索、语义聚类和文本重排序等任务至关重要，但大多数现有的开源模型，尤其是基于BERT等架构构建的模型，难以表示长篇文档，并且常常会进行截断。为了缓解这个挑战，一种常见的方法是将文档分割成更小的段落进行嵌入。然而，这种策略会导致更大的向量集合，进而增加内存消耗，并且在向量搜索时会出现计算密集和延迟升高的问题。为了解决这些挑战，我们介绍了Jina Embeddings 2，这是一个开源的文本嵌入模型，可以容纳高达8192个标记。该模型旨在突破传统的512个标记限制，能够灵活处理长篇文档。

    Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
    
[^64]: 揭示偏见和不平等：利用电子健康记录的医疗人工智能中偏见检测和缓解的系统综述

    Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records. (arXiv:2310.19917v1 [cs.AI])

    [http://arxiv.org/abs/2310.19917](http://arxiv.org/abs/2310.19917)

    本综述对涉及利用电子健康记录数据的医疗人工智能研究中的偏见进行了系统综述，共涵盖了六种主要的偏见类型，同时总结了现有的偏见处理方法。

    

    目的：利用电子健康记录的人工智能应用在医疗领域越来越受到欢迎，但也引入了各种类型的偏见。本研究旨在系统综述涉及利用电子健康记录数据的人工智能研究中的偏见。方法：遵循Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA)准则进行了系统综述。从PubMed、Web of Science和电气和电子工程师学会中检索了2010年1月1日至2022年10月31日期间发表的文章。我们定义了六种主要的偏见类型，并总结了现有的偏见处理方法。结果：在检索到的252篇文章中，有20篇符合最终综述的纳入标准。本综述涵盖了六种偏见中的五种：八项研究分析了选择偏见；六项研究针对隐性偏见；五项研究对混杂偏见进行了研究；四项研究对测量偏见进行了研究；两项研究对算法偏见进行了研究。在偏见处理方法方面，有十项研究进行了探讨。

    Objectives: Artificial intelligence (AI) applications utilizing electronic health records (EHRs) have gained popularity, but they also introduce various types of bias. This study aims to systematically review the literature that address bias in AI research utilizing EHR data. Methods: A systematic review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guideline. We retrieved articles published between January 1, 2010, and October 31, 2022, from PubMed, Web of Science, and the Institute of Electrical and Electronics Engineers. We defined six major types of bias and summarized the existing approaches in bias handling. Results: Out of the 252 retrieved articles, 20 met the inclusion criteria for the final review. Five out of six bias were covered in this review: eight studies analyzed selection bias; six on implicit bias; five on confounding bias; four on measurement bias; two on algorithmic bias. For bias handling approaches, ten st
    
[^65]: 可解释的基于原型的图信息瓶颈

    Interpretable Prototype-based Graph Information Bottleneck. (arXiv:2310.19906v1 [cs.LG])

    [http://arxiv.org/abs/2310.19906](http://arxiv.org/abs/2310.19906)

    这项工作提出了一种新颖的可解释的GNN框架，通过在信息瓶颈框架中将原型学习与输入图的关键子图相结合，为模型的解释能力和性能提供了改进。

    

    图神经网络（GNN）的成功导致了对其决策过程的理解和对其预测的解释的需求，这催生了可解释的人工智能（XAI），为黑盒模型提供透明的解释。最近，原型的使用成功提高了模型的可解释性，通过学习原型来暗示影响预测的训练图。然而，这些方法往往会给原型提供来自整个图的过多信息，导致关键子结构的排除或无关子结构的包含，这可以限制模型在下游任务中的解释能力和性能。在这项工作中，我们提出了一种新颖的可解释的GNN框架，称为解释性的基于原型的图信息瓶颈 (PGIB)，将原型学习纳入信息瓶颈框架，为原型提供输入图的关键子图。

    The success of Graph Neural Networks (GNNs) has led to a need for understanding their decision-making process and providing explanations for their predictions, which has given rise to explainable AI (XAI) that offers transparent explanations for black-box models. Recently, the use of prototypes has successfully improved the explainability of models by learning prototypes to imply training graphs that affect the prediction. However, these approaches tend to provide prototypes with excessive information from the entire graph, leading to the exclusion of key substructures or the inclusion of irrelevant substructures, which can limit both the interpretability and the performance of the model in downstream tasks. In this work, we propose a novel framework of explainable GNNs, called interpretable Prototype-based Graph Information Bottleneck (PGIB) that incorporates prototype learning within the information bottleneck framework to provide prototypes with the key subgraph from the input graph
    
[^66]: Herd：通过智能组合器使用多个较小的LLM来与专有的大型LLM达到相同的性能匹配

    Herd: Using multiple, smaller LLMs to match the performances of proprietary, large LLMs via an intelligent composer. (arXiv:2310.19902v1 [cs.AI])

    [http://arxiv.org/abs/2310.19902](http://arxiv.org/abs/2310.19902)

    使用智能组合器，一群开源模型可以达到或超过专有模型的性能。

    

    目前存在超过一千个多功能的LLM，可以执行实际任务，包括问答、文本摘要、内容生成等。然而，免费模型的可访问性、规模和可靠性限制了它们在日常使用中的广泛应用。为了解决访问和规模的问题，像HuggingFace这样的组织已经创建了模型仓库，用户可以上传已经过训练的模型权重和量化版本，以及描述训练过程的模型卡片。尽管一些模型报告了常用基准测试的性能，但并非所有模型都如此，并且在性能与模型部署成本之间进行权衡的真实世界影响并不清楚。在这里，我们展示了开源模型的群可以通过智能路由器达到或超过专有模型的性能。我们证明了一个开源模型的群能够达到ChatGPT的准确性。

    Currently, over a thousand LLMs exist that are multi-purpose and are capable of performing real world tasks, including Q&A, text summarization, content generation, etc. However, accessibility, scale and reliability of free models prevents them from being widely deployed in everyday use cases. To address the first two issues of access and scale, organisations such as HuggingFace have created model repositories where users have uploaded model weights and quantized versions of models trained using different paradigms, as well as model cards describing their training process. While some models report performance on commonly used benchmarks, not all do, and interpreting the real world impact of trading off performance on a benchmark for model deployment cost, is unclear. Here, we show that a herd of open source models can match or exceed the performance of proprietary models via an intelligent router. We show that a Herd of open source models is able to match the accuracy of ChatGPT, despit
    
[^67]: 探索视觉模型中的盲点几何

    Exploring Geometry of Blind Spots in Vision Models. (arXiv:2310.19889v1 [cs.CV])

    [http://arxiv.org/abs/2310.19889](http://arxiv.org/abs/2310.19889)

    本研究探索了在视觉模型中存在的不敏感现象，并提出了一种通过研究网络的等置信度级别集合的几何形状和范围的技术。通过提出的级别集遍历算法，可以找到与给定源图像相似但属于相同等置信度级别集的输入图像。

    

    尽管深度神经网络在各种应用中取得了显著的成功，但一些研究表明它们对接近无法察觉的扰动非常敏感，即所谓的对抗攻击。另一方面，先前的研究还观察到，深度网络也可能出现对输入空间中的大幅扰动不敏感的情况，而这并不会导致网络激活发生明显改变。在本研究中，我们详细研究了在CNNs和Transformers等视觉模型中的不敏感现象，并提出了研究这些网络“等置信度”级别集合的几何形状和范围的技术。我们提出了一种级别集遍历算法，通过使用局部梯度的正交分量来迭代地探索与输入空间中高置信度区域。给定一个源图像，我们使用该算法来识别与源图像属于相同等置信度级别集的输入，尽管它们在感知上与任意图像相似。

    Despite the remarkable success of deep neural networks in a myriad of settings, several works have demonstrated their overwhelming sensitivity to near-imperceptible perturbations, known as adversarial attacks. On the other hand, prior works have also observed that deep networks can be under-sensitive, wherein large-magnitude perturbations in input space do not induce appreciable changes to network activations. In this work, we study in detail the phenomenon of under-sensitivity in vision models such as CNNs and Transformers, and present techniques to study the geometry and extent of "equi-confidence" level sets of such networks. We propose a Level Set Traversal algorithm that iteratively explores regions of high confidence with respect to the input space using orthogonal components of the local gradients. Given a source image, we use this algorithm to identify inputs that lie in the same equi-confidence level set as the source image despite being perceptually similar to arbitrary image
    
[^68]: Res-Tuning: 通过解绑与主干的调节器来实现灵活高效的调节范式

    Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone. (arXiv:2310.19859v1 [cs.CV])

    [http://arxiv.org/abs/2310.19859](http://arxiv.org/abs/2310.19859)

    Res-Tuning是一种新的调节范式，通过解绑调节器与主干模型，实现了灵活高效的调节。这种结构解离使得调节器的设计与网络架构无关，方便了各种调节策略的灵活组合。

    

    高效参数调节已成为将大规模基础模型转移到下游应用的趋势。现有方法通常将一些轻量级调节器嵌入主干中，调节器的设计和学习都高度依赖于基础模型。本研究提出了一种名为Res-Tuning的新的调节范式，它有意将调节器从主干中解绑。通过理论和实证证据，我们展示了流行的调节方法在我们的解绑公式下拥有等效的对应物，并因此可以轻松地集成到我们的框架中。由于结构解离，我们可以自由设计调节器而不受网络架构的限制，从而方便地组合各种调节策略。我们进一步提出了一种内存高效的Res-Tuning变体，其中绕过（由一系列调节器形成）有效地从主支分离，从而实现了梯度的反向传播。

    Parameter-efficient tuning has become a trend in transferring large-scale foundation models to downstream applications. Existing methods typically embed some light-weight tuners into the backbone, where both the design and the learning of the tuners are highly dependent on the base model. This work offers a new tuning paradigm, dubbed Res-Tuning, which intentionally unbinds tuners from the backbone. With both theoretical and empirical evidence, we show that popular tuning approaches have their equivalent counterparts under our unbinding formulation, and hence can be integrated into our framework effortlessly. Thanks to the structural disentanglement, we manage to free the design of tuners from the network architecture, facilitating flexible combination of various tuning strategies. We further propose a memory-efficient variant of Res-Tuning, where the bypass i.e., formed by a sequence of tuners) is effectively detached from the main branch, such that the gradients are back-propagated o
    
[^69]: AI对齐: 一项全面调查

    AI Alignment: A Comprehensive Survey. (arXiv:2310.19852v1 [cs.AI])

    [http://arxiv.org/abs/2310.19852](http://arxiv.org/abs/2310.19852)

    本篇论文主要介绍了AI对齐的概念、方法和实践。研究围绕四个关键目标：健壮性、可解释性、可控性和道德性展开，并将其分为前向对齐和后向对齐两个部分。 AI对齐是为了构建符合人类意图和价值观的AI系统，并减轻由于系统不对齐带来的潜在风险。

    

    AI对齐旨在构建符合人类意图和价值观的AI系统。随着拥有超人类能力的AI系统的出现，错误对齐系统所带来的潜在大规模风险变得明显。数百名AI专家和公众人物都对AI风险表达了关注，认为减轻AI带来的灭绝风险应该成为全球的优先事项，与大规模社会风险如大流行病和核战争并列。鉴于AI对齐领域缺乏最新的系统调查，本文深入探讨对齐研究的核心概念、方法论和实践。首先，我们确定了四个目标原则作为AI对齐的关键目标：健壮性、可解释性、可控性和道德性（RICE）。我们概述了当前对齐研究的现状，并将其分解为两个关键组成部分：前向对齐和后向对齐。前者旨在使AI系统与人类意图对齐。

    AI alignment aims to build AI systems that are in accordance with human intentions and values. With the emergence of AI systems possessing superhuman capabilities, the potential large-scale risks associated with misaligned systems become apparent. Hundreds of AI experts and public figures have expressed their concerns about AI risks, arguing that mitigating the risk of extinction from AI should be a global priority, alongside other societal-scale risks such as pandemics and nuclear war. Motivated by the lack of an up-to-date systematic survey on AI alignment, in this paper, we delve into the core concepts, methodology, and practice of alignment research. To begin with, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). We outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned v
    
[^70]: 修改的遗传算法用于特征选择和超参数优化：以XGBoost在垃圾邮件预测中为例

    Modified Genetic Algorithm for Feature Selection and Hyper Parameter Optimization: Case of XGBoost in Spam Prediction. (arXiv:2310.19845v1 [cs.LG])

    [http://arxiv.org/abs/2310.19845](http://arxiv.org/abs/2310.19845)

    本文提出了一种修改的遗传算法，用于同时降低维度和优化超参数，在不平衡数据集上进行垃圾邮件预测。实验结果表明，该模型在几何平均和准确率上表现良好。

    

    近期，在在线社交网络上的垃圾邮件问题引起了研究界和商业界的关注。Twitter已成为传播垃圾邮件内容的首选媒介。许多研究努力试图应对社交网络垃圾邮件。Twitter带来了额外的挑战，包括特征空间的大小和不平衡的数据分布。通常，相关研究工作关注其中的一部分主要挑战，或者产生黑盒模型。在本文中，我们提出了一种修改的遗传算法，用于同时降低维度和优化超参数在不平衡数据集上。该算法初始化了一个eXtreme Gradient Boosting分类器，并减少了推文数据集的特征空间，以生成一个垃圾邮件预测模型。该模型使用50次重复的10倍分层交叉验证进行验证，并使用非参数统计检验进行分析。结果预测模型在几何平均和准确率上平均达到82.32％和92.67％。

    Recently, spam on online social networks has attracted attention in the research and business world. Twitter has become the preferred medium to spread spam content. Many research efforts attempted to encounter social networks spam. Twitter brought extra challenges represented by the feature space size, and imbalanced data distributions. Usually, the related research works focus on part of these main challenges or produce black-box models. In this paper, we propose a modified genetic algorithm for simultaneous dimensionality reduction and hyper parameter optimization over imbalanced datasets. The algorithm initialized an eXtreme Gradient Boosting classifier and reduced the features space of tweets dataset; to generate a spam prediction model. The model is validated using a 50 times repeated 10-fold stratified cross-validation, and analyzed using nonparametric statistical tests. The resulted prediction model attains on average 82.32\% and 92.67\% in terms of geometric mean and accuracy r
    
[^71]: 使用遗传算法和极限增强模型对电话营销过程进行建模：特征选择和成本敏感的分析方法

    Modeling the Telemarketing Process using Genetic Algorithms and Extreme Boosting: Feature Selection and Cost-Sensitive Analytical Approach. (arXiv:2310.19843v1 [cs.LG])

    [http://arxiv.org/abs/2310.19843](http://arxiv.org/abs/2310.19843)

    本研究提出了一个使用遗传算法和极限增强模型进行电话营销过程建模的方法，其中包括特征选择和成本敏感的分析方法。研究通过利用电话营销数据和社会经济指标对客户意愿进行建模，并构建出可解释的预测模型。

    

    目前，几乎所有的直接营销活动都是通过虚拟方式而不是面对面进行的，这加快了人际交往技巧的衰退速度。此外，企业一直在努力感知和促进客户接受营销提案的倾向。数字转型和增加的虚拟存在迫使企业寻求新的营销研究方法。本研究旨在利用电话营销数据建模客户办理定期存款的意愿，并找出客户的最重要特征。使用葡萄牙银行的真实数据和国家社会经济指标来建模电话营销决策过程。本研究提供了两个重要的贡献。首先，提出了一种基于遗传算法的分类器，可以同时选择最佳的区分特征和调整分类器参数。其次，构建了一个可解释的预测模型。

    Currently, almost all direct marketing activities take place virtually rather than in person, weakening interpersonal skills at an alarming pace. Furthermore, businesses have been striving to sense and foster the tendency of their clients to accept a marketing offer. The digital transformation and the increased virtual presence forced firms to seek novel marketing research approaches. This research aims at leveraging the power of telemarketing data in modeling the willingness of clients to make a term deposit and finding the most significant characteristics of the clients. Real-world data from a Portuguese bank and national socio-economic metrics are used to model the telemarketing decision-making process. This research makes two key contributions. First, propose a novel genetic algorithm-based classifier to select the best discriminating features and tune classifier parameters simultaneously. Second, build an explainable prediction model. The best-generated classification models were 
    
[^72]: AMIR：基于COVID-19疫苗数据集的自动化虚假信息驳斥推荐系统

    AMIR: Automated MisInformation Rebuttal -- A COVID-19 Vaccination Datasets based Recommendation System. (arXiv:2310.19834v1 [cs.AI])

    [http://arxiv.org/abs/2310.19834](http://arxiv.org/abs/2310.19834)

    本研究提出了基于COVID-19疫苗数据集的自动化虚假信息驳斥推荐系统，利用社交媒体信息和经过策划的事实核查数据，实现了大规模自动驳斥虚假信息。这为对抗虚假信息提供了一种成本效益高、可扩展的解决方案。

    

    虚假信息近年来已成为一个重要的社会威胁，特别是在COVID-19大流行的背景下，它加剧了疫苗犹豫不决。对抗虚假信息的成本效益高、可扩展的解决方案是当务之急。本研究探讨如何利用社交媒体获取的现有信息，并与更多经过策划的事实核查数据库相结合，以促进大规模自动驳斥虚假信息。虽然这里的想法可以推广并重新应用于使用多种信息来源和满足社交媒体平台光谱的较大范围的虚假信息缓解，但本工作作为一个概念验证受限于其范围，仅限于对推文的反驳，且仅限于COVID-19相关的虚假信息。它利用了两个公开可用的数据集，即FaCov（经事实核查的文章）和misleading（社交媒体推文）。

    Misinformation has emerged as a major societal threat in recent years in general; specifically in the context of the COVID-19 pandemic, it has wrecked havoc, for instance, by fuelling vaccine hesitancy. Cost-effective, scalable solutions for combating misinformation are the need of the hour. This work explored how existing information obtained from social media and augmented with more curated fact checked data repositories can be harnessed to facilitate automated rebuttal of misinformation at scale. While the ideas herein can be generalized and reapplied in the broader context of misinformation mitigation using a multitude of information sources and catering to the spectrum of social media platforms, this work serves as a proof of concept, and as such, it is confined in its scope to only rebuttal of tweets, and in the specific context of misinformation regarding COVID-19. It leverages two publicly available datasets, viz. FaCov (fact-checked articles) and misleading (social media Twitt
    
[^73]: GalliformeSpectra: 一份鸡品种数据集

    GalliformeSpectra: A Hen Breed Dataset. (arXiv:2310.19830v1 [q-bio.QM])

    [http://arxiv.org/abs/2310.19830](http://arxiv.org/abs/2310.19830)

    GalliformeSpectra是一个包含十种不同鸡品种的数据集，共有1010张图像，展示了每个品种的独特特征，对于家禽科学、遗传学和农业研究有重要潜力。

    

    本文介绍了一个全面的数据集，包含了十种不同的母鸡品种，来自不同地区，捕捉了每个品种的独特特征和特点。该数据集包括了Bielefeld、Blackorpington、Brahma、Buckeye、Fayoumi、Leghorn、Newhampshire、Plymouthrock、Sussex和Turken品种，为世界范围内常见的家禽提供了多样化的代表性。共收集了1010张原始的JPG图片，展示了每个母鸡品种的体态特征、羽毛图案和独特特征。这些图片随后被标准化、调整大小并转换为PNG格式，以保持数据集的一致性。虽然编制的过程在品种之间分布不均匀，但这个数据集作为一个基础资源，为家禽科学、遗传学和农业研究领域的研究和应用提供了丰富的资源。这个数据集有着重要的潜力，可以通过探索和分析独特的特征来贡献于各个领域。

    This article presents a comprehensive dataset featuring ten distinct hen breeds, sourced from various regions, capturing the unique characteristics and traits of each breed. The dataset encompasses Bielefeld, Blackorpington, Brahma, Buckeye, Fayoumi, Leghorn, Newhampshire, Plymouthrock, Sussex, and Turken breeds, offering a diverse representation of poultry commonly bred worldwide. A total of 1010 original JPG images were meticulously collected, showcasing the physical attributes, feather patterns, and distinctive features of each hen breed. These images were subsequently standardized, resized, and converted to PNG format for consistency within the dataset. The compilation, although unevenly distributed across the breeds, provides a rich resource, serving as a foundation for research and applications in poultry science, genetics, and agricultural studies. This dataset holds significant potential to contribute to various fields by enabling the exploration and analysis of unique characte
    
[^74]: 机器学习和知识：为什么鲁棒性很重要

    Machine Learning and Knowledge: Why Robustness Matters. (arXiv:2310.19819v1 [cs.LG])

    [http://arxiv.org/abs/2310.19819](http://arxiv.org/abs/2310.19819)

    机器学习算法的鲁棒性对于信任和知识的形成至关重要，只有在正确特征下跨情景良好运行的算法才能提供可靠的知识。

    

    信任机器学习算法需要对其输出有信心。信心通常以模型的可靠性来解释，即模型产生高比例正确输出时可靠。然而，模型可靠性不能解决机器学习模型鲁棒性的担忧，例如模型依赖错误特征或性能在不同情境下的变化。我认为，对信任的认识维度可以通过知识的概念来理解，算法的可信度取决于其用户是否能够确认其输出的正确性。知识要求信念基于正确的原因形成，并且对错误具有鲁棒性，因此机器学习算法只有在跨反事实情景中良好运行，并基于正确特征做出决策时，才能提供知识。我认为，这可以解释为什么我们应该关心像可解释性这样的模型属性。

    Trusting machine learning algorithms requires having confidence in their outputs. Confidence is typically interpreted in terms of model reliability, where a model is reliable if it produces a high proportion of correct outputs. However, model reliability does not address concerns about the robustness of machine learning models, such as models relying on the wrong features or variations in performance based on context. I argue that the epistemic dimension of trust can instead be understood through the concept of knowledge, where the trustworthiness of an algorithm depends on whether its users are in the position to know that its outputs are correct. Knowledge requires beliefs to be formed for the right reasons and to be robust to error, so machine learning algorithms can only provide knowledge if they work well across counterfactual scenarios and if they make decisions based on the right features. This, I argue, can explain why we should care about model properties like interpretability
    
[^75]: 训练无需浮点精度的二进制神经网络

    Training binary neural networks without floating point precision. (arXiv:2310.19815v1 [cs.LG])

    [http://arxiv.org/abs/2310.19815](http://arxiv.org/abs/2310.19815)

    本研究提出了两种解决方案，通过拓扑变化和策略训练，实现了高效训练的二进制神经网络，具有低延迟和低能耗的特点。

    

    本研究的主要目标是提高训练二进制神经网络的效率，这些网络具有低延迟和低能耗的特点。本研究的主要贡献是提出了两种解决方案，包括拓扑变化和策略训练，使得网络能够达到接近最先进性能和高效的训练。训练所需的时间和内存是促进高效训练的两个因素。

    The main goal of this work is to improve the efficiency of training binary neural networks, which are low latency and low energy networks. The main contribution of this work is the proposal of two solutions comprised of topology changes and strategy training that allow the network to achieve near the state-of-the-art performance and efficient training. The time required for training and the memory required in the process are two factors that contribute to efficient training.
    
[^76]: 利用大型语言模型增强遗传改进突变

    Enhancing Genetic Improvement Mutations Using Large Language Models. (arXiv:2310.19813v1 [cs.SE])

    [http://arxiv.org/abs/2310.19813](http://arxiv.org/abs/2310.19813)

    本文研究了利用大型语言模型作为遗传改进的变异操作符来提高搜索过程，发现使用LLM编辑的补丁通过单元测试的数量高达75％，但相比较标准编辑，LLMs找到的补丁较少多样化。尽管LLM增强的GI找到了许多改进的补丁，但最好的改进补丁是通过标准GI找到的。

    

    大型语言模型（LLMs）已成功应用于软件工程任务，包括程序修复。然而，它们在遗传改进（GI）等基于搜索的技术中的应用仍然很少被探索。在本文中，我们评估了将LLMs作为GI的变异操作符以改进搜索过程的使用。我们扩展了Gin Java GI工具包，以调用OpenAI的API为JCodec工具生成编辑。我们使用5种不同的编辑类型随机抽样编辑空间。我们发现，通过LLM编辑，通过单元测试的补丁数量高于使用标准插入编辑的补丁数量高达75％。此外，我们观察到与标准编辑相比，LLMs找到的补丁通常较少多样化。我们使用局部搜索运行GI以寻找运行时改进。尽管LLM增强的GI找到了许多改进的补丁，但最好的改进补丁是通过标准GI找到的。

    Large language models (LLMs) have been successfully applied to software engineering tasks, including program repair. However, their application in search-based techniques such as Genetic Improvement (GI) is still largely unexplored. In this paper, we evaluate the use of LLMs as mutation operators for GI to improve the search process. We expand the Gin Java GI toolkit to call OpenAI's API to generate edits for the JCodec tool. We randomly sample the space of edits using 5 different edit types. We find that the number of patches passing unit tests is up to 75% higher with LLM-based edits than with standard Insert edits. Further, we observe that the patches found with LLMs are generally less diverse compared to standard edits. We ran GI with local search to find runtime improvements. Although many improving patches are found by LLM-enhanced GI, the best improving patch was found by standard GI.
    
[^77]: 脑解码：走向实时重建视觉知觉

    Brain decoding: toward real-time reconstruction of visual perception. (arXiv:2310.19812v1 [eess.IV])

    [http://arxiv.org/abs/2310.19812](http://arxiv.org/abs/2310.19812)

    本研究提出了一种基于脑磁图（MEG）的脑解码方法，通过训练一个具有预训练嵌入、MEG模块和图像生成器的模型，在实时应用中实现了对视觉知觉的高时间分辨率解码，并在图像检索上取得了7倍的改进。

    

    在过去的五年中，生成式和基础性人工智能系统的使用极大地提高了对大脑活动的解码能力。特别是对于视觉知觉，现在可以从功能性磁共振成像（fMRI）中解码出令人瞩目的准确度。然而，这种神经影像技术的时间分辨率有限（约为0.5 Hz），因此在实时应用方面存在根本性的限制。在这里，我们提出了一种基于脑磁图（MEG）的替代方法，这是一种能够以高时间分辨率（约为5000 Hz）测量脑活动的神经影像设备。为此，我们开发了一个MEG解码模型，该模型通过对比和回归目标进行训练，并由三个模块组成：i）从图像中获得的预训练嵌入、ii）端到端训练的MEG模块以及iii）预训练的图像生成器。我们的结果有三个方面：首先，我们的MEG解码器在经典线性解码器上显示出7倍的图像检索改进。其次，后期脑部

    In the past five years, the use of generative and foundational AI systems has greatly improved the decoding of brain activity. Visual perception, in particular, can now be decoded from functional Magnetic Resonance Imaging (fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers from a limited temporal resolution ($\approx$0.5 Hz) and thus fundamentally constrains its real-time usage. Here, we propose an alternative approach based on magnetoencephalography (MEG), a neuroimaging device capable of measuring brain activity with high temporal resolution ($\approx$5,000 Hz). For this, we develop an MEG decoding model trained with both contrastive and regression objectives and consisting of three modules: i) pretrained embeddings obtained from the image, ii) an MEG module trained end-to-end and iii) a pretrained image generator. Our results are threefold: Firstly, our MEG decoder shows a 7X improvement of image-retrieval over classic linear decoders. Second, late brain 
    
[^78]: 高级逻辑程序等价性属性的自动验证-学士论文

    Automated Verification of Equivalence Properties in Advanced Logic Programs -- Bachelor Thesis. (arXiv:2310.19806v1 [cs.LO])

    [http://arxiv.org/abs/2310.19806](http://arxiv.org/abs/2310.19806)

    这篇论文介绍了一种自动验证工具，用于验证优化的逻辑子程序是否可以替代原始子程序，在工业应用中具有重要意义。

    

    随着使用答案集编程的工业应用增加，对形式验证工具，特别是对关键应用的需求也增加了。在程序优化过程中，希望有一种工具可以自动验证优化的子程序是否可以替代原始子程序。从形式上讲，这对应于验证两个程序的强等价性的问题。为了做到这一点，开发了翻译工具anthem。它可以与用于经典逻辑的自动定理证明器一起使用，以验证两个程序是否强等价。在当前版本的anthem中，只能验证具有受限输入语言的正程序的强等价性。这是anthem中实现的翻译τ*的结果，它生成了here-and-there逻辑中的公式，该逻辑只对正程序与经典逻辑相一致。这篇论文扩展了anthem，以便可以验证更广泛的高级逻辑程序的强等价性。

    With the increase in industrial applications using Answer Set Programming, the need for formal verification tools, particularly for critical applications, has also increased. During the program optimisation process, it would be desirable to have a tool which can automatically verify whether an optimised subprogram can replace the original subprogram. Formally this corresponds to the problem of verifying the strong equivalence of two programs. In order to do so, the translation tool anthem was developed. It can be used in conjunction with an automated theorem prover for classical logic to verify that two programs are strongly equivalent. With the current version of anthem, only the strong equivalence of positive programs with a restricted input language can be verified. This is a result of the translation $\tau^*$ implemented in anthem that produces formulas in the logic of here-and-there, which coincides with classical logic only for positive programs. This thesis extends anthem in ord
    
[^79]: SERA：离线到在线强化学习中的样本高效奖励增强

    SERA:Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning. (arXiv:2310.19805v1 [cs.LG])

    [http://arxiv.org/abs/2310.19805](http://arxiv.org/abs/2310.19805)

    这篇论文提出了一种称为SERA的奖励增强框架，用于改善离线到在线强化学习中的探索能力。它通过设计内在奖励来鼓励agent进行探索，并实现更好的在线微调效果。

    

    离线强化学习的一个潜在应用是使用现有的静态数据集来初始化预训练策略，然后进行后续在线微调。然而，直接对离线预训练策略进行微调往往会导致次优性能。主要原因是离线保守方法降低了agent的探索能力，从而影响了在线微调的性能。为了增强在线微调过程中的探索能力，从而提高整体的在线微调性能，我们引入了一种称为样本高效奖励增强（SERA）的通用奖励增强框架。SERA旨在通过设计鼓励agent进行探索的内在奖励来改善在线微调的性能。具体来说，它隐式地实现了状态边缘匹配（SMM）并惩罚超出分布范围的状态行动，从而鼓励agent覆盖目标状态密度，并实现更好的在线微调结果。

    A prospective application of offline reinforcement learning (RL) involves initializing a pre-trained policy using existing static datasets for subsequent online fine-tuning. However, direct fine-tuning of the offline pre-trained policy often results in sub-optimal performance. A primary reason is that offline conservative methods diminish the agent's capability of exploration, thereby impacting online fine-tuning performance. To enhance exploration during online fine-tuning and thus enhance the overall online fine-tuning performance, we introduce a generalized reward augmentation framework called Sample Efficient Reward Augmentation (SERA). SERA aims to improve the performance of online fine-tuning by designing intrinsic rewards that encourage the agent to explore. Specifically, it implicitly implements State Marginal Matching (SMM) and penalizes out-of-distribution (OOD) state actions, thus encouraging agents to cover the target state density, and achieving better online fine-tuning r
    
[^80]: 从核的角度看马尔科夫决策过程的行为度量

    A Kernel Perspective on Behavioural Metrics for Markov Decision Processes. (arXiv:2310.19804v1 [cs.LG])

    [http://arxiv.org/abs/2310.19804](http://arxiv.org/abs/2310.19804)

    本文从核的角度论述了马尔科夫决策过程行为度量的新视角，并提出了一种新的度量与MICo距离等价。此外，核的视角还使我们能够提供新的理论结果，包括界定价值函数差异和嵌入到低失真误差的欧氏空间中。这些结果对于使用行为度量构建强化学习表示至关重要。同时，我们通过实证结果证明了这些方法的实际有效性。

    

    研究表明，行为度量是构建强化学习表示的有效机制。本文通过使用正定核，提出了一种关于马尔科夫决策过程行为度量的新视角。我们利用这个新视角定义了一个新的度量，可以被证明与最近引入的MICo距离（Castro等人，2021年）等价。核的视角进一步使我们能够提供新的理论结果，这在之前的工作中一直存在困难。这些结果包括通过我们的度量来界定价值函数差异，并证明我们的度量可以被证明嵌入到具有低失真误差的有限维欧氏空间中。这是在使用行为度量进行强化学习表示时的两个关键属性。我们通过强有力的实证结果来补充我们的理论，证明了这些方法在实践中的有效性。

    Behavioural metrics have been shown to be an effective mechanism for constructing representations in reinforcement learning. We present a novel perspective on behavioural metrics for Markov decision processes via the use of positive definite kernels. We leverage this new perspective to define a new metric that is provably equivalent to the recently introduced MICo distance (Castro et al., 2021). The kernel perspective further enables us to provide new theoretical results, which has so far eluded prior work. These include bounding value function differences by means of our metric, and the demonstration that our metric can be provably embedded into a finite-dimensional Euclidean space with low distortion error. These are two crucial properties when using behavioural metrics for reinforcement learning representations. We complement our theory with strong empirical results that demonstrate the effectiveness of these methods in practice.
    
[^81]: 基于症状的自动猴痘检测系统SyMPox：使用XGBoost算法

    SyMPox: An Automated Monkeypox Detection System Based on Symptoms Using XGBoost. (arXiv:2310.19801v1 [cs.LG])

    [http://arxiv.org/abs/2310.19801](http://arxiv.org/abs/2310.19801)

    SyMPox是一个基于症状的自动猴痘检测系统，利用XGBoost算法分析症状模式并提供准确的猴痘诊断，为用户提供了一个用户友好的平台。

    

    猴痘是一种人畜共患病。到2023年6月10日，世界卫生组织确认了约87000例猴痘病例。目前最常用的鉴定方法是基于图像识别技术，然而这些方法速度较慢，且仅供少数人使用。本研究提出了一种名为SyMPox的独立应用，基于症状诊断猴痘病例。SyMPox利用强大的XGBoost算法分析症状模式，并提供准确的评估结果。SyMPox使用Gradio框架开发，为个人提供一个用户友好的平台，用于评估症状并获得可靠的猴痘诊断结果。

    Monkeypox is a zoonotic disease. About 87000 cases of monkeypox were confirmed by the World Health Organization until 10th June 2023. The most prevalent methods for identifying this disease are image-based recognition techniques. Still, they are not too fast and could only be available to a few individuals. This study presents an independent application named SyMPox, developed to diagnose Monkeypox cases based on symptoms. SyMPox utilizes the robust XGBoost algorithm to analyze symptom patterns and provide accurate assessments. Developed using the Gradio framework, SyMPox offers a user-friendly platform for individuals to assess their symptoms and obtain reliable Monkeypox diagnoses.
    
[^82]: 从外部到Swap遗憾2.0：针对大动作空间的高效约化和无知对手

    From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces. (arXiv:2310.19786v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.19786](http://arxiv.org/abs/2310.19786)

    通过新的约化方法，我们改进了经典的Swap遗憾最小化算法，并提供了一个无外部遗憾算法的替代方法。对于学习专家建议问题，我们的算法可以在较少的轮数和更低的复杂度下达到相同的Swap遗憾限制。

    

    我们提供了一种新颖的从Swap遗憾最小化到外部遗憾最小化的约化方法，改进了Blum-Mansour和Stolz-Lugosi的经典约化方法，不需要行为空间的有限性。我们证明，只要存在某个假设类的无外部遗憾算法，就必然存在相同类别的无Swap遗憾算法。对于学习专家建议问题，我们的结果意味着可以保证在$\log(N)^{O(1/\epsilon)}$轮后，每次迭代复杂度为$O(N)$的情况下，Swap遗憾被限定为$\epsilon$，而Blum-Mansour和Stolz-Lugosi的经典约化方法需要$O(N/\epsilon^2)$轮和至少$\Omega(N^2)$的每次迭代复杂度。我们的结果伴随着一个相关的下界，与[BM07]不同，这个下界适用于无知和$\ell_1$-受限的对手和可以利用这个下界的学习者。

    We provide a novel reduction from swap-regret minimization to external-regret minimization, which improves upon the classical reductions of Blum-Mansour [BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the space of actions. We show that, whenever there exists a no-external-regret algorithm for some hypothesis class, there must also exist a no-swap-regret algorithm for that same class. For the problem of learning with expert advice, our result implies that it is possible to guarantee that the swap regret is bounded by {\epsilon} after $\log(N)^{O(1/\epsilon)}$ rounds and with $O(N)$ per iteration complexity, where $N$ is the number of experts, while the classical reductions of Blum-Mansour and Stolz-Lugosi require $O(N/\epsilon^2)$ rounds and at least $\Omega(N^2)$ per iteration complexity. Our result comes with an associated lower bound, which -- in contrast to that in [BM07] -- holds for oblivious and $\ell_1$-constrained adversaries and learners that can emplo
    
[^83]: 设计AI支持人类参与AI辅助决策：从系统性回顾中分类人机交互的税表

    Designing AI Support for Human Involvement in AI-assisted Decision Making: A Taxonomy of Human-AI Interactions from a Systematic Review. (arXiv:2310.19778v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2310.19778](http://arxiv.org/abs/2310.19778)

    通过对AI辅助决策文献的系统回顾，我们填补了人机交互协议的共同词汇的空白，并强调了解AI应该提供什么信息来帮助人类的重要性。

    

    在决策支持系统中利用人工智能（AI）的努力过分关注技术进步，往往忽视了算法输出与人类期望之间的一致性。为了解决这个问题，可解释性人工智能从更以人为中心的角度促进AI的发展。确定AI应该提供哪些信息来帮助人类是至关重要的，然而，信息如何呈现，比如推荐的顺序和解释的要求同样重要。这推动了更精确地研究人机交互作为基于AI的决策支持的关键组成部分的需求。尽管已有几个实证研究评估了多个应用领域中的人机交互，其中交互形式各异，但目前还没有一个共同的词汇来描述人机交互协议。为了填补这一空白，我们描述了对AI辅助决策文献的系统回顾结果。

    Efforts in levering Artificial Intelligence (AI) in decision support systems have disproportionately focused on technological advancements, often overlooking the alignment between algorithmic outputs and human expectations. To address this, explainable AI promotes AI development from a more human-centered perspective. Determining what information AI should provide to aid humans is vital, however, how the information is presented, e. g., the sequence of recommendations and the solicitation of interpretations, is equally crucial. This motivates the need to more precisely study Human-AI interaction as a pivotal component of AI-based decision support. While several empirical studies have evaluated Human-AI interactions in multiple application domains in which interactions can take many forms, there is not yet a common vocabulary to describe human-AI interaction protocols. To address this gap, we describe the results of a systematic review of the AI-assisted decision making literature, anal
    
[^84]: 评估大型语言模型：一项全面调查

    Evaluating Large Language Models: A Comprehensive Survey. (arXiv:2310.19736v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19736](http://arxiv.org/abs/2310.19736)

    本调查综述了对大型语言模型（LLMs）的评估，包括知识和能力评估、对齐评估和安全评估。对于充分利用LLMs的能力以及确保其安全和有益的发展至关重要。

    

    大型语言模型（LLMs）在各种任务中展示了卓越的能力。它们吸引了广泛的关注，并在许多下游应用中得到了应用。然而，与双刃剑一样，LLMs也存在潜在风险。它们可能受到私人数据泄露，产生不适当、有害或误导性的内容。此外，LLMs的快速进展引发了对可能出现没有足够保障的超智能系统的担忧。为了有效利用LLMs的能力，并确保其安全和有益的发展，对LLMs进行严格和全面的评估至关重要。本调查旨在提供对LLMs评估的全面概述。我们将LLMs的评估分为三大类别：知识和能力评估，对齐评估和安全评估。除了全面回顾评估方法和技术之外，

    Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs.  This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and
    
[^85]: 大型语言模型：对当前辩论的细微差别的需求和对理解的务实观点(arXiv:2310.19671v2 [cs.CL] 更新)

    Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding. (arXiv:2310.19671v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19671](http://arxiv.org/abs/2310.19671)

    当前的大型语言模型在生成文本方面表现出了出色的能力，但对其能力的辩论缺乏细致的考虑。这篇论文评估了三个常见批评观点，并提出了对LLMs理解和意图问题的务实观点。

    

    目前的大型语言模型（LLMs）在生成语法正确、流畅的文本方面无与伦比。LLMs正在快速出现，并且关于LLM能力的辩论已经开始，但反思滞后。因此，在这篇立场论文中，我们首先聚焦于辩论，并对LLM能力的三个重复出现的批评进行批判性评估：i) LLMs只是模仿训练数据中的统计模式；ii) LLMs掌握了形式但并非功能性语言能力；iii) LLMs中的语言学习不能为人类语言学习提供信息。通过实证和理论论证，我们展示了这些观点需要更多细微之处。其次，我们概述了一个对LLMs中“真正”的理解和意图问题的务实观点。理解和意图涉及到我们假设他人具有的不可观察的心理状态，因为它们具有实用价值：它们使我们能够抽象出复杂的底层机制并预测行为。

    Current Large Language Models (LLMs) are unparalleled in their ability to generate grammatically correct, fluent text. LLMs are appearing rapidly, and debates on LLM capacities have taken off, but reflection is lagging behind. Thus, in this position paper, we first zoom in on the debate and critically assess three points recurring in critiques of LLM capacities: i) that LLMs only parrot statistical patterns in the training data; ii) that LLMs master formal but not functional language competence; and iii) that language learning in LLMs cannot inform human language learning. Drawing on empirical and theoretical arguments, we show that these points need more nuance. Second, we outline a pragmatic perspective on the issue of `real' understanding and intentionality in LLMs. Understanding and intentionality pertain to unobservable mental states we attribute to other humans because they have pragmatic value: they allow us to abstract away from complex underlying mechanics and predict behaviou
    
[^86]: LLMaAA: 将大型语言模型作为主动批注器

    LLMaAA: Making Large Language Models as Active Annotators. (arXiv:2310.19596v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19596](http://arxiv.org/abs/2310.19596)

    LLMaAA是一种利用大型语言模型作为主动批注器的方法，通过在主动学习循环中使用LLM确定高效批注内容，以最大限度地利用LLM的潜力并利用大量未标记数据。

    

    在自然语言处理（NLP）中，普遍的监督学习方法因需求大量高质量标注数据而声名狼藉。实际上，获取这样的数据是一项昂贵的事业。最近，大型语言模型（LLM）出色的少样本性能推动了数据集生成的发展，其中训练数据仅从LLM中合成。然而，这种方法通常存在质量低的问题，并且需要多个数量级的已标记数据才能实现令人满意的性能。为了充分发挥LLM的潜力并利用大量未标记数据，我们提出了LLMaAA，它将LLM作为批注器，并将它们放入主动学习循环中以高效确定批注内容。为了利用伪标签进行稳健学习，我们优化了批注和训练过程：（1）我们从小的示范池中抽取k-NN示例作为上下文示例，（2）我们采用示例加权技术。

    Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting techniqu
    
[^87]: 基于知识图谱的自主决策AI中的信任、问责和自主性

    Trust, Accountability, and Autonomy in Knowledge Graph-based AI for Self-determination. (arXiv:2310.19503v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.19503](http://arxiv.org/abs/2310.19503)

    基于知识图谱的人工智能技术带来了许多好处，但其普及可能导致市民的自主决策权受限。为了解决这个问题，一些地区提出了人工智能管制措施。

    

    知识图谱成为了推动智能决策和各种人工智能服务的基础平台，被谷歌、沃尔玛和AirBnb等主要公司广泛采用。知识图谱通过提供数据上下文和语义来补充机器学习算法，从而实现进一步的推理和问答能力。将知识图谱与神经网络学习（例如大型语言模型）结合起来的研究被称为神经符号人工智能。尽管基于知识图谱的人工智能可以带来许多好处，但其在在线服务中越来越普及可能导致市民失去自主决策的问题。我们越依赖这些通常是集中化的技术，市民就越无法决定自己的命运。为了应对这一威胁，在某些地区提出了人工智能管制，例如欧盟的人工智能法案。

    Knowledge Graphs (KGs) have emerged as fundamental platforms for powering intelligent decision-making and a wide range of Artificial Intelligence (AI) services across major corporations such as Google, Walmart, and AirBnb. KGs complement Machine Learning (ML) algorithms by providing data context and semantics, thereby enabling further inference and question-answering capabilities. The integration of KGs with neuronal learning (e.g., Large Language Models (LLMs)) is currently a topic of active research, commonly named neuro-symbolic AI. Despite the numerous benefits that can be accomplished with KG-based AI, its growing ubiquity within online services may result in the loss of self-determination for citizens as a fundamental societal issue. The more we rely on these technologies, which are often centralised, the less citizens will be able to determine their own destinies. To counter this threat, AI regulation, such as the European Union (EU) AI Act, is being proposed in certain regions.
    
[^88]: 基于分类器分数蒸馏的文本到3D生成

    Text-to-3D with Classifier Score Distillation. (arXiv:2310.19415v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.19415](http://arxiv.org/abs/2310.19415)

    本文提出了一种名为分类器分数蒸馏（CSD）的方法，它利用隐式分类模型进行文本到3D生成，证明了仅凭分类器无关指导就可以有效实现文本到3D生成任务，并在各种任务中取得了优于最先进方法的结果。

    

    最近，文本到3D生成取得了显著的进展，尤其是基于分数蒸馏采样的方法，利用了预训练的2D扩散模型。虽然使用无分类器的指导被广泛认为是成功优化的关键，但它被视为辅助技巧而不是最重要的组成部分。在本文中，我们重新评估了分类器无关指导在分数蒸馏中的作用，并发现了一个令人惊讶的发现：仅凭指导就足以有效实现文本到3D生成任务。我们将这种方法命名为分类器分数蒸馏（CSD），可以将其解释为使用隐式分类模型进行生成。这种新的视角揭示了对现有技术的新见解。我们验证了CSD在各种文本到3D任务中的有效性，包括形状生成、纹理合成和形状编辑，在结果上超过了最先进方法。

    Text-to-3D generation has made remarkable progress recently, particularly with methods based on Score Distillation Sampling (SDS) that leverages pre-trained 2D diffusion models. While the usage of classifier-free guidance is well acknowledged to be crucial for successful optimization, it is considered an auxiliary trick rather than the most essential component. In this paper, we re-evaluate the role of classifier-free guidance in score distillation and discover a surprising finding: the guidance alone is enough for effective text-to-3D generation tasks. We name this method Classifier Score Distillation (CSD), which can be interpreted as using an implicit classification model for generation. This new perspective reveals new insights for understanding existing techniques. We validate the effectiveness of CSD across a variety of text-to-3D tasks including shape generation, texture synthesis, and shape editing, achieving results superior to those of state-of-the-art methods. Our project pa
    
[^89]: TeacherLM: 教人打鱼而不是给鱼，语言建模同理

    TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise. (arXiv:2310.19019v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.19019](http://arxiv.org/abs/2310.19019)

    TeacherLM-7.1B是一个小型模型，通过给自然语言处理样本进行注释，教会其他模型“为什么”而不仅仅是“什么”。它在MMLU上取得了52.3的零样本得分，同时具有出色的数据增强能力。发布TeacherLM系列模型和增强的数据集作为开源项目。

    

    大型语言模型(LLMs)在各种自然语言处理任务中展现了惊人的推理和数据增强能力。然而，小型模型呢？在这项工作中，我们提出了TeacherLM-7.1B，能够给大多数自然语言处理样本进行相关基础知识、思维链和常见错误的注释，使注释不仅仅是一个答案，而且使其他模型可以学习“为什么”，而不仅仅是“什么”。TeacherLM-7.1B模型在MMLU上实现了52.3的零样本得分，超过了拥有100B参数的大多数模型。更令人印象深刻的是其数据增强能力。基于TeacherLM-7.1B，我们在多任务设置中使用了来自OPT和BLOOM系列的不同参数的多个学生模型对58个自然语言处理数据集进行了增强。实验结果表明，TeacherLM提供的数据增强带来了显着的好处。我们将作为开源发布TeacherLM系列模型和增强的数据集。

    Large Language Models (LLMs) exhibit impressive reasoning and data augmentation capabilities in various NLP tasks. However, what about small models? In this work, we propose TeacherLM-7.1B, capable of annotating relevant fundamentals, chain of thought, and common mistakes for most NLP samples, which makes annotation more than just an answer, thus allowing other models to learn "why" instead of just "what". The TeacherLM-7.1B model achieved a zero-shot score of 52.3 on MMLU, surpassing most models with over 100B parameters. Even more remarkable is its data augmentation ability. Based on TeacherLM-7.1B, we augmented 58 NLP datasets and taught various student models with different parameters from OPT and BLOOM series in a multi-task setting. The experimental results indicate that the data augmentation provided by TeacherLM has brought significant benefits. We will release the TeacherLM series of models and augmented datasets as open-source.
    
[^90]: 人工智能在开放科学中的应用：从伦理角度将数据转化为知识的多智能体视角

    AI for Open Science: A Multi-Agent Perspective for Ethically Translating Data to Knowledge. (arXiv:2310.18852v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.18852](http://arxiv.org/abs/2310.18852)

    这项研究提出了AI for Open Science (AI4OS)的概念，将其作为AI for Science (AI4Science)的多智能体扩展，旨在通过在科学企业中最大化开放知识转化来促进开放科学。研究使用知识发现与数据挖掘（KDD）的原则来形式化AI4OS，并提出了具体应用开放性的方法。

    

    人工智能在科学中的应用，尤其是自动驾驶实验室的形式，有可能排除人类参与并阻碍更广泛社区内的科学发现。虽然以前的研究已经关注确保人工智能应用的负责任部署、增强安全性和确保可解释性，但我们还提出应仔细考虑推动人工智能应用于开放科学领域的发现。在本文中，我们介绍了AI for Open Science (AI4OS)的概念，将其作为AI for Science (AI4Science)的多智能体扩展，其核心原则是通过科学企业中的开放知识转化来最大化开放性。我们使用知识发现与数据挖掘（KDD）的已建立原则来形式化AI4OS的语言。然后，我们讨论了嵌入在AI4Science系统中的三个知识转化阶段，并详细介绍了开放性可以应用于产生AI4OS替代方案的特定点。最后，我们提出了一个数学框架，可以用于量化AI4OS的开放程度。

    AI for Science (AI4Science), particularly in the form of self-driving labs, has the potential to sideline human involvement and hinder scientific discovery within the broader community. While prior research has focused on ensuring the responsible deployment of AI applications, enhancing security, and ensuring interpretability, we also propose that promoting openness in AI4Science discoveries should be carefully considered. In this paper, we introduce the concept of AI for Open Science (AI4OS) as a multi-agent extension of AI4Science with the core principle of maximizing open knowledge translation throughout the scientific enterprise rather than a single organizational unit. We use the established principles of Knowledge Discovery and Data Mining (KDD) to formalize a language around AI4OS. We then discuss three principle stages of knowledge translation embedded in AI4Science systems and detail specific points where openness can be applied to yield an AI4OS alternative. Lastly, we formul
    
[^91]: Reboost基于大型语言模型的文本到SQL、文本到Python和文本到函数的方法——以交通领域的实际应用为例

    Reboost Large Language Model-based Text-to-SQL, Text-to-Python, and Text-to-Function -- with Real Applications in Traffic Domain. (arXiv:2310.18752v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.18752](http://arxiv.org/abs/2310.18752)

    本论文提出了一种基于大型语言模型的文本到SQL、文本到Python和文本到函数的方法，通过查询重写和SQL增强等技术，将模糊信息转化为确切和精确的信息，并引入执行反馈和查询结果增强SQL本身。该方法在交通领域的实际应用中取得了显著的性能提升。

    

    先前的最先进方法在Spider数据集上取得了卓越的执行准确性，该数据集是文本到SQL领域中最大且最多样化的数据集之一。然而，在我们复现业务数据集时，我们观察到性能显著下降。我们对数据集复杂性和问题意图的明确程度进行了比较，并评估了这些差异对提示方法性能的影响。随后，我们开发了一种更适应性更强、更通用的提示方法，主要包括查询重写和SQL增强，分别将模糊信息转化为确切和精确的信息，并通过整合来自数据库内容的执行反馈和查询结果来增强SQL本身。为了防止信息缺失，我们将列的注释、值类型和值示例作为数据库描述的一部分包含在提示中。我们使用大型语言模型进行了实验证明了这种方法的有效性。

    The previous state-of-the-art (SOTA) method achieved a remarkable execution accuracy on the Spider dataset, which is one of the largest and most diverse datasets in the Text-to-SQL domain. However, during our reproduction of the business dataset, we observed a significant drop in performance. We examined the differences in dataset complexity, as well as the clarity of questions' intentions, and assessed how those differences could impact the performance of prompting methods. Subsequently, We develop a more adaptable and more general prompting method, involving mainly query rewriting and SQL boosting, which respectively transform vague information into exact and precise information and enhance the SQL itself by incorporating execution feedback and the query results from the database content. In order to prevent information gaps, we include the comments, value types, and value samples for columns as part of the database description in the prompt. Our experiments with Large Language Model
    
[^92]: LoRAShear: 高效的大型语言模型结构剪枝和知识恢复

    LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery. (arXiv:2310.18356v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.18356](http://arxiv.org/abs/2310.18356)

    LoRAShear是一种高效的大型语言模型结构剪枝和知识恢复方法，通过逐步剪枝和动态微调，有效减少LLMs的占用空间并且保持性能。

    

    大型语言模型（LLMs）已经改变了人工智能的格局，但其庞大的规模在计算成本方面带来了重大挑战。我们引入了LoRAShear，一种新颖的高效方法，用于结构化剪枝LLMs并恢复知识。LoRAShear首先在LoRA模块上创建依赖图，以发现最小删除结构并分析知识分布。然后，它在LoRA适配器上进行渐进式结构剪枝，并实现内在的知识转移，以更好地保留冗余结构中的信息。为了恢复剪枝期间丢失的知识，LoRAShear仔细研究并提出了一种动态微调方案，使用动态数据适配器，以有效缩小与完整模型的性能差距。数值结果表明，只使用一块GPU在几天内，LoRAShear将LLMs的占用空间有效减少了20%，仅有1.0%的性能损失。

    Large Language Models (LLMs) have transformed the landscape of artificial intelligence, while their enormous size presents significant challenges in terms of computational costs. We introduce LoRAShear, a novel efficient approach to structurally prune LLMs and recover knowledge. Given general LLMs, LoRAShear at first creates the dependency graphs over LoRA modules to discover minimally removal structures and analyze the knowledge distribution. It then proceeds progressive structured pruning on LoRA adaptors and enables inherent knowledge transfer to better preserve the information in the redundant structures. To recover the lost knowledge during pruning, LoRAShear meticulously studies and proposes a dynamic fine-tuning schemes with dynamic data adaptors to effectively narrow down the performance gap to the full models. Numerical results demonstrate that by only using one GPU within a couple of GPU days, LoRAShear effectively reduced footprint of LLMs by 20% with only 1.0% performance d
    
[^93]: BioImage.IO Chatbot: 一个以社区知识库增强的生物图像分析个人助手

    BioImage.IO Chatbot: A Personalized Assistant for BioImage Analysis Augmented by Community Knowledge Base. (arXiv:2310.18351v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.18351](http://arxiv.org/abs/2310.18351)

    BioImage.IO Chatbot 是一个根据用户个性化需求提供答案的AI聊天助手，通过汇集和解释多个数据库、工具文档和数据源的信息，以及社区贡献的知识库和优化的检索方法，为生物图像分析工具的使用者提供了个性化、上下文感知的体验，为可访问的科学研究设立了新的标准。

    

    快速扩展的生物图像分析工具景观给专家和新来者都带来了导航挑战。传统的搜索方法在这个复杂环境中常常无法提供帮助。为了解决这个问题，我们引入了BioImage.IO Chatbot，一个为生物图像社区量身定制的基于人工智能的对话助手。这个聊天机器人建立在大型语言模型的基础上，通过聚合和解释来自多个数据库、特定工具文档和结构化数据源的信息，提供个性化的、上下文感知的答案。通过社区贡献的知识库和经过优化的检索方法，BioImage.IO Chatbot 不仅提供个性化的互动，还提供丰富的知识、上下文感知的体验。它从根本上改变了生物学家、生物图像分析师和开发者导航和利用先进的生物图像分析工具的方式，为社区驱动的可访问科学研究树立了新的标准。

    The rapidly expanding landscape of bioimage analysis tools presents a navigational challenge for both experts and newcomers. Traditional search methods often fall short in assisting users in this complex environment. To address this, we introduce the BioImage$.$IO Chatbot, an AI-driven conversational assistant tailored for the bioimage community. Built upon large language models, this chatbot provides personalized, context-aware answers by aggregating and interpreting information from diverse databases, tool-specific documentation, and structured data sources. Enhanced by a community-contributed knowledge base and fine-tuned retrieval methods, the BioImage$.$IO Chatbot offers not just a personalized interaction but also a knowledge-enriched, context-aware experience. It fundamentally transforms the way biologists, bioimage analysts, and developers navigate and utilize advanced bioimage analysis tools, setting a new standard for community-driven, accessible scientific research.
    
[^94]: AllTogether：使用大型语言模型对使用拼接提示进行Web导航的效果进行研究

    AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models. (arXiv:2310.18331v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.18331](http://arxiv.org/abs/2310.18331)

    AllTogether是一个标准化的提示模板，通过增强任务背景表示，提高了大型语言模型（LLMs）在基于HTML的Web导航中的性能。研究结果显示，像GPT-4这样的模型在这类任务中优于较小的模型，并且HTML代码片段的长度和历史轨迹对性能有显著影响。同时，在实时环境反馈方面，优于之前的逐步指导。这项工作为未来LLM驱动的Web代理研究提供了宝贵的见解。

    

    大型语言模型（LLMs）已经成为用于Web导航任务的有前景的代理，它们解释目标并与Web页面进行交互。然而，这种任务中使用拼接提示的效果仍未得到充分探索。我们引入了AllTogether，一个标准化的提示模板，增强任务背景表示，从而提高LLMs在基于HTML的Web导航中的性能。我们通过来自开源Llama-2和可访问的GPT模型的提示学习和指令微调来评估这种方法的效果。我们的结果表明，像GPT-4这样的模型在Web导航任务中优于较小的模型。此外，我们发现HTML代码片段的长度和历史轨迹显著影响性能，并且之前的逐步指导比实时环境反馈更有效。总体而言，我们相信我们的工作为未来LLM驱动的Web代理研究提供了宝贵的见解。

    Large Language Models (LLMs) have emerged as promising agents for web navigation tasks, interpreting objectives and interacting with web pages. However, the efficiency of spliced prompts for such tasks remains underexplored. We introduces AllTogether, a standardized prompt template that enhances task context representation, thereby improving LLMs' performance in HTML-based web navigation. We evaluate the efficacy of this approach through prompt learning and instruction finetuning based on open-source Llama-2 and API-accessible GPT models. Our results reveal that models like GPT-4 outperform smaller models in web navigation tasks. Additionally, we find that the length of HTML snippet and history trajectory significantly influence performance, and prior step-by-step instructions prove less effective than real-time environmental feedback. Overall, we believe our work provides valuable insights for future research in LLM-driven web agents.
    
[^95]: 使用RadGraph和少样本提示的风格感知放射学报告生成

    Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting. (arXiv:2310.17811v1 [cs.AI])

    [http://arxiv.org/abs/2310.17811](http://arxiv.org/abs/2310.17811)

    该论文提出了一种使用RadGraph和少样本提示的风格感知放射学报告生成的方法。通过将报告的内容和风格分开处理，可以避免生成临床不准确的报告。定量评估和人工评估结果均表明该方法表现出良好的性能，并生成与个体放射科医生风格完全相同的报告。

    

    自动从医学影像中生成报告有望改善放射科医生的工作流程。现有方法通过直接从图像生成完整的报告来考虑图像到报告的建模任务。然而，这样混淆了报告的内容（如发现和其属性）与其风格（如格式和词汇选择），可能导致临床不准确的报告。为了解决这个问题，我们提出了一种放射学报告生成的两步方法。首先，我们从图像中提取内容，然后将提取的内容转化为与特定放射科医生风格相匹配的报告。为此，我们利用RadGraph——一种报告的图表示——以及大型语言模型（LLM）。在定量评估中，我们发现我们的方法在性能方面具有益处。通过临床评估者进行的人工评估表明，AI生成的报告与个体放射科医生的风格完全相同，无法区别。

    Automatically generated reports from medical images promise to improve the workflow of radiologists. Existing methods consider an image-to-report modeling task by directly generating a fully-fledged report from an image. However, this conflates the content of the report (e.g., findings and their attributes) with its style (e.g., format and choice of words), which can lead to clinically inaccurate reports. To address this, we propose a two-step approach for radiology report generation. First, we extract the content from an image; then, we verbalize the extracted content into a report that matches the style of a specific radiologist. For this, we leverage RadGraph -- a graph representation of reports -- together with large language models (LLMs). In our quantitative evaluations, we find that our approach leads to beneficial performance. Our human evaluation with clinical raters highlights that the AI-generated reports are indistinguishably tailored to the style of individual radiologist 
    
[^96]: CodeFusion: 一种用于代码生成的预训练扩散模型

    CodeFusion: A Pre-trained Diffusion Model for Code Generation. (arXiv:2310.17680v1 [cs.SE])

    [http://arxiv.org/abs/2310.17680](http://arxiv.org/abs/2310.17680)

    CodeFusion是一种预训练的代码生成模型，通过扩散的方式解决了自然语言代码生成中遇到的限制，实验表明其在准确率和多样性上优于最先进的自回归系统。

    

    假设一个开发者只能修改其最后一行代码，在正确之前，他们需要多少次从头开始编写函数呢？自然语言代码生成的自回归模型也有类似的限制：它们不容易重新考虑之前生成的标记。我们介绍了一种名为CodeFusion的预训练扩散代码生成模型，通过迭代地对以编码的自然语言为条件的完整程序进行去噪，以解决这个限制。我们针对Bash、Python和Microsoft Excel条件格式(CF)规则的自然语言到代码生成任务对CodeFusion进行评估。实验结果显示，CodeFusion（75M参数）在top-1准确率上表现与最先进的自回归系统（350M-175B参数）相当，并且在top-3和top-5准确率上表现优于它们，这是由于它在多样性与质量之间的平衡更好。

    Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.
    
[^97]: FormaT5: 以自然语言生成条件表格格式化的抽样和示例

    FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language. (arXiv:2310.17306v1 [cs.AI])

    [http://arxiv.org/abs/2310.17306](http://arxiv.org/abs/2310.17306)

    FormaT5是一个基于转换器的模型，可以根据目标表格和自然语言描述生成数据相关的条件格式规则。为了解决描述不足的问题，FormaT5通过放弃目标的方式学习预测占位符。

    

    表格的格式化是可视化、展示和分析中的重要属性。电子表格软件允许用户通过编写数据相关的条件格式规则来自动格式化表格。但对用户来说，编写这样的规则通常是具有挑战性的，因为它要求他们理解和实现底层逻辑。我们提出了一个基于转换器的模型FormaT5，可以根据目标表格和期望的格式逻辑的自然语言描述生成一个条件格式规则。我们发现，用户为这些任务提供的描述通常是不明确或含糊的，这使得代码生成系统难以在一步中准确学习到所需的规则。为了解决这个规范不足的问题并减少参数错误，FormaT5通过放弃目标的方式学习预测占位符。这些占位符可以由第二个模型或者当可用的行示例时，由一个基于示例的编程系统填充。

    Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system
    
[^98]: CosmosDSR -- 一种使用无味卡尔曼滤波器自动检测和跟踪轨道碎片的方法论

    CosmosDSR -- a methodology for automated detection and tracking of orbital debris using the Unscented Kalman Filter. (arXiv:2310.17158v1 [astro-ph.EP])

    [http://arxiv.org/abs/2310.17158](http://arxiv.org/abs/2310.17158)

    CosmosDSR是一种新颖的方法，将YOLOv3与无味卡尔曼滤波器结合起来，用于自动检测和跟踪轨道碎片。在使用SPARK数据集进行训练和测试时，YOLOv3表现出很高的准确度，并且CosmosDSR和LKF都能准确地跟踪卫星。

    

    Kessler综合征是指频繁的太空活动产生的升级的太空碎片，威胁到未来的太空探索。解决这个问题至关重要。过去的研究强调了YOLO目标检测器和线性卡尔曼滤波器的组合用于物体检测和跟踪。在此基础上，我们的项目引入了CosmosDSR，一种新颖的方法，将YOLOv3与无味卡尔曼滤波器结合起来，用于在序列图像中跟踪卫星，与线性卡尔曼滤波器相比。使用卢森堡大学的SPARK数据集进行训练和测试，YOLOv3准确地检测和分类了所有卫星类别（mAP=97.18％，F1=0.95），出现了少量错误（TP=4163，FP=209，FN=237）。CosmosDSR和LKF都准确跟踪卫星（UKF：MSE=2.83/RMSE=1.66，LKF：...

    The Kessler syndrome refers to the escalating space debris from frequent space activities, threatening future space exploration. Addressing this issue is vital. Several AI models, including Convolutional Neural Networks (CNN), Kernel Principal Component Analysis (KPCA), and Model-Agnostic Meta-Learning (MAML), have been assessed with various data types. Earlier studies highlighted the combination of the YOLO object detector and a linear Kalman filter for object detection and tracking. Building on this, our project introduces CosmosDSR, a novel methodology combining YOLOv3 with an Unscented Kalman Filter for tracking satellites in sequential images, compared to a linear Kalman filter. Using the SPARK dataset from the University of Luxembourg for training and testing, the YOLOv3 precisely detected and classified all satellite categories (mAP=97.18%, F1=0.95) with few errors (TP=4163, FP=209, FN=237). Both CosmosDSR and the LKF tracked satellites accurately (UKF: MSE=2.83/RMSE=1.66, LKF: 
    
[^99]: 通过回溯法纠正，减少摘要中的幻觉

    Correction with Backtracking Reduces Hallucination in Summarization. (arXiv:2310.16176v1 [cs.CL])

    [http://arxiv.org/abs/2310.16176](http://arxiv.org/abs/2310.16176)

    本文介绍了一种简单而有效的技术，CoBa，用于减少摘要中的幻觉。该方法通过测量条件词概率和上下文词距离的统计信息进行幻觉检测，并通过直观的回溯法进行减轻。实验证明，CoBa在减少摘要幻觉方面是有效且高效的。

    

    摘要生成旨在生成源文件的自然语言摘要，既简洁又保留重要元素。尽管最近取得了一些进展，但神经文本摘要模型容易产生幻觉（或更准确地说是混淆），即生成的摘要包含源文件中没有根据的细节。在本文中，我们引入了一种简单而有效的技术，CoBa，用于减少摘要中的幻觉。该方法基于两个步骤：幻觉检测和减轻。我们展示了通过测量有关条件词概率和上下文词距离的简单统计信息可以实现前者。此外，我们还证明了直观的回溯法在减轻幻觉方面的惊人效果。我们在三个文本摘要基准数据集上对所提出的方法进行了全面评估。结果表明，CoBa在减少摘要幻觉方面是有效且高效的。

    Abstractive summarization aims at generating natural language summaries of a source document that are succinct while preserving the important elements. Despite recent advances, neural text summarization models are known to be susceptible to hallucinating (or more correctly confabulating), that is to produce summaries with details that are not grounded in the source document. In this paper, we introduce a simple yet efficient technique, CoBa, to reduce hallucination in abstractive summarization. The approach is based on two steps: hallucination detection and mitigation. We show that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words. Further, we demonstrate that straight-forward backtracking is surprisingly effective at mitigation. We thoroughly evaluate the proposed method with prior art on three benchmark datasets for text summarization. The results show that CoBa is effective and efficient in reducing hall
    
[^100]: FANToM: 在交互中对机器心智理论进行压力测试的基准

    FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions. (arXiv:2310.15421v1 [cs.CL])

    [http://arxiv.org/abs/2310.15421](http://arxiv.org/abs/2310.15421)

    FANToM是一个新的基准，用于通过问答在信息不对称的对话环境中压力测试机器的心智理论。这个基准对最先进的大型语言模型来说具有挑战性，即使是具有思维链推理和微调的模型也比人类表现得差。

    

    目前关于心智理论（ToM）的评估主要集中在使用缺乏互动性的被动故事，我们介绍了FANToM，一个新的基准，通过问答在信息不对称的对话环境中进行心智理论的压力测试。我们的基准结合了心理学中的重要理论要求和对评估大型语言模型（LLM）时必要的经验考虑。特别地，我们制定了多种类型的问题，要求相同的基本推理来识别LLM中不存在或虚假的心智理论能力。我们展示了FANToM对最先进的LLM来说具有挑战性，即使是具有思维链推理和微调的LLM也表现比人类差得多。

    Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.
    
[^101]: 学习可解释的规则以实现可扩展的数据表示和分类

    Learning Interpretable Rules for Scalable Data Representation and Classification. (arXiv:2310.14336v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14336](http://arxiv.org/abs/2310.14336)

    这项研究提出了一种名为RRL的新型分类器，通过自动学习可解释的非模糊规则，实现了数据表示和分类的良好可扩展性和解释性。

    

    基于规则的模型（如决策树）在需要高模型解释性的场景中被广泛使用，因为它们具有透明的内部结构和良好的模型表达能力。然而，由于离散的参数和结构，基于规则的模型在优化方面很难应对大规模的数据集。集成方法和模糊/软规则通常用于提高性能，但会牺牲模型的解释性。为了获得良好的可扩展性和可解释性，我们提出了一种新的分类器，称为基于规则的表示学习器（RRL），它可以自动学习用于数据表示和分类的可解释的非模糊规则。为了有效训练不可微分的RRL，我们将其映射到连续空间，并提出一种称为梯度嵌入的新的训练方法，可以使用梯度下降直接优化离散模型。此外，还设计了一种新颖的逻辑激活函数，以增加RRL的可扩展性，并使其能够进行判别。

    Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. A novel design of logical activation functions is also devised to increase the scalability of RRL and enable it to discr
    
[^102]: AI反馈促进的质量-多样性算法

    Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])

    [http://arxiv.org/abs/2310.13032](http://arxiv.org/abs/2310.13032)

    基于AI反馈的质量-多样性（QDAIF）算法利用语言模型来生成和评估创造性写作，比传统算法更广泛地覆盖高质量样本的搜索空间。

    

    在许多文本生成问题中，用户可能不仅偏好单一回复，而是希望得到多样性的高质量输出以供选择。质量-多样性（QD）搜索算法旨在通过不断改进和多样化候选人群来实现这一目标。然而，QD在创作性写作等质性领域的应用受到算法指定质量和多样性度量的困难的限制。有趣的是，最近语言模型（LMs）的发展使得通过AI反馈指导搜索成为可能，其中LMs在自然语言中被提示来评估文本的质性方面。借助这一进展，我们引入了通过AI反馈实现的质量-多样性算法（QDAIF），其中进化算法应用LMs来生成变异并评估候选文本的质量和多样性。在创作性写作领域的评估中，与非QDAIF算法相比，QDAIF更广泛地覆盖高质量样本的指定搜索空间。

    In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-
    
[^103]: 联邦多目标学习

    Federated Multi-Objective Learning. (arXiv:2310.09866v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.09866](http://arxiv.org/abs/2310.09866)

    本研究提出了一种新的联邦多目标学习（FMOL）框架，在满足多代理多任务学习应用的分布式性质和数据隐私需求的同时，支持不同客户端上的不同目标函数集合。通过引入联邦学习的范式，将多目标优化（MOO）推广到联邦学习领域。

    

    在最近几年中，多目标优化（MOO）作为许多多代理多任务学习应用的基础问题出现。然而，现有的MOO算法仍局限于集中式学习环境，无法满足这些多代理多任务学习应用的分布式性质和数据隐私需求。这激发了我们提出一种新的联邦多目标学习（FMOL）框架，其中多个客户端在保持他们的训练数据私密的同时，分布式协作解决一个MOO问题。值得注意的是，我们的FMOL框架允许不同客户端上的不同目标函数集合，以支持广泛的应用，这首次将MOO形式化推广到联邦学习范式中。对于这个FMOL框架，我们提出了两种新的联邦多目标优化（FMOO）算法，称为联邦多梯度下降平均（FMGDA）和联邦随机梯度下降（Federated SGD）。

    In recent years, multi-objective optimization (MOO) emerges as a foundational problem underpinning many multi-agent multi-task learning applications. However, existing algorithms in MOO literature remain limited to centralized learning settings, which do not satisfy the distributed nature and data privacy needs of such multi-agent multi-task learning applications. This motivates us to propose a new federated multi-objective learning (FMOL) framework with multiple clients distributively and collaboratively solving an MOO problem while keeping their training data private. Notably, our FMOL framework allows a different set of objective functions across different clients to support a wide range of applications, which advances and generalizes the MOO formulation to the federated learning paradigm for the first time. For this FMOL framework, we propose two new federated multi-objective optimization (FMOO) algorithms called federated multi-gradient descent averaging (FMGDA) and federated stoc
    
[^104]: MIR2:面向通过互信息正则化进行可证明鲁棒多智能体强化学习

    MIR2: Towards Provably Robust Multi-Agent Reinforcement Learning by Mutual Information Regularization. (arXiv:2310.09833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.09833](http://arxiv.org/abs/2310.09833)

    MIR2提出了一种针对鲁棒多智能体强化学习的方法，通过在常规情况下训练策略并最小化互信息作为鲁棒正则化，实现了在不准备每种可能的最坏情况的情况下提升鲁棒性的目标。

    

    鲁棒多智能体强化学习(MARL)对于未知盟友的不确定或最坏情况行动需要具备弹性。现有的鲁棒MARL中的最大最小优化技术通过训练智能体抵抗最坏情况的对手来增强鲁棒性，但随着智能体数量的增加，这种方法变得难以操作，导致最坏情况的数量呈指数级增长。试图简化这种复杂性往往会导致过于悲观的策略、在各种情况下鲁棒性不足和高计算需求。与这些方法不同，人类在学习适应性和鲁棒行为时自然而然地不需要准备每种可能的最坏情况。受此启发，我们提出了MIR2，它在常规情况下训练策略，并将互信息最小化作为鲁棒正则化。从理论上讲，我们将鲁棒性视为一个推理问题，并证明了在历史和行动之间最小化互信息隐含地最大化了鲁棒性的下界。

    Robust multi-agent reinforcement learning (MARL) necessitates resilience to uncertain or worst-case actions by unknown allies. Existing max-min optimization techniques in robust MARL seek to enhance resilience by training agents against worst-case adversaries, but this becomes intractable as the number of agents grows, leading to exponentially increasing worst-case scenarios. Attempts to simplify this complexity often yield overly pessimistic policies, inadequate robustness across scenarios and high computational demands. Unlike these approaches, humans naturally learn adaptive and resilient behaviors without the necessity of preparing for every conceivable worst-case scenario. Motivated by this, we propose MIR2, which trains policy in routine scenarios and minimize Mutual Information as Robust Regularization. Theoretically, we frame robustness as an inference problem and prove that minimizing mutual information between histories and actions implicitly maximizes a lower bound on robust
    
[^105]: OptiMUS: 使用mip求解器和大规模语言模型的优化建模

    OptiMUS: Optimization Modeling Using mip Solvers and large language models. (arXiv:2310.06116v1 [cs.AI])

    [http://arxiv.org/abs/2310.06116](http://arxiv.org/abs/2310.06116)

    介绍了OptiMUS，一种基于大规模语言模型的优化建模代理，用于解决MILP问题。该代理能够自动生成数学模型、编写和调试求解器代码，并具有较高的解决率。

    

    优化问题在各个领域中普遍存在，从制造和分销到医疗保健。然而，大多数这类问题仍然是通过手工启发式解决而不是通过最先进的求解器进行最优解，因为制定和解决这些问题所需的专业知识限制了优化工具和技术的广泛应用。我们介绍了OptiMUS，一种基于大规模语言模型(LLM)的代理，旨在从自然语言描述中制定和解决MILP问题。OptiMUS能够开发数学模型，编写和调试求解器代码，开发测试，并检查生成解的有效性。为了对我们的代理进行基准测试，我们提供了NLP4LP，这是一个线性规划(LP)和混合整数线性规划(MILP)问题的新数据集。我们的实验表明，与基本的LLM提示策略相比，OptiMUS能够解决更多问题，解决率提高了67％。OptiMUS的代码和NLP4LP数据集可在\href{https://github中获得.

    Optimization problems are pervasive across various sectors, from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers, as the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. We introduce OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve MILP problems from their natural language descriptions. OptiMUS is capable of developing mathematical models, writing and debugging solver code, developing tests, and checking the validity of generated solutions. To benchmark our agent, we present NLP4LP, a novel dataset of linear programming (LP) and mixed integer linear programming (MILP) problems. Our experiments demonstrate that OptiMUS is able to solve 67\% more problems compared to a basic LLM prompting strategy. OptiMUS code and NLP4LP dataset are available at \href{https://github
    
[^106]: 利用扩散分离表示来缓解不完全规定的视觉任务中的捷径问题

    Leveraging Diffusion Disentangled Representations to Mitigate Shortcuts in Underspecified Visual Tasks. (arXiv:2310.02230v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.02230](http://arxiv.org/abs/2310.02230)

    本文提出了一种利用扩散分离表示来处理不完全规定的视觉任务中捷径学习问题的方法，通过生成合成反事实来促进模型的多样性，从而使模型能够忽略捷径线索并达到与其他方法相当的性能。

    

    数据中的伪相关性，其中多个线索预测目标标签，通常会导致捷径学习现象，即模型可能依赖于错误的、易于学习的线索而忽略可靠线索。在这项工作中，我们提出了一个利用扩散概率模型（DPMs）生成合成反事实的集成多样化框架。我们发现，即使训练数据中这些线索高度相关，DPMs具有独立表示多个视觉线索的固有能力。我们利用这个特性来促进模型的多样性，并在几个多样化目标上实证证明了该方法的有效性。我们展示了扩散引导的多样化可以使模型避开捷径线索的注意，实现了与需要额外数据收集的先前方法可比较的集成多样性性能。

    Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to shortcut learning phenomena, where a model may rely on erroneous, easy-to-learn, cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting the generation of synthetic counterfactuals using Diffusion Probabilistic Models (DPMs). We discover that DPMs have the inherent capability to represent multiple visual cues independently, even when they are largely correlated in the training data. We leverage this characteristic to encourage model diversity and empirically show the efficacy of the approach with respect to several diversification objectives. We show that diffusion-guided diversification can lead models to avert attention from shortcut cues, achieving ensemble diversity performance comparable to previous methods requiring additional data collection.
    
[^107]: NoxTrader: 基于LSTM的股票收益动量预测

    NoxTrader: LSTM-Based Stock Return Momentum Prediction. (arXiv:2310.00747v1 [q-fin.PM])

    [http://arxiv.org/abs/2310.00747](http://arxiv.org/abs/2310.00747)

    NoxTrader是一种基于LSTM的股票收益动量预测模型，旨在实现中长期盈利。它通过对历史交易数据进行时间序列分析，并进行特征工程、预测建模、参数配置和回测验证，证明了算法交易模型在现实交易场景中的潜在可行性。

    

    我们介绍了NoxTrader，它旨在用于投资组合构建和交易执行，旨在产生盈利结果。NoxTrader的主要关注点是股票市场交易，重点是培养中长期利润。NoxTrader的学习过程基于对历史交易数据的洞察力的吸收，主要依靠时间序列分析，因为所使用的数据集的固有性质。我们详细描述了包括数据获取、特征工程、预测建模、参数配置、建立严谨的回测框架等的顺序进展，并最终将NoxTrader定位为算法交易模型在现实交易场景中的潜在可行性的证明。

    We introduce NoxTrader, which is designed for portfolio construction and trading execution, aims at generating profitable outcomes. The primary focus of NoxTrader is on stock market trading with an emphasis on cultivating moderate to long-term profits. The underlying learning process of NoxTrader hinges on the assimilation of insights gleaned from historical trading data, primarily hinging on time-series analysis due to the inherent nature of the employed dataset. We delineate the sequential progression encompassing data acquisition, feature engineering, predictive modeling, parameter configuration, establishment of a rigorous backtesting framework, and ultimately position NoxTrader as a testament to the prospective viability of algorithmic trading models within real-world trading scenarios.
    
[^108]: 因果自我解释中的D-分离

    D-Separation for Causal Self-Explanation. (arXiv:2309.13391v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.13391](http://arxiv.org/abs/2309.13391)

    本研究提出了一种新的准则，最小条件依赖（MCD）准则，来揭示因果解释。通过最小化选择理由候选项上未选择部分与目标标签的依赖，强制选择所有的标签原因。

    

    理性化是一种用于自然语言处理模型的自我解释框架。传统方法通常使用最大互信息（MMI）准则来找到最能说明目标标签的理由。然而，这个准则可能受到与因果解释或目标标签相关的虚假特征的影响。我们提出了一种新颖的准则来揭示因果解释，称为最小条件依赖（MCD）准则，该准则建立在我们发现的非因果特征与目标标签通过因果解释被“分离”之上。通过在选择的理由候选项上给出未选择部分与目标标签之间的依赖最小化，强制选择所有的标签原因。在这项研究中，我们使用一种简单而实用的依赖度量，具体是KL散度，来验证我们提出的MCD准则。

    Rationalization is a self-explaining framework for NLP models. Conventional work typically uses the maximum mutual information (MMI) criterion to find the rationale that is most indicative of the target label. However, this criterion can be influenced by spurious features that correlate with the causal rationale or the target label. Instead of attempting to rectify the issues of the MMI criterion, we propose a novel criterion to uncover the causal rationale, termed the Minimum Conditional Dependence (MCD) criterion, which is grounded on our finding that the non-causal features and the target label are \emph{d-separated} by the causal rationale. By minimizing the dependence between the unselected parts of the input and the target label conditioned on the selected rationale candidate, all the causes of the label are compelled to be selected. In this study, we employ a simple and practical measure of dependence, specifically the KL-divergence, to validate our proposed MCD criterion. Empir
    
[^109]: ChatGPT的行为随时间变化如何？

    How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])

    [http://arxiv.org/abs/2307.09009](http://arxiv.org/abs/2307.09009)

    本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。

    

    GPT-3.5和GPT-4是两种广泛使用的大型语言模型（LLM）服务。然而，这些模型何时以及如何进行更新是不透明的。在这里，我们对GPT-3.5和GPT-4的2023年3月和2023年6月版本进行了评估，涉及四项不同的任务：1）解决数学问题，2）回答敏感/危险问题，3）生成代码和4）视觉推理。我们发现，GPT-3.5和GPT-4的性能和行为在时间上可以有很大的变化。例如，GPT-4（2023年3月）在识别质数方面表现非常出色（准确率为97.6%），但GPT-4（2023年6月）在相同的问题上表现非常差（准确率为2.4%）。有趣的是，GPT-3.5（2023年6月）在这个任务上比GPT-3.5（2023年3月）要好得多。GPT-4在6月份对回答敏感问题的意愿较3月份要低，而无论是GPT-4还是GPT-3.5在6月份的代码生成中都有更多的格式错误。总体而言，我们的发现表明相同LLM服务的行为在相对较短的时间内可以发生重大变化。

    GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
    
[^110]: 通过3分钟的人类反馈进行扩散模型的有限采样

    Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback. (arXiv:2307.02770v1 [cs.CV])

    [http://arxiv.org/abs/2307.02770](http://arxiv.org/abs/2307.02770)

    该论文研究了通过3分钟的人类反馈来扩散模型的有限采样，并表明仅几分钟的人类反馈生成的标签就足够实现图像的审查任务。

    

    近期，扩散模型在高质量图像生成方面取得了显著成果。然而，有时预训练的扩散模型在某种程度上存在错误对齐的问题，即模型可以生成好的图像，但有时会生成不理想的图像。如果是这样，我们只需要阻止生成糟糕的图像，我们称之为审查任务。在这项工作中，我们提出了使用经过最小人类反馈训练的奖励模型对预训练扩散模型进行审查生成。我们展示了通过极高的人类反馈效率可以实现审查，并且仅几分钟的人类反馈生成的标签就足够了。

    Diffusion models have recently shown remarkable success in high-quality image generation. Sometimes, however, a pre-trained diffusion model exhibits partial misalignment in the sense that the model can generate good images, but it sometimes outputs undesirable images. If so, we simply need to prevent the generation of the bad images, and we call this task censoring. In this work, we present censored generation with a pre-trained diffusion model using a reward model trained on minimal human feedback. We show that censoring can be accomplished with extreme human feedback efficiency and that labels generated with a mere few minutes of human feedback are sufficient. Code available at: https://github.com/tetrzim/diffusion-human-feedback.
    
[^111]: SAMAug: Segment Anything Model的点提示增强方法

    SAMAug: Point Prompt Augmentation for Segment Anything Model. (arXiv:2307.01187v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.01187](http://arxiv.org/abs/2307.01187)

    SAMAug是一种用于增强交互式图像分割性能的方法，通过生成增强的点提示，结合初始提示，可以提高Segment Anything Model的分割结果。

    

    本文介绍了SAMAug，一种用于增强交互式图像分割性能的新型视觉点提示增强方法。SAMAug生成增强的点提示，以提供更多关于用户意图的信息给SAM。SAM从一个初始点提示开始生成一个初始掩码，然后将其输入我们提出的SAMAug来生成增强的点提示。通过结合这些额外的点，SAM可以基于增强的点提示和初始提示生成增强的分割掩码，从而提高分割性能。我们使用了四种不同的点增强策略进行评估：随机采样，基于最大差异熵的采样，最大距离和显著性。在COCO、Fundus、COVID QUEx和ISIC2018数据集上的实验证实了SAMAug可以提升SAM的分割结果，尤其是使用最大距离和显著性。SAMAug证明了其应用潜力。

    This paper introduces SAMAug, a novel visual point augmentation method for the Segment Anything Model (SAM) that enhances interactive image segmentation performance. SAMAug generates augmented point prompts to provide more information about the user's intention to SAM. Starting with an initial point prompt, SAM produces an initial mask, which is then fed into our proposed SAMAug to generate augmented point prompts. By incorporating these extra points, SAM can generate augmented segmentation masks based on both the augmented point prompts and the initial prompt, resulting in improved segmentation performance. We conducted evaluations using four different point augmentation strategies: random sampling, sampling based on maximum difference entropy, maximum distance, and saliency. Experiment results on the COCO, Fundus, COVID QUEx, and ISIC2018 datasets show that SAMAug can boost SAM's segmentation results, especially using the maximum distance and saliency. SAMAug demonstrates the potenti
    
[^112]: 软抓取：针对可信度的规范化要求

    Soft Gripping: Specifying for Trustworthiness. (arXiv:2307.01159v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2307.01159](http://arxiv.org/abs/2307.01159)

    本研究讨论了在开发软机器人系统时制定规范的重要性，并针对购物杂货拾取任务的软夹持器提出了一个广泛的示例规范，涵盖了可靠性、安全性、适应性、可预测性、伦理和法规等功能和非功能要求。

    

    软机器人技术是一种新兴技术，工程师用来制造各种应用的灵活设备。为了推动软机器人的广泛应用，确保它们的可信度至关重要；如果软机器人不可信，它们就无法发挥其全部潜力。为了证明可信度，需要制定一个规范来定义什么是可信的。然而，即使对于软机器人夹持器这样的软机器人领域中最成熟的领域之一，软机器人社区在制定规范方面的关注度仍然很少。在这项工作中，我们讨论了在开发软机器人系统期间开发规范的重要性，并提出了一个用于购物杂货拾取任务的软夹持器的广泛示例规范。所提议的规范涵盖了功能和非功能要求，如可靠性，安全性，适应性，可预测性，伦理和法规。

    Soft robotics is an emerging technology in which engineers create flexible devices for use in a variety of applications. In order to advance the wide adoption of soft robots, ensuring their trustworthiness is essential; if soft robots are not trusted, they will not be used to their full potential. In order to demonstrate trustworthiness, a specification needs to be formulated to define what is trustworthy. However, even for soft robotic grippers, which is one of the most mature areas in soft robotics, the soft robotics community has so far given very little attention to formulating specifications. In this work, we discuss the importance of developing specifications during development of soft robotic systems, and present an extensive example specification for a soft gripper for pick-and-place tasks for grocery items. The proposed specification covers both functional and non-functional requirements, such as reliability, safety, adaptability, predictability, ethics, and regulations. We al
    
[^113]: 以提示为基础的个性化冷启动推荐的研究

    Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])

    [http://arxiv.org/abs/2306.17256](http://arxiv.org/abs/2306.17256)

    本研究旨在解决个性化冷启动推荐问题，通过利用预训练语言模型的能力，将推荐过程转化为自然语言情感分析，提供适用于创业企业和用户参与历史不足的平台的个性化推荐。

    

    推荐系统在根据用户过去的行为帮助用户发现与其兴趣相符的信息方面发挥着关键作用。然而，当用户和物品之间的历史交互记录不可用时，开发个性化推荐系统变得具有挑战性，这就是所谓的系统冷启动推荐问题。此问题在创业企业或用户参与历史不足的平台中尤为突出。以往的研究集中在用户或物品的冷启动场景，其中系统仍然通过在同一领域中的历史用户和物品交互进行训练来为新用户或物品提供推荐，而无法解决我们的问题。为了弥合这一鸿沟，我们的研究引入了一种创新且有效的方法，利用预训练语言模型的能力。我们将推荐过程转化为自然语言情感分析，其中包含用户资料和物品属性的信息。

    Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
    
[^114]: 可分离的物理信息神经网络

    Separable Physics-Informed Neural Networks. (arXiv:2306.15969v1 [cs.LG])

    [http://arxiv.org/abs/2306.15969](http://arxiv.org/abs/2306.15969)

    这项研究提出了一种可分离的物理信息神经网络（SPINN），通过逐个处理轴来显著减少了多维 PDE 中的网络传播数量，并使用正向模式自动微分降低了计算成本，使得可以在单个普通 GPU 上使用大量的配点。

    

    物理信息神经网络(PINNs)最近已经成为有希望的基于数据的PDE求解器，在各种PDE上显示出令人鼓舞的结果。然而，训练PINNs来解决多维PDE和逼近高度复杂解函数存在根本限制。在这些具有挑战性的PDE上所需的训练点数量(配点)大大增加，但由于昂贵的计算成本和庞大的内存开销，其受到严重限制。为了解决这个问题，我们提出了一种用于PINNs的网络架构和训练算法。所提出的方法，可分离的PINN (SPINN)，在多维PDE中按轴逐个处理，从而显著减少了网络传播的数量，不同于传统PINNs中的逐点处理。我们还提出使用正向模式自动微分来降低计算PDE残差的计算成本，从而在单个普通GPU上可以使用大量的配点(>10^7)。

    Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. However, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate highly complex solution functions. The number of training points (collocation points) required on these challenging PDEs grows substantially, but it is severely limited due to the expensive computational costs and heavy memory overhead. To overcome this issue, we propose a network architecture and training algorithm for PINNs. The proposed method, separable PINN (SPINN), operates on a per-axis basis to significantly reduce the number of network propagations in multi-dimensional PDEs unlike point-wise processing in conventional PINNs. We also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points (>10^7) on a single commodity GPU. The
    
[^115]: 为大型图表示简化和增强Transformer

    Simplifying and Empowering Transformers for Large-Graph Representations. (arXiv:2306.10759v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10759](http://arxiv.org/abs/2306.10759)

    本文通过实验证明，在大型图上使用一层注意力即可获得令人惊讶的竞争性能，挑战了在语言和视觉任务中复杂模型的应用。这促使我们重新思考在大型图上设计Transformer的理念，以提高可扩展性。

    

    在大型图上学习表示是一个长期存在的挑战，因为其中涉及了大量数据点之间的相互依赖关系。Transformer作为一种新兴的用于图结构数据的基本编码器类别，由于其全局注意力可以捕捉到邻节点之外的所有对影响，因此在小型图上表现出了有希望的性能。尽管如此，现有方法往往继承了Transformer在语言和视觉任务中的思想，并通过堆叠深层多头注意力来采用复杂的模型。本文通过关于节点属性预测基准的实验证明，即使只使用一层注意力也能在节点数量从千级到十亿级的范围内带来令人惊讶的竞争性能。这鼓励我们重新思考在大型图上设计Transformer的理念，其中全局注意力是一个阻碍可扩展性的计算开销。我们将提出的方案称为简化图Transformer。

    Learning representations on large-sized graphs is a long-standing challenge due to the inter-dependence nature involved in massive data points. Transformers, as an emerging class of foundation encoders for graph-structured data, have shown promising performance on small graphs due to its global attention capable of capturing all-pair influence beyond neighboring nodes. Even so, existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated models by stacking deep multi-head attentions. In this paper, we critically demonstrate that even using a one-layer attention can bring up surprisingly competitive performance across node property prediction benchmarks where node numbers range from thousand-level to billion-level. This encourages us to rethink the design philosophy for Transformers on large graphs, where the global attention is a computation overhead hindering the scalability. We frame the proposed scheme as Simplified Graph Trans
    
[^116]: QH9：QM9分子的量子哈密顿预测基准测试

    QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules. (arXiv:2306.09549v1 [physics.chem-ph])

    [http://arxiv.org/abs/2306.09549](http://arxiv.org/abs/2306.09549)

    该论文提出了一种新的量子哈密顿数据集QH9，用于为各种分子提供精确的哈密顿矩阵。通过设计基准任务，展示了当前机器学习模型有能力预测任意分子的哈密顿矩阵。

    

    监督式机器学习方法越来越被用于加速电子结构预测，作为第一性原理计算方法（如密度泛函理论（DFT））的替代品。虽然许多量子化学数据集侧重于化学性质和原子力，但准确且高效地预测哈密顿矩阵的能力是非常重要和基本的物理量，它确定了物理系统和化学性质的量子状态。在这项工作中，我们生成了一个新的量子哈密顿数据集，命名为QH9，基于QM9数据集为2,399个分子动力学轨迹和130,831个稳定分子几何形态提供精确的哈密顿矩阵。通过设计各种分子的基准任务，我们展示了当前机器学习模型有能力预测任意分子的哈密顿矩阵。QH9数据集和基准模型都提供。

    Supervised machine learning approaches have been increasingly used in accelerating electronic structure prediction as surrogates of first-principle computational methods, such as density functional theory (DFT). While numerous quantum chemistry datasets focus on chemical properties and atomic forces, the ability to achieve accurate and efficient prediction of the Hamiltonian matrix is highly desired, as it is the most important and fundamental physical quantity that determines the quantum states of physical systems and chemical properties. In this work, we generate a new Quantum Hamiltonian dataset, named as QH9, to provide precise Hamiltonian matrices for 2,399 molecular dynamics trajectories and 130,831 stable molecular geometries, based on the QM9 dataset. By designing benchmark tasks with various molecules, we show that current machine learning models have the capacity to predict Hamiltonian matrices for arbitrary molecules. Both the QH9 dataset and the baseline models are provided
    
[^117]: 基于范数引导的文本到图像生成的潜空间探索

    Norm-guided latent space exploration for text-to-image generation. (arXiv:2306.08687v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.08687](http://arxiv.org/abs/2306.08687)

    本研究观察到当前训练过程中，扩散模型只观测到具有狭窄范值的输入，这对于图像生成方法和少样本学习任务具有重要影响。本文提出了一种新的插值方法，并定义了一种基于范数的非欧几里得度量，以解决这个问题。

    

    文本到图像扩散模型在合成各种概念的新构图和场景方面显示出巨大潜力。然而，初始种子的潜空间仍不被很好理解，并且其结构已被证明会影响各种概念的生成。具体来说，使用标准的欧几里得或球面度量在潜空间中进行插值和寻找种子集的质心等简单操作性能较差。本文观察到，在当前的训练过程中，扩散模型只观测到具有狭窄范值的输入。这对于依赖于种子操作进行图像生成的方法，以及在少样本和长尾学习任务中的应用，具有重要影响。为了解决这个问题，我们提出了一种新方法，在两个种子之间进行插值，并证明它定义了一种基于范数的先验的新的非欧几里得度量。我们描述了一种简单而高效的算法以解决这个问题。

    Text-to-image diffusion models show great potential in synthesizing a large variety of concepts in new compositions and scenarios. However, the latent space of initial seeds is still not well understood and its structure was shown to impact the generation of various concepts. Specifically, simple operations like interpolation and finding the centroid of a set of seeds perform poorly when using standard Euclidean or spherical metrics in the latent space. This paper makes the observation that, in current training procedures, diffusion models observed inputs with a narrow range of norm values. This has strong implications for methods that rely on seed manipulation for image generation, with applications to few-shot and long-tail learning tasks. To address this issue, we propose a novel method for interpolating between two seeds and demonstrate that it defines a new non-Euclidean metric that takes into account a norm-based prior on seeds. We describe a simple yet efficient algorithm for ap
    
[^118]: 非线性潜变量层次模型的识别

    Identification of Nonlinear Latent Hierarchical Models. (arXiv:2306.07916v1 [cs.LG])

    [http://arxiv.org/abs/2306.07916](http://arxiv.org/abs/2306.07916)

    本文提出了一种方法，可以在观测变量由因果相关的潜变量生成的非线性潜变量层次因果模型中实现因果结构和潜变量的可识别性。

    

    从观测数据中识别潜变量和因果结构对于许多涉及生物数据、医学数据和非结构化数据（如图像和语言）的实际应用至关重要。然而，当观测变量由因果相关的潜变量生成，并且关系是非线性的时，这项任务可能非常具有挑战性。在这项工作中，我们研究了非线性潜变量层次因果模型的识别问题，在这种模型中，观测变量由一组因果相关的潜变量生成，有些潜变量可能没有观察到的后代。我们证明，在温和的假设下可以实现因果结构和潜变量的可识别性：对于因果结构，我们允许图中任意两个变量之间存在多条路径，这放宽了先前工作中的潜变量树假设；对于结构函数，我们没有进行参数假设，因此可以允许基因

    Identifying latent variables and causal structures from observational data is essential to many real-world applications involving biological data, medical data, and unstructured data such as images and languages. However, this task can be highly challenging, especially when observed variables are generated by causally related latent variables and the relationships are nonlinear. In this work, we investigate the identification problem for nonlinear latent hierarchical causal models in which observed variables are generated by a set of causally related latent variables, and some latent variables may not have observed children. We show that the identifiability of both causal structure and latent variables can be achieved under mild assumptions: on causal structures, we allow for the existence of multiple paths between any pair of variables in the graph, which relaxes latent tree assumptions in prior work; on structural functions, we do not make parametric assumptions, thus permitting gene
    
[^119]: TrojPrompt：基于黑盒方式的预训练语言模型木马攻击

    TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (arXiv:2306.06815v1 [cs.CR] CROSS LISTED)

    [http://arxiv.org/abs/2306.06815](http://arxiv.org/abs/2306.06815)

    本文开创性地研究了基于 prompt 学习的预训练语言模型 API 的特洛伊易感性，并提出了一种自动黑盒框架——TrojPrompt，用于生成通用和隐蔽的触发器，并将特洛伊木马插入硬提示。

    

    Prompt学习被证明在提高预训练语言模型（PLM）适应性方面非常有效，超越了传统的微调范式，并在专为少样本学习场景量身定制的应用程序和API中展现了杰出的前景。但是，尽管prompt学习的API越来越受欢迎，但它们的安全问题仍未得到充分探索。本文在prompt学习的PLM API的特洛伊易感性方面进行了开创性研究。我们发现，离散提示，少样本和黑盒设置是几个关键挑战，限制了现有后门攻击的适用性。为了解决这些挑战，我们提出了TrojPrompt，这是一种自动的黑盒框架，可有效生成通用的和隐秘的触发器，并将特洛伊木马插入硬提示。具体而言，我们提出了一种API驱动的通用触发器发现算法，通过查询受害者PLM API，为各种输入生成通用触发器。

    Prompt learning has been proven to be highly effective in improving pre-trained language model (PLM) adaptability, surpassing conventional fine-tuning paradigms, and showing exceptional promise in an ever-growing landscape of applications and APIs tailored for few-shot learning scenarios. Despite the growing prominence of prompt learning-based APIs, their security concerns remain underexplored. In this paper, we undertake a pioneering study on the Trojan susceptibility of prompt-learning PLM APIs. We identified several key challenges, including discrete-prompt, few-shot, and black-box settings, which limit the applicability of existing backdoor attacks. To address these challenges, we propose TrojPrompt, an automatic and black-box framework to effectively generate universal and stealthy triggers and insert Trojans into hard prompts. Specifically, we propose a universal API-driven trigger discovery algorithm for generating universal triggers for various inputs by querying victim PLM API
    
[^120]: 多体SE（3）等变性用于无监督的刚体分割和运动估计

    Multi-body SE(3) Equivariance for Unsupervised Rigid Segmentation and Motion Estimation. (arXiv:2306.05584v1 [cs.CV])

    [http://arxiv.org/abs/2306.05584](http://arxiv.org/abs/2306.05584)

    本文提出了一种基于SE（3）等变结构和非监督训练策略的方法，可以实现刚性分割和运动估计，不需要类别信息且具有极高的模型效率。

    

    实现刚体分割和运动估计的真正通用方法对于理解关节物体和移动场景的三维影像至关重要。鉴于分割和运动估计之间密切的关系，我们提出了一种SE（3）等变体系结构和培训策略，以无监督的方式解决这个任务。我们的体系结构包括两个轻量级和相互连接的头部，使用点级不变特征和来自SE（3）等变特征的运动估计来预测分割掩模，而不需要类别信息。我们的统一培训策略可以在线执行，通过利用场景流，分割掩模和刚性变换之间的相互关系来同时优化两个预测。我们在四个数据集上的实验表明，我们的方法在模型性能和计算效率方面均表现出优越性，只有0.25M参数和0.92G FLOPs。

    A truly generalizable approach to rigid segmentation and motion estimation is fundamental to 3D understanding of articulated objects and moving scenes. In view of the tightly coupled relationship between segmentation and motion estimates, we present an SE(3) equivariant architecture and a training strategy to tackle this task in an unsupervised manner. Our architecture comprises two lightweight and inter-connected heads that predict segmentation masks using point-level invariant features and motion estimates from SE(3) equivariant features without the prerequisites of category information. Our unified training strategy can be performed online while jointly optimizing the two predictions by exploiting the interrelations among scene flow, segmentation mask, and rigid transformations. We show experiments on four datasets as evidence of the superiority of our method both in terms of model performance and computational efficiency with only 0.25M parameters and 0.92G FLOPs. To the best of ou
    
[^121]: FACTIFY3M: 通过5W问答解释的多模式事实验证基准

    FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering. (arXiv:2306.05523v1 [cs.CL])

    [http://arxiv.org/abs/2306.05523](http://arxiv.org/abs/2306.05523)

    FACTIFY3M是一个以多模式虚假信息验证为目标的数据集。虚假信息如今已成为当下重大的社会问题，这一数据集旨在通过多模式验证来及时识别和缓解虚假信息。

    

    打击虚假信息是当前亟待解决的社会危机之一——大约67%的美国人认为虚假信息会产生大量的不确定性，其中有10%的人有意识地传播虚假信息。证据表明，虚假信息可以操纵民主进程和公众舆论，并在危机期间引起股市动荡、社会恐慌甚至死亡。因此，应及时识别并尽可能缓解虚假信息。然而，由于社交媒体平台每天分享大约32亿张图像和720,000 小时的视频，因此对于多模式虚假信息的可扩展性检测需要高效的事实验证。尽管在文本模式下自动事实验证取得了进展(例如，FEVER, LIAR)，但学术界在多模式事实验证方面缺乏实质性的努力。为了填补这一空白，我们引入了FACTIFY3M数据集，该数据集包含300万个样本，通过多种模式和5W问答提高了事实验证领域的极限。

    Combating disinformation is one of the burning societal crises -- about 67% of the American population believes that disinformation produces a lot of uncertainty, and 10% of them knowingly propagate disinformation. Evidence shows that disinformation can manipulate democratic processes and public opinion, causing disruption in the share market, panic and anxiety in society, and even death during crises. Therefore, disinformation should be identified promptly and, if possible, mitigated. With approximately 3.2 billion images and 720,000 hours of video shared online daily on social media platforms, scalable detection of multimodal disinformation requires efficient fact verification. Despite progress in automatic text-based fact verification (e.g., FEVER, LIAR), the research community lacks substantial effort in multimodal fact verification. To address this gap, we introduce FACTIFY 3M, a dataset of 3 million samples that pushes the boundaries of the domain of fact verification via a multi
    
[^122]: 语言模型和神经响应测量之间的结构相似性

    Structural Similarities Between Language Models and Neural Response Measurements. (arXiv:2306.01930v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.01930](http://arxiv.org/abs/2306.01930)

    本研究研究了语言模型和神经响应测量之间的结构相似性，发现神经语言模型越大，其表示越相似于脑成像的神经响应测量。

    

    大型语言模型具有复杂的内部动态，但可以研究其词汇和短语的表示的几何结构。人类语言处理也很难理解，但神经响应测量可以提供在听或读时激活的（嘈杂的）记录，我们可以从中提取相似的词汇和短语表示。本研究在脑解码的背景下研究了这些表示所引发的几何结构之间的相似性。我们发现，神经语言模型越大，其表示与脑成像的神经响应测量越相似。

    Large language models (LLMs) have complicated internal dynamics, but induce representations of words and phrases whose geometry we can study. Human language processing is also opaque, but neural response measurements can provide (noisy) recordings of activation during listening or reading, from which we can extract similar representations of words and phrases. Here we study the extent to which the geometries induced by these representations, share similarities in the context of brain decoding. We find that the larger neural language models get, the more their representations are structurally similar to neural response measurements from brain imaging. Code is available at \url{https://github.com/coastalcph/brainlm}.
    
[^123]: 信仰与命运：Transformer在组合性方面的局限性。

    Faith and Fate: Limits of Transformers on Compositionality. (arXiv:2305.18654v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18654](http://arxiv.org/abs/2305.18654)

    研究了Transformer模型在三个代表性组合型任务中的表现，发现其通过线性子图匹配解决多步组合推理问题。

    

    Transformer大型语言模型在需要复杂多步推理的任务上表现卓越，但同时在一些简单问题上也会出现失败。这引发了疑问：这些错误是偶然的，还是它们表明了更实质性的限制？为了揭示Transformer的神秘面纱，我们研究了这些模型在三个代表性的组合型任务中的极限 - 多位数乘法、逻辑网格谜题和一个经典的动态规划问题。 这些任务需要将问题分解为子步骤，并将这些步骤综合成精确的答案。我们将组合型任务转化为计算图，以系统地量化其复杂性，并将推理步骤分解为中间子程序。我们的实证结果表明，Transformer通过将多步组合推理转化为线性子图匹配来解决组合型任务。

    Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, wi
    
[^124]: 理解情绪价值是一项联合深度学习任务

    Understanding Emotion Valence is a Joint Deep Learning Task. (arXiv:2305.17422v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17422](http://arxiv.org/abs/2305.17422)

    研究通过多任务学习方法，探索情绪价值与情绪载体之间的相互依赖关系，并在联合预测设置中使用判别性模型取得了最佳平衡。

    

    对说话人的话语或写作中情绪价值分析有助于理解对话中情绪状态的激活和变化。最近，引入了情绪载体（EC）的概念来解释说话人所感受到的情绪及其表现。在这项工作中，我们通过多任务学习方法研究了情绪价值和EC之间的自然相互依赖关系。我们尝试了预训练语言模型（PLM）在情绪价值和EC预测任务中的单任务、两步法和联合设置。我们比较和评估了生成性（GPT-2）和判别性（BERT）架构在每种设置中的性能。结果显示，在一个任务中提供了另一个任务的真实标签可以提高模型在另一个任务中的预测性能。我们进一步观察到，判别性模型在联合预测设置中取得了情绪价值和EC预测任务的最佳平衡。因此，我们获得了一个能在这两个任务中表现出色的单一模型。

    The valence analysis of speakers' utterances or written posts helps to understand the activation and variations of the emotional state throughout the conversation. More recently, the concept of Emotion Carriers (EC) has been introduced to explain the emotion felt by the speaker and its manifestations. In this work, we investigate the natural inter-dependency of valence and ECs via a multi-task learning approach. We experiment with Pre-trained Language Models (PLM) for single-task, two-step, and joint settings for the valence and EC prediction tasks. We compare and evaluate the performance of generative (GPT-2) and discriminative (BERT) architectures in each setting. We observed that providing the ground truth label of one task improves the prediction performance of the models in the other task. We further observed that the discriminative model achieves the best trade-off of valence and EC prediction tasks in the joint prediction setting. As a result, we attain a single model that perfo
    
[^125]: 扩散模型是否是视觉语言推理器？

    Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v1 [cs.CV])

    [http://arxiv.org/abs/2305.16397](http://arxiv.org/abs/2305.16397)

    本文针对扩散-语言图像生成模型进行转换和评估，介绍了生成-鉴别评估基准(GDBench)基于7个视觉语言复杂任务，并发现转换后的模型在组合性任务方面的表现优于CLIP，通过微调可提高其组合性能。

    

    近期，使用去噪扩散过程的文本-图像生成模型已取得了巨大的定性成功。然而，与鉴别式视觉-语言模型不同，将基于扩散的生成模型置于自动细粒度定量评估高级现象（如组合性）的任务中是一项非常棘手的任务。为此，我们开展了两项创新。首先，我们使用一种称为DiffusionITM的新方法将基于扩散的模型（在我们的情况下，是稳定扩散）转换为任何图像文本匹配(ITM)任务。其次，我们引入了7个复杂的视觉语言任务、偏差评估和详细分析的生成-鉴别评估基准(GDBench)。我们发现，Stable Diffusion + DiffusionITM在许多任务上具有竞争力，并在组合性任务（如CLEVR和Winoground等）上优于CLIP。我们通过在MS-COCO上微调保持图像特征的转移设置进一步提高其组合性能。

    Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality. Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM. Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis. We find that Stable Diffusion + DiffusionITM is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground. We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining ge
    
[^126]: 具有函数逼近和理论保证的决策感知演员-评论家算法

    Decision-Aware Actor-Critic with Function Approximation and Theoretical Guarantees. (arXiv:2305.15249v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15249](http://arxiv.org/abs/2305.15249)

    提出了一种具有函数逼近和理论保证的决策感知演员-评论家算法，通过设计联合目标来解决演员和评论家之间的不匹配，并且无论策略和评论家参数化的选择如何，该算法都保证单调策略改进。

    

    演员评论家 (AC) 方法广泛应用于强化学习 (RL) 中，并从使用任何策略梯度方法作为演员和基于值方法作为评论家的灵活性中受益。评论家通常通过最小化 TD 误差来训练，这是一个与实现高奖励的真实目标可能脱钩的客观标准。我们通过设计一个决策感知的联合目标来解决这种不匹配。我们使用提出的目标来设计一个通用的 AC 算法，可以轻松处理任何函数逼近。我们明确表征了在选择策略和评论家参数化的情况下，所得算法保证单调策略改进的条件。实例化通用算法将导致涉及最大化一系列替代函数 (类似于 TRPO、PPO) 的演员和涉及最小化一个密切相关目标的评论家。使用简单的方法加速了证明的过程，同时还引入了新的研究方向。

    Actor-critic (AC) methods are widely used in reinforcement learning (RL) and benefit from the flexibility of using any policy gradient method as the actor and value-based method as the critic. The critic is usually trained by minimizing the TD error, an objective that is potentially decorrelated with the true goal of achieving a high reward with the actor. We address this mismatch by designing a joint objective for training the actor and critic in a decision-aware fashion. We use the proposed objective to design a generic, AC algorithm that can easily handle any function approximation. We explicitly characterize the conditions under which the resulting algorithm guarantees monotonic policy improvement, regardless of the choice of the policy and critic parameterization. Instantiating the generic algorithm results in an actor that involves maximizing a sequence of surrogate functions (similar to TRPO, PPO) and a critic that involves minimizing a closely connected objective. Using simple 
    
[^127]: 基于语法约束的语言模型灵活解码技术

    Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])

    [http://arxiv.org/abs/2305.13971](http://arxiv.org/abs/2305.13971)

    本文提出了一种使用形式语法约束丰富解码步骤的方法，有效生成符合特定语法的复杂输出结构，同时允许任何上下文无关语法集成。实验证明该方法在四个信息提取任务上实现了最先进的性能表现。

    

    LLM在许多任务中展现出了惊人的少量样本表现，但在生成信息提取所需的复杂输出结构时仍存在困难。这个限制源于LLM在没有微调的情况下倾向于生成自由文本而不是遵循特定语法的精确结构。在本文中，我们提出在解码步骤中使用形式语法约束来丰富模型。在搜索过程中，只有符合语法产生规则的有效令牌能被考虑到。这样就强制只产生有效的序列。我们的框架非常通用和灵活，允许任何上下文无关语法(CFG)集成到我们的自定义约束beam搜索实现中。我们展示了许多NLP任务的输出可以被表示为形式语言，使它们适合在我们的框架中直接使用。对于输出空间取决于输入的任务，我们提出了基于输入的CFG，根据特定于输入的特征更新产生规则。实验证明了我们的方法在生成复杂输出结构方面的有效性，并在四个信息提取任务上实现了最先进的性能。

    LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
    
[^128]: 论文标题：《感知测试：多模态视频模型的诊断基准》

    Perception Test: A Diagnostic Benchmark for Multimodal Video Models. (arXiv:2305.13786v1 [cs.CV])

    [http://arxiv.org/abs/2305.13786](http://arxiv.org/abs/2305.13786)

    该论文提出了一个名为“感知测试”的多模态视频基准测试，可以评估预训练模型的感知和推理能力，测试涵盖了记忆、抽象、物理、语义等技能和描述性、解释性、预测性、反事实性等推理类型。

    

    我们提出了一种新颖的多模态视频基准——感知测试，用于评估预训练的多模态模型（例如 Flamingo、BEiT-3 或 GPT-4）的感知和推理技能。与现有的基准侧重于计算任务（例如分类、检测或跟踪）不同，感知测试侧重于视频、音频和文本模态跨越记忆、抽象、物理、语义等技能和推理类型（描述性、解释性、预测性、反事实性），以提供全面而高效的评估工具。该基准测试通过零样本/少样本或有限微调下挑选预训练模型的转移能力。为实现这些目的，感知测试介绍了11.6k种真实世界视频，平均长度为23秒，旨在展示感知上有趣的情境，由全球约100名参与者拍摄。这些视频密集地带有六种标签（多项选择和基于视频问题回答，对象a）

    We propose a novel multimodal video benchmark - the Perception Test - to evaluate the perception and reasoning skills of pre-trained multimodal models (e.g. Flamingo, BEiT-3, or GPT-4). Compared to existing benchmarks that focus on computational tasks (e.g. classification, detection or tracking), the Perception Test focuses on skills (Memory, Abstraction, Physics, Semantics) and types of reasoning (descriptive, explanatory, predictive, counterfactual) across video, audio, and text modalities, to provide a comprehensive and efficient evaluation tool. The benchmark probes pre-trained models for their transfer capabilities, in a zero-shot / few-shot or limited finetuning regime. For these purposes, the Perception Test introduces 11.6k real-world videos, 23s average length, designed to show perceptually interesting situations, filmed by around 100 participants worldwide. The videos are densely annotated with six types of labels (multiple-choice and grounded video question-answers, object a
    
[^129]: 一种可扩展的神经网络用于DSIC仿射极大价拍卖设计

    A Scalable Neural Network for DSIC Affine Maximizer Auction Design. (arXiv:2305.12162v1 [cs.GT])

    [http://arxiv.org/abs/2305.12162](http://arxiv.org/abs/2305.12162)

    该论文提出了一种可扩展的神经网络AMenuNet来构造AMAs参数和生成候选分配，解决了现有方法在占优策略激励兼容性和可扩展性方面的限制，其在协商一致的价值和社会残余价值方面优于强基线模型。

    

    自动拍卖设计旨在通过机器学习寻找经验上高收入的机制。现有的多物品拍卖情景的工作可以粗略地分为RegretNet类和仿射极大价（AMAs）方法。然而，前者不能严格保证占优策略激励兼容性（DSIC），而后者因为分配候选人数过多而面临可扩展性问题。为解决这些限制，我们提出了AMenuNet，一种可扩展的神经网络，它从出价人和物品表示中构造AMA参数（甚至包括分配菜单）。由于AMA的属性，AMenuNet始终是DSIC和个人理性（IR）的，通过神经网络生成候选分配来增强可伸缩性。此外，AMenuNet是置换等变的，其参数数量不受拍卖规模的影响。我们进行了大量实验，证明AMenuNet在协商一致的价值和社会残余价值方面优于强基线模型。

    Automated auction design aims to find empirically high-revenue mechanisms through machine learning. Existing works on multi item auction scenarios can be roughly divided into RegretNet-like and affine maximizer auctions (AMAs) approaches. However, the former cannot strictly ensure dominant strategy incentive compatibility (DSIC), while the latter faces scalability issue due to the large number of allocation candidates. To address these limitations, we propose AMenuNet, a scalable neural network that constructs the AMA parameters (even including the allocation menu) from bidder and item representations. AMenuNet is always DSIC and individually rational (IR) due to the properties of AMAs, and it enhances scalability by generating candidate allocations through a neural network. Additionally, AMenuNet is permutation equivariant, and its number of parameters is independent of auction scale. We conduct extensive experiments to demonstrate that AMenuNet outperforms strong baselines in both co
    
[^130]: 神经网络何时在表格数据上胜过增强树？

    When Do Neural Nets Outperform Boosted Trees on Tabular Data?. (arXiv:2305.02997v1 [cs.LG])

    [http://arxiv.org/abs/2305.02997](http://arxiv.org/abs/2305.02997)

    这项研究通过对176个数据集的比较分析发现，在许多数据集中，GBDT和NN之间的性能差异可以忽略不计，或者GBDT的轻微超参数调整比选择最佳算法更重要。此外，研究人员对965个元特征进行了分析，发现GBDT在高维稀疏数据上表现更好。

    

    表格数据是机器学习中最常用的数据类型之一。尽管神经网络（NN）在表格数据上取得了最近的进展，但人们仍在积极讨论NN是否通常优于梯度提升决策树（GBDT）在表格数据上的表现，一些最近的工作要么认为GBDT在表格数据上一贯优于NN，要么认为NN优于GBDT。在这项工作中，我们退一步问：'这重要吗？'我们通过对176个数据集比较19种算法，进行了迄今为止最大的表格数据分析，并发现'NN vs. GBDT'争论被过分强调：令人惊讶的是，在相当多的数据集中，GBDT和NN之间的性能差异要么可以忽略不计，要么GBDT的轻微超参数调整比选择最佳算法更重要。接下来，我们分析了965个元特征，以确定数据集的哪些特性使NN或GBDT更适合表现良好。例如，我们发现GBDT要比NN在高维稀疏数据上表现更好。

    Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and ask, 'does it matter?' We conduct the largest tabular data analysis to date, by comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than selecting the best algorithm. Next, we analyze 965 metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better th
    
[^131]: 学习使用自注记进行推理和记忆

    Learning to Reason and Memorize with Self-Notes. (arXiv:2305.00833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.00833](http://arxiv.org/abs/2305.00833)

    该论文提出了一种学习使用自注记进行推理和记忆的方法，通过允许模型明确思考、记录自己的想法，并整合先前的推理步骤，从而提高了多步推理的能力。

    

    大型语言模型在多步推理方面表现不佳，且不能保留以供将来使用的先前推理步骤。我们提出了一种解决这两个问题的简单方法，即允许模型进行自注记。与最近的思维链或草稿本方法不同，该模型可以随时偏离输入上下文来明确思考和记录自己的想法。这使得模型可以在阅读上下文时即时推理，并整合先前的推理步骤，从而增强其记忆并进行多步推理。广泛的实验表明，通过使用交织输入文本的自注记方法，我们的方法可以胜过思维链和草稿本方法。

    Large language models have been shown to struggle with multi-step reasoning, and do not retain previous reasoning steps for future use. We propose a simple method for solving both of these problems by allowing the model to take Self-Notes. Unlike recent chain-of-thought or scratchpad approaches, the model can deviate from the input context at any time to explicitly think and write down its thoughts. This allows the model to perform reasoning on the fly as it reads the context and even integrate previous reasoning steps, thus enhancing its memory with useful information and enabling multi-step reasoning. Experiments across a wide variety of tasks demonstrate that our method can outperform chain-of-thought and scratchpad methods by taking Self-Notes that interleave the input text.
    
[^132]: 用于教育的通用人工智能（AGI）

    Artificial General Intelligence (AGI) for Education. (arXiv:2304.12479v1 [cs.AI])

    [http://arxiv.org/abs/2304.12479](http://arxiv.org/abs/2304.12479)

    AGI技术具有革命教育领域潜力，可以建立e-learning平台、教育协作工具等，弥补传统AI模型因受限于数据和人际交互限制而无法满足教育需求的不足。

    

    由于最新的大型语言模型和聊天机器人（如GPT-4和ChatGPT）的出现，通用人工智能（AGI）作为未来技术已经得到全球认可。AGI旨在通过计算机系统复制人类智能，是具有革命教育领域潜力的关键技术之一。与传统的人工智能模型相比，这些模型通常只针对有限范围的任务进行设计，需要大量特定领域的数据进行训练，可能无法考虑教育中复杂的人际动态。受最近的大规模预训练模型驱动，AGI代表了机器在执行需要人类水平智能的任务方面的重大飞跃，例如推理、解决问题、做出决策，甚至理解人类情感和社交互动。本研究回顾了AGI的关键概念、能力、范围和在未来教育中的潜力，包括建立e-learning平台和教育协作工具等。

    Artificial general intelligence (AGI) has gained global recognition as a future technology due to the emergence of breakthrough large language models and chatbots such as GPT-4 and ChatGPT, respectively. AGI aims to replicate human intelligence through computer systems, which is one of the critical technologies having the potential to revolutionize the field of education. Compared to conventional AI models, typically designed for a limited range of tasks, demand significant amounts of domain-specific data for training and may not always consider intricate interpersonal dynamics in education. AGI, driven by the recent large pre-trained models, represents a significant leap in the capability of machines to perform tasks that require human-level intelligence, such as reasoning, problem-solving, decision-making, and even understanding human emotions and social interactions. This work reviews AGI's key concepts, capabilities, scope, and potential within future education, including setting e
    
[^133]: OpenAssistant Conversations -- 民主化大型语言模型的对齐方法

    OpenAssistant Conversations -- Democratizing Large Language Model Alignment. (arXiv:2304.07327v1 [cs.CL])

    [http://arxiv.org/abs/2304.07327](http://arxiv.org/abs/2304.07327)

    释放了OpenAssistant Conversations，这是一个由全球超过1,000名参与者进行人工生成和人工注释的助手风格对话语料库，可以通过SFT和RLHF有效地用于LLM对齐，提高模型性能和可用性。

    

    对齐大型语言模型（LLM）与人类偏好的技术已被证明可以显著提高可用性并推动其快速应用，如ChatGPT所示。 监督微调（SFT）和根据人类反馈进行的强化学习（RLHF）等对齐技术大大降低了有效发挥LLM能力所需的技能和领域知识，提高了它们在各个领域的可访问性和实用性。 然而，像RLHF这样的最先进的对齐技术依赖于高质量的人类反馈数据，这些数据往往昂贵且保密。 为了民主化大规模对齐的研究，我们发布了OpenAssistant Conversations，这是一个由全球超过1,000名参与者进行人工生成和人工注释的助手风格对话语料库，包含161,443条消息，分布在66,497个对话树中，并在35种不同的语言中用461,292个质量评分进行注释。我们的实验表明，OpenAssistant Conversations可以通过SFT和RLHF有效地用于LLM对齐，从而提高模型性能和可用性。我们发布语料库，使更广泛的研究社区能够进一步研究民主化LLM的能力，从而改善人类交互。

    Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages, annotated with 461,292 quality ratings. The corpus is a product of a worldwide crowd-sourcing effort involving over 1
    
[^134]: RoboPianist：用于高维机器人控制的基准测试

    RoboPianist: A Benchmark for High-Dimensional Robot Control. (arXiv:2304.04150v1 [cs.RO])

    [http://arxiv.org/abs/2304.04150](http://arxiv.org/abs/2304.04150)

    RoboPianist是一个新的高维机器人控制基准测试，旨在测试高精度、协调和规划，并通过反复接触的欠驱动系统进行钢琴演奏。该基准测试提供了性能特征的定量数据，并具有易于解释的结果。

    

    我们介绍了一个新的基准测试套件，针对测试高空间和时间精度、协调和规划，所有这些都是在频繁进行接触的欠驱动系统中进行的。所提出的挑战是通过双手灵巧，使用一对仿人机器人手来掌握钢琴演奏。我们称之为RoboPianist，最初版本涵盖了150首难度不同的歌曲。我们在此基准测试上研究了基于模型的和无模型的方法，表征了它们的性能特征。我们观察到，尽管某些现有方法在某些方面表现出色，但在某些方面还有很大的改进空间。RoboPianist提供了一个丰富的定量基准测试环境，具有易于解释的结果、通过简单增加新歌曲来扩展曲目的高易用性，并提供了进一步研究的机会，包括多任务学习和零样本学习等领域。

    We introduce a new benchmarking suite for high-dimensional control, targeted at testing high spatial and temporal precision, coordination, and planning, all with an underactuated system frequently making-and-breaking contacts. The proposed challenge is mastering the piano through bi-manual dexterity, using a pair of simulated anthropomorphic robot hands. We call it RoboPianist, and the initial version covers a broad set of 150 variable-difficulty songs. We investigate both model-free and model-based methods on the benchmark, characterizing their performance envelopes. We observe that while certain existing methods, when well-tuned, can achieve impressive levels of performance in certain aspects, there is significant room for improvement. RoboPianist provides a rich quantitative benchmarking environment, with human-interpretable results, high ease of expansion by simply augmenting the repertoire with new songs, and opportunities for further research, including in multi-task learning, ze
    
[^135]: 关于多语言神经机器翻译的Pareto前沿研究

    On the Pareto Front of Multilingual Neural Machine Translation. (arXiv:2304.03216v1 [cs.CL])

    [http://arxiv.org/abs/2304.03216](http://arxiv.org/abs/2304.03216)

    本研究针对多语言神经机器翻译的数据不平衡问题，提出双重幂律方法用于预测独特的性能权衡前沿，并建立基于该方法的样本比例选择优化问题，取得更好的结果。

    

    本研究探讨了在多语言神经机器翻译中，给定方向的泛化性能如何随其采样比例的变化而变化。通过训练200多个具有不同模型大小、方向和总任务数量的多语言模型，我们发现在训练语料库存在数据不平衡时，标量化导致了一个多任务权衡前沿，该前沿偏离了传统的Pareto前沿。基于我们的观察，我们提出了双重幂律来预测MNMT中独特的性能权衡前沿，该方法在各种语言、数据充足性和任务数量方面都很鲁棒。最后，我们将MNMT中的样本比例选择问题建模为基于双重幂律的优化问题，取得了更好的结果。

    In this work, we study how the generalization performance of a given direction changes with its sampling ratio in Multilingual Neural Machine Translation (MNMT). By training over 200 multilingual models with various model sizes, directions, and total numbers of tasks, we find that scalarization leads to a multitask trade-off front that deviates from the traditional Pareto front when there exists data imbalance in the training corpus. That is, the performance of certain translation directions does not improve with the increase of its weight in the multi-task optimization objective, which poses greater challenge to improve the overall performance of all directions. Based on our observations, we propose the Double Power Law to predict the unique performance trade-off front in MNMT, which is robust across various languages, data adequacy and number of tasks. Finally, we formulate sample ratio selection in MNMT as an optimization problem based on the Double Power Law, which achieves better 
    
[^136]: 有效地对齐跨语言会话任务的提示调整跨语言转移学习

    Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v1 [cs.CL])

    [http://arxiv.org/abs/2304.01295](http://arxiv.org/abs/2304.01295)

    本文提出了一个平行大规模多语种会话数据集XSGD，开发了一种有效的基于提示调整的方法来学习对齐提示，同时研究了跨语言任务的NLI-based和vanilla分类器，并在插槽填充和意图分类任务上评估了模型的跨语言泛化能力。

    

    针对自然语言处理任务，跨语言转移的语言模型已被广泛研究，但是对于会话任务的研究相对较少。本文提出了XSGD，这是一个由Schema-Guided Dialogue（SGD）翻译成105种其他语言的平行大规模多语种会话数据集。为了实现对齐的跨语言表示方法，我们开发了一种有效的基于提示调整的方法来学习对齐提示。我们还研究了两种不同的分类器：NLI-based和vanilla分类器，并测试了对齐提示所实现的跨语言能力。我们在两个对话任务（插槽填充和意图分类）上评估了我们模型的跨语言泛化能力。

    Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce XSGD, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains approximately 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model's cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent cla
    
[^137]: 时间序列视为图像：用视觉transformer处理不规则采样时间序列

    Time Series as Images: Vision Transformer for Irregularly Sampled Time Series. (arXiv:2303.12799v1 [cs.LG])

    [http://arxiv.org/abs/2303.12799](http://arxiv.org/abs/2303.12799)

    本文提出了一种新颖的方法，将不规则采样的时间序列转换为线图像，并适应强大的视觉transformer进行时间序列分类。该方法简化了算法设计，具有通用性，并展示了在多个医疗和人体活动数据集上明显优于最先进的专业算法的表现。

    

    在各个领域中，尤其是在医疗应用中，不规则抽样的时间序列越来越普遍。尽管已经提出了不同的高度定制化方法来解决不规则性问题，但如何有效地模拟它们的复杂动态和高稀疏性仍然是一个开放的问题。本文从全新的角度研究了这个问题：将不规则采样的时间序列转换为线图像，并调整强大的视觉transformer以执行与图像分类相同的时间序列分类。我们的方法在不假设先前知识的情况下大大简化了算法设计，并且可以被潜在地扩展为一个通用框架。尽管其简单性，我们展示了它在几个流行的医疗保健和人体活动数据集上明显优于最先进的专业算法。特别是在具有挑战性的离传感器设置中，即在测试期间屏蔽变量的子集中，性能比最佳基准提高了高达11％。

    Irregularly sampled time series are becoming increasingly prevalent in various domains, especially in medical applications. Although different highly-customized methods have been proposed to tackle irregularity, how to effectively model their complicated dynamics and high sparsity is still an open problem. This paper studies the problem from a whole new perspective: transforming irregularly sampled time series into line graph images and adapting powerful vision transformers to perform time series classification in the same way as image classification. Our approach largely simplifies algorithm designs without assuming prior knowledge and can be potentially extended as a general-purpose framework. Despite its simplicity, we show that it substantially outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Especially in the challenging leave-sensors-out setting where a subset of variables is masked during testing, the performance impr
    
[^138]: 分布模型和半监督学习器在少量标签上互相受益

    Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels. (arXiv:2302.10586v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10586](http://arxiv.org/abs/2302.10586)

    本文介绍了一种名为双伪训练（DPT）的训练策略，该策略结合了强大的半监督学习器和扩散模型来进一步推进半监督生成和分类任务。实验结果表明，DPT在各种情况下都能实现半监督生成和分类任务的SOTA性能，特别是在每个类别只有一个或两个标签的情况下，超过了其他一些模型。

    

    为了进一步推进半监督生成和分类任务，本文提出了一种简单而有效的训练策略——双伪训练（DPT），该策略建立在强大的半监督学习器和扩散模型之上。DPT分为三个阶段：使用部分标记数据训练分类器以预测伪标签；使用这些伪标签训练条件生成模型以生成伪图像；并使用真实和伪造的图像混合重新训练分类器。实验结果表明，在各种情况下，DPT始终实现了半监督生成和分类的SOTA性能。特别是，在每个类别只有一个或两个标签的情况下，在ImageNet 256x256上，DPT的Fr\'echet Inception Distance（FID）得分分别为3.08或2.52，超过了具有完整标签的强扩散模型（如IDDPM，CDM，ADM和LDM）。此外，DPT在ImageNet分类任务上显著优于竞争性的半监督基线，实现了顶级1的准确性。

    In an effort to further advance semi-supervised generative and classification tasks, we propose a simple yet effective training strategy called dual pseudo training (DPT), built upon strong semi-supervised learners and diffusion models. DPT operates in three stages: training a classifier on partially labeled data to predict pseudo-labels; training a conditional generative model using these pseudo-labels to generate pseudo images; and retraining the classifier with a mix of real and pseudo images. Empirically, DPT consistently achieves SOTA performance of semi-supervised generation and classification across various settings. In particular, with one or two labels per class, DPT achieves a Fr\'echet Inception Distance (FID) score of 3.08 or 2.52 on ImageNet 256x256, surpassing strong diffusion models with full labels, such as IDDPM, CDM, ADM, and LDM. Besides, DPT outperforms competitive semi-supervised baselines substantially on ImageNet classification tasks, achieving top-1 accuracies o
    
[^139]: 大规模多模态预训练模型：综合调查

    Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey. (arXiv:2302.10035v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10035](http://arxiv.org/abs/2302.10035)

    本文综合调查了大规模多模态预训练模型，介绍了背景、任务定义、关键挑战和优势，并讨论了数据、目标、网络架构和知识增强预训练等方面的相关内容。

    

    随着对通用深度模型的迫切需求，许多预训练模型被提出，例如BERT，ViT，GPT等。受到这些模型在单一领域（如计算机视觉和自然语言处理）中的成功启发，多模态预训练大模型近年来也越来越受到关注。在这项工作中，我们对这些模型进行了全面调查，并希望本论文能提供新的见解，并帮助新研究人员追踪最前沿的工作。具体而言，我们首先通过回顾传统的深度学习、自然语言处理、计算机视觉和语音的预训练研究工作，介绍了多模态预训练的背景。然后，我们介绍了多模态预训练模型（MM-PTMs）的任务定义、关键挑战和优势，并重点讨论了数据、目标、网络架构和知识增强预训练方面的MM-PTMs。之后，我们介绍了用于后续任务的数据集以及评估指标。最后，我们提供了一个综合的比较和总结，并讨论了未来发展方向。

    With the urgent demand for generalized deep models, many pre-trained big models are proposed, such as BERT, ViT, GPT, etc. Inspired by the success of these models in single domains (like computer vision and natural language processing), the multi-modal pre-trained big models have also drawn more and more attention in recent years. In this work, we give a comprehensive survey of these models and hope this paper could provide new insights and helps fresh researchers to track the most cutting-edge works. Specifically, we firstly introduce the background of multi-modal pre-training by reviewing the conventional deep learning, pre-training works in natural language process, computer vision, and speech. Then, we introduce the task definition, key challenges, and advantages of multi-modal pre-training models (MM-PTMs), and discuss the MM-PTMs with a focus on data, objectives, network architectures, and knowledge enhanced pre-training. After that, we introduce the downstream tasks used for the
    
[^140]: 机器学习在整数规划中的切割平面：一项调查

    Machine Learning for Cutting Planes in Integer Programming: A Survey. (arXiv:2302.09166v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.09166](http://arxiv.org/abs/2302.09166)

    该论文调查了机器学习技术在混合整数线性规划中选择切割平面的最新研究，提出了使用数据进行优化切割选择的有前景的方法。

    

    我们对机器学习技术在混合整数线性规划中选择切割平面（或切割）的最新研究进行了调查。尽管存在各种各样的切割类别，但在分支定界树的给定节点的线性规划放松中选择一组要添加的切割平面的任务迄今为止无法得到正式和启发式解法。机器学习通过使用数据来识别加速解决混合整数线性规划实例的有前景的切割，为改进切割选择过程提供了一种有希望的方法。本文概述了该主题，重点介绍了文献中的最新进展，数据收集、评估和机器学习模型架构的常见方法。我们分析了文献中的实证结果，试图量化已取得的进展，并在最后提出了未来研究的方向。

    We survey recent work on machine learning (ML) techniques for selecting cutting planes (or cuts) in mixed-integer linear programming (MILP). Despite the availability of various classes of cuts, the task of choosing a set of cuts to add to the linear programming (LP) relaxation at a given node of the branch-and-bound (B&B) tree has defied both formal and heuristic solutions to date. ML offers a promising approach for improving the cut selection process by using data to identify promising cuts that accelerate the solution of MILP instances. This paper presents an overview of the topic, highlighting recent advances in the literature, common approaches to data collection, evaluation, and ML model architectures. We analyze the empirical results in the literature in an attempt to quantify the progress that has been made and conclude by suggesting avenues for future research.
    
[^141]: 自监督的时间图学习与时间和结构强度对齐

    Self-Supervised Temporal Graph learning with Temporal and Structural Intensity Alignment. (arXiv:2302.07491v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07491](http://arxiv.org/abs/2302.07491)

    该论文提出了一种自监督的时间图学习方法，通过提取时间和结构信息来学习更具信息量的节点表示。

    

    时间图学习旨在生成用于基于图的任务的高质量表示，同时包含动态信息，最近引起了越来越多的关注。与静态图不同，时间图通常以连续时间上的节点交互序列的形式组织，而不是邻接矩阵。大多数时间图学习方法通过在时间上组合历史信息来建模当前的交互。然而，这些方法仅考虑了一阶时间信息，而忽视了重要的高阶结构信息，导致性能不佳。为了解决这个问题，我们提出了一种自监督方法，名为S2T，通过提取时间和结构信息来学习更具信息量的节点表示。

    Temporal graph learning aims to generate high-quality representations for graph-based tasks along with dynamic information, which has recently drawn increasing attention. Unlike the static graph, a temporal graph is usually organized in the form of node interaction sequences over continuous time instead of an adjacency matrix. Most temporal graph learning methods model current interactions by combining historical information over time. However, such methods merely consider the first-order temporal information while ignoring the important high-order structural information, leading to sub-optimal performance. To solve this issue, by extracting both temporal and structural information to learn more informative node representations, we propose a self-supervised method termed S2T for temporal graph learning. Note that the first-order temporal information and the high-order structural information are combined in different ways by the initial node representations to calculate two conditional 
    
[^142]: 几何深度学习仅依靠距离矩阵足够吗？

    Is Distance Matrix Enough for Geometric Deep Learning?. (arXiv:2302.05743v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05743](http://arxiv.org/abs/2302.05743)

    本文证明了消息传递神经网络（MPNNs）不能学习几何信息，提出了$k$-DisGNNs可以利用距离矩阵中的信息，并建立了几何深度学习和传统图表示学习之间的联系。

    

    图神经网络（GNN）常用于涉及图形几何的任务，例如分子动力学模拟。虽然几何图的距离矩阵包含完整的几何信息，但已经证明消息传递神经网络（MPNNs）无法学习这种几何信息。本文通过构造新颖的对称几何图的家族，扩展了MPNN无法区分其距离矩阵的反例家族，并提出$k$-DisGNNs，可以有效地利用距离矩阵中丰富的几何结构。我们证明了模型的高表达能力，并证明了一些现有的精心设计的几何模型可以作为$k$-DisGNNs的特殊情况统一起来。最重要的是，我们建立了几何深度学习和传统图表示学习之间的联系，展示了那些最初为低度表达能力的GNN模型设计的高度表达力的GNN模型。

    Graph Neural Networks (GNNs) are often used for tasks involving the geometry of a given graph, such as molecular dynamics simulation. Although the distance matrix of a geometric graph contains complete geometric information, it has been demonstrated that Message Passing Neural Networks (MPNNs) are insufficient for learning this geometry. In this work, we expand on the families of counterexamples that MPNNs are unable to distinguish from their distance matrices, by constructing families of novel and symmetric geometric graphs. We then propose $k$-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of our models and prove that some existing well-designed geometric models can be unified by $k$-DisGNNs as special cases. Most importantly, we establish a connection between geometric deep learning and traditional graph representation learning, showing that those highly expressive GNN models originally designed for
    
[^143]: 使用ChatGPT-3作为学生论文写作辅助的效果比较

    Better by you, better than me, chatgpt3 as writing assistance in students essays. (arXiv:2302.04536v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.04536](http://arxiv.org/abs/2302.04536)

    本研究比较了使用ChatGPT-3作为写作辅助工具和不使用对学生论文写作表现的影响，结果显示两组学生的平均分数相似，但实验组的文本不真实性稍高，整体样本中论文相似性较低。

    

    目的：比较使用ChatGPT-3作为写作辅助工具和不使用的学生论文写作表现。材料和方法：研究中有18名学生参与（9名对照组和9名使用ChatGPT-3的实验组）。我们用成绩（A-D）和对应的数值（4-1）对论文要素进行评分。我们比较了论文分数与学生的GPT分数、写作时间、真实性和内容相似性。结果：两组的平均分都是C，对照组为2.39（标准差=0.71），实验组为2.00（标准差=0.73）。没有任何预测因素对论文分数产生影响：组别（P=0.184），写作持续时间（P=0.669），模块（P=0.388）和GPA（P=0.532）。实验组中的文本不真实性稍高一些（11.87%，标准差=13.45 对比 9.96%，标准差=9.81%），但整体样本中论文之间的相似性普遍较低（Jaccard相似度指数在0到0.054之间）。在实验组中，AI分类器识别出更多潜在的AI生成文本。

    Aim: To compare students' essay writing performance with or without employing ChatGPT-3 as a writing assistant tool. Materials and methods: Eighteen students participated in the study (nine in control and nine in the experimental group that used ChatGPT-3). We scored essay elements with grades (A-D) and corresponding numerical values (4-1). We compared essay scores to students' GPTs, writing time, authenticity, and content similarity. Results: Average grade was C for both groups; for control (2.39, SD=0.71) and for experimental (2.00, SD=0.73). None of the predictors affected essay scores: group (P=0.184), writing duration (P=0.669), module (P=0.388), and GPA (P=0.532). The text unauthenticity was slightly higher in the experimental group (11.87%, SD=13.45 to 9.96%, SD=9.81%), but the similarity among essays was generally low in the overall sample (the Jaccard similarity index ranging from 0 to 0.054). In the experimental group, AI classifier recognized more potential AI-generated text
    
[^144]: 学习用于排名的列表级别领域不变表示

    Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2212.10764](http://arxiv.org/abs/2212.10764)

    本文提出了一种针对排名问题的列表级别对齐的学习方法，该方法利用列表的结构特性，在领域适应中实现从源领域到目标领域的知识转移。

    

    领域适应旨在将在（数据丰富）源领域学到的知识转移到（资源有限）目标领域，一种常用的方法是不变表示学习，它匹配并对齐特征空间上的数据分布。尽管这种方法在分类和回归问题上得到了广泛研究和应用，但在排名问题上的应用却是零散的，并且现有的几种实现缺乏理论上的证明。本文重新审视了用于排名的不变表示学习。在审查之前的工作时，我们发现他们实施了我们称之为项目级别对齐的方法，该方法在聚合的所有列表中对进行排名的项目分布进行对齐，但忽略了列表的结构。然而，列表的结构应该被利用，因为它是排名问题的固有特性，其中数据和度量是在列表上定义和计算的，而不是在项目本身上。为了解决这一不一致，我们提出了列表级别对齐的学习

    Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to (low-resource) target domains, and a popular method is invariant representation learning, which matches and aligns the data distributions on the feature space. Although this method is studied extensively and applied on classification and regression problems, its adoption on ranking problems is sporadic, and the few existing implementations lack theoretical justifications. This paper revisits invariant representation learning for ranking. Upon reviewing prior work, we found that they implement what we call item-level alignment, which aligns the distributions of the items being ranked from all lists in aggregate but ignores their list structure. However, the list structure should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are defined and computed on lists, not the items by themselves. To close this discrepancy, we propose list-level alignment -learning
    
[^145]: 无视觉基线的多模式语法归纳

    A Vision-free Baseline for Multimodal Grammar Induction. (arXiv:2212.10564v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10564](http://arxiv.org/abs/2212.10564)

    本论文研究了在多模式设置下，只使用文本进行训练的大型语言模型（LLMs）是否能够提供强大的辅助来进行语法归纳。结果显示，基于LLM的纯文本方法在多种多模式数据集上优于先前的方法，并且在性能、参数数量和训练速度方面取得了最先进的结果。

    

    过去的研究表明，配对的视觉与语言信号能够显著改善多模式数据集（如MSCOCO）中的语法归纳。我们研究了只使用文本进行训练的大型语言模型（LLMs）在多模式设置下是否能够提供强大的辅助来进行语法归纳。我们发现，我们的纯文本方法，即基于LLM的C-PCFG（LC-PCFG），在各种多模式数据集上优于先前的多模式方法，并且获得了最先进的语法归纳性能。与带图像的语法归纳相比，LC-PCFG在语料库F1得分上超过了先前的最先进方法7.9个点，参数数量减少了85％，训练速度加快了1.7倍。在三个辅助视频的语法归纳基准中，LC-PCFG在语料库F1上优于先前的最先进方法最多7.7个点，训练速度加快了8.8倍。

    Past work has shown that paired vision-language signals substantially improve grammar induction in multimodal datasets such as MSCOCO. We investigate whether advancements in large language models (LLMs) that are only trained with text could provide strong assistance for grammar induction in multimodal settings. We find that our text-only approach, an LLM-based C-PCFG (LC-PCFG), outperforms previous multi-modal methods, and achieves state-of-the-art grammar induction performance for various multimodal datasets. Compared to image-aided grammar induction, LC-PCFG outperforms the prior state-of-the-art by 7.9 Corpus-F1 points, with an 85% reduction in parameter count and 1.7x faster training speed. Across three video-assisted grammar induction benchmarks, LC-PCFG outperforms prior state-of-the-art by up to 7.7 Corpus-F1, with 8.8x faster training. These results shed light on the notion that text-only language models might include visually grounded cues that aid in grammar induction in mult
    
[^146]: Spuriosity Rankings: 使用排序数据来测量和减少偏见的方法

    Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases. (arXiv:2212.02648v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02648](http://arxiv.org/abs/2212.02648)

    这个论文提出了一种使用排序数据来测量和减少模型对虚假线索的偏见的简单有效方法。通过排名图像的虚假性，可以识别出少数子群体，并通过准确率差来评估模型的偏见。此外，通过在虚假性较低的图像上微调模型，可以在几乎不损失准确率的情况下消除模型的偏见，实现对样本的公正处理。

    

    我们提出了一种简单但有效的方法，通过排序数据来测量和减少模型对虚假线索的依赖所引起的偏见。我们的方法不需要对数据或模型训练进行昂贵的改变，而是更好地利用已有的数据。具体而言，我们基于通过可解释网络的深度神经特征来对图像进行类内排序，以衡量其虚假性（即常见虚假线索的存在程度）。通过虚假性排名，可以很容易地识别出少数子群体（即虚假性较低的图像），并通过准确率差来评估模型的偏见。甚至可以通过在虚假性较低的图像上微调分类头部，以极少的准确率损失来有效消除模型的偏见，从而实现对样本的更公正处理，无论虚假性如何。我们在ImageNet上展示了我们的方法，注释了5000个类特征依赖关系（其中630个是虚假的），并生成了一个包含325k个软分割数据的数据集。

    We present a simple but effective method to measure and mitigate model biases caused by reliance on spurious cues. Instead of requiring costly changes to one's data or model training, our method better utilizes the data one already has by sorting them. Specifically, we rank images within their classes based on spuriosity (the degree to which common spurious cues are present), proxied via deep neural features of an interpretable network. With spuriosity rankings, it is easy to identify minority subpopulations (i.e. low spuriosity images) and assess model bias as the gap in accuracy between high and low spuriosity images. One can even efficiently remove a model's bias at little cost to accuracy by finetuning its classification head on low spuriosity images, resulting in fairer treatment of samples regardless of spuriosity. We demonstrate our method on ImageNet, annotating $5000$ class-feature dependencies ($630$ of which we find to be spurious) and generating a dataset of $325k$ soft seg
    
[^147]: 熔炉2.0

    Melting Pot 2.0. (arXiv:2211.13746v4 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2211.13746](http://arxiv.org/abs/2211.13746)

    研究工具Melting Pot 2.0为多智能体人工智能提供了评估协议，在一组典型测试场景中测量它们对新颖社交伙伴的泛化能力。

    

    多智能体人工智能研究承诺开发比“自我中心”方法更具人类特点和更易于与人类兼容的智能技术。 Melting Pot是为促进多智能体人工智能工作而开发的研究工具，并提供一个评估协议，该协议在一组典型的测试场景中测量对新颖社交伙伴的泛化能力。每种情景将一个物理环境（“基板”）与一组参考合作者（“背景人群”）配对，以创建一个具有个体间相互依存性的社交情境。例如，一些情形受到了基于制度经济学的自然资源管理和公共物品供给困境的考虑的启发，而其他情形则受到了进化生物学、博弈论和人工生命等方面的考虑所启发。Melting Pot旨在涵盖一组最大多样化的情形。

    Multi-agent artificial intelligence research promises a path to develop intelligent technologies that are more human-like and more human-compatible than those produced by "solipsistic" approaches, which do not consider interactions between agents. Melting Pot is a research tool developed to facilitate work on multi-agent artificial intelligence, and provides an evaluation protocol that measures generalization to novel social partners in a set of canonical test scenarios. Each scenario pairs a physical environment (a "substrate") with a reference set of co-players (a "background population"), to create a social situation with substantial interdependence between the individuals involved. For instance, some scenarios were inspired by institutional-economics-based accounts of natural resource management and public-good-provision dilemmas. Others were inspired by considerations from evolutionary biology, game theory, and artificial life. Melting Pot aims to cover a maximally diverse set of 
    
[^148]: 通过基于抽象的训练驯服DNN控制系统的可达性分析

    Taming Reachability Analysis of DNN-Controlled Systems via Abstraction-Based Training. (arXiv:2211.11127v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11127](http://arxiv.org/abs/2211.11127)

    本文提出了一种基于抽象的方法，用于绕过在DNN控制系统中对DNN进行过度近似的可达性分析问题。通过在传统的DNN中插入一个抽象层，将实数抽象化为一个区间进行训练，进而实现对DNN控制系统的黑盒可达性分析。

    

    深度神经网络(DNNs)的内在复杂性使得验证网络本身和托管DNN控制系统变得具有挑战性。这些系统的可达性分析面临相同的挑战。现有的方法依赖于使用更简单的多项式模型对DNN进行过度近似。然而，它们效率低下并且过度估计较大，并且限于特定类型的DNNs。本文提出了一种新颖的基于抽象的方法，绕过在可达性分析中对DNNs进行过度近似的关键问题。具体而言，我们通过插入一个额外的抽象层来扩展传统的DNNs，该抽象层将实数抽象化为一个区间进行训练。插入的抽象层确保区间表示的值在训练和决策过程中对网络不可区分。利用这一点，我们设计了第一个适用于DNN控制系统的黑盒可达性分析方法，其中只对训练过的DNN进行查询。

    The intrinsic complexity of deep neural networks (DNNs) makes it challenging to verify not only the networks themselves but also the hosting DNN-controlled systems. Reachability analysis of these systems faces the same challenge. Existing approaches rely on over-approximating DNNs using simpler polynomial models. However, they suffer from low efficiency and large overestimation, and are restricted to specific types of DNNs. This paper presents a novel abstraction-based approach to bypass the crux of over-approximating DNNs in reachability analysis. Specifically, we extend conventional DNNs by inserting an additional abstraction layer, which abstracts a real number to an interval for training. The inserted abstraction layer ensures that the values represented by an interval are indistinguishable to the network for both training and decision-making. Leveraging this, we devise the first black-box reachability analysis approach for DNN-controlled systems, where trained DNNs are only querie
    
[^149]: 基于概念器辅助的大型语言模型去偏见

    Conceptor-Aided Debiasing of Large Language Models. (arXiv:2211.11087v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.11087](http://arxiv.org/abs/2211.11087)

    本论文提出一种基于概念器的大型语言模型去偏见方法。我们通过后处理和一种新架构CI-BERT将概念器投影纳入所有层中。概念器后处理方法取得了最先进的去偏见结果，同时保持或改善了模型的性能。

    

    预训练的大型语言模型(LLMs)反映了它们训练语料库中固有的社会偏见。许多方法已被提出来减轻这个问题，但它们通常未能去偏见或者会牺牲模型的准确性。我们使用概念器——一种软投影方法——来识别和去除如BERT和GPT等LLMs中的偏见子空间。我们提出了两种应用概念器的方法：（1）通过后处理进行偏见子空间投影；（2）一种新的架构——概念器介入BERT(CI-BERT)，它在训练期间明确地将概念器投影纳入所有层中。我们发现，概念器后处理在保持或提高LLMs在GLUE基准测试中的性能的同时，实现了最先进的去偏见结果。此外，它在各种情况下都很稳健，并且可以通过对现有偏见子空间的逻辑操作来有效地减轻交集偏见。虽然CI-BERT的训练考虑了所有层的偏见，并且在某些任务上表现更好，但它的训练成本更高。

    Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use conceptors--a soft projection method--to identify and remove the bias subspace in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1) bias subspace projection by post-processing; and (2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly incorporates the conceptor projection into all layers during training. We find that conceptor post-processing achieves state-of-the-art (SoTA) debiasing results while maintaining or improving LLMs' performance on the GLUE benchmark. Also, it is robust in various scenarios and can mitigate intersectional bias efficiently by its logical operation on the existing bias subspaces. Although CI-BERT's training takes all layers' bias into account and can beat its post-processing counterpa
    
[^150]: 在具有不确定性集合正则化的连续控制任务中的强化学习

    Robust Reinforcement Learning in Continuous Control Tasks with Uncertainty Set Regularization. (arXiv:2207.02016v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.02016](http://arxiv.org/abs/2207.02016)

    该论文提出了一种新的正则化器USR，通过构建转换函数参数空间上的不确定性集合来提高连续控制任务中的强化学习性能。通过对值函数进行对抗生成未知不确定性集合，进一步增强了USR的灵活性。在真实世界强化学习基准测试中得到了改进的结果。

    

    强化学习（RL）被认为在环境扰动下缺乏泛化性和鲁棒性，这严重限制了其在实际机器人领域的应用。以前的研究声称，在值函数中添加正则化等价于学习具有不确定转换的鲁棒策略。尽管正则化-鲁棒性转换因其简单和高效而具有吸引力，但在连续控制任务中仍然存在不足。在本文中，我们提出了一种新的正则化器，名为不确定性集合正则化器（USR），通过在转换函数的参数空间上构建不确定性集合来实现。特别是，USR足够灵活，可以插入到任何现有的RL框架中。为了处理未知的不确定性集合，我们进一步提出了一种基于值函数生成的新颖对抗方法来生成它们。我们在真实世界强化学习（RWRL）基准测试上评估了USR，展示了改进的结果。

    Reinforcement learning (RL) is recognized as lacking generalization and robustness under environmental perturbations, which excessively restricts its application for real-world robotics. Prior work claimed that adding regularization to the value function is equivalent to learning a robust policy with uncertain transitions. Although the regularization-robustness transformation is appealing for its simplicity and efficiency, it is still lacking in continuous control tasks. In this paper, we propose a new regularizer named $\textbf{U}$ncertainty $\textbf{S}$et $\textbf{R}$egularizer (USR), by formulating the uncertainty set on the parameter space of the transition function. In particular, USR is flexible enough to be plugged into any existing RL framework. To deal with unknown uncertainty sets, we further propose a novel adversarial approach to generate them based on the value function. We evaluate USR on the Real-world Reinforcement Learning (RWRL) benchmark, demonstrating improvements i
    
[^151]: 一类稳定高效的强化学习用替代函数的普适框架

    A general class of surrogate functions for stable and efficient reinforcement learning. (arXiv:2108.05828v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.05828](http://arxiv.org/abs/2108.05828)

    本研究提出了一个基于函数镜像上升的普适框架(FMA-PG)，构建了一系列替代函数，这些函数可以实现策略改进，并且不受策略参数化选择的影响。通过实验证实，该方法具有良好的性能和理论保证。

    

    传统的策略梯度方法依赖于一系列替代函数的最大化。近年来，提出了许多这样的替代函数，大多数没有强有力的理论保证，从而导致了TRPO、PPO或MPO等算法的出现。我们不是设计另一个替代函数，而是提出了一个基于函数镜像上升的普适框架（FMA-PG），从而产生了一整套替代函数。我们构建了替代函数，使其能够保证策略改进，这是大多数现有替代函数所没有的特性。关键是，这些保证不受策略参数化选择的影响。此外，FMA-PG的特定实例恢复了重要的实现启发式方法（例如，使用前向和反向KL散度），从而产生了具有额外理想性质的TRPO变种。通过在简单贝叶斯问题上进行实验，我们评估了FMA-PG产生的算法实例。该框架也支持其他应用。

    Common policy gradient methods rely on the maximization of a sequence of surrogate functions. In recent years, many such surrogate functions have been proposed, most without strong theoretical guarantees, leading to algorithms such as TRPO, PPO or MPO. Rather than design yet another surrogate function, we instead propose a general framework (FMA-PG) based on functional mirror ascent that gives rise to an entire family of surrogate functions. We construct surrogate functions that enable policy improvement guarantees, a property not shared by most existing surrogate functions. Crucially, these guarantees hold regardless of the choice of policy parameterization. Moreover, a particular instantiation of FMA-PG recovers important implementation heuristics (e.g., using forward vs reverse KL divergence) resulting in a variant of TRPO with additional desirable properties. Via experiments on simple bandit problems, we evaluate the algorithms instantiated by FMA-PG. The proposed framework also su
    
[^152]: 重访多智能体深度强化学习中的参数共享

    Revisiting Parameter Sharing in Multi-Agent Deep Reinforcement Learning. (arXiv:2005.13625v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2005.13625](http://arxiv.org/abs/2005.13625)

    本研究重访了多智能体深度强化学习中的参数共享方法。我们通过引入智能体指示信号实现了在不同策略网络共享参数的同时学习不同策略或任务的能力，并且证明了这些方法在异构观测和行动空间学习中可以收敛到最优策略。

    

    参数共享是多智能体深度强化学习中一种常用的基准方法，每个智能体都独立学习一个策略，并且所有策略之间共享参数。然而，由于所有智能体共享同一策略网络，它们无法学习不同的策略或任务。为了解决这个问题，我们通过向观测中添加智能体特定的指示信号（称为“智能体指示”）来进行实验性的改进。然而，智能体指示的局限在于，如果不进行修改，它无法应用于行动空间和/或观测空间不同质的环境。本研究正式定义了智能体指示的概念，并证明了它首次实现了收敛到最优策略。接下来，我们正式介绍了扩展参数共享到异构观测和行动空间学习的方法，并证明了这些方法可以实现收敛到最优策略。最后，我们进行了实验验证并对比了各种方法的性能。

    Parameter sharing, where each agent independently learns a policy with fully shared parameters between all policies, is a popular baseline method for multi-agent deep reinforcement learning. Unfortunately, since all agents share the same policy network, they cannot learn different policies or tasks. This issue has been circumvented experimentally by adding an agent-specific indicator signal to observations, which we term "agent indication". Agent indication is limited, however, in that without modification it does not allow parameter sharing to be applied to environments where the action spaces and/or observation spaces are heterogeneous. This work formalizes the notion of agent indication and proves that it enables convergence to optimal policies for the first time. Next, we formally introduce methods to extend parameter sharing to learning in heterogeneous observation and action spaces, and prove that these methods allow for convergence to optimal policies. Finally, we experimentally
    

