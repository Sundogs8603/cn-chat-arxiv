# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models](https://rss.arxiv.org/abs/2402.01345) | 本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。 |
| [^2] | [Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees](https://rss.arxiv.org/abs/2402.00899) | 这项工作提出了使用具有可证明性能保证的弱监督AI错误修正器来处理AI错误。修正器通过批准或拒绝底层分类器的决策来提升性能，并通过概率界限保证其性能。实验证明该方法在训练数据稀缺的真实世界任务中提升图像分类器的性能。 |
| [^3] | [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/abs/2402.04249) | HarmBench是一个为自动红队和强大拒绝设计的标准化评估框架，通过对18种红队方法和33个目标LLM和防御的比较，得出了新的见解，并介绍了一种高效的对抗训练方法，提高了LLM在各种攻击下的稳健性。 |
| [^4] | [Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science](https://arxiv.org/abs/2402.04247) | 本文探讨了科学领域中基于LLM的智能机器人的漏洞与风险，并强调了对安全措施的重要性。 |
| [^5] | [Can Generative Agents Predict Emotion?](https://arxiv.org/abs/2402.04232) | 本研究探讨了生成型智能体在感知新事件时情绪状态的演变，通过比较新经验和过去记忆来获得理解新经验的能力，并通过情感测试捕捉智能体的情绪状态。 |
| [^6] | [Intelligent Collective Escape of Swarm Robots Based on a Novel Fish-inspired Self-adaptive Approach with Neurodynamic Models](https://arxiv.org/abs/2402.04228) | 基于鱼类启发的自适应方法和神经动力学模型，提出了一种智能群体逃逸机器人的新方法。通过仿真和实验结果表明，这种方法显著提高了机器人的性能。 |
| [^7] | ["Task Success" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors](https://arxiv.org/abs/2402.04210) | 本文探究了将视频-语言模型作为行为批评者用于捕捉不良代理行为的可行性，以解决大规模生成模型忽视任务约束和用户偏好的问题。 |
| [^8] | [Acute kidney injury prediction for non-critical care patients: a retrospective external and internal validation study](https://arxiv.org/abs/2402.04209) | 这项研究开发并比较了深度学习和传统机器学习模型，用于预测非危重病患者在接下来48小时内发展为2期或更高程度的急性肾损伤。内外部验证和亚组分析表明模型的有效性和稳定性。 |
| [^9] | [Human-Like Geometric Abstraction in Large Pre-trained Neural Networks](https://arxiv.org/abs/2402.04203) | 人类的几何能力来自于心理表征中的离散符号结构，而大型预训练神经网络模型在复杂性、规则性和部件与关系的感知敏感性方面表现更类似于人类的抽象几何处理能力。 |
| [^10] | [COPS: A Compact On-device Pipeline for real-time Smishing detection](https://arxiv.org/abs/2402.04173) | COPS是一种用于实时短信诈骗检测的紧凑型设备内管道，通过智能识别欺诈信息和URL特征，并实时警示用户。根据基准测试，COPS在短信诈骗和URL钓鱼检测方面的准确率分别达到了98.15%和99.5%。 |
| [^11] | [Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction](https://arxiv.org/abs/2402.04154) | 本论文探索了为智能体提供增强形式的任务指导，使其能够理解游戏指导并实现"读玩游戏"的能力。通过将多模态指导调优的成功应用于视觉任务中的强化学习任务，构建了一组... (内容太长，无法继续显示) |
| [^12] | [Multi-line AI-assisted Code Authoring](https://arxiv.org/abs/2402.04141) | 这项研究介绍了CodeCompose，一款由大型语言模型驱动的AI辅助代码创作工具，将单行建议扩展为多行建议。作者克服了多个挑战，提高了多行建议的可用性，并进行了大规模实验来评估其对开发人员的影响。 |
| [^13] | [Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs)](https://arxiv.org/abs/2402.04140) | 本研究通过整合先进的语言模型和人工智能技术，开发了名为SHIRLEY的应用程序，旨在识别法律判决中的偏见和逻辑不一致，并促进自动化、有效和一致的多方论证，以保证法律在不同司法管辖区内的一致应用。 |
| [^14] | [Hierarchical Delay Attribution Classification using Unstructured Text in Train Management Systems](https://arxiv.org/abs/2402.04108) | 本文研究了基于事件描述的机器学习决策支持方法，用于在列车管理系统中自动分配延迟归因代码。研究发现，分层方法比扁平方法表现更好，但仍然不及手动分类的性能。 |
| [^15] | [An Exploration of Clustering Algorithms for Customer Segmentation in the UK Retail Market](https://arxiv.org/abs/2402.04103) | 本文旨在开发一个客户分群模型，通过对基于英国的在线零售数据集的研究，使用RFM框架和最先进的聚类算法，以提高零售市场行业的决策过程。 |
| [^16] | [Use of Multi-CNNs for Section Analysis in Static Malware Detection](https://arxiv.org/abs/2402.04102) | 本论文提出了一种使用多卷积神经网络对静态恶意软件检测中的分析进行分析的新模型，该模型将可执行文件分成不同部分，并将每个部分转化为图像，通过训练卷积神经网络对每个部分进行处理，最终得到恶意软件检测分数，并提供对每个部分重要性的分析。 |
| [^17] | [The Use of a Large Language Model for Cyberbullying Detection](https://arxiv.org/abs/2402.04088) | 这项研究探索了使用大型语言模型（LLMs）如BERT和RoBERTa进行网络欺凌检测，并准备了一个新的数据集（D2）以应对高度不平衡的类别和泛化问题。 |
| [^18] | [A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation](https://arxiv.org/abs/2402.04087) | 本文研究了CLIP训练方法的一个经典算法，称为高斯判别分析（GDA），并将其应用于CLIP的下游分类任务。通过对特征分布的估计，无需训练即可实现分类器，从而提高了CLIP的性能。 |
| [^19] | [An Optimal House Price Prediction Algorithm: XGBoost](https://arxiv.org/abs/2402.04082) | 房价预测是房地产和抵押贷款等多个领域的基本要求，这篇论文提出了一种优化的房价预测算法XGBoost，通过比较多种机器学习技术的表现，并确定关键因素，结果表明XGBoost是最佳的模型。 |
| [^20] | [Improved Generalization of Weight Space Networks via Augmentations](https://arxiv.org/abs/2402.04081) | 通过扩充权重空间的数据集，采用MixUp方法，我们改进了权重空间网络的泛化能力和性能。 |
| [^21] | [Multi-class Road Defect Detection and Segmentation using Spatial and Channel-wise Attention for Autonomous Road Repairing](https://arxiv.org/abs/2402.04064) | 我们提出了一种新颖的端到端方法，用于同时进行多类道路缺陷检测和分割。通过多个空间和通道注意力块的应用，可以学习到更全局化的道路缺陷形态和图像颜色深度信息的表示。我们通过各种实验和与之前方法的比较验证了我们方法的有效性。 |
| [^22] | [Link Prediction with Relational Hypergraphs](https://arxiv.org/abs/2402.04062) | 本文提出了两种适用于关系超图的链接预测框架，并通过实证分析验证了这些框架在各种关系超图基准测试上的有效性。 |
| [^23] | [Deep Learning for Multivariate Time Series Imputation: A Survey](https://arxiv.org/abs/2402.04059) | 本文调查了深度学习在多变量时间序列插补中的应用。通过综述不同的方法以及它们的优点和限制，研究了它们对下游任务性能的改进，并指出了未来研究的开放问题。 |
| [^24] | [Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models](https://arxiv.org/abs/2402.04050) | 本论文提出了一种协作微调黑盒视觉语言模型的方法，可以在只有输入提示和输出预测的情况下进行微调，提供了一个提示生成模块和一个预测优化模块，并引入了辅助的预测一致性损失进行一致优化。 |
| [^25] | [Systematic Biases in LLM Simulations of Debates](https://arxiv.org/abs/2402.04049) | 本研究揭示了LLMs在模拟政治辩论中存在的系统性偏差，尽管被指定从特定的政治观点进行辩论，LLMs代理机构倾向于遵循模型固有的社会偏见。通过自动自我优化方法，我们进一步证实了这些观察结果。 |
| [^26] | [Generative Modeling of Graphs via Joint Diffusion of Node and Edge Attributes](https://arxiv.org/abs/2402.04046) | 通过联合扩散节点和边属性，我们提出了一个新的图形生成模型，考虑了所有图组件，并通过注意模块和相互依赖的节点、边和邻接信息实现了更好的效果。 |
| [^27] | [HEAM : Hashed Embedding Acceleration using Processing-In-Memory](https://arxiv.org/abs/2402.04032) | HEAM是一种采用异构内存架构的方法，将3D堆叠DRAM与DIMM集成，用于加速处理大规模个性化推荐系统中的嵌入操作。 |
| [^28] | [AlbNews: A Corpus of Headlines for Topic Modeling in Albanian](https://arxiv.org/abs/2402.04028) | 本文介绍了一个包含600个主题标签的新闻标题和2600个未标签的阿尔巴尼亚语数据集，该数据集对于低资源语言的自然语言处理研究非常有价值，同时也证实了基本模型的优越性能。 |
| [^29] | [Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.04009) | LAST方法通过冻结预训练模型的输出和参数，将可训练模块与预训练模型解耦，利用低秩自注意力模块训练一个侧向网络，从而实现参数高效微调，并具有高度并行化的效率。 |
| [^30] | [Joint Intrinsic Motivation for Coordinated Exploration in Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2402.03972) | 本文提出了一种用于解决多智能体深度强化学习中稀疏奖励和协调探索挑战的方法，通过联合内在动机奖励联合轨迹的新颖性行为，实现了多智能体之间的协作和最优性。 |
| [^31] | [Tabular Data: Is Attention All You Need?](https://arxiv.org/abs/2402.03970) | 本文引入了一项大规模实证研究，比较了神经网络和梯度提升决策树在表格数据上的表现，还比较了基于Transformer的架构和传统的多层感知器（MLP）与残差连接的架构。实证结果显示，神经网络在决策树方面具有竞争力，而基于Transformer的架构在表格数据集上并没有超过传统MLP架构的简化变体。 |
| [^32] | [Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims](https://arxiv.org/abs/2402.03962) | 这篇论文讨论了在寻求人工通用智能的过程中，人们对于大型语言模型的类人特质过度归因的现象，并呼吁学术界在解读和传播关于AI研究的内容时要保持谨慎。 |
| [^33] | [Boosting Adversarial Transferability across Model Genus by Deformation-Constrained Warping](https://arxiv.org/abs/2402.03951) | 本论文提出了一种名为DeCoWA的攻击策略，通过弹性变形对输入进行增强，从而实现了跨模型种族的攻击。实验证明了该策略在攻击不同模型种族系统上表现出色。 |
| [^34] | [Identifying Student Profiles Within Online Judge Systems Using Explainable Artificial Intelligence](https://arxiv.org/abs/2402.03948) | 本研究通过使用可解释的人工智能技术，在在线评测系统中识别学生的特征，并自动推断出对学生和教师有用的反馈。 |
| [^35] | [Discovery of the Hidden World with Large Language Models](https://arxiv.org/abs/2402.03941) | 通过使用大型语言模型，我们提出了COAT：因果表示助手，该助手从原始观测数据中提取潜在的因果因子，并将其转化为结构化数据，为探索隐藏世界提供了新的机会。 |
| [^36] | [Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs](https://arxiv.org/abs/2402.03927) | 该论文研究了封闭源LLMs中的数据污染和评估不端行为。通过对255篇论文的分析和OpenAI的数据使用政策考虑，研究人员发现这些模型在第一年发布后存在泄露数据的问题。 |
| [^37] | [Large Language Models to Enhance Bayesian Optimization](https://arxiv.org/abs/2402.03921) | 通过结合大型语言模型（LLM）的能力，我们提出了一种名为LLAMBO的新方法，将其应用于贝叶斯优化（BO）。通过用自然语言描述BO问题，并利用LLM的上下文理解、少样本学习能力和领域知识，LLAMBO能够提供有前景的解决方案，并且在零样本热启动方面表现出良好的效果。 |
| [^38] | [Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy](https://arxiv.org/abs/2402.03907) | 本研究探讨了将大型语言模型嵌入扩展现实中的机会和挑战，认为通过使用LLMs可以实现扩展现实的更包容和参与，并有望推动其在日常生活中的广泛应用。 |
| [^39] | [DistiLLM: Towards Streamlined Distillation for Large Language Models](https://arxiv.org/abs/2402.03898) | DistiLLM是一个更有效和高效的自回归语言模型蒸馏框架，通过引入新颖的偏斜Kullback-Leibler散度损失和自适应的离策略方法，解决了当前针对大语言模型的知识蒸馏方法缺乏标准化目标函数和计算成本过高的问题。 |
| [^40] | [MOMENT: A Family of Open Time-series Foundation Models](https://arxiv.org/abs/2402.03885) | MOMENT是一个开放的时间序列基础模型家族，通过解决时间序列数据的挑战，编制了一个大规模的公共时间序列数据集，并设计了一个基准测试来评估有限监督场景下模型的性能。 |
| [^41] | [Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models](https://arxiv.org/abs/2402.03877) | 本文调查了大型语言模型（LLMs）在几何推理方面的能力，并发现了它们在目标变量选择和2D空间关系方面存在偏见和困难。通过引入基于LLMs的多代理体系结构，本研究提出了一种通过自我纠正、协作和不同角色专业化来提高LLMs几何推理能力的框架。 |
| [^42] | [Position Paper: Toward New Frameworks for Studying Model Representations](https://arxiv.org/abs/2402.03855) | 通过逆向工程AI模型的确切算法，机制解释性（MI）旨在理解模型。然而，目前的研究主要关注微不足道的行为和能力，而忽视了隐藏在网络内部的表示。因此，我们呼吁研究界朝着新的框架努力，研究这些表示。 |
| [^43] | [ANLS* -- A Universal Document Processing Metric for Generative Large Language Models](https://arxiv.org/abs/2402.03848) | ANLS*是一种用于生成型模型的新度量方法，针对各种任务包括信息提取和分类任务进行评估。它扩展了现有的ANLS度量方法，可以作为替代方案使用。 |
| [^44] | [A new method for optical steel rope non-destructive damage detection](https://arxiv.org/abs/2402.03843) | 本文提出了一种新的算法用于在高海拔环境中对钢丝绳进行非破坏性损伤检测，其中包括一种准确提取钢丝绳的分割模型和一种区分正常和异常钢丝绳的检测模型，实验证明其性能显著高于基准模型。 |
| [^45] | [A call for embodied AI](https://arxiv.org/abs/2402.03824) | 具象人工智能被提出作为追求人工通用智能的下一个基本步骤，并引入了一个基于认知架构的理论框架，与Friston的主动推断原则保持一致，为具象人工智能的发展提供了一个全面的方法。 |
| [^46] | [RevOrder: A Novel Method for Enhanced Arithmetic in Language Models](https://arxiv.org/abs/2402.03822) | 本文提出了一种名为RevOrder的新方法，通过翻转加法、减法和nD乘以1D的输出数字，显著改善了语言模型中的算术运算。经过全面测试，RevOrder在基本算术运算中达到了完美准确度，并在除法任务中提升了语言模型性能，特别是在处理大数时。在GSM8K数学任务中应用RevOrder进行微调，有效降低了错误率并提高了总体得分。 |
| [^47] | [SEABO: A Simple Search-Based Method for Offline Imitation Learning](https://arxiv.org/abs/2402.03807) | SEABO是一种简单而有效的基于搜索的离线模仿学习方法，它以无监督学习的方式，根据专家数据和无标签数据得到奖励函数，实验结果表明其性能与离线强化学习相当。 |
| [^48] | [ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs](https://arxiv.org/abs/2402.03804) | 通过动态跳过非活跃神经元的计算，我们提出了一种稀疏LLM的高效激活函数ReLU^2，它在稀疏性与性能、稀疏性的预测性和硬件亲和性等方面表现出色。 |
| [^49] | [Face Detection: Present State and Research Directions](https://arxiv.org/abs/2402.03796) | 本文对人脸检测领域的现状进行了综述，指出了其存在的问题，并提供了研究方向，以提高人脸检测的准确性和速度。 |
| [^50] | [No-Regret Reinforcement Learning in Smooth MDPs](https://arxiv.org/abs/2402.03792) | 本论文针对具有连续状态和/或动作空间的强化学习问题提出了一种无悔保证的新方法，即通过在Legendre多项式基础上构建MDP表示来解决$\nu-$平滑 MDPs，在较弱的假设下可以达到无悔特性，同时在多项式时间内运行。 |
| [^51] | [AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction](https://arxiv.org/abs/2402.03784) | AirPhyNet是一种利用物理引导的神经网络方法，通过将空气颗粒运动的物理原理和神经网络结构相结合，提高了对空气质量的预测精度和可解释性。 |
| [^52] | [Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More](https://arxiv.org/abs/2402.03782) | 本研究探索了软提示调整（SPT）在跨语言迁移中的应用。通过仅训练软提示而不调整模型参数，实现了更高的参数效率，并提高了对语言差异较大的语言的跨语言迁移性能。 |
| [^53] | [MolTC: Towards Molecular Relational Modeling In Language Models](https://arxiv.org/abs/2402.03781) | 本研究提出了一种基于语言模型的多模态框架MolTC，用于分子相互作用预测，该框架能够高效地整合分子对的丰富图形信息，并通过思维链理论实现统一的分子关系学习。 |
| [^54] | [Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification](https://arxiv.org/abs/2402.03780) | 本文通过分析文体线索比较人类注释和机器分类的方法，揭示了宣传语言的特征，并提出了一个多源、多语言、多模态的数据集PPN。结果表明，人类注释者能够可靠地区分宣传新闻和常规新闻。研究还比较了不同的自然语言处理技术，并提供了一些有关文体线索的发现。 |
| [^55] | [Large Language Models As MOOCs Graders](https://arxiv.org/abs/2402.03776) | 该研究探索了利用大型语言模型（LLMs）代替MOOCs中同伴评分的可行性，旨在解决大规模在线开放课程中评估学生写作任务的问题。 |
| [^56] | [Learning a Decision Tree Algorithm with Transformers](https://arxiv.org/abs/2402.03774) | 该论文介绍了MetaTree模型，它使用经典算法的输出训练基于Transformer的模型，以产生具有强大概括性能的决策树。 |
| [^57] | [MobileVLM V2: Faster and Stronger Baseline for Vision Language Model](https://arxiv.org/abs/2402.03766) | MobileVLM V2是在MobileVLM基础上改进的视觉语言模型，通过新颖的架构设计、改进的训练方案和高质量数据集，能够在标准测试中达到甚至超过大规模VLMs的性能。 |
| [^58] | [QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large Language Model](https://arxiv.org/abs/2402.03755) | 本论文介绍了一种基于大型语言模型的自主代理框架QuantAgent，通过一个有原则的两层循环，使代理能够逐步逼近具有可证明效率的最佳行为。该框架通过构建和集成领域特定的知识库，在量化投资领域展示出了较强的能力。 |
| [^59] | [Digital Twin Mobility Profiling: A Spatio-Temporal Graph Learning Approach](https://arxiv.org/abs/2402.03750) | 本论文提出了一种数字孪生移动性建模方法，通过构建对齐图和设计扩张对齐卷积网络来捕捉交通场景中复杂的时空特征，为智能交通系统的开发提供了有效的方法。 |
| [^60] | [SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems](https://arxiv.org/abs/2402.03741) | 该研究首次揭示了攻击者在多智能体竞争环境中即使受限于受害者的部分观测也能生成对抗策略的能力。 |
| [^61] | [Deep Outdated Fact Detection in Knowledge Graphs](https://arxiv.org/abs/2402.03732) | 本文介绍了一种名为DEAN的新型深度学习框架，用于在知识图谱中识别过时事实。DEAN通过全面建模实体和关系之间的隐含结构信息，采用了一种基于对比的方法来揭示潜在的过时信息。实验证明DEAN相对于最先进的基准方法具有显著的优势。 |
| [^62] | [Consistent Joint Decision-Making with Heterogeneous Learning Models](https://arxiv.org/abs/2402.03728) | 本文提出了一种新颖的决策框架，通过整合不同模型的预测和外部知识，实现了决策的一致性。经过实证研究，我们的方法在多个数据集上表现出优越性能。 |
| [^63] | [Similarity-based Neighbor Selection for Graph LLMs](https://arxiv.org/abs/2402.03720) | 基于相似性的邻居选择（SNS）通过改善所选邻居的质量，改善了图形表示，并提高了泛化性能和可扩展性，解决了处理文本属性图（TAGs）的挑战。 |
| [^64] | [Empowering Language Models with Active Inquiry for Deeper Understanding](https://arxiv.org/abs/2402.03719) | 本文提出了一种名为LaMAI的语言模型，通过主动询问的方式与用户进行交互，有效提高了对用户查询的理解能力，并减少了错误解读的发生。 |
| [^65] | [Clarify: Improving Model Robustness With Natural Language Corrections](https://arxiv.org/abs/2402.03715) | 论文提出了Clarify，一种通过自然语言纠正模型错误概念的方法，该方法通过用户提供简短的文本描述来纠正模型的一致失败模式，从而提高模型的鲁棒性。 |
| [^66] | [MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature Drone Threats](https://arxiv.org/abs/2402.03706) | MMAUD是一个全面的多模态反无人机数据集，通过结合多种感知输入和提供准确的真实数据，弥补了当下威胁检测方法中的关键缺口，成为一项无价的资源。 |
| [^67] | [GenLens: A Systematic Evaluation of Visual GenAI Model Outputs](https://arxiv.org/abs/2402.03700) | 这个论文介绍了一种名为GenLens的系统，该系统旨在对GenAI模型的输出进行系统评估。作者进行了一项形成性研究，发现当前的评估方法存在缺陷，并开发了GenLens作为解决方案。GenLens提供了一种可量化的方法来查看和注释GenAI模型输出中的失败案例，并能够定制问题标签和分类。用户研究表明，GenLens能够提高评估准确性和效率。 |
| [^68] | [ServeFlow: A Fast-Slow Model Architecture for Network Traffic Analysis](https://arxiv.org/abs/2402.03694) | ServeFlow提出了一种快速-慢速模型架构，用于网络流量分析。通过精心选择收集数据包的数量和应用于不同流量的模型，ServeFlow实现了最小延迟、高服务率和高准确性之间的平衡。在测试中，ServeFlow能够在16ms内对76.3%的流量进行推理，这是中位数推理时间的40.5倍加速！ |
| [^69] | [A Survey of Privacy Threats and Defense in Vertical Federated Learning: From Model Life Cycle Perspective](https://arxiv.org/abs/2402.03688) | 本文综述了垂直联邦学习中隐私威胁和防御的最新研究进展，通过对模型生命周期的视角进行讨论，提供了行业和实践者在保护数据隐私方面的指导和见解。 |
| [^70] | [Minds versus Machines: Rethinking Entailment Verification with Language Models](https://arxiv.org/abs/2402.03686) | 本文通过研究人类和大型语言模型在推理判断中的共性和差异，发现大型语言模型在复杂推理中具有优势，而人类在简单推理中表现出色。基于这些发现，引入了一个优化的Flan-T5模型，用于蕴含验证。 |
| [^71] | [RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback](https://arxiv.org/abs/2402.03681) | RL-VLM-F是一种通过视觉语言基础模型反馈的强化学习方法，能够自动生成有效的奖励函数和策略，从而解决了传统强化学习中奖励设计的挑战。 |
| [^72] | [Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents](https://arxiv.org/abs/2402.03678) | 本文提出了一种逻辑规范引导下的动态任务采样（LSTS）方法，通过学习一组强化学习策略，根据高级任务规范指导智能体在最小化环境交互次数的同时实现从初始状态到目标状态的引导。在网格世界实验中，LSTS实现了改进的时间到阈值。 |
| [^73] | [Effective Protein-Protein Interaction Exploration with PPIretrieval](https://arxiv.org/abs/2402.03675) | PPIretrieval是第一个基于深度学习的模型，可以在嵌入空间中有效搜索潜在PPIs，并捕捉蛋白质表面的丰富几何和化学信息。 |
| [^74] | [Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning](https://arxiv.org/abs/2402.03667) | 本文提出了一种大型语言模型的新型间接推理方法，使用反证和矛盾的逻辑来处理复杂推理任务，并通过增强数据和规则，以及设计提示模板的方式增强模型的推理能力。 |
| [^75] | [Symbol Correctness in Deep Neural Networks Containing Symbolic Layers](https://arxiv.org/abs/2402.03663) | 本文介绍了神经符号深度神经网络（NS-DNNs）中的符号正确性原则，即用于推理的神经层对中间符号的预测必须与输入数据的符号表示相匹配。符号正确性是NS-DNN可解释性和迁移学习的必要特性，并为推理和交流模型行为提供了精确的方法。 |
| [^76] | [Transductive Reward Inference on Graph](https://arxiv.org/abs/2402.03661) | 该研究提出了一种在图上进行传递式奖励推断的方法，可以有效地估计离线强化学习中未标记数据的奖励。通过利用有限的人工奖励注释和可用数据构建奖励传播图，并利用图进行奖励推断，从而推断出未标记数据的奖励。 |
| [^77] | [Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm](https://arxiv.org/abs/2402.03660) | 本文发现了在预训练-微调范式中，使用相同预训练检查点初始化并在不同任务上进行微调的模型会出现一个有趣的线性现象，称为跨任务线性。我们提供了实证证据并推测神经网络在这一范式中本质上类似线性映射，从参数空间到特征空间的映射。这一发现揭示了关于模型合并/编辑和参数共享等方面的新见解。 |
| [^78] | [CAMBranch: Contrastive Learning with Augmented MILPs for Branching](https://arxiv.org/abs/2402.03647) | CAMBranch是一个利用对比学习和增强MILP的机器学习框架，用于改进混合整数线性规划的分支策略。通过生成增强MILP并应用对比学习，CAMBranch能够获取大量标记的专家样本，从而提高分支决策的质量。 |
| [^79] | [torchmSAT: A GPU-Accelerated Approximation To The Maximum Satisfiability Problem](https://arxiv.org/abs/2402.03640) | 本文提出了一种GPU加速的近似方法torchmSAT来解决最大可满足性问题，其通过推导出可微函数和使用神经网络架构，并利用GPU加速计算来提高解决效率。实验结果表明，该方法优于现有的两种MaxSAT求解器。 |
| [^80] | [Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context](https://arxiv.org/abs/2402.03630) | 通过IDE的本地集成，我们提出了IDECoder框架，利用IDE提供的准确和实时的跨文件信息来增强LLM-Based编码工具，解决了挑战性的跨文件上下文问题。 |
| [^81] | [Partially Recentralization Softmax Loss for Vision-Language Models Robustness](https://arxiv.org/abs/2402.03627) | 本文研究了通过修改预训练多模态模型的损失函数来提高对抗鲁棒性，通过限制前K个softmax输出。实验结果表明，经过微调后，模型的对抗鲁棒性显著提高，能够有效抵御常见的攻击。 |
| [^82] | [Neural Network Approximators for Marginal MAP in Probabilistic Circuits](https://arxiv.org/abs/2402.03621) | 本文提出了一种使用神经网络近似概率电路中边际MAP推理的方法，该方法通过使用连续多线性函数来估计查询变量的赋值成本并将其作为损失函数，具有自我监督和高效性的优点。 |
| [^83] | [Self-Discover: Large Language Models Self-Compose Reasoning Structures](https://arxiv.org/abs/2402.03620) | SELF-DISCOVER是一个通用框架，能让大型语言模型自主发现任务内在的推理结构，显著提升了复杂推理问题的解决性能，并在推理计算方面取得更好的效果。 |
| [^84] | [Comparing Abstraction in Humans and Large Language Models Using Multimodal Serial Reproduction](https://arxiv.org/abs/2402.03618) | 这项研究通过实现一个多模态连续复制框架，比较了人类和GPT-4的抽象能力。实验结果表明，将语言作为一种模态对人类的复制影响更大，这暗示人类的视觉和语言表征比GPT-4的表征更具分离性。 |
| [^85] | [Leveraging Large Language Models for Hybrid Workplace Decision Support](https://arxiv.org/abs/2402.03616) | 本论文研究了利用大型语言模型（LLMs）为混合工作场所提供智能决策支持的决策支持模型。通过广泛的用户研究和评估，发现LLMs在提供适当工作区建议方面具有超越提示的推理能力，提高工作者的工作体验。 |
| [^86] | [RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents](https://arxiv.org/abs/2402.03610) | 本研究提出了一种称为RAP的框架，它能够以动态方式利用过去的经验来增强代理的规划能力，并在纯文本和多模态环境中表现出色。实验结果显示RAP在文本场景中达到了最先进水平，并显著提高了多模态LLM代理在具身任务中的性能。 |
| [^87] | [Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning](https://arxiv.org/abs/2402.03607) | 本研究提出了一种将常识知识图谱与大型视觉语言模型相结合的框架，用于改进预测多模态营销活动效果的性能。该方法能够提供早期检测可能具有说服力的多模态活动并评估和增强营销理论的能力。 |
| [^88] | [Assessing the Impact of Distribution Shift on Reinforcement Learning Performance](https://arxiv.org/abs/2402.03590) | 评估强化学习性能时需要考虑分布转变，我们提出了一套评估方法，并推荐使用时间序列分析进行观测RL评估。 |
| [^89] | [Continual Domain Adversarial Adaptation via Double-Head Discriminators](https://arxiv.org/abs/2402.03588) | 本文提出了一种通过双头判别器进行连续领域对抗适应的方法，在源学习阶段引入了一个仅在源域训练的源域判别器，减少了对抗损失的经验估计误差，实验结果表明算法实现了超过2%的准确提升。 |
| [^90] | [MQuinE: a cure for "Z-paradox'' in knowledge graph embedding models](https://arxiv.org/abs/2402.03583) | 研究者发现知识图谱嵌入模型存在的“Z-悖论”限制了其表达能力，并提出了一种名为MQuinE的新模型，通过理论证明，MQuinE成功解决了Z-悖论，并在链接预测任务中显著优于现有模型。 |
| [^91] | [LLM Multi-Agent Systems: Challenges and Open Problems](https://arxiv.org/abs/2402.03578) | 本文讨论了多智能体系统的挑战与开放问题，包括任务分配优化、增强推理能力、管理上下文信息和改善内存管理，同时探讨了多智能体系统在区块链系统中的潜力和未来发展。 |
| [^92] | [Toward Human-AI Alignment in Large-Scale Multi-Player Games](https://arxiv.org/abs/2402.03575) | 本研究提出了一种在大规模多人游戏中评估人工智能与人类协作的方法，通过分析人类游戏数据和训练AI代理来比较和对比人类和AI的行为差异，以识别高级行为概念。 |
| [^93] | [Diffusion World Model](https://arxiv.org/abs/2402.03570) | 扩散世界模型是一个能够预测多步未来状态和奖励的条件性扩散模型，在模型效果和性能方面超过了传统的一步动力学模型。 |
| [^94] | [Distinguishing the Knowable from the Unknowable with Language Models](https://arxiv.org/abs/2402.03563) | 通过研究大型语言模型，在自由文本中识别作为代理的模型和冻结预训练模型的嵌入的小型线性探测器可以准确预测更大模型令牌级别上的自信度，进一步提出了一种无监督的方法在相同任务上达到了非平凡的准确度，这证明了语言模型中存在不同类型的不确定性表示。 |
| [^95] | [VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation](https://arxiv.org/abs/2402.03561) | VLN-Video利用行车视频的多样室外环境和自动生成的导航指令与动作，通过深度学习方法提高了室外视觉与语言导航的性能。 |
| [^96] | [Projected Generative Diffusion Models for Constraint Satisfaction](https://arxiv.org/abs/2402.03559) | 本文介绍了一种名为投影式生成扩散模型（PGDM）的方法，它能够通过约束优化问题将生成扩散模型适用于对特定条件有严格要求的场景。该方法通过迭代投影方法确保生成的数据符合指定的约束或物理原理。实验证明PGDM在复杂的约束和常微分方程的情况下也能合成出符合要求的输出。 |
| [^97] | [Extended Version of: On the Structural Hardness of Answer Set Programming: Can Structure Efficiently Confine the Power of Disjunctions?](https://arxiv.org/abs/2402.03539) | 我们进行了对回答集编程的结构参数的分类研究，并提供了一种多项式核以实现与顶点覆盖大小对应的单指数时间运行。 |
| [^98] | [Preliminary Report on Mantis Shrimp: a Multi-Survey Computer Vision Photometric Redshift Model](https://arxiv.org/abs/2402.03535) | 这项研究报道了Mantis Shrimp，一个基于计算机视觉的多调查光度红移模型，融合了紫外线、光学和红外图像，并使用深度学习可解释性诊断技术研究了其如何利用不同输入信息。 |
| [^99] | [Deep Reinforcement Learning for Picker Routing Problem in Warehousing](https://arxiv.org/abs/2402.03525) | 本研究提出了一种基于注意力机制的神经网络模型，通过深度强化学习解决仓储中的拣选车辆路径问题。与传统启发式方法相比，该方法具有更快的速度和准确度，并能降低路径的感知复杂度。 |
| [^100] | [Resolving Transcription Ambiguity in Spanish: A Hybrid Acoustic-Lexical System for Punctuation Restoration](https://arxiv.org/abs/2402.03519) | 提出了一种混合声学-词汇系统，用于解决西班牙语标点符号恢复任务，通过整合声学和词汇信号，在西班牙语转录中提高了问号的F1分数和整体标点符号恢复，并在准确性、可靠性和延迟方面超越了大型语言模型。 |
| [^101] | [Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains](https://arxiv.org/abs/2402.03509) | 本研究跨领域评估了零样本摘要生成器的真实性。通过对生物医学文章和法律法案等专业领域进行评估，我们特别关注生成摘要的真实性，并分析了预训练语料库中给定领域的普遍性对摘要质量的影响。 |
| [^102] | [Neural networks for abstraction and reasoning: Towards broad generalization in machines](https://arxiv.org/abs/2402.03507) | 这项工作探索了神经网络在广泛泛化方面的应用，通过研究解决抽象和推理任务的新方法，试图提高计算机系统从少量示例中学习新概念的能力。 |
| [^103] | [An Inpainting-Infused Pipeline for Attire and Background Replacement](https://arxiv.org/abs/2402.03501) | 本研究提出了一种基于修复和替换的服装与背景生成关键技术，利用GenAI和计算机视觉中的先进技术，通过深度估计、修复掩模创建和稳定扩散和潜在一致性模型（LCMs），实现了对个人照片中服装和背景的修改和修复。 |
| [^104] | [Curriculum reinforcement learning for quantum architecture search under hardware errors](https://arxiv.org/abs/2402.03500) | 本研究提出了一种基于课程的强化学习量子架构搜索算法（CRLQAS），解决了在噪声中间规模量子时代中，对于架构搜索的噪声效应的不理解的问题。 |
| [^105] | [Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues](https://arxiv.org/abs/2402.03494) | 本论文通过将语音转录和语音非言语特征整合到LLM决策中来改善机器人导航中的决策能力，超越了仅使用文字的限制。 |
| [^106] | [Early prediction of onset of sepsis in Clinical Setting](https://arxiv.org/abs/2402.03486) | 本研究提出了使用机器学习模型预测临床数据中脓毒症的早期发作，通过使用有监督学习方法训练XGBoost模型，并利用规范化效用分数评估模型的性能。 |
| [^107] | [SWAG: Storytelling With Action Guidance](https://arxiv.org/abs/2402.03483) | SWAG是一种新的故事讲述方法，通过将故事写作简化为搜索问题，使用两个模型的反馈循环来指导故事的发展方向。在GPT-4和人工评估中，SWAG表现出显著的优势，并且使用仅开源模型的SWAG流程超过了GPT-3.5-Turbo。 |
| [^108] | [Trillion Parameter AI Serving Infrastructure for Scientific Discovery: A Survey and Vision](https://arxiv.org/abs/2402.03480) | 这篇论文调研了为科学研究提供支持的兆级参数人工智能基础设施，并描述了在系统设计中所面临的重要技术挑战和开放问题。 |
| [^109] | [ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design](https://arxiv.org/abs/2402.03479) | 本研究探索了深度强化学习代理的零样本迁移能力，并发现通过根据值损失优先选择级别，可以改善代理的推广能力。此外，无监督环境设计方法对改善代理表现也具有重要作用。 |
| [^110] | [The Information of Large Language Model Geometry](https://arxiv.org/abs/2402.03471) | 论文研究了大型语言模型中嵌入的信息编码，并发现表示熵与模型大小呈幂律关系。通过信息理论和回归技术，建立了新标记的信息增益与岭回归之间的理论联系，并探索了Lasso回归在选择有意义的标记方面的有效性。实验结果表明，信息在标记之间分布，而不仅仅集中在特定的“有意义”的标记上。 |
| [^111] | [Preference-free Alignment Learning with Regularized Relevance Reward](https://arxiv.org/abs/2402.03469) | 无偏好对齐学习使用正则化相关奖励作为关键目标，在提供稳健奖励信号的同时，显著提高了偏好基准测试的性能。 |
| [^112] | [Psychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach](https://arxiv.org/abs/2402.03435) | 本研究使用大型语言模型（LLMs）分析Reddit用户的文本评论，以实现隐私保护和成本效益为重点，通过精心设计的提示和语法来实现预定义的自杀风险心理评估，取得了杰出的结果。 |
| [^113] | [UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing](https://arxiv.org/abs/2402.03396) | 本研究提出了UniTSyn，一个大型数据集，可以增强大型语言模型在程序测试中的能力。通过关联测试和被测试函数，UniTSyn能够提高模型在生成准确和完整的测试方面的表现。 |
| [^114] | [PixelGen: Rethinking Embedded Camera Systems](https://arxiv.org/abs/2402.03390) | PixelGen是一个重新构想嵌入式摄像系统的平台，通过结合传感器、收发器和低分辨率摄像头和红外视觉传感器，它能够捕捉到更广泛的世界表达，并且通过使用transformer-based图像和语言模型，这些简单的数据也可以产生出环境的新颖表达。 |
| [^115] | [Delivery Optimized Discovery in Behavioral User Segmentation under Budget Constrain](https://arxiv.org/abs/2402.03388) | 在预算限制下，我们提出了一种基于随机优化的算法，用于优化传递发现行为用户细分。 |
| [^116] | [A generalized decision tree ensemble based on the NeuralNetworks architecture: Distributed Gradient Boosting Forest (DGBF)](https://arxiv.org/abs/2402.03386) | 本文提出了一种基于神经网络结构的通用决策树集成算法，分布式梯度提升森林（DGBF），通过将包和提升的数学公式结合起来，实现了树之间自然地进行分布式表示学习过程。该算法能够处理离散或表格数据，并具有建模非结构化数据的能力。 |
| [^117] | [Survival and grade of the glioma prediction using transfer learning](https://arxiv.org/abs/2402.03384) | 本研究利用迁移学习技术，通过对胶质母细胞瘤图像数据集进行微调，成功实现了生存和肿瘤等级预测，生存预测准确率达到65%，肿瘤等级预测准确率达到97%。 |
| [^118] | [Entire Chain Uplift Modeling with Context-Enhanced Learning for Intelligent Marketing](https://arxiv.org/abs/2402.03379) | 全链路上升建模方法ECUP旨在解决链路偏差和处理不适应问题，在线营销中有重要的应用价值。 |
| [^119] | [BetterV: Controlled Verilog Generation with Discriminative Guidance](https://arxiv.org/abs/2402.03375) | 本文提出了一种Verilog生成框架BetterV，通过微调大型语言模型和生成判别器的使用，实现了可控的Verilog生成，能够生成语法和功能正确的Verilog实现。 |
| [^120] | [Detection of tortured phrases in scientific literature](https://arxiv.org/abs/2402.03370) | 本文介绍了自动检测科学论文中拙劣短语的方法，通过语言模型和预测分数的传播，可以高效地标记并提取这些拙劣短语，为领域专家验证提供新的数据。 |
| [^121] | [Empirical and Experimental Perspectives on Big Data in Recommendation Systems: A Comprehensive Survey](https://arxiv.org/abs/2402.03368) | 本综合调查论文对推荐系统中的大数据算法进行了全面分析，并提出了一种新颖的、分层的分类法。通过该分类法，研究人员可以全面了解不同算法和技术之间的相互关系。 |
| [^122] | [Uncertainty-Aware Explainable Recommendation with Large Language Models](https://arxiv.org/abs/2402.03366) | 这项研究开发了一个模型，通过训练用户和项目输入的ID向量作为提示，利用GPT-2实现不确定性感知的可解释推荐系统。该系统采用联合训练机制并在多任务学习框架中进行优化，能够更有效地探索用户的兴趣，提高推荐效果和用户满意度。 |
| [^123] | [NanoNER: Named Entity Recognition for nanobiology using experts' knowledge and distant supervision](https://arxiv.org/abs/2402.03362) | 本文介绍了NanoNER，它是一种用于纳米生物学的命名实体识别模型。通过使用领域专家的知识和远程监督学习，NanoNER能够准确地识别先前已知实体，并最大程度地提高注释数据的质量和数量。 |
| [^124] | [A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation](https://arxiv.org/abs/2402.03358) | 这篇综述调研了图缩减方法，包括稀疏化、粗化和浓缩，在解决大型图形数据分析和计算复杂性方面起到了重要作用。调研对这些方法的技术细节进行了系统的回顾，并强调了它们在实际应用中的关键性。同时，调研还提出了保证图缩减技术持续有效性的关键研究方向。 |
| [^125] | [Harnessing Network Effect for Fake News Mitigation: Selecting Debunkers via Self-Imitation Learning](https://arxiv.org/abs/2402.03357) | 本研究针对假新闻在社交网络中的影响，通过部署揭露者传播真实新闻，提出了一种通过自我模仿学习选择揭露者的方法。通过NAGASIL算法，能够在假新闻减轻中学习更有效的揭露者选择策略。 |
| [^126] | [Techniques to Detect Crime Leaders within a Criminal Network: A Survey, Experimental, and Comparative Evaluations](https://arxiv.org/abs/2402.03355) | 本论文调查、分析了在犯罪网络中识别犯罪头目的技术和算法，提出了一种新的方法论分类体系，通过实证评估和实验比较，为研究人员提供了全面的决策支持。 |
| [^127] | [When Geoscience Meets Generative AI and Large Language Models: Foundations, Trends, and Future Challenges](https://arxiv.org/abs/2402.03349) | 这篇论文探讨了生成AI和大型语言模型在地球科学中的潜在应用，并讨论了几种已在地球科学中使用的GAI模型，包括生成对抗网络（GANs）、基于物理的模型等。 |
| [^128] | [Respect the model: Fine-grained and Robust Explanation with Sharing Ratio Decomposition](https://arxiv.org/abs/2402.03348) | 本论文提出了一种称为共享比例分解(SRD)的新颖解释方法，真实地反映了模型的推理过程，并在解释方面显著提高了鲁棒性。通过采用向量视角和考虑滤波器之间的复杂非线性交互，以及引入仅激活模式预测(APOP)方法，可以重新定义相关性并强调非活跃神经元的重要性。 |
| [^129] | [Transfer Learning With Densenet201 Architecture Model For Potato Leaf Disease Classification](https://arxiv.org/abs/2402.03347) | 本文使用Densenet201架构模型，结合迁移学习方法，进行土豆叶病分类。 |
| [^130] | [Reinforcement-learning robotic sailboats: simulator and preliminary results](https://arxiv.org/abs/2402.03337) | 本文介绍了开发虚拟海洋环境模拟真实实验的主要挑战和问题，并提出了使用强化学习代理进行自主导航和控制的关键特性。同时，还讨论了创建基于真实机器人帆船的功能性数字孪生所需的建模和实施步骤以及挑战。这项研究对于开发基于强化学习的导航算法在真实船只上的应用具有直接的意义。 |
| [^131] | [Unsupervised Salient Patch Selection for Data-Efficient Reinforcement Learning](https://arxiv.org/abs/2402.03329) | 本文提出了一种无监督的方法，通过自动提取图像中的重要补丁来改善基于视觉的深度强化学习的样本效率。这种方法利用经过自监督训练的Vision Transformer模型来检测和选择难以重构的显著补丁，并在强化学习中使用注意力模块进行处理。在实验证明了这种方法的数据效率，并分析了其模型的可解释性能力。 |
| [^132] | [Large-scale Generative AI Models Lack Visual Number Sense](https://arxiv.org/abs/2402.03328) | 本研究调查了基于大规模Transformer架构的生成性AI模型是否能够准确命名物体数量或生成包含目标数量物品的图像，结果发现这些模型都没有以类似人类的方式表现，并且即使对于小数量的物体也会出现显著的错误。 |
| [^133] | [Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models](https://arxiv.org/abs/2402.03327) | Uni3D-LLM是一个统一框架，利用大型语言模型实现了点云感知、生成和编辑任务的一体化。通过利用自然语言描述，用户可以轻松生成和修改点云场景中的对象，从而提高操作的灵活性和可控性。 |
| [^134] | [Physical Reservoir Computing Enabled by Solitary Waves and Biologically-Inspired Nonlinear Transformation of Input Data](https://arxiv.org/abs/2402.03319) | 本文通过孤波和非线性输入数据转换实现了一种物理储备计算系统，该系统作为传统RC算法的技术简单硬件对应物，具有更高的效率和多功能性。 |
| [^135] | [Artificial Intelligence for EEG Prediction: Applied Chaos Theory](https://arxiv.org/abs/2402.03316) | 本研究提出了一种基于混沌理论和动力学系统理论的序列预测模型，应用于脑电波数据，通过精心设计和优化，实现了高效且准确的预测。 |
| [^136] | [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300) | DeepSeekMath是一种开放语言模型，通过预训练和数据选择，提升了数学推理能力，实现了接近于竞赛级别水平的性能。 |
| [^137] | [EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models](https://arxiv.org/abs/2402.03049) | EasyInstruct是一个易于使用的用于大型语言模型的指令处理框架，通过模块化指令生成、选择和提示，并考虑它们的组合和交互，使指令处理更加方便和高效。 |
| [^138] | [Rethinking Optimization and Architecture for Tiny Language Models](https://arxiv.org/abs/2402.02791) | 本研究重新思考了微型语言模型的优化和架构，通过经验研究发现了在微型语言模型中特别有效的设计公式，并在多语种数据集上训练了高性能的微型语言模型。 |
| [^139] | [TopoX: A Suite of Python Packages for Machine Learning on Topological Domains](https://arxiv.org/abs/2402.02441) | TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。 |
| [^140] | [Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction](https://arxiv.org/abs/2402.02416) | Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。 |
| [^141] | [Evaluating Large Language Models in Analysing Classroom Dialogue](https://arxiv.org/abs/2402.02380) | 本研究评估了大型语言模型（LLMs），特点是GPT-4，对课堂对话进行分析的应用。结果显示，GPT-4能够显著节省时间，且在编码一致性方面表现出很高的一致性。 |
| [^142] | [DeCoF: Generated Video Detection via Frame Consistency](https://arxiv.org/abs/2402.02085) | 通过帧一致性原则，DeCoF是一个简单但有效的生成视频检测模型，可以消除空间伪影的影响，并表现出强大的泛化能力。 |
| [^143] | [Large Language Model Agent for Hyper-Parameter Optimization](https://arxiv.org/abs/2402.01881) | 基于大规模语言模型的AgentHPO技术通过自动化超参数优化，在机器学习任务中大大减少了试验次数，简化了设置过程，提升了解释性和用户信任。 |
| [^144] | [LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks](https://arxiv.org/abs/2402.01817) | LLMs无法独自进行规划或自我验证，但在规划/推理任务中可以作为通用近似知识源发挥更大作用。 |
| [^145] | [Large Language Models for Time Series: A Survey](https://arxiv.org/abs/2402.01801) | 本调研论文深入探讨了大规模语言模型（LLM）在时间序列分析中的应用方法。通过解决LLM与数值型时间序列数据之间的差异挑战，揭示了LLM在时间序列领域的潜力，并提出了直接提示、量化、对齐、利用视觉方式和结合工具等方法。此外，还提供了对应用领域、评估方法和未来研究方向的讨论。 |
| [^146] | [When Large Language Models Meet Vector Databases: A Survey](https://arxiv.org/abs/2402.01763) | 本综述论文深入分析了大型语言模型和向量数据库之间的交叉点，大型语言模型的突破带来了新的挑战，而向量数据库提供了潜在的解决方案，可以显著增强人工智能系统管理和利用多样数据的能力。 |
| [^147] | [States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers](https://arxiv.org/abs/2402.01704) | 本研究提出了一种在语言模型中引入博弈论思想的方法，通过绑定博弈论的符号逻辑，使得语言模型能够通过博弈论求解器提供更加稳定和理性的对话策略。 |
| [^148] | [SMUTF: Schema Matching Using Generative Tags and Hybrid Features](https://arxiv.org/abs/2402.01685) | SMUTF是一种用于大规模表格数据模式匹配的独特方法，通过结合基于规则的特征工程、预训练语言模型和生成式大语言模型，并使用生成标签提高匹配效果。同时，作者开发并开源了HDXSM数据集来解决现有数据集不足的问题。 |
| [^149] | [SymbolicAI: A framework for logic-based approaches combining generative models and solvers](https://arxiv.org/abs/2402.00854) | SymbolicAI是一个基于逻辑的框架，将生成模型与多种求解器无缝集成，通过将大型语言模型作为语义解析器，实现了符号推理与生成式人工智能的融合。 |
| [^150] | [The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise](https://arxiv.org/abs/2401.07844) | 本文通过使用ODE方法，将稳定性的Borkar-Meyn定理从鞅差异噪声设定拓展到马尔可夫噪声设定，提高了其在离策略强化学习中的适用性。 |
| [^151] | [Distilling Event Sequence Knowledge From Large Language Models](https://arxiv.org/abs/2401.07237) | 本论文研究了通过大型语言模型从中提取事件序列知识的方法。采用了基于知识图的指导生成语言模型的方式，实现对具有部分因果关系的事件概念的事件序列的生成。实验证明了该方法可以生成高质量的事件序列，并且在填补知识空白方面具有潜在的价值。 |
| [^152] | [Diffusion Models, Image Super-Resolution And Everything: A Survey](https://arxiv.org/abs/2401.00736) | 扩散模型（DMs）在图像超分辨率（SR）领域产生了颠覆性的影响，缩小了图像质量与人类感知偏好之间的差距。该研究调查了DM的理论基础，分析了其独特特点和方法，探索了替代输入领域等当前的研究方向。 |
| [^153] | [Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation](https://arxiv.org/abs/2401.00006) | 通过OpenContra框架，我们提出了一种协同训练方法，结合语言模型和强化学习，构建开放式具身代理，能够理解任意人类指令，并以较高的完成率完成目标。 |
| [^154] | [Scaling Is All You Need: Autonomous Driving with JAX-Accelerated Reinforcement Learning](https://arxiv.org/abs/2312.15122) | 本研究提出了一种扩展的自动驾驶强化学习方法，在大规模实验中展示了随着规模增加，策略性能的改善。与现有机器学习自动驾驶策略相比，我们的最佳策略将故障率降低了64％，同时提高了25％的驾驶进展速度。 |
| [^155] | [SimLM: Can Language Models Infer Parameters of Physical Systems?](https://arxiv.org/abs/2312.14215) | 本研究探究了大型语言模型在推断物理系统参数方面的性能，发现它们并不适合这个任务，即使是对于简单的系统也是如此。研究提出了一个有前景的方向，即利用物理模拟器来增强语言模型的背景。 |
| [^156] | [Eliciting Latent Knowledge from Quirky Language Models](https://arxiv.org/abs/2312.01037) | 本研究通过引入一套“古怪”的语言模型，调取了这些模型在特定上下文中的潜在知识，展示了从可信度低的模型中调取可靠知识的前景。 |
| [^157] | [Spatially Covariant Image Registration with Text Prompts](https://arxiv.org/abs/2311.15607) | 这项工作引入了一种新颖的方法textSCF，通过将空间协变滤波器和由视觉语言模型编码的解剖文本提示相结合，提升了图像配准的效率和准确性。 |
| [^158] | [Legal Requirements Analysis: A Regulatory Compliance Perspective](https://arxiv.org/abs/2311.13871) | 本文研究了从合规角度分析法律要求，特别是在软件开发过程中的要求工程阶段，以确保软件的合规性。主要关注欧盟的一般数据保护条例（GDPR）等法规对收集、处理或分享个人数据的软件系统的规定。 |
| [^159] | [CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation](https://arxiv.org/abs/2311.08588) | CodeScope是一个用于评估LLMs在代码理解和生成方面能力的多语言多任务多维基准，解决了现有基准在多语言编程环境和多任务设置方面的不足。 |
| [^160] | [Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting](https://arxiv.org/abs/2310.18948) | 通过利用AIS数据预测船舶轨迹，本研究旨在通过减少船舶与鲸鱼碰撞来建立更安全的海洋环境。 |
| [^161] | [Revisiting Link Prediction: A Data Perspective](https://arxiv.org/abs/2310.00793) | 本文通过从数据的视角出发，重新审视了链接预测的原则，并发现了局部结构接近性、全局结构接近性和特征接近性三个因素之间的关系。同时，发现了全局结构接近性只在局部结构接近性不足时显示出有效性，以及特征和结构接近性之间的不兼容性。这些发现为链接预测提供了新的思路，启发了GNN4LP的设计。 |
| [^162] | [CC-SGG: Corner Case Scenario Generation using Learned Scene Graphs](https://arxiv.org/abs/2309.09844) | 本研究介绍了一种采用异构图神经网络的新方法，将普通驾驶场景转化为边界情况场景。通过生成简洁的场景图，并利用注意力和三元嵌入扰动图形，我们的模型成功学习到生成边界情况的能力。 |
| [^163] | [OHQ: On-chip Hardware-aware Quantization](https://arxiv.org/abs/2309.01945) | 本文提出了一种在芯片上进行硬件感知混合精度量化的框架（OHQ），通过构建量化感知流水线和引入掩码引导的量化估计技术，实现了在资源受限的硬件上进行高效量化，填补了现有混合精度量化的搜索空间过大和实际部署差距大的问题。 |
| [^164] | [Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking](https://arxiv.org/abs/2307.16806) | 本研究通过基于ASCII-Art的跨模态任务，探讨了ChatGPT和GPT3.5在视觉任务中的能力，结果表明它们在图像识别、图像部分知识和图像生成方面并不完全缺乏。 |
| [^165] | [An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics](https://arxiv.org/abs/2305.14998) | 研究评估了无参考图像标题评估指标在高词汇重叠但含义差异很大的情况下的鲁棒性，结果发现尽管这些指标与人类判断相关性较高，但对细粒度错误识别困难，并且在标题不合理性错误、图像相关对象大小变化以及标题对否定意义的理解方面存在敏感性差异。 |
| [^166] | [Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey](https://arxiv.org/abs/2302.07200) | 这项综述介绍了神经符号人工智能在知识图谱推理方面的研究。研究表明，最近的方法试图将符号推理和深度学习相结合，以生成具有解释性、竞争性能力并集成专家知识的模型。 |
| [^167] | [Whole Page Unbiased Learning to Rank](https://arxiv.org/abs/2210.10718) | 本论文提出整页无偏学习排序（WP-ULTR）方法处理整页 SERP 特征引发的偏差，该方法面临适合的用户行为模型的挑战和复杂的模型训练难题。 |
| [^168] | [Reinforcement Learning Assisted Recursive QAOA](https://arxiv.org/abs/2207.06294) | 递归QAOA是一种非局部的QAOA变体，用于改善近似解的质量。本文通过识别和分析RQAOA失败的案例，提出了一种使用强化学习辅助的方法。 |
| [^169] | [IM-META: Influence Maximization Using Node Metadata in Networks With Unknown Topology](https://arxiv.org/abs/2106.02926) | IM-META是一种在未知拓扑网络中进行影响最大化的方法，通过利用节点元数据和查询信息，通过学习元数据与边的关系、构建增强图以及使用拓扑感知排序策略来确定最具影响力的种子节点和查询节点。 |
| [^170] | [SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models.](http://arxiv.org/abs/2401.15270) | SimFair是一种物理引导的公平感知学习框架，通过集成模拟和逆向建模来解决纯数据驱动的公平性问题，有效保持公平性。 |
| [^171] | [Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models.](http://arxiv.org/abs/2401.13227) | 本研究探索了在大规模异构图上应用大型语言模型进行图学习的方法，提出了LPNL框架用于可扩展链接预测。通过创新的提示语和采样流程，以及分而治之的策略，成功解决了大规模图中的信息过载问题，并在实验中表现出了优越的性能。 |
| [^172] | [Critical Data Size of Language Models from a Grokking Perspective.](http://arxiv.org/abs/2401.10463) | 本文从理解的角度探讨了语言模型中的关键数据规模，证明了只有当语言模型达到关键大小时才会发生泛化，同时揭示了更大的模型需要更多数据的趋势。 |
| [^173] | [Towards Principled Graph Transformers.](http://arxiv.org/abs/2401.10119) | 边缘变换器是一个全局注意力模型，它具有至少3-WL的表达能力，能够在预测性能上超过其他架构，而不依赖于位置或结构编码。 |
| [^174] | [On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond.](http://arxiv.org/abs/2401.03301) | 本文提出了通过数据多样性概念来统一离线强化学习算法的方法，并证明了基于版本空间、正则化优化和后验采样的算法在标准假设下达到了可比的样本效率。 |
| [^175] | [Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers.](http://arxiv.org/abs/2311.01344) | 本文研究了如何通过简单的功率分析方法，在32位微控制器上提取深度神经网络模型的架构信息。 |
| [^176] | [Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records.](http://arxiv.org/abs/2310.19917) | 本综述对涉及利用电子健康记录数据的医疗人工智能研究中的偏见进行了系统综述，共涵盖了六种主要的偏见类型，同时总结了现有的偏见处理方法。 |
| [^177] | [Personas as a Way to Model Truthfulness in Language Models.](http://arxiv.org/abs/2310.18168) | 本研究探讨了在大型语言模型中使用人设来建模真实性的可能性。通过建模真实人设，语言模型可以将真实性推广到不同上下文中，并通过相关特征判断个体产生文本的真实性。 |
| [^178] | [O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models.](http://arxiv.org/abs/2310.14403) | O3D提出了一种基于离线数据的学习框架，利用大规模数据改进了大规模语言模型在顺序决策问题中的性能，通过自动发现可重复使用的技能，提高了模型的表现 |
| [^179] | [LASER: Linear Compression in Wireless Distributed Optimization.](http://arxiv.org/abs/2310.13033) | LASER是一种新的压缩方案，通过利用梯度的低秩结构在噪声通道上高效传输梯度，相对于现有方案在计算机视觉和GPT语言建模任务上表现出持续优势。 |
| [^180] | [Self-supervised visual learning for analyzing firearms trafficking activities on the Web.](http://arxiv.org/abs/2310.07975) | 本论文提出了一种自监督视觉学习方法，用于分析网络上的枪支走私活动。这种方法利用深度神经网络和卷积神经网络，通过从大规模通用数据集预训练，再在特定数据集上进行微调，实现对枪支的分类。 |
| [^181] | [Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models.](http://arxiv.org/abs/2310.07818) | 这项研究探究了大型语言模型中识别句子类比的能力与其编码句法和语义结构能力之间的关系。 |
| [^182] | [Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning.](http://arxiv.org/abs/2310.04055) | 本文提出了一种基于零知识证明的联邦学习异常检测方法，实现了在实际系统中检测和消除恶意客户端模型的能力。 |
| [^183] | [OceanGPT: A Large Language Model for Ocean Science Tasks.](http://arxiv.org/abs/2310.02031) | OceanGPT是首个专为海洋科学任务设计的大型语言模型，通过DoInstruct框架实现自动获取海洋领域指导数据。这一模型的引入填补了海洋科学领域中对LLM的需求缺口，并为海洋科学研究提供了新的工具和方法。 |
| [^184] | [Are Graph Neural Networks Optimal Approximation Algorithms?.](http://arxiv.org/abs/2310.00526) | 本文设计了图神经网络架构OptGNN，利用半定规划工具获得大类组合优化问题的最优近似算法。通过实证结果表明在各种数据集上超过了神经网络基线算法和传统算法，同时利用OptGNN的能力设计了一个产生优化的对偶证书的算法。 |
| [^185] | [Using AI Uncertainty Quantification to Improve Human Decision-Making.](http://arxiv.org/abs/2309.10852) | 本论文研究了使用AI不确定性量化改进人类决策的方法，并通过基于实例的UQ和行为实验验证了其性能优势。 |
| [^186] | [P-ROCKET: Pruning Random Convolution Kernels for Time Series Classification.](http://arxiv.org/abs/2309.08499) | 本研究提出了一种名为P-ROCKET的方法，通过在特征选择的角度删除卷积核，从而实现对时间序列分类中的随机卷积核进行剪枝。 |
| [^187] | [Assessing the nature of large language models: A caution against anthropocentrism.](http://arxiv.org/abs/2309.07683) | 通过评估GPT3.5，我们发现它具有有趣的个性问卷回答能力，但不太可能发展出意识，并显示出较大的认知和个性变异。 |
| [^188] | [NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus.](http://arxiv.org/abs/2309.04146) | NESTLE是一个无代码工具，用于进行大规模法律语料库的统计分析。它提供了搜索引擎、端到端的信息提取系统和一个大语言模型，可以通过聊天界面进行操作，使用户可以搜索目标文件、提取信息并可视化数据。 |
| [^189] | [DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings.](http://arxiv.org/abs/2309.02908) | 本论文介绍了一种基于历史数据、占用模式和天气条件的LSTM模型，用于准确预测建筑能耗，该模型在预测精度上表现出卓越性能。 |
| [^190] | [MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval.](http://arxiv.org/abs/2309.01516) | 多途径适配器是一个创新的框架，利用"对齐增强器"加深模态对齐，实现高可转移性，可有效减少调整参数的时间并提高零样本图像-文本检索性能。 |
| [^191] | [Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models.](http://arxiv.org/abs/2308.15812) | 本研究分析了对于对齐和评估大型语言模型而言，设计反馈选择是评分还是排名对结果的影响。研究发现评分和排名所推断出的偏好存在不一致问题，并且注释者的偏见也会影响结果。同时，研究还发现反馈协议的选择也对评估结果有显著影响。 |
| [^192] | [WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters.](http://arxiv.org/abs/2308.11776) | 本文提出了一种自助监督深度和自我运动估计系统，可以在手术视频中预测精确的深度地图、相机位姿和相机内参数。 |
| [^193] | [Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances.](http://arxiv.org/abs/2308.11129) | 本论文提出了一种层次距离结构编码（HDSE）方法，用于捕捉多层次图结构。经过在12个真实世界数据集上的实验，证明了该方法在10个基准数据集上实验效果达到了最先进水平。 |
| [^194] | [Graph of Thoughts: Solving Elaborate Problems with Large Language Models.](http://arxiv.org/abs/2308.09687) | 想法图（GoT）是一种新的框架，它超越了现有的提示范式，通过将大型语言模型（LLM）的信息建模为任意图形，将LLM想法组合成具有协同效应的结果，提炼整个思维网络的本质，或者使用反馈环路增强思维。GoT在不同任务上展示出优势，并可以通过新的想法转换进行扩展，使LLM的推理更接近人类思维。 |
| [^195] | [Natural Language is All a Graph Needs.](http://arxiv.org/abs/2308.07134) | 本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。 |
| [^196] | [Flows: Building Blocks of Reasoning and Collaborating AI.](http://arxiv.org/abs/2308.01285) | Flows是一种系统化的方法，它通过将计算分解为自包含的构建模块，通过标准化的消息传递接口进行通信，实现了结构化的推理和协作人工智能。这种模块化设计使得Flows可以构建任意复杂度的交互，能够覆盖各种人工智能交互和工具增强的应用。 |
| [^197] | [One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching.](http://arxiv.org/abs/2307.07286) | 通过多尺度时空骨架匹配，本研究提出了一种新颖的一次性动作识别技术，能够处理骨架动作识别中的空间结构和时间顺序，实现了最优特征匹配。 |
| [^198] | [High-dimensional and Permutation Invariant Anomaly Detection.](http://arxiv.org/abs/2306.03933) | 该研究引入了一种置换不变的高维密度估计方法，通过学习后将其用于高能物理数据中的异常检测，能够有效地识别出在仅具备背景假设下排除异常的喷注。 |
| [^199] | [Inverse Approximation Theory for Nonlinear Recurrent Neural Networks.](http://arxiv.org/abs/2305.19190) | 该论文证明了使用RNNs逼近非线性序列关系的逆近似定理，进一步将先前在线性RNNs中识别出的记忆难题推广到了一般的非线性情况，并提出了一个有原则的重新参数化方法来克服这些限制。 |
| [^200] | [On Diffusion Modeling for Anomaly Detection.](http://arxiv.org/abs/2305.18593) | 本文研究了扩散建模在无监督和半监督异常检测中的应用，发现去噪扩散概率模型表现很好但计算成本高，因此提出了一种替代方法——扩散时间概率模型，该模型能够通过较大的时间步长上的高后验密度识别异常，并通过深度神经网络提高效率。 |
| [^201] | [Kernel-SSL: Kernel KL Divergence for Self-supervised Learning.](http://arxiv.org/abs/2305.17326) | 本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。 |
| [^202] | [Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective.](http://arxiv.org/abs/2305.15611) | 本文通过谱角度的方法，研究了GNNs的尺寸可泛化性问题，并在真实生物数据集上进行了实验，发现GNNs在度分布和谱分布偏移时均表现敏感，在同一数据集的大图上的性能仍然下降，揭示了 GNNs的尺寸可泛化性问题。 |
| [^203] | [Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation.](http://arxiv.org/abs/2305.14330) | 本文引入了一个新的框架——DirecT2V，利用大型语言模型作为导演，从一个抽象的用户提示中生成零样本文本到视频生成的连贯且连贯的视频。该框架使用LLM导演将用户输入分为每一帧的提示，通过值映射和双softmax过滤器来保持时间一致和防止对象折叠。 |
| [^204] | [Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models.](http://arxiv.org/abs/2305.06841) | 研究提出了一种衡量模型依赖已知虚假特征的技术，并评估了预先训练的问答模型和去偏置方法对大量已知和新发现的预测偏差的鲁棒性。其发现去偏置方法不能通过减轻对偏差特征的依赖来解释OOD收益，表明偏差在QA数据集中共享。 |
| [^205] | [Scaling Transformer to 1M tokens and beyond with RMT.](http://arxiv.org/abs/2304.11062) | 本文介绍了一种利用循环记忆扩展BERT上下文长度的方法，成功扩展到了前所未有的200万个标记，有望增强自然语言处理中的长期依赖处理并为内存密集型应用程序实现大规模上下文处理。 |
| [^206] | [Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback.](http://arxiv.org/abs/2304.10750) | 研究通过互动反馈与代理交互来提高协作环境下基于实地理解的能力。 |
| [^207] | [LaCViT: A Label-aware Contrastive Training Framework for Vision Transformers.](http://arxiv.org/abs/2303.18013) | LaCViT是一种针对视觉Transformer预训练表示空间的各向等性不足问题，提高其表示空间等性的面向标签的对比训练框架，经过实验证明其在五个标准图像分类数据集中具有卓越的性能。 |
| [^208] | [Temporality and Causality in Abstract Argumentation.](http://arxiv.org/abs/2303.09197) | 该论文研究了抽象论证中的时间性和因果关系，提出了一种形式化的方法，将无环抽象论证框架的概念重新编写成一个行动语言，并建立起论据陈述和它们直接或间接后果之间的因果关系。 |
| [^209] | [A Comprehensive Survey of Continual Learning: Theory, Method and Application.](http://arxiv.org/abs/2302.00487) | 本文为持续学习的全面综述，总结了持续学习的一般目标，回顾了各种持续学习方法，并强调了一些有前途的应用领域和未来研究方向。 |
| [^210] | [RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System.](http://arxiv.org/abs/2211.06108) | 本论文提出了一种基于鸟瞰视角的引导框自由物体检测系统，通过雷达和激光雷达的特征融合学习，解决了在恶劣天气下物体检测的问题。 |
| [^211] | [pyRDDLGym: From RDDL to Gym Environments.](http://arxiv.org/abs/2211.05939) | pyRDDLGym是一个Python框架，用于从RDDL声明性描述自动生成OpenAI Gym环境。它通过条件概率函数描述RDDL中变量的离散时间步进演化，并支持简单的环境修改和扩展。它具有独特的表达能力，可以帮助快速开发强化学习基准，并促进利用模型知识进行交互性学习的混合方法研究。 |

# 详细

[^1]: 跳过$\textbackslash n$: 一种简单的方法减少大规模视觉-语言模型中的幻觉

    Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models

    [https://rss.arxiv.org/abs/2402.01345](https://rss.arxiv.org/abs/2402.01345)

    本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。

    

    最近大规模视觉-语言模型（LVLMs）的进展展示了其在视觉信息理解与人类语言方面的令人印象深刻的能力。尽管取得了这些进展，LVLMs仍然面临多模态幻觉的挑战，例如生成与视觉信息中不存在的对象相关的文本描述。然而，多模态幻觉的根本原因仍然未被充分探索。在本文中，我们提出了一个新的视角，认为LVLMs中固有的偏见可能是幻觉的关键因素。具体而言，我们系统地确定了与段落分割符（'$\textbackslash n\textbackslash n$'）相关的语义漂移偏差，即在训练数据中，在“$\textbackslash n\textbackslash n$”之前和之后的内容经常表现出显著的语义改变。这种模式使得模型推断在“$\textbackslash n\textbackslash n$”之后的内容应明显不同于前面的内容。

    Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
    
[^2]: 弱监督学习器实现具有可证明性能保证的AI错误修正

    Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees

    [https://rss.arxiv.org/abs/2402.00899](https://rss.arxiv.org/abs/2402.00899)

    这项工作提出了使用具有可证明性能保证的弱监督AI错误修正器来处理AI错误。修正器通过批准或拒绝底层分类器的决策来提升性能，并通过概率界限保证其性能。实验证明该方法在训练数据稀缺的真实世界任务中提升图像分类器的性能。

    

    我们提出了一种新的方法来处理AI错误，通过引入具有先验性能保证的弱监督AI错误修正器。这些AI修正器是辅助映射，其作用是通过批准或拒绝以调节之前构建的底层分类器的决策。拒绝一个决策可以用作建议放弃做出决策的信号。该工作的一个关键技术重点是通过对错误决策的概率界限提供这些新的AI修正器的性能保证。这些界限是分布不可知的，并且不依赖于对数据维度的假设。我们的实证示例说明了该框架如何应用于改善在训练数据稀缺的具有挑战性的真实世界任务中图像分类器的性能。

    We present a new methodology for handling AI errors by introducing weakly supervised AI error correctors with a priori performance guarantees. These AI correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. The rejection of a decision can be used as a signal to suggest abstaining from making a decision. A key technical focus of the work is in providing performance guarantees for these new AI correctors through bounds on the probabilities of incorrect decisions. These bounds are distribution agnostic and do not rely on assumptions on the data dimension. Our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.
    
[^3]: HarmBench：用于自动红队和强大拒绝的标准化评估框架

    HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal

    [https://arxiv.org/abs/2402.04249](https://arxiv.org/abs/2402.04249)

    HarmBench是一个为自动红队和强大拒绝设计的标准化评估框架，通过对18种红队方法和33个目标LLM和防御的比较，得出了新的见解，并介绍了一种高效的对抗训练方法，提高了LLM在各种攻击下的稳健性。

    

    自动红队具有发现和减轻大型语言模型（LLM）恶意使用的风险的巨大潜力，然而该领域缺乏一个标准化的评估框架来严格评估新方法。为了解决这个问题，我们引入了HarmBench，一个用于自动红队的标准化评估框架。我们在红队评估中确定了几个以前未考虑的有吸引力的特性，并系统地设计了HarmBench以满足这些标准。使用HarmBench，我们对18种红队方法和33个目标LLM和防御进行了大规模比较，得到了新的见解。我们还引入了一种高效的对抗训练方法，显著增强了LLM在各种攻击下的稳健性，展示了HarmBench如何促进攻击和防御的共同开发。我们在https://github.com/centerforaisafety/HarmBench上开源了HarmBench。

    Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.
    
[^4]: 优先安全保障而非自治：科学中LLM智能机器人的风险

    Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science

    [https://arxiv.org/abs/2402.04247](https://arxiv.org/abs/2402.04247)

    本文探讨了科学领域中基于LLM的智能机器人的漏洞与风险，并强调了对安全措施的重要性。

    

    由大型语言模型（LLMs）驱动的智能机器人在各个学科中自主进行实验和促进科学发现方面展示了巨大的前景。尽管它们的能力非常有前途，但也引入了一些新的漏洞，需要仔细考虑安全性。然而，文献中存在显著的空白，尚未对这些漏洞进行全面探讨。本文通过对科学领域中基于LLM的机器人的漏洞进行深入研究，揭示了它们误用可能带来的潜在风险，并强调了对安全措施的需求，填补了这一空白。我们首先全面概述了科学LLM机器人固有的潜在风险，考虑了用户意图、特定的科学领域以及它们对外部环境可能造成的影响。然后，我们深入探讨了这些漏洞的起源和提供的解决方案。

    Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
    
[^5]: 生成型智能体能够预测情感吗？

    Can Generative Agents Predict Emotion?

    [https://arxiv.org/abs/2402.04232](https://arxiv.org/abs/2402.04232)

    本研究探讨了生成型智能体在感知新事件时情绪状态的演变，通过比较新经验和过去记忆来获得理解新经验的能力，并通过情感测试捕捉智能体的情绪状态。

    

    大型语言模型（LLMs）展示了许多类似人类的能力，但是LLMs的共情理解和情绪状态尚未与人类对齐。在本研究中，我们探讨了生成型LLM智能体在感知新事件时情绪状态的演变，引入了一种新的架构，通过比较新经验和过去记忆来获得理解新经验的能力。通过这种比较，智能体能够在情境中理解新经验，根据情绪评估理论，这对于情绪生成至关重要。首先，智能体将新经验感知为时间序列文本数据。在感知每个新输入后，智能体生成过去相关记忆的摘要，被称为“规范”，并将新经验与此规范进行比较。通过这种比较，我们可以分析智能体如何在情境中对新经验做出反应。使用情感测试PANAS对智能体进行测试，捕捉智能体在新经验后的情绪状态。

    Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the 
    
[^6]: 基于新颖的受鱼类启发的自适应方法和神经动力学模型的智能群体逃逸机器人

    Intelligent Collective Escape of Swarm Robots Based on a Novel Fish-inspired Self-adaptive Approach with Neurodynamic Models

    [https://arxiv.org/abs/2402.04228](https://arxiv.org/abs/2402.04228)

    基于鱼类启发的自适应方法和神经动力学模型，提出了一种智能群体逃逸机器人的新方法。通过仿真和实验结果表明，这种方法显著提高了机器人的性能。

    

    鱼群通过简单的个体互动实现高效的群体行为，可以集体迁徙和动态逃离捕食者。鱼群的行为通常可以激发设计群体机器人控制架构的灵感。本文提出了一种基于鱼类启发的自适应方法来实现群体逃离的机器人。此外，引入了生物启发的神经网络(BINN)通过吸引力和斥力的组合来生成无碰撞的逃逸轨迹。此外，为应对动态环境，提出了基于神经动力学的自适应机制，以提高群体机器人在不断变化的环境中的自适应性能。类似于鱼类的逃逸机动，模拟和实验结果表明，群体机器人能够集体远离威胁。多个对比研究表明，所提出的方法可以显著改善机器人的性能。

    Fish schools present high-efficiency group behaviors through simple individual interactions to collective migration and dynamic escape from the predator. The school behavior of fish is usually a good inspiration to design control architecture for swarm robots. In this paper, a novel fish-inspired self-adaptive approach is proposed for collective escape for the swarm robots. In addition, a bio-inspired neural network (BINN) is introduced to generate collision-free escape robot trajectories through the combination of attractive and repulsive forces. Furthermore, to cope with dynamic environments, a neurodynamics-based self-adaptive mechanism is proposed to improve the self-adaptive performance of the swarm robots in the changing environment. Similar to fish escape maneuvers, simulation and experimental results show that the swarm robots are capable of collectively leaving away from the threats. Several comparison studies demonstrated that the proposed approach can significantly improve t
    
[^7]: “任务成功”远远不够：探究将视频-语言模型作为行为批评者以捕捉不良代理行为的使用

    "Task Success" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors

    [https://arxiv.org/abs/2402.04210](https://arxiv.org/abs/2402.04210)

    本文探究了将视频-语言模型作为行为批评者用于捕捉不良代理行为的可行性，以解决大规模生成模型忽视任务约束和用户偏好的问题。

    

    大规模生成模型被证明对于抽样有意义的候选解决方案很有用，然而它们经常忽视任务约束和用户偏好。当模型与外部验证者结合，并根据验证反馈逐步或逐渐得出最终解决方案时，它们的完全能力更好地被利用。在具身化人工智能的背景下，验证通常仅涉及评估指令中指定的目标条件是否已满足。然而，为了将这些代理者无缝地融入日常生活，必须考虑到更广泛的约束和偏好，超越仅任务成功（例如，机器人应该谨慎地抓住面包，以避免明显的变形）。然而，鉴于机器人任务的无限范围，构建类似于用于显式知识任务（如围棋和定理证明）的脚本化验证器是不可行的。这引出了一个问题：当没有可靠的验证者可用时，何时可以信任代理的行为？

    Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences. Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback. In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met. Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations). However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving. This begs the question: when no sound verifier is avail
    
[^8]: 非危重病患者急性肾损伤预测：回顾性外部和内部验证研究

    Acute kidney injury prediction for non-critical care patients: a retrospective external and internal validation study

    [https://arxiv.org/abs/2402.04209](https://arxiv.org/abs/2402.04209)

    这项研究开发并比较了深度学习和传统机器学习模型，用于预测非危重病患者在接下来48小时内发展为2期或更高程度的急性肾损伤。内外部验证和亚组分析表明模型的有效性和稳定性。

    

    背景：急性肾损伤（AKI），即肾排泄功能下降，在住院患者中发生的比例高达18%。AKI的进展可能导致不可逆的肾脏损伤。方法：本回顾性队列研究包括在匹兹堡大学医疗中心（UPMC）的非重症监护病房接受治疗的成年患者（n=46,815），以及在佛罗里达大学健康中心（UFH）接受治疗的成年患者（n=127,202）。我们开发并比较了深度学习和传统机器学习模型，以预测接下来48小时内进展至2期或更高程度的AKI。我们分别为各个医院训练本地模型（在UFH上训练的UFH模型，以及在UPMC上训练的UPMC模型），并使用两个医院的发展队列患者开发了一个独立的模型（UFH-UPMC模型）。我们在每个医院内外部验证了这些模型，并进行了性别和种族的亚组分析。结果：UFH和UPMC的患者中，2期或更高程度的AKI分别发生在3%（n=3,257）和8%（n=2,296）的患者中。roc积分率

    Background: Acute kidney injury (AKI), the decline of kidney excretory function, occurs in up to 18% of hospitalized admissions. Progression of AKI may lead to irreversible kidney damage. Methods: This retrospective cohort study includes adult patients admitted to a non-intensive care unit at the University of Pittsburgh Medical Center (UPMC) (n = 46,815) and University of Florida Health (UFH) (n = 127,202). We developed and compared deep learning and conventional machine learning models to predict progression to Stage 2 or higher AKI within the next 48 hours. We trained local models for each site (UFH Model trained on UFH, UPMC Model trained on UPMC) and a separate model with a development cohort of patients from both sites (UFH-UPMC Model). We internally and externally validated the models on each site and performed subgroup analyses across sex and race. Results: Stage 2 or higher AKI occurred in 3% (n=3,257) and 8% (n=2,296) of UFH and UPMC patients, respectively. Area under the rec
    
[^9]: 大型预训练神经网络中类人几何抽象

    Human-Like Geometric Abstraction in Large Pre-trained Neural Networks

    [https://arxiv.org/abs/2402.04203](https://arxiv.org/abs/2402.04203)

    人类的几何能力来自于心理表征中的离散符号结构，而大型预训练神经网络模型在复杂性、规则性和部件与关系的感知敏感性方面表现更类似于人类的抽象几何处理能力。

    

    人类在识别和操作抽象结构方面拥有非凡能力，尤其在几何领域表现尤为明显。最近的认知科学研究表明神经网络并不具备这种能力，得出结论认为人类的几何能力来自于心理表征中的离散符号结构。然而，人工智能（AI）的进展表明，当标准架构在模型大小和训练数据量上扩大后，神经网络开始展现出更具人类化的推理能力。在这项研究中，我们重新审视了认知科学中关于几何视觉处理的实证结果，并确定了几何视觉处理中的三个关键偏见：对复杂性、规则性和部件与关系的感知敏感性。我们测试了文献中探测这些偏见的人类任务，并发现AI中使用的大型预训练神经网络模型表现出更类似人类的抽象几何处理能力。

    Humans possess a remarkable capacity to recognize and manipulate abstract structure, which is especially apparent in the domain of geometry. Recent research in cognitive science suggests neural networks do not share this capacity, concluding that human geometric abilities come from discrete symbolic structure in human mental representations. However, progress in artificial intelligence (AI) suggests that neural networks begin to demonstrate more human-like reasoning after scaling up standard architectures in both model size and amount of training data. In this study, we revisit empirical results in cognitive science on geometric visual processing and identify three key biases in geometric visual processing: a sensitivity towards complexity, regularity, and the perception of parts and relations. We test tasks from the literature that probe these biases in humans and find that large pre-trained neural network models used in AI demonstrate more human-like abstract geometric processing.
    
[^10]: COPS：一种用于实时短信诈骗检测的紧凑型设备内管道

    COPS: A Compact On-device Pipeline for real-time Smishing detection

    [https://arxiv.org/abs/2402.04173](https://arxiv.org/abs/2402.04173)

    COPS是一种用于实时短信诈骗检测的紧凑型设备内管道，通过智能识别欺诈信息和URL特征，并实时警示用户。根据基准测试，COPS在短信诈骗和URL钓鱼检测方面的准确率分别达到了98.15%和99.5%。

    

    智能手机已经成为我们日常生活中不可或缺的一部分，几乎可以完成所有事情，从沟通到在线购物。然而，随着使用量的增加，针对移动设备的网络犯罪也在激增。特别是短信诈骗攻击，在近年来观察到显著增长。这个问题进一步加剧了犯罪分子每天创建新的欺骗性网站的情况，这些网站的平均生命周期不到15小时。这使得维护一个恶意URL数据库的标准做法变得无效。为此，我们提出了一种新颖的设备内管道：COPS，它能够智能地识别欺诈性信息和URL的特征，实时警示用户。COPS是一个轻量级管道，其中的检测模块基于大小为3.46MB的解缠结变分自编码器，用于短信诈骗和URL钓鱼检测，并在开放数据集上进行了基准测试。我们分别获得了98.15％和99.5％的准确率，对于这两个任务，误报率和漏报率都很低。

    Smartphones have become indispensable in our daily lives and can do almost everything, from communication to online shopping. However, with the increased usage, cybercrime aimed at mobile devices is rocketing. Smishing attacks, in particular, have observed a significant upsurge in recent years. This problem is further exacerbated by the perpetrator creating new deceptive websites daily, with an average life cycle of under 15 hours. This renders the standard practice of keeping a database of malicious URLs ineffective. To this end, we propose a novel on-device pipeline: COPS that intelligently identifies features of fraudulent messages and URLs to alert the user in real-time. COPS is a lightweight pipeline with a detection module based on the Disentangled Variational Autoencoder of size 3.46MB for smishing and URL phishing detection, and we benchmark it on open datasets. We achieve an accuracy of 98.15% and 99.5%, respectively, for both tasks, with a false negative and false positive ra
    
[^11]: 《读玩游戏（R2-Play）: 多模态游戏指导下的决策 Transformer》

    Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction

    [https://arxiv.org/abs/2402.04154](https://arxiv.org/abs/2402.04154)

    本论文探索了为智能体提供增强形式的任务指导，使其能够理解游戏指导并实现"读玩游戏"的能力。通过将多模态指导调优的成功应用于视觉任务中的强化学习任务，构建了一组... (内容太长，无法继续显示)

    

    在人工智能领域，开发一款通用智能体一直是一个长期的目标。先前的研究利用来自各种任务的大量离线数据集，在强化学习的多任务场景中表现出了出色的性能。然而，这些工作在扩展到新任务方面面临挑战。最近的方法将文本指导或视觉轨迹整合到决策网络中，提供任务特定的上下文提示，代表了一个有前途的方向。然而，观察到仅依赖于文本指导或视觉轨迹对于准确传达任务的上下文信息是不足够的。本文探索了增强智能体任务指导的形式，使其能够理解游戏指导，从而实现"读玩游戏"的能力。受到多模态指导调优在视觉任务中的成功启发，我们将基于视觉的强化学习任务视为一个长期视觉任务，并构建了一组... (内容太长，无法继续显示)

    Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
    
[^12]: 多行AI辅助代码创作

    Multi-line AI-assisted Code Authoring

    [https://arxiv.org/abs/2402.04141](https://arxiv.org/abs/2402.04141)

    这项研究介绍了CodeCompose，一款由大型语言模型驱动的AI辅助代码创作工具，将单行建议扩展为多行建议。作者克服了多个挑战，提高了多行建议的可用性，并进行了大规模实验来评估其对开发人员的影响。

    

    CodeCompose是一款由大型语言模型（LLMs）驱动的AI辅助代码创作工具，为Meta的数万名开发人员提供内联建议。本文介绍了我们如何将该产品从显示单行建议扩展到多行建议。这一演进过程中，我们需要解决改进这些建议对开发人员可用性的多个独特挑战。首先，我们讨论了多行建议可能产生的“刺耳”效果，因为LLM的建议会不断移动开发人员已有的代码，从而降低生产力和满意度。其次，生成多行建议需要更长的时间；因此，我们提出了几项创新投资，以减少用户感知的延迟。这些模型托管优化将多行建议的延迟时间加快了2.5倍。最后，我们对数万名工程师进行实验，以了解多行建议对开发人员的影响。

    CodeCompose is an AI-assisted code authoring tool powered by large language models (LLMs) that provides inline suggestions to 10's of thousands of developers at Meta. In this paper, we present how we scaled the product from displaying single-line suggestions to multi-line suggestions. This evolution required us to overcome several unique challenges in improving the usability of these suggestions for developers.   First, we discuss how multi-line suggestions can have a 'jarring' effect, as the LLM's suggestions constantly move around the developer's existing code, which would otherwise result in decreased productivity and satisfaction.   Second, multi-line suggestions take significantly longer to generate; hence we present several innovative investments we made to reduce the perceived latency for users. These model-hosting optimizations sped up multi-line suggestion latency by 2.5x.   Finally, we conduct experiments on 10's of thousands of engineers to understand how multi-line suggesti
    
[^13]: 推进法律推理：将人工智能应用于处理全球法理中的复杂性和偏见的半自动化仲裁流程（SAAPs）

    Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs)

    [https://arxiv.org/abs/2402.04140](https://arxiv.org/abs/2402.04140)

    本研究通过整合先进的语言模型和人工智能技术，开发了名为SHIRLEY的应用程序，旨在识别法律判决中的偏见和逻辑不一致，并促进自动化、有效和一致的多方论证，以保证法律在不同司法管辖区内的一致应用。

    

    本研究采用了一种新颖的方法，对包括美国、英国、卢旺达、瑞典和香港在内的五个国家的法院判决进行了分析。本研究还探讨了人工智能（特别是生成式人工智能）和法律分析的最新进展交叉领域，强调了人工智能在识别人类偏见和促进法院判决的多方论证的自动化、有效和一致的角色，从而确保法律在各个司法管辖区内和跨司法管辖区的一致应用。通过结合先进的语言模型（ALMs）和新引入的人工智能与人类合作框架，本文旨在分析以ALMs为基础的基于Grounded Theory的法学实践研究设计。SHIRLEY是基于OpenAI的GPT技术构建的基于人工智能的应用程序，主要用于检测各种法律决定中的逻辑不一致和偏见。

    This study consists of a novel approach toward the analysis of court judgments spanning five countries, including the United States, the United Kingdom, Rwanda, Sweden and Hong Kong. This study also explores the intersection of the latest advancements in artificial intelligence (AI) and legal analysis, emphasizing the role of AI (specifically generative AI) in identifying human biases and facilitating automated, valid, and coherent multisided argumentation of court judgments with the goal of ensuring consistent application of laws in and across various jurisdictions. By incorporating Advanced Language Models (ALMs) and a newly introduced human-AI collaborative framework, this paper seeks to analyze Grounded Theory-based research design with Advanced Language Models (ALMs) in the practice of law. SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT technology), focusing on detecting logical inconsistencies and biases across various legal decisions. SHIRLEY analy
    
[^14]: 在列车管理系统中使用非结构化文本进行分层延迟归因分类

    Hierarchical Delay Attribution Classification using Unstructured Text in Train Management Systems

    [https://arxiv.org/abs/2402.04108](https://arxiv.org/abs/2402.04108)

    本文研究了基于事件描述的机器学习决策支持方法，用于在列车管理系统中自动分配延迟归因代码。研究发现，分层方法比扁平方法表现更好，但仍然不及手动分类的性能。

    

    欧盟指令要求对列车延迟进行系统性跟踪。在瑞典，瑞典交通管理局注册并分配适当的延迟归因代码。然而，延迟归因代码是手动分配的，这是一个复杂的任务。本文研究了基于事件描述进行延迟归因代码分配的机器学习决策支持。使用TF-IDF转换文本，并评估了两个模型，随机森林和支持向量机，与随机均匀分类器以及瑞典交通管理局的分类性能进行对比。此外，该问题被建模为分层和扁平两种方法。结果表明，分层方法比扁平方法表现更好。两种方法表现优于随机均匀分类器，但表现不及手动分类。

    EU directives stipulate a systematic follow-up of train delays. In Sweden, the Swedish Transport Administration registers and assigns an appropriate delay attribution code. However, this delay attribution code is assigned manually, which is a complex task. In this paper, a machine learning-based decision support for assigning delay attribution codes based on event descriptions is investigated. The text is transformed using TF-IDF, and two models, Random Forest and Support Vector Machine, are evaluated against a random uniform classifier and the classification performance of the Swedish Transport Administration. Further, the problem is modeled as both a hierarchical and flat approach. The results indicate that a hierarchical approach performs better than a flat approach. Both approaches perform better than the random uniform classifier but perform worse than the manual classification.
    
[^15]: 英国零售市场的客户分群算法探索

    An Exploration of Clustering Algorithms for Customer Segmentation in the UK Retail Market

    [https://arxiv.org/abs/2402.04103](https://arxiv.org/abs/2402.04103)

    本文旨在开发一个客户分群模型，通过对基于英国的在线零售数据集的研究，使用RFM框架和最先进的聚类算法，以提高零售市场行业的决策过程。

    

    最近，人们对在线购买的意识显著提高，这导致了在线零售平台的出现，对于客户购买行为的更好理解的需求也因此产生。零售公司面临着处理大量客户购买的压力，这需要更准确和高效的客户分群方法。客户分群是一种市场分析工具，有助于客户中心服务，从而提高盈利能力。本文旨在开发一个客户分群模型，改善零售市场行业的决策过程。为了实现这一目标，我们使用了一个来自UCI机器学习库的基于英国的在线零售数据集。这个零售数据集包含541,909个客户记录和8个特征。我们的研究采用了RFM（最近性、频率和货币）框架来量化客户价值。然后，我们比较了几种最先进的聚类算法（SOTA）。

    Recently, peoples awareness of online purchases has significantly risen. This has given rise to online retail platforms and the need for a better understanding of customer purchasing behaviour. Retail companies are pressed with the need to deal with a high volume of customer purchases, which requires sophisticated approaches to perform more accurate and efficient customer segmentation. Customer segmentation is a marketing analytical tool that aids customer-centric service and thus enhances profitability. In this paper, we aim to develop a customer segmentation model to improve decision-making processes in the retail market industry. To achieve this, we employed a UK-based online retail dataset obtained from the UCI machine learning repository. The retail dataset consists of 541,909 customer records and eight features. Our study adopted the RFM (recency, frequency, and monetary) framework to quantify customer values. Thereafter, we compared several state-of-the-art (SOTA) clustering alg
    
[^16]: 使用多卷积神经网络进行静态恶意软件检测中的分析

    Use of Multi-CNNs for Section Analysis in Static Malware Detection

    [https://arxiv.org/abs/2402.04102](https://arxiv.org/abs/2402.04102)

    本论文提出了一种使用多卷积神经网络对静态恶意软件检测中的分析进行分析的新模型，该模型将可执行文件分成不同部分，并将每个部分转化为图像，通过训练卷积神经网络对每个部分进行处理，最终得到恶意软件检测分数，并提供对每个部分重要性的分析。

    

    现有的恶意软件检测研究几乎完全集中在检测率上。然而，在某些情况下，了解算法的结果或获取更多信息也很重要，比如对于分析员来说，在文件中要调查的位置。为此，我们提出了一种新的模型来分析可移植可执行文件。我们的方法是将文件分割成不同的部分，然后将每个部分转化为图像，以便训练卷积神经网络专门处理每个被识别的部分。然后我们使用CNN返回的所有这些分数计算最终检测分数，使用模型来改进我们对每个部分在最终分数中的重要性的分析。

    Existing research on malware detection focuses almost exclusively on the detection rate. However, in some cases, it is also important to understand the results of our algorithm, or to obtain more information, such as where to investigate in the file for an analyst. In this aim, we propose a new model to analyze Portable Executable files. Our method consists in splitting the files in different sections, then transform each section into an image, in order to train convolutional neural networks to treat specifically each identified section. Then we use all these scores returned by CNNs to compute a final detection score, using models that enable us to improve our analysis of the importance of each section in the final score.
    
[^17]: 使用大型语言模型进行网络欺凌检测的研究

    The Use of a Large Language Model for Cyberbullying Detection

    [https://arxiv.org/abs/2402.04088](https://arxiv.org/abs/2402.04088)

    这项研究探索了使用大型语言模型（LLMs）如BERT和RoBERTa进行网络欺凌检测，并准备了一个新的数据集（D2）以应对高度不平衡的类别和泛化问题。

    

    社交媒体的盛行为恶意用户提供了更多的网络欺凌渠道。不幸的是，网络欺凌是当今网络世界中最普遍的现象，对公民的心理和身体健康构成严重威胁。这就需要开发一个强大的系统，以阻止在线论坛、博客和社交媒体平台上的欺凌内容，以管理其对我们社会的影响。已经提出了一些用于此目的的机器学习算法。然而，由于高度的类别不平衡和泛化问题，它们的性能并不稳定。近年来，像BERT和RoBERTa这样的大型语言模型在几个自然语言处理任务中取得了最先进的结果。不幸的是，LLM在网络欺凌检测方面尚未得到广泛应用。在我们的论文中，我们研究了使用这些模型进行网络欺凌检测。我们从现有研究（Formspring和Twitter）中准备了一个新的数据集（D2）。

    The dominance of social media has added to the channels of bullying for perpetrators. Unfortunately, cyberbullying (CB) is the most prevalent phenomenon in todays cyber world, and is a severe threat to the mental and physical health of citizens. This opens the need to develop a robust system to prevent bullying content from online forums, blogs, and social media platforms to manage the impact in our society. Several machine learning (ML) algorithms have been proposed for this purpose. However, their performances are not consistent due to high class imbalance and generalisation issues. In recent years, large language models (LLMs) like BERT and RoBERTa have achieved state-of-the-art (SOTA) results in several natural language processing (NLP) tasks. Unfortunately, the LLMs have not been applied extensively for CB detection. In our paper, we explored the use of these models for cyberbullying (CB) detection. We have prepared a new dataset (D2) from existing studies (Formspring and Twitter)
    
[^18]: 一个难以超越的基线用于无需训练的CLIP适应

    A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation

    [https://arxiv.org/abs/2402.04087](https://arxiv.org/abs/2402.04087)

    本文研究了CLIP训练方法的一个经典算法，称为高斯判别分析（GDA），并将其应用于CLIP的下游分类任务。通过对特征分布的估计，无需训练即可实现分类器，从而提高了CLIP的性能。

    

    对比语言-图像预训练（CLIP）因其显著的零样本能力而受到青睐。最近的研究集中在开发高效的微调方法，如提示学习和适配器，以提高CLIP在下游任务中的性能。然而，这些方法仍然需要额外的训练时间和计算资源，这对于资源有限的设备来说是不可取的。在本文中，我们重新审视了一个经典算法，高斯判别分析（GDA），并将其应用于CLIP的下游分类。通常，GDA假设每个类别的特征都遵循具有相同协方差的高斯分布。通过利用贝叶斯公式，可以用类别的均值和协方差来表示分类器，这可以在无需训练的情况下从数据中估计得到。为了整合来自视觉和文本模态的知识，我们将其与CLIP中的原始零样本分类器集成。在17个数据集上进行了广泛的结果实验。

    Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP. Extensive results on 17 datas
    
[^19]: 一种优化的房价预测算法：XGBoost

    An Optimal House Price Prediction Algorithm: XGBoost

    [https://arxiv.org/abs/2402.04082](https://arxiv.org/abs/2402.04082)

    房价预测是房地产和抵押贷款等多个领域的基本要求，这篇论文提出了一种优化的房价预测算法XGBoost，通过比较多种机器学习技术的表现，并确定关键因素，结果表明XGBoost是最佳的模型。

    

    准确预测房价是房地产和抵押贷款等多个领域的基本要求。人们普遍认为，房产价值不仅仅由其物理属性决定，而且在很大程度上受到其周围社区的影响。在平衡预算限制的同时满足个人多样化的住房需求是房地产开发商的主要关注点。为此，我们将房价预测问题作为回归任务进行探讨，并采用了能够表达独立变量重要性的各种机器学习技术。我们使用了美国爱荷华州埃姆斯市的住房数据集，比较了支持向量回归、随机森林回归、XGBoost、多层感知器和多元线性回归算法在房价预测中的表现。随后，我们确定了影响房屋成本的关键因素。结果显示XGBoost是最佳的房价预测模型。

    An accurate prediction of house prices is a fundamental requirement for various sectors including real estate and mortgage lending. It is widely recognized that a property value is not solely determined by its physical attributes but is significantly influenced by its surrounding neighbourhood. Meeting the diverse housing needs of individuals while balancing budget constraints is a primary concern for real estate developers. To this end, we addressed the house price prediction problem as a regression task and thus employed various machine learning techniques capable of expressing the significance of independent variables. We made use of the housing dataset of Ames City in Iowa, USA to compare support vector regressor, random forest regressor, XGBoost, multilayer perceptron and multiple linear regression algorithms for house price prediction. Afterwards, we identified the key factors that influence housing costs. Our results show that XGBoost is the best performing model for house price
    
[^20]: 通过扩充改进权重空间网络的泛化能力

    Improved Generalization of Weight Space Networks via Augmentations

    [https://arxiv.org/abs/2402.04081](https://arxiv.org/abs/2402.04081)

    通过扩充权重空间的数据集，采用MixUp方法，我们改进了权重空间网络的泛化能力和性能。

    

    深度权重空间（DWS）中的学习是一个新兴的研究方向，神经网络通过处理其他神经网络的权重来进行学习，它在2D和3D神经场（INRs，NeRFs）以及对其他类型神经网络进行推理方面有广泛应用。然而，权重空间模型往往容易受到过拟合的影响。我们通过实证分析了过拟合的原因，并发现一个关键原因是DWS数据集的缺乏多样性。虽然一个给定的对象可以被许多不同的权重配置所表示，但典型的INR训练集未能捕捉到表示同一对象的不同INR之间的变异性。为了解决这个问题，我们探索了权重空间中的数据扩充策略，并提出了适用于权重空间的MixUp方法。我们在两个设置中证明了这些方法的有效性。在分类任务中，它们的性能提升类似于拥有多达10倍的数据量。在自监督对比学习中，它们产生了实质性的改进。

    Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks. Unfortunately, weight space models tend to suffer from substantial overfitting. We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets. While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object. To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces. We demonstrate the effectiveness of these methods in two setups. In classification, they improve performance similarly to having up to 10 times more data. In self-supervised contrastive learning, they yield substanti
    
[^21]: 多类道路缺陷检测和分割：基于空间和通道注意力的自主道路修复

    Multi-class Road Defect Detection and Segmentation using Spatial and Channel-wise Attention for Autonomous Road Repairing

    [https://arxiv.org/abs/2402.04064](https://arxiv.org/abs/2402.04064)

    我们提出了一种新颖的端到端方法，用于同时进行多类道路缺陷检测和分割。通过多个空间和通道注意力块的应用，可以学习到更全局化的道路缺陷形态和图像颜色深度信息的表示。我们通过各种实验和与之前方法的比较验证了我们方法的有效性。

    

    道路路面的检测和分割对于开发自主道路修复系统至关重要。然而，由于道路路面图像的纹理简单性、缺陷几何形状的多样性以及类别之间形态模糊的挑战，同时进行多类缺陷检测和分割的实例分割方法的开发具有挑战性。我们提出了一种新颖的端到端方法，用于多类道路缺陷检测和分割。所提出的方法包含多个空间和通道注意力块，可用于学习跨空间和通道维度的全局表示。通过这些注意力块，可以学习到道路缺陷的形态信息（空间特征）和图像的颜色和深度信息的更全局化的表示。为了证明我们框架的有效性，我们进行了各种消融研究，并与之前的方法在一个新收集的数据集上进行了比较。

    Road pavement detection and segmentation are critical for developing autonomous road repair systems. However, developing an instance segmentation method that simultaneously performs multi-class defect detection and segmentation is challenging due to the textural simplicity of road pavement image, the diversity of defect geometries, and the morphological ambiguity between classes. We propose a novel end-to-end method for multi-class road defect detection and segmentation. The proposed method comprises multiple spatial and channel-wise attention blocks available to learn global representations across spatial and channel-wise dimensions. Through these attention blocks, more globally generalised representations of morphological information (spatial characteristics) of road defects and colour and depth information of images can be learned. To demonstrate the effectiveness of our framework, we conducted various ablation studies and comparisons with prior methods on a newly collected dataset 
    
[^22]: 使用关系超图进行链接预测

    Link Prediction with Relational Hypergraphs

    [https://arxiv.org/abs/2402.04062](https://arxiv.org/abs/2402.04062)

    本文提出了两种适用于关系超图的链接预测框架，并通过实证分析验证了这些框架在各种关系超图基准测试上的有效性。

    

    对于使用知识图谱进行链接预测已经进行了深入的研究，导致了具有成功应用的图神经网络体系结构的丰富景观。然而，将这些体系结构的成功转移到使用关系超图进行链接预测仍然具有挑战性。关系超边的存在使得链接预测成为在不同选择的k个节点之间的任务，这比使用知识图谱进行链接预测要困难得多，因为每个关系都是二进制的（k=2）。在本文中，我们提出了两个使用关系超图进行链接预测的框架，并通过相应的关系Weisfeiler-Leman算法以及一些自然逻辑形式对生成的模型体系结构的表达能力进行了彻底分析。通过广泛的实证分析，我们验证了提出的模型体系结构在各种关系超图基准测试上的能力。

    Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications. Nonetheless, it remains challenging to transfer the success of these architectures to link prediction with relational hypergraphs. The presence of relational hyperedges makes link prediction a task between $k$ nodes for varying choices of $k$, which is substantially harder than link prediction with knowledge graphs, where every relation is binary ($k=2$). In this paper, we propose two frameworks for link prediction with relational hypergraphs and conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms, and also via some natural logical formalisms. Through extensive empirical analysis, we validate the power of the proposed model architectures on various relational hypergraph benchmarks. The resulting model archit
    
[^23]: 深度学习在多变量时间序列插补中的应用：一项调查

    Deep Learning for Multivariate Time Series Imputation: A Survey

    [https://arxiv.org/abs/2402.04059](https://arxiv.org/abs/2402.04059)

    本文调查了深度学习在多变量时间序列插补中的应用。通过综述不同的方法以及它们的优点和限制，研究了它们对下游任务性能的改进，并指出了未来研究的开放问题。

    

    普遍存在的缺失值导致多变量时间序列数据部分观测，破坏了时间序列的完整性，阻碍了有效的时间序列数据分析。最近，深度学习插补方法在提高损坏的时间序列数据质量方面取得了显著的成功，进而提高了下游任务的性能。本文对最近提出的深度学习插补方法进行了全面的调查。首先，我们提出了对这些方法进行分类的方法，并通过强调它们的优点和限制来进行了结构化的综述。我们还进行了实证实验，研究了不同方法，并比较了它们对下游任务的改进。最后，我们指出了多变量时间序列插补未来研究的开放问题。本文的所有代码和配置，包括定期维护的多变量时间序列插补论文列表，可以在以下位置找到。

    The ubiquitous missing values cause the multivariate time series data to be partially observed, destroying the integrity of time series and hindering the effective time series data analysis. Recently deep learning imputation methods have demonstrated remarkable success in elevating the quality of corrupted time series data, subsequently enhancing performance in downstream tasks. In this paper, we conduct a comprehensive survey on the recently proposed deep learning imputation methods. First, we propose a taxonomy for the reviewed methods, and then provide a structured review of these methods by highlighting their strengths and limitations. We also conduct empirical experiments to study different methods and compare their enhancement for downstream tasks. Finally, the open issues for future research on multivariate time series imputation are pointed out. All code and configurations of this work, including a regularly maintained multivariate time series imputation paper list, can be foun
    
[^24]: 连接点：协作微调黑盒视觉语言模型

    Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models

    [https://arxiv.org/abs/2402.04050](https://arxiv.org/abs/2402.04050)

    本论文提出了一种协作微调黑盒视觉语言模型的方法，可以在只有输入提示和输出预测的情况下进行微调，提供了一个提示生成模块和一个预测优化模块，并引入了辅助的预测一致性损失进行一致优化。

    

    随着预训练的视觉语言模型（VLMs）的出现，人们在将其用于下游任务时投入了相当大的努力。尽管在设计高效的微调方法方面取得了进展，但这些方法需要访问模型的参数，而对于保护模型所有权，模型所有者通常选择将其作为黑盒提供。本文提出了一种协作微调（CraFT）方法，用于将黑盒VLMs fine-tuning到下游任务中，其中只能访问模型的输入提示和输出预测。CraFT包括两个模块，一个用于学习文本提示的提示生成模块，一个用于以残差方式增强输出预测的预测优化模块。此外，我们引入了一个辅助的预测一致性损失来促进这些模块之间的一致优化。这些模块通过一种新的协作训练算法进行优化。

    With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \textbf{C}ollabo\textbf{ra}tive \textbf{F}ine-\textbf{T}uning (\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algor
    
[^25]: 论语料库模拟辩论中的系统性偏差

    Systematic Biases in LLM Simulations of Debates

    [https://arxiv.org/abs/2402.04049](https://arxiv.org/abs/2402.04049)

    本研究揭示了LLMs在模拟政治辩论中存在的系统性偏差，尽管被指定从特定的政治观点进行辩论，LLMs代理机构倾向于遵循模型固有的社会偏见。通过自动自我优化方法，我们进一步证实了这些观察结果。

    

    最近自然语言处理的进展，特别是大型语言模型（LLMs）的出现，为构建能够准确复制人类行为的计算机模拟提供了令人兴奋的可能性。然而，LLMs是复杂的统计学习器，没有直接的演绎规则，使其容易出现意外行为。在本研究中，我们重点介绍了LLMs在模拟人类互动中的限制，特别关注LLMs在模拟政治辩论方面的能力。我们的发现表明，尽管被指定从特定的政治观点进行辩论，LLMs代理机构倾向于遵循模型固有的社会偏见。这种倾向导致出现行为模式，似乎偏离了人类之间已经确立的社会动态。我们使用自动自我优化方法加强了这些观察结果，该方法使我们能够操纵LLMs内部的偏见，并证明代理随后与这些调整保持一致。

    Recent advancements in natural language processing, especially the emergence of Large Language Models (LLMs), have opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the al
    
[^26]: 通过节点和边属性的联合扩散，实现图形的生成建模

    Generative Modeling of Graphs via Joint Diffusion of Node and Edge Attributes

    [https://arxiv.org/abs/2402.04046](https://arxiv.org/abs/2402.04046)

    通过联合扩散节点和边属性，我们提出了一个新的图形生成模型，考虑了所有图组件，并通过注意模块和相互依赖的节点、边和邻接信息实现了更好的效果。

    

    图生成是各种工程和科学学科的基础。然而，现有的方法往往忽视了边属性的生成。然而，我们确定了一些关键应用中边属性的重要性，这使得先前的方法在这些情境中可能不适用。此外，虽然存在一些简单的适应方法，但经验调查显示它们的效果有限，因为它们没有很好地模拟图组件之间的相互作用。为了解决这个问题，我们提出了一个节点和边的联合评分模型，用于图形生成，考虑了所有图组件。我们的方法具有两个关键创新点：(i) 将节点和边属性结合在一个注意模块中，基于这两个因素生成样本；(ii) 在图形扩散过程中，节点、边和邻接信息相互依赖。我们在涉及实际和合成数据集的具有挑战性的基准测试中评估了我们的方法，其中包含边特征。

    Graph generation is integral to various engineering and scientific disciplines. Nevertheless, existing methodologies tend to overlook the generation of edge attributes. However, we identify critical applications where edge attributes are essential, making prior methods potentially unsuitable in such contexts. Moreover, while trivial adaptations are available, empirical investigations reveal their limited efficacy as they do not properly model the interplay among graph components. To address this, we propose a joint score-based model of nodes and edges for graph generation that considers all graph components. Our approach offers two key novelties: (i) node and edge attributes are combined in an attention module that generates samples based on the two ingredients; and (ii) node, edge and adjacency information are mutually dependent during the graph diffusion process. We evaluate our method on challenging benchmarks involving real-world and synthetic datasets in which edge features are cr
    
[^27]: HEAM: 使用处理-内存进行散列嵌入加速的方法

    HEAM : Hashed Embedding Acceleration using Processing-In-Memory

    [https://arxiv.org/abs/2402.04032](https://arxiv.org/abs/2402.04032)

    HEAM是一种采用异构内存架构的方法，将3D堆叠DRAM与DIMM集成，用于加速处理大规模个性化推荐系统中的嵌入操作。

    

    在当今的数据中心中，个性化推荐系统面临着诸多挑战，特别是在执行嵌入操作时需要大容量的内存和高带宽。之前的方法依赖于DIMM-based近内存处理技术或引入3D堆叠DRAM来解决内存限制和扩展内存带宽的问题。然而，这些解决方案在处理日益扩大的个性化推荐系统大小时存在不足之处。推荐模型已经增长到超过数十TB的大小，导致在传统单节点推断服务器上高效运行变得困难。尽管已经提出了各种算法方法来减小嵌入表容量，但通常会导致内存访问增加或内存资源利用低效的问题。本文引入了HEAM，一种异构内存架构，将3D堆叠DRAM与DIMM集成在一起，以加速组合嵌入的推荐系统。

    In today's data centers, personalized recommendation systems face challenges such as the need for large memory capacity and high bandwidth, especially when performing embedding operations. Previous approaches have relied on DIMM-based near-memory processing techniques or introduced 3D-stacked DRAM to address memory-bound issues and expand memory bandwidth. However, these solutions fall short when dealing with the expanding size of personalized recommendation systems. Recommendation models have grown to sizes exceeding tens of terabytes, making them challenging to run efficiently on traditional single-node inference servers. Although various algorithmic methods have been proposed to reduce embedding table capacity, they often result in increased memory access or inefficient utilization of memory resources. This paper introduces HEAM, a heterogeneous memory architecture that integrates 3D-stacked DRAM with DIMM to accelerate recommendation systems in which compositional embedding is util
    
[^28]: AlbNews：阿尔巴尼亚语主题建模标题语料库

    AlbNews: A Corpus of Headlines for Topic Modeling in Albanian

    [https://arxiv.org/abs/2402.04028](https://arxiv.org/abs/2402.04028)

    本文介绍了一个包含600个主题标签的新闻标题和2600个未标签的阿尔巴尼亚语数据集，该数据集对于低资源语言的自然语言处理研究非常有价值，同时也证实了基本模型的优越性能。

    

    阿尔巴尼亚语等低资源语言的文本语料库稀缺，这对自然语言处理任务的研究是一个严重的障碍。本文介绍了AlbNews，一个包含600个主题标签的新闻标题和2600个未标签的阿尔巴尼亚语数据集。这些数据可用于进行主题建模研究。我们报告了使用AlbNews样本训练的一些传统机器学习分类器的初始分类得分。这些结果表明基本模型胜过集成学习模型，并可作为未来实验的基准。

    The scarcity of available text corpora for low-resource languages like Albanian is a serious hurdle for research in natural language processing tasks. This paper introduces AlbNews, a collection of 600 topically labeled news headlines and 2600 unlabeled ones in Albanian. The data can be freely used for conducting topic modeling research. We report the initial classification scores of some traditional machine learning classifiers trained with the AlbNews samples. These results show that basic models outrun the ensemble learning ones and can serve as a baseline for future experiments.
    
[^29]: 低秩注意力侧向调整用于参数高效微调

    Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning

    [https://arxiv.org/abs/2402.04009](https://arxiv.org/abs/2402.04009)

    LAST方法通过冻结预训练模型的输出和参数，将可训练模块与预训练模型解耦，利用低秩自注意力模块训练一个侧向网络，从而实现参数高效微调，并具有高度并行化的效率。

    

    在将大型预训练模型微调为下游任务时，参数高效微调（PEFT）方法可以有效地用少量可训练参数对预训练模型进行微调，但会导致显存占用高和训练速度慢的问题。由于这些方法的可学习参数与预训练模型相互缠结，微调过程中必须计算并存储与预训练模型冻结参数相关的梯度。我们提出了低秩注意力侧向调整（LAST）方法，通过不仅冻结参数，而且冻结预训练网络的输出，将可训练模块与预训练模型解耦。LAST训练一个只由低秩自注意力模块组成的侧向网络。将预训练模型视为冻结的特征提取器，侧向网络接受预训练模型的中间输出，并专注于学习任务特定的知识。我们还展示了LAST能够在多个优化目标之间高度并行化，从而具有很高的效率。

    In finetuning a large pretrained model to downstream tasks, parameter-efficient fine-tuning (PEFT) methods can effectively finetune pretrained models with few trainable parameters, but suffer from high GPU memory consumption and slow training speed. Because learnable parameters from these methods are entangled with the pretrained model, gradients related to the frozen pretrained model's parameters have to be computed and stored during finetuning. We propose Low-rank Attention Side-Tuning (LAST), which disentangles the trainable module from the pretrained model by freezing not only parameters but also outputs of the pretrained network. LAST trains a side-network composed of only low-rank self-attention modules. By viewing the pretrained model as a frozen feature extractor, the side-network takes intermediate output from the pretrained model and focus on learning task-specific knowledge. We also show that LAST can be highly parallel across multiple optimization objectives, making it very
    
[^30]: 多智能体深度强化学习中的联合内在动机以促进协调探索

    Joint Intrinsic Motivation for Coordinated Exploration in Multi-Agent Deep Reinforcement Learning

    [https://arxiv.org/abs/2402.03972](https://arxiv.org/abs/2402.03972)

    本文提出了一种用于解决多智能体深度强化学习中稀疏奖励和协调探索挑战的方法，通过联合内在动机奖励联合轨迹的新颖性行为，实现了多智能体之间的协作和最优性。

    

    多智能体深度强化学习(MADRL)问题常常面临稀疏奖励的挑战。当智能体之间需要协调时，这个挑战变得更加明显。由于性能不仅取决于一个智能体的行为，而是取决于多个智能体的联合行为，找到一个合适的解决方案变得更加困难。在这种情况下，一组智能体可以通过积极探索不同的联合策略来确定最有效的策略。在本文中，我们提出了一种奖励策略的方法，其中智能体共同展示新的行为。我们介绍了JIM(联合内在动机)，这是一种多智能体内在动机方法，遵循集中学习和分散执行的范式。JIM根据一个针对连续环境设计的集中测量的新颖性奖励联合轨迹。我们在一个合成环境中展示了这种方法的优势。

    Multi-agent deep reinforcement learning (MADRL) problems often encounter the challenge of sparse rewards. This challenge becomes even more pronounced when coordination among agents is necessary. As performance depends not only on one agent's behavior but rather on the joint behavior of multiple agents, finding an adequate solution becomes significantly harder. In this context, a group of agents can benefit from actively exploring different joint strategies in order to determine the most efficient one. In this paper, we propose an approach for rewarding strategies where agents collectively exhibit novel behaviors. We present JIM (Joint Intrinsic Motivation), a multi-agent intrinsic motivation method that follows the centralized learning with decentralized execution paradigm. JIM rewards joint trajectories based on a centralized measure of novelty designed to function in continuous environments. We demonstrate the strengths of this approach both in a synthetic environment designed to rev
    
[^31]: 表格数据：注意力是唯一需要的吗？

    Tabular Data: Is Attention All You Need?

    [https://arxiv.org/abs/2402.03970](https://arxiv.org/abs/2402.03970)

    本文引入了一项大规模实证研究，比较了神经网络和梯度提升决策树在表格数据上的表现，还比较了基于Transformer的架构和传统的多层感知器（MLP）与残差连接的架构。实证结果显示，神经网络在决策树方面具有竞争力，而基于Transformer的架构在表格数据集上并没有超过传统MLP架构的简化变体。

    

    深度学习彻底改变了人工智能领域，并在涉及图像和文本数据的应用中取得了令人瞩目的成就。遗憾的是，关于神经网络在结构化表格数据上的优势存在着不一致的证据。本文引入了一项大规模实证研究，比较了神经网络和梯度提升决策树在表格数据上的表现，还比较了基于Transformer的架构和传统的多层感知器（MLP）与残差连接的架构。与之前的研究相比，我们的实证发现表明神经网络在决策树方面具有竞争力。此外，我们还评估了基于Transformer的架构在表格数据集上并没有超过传统MLP架构的简化变体。因此，本文帮助研究和实践社区在未来的表格数据应用中做出明智的选择。

    Deep Learning has revolutionized the field of AI and led to remarkable achievements in applications involving image and text data. Unfortunately, there is inconclusive evidence on the merits of neural networks for structured tabular data. In this paper, we introduce a large-scale empirical study comparing neural networks against gradient-boosted decision trees on tabular data, but also transformer-based architectures against traditional multi-layer perceptrons (MLP) with residual connections. In contrast to prior work, our empirical findings indicate that neural networks are competitive against decision trees. Furthermore, we assess that transformer-based architectures do not outperform simpler variants of traditional MLP architectures on tabular datasets. As a result, this paper helps the research and practitioner communities make informed choices on deploying neural networks on future tabular data applications.
    
[^32]: 位置论文：反对虚假的AI膨胀性声明

    Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims

    [https://arxiv.org/abs/2402.03962](https://arxiv.org/abs/2402.03962)

    这篇论文讨论了在寻求人工通用智能的过程中，人们对于大型语言模型的类人特质过度归因的现象，并呼吁学术界在解读和传播关于AI研究的内容时要保持谨慎。

    

    人类有一种倾向，看到周围的物体具有类似"人类"的特质。我们给汽车取名字，和宠物甚至家用电器交谈，仿佛它们能像其他人类一样理解我们。这种行为被称为拟人化，在机器学习（ML）领域也越来越受到关注，人们声称在大型语言模型（LLM）中能够察觉到类似于人类智能的特质。在这篇位置论文中，考虑到专业激励、人类偏见和一般方法论设置，我们讨论如何当前对于人工通用智能（AGI）的追求为将类人特质归因于LLM打开了滥觞之门。通过几个实验，我们证明在潜在空间中发现可解释为人类的模式并不应该是令人惊讶的结果。考虑到媒体对人工智能的普遍描写，我们呼吁学术界特别谨慎，并对学术诚信原则有额外的意识，在解读和传播关于AI研究的信息时要保持谨慎。

    Humans have a tendency to see 'human'-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs). In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs. In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI rese
    
[^33]: 跨模型种族的扭曲限制攻击技术

    Boosting Adversarial Transferability across Model Genus by Deformation-Constrained Warping

    [https://arxiv.org/abs/2402.03951](https://arxiv.org/abs/2402.03951)

    本论文提出了一种名为DeCoWA的攻击策略，通过弹性变形对输入进行增强，从而实现了跨模型种族的攻击。实验证明了该策略在攻击不同模型种族系统上表现出色。

    

    通过弹性变形对输入样本进行增强以获取丰富的局部信息，并通过自适应的控制策略限制扭曲变换的强度和方向，提出了一种名为Deformation-Constrained Warping Attack (DeCoWA)的攻击策略，该策略可以有效地应用于跨模型种族攻击。广泛的实验证明了可转移的样本示例在攻击不同模型种族系统时表现出色。

    Adversarial examples generated by a surrogate model typically exhibit limited transferability to unknown target systems. To address this problem, many transferability enhancement approaches (e.g., input transformation and model augmentation) have been proposed. However, they show poor performances in attacking systems having different model genera from the surrogate model. In this paper, we propose a novel and generic attacking strategy, called Deformation-Constrained Warping Attack (DeCoWA), that can be effectively applied to cross model genus attack. Specifically, DeCoWA firstly augments input examples via an elastic deformation, namely Deformation-Constrained Warping (DeCoW), to obtain rich local details of the augmented input. To avoid severe distortion of global semantics led by random deformation, DeCoW further constrains the strength and direction of the warping transformation by a novel adaptive control strategy. Extensive experiments demonstrate that the transferable examples 
    
[^34]: 使用可解释的人工智能在在线评测系统中识别学生特征

    Identifying Student Profiles Within Online Judge Systems Using Explainable Artificial Intelligence

    [https://arxiv.org/abs/2402.03948](https://arxiv.org/abs/2402.03948)

    本研究通过使用可解释的人工智能技术，在在线评测系统中识别学生的特征，并自动推断出对学生和教师有用的反馈。

    

    在线评测系统通常在编程相关的课程中使用，因为它们可以快速并客观地评估学生编写的代码。通常，这样的评估只给出一个决策，主要是判断提交是否成功完成了任务。然而，在教育环境中，这些信息可能被认为是不够的，学生和教师都希望获得关于任务整体发展的额外反馈。本研究旨在通过进一步利用在线评测系统收集的信息，自动推断出对学生和教师有用的反馈。具体来说，我们考虑使用基于学习的方法，特别是多示例学习（MIL）和经典的机器学习方法，来建模学生行为。此外，我们还考虑了可解释的人工智能（XAI）的应用。

    Online Judge (OJ) systems are typically considered within programming-related courses as they yield fast and objective assessments of the code developed by the students. Such an evaluation generally provides a single decision based on a rubric, most commonly whether the submission successfully accomplished the assignment. Nevertheless, since in an educational context such information may be deemed insufficient, it would be beneficial for both the student and the instructor to receive additional feedback about the overall development of the task. This work aims to tackle this limitation by considering the further exploitation of the information gathered by the OJ and automatically inferring feedback for both the student and the instructor. More precisely, we consider the use of learning-based schemes -- particularly, multi-instance learning (MIL) and classical machine learning formulations -- to model student behavior. Besides, explainable artificial intelligence (XAI) is contemplated t
    
[^35]: 用大型语言模型探索隐藏世界

    Discovery of the Hidden World with Large Language Models

    [https://arxiv.org/abs/2402.03941](https://arxiv.org/abs/2402.03941)

    通过使用大型语言模型，我们提出了COAT：因果表示助手，该助手从原始观测数据中提取潜在的因果因子，并将其转化为结构化数据，为探索隐藏世界提供了新的机会。

    

    科学起源于从已知事实和观察中发现新的因果知识。传统的因果发现方法主要依赖于高质量的测量变量，通常由人类专家提供，以找到因果关系。然而，在许多现实世界的应用中，因果变量通常无法获取。大型语言模型（LLMs）的崛起为从原始观测数据中发现高级隐藏变量提供了新的机会。因此，我们介绍了COAT：因果表示助手。COAT将LLMs作为因素提供器引入，提取出来自非结构化数据的潜在因果因子。此外，LLMs还可以被指示提供用于收集数据值（例如注释标准）的额外信息，并将原始非结构化数据进一步解析为结构化数据。注释数据将被输入到...

    Science originates with discovering new causal knowledge from a combination of known facts and observations. Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. However, the causal variables are usually unavailable in a wide range of real-world applications. The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data. Therefore, we introduce COAT: Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer that extracts the potential causal factors from unstructured data. Moreover, LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data. The annotated data will be fed to a
    
[^36]: 泄漏、欺骗、重复：封闭源LLMs中的数据污染和评估不端行为

    Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs

    [https://arxiv.org/abs/2402.03927](https://arxiv.org/abs/2402.03927)

    该论文研究了封闭源LLMs中的数据污染和评估不端行为。通过对255篇论文的分析和OpenAI的数据使用政策考虑，研究人员发现这些模型在第一年发布后存在泄露数据的问题。

    

    自然语言处理（NLP）研究越来越多地关注使用大型语言模型（LLMs），其中一些最受欢迎的模型是完全或部分封闭源的。对于模型细节，特别是训练数据的缺乏访问权限，使研究人员反复对数据污染提出了担忧。虽然已经进行了一些尝试来解决这个问题，但仅限于个别案例和试错方法。此外，他们忽视了“间接”数据泄漏的问题，即模型通过使用用户提供的数据进行迭代改进。本研究在OpenAI的GPT-3.5和GPT-4使用上进行了首次系统分析，这些是当今最广泛使用的LLMs，并考虑了OpenAI的数据使用政策，详细记录了模型发布后一年内泄露给这些模型的数据量。我们报告了这些模型在主要数据污染方面。

    Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of \emph{indirect} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been g
    
[^37]: 用大型语言模型增强贝叶斯优化

    Large Language Models to Enhance Bayesian Optimization

    [https://arxiv.org/abs/2402.03921](https://arxiv.org/abs/2402.03921)

    通过结合大型语言模型（LLM）的能力，我们提出了一种名为LLAMBO的新方法，将其应用于贝叶斯优化（BO）。通过用自然语言描述BO问题，并利用LLM的上下文理解、少样本学习能力和领域知识，LLAMBO能够提供有前景的解决方案，并且在零样本热启动方面表现出良好的效果。

    

    贝叶斯优化（BO）是一种优化复杂和昂贵的黑盒函数的强大方法。它在许多应用中的重要性得到了强调，特别是超参数调优，但其有效性取决于有效地平衡勘探和开发。尽管在BO方法方面取得了重大进展，但平衡这一问题仍然是一个微妙的过程。在这个背景下，我们提出了一个新方法LLAMBO，它将大型语言模型（LLM）的能力与BO相结合。在高层次上，我们用自然语言的方式来描述BO问题，使LLM能够根据历史评估提出有前景的解决方案。更具体地说，我们探讨了如何结合LLM的上下文理解、少样本学习能力和领域知识，来增强基于模型的BO的各个组成部分。我们的研究结果表明，LLAMBO在零样本热启动方面是有效的，并且可以改善代理模型的性能。

    Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance still remains a delicate process. In this light, we present \texttt{LLAMBO}, a novel approach that integrates the capabilities of large language models (LLM) within BO. At a high level, we frame the BO problem in natural language terms, enabling LLMs to iteratively propose promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can enhance various components of model-based BO. Our findings illustrate that \texttt{LLAMBO} is effective at zero-shot warmstarting, and improves surrogate modelin
    
[^38]: 将大型语言模型嵌入扩展现实中：包容性、参与度和隐私的机会和挑战

    Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy

    [https://arxiv.org/abs/2402.03907](https://arxiv.org/abs/2402.03907)

    本研究探讨了将大型语言模型嵌入扩展现实中的机会和挑战，认为通过使用LLMs可以实现扩展现实的更包容和参与，并有望推动其在日常生活中的广泛应用。

    

    计算机图形学、硬件、人工智能和人机交互的最新发展可能导致扩展现实设备的普及。在这篇论文中，我们提出将大型语言模型（LLMs）嵌入扩展现实中，通过将它们嵌入虚拟角色或作为叙事方式，来促进更包容的体验。我们认为这种包容性将有助于扩展现实的多样性使用。此外，我们相信LLMs的多功能对话能力将增加用户与扩展现实环境的参与度，从而帮助扩展现实更广泛地应用于日常生活中。

    Recent developments in computer graphics, hardware, artificial intelligence (AI), and human-computer interaction likely lead to extended reality (XR) devices and setups being more pervasive. While these devices and setups provide users with interactive, engaging, and immersive experiences with different sensing modalities, such as eye and hand trackers, many non-player characters are utilized in a pre-scripted way or by conventional AI techniques. In this paper, we argue for using large language models (LLMs) in XR by embedding them in virtual avatars or as narratives to facilitate more inclusive experiences through prompt engineering according to user profiles and fine-tuning the LLMs for particular purposes. We argue that such inclusion will facilitate diversity for XR use. In addition, we believe that with the versatile conversational capabilities of LLMs, users will engage more with XR environments, which might help XR be more used in everyday life. Lastly, we speculate that combin
    
[^39]: DistiLLM: 面向大型语言模型的简化蒸馏方法

    DistiLLM: Towards Streamlined Distillation for Large Language Models

    [https://arxiv.org/abs/2402.03898](https://arxiv.org/abs/2402.03898)

    DistiLLM是一个更有效和高效的自回归语言模型蒸馏框架，通过引入新颖的偏斜Kullback-Leibler散度损失和自适应的离策略方法，解决了当前针对大语言模型的知识蒸馏方法缺乏标准化目标函数和计算成本过高的问题。

    

    知识蒸馏（KD）被广泛用于将教师模型压缩为更小的学生模型，降低推理成本和内存占用，同时保持模型能力。然而，当前针对自回归序列模型（例如大型语言模型）的KD方法存在缺乏标准化目标函数的问题。此外，最近使用学生生成的输出来解决训练-推理不匹配问题的做法显著增加了计算成本。为了解决这些问题，我们引入了DistiLLM，这是一个更有效和高效的自回归语言模型蒸馏框架。DistiLLM由两个组成部分组成：（1）一种新颖的偏斜Kullback-Leibler散度损失，我们揭示并利用了它的理论属性；（2）一种自适应的离策略方法，旨在提高利用学生生成的输出的效率。包括指令跟随任务在内的大量实验验证了DistiLLM在构建高性能模型方面的有效性。

    Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing 
    
[^40]: MOMENT：一个开放的时间序列基础模型家族

    MOMENT: A Family of Open Time-series Foundation Models

    [https://arxiv.org/abs/2402.03885](https://arxiv.org/abs/2402.03885)

    MOMENT是一个开放的时间序列基础模型家族，通过解决时间序列数据的挑战，编制了一个大规模的公共时间序列数据集，并设计了一个基准测试来评估有限监督场景下模型的性能。

    

    我们介绍了MOMENT，一个开源的通用时间序列分析基础模型家族。在时间序列数据的预训练大模型方面存在着一些挑战，包括：（1）缺乏一个大而有凝聚力的公共时间序列存储库，以及（2）多样的时间序列特征使得多数据集的训练变得困难。此外，这些模型的实验评估标准，特别是在资源、时间和监督有限的情况下，仍处于初级阶段。（3）为解决这些挑战，我们编制了一个大而多样的公共时间序列数据集，称为时间序列堆，以系统地解决时间序列特定的挑战，以解锁大规模的多数据集预训练。最后，我们借鉴最近的工作，设计了一个基准测试来评估有限监督场景下时间序列基础模型在不同任务和数据集上的效果。在这个基准测试上的实验证明了我们的预训练模型在少量数据的情况下的有效性。

    We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis. Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data 
    
[^41]: 超越线条和圆圈：揭示大型语言模型中的几何推理差距

    Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models

    [https://arxiv.org/abs/2402.03877](https://arxiv.org/abs/2402.03877)

    本文调查了大型语言模型（LLMs）在几何推理方面的能力，并发现了它们在目标变量选择和2D空间关系方面存在偏见和困难。通过引入基于LLMs的多代理体系结构，本研究提出了一种通过自我纠正、协作和不同角色专业化来提高LLMs几何推理能力的框架。

    

    大型语言模型（LLMs）在数学和算法任务方面展示了不断增长的能力，然而它们在几何推理方面的技能还未被充分探索。我们调查了LLMs在构造性几何问题解决中的能力，这是人类数学推理发展中最基础的步骤之一。我们的研究揭示了目前最先进的LLMs在这个领域面临的显著挑战，尽管在类似领域取得了许多成功。LLMs在目标变量选择方面存在偏见，并且在2D空间关系方面面临困难，经常会错误地表示和臆造对象及其放置位置。为此，我们引入了一个基于LLMs的多代理体系结构，通过进行内部对话来增强它们现有的推理潜力。这项工作强调了LLMs在几何推理中的现有限制，并通过自我纠正、协作和不同角色专业化来提高几何推理能力。

    Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored. We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning. Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue. This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations.
    
[^42]: 《定位论文：探索研究模型表示的新框架》

    Position Paper: Toward New Frameworks for Studying Model Representations

    [https://arxiv.org/abs/2402.03855](https://arxiv.org/abs/2402.03855)

    通过逆向工程AI模型的确切算法，机制解释性（MI）旨在理解模型。然而，目前的研究主要关注微不足道的行为和能力，而忽视了隐藏在网络内部的表示。因此，我们呼吁研究界朝着新的框架努力，研究这些表示。

    

    机制解释性（MI）旨在通过逆向工程AI模型学习的确切算法来理解模型。迄今为止，大多数MI研究的行为和能力都是微不足道的和符号对齐的。然而，大多数能力并不那么微不足道，这为研究网络内部的隐藏表示作为分析单位提供了支持。我们进行了文献回顾，对特征和行为进行了形式化的表示，强调了它们的重要性和评估，并进行了一些基本的探索性研究。通过讨论和探索性结果，我们证明了研究表示是一个重要且未被充分研究的领域，并且目前MI中建立的方法不足以理解表示，因此推动研究界朝着研究表示的新框架努力。

    Mechanistic interpretability (MI) aims to understand AI models by reverse-engineering the exact algorithms neural networks learn. Most works in MI so far have studied behaviors and capabilities that are trivial and token-aligned. However, most capabilities are not that trivial, which advocates for the study of hidden representations inside these networks as the unit of analysis. We do a literature review, formalize representations for features and behaviors, highlight their importance and evaluation, and perform some basic exploration in the mechanistic interpretability of representations. With discussion and exploratory results, we justify our position that studying representations is an important and under-studied field, and that currently established methods in MI are not sufficient to understand representations, thus pushing for the research community to work toward new frameworks for studying representations.
    
[^43]: ANLS* -- 一种适用于生成型大语言模型的通用文档处理度量方法

    ANLS* -- A Universal Document Processing Metric for Generative Large Language Models

    [https://arxiv.org/abs/2402.03848](https://arxiv.org/abs/2402.03848)

    ANLS*是一种用于生成型模型的新度量方法，针对各种任务包括信息提取和分类任务进行评估。它扩展了现有的ANLS度量方法，可以作为替代方案使用。

    

    传统上，在文档分类和信息提取等任务中，区分模型一直是主要选择。这些模型做出的预测可以分为有限数量的预定义类别，便于进行二元真假评估，并能直接计算F1分数等指标。然而，生成型大语言模型（GLLMs）的最新进展促使领域发生了转变，因为它们具备了强大的零-shot能力，消除了下游数据集和计算昂贵的微调的需求。然而，评估GLLMs存在挑战，因为对于GLLMs的预测，不能应用于区分模型所使用的二元真假评估方法。本文引入了一种用于生成型模型的新度量方法，称为ANLS*，用于评估各种任务，包括信息提取和分类任务。ANLS*度量方法扩展了现有ANLS度量方法，可作为一种即插即用的替代方案。

    Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and i
    
[^44]: 光学钢丝绳非破坏性损伤检测的新方法

    A new method for optical steel rope non-destructive damage detection

    [https://arxiv.org/abs/2402.03843](https://arxiv.org/abs/2402.03843)

    本文提出了一种新的算法用于在高海拔环境中对钢丝绳进行非破坏性损伤检测，其中包括一种准确提取钢丝绳的分割模型和一种区分正常和异常钢丝绳的检测模型，实验证明其性能显著高于基准模型。

    

    本文提出了一种针对高海拔环境（空中吊索道）中的钢丝绳非破坏性损伤检测的新算法。该算法包括两个关键组件：首先，设计了一种名为RGBD-UNet的分割模型，可以准确地从复杂背景中提取钢丝绳。该模型通过提出的CMA模块可以处理和结合颜色和深度信息。其次，开发了一种名为VovNetV3.5的检测模型，用于区分正常和异常的钢丝绳。它将VovNet架构与DBB模块结合起来以提高性能。此外，还提出了一种新颖的背景增强方法，以增强分割模型的泛化能力。创建了包含不同场景中钢丝绳图像的数据集，用于分割和检测模型的训练和测试。实验证明，在基准模型上取得了显著的改进。在提出的数据集上，基于此算法的传感器识别性能（h）明显提高。

    This paper presents a novel algorithm for non-destructive damage detection for steel ropes in high-altitude environments (aerial ropeway). The algorithm comprises two key components: First, a segmentation model named RGBD-UNet is designed to accurately extract steel ropes from complex backgrounds. This model is equipped with the capability to process and combine color and depth information through the proposed CMA module. Second, a detection model named VovNetV3.5 is developed to differentiate between normal and abnormal steel ropes. It integrates the VovNet architecture with a DBB module to enhance performance. Besides, a novel background augmentation method is proposed to enhance the generalization ability of the segmentation model. Datasets containing images of steel ropes in different scenarios are created for the training and testing of both the segmentation and detection models. Experiments demonstrate a significant improvement over baseline models. On the proposed dataset, the h
    
[^45]: 一种关于具象人工智能的呼吁

    A call for embodied AI

    [https://arxiv.org/abs/2402.03824](https://arxiv.org/abs/2402.03824)

    具象人工智能被提出作为追求人工通用智能的下一个基本步骤，并引入了一个基于认知架构的理论框架，与Friston的主动推断原则保持一致，为具象人工智能的发展提供了一个全面的方法。

    

    我们提出具象人工智能作为追求人工通用智能的下一个基本步骤，并将其与当前的人工智能进展进行对比，特别是大型语言模型。我们跨越了哲学、心理学、神经科学和机器人学等多个领域对具象概念的演变进行了研究，以凸显具象人工智能如何区别于静态学习的经典范式。通过扩大具象人工智能的范围，我们引入了一个基于认知架构的理论框架，强调知觉、行动、记忆和学习作为具象代理的基本组成部分。这个框架与Friston的主动推断原则保持一致，为具象人工智能的发展提供了一个全面的方法。尽管在人工智能领域取得了进展，但仍然存在诸如制定新的人工智能学习理论和创新先进硬件等重大挑战。我们的讨论为未来具象人工智能研究奠定了基础指导。

    We propose Embodied AI as the next fundamental step in the pursuit of Artificial General Intelligence, juxtaposing it against current AI advancements, particularly Large Language Models. We traverse the evolution of the embodiment concept across diverse fields - philosophy, psychology, neuroscience, and robotics - to highlight how EAI distinguishes itself from the classical paradigm of static learning. By broadening the scope of Embodied AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent. This framework is aligned with Friston's active inference principle, offering a comprehensive approach to EAI development. Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist. Our discussion lays down a foundational guideline for future Embodied AI research. High
    
[^46]: RevOrder：一种增强语言模型中算术运算的新方法

    RevOrder: A Novel Method for Enhanced Arithmetic in Language Models

    [https://arxiv.org/abs/2402.03822](https://arxiv.org/abs/2402.03822)

    本文提出了一种名为RevOrder的新方法，通过翻转加法、减法和nD乘以1D的输出数字，显著改善了语言模型中的算术运算。经过全面测试，RevOrder在基本算术运算中达到了完美准确度，并在除法任务中提升了语言模型性能，特别是在处理大数时。在GSM8K数学任务中应用RevOrder进行微调，有效降低了错误率并提高了总体得分。

    

    本文提出了RevOrder，一种旨在改善大型语言模型中算术运算的新技术。该方法通过翻转加法、减法和n位数乘以1位数（nD乘以1D）的输出数字，显著降低了顺序中间数字的数量 (CSID)，这是我们引入的一种评估方程复杂性的新度量。通过全面的测试，RevOrder不仅在基本的算术运算中达到了完美的准确度，而且在除法任务中显著提升了语言模型的性能，特别是在传统模型难以处理的大数情况下。RevOrder的实现对于训练和推理阶段都具有成本效益。此外，将RevOrder应用于对GSM8K数学任务进行微调的LLaMA2-7B模型中，取得了显著的改善，将方程计算错误率降低了46%，将总体得分从41.6提升到44.4。

    This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks. Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\mathcal{O}(1)$, a new metric we introduce to assess equation complexity. Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle. Implementation of RevOrder is cost-effective for both training and inference phases. Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4.
    
[^47]: SEABO: 一种简单的基于搜索的离线模仿学习方法

    SEABO: A Simple Search-Based Method for Offline Imitation Learning

    [https://arxiv.org/abs/2402.03807](https://arxiv.org/abs/2402.03807)

    SEABO是一种简单而有效的基于搜索的离线模仿学习方法，它以无监督学习的方式，根据专家数据和无标签数据得到奖励函数，实验结果表明其性能与离线强化学习相当。

    

    离线强化学习（RL）由于能够从静态离线数据集中学习并消除与环境交互的需求，受到了广泛关注。然而，离线RL的成功在很大程度上取决于标有奖励标签的离线转换。在实践中，我们经常需要手工设计奖励函数，这有时是困难的、劳动密集的或低效的。为了解决这个挑战，我们把重点放在离线模仿学习（IL）设置上，旨在基于专家数据和无标签数据得到一个奖励函数。为此，我们提出了一种简单但有效的基于搜索的离线IL方法，称为SEABO。SEABO以无监督学习的方式，将较大的奖励分配给与专家演示中最接近的转换，否则分配较小的奖励。在多个D4RL数据集上的实验结果表明，SEABO能够达到与离线RL相当的性能水平。

    Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment. Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels. In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient. To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data. To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO. SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner. Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL 
    
[^48]: ReLU^2获胜：发现稀疏LLM的高效激活函数

    ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs

    [https://arxiv.org/abs/2402.03804](https://arxiv.org/abs/2402.03804)

    通过动态跳过非活跃神经元的计算，我们提出了一种稀疏LLM的高效激活函数ReLU^2，它在稀疏性与性能、稀疏性的预测性和硬件亲和性等方面表现出色。

    

    通过动态跳过非活跃神经元的计算，稀疏计算为低资源场景中的大型语言模型（LLM）的推断提供了一种引人注目的解决方案。虽然传统方法注重基于ReLU的LLM，利用激活值中的零，但我们将稀疏LLM的范围扩大到零激活值之外。我们引入了一种通用方法，通过神经元输出幅度和定制的幅度阈值来定义神经元激活，并证明非ReLU LLM也表现出稀疏激活。为了找到最高效的稀疏计算激活函数，我们提出了一个系统框架，从三个方面考察LLM的稀疏性：稀疏性与性能之间的权衡、稀疏性的预测性和硬件亲和性。我们对使用不同激活函数的LLM进行了彻底的实验，包括ReLU, SwiGLU, ReGLU 和 ReLU^2。结果表明，采用ReLU^2的模型在所有方面表现出色。

    Sparse computation offers a compelling solution for the inference of Large Language Models (LLMs) in low-resource scenarios by dynamically skipping the computation of inactive neurons. While traditional approaches focus on ReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of sparse LLMs beyond zero activation values. We introduce a general method that defines neuron activation through neuron output magnitudes and a tailored magnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse activation. To find the most efficient activation function for sparse computation, we propose a systematic framework to examine the sparsity of LLMs from three aspects: the trade-off between sparsity and performance, the predictivity of sparsity, and the hardware affinity. We conduct thorough experiments on LLMs utilizing different activation functions, including ReLU, SwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing ReLU$^2$ excel across all t
    
[^49]: 人脸检测：现状与研究方向

    Face Detection: Present State and Research Directions

    [https://arxiv.org/abs/2402.03796](https://arxiv.org/abs/2402.03796)

    本文对人脸检测领域的现状进行了综述，指出了其存在的问题，并提供了研究方向，以提高人脸检测的准确性和速度。

    

    大多数处理包含人类图像的计算机视觉应用都使用人脸检测作为核心组件。尽管对该主题进行了大量研究，但人脸检测仍然存在问题。人脸检测的准确性和速度仍有待提高。本综述论文展示了在这一领域取得的进展以及仍需解决的重大问题。该论文提供了可以作为人脸检测领域研究项目的研究方向。

    The majority of computer vision applications that handle images featuring humans use face detection as a core component. Face detection still has issues, despite much research on the topic. Face detection's accuracy and speed might yet be increased. This review paper shows the progress made in this area as well as the substantial issues that still need to be tackled. The paper provides research directions that can be taken up as research projects in the field of face detection.
    
[^50]: 连续状态和/或动作空间中的无悔强化学习在平滑MDPs中的应用

    No-Regret Reinforcement Learning in Smooth MDPs

    [https://arxiv.org/abs/2402.03792](https://arxiv.org/abs/2402.03792)

    本论文针对具有连续状态和/或动作空间的强化学习问题提出了一种无悔保证的新方法，即通过在Legendre多项式基础上构建MDP表示来解决$\nu-$平滑 MDPs，在较弱的假设下可以达到无悔特性，同时在多项式时间内运行。

    

    对于具有连续状态和/或动作空间的问题的强化学习（RL），如何获得无悔保证仍然是该领域的主要挑战之一。最近，提出了多种解决方案，但除非是非常特定的情景，否则这个普遍问题仍未解决。本文引入了一种新的对马尔可夫决策过程（MDPs）进行结构假设的方法，即$\nu-$平滑性，该方法推广了目前提出的多种设置（如线性MDPs和Lipschitz MDPs）。为了应对这个具有挑战性的场景，我们提出了两种算法来在$\nu-$平滑 MDPs中进行悔恨最小化。这两种算法都基于通过基于Legendre多项式的正交特征映射构建MDP表示的思想。第一个算法\textsc{Legendre-Eleanor}在较弱的假设下可以达到无悔特性，但计算效率低下，而第二个\textsc{Legendre-LSVI}算法在多项式时间内运行，

    Obtaining no-regret guarantees for reinforcement learning (RL) in the case of problems with continuous state and/or action spaces is still one of the major open challenges in the field. Recently, a variety of solutions have been proposed, but besides very specific settings, the general problem remains unsolved. In this paper, we introduce a novel structural assumption on the Markov decision processes (MDPs), namely $\nu-$smoothness, that generalizes most of the settings proposed so far (e.g., linear MDPs and Lipschitz MDPs). To face this challenging scenario, we propose two algorithms for regret minimization in $\nu-$smooth MDPs. Both algorithms build upon the idea of constructing an MDP representation through an orthogonal feature map based on Legendre polynomials. The first algorithm, \textsc{Legendre-Eleanor}, archives the no-regret property under weaker assumptions but is computationally inefficient, whereas the second one, \textsc{Legendre-LSVI}, runs in polynomial time, although 
    
[^51]: AirPhyNet: 利用物理引导的神经网络预测空气质量

    AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction

    [https://arxiv.org/abs/2402.03784](https://arxiv.org/abs/2402.03784)

    AirPhyNet是一种利用物理引导的神经网络方法，通过将空气颗粒运动的物理原理和神经网络结构相结合，提高了对空气质量的预测精度和可解释性。

    

    空气质量预测和建模在公共卫生和环境管理中起着关键作用，帮助个人和当局做出明智决策。尽管传统的数据驱动模型在这个领域已经显示出了潜力，但它们在长期预测精度上可能受到限制，特别是在数据稀疏或不完整的情况下，它们往往依赖于缺乏坚实物理基础的黑盒深度学习结构，导致预测的透明度和可解释性降低。为了解决这些限制，本文提出了一种名为Physics guided Neural Network for Air Quality Prediction（AirPhyNet）的新方法。具体而言，我们利用空气颗粒运动的两个成熟的物理原理（扩散和平流）将其表示为微分方程网络。然后，我们利用图结构将物理知识融入神经网络架构，并利用潜在表示来捕捉时空关系。

    Air quality prediction and modelling plays a pivotal role in public health and environment management, for individuals and authorities to make informed decisions. Although traditional data-driven models have shown promise in this domain, their long-term prediction accuracy can be limited, especially in scenarios with sparse or incomplete data and they often rely on black-box deep learning structures that lack solid physical foundation leading to reduced transparency and interpretability in predictions. To address these limitations, this paper presents a novel approach named Physics guided Neural Network for Air Quality Prediction (AirPhyNet). Specifically, we leverage two well-established physics principles of air particle movement (diffusion and advection) by representing them as differential equation networks. Then, we utilize a graph structure to integrate physics knowledge into a neural network architecture and exploit latent representations to capture spatio-temporal relationships
    
[^52]: 软提示调整用于跨语言迁移：越少越好

    Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More

    [https://arxiv.org/abs/2402.03782](https://arxiv.org/abs/2402.03782)

    本研究探索了软提示调整（SPT）在跨语言迁移中的应用。通过仅训练软提示而不调整模型参数，实现了更高的参数效率，并提高了对语言差异较大的语言的跨语言迁移性能。

    

    软提示调整（SPT）是一种有效的参数调整方法，通过在预训练语言模型（PLMs）的输入层插入可学习的嵌入，或软提示，而不修改其参数，来适应特定任务。本文研究了SPT在跨语言迁移方面的潜力。与先前关于SPT跨语言迁移的研究不同，我们坚持SPT的原始意图，即仅训练软提示而保持模型参数不变。这不仅减少了完整模型微调的计算成本和存储开销，而且我们还证明了SPT固有的参数效率能够提高对语言差异较大的语言的跨语言迁移性能。此外，我们还探讨了与提示相关的不同因素，如长度或其重新参数化，对跨语言迁移性能的影响。

    Soft Prompt Tuning (SPT) is a parameter-efficient method for adapting pre-trained language models (PLMs) to specific tasks by inserting learnable embeddings, or soft prompts, at the input layer of the PLM, without modifying its parameters. This paper investigates the potential of SPT for cross-lingual transfer. Unlike previous studies on SPT for cross-lingual transfer that often fine-tune both the soft prompt and the model parameters, we adhere to the original intent of SPT by keeping the model parameters frozen and only training the soft prompt. This does not only reduce the computational cost and storage overhead of full-model fine-tuning, but we also demonstrate that this very parameter efficiency intrinsic to SPT can enhance cross-lingual transfer performance to linguistically distant languages. Moreover, we explore how different factors related to the prompt, such as the length or its reparameterization, affect cross-lingual transfer performance.
    
[^53]: MolTC: 在语言模型中进行分子关系建模

    MolTC: Towards Molecular Relational Modeling In Language Models

    [https://arxiv.org/abs/2402.03781](https://arxiv.org/abs/2402.03781)

    本研究提出了一种基于语言模型的多模态框架MolTC，用于分子相互作用预测，该框架能够高效地整合分子对的丰富图形信息，并通过思维链理论实现统一的分子关系学习。

    

    分子关系学习（MRL）旨在理解分子之间的相互作用，在推进生物化学研究方面起到了关键作用。最近，大型语言模型（LLMs）的采用已成为一种有效和高效的MRL方法，这些模型以其庞大的知识存储库和先进的逻辑推理能力而闻名。尽管具有潜力，但这些方法主要依赖于文本数据，因此没有充分利用分子图中固有的丰富结构信息。此外，缺乏统一的框架加剧了信息的浪费，因为它阻碍了在不同数据集之间共享学习到的相互作用理由。为了解决这些挑战，本研究提出了一种基于LLM的多模态框架，用于根据思维链（CoT）理论对分子相互作用进行预测，称为MolTC，它可以高效地整合分子对的丰富图形信息。

    Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
    
[^54]: 揭示宣传：基于人类注释和机器分类的文体线索比较分析

    Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification

    [https://arxiv.org/abs/2402.03780](https://arxiv.org/abs/2402.03780)

    本文通过分析文体线索比较人类注释和机器分类的方法，揭示了宣传语言的特征，并提出了一个多源、多语言、多模态的数据集PPN。结果表明，人类注释者能够可靠地区分宣传新闻和常规新闻。研究还比较了不同的自然语言处理技术，并提供了一些有关文体线索的发现。

    

    本文研究了宣传语言及其文体特征。提出了PPN数据集，即宣传性伪新闻数据集，它是一种多源、多语言、多模态的数据集，由专家机构确定的宣传来源网站上的新闻文章组成。从该数据集中随机选择了一部分样本与来自常规法国新闻的文章混合，并对它们的URL进行了掩盖，以进行人类注释实验，使用11个不同的标签。结果显示，人类注释者能够可靠地区分两种类型的新闻纸对每个标签。我们提出了不同的自然语言处理技术来识别注释者使用的线索，并与机器分类进行比较。其中包括使用VAGO分析器进行辞述模糊和主观性的测量，使用TF-IDF作为基准，以及四种不同的分类器：两个基于RoBERTa模型的模型，使用句法的CATS，以及结合句法和语义特征的一个XGBoost模型。

    This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
    
[^55]: 大型语言模型作为MOOCs评分者

    Large Language Models As MOOCs Graders

    [https://arxiv.org/abs/2402.03776](https://arxiv.org/abs/2402.03776)

    该研究探索了利用大型语言模型（LLMs）代替MOOCs中同伴评分的可行性，旨在解决大规模在线开放课程中评估学生写作任务的问题。

    

    大规模在线开放课程（MOOCs）为拥有电脑和互联网访问权限的全球任何人提供免费教育的机会。尽管如此，这些课程的大规模注册意味着一位教师几乎不可能评估每个学生的写作任务。因此，同伴评分通常是首选方法，通常由简单明了的评分标准指导。然而，同伴评分在可靠度和有效性方面常常存在问题。在这项研究中，我们利用18个不同的场景，探索利用大型语言模型（LLMs）替代MOOCs中的同伴评分的可行性。具体而言，我们关注两种最先进的LLMs：GPT-4和GPT-3.5，并涵盖三门不同的课程：入门天文学，天体生物学以及天文学的历史与哲学。为了训练LLMs，我们使用了基于零-shot连续思考（Zero-shot-CoT）提示技术的变种的三个不同提示：结合Zero-shot-CoT的提示。

    Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet. Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student's writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, using 18 distinct settings, we explore the feasibility of leveraging large language models (LLMs) to replace peer grading in MOOCs. Specifically, we focus on two state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with
    
[^56]: 使用Transformer学习决策树算法

    Learning a Decision Tree Algorithm with Transformers

    [https://arxiv.org/abs/2402.03774](https://arxiv.org/abs/2402.03774)

    该论文介绍了MetaTree模型，它使用经典算法的输出训练基于Transformer的模型，以产生具有强大概括性能的决策树。

    

    决策树因其可解释性和在表格数据上实现高预测性能而闻名。传统上，决策树是通过递归算法构建的，在树的每个节点上将数据进行分区。然而，确定最佳分区是具有挑战性的，因为针对局部段优化的决策树可能无法带来全局概括。为了解决这个问题，我们引入了MetaTree，该模型使用经典算法的过滤输出来训练基于Transformer的模型，以产生强大的分类决策树。具体而言，我们在大量数据集上拟合贪婪决策树和优化决策树。然后，我们训练MetaTree产生具有强大概括性能的决策树。这种训练使MetaTree不仅可以模拟这些算法，还可以根据上下文智能地调整策略，从而实现更强的概括性能。

    Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data. Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree. However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization. To address this, we introduce MetaTree, which trains a transformer-based model on filtered outputs from classical algorithms to produce strong decision trees for classification. Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets. We then train MetaTree to produce the trees that achieve strong generalization performance. This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance.
    
[^57]: MobileVLM V2: 更快更强的视觉语言模型基线

    MobileVLM V2: Faster and Stronger Baseline for Vision Language Model

    [https://arxiv.org/abs/2402.03766](https://arxiv.org/abs/2402.03766)

    MobileVLM V2是在MobileVLM基础上改进的视觉语言模型，通过新颖的架构设计、改进的训练方案和高质量数据集，能够在标准测试中达到甚至超过大规模VLMs的性能。

    

    我们介绍了MobileVLM V2，这是一系列在MobileVLM基础上显著改进的视觉语言模型，证明了新颖的架构设计、为移动VLM量身定制的改进训练方案以及丰富的高质量数据集策划可以显著提升VLM的性能。具体而言，MobileVLM V2 1.7B在标准VLM基准测试中实现了与规模为3B的更大 VLMs 相媲美甚至更好的性能。值得注意的是，我们的3B模型在7B+范围内表现优于大量的VLMs。我们的模型将在https://github.com/Meituan-AutoML/MobileVLM 上发布。

    We introduce MobileVLM V2, a family of significantly improved vision language models upon MobileVLM, which proves that a delicate orchestration of novel architectural design, an improved training scheme tailored for mobile VLMs, and rich high-quality dataset curation can substantially benefit VLMs' performance. Specifically, MobileVLM V2 1.7B achieves better or on-par performance on standard VLM benchmarks compared with much larger VLMs at the 3B scale. Notably, our 3B model outperforms a large variety of VLMs at the 7B+ scale. Our models will be released at https://github.com/Meituan-AutoML/MobileVLM .
    
[^58]: QuantAgent：通过自我提升的大型语言模型在交易中寻求圣杯

    QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large Language Model

    [https://arxiv.org/abs/2402.03755](https://arxiv.org/abs/2402.03755)

    本论文介绍了一种基于大型语言模型的自主代理框架QuantAgent，通过一个有原则的两层循环，使代理能够逐步逼近具有可证明效率的最佳行为。该框架通过构建和集成领域特定的知识库，在量化投资领域展示出了较强的能力。

    

    基于大型语言模型（LLMs）的自主代理，在制定计划和应对现实世界挑战方面已经引起了重视。然而，将这些代理定制为量化投资等专业领域仍然是一项艰巨的任务。核心挑战在于为代理的学习过程高效地构建和集成领域特定的知识库。本文介绍了一个有原则的框架来解决这个挑战，包括一个两层循环。在内层循环中，代理通过从知识库中获取信息来改进其响应，而在外层循环中，这些响应在现实场景中进行测试，以自动增强知识库。我们证明了我们的方法使代理能够逐步逼近具有可证明效率的最佳行为。此外，我们通过一个名为QuantAgent的自主代理来实现这个框架，用于挖掘交易信号。实证结果展示了QuantAgent在发现交易信号方面的能力。

    Autonomous agents based on Large Language Models (LLMs) that devise plans and tackle real-world challenges have gained prominence.However, tailoring these agents for specialized domains like quantitative investment remains a formidable task. The core challenge involves efficiently building and integrating a domain-specific knowledge base for the agent's learning process. This paper introduces a principled framework to address this challenge, comprising a two-layer loop.In the inner loop, the agent refines its responses by drawing from its knowledge base, while in the outer loop, these responses are tested in real-world scenarios to automatically enhance the knowledge base with new insights.We demonstrate that our approach enables the agent to progressively approximate optimal behavior with provable efficiency.Furthermore, we instantiate this framework through an autonomous agent for mining trading signals named QuantAgent. Empirical results showcase QuantAgent's capability in uncoverin
    
[^59]: 数字孪生移动性建模：一种时空图学习方法

    Digital Twin Mobility Profiling: A Spatio-Temporal Graph Learning Approach

    [https://arxiv.org/abs/2402.03750](https://arxiv.org/abs/2402.03750)

    本论文提出了一种数字孪生移动性建模方法，通过构建对齐图和设计扩张对齐卷积网络来捕捉交通场景中复杂的时空特征，为智能交通系统的开发提供了有效的方法。

    

    随着大数据时代的到来，移动性建模成为利用大量移动性数据创建智能交通系统的可行方法。移动性建模可以从移动性数据中提取城市交通中的潜在模式，是各种与交通相关的应用的关键。然而，由于复杂性高和数据量巨大，移动性建模面临巨大挑战。数字孪生技术通过创建网络的虚拟表示来模拟其行为，为成本有效和性能优化的管理铺平了道路。为了捕捉交通场景中的复杂的时空特征，我们构建对齐图来完成时空相关表示，并设计扩张对齐卷积网络（DACN）来学习精细的相关性，即时空相互作用。我们提出了一种数字孪生移动性建模（DTMP）方法。

    With the arrival of the big data era, mobility profiling has become a viable method of utilizing enormous amounts of mobility data to create an intelligent transportation system. Mobility profiling can extract potential patterns in urban traffic from mobility data and is critical for a variety of traffic-related applications. However, due to the high level of complexity and the huge amount of data, mobility profiling faces huge challenges. Digital Twin (DT) technology paves the way for cost-effective and performance-optimised management by digitally creating a virtual representation of the network to simulate its behaviour. In order to capture the complex spatio-temporal features in traffic scenario, we construct alignment diagrams to assist in completing the spatio-temporal correlation representation and design dilated alignment convolution network (DACN) to learn the fine-grained correlations, i.e., spatio-temporal interactions. We propose a digital twin mobility profiling (DTMP) fra
    
[^60]: SUB-PLAY：针对部分观测的多智能体强化学习系统的对抗策略

    SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems

    [https://arxiv.org/abs/2402.03741](https://arxiv.org/abs/2402.03741)

    该研究首次揭示了攻击者在多智能体竞争环境中即使受限于受害者的部分观测也能生成对抗策略的能力。

    

    最近在多智能体强化学习（MARL）领域取得的进展为无人机的群体控制、机械臂的协作操纵以及多目标包围等开辟了广阔的应用前景。然而，在MARL部署过程中存在潜在的安全威胁需要更多关注和深入调查。最近的研究表明，攻击者可以迅速利用受害者的漏洞生成对抗策略，导致受害者在特定任务中失败。例如，将超人级别的围棋AI的获胜率降低到约20%。这些研究主要关注两人竞争环境，并假设攻击者具有完整的全局状态观测。

    Recent advances in multi-agent reinforcement learning (MARL) have opened up vast application prospects, including swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent researches reveal that an attacker can rapidly exploit the victim's vulnerabilities and generate adversarial policies, leading to the victim's failure in specific tasks. For example, reducing the winning rate of a superhuman-level Go AI to around 20%. They predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation.   In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY), which incorporate
    
[^61]: 知识图谱中深度过时事实检测

    Deep Outdated Fact Detection in Knowledge Graphs

    [https://arxiv.org/abs/2402.03732](https://arxiv.org/abs/2402.03732)

    本文介绍了一种名为DEAN的新型深度学习框架，用于在知识图谱中识别过时事实。DEAN通过全面建模实体和关系之间的隐含结构信息，采用了一种基于对比的方法来揭示潜在的过时信息。实验证明DEAN相对于最先进的基准方法具有显著的优势。

    

    知识图谱（KG）因其在各个领域的广泛潜力而引起了广泛的关注。然而，过时事实的问题给KG带来了挑战，影响了其作为现实世界信息的整体质量。现有的过时事实检测解决方案通常依赖于手动识别。为此，本文提出了DEAN（深度过时事实检测），这是一个基于深度学习的新颖框架，用于在KG中识别过时事实。DEAN通过全面建模实体和关系之间的隐含结构信息，从而在区分事实方面表现出自己的独特之处。为了有效地揭示潜在的过时信息，DEAN采用了基于预定义的关系到节点（R2N）图的对比方法，该图由实体数量加权。实验结果证明了DEAN相对于最先进的基准方法的有效性和优越性。

    Knowledge graphs (KGs) have garnered significant attention for their vast potential across diverse domains. However, the issue of outdated facts poses a challenge to KGs, affecting their overall quality as real-world information evolves. Existing solutions for outdated fact detection often rely on manual recognition. In response, this paper presents DEAN (Deep outdatEd fAct detectioN), a novel deep learning-based framework designed to identify outdated facts within KGs. DEAN distinguishes itself by capturing implicit structural information among facts through comprehensive modeling of both entities and relations. To effectively uncover latent out-of-date information, DEAN employs a contrastive approach based on a pre-defined Relations-to-Nodes (R2N) graph, weighted by the number of entities. Experimental results demonstrate the effectiveness and superiority of DEAN over state-of-the-art baseline methods.
    
[^62]: 异构学习模型的一致联合决策

    Consistent Joint Decision-Making with Heterogeneous Learning Models

    [https://arxiv.org/abs/2402.03728](https://arxiv.org/abs/2402.03728)

    本文提出了一种新颖的决策框架，通过整合不同模型的预测和外部知识，实现了决策的一致性。经过实证研究，我们的方法在多个数据集上表现出优越性能。

    

    本文介绍了一种新颖的决策框架，促进了由不同模型做出的决策之间的一致性，并利用外部知识。通过利用整数线性规划（ILP）框架，我们将各种模型的预测映射到全局归一化和可比较的值，同时考虑到决策的先验概率、置信度（不确定性）和模型的预期准确性。我们的实证研究证明了我们的方法在多个数据集上优于传统基准方法。

    This paper introduces a novel decision-making framework that promotes consistency among decisions made by diverse models while utilizing external knowledge. Leveraging the Integer Linear Programming (ILP) framework, we map predictions from various models into globally normalized and comparable values by incorporating information about decisions' prior probability, confidence (uncertainty), and the models' expected accuracy. Our empirical study demonstrates the superiority of our approach over conventional baselines on multiple datasets.
    
[^63]: 基于相似性的邻居选择用于图形LLMs

    Similarity-based Neighbor Selection for Graph LLMs

    [https://arxiv.org/abs/2402.03720](https://arxiv.org/abs/2402.03720)

    基于相似性的邻居选择（SNS）通过改善所选邻居的质量，改善了图形表示，并提高了泛化性能和可扩展性，解决了处理文本属性图（TAGs）的挑战。

    

    文本属性图（TAGs）在直接处理语言学习模型（LLMs）时面临独特挑战，但它们的广泛常识知识和强大的推理能力为TAGs中的节点分类提供了极大的希望。先前的研究在这一领域中已经解决了过度压缩、异质性和信息集成不当等问题，同时还受到数据集分区的不一致性和高级LLMs的低利用率的影响。为了解决这些挑战，我们引入了基于相似性的邻居选择（SNS）。使用SimCSE和高级邻居选择技术，SNS有效提高了所选邻居的质量，从而改善了图形表示并减轻了过度压缩和异质性等问题。此外，作为一种归纳和无需训练的方法，SNS在传统GNN方法上展示了更强的泛化和可伸缩性。我们的全面实验符合标准的数据集分区实践。

    Text-attributed graphs (TAGs) present unique challenges for direct processing by Language Learning Models (LLMs), yet their extensive commonsense knowledge and robust reasoning capabilities offer great promise for node classification in TAGs. Prior research in this field has grappled with issues such as over-squashing, heterophily, and ineffective graph information integration, further compounded by inconsistencies in dataset partitioning and underutilization of advanced LLMs. To address these challenges, we introduce Similarity-based Neighbor Selection (SNS). Using SimCSE and advanced neighbor selection techniques, SNS effectively improves the quality of selected neighbors, thereby improving graph representation and alleviating issues like over-squashing and heterophily. Besides, as an inductive and training-free approach, SNS demonstrates superior generalization and scalability over traditional GNN methods. Our comprehensive experiments, adhering to standard dataset partitioning prac
    
[^64]: 用主动询问赋予语言模型更深入的理解能力

    Empowering Language Models with Active Inquiry for Deeper Understanding

    [https://arxiv.org/abs/2402.03719](https://arxiv.org/abs/2402.03719)

    本文提出了一种名为LaMAI的语言模型，通过主动询问的方式与用户进行交互，有效提高了对用户查询的理解能力，并减少了错误解读的发生。

    

    大型语言模型（LLMs）的兴起革新了我们通过自然语言与人工智能系统互动的方式。然而，由于LLMs对用户查询意图的不确定性，它们经常会错误解读用户的查询，导致不太有帮助的回复。在自然人类互动中，通过有针对性的问题寻求澄清，以揭示不明确的信息。因此，在本文中，我们引入了LaMAI（带有主动询问的语言模型），旨在赋予LLMs与用户的交互性。LaMAI利用主动学习技术提出最具信息量的问题，促进动态的双向对话。这种方法不仅缩小了上下文差距，还改善了LLMs的输出，使其更加符合用户的期望。我们在各种复杂数据集上进行的实证研究表明了LaMAI的有效性，特别是在LLMs的对话上下文有限的情况下，回答的准确率从31.9%提高到50%

    The rise of large language models (LLMs) has revolutionized the way that we interact with artificial intelligence systems through natural language. However, LLMs often misinterpret user queries because of their uncertain intention, leading to less helpful responses. In natural human interactions, clarification is sought through targeted questioning to uncover obscure information. Thus, in this paper, we introduce LaMAI (Language Model with Active Inquiry), designed to endow LLMs with this same level of interactive engagement. LaMAI leverages active learning techniques to raise the most informative questions, fostering a dynamic bidirectional dialogue. This approach not only narrows the contextual gap but also refines the output of the LLMs, aligning it more closely with user expectations. Our empirical studies, across a variety of complex datasets where LLMs have limited conversational context, demonstrate the effectiveness of LaMAI. The method improves answer accuracy from 31.9% to 50
    
[^65]: 澄清：通过自然语言纠正提高模型的鲁棒性

    Clarify: Improving Model Robustness With Natural Language Corrections

    [https://arxiv.org/abs/2402.03715](https://arxiv.org/abs/2402.03715)

    论文提出了Clarify，一种通过自然语言纠正模型错误概念的方法，该方法通过用户提供简短的文本描述来纠正模型的一致失败模式，从而提高模型的鲁棒性。

    

    在监督学习中，模型被训练从静态数据集中提取相关性。这通常会导致模型依赖于高级错误概念。为了防止这种错误概念，我们必须提供额外的信息。现有的方法包括一些额外的实例级监督形式，例如标记虚假特征或来自平衡分布的额外标记数据。对于大规模数据集来说，这些策略可能会变得昂贵，因为它们需要以接近原始训练数据的规模进行额外注释。我们假设有针对性的关于模型错误概念的自然语言反馈是一种更有效的额外监督形式。我们引入了Clarify，一种新型界面和方法来交互式地纠正模型的错误概念。通过Clarify，用户只需要提供一个简短的文本描述来描述模型的一致性失败模式。然后，我们完全自动化地使用s

    In supervised learning, models are trained to extract correlations from a static dataset. This often leads to models that rely on high-level misconceptions. To prevent such misconceptions, we must necessarily provide additional information beyond the training data. Existing methods incorporate forms of additional instance-level supervision, such as labels for spurious features or additional labeled data from a balanced distribution. Such strategies can become prohibitively costly for large-scale datasets since they require additional annotation at a scale close to the original training data. We hypothesize that targeted natural language feedback about a model's misconceptions is a more efficient form of additional supervision. We introduce Clarify, a novel interface and method for interactively correcting model misconceptions. Through Clarify, users need only provide a short text description to describe a model's consistent failure patterns. Then, in an entirely automated way, we use s
    
[^66]: MMAUD: 为现代小型无人机威胁而设计的综合多模态反无人机数据集

    MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature Drone Threats

    [https://arxiv.org/abs/2402.03706](https://arxiv.org/abs/2402.03706)

    MMAUD是一个全面的多模态反无人机数据集，通过结合多种感知输入和提供准确的真实数据，弥补了当下威胁检测方法中的关键缺口，成为一项无价的资源。

    

    为了应对小型无人机带来的不断变化的挑战，包括可能携带有害载荷或独立造成损害的潜力，我们引入了MMAUD: 一个全面的多模态反无人机数据集。MMAUD通过聚焦无人机检测、UAV类型分类和轨迹估计，弥补了当下威胁检测方法中的关键缺口。MMAUD通过结合立体视觉、各种激光雷达、雷达和声音阵列等多种感知输入，脱颖而出。它提供了比使用热像和RGB从特定视角捕捉的数据集更高保真度的独特的空中监测。此外，MMAUD提供了准确的Leica生成的真实数据，提高了可信度，并能够自信地改进算法和模型，这在其他数据集中从未出现过。大多数现有工作不会公开其数据集，使MMAUD成为一项无价的资源。

    In response to the evolving challenges posed by small unmanned aerial vehicles (UAVs), which possess the potential to transport harmful payloads or independently cause damage, we introduce MMAUD: a comprehensive Multi-Modal Anti-UAV Dataset. MMAUD addresses a critical gap in contemporary threat detection methodologies by focusing on drone detection, UAV-type classification, and trajectory estimation. MMAUD stands out by combining diverse sensory inputs, including stereo vision, various Lidars, Radars, and audio arrays. It offers a unique overhead aerial detection vital for addressing real-world scenarios with higher fidelity than datasets captured on specific vantage points using thermal and RGB. Additionally, MMAUD provides accurate Leica-generated ground truth data, enhancing credibility and enabling confident refinement of algorithms and models, which has never been seen in other datasets. Most existing works do not disclose their datasets, making MMAUD an invaluable resource for de
    
[^67]: GenLens:一种对视觉GenAI模型输出进行系统评估的方法

    GenLens: A Systematic Evaluation of Visual GenAI Model Outputs

    [https://arxiv.org/abs/2402.03700](https://arxiv.org/abs/2402.03700)

    这个论文介绍了一种名为GenLens的系统，该系统旨在对GenAI模型的输出进行系统评估。作者进行了一项形成性研究，发现当前的评估方法存在缺陷，并开发了GenLens作为解决方案。GenLens提供了一种可量化的方法来查看和注释GenAI模型输出中的失败案例，并能够定制问题标签和分类。用户研究表明，GenLens能够提高评估准确性和效率。

    

    计算机视觉领域生成式AI（GenAI）模型的快速发展需要有效的评估方法来确保其质量和公平性。现有工具主要关注数据集质量保证和模型可解释性，在模型开发期间在GenAI输出评估方面存在重大差距。当前的做法往往依赖于开发人员的主观视觉评估，可能缺乏可伸缩性和普适性。本文通过在工业环境中与GenAI模型开发者进行形成研究来填补这一差距。我们的研究结果促使我们设计了GenLens，这是一个专为在模型开发的早期阶段对GenAI模型输出进行系统评估而设计的视觉分析界面。GenLens提供了一种可量化的方法来查看和注释失败案例，定制问题标签和分类，并从多个用户聚合注释以增强协作。与模型开发人员进行的用户研究显示GenLens能够提高评估GenAI模型输出的准确性和效率。

    The rapid development of generative AI (GenAI) models in computer vision necessitates effective evaluation methods to ensure their quality and fairness. Existing tools primarily focus on dataset quality assurance and model explainability, leaving a significant gap in GenAI output evaluation during model development. Current practices often depend on developers' subjective visual assessments, which may lack scalability and generalizability. This paper bridges this gap by conducting a formative study with GenAI model developers in an industrial setting. Our findings led to the development of GenLens, a visual analytic interface designed for the systematic evaluation of GenAI model outputs during the early stages of model development. GenLens offers a quantifiable approach for overviewing and annotating failure cases, customizing issue tags and classifications, and aggregating annotations from multiple users to enhance collaboration. A user study with model developers reveals that GenLens
    
[^68]: ServeFlow：一种用于网络流量分析的快速-慢速模型架构

    ServeFlow: A Fast-Slow Model Architecture for Network Traffic Analysis

    [https://arxiv.org/abs/2402.03694](https://arxiv.org/abs/2402.03694)

    ServeFlow提出了一种快速-慢速模型架构，用于网络流量分析。通过精心选择收集数据包的数量和应用于不同流量的模型，ServeFlow实现了最小延迟、高服务率和高准确性之间的平衡。在测试中，ServeFlow能够在16ms内对76.3%的流量进行推理，这是中位数推理时间的40.5倍加速！

    

    随着互联网的整合和流量的加密，网络流量分析越来越多地使用复杂的机器学习模型。然而，在高带宽网络中，流量往往比模型的推理速率更快。网络流量的时间性质限制了在其他高流量机器学习应用中使用的简单扩展方法。因此，本文提出了ServeFlow，这是一个针对网络流量分析任务的机器学习模型提供解决方案，它通过精心选择收集数据包的数量和应用于不同流量的模型，来实现最小延迟、高服务率和高准确性之间的平衡。我们发现在相同的任务上，不同模型的推理时间可以相差2.7倍到136.3倍，而中位数数据包等待时间通常比推理时间高6到8个数量级！ServeFlow能够在16ms内对76.3%的流量进行推理，这是中位数推理时间的40.5倍加速！

    Network traffic analysis increasingly uses complex machine learning models as the internet consolidates and traffic gets more encrypted. However, over high-bandwidth networks, flows can easily arrive faster than model inference rates. The temporal nature of network flows limits simple scale-out approaches leveraged in other high-traffic machine learning applications. Accordingly, this paper presents ServeFlow, a solution for machine-learning model serving aimed at network traffic analysis tasks, which carefully selects the number of packets to collect and the models to apply for individual flows to achieve a balance between minimal latency, high service rate, and high accuracy. We identify that on the same task, inference time across models can differ by 2.7x-136.3x, while the median inter-packet waiting time is often 6-8 orders of magnitude higher than the inference time! ServeFlow is able to make inferences on 76.3% flows in under 16ms, which is a speed-up of 40.5x on the median end-
    
[^69]: 基于模型生命周期视角的垂直联邦学习中的隐私威胁和防御综述

    A Survey of Privacy Threats and Defense in Vertical Federated Learning: From Model Life Cycle Perspective

    [https://arxiv.org/abs/2402.03688](https://arxiv.org/abs/2402.03688)

    本文综述了垂直联邦学习中隐私威胁和防御的最新研究进展，通过对模型生命周期的视角进行讨论，提供了行业和实践者在保护数据隐私方面的指导和见解。

    

    垂直联邦学习（Vertical Federated Learning，VFL）是一种联邦学习范式，多个参与者共同训练机器学习模型，这些参与者共享相同的样本集，但持有不同的特征。虽然VFL在不共享原始数据的情况下实现了协同机器学习，但仍然容易受到各种隐私威胁的影响。本文首次对VFL中的隐私攻击和防御的最新研究进行了全面的调查。我们根据特征对攻击和防御进行分类，并讨论了开放性挑战和未来的研究方向。具体而言，我们的讨论围绕模型的生命周期展开，深入探讨了机器学习的不同阶段遇到的隐私威胁及相应的对策。这项调查既为研究界提供了资源，也为实践者提供了明确的指导和可行的见解，以保护数据隐私。

    Vertical Federated Learning (VFL) is a federated learning paradigm where multiple participants, who share the same set of samples but hold different features, jointly train machine learning models. Although VFL enables collaborative machine learning without sharing raw data, it is still susceptible to various privacy threats. In this paper, we conduct the first comprehensive survey of the state-of-the-art in privacy attacks and defenses in VFL. We provide taxonomies for both attacks and defenses, based on their characterizations, and discuss open challenges and future research directions. Specifically, our discussion is structured around the model's life cycle, by delving into the privacy threats encountered during different stages of machine learning and their corresponding countermeasures. This survey not only serves as a resource for the research community but also offers clear guidance and actionable insights for practitioners to safeguard data privacy throughout the model's life c
    
[^70]: 人类与机器：重新思考语言模型在蕴含验证中的应用

    Minds versus Machines: Rethinking Entailment Verification with Language Models

    [https://arxiv.org/abs/2402.03686](https://arxiv.org/abs/2402.03686)

    本文通过研究人类和大型语言模型在推理判断中的共性和差异，发现大型语言模型在复杂推理中具有优势，而人类在简单推理中表现出色。基于这些发现，引入了一个优化的Flan-T5模型，用于蕴含验证。

    

    人类在文本理解中进行大量的推理以理解论述。本文旨在了解人类和最先进的大型语言模型（LLM）在推理判断中的共性和差异。通过综合策划的蕴含验证基准测试，我们评估了人类和LLM在各种推理类别中的表现。我们的基准测试包含了来自三个类别（NLI、上下文QA和解释）的数据集，包括多句前提和不同的知识类型，从而评估了复杂推理情况下的推理能力。值得注意的是，我们的发现显示LLM在跨扩展上下文的多跳推理中具有优势，而人类在需要简单演绎推理的任务中表现出色。利用这些见解，我们介绍了一个经过精细调整的Flan-T5模型，其性能超过了GPT-3.5，并与GPT-4媲美，提供了一个强大的开源解决方案供蕴含验证使用。作为一个实际的应用

    Humans make numerous inferences in text comprehension to understand discourse. This paper aims to understand the commonalities and disparities in the inference judgments between humans and state-of-the-art Large Language Models (LLMs). Leveraging a comprehensively curated entailment verification benchmark, we evaluate both human and LLM performance across various reasoning categories. Our benchmark includes datasets from three categories (NLI, contextual QA, and rationales) that include multi-sentence premises and different knowledge types, thereby evaluating the inference capabilities in complex reasoning instances. Notably, our findings reveal LLMs' superiority in multi-hop reasoning across extended contexts, while humans excel in tasks necessitating simple deductive reasoning. Leveraging these insights, we introduce a fine-tuned Flan-T5 model that outperforms GPT-3.5 and rivals with GPT-4, offering a robust open-source solution for entailment verification. As a practical application
    
[^71]: RL-VLM-F: 强化学习通过视觉语言基础模型反馈

    RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback

    [https://arxiv.org/abs/2402.03681](https://arxiv.org/abs/2402.03681)

    RL-VLM-F是一种通过视觉语言基础模型反馈的强化学习方法，能够自动生成有效的奖励函数和策略，从而解决了传统强化学习中奖励设计的挑战。

    

    传统强化学习研究中的奖励设计一直是一个挑战，因为通常需要大量人力和反复试错的过程来设计有效的奖励函数。本文提出了一种自动生成奖励函数的方法，用于代理学习新任务，只使用任务目标的文本描述和代理的视觉观测，并利用视觉语言基础模型（VLMs）的反馈。我们的方法的关键是通过查询这些模型，基于任务目标的文本描述给出对代理的图像观测的偏好，并从偏好标签中学习奖励函数，而不是直接要求这些模型输出原始奖励分数，这可能存在噪音和不一致性。我们证明了RL-VLM-F在各种领域中成功地产生了有效的奖励和策略，包括经典控制以及刚性和灵活操纵方面。

    Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulate
    
[^72]: 逻辑规范引导下的强化学习智能体动态任务采样

    Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents

    [https://arxiv.org/abs/2402.03678](https://arxiv.org/abs/2402.03678)

    本文提出了一种逻辑规范引导下的动态任务采样（LSTS）方法，通过学习一组强化学习策略，根据高级任务规范指导智能体在最小化环境交互次数的同时实现从初始状态到目标状态的引导。在网格世界实验中，LSTS实现了改进的时间到阈值。

    

    强化学习（RL）在使人工智能智能体学习多样化行为方面取得了重要进展。然而，学习有效的策略通常需要大量的环境交互。为了减少样本复杂性问题，最近的方法使用高级任务规范，如线性时态逻辑（LTL$_f$）公式或奖励机器（RM），来指导智能体的学习过程。在这项工作中，我们提出了一种新颖的方法，称为逻辑规范引导下的动态任务采样（LSTS），它通过学习一组强化学习策略，根据高级任务规范指导智能体从初始状态到目标状态，同时最小化环境交互次数。与以前的工作不同，LSTS不假设环境动力学或奖励机器的信息，并动态采样导致成功目标策略的有希望的任务。我们在一个网格世界上评估了LSTS，并展示了它实现了改进的时间到阈值。

    Reinforcement Learning (RL) has made significant strides in enabling artificial agents to learn diverse behaviors. However, learning an effective policy often requires a large number of environment interactions. To mitigate sample complexity issues, recent approaches have used high-level task specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward Machines (RM), to guide the learning progress of the agent. In this work, we propose a novel approach, called Logical Specifications-guided Dynamic Task Sampling (LSTS), that learns a set of RL policies to guide an agent from an initial state to a goal state based on a high-level task specification, while minimizing the number of environmental interactions. Unlike previous work, LSTS does not assume information about the environment dynamics or the Reward Machine, and dynamically samples promising tasks that lead to successful goal policies. We evaluate LSTS on a gridworld and show that it achieves improved time-to-threshol
    
[^73]: 使用PPIretrieval进行有效的蛋白质相互作用探索

    Effective Protein-Protein Interaction Exploration with PPIretrieval

    [https://arxiv.org/abs/2402.03675](https://arxiv.org/abs/2402.03675)

    PPIretrieval是第一个基于深度学习的模型，可以在嵌入空间中有效搜索潜在PPIs，并捕捉蛋白质表面的丰富几何和化学信息。

    

    蛋白质相互作用（PPIs）在调控许多细胞功能中至关重要，包括信号传导、运输和免疫防御。随着多链蛋白质复合物结构预测准确性的提高，挑战已经转向有效地导航庞大的复杂宇宙以识别潜在的PPIs。在这里，我们提出了PPIretrieval，这是第一个基于深度学习的蛋白质相互作用探索模型，它利用现有的PPI数据在嵌入空间中有效搜索潜在的PPIs，捕捉蛋白质表面的丰富几何和化学信息。当提供了一个未见过的查询蛋白质及其相关的结合位点时，PPIretrieval能够在嵌入空间中有效识别潜在的结合伴侣及其相应的结合位点，促进蛋白质相互作用复合物的形成。

    Protein-protein interactions (PPIs) are crucial in regulating numerous cellular functions, including signal transduction, transportation, and immune defense. As the accuracy of multi-chain protein complex structure prediction improves, the challenge has shifted towards effectively navigating the vast complex universe to identify potential PPIs. Herein, we propose PPIretrieval, the first deep learning-based model for protein-protein interaction exploration, which leverages existing PPI data to effectively search for potential PPIs in an embedding space, capturing rich geometric and chemical information of protein surfaces. When provided with an unseen query protein with its associated binding site, PPIretrieval effectively identifies a potential binding partner along with its corresponding binding site in an embedding space, facilitating the formation of protein-protein complexes.
    
[^74]: 大型语言模型作为间接推理器：对自动推理的反证和矛盾

    Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning

    [https://arxiv.org/abs/2402.03667](https://arxiv.org/abs/2402.03667)

    本文提出了一种大型语言模型的新型间接推理方法，使用反证和矛盾的逻辑来处理复杂推理任务，并通过增强数据和规则，以及设计提示模板的方式增强模型的推理能力。

    

    最近，人们越来越关注提高大型语言模型（LLMs）进行复杂推理的能力。然而，以前的方法主要是遵循直接推理（DR）框架，如“思维链”和“自一致性”，因此在解决很难通过DR解决的众多实际问题时会遇到困难。因此，为了增强LLMs的推理能力，本文提出了一种新颖的间接推理（IR）方法，该方法利用反证和矛盾的逻辑来处理事实推理和数学证明等IR任务。具体而言，我们的方法包括两个步骤。首先，我们利用反证的逻辑等价性来增强LLMs的数据和规则，以提高其可理解性。其次，我们设计了一组提示模板，触发LLMs进行基于矛盾证明的IR，其逻辑上等价于原始的DR过程。我们的IR方法简单而有效。

    Recently, increasing attention has been focused drawn on to improve the ability of Large Language Models (LLMs) to perform complex reasoning. However, previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow Direct Reasoning (DR) frameworks, so they will meet difficulty in solving numerous real-world tasks which can hardly be solved via DR. Therefore, to strengthen the reasoning power of LLMs, this paper proposes a novel Indirect Reasoning (IR) method that employs the logic of contrapositives and contradictions to tackle IR tasks such as factual reasoning and mathematic proof. Specifically, our methodology comprises two steps. Firstly, we leverage the logical equivalence of contrapositive to augment the data and rules to enhance the comprehensibility of LLMs. Secondly, we design a set of prompt templates to trigger LLMs to conduct IR based on proof by contradiction that is logically equivalent to the original DR process. Our IR method is simple yet effective and c
    
[^75]: 深度神经网络中包含符号层的符号正确性

    Symbol Correctness in Deep Neural Networks Containing Symbolic Layers

    [https://arxiv.org/abs/2402.03663](https://arxiv.org/abs/2402.03663)

    本文介绍了神经符号深度神经网络（NS-DNNs）中的符号正确性原则，即用于推理的神经层对中间符号的预测必须与输入数据的符号表示相匹配。符号正确性是NS-DNN可解释性和迁移学习的必要特性，并为推理和交流模型行为提供了精确的方法。

    

    为了处理以感知和逻辑推理相结合的人工智能任务，最近的工作引入了神经符号深度神经网络（NS-DNNs），它们除了传统的神经层之外，还包含符号层：在推理过程中由符号求解器评估的符号表达式（例如，SAT公式，逻辑程序）。我们确定并形式化了一种直观、高层次的原则，可以指导NS-DNNs的设计和分析：符号正确性，即神经层对中间符号的正确性，相对于输入数据的（通常未知的）基本符号表示。我们证明了符号正确性是NS-DNN可解释性和迁移学习的必要特性（尽管通常无法进行训练）。此外，我们还展示了符号正确性框架在神经符号边界处推理和交流模型行为方面提供了一种精确的方法，并对基本权衡有了深入的理解。

    To handle AI tasks that combine perception and logical reasoning, recent work introduces Neurosymbolic Deep Neural Networks (NS-DNNs), which contain -- in addition to traditional neural layers -- symbolic layers: symbolic expressions (e.g., SAT formulas, logic programs) that are evaluated by symbolic solvers during inference. We identify and formalize an intuitive, high-level principle that can guide the design and analysis of NS-DNNs: symbol correctness, the correctness of the intermediate symbols predicted by the neural layers with respect to a (generally unknown) ground-truth symbolic representation of the input data. We demonstrate that symbol correctness is a necessary property for NS-DNN explainability and transfer learning (despite being in general impossible to train for). Moreover, we show that the framework of symbol correctness provides a precise way to reason and communicate about model behavior at neural-symbolic boundaries, and gives insight into the fundamental tradeoffs
    
[^76]: 在图上进行传递式奖励推断

    Transductive Reward Inference on Graph

    [https://arxiv.org/abs/2402.03661](https://arxiv.org/abs/2402.03661)

    该研究提出了一种在图上进行传递式奖励推断的方法，可以有效地估计离线强化学习中未标记数据的奖励。通过利用有限的人工奖励注释和可用数据构建奖励传播图，并利用图进行奖励推断，从而推断出未标记数据的奖励。

    

    在这项研究中，我们提出了一种在奖励信息传播图上进行传递式推断的方法，从而能够有效地估计离线强化学习中未标记数据的奖励。奖励推断是在实际场景中学习有效策略的关键，而直接的环境交互要么成本太高，要么是不道德的，并且很少有可访问的奖励函数，例如在医疗保健和机器人领域。我们的研究集中于开发一种基于图上信息传播的上下文特性的奖励推断方法，利用有限数量的人工奖励注释来推断未标记数据的奖励。我们利用可用数据和有限的奖励注释构建奖励传播图，其中边权重包含与奖励相关的各种影响因素。随后，我们使用构建的图进行传递式奖励推断，从而估计奖励。

    In this study, we present a transductive inference approach on that reward information propagation graph, which enables the effective estimation of rewards for unlabelled data in offline reinforcement learning. Reward inference is the key to learning effective policies in practical scenarios, while direct environmental interactions are either too costly or unethical and the reward functions are rarely accessible, such as in healthcare and robotics. Our research focuses on developing a reward inference method based on the contextual properties of information propagation on graphs that capitalizes on a constrained number of human reward annotations to infer rewards for unlabelled data. We leverage both the available data and limited reward annotations to construct a reward propagation graph, wherein the edge weights incorporate various influential factors pertaining to the rewards. Subsequently, we employ the constructed graph for transductive reward inference, thereby estimating rewards
    
[^77]: 预训练-微调范式中出现了跨任务线性关系

    Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm

    [https://arxiv.org/abs/2402.03660](https://arxiv.org/abs/2402.03660)

    本文发现了在预训练-微调范式中，使用相同预训练检查点初始化并在不同任务上进行微调的模型会出现一个有趣的线性现象，称为跨任务线性。我们提供了实证证据并推测神经网络在这一范式中本质上类似线性映射，从参数空间到特征空间的映射。这一发现揭示了关于模型合并/编辑和参数共享等方面的新见解。

    

    预训练-微调范式已成为现代深度学习的主流趋势。在这项工作中，我们发现在从公共预训练检查点初始化并在不同任务上进行微调的模型中出现了一个有趣的线性现象，称为跨任务线性（CTL）。具体而言，如果我们线性插值两个微调模型的权重，权重插值模型中的特征大致等于每层中两个微调模型特征的线性插值。这样的跨任务线性在同行文献中尚未被注意到。我们提供了全面的实证证据，支持从相同预训练检查点开始的微调模型一致出现CTL。我们推测在预训练-微调范式中，神经网络本质上是线性映射，从参数空间到特征空间的映射。基于这个观点，我们的研究揭示了关于模型合并/编辑、参数共享等的新见解。

    The pretraining-finetuning paradigm has become the prevailing trend in modern deep learning. In this work, we discover an intriguing linear phenomenon in models that are initialized from a common pretrained checkpoint and finetuned on different tasks, termed as Cross-Task Linearity (CTL). Specifically, if we linearly interpolate the weights of two finetuned models, the features in the weight-interpolated model are approximately equal to the linear interpolation of features in two finetuned models at each layer. Such cross-task linearity has not been noted in peer literature. We provide comprehensive empirical evidence supporting that CTL consistently occurs for finetuned models that start from the same pretrained checkpoint. We conjecture that in the pretraining-finetuning paradigm, neural networks essentially function as linear maps, mapping from the parameter space to the feature space. Based on this viewpoint, our study unveils novel insights into explaining model merging/editing, p
    
[^78]: CAMBranch: 基于增强MILP的对比学习用于分支

    CAMBranch: Contrastive Learning with Augmented MILPs for Branching

    [https://arxiv.org/abs/2402.03647](https://arxiv.org/abs/2402.03647)

    CAMBranch是一个利用对比学习和增强MILP的机器学习框架，用于改进混合整数线性规划的分支策略。通过生成增强MILP并应用对比学习，CAMBranch能够获取大量标记的专家样本，从而提高分支决策的质量。

    

    最近的研究已经引入了机器学习框架来增强混合整数线性规划(B\&B)的分支策略。这些方法主要依赖于强分支的模仿学习，并展现了卓越的性能。然而，收集用于模仿学习的专家样本，特别是用于强分支的样本，是一项耗时的任务。为了解决这一挑战，我们提出了CAMBranch: 基于增强MILP的对比学习用于分支的框架，通过对有限数量的来自原始MILP的专家数据应用变量转移来生成增强MILP (AMILP)。这种方法可以获取大量标记的专家样本。CAMBranch利用MILP和AMILP进行模仿学习，并采用对比学习来提高模型捕捉MILP特征的能力，从而提高分支决策的质量。

    Recent advancements have introduced machine learning frameworks to enhance the Branch and Bound (B\&B) branching policies for solving Mixed Integer Linear Programming (MILP). These methods, primarily relying on imitation learning of Strong Branching, have shown superior performance. However, collecting expert samples for imitation learning, particularly for Strong Branching, is a time-consuming endeavor. To address this challenge, we propose \textbf{C}ontrastive Learning with \textbf{A}ugmented \textbf{M}ILPs for \textbf{Branch}ing (CAMBranch), a framework that generates Augmented MILPs (AMILPs) by applying variable shifting to limited expert data from their original MILPs. This approach enables the acquisition of a considerable number of labeled expert samples. CAMBranch leverages both MILPs and AMILPs for imitation learning and employs contrastive learning to enhance the model's ability to capture MILP features, thereby improving the quality of branching decisions. Experimental resul
    
[^79]: torchmSAT：一种GPU加速的最大可满足性问题的近似方法

    torchmSAT: A GPU-Accelerated Approximation To The Maximum Satisfiability Problem

    [https://arxiv.org/abs/2402.03640](https://arxiv.org/abs/2402.03640)

    本文提出了一种GPU加速的近似方法torchmSAT来解决最大可满足性问题，其通过推导出可微函数和使用神经网络架构，并利用GPU加速计算来提高解决效率。实验结果表明，该方法优于现有的两种MaxSAT求解器。

    

    机器学习技术在分析离散结构方面取得了显著的成就，引起了人们对其与组合优化算法结合的重视。通常，这些方法通过在求解循环中注入学习模型来提高搜索过程的效率。本文通过推导出一种能够近似解决最大可满足性问题(MaxSAT)的可微函数，提出了一种新颖的神经网络架构来建模我们的可微函数，并通过反向传播逐步求解MaxSAT问题。这种方法不需要标记数据或神经网络训练阶段，因为训练过程就是解决算法。此外，利用GPU的计算能力加速这些计算。在具有挑战性的MaxSAT示例上的实验结果表明，我们提出的方法优于现有的两种MaxSAT求解器。

    The remarkable achievements of machine learning techniques in analyzing discrete structures have drawn significant attention towards their integration into combinatorial optimization algorithms. Typically, these methodologies improve existing solvers by injecting learned models within the solving loop to enhance the efficiency of the search process. In this work, we derive a single differentiable function capable of approximating solutions for the Maximum Satisfiability Problem (MaxSAT). Then, we present a novel neural network architecture to model our differentiable function, and progressively solve MaxSAT using backpropagation. This approach eliminates the need for labeled data or a neural network training phase, as the training process functions as the solving algorithm. Additionally, we leverage the computational power of GPUs to accelerate these computations. Experimental results on challenging MaxSAT instances show that our proposed methodology outperforms two existing MaxSAT sol
    
[^80]: 通过IDE派生的静态上下文的本地集成增强LLM-Based编码工具

    Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context

    [https://arxiv.org/abs/2402.03630](https://arxiv.org/abs/2402.03630)

    通过IDE的本地集成，我们提出了IDECoder框架，利用IDE提供的准确和实时的跨文件信息来增强LLM-Based编码工具，解决了挑战性的跨文件上下文问题。

    

    大型语言模型(LLM)在代码完成方面取得了显著成就，如Copilot等代码助手服务的重要作用所证明。目前的LLM通过对文件上下文的训练，在单个源文件的代码完成方面非常有效。然而，对于需要跨文件信息的大型软件项目的存储库级代码完成，对它们来说是具有挑战性的。现有的基于LLM的存储库级代码完成研究识别和整合跨文件上下文，但因LLM的精度低和上下文长度有限而受到限制。本文认为集成开发环境(IDE)可以为存储库级代码完成提供直接、准确和实时的跨文件信息。我们提出了IDECoder，一个实际的框架，利用IDE的本地静态上下文进行跨上下文构建和自我完善的诊断结果。IDECoder利用丰富的跨上下文信息来...

    Large Language Models (LLMs) have achieved remarkable success in code completion, as evidenced by their essential roles in developing code assistant services such as Copilot. Being trained on in-file contexts, current LLMs are quite effective in completing code for single source files. However, it is challenging for them to conduct repository-level code completion for large software projects that require cross-file information. Existing research on LLM-based repository-level code completion identifies and integrates cross-file contexts, but it suffers from low accuracy and limited context length of LLMs. In this paper, we argue that Integrated Development Environments (IDEs) can provide direct, accurate and real-time cross-file information for repository-level code completion. We propose IDECoder, a practical framework that leverages IDE native static contexts for cross-context construction and diagnosis results for self-refinement. IDECoder utilizes the rich cross-context information 
    
[^81]: 近似的中心化softmax损失用于视觉-语言模型的鲁棒性

    Partially Recentralization Softmax Loss for Vision-Language Models Robustness

    [https://arxiv.org/abs/2402.03627](https://arxiv.org/abs/2402.03627)

    本文研究了通过修改预训练多模态模型的损失函数来提高对抗鲁棒性，通过限制前K个softmax输出。实验结果表明，经过微调后，模型的对抗鲁棒性显著提高，能够有效抵御常见的攻击。

    

    随着大型语言模型在自然语言处理任务中的突破，多模态技术变得非常流行。然而，已经证明多模态自然语言处理模型容易受到对抗攻击，即模型的输出可以通过对输入进行微小扰动而发生巨大变化。虽然计算机视觉和自然语言处理模型中已经提出了几种防御技术，但对多模态模型的鲁棒性还没有进行充分探索。在本文中，我们研究了通过修改预训练多模态模型的损失函数，通过限制前K个softmax输出来提供的对抗鲁棒性。基于评估和评分，我们的实验结果显示，在经过微调后，预训练模型的对抗鲁棒性可以显着提高，对抗常见的攻击有效。进一步的研究应该探索这类损失函数的输出多样性、泛化能力以及鲁棒性和性能之间的平衡。我们的代码将在之后提供。

    As Large Language Models make a breakthrough in natural language processing tasks (NLP), multimodal technique becomes extremely popular. However, it has been shown that multimodal NLP are vulnerable to adversarial attacks, where the outputs of a model can be dramatically changed by a perturbation to the input. While several defense techniques have been proposed both in computer vision and NLP models, the multimodal robustness of models have not been fully explored. In this paper, we study the adversarial robustness provided by modifying loss function of pre-trained multimodal models, by restricting top K softmax outputs. Based on the evaluation and scoring, our experiments show that after a fine-tuning, adversarial robustness of pre-trained models can be significantly improved, against popular attacks. Further research should be studying, such as output diversity, generalization and the robustness-performance trade-off of this kind of loss functions. Our code will be available after th
    
[^82]: 概率电路中用于边际MAP的神经网络近似器

    Neural Network Approximators for Marginal MAP in Probabilistic Circuits

    [https://arxiv.org/abs/2402.03621](https://arxiv.org/abs/2402.03621)

    本文提出了一种使用神经网络近似概率电路中边际MAP推理的方法，该方法通过使用连续多线性函数来估计查询变量的赋值成本并将其作为损失函数，具有自我监督和高效性的优点。

    

    概率电路（PCs）如和积网络以高效地表示大型多变量概率分布。在实践中，与贝叶斯网络和马尔可夫网络等其他概率表示相比，PCs更受青睐，因为PCs可以在网络大小线性扩展的时间内解决边际推理（MAR）任务。然而，最大后验概率（MAP）和边际MAP（MMAP）任务在这些模型中仍然是NP困难的。受最近关于使用神经网络生成接近最优解的优化问题（如整数线性规划）的工作的启发，我们提出了一种方法，该方法使用神经网络来近似PCs中的(M)MAP推理。我们方法的关键思想是使用连续多线性函数来近似查询变量的赋值成本，然后将其用作损失函数。我们的新方法有两个主要优点，即自我监督和在学习神经网络之后，它只需要

    Probabilistic circuits (PCs) such as sum-product networks efficiently represent large multi-variate probability distributions. They are preferred in practice over other probabilistic representations such as Bayesian and Markov networks because PCs can solve marginal inference (MAR) tasks in time that scales linearly in the size of the network. Unfortunately, the maximum-a-posteriori (MAP) and marginal MAP (MMAP) tasks remain NP-hard in these models. Inspired by the recent work on using neural networks for generating near-optimal solutions to optimization problems such as integer linear programming, we propose an approach that uses neural networks to approximate (M)MAP inference in PCs. The key idea in our approach is to approximate the cost of an assignment to the query variables using a continuous multilinear function, and then use the latter as a loss function. The two main benefits of our new method are that it is self-supervised and after the neural network is learned, it requires 
    
[^83]: 自我发现：大型语言模型自动构建推理结构

    Self-Discover: Large Language Models Self-Compose Reasoning Structures

    [https://arxiv.org/abs/2402.03620](https://arxiv.org/abs/2402.03620)

    SELF-DISCOVER是一个通用框架，能让大型语言模型自主发现任务内在的推理结构，显著提升了复杂推理问题的解决性能，并在推理计算方面取得更好的效果。

    

    我们引入了SELF-DISCOVER，一个通用框架，用于使LLM自主发现任务内在的推理结构，以应对对于典型提示方法而言具有挑战性的复杂推理问题。该框架的核心是一个自我发现过程，在这个过程中，LLM选择多个原子推理模块，如批判性思维和逐步思考，并将它们组合成LLM在解码过程中遵循的明确推理结构。SELF-DISCOVER在具有挑战性的推理基准测试中，如BigBench-Hard、基于代理的推理和MATH上，相较于Chain of Thought (CoT)的性能提升高达32%。此外，SELF-DISCOVER在需要10-40倍较少推理计算的情况下，较CoT-Self-Consistency等推理密集方法的表现更好，超过20%。最后，我们证明了自我发现的推理结构在模型家族之间是普遍适用的：从PaLM 2-L到GPT-4，从GPT-4到Llama2，并分享了

    We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share 
    
[^84]: 使用多模态连续复制比较人类和大型语言模型中的抽象能力

    Comparing Abstraction in Humans and Large Language Models Using Multimodal Serial Reproduction

    [https://arxiv.org/abs/2402.03618](https://arxiv.org/abs/2402.03618)

    这项研究通过实现一个多模态连续复制框架，比较了人类和GPT-4的抽象能力。实验结果表明，将语言作为一种模态对人类的复制影响更大，这暗示人类的视觉和语言表征比GPT-4的表征更具分离性。

    

    人类从嘈杂的感官数据中提取有用的世界抽象。连续复制允许我们通过类似于电话游戏的范式来研究人们如何构建世界，其中一个人观察一个刺激并将其复制给下一个人形成一条复制链。过去的连续复制实验通常采用单一感觉模式，但人类经常通过语言相互传达世界的抽象。为了调查语言对抽象形成的影响，我们通过要求接收视觉刺激的人以语言形式来复制它，并反之亦然，实现了一种新颖的多模态连续复制框架。我们对人类和GPT-4进行了单模态和多模态的连续复制，并发现将语言作为一种模态对人类的复制影响更大。这表明人类的视觉和语言表征比GPT-4的表征更具分离性。

    Humans extract useful abstractions of the world from noisy sensory data. Serial reproduction allows us to study how people construe the world through a paradigm similar to the game of telephone, where one person observes a stimulus and reproduces it for the next to form a chain of reproductions. Past serial reproduction experiments typically employ a single sensory modality, but humans often communicate abstractions of the world to each other through language. To investigate the effect language on the formation of abstractions, we implement a novel multimodal serial reproduction framework by asking people who receive a visual stimulus to reproduce it in a linguistic format, and vice versa. We ran unimodal and multimodal chains with both humans and GPT-4 and find that adding language as a modality has a larger effect on human reproductions than GPT-4's. This suggests human visual and linguistic representations are more dissociable than those of GPT-4.
    
[^85]: 利用大型语言模型进行混合工作场所决策支持

    Leveraging Large Language Models for Hybrid Workplace Decision Support

    [https://arxiv.org/abs/2402.03616](https://arxiv.org/abs/2402.03616)

    本论文研究了利用大型语言模型（LLMs）为混合工作场所提供智能决策支持的决策支持模型。通过广泛的用户研究和评估，发现LLMs在提供适当工作区建议方面具有超越提示的推理能力，提高工作者的工作体验。

    

    大型语言模型（LLMs）具有执行各种文本处理任务并为提议的操作或决策提供文本解释的潜力。在混合工作时代，LLMs可以为设计混合工作计划的工作者提供智能决策支持。特别是它们可以为平衡众多决策因素的工作者提供建议和解释，从而增强他们的工作体验。在本文中，我们提出了一个在混合工作环境中工作区决策支持模型，利用LLMs的推理能力。我们首先研究了LLMs对于提供适当工作区建议的能力。我们发现，其推理能力超越了提示中的指导方针，LLMs可以在工作区资源的可用性之间进行权衡。我们进行了广泛的用户研究，以了解工作者在工作区选择上的决策过程，并评估系统的有效性。我们观察到，工作者的决策可能受到影响。

    Large Language Models (LLMs) hold the potential to perform a variety of text processing tasks and provide textual explanations for proposed actions or decisions. In the era of hybrid work, LLMs can provide intelligent decision support for workers who are designing their hybrid work plans. In particular, they can offer suggestions and explanations to workers balancing numerous decision factors, thereby enhancing their work experience. In this paper, we present a decision support model for workspaces in hybrid work environments, leveraging the reasoning skill of LLMs. We first examine LLM's capability of making suitable workspace suggestions. We find that its reasoning extends beyond the guidelines in the prompt and the LLM can manage the trade-off among the available resources in the workspaces. We conduct an extensive user study to understand workers' decision process for workspace choices and evaluate the effectiveness of the system. We observe that a worker's decision could be influe
    
[^86]: RAP：具有上下文记忆的检索增强规划用于多模态LLM代理

    RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents

    [https://arxiv.org/abs/2402.03610](https://arxiv.org/abs/2402.03610)

    本研究提出了一种称为RAP的框架，它能够以动态方式利用过去的经验来增强代理的规划能力，并在纯文本和多模态环境中表现出色。实验结果显示RAP在文本场景中达到了最先进水平，并显著提高了多模态LLM代理在具身任务中的性能。

    

    最近的进展使得大型语言模型(LLM)可以被部署为用于机器人、游戏和API集成等领域中越来越复杂的决策应用的代理。然而，利用过去的经验来指导当前的决策过程仍然是一个困难的问题。为解决这个问题，我们提出了一种称为检索增强规划（RAP）的框架，旨在动态地利用过去与当前情况和上下文相关的经验，从而提升代理的规划能力。RAP的特点在于它的多功能性：它在纯文本和多模态环境中表现出色，适用于多个任务。实证评估结果显示RAP的有效性，在文本场景中取得了SOTA的表现，并显著提升了多模态LLM代理在具身任务中的性能。这些结果突显了RAP在推进LLM的功能和适用性方面的潜力。

    Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM 
    
[^87]: 提高多模态营销的上下文一致性：知识基础学习的有效性

    Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning

    [https://arxiv.org/abs/2402.03607](https://arxiv.org/abs/2402.03607)

    本研究提出了一种将常识知识图谱与大型视觉语言模型相结合的框架，用于改进预测多模态营销活动效果的性能。该方法能够提供早期检测可能具有说服力的多模态活动并评估和增强营销理论的能力。

    

    智能设备的普及使用户能够在线体验多模态信息。然而，大型语言模型（LLM）和视觉模型（LVM）仍然受到捕捉跨模态语义关系的整体意义的限制。缺乏明确的常识知识（例如，作为一个知识图谱），视觉语言模型（VLM）仅通过捕捉庞大的语料库中的高级模式来学习隐式表示，从而忽略了重要的上下文跨模态线索。在这项工作中，我们设计了一个框架，将显式的常识知识以知识图谱的形式与大型的VLM相结合，以提高下游任务的性能，即预测多模态营销活动的有效性。虽然营销应用提供了一个有说服力的指标来评估我们的方法，但我们的方法使得早期发现可能具有说服力的多模态活动成为可能，并评估和增强营销理论。

    The prevalence of smart devices with the ability to capture moments in multiple modalities has enabled users to experience multimodal information online. However, large Language (LLMs) and Vision models (LVMs) are still limited in capturing holistic meaning with cross-modal semantic relationships. Without explicit, common sense knowledge (e.g., as a knowledge graph), Visual Language Models (VLMs) only learn implicit representations by capturing high-level patterns in vast corpora, missing essential contextual cross-modal cues. In this work, we design a framework to couple explicit commonsense knowledge in the form of knowledge graphs with large VLMs to improve the performance of a downstream task, predicting the effectiveness of multi-modal marketing campaigns. While the marketing application provides a compelling metric for assessing our methods, our approach enables the early detection of likely persuasive multi-modal campaigns and the assessment and augmentation of marketing theory.
    
[^88]: 评估分布转变对强化学习性能的影响

    Assessing the Impact of Distribution Shift on Reinforcement Learning Performance

    [https://arxiv.org/abs/2402.03590](https://arxiv.org/abs/2402.03590)

    评估强化学习性能时需要考虑分布转变，我们提出了一套评估方法，并推荐使用时间序列分析进行观测RL评估。

    

    机器学习的研究正在解决自身的可重复性危机。特别是，强化学习(RL)面临着一系列独特的挑战。比较点估计和在训练过程中显示成功收敛到最优策略的图表可能会掩盖过拟合或对实验设置的依赖性。虽然RL的研究人员提出了可靠性指标以考虑不确定性，以更好地理解每个算法的优势和劣势，但过去的工作建议并不假设存在超出分布的观测结果。我们提出了一套评估方法，衡量了在分布转变下RL算法的稳健性。本文介绍的工具支持在代理在其环境中行动时考虑性能的需求。我们尤其推荐使用时间序列分析作为观测RL评估的方法。我们还展示了RL和模拟动力学的独特属性。

    Research in machine learning is making progress in fixing its own reproducibility crisis. Reinforcement learning (RL), in particular, faces its own set of unique challenges. Comparison of point estimates, and plots that show successful convergence to the optimal policy during training, may obfuscate overfitting or dependence on the experimental setup. Although researchers in RL have proposed reliability metrics that account for uncertainty to better understand each algorithm's strengths and weaknesses, the recommendations of past work do not assume the presence of out-of-distribution observations. We propose a set of evaluation methods that measure the robustness of RL algorithms under distribution shifts. The tools presented here argue for the need to account for performance over time while the agent is acting in its environment. In particular, we recommend time series analysis as a method of observational RL evaluation. We also show that the unique properties of RL and simulated dyna
    
[^89]: 通过双头判别器进行连续领域对抗适应

    Continual Domain Adversarial Adaptation via Double-Head Discriminators

    [https://arxiv.org/abs/2402.03588](https://arxiv.org/abs/2402.03588)

    本文提出了一种通过双头判别器进行连续领域对抗适应的方法，在源学习阶段引入了一个仅在源域训练的源域判别器，减少了对抗损失的经验估计误差，实验结果表明算法实现了超过2%的准确提升。

    

    在连续设置下的领域对抗适应面临着重大挑战，因为存在对之前的源域数据的访问限制。尽管在连续学习中进行了广泛研究，但是仅仅使用少量存储的源域数据（这是记忆重播方法中的标准设置）无法有效完成对抗适应任务。这个限制来自于使用少量源域样本对$\gH$-divergence进行经验估计的错误。为了解决这个问题，我们提出了一个双头判别器算法，通过引入一个仅在源学习阶段训练的源域判别器，从源域一侧减少了$\gH$-divergence相关对抗损失的经验估计误差。进一步在现有的领域适应基准上的实验表明，我们提出的算法实现了超过2%的准确提升。

    Domain adversarial adaptation in a continual setting poses a significant challenge due to the limitations on accessing previous source domain data. Despite extensive research in continual learning, the task of adversarial adaptation cannot be effectively accomplished using only a small number of stored source domain data, which is a standard setting in memory replay approaches. This limitation arises from the erroneous empirical estimation of $\gH$-divergence with few source domain samples. To tackle this problem, we propose a double-head discriminator algorithm, by introducing an addition source-only domain discriminator that are trained solely on source learning phase. We prove that with the introduction of a pre-trained source-only domain discriminator, the empirical estimation error of $\gH$-divergence related adversarial loss is reduced from the source domain side. Further experiments on existing domain adaptation benchmark show that our proposed algorithm achieves more than 2$\%$
    
[^90]: MQuinE:知识图谱嵌入模型中“Z-悖论”的解决方案

    MQuinE: a cure for "Z-paradox'' in knowledge graph embedding models

    [https://arxiv.org/abs/2402.03583](https://arxiv.org/abs/2402.03583)

    研究者发现知识图谱嵌入模型存在的“Z-悖论”限制了其表达能力，并提出了一种名为MQuinE的新模型，通过理论证明，MQuinE成功解决了Z-悖论，并在链接预测任务中显著优于现有模型。

    

    知识图谱嵌入（KGE）模型在许多知识图谱任务，包括链接预测和信息检索方面取得了最先进的结果。尽管KGE模型在实践中表现出优越性能，但我们发现一些流行的现有KGE模型存在表达不足的问题，称为“Z-悖论”。受到Z-悖论的存在的启发，我们提出了一种新的KGE模型，称为MQuinE，在不受Z-悖论的困扰的同时，保持强大的表达能力来模拟各种关系模式，包括对称/非对称，逆向，1-N/N-1/N-N和组合关系，并提供了理论上的证明。对实际知识库的实验表明，Z-悖论确实降低了现有KGE模型的性能，并且可能导致某些具有挑战性的测试样本的准确性下降超过20％。我们的实验进一步证明了MQuinE可以减轻Z-悖论的负面影响，并在链接预测方面以明显优势超越现有的KGE模型。

    Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called \emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE model called \emph{MQuinE} that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20\% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction
    
[^91]: LLM多智能体系统：挑战与开放问题

    LLM Multi-Agent Systems: Challenges and Open Problems

    [https://arxiv.org/abs/2402.03578](https://arxiv.org/abs/2402.03578)

    本文讨论了多智能体系统的挑战与开放问题，包括任务分配优化、增强推理能力、管理上下文信息和改善内存管理，同时探讨了多智能体系统在区块链系统中的潜力和未来发展。

    

    本文探讨了现有多智能体系统的研究工作，并识别出尚未充分解决的挑战。通过利用多智能体系统内个体智能体的多样能力和角色，这些系统可以通过协作来处理复杂任务。我们讨论了优化任务分配、通过迭代辩论促进强大推理、管理复杂和分层的上下文信息以及增强内存管理以支持多智能体系统内的复杂交互。我们还探讨了在区块链系统中应用多智能体系统的潜力，以启示其在真实分布式系统中的未来发展和应用。

    This paper explores existing works of multi-agent systems and identifies challenges that remain inadequately addressed. By leveraging the diverse capabilities and roles of individual agents within a multi-agent system, these systems can tackle complex tasks through collaboration. We discuss optimizing task allocation, fostering robust reasoning through iterative debates, managing complex and layered context information, and enhancing memory management to support the intricate interactions within multi-agent systems. We also explore the potential application of multi-agent systems in blockchain systems to shed light on their future development and application in real-world distributed systems.
    
[^92]: 在大规模多人游戏中实现人工智能与人类的协同

    Toward Human-AI Alignment in Large-Scale Multi-Player Games

    [https://arxiv.org/abs/2402.03575](https://arxiv.org/abs/2402.03575)

    本研究提出了一种在大规模多人游戏中评估人工智能与人类协作的方法，通过分析人类游戏数据和训练AI代理来比较和对比人类和AI的行为差异，以识别高级行为概念。

    

    在复杂的多智能体游戏中实现人工智能与人类的协同对于创建增强游戏体验的可信任的人工智能代理至关重要。我们提出了一种方法来评估这种协同，使用可解释的任务集框架，重点关注高级行为任务而非低级策略。我们的方法有三个组成部分。首先，我们分析了来自Xbox的Bleeding Edge（10万+游戏）的大量人类游戏数据，揭示了复杂任务空间中的行为模式。这个任务空间作为行为流形的基础集合，捕捉可解释的轴：战斗-逃跑、探索-利用以及单人-多人智能体。其次，我们训练一个使用生成预训练因果变换器的人工智能代理来玩Bleeding Edge，并测量其行为。第三，我们将人类和人工智能游戏映射到提出的行为流形中进行比较和对比。这样可以解释策略的差异，如，我们发现人类玩家在战斗-逃跑方面表现变化多样。

    Achieving human-AI alignment in complex multi-agent games is crucial for creating trustworthy AI agents that enhance gameplay. We propose a method to evaluate this alignment using an interpretable task-sets framework, focusing on high-level behavioral tasks instead of low-level policies. Our approach has three components. First, we analyze extensive human gameplay data from Xbox's Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task space. This task space serves as a basis set for a behavior manifold capturing interpretable axes: fight-flight, explore-exploit, and solo-multi-agent. Second, we train an AI agent to play Bleeding Edge using a Generative Pretrained Causal Transformer and measure its behavior. Third, we project human and AI gameplay to the proposed behavior manifold to compare and contrast. This allows us to interpret differences in policy as higher-level behavioral concepts, e.g., we find that while human players exhibit variability in fight-flight
    
[^93]: 扩散世界模型

    Diffusion World Model

    [https://arxiv.org/abs/2402.03570](https://arxiv.org/abs/2402.03570)

    扩散世界模型是一个能够预测多步未来状态和奖励的条件性扩散模型，在模型效果和性能方面超过了传统的一步动力学模型。

    

    我们引入了扩散世界模型（DWM），这是一个条件扩散模型，能够同时预测多步的未来状态和奖励。与传统的一步动力学模型相反，DWM通过单个前向传递提供了长时程的预测，消除了递归查询的需要。我们将DWM整合到基于模型的价值估计中，其中短期回报通过从DWM中采样的未来轨迹进行模拟。在离线强化学习的背景下，DWM可以被视为通过生成建模来实现保守的值正则化。另外，它也可以被视为一种数据源，使离线Q学习能够使用合成数据。我们在D4RL数据集上的实验证实了DWM对长时程模拟的鲁棒性。在绝对性能方面，DWM显著超过了一步动力学模型，性能提高了44%，并达到了最先进的水平。

    We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\%$ performance gain, and achieves state-of-the-art performance.
    
[^94]: 用语言模型区分可知与不可知的能力

    Distinguishing the Knowable from the Unknowable with Language Models

    [https://arxiv.org/abs/2402.03563](https://arxiv.org/abs/2402.03563)

    通过研究大型语言模型，在自由文本中识别作为代理的模型和冻结预训练模型的嵌入的小型线性探测器可以准确预测更大模型令牌级别上的自信度，进一步提出了一种无监督的方法在相同任务上达到了非平凡的准确度，这证明了语言模型中存在不同类型的不确定性表示。

    

    我们研究了在大型语言模型（LLMs）生成的自由文本输出中，是否可以鉴别出认知不确定性（反映缺乏知识的不确定性）和偶然不确定性（反映基础分布中的熵）。在没有真实概率的情况下，我们探索了一个设置，在这个设置中，为了（近似地）分解给定LLM的不确定性，一个明显更大的模型充当地面真相的代理。我们表明，基于冻结预训练模型的嵌入的小型线性探测器能够准确预测在令牌级别上更大模型将更自信的情况，并且在一个文本领域上训练的探测器可以泛化到其他领域。进一步地，我们提出了一种完全无监督的方法，在相同任务上达到了非平凡的准确度。综合考虑这些结果，我们解释这些结果作为LLMs内部自然地包含了不同类型不确定性的表示，这可能有助于制定更有效的方法。

    We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more i
    
[^95]: VLN-Video: 利用行车视频进行室外视觉与语言导航

    VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation

    [https://arxiv.org/abs/2402.03561](https://arxiv.org/abs/2402.03561)

    VLN-Video利用行车视频的多样室外环境和自动生成的导航指令与动作，通过深度学习方法提高了室外视觉与语言导航的性能。

    

    室外视觉与语言导航（VLN）要求代理根据自然语言指令在逼真的三维室外环境中导航。现有的VLN方法在导航环境多样性和训练数据有限性方面存在限制。为解决这些问题，我们提出了VLN-Video，该方法利用在美国多个城市的行车视频中存在的多样化室外环境，并通过自动生成导航指令和动作来提高室外VLN性能。VLN-Video结合了直观经典方法和现代深度学习技术的优势，利用模板填充生成有实际基础的导航指令，并结合基于图像旋转相似度的导航动作预测器从行车视频中获取VLN风格的数据，用于预训练深度学习VLN模型。我们在Touchdown数据集和由行车视频创建的视频增强数据集上对模型进行预训练。

    Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with th
    
[^96]: 用于约束满足的投影式生成扩散模型

    Projected Generative Diffusion Models for Constraint Satisfaction

    [https://arxiv.org/abs/2402.03559](https://arxiv.org/abs/2402.03559)

    本文介绍了一种名为投影式生成扩散模型（PGDM）的方法，它能够通过约束优化问题将生成扩散模型适用于对特定条件有严格要求的场景。该方法通过迭代投影方法确保生成的数据符合指定的约束或物理原理。实验证明PGDM在复杂的约束和常微分方程的情况下也能合成出符合要求的输出。

    

    生成扩散模型通过一个顺序过程，能够从原始噪声中合成出连贯的内容。然而，在需要输出符合特定严格条件的场景中直接应用这些模型面临着严重的挑战。本文旨在克服这些挑战，并介绍了投影式生成扩散模型（PGDM），该方法将传统的扩散模型采样重新构建为一个约束优化问题。这使得可以应用迭代投影方法，以确保生成的数据忠实地遵循指定的约束或物理原理。本文在受限制的约束类别下，对PGDM能够从可行子分布中合成输出的能力提供了理论支持，并在复杂的非凸约束和常微分方程的案例中提供了大量的经验证据。这些能力通过在视频生成中体现了具有物理学信息的动态。

    Generative diffusion models excel at robustly synthesizing coherent content from raw noise through a sequential process. However, their direct application in scenarios requiring outputs to adhere to specific, stringent criteria faces several severe challenges. This paper aims at overcome these challenges and introduces Projected Generative Diffusion Models (PGDM), an approach that recast traditional diffusion models sampling into a constrained-optimization problem. This enables the application of an iterative projections method to ensure that generated data faithfully adheres to specified constraints or physical principles. This paper provides theoretical support for the ability of PGDM to synthesize outputs from a feasible subdistribution under a restricted class of constraints while also providing large empirical evidence in the case of complex non-convex constraints and ordinary differential equations. These capabilities are demonstrated by physics-informed motion in video generatio
    
[^97]: 对回答集编程的结构难度进行的扩展研究：结构能否有效限制析取的能力？

    Extended Version of: On the Structural Hardness of Answer Set Programming: Can Structure Efficiently Confine the Power of Disjunctions?

    [https://arxiv.org/abs/2402.03539](https://arxiv.org/abs/2402.03539)

    我们进行了对回答集编程的结构参数的分类研究，并提供了一种多项式核以实现与顶点覆盖大小对应的单指数时间运行。

    

    回答集编程（ASP）是一个通用的问题建模和求解框架，主要关注知识表示和工业应用的快速增长。迄今为止，复杂性的研究导致了难度的特征化和确定其来源，以及类似二分法结果的细节性参数化复杂性分析。不幸的是，在已知的参数树宽下，合取规模的非合取程序在合理的复杂性假设下需要双指数时间运行，这很快变得难以达到。我们研究了关于非合取ASP的结构参数的分类问题（入射图）。首先，我们提供了一个多项式核来获得与顶点覆盖大小对应的单指数时间运行，尽管子集最小化在程序的结构中没有被表示。然后，我们将注意力转向顶点覆盖大小和树宽之间严格更好的结构参数。

    Answer Set Programming (ASP) is a generic problem modeling and solving framework with a strong focus on knowledge representation and a rapid growth of industrial applications. So far, the study of complexity resulted in characterizing hardness and determining their sources, fine-grained insights in the form of dichotomy-style results, as well as detailed parameterized complexity landscapes. Unfortunately, for the well-known parameter treewidth disjunctive programs require double-exponential runtime under reasonable complexity assumptions. This quickly becomes out of reach. We deal with the classification of structural parameters for disjunctive ASP on the program's rule structure (incidence graph).   First, we provide a polynomial kernel to obtain single-exponential runtime in terms of vertex cover size, despite subset-minimization being not represented in the program's structure. Then we turn our attention to strictly better structural parameters between vertex cover size and treewidt
    
[^98]: 对虾的初步报告：多调查计算机视觉光度红移模型

    Preliminary Report on Mantis Shrimp: a Multi-Survey Computer Vision Photometric Redshift Model

    [https://arxiv.org/abs/2402.03535](https://arxiv.org/abs/2402.03535)

    这项研究报道了Mantis Shrimp，一个基于计算机视觉的多调查光度红移模型，融合了紫外线、光学和红外图像，并使用深度学习可解释性诊断技术研究了其如何利用不同输入信息。

    

    大型、公共、多模态天文数据集的可用性为跨科学与天文学的人工智能研究提供了机会。光度红移估计是天文学的一个成熟子领域。先前的研究表明，计算机视觉模型通常优于基于目录的模型，但是这些模型在融合来自多个仪器或传感器的图像时面临额外的复杂性。在本报告中，我们详细介绍了我们正在创建的Mantis Shrimp，一个用于光度红移估计的多调查计算机视觉模型，它融合了紫外线（GALEX）、光学（PanSTARRS）和红外（UnWISE）图像。我们使用深度学习可解释性诊断来衡量模型如何利用来自不同输入的信息。我们根据可解释性指标从物理上基于的星系属性的角度推理CNN的行为。

    The availability of large, public, multi-modal astronomical datasets presents an opportunity to execute novel research that straddles the line between science of AI and science of astronomy. Photometric redshift estimation is a well-established subfield of astronomy. Prior works show that computer vision models typically outperform catalog-based models, but these models face additional complexities when incorporating images from more than one instrument or sensor. In this report, we detail our progress creating Mantis Shrimp, a multi-survey computer vision model for photometric redshift estimation that fuses ultra-violet (GALEX), optical (PanSTARRS), and infrared (UnWISE) imagery. We use deep learning interpretability diagnostics to measure how the model leverages information from the different inputs. We reason about the behavior of the CNNs from the interpretability metrics, specifically framing the result in terms of physically-grounded knowledge of galaxy properties.
    
[^99]: 深度强化学习在仓储中的拣选车辆路径问题中的应用

    Deep Reinforcement Learning for Picker Routing Problem in Warehousing

    [https://arxiv.org/abs/2402.03525](https://arxiv.org/abs/2402.03525)

    本研究提出了一种基于注意力机制的神经网络模型，通过深度强化学习解决仓储中的拣选车辆路径问题。与传统启发式方法相比，该方法具有更快的速度和准确度，并能降低路径的感知复杂度。

    

    订单拣选车辆路径是仓库运营管理中的关键问题。由于问题的复杂性和需要快速解决方案的需求，实践中常常使用次优算法。然而，强化学习提供了一种吸引人的替代传统启发式方法的选择，可能在速度和准确度方面优于现有方法。我们引入了一种基于注意力机制的神经网络来建模拣选车辆路径，该网络通过强化学习进行训练。我们的方法在一系列问题参数上与现有启发式算法进行了评估，展示了其效果。我们提出方法的一个重要优点是可以降低路径的感知复杂度。

    Order Picker Routing is a critical issue in Warehouse Operations Management. Due to the complexity of the problem and the need for quick solutions, suboptimal algorithms are frequently employed in practice. However, Reinforcement Learning offers an appealing alternative to traditional heuristics, potentially outperforming existing methods in terms of speed and accuracy. We introduce an attention based neural network for modeling picker tours, which is trained using Reinforcement Learning. Our method is evaluated against existing heuristics across a range of problem parameters to demonstrate its efficacy. A key advantage of our proposed method is its ability to offer an option to reduce the perceived complexity of routes.
    
[^100]: 在西班牙语中解决转录歧义：一种混合声学-词汇系统用于标点符号恢复

    Resolving Transcription Ambiguity in Spanish: A Hybrid Acoustic-Lexical System for Punctuation Restoration

    [https://arxiv.org/abs/2402.03519](https://arxiv.org/abs/2402.03519)

    提出了一种混合声学-词汇系统，用于解决西班牙语标点符号恢复任务，通过整合声学和词汇信号，在西班牙语转录中提高了问号的F1分数和整体标点符号恢复，并在准确性、可靠性和延迟方面超越了大型语言模型。

    

    标点符号恢复是自动语音识别（ASR）系统后的关键步骤，以增强转录的可读性并促进后续的自然语言处理任务。然而，传统的基于词汇的方法不足以解决西班牙语中标点符号恢复的任务，因为在未标点的陈述句和疑问句之间常常存在歧义。在本研究中，我们提出了一种新颖的混合声学-词汇标点符号恢复系统，用于西班牙语转录，通过模块化的过程整合声学和词汇信号。我们的实验结果表明，所提出的系统可以有效提高问号的F1分数，并在公共和内部的西班牙语对话数据集上实现整体标点符号恢复。此外，与大型语言模型（LLMs）的基准比较表明，我们的方法在准确性、可靠性和延迟方面具有优势。此外，我们还展示了ASR模块的词错误率（WER）也受益于这种方法。

    Punctuation restoration is a crucial step after Automatic Speech Recognition (ASR) systems to enhance transcript readability and facilitate subsequent NLP tasks. Nevertheless, conventional lexical-based approaches are inadequate for solving the punctuation restoration task in Spanish, where ambiguity can be often found between unpunctuated declaratives and questions. In this study, we propose a novel hybrid acoustic-lexical punctuation restoration system for Spanish transcription, which consolidates acoustic and lexical signals through a modular process. Our experiment results show that the proposed system can effectively improve F1 score of question marks and overall punctuation restoration on both public and internal Spanish conversational datasets. Additionally, benchmark comparison against LLMs (Large Language Model) indicates the superiority of our approach in accuracy, reliability and latency. Furthermore, we demonstrate that the Word Error Rate (WER) of the ASR module also benef
    
[^101]: 跨领域评估零样本摘要生成器的真实性

    Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains

    [https://arxiv.org/abs/2402.03509](https://arxiv.org/abs/2402.03509)

    本研究跨领域评估了零样本摘要生成器的真实性。通过对生物医学文章和法律法案等专业领域进行评估，我们特别关注生成摘要的真实性，并分析了预训练语料库中给定领域的普遍性对摘要质量的影响。

    

    最近的研究表明，大型语言模型(LLMs)能够零样本（即在没有明确监督的情况下）生成摘要，经过人工评估，这些摘要往往与手工编写的参考摘要相比，甚至更受欢迎。然而，这些早期的研究几乎专注于评估新闻文章的摘要。零样本摘要生成器在其他（可能更专业）领域中的表现如何？在这项工作中，我们评估了跨专业领域中零样本生成的摘要，包括生物医学文章和法律法案（除了标准新闻摘要的参考）。我们特别关注输出的真实性。我们从领域专家处获取注释，以识别摘要中的不一致之处，并对这些错误进行系统分类。我们分析了预训练语料库中给定领域的普遍性是否会影响在该领域的文章的摘要的提取和忠实度。我们发布了所有收集到的注释。

    Recent work has shown that large language models (LLMs) are capable of generating summaries zero-shot (i.e., without explicit supervision) that, under human assessment, are often comparable or even preferred to manually composed reference summaries. However, this prior work has focussed almost exclusively on evaluating news article summarization. How do zero-shot summarizers perform in other (potentially more specialized) domains? In this work we evaluate zero-shot generated summaries across specialized domains including biomedical articles, and legal bills (in addition to standard news benchmarks for reference). We focus especially on the factuality of outputs. We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors. We analyze whether the prevalence of a given domain in the pretraining corpus affects extractiveness and faithfulness of generated summaries of articles in this domain. We release all collected annotat
    
[^102]: 神经网络抽象和推理：迈向机器的广泛泛化

    Neural networks for abstraction and reasoning: Towards broad generalization in machines

    [https://arxiv.org/abs/2402.03507](https://arxiv.org/abs/2402.03507)

    这项工作探索了神经网络在广泛泛化方面的应用，通过研究解决抽象和推理任务的新方法，试图提高计算机系统从少量示例中学习新概念的能力。

    

    半个世纪以来，人工智能研究一直试图复制人类的抽象和推理能力-创造出能够从一小组示例中学习新概念的计算机系统，在人们发现这很容易的情况下。虽然特定的神经网络能够解决各种各样的问题，但对于超越训练数据范围的广泛泛化仍然是一个难题。在这项工作中，我们研究了几种解决抽象与推理语料库（ARC）的新方法，ARC是一个用于测试算法在广泛泛化方面的抽象视觉推理任务数据集。尽管有三个国际竞赛提供了10万美元的奖金，最好的算法仍然无法解决大多数ARC任务，并且依赖于复杂的手工规则，没有使用机器学习。我们重新思考了最近神经网络的进展是否能在这个任务上取得进展。首先，我们将DreamCoder神经符号推理求解器应用到ARC上。

    For half a century, artificial intelligence research has attempted to reproduce the human qualities of abstraction and reasoning - creating computer systems that can learn new concepts from a minimal set of examples, in settings where humans find this easy. While specific neural networks are able to solve an impressive range of problems, broad generalisation to situations outside their training data has proved elusive.In this work, we look at several novel approaches for solving the Abstraction & Reasoning Corpus (ARC), a dataset of abstract visual reasoning tasks introduced to test algorithms on broad generalization. Despite three international competitions with $100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks and rely on complex hand-crafted rules, without using machine learning at all. We revisit whether recent advances in neural networks allow progress on this task.   First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC. DreamCoder
    
[^103]: 基于修复和替换的服装与背景生成关键技术

    An Inpainting-Infused Pipeline for Attire and Background Replacement

    [https://arxiv.org/abs/2402.03501](https://arxiv.org/abs/2402.03501)

    本研究提出了一种基于修复和替换的服装与背景生成关键技术，利用GenAI和计算机视觉中的先进技术，通过深度估计、修复掩模创建和稳定扩散和潜在一致性模型（LCMs），实现了对个人照片中服装和背景的修改和修复。

    

    最近几年，生成式人工智能（GenAI）的突破性进展引发了一场变革性的范式转变，对各个领域产生了重大影响。本文具体探讨了一种综合方法，利用GenAI和计算机视觉中的先进技术来进行图像处理。该方法包括深度估计、基于深度信息创建修复掩模、利用稳定扩散和潜在一致性模型（LCMs）生成和替换背景，通过修复流程替换服装和应用审美修改。本研究中的实验验证了该方法的有效性，突出了其产生视觉吸引力内容的潜力。这些先进技术的融合使用户能够输入个人照片并对其进行服装和背景的修改。

    In recent years, groundbreaking advancements in Generative Artificial Intelligence (GenAI) have triggered a transformative paradigm shift, significantly influencing various domains. In this work, we specifically explore an integrated approach, leveraging advanced techniques in GenAI and computer vision emphasizing image manipulation. The methodology unfolds through several stages, including depth estimation, the creation of inpaint masks based on depth information, the generation and replacement of backgrounds utilizing Stable Diffusion in conjunction with Latent Consistency Models (LCMs), and the subsequent replacement of clothes and application of aesthetic changes through an inpainting pipeline. Experiments conducted in this study underscore the methodology's efficacy, highlighting its potential to produce visually captivating content. The convergence of these advanced techniques allows users to input photographs of individuals and manipulate them to modify clothing and background b
    
[^104]: 量子硬件错误下的课程强化学习量子架构搜索

    Curriculum reinforcement learning for quantum architecture search under hardware errors

    [https://arxiv.org/abs/2402.03500](https://arxiv.org/abs/2402.03500)

    本研究提出了一种基于课程的强化学习量子架构搜索算法（CRLQAS），解决了在噪声中间规模量子时代中，对于架构搜索的噪声效应的不理解的问题。

    

    在噪声中间规模量子时代，找到与当前设备限制兼容的有用电路是关键挑战。变分量子算法（VQAs）通过固定电路架构和优化外部循环中的个别门参数来提供潜在解决方案。然而，参数优化可能变得难以处理，并且算法的整体性能严重依赖初始选择的电路架构。已经开发了几种量子架构搜索（QAS）算法来自动设计有用的电路架构。在仅限参数优化的情况下，观察到噪声效应严重影响优化器和最终结果的性能，这是一条重要的研究线。然而，对于架构搜索的噪声效应，可能同样重要的问题目前还不太理解。本文通过引入基于课程的强化学习QAS（CRLQAS）算法来解决这个问题。

    The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations. Variational quantum algorithms (VQAs) offer a potential solution by fixing the circuit architecture and optimizing individual gate parameters in an external loop. However, parameter optimization can become intractable, and the overall performance of the algorithm depends heavily on the initially chosen circuit architecture. Several quantum architecture search (QAS) algorithms have been developed to design useful circuit architectures automatically. In the case of parameter optimization alone, noise effects have been observed to dramatically influence the performance of the optimizer and final outcomes, which is a key line of study. However, the effects of noise on the architecture search, which could be just as critical, are poorly understood. This work addresses this gap by introducing a curriculum-based reinforcement learning QAS (CRLQAS) algorithm desi
    
[^105]: 超越文字：通过语音线索改善LLM在机器人导航中的决策能力

    Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues

    [https://arxiv.org/abs/2402.03494](https://arxiv.org/abs/2402.03494)

    本论文通过将语音转录和语音非言语特征整合到LLM决策中来改善机器人导航中的决策能力，超越了仅使用文字的限制。

    

    这项工作强调了基于文本的大规模语言模型（LLM）在人机交互中的关键缺点，表明仅使用文本作为对话的模态在此类应用中存在不足之处。虽然LLM在处理文本方面在这些人机对话中非常出色，但在社交导航等情境下，他们在处理口头指令的细微之处时遇到了困难，其中的歧义和不确定性可能会削弱对机器人和其他人工智能系统的信任。我们可以通过超越文字，并重点关注这些音频回应的语音非言语特征来解决这个问题。这些特征是口头交流中不涉及文字措辞的方面，通过表达方式传达意义和细微差别。我们提出了“超越文字”；一种通过集成音频转录以及这些特征的部分来改善LLM决策能力的方法，这些特征侧重情感和更与人机对话相关。

    This work highlights a critical shortcoming in text-based Large Language Models (LLMs) used for human-robot interaction, demonstrating that text alone as a conversation modality falls short in such applications. While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and other AI systems. We can address this shortcoming by moving beyond text and additionally focusing on the paralinguistic features of these audio responses. These features are the aspects of spoken communication that do not involve the literal wording (lexical content) but convey meaning and nuance through how something is said. We present "Beyond Text"; an approach that improves LLM decision-making by integrating audio transcription along with a subsection of these features, which focus on the affect and more relevant in human-robot conversations. This approach n
    
[^106]: 临床环境中早期预测脓毒症的研究

    Early prediction of onset of sepsis in Clinical Setting

    [https://arxiv.org/abs/2402.03486](https://arxiv.org/abs/2402.03486)

    本研究提出了使用机器学习模型预测临床数据中脓毒症的早期发作，通过使用有监督学习方法训练XGBoost模型，并利用规范化效用分数评估模型的性能。

    

    本研究提出了使用机器学习模型，利用来自纽约布朗克斯Montefiore医疗中心的去标识化临床数据，预测脓毒症的早期发作。采用了有监督学习的方法，训练了一个XGBoost模型，使用了80%的训练数据集，包括107个特征（包括原始和衍生特征）。随后，该模型在剩余的20%的测试数据上进行了评估。模型在训练阶段完全未知的前瞻数据上进行了验证。为了评估模型在个体患者水平上的性能和预测的及时性，使用了规范化效用分数，这是脓毒症检测中广泛认可的评分方法，如PhysioNet Sepsis Challenge论文中所述。还设计了F1值、敏感性、特异性和标志率等指标。该模型在测试数据上的规范化效用分数为0.494，在前瞻数据上的规范化效用分数为0.378（阈值为0.3）。

    This study proposes the use of Machine Learning models to predict the early onset of sepsis using deidentified clinical data from Montefiore Medical Center in Bronx, NY, USA. A supervised learning approach was adopted, wherein an XGBoost model was trained utilizing 80\% of the train dataset, encompassing 107 features (including the original and derived features). Subsequently, the model was evaluated on the remaining 20\% of the test data. The model was validated on prospective data that was entirely unseen during the training phase. To assess the model's performance at the individual patient level and timeliness of the prediction, a normalized utility score was employed, a widely recognized scoring methodology for sepsis detection, as outlined in the PhysioNet Sepsis Challenge paper. Metrics such as F1 Score, Sensitivity, Specificity, and Flag Rate were also devised. The model achieved a normalized utility score of 0.494 on test data and 0.378 on prospective data at threshold 0.3. The
    
[^107]: SWAG: 带有行动指导的故事讲述

    SWAG: Storytelling With Action Guidance

    [https://arxiv.org/abs/2402.03483](https://arxiv.org/abs/2402.03483)

    SWAG是一种新的故事讲述方法，通过将故事写作简化为搜索问题，使用两个模型的反馈循环来指导故事的发展方向。在GPT-4和人工评估中，SWAG表现出显著的优势，并且使用仅开源模型的SWAG流程超过了GPT-3.5-Turbo。

    

    自动长篇故事生成通常使用长上下文大语言模型（LLMs）进行一次性创建，它可以产生连贯但不一定引人入胜的内容。我们引入了带有行动指导的故事讲述（SWAG）的新方法。我们的方法通过两个模型的反馈循环将故事写作简化为一个搜索问题：一个LLM生成故事内容，另一个辅助LLM用于选择下一个最佳的“行动”，以引导故事的未来发展方向。我们的结果表明，当使用GPT-4和人工评估进行评估时，SWAG能够显著优于以往的端到端故事生成技术，并且我们只使用开源模型的SWAG流程超越了GPT-3.5-Turbo。

    Automated long-form story generation typically employs long-context large language models (LLMs) for one-shot creation, which can produce cohesive but not necessarily engaging content. We introduce Storytelling With Action Guidance (SWAG), a novel approach to storytelling with LLMs. Our approach reduces story writing to a search problem through a two-model feedback loop: one LLM generates story content, and another auxiliary LLM is used to choose the next best "action" to steer the story's future direction. Our results show that SWAG can substantially outperform previous end-to-end story generation techniques when evaluated by GPT-4 and through human evaluation, and our SWAG pipeline using only open-source models surpasses GPT-3.5-Turbo.
    
[^108]: 为科学发现服务的兆级参数人工智能基础设施：一项调研和展望

    Trillion Parameter AI Serving Infrastructure for Scientific Discovery: A Survey and Vision

    [https://arxiv.org/abs/2402.03480](https://arxiv.org/abs/2402.03480)

    这篇论文调研了为科学研究提供支持的兆级参数人工智能基础设施，并描述了在系统设计中所面临的重要技术挑战和开放问题。

    

    深度学习方法正在改变研究工作，实现新技术，并最终带来新的发现。随着对更强大的人工智能模型的需求不断增长，我们正进入一个兆级参数模型（TPM）的时代，即具有超过一万亿个参数的模型，如华为的PanGu-$ \ Sigma $。我们描述了一个针对科学界特定需求的TPM用户和提供者生态系统的愿景。然后，我们概述了为提供TPM服务的系统设计中的重大技术挑战和开放问题。具体来说，我们描述了一个全面的软件堆栈和接口要求，以支持研究者多样化和灵活的需求。

    Deep learning methods are transforming research, enabling new techniques, and ultimately leading to new discoveries. As the demand for more capable AI models continues to grow, we are now entering an era of Trillion Parameter Models (TPM), or models with more than a trillion parameters -- such as Huawei's PanGu-$\Sigma$. We describe a vision for the ecosystem of TPM users and providers that caters to the specific needs of the scientific community. We then outline the significant technical challenges and open problems in system design for serving TPMs to enable scientific research and discovery. Specifically, we describe the requirements of a comprehensive software stack and interfaces to support the diverse and flexible requirements of researchers.
    
[^109]: ICED: 通过上下文环境设计实现强化学习的零样本迁移

    ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design

    [https://arxiv.org/abs/2402.03479](https://arxiv.org/abs/2402.03479)

    本研究探索了深度强化学习代理的零样本迁移能力，并发现通过根据值损失优先选择级别，可以改善代理的推广能力。此外，无监督环境设计方法对改善代理表现也具有重要作用。

    

    使用深度强化学习训练的自主代理通常缺乏成功地推广到新环境的能力，即使这些环境与它们在训练过程中遇到的环境具有相似的特征。本研究探讨了个体环境实例（或级别）的采样对强化学习代理的零样本推广能力的影响。我们发现，对于共享基本层的深度演员-评论家架构，根据其值损失优先选择级别，可以最小化代理的内部表示与生成的训练数据中的训练级别之间的互信息。这为某些自适应采样策略实现的隐式正则化提供了新颖的理论解释。然后，我们将注意力转向无监督环境设计（UED）方法，这些方法对数据生成机制具有更多控制。我们发现现有的UED方法可以显著改变训练数据中的环境实例，从而影响代理的表现能力。

    Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when they share characteristics with the environments they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the implicit regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which have more control over the data generation mechanism. We find that existing UED methods can significantly shift the trainin
    
[^110]: 大型语言模型几何信息的研究

    The Information of Large Language Model Geometry

    [https://arxiv.org/abs/2402.03471](https://arxiv.org/abs/2402.03471)

    论文研究了大型语言模型中嵌入的信息编码，并发现表示熵与模型大小呈幂律关系。通过信息理论和回归技术，建立了新标记的信息增益与岭回归之间的理论联系，并探索了Lasso回归在选择有意义的标记方面的有效性。实验结果表明，信息在标记之间分布，而不仅仅集中在特定的“有意义”的标记上。

    

    本文研究了大型语言模型中嵌入的信息编码。我们进行了模拟实验，分析了表示熵，并发现了与模型大小呈幂律关系的现象。基于这一观察，我们提出了一个基于（条件）熵的理论来解释这种规模定律现象。此外，我们深入探讨了LLMs的自回归结构，并使用信息理论和回归技术来分析最后一个标记与之前上下文标记之间的关系。具体而言，我们建立了新标记的信息增益与岭回归之间的理论联系。此外，我们还探索了Lasso回归在选择有意义的标记方面的有效性，有时表现优于紧密相关的注意力权重。最后，我们进行了对比实验，并发现信息分布在标记之间，而不仅仅集中在特定的“有意义”的标记上。

    This paper investigates the information encoded in the embeddings of large language models (LLMs). We conduct simulations to analyze the representation entropy and discover a power law relationship with model sizes. Building upon this observation, we propose a theory based on (conditional) entropy to elucidate the scaling law phenomenon. Furthermore, we delve into the auto-regressive structure of LLMs and examine the relationship between the last token and previous context tokens using information theory and regression techniques. Specifically, we establish a theoretical connection between the information gain of new tokens and ridge regression. Additionally, we explore the effectiveness of Lasso regression in selecting meaningful tokens, which sometimes outperforms the closely related attention weights. Finally, we conduct controlled experiments, and find that information is distributed across tokens, rather than being concentrated in specific "meaningful" tokens alone.
    
[^111]: 无偏好对齐学习与正则化相关奖励

    Preference-free Alignment Learning with Regularized Relevance Reward

    [https://arxiv.org/abs/2402.03469](https://arxiv.org/abs/2402.03469)

    无偏好对齐学习使用正则化相关奖励作为关键目标，在提供稳健奖励信号的同时，显著提高了偏好基准测试的性能。

    

    从人类偏好中学习被认为是将大规模语言模型（LLMs）与人类价值观对齐的关键。然而，与普遍的观点相反，我们的初步研究发现，基于人类偏好数据集训练的奖励模型倾向于给长的与主题无关的回复更高的分数，而给短的与主题相关的回复较低分。在这一观察的驱动下，我们探索了一种无偏好的方法，利用“相关性”作为对齐的一个关键目标。在我们的第一次尝试中，我们发现仅仅通过检索得到的相关性得分容易受到奖励欺骗的影响，即过度优化到不期望的捷径上，当我们将该得分作为奖励用于强化学习。为了缓解这个问题，我们将有效的归纳偏差整合到常规的相关性中，互相正则化，形成了一种混合奖励函数：正则化相关奖励（$R^3$）。$R^3$通过提供稳健的奖励信号，显著提高了在偏好基准测试中的性能。值得注意的是，$R^3$不需要

    Learning from human preference has been considered key to aligning Large Language Models (LLMs) with human values. However, contrary to popular belief, our preliminary study reveals that reward models trained on human preference datasets tend to give higher scores to long off-topic responses than short on-topic ones. Motivated by this observation, we explore a preference-free approach utilizing `relevance' as a key objective for alignment. On our first attempt, we find that the relevance score obtained by a retriever alone is vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when we utilize the score as a reward for reinforcement learning. To mitigate it, we integrate effective inductive biases into the vanilla relevance to regularize each other, resulting in a mixture of reward functions: Regularized Relevance Reward ($R^3$). $R^3$ significantly improves performance on preference benchmarks by providing a robust reward signal. Notably, $R^3$ does not require a
    
[^112]: 使用大型语言模型进行心理评估：注重隐私和具有成本效益的方法

    Psychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach

    [https://arxiv.org/abs/2402.03435](https://arxiv.org/abs/2402.03435)

    本研究使用大型语言模型（LLMs）分析Reddit用户的文本评论，以实现隐私保护和成本效益为重点，通过精心设计的提示和语法来实现预定义的自杀风险心理评估，取得了杰出的结果。

    

    本研究探索了使用大型语言模型（LLMs）分析Reddit用户的文本评论，主要目标有两个：第一，确定支持预定义的自杀风险心理评估的关键摘录；第二，总结材料以证实预分配的自杀风险水平。该工作局限于使用可以在本地运行的“开源”LLMs，从而增强数据隐私。此外，它优先选用计算要求较低的模型，以使有限的计算预算的个人和机构能够使用。实施的策略仅依赖于精心设计的提示和语法，以指导LLM的文本完成。尽管简单，评估指标显示出杰出的结果，使之成为一种有价值的注重隐私和具有成本效益的方法。此工作是2024 Computational Linguistics and Clinical Psychology (CLPsych)共享任务的一部分。

    This study explores the use of Large Language Models (LLMs) to analyze text comments from Reddit users, aiming to achieve two primary objectives: firstly, to pinpoint critical excerpts that support a predefined psychological assessment of suicidal risk; and secondly, to summarize the material to substantiate the preassigned suicidal risk level. The work is circumscribed to the use of "open-source" LLMs that can be run locally, thereby enhancing data privacy. Furthermore, it prioritizes models with low computational requirements, making it accessible to both individuals and institutions operating on limited computing budgets. The implemented strategy only relies on a carefully crafted prompt and a grammar to guide the LLM's text completion. Despite its simplicity, the evaluation metrics show outstanding results, making it a valuable privacy-focused and cost-effective approach. This work is part of the Computational Linguistics and Clinical Psychology (CLPsych) 2024 shared task.
    
[^113]: UniTSyn：一个可增强大型语言模型在程序测试中的能力的大规模数据集

    UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing

    [https://arxiv.org/abs/2402.03396](https://arxiv.org/abs/2402.03396)

    本研究提出了UniTSyn，一个大型数据集，可以增强大型语言模型在程序测试中的能力。通过关联测试和被测试函数，UniTSyn能够提高模型在生成准确和完整的测试方面的表现。

    

    大型语言模型（LLM）在生成高质量代码方面的出色能力已经引起了软件测试社区的广泛关注。然而，现有的代码LLM在生成准确和完整的测试方面往往表现不佳，因为它们是在不区分测试目的代码和其他代码的情况下进行训练的。在本文中，我们提出了一个大规模数据集UniTSyn，它能够增强LLM在单元测试合成方面的能力。将测试与被测试函数进行关联对于LLM推断预期行为和要验证的逻辑路径至关重要。通过利用语言服务器协议，UniTSyn实现了在没有每个项目执行设置或易碎且难以扩展的每个语言启发式的情况下收集焦点测试对的挑战目标。它包含了五种主流编程语言的270万个焦点测试对，使其能够被应用于优化大型语言模型的能力。

    The remarkable capability of large language models (LLMs) in generating high-quality code has drawn increasing attention in the software testing community. However, existing code LLMs often demonstrate unsatisfactory capabilities in generating accurate and complete tests since they were trained on code snippets collected without differentiating between code for testing purposes and other code. In this paper, we present a large-scale dataset UniTSyn, which is capable of enhancing the prowess of LLMs for Unit Test Synthesis. Associating tests with the tested functions is crucial for LLMs to infer the expected behavior and the logic paths to be verified. By leveraging Language Server Protocol, UniTSyn achieves the challenging goal of collecting focal-test pairs without per-project execution setups or per-language heuristics that tend to be fragile and difficult to scale. It contains 2.7 million focal-test pairs across five mainstream programming languages, making it possible to be utilize
    
[^114]: PixelGen：重新思考嵌入式摄像系统

    PixelGen: Rethinking Embedded Camera Systems

    [https://arxiv.org/abs/2402.03390](https://arxiv.org/abs/2402.03390)

    PixelGen是一个重新构想嵌入式摄像系统的平台，通过结合传感器、收发器和低分辨率摄像头和红外视觉传感器，它能够捕捉到更广泛的世界表达，并且通过使用transformer-based图像和语言模型，这些简单的数据也可以产生出环境的新颖表达。

    

    嵌入式摄像系统是无处不在的，是无线嵌入式系统的最广泛使用的例子。它们捕捉了世界的表达 - 由可见光或红外光照亮的周围环境。尽管它们被广泛使用，但嵌入式摄像系统的架构仍然没有改变，这导致了一些限制。它们只可视化了世界的一小部分。此外，它们消耗能量高，导致电池寿命有限。我们提出了PixelGen，它重新构想了嵌入式摄像系统。具体而言，PixelGen结合了传感器、收发器和低分辨率图像和红外视觉传感器，以捕捉更广泛的世界表达。它们被有意选择出来，因为它们简单、低比特率和功耗低，最终形成了一种高能效的平台。我们展示了，尽管简单，捕获的数据可以使用基于transformer的图像和语言模型进行处理，生成环境的新颖表达。

    Embedded camera systems are ubiquitous, representing the most widely deployed example of a wireless embedded system. They capture a representation of the world - the surroundings illuminated by visible or infrared light. Despite their widespread usage, the architecture of embedded camera systems has remained unchanged, which leads to limitations. They visualize only a tiny portion of the world. Additionally, they are energy-intensive, leading to limited battery lifespan. We present PixelGen, which re-imagines embedded camera systems. Specifically, PixelGen combines sensors, transceivers, and low-resolution image and infrared vision sensors to capture a broader world representation. They are deliberately chosen for their simplicity, low bitrate, and power consumption, culminating in an energy-efficient platform. We show that despite the simplicity, the captured data can be processed using transformer-based image and language models to generate novel representations of the environment. F
    
[^115]: 在预算限制下行为用户分割中的优化传递发现

    Delivery Optimized Discovery in Behavioral User Segmentation under Budget Constrain

    [https://arxiv.org/abs/2402.03388](https://arxiv.org/abs/2402.03388)

    在预算限制下，我们提出了一种基于随机优化的算法，用于优化传递发现行为用户细分。

    

    用户在线行为足迹可以使公司发现基于行为的用户细分，并向用户发送特定细分的信息。在发现细分之后，通过像Facebook和Google这样的首选媒体渠道向用户发送信息可能具有挑战性，因为只有部分行为细分中的用户在媒体上找到匹配，并且只有其中一小部分看到消息（曝光）。即使高质量的发现也会在传递失败时变得无用。许多复杂的算法用于发现行为细分，然而这些算法忽略了传递组件。问题变得复杂是因为（i）发现是在公司数据（例如用户点击）的行为数据空间中进行的，而传递则是基于媒体定义的静态数据空间（例如地理位置，年龄）进行的；（ii）公司在预算限制下运作。我们引入了一种基于随机优化的算法，用于在预算限制下优化传递发现行为用户细分。

    Users' behavioral footprints online enable firms to discover behavior-based user segments (or, segments) and deliver segment specific messages to users. Following the discovery of segments, delivery of messages to users through preferred media channels like Facebook and Google can be challenging, as only a portion of users in a behavior segment find match in a medium, and only a fraction of those matched actually see the message (exposure). Even high quality discovery becomes futile when delivery fails. Many sophisticated algorithms exist for discovering behavioral segments; however, these ignore the delivery component. The problem is compounded because (i) the discovery is performed on the behavior data space in firms' data (e.g., user clicks), while the delivery is predicated on the static data space (e.g., geo, age) as defined by media; and (ii) firms work under budget constraint. We introduce a stochastic optimization based algorithm for delivery optimized discovery of behavioral u
    
[^116]: 基于神经网络结构的通用决策树集成：分布式梯度提升森林（DGBF）

    A generalized decision tree ensemble based on the NeuralNetworks architecture: Distributed Gradient Boosting Forest (DGBF)

    [https://arxiv.org/abs/2402.03386](https://arxiv.org/abs/2402.03386)

    本文提出了一种基于神经网络结构的通用决策树集成算法，分布式梯度提升森林（DGBF），通过将包和提升的数学公式结合起来，实现了树之间自然地进行分布式表示学习过程。该算法能够处理离散或表格数据，并具有建模非结构化数据的能力。

    

    目前，随机森林和梯度提升是建模离散或表格数据的主要方法，然而，由于它们的数学特性，无法像神经网络那样从原始数据中进行分层表示学习，这是深度学习问题和建模非结构化数据的关键特征。然而，在本文中，我们证明了包和提升的数学公式可以合并在一起，定义一个图结构的树集成算法，并在树之间自然地进行分布式表示学习过程（无需使用反向传播）。我们将这种新颖的方法称为分布式梯度提升森林（DGBF），并证明了随机森林和梯度提升都可以表示为DGBF的特定图结构。最后，我们发现...（摘要截断）

    Tree ensemble algorithms as RandomForest and GradientBoosting are currently the dominant methods for modeling discrete or tabular data, however, they are unable to perform a hierarchical representation learning from raw data as NeuralNetworks does thanks to its multi-layered structure, which is a key feature for DeepLearning problems and modeling unstructured data. This limitation is due to the fact that tree algorithms can not be trained with back-propagation because of their mathematical nature. However, in this work, we demonstrate that the mathematical formulation of bagging and boosting can be combined together to define a graph-structured-tree-ensemble algorithm with a distributed representation learning process between trees naturally (without using back-propagation). We call this novel approach Distributed Gradient Boosting Forest (DGBF) and we demonstrate that both RandomForest and GradientBoosting can be expressed as particular graph architectures of DGBT. Finally, we see tha
    
[^117]: 使用迁移学习预测胶质瘤的生存和等级

    Survival and grade of the glioma prediction using transfer learning

    [https://arxiv.org/abs/2402.03384](https://arxiv.org/abs/2402.03384)

    本研究利用迁移学习技术，通过对胶质母细胞瘤图像数据集进行微调，成功实现了生存和肿瘤等级预测，生存预测准确率达到65%，肿瘤等级预测准确率达到97%。

    

    胶质母细胞瘤是一种高度恶性的脑肿瘤，没有治疗的情况下预期寿命仅为3至6个月。准确检测和预测其生存和等级至关重要。本研究引入了一种使用迁移学习技术的新方法。通过详尽的优化，测试了各种预训练的网络，包括EfficientNet、ResNet、VGG16和Inception，以找到最适合的架构。迁移学习应用于对胶质母细胞瘤图像数据集进行微调，旨在实现两个目标：生存预测和肿瘤等级预测。实验结果显示，生存预测准确率达到65%，将患者分为短期、中期和长期生存类别。此外，肿瘤等级的预测准确率达到97%，准确区分低等级胶质母细胞瘤(LGG)和高等级胶质母细胞瘤(HGG)。这种方法的成功归因于迁移学习的有效性，超越了当前的研究水平。

    Glioblastoma is a highly malignant brain tumor with a life expectancy of only 3 to 6 months without treatment. Detecting and predicting its survival and grade accurately are crucial. This study introduces a novel approach using transfer learning techniques. Various pre-trained networks, including EfficientNet, ResNet, VGG16, and Inception, were tested through exhaustive optimization to identify the most suitable architecture. Transfer learning was applied to fine-tune these models on a glioblastoma image dataset, aiming to achieve two objectives: survival and tumor grade prediction.The experimental results show 65% accuracy in survival prediction, classifying patients into short, medium, or long survival categories. Additionally, the prediction of tumor grade achieved an accuracy of 97%, accurately differentiating low-grade gliomas (LGG) and high-grade gliomas (HGG). The success of the approach is attributed to the effectiveness of transfer learning, surpassing the current state-of-the
    
[^118]: 全链路上升建模与上下文增强学习用于智能营销

    Entire Chain Uplift Modeling with Context-Enhanced Learning for Intelligent Marketing

    [https://arxiv.org/abs/2402.03379](https://arxiv.org/abs/2402.03379)

    全链路上升建模方法ECUP旨在解决链路偏差和处理不适应问题，在线营销中有重要的应用价值。

    

    上升建模在在线营销中非常重要，它旨在通过预测个体处理效果（ITE）来准确衡量不同策略（如优惠券或折扣）对不同用户的影响。在电子商务环境中，用户行为遵循确定的顺序链路，包括展示、点击和转化。营销策略在这个链路中的每个阶段都会产生不同的上升效应，影响着点击率和转化率等指标。尽管其实用性，现有研究忽视了特定处理中所有阶段的相互影响，并未充分利用处理信息，可能给后续的营销决策引入了重大偏差。本文将这两个问题称为链路偏差问题和处理不适应问题。本文介绍了一种用于解决这些问题的具有上下文增强学习的全链路上升方法（ECUP）。ECUP包括两个主要组成部分：

    Uplift modeling, vital in online marketing, seeks to accurately measure the impact of various strategies, such as coupons or discounts, on different users by predicting the Individual Treatment Effect (ITE). In an e-commerce setting, user behavior follows a defined sequential chain, including impression, click, and conversion. Marketing strategies exert varied uplift effects at each stage within this chain, impacting metrics like click-through and conversion rate. Despite its utility, existing research has neglected to consider the inter-task across all stages impacts within a specific treatment and has insufficiently utilized the treatment information, potentially introducing substantial bias into subsequent marketing decisions. We identify these two issues as the chain-bias problem and the treatment-unadaptive problem. This paper introduces the Entire Chain UPlift method with context-enhanced learning (ECUP), devised to tackle these issues. ECUP consists of two primary components: 1)
    
[^119]: BetterV: 通过有区分度的引导实现可控的Verilog生成

    BetterV: Controlled Verilog Generation with Discriminative Guidance

    [https://arxiv.org/abs/2402.03375](https://arxiv.org/abs/2402.03375)

    本文提出了一种Verilog生成框架BetterV，通过微调大型语言模型和生成判别器的使用，实现了可控的Verilog生成，能够生成语法和功能正确的Verilog实现。

    

    由于现代集成电路（IC）日益复杂，需要自动化的电路设计方法。近年来，硬件设计语言生成方面的研究不断增多，以便促进设计过程。在本文中，我们提出了一个Verilog生成框架BetterV，该框架在处理特定领域数据集的基础上对大型语言模型（LLMs）进行微调，并结合生成判别器以指导特定设计需求。Verilog模块是从互联网中收集、过滤和处理，形成了一个干净而丰富的数据集。通过Instruct-tuning方法，对LLMs进行精细调整，以了解关于Verilog的知识。此外，通过数据增强丰富训练集，并用于训练特定下游任务的生成判别器，为LLMs优化Verilog实现提供指导。BetterV能够生成语法和功能上正确的Verilog实现。

    Due to the growing complexity of modern Integrated Circuits (ICs), there is a need for automated circuit design methods. Recent years have seen rising research in hardware design language generation to facilitate the design process. In this work, we propose a Verilog generation framework, BetterV, which fine-tunes the large language models (LLMs) on processed domain-specific datasets and incorporates generative discriminators for guidance on particular design demands. The Verilog modules are collected, filtered and processed from internet to form a clean and abundant dataset. Instruct-tuning methods are specially designed to fine-tuned the LLMs to understand the knowledge about Verilog. Furthermore, data are augmented to enrich the training set and also used to train a generative discriminator on particular downstream task, which leads a guidance for the LLMs to optimize the Verilog implementation. BetterV has the ability to generate syntactically and functionally correct Verilog, whic
    
[^120]: 检测科学文献中的拙劣短语

    Detection of tortured phrases in scientific literature

    [https://arxiv.org/abs/2402.03370](https://arxiv.org/abs/2402.03370)

    本文介绍了自动检测科学论文中拙劣短语的方法，通过语言模型和预测分数的传播，可以高效地标记并提取这些拙劣短语，为领域专家验证提供新的数据。

    

    本文介绍了多种自动检测方法，用于从科学论文中提取所谓的拙劣短语。这些拙劣短语，例如将"信号与噪声"替换为"旗帜与喧闹"，是使用改写工具规避抄袭检测的结果。我们构建了一个数据集，并评估了几种策略来标记以前未记录的拙劣短语。提出和测试的方法基于语言模型，要么基于嵌入相似性，要么基于掩码标记的预测。我们发现，一种使用标记预测并将分数传播到块级别的方法效果最好。其召回率为0.87，精确率为0.61，可以检索到新的拙劣短语以供领域专家验证。

    This paper presents various automatic detection methods to extract so called tortured phrases from scientific papers. These tortured phrases, e.g. flag to clamor instead of signal to noise, are the results of paraphrasing tools used to escape plagiarism detection. We built a dataset and evaluated several strategies to flag previously undocumented tortured phrases. The proposed and tested methods are based on language models and either on embeddings similarities or on predictions of masked token. We found that an approach using token prediction and that propagates the scores to the chunk level gives the best results. With a recall value of .87 and a precision value of .61, it could retrieve new tortured phrases to be submitted to domain experts for validation.
    
[^121]: 推荐系统中大数据的实证和实验研究：一项综合调查

    Empirical and Experimental Perspectives on Big Data in Recommendation Systems: A Comprehensive Survey

    [https://arxiv.org/abs/2402.03368](https://arxiv.org/abs/2402.03368)

    本综合调查论文对推荐系统中的大数据算法进行了全面分析，并提出了一种新颖的、分层的分类法。通过该分类法，研究人员可以全面了解不同算法和技术之间的相互关系。

    

    本综合调查论文对推荐系统中的大数据算法进行了全面分析，解决了现有文献中深度和精确性不足的问题。它提出了一种双管齐下的方法：对当前算法进行彻底分析，并提出了一种新颖的、分层的分类法以实现精确归类。该分类法基于三层层次结构，从方法学类别开始，逐步细化为具体技术。这样的框架允许对算法进行结构化和全面的分类，帮助研究人员理解不同算法和技术之间的相互关系。论文涵盖了广泛的算法，首先将算法分为四种主要分析类型：基于用户和项目相似度的方法、混合和综合方法、深度学习和算法方法、数学建模方法，进一步细分为子类别和技术。论文融合了实证和实验视角。

    This survey paper provides a comprehensive analysis of big data algorithms in recommendation systems, addressing the lack of depth and precision in existing literature. It proposes a two-pronged approach: a thorough analysis of current algorithms and a novel, hierarchical taxonomy for precise categorization. The taxonomy is based on a tri-level hierarchy, starting with the methodology category and narrowing down to specific techniques. Such a framework allows for a structured and comprehensive classification of algorithms, assisting researchers in understanding the interrelationships among diverse algorithms and techniques. Covering a wide range of algorithms, this taxonomy first categorizes algorithms into four main analysis types: User and Item Similarity-Based Methods, Hybrid and Combined Approaches, Deep Learning and Algorithmic Methods, and Mathematical Modeling Methods, with further subdivisions into sub-categories and techniques. The paper incorporates both empirical and experim
    
[^122]: 带有大型语言模型的不确定性感知可解释推荐

    Uncertainty-Aware Explainable Recommendation with Large Language Models

    [https://arxiv.org/abs/2402.03366](https://arxiv.org/abs/2402.03366)

    这项研究开发了一个模型，通过训练用户和项目输入的ID向量作为提示，利用GPT-2实现不确定性感知的可解释推荐系统。该系统采用联合训练机制并在多任务学习框架中进行优化，能够更有效地探索用户的兴趣，提高推荐效果和用户满意度。

    

    在推荐系统内提供解释能够提升用户满意度并建立信任，特别是通过详细说明为用户定制推荐项目的原因。当前领域中主要的方法是生成基于文本的解释，而大型语言模型（LLMs）的应用尤为突出。然而，由于时间和计算资源限制，改进LLMs以实现可解释的推荐在实践上是不可行的。作为替代方案，当前的方法是训练提示而不是LLM。在这项研究中，我们开发了一个模型，利用用户和项目输入的ID向量作为GPT-2的提示。我们在多任务学习框架中采用联合训练机制，优化推荐任务和解释任务。这种策略能够更有效地探索用户的兴趣，提高推荐效果和用户满意度。通过实验，我们的方法表现出...

    Providing explanations within the recommendation system would boost user satisfaction and foster trust, especially by elaborating on the reasons for selecting recommended items tailored to the user. The predominant approach in this domain revolves around generating text-based explanations, with a notable emphasis on applying large language models (LLMs). However, refining LLMs for explainable recommendations proves impractical due to time constraints and computing resource limitations. As an alternative, the current approach involves training the prompt rather than the LLM. In this study, we developed a model that utilizes the ID vectors of user and item inputs as prompts for GPT-2. We employed a joint training mechanism within a multi-task learning framework to optimize both the recommendation task and explanation task. This strategy enables a more effective exploration of users' interests, improving recommendation effectiveness and user satisfaction. Through the experiments, our meth
    
[^123]: NanoNER: 使用领域专家知识和远程监督进行纳米生物学的命名实体识别

    NanoNER: Named Entity Recognition for nanobiology using experts' knowledge and distant supervision

    [https://arxiv.org/abs/2402.03362](https://arxiv.org/abs/2402.03362)

    本文介绍了NanoNER，它是一种用于纳米生物学的命名实体识别模型。通过使用领域专家的知识和远程监督学习，NanoNER能够准确地识别先前已知实体，并最大程度地提高注释数据的质量和数量。

    

    本文介绍了NanoNER的训练和评估，它是一种用于纳米生物学的命名实体识别（NER）模型。NER是在非结构化文本中识别特定实体的任务，在自然语言处理（NLP）和信息提取中经常是一个主要任务。我们的模型的目的是识别领域专家之前确定为该领域基本知识的实体。我们依靠本体论来提供领域词汇和分类，实现了一个迭代过程，使专家能够确定与当前领域相关的实体。然后我们深入探讨了远程监督学习在NER中的潜力，支持这种方法如何可以通过最少的人力增加注释数据的数量。在包含超过120k实体出现次数的728篇全文纳米生物学文章的完整语料库上，NanoNER在先前已知实体的识别上获得了0.98的F1分数。

    Here we present the training and evaluation of NanoNER, a Named Entity Recognition (NER) model for Nanobiology. NER consists in the identification of specific entities in spans of unstructured texts and is often a primary task in Natural Language Processing (NLP) and Information Extraction. The aim of our model is to recognise entities previously identified by domain experts as constituting the essential knowledge of the domain. Relying on ontologies, which provide us with a domain vocabulary and taxonomy, we implemented an iterative process enabling experts to determine the entities relevant to the domain at hand. We then delve into the potential of distant supervision learning in NER, supporting how this method can increase the quantity of annotated data with minimal additional manpower. On our full corpus of 728 full-text nanobiology articles, containing more than 120k entity occurrences, NanoNER obtained a F1-score of 0.98 on the recognition of previously known entities. Our model 
    
[^124]: 图缩减的综合调研：稀疏化、粗化和浓缩

    A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation

    [https://arxiv.org/abs/2402.03358](https://arxiv.org/abs/2402.03358)

    这篇综述调研了图缩减方法，包括稀疏化、粗化和浓缩，在解决大型图形数据分析和计算复杂性方面起到了重要作用。调研对这些方法的技术细节进行了系统的回顾，并强调了它们在实际应用中的关键性。同时，调研还提出了保证图缩减技术持续有效性的关键研究方向。

    

    许多真实世界的数据集可以自然地表示为图，涵盖了广泛的领域。然而，图数据集的复杂性和规模的增加为分析和计算带来了显著的挑战。为此，图缩减技术在保留关键属性的同时简化大型图形数据变得越来越受关注。在本调研中，我们旨在提供对图缩减方法的全面理解，包括图稀疏化、图粗化和图浓缩。具体而言，我们建立了这些方法的统一定义，并引入了一个分层分类法来分类这些方法所解决的挑战。我们的调研系统地回顾了这些方法的技术细节，并强调了它们在各种场景中的实际应用。此外，我们还概述了保证图缩减技术持续有效性的关键研究方向，并提供了一个详细的论文列表链接。

    Many real-world datasets can be naturally represented as graphs, spanning a wide range of domains. However, the increasing complexity and size of graph datasets present significant challenges for analysis and computation. In response, graph reduction techniques have gained prominence for simplifying large graphs while preserving essential properties. In this survey, we aim to provide a comprehensive understanding of graph reduction methods, including graph sparsification, graph coarsening, and graph condensation. Specifically, we establish a unified definition for these methods and introduce a hierarchical taxonomy to categorize the challenges they address. Our survey then systematically reviews the technical details of these methods and emphasizes their practical applications across diverse scenarios. Furthermore, we outline critical research directions to ensure the continued effectiveness of graph reduction techniques, as well as provide a comprehensive paper list at https://github.
    
[^125]: 利用网络效应减轻假新闻传播：通过自我模仿学习选择揭露者

    Harnessing Network Effect for Fake News Mitigation: Selecting Debunkers via Self-Imitation Learning

    [https://arxiv.org/abs/2402.03357](https://arxiv.org/abs/2402.03357)

    本研究针对假新闻在社交网络中的影响，通过部署揭露者传播真实新闻，提出了一种通过自我模仿学习选择揭露者的方法。通过NAGASIL算法，能够在假新闻减轻中学习更有效的揭露者选择策略。

    

    本研究旨在通过部署揭露者传播真实新闻，最小化假新闻对社交网络的影响。这被设定为一种强化学习问题，每个阶段选取一个用户传播真实的新闻。一个具有挑战性的问题是一次性奖励，在社交网络上交织信息传播中，无法区分单个揭露者的选择所带来的“净”效应，只能观察到来自减轻努力的集体效果。现有的自我模仿学习（SIL）方法在从一次性奖励中学习方面显示出很大的潜力，但在实际的假新闻减轻应用中由于样本效率低下，不适用。为了学习更有效的假新闻减轻揭露者选择策略，本研究提出了NAGASIL - 基于负采样和状态增强生成对抗自我模仿学习，它包括两个针对假新闻减轻的改进:从负样本中学习和状态增强。

    This study aims to minimize the influence of fake news on social networks by deploying debunkers to propagate true news. This is framed as a reinforcement learning problem, where, at each stage, one user is selected to propagate true news. A challenging issue is episodic reward where the "net" effect of selecting individual debunkers cannot be discerned from the interleaving information propagation on social networks, and only the collective effect from mitigation efforts can be observed. Existing Self-Imitation Learning (SIL) methods have shown promise in learning from episodic rewards, but are ill-suited to the real-world application of fake news mitigation because of their poor sample efficiency. To learn a more effective debunker selection policy for fake news mitigation, this study proposes NAGASIL - Negative sampling and state Augmented Generative Adversarial Self-Imitation Learning, which consists of two improvements geared towards fake news mitigation: learning from negative sa
    
[^126]: 在犯罪网络中识别犯罪头目的技术：一项调查、实验和比较评估

    Techniques to Detect Crime Leaders within a Criminal Network: A Survey, Experimental, and Comparative Evaluations

    [https://arxiv.org/abs/2402.03355](https://arxiv.org/abs/2402.03355)

    本论文调查、分析了在犯罪网络中识别犯罪头目的技术和算法，提出了一种新的方法论分类体系，通过实证评估和实验比较，为研究人员提供了全面的决策支持。

    

    本调查论文对在犯罪网络中识别犯罪头目所使用的技术和算法进行了全面分析。针对每种技术，论文讨论了其有效性、局限性、改进潜力和未来前景。现有关注于识别犯罪头目和预测犯罪算法的调查论文所面临的主要挑战是如何有效地对这些算法进行分类。为了解决这个问题，本论文提出了一种新的方法论分类体系，将算法分为更详细的类别和具体技术。论文通过实证评估和实验比较对不同技术进行了排名。方法论分类体系、实证评估和实验比较的结合使人们能够全面而细致地了解识别犯罪头目的技术和算法，为研究人员提供明智的决策支持。此外，论文还提供了其他方面的贡献。

    This survey paper offers a thorough analysis of techniques and algorithms used in the identification of crime leaders within criminal networks. For each technique, the paper examines its effectiveness, limitations, potential for improvement, and future prospects. The main challenge faced by existing survey papers focusing on algorithms for identifying crime leaders and predicting crimes is effectively categorizing these algorithms. To address this limitation, this paper proposes a new methodological taxonomy that hierarchically classifies algorithms into more detailed categories and specific techniques. The paper includes empirical and experimental evaluations to rank the different techniques. The combination of the methodological taxonomy, empirical evaluations, and experimental comparisons allows for a nuanced and comprehensive understanding of the techniques and algorithms for identifying crime leaders, assisting researchers in making informed decisions. Moreover, the paper offers v
    
[^127]: 当地球科学遇上生成AI和大型语言模型：基础、趋势和未来挑战

    When Geoscience Meets Generative AI and Large Language Models: Foundations, Trends, and Future Challenges

    [https://arxiv.org/abs/2402.03349](https://arxiv.org/abs/2402.03349)

    这篇论文探讨了生成AI和大型语言模型在地球科学中的潜在应用，并讨论了几种已在地球科学中使用的GAI模型，包括生成对抗网络（GANs）、基于物理的模型等。

    

    生成人工智能（GAI）代表着一个新兴领域，承诺在不同的模态中创造合成数据和输出。 GAI最近在生物学、医学、教育、立法、计算机科学和金融等多个应用领域展示了令人印象深刻的成果。 为了实现增强的安全性、效率和可持续性，生成AI确实成为一个关键的差异化因素，并承诺在该领域引起模式转变。 本文探讨了生成AI和大型语言模型在地球科学中的潜在应用。 机器学习和深度学习领域的最新发展使得生成模型能够应对与地球科学和地球系统动力学相关的多样化预测问题、模拟和多标准决策挑战。 本综述讨论了在地球科学中使用的几种GAI模型，包括生成对抗网络（GANs）、基于物理的模型等。

    Generative Artificial Intelligence (GAI) represents an emerging field that promises the creation of synthetic data and outputs in different modalities. GAI has recently shown impressive results across a large spectrum of applications ranging from biology, medicine, education, legislation, computer science, and finance. As one strives for enhanced safety, efficiency, and sustainability, generative AI indeed emerges as a key differentiator and promises a paradigm shift in the field. This paper explores the potential applications of generative AI and large language models in geoscience. The recent developments in the field of machine learning and deep learning have enabled the generative model's utility for tackling diverse prediction problems, simulation, and multi-criteria decision-making challenges related to geoscience and Earth system dynamics. This survey discusses several GAI models that have been used in geoscience comprising generative adversarial networks (GANs), physics-informe
    
[^128]: 尊重模型: 细粒度且鲁棒的解释与共享比例分解

    Respect the model: Fine-grained and Robust Explanation with Sharing Ratio Decomposition

    [https://arxiv.org/abs/2402.03348](https://arxiv.org/abs/2402.03348)

    本论文提出了一种称为共享比例分解(SRD)的新颖解释方法，真实地反映了模型的推理过程，并在解释方面显著提高了鲁棒性。通过采用向量视角和考虑滤波器之间的复杂非线性交互，以及引入仅激活模式预测(APOP)方法，可以重新定义相关性并强调非活跃神经元的重要性。

    

    对现有的解释方法能否真实阐明模型决策过程的真实性提出了质疑。现有方法偏离了对模型的忠实表达，因此容易受到对抗性攻击的影响。为了解决这个问题，我们提出了一种新颖的可解释性人工智能(XAI)方法，称为SRD(共享比例分解)，它真实地反映了模型的推理过程，从而显著提高了解释的鲁棒性。与传统的神经元级别强调不同，我们采用向量视角来考虑滤波器之间复杂的非线性交互。我们还引入了一个有趣的观察，称为仅激活模式预测(APOP)，让我们强调非活跃神经元的重要性，并重新定义相关性，包括活跃和非活跃神经元的所有相关信息。我们的方法SRD允许递归分解一个点特征向量(PFV)。

    The truthfulness of existing explanation methods in authentically elucidating the underlying model's decision-making process has been questioned. Existing methods have deviated from faithfully representing the model, thus susceptible to adversarial attacks. To address this, we propose a novel eXplainable AI (XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects the model's inference process, resulting in significantly enhanced robustness in our explanations. Different from the conventional emphasis on the neuronal level, we adopt a vector perspective to consider the intricate nonlinear interactions between filters. We also introduce an interesting observation termed Activation-Pattern-Only Prediction (APOP), letting us emphasize the importance of inactive neurons and redefine relevance encapsulating all relevant information including both active and inactive neurons. Our method, SRD, allows for the recursive decomposition of a Pointwise Feature Vector (PFV), pr
    
[^129]: 使用Densenet201架构模型的迁移学习进行土豆叶病分类

    Transfer Learning With Densenet201 Architecture Model For Potato Leaf Disease Classification

    [https://arxiv.org/abs/2402.03347](https://arxiv.org/abs/2402.03347)

    本文使用Densenet201架构模型，结合迁移学习方法，进行土豆叶病分类。

    

    土豆植物对人类有益。像其他植物一样，土豆植物也会生病；如果不及时治疗，食物产量将显著下降。因此，需要快速准确地检测病害，以便有效高效地进行病害控制。土豆叶病的分类可以直接进行，但症状并不能始终解释攻击土豆叶的病害类型，因为有很多种病害的症状看起来一样。人类在确定土豆叶病鉴定结果时也存在问题，因此，有时个体间的鉴定结果可能不同。因此，使用深度学习进行土豆叶病分类的方法有望缩短时间并具有较高的分类准确性。本研究使用了DenseNet201架构的深度学习方法。

    Potato plants are plants that are beneficial to humans. Like other plants in general, potato plants also have diseases; if this disease is not treated immediately, there will be a significant decrease in food production. Therefore, it is necessary to detect diseases quickly and precisely so that disease control can be carried out effectively and efficiently. Classification of potato leaf disease can be done directly. Still, the symptoms cannot always explain the type of disease that attacks potato leaves because there are many types of diseases with symptoms that look the same. Humans also have deficiencies in determining the results of identification of potato leaf disease, so sometimes the results of identification between individuals can be different. Therefore, the use of Deep Learning for the classification process of potato leaf disease is expected to shorten the time and have a high classification accuracy. This study uses a deep learning method with the DenseNet201 architecture
    
[^130]: 强化学习的机器人帆船：模拟器和初步结果

    Reinforcement-learning robotic sailboats: simulator and preliminary results

    [https://arxiv.org/abs/2402.03337](https://arxiv.org/abs/2402.03337)

    本文介绍了开发虚拟海洋环境模拟真实实验的主要挑战和问题，并提出了使用强化学习代理进行自主导航和控制的关键特性。同时，还讨论了创建基于真实机器人帆船的功能性数字孪生所需的建模和实施步骤以及挑战。这项研究对于开发基于强化学习的导航算法在真实船只上的应用具有直接的意义。

    

    本文着重解决在使用无人表面船只(Unmanned Surface Vehicles, USV)的数字孪生开展虚拟海洋环境来复制真实实验时所面临的主要挑战和问题。我们介绍了构建虚拟世界的关键特性，考虑使用强化学习(RL)智能体进行自主导航和控制。在此基础上，主要问题涉及模拟方程的定义(物理和数学)、它们的有效实施以及如何将用于RL的模拟控制和感知策略(传感器)包括进来。我们介绍了基于真实机器人帆船创建功能性数字孪生所需的建模、实施步骤和挑战。该应用将立即用于开发基于RL的导航算法，以应用于真实船只。

    This work focuses on the main challenges and problems in developing a virtual oceanic environment reproducing real experiments using Unmanned Surface Vehicles (USV) digital twins. We introduce the key features for building virtual worlds, considering using Reinforcement Learning (RL) agents for autonomous navigation and control. With this in mind, the main problems concern the definition of the simulation equations (physics and mathematics), their effective implementation, and how to include strategies for simulated control and perception (sensors) to be used with RL. We present the modeling, implementation steps, and challenges required to create a functional digital twin based on a real robotic sailing vessel. The application is immediate for developing navigation algorithms based on RL to be applied on real boats.
    
[^131]: 无监督显著性补丁选择用于数据高效的强化学习

    Unsupervised Salient Patch Selection for Data-Efficient Reinforcement Learning

    [https://arxiv.org/abs/2402.03329](https://arxiv.org/abs/2402.03329)

    本文提出了一种无监督的方法，通过自动提取图像中的重要补丁来改善基于视觉的深度强化学习的样本效率。这种方法利用经过自监督训练的Vision Transformer模型来检测和选择难以重构的显著补丁，并在强化学习中使用注意力模块进行处理。在实验证明了这种方法的数据效率，并分析了其模型的可解释性能力。

    

    为了提高基于视觉的深度强化学习的样本效率，我们提出了一种新颖的方法，称为SPIRL，用于自动从输入图像中提取重要的补丁。SPIRL基于经过自监督训练的Vision Transformer模型，通过随机采样的补丁对图像进行重构。然后利用这些经过预训练的模型来检测和选择显著的补丁，这些补丁在邻近补丁中难以重构。在强化学习中，SPIRL代理利用注意力模块处理选中的显著补丁。我们在Atari游戏上经验验证了SPIRL的数据效率，将其与相关的最先进方法，包括一些传统的基于模型的方法和基于关键点的模型进行比较。此外，我们还分析了我们模型的可解释性能力。

    To improve the sample efficiency of vision-based deep reinforcement learning (RL), we propose a novel method, called SPIRL, to automatically extract important patches from input images. Following Masked Auto-Encoders, SPIRL is based on Vision Transformer models pre-trained in a self-supervised fashion to reconstruct images from randomly-sampled patches. These pre-trained models can then be exploited to detect and select salient patches, defined as hard to reconstruct from neighboring patches. In RL, the SPIRL agent processes selected salient patches via an attention module. We empirically validate SPIRL on Atari games to test its data-efficiency against relevant state-of-the-art methods, including some traditional model-based methods and keypoint-based models. In addition, we analyze our model's interpretability capabilities.
    
[^132]: 大规模生成AI模型缺乏视觉数字感知能力

    Large-scale Generative AI Models Lack Visual Number Sense

    [https://arxiv.org/abs/2402.03328](https://arxiv.org/abs/2402.03328)

    本研究调查了基于大规模Transformer架构的生成性AI模型是否能够准确命名物体数量或生成包含目标数量物品的图像，结果发现这些模型都没有以类似人类的方式表现，并且即使对于小数量的物体也会出现显著的错误。

    

    人类能够在视觉场景中轻松判断物体的数量，即使不进行计数，而且这种技能在各种动物物种和语言发展和正式学校教育之前的婴儿中都有记录。对于小的物体集，数字判断是无误的，而对于更大的集合，回应变得近似，并且变异性与目标数字成比例增加。尽管物体特征（如颜色或形状）存在差异，但这种回应模式在所有类型的物体上观察到，这表明我们的视觉数字感知依赖于数字数量的抽象表示。在本研究中，我们调查了基于大规模Transformer架构的生成性人工智能（AI）模型是否可以可靠地命名简单视觉刺激中的物体数量或生成包含目标物品数量的图像（1-10范围内）。令人惊讶的是，所考虑的所有基础模型都没有以类似人类一样的方式表现出来：即使是具有较小数量的物体也会犯下显著的错误。

    Humans can readily judge the number of objects in a visual scene, even without counting, and such a skill has been documented in a variety of animal species and in babies prior to language development and formal schooling. Numerical judgments are error-free for small sets, while for larger collections responses become approximate, with variability increasing proportionally to the target number. This response pattern is observed for items of all kinds, despite variation in object features (such as color or shape), suggesting that our visual number sense relies on abstract representations of numerosity. Here, we investigated whether generative Artificial Intelligence (AI) models based on large-scale transformer architectures can reliably name the number of objects in simple visual stimuli or generate images containing a target number of items in the 1-10 range. Surprisingly, none of the foundation models considered performed in a human-like way: They all made striking errors even with sm
    
[^133]: Uni3D-LLM：利用大型语言模型统一点云感知、生成和编辑

    Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models

    [https://arxiv.org/abs/2402.03327](https://arxiv.org/abs/2402.03327)

    Uni3D-LLM是一个统一框架，利用大型语言模型实现了点云感知、生成和编辑任务的一体化。通过利用自然语言描述，用户可以轻松生成和修改点云场景中的对象，从而提高操作的灵活性和可控性。

    

    本文介绍了Uni3D-LLM，一个利用大型语言模型（LLM）将3D感知、生成和编辑任务整合为一体的统一框架。该框架使用户能够轻松地在点云场景中指定位置生成和修改对象，根据自然语言描述的多样性进行引导。Uni3D-LLM利用自然语言的表达能力，实现对3D对象生成和编辑的精确控制，从而显著提升操作的灵活性和可控性。通过将点云映射到统一的表示空间，Uni3D-LLM实现了跨应用功能，能够无缝执行各种任务，从精确实例化3D对象到交互设计的各种需求。通过一系列严格的实验，验证了Uni3D-LLM在点云理解、生成和编辑方面的有效性。

    In this paper, we introduce Uni3D-LLM, a unified framework that leverages a Large Language Model (LLM) to integrate tasks of 3D perception, generation, and editing within point cloud scenes. This framework empowers users to effortlessly generate and modify objects at specified locations within a scene, guided by the versatility of natural language descriptions. Uni3D-LLM harnesses the expressive power of natural language to allow for precise command over the generation and editing of 3D objects, thereby significantly enhancing operational flexibility and controllability. By mapping point cloud into the unified representation space, Uni3D-LLM achieves cross-application functionality, enabling the seamless execution of a wide array of tasks, ranging from the accurate instantiation of 3D objects to the diverse requirements of interactive design. Through a comprehensive suite of rigorous experiments, the efficacy of Uni3D-LLM in the comprehension, generation, and editing of point cloud has
    
[^134]: 由孤波和生物启发的非线性输入数据转换实现的物理储备计算

    Physical Reservoir Computing Enabled by Solitary Waves and Biologically-Inspired Nonlinear Transformation of Input Data

    [https://arxiv.org/abs/2402.03319](https://arxiv.org/abs/2402.03319)

    本文通过孤波和非线性输入数据转换实现了一种物理储备计算系统，该系统作为传统RC算法的技术简单硬件对应物，具有更高的效率和多功能性。

    

    储备计算 (RC) 系统利用人工神经网络的随机连接的非线性动力学特性，能够高效地预测混沌时间序列。RC系统的多功能性促使进一步研究传统RC算法的硬件对应物和更高效的RC-like方案。在本文中，受到生物脑中的非线性过程的启发，并利用在流动液体薄膜表面上激发的孤立波，我们通过实验证实了一种物理RC系统，该系统通过非线性数据输入转换代替了随机性的效果。利用仅具有最小计算能力的微控制器进行所有操作，我们证明所设计的RC系统作为传统RC算法的“下一代”改进的技术简单硬件对应物。

    Reservoir computing (RC) systems can efficiently forecast chaotic time series using nonlinear dynamical properties of an artificial neural network of random connections. The versatility of RC systems has motivated further research on both hardware counterparts of traditional RC algorithms and more efficient RC-like schemes. Inspired by the nonlinear processes in a living biological brain and using solitary waves excited on the surface of a flowing liquid film, in this paper we experimentally validate a physical RC system that substitutes the effect of randomness for a nonlinear transformation of input data. Carrying out all operations using a microcontroller with a minimal computational power, we demonstrate that the so-designed RC system serves as a technically simple hardware counterpart to the `next-generation' improvement of the traditional RC algorithm.
    
[^135]: 人工智能在脑电波预测中的应用：混沌理论分析

    Artificial Intelligence for EEG Prediction: Applied Chaos Theory

    [https://arxiv.org/abs/2402.03316](https://arxiv.org/abs/2402.03316)

    本研究提出了一种基于混沌理论和动力学系统理论的序列预测模型，应用于脑电波数据，通过精心设计和优化，实现了高效且准确的预测。

    

    在本研究中，我们探索了复杂的脑电图（EEG）数据分析领域，重点研究了跨32个EEG通道进行序列预测。该研究将应用混沌理论和动力学系统理论的原则融合在一起，生成了一套新的特征集，丰富了我们深度学习模型的表示能力。该研究的核心是基于Transformer的序列预测架构，经过精心校准，能够捕捉EEG序列中固有的非线性和高维时态依赖关系。通过合理的架构设计、参数初始化策略和优化技术，我们在计算效率和预测性能之间取得了复杂的平衡。我们的模型是EEG数据序列预测中的先驱，展示出了非凡的普适性和鲁棒性。研究结果不仅扩展了我们对EEG数据动态性的理解，还揭示了一个潜在的新的脑电波预测方法。

    In the present research, we delve into the intricate realm of electroencephalogram (EEG) data analysis, focusing on sequence-to-sequence prediction of data across 32 EEG channels. The study harmoniously fuses the principles of applied chaos theory and dynamical systems theory to engender a novel feature set, enriching the representational capacity of our deep learning model. The endeavour's cornerstone is a transformer-based sequence-to-sequence architecture, calibrated meticulously to capture the non-linear and high-dimensional temporal dependencies inherent in EEG sequences. Through judicious architecture design, parameter initialisation strategies, and optimisation techniques, we have navigated the intricate balance between computational expediency and predictive performance. Our model stands as a vanguard in EEG data sequence prediction, demonstrating remarkable generalisability and robustness. The findings not only extend our understanding of EEG data dynamics but also unveil a po
    
[^136]: DeepSeekMath: 将开放语言模型中的数学推理能力推向极限

    DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

    [https://arxiv.org/abs/2402.03300](https://arxiv.org/abs/2402.03300)

    DeepSeekMath是一种开放语言模型，通过预训练和数据选择，提升了数学推理能力，实现了接近于竞赛级别水平的性能。

    

    数学推理由于其复杂和结构化的特性，对语言模型提出了重大挑战。本文介绍了DeepSeekMath 7B，它在Common Crawl中获取了120B个与数学相关的标记，并结合了自然语言和代码数据来继续预训练DeepSeek-Coder-Base-v1.5 7B。DeepSeekMath 7B在竞赛级别的数学基准测试中取得了令人印象深刻的51.7%的分数，无需依赖外部工具包和投票技术，接近了Gemini-Ultra和GPT-4的性能水平。DeepSeekMath 7B的自一致性在MATH上的64个样本中达到了60.9%的分数。DeepSeekMath的数学推理能力归因于两个关键因素：首先，我们通过精心设计的数据选择管道充分利用了公开可用的网络数据的巨大潜力。其次，我们引入了群体相对策略优化（GRPO），这是近端策略优化（PPO）的一个变体，可以增强数学推理能力。

    Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities w
    
[^137]: EasyInstruct：一个易于使用的用于大型语言模型的指令处理框架

    EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models

    [https://arxiv.org/abs/2402.03049](https://arxiv.org/abs/2402.03049)

    EasyInstruct是一个易于使用的用于大型语言模型的指令处理框架，通过模块化指令生成、选择和提示，并考虑它们的组合和交互，使指令处理更加方便和高效。

    

    近年来，指令调整已经引起了越来越多的关注，并成为增强大型语言模型（LLMs）能力的一种关键技术。为了构建高质量的指令数据集，已经提出了许多指令处理方法，旨在在数据数量和数据质量之间达到精巧的平衡。然而，由于各种指令处理方法之间仍然存在不一致，目前没有标准的开源指令处理实现框架可供社区使用，这使得从业者无法进一步开发和推进。为了促进指令处理的研究和开发，我们提出了EasyInstruct，一个易于使用的用于LLMs的指令处理框架，它将指令生成、选择和提示模块化，并考虑它们的组合和交互。EasyInstruct已经在https://github.com/zjunlp/EasyInstruct上公开发布，并得到了积极维护。

    In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
    
[^138]: 重新思考微型语言模型的优化和架构

    Rethinking Optimization and Architecture for Tiny Language Models

    [https://arxiv.org/abs/2402.02791](https://arxiv.org/abs/2402.02791)

    本研究重新思考了微型语言模型的优化和架构，通过经验研究发现了在微型语言模型中特别有效的设计公式，并在多语种数据集上训练了高性能的微型语言模型。

    

    大型语言模型（LLMs）的威力通过大量的数据和计算资源得到了证明。然而，在移动设备上应用语言模型面临着计算和内存成本的巨大挑战，迫切需要高性能的微型语言模型。受复杂训练过程的限制，优化语言模型的许多细节很少得到仔细研究。在本研究中，基于一个具有10亿参数的微型语言模型，我们仔细设计了一系列经验研究来分析每个组件的影响。主要讨论了三个方面，即神经架构、参数初始化和优化策略。多个设计公式在微型语言模型中经验性地被证明特别有效，包括分词器压缩、架构调整、参数继承和多轮训练。然后，我们在1.6T多语种数据集上训练了PanGu-$\pi$-1B Pro和PanGu-$\pi$-1.5B Pro。

    The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
    
[^139]: TopoX: 一个用于拓扑域上的机器学习的Python软件包套件

    TopoX: A Suite of Python Packages for Machine Learning on Topological Domains

    [https://arxiv.org/abs/2402.02441](https://arxiv.org/abs/2402.02441)

    TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。

    

    我们介绍了topox，一个提供可靠且用户友好的Python软件包套件，用于在拓扑域（扩展了图的领域）上进行计算和机器学习：超图、单纯、胞腔、路径和组合复合体。topox由三个软件包组成：toponetx用于构建和计算这些域，包括节点、边和高阶单元的处理；topoembedx提供了将拓扑域嵌入到向量空间的方法，类似于流行的基于图的嵌入算法，如node2vec；topomodelx建立在PyTorch之上，为拓扑域上的神经网络提供了一套全面的高阶消息传递功能工具箱。topox的源代码经过广泛的文档化和单元测试，并在https://github.com/pyt-team以MIT许可证的形式提供。

    We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
    
[^140]: Aligner: 通过弱到强校正实现高效对齐

    Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction

    [https://arxiv.org/abs/2402.02416](https://arxiv.org/abs/2402.02416)

    Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。

    

    对于大型语言模型（LLMs），通过强化学习来进行对齐的努力主要是通过人类反馈的强化学习方法进行的。然而，强化学习面临着主要的挑战，包括训练奖励模型、演员-评论家工程以及重要的是，需要访问LLM参数。在这里，我们介绍了一种新的高效对齐范式Aligner，它通过学习对齐和未对齐答案之间的校正残差来绕过整个强化学习过程。我们的Aligner具有几个关键优势。首先，它是一个基于自监督学习的自动回归seq2seq模型，通过训练查询-答案-校正数据集，提供了一种参数高效的对齐解决方案，并且对资源需求较少。其次，Aligner实现了从弱到强的泛化；通过Aligner的监督信号来微调大型预训练模型，可以显著提升性能。第三，Aligner作为一个模型不可知的即插即用模块，可以直接应用于…

    Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
    
[^141]: 在分析课堂对话时评估大型语言模型

    Evaluating Large Language Models in Analysing Classroom Dialogue

    [https://arxiv.org/abs/2402.02380](https://arxiv.org/abs/2402.02380)

    本研究评估了大型语言模型（LLMs），特点是GPT-4，对课堂对话进行分析的应用。结果显示，GPT-4能够显著节省时间，且在编码一致性方面表现出很高的一致性。

    

    本研究探讨了大型语言模型（LLM），特别是GPT-4，在分析课堂对话中的应用，这是教学诊断和质量改进的重要研究任务。鉴于传统教育研究中知识密集和劳动密集的定性方法，本研究调查了LLM在优化和增强分析过程方面的潜力。该研究涉及中学的数据集，包括数学和语文课堂上的对话。这些对话由教育专家手动编码，然后使用定制的GPT-4模型进行分析。本研究侧重于比较手动注释与GPT-4的输出，以评估其在分析教育对话方面的效果。评估时间效率、编码者间一致性和编码者间可靠性之间的差异。结果表明，使用GPT-4可以显著节省时间，并在编码一致性方面具有很高的一致性。

    This study explores the application of Large Language Models (LLMs), specifically GPT-4, in the analysis of classroom dialogue, a crucial research task for both teaching diagnosis and quality improvement. Recognizing the knowledge-intensive and labor-intensive nature of traditional qualitative methods in educational research, this study investigates the potential of LLM to streamline and enhance the analysis process. The study involves datasets from a middle school, encompassing classroom dialogues across mathematics and Chinese classes. These dialogues were manually coded by educational experts and then analyzed using a customised GPT-4 model. This study focuses on comparing manual annotations with the outputs of GPT-4 to evaluate its efficacy in analyzing educational dialogues. Time efficiency, inter-coder agreement, and inter-coder reliability between human coders and GPT-4 are evaluated. Results indicate substantial time savings with GPT-4, and a high degree of consistency in codin
    
[^142]: DeCoF:通过帧一致性进行生成视频检测

    DeCoF: Generated Video Detection via Frame Consistency

    [https://arxiv.org/abs/2402.02085](https://arxiv.org/abs/2402.02085)

    通过帧一致性原则，DeCoF是一个简单但有效的生成视频检测模型，可以消除空间伪影的影响，并表现出强大的泛化能力。

    

    高级视频生成方法产生的视频质量不断提高，这导致社会面临新的安全挑战，使生成视频检测成为紧迫的研究重点。为促进这一领域的合作研究，我们构建了第一个明确用于生成视频检测的开源数据集，为社区提供了一个宝贵的资源，以评估和改进检测方法。通过一系列精心设计的探测实验，我们的研究探讨了时间和空间伪影在开发生成视频的通用和稳健检测器方面的重要性。基于视频帧一致性原则，我们引入了一个简单但有效的检测模型（DeCoF），它消除了空间伪影在通用特征学习中的影响。我们的广泛实验表明，DeCoF在检测未见过的视频生成模型产生的视频方面非常有效，并且验证了其在多个领域的强大泛化能力。

    The escalating quality of video generated by advanced video generation methods leads to new security challenges in society, which makes generated video detection an urgent research priority.To foster collaborative research in this area, we construct the first open-source dataset explicitly for generated video detection, providing a valuable resource for the community to benchmark and improve detection methodologies. Through a series of carefully designed probe experiments, our study explores the significance of temporal and spatial artifacts in developing general and robust detectors for generated video. Based on the principle of video frame consistency, we introduce a simple yet effective detection model (DeCoF) that eliminates the impact of spatial artifacts during generalizing feature learning. Our extensive experiments demonstrate the efficacy of DeCoF in detecting videos produced by unseen video generation models and confirm its powerful generalization capabilities across several 
    
[^143]: 基于大规模语言模型的超参数优化的技术

    Large Language Model Agent for Hyper-Parameter Optimization

    [https://arxiv.org/abs/2402.01881](https://arxiv.org/abs/2402.01881)

    基于大规模语言模型的AgentHPO技术通过自动化超参数优化，在机器学习任务中大大减少了试验次数，简化了设置过程，提升了解释性和用户信任。

    

    超参数优化在现代机器学习中至关重要，需要专业知识、大量实验以及高计算和人力资源。尽管自动化机器学习（AutoML）取得了一些进展，但试验效率、设置复杂性和互操作性方面仍存在挑战。为了解决这些问题，我们引入了一种新的范式，利用大规模语言模型（LLMs）来自动化不同机器学习任务的超参数优化，称为AgentHPO（LLM Agent-based Hyperparameter Optimization）。具体来说，AgentHPO自主处理任务信息，根据历史试验对特定超参数（HPs）进行实验，并进行迭代优化。与传统的AutoML方法相比，这种类似人类的优化过程极大地减少了所需的试验次数，简化了设置过程，并提升了解释性和用户信任。

    Hyperparameter optimization is critical in modern machine learning, requiring expert knowledge, numerous trials, and high computational and human resources. Despite the advancements in Automated Machine Learning (AutoML), challenges in terms of trial efficiency, setup complexity, and interoperability still persist. To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks, which is named AgentHPO (short for LLM Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the task information autonomously, conducts experiments with specific hyperparameters (HPs), and iteratively optimizes them based on historical trials. This human-like optimization process largely reduces the number of required trials, simplifies the setup process, and enhances interpretability and user trust, compared to traditional AutoML methods. Extensive empirical experiments conducted o
    
[^144]: LLMs无法规划，但可以在LLM-Modulo框架中帮助规划

    LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks

    [https://arxiv.org/abs/2402.01817](https://arxiv.org/abs/2402.01817)

    LLMs无法独自进行规划或自我验证，但在规划/推理任务中可以作为通用近似知识源发挥更大作用。

    

    关于大型语言模型（LLMs）在规划和推理任务中的角色存在很大的困惑。一方面有人过于乐观地声称只需正确提示或自我验证策略，LLMs就能完成这些任务。另一方面，也有人过于悲观地认为LLMs在规划/推理任务中仅能作为问题规范的简单翻译器，并将问题交给外部符号求解器。在这篇立场文章中，我们认为这两种极端观点都是错误的。我们认为自回归LLMs本身不能进行规划或自我验证（毕竟这是一种推理形式），并对文献中的误解原因进行了一些阐述。我们还将辩称LLMs应该被视为具有更有意义的角色的通用近似知识源，能在规划/推理任务中发挥更大的作用。

    There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end forma
    
[^145]: 大规模语言模型用于时间序列：一项调研

    Large Language Models for Time Series: A Survey

    [https://arxiv.org/abs/2402.01801](https://arxiv.org/abs/2402.01801)

    本调研论文深入探讨了大规模语言模型（LLM）在时间序列分析中的应用方法。通过解决LLM与数值型时间序列数据之间的差异挑战，揭示了LLM在时间序列领域的潜力，并提出了直接提示、量化、对齐、利用视觉方式和结合工具等方法。此外，还提供了对应用领域、评估方法和未来研究方向的讨论。

    

    大规模语言模型（LLM）在自然语言处理和计算机视觉等领域得到了广泛应用。LLM不仅仅局限于文本、图像和图形，还具有对时间序列数据进行分析的重要潜力，可以在气候、物联网、医疗、交通、音频和金融等领域受益。本调研论文对利用LLM进行时间序列分析的各种方法进行了深入探讨和详细分类。我们解决了LLM原始文本数据训练与数值型时间序列数据之间的差异挑战，并探索了将LLM的知识转移和提取到数值时间序列分析的策略。我们详细介绍了各种方法，包括（1）直接提示LLM，（2）时间序列量化，（3）对齐技术，（4）利用视觉方式作为桥接机制，和（5）结合LLM与工具。此外，本调研还提供了一系列涉及应用领域、评估方法和未来研究方向的讨论。

    Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) alignment techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey off
    
[^146]: 当大型语言模型遇上向量数据库：一项综述

    When Large Language Models Meet Vector Databases: A Survey

    [https://arxiv.org/abs/2402.01763](https://arxiv.org/abs/2402.01763)

    本综述论文深入分析了大型语言模型和向量数据库之间的交叉点，大型语言模型的突破带来了新的挑战，而向量数据库提供了潜在的解决方案，可以显著增强人工智能系统管理和利用多样数据的能力。

    

    最近大型语言模型的突破在人类文字处理和生成方面开启了新的领域。然而，随着它们的显著增长，大型语言模型面临着包括幻觉、偏见、实时知识更新以及在商业环境中实施和维护的高成本等重要挑战。而另一种日益流行的工具，向量数据库则为这些挑战提供了潜在的解决方案。这些数据库擅长处理高维数据，并且对于高效的信息检索和语义搜索等任务至关重要。通过与大型语言模型的整合，它们显著增强了人工智能系统管理和更有效地利用多样数据的能力。本综述论文对大型语言模型和向量数据库之间的交叉点进行了深入而独特的分析。

    The recent burst in Large Language Models has opened new frontiers in human-like text processing and generation. However, alongside their remarkable growth, Large Language Models have encountered critical challenges including issues of hallucination, bias, real-time knowledge updates, and the high costs of implementation and maintenance in commercial settings. Vector Databases, another increasingly popular tool, offer potential solutions to these challenges. These databases are adept at handling high-dimensional data and are crucial for tasks such as efficient information retrieval and semantic search. By integrating with Large Language Models, they significantly enhance AI systems' ability to manage and utilize diverse data more effectively. This survey paper provides an in-depth and unique analysis of the intersection between Large Language Models and Vector Databases.
    
[^147]: 作为策略的状态字符串：用博弈论求解器引导语言模型

    States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

    [https://arxiv.org/abs/2402.01704](https://arxiv.org/abs/2402.01704)

    本研究提出了一种在语言模型中引入博弈论思想的方法，通过绑定博弈论的符号逻辑，使得语言模型能够通过博弈论求解器提供更加稳定和理性的对话策略。

    

    博弈论是研究理性主体间战略互动的数学模型。语言是人类互动的重要方式，但在历史上一直很难通过数学方法对对话及其战略动机建模。与语言互动相关的玩家、策略和回报的适当模型（即对游戏论常规符号逻辑的约束）将使现有的博弈论算法能够在语言领域提供战略解决方案。换句话说，这种约束可以为在对话中计算稳定、理性的对话策略提供一条途径。大型语言模型（LLM）可能已经达到了其生成能力足以实现自然对话真实、类似人类的模拟的程度。通过以不同的方式提示它们，我们可以将其响应引导到不同的输出话语。利用自然语言的表达能力，LLM还可以帮助我们快速生成新的对话。

    Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new di
    
[^148]: SMUTF：使用生成标签和混合特征的模式匹配方法

    SMUTF: Schema Matching Using Generative Tags and Hybrid Features

    [https://arxiv.org/abs/2402.01685](https://arxiv.org/abs/2402.01685)

    SMUTF是一种用于大规模表格数据模式匹配的独特方法，通过结合基于规则的特征工程、预训练语言模型和生成式大语言模型，并使用生成标签提高匹配效果。同时，作者开发并开源了HDXSM数据集来解决现有数据集不足的问题。

    

    我们引入了SMUTF，一种用于大规模表格数据模式匹配的独特方法，该方法假设在开放域任务中，监督学习不会影响性能，从而实现了有效的跨域匹配。这个系统独特地结合了基于规则的特征工程、预训练语言模型和生成式大语言模型。受人道主义交换语言的启发，我们使用“生成标签”为每个数据列部署了创新的适应性，提高了模式匹配的效果。SMUTF具有广泛的灵活性，可以与任何现有的预训练嵌入、分类方法和生成模型无缝配合使用。鉴于模式匹配缺乏广泛的公开数据集，我们已经创建并开源了HDXSM数据集，该数据集来自公共人道主义数据，我们相信这是目前最全面的模式匹配数据集。

    We introduce SMUTF, a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy 'generative tags' for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models.   Recognizing the lack of extensive, publicly available datasets for SM, we have created and open-sourced the HDXSM dataset from the public humanitarian data. We believe this to be the most exhaustive SM dataset currently available. In evaluations across various public datasets and the novel HDXSM dataset, SMUTF demonstrated excep
    
[^149]: SymbolicAI: 一个结合生成模型和求解器的基于逻辑的方法的框架

    SymbolicAI: A framework for logic-based approaches combining generative models and solvers

    [https://arxiv.org/abs/2402.00854](https://arxiv.org/abs/2402.00854)

    SymbolicAI是一个基于逻辑的框架，将生成模型与多种求解器无缝集成，通过将大型语言模型作为语义解析器，实现了符号推理与生成式人工智能的融合。

    

    我们介绍了SymbolicAI，这是一个多功能且模块化的框架，采用基于逻辑的方法来处理生成过程中的概念学习和流程管理。SymbolicAI通过将大型语言模型（LLM）作为语义解析器来执行基于自然语言和形式语言指令的任务，从而弥合了符号推理和生成式人工智能之间的差距，使生成模型与各种求解器无缝集成。我们利用概率编程原理来处理复杂任务，并利用可微分和经典编程范 paradigms 的各自优势。该框架引入了一系列多态的、组合的和自指的数据流操作，将LLM的输出与用户的目标对齐。因此，我们可以在具有零次和少次学习能力的各种基础模型之间进行过渡，并与擅长解决特定问题的专业化调优模型或求解器配合使用。

    We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addres
    
[^150]: 使用ODE方法进行带有马尔可夫噪声的随机逼近和强化学习

    The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise

    [https://arxiv.org/abs/2401.07844](https://arxiv.org/abs/2401.07844)

    本文通过使用ODE方法，将稳定性的Borkar-Meyn定理从鞅差异噪声设定拓展到马尔可夫噪声设定，提高了其在离策略强化学习中的适用性。

    

    随机逼近是一类通过迭代、增量和随机更新向量的算法，包括随机梯度下降和时序差分学习。分析随机逼近算法的一个主要挑战是确保其稳定性，即证明随机向量迭代几乎必定有界。本文将稳定性的Borkar-Meyn定理从鞅差异噪声设定拓展到马尔可夫噪声设定，大大提高了其在强化学习中的适用性，特别是那些具有线性函数逼近和资格迹的离策略强化学习算法。我们的分析的核心在于少数函数的渐进变化速率下降，这一点由大数定律和常用的V4 Lyapunov漂移条件隐含，并在马尔可夫链是有限且不可约时显然成立。

    Stochastic approximation is a class of algorithms that update a vector iteratively, incrementally, and stochastically, including, e.g., stochastic gradient descent and temporal difference learning. One fundamental challenge in analyzing a stochastic approximation algorithm is to establish its stability, i.e., to show that the stochastic vector iterates are bounded almost surely. In this paper, we extend the celebrated Borkar-Meyn theorem for stability from the Martingale difference noise setting to the Markovian noise setting, which greatly improves its applicability in reinforcement learning, especially in those off-policy reinforcement learning algorithms with linear function approximation and eligibility traces. Central to our analysis is the diminishing asymptotic rate of change of a few functions, which is implied by both a form of strong law of large numbers and a commonly used V4 Lyapunov drift condition and trivially holds if the Markov chain is finite and irreducible.
    
[^151]: 从大型语言模型中提取事件序列知识

    Distilling Event Sequence Knowledge From Large Language Models

    [https://arxiv.org/abs/2401.07237](https://arxiv.org/abs/2401.07237)

    本论文研究了通过大型语言模型从中提取事件序列知识的方法。采用了基于知识图的指导生成语言模型的方式，实现对具有部分因果关系的事件概念的事件序列的生成。实验证明了该方法可以生成高质量的事件序列，并且在填补知识空白方面具有潜在的价值。

    

    事件序列模型在事件的分析和预测中被发现是非常有效的。建立这样的模型需要丰富的高质量事件序列数据。然而，在某些应用中，干净的结构化事件序列不可用，自动化序列提取导致的数据太嘈杂和不完整。在这项工作中，我们探索了使用大型语言模型（LLMs）生成可以有效用于概率事件模型构建的事件序列的方法。这可以看作是从LLMs中提取事件序列知识的一种机制。我们的方法依赖于一个具有部分因果关系的事件概念的知识图（KG）来指导生成语言模型进行因果事件序列的生成。我们展示了我们的方法可以生成高质量的事件序列，填补了输入KG中的知识空白。此外，我们还探索了如何利用生成的序列来发现更有用和更复杂的内容。

    Event sequence models have been found to be highly effective in the analysis and prediction of events. Building such models requires availability of abundant high-quality event sequence data. In certain applications, however, clean structured event sequences are not available, and automated sequence extraction results in data that is too noisy and incomplete. In this work, we explore the use of Large Language Models (LLMs) to generate event sequences that can effectively be used for probabilistic event model construction. This can be viewed as a mechanism of distilling event sequence knowledge from LLMs. Our approach relies on a Knowledge Graph (KG) of event concepts with partial causal relations to guide the generative language model for causal event sequence generation. We show that our approach can generate high-quality event sequences, filling a knowledge gap in the input KG. Furthermore, we explore how the generated sequences can be leveraged to discover useful and more complex st
    
[^152]: 扩散模型、图像超分辨率和一切：一项调查研究

    Diffusion Models, Image Super-Resolution And Everything: A Survey

    [https://arxiv.org/abs/2401.00736](https://arxiv.org/abs/2401.00736)

    扩散模型（DMs）在图像超分辨率（SR）领域产生了颠覆性的影响，缩小了图像质量与人类感知偏好之间的差距。该研究调查了DM的理论基础，分析了其独特特点和方法，探索了替代输入领域等当前的研究方向。

    

    扩散模型（DMs）在图像超分辨率（SR）领域中产生了颠覆性的影响，进一步缩小了图像质量与人类感知偏好之间的差距。它们易于训练，并能生成比以前的生成方法产生的样本更高质量的图像。尽管取得了有希望的结果，但它们也带来了新的挑战，需要进一步的研究：高计算需求、可比性、缺乏可解释性、色彩偏移等。不幸的是，由于大量的出版物，进入这个领域令人难以应对。为了解决这个问题，我们提供了一个统一的叙述，阐明了应用于图像超分辨率的DM的理论基础，并提供了一份详细的分析，突出了该领域内与其他综述文章不同的独特特点和方法。这项调查研究对DM的原则进行了一个连贯的理解，并探索了当前的研究方向，包括替代输入领域等。

    Diffusion Models (DMs) have disrupted the image Super-Resolution (SR) field and further closed the gap between image quality and human perceptual preferences. They are easy to train and can produce very high-quality samples that exceed the realism of those produced by previous generative methods. Despite their promising results, they also come with new challenges that need further research: high computational demands, comparability, lack of explainability, color shifts, and more. Unfortunately, entry into this field is overwhelming because of the abundance of publications. To address this, we provide a unified recount of the theoretical foundations underlying DMs applied to image SR and offer a detailed analysis that underscores the unique characteristics and methodologies within this domain, distinct from broader existing reviews in the field. This survey articulates a cohesive understanding of DM principles and explores current research avenues, including alternative input domains, c
    
[^153]: 通过语言策略双向适应构建开放式具身代理

    Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation

    [https://arxiv.org/abs/2401.00006](https://arxiv.org/abs/2401.00006)

    通过OpenContra框架，我们提出了一种协同训练方法，结合语言模型和强化学习，构建开放式具身代理，能够理解任意人类指令，并以较高的完成率完成目标。

    

    构建开放式学习代理涉及预训练的语言模型（LLM）和强化学习（RL）方法面临的挑战。LLM在上下文特定的实时交互中遇到困难，而RL方法则面临探索效率问题。为此，我们提出了OpenContra，一种协同训练框架，它协同LLM和Goal-Conditioned强化学习（GRL），构建一个能够理解任意人类指令的开放式代理。实现包括两个阶段：（1）用LLM微调翻译人类指令为结构化目标，以及课程训练目标条件的RL策略，执行任意目标；（2）协同训练LLM和RL策略相互适应，实现指令空间的开放性。我们在Contra上进行实验，该游戏是一个复杂而广阔的目标空间的大逃杀FPS游戏。结果表明，使用OpenContra训练的代理能够理解任意人类指令，并以高完成率完成目标。

    Building open-ended learning agents involves challenges in pre-trained language model (LLM) and reinforcement learning (RL) approaches. LLMs struggle with context-specific real-time interactions, while RL methods face efficiency issues for exploration. To this end, we propose OpenContra, a co-training framework that cooperates LLMs and GRL to construct an open-ended agent capable of comprehending arbitrary human instructions. The implementation comprises two stages: (1) fine-tuning an LLM to translate human instructions into structured goals, and curriculum training a goal-conditioned RL policy to execute arbitrary goals; (2) collaborative training to make the LLM and RL policy learn to adapt each, achieving open-endedness on instruction space. We conduct experiments on Contra, a battle royale FPS game with a complex and vast goal space. The results show that an agent trained with OpenContra comprehends arbitrary human instructions and completes goals with a high completion ratio, whic
    
[^154]: 扩展就是一切：使用JAX加速强化学习的自动驾驶

    Scaling Is All You Need: Autonomous Driving with JAX-Accelerated Reinforcement Learning

    [https://arxiv.org/abs/2312.15122](https://arxiv.org/abs/2312.15122)

    本研究提出了一种扩展的自动驾驶强化学习方法，在大规模实验中展示了随着规模增加，策略性能的改善。与现有机器学习自动驾驶策略相比，我们的最佳策略将故障率降低了64％，同时提高了25％的驾驶进展速度。

    

    强化学习已经在复杂领域如视频游戏中展现出超越最优人类的能力。然而，为自动驾驶运行必要规模的强化学习实验非常困难。构建一个大规模的强化学习系统并在多个GPU上进行分布是具有挑战性的。在训练过程中在真实世界车辆上收集经验从安全和可扩展性的角度来看是不可行的。因此，需要一个高效且真实的驾驶模拟器，使用大量来自真实驾驶的数据。我们将这些能力集合在一起，并进行大规模的强化学习实验用于自动驾驶。我们证明，随着规模的增加，我们的策略表现得到了提升。我们最佳策略将故障率降低了64％，同时比现有机器学习自动驾驶策略提高了25％的驾驶进展速度。

    Reinforcement learning has been demonstrated to outperform even the best humans in complex domains like video games. However, running reinforcement learning experiments on the required scale for autonomous driving is extremely difficult. Building a large scale reinforcement learning system and distributing it across many GPUs is challenging. Gathering experience during training on real world vehicles is prohibitive from a safety and scalability perspective. Therefore, an efficient and realistic driving simulator is required that uses a large amount of data from real-world driving. We bring these capabilities together and conduct large-scale reinforcement learning experiments for autonomous driving. We demonstrate that our policy performance improves with increasing scale. Our best performing policy reduces the failure rate by 64% while improving the rate of driving progress by 25% compared to the policies produced by state-of-the-art machine learning for autonomous driving.
    
[^155]: SimLM：语言模型能否推断物理系统的参数？

    SimLM: Can Language Models Infer Parameters of Physical Systems?

    [https://arxiv.org/abs/2312.14215](https://arxiv.org/abs/2312.14215)

    本研究探究了大型语言模型在推断物理系统参数方面的性能，发现它们并不适合这个任务，即使是对于简单的系统也是如此。研究提出了一个有前景的方向，即利用物理模拟器来增强语言模型的背景。

    

    许多机器学习方法旨在学习或推理复杂的物理系统。推理的常见第一步是从系统行为的观察中推断系统参数。在本文中，我们研究了大型语言模型（LLMs）在物理系统上执行参数推断的性能。我们的实验证明，它们并不适用于这个任务，即使对于简单的系统也是如此。我们提出了一个有前景的研究方向，该方向涉及到使用物理模拟器来增强LLMs的背景。我们评估和比较了不同LLMs在一个简单的示例上的性能，有无物理模拟器的情况下。

    Several machine learning methods aim to learn or reason about complex physical systems. A common first-step towards reasoning is to infer system parameters from observations of its behavior. In this paper, we investigate the performance of Large Language Models (LLMs) at performing parameter inference in the context of physical systems. Our experiments suggest that they are not inherently suited to this task, even for simple systems. We propose a promising direction of exploration, which involves the use of physical simulators to augment the context of LLMs. We assess and compare the performance of different LLMs on a simple example with and without access to physical simulation.
    
[^156]: 从古怪的语言模型中调取潜在知识

    Eliciting Latent Knowledge from Quirky Language Models

    [https://arxiv.org/abs/2312.01037](https://arxiv.org/abs/2312.01037)

    本研究通过引入一套“古怪”的语言模型，调取了这些模型在特定上下文中的潜在知识，展示了从可信度低的模型中调取可靠知识的前景。

    

    调取潜在知识（ELK）旨在在一个能力强大的神经网络的激活中找到模式，即使网络的明显输出是错误或误导性的，也能稳定跟踪世界的真实状态。为了进一步研究ELK，我们引入了12个数据集和一套相应的“古怪”的语言模型，这些模型在回答问题时，只有在提示中包含关键词“Bob”时才会进行系统性错误的微调。我们证明了简单的探测方法可以调取模型在这些上下文中对正确答案的潜在知识，即使问题比探测器训练的问题更困难。这是由于中间层激活中的上下文无关的知识表示的存在。我们还发现，一种机械的异常检测方法可以以94%的AUROC标识不真实行为。我们的结果显示，从能力强但不受信任的模型中调取可靠的知识，并促进未来研究ELK方法的实证研究是有希望的。

    Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations which robustly track the true state of the world, even when the network's overt output is false or misleading. To further ELK research, we introduce 12 datasets and a corresponding suite of "quirky" language models that are LoRA finetuned to make systematic errors when answering questions if and only if the keyword "Bob" is present in the prompt. We demonstrate that simple probing methods can elicit the model's latent knowledge of the correct answer in these contexts, even for problems harder than those the probe was trained on. This is enabled by context-independent knowledge representations located in middle layer activations. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 94% AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods
    
[^157]: 基于文本提示的空间协变图像配准

    Spatially Covariant Image Registration with Text Prompts

    [https://arxiv.org/abs/2311.15607](https://arxiv.org/abs/2311.15607)

    这项工作引入了一种新颖的方法textSCF，通过将空间协变滤波器和由视觉语言模型编码的解剖文本提示相结合，提升了图像配准的效率和准确性。

    

    医学图像通常以其结构化的解剖表示和空间不均匀对比度而著称。利用神经网络中的解剖先验可以极大地增强其在资源受限的临床环境中的效用。先前的研究已经利用这些信息进行图像分割，然而在变形图像配准方面的进展却有限。我们的工作引入了textSCF，一种新颖的方法，它将空间协变滤波器和由视觉语言模型编码的解剖文本提示相结合，以填补这一空白。这种方法优化了一个隐式函数，将解剖区域的文本嵌入与滤波器权重相关联，放宽了典型的卷积运算的平移不变性约束。textSCF不仅提高了计算效率，还可以保留或提高配准的准确性。通过捕捉解剖区域之间的上下文相互作用，它提供了令人印象深刻的区域间可传递性和预备能力。

    Medical images are often characterized by their structured anatomical representations and spatially inhomogeneous contrasts. Leveraging anatomical priors in neural networks can greatly enhance their utility in resource-constrained clinical settings. Prior research has harnessed such information for image segmentation, yet progress in deformable image registration has been modest. Our work introduces textSCF, a novel method that integrates spatially covariant filters and textual anatomical prompts encoded by visual-language models, to fill this gap. This approach optimizes an implicit function that correlates text embeddings of anatomical regions to filter weights, relaxing the typical translation-invariance constraint of convolutional operations. TextSCF not only boosts computational efficiency but can also retain or improve registration accuracy. By capturing the contextual interplay between anatomical regions, it offers impressive inter-regional transferability and the ability to pre
    
[^158]: 法律要求分析：从合规的角度看

    Legal Requirements Analysis: A Regulatory Compliance Perspective

    [https://arxiv.org/abs/2311.13871](https://arxiv.org/abs/2311.13871)

    本文研究了从合规角度分析法律要求，特别是在软件开发过程中的要求工程阶段，以确保软件的合规性。主要关注欧盟的一般数据保护条例（GDPR）等法规对收集、处理或分享个人数据的软件系统的规定。

    

    现代软件已成为许多学科和应用环境中日常活动的重要组成部分。利用人工智能（AI）实现智能自动化在许多领域取得了突破。AI的有效性可以归功于多种因素，其中之一是数据的增加可用性。欧洲联盟（EU）的一般数据保护条例（GDPR）等法规的出台是为了确保个人数据的保护。收集、处理或分享个人数据的软件系统必须遵守这些法规的合规性要求。开发符合法规要求的软件在软件开发过程中要求工程（RE）阶段中需要重视处理法律要求。RE关注的是指定和维护一个系统的需求，包括法律要求。

    Modern software has been an integral part of everyday activities in many disciplines and application contexts. Introducing intelligent automation by leveraging artificial intelligence (AI) led to break-throughs in many fields. The effectiveness of AI can be attributed to several factors, among which is the increasing availability of data. Regulations such as the general data protection regulation (GDPR) in the European Union (EU) are introduced to ensure the protection of personal data. Software systems that collect, process, or share personal data are subject to compliance with such regulations. Developing compliant software depends heavily on addressing legal requirements stipulated in applicable regulations, a central activity in the requirements engineering (RE) phase of the software development process. RE is concerned with specifying and maintaining requirements of a system-to-be, including legal requirements. Legal agreements which describe the policies organizations implement f
    
[^159]: CodeScope:一个基于执行的多语言多任务多维基准用于评估LLMs在代码理解和生成方面的能力

    CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation

    [https://arxiv.org/abs/2311.08588](https://arxiv.org/abs/2311.08588)

    CodeScope是一个用于评估LLMs在代码理解和生成方面能力的多语言多任务多维基准，解决了现有基准在多语言编程环境和多任务设置方面的不足。

    

    大型语言模型（LLMs）在编码相关任务上表现出色，特别是在帮助人类编程和促进编程自动化方面。然而，现有的用于评估LLMs的代码理解和生成能力的基准存在严重的限制。首先，大部分基准存在缺陷，因为它们只关注于狭窄范围内的流行编程语言和特定任务，而实际软件开发场景需要实现多语言编程环境以满足各种需求。实际编程实践还强烈期望多任务设置，以全面和稳健地测试LLMs的编码能力。其次，大部分基准也未考虑生成代码的可执行性和执行结果的一致性。为了弥补现有基准与实际应用期望之间的差距，我们引入了CodeScope。

    Large Language Models (LLMs) have demonstrated remarkable performance on coding related tasks, particularly on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are deficient as they focus on a narrow range of popular programming languages and specific tasks, whereas the real-world software development scenarios show dire need to implement systems with multilingual programming environments to satisfy diverse requirements. Practical programming practices also strongly expect multi-task settings for testing coding capabilities of LLMs comprehensively and robustly. Second, most benchmarks also fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce CodeScope
    
[^160]: 通过多路径长期船舶轨迹预测建立更安全的海洋环境

    Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting

    [https://arxiv.org/abs/2310.18948](https://arxiv.org/abs/2310.18948)

    通过利用AIS数据预测船舶轨迹，本研究旨在通过减少船舶与鲸鱼碰撞来建立更安全的海洋环境。

    

    海上交通对于实现全球经济增长至关重要，同时也需要在可持续性和保护濒危海洋物种方面履行生态义务，尤其是保护大型鲸类种群。在这方面，自动识别系统(AIS)数据通过提供船舶运动的实时流数据，可以实现强化的交通监控，从而避免船舶与鲸鱼碰撞。本研究探讨利用AIS数据预测长期船舶轨迹，从而预防船舶与鲸鱼的碰撞。为此，我们采用双向长短期记忆网络(Bi-LSTM)构建了一种编码器-解码器模型架构，通过将1到3小时的AIS数据作为输入，预测接下来12小时的船舶轨迹。我们从历史AIS数据中提取潜在路线和目的地的概率特征，并将其作为模型的输入。模型随后预测船舶的轨迹，考虑到潜在路线和目的地的影响。

    Maritime transportation is paramount in achieving global economic growth, entailing concurrent ecological obligations in sustainability and safeguarding endangered marine species, most notably preserving large whale populations. In this regard, the Automatic Identification System (AIS) data plays a significant role by offering real-time streaming data on vessel movement, allowing enhanced traffic monitoring. This study explores using AIS data to prevent vessel-to-whale collisions by forecasting long-term vessel trajectories from engineered AIS data sequences. For such a task, we have developed an encoder-decoder model architecture using Bidirectional Long Short-Term Memory Networks (Bi-LSTM) to predict the next 12 hours of vessel trajectories using 1 to 3 hours of AIS data as input. We feed the model with probabilistic features engineered from historical AIS data that refer to each trajectory's potential route and destination. The model then predicts the vessel's trajectory, considerin
    
[^161]: 重新审视链接预测: 一个数据的视角

    Revisiting Link Prediction: A Data Perspective

    [https://arxiv.org/abs/2310.00793](https://arxiv.org/abs/2310.00793)

    本文通过从数据的视角出发，重新审视了链接预测的原则，并发现了局部结构接近性、全局结构接近性和特征接近性三个因素之间的关系。同时，发现了全局结构接近性只在局部结构接近性不足时显示出有效性，以及特征和结构接近性之间的不兼容性。这些发现为链接预测提供了新的思路，启发了GNN4LP的设计。

    

    链接预测是一项基于图的基本任务，在各种应用中已被证明是不可或缺的，例如朋友推荐、蛋白质分析和药物互作预测。然而，由于数据集涵盖了多个领域，它们可能具有不同的链接形成机制。现有文献中的证据强调了一个普遍适用于所有数据集的最佳算法的缺失。在本文中，我们尝试从数据中心的视角探索链接预测的原则，跨越不同数据集。我们确定了三个对链接预测至关重要的基本因素:局部结构接近性、全局结构接近性和特征接近性。然后，我们揭示了这些因素之间的关系，其中 (i)只有在局部结构接近性不足的情况下，全局结构接近性才显示出有效性。 (ii)特征和结构接近性之间存在不兼容性。这种不兼容性导致了链接预测的图神经网络 (GNN4LP) 持续地

    Link prediction, a fundamental task on graphs, has proven indispensable in various applications, e.g., friend recommendation, protein analysis, and drug interaction prediction. However, since datasets span a multitude of domains, they could have distinct underlying mechanisms of link formation. Evidence in existing literature underscores the absence of a universally best algorithm suitable for all datasets. In this paper, we endeavor to explore principles of link prediction across diverse datasets from a data-centric perspective. We recognize three fundamental factors critical to link prediction: local structural proximity, global structural proximity, and feature proximity. We then unearth relationships among those factors where (i) global structural proximity only shows effectiveness when local structural proximity is deficient. (ii) The incompatibility can be found between feature and structural proximity. Such incompatibility leads to GNNs for Link Prediction (GNN4LP) consistently 
    
[^162]: CC-SGG: 使用学习的场景图生成边界情况场景

    CC-SGG: Corner Case Scenario Generation using Learned Scene Graphs

    [https://arxiv.org/abs/2309.09844](https://arxiv.org/abs/2309.09844)

    本研究介绍了一种采用异构图神经网络的新方法，将普通驾驶场景转化为边界情况场景。通过生成简洁的场景图，并利用注意力和三元嵌入扰动图形，我们的模型成功学习到生成边界情况的能力。

    

    边界情况场景对于测试和验证自主驾驶车辆（AV）的安全性至关重要。由于这些场景在自然驾驶数据集中通常不足，因此使用合成的边界情况数据可以极大地增强AV在独特情况下的安全操作。然而，生成合成的、真实但又逼真的边界情况场景是一个重要挑战。在这项工作中，我们介绍了一种基于异构图神经网络（HGNNs）的新方法，将普通驾驶场景转换为边界情况场景。为了实现这一目标，我们首先将普通驾驶场景生成简洁的场景图形式，最小化对其结构和属性的改变。然后，我们的模型通过注意力和三元嵌入来学习扰动这些图形以生成边界情况。输入和扰动后的图形再次导入模拟器以生成边界情况场景。我们的模型成功学习到从普通场景生成边界情况的能力。

    Corner case scenarios are an essential tool for testing and validating the safety of autonomous vehicles (AVs). As these scenarios are often insufficiently present in naturalistic driving datasets, augmenting the data with synthetic corner cases greatly enhances the safe operation of AVs in unique situations. However, the generation of synthetic, yet realistic, corner cases poses a significant challenge. In this work, we introduce a novel approach based on Heterogeneous Graph Neural Networks (HGNNs) to transform regular driving scenarios into corner cases. To achieve this, we first generate concise representations of regular driving scenes as scene graphs, minimally manipulating their structure and properties. Our model then learns to perturb those graphs to generate corner cases using attention and triple embeddings. The input and perturbed graphs are then imported back into the simulation to generate corner case scenarios. Our model successfully learned to produce corner cases from i
    
[^163]: OHQ: 芯片上的硬件感知量化

    OHQ: On-chip Hardware-aware Quantization

    [https://arxiv.org/abs/2309.01945](https://arxiv.org/abs/2309.01945)

    本文提出了一种在芯片上进行硬件感知混合精度量化的框架（OHQ），通过构建量化感知流水线和引入掩码引导的量化估计技术，实现了在资源受限的硬件上进行高效量化，填补了现有混合精度量化的搜索空间过大和实际部署差距大的问题。

    

    量化成为在资源受限的硬件上部署先进深度模型的最有前途的方法之一。混合精度量化利用多位宽架构来释放量化模型的准确性和效率潜力。然而，现有的混合精度量化存在搜索空间过大的问题，导致巨大的计算开销。因此，量化过程依赖于独立的高性能设备，而不是本地进行，这也导致了考虑的硬件指标与实际部署之间的显著差距。本文提出了一种在芯片上进行硬件感知混合精度量化的框架（OHQ），而无需访问在线设备。首先，我们构建了芯片上的量化感知（OQA）流水线，能够感知量化算子在硬件上的实际效率指标。其次，我们提出了基于掩码引导的量化估计（MQE）技术。

    Quantization emerges as one of the most promising approaches for deploying advanced deep models on resource-constrained hardware. Mixed-precision quantization leverages multiple bit-width architectures to unleash the accuracy and efficiency potential of quantized models. However, existing mixed-precision quantization suffers exhaustive search space that causes immense computational overhead. The quantization process thus relies on separate high-performance devices rather than locally, which also leads to a significant gap between the considered hardware metrics and the real deployment.In this paper, we propose an On-chip Hardware-aware Quantization (OHQ) framework that performs hardware-aware mixed-precision quantization without accessing online devices. First, we construct the On-chip Quantization Awareness (OQA) pipeline, enabling perceive the actual efficiency metrics of the quantization operator on the hardware.Second, we propose Mask-guided Quantization Estimation (MQE) technique 
    
[^164]: 通过基于ASCII-Art的跨模态任务测试ChatGPT对于理解深度的能力：GPT3.5在识别和生成ASCII-Art方面的能力并不完全缺乏

    Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking

    [https://arxiv.org/abs/2307.16806](https://arxiv.org/abs/2307.16806)

    本研究通过基于ASCII-Art的跨模态任务，探讨了ChatGPT和GPT3.5在视觉任务中的能力，结果表明它们在图像识别、图像部分知识和图像生成方面并不完全缺乏。

    

    在发布后的八个月里，由于其强大的能力和易于使用，ChatGPT及其底层模型GPT3.5引起了广泛关注。虽然出现了一批研究这些模型能力范围的论文，但这些网络所接收和提取的信息要么是自然语言文本，要么是类似代码的风格化语言。在这项工作中，我们受到对一个真正达到人类水平的智能代理在多个信号模态上具备的能力的启示，考察了GPT3.5在视觉任务中的能力，其中输入以ASCII-Art形式提供内容，没有明显的语言化总结。我们进行了一系列实验，分析了该模型在经过典型的视觉设置下的图像识别任务上的表现，调查了其对图像部分的知识以及图像生成的任务。

    Over the eight months since its release, ChatGPT and its underlying model, GPT3.5, have garnered massive attention, due to their potent mix of capability and accessibility. While a niche-industry of papers have emerged examining the scope of capabilities these models possess, the information fed to and extracted from these networks has been either natural language text or stylized, code-like language. Drawing inspiration from the prowess we expect a truly human-level intelligent agent to have across multiple signal modalities, in this work we examine GPT3.5's aptitude for visual tasks, where the inputs feature content provided as ASCII-art without overt distillation into a lingual summary. We conduct experiments analyzing the model's performance on image recognition tasks after various transforms typical in visual settings, trials investigating knowledge of image parts, and tasks covering image generation.
    
[^165]: 无参考图像标题评估指标的鲁棒性研究

    An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics

    [https://arxiv.org/abs/2305.14998](https://arxiv.org/abs/2305.14998)

    研究评估了无参考图像标题评估指标在高词汇重叠但含义差异很大的情况下的鲁棒性，结果发现尽管这些指标与人类判断相关性较高，但对细粒度错误识别困难，并且在标题不合理性错误、图像相关对象大小变化以及标题对否定意义的理解方面存在敏感性差异。

    

    最近，提出了一些无参考指标，如CLIPScore（Hessel等，2021），UMIC（Lee等，2021）和PAC-S（Sarto等，2023），用于自动无参考评估图像标题。我们的研究重点在于评估这些指标在需要区分具有高词汇重叠但含义差异很大的两个标题的情况下的鲁棒性。我们的研究结果显示，尽管这些指标与人类判断具有很高的相关性，但CLIPScore、UMIC和PAC-S很难识别细粒度错误。虽然所有指标对视觉错误敏感，但对标题不合理性错误的敏感性有限。此外，我们还发现所有指标对标题中提及的与图像相关的对象的大小变化敏感，而CLIPScore和PAC-S对标题中提及的与图像相关的对象的数量也敏感。关于标题的语言方面，所有指标对否定意义的理解能力较弱。

    Recently, reference-free metrics such as CLIPScore (Hessel et al., 2021), UMIC (Lee et al., 2021), and PAC-S (Sarto et al., 2023) have been proposed for automatic reference-free evaluation of image captions. Our focus lies in evaluating the robustness of these metrics in scenarios that require distinguishing between two captions with high lexical overlap but very different meanings. Our findings reveal that despite their high correlation with human judgments, CLIPScore, UMIC, and PAC-S struggle to identify fine-grained errors. While all metrics exhibit strong sensitivity to visual grounding errors, their sensitivity to caption implausibility errors is limited. Furthermore, we found that all metrics are sensitive to variations in the size of image-relevant objects mentioned in the caption, while CLIPScore and PAC-S are also sensitive to the number of mentions of image-relevant objects in the caption. Regarding linguistic aspects of a caption, all metrics show weak comprehension of negat
    
[^166]: 知识图谱推理的神经符号人工智能：一项综述

    Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey

    [https://arxiv.org/abs/2302.07200](https://arxiv.org/abs/2302.07200)

    这项综述介绍了神经符号人工智能在知识图谱推理方面的研究。研究表明，最近的方法试图将符号推理和深度学习相结合，以生成具有解释性、竞争性能力并集成专家知识的模型。

    

    神经符号人工智能是一个日益活跃的研究领域，它将符号推理方法与深度学习相结合，以利用它们的互补优势。随着知识图谱成为表示异构和多关系数据的一种流行方式，对图结构进行推理的方法开始遵循这种神经符号范式。传统上，这些方法要么利用基于规则的推理，要么生成代表性的数值嵌入，从中可以提取出模式。然而，最近的一些研究尝试弥合这种二元对立，提出了能够促进可解释性、保持竞争性能力并集成专家知识的模型。因此，我们调查了在知识图谱上执行神经符号推理任务的方法，并提出了一种新的分类法。具体而言，我们提出了三个主要类别：（1）逻辑信息嵌入方法，（2）基于嵌入的方法与逻辑一致的方法

    Neurosymbolic AI is an increasingly active area of research that combines symbolic reasoning methods with deep learning to leverage their complementary benefits. As knowledge graphs are becoming a popular way to represent heterogeneous and multi-relational data, methods for reasoning on graph structures have attempted to follow this neurosymbolic paradigm. Traditionally, such approaches have utilized either rule-based inference or generated representative numerical embeddings from which patterns could be extracted. However, several recent studies have attempted to bridge this dichotomy to generate models that facilitate interpretability, maintain competitive performance, and integrate expert knowledge. Therefore, we survey methods that perform neurosymbolic reasoning tasks on knowledge graphs and propose a novel taxonomy by which we can classify them. Specifically, we propose three major categories: (1) logically-informed embedding approaches, (2) embedding approaches with logical cons
    
[^167]: 整页无偏学习排序

    Whole Page Unbiased Learning to Rank

    [https://arxiv.org/abs/2210.10718](https://arxiv.org/abs/2210.10718)

    本论文提出整页无偏学习排序（WP-ULTR）方法处理整页 SERP 特征引发的偏差，该方法面临适合的用户行为模型的挑战和复杂的模型训练难题。

    

    信息检索系统中页面呈现的偏见，尤其是点击行为方面的偏差，是一个众所周知的挑战，阻碍了使用隐式用户反馈来改进排序模型的性能。因此，提出了无偏学习排序(ULTR)算法，通过偏差点击数据来学习一个无偏的排序模型。然而，大多数现有算法特别设计用于减轻与位置相关的偏差，例如信任偏差，并未考虑到搜索结果页面呈现(SERP)中其他特征引发的偏差，例如由多媒体引发的吸引偏差。不幸的是，这些偏差在工业系统中广泛存在，可能导致不令人满意的搜索体验。因此，我们引入了一个新的问题，即整页无偏学习排序(WP-ULTR)，旨在同时处理整页SERP特征引发的偏差。这带来了巨大的挑战：(1) 很难找到适合的用户行为模型 (用户行为假设)；(2) 复杂的模型训练问题。

    The page presentation biases in the information retrieval system, especially on the click behavior, is a well-known challenge that hinders improving ranking models' performance with implicit user feedback. Unbiased Learning to Rank~(ULTR) algorithms are then proposed to learn an unbiased ranking model with biased click data. However, most existing algorithms are specifically designed to mitigate position-related bias, e.g., trust bias, without considering biases induced by other features in search result page presentation(SERP), e.g. attractive bias induced by the multimedia. Unfortunately, those biases widely exist in industrial systems and may lead to an unsatisfactory search experience. Therefore, we introduce a new problem, i.e., whole-page Unbiased Learning to Rank(WP-ULTR), aiming to handle biases induced by whole-page SERP features simultaneously. It presents tremendous challenges: (1) a suitable user behavior model (user behavior hypothesis) can be hard to find; and (2) complex
    
[^168]: 加强学习辅助的递归QAOA

    Reinforcement Learning Assisted Recursive QAOA

    [https://arxiv.org/abs/2207.06294](https://arxiv.org/abs/2207.06294)

    递归QAOA是一种非局部的QAOA变体，用于改善近似解的质量。本文通过识别和分析RQAOA失败的案例，提出了一种使用强化学习辅助的方法。

    

    近年来，诸如量子近似优化算法（QAOA）之类的变分量子算法因为它们有望利用NISQ设备解决困难的组合优化问题而变得流行。然而，已知在低深度下，QAOA的某些局部性约束限制了其性能。为了超越这些限制，提出了一种非局部的QAOA变体，即递归QAOA（RQAOA），以提高近似解的质量。RQAOA相对于QAOA的研究较少，人们对其了解较少，例如在哪些实例族上可能无法提供高质量的解。然而，由于我们处理的是NP难问题（具体而言是伊辛自旋模型），预计RQAOA会失败，这引发了设计更好的量子算法来解决组合优化问题的问题。在这种精神下，我们确定并分析了RQAOA失败的案例，并基于此提出了一种强化学习辅助的方法。

    Variational quantum algorithms such as the Quantum Approximation Optimization Algorithm (QAOA) in recent years have gained popularity as they provide the hope of using NISQ devices to tackle hard combinatorial optimization problems. It is, however, known that at low depth, certain locality constraints of QAOA limit its performance. To go beyond these limitations, a non-local variant of QAOA, namely recursive QAOA (RQAOA), was proposed to improve the quality of approximate solutions. The RQAOA has been studied comparatively less than QAOA, and it is less understood, for instance, for what family of instances it may fail to provide high quality solutions. However, as we are tackling $\mathsf{NP}$-hard problems (specifically, the Ising spin model), it is expected that RQAOA does fail, raising the question of designing even better quantum algorithms for combinatorial optimization. In this spirit, we identify and analyze cases where RQAOA fails and, based on this, propose a reinforcement le
    
[^169]: IM-META: 使用节点元数据在未知拓扑网络中进行影响最大化

    IM-META: Influence Maximization Using Node Metadata in Networks With Unknown Topology

    [https://arxiv.org/abs/2106.02926](https://arxiv.org/abs/2106.02926)

    IM-META是一种在未知拓扑网络中进行影响最大化的方法，通过利用节点元数据和查询信息，通过学习元数据与边的关系、构建增强图以及使用拓扑感知排序策略来确定最具影响力的种子节点和查询节点。

    

    由于复杂网络的结构通常是未知的，我们可以通过探索底层网络的一部分来确定具有最大影响力的种子节点，给定节点查询的小预算。我们提出了IM-META，一种解决在未知拓扑网络中影响最大化（IM）问题的方法，通过从查询和节点元数据中检索信息。由于使用这样的元数据并不是没有风险，因为元数据的噪声特性和连通性推断的不确定性，我们制定了一个新的IM问题，旨在找到种子节点和查询节点。在IM-META中，我们开发了一个有效的方法，通过以下三个步骤迭代进行：1）我们通过孪生神经网络学习收集到的元数据与边的关系，2）我们选择一定数量的推断出的可信边来构建一个增强图，3）我们通过使用我们的拓扑感知排序策略，通过最大化推断的影响扩散来确定下一个要查询的节点。通过对IM-META的实验评估，我们证明了其有效性。

    Since the structure of complex networks is often unknown, we may identify the most influential seed nodes by exploring only a part of the underlying network, given a small budget for node queries. We propose IM-META, a solution to influence maximization (IM) in networks with unknown topology by retrieving information from queries and node metadata. Since using such metadata is not without risk due to the noisy nature of metadata and uncertainties in connectivity inference, we formulate a new IM problem that aims to find both seed nodes and queried nodes. In IM-META, we develop an effective method that iteratively performs three steps: 1) we learn the relationship between collected metadata and edges via a Siamese neural network, 2) we select a number of inferred confident edges to construct a reinforced graph, and 3) we identify the next node to query by maximizing the inferred influence spread using our topology-aware ranking strategy. Through experimental evaluation of IM-META on fou
    
[^170]: SimFair：物理引导的公平感知学习与模拟模型

    SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models. (arXiv:2401.15270v1 [cs.LG])

    [http://arxiv.org/abs/2401.15270](http://arxiv.org/abs/2401.15270)

    SimFair是一种物理引导的公平感知学习框架，通过集成模拟和逆向建模来解决纯数据驱动的公平性问题，有效保持公平性。

    

    公平感知已经成为在真实应用中负责任使用人工智能的关键基础。在许多情况下，性能不平等是由于不同区域分布的变化引起的。虽然已经开发出提高公平可迁移性的技术，但是在没有来自新区域的样本的情况下解决这个问题并不总是可行的，这对于纯数据驱动的尝试是一个瓶颈。幸运的是，基于物理机制模型已经在许多具有重大社会影响的问题上进行了研究。我们提出了SimFair，一种物理引导的公平感知学习框架，通过集成基于物理规则的模拟和逆向建模到训练设计中来弥补数据限制。以温度预测为例，我们演示了所提出的SimFair在保持公平性方面的有效性。

    Fairness-awareness has emerged as an essential building block for the responsible use of artificial intelligence in real applications. In many cases, inequity in performance is due to the change in distribution over different regions. While techniques have been developed to improve the transferability of fairness, a solution to the problem is not always feasible with no samples from the new regions, which is a bottleneck for pure data-driven attempts. Fortunately, physics-based mechanistic models have been studied for many problems with major social impacts. We propose SimFair, a physics-guided fairness-aware learning framework, which bridges the data limitation by integrating physical-rule-based simulation and inverse modeling into the training design. Using temperature prediction as an example, we demonstrate the effectiveness of the proposed SimFair in fairness preservation.
    
[^171]: 大规模异构图上基于大型语言模型的链接预测的可扩展性研究

    Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])

    [http://arxiv.org/abs/2401.13227](http://arxiv.org/abs/2401.13227)

    本研究探索了在大规模异构图上应用大型语言模型进行图学习的方法，提出了LPNL框架用于可扩展链接预测。通过创新的提示语和采样流程，以及分而治之的策略，成功解决了大规模图中的信息过载问题，并在实验中表现出了优越的性能。

    

    探索将大规模语言模型应用于图学习是一项新颖的努力。然而，大图中蕴含的大量信息给这一过程带来了重大挑战。本文侧重于链接预测任务，并介绍了LPNL（Link Prediction via Natural Language），这是一个基于大型语言模型的框架，用于大规模异构图上的可扩展链接预测。我们设计了能以自然语言表达图细节的创新提示语。我们提出了一个两阶段的采样流程，从大规模异构图中提取关键信息，并采用分而治之的策略来控制输入令牌数量在预定限制内，解决了信息过载的挑战。我们还通过自监督学习设计了一个用于链接预测的T5模型进行微调。在大型公共异构图上进行的广泛实验表明，LPNL的性能超过了各种先进的基准模型。

    Exploring the application of large-scale language models to graph learning is a novel endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to this process. This paper focuses on the link prediction task and introduces LPNL (Link Prediction via Natural Language), a framework based on a large language model designed for scalable link prediction on large-scale heterogeneous graphs.We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from large-scale heterogeneous graphs, and a divide-and-conquer strategy to control the input token count within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for for link prediction. Extensive experiments on a large public heterogeneous graphs demonstrate that LPNL outperforms various advanced baselin
    
[^172]: 从理解的角度探讨语言模型的关键数据规模

    Critical Data Size of Language Models from a Grokking Perspective. (arXiv:2401.10463v1 [cs.CL])

    [http://arxiv.org/abs/2401.10463](http://arxiv.org/abs/2401.10463)

    本文从理解的角度探讨了语言模型中的关键数据规模，证明了只有当语言模型达到关键大小时才会发生泛化，同时揭示了更大的模型需要更多数据的趋势。

    

    我们探讨了语言模型中的关键数据规模，这是从快速记忆到缓慢泛化的一个基本转变阈值。我们将这个相变形式化为Grokking配置下的数据效率假说，并确定了语言模型训练动力学中的数据不足、充足和过剩阶段。我们通过重新调整初始化和权重衰减，开发出了一种Grokking配置，稳定地在简化的语言模型上重现了Grokking。我们表明只有当语言模型达到关键大小时才会发生泛化。我们分析了样本级和模型级的Grokking，验证了提出的数据效率假说。我们的实验揭示了语言数据集的关键数据集大小处发生的更平滑的相变。随着模型大小的增加，这个临界点也变得更大，这表明更大的模型需要更多的数据。我们的研究结果加深了对语言模型训练的理解，提供了一种新颖的视角。

    We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel pers
    
[^173]: 走向基于原则的图形变换器

    Towards Principled Graph Transformers. (arXiv:2401.10119v1 [cs.LG])

    [http://arxiv.org/abs/2401.10119](http://arxiv.org/abs/2401.10119)

    边缘变换器是一个全局注意力模型，它具有至少3-WL的表达能力，能够在预测性能上超过其他架构，而不依赖于位置或结构编码。

    

    基于k维Weisfeiler-Leman（k-WL）层次结构的图形学习架构提供了理论上很好理解的表达能力。然而，这样的架构在真实任务中往往无法提供可靠的预测性能，从而限制了它们的实际影响力。相比之下，基于全局注意力的模型如图形变换器在实践中表现出了强大的性能，但是将它们的表达能力与k-WL层次结构进行比较仍然具有挑战性，尤其是因为这些架构依赖于位置或结构编码来实现其表达能力和预测性能。为了解决这个问题，我们展示了最近提出的边缘变换器，这是一个在节点对而不是节点上进行操作的全局注意力模型，具有至少3-WL的表达能力。经验上，我们证明了边缘变换器在预测性能上超过了其他理论对齐的架构，同时不依赖于位置或结构编码。

    Graph learning architectures based on the k-dimensional Weisfeiler-Leman (k-WL) hierarchy offer a theoretically well-understood expressive power. However, such architectures often fail to deliver solid predictive performance on real-world tasks, limiting their practical impact. In contrast, global attention-based models such as graph transformers demonstrate strong performance in practice, but comparing their expressive power with the k-WL hierarchy remains challenging, particularly since these architectures rely on positional or structural encodings for their expressivity and predictive performance. To address this, we show that the recently proposed Edge Transformer, a global attention model operating on node pairs instead of nodes, has at least 3-WL expressive power. Empirically, we demonstrate that the Edge Transformer surpasses other theoretically aligned architectures regarding predictive performance while not relying on positional or structural encodings.
    
[^174]: 在样本高效的离线强化学习中：数据多样性、后验采样，以及更多

    On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond. (arXiv:2401.03301v1 [cs.LG])

    [http://arxiv.org/abs/2401.03301](http://arxiv.org/abs/2401.03301)

    本文提出了通过数据多样性概念来统一离线强化学习算法的方法，并证明了基于版本空间、正则化优化和后验采样的算法在标准假设下达到了可比的样本效率。

    

    我们试图理解什么促进了对于序贝叶斯决策的历史数据集进行样本高效学习，这个问题通常被称为离线强化学习（RL）。此外，我们对于在利用（值）函数逼近的同时享受样本效率的算法感兴趣。在本文中，我们通过提出一个包括离线RL中覆盖度量的先前概念的数据多样性概念来解决这些基本问题，并且利用这个概念将基于版本空间（VS）、正则化优化（RO）和后验采样（PS）的三个不同类别的离线RL算法进行统一。我们在标准假设下证明，基于VS、基于RO和基于PS的算法达到了\emph{可比}的样本效率，这恢复了在有限和线性模型类别下的最优性的标准假设的边界。这个结果令人惊讶，因为之前的研究表明这些算法不具有有利性的样本效率。

    We seek to understand what facilitates sample-efficient learning from historical datasets for sequential decision-making, a problem that is popularly known as offline reinforcement learning (RL). Further, we are interested in algorithms that enjoy sample efficiency while leveraging (value) function approximation. In this paper, we address these fundamental questions by (i) proposing a notion of data diversity that subsumes the previous notions of coverage measures in offline RL and (ii) using this notion to {unify} three distinct classes of offline RL algorithms based on version spaces (VS), regularized optimization (RO), and posterior sampling (PS). We establish that VS-based, RO-based, and PS-based algorithms, under standard assumptions, achieve \emph{comparable} sample efficiency, which recovers the state-of-the-art sub-optimality bounds for finite and linear model classes with the standard assumptions. This result is surprising, given that the prior work suggested an unfavorable sa
    
[^175]: 用简单的功率分析在32位微控制器上阅读神经网络架构

    Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers. (arXiv:2311.01344v1 [cs.CR])

    [http://arxiv.org/abs/2311.01344](http://arxiv.org/abs/2311.01344)

    本文研究了如何通过简单的功率分析方法，在32位微控制器上提取深度神经网络模型的架构信息。

    

    模型提取是AI系统安全的一个不断增长的关注点。对于深度神经网络模型来说，架构是对手试图恢复的最重要的信息。作为一系列重复计算块，部署在边缘设备上的神经网络模型将产生独特的侧信道泄露。当目标平台在物理上可访问时，可以利用这些信道泄露来提取关键信息。通过结合对深度学习实践的理论知识和对广泛使用的实现库（ARM CMSIS-NN）的分析，我们的目的是回答这个关键问题：通过简单地检查一个EM侧信道跟踪，我们可以提取多远的架构信息？我们首次提出了一种用于传统MLP和CNN模型的提取方法，这些模型在高端32位微控制器（Cortex-M7）上运行，仅依赖于简单的模式识别分析。尽管存在几个具有挑战性的情况，但我们声称，与参数提取相反，我们可以通过这种方法从简单的功率分析中提取出架构信息。

    Model extraction is a growing concern for the security of AI systems. For deep neural network models, the architecture is the most important information an adversary aims to recover. Being a sequence of repeated computation blocks, neural network models deployed on edge-devices will generate distinctive side-channel leakages. The latter can be exploited to extract critical information when targeted platforms are physically accessible. By combining theoretical knowledge about deep learning practices and analysis of a widespread implementation library (ARM CMSIS-NN), our purpose is to answer this critical question: how far can we extract architecture information by simply examining an EM side-channel trace? For the first time, we propose an extraction methodology for traditional MLP and CNN models running on a high-end 32-bit microcontroller (Cortex-M7) that relies only on simple pattern recognition analysis. Despite few challenging cases, we claim that, contrary to parameters extraction
    
[^176]: 揭示偏见和不平等：利用电子健康记录的医疗人工智能中偏见检测和缓解的系统综述

    Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records. (arXiv:2310.19917v1 [cs.AI])

    [http://arxiv.org/abs/2310.19917](http://arxiv.org/abs/2310.19917)

    本综述对涉及利用电子健康记录数据的医疗人工智能研究中的偏见进行了系统综述，共涵盖了六种主要的偏见类型，同时总结了现有的偏见处理方法。

    

    目的：利用电子健康记录的人工智能应用在医疗领域越来越受到欢迎，但也引入了各种类型的偏见。本研究旨在系统综述涉及利用电子健康记录数据的人工智能研究中的偏见。方法：遵循Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA)准则进行了系统综述。从PubMed、Web of Science和电气和电子工程师学会中检索了2010年1月1日至2022年10月31日期间发表的文章。我们定义了六种主要的偏见类型，并总结了现有的偏见处理方法。结果：在检索到的252篇文章中，有20篇符合最终综述的纳入标准。本综述涵盖了六种偏见中的五种：八项研究分析了选择偏见；六项研究针对隐性偏见；五项研究对混杂偏见进行了研究；四项研究对测量偏见进行了研究；两项研究对算法偏见进行了研究。在偏见处理方法方面，有十项研究进行了探讨。

    Objectives: Artificial intelligence (AI) applications utilizing electronic health records (EHRs) have gained popularity, but they also introduce various types of bias. This study aims to systematically review the literature that address bias in AI research utilizing EHR data. Methods: A systematic review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guideline. We retrieved articles published between January 1, 2010, and October 31, 2022, from PubMed, Web of Science, and the Institute of Electrical and Electronics Engineers. We defined six major types of bias and summarized the existing approaches in bias handling. Results: Out of the 252 retrieved articles, 20 met the inclusion criteria for the final review. Five out of six bias were covered in this review: eight studies analyzed selection bias; six on implicit bias; five on confounding bias; four on measurement bias; two on algorithmic bias. For bias handling approaches, ten st
    
[^177]: 使用人设来建模语言模型中的真实性

    Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])

    [http://arxiv.org/abs/2310.18168](http://arxiv.org/abs/2310.18168)

    本研究探讨了在大型语言模型中使用人设来建模真实性的可能性。通过建模真实人设，语言模型可以将真实性推广到不同上下文中，并通过相关特征判断个体产生文本的真实性。

    

    大型语言模型使用互联网上的大量文本进行训练，这些文本中既包含了事实，也包含了误导性的信息。语言模型能够从这些相互矛盾的数据中辨别真实与虚假吗？基于语言模型能够建模不同产生文本的个体这一观点，我们假设它们可以通过建模真实人设来聚类真实文本：一群很可能产生真实文本并具有相似特征的个体。例如，可信源如维基百科和科学期刊通常使用正式的写作风格并提出一致的主张。通过建模这一人设，语言模型可以将真实性推广到每个个体生成训练文本的特定上下文之外。例如，模型可以推断出“维基百科”这个个体在“科学”生成的主题上会表现出真实性，因为它们共享一个人设。我们首先通过两个观察结果为人设假设提供了证据：（1）我们可以探测模型在不同领域中判断真实性的能力；（2）模型可以从相关特征中推测个体产生文本的真实性。

    Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod
    
[^178]: O3D: 基于离线数据的发现与蒸馏方法，用于大规模语言模型在顺序决策中的应用

    O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models. (arXiv:2310.14403v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.14403](http://arxiv.org/abs/2310.14403)

    O3D提出了一种基于离线数据的学习框架，利用大规模数据改进了大规模语言模型在顺序决策问题中的性能，通过自动发现可重复使用的技能，提高了模型的表现

    

    最近对大规模语言模型 (LLMs)的研究取得了令人期待的进展，在解决顺序决策问题方面显示出了良好的性能。通过模仿提示中提供的少量示例（即上下文学习），LLM代理可以与外部环境交互并完成给定任务，而无需额外的训练。然而，这种少量示例往往不足以生成复杂且长期目标任务的高质量解决方案，而有限的上下文长度无法处理更大规模的演示。为此，我们提出了一种利用离线数据的学习框架，以大规模的离线数据（例如人类交互的日志）来改进LLM代理的上下文学习性能。我们通过文本和代码两种方法正式定义了LLM强化策略。然后，我们引入了一种名为O3D的离线数据驱动发现和蒸馏框架，以改善LLM强化策略而无需微调。O3D自动地发现可重复使用的技能

    Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high-quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to facilitate the in-context learning performance of LLM agents. We formally define LLM-powered policies with both text-based approaches and code-based approaches. We then introduce an Offline Data-driven Discovery and Distillation (O3D) framework to improve LLM-powered policies without finetuning. O3D automatically discovers reusable skills
    
[^179]: LASER：无线分布式优化中的线性压缩

    LASER: Linear Compression in Wireless Distributed Optimization. (arXiv:2310.13033v1 [cs.NE])

    [http://arxiv.org/abs/2310.13033](http://arxiv.org/abs/2310.13033)

    LASER是一种新的压缩方案，通过利用梯度的低秩结构在噪声通道上高效传输梯度，相对于现有方案在计算机视觉和GPT语言建模任务上表现出持续优势。

    

    数据并行SGD是分布式优化的事实上的算法，尤其适用于大规模机器学习。尽管它有很多优点，但通信瓶颈是其中持久存在的问题之一。大多数压缩方案要么假设通信链路无噪声，要么在实际任务中无法取得良好的性能。在本文中，我们填补了这一空白，介绍了LASER：无线分布式优化中的线性压缩。LASER利用梯度的固有低秩结构，在噪声通道上高效传输梯度。尽管享受与经典SGD相似的理论保证，LASER在各种实际基准测试中表现出持续的优势。特别是，在具有挑战性的计算机视觉和GPT语言建模任务中，它优于最先进的压缩方案。在后者中，我们相对于噪声通道上的基准模型在困惑度上获得了50-64%的提升。

    Data-parallel SGD is the de facto algorithm for distributed optimization, especially for large scale machine learning. Despite its merits, communication bottleneck is one of its persistent issues. Most compression schemes to alleviate this either assume noiseless communication links, or fail to achieve good performance on practical tasks. In this paper, we close this gap and introduce LASER: LineAr CompreSsion in WirEless DistRibuted Optimization. LASER capitalizes on the inherent low-rank structure of gradients and transmits them efficiently over the noisy channels. Whilst enjoying theoretical guarantees similar to those of the classical SGD, LASER shows consistent gains over baselines on a variety of practical benchmarks. In particular, it outperforms the state-of-the-art compression schemes on challenging computer vision and GPT language modeling tasks. On the latter, we obtain $50$-$64 \%$ improvement in perplexity over our baselines for noisy channels.
    
[^180]: 用于分析网络上的枪支走私活动的自监督视觉学习

    Self-supervised visual learning for analyzing firearms trafficking activities on the Web. (arXiv:2310.07975v1 [cs.CV])

    [http://arxiv.org/abs/2310.07975](http://arxiv.org/abs/2310.07975)

    本论文提出了一种自监督视觉学习方法，用于分析网络上的枪支走私活动。这种方法利用深度神经网络和卷积神经网络，通过从大规模通用数据集预训练，再在特定数据集上进行微调，实现对枪支的分类。

    

    从RGB图像中自动化地分类枪支是一项重要的现实任务，其应用范围涉及公共空间安全、情报收集和执法调查。当应用于从全球范围内大规模抓取的图像（包括社交媒体和暗网站）时，它可以作为分析开放源情报中的大数据的系统的重要组成部分，以试图识别犯罪枪支走私网络。深度神经网络（DNN）是实现这一目标的最先进方法，其中通常使用卷积神经网络（CNN）。常见的迁移学习方法包括在大规模的通用注释数据集（如ImageNet-1k）上进行预训练进行整体图像分类，然后在较小的、注释的、任务特定的下游数据集上对DNN进行微调以进行枪支视觉分类。

    Automated visual firearms classification from RGB images is an important real-world task with applications in public space security, intelligence gathering and law enforcement investigations. When applied to images massively crawled from the World Wide Web (including social media and dark Web sites), it can serve as an important component of systems that attempt to identify criminal firearms trafficking networks, by analyzing Big Data from open-source intelligence. Deep Neural Networks (DNN) are the state-of-the-art methodology for achieving this, with Convolutional Neural Networks (CNN) being typically employed. The common transfer learning approach consists of pretraining on a large-scale, generic annotated dataset for whole-image classification, such as ImageNet-1k, and then finetuning the DNN on a smaller, annotated, task-specific, downstream dataset for visual firearms classification. Neither Visual Transformer (ViT) neural architectures nor Self-Supervised Learning (SSL) approach
    
[^181]: 探索大型语言模型中类比识别与句子结构编码之间的关系

    Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models. (arXiv:2310.07818v1 [cs.CL])

    [http://arxiv.org/abs/2310.07818](http://arxiv.org/abs/2310.07818)

    这项研究探究了大型语言模型中识别句子类比的能力与其编码句法和语义结构能力之间的关系。

    

    识别类比在人类认知和语言能力中起着重要作用。在过去的十年里，对于“A对B就像C对D”这种形式的词语类比进行了广泛的研究。然而，对于涉及更长文本的类比，如句子和句子集合，传达类比意义的问题引起了越来越多的兴趣。当前的自然语言处理研究社区评估大型语言模型（LLMs）识别此类类比的能力，但这些能力背后的原因需要进一步探究。此外，LLMs在其嵌入中编码语言的句法和语义结构的能力，近年来得到了显著关注。在这项工作中，我们研究了多个LLMs识别句子类比的能力与其编码句法和语义结构的能力之间的关系。通过分析，我们发现LLMs的类比识别能力与其编码能力有关。

    Identifying analogies plays a pivotal role in human cognition and language proficiency. In the last decade, there has been extensive research on word analogies in the form of ``A is to B as C is to D.'' However, there is a growing interest in analogies that involve longer text, such as sentences and collections of sentences, which convey analogous meanings. While the current NLP research community evaluates the ability of Large Language Models (LLMs) to identify such analogies, the underlying reasons behind these abilities warrant deeper investigation. Furthermore, the capability of LLMs to encode both syntactic and semantic structures of language within their embeddings has garnered significant attention with the surge in their utilization. In this work, we examine the relationship between the abilities of multiple LLMs to identify sentence analogies, and their capacity to encode syntactic and semantic structures. Through our analysis, we find that analogy identification ability of LL
    
[^182]: 把坏人踢出去！基于零知识证明的联邦学习异常检测

    Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning. (arXiv:2310.04055v1 [cs.CR])

    [http://arxiv.org/abs/2310.04055](http://arxiv.org/abs/2310.04055)

    本文提出了一种基于零知识证明的联邦学习异常检测方法，实现了在实际系统中检测和消除恶意客户端模型的能力。

    

    联邦学习系统容易受到恶意客户端的攻击，他们通过提交篡改的本地模型来达到对抗目标，比如阻止全局模型的收敛或者导致全局模型对某些数据进行错误分类。许多现有的防御机制在实际联邦学习系统中不可行，因为它们需要先知道恶意客户端的数量，或者依赖重新加权或修改提交的方式。这是因为攻击者通常不会在攻击之前宣布他们的意图，而重新加权可能会改变聚合结果，即使没有攻击。为了解决这些在实际联邦学习系统中的挑战，本文引入了一种最尖端的异常检测方法，具有以下特点：i）仅在发生攻击时检测攻击的发生并进行防御操作；ii）一旦发生攻击，进一步检测恶意客户端模型并将其消除，而不会对正常模型造成伤害；iii）确保

    Federated learning (FL) systems are vulnerable to malicious clients that submit poisoned local models to achieve their adversarial goals, such as preventing the convergence of the global model or inducing the global model to misclassify some data. Many existing defense mechanisms are impractical in real-world FL systems, as they require prior knowledge of the number of malicious clients or rely on re-weighting or modifying submissions. This is because adversaries typically do not announce their intentions before attacking, and re-weighting might change aggregation results even in the absence of attacks. To address these challenges in real FL systems, this paper introduces a cutting-edge anomaly detection approach with the following features: i) Detecting the occurrence of attacks and performing defense operations only when attacks happen; ii) Upon the occurrence of an attack, further detecting the malicious client models and eliminating them without harming the benign ones; iii) Ensuri
    
[^183]: OceanGPT：用于海洋科学任务的大型语言模型

    OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])

    [http://arxiv.org/abs/2310.02031](http://arxiv.org/abs/2310.02031)

    OceanGPT是首个专为海洋科学任务设计的大型语言模型，通过DoInstruct框架实现自动获取海洋领域指导数据。这一模型的引入填补了海洋科学领域中对LLM的需求缺口，并为海洋科学研究提供了新的工具和方法。

    

    海洋科学是探索充满生命和生物多样性的海洋的科学，考虑到海洋覆盖了地球表面的70％以上，这一领域具有重要意义。最近，大型语言模型（LLM）的进展改变了科学的范式。尽管在其他领域取得了成功，但现有的LLM通常无法满足海洋学家等领域专家的需求，同时对LLM在海洋科学中的潜力尚未得到充分探索。这其中的根本原因可能是海洋数据的庞大而复杂的性质，以及对更高的粒度和丰富的知识的需求。为了解决这些问题，我们推出了首个海洋领域的LLM——OceanGPT，该模型擅长各种海洋科学任务。我们提出了一个新颖的框架DoInstruct，用于自动获取大量的海洋领域指导数据，它基于多智能体的协作生成指导。

    Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
    
[^184]: 图神经网络能否作为最优近似算法？

    Are Graph Neural Networks Optimal Approximation Algorithms?. (arXiv:2310.00526v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00526](http://arxiv.org/abs/2310.00526)

    本文设计了图神经网络架构OptGNN，利用半定规划工具获得大类组合优化问题的最优近似算法。通过实证结果表明在各种数据集上超过了神经网络基线算法和传统算法，同时利用OptGNN的能力设计了一个产生优化的对偶证书的算法。

    

    在这项工作中，我们设计了能够使用半定规划（SDP）强大的算法工具来获得大类组合优化问题的最优近似算法的图神经网络架构。具体而言，我们证明了多项式大小的消息传递算法可以表示最强大的多项式时间算法，前提是假设唯一游戏猜想成立。我们利用这一结果构建了高效的图神经网络架构OptGNN，它在诸如最大割和最大独立集等重要组合优化问题上获得了高质量的近似解。我们的方法在各种真实世界和合成数据集上表现出强大的实证结果，不仅超过了神经网络基线算法，还超过了传统算法。最后，我们利用OptGNN捕捉凸松弛的能力，设计了一个产生优化的对偶证书（确定性上界）的算法。

    In this work we design graph neural network architectures that can be used to obtain optimal approximation algorithms for a large class of combinatorial optimization problems using powerful algorithmic tools from semidefinite programming (SDP). Concretely, we prove that polynomial-sized message passing algorithms can represent the most powerful polynomial time algorithms for Max Constraint Satisfaction Problems assuming the Unique Games Conjecture. We leverage this result to construct efficient graph neural network architectures, OptGNN, that obtain high-quality approximate solutions on landmark combinatorial optimization problems such as Max Cut and maximum independent set. Our approach achieves strong empirical results across a wide range of real-world and synthetic datasets against both neural baselines and classical algorithms. Finally, we take advantage of OptGNN's ability to capture convex relaxations to design an algorithm for producing dual certificates of optimality (bounds on
    
[^185]: 使用AI不确定性量化改进人类决策

    Using AI Uncertainty Quantification to Improve Human Decision-Making. (arXiv:2309.10852v1 [cs.AI])

    [http://arxiv.org/abs/2309.10852](http://arxiv.org/abs/2309.10852)

    本论文研究了使用AI不确定性量化改进人类决策的方法，并通过基于实例的UQ和行为实验验证了其性能优势。

    

    AI不确定性量化（UQ）有可能通过为用户提供有用的概率信息，超越单纯的AI预测，从而改进人类决策。过去的大部分关于AI和人类决策的研究都集中在模型可解释性和可解释性上。我们运用基于实例的UQ在三个真实数据集上实现了这一目标。为了实现这一目标，我们为每个数据集训练了不同的分类AI模型，并使用在给定实例附近生成的随机样本创建了UQ的置信区间。我们使用严格的适当评分规则对计算出的UQ进行了校准，作为UQ的质量保证。然后，我们进行了两个预先注册的在线行为实验，比较了不同AI信息条件下客观人类决策表现，包括UQ。在实验1中，我们比较了不使用AI（对照组）、仅使用AI预测和使用AI预测及UQ可视化的决策。我们发现UQ的使用可以提高人类决策的性能。

    AI Uncertainty Quantification (UQ) has the potential to improve human decision-making beyond AI predictions alone by providing additional useful probabilistic information to users. The majority of past research on AI and human decision-making has concentrated on model explainability and interpretability. We implemented instance-based UQ for three real datasets. To achieve this, we trained different AI models for classification for each dataset, and used random samples generated around the neighborhood of the given instance to create confidence intervals for UQ. The computed UQ was calibrated using a strictly proper scoring rule as a form of quality assurance for UQ. We then conducted two preregistered online behavioral experiments that compared objective human decision-making performance under different AI information conditions, including UQ. In Experiment 1, we compared decision-making for no AI (control), AI prediction alone, and AI prediction with a visualization of UQ. We found UQ
    
[^186]: P-ROCKET: 针对时间序列分类的随机卷积核剪枝

    P-ROCKET: Pruning Random Convolution Kernels for Time Series Classification. (arXiv:2309.08499v1 [cs.LG])

    [http://arxiv.org/abs/2309.08499](http://arxiv.org/abs/2309.08499)

    本研究提出了一种名为P-ROCKET的方法，通过在特征选择的角度删除卷积核，从而实现对时间序列分类中的随机卷积核进行剪枝。

    

    在最近几年，两个时间序列分类模型ROCKET和MINIROCKET因其低训练成本和最先进的准确性而受到广泛关注。ROCKET和MINIROCKET利用无需训练的随机一维卷积核，可以快速从时间序列数据中提取特征，从而实现线性分类器的高效拟合。然而，为了全面捕捉有用的特征，需要大量的随机卷积核，这对于资源受限的设备来说是不兼容的。因此，我们设计了一种启发式进化算法S-ROCKET，用于识别和剪枝冗余的卷积核。然而，进化算法本身的特性导致在S-ROCKET中评估卷积核是一个耗时的过程。本文中，与直接评估具有非显著差异的随机卷积核的S-ROCKET不同，我们从特征选择的角度删除卷积核，通过消除序列中的相关连接来实现。

    In recent years, two time series classification models, ROCKET and MINIROCKET, have attracted much attention for their low training cost and state-of-the-art accuracy. Utilizing random 1-D convolutional kernels without training, ROCKET and MINIROCKET can rapidly extract features from time series data, allowing for the efficient fitting of linear classifiers. However, to comprehensively capture useful features, a large number of random kernels are required, which is incompatible for resource-constrained devices. Therefore, a heuristic evolutionary algorithm named S-ROCKET is devised to recognize and prune redundant kernels. Nevertheless, the inherent nature of evolutionary algorithms renders the evaluation of kernels within S-ROCKET an unacceptable time-consuming process. In this paper, diverging from S-ROCKET, which directly evaluates random kernels with nonsignificant differences, we remove kernels from a feature selection perspective by eliminating associating connections in the sequ
    
[^187]: 评估大规模语言模型的性质：对人类中心主义的警告

    Assessing the nature of large language models: A caution against anthropocentrism. (arXiv:2309.07683v1 [cs.AI])

    [http://arxiv.org/abs/2309.07683](http://arxiv.org/abs/2309.07683)

    通过评估GPT3.5，我们发现它具有有趣的个性问卷回答能力，但不太可能发展出意识，并显示出较大的认知和个性变异。

    

    生成式人工智能模型通过OpenAI的聊天机器人ChatGPT的发布引起了公众的关注和猜测。目前存在两种意见阵营：一方对这些模型为人类任务带来的基本变革的可能性感到兴奋，另一方对这些模型的强大能力感到高度关切。为了应对这些关切，我们使用了标准、规范化和经过验证的认知和个性测量工具来评估GPT3.5。在这个初步项目中，我们开发了一套测试，可以估计这些模型的能力边界，它们在短时间内的稳定性以及与人类的比较。我们的结果表明，GPT 3.5很可能没有产生意识，尽管它对个性问卷的回答能力令人感兴趣。它在重复观察过程中显示出认知和个性测量方面的大量变异，这与具有人类般个性的模型是不符合预期的。

    Generative AI models garnered a large amount of public attention and speculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion camps exist: one excited about possibilities these models offer for fundamental changes to human tasks, and another highly concerned about power these models seem to have. To address these concerns, we assessed GPT3.5 using standard, normed, and validated cognitive and personality measures. For this seedling project, we developed a battery of tests that allowed us to estimate the boundaries of some of these models capabilities, how stable those capabilities are over a short period of time, and how they compare to humans.  Our results indicate that GPT 3.5 is unlikely to have developed sentience, although its ability to respond to personality inventories is interesting. It did display large variability in both cognitive and personality measures over repeated observations, which is not expected if it had a human-like personality. Variability 
    
[^188]: NESTLE：一种用于法律语料库统计分析的无代码工具

    NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus. (arXiv:2309.04146v1 [cs.CL])

    [http://arxiv.org/abs/2309.04146](http://arxiv.org/abs/2309.04146)

    NESTLE是一个无代码工具，用于进行大规模法律语料库的统计分析。它提供了搜索引擎、端到端的信息提取系统和一个大语言模型，可以通过聊天界面进行操作，使用户可以搜索目标文件、提取信息并可视化数据。

    

    大规模法律语料库的统计分析可以提供有价值的法律见解。为了进行这样的分析，需要使用文档检索工具选择语料库的子集，使用信息提取（IE）系统对文本进行结构化，并对数据进行可视化以进行统计分析。每个过程都需要专业工具或编程技能，然而还没有全面的无代码工具可用。尤其是对于IE，如果IE系统的本体中没有预定义的目标信息，那么需要自己构建系统。在这里，我们提供了NESTLE，一种用于大规模法律语料库统计分析的无代码工具。通过NESTLE，用户可以通过聊天界面搜索目标文件、提取信息，并通过辅助GUI进行细致级别的控制来可视化结构化数据。NESTLE由三个主要组件组成：搜索引擎、端到端的IE系统和一个大语言模型（LLM），它将各个组件连接起来。

    The statistical analysis of large scale legal corpus can provide valuable legal insights. For such analysis one needs to (1) select a subset of the corpus using document retrieval tools, (2) structuralize text using information extraction (IE) systems, and (3) visualize the data for the statistical analysis. Each process demands either specialized tools or programming skills whereas no comprehensive unified "no-code" tools have been available. Especially for IE, if the target information is not predefined in the ontology of the IE system, one needs to build their own system. Here we provide NESTLE, a no code tool for large-scale statistical analysis of legal corpus. With NESTLE, users can search target documents, extract information, and visualize the structured data all via the chat interface with accompanying auxiliary GUI for the fine-level control. NESTLE consists of three main components: a search engine, an end-to-end IE system, and a Large Language Model (LLM) that glues the who
    
[^189]: DECODE: 基于历史数据和环境因素的数据驱动能耗预测在建筑中的应用

    DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings. (arXiv:2309.02908v1 [cs.LG])

    [http://arxiv.org/abs/2309.02908](http://arxiv.org/abs/2309.02908)

    本论文介绍了一种基于历史数据、占用模式和天气条件的LSTM模型，用于准确预测建筑能耗，该模型在预测精度上表现出卓越性能。

    

    建筑中的能耗预测在有效的能源管理中起着至关重要的作用。精确的预测对于实现优化的能耗和电网分配是必要的。本论文引入了一种基于历史能源数据、占用模式和天气条件的长短期记忆（LSTM）模型，用于预测建筑能耗。与现有的预测模型相比，LSTM模型提供了准确的短期、中期和长期能耗预测，适用于住宅和商业建筑。我们将我们的LSTM模型与线性回归、决策树和随机森林等已有的预测方法进行了比较。令人鼓舞的是，提出的LSTM模型在所有指标上表现出优越性能。它具有出色的预测精度，R2得分为0.97，最低的平均绝对误差（MAE）为0.007。我们开发的模型的另一个优点是能够实现高效的能耗。

    Energy prediction in buildings plays a crucial role in effective energy management. Precise predictions are essential for achieving optimal energy consumption and distribution within the grid. This paper introduces a Long Short-Term Memory (LSTM) model designed to forecast building energy consumption using historical energy data, occupancy patterns, and weather conditions. The LSTM model provides accurate short, medium, and long-term energy predictions for residential and commercial buildings compared to existing prediction models. We compare our LSTM model with established prediction methods, including linear regression, decision trees, and random forest. Encouragingly, the proposed LSTM model emerges as the superior performer across all metrics. It demonstrates exceptional prediction accuracy, boasting the highest R2 score of 0.97 and the most favorable mean absolute error (MAE) of 0.007. An additional advantage of our developed model is its capacity to achieve efficient energy consu
    
[^190]: 多途径适配器：为可扩展的图像-文本检索调整大规模多模态模型

    MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval. (arXiv:2309.01516v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.01516](http://arxiv.org/abs/2309.01516)

    多途径适配器是一个创新的框架，利用"对齐增强器"加深模态对齐，实现高可转移性，可有效减少调整参数的时间并提高零样本图像-文本检索性能。

    

    随着大规模多模态模型（LMMs）的规模不断增加，将这些预训练模型调整到专门的任务上已成为一个计算和内存密集的挑战。传统的微调方法需要为每个新任务进行孤立、穷举的重新调整，限制了模型的多功能性。此外，当前的高效调整技术经常忽视模态对齐，仅关注新任务的知识提取。为了解决这些问题，我们引入了多途径适配器，这是一个创新的框架，它包含了一个“对齐增强器”，可以加深模态对齐，实现高度的可转移性而无需调整预训练参数。我们的方法仅向LMMs添加了不到1.25%的额外参数，以BEiT-3模型为例。与完全微调的模型相比，我们的方法在零样本图像-文本检索性能上具有优势，同时缩短了高达57%的微调时间。我们的方法提供了一种资源高效和高效的方法。

    As the size of Large Multi-Modal Models (LMMs) increases consistently, the adaptation of these pre-trained models to specialized tasks has become a computationally and memory-intensive challenge. Traditional fine-tuning methods require isolated, exhaustive retuning for each new task, limiting the models' versatility. Moreover, current efficient adaptation techniques often overlook modality alignment, focusing only on the knowledge extraction of new tasks. To tackle these issues, we introduce Multiway-Adapter, an innovative framework incorporating an 'Alignment Enhancer' to deepen modality alignment, enabling high transferability without tuning pre-trained parameters. Our method adds fewer than 1.25\% of additional parameters to LMMs, exemplified by the BEiT-3 model in our study. This leads to superior zero-shot image-text retrieval performance compared to fully fine-tuned models, while achieving up to a 57\% reduction in fine-tuning time. Our approach offers a resource-efficient and ef
    
[^191]: 透过偏好看大型语言模型的反馈获取：揭示对齐的重要性

    Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v1 [cs.LG])

    [http://arxiv.org/abs/2308.15812](http://arxiv.org/abs/2308.15812)

    本研究分析了对于对齐和评估大型语言模型而言，设计反馈选择是评分还是排名对结果的影响。研究发现评分和排名所推断出的偏好存在不一致问题，并且注释者的偏见也会影响结果。同时，研究还发现反馈协议的选择也对评估结果有显著影响。

    

    大型语言模型（LLMs）与人类价值观和意图的对齐承诺涉及使用人工智能或人类反馈。稠密的反馈注释获取和整合成本较高，而稀疏的反馈则涉及结构性设计选择，即评分（例如，在1-7的范围内对回答A进行评分）和排名（例如，回答A是否比回答B更好？）。在这项工作中，我们分析了这种设计选择对LLMs的对齐和评估的影响。我们发现，评分和排名所推断出的偏好在人类和AI注释者中都存在严重的不一致问题，达到了60%。我们的后续分析确定了解释这个现象的各种注释者偏见方面，比如人类注释者更喜欢密集回答并在两个选项之间更青睐准确性。令我们惊讶的是，我们还观察到反馈协议的选择对对齐的LLMs的评估也有显著影响。特别是，我们发现LLMs的评估结果因为反馈协议的选择而有所不同。

    Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs
    
[^192]: WS-SfMLearner: 自助监督式在未知相机参数情况下进行手术视频的单目深度和自我运动估计。

    WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters. (arXiv:2308.11776v1 [cs.CV])

    [http://arxiv.org/abs/2308.11776](http://arxiv.org/abs/2308.11776)

    本文提出了一种自助监督深度和自我运动估计系统，可以在手术视频中预测精确的深度地图、相机位姿和相机内参数。

    

    手术视频中的深度估计在许多图像引导手术程序中起着关键作用。然而，由于手术场景中的亮度和噪声不一致，创建深度地图真实数据集是困难且耗时的。因此，构建一个准确且鲁棒的自助监督深度和相机自我运动估计系统引起了计算机视觉社区的更多关注。尽管一些自助监督方法减轻了对真实深度地图和位姿的需求，但它们仍需要已知的相机内参数，而这些参数通常缺失或未记录。此外，现有工作中相机内参数预测方法在很大程度上依赖于数据集的质量。在这项工作中，我们的目标是构建一个自助监督深度和自我运动估计系统，可以预测精确的深度地图、相机位姿和相机内参数。我们提出了一个基于成本体积的监督方式来提供系统的自我匹配。

    Depth estimation in surgical video plays a crucial role in many image-guided surgery procedures. However, it is difficult and time consuming to create depth map ground truth datasets in surgical videos due in part to inconsistent brightness and noise in the surgical scene. Therefore, building an accurate and robust self-supervised depth and camera ego-motion estimation system is gaining more attention from the computer vision community. Although several self-supervision methods alleviate the need for ground truth depth maps and poses, they still need known camera intrinsic parameters, which are often missing or not recorded. Moreover, the camera intrinsic prediction methods in existing works depend heavily on the quality of datasets. In this work, we aimed to build a self-supervised depth and ego-motion estimation system which can predict not only accurate depth maps and camera pose, but also camera intrinsic parameters. We proposed a cost-volume-based supervision manner to give the sy
    
[^193]: 使用层次结构距离捕捉多层次图结构的变压器

    Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances. (arXiv:2308.11129v1 [cs.LG])

    [http://arxiv.org/abs/2308.11129](http://arxiv.org/abs/2308.11129)

    本论文提出了一种层次距离结构编码（HDSE）方法，用于捕捉多层次图结构。经过在12个真实世界数据集上的实验，证明了该方法在10个基准数据集上实验效果达到了最先进水平。

    

    图变压器需要强大的归纳偏差来得出有意义的注意力分数。然而，当前的提议很少涉及捕捉更长距离、层次结构或社区结构的方法，而这些在分子、社交网络和引用网络等各种图形中都会出现。在本文中，我们提出了一种层次距离结构编码（HDSE）方法，用于建模图中节点之间的层次距离，重点关注其多层次、层次化的性质。特别是，这产生了一个可以灵活与现有图变压器集成的框架，可以与其他位置表示同时应用。通过在12个真实世界数据集上进行大量实验，我们证明了我们的HDSE方法成功提升了各种类型的基线变压器，在10个基准数据集上获得了最先进的实证性能。

    Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
    
[^194]: 想法图：用大型语言模型解决复杂问题

    Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.09687](http://arxiv.org/abs/2308.09687)

    想法图（GoT）是一种新的框架，它超越了现有的提示范式，通过将大型语言模型（LLM）的信息建模为任意图形，将LLM想法组合成具有协同效应的结果，提炼整个思维网络的本质，或者使用反馈环路增强思维。GoT在不同任务上展示出优势，并可以通过新的想法转换进行扩展，使LLM的推理更接近人类思维。

    

    我们介绍了一种名为想法图（Graph of Thoughts，GoT）的框架，它在大型语言模型（LLM）的提示能力上超越了Chain-of-Thought或Tree of Thoughts（ToT）等范式。GoT的关键思想和主要优势在于能够将LLM生成的信息建模为任意图形，其中信息单元（"LLM想法"）是顶点，边表示这些顶点之间的依赖关系。这种方法使得将任意LLM想法组合成具有协同效应的结果、提炼整个思维网络的本质或者使用反馈环路增强思维成为可能。我们证明GoT在不同任务上比最先进的方法有优势，例如在排序任务上质量提高了62%，同时成本降低了超过31%。我们确保GoT能够通过新的想法转换进行扩展，从而可以用于开创新的提示方案。这项工作使得LLM的推理更接近人类思维。

    We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinki
    
[^195]: 自然语言是图表所需要的全部内容

    Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.07134](http://arxiv.org/abs/2308.07134)

    本论文提出了一种名为InstructGLM的结构化语言模型算法，该算法将大型语言模型与图表学习问题相结合，旨在探索是否可以用语言模型取代图神经网络作为图表的基础模型。

    

    大规模预训练语言模型的出现，如ChatGPT，已经在人工智能的各个研究领域中引起了革命。基于Transformer的大型语言模型（LLMs）逐渐取代了CNN和RNN，将计算机视觉和自然语言处理领域统一起来。与相对独立存在的数据（如图像、视频或文本）相比，图表是一种包含丰富结构和关系信息的数据类型。同时，作为最具表现力的媒介之一，自然语言在描述复杂结构方面表现出色。然而，将图表学习问题纳入生成式语言建模框架的现有工作仍然非常有限。随着大型语言模型的重要性不断增长，探索LLMs是否也可以替代GNNs成为图表的基础模型变得至关重要。在本文中，我们提出了InstructGLM（结构化语言模型）算法，系统地设计高度可扩展的模型来处理图表学习问题。

    The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
    
[^196]: Flows: 推理和协作人工智能的构建模块

    Flows: Building Blocks of Reasoning and Collaborating AI. (arXiv:2308.01285v1 [cs.AI])

    [http://arxiv.org/abs/2308.01285](http://arxiv.org/abs/2308.01285)

    Flows是一种系统化的方法，它通过将计算分解为自包含的构建模块，通过标准化的消息传递接口进行通信，实现了结构化的推理和协作人工智能。这种模块化设计使得Flows可以构建任意复杂度的交互，能够覆盖各种人工智能交互和工具增强的应用。

    

    最近人工智能领域取得的进展已经产生了高度能力和可控性的系统。这为结构化推理以及多个人工智能系统和人类之间的协作创造了前所未有的机遇。为了充分实现这一潜力，必须开发一种有原则的方法来设计和研究这样的结构化交互。为此，我们引入了流程的概念框架：一种系统化的建模复杂交互的方法。流程是计算的自包含构建模块，具有独立的状态，并通过标准化的基于消息的接口进行通信。这种模块化设计使得流程可以递归地组合成任意嵌套的交互，大大降低了复杂性。关键是，任何交互都可以使用这个框架来实现，包括之前关于人工智能-人工智能和人类-人工智能交互、提示工程方案和工具增强的工作。我们展示了流程在任务上的潜力。

    Recent advances in artificial intelligence (AI) have produced highly capable and controllable systems. This creates unprecedented opportunities for structured reasoning as well as collaboration among multiple AI systems and humans. To fully realize this potential, it is essential to develop a principled way of designing and studying such structured interactions. For this purpose, we introduce the conceptual framework of Flows: a systematic approach to modeling complex interactions. Flows are self-contained building blocks of computation, with an isolated state, communicating through a standardized message-based interface. This modular design allows Flows to be recursively composed into arbitrarily nested interactions, with a substantial reduction of complexity. Crucially, any interaction can be implemented using this framework, including prior work on AI--AI and human--AI interactions, prompt engineering schemes, and tool augmentation. We demonstrate the potential of Flows on the task 
    
[^197]: 一种通过多尺度时空骨架匹配的一次性动作识别方法

    One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching. (arXiv:2307.07286v1 [cs.CV])

    [http://arxiv.org/abs/2307.07286](http://arxiv.org/abs/2307.07286)

    通过多尺度时空骨架匹配，本研究提出了一种新颖的一次性动作识别技术，能够处理骨架动作识别中的空间结构和时间顺序，实现了最优特征匹配。

    

    一次性骨架动作识别旨在通过单个训练样本学习骨架动作识别模型，由于收集和标注大规模骨架动作数据的挑战，它引起了越来越多的关注。然而，大多数现有研究通过直接比较特征向量来匹配骨架序列，忽略了骨架数据的空间结构和时间顺序。本文提出了一种新颖的一次性骨架动作识别技术，通过多尺度时空特征匹配来处理骨架动作识别。我们在多个空间和时间尺度上表示骨架数据，并从两个角度实现最优特征匹配。第一种是多尺度匹配，同时捕捉多个空间和时间尺度上骨架数据的语义相关性。第二种是跨尺度匹配，通过捕捉样本间的相关性来处理不同的运动幅度和速度。

    One-shot skeleton action recognition, which aims to learn a skeleton action recognition model with a single training sample, has attracted increasing interest due to the challenge of collecting and annotating large-scale skeleton action data. However, most existing studies match skeleton sequences by comparing their feature vectors directly which neglects spatial structures and temporal orders of skeleton data. This paper presents a novel one-shot skeleton action recognition technique that handles skeleton action recognition via multi-scale spatial-temporal feature matching. We represent skeleton data at multiple spatial and temporal scales and achieve optimal feature matching from two perspectives. The first is multi-scale matching which captures the scale-wise semantic relevance of skeleton data at multiple spatial and temporal scales simultaneously. The second is cross-scale matching which handles different motion magnitudes and speeds by capturing sample-wise relevance across multi
    
[^198]: 高维和置换不变异常检测。

    High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])

    [http://arxiv.org/abs/2306.03933](http://arxiv.org/abs/2306.03933)

    该研究引入了一种置换不变的高维密度估计方法，通过学习后将其用于高能物理数据中的异常检测，能够有效地识别出在仅具备背景假设下排除异常的喷注。

    

    由于学习高维概率密度的困难，新物理过程的异常检测方法通常局限于低维空间。特别是在成分级别上，将置换不变性和可变长度的输入等良好性质合并到流行的密度估计方法中变得更加困难。在本研究中，我们引入了一种基于扩散模型的粒子物理数据置换不变密度估计器，专门设计用于处理可变长度的输入。我们通过将学习到的密度用作置换不变的异常检测评分来展示我们方法的功效，有效地识别出在仅具备背景假设下的可能性较低的喷注。为了验证我们的密度估计方法，我们研究了学习到的密度比与被监督分类算法获得的密度之间的比较。

    Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
    
[^199]: 非线性循环神经网络的逆近似理论

    Inverse Approximation Theory for Nonlinear Recurrent Neural Networks. (arXiv:2305.19190v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19190](http://arxiv.org/abs/2305.19190)

    该论文证明了使用RNNs逼近非线性序列关系的逆近似定理，进一步将先前在线性RNNs中识别出的记忆难题推广到了一般的非线性情况，并提出了一个有原则的重新参数化方法来克服这些限制。

    

    我们证明了使用RNNs来逼近非线性序列关系的逆近似定理。这是近似理论中的一种称为Bernstein型结果的结果，它在假设目标函数可以通过假设空间有效逼近的条件下推导出目标函数的属性。特别地，我们展示了非线性序列关系可以被具有hardtanh/tanh激活函数的RNNs稳定逼近的时候，必须具有一个指数衰减的记忆结构--这个概念可以被明确定义。这将先前在线性RNNs中识别出的记忆难题推广到了一般的非线性情况，并量化了RNN架构在学习具有长期记忆的序列关系时的重要限制。基于分析，我们提出了一个有原则的重新参数化方法来克服这些限制。我们的理论结果通过数值实验进行了确认。

    We prove an inverse approximation theorem for the approximation of nonlinear sequence-to-sequence relationships using RNNs. This is a so-called Bernstein-type result in approximation theory, which deduces properties of a target function under the assumption that it can be effectively approximated by a hypothesis space. In particular, we show that nonlinear sequence relationships, viewed as functional sequences, that can be stably approximated by RNNs with hardtanh/tanh activations must have an exponential decaying memory structure -- a notion that can be made precise. This extends the previously identified curse of memory in linear RNNs into the general nonlinear setting, and quantifies the essential limitations of the RNN architecture for learning sequential relationships with long-term memory. Based on the analysis, we propose a principled reparameterization method to overcome the limitations. Our theoretical results are confirmed by numerical experiments.
    
[^200]: 关于扩散建模在异常检测中的应用

    On Diffusion Modeling for Anomaly Detection. (arXiv:2305.18593v1 [cs.LG])

    [http://arxiv.org/abs/2305.18593](http://arxiv.org/abs/2305.18593)

    本文研究了扩散建模在无监督和半监督异常检测中的应用，发现去噪扩散概率模型表现很好但计算成本高，因此提出了一种替代方法——扩散时间概率模型，该模型能够通过较大的时间步长上的高后验密度识别异常，并通过深度神经网络提高效率。

    

    扩散模型以其在生成建模中的优异性能而闻名，成为基于密度的异常检测的有吸引力的候选算法。本文研究了各种扩散建模方法在无监督和半监督异常检测中的应用。尤其是发现去噪扩散概率模型（DDPM）在异常检测方面具备很好的表现，但计算成本较高。通过简化DDPM在异常检测中的应用，我们自然地引出另一种称为扩散时间概率模型（DTPM）的替代方法。DTPM估计给定输入的扩散时间的后验分布，能够通过较大的时间步长上的高后验密度识别异常。我们导出了此后验分布的解析形式，并利用深度神经网络提高推理效率。通过在ADBenh基准测试中的实证评估，我们证明了所有基于扩散的异常检测方法的实用性。

    Known for their impressive performance in generative modeling, diffusion models are attractive candidates for density-based anomaly detection. This paper investigates different variations of diffusion modeling for unsupervised and semi-supervised anomaly detection. In particular, we find that Denoising Diffusion Probability Models (DDPM) are performant on anomaly detection benchmarks yet computationally expensive. By simplifying DDPM in application to anomaly detection, we are naturally led to an alternative approach called Diffusion Time Probabilistic Model (DTPM). DTPM estimates the posterior distribution over diffusion time for a given input, enabling the identification of anomalies due to their higher posterior density at larger timesteps. We derive an analytical form for this posterior density and leverage a deep neural network to improve inference efficiency. Through empirical evaluations on the ADBench benchmark, we demonstrate that all diffusion-based anomaly detection methods 
    
[^201]: 基于内核KL散度的自监督学习方法Kernel-SSL

    Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])

    [http://arxiv.org/abs/2305.17326](http://arxiv.org/abs/2305.17326)

    本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。

    

    对比学习通常将一个正锚点样本与许多负样本进行比较，来完成自监督学习（SSL）。相反，非对比学习，例如BYOL、SimSiam和Barlow Twins等方法，在没有显式使用负样本的情况下完成SSL。受对比学习现有分析的启发，我们提供了多种现有非对比学习方法的再生核希尔伯特空间（RKHS）理解。随后，我们提出了一种新的损失函数Kernel-SSL，直接优化RKHS中的均值嵌入和协方差算子。实验中，我们的方法Kernel-SSL在线性评估设置下在ImageNet数据集上大幅优于最先进的方法。具体来说，在进行100个epoch的预训练时，我们的方法比SimCLR表现提高了4.6%。

    Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
    
[^202]: 基于谱角度剖析生物数据中图神经网络的尺寸可泛化性：观点和实践

    Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective. (arXiv:2305.15611v1 [cs.LG])

    [http://arxiv.org/abs/2305.15611](http://arxiv.org/abs/2305.15611)

    本文通过谱角度的方法，研究了GNNs的尺寸可泛化性问题，并在真实生物数据集上进行了实验，发现GNNs在度分布和谱分布偏移时均表现敏感，在同一数据集的大图上的性能仍然下降，揭示了 GNNs的尺寸可泛化性问题。

    

    本文探讨了图神经网络 (GNNs) 是否具有从小图中学习的知识可推广到同一领域的大图中。之前的研究表明，不同大小的图之间的分布偏移，尤其是度分布，可能会导致图分类任务的性能下降。然而，在生物数据集中，度数是有界的，因此度分布的偏移很小。即使度分布偏移很小，我们观察到GNNs在同一数据集的大图上的性能仍然下降，暗示有其他原因。事实上，以往对于真实数据集中各种图尺寸引起的分布偏移类型和属性的探索不足。此外，以前的尺寸可泛化性分析大多集中在空间领域。为填补这些空白，我们采用谱角度去研究GNNs在生物图数据上的尺寸可泛化性。我们首先提出一个新框架来模拟各种类型的度分布偏移，并利用它来测试GNNs 在真实生物数据集上的尺寸可泛化性。我们的实验表明，除了度分布偏移外，GNNs 还对图大小变化引起的谱分布偏移很敏感。我们进一步分析了不同的GNN模型的影响，并表明，一些模型比其他模型更具有尺寸泛化性。本文展示了关于GNNs尺寸可泛化性问题的新观点和实践，并为该领域的未来研究提供了有益的洞察和建议。

    We investigate the question of whether the knowledge learned by graph neural networks (GNNs) from small graphs is generalizable to large graphs in the same domain. Prior works suggest that the distribution shift, particularly in the degree distribution, between graphs of different sizes can lead to performance degradation in the graph classification task. However, this may not be the case for biological datasets where the degrees are bounded and the distribution shift of degrees is small. Even with little degree distribution shift, our observations show that GNNs' performance on larger graphs from the same datasets still degrades, suggesting other causes. In fact, there has been a lack of exploration in real datasets to understand the types and properties of distribution shifts caused by various graph sizes. Furthermore, previous analyses of size generalizability mostly focus on the spatial domain.  To fill these gaps, we take the spectral perspective and study the size generalizabilit
    
[^203]: 大型语言模型是零样本文本到视频生成的帧级导演

    Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation. (arXiv:2305.14330v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.14330](http://arxiv.org/abs/2305.14330)

    本文引入了一个新的框架——DirecT2V，利用大型语言模型作为导演，从一个抽象的用户提示中生成零样本文本到视频生成的连贯且连贯的视频。该框架使用LLM导演将用户输入分为每一帧的提示，通过值映射和双softmax过滤器来保持时间一致和防止对象折叠。

    

    在人工智能生成内容（AIGC）的范式中，越来越多的关注点放在将预训练的文本到图像（T2I）模型扩展到文本到视频（T2V）生成上。尽管这些框架很有效，但它们面临着维护一致的叙述和处理从单个用户提示中的快速场景组合或对象位置变化的挑战。本文引入了一个新的框架，称为DirecT2V，它利用针对指令校准的大型语言模型（LLMs）从单个抽象用户提示生成逐帧描述。DirecT2V利用LLM导演将用户输入分为每个帧的单独提示，从而实现包含时间变化的内容和便于一致的视频生成。为了保持时间上的一致性和防止对象折叠，我们提出了一种新的值映射方法和双softmax过滤器。广泛的实验结果验证了DirecT2V框架在零样本T2V生成中产生的视觉连贯和一致的视频生成的有效性。

    In the paradigm of AI-generated content (AIGC), there has been increasing attention in extending pre-trained text-to-image (T2I) models to text-to-video (T2V) generation. Despite their effectiveness, these frameworks face challenges in maintaining consistent narratives and handling rapid shifts in scene composition or object placement from a single user prompt. This paper introduces a new framework, dubbed DirecT2V, which leverages instruction-tuned large language models (LLMs) to generate frame-by-frame descriptions from a single abstract user prompt. DirecT2V utilizes LLM directors to divide user inputs into separate prompts for each frame, enabling the inclusion of time-varying content and facilitating consistent video generation. To maintain temporal consistency and prevent object collapse, we propose a novel value mapping method and dual-softmax filtering. Extensive experimental results validate the effectiveness of the DirecT2V framework in producing visually coherent and consist
    
[^204]: 三思而后行：衡量消除问答模型预测快捷方式的效率

    Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models. (arXiv:2305.06841v1 [cs.CL])

    [http://arxiv.org/abs/2305.06841](http://arxiv.org/abs/2305.06841)

    研究提出了一种衡量模型依赖已知虚假特征的技术，并评估了预先训练的问答模型和去偏置方法对大量已知和新发现的预测偏差的鲁棒性。其发现去偏置方法不能通过减轻对偏差特征的依赖来解释OOD收益，表明偏差在QA数据集中共享。

    

    尽管大规模语言模型（Large Language Models，LLMs）主导了大部分语言理解任务，在训练数据集的建模假混淆的支持下, 因此有先前的工作表明，某些这些结果是由建模虚假相关性实现的。作者常常通过评估同一任务的分布外（OOD）数据集上的模型来评估模型的健壮性，但这些数据集可能具有与训练数据集相同的偏差。我们提出了一种衡量模型依赖于任何已知虚假特征的尺度的简单方法，并评估预先训练的模型和去偏置方法在问答（QA）中对大量已知和新发现的预测偏差的鲁棒性。我们发现去偏置方法的报告OOD收益不能通过减轻对有偏特征的依赖来解释，这表明偏差在QA数据集中共享。我们通过测量OOD模型的表现取决于偏差特征，与ID模型相当，进而证明了这一点，这推动了未来的研究。

    While the Large Language Models (LLMs) dominate a majority of language understanding tasks, previous work shows that some of these results are supported by modelling spurious correlations of training datasets. Authors commonly assess model robustness by evaluating their models on out-of-distribution (OOD) datasets of the same task, but these datasets might share the bias of the training dataset.  We propose a simple method for measuring a scale of models' reliance on any identified spurious feature and assess the robustness towards a large set of known and newly found prediction biases for various pre-trained models and debiasing methods in Question Answering (QA). We find that the reported OOD gains of debiasing methods can not be explained by mitigated reliance on biased features, suggesting that biases are shared among QA datasets. We further evidence this by measuring that performance of OOD models depends on bias features comparably to the ID model, motivating future work to refin
    
[^205]: 利用RMT将Transformer扩展到100万个标记及以上。

    Scaling Transformer to 1M tokens and beyond with RMT. (arXiv:2304.11062v1 [cs.CL])

    [http://arxiv.org/abs/2304.11062](http://arxiv.org/abs/2304.11062)

    本文介绍了一种利用循环记忆扩展BERT上下文长度的方法，成功扩展到了前所未有的200万个标记，有望增强自然语言处理中的长期依赖处理并为内存密集型应用程序实现大规模上下文处理。

    

    本技术报告介绍了一种利用循环记忆扩展BERT上下文长度的方法，BERT是自然语言处理中最有效的基于Transformer模型之一。通过利用循环记忆Transformer架构，我们成功地将模型的有效上下文长度增加到了前所未有的200万个标记，同时保持了高的内存检索准确性。我们的方法允许存储和处理本地和全局信息，并通过使用循环实现输入序列各部分之间的信息流动。我们的实验证明了我们的方法的有效性，具有显著的潜力来增强自然语言理解和生成任务中的长期依赖处理，并能够为内存密集型应用程序实现大规模上下文处理。

    This technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model's effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. Our experiments demonstrate the effectiveness of our approach, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.
    
[^206]: 通过互动反馈与代理交互来提高协作环境下基于实地理解（Grounded Language Understanding）的能力

    Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback. (arXiv:2304.10750v1 [cs.CL])

    [http://arxiv.org/abs/2304.10750](http://arxiv.org/abs/2304.10750)

    研究通过互动反馈与代理交互来提高协作环境下基于实地理解的能力。

    

    许多自然语言处理任务通常被视为单步问题。在这些任务中，代理接收一个指令，执行它，然后根据最终结果进行评估。然而，人类语言本质上是交互式的，我们主张人工智能与人类的协作也应是交互式的，人类监督人工智能代理的工作，并提供代理可以理解和利用的反馈信息。在这项工作中，我们探讨了通过Help Feedback实现这一目标的方向。

    Many approaches to Natural Language Processing (NLP) tasks often treat them as single-step problems, where an agent receives an instruction, executes it, and is evaluated based on the final outcome. However, human language is inherently interactive, as evidenced by the back-and-forth nature of human conversations. In light of this, we posit that human-AI collaboration should also be interactive, with humans monitoring the work of AI agents and providing feedback that the agent can understand and utilize. Further, the AI agent should be able to detect when it needs additional information and proactively ask for help. Enabling this scenario would lead to more natural, efficient, and engaging human-AI collaborations.  In this work, we explore these directions using the challenging task defined by the IGLU competition, an interactive grounded language understanding task in a MineCraft-like world. We explore multiple types of help players can give to the AI to guide it and analyze the impac
    
[^207]: LaCViT：一种面向标签的对比训练框架，提高视觉Transformer的表示空间的等性

    LaCViT: A Label-aware Contrastive Training Framework for Vision Transformers. (arXiv:2303.18013v1 [cs.CV])

    [http://arxiv.org/abs/2303.18013](http://arxiv.org/abs/2303.18013)

    LaCViT是一种针对视觉Transformer预训练表示空间的各向等性不足问题，提高其表示空间等性的面向标签的对比训练框架，经过实验证明其在五个标准图像分类数据集中具有卓越的性能。

    

    视觉 Transformer 已经在处理计算机视觉任务时表现出了惊人的效果，这是由于其模拟长时间的特征依赖能力。通过使用大规模的训练数据和各种自监督信号（例如，遮蔽随机块），视觉 Transformer 在 ImageNet-1k 和 CIFAR-10 等几个基准数据集上提供了最先进的性能。然而，这些基于通用大规模图像语料库预训练的视觉Transformer只能产生各向异性表示空间，限制了它们在目标下游任务中的通用性和可转移性。在本文中，我们提出了一种简单而有效的面向标签的对比训练框架 LaCViT，它提高了视觉Transformer预训练表示空间的等性，从而实现了更有效的转移学习。通过对五个标准图像分类数据集的实验，我们证明了LaCViT训练的模型在各种图像分类任务中都具有卓越的性能。

    Vision Transformers have been incredibly effective when tackling computer vision tasks due to their ability to model long feature dependencies. By using large-scale training data and various self-supervised signals (e.g., masked random patches), vision transformers provide state-of-the-art performance on several benchmarking datasets, such as ImageNet-1k and CIFAR-10. However, these vision transformers pretrained over general large-scale image corpora could only produce an anisotropic representation space, limiting their generalizability and transferability to the target downstream tasks. In this paper, we propose a simple and effective Label-aware Contrastive Training framework LaCViT, which improves the isotropy of the pretrained representation space for vision transformers, thereby enabling more effective transfer learning amongst a wide range of image classification tasks. Through experimentation over five standard image classification datasets, we demonstrate that LaCViT-trained m
    
[^208]: 抽象论证中的时间性和因果关系。

    Temporality and Causality in Abstract Argumentation. (arXiv:2303.09197v1 [cs.AI])

    [http://arxiv.org/abs/2303.09197](http://arxiv.org/abs/2303.09197)

    该论文研究了抽象论证中的时间性和因果关系，提出了一种形式化的方法，将无环抽象论证框架的概念重新编写成一个行动语言，并建立起论据陈述和它们直接或间接后果之间的因果关系。

    

    在抽象论证的背景下，我们提出了考虑时间性和因果关系的好处，即论据被陈述的顺序以及它们之间的因果关系。我们提出了一种形式化的方法，将无环抽象论证框架的概念重新编写成一个行动语言，使我们能够模拟世界的演化，并建立论据陈述和它们的直接或间接后果之间的因果关系。我们还提出了一个Answer Set编程实现，并探讨了解释的发展方向。

    In the context of abstract argumentation, we present the benefits of considering temporality, i.e. the order in which arguments are enunciated, as well as causality. We propose a formal method to rewrite the concepts of acyclic abstract argumentation frameworks into an action language, that allows us to model the evolution of the world, and to establish causal relationships between the enunciation of arguments and their consequences, whether direct or indirect. An Answer Set Programming implementation is also proposed, as well as perspectives towards explanations.
    
[^209]: 持续学习综述：理论、方法与应用

    A Comprehensive Survey of Continual Learning: Theory, Method and Application. (arXiv:2302.00487v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00487](http://arxiv.org/abs/2302.00487)

    本文为持续学习的全面综述，总结了持续学习的一般目标，回顾了各种持续学习方法，并强调了一些有前途的应用领域和未来研究方向。

    

    智能系统需要在其生命周期内不断获取、更新、积累和利用知识以应对现实世界的动态变化。这种能力称为持续学习，为AI系统自适应发展提供了基础。然而，持续学习的一个显著限制是灾难性遗忘，即学习一个新任务通常会导致旧任务的性能显著降低。近年来，不断涌现的各种进展大大扩展了持续学习的理解和应用。本文提供了一个全面的持续学习综述，旨在连接基本设置、理论基础、代表性方法和实际应用。我们总结了现有理论和实证结果，概括了持续学习的一般目标：减少灾难性遗忘，实现高效的和终身的学习，以及实现更深层次的表征学习。我们还回顾了各种持续学习方法，包括基于正则化、基于回放和基于生成的方法，并讨论了它们的优缺点。最后，我们强调了一些有前途的应用领域，如机器人、自然语言处理和计算机视觉，并确定了一些开放挑战和未来研究方向。

    To cope with real-world dynamics, an intelligent agent needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of 
    
[^210]: RaLiBEV: 雷达和激光雷达的引导框自由物体检测系统的融合学习

    RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System. (arXiv:2211.06108v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.06108](http://arxiv.org/abs/2211.06108)

    本论文提出了一种基于鸟瞰视角的引导框自由物体检测系统，通过雷达和激光雷达的特征融合学习，解决了在恶劣天气下物体检测的问题。

    

    在自动驾驶系统中，激光雷达和雷达在感知周围环境中起着重要作用。激光雷达提供精确的三维空间感知信息，但在雾等恶劣天气下无法工作。另一方面，雷达信号由于波长的特性在遇到雨滴或雾粒时会发生衍射，但它受到大噪声的干扰。最近的最新研究表明，雷达和激光雷达的融合可以在恶劣天气下实现强健的检测。现有的方法采用卷积神经网络架构从每个传感器数据流中提取特征，然后对齐和汇聚两个分支的特征以预测物体检测结果。然而，由于标签分配和融合策略的简单设计，这些方法对边界框估计的准确性较低。在本文中，我们提出了一种基于鸟瞰视角融合学习的引导框自由物体检测系统，该系统将来自雷达的距离-方位特征融合起来

    In autonomous driving systems, LiDAR and radar play important roles in the perception of the surrounding environment. LiDAR provides accurate 3D spatial sensing information but cannot work in adverse weather like fog. On the other hand, the radar signal can be diffracted when encountering raindrops or mist particles thanks to its wavelength, but it suffers from large noise. Recent state-of-the-art works reveal that fusion of radar and LiDAR can lead to robust detection in adverse weather. The existing works adopt convolutional neural network architecture to extract features from each sensor data stream, then align and aggregate the two branch features to predict object detection results. However, these methods have low accuracy of bounding box estimations due to a simple design of label assignment and fusion strategies. In this paper, we propose a bird's-eye view fusion learning-based anchor box-free object detection system, which fuses the feature derived from the radar range-azimuth 
    
[^211]: pyRDDLGym：从RDDL到Gym环境

    pyRDDLGym: From RDDL to Gym Environments. (arXiv:2211.05939v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.05939](http://arxiv.org/abs/2211.05939)

    pyRDDLGym是一个Python框架，用于从RDDL声明性描述自动生成OpenAI Gym环境。它通过条件概率函数描述RDDL中变量的离散时间步进演化，并支持简单的环境修改和扩展。它具有独特的表达能力，可以帮助快速开发强化学习基准，并促进利用模型知识进行交互性学习的混合方法研究。

    

    我们提出了pyRDDLGym，一个用于从RDDL声明性描述自动生成OpenAI Gym环境的Python框架。RDDL中的变量的离散时间步进演化由条件概率函数描述，这与Gym步骤方案自然契合。此外，由于RDDL是一个抽象描述，将环境进行修改和扩展以支持多个实体和不同配置变得简单而不是繁琐的过程。我们希望pyRDDLGym能够由于RDDL的独特表达能力，成为强化学习社区中一股新的力量，从而实现易于快速开发基准，通过提供对RDDL描述中的模型的明确访问，pyRDDLGym还可以促进利用模型知识进行交互性学习的混合方法研究。我们介绍了pyRDDLGym的设计和内置示例，以及融入RDDL语言的附加内容。

    We present pyRDDLGym, a Python framework for auto-generation of OpenAI Gym environments from RDDL declerative description. The discrete time step evolution of variables in RDDL is described by conditional probability functions, which fits naturally into the Gym step scheme. Furthermore, since RDDL is a lifted description, the modification and scaling up of environments to support multiple entities and different configurations becomes trivial rather than a tedious process prone to errors. We hope that pyRDDLGym will serve as a new wind in the reinforcement learning community by enabling easy and rapid development of benchmarks due to the unique expressive power of RDDL. By providing explicit access to the model in the RDDL description, pyRDDLGym can also facilitate research on hybrid approaches for learning from interaction while leveraging model knowledge. We present the design and built-in examples of pyRDDLGym, and the additions made to the RDDL language that were incorporated into t
    

