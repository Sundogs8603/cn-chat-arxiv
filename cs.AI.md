# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Practical Lessons on Optimizing Sponsored Products in eCommerce.](http://arxiv.org/abs/2304.09107) | 本文提出了一个实用的机器学习框架，可以解决电商广告系统中赞助产品优化的多个问题，而不需要改变现有机器学习模型结构。使用该框架可以处理广告系统中的长期问题，并为多个评估指标带来增量。 |
| [^2] | [Solving Math Word Problems by Combining Language Models With Symbolic Solvers.](http://arxiv.org/abs/2304.09102) | 该论文提出了一种将语言模型与符号求解器相结合的方法，以解决数学单词问题，尤其是具有挑战性的问题，并在此过程中突出了使用声明性和逐步表示的好处。 |
| [^3] | [MATURE-HEALTH: HEALTH Recommender System for MAndatory FeaTURE choices.](http://arxiv.org/abs/2304.09099) | 该论文提出和实施了一个名为MATURE-HEALTH的健康推荐系统，该系统能够预测电解质不平衡并推荐营养平衡的食物，从而增加早期检测疾病的机会并防止健康进一步恶化。 |
| [^4] | [MDDL: A Framework for Reinforcement Learning-based Position Allocation in Multi-Channel Feed.](http://arxiv.org/abs/2304.09087) | 本研究提出了一种名为MDDL的多通道深度确定性策略梯度学习框架，旨在整合多种策略，以增强位置分配的强化学习模型训练。该框架在在线和离线性能方面表现优于一些最先进的方法。 |
| [^5] | [LLM-based Interaction for Content Generation: A Case Study on the Perception of Employees in an IT department.](http://arxiv.org/abs/2304.09064) | 本文通过IT部门员工的问卷调查研究了他们即将使用的基于LLM技术的内容生成工具的意愿，结果显示实用性高的工具更容易被接受和使用。 |
| [^6] | [Revisiting k-NN for Pre-trained Language Models.](http://arxiv.org/abs/2304.09058) | 本研究提出一种新方法，结合k-NN和预训练语言模型（PLMs）能够提高自然语言处理（NLP）的性能，并在多个基准数据集上得到验证。 |
| [^7] | [CodeKGC: Code Language Model for Generative Knowledge Graph Construction.](http://arxiv.org/abs/2304.09048) | 本文提出了一种使用代码语言模型处理生成式知识图谱构建任务的方法，能够有效利用知识图谱内的语义结构，提高模型的可解释性。 |
| [^8] | [PaTeCon: A Pattern-Based Temporal Constraint Mining Method for Conflict Detection on Knowledge Graphs.](http://arxiv.org/abs/2304.09015) | PaTeCon是一种基于模式的知识图谱时间约束挖掘方法，能够自动生成时间约束来维护KG的时间一致性，并在不需要人工专家的情况下准确地检测潜在的时间冲突。 |
| [^9] | [A Biomedical Entity Extraction Pipeline for Oncology Health Records in Portuguese.](http://arxiv.org/abs/2304.08999) | 本研究开发了一种有效提取葡萄牙语肿瘤健康记录中过程、药物和疾病的方法，帮助医护人员更高效地获取患者治疗状况的完整概述，有助于提升肿瘤治疗效果。 |
| [^10] | [D2CSE: Difference-aware Deep continuous prompts for Contrastive Sentence Embeddings.](http://arxiv.org/abs/2304.08991) | D2CSE是一种用于学习句子嵌入的新模型，采用基于差异感知的深度连续提示来计算具有区分微妙差异能力的句子向量。与现有方法相比，D2CSE只使用一个预训练语言模型，避免了繁琐的微调，并大大减少了训练参数数量，同时显著提高了句子嵌入的质量。 |
| [^11] | [Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs.](http://arxiv.org/abs/2304.08968) | LLMs在公众中广泛应用，但是目前大部分检测工具存在严重缺陷。研究发现，LLMs容易微调且难以被其他LLMs检测到。 |
| [^12] | [Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning.](http://arxiv.org/abs/2304.08944) | 本文提出了一种主动奖励学习的强化学习算法，采用人机交互来指定任务的奖励，大大减少了所需的人类反馈，并在效率上提供了理论保证。 |
| [^13] | [A Study of Neural Collapse Phenomenon: Grassmannian Frame, Symmetry, Generalization.](http://arxiv.org/abs/2304.08914) | 本文提出了广义神经崩溃假设，发现了Grassmannian Frame结构和对称泛化现象，这对特征选择和神经网络设计都具有重要作用。 |
| [^14] | [Computational and Exploratory Landscape Analysis of the GKLS Generator.](http://arxiv.org/abs/2304.08913) | 本文对GKLS发生器进行了计算和探索性景观分析，发现GKLS发生器生成的问题在更高维度优化变得极其困难。 |
| [^15] | [Safe reinforcement learning with self-improving hard constraints for multi-energy management systems.](http://arxiv.org/abs/2304.08897) | 本论文提出了一种安全强化学习方法，能够实现多能源管理系统中的最优控制，在保证硬约束的前提下减少工程工作，降低建模偏差，并避免潜在的不安全行为。 |
| [^16] | [Autonomous Systems: Autonomous Systems: Indoor Drone Navigation.](http://arxiv.org/abs/2304.08893) | 该论文介绍了一种使用ROS slam工具箱和Nav2导航系统框架构建的模拟无人机，能够在室内环境中自主移动的技术。 |
| [^17] | [NPS: A Framework for Accurate Program Sampling Using Graph Neural Network.](http://arxiv.org/abs/2304.08880) | NPS是一种使用图神经网络进行程序采样的框架，通过学习执行嵌入并快速生成代表性模拟点，实现了高效的微处理器设计。 |
| [^18] | [Impact Of Explainable AI On Cognitive Load: Insights From An Empirical Study.](http://arxiv.org/abs/2304.08861) | 一项实证研究表明，可解释人工智能对最终用户的认知负荷、任务表现和任务时间有重要影响，推荐使用实现无关的地方 XAI 说明类型。 |
| [^19] | [Unveiling and unraveling aggregation and dispersion fallacies in group MCDM.](http://arxiv.org/abs/2304.08859) | 本研究发现在群体MCDM中普遍存在的错误有三种，包括用错误的平均方法、误用标准偏差和距离函数等。通过使用组成数据分析，我们提出了聚合和分析优先级的新方法，有助于避免这些错误。 |
| [^20] | [A Domain-Region Based Evaluation of ML Performance Robustness to Covariate Shift.](http://arxiv.org/abs/2304.08855) | 本文实验评估了传统机器学习模型在协变量转移存在的情况下的性能。根据实验分析，随机森林是一种对协变量移位较不敏感的模型。 |
| [^21] | [UDTIRI: An Open-Source Road Pothole Detection Benchmark Suite.](http://arxiv.org/abs/2304.08842) | 该论文介绍了一个开源的道路坑洞检测基准套件UDTIRI，包含了标记齐全的1000张道路坑洞图像，可以用于深度学习方法在城市道路检查中的目标检测、语义分割和实例分割任务。 |
| [^22] | [On the Interdependence of Reliance Behavior and Accuracy in AI-Assisted Decision-Making.](http://arxiv.org/abs/2304.08804) | 该论文分析了AI辅助决策中依赖行为和准确性之间的相互关系，并提出了一个视觉框架来更好地理解这种关系。该框架揭示了当人类在决策中过度依赖AI时，改善信任可能会降低准确性的有趣属性。 |
| [^23] | [Speaker Profiling in Multiparty Conversations.](http://arxiv.org/abs/2304.08801) | 本文提出了一个名为SPC的任务，旨在为对话中每个发言者生成个人特征摘要。任务被分为三个子任务：个人特征发现、个人特征类型识别和个人特征价值提取。任务对于银行、酒店预订和航空预订等行业中的聊天机器人非常重要，可以使聊天机器人更好地了解和回应每个发言人的需求。 |
| [^24] | [Self-Supervised 3D Action Representation Learning with Skeleton Cloud Colorization.](http://arxiv.org/abs/2304.08799) | 本文提出了一种基于自监督学习和骨架云着色技术的无监督三维动作表示学习方法，可以在未标注数据上进行空间和时间表示学习，实验结果表明，在三个基准数据集上实现了最先进的无监督骨架动作识别性能。 |
| [^25] | [A Systematic Literature Review of User Trust in AI-Enabled Systems: An HCI Perspective.](http://arxiv.org/abs/2304.08795) | 本文系统地综述了23项实证研究关于用户信任定义、影响因素和测量方法的结果。结果表明，选择最适合特定情境中描述用户信任定义应该是重点，而不是比较定义。用户对AI能力系统的信任受到三个主要主题的影响，即社会伦理考虑、技术和设计特征、以及用户特征。 |
| [^26] | [3-Objective Pareto Optimization for Problems with Chance Constraints.](http://arxiv.org/abs/2304.08774) | 本文研究了具有机会约束的问题中的三目标帕累托优化，该模型可以通过使用1位翻转来计算所有需要的权衡，并在机会约束的支配集问题中展示了其效益。 |
| [^27] | [Masked Language Model Based Textual Adversarial Example Detection.](http://arxiv.org/abs/2304.08767) | 通过探索掩码语言模型引起的流形变化，我们提出了一种插即用的文本对抗例子检测方法，可以在保持对分类任务、模型结构和数据集无依赖的前提下，有效地检测到对抗例子。 |
| [^28] | [Exoskeleton for the Mind: Exploring Strategies Against Misinformation with a Metacognitive Agent.](http://arxiv.org/abs/2304.08759) | 本研究基于元认知理论提出了一种反信息误导代理，在Twitter环境中进行了试点和实验研究，发现单一策略无法完全解决问题，并提出了对透明度和用户参与的重视。 |
| [^29] | [W-MAE: Pre-trained weather model with masked autoencoder for multi-variable weather forecasting.](http://arxiv.org/abs/2304.08754) | 本文介绍了一种名为 W-MAE 的预训练天气模型，它使用遮蔽自编码器重建气象变量之间的空间相关性，并通过微调预测气象变量的未来状态，从而对天气数据中存在的时空依赖关系进行建模。 |
| [^30] | [Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets.](http://arxiv.org/abs/2304.08742) | 本论文提出了一种简单的方法，利用少量下游专家数据从离线未标记数据集中选择性地查询相关行为（包括许多次优行为），实现了行为检索，进而实现了少样本学习。 |
| [^31] | [Addressing Variable Dependency in GNN-based SAT Solving.](http://arxiv.org/abs/2304.08738) | 该论文提出了一种名为AsymSAT的GNN-based SAT求解器，通过解决变量依赖问题，提高了该方法的求解能力，在大型测试集上解决的SAT实例数有所提高。 |
| [^32] | [Do humans and machines have the same eyes? Human-machine perceptual differences on image classification.](http://arxiv.org/abs/2304.08733) | 本文研究通过图像分类探究了人机感知差异，发现即使准确率相似，人类和机器的答案分布也可能不同，并提出了一种后期人机合作来提高任务表现。 |
| [^33] | [LTC-SE: Expanding the Potential of Liquid Time-Constant Neural Networks for Scalable AI and Embedded Systems.](http://arxiv.org/abs/2304.08691) | LTC-SE是一种液态时常神经网络算法，将多种神经元模型统一，其增强版专注于灵活性、兼容性和代码组织，满足嵌入式系统的性能要求，扩展了液态神经网络在可扩展人工智能和嵌入式系统中的适用性。 |
| [^34] | [An end-to-end, interactive Deep Learning based Annotation system for cursive and print English handwritten text.](http://arxiv.org/abs/2304.08670) | 本文提出了一个端到端、交互式的手写英文文本注释系统，解决了手写文本数据稀缺的问题，并能够有效提高手写文本识别模型的识别准确率。 |
| [^35] | [Insta(nt) Pet Therapy: GAN-generated Images for Therapeutic Social Media Content.](http://arxiv.org/abs/2304.08665) | 本研究使用GAN生成大量虚假宠物图像，上传到社交媒体账户达到了与传统宠物图像账户相同的用户参与度，为宠物疗法社交媒体内容的发展提供了新思路。 |
| [^36] | [Continuous Versatile Jumping Using Learned Action Residuals.](http://arxiv.org/abs/2304.08663) | 本文提出了一个层次化框架，将最优控制和强化学习结合起来，为四足机器人学习连续跳跃动作。通过学习动作剩余值，在模拟和真实环境中实现了多功能、连续的跳跃动作。 |
| [^37] | [(LC)$^2$: LiDAR-Camera Loop Constraints For Cross-Modal Place Recognition.](http://arxiv.org/abs/2304.08660) | 提出了一种新的(LC)$^2$方法，通过将2D图像和3D点云数据转换为2.5D深度图像进行匹配，实现了没有先验点云地图的LiDAR定位。 |
| [^38] | [Classification of US Supreme Court Cases using BERT-Based Techniques.](http://arxiv.org/abs/2304.08649) | 本文基于BERT技术探究了对美国最高法院案例进行分类的方法，比较了使用BERT模型与其他先进模型的准确性，最终在15个广泛类别上取得了80%的准确度，在279个细粒度类别上取得了60%的准确度。 |
| [^39] | [An Evaluation on Large Language Model Outputs: Discourse and Memorization.](http://arxiv.org/abs/2304.08637) | 评估了九个大语言模型的输出，发现其中80％包含记忆数据，但包含最多记忆内容的输出更可能是高质量的。提出了缓解策略以降低记忆文本率。 |
| [^40] | [Bridging Discrete and Backpropagation: Straight-Through and Beyond.](http://arxiv.org/abs/2304.08612) | 本文提出了一种新方法来逼近生成离散潜变量的参数的梯度，其中包括了一些数值方法，实现了二阶精度，取得了实验上的持续改进。 |
| [^41] | [Crossing Roads of Federated Learning and Smart Grids: Overview, Challenges, and Perspectives.](http://arxiv.org/abs/2304.08602) | 本文探讨了在智能电网中应用联邦学习以保护消费者隐私和提高数据模型预测性能的优势和缺点，并提供了可能的数据分区、通信拓扑和安全机制分类方法。同时，本文总结了该技术面临的主要挑战和未来发展方向。 |
| [^42] | [PALF: Pre-Annotation and Camera-LiDAR Late Fusion for the Easy Annotation of Point Clouds.](http://arxiv.org/abs/2304.08591) | 本研究提出了一种使用预注释和相机-LiDAR后期融合的算法，以便于准确进行点云数据的注释。 |
| [^43] | [Generative Disco: Text-to-Video Generation for Music Visualization.](http://arxiv.org/abs/2304.08551) | Generative Disco是一个生成式人工智能系统，利用大型语言模型和文本到图像模型帮助生成音乐可视化；用户通过定义开始和结束提示来参数化可视化，可生成反应音频的视频，引入了“过渡”和“保持”的设计模式，能够产生高度表现力并适用于专业人员。 |
| [^44] | [A Scalable Test Problem Generator for Sequential Transfer Optimization.](http://arxiv.org/abs/2304.08503) | STO中已有的测试问题设计不完善，难以代表真实问题多样化关系，限制了算法的表现。本文介绍了一种可扩展的序列转移优化问题生成器。 |
| [^45] | [Ranking Loss and Sequestering Learning for Reducing Image Search Bias in Histopathology.](http://arxiv.org/abs/2304.08498) | 本文提出两个新颖的想法，分别采用排名损失函数和交错学习的方法，避免了分类误差和模型内部偏见，以提高图像搜索性能。 |
| [^46] | [Optimizing Group Utility in Itinerary Planning: A Strategic and Crowd-Aware Approach.](http://arxiv.org/abs/2304.08495) | 本论文介绍了一种名为SCAIR的算法，可以优化群体效用，解决行程规划中的多个用户排队时间和人群水平优化的问题。 |
| [^47] | [Smart Home Environment Modelled with a Multi-Agent System.](http://arxiv.org/abs/2304.08494) | 本文描述了一个智能家居上下文感知环境的原型，使用多智能体系统模拟，目的是通过预定义规则降低设备运行的操作成本，并监控居民的健康。 |
| [^48] | [Coordinated Multi-Agent Reinforcement Learning for Unmanned Aerial Vehicle Swarms in Autonomous Mobile Access Applications.](http://arxiv.org/abs/2304.08493) | 本论文提出了一种集中式训练和分布式执行的多智能体深度强化学习方法，用于协调控制多个无人机在自主移动接入应用中，最大化服务质量。 |
| [^49] | [Chinese Open Instruction Generalist: A Preliminary Release.](http://arxiv.org/abs/2304.07987) | 本论文旨在通过适应不同子任务的固有特性，创建一个中文指令数据集，以填补指令调整技术在中文语言领域的空白。 |
| [^50] | [A Data-Centric Solution to NonHomogeneous Dehazing via Vision Transformer.](http://arxiv.org/abs/2304.07874) | 本文提出了一种基于Vision Transformer的数据中心的解决方案，用于解决非均质去雾问题。传统方法在处理NH-HAZE23数据集等非均质雾图像时存在问题，因为它们无法满足建模均质雾所需的假设之一。同时，本文指出光靠数据增广并不能解决问题，因为需要处理分布差异。 |
| [^51] | [Neural Machine Translation For Low Resource Languages.](http://arxiv.org/abs/2304.07869) | 该论文研究了低资源语言的神经机器翻译，并构建了一个基于 \texttt{mBART.CC25} 语言模型的模型，利用后向翻译和迁移学习等 NLP 和深度学习技术进行增强，以达到最先进的结果。 |
| [^52] | [Learning Empirical Bregman Divergence for Uncertain Distance Representation.](http://arxiv.org/abs/2304.07689) | 本文介绍了一种新的基于Deep Metric Learning的方法，通过学习经验Bregman散度直接从数据中进行不确定距离表示，能够有效的在模式识别和聚类任务上提高准确性。 |
| [^53] | [Tractable Control for Autoregressive Language Generation.](http://arxiv.org/abs/2304.07438) | 本文提出了一种在自回归文本生成中使用可操作概率模型来强制实施限制的控制方法GeLaTo，并取得了在常见的约束文本生成测试上的最先进性能。 |
| [^54] | [H2TNE: Temporal Heterogeneous Information Network Embedding in Hyperbolic Spaces.](http://arxiv.org/abs/2304.06970) | 该论文提出了 H2TNE 模型，用于将时态异构信息网络嵌入到双曲空间中。通过时间和异质性双重约束的随机游走策略，该模型能够捕捉结构与语义信息。 |
| [^55] | [Attributed Multi-order Graph Convolutional Network for Heterogeneous Graphs.](http://arxiv.org/abs/2304.06336) | 本文提出了一个AMOGCN模型，它自动从多阶邻接矩阵的自适应聚合中研究包含多跳邻居的元路径，并使用节点属性评价监督。其能够有效地从异构图中发现有区别的节点嵌入和关系。 |
| [^56] | [CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic inference and learning.](http://arxiv.org/abs/2304.05949) | 本文展示了如何将基于随机磁隧道结（sMTJ）的概率比特（p位）与多功能可编程门阵列（FPGA）相结合，设计出一种能源高效的异构CMOS + X（X = sMTJ）原型，其成功地执行了概率推理和异步Boltzmann学习。 |
| [^57] | [NeRF applied to satellite imagery for surface reconstruction.](http://arxiv.org/abs/2304.04133) | 本文提出了Sat-NeRF模型，能够从少量的卫星图像集合中合成新的视角，并准确地估计场景表面的高程。 |
| [^58] | [When do you need Chain-of-Thought Prompting for ChatGPT?.](http://arxiv.org/abs/2304.03262) | 该论文讨论了连续思考提示（CoT）对ChatGPT的有效性，发现在算术推理等任务中，这种提示不再有效，但在其他推理任务中仍有效。分析表明，在大型语言模型受训练推理时存在过拟合/偏差的风险，需要在更多任务和模型上评估和改进连续思考提示的鲁棒性。 |
| [^59] | [BotTriNet: A Unified and Efficient Embedding for Social Bots Detection via Metric Learning.](http://arxiv.org/abs/2304.03144) | BOTTRINET基于文本内容检测机器人，并设计了三元组网络以提高分类性能。在真实世界数据集CRESCI2017上，系统表现最好。 |
| [^60] | [Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders.](http://arxiv.org/abs/2304.01016) | 本文提出了一种通过结构压缩和模型尺寸不对称的双编码器模型 KALE，有效提高密集信息检索的推理效率，同时允许查询编码器的有效压缩，而无需进行全部的再训练或索引生成，此方法能够生成超过DistilBERT性能的模型。 |
| [^61] | [OTS: A One-shot Learning Approach for Text Spotting in Historical Manuscripts.](http://arxiv.org/abs/2304.00746) | 提出了一种基于单次学习的历史手稿文本检测方法 OTS， 尤其对于低资源检测任务，使用新型的“环形损失”损失函数提高了检测能力，同时创建了包含古代东巴象形文字的手稿数据集。 |
| [^62] | [Dual-stream Time-Delay Neural Network with Dynamic Global Filter for Speaker Verification.](http://arxiv.org/abs/2303.11020) | 该论文提出了具有动态全局滤波器的双流时延神经网络，在说话人验证领域的表现优于其他最先进的方法，并实现了最先进的性能。 |
| [^63] | [BotShape: A Novel Social Bots Detection Approach via Behavioral Patterns.](http://arxiv.org/abs/2303.10214) | BotShape是一种新型的基于行为模式的社交机器人检测方法，通过提取重要的行为特征可以提高检测性能。 |
| [^64] | [The NCI Imaging Data Commons as a platform for reproducible research in computational pathology.](http://arxiv.org/abs/2303.09354) | 国家癌症研究所影像数据共享平台 (IDC) 旨在促进计算病理学领域的研究可重复性，实现了 FAIR 原则，提供公共库和云端技术支持，方便使用机器学习方法进行癌症组织分类研究。 |
| [^65] | [A System for Generalized 3D Multi-Object Search.](http://arxiv.org/abs/2303.03178) | 本文介绍了GenMOS，这是第一个适用于实际机器人和环境的通用三维多物体搜索系统。它使用点云观察的三种方式，并通过在线规划输出一个六自由度视点，成功展示了其在多个数据集上的有效性和效率。 |
| [^66] | [Neural Common Neighbor with Completion for Link Prediction.](http://arxiv.org/abs/2302.00890) | 提出了神经通用邻居模型（NCN）用于链接预测，使用可学习的成对表示来捕捉节点之间的成对关系，以提高性能，同时解决链路不完整问题。 |
| [^67] | [MTP-GO: Graph-Based Probabilistic Multi-Agent Trajectory Prediction with Neural ODEs.](http://arxiv.org/abs/2302.00735) | 本文介绍了一种基于图的概率多智能体轨迹预测模型MTP-GO。该模型利用时间图神经网络编码场景，采用神经常微分方程实现运动模型，并结合混合密度网络和卡尔曼滤波实现多模态概率预测，在多个指标上优于其他最先进的方法。 |
| [^68] | [AttMEMO : Accelerating Transformers with Memoization on Big Memory Systems.](http://arxiv.org/abs/2301.09262) | 本文提出一种利用记忆技术加速自注意力机制的Transformer模型的推理过程的方法，该方法可以在不需要修改模型架构或使用特殊硬件的情况下进行，并可以使推理延迟降低22%。 |
| [^69] | [Consciousness is learning: predictive processing systems that learn by binding may perceive themselves as conscious.](http://arxiv.org/abs/2301.07016) | 通过层级绑定和联想检索变为短期和长期声明性记忆的在线预测处理系统可能会感知到自己具有意识。 |
| [^70] | [Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications.](http://arxiv.org/abs/2301.00752) | 本研究提出了一种基于点云的毫米波通信主动链路质量预测方法，相比于基于图像的方法，其适用性更广且不涉及敏感信息。 |
| [^71] | [A Semantic Framework for Neural-Symbolic Computing.](http://arxiv.org/abs/2212.12050) | 该论文提出了一个神经符号计算的语义框架，用于将神经网络和符号AI相结合成为综合系统，并通过将符号知识编码到神经网络中来解决通用推理能力的问题。 |
| [^72] | [Active Task Randomization: Learning Robust Skills via Unsupervised Generation of Diverse and Feasible Tasks.](http://arxiv.org/abs/2211.06134) | 本文提出了一种称为主动任务随机化（ATR）的方法，通过无监督生成任务来学习鲁棒技能，该方法选择适合学习鲁棒技能的任务，通过平衡任务的多样性和可行性来预测任务多样性和可行性，并使用基于图的参数化程序生成任务，从而允许鲁棒地处理任务的各种情况，包括以前未见过的场景。 |
| [^73] | [Knowledge is Power: Understanding Causality Makes Legal judgment Prediction Models More Generalizable and Robust.](http://arxiv.org/abs/2211.03046) | 本研究使用因果结构模型分析了法律判决预测模型学习决策的原理，发现现有最先进模型利用非因果信息进行判决预测，违反法律规则会削弱模型鲁棒性和普适性并导致歧视问题，提出基于因果干预的解决方案。 |
| [^74] | [FedTP: Federated Learning by Transformer Personalization.](http://arxiv.org/abs/2211.01572) | 本文研究发现联邦平均算法对Transformer模型中的自注意力存在负面影响，限制了联邦学习的能力。为此提出了FedTP，在学习客户端个性化自注意力的同时，将其他参数聚合在客户端之间。 |
| [^75] | [Generalization with Lossy Affordances: Leveraging Broad Offline Data for Learning Visuomotor Tasks.](http://arxiv.org/abs/2210.06601) | 本文提出了一个利用有损表示空间下的子目标指导在线微调的框架，它可以从广泛的数据中学习而来的有损表示来规划一系列子目标，分解原始任务，并强调任务相关信息，从而降低泛化过程中冗余内容的干扰，以此应对如何利用多样化的多任务数据进行新领域下游任务的学习的挑战。 |
| [^76] | [A Maintenance Planning Framework using Online and Offline Deep Reinforcement Learning.](http://arxiv.org/abs/2208.00808) | 本论文提出了一种利用在线和离线深度强化学习的维护计划框架，以确定最优恢复策略，经过实验证明该框架比标准的预防性、纠正性和贪婪式计划方案有所改进。 |
| [^77] | [Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering.](http://arxiv.org/abs/2207.12647) | 本篇论文提出了一个新型的事件级视觉问答框架——跨模态因果关系推理（CMCIR），通过引入因果干预方法，发现视觉和语言模态的真正因果结构，实现强健的因果感知视觉语言问答。 |
| [^78] | [q-Learning in Continuous Time.](http://arxiv.org/abs/2207.00713) | 本文研究了连续时间下的q-Learning，通过引入小q函数作为一阶近似，研究了q-learning理论，应用于设计不同的演员-评论家算法。 |
| [^79] | [Checking Trustworthiness of Probabilistic Computations in a Typed Natural Deduction System.](http://arxiv.org/abs/2206.12934) | 本文介绍了一种名为TPTND的概率类型自然演算系统，该系统能够检验并推导概率计算过程的可信性，具有可检查性的优势。 |
| [^80] | [HyGNN: Drug-Drug Interaction Prediction via Hypergraph Neural Network.](http://arxiv.org/abs/2206.12747) | 本研究提出了基于超图注意力神经网络的药物相互作用预测模型HyGNN，可以基于药物的SMILES字符串进行预测，并在多个基准数据集上的表现优于其他最先进的方法。 |
| [^81] | [Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space.](http://arxiv.org/abs/2205.08129) | 本文提出了一种名为PTP的方法，利用高层规划器和潜空间中的条件生成器来分解目标成子目标，并在以前的数据上预训练条件子目标生成器和策略，然后在线微调以适应新的目标，从而训练目标导向的策略，有效提高了机器人长期目标导向的经验采集效率。 |
| [^82] | [Reducing Predictive Feature Suppression in Resource-Constrained Contrastive Image-Caption Retrieval.](http://arxiv.org/abs/2204.13382) | 本文提出了一种名为潜在目标解码（LTD）的方法，可以在资源受限的情况下减少预测特征抑制，从而为对比图像-字幕检索（ICR）方法提供了一种解决方案。 |
| [^83] | [A Fully Polynomial Time Approximation Scheme for Constrained MDPs and Stochastic Shortest Path under Local Transitions.](http://arxiv.org/abs/2204.04780) | 本文提出了约束MDP和局部转移条件下随机最短路径的全多项式时间逼近方案。 |
| [^84] | [Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective.](http://arxiv.org/abs/2202.08063) | 低资源情境下，如何让知识抽取更好地从非结构化文本中提取信息？本文调研了三种解决范式：高资源数据、更强的模型和数据与模型的结合，提出了未来的研究方向。 |
| [^85] | [A Cognitive Explainer for Fetal ultrasound images classifier Based on Medical Concepts.](http://arxiv.org/abs/2201.07798) | 本研究提出了一种基于关键医学概念的可解释框架，利用概念间的关系构建图卷积神经网络，解释胎儿超声图像分类器的决策过程，为临床医生提供易于理解的推理结果见解。 |
| [^86] | [Faster Deep Reinforcement Learning with Slower Online Network.](http://arxiv.org/abs/2112.05848) | 本文改进了DQN和Rainbow两个深度强化学习算法，大大提高了它们在Atari游戏基准测试中的性能，我们的方法是在在线网络和目标网络之间引入一定的接近度，以提高深度强化学习的鲁棒性。 |
| [^87] | [Binarized ResNet: Enabling Robust Automatic Modulation Classification at the resource-constrained Edge.](http://arxiv.org/abs/2110.14357) | 本研究提出了一个旋转二值化大型ResNet（RBLResNet），可用于资源受限的边缘自动调制识别。通过多级分类和保留低内存和计算功率的RBLResNet打包来提高准确率，达到至高94.49%的性能，优于其他二元和实值网络。 |
| [^88] | [A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues.](http://arxiv.org/abs/2107.08574) | 提出了一种新颖的神经网络修正方法，用于缓解低质量和缺失数据的影响，具备神经调制特征，通过一个额外的输入的函数替换了全连接层的固定权重，使得在测试中具有调制层的模型对于数据质量的降解更加鲁棒，同时也能够节省训练时间并且不会受到插补错误的影响。 |
| [^89] | [Online Sub-Sampling for Reinforcement Learning with General Function Approximation.](http://arxiv.org/abs/2106.07203) | 本文提出了一种基于在线子采样框架的强化学习算法，利用数据点的信息增益量来指导探索，与现有方法相比更新RL算法的策略次数大大减少，但仍保持较小的近似最优遗憾边界。 |
| [^90] | [A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics.](http://arxiv.org/abs/2103.01403) | 本文提出了一个新的数据集HINT，旨在检验机器学习通用概念的能力，包括感知、语法和语义三个层次。为了检验模型的插值和外推能力，我们设计了一个五倍交叉测试集。通过对几种最先进的模型进行广泛实验，进一步探究其局限性。 |

# 详细

[^1]: 电商中优化赞助产品的实践经验

    Practical Lessons on Optimizing Sponsored Products in eCommerce. (arXiv:2304.09107v1 [cs.IR])

    [http://arxiv.org/abs/2304.09107](http://arxiv.org/abs/2304.09107)

    本文提出了一个实用的机器学习框架，可以解决电商广告系统中赞助产品优化的多个问题，而不需要改变现有机器学习模型结构。使用该框架可以处理广告系统中的长期问题，并为多个评估指标带来增量。

    

    本文研究了赞助产品优化中的多个问题，包括基于位置的去偏差、点击-转化多任务学习以及预测点击率的校准。我们提出了一个实用的机器学习框架，可以解决这些问题，而不需要改变现有机器学习模型的结构，因此可以与大多数机器学习模型结合使用（包括浅层模型，如梯度提升决策树、支持向量机）。在本文中，我们首先提出了数据和特征工程技术，以处理广告系统中的上述问题; 然后，我们评估了我们的实用框架在来自在线购物网站流量日志的实际数据集上的效益。我们表明，我们的提议的实用框架与数据和特征工程也可以处理广告系统中的长期问题，并为多个评估指标带来增量。

    In this paper, we study multiple problems from sponsored product optimization in ad system, including position-based de-biasing, click-conversion multi-task learning, and calibration on predicted click-through-rate (pCTR). We propose a practical machine learning framework that provides the solutions to such problems without structural change to existing machine learning models, thus can be combined with most machine learning models including shallow models (e.g. gradient boosting decision trees, support vector machines). In this paper, we first propose data and feature engineering techniques to handle the aforementioned problems in ad system; after that, we evaluate the benefit of our practical framework on real-world data sets from our traffic logs from online shopping site. We show that our proposed practical framework with data and feature engineering can also handle the perennial problems in ad systems and bring increments to multiple evaluation metrics.
    
[^2]: 将语言模型与符号求解器相结合求解数学单词问题

    Solving Math Word Problems by Combining Language Models With Symbolic Solvers. (arXiv:2304.09102v1 [cs.CL])

    [http://arxiv.org/abs/2304.09102](http://arxiv.org/abs/2304.09102)

    该论文提出了一种将语言模型与符号求解器相结合的方法，以解决数学单词问题，尤其是具有挑战性的问题，并在此过程中突出了使用声明性和逐步表示的好处。

    

    自动生成高质量的逐步解决数学单词问题的解决方案在教育中有许多应用。最近，将大型语言模型（LLM）与外部工具结合使用以执行复杂的推理和计算已成为解决数学单词问题的有前途的方向，但先前的方法（如程序辅助语言模型（PAL））对于需要陈述性推理的问题具有偏见，而对于简单的过程问题则不太有效。我们提出了一种方法，将一种可以将单词问题逐步正式化为一组变量和方程式的LLM与外部符号求解器相结合，以解决方程。我们的方法在GSM8K数学单词问题基准测试上实现了与原始PAL相当的准确度，而在ALGEBRA上则明显优于PAL，该数据集从代数教科书中提取更具挑战性的单词问题。我们的工作突出了在解决数学单词问题时使用声明性和逐步表示的好处。

    Automatically generating high-quality step-by-step solutions to math word problems has many applications in education. Recently, combining large language models (LLMs) with external tools to perform complex reasoning and calculation has emerged as a promising direction for solving math word problems, but prior approaches such as Program-Aided Language model (PAL) are biased towards simple procedural problems and less effective for problems that require declarative reasoning. We propose an approach that combines an LLM that can incrementally formalize word problems as a set of variables and equations with an external symbolic solver that can solve the equations. Our approach achieves comparable accuracy to the original PAL on the GSM8K benchmark of math word problems and outperforms PAL by an absolute 20% on ALGEBRA, a new dataset of more challenging word problems extracted from Algebra textbooks. Our work highlights the benefits of using declarative and incremental representations when
    
[^3]: MATURE-HEALTH: MAndatory FeaTURE选择的健康推荐系统

    MATURE-HEALTH: HEALTH Recommender System for MAndatory FeaTURE choices. (arXiv:2304.09099v1 [cs.IR])

    [http://arxiv.org/abs/2304.09099](http://arxiv.org/abs/2304.09099)

    该论文提出和实施了一个名为MATURE-HEALTH的健康推荐系统，该系统能够预测电解质不平衡并推荐营养平衡的食物，从而增加早期检测疾病的机会并防止健康进一步恶化。

    

    平衡电解质对于人体器官的适当功能至关重要和必不可少，因为电解质失衡可能是潜在病理生理学发展的指示。高效监测电解质失衡不仅可以增加疾病早期检测的机会，而且可以通过严格遵循营养控制饮食以平衡电解质从而防止健康进一步恶化。本研究提出并实施了一个推荐系统MATURE Health，该系统预测血液中必需电解质和其他物质的不平衡，然后推荐含有平衡营养的食物，以避免电解质不平衡的发生。该模型考虑到用户最近的实验室结果和每日食物摄入量来预测电解质不平衡。MATURE Health依赖于MATURE Food算法推荐食物，后者仅推荐那些

    Balancing electrolytes is utmost important and essential for appropriate functioning of organs in human body as electrolytes imbalance can be an indication of the development of underlying pathophysiology. Efficient monitoring of electrolytes imbalance not only can increase the chances of early detection of disease, but also prevents the further deterioration of the health by strictly following nutrient controlled diet for balancing the electrolytes post disease detection. In this research, a recommender system MATURE Health is proposed and implemented, which predicts the imbalance of mandatory electrolytes and other substances presented in blood and recommends the food items with the balanced nutrients to avoid occurrence of the electrolytes imbalance. The proposed model takes user most recent laboratory results and daily food intake into account to predict the electrolytes imbalance. MATURE Health relies on MATURE Food algorithm to recommend food items as latter recommends only those
    
[^4]: MDDL: 基于强化学习的多通道Feed位置分配框架

    MDDL: A Framework for Reinforcement Learning-based Position Allocation in Multi-Channel Feed. (arXiv:2304.09087v1 [cs.IR])

    [http://arxiv.org/abs/2304.09087](http://arxiv.org/abs/2304.09087)

    本研究提出了一种名为MDDL的多通道深度确定性策略梯度学习框架，旨在整合多种策略，以增强位置分配的强化学习模型训练。该框架在在线和离线性能方面表现优于一些最先进的方法。

    

    目前，位置分配系统的主流方法是利用强化学习模型为各通道的物品分配合适的位置，然后混合到Feed中。强化学习模型的训练使用两种数据：策略数据和随机数据。策略数据来自当前在线模型，它受到状态-动作对分布不均衡的困扰，导致训练过程中存在严重的高估问题。另一方面，随机数据提供了更均匀的状态-动作对分布，但在工业场景中很难获取，因为随机探索可能会对平台收入和用户体验产生负面影响。由于这两种数据具有不同的分布，因此设计一种有效的策略来利用两种数据以增强强化学习模型的训练效果已成为一个极具挑战性的问题。本研究提出了一种名为MDDL（多通道深度确定性策略梯度学习）的框架来解决上述问题。我们的框架旨在整合多种策略，以增强位置分配的RL模型训练。实验证明，在线和离线性能方面，MDDL表现优于一些最先进的方法。

    Nowadays, the mainstream approach in position allocation system is to utilize a reinforcement learning model to allocate appropriate locations for items in various channels and then mix them into the feed. There are two types of data employed to train reinforcement learning (RL) model for position allocation, named strategy data and random data. Strategy data is collected from the current online model, it suffers from an imbalanced distribution of state-action pairs, resulting in severe overestimation problems during training. On the other hand, random data offers a more uniform distribution of state-action pairs, but is challenging to obtain in industrial scenarios as it could negatively impact platform revenue and user experience due to random exploration. As the two types of data have different distributions, designing an effective strategy to leverage both types of data to enhance the efficacy of the RL model training has become a highly challenging problem. In this study, we propo
    
[^5]: 基于LLM的交互式内容生成：IT部门员工知觉案例研究

    LLM-based Interaction for Content Generation: A Case Study on the Perception of Employees in an IT department. (arXiv:2304.09064v1 [cs.HC])

    [http://arxiv.org/abs/2304.09064](http://arxiv.org/abs/2304.09064)

    本文通过IT部门员工的问卷调查研究了他们即将使用的基于LLM技术的内容生成工具的意愿，结果显示实用性高的工具更容易被接受和使用。

    

    近年来，人工智能在自然语言处理领域取得了许多进展，LLM技术的出现使得人类可以更好地访问或生成内容。目前关于LLM技术生成工具的研究主要关注这些工具在生成相关内容（代码、文本或图像）方面的性能。然而，与生成工具设计和使用相关的伦理问题似乎越来越多，这影响了特定任务的公众可接受性。本文通过问卷调查，基于测量使用意愿的经验模型（Davis, 1989的TAM和Venkatesh等人的UTAUT2），旨在确定IT公司员工在工作环境中使用生成工具的意愿。结果显示，虽然对生成工具的可接受性评价较为一般，但工具的实用性被认为越高，使用意愿就越强烈。此外，我们的分析表明……（未完整公开）。

    In the past years, AI has seen many advances in the field of NLP. This has led to the emergence of LLMs, such as the now famous GPT-3.5, which revolutionise the way humans can access or generate content. Current studies on LLM-based generative tools are mainly interested in the performance of such tools in generating relevant content (code, text or image). However, ethical concerns related to the design and use of generative tools seem to be growing, impacting the public acceptability for specific tasks. This paper presents a questionnaire survey to identify the intention to use generative tools by employees of an IT company in the context of their work. This survey is based on empirical models measuring intention to use (TAM by Davis, 1989, and UTAUT2 by Venkatesh and al., 2008). Our results indicate a rather average acceptability of generative tools, although the more useful the tool is perceived to be, the higher the intention to use seems to be. Furthermore, our analyses suggest th
    
[^6]: 重访基于预训练语言模型的k-NN

    Revisiting k-NN for Pre-trained Language Models. (arXiv:2304.09058v1 [cs.CL])

    [http://arxiv.org/abs/2304.09058](http://arxiv.org/abs/2304.09058)

    本研究提出一种新方法，结合k-NN和预训练语言模型（PLMs）能够提高自然语言处理（NLP）的性能，并在多个基准数据集上得到验证。

    

    预训练语言模型（PLMs）作为参数化的急切学习器，已成为自然语言处理（NLP）当前范式的实际选择。与此形成对比的是，k-最近邻（k-NN）分类器作为延迟学习模型，倾向于减轻过拟合和孤立噪声。本文中我们重访了k-NN分类器，以增强基于PLMs的分类器。从方法层面上，我们提出采用文本表示的PLMs在两个步骤中采用k-NN：（1）利用k-NN作为先验知识来校准训练过程（2）线性插值k-NN预测的概率分布和PLMs分类器的概率分布。我们的方法核心是实现了k-NN校准训练，将预测结果作为训练过程中易于和难以学习的示例的指标。从应用场景多样性的角度出发，我们在各种基准数据集上进行了广泛的微调、提示微调范式和零样本任务设置的实验。我们的结果表明，结合k-NN可以在所有受到检查的设置中持续提高PLMs的性能，并且在所有受到考虑的设置中跑赢了基于普通PLMs的方法。

    Pre-trained Language Models (PLMs), as parametric-based eager learners, have become the de-facto choice for current paradigms of Natural Language Processing (NLP). In contrast, k-Nearest-Neighbor (k-NN) classifiers, as the lazy learning paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we revisit k-NN classifiers for augmenting the PLMs-based classifiers. From the methodological level, we propose to adopt k-NN with textual representations of PLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the training process. (2) Linearly interpolate the probability distribution predicted by k-NN with that of the PLMs' classifier. At the heart of our approach is the implementation of k-NN-calibrated training, which treats predicted results as indicators for easy versus hard examples during the training process. From the perspective of the diversity of application scenarios, we conduct extensive experiments on fine-tuning, prompt-tuning paradigms and zero-sh
    
[^7]: CodeKGC：用于生成知识图谱构建的代码语言模型

    CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v1 [cs.CL])

    [http://arxiv.org/abs/2304.09048](http://arxiv.org/abs/2304.09048)

    本文提出了一种使用代码语言模型处理生成式知识图谱构建任务的方法，能够有效利用知识图谱内的语义结构，提高模型的可解释性。

    

    目前的生成式知识图谱构建方法通常无法捕捉结构性知识，而只是将自然语言转化为序列化文本或规范语言。然而，对于像代码这样的结构化数据进行训练的大型生成式语言模型已经展现了在理解自然语言以进行结构性预测和推理任务方面的卓越能力。本文提出了一种使用代码语言模型处理生成式知识图谱构建任务的方法。具体而言，在给定代码格式的自然语言输入的情况下，目标是生成可以表示为代码补全任务的三元组。我们开发了具有模式感知型提示的方法，可以有效利用知识图谱内的语义结构。由于代码本质上具有结构，如类和函数定义，因此它作为先验的语义结构知识模型非常有用。此外，我们采用了基于原理的生成方法来提高性能。原理提供了模型生成结果的可解释性。

    Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provi
    
[^8]: PaTeCon：基于模式的知识图谱时间约束挖掘方法用于冲突检测

    PaTeCon: A Pattern-Based Temporal Constraint Mining Method for Conflict Detection on Knowledge Graphs. (arXiv:2304.09015v1 [cs.AI])

    [http://arxiv.org/abs/2304.09015](http://arxiv.org/abs/2304.09015)

    PaTeCon是一种基于模式的知识图谱时间约束挖掘方法，能够自动生成时间约束来维护KG的时间一致性，并在不需要人工专家的情况下准确地检测潜在的时间冲突。

    

    在知识图谱（KG）研究社区中，时间事实指特定时间段内发生的事件的数据。引入时间限制给KG的时间一致性维护带来了新的挑战，先前的研究依赖于手动列举时间约束来检测冲突，这很费力且可能存在粒度问题。本文提出了一种基于模式的时间约束挖掘方法PaTeCon，它使用自动确定的图形模式及其相关统计信息代替人工专家来生成时间约束。具体地，PaTeCon根据其测量得分动态地将类限制附加到候选约束上。我们基于维基数据集评估了PaTeCon的效果。

    Temporal facts, the facts for characterizing events that hold in specific time periods, are attracting rising attention in the knowledge graph (KG) research communities. In terms of quality management, the introduction of time restrictions brings new challenges to maintaining the temporal consistency of KGs and detecting potential temporal conflicts. Previous studies rely on manually enumerated temporal constraints to detect conflicts, which are labor-intensive and may have granularity issues. We start from the common pattern of temporal facts and constraints and propose a pattern-based temporal constraint mining method, PaTeCon. PaTeCon uses automatically determined graph patterns and their relevant statistical information over the given KG instead of human experts to generate time constraints. Specifically, PaTeCon dynamically attaches class restriction to candidate constraints according to their measuring scores.We evaluate PaTeCon on two large-scale datasets based on Wikidata and F
    
[^9]: 一种用于提取葡萄牙语肿瘤健康记录的生物医学实体提取流水线

    A Biomedical Entity Extraction Pipeline for Oncology Health Records in Portuguese. (arXiv:2304.08999v1 [cs.CL])

    [http://arxiv.org/abs/2304.08999](http://arxiv.org/abs/2304.08999)

    本研究开发了一种有效提取葡萄牙语肿瘤健康记录中过程、药物和疾病的方法，帮助医护人员更高效地获取患者治疗状况的完整概述，有助于提升肿瘤治疗效果。

    

    癌症患者的文本健康记录通常很冗长且高度不结构化，使得医护人员获取完整患者治疗状况的完整概述非常耗时。由于这些限制可能导致次优和/或低效的治疗程序，因此医疗服务提供者将极大地受益于有效地概括这些记录的系统。随着深度神经模型的出现，对于英语临床文本，这个目标已经部分实现，然而，研究社区仍缺乏针对资源有限语言的有效解决方案。在本文中，我们介绍了我们开发的方法，从欧洲葡萄牙语肿瘤健康记录中提取过程、药物和疾病。这个项目与葡萄牙肿瘤研究所合作完成，该所除了拥有十多年的受保护的医疗记录外，在整个开发过程中还提供了肿瘤学专家的专业知识。

    Textual health records of cancer patients are usually protracted and highly unstructured, making it very time-consuming for health professionals to get a complete overview of the patient's therapeutic course. As such limitations can lead to suboptimal and/or inefficient treatment procedures, healthcare providers would greatly benefit from a system that effectively summarizes the information of those records. With the advent of deep neural models, this objective has been partially attained for English clinical texts, however, the research community still lacks an effective solution for languages with limited resources. In this paper, we present the approach we developed to extract procedures, drugs, and diseases from oncology health records written in European Portuguese. This project was conducted in collaboration with the Portuguese Institute for Oncology which, besides holding over $10$ years of duly protected medical records, also provided oncologist expertise throughout the develop
    
[^10]: D2CSE: 基于差异感知的深度连续提示用于对比句子嵌入

    D2CSE: Difference-aware Deep continuous prompts for Contrastive Sentence Embeddings. (arXiv:2304.08991v1 [cs.CL])

    [http://arxiv.org/abs/2304.08991](http://arxiv.org/abs/2304.08991)

    D2CSE是一种用于学习句子嵌入的新模型，采用基于差异感知的深度连续提示来计算具有区分微妙差异能力的句子向量。与现有方法相比，D2CSE只使用一个预训练语言模型，避免了繁琐的微调，并大大减少了训练参数数量，同时显著提高了句子嵌入的质量。

    

    本文介绍了一种名为D2CSE的基于差异感知的深度连续提示模型，用于学习句子嵌入。与现有方法相比，D2CSE采用了简单的神经架构来计算句子向量，使得其对于在类似句子中区分微妙差异有很好的表现。与需要多个预训练语言模型（PLMs）处理原始和损坏（微妙修改）句子对的现有神经网络不同，D2CSE仅通过执行多个任务（即，对比学习和条件替换标记检测）自主引导地优化了连续提示，从而避免了繁琐的多个PLMs的微调。D2CSE将单个PLM重载到连续提示上，大大节省了存储空间。 D2CSE的训练参数数量约为现有方法的1％，同时显著提高了句子嵌入的质量。

    This paper describes Difference-aware Deep continuous prompt for Contrastive Sentence Embeddings (D2CSE) that learns sentence embeddings. Compared to state-of-the-art approaches, D2CSE computes sentence vectors that are exceptional to distinguish a subtle difference in similar sentences by employing a simple neural architecture for continuous prompts. Unlike existing architectures that require multiple pretrained language models (PLMs) to process a pair of the original and corrupted (subtly modified) sentences, D2CSE avoids cumbersome fine-tuning of multiple PLMs by only optimizing continuous prompts by performing multiple tasks -- i.e., contrastive learning and conditional replaced token detection all done in a self-guided manner. D2CSE overloads a single PLM on continuous prompts and greatly saves memory consumption as a result. The number of training parameters in D2CSE is reduced to about 1\% of existing approaches while substantially improving the quality of sentence embeddings. W
    
[^11]: 随机鹦鹉寻找随机鹦鹉：LLMs易于微调且难以被其他LLMs检测到

    Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs. (arXiv:2304.08968v1 [cs.CL])

    [http://arxiv.org/abs/2304.08968](http://arxiv.org/abs/2304.08968)

    LLMs在公众中广泛应用，但是目前大部分检测工具存在严重缺陷。研究发现，LLMs容易微调且难以被其他LLMs检测到。

    

    自我注意力革命使生成式语言模型得以扩展并实现越来越惊人的能力。这些模型通常称为大型语言模型（LLMs），最近由于对话微调而在公众中获得了广泛关注，从而使其行为符合公众对于AI的期望。然而，这种突出也加大了关注LLMs误用的先前担忧，并导致出现许多在野外检测LLMs的工具。不幸的是，大多数这样的工具都存在严重缺陷。我们在这里展示了一种新方法，可以大大降低基于微调的自动编码器检测LLMs的成功率，并说明我们的工作涉及的重要细节。

    The self-attention revolution allowed generative language models to scale and achieve increasingly impressive abilities. Such models - commonly referred to as Large Language Models (LLMs) - have recently gained prominence with the general public, thanks to conversational fine-tuning, putting their behavior in line with public expectations regarding AI. This prominence amplified prior concerns regarding the misuse of LLMs and led to the emergence of numerous tools to detect LLMs in the wild.  Unfortunately, most such tools are critically flawed. While major publications in the LLM detectability field suggested that LLMs were easy to detect with fine-tuned autoencoders, the limitations of their results are easy to overlook. Specifically, they assumed publicly available generative models without fine-tunes or non-trivial prompts. While the importance of these assumptions has been demonstrated, until now, it remained unclear how well such detection could be countered.  Here, we show that a
    
[^12]: 通过主动奖励学习实现可证明反馈高效的强化学习

    Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning. (arXiv:2304.08944v1 [cs.LG])

    [http://arxiv.org/abs/2304.08944](http://arxiv.org/abs/2304.08944)

    本文提出了一种主动奖励学习的强化学习算法，采用人机交互来指定任务的奖励，大大减少了所需的人类反馈，并在效率上提供了理论保证。

    

    在强化学习中，合适的奖励函数对于明确任务非常重要。但是，即使对于简单的任务而言，设计正确的奖励函数也是非常具有挑战性的。人机交互强化学习（HiL RL）允许人类通过提供各种类型的反馈来向RL代理传达复杂的目标。然而，尽管取得了巨大的经验成功，但HiL RL通常需要太多人类教师的反馈，并且理论理解不足。本文从理论角度解决了这个问题，旨在提供可证明的反馈高效的算法框架，以采用人机交互来指定给定任务的奖励。我们提供了一种基于主动学习的RL算法，该算法首先在不指定奖励函数的情况下探索环境，然后仅询问人类教师有关某些状态动作对的任务奖励的少量查询。之后，该算法学习具有可证明遗憾界限的奖励函数，并实现接近最优的性能。与先前的HiL RL方法相比，我们的算法大大减少了所需的人类反馈，并在效率上提供了理论保证。

    An appropriate reward function is of paramount importance in specifying a task in reinforcement learning (RL). Yet, it is known to be extremely challenging in practice to design a correct reward function for even simple tasks. Human-in-the-loop (HiL) RL allows humans to communicate complex goals to the RL agent by providing various types of feedback. However, despite achieving great empirical successes, HiL RL usually requires too much feedback from a human teacher and also suffers from insufficient theoretical understanding. In this paper, we focus on addressing this issue from a theoretical perspective, aiming to provide provably feedback-efficient algorithmic frameworks that take human-in-the-loop to specify rewards of given tasks. We provide an active-learning-based RL algorithm that first explores the environment without specifying a reward function and then asks a human teacher for only a few queries about the rewards of a task at some state-action pairs. After that, the algorith
    
[^13]: 神经崩溃现象的研究：Grassmannian Frame、对称性和泛化

    A Study of Neural Collapse Phenomenon: Grassmannian Frame, Symmetry, Generalization. (arXiv:2304.08914v1 [cs.LG])

    [http://arxiv.org/abs/2304.08914](http://arxiv.org/abs/2304.08914)

    本文提出了广义神经崩溃假设，发现了Grassmannian Frame结构和对称泛化现象，这对特征选择和神经网络设计都具有重要作用。

    

    本文通过证明广义神经崩溃假设推广了原始的神经崩溃现象。我们通过分类的优化和泛化得到了Grassmannian Frame结构。该结构在球面上最大化地分离了每两个类别的特征，并且不需要一个更大的特征维度。出于对Grassmannian Frame对称性的好奇，我们进行了一系列实验，探索不同Grassmannian Frame模型是否会产生不同的表现。结果我们发现了对称泛化现象。我们提出了一个关于置换对称泛化的定理。然而，为什么特征的不同方向会导致如此不同的泛化现象的问题仍然需要进一步研究。

    In this paper, we extends original Neural Collapse Phenomenon by proving Generalized Neural Collapse hypothesis. We obtain Grassmannian Frame structure from the optimization and generalization of classification. This structure maximally separates features of every two classes on a sphere and does not require a larger feature dimension than the number of classes. Out of curiosity about the symmetry of Grassmannian Frame, we conduct experiments to explore if models with different Grassmannian Frames have different performance. As a result, we discover the Symmetric Generalization phenomenon. We provide a theorem to explain Symmetric Generalization of permutation. However, the question of why different directions of features can lead to such different generalization is still open for future investigation.
    
[^14]: GKLS发生器的计算和探索性景观分析

    Computational and Exploratory Landscape Analysis of the GKLS Generator. (arXiv:2304.08913v1 [cs.NE])

    [http://arxiv.org/abs/2304.08913](http://arxiv.org/abs/2304.08913)

    本文对GKLS发生器进行了计算和探索性景观分析，发现GKLS发生器生成的问题在更高维度优化变得极其困难。

    

    GKLS发生器是用于测试全局优化算法的最常用测试平台之一。在本文中，我们对GKLS发生器进行了计算分析和探索性景观分析(ELA)。我们使用GKLS生成的经典类和新生成的类问题进行基准测试，比较了三种最先进的方法（来自进化和确定性社区）在5维和10维中的表现。我们发现GKLS生成器产生了“大海捞针”类型的问题，在更高的维度中优化变得极其困难。此外，我们对GKLS发生器进行了ELA，然后将其与另外两个广泛使用的基准测试集(BBOB和CEC 2014)进行了比较，并讨论了结果的意义。

    The GKLS generator is one of the most used testbeds for benchmarking global optimization algorithms. In this paper, we conduct both a computational analysis and the Exploratory Landscape Analysis (ELA) of the GKLS generator. We utilize both canonically used and newly generated classes of GKLS-generated problems and show their use in benchmarking three state-of-the-art methods (from evolutionary and deterministic communities) in dimensions 5 and 10. We show that the GKLS generator produces ``needle in a haystack'' type problems that become extremely difficult to optimize in higher dimensions. Furthermore, we conduct the ELA on the GKLS generator and then compare it to the ELA of two other widely used benchmark sets (BBOB and CEC 2014), and discuss the meaningfulness of the results.
    
[^15]: 带有自我改进硬约束的多能源管理系统的安全强化学习

    Safe reinforcement learning with self-improving hard constraints for multi-energy management systems. (arXiv:2304.08897v1 [eess.SY])

    [http://arxiv.org/abs/2304.08897](http://arxiv.org/abs/2304.08897)

    本论文提出了一种安全强化学习方法，能够实现多能源管理系统中的最优控制，在保证硬约束的前提下减少工程工作，降低建模偏差，并避免潜在的不安全行为。

    

    带有硬约束保证的安全强化学习是多能源管理系统中最有前途的最优控制方向。它只需要在环境特定的约束函数本身上预先而不是完整的模型（即植物，干扰和噪声模型，以及未包括在植物模型中的状态的预测模型 - 例如需求，天气和价格预测）。因此，可减少项目特定的前期和持续的工程工作，仍可以学习更好地表示基础系统动态，并使建模偏差最小化（无基于模型的目标函数）。然而，即使仅约束函数本身有时也不总是容易提供准确的先验（例如能量平衡约束需要详细确定所有能量输入和输出），从而导致潜在的不安全行为。在本文中，我们提出了两个新的进展：（I）将Optlayer和SafeFallback方法结合起来，命名为O

    Safe reinforcement learning (RL) with hard constraint guarantees is a promising optimal control direction for multi-energy management systems. It only requires the environment-specific constraint functions itself a prior and not a complete model (i.e. plant, disturbance and noise models, and prediction models for states not included in the plant model - e.g. demand, weather, and price forecasts). The project-specific upfront and ongoing engineering efforts are therefore still reduced, better representations of the underlying system dynamics can still be learned and modeling bias is kept to a minimum (no model-based objective function). However, even the constraint functions alone are not always trivial to accurately provide in advance (e.g. an energy balance constraint requires the detailed determination of all energy inputs and outputs), leading to potentially unsafe behavior. In this paper, we present two novel advancements: (I) combining the Optlayer and SafeFallback method, named O
    
[^16]: 自主系统：室内无人机导航

    Autonomous Systems: Autonomous Systems: Indoor Drone Navigation. (arXiv:2304.08893v1 [cs.RO])

    [http://arxiv.org/abs/2304.08893](http://arxiv.org/abs/2304.08893)

    该论文介绍了一种使用ROS slam工具箱和Nav2导航系统框架构建的模拟无人机，能够在室内环境中自主移动的技术。

    

    无人机是一种有前途的自主数据收集和室内感知技术。当人控制的无人机可能不实际或不可靠时，例如在未知或危险的地点，使用自主无人机提供了灵活性、节约成本和降低风险。该系统使用gazebo模拟工具和称为Nav2的ros导航系统框架创建了一个模拟四轴飞行器，能够在室内环境中自主移动。虽然Nav2在陆地机器人和车辆中成功展示了自主导航的功能，但在无人机中还没有达成此目标。目标是利用ROS的slam工具箱和Nav2导航系统框架构建一个模拟的无人机，可以在室内（无gps）环境中自主移动。

    Drones are a promising technology for autonomous data collection and indoor sensing. In situations when human-controlled UAVs may not be practical or dependable, such as in uncharted or dangerous locations, the usage of autonomous UAVs offers flexibility, cost savings, and reduced risk. The system creates a simulated quadcopter capable of autonomously travelling in an indoor environment using the gazebo simulation tool and the ros navigation system framework known as Navigaation2. While Nav2 has successfully shown the functioning of autonomous navigation in terrestrial robots and vehicles, the same hasn't been accomplished with unmanned aerial vehicles and still has to be done. The goal is to use the slam toolbox for ROS and the Nav2 navigation system framework to construct a simulated drone that can move autonomously in an indoor (gps-less) environment.
    
[^17]: NPS: 使用图神经网络实现精准程序采样的框架

    NPS: A Framework for Accurate Program Sampling Using Graph Neural Network. (arXiv:2304.08880v1 [cs.AR])

    [http://arxiv.org/abs/2304.08880](http://arxiv.org/abs/2304.08880)

    NPS是一种使用图神经网络进行程序采样的框架，通过学习执行嵌入并快速生成代表性模拟点，实现了高效的微处理器设计。

    

    随着摩尔定律结束，现代处理器（如RISC-V自定义扩展）需要快速的架构创新来维持性能增长，程序采样是微处理器设计中关键的一步，因为它选择工作量模拟的代表性模拟点。本文介绍了神经程序采样（NPS），它使用动态快照的图神经网络学习执行嵌入。NPS采用AssemblyNet进行嵌入生成，利用应用程序的代码结构和运行时状态，将AssemblyNet作为NPS的图模型和神经架构，捕获程序的行为，在数据计算、代码路径和数据流等方面。实验结果显示，NPS在速度和准确性方面优于SimPoint，在提高工作负载代表性率的同时，将模拟时间缩短高达60%。

    With the end of Moore's Law, there is a growing demand for rapid architectural innovations in modern processors, such as RISC-V custom extensions, to continue performance scaling. Program sampling is a crucial step in microprocessor design, as it selects representative simulation points for workload simulation. While SimPoint has been the de-facto approach for decades, its limited expressiveness with Basic Block Vector (BBV) requires time-consuming human tuning, often taking months, which impedes fast innovation and agile hardware development. This paper introduces Neural Program Sampling (NPS), a novel framework that learns execution embeddings using dynamic snapshots of a Graph Neural Network. NPS deploys AssemblyNet for embedding generation, leveraging an application's code structures and runtime states. AssemblyNet serves as NPS's graph model and neural architecture, capturing a program's behavior in aspects such as data computation, code path, and data flow. AssemblyNet is trained
    
[^18]: 可解释人工智能对认知负荷的影响：一项实证研究的见解

    Impact Of Explainable AI On Cognitive Load: Insights From An Empirical Study. (arXiv:2304.08861v1 [cs.AI])

    [http://arxiv.org/abs/2304.08861](http://arxiv.org/abs/2304.08861)

    一项实证研究表明，可解释人工智能对最终用户的认知负荷、任务表现和任务时间有重要影响，推荐使用实现无关的地方 XAI 说明类型。

    

    虽然可解释人工智能（XAI）这一新兴研究领域旨在解决高性能机器学习模型解释缺陷的问题，但在实践中，XAI 更多地面向开发人员而非最终用户。因此，最终用户常常不愿意使用基于 XAI 的决策支持系统。同时，关于最终用户在使用 XAI 说明时的行为缺乏跨学科研究，因此我们通过一项 COVID-19 的使用案例，对271名潜在医生进行了实证研究，测量了他们在使用不同的实现无关 XAI 说明类型时的认知负荷，任务表现和任务时间。我们发现，这些说明类型强烈影响了最终用户的认知负荷、任务表现和任务时间。进一步地，我们研究了一个精神效率度量标准，其中地方 XAI 说明类型排名最佳，提供了建议。

    While the emerging research field of explainable artificial intelligence (XAI) claims to address the lack of explainability in high-performance machine learning models, in practice, XAI targets developers rather than actual end-users. Unsurprisingly, end-users are often unwilling to use XAI-based decision support systems. Similarly, there is limited interdisciplinary research on end-users' behavior during XAI explanations usage, rendering it unknown how explanations may impact cognitive load and further affect end-user performance. Therefore, we conducted an empirical study with 271 prospective physicians, measuring their cognitive load, task performance, and task time for distinct implementation-independent XAI explanation types using a COVID-19 use case. We found that these explanation types strongly influence end-users' cognitive load, task performance, and task time. Further, we contextualized a mental efficiency metric, ranking local XAI explanation types best, to provide recommen
    
[^19]: 揭示群体MCDM中聚合和分散谬误的问题

    Unveiling and unraveling aggregation and dispersion fallacies in group MCDM. (arXiv:2304.08859v1 [stat.ME])

    [http://arxiv.org/abs/2304.08859](http://arxiv.org/abs/2304.08859)

    本研究发现在群体MCDM中普遍存在的错误有三种，包括用错误的平均方法、误用标准偏差和距离函数等。通过使用组成数据分析，我们提出了聚合和分析优先级的新方法，有助于避免这些错误。

    

    多准则决策中的优先级传达了一个准则优先于另一个的相关偏好，通常通过施加非负和单位和约束来反映。这些优先级的处理与其他非约束数据不同，但研究人员往往忽视了这一点，导致了谬误的统计分析。本文研究了群体MCDM中的三种普遍错误，以及基于组成数据分析的解决方案，以避免误用统计操作。首先，我们使用组成方法来聚合DM组的优先级，并显示组成分析的结果与标准化几何平均值相同，这意味着应避免算术平均值。此外，我们开发了一种新的聚合方法，它是几何平均数的强大替代品。我们还讨论了计算离散度量，包括标准偏差和距离函数的误差。

    Priorities in multi-criteria decision-making (MCDM) convey the relevance preference of one criterion over another, which is usually reflected by imposing the non-negativity and unit-sum constraints. The processing of such priorities is different than other unconstrained data, but this point is often neglected by researchers, which results in fallacious statistical analysis. This article studies three prevalent fallacies in group MCDM along with solutions based on compositional data analysis to avoid misusing statistical operations. First, we use a compositional approach to aggregate the priorities of a group of DMs and show that the outcome of the compositional analysis is identical to the normalized geometric mean, meaning that the arithmetic mean should be avoided. Furthermore, a new aggregation method is developed, which is a robust surrogate for the geometric mean. We also discuss the errors in computing measures of dispersion, including standard deviation and distance functions. D
    
[^20]: 基于领域区域的机器学习性能鲁棒性对协变量偏移的评估

    A Domain-Region Based Evaluation of ML Performance Robustness to Covariate Shift. (arXiv:2304.08855v1 [cs.LG])

    [http://arxiv.org/abs/2304.08855](http://arxiv.org/abs/2304.08855)

    本文实验评估了传统机器学习模型在协变量转移存在的情况下的性能。根据实验分析，随机森林是一种对协变量移位较不敏感的模型。

    

    大多数机器学习方法都假设训练和测试阶段的输入数据分布相同。然而，在实践中，这种稳定性通常不能满足，导致了所学模型在部署时出现了意外的表现。当训练和测试数据输入的概率分布不同，但输入输出关系保持不变时，这个问题被称为协变量转移。本文实验评估了传统机器学习模型在协变量转移存在的情况下的性能。此外，通过对输入数据的概率密度函数的域进行分解，进行了基于区域的评估，以评估分类器在每个域区域的性能。在一个二维分类问题中模拟了分布变化，随后进行了更高维度的四维实验。根据实验分析，随机森林是一种对协变量移位较不敏感的模型。

    Most machine learning methods assume that the input data distribution is the same in the training and testing phases. However, in practice, this stationarity is usually not met and the distribution of inputs differs, leading to unexpected performance of the learned model in deployment. The issue in which the training and test data inputs follow different probability distributions while the input-output relationship remains unchanged is referred to as covariate shift. In this paper, the performance of conventional machine learning models was experimentally evaluated in the presence of covariate shift. Furthermore, a region-based evaluation was performed by decomposing the domain of probability density function of the input data to assess the classifier's performance per domain region. Distributional changes were simulated in a two-dimensional classification problem. Subsequently, a higher four-dimensional experiments were conducted. Based on the experimental analysis, the Random Forests
    
[^21]: UDTIRI:一个开源的道路坑洞检测基准套件

    UDTIRI: An Open-Source Road Pothole Detection Benchmark Suite. (arXiv:2304.08842v1 [cs.CV])

    [http://arxiv.org/abs/2304.08842](http://arxiv.org/abs/2304.08842)

    该论文介绍了一个开源的道路坑洞检测基准套件UDTIRI，包含了标记齐全的1000张道路坑洞图像，可以用于深度学习方法在城市道路检查中的目标检测、语义分割和实例分割任务。

    

    看到在城市数字孪生领域中利用强大的深度学习方法的巨大潜力。特别是在智能道路检查领域，目前研究和数据有限。为了促进这一领域的进展，我们开发了一个名为Urban Digital Twins Intelligent Road Inspection (UDTIRI)数据集的标记齐全的道路坑洞数据集。我们希望这个数据集能够让强大的深度学习方法在城市道路检查中发挥作用，让算法更全面地理解场景并最大化其潜力。我们的数据集包括1000张道路坑洞图像，拍摄于不同的情境中，具有不同的光照和湿度条件。我们的意图是将这个数据集应用于目标检测、语义分割和实例分割任务。我们的团队花费了大量精力进行了详细的统计分析，并对UDTIRI数据集的一些代表性深度学习模型进行了基准测试。

    It is seen that there is enormous potential to leverage powerful deep learning methods in the emerging field of urban digital twins. It is particularly in the area of intelligent road inspection where there is currently limited research and data available. To facilitate progress in this field, we have developed a well-labeled road pothole dataset named Urban Digital Twins Intelligent Road Inspection (UDTIRI) dataset. We hope this dataset will enable the use of powerful deep learning methods in urban road inspection, providing algorithms with a more comprehensive understanding of the scene and maximizing their potential. Our dataset comprises 1000 images of potholes, captured in various scenarios with different lighting and humidity conditions. Our intention is to employ this dataset for object detection, semantic segmentation, and instance segmentation tasks. Our team has devoted significant effort to conducting a detailed statistical analysis, and benchmarking a selection of represent
    
[^22]: 关于AI辅助决策中依赖行为与准确性的相互关系

    On the Interdependence of Reliance Behavior and Accuracy in AI-Assisted Decision-Making. (arXiv:2304.08804v1 [cs.HC])

    [http://arxiv.org/abs/2304.08804](http://arxiv.org/abs/2304.08804)

    该论文分析了AI辅助决策中依赖行为和准确性之间的相互关系，并提出了一个视觉框架来更好地理解这种关系。该框架揭示了当人类在决策中过度依赖AI时，改善信任可能会降低准确性的有趣属性。

    

    在AI辅助决策中，将人类置于决策环路中央的主要承诺是，他们应该能够通过符合其正确的和覆盖其错误的建议来补充AI系统。然而实践中，我们经常看到人类倾向于过度或不足地依赖AI建议，这意味着他们要么依从错误的建议，要么覆盖正确的建议。这种依赖行为对决策准确性有害。在这项工作中，我们阐述并分析了在AI辅助决策中依赖行为和准确性之间的相互关系，这在以前的工作中很大程度上被忽视了。我们还提出了一个视觉框架，使这种相互关系更加具体化。该框架帮助我们解释和比较实证研究结果，并获得对AI辅助决策干预（例如解释）影响的细致理解。最后，我们从框架中推出了几个有趣的属性：（i）当人类不足地依赖AI建议时，改善信任将显着提高准确性，但在他们过度依赖时，信任的改善却可能降低准确性。

    In AI-assisted decision-making, a central promise of putting a human in the loop is that they should be able to complement the AI system by adhering to its correct and overriding its mistaken recommendations. In practice, however, we often see that humans tend to over- or under-rely on AI recommendations, meaning that they either adhere to wrong or override correct recommendations. Such reliance behavior is detrimental to decision-making accuracy. In this work, we articulate and analyze the interdependence between reliance behavior and accuracy in AI-assisted decision-making, which has been largely neglected in prior work. We also propose a visual framework to make this interdependence more tangible. This framework helps us interpret and compare empirical findings, as well as obtain a nuanced understanding of the effects of interventions (e.g., explanations) in AI-assisted decision-making. Finally, we infer several interesting properties from the framework: (i) when humans under-rely o
    
[^23]: 多方会话中的发言人个人特征分析

    Speaker Profiling in Multiparty Conversations. (arXiv:2304.08801v1 [cs.CL])

    [http://arxiv.org/abs/2304.08801](http://arxiv.org/abs/2304.08801)

    本文提出了一个名为SPC的任务，旨在为对话中每个发言者生成个人特征摘要。任务被分为三个子任务：个人特征发现、个人特征类型识别和个人特征价值提取。任务对于银行、酒店预订和航空预订等行业中的聊天机器人非常重要，可以使聊天机器人更好地了解和回应每个发言人的需求。

    

    在对话环境中，个体展现出独特的行为，使得“一刀切”的方法不足以为对话代理生成回应。虽然过去的研究旨在使用发言人个人信息创建个性化对话代理，但它们依赖于前提，即发言人个人特征已经被提供。然而，在像银行、酒店预订和航空预订等行业中使用的聊天机器人方面，这一假设并不总是正确的。本文旨在通过探索对话中的发言人个人特征分析 (SPC)任务来填补这个空白。SPC的主要目标是为对话中每个发言人产生个人特征摘要。为了实现这一目标，我们将任务分为三个子任务：个人特征发现、个人特征类型识别和个人特征价值提取。在给定对话的情况下，第一个子任务旨在识别包含个人信息的所有话语。

    In conversational settings, individuals exhibit unique behaviors, rendering a one-size-fits-all approach insufficient for generating responses by dialogue agents. Although past studies have aimed to create personalized dialogue agents using speaker persona information, they have relied on the assumption that the speaker's persona is already provided. However, this assumption is not always valid, especially when it comes to chatbots utilized in industries like banking, hotel reservations, and airline bookings. This research paper aims to fill this gap by exploring the task of Speaker Profiling in Conversations (SPC). The primary objective of SPC is to produce a summary of persona characteristics for each individual speaker present in a dialogue. To accomplish this, we have divided the task into three subtasks: persona discovery, persona-type identification, and persona-value extraction. Given a dialogue, the first subtask aims to identify all utterances that contain persona information.
    
[^24]: 无监督三维动作表示学习：基于骨架云着色的自监督学习方法

    Self-Supervised 3D Action Representation Learning with Skeleton Cloud Colorization. (arXiv:2304.08799v1 [cs.CV])

    [http://arxiv.org/abs/2304.08799](http://arxiv.org/abs/2304.08799)

    本文提出了一种基于自监督学习和骨架云着色技术的无监督三维动作表示学习方法，可以在未标注数据上进行空间和时间表示学习，实验结果表明，在三个基准数据集上实现了最先进的无监督骨架动作识别性能。

    

    近年来，基于三维骨架的人体动作识别逐渐受到人们的关注。然而，现有的方法往往需要大量的标注数据，标注成本高且耗时。本文提出了一种基于自监督学习的方法进行三维动作表示学习，利用骨架云着色技术对未标注的骨架数据进行空间和时间表示学习。实验结果表明，该方法在三个基准数据集上实现了最先进的无监督骨架动作识别性能。

    3D Skeleton-based human action recognition has attracted increasing attention in recent years. Most of the existing work focuses on supervised learning which requires a large number of labeled action sequences that are often expensive and time-consuming to annotate. In this paper, we address self-supervised 3D action representation learning for skeleton-based action recognition. We investigate self-supervised representation learning and design a novel skeleton cloud colorization technique that is capable of learning spatial and temporal skeleton representations from unlabeled skeleton sequence data. We represent a skeleton action sequence as a 3D skeleton cloud and colorize each point in the cloud according to its temporal and spatial orders in the original (unannotated) skeleton sequence. Leveraging the colorized skeleton point cloud, we design an auto-encoder framework that can learn spatial-temporal features from the artificial color labels of skeleton joints effectively. Specifical
    
[^25]: AI能力系统中用户信任的系统性文献综述：基于人机交互的角度

    A Systematic Literature Review of User Trust in AI-Enabled Systems: An HCI Perspective. (arXiv:2304.08795v1 [cs.HC])

    [http://arxiv.org/abs/2304.08795](http://arxiv.org/abs/2304.08795)

    本文系统地综述了23项实证研究关于用户信任定义、影响因素和测量方法的结果。结果表明，选择最适合特定情境中描述用户信任定义应该是重点，而不是比较定义。用户对AI能力系统的信任受到三个主要主题的影响，即社会伦理考虑、技术和设计特征、以及用户特征。

    

    用户对人工智能（AI）能力系统的信任日益被认为是促进采用的关键因素。已经有人建议，AI能力系统必须超越技术中心的方法，转向接纳更加人性化的方法，这是人机交互（HCI）领域的核心原则。本文旨在提供23项实证研究中关于用户信任定义、影响因素和测量方法的综述，为未来技术和设计战略、研究和计划提供洞察。研究结果确认存在多种方式定义信任。选择最适合描述特定情境中用户对信任的定义应该是重点，而不是比较定义。发现用户对AI能力系统的信任受到三个主要主题的影响，即社会伦理考虑、技术和设计特征、以及用户特征。

    User trust in Artificial Intelligence (AI) enabled systems has been increasingly recognized and proven as a key element to fostering adoption. It has been suggested that AI-enabled systems must go beyond technical-centric approaches and towards embracing a more human centric approach, a core principle of the human-computer interaction (HCI) field. This review aims to provide an overview of the user trust definitions, influencing factors, and measurement methods from 23 empirical studies to gather insight for future technical and design strategies, research, and initiatives to calibrate the user AI relationship. The findings confirm that there is more than one way to define trust. Selecting the most appropriate trust definition to depict user trust in a specific context should be the focus instead of comparing definitions. User trust in AI-enabled systems is found to be influenced by three main themes, namely socio-ethical considerations, technical and design features, and user characte
    
[^26]: 具有机会约束问题的三目标帕累托优化

    3-Objective Pareto Optimization for Problems with Chance Constraints. (arXiv:2304.08774v1 [cs.NE])

    [http://arxiv.org/abs/2304.08774](http://arxiv.org/abs/2304.08774)

    本文研究了具有机会约束的问题中的三目标帕累托优化，该模型可以通过使用1位翻转来计算所有需要的权衡，并在机会约束的支配集问题中展示了其效益。

    

    进化多目标算法在帕累托优化中已经成功地把一个给定的约束条件放宽成一个附加目标。本文探讨了在具有机会约束的问题中使用三目标公式的方法。我们的公式权衡了随机成分的期望成本、方差以及给定的确定性约束。我们指出了这个三目标公式与最近研究的具有正态分布的随机成分的机会约束双目标公式相比的优点。我们的分析表明，当处理确定性基数约束时，三目标公式只需要使用1位翻转就可以计算出所有需要的权衡。此外，我们对机会约束的支配集问题进行了实验研究，并展示了对这个经典的NP-hard问题的效益。

    Evolutionary multi-objective algorithms have successfully been used in the context of Pareto optimization where a given constraint is relaxed into an additional objective. In this paper, we explore the use of 3-objective formulations for problems with chance constraints. Our formulation trades off the expected cost and variance of the stochastic component as well as the given deterministic constraint. We point out benefits that this 3-objective formulation has compared to a bi-objective one recently investigated for chance constraints with Normally distributed stochastic components. Our analysis shows that the 3-objective formulation allows to compute all required trade-offs using 1-bit flips only, when dealing with a deterministic cardinality constraint. Furthermore, we carry out experimental investigations for the chance constrained dominating set problem and show the benefit for this classical NP-hard problem.
    
[^27]: 基于掩码语言模型的文本对抗样本检测

    Masked Language Model Based Textual Adversarial Example Detection. (arXiv:2304.08767v1 [cs.CR])

    [http://arxiv.org/abs/2304.08767](http://arxiv.org/abs/2304.08767)

    通过探索掩码语言模型引起的流形变化，我们提出了一种插即用的文本对抗例子检测方法，可以在保持对分类任务、模型结构和数据集无依赖的前提下，有效地检测到对抗例子。

    

    对抗攻击是机器学习模型在关键安全应用中可靠部署的严重威胁，稍微修改输入即可误导当前模型进行错误预测。最近，大量研究表明，对抗样本往往偏离正常样本的基础数据流形，而预训练的掩码语言模型可以适应正常的NLP数据流形。为了探索如何将掩码语言模型用于对抗性检测，我们提出了一种新颖的文本对抗例子检测方法，即基于掩码语言模型的检测（MLMD），它可以通过探索掩码语言模型引起的流形变化，在正常样本和对抗样本之间产生明显可区分的信号。MLMD具有即插即用的使用方法（即无需重新训练受害模型）用于对抗性防御，而且不受分类任务、受害模型结构和待防御的数据集的影响。

    Adversarial attacks are a serious threat to the reliable deployment of machine learning models in safety-critical applications. They can misguide current models to predict incorrectly by slightly modifying the inputs. Recently, substantial work has shown that adversarial examples tend to deviate from the underlying data manifold of normal examples, whereas pre-trained masked language models can fit the manifold of normal NLP data. To explore how to use the masked language model in adversarial detection, we propose a novel textual adversarial example detection method, namely Masked Language Model-based Detection (MLMD), which can produce clearly distinguishable signals between normal examples and adversarial examples by exploring the changes in manifolds induced by the masked language model. MLMD features a plug and play usage (i.e., no need to retrain the victim model) for adversarial defense and it is agnostic to classification tasks, victim model's architectures, and to-be-defended a
    
[^28]: 智能外骨骼：基于元认知代理的打击信息误导策略探索

    Exoskeleton for the Mind: Exploring Strategies Against Misinformation with a Metacognitive Agent. (arXiv:2304.08759v1 [cs.HC])

    [http://arxiv.org/abs/2304.08759](http://arxiv.org/abs/2304.08759)

    本研究基于元认知理论提出了一种反信息误导代理，在Twitter环境中进行了试点和实验研究，发现单一策略无法完全解决问题，并提出了对透明度和用户参与的重视。

    

    在现代社交媒体平台上，信息误导是一个全球性问题，很少有已知有效的解决方案。社交媒体平台提供了提高信息意识的工具，但这些都是封闭的系统，尚未经过实证评估。其他人开发了新颖的工具和策略，但大多数是在静态刺激、研究员提示或低保真原型的背景下研究的。我们提供了一种基于元认知理论评估于Twitter内部的反信息误导代理。我们报告了一项试点研究（n=17）和一项多部分实验研究（n=57，n=49），其中参与者体验了三个版本的代理，每个版本都应用了不同的策略。我们发现，没有一种单一的策略优于对照组。我们还确认了透明度和清晰度在代理的底层逻辑上的必要性，以及对于反复接触误导信息和缺乏用户参与的担忧。

    Misinformation is a global problem in modern social media platforms with few solutions known to be effective. Social media platforms have offered tools to raise awareness of information, but these are closed systems that have not been empirically evaluated. Others have developed novel tools and strategies, but most have been studied out of context using static stimuli, researcher prompts, or low fidelity prototypes. We offer a new anti-misinformation agent grounded in theories of metacognition that was evaluated within Twitter. We report on a pilot study (n=17) and multi-part experimental study (n=57, n=49) where participants experienced three versions of the agent, each deploying a different strategy. We found that no single strategy was superior over the control. We also confirmed the necessity of transparency and clarity about the agent's underlying logic, as well as concerns about repeated exposure to misinformation and lack of user engagement.
    
[^29]: W-MAE：具有遮蔽自编码器的预训练天气模型，用于多变量天气预测

    W-MAE: Pre-trained weather model with masked autoencoder for multi-variable weather forecasting. (arXiv:2304.08754v1 [cs.LG])

    [http://arxiv.org/abs/2304.08754](http://arxiv.org/abs/2304.08754)

    本文介绍了一种名为 W-MAE 的预训练天气模型，它使用遮蔽自编码器重建气象变量之间的空间相关性，并通过微调预测气象变量的未来状态，从而对天气数据中存在的时空依赖关系进行建模。

    

    天气预测是具有直接社会和经济影响的长期计算挑战。该任务涉及大量的连续数据收集，并在长时间内表现出丰富的时空依赖性，因此非常适合深度学习模型。本文将预训练技术应用于天气预测，并提出了一种用于多变量天气预测的具有遮蔽自编码器预训练的天气模型W-MAE。W-MAE以自监督的方式进行预训练，以重建气象变量之间的空间相关性。在时间尺度上，我们微调预训练的W-MAE以预测气象变量的未来状态，从而对天气数据中存在的时间依赖关系进行建模。我们使用每六小时选择一次样本，仅使用两年的ERA5数据，对W-MAE进行预训练。在相同的训练数据条件下，我们将W-MAE与FourCastNet进行比较。

    Weather forecasting is a long-standing computational challenge with direct societal and economic impacts. This task involves a large amount of continuous data collection and exhibits rich spatiotemporal dependencies over long periods, making it highly suitable for deep learning models. In this paper, we apply pre-training techniques to weather forecasting and propose W-MAE, a Weather model with Masked AutoEncoder pre-training for multi-variable weather forecasting. W-MAE is pre-trained in a self-supervised manner to reconstruct spatial correlations within meteorological variables. On the temporal scale, we fine-tune the pre-trained W-MAE to predict the future states of meteorological variables, thereby modeling the temporal dependencies present in weather data. We pre-train W-MAE using the fifth-generation ECMWF Reanalysis (ERA5) data, with samples selected every six hours and using only two years of data. Under the same training data conditions, we compare W-MAE with FourCastNet, and 
    
[^30]: 行为检索：通过查询未标记数据集实现少样本模仿学习

    Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets. (arXiv:2304.08742v1 [cs.RO])

    [http://arxiv.org/abs/2304.08742](http://arxiv.org/abs/2304.08742)

    本论文提出了一种简单的方法，利用少量下游专家数据从离线未标记数据集中选择性地查询相关行为（包括许多次优行为），实现了行为检索，进而实现了少样本学习。

    

    在数据效率方面使机器人学习新的视觉动作技能仍然是一个难题，有许多挑战。解决这个问题的一种流行范式是利用大型未标记的数据集，其中包含许多行为，然后使用少量任务特定的人类监督（即介入或演示）来适应特定任务的策略。但是，如何最好地利用狭窄的任务特定监督并将其与离线数据平衡仍然是一个待解决的问题。我们的关键洞察力在于任务特定数据不仅为代理提供了新的训练数据，还可以为代理的学习提供有关先前数据类型的信息。具体来说，我们提出了一种简单的方法，利用少量下游专家数据从离线未标记数据集中选择性地查询相关行为（包括许多次优行为）。然后代理被联合训练在专家和查询数据上。我们观察到，我们的

    Enabling robots to learn novel visuomotor skills in a data-efficient manner remains an unsolved problem with myriad challenges. A popular paradigm for tackling this problem is through leveraging large unlabeled datasets that have many behaviors in them and then adapting a policy to a specific task using a small amount of task-specific human supervision (i.e. interventions or demonstrations). However, how best to leverage the narrow task-specific supervision and balance it with offline data remains an open question. Our key insight in this work is that task-specific data not only provides new data for an agent to train on but can also inform the type of prior data the agent should use for learning. Concretely, we propose a simple approach that uses a small amount of downstream expert data to selectively query relevant behaviors from an offline, unlabeled dataset (including many sub-optimal behaviors). The agent is then jointly trained on the expert and queried data. We observe that our 
    
[^31]: 基于GNN的SAT求解中的变量依赖问题研究

    Addressing Variable Dependency in GNN-based SAT Solving. (arXiv:2304.08738v1 [cs.AI])

    [http://arxiv.org/abs/2304.08738](http://arxiv.org/abs/2304.08738)

    该论文提出了一种名为AsymSAT的GNN-based SAT求解器，通过解决变量依赖问题，提高了该方法的求解能力，在大型测试集上解决的SAT实例数有所提高。

    

    布尔可满足性问题（SAT）在许多应用中至关重要。现有的研究使用图神经网络（GNN）进行（近似）SAT求解。典型的GNN-based SAT求解器并行预测SAT解。我们表明，在对称SAT问题组中，由于忽略了SAT问题中布尔变量之间的依赖关系，因此并行预测保证会产生错误的答案。我们提出AsymSAT，一种基于GNN的体系结构，该结构集成了递归神经网络，以生成变量分配的依赖预测。实验结果表明，依赖变量预测扩展了GNN-based方法的求解能力，因为它提高了大型测试集上解决的SAT实例数。

    Boolean satisfiability problem (SAT) is fundamental to many applications. Existing works have used graph neural networks (GNNs) for (approximate) SAT solving. Typical GNN-based end-to-end SAT solvers predict SAT solutions concurrently. We show that for a group of symmetric SAT problems, the concurrent prediction is guaranteed to produce a wrong answer because it neglects the dependency among Boolean variables in SAT problems. % We propose AsymSAT, a GNN-based architecture which integrates recurrent neural networks to generate dependent predictions for variable assignments. The experiment results show that dependent variable prediction extends the solving capability of the GNN-based method as it improves the number of solved SAT instances on large test sets.
    
[^32]: 人类和机器有相同的眼睛吗？基于图像分类的人机感知差异研究

    Do humans and machines have the same eyes? Human-machine perceptual differences on image classification. (arXiv:2304.08733v1 [cs.CV])

    [http://arxiv.org/abs/2304.08733](http://arxiv.org/abs/2304.08733)

    本文研究通过图像分类探究了人机感知差异，发现即使准确率相似，人类和机器的答案分布也可能不同，并提出了一种后期人机合作来提高任务表现。

    

    训练良好的计算机视觉模型通常通过模仿从训练标签中学到的人类行为来解决视觉任务。近期视觉研究的大部分努力集中在使用标准化基准来测量模型任务性能。然而，了解人与机器之间的感知差异方面的工作还很有限。为了填补这一空白，我们的研究首先量化并分析了两种来源错误的统计分布。然后我们通过难度级别对任务进行排序，探讨人类与机器专业知识的差异。即使人类和机器的整体准确性相似，答案的分布也可能会有所不同。利用人类和机器之间的感知差异，我们通过实证研究表明了一种后期人机合作，其表现比单独的人或机器更好。

    Trained computer vision models are assumed to solve vision tasks by imitating human behavior learned from training labels. Most efforts in recent vision research focus on measuring the model task performance using standardized benchmarks. Limited work has been done to understand the perceptual difference between humans and machines. To fill this gap, our study first quantifies and analyzes the statistical distributions of mistakes from the two sources. We then explore human vs. machine expertise after ranking tasks by difficulty levels. Even when humans and machines have similar overall accuracies, the distribution of answers may vary. Leveraging the perceptual difference between humans and machines, we empirically demonstrate a post-hoc human-machine collaboration that outperforms humans or machines alone.
    
[^33]: LTC-SE: 扩展液态时常神经网络在可扩展人工智能和嵌入式系统中的潜力

    LTC-SE: Expanding the Potential of Liquid Time-Constant Neural Networks for Scalable AI and Embedded Systems. (arXiv:2304.08691v1 [cs.LG])

    [http://arxiv.org/abs/2304.08691](http://arxiv.org/abs/2304.08691)

    LTC-SE是一种液态时常神经网络算法，将多种神经元模型统一，其增强版专注于灵活性、兼容性和代码组织，满足嵌入式系统的性能要求，扩展了液态神经网络在可扩展人工智能和嵌入式系统中的适用性。

    

    我们提出了LTC-SE，这是Hasani等人于2021年最初提出的液态时常神经网络算法的改进版本。该算法将漏电积分-火神经元模型与连续时间递归神经网络（CTRNN）、神经常微分方程（NODE）和量身定制的门控循环单元（GRU）统一起来。LTC-SE的增强版专注于增强灵活性、兼容性和代码组织，以满足具有有限计算资源和严格性能要求的嵌入式系统的独特约束。更新后的代码是一个与TensorFlow 2.x兼容的综合类库，为LTCCell、CTRNN、NODE和CTGRU类提供了全面的配置选项。我们通过对比以往的版本，展示了我们优化在用户体验、Keras函数兼容性和代码清晰度方面的优势，这些改进扩展了液态神经网络在可扩展人工智能和嵌入式系统中的适用性。

    We present LTC-SE, an improved version of the Liquid Time-Constant (LTC) neural network algorithm originally proposed by Hasani et al. in 2021. This algorithm unifies the Leaky-Integrate-and-Fire (LIF) spiking neural network model with Continuous-Time Recurrent Neural Networks (CTRNNs), Neural Ordinary Differential Equations (NODEs), and bespoke Gated Recurrent Units (GRUs). The enhancements in LTC-SE focus on augmenting flexibility, compatibility, and code organization, targeting the unique constraints of embedded systems with limited computational resources and strict performance requirements. The updated code serves as a consolidated class library compatible with TensorFlow 2.x, offering comprehensive configuration options for LTCCell, CTRNN, NODE, and CTGRU classes. We evaluate LTC-SE against its predecessors, showcasing the advantages of our optimizations in user experience, Keras function compatibility, and code clarity. These refinements expand the applicability of liquid neural
    
[^34]: 一种端到端的、交互式的基于深度学习的手写英文文本注释系统

    An end-to-end, interactive Deep Learning based Annotation system for cursive and print English handwritten text. (arXiv:2304.08670v1 [cs.CV])

    [http://arxiv.org/abs/2304.08670](http://arxiv.org/abs/2304.08670)

    本文提出了一个端到端、交互式的手写英文文本注释系统，解决了手写文本数据稀缺的问题，并能够有效提高手写文本识别模型的识别准确率。

    

    随着人们越来越倾向于使用计算设备和数字媒介进行任务，将以前手动完成的任务转换为数字化版本的任何方法都会受到欢迎。尽管今天可以在线完成许多文档任务，但仍有许多应用和领域无法避免手写文本，这使手写文档的数字化成为一项非常重要的任务。过去几十年来，离线手写文本识别得到了广泛的研究。最近，大部分的尝试已经转向了基于机器学习和深度学习的方法。为了设计更复杂和更深入的网络，并保证出色的性能，有更多的注释数据是必不可少的。今天用于离线手写文本识别的大部分数据库都是手动或半自动注释的。这些数据库通常规模较小，无法满足当前基于深度学习的方法的要求。在本文中，我们提出了一种端到端的、交互式的注释系统，以解决手写文本数据的稀缺问题。我们的系统包括三个模块：数据管理模块、注释模块和模型训练模块。注释模块用户友好，允许交互式注释草写和印刷英语手写文本。所提出的系统还可以处理大量数据，具有合理的注释时间。实验结果表明，该系统可以有效地提高手写文本识别模型的识别准确率。

    With the surging inclination towards carrying out tasks on computational devices and digital mediums, any method that converts a task that was previously carried out manually, to a digitized version, is always welcome. Irrespective of the various documentation tasks that can be done online today, there are still many applications and domains where handwritten text is inevitable, which makes the digitization of handwritten documents a very essential task. Over the past decades, there has been extensive research on offline handwritten text recognition. In the recent past, most of these attempts have shifted to Machine learning and Deep learning based approaches. In order to design more complex and deeper networks, and ensure stellar performances, it is essential to have larger quantities of annotated data. Most of the databases present for offline handwritten text recognition today, have either been manually annotated or semi automatically annotated with a lot of manual involvement. Thes
    
[^35]: Insta（nt）宠物疗法：GAN生成的图像用于治疗性社交媒体内容

    Insta(nt) Pet Therapy: GAN-generated Images for Therapeutic Social Media Content. (arXiv:2304.08665v1 [cs.CV])

    [http://arxiv.org/abs/2304.08665](http://arxiv.org/abs/2304.08665)

    本研究使用GAN生成大量虚假宠物图像，上传到社交媒体账户达到了与传统宠物图像账户相同的用户参与度，为宠物疗法社交媒体内容的发展提供了新思路。

    

    在线查看宠物图像对人的治疗效果已得到充分研究。然而，由于依赖于宠物主人拍摄照片并上传，因此很难获得大规模的这种内容生产。本文使用基于生成对抗网络的框架创建大规模的虚假宠物图像。这些图像上传到Instagram账户，使用户参与度与传统宠物照片账户中的图片一样高，强调该框架可用于宠物疗法社交媒体内容。

    The positive therapeutic effect of viewing pet images online has been well-studied. However, it is difficult to obtain large-scale production of such content since it relies on pet owners to capture photographs and upload them. I use a Generative Adversarial Network-based framework for the creation of fake pet images at scale. These images are uploaded on an Instagram account where they drive user engagement at levels comparable to those seen with images from accounts with traditional pet photographs, underlining the applicability of the framework to be used for pet-therapy social media content.
    
[^36]: 学习动作剩余值的连续多功能跳跃

    Continuous Versatile Jumping Using Learned Action Residuals. (arXiv:2304.08663v1 [cs.RO])

    [http://arxiv.org/abs/2304.08663](http://arxiv.org/abs/2304.08663)

    本文提出了一个层次化框架，将最优控制和强化学习结合起来，为四足机器人学习连续跳跃动作。通过学习动作剩余值，在模拟和真实环境中实现了多功能、连续的跳跃动作。

    

    跳跃对于腿式机器人通过困难地形至关重要。在本文中，我们提出了一个层次化框架，结合最优控制和强化学习，为四足机器人学习连续跳跃动作。我们的框架的核心是一个姿态控制器，它将手动设计的加速度控制器与学习到的剩余策略结合在一起。由于加速度控制器为高效训练warm start策略，所以经过训练的策略克服了加速度控制器的局限并提高了跳跃稳定性。此外，一个低层全身控制器将姿势控制器的身体姿势命令转换成电机命令。经过在模拟环境下的训练后，我们的框架可以直接部署到真实机器人上，执行多功能的连续跳跃动作，包括高达50cm、向前60cm的全向跳跃和高达90度的跳跃转向。请访问我们的网站以获取更多结果:https://sites.google.com/view/jumping-rl/home

    Jumping is essential for legged robots to traverse through difficult terrains. In this work, we propose a hierarchical framework that combines optimal control and reinforcement learning to learn continuous jumping motions for quadrupedal robots. The core of our framework is a stance controller, which combines a manually designed acceleration controller with a learned residual policy. As the acceleration controller warm starts policy for efficient training, the trained policy overcomes the limitation of the acceleration controller and improves the jumping stability. In addition, a low-level whole-body controller converts the body pose command from the stance controller to motor commands. After training in simulation, our framework can be deployed directly to the real robot, and perform versatile, continuous jumping motions, including omni-directional jumps at up to 50cm high, 60cm forward, and jump-turning at up to 90 degrees. Please visit our website for more results: https://sites.goo
    
[^37]: (LC)$^2$:基于LiDAR-Camera循环约束的跨模态地点识别算法

    (LC)$^2$: LiDAR-Camera Loop Constraints For Cross-Modal Place Recognition. (arXiv:2304.08660v1 [cs.RO])

    [http://arxiv.org/abs/2304.08660](http://arxiv.org/abs/2304.08660)

    提出了一种新的(LC)$^2$方法，通过将2D图像和3D点云数据转换为2.5D深度图像进行匹配，实现了没有先验点云地图的LiDAR定位。

    

    定位一直是自主导航的一个难题，地点识别和重新定位对于机器人的实现至关重要。因此，深度学习被广泛研究用于测量数据到定位描述符的一致转换。街景图像容易获取，但容易受到外观变化的影响。LiDAR能够提供精确的结构信息，但构建点云数据库成本高，点云只在有限的地方存在。与以往的工作不同，我们将2D图像和3D点云数据转换为2.5D深度图像进行匹配，提出了一种新的跨数据匹配方法，称为(LC)$^2$，用于实现没有先验点云地图的LiDAR定位。为此，在匹配之前，将LiDAR测量表示为距离图像的形式。

    Localization has been a challenging task for autonomous navigation. A loop detection algorithm must overcome environmental changes for the place recognition and re-localization of robots. Therefore, deep learning has been extensively studied for the consistent transformation of measurements into localization descriptors. Street view images are easily accessible; however, images are vulnerable to appearance changes. LiDAR can robustly provide precise structural information. However, constructing a point cloud database is expensive, and point clouds exist only in limited places. Different from previous works that train networks to produce shared embedding directly between the 2D image and 3D point cloud, we transform both data into 2.5D depth images for matching. In this work, we propose a novel cross-matching method, called (LC)$^2$, for achieving LiDAR localization without a prior point cloud map. To this end, LiDAR measurements are expressed in the form of range images before matching
    
[^38]: 基于BERT的技术对美国最高法院案例进行分类

    Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v1 [cs.CL])

    [http://arxiv.org/abs/2304.08649](http://arxiv.org/abs/2304.08649)

    本文基于BERT技术探究了对美国最高法院案例进行分类的方法，比较了使用BERT模型与其他先进模型的准确性，最终在15个广泛类别上取得了80%的准确度，在279个细粒度类别上取得了60%的准确度。

    

    基于双向编码器表示来自变压器的模型（BERT）在许多自然语言处理（NLP）任务（如命名实体识别（NER），词性（POS）标记等）上产生了最新技术（SOTA）结果。当分类长文档（例如来自美国最高法院的文档）时，使用BERT模型可能比较困难。本文中，我们尝试了几种基于BERT的分类技术，用于对美国最高法院决定或最高法院数据库（SCDB）进行分类，并将其与先前的SOTA结果进行了比较。我们还将我们的结果与针对长文档的SOTA模型进行了比较。我们对两个分类任务进行了比较：（1）广泛的分类任务，具有15个类别；（2）细粒度的分类任务，具有279个类别。我们的最佳结果在15个广泛类别上产生80％的准确度，在279个细粒度类别上产生60％的准确度。

    Models based on bidirectional encoder representations from transformers (BERT) produce state of the art (SOTA) results on many natural language processing (NLP) tasks such as named entity recognition (NER), part-of-speech (POS) tagging etc. An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis. In this paper, we experiment with several BERT-based classification techniques for US supreme court decisions or supreme court database (SCDB) and compare them with the previous SOTA results. We then compare our results specifically with SOTA models for long documents. We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80\% on the 15 broad categories and 60\% on the fine-grained 279 categories 
    
[^39]: 大型语言模型输出的评估：话语和记忆

    An Evaluation on Large Language Model Outputs: Discourse and Memorization. (arXiv:2304.08637v1 [cs.CL])

    [http://arxiv.org/abs/2304.08637](http://arxiv.org/abs/2304.08637)

    评估了九个大语言模型的输出，发现其中80％包含记忆数据，但包含最多记忆内容的输出更可能是高质量的。提出了缓解策略以降低记忆文本率。

    

    我们对九个最广泛可用的大型语言模型（LLMs）生成的各种输出进行了经验性评估。我们使用现成的工具进行分析，发现在与输出病态（例如，反事实和逻辑上的错误陈述）以及不保持主题等方面的关系中，记忆文本百分比、独特文本百分比和整体输出质量之间存在相关性。总体而言，80.0％的输出包含记忆数据，但包含最多记忆内容的输出也更有可能被认为具有高质量。我们讨论和评估了缓解策略，并显示，在评估的模型中，输出的记忆文本率有所降低。最后，我们就学习、记忆和评估优质文本的潜在影响进行了讨论。

    We present an empirical evaluation of various outputs generated by nine of the most widely-available large language models (LLMs). Our analysis is done with off-the-shelf, readily-available tools. We find a correlation between percentage of memorized text, percentage of unique text, and overall output quality, when measured with respect to output pathologies such as counterfactual and logically-flawed statements, and general failures like not staying on topic. Overall, 80.0% of the outputs evaluated contained memorized data, but outputs containing the most memorized content were also more likely to be considered of high quality. We discuss and evaluate mitigation strategies, showing that, in the models evaluated, the rate of memorized text being output is reduced. We conclude with a discussion on potential implications around what it means to learn, to memorize, and to evaluate quality text.
    
[^40]: 离散与反向传播的桥梁：直通法与其它方法

    Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])

    [http://arxiv.org/abs/2304.08612](http://arxiv.org/abs/2304.08612)

    本文提出了一种新方法来逼近生成离散潜变量的参数的梯度，其中包括了一些数值方法，实现了二阶精度，取得了实验上的持续改进。

    

    反向传播是深度学习中的基石，但其仅限于计算连续变量的梯度，限制了涉及离散潜变量的问题的研究。针对这个问题，我们提出了一种新的方法来近似生成离散潜变量的参数的梯度。我们首先考察了广泛使用的 Straight-Through（ST）启发式方法，并证明它作为梯度的一阶近似值。在此基础上，我们提出了一种新的方法，称为 ReinMax，它集成了 Heun's Method，一种解ODE的二阶数值方法，以近似梯度。我们的方法实现了二阶精度，而不需要 Hessian 或其他二阶导数。我们进行了结构化输出预测和无监督生成建模任务的实验。我们的结果显示，\ours 在现有技术中带来了持续的改进，包括 ST 和 Straight-Through Gum。

    Backpropagation, the cornerstone of deep learning, is limited to computing gradients solely for continuous variables. This limitation hinders various research on problems involving discrete latent variables. To address this issue, we propose a novel approach for approximating the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose a novel method called ReinMax, which integrates Heun's Method, a second-order numerical method for solving ODEs, to approximate the gradient. Our method achieves second-order accuracy without requiring Hessian or other second-order derivatives. We conduct experiments on structured output prediction and unsupervised generative modeling tasks. Our results show that \ours brings consistent improvements over the state of the art, including ST and Straight-Through Gum
    
[^41]: 联邦学习与智能电网的交叉路径：概述，挑战和前景。

    Crossing Roads of Federated Learning and Smart Grids: Overview, Challenges, and Perspectives. (arXiv:2304.08602v1 [cs.LG])

    [http://arxiv.org/abs/2304.08602](http://arxiv.org/abs/2304.08602)

    本文探讨了在智能电网中应用联邦学习以保护消费者隐私和提高数据模型预测性能的优势和缺点，并提供了可能的数据分区、通信拓扑和安全机制分类方法。同时，本文总结了该技术面临的主要挑战和未来发展方向。

    

    能源数据的敏感性使得消费者的隐私成为智能电网（SGs）中的主要关注点，尤其是在训练机器学习模型以用于不同服务时。联邦学习可以将训练推向边缘，为隐私保护和模型预测性能之间提供了很好的折衷方案。本文概述了联邦学习在SGs中的应用，并讨论了其优缺点，主要包括负载预测，电动汽车，故障诊断，负载分解和可再生能源等。此外，还探讨了数据分区、通信拓扑和安全机制等方面的主要设计趋势和可能的分类方法。最后，本文总结了该技术面临的主要挑战和未来发展方向。

    Consumer's privacy is a main concern in Smart Grids (SGs) due to the sensitivity of energy data, particularly when used to train machine learning models for different services. These data-driven models often require huge amounts of data to achieve acceptable performance leading in most cases to risks of privacy leakage. By pushing the training to the edge, Federated Learning (FL) offers a good compromise between privacy preservation and the predictive performance of these models. The current paper presents an overview of FL applications in SGs while discussing their advantages and drawbacks, mainly in load forecasting, electric vehicles, fault diagnoses, load disaggregation and renewable energies. In addition, an analysis of main design trends and possible taxonomies is provided considering data partitioning, the communication topology, and security mechanisms. Towards the end, an overview of main challenges facing this technology and potential future directions is presented.
    
[^42]: PALF: 预注释和相机-LiDAR后期融合，为点云的易注释提供便利

    PALF: Pre-Annotation and Camera-LiDAR Late Fusion for the Easy Annotation of Point Clouds. (arXiv:2304.08591v1 [cs.CV])

    [http://arxiv.org/abs/2304.08591](http://arxiv.org/abs/2304.08591)

    本研究提出了一种使用预注释和相机-LiDAR后期融合的算法，以便于准确进行点云数据的注释。

    

    在自动驾驶领域中，3D物体检测变得不可或缺。深度学习算法在3D物体检测方面已取得了显著的突破，但是这些算法需要大量标注的点云数据进行训练和评估。与2D图像标签不同，点云数据的注释具有稀疏性、不规则性和低分辨率等困难，注释效率比2D图像低得多。因此，我们提出了一种点云数据注释算法，使用预注释和相机-LiDAR后期融合算法，以便于准确进行注释。本研究的贡献如下：我们提出了（1）一种预注释算法，利用3D物体检测和自适应拟合，以便于注释点云；（2）一种相机-LiDAR后期融合算法，利用2D和3D结果进行轻松的错误检查。

    3D object detection has become indispensable in the field of autonomous driving. To date, gratifying breakthroughs have been recorded in 3D object detection research, attributed to deep learning. However, deep learning algorithms are data-driven and require large amounts of annotated point cloud data for training and evaluation. Unlike 2D image labels, annotating point cloud data is difficult due to the limitations of sparsity, irregularity, and low resolution, which requires more manual work, and the annotation efficiency is much lower than 2D image.Therefore, we propose an annotation algorithm for point cloud data, which is pre-annotation and camera-LiDAR late fusion algorithm to easily and accurately annotate. The contributions of this study are as follows. We propose (1) a pre-annotation algorithm that employs 3D object detection and auto fitting for the easy annotation of point clouds, (2) a camera-LiDAR late fusion algorithm using 2D and 3D results for easily error checking, whic
    
[^43]: 创作迪斯科：音乐可视化的文本到视频生成

    Generative Disco: Text-to-Video Generation for Music Visualization. (arXiv:2304.08551v1 [cs.HC])

    [http://arxiv.org/abs/2304.08551](http://arxiv.org/abs/2304.08551)

    Generative Disco是一个生成式人工智能系统，利用大型语言模型和文本到图像模型帮助生成音乐可视化；用户通过定义开始和结束提示来参数化可视化，可生成反应音频的视频，引入了“过渡”和“保持”的设计模式，能够产生高度表现力并适用于专业人员。

    

    视觉是音乐体验的核心组成部分，因为它们可以放大音乐传达的情感和信息。然而，创造音乐可视化是一个复杂、耗时和资源密集的过程。我们介绍了Generative Disco，一个生成式人工智能系统，利用大型语言模型和文本到图像模型帮助生成音乐可视化。用户选择要可视化的音乐间隔，然后通过定义开始和结束提示来参数化该可视化。这些提示根据音乐的节奏进行变形和生成，以产生反应音频的视频。我们介绍了改进生成视频的设计模式：“过渡”，表达颜色、时间、主题或风格的变化，“保持”，鼓励视觉重点和一致性。专业人员参与的研究表明，该系统具有愉悦性、易于探索和高度表现力。我们总结了Generative Disco在专业人士中的应用案例以及AI生成内容与版权的联系。

    Visuals are a core part of our experience of music, owing to the way they can amplify the emotions and messages conveyed through the music. However, creating music visualization is a complex, time-consuming, and resource-intensive process. We introduce Generative Disco, a generative AI system that helps generate music visualizations with large language models and text-to-image models. Users select intervals of music to visualize and then parameterize that visualization by defining start and end prompts. These prompts are warped between and generated according to the beat of the music for audioreactive video. We introduce design patterns for improving generated videos: "transitions", which express shifts in color, time, subject, or style, and "holds", which encourage visual emphasis and consistency. A study with professionals showed that the system was enjoyable, easy to explore, and highly expressive. We conclude on use cases of Generative Disco for professionals and how AI-generated c
    
[^44]: 一种可扩展的序列转移优化问题生成器

    A Scalable Test Problem Generator for Sequential Transfer Optimization. (arXiv:2304.08503v1 [cs.NE])

    [http://arxiv.org/abs/2304.08503](http://arxiv.org/abs/2304.08503)

    STO中已有的测试问题设计不完善，难以代表真实问题多样化关系，限制了算法的表现。本文介绍了一种可扩展的序列转移优化问题生成器。

    

    近年来，序列转移优化(STO)受到越来越多的研究关注，旨在利用储存在数据库中以前求解的优化任务的知识来提高优化性能。然而，尽管算法设计已有重大进展，但STO中的测试问题设计并不完善。它们往往是由其他基准函数随机组合而成，这些基准函数具有相同的最佳值，或者生成自表现出有限变化的实际问题。这些问题中源任务和目标任务的最优解之间的关系是手动配置的，因此单调，限制了它们表征真实问题多样化关系的能力。因此，许多算法在这些问题上取得的有前途的结果具有高度的偏见，并且难以推广到其他问题。鉴于此，我们首先引入了一些表征STO问题的基本概念。

    Sequential transfer optimization (STO), which aims to improve optimization performance by exploiting knowledge captured from previously-solved optimization tasks stored in a database, has been gaining increasing research attention in recent years. However, despite significant advancements in algorithm design, the test problems in STO are not well designed. Oftentimes, they are either randomly assembled by other benchmark functions that have identical optima or are generated from practical problems that exhibit limited variations. The relationships between the optimal solutions of source and target tasks in these problems are manually configured and thus monotonous, limiting their ability to represent the diverse relationships of real-world problems. Consequently, the promising results achieved by many algorithms on these problems are highly biased and difficult to be generalized to other problems. In light of this, we first introduce a few rudimentary concepts for characterizing STO pr
    
[^45]: 排名损失和交错学习减少组织病理学图像搜索偏见

    Ranking Loss and Sequestering Learning for Reducing Image Search Bias in Histopathology. (arXiv:2304.08498v1 [eess.IV])

    [http://arxiv.org/abs/2304.08498](http://arxiv.org/abs/2304.08498)

    本文提出两个新颖的想法，分别采用排名损失函数和交错学习的方法，避免了分类误差和模型内部偏见，以提高图像搜索性能。

    

    近年来，深度学习在医疗保健应用中有了重要作用，包括数字病理学中的图像搜索。尽管计算机视觉取得了重大进展，但在病理学档案的图像搜索方面仍存在重大问题。一个众所周知的问题是AI偏见和缺乏泛化能力。一个更特别的深度模型缺点是对搜索功能的无知。前者影响每个模型，后者只影响搜索和匹配。由于缺乏基于排名的学习，研究人员必须基于分类误差来训练模型，然后使用所得的嵌入进行图像搜索。此外，即使使用各种医院的大型图像库，深度模型似乎也容易产生内部偏见。本文提出了两个新颖的想法以提高图像搜索性能。首先，我们使用排名损失函数来指导特征提取朝向搜索的匹配导向性。通过强制模型学习排名，我们可以避免分类误差带来的问题。其次，我们提出一种称为交错学习的方法，在扩大训练数据集的同时减少模型内部偏差。

    Recently, deep learning has started to play an essential role in healthcare applications, including image search in digital pathology. Despite the recent progress in computer vision, significant issues remain for image searching in histopathology archives. A well-known problem is AI bias and lack of generalization. A more particular shortcoming of deep models is the ignorance toward search functionality. The former affects every model, the latter only search and matching. Due to the lack of ranking-based learning, researchers must train models based on the classification error and then use the resultant embedding for image search purposes. Moreover, deep models appear to be prone to internal bias even if using a large image repository of various hospitals. This paper proposes two novel ideas to improve image search performance. First, we use a ranking loss function to guide feature extraction toward the matching-oriented nature of the search. By forcing the model to learn the ranking o
    
[^46]: 行程规划中的群体效用优化：一种策略性和众包意识方法

    Optimizing Group Utility in Itinerary Planning: A Strategic and Crowd-Aware Approach. (arXiv:2304.08495v1 [cs.AI])

    [http://arxiv.org/abs/2304.08495](http://arxiv.org/abs/2304.08495)

    本论文介绍了一种名为SCAIR的算法，可以优化群体效用，解决行程规划中的多个用户排队时间和人群水平优化的问题。

    

    行程推荐是一个具有许多实际应用的复杂的序列预测问题。当考虑到优化多个用户排队时间和人群水平时，这项任务变得更具挑战性，因为涉及到诸多参数，如景点受欢迎程度、排队时间、步行时间和营业时间等。现有的解决方案通常集中在单人视角上，未能解决自然人群行为引起的现实问题，如贪婪路由问题。本文介绍了一种名为“战略和众包意识行程推荐（SCAIR）”算法，该算法在现实环境中优化群体效用。我们将路线推荐策略建模为马尔可夫决策过程，并提出了一种状态编码机制，使得可以在线性时间内实现实时规划和分配。我们使用主题公园数据集对我们的算法进行各种竞争性和现实的基线测试，证明SCAIR优于其他算法。

    Itinerary recommendation is a complex sequence prediction problem with numerous real-world applications. This task becomes even more challenging when considering the optimization of multiple user queuing times and crowd levels, as well as numerous involved parameters, such as attraction popularity, queuing time, walking time, and operating hours. Existing solutions typically focus on single-person perspectives and fail to address real-world issues resulting from natural crowd behavior, like the Selfish Routing problem. In this paper, we introduce the Strategic and Crowd-Aware Itinerary Recommendation (SCAIR) algorithm, which optimizes group utility in real-world settings. We model the route recommendation strategy as a Markov Decision Process and propose a State Encoding mechanism that enables real-time planning and allocation in linear time. We evaluate our algorithm against various competitive and realistic baselines using a theme park dataset, demonstrating that SCAIR outperforms th
    
[^47]: 基于多智能体系统模型的智能家居环境

    Smart Home Environment Modelled with a Multi-Agent System. (arXiv:2304.08494v1 [cs.MA])

    [http://arxiv.org/abs/2304.08494](http://arxiv.org/abs/2304.08494)

    本文描述了一个智能家居上下文感知环境的原型，使用多智能体系统模拟，目的是通过预定义规则降低设备运行的操作成本，并监控居民的健康。

    

    智能家居是指通过自动化技术管理家用电器和系统，以帮助日常生活的居住地。本文描述了一个原型，该原型模拟了一个上下文感知环境，在设计的智能家居中进行了开发。智能家居环境使用三个智能体和五个位置在房子里进行了模拟。上下文感知智能体基于为日常活动设计的预定义规则进行行为。我们的建议旨在降低设备运行的操作成本。未来，居住在智能家居内的居民健康方面的监控将维持他们的日常健康生活。

    A smart home can be considered a place of residence that enables the management of appliances and systems to help with day-to-day life by automated technology. In the current paper is described a prototype that simulates a context-aware environment, developed in a designed smart home. The smart home environment has been simulated using three agents and five locations in a house. The context-aware agents behave based on predefined rules designed for daily activities. Our proposal aims to reduce operational cost of running devices. In the future, monitors of health aspects belonging to home residents will sustain their healthy life daily.
    
[^48]: 无人机群在自主移动接入应用中的协同多智能体强化学习

    Coordinated Multi-Agent Reinforcement Learning for Unmanned Aerial Vehicle Swarms in Autonomous Mobile Access Applications. (arXiv:2304.08493v1 [cs.MA])

    [http://arxiv.org/abs/2304.08493](http://arxiv.org/abs/2304.08493)

    本论文提出了一种集中式训练和分布式执行的多智能体深度强化学习方法，用于协调控制多个无人机在自主移动接入应用中，最大化服务质量。

    

    本文提出了一种新颖的基于集中式训练和分布式执行 (CTDE) 的多智能体深度强化学习 (MADRL) 方法，用于控制多个无人机在自主移动接入应用中。为此，单个神经网络在集中式训练中用于协作多个智能体，同时最大化移动接入应用中的总服务质量 (QoS)。

    This paper proposes a novel centralized training and distributed execution (CTDE)-based multi-agent deep reinforcement learning (MADRL) method for multiple unmanned aerial vehicles (UAVs) control in autonomous mobile access applications. For the purpose, a single neural network is utilized in centralized training for cooperation among multiple agents while maximizing the total quality of service (QoS) in mobile access applications.
    
[^49]: 中文开放式指令广义语言模型：初步发布

    Chinese Open Instruction Generalist: A Preliminary Release. (arXiv:2304.07987v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.07987](http://arxiv.org/abs/2304.07987)

    本论文旨在通过适应不同子任务的固有特性，创建一个中文指令数据集，以填补指令调整技术在中文语言领域的空白。

    

    指令调整被广泛认为是构建广义语言模型的关键技术，随着InstructGPT和ChatGPT的发布，它已经引起了研究人员和公众的关注。尽管英语为基础的大规模语言模型取得了令人瞩目的进展，但是还未探索英语为基础的语言模型在多语任务上是否可以像英语任务那样通过精心设计的指令调整来执行，以及我们如何构建所需的语料库进行调整。为填补这一空白，我们提出了一个项目，试图通过适应4个子任务的固有特性，采用各种方法创建一个中文指令数据集。我们收集了约20万个中文指令调整样本，并进行了人工检查以确保高质量。我们还总结了现有的英文和中文指令语料库，并对一些潜在的应用进行了简要描述。

    Instruction tuning is widely recognized as a key technique for building generalist language models, which has attracted the attention of researchers and the public with the release of InstructGPT~\citep{ouyang2022training} and ChatGPT\footnote{\url{https://chat.openai.com/}}. Despite impressive progress in English-oriented large-scale language models (LLMs), it is still under-explored whether English-based foundation LLMs can perform similarly on multilingual tasks compared to English tasks with well-designed instruction tuning and how we can construct the corpora needed for the tuning.  To remedy this gap, we propose the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks. We collect around 200k Chinese instruction tuning samples, which have been manually checked to guarantee high quality. We also summarize the existing English and Chinese instruction corpora and briefly describe some potential applic
    
[^50]: 一种数据中心的、基于Vision Transformer的非均质去雾解决方案。

    A Data-Centric Solution to NonHomogeneous Dehazing via Vision Transformer. (arXiv:2304.07874v1 [cs.CV])

    [http://arxiv.org/abs/2304.07874](http://arxiv.org/abs/2304.07874)

    本文提出了一种基于Vision Transformer的数据中心的解决方案，用于解决非均质去雾问题。传统方法在处理NH-HAZE23数据集等非均质雾图像时存在问题，因为它们无法满足建模均质雾所需的假设之一。同时，本文指出光靠数据增广并不能解决问题，因为需要处理分布差异。

    

    近年来，图像去雾引起了越来越多的关注。许多深度学习方法已经被提出来应对这一挑战，并在处理均质雾时取得了显著的成就。然而，这些解决方案无法在应用于存在非均质雾的图像时保持类似的性能，例如NTIRE挑战介绍的NH-HAZE23数据集。其中一个失败的原因是非均质雾不符合建模均质雾所需的假设之一。此外，传统的端到端训练方法需要对大量的非均质雾图像与其清晰对应项进行配对，而NH-HAZE23数据集的数量是有限的。尽管可以通过利用其他非均质去雾数据集扩充NH-HAZE23数据集，但我们观察到有必要设计一种适当的数据预处理方法以减少目标数据集之间的分布差距。

    Recent years have witnessed an increased interest in image dehazing. Many deep learning methods have been proposed to tackle this challenge, and have made significant accomplishments dealing with homogeneous haze. However, these solutions cannot maintain comparable performance when they are applied to images with non-homogeneous haze, e.g., NH-HAZE23 dataset introduced by NTIRE challenges. One of the reasons for such failures is that non-homogeneous haze does not obey one of the assumptions that is required for modeling homogeneous haze. In addition, a large number of pairs of non-homogeneous hazy image and the clean counterpart is required using traditional end-to-end training approaches, while NH-HAZE23 dataset is of limited quantities. Although it is possible to augment the NH-HAZE23 dataset by leveraging other non-homogeneous dehazing datasets, we observe that it is necessary to design a proper data-preprocessing approach that reduces the distribution gaps between the target datase
    
[^51]: 低资源语言的神经机器翻译

    Neural Machine Translation For Low Resource Languages. (arXiv:2304.07869v1 [cs.CL])

    [http://arxiv.org/abs/2304.07869](http://arxiv.org/abs/2304.07869)

    该论文研究了低资源语言的神经机器翻译，并构建了一个基于 \texttt{mBART.CC25} 语言模型的模型，利用后向翻译和迁移学习等 NLP 和深度学习技术进行增强，以达到最先进的结果。

    

    由于自然语言的内在复杂性和流动性，神经机器翻译是一个具有挑战性的任务。尽管近年来在几种语言对中取得了最先进的表现，但在多语言神经机器翻译 (MNMT) 领域看到了很多关注，却没有进行全面调查以确定哪些方法表现良好。该项目的目标是研究低资源语言的领域，并构建一个神经机器翻译模型，以实现最先进的结果。该项目旨在建立在 \texttt{mBART.CC25} 语言模型基础上，并探索利用各种 NLP 和深度学习技术（如后向翻译和迁移学习）来增强它的策略。该实现试图解开 NMT 应用程序的架构，并确定不同的组件，这些组件为我们提供了修改所述应用程序的机会。

    Neural Machine translation is a challenging task due to the inherent complex nature and the fluidity that natural languages bring. Nonetheless, in recent years, it has achieved state-of-the-art performance in several language pairs. Although, a lot of traction can be seen in the areas of multilingual neural machine translation (MNMT) in the recent years, there are no comprehensive survey done to identify what approaches work well. The goal of this project is to investigate the realm of low resource languages and build a Neural Machine Translation model to achieve state-of-the-art results. The project looks to build upon the \texttt{mBART.CC25} \cite{liu2020multilingual} language model and explore strategies to augment it with various NLP and Deep Learning techniques like back translation and transfer learning. This implementation tries to unpack the architecture of the NMT application and determine the different components which offers us opportunities to amend the said application wit
    
[^52]: 学习经验Bregman散度用于不确定距离表示

    Learning Empirical Bregman Divergence for Uncertain Distance Representation. (arXiv:2304.07689v1 [cs.CV])

    [http://arxiv.org/abs/2304.07689](http://arxiv.org/abs/2304.07689)

    本文介绍了一种新的基于Deep Metric Learning的方法，通过学习经验Bregman散度直接从数据中进行不确定距离表示，能够有效的在模式识别和聚类任务上提高准确性。

    

    深度度量学习技术已应用于各种监督和无监督学习任务，通过深度网络学习样本嵌入来进行视觉表示。然而，经典方法采用固定距离度量作为两个嵌入之间的相似性函数，可能导致捕捉复杂数据分布的亚最优性能。Bregman散度概括了各种距离度量的度量，并在许多深度度量学习领域中产生。本文首先展示了如何从Bregman散度获得深度度量学习损失。然后，我们介绍了一种直接从数据中学习经验Bregman散度的新方法，通过使用深度学习设置对Bregman散度下的凸函数进行参数化。我们进一步实验证明，与其他SOTA深度度量学习方法相比，我们的方法在五个流行公共数据集上表现出色，特别是在模式识别和聚类任务上。

    Deep metric learning techniques have been used for visual representation in various supervised and unsupervised learning tasks through learning embeddings of samples with deep networks. However, classic approaches, which employ a fixed distance metric as a similarity function between two embeddings, may lead to suboptimal performance for capturing the complex data distribution. The Bregman divergence generalizes measures of various distance metrics and arises throughout many fields of deep metric learning. In this paper, we first show how deep metric learning loss can arise from the Bregman divergence. We then introduce a novel method for learning empirical Bregman divergence directly from data based on parameterizing the convex function underlying the Bregman divergence with a deep learning setting. We further experimentally show that our approach performs effectively on five popular public datasets compared to other SOTA deep metric learning methods, particularly for pattern recognit
    
[^53]: 可操作的自回归语言生成控制方法

    Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v1 [cs.CL])

    [http://arxiv.org/abs/2304.07438](http://arxiv.org/abs/2304.07438)

    本文提出了一种在自回归文本生成中使用可操作概率模型来强制实施限制的控制方法GeLaTo，并取得了在常见的约束文本生成测试上的最先进性能。

    

    尽管自回归大语言模型在文本生成方面取得了成功，但生成满足复杂限制的文本仍然是一个重大挑战：即使是最简单的词汇限制也使条件分布$\Pr(\text{text} | \alpha)$的采样变得不可计算。为了克服这个挑战，我们提出使用可操作的概率模型将词汇限制强加于自回归文本生成中，我们将其称为 GeLaTo。为了证明这个框架的有效性，我们使用了精简的隐马尔可夫模型来控制从GPT2到自回归的生成。GeLaTo在约束文本生成的具有挑战性的基准测试CommonGen上取得了最先进的性能，大幅击败了各种强基线。我们的工作不仅为控制大型语言模型开辟了新的途径，还激励人们开发更具表现力的可操作概率模型。

    Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution $\Pr(\text{text} | \alpha)$ is intractable for even the simplest lexical constraints $\alpha$. To overcome this challenge, we propose to use tractable probabilistic models to impose lexical constraints in autoregressive text generation, which we refer to as GeLaTo. To demonstrate the effectiveness of this framework, we use distilled hidden Markov models to control autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on CommonGen, a challenging benchmark for constrained text generation, beating a wide range of strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive tractable probabilistic models.
    
[^54]: H2TNE：时态异构信息网络在双曲空间中的嵌入

    H2TNE: Temporal Heterogeneous Information Network Embedding in Hyperbolic Spaces. (arXiv:2304.06970v1 [cs.SI])

    [http://arxiv.org/abs/2304.06970](http://arxiv.org/abs/2304.06970)

    该论文提出了 H2TNE 模型，用于将时态异构信息网络嵌入到双曲空间中。通过时间和异质性双重约束的随机游走策略，该模型能够捕捉结构与语义信息。

    

    时间异构信息网络（temporal HIN）嵌入，旨在将不同时间戳的各种类型节点表示为低维空间，并同时保留结构和语义信息，在各种实际任务中至关重要。研究人员在欧几里得空间中进行了许多关于时间HIN嵌入的努力，并取得了一些可观的成果。然而，在现实世界中，许多网络都显示出分层属性和幂律分布，并不是欧几里得空间的等距的。最近，双曲空间中的表示学习已被证明对具有分层和幂律结构的数据是有效的。受这个特性的启发，我们提出了一个双曲异构时间网络嵌入（H2TNE）模型，用于时态HIN。具体而言，我们利用一个时间和异质性双重约束的随机游走策略来捕捉结构和语义信息，然后计算

    Temporal heterogeneous information network (temporal HIN) embedding, aiming to represent various types of nodes of different timestamps into low dimensional spaces while preserving structural and semantic information, is of vital importance in diverse real-life tasks. Researchers have made great efforts on temporal HIN embedding in Euclidean spaces and got some considerable achievements. However, there is always a fundamental conflict that many real-world networks show hierarchical property and power-law distribution, and are not isometric of Euclidean spaces. Recently, representation learning in hyperbolic spaces has been proved to be valid for data with hierarchical and power-law structure. Inspired by this character, we propose a hyperbolic heterogeneous temporal network embedding (H2TNE) model for temporal HINs. Specifically, we leverage a temporally and heterogeneously double-constrained random walk strategy to capture the structural and semantic information, and then calculate th
    
[^55]: 多属性多阶图卷积神经网络用于异构图

    Attributed Multi-order Graph Convolutional Network for Heterogeneous Graphs. (arXiv:2304.06336v1 [cs.LG])

    [http://arxiv.org/abs/2304.06336](http://arxiv.org/abs/2304.06336)

    本文提出了一个AMOGCN模型，它自动从多阶邻接矩阵的自适应聚合中研究包含多跳邻居的元路径，并使用节点属性评价监督。其能够有效地从异构图中发现有区别的节点嵌入和关系。

    

    异构图神经网络旨在从多关系网络中发现有区别的节点嵌入和关系。异构图学习的一个挑战是设计可学习的元路径，它显着地影响了学习到的嵌入的质量。因此，在本文中，我们提出了一个带属性的多阶图卷积网络（AMOGCN），它自动从多阶邻接矩阵的自适应聚合中研究包含多跳邻居的元路径。该模型首先从手动设计的节点连接中构建不同阶数的邻接矩阵。之后，从各种阶数的邻接矩阵的自动融合中附加一个完整的多阶邻接矩阵。这个过程由从节点同质性通过属性评价提取的节点语义信息监督。最终，我们使用一个学习到的多阶邻接矩阵的一层简化图卷积网络。

    Heterogeneous graph neural networks aim to discover discriminative node embeddings and relations from multi-relational networks.One challenge of heterogeneous graph learning is the design of learnable meta-paths, which significantly influences the quality of learned embeddings.Thus, in this paper, we propose an Attributed Multi-Order Graph Convolutional Network (AMOGCN), which automatically studies meta-paths containing multi-hop neighbors from an adaptive aggregation of multi-order adjacency matrices. The proposed model first builds different orders of adjacency matrices from manually designed node connections. After that, an intact multi-order adjacency matrix is attached from the automatic fusion of various orders of adjacency matrices. This process is supervised by the node semantic information, which is extracted from the node homophily evaluated by attributes. Eventually, we utilize a one-layer simplifying graph convolutional network with the learned multi-order adjacency matrix,
    
[^56]: CMOS + 随机纳米磁体：概率推理与学习异构计算机

    CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic inference and learning. (arXiv:2304.05949v1 [cond-mat.mes-hall])

    [http://arxiv.org/abs/2304.05949](http://arxiv.org/abs/2304.05949)

    本文展示了如何将基于随机磁隧道结（sMTJ）的概率比特（p位）与多功能可编程门阵列（FPGA）相结合，设计出一种能源高效的异构CMOS + X（X = sMTJ）原型，其成功地执行了概率推理和异步Boltzmann学习。

    

    随着摩尔定律的放缓，利用新兴的纳米技术（X）增强互补金属氧化物半导体（CMOS）晶体管变得越来越重要。本文展示了如何将基于随机磁隧道结（sMTJ）的概率比特（p位）与多功能可编程门阵列（FPGA）相结合，设计出一种能源高效的异构CMOS + X（X = sMTJ）原型。尽管sMTJs设备间存在差异，我们的异构计算机成功地执行了概率推理和异步Boltzmann学习。使用CMOS预测流程设计套件（PDK）进行全面比较，数字CMOS-based p-bits模拟高质量随机性需要超过10,000个晶体管，每生成一个随机数的能量比使用只消耗2fJ的sMTJ-based p-bits高约两个数量级。我们的方法的缩放和集成版本可以显着推进概率性的推理。

    With the slowing down of Moore's law, augmenting complementary-metal-oxide semiconductor (CMOS) transistors with emerging nanotechnologies (X) is becoming increasingly important. In this paper, we demonstrate how stochastic magnetic tunnel junction (sMTJ)-based probabilistic bits, or p-bits, can be combined with versatile Field Programmable Gate Arrays (FPGA) to design an energy-efficient, heterogeneous CMOS + X (X = sMTJ) prototype. Our heterogeneous computer successfully performs probabilistic inference and asynchronous Boltzmann learning despite device-to-device variations in sMTJs. A comprehensive comparison using a CMOS predictive process design kit (PDK) reveals that digital CMOS-based p-bits emulating high-quality randomness use over 10,000 transistors with the energy per generated random number being roughly two orders of magnitude greater than the sMTJ-based p-bits that dissipate only 2 fJ. Scaled and integrated versions of our approach can significantly advance probabilistic 
    
[^57]: 基于NeRF技术的卫星图像表面重建

    NeRF applied to satellite imagery for surface reconstruction. (arXiv:2304.04133v1 [cs.CV])

    [http://arxiv.org/abs/2304.04133](http://arxiv.org/abs/2304.04133)

    本文提出了Sat-NeRF模型，能够从少量的卫星图像集合中合成新的视角，并准确地估计场景表面的高程。

    

    本文提出了Sat-NeRF模型，是对最近引入的S-NeRF模型的修改实现。该模型能够从稀疏的卫星图像集合中合成新的视角，同时考虑到图片中的光照变化。训练好的模型还能够精确地估计场景表面的高程，这对卫星观测应用非常有帮助。S-NeRF方法改进了标准的NeRF方法，将辐射强度考虑为高反射率和入射辐照度的函数。这两个量都是模型的全连接神经网络枝条的输出，而后者则被视为来自太阳的直接光线和来自天空的漫反射颜色函数。该实现基于用缩放-裁剪技术增强的卫星图像数据集。对NeRF进行了超参数研究，得出了一些有趣的观察结果。

    We present Sat-NeRF, a modified implementation of the recently introduced Shadow Neural Radiance Field (S-NeRF) model. This method is able to synthesize novel views from a sparse set of satellite images of a scene, while accounting for the variation in lighting present in the pictures. The trained model can also be used to accurately estimate the surface elevation of the scene, which is often a desirable quantity for satellite observation applications. S-NeRF improves on the standard Neural Radiance Field (NeRF) method by considering the radiance as a function of the albedo and the irradiance. Both these quantities are output by fully connected neural network branches of the model, and the latter is considered as a function of the direct light from the sun and the diffuse color from the sky. The implementations were run on a dataset of satellite images, augmented using a zoom-and-crop technique. A hyperparameter study for NeRF was carried out, leading to intriguing observations on the 
    
[^58]: ChatGPT何时需要连续思考提示？

    When do you need Chain-of-Thought Prompting for ChatGPT?. (arXiv:2304.03262v1 [cs.AI])

    [http://arxiv.org/abs/2304.03262](http://arxiv.org/abs/2304.03262)

    该论文讨论了连续思考提示（CoT）对ChatGPT的有效性，发现在算术推理等任务中，这种提示不再有效，但在其他推理任务中仍有效。分析表明，在大型语言模型受训练推理时存在过拟合/偏差的风险，需要在更多任务和模型上评估和改进连续思考提示的鲁棒性。

    

    连续思考提示可以有效地引出大型语言模型的复杂多步推理，例如，在每个输入查询中添加连续思考提示“让我们逐步思考”，可以将GPT-3在MultiArith数据集上的准确性从17.7％提高到78.7％。然而，不清楚连续思考提示是否对更近期的指令微调型大型语言模型（如ChatGPT）仍然有效。令人惊讶的是，在ChatGPT上，连续思考提示对某些任务（如算术推理）不再有效，但对其他推理任务仍然有效。此外，在前者的任务上，ChatGPT通常表现最佳，甚至可以在没有被指示的情况下生成连续思考提示。因此，ChatGPT可能已经通过连续思考提示在这些任务上进行了训练，并且即使没有连续思考提示，也会在应用于相同的查询时隐含地遵循此类提示。我们的分析反映了大型语言模型在受训练推理时存在过拟合/偏差的潜在风险，并强调了评估和改进连续思考提示鲁棒性在更多任务和模型上的必要性。

    Chain-of-Thought (CoT) prompting can effectively elicit complex multi-step reasoning from Large Language Models~(LLMs). For example, by simply adding CoT instruction ``Let's think step-by-step'' to each input query of MultiArith dataset, GPT-3's accuracy can be improved from 17.7\% to 78.7\%. However, it is not clear whether CoT is still effective on more recent instruction finetuned (IFT) LLMs such as ChatGPT. Surprisingly, on ChatGPT, CoT is no longer effective for certain tasks such as arithmetic reasoning while still keeping effective on other reasoning tasks. Moreover, on the former tasks, ChatGPT usually achieves the best performance and can generate CoT even without being instructed to do so. Hence, it is plausible that ChatGPT has already been trained on these tasks with CoT and thus memorized the instruction so it implicitly follows such an instruction when applied to the same queries, even without CoT. Our analysis reflects a potential risk of overfitting/bias toward instruct
    
[^59]: BotTriNet: 一种基于度量学习的社交机器人检测统一高效的嵌入式框架

    BotTriNet: A Unified and Efficient Embedding for Social Bots Detection via Metric Learning. (arXiv:2304.03144v1 [cs.AI])

    [http://arxiv.org/abs/2304.03144](http://arxiv.org/abs/2304.03144)

    BOTTRINET基于文本内容检测机器人，并设计了三元组网络以提高分类性能。在真实世界数据集CRESCI2017上，系统表现最好。

    

    在在线社交网络中，快速准确地发现机器人账户以防止它们侵犯和骚扰真实用户是一个持久受欢迎的话题。我们提出了一种叫作BOTTRINET的统一嵌入式框架，它利用账户发布的文本内容检测机器人，基于的假设是上下文自然地揭示账户个性和习惯。如果系统能够使用嵌入技术有效地提取与机器人相关的信息，那么内容就是丰富和有价值的。除了生成词、句和账户嵌入的一般嵌入式框架外，我们设计了一个三元组网络来调整原始嵌入（由传统的自然语言处理技术生成）以获得更好的分类性能。我们在一个真实世界的数据集CRESCI2017上评估了检测准确性和F1得分，该数据集包括三个机器人账户类别和五个机器人样本集。我们的系统在两个内容集上实现了最高的平均准确性98.34%和F1得分97.99%。

    A persistently popular topic in online social networks is the rapid and accurate discovery of bot accounts to prevent their invasion and harassment of genuine users. We propose a unified embedding framework called BOTTRINET, which utilizes textual content posted by accounts for bot detection based on the assumption that contexts naturally reveal account personalities and habits. Content is abundant and valuable if the system efficiently extracts bot-related information using embedding techniques. Beyond the general embedding framework that generates word, sentence, and account embeddings, we design a triplet network to tune the raw embeddings (produced by traditional natural language processing techniques) for better classification performance. We evaluate detection accuracy and f1score on a real-world dataset CRESCI2017, comprising three bot account categories and five bot sample sets. Our system achieves the highest average accuracy of 98.34% and f1score of 97.99% on two content-inte
    
[^60]: 快速密集信息检索器利用KALE进行后置KL对齐的异形双编码器模型训练 (arXiv:2304.01016v2 [cs.CL] UPDATED)

    Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders. (arXiv:2304.01016v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.01016](http://arxiv.org/abs/2304.01016)

    本文提出了一种通过结构压缩和模型尺寸不对称的双编码器模型 KALE，有效提高密集信息检索的推理效率，同时允许查询编码器的有效压缩，而无需进行全部的再训练或索引生成，此方法能够生成超过DistilBERT性能的模型。

    

    本文提出了一种有结构压缩和模型尺寸不对称的双编码器模型，旨在提高基于语言模型的密集信息检索系统的推理速度。通过对MSMARCO、自然问答、问答游戏等多个数据集进行前后训练压缩实验，研究了压缩对系统推理效率的影响，结果表明密集信息检索器的双编码器结构异形化有助于提高其推理效率。基于此，我们引入了一种名为Kullback Leibler Alignment of Embeddings (KALE)的方法，通过裁剪和对齐查询编码器，提高了密集信息检索的推理效率。KALE扩展了传统的知识蒸馏方法，使得在双编码器训练后可以有效地对查询编码器进行压缩而无需进行完整的再训练或索引生成。使用KALE和不对称训练，我们可以生成超过DistilBERT性能的模型，同时模型尺寸更小。

    In this paper, we consider the problem of improving the inference latency of language model-based dense retrieval systems by introducing structural compression and model size asymmetry between the context and query encoders. First, we investigate the impact of pre and post-training compression on the MSMARCO, Natural Questions, TriviaQA, SQUAD, and SCIFACT, finding that asymmetry in the dual encoders in dense retrieval can lead to improved inference efficiency. Knowing this, we introduce Kullback Leibler Alignment of Embeddings (KALE), an efficient and accurate method for increasing the inference efficiency of dense retrieval methods by pruning and aligning the query encoder after training. Specifically, KALE extends traditional Knowledge Distillation after bi-encoder training, allowing for effective query encoder compression without full retraining or index generation. Using KALE and asymmetric training, we can generate models which exceed the performance of DistilBERT despite having 
    
[^61]: 基于单次学习的历史手稿文本检测方法 OTS

    OTS: A One-shot Learning Approach for Text Spotting in Historical Manuscripts. (arXiv:2304.00746v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.00746](http://arxiv.org/abs/2304.00746)

    提出了一种基于单次学习的历史手稿文本检测方法 OTS， 尤其对于低资源检测任务，使用新型的“环形损失”损失函数提高了检测能力，同时创建了包含古代东巴象形文字的手稿数据集。

    

    历史手稿处理面临有限的注释训练数据和新类别出现等挑战。为解决这一问题，我们提出了一种基于单次学习的文本检测方法 OTS，通过仅一个注释样本，准确可靠地检测出新颖字符。灵感源自认知研究，引入空间对齐模块，基于一个支持图像发现、关注和学习查询图像中最具有区别性的空间区域。尤其是，针对低资源检测任务通常面临样本不平衡问题，我们提出了一种名为“环形损失”的新型损失函数，可以使距离度量的嵌入空间更具有区分性。该方法高效，只需要少量的训练样本，具有处理新颖字符和符号的显著能力。为了增强数据集的多样性，我们创建了一个包含古代东巴象形文字（DBH）的手稿数据集。

    Historical manuscript processing poses challenges like limited annotated training data and novel class emergence. To address this, we propose a novel One-shot learning-based Text Spotting (OTS) approach that accurately and reliably spots novel characters with just one annotated support sample. Drawing inspiration from cognitive research, we introduce a spatial alignment module that finds, focuses on, and learns the most discriminative spatial regions in the query image based on one support image. Especially, since the low-resource spotting task often faces the problem of example imbalance, we propose a novel loss function called torus loss which can make the embedding space of distance metric more discriminative. Our approach is highly efficient and requires only a few training samples while exhibiting the remarkable ability to handle novel characters, and symbols. To enhance dataset diversity, a new manuscript dataset that contains the ancient Dongba hieroglyphics (DBH) is created. We
    
[^62]: 具有动态全局滤波器的双流时延神经网络用于说话人验证

    Dual-stream Time-Delay Neural Network with Dynamic Global Filter for Speaker Verification. (arXiv:2303.11020v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2303.11020](http://arxiv.org/abs/2303.11020)

    该论文提出了具有动态全局滤波器的双流时延神经网络，在说话人验证领域的表现优于其他最先进的方法，并实现了最先进的性能。

    

    时延神经网络(TDNN)是文本无关说话人验证领域的最先进模型之一。然而，对于传统的TDNN来说，捕捉被证明对于鲁棒说话人表示和长时间说话人验证至关重要的全局上下文是困难的。此外，常见的解决方案(例如自我关注)对于输入令牌具有二次复杂度，当应用于TDNN中具有大尺寸特征映射时，这使它们在计算上无法承受。为了解决这些问题，我们提出了TDNN的全局滤波器(GFTDNN)，它应用对数线性复杂度的FFT/IFFT和一组可微频域滤波器来高效地建模语音中的长期依赖关系。此外，还为增强全局滤波器的性能并防止过度拟合，特别设计了动态滤波策略和稀疏正则化方法。此外，我们构建了一个双流TDNN(DS-TDNN)，将基本通道分成两个并行通道，使用不同的频域滤波器训练每个通道。在两个不同的大规模说话人验证数据集上的实验表明，所提出的方法在性能上优于标准TDNN和其他最先进方法，并实现了最先进的性能。

    The time-delay neural network (TDNN) is one of the state-of-the-art models for text-independent speaker verification. However, it is difficult for conventional TDNN to capture global context that has been proven critical for robust speaker representations and long-duration speaker verification in many recent works. Besides, the common solutions, e.g., self-attention, have quadratic complexity for input tokens, which makes them computationally unaffordable when applied to the feature maps with large sizes in TDNN. To address these issues, we propose the Global Filter for TDNN, which applies log-linear complexity FFT/IFFT and a set of differentiable frequency-domain filters to efficiently model the long-term dependencies in speech. Besides, a dynamic filtering strategy, and a sparse regularization method are specially designed to enhance the performance of the global filter and prevent it from overfitting. Furthermore, we construct a dual-stream TDNN (DS-TDNN), which splits the basic cha
    
[^63]: BotShape：一种基于行为模式的社交机器人检测方法

    BotShape: A Novel Social Bots Detection Approach via Behavioral Patterns. (arXiv:2303.10214v1 [cs.SI])

    [http://arxiv.org/abs/2303.10214](http://arxiv.org/abs/2303.10214)

    BotShape是一种新型的基于行为模式的社交机器人检测方法，通过提取重要的行为特征可以提高检测性能。

    

    在在线社交网络安全中，准确检测机器人账户并减轻其对真实用户的有害影响（如误导、谣言和垃圾信息）是一个重要的话题。基于真实数据集，我们从原始事件日志构建了行为序列，并提取了关键的行为特征。我们观察到机器人账户与真实用户之间的差异以及机器人账户之间相似的行为模式。我们提出了一种新型社交机器人检测系统BotShape，自动捕捉行为序列和特征，并作为分类器检测机器人账户。我们评估了我们系统的检测性能，展示了各种类型分类器的平均准确率为98.52％，平均f1分数为96.65％。与其他研究相比，我们得出结论：BotShape是一种新的账号建模方法，通过提供重要的行为特征可以提高大多数方法的性能。

    An essential topic in online social network security is how to accurately detect bot accounts and relieve their harmful impacts (e.g., misinformation, rumor, and spam) on genuine users. Based on a real-world data set, we construct behavioral sequences from raw event logs. After extracting critical characteristics from behavioral time series, we observe differences between bots and genuine users and similar patterns among bot accounts. We present a novel social bot detection system BotShape, to automatically catch behavioral sequences and characteristics as features for classifiers to detect bots. We evaluate the detection performance of our system in ground-truth instances, showing an average accuracy of 98.52% and an average f1-score of 96.65% on various types of classifiers. After comparing it with other research, we conclude that BotShape is a novel approach to profiling an account, which could improve performance for most methods by providing significant behavioral features.
    
[^64]: 国家癌症研究所影像数据共享平台：计算病理学可重复研究的基础

    The NCI Imaging Data Commons as a platform for reproducible research in computational pathology. (arXiv:2303.09354v1 [cs.CV])

    [http://arxiv.org/abs/2303.09354](http://arxiv.org/abs/2303.09354)

    国家癌症研究所影像数据共享平台 (IDC) 旨在促进计算病理学领域的研究可重复性，实现了 FAIR 原则，提供公共库和云端技术支持，方便使用机器学习方法进行癌症组织分类研究。

    

    目的：可重复性对于将计算病理学（CompPath）中基于机器学习（ML）的解决方案转化为实践至关重要。然而，越来越多的研究报告难以重复 ML 结果的困难。国家癌症研究所影像数据共享平台（IDC）是一个公共库，包含 >120 个癌症图像收集，包括 >38,000 张全切片图像（WSIs），旨在与云端 ML 服务一起使用。本文探讨了 IDC 促进 CompPath 研究可重复性的潜力。 材料和方法：IDC 实现了 FAIR 原则：所有图像都根据 DICOM 标准进行编码，具有持久化标识符、可通过丰富的元数据进行发现，并可通过开放式工具访问。借此优势，我们在 IDC 的不同数据集上实现了两个实验，针对肺癌组织分类的一种代表性基于 ML 的方法进行了训练和/或评估。为评估可重复性，实验被多次运行。

    Objective: Reproducibility is critical for translating machine learning-based (ML) solutions in computational pathology (CompPath) into practice. However, an increasing number of studies report difficulties in reproducing ML results. The NCI Imaging Data Commons (IDC) is a public repository of >120 cancer image collections, including >38,000 whole-slide images (WSIs), that is designed to be used with cloud-based ML services. Here, we explore the potential of the IDC to facilitate reproducibility of CompPath research.  Materials and Methods: The IDC realizes the FAIR principles: All images are encoded according to the DICOM standard, persistently identified, discoverable via rich metadata, and accessible via open tools. Taking advantage of this, we implemented two experiments in which a representative ML-based method for classifying lung tumor tissue was trained and/or evaluated on different datasets from the IDC. To assess reproducibility, the experiments were run multiple times with i
    
[^65]: 通用的三维多物体搜索系统

    A System for Generalized 3D Multi-Object Search. (arXiv:2303.03178v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.03178](http://arxiv.org/abs/2303.03178)

    本文介绍了GenMOS，这是第一个适用于实际机器人和环境的通用三维多物体搜索系统。它使用点云观察的三种方式，并通过在线规划输出一个六自由度视点，成功展示了其在多个数据集上的有效性和效率。

    

    对物体进行搜索是机器人的一项基本技能。因此，我们希望物体搜索最终能够成为类似于物体检测和SLAM等机器人的即插即用能力。然而，迄今没有一个3D物体搜索系统能够适用于实际机器人和环境。基于最近利用八叉树结构表示3D信念的理论框架，本文提出了GenMOS（通用多物体搜索），这是第一个独立于机器人和环境的三维多物体搜索（MOS）通用系统。GenMOS以本地区域的点云观察、物体检测结果和机器人视角定位作为输入，并通过在线规划输出一个六自由度视点。特别是，GenMOS使用点云观察的三种方式：（1）模拟遮挡；（2）信息占用并初始化八叉树信念；（3）为每个物体采样信念相关的3D位姿先验。该系统在两个模拟数据集和一个真实世界数据集上进行了评估，展示了GenMOS的有效性和效率。

    Searching for objects is a fundamental skill for robots. As such, we expect object search to eventually become an off-the-shelf capability for robots, similar to e.g., object detection and SLAM. In contrast, however, no system for 3D object search exists that generalizes across real robots and environments. In this paper, building upon a recent theoretical framework that exploited the octree structure for representing belief in 3D, we present GenMOS (Generalized Multi-Object Search), the first general-purpose system for multi-object search (MOS) in a 3D region that is robot-independent and environment-agnostic. GenMOS takes as input point cloud observations of the local region, object detection results, and localization of the robot's view pose, and outputs a 6D viewpoint to move to through online planning. In particular, GenMOS uses point cloud observations in three ways: (1) to simulate occlusion; (2) to inform occupancy and initialize octree belief; and (3) to sample a belief-depend
    
[^66]: 具有完成功能的神经通用邻居用于链接预测

    Neural Common Neighbor with Completion for Link Prediction. (arXiv:2302.00890v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00890](http://arxiv.org/abs/2302.00890)

    提出了神经通用邻居模型（NCN）用于链接预测，使用可学习的成对表示来捕捉节点之间的成对关系，以提高性能，同时解决链路不完整问题。

    

    尽管vanilla信息传递神经网络（MPNN）在各种图任务中具有出色的性能，但在链接预测任务中通常失败，因为它只使用两个单独目标节点的表示，并忽略它们之间的成对关系。为了捕获成对关系，一些模型将手动功能添加到输入图中，并使用MPNN的输出来生成成对表示。相反，其他人直接将手动功能用作成对表示。尽管此简化避免了将GNN逐个链接地应用于每个链接，从而提高了可扩展性，但由于手工制作的和不可学习的成对特征，这些模型仍有很大的性能提升空间。为了在保持可扩展性的同时提高性能，我们提出了神经通用邻居（NCN），它使用可学习的成对表示。为了进一步提高NCN的性能，我们研究了未观察到的链接问题。图的不完整性是普遍存在的，并导致分布偏移

    Despite its outstanding performance in various graph tasks, vanilla Message Passing Neural Network (MPNN) usually fails in link prediction tasks, as it only uses representations of two individual target nodes and ignores the pairwise relation between them. To capture the pairwise relations, some models add manual features to the input graph and use the output of MPNN to produce pairwise representations. In contrast, others directly use manual features as pairwise representations. Though this simplification avoids applying a GNN to each link individually and thus improves scalability, these models still have much room for performance improvement due to the hand-crafted and unlearnable pairwise features. To upgrade performance while maintaining scalability, we propose Neural Common Neighbor (NCN), which uses learnable pairwise representations. To further boost NCN, we study the unobserved link problem. The incompleteness of the graph is ubiquitous and leads to distribution shifts between
    
[^67]: 基于图的概率多智能体轨迹预测模型MTP-GO与神经ODE

    MTP-GO: Graph-Based Probabilistic Multi-Agent Trajectory Prediction with Neural ODEs. (arXiv:2302.00735v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.00735](http://arxiv.org/abs/2302.00735)

    本文介绍了一种基于图的概率多智能体轨迹预测模型MTP-GO。该模型利用时间图神经网络编码场景，采用神经常微分方程实现运动模型，并结合混合密度网络和卡尔曼滤波实现多模态概率预测，在多个指标上优于其他最先进的方法。

    

    实现弹性的自主路径规划需要对周围道路用户未来行为做出可靠的预测。为响应此需求及相关挑战，本文引入了名为MTP-GO的模型。该模型采用时间图神经网络对场景进行编码，生成底层运动模型的输入。运动模型采用了神经常微分方程，其中的状态转移函数将和其他部分一起进行学习。结合混合密度网络和卡尔曼滤波的概念，可以获得多模态的概率预测。结果表明，所提出的模型在各种数据集上的预测能力优于几种最先进的方法，并且在多个指标上表现出色。

    Enabling resilient autonomous motion planning requires robust predictions of surrounding road users' future behavior. In response to this need and the associated challenges, we introduce our model titled MTP-GO. The model encodes the scene using temporal graph neural networks to produce the inputs to an underlying motion model. The motion model is implemented using neural ordinary differential equations where the state-transition functions are learned with the rest of the model. Multimodal probabilistic predictions are obtained by combining the concept of mixture density networks and Kalman filtering. The results illustrate the predictive capabilities of the proposed model across various data sets, outperforming several state-of-the-art methods on a number of metrics.
    
[^68]: AttMEMO: 在大内存系统上利用记忆化加速Transformers

    AttMEMO : Accelerating Transformers with Memoization on Big Memory Systems. (arXiv:2301.09262v2 [cs.PF] UPDATED)

    [http://arxiv.org/abs/2301.09262](http://arxiv.org/abs/2301.09262)

    本文提出一种利用记忆技术加速自注意力机制的Transformer模型的推理过程的方法，该方法可以在不需要修改模型架构或使用特殊硬件的情况下进行，并可以使推理延迟降低22%。

    

    Transformer模型因其优越的推理准确性和推理吞吐量而受到欢迎。然而，由于Transformer是计算密集型的，导致推理时间较长。现有的Transformer推理加速工作存在限制，这些限制要么是由于修改Transformer架构，要么是需要专门的硬件。本文提出了使用记忆化加速Transformer模型的机会，而不涉及以上限制。基于这样的独特观察，即在推理序列内Attention计算中存在丰富的相似性，我们构建了一个利用新兴的大内存系统的记忆化数据库。我们引入了一种新的嵌入技术来查找语义上相似的输入，以识别计算相似性。我们还引入了一系列技术，如内存映射和选择性记忆化，以避免内存复制和不必要的开销。我们使得推理延迟降低了22%，而且内存需求适中。我们的方法可以很容易地适用于各种Transformer模型，而且无需进行重要修改。

    Transformer models gain popularity because of their superior inference accuracy and inference throughput. However, the transformer is computation-intensive, causing a long inference time. The existing works on transformer inference acceleration have limitations caused by either the modification of transformer architectures or the need of specialized hardware. In this paper, we identify the opportunities of using memoization to accelerate the self-attention mechanism in transformers without the above limitations. Built upon a unique observation that there is rich similarity in attention computation across inference sequences, we build a memoization database that leverages the emerging big memory system. We introduce a novel embedding technique to find semantically similar inputs to identify computation similarity. We also introduce a series of techniques such as memory mapping and selective memoization to avoid memory copy and unnecessary overhead. We enable 22% inference-latency reduct
    
[^69]: 意识是学习的过程：通过绑定学习的预测处理系统可能会将自己感知为有意识的

    Consciousness is learning: predictive processing systems that learn by binding may perceive themselves as conscious. (arXiv:2301.07016v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2301.07016](http://arxiv.org/abs/2301.07016)

    通过层级绑定和联想检索变为短期和长期声明性记忆的在线预测处理系统可能会感知到自己具有意识。

    

    机器学习算法在特定复杂领域实现了超越人类的表现。然而，从少量示例中进行在线学习，并在不同领域之间高效地泛化仍然是难以实现的。在人类身上，这种学习通过声明性存储过程进行，并且与意识密切相关。预测处理被推广为一种基于贝叶斯推理框架的原则性方法，用于理解皮质如何实现深度生成感知模型，用于感官数据和行为控制。然而，预测处理对于快速组成式学习或意识之谜提供了很少的直接见解。在这里，我们提出，通过通过绑定预测中的层次模型来实现在线学习，预测处理系统可以通过从单个示例中为感知和行动形成工作记忆，在新情况下灵活泛化，这可通过联想检索变为短期和长期的声明性记忆。我们认为，这个过程，我们称之为“在线层级预测绑定”，也可能是系统感知自己具有意识的必要条件。由此产生的模型提供了一种关于感知的、运动的、认知的和情感的意识的统一解释，并具有进化和发育生物学的深刻根源。

    Machine learning algorithms have achieved superhuman performance in specific complex domains. Yet learning online from few examples and efficiently generalizing across domains remains elusive. In humans such learning proceeds via declarative memory formation and is closely associated with consciousness. Predictive processing has been advanced as a principled Bayesian inference framework for understanding the cortex as implementing deep generative perceptual models for both sensory data and action control. However, predictive processing offers little direct insight into fast compositional learning or the mystery of consciousness. Here we propose that through implementing online learning by hierarchical binding of unpredicted inferences, a predictive processing system may flexibly generalize in novel situations by forming working memories for perceptions and actions from single examples, which can become short- and long-term declarative memories retrievable by associative recall. We argu
    
[^70]: 基于点云的毫米波通信主动链路质量预测

    Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications. (arXiv:2301.00752v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2301.00752](http://arxiv.org/abs/2301.00752)

    本研究提出了一种基于点云的毫米波通信主动链路质量预测方法，相比于基于图像的方法，其适用性更广且不涉及敏感信息。

    

    本研究展示了基于点云的毫米波（mmWave）通信的主动链路质量预测的可行性。以往的研究提出了基于机器学习的方法，利用深度图像的时间序列来预测未来时间段的接收信号强度，以缓解行人阻挡因素对mmWave通信的影响。但是，由于隐私问题，这些基于图像的方法的适用性有限，因为摄像头图像可能包含敏感信息。本研究提出了一种基于点云的mmWave链路质量预测方法，并通过实验证明其可行性。点云将三维空间表示为点集，其空间性质更加稀疏，不太可能包含敏感信息，并且还提供了3D位置和运动信息，这对了解涉及行人的无线电传播环境是必要的。

    This study demonstrates the feasibility of point cloud-based proactive link quality prediction for millimeter-wave (mmWave) communications. Previous studies have proposed machine learning-based methods to predict received signal strength for future time periods using time series of depth images to mitigate the line-of-sight (LOS) path blockage by pedestrians in mmWave communication. However, these image-based methods have limited applicability due to privacy concerns as camera images may contain sensitive information. This study proposes a point cloud-based method for mmWave link quality prediction and demonstrates its feasibility through experiments. Point clouds represent three-dimensional (3D) spaces as a set of points and are sparser and less likely to contain sensitive information than camera images. Additionally, point clouds provide 3D position and motion information, which is necessary for understanding the radio propagation environment involving pedestrians. This study designs
    
[^71]: 神经符号计算的语义框架

    A Semantic Framework for Neural-Symbolic Computing. (arXiv:2212.12050v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.12050](http://arxiv.org/abs/2212.12050)

    该论文提出了一个神经符号计算的语义框架，用于将神经网络和符号AI相结合成为综合系统，并通过将符号知识编码到神经网络中来解决通用推理能力的问题。

    

    人工智能的两种方法，神经网络和符号系统，对于一系列AI问题已经被证明非常成功。然而，两者均未能达到人类智能所需的通用推理能力。人们认为这是每种方法内在弱点所致。幸运的是，这些弱点似乎是互补的，符号系统擅长神经网络难以处理的事物，反之亦然。神经符号AI领域试图利用这种不对称性通过将神经网络和符号AI相结合成为综合系统。通常这是通过将符号知识编码到神经网络中实现的。不幸的是，虽然提出了许多不同的方法来实现这一点，但没有公共的编码定义可供比较。我们通过引入神经符号AI的语义框架来解决这个问题，然后证明它足以解释大量神经符号系统。我们的框架是基于符号系统植根于其领域的神经表征的概念。我们展示了我们的框架可以解释各种符号系统在神经表征中的实现方式，包括使用学习的神经表征和使用固定神经表征的系统。

    Two approaches to AI, neural networks and symbolic systems, have been proven very successful for an array of AI problems. However, neither has been able to achieve the general reasoning ability required for human-like intelligence. It has been argued that this is due to inherent weaknesses in each approach. Luckily, these weaknesses appear to be complementary, with symbolic systems being adept at the kinds of things neural networks have trouble with and vice-versa. The field of neural-symbolic AI attempts to exploit this asymmetry by combining neural networks and symbolic AI into integrated systems. Often this has been done by encoding symbolic knowledge into neural networks. Unfortunately, although many different methods for this have been proposed, there is no common definition of an encoding to compare them. We seek to rectify this problem by introducing a semantic framework for neural-symbolic AI, which is then shown to be general enough to account for a large family of neural-symb
    
[^72]: 主动任务随机化：通过无监督生成多样和可行任务学习鲁棒技能。

    Active Task Randomization: Learning Robust Skills via Unsupervised Generation of Diverse and Feasible Tasks. (arXiv:2211.06134v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.06134](http://arxiv.org/abs/2211.06134)

    本文提出了一种称为主动任务随机化（ATR）的方法，通过无监督生成任务来学习鲁棒技能，该方法选择适合学习鲁棒技能的任务，通过平衡任务的多样性和可行性来预测任务多样性和可行性，并使用基于图的参数化程序生成任务，从而允许鲁棒地处理任务的各种情况，包括以前未见过的场景。

    

    解决现实世界中的操纵任务需要机器人具备适用于各种情况的技能库。使用基于学习的方法获得这种技能的关键挑战是获取覆盖任务的多样性和可行性的训练数据，这通常需要非常复杂的手动劳动和领域知识。在这项工作中，我们介绍了一种名为主动任务随机化（ATR）的方法，通过无监督生成训练任务来学习鲁棒的技能。 ATR通过平衡任务的多样性和可行性，选择适合学习鲁棒技能的任务，其中包括初始环境状态和操作目标。我们提出通过共同学习紧凑任务表示来预测任务多样性和可行性。然后使用基于图的参数化在模拟中程序生成所选的任务。这些训练任务的主动选择使得使用我们的框架训练的技能策略能够在不需要大量手动注释或领域知识的情况下，鲁棒地处理任务的各种情况，包括以前未见过的场景。

    Solving real-world manipulation tasks requires robots to have a repertoire of skills applicable to a wide range of circumstances. When using learning-based methods to acquire such skills, the key challenge is to obtain training data that covers diverse and feasible variations of the task, which often requires non-trivial manual labor and domain knowledge. In this work, we introduce Active Task Randomization (ATR), an approach that learns robust skills through the unsupervised generation of training tasks. ATR selects suitable tasks, which consist of an initial environment state and manipulation goal, for learning robust skills by balancing the diversity and feasibility of the tasks. We propose to predict task diversity and feasibility by jointly learning a compact task representation. The selected tasks are then procedurally generated in simulation using graph-based parameterization. The active selection of these training tasks enables skill policies trained with our framework to robus
    
[^73]: 知识就是力量：理解因果关系使法律判决预测模型更具普适性和鲁棒性

    Knowledge is Power: Understanding Causality Makes Legal judgment Prediction Models More Generalizable and Robust. (arXiv:2211.03046v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.03046](http://arxiv.org/abs/2211.03046)

    本研究使用因果结构模型分析了法律判决预测模型学习决策的原理，发现现有最先进模型利用非因果信息进行判决预测，违反法律规则会削弱模型鲁棒性和普适性并导致歧视问题，提出基于因果干预的解决方案。

    

    法律判决预测是一种基于法律规则根据事实描述预测判决的法律辅助工具，旨在缓解有限法律从业人员的巨大工作负担。目前，大多数现有方法都采用各种大规模预训练语言模型（PLMs）在LJP任务中进行微调，从而实现了一致的改进。然而，我们发现现有的最先进模型根据无关（或非因果）信息进行判决预测。违反法律规则不仅削弱了模型的鲁棒性和普适性，还会导致严重的社会问题，如歧视。本文使用因果结构模型（SCMs）理论地分析了LJP模型如何学习做出决策以及为什么它们可以在不学习因果关系的情况下成功通过传统的测试范式。根据我们的分析，我们提供两种分别基于数据和模型的因果干预解决方案。

    Legal Judgment Prediction (LJP), aiming to predict a judgment based on fact descriptions according to rule of law, serves as legal assistance to mitigate the great work burden of limited legal practitioners. Most existing methods apply various large-scale pre-trained language models (PLMs) finetuned in LJP tasks to obtain consistent improvements. However, we discover the fact that the state-of-the-art (SOTA) model makes judgment predictions according to irrelevant (or non-casual) information. The violation of rule of law not only weakens the robustness and generalization ability of models but also results in severe social problems like discrimination. In this paper, we use causal structural models (SCMs) to theoretically analyze how LJP models learn to make decisions and why they can succeed in passing the traditional testing paradigm without learning causality. According to our analysis, we provide two solutions intervening on data and model by causality, respectively. In detail, we f
    
[^74]: FedTP: 联邦学习中的Transformer个性化

    FedTP: Federated Learning by Transformer Personalization. (arXiv:2211.01572v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01572](http://arxiv.org/abs/2211.01572)

    本文研究发现联邦平均算法对Transformer模型中的自注意力存在负面影响，限制了联邦学习的能力。为此提出了FedTP，在学习客户端个性化自注意力的同时，将其他参数聚合在客户端之间。

    

    联邦学习是一种新兴的学习范式，多个客户端协作地进行隐私保护的机器学习模型训练。个性化联邦学习通过学习个性化模型，克服客户端之间的异质性。最近，已经有一些尝试将Transformer应用于联邦学习。然而，联邦学习算法对自注意力的影响尚未被研究。本文研究了这种关系，并揭示了在存在数据异质性时，联邦平均算法实际上会对自注意力产生负面影响。这些影响限制了Transformer模型在联邦学习环境中的能力。基于此，我们提出了FedTP，一种新的基于Transformer的联邦学习框架，它为每个客户端学习个性化的自注意力，同时将其他参数聚合在客户端之间而不是使用纯个性化机制。

    Federated learning is an emerging learning paradigm where multiple clients collaboratively train a machine learning model in a privacy-preserving manner. Personalized federated learning extends this paradigm to overcome heterogeneity across clients by learning personalized models. Recently, there have been some initial attempts to apply Transformers to federated learning. However, the impacts of federated learning algorithms on self-attention have not yet been studied. This paper investigates this relationship and reveals that federated averaging algorithms actually have a negative impact on self-attention where there is data heterogeneity. These impacts limit the capabilities of the Transformer model in federated learning settings. Based on this, we propose FedTP, a novel Transformer-based federated learning framework that learns personalized self-attention for each client while aggregating the other parameters among the clients. Instead of using a vanilla personalization mechanism th
    
[^75]: 损失熵机会下的泛化：利用广泛的离线数据学习视觉运动任务。

    Generalization with Lossy Affordances: Leveraging Broad Offline Data for Learning Visuomotor Tasks. (arXiv:2210.06601v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.06601](http://arxiv.org/abs/2210.06601)

    本文提出了一个利用有损表示空间下的子目标指导在线微调的框架，它可以从广泛的数据中学习而来的有损表示来规划一系列子目标，分解原始任务，并强调任务相关信息，从而降低泛化过程中冗余内容的干扰，以此应对如何利用多样化的多任务数据进行新领域下游任务的学习的挑战。

    

    广泛数据集的利用已被证明对于各个领域的泛化都至关重要。然而如何有效利用多样化的多任务数据进行新领域下游任务的学习仍然是机器人学中的一大挑战。为了解决这个问题，本文提出了一个框架，它通过离线强化学习在广泛的数据上获取未见过的时间延迟任务的目标条件策略，同时结合学习到的有损失表示空间下的子目标指导在线微调。当面对新的任务目标时，该框架使用机会模型来规划一系列有损表示作为子目标，将原始任务分解成更容易的问题。这些有损表示从广泛的数据中学习而来，它们强调有关状态和目标的任务相关信息，同时抽象出妨碍泛化的冗余内容。因此，它可以为未见过的任务提供子目标规划，为策略提供紧凑的输入，也为奖励提供了较好的抽象。

    The utilization of broad datasets has proven to be crucial for generalization for a wide range of fields. However, how to effectively make use of diverse multi-task data for novel downstream tasks still remains a grand challenge in robotics. To tackle this challenge, we introduce a framework that acquires goal-conditioned policies for unseen temporally extended tasks via offline reinforcement learning on broad data, in combination with online fine-tuning guided by subgoals in learned lossy representation space. When faced with a novel task goal, the framework uses an affordance model to plan a sequence of lossy representations as subgoals that decomposes the original task into easier problems. Learned from the broad data, the lossy representation emphasizes task-relevant information about states and goals while abstracting away redundant contexts that hinder generalization. It thus enables subgoal planning for unseen tasks, provides a compact input to the policy, and facilitates reward
    
[^76]: 一种利用在线和离线深度强化学习的维护计划框架

    A Maintenance Planning Framework using Online and Offline Deep Reinforcement Learning. (arXiv:2208.00808v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.00808](http://arxiv.org/abs/2208.00808)

    本论文提出了一种利用在线和离线深度强化学习的维护计划框架，以确定最优恢复策略，经过实验证明该框架比标准的预防性、纠正性和贪婪式计划方案有所改进。

    

    成本效益的资产管理是各个行业关注的领域。本文针对不断恶化的水管开发了一种深度强化学习（DRL）解决方案，用于自动确定最优恢复策略。我们采用在线和离线DRL设置来解决维修计划问题。在在线DRL中，智能体与具有不同长度、材料和失效率特征的多个水管的模拟环境进行交互。我们使用深度Q学习（DQN）对智能体进行训练，以学习具有最小平均成本和降低失效概率的最优策略。在离线学习中，智能体使用静态数据（如DQN重放数据）通过保守的Q学习算法学习最优策略，无需与环境进行进一步交互。我们证明了基于DRL的策略比标准的预防性、纠正性和贪婪式计划方案有所改进。此外，我们还能够从固定的DQN重放数据中学习知识。

    Cost-effective asset management is an area of interest across several industries. Specifically, this paper develops a deep reinforcement learning (DRL) solution to automatically determine an optimal rehabilitation policy for continuously deteriorating water pipes. We approach the problem of rehabilitation planning in an online and offline DRL setting. In online DRL, the agent interacts with a simulated environment of multiple pipes with distinct lengths, materials, and failure rate characteristics. We train the agent using deep Q-learning (DQN) to learn an optimal policy with minimal average costs and reduced failure probability. In offline learning, the agent uses static data, e.g., DQN replay data, to learn an optimal policy via a conservative Q-learning algorithm without further interactions with the environment. We demonstrate that DRL-based policies improve over standard preventive, corrective, and greedy planning alternatives. Additionally, learning from the fixed DQN replay data
    
[^77]: 跨模态因果关系推理在事件级视觉问答中的应用

    Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering. (arXiv:2207.12647v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.12647](http://arxiv.org/abs/2207.12647)

    本篇论文提出了一个新型的事件级视觉问答框架——跨模态因果关系推理（CMCIR），通过引入因果干预方法，发现视觉和语言模态的真正因果结构，实现强健的因果感知视觉语言问答。

    

    现有的视觉问答方法往往捕捉跨模态的伪相关性，而未能发现真正的因果机制，以真实地基于主导视觉证据和问题意图进行推理。此外，现有方法通常忽略了跨模态事件级理解，需要联合建模事件的时间性、因果性和动态性。在本文中，我们从新的角度，即跨模态因果关系推理，聚焦于事件级视觉问答，引入因果干预方法来发现视觉和语言模态的真正因果结构。具体而言，我们提出了一个名为跨模态因果关系推理（CMCIR）的新型事件级视觉问答框架，以实现强健的因果感知视觉语言问答。为了发现跨模态因果结构，我们提出了因果感知视觉语言推理（CVLR）模块，用于共同对视觉和语言模态建模。

    Existing visual question answering methods tend to capture the cross-modal spurious correlations and fail to discover the true causal mechanism that facilitates reasoning truthfully based on the dominant visual evidence and the question intention. Additionally, the existing methods usually ignore the cross-modal event-level understanding that requires to jointly model event temporality, causality, and dynamics. In this work, we focus on event-level visual question answering from a new perspective, i.e., cross-modal causal relational reasoning, by introducing causal intervention methods to discover the true causal structures for visual and linguistic modalities. Specifically, we propose a novel event-level visual question answering framework named Cross-Modal Causal RelatIonal Reasoning (CMCIR), to achieve robust causality-aware visual-linguistic question answering. To discover cross-modal causal structures, the Causality-aware Visual-Linguistic Reasoning (CVLR) module is proposed to co
    
[^78]: 连续时间下的q-Learning

    q-Learning in Continuous Time. (arXiv:2207.00713v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.00713](http://arxiv.org/abs/2207.00713)

    本文研究了连续时间下的q-Learning，通过引入小q函数作为一阶近似，研究了q-learning理论，应用于设计不同的演员-评论家算法。

    

    我们研究了基于熵正则化的探索性扩散过程的Q-learning在连续时间下的应用。我们引入了“小q函数”作为大Q函数的一阶近似，研究了q函数的q-learning理论，并应用于设计不同的演员-评论家算法。

    We study the continuous-time counterpart of Q-learning for reinforcement learning (RL) under the entropy-regularized, exploratory diffusion process formulation introduced by Wang et al. (2020). As the conventional (big) Q-function collapses in continuous time, we consider its first-order approximation and coin the term ``(little) q-function". This function is related to the instantaneous advantage rate function as well as the Hamiltonian. We develop a ``q-learning" theory around the q-function that is independent of time discretization. Given a stochastic policy, we jointly characterize the associated q-function and value function by martingale conditions of certain stochastic processes, in both on-policy and off-policy settings. We then apply the theory to devise different actor-critic algorithms for solving underlying RL problems, depending on whether or not the density function of the Gibbs measure generated from the q-function can be computed explicitly. One of our algorithms inter
    
[^79]: 在类型化自然演算系统中检验概率计算的可信性

    Checking Trustworthiness of Probabilistic Computations in a Typed Natural Deduction System. (arXiv:2206.12934v2 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2206.12934](http://arxiv.org/abs/2206.12934)

    本文介绍了一种名为TPTND的概率类型自然演算系统，该系统能够检验并推导概率计算过程的可信性，具有可检查性的优势。

    

    本文介绍了一种名为 TPTND 的概率类型自然演算系统，该系统旨在推导有关概率计算过程的可信性属性，例如当今人工智能应用程序中的那些属性。TPTND 中的推导被解释为从给定的分类分布中提取 n 个可能复杂输出样本的过程。我们将这些输出样本的可信性形式化为一种假设测试，即计算出现有的频率与预期的概率之间的距离。这个演算系统的主要优势在于能够检查这种可信性的概念。我们为推理过程中出现的项提供了计算语义，并定义了逻辑运算符以及信任运算符的引入和消解规则。我们重点介绍了系统的结构和元理论属性，尤其是能够确定哪些项演化和逻辑规则应用时，计算仍然是可信的。

    In this paper we present the probabilistic typed natural deduction calculus TPTND, designed to reason about and derive trustworthiness properties of probabilistic computational processes, like those underlying current AI applications. Derivability in TPTND is interpreted as the process of extracting $n$ samples of possibly complex outputs with a certain frequency from a given categorical distribution. We formalize trust for such outputs as a form of hypothesis testing on the distance between such frequency and the intended probability. The main advantage of the calculus is to render such notion of trustworthiness checkable. We present a computational semantics for the terms over which we reason and then the semantics of TPTND, where logical operators as well as a Trust operator are defined through introduction and elimination rules. We illustrate structural and metatheoretical properties, with particular focus on the ability to establish under which term evolutions and logical rules ap
    
[^80]: HyGNN: 基于超图神经网络的药物相互作用预测

    HyGNN: Drug-Drug Interaction Prediction via Hypergraph Neural Network. (arXiv:2206.12747v4 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2206.12747](http://arxiv.org/abs/2206.12747)

    本研究提出了基于超图注意力神经网络的药物相互作用预测模型HyGNN，可以基于药物的SMILES字符串进行预测，并在多个基准数据集上的表现优于其他最先进的方法。

    

    药物相互作用(DDIs)可能会影响药物功能，在最糟糕的情况下可能会导致不良药物反应(ADRs)。预测所有的药物相互作用是一个具有挑战性和关键性的问题。本文提出了一种新颖的基于超图神经网络(HyGNN)模型，仅基于药物的SMILES字符串，为药物相互作用预测问题提供了新的解决方案。为了捕捉药物之间的相似性，我们从SMILES字符串中提取药物的化学亚结构创建超图。然后，我们开发了HyGNN，包括一种新颖的基于关注度的超图边编码器，以获取药物的超边表示，以及一个解码器，以预测药物对之间的相互作用。此外，我们引入了一种新颖的超图注意机制，以自适应地学习亚结构和超边的重要性。实验结果表明，在三个基准数据集上，HyGNN的性能优于几种最先进的方法。

    Drug-Drug Interactions (DDIs) may hamper the functionalities of drugs, and in the worst scenario, they may lead to adverse drug reactions (ADRs). Predicting all DDIs is a challenging and critical problem. Most existing computational models integrate drug-centric information from different sources and leverage them as features in machine learning classifiers to predict DDIs. However, these models have a high chance of failure, especially for the new drugs when all the information is not available. This paper proposes a novel Hypergraph Neural Network (HyGNN) model based on only the SMILES string of drugs, available for any drug, for the DDI prediction problem. To capture the drug similarities, we create a hypergraph from drugs' chemical substructures extracted from the SMILES strings. Then, we develop HyGNN consisting of a novel attention-based hypergraph edge encoder to get the representation of drugs as hyperedges and a decoder to predict the interactions between drug pairs. Furthermo
    
[^81]: 计划到实践：在潜空间中组合目标的高效在线微调

    Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space. (arXiv:2205.08129v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2205.08129](http://arxiv.org/abs/2205.08129)

    本文提出了一种名为PTP的方法，利用高层规划器和潜空间中的条件生成器来分解目标成子目标，并在以前的数据上预训练条件子目标生成器和策略，然后在线微调以适应新的目标，从而训练目标导向的策略，有效提高了机器人长期目标导向的经验采集效率。

    

    通用型机器人需要多样化的行为来完成现实世界中无结构环境下的挑战性任务。为了解决这个问题，目标导向强化学习旨在获得可以在命令下达时到达可配置目标的策略，完成广泛的任务。然而，这类目标导向策略非常难以从头开始训练。本文提出了Planning to Practice（PTP），一种实现训练目标导向策略的方法，用于解决需要多个不同类型交互才能解决的长时程任务。我们的方法基于两个重要的思想。首先，我们分层地分解了到达目标的问题，使用条件子目标生成器在低级无模型策略中的潜空间中设置中间子目标的高级计划器。其次，我们提出了一种混合方法，即首先在以前收集的数据上预训练条件子目标生成器和策略，然后在线微调以适应新的目标，以提高收敛速度。实验表明，PTP可以同时提高数据效率和样本效率，使得机器人可以很高效地采集长期目标导向的经验。

    General-purpose robots require diverse repertoires of behaviors to complete challenging tasks in real-world unstructured environments. To address this issue, goal-conditioned reinforcement learning aims to acquire policies that can reach configurable goals for a wide range of tasks on command. However, such goal-conditioned policies are notoriously difficult and time-consuming to train from scratch. In this paper, we propose Planning to Practice (PTP), a method that makes it practical to train goal-conditioned policies for long-horizon tasks that require multiple distinct types of interactions to solve. Our approach is based on two key ideas. First, we decompose the goal-reaching problem hierarchically, with a high-level planner that sets intermediate subgoals using conditional subgoal generators in the latent space for a low-level model-free policy. Second, we propose a hybrid approach which first pre-trains both the conditional subgoal generator and the policy on previously collected
    
[^82]: 减少资源受限对比图像-字幕检索中的预测特征抑制

    Reducing Predictive Feature Suppression in Resource-Constrained Contrastive Image-Caption Retrieval. (arXiv:2204.13382v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2204.13382](http://arxiv.org/abs/2204.13382)

    本文提出了一种名为潜在目标解码（LTD）的方法，可以在资源受限的情况下减少预测特征抑制，从而为对比图像-字幕检索（ICR）方法提供了一种解决方案。

    

    对于训练图像-字幕检索（ICR）方法，对比损失函数是优化函数的常见选择。然而，对比ICR方法容易受到预测特征抑制的影响。预测特征是正确指示查询和候选项之间相似性的特征。然而，在训练过程中存在多个预测特征时，编码器模型往往会抑制冗余的预测特征，因为这些特征不需要学习区分正面和负面对。虽然有些预测特征在训练过程中是冗余的，但在评估过程中可能是有用的。我们提出了一种减少资源受限ICR方法中预测特征抑制的方法：潜在目标解码（LTD）。我们在对比ICR框架中添加了一个额外的解码器，以在通用句子编码器的潜在空间中重建输入字幕，从而防止图像和字幕编码器在不匹配的负面对中抑制预测特征。我们在Flikr30k和MS COCO数据集上验证了LTD，并表明它比资源受限场景中的基线方法改进了性能。

    To train image-caption retrieval (ICR) methods, contrastive loss functions are a common choice for optimization functions. Unfortunately, contrastive ICR methods are vulnerable to predictive feature suppression. Predictive features are features that correctly indicate the similarity between a query and a candidate item. However, in the presence of multiple predictive features during training, encoder models tend to suppress redundant predictive features, since these features are not needed to learn to discriminate between positive and negative pairs. While some predictive features are redundant during training, these features might be relevant during evaluation. We introduce an approach to reduce predictive feature suppression for resource-constrained ICR methods: latent target decoding (LTD). We add an additional decoder to the contrastive ICR framework, to reconstruct the input caption in a latent space of a general-purpose sentence encoder, which prevents the image and caption encod
    
[^83]: 约束MDP和局部转移条件下随机最短路径的全多项式时间逼近方案

    A Fully Polynomial Time Approximation Scheme for Constrained MDPs and Stochastic Shortest Path under Local Transitions. (arXiv:2204.04780v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2204.04780](http://arxiv.org/abs/2204.04780)

    本文提出了约束MDP和局部转移条件下随机最短路径的全多项式时间逼近方案。

    

    固定时间段约束马尔可夫决策过程（C-MDP）是在操作限制下规划随机环境中的井知模型。概率约束MDP（CC-MDP）是一种变体，允许限制违规概率，在许多安全关键应用中是必需的。CC-MDP也可以将一类MDP建模为带终点的随机最短路径（SSP），在这种情况下，存在着一种在目标概率和成本之间的权衡。本文研究了（C）C-MDP的结构，特别是涉及局部转移的重要变体。在这种变体中，状态可达性表现出一定程度的局部性和独立性。更精确地说，给定时间点上，共享某些可达未来状态的状态数量始终保持不变。即使对于规划时间为2的情况，局部转移条件下的（C）C-MDP仍是NP难的。在本文中，我们提出了（C）C-MDP的一个全多项式时间逼近方案。

    The fixed-horizon constrained Markov Decision Process (C-MDP) is a well-known model for planning in stochastic environments under operating constraints. Chance-Constrained MDP (CC-MDP) is a variant that allows bounding the probability of constraint violation, which is desired in many safety-critical applications. CC-MDP can also model a class of MDPs, called Stochastic Shortest Path (SSP), under dead-ends, where there is a trade-off between the probability-to-goal and cost-to-goal. This work studies the structure of (C)C-MDP, particularly an important variant that involves local transition. In this variant, the state reachability exhibits a certain degree of locality and independence from the remaining states. More precisely, the number of states, at a given time, that share some reachable future states is always constant. (C)C-MDP under local transition is NP-Hard even for a planning horizon of two. In this work, we propose a fully polynomial-time approximation scheme for (C)C-MDP tha
    
[^84]: 低资源情境下的知识抽取：调研与展望

    Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.08063](http://arxiv.org/abs/2202.08063)

    低资源情境下，如何让知识抽取更好地从非结构化文本中提取信息？本文调研了三种解决范式：高资源数据、更强的模型和数据与模型的结合，提出了未来的研究方向。

    

    知识抽取（KE）旨在从非结构化文本中提取结构信息，通常遭受数据匮乏和出现未见类型（低资源情境）的困扰。许多神经网络方法已广泛研究并取得了令人瞩目的表现。本文对低资源情境下KE进行文献综述，并将现有的工作系统性地分为三种范式：（1）利用高资源数据，（2）利用更强的模型，（3）同时利用数据和模型。此外，本文提出有前途的应用，并概述了未来研究的一些潜在方向。我们希望我们的调研可以帮助学术和工业界更好地理解这一领域，激发更多的创意，提升更广泛的应用。

    Knowledge Extraction (KE), aiming to extract structural information from unstructured texts, often suffers from data scarcity and emerging unseen types, i.e., low-resource scenarios. Many neural approaches to low-resource KE have been widely investigated and achieved impressive performance. In this paper, we present a literature review towards KE in low-resource scenarios, and systematically categorize existing works into three paradigms: (1) exploiting higher-resource data, (2) exploiting stronger models, and (3) exploiting data and models together. In addition, we highlight promising applications and outline some potential directions for future research. We hope that our survey can help both the academic and industrial communities to better understand this field, inspire more ideas, and boost broader applications.
    
[^85]: 基于医学概念的胎儿超声图像分类器的认知解释器

    A Cognitive Explainer for Fetal ultrasound images classifier Based on Medical Concepts. (arXiv:2201.07798v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.07798](http://arxiv.org/abs/2201.07798)

    本研究提出了一种基于关键医学概念的可解释框架，利用概念间的关系构建图卷积神经网络，解释胎儿超声图像分类器的决策过程，为临床医生提供易于理解的推理结果见解。

    

    在二维孕期检查中，胎儿标准扫描平面的检测是一项非常复杂的任务，需要广泛的医学知识和多年的培训。虽然深度神经网络(DNN)可以协助经验不足的医生完成这些任务，但它们的透明度和可解释性限制了它们的应用。本研究提出了一种基于关键医学概念的可解释框架，从临床医生的认知角度提供解释。此外，我们利用基于概念的图卷积神经网络(GCN)构建关键医学概念之间的关系。对一个私有数据集的广泛实验分析表明，所提出的方法为临床医生提供了易于理解的推理结果见解。

    Fetal standard scan plane detection during 2-D mid-pregnancy examinations is a highly complex task, which requires extensive medical knowledge and years of training. Although deep neural networks (DNN) can assist inexperienced operators in these tasks, their lack of transparency and interpretability limit their application. Despite some researchers have been committed to visualizing the decision process of DNN, most of them only focus on the pixel-level features and do not take into account the medical prior knowledge. In this work, we propose an interpretable framework based on key medical concepts, which provides explanations from the perspective of clinicians' cognition. Moreover, we utilize a concept-based graph convolutional neural(GCN) network to construct the relationships between key medical concepts. Extensive experimental analysis on a private dataset has shown that the proposed method provides easy-to-understand insights about reasoning results for clinicians.
    
[^86]: 使用较慢的在线网络实现更快的深度强化学习

    Faster Deep Reinforcement Learning with Slower Online Network. (arXiv:2112.05848v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.05848](http://arxiv.org/abs/2112.05848)

    本文改进了DQN和Rainbow两个深度强化学习算法，大大提高了它们在Atari游戏基准测试中的性能，我们的方法是在在线网络和目标网络之间引入一定的接近度，以提高深度强化学习的鲁棒性。

    

    深度强化学习算法通常使用两个网络进行价值函数优化：一个在线网络和一个目标网络，后者带有一定的延迟。使用两个独立的网络使得智能体能够对抗启发式引导时出现的问题。本文在两个流行的深度强化学习算法（即DQN和Rainbow）中引入了更新算法，以激励在线网络保持与目标网络的接近，从而提高在存在噪声更新时深度强化学习的鲁棒性。由此产生的代理称为DQN Pro和Rainbow Pro，它们在Atari基准测试上相对于原始算法表现出显著的性能提升，证明了这种深度强化学习中的简单思想的有效性。我们的论文代码可在Github.com/amazon-research/fast-rl-with-slow-updates 上获取。

    Deep reinforcement learning algorithms often use two networks for value function optimization: an online network, and a target network that tracks the online network with some delay. Using two separate networks enables the agent to hedge against issues that arise when performing bootstrapping. In this paper we endow two popular deep reinforcement learning algorithms, namely DQN and Rainbow, with updates that incentivize the online network to remain in the proximity of the target network. This improves the robustness of deep reinforcement learning in presence of noisy updates. The resultant agents, called DQN Pro and Rainbow Pro, exhibit significant performance improvements over their original counterparts on the Atari benchmark demonstrating the effectiveness of this simple idea in deep reinforcement learning. The code for our paper is available here: Github.com/amazon-research/fast-rl-with-slow-updates.
    
[^87]: 二值化 ResNet：在资源受限的边缘实现鲁棒的自动调制识别

    Binarized ResNet: Enabling Robust Automatic Modulation Classification at the resource-constrained Edge. (arXiv:2110.14357v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2110.14357](http://arxiv.org/abs/2110.14357)

    本研究提出了一个旋转二值化大型ResNet（RBLResNet），可用于资源受限的边缘自动调制识别。通过多级分类和保留低内存和计算功率的RBLResNet打包来提高准确率，达到至高94.49%的性能，优于其他二元和实值网络。

    

    近年来，深度神经网络被广泛应用于自动调制识别（AMC），并且结果非常有前途。然而，DNN具有高存储和计算需求，使它们对于资源受限的边缘网络不实用。它们也容易受到对手的攻击，这是一个重要的安全问题。本研究提出了一个旋转二值化大型ResNet（RBLResNet）用于AMC，可以在边缘网络上部署，因为其具有低内存和计算复杂度。通过两种提出的集成方法：（i）多级分类（MC），和（ii）保留低内存和计算功率的多个RBLResNet打包，可以缩小RBLResNet和现有使用浮点权重和激活函数的结构之间的性能差距。MC方法在Deepsig数据集的24种调制类别的全部测试上实现了93.39％的准确率。该性能与最先进的方法相当，同时仅使用二元权重和激活。所提出的集成方法进一步将性能提高到10 dB时94.49%的准确率，优于其他二元和实值网络。

    Recently, deep neural networks (DNNs) have been used extensively for automatic modulation classification (AMC), and the results have been quite promising. However, DNNs have high memory and computation requirements making them impractical for edge networks where the devices are resource-constrained. They are also vulnerable to adversarial attacks, which is a significant security concern. This work proposes a rotated binary large ResNet (RBLResNet) for AMC that can be deployed at the edge network because of low memory and computational complexity. The performance gap between the RBLResNet and existing architectures with floating-point weights and activations can be closed by two proposed ensemble methods: (i) multilevel classification (MC), and (ii) bagging multiple RBLResNets while retaining low memory and computational power. The MC method achieves an accuracy of $93.39\%$ at $10$dB over all the $24$ modulation classes of the Deepsig dataset. This performance is comparable to state-of
    
[^88]: 一种增加神经网络对数据质量问题的鲁棒性的调制层

    A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues. (arXiv:2107.08574v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.08574](http://arxiv.org/abs/2107.08574)

    提出了一种新颖的神经网络修正方法，用于缓解低质量和缺失数据的影响，具备神经调制特征，通过一个额外的输入的函数替换了全连接层的固定权重，使得在测试中具有调制层的模型对于数据质量的降解更加鲁棒，同时也能够节省训练时间并且不会受到插补错误的影响。

    

    数据缺失和质量是机器学习中常见的问题，特别是在高风险应用领域，如医疗保健。开发者通常只使用高质量数据精心筛选出的数据集来训练机器学习模型；然而，这会降低这些模型在生产环境中的效用。本文提出了一种新颖的神经网络修正方法，用于缓解低质量和缺失数据的影响，其中利用一个额外的输入的函数替换了全连接层的固定权重。这受启发于生物神经网络中的神经调制，皮质可以根据输入的可靠性和其他数据的存在程度上下调节输入。在测试中，使用可靠性得分作为调制信号，发现具有调制层的模型对于数据质量的降解（包括额外的缺失数据）更加鲁棒。这些模型优于插补方法，因为它们通过完全跳过插补过程节省了训练时间，并且不会受到插补错误的影响。

    Data missingness and quality are common problems in machine learning, especially for high-stakes applications such as healthcare. Developers often train machine learning models on carefully curated datasets using only high quality data; however, this reduces the utility of such models in production environments. We propose a novel neural network modification to mitigate the impacts of low quality and missing data which involves replacing the fixed weights of a fully-connected layer with a function of an additional input. This is inspired from neuromodulation in biological neural networks where the cortex can up- and down-regulate inputs based on their reliability and the presence of other data. In testing, with reliability scores as a modulating signal, models with modulating layers were found to be more robust against degradation of data quality, including additional missingness. These models are superior to imputation as they save on training time by completely skipping the imputatio
    
[^89]: 基于普适函数逼近的强化学习的在线子采样

    Online Sub-Sampling for Reinforcement Learning with General Function Approximation. (arXiv:2106.07203v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.07203](http://arxiv.org/abs/2106.07203)

    本文提出了一种基于在线子采样框架的强化学习算法，利用数据点的信息增益量来指导探索，与现有方法相比更新RL算法的策略次数大大减少，但仍保持较小的近似最优遗憾边界。

    

    现有的大多数强化学习（RL）普适函数逼近（FA）方法都专注于理解统计复杂性或遗憾边界，但这些方法的计算复杂性远未得到理解——事实上，函数类上的简单优化问题可能同样难以处理。本文通过建立一种高效的在线子采样框架来解决这个问题，该框架测量RL算法收集的数据点的信息增益，并使用该测量指导探索。对于基于价值的方法和复杂度有界的函数类，我们证明了策略只需要更新$\propto\operatorname{poly}\log(K)$ 次，就可以运行 $K$ 次RL算法而仍然实现较小的近似最优遗憾边界。与现有方法更新策略至少要 $\Omega(K)$ 次相比，我们的方法大大减少了解决方案中的优化调用次数。

    Most of the existing works for reinforcement learning (RL) with general function approximation (FA) focus on understanding the statistical complexity or regret bounds. However, the computation complexity of such approaches is far from being understood -- indeed, a simple optimization problem over the function class might be as well intractable. In this paper, we tackle this problem by establishing an efficient online sub-sampling framework that measures the information gain of data points collected by an RL algorithm and uses the measurement to guide exploration. For a value-based method with complexity-bounded function class, we show that the policy only needs to be updated for $\propto\operatorname{poly}\log(K)$ times for running the RL algorithm for $K$ episodes while still achieving a small near-optimal regret bound. In contrast to existing approaches that update the policy for at least $\Omega(K)$ times, our approach drastically reduces the number of optimization calls in solving 
    
[^90]: 一种用于系统化感知、语法和语义的通用数据集的极简主义方法

    A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics. (arXiv:2103.01403v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.01403](http://arxiv.org/abs/2103.01403)

    本文提出了一个新的数据集HINT，旨在检验机器学习通用概念的能力，包括感知、语法和语义三个层次。为了检验模型的插值和外推能力，我们设计了一个五倍交叉测试集。通过对几种最先进的模型进行广泛实验，进一步探究其局限性。

    

    受到人类掌握算术并将其推广到新问题的例外能力的启发，我们提出了一个新的数据集Handwritten arithmetic with INTegers（HINT）来检验机器学习通用概念的能力，包括感知、语法和语义三个层次。在HINT中，机器被赋予学习如何从原始信号（如图像）中感知概念（即感知），如何将多个概念结构化组合以形成有效表达式（即语法），以及如何实现概念以支持各种推理任务（即语义），全部在弱监督的情况下。我们专注于系统化通用能力，精心设计了一个五倍交叉测试集，以评估关于三层级别的学习概念的插值和外推。此外，我们还设计了一个少样本学习分割，以确定模型是否能够快速学习新概念并将其推广到更复杂的情况。为了了解现有模型的局限性，我们还对几种最先进的模型进行了广泛实验，并详细分析了它们在HINT上的表现。

    Inspired by humans' exceptional ability to master arithmetic and generalize to new problems, we present a new dataset, Handwritten arithmetic with INTegers (HINT), to examine machines' capability of learning generalizable concepts at three levels: perception, syntax, and semantics. In HINT, machines are tasked with learning how concepts are perceived from raw signals such as images (i.e., perception), how multiple concepts are structurally combined to form a valid expression (i.e., syntax), and how concepts are realized to afford various reasoning tasks (i.e., semantics), all in a weakly supervised manner. Focusing on systematic generalization, we carefully design a five-fold test set to evaluate both the interpolation and the extrapolation of learned concepts w.r.t. the three levels. Further, we design a few-shot learning split to determine whether or not models can rapidly learn new concepts and generalize them to more complex scenarios. To comprehend existing models' limitations, we
    

