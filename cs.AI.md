# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models](https://arxiv.org/abs/2403.18814) | Mini-Gemini 挖掘了VLMs的潜力，通过高分辨率视觉标记、高质量数据和VLM引导生成等方式，缩小了与先进模型之间的性能差距 |
| [^2] | [ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation](https://arxiv.org/abs/2403.18807) | 通过使用预训练的ViT模型生成的全局图像先验，为单图深度估计模型提供更详细的上下文信息，并提出了一种新的使用扩散骨干且受ViT嵌入条件约束的深度估计模型。 |
| [^3] | [Long-form factuality in large language models](https://arxiv.org/abs/2403.18802) | 该论文提出了一种通过使用大型语言模型将长篇回应分解为单个事实，并通过发送搜索查询到Google搜索，评估事实准确性的方法，并扩展了F1分数作为长篇事实性的聚合度量。 |
| [^4] | [Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction](https://arxiv.org/abs/2403.18795) | 提出了Gamba，一种单视图3D重建模型，创新地结合了大量的3D高斯点进行高效重建，并引入了基于曼巴的顺序网络，促进依赖上下文的推理，实现了线性可扩展性。 |
| [^5] | [ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object](https://arxiv.org/abs/2403.18775) | 本研究引入生成模型作为数据源，通过扩散模型生成了更多样化的背景、纹理和材料图像，建立了ImageNet-D基准评估深度模型的鲁棒性，在一系列视觉模型中显著降低了准确性高达60%。 |
| [^6] | [Superior Parallel Big Data Clustering through Competitive Stochastic Sample Size Optimization in Big-means](https://arxiv.org/abs/2403.18766) | 该论文介绍了一种新型K-means聚类算法，通过竞争性随机样本大小优化，有效提高了Big-means中的并行大数据聚类效率。 |
| [^7] | [ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place Recognition](https://arxiv.org/abs/2403.18762) | 提出了一个快速轻量级的框架，将图像和点云编码为地点特征描述符，并设计了一种有效的Field of View（FoV）转换模块来消除深度估计的必要性，从而实现了实时性能。 |
| [^8] | [Detection of subclinical atherosclerosis by image-based deep learning on chest x-ray](https://arxiv.org/abs/2403.18756) | 该研究开发了一种基于深度学习的系统，可以在胸部X射线上识别亚临床动脉硬化，为检测心血管疾病提供了新方法。 |
| [^9] | [Many-Objective Evolutionary Influence Maximization: Balancing Spread, Budget, Fairness, and Time](https://arxiv.org/abs/2403.18755) | 通过引入多目标进化算法，本研究在最大化影响和最小化种子集大小的基础上，优化了多个IM特定目标函数，包括预算、公平性、社区和时间。 |
| [^10] | [Understanding the Learning Dynamics of Alignment with Human Feedback](https://arxiv.org/abs/2403.18742) | 本研究对人类偏好对齐的学习动态进行了理论分析，显示了偏好数据集的分布如何影响模型更新速度，并提供了对训练准确度的严格保证，同时揭示了优化易于优先考虑高偏好可区分性行为的复杂现象。 |
| [^11] | [Enhancing Manufacturing Quality Prediction Models through the Integration of Explainability Methods](https://arxiv.org/abs/2403.18731) | 该研究提出了一种利用可解释性技术来增强制造质量预测模型性能的方法，通过消除无关特征进行微调，提高了性能，为降低制造成本和更好理解训练后的模型铺平了道路。 |
| [^12] | [Probabilistic Model Checking of Stochastic Reinforcement Learning Policies](https://arxiv.org/abs/2403.18725) | 该方法介绍了一种验证随机强化学习策略的方法，通过将模型检验技术与RL相结合，建立了能够通过模型检验器验证的形式模型。 |
| [^13] | [Semi-Supervised Learning for Deep Causal Generative Models](https://arxiv.org/abs/2403.18717) | 首次开发了一种利用变量之间因果关系的半监督深度因果生成模型，以最大限度地利用所有可用数据。 |
| [^14] | [Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding](https://arxiv.org/abs/2403.18715) | 这项研究提出了一种名为指示对比解码(ICD)方法，旨在减少大规模视觉-语言模型(LVLMs)推断过程中的幻觉，通过对标准和指示扰动的分布进行对比，从原始分布中减去幻觉概念。 |
| [^15] | [SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable Transient-Free 3D reconstruction from Satellite Imagery](https://arxiv.org/abs/2403.18711) | 提出了一种名为SAT-NGP的模型，在多日期卫星图像的三维重建中，通过高效的采样策略和多分辨率哈希编码，将学习时间缩短至15分钟，同时保持重建质量。 |
| [^16] | [Contrastive Learning with Orthonormal Anchors (CLOA)](https://arxiv.org/abs/2403.18699) | 该研究提出了一种新的损失函数称为正交锚点回归损失，用于解开嵌入聚类，显著增强嵌入的独特性 |
| [^17] | [Annolid: Annotate, Segment, and Track Anything You Need](https://arxiv.org/abs/2403.18690) | Annolid是一个基于深度学习的软件包，旨在对视频文件中的研究目标进行分割、标记和跟踪，主要集中在动物行为分析上，通过弹性、无标记跟踪多个动物并通过文本命令自动遮罩和分割可识别的动物和物体。 |
| [^18] | [TransFusion: Contrastive Learning with Transformers](https://arxiv.org/abs/2403.18681) | TransFusion的主要创新在于定义了对比学习领域中的两个基本问题的理论极限，并成功实现了从复杂的现实世界数据中提取特征以改善分类精度。 |
| [^19] | [Aiming for Relevance](https://arxiv.org/abs/2403.18668) | 引入了与临床背景相一致的新颖生命体征预测性能指标，通过捕捉与临床规范的偏差、整体趋势和趋势偏差，为早期发现不良事件铺平道路。 |
| [^20] | [INEXA: Interactive and Explainable Process Model Abstraction Through Object-Centric Process Mining](https://arxiv.org/abs/2403.18659) | INEXA是一个交互式、可解释的过程模型抽象工具，能够帮助用户在不同粒度级别上探索和理解发现的过程模型。 |
| [^21] | [Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic Learning over Low-power Devices](https://arxiv.org/abs/2403.18607) | 本文探索了基于时间分割复用概念的Spikewhisper，允许攻击者在联合神经形态学学习系统中实施难以被检测的斯派克后门攻击。 |
| [^22] | [RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos](https://arxiv.org/abs/2403.18600) | 提出了一种新的实际设置，称为指导视频中的自适应程序规划，克服了在实际场景中步骤长度变化的模型不具有泛化能力、理解步骤之间的时间关系知识对于生成合理且可执行的计划至关重要以及用步骤级标签或序列级标签标注指导视频耗时且劳动密集的问题 |
| [^23] | [Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote Sensing Image Understanding](https://arxiv.org/abs/2403.18593) | 通过定义语义独立区域（SIRs）并设计HOmogeneous视觉tOKenizer (HOOK)，实现了使用有意义的基本元素来加强遥感图像理解。 |
| [^24] | [Physics-Informed Graph Neural Networks for Water Distribution Systems](https://arxiv.org/abs/2403.18570) | 提出了一种用于水配系统的物理信息图神经网络模型，利用水力原理以无监督方式重建水力状态特征。 |
| [^25] | [PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop Prediction](https://arxiv.org/abs/2403.18569) | 提出了面向动态IR掉电预测的PDN感知GNN-CNN异构网络，引入了PDNGraph图结构和双分支异构网络PDNNet，以同时考虑PDN结构和单元-PDN关系，有助于更好地预测IR掉电。 |
| [^26] | [Neural Architecture Search for Sentence Classification with BERT](https://arxiv.org/abs/2403.18547) | 本文质疑了仅在BERT网络顶部添加单个输出层作为分类头的常规做法，通过进行AutoML搜索找到了在较小计算成本下优于当前单层的架构，并在GLUE数据集上进行了验证。 |
| [^27] | [Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes](https://arxiv.org/abs/2403.18546) | 本文提出了一种利用抓取热图引导的高效局部抓取生成方法，从全局到局部语义到点的方式推断，显著提高了抓取准确性和多样性。 |
| [^28] | [A Path Towards Legal Autonomy: An interoperable and explainable approach to extracting, transforming, loading and computing legal information using large language models, expert systems and Bayesian networks](https://arxiv.org/abs/2403.18537) | 本文提出了一种可互操作和可解释的方法，利用大语言模型、专家系统和贝叶斯网络来提取、转换、加载和计算法律信息，从而实现通向法律自治的路径。 |
| [^29] | [A Novel Behavior-Based Recommendation System for E-commerce](https://arxiv.org/abs/2403.18536) | 提出了一种基于顾客行为的推荐方法，通过利用顾客在电子商务平台上的自然行为来生成准确的推荐结果 |
| [^30] | [Improving Line Search Methods for Large Scale Neural Network Training](https://arxiv.org/abs/2403.18519) | 本文改进了大规模神经网络训练的线搜索方法，通过将ADAM的动量项集成到Armijo线搜索中，实现了高效的大规模训练，并且优于以往的方法和Adam的调整学习率。 |
| [^31] | [Faster Convergence for Transformer Fine-tuning with Line Search Methods](https://arxiv.org/abs/2403.18506) | 将Armijo线搜索与Adam优化器相结合，通过在本地单元执行线搜索，实现了在Transformer微调中收敛速度更快的优化方法。 |
| [^32] | [Impact of Employing Weather Forecast Data as Input to the Estimation of Evapotranspiration by Deep Neural Network Models](https://arxiv.org/abs/2403.18489) | 利用天气预报数据作为深度神经网络模型估计蒸散发的输入，解决了使用FAO56-PM方法计算ET0时太阳辐射参数不易获取的问题 |
| [^33] | [Synthesizing EEG Signals from Event-Related Potential Paradigms with Conditional Diffusion Models](https://arxiv.org/abs/2403.18486) | 通过引入使用无分类器引导的条件扩散模型，可以直接生成特定主体、会话和类别的EEG数据，结果表明该模型生成的数据与真实数据相似。 |
| [^34] | [Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain Adaptive Segmentation of 3D Point Clouds](https://arxiv.org/abs/2403.18469) | 提出了一种密度引导的翻译器（DGT）以改善三维点云的合成到真实无监督领域自适应分割，解决了不同传感器采样模式和不完整训练策略的限制。 |
| [^35] | [CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration](https://arxiv.org/abs/2403.18459) | CoBOS提出了一种新颖的在线基于约束的调度方法，在人机协作中实现了机器人对不确定事件的适应性，大大减轻了用户的压力，提高了工作效率。 |
| [^36] | [CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in Resource-Constrained CPS and IoT](https://arxiv.org/abs/2403.18451) | CoRAST提出了一种新颖的学习框架，利用基础模型(FMs)增强了分布式、相关的异构数据的分析。 |
| [^37] | [U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models](https://arxiv.org/abs/2403.18425) | 引入了U-Sketch框架，具有U-Net类型的潜在边缘预测器，能够有效地改进草图到图像扩散模型的空间布局生成。 |
| [^38] | [BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text](https://arxiv.org/abs/2403.18421) | BioMedLM是一个27亿参数的语言模型，在PubMed文献上训练，可以在生物医学领域表现出色，尤其适用于多项选择问题回答和患者提问。 |
| [^39] | [A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is Critical for Semi-supervised Classification](https://arxiv.org/abs/2403.18407) | 通过提出一种轻量级基于通道的集成方法，将多个较差的伪标签有效地合并为理论上保证的无偏差和低方差的一个伪标签，解决半监督学习中自我训练模型产生的有偏差和高方差预测问题。 |
| [^40] | [An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM](https://arxiv.org/abs/2403.18406) | 本研究提出了一种新颖的策略，使用单一的Vision Language Model (VLM) 来进行零样本视频问答，将视频转换为单个合成图像以实现视频理解。 |
| [^41] | [Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval](https://arxiv.org/abs/2403.18405) | 设计一种新颖的几轮工作流程，专门用于法律案例的相关判断，能够通过模仿人类注释者的过程并整合专家推理，提高相关性判断的准确性。 |
| [^42] | [Colour and Brush Stroke Pattern Recognition in Abstract Art using Modified Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/2403.18397) | 本文通过引入改进的深度卷积生成对抗网络(mDCGAN)，针对高质量艺术品生成进行了研究，解决了普遍训练问题，有效探索抽象绘画中的颜色和笔触模式。 |
| [^43] | [FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion](https://arxiv.org/abs/2403.18388) | 通过前向时间偏差校正技术，本研究提出一种能够提高ANN-SNN转换准确性、避免计算开销的新方法，并通过理论发现表明，适当的时间偏差校准能够将转换误差降低至零。 |
| [^44] | [Generative Multi-modal Models are Good Class-Incremental Learners](https://arxiv.org/abs/2403.18383) | 多模态生成模型为类增量学习提出了一种新的框架，可以直接为图像生成标签。 |
| [^45] | [Improving Attributed Text Generation of Large Language Models via Preference Learning](https://arxiv.org/abs/2403.18381) | 通过偏好学习建模和引入自动偏好优化框架，该研究解决了大型语言模型生成不可靠内容的挑战，并提出了一种自动生成归因偏好数据的方法。 |
| [^46] | [IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining Useful Life Prediction](https://arxiv.org/abs/2403.18379) | 提出了一种基于MLPs的IIP-Mixer架构，旨在通过在内部补丁和跨补丁维度进行混合操作，实现锂离子电池剩余寿命预测。 |
| [^47] | [Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR](https://arxiv.org/abs/2403.18364) | 提出了一种面向工业物联网用户设备（IIoT UEs）的意图感知DRL上行动态调度器，通过深度强化学习（DRL）学习如何调度通信资源，并利用图结构的简化方案加速收敛，相较于传统调度方案，能有效保证IIoT UEs的意图表达。 |
| [^48] | [Generating Diverse Agricultural Data for Vision-Based Farming Applications](https://arxiv.org/abs/2403.18351) | 提出了一种专门用于生成合成农业场景的程序化模型，能够模拟植物的生长阶段、土壤条件和光照变化，为精准农业中的计算机视觉任务提供了全面资源，验证了该模型在提供机器学习训练数据方面的潜力 |
| [^49] | [A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal Holes](https://arxiv.org/abs/2403.18347) | 本论文提出了一种基于量子计算的快速模糊c-均值技术，用于快速检测太阳日冕空洞（CHs）的区域。 |
| [^50] | [LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models](https://arxiv.org/abs/2403.18344) | 该论文提出了一个使用大型语言模型的可解释性车道变换预测模型，将车道变换预测任务重新构建为语言建模问题，以提高长期预测准确性和可解释性。 |
| [^51] | [mALBERT: Is a Compact Multilingual BERT Model Still Worth It?](https://arxiv.org/abs/2403.18338) | 提出了第一个版本的多语言小型ALBERT模型，旨在解决预训练语言模型对道德和生态的影响，并通过与传统多语言PLM的比较，在经典NLP任务中评估了该模型的性能。 |
| [^52] | [Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications](https://arxiv.org/abs/2403.18327) | 本文提出了一种方法，利用两个LLM的副本与验证器结合使用，能够自动评估其在自然语言描述和正式规范之间转换的能力，无需额外的人工输入。 |
| [^53] | [Chinese Offensive Language Detection:Current Status and Future Directions](https://arxiv.org/abs/2403.18314) | 总体而言，这篇论文讨论了在中文中检测 offensive 语言的挑战，并强调了开发解决这一问题的特定模型和工具。 |
| [^54] | [A thermodynamically consistent physics-informed deep learning material model for short fiber/polymer nanocomposites](https://arxiv.org/abs/2403.18310) | 提出了一种基于物理信息深度学习的热力学一致材料模型，用于研究短纤维/聚合物纳米复合材料的行为，通过组合深度学习网络预测内部变量并定义整个系统的热力学状态。 |
| [^55] | [A Recommender System for NFT Collectibles with Item Feature](https://arxiv.org/abs/2403.18305) | 该研究提出了一种针对NFT的推荐系统，综合利用NFT交易记录和外部项目特征等多种数据源，通过数据高效的基于图的方法生成个性化推荐，并利用超出用户-项目互动的输入验证了模型的有效性。 |
| [^56] | [Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives](https://arxiv.org/abs/2403.18301) | 提出了SelMix，一种选择性混合的廉价微调技术，用于优化预训练模型以实现所需的非可分解目标。 |
| [^57] | [GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm](https://arxiv.org/abs/2403.18296) | GeNet提出了一种基于图神经网络的语义通信范式，通过将数据转换为图结构、利用编码器提取语义信息并利用解码器重建信息的方法来实现抗噪声任务导向通信。 |
| [^58] | [Few-Shot Recalibration of Language Models](https://arxiv.org/abs/2403.18286) | 提出了一种新框架，用于少样本特定切片重校准语言模型，实现在任意分布片段上获得校准的置信度估计。 |
| [^59] | [Identification and Uses of Deep Learning Backbones via Pattern Mining](https://arxiv.org/abs/2403.18278) | 通过模式挖掘识别和使用深度学习骨干，研究了深度学习如何进行预测的核心机制，提出了集合覆盖风格问题和相关的启发式方法，并表明其收敛到ILP公式的帕累托均衡点。 |
| [^60] | [DSF-GAN: DownStream Feedback Generative Adversarial Network](https://arxiv.org/abs/2403.18267) | DSF-GAN提出了一种新架构，通过在训练中结合下游预测模型的反馈信息，增强了生成器的损失函数，从而提高了合成样本的实用性。 |
| [^61] | [Enhancing Generative Class Incremental Learning Performance with Model Forgetting Approach](https://arxiv.org/abs/2403.18258) | 本研究通过引入遗忘机制提升生成式类增量学习性能。 |
| [^62] | [Manipulating Neural Path Planners via Slight Perturbations](https://arxiv.org/abs/2403.18256) | 在这篇论文中，我们提出了一种新颖的方法，可以通过轻微扰动来指定和注入各种隐藏的恶意行为，即后门，到神经路径规划器中。 |
| [^63] | [Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models](https://arxiv.org/abs/2403.18252) | 提出了Visual Table，一种为MLLMs量身定制的新型视觉表示，通过提供层次化文本描述的全面视觉场景来弥补现有视觉表示的不足。 |
| [^64] | [Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check](https://arxiv.org/abs/2403.18243) | 该论文提出了一种面向对话式问答的会话级RAG方法，通过细粒度检索增强和自检，实现了问句理解和相关信息获取，相较于现有方法具有显著优势。 |
| [^65] | [NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation](https://arxiv.org/abs/2403.18241) | 提出一种新颖的空间感知3D形状生成框架，利用2D平面表示增强建模，并结合混合形状表示技术直接学习连续有向距离场表示，从而确保空间一致性和降低内存使用。 |
| [^66] | [Large Language Models Need Consultants for Reasoning: Becoming an Expert in a Complex Human System Through Behavior Simulation](https://arxiv.org/abs/2403.18230) | 通过行为模拟，大型语言模型结合推理强化方法，在复杂人类系统中展现出与人类相媲美的推理能力。 |
| [^67] | [A Transformer-Based Framework for Payload Malware Detection and Classification](https://arxiv.org/abs/2403.18223) | 本文提出了一种基于Transformer的DPI算法，旨在检测恶意流量，通过学习复杂的序列数据内容并推广到类似场景中。 |
| [^68] | [From Two-Dimensional to Three-Dimensional Environment with Q-Learning: Modeling Autonomous Navigation with Reinforcement Learning and no Libraries](https://arxiv.org/abs/2403.18219) | 本研究通过强化学习算法在二维和三维环境中的表现，探讨了在没有预制库的情况下通过计算数学开发算法的可行性。 |
| [^69] | [Leveraging Large Language Models for Fuzzy String Matching in Political Science](https://arxiv.org/abs/2403.18218) | 提出利用大型语言模型来解决政治科学中模糊字符串匹配问题，可显著提高平均精度达39%，更易使用且具有鲁棒性 |
| [^70] | [Preference-Based Planning in Stochastic Environments: From Partially-Ordered Temporal Goals to Most Preferred Policies](https://arxiv.org/abs/2403.18212) | 使用偏序时序目标，将部分有序偏好映射到MDP策略偏好，并通过引入序理论实现最优策略的合成。 |
| [^71] | [Long and Short-Term Constraints Driven Safe Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2403.18209) | 本文提出了一种基于长期和短期约束的新算法用于安全强化学习，在自动驾驶任务中可以同时保证车辆的短期和长期安全性。 |
| [^72] | [An Evolutionary Network Architecture Search Framework with Adaptive Multimodal Fusion for Hand Gesture Recognition](https://arxiv.org/abs/2403.18208) | 提出了一种演化网络架构搜索框架，具有自适应多模态融合，可以自动构建不同架构的多模态网络，并考虑了来自不同输入流的数据。 |
| [^73] | [Exploring the Privacy Protection Capabilities of Chinese Large Language Models](https://arxiv.org/abs/2403.18205) | 设计了一个三层渐进式框架来评估语言系统中的隐私，全面评估大型语言模型对私人信息的敏感性以及其防范隐私侵犯的有效性 |
| [^74] | [EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning Applications](https://arxiv.org/abs/2403.18203) | 提出了一个面向生命科学的开源端到端机器学习管道，为生物信息学社区提供了一个无需编程技能即可分析复杂生物数据的用户友好界面。 |
| [^75] | [Looking Beyond What You See: An Empirical Analysis on Subgroup Intersectional Fairness for Multi-label Chest X-ray Classification Using Social Determinants of Racial Health Inequities](https://arxiv.org/abs/2403.18196) | 通过考虑社会决定因素中的复杂相互作用，提出了一种简单而强大的方法，实现跨受保护群体的准确诊断结果和公平性，为高维胸部X射线多标签分类带来创新。 |
| [^76] | [SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly Network](https://arxiv.org/abs/2403.18195) | 介绍了单步组装错误校正任务和LEGO错误校正组装数据集（LEGO-ECA），提出了用于这一任务的自校正组装网络（SCANet）。 |
| [^77] | [Can AI Models Appreciate Document Aesthetics? An Exploration of Legibility and Layout Quality in Relation to Prediction Confidence](https://arxiv.org/abs/2403.18183) | 本研究探讨了AI模型在文档理解任务中对于布局和图像数据有益的影响，以及AI是否能有效捕捉文件美学的微妙之处。 |
| [^78] | [Mechanisms of non-factual hallucinations in language models](https://arxiv.org/abs/2403.18167) | 研究揭示了语言模型中非事实性幻觉的两个通用机制：主题属性知识不足和未能正确选择对象属性，这有助于深入理解和减轻幻觉。 |
| [^79] | [Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models](https://arxiv.org/abs/2403.18159) | 通过信号传播分析，提出了一种改进大型语言模型的量化知识蒸馏方法，并提供了ov-freeze稳定KD-QAT过程的洞察。 |
| [^80] | [Large Language Models Produce Responses Perceived to be Empathic](https://arxiv.org/abs/2403.18148) | 通过两项研究，发现大型语言模型生成的回复在共情性方面被认为比人类撰写的回复更具有共情性，这表明了在人际支持方面使用LLMs的潜力。 |
| [^81] | [A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution](https://arxiv.org/abs/2403.18145) | 该研究提出了一种名为Switchable-Edge Search（SES）的A*风格算法，用于实现智能体通过顺序的重新调度，最佳变体在效率上比基线快4倍。 |
| [^82] | [Juru: Legal Brazilian Large Language Model from Reputable Sources](https://arxiv.org/abs/2403.18140) | Juru 模型通过从巴西法律来源提取的19亿个唯一标记，展示了领域专门化可以在减少预训练数据量方面发挥作用，但这种专门化会导致同一语言中其他知识领域性能下降。 |
| [^83] | [Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs](https://arxiv.org/abs/2403.18136) | 提出了一种基于解释的方法来识别GNN中的后门训练图，设计了七种新的度量指标以更有效地检测后门攻击，并且通过自适应攻击进行了方法评估。 |
| [^84] | [AE SemRL: Learning Semantic Association Rules with Autoencoders](https://arxiv.org/abs/2403.18133) | 使用自动编码器学习时间序列数据中的语义关联关系，实验表明这种方法速度快上百倍 |
| [^85] | [Recommendation of data-free class-incremental learning algorithms by simulating future data](https://arxiv.org/abs/2403.18132) | 通过模拟未来数据流，推荐无数据的类递增学习算法，实验结果表明方法胜过竞争基线 |
| [^86] | [Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization](https://arxiv.org/abs/2403.18120) | 通过将非正式的数学陈述翻译为形式的Isabelle代码并进行自动验证，我们提供了一种机制，可以自动拒绝在内部一致性方面与形式化问题陈述不一致的解决方案。 |
| [^87] | [QuakeSet: A Dataset and Low-Resource Models to Monitor Earthquakes through Sentinel-1](https://arxiv.org/abs/2403.18116) | QuakeSet提出了一个数据集和低资源模型，利用Sentinel-1来监测地震。通过社交媒体图像进行地震监测在危机管理中显示出了有效性，但在估计地震的严重性和特征方面仍存在限制。 |
| [^88] | [Large Language Models for Education: A Survey and Outlook](https://arxiv.org/abs/2403.18105) | 大型语言模型在教育领域的应用调研总结了LLMs在教育中的各种技术应用，包括学生和教师辅助、自适应学习和商业工具，提出了未来研究机会和潜在方向。 |
| [^89] | [Towards Explainable Clustering: A Constrained Declarative based Approach](https://arxiv.org/abs/2403.18101) | 该论文提出了一种新颖的可解释的约束聚类方法ECS，旨在找到高质量且可解释的聚类，强调在构建聚类时应考虑经典聚类标准和可解释性。 |
| [^90] | [Driving Intelligent IoT Monitoring and Control through Cloud Computing and Machine Learning](https://arxiv.org/abs/2403.18100) | 通过边缘计算和物联网的组合，可以减少延迟，提高效率。 |
| [^91] | [GPTs and Language Barrier: A Cross-Lingual Legal QA Examination](https://arxiv.org/abs/2403.18098) | 本研究探讨了GPT在跨语言法律问答领域的应用，通过分析英语和日语提示对性能的影响，为发展更高效准确的跨语言问答解决方案做出贡献。 |
| [^92] | [Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models](https://arxiv.org/abs/2403.18093) | 将提示技术作为检索系统的最后阶段，通过BM25预排序和基于BERT的重新排序的支持，可以显著提高法律文件检索的准确性。 |
| [^93] | [Paths to Equilibrium in Normal-Form Games](https://arxiv.org/abs/2403.18079) | 本文研究在多智体强化学习中的策略序列，探讨满足特定约束的策略路径，即满足路径，对于构建终止于均衡策略的路径具有重要意义。 |
| [^94] | [State of the art applications of deep learning within tracking and detecting marine debris: A survey](https://arxiv.org/abs/2403.18067) | 深度学习技术在海洋垃圾领域取得了长足进展，特别是YOLO系列在目标检测方面表现优异，但研究显示当前缺乏全面的水下垃圾数据库，这成为了进一步研究的瓶颈。 |
| [^95] | [Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer](https://arxiv.org/abs/2403.18063) | 该论文提出了光谱卷积变压器 (SCT)，通过结合局部信息的卷积操作和全局信息的复杂傅里叶基础，实现了对视觉变压器中实部和复部多视图光谱算子的协调，从而实现了更好的性能。 |
| [^96] | [ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models through Geometric Decomposition](https://arxiv.org/abs/2403.18062) | 通过几何分解并利用大型语言模型，我们提出了一种零样本任务导向抓取方法，通过最少的信息实现智能抓取。 |
| [^97] | [COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning](https://arxiv.org/abs/2403.18058) | COIG-CQIA 是一个高质量的中文指令微调数据集，旨在构建一个多样化、广泛的指令微调数据集，以更好地使模型行为与人类交互保持一致。 |
| [^98] | [Prioritized League Reinforcement Learning for Large-Scale Heterogeneous Multiagent Systems](https://arxiv.org/abs/2403.18057) | 提出了一种优先级异构联盟强化学习（PHLRL）方法，旨在解决大规模异构合作问题，通过记录智能体探索过的各种策略并建立异构联盟来优化未来的策略。 |
| [^99] | [Self-Clustering Hierarchical Multi-Agent Reinforcement Learning with Extensible Cooperation Graph](https://arxiv.org/abs/2403.18056) | 本论文提出了一种名为Hierarchical Cooperation Graph Learning（HCGL）的新型分层MARL模型，其中代理的行为受可扩展合作图（ECG）的拓扑结构引导，解决了一般多智能体问题。 |
| [^100] | [Supervisory Prompt Training](https://arxiv.org/abs/2403.18051) | 提出了一种督导提示训练（SPT）方法，利用双LLM系统生成高效提示并引入影响分数概念，通过优化提示成功提高了LLMs的性能。 |
| [^101] | [Predicting species occurrence patterns from partial observations](https://arxiv.org/abs/2403.18028) | 提出了一个问题，即采用卫星图像和其他物种出现信息来预测物种出现模式，并提出了一个通用模型R-Tran，可以利用部分观测数据进行预测，优于其他方法。 |
| [^102] | [Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER](https://arxiv.org/abs/2403.18025) | 提出了Mask Specific Language Modeling（MSLM）方法来改善LM在微调过程中对目标领域知识的敏感性，通过加权领域特定术语的重要性进行学习。 |
| [^103] | [Semi-Supervised Image Captioning Considering Wasserstein Graph Matching](https://arxiv.org/abs/2403.17995) | 提出了一种考虑Wasserstein图匹配的半监督图像字幕生成方法，用于解决半监督图像字幕生成中的困境 |
| [^104] | [Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence](https://arxiv.org/abs/2403.17993) | 人工智能对通过创新性使用深度神经网络推动湍流减少的拉格朗日模型具有重要影响，为AI和湍流研究之间紧密交织的未来铺平道路。 |
| [^105] | [Interpretable cancer cell detection with phonon microscopy using multi-task conditional neural networks for inter-batch calibration](https://arxiv.org/abs/2403.17992) | 提出了一个多任务条件神经网络框架，用于解释性癌细胞检测的声子显微镜，实现了批间校准和时间分辨声子信号精准细胞分类，实现了89.22%的平衡精度和0.5秒的快速分类。 |
| [^106] | [Holographic Global Convolutional Networks for Long-Range Prediction Tasks in Malware Detection](https://arxiv.org/abs/2403.17978) | 提出了一种利用全息全局卷积网络（HGConv）和全息简化表示（HRR）属性的方法，不需要复杂的核计算或设计，在恶意软件检测领域取得了新的SOTA结果。 |
| [^107] | [Deep Generative Domain Adaptation with Temporal Attention for Cross-User Activity Recognition](https://arxiv.org/abs/2403.17958) | 深度生成式领域自适应方法 DGDATA 独特地在领域自适应过程中识别和整合了时间关系。 |
| [^108] | [A Note On Lookahead In Real Life And Computing](https://arxiv.org/abs/2403.17942) | 本文旨在学习、理解和探索前瞻概念，并设计新颖的模型作为解决现实世界问题的解决方案。 |
| [^109] | [Fake or JPEG? Revealing Common Biases in Generated Image Detection Datasets](https://arxiv.org/abs/2403.17608) | 许多AI生成图像检测数据集存在与JPEG压缩和图像大小相关的偏见，去除这些偏见可以显著提高对JPEG压缩的稳健性并显著改变检测器的跨生成器性能。 |
| [^110] | [Imitating Cost-Constrained Behaviors in Reinforcement Learning](https://arxiv.org/abs/2403.17456) | 该论文介绍了在强化学习中模仿受成本约束的行为的重要性，提出了模仿学习在受约束设置下的应用，并探讨了在实际领域中专家行为受限制因素影响的问题。 |
| [^111] | [MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification](https://arxiv.org/abs/2403.17421) | 引入了基于多智能体强化学习的MA4DIV方法，将搜索结果多样化建模为多个智能体之间的合作任务，直接优化多样性指标，如$\alpha$-NDCG，以实现高训练效率。 |
| [^112] | [SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies](https://arxiv.org/abs/2403.17219) | SeSaMe框架利用大型语言模型为心理健康研究中的参与者模拟自我报告，减轻了他们的负担 |
| [^113] | [Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models](https://arxiv.org/abs/2403.16915) | 本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调，在专题文档检索中显著改善了效果。 |
| [^114] | [LLMs Are Few-Shot In-Context Low-Resource Language Learners](https://arxiv.org/abs/2403.16512) | 该研究对25种低资源语言和7种相对较高资源语言上的情境学习（ICL）及其跨语言变体进行了研究，发现了在低资源语言中使用LLMs进行ICL的有效性，提出了替代方法查询对齐，并为低资源语言的ICL提供了宝贵见解。 |
| [^115] | [DeepMachining: Online Prediction of Machining Errors of Lathe Machines](https://arxiv.org/abs/2403.16451) | DeepMachining是一种基于深度学习的AI系统，可以在线预测车床机床加工操作的误差，通过预训练和微调模型，实现了高准确性预测，是首批使用预训练深度学习模型预测车床机床加工误差的工厂实验之一。 |
| [^116] | [$\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models](https://arxiv.org/abs/2403.16432) | 基于提示的语言模型的优化过程揭示了生成对抗提示以误导模型的见解，引发了对该范式对抗性脆弱性的担忧。 |
| [^117] | [Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation](https://arxiv.org/abs/2403.16427) | Re2LLM是为基于会话的推荐提出的反射式强化大型语言模型，引导LLMs专注于更准确推荐所需的专业知识，实现有效和高效。 |
| [^118] | [Enhancing Programming Education with ChatGPT: A Case Study on Student Perceptions and Interactions in a Python Course](https://arxiv.org/abs/2403.15472) | 本研究探讨了将ChatGPT整合到Python编程课程中对学习的影响，揭示了学生对ChatGPT的积极态度，并提供了关于其在增强编程教育体验中作用的见解。 |
| [^119] | [Solving a Real-World Package Delivery Routing Problem Using Quantum Annealers](https://arxiv.org/abs/2403.15114) | 这项研究开发了一种量子-经典混合求解器 Q4RPD，旨在解决现实世界的包裹投递路径问题，避免了问题简化和技术捷径，针对包裹重量和尺寸的真实约束条件。 |
| [^120] | [Learning Quadruped Locomotion Using Differentiable Simulation](https://arxiv.org/abs/2403.14864) | 本文提出了一种新的可微分仿真框架，通过将复杂的全身仿真解耦为两个单独的连续域，并与更精确的模型对齐，来克服四足动作中的不连续性挑战。 |
| [^121] | [The opportunities and risks of large language models in mental health](https://arxiv.org/abs/2403.14814) | 大型语言模型在心理健康领域有望提供新颖的解决方案，但应注意其应用可能带来的风险，并积极采取策略减轻这些风险。 |
| [^122] | [Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity](https://arxiv.org/abs/2403.13374) | 通过提出新的Robust Average Gradient Algorithm（RAGA），本研究在联邦学习中解决了恶意拜占庭攻击和数据异构性的问题，实现了在非凸损失函数和异构数据集上的收敛性分析，并展示了RAGA的良好收敛性能。 |
| [^123] | [Functional Graph Convolutional Networks: A unified multi-task and multi-modal learning framework to facilitate health and social-care insights](https://arxiv.org/abs/2403.10158) | 该论文提出了一个新颖的函数图卷积网络框架，结合了函数数据分析和图卷积网络，解决了数字健康和纵向研究中的多任务和多模态学习复杂性，关键创新包括任务特定嵌入组件、执行分类、回归和预测的能力，以及创建知识图进行数据解释。 |
| [^124] | [Sabi\'a-2: A New Generation of Portuguese Large Language Models](https://arxiv.org/abs/2403.09887) | Sabi'a-2是一代新的葡萄牙大型语言模型，其中的Sabi'a-2 Medium模型在多个考试中的表现超越了GPT-4，且在大多数考试中超过了GPT-3.5，同时专业化对模型的性能有显著影响，可在无需增大模型尺寸的情况下以比GPT-4便宜10倍的价格提供。 |
| [^125] | [Shapley Values-Powered Framework for Fair Reward Split in Content Produced by GenAI](https://arxiv.org/abs/2403.09700) | 提出了一种利用Shapley Values量化艺术家对生成模型所做贡献的方法，以实现模型开发者和数据提供者之间合作的公平分配。 |
| [^126] | [ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text](https://arxiv.org/abs/2403.09131) | ProSwitch通过知识引导的指令微调，在专业和非专业风格之间生成文本，并在专业性评估和质量评估方面表现出优越性。 |
| [^127] | [SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces](https://arxiv.org/abs/2403.07711) | 提出了一种基于状态空间模型（SSMs）的方法，用于解决使用扩散模型生成长视频序列时注意力层内存消耗增长快、限制较大的问题 |
| [^128] | [Decoupled Data Consistency with Diffusion Purification for Image Restoration](https://arxiv.org/abs/2403.06054) | 通过分离反向过程和数据一致性步骤，提出了一种新颖的基于扩散的图像恢复求解器。 |
| [^129] | [Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference](https://arxiv.org/abs/2403.05465) | 引入了对数正定编码（LP）和LP量化（LPQ）框架，采用基因算法寻找最优的LP参数，设计了统一的混合精度LP加速器（LPA）体系结构，可动态适应DNN参数分布，减少量化和完整精度模型之间的表示性差异。 |
| [^130] | [NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models](https://arxiv.org/abs/2403.03100) | NaturalSpeech 3利用分解设计的扩散模型实现零-shot方式生成自然语音 |
| [^131] | [SoftTiger: A Clinical Foundation Model for Healthcare Workflows](https://arxiv.org/abs/2403.00868) | SoftTiger是一个专为医疗工作流设计的临床大型语言模型，通过处理临床笔记的结构化，实现了基本临床任务以及更复杂的下游临床任务的执行。 |
| [^132] | [LLMs in Political Science: Heralding a New Era of Visual Analysis](https://arxiv.org/abs/2403.00154) | 本文旨在提高使用Gemini进行政治科学图像内容分析的可行性意识，并展示Gemini在对象检测方面的高准确性。 |
| [^133] | [Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation](https://arxiv.org/abs/2402.18920) | 该研究提出了一个统一的框架，结合光谱和空间域的映射，以预测3D形状之间的点对应和形状插值，相比先前方法，取得更准确、平滑的点对应结果，并且在计算上更高效。 |
| [^134] | [Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization](https://arxiv.org/abs/2402.17574) | Agent-Pro提出了一种基于LLM的代理，通过策略级别的反思和优化，可以从互动经验中学习并逐步提升其行为策略。 |
| [^135] | [Probing Multimodal Large Language Models for Global and Local Semantic Representation](https://arxiv.org/abs/2402.17304) | 通过研究发现，多模态大型语言模型的中间层能够更好地编码全局语义信息，在视觉-语言任务中表现出更好的性能。顶层可能过多关注局部信息，导致理解全局信息的能力下降。 |
| [^136] | [Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2402.15764) | PEP提出了一个新方法来改善LLMs的数学能力，通过在推理之前细化和阐明问题背景，提升全局上下文建模能力，减少解析困难。 |
| [^137] | [Structure Guided Large Language Model for SQL Generation](https://arxiv.org/abs/2402.13284) | 通过引入结构信息，提出了一个结构引导的SQL生成模型，以改善大型语言模型生成SQL的准确性和可执行性。 |
| [^138] | [Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling](https://arxiv.org/abs/2402.11800) | 延迟更新的随机逼近方案在时间变化有界延迟下，保证了每次迭代快速收敛到固定点周围的球体，界限依赖于最大延迟和混合时间。 |
| [^139] | [GPT-4's assessment of its performance in a USMLE-based case study](https://arxiv.org/abs/2402.09654) | 本研究探讨了GPT-4在医疗应用中的表现评估。实验结果表明，反馈对相对置信度有影响，但并不一致地增加或减少。 |
| [^140] | [Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey](https://arxiv.org/abs/2402.09283) | 这篇调查提供了LLM对话安全性的全面概述，涵盖了攻击、防御和评估三个关键方面，旨在提高对该主题的理解并促进进一步的研究。 |
| [^141] | [OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2402.01739) | OpenMoE是一种开源的混合专家语言模型，通过训练和发布一系列具有可复现性的解码器模型，我们确认了MoE模型相比密集模型具有更有利的成本效益平衡，并且进行了对路由机制的深入分析，得出了三个重要发现。 |
| [^142] | [Learning to Act without Actions](https://arxiv.org/abs/2312.10812) | 通过从视频中恢复潜在动作信息，LAPO能够训练可以迅速微调为专家级策略的潜在动作策略。 |
| [^143] | [World Models via Policy-Guided Trajectory Diffusion](https://arxiv.org/abs/2312.08533) | 这项工作提出了一个新颖的世界建模方法，Policy-Guided Trajectory Diffusion (PolyGRAD)，通过扩散模型一次生成整个在线策略轨迹，避免了自回归模型中随着轨迹长度增长而积累的预测误差。 |
| [^144] | [FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects](https://arxiv.org/abs/2312.08344) | 提出了FoundationPose，一个统一的基础模型，支持新物体的6D姿势估计和跟踪，通过神经隐式表示和大规模训练实现了强大的泛化能力。 |
| [^145] | [Batched Low-Rank Adaptation of Foundation Models](https://arxiv.org/abs/2312.05677) | 提出了Fast LoRA（FLoRA）框架，使得基础模型的低秩调整可以高效批处理异构请求，并在绩效上保持竞争性。 |
| [^146] | [SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM](https://arxiv.org/abs/2312.02126) | 该方法提出了SplaTAM，通过利用3D高斯函数的显式体积表示，实现了从单个未定位RGB-D相机进行高保真重建，超越了现有方法的能力 |
| [^147] | [Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation](https://arxiv.org/abs/2311.12028) | 提出了一种名为Hourglass Tokenizer（HoT）的修剪和恢复框架，用于从视频中高效地基于Transformer进行3D人体姿势估计，通过动态选择具有高语义多样性的代表性标记并消除视频帧的冗余，最终提高了模型的效率。 |
| [^148] | [Enhancing Object Coherence in Layout-to-Image Synthesis](https://arxiv.org/abs/2311.10522) | 本文提出了一种新颖的扩散模型，结合全局语义融合和自相似特征增强模块，以引导布局到图像合成中的对象连贯性。 |
| [^149] | [Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification](https://arxiv.org/abs/2311.10319) | 引入了S4MI流程，利用自监督和半监督学习的高效方法，能够简化医学图像的机器监督过程，自监督学习在分类任务中表现明显优于监督方法。 |
| [^150] | [LLatrieval: LLM-Verified Retrieval for Verifiable Generation](https://arxiv.org/abs/2311.07838) | 可验证生成中检索的文件不仅帮助LLM生成正确答案，还作为用户验证LLM输出的证据，但目前广泛使用的检索器已成为性能瓶颈，需要解决。 |
| [^151] | [Challenging Common Paradigms in Multi-Task Learning](https://arxiv.org/abs/2311.04698) | 我们挑战了多任务学习中的常见范式，通过研究在单任务学习中的影响，揭示了优化器选择在MTL中的关键作用，并理论推导出了梯度冲突的角色。 |
| [^152] | [Approximate and Weighted Data Reconstruction Attack in Federated Learning](https://arxiv.org/abs/2308.06822) | 提出了一种基于插值的近似方法和层次加权损失函数，用于攻击FedAvg场景中的数据重构攻击。 |
| [^153] | [Nonlinear Control Allocation: A Learning Based Approach](https://arxiv.org/abs/2201.06180) | 本研究提出了一种基于人工神经网络（ANN）的非线性控制分配方案，在学习控制效能映射的逆过程后，将其作为分配器实施，避免了在线优化问题的计算资源需求。 |
| [^154] | [Context-driven self-supervised visual learning: Harnessing the environment as a data source.](http://arxiv.org/abs/2401.15120) | 这项研究提出了一种基于环境的上下文驱动的自我监督视觉学习方法，通过利用环境的历史空间上下文提供的相似性信号进行对比学习，并展示了在模拟环境中的优越性能，尤其在陌生环境中。该方法有潜力为代理在具有独特视觉特征的新环境中实现快速的视觉学习。 |
| [^155] | [Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?.](http://arxiv.org/abs/2401.12492) | 本研究比较了以群体属性、个体用户和组合方法来模拟人的上下文。合并群体和个体特征显著提高了用户级回归任务的性能，而模拟个体用户则显著提高了单个文档级分类任务的性能。 |
| [^156] | [Artificial Intelligence for Operations Research: Revolutionizing the Operations Research Process.](http://arxiv.org/abs/2401.03244) | 本研究回顾了人工智能技术在运筹学中的应用，旨在通过改进运筹学过程的不同阶段来提高其效果和效率。人工智能与运筹学的协同作用将推动领域内的创新和决策制定的改进。 |
| [^157] | [Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives.](http://arxiv.org/abs/2401.02009) | 自我对比是一种通过对比不同求解视角和总结差异，提高大型语言模型（LLM）的反思能力的方法。 |
| [^158] | [Retrieval-Augmented Generation for Large Language Models: A Survey.](http://arxiv.org/abs/2312.10997) | 本综述论文调查了基于检索增强的大型语言模型的发展，包括三个主要范式：Naive RAG、Advanced RAG和Modular RAG。RAG通过整合外部数据库的知识，增强模型的准确性和可信度，并实现了持续更新知识和整合领域特定信息的功能。 |
| [^159] | [FedSN: A General Federated Learning Framework over LEO Satellite Networks.](http://arxiv.org/abs/2311.01483) | FedSN是一个通用的联邦学习框架，用于解决在LEO卫星网络中的异构计算和存储能力、有限的上行速率以及模型陈旧等关键挑战。 |
| [^160] | [VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification.](http://arxiv.org/abs/2311.01191) | VIGraph是一个基于自我监督学习的模型，通过利用自编码器生成少数类节点来解决图数据中的类别不平衡问题，并通过引入孪生对比策略提高生成节点的质量。 |
| [^161] | [Isometric Motion Manifold Primitives.](http://arxiv.org/abs/2310.17072) | Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods. |
| [^162] | [Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning.](http://arxiv.org/abs/2310.03325) | 本文提出了一种面向视觉规划的可解释和可推广的框架，通过将视觉输入转化为概念表示、符号抽象和推理以及将视觉因果转换与真实世界行为关联，实现了目标条件的视觉规划。 |
| [^163] | [ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models.](http://arxiv.org/abs/2310.00117) | ABScribe是一种界面，支持在人工智能与人类共同写作任务中快速探索多种写作变化。用户可以使用大型语言模型提示快速生成多个变体，这些变体以可重用的按钮形式呈现，并且可以通过上下文工具栏进行快速的就地比较。 |
| [^164] | [Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing.](http://arxiv.org/abs/2309.11427) | 本研究提出了TRACE-GPT模型，用于半导体制造业中无监督故障检测。通过使用时间卷积嵌入和生成式预训练Transformer来预训练时间序列数据，并使用交叉熵损失函数进行异常序列和正常序列的分类，实验结果表明模型具有更好的性能。 |
| [^165] | [Generalization Bounds: Perspectives from Information Theory and PAC-Bayes.](http://arxiv.org/abs/2309.04381) | 该论文介绍了一般化界限的两个视角：信息论和PAC-Bayesian，并探讨了它们之间的联系和共同点。这对于理论机器学习的进一步发展和新算法的设计具有重要意义。 |
| [^166] | [DropMix: Reducing Class Dependency in Mixed Sample Data Augmentation.](http://arxiv.org/abs/2307.09136) | 该论文提出了一种名为DropMix的方法，通过排除一定比例的数据来减少混合样本数据增强（MSDA）中的类别相关性。在两个数据集上的实验证明了该方法可以提高之前因为MSDA而下降的类别的性能，并增加整体的平均准确率。 |
| [^167] | [Learning from Synthetic Human Group Activities.](http://arxiv.org/abs/2306.16772) | 提出了M3Act，一个多视图多团队多人的人类原子动作和团队活动数据生成器，通过Unity引擎驱动实现。该生成器具有大规模数据生成、多模态和高质量注释等特点，能够用于研究复杂的人类互动和团队活动。 |
| [^168] | [Towards Regulatable AI Systems: Technical Gaps and Policy Opportunities.](http://arxiv.org/abs/2306.12609) | 该论文探讨了如何确保AI系统遵循规定，提出了当前和潜在可能的技术实现，以及需要跨学科方法解决的问题。 |
| [^169] | [Recurrent Memory Decision Transformer.](http://arxiv.org/abs/2306.09459) | 本文提出了循环记忆决策变压器（RMDT）模型，用于处理强化学习中的长序列问题。在Atari游戏和MoJoCo控制问题上的实验表明，采用循环记忆机制的RMDT模型显着优于其没有循环记忆机制的对应模型。 |
| [^170] | [ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems.](http://arxiv.org/abs/2306.04357) | 本研究提出了一种针对对话响应选择的后训练技术Dial-MAE，利用生成方法更好地压缩对话语义至密集向量，并提高对话响应选择准确性。 |
| [^171] | [Weakly Supervised AUC Optimization: A Unified Partial AUC Approach.](http://arxiv.org/abs/2305.14258) | 本文提出了WSAUC，一种解决弱监督下AUC优化问题的统一框架，它包括噪声标签学习、正-无标签学习、多实例学习和半监督学习场景，并提出了一种新型的部分AUC——反转部分AUC（rpAUC），作为鲁棒的AUC最大化训练目标，为各种弱监督场景下的AUC优化提供了一种通用解决方案。 |
| [^172] | [Communication-minimizing Asynchronous Tensor Parallelism.](http://arxiv.org/abs/2305.13525) | 本文提出了Tensor3D，一种最小化通信消耗的三维张量计算并行化方法。它利用智能分布神经网络参数、新颖超分解方法以及通信模型，使训练速度提高了约3倍，GPU空闲时间降低了50％以上。 |
| [^173] | [ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review.](http://arxiv.org/abs/2305.03123) | 本文研究关注ChatGPT面临的可持续性、隐私、数字鸿沟和伦理问题，提出了SPADE评估的必要性，并给出了缓解和建议。 |
| [^174] | [Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space.](http://arxiv.org/abs/2305.02151) | 探索了语言特征对多语言表示空间中的跨语言传递性能的影响，初步提供了方法以增强对语言上相距较远的语言的传递能力。 |
| [^175] | [In-Distribution and Out-of-Distribution Self-supervised ECG Representation Learning for Arrhythmia Detection.](http://arxiv.org/abs/2304.06427) | 本文系统研究了自监督学习方法在ECG表征学习上的应用，首次对三个常用ECG心律失常数据集进行了分布分析，实验发现SwAV方法表现最佳，能够超越传统的有监督学习方法，还具有较强的鲁棒性，有望在大规模和多样化人群中检测心律失常。 |
| [^176] | [Demystifying Misconceptions in Social Bots Research.](http://arxiv.org/abs/2303.17251) | 这篇文章揭示了关于社交机器人研究的普遍误解，强调需要以严谨、公正和负责任的方式讨论虚假信息研究。 |
| [^177] | [Innovation Slowdown: Decelerating Concept Creation and Declining Originality in New Technological Concepts.](http://arxiv.org/abs/2303.13300) | 人类智力的局限性导致技术概念创造放缓和原创性下降，因此建议开发和实施创造性人工智能增强创新过程。 |
| [^178] | [Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised Learning.](http://arxiv.org/abs/2303.12091) | 本文提出了ANEDL框架，应用证据深度学习量化不同类型的不确定性，并设计了新颖的适应性负优化策略，有效应对在未标记数据集中包含内部值和异常值的开放式半监督学习。 |
| [^179] | [CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning.](http://arxiv.org/abs/2303.10365) | CroSel是一种处理伪标签噪声的新方法，通过利用历史预测信息和一致性正则化项来准确识别部分标签数据的真实标签。 |
| [^180] | [HIVE: Harnessing Human Feedback for Instructional Visual Editing.](http://arxiv.org/abs/2303.09618) | 本文提出了一种新的框架，利用人类反馈进行指导性视觉编辑。通过收集被编辑图像的人类反馈，并学习奖励函数捕捉用户的偏好，可以缓解数据限制所带来的偏差，并提高模型性能。 |
| [^181] | [Regret-Based Optimization for Robust Reinforcement Learning.](http://arxiv.org/abs/2302.06912) | 本论文提出了一种基于后悔的优化方法，用于使强化学习算法更加鲁棒，以应对观测中的对抗性噪声。 |
| [^182] | [Follower Agnostic Methods for Stackelberg Games.](http://arxiv.org/abs/2302.01421) | 该论文提出了一种追随者不可知的方法来解决斯塔克贝格博弈中的问题，与其他现有作品不同的是，该算法不需要使用预估器或了解追随者的策略、效用函数。作者设计了一个两层循环算法，通过特别设计的策略探测追随者来更新领导者的策略，发现追随者的联合策略收敛于均衡。非渐进收敛速度到领导者目标的局部稳定点，并进一步展示了对领导者目标的局部极小值的渐进收敛。 |

# 详细

[^1]: Mini-Gemini: 挖掘多模态视觉语言模型的潜力

    Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models

    [https://arxiv.org/abs/2403.18814](https://arxiv.org/abs/2403.18814)

    Mini-Gemini 挖掘了VLMs的潜力，通过高分辨率视觉标记、高质量数据和VLM引导生成等方式，缩小了与先进模型之间的性能差距

    

    在这项工作中，我们介绍了Mini-Gemini，这是一个简单而有效的框架，可以增强多模态视觉语言模型（VLMs）。尽管VLMs在促进基本视觉对话和推理方面取得了进展，但与GPT-4和Gemini等先进模型相比仍存在性能差距。我们试图通过从高分辨率视觉标记、高质量数据和VLM引导生成三个方面挖掘VLMs的潜力，以缩小这一差距。为了增强视觉标记，我们提出利用额外的视觉编码器进行高分辨率细化，而不增加视觉标记数量。我们进一步构建了一个高质量数据集，促进精确的图像理解和基于推理的生成，拓展了当前VLMs的操作范围。总的来说，Mini-Gemini进一步挖掘了VLMs的潜力，并赋予当前框架图像理解、推理和生成的能力。

    arXiv:2403.18814v1 Announce Type: cross  Abstract: In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini. We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i.e., high-resolution visual tokens, high-quality data, and VLM-guided generation. To enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count. We further construct a high-quality dataset that promotes precise image comprehension and reasoning-based generation, expanding the operational scope of current VLMs. In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and gen
    
[^2]: ECoDepth: 有效调整扩散模型以用于单目深度估计

    ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation

    [https://arxiv.org/abs/2403.18807](https://arxiv.org/abs/2403.18807)

    通过使用预训练的ViT模型生成的全局图像先验，为单图深度估计模型提供更详细的上下文信息，并提出了一种新的使用扩散骨干且受ViT嵌入条件约束的深度估计模型。

    

    在缺乏视差线索的情况下，基于学习的单图深度估计（SIDE）模型严重依赖图像中的阴影和上下文线索。我们从已有研究的启发中探讨使用从预训练的ViT模型生成的全局图像先验，以提供更详细的上下文信息。基于这一想法，我们提出了一种新的使用扩散骨干的SIDE模型，其受到ViT嵌入的条件约束。

    arXiv:2403.18807v1 Announce Type: cross  Abstract: In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embedding
    
[^3]: 大型语言模型中的长篇事实性

    Long-form factuality in large language models

    [https://arxiv.org/abs/2403.18802](https://arxiv.org/abs/2403.18802)

    该论文提出了一种通过使用大型语言模型将长篇回应分解为单个事实，并通过发送搜索查询到Google搜索，评估事实准确性的方法，并扩展了F1分数作为长篇事实性的聚合度量。

    

    大型语言模型（LLMs）在回答开放性主题的事实性提示时，经常生成包含事实错误的内容。为了在开放领域中对模型的长篇事实性进行基准测试，我们首先使用GPT-4生成了一个名为LongFact的提示集，其中包含数千个囊括38个主题的问题。然后，我们提出LLM代理可以通过一种名为Search-Augmented Factuality Evaluator（SAFE）的方法作为长篇事实性的自动评估器。SAFE利用LLM将长篇回应分解为一组单独的事实，并通过发送搜索查询到Google搜索以及确定一个事实是否得到搜索结果支持的多步推理过程来评估每个事实的准确性。此外，我们还提议将F1分数扩展为长篇事实性的聚合度量。为此，我们平衡了回应中支持事实的百分比（精度）与

    arXiv:2403.18802v1 Announce Type: cross  Abstract: Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the 
    
[^4]: Gamba：将高斯飘点与曼巴相结合，用于单视图3D重建

    Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction

    [https://arxiv.org/abs/2403.18795](https://arxiv.org/abs/2403.18795)

    提出了Gamba，一种单视图3D重建模型，创新地结合了大量的3D高斯点进行高效重建，并引入了基于曼巴的顺序网络，促进依赖上下文的推理，实现了线性可扩展性。

    

    我们致力于解决从单个图像高效重建3D资产的挑战，随着对自动化3D内容创建流水线的需求不断增长。我们引入了Gamba，这是一个端到端的从单视图图像进行摊余化3D重建的模型，强调了两个主要见解：(1) 3D表示：利用大量3D高斯来进行高效的3D高斯飘点过程；(2) 主干设计：引入基于曼巴的顺序网络，促进依赖上下文的推理，并具有与序列（标记）长度的线性可扩展性，适应大量高斯点。Gamba在数据预处理、正则化等方面融入了重大进展。

    arXiv:2403.18795v1 Announce Type: cross  Abstract: We tackle the challenge of efficiently reconstructing a 3D asset from a single image with growing demands for automated 3D content creation pipelines. Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural Radiance Fields (NeRF). Despite their significant success, these approaches encounter practical limitations due to lengthy optimization and considerable memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D reconstruction model from single-view images, emphasizing two main insights: (1) 3D representation: leveraging a large number of 3D Gaussians for an efficient 3D Gaussian splatting process; (2) Backbone design: introducing a Mamba-based sequential network that facilitates context-dependent reasoning and linear scalability with the sequence (token) length, accommodating a substantial number of Gaussians. Gamba incorporates significant advancements in data preprocessing, regularization
    
[^5]: ImageNet-D: 在扩散合成对象上评估神经网络的鲁棒性基准

    ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object

    [https://arxiv.org/abs/2403.18775](https://arxiv.org/abs/2403.18775)

    本研究引入生成模型作为数据源，通过扩散模型生成了更多样化的背景、纹理和材料图像，建立了ImageNet-D基准评估深度模型的鲁棒性，在一系列视觉模型中显著降低了准确性高达60%。

    

    我们为视觉感知鲁棒性建立了严格的基准。合成图像，如ImageNet-C、ImageNet-9和Stylized ImageNet，提供了对合成破坏、背景和纹理的特定类型评估，然而这些鲁棒性基准受限于指定的变体，并具有较低的合成质量。在这项工作中，我们引入生成模型作为合成难图像的数据源来评估深度模型的鲁棒性。利用扩散模型，我们能够生成比任何先前工作更多样化的背景、纹理和材料图像，我们将这个基准称为ImageNet-D。实验结果表明，ImageNet-D导致了一系列视觉模型的显著准确性下降，从标准ResNet视觉分类器到最新的基础模型，如CLIP和MiniGPT-4，将它们的准确性显著降低了高达60\%。我们的工作表明，扩散模型...

    arXiv:2403.18775v1 Announce Type: cross  Abstract: We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models' robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\%. Our work suggests that diffusion models 
    
[^6]: 通过竞争性随机样本大小优化在Big-means中实现优越的并行大数据聚类

    Superior Parallel Big Data Clustering through Competitive Stochastic Sample Size Optimization in Big-means

    [https://arxiv.org/abs/2403.18766](https://arxiv.org/abs/2403.18766)

    该论文介绍了一种新型K-means聚类算法，通过竞争性随机样本大小优化，有效提高了Big-means中的并行大数据聚类效率。

    

    这篇论文介绍了一种新颖的K-means聚类算法，是对传统Big-means方法的进步。所提出的方法有效地整合了并行处理、随机抽样和竞争性优化，创建了一个专为大数据应用设计的可扩展变体。它解决了传统技术通常面临的可伸缩性和计算时间挑战。该算法在执行过程中动态调整每个工作人员的样本大小，优化性能。这些样本大小的数据不断被分析，促进了找到最有效配置的识别。通过在使用不同样本大小的工作人员之间引入竞争因素，进一步刺激了Big-means算法内的效率。本质上，该算法通过在并行计算环境中采用随机、竞争性抽样策略，平衡了计算时间和聚类质量。

    arXiv:2403.18766v1 Announce Type: cross  Abstract: This paper introduces a novel K-means clustering algorithm, an advancement on the conventional Big-means methodology. The proposed method efficiently integrates parallel processing, stochastic sampling, and competitive optimization to create a scalable variant designed for big data applications. It addresses scalability and computation time challenges typically faced with traditional techniques. The algorithm adjusts sample sizes dynamically for each worker during execution, optimizing performance. Data from these sample sizes are continually analyzed, facilitating the identification of the most efficient configuration. By incorporating a competitive element among workers using different sample sizes, efficiency within the Big-means algorithm is further stimulated. In essence, the algorithm balances computational time and clustering quality by employing a stochastic, competitive sampling strategy in a parallel computing setting.
    
[^7]: ModaLink：统一模态以实现高效的图像到点云地点识别

    ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place Recognition

    [https://arxiv.org/abs/2403.18762](https://arxiv.org/abs/2403.18762)

    提出了一个快速轻量级的框架，将图像和点云编码为地点特征描述符，并设计了一种有效的Field of View（FoV）转换模块来消除深度估计的必要性，从而实现了实时性能。

    

    场所识别对于机器人和自动驾驶汽车来定位自身并关闭在预先构建的地图中的循环是一个重要的任务。而单模态传感器方法已经显示出令人满意的性能，而检索图像的交叉模态地点识别仍然是一个具有挑战性的问题。当前的交叉模态方法将图像转换为3D点使用深度估计进行模态转换，这通常计算密集且需要昂贵的标记数据进行深度监督。在这项工作中，我们介绍了一个快速且轻量级的框架，将图像和点云编码为地点特征描述符。我们提出了一个有效的视场（FoV）转换模块，将点云转换为与图像类似的模态。该模块消除了深度估计的必要性，并帮助后续模块实现实时性能。

    arXiv:2403.18762v1 Announce Type: cross  Abstract: Place recognition is an important task for robots and autonomous cars to localize themselves and close loops in pre-built maps. While single-modal sensor-based methods have shown satisfactory performance, cross-modal place recognition that retrieving images from a point-cloud database remains a challenging problem. Current cross-modal methods transform images into 3D points using depth estimation for modality conversion, which are usually computationally intensive and need expensive labeled data for depth supervision. In this work, we introduce a fast and lightweight framework to encode images and point clouds into place-distinctive descriptors. We propose an effective Field of View (FoV) transformation module to convert point clouds into an analogous modality as images. This module eliminates the necessity for depth estimation and helps subsequent modules achieve real-time performance. We further design a non-negative factorization-ba
    
[^8]: 基于图像深度学习在胸部X射线上检测亚临床动脉硬化

    Detection of subclinical atherosclerosis by image-based deep learning on chest x-ray

    [https://arxiv.org/abs/2403.18756](https://arxiv.org/abs/2403.18756)

    该研究开发了一种基于深度学习的系统，可以在胸部X射线上识别亚临床动脉硬化，为检测心血管疾病提供了新方法。

    

    目的是开发一个基于深度学习的系统，用于识别胸部正位X射线上的亚临床动脉硬化。通过460例初级预防患者（58.4%男性，中间年龄63 [51-74]岁）的胸部X射线（80%训练队列，20%内部验证队列），开发了一个用于预测冠状动脉钙化（CAC）评分的深度学习算法（AI-CAC模型），这些患者在临床原因下有可用的成对胸部X射线和胸部计算机断层扫描（CT），而且这两项检查在3个月内完成。在来自相同机构的90名病人的时间独立队列上对模型进行了验证（外部验证）。通过曲线下面积（AUC）评估AI-CAC模型的诊断准确性为主要结果。总体来说，中位AI-CAC评分为35（0-388），28.9%的患者没有AI-CAC。

    arXiv:2403.18756v1 Announce Type: cross  Abstract: Aims. To develop a deep-learning based system for recognition of subclinical atherosclerosis on a plain frontal chest x-ray. Methods and Results. A deep-learning algorithm to predict coronary artery calcium (CAC) score (the AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20% internal validation cohort) of primary prevention patients (58.4% male, median age 63 [51-74] years) with available paired chest x-ray and chest computed tomography (CT) indicated for any clinical reason and performed within 3 months. The CAC score calculated on chest CT was used as ground truth. The model was validated on an temporally-independent cohort of 90 patients from the same institution (external validation). The diagnostic accuracy of the AI-CAC model assessed by the area under the curve (AUC) was the primary outcome. Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC. AUC of the AI-CAC model to identify a CA
    
[^9]: 多目标进化影响最大化：平衡传播、预算、公平性和时间

    Many-Objective Evolutionary Influence Maximization: Balancing Spread, Budget, Fairness, and Time

    [https://arxiv.org/abs/2403.18755](https://arxiv.org/abs/2403.18755)

    通过引入多目标进化算法，本研究在最大化影响和最小化种子集大小的基础上，优化了多个IM特定目标函数，包括预算、公平性、社区和时间。

    

    影响最大化（IM）问题旨在发现图中可以最大程度传播信息传播的节点集。这个问题被称为NP难题，通常通过最大化影响（传播）以及选择性优化第二个目标（例如最小化种子集大小或最大化影响公平性）来研究。然而，在许多实际场景中，IM问题的多个方面必须同时进行优化。在这项工作中，我们提出了一个第一个案例研究，在此基础上优化了几个IM特定目标函数，即预算、公平性、社区和时间，同时最大化传播影响并最小化种子集大小。为此，我们引入了MOEIM（用于影响最大化的多目标进化算法），这是一个基于NSGA-II的多目标进化算法（MOEA），结合了具有图感知性的算子和智能初始化。我们通过...

    arXiv:2403.18755v1 Announce Type: cross  Abstract: The Influence Maximization (IM) problem seeks to discover the set of nodes in a graph that can spread the information propagation at most. This problem is known to be NP-hard, and it is usually studied by maximizing the influence (spread) and, optionally, optimizing a second objective, such as minimizing the seed set size or maximizing the influence fairness. However, in many practical scenarios multiple aspects of the IM problem must be optimized at the same time. In this work, we propose a first case study where several IM-specific objective functions, namely budget, fairness, communities, and time, are optimized on top of the maximization of influence and minimization of the seed set size. To this aim, we introduce MOEIM (Many-Objective Evolutionary Algorithm for Influence Maximization) a Multi-Objective Evolutionary Algorithm (MOEA) based on NSGA-II incorporating graph-aware operators and a smart initialization. We compare MOEIM in
    
[^10]: 理解人类反馈对齐学习动态的研究

    Understanding the Learning Dynamics of Alignment with Human Feedback

    [https://arxiv.org/abs/2403.18742](https://arxiv.org/abs/2403.18742)

    本研究对人类偏好对齐的学习动态进行了理论分析，显示了偏好数据集的分布如何影响模型更新速度，并提供了对训练准确度的严格保证，同时揭示了优化易于优先考虑高偏好可区分性行为的复杂现象。

    

    大型语言模型（LLMs）与人类意图对齐已成为安全部署模型在实际系统中的关键任务。现有的对齐方法虽然在经验上取得了成功，但理论上了解这些方法如何影响模型行为仍然是一个悬而未决的问题。我们的工作首次尝试在理论上分析人类偏好对齐的学习动态。我们正式展示了偏好数据集的分布如何影响模型更新速度，并对训练准确度提供了严格的保证。我们的理论还揭示了一个复杂现象，即优化易于优先考虑具有更高偏好可区分性的行为。我们在当代LLMs和对齐任务上在实证上验证了我们的发现，强化了我们的理论见解，并为未来的对齐方法提供了启示。免责声明：本文包含有效

    arXiv:2403.18742v1 Announce Type: cross  Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potent
    
[^11]: 通过集成可解释性方法增强制造质量预测模型

    Enhancing Manufacturing Quality Prediction Models through the Integration of Explainability Methods

    [https://arxiv.org/abs/2403.18731](https://arxiv.org/abs/2403.18731)

    该研究提出了一种利用可解释性技术来增强制造质量预测模型性能的方法，通过消除无关特征进行微调，提高了性能，为降低制造成本和更好理解训练后的模型铺平了道路。

    

    这项研究提出了一种利用可解释性技术来增强机器学习（ML）模型在预测铣削过程质量方面表现的方法，通过一个制造业案例展示了这一点。该方法包括首先训练ML模型，然后通过可解释性方法识别并消除无关特征进行微调。这种过程的完善结果导致性能的提升，为降低制造成本和更好理解训练后的ML模型铺平了道路。该研究突出了可解释性技术在解释和优化制造领域预测模型中的用处。

    arXiv:2403.18731v1 Announce Type: new  Abstract: This research presents a method that utilizes explainability techniques to amplify the performance of machine learning (ML) models in forecasting the quality of milling processes, as demonstrated in this paper through a manufacturing use case. The methodology entails the initial training of ML models, followed by a fine-tuning phase where irrelevant features identified through explainability methods are eliminated. This procedural refinement results in performance enhancements, paving the way for potential reductions in manufacturing costs and a better understanding of the trained ML models. This study highlights the usefulness of explainability techniques in both explaining and optimizing predictive models in the manufacturing realm.
    
[^12]: 随机强化学习策略的概率模型检验

    Probabilistic Model Checking of Stochastic Reinforcement Learning Policies

    [https://arxiv.org/abs/2403.18725](https://arxiv.org/abs/2403.18725)

    该方法介绍了一种验证随机强化学习策略的方法，通过将模型检验技术与RL相结合，建立了能够通过模型检验器验证的形式模型。

    

    我们引入了一种验证随机强化学习（RL）策略的方法。该方法与任何RL算法兼容，只要算法及其对应的环境共同遵守马尔可夫性质。在这种情况下，环境的未来状态应仅取决于其当前状态和执行的动作，而与任何先前的状态或动作无关。我们的方法将一种称为模型检验的验证技术与RL相结合，利用马尔可夫决策过程、训练后的RL策略和概率计算树逻辑（PCTL）公式构建一个形式模型，随后可以通过模型检验器Storm进行验证。我们展示了我们的方法在多个基准测试中的适用性，并将其与称为确定性安全估计和天真的整体模型检验的基线方法进行比较。我们的结果表明我们的方法适用于验证随机RL策略。

    arXiv:2403.18725v1 Announce Type: new  Abstract: We introduce a method to verify stochastic reinforcement learning (RL) policies. This approach is compatible with any RL algorithm as long as the algorithm and its corresponding environment collectively adhere to the Markov property. In this setting, the future state of the environment should depend solely on its current state and the action executed, independent of any previous states or actions. Our method integrates a verification technique, referred to as model checking, with RL, leveraging a Markov decision process, a trained RL policy, and a probabilistic computation tree logic (PCTL) formula to build a formal model that can be subsequently verified via the model checker Storm. We demonstrate our method's applicability across multiple benchmarks, comparing it to baseline methods called deterministic safety estimates and naive monolithic model checking. Our results show that our method is suited to verify stochastic RL policies.
    
[^13]: 深度因果生成模型的半监督学习

    Semi-Supervised Learning for Deep Causal Generative Models

    [https://arxiv.org/abs/2403.18717](https://arxiv.org/abs/2403.18717)

    首次开发了一种利用变量之间因果关系的半监督深度因果生成模型，以最大限度地利用所有可用数据。

    

    开发能够回答“如果$y$变为$z$，$x$会如何变化？”这类问题的模型对于推动医学图像分析至关重要。然而，训练能够解决这类反事实问题的因果生成模型目前要求所有相关变量均已被观察到，并且相应的标签在训练数据中可用。我们首次开发了一种利用变量之间因果关系的半监督深度因果生成模型，以最大限度地利用所有可用数据。

    arXiv:2403.18717v1 Announce Type: cross  Abstract: Developing models that can answer questions of the form "How would $x$ change if $y$ had been $z$?" is fundamental for advancing medical image analysis. Training causal generative models that address such counterfactual questions, though, currently requires that all relevant variables have been observed and that corresponding labels are available in training data. However, clinical data may not have complete records for all patients and state of the art causal generative models are unable to take full advantage of this. We thus develop, for the first time, a semi-supervised deep causal generative model that exploits the causal relationships between variables to maximise the use of all available data. We explore this in the setting where each sample is either fully labelled or fully unlabelled, as well as the more clinically realistic case of having different labels missing for each sample. We leverage techniques from causal inference t
    
[^14]: 使用指示对比解码减轻大规模视觉-语言模型中的幻觉

    Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding

    [https://arxiv.org/abs/2403.18715](https://arxiv.org/abs/2403.18715)

    这项研究提出了一种名为指示对比解码(ICD)方法，旨在减少大规模视觉-语言模型(LVLMs)推断过程中的幻觉，通过对标准和指示扰动的分布进行对比，从原始分布中减去幻觉概念。

    

    大规模视觉-语言模型(LVLMs)越来越擅长从视觉输入生成具有上下文细节和连贯性的响应。然而，在多模式决策和开放式生成中应用它们时，其应用受到幻觉的阻碍，即生成的文本不准确地代表了视觉内容。为了解决这一问题，本文介绍了指示对比解码(ICD)方法，这是一种旨在在LVLM推断过程中减少幻觉的新方法。我们的方法受到我们观察到的扰动指示显著加剧多模态融合模块中的幻觉的启发。ICD对标准和指示扰动的分布进行对比，从而增加对齐不确定性并有效地从原始分布中减去幻觉概念。通过在判别基准(POPE和MME)和生成基准上进行全面实验

    arXiv:2403.18715v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs. However, their application in multimodal decision-making and open-ended generation is hindered by a notable rate of hallucinations, where generated text inaccurately represents the visual contents. To address this issue, this paper introduces the Instruction Contrastive Decoding (ICD) method, a novel approach designed to reduce hallucinations during LVLM inference. Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in multimodal fusion modules. ICD contrasts distributions from standard and instruction disturbance, thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution. Through comprehensive experiments on discriminative benchmarks (POPE and MME) and a generativ
    
[^15]: SAT-NGP: 释放神经图形基元，从卫星图像进行快速可重光无暂变的三维重建

    SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable Transient-Free 3D reconstruction from Satellite Imagery

    [https://arxiv.org/abs/2403.18711](https://arxiv.org/abs/2403.18711)

    提出了一种名为SAT-NGP的模型，在多日期卫星图像的三维重建中，通过高效的采样策略和多分辨率哈希编码，将学习时间缩短至15分钟，同时保持重建质量。

    

    当使用多对或三元卫星图像时，当前的立体视觉流水线在实现高准确度的三维重建时表现出色。然而，这些流水线对图像之间变化敏感，这些变化可能是由于多日期获取导致的。这种变化主要是由于变化的阴影、反射和瞬变物体（车辆、植被）引起的。为了考虑这些变化，最近已将神经辐射场（NeRF）应用于多日期卫星图像。然而，神经方法非常计算密集，需要数十小时的学习时间，而标准立体视觉流水线只需几分钟。根据即时神经图形基元的思想，我们提议使用高效的采样策略和多分辨率哈希编码来加速学习。我们的模型，卫星神经图形基元（SAT-NGP）将学习时间缩短至15分钟，同时保持三维重建的质量。

    arXiv:2403.18711v1 Announce Type: cross  Abstract: Current stereo-vision pipelines produce high accuracy 3D reconstruction when using multiple pairs or triplets of satellite images. However, these pipelines are sensitive to the changes between images that can occur as a result of multi-date acquisitions. Such variations are mainly due to variable shadows, reflexions and transient objects (cars, vegetation). To take such changes into account, Neural Radiance Fields (NeRF) have recently been applied to multi-date satellite imagery. However, Neural methods are very compute-intensive, taking dozens of hours to learn, compared with minutes for standard stereo-vision pipelines. Following the ideas of Instant Neural Graphics Primitives we propose to use an efficient sampling strategy and multi-resolution hash encoding to accelerate the learning. Our model, Satellite Neural Graphics Primitives (SAT-NGP) decreases the learning time to 15 minutes while maintaining the quality of the 3D reconstru
    
[^16]: 具有正交锚点的对比学习（CLOA）

    Contrastive Learning with Orthonormal Anchors (CLOA)

    [https://arxiv.org/abs/2403.18699](https://arxiv.org/abs/2403.18699)

    该研究提出了一种新的损失函数称为正交锚点回归损失，用于解开嵌入聚类，显著增强嵌入的独特性

    

    本研究关注解决对比学习中普遍存在的不稳定性问题，特别是检查InfoNCE损失函数及其导数。我们揭示了一个关键观察，即这些损失函数表现出限制性行为，导致嵌入趋于融合为一个奇异点的收敛现象。这种“过度融合”效应对后续监督学习任务中的分类准确性产生不利影响。通过理论分析，我们证明了嵌入在等于或局限于秩-1线性子空间时表示InfoNCE的局部最小值。针对这一挑战，我们的研究提出了一种创新策略，利用与微调阶段典型使用的相同或更少的标记数据。我们提出的损失函数，即正交锚点回归损失，旨在解开嵌入聚类，显著增强每个嵌入的独特性。

    arXiv:2403.18699v1 Announce Type: cross  Abstract: This study focuses on addressing the instability issues prevalent in contrastive learning, specifically examining the InfoNCE loss function and its derivatives. We reveal a critical observation that these loss functions exhibit a restrictive behavior, leading to a convergence phenomenon where embeddings tend to merge into a singular point. This "over-fusion" effect detrimentally affects classification accuracy in subsequent supervised-learning tasks. Through theoretical analysis, we demonstrate that embeddings, when equalized or confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In response to this challenge, our research introduces an innovative strategy that leverages the same or fewer labeled data than typically used in the fine-tuning phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to disentangle embedding clusters, significantly enhancing the distinctiveness of each embedding 
    
[^17]: Annolid: Annotate, Segment, and Track Anything You Need

    Annolid: Annotate, Segment, and Track Anything You Need

    [https://arxiv.org/abs/2403.18690](https://arxiv.org/abs/2403.18690)

    Annolid是一个基于深度学习的软件包，旨在对视频文件中的研究目标进行分割、标记和跟踪，主要集中在动物行为分析上，通过弹性、无标记跟踪多个动物并通过文本命令自动遮罩和分割可识别的动物和物体。

    

    Annolid是一个基于深度学习的软件包，旨在对视频文件中的研究目标进行分割、标记和跟踪，主要集中在动物行为分析上。基于最先进的实例分割方法，Annolid现在利用Cutie视频对象分割模型实现从单个注释帧中实现多个动物的弹性、无标记跟踪，即使它们可能部分或完全被环境特征或彼此遮挡。我们还整合了“Segment Anything”和“Grounding-DINO”策略，通过文本命令自动遮罩和分割可识别的动物和物体，消除了手动注释的需要。Annolid的综合对象分割方法灵活适应广泛的行为分析应用，能够对不同的行为状态进行分类，如“freezing”。

    arXiv:2403.18690v1 Announce Type: cross  Abstract: Annolid is a deep learning-based software package designed for the segmentation, labeling, and tracking of research targets within video files, focusing primarily on animal behavior analysis. Based on state-of-the-art instance segmentation methods, Annolid now harnesses the Cutie video object segmentation model to achieve resilient, markerless tracking of multiple animals from single annotated frames, even in environments in which they may be partially or entirely concealed by environmental features or by one another. Our integration of Segment Anything and Grounding-DINO strategies additionally enables the automatic masking and segmentation of recognizable animals and objects by text command, removing the need for manual annotation. Annolid's comprehensive approach to object segmentation flexibly accommodates a broad spectrum of behavior analysis applications, enabling the classification of diverse behavioral states such as freezing, 
    
[^18]: TransFusion：具有变压器的对比学习

    TransFusion: Contrastive Learning with Transformers

    [https://arxiv.org/abs/2403.18681](https://arxiv.org/abs/2403.18681)

    TransFusion的主要创新在于定义了对比学习领域中的两个基本问题的理论极限，并成功实现了从复杂的现实世界数据中提取特征以改善分类精度。

    

    这篇论文提出了一个新的框架，TransFusion，旨在使对比学习的过程更具分析性和可解释性。 TransFusion由注意力块组成，其中的softmax被替换为ReLU，并且其最终块的加权和操作被截断，以使邻接矩阵成为输出。该模型通过最小化其输出与目标关联矩阵之间的Jensen-Shannon散度来进行训练，该矩阵指示每对样本是否属于相同类别或不同类别。 TransFusion的主要贡献在于定义了回答该领域两个基本问题的理论极限：数据增强的最大级别和有效对比学习所需的最小批量大小。 此外，实验结果表明，TransFusion成功地提取出能够从复杂的现实世界数据中分离集群的特征，从而提高了分类精度。

    arXiv:2403.18681v1 Announce Type: cross  Abstract: This paper proposes a novel framework, TransFusion, designed to make the process of contrastive learning more analytical and explainable. TransFusion consists of attention blocks whose softmax being replaced by ReLU, and its final block's weighted-sum operation is truncated to leave the adjacency matrix as the output. The model is trained by minimizing the Jensen-Shannon Divergence between its output and the target affinity matrix, which indicates whether each pair of samples belongs to the same or different classes. The main contribution of TransFusion lies in defining a theoretical limit for answering two fundamental questions in the field: the maximum level of data augmentation and the minimum batch size required for effective contrastive learning. Furthermore, experimental results indicate that TransFusion successfully extracts features that isolate clusters from complex real-world data, leading to improved classification accuracy 
    
[^19]: 以相关性为目标

    Aiming for Relevance

    [https://arxiv.org/abs/2403.18668](https://arxiv.org/abs/2403.18668)

    引入了与临床背景相一致的新颖生命体征预测性能指标，通过捕捉与临床规范的偏差、整体趋势和趋势偏差，为早期发现不良事件铺平道路。

    

    在重症监护病房（ICU）中，生命体征至关重要。它们用于跟踪患者的状态，并识别临床上显著的变化。预测生命体征轨迹对于早期发现不良事件具有重要价值。然而，传统的机器学习指标如RMSE往往无法捕捉这些预测的真正临床相关性。我们引入了新颖的生命体征预测性能指标，与临床背景相一致，关注与临床规范的偏差、整体趋势和趋势偏差。这些指标源自通过与ICU临床医生的访谈获得的实证效用曲线。我们使用模拟和真实临床数据集（MIMIC和eICU）验证了这些指标的有用性。此外，我们将这些指标作为神经网络的损失函数，从而得到在预测临床重要事件方面表现出色的模型。这项研究为临床实践铺平了道路。

    arXiv:2403.18668v1 Announce Type: cross  Abstract: Vital signs are crucial in intensive care units (ICUs). They are used to track the patient's state and to identify clinically significant changes. Predicting vital sign trajectories is valuable for early detection of adverse events. However, conventional machine learning metrics like RMSE often fail to capture the true clinical relevance of such predictions. We introduce novel vital sign prediction performance metrics that align with clinical contexts, focusing on deviations from clinical norms, overall trends, and trend deviations. These metrics are derived from empirical utility curves obtained in a previous study through interviews with ICU clinicians. We validate the metrics' usefulness using simulated and real clinical datasets (MIMIC and eICU). Furthermore, we employ these metrics as loss functions for neural networks, resulting in models that excel in predicting clinically significant events. This research paves the way for clin
    
[^20]: INEXA: 通过面向对象的过程挖掘实现交互和可解释的过程模型抽象

    INEXA: Interactive and Explainable Process Model Abstraction Through Object-Centric Process Mining

    [https://arxiv.org/abs/2403.18659](https://arxiv.org/abs/2403.18659)

    INEXA是一个交互式、可解释的过程模型抽象工具，能够帮助用户在不同粒度级别上探索和理解发现的过程模型。

    

    多个信息系统记录的过程事件具有不同的粒度级别。根据生成的事件日志，在不同的粒度级别上发现过程模型。例如，存储在细粒度级别的事件可能由于结果模型元素过多而妨碍显示所发现的过程模型。例如，真实制造过程的发现过程模型由1,489个模型元素和2,000多个弧组成。现有的过程模型抽象技术可以帮助减小模型的大小，但会将其与基础事件日志断开联系。现有的事件抽象技术既不支持混合粒度级别的分析，也不支持交互式探索合适的粒度级别。为了能够在不同粒度级别上探索发现的过程模型，我们提出了INEXA，这是一个交互式、可解释的过程m

    arXiv:2403.18659v1 Announce Type: new  Abstract: Process events are recorded by multiple information systems at different granularity levels. Based on the resulting event logs, process models are discovered at different granularity levels, as well. Events stored at a fine-grained granularity level, for example, may hinder the discovered process model to be displayed due the high number of resulting model elements. The discovered process model of a real-world manufacturing process, for example, consists of 1,489 model elements and over 2,000 arcs. Existing process model abstraction techniques could help reducing the size of the model, but would disconnect it from the underlying event log. Existing event abstraction techniques do neither support the analysis of mixed granularity levels, nor interactive exploration of a suitable granularity level. To enable the exploration of discovered process models at different granularity levels, we propose INEXA, an interactive, explainable process m
    
[^21]: Spikewhisper：基于时间的斯派克后门攻击对低功耗设备上的联合神经形态学学习的影响

    Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic Learning over Low-power Devices

    [https://arxiv.org/abs/2403.18607](https://arxiv.org/abs/2403.18607)

    本文探索了基于时间分割复用概念的Spikewhisper，允许攻击者在联合神经形态学学习系统中实施难以被检测的斯派克后门攻击。

    

    联合神经形态学学习（FedNL）利用事件驱动的脉冲神经网络和联合学习框架来有效执行智能分析任务，涉及分布式低功耗设备的大量数据，但同时也容易受到毒化攻击的影响。传统深度神经网络面临的后门攻击威胁通常来自于时间不变的数据。然而，在FedNL中，未知的威胁可能隐藏在时变的脉冲信号中。本文开始探索基于FedNL系统概念的一种新型脆弱性，称为“Spikewhisper”，该概念利用时间分割复用，允许攻击者尽可能规避检测，因为多个恶意客户端可以在不同时间片段使用不同的触发器不被察觉地进行毒化。特别地，Spikewhisper的隐蔽性源自于全局触发器的时域可分割性，每个恶意客户端仅粘贴一个本地触发器。

    arXiv:2403.18607v1 Announce Type: cross  Abstract: Federated neuromorphic learning (FedNL) leverages event-driven spiking neural networks and federated learning frameworks to effectively execute intelligent analysis tasks over amounts of distributed low-power devices but also perform vulnerability to poisoning attacks. The threat of backdoor attacks on traditional deep neural networks typically comes from time-invariant data. However, in FedNL, unknown threats may be hidden in time-varying spike signals. In this paper, we start to explore a novel vulnerability of FedNL-based systems with the concept of time division multiplexing, termed Spikewhisper, which allows attackers to evade detection as much as possible, as multiple malicious clients can imperceptibly poison with different triggers at different timeslices. In particular, the stealthiness of Spikewhisper is derived from the time-domain divisibility of global triggers, in which each malicious client pastes only one local trigger 
    
[^22]: RAP：检索增强型规划器用于指导视频中的自适应程序规划

    RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in Instructional Videos

    [https://arxiv.org/abs/2403.18600](https://arxiv.org/abs/2403.18600)

    提出了一种新的实际设置，称为指导视频中的自适应程序规划，克服了在实际场景中步骤长度变化的模型不具有泛化能力、理解步骤之间的时间关系知识对于生成合理且可执行的计划至关重要以及用步骤级标签或序列级标签标注指导视频耗时且劳动密集的问题

    

    指导视频中的程序规划涉及根据初始和目标状态的视觉观察生成一系列动作步骤。尽管这一任务取得了快速进展，仍然存在一些关键挑战需要解决：（1）自适应程序：先前的工作存在一个不切实际的假设，即动作步骤的数量是已知且固定的，导致在实际场景中，步骤长度变化的模型不具有泛化能力。（2）时间关系：理解步骤之间的时间关系知识对于生成合理且可执行的计划至关重要。（3）注释成本：用步骤级标签（即时间戳）或序列级标签（即动作类别）标注指导视频是耗时且劳动密集的，限制了其泛化能力到大规模数据集。在这项工作中，我们提出了一个新的实际设置，称为指导视频中的自适应程序规划

    arXiv:2403.18600v1 Announce Type: cross  Abstract: Procedure Planning in instructional videos entails generating a sequence of action steps based on visual observations of the initial and target states. Despite the rapid progress in this task, there remain several critical challenges to be solved: (1) Adaptive procedures: Prior works hold an unrealistic assumption that the number of action steps is known and fixed, leading to non-generalizable models in real-world scenarios where the sequence length varies. (2) Temporal relation: Understanding the step temporal relation knowledge is essential in producing reasonable and executable plans. (3) Annotation cost: Annotating instructional videos with step-level labels (i.e., timestamp) or sequence-level labels (i.e., action category) is demanding and labor-intensive, limiting its generalizability to large-scale datasets.In this work, we propose a new and practical setting, called adaptive procedure planning in instructional videos, where the
    
[^23]: 均匀分词器的重要性：用于遥感图像理解的均匀视觉分词器

    Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote Sensing Image Understanding

    [https://arxiv.org/abs/2403.18593](https://arxiv.org/abs/2403.18593)

    通过定义语义独立区域（SIRs）并设计HOmogeneous视觉tOKenizer (HOOK)，实现了使用有意义的基本元素来加强遥感图像理解。

    

    标记器作为大型模型的基本组件之一，长期以来在视觉任务中被忽视甚至误解。大语言模型具有强大理解能力的一个关键因素是自然语言标记器利用有意义的词或子词作为语言的基本元素。相比之下，以基于补丁的方法如Patch Embed为代表的主流视觉标记器依赖于无意义的矩形补丁作为视觉的基本元素，这不能像语言中的词或子词一样有效地发挥作用。从标记器的本质出发，我们为视觉定义了语义独立区域（SIRs）。我们设计了一个简单的HOmogeneous视觉tOKenizer: HOOK。HOOK主要由两个模块组成：物体感知模块（OPM）和物体矢量化模块（OVM）。为实现均匀性，OPM将图像分割为4*4像素种子，然后利用注意力机制来。

    arXiv:2403.18593v1 Announce Type: cross  Abstract: The tokenizer, as one of the fundamental components of large models, has long been overlooked or even misunderstood in visual tasks. One key factor of the great comprehension power of the large language model is that natural language tokenizers utilize meaningful words or subwords as the basic elements of language. In contrast, mainstream visual tokenizers, represented by patch-based methods such as Patch Embed, rely on meaningless rectangular patches as basic elements of vision, which cannot serve as effectively as words or subwords in language. Starting from the essence of the tokenizer, we defined semantically independent regions (SIRs) for vision. We designed a simple HOmogeneous visual tOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception Module (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity, the OPM splits the image into 4*4 pixel seeds and then utilizes the attention mechanism to pe
    
[^24]: 物理信息图神经网络用于水配系统

    Physics-Informed Graph Neural Networks for Water Distribution Systems

    [https://arxiv.org/abs/2403.18570](https://arxiv.org/abs/2403.18570)

    提出了一种用于水配系统的物理信息图神经网络模型，利用水力原理以无监督方式重建水力状态特征。

    

    水配系统（WDS）是城市发展至关重要的关键基础设施。由于世界70%的人口可能会在2050年生活在城市环境中，因此对于WDS的高效仿真和规划工具在实现联合国可持续发展目标（SDG）6 - “为所有人提供清洁水和卫生设施”中发挥着至关重要的作用。在这个领域中，我们提出了一个新颖而高效的机器学习仿真器，更确切地说，是一个用于WDS中的水力状态估计的物理信息深度学习（DL）模型。我们的模型使用了一种递归方法，只需要几个图卷积神经网络（GCN）层，并采用了一种基于消息传递的创新算法。与传统的机器学习任务不同，该模型利用水力原理在无监督方式下推断出两个额外的水力状态特征，从而重建可用的地面实况特征。

    arXiv:2403.18570v1 Announce Type: cross  Abstract: Water distribution systems (WDS) are an integral part of critical infrastructure which is pivotal to urban development. As 70% of the world's population will likely live in urban environments in 2050, efficient simulation and planning tools for WDS play a crucial role in reaching UN's sustainable developmental goal (SDG) 6 - "Clean water and sanitation for all". In this realm, we propose a novel and efficient machine learning emulator, more precisely, a physics-informed deep learning (DL) model, for hydraulic state estimation in WDS. Using a recursive approach, our model only needs a few graph convolutional neural network (GCN) layers and employs an innovative algorithm based on message passing. Unlike conventional machine learning tasks, the model uses hydraulic principles to infer two additional hydraulic state features in the process of reconstructing the available ground truth feature in an unsupervised manner. To the best of our k
    
[^25]: PDNNet：面向动态IR掉电预测的PDN感知GNN-CNN异构网络

    PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop Prediction

    [https://arxiv.org/abs/2403.18569](https://arxiv.org/abs/2403.18569)

    提出了面向动态IR掉电预测的PDN感知GNN-CNN异构网络，引入了PDNGraph图结构和双分支异构网络PDNNet，以同时考虑PDN结构和单元-PDN关系，有助于更好地预测IR掉电。

    

    电源供应网络（PDN）上的IR掉电与PDN的配置和电流消耗密切相关。随着集成电路（IC）设计的不断增大，动态IR掉电仿真变得计算成本高昂，基于机器学习的IR掉电预测被探索为一种有前途的解决方案。本文考虑不仅如何正确表示单元-PDN关系，还考虑如何在特征聚合过程中模拟IR掉电遵循其物理特性。因此，我们提出了一种新颖的图结构，PDNGraph，统一了PDN结构和细粒度单元-PDN关系的表示。我们进一步提出了一种双分支异构网络，PDNNet，将两个并行的GNN-CNN分支整合在一起，有利于捕捉上述特征。

    arXiv:2403.18569v1 Announce Type: cross  Abstract: IR drop on the power delivery network (PDN) is closely related to PDN's configuration and cell current consumption. As the integrated circuit (IC) design is growing larger, dynamic IR drop simulation becomes computationally unaffordable and machine learning based IR drop prediction has been explored as a promising solution. Although CNN-based methods have been adapted to IR drop prediction task in several works, the shortcomings of overlooking PDN configuration is non-negligible. In this paper, we consider not only how to properly represent cell-PDN relation, but also how to model IR drop following its physical nature in the feature aggregation procedure. Thus, we propose a novel graph structure, PDNGraph, to unify the representations of the PDN structure and the fine-grained cell-PDN relation. We further propose a dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN branches to favorably capture the above feat
    
[^26]: 使用BERT进行句子分类的神经架构搜索

    Neural Architecture Search for Sentence Classification with BERT

    [https://arxiv.org/abs/2403.18547](https://arxiv.org/abs/2403.18547)

    本文质疑了仅在BERT网络顶部添加单个输出层作为分类头的常规做法，通过进行AutoML搜索找到了在较小计算成本下优于当前单层的架构，并在GLUE数据集上进行了验证。

    

    在自然语言处理中，对大型文本语料库进行语言模型的预训练是常见做法，随后对这些模型进行微调以在各种任务上取得最佳结果。本文质疑仅在网络顶部添加单个输出层作为分类头的常规做法。我们进行自动机器学习搜索，找到一些在较小计算成本下胜过当前单层的架构。我们在GLUE数据集的各种自然语言处理基准上验证了我们的分类架构。

    arXiv:2403.18547v1 Announce Type: new  Abstract: Pre training of language models on large text corpora is common practice in Natural Language Processing. Following, fine tuning of these models is performed to achieve the best results on a variety of tasks. In this paper we question the common practice of only adding a single output layer as a classification head on top of the network. We perform an AutoML search to find architectures that outperform the current single layer at only a small compute cost. We validate our classification architecture on a variety of NLP benchmarks from the GLUE dataset.
    
[^27]: 在混乱场景中高效的基于热图引导的六自由度抓取检测

    Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes

    [https://arxiv.org/abs/2403.18546](https://arxiv.org/abs/2403.18546)

    本文提出了一种利用抓取热图引导的高效局部抓取生成方法，从全局到局部语义到点的方式推断，显著提高了抓取准确性和多样性。

    

    在机器人领域中，快速而稳健的物体抓取是至关重要的。本文指出当前大多数研究都仅利用整个观察到的点云来生成六自由度的抓取，忽略了从全局语义中挖掘的引导信息，从而限制了高质量抓取的生成和实时性能。为此，本文展示了广泛使用的热图在六自由度抓取生成效率方面被低估。因此，我们提出了一个有效的局部抓取生成器，并结合抓取热图作为引导，以全局到局部语义到点的方式进行推断。具体来说，我们应用了高斯编码和基于网格的策略来预测抓取热图以指导将局部点聚合到可抓取区域，并提供全局语义信息。此外，设计了一种新颖的非均匀锚定采样机制来提高抓取的准确性和多样性。受益于图像空间中的高效编码

    arXiv:2403.18546v1 Announce Type: cross  Abstract: Fast and robust object grasping in clutter is a crucial component of robotics. Most current works resort to the whole observed point cloud for 6-Dof grasp generation, ignoring the guidance information excavated from global semantics, thus limiting high-quality grasp generation and real-time performance. In this work, we show that the widely used heatmaps are underestimated in the efficiency of 6-Dof grasp generation. Therefore, we propose an effective local grasp generator combined with grasp heatmaps as guidance, which infers in a global-to-local semantic-to-point way. Specifically, Gaussian encoding and the grid-based strategy are applied to predict grasp heatmaps as guidance to aggregate local points into graspable regions and provide global semantic information. Further, a novel non-uniform anchor sampling mechanism is designed to improve grasp accuracy and diversity. Benefiting from the high-efficiency encoding in the image space 
    
[^28]: 通往法律自治的路径：利用大语言模型、专家系统和贝叶斯网络提取、转换、加载和计算法律信息的可互操作和可解释方法

    A Path Towards Legal Autonomy: An interoperable and explainable approach to extracting, transforming, loading and computing legal information using large language models, expert systems and Bayesian networks

    [https://arxiv.org/abs/2403.18537](https://arxiv.org/abs/2403.18537)

    本文提出了一种可互操作和可解释的方法，利用大语言模型、专家系统和贝叶斯网络来提取、转换、加载和计算法律信息，从而实现通向法律自治的路径。

    

    法律自治 - 人工智能代理的合法活动 - 可以通过两种方式实现。一种方式是对AI行为者（如开发者、部署者和用户）和AI资源（如数据）施加约束，或者是对AI代理在环境中可能产生影响的范围和影响程度施加约束。后一种方法涉及将关于由AI驱动的设备的现有规则编码到控制这些设备的AI代理软件中（例如，将关于无人机设备操作范围限制的规则编码到无人机设备的代理软件中）。然而，这是一个挑战，因为这种方法的有效性需要一种提取、加载、转换和计算法律信息的方法，这种方法既可解释又可在法律上相互操作，并能让AI代理推理法律。本文中，我们将概述使用大型语言模型、专家系统和贝叶斯网络证明这种方法的原理。

    arXiv:2403.18537v1 Announce Type: new  Abstract: Legal autonomy - the lawful activity of artificial intelligence agents - can be achieved in one of two ways. It can be achieved either by imposing constraints on AI actors such as developers, deployers and users, and on AI resources such as data, or by imposing constraints on the range and scope of the impact that AI agents can have on the environment. The latter approach involves encoding extant rules concerning AI driven devices into the software of AI agents controlling those devices (e.g., encoding rules about limitations on zones of operations into the agent software of an autonomous drone device). This is a challenge since the effectivity of such an approach requires a method of extracting, loading, transforming and computing legal information that would be both explainable and legally interoperable, and that would enable AI agents to reason about the law. In this paper, we sketch a proof of principle for such a method using large 
    
[^29]: 一种新颖的基于行为的电子商务推荐系统

    A Novel Behavior-Based Recommendation System for E-commerce

    [https://arxiv.org/abs/2403.18536](https://arxiv.org/abs/2403.18536)

    提出了一种基于顾客行为的推荐方法，通过利用顾客在电子商务平台上的自然行为来生成准确的推荐结果

    

    大多数现有的推荐系统依赖于用户评分，这受到用户协作欠缺和稀疏问题的限制。为了解决这些问题，本研究提出了一种基于行为的推荐系统，利用顾客在电子商务平台上的自然行为，如浏览和点击。提出的推荐系统涉及对活跃顾客进行聚类、确定邻域、收集相似用户、基于相似用户计算产品声誉以及推荐高声誉产品。为了克服顾客行为和传统聚类方法的复杂性，开发了一种基于产品类别的无监督聚类方法，以增强推荐方法。该研究在几个方面做出了显著贡献。

    arXiv:2403.18536v1 Announce Type: cross  Abstract: The majority of existing recommender systems rely on user ratings, which are limited by the lack of user collaboration and the sparsity problem. To address these issues, this study proposes a behavior-based recommender system that leverages customers' natural behaviors, such as browsing and clicking, on e-commerce platforms. The proposed recommendation system involves clustering active customers, determining neighborhoods, collecting similar users, calculating product reputation based on similar users, and recommending high-reputation products. To overcome the complexity of customer behaviors and traditional clustering methods, an unsupervised clustering approach based on product categories is developed to enhance the recommendation methodology. This study makes notable contributions in several aspects. Firstly, a groundbreaking behavior-based recommendation methodology is developed, incorporating customer behavior to generate accurate
    
[^30]: 改进大规模神经网络训练的线搜索方法

    Improving Line Search Methods for Large Scale Neural Network Training

    [https://arxiv.org/abs/2403.18519](https://arxiv.org/abs/2403.18519)

    本文改进了大规模神经网络训练的线搜索方法，通过将ADAM的动量项集成到Armijo线搜索中，实现了高效的大规模训练，并且优于以往的方法和Adam的调整学习率。

    

    在最近的研究中，线搜索方法在传统随机梯度下降技术的性能方面取得了显著进展，消除了需要特定学习率调度的需求。本文识别了现有最先进线搜索方法中存在的问题，提出了增强措施，并对其效果进行了严格评估。我们在比以往更大的数据集和更复杂的数据领域上测试了这些方法。具体来说，我们通过将ADAM的动量项集成到Armijo线搜索中的搜索方向中，改进了Armijo线搜索，实现了高效的大规模训练，这是以前使用Armijo线搜索方法容易失败的任务。我们的优化方法胜过以前的Armijo实现和Adam的调整学习率调度。我们的评估重点放在NLP和图像数据领域的Transformer和CNN上。我们的工作以Python包的形式公开发布，可以下载使用。

    arXiv:2403.18519v1 Announce Type: cross  Abstract: In recent studies, line search methods have shown significant improvements in the performance of traditional stochastic gradient descent techniques, eliminating the need for a specific learning rate schedule. In this paper, we identify existing issues in state-of-the-art line search methods, propose enhancements, and rigorously evaluate their effectiveness. We test these methods on larger datasets and more complex data domains than before. Specifically, we improve the Armijo line search by integrating the momentum term from ADAM in its search direction, enabling efficient large-scale training, a task that was previously prone to failure using Armijo line search methods. Our optimization approach outperforms both the previous Armijo implementation and tuned learning rate schedules for Adam. Our evaluation focuses on Transformers and CNNs in the domains of NLP and image data. Our work is publicly available as a Python package, which prov
    
[^31]: 使用线搜索方法加速Transformer微调的收敛速度

    Faster Convergence for Transformer Fine-tuning with Line Search Methods

    [https://arxiv.org/abs/2403.18506](https://arxiv.org/abs/2403.18506)

    将Armijo线搜索与Adam优化器相结合，通过在本地单元执行线搜索，实现了在Transformer微调中收敛速度更快的优化方法。

    

    最近的研究表明，线搜索方法极大地提高了传统随机梯度下降方法在各种数据集和架构上的性能。在这项工作中，我们成功将线搜索方法扩展到了新颖且备受欢迎的Transformer架构和自然语言处理领域的数据集。具体来说，我们将Armijo线搜索与Adam优化器相结合，并通过将网络架构细分为合理的单元，在这些本地单元上分别执行线搜索。我们的优化方法优于传统的Adam优化器，在小数据集或小训练预算的情况下实现了显著的性能改进，同时在其他测试案例中表现相等或更好。我们的工作作为一个Python包公开可用，提供了一个无需超参数的PyTorch优化器，与任意网络架构兼容。

    arXiv:2403.18506v1 Announce Type: cross  Abstract: Recent works have shown that line search methods greatly increase performance of traditional stochastic gradient descent methods on a variety of datasets and architectures [1], [2]. In this work we succeed in extending line search methods to the novel and highly popular Transformer architecture and dataset domains in natural language processing. More specifically, we combine the Armijo line search with the Adam optimizer and extend it by subdividing the networks architecture into sensible units and perform the line search separately on these local units. Our optimization method outperforms the traditional Adam optimizer and achieves significant performance improvements for small data sets or small training budgets, while performing equal or better for other tested cases. Our work is publicly available as a python package, which provides a hyperparameter-free pytorch optimizer that is compatible with arbitrary network architectures.
    
[^32]: 利用天气预报数据作为深度神经网络模型估计蒸散发的输入的影响

    Impact of Employing Weather Forecast Data as Input to the Estimation of Evapotranspiration by Deep Neural Network Models

    [https://arxiv.org/abs/2403.18489](https://arxiv.org/abs/2403.18489)

    利用天气预报数据作为深度神经网络模型估计蒸散发的输入，解决了使用FAO56-PM方法计算ET0时太阳辐射参数不易获取的问题

    

    参考蒸散发（ET0）是设计智能灌溉调度的关键参数，因为它通过系数与作物的水需求相关。联合国粮食和农业组织提出了一种标准的ET0计算方法（FAO56PM），基于Penman-Monteith方程的参数化，该方法在文献中被广泛采用。使用FAO56-PM方法计算ET0需要四个主要的天气参数：温度、湿度、风速和太阳辐射（SR）。一种预测未来几天的每日ET0值的方法是利用免费提供的天气预报服务（WFSs），这些服务可估计多种气象参数长达未来15天。这种方法的问题在于当前大多数在线服务没有提供SR作为免费的预测参数，通常这样的预测需要支付金钱。因此，出现了几种使用

    arXiv:2403.18489v1 Announce Type: new  Abstract: Reference Evapotranspiration (ET0) is a key parameter for designing smart irrigation scheduling, since it is related by a coefficient to the water needs of a crop. The United Nations Food and Agriculture Organization, proposed a standard method for ET0 computation (FAO56PM), based on the parameterization of the Penman-Monteith equation, that is widely adopted in the literature. To compute ET0 using the FAO56-PM method, four main weather parameters are needed: temperature, humidity, wind, and solar radiation (SR). One way to make daily ET0 estimations for future days is to use freely available weather forecast services (WFSs), where many meteorological parameters are estimated up to the next 15 days. A problem with this method is that currently, SR is not provided as a free forecast parameter on most of those online services or, normally, such forecasts present a financial cost penalty. For this reason, several ET0 estimation models using
    
[^33]: 使用条件扩散模型从事件相关电位范式中合成脑电图信号

    Synthesizing EEG Signals from Event-Related Potential Paradigms with Conditional Diffusion Models

    [https://arxiv.org/abs/2403.18486](https://arxiv.org/abs/2403.18486)

    通过引入使用无分类器引导的条件扩散模型，可以直接生成特定主体、会话和类别的EEG数据，结果表明该模型生成的数据与真实数据相似。

    

    大脑-计算机接口领域中的数据稀缺问题可以通过使用生成模型，特别是扩散模型得以缓解。虽然扩散模型先前已成功应用于脑电图（EEG）数据，但现有模型在采样灵活性方面存在限制或需要EEG数据的替代表示。为了克服这些限制，我们介绍了一种新颖的条件扩散模型方法，利用无分类器引导直接生成特定主体、会话和类别的EEG数据。除了常用的指标外，还使用领域特定的指标来评估生成样本的特定性。结果表明，所提出的模型可以生成与每个主体、会话和类别的真实数据相似的EEG数据。

    arXiv:2403.18486v1 Announce Type: cross  Abstract: Data scarcity in the brain-computer interface field can be alleviated through the use of generative models, specifically diffusion models. While diffusion models have previously been successfully applied to electroencephalogram (EEG) data, existing models lack flexibility w.r.t.~sampling or require alternative representations of the EEG data. To overcome these limitations, we introduce a novel approach to conditional diffusion models that utilizes classifier-free guidance to directly generate subject-, session-, and class-specific EEG data. In addition to commonly used metrics, domain-specific metrics are employed to evaluate the specificity of the generated samples. The results indicate that the proposed model can generate EEG data that resembles real data for each subject, session, and class.
    
[^34]: 密度引导的翻译器推动了三维点云的合成到真实无监督领域自适应分割

    Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain Adaptive Segmentation of 3D Point Clouds

    [https://arxiv.org/abs/2403.18469](https://arxiv.org/abs/2403.18469)

    提出了一种密度引导的翻译器（DGT）以改善三维点云的合成到真实无监督领域自适应分割，解决了不同传感器采样模式和不完整训练策略的限制。

    

    3D合成到真实无监督领域自适应分割对于标注新领域至关重要。本研究提出了一种密度引导的翻译器（DGT），用于在两阶段自训练管道中传输点云密度。首先，我们采用不可学习的DGT来在输入层级上弥合域之间的差距。其次，在第一阶段提出了一个类别级对抗网络，利用原型来为自训练提供良好初始化模型，以防止负迁移。最后，通过利用上述设计，实现了一个混合领域的自训练。

    arXiv:2403.18469v1 Announce Type: cross  Abstract: 3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to annotating new domains. Self-training is a competitive approach for this task, but its performance is limited by different sensor sampling patterns (i.e., variations in point density) and incomplete training strategies. In this work, we propose a density-guided translator (DGT), which translates point density between domains, and integrates it into a two-stage self-training pipeline named DGT-ST. First, in contrast to existing works that simultaneously conduct data generation and feature/output alignment within unstable adversarial training, we employ the non-learnable DGT to bridge the domain gap at the input level. Second, to provide a well-initialized model for self-training, we propose a category-level adversarial network in stage one that utilizes the prototype to prevent negative transfer. Finally, by leveraging the designs above, a domain-mixed self-tra
    
[^35]: CoBOS: 基于约束的人机协作在线调度器

    CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration

    [https://arxiv.org/abs/2403.18459](https://arxiv.org/abs/2403.18459)

    CoBOS提出了一种新颖的在线基于约束的调度方法，在人机协作中实现了机器人对不确定事件的适应性，大大减轻了用户的压力，提高了工作效率。

    

    涉及人类和机器人的装配过程是具有挑战性的场景，因为个人活动和共享工作空间的访问必须协调。固定的机器人程序不允许偏离固定协议。在这样的过程中工作可能会让用户感到有压力，并导致行为无效或失败。我们提出了一种新颖的在线基于约束的调度方法，位于支持行为树的反应式执行控制框架中，名为CoBOS。这使得机器人能够适应延迟活动完成和活动选择（由人类）等不确定事件。用户将体验到较少的压力，因为机器人同事会调整其行为以最好地补充人类选择的活动，以完成共同任务。除了改善的工作条件，我们的算法还导致了效率的提高，即使在高度不确定的情况下也是如此。我们使用一个概率性的si来评估我们的算法

    arXiv:2403.18459v1 Announce Type: cross  Abstract: Assembly processes involving humans and robots are challenging scenarios because the individual activities and access to shared workspace have to be coordinated. Fixed robot programs leave no room to diverge from a fixed protocol. Working on such a process can be stressful for the user and lead to ineffective behavior or failure. We propose a novel approach of online constraint-based scheduling in a reactive execution control framework facilitating behavior trees called CoBOS. This allows the robot to adapt to uncertain events such as delayed activity completions and activity selection (by the human). The user will experience less stress as the robotic coworkers adapt their behavior to best complement the human-selected activities to complete the common task. In addition to the improved working conditions, our algorithm leads to increased efficiency, even in highly uncertain scenarios. We evaluate our algorithm using a probabilistic si
    
[^36]: CoRAST：面向资源受限的CPS和IoT中基于基础模型的相关数据分析

    CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in Resource-Constrained CPS and IoT

    [https://arxiv.org/abs/2403.18451](https://arxiv.org/abs/2403.18451)

    CoRAST提出了一种新颖的学习框架，利用基础模型(FMs)增强了分布式、相关的异构数据的分析。

    

    基础模型(FMs)作为一种有前途的解决方案出现，通过利用先前的知识来理解异构数据集中复杂的时间和空间相关性，从而利用分布式和多样化的环境数据。与联合学习等分布式学习框架不同，它们通常难以处理多模态数据，FMs可以将不同的输入转换为嵌入。这一过程有助于整合各种模态的信息，并将先前的学习应用于新的领域。然而，在资源受限的边缘系统中部署FMs会带来重大挑战。为此，我们引入了CoRAST，这是一个新颖的学习框架，利用FMs增强了分布式、相关的异构数据的分析。通过利用基于服务器的FM，CoRAST可以利用现有的环境信息来提取传感器数据之间的时间、空间和跨模态相关性。

    arXiv:2403.18451v1 Announce Type: cross  Abstract: Foundation models (FMs) emerge as a promising solution to harness distributed and diverse environmental data by leveraging prior knowledge to understand the complicated temporal and spatial correlations within heterogeneous datasets. Unlike distributed learning frameworks such as federated learning, which often struggle with multimodal data, FMs can transform diverse inputs into embeddings. This process facilitates the integration of information from various modalities and the application of prior learning to new domains. However, deploying FMs in resource-constrained edge systems poses significant challenges. To this end, we introduce CoRAST, a novel learning framework that utilizes FMs for enhanced analysis of distributed, correlated heterogeneous data. Utilizing a server-based FM, CoRAST can exploit existing environment information to extract temporal, spatial, and cross-modal correlations among sensor data. This enables CoRAST to o
    
[^37]: U-Sketch: 一种用于草图到图像扩散模型的高效方法

    U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models

    [https://arxiv.org/abs/2403.18425](https://arxiv.org/abs/2403.18425)

    引入了U-Sketch框架，具有U-Net类型的潜在边缘预测器，能够有效地改进草图到图像扩散模型的空间布局生成。

    

    扩散模型在文本到图像合成中表现出色，产生了符合相应文本提示的逼真高分辨率图像。尽管取得了巨大成功，但在草图到图像合成任务中仍有所欠缺，生成图像的空间布局不仅要遵循文本提示，还必须紧密跟随某些参考草图的轮廓。最近提出了使用MLP潜在边缘预测器来引导合成图像的空间布局，通过在每个去噪步骤预测边缘地图。尽管取得了有希望的结果，但MLP的逐像素操作并未将空间布局作为整体考虑进来，需要大量去噪迭代才能生成令人满意的图像，导致时间效率低下。因此，我们引入了U-Sketch，这是一个引入了U-Net类型潜在边缘预测器的框架，能够高效地

    arXiv:2403.18425v1 Announce Type: cross  Abstract: Diffusion models have demonstrated remarkable performance in text-to-image synthesis, producing realistic and high resolution images that faithfully adhere to the corresponding text-prompts. Despite their great success, they still fall behind in sketch-to-image synthesis tasks, where in addition to text-prompts, the spatial layout of the generated images has to closely follow the outlines of certain reference sketches. Employing an MLP latent edge predictor to guide the spatial layout of the synthesized image by predicting edge maps at each denoising step has been recently proposed. Despite yielding promising results, the pixel-wise operation of the MLP does not take into account the spatial layout as a whole, and demands numerous denoising iterations to produce satisfactory images, leading to time inefficiency. To this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge predictor, which is capable of efficiently
    
[^38]: BioMedLM：基于生物医学文本训练的27亿参数语言模型

    BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text

    [https://arxiv.org/abs/2403.18421](https://arxiv.org/abs/2403.18421)

    BioMedLM是一个27亿参数的语言模型，在PubMed文献上训练，可以在生物医学领域表现出色，尤其适用于多项选择问题回答和患者提问。

    

    arXiv:2403.18421v1 公告类型：跨领域 摘要：GPT-4和Med-PaLM 2等模型在各种生物医学NLP任务上表现出色。然而，这些模型有数千亿个参数，计算代价高昂，需要用户通过互联网发送输入数据，并且是在未知数据来源上训练的。更小且更有针对性的模型能否竞争？为了解决这个问题，我们构建并发布了BioMedLM，一个仅在PubMed摘要和全文上训练的27亿参数GPT风格的自回归模型。在进行微调时，BioMedLM可以产生强大的多项选择生物医学问题回答结果，与更大的模型竞争，例如在MedMCQA（dev）上取得57.3%的得分，在MMLU医学遗传学考试上取得69.0%的得分。BioMedLM还可以进行微调，以对医学话题上患者提出的问题提供有用的答案。这表明较小的模型潜在地可以作为透明且隐私性的服务提供者

    arXiv:2403.18421v1 Announce Type: cross  Abstract: Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance on a wide variety of biomedical NLP tasks. However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources. Can smaller, more targeted models compete? To address this question, we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive model trained exclusively on PubMed abstracts and full articles. When fine-tuned, BioMedLM can produce strong multiple-choice biomedical question-answering results competitive with much larger models, such as achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical Genetics exam. BioMedLM can also be fine-tuned to produce useful answers to patient questions on medical topics. This demonstrates that smaller models can potentially serve as transparent, privacy-
    
[^39]: 一种通道集成方法：无偏差和低方差的伪标签对半监督分类至关重要

    A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is Critical for Semi-supervised Classification

    [https://arxiv.org/abs/2403.18407](https://arxiv.org/abs/2403.18407)

    通过提出一种轻量级基于通道的集成方法，将多个较差的伪标签有效地合并为理论上保证的无偏差和低方差的一个伪标签，解决半监督学习中自我训练模型产生的有偏差和高方差预测问题。

    

    半监督学习（SSL）在计算机视觉中是一个实际挑战。伪标签（PL）方法，例如FixMatch和FreeMatch，在SSL中获得了现有技术的最佳性能。这些方法利用阈值到伪标签（T2L）处理通过截断自我训练方法预测的无标签数据的置信度得到PL。然而，自我训练模型通常会产生有偏差和高方差的预测，特别是在提供少量标记数据的情况下。为了解决这个问题，我们提出了一种轻量级基于通道的集成方法，将多个较差的伪标签有效地合并为理论上保证的无偏差和低方差的伪标签。重要的是，我们的方法可以轻松扩展到任何SSL框架，例如FixMatch或FreeMatch。实验结果表明，我们的方法在CIFAR10/100的效果方面显著优于现有技术。

    arXiv:2403.18407v1 Announce Type: cross  Abstract: Semi-supervised learning (SSL) is a practical challenge in computer vision. Pseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of The Art (SOTA) performances in SSL. These approaches employ a threshold-to-pseudo-label (T2L) process to generate PLs by truncating the confidence scores of unlabeled data predicted by the self-training method. However, self-trained models typically yield biased and high-variance predictions, especially in the scenarios when a little labeled data are supplied. To address this issue, we propose a lightweight channel-based ensemble method to effectively consolidate multiple inferior PLs into the theoretically guaranteed unbiased and low-variance one. Importantly, our approach can be readily extended to any SSL framework, such as FixMatch or FreeMatch. Experimental results demonstrate that our method significantly outperforms state-of-the-art techniques on CIFAR10/100 in terms of effectiv
    
[^40]: 图像网格可能比视频更有价值：使用VLM进行零样本视频问答

    An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM

    [https://arxiv.org/abs/2403.18406](https://arxiv.org/abs/2403.18406)

    本研究提出了一种新颖的策略，使用单一的Vision Language Model (VLM) 来进行零样本视频问答，将视频转换为单个合成图像以实现视频理解。

    

    受最近大型语言模型（LLM）复杂推理能力的启发，人们提出了各种用于连接视频模态的策略。其中一种突出的策略涉及视频语言模型（VideoLMs），通过训练一个可学习的接口将先进的视觉编码器与LLMs连接起来。最近，出现了另一种旨在通过多个阶段跨模态进行模态桥接的策略，利用现成的基础模型，如VideoLMs和LLMs。本研究中，我们介绍了一种简单却新颖的策略，只使用单一的视觉语言模型（VLM）。我们的出发点是视频包含一系列图像或帧，这些图像与时间信息交织在一起的简单洞察。视频理解的精髓在于巧妙地管理每个帧的时间方面以及空间细节。初始时，我们通过排列多个帧将视频转换为单个合成图像。

    arXiv:2403.18406v1 Announce Type: cross  Abstract: Stimulated by the sophisticated reasoning capabilities of recent Large Language Models (LLMs), a variety of strategies for bridging video modality have been devised. A prominent strategy involves Video Language Models (VideoLMs), which train a learnable interface with video data to connect advanced vision encoders with LLMs. Recently, an alternative strategy has surfaced, employing readily available foundation models, such as VideoLMs and LLMs, across multiple stages for modality bridging. In this study, we introduce a simple yet novel strategy where only a single Vision Language Model (VLM) is utilized. Our starting point is the plain insight that a video comprises a series of images, or frames, interwoven with temporal information. The essence of video comprehension lies in adeptly managing the temporal aspects along with the spatial details of each frame. Initially, we transform a video into a single composite image by arranging mul
    
[^41]: 利用大型语言模型进行法律案例检索中的相关性判断

    Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval

    [https://arxiv.org/abs/2403.18405](https://arxiv.org/abs/2403.18405)

    设计一种新颖的几轮工作流程，专门用于法律案例的相关判断，能够通过模仿人类注释者的过程并整合专家推理，提高相关性判断的准确性。

    

    收集法律案例检索的相关判决是一项具有挑战性且耗时的任务。准确判断两个法律案例之间的相关性需要阅读冗长的文本并具备高水平的领域专业知识以提取法律事实并作出司法判断。随着先进的大型语言模型的出现，一些最近的研究表明使用LLM（Large Language Models）进行相关性判断是有前途的。然而，将一般性大型语言模型应用于法律案例检索中可靠的相关性判断的方法尚未得到充分探讨。为了填补这一研究空白，我们设计了一种新颖的几轮工作流程，专门用于法律案例的相关判断。所提出的工作流程将注释过程分解为一系列阶段，模仿人类注释者所使用的过程，并使专家推理能够灵活地整合以增强相关性判断的准确性。

    arXiv:2403.18405v1 Announce Type: new  Abstract: Collecting relevant judgments for legal case retrieval is a challenging and time-consuming task. Accurately judging the relevance between two legal cases requires a considerable effort to read the lengthy text and a high level of domain expertise to extract Legal Facts and make juridical judgments. With the advent of advanced large language models, some recent studies have suggested that it is promising to use LLMs for relevance judgment. Nonetheless, the method of employing a general large language model for reliable relevance judgments in legal case retrieval is yet to be thoroughly explored. To fill this research gap, we devise a novel few-shot workflow tailored to the relevant judgment of legal cases. The proposed workflow breaks down the annotation process into a series of stages, imitating the process employed by human annotators and enabling a flexible integration of expert reasoning to enhance the accuracy of relevance judgments.
    
[^42]: 使用改进的深度卷积生成对抗网络在抽象艺术中进行颜色和笔触模式识别

    Colour and Brush Stroke Pattern Recognition in Abstract Art using Modified Deep Convolutional Generative Adversarial Networks

    [https://arxiv.org/abs/2403.18397](https://arxiv.org/abs/2403.18397)

    本文通过引入改进的深度卷积生成对抗网络(mDCGAN)，针对高质量艺术品生成进行了研究，解决了普遍训练问题，有效探索抽象绘画中的颜色和笔触模式。

    

    抽象艺术是一种广受欢迎、被广泛讨论的艺术形式，通常能够描绘出艺术家的情感。许多研究人员尝试使用机器学习和深度学习的边缘检测、笔触和情感识别算法来研究抽象艺术。本文描述了使用生成对抗神经网络(GAN)对广泛分布的抽象绘画进行研究。 GAN具有学习和再现分布的能力，使研究人员能够有效地探索和研究生成的图像空间。然而，挑战在于开发一种能够克服常见训练问题的高效GAN架构。本文通过引入专门设计用于高质量艺术品生成的改进DCGAN(mDCGAN)来解决这一挑战。该方法涉及对所做修改的深入探讨，深入研究DCGAN的复杂工作。

    arXiv:2403.18397v1 Announce Type: cross  Abstract: Abstract Art is an immensely popular, discussed form of art that often has the ability to depict the emotions of an artist. Many researchers have made attempts to study abstract art in the form of edge detection, brush stroke and emotion recognition algorithms using machine and deep learning. This papers describes the study of a wide distribution of abstract paintings using Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and reproduce a distribution enabling researchers and scientists to effectively explore and study the generated image space. However, the challenge lies in developing an efficient GAN architecture that overcomes common training pitfalls. This paper addresses this challenge by introducing a modified-DCGAN (mDCGAN) specifically designed for high-quality artwork generation. The approach involves a thorough exploration of the modifications made, delving into the intricate workings of DCGANs, opt
    
[^43]: FTBC: 用于优化ANN-SNN转换的前向时间偏差校正

    FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion

    [https://arxiv.org/abs/2403.18388](https://arxiv.org/abs/2403.18388)

    通过前向时间偏差校正技术，本研究提出一种能够提高ANN-SNN转换准确性、避免计算开销的新方法，并通过理论发现表明，适当的时间偏差校准能够将转换误差降低至零。

    

    脉冲神经网络（SNNs）相对于人工神经网络（ANNs）提供了一种节能计算的有前途途径，与生物神经过程密切相似。然而，直接通过时空反向传播训练SNNs存在固有挑战，源自尖峰神经元的时间动态和其离散信号处理，这需要通过ANN-SNN转换等替代训练方式。在本研究中，我们介绍了一种轻量级的前向时间偏差校正（FTBC）技术，旨在提高转换准确性而无需计算开销。我们基于提供的理论发现，通过正确的时间偏差校准，每个时间步后ANN-SNN转换的预期误差可以降至零。我们进一步提出了一种启发式算法，用于在前向传递中找到时间偏差。

    arXiv:2403.18388v1 Announce Type: new  Abstract: Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient computing compared with Artificial Neural Networks (ANNs), closely mirroring biological neural processes. However, this potential comes with inherent challenges in directly training SNNs through spatio-temporal backpropagation -- stemming from the temporal dynamics of spiking neurons and their discrete signal processing -- which necessitates alternative ways of training, most notably through ANN-SNN conversion. In this work, we introduce a lightweight Forward Temporal Bias Correction (FTBC) technique, aimed at enhancing conversion accuracy without the computational overhead. We ground our method on provided theoretical findings that through proper temporal bias calibration the expected error of ANN-SNN conversion can be reduced to be zero after each time step. We further propose a heuristic algorithm for finding the temporal bias only in the forward pass, thus e
    
[^44]: 多模态生成模型是良好的类增量学习器

    Generative Multi-modal Models are Good Class-Incremental Learners

    [https://arxiv.org/abs/2403.18383](https://arxiv.org/abs/2403.18383)

    多模态生成模型为类增量学习提出了一种新的框架，可以直接为图像生成标签。

    

    在类增量学习（CIL）场景中，由于分类器对当前任务的偏见而导致的灾难性遗忘现象长期以来一直是一个重大挑战。这主要是由判别模型的特性所致。随着多模态生成模型越来越受欢迎，我们将探讨将判别模型替换为生成模型以用于CIL。然而，从判别模型过渡到生成模型需要解决两个关键挑战。主要挑战在于将生成的文本信息转移到不同类别的分类中。此外，它还需要将CIL任务置于生成框架中。为此，我们提出了一种用于类增量学习的新型多模态生成模型（GMM）框架。我们的方法直接使用经调整的生成模型为图像生成标签。在获得详细文本后，我们使用

    arXiv:2403.18383v1 Announce Type: cross  Abstract: In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic forgetting caused by the classifier's bias towards the current task has long posed a significant challenge. It is mainly caused by the characteristic of discriminative models. With the growing popularity of the generative multi-modal models, we would explore replacing discriminative models with generative ones for CIL. However, transitioning from discriminative to generative models requires addressing two key challenges. The primary challenge lies in transferring the generated textual information into the classification of distinct categories. Additionally, it requires formulating the task of CIL within a generative framework. To this end, we propose a novel generative multi-modal model (GMM) framework for class-incremental learning. Our approach directly generates labels for images using an adapted generative model. After obtaining the detailed text, we use 
    
[^45]: 通过偏好学习改进大型语言模型的属性文本生成

    Improving Attributed Text Generation of Large Language Models via Preference Learning

    [https://arxiv.org/abs/2403.18381](https://arxiv.org/abs/2403.18381)

    通过偏好学习建模和引入自动偏好优化框架，该研究解决了大型语言模型生成不可靠内容的挑战，并提出了一种自动生成归因偏好数据的方法。

    

    大型语言模型已广泛应用于自然语言处理，但它们面临生成不可靠内容的挑战。最近的研究旨在通过归因作为提供证据（即引用）的手段来减少错误信息和幻觉。然而，目前的归因方法通常侧重于检索阶段和自动评估，忽视了在人类学术写作中反映引文机制以增强可信度。本文通过将归因任务建模为偏好学习，并引入自动偏好优化（APO）框架来解决这些挑战。首先，我们通过从现有数据集中收集和过滤创建了一个包含6,330个示例供后期训练使用的精心收集集合。其次，考虑到标记偏好数据的高成本，我们进一步提出了一种自动生成归因偏好数据的自动方法，生成了95,263对数据。

    arXiv:2403.18381v1 Announce Type: cross  Abstract: Large language models have been widely adopted in natural language processing, yet they face the challenge of generating unreliable content. Recent works aim to reduce misinformation and hallucinations by resorting to attribution as a means to provide evidence (i.e., citations). However, current attribution methods usually focus on the retrieval stage and automatic evaluation that neglect mirroring the citation mechanisms in human scholarly writing to bolster credibility. In this paper, we address these challenges by modelling the attribution task as preference learning and introducing an Automatic Preference Optimization (APO) framework. First, we create a curated collection for post-training with 6,330 examples by collecting and filtering from existing datasets. Second, considering the high cost of labelling preference data, we further propose an automatic method to synthesize attribution preference data resulting in 95,263 pairs. Mo
    
[^46]: IIP-Mixer：用于电池剩余寿命预测的Intra-Inter Patch混合架构

    IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining Useful Life Prediction

    [https://arxiv.org/abs/2403.18379](https://arxiv.org/abs/2403.18379)

    提出了一种基于MLPs的IIP-Mixer架构，旨在通过在内部补丁和跨补丁维度进行混合操作，实现锂离子电池剩余寿命预测。

    

    准确估计锂离子电池的剩余寿命对于维持可充电电池管理系统的安全稳定运行至关重要。然而，由于涉及复杂的时间动态，这项任务通常具有挑战性。最近，基于注意力的网络，如变压器和Informer，在时间序列预测中已经成为流行的架构。尽管它们有效，但这些具有丰富参数的模型必须耗费大量训练时间才能揭示时间模式。为了解决这些挑战，我们提出了一个简单的基于MLP-Mixer的架构，名为“Intra-Inter Patch Mixer”（IIP-Mixer），它是一种仅基于多层感知器（MLPs）的架构，通过沿着内部补丁和跨补丁维度进行混合操作来提取有关电池剩余寿命预测的信息。所提出的IIP-Mixer包括并行的双头混合器层：内部补丁混合ML

    arXiv:2403.18379v1 Announce Type: cross  Abstract: Accurately estimating the Remaining Useful Life (RUL) of lithium-ion batteries is crucial for maintaining the safe and stable operation of rechargeable battery management systems. However, this task is often challenging due to the complex temporal dynamics involved. Recently, attention-based networks, such as Transformers and Informer, have been the popular architecture in time series forecasting. Despite their effectiveness, these models with abundant parameters necessitate substantial training time to unravel temporal patterns. To tackle these challenges, we propose a simple MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which is an architecture based exclusively on multi-layer perceptrons (MLPs), extracting information by mixing operations along both intra-patch and inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer comprises parallel dual-head mixer layers: the intra-patch mixing ML
    
[^47]: 面向5G-NR的意图感知DRL上行动态调度器

    Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR

    [https://arxiv.org/abs/2403.18364](https://arxiv.org/abs/2403.18364)

    提出了一种面向工业物联网用户设备（IIoT UEs）的意图感知DRL上行动态调度器，通过深度强化学习（DRL）学习如何调度通信资源，并利用图结构的简化方案加速收敛，相较于传统调度方案，能有效保证IIoT UEs的意图表达。

    

    我们研究了支持工业物联网用户设备（IIoT UEs）具有意图（即所请求的服务质量（QoS））和随机流量到达的问题。提出了一种基于深度强化学习（DRL）的集中动态调度器，用于学习如何在IIoT UEs之间调度可用通信资源的时间频率资源。所提出的调度器利用RL框架来适应无线通信系统和流量到达中的动态变化。此外，提出了一种基于图的简化方案，以减少RL框架的状态和动作空间，以实现快速收敛和更好的学习策略。仿真结果表明，与几种传统调度方案（如轮询、半静态和启发式方法）相比，所提出的智能调度器在保证IIoT UEs所表达的意图方面具有有效性。

    arXiv:2403.18364v1 Announce Type: cross  Abstract: We investigate the problem of supporting Industrial Internet of Things user equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and random traffic arrival. A deep reinforcement learning (DRL) based centralized dynamic scheduler for time-frequency resources is proposed to learn how to schedule the available communication resources among the IIoT UEs. The proposed scheduler leverages an RL framework to adapt to the dynamic changes in the wireless communication system and traffic arrivals. Moreover, a graph-based reduction scheme is proposed to reduce the state and action space of the RL framework to allow fast convergence and a better learning strategy. Simulation results demonstrate the effectiveness of the proposed intelligent scheduler in guaranteeing the expressed intent of IIoT UEs compared to several traditional scheduling schemes, such as round-robin, semi-static, and heuristic approaches. The proposed sche
    
[^48]: 生成用于基于视觉的农业应用的多样化农业数据

    Generating Diverse Agricultural Data for Vision-Based Farming Applications

    [https://arxiv.org/abs/2403.18351](https://arxiv.org/abs/2403.18351)

    提出了一种专门用于生成合成农业场景的程序化模型，能够模拟植物的生长阶段、土壤条件和光照变化，为精准农业中的计算机视觉任务提供了全面资源，验证了该模型在提供机器学习训练数据方面的潜力

    

    我们提出了一个专门的程序化模型，用于生成合成的农业场景，重点是大豆作物以及各种杂草。该模型能够模拟这些植物的不同生长阶段、多样化的土壤条件，以及在不同光照条件下的随机田地布局。将现实世界的纹理和环境因素整合到程序化生成过程中，增强了合成数据的逼真度和适用性。我们的数据集包括带有语义标签的12,000张图像，为精准农业中的计算机视觉任务提供了全面的资源，如用于自主除草的语义分割。通过将合成数据与真实农业图像进行比较，我们验证了我们模型的有效性，展示了它在为农业中的机器学习模型提供训练数据方面的潜力。这种方法不仅提供了一种成本-effective的方式

    arXiv:2403.18351v1 Announce Type: cross  Abstract: We present a specialized procedural model for generating synthetic agricultural scenes, focusing on soybean crops, along with various weeds. This model is capable of simulating distinct growth stages of these plants, diverse soil conditions, and randomized field arrangements under varying lighting conditions. The integration of real-world textures and environmental factors into the procedural generation process enhances the photorealism and applicability of the synthetic data. Our dataset includes 12,000 images with semantic labels, offering a comprehensive resource for computer vision tasks in precision agriculture, such as semantic segmentation for autonomous weed control. We validate our model's effectiveness by comparing the synthetic data against real agricultural images, demonstrating its potential to significantly augment training data for machine learning models in agriculture. This approach not only provides a cost-effective s
    
[^49]: 一种基于量子模糊的方法用于实时检测太阳日冕空洞

    A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal Holes

    [https://arxiv.org/abs/2403.18347](https://arxiv.org/abs/2403.18347)

    本论文提出了一种基于量子计算的快速模糊c-均值技术，用于快速检测太阳日冕空洞（CHs）的区域。

    

    太阳日冕空洞（CHs）的检测和分析是太阳物理领域的重要研究领域。对于准确预测地磁风暴，这对各种空间和地面系统都产生直接或间接影响至关重要。迄今为止，太阳科学家依赖手动绘制方法来检测CHs。然而，随着图像处理技术的进步，一些自动图像分割方法已被用于检测CHs。尽管如此，快速准确检测CHs仍然是一个主要问题。在这项工作中，开发了一种新型基于量子计算的快速模糊c-均值技术，用于快速检测CHs区域。任务分为两个阶段进行，在第一阶段使用基于量子计算的快速模糊c-均值（QCFFCM）对太阳图像进行了分割，然后在后续阶段从中提取出CHs。

    arXiv:2403.18347v1 Announce Type: cross  Abstract: The detection and analysis of the solar coronal holes (CHs) is an important field of study in the domain of solar physics. Mainly, it is required for the proper prediction of the geomagnetic storms which directly or indirectly affect various space and ground-based systems. For the detection of CHs till date, the solar scientist depends on manual hand-drawn approaches. However, with the advancement of image processing technologies, some automated image segmentation methods have been used for the detection of CHs. In-spite of this, fast and accurate detection of CHs are till a major issues. Here in this work, a novel quantum computing-based fast fuzzy c-mean technique has been developed for fast detection of the CHs region. The task has been carried out in two stages, in first stage the solar image has been segmented using a quantum computing based fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted out from the 
    
[^50]: LC-LLM: 使用大型语言模型解释性车道变换意图和轨迹预测

    LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models

    [https://arxiv.org/abs/2403.18344](https://arxiv.org/abs/2403.18344)

    该论文提出了一个使用大型语言模型的可解释性车道变换预测模型，将车道变换预测任务重新构建为语言建模问题，以提高长期预测准确性和可解释性。

    

    为了确保在动态环境中安全驾驶，自动驾驶车辆应具备准确预测周围车辆车道变换意图并预测其未来轨迹的能力。本文通过提出LC-LLM，一个可解释性车道变换预测模型，利用大型语言模型（LLM）的强大推理能力和自我解释能力来解决现有运动预测方法在长期预测准确性和可解释性方面存在的问题。我们将车道变换预测任务重新构建为一个语言建模问题，通过自然语言处理异构驾驶场景信息作为LLM的输入提示，并使用监督微调技术专门为我们的车道变换预测任务定制LLM。这使我们能够利用LLM的强大功能

    arXiv:2403.18344v1 Announce Type: new  Abstract: To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict the lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information in natural language as prompts for input into the LLM and employing a supervised fine-tuning technique to tailor the LLM specifically for our lane change prediction task. This allows us to utilize the LLM's powerfu
    
[^51]: mALBERT：小型多语言BERT模型仍然值得吗？

    mALBERT: Is a Compact Multilingual BERT Model Still Worth It?

    [https://arxiv.org/abs/2403.18338](https://arxiv.org/abs/2403.18338)

    提出了第一个版本的多语言小型ALBERT模型，旨在解决预训练语言模型对道德和生态的影响，并通过与传统多语言PLM的比较，在经典NLP任务中评估了该模型的性能。

    

    在当前预训练语言模型（PLM）的趋势中，越来越多的批评涉及这些模型的道德和生态影响。本文考虑到这些批评意见，提出关注更小型的模型，如ALBERT这样的小型模型，这些模型在生态上比PLM更有优势。然而，PLM在自然语言处理任务中取得了巨大突破，如口语和自然语言理解、分类、问答任务。此外，PLM还具有多语言的优势，据我们所知，小型ALBERT模型的多语言版本尚不存在。基于这些事实，我们提出免费释放第一个使用Wikipedia数据预训练的多语言小型ALBERT模型的第一版，以符合这种语言模型的道德方面。我们还对模型在经典NLP任务中与传统多语言PLM进行评估。最后，这篇论文...

    arXiv:2403.18338v1 Announce Type: new  Abstract: Within the current trend of Pretained Language Models (PLM), emerge more and more criticisms about the ethical andecological impact of such models. In this article, considering these critical remarks, we propose to focus on smallermodels, such as compact models like ALBERT, which are more ecologically virtuous than these PLM. However,PLMs enable huge breakthroughs in Natural Language Processing tasks, such as Spoken and Natural LanguageUnderstanding, classification, Question--Answering tasks. PLMs also have the advantage of being multilingual, and,as far as we know, a multilingual version of compact ALBERT models does not exist. Considering these facts, wepropose the free release of the first version of a multilingual compact ALBERT model, pre-trained using Wikipediadata, which complies with the ethical aspect of such a language model. We also evaluate the model against classicalmultilingual PLMs in classical NLP tasks. Finally, this pap
    
[^52]: LLM可以进行正式对话吗？自动评估LLMs在转换和解释正式规范中的表现

    Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications

    [https://arxiv.org/abs/2403.18327](https://arxiv.org/abs/2403.18327)

    本文提出了一种方法，利用两个LLM的副本与验证器结合使用，能够自动评估其在自然语言描述和正式规范之间转换的能力，无需额外的人工输入。

    

    利益相关者经常用自然语言描述系统需求，然后由领域专家将其转换为形式化语法，从而增加设计成本。本文评估了大型语言模型（LLMs）在自然语言描述和正式规范之间转换的能力。我们提出了一种方法，可以利用两个LLM的副本与现成的验证器结合使用，无需任何额外的人工输入就可以自动评估其翻译能力。我们的方法使用语言语法生成形式化语法，自动生成数据集。我们进行了经验评估以衡量这种翻译任务的准确性。

    arXiv:2403.18327v1 Announce Type: cross  Abstract: Stakeholders often describe system requirements using natural language which are then converted to formal syntax by a domain-expert leading to increased design costs. This paper assesses the capabilities of Large Language Models (LLMs) in converting between natural language descriptions and formal specifications. Existing work has evaluated the capabilities of LLMs in generating formal syntax such as source code but such experiments are typically hand-crafted and use problems that are likely to be in the training set of LLMs, and often require human-annotated datasets. We propose an approach that can use two copies of an LLM in conjunction with an off-the-shelf verifier to automatically evaluate its translation abilities without any additional human input. Our approach generates formal syntax using language grammars to automatically generate a dataset. We conduct an empirical evaluation to measure the accuracy of this translation task 
    
[^53]: 中国 offensive 语言检测：现状与未来方向

    Chinese Offensive Language Detection:Current Status and Future Directions

    [https://arxiv.org/abs/2403.18314](https://arxiv.org/abs/2403.18314)

    总体而言，这篇论文讨论了在中文中检测 offensive 语言的挑战，并强调了开发解决这一问题的特定模型和工具。

    

    虽然社交媒体平台正在做出相当大的努力监测和规范用户生成内容，但在数字空间中，恶意语言（如仇恨言论或网络欺凌）的普遍存在仍然是一个重要挑战。鉴于维护文明和尊重的在线环境的重要性，迫切需要能够实时检测恶意言论的自动系统。然而，为了开发处理汉语等语言的有效系统，面临着重大挑战，因为这些语言的复杂和微妙性使得自动处理变得困难。本文全面总结了中国 offensive 语言检测情况，审查了当前的基准和方法，并重点介绍了用于解决在这种复杂语言中检测恶意语言的独特挑战的特定模型和工具。

    arXiv:2403.18314v1 Announce Type: cross  Abstract: Despite the considerable efforts being made to monitor and regulate user-generated content on social media platforms, the pervasiveness of offensive language, such as hate speech or cyberbullying, in the digital space remains a significant challenge. Given the importance of maintaining a civilized and respectful online environment, there is an urgent and growing need for automatic systems capable of detecting offensive speech in real time. However, developing effective systems for processing languages such as Chinese presents a significant challenge, owing to the language's complex and nuanced nature, which makes it difficult to process automatically. This paper provides a comprehensive overview of offensive language detection in Chinese, examining current benchmarks and approaches and highlighting specific models and tools for addressing the unique challenges of detecting offensive language in this complex language. The primary object
    
[^54]: 一种热力学一致的基于物理信息深度学习材料模型，用于短纤维/聚合物纳米复合材料

    A thermodynamically consistent physics-informed deep learning material model for short fiber/polymer nanocomposites

    [https://arxiv.org/abs/2403.18310](https://arxiv.org/abs/2403.18310)

    提出了一种基于物理信息深度学习的热力学一致材料模型，用于研究短纤维/聚合物纳米复合材料的行为，通过组合深度学习网络预测内部变量并定义整个系统的热力学状态。

    

    这项工作提出了一种基于物理信息深度学习（PIDL）的本构模型，用于研究在各种环境条件下短纤维增强纳米粒子填充环氧树脂的粘弹-粘塑行为。深度学习模型经过训练以强制执行热力学原理，从而得到热力学一致的本构模型。为实现此目标，将长短期记忆网络与前馈神经网络相结合，以预测表征纳米复合材料内部耗散所需的内部变量。此外，另一个前馈神经网络用于指示自由能函数，从而定义整个系统的热力学状态。PIDL模型最初针对三维情况进行开发，通过从经典本构模型中生成合成数据来训练模型，然后通过直接提取循环加载数据进行训练。

    arXiv:2403.18310v1 Announce Type: cross  Abstract: This work proposes a physics-informed deep learning (PIDL)-based constitutive model for investigating the viscoelastic-viscoplastic behavior of short fiber-reinforced nanoparticle-filled epoxies under various ambient conditions. The deep-learning model is trained to enforce thermodynamic principles, leading to a thermodynamically consistent constitutive model. To accomplish this, a long short-term memory network is combined with a feed-forward neural network to predict internal variables required for characterizing the internal dissipation of the nanocomposite materials. In addition, another feed-forward neural network is used to indicate the free-energy function, which enables defining the thermodynamic state of the entire system. The PIDL model is initially developed for the three-dimensional case by generating synthetic data from a classical constitutive model. The model is then trained by extracting the data directly from cyclic lo
    
[^55]: 一种具有项目特征的NFT可收藏品推荐系统

    A Recommender System for NFT Collectibles with Item Feature

    [https://arxiv.org/abs/2403.18305](https://arxiv.org/abs/2403.18305)

    该研究提出了一种针对NFT的推荐系统，综合利用NFT交易记录和外部项目特征等多种数据源，通过数据高效的基于图的方法生成个性化推荐，并利用超出用户-项目互动的输入验证了模型的有效性。

    

    推荐系统已被积极研究并应用于各个领域以解决信息过载问题。尽管有许多关于电影、音乐和电子商务的推荐系统的研究，但相比之下，尽管NFT市场持续增长，对于NFT的推荐系统却受到了相对较少的关注。本文提出了一种针对NFT的推荐系统，利用各种数据源，从NFT交易记录到外部项目特征，生成符合个人偏好的精确推荐。我们开发了一种数据高效的基于图的推荐系统，以有效捕捉每个项目与用户之间的复杂关系，并生成包含节点特征信息和图结构的节点（项目）嵌入。此外，我们利用超出用户-项目互动的输入，如图像特征、文本特征和价格特征。数值实验证实了该模型的性能。

    arXiv:2403.18305v1 Announce Type: cross  Abstract: Recommender systems have been actively studied and applied in various domains to deal with information overload. Although there are numerous studies on recommender systems for movies, music, and e-commerce, comparatively less attention has been paid to the recommender system for NFTs despite the continuous growth of the NFT market. This paper presents a recommender system for NFTs that utilizes a variety of data sources, from NFT transaction records to external item features, to generate precise recommendations that cater to individual preferences. We develop a data-efficient graph-based recommender system to efficiently capture the complex relationship between each item and users and generate node(item) embeddings which incorporate both node feature information and graph structure. Furthermore, we exploit inputs beyond user-item interactions, such as image feature, text feature, and price feature. Numerical experiments verify the perf
    
[^56]: 选择性混合微调以优化非可分解目标

    Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives

    [https://arxiv.org/abs/2403.18301](https://arxiv.org/abs/2403.18301)

    提出了SelMix，一种选择性混合的廉价微调技术，用于优化预训练模型以实现所需的非可分解目标。

    

    互联网使用的增加导致了大量数据的生成，从而采用了各种监督和半监督机器学习算法，这些算法可以有效利用大量数据来训练模型。然而，在将这些模型部署到现实世界之前，必须严格评估它们在诸如最坏情况召回率之类的性能指标上的表现，并满足公平性等约束条件。我们发现，当前最先进的经验技术在这些实际的、非可分解的性能目标上提供了次优性能。另一方面，理论技术需要为每个性能目标从头开始训练一个新模型。为了弥合这一差距，我们提出了SelMix，这是一种基于选择性混合的廉价微调技术，用于针对所需目标进行优化。

    arXiv:2403.18301v1 Announce Type: cross  Abstract: The rise in internet usage has led to the generation of massive amounts of data, resulting in the adoption of various supervised and semi-supervised machine learning algorithms, which can effectively utilize the colossal amount of data to train models. However, before deploying these models in the real world, these must be strictly evaluated on performance measures like worst-case recall and satisfy constraints such as fairness. We find that current state-of-the-art empirical techniques offer sub-optimal performance on these practical, non-decomposable performance objectives. On the other hand, the theoretical techniques necessitate training a new model from scratch for each performance objective. To bridge the gap, we propose SelMix, a selective mixup-based inexpensive fine-tuning technique for pre-trained models, to optimize for the desired objective. The core idea of our framework is to determine a sampling distribution to perform a
    
[^57]: GeNet:一种基于图神经网络的抗噪声任务导向语义通信范式

    GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm

    [https://arxiv.org/abs/2403.18296](https://arxiv.org/abs/2403.18296)

    GeNet提出了一种基于图神经网络的语义通信范式，通过将数据转换为图结构、利用编码器提取语义信息并利用解码器重建信息的方法来实现抗噪声任务导向通信。

    

    传统的语义通信任务方法依赖于了解信噪比（SNR）来减轻通道噪声。然而，这些方法需要在特定的SNR条件下进行训练，需要大量时间和计算资源。在本文中，我们提出了GeNet，这是一种基于图神经网络（GNN）的语义通信范式，旨在抵抗噪声，从而促进任务导向通信（TOC）。我们提出了一种新颖的方法，首先将输入数据图像转换为图结构。然后利用基于GNN的编码器从源数据中提取语义信息。这些提取的语义信息然后通过通道传输。在接收端，使用基于GNN的解码器从源数据中重建相关的语义信息以用于TOC。通过实验评估，我们展示了GeNet在抗噪声TOC中的有效性。

    arXiv:2403.18296v1 Announce Type: cross  Abstract: Traditional approaches to semantic communication tasks rely on the knowledge of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these methods necessitate training under specific SNR conditions, entailing considerable time and computational resources. In this paper, we propose GeNet, a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at combating noise, thereby facilitating Task-Oriented Communication (TOC). We propose a novel approach where we first transform the input data image into graph structures. Then we leverage a GNN-based encoder to extract semantic information from the source data. This extracted semantic information is then transmitted through the channel. At the receiver's end, a GNN-based decoder is utilized to reconstruct the relevant semantic information from the source data for TOC. Through experimental evaluation, we show GeNet's effectiveness in anti-noise TOC while decoup
    
[^58]: 少样本重校准语言模型

    Few-Shot Recalibration of Language Models

    [https://arxiv.org/abs/2403.18286](https://arxiv.org/abs/2403.18286)

    提出了一种新框架，用于少样本特定切片重校准语言模型，实现在任意分布片段上获得校准的置信度估计。

    

    最近的研究发现了一些有希望的方法，可以从语言模型（LMs）中提取出校准良好的置信度估计，其中模型的置信度分数反映了其正确性可能性。然而，虽然LMs在广泛分布上可能具有良好的校准性，但这往往隐藏在更窄分片内存在显著的校准不准确性（例如，在数学中存在系统性过度自信可能会平衡历史中的系统性不足自信，从而在总体上实现完美校准）。为了获得任何分布片段的校准良好的置信度估计，我们提出了一种用于少样本特定切片重校准的新框架。具体来说，我们训练一个重新校准模型，该模型接受来自任何给定切片的少量无标签示例，并预测一条重新映射置信度分数以使其对该切片更准确的曲线。我们训练的模型可以重新校准任意新的切片，而无需使用该切片的任何标记数据。这使我们能够识别d

    arXiv:2403.18286v1 Announce Type: cross  Abstract: Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify d
    
[^59]: 通过模式挖掘识别和使用深度学习骨干

    Identification and Uses of Deep Learning Backbones via Pattern Mining

    [https://arxiv.org/abs/2403.18278](https://arxiv.org/abs/2403.18278)

    通过模式挖掘识别和使用深度学习骨干，研究了深度学习如何进行预测的核心机制，提出了集合覆盖风格问题和相关的启发式方法，并表明其收敛到ILP公式的帕累托均衡点。

    

    arXiv:2403.18278v1 公告类型: 新摘要: 深度学习作为一种黑盒方法在许多数据挖掘领域被广泛使用，并取得了令人印象深刻的结果。然而，理解深度学习如何进行预测的核心机制是一个相对少被研究的问题。本文探讨了为给定组实例识别深度学习的骨干的概念。这里的“组”可以是同一类别的实例，甚至是同一类别的误分类实例。我们将给定组的每个实例视为激活一组神经元，并尝试找到与给定概念/组相关联的神经元子图。我们将这个问题定义为集合覆盖风格问题，并展示了它是不可解的，并提出了高度受限的整数线性规划（ILP）公式。作为替代，我们探讨了一种基于覆盖率的启发式方法，与模式挖掘相关，并展示了它收敛到ILP公式的帕累托均衡点。在实验中，我们探索了这些背景

    arXiv:2403.18278v1 Announce Type: new  Abstract: Deep learning is extensively used in many areas of data mining as a black-box method with impressive results. However, understanding the core mechanism of how deep learning makes predictions is a relatively understudied problem. Here we explore the notion of identifying a backbone of deep learning for a given group of instances. A group here can be instances of the same class or even misclassified instances of the same class. We view each instance for a given group as activating a subset of neurons and attempt to find a subgraph of neurons associated with a given concept/group. We formulate this problem as a set cover style problem and show it is intractable and presents a highly constrained integer linear programming (ILP) formulation. As an alternative, we explore a coverage-based heuristic approach related to pattern mining, and show it converges to a Pareto equilibrium point of the ILP formulation. Experimentally we explore these bac
    
[^60]: DSF-GAN: 下游反馈生成对抗网络

    DSF-GAN: DownStream Feedback Generative Adversarial Network

    [https://arxiv.org/abs/2403.18267](https://arxiv.org/abs/2403.18267)

    DSF-GAN提出了一种新架构，通过在训练中结合下游预测模型的反馈信息，增强了生成器的损失函数，从而提高了合成样本的实用性。

    

    实用性和隐私性是衡量合成表格数据质量的两个关键指标。尽管在隐私措施方面已经取得了显著进展，但生成具有高实用性的合成样本仍然具有挑战性。为了提高合成样本的实用性，我们提出了一种名为DownStream Feedback生成对抗网络（DSF-GAN）的新架构。该方法在训练过程中结合了下游预测模型的反馈，用有价值的信息增强生成器的损失函数。因此，DSF-GAN利用下游预测任务来增强合成样本的实用性。为了评估我们的方法，我们使用了两个流行的数据集进行测试。我们的实验表明，在使用DSF-GAN生成的合成样本进行训练时，模型性能得到了改善，相比于没有反馈的相同GAN架构生成的样本。评估是在同一个验证集上进行的，其中包含了re

    arXiv:2403.18267v1 Announce Type: cross  Abstract: Utility and privacy are two crucial measurements of the quality of synthetic tabular data. While significant advancements have been made in privacy measures, generating synthetic samples with high utility remains challenging. To enhance the utility of synthetic samples, we propose a novel architecture called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This approach incorporates feedback from a downstream prediction model during training to augment the generator's loss function with valuable information. Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of synthetic samples. To evaluate our method, we tested it using two popular datasets. Our experiments demonstrate improved model performance when training on synthetic samples generated by DSF-GAN, compared to those generated by the same GAN architecture without feedback. The evaluation was conducted on the same validation set comprising re
    
[^61]: 用遗忘机制方法提升生成式类增量学习性能

    Enhancing Generative Class Incremental Learning Performance with Model Forgetting Approach

    [https://arxiv.org/abs/2403.18258](https://arxiv.org/abs/2403.18258)

    本研究通过引入遗忘机制提升生成式类增量学习性能。

    

    这项研究介绍了一种新颖的方法，通过引入遗忘机制来增强生成式类增量学习（GCIL），旨在动态管理类信息，以更好地适应数据流。 GCIL 是计算机视觉领域的热门话题之一，被认为是社会中至关重要的任务之一，特别是生成模型的持续学习。 遗忘是一种关键的大脑功能，通过选择性地丢弃对人类不太相关的信息，有助于持续学习。 然而，在机器学习模型领域，故意忘记的概念尚未得到广泛研究。 在本研究中，我们旨在通过将遗忘机制纳入GCIL中来弥合这一差距，从而检验它们对模型在持续学习中学习能力的影响。 通过我们的实验，我们发现整合遗忘机制显着

    arXiv:2403.18258v1 Announce Type: cross  Abstract: This study presents a novel approach to Generative Class Incremental Learning (GCIL) by introducing the forgetting mechanism, aimed at dynamically managing class information for better adaptation to streaming data. GCIL is one of the hot topics in the field of computer vision, and this is considered one of the crucial tasks in society, specifically the continual learning of generative models. The ability to forget is a crucial brain function that facilitates continual learning by selectively discarding less relevant information for humans. However, in the field of machine learning models, the concept of intentionally forgetting has not been extensively investigated. In this study we aim to bridge this gap by incorporating the forgetting mechanisms into GCIL, thereby examining their impact on the models' ability to learn in continual learning. Through our experiments, we have found that integrating the forgetting mechanisms significantl
    
[^62]: 通过轻微扰动操纵神经路径规划器

    Manipulating Neural Path Planners via Slight Perturbations

    [https://arxiv.org/abs/2403.18256](https://arxiv.org/abs/2403.18256)

    在这篇论文中，我们提出了一种新颖的方法，可以通过轻微扰动来指定和注入各种隐藏的恶意行为，即后门，到神经路径规划器中。

    

    数据驱动的神经路径规划器在机器人领域越来越受到关注。然而，它们的神经网络部件通常作为黑匣子呈现，掩盖了其基础决策过程。它们的黑匣子性质使它们面临被通过插入隐藏恶意行为来篡改的风险。本文提出了一种新颖的方法，用于指定和注入各种隐藏恶意行为，称为后门，到神经路径规划器中。我们的方法提供了一种简洁但灵活的定义这些行为的方式，我们展示了隐藏行为可以通过轻微扰动（例如，插入微小的不明显的扰动）来触发。

    arXiv:2403.18256v1 Announce Type: cross  Abstract: Data-driven neural path planners are attracting increasing interest in the robotics community. However, their neural network components typically come as black boxes, obscuring their underlying decision-making processes. Their black-box nature exposes them to the risk of being compromised via the insertion of hidden malicious behaviors. For example, an attacker may hide behaviors that, when triggered, hijack a delivery robot by guiding it to a specific (albeit wrong) destination, trapping it in a predefined region, or inducing unnecessary energy expenditure by causing the robot to repeatedly circle a region. In this paper, we propose a novel approach to specify and inject a range of hidden malicious behaviors, known as backdoors, into neural path planners. Our approach provides a concise but flexible way to define these behaviors, and we show that hidden behaviors can be triggered by slight perturbations (e.g., inserting a tiny unnotic
    
[^63]: 超越嵌入：多模态模型中视觉表格的价值

    Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models

    [https://arxiv.org/abs/2403.18252](https://arxiv.org/abs/2403.18252)

    提出了Visual Table，一种为MLLMs量身定制的新型视觉表示，通过提供层次化文本描述的全面视觉场景来弥补现有视觉表示的不足。

    

    视觉表示学习一直是计算机视觉中的基石，从具有人类注释标签的监督学习发展到对齐来自互联网的图像-文本对。尽管多模态大语言模型（MLLMs）方面取得了近期的进展，但它们依赖的视觉表示（如CLIP嵌入）通常缺乏关键的外部世界知识，这对于现实世界的视觉推理至关重要。在这项工作中，我们提出了Visual Table，这是为MLLMs量身定制的新型视觉表示。它提供全面视觉场景的层次化文本描述，包括场景描述和涵盖类别、属性和实例级别知识的多个以对象为中心的描述。我们进一步开发了一个可扩展的生成器，用于从GPT4V的小规模注释中生成视觉表格，并训练它。广泛的评估表明，通过将生成的视觉表格作为额外视觉表示，我们

    arXiv:2403.18252v1 Announce Type: cross  Abstract: Visual representation learning has been a cornerstone in computer vision, evolving from supervised learning with human-annotated labels to aligning image-text pairs from the Internet. Despite recent advancements in multi-modal large language models (MLLMs), the visual representations they rely on, such as CLIP embeddings, often lack access to external world knowledge critical for real-world visual reasoning. In this work, we propose Visual Table, a novel visual representation tailored for MLLMs. It provides hierarchical text descriptions of holistic visual scenes, consisting of a scene description and multiple object-centric descriptions that encompass categories, attributes, and knowledge at instance level. We further develop a scalable generator for visual table generation and train it on small-scale annotations from GPT4V. Extensive evaluations demonstrate that, with generated visual tables as additional visual representations, our 
    
[^64]: 用细粒度检索增强和自检提升对话式问答

    Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check

    [https://arxiv.org/abs/2403.18243](https://arxiv.org/abs/2403.18243)

    该论文提出了一种面向对话式问答的会话级RAG方法，通过细粒度检索增强和自检，实现了问句理解和相关信息获取，相较于现有方法具有显著优势。

    

    检索增强生成（RAG）旨在通过向大型语言模型（LLMs）增加外部庞大而动态的知识，生成更可靠和准确的响应。本文提出了一种面向对话式问答（CQA）的会话级RAG方法，其中包括细粒度检索增强和自检。我们的方法包括对话式问题细化器、细粒度检索器和基于自检的响应生成器三个组件，它们在对话情境中协作工作，用于问句理解和相关信息获取。广泛实验证明，我们的方法在对话式问答中具有巨大优势。

    arXiv:2403.18243v1 Announce Type: new  Abstract: Retrieval-Augmented Generation (RAG) aims to generate more reliable and accurate responses, by augmenting large language models (LLMs) with the external vast and dynamic knowledge. Most previous work focuses on using RAG for single-round question answering, while how to adapt RAG to the complex conversational setting wherein the question is interdependent on the preceding context is not well studied. In this paper, we propose a conversation-level RAG approach, which incorporates fine-grained retrieval augmentation and self-check for conversational question answering (CQA). In particular, our approach consists of three components, namely conversational question refiner, fine-grained retriever and self-check based response generator, which work collaboratively for question understanding and relevant information acquisition in conversational settings. Extensive experiments demonstrate the great advantages of our approach over the state-of-t
    
[^65]: NeuSDFusion: 一种空间感知的生成模型，用于3D形状的完成、重建和生成

    NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation

    [https://arxiv.org/abs/2403.18241](https://arxiv.org/abs/2403.18241)

    提出一种新颖的空间感知3D形状生成框架，利用2D平面表示增强建模，并结合混合形状表示技术直接学习连续有向距离场表示，从而确保空间一致性和降低内存使用。

    

    3D形状生成旨在生成符合特定条件和约束的创新性3D内容。现有方法通常将3D形状分解为一系列局部组件，将每个元素孤立处理而不考虑空间一致性。因此，这些方法在3D数据表示和形状生成方面表现出有限的多样性，阻碍了它们生成高度多样化且符合指定约束的3D形状的能力。为此，我们引入了一种新颖的空间感知3D形状生成框架，利用2D平面表示来增强3D形状建模。为确保空间一致性并减少内存使用，我们结合了一种混合形状表示技术，直接使用正交的2D平面学习3D形状的连续有向距离场表示。此外，我们通过传

    arXiv:2403.18241v1 Announce Type: cross  Abstract: 3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling. To ensure spatial coherence and reduce memory usage, we incorporate a hybrid shape representation technique that directly learns a continuous signed distance field representation of the 3D shape using orthogonal 2D planes. Additionally, we meticulously enforce spatial correspondences across distinct planes using a tra
    
[^66]: 大型语言模型需要研究员进行推理：通过行为模拟在复杂人类系统中成为专家

    Large Language Models Need Consultants for Reasoning: Becoming an Expert in a Complex Human System Through Behavior Simulation

    [https://arxiv.org/abs/2403.18230](https://arxiv.org/abs/2403.18230)

    通过行为模拟，大型语言模型结合推理强化方法，在复杂人类系统中展现出与人类相媲美的推理能力。

    

    大型语言模型（LLMs）结合各种推理强化方法，在数学、法律、编码、常识和世界知识等领域展示出与人类相媲美的显著能力。本文探讨了LLMs在复杂人类系统中的推理能力。我们提出了一个新颖的推理框架，称为“马赛克专家观察墙”（MEOW），利用生成式智能体模拟技术。在MEOW框架中，利用模拟数据训练专家模型，集中于每个独立模拟时间内关于特定任务的“经验”。正是通过模拟累积的“经验”使之成为复杂人类系统中任务专家。我们在一个反映真实安全场景的通信游戏中进行了实验。结果表明我们提出的方法可以与现有方法相辅相成。

    arXiv:2403.18230v1 Announce Type: new  Abstract: Large language models (LLMs), in conjunction with various reasoning reinforcement methodologies, have demonstrated remarkable capabilities comparable to humans in fields such as mathematics, law, coding, common sense, and world knowledge. In this paper, we delve into the reasoning abilities of LLMs within complex human systems. We propose a novel reasoning framework, termed ``Mosaic Expert Observation Wall'' (MEOW) exploiting generative-agents-based simulation technique. In the MEOW framework, simulated data are utilized to train an expert model concentrating ``experience'' about a specific task in each independent time of simulation. It is the accumulated ``experience'' through the simulation that makes for an expert on a task in a complex human system. We conduct the experiments within a communication game that mirrors real-world security scenarios. The results indicate that our proposed methodology can cooperate with existing methodol
    
[^67]: 基于Transformer的有效载荷恶意软件检测和分类框架

    A Transformer-Based Framework for Payload Malware Detection and Classification

    [https://arxiv.org/abs/2403.18223](https://arxiv.org/abs/2403.18223)

    本文提出了一种基于Transformer的DPI算法，旨在检测恶意流量，通过学习复杂的序列数据内容并推广到类似场景中。

    

    随着恶意网络威胁在入侵计算机网络方面变得更加复杂，有效的入侵检测系统（IDSs）的需求变得至关重要。传统上，IDSs依赖于异常检测和基于特征库的检测技术来检测未识别和可疑活动。深度学习技术在DPI方面展现出了巨大潜力，因为它们能够从通过网络传输的数据包内容中学习复杂的模式。在本文中，我们提出了一种革命性的基于Transformer的DPI算法，旨在通过带有分类器头的转变器检测恶意流量。

    arXiv:2403.18223v1 Announce Type: cross  Abstract: As malicious cyber threats become more sophisticated in breaching computer networks, the need for effective intrusion detection systems (IDSs) becomes crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced to allow IDSs analyze the content of network packets, providing more context for identifying potential threats. IDSs traditionally rely on using anomaly-based and signature-based detection techniques to detect unrecognized and suspicious activity. Deep learning techniques have shown great potential in DPI for IDSs due to their efficiency in learning intricate patterns from the packet content being transmitted through the network. In this paper, we propose a revolutionary DPI algorithm based on transformers adapted for the purpose of detecting malicious traffic with a classifier head. Transformers learn the complex content of sequence data and generalize them well to similar scenarios thanks to their self-attent
    
[^68]: 从二维到三维环境的Q学习：使用强化学习建模自主导航而无需库

    From Two-Dimensional to Three-Dimensional Environment with Q-Learning: Modeling Autonomous Navigation with Reinforcement Learning and no Libraries

    [https://arxiv.org/abs/2403.18219](https://arxiv.org/abs/2403.18219)

    本研究通过强化学习算法在二维和三维环境中的表现，探讨了在没有预制库的情况下通过计算数学开发算法的可行性。

    

    强化学习（RL）算法已成为人工智能中不可或缺的工具，使代理通过与环境和反馈机制的交互获得最优决策策略。本研究探讨了RL代理在二维（2D）和三维（3D）环境中的表现，旨在研究跨不同空间维度学习的动态。这项研究的一个关键方面是没有用于学习的预制库，算法完全通过计算数学开发。方法论框架集中在RL原则上，采用Q学习代理类和针对每个空间维度量身定制的不同环境类。研究旨在探讨一个问题：强化学习代理在不同空间维度的环境中如何适应和表现，特别是在2D和3D设置中？通过实证研究

    arXiv:2403.18219v1 Announce Type: cross  Abstract: Reinforcement learning (RL) algorithms have become indispensable tools in artificial intelligence, empowering agents to acquire optimal decision-making policies through interactions with their environment and feedback mechanisms. This study explores the performance of RL agents in both two-dimensional (2D) and three-dimensional (3D) environments, aiming to research the dynamics of learning across different spatial dimensions. A key aspect of this investigation is the absence of pre-made libraries for learning, with the algorithm developed exclusively through computational mathematics. The methodological framework centers on RL principles, employing a Q-learning agent class and distinct environment classes tailored to each spatial dimension. The research aims to address the question: How do reinforcement learning agents adapt and perform in environments of varying spatial dimensions, particularly in 2D and 3D settings? Through empirical
    
[^69]: 利用大型语言模型进行政治科学中模糊字符串匹配

    Leveraging Large Language Models for Fuzzy String Matching in Political Science

    [https://arxiv.org/abs/2403.18218](https://arxiv.org/abs/2403.18218)

    提出利用大型语言模型来解决政治科学中模糊字符串匹配问题，可显著提高平均精度达39%，更易使用且具有鲁棒性

    

    模糊字符串匹配在政治科学家从不同来源组合数据时仍然是一个关键问题。现有的匹配方法通常依赖于字符串距离，如Levenshtein距离和余弦相似度。因此，它们无法匹配那些用不同名称指代同一实体的字符串，比如“JP Morgan”和“Chase Bank”，“DPRK”和“北韩”，“Chuck Fleischmann（R）”和“Charles Fleischmann（R）”。在这封信中，我们提议使用大型语言模型完全规避这个问题，以一种简单直观的方式。大量实验证明，我们提出的方法可以在平均精度方面将业界水平提高多达39%，同时政治科学家使用起来更加简单直观。此外，我们的结果对各种温度变化具有鲁棒性。我们进一步注意到，更好的提示可以带来额外的性能提升。

    arXiv:2403.18218v1 Announce Type: new  Abstract: Fuzzy string matching remains a key issue when political scientists combine data from different sources. Existing matching methods invariably rely on string distances, such as Levenshtein distance and cosine similarity. As such, they are inherently incapable of matching strings that refer to the same entity with different names such as ''JP Morgan'' and ''Chase Bank'', ''DPRK'' and ''North Korea'', ''Chuck Fleischmann (R)'' and ''Charles Fleischmann (R)''. In this letter, we propose to use large language models to entirely sidestep this problem in an easy and intuitive manner. Extensive experiments show that our proposed methods can improve the state of the art by as much as 39% in terms of average precision while being substantially easier and more intuitive to use by political scientists. Moreover, our results are robust against various temperatures. We further note that enhanced prompting can lead to additional performance improvement
    
[^70]: 在随机环境中基于偏序时序目标的首选规划

    Preference-Based Planning in Stochastic Environments: From Partially-Ordered Temporal Goals to Most Preferred Policies

    [https://arxiv.org/abs/2403.18212](https://arxiv.org/abs/2403.18212)

    使用偏序时序目标，将部分有序偏好映射到MDP策略偏好，并通过引入序理论实现最优策略的合成。

    

    人类偏好并非总是通过完全的线性顺序来表示：使用部分有序偏好来表达不可比较的结果是自然的。在这项工作中，我们考虑在随机系统中做决策和概率规划，这些系统被建模为马尔可夫决策过程（MDPs），给定一组有序偏好的时间延伸目标。具体而言，每个时间延伸目标都是使用线性时序逻辑有限轨迹（LTL$_f$）中的公式来表示的。为了根据部分有序偏好进行规划，我们引入了序理论来将对时间目标的偏好映射到对MDP策略的偏好。因此，在随机顺序下的一个最优选策略将导致MDP中有限路径上的一个随机非支配概率分布。为了合成一个最优选策略，我们的技术方法包括两个关键步骤。在第一步中，我们开发了一个程序...

    arXiv:2403.18212v1 Announce Type: cross  Abstract: Human preferences are not always represented via complete linear orders: It is natural to employ partially-ordered preferences for expressing incomparable outcomes. In this work, we consider decision-making and probabilistic planning in stochastic systems modeled as Markov decision processes (MDPs), given a partially ordered preference over a set of temporally extended goals. Specifically, each temporally extended goal is expressed using a formula in Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially ordered preference, we introduce order theory to map a preference over temporal goals to a preference over policies for the MDP. Accordingly, a most preferred policy under a stochastic ordering induces a stochastic nondominated probability distribution over the finite paths in the MDP. To synthesize a most preferred policy, our technical approach includes two key steps. In the first step, we develop a procedure to
    
[^71]: 长短期约束驱动的安全强化学习用于自动驾驶

    Long and Short-Term Constraints Driven Safe Reinforcement Learning for Autonomous Driving

    [https://arxiv.org/abs/2403.18209](https://arxiv.org/abs/2403.18209)

    本文提出了一种基于长期和短期约束的新算法用于安全强化学习，在自动驾驶任务中可以同时保证车辆的短期和长期安全性。

    

    强化学习（RL）在决策任务中被广泛使用，但由于需要与环境交互，无法保证代理的安全性，这严重限制了其在自动驾驶等工业应用中的应用。本文提出了一种基于长期和短期约束（LSTC）的新算法用于安全RL。短期约束旨在确保车辆探测到的短期状态安全，而长期约束则确保整体安全性。

    arXiv:2403.18209v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has been widely used in decision-making tasks, but it cannot guarantee the agent's safety in the training process due to the requirements of interaction with the environment, which seriously limits its industrial applications such as autonomous driving. Safe RL methods are developed to handle this issue by constraining the expected safety violation costs as a training objective, but they still permit unsafe state occurrence, which is unacceptable in autonomous driving tasks. Moreover, these methods are difficult to achieve a balance between the cost and return expectations, which leads to learning performance degradation for the algorithms. In this paper, we propose a novel algorithm based on the long and short-term constraints (LSTC) for safe RL. The short-term constraint aims to guarantee the short-term state safety that the vehicle explores, while the long-term constraint ensures the overall safety of the
    
[^72]: 一种具有自适应多模态融合的演化网络架构搜索框架用于手势识别

    An Evolutionary Network Architecture Search Framework with Adaptive Multimodal Fusion for Hand Gesture Recognition

    [https://arxiv.org/abs/2403.18208](https://arxiv.org/abs/2403.18208)

    提出了一种演化网络架构搜索框架，具有自适应多模态融合，可以自动构建不同架构的多模态网络，并考虑了来自不同输入流的数据。

    

    基于多模态数据的手势识别(HGR)因其在应用中的巨大潜力而引起了广泛关注。各种手动设计的多模态深度网络在多模态HGR（MHGR）中表现良好，但大多数现有算法需要大量专家经验和耗时的手动试验。为了解决这些问题，我们提出了一个具有自适应多模态融合的演化网络架构搜索框架(AMF-ENAS)。具体地，我们设计了一个编码空间，同时考虑了多模态数据的融合位置和比例，允许通过解码自动构建具有不同架构的多模态网络。此外，我们考虑了对应于单模态表面肌电图(sEMG)、单模态加速度计(ACC)和跨模态(sEMG-ACC)的三个输入流。为了自动适应各种数据集，ENAS框架被设计为

    arXiv:2403.18208v1 Announce Type: cross  Abstract: Hand gesture recognition (HGR) based on multimodal data has attracted considerable attention owing to its great potential in applications. Various manually designed multimodal deep networks have performed well in multimodal HGR (MHGR), but most of existing algorithms require a lot of expert experience and time-consuming manual trials. To address these issues, we propose an evolutionary network architecture search framework with the adaptive multimodel fusion (AMF-ENAS). Specifically, we design an encoding space that simultaneously considers fusion positions and ratios of the multimodal data, allowing for the automatic construction of multimodal networks with different architectures through decoding. Additionally, we consider three input streams corresponding to intra-modal surface electromyography (sEMG), intra-modal accelerometer (ACC), and inter-modal sEMG-ACC. To automatically adapt to various datasets, the ENAS framework is designe
    
[^73]: 探究中国大型语言模型的隐私保护能力

    Exploring the Privacy Protection Capabilities of Chinese Large Language Models

    [https://arxiv.org/abs/2403.18205](https://arxiv.org/abs/2403.18205)

    设计了一个三层渐进式框架来评估语言系统中的隐私，全面评估大型语言模型对私人信息的敏感性以及其防范隐私侵犯的有效性

    

    大型语言模型（LLMs）以其在各种任务中出色表现而闻名，在推动人工智能方面取得了重大进展。然而，这些进步引发了人们对隐私和安全影响日益增长的关注。为了解决这些问题并解释这些模型固有风险，我们设计了一个分为三层的渐进式框架，专门用于评估语言系统中的隐私。该框架在每个层次都包含逐渐复杂和深入的隐私测试任务。我们的主要目标是全面评估大型语言模型对私人信息的敏感性，考察它们在不同情境下如何有效辨别、管理和保护敏感数据。这种系统化评估有助于我们了解这些模型遵守隐私保护准则的程度，以及其固有的防范隐私侵犯措施的有效性。我们的观察表明，

    arXiv:2403.18205v1 Announce Type: new  Abstract: Large language models (LLMs), renowned for their impressive capabilities in various tasks, have significantly advanced artificial intelligence. Yet, these advancements have raised growing concerns about privacy and security implications. To address these issues and explain the risks inherent in these models, we have devised a three-tiered progressive framework tailored for evaluating privacy in language systems. This framework consists of progressively complex and in-depth privacy test tasks at each tier. Our primary objective is to comprehensively evaluate the sensitivity of large language models to private information, examining how effectively they discern, manage, and safeguard sensitive data in diverse scenarios. This systematic evaluation helps us understand the degree to which these models comply with privacy protection guidelines and the effectiveness of their inherent safeguards against privacy breaches. Our observations indicat
    
[^74]: EndToEndML: 一个面向机器学习应用的开源端到端管道

    EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning Applications

    [https://arxiv.org/abs/2403.18203](https://arxiv.org/abs/2403.18203)

    提出了一个面向生命科学的开源端到端机器学习管道，为生物信息学社区提供了一个无需编程技能即可分析复杂生物数据的用户友好界面。

    

    arXiv:2403.18203v1 公告类型: 新的 摘要: 人工智能 (AI) 技术广泛应用于生命科学。然而，将创新的 AI 技术应用于理解和揭示生物复杂性受到生命科学科学家理解和使用计算语言的学习曲线的阻碍。一个面向 AI 模型的开源、用户友好的界面，不需要编程技能来分析复杂的生物数据，对生物信息学社区将非常有价值。随着对不同测序技术的易于存取以及对不同 '组学' 研究的增加兴趣，生成的生物数据集数量增加，并且分析这些高通量数据集要求计算能力。今天的大多数 AI 库都需要高级编程技能以及机器学习、数据预处理和可视化技能。在这项研究中，我们提出了一个基于 web 的端到端管道，能够进行预处理。

    arXiv:2403.18203v1 Announce Type: new  Abstract: Artificial intelligence (AI) techniques are widely applied in the life sciences. However, applying innovative AI techniques to understand and deconvolute biological complexity is hindered by the learning curve for life science scientists to understand and use computing languages. An open-source, user-friendly interface for AI models, that does not require programming skills to analyze complex biological data will be extremely valuable to the bioinformatics community. With easy access to different sequencing technologies and increased interest in different 'omics' studies, the number of biological datasets being generated has increased and analyzing these high-throughput datasets is computationally demanding. The majority of AI libraries today require advanced programming skills as well as machine learning, data preprocessing, and visualization skills. In this research, we propose a web-based end-to-end pipeline that is capable of preproc
    
[^75]: 超越表面所见：基于种族健康不平等社会决定因素的多标签胸部X射线分类的子组交叉公平的实证分析

    Looking Beyond What You See: An Empirical Analysis on Subgroup Intersectional Fairness for Multi-label Chest X-ray Classification Using Social Determinants of Racial Health Inequities

    [https://arxiv.org/abs/2403.18196](https://arxiv.org/abs/2403.18196)

    通过考虑社会决定因素中的复杂相互作用，提出了一种简单而强大的方法，实现跨受保护群体的准确诊断结果和公平性，为高维胸部X射线多标签分类带来创新。

    

    在使用胸部X射线进行疾病诊断方面，实施深度学习模型取得了显著进展。尽管取得了这些进展，但这些模型中固有的偏见可能导致跨受保护群体的预测准确性差异。在本研究中，我们提出了一个框架，以实现准确的诊断结果，并确保在高维胸部X射线多标签分类中跨交叉群体的公平性。我们不拘一格地考虑社会决定因素中的复杂相互作用，实现了更精细的公平性基准和评估。我们提出了一种简单而强大的方法，涉及使用跨组平衡数据集重新训练预先训练模型的最后分类层。此外，我们考虑了公平性约束，并为多标签设置整合了类平衡微调。我们在MIMIC-CXR数据集上评估了我们的方法，结果表明

    arXiv:2403.18196v1 Announce Type: cross  Abstract: There has been significant progress in implementing deep learning models in disease diagnosis using chest X- rays. Despite these advancements, inherent biases in these models can lead to disparities in prediction accuracy across protected groups. In this study, we propose a framework to achieve accurate diagnostic outcomes and ensure fairness across intersectional groups in high-dimensional chest X- ray multi-label classification. Transcending traditional protected attributes, we consider complex interactions within social determinants, enabling a more granular benchmark and evaluation of fairness. We present a simple and robust method that involves retraining the last classification layer of pre-trained models using a balanced dataset across groups. Additionally, we account for fairness constraints and integrate class-balanced fine-tuning for multi-label settings. The evaluation of our method on the MIMIC-CXR dataset demonstrates that
    
[^76]: 用自校正组装网络纠正LEGO组装错误

    SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly Network

    [https://arxiv.org/abs/2403.18195](https://arxiv.org/abs/2403.18195)

    介绍了单步组装错误校正任务和LEGO错误校正组装数据集（LEGO-ECA），提出了用于这一任务的自校正组装网络（SCANet）。

    

    在机器人学和3D视觉中，自主组装面临着重大挑战，尤其是确保组装正确性。主流方法如MEPNet目前专注于基于手动提供的图像进行组件组装。然而，这些方法在需要长期规划的任务中往往难以取得满意的结果。在同一时间，我们观察到整合自校正模块可以在一定程度上缓解这些问题。受此问题启发，我们引入了单步组装错误校正任务，其中涉及识别和纠正组件组装错误。为支持这一领域的研究，我们提出了LEGO错误校正组装数据集（LEGO-ECA），包括用于组装步骤和组装失败实例的手动图像。此外，我们提出了自校正组装网络（SCANet），这是一种新颖的方法来解决这一任务。SCANet将组装的部件视为查询，

    arXiv:2403.18195v1 Announce Type: cross  Abstract: Autonomous assembly in robotics and 3D vision presents significant challenges, particularly in ensuring assembly correctness. Presently, predominant methods such as MEPNet focus on assembling components based on manually provided images. However, these approaches often fall short in achieving satisfactory results for tasks requiring long-term planning. Concurrently, we observe that integrating a self-correction module can partially alleviate such issues. Motivated by this concern, we introduce the single-step assembly error correction task, which involves identifying and rectifying misassembled components. To support research in this area, we present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising manual images for assembly steps and instances of assembly failures. Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel method to address this task. SCANet treats assembled components as queries, de
    
[^77]: AI模型能否欣赏文档美学？探究可读性与版面质量与预测置信度的关系

    Can AI Models Appreciate Document Aesthetics? An Exploration of Legibility and Layout Quality in Relation to Prediction Confidence

    [https://arxiv.org/abs/2403.18183](https://arxiv.org/abs/2403.18183)

    本研究探讨了AI模型在文档理解任务中对于布局和图像数据有益的影响，以及AI是否能有效捕捉文件美学的微妙之处。

    

    一份精心设计的文件不仅通过文字传达信息，还通过视觉优雅传达信息。作者利用颜色、字体、图形和布局等美学元素来塑造信息的感知。经过心理洞察力启发的周到文件设计既增强了视觉吸引力，也增进了内容的理解。尽管最先进的文件AI模型展示了将布局和图像数据融入的好处，但文件美学的微妙之处是否被有效捕捉仍不清楚。为了弥合人类认知与AI对美学元素解释之间的差距，我们提出了假设，涉及AI在文件理解任务中的行为，特别是植根于文件设计原则。在关注可读性和版面质量的基础上，我们测试了美学效果的四个方面：噪音、字体大小对比、对齐和复杂性，以模型置信度为基准。

    arXiv:2403.18183v1 Announce Type: new  Abstract: A well-designed document communicates not only through its words but also through its visual eloquence. Authors utilize aesthetic elements such as colors, fonts, graphics, and layouts to shape the perception of information. Thoughtful document design, informed by psychological insights, enhances both the visual appeal and the comprehension of the content. While state-of-the-art document AI models demonstrate the benefits of incorporating layout and image data, it remains unclear whether the nuances of document aesthetics are effectively captured. To bridge the gap between human cognition and AI interpretation of aesthetic elements, we formulated hypotheses concerning AI behavior in document understanding tasks, specifically anchored in document design principles. With a focus on legibility and layout quality, we tested four aspects of aesthetic effects: noise, font-size contrast, alignment, and complexity, on model confidence using corre
    
[^78]: 语言模型中非事实性幻觉的机制

    Mechanisms of non-factual hallucinations in language models

    [https://arxiv.org/abs/2403.18167](https://arxiv.org/abs/2403.18167)

    研究揭示了语言模型中非事实性幻觉的两个通用机制：主题属性知识不足和未能正确选择对象属性，这有助于深入理解和减轻幻觉。

    

    现今最先进的语言模型（LMs）有时会产生与世界知识不符的非事实幻觉。尽管人们已经付出了大量努力来检测和减轻幻觉，但理解它们的内在机制仍然是困难的。 我们的研究调查了幻觉的机制原因，特别是 LM 在对主题关系查询做出回答时错误地预测对象属性的非事实形式。通过因果中介分析和嵌入空间投影，我们确认了跨不同规模和设计的 LM 中共享的两个造成幻觉的一般机制原因：1）在较低层 MLPs 中主题属性知识不足，以及2）在较高层注意力头和 MLPs 中未能选择正确的对象属性。这两个机制展示了不同程度的主宾关系、预测不确定性和扰动鲁棒性。此外，我们还审查了 LM 的预训练检查点。

    arXiv:2403.18167v1 Announce Type: cross  Abstract: State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge. Despite extensive efforts to detect and mitigate hallucinations, understanding their internal mechanisms remains elusive. Our study investigates the mechanistic causes of hallucination, specifically non-factual ones where the LM incorrectly predicts object attributes in response to subject-relation queries. With causal mediation analysis and embedding space projection, we identify two general mechanistic causes of hallucinations shared across LMs of various scales and designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and 2) failing to select the correct object attribute in upper layer attention heads and MLPs. These two mechanisms exhibit varying degrees of subject-object association, predictive uncertainty and perturbation robustness. Additionally, we scrutinize LM pre-training checkpoints, r
    
[^79]: 噢！我们冷冻：通过信号传播分析改进大型语言模型的量化知识蒸馏

    Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models

    [https://arxiv.org/abs/2403.18159](https://arxiv.org/abs/2403.18159)

    通过信号传播分析，提出了一种改进大型语言模型的量化知识蒸馏方法，并提供了ov-freeze稳定KD-QAT过程的洞察。

    

    大型生成模型，如大型语言模型（LLMs）和扩散模型分别在NLP和计算机视觉领域引起了革命。然而，它们的推理速度慢，计算和内存需求高，这使得在边缘设备上部署它们变得具有挑战性。在这项研究中，我们提出了一种轻量级的量化感知微调技术，使用知识蒸馏（KD-QAT）来改善使用常用数据集改进4位重量量化的LLMs的性能，以实现流行的语言使用案例，在设备聊天应用中。为了改进这种微调范式，作为主要贡献，我们通过经验研究训练过程中的梯度传播，提供对KD-QAT稳定性的洞察，以更好地理解基于KD-QAT的方法对低位量化误差的脆弱性。根据我们的见解，我们提出了ov-freeze，一种稳定KD-QAT过程的简单技术。最后，我们进行了实验

    arXiv:2403.18159v1 Announce Type: cross  Abstract: Large generative models, such as large language models (LLMs) and diffusion models have as revolutionized the fields of NLP and computer vision respectively. However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices. In this study, we propose a light-weight quantization aware fine tuning technique using knowledge distillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs using commonly available datasets to realize a popular language use case, on device chat applications. To improve this paradigm of finetuning, as main contributions, we provide insights into stability of KD-QAT by empirically studying the gradient propagation during training to better understand the vulnerabilities of KD-QAT based approaches to low-bit quantization errors. Based on our insights, we propose ov-freeze, a simple technique to stabilize the KD-QAT process. Finally, we expe
    
[^80]: 大型语言模型生成被认为具有共情的回应

    Large Language Models Produce Responses Perceived to be Empathic

    [https://arxiv.org/abs/2403.18148](https://arxiv.org/abs/2403.18148)

    通过两项研究，发现大型语言模型生成的回复在共情性方面被认为比人类撰写的回复更具有共情性，这表明了在人际支持方面使用LLMs的潜力。

    

    大型语言模型（LLMs）在许多任务中表现出令人惊讶的性能，包括撰写显示共情的支持性消息。在这项研究中，我们让这些模型根据描述常见生活经历的帖子生成共情消息，如工作场景、育儿、人际关系以及其他引发焦虑和愤怒的情况。在两项研究中（N=192，202），我们向人类评分员展示了由几种模型（GPT4 Turbo，Llama2 和 Mistral）撰写的各种回复，并让人们根据这些回复在共情程度上评分。我们发现，LLM 生成的回复被一致评为比人类撰写的回复更具共情性。语言分析还表明，这些模型在使用标点符号、表情符号和某些词汇方面具有明显、可预测的“风格”。这些结果凸显了在需要共情的情境中利用LLMs提升人类同行支持的潜力。

    arXiv:2403.18148v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated surprising performance on many tasks, including writing supportive messages that display empathy. Here, we had these models generate empathic messages in response to posts describing common life experiences, such as workplace situations, parenting, relationships, and other anxiety- and anger-eliciting situations. Across two studies (N=192, 202), we showed human raters a variety of responses written by several models (GPT4 Turbo, Llama2, and Mistral), and had people rate these responses on how empathic they seemed to be. We found that LLM-generated responses were consistently rated as more empathic than human-written responses. Linguistic analyses also show that these models write in distinct, predictable ``styles", in terms of their use of punctuation, emojis, and certain words. These results highlight the potential of using LLMs to enhance human peer support in contexts where empathy is i
    
[^81]: 一个用于多机器人计划执行的实时重调度算法

    A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution

    [https://arxiv.org/abs/2403.18145](https://arxiv.org/abs/2403.18145)

    该研究提出了一种名为Switchable-Edge Search（SES）的A*风格算法，用于实现智能体通过顺序的重新调度，最佳变体在效率上比基线快4倍。

    

    多智能体路径规划中的一个研究领域是确定在执行过程中智能体延迟时如何有效实现重新规划。一种选择是重新安排智能体的通过顺序，即智能体访问相同位置的顺序。作为回应，我们提出了Switchable-Edge Search（SES），这是一种设计用于找到最优通过顺序的A*风格算法。我们证明了SES的最优性，并通过模拟评估了其效率。SES的最佳变体对于小型和中等规模问题需要不到1秒钟，并且在大型问题上比基线运行快4倍。

    arXiv:2403.18145v1 Announce Type: new  Abstract: One area of research in multi-agent path finding is to determine how replanning can be efficiently achieved in the case of agents being delayed during execution. One option is to reschedule the passing order of agents, i.e., the sequence in which agents visit the same location. In response, we propose Switchable-Edge Search (SES), an A*-style algorithm designed to find optimal passing orders. We prove the optimality of SES and evaluate its efficiency via simulations. The best variant of SES takes less than 1 second for small- and medium-sized problems and runs up to 4 times faster than baselines for large-sized problems.
    
[^82]: Juru: 来自可靠来源的巴西法律大语言模型

    Juru: Legal Brazilian Large Language Model from Reputable Sources

    [https://arxiv.org/abs/2403.18140](https://arxiv.org/abs/2403.18140)

    Juru 模型通过从巴西法律来源提取的19亿个唯一标记，展示了领域专门化可以在减少预训练数据量方面发挥作用，但这种专门化会导致同一语言中其他知识领域性能下降。

    

    与预训练大型语言模型相关的高计算成本限制了相关研究。为解决这一问题，出现了两种策略：领域专门化和使用高质量数据进行预训练。为探索这些策略，我们使用来自可靠巴西法律来源的19亿个唯一标记专门化了Sabi\'a-2 Small模型，并在法律和一般知识考试中进行了少样本评估。我们的模型Juru展示了领域专门化在减少预训练数据量方面的优势。然而，这种专门化是以在同一语言中其他知识领域性能下降为代价的。这项研究有助于增加的科学证据，表明预训练数据的选择可能提高大型语言模型的性能，从而能够以较低成本探索这些模型。

    arXiv:2403.18140v1 Announce Type: cross  Abstract: The high computational cost associated with pretraining large language models limits their research. Two strategies have emerged to address this issue: domain specialization and pretraining with high-quality data. To explore these strategies, we specialized the Sabi\'a-2 Small model with 1.9 billion unique tokens from reputable Brazilian legal sources and conducted few-shot evaluations on legal and general knowledge exams. Our model, Juru, demonstrates the benefits of domain specialization with a reduced amount of pretraining data. However, this specialization comes at the expense of degrading performance in other knowledge areas within the same language. This study contributes to the growing body of scientific evidence showing that pretraining data selection may enhance the performance of large language models, enabling the exploration of these models at a lower cost.
    
[^83]: 保护GNN：基于解释的后门训练图识别

    Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs

    [https://arxiv.org/abs/2403.18136](https://arxiv.org/abs/2403.18136)

    提出了一种基于解释的方法来识别GNN中的后门训练图，设计了七种新的度量指标以更有效地检测后门攻击，并且通过自适应攻击进行了方法评估。

    

    Graph Neural Networks (GNNs)已经在许多领域流行起来，但它们容易受到后门攻击，这可能会损害它们的性能和道德应用。检测这些攻击对于保持GNN分类任务的可靠性和安全性至关重要，但有效的检测技术并不多见。我们观察到，尽管图级解释能够提供一些有限的见解，但它们在检测后门触发器方面的有效性是不一致且不完整的。为弥补这一差距，我们提取并转换GNN解释机制的次要输出，设计了七种更有效地检测后门攻击的新度量。此外，我们还开发了一种自适应攻击来严格评估我们的方法。我们在多个基准数据集上测试了我们的方法，并检查其对各种攻击模型的有效性。我们的结果表明，我们的方法可以取得较高的效果。

    arXiv:2403.18136v1 Announce Type: cross  Abstract: Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet they are vulnerable to backdoor attacks that can compromise their performance and ethical application. The detection of these attacks is crucial for maintaining the reliability and security of GNN classification tasks, but effective detection techniques are lacking. Following an initial investigation, we observed that while graph-level explanations can offer limited insights, their effectiveness in detecting backdoor triggers is inconsistent and incomplete. To bridge this gap, we extract and transform secondary outputs of GNN explanation mechanisms, designing seven novel metrics that more effectively detect backdoor attacks. Additionally, we develop an adaptive attack to rigorously evaluate our approach. We test our method on multiple benchmark datasets and examine its efficacy against various attack models. Our results show that our method can achieve high de
    
[^84]: AE SemRL：使用自动编码器学习语义关联规则

    AE SemRL: Learning Semantic Association Rules with Autoencoders

    [https://arxiv.org/abs/2403.18133](https://arxiv.org/abs/2403.18133)

    使用自动编码器学习时间序列数据中的语义关联关系，实验表明这种方法速度快上百倍

    

    关联规则挖掘（ARM）是学习数据特征之间逻辑规则关联的任务。从高维数值数据中挖掘关联规则，例如从智能环境中大量传感器的时间序列数据，是一项计算密集型的任务。在本研究中，我们提出了一种基于自动编码器的方法，用于从时间序列数据中学习和提取关联规则（AE SemRL）。此外，我们认为，在与时间序列数据源相关的语义信息存在时，语义可以促进学习可推广和可解释的关联规则。尽管通过额外的语义特征丰富时间序列数据，AE SemRL使得从高维数据中学习关联规则变得可行。我们的实验表明，可以从自动编码器生成的潜在表示中提取语义关联规则，并且此方法速度快上百倍。

    arXiv:2403.18133v1 Announce Type: cross  Abstract: Association Rule Mining (ARM) is the task of learning associations among data features in the form of logical rules. Mining association rules from high-dimensional numerical data, for example, time series data from a large number of sensors in a smart environment, is a computationally intensive task. In this study, we propose an Autoencoder-based approach to learn and extract association rules from time series data (AE SemRL). Moreover, we argue that in the presence of semantic information related to time series data sources, semantics can facilitate learning generalizable and explainable association rules. Despite enriching time series data with additional semantic features, AE SemRL makes learning association rules from high-dimensional data feasible. Our experiments show that semantic association rules can be extracted from a latent representation created by an Autoencoder and this method has in the order of hundreds of times faster
    
[^85]: 通过模拟未来数据推荐无数据的类递增学习算法

    Recommendation of data-free class-incremental learning algorithms by simulating future data

    [https://arxiv.org/abs/2403.18132](https://arxiv.org/abs/2403.18132)

    通过模拟未来数据流，推荐无数据的类递增学习算法，实验结果表明方法胜过竞争基线

    

    类递增学习处理由类别批次组成的顺序数据流。已经提出了各种算法来解决从无法存储过去类别的样本的具有挑战性的情况。然而，为用户定义的设置选择适当的算法是一个未解之谜，因为这些算法的相对性能取决于递增设置。为了解决这个问题，我们引入了一种模拟未来数据流的算法推荐方法。给定一个初始类别集合，它利用生成模型从相同的视觉域模拟未来类别。我们在模拟流上评估了最近的算法，并推荐在用户定义的递增设置中表现最佳的算法。我们使用六种算法和六个递增设置，在三个大型数据集上展示了我们方法的有效性。我们的方法胜过了竞争基线，并且性能接近

    arXiv:2403.18132v1 Announce Type: cross  Abstract: Class-incremental learning deals with sequential data streams composed of batches of classes. Various algorithms have been proposed to address the challenging case where samples from past classes cannot be stored. However, selecting an appropriate algorithm for a user-defined setting is an open problem, as the relative performance of these algorithms depends on the incremental settings. To solve this problem, we introduce an algorithm recommendation method that simulates the future data stream. Given an initial set of classes, it leverages generative models to simulate future classes from the same visual domain. We evaluate recent algorithms on the simulated stream and recommend the one which performs best in the user-defined incremental setting. We illustrate the effectiveness of our method on three large datasets using six algorithms and six incremental settings. Our method outperforms competitive baselines, and performance is close 
    
[^86]: 不要相信：验证--用自动形式化为基础的LLM定量推理

    Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization

    [https://arxiv.org/abs/2403.18120](https://arxiv.org/abs/2403.18120)

    通过将非正式的数学陈述翻译为形式的Isabelle代码并进行自动验证，我们提供了一种机制，可以自动拒绝在内部一致性方面与形式化问题陈述不一致的解决方案。

    

    大型语言模型（LLM），如Google的Minerva和OpenAI的GPT系列，正在越来越能够解决数学定量推理问题。然而，它们在推理步骤和答案中仍然存在没有理由的逻辑和计算错误。本文利用LLMs的训练语料库包含足够多的形式化数学示例（例如在Isabelle中，一个形式定理证明环境），它们可以被提示将非正式的数学陈述翻译即自动形式化为形式的Isabelle代码--该代码可以被自动验证内部一致性。这提供了一个机制，可以自动拒绝那些其形式化版本在其内部或与形式化问题陈述不一致的解决方案。我们在GSM8K、MATH和MultiArith数据集上评估了我们的方法，并证明我们的方法提供了一个一直比

    arXiv:2403.18120v1 Announce Type: new  Abstract: Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than 
    
[^87]: QuakeSet：一个数据集和低资源模型，用于通过Sentinel-1监测地震

    QuakeSet: A Dataset and Low-Resource Models to Monitor Earthquakes through Sentinel-1

    [https://arxiv.org/abs/2403.18116](https://arxiv.org/abs/2403.18116)

    QuakeSet提出了一个数据集和低资源模型，利用Sentinel-1来监测地震。通过社交媒体图像进行地震监测在危机管理中显示出了有效性，但在估计地震的严重性和特征方面仍存在限制。

    

    地震监测是必要的，以迅速确定受影响区域、事件的严重程度，最终估计损害并计划所需的恢复过程。使用社交媒体图像在危机管理中已被证明在各种情况下是有效的。然而，在地震情况下，它们仍受限于使用通信基础设施的可能性以及该地区的人员存在。此外，社交媒体图像和消息无法有效地用于估计地震的实际严重性和特征。

    arXiv:2403.18116v1 Announce Type: cross  Abstract: Earthquake monitoring is necessary to promptly identify the affected areas, the severity of the events, and, finally, to estimate damages and plan the actions needed for the restoration process. The use of seismic stations to monitor the strength and origin of earthquakes is limited when dealing with remote areas (we cannot have global capillary coverage). Identification and analysis of all affected areas is mandatory to support areas not monitored by traditional stations. Using social media images in crisis management has proven effective in various situations. However, they are still limited by the possibility of using communication infrastructures in case of an earthquake and by the presence of people in the area. Moreover, social media images and messages cannot be used to estimate the actual severity of earthquakes and their characteristics effectively. The employment of satellites to monitor changes around the globe grants the po
    
[^88]: 大型语言模型在教育领域的应用：调研与展望

    Large Language Models for Education: A Survey and Outlook

    [https://arxiv.org/abs/2403.18105](https://arxiv.org/abs/2403.18105)

    大型语言模型在教育领域的应用调研总结了LLMs在教育中的各种技术应用，包括学生和教师辅助、自适应学习和商业工具，提出了未来研究机会和潜在方向。

    

    大型语言模型（LLMs）的出现为教育领域带来了新的可能性。这篇调研论文总结了LLMs在教育环境中的各种技术，涵盖了学生和教师的辅助，自适应学习和商业工具。我们系统地审查了每个视角中的技术进步，整理了相关数据集和基准测试，并确定了在教育中部署LLMs所涉及的风险和挑战。此外，我们概述了未来的研究机会，突出了潜在的有前途的方向。我们的调研旨在为教育工作者、研究人员和决策者提供全面的技术图景，以利用LLMs的力量，彻底改革教育实践，并促进更有效的个性化学习环境。

    arXiv:2403.18105v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education. This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment.
    
[^89]: 迈向可解释的聚类：一种基于约束声明的方法

    Towards Explainable Clustering: A Constrained Declarative based Approach

    [https://arxiv.org/abs/2403.18101](https://arxiv.org/abs/2403.18101)

    该论文提出了一种新颖的可解释的约束聚类方法ECS，旨在找到高质量且可解释的聚类，强调在构建聚类时应考虑经典聚类标准和可解释性。

    

    可解释AI领域在所有机器学习领域都备受关注，而在聚类中更为重要，聚类是一项无监督任务，其结果必须由领域专家验证。我们的目标是找到一个在经典聚类标准方面具有高质量且可解释的聚类，我们认为在构建聚类时必须考虑这两个维度。我们认为，聚类的一个良好的全局解释应该考虑每个簇的特征，考虑到它们描述其对象的能力（覆盖率），同时将其与其他簇区分开（区分度）。此外，我们旨在利用知识专家在期望聚类的结构或其解释方面的不同级别的知识。在我们的框架中，一个簇的解释是一组模式，我们提出了一种新颖的可解释的约束聚类方法，称为ECS来支持声明性聚类。

    arXiv:2403.18101v1 Announce Type: new  Abstract: The domain of explainable AI is of interest in all Machine Learning fields, and it is all the more important in clustering, an unsupervised task whose result must be validated by a domain expert. We aim at finding a clustering that has high quality in terms of classic clustering criteria and that is explainable, and we argue that these two dimensions must be considered when building the clustering. We consider that a good global explanation of a clustering should give the characteristics of each cluster taking into account their abilities to describe its objects (coverage) while distinguishing it from the other clusters (discrimination). Furthermore, we aim at leveraging expert knowledge, at different levels, on the structure of the expected clustering or on its explanations. In our framework an explanation of a cluster is a set of patterns, and we propose a novel interpretable constrained clustering method called ECS for declarative clu
    
[^90]: 通过云计算和机器学习驱动智能物联网监控与控制

    Driving Intelligent IoT Monitoring and Control through Cloud Computing and Machine Learning

    [https://arxiv.org/abs/2403.18100](https://arxiv.org/abs/2403.18100)

    通过边缘计算和物联网的组合，可以减少延迟，提高效率。

    

    这篇文章探讨了如何通过云计算和机器学习驱动智能物联网的监控和控制。随着物联网和云继续在网络中生成大量和多样化的数据作为传感器设备，收集到的数据被发送到云端进行统计分析、预测和数据分析以实现业务目标。然而，由于云计算模型受限于距离，对于网络连接质量不理想的环境可能在关键操作上存在问题。因此，边缘计算作为分布式计算架构，将处理应用程序、数据和服务的位置从网络的中心节点移动到网络的逻辑边缘节点，以减少对云处理和数据分析的依赖，实现近端数据处理和分析。物联网和边缘计算的结合可以减少延迟，提高效率。

    arXiv:2403.18100v1 Announce Type: new  Abstract: This article explores how to drive intelligent iot monitoring and control through cloud computing and machine learning. As iot and the cloud continue to generate large and diverse amounts of data as sensor devices in the network, the collected data is sent to the cloud for statistical analysis, prediction, and data analysis to achieve business objectives. However, because the cloud computing model is limited by distance, it can be problematic in environments where the quality of the Internet connection is not ideal for critical operations. Therefore, edge computing, as a distributed computing architecture, moves the location of processing applications, data and services from the central node of the network to the logical edge node of the network to reduce the dependence on cloud processing and analysis of data, and achieve near-end data processing and analysis. The combination of iot and edge computing can reduce latency, improve efficie
    
[^91]: GPT与语言障碍：跨语言法律问答的研究

    GPTs and Language Barrier: A Cross-Lingual Legal QA Examination

    [https://arxiv.org/abs/2403.18098](https://arxiv.org/abs/2403.18098)

    本研究探讨了GPT在跨语言法律问答领域的应用，通过分析英语和日语提示对性能的影响，为发展更高效准确的跨语言问答解决方案做出贡献。

    

    在这篇论文中，我们使用COLIEE Task 4数据集探讨了生成式预训练变换器（GPTs）在跨语言法律问答系统中的应用。在COLIEE Task 4中，给定一个陈述和一组作为上下文的相关法律文章，目标是确定该陈述是否在法律上有效，即是否可以从提供的上下文文章中推断出来，也称为蕴涵任务。通过评估四种不同的英语和日语提示和数据组合，我们为多语言法律问答场景中GPTs的性能提供了宝贵的见解，有助于开发在法律领域更高效准确的跨语言问答解决方案。

    arXiv:2403.18098v1 Announce Type: cross  Abstract: In this paper, we explore the application of Generative Pre-trained Transformers (GPTs) in cross-lingual legal Question-Answering (QA) systems using the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a set of related legal articles that serve as context, the objective is to determine whether the statement is legally valid, i.e., if it can be inferred from the provided contextual articles or not, which is also known as an entailment task. By benchmarking four different combinations of English and Japanese prompts and data, we provide valuable insights into GPTs' performance in multilingual legal QA scenarios, contributing to the development of more efficient and accurate cross-lingual QA solutions in the legal domain.
    
[^92]: 提升法律文件检索：采用大型语言模型的多阶段方法

    Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models

    [https://arxiv.org/abs/2403.18093](https://arxiv.org/abs/2403.18093)

    将提示技术作为检索系统的最后阶段，通过BM25预排序和基于BERT的重新排序的支持，可以显著提高法律文件检索的准确性。

    

    大规模语言模型，如GPT-3.5、GPT-4和LLaMA等拥有数十亿参数的模型正变得越来越普遍。许多研究探索了有效的提示技术，以利用这些LLM在各种研究问题上的能力。在法律数据领域，具体在检索上，由于法律文章数量庞大且长度可观，直接应用提示技术面临着挑战。这项研究专注于最大程度地发挥提示的潜力，将其置于检索系统的最后阶段，前面有两个阶段的支持：BM25预排序和基于BERT的重新排序。在COLIEE 2023数据集上的实验表明，将提示技术整合到检索系统中显著提高了检索精度。然而，错误分析揭示了检索系统中仍需要解决的一些现有问题。

    arXiv:2403.18093v1 Announce Type: cross  Abstract: Large language models with billions of parameters, such as GPT-3.5, GPT-4, and LLaMA, are increasingly prevalent. Numerous studies have explored effective prompting techniques to harness the power of these LLMs for various research problems. Retrieval, specifically in the legal data domain, poses a challenging task for the direct application of Prompting techniques due to the large number and substantial length of legal articles. This research focuses on maximizing the potential of prompting by placing it as the final phase of the retrieval system, preceded by the support of two phases: BM25 Pre-ranking and BERT-based Re-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating prompting techniques on LLMs into the retrieval system significantly improves retrieval accuracy. However, error analysis reveals several existing issues in the retrieval system that still need resolution.
    
[^93]: 正态形式博弈中的均衡路径

    Paths to Equilibrium in Normal-Form Games

    [https://arxiv.org/abs/2403.18079](https://arxiv.org/abs/2403.18079)

    本文研究在多智体强化学习中的策略序列，探讨满足特定约束的策略路径，即满足路径，对于构建终止于均衡策略的路径具有重要意义。

    

    在多智体强化学习（MARL）中，智体会反复在时间上交互，并随着新数据的到来修订他们的策略，从而产生一系列策略概况。本文研究满足一种由强化学习中政策更新启发的成对约束的策略序列，其中在第 $t$ 期最优应答的智体在下一期 $t+1$ 不会改变其策略。这种约束仅要求优化智体不更改策略，但并不以任何方式限制其他非最优化智体，因此允许探索。具有此属性的序列被称为满足路径，并在许多 MARL 算法中自然出现。关于战略动态的一个基本问题是：对于给定的博弈和初始策略概况，是否总可以构建一个终止于均衡策略的满足路径？这个问题的解决对应着一些重要含义。

    arXiv:2403.18079v1 Announce Type: cross  Abstract: In multi-agent reinforcement learning (MARL), agents repeatedly interact across time and revise their strategies as new data arrives, producing a sequence of strategy profiles. This paper studies sequences of strategies satisfying a pairwise constraint inspired by policy updating in reinforcement learning, where an agent who is best responding in period $t$ does not switch its strategy in the next period $t+1$. This constraint merely requires that optimizing agents do not switch strategies, but does not constrain the other non-optimizing agents in any way, and thus allows for exploration. Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms. A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium strategy? The resolution of this question has implication
    
[^94]: 深度学习在海洋垃圾跟踪和检测中的最新应用：一项调查

    State of the art applications of deep learning within tracking and detecting marine debris: A survey

    [https://arxiv.org/abs/2403.18067](https://arxiv.org/abs/2403.18067)

    深度学习技术在海洋垃圾领域取得了长足进展，特别是YOLO系列在目标检测方面表现优异，但研究显示当前缺乏全面的水下垃圾数据库，这成为了进一步研究的瓶颈。

    

    深度学习技术在海洋垃圾问题中已经被探索了大约20年，但大多数研究在过去五年内快速发展。我们提供了对最近28项深度学习在海洋垃圾领域中最新和最重要贡献的深入、最新的总结和分析。通过交叉引用研究论文结果，YOLO系列明显优于所有其他目标检测方法，但许多受尊敬的贡献者明确表示，目前没有现成的用于机器学习的全面水下垃圾数据库。我们使用由我们精心策划和标记的小型数据集，在一个二分类任务上测试了YOLOv5，并发现准确率较低，假阳性率较高，突显了全面数据库的重要性。我们在这项调查中总结了超过40项未来的研究建议。

    arXiv:2403.18067v1 Announce Type: cross  Abstract: Deep learning techniques have been explored within the marine litter problem for approximately 20 years but the majority of the research has developed rapidly in the last five years. We provide an in-depth, up to date, summary and analysis of 28 of the most recent and significant contributions of deep learning in marine debris. From cross referencing the research paper results, the YOLO family significantly outperforms all other methods of object detection but there are many respected contributions to this field that have categorically agreed that a comprehensive database of underwater debris is not currently available for machine learning. Using a small dataset curated and labelled by us, we tested YOLOv5 on a binary classification task and found the accuracy was low and the rate of false positives was high; highlighting the importance of a comprehensive database. We conclude this survey with over 40 future research recommendations an
    
[^95]: 光谱卷积变压器：协调视觉变压器中的实部和复部多视图光谱算子

    Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer

    [https://arxiv.org/abs/2403.18063](https://arxiv.org/abs/2403.18063)

    该论文提出了光谱卷积变压器 (SCT)，通过结合局部信息的卷积操作和全局信息的复杂傅里叶基础，实现了对视觉变压器中实部和复部多视图光谱算子的协调，从而实现了更好的性能。

    

    视觉中使用的Transformer已经通过各种结构进行了研究 - 如ViT、PVT和Swin。这些工作旨在改进注意力机制并使其更加高效。与此不同的是，人们感受到了包含局部信息的需要，这导致在Transformer中引入卷积，如CPVT和CvT。我们使用复杂傅立叶基础捕捉全局信息，通过各种方法，如AFNO、GFNet和Spectformer实现全局令牌混合。我们提倡结合数据的三种不同视图 - 局部、全局和长程依赖性。我们还研究了仅使用实域光谱表示的最简单全局表示 - 通过Hartley变换获得。我们在初始层中使用卷积算子捕捉局部信息。通过这两个贡献，我们能够优化并获得一个提供改进性能的光谱卷积变压器（SCT）。

    arXiv:2403.18063v1 Announce Type: cross  Abstract: Transformers used in vision have been investigated through diverse architectures - ViT, PVT, and Swin. These have worked to improve the attention mechanism and make it more efficient. Differently, the need for including local information was felt, leading to incorporating convolutions in transformers such as CPVT and CvT. Global information is captured using a complex Fourier basis to achieve global token mixing through various methods, such as AFNO, GFNet, and Spectformer. We advocate combining three diverse views of data - local, global, and long-range dependence. We also investigate the simplest global representation using only the real domain spectral representation - obtained through the Hartley transform. We use a convolutional operator in the initial layers to capture local information. Through these two contributions, we are able to optimize and obtain a spectral convolution transformer (SCT) that provides improved performance 
    
[^96]: ShapeGrasp：利用几何分解通过大型语言模型进行零样本任务导向抓取

    ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models through Geometric Decomposition

    [https://arxiv.org/abs/2403.18062](https://arxiv.org/abs/2403.18062)

    通过几何分解并利用大型语言模型，我们提出了一种零样本任务导向抓取方法，通过最少的信息实现智能抓取。

    

    在家庭动态环境中，针对陌生物体进行任务导向的抓取是机器人必备的技能。受到人类通过对物体形状和结构的直觉来抓取物体的能力的启发，我们提出了一种新颖的零样本任务导向抓取方法，利用将目标物体几何分解为简单凸形状的图结构，包括几何属性和空间关系。我们的方法利用最小的必要信息 - 物体的名称和预期任务 - 促进零样本任务导向抓取。我们利用大型语言模型的常识推理能力，动态地为每个分解部分分配语义含义，随后对每个部分对于预期任务的实用性进行推理。通过在一个真实的机器人平台上进行大量实验，我们展示了我们的抓取方法的分解和推理...

    arXiv:2403.18062v1 Announce Type: cross  Abstract: Task-oriented grasping of unfamiliar objects is a necessary skill for robots in dynamic in-home environments. Inspired by the human capability to grasp such objects through intuition about their shape and structure, we present a novel zero-shot task-oriented grasping method leveraging a geometric decomposition of the target object into simple, convex shapes that we represent in a graph structure, including geometric attributes and spatial relationships. Our approach employs minimal essential information - the object's name and the intended task - to facilitate zero-shot task-oriented grasping. We utilize the commonsense reasoning capabilities of large language models to dynamically assign semantic meaning to each decomposed part and subsequently reason over the utility of each part for the intended task. Through extensive experiments on a real-world robotics platform, we demonstrate that our grasping approach's decomposition and reason
    
[^97]: COIG-CQIA：只需质量——面向中文指令微调的论文

    COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning

    [https://arxiv.org/abs/2403.18058](https://arxiv.org/abs/2403.18058)

    COIG-CQIA 是一个高质量的中文指令微调数据集，旨在构建一个多样化、广泛的指令微调数据集，以更好地使模型行为与人类交互保持一致。

    

    最近，大型语言模型（LLMs）取得了显著进展，特别是在英语领域。这些进展使得这些LLMs能够以前所未有的准确性和流畅度理解并执行复杂指令。然而，尽管取得了这些进展，中文指令微调的发展仍存在明显差距。中文语言的独特语言特征和文化深度为指令微调任务带来挑战。现有的数据集要么源自以英语为中心的LLMs，要么不适合与现实中文用户的交互模式相符。为填补这一差距，我们引入了COIG-CQIA，一个高质量的中文指令微调数据集。我们的目标是构建一个多样化、广泛的指令微调数据集，以更好地使模型行为与人类交互保持一致。为此，我们从不同来源收集了高质量的人类编写语料库。

    arXiv:2403.18058v1 Announce Type: cross  Abstract: Recently, there have been significant advancements in large language models (LLMs), particularly focused on the English language. These advancements have enabled these LLMs to understand and execute complex instructions with unprecedented accuracy and fluency. However, despite these advancements, there remains a noticeable gap in the development of Chinese instruction tuning. The unique linguistic features and cultural depth of the Chinese language pose challenges for instruction tuning tasks. Existing datasets are either derived from English-centric LLMs or are ill-suited for aligning with the interaction patterns of real-world Chinese users. To bridge this gap, we introduce COIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to build a diverse, wide-ranging instruction-tuning dataset to better align model behavior with human interactions. To this end, we collect a high-quality human-written corpus from various so
    
[^98]: 面向大规模异构多智能体系统的优先级联强化学习

    Prioritized League Reinforcement Learning for Large-Scale Heterogeneous Multiagent Systems

    [https://arxiv.org/abs/2403.18057](https://arxiv.org/abs/2403.18057)

    提出了一种优先级异构联盟强化学习（PHLRL）方法，旨在解决大规模异构合作问题，通过记录智能体探索过的各种策略并建立异构联盟来优化未来的策略。

    

    大规模异构多智能体系统在现实世界中具有各种真实因素，如具有不同能力和整体系统成本的智能体。与同质系统相比，异构系统具有显著的实际优势。然而，它们也为多智能体强化学习带来挑战，包括解决非静态问题和管理具有不同类型的不平衡数量智能体。我们提出了一种名为优先级异构联盟强化学习（PHLRL）的方法，以解决大规模异构合作问题。PHLRL记录智能体在训练过程中探索过的各种策略，并建立一个由多样策略组成的异构联盟，以帮助未来的策略优化。此外，我们设计了一种优先级策略梯度方法，以补偿由不同类型智能体数量差异引起的差距。

    arXiv:2403.18057v1 Announce Type: new  Abstract: Large-scale heterogeneous multiagent systems feature various realistic factors in the real world, such as agents with diverse abilities and overall system cost. In comparison to homogeneous systems, heterogeneous systems offer significant practical advantages. Nonetheless, they also present challenges for multiagent reinforcement learning, including addressing the non-stationary problem and managing an imbalanced number of agents with different types. We propose a Prioritized Heterogeneous League Reinforcement Learning (PHLRL) method to address large-scale heterogeneous cooperation problems. PHLRL maintains a record of various policies that agents have explored during their training and establishes a heterogeneous league consisting of diverse policies to aid in future policy optimization. Furthermore, we design a prioritized policy gradient approach to compensate for the gap caused by differences in the number of different types of agent
    
[^99]: 具有可扩展合作图的自主分层多智能体强化学习

    Self-Clustering Hierarchical Multi-Agent Reinforcement Learning with Extensible Cooperation Graph

    [https://arxiv.org/abs/2403.18056](https://arxiv.org/abs/2403.18056)

    本论文提出了一种名为Hierarchical Cooperation Graph Learning（HCGL）的新型分层MARL模型，其中代理的行为受可扩展合作图（ECG）的拓扑结构引导，解决了一般多智能体问题。

    

    多智能体强化学习（MARL）在解决许多合作挑战方面取得了成功。然而，经典的非分层MARL算法仍然无法解决需要分层合作行为的各种复杂多智能体问题。非分层算法学习的合作知识和策略是隐式的，不易解释，从而限制了现有知识的整合。本文提出了一种解决一般多智能体问题的新型分层MARL模型，名为Hierarchical Cooperation Graph Learning（HCGL）。HCGL有三个组成部分：用于实现自我聚类合作的动态可扩展合作图（ECG）；一组用于调整ECG拓扑结构的图操作器；以及用于训练这些图操作器的MARL优化器。HCGL与其他MARL模型的主要区别在于，代理的行为受ECG拓扑结构的引导，而不是策略神经网络。

    arXiv:2403.18056v1 Announce Type: new  Abstract: Multi-Agent Reinforcement Learning (MARL) has been successful in solving many cooperative challenges. However, classic non-hierarchical MARL algorithms still cannot address various complex multi-agent problems that require hierarchical cooperative behaviors. The cooperative knowledge and policies learned in non-hierarchical algorithms are implicit and not interpretable, thereby restricting the integration of existing knowledge. This paper proposes a novel hierarchical MARL model called Hierarchical Cooperation Graph Learning (HCGL) for solving general multi-agent problems. HCGL has three components: a dynamic Extensible Cooperation Graph (ECG) for achieving self-clustering cooperation; a group of graph operators for adjusting the topology of ECG; and an MARL optimizer for training these graph operators. HCGL's key distinction from other MARL models is that the behaviors of agents are guided by the topology of ECG instead of policy neural
    
[^100]: 督导提示训练

    Supervisory Prompt Training

    [https://arxiv.org/abs/2403.18051](https://arxiv.org/abs/2403.18051)

    提出了一种督导提示训练（SPT）方法，利用双LLM系统生成高效提示并引入影响分数概念，通过优化提示成功提高了LLMs的性能。

    

    大型语言模型（LLMs）的性能在很大程度上取决于提示的质量，这些提示通常是手工设计的并且特定于任务，使得它们昂贵且不可扩展。我们提出了一种新颖的方法，即督导提示训练（SPT）。SPT利用双LLM系统自动生成高效的提示。在该系统中，一个LLM，即生成器，执行任务，而另一个LLM，即校正器，提供反馈并生成改进的提示。与先前的技术相比，生成器和校正器在时间上共同并持续改进它们的提示。我们还介绍了“影响分数”的概念，用于衡量提示的句子级有效性。我们的方法在四个基准上进行了测试，测试LLMs中幻觉的水平。值得注意的是，我们成功将GPT-4在GSM8K上的准确率从65.8%提高到94.1%（增加28.3%）。SPT通过优化提示提高了LLMs的性能。

    arXiv:2403.18051v1 Announce Type: cross  Abstract: The performance of Large Language Models (LLMs) relies heavily on the quality of prompts, which are often manually engineered and task-specific, making them costly and non-scalable. We propose a novel approach, Supervisory Prompt Training (SPT). SPT automates the generation of highly effective prompts using a dual LLM system. In this system, one LLM, the generator, performs a task while the other, the corrector, provides feedback and generates improved prompts. In contrast to earlier techniques, both the generator and corrector collaboratively and continuously improve their prompts over time. We also introduce the concept of \textit{impact scores} to measure the sentence-level effectiveness of the prompts. Our method was tested on four benchmarks, testing the level of hallucinations in LLMs. Notably, we were able to increase the accuracy of GPT-4 on GSM8K from 65.8\% to 94.1\% (28.3\% increase). SPT advances LLMs by refining prompts to
    
[^101]: 从部分观测预测物种出现模式

    Predicting species occurrence patterns from partial observations

    [https://arxiv.org/abs/2403.18028](https://arxiv.org/abs/2403.18028)

    提出了一个问题，即采用卫星图像和其他物种出现信息来预测物种出现模式，并提出了一个通用模型R-Tran，可以利用部分观测数据进行预测，优于其他方法。

    

    为了应对生物多样性和气候危机，我们需要了解物种分布的位置以及这些模式如何变化。然而，大多数物种的观测数据仍然非常有限，可用数据的量在不同分类群之间差异很大。我们提出了一个问题，即在给定卫星图像和其他物种出现信息的情况下预测物种出现模式。为了在此任务上评估算法，我们介绍了SatButterfly数据集，其中包含了蝴蝶的卫星图像、环境数据和观测数据，旨在与现有的鸟类观测数据集SatBird配对。为了解决这一任务，我们提出了一个通用模型R-Tran，用于预测物种出现模式，可以在任何地方使用部分观测数据。我们发现，R-Tran在预测物种遭遇率方面优于其他方法。

    arXiv:2403.18028v1 Announce Type: cross  Abstract: To address the interlinked biodiversity and climate crises, we need an understanding of where species occur and how these patterns are changing. However, observational data on most species remains very limited, and the amount of data available varies greatly between taxonomic groups. We introduce the problem of predicting species occurrence patterns given (a) satellite imagery, and (b) known information on the occurrence of other species. To evaluate algorithms on this task, we introduce SatButterfly, a dataset of satellite images, environmental data and observational data for butterflies, which is designed to pair with the existing SatBird dataset of bird observational data. To address this task, we propose a general model, R-Tran, for predicting species occurrence patterns that enables the use of partial observational data wherever found. We find that R-Tran outperforms other methods in predicting species encounter rates with partial
    
[^102]: 通过特定掩码损失改善预训练语言模型的敏感性：以生物医学实体识别为例

    Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER

    [https://arxiv.org/abs/2403.18025](https://arxiv.org/abs/2403.18025)

    提出了Mask Specific Language Modeling（MSLM）方法来改善LM在微调过程中对目标领域知识的敏感性，通过加权领域特定术语的重要性进行学习。

    

    将语言模型（LMs）调整到新领域通常通过在特定领域数据上微调预训练LM（PLM）来实现。微调将新知识引入LM，使它能够理解和有效执行目标域任务。然而，微调可能会无意中变得不够敏感，如果它忽视了源域和目标域之间的广泛差异（例如在词义上）。为了解决微调不敏感的问题，我们提出了Mask Specific Language Modeling（MSLM），一种通过在微调过程中适当加权领域特定术语（DS-terms）的重要性来有效获取目标领域知识的方法。MSLM同时屏蔽DS术语和通用词，然后通过确保LM受到更大惩罚来学习特定于掩码的损失。

    arXiv:2403.18025v1 Announce Type: cross  Abstract: Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for in
    
[^103]: 考虑Wasserstein图匹配的半监督图像字幕生成

    Semi-Supervised Image Captioning Considering Wasserstein Graph Matching

    [https://arxiv.org/abs/2403.17995](https://arxiv.org/abs/2403.17995)

    提出了一种考虑Wasserstein图匹配的半监督图像字幕生成方法，用于解决半监督图像字幕生成中的困境

    

    图像字幕生成可以自动生成给定图像的字幕，其关键挑战是学习从视觉特征到自然语言特征的映射函数。为了解决对描述图像需要大量人力的困境，提出了一种考虑Wasserstein图匹配的新型半监督图像字幕生成方法（SSIC-WGM），以监督生成的句子。

    arXiv:2403.17995v1 Announce Type: cross  Abstract: Image captioning can automatically generate captions for the given images, and the key challenge is to learn a mapping function from visual features to natural language features. Existing approaches are mostly supervised ones, i.e., each image has a corresponding sentence in the training set. However, considering that describing images always requires a huge of manpower, we usually have limited amount of described images (i.e., image-text pairs) and a large number of undescribed images in real-world applications. Thereby, a dilemma is the "Semi-Supervised Image Captioning". To solve this problem, we propose a novel Semi-Supervised Image Captioning method considering Wasserstein Graph Matching (SSIC-WGM), which turns to adopt the raw image inputs to supervise the generated sentences. Different from traditional single modal semi-supervised methods, the difficulty of semi-supervised cross-modal learning lies in constructing intermediately
    
[^104]: 将人工智能与自然智能相融合：从统计力学到人工智能再到湍流

    Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence

    [https://arxiv.org/abs/2403.17993](https://arxiv.org/abs/2403.17993)

    人工智能对通过创新性使用深度神经网络推动湍流减少的拉格朗日模型具有重要影响，为AI和湍流研究之间紧密交织的未来铺平道路。

    

    这篇论文反思了人工智能在科学研究中的未来角色，特别关注了湍流研究，并通过根植于非平衡统计力学的扩散模型来检验人工智能的发展，强调了人工智能通过创新性地利用深度神经网络推动减少的拉格朗日湍流模型的重要影响。此外，论文审查了湍流研究中的各种其他人工智能应用，并概述了在人工智能和统计流体力学的同时发展中的潜在挑战和机会。

    arXiv:2403.17993v1 Announce Type: cross  Abstract: The paper reflects on the future role of AI in scientific research, with a special focus on turbulence studies, and examines the evolution of AI, particularly through Diffusion Models rooted in non-equilibrium statistical mechanics. It underscores the significant impact of AI on advancing reduced, Lagrangian models of turbulence through innovative use of deep neural networks. Additionally, the paper reviews various other AI applications in turbulence research and outlines potential challenges and opportunities in the concurrent advancement of AI and statistical hydrodynamics. This discussion sets the stage for a future where AI and turbulence research are intricately intertwined, leading to more profound insights and advancements in both fields.
    
[^105]: 使用多任务条件神经网络进行解释性癌细胞检测的声子显微镜

    Interpretable cancer cell detection with phonon microscopy using multi-task conditional neural networks for inter-batch calibration

    [https://arxiv.org/abs/2403.17992](https://arxiv.org/abs/2403.17992)

    提出了一个多任务条件神经网络框架，用于解释性癌细胞检测的声子显微镜，实现了批间校准和时间分辨声子信号精准细胞分类，实现了89.22%的平衡精度和0.5秒的快速分类。

    

    人工智能的进展在揭示声子显微镜（高频超声波）数据中的基本信息以识别癌细胞方面显示出巨大潜力。然而，这项技术受到“批次效应”的影响，这是由于每次实验之间无法避免的技术变化所造成的，这些变化会产生混淆变量，使人工智能模型可能会无意中学习到。因此，我们提出了一个多任务条件神经网络框架，旨在同时实现批间校准，通过消除混淆变量，并对基于时间分辨声子信号的细胞准确分类。我们通过在不同的实验批次上进行训练和验证来验证我们的方法，实现了背景、健康和癌症区域分类的平衡精度为89.22％，交叉验证平均精度为89.07％。分类可在0.5秒内完成，只需要简单的先前批次信息。

    arXiv:2403.17992v1 Announce Type: cross  Abstract: Advances in artificial intelligence (AI) show great potential in revealing underlying information from phonon microscopy (high-frequency ultrasound) data to identify cancerous cells. However, this technology suffers from the 'batch effect' that comes from unavoidable technical variations between each experiment, creating confounding variables that the AI model may inadvertently learn. We therefore present a multi-task conditional neural network framework to simultaneously achieve inter-batch calibration, by removing confounding variables, and accurate cell classification of time-resolved phonon-derived signals. We validate our approach by training and validating on different experimental batches, achieving a balanced precision of 89.22% and an average cross-validated precision of 89.07% for classifying background, healthy and cancerous regions. Classification can be performed in 0.5 seconds with only simple prior batch information requ
    
[^106]: 用于恶意软件检测中的长程预测任务的全息全局卷积网络

    Holographic Global Convolutional Networks for Long-Range Prediction Tasks in Malware Detection

    [https://arxiv.org/abs/2403.17978](https://arxiv.org/abs/2403.17978)

    提出了一种利用全息全局卷积网络（HGConv）和全息简化表示（HRR）属性的方法，不需要复杂的核计算或设计，在恶意软件检测领域取得了新的SOTA结果。

    

    恶意软件检测是一个有趣且有价值的领域，因为它对现实世界有重要影响并具有独特的机器学习挑战。本文研究现有的长程技术和基准，并发现它们在这个问题领域不太适用。我们引入了利用全息简化表示（HRR）属性来编码和解码序列元素特征的全息全局卷积网络（HGConv）。与其他全局卷积方法不同，我们的方法不需要任何复杂的核计算或精心设计的核。HGConv核被定义为通过反向传播学习的简单参数。该方法在Microsoft恶意软件分类挑战赛、Drebin和EMBER恶意软件基准测试中取得了新的SOTA结果。在序列长度的对数级复杂度下，实证结果表明HGConv的运行时间明显更快。

    arXiv:2403.17978v1 Announce Type: cross  Abstract: Malware detection is an interesting and valuable domain to work in because it has significant real-world impact and unique machine-learning challenges. We investigate existing long-range techniques and benchmarks and find that they're not very suitable in this problem area. In this paper, we introduce Holographic Global Convolutional Networks (HGConv) that utilize the properties of Holographic Reduced Representations (HRR) to encode and decode features from sequence elements. Unlike other global convolutional methods, our method does not require any intricate kernel computation or crafted kernel design. HGConv kernels are defined as simple parameters learned through backpropagation. The proposed method has achieved new SOTA results on Microsoft Malware Classification Challenge, Drebin, and EMBER malware benchmarks. With log-linear complexity in sequence length, the empirical results demonstrate substantially faster run-time by HGConv c
    
[^107]: 深度生成式领域自适应与时间注意力跨用户活动识别

    Deep Generative Domain Adaptation with Temporal Attention for Cross-User Activity Recognition

    [https://arxiv.org/abs/2403.17958](https://arxiv.org/abs/2403.17958)

    深度生成式领域自适应方法 DGDATA 独特地在领域自适应过程中识别和整合了时间关系。

    

    在人体活动识别（HAR）中，一个主要的假设是用于训练和评估目的的数据来自相同的分布。然而，在实际实现中，往往挑战这一概念，尤其是在跨用户HAR等场景中表现为数据分布差异。领域自适应是应对跨用户HAR任务中这些挑战的有效方法。然而，领域自适应技术中一个明显的缺陷是忽视了在调整数据分布阶段嵌入在时间序列数据中的时间关系。针对这一疏漏，我们的研究提出了带有时间注意力的深度生成式领域自适应（DGDATA）方法。

    arXiv:2403.17958v1 Announce Type: cross  Abstract: In Human Activity Recognition (HAR), a predominant assumption is that the data utilized for training and evaluation purposes are drawn from the same distribution. It is also assumed that all data samples are independent and identically distributed ($\displaystyle i.i.d.$). Contrarily, practical implementations often challenge this notion, manifesting data distribution discrepancies, especially in scenarios such as cross-user HAR. Domain adaptation is the promising approach to address these challenges inherent in cross-user HAR tasks. However, a clear gap in domain adaptation techniques is the neglect of the temporal relation embedded within time series data during the phase of aligning data distributions. Addressing this oversight, our research presents the Deep Generative Domain Adaptation with Temporal Attention (DGDATA) method. This novel method uniquely recognises and integrates temporal relations during the domain adaptation proce
    
[^108]: 关于现实生活和计算中的前瞻性的注解

    A Note On Lookahead In Real Life And Computing

    [https://arxiv.org/abs/2403.17942](https://arxiv.org/abs/2403.17942)

    本文旨在学习、理解和探索前瞻概念，并设计新颖的模型作为解决现实世界问题的解决方案。

    

    过去、现在和未来被认为是人类为了他们的存在和成长而定义的三个时间和逻辑概念。我们作为人类有幸能够利用我们的智慧在真实世界中的物理发生之前在头脑中执行某项活动。过去的知识、现在的冷静和未来的展望对应于现实生活以及计算的各个领域中的三个概念，分别是回顾、观察和前瞻。前瞻（LA）处理信息的未来预测和处理输入以提前生成输出。在本文中，我们的主要目标是学习、理解和探索LA概念，并设计新颖的模型作为解决现实世界问题的解决方案。我们提出了三种实践中使用的基于输入信息可用性的知名算法框架，例如脱机、在线和半在线。

    arXiv:2403.17942v1 Announce Type: cross  Abstract: Past, Present and Future are considered to be three temporal and logical concepts which are well defined by human beings for their existence and growth. We, as human beings, have the privilege of using our intelligence to mentally execute an activity before physical occurrence of the same in the real world. Knowledge of the past, aplomb of present and visualisation for the future correspond to three concepts such as look-back, look-at and look-ahead respectively in real life as well as in diversified domains of computing. Look-Ahead(LA) deals with the future prediction of information and processing of input to produce the output in advance. In this article, our main objective is to learn, understand and explore the concept of LA and design novel models as solution for real world problems. We present three well known algorithmic frameworks used in practice based on availability of input information such as offline, online and semi-onlin
    
[^109]: 伪造还是JPEG？揭示生成图像检测数据集中的常见偏见

    Fake or JPEG? Revealing Common Biases in Generated Image Detection Datasets

    [https://arxiv.org/abs/2403.17608](https://arxiv.org/abs/2403.17608)

    许多AI生成图像检测数据集存在与JPEG压缩和图像大小相关的偏见，去除这些偏见可以显著提高对JPEG压缩的稳健性并显著改变检测器的跨生成器性能。

    

    生成图像模型的广泛应用凸显了检测人造内容的迫切需求，这是打击广泛操纵和误导的关键一步。因此，许多检测器和相关数据集已经出现。然而，许多这些数据集不经意地引入了不良偏见，从而影响了检测器的效果和评估。本文强调了许多用于AI生成图像检测的数据集包含与JPEG压缩和图像大小有关的偏见。使用GenImage数据集，我们证明检测器确实从这些不受欢迎的因素中学习。此外，我们展示去除这些命名偏见会显著增加针对JPEG压缩的鲁棒性，并显著改变评估检测器的跨生成器性能。具体来说，对于ResNet50和S

    arXiv:2403.17608v1 Announce Type: cross  Abstract: The widespread adoption of generative image models has highlighted the urgent need to detect artificial content, which is a crucial step in combating widespread manipulation and misinformation. Consequently, numerous detectors and associated datasets have emerged. However, many of these datasets inadvertently introduce undesirable biases, thereby impacting the effectiveness and evaluation of detectors. In this paper, we emphasize that many datasets for AI-generated image detection contain biases related to JPEG compression and image size. Using the GenImage dataset, we demonstrate that detectors indeed learn from these undesired factors. Furthermore, we show that removing the named biases substantially increases robustness to JPEG compression and significantly alters the cross-generator performance of evaluated detectors. Specifically, it leads to more than 11 percentage points increase in cross-generator performance for ResNet50 and S
    
[^110]: 在强化学习中模仿受成本约束的行为

    Imitating Cost-Constrained Behaviors in Reinforcement Learning

    [https://arxiv.org/abs/2403.17456](https://arxiv.org/abs/2403.17456)

    该论文介绍了在强化学习中模仿受成本约束的行为的重要性，提出了模仿学习在受约束设置下的应用，并探讨了在实际领域中专家行为受限制因素影响的问题。

    

    长期以来，复杂的计划和调度问题一直通过各种优化或启发式方法来解决。最近，提出了从专家演示中学习的模仿学习作为解决这些问题的一种可行替代方法。模仿学习旨在通过观察专家的行为来学习奖励（或偏好）模型或直接行为策略。现有的模仿学习和逆向强化学习工作主要集中在无限制设置下的模仿（例如，车辆消耗的燃油量没有限制）。然而，在许多实际应用中，专家的行为不仅受奖励（或偏好）的影响，还受约束的影响。例如，自动驾驶送货车的决策不仅取决于路径偏好/奖励（根据过去的需求数据），还取决于车辆内的燃油和送达时间等约束。

    arXiv:2403.17456v1 Announce Type: cross  Abstract: Complex planning and scheduling problems have long been solved using various optimization or heuristic approaches. In recent years, imitation learning that aims to learn from expert demonstrations has been proposed as a viable alternative to solving these problems. Generally speaking, imitation learning is designed to learn either the reward (or preference) model or directly the behavioral policy by observing the behavior of an expert. Existing work in imitation learning and inverse reinforcement learning has focused on imitation primarily in unconstrained settings (e.g., no limit on fuel consumed by the vehicle). However, in many real-world domains, the behavior of an expert is governed not only by reward (or preference) but also by constraints. For instance, decisions on self-driving delivery vehicles are dependent not only on the route preferences/rewards (depending on past demand data) but also on the fuel in the vehicle and the ti
    
[^111]: MA4DIV：用于搜索结果多样化的多智能体强化学习

    MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification

    [https://arxiv.org/abs/2403.17421](https://arxiv.org/abs/2403.17421)

    引入了基于多智能体强化学习的MA4DIV方法，将搜索结果多样化建模为多个智能体之间的合作任务，直接优化多样性指标，如$\alpha$-NDCG，以实现高训练效率。

    

    搜索结果多样化（SRD）的目标是确保所选文档涵盖尽可能多的不同子主题。现有方法主要利用“贪婪选择”范式，即一次选择一个具有最高多样性分数的文档。这些方法往往效率低下，容易陷入次优状态。此外，一些其他方法旨在近似优化多样性指标，如$\alpha$-NDCG，但结果仍然不尽如人意。为了解决这些挑战，我们引入了用于搜索结果多样性的多智能体强化学习（MARL）方法，称为MA4DIV。在这种方法中，每个文档都是一个智能体，搜索结果多样化被建模为多个智能体之间的合作任务。该方法允许直接优化多样性指标，如$\alpha$-NDCG，同时实现高训练效率。我们进行了初步实验。

    arXiv:2403.17421v1 Announce Type: cross  Abstract: The objective of search result diversification (SRD) is to ensure that selected documents cover as many different subtopics as possible. Existing methods primarily utilize a paradigm of "greedy selection", i.e., selecting one document with the highest diversity score at a time. These approaches tend to be inefficient and are easily trapped in a suboptimal state. In addition, some other methods aim to approximately optimize the diversity metric, such as $\alpha$-NDCG, but the results still remain suboptimal. To address these challenges, we introduce Multi-Agent reinforcement learning (MARL) for search result DIVersity, which called MA4DIV. In this approach, each document is an agent and the search result diversification is modeled as a cooperative task among multiple agents. This approach allows for directly optimizing the diversity metrics, such as $\alpha$-NDCG, while achieving high training efficiency. We conducted preliminary experi
    
[^112]: SeSaMe：模拟自我报告的地面真实性的框架，用于心理健康感知研究

    SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies

    [https://arxiv.org/abs/2403.17219](https://arxiv.org/abs/2403.17219)

    SeSaMe框架利用大型语言模型为心理健康研究中的参与者模拟自我报告，减轻了他们的负担

    

    移动和可穿戴技术的进步使得可以被动监测一个人的心理、行为和情感健康的潜力成为可能。这些方法通常依赖于自我报告结果的长期收集，例如抑郁、压力和焦虑，以训练机器学习（ML）模型。然而，持续自我报告会给参与者带来重大负担，经常导致流失、缺失标签或不真诚的回应。在这项工作中，我们介绍了使用心理模型模拟规模分数的SeSaMe框架，以减轻数字心理健康研究中参与者的负担。通过利用预先训练的大型语言模型（LLMs），SeSaMe能够模拟参与者在心理量表上的回应。

    arXiv:2403.17219v1 Announce Type: cross  Abstract: Advances in mobile and wearable technologies have enabled the potential to passively monitor a person's mental, behavioral, and affective health. These approaches typically rely on longitudinal collection of self-reported outcomes, e.g., depression, stress, and anxiety, to train machine learning (ML) models. However, the need to continuously self-report adds a significant burden on the participants, often resulting in attrition, missing labels, or insincere responses. In this work, we introduce the Scale Scores Simulation using Mental Models (SeSaMe) framework to alleviate participants' burden in digital mental health studies. By leveraging pre-trained large language models (LLMs), SeSaMe enables the simulation of participants' responses on psychological scales. In SeSaMe, researchers can prompt LLMs with information on participants' internal behavioral dispositions, enabling LLMs to construct mental models of participants to simulate 
    
[^113]: 利用预训练语言模型进行粗调优的专题文档检索

    Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models

    [https://arxiv.org/abs/2403.16915](https://arxiv.org/abs/2403.16915)

    本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调，在专题文档检索中显著改善了效果。

    

    在信息检索系统中，利用预训练语言模型（PLM-based IR）进行微调需要学习查询表示和查询-文档关系，除了下游任务特定的学习。本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调。通过在粗调优学习查询表示和查询-文档关系，我们旨在减少微调的负担，提高下游IR任务的学习效果。我们提出了用于粗调优的查询-文档对预测（QDPP），其预测查询-文档对的适当性。评估实验显示，所提出的方法显著改善了四个专题文档检索数据集中的MRR和/或nDCG@5。此外，查询预测任务的结果表明，粗调优促进了查询表示和查询-文档关系的学习。

    arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
    
[^114]: LLMs是少样本情境低资源语言学习器

    LLMs Are Few-Shot In-Context Low-Resource Language Learners

    [https://arxiv.org/abs/2403.16512](https://arxiv.org/abs/2403.16512)

    该研究对25种低资源语言和7种相对较高资源语言上的情境学习（ICL）及其跨语言变体进行了研究，发现了在低资源语言中使用LLMs进行ICL的有效性，提出了替代方法查询对齐，并为低资源语言的ICL提供了宝贵见解。

    

    在情境学习（ICL）的支持下，大型语言模型（LLMs）可以利用短时的情境信息执行各种任务，这为缩小高资源语言和低资源语言之间的差距提供了重要途径。然而，目前只有少数研究探讨了针对低资源语言的ICL，其中大部分集中在相对高资源的语言，比如法语和西班牙语。在这项工作中，我们对25种低资源语言和7种相对较高资源语言上的ICL及其跨语言变体（X-ICL）进行了广泛研究。我们的研究不仅评估了LLMs在低资源语言中使用ICL的有效性，还发现了情境标签对齐的缺陷，并引入了更有效的替代方法：查询对齐。此外，我们为低资源语言的ICL的各个方面提供了宝贵的见解。我们的研究总结了少样本情境学习的重要性。

    arXiv:2403.16512v1 Announce Type: cross  Abstract: In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages. Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages. Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages. Our study concludes the significance of few-shot in-cont
    
[^115]: DeepMachining: 铣床机床加工误差在线预测

    DeepMachining: Online Prediction of Machining Errors of Lathe Machines

    [https://arxiv.org/abs/2403.16451](https://arxiv.org/abs/2403.16451)

    DeepMachining是一种基于深度学习的AI系统，可以在线预测车床机床加工操作的误差，通过预训练和微调模型，实现了高准确性预测，是首批使用预训练深度学习模型预测车床机床加工误差的工厂实验之一。

    

    我们描述了DeepMachining，这是一种基于深度学习的人工智能系统，用于在线预测车床加工操作的加工误差。我们基于工厂的制造数据构建并评估了DeepMachining。具体来说，我们首先对特定车床机床操作预训练深度学习模型，以学习加工状态的显著特征。然后，我们微调预训练模型以适应特定加工任务。我们展示了DeepMachining在涉及不同工件和刀具的多个任务中实现了高预测准确性。据我们所知，这项工作是使用预训练深度学习模型预测车床机床加工误差的首批工厂实验之一。

    arXiv:2403.16451v1 Announce Type: cross  Abstract: We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states. Then, we fine-tune the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.
    
[^116]: $\textit{LinkPrompt}$: 基于提示的语言模型的自然和通用对抗攻击

    $\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models

    [https://arxiv.org/abs/2403.16432](https://arxiv.org/abs/2403.16432)

    基于提示的语言模型的优化过程揭示了生成对抗提示以误导模型的见解，引发了对该范式对抗性脆弱性的担忧。

    

    Prompt-based learning 是一种新的语言模型训练范式，它将预训练语言模型（PLMs）调整到下游任务，从而在各种自然语言处理（NLP）任务中提升了性能基准。一些研究表明，通过优化搜索提示的有效性，而不是使用固定的提示模板来微调模型。这种基于提示优化过程对PLMs的学习也揭示了生成对抗提示以误导模型的见解，引发了对这一范式对抗性脆弱性的担忧。最近的研究表明，可以生成通用对抗触发器（UATs）来改变不仅目标PLMs的预测，还有对应Prompt-based Fine-tuning Models（PFMs）的预测。然而，以前作品中发现的UATs通常是无法阅读的令牌或字符。

    arXiv:2403.16432v1 Announce Type: cross  Abstract: Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters a
    
[^117]: Re2LLM: 反射式强化大型语言模型用于基于会话的推荐

    Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation

    [https://arxiv.org/abs/2403.16427](https://arxiv.org/abs/2403.16427)

    Re2LLM是为基于会话的推荐提出的反射式强化大型语言模型，引导LLMs专注于更准确推荐所需的专业知识，实现有效和高效。

    

    大型语言模型(LLMs)正日益被看作是增强基于会话推荐(SBR)的有前途的方法，其中已广泛研究了基于提示和微调的方法，以使LLMs与SBR对齐。然而，前者因缺乏任务特定反馈而难以找到引导LLMs正确推理的最佳提示，导致推荐结果不佳。尽管后者试图用领域特定知识微调LLMs，但它们面临诸如高计算成本和依赖开源骨干的限制。为解决这些问题，我们提出了一种用于SBR的反射式强化大型语言模型(Re2LLM)，引导LLMs专注于更准确推荐所需的专业知识，实现有效和高效。具体来说，我们首先设计了反射式探索模块

    arXiv:2403.16427v1 Announce Type: new  Abstract: Large Language Models (LLMs) are emerging as promising approaches to enhance session-based recommendation (SBR), where both prompt-based and fine-tuning-based methods have been widely investigated to align LLMs with SBR.   However, the former methods struggle with optimal prompts to elicit the correct reasoning of LLMs due to the lack of task-specific feedback, leading to unsatisfactory recommendations.   Although the latter methods attempt to fine-tune LLMs with domain-specific knowledge, they face limitations such as high computational costs and reliance on open-source backbones.   To address such issues, we propose a \underline{Re}flective \underline{Re}inforcement \underline{L}arge \underline{L}anguage \underline{M}odel (Re2LLM) for SBR, guiding LLMs to focus on specialized knowledge essential for more accurate recommendations effectively and efficiently.   In particular, we first design the Reflective Exploration Module to effective
    
[^118]: 用ChatGPT增强编程教育：针对Python课程中学生感知和互动的案例研究

    Enhancing Programming Education with ChatGPT: A Case Study on Student Perceptions and Interactions in a Python Course

    [https://arxiv.org/abs/2403.15472](https://arxiv.org/abs/2403.15472)

    本研究探讨了将ChatGPT整合到Python编程课程中对学习的影响，揭示了学生对ChatGPT的积极态度，并提供了关于其在增强编程教育体验中作用的见解。

    

    ChatGPT作为一种支持性工具整合到教育中，特别是在编程课程中，通过提供调试、代码生成和解释方面的帮助，解决了编程教育的独特挑战。尽管已有研究验证了ChatGPT的有效性，但其在大学级别的编程教育中的应用以及学生互动和观点的详细理解仍有限。本文探讨了在为大一学生量身定制的Python编程课程中，在八周的时间内，ChatGPT对学习的影响。通过分析来自调查、开放性问题以及学生-ChatGPT对话数据的回应，我们旨在全面了解ChatGPT的实用性，并确定学生所感知的其优势和局限性。我们的研究揭示了学生对ChatGPT普遍持肯定态度，并提供了有关其在增强编程教育体验中的作用的见解。

    arXiv:2403.15472v1 Announce Type: cross  Abstract: The integration of ChatGPT as a supportive tool in education, notably in programming courses, addresses the unique challenges of programming education by providing assistance with debugging, code generation, and explanations. Despite existing research validating ChatGPT's effectiveness, its application in university-level programming education and a detailed understanding of student interactions and perspectives remain limited. This paper explores ChatGPT's impact on learning in a Python programming course tailored for first-year students over eight weeks. By analyzing responses from surveys, open-ended questions, and student-ChatGPT dialog data, we aim to provide a comprehensive view of ChatGPT's utility and identify both its advantages and limitations as perceived by students. Our study uncovers a generally positive reception toward ChatGPT and offers insights into its role in enhancing the programming education experience. These fin
    
[^119]: 使用量子退火器解决现实世界的包裹投递路径问题

    Solving a Real-World Package Delivery Routing Problem Using Quantum Annealers

    [https://arxiv.org/abs/2403.15114](https://arxiv.org/abs/2403.15114)

    这项研究开发了一种量子-经典混合求解器 Q4RPD，旨在解决现实世界的包裹投递路径问题，避免了问题简化和技术捷径，针对包裹重量和尺寸的真实约束条件。

    

    最近几年关于量子计算和路径问题之间的研究非常多产。大部分作品围绕着经典问题，如旅行推销员问题或车辆路径问题。尽管处理这些问题具有价值，但不可否认的是它们以学术为导向的特点无法满足现实世界的要求。本研究的主要目标是提出一种解决现实情况的方法，避免问题简化或技术捷径。相反，开发了一种量子-经典混合求解器，命名为Q4RPD，考虑到一系列真实约束条件，如异构车队、优先投递和根据包裹重量和尺寸确定的容量。Q4RPD采用了D-Wave的Leap受限二次模型混合求解器。为了证明Q4RPD的应用，设计了一个实验组

    arXiv:2403.15114v1 Announce Type: cross  Abstract: Research focused on the conjunction between quantum computing and routing problems has been very prolific in recent years. Most of the works revolve around classical problems such as the Traveling Salesman Problem or the Vehicle Routing Problem. Even though working on these problems is valuable, it is also undeniable that their academic-oriented nature falls short of real-world requirements. The main objective of this research is to present a solving method for realistic instances, avoiding problem relaxations or technical shortcuts. Instead, a quantum-classical hybrid solver has been developed, coined Q4RPD, that considers a set of real constraints such as a heterogeneous fleet of vehicles, priority deliveries, and capacities characterized by two values: weight and dimensions of the packages. Q4RPD resorts to the Leap Constrained Quadratic Model Hybrid Solver of D-Wave. To demonstrate the application of Q4RPD, an experimentation compo
    
[^120]: 使用可微分仿真学习四足动作

    Learning Quadruped Locomotion Using Differentiable Simulation

    [https://arxiv.org/abs/2403.14864](https://arxiv.org/abs/2403.14864)

    本文提出了一种新的可微分仿真框架，通过将复杂的全身仿真解耦为两个单独的连续域，并与更精确的模型对齐，来克服四足动作中的不连续性挑战。

    

    最近大部分机器人运动控制的进展都是由无模型强化学习驱动的，本文探讨了可微分仿真的潜力。可微分仿真通过使用机器人模型计算低变异一阶梯度，承诺了更快的收敛速度和更稳定的训练，但到目前为止，其在四足机器人控制方面的应用仍然有限。可微分仿真面临的主要挑战在于由于接触丰富环境（如四足动作）中的不连续性，导致机器人任务的复杂优化景观。本文提出了一个新的可微分仿真框架以克服这些挑战。关键想法包括将可能由于接触而出现不连续性的复杂全身仿真解耦为两个单独的连续域。随后，我们将简化模型产生的机器人状态与更精确的不可微分模型对齐。

    arXiv:2403.14864v1 Announce Type: cross  Abstract: While most recent advancements in legged robot control have been driven by model-free reinforcement learning, we explore the potential of differentiable simulation. Differentiable simulation promises faster convergence and more stable training by computing low-variant first-order gradients using the robot model, but so far, its use for legged robot control has remained limited to simulation. The main challenge with differentiable simulation lies in the complex optimization landscape of robotic tasks due to discontinuities in contact-rich environments, e.g., quadruped locomotion. This work proposes a new, differentiable simulation framework to overcome these challenges. The key idea involves decoupling the complex whole-body simulation, which may exhibit discontinuities due to contact, into two separate continuous domains. Subsequently, we align the robot state resulting from the simplified model with a more precise, non-differentiable 
    
[^121]: 大型语言模型在心理健康领域的机会和风险

    The opportunities and risks of large language models in mental health

    [https://arxiv.org/abs/2403.14814](https://arxiv.org/abs/2403.14814)

    大型语言模型在心理健康领域有望提供新颖的解决方案，但应注意其应用可能带来的风险，并积极采取策略减轻这些风险。

    

    全球心理健康问题的发生率正在上升，人们越来越意识到现有的心理保健模式无法充分扩展以满足需求。随着大型语言模型（LLMs）的出现，人们对它们具有创造新颖、大规模解决方案以支持心理健康的承诺感到乐观。尽管它们还处于初期阶段，LLMs已被应用于与心理健康相关的任务。本综述总结了已有文献中关于利用LLMs提供心理健康教育、评估和干预的努力，并突出了每个领域中产生积极影响的关键机会。然后，我们强调了将LLMs应用于心理健康领域所伴随的风险，并鼓励采用策略来减轻这些风险。对于心理健康支持的迫切需求必须与负责任的心理健康LLMs的开发、测试和部署相平衡。特别关键的是确保心理健康...

    arXiv:2403.14814v1 Announce Type: cross  Abstract: Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this review, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. Especially critical is ensuring that mental he
    
[^122]: 具有对数据异构性的自适应的拜占庭弹性联邦学习

    Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity

    [https://arxiv.org/abs/2403.13374](https://arxiv.org/abs/2403.13374)

    通过提出新的Robust Average Gradient Algorithm（RAGA），本研究在联邦学习中解决了恶意拜占庭攻击和数据异构性的问题，实现了在非凸损失函数和异构数据集上的收敛性分析，并展示了RAGA的良好收敛性能。

    

    本文处理了在存在恶意拜占庭攻击和数据异构性的情况下的联邦学习（FL）。提出了一种新颖的鲁棒平均梯度算法（RAGA），该算法利用几何中位数进行聚合，并可以自由选择本地更新的轮数。与大多数现有的弹性方法不同，这些方法基于强凸损失函数或均匀分布的数据集进行收敛分析，我们进行了对强凸和非凸损失函数在异构数据集上的收敛分析。根据我们的理论分析，只要恶意用户数据集的比例小于一半，RAGA就可以以$\mathcal{O}({1}/{T^{2/3- \delta}})$的速度实现非凸损失函数的收敛，其中$T$为迭代次数，$\delta \in (0, 2/3)$，对于强凸损失函数则呈线性收敛。此外，稳定点或全局最优解

    arXiv:2403.13374v1 Announce Type: new  Abstract: This paper deals with federated learning (FL) in the presence of malicious Byzantine attacks and data heterogeneity. A novel Robust Average Gradient Algorithm (RAGA) is proposed, which leverages the geometric median for aggregation and can freely select the round number for local updating. Different from most existing resilient approaches, which perform convergence analysis based on strongly-convex loss function or homogeneously distributed dataset, we conduct convergence analysis for not only strongly-convex but also non-convex loss function over heterogeneous dataset. According to our theoretical analysis, as long as the fraction of dataset from malicious users is less than half, RAGA can achieve convergence at rate $\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and $\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for strongly-convex loss function. Moreover, stationary point or global optim
    
[^123]: 函数图卷积网络：一个统一的多任务和多模态学习框架，促进健康和社会关怀洞见

    Functional Graph Convolutional Networks: A unified multi-task and multi-modal learning framework to facilitate health and social-care insights

    [https://arxiv.org/abs/2403.10158](https://arxiv.org/abs/2403.10158)

    该论文提出了一个新颖的函数图卷积网络框架，结合了函数数据分析和图卷积网络，解决了数字健康和纵向研究中的多任务和多模态学习复杂性，关键创新包括任务特定嵌入组件、执行分类、回归和预测的能力，以及创建知识图进行数据解释。

    

    本文介绍了一种新颖的函数图卷积网络（funGCN）框架，将函数数据分析和图卷积网络相结合，以解决数字健康和纵向研究中的多任务和多模态学习的复杂性。随着健康解决方案对改善医疗保健和社会支持的重要性日益增长，确保各年龄段的健康生活和促进幸福感，funGCN提供了一种统一的方法来处理多个实体的多元纵向数据，并确保即使在样本量较小的情况下也具有可解释性。关键创新包括管理不同数据类型的任务特定嵌入组件、执行分类、回归和预测的能力，以及创建知识图以获取洞察性数据解释。通过模拟实验和实际数据应用验证了funGCN的有效性。

    arXiv:2403.10158v1 Announce Type: cross  Abstract: This paper introduces a novel Functional Graph Convolutional Network (funGCN) framework that combines Functional Data Analysis and Graph Convolutional Networks to address the complexities of multi-task and multi-modal learning in digital health and longitudinal studies. With the growing importance of health solutions to improve health care and social support, ensure healthy lives, and promote well-being at all ages, funGCN offers a unified approach to handle multivariate longitudinal data for multiple entities and ensures interpretability even with small sample sizes. Key innovations include task-specific embedding components that manage different data types, the ability to perform classification, regression, and forecasting, and the creation of a knowledge graph for insightful data interpretation. The efficacy of funGCN is validated through simulation experiments and a real-data application.
    
[^124]: Sabi\'a-2:一代新的葡萄牙大型语言模型

    Sabi\'a-2: A New Generation of Portuguese Large Language Models

    [https://arxiv.org/abs/2403.09887](https://arxiv.org/abs/2403.09887)

    Sabi'a-2是一代新的葡萄牙大型语言模型，其中的Sabi'a-2 Medium模型在多个考试中的表现超越了GPT-4，且在大多数考试中超过了GPT-3.5，同时专业化对模型的性能有显著影响，可在无需增大模型尺寸的情况下以比GPT-4便宜10倍的价格提供。

    

    我们介绍了Sabi'a-2，这是一族在葡萄牙文本上训练的大型语言模型。这些模型在多个考试中进行了评估，包括巴西大学的入学考试、专业认证考试以及各种学科（如会计、经济学、工程学、法律和医学）的研究生入学考试。我们的结果显示，到目前为止我们最优秀的模型Sabi'a-2 Medium，在64场考试中有23场与GPT-4的表现相匹敌或超越，并且在64场考试中有58场超过了GPT-3.5。值得注意的是，专业化对模型的性能有显著影响，无需增大模型尺寸，我们可以提供Sabi'a-2 Medium，每个记号的价格比GPT-4便宜10倍。最后，我们发现数学和编码是需要改进的关键能力。

    arXiv:2403.09887v1 Announce Type: cross  Abstract: We introduce Sabi\'a-2, a family of large language models trained on Portuguese texts. The models are evaluated on a diverse range of exams, including entry-level tests for Brazilian universities, professional certification exams, and graduate-level exams for various disciplines such as accounting, economics, engineering, law and medicine. Our results reveal that our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64 exams. Notably, specialization has a significant impact on a model's performance without the need to increase its size, allowing us to offer Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4. Finally, we identified that math and coding are key abilities that need improvement.
    
[^125]: Shapley Values 驱动的用于GenAI生成内容的公平奖励分配框架

    Shapley Values-Powered Framework for Fair Reward Split in Content Produced by GenAI

    [https://arxiv.org/abs/2403.09700](https://arxiv.org/abs/2403.09700)

    提出了一种利用Shapley Values量化艺术家对生成模型所做贡献的方法，以实现模型开发者和数据提供者之间合作的公平分配。

    

    明显的是，当前，生成模型在质量上被人类专业人士超越。然而，随着人工智能的进步，这种差距将会缩小，导致那些投入了多年时间来掌握一项技能的个体因其高昂成本而变得过时，这种成本与他们完成任务所需的时间紧密相连 -- 一项人工智能可以在几分钟或几秒钟内完成的任务。为了避免未来的社会动荡，我们必须即刻思考如何公平评估这些个体在训练生成模型中的贡献，并如何补偿他们由此而导致的收入减少或完全丧失。在这项工作中，我们提出了一种方法来构建模型开发者和数据提供者之间的合作关系。为了实现这一目标，我们利用Shapley Values来量化Stable Diffusion-v1.5模型生成的图像中艺术家的贡献，并公平地对待他们。

    arXiv:2403.09700v1 Announce Type: cross  Abstract: It is evident that, currently, generative models are surpassed in quality by human professionals. However, with the advancements in Artificial Intelligence, this gap will narrow, leading to scenarios where individuals who have dedicated years of their lives to mastering a skill become obsolete due to their high costs, which are inherently linked to the time they require to complete a task -- a task that AI could accomplish in minutes or seconds. To avoid future social upheavals, we must, even now, contemplate how to fairly assess the contributions of such individuals in training generative models and how to compensate them for the reduction or complete loss of their incomes. In this work, we propose a method to structure collaboration between model developers and data providers. To achieve this, we employ Shapley Values to quantify the contribution of artist(s) in an image generated by the Stable Diffusion-v1.5 model and to equitably a
    
[^126]: ProSwitch：知识引导的语言模型微调，生成专业和非专业风格的文本

    ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text

    [https://arxiv.org/abs/2403.09131](https://arxiv.org/abs/2403.09131)

    ProSwitch通过知识引导的指令微调，在专业和非专业风格之间生成文本，并在专业性评估和质量评估方面表现出优越性。

    

    大语言模型（LLMs）在各种语言应用中表现出有效性，包括文本摘要和可控文本生成。然而，关于它们通过微调在不同风格间切换的能力的研究仍未被充分探讨。本研究聚焦于文本专业性，并引入了一种新颖的方法，名为ProSwitch，通过知识引导的指令微调，使语言模型具备生成专业和非专业回复的能力。ProSwitch分为三个阶段：数据准备，用于收集领域知识和训练语料库；指令微调，用于优化带有多种指令格式的语言模型；全面评估，用于评估生成文本的专业性区分能力和基于参考的质量。 ProSwitch相对于通用和专门语言模型的比较分析显示了我们的方法的优越性。

    arXiv:2403.09131v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation. However, studies into their capacity of switching between styles via fine-tuning remain underexplored. This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided instruction tuning. ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; instruction tuning for optimizing language models with multiple levels of instruction formats; and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated text. Comparative analysis of ProSwitch against both general and specialized language models reveals that our appro
    
[^127]: SSM遇上视频扩散模型: 结构化状态空间下的高效视频生成

    SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces

    [https://arxiv.org/abs/2403.07711](https://arxiv.org/abs/2403.07711)

    提出了一种基于状态空间模型（SSMs）的方法，用于解决使用扩散模型生成长视频序列时注意力层内存消耗增长快、限制较大的问题

    

    鉴于图像生成通过扩散模型取得的显著成就，研究界对将这些模型扩展到视频生成表现出越来越大的兴趣。最近用于视频生成的扩散模型主要利用注意力层来提取时间特征。然而，由于注意力层的内存消耗随着序列长度的增加呈二次增长，这种限制在尝试使用扩散模型生成更长视频序列时会带来重大挑战。为了克服这一挑战，我们提出利用状态空间模型（SSMs）。由于相对于序列长度，SSMs具有线性内存消耗，最近已经引起了越来越多的关注。在实验中，我们首先通过使用UCF101这一视频生成的标准基准来评估我们基于SSM的模型。此外，为探讨SSMs在更长视频生成中的潜力，

    arXiv:2403.07711v1 Announce Type: cross  Abstract: Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, 
    
[^128]: 具有扩散净化的分离数据一致性的图像恢复

    Decoupled Data Consistency with Diffusion Purification for Image Restoration

    [https://arxiv.org/abs/2403.06054](https://arxiv.org/abs/2403.06054)

    通过分离反向过程和数据一致性步骤，提出了一种新颖的基于扩散的图像恢复求解器。

    

    最近，扩散模型作为一种强大的深度生成先验类别已经引起了人们的关注，由于其出色地建模数据分布的能力，在各种图像恢复任务中表现出色。为了解决图像恢复问题，许多现有技术通过将额外的似然梯度步骤纳入到扩散模型的反向采样过程中来实现数据一致性。然而，这些额外的梯度步骤对于实际应用中存在挑战，因为它们造成了巨大的计算开销，从而增加了推理时间。当使用加速的扩散模型采样器时，这些额外的步骤还会导致额外的困难，因为数据一致性步骤的数量受限于反向采样步骤的数量。在这项工作中，我们提出了一种新颖的基于扩散的图像恢复求解器，通过将反向过程与数据一致性步骤分离来解决这些问题。我们的方法涉及

    arXiv:2403.06054v1 Announce Type: cross  Abstract: Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involv
    
[^129]: 分布感知对数正定编码的算法硬件协同设计，用于高效的DNN推断

    Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit Encodings for Efficient DNN Inference

    [https://arxiv.org/abs/2403.05465](https://arxiv.org/abs/2403.05465)

    引入了对数正定编码（LP）和LP量化（LPQ）框架，采用基因算法寻找最优的LP参数，设计了统一的混合精度LP加速器（LPA）体系结构，可动态适应DNN参数分布，减少量化和完整精度模型之间的表示性差异。

    

    传统的深度神经网络（DNN）量化方法使用整数、定点或浮点数据类型时，往往难以在低精度下捕捉不同的DNN参数分布，通常需要大量硅开销和密集的量化感知训练。在本研究中，我们引入了对数正定（LP）编码，这是一种受到正定启发的自适应、硬件友好的数据类型，通过参数化LP位域动态适应DNN权重/激活分布。我们还开发了一种基于遗传算法的新颖框架，LP量化（LPQ），用于寻找最优的逐层LP参数，同时通过一种新颖的全局-局部对比目标减少量化和完整精度模型之间的表示性差异。此外，我们设计了一个统一的混合精度LP加速器（LPA）体系结构，包括将LP纳入计算数据通路中的处理单元（PEs）。

    arXiv:2403.05465v1 Announce Type: cross  Abstract: Traditional Deep Neural Network (DNN) quantization methods using integer, fixed-point, or floating-point data types struggle to capture diverse DNN parameter distributions at low precision, and often require large silicon overhead and intensive quantization-aware training. In this study, we introduce Logarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by posits that dynamically adapts to DNN weight/activation distributions by parameterizing LP bit fields. We also develop a novel genetic-algorithm based framework, LP Quantization (LPQ), to find optimal layer-wise LP parameters while reducing representational divergence between quantized and full-precision models through a novel global-local contrastive objective. Additionally, we design a unified mixed-precision LP accelerator (LPA) architecture comprising of processing elements (PEs) incorporating LP in the computational datapath. Our algorithm-hardware co-design
    
[^130]: NaturalSpeech 3: 利用分解编解码器和扩散模型实现零-shot语音合成

    NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models

    [https://arxiv.org/abs/2403.03100](https://arxiv.org/abs/2403.03100)

    NaturalSpeech 3利用分解设计的扩散模型实现零-shot方式生成自然语音

    

    近期大规模文本到语音（TTS）模型取得了显著进展，然而在语音质量、相似度和韵律方面仍存在不足。鉴于语音复杂地包含各种属性（例如内容、韵律、音色和声学细节），给生成带来了重大挑战，一个自然的想法是将语音因子分解为代表不同属性的各个子空间，并单独生成它们。在此基础上，我们提出了NaturalSpeech 3，这是一个具有新颖的分解扩散模型的TTS系统，可以以零-shot方式生成自然语音。具体来说，1) 我们设计了一个具有分解向量量化（FVQ）的神经编解码器，将语音波形分解为内容、韵律、音色和声学细节的子空间；2) 我们提出了一个分解扩散模型，根据其相应的提示生成每个子空间中的属性。借助这种分解设计，NaturalSpeech 3能够ef

    arXiv:2403.03100v1 Announce Type: cross  Abstract: While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can ef
    
[^131]: SoftTiger: 用于医疗工作流的临床基础模型

    SoftTiger: A Clinical Foundation Model for Healthcare Workflows

    [https://arxiv.org/abs/2403.00868](https://arxiv.org/abs/2403.00868)

    SoftTiger是一个专为医疗工作流设计的临床大型语言模型，通过处理临床笔记的结构化，实现了基本临床任务以及更复杂的下游临床任务的执行。

    

    我们发布并介绍了SoftTiger，一个专为医疗保健工作流设计的临床大型语言模型（CLaM）作为基础模型。临床笔记的叙述性和非结构化特性是医疗智能化的主要障碍。我们致力于按照国际互操作性标准将临床笔记结构化为临床数据，涉及国际患者摘要、临床印象和医疗接触三个关键子任务的数据收集和标注。然后，我们使用公开和验证的临床数据对最先进的LLM进行监督微调。训练过程中，目标模型首先能够支持基本的临床任务，如缩写扩展和时间信息提取，然后学习执行更复杂的下游临床任务，如印象和接触摘要。此外，我们解决了医疗模型中的一些建模挑战。

    arXiv:2403.00868v1 Announce Type: cross  Abstract: We release and introduce SoftTiger, a clinical large language model (CLaM) designed as a foundation model for healthcare workflows. The narrative and unstructured nature of clinical notes is a major obstacle for healthcare intelligentization. We address a critical problem of structuring clinical notes into clinical data, according to international interoperability standards. We collect and annotate data for three critical subtasks, namely, international patient summary, clinical impression and medical encounter. We then supervised fine-tuned a state-of-the-art LLM using public and credentialed clinical data. The training is orchestrated in a way that the target model can first support basic clinical tasks such as abbreviation expansion and temporal information extraction, and then learn to perform more complex downstream clinical tasks such as impression and encounter summary. Moreover, we address, several modeling challenges in the he
    
[^132]: 政治科学中的LLMs：开启视觉分析新时代

    LLMs in Political Science: Heralding a New Era of Visual Analysis

    [https://arxiv.org/abs/2403.00154](https://arxiv.org/abs/2403.00154)

    本文旨在提高使用Gemini进行政治科学图像内容分析的可行性意识，并展示Gemini在对象检测方面的高准确性。

    

    政治学家中越来越多的人开始利用图像中丰富的信息。然而，解读这些图像的挑战在于需要计算机视觉领域的专业知识和专门硬件的访问。因此，图像分析一直局限于政治科学界中的一小部分人群。由于大型语言模型（LLMs）的崛起，这种情况有可能发生变化。本文旨在提高使用Gemini进行图像内容分析的可行性意识。对688幅图像语料库进行了回顾性分析。对每幅图像从Gemini中获取内容报告，然后由作者手动评估。我们发现Gemini在执行对象检测方面非常准确，这在政治科学中是可能最常见和基本的图像分析任务。同样重要的是，我们展示了使用Gemini进行图像内容分析是简单的。

    arXiv:2403.00154v1 Announce Type: cross  Abstract: Interest is increasing among political scientists in leveraging the extensive information available in images. However, the challenge of interpreting these images lies in the need for specialized knowledge in computer vision and access to specialized hardware. As a result, image analysis has been limited to a relatively small group within the political science community. This landscape could potentially change thanks to the rise of large language models (LLMs). This paper aims to raise awareness of the feasibility of using Gemini for image content analysis. A retrospective analysis was conducted on a corpus of 688 images. Content reports were elicited from Gemini for each image and then manually evaluated by the authors. We find that Gemini is highly accurate in performing object detection, which is arguably the most common and fundamental task in image analysis for political scientists. Equally important, we show that it is easy to im
    
[^133]: 光谱遇见空间: 和谐3D形状匹配和插值

    Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation

    [https://arxiv.org/abs/2402.18920](https://arxiv.org/abs/2402.18920)

    该研究提出了一个统一的框架，结合光谱和空间域的映射，以预测3D形状之间的点对应和形状插值，相比先前方法，取得更准确、平滑的点对应结果，并且在计算上更高效。

    

    虽然3D形状匹配和插值密切相关，但它们经常被分开研究并依次应用于关联不同的3D形状，从而导致性能不佳。在这项工作中，我们提出了一个统一的框架，用于预测3D形状之间的点对应和形状插值。为此，我们将深度功能映射框架与经典表面变形模型结合起来，以在光谱和空间域中映射形状。一方面，通过整合空间映射，我们的方法相对于先前用于形状匹配的功能映射方法获得更精确和平滑的点对应。另一方面，通过引入光谱映射，我们的方法摆脱了通常使用但计算昂贵的仅对近等距形状变形有效的测地距离约束。

    arXiv:2402.18920v1 Announce Type: cross  Abstract: Although 3D shape matching and interpolation are highly interrelated, they are often studied separately and applied sequentially to relate different 3D shapes, thus resulting in sub-optimal performance. In this work we present a unified framework to predict both point-wise correspondences and shape interpolation between 3D shapes. To this end, we combine the deep functional map framework with classical surface deformation models to map shapes in both spectral and spatial domains. On the one hand, by incorporating spatial maps, our method obtains more accurate and smooth point-wise correspondences compared to previous functional map methods for shape matching. On the other hand, by introducing spectral maps, our method gets rid of commonly used but computationally expensive geodesic distance constraints that are only valid for near-isometric shape deformations. Furthermore, we propose a novel test-time adaptation scheme to capture both 
    
[^134]: Agent-Pro: 通过策略级别反思和优化学习进化

    Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization

    [https://arxiv.org/abs/2402.17574](https://arxiv.org/abs/2402.17574)

    Agent-Pro提出了一种基于LLM的代理，通过策略级别的反思和优化，可以从互动经验中学习并逐步提升其行为策略。

    

    大型语言模型表现出在各种任务中具有强大问题解决能力。然而，大多数基于LLM的代理都是特定任务求解器，并具有复杂的提示工程，而不是能够通过互动学习和进化的代理。本文提出了Agent-Pro：一种基于LLM的代理，具有策略级别的反思和优化，可以从互动经验中学习丰富的专业知识，并逐渐提升其行为策略。具体来说，它涉及一个动态信念生成和反思过程，用于策略演化。

    arXiv:2402.17574v1 Announce Type: new  Abstract: Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover
    
[^135]: 探究多模态大型语言模型对全局和局部语义表示的影响

    Probing Multimodal Large Language Models for Global and Local Semantic Representation

    [https://arxiv.org/abs/2402.17304](https://arxiv.org/abs/2402.17304)

    通过研究发现，多模态大型语言模型的中间层能够更好地编码全局语义信息，在视觉-语言任务中表现出更好的性能。顶层可能过多关注局部信息，导致理解全局信息的能力下降。

    

    大型语言模型的成功启发了研究人员将其优秀的表示能力转移到其他模态。最近的一些研究利用图像描述对齐数据集训练多模态大型语言模型（MLLMs），在图像到文本任务中取得了最新的性能表现。然而，很少有研究探讨MLLMs是否真正理解完整的图像信息，即全局信息，或者它们只能捕捉一些局部对象信息。本研究发现模型的中间层可以编码更多全局语义信息，其表示向量在视觉-语言蕴涵任务上表现更好，而不是顶层。我们通过目标检测任务进一步探究模型的局部语义表示。我们得出的结论是顶层可能过多专注于局部信息，导致减弱了对全局信息的理解能力。

    arXiv:2402.17304v1 Announce Type: cross  Abstract: The success of large language models has inspired researchers to transfer their exceptional representing ability to other modalities. Several recent works leverage image-caption alignment datasets to train multimodal large language models (MLLMs), which achieve state-of-the-art performance on image-to-text tasks. However, there are very few studies exploring whether MLLMs truly understand the complete image information, i.e., global information, or if they can only capture some local object information. In this study, we find that the intermediate layers of models can encode more global semantic information, whose representation vectors perform better on visual-language entailment tasks, rather than the topmost layers. We further probe models for local semantic representation through object detection tasks. And we draw a conclusion that the topmost layers may excessively focus on local information, leading to a diminished ability to en
    
[^136]: 在跳槽之前三思：问题细化提示改善大型语言模型的数学推理能力

    Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models

    [https://arxiv.org/abs/2402.15764](https://arxiv.org/abs/2402.15764)

    PEP提出了一个新方法来改善LLMs的数学能力，通过在推理之前细化和阐明问题背景，提升全局上下文建模能力，减少解析困难。

    

    大型语言模型（LLMs）在自然语言处理任务中表现出色，但在复杂推理任务中仍面临挑战，并且对输入上下文敏感。本研究提出了一种新的方法，名为问题细化提示（PEP），旨在在推理之前分解和阐明问题背景，从而增强全局上下文建模和减少解析困难。实验结果表明，PEP在复杂推理任务上表现出色，对于问题提出的效果显著。

    arXiv:2402.15764v1 Announce Type: cross  Abstract: Large language models~(LLMs) have exhibited impressive performance across NLP tasks. So far they still face challenges in complex reasoning tasks and can be sensitive to input context. Despite significant efforts have been invested in enhancing reasoning process and improving prefix-prompts robustness, the crucial role of problem context has been overlooked. In this study, we propose a new approach to improve the mathematical capacities of LLMs, named Problem Elaboration Prompting~(PEP). Specifically, PEP decomposes and elucidates the problem context before reasoning, thus enhancing the global context modeling and reducing the parsing difficulties. Experiments on datasets demonstrate promising performances on complex reasoning and indicate the beneficial impact for ill-formed problems. For instance, with the GPT-3.5 model~(\texttt{text-davinci-003}), we observed a 9.93\% improvement with greedy decoding and 8.80\% improvement with self
    
[^137]: 结构引导的大型语言模型用于SQL生成

    Structure Guided Large Language Model for SQL Generation

    [https://arxiv.org/abs/2402.13284](https://arxiv.org/abs/2402.13284)

    通过引入结构信息，提出了一个结构引导的SQL生成模型，以改善大型语言模型生成SQL的准确性和可执行性。

    

    生成准确的结构化查询语言（SQL）是一个长期存在的问题，特别是在将用户的语义查询与结构化数据库匹配，然后生成结构化SQL方面。现有模型通常将查询和数据库模式输入到LLM中，并依赖LLM执行语义-结构匹配并生成结构化SQL。然而，这种解决方案忽略了用户查询和数据库中的结构信息，而这些信息可以用来增强结构化SQL的生成。这一疏忽可能导致不准确或无法执行的SQL生成。为了充分利用结构，我们提出了一个结构到SQL的框架，利用固有的结构信息来改善LLM的SQL生成。具体地，我们介绍了我们的结构引导SQL（SGU-SQL）生成模型。

    arXiv:2402.13284v1 Announce Type: cross  Abstract: Generating accurate Structured Querying Language (SQL) is a long-standing problem, especially in matching users' semantic queries with structured databases and then generating structured SQL. Existing models typically input queries and database schemas into the LLM and rely on the LLM to perform semantic-structure matching and generate structured SQL. However, such solutions overlook the structural information within user queries and databases, which can be utilized to enhance the generation of structured SQL. This oversight can lead to inaccurate or unexecutable SQL generation. To fully exploit the structure, we propose a structure-to-SQL framework, which leverages the inherent structure information to improve the SQL generation of LLMs. Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model. SGU-SQL first links user queries and databases in a structure-enhanced manner. It then decomposes complicated linked str
    
[^138]: 具有延迟更新的随机逼近：马尔科夫采样下的有限时间速率

    Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling

    [https://arxiv.org/abs/2402.11800](https://arxiv.org/abs/2402.11800)

    延迟更新的随机逼近方案在时间变化有界延迟下，保证了每次迭代快速收敛到固定点周围的球体，界限依赖于最大延迟和混合时间。

    

    受大规模和多智能体强化学习应用的启发，我们研究了在马尔科夫采样下具有延迟更新的随机逼近（SA）方案的非渐近性能。虽然延迟的影响在优化中得到了广泛研究，但它们与底层马尔科夫过程相互作用以塑造SA的有限时间性能的方式仍然不太清楚。在这个背景下，我们的第一个主要贡献是证明在时间变化有界延迟下，延迟的SA更新规则确保最后迭代收敛到SA运算符固定点周围的球体具有指数快速的速度。值得注意的是，我们的界限在依赖于最大延迟$\tau_{max}$和混合时间$\tau_{mix}$方面是\emph{紧致的}。为了实现这一紧密界限，我们开发了一种新颖的归纳证明技术，与各种现有延迟优化分析不同，它依赖于建立未...

    arXiv:2402.11800v1 Announce Type: cross  Abstract: Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling. While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood. In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \emph{last iterate} to a ball around the SA operator's fixed point. Notably, our bound is \emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing un
    
[^139]: GPT-4在基于USMLE的案例研究中的表现评估

    GPT-4's assessment of its performance in a USMLE-based case study

    [https://arxiv.org/abs/2402.09654](https://arxiv.org/abs/2402.09654)

    本研究探讨了GPT-4在医疗应用中的表现评估。实验结果表明，反馈对相对置信度有影响，但并不一致地增加或减少。

    

    本研究调查GPT-4在医疗应用中的表现评估。通过使用简单的提示技术，从美国医学执照考试（USMLE）问卷中提取问题的方式，任务是评估模型在提问之前和提问之后的置信度得分。问卷根据是否有反馈分为两组：反馈组（WF）和无反馈组（NF）。要求模型在每个问题之前和之后提供绝对和相对置信度得分。通过使用统计工具分析实验结果，研究了WF和NF组的置信度变异性。此外，进行了顺序分析以观察WF和NF组的性能变化。结果表明，反馈会影响相对置信度，但并不总是增加或减少。

    arXiv:2402.09654v1 Announce Type: new  Abstract: This study investigates GPT-4's assessment of its performance in healthcare applications. A simple prompting technique was used to prompt the LLM with questions taken from the United States Medical Licensing Examination (USMLE) questionnaire and it was tasked to evaluate its confidence score before posing the question and after asking the question. The questionnaire was categorized into two groups-questions with feedback (WF) and questions with no feedback(NF) post-question. The model was asked to provide absolute and relative confidence scores before and after each question. The experimental findings were analyzed using statistical tools to study the variability of confidence in WF and NF groups. Additionally, a sequential analysis was conducted to observe the performance variation for the WF and NF groups. Results indicate that feedback influences relative confidence but doesn't consistently increase or decrease it. Understanding the p
    
[^140]: 攻击、防御和评估LLM对话安全性的调查

    Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey

    [https://arxiv.org/abs/2402.09283](https://arxiv.org/abs/2402.09283)

    这篇调查提供了LLM对话安全性的全面概述，涵盖了攻击、防御和评估三个关键方面，旨在提高对该主题的理解并促进进一步的研究。

    

    arXiv:2402.09283v1 公告类型: 新的摘要: 大型语言模型（LLMs）在对话应用中已经很常见。然而，它们可能被误用生成有害回复的风险引起了严重的社会关切，并激发了LLM对话安全性的最新研究。因此，在此调查中，我们提供了最近研究的全面概述，涵盖了LLM对话安全性的三个关键方面：攻击、防御和评估。我们的目标是提供一个结构化的摘要，增进对LLM对话安全性的理解，并鼓励进一步研究这一重要课题。为了方便参考，我们根据我们的分类法对所有在此调查中提到的研究进行了分类，可在以下网址找到：https://github.com/niconi19/LLM-conversation-safety。

    arXiv:2402.09283v1 Announce Type: new Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.
    
[^141]: OpenMoE：开源混合专家语言模型的早期努力

    OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models

    [https://arxiv.org/abs/2402.01739](https://arxiv.org/abs/2402.01739)

    OpenMoE是一种开源的混合专家语言模型，通过训练和发布一系列具有可复现性的解码器模型，我们确认了MoE模型相比密集模型具有更有利的成本效益平衡，并且进行了对路由机制的深入分析，得出了三个重要发现。

    

    为了帮助开源社区更好地理解基于混合专家(MoE)的大型语言模型(LLM)，我们训练并发布了OpenMoE，一系列完全开放源码和可复现的仅解码器MoE LLM，参数范围从650M到34B，训练数据超过1T个标记。我们的研究证实，MoE-based LLM可以提供比密集LLM更有利的成本效益平衡，突出了未来LLM开发的潜在有效性。本研究的另一个重要贡献是对我们的OpenMoE模型中的路由机制进行深入分析，得到了三个重要发现：上下文无关专业化、早期路由学习和末尾降低。我们发现，MoE模型中的路由决策主要基于标记ID，与上下文相关性很小。标记到专家的分配在预训练阶段早期确定，并且基本保持不变。这种不完全的路由可能导致...

    To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development.   One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can resu
    
[^142]: 无需动作的行为学习

    Learning to Act without Actions

    [https://arxiv.org/abs/2312.10812](https://arxiv.org/abs/2312.10812)

    通过从视频中恢复潜在动作信息，LAPO能够训练可以迅速微调为专家级策略的潜在动作策略。

    

    在大规模网络数据上进行预训练已被证明是获取强大通用模型的有效方法，例如在语言和视觉领域。但是，这种范式尚未在强化学习中得以推广。我们介绍了Latent Action Policies（LAPO），这是一种从视频中纯粹恢复潜在动作信息的方法，从而产生潜在动作策略、世界模型和逆动力学模型。LAPO是第一个能够仅通过观察到的动态从视频中恢复真实动作空间结构的方法，即使在具有挑战性的过程生成环境中也是如此。

    arXiv:2312.10812v2 Announce Type: replace-cross  Abstract: Pre-training large models on vast amounts of web data has proven to be an effective approach for obtaining powerful, general models in domains such as language and vision. However, this paradigm has not yet taken hold in reinforcement learning. This is because videos, the most abundant form of embodied behavioral data on the web, lack the action labels required by existing methods for imitating behavior from demonstrations. We introduce Latent Action Policies (LAPO), a method for recovering latent action information, and thereby latent-action policies, world models, and inverse dynamics models, purely from videos. LAPO is the first method able to recover the structure of the true action space just from observed dynamics, even in challenging procedurally-generated environments. LAPO enables training latent-action policies that can be rapidly fine-tuned into expert-level policies, either offline using a small action-labeled datas
    
[^143]: 通过策略引导的轨迹扩散实现世界模型

    World Models via Policy-Guided Trajectory Diffusion

    [https://arxiv.org/abs/2312.08533](https://arxiv.org/abs/2312.08533)

    这项工作提出了一个新颖的世界建模方法，Policy-Guided Trajectory Diffusion (PolyGRAD)，通过扩散模型一次生成整个在线策略轨迹，避免了自回归模型中随着轨迹长度增长而积累的预测误差。

    

    世界模型是开发智能agent的强大工具。通过预测一系列行动的结果，世界模型使得可以通过在“想象中”使用合成数据来优化策略，即通过在线策略增强学习（RL）来实现。现有的世界模型是自回归的，因为它们在预测下一个状态的同时从策略中采样下一个行动。随着轨迹长度的增长，预测误差必然会累积。在这项工作中，我们提出了一种新颖的世界建模方法，不是自回归的，而是通过扩散模型一次生成整个在线策略轨迹。我们的方法，Policy-Guided Trajectory Diffusion (PolyGRAD)，利用了除了策略的动作分布梯度之外的一个去噪模型，将最初随机状态和动作的轨迹扩散成一个在线合成轨迹。我们分析了PolyGRAD与

    arXiv:2312.08533v3 Announce Type: replace-cross  Abstract: World models are a powerful tool for developing intelligent agents. By predicting the outcome of a sequence of actions, world models enable policies to be optimised via on-policy reinforcement learning (RL) using synthetic data, i.e. in "in imagination". Existing world models are autoregressive in that they interleave predicting the next state with sampling the next action from the policy. Prediction error inevitably compounds as the trajectory length grows. In this work, we propose a novel world modelling approach that is not autoregressive and generates entire on-policy trajectories in a single pass through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory. We analyse the connections between PolyGRAD,
    
[^144]: FoundationPose: 统一的新物体6D姿势估计和跟踪

    FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects

    [https://arxiv.org/abs/2312.08344](https://arxiv.org/abs/2312.08344)

    提出了FoundationPose，一个统一的基础模型，支持新物体的6D姿势估计和跟踪，通过神经隐式表示和大规模训练实现了强大的泛化能力。

    

    我们提出了FoundationPose，这是一个统一的基础模型，用于6D物体姿势估计和跟踪，支持基于模型和无模型的设置。我们的方法可以在测试时立即应用于新物体，无需微调，只要给出其CAD模型，或者捕获少量参考图像。我们通过神经隐式表示来弥合这两种设置之间的差距，该表示允许有效的新视图合成，并使下游姿势估计模块在相同统一框架下保持不变。通过大规模合成训练、大型语言模型（LLM）、一种新颖的基于transformer的架构以及对比学习公式，我们实现了强大的泛化能力。在涉及挑战性场景和物体的多个公共数据集上进行的广泛评估表明，我们的统一方法在很大程度上优于专门针对每个任务的现有方法。

    arXiv:2312.08344v2 Announce Type: replace-cross  Abstract: We present FoundationPose, a unified foundation model for 6D object pose estimation and tracking, supporting both model-based and model-free setups. Our approach can be instantly applied at test-time to a novel object without fine-tuning, as long as its CAD model is given, or a small number of reference images are captured. We bridge the gap between these two setups with a neural implicit representation that allows for effective novel view synthesis, keeping the downstream pose estimation modules invariant under the same unified framework. Strong generalizability is achieved via large-scale synthetic training, aided by a large language model (LLM), a novel transformer-based architecture, and contrastive learning formulation. Extensive evaluation on multiple public datasets involving challenging scenarios and objects indicate our unified approach outperforms existing methods specialized for each task by a large margin. In additi
    
[^145]: 基于批处理的基础模型低秩调整

    Batched Low-Rank Adaptation of Foundation Models

    [https://arxiv.org/abs/2312.05677](https://arxiv.org/abs/2312.05677)

    提出了Fast LoRA（FLoRA）框架，使得基础模型的低秩调整可以高效批处理异构请求，并在绩效上保持竞争性。

    

    最近，低秩适应（LoRA）因通过引入可训练的低秩矩阵微调基础模型并减少可训练参数的数量而引起关注。虽然LoRA提供了许多优点，但其在实时为各种全球用户提供服务时无法高效处理多个特定任务适配器的能力受到限制。这为需要为每个传入请求个性化、特定任务适应的场景中造成了性能瓶颈。为了减轻这一约束，我们提出了快速LoRA（FLoRA）框架，其中批处理中的每个输入示例都可以与其独特的低秩适应权重相关联，从而实现对异构请求的高效批处理。我们通过实证表明，FLoRA保留了LoRA的绩效优点，在跨越8种语言的MultiPL-E代码生成基准测试上展示出竞争结果。

    arXiv:2312.05677v2 Announce Type: replace-cross  Abstract: Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While LoRA offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request. To mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that FLoRA retains the performance merits of LoRA, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and 
    
[^146]: SplaTAM: 用于密集RGB-D SLAM的Splat、追踪和映射3D高斯函数

    SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM

    [https://arxiv.org/abs/2312.02126](https://arxiv.org/abs/2312.02126)

    该方法提出了SplaTAM，通过利用3D高斯函数的显式体积表示，实现了从单个未定位RGB-D相机进行高保真重建，超越了现有方法的能力

    

    密集的同时定位和地图构建（SLAM）对于机器人和增强现实应用至关重要。然而，当前的方法往往受到它们表示场景的非体积或隐式方式的阻碍。本文介绍了SplaTAM，一种首次利用显式体积表示（即3D高斯函数）的方法，从而能够通过单个未定位RGB-D相机实现高保真重建，超越现有方法的能力。SplaTAM采用了一种简单的在线追踪和映射系统，专为底层高斯函数表示进行了调整。它利用剪影掩膜优雅地捕捉场景密度的存在。该组合相对于先前的表示提供了多个优点，包括快速渲染和密集优化、快速确定区域是否已被映射以及通过添加更多高斯函数进行结构化地图扩展。大量实验表明SplaTAM...

    arXiv:2312.02126v2 Announce Type: replace-cross  Abstract: Dense simultaneous localization and mapping (SLAM) is crucial for robotics and augmented reality applications. However, current methods are often hampered by the non-volumetric or implicit way they represent a scene. This work introduces SplaTAM, an approach that, for the first time, leverages explicit volumetric representations, i.e., 3D Gaussians, to enable high-fidelity reconstruction from a single unposed RGB-D camera, surpassing the capabilities of existing methods. SplaTAM employs a simple online tracking and mapping system tailored to the underlying Gaussian representation. It utilizes a silhouette mask to elegantly capture the presence of scene density. This combination enables several benefits over prior representations, including fast rendering and dense optimization, quickly determining if areas have been previously mapped, and structured map expansion by adding more Gaussians. Extensive experiments show that SplaTAM
    
[^147]: 高效基于Transformer的3D人体姿势估计的Hourglass Tokenizer

    Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation

    [https://arxiv.org/abs/2311.12028](https://arxiv.org/abs/2311.12028)

    提出了一种名为Hourglass Tokenizer（HoT）的修剪和恢复框架，用于从视频中高效地基于Transformer进行3D人体姿势估计，通过动态选择具有高语义多样性的代表性标记并消除视频帧的冗余，最终提高了模型的效率。

    

    Transformers已经成功应用于基于视频的3D人体姿势估计领域。然而，这些视频姿势Transformer（VPTs）的高计算成本使得它们在资源受限设备上不切实际。在本文中，我们提出了一种名为Hourglass Tokenizer（HoT）的即插即用的修剪和恢复框架，用于从视频中高效地基于Transformer进行3D人体姿势估计。我们的HoT首先通过修剪冗余帧的姿势标记开始，然后以恢复全长度标记结束，从而在中间的Transformer块中产生少量姿势标记，从而提高模型的效率。为了有效实现这一目标，我们提出了一个标记修剪集群（TPC），动态选择一些具有高语义多样性的代表性标记，同时消除视频帧的冗余。此外，我们开发了一个标记恢复注意力（TRA）来恢复详细的时空信息。

    arXiv:2311.12028v2 Announce Type: replace-cross  Abstract: Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a plug-and-play pruning-and-recovering framework, called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose estimation from videos. Our HoT begins with pruning pose tokens of redundant frames and ends with recovering full-length tokens, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. To effectively achieve this, we propose a token pruning cluster (TPC) that dynamically selects a few representative tokens with high semantic diversity while eliminating the redundancy of video frames. In addition, we develop a token recovering attention (TRA) to restore the detailed spatio-temporal information b
    
[^148]: 提升布局到图像合成中的对象连贯性

    Enhancing Object Coherence in Layout-to-Image Synthesis

    [https://arxiv.org/abs/2311.10522](https://arxiv.org/abs/2311.10522)

    本文提出了一种新颖的扩散模型，结合全局语义融合和自相似特征增强模块，以引导布局到图像合成中的对象连贯性。

    

    布局到图像合成是一种新兴的条件图像生成技术。它旨在生成复杂场景，用户可以对场景中的对象布局进行精细控制。然而，控制对象连贯性，包括语义连贯性（例如，猫是否看向花朵）和物理连贯性（例如，手和球拍不应错位）仍然具有挑战性。在本文中，我们提出了一种新颖的扩散模型，配备了有效的全局语义融合（GSF）和自相似特征增强模块，以引导该任务的对象连贯性。对于语义连贯性，我们认为图像标题包含丰富信息，可以定义图像中对象之间的语义关系。与其简单地使用标题和生成图像之间的跨注意力，我们提出了一种能同时处理高度相关布局限制和语义连贯性的方法，从而使

    arXiv:2311.10522v4 Announce Type: replace-cross  Abstract: Layout-to-image synthesis is an emerging technique in conditional image generation. It aims to generate complex scenes, where users require fine control over the layout of the objects in a scene. However, it remains challenging to control the object coherence, including semantic coherence (e.g., the cat looks at the flowers or not) and physical coherence (e.g., the hand and the racket should not be misaligned). In this paper, we propose a novel diffusion model with effective global semantic fusion (GSF) and self-similarity feature enhancement modules to guide the object coherence for this task. For semantic coherence, we argue that the image caption contains rich information for defining the semantic relationship within the objects in the images. Instead of simply employing cross-attention between captions and generated images, which addresses the highly relevant layout restriction and semantic coherence separately and thus lea
    
[^149]: 转向机器监督：用于自动医学图像分割和分类的标注高效的半监督和自监督学习

    Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification

    [https://arxiv.org/abs/2311.10319](https://arxiv.org/abs/2311.10319)

    引入了S4MI流程，利用自监督和半监督学习的高效方法，能够简化医学图像的机器监督过程，自监督学习在分类任务中表现明显优于监督方法。

    

    临床治疗的进展越来越受到监督学习技术的限制，这些技术严重依赖于大量标注数据。标注过程不仅成本高昂，而且需要临床专家大量时间。为了解决这一问题，我们引入了S4MI（医学图像的自监督和半监督）流程，这是一种利用自监督和半监督学习的发展的新方法。这些技术参与不需要标记的辅助任务，从而简化了机器监督的扩展，相比完全监督的方法。我们的研究在三个不同的医学图像数据集上对这些技术进行基准测试，以评估它们在分类和分割任务中的有效性。值得注意的是，我们发现自监督学习在分类任务中明显优于监督方法。

    arXiv:2311.10319v3 Announce Type: replace-cross  Abstract: Advancements in clinical treatment are increasingly constrained by the limitations of supervised learning techniques, which depend heavily on large volumes of annotated data. The annotation process is not only costly but also demands substantial time from clinical specialists. Addressing this issue, we introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging) pipeline, a novel approach that leverages the advancements in self-supervised and semi-supervised learning. These techniques engage in auxiliary tasks that do not require labeling, thus simplifying the scaling of machine supervision compared to fully-supervised methods. Our study benchmarks these techniques on three distinct medical imaging datasets to evaluate their effectiveness in classification and segmentation tasks. Notably, we observed that self-supervised learning significantly surpassed the performance of supervised methods in the classificati
    
[^150]: LLatrieval：LLM-验证检索用于可验证生成

    LLatrieval: LLM-Verified Retrieval for Verifiable Generation

    [https://arxiv.org/abs/2311.07838](https://arxiv.org/abs/2311.07838)

    可验证生成中检索的文件不仅帮助LLM生成正确答案，还作为用户验证LLM输出的证据，但目前广泛使用的检索器已成为性能瓶颈，需要解决。

    

    可验证生成旨在使大型语言模型（LLM）生成具有支撑文件的文本，这使用户能够灵活验证答案，并使LLM的输出更可靠。检索在可验证生成中起着至关重要的作用。具体而言，检索到的文件不仅补充知识以帮助LLM生成正确答案，还作为支持用户验证LLM输出的证据。然而，广泛使用的检索器成为整个流程的瓶颈，并限制了整体性能。由于通常具有的参数比大型语言模型少得多，并且尚未证明能够良好地扩展到LLM的规模，因此它们的能力通常比LLMs差。如果检索器未能正确找到支持文件，则LLM将无法生成正确和可验证的答案，这会掩盖LLM的显著能力。为解决这些问

    arXiv:2311.07838v2 Announce Type: replace  Abstract: Verifiable generation aims to let the large language model (LLM) generate text with supporting documents, which enables the user to flexibly verify the answer and makes the LLM's output more reliable. Retrieval plays a crucial role in verifiable generation. Specifically, the retrieved documents not only supplement knowledge to help the LLM generate correct answers, but also serve as supporting evidence for the user to verify the LLM's output. However, the widely used retrievers become the bottleneck of the entire pipeline and limit the overall performance. Their capabilities are usually inferior to LLMs since they often have much fewer parameters than the large language model and have not been demonstrated to scale well to the size of LLMs. If the retriever does not correctly find the supporting documents, the LLM can not generate the correct and verifiable answer, which overshadows the LLM's remarkable abilities. To address these li
    
[^151]: 在多任务学习中挑战常见范式

    Challenging Common Paradigms in Multi-Task Learning

    [https://arxiv.org/abs/2311.04698](https://arxiv.org/abs/2311.04698)

    我们挑战了多任务学习中的常见范式，通过研究在单任务学习中的影响，揭示了优化器选择在MTL中的关键作用，并理论推导出了梯度冲突的角色。

    

    尽管近年来多任务学习（MTL）受到了极大关注，但其基本机制仍然知之甚少。最近的方法并未带来一致的性能改进，相比单任务学习（STL）基线，强调了更深入了解MTL特定挑战的重要性。在我们的研究中，我们挑战了MTL中的范式，提出了几点关于STL的重要影响：首先，优化器的选择对MTL的影响只受到了轻微的调查。我们通过各种实验的实证方法展示了常见STL工具（例如Adam优化器）在MTL中的关键作用。为了进一步研究Adam的有效性，我们在一定的假设下从理论上推导出部分损失尺度不变性。其次，梯度冲突的概念经常被描述为MTL中的一个特定问题。我们深入探讨了梯度冲突在MTL中的作用，并将其与STL进行比较。在角度梯度对齐方面，我们没有找到

    arXiv:2311.04698v3 Announce Type: replace-cross  Abstract: While multi-task learning (MTL) has gained significant attention in recent years, its underlying mechanisms remain poorly understood. Recent methods did not yield consistent performance improvements over single task learning (STL) baselines, underscoring the importance of gaining more profound insights about challenges specific to MTL. In our study, we challenge paradigms in MTL in the context of STL: First, the impact of the choice of optimizer has only been mildly investigated in MTL. We show the pivotal role of common STL tools such as the Adam optimizer in MTL empirically in various experiments. To further investigate Adam's effectiveness, we theoretical derive a partial loss-scale invariance under mild assumptions. Second, the notion of gradient conflicts has often been phrased as a specific problem in MTL. We delve into the role of gradient conflicts in MTL and compare it to STL. For angular gradient alignment we find no 
    
[^152]: 联邦学习中的近似和加权数据重构攻击

    Approximate and Weighted Data Reconstruction Attack in Federated Learning

    [https://arxiv.org/abs/2308.06822](https://arxiv.org/abs/2308.06822)

    提出了一种基于插值的近似方法和层次加权损失函数，用于攻击FedAvg场景中的数据重构攻击。

    

    联邦学习（FL）是一种分布式学习范例，使多个客户端能够在不共享私人数据的情况下合作构建机器学习模型。虽然FL被认为是通过设计保护隐私的，但最近的数据重构攻击表明，攻击者可以基于在FL中共享的参数恢复客户端的训练数据。然而，大多数现有方法未能攻击最广泛使用的水平联邦平均（FedAvg）场景，在此场景中，客户端在多个局部训练步骤之后共享模型参数。为了解决这个问题，我们提出了一种基于插值的近似方法，通过生成客户端局部训练过程的中间模型更新，使攻击FedAvg场景变得可行。然后，我们设计了一种层次加权损失函数来改善重构的数据质量。我们为不同层次的模型更新分配不同的权重

    arXiv:2308.06822v2 Announce Type: replace-cross  Abstract: Federated Learning (FL) is a distributed learning paradigm that enables multiple clients to collaborate on building a machine learning model without sharing their private data. Although FL is considered privacy-preserved by design, recent data reconstruction attacks demonstrate that an attacker can recover clients' training data based on the parameters shared in FL. However, most existing methods fail to attack the most widely used horizontal Federated Averaging (FedAvg) scenario, where clients share model parameters after multiple local training steps. To tackle this issue, we propose an interpolation-based approximation method, which makes attacking FedAvg scenarios feasible by generating the intermediate model updates of the clients' local training processes. Then, we design a layer-wise weighted loss function to improve the data quality of reconstruction. We assign different weights to model updates in different layers conc
    
[^153]: 非线性控制分配：基于学习的方法

    Nonlinear Control Allocation: A Learning Based Approach

    [https://arxiv.org/abs/2201.06180](https://arxiv.org/abs/2201.06180)

    本研究提出了一种基于人工神经网络（ANN）的非线性控制分配方案，在学习控制效能映射的逆过程后，将其作为分配器实施，避免了在线优化问题的计算资源需求。

    

    现代飞机设计具有冗余控制执行器，以满足容错和机动性要求。这导致飞机过度作用，并需要控制分配方案在控制执行器之间分配控制指令。传统上，基于优化的控制分配方案被使用；然而，对于非线性分配问题，这些方法需要大量计算资源。在这项工作中，提出了一种基于人工神经网络（ANN）的非线性控制分配方案。该方案由通过ANN学习控制效能映射的逆过程组成，然后将其作为分配器实施，而不是解决在线优化问题。提出了包含分配器的闭环系统的稳定性条件，并通过分段线性效能函数和基于ANN的分配器探讨了计算挑战。

    arXiv:2201.06180v2 Announce Type: replace-cross  Abstract: Modern aircraft are designed with redundant control effectors to cater for fault tolerance and maneuverability requirements. This leads to aircraft being over-actuated and requires control allocation schemes to distribute the control commands among control effectors. Traditionally, optimization-based control allocation schemes are used; however, for nonlinear allocation problems, these methods require large computational resources. In this work, an artificial neural network (ANN) based nonlinear control allocation scheme is proposed. The proposed scheme is composed of learning the inverse of the control effectiveness map through ANN, and then implementing it as an allocator instead of solving an online optimization problem. Stability conditions are presented for closed-loop systems incorporating the allocator, and computational challenges are explored with piece-wise linear effectiveness functions and ANN-based allocators. To d
    
[^154]: 基于环境的上下文驱动的自我监督视觉学习：利用环境作为数据源

    Context-driven self-supervised visual learning: Harnessing the environment as a data source. (arXiv:2401.15120v1 [cs.CV])

    [http://arxiv.org/abs/2401.15120](http://arxiv.org/abs/2401.15120)

    这项研究提出了一种基于环境的上下文驱动的自我监督视觉学习方法，通过利用环境的历史空间上下文提供的相似性信号进行对比学习，并展示了在模拟环境中的优越性能，尤其在陌生环境中。该方法有潜力为代理在具有独特视觉特征的新环境中实现快速的视觉学习。

    

    视觉学习通常发生在特定的上下文中，其中代理通过对一致环境中的位置的探索和跟踪来获取技能。代理的历史空间上下文提供了自我监督对比学习的相似性信号。我们提出了一种独特的方法，称为环境空间相似性（ESS），它补充了现有的对比学习方法。在模拟的逼真环境中使用图像作为实验设置，我们证明了ESS优于传统的实例鉴别方法。此外，从相同环境中采样更多数据显著提高了准确性并提供了新的增强。ESS在房间分类和空间预测任务中表现出卓越的熟练度，特别是在陌生环境中。这种学习范式有潜力在具有独特视觉特征的新环境中使代理能够快速进行视觉学习。

    Visual learning often occurs in a specific context, where an agent acquires skills through exploration and tracking of its location in a consistent environment. The historical spatial context of the agent provides a similarity signal for self-supervised contrastive learning. We present a unique approach, termed Environmental Spatial Similarity (ESS), that complements existing contrastive learning methods. Using images from simulated, photorealistic environments as an experimental setting, we demonstrate that ESS outperforms traditional instance discrimination approaches. Moreover, sampling additional data from the same environment substantially improves accuracy and provides new augmentations. ESS allows remarkable proficiency in room classification and spatial prediction tasks, especially in unfamiliar environments. This learning paradigm has the potential to enable rapid visual learning in agents operating in new environments with unique visual characteristics. Potentially transforma
    
[^155]: 比较以人为中心的语言建模：模拟群体、个体特点还是两者兼顾？

    Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?. (arXiv:2401.12492v1 [cs.CL])

    [http://arxiv.org/abs/2401.12492](http://arxiv.org/abs/2401.12492)

    本研究比较了以群体属性、个体用户和组合方法来模拟人的上下文。合并群体和个体特征显著提高了用户级回归任务的性能，而模拟个体用户则显著提高了单个文档级分类任务的性能。

    

    自然语言处理在将人的上下文纳入其模型中取得了进展，但使用群体属性（如45岁以上的人群）还是模拟个体人物更有效的问题尚未确定。群体属性在技术上更容易实现，但是过于粗糙：并非所有45岁以上的人都以相同的方式书写。相反，模拟个体人物能够捕捉每个人身份的复杂性，允许更个性化的表示，但我们可能需要模拟无限数量的用户并且需要可能无法获取的数据。我们比较了通过群体属性、个体用户和组合方法来模拟人的上下文。将群体和个体特征结合起来，显著提高了基于用户文档的用户级回归任务（如年龄估计或个性评估）的性能。模拟个体用户显著提高了单个文档级分类任务（如立场和主题检测）的性能。

    Natural language processing has made progress in incorporating human context into its models, but whether it is more effective to use group-wise attributes (e.g., over-45-year-olds) or model individuals remains open. Group attributes are technically easier but coarse: not all 45-year-olds write the same way. In contrast, modeling individuals captures the complexity of each person's identity. It allows for a more personalized representation, but we may have to model an infinite number of users and require data that may be impossible to get. We compare modeling human context via group attributes, individual users, and combined approaches. Combining group and individual features significantly benefits user-level regression tasks like age estimation or personality assessment from a user's documents. Modeling individual users significantly improves the performance of single document-level classification tasks like stance and topic detection. We also find that individual-user modeling does w
    
[^156]: 运筹学的人工智能：改变运筹学过程的革命性技术

    Artificial Intelligence for Operations Research: Revolutionizing the Operations Research Process. (arXiv:2401.03244v1 [math.OC])

    [http://arxiv.org/abs/2401.03244](http://arxiv.org/abs/2401.03244)

    本研究回顾了人工智能技术在运筹学中的应用，旨在通过改进运筹学过程的不同阶段来提高其效果和效率。人工智能与运筹学的协同作用将推动领域内的创新和决策制定的改进。

    

    人工智能技术的迅速发展为各个领域，包括运筹学，开辟了新的机遇。本调查论文探讨了在运筹学过程中的人工智能整合（AI4OR），以提高其在参数生成、模型构建和模型优化等多个阶段的效果和效率。通过全面概述现有技术的同时，研究人工智能改变运筹学的潜力，本文旨在激发进一步研究和创新，开发人工智能增强的运筹学方法和工具。人工智能和运筹学的协同作用将推动各个领域的重大进展和新型解决方案，最终实现更加有效和高效的决策制定。

    The rapid advancement of artificial intelligence (AI) techniques has opened up new opportunities to revolutionize various fields, including operations research (OR). This survey paper explores the integration of AI within the OR process (AI4OR) to enhance its effectiveness and efficiency across multiple stages, such as parameter generation, model formulation, and model optimization. By providing a comprehensive overview of the state-of-the-art and examining the potential of AI to transform OR, this paper aims to inspire further research and innovation in the development of AI-enhanced OR methods and tools. The synergy between AI and OR is poised to drive significant advancements and novel solutions in a multitude of domains, ultimately leading to more effective and efficient decision-making.
    
[^157]: 自我对比：通过不一致的求解视角获得更好的反思能力

    Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives. (arXiv:2401.02009v1 [cs.CL])

    [http://arxiv.org/abs/2401.02009](http://arxiv.org/abs/2401.02009)

    自我对比是一种通过对比不同求解视角和总结差异，提高大型语言模型（LLM）的反思能力的方法。

    

    大型语言模型（LLM）的反思能力引起了广泛关注。一种事后提示策略，例如反思和自我改进，根据自我评估或外部反馈来改善LLM的响应。然而，最近的研究表明，在没有外部反馈的情况下，LLM的内在反思是不稳定的。我们的调查揭示了自我评估反馈质量是关键瓶颈。我们发现LLM在自我评估时常常表现出过度自信或高度随机性，提供固执或不一致的反馈，导致反思能力不佳。为了解决这个问题，我们提出了自我对比的方法：它根据请求自适应地探索多样的求解视角，对比差异，并将这些差异总结为一个检查表，用于重新审视和消除差异。我们的方法赋予LLM多样的视角以减轻固执偏见。此外，差异指示了潜在的错误或固有的不确定性。

    The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties tha
    
[^158]: 基于检索增强的大型语言模型：一项调研

    Retrieval-Augmented Generation for Large Language Models: A Survey. (arXiv:2312.10997v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.10997](http://arxiv.org/abs/2312.10997)

    本综述论文调查了基于检索增强的大型语言模型的发展，包括三个主要范式：Naive RAG、Advanced RAG和Modular RAG。RAG通过整合外部数据库的知识，增强模型的准确性和可信度，并实现了持续更新知识和整合领域特定信息的功能。

    

    大型语言模型（LLMs）展示了显著的能力，但面临幻觉、过时知识和非透明、不可追溯的推理过程等挑战。检索增强生成（RAG）已经成为一种有前途的解决方案，通过整合来自外部数据库的知识，增强模型的准确性和可信度，特别适用于知识密集型任务，并允许持续更新知识和整合领域特定信息。RAG将LLMs自身的知识与庞大、动态的外部数据库相结合，实现协同效应。本综述论文详细考察了RAG范式的发展，包括Naive RAG、Advanced RAG和Modular RAG。它详细审视了RAG框架的三个基本要素，包括检索、生成和增强技术。本文强调了嵌入在RAG框架中的最新技术。

    Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in e
    
[^159]: FedSN：一个适用于LEO卫星网络的通用联邦学习框架

    FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v1 [cs.LG])

    [http://arxiv.org/abs/2311.01483](http://arxiv.org/abs/2311.01483)

    FedSN是一个通用的联邦学习框架，用于解决在LEO卫星网络中的异构计算和存储能力、有限的上行速率以及模型陈旧等关键挑战。

    

    最近，许多低地球轨道（LEO）卫星已经由商业公司成功地发射和部署到太空中，如SpaceX。由于LEO卫星配备了多模传感器，它们不仅用于通信，还用于各种机器学习应用，如空间调制识别、遥感图像分类等。然而，由于与LEO卫星的有限接触时间（例如5分钟），地面站（GS）可能无法下载如此大量的原始感测数据进行集中模型训练。因此，联邦学习（FL）已经成为解决这个问题的有希望的解决方案，通过在设备上进行训练。不幸的是，要在LEO卫星上使用FL，我们仍然面临三个关键挑战，即i）异构计算和存储能力，ii）有限的上行速率，以及iii）模型陈旧问题。为此，我们提出了一种名为FedSN的通用FL框架来解决上述挑战，一

    Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, an
    
[^160]: VIGraph：自我监督学习用于类别不平衡节点分类

    VIGraph: Self-supervised Learning for Class-Imbalanced Node Classification. (arXiv:2311.01191v1 [cs.LG])

    [http://arxiv.org/abs/2311.01191](http://arxiv.org/abs/2311.01191)

    VIGraph是一个基于自我监督学习的模型，通过利用自编码器生成少数类节点来解决图数据中的类别不平衡问题，并通过引入孪生对比策略提高生成节点的质量。

    

    图数据中的类别不平衡为节点分类带来了重大挑战。现有方法，如基于SMOTE的方法，在不平衡场景构建过程中仍存在局限性。自我监督学习（SSL）通过从数据中合成少数类节点提供了一个有前景的解决方案，然而其潜力尚未被探索。本文分析了基于SMOTE的方法的限制，并引入了VIGraph，这是一种基于自我监督变分图自编码器（VGAE）的新型SSL模型，利用变分推断（VI）生成少数类节点。具体而言，VIGraph在构建不平衡图时严格遵循不平衡的概念，并利用生成型VGAE生成少数类节点。此外，VIGraph在解码阶段引入了一种新颖的孪生对比策略，以提高生成节点的整体质量。VIGraph能够生成高质量的节点，无需重新集成。

    Class imbalance in graph data poses significant challenges for node classification. Existing methods, represented by SMOTE-based approaches, partially alleviate this issue but still exhibit limitations during imbalanced scenario construction. Self-supervised learning (SSL) offers a promising solution by synthesizing minority nodes from the data itself, yet its potential remains unexplored. In this paper, we analyze the limitations of SMOTE-based approaches and introduce VIGraph, a novel SSL model based on the self-supervised Variational Graph Auto-Encoder (VGAE) that leverages Variational Inference (VI) to generate minority nodes. Specifically, VIGraph strictly adheres to the concept of imbalance when constructing imbalanced graphs and utilizes the generative VGAE to generate minority nodes. Moreover, VIGraph introduces a novel Siamese contrastive strategy at the decoding phase to improve the overall quality of generated nodes. VIGraph can generate high-quality nodes without reintegrat
    
[^161]: 等距运动流形基元

    Isometric Motion Manifold Primitives. (arXiv:2310.17072v1 [cs.AI])

    [http://arxiv.org/abs/2310.17072](http://arxiv.org/abs/2310.17072)

    Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.

    

    运动流形基元（MMP）为给定任务生成一系列连续轨迹流形，每一个轨迹流形都能成功完成任务。它由对流形进行参数化的解码函数以及潜在坐标空间中的概率密度组成。本文首先展示了由于潜在空间中的几何扭曲，MMP的性能可能会显著降低--通过变形，我们指的是相似的运动在潜在空间中无法相邻。然后，我们提出了等距运动流形基元（IMMP），其潜在坐标空间保持了流形的几何结构。为此，我们建立和使用了一个Riemannian度量，用于运动空间（即，参数化曲线空间），我们称之为CurveGeom Riemannian度量。对于平面障碍避让运动和推动操纵任务的实验表明，IMMP明显优于现有的MMP方法。代码可在https://github.com/Gabe-YHLee/IMMP找到。

    The Motion Manifold Primitive (MMP) produces, for a given task, a continuous manifold of trajectories each of which can successfully complete the task. It consists of the decoder function that parametrizes the manifold and the probability density in the latent coordinate space. In this paper, we first show that the MMP performance can significantly degrade due to the geometric distortion in the latent space -- by distortion, we mean that similar motions are not located nearby in the latent space. We then propose {\it Isometric Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the geometry of the manifold. For this purpose, we formulate and use a Riemannian metric for the motion space (i.e., parametric curve space), which we call a {\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding motions and pushing manipulation tasks show that IMMP significantly outperforms existing MMP methods. Code is available at https://github.com/Gabe-YHLee/IMMP
    
[^162]: 学习基于概念的视觉因果转换和符号推理用于视觉规划

    Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning. (arXiv:2310.03325v1 [cs.AI])

    [http://arxiv.org/abs/2310.03325](http://arxiv.org/abs/2310.03325)

    本文提出了一种面向视觉规划的可解释和可推广的框架，通过将视觉输入转化为概念表示、符号抽象和推理以及将视觉因果转换与真实世界行为关联，实现了目标条件的视觉规划。

    

    视觉规划模拟了人类在搜索初始视觉状态和最终视觉目标状态之间的视觉因果转换来实现期望目标时所做的决策过程。在以自我为中心的视觉中，视觉规划越来越重要，因为它在引导智能体在复杂环境中执行日常任务方面具有优势。本文提出了一个可解释和可推广的视觉规划框架，包括：i）一种新颖的基于替代的概念学习器（SCL），将视觉输入转化为分解的概念表示；ii）通过自学符号进行任务规划的符号抽象和推理；iii）将视觉因果转换与语义相似的真实世界行为进行关联的视觉因果转换模型（ViCT）。给定一个初始状态，我们通过学习到的表示和因果转换的符号推理方法进行目标条件的视觉规划，以达到目标状态。

    Visual planning simulates how humans make decisions to achieve desired goals in the form of searching for visual causal transitions between an initial visual state and a final visual goal state. It has become increasingly important in egocentric vision with its advantages in guiding agents to perform daily tasks in complex environments. In this paper, we propose an interpretable and generalizable visual planning framework consisting of i) a novel Substitution-based Concept Learner (SCL) that abstracts visual inputs into disentangled concept representations, ii) symbol abstraction and reasoning that performs task planning via the self-learned symbols, and iii) a Visual Causal Transition model (ViCT) that grounds visual causal transitions to semantically similar real-world actions. Given an initial state, we perform goal-conditioned visual planning with a symbolic reasoning method fueled by the learned representations and causal transitions to reach the goal state. To verify the effectiv
    
[^163]: ABScribe: 使用大型语言模型在人工智能与人类共同写作任务中快速探索多种写作变化

    ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models. (arXiv:2310.00117v1 [cs.HC])

    [http://arxiv.org/abs/2310.00117](http://arxiv.org/abs/2310.00117)

    ABScribe是一种界面，支持在人工智能与人类共同写作任务中快速探索多种写作变化。用户可以使用大型语言模型提示快速生成多个变体，这些变体以可重用的按钮形式呈现，并且可以通过上下文工具栏进行快速的就地比较。

    

    通过重新书写文本来探索替代想法是写作过程的关键。最先进的大型语言模型（LLM）可以简化写作变化生成的过程。然而，当前的界面存在同时考虑多种变化的挑战：在不覆盖文本的情况下创建新的版本可能很困难，而按顺序粘贴它们可能会使文档变得杂乱，增加工作量，并打断作者的流程。为了解决这个问题，我们提出了ABScribe，一种支持在人工智能与人类共同写作任务中快速且结构化地探索写作变化的界面。通过ABScribe，用户可以使用LLM提示快速产生多个变体，这些变体会自动转换成可重用的按钮形式。变体在文本段落中被存储在相邻位置，通过在上下文工具栏上的鼠标悬停交互进行快速的就地比较。我们对12名撰写人员进行的用户研究表明，ABScribe能显著减轻任务负荷（d = 1.20, p < 0.001），提高用户的认知程度。

    Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art large language models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new versions without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly produce multiple variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text segments for rapid in-place comparisons using mouse-over interactions on a context toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances user perceptions o
    
[^164]: 半导体制造业中无监督故障检测的时间序列数据生成预训练

    Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing. (arXiv:2309.11427v1 [cs.LG])

    [http://arxiv.org/abs/2309.11427](http://arxiv.org/abs/2309.11427)

    本研究提出了TRACE-GPT模型，用于半导体制造业中无监督故障检测。通过使用时间卷积嵌入和生成式预训练Transformer来预训练时间序列数据，并使用交叉熵损失函数进行异常序列和正常序列的分类，实验结果表明模型具有更好的性能。

    

    本文介绍了TRACE-GPT（Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers），它是用于半导体制造中的未标记数据集上预训练单变量时间序列传感器数据并检测故障的模型。研究表明，通过使用时间卷积嵌入和生成式预训练Transformer（GPT）来提取时间序列数据的特征，并使用交叉熵损失函数对异常序列和正常序列进行分类，我们的模型表现出更好的性能。

    This paper introduces TRACE-GPT, which stands for Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor data and detect faults on unlabeled datasets in semiconductor manufacturing. In semiconductor industry, classifying abnormal time-series sensor data from normal data is important because it is directly related to wafer defect. However, small, unlabeled, and even mixed training data without enough anomalies make classification tasks difficult. In this research, we capture features of time-series data with temporal convolutional embedding and Generative Pre-trained Transformer (GPT) to classify abnormal sequences from normal sequences using cross entropy loss. We prove that our model shows better performance than previous unsupervised models with both an open dataset, the University of California Riverside (UCR) time-series classification archive, and the process log of our Ch
    
[^165]: 一般化界限：信息论和PAC-Bayesian的视角

    Generalization Bounds: Perspectives from Information Theory and PAC-Bayes. (arXiv:2309.04381v1 [cs.LG])

    [http://arxiv.org/abs/2309.04381](http://arxiv.org/abs/2309.04381)

    该论文介绍了一般化界限的两个视角：信息论和PAC-Bayesian，并探讨了它们之间的联系和共同点。这对于理论机器学习的进一步发展和新算法的设计具有重要意义。

    

    在理论机器学习中，一个基本问题是一般化。在过去的几十年里，PAC-Bayesian方法已经被确定为一个灵活的框架，用来解决机器学习算法的一般化能力，并设计新的算法。最近，由于其对多种学习算法（包括深度神经网络）的潜在适用性，它引起了越来越多的关注。与此同时，还发展了一种信息论的视角，其中建立了一般化与各种信息度量之间的关系。这个框架与PAC-Bayesian方法密切相关，并且在两个方面都有独立发现的很多结果。在本文中，我们强调这种强连接，并提出一种统一的一般化处理方法。我们介绍了两个视角共同拥有的技术和结果，并讨论了不同的方法和解释。特别是，我们展示了这种连接如何产生新的洞见和理论的发展，并展示了这两个领域的交叉应用和潜在的进一步研究方向。

    A fundamental question in theoretical machine learning is generalization. Over the past decades, the PAC-Bayesian approach has been established as a flexible framework to address the generalization capabilities of machine learning algorithms, and design new ones. Recently, it has garnered increased interest due to its potential applicability for a variety of learning algorithms, including deep neural networks. In parallel, an information-theoretic view of generalization has developed, wherein the relation between generalization and various information measures has been established. This framework is intimately connected to the PAC-Bayesian approach, and a number of results have been independently discovered in both strands. In this monograph, we highlight this strong connection and present a unified treatment of generalization. We present techniques and results that the two perspectives have in common, and discuss the approaches and interpretations that differ. In particular, we demons
    
[^166]: DropMix: 减少混合样本数据增强中的类别相关性

    DropMix: Reducing Class Dependency in Mixed Sample Data Augmentation. (arXiv:2307.09136v1 [cs.CV])

    [http://arxiv.org/abs/2307.09136](http://arxiv.org/abs/2307.09136)

    该论文提出了一种名为DropMix的方法，通过排除一定比例的数据来减少混合样本数据增强（MSDA）中的类别相关性。在两个数据集上的实验证明了该方法可以提高之前因为MSDA而下降的类别的性能，并增加整体的平均准确率。

    

    混合样本数据增强（MSDA）是一种广泛应用的技术，已被证明可以提高各种任务的性能。然而，在本文中，我们展示了MSDA的效果是与类别相关的，一些类别的性能得到了改进，而其他类别则出现了下降。为了减少类别相关性，我们提出了DropMix方法，在MSDA计算中排除了特定比例的数据。通过在MSDA和非MSDA数据的组合上进行训练，所提出的方法不仅提高了之前由MSDA降低的类别的性能，还提高了整体的平均准确率，在使用三种MSDA方法（Mixup、CutMix和PuzzleMix）对两个数据集（CIFAR-100和ImageNet）进行的实验证明了这一点。

    Mixed sample data augmentation (MSDA) is a widely used technique that has been found to improve performance in a variety of tasks. However, in this paper, we show that the effects of MSDA are class-dependent, with some classes seeing an improvement in performance while others experience a decline. To reduce class dependency, we propose the DropMix method, which excludes a specific percentage of data from the MSDA computation. By training on a combination of MSDA and non-MSDA data, the proposed method not only improves the performance of classes that were previously degraded by MSDA, but also increases overall average accuracy, as shown in experiments on two datasets (CIFAR-100 and ImageNet) using three MSDA methods (Mixup, CutMix and PuzzleMix).
    
[^167]: 从合成的人类团队活动中学习

    Learning from Synthetic Human Group Activities. (arXiv:2306.16772v1 [cs.CV])

    [http://arxiv.org/abs/2306.16772](http://arxiv.org/abs/2306.16772)

    提出了M3Act，一个多视图多团队多人的人类原子动作和团队活动数据生成器，通过Unity引擎驱动实现。该生成器具有大规模数据生成、多模态和高质量注释等特点，能够用于研究复杂的人类互动和团队活动。

    

    在以人为中心的计算机视觉中，对复杂的人类互动和团队活动的理解引起了人们的关注。然而，相关任务的进展受到了获取大规模标记的真实世界数据集的困难的限制。为了缓解这个问题，我们提出了M3Act，一个多视图多团队多人的人类原子动作和团队活动数据生成器。M3Act采用Unity引擎驱动，包含可供仿真使用的三维场景和人物资源，可配置的照明和摄像系统，高度参数化的模块化团队活动，以及在数据生成过程中具有大量领域随机化的特点。我们的数据生成器能够生成具有多个视图、模态（RGB图像、2D姿势、3D动作）和高质量注释的大规模人类活动数据集（2D边界框、实例分割掩模、个体动作和团队活动类别）。利用M3Act，我们可以生成大规模的人类活动数据集，用于研究人类互动和团队活动。

    The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision. However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets. To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator. Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process. Our data generator is capable of generating large-scale datasets of human activities with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories). Using M3Act, we
    
[^168]: “构建可管控的AI系统：技术挑战和政策机遇”

    Towards Regulatable AI Systems: Technical Gaps and Policy Opportunities. (arXiv:2306.12609v1 [cs.AI])

    [http://arxiv.org/abs/2306.12609](http://arxiv.org/abs/2306.12609)

    该论文探讨了如何确保AI系统遵循规定，提出了当前和潜在可能的技术实现，以及需要跨学科方法解决的问题。

    

    针对如何规范AI系统的问题，越来越多的关注被给予。在管控机构努力确定要封装到法规中的价值观时，本文考虑了其中的技术问题：AI专家们能够审查AI系统是否遵守监管要求的程度是多少？我们通过两个公共部门的采购清单对这个问题进行了调查，并定义了三个方面：目前有哪些方面是我们可以做的；利用AI技术创新我们能做到哪些方面； 哪些要求需要更多跨学科的方法。

    There is increasing attention being given to how to regulate AI systems. As governing bodies grapple with what values to encapsulate into regulation, we consider the technical half of the question: To what extent can AI experts vet an AI system for adherence to regulatory requirements? We investigate this question through two public sector procurement checklists, identifying what we can do now, what we should be able to do with technical innovation in AI, and what requirements necessitate a more interdisciplinary approach.
    
[^169]: 循环记忆决策变压器

    Recurrent Memory Decision Transformer. (arXiv:2306.09459v1 [cs.LG])

    [http://arxiv.org/abs/2306.09459](http://arxiv.org/abs/2306.09459)

    本文提出了循环记忆决策变压器（RMDT）模型，用于处理强化学习中的长序列问题。在Atari游戏和MoJoCo控制问题上的实验表明，采用循环记忆机制的RMDT模型显着优于其没有循环记忆机制的对应模型。

    

    变革性模型最初是为自然语言问题而开发的，最近在离线强化学习任务中得到广泛应用。这是因为代理的历史可以表示为序列，并且整个任务可以缩减为序列建模任务。然而，变压器操作的二次复杂性限制了上下文的潜在增加。因此，为了在自然语言中处理长序列，使用了不同版本的记忆机制。在本文中，我们提出了循环记忆决策变压器（RMDT），这是一种在强化学习问题中使用循环记忆机制的模型。我们在Atari游戏和MoJoCo控制问题上进行了彻底的实验，并表明我们提出的模型在Atari游戏上显着优于没有循环记忆机制的对应模型。我们还仔细研究了记忆对所提出的模型绩效的影响。这些发现为开发更高效和更有效的处理长序列的强化学习模型提供了启示。

    Transformative models, originally developed for natural language problems, have recently been widely used in offline reinforcement learning tasks. This is due to the fact that the agent's history can be represented as a sequence, and the whole task can be reduced to the sequence modeling task. However, the quadratic complexity of the transformer operation limits the potential increase in context. Therefore, to work with long sequences in a natural language, different versions of the memory mechanism are used. In this paper, we propose the Recurrent Memory Decision Transformer (RMDT), a model that uses a recurrent memory mechanism for reinforcement learning problems. We conduct thorough experiments on Atari games and MoJoCo control problems, and show that our proposed model is significantly superior to its counterparts without the recurrent memory mechanism on Atari games. We also carefully study the effect of memory on the performance of the proposed model. These findings shed light on
    
[^170]: 用于基于检索的对话系统的上下文掩码自编码器

    ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v1 [cs.CL])

    [http://arxiv.org/abs/2306.04357](http://arxiv.org/abs/2306.04357)

    本研究提出了一种针对对话响应选择的后训练技术Dial-MAE，利用生成方法更好地压缩对话语义至密集向量，并提高对话响应选择准确性。

    

    对话响应选择旨在根据给定的用户和系统话语历史记录从几个候选响应中选择适当的响应。最近的研究通过后训练大多依赖于单纯的掩码语言建模方法来提高对话响应选择的准确性。但是，最近开发的生成方法在IR社区展示了有希望的文本表示能力，这可能会导致更好的对话语义建模。因此，在本文中，我们提出 Dial-MAE（对话上下文掩码自编码器），这是一种简单而有效的针对对话响应选择的后训练技术。 Dial-MAE使用一个不对称的编码器-解码器架构，学习将对话的语义更好地压缩到密集向量中。 Dial-MAE的过程包括由深度编码器创建带有掩码对话上下文的对话嵌入，然后是浅解码器，该解码器使用此嵌入以及上下文向量来生成响应。

    Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Recent studies have been improving the accuracy of dialogue response selection through post-training, mostly relying on naive masked language modeling methods. However, the recently developed generative methods have shown promising text representation capabilities in IR community, which could potentially lead to better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE (Dialogue Contextual Masking Auto-encoder), a straightforward yet effective post-training technique tailored for dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture that learns to better compress the semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE involves a deep encoder creating a dialogue embedding with the masked dialogue context, followed by a shallow decoder that uses this embedding along with
    
[^171]: 弱监督下AUC优化：统一的部分AUC方法

    Weakly Supervised AUC Optimization: A Unified Partial AUC Approach. (arXiv:2305.14258v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2305.14258](http://arxiv.org/abs/2305.14258)

    本文提出了WSAUC，一种解决弱监督下AUC优化问题的统一框架，它包括噪声标签学习、正-无标签学习、多实例学习和半监督学习场景，并提出了一种新型的部分AUC——反转部分AUC（rpAUC），作为鲁棒的AUC最大化训练目标，为各种弱监督场景下的AUC优化提供了一种通用解决方案。

    

    由于获取完美的监督通常很困难，现实中的机器学习任务通常面临不准确、不完整或不精确的监督，统称为弱监督。在本文中，我们提出了WSAUC，一种用于解决弱监督下AUC优化问题的统一框架，它涵盖了噪声标签学习、正-无标签学习、多实例学习和半监督学习场景。在WSAUC框架内，我们首先将各种弱监督场景下的AUC优化问题框架化为最小化受污染集合上AUC风险的常见形式，并证明经验风险最小化问题与真实AUC一致。然后，我们介绍一种新型的部分AUC，即反转部分AUC（rpAUC），它作为鲁棒的AUC最大化训练目标，在存在污染标签的情况下发挥作用。WSAUC为各种弱监督场景下的AUC优化提供了一种通用解决方案。

    Since acquiring perfect supervision is usually difficult, real-world machine learning tasks often confront inaccurate, incomplete, or inexact supervision, collectively referred to as weak supervision. In this work, we present WSAUC, a unified framework for weakly supervised AUC optimization problems, which covers noisy label learning, positive-unlabeled learning, multi-instance learning, and semi-supervised learning scenarios. Within the WSAUC framework, we first frame the AUC optimization problems in various weakly supervised scenarios as a common formulation of minimizing the AUC risk on contaminated sets, and demonstrate that the empirical risk minimization problems are consistent with the true AUC. Then, we introduce a new type of partial AUC, specifically, the reversed partial AUC (rpAUC), which serves as a robust training objective for AUC maximization in the presence of contaminated labels. WSAUC offers a universal solution for AUC optimization in various weakly supervised scena
    
[^172]: 最小化通信的异步张量并行性

    Communication-minimizing Asynchronous Tensor Parallelism. (arXiv:2305.13525v1 [cs.LG])

    [http://arxiv.org/abs/2305.13525](http://arxiv.org/abs/2305.13525)

    本文提出了Tensor3D，一种最小化通信消耗的三维张量计算并行化方法。它利用智能分布神经网络参数、新颖超分解方法以及通信模型，使训练速度提高了约3倍，GPU空闲时间降低了50％以上。

    

    随着现代神经网络规模扩大到数十亿个参数，设计能够在多GPU集群上高效训练这些网络的并行算法变得至关重要。本文提出了Tensor3D，一种全新的三维（3D）张量计算并行化方法，旨在最小化大型多十亿参数模型的并行训练中由通信引起的空闲时间。首先，我们引入了一种智能的神经网络参数分布方式，消除了为满足各层数据依赖而需要的通信。然后，我们提出了一种新颖的并行训练过程超分解方法，利用它可以显著提高通信与计算的重叠度，从而减少GPU空闲时间。最后，我们提出了一种通信模型，帮助用户为给定的神经网络识别通信最优的可用硬件资源分解。 对于256 A100 GPU上的28B参数CNN，在本文的 Tensor3D 方法下，训练速度提高了约3倍，与以前的方法相比 GPU 空闲时间也降低了约50％以上。

    As state-of-the-art neural networks scale to billions of parameters, designing parallel algorithms that can train these networks efficiently on multi-GPU clusters has become critical. This paper presents Tensor3D, a novel three-dimensional (3D) approach to parallelize tensor computations, that strives to minimize the idle time incurred due to communication in parallel training of large multi-billion parameter models. First, we introduce an intelligent distribution of neural network parameters across GPUs that eliminates communication required for satisfying data dependencies of individual layers. Then, we propose a novel overdecomposition of the parallel training process, using which we achieve significant overlap of communication with computation, thereby reducing GPU idle time. Finally, we present a communication model, which helps users identify communication optimal decompositions of available hardware resources for a given neural network. For a 28B parameter CNN on 256 A100 GPUs, 
    
[^173]: ChatGPT 需要进行SPADE（可持续性、隐私、数字鸿沟和伦理）评估：一项综述。

    ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])

    [http://arxiv.org/abs/2305.03123](http://arxiv.org/abs/2305.03123)

    本文研究关注ChatGPT面临的可持续性、隐私、数字鸿沟和伦理问题，提出了SPADE评估的必要性，并给出了缓解和建议。

    

    ChatGPT是另一个大型语言模型（LLM），由于其性能和有效的对话能力，在研究和工业界中得到了巨大的关注。最近，许多研究已经发表，以展示ChatGPT和其他LLMs的有效性、效率、集成和情感。相反，本研究关注的是大多数被忽视的重要方面，即可持续性、隐私、数字鸿沟和伦理，并建议不仅仅是ChatGPT，而是在对话机器人类别中的每一个后续入口都应该进行SPADE评估。本文详细讨论了关于ChatGPT的问题和关注点与上述特征一致。我们通过一些初步的数据收集和可视化以及假设的事实来支持我们的假设。我们还为每个问题提出了缓解和建议。此外，我们还提供了一些未来方向和开放问题的探讨。

    ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
    
[^174]: 语言距离与多语言表示空间中的跨语言传递的相关性研究

    Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space. (arXiv:2305.02151v1 [cs.CL])

    [http://arxiv.org/abs/2305.02151](http://arxiv.org/abs/2305.02151)

    探索了语言特征对多语言表示空间中的跨语言传递性能的影响，初步提供了方法以增强对语言上相距较远的语言的传递能力。

    

    先前的研究已经探讨了不同语言特征对跨语言传递性能的影响。本研究探讨了这种效应如何映射到表示空间中。过去的研究集中在微调期间多语言语言模型的跨语言对齐上的影响，而本研究研究的是由MLLMs生成的相应语言表示空间的绝对演变。我们特别强调语言特征的作用，并调查其与表示空间和跨语言传递性能的影响之间的相互关系。此外，本文提供了初步证据，说明如何利用这些发现增强对语言上相距较远的语言的传递能力。

    Prior research has investigated the impact of various linguistic features on cross-lingual transfer performance. In this study, we investigate the manner in which this effect can be mapped onto the representation space. While past studies have focused on the impact on cross-lingual alignment in multilingual language models during fine-tuning, this study examines the absolute evolution of the respective language representation spaces produced by MLLMs. We place a specific emphasis on the role of linguistic characteristics and investigate their inter-correlation with the impact on representation spaces and cross-lingual transfer performance. Additionally, this paper provides preliminary evidence of how these findings can be leveraged to enhance transfer to linguistically distant languages.
    
[^175]: 自监督ECG表征学习在心律失常检测中的应用研究：分布分析及实验探究

    In-Distribution and Out-of-Distribution Self-supervised ECG Representation Learning for Arrhythmia Detection. (arXiv:2304.06427v1 [cs.LG])

    [http://arxiv.org/abs/2304.06427](http://arxiv.org/abs/2304.06427)

    本文系统研究了自监督学习方法在ECG表征学习上的应用，首次对三个常用ECG心律失常数据集进行了分布分析，实验发现SwAV方法表现最佳，能够超越传统的有监督学习方法，还具有较强的鲁棒性，有望在大规模和多样化人群中检测心律失常。

    

    本文针对心电图(ECG)心律失常检测问题，系统地研究了自监督学习(Self-Supervised Learning, SSL)方法的有效性。我们首先对三个常用的ECG心律失常数据集进行了分布分析，并进行了综合性实验，使用不同增强和参数评估了各种SSL方法（如SimCRL、BYOL和SwAV）在ECG表征学习方面的有效性。实验结果表明，SwAV方法表现最佳。我们进一步进行了针对In-Distribution (ID)和Out-of-Distribution (OOD) ECG数据的交叉数据集训练和测试实验，结果表明SSL方法，特别是SwAV，在ECG表征学习方面具有很高的竞争力，并且对不同种类的ECG数据具有较强的鲁棒性，从而有望在大规模和多样化人群中检测心律失常。

    This paper presents a systematic investigation into the effectiveness of Self-Supervised Learning (SSL) methods for Electrocardiogram (ECG) arrhythmia detection. We begin by conducting a novel distribution analysis on three popular ECG-based arrhythmia datasets: PTB-XL, Chapman, and Ribeiro. To the best of our knowledge, our study is the first to quantify these distributions in this area. We then perform a comprehensive set of experiments using different augmentations and parameters to evaluate the effectiveness of various SSL methods, namely SimCRL, BYOL, and SwAV, for ECG representation learning, where we observe the best performance achieved by SwAV. Furthermore, our analysis shows that SSL methods achieve highly competitive results to those achieved by supervised state-of-the-art methods. To further assess the performance of these methods on both In-Distribution (ID) and Out-of-Distribution (OOD) ECG data, we conduct cross-dataset training and testing experiments. Our comprehensive
    
[^176]: 揭开对社交机器人研究的误解

    Demystifying Misconceptions in Social Bots Research. (arXiv:2303.17251v1 [cs.SI])

    [http://arxiv.org/abs/2303.17251](http://arxiv.org/abs/2303.17251)

    这篇文章揭示了关于社交机器人研究的普遍误解，强调需要以严谨、公正和负责任的方式讨论虚假信息研究。

    

    社交机器人科学寻求解决网络虚假信息最受争议的形式之一的知识和解决方案。然而，社交机器人研究受到普遍的偏见、夸大的结果和误解的困扰，这些都为歧义、不切实际的期望和看似无法调和的发现打下了基础。克服这些问题对于确保可靠的解决方案和重申科学方法的有效性至关重要。在这篇文章中，我们修订了社交机器人研究中的一些最新结果，强调和纠正了事实错误以及方法论和概念问题。更重要的是，我们揭开了普遍的误解，解决了有关如何讨论社交机器人研究的基本问题。我们的分析揭示了以严谨、公正和负责任的方式讨论虚假信息研究的必要性。本文通过确定并驳斥社交机器人研究的支持者和反对者常用的谬误论证，支持这种努力。

    The science of social bots seeks knowledge and solutions to one of the most debated forms of online misinformation. Yet, social bots research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. In this contribution we revise some recent results in social bots research, highlighting and correcting factual errors as well as methodological and conceptual issues. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss misinformation research in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research as
    
[^177]: 创新放缓：技术概念创造的减速及新技术概念原创性的下降

    Innovation Slowdown: Decelerating Concept Creation and Declining Originality in New Technological Concepts. (arXiv:2303.13300v1 [cs.SI])

    [http://arxiv.org/abs/2303.13300](http://arxiv.org/abs/2303.13300)

    人类智力的局限性导致技术概念创造放缓和原创性下降，因此建议开发和实施创造性人工智能增强创新过程。

    

    通过对之前的概念重用、重组和合成进行新技术概念的创造可能会导致概念空间的指数增长。然而，我们对由专利文本中超过400万个概念组成的大规模技术语义网络进行的统计分析发现，概念创造的步伐在持续减缓，并且新创造出的概念的原创性有所下降。这些趋势可以归因于人类智力在创新超出现有技术的拓展空间方面的局限。为了保持创新，我们建议开发和实施创造性人工智能，以增强创新过程的多个方面，包括学习、创造和评估。

    The creation of new technological concepts through design reuses, recombination, and synthesis of prior concepts to create new ones may lead to exponential growth of the concept space over time. However, our statistical analysis of a large-scale technology semantic network consisting of over four million concepts from patent texts found evidence of a persistent deceleration in the pace of concept creation and a decline in the originality of newly created concepts. These trends may be attributed to the limitations of human intelligence in innovating beyond an expanding space of prior art. To sustain innovation, we recommend the development and implementation of creative artificial intelligence that can augment various aspects of the innovation process, including learning, creation, and evaluation.
    
[^178]: 适应性负证据深度学习用于开放式半监督学习

    Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised Learning. (arXiv:2303.12091v1 [cs.LG])

    [http://arxiv.org/abs/2303.12091](http://arxiv.org/abs/2303.12091)

    本文提出了ANEDL框架，应用证据深度学习量化不同类型的不确定性，并设计了新颖的适应性负优化策略，有效应对在未标记数据集中包含内部值和异常值的开放式半监督学习。

    

    半监督学习方法假设标记数据、未标记数据和测试数据来自同一分布。开放式半监督学习考虑到一个更实际的情况，即未标记数据和测试数据包含标记数据中未观察到的新类别（异常值）。本文提出了一种新颖的框架——适应性负证据深度学习（ANEDL），以应对二元分类器的不足之处，如缺乏可扩展性和无法区分不同类型的不确定性。具体而言，我们首先介绍证据深度学习（EDL）作为一种异常检测器来量化不同类型的不确定性，并设计不同的不确定性度量方法进行自我训练和推理。此外，我们提出了一种新颖的适应性负优化策略，使EDL更加适合包含内部值和异常值的未标记数据集。通过在基准数据集上的实验验证，我们的ANEDL显著优于现有的开放式半监督学习方法。

    Semi-supervised learning (SSL) methods assume that labeled data, unlabeled data and test data are from the same distribution. Open-set semi-supervised learning (Open-set SSL) considers a more practical scenario, where unlabeled data and test data contain new categories (outliers) not observed in labeled data (inliers). Most previous works focused on outlier detection via binary classifiers, which suffer from insufficient scalability and inability to distinguish different types of uncertainty. In this paper, we propose a novel framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these limitations. Concretely, we first introduce evidential deep learning (EDL) as an outlier detector to quantify different types of uncertainty, and design different uncertainty metrics for self-training and inference. Furthermore, we propose a novel adaptive negative optimization strategy, making EDL more tailored to the unlabeled dataset containing both inliers and outliers. As demonstrat
    
[^179]: CroSel: 用于部分标签学习的自信伪标签的跨选择

    CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning. (arXiv:2303.10365v1 [cs.LG])

    [http://arxiv.org/abs/2303.10365](http://arxiv.org/abs/2303.10365)

    CroSel是一种处理伪标签噪声的新方法，通过利用历史预测信息和一致性正则化项来准确识别部分标签数据的真实标签。

    

    部分标签学习(PLL)是一个重要的弱监督学习问题，它允许每个训练示例有一个候选标签集，而不是一个单一的ground-truth标签。已经广泛探索了基于识别的方法来解决PLL中的标签歧义问题，这些方法将真实标签视为要识别的潜在变量。然而，准确和完整地识别真实标签仍然具有挑战性，这会在模型训练过程中导致伪标签中的噪声。本文提出了一种名为CroSel的新方法，该方法利用模型的历史预测信息来识别大多数训练示例的真实标签。首先，我们引入了一种交叉选择策略，使得两个深度模型可以相互选择部分标记数据的真实标签。此外，我们提出了一种新颖的一致性正则化项co-mix，以避免因虚假选择而引起的样本浪费和微小噪声。通过这种方式，CroSel能够挑选出大多数示例的真实标签。

    Partial-label learning (PLL) is an important weakly supervised learning problem, which allows each training example to have a candidate label set instead of a single ground-truth label. Identification-based methods have been widely explored to tackle label ambiguity issues in PLL, which regard the true label as a latent variable to be identified. However, identifying the true labels accurately and completely remains challenging, causing noise in pseudo labels during model training. In this paper, we propose a new method called CroSel, which leverages historical prediction information from models to identify true labels for most training examples. First, we introduce a cross selection strategy, which enables two deep models to select true labels of partially labeled data for each other. Besides, we propose a novel consistent regularization term called co-mix to avoid sample waste and tiny noise caused by false selection. In this way, CroSel can pick out the true labels of most examples 
    
[^180]: HIVE：利用人类反馈进行指导性视觉编辑

    HIVE: Harnessing Human Feedback for Instructional Visual Editing. (arXiv:2303.09618v1 [cs.CV])

    [http://arxiv.org/abs/2303.09618](http://arxiv.org/abs/2303.09618)

    本文提出了一种新的框架，利用人类反馈进行指导性视觉编辑。通过收集被编辑图像的人类反馈，并学习奖励函数捕捉用户的偏好，可以缓解数据限制所带来的偏差，并提高模型性能。

    

    研究表明，将人类反馈纳入大型语言模型生成的文本对齐到人类偏好至关重要。本文假设，最先进的指导性图像编辑模型，其输出基于输入图像和编辑指令，同样可以从人类反馈中受益，因为其输出可能不符合用户的正确指令和偏好。本文提出了一种利用人类反馈进行指导性视觉编辑（HIVE）的新框架。具体而言，我们在编辑的图像上收集人类反馈并学习奖励函数以捕捉基础用户偏好。随后，我们引入可扩展的扩散模型微调方法，可根据估计的奖励值融入人类偏好。此外，为减轻数据限制带来的偏差，我们贡献了1M训练数据集，3.6K奖励数据集以用于奖励学习，以及1K评估数据集，以提高指导性图像编辑模型的性能。

    Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of inst
    
[^181]: 强化学习的后悔优化方法

    Regret-Based Optimization for Robust Reinforcement Learning. (arXiv:2302.06912v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06912](http://arxiv.org/abs/2302.06912)

    本论文提出了一种基于后悔的优化方法，用于使强化学习算法更加鲁棒，以应对观测中的对抗性噪声。

    

    深度强化学习策略对观测中的微小对抗性噪声容易受到攻击。这种对抗性噪声在安全关键环境中可能造成灾难性后果。现有的使强化学习算法对观测扰动的对抗策略主要集中在迭代改进每个迭代中生成的对抗示例。虽然这些方法已经显示出对普通强化学习方法的改进，但它们是被动性的，如果某些类别的对抗性示例在训练中没有产生，它们可能会表现得更差。因此，我们追求一种更积极的方法，依赖于直接优化一个经过充分研究的鲁棒指标。

    Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable to small adversarial noise in observations. Such adversarial noise can have disastrous consequences in safety-critical environments. For instance, a self-driving car receiving adversarially perturbed sensory observations about nearby signs (e.g., a stop sign physically altered to be perceived as a speed limit sign) or objects (e.g., cars altered to be recognized as trees) can be fatal. Existing approaches for making RL algorithms robust to an observation-perturbing adversary have focused on reactive approaches that iteratively improve against adversarial examples generated at each iteration. While such approaches have been shown to provide improvements over regular RL methods, they are reactive and can fare significantly worse if certain categories of adversarial examples are not generated during training. To that end, we pursue a more proactive approach that relies on directly optimizing a well-studied robustn
    
[^182]: Follower Agnostic Methods for Stackelberg Games（适用于斯塔克贝格博弈的追随者不可知方法）

    Follower Agnostic Methods for Stackelberg Games. (arXiv:2302.01421v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2302.01421](http://arxiv.org/abs/2302.01421)

    该论文提出了一种追随者不可知的方法来解决斯塔克贝格博弈中的问题，与其他现有作品不同的是，该算法不需要使用预估器或了解追随者的策略、效用函数。作者设计了一个两层循环算法，通过特别设计的策略探测追随者来更新领导者的策略，发现追随者的联合策略收敛于均衡。非渐进收敛速度到领导者目标的局部稳定点，并进一步展示了对领导者目标的局部极小值的渐进收敛。

    

    我们提出了一种算法，以追随者不可知的方式解决斯塔克贝格博弈的一类问题（可能有多个追随者）。与其他当代作品不同的是，我们的算法不需要使用领导者目标的梯度的预估器，也不需要了解追随者的效用函数或策略空间。相反，我们设计了一个两层循环算法，其中领导者使用通过用特别设计的策略探测追随者获得的特殊构造的梯度估计器更新其策略。在接收到这些策略后，追随者采用适应规则，使得追随者的联合策略收敛于均衡状态，这是领导者观察到的唯一信息，用于构造前述梯度估计器。我们提供了非渐进收敛速度到领导者目标的局部稳定点，尽管闭环函数不具有凸性。同时，我们进一步展示了对领导者目标的局部极小值的渐进收敛。

    We propose an algorithm to solve a class of Stackelberg games (possibly with multiple followers) in a follower agnostic manner. Particularly, unlike other contemporary works, our algorithm does not require the use of an oracle estimator for the gradient of the leader's objective or knowledge about the follower's utility function or strategy space. Instead, we design two-loop algorithm where the leader updates its strategies using specially constructed gradient estimator obtained by probing followers with specially designed strategies. Upon receiving the followers engage in an adaptation rule such that the joint strategy of followers converges near equilibrium which is the only information observed by leader to construct the aforementioned gradient estimator. We provide non-asymptotic convergence rates to stationary points of the leader's objective in the absence of convexity of the closed-loop function and further show asymptotic convergence to a local minima of the leader's objective.
    

