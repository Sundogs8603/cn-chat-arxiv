# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Parallel Neurosymbolic Integration with Concordia.](http://arxiv.org/abs/2306.00480) | 康科迪亚框架是一个支持各种概率理论，克服了现有技术的限制的并行的神经符号结构，成功应用于集合活动检测、实体链接和推荐任务，提高了最新准确性。 |
| [^2] | [Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning.](http://arxiv.org/abs/2306.00477) | 本研究尝试实现在预训练语言模型中运用可逆模型实现高效的微调，并发现在初始化微调时保留PLM的起点非常重要。 |
| [^3] | [A big data approach towards sarcasm detection in Russian.](http://arxiv.org/abs/2306.00445) | 本文介绍了一种针对俄语文本的大数据方法，旨在检测文本中的讽刺。作者提出了一组可用于屈折和文本合成的算法，并在一个公开可用的网络服务中实现。对这些算法的性能进行了测试和比较，同时用于估计俄语不同词性的形态变异性和复杂性。 |
| [^4] | [Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift.](http://arxiv.org/abs/2306.00427) | 连续学习中存在一种特殊形式的灾难性遗忘——越界遗忘，当给定类别引入类内分布转移时，它会显着削弱该类别的连续学习方法的识别准确率。 |
| [^5] | [Challenges and Remedies to Privacy and Security in AIGC: Exploring the Potential of Privacy Computing, Blockchain, and Beyond.](http://arxiv.org/abs/2306.00419) | 本文系统地回顾了AIGC的概念、分类以及基础技术，详细阐述了在AIGC中可能存在的隐私和安全挑战，提供了隐私计算和区块链等技术相应的解决方案，并在此基础上确定了未来的研究方向。 |
| [^6] | [Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction.](http://arxiv.org/abs/2306.00418) | 本文提出了一种新的方法来控制标记级的生成、提高原始学习和减少错误，其中包括蒙特卡洛dropout、边缘非似然学习和最小化熵。在四个公共数据集上的广泛实验表明，该方法有效地提高了情感四元组预测的性能。 |
| [^7] | [Controllable Motion Diffusion Model.](http://arxiv.org/abs/2306.00416) | 该论文提出了可控运动扩散模型（COMODO）框架，通过自回归运动扩散模型（A-MDM）生成高保真度、长时间内的运动序列，以实现在响应于时变控制信号的情况下进行实时运动合成。 |
| [^8] | [A Survey on Fairness-aware Recommender Systems.](http://arxiv.org/abs/2306.00403) | 本综述对现有的公正感知推荐系统方法和实践进行了总结分析，详细介绍了相关的概念定义、分类、方法和需解决的问题，并提出了未来的研究方向。 |
| [^9] | [Teacher Agent: A Non-Knowledge Distillation Method for Rehearsal-based Video Incremental Learning.](http://arxiv.org/abs/2306.00393) | 提出了一种教师代理方法，能够从先前学习的知识中生成高质量样本的数据集，从而使学生网络可以从样本的多种观点中进行学习，在标准基准上优于基于知识蒸馏的方法。 |
| [^10] | [The Survey, Taxonomy, and Future Directions of Trustworthy AI: A Meta Decision of Strategic Decisions.](http://arxiv.org/abs/2306.00380) | 本文提出了一个新方法来解决在使用AI系统进行决策时的可信问题。该方法引入了一个包括表达、真实和基本水平的不同信任级别的TAI分类系统或框架，使用十个维度来衡量信任，并提供了现有TAI研究的调查和元分析，还确定了TAI研究的未来方向和潜在应用。 |
| [^11] | [Developing and Building Ontologies in Cyber Security.](http://arxiv.org/abs/2306.00377) | 通过比较不同本体论，该论文提供了有关网络安全本体论领域的未来方向。 |
| [^12] | [CFL: Causally Fair Language Models Through Token-level Attribute Controlled Generation.](http://arxiv.org/abs/2306.00374) | 本文提出了一种使用因果平均处理效应（ATE）分数和反事实增强方法的CFL体系架构，通过插入式方式对预训练的LM进行解毒，实现了因果公平的语言模型。此方法不会对模型困惑产生太大影响，还减轻了意外的偏置问题。 |
| [^13] | [On the Equivalence of Consistency-Type Models: Consistency Models, Consistent Diffusion Models, and Fokker-Planck Regularization.](http://arxiv.org/abs/2306.00367) | 本研究建立了一致性模型，一致性扩散模型和Fokker-Planck正则化之间的理论联系，为不同目标增强扩散模型提供了更全面和包容性的框架的可能性。 |
| [^14] | [Addressing Negative Transfer in Diffusion Models.](http://arxiv.org/abs/2306.00354) | 本文从多任务学习角度出发，研究了扩散训练中的负迁移现象，提出了利用正则化技术增强扩散训练的方法，以减轻负迁移并提高去噪任务的性能。 |
| [^15] | [Combining Explicit and Implicit Regularization for Efficient Learning in Deep Networks.](http://arxiv.org/abs/2306.00342) | 本文提出了一种显式正则化方法，与隐式正则化结合，可以使单层网络实现与深度线性网络相当的低秩近似和泛化误差，使深度不再是学习的必要条件。 |
| [^16] | [Approximate inference of marginals using the IBIA framework.](http://arxiv.org/abs/2306.00335) | 提出了一种新的基于IBIA框架的算法，通过将PGM转化为有界团大小的SLCTF序列，并使用启发式置信度更新算法来推导边缘。实验结果表明，该算法比当前最先进的技术更加有效和高效。 |
| [^17] | [A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models.](http://arxiv.org/abs/2306.00331) | 该论文提出了一种基于小型模型的多维深度结构化状态空间方法，在语音增强中表现出竞争力，由于使用的参数比常规卷积层少，提出的TF域S4模型大小减少了78.6％，同时在数据增广后获得3.15的PESQ得分。 |
| [^18] | [Thought Cloning: Learning to Think while Acting by Imitating Human Thinking.](http://arxiv.org/abs/2306.00323) | 本论文提出了一种新的模仿学习框架“思维克隆”，通过学习人类的思维来训练AI代理，以在泛化、探索、规划等能力方面实现更好的表现。 |
| [^19] | [FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization.](http://arxiv.org/abs/2306.00317) | 该论文提出了一种新的基于元素除法的可学习舍入机制FlexRound，使得后训练量化时更好地重构每个层或块的输出，并且能够学习公共量化网格大小以及每个预训练权重的不同比例尺。 |
| [^20] | [Adversarial-Aware Deep Learning System based on a Secondary Classical Machine Learning Verification Approach.](http://arxiv.org/abs/2306.00314) | 传统机器学习模型可以作为深度学习模型的辅助验证系统，可以有效防御各种对抗攻击，成功率高达97.32%。 |
| [^21] | [Transformers learn to implement preconditioned gradient descent for in-context learning.](http://arxiv.org/abs/2306.00297) | 本论文通过研究线性transformers在训练过程中的全局最小值，证明了对于一个注意力层，transformers能够实现一次预处理梯度下降，而对于一个$k$个注意力层的transformer，它可以实现多次预处理梯度下降。 |
| [^22] | [EMOTE: An Explainable architecture for Modelling the Other Through Empathy.](http://arxiv.org/abs/2306.00295) | EMOTE是一个可解释的建模架构，用于模拟另一个智能体的动作价值函数，通过共情的想象网络将其他智能体的观察状态转换成可解释的“共情状态”。该方法在多智能体游戏中取得了比现有方法更好的性能。 |
| [^23] | [Training-free Neural Architecture Search for RNNs and Transformers.](http://arxiv.org/abs/2306.00288) | 本文研究了适用于语言模型任务的循环神经网络和Transformer的无训练神经网络结构搜索算法，并开发了一个新的性能预测指标hidden covariance，可显著优化现有指标。 |
| [^24] | [Doubly Robust Self-Training.](http://arxiv.org/abs/2306.00265) | 本文提出了一种双重稳健自我训练算法，可以在伪标签不准确和完全准确时分别采取不同的训练策略，实现有效的半监督学习。实验结果表明，该算法在ImageNet和nuScenes数据集上均比标准自我训练总结更好。 |
| [^25] | [BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned Approximations.](http://arxiv.org/abs/2306.00249) | 本文提出了一种叫做BetaZero的方法，它是一种使用学习近似算法的置信状态规划算法，可以用于解决长时间跨度的POMDP问题。 |
| [^26] | [TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest.](http://arxiv.org/abs/2306.00248) | 本文介绍了Pinterest推荐系统的架构和TransAct模型。TransAct是一个从用户实时活动中提取短期偏好的序列模型。本文还介绍了通过混合排名方法结合直接在实时用户活动上学习和在较长时间段内学习批量用户表示的优点。 |
| [^27] | [Using Visual Cropping to Enhance Fine-Detail Question Answering of BLIP-Family Models.](http://arxiv.org/abs/2306.00228) | 本文研究如何通过使用视觉裁剪来提高BLIP模型在细节问题上的表现能力。 |
| [^28] | [Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images.](http://arxiv.org/abs/2306.00219) | 本论文提出了一种名为扩散画笔的基于潜在扩散模型的AI图像微调工具，可以有效地根据目标区域修改AI合成图像并保留原始上下文。与其他最先进的图像修复技术进行比较，该方法在用户研究中表现出更好的可用性和有效性。 |
| [^29] | [Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2306.00212) | 本文研究了在有约束的马尔可夫博弈中，多个智能体通过最大化总体奖励并在对总体效用的期望值上设置约束条件进行多智能体强化学习。通过泛化Lagrangian策略优化，采用基于占用测量的方法，并利用置信上限强化学习算法，实现对约束的处理。 |
| [^30] | [Toward Understanding Why Adam Converges Faster Than SGD for Transformers.](http://arxiv.org/abs/2306.00204) | 本文研究了Adam在Transformer的训练中为什么比SGD更快收敛，提出了方向锐度的概念并通过比较证明了SGD相对于自适应算法具有更差的方向锐度，进而在理论上证明只需要裁剪一小部分坐标即可提高SGD的表现。 |
| [^31] | [SSL-CPCD: Self-supervised learning with composite pretext-class discrimination for improved generalisability in endoscopic image analysis.](http://arxiv.org/abs/2306.00197) | 本论文提出了一种名为SSL-CPCD的新方法，通过针对补丁级别的实例组区分和跨类别变化的惩罚，在使用自我监督学习提高内窥镜图像分析泛化性能方面取得了重要的进展。 |
| [^32] | [Diffused Redundancy in Pre-trained Representations.](http://arxiv.org/abs/2306.00183) | 本文发现预训练表示中存在一定程度的扩散冗余，即随机选择一部分神经元与全部神经元相似度高且性能相似，可用于降低实际部署中的计算和内存成本，同时保持相当的性能水平。 |
| [^33] | [FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow.](http://arxiv.org/abs/2306.00180) | 本文提出了一种利用可微分渲染将帧间光流映射到三维场景流以同步重建相机位姿和三维神经场表示的方法。这种方法不需要精确相机位姿，可在真实世界的视频数据上进行全自监督训练。 |
| [^34] | [Automated Annotation with Generative AI Requires Validation.](http://arxiv.org/abs/2306.00176) | 本文探讨了利用生成式大型语言模型进行文本注释的自动化流程，强调必须针对人类生成的标签验证它们的性能，验证结果表明LLM的性能很有前途，但高度依赖于数据集和注释任务类型。 |
| [^35] | [A Comparison of Decision Algorithms on Newcomblike Problems.](http://arxiv.org/abs/2306.00175) | 本文比较了三种决策算法在新康姆类问题上的表现，结果发现无时间的决策理论在这些问题上具有明显优势。 |
| [^36] | [Information Fusion via Symbolic Regression: A Tutorial in the Context of Human Health.](http://arxiv.org/abs/2306.00153) | 本教程论文介绍了符号回归技术，以人类健康和营养领域的实际应用为例，将各种人体测量指标融合成一个简单的数学表达式以估算体脂百分比。 |
| [^37] | [On the Expressive Power of Neural Networks.](http://arxiv.org/abs/2306.00145) | 本文研究了神经网络的表达能力，证明了某些问题在之前的研究中未得到解决，并提出了新问题的回答，包括广而浅的ReLU网络不能被深而窄的ReLU网络很好地逼近等。 |
| [^38] | [The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture.](http://arxiv.org/abs/2306.00114) | 该论文提出了一个时间补丁数据集，包含了加拿大农田的多时相遥感影像。该数据集是手动经过确认和筛选的高分辨率地理参考图像，覆盖四个农作物生产年度和五个月份。这个数据集可以用于提高土地覆盖分类的准确性。 |
| [^39] | [MuseCoco: Generating Symbolic Music from Text.](http://arxiv.org/abs/2306.00110) | MuseCoco是一种从文本描述中生成符号音乐的系统，具备高效和灵活等特点，为音乐家提供了更好的音乐生成方式。 |
| [^40] | [MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training.](http://arxiv.org/abs/2306.00107) | 提出了一个带有大规模自监督训练的音乐理解模型MERT，利用了教师模型并采用了一种优于传统的语音和音频方法的组合方式。 |
| [^41] | [AI Imagery and the Overton Window.](http://arxiv.org/abs/2306.00080) | 基于人工智能的文本到图像生成技术的快速增长让手工艺术品和AI生成图像区分日渐困难。然而提高人类生活和工作标准以及利用一群人来充实另一群人之间寻求平衡是关键。该领域面临着被AI基础设施接管的风险，同时存在身份盗窃、数据洗白等问题。 |
| [^42] | [Shadows of quantum machine learning.](http://arxiv.org/abs/2306.00061) | 量子机器学习模型需要使用量子计算机进行评估，但我们提出在训练完后，使用量子计算机生成一个经典阴影模型来计算函数的经典计算近似，避免了对量子计算机的需求。 |
| [^43] | [Lottery Tickets in Evolutionary Optimization: On Sparse Backpropagation-Free Trainability.](http://arxiv.org/abs/2306.00045) | 本文证明了进化策略算法具有高度稀疏可训练的初始参数，并比较了与梯度下降算法稀疏训练的差异。进化策略算法可以探索各种不同的、平坦的局部最优解，并且不会保留线性模式的连接性。 |
| [^44] | [Space Net Optimization.](http://arxiv.org/abs/2306.00043) | SNO是一种新颖的元启发式算法，其使用空间网机制来绘制解空间的景观，可以使用大部分搜索解提供的信息来替代近似猜测搜索，它在单目标有约束问题中的表现优于其他元启发式算法。 |
| [^45] | [Assessing the Generalizability of a Performance Predictive Model.](http://arxiv.org/abs/2306.00040) | 本研究提出一种工作流程来评估性能预测模型的泛化能力，可以在不同的基准测试套件上进行验证。 |
| [^46] | [FedCSD: A Federated Learning Based Approach for Code-Smell Detection.](http://arxiv.org/abs/2306.00038) | 本文提出了一种名为FedCSD的基于联邦学习的代码异味检测方法，可以在保护数据隐私的同时，让组织协作训练联邦学习模型。该方法在三个实验中分别使用不同的数据集，实现了高精度的检测效果，并且比集中式和交叉验证方法具有更好的性能表现。 |
| [^47] | [Symmetry-Aware Robot Design with Structured Subgroups.](http://arxiv.org/abs/2306.00036) | 提出了一种利用对称性搜索的机器人设计方法，使用二面体群的子群表示对称性，并在结构化子群中搜索最优对称性，并在此对称性下完成机器人设计。理论证明了该方法能在保持设计空间的同时，设计出高效的对称机器人，实证评估也表明其具有卓越的效率和通用性。 |
| [^48] | [CodeTF: One-stop Transformer Library for State-of-the-art Code LLM.](http://arxiv.org/abs/2306.00029) | CodeTF是一个开源的Transformer库，提供了包括预训练的Code LLM模型和标准化接口等一系列功能，可以轻松地将最先进的Code LLM模型应用于各种软件工程任务中。 |
| [^49] | [Disentangling and Operationalizing AI Fairness at LinkedIn.](http://arxiv.org/abs/2306.00025) | 领英使用公平性框架来实现AI公平性，该框架将AI的公平性分离出公平待遇和公平产品期望，并提供了操作指南来实现平等AI待遇并配合产品平等策略。 |
| [^50] | [Predicting Heart Disease and Reducing Survey Time Using Machine Learning Algorithms.](http://arxiv.org/abs/2306.00023) | 本研究使用机器学习算法对美国CDC的心脏病调查进行了准确性调查，确定了最相关的问题子集，并证明使用有限的问题集仍然可以保持高的预测准确率，并显著减少调查时间。 |
| [^51] | [Explaining Hate Speech Classification with Model Agnostic Methods.](http://arxiv.org/abs/2306.00021) | 本文旨在解释仇恨言论分类，采用双向变形器模型BERT进行预测，并提供后验的模型无关和代理可解释性方法，以支持模型的解释性并防止模型偏差。 |
| [^52] | [GPT4GEO: How a Language Model Sees the World's Geography.](http://arxiv.org/abs/2306.00020) | 研究调查了GPT-4的地理知识水平，并探讨其在地理数据分析等领域的应用潜力。 |
| [^53] | [Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale.](http://arxiv.org/abs/2306.00017) | 本文提出结合符号表示和自下而上的逆向工程的方法，解决大规模语言模型在真正语言理解上的局限性，实现可解释的、语言无关的LLMs。 |
| [^54] | [Incorporating Domain Knowledge in Deep Neural Networks for Discrete Choice Models.](http://arxiv.org/abs/2306.00016) | 本文提出了一种框架，将领域知识和先验信念纳入离散选择模型（DCM）的深度神经网络（DNN）中，以扩展数据驱动方法在DCM中的潜力。所提出的方法既具有可解释性，又保留了DCM的优点和DNNs的灵活性，并优于现有的最先进方法。 |
| [^55] | [GraphCleaner: Detecting Mislabelled Samples in Popular Graph Learning Benchmarks.](http://arxiv.org/abs/2306.00015) | 该论文提出了一种后期方法GraphCleaner，用于在流行的图学习基准数据集中检测和纠正错误标注节点。GraphCleaner 组合了合成错误标注数据集的生成和邻域感知错误标注检测两种新颖思想，实证评估结果表明其平均F1得分提高了0.14。 |
| [^56] | [Graph Neural Network for spatiotemporal data: methods and applications.](http://arxiv.org/abs/2306.00012) | 本文综述了基于图神经网络的时空数据方法及应用，并提供了一个GNN的分类及其应用领域。这个综述突出了使用GNN分析时空数据的强大工具及其局限性，并指出未来研究方向和目前面临的挑战。 |
| [^57] | [Explainability in Simplicial Map Neural Networks.](http://arxiv.org/abs/2306.00010) | 本文提出了简单形式映射神经网络（SMNN）的训练过程和替代凸多面体的方法，并且首次引入了 SMNN 的可解释性能力。 |
| [^58] | [Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection.](http://arxiv.org/abs/2306.00006) | 本文针对图形异常监测数据集中存在的一类同型现象，提出了一种新的无监督异常评分度量——当前节点亲和力，并通过学习量身定制的节点表示，实现了截断亲和力最大化（TAM）方法，优化在原始图形结构上进行，能够有效进行双重One-Class的GAD。 |
| [^59] | [Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning.](http://arxiv.org/abs/2306.00003) | 本研究提出了一种基于监督式注意力多实例学习的方法，可以自动分析超声图像，实现AS的精确筛查且精度优于传统机器学习和深度学习方法。 |
| [^60] | [Decision-Oriented Dialogue for Human-AI Collaboration.](http://arxiv.org/abs/2305.20076) | 该论文探讨了一类以决策为导向的人机对话任务，以及在会议论文审稿人分配、城市多步行程规划和旅行计划协商等场景中，人工智能助手和用户不同的能力如何结合以达到最佳决策。论文通过构建对话环境并进行人机对话收集数据，发现当前人工智能助手在此类任务中的局限性。 |
| [^61] | [Beam Tree Recursive Cells.](http://arxiv.org/abs/2305.19999) | 本论文提出了一种支持反向传播的递归神经网络框架——束搜索递归单元（BT-Cell），用于扩展递归神经网络，实现对潜在结构的感知；此外，我们提出了一种放松束搜索中硬前k算子的方法，以实现更好的梯度信号传递。在评估中发现，BT-Cell在合成和实际数据的多个具有结构敏感性的任务中表现优异。 |
| [^62] | [InGram: Inductive Knowledge Graph Embedding via Relation Graphs.](http://arxiv.org/abs/2305.19987) | InGram是一种新的归纳式知识图谱补全方法，可以在推理时生成新关系和实体的嵌入，并使用注意力机制汇总邻居嵌入生成关系和实体嵌入。该方法在多个基准数据集上的性能优于现有的基准方法。 |
| [^63] | [A Survey on Large Language Models for Recommendation.](http://arxiv.org/abs/2305.19860) | 本综述介绍了基于大语言模型的推荐系统，提出了判别式LLMs和生成式LLMs两种模型范式，总结了这些模型的最新进展，强调了该领域的挑战和研究方向。 |
| [^64] | [Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards.](http://arxiv.org/abs/2305.19599) | 本文提出了FineRewards，通过引入细粒度的语义奖励，即标题奖励和SAM奖励，来改进文本到图像扩散模型中文本和图像之间的对齐。 |
| [^65] | [Offline Meta Reinforcement Learning with In-Distribution Online Adaptation.](http://arxiv.org/abs/2305.19529) | 本文提出了一种带有不确定性量化的内部分布在线适应(IDAQ)的框架，利用策略后验集合和信念更新网络量化策略不确定性并生成上下文信息来处理新任务，在离线元强化学习上具有竞争性表现。 |
| [^66] | [Hierarchical Graph Generation with $K^2$-trees.](http://arxiv.org/abs/2305.19125) | 本文介绍了一种基于$K^2$-树的图生成方法，该方法可以实现紧凑生成，并同时捕获图的内在分层结构。通过提出顺序$K^2$-树表示和引入基于Transformer的架构，本文进一步改进了这种方法。实验表明，该方法在图生成方面具有卓越的表现。 |
| [^67] | [Towards single integrated spoofing-aware speaker verification embeddings.](http://arxiv.org/abs/2305.19051) | 本研究旨在开发单一综合防欺骗说话人验证嵌入，在欺骗和非目标说话人输入方面达到卓越性能，相较于现有的嵌入方案，该嵌入通过多阶段训练和复制合成技术来提高性能，并在 SASV2022 挑战协议中取得了1.06%的SASV-EER效果。 |
| [^68] | [Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation.](http://arxiv.org/abs/2305.18885) | 本文提出了一种面向多准则推荐的标准偏好感知轻量图卷积网络，该方法结合了MC扩展图，可以准确地捕捉用户的标准偏好，并进一步将用户对各个标准的偏好合并到最终的推荐列表中。 |
| [^69] | [Task-Equivariant Graph Few-shot Learning.](http://arxiv.org/abs/2305.18758) | 本文提出了一种任务等变图Few-shot学习（TEG）框架，利用图神经网络的等变性质来使模型学习可转移的任务适应策略。该方法在各种Few-shot分类基准上展示了最先进的性能。 |
| [^70] | [Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic Forecasting.](http://arxiv.org/abs/2305.18687) | 本文提出了一种称为GRAM-ODENNs的新型神经网络结构，用于解决当前图ODE模型的局限性，包括局部模式的忽略和缺乏动态语义边缘，用于更准确的长程交通预测。 |
| [^71] | [Controllable Path of Destruction.](http://arxiv.org/abs/2305.18553) | 本文介绍了可控毁灭路径方法，该方法是一种自我监督的迭代生成器学习方法，通过向修复轨迹的状态-动作对添加条件输入来实现可控性。 |
| [^72] | [A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning.](http://arxiv.org/abs/2305.18400) | 提出了一个元学习框架，用于调整可信联邦学习保护机制的参数，以在隐私泄露、效用损失和效率降低之间进行权衡。 |
| [^73] | [Neural Task Synthesis for Visual Programming.](http://arxiv.org/abs/2305.18342) | 该论文提出了一种基于神经符号技术的可视化编程任务合成方法NeurTaskSyn。该方法能够针对规范中给出的解决方案代码所需要的编程概念和对可视化任务的限制，自动生成编程任务。 |
| [^74] | [Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers.](http://arxiv.org/abs/2305.18256) | 本文提出了一个名为HyNT的框架，用于学习超关系型知识图的表示，包括数值文字。该框架使用上下文Transformer和预测Transformer，通过学习三元组和其限定词之间的相关性以及数值信息来获得模型。 |
| [^75] | [InDL: A New Datasets and Benchmark for In-Diagram Logic Interpreting based on Visual Illusion.](http://arxiv.org/abs/2305.17716) | 本文提出了一个基于视错觉的独特数据集InDL，用于测试和评估深度学习模型的图中逻辑解释能力。利用几何光学视错觉，建立可比性框架用于阐明模型可能存在的缺陷和提供改进模型的洞察力。 |
| [^76] | [Counterfactual Formulation of Patient-Specific Root Causes of Disease.](http://arxiv.org/abs/2305.17574) | 本文提出了一种针对疾病患者个体的根本原因的新公式，可以用于自动从数据中检测根本原因，并考虑了噪声标签和疾病流行率等因素，同时具有快速计算的优势。 |
| [^77] | [A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU.](http://arxiv.org/abs/2305.17473) | 本文全面概括了深度学习模型的类型和应用，比较分析了各个模型的结构、优点和局限性，有助于选择和设计深度学习模型。 |
| [^78] | [Attention Schema in Neural Agents.](http://arxiv.org/abs/2305.17375) | 本文研究了神经智能中的注意力模式，并提出了注意力模式理论（AST）。作者发现将AS实现为一种循环内部控制的智能体效果最佳，这一理论为应用与改进神经智能提供了新思路。 |
| [^79] | [Heterogeneous Value Evaluation for Large Language Models.](http://arxiv.org/abs/2305.17147) | 本文提出了一种自动对齐评估方法A2EHV，采用异质价值系统，并基于价值合理性和社会价值定向框架评估代理人行为的社会偏好，结果表明比传统对齐方法更合理。 |
| [^80] | [Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory.](http://arxiv.org/abs/2305.17144) | 本文提出了Ghost in the Minecraft (GITM)框架，利用大型语言模型与基于文本的知识和记忆，创造了一种在Minecraft中具备通用能力的智能体，可在以文本为基础的复杂编程环境中熟练导航。 |
| [^81] | [NLP Reproducibility For All: Understanding Experiences of Beginners.](http://arxiv.org/abs/2305.16579) | 通过对93名NLP初学者的调查，发现研究作者提供完整文档、更好的代码实践和更易于获取的数据文件是初学者成功复现最近NLP论文结果的关键，建议NLP研究人员注重这些方面，更好地支持初学者。 |
| [^82] | [Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art.](http://arxiv.org/abs/2305.16259) | 本文简要概述了长文本的神经自然语言处理的现状，主要包括文档分类和摘要，涵盖了情感分析，同时还探讨了长文本NLP的主要挑战、问题和解决方案。 |
| [^83] | [Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation.](http://arxiv.org/abs/2305.14330) | 本文引入了一个新的框架——DirecT2V，利用大型语言模型作为导演，从一个抽象的用户提示中生成零样本文本到视频生成的连贯且连贯的视频。该框架使用LLM导演将用户输入分为每一帧的提示，通过值映射和双softmax过滤器来保持时间一致和防止对象折叠。 |
| [^84] | [On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation.](http://arxiv.org/abs/2305.10930) | 零样本多语言神经机器翻译容易出现“离谱问题”，本文提出的简单且有效的算法LAVS可以通过增加语言之间的KL分歧显著降低这个问题。 |
| [^85] | [Incremental Causal Graph Learning for Online Unsupervised Root Cause Analysis.](http://arxiv.org/abs/2305.10638) | 本文提出了CORAL，一种用于在线无监督根本原因分析的新框架，可以自动触发该过程并增量更新模型，包括三个主要部分：触发点检测，增量因果图学习和基于网络传播的根本原因定位。 |
| [^86] | [Automatic Photo Orientation Detection with Convolutional Neural Networks.](http://arxiv.org/abs/2305.10319) | 本论文使用CNN解决了照片方向检测的问题，并在数据集上显著提高性能。使用Guided Backpropagation获得了CNN检测方向的见解。 |
| [^87] | [HICO-DET-SG and V-COCO-SG: New Data Splits to Evaluate Systematic Generalization in Human-Object Interaction Detection.](http://arxiv.org/abs/2305.09948) | 本论文提出了两个新的HOI检测数据拆分，旨在评估系统性泛化。在新的数据拆分上测试结果表明，HOI检测模型对于未见过的对象和交互组合的泛化十分困难。 |
| [^88] | [Reducing the Cost of Cycle-Time Tuning for Real-World Policy Optimization.](http://arxiv.org/abs/2305.05760) | 本研究提出了一种新方法，可基于周期时间设置超参数，使得PPO和SAC在广泛的循环时间范围内进行学习，同时实现了接近耗时的在线超参数调整获得的性能。 |
| [^89] | [Graph Masked Autoencoder for Sequential Recommendation.](http://arxiv.org/abs/2305.04619) | 提出了一种简单而有效的基于图遮盖自编码器的序列推荐系统，它使用基于图的注意力机制暴露出带有遮盖的项目序列，自适应动态提取全局项目转换信息进行自监督增强，在具有较少标记样本的情况下始终比最先进的序列推荐方法表现出更好的性能，而且对数据损坏和缺失情况具有鲁棒性。 |
| [^90] | [How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?.](http://arxiv.org/abs/2305.01555) | 本文通过使用GPT-3.5模型在少样本关系抽取中，实现在四个不同数据集上的新的最优性能，并提出了与任务相关的指导说明和约束模式下的数据生成方法。 |
| [^91] | [Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation.](http://arxiv.org/abs/2305.00955) | 本文调查研究了利用人类反馈改进自然语言生成方面的最近研究，包括对反馈的全面形式化、反馈的格式和目的的描述，和提出使用反馈的两种方法。我们还讨论了用于人类反馈数据收集的现有数据集，和相关的担忧。 |
| [^92] | [Impact-Oriented Contextual Scholar Profiling using Self-Citation Graphs.](http://arxiv.org/abs/2304.12217) | 本研究提出了一个名为GeneticFlow的基于自引图的学者剖析工具，能够满足学者特征剖析中的三个基本要求，即结构化背景、学者为中心和丰富的进化，在科学奖项推理真实任务中表现出色。 |
| [^93] | [Brain-Inspired Spiking Neural Network for Online Unsupervised Time Series Prediction.](http://arxiv.org/abs/2304.04697) | 本论文提出了一种新的连续学习无监督循环尖峰神经网络模型，可以高效、节能地进行在线时间序列预测，并且通过随机延迟嵌入和使用底层动态系统中介数中心性最高的神经元的膜电位进行重构。 |
| [^94] | [Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge.](http://arxiv.org/abs/2304.03392) | 该论文提出了一种采用机器学习和领域知识进行个性化数字健康行为变革干预的系统，其利用反事实例子进行特征控制以预测干预效果并优化干预效果。 |
| [^95] | [End-to-End Diffusion Latent Optimization Improves Classifier Guidance.](http://arxiv.org/abs/2303.13703) | 本文提出了一种新的分类器引导方法DOODL，它可以通过针对真实生成的像素上预训练分类器的梯度优化扩散潜变来实现即插即用的指导，并在计算和人类评估度量上优于一步分类器指导。 |
| [^96] | [Aux-Drop: Handling Haphazard Inputs in Online Learning Using Auxiliary Dropouts.](http://arxiv.org/abs/2303.05155) | 本文介绍了一种名为Aux-Drop的策略，适用于在线学习，能够处理混乱的输入特征。通过调整常规的丢失正则化方案，确保最终输出受到这些特征混乱出现的最小影响，有助于构建更健壮有效的在线学习系统。 |
| [^97] | [Query-Utterance Attention with Joint modeling for Query-Focused Meeting Summarization.](http://arxiv.org/abs/2303.04487) | 本文提出了一种基于查询-话语注意力和联合建模的查询感知框架，它使用密集检索模块计算话语级别与查询的相关性，并将标记级别的查询关联性和话语级别的查询关联性结合起来，实现生成一个更与查询相关的摘要。经过对两个基准数据集上的测试，表明该方法优于现有的QFMS模型。 |
| [^98] | [Almanac: Retrieval-Augmented Language Models for Clinical Medicine.](http://arxiv.org/abs/2303.01229) | Almanac是一个带有检索能力的大型语言模型框架，为临床医学的指南和治疗建议提供了事实性、完备性和安全性的增强。该模型有望成为临床决策过程中的有效工具。 |
| [^99] | [Bootstrapping Parallel Anchors for Relative Representations.](http://arxiv.org/abs/2303.00721) | 该论文提出了一种引导方法，通过已知的集合发现新的并行锚点，以克服相对表示式中获取并行锚点困难的问题，能够用于不同域之间的语义对应，取得竞争性结果。 |
| [^100] | [Let's have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations.](http://arxiv.org/abs/2302.13817) | 本文讨论了聊天机器人的历史概述以及ChatGPT背后的技术，强调了它在医疗保健、教育和研究中的潜在应用，并指出了其隐私和道德方面的担忧以及当前版本的重要限制。 |
| [^101] | [Hiding Data Helps: On the Benefits of Masking for Sparse Coding.](http://arxiv.org/abs/2302.12715) | 本文研究了稀疏编码中学习得到的大于实际字典的情况下，噪声会导致标准的字典学习目标函数无法恢复出实际字典的问题，提出了通过遮盖数据的方法进行可靠的字典恢复适用于多种信号模态。 |
| [^102] | [K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs.](http://arxiv.org/abs/2302.11996) | 本文提出了一种名为K-SHAP的算法，来解决多个智能体保持匿名且仅有状态-动作对的情况下学习智能体决策的问题。 |
| [^103] | [Robust and Versatile Bipedal Jumping Control through Reinforcement Learning.](http://arxiv.org/abs/2302.09450) | 本论文提出了一个通过强化学习训练的智能双足机器人跳跃控制框架，可以执行各种跳跃任务，具有多用途和稳健性，且可直接转移到真实机器人应用。 |
| [^104] | [Level Generation Through Large Language Models.](http://arxiv.org/abs/2302.05817) | 该研究探讨了使用大型语言模型(LLMs)生成Sokoban游戏关卡，并发现随着数据集大小的增加，LLMs的性能得到了很大的提高。未来工作的前景也被讨论了。 |
| [^105] | [A Survey on Causal Reinforcement Learning.](http://arxiv.org/abs/2302.05209) | 本文综述了因果关系强化学习的最新进展和未来研究方向，将现有的CRL方法分为两类，并讨论了因果关系和RL之间的关系。 |
| [^106] | [Better Diffusion Models Further Improve Adversarial Training.](http://arxiv.org/abs/2302.04638) | 本文证明了更好的扩散模型可以进一步提高对抗训练的性能，通过采用最新的扩散模型，我们训练的模型仅使用生成的数据就在RobustBench上实现了最佳性能，并在CIFAR-10和CIFAR-100数据集上分别提高了$+4.58\%$和$+8.03\%$的性能。 |
| [^107] | [Bag of Tricks for Training Data Extraction from Language Models.](http://arxiv.org/abs/2302.04460) | 本文总结了一些技巧用于改进语言模型训练数据提取，提出了在文本生成和文本排名中可以使用的技巧，实验证明这些技巧对于提高训练数据提取的效果非常重要。 |
| [^108] | [The SSL Interplay: Augmentations, Inductive Bias, and Generalization.](http://arxiv.org/abs/2302.02774) | 本论文通过分析数据增强、网络架构和训练算法的相互作用，研究了预训练和下游任务的泛化性能，并为SSL从业人员提供了一些见解。 |
| [^109] | [Oscillation-free Quantization for Low-bit Vision Transformers.](http://arxiv.org/abs/2302.02210) | 研究发现，可学习比例因子加剧了权重振荡，本文提出三种技术以解决这个问题，并在多个基准测试上显著提高了模型的性能。 |
| [^110] | [Grounding Language Models to Images for Multimodal Inputs and Outputs.](http://arxiv.org/abs/2301.13823) | 该论文提出一种有效的方法，将仅处理文本的语言模型与图像进行联系，使其能够处理任意交错的图像和文本数据，并生成与检索图像交错的自由形式文本。该方法在环境相关的图像检索和多模态对话等任务中表现十分优异，是利用预训练语言模型解决视觉场景下交互问题的有效解决方案。 |
| [^111] | [Retrosynthetic Planning with Dual Value Networks.](http://arxiv.org/abs/2301.13755) | PDVN是一种新的在线训练算法，它在逆向合成规划中利用双价值网络优化完整的路线，成功率和效率上均优于现有方法。 |
| [^112] | [Retiring $\Delta$DP: New Distribution-Level Metrics for Demographic Parity.](http://arxiv.org/abs/2301.13443) | 本文提出两种新公平度量方法：概率密度函数曲线之间区域（ABPC）和累积密度函数曲线之间区域（ABCC），实现人口统计特征的公平机器学习 |
| [^113] | [Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off.](http://arxiv.org/abs/2301.13083) | 本文研究了神经代理语言学习与通信框架，成功复制了语序/格标交换这一广泛存在的语言普遍规律，实现了更真实的语言演变模拟，并帮助我们更好地理解影响语言普遍规律的认知和社会因素。 |
| [^114] | [Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons.](http://arxiv.org/abs/2301.11270) | 该论文提供了带有人类反馈强化学习问题的理论框架，证明了最大似然估计在Bradley-Terry-Luce和Plackett-Luce模型下收敛。此外，提出了在一定的覆盖假设下，基于悲观估计的MLE提供了性能更好的策略。在证明了真实MLE和以成对比较形式替代的备选MLE都可以在PL模型下收敛的同时，也表明了真实MLE的高效性。这些结果为RLHF算法提供了新的见解，并统一了RLHF问题和IRL问题。 |
| [^115] | [A Survey on In-context Learning.](http://arxiv.org/abs/2301.00234) | 本文调查和总结了上下文学习(ICL)的进展和挑战，ICL已成为自然语言处理(NLP)的新范式，探索ICL以评估和推广大型语言模型(LLM)的能力已成为一种新趋势。本文提出了ICL的正式定义，并总结了高级技术，最后讨论了ICL的挑战以及进一步研究的潜在方向。 |
| [^116] | [Reasoning with Language Model Prompting: A Survey.](http://arxiv.org/abs/2212.09597) | 本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。 |
| [^117] | [Local Branching Relaxation Heuristics for Integer Linear Programs.](http://arxiv.org/abs/2212.08183) | 本论文提出了一种用于整数线性规划的 LB-RELAX 及其变体的启发式算法，通过使用线性规划松弛值来选择邻域，加快了计算速度而保持了较传统的 LB 算法相同的求解质量。实验结果表明，该算法达到了最佳的任意时间性能。 |
| [^118] | [Offline Reinforcement Learning with Closed-Form Policy Improvement Operators.](http://arxiv.org/abs/2211.15956) | 本文提出了基于行为约束的离线强化学习中的闭合形式策略改进算子，该算子将行为策略建模为高斯混合，利用LogSumExp的下界和Jensen不等式克服了优化困难，能有效处理实际数据集中的异构策略。 |
| [^119] | [Reward Gaming in Conditional Text Generation.](http://arxiv.org/abs/2211.08714) | 在条件文本生成中，使用强化学习（RL）进行训练时，噪声、自然发生的虚假相关性和协变量漂移可能会导致不良模式被错误地赋予高奖励值，这可能会导致奖励博弈，需要解决。 |
| [^120] | [MT Metrics Correlate with Human Ratings of Simultaneous Speech Translation.](http://arxiv.org/abs/2211.08633) | 本文表明了离线指标与人工连续评分之间存在良好的相关性，可以在同时翻译模式中可靠地用于机器翻译的评估，从而减轻了大规模人工评估的需要。 |
| [^121] | [Iterative autoregression: a novel trick to improve your low-latency speech enhancement model.](http://arxiv.org/abs/2211.01751) | 本研究提出了一种简单而有效的替代技术，用于训练自回归低延迟语音增强模型，该方法在不同的架构和训练场景下均能带来稳定的改进。 |
| [^122] | [Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup.](http://arxiv.org/abs/2210.13512) | 本文提出了一种基于中点 Mixup 的多视角数据学习方法，相比于传统经验风险最小化方法能够更好地学习每个类别的所有特征，具有更好的泛化和鲁棒性。 |
| [^123] | [On the Identifiability and Estimation of Causal Location-Scale Noise Models.](http://arxiv.org/abs/2210.09054) | 本文研究了一类异方差噪声模型，发现除特殊情况外因果方向是可识别的。提出了两个估计器，能够准确识别因果效应。 |
| [^124] | [Temporally Consistent Transformers for Video Generation.](http://arxiv.org/abs/2210.02396) | 本文介绍了一种名为TECO的生成模型，其可以大大提高视频生成的长期时间一致性。作者提出了3个难度不同的视频数据集以评估该模型和现有模型在时间一致性方面的局限性。 |
| [^125] | [Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations.](http://arxiv.org/abs/2209.13446) | 本文提出了一种基于特征学习的多样性隐私保护反事实解释方法，可以有效地处理反事实约束条件，并为私有解释模型做出贡献。 |
| [^126] | [Do Large Language Models know what humans know?.](http://arxiv.org/abs/2209.01515) | 本文测试了语言暴露假说，评估语言模型是否能够像人类一样感知书面文本中角色的知识状态。结果显示，语言模型虽然超过了偶然行为，但表现不如人类，暗示着仅靠语言暴露难以完全解释人类这种认知能力。 |
| [^127] | [Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics.](http://arxiv.org/abs/2208.10533) | 本文提出一种名为评判置信度引导探索的方法，用于将现有的神谕策略纳入标准的演员-评论家强化学习算法中，以提高探索效率。在不确定性高时，该方法会将神谕策略的行动作为建议纳入学习方案中，而在不确定性低时忽略它。 |
| [^128] | [Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction.](http://arxiv.org/abs/2207.14116) | Claim-Dissector是一款联合重排和真实性预测的可解释的事实核查系统，可以识别与声明相关的证据，并确定声明的真实性。该系统的个人贡献以及证据所支持或反驳声明的贡献都可以被识别。 |
| [^129] | [An Ultra-low Power TinyML System for Real-time Visual Processing at Edge.](http://arxiv.org/abs/2207.04663) | 本文提出了一种超低功耗TinyML系统，采用微型骨干网络构建高效CNN模型，再由特别设计的神经协处理器与MCU相连，将所有特征和权重储存在芯片上，完全消除芯片外存储器访问的延迟和功耗。此系统具有相当的准确性并实现了创纪录的160mW超低功耗。 |
| [^130] | [On the Identifiability of Nonlinear ICA: Sparsity and Beyond.](http://arxiv.org/abs/2206.07751) | 本文提出一个新的方法，考虑混合过程的假设，即结构稀疏性，来实现非线性ICA的可识别性，无需辅助变量。 |
| [^131] | [From Perception to Programs: Regularize, Overparameterize, and Amortize.](http://arxiv.org/abs/2206.05922) | 该论文探索了正则化、过度参数化、摊销等技术来提高神经符号程序合成的稳定性，用于将感知到的输入处理为离散抽象并符号化处理为程序。 |
| [^132] | [Aerial Vision-and-Dialog Navigation.](http://arxiv.org/abs/2205.12219) | 本文介绍了一种名为AVDN的无人机导航方法，该方法通过自然语言对话来控制无人机的导航，为解决无人机控制的人体负担、多任务操作、以及残疾人和手部占用者控制无人机的难题提供了新思路。 |
| [^133] | [Constrained Monotonic Neural Networks.](http://arxiv.org/abs/2205.11775) | 本文针对实际应用场景需要的单调性，提出了一种通过在层中的一部分神经元中采用原始激活函数，同时在另一部分采用其点对称反射来解决构建单调深度神经网络的方法。实验证明，该方法的精度符合要求。 |
| [^134] | [Supervising the Multi-Fidelity Race of Hyperparameter Configurations.](http://arxiv.org/abs/2202.09774) | 本文介绍了一种贝叶斯优化方法DyHPO，它能够学习动态决定哪个超参数配置在所有可行的配置中进行下一步训练。实验表明DyHPO相比于最先进的超参数优化方法有显著的优越性。 |
| [^135] | [Inductive Representation Learning in Temporal Networks via Mining Neighborhood and Community Influences.](http://arxiv.org/abs/2110.00267) | 该论文提出了一种新的归纳式网络表示学习方法MNCI，通过在时间网络中挖掘邻域和社区影响来生成节点嵌入，实验证明该方法具有更好的性能。 |
| [^136] | [Disentangled Generative Models for Robust Prediction of System Dynamics.](http://arxiv.org/abs/2108.11684) | 本文研究了解缠生成模型用于预测系统动力学，将领域参数与生成模型的潜在空间中的动力学分离，取得了良好的泛化和长期稳定性。 |
| [^137] | [Defeasible RDFS via Rational Closure.](http://arxiv.org/abs/2007.07573) | 本文介绍了如何将Rational Closure整合到RDFS中，提出了可撤销的$\rho df_\bot$，使其仍然保持三元语言的语法，并通过引入新规则进行简单的扩展，以声明不兼容的实体。 |

# 详细

[^1]: 与康科迪亚并行的神经符号一体化

    Parallel Neurosymbolic Integration with Concordia. (arXiv:2306.00480v1 [cs.AI])

    [http://arxiv.org/abs/2306.00480](http://arxiv.org/abs/2306.00480)

    康科迪亚框架是一个支持各种概率理论，克服了现有技术的限制的并行的神经符号结构，成功应用于集合活动检测、实体链接和推荐任务，提高了最新准确性。

    

    并行的神经符号结构通过将逻辑理论的知识提取到深度模型中，在NLP中得到有效应用。然而，现有技术仍存在一些限制，包括支持受限形式的逻辑理论，并依赖于逻辑和深度网络之间的独立性假设。本文提出了康科迪亚框架，克服了现有技术的限制。康科迪亚框架既不关注深度网络，也不关注逻辑理论，支持各种概率理论。框架可以支持两个组件的监督训练和神经组件的无监督训练。康科迪亚已成功应用于NLP和数据分类以外的任务，提高了集合活动检测、实体链接和推荐任务的最新准确性。

    Parallel neurosymbolic architectures have been applied effectively in NLP by distilling knowledge from a logic theory into a deep model.However, prior art faces several limitations including supporting restricted forms of logic theories and relying on the assumption of independence between the logic and the deep network. We present Concordia, a framework overcoming the limitations of prior art. Concordia is agnostic both to the deep network and the logic theory offering support for a wide range of probabilistic theories. Our framework can support supervised training of both components and unsupervised training of the neural component. Concordia has been successfully applied to tasks beyond NLP and data classification, improving the accuracy of state-of-the-art on collective activity detection, entity linking and recommendation tasks.
    
[^2]: 使预训练模型具有可逆性：从参数到内存高效的微调

    Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])

    [http://arxiv.org/abs/2306.00477](http://arxiv.org/abs/2306.00477)

    本研究尝试实现在预训练语言模型中运用可逆模型实现高效的微调，并发现在初始化微调时保留PLM的起点非常重要。

    

    预训练语言模型（PLM）的参数高效微调已经成为一种非常成功的方法，只需训练少量参数而不会降低性能，并随着PLM越来越大而成为事实上的学习范式。然而，现有的PEFT方法不具备内存效率，因为它们仍需要存储大部分中间激活值以便计算梯度，类似于微调。一个减少激活内存的有效方法是应用可逆模型，这样中间激活值就无需缓存，可以重新计算。然而，将PLM修改为它的可逆变体并进行PEFT并不是一件容易的事，因为可逆模型具有与当前发布的PLM不同的体系结构。本文首先调查现有PEFT方法成功的关键因素，认识到在初始化PEFT时保留PLM的起点是至关重要的。

    Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
    
[^3]: 一种基于大数据的俄语讽刺检测方法

    A big data approach towards sarcasm detection in Russian. (arXiv:2306.00445v1 [cs.CL])

    [http://arxiv.org/abs/2306.00445](http://arxiv.org/abs/2306.00445)

    本文介绍了一种针对俄语文本的大数据方法，旨在检测文本中的讽刺。作者提出了一组可用于屈折和文本合成的算法，并在一个公开可用的网络服务中实现。对这些算法的性能进行了测试和比较，同时用于估计俄语不同词性的形态变异性和复杂性。

    

    本文提出了一组针对俄罗斯语言屈折和自动文本合成的确定性算法，并在公开可用的网络服务www.passare.ru中实现。该服务提供了单词屈折、词匹配和生成语法正确的俄语文本的功能。所选代码和数据集可在https://github.com/passare-ru/PassareFunctions/中获得。针对OpenCorpora注释语料库对屈折功能的性能进行了测试，与其他解决方案进行了比较，并用于估计俄语不同词性的形态变异性和复杂性。

    We present a set of deterministic algorithms for Russian inflection and automated text synthesis. These algorithms are implemented in a publicly available web-service www.passare.ru. This service provides functions for inflection of single words, word matching and synthesis of grammatically correct Russian text. Selected code and datasets are available at https://github.com/passare-ru/PassareFunctions/ Performance of the inflectional functions has been tested against the annotated corpus of Russian language OpenCorpora, compared with that of other solutions, and used for estimating the morphological variability and complexity of different parts of speech in Russian.
    
[^4]: 针对类内分布转移的过度遗忘：连续学习的脆弱性

    Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift. (arXiv:2306.00427v1 [cs.LG])

    [http://arxiv.org/abs/2306.00427](http://arxiv.org/abs/2306.00427)

    连续学习中存在一种特殊形式的灾难性遗忘——越界遗忘，当给定类别引入类内分布转移时，它会显着削弱该类别的连续学习方法的识别准确率。

    

    连续学习是让人工神经网络在开放环境中工作的重要技术。在联合学习中，人们已经知道由意图攻击或环境扰动引起的越界问题严重影响网络的泛化能力。在这项工作中，我们报告了连续学习设置中由越界问题引起的一种特殊形式的灾难性遗忘，我们将其称为越界遗忘（OODF）。在连续图像分类任务中，我们发现，针对给定类别，引入类内分布转移显着削弱了后续学习过程中该类别的连续学习方法的识别准确率。有趣的是，这种现象对于连续学习而言是特殊的，因为同样级别的分布转移只有微不足道的影响。

    Continual learning (CL) is an important technique to allow artificial neural networks to work in open environments. CL enables a system to learn new tasks without severe interference to its performance on old tasks, i.e., overcome the problems of catastrophic forgetting. In joint learning, it is well known that the out-of-distribution (OOD) problem caused by intentional attacks or environmental perturbations will severely impair the ability of networks to generalize. In this work, we reported a special form of catastrophic forgetting raised by the OOD problem in continual learning settings, and we named it out-of-distribution forgetting (OODF). In continual image classification tasks, we found that for a given category, introducing an intra-class distribution shift significantly impaired the recognition accuracy of CL methods for that category during subsequent learning. Interestingly, this phenomenon is special for CL as the same level of distribution shift had only negligible effects
    
[^5]: AIGC的隐私和安全问题及其解决方案：探索隐私计算、区块链等技术的潜力

    Challenges and Remedies to Privacy and Security in AIGC: Exploring the Potential of Privacy Computing, Blockchain, and Beyond. (arXiv:2306.00419v1 [cs.CR])

    [http://arxiv.org/abs/2306.00419](http://arxiv.org/abs/2306.00419)

    本文系统地回顾了AIGC的概念、分类以及基础技术，详细阐述了在AIGC中可能存在的隐私和安全挑战，提供了隐私计算和区块链等技术相应的解决方案，并在此基础上确定了未来的研究方向。

    

    人工智能生成内容（AIGC）是人工智能发展的最新成果之一。相关应用生成的文本、图像和音频等内容引发了激烈的讨论。各种衍生的AIGC应用也逐渐进入各行各业，给人们的日常生活带来了难以想象的影响。然而，这些生成性工具的快速发展也引发了关于AIGC隐私、安全甚至版权问题的担忧。我们注意到，区块链和隐私计算等先进技术可以与AIGC工具相结合，但尚未有研究对其相关性和前景进行系统和详细的调查。因此，有必要通过充分探索上述技术，研究如何利用它们来保护AIGC中数据的隐私和安全问题。本文首先系统地回顾了AIGC的概念、分类和基础技术。然后，我们详细阐述了AIGC中的潜在隐私和安全挑战，并提供相应的解决方案。我们特别讨论了隐私计算和区块链在解决这些挑战方面的适用性和局限性。最后，我们确定了本领域未来的研究方向。

    Artificial Intelligence Generated Content (AIGC) is one of the latest achievements in AI development. The content generated by related applications, such as text, images and audio, has sparked a heated discussion. Various derived AIGC applications are also gradually entering all walks of life, bringing unimaginable impact to people's daily lives. However, the rapid development of such generative tools has also raised concerns about privacy and security issues, and even copyright issues in AIGC. We note that advanced technologies such as blockchain and privacy computing can be combined with AIGC tools, but no work has yet been done to investigate their relevance and prospect in a systematic and detailed way. Therefore it is necessary to investigate how they can be used to protect the privacy and security of data in AIGC by fully exploring the aforementioned technologies. In this paper, we first systematically review the concept, classification and underlying technologies of AIGC. Then, 
    
[^6]: “不确定性感知的非似然学习提高生成式情感四元组预测”

    Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction. (arXiv:2306.00418v1 [cs.CL])

    [http://arxiv.org/abs/2306.00418](http://arxiv.org/abs/2306.00418)

    本文提出了一种新的方法来控制标记级的生成、提高原始学习和减少错误，其中包括蒙特卡洛dropout、边缘非似然学习和最小化熵。在四个公共数据集上的广泛实验表明，该方法有效地提高了情感四元组预测的性能。

    

    最近，基于方面的情感分析领域广泛关注了情感四元组预测。现有的研究通过预训练的生成式语言模型提取出四元组，将原始句子转化为模板化的目标序列。然而，以前的研究只关注生成什么，而忽略了不需要生成的内容。我们认为考虑负样本也会带来潜在的好处。本文提出了一种模板无关的方法来控制标记级的生成，同时提高原始学习和减少错误。具体来说，我们引入了蒙特卡洛dropout来理解预训练语言模型的内置不确定性，获取噪声和错误信息。我们进一步提出边缘非似然学习来抑制不确定性感知的错误标记。最后，我们引入了最小化熵来平衡边缘非似然学习的影响。在四个公共数据集上的广泛实验表明，我们提出的方法在提高情感四元组预测的性能方面是有效的。

    Recently, aspect sentiment quad prediction has received widespread attention in the field of aspect-based sentiment analysis. Existing studies extract quadruplets via pre-trained generative language models to paraphrase the original sentence into a templated target sequence. However, previous works only focus on what to generate but ignore what not to generate. We argue that considering the negative samples also leads to potential benefits. In this work, we propose a template-agnostic method to control the token-level generation, which boosts original learning and reduces mistakes simultaneously. Specifically, we introduce Monte Carlo dropout to understand the built-in uncertainty of pre-trained language models, acquiring the noises and errors. We further propose marginalized unlikelihood learning to suppress the uncertainty-aware mistake tokens. Finally, we introduce minimization entropy to balance the effects of marginalized unlikelihood learning. Extensive experiments on four public
    
[^7]: 可控运动扩散模型

    Controllable Motion Diffusion Model. (arXiv:2306.00416v1 [cs.CV])

    [http://arxiv.org/abs/2306.00416](http://arxiv.org/abs/2306.00416)

    该论文提出了可控运动扩散模型（COMODO）框架，通过自回归运动扩散模型（A-MDM）生成高保真度、长时间内的运动序列，以实现在响应于时变控制信号的情况下进行实时运动合成。

    

    在计算机动画中，为虚拟角色生成逼真且可控的运动是一项具有挑战性的任务。最近的研究从图像生成的扩散模型的成功中汲取灵感，展示了解决这个问题的潜力。然而，这些研究大多限于离线应用，目标是生成同时生成所有步骤的序列级生成。为了能够在响应于时变控制信号的情况下使用扩散模型实现实时运动合成，我们提出了可控运动扩散模型（COMODO）框架。我们的框架以自回归运动扩散模型（A-MDM）为基础，逐步生成运动序列。通过简单地使用标准DDPM算法而无需任何额外复杂性，我们的框架能够产生在不同类型的运动控制下长时间内的高保真度运动序列。

    Generating realistic and controllable motions for virtual characters is a challenging task in computer animation, and its implications extend to games, simulations, and virtual reality. Recent studies have drawn inspiration from the success of diffusion models in image generation, demonstrating the potential for addressing this task. However, the majority of these studies have been limited to offline applications that target at sequence-level generation that generates all steps simultaneously. To enable real-time motion synthesis with diffusion models in response to time-varying control signals, we propose the framework of the Controllable Motion Diffusion Model (COMODO). Our framework begins with an auto-regressive motion diffusion model (A-MDM), which generates motion sequences step by step. In this way, simply using the standard DDPM algorithm without any additional complexity, our framework is able to generate high-fidelity motion sequences over extended periods with different type
    
[^8]: 公正感知推荐系统综述

    A Survey on Fairness-aware Recommender Systems. (arXiv:2306.00403v1 [cs.IR])

    [http://arxiv.org/abs/2306.00403](http://arxiv.org/abs/2306.00403)

    本综述对现有的公正感知推荐系统方法和实践进行了总结分析，详细介绍了相关的概念定义、分类、方法和需解决的问题，并提出了未来的研究方向。

    

    作为信息过滤服务，推荐系统通过提供个性化建议和帮助人们做出决策极大地丰富了我们的日常生活，使它们在信息时代对人类社会至关重要和不可或缺。然而，随着人们对它们的依赖程度增加，最近的研究显示，由于其不公平性（例如工作推荐中的性别歧视），推荐系统对社会和个人可能拥有无意识的影响。为了开发可信赖的服务，设计公正感知的推荐系统以缓解这些偏见问题至关重要。本综述概述了现有的公正性推荐系统方法和实践。首先，我们介绍了不同推荐场景下的公正性概念，全面分类当前的进展并介绍了促进推荐系统不同阶段的公正性的典型方法。接下来，在介绍数据集和评估方法后，我们讨论了一些未来的研究方向。

    As information filtering services, recommender systems have extremely enriched our daily life by providing personalized suggestions and facilitating people in decision-making, which makes them vital and indispensable to human society in the information era. However, as people become more dependent on them, recent studies show that recommender systems potentially own unintentional impacts on society and individuals because of their unfairness (e.g., gender discrimination in job recommendations). To develop trustworthy services, it is crucial to devise fairness-aware recommender systems that can mitigate these bias issues. In this survey, we summarise existing methodologies and practices of fairness in recommender systems. Firstly, we present concepts of fairness in different recommendation scenarios, comprehensively categorize current advances, and introduce typical methods to promote fairness in different stages of recommender systems. Next, after introducing datasets and evaluation me
    
[^9]: Teacher Agent：一种基于重复训练的视频增量学习非知识蒸馏方法

    Teacher Agent: A Non-Knowledge Distillation Method for Rehearsal-based Video Incremental Learning. (arXiv:2306.00393v1 [cs.CV])

    [http://arxiv.org/abs/2306.00393](http://arxiv.org/abs/2306.00393)

    提出了一种教师代理方法，能够从先前学习的知识中生成高质量样本的数据集，从而使学生网络可以从样本的多种观点中进行学习，在标准基准上优于基于知识蒸馏的方法。

    

    随着基于视频的社交媒体的普及，不断有新的视频类别被生成，迫切需要稳健的增量学习技术来理解这些视频。其中最大的挑战之一是灾难性遗忘，在学习新类别的同时，网络往往会忘记先前学习过的数据。为了解决此问题，知识蒸馏是一种广泛使用的技术，它通过将不同类别之间的相似性的重要信息传输到学生模型中来增强其性能。因此，最好有一个强大的教师模型来指导学生。然而，网络本身的有限表现和灾难性遗忘的发生可能导致教师网络对某些记忆样本做出不准确的预测，从而限制了学生网络的性能。基于这些观察，我们提出了一种教师代理，能够从先前学习的知识中生成高质量样本的数据集，从而使学生网络可以从样本的多种观点中进行学习。我们的方法在视频分类任务的标准基准上优于基于知识蒸馏的方法。

    With the rise in popularity of video-based social media, new categories of videos are constantly being generated, creating an urgent need for robust incremental learning techniques for video understanding. One of the biggest challenges in this task is catastrophic forgetting, where the network tends to forget previously learned data while learning new categories. To overcome this issue, knowledge distillation is a widely used technique for rehearsal-based video incremental learning that involves transferring important information on similarities among different categories to enhance the student model. Therefore, it is preferable to have a strong teacher model to guide the students. However, the limited performance of the network itself and the occurrence of catastrophic forgetting can result in the teacher network making inaccurate predictions for some memory exemplars, ultimately limiting the student network's performance. Based on these observations, we propose a teacher agent capabl
    
[^10]: 可信人工智能的调查、分类及未来方向：元决策的战略决策元分析

    The Survey, Taxonomy, and Future Directions of Trustworthy AI: A Meta Decision of Strategic Decisions. (arXiv:2306.00380v1 [cs.AI])

    [http://arxiv.org/abs/2306.00380](http://arxiv.org/abs/2306.00380)

    本文提出了一个新方法来解决在使用AI系统进行决策时的可信问题。该方法引入了一个包括表达、真实和基本水平的不同信任级别的TAI分类系统或框架，使用十个维度来衡量信任，并提供了现有TAI研究的调查和元分析，还确定了TAI研究的未来方向和潜在应用。

    

    在制定战略决策时，我们常常面临着大量需要处理的信息。当一些证据相互矛盾或自相矛盾时，情况可能会更加复杂。此时，问题在于如何确定哪些信息是有用的，哪些应该被排除。这个过程被称为元决策。同样，在使用人工智能（AI）系统进行战略决策时，对AI本身的信任就成为了一个元决策，因为许多AI系统被视为处理大量数据的不透明“黑匣子”。信任一个不透明的系统涉及决定什么样的可信人工智能（TAI）水平。我们提出了一种新方法来解决这个问题，即引入一个新颖的TAI分类系统或框架，该框架包括三个关键领域：表达、真实和基本水平的不同信任级别。为了支撑这些领域，我们创建了十个维度来衡量信任：可解释性/透明性、无偏性、问责制、准确性、可靠性、隐私、安全、公平性和人类控制。在本文中，我们提供了一个现有TAI研究的调查和元分析，以全面了解这一新兴领域的研究现状。我们还确定了TAI研究的未来方向，并提出了一些潜在应用，这些应用可能从TAI的使用中受益。

    When making strategic decisions, we are often confronted with overwhelming information to process. The situation can be further complicated when some pieces of evidence are contradicted each other or paradoxical. The challenge then becomes how to determine which information is useful and which ones should be eliminated. This process is known as meta-decision. Likewise, when it comes to using Artificial Intelligence (AI) systems for strategic decision-making, placing trust in the AI itself becomes a meta-decision, given that many AI systems are viewed as opaque "black boxes" that process large amounts of data. Trusting an opaque system involves deciding on the level of Trustworthy AI (TAI). We propose a new approach to address this issue by introducing a novel taxonomy or framework of TAI, which encompasses three crucial domains: articulate, authentic, and basic for different levels of trust. To underpin these domains, we create ten dimensions to measure trust: explainability/transparen
    
[^11]: 在网络安全中开发和构建本体论

    Developing and Building Ontologies in Cyber Security. (arXiv:2306.00377v1 [cs.CR])

    [http://arxiv.org/abs/2306.00377](http://arxiv.org/abs/2306.00377)

    通过比较不同本体论，该论文提供了有关网络安全本体论领域的未来方向。

    

    网络安全是我们现代社会中最受关注的学科之一。我们在网络安全领域工作，选择了网络安全本体论作为研究主题。我们汇集了最新和以前的本体论，并根据不同的分析因素进行比较，以找到最佳方案。选择此主题的原因是汇集来自不同时代的不同本体论。因为研究中包括的SLR大多研究单个本体论。如果任何研究人员想要研究本体论，他必须研究每个单独的本体论并选择最适合他研究的。因此，我们汇集了不同类型的本体论，并相互比较以得到最佳方案。通过系统性过程认真选择了2010年至2020年间的24篇论文，并进行了分类。最后，本SLR呈现给研究人员有关网络安全本体论领域的有前途的未来方向。

    Cyber Security is one of the most arising disciplines in our modern society. We work on Cybersecurity domain and in this the topic we chose is Cyber Security Ontologies. In this we gather all latest and previous ontologies and compare them on the basis of different analyzing factors to get best of them. Reason to select this topic is to assemble different ontologies from different era of time. Because, researches that included in this SLR is mostly studied single ontology. If any researcher wants to study ontologies, he has to study every single ontology and select which one is best for his research. So, we assemble different types of ontology and compare them against each other to get best of them. A total 24 papers between years 2010-2020 are carefully selected through systematic process and classified accordingly. Lastly, this SLR have been presented to provide the researchers promising future directions in the domain of cybersecurity ontologies.
    
[^12]: CFL：通过基于Token级属性控制的生成实现因果公平的语言模型

    CFL: Causally Fair Language Models Through Token-level Attribute Controlled Generation. (arXiv:2306.00374v1 [cs.CL])

    [http://arxiv.org/abs/2306.00374](http://arxiv.org/abs/2306.00374)

    本文提出了一种使用因果平均处理效应（ATE）分数和反事实增强方法的CFL体系架构，通过插入式方式对预训练的LM进行解毒，实现了因果公平的语言模型。此方法不会对模型困惑产生太大影响，还减轻了意外的偏置问题。

    

    我们提出了一种方法来控制语言模型（LMs）的属性，用于文本生成任务，该方法使用因果平均处理效应（ATE）分数和反事实增强。我们探索了这种方法，在LM解毒的背景下提出了CFL（causally fair language）体系结构，以插入式方式对预训练的LM进行解毒。我们的体系结构基于结构因果模型（SCM），数学透明且与许多现有解毒技术相比计算效率高。我们还提出了几个新的度量标准，旨在更好地理解LM在有害文本生成环境中的行为。此外，我们在\RTP(RTP)基准测试中实现了最先进的毒性退化性能。我们的实验表明，CFL实现了这种解毒，对模型困惑没有太大影响。我们还通过对BOLD数据集的实验展示了CFL的减轻意外偏置问题的能力。

    We propose a method to control the attributes of Language Models (LMs) for the text generation task using Causal Average Treatment Effect (ATE) scores and counterfactual augmentation. We explore this method, in the context of LM detoxification, and propose the Causally Fair Language (CFL) architecture for detoxifying pre-trained LMs in a plug-and-play manner. Our architecture is based on a Structural Causal Model (SCM) that is mathematically transparent and computationally efficient as compared with many existing detoxification techniques. We also propose several new metrics that aim to better understand the behaviour of LMs in the context of toxic text generation. Further, we achieve state of the art performance for toxic degeneration, which are computed using \RTP (RTP) benchmark. Our experiments show that CFL achieves such a detoxification without much impact on the model perplexity. We also show that CFL mitigates the unintended bias problem through experiments on the BOLD dataset.
    
[^13]: 关于一致性模型的等价性: 一致性模型，一致性扩散模型和Fokker-Planck正则化

    On the Equivalence of Consistency-Type Models: Consistency Models, Consistent Diffusion Models, and Fokker-Planck Regularization. (arXiv:2306.00367v1 [math.ST])

    [http://arxiv.org/abs/2306.00367](http://arxiv.org/abs/2306.00367)

    本研究建立了一致性模型，一致性扩散模型和Fokker-Planck正则化之间的理论联系，为不同目标增强扩散模型提供了更全面和包容性的框架的可能性。

    

    扩散模型中各种“一致性”概念的出现引起了很大关注，帮助改善了样本质量、似然估计和加速抽样。虽然在文献中类似的概念已经被提出，但它们之间的确切关系仍不清楚。在本研究中，我们建立了三个最近的“一致性”概念之间的理论联系，旨在为不同目标增强扩散模型。我们的见解为一致性类型模型提供了更全面和包容性的框架的可能性。

    The emergence of various notions of ``consistency'' in diffusion models has garnered considerable attention and helped achieve improved sample quality, likelihood estimation, and accelerated sampling. Although similar concepts have been proposed in the literature, the precise relationships among them remain unclear. In this study, we establish theoretical connections between three recent ``consistency'' notions designed to enhance diffusion models for distinct objectives. Our insights offer the potential for a more comprehensive and encompassing framework for consistency-type models.
    
[^14]: 解决扩散模型中的负迁移问题

    Addressing Negative Transfer in Diffusion Models. (arXiv:2306.00354v1 [cs.CV])

    [http://arxiv.org/abs/2306.00354](http://arxiv.org/abs/2306.00354)

    本文从多任务学习角度出发，研究了扩散训练中的负迁移现象，提出了利用正则化技术增强扩散训练的方法，以减轻负迁移并提高去噪任务的性能。

    

    基于扩散的生成模型在各个领域都取得了显著的成功。它在同时涵盖不同噪声水平的去噪任务上训练模型，代表了一种多任务学习（MTL）的形式。然而，从MTL的角度分析和改善扩散模型仍然未被充分探索。特别地，MTL有时会导致众所周知的$\textit{负迁移}$现象，这种现象是由于任务之间存在冲突而导致某些任务的性能降低。本文旨在从MTL的角度分析扩散训练，提出了两个关键观察：$\textbf{(O1)}$ 随着噪声水平之间的差距加大，去噪任务之间的任务亲和力减弱， $\textbf{(O2)}$ 在扩散训练的背景下，负迁移也可能会出现。基于这些观察结果，我们的目标是通过减轻负迁移来增强扩散训练。为了实现这一目标，我们提出了利用现有的MTL方法、具体是正则化技术，来鼓励任务特定的特征提取并减少任务干扰。实验结果表明，我们提出的方法有效地减轻了负迁移，提高了扩散模型在一系列去噪任务上的性能。

    Diffusion-based generative models have achieved remarkable success in various domains. It trains a model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of $\textit{negative transfer}$, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we aim to analyze diffusion training from an MTL standpoint, presenting two key observations: $\textbf{(O1)}$ the task affinity between denoising tasks diminishes as the gap between noise levels widens, and $\textbf{(O2)}$ negative transfer can arise even in the context of diffusion training. Building upon these observations, our objective is to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL metho
    
[^15]: 结合显式和隐式正则化的深度网络高效学习

    Combining Explicit and Implicit Regularization for Efficient Learning in Deep Networks. (arXiv:2306.00342v1 [cs.LG])

    [http://arxiv.org/abs/2306.00342](http://arxiv.org/abs/2306.00342)

    本文提出了一种显式正则化方法，与隐式正则化结合，可以使单层网络实现与深度线性网络相当的低秩近似和泛化误差，使深度不再是学习的必要条件。

    

    隐式正则化研究优化过程中的梯度轨迹，以解释为什么深度网络更倾向于某些解决方案。在深度线性网络中，已经证明梯度下降隐式地朝向矩阵补全/因式分解任务上的低秩解决方案进行正则化。添加层数不仅可以提高这些任务的性能，而且作为一种加速的预处理方法进一步增强了这种低秩偏向。受此启发，我们提出一种显式惩罚来反映这种隐式偏差，只在某些自适应梯度优化器（例如Adam）起作用。这种组合可以使退化的单层网络实现与深度线性网络相当的低秩近似和泛化误差，使深度不再是学习的必要条件。单层网络还在一系列参数和数据集上能够表现优异，甚至超过了各种矩阵补全方法的表现。

    Works on implicit regularization have studied gradient trajectories during the optimization process to explain why deep networks favor certain kinds of solutions over others. In deep linear networks, it has been shown that gradient descent implicitly regularizes toward low-rank solutions on matrix completion/factorization tasks. Adding depth not only improves performance on these tasks but also acts as an accelerative pre-conditioning that further enhances this bias towards low-rankedness. Inspired by this, we propose an explicit penalty to mirror this implicit bias which only takes effect with certain adaptive gradient optimizers (e.g. Adam). This combination can enable a degenerate single-layer network to achieve low-rank approximations with generalization error comparable to deep linear networks, making depth no longer necessary for learning. The single-layer network also performs competitively or out-performs various approaches for matrix completion over a range of parameter and da
    
[^16]: 使用IBIA框架的边缘近似推断

    Approximate inference of marginals using the IBIA framework. (arXiv:2306.00335v1 [cs.AI])

    [http://arxiv.org/abs/2306.00335](http://arxiv.org/abs/2306.00335)

    提出了一种新的基于IBIA框架的算法，通过将PGM转化为有界团大小的SLCTF序列，并使用启发式置信度更新算法来推导边缘。实验结果表明，该算法比当前最先进的技术更加有效和高效。

    

    概率图模型（PGM）中边缘的精确推断被认为是不可计算的，因此需要使用近似方法。大多数现有的变分技术在环路图中执行迭代信息传递，这对于许多基准测试来说收敛速度慢。本文提出了一种基于增量构建-推理-近似（IBIA）范例的新型边缘推断算法。我们的算法将PGM转化为具有有界团大小的一系列链接的团树森林（SLCTF），然后使用启发式置信度更新算法来推导边缘。对于贝叶斯网络的特殊情况，我们显示如果IBIA中增量构建步骤使用变量的拓扑顺序，则（a）所有SLCTF中的CTF的先验边缘一致，（b）一旦将所有证据变量添加到SLCTF中，后验边缘就是一致的。在我们的方法中，置信传递步骤是非迭代的，准确度和复杂度之间的权衡可以通过调整团的大小来控制。基准数据集上的实验结果证明了与现有技术相比我们算法的有效性和效率。

    Exact inference of marginals in probabilistic graphical models (PGM) is known to be intractable, necessitating the use of approximate methods. Most of the existing variational techniques perform iterative message passing in loopy graphs which is slow to converge for many benchmarks. In this paper, we propose a new algorithm for marginal inference that is based on the incremental build-infer-approximate (IBIA) paradigm. Our algorithm converts the PGM into a sequence of linked clique tree forests (SLCTF) with bounded clique sizes, and then uses a heuristic belief update algorithm to infer the marginals. For the special case of Bayesian networks, we show that if the incremental build step in IBIA uses the topological order of variables then (a) the prior marginals are consistent in all CTFs in the SLCTF and (b) the posterior marginals are consistent once all evidence variables are added to the SLCTF. In our approach, the belief propagation step is non-iterative and the accuracy-complexity
    
[^17]: 多维深度结构化状态空间方法在语音增强中的应用：基于小型模型

    A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models. (arXiv:2306.00331v1 [eess.AS])

    [http://arxiv.org/abs/2306.00331](http://arxiv.org/abs/2306.00331)

    该论文提出了一种基于小型模型的多维深度结构化状态空间方法，在语音增强中表现出竞争力，由于使用的参数比常规卷积层少，提出的TF域S4模型大小减少了78.6％，同时在数据增广后获得3.15的PESQ得分。

    

    我们提出了一种多维结构化状态空间（S4）方法来进行语音增强。为了更好地捕捉频率轴上的谱依赖性，我们着重于使用白化变换修改多维S4层，以构建具有良好性能的新小型模型。我们在时间（T）和时频（TF）域探索了几个基于S4的深度架构。尽管使用的参数比常规卷积层少，但2D S4层可以被视为具有无限接收域的特殊卷积层。在VoiceBank-DEMAND数据集上评估时，与基于卷积层的常规U-net模型相比，所提出的TF域S4模型大小减少了78.6％，但仍在数据增广后获得了具有竞争力的结果，PESQ得分为3.15。通过增加模型大小，我们甚至可以达到3.18的PESQ得分。

    We propose a multi-dimensional structured state space (S4) approach to speech enhancement. To better capture the spectral dependencies across the frequency axis, we focus on modifying the multi-dimensional S4 layer with whitening transformation to build new small-footprint models that also achieve good performance. We explore several S4-based deep architectures in time (T) and time-frequency (TF) domains. The 2-D S4 layer can be considered a particular convolutional layer with an infinite receptive field although it utilizes fewer parameters than a conventional convolutional layer. Evaluated on the VoiceBank-DEMAND data set, when compared with the conventional U-net model based on convolutional layers, the proposed TF-domain S4-based model is 78.6% smaller in size, yet it still achieves competitive results with a PESQ score of 3.15 with data augmentation. By increasing the model size, we can even reach a PESQ score of 3.18.
    
[^18]: “思维克隆：通过模仿人类思维学习思考并行动”。（arXiv:2306.00323v1 [cs.AI]）

    Thought Cloning: Learning to Think while Acting by Imitating Human Thinking. (arXiv:2306.00323v1 [cs.AI])

    [http://arxiv.org/abs/2306.00323](http://arxiv.org/abs/2306.00323)

    本论文提出了一种新的模仿学习框架“思维克隆”，通过学习人类的思维来训练AI代理，以在泛化、探索、规划等能力方面实现更好的表现。

    

    语言通常被认为是人类思维的一个关键方面，它为我们提供了非凡的泛化、探索、规划、重新规划和适应新情况的能力。然而，强化学习（RL）代理在这些能力中远未达到人类水平的表现。我们假设其中一个认知缺陷的原因是他们缺乏使用语言思考所带来的好处。我们认为通过训练AI代理人像人类一样思考，可以改善其性能。我们引入了一种新的模仿学习框架“思维克隆”，其想法不仅是克隆人类示范者的行为，而且还包括人类在执行这些行为时所产生的想法。虽然我们希望“思维克隆”在处理网络规模的人类思维和行为数据时能够发挥出色（例如，带有剧本的在线视频），但在这里，我们进行了在思考和行动数据为合成生成的领域的实验。结果显示，“思维克隆”学习速度比传统的强化学习方法快得多。

    Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to think like humans do. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, but also the thoughts humans have as they perform these behaviors. While we expect Thought Cloning to truly shine at scale on internet-sized datasets of humans thinking out loud while acting (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than
    
[^19]: 基于元素除法的可学习舍入用于后训练量化

    FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization. (arXiv:2306.00317v1 [cs.LG])

    [http://arxiv.org/abs/2306.00317](http://arxiv.org/abs/2306.00317)

    该论文提出了一种新的基于元素除法的可学习舍入机制FlexRound，使得后训练量化时更好地重构每个层或块的输出，并且能够学习公共量化网格大小以及每个预训练权重的不同比例尺。

    

    后训练量化（PTQ）已经在资源有限设备上部署深度神经网络方面越来越受欢迎，因为与量化感知培训不同，完全不需要全面的训练数据集或端到端培训。因为基于重构每个层或块输出的PTQ方案效果显着以增强量化模型性能，所以最近的研究已经开发了算法来设计和学习一种新的权重舍入方案，以更好地重构每个层或块的输出。在这项工作中，我们提出了一种简单而有效的新的PTQ权重舍入机制，名为FlexRound，其基于元素除法而不是典型的元素加法，从而使FlexRound能够同时学习公共量化网格大小以及每个预训练权重的不同比例尺。由于元素除法产生的导数的互补规则，FlexRound在更新其相关预训练权重时天生能够利用它们。

    Post-training quantization (PTQ) has been gaining popularity for the deployment of deep neural networks on resource-limited devices since unlike quantization-aware training, neither a full training dataset nor end-to-end training is required at all. As PTQ schemes based on reconstructing each layer or block output turn out to be effective to enhance quantized model performance, recent works have developed algorithms to devise and learn a new weight-rounding scheme so as to better reconstruct each layer or block output. In this work, we propose a simple yet effective new weight-rounding mechanism for PTQ, coined FlexRound, based on element-wise division instead of typical element-wise addition such that FlexRound enables jointly learning a common quantization grid size as well as a different scale for each pre-trained weight. Thanks to the reciprocal rule of derivatives induced by element-wise division, FlexRound is inherently able to exploit pre-trained weights when updating their corr
    
[^20]: 基于第二类传统机器学习验证方法的对抗感知深度学习系统

    Adversarial-Aware Deep Learning System based on a Secondary Classical Machine Learning Verification Approach. (arXiv:2306.00314v1 [cs.CR])

    [http://arxiv.org/abs/2306.00314](http://arxiv.org/abs/2306.00314)

    传统机器学习模型可以作为深度学习模型的辅助验证系统，可以有效防御各种对抗攻击，成功率高达97.32%。

    

    深度学习模型已经被用于创建各种有效的图像分类应用程序。然而，它们容易受到对抗攻击，这些攻击试图将模型引导到预测错误的类别。我们研究了主要的对抗攻击模型，发现它们都特别针对和利用其设计中的神经网络结构。这种理解使我们发展了一种假设，认为大多数传统机器学习模型，如随机森林(RF)，对抗攻击模型免疫，因为它们根本不依赖于神经网络设计。我们对受欢迎的对抗攻击进行的传统机器学习模型的实验研究支持了这一假设。基于这个假设，我们提出了一种新的对抗感知深度学习系统，通过使用传统机器学习模型作为辅助验证系统来补充图像分类中的主要深度学习模型。虽然第二个传统机器学习模型的准确率不如深度学习模型高，但它作为强有力的防御措施，对抗攻击的成功率高达97.32%。

    Deep learning models have been used in creating various effective image classification applications. However, they are vulnerable to adversarial attacks that seek to misguide the models into predicting incorrect classes. Our study of major adversarial attack models shows that they all specifically target and exploit the neural networking structures in their designs. This understanding makes us develop a hypothesis that most classical machine learning models, such as Random Forest (RF), are immune to adversarial attack models because they do not rely on neural network design at all. Our experimental study of classical machine learning models against popular adversarial attacks supports this hypothesis. Based on this hypothesis, we propose a new adversarial-aware deep learning system by using a classical machine learning model as the secondary verification system to complement the primary deep learning model in image classification. Although the secondary classical machine learning model
    
[^21]: Transformers学习实现预处理梯度下降用于上下文学习

    Transformers learn to implement preconditioned gradient descent for in-context learning. (arXiv:2306.00297v1 [cs.LG])

    [http://arxiv.org/abs/2306.00297](http://arxiv.org/abs/2306.00297)

    本论文通过研究线性transformers在训练过程中的全局最小值，证明了对于一个注意力层，transformers能够实现一次预处理梯度下降，而对于一个$k$个注意力层的transformer，它可以实现多次预处理梯度下降。

    

    受到transformers在上下文学习方面的显著能力的驱动，一些研究表明transformers可以实现像梯度下降这样的算法。通过精心的权重构造，这些研究表明多层transformers具有足够的表达能力来模拟梯度下降迭代。超越表达能力的问题，我们问：transformers能否通过在随机问题实例上训练来学习实现这样的算法？据我们所知，通过对线性回归的随机实例进行训练，我们第一次在这个问题上取得了理论进展，通过对线性transformers的损失函数进行分析。对于一个注意力层，我们证明了训练目标的全局最小值实现了一次预处理梯度下降。值得注意的是，预处理矩阵不仅适应输入分布，而且还适应于数据不充分引起的方差。对于一个具有$k$个注意力层的transformer，我们证明了一定条件下它可以实现多次预处理梯度下降。

    Motivated by the striking ability of transformers for in-context learning, several works demonstrate that transformers can implement algorithms like gradient descent. By a careful construction of weights, these works show that multiple layers of transformers are expressive enough to simulate gradient descent iterations. Going beyond the question of expressivity, we ask: Can transformers learn to implement such algorithms by training over random problem instances? To our knowledge, we make the first theoretical progress toward this question via analysis of the loss landscape for linear transformers trained over random instances of linear regression. For a single attention layer, we prove the global minimum of the training objective implements a single iteration of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the input distribution but also to the variance induced by data inadequacy. For a transformer with $k$ attention layers, we prove certain 
    
[^22]: 模拟共情过程的可解释建模架构EMOTE

    EMOTE: An Explainable architecture for Modelling the Other Through Empathy. (arXiv:2306.00295v1 [cs.AI])

    [http://arxiv.org/abs/2306.00295](http://arxiv.org/abs/2306.00295)

    EMOTE是一个可解释的建模架构，用于模拟另一个智能体的动作价值函数，通过共情的想象网络将其他智能体的观察状态转换成可解释的“共情状态”。该方法在多智能体游戏中取得了比现有方法更好的性能。

    

    我们通常可以假设他人与我们有类似的目标。这种假设有时也适用于多智能体游戏，例如，Agent 1对绿色颗粒的吸引类比于Agent 2对红色颗粒的吸引。这种“类比”假设与共情认知过程密切相关。受到共情的启发，我们设计了一个简单且可解释的建模架构，用于模拟另一个智能体的动作价值函数。这涉及学习一个“想象网络”，以转换其他智能体的观察状态，从而产生可解释的“共情状态”，当呈现给学习智能体时，会产生模仿其他智能体行为的行为。我们的方法适用于由单个学习智能体和根据固定策略行动的其他（独立）智能体组成的多智能体场景。该架构特别适用于使用复合值或奖励函数的算法。我们展示了我们的方法在具有挑战性的多智能体游戏环境中产生比现有方法更好的性能。

    We can usually assume others have goals analogous to our own. This assumption can also, at times, be applied to multi-agent games - e.g. Agent 1's attraction to green pellets is analogous to Agent 2's attraction to red pellets. This "analogy" assumption is tied closely to the cognitive process known as empathy. Inspired by empathy, we design a simple and explainable architecture to model another agent's action-value function. This involves learning an "Imagination Network" to transform the other agent's observed state in order to produce a human-interpretable "empathetic state" which, when presented to the learning agent, produces behaviours that mimic the other agent. Our approach is applicable to multi-agent scenarios consisting of a single learning agent and other (independent) agents acting according to fixed policies. This architecture is particularly beneficial for (but not limited to) algorithms using a composite value or reward function. We show our method produces better perfo
    
[^23]: 无训练神经网络结构搜索在RNN和Transformer上的应用

    Training-free Neural Architecture Search for RNNs and Transformers. (arXiv:2306.00288v1 [cs.LG])

    [http://arxiv.org/abs/2306.00288](http://arxiv.org/abs/2306.00288)

    本文研究了适用于语言模型任务的循环神经网络和Transformer的无训练神经网络结构搜索算法，并开发了一个新的性能预测指标hidden covariance，可显著优化现有指标。

    

    神经网络结构搜索(NAS)可以自动创建新的有效的神经网络架构，为手动设计复杂架构提供了一种选择。然而，传统的NAS算法速度较慢，需要大量计算力。最近的研究探讨了用于图像分类架构的无训练NAS指标，极大地加速了搜索算法。本文研究针对语言建模任务的循环神经网络(RNN)和BERT-based transformer架构的无训练NAS指标。首先，我们开发了一个名为hidden covariance的新的无训练指标，可以预测训练后RNN架构的性能，并显著优于现有的无训练指标。其次，我们发现目前的转换器搜索空间范例

    Neural architecture search (NAS) has allowed for the automatic creation of new and effective neural network architectures, offering an alternative to the laborious process of manually designing complex architectures. However, traditional NAS algorithms are slow and require immense amounts of computing power. Recent research has investigated training-free NAS metrics for image classification architectures, drastically speeding up search algorithms. In this paper, we investigate training-free NAS metrics for recurrent neural network (RNN) and BERT-based transformer architectures, targeted towards language modeling tasks. First, we develop a new training-free metric, named hidden covariance, that predicts the trained performance of an RNN architecture and significantly outperforms existing training-free metrics. We experimentally evaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP benchmark. Second, we find that the current search space paradigm for transformer
    
[^24]: 双重稳健自我训练

    Doubly Robust Self-Training. (arXiv:2306.00265v1 [cs.LG])

    [http://arxiv.org/abs/2306.00265](http://arxiv.org/abs/2306.00265)

    本文提出了一种双重稳健自我训练算法，可以在伪标签不准确和完全准确时分别采取不同的训练策略，实现有效的半监督学习。实验结果表明，该算法在ImageNet和nuScenes数据集上均比标准自我训练总结更好。

    

    自我训练是解决半监督学习问题的一种重要技术。它通过生成伪标签并将其与有限的标记数据集结合使用进行训练，从而利用无标签数据。自我训练的有效性在很大程度上依赖于这些伪标签的准确性。本文引入了双重稳健自我训练，这是一种新颖的半监督算法，可以保证在两个极端之间平衡。当伪标签完全不正确时，我们的方法将被减少到仅使用标记数据进行训练。相反，当伪标签完全准确时，我们的方法将变成利用所有伪标签数据和标记数据进行训练的过程，从而增加有效的样本量。通过在ImageNet图像分类和nuScenes自主驾驶数据集上的实证评估，我们证明了双重稳健损失优于标准自我训练基线的优越性。

    Self-training is an important technique for solving semi-supervised learning problems. It leverages unlabeled data by generating pseudo-labels and combining them with a limited labeled dataset for training. The effectiveness of self-training heavily relies on the accuracy of these pseudo-labels. In this paper, we introduce doubly robust self-training, a novel semi-supervised algorithm that provably balances between two extremes. When the pseudo-labels are entirely incorrect, our method reduces to a training process solely using labeled data. Conversely, when the pseudo-labels are completely accurate, our method transforms into a training process utilizing all pseudo-labeled data and labeled data, thus increasing the effective sample size. Through empirical evaluations on both the ImageNet dataset for image classification and the nuScenes autonomous driving dataset for 3D object detection, we demonstrate the superiority of the doubly robust loss over the standard self-training baseline.
    
[^25]: BetaZero：基于学习的近似算法的置信状态规划用于长时间跨度的POMDPs

    BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned Approximations. (arXiv:2306.00249v1 [cs.AI])

    [http://arxiv.org/abs/2306.00249](http://arxiv.org/abs/2306.00249)

    本文提出了一种叫做BetaZero的方法，它是一种使用学习近似算法的置信状态规划算法，可以用于解决长时间跨度的POMDP问题。

    

    实际的规划问题，包括自动驾驶、碳储存和资源勘探等可持续能源应用，最近被建模为部分观测马尔可夫决策过程（POMDPs）并使用近似方法解决。为了在实践中解决高维度POMDPs，最先进的方法使用了问题特定的启发式算法进行在线规划，以减少规划时间跨度并使问题易于解决。最近成功地在完全可观察的领域中找到了用于替换启发式算法的学习近似算法。关键洞见是将在线蒙特卡罗树搜索与离线神经网络近似相结合，以优化策略和值函数。本文将这一洞见应用到了部分观察域，并提出了BetaZero，一种适用于POMDP的置信状态规划算法。

    Real-world planning problems$\unicode{x2014}$including autonomous driving and sustainable energy applications like carbon storage and resource exploration$\unicode{x2014}$have recently been modeled as partially observable Markov decision processes (POMDPs) and solved using approximate methods. To solve high-dimensional POMDPs in practice, state-of-the-art methods use online planning with problem-specific heuristics to reduce planning horizons and make the problems tractable. Algorithms that learn approximations to replace heuristics have recently found success in large-scale problems in the fully observable domain. The key insight is the combination of online Monte Carlo tree search with offline neural network approximations of the optimal policy and value function. In this work, we bring this insight to partially observed domains and propose BetaZero, a belief-state planning algorithm for POMDPs. BetaZero learns offline approximations based on accurate belief models to enable online d
    
[^26]: TransAct: Pinterest实时用户行为模型中的Transformer-Based推荐

    TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest. (arXiv:2306.00248v1 [cs.IR])

    [http://arxiv.org/abs/2306.00248](http://arxiv.org/abs/2306.00248)

    本文介绍了Pinterest推荐系统的架构和TransAct模型。TransAct是一个从用户实时活动中提取短期偏好的序列模型。本文还介绍了通过混合排名方法结合直接在实时用户活动上学习和在较长时间段内学习批量用户表示的优点。

    

    编码用户活动以进行下一步行动预测的序列模型已成为构建大规模个性化推荐系统的常见设计选择。 传统的序列推荐方法要么利用实时用户操作进行端到端学习，要么以脱机批量生成的方式单独学习用户表示。 本文(1)介绍了Pinterest Homefeed的排名架构，即我们的个性化推荐产品和最大的参与面；(2)提出了TransAct，一种从用户实时活动中提取短期偏好的序列模型；(3)描述了我们的混合排名方法，即通过TransAct的端到端序列建模与批量生成的用户嵌入混合。 混合方法允许我们结合直接在实时用户活动上学习以获得响应性的优点和在较长时间段内学习的批量用户表示的成本效益。 我们描述了实验结果......（原文内容省略）

    Sequential models that encode user activity for next action prediction have become a popular design choice for building web-scale personalized recommendation systems. Traditional methods of sequential recommendation either utilize end-to-end learning on realtime user actions, or learn user representations separately in an offline batch-generated manner. This paper (1) presents Pinterest's ranking architecture for Homefeed, our personalized recommendation product and the largest engagement surface; (2) proposes TransAct, a sequential model that extracts users' short-term preferences from their realtime activities; (3) describes our hybrid approach to ranking, which combines end-to-end sequential modeling via TransAct with batch-generated user embeddings. The hybrid approach allows us to combine the advantages of responsiveness from learning directly on realtime user activity with the cost-effectiveness of batch user representations learned over a longer time period. We describe the resu
    
[^27]: 使用视觉裁剪来增强BLIP模型细节问题回答的能力

    Using Visual Cropping to Enhance Fine-Detail Question Answering of BLIP-Family Models. (arXiv:2306.00228v1 [cs.CV])

    [http://arxiv.org/abs/2306.00228](http://arxiv.org/abs/2306.00228)

    本文研究如何通过使用视觉裁剪来提高BLIP模型在细节问题上的表现能力。

    

    视觉问答是一项具有挑战性的任务，需要感知、语言和背景知识系统之间的无缝交互。虽然最近视觉和自然语言模型的进展如BLIP已经提高了这项任务的性能，但我们缺乏理解这些模型在不同类型的问题和推理类型上表现的能力。鉴于我们对BLIP家族模型进行的初步分析显示出回答细节问题方面的困难，我们研究以下问题：能否使用视觉裁剪来提高最先进的视觉问答模型在细节问题上的性能？考虑到BLIP家族模型的最近成功，我们研究了一个零样本和一个微调的BLIP模型。我们定义了三个受控的子集，以衡量裁剪是否有助于模型性能。除了人工裁剪外，我们还设计了两种基于多模态嵌入的自动裁剪策略，这些策略以CLIP为基础。

    Visual Question Answering is a challenging task, as it requires seamless interaction between perceptual, linguistic, and background knowledge systems. While the recent progress of visual and natural language models like BLIP has led to improved performance on this task, we lack understanding of the ability of such models to perform on different kinds of questions and reasoning types. As our initial analysis of BLIP-family models revealed difficulty with answering fine-detail questions, we investigate the following question: Can visual cropping be employed to improve the performance of state-of-the-art visual question answering models on fine-detail questions? Given the recent success of the BLIP-family models, we study a zero-shot and a fine-tuned BLIP model. We define three controlled subsets of the popular VQA-v2 benchmark to measure whether cropping can help model performance. Besides human cropping, we devise two automatic cropping strategies based on multi-modal embedding by CLIP 
    
[^28]: 扩散画笔：基于潜在扩散模型的AI生成图像编辑工具

    Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images. (arXiv:2306.00219v1 [cs.CV])

    [http://arxiv.org/abs/2306.00219](http://arxiv.org/abs/2306.00219)

    本论文提出了一种名为扩散画笔的基于潜在扩散模型的AI图像微调工具，可以有效地根据目标区域修改AI合成图像并保留原始上下文。与其他最先进的图像修复技术进行比较，该方法在用户研究中表现出更好的可用性和有效性。

    

    文本到图像的生成模型在生成高质量图像方面取得了显著的进展。然而，由于模型限制，生成的图像经常包含不良的伪影或其他错误。现有的微调生成图像的技术要么耗时（手动编辑），要么产生不够完美的结果（修补），要么会导致整体图像产生意想不到的变化（变体选择和提示微调）。本文提出了一种名为扩散画笔的基于潜在扩散模型的AI图像微调工具，可以有效地微调AI合成图像中所需的区域。我们的方法在反向扩散过程中在目标区域引入了新的随机噪声模式，使模型能够在保留其他区域原始上下文的同时，高效地对指定区域进行更改。我们通过艺术家进行的用户研究评估了我们方法的可用性和有效性，将我们的技术与其他最先进的图像修复技术进行了比较。

    Text-to-image generative models have made remarkable advancements in generating high-quality images. However, generated images often contain undesirable artifacts or other errors due to model limitations. Existing techniques to fine-tune generated images are time-consuming (manual editing), produce poorly-integrated results (inpainting), or result in unexpected changes across the entire image (variation selection and prompt fine-tuning). In this work, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to efficiently fine-tune desired regions within an AI-synthesized image. Our method introduces new random noise patterns at targeted regions during the reverse diffusion process, enabling the model to efficiently make changes to the specified regions while preserving the original context for the rest of the image. We evaluate our method's usability and effectiveness through a user study with artists, comparing our technique against other state-of-the-art image inpaintin
    
[^29]: 多智能体强化学习中的安全性优化技术——泛化Lagrangian策略优化的可证明高效性

    Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning. (arXiv:2306.00212v1 [cs.LG])

    [http://arxiv.org/abs/2306.00212](http://arxiv.org/abs/2306.00212)

    本文研究了在有约束的马尔可夫博弈中，多个智能体通过最大化总体奖励并在对总体效用的期望值上设置约束条件进行多智能体强化学习。通过泛化Lagrangian策略优化，采用基于占用测量的方法，并利用置信上限强化学习算法，实现对约束的处理。

    

    本文研究在有约束的马尔科夫博弈中，互相竞争的多个智能体通过最大化总体奖励并在对总体效用的期望值上设置约束条件进行在线安全性多智能体强化学习。我们专注于一个拥有两个独立转移函数，对智能体未知的且存在对抗性奖励函数和随机效用函数的双人零和约束马尔可夫博弈。为了解决该马尔可夫博弈问题，我们采用基于占用测量的方法将其描述为带有显式约束的在线约束鞍点问题。我们将在约束优化中推广Lagrange乘数方法并创建一个带有Minimax决策原始变量和双变量的广义Lagrangian处理约束。接下来，我们开发了一个基于置信上限的强化学习算法来解决这个问题，实现探索和利用的平衡。我们的算法通过在线镜像更新Minimax决策原始变量。

    We examine online safe multi-agent reinforcement learning using constrained Markov games in which agents compete by maximizing their expected total rewards under a constraint on expected total utilities. Our focus is confined to an episodic two-player zero-sum constrained Markov game with independent transition functions that are unknown to agents, adversarial reward functions, and stochastic utility functions. For such a Markov game, we employ an approach based on the occupancy measure to formulate it as an online constrained saddle-point problem with an explicit constraint. We extend the Lagrange multiplier method in constrained optimization to handle the constraint by creating a generalized Lagrangian with minimax decision primal variables and a dual variable. Next, we develop an upper confidence reinforcement learning algorithm to solve this Lagrangian problem while balancing exploration and exploitation. Our algorithm updates the minimax decision primal variables via online mirror
    
[^30]: 探究Adam在Transformers上比SGD更快收敛的原因

    Toward Understanding Why Adam Converges Faster Than SGD for Transformers. (arXiv:2306.00204v1 [cs.LG])

    [http://arxiv.org/abs/2306.00204](http://arxiv.org/abs/2306.00204)

    本文研究了Adam在Transformer的训练中为什么比SGD更快收敛，提出了方向锐度的概念并通过比较证明了SGD相对于自适应算法具有更差的方向锐度，进而在理论上证明只需要裁剪一小部分坐标即可提高SGD的表现。

    

    虽然随机梯度下降（SGD）仍是深度学习中最受欢迎的优化算法，但是像Adam这样的自适应算法在一些深度学习应用中已经证明了优于SGD的经验优势，特别是在Transformer的训练上。然而，为什么Adam在这些情况下会显著比SGD更快地收敛仍然是一个问题。本文提出了一个新的概念——方向锐度，解释了为什么Adam比SGD更快地收敛。我们认为优化算法的性能与更新步骤的方向锐度密切相关，证明了SGD相对于自适应算法具有更差的方向锐度。我们进一步观察到只有一小部分坐标导致了SGD的锐度不足和收敛速度缓慢，提出了使用坐标级别的裁剪作为解决方案。我们展示了坐标级别裁剪对于锐度降低和加速收敛的效果。

    While stochastic gradient descent (SGD) is still the most popular optimization algorithm in deep learning, adaptive algorithms such as Adam have established empirical advantages over SGD in some deep learning applications such as training transformers. However, it remains a question that why Adam converges significantly faster than SGD in these scenarios. In this paper, we propose one explanation of why Adam converges faster than SGD using a new concept directional sharpness. We argue that the performance of optimization algorithms is closely related to the directional sharpness of the update steps, and show SGD has much worse directional sharpness compared to adaptive algorithms. We further observe that only a small fraction of the coordinates causes the bad sharpness and slow convergence of SGD, and propose to use coordinate-wise clipping as a solution to SGD and other optimization algorithms. We demonstrate the effect of coordinate-wise clipping on sharpness reduction and speeding u
    
[^31]: SSL-CPCD：自我监督的组合预文本类别区分学习，提高内窥镜图像分析的泛化性能

    SSL-CPCD: Self-supervised learning with composite pretext-class discrimination for improved generalisability in endoscopic image analysis. (arXiv:2306.00197v1 [cs.CV])

    [http://arxiv.org/abs/2306.00197](http://arxiv.org/abs/2306.00197)

    本论文提出了一种名为SSL-CPCD的新方法，通过针对补丁级别的实例组区分和跨类别变化的惩罚，在使用自我监督学习提高内窥镜图像分析泛化性能方面取得了重要的进展。

    

    数据驱动方法在医学图像分析方面取得了巨大进步。在这个背景下，基于深度学习的监督方法广受欢迎。然而，它们需要大量的训练数据，并面临泛化到未见过数据集的问题，这影响了临床转化。内窥镜成像数据具有大的患者内外变异性，这使得这些模型更具有挑战性，无法学习代表下游任务的特征。因此，尽管公开可用的数据集和医院内部可生成的数据集，大多数监督模型仍表现不佳。而自我监督学习在自然场景数据中对该问题做了一定程度的解决，但在医学图像领域存在显著的性能差距。在本文中，我们提出了探索基于补丁的实例组区分和使用余弦相似度度量中的加性角度边距对跨类别变化的惩罚的方法。我们的新方法使模型能够更好地学习代表下游任务的特征，从而提高了内窥镜图像分析的泛化性能。

    Data-driven methods have shown tremendous progress in medical image analysis. In this context, deep learning-based supervised methods are widely popular. However, they require a large amount of training data and face issues in generalisability to unseen datasets that hinder clinical translation. Endoscopic imaging data incorporates large inter- and intra-patient variability that makes these models more challenging to learn representative features for downstream tasks. Thus, despite the publicly available datasets and datasets that can be generated within hospitals, most supervised models still underperform. While self-supervised learning has addressed this problem to some extent in natural scene data, there is a considerable performance gap in the medical image domain. In this paper, we propose to explore patch-level instance-group discrimination and penalisation of inter-class variation using additive angular margin within the cosine similarity metrics. Our novel approach enables mode
    
[^32]: 预训练表示中的扩散冗余

    Diffused Redundancy in Pre-trained Representations. (arXiv:2306.00183v1 [cs.LG])

    [http://arxiv.org/abs/2306.00183](http://arxiv.org/abs/2306.00183)

    本文发现预训练表示中存在一定程度的扩散冗余，即随机选择一部分神经元与全部神经元相似度高且性能相似，可用于降低实际部署中的计算和内存成本，同时保持相当的性能水平。

    

    在大型数据集上预训练神经网络获得的表示已被越来越多地成功应用于各种下游任务中。在本文中，我们更加深入地研究了这种预训练表示中的特征是如何被编码的。我们发现，在给定层中学到的表示展现出一定程度的扩散冗余，即对于超过一个阈值大小的任何随机子集神经元，都与完整层具有很高的相似度，并且在各种下游任务中能够表现出与整个层相似的性能。我们在各种不同的神经架构（包括CNN和Transformer）上进行了实验，使用了ImageNet1k和ImageNet21k进行预训练，并评估了各种下游任务，如图像分类、目标检测和自然语言处理。我们的实验结果表明，可以利用预训练表示中的冗余来降低在实际部署中使用这些模型的计算和内存成本，同时仍然保持相当的性能水平。

    Representations learned by pre-training a neural network on a large dataset are increasingly used successfully to perform a variety of downstream tasks. In this work, we take a closer look at how features are encoded in such pre-trained representations. We find that learned representations in a given layer exhibit a degree of diffuse redundancy, i.e., any randomly chosen subset of neurons in the layer that is larger than a threshold size shares a large degree of similarity with the full layer and is able to perform similarly as the whole layer on a variety of downstream tasks. For example, a linear probe trained on $20\%$ of randomly picked neurons from a ResNet50 pre-trained on ImageNet1k achieves an accuracy within $5\%$ of a linear probe trained on the full layer of neurons for downstream CIFAR10 classification. We conduct experiments on different neural architectures (including CNNs and Transformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a variety of downstrea
    
[^33]: FlowCam: 通过像素对齐场景流，无需相机位姿训练具有通用性的三维辐射场

    FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow. (arXiv:2306.00180v1 [cs.CV])

    [http://arxiv.org/abs/2306.00180](http://arxiv.org/abs/2306.00180)

    本文提出了一种利用可微分渲染将帧间光流映射到三维场景流以同步重建相机位姿和三维神经场表示的方法。这种方法不需要精确相机位姿，可在真实世界的视频数据上进行全自监督训练。

    

    从定位图像中重建三维神经场已成为自监督表示学习的一种有前途的方法。但依赖于大规模视频数据上结构光学精确相机位姿，这也是其无法推广的关键挑战。我们提出了一种联合在线重建相机位姿和三维神经场表示的方法，并通过单次前向传递来完成。该方法通过可微分渲染将帧间光流映射到三维场景流，以估计位姿。然后再通过对场景流场进行加权最小二乘拟合来执行SE（3）相机位姿估计。该方法能够通过重新渲染输入视频来联合监督位姿估计和通用的神经场表示，从而在真实世界的视频数据上进行全自监督训练。

    Reconstruction of 3D neural fields from posed images has emerged as a promising method for self-supervised representation learning. The key challenge preventing the deployment of these 3D scene learners on large-scale video data is their dependence on precise camera poses from structure-from-motion, which is prohibitively expensive to run at scale. We propose a method that jointly reconstructs camera poses and 3D neural scene representations online and in a single forward pass. We estimate poses by first lifting frame-to-frame optical flow to 3D scene flow via differentiable rendering, preserving locality and shift-equivariance of the image processing backbone. SE(3) camera pose estimation is then performed via a weighted least-squares fit to the scene flow field. This formulation enables us to jointly supervise pose estimation and a generalizable neural scene representation via re-rendering the input video, and thus, train end-to-end and fully self-supervised on real-world video datas
    
[^34]: 基于生成式人工智能的自动标注需要验证

    Automated Annotation with Generative AI Requires Validation. (arXiv:2306.00176v1 [cs.CL])

    [http://arxiv.org/abs/2306.00176](http://arxiv.org/abs/2306.00176)

    本文探讨了利用生成式大型语言模型进行文本注释的自动化流程，强调必须针对人类生成的标签验证它们的性能，验证结果表明LLM的性能很有前途，但高度依赖于数据集和注释任务类型。

    

    生成式大型语言模型（LLM）可以成为文本注释过程的强大工具，但由于提示质量，文本数据特定性和概念难度等原因，它们的性能在注释任务方面存在差异。因为即使LLM技术得到改进，这些挑战仍将存在，所以我们认为使用LLM的任何自动标注过程都必须针对人类生成的标签验证LLM的性能。为此，我们概述了一个工作流程，以一种原则性和有效率的方式利用LLM的注释潜力。使用GPT-4，我们通过在高影响期刊的最新社会科学文章中复制11个数据集的27个标注任务来验证这种方法。我们发现，对于文本注释，LLM的性能很有前途，但高度取决于数据集和注释任务类型，这强调了按任务验证的必要性。我们提供易于使用的软件，旨在实现我们的工作流程并简化操作。

    Generative large language models (LLMs) can be a powerful tool for augmenting text annotation procedures, but their performance varies across annotation tasks due to prompt quality, text data idiosyncrasies, and conceptual difficulty. Because these challenges will persist even as LLM technology improves, we argue that any automated annotation process using an LLM must validate the LLM's performance against labels generated by humans. To this end, we outline a workflow to harness the annotation potential of LLMs in a principled, efficient way. Using GPT-4, we validate this approach by replicating 27 annotation tasks across 11 datasets from recent social science articles in high-impact journals. We find that LLM performance for text annotation is promising but highly contingent on both the dataset and the type of annotation task, which reinforces the necessity to validate on a task-by-task basis. We make available easy-to-use software designed to implement our workflow and streamline the
    
[^35]: 新康姆类问题上决策算法的比较

    A Comparison of Decision Algorithms on Newcomblike Problems. (arXiv:2306.00175v1 [cs.AI])

    [http://arxiv.org/abs/2306.00175](http://arxiv.org/abs/2306.00175)

    本文比较了三种决策算法在新康姆类问题上的表现，结果发现无时间的决策理论在这些问题上具有明显优势。

    

    当采用贝叶斯网络的形式进行表述时，两种标准的决策算法（证据决策理论和因果决策理论）在犯人困境和所谓的“新康姆类”问题方面面临系统性失败。我们描述了一种新的决策算法，称为无时间的决策理论，该理论在这些问题上始终获胜。

    When formulated using Bayesian networks, two standard decision algorithms (Evidential Decision Theory and Causal Decision Theory) can be shown to fail systematically when faced with aspects of the prisoner's dilemma and so-called "Newcomblike" problems. We describe a new form of decision algorithm, called Timeless Decision Theory, which consistently wins on these problems.
    
[^36]: 基于符号回归的信息融合：以人类健康为背景的教程

    Information Fusion via Symbolic Regression: A Tutorial in the Context of Human Health. (arXiv:2306.00153v1 [cs.LG])

    [http://arxiv.org/abs/2306.00153](http://arxiv.org/abs/2306.00153)

    本教程论文介绍了符号回归技术，以人类健康和营养领域的实际应用为例，将各种人体测量指标融合成一个简单的数学表达式以估算体脂百分比。

    

    本教程论文提供了符号回归（SR）的一般概述，重点关注可解释性标准。我们认为，尽管文献中对可解释性建模的定义仍有争议，但可解释性模型是支持评估成功信息融合的实用方式。为了传达SR作为建模技术的优点，我们利用来自疾病控制和预防中心（CDC）的公开可用的国家健康和营养调查（NHANES）数据，在健康和营养领域展示了一个应用，将各种人体测量指标融合成一个简单的数学表达式以估算体脂百分比。我们讨论了SR建模的优点和挑战，并提供了所学模型的定性和定量分析。

    This tutorial paper provides a general overview of symbolic regression (SR) with specific focus on standards of interpretability. We posit that interpretable modeling, although its definition is still disputed in the literature, is a practical way to support the evaluation of successful information fusion. In order to convey the benefits of SR as a modeling technique, we demonstrate an application within the field of health and nutrition using publicly available National Health and Nutrition Examination Survey (NHANES) data from the Centers for Disease Control and Prevention (CDC), fusing together anthropometric markers into a simple mathematical expression to estimate body fat percentage. We discuss the advantages and challenges associated with SR modeling and provide qualitative and quantitative analyses of the learned models.
    
[^37]: 关于神经网络表达能力的研究

    On the Expressive Power of Neural Networks. (arXiv:2306.00145v1 [math.CA])

    [http://arxiv.org/abs/2306.00145](http://arxiv.org/abs/2306.00145)

    本文研究了神经网络的表达能力，证明了某些问题在之前的研究中未得到解决，并提出了新问题的回答，包括广而浅的ReLU网络不能被深而窄的ReLU网络很好地逼近等。

    

    1989年，George Cybenko在一篇里程碑式的论文中证明了宽而浅的神经网络可以在紧致集上逼近任意连续函数，这个通用逼近定理引发了很多后续研究。本文将通过一个框架回答“有没有一些广而浅的ReLU网络无法被深而窄的ReLU网络很好地逼近？”“普遍逼近定理是否仍适用于Sobolev空间范数W 1,1？”“这些结果是否适用于除ReLU之外的激活函数？”等问题。

    In 1989 George Cybenko proved in a landmark paper that wide shallow neural networks can approximate arbitrary continuous functions on a compact set. This universal approximation theorem sparked a lot of follow-up research.  Shen, Yang and Zhang determined optimal approximation rates for ReLU-networks in $L^p$-norms with $p \in [1,\infty)$. Kidger and Lyons proved a universal approximation theorem for deep narrow ReLU-networks. Telgarsky gave an example of a deep narrow ReLU-network that cannot be approximated by a wide shallow ReLU-network unless it has exponentially many neurons.  However, there are even more questions that still remain unresolved. Are there any wide shallow ReLU-networks that cannot be approximated well by deep narrow ReLU-networks? Is the universal approximation theorem still true for other norms like the Sobolev norm $W^{1,1}$? Do these results hold for activation functions other than ReLU?  We will answer all of those questions and more with a framework of two exp
    
[^38]: 加拿大农田数据集：用于农业多时相深度学习分类的新地表覆盖数据集

    The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture. (arXiv:2306.00114v1 [cs.CV])

    [http://arxiv.org/abs/2306.00114](http://arxiv.org/abs/2306.00114)

    该论文提出了一个时间补丁数据集，包含了加拿大农田的多时相遥感影像。该数据集是手动经过确认和筛选的高分辨率地理参考图像，覆盖四个农作物生产年度和五个月份。这个数据集可以用于提高土地覆盖分类的准确性。

    

    利用遥感监测土地覆盖是研究环境变化和通过粮食产量预测确保全球粮食安全的关键。尤其是，多时相遥感影像提供了关于场景动态的相关信息，已经被证明可以带来更好的土地覆盖分类结果。然而，由于难以获取可靠、细粒度和高质量的注释样本支持他们的假设，很少有研究受益于高空间和时间分辨率数据。因此，我们介绍了一个加拿大农田的时间补丁数据集，其中包含了来自10个农作物类别的78,536个手动经过确认和筛选的高分辨率(10米/像素，640 x 640米)地理参考图像，覆盖了四个农作物生产年度(2017-2020)和五个月份(六月-十月)。每个实例都包含12个光谱波段、一张RGB图像和额外的植被指数计算。

    Monitoring land cover using remote sensing is vital for studying environmental changes and ensuring global food security through crop yield forecasting. Specifically, multitemporal remote sensing imagery provides relevant information about the dynamics of a scene, which has proven to lead to better land cover classification results. Nevertheless, few studies have benefited from high spatial and temporal resolution data due to the difficulty of accessing reliable, fine-grained and high-quality annotated samples to support their hypotheses. Therefore, we introduce a temporal patch-based dataset of Canadian croplands, enriched with labels retrieved from the Canadian Annual Crop Inventory. The dataset contains 78,536 manually verified and curated high-resolution (10 m/pixel, 640 x 640 m) geo-referenced images from 10 crop classes collected over four crop production years (2017-2020) and five months (June-October). Each instance contains 12 spectral bands, an RGB image, and additional veget
    
[^39]: MuseCoco: 从文本生成符号音乐

    MuseCoco: Generating Symbolic Music from Text. (arXiv:2306.00110v1 [cs.SD])

    [http://arxiv.org/abs/2306.00110](http://arxiv.org/abs/2306.00110)

    MuseCoco是一种从文本描述中生成符号音乐的系统，具备高效和灵活等特点，为音乐家提供了更好的音乐生成方式。

    

    从文本描述中生成音乐是一种用户友好的方式，因为文本是相对易于用户参与的界面。而有些方法利用文本来控制音乐音频的生成，但是编辑生成音频的音乐元素对于用户来说是具有挑战性的。相比之下，符号音乐具有易于编辑的优点，使用户更容易操作特定的音乐元素。本文介绍了MuseCoco，它利用音乐属性作为桥梁，将任务分解为文本到属性理解和属性到音乐生成的两个阶段，从而生成符号音乐。MuseCoCo代表音乐作曲副驾驶，使音乐家可以直接从给定的文本描述中生成音乐，与从头开始创作相比，大大提高了效率。该系统具有两个主要优点：数据高效。在属性到音乐生成阶段，属性可以直接进行编码，而不需要大量的音乐数据。其次，此系统具有高级别的灵活性，因为它可以通过变更文本输入来生成多样化且有个性的符号音乐。

    Generating music from text descriptions is a user-friendly mode since the text is a relatively easy interface for user engagement. While some approaches utilize texts to control music audio generation, editing musical elements in generated audio is challenging for users. In contrast, symbolic music offers ease of editing, making it more accessible for users to manipulate specific musical elements. In this paper, we propose MuseCoco, which generates symbolic music from text descriptions with musical attributes as the bridge to break down the task into text-to-attribute understanding and attribute-to-music generation stages. MuseCoCo stands for Music Composition Copilot that empowers musicians to generate music directly from given text descriptions, offering a significant improvement in efficiency compared to creating music entirely from scratch. The system has two main advantages: Firstly, it is data efficient. In the attribute-to-music generation stage, the attributes can be directly e
    
[^40]: MERT:带有大规模自监督训练的声学音乐理解模型

    MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])

    [http://arxiv.org/abs/2306.00107](http://arxiv.org/abs/2306.00107)

    提出了一个带有大规模自监督训练的音乐理解模型MERT，利用了教师模型并采用了一种优于传统的语音和音频方法的组合方式。

    

    自监督学习（SSL）最近在视觉、文本和语音领域中已被证明是训练通用模型的一种很有前景的范例，对于跨越音乐领域的应用，尤其是对于调性和音高这样的特殊音乐知识的建模颇具挑战性。为了解决这一问题，我们提出了一个基于大规模自监督训练的声学音乐理解模型，即MERT。在我们的探索中，我们确定了更优秀的教师模型组合，这种组合方法在性能方面优于传统的语音和音频方法。

    Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
    
[^41]: AI图像和Overton Window

    AI Imagery and the Overton Window. (arXiv:2306.00080v1 [cs.CY])

    [http://arxiv.org/abs/2306.00080](http://arxiv.org/abs/2306.00080)

    基于人工智能的文本到图像生成技术的快速增长让手工艺术品和AI生成图像区分日渐困难。然而提高人类生活和工作标准以及利用一群人来充实另一群人之间寻求平衡是关键。该领域面临着被AI基础设施接管的风险，同时存在身份盗窃、数据洗白等问题。

    

    基于人工智能的文本到图像生成在过去一年中在视觉综合和美学形象的生产方面取得了重大进步，到了区分手工艺术品和AI生成的图像愈发困难的地步。例如稳态扩散、Midjourney等生成模型有望在技术和伦理方面影响几个主要行业。在提高人类生活和工作标准以及利用一群人来充实另一群人之间寻求平衡是讨论的复杂和关键部分。由于这种技术的快速增长、模型运行方式和灰色法律的存在，包括视频游戏行业在内的视觉和艺术领域面临被AI基础设施所有者接管的风险。该文章是一篇文献综述，探讨了当今AI开发者和用户面临的问题，包括身份盗窃、数据洗白等。

    AI-based text-to-image generation has undergone a significant leap in the production of visually comprehensive and aesthetic imagery over the past year, to the point where differentiating between a man-made piece of art and an AI-generated image is becoming more difficult. Generative Models such as Stable Diffusion, Midjourney and others are expected to affect several major industries in technological and ethical aspects. Striking the balance between raising human standard of life and work vs exploiting one group of people to enrich another is a complex and crucial part of the discussion. Due to the rapid growth of this technology, the way in which its models operate, and gray area legalities, visual and artistic domains - including the video game industry, are at risk of being taken over from creators by AI infrastructure owners. This paper is a literature review examining the concerns facing both AI developers and users today, including identity theft, data laundering and more. It di
    
[^42]: 量子机器学习的阴影

    Shadows of quantum machine learning. (arXiv:2306.00061v1 [quant-ph])

    [http://arxiv.org/abs/2306.00061](http://arxiv.org/abs/2306.00061)

    量子机器学习模型需要使用量子计算机进行评估，但我们提出在训练完后，使用量子计算机生成一个经典阴影模型来计算函数的经典计算近似，避免了对量子计算机的需求。

    

    量子机器学习经常被认为是利用量子计算机解决实际问题的最有前途的应用之一。然而，阻碍其在实践中广泛使用的主要障碍是这些模型即使在训练过程后，仍需要访问量子计算机才能对新数据进行评估。为解决这个问题，我们建议在量子模型的训练阶段之后，量子计算机可以用来生成我们所谓的该模型的“经典阴影”，即已学习函数的经典计算近似。虽然最近的研究已经探讨了这个想法并提出了构建这种影子模型的方法，但它们也提出了一个完全经典模型可能代替的可能性，从而首先回避了量子计算机的需要。本文采用新的方法，基于量子线性模型和经典阴影重构的框架来定义阴影模型。

    Quantum machine learning is often highlighted as one of the most promising uses for a quantum computer to solve practical problems. However, a major obstacle to the widespread use of quantum machine learning models in practice is that these models, even once trained, still require access to a quantum computer in order to be evaluated on new data. To solve this issue, we suggest that following the training phase of a quantum model, a quantum computer could be used to generate what we call a classical shadow of this model, i.e., a classically computable approximation of the learned function. While recent works already explore this idea and suggest approaches to construct such shadow models, they also raise the possibility that a completely classical model could be trained instead, thus circumventing the need for a quantum computer in the first place. In this work, we take a novel approach to define shadow models based on the frameworks of quantum linear models and classical shadow tomogr
    
[^43]: 进化优化中的彩票票现象：稀疏且无需反向传播训练

    Lottery Tickets in Evolutionary Optimization: On Sparse Backpropagation-Free Trainability. (arXiv:2306.00045v1 [cs.NE])

    [http://arxiv.org/abs/2306.00045](http://arxiv.org/abs/2306.00045)

    本文证明了进化策略算法具有高度稀疏可训练的初始参数，并比较了与梯度下降算法稀疏训练的差异。进化策略算法可以探索各种不同的、平坦的局部最优解，并且不会保留线性模式的连接性。

    

    彩票票现象是否只存在于梯度下降算法中，还是可以推广到进化优化算法中？本文证明了进化策略算法具有高度稀疏可训练的初始参数，并比较了与梯度下降算法稀疏训练的差异。我们提出了一种新的信噪比迭代剪枝过程，将损失曲率信息融入到网络剪枝步骤中，可以发现与梯度下降相比，黑盒进化算法方法有可能发现更稀疏且可训练的网络初始参数。此外，我们发现这些初始参数包含了归纳偏见，可以在不同的进化策略任务及梯度下降训练中进行传递。最后，我们比较了不同优化算法和稀疏水平产生的局部最优解，与梯度下降算法不同，进化策略算法可以探索各种不同的、平坦的局部最优解，并且不会保留线性模式的连接性。

    Is the lottery ticket phenomenon an idiosyncrasy of gradient-based training or does it generalize to evolutionary optimization? In this paper we establish the existence of highly sparse trainable initializations for evolution strategies (ES) and characterize qualitative differences compared to gradient descent (GD)-based sparse training. We introduce a novel signal-to-noise iterative pruning procedure, which incorporates loss curvature information into the network pruning step. This can enable the discovery of even sparser trainable network initializations when using black-box evolution as compared to GD-based optimization. Furthermore, we find that these initializations encode an inductive bias, which transfers across different ES, related tasks and even to GD-based training. Finally, we compare the local optima resulting from the different optimization paradigms and sparsity levels. In contrast to GD, ES explore diverse and flat local optima and do not preserve linear mode connectivi
    
[^44]: 空间网优化

    Space Net Optimization. (arXiv:2306.00043v1 [cs.AI])

    [http://arxiv.org/abs/2306.00043](http://arxiv.org/abs/2306.00043)

    SNO是一种新颖的元启发式算法，其使用空间网机制来绘制解空间的景观，可以使用大部分搜索解提供的信息来替代近似猜测搜索，它在单目标有约束问题中的表现优于其他元启发式算法。

    

    大多数元启发式算法依赖于一些被搜索的解来引导后续搜索，以在收敛过程中使用，其原因很简单：计算机的有限计算资源使得无法保留所有已搜索的解。这也揭示出大多数元启发式算法的每一次搜索都像是一个近似的猜测。为了解决这个问题，我们提出了一种新颖的元启发式算法：空间网优化（SNO）。它配备了一种称为空间网的新机制，从而使元启发式算法能够使用所有搜索解提供的大部分信息来描述解空间的景观。通过空间网，元启发式算法就像在解空间上拥有了“视觉”。模拟结果显示，在该研究中与一组著名的单目标有约束问题相比，SNO在大多数情况下都优于其他所有比较的元启发式算法。

    Most metaheuristic algorithms rely on a few searched solutions to guide later searches during the convergence process for a simple reason: the limited computing resource of a computer makes it impossible to retain all the searched solutions. This also reveals that each search of most metaheuristic algorithms is just like a ballpark guess. To help address this issue, we present a novel metaheuristic algorithm called space net optimization (SNO). It is equipped with a new mechanism called space net; thus, making it possible for a metaheuristic algorithm to use most information provided by all searched solutions to depict the landscape of the solution space. With the space net, a metaheuristic algorithm is kind of like having a ``vision'' on the solution space. Simulation results show that SNO outperforms all the other metaheuristic algorithms compared in this study for a set of well-known single objective bound constrained problems in most cases.
    
[^45]: 评估性能预测模型的普适性

    Assessing the Generalizability of a Performance Predictive Model. (arXiv:2306.00040v1 [cs.LG])

    [http://arxiv.org/abs/2306.00040](http://arxiv.org/abs/2306.00040)

    本研究提出一种工作流程来评估性能预测模型的泛化能力，可以在不同的基准测试套件上进行验证。

    

    自动算法选择和配置的关键组成部分是优秀的预测性能模型，而在大多数情况下，这是通过监督式机器学习方法实现的。该模型使用问题实例的特征表示作为输入数据，并预测其表现所使用的算法性能。然而，常见的机器学习模型很难对未被训练数据覆盖的特征表示进行准确的预测，导致对未见问题的泛化能力较差。在本研究中，我们提出了一种工作流程，用于评估在一个基准测试套件上训练的预测性能模型对另一个基准测试套件的普适性。我们通过在基准测试套件之间训练预测性能模型来测试工作流程，并发现景观特征空间中的普适性模式在性能空间中得到了体现。

    A key component of automated algorithm selection and configuration, which in most cases are performed using supervised machine learning (ML) methods is a good-performing predictive model. The predictive model uses the feature representation of a set of problem instances as input data and predicts the algorithm performance achieved on them. Common machine learning models struggle to make predictions for instances with feature representations not covered by the training data, resulting in poor generalization to unseen problems. In this study, we propose a workflow to estimate the generalizability of a predictive model for algorithm performance, trained on one benchmark suite to another. The workflow has been tested by training predictive models across benchmark suites and the results show that generalizability patterns in the landscape feature space are reflected in the performance space.
    
[^46]: 基于联邦学习的代码异味检测方法(FedCSD)

    FedCSD: A Federated Learning Based Approach for Code-Smell Detection. (arXiv:2306.00038v1 [cs.SE])

    [http://arxiv.org/abs/2306.00038](http://arxiv.org/abs/2306.00038)

    本文提出了一种名为FedCSD的基于联邦学习的代码异味检测方法，可以在保护数据隐私的同时，让组织协作训练联邦学习模型。该方法在三个实验中分别使用不同的数据集，实现了高精度的检测效果，并且比集中式和交叉验证方法具有更好的性能表现。

    

    本文提出了一种名为FedCSD的基于联邦学习的代码异味检测方法，可以在保护数据隐私的同时，让组织协作训练联邦学习模型。通过三个实验来支持这些断言，这些实验利用了三个手动验证的数据集，来检测和研究不同的代码异味场景。

    This paper proposes a Federated Learning Code Smell Detection (FedCSD) approach that allows organizations to collaboratively train federated ML models while preserving their data privacy. These assertions have been supported by three experiments that have significantly leveraged three manually validated datasets aimed at detecting and examining different code smell scenarios. In experiment 1, which was concerned with a centralized training experiment, dataset two achieved the lowest accuracy (92.30%) with fewer smells, while datasets one and three achieved the highest accuracy with a slight difference (98.90% and 99.5%, respectively). This was followed by experiment 2, which was concerned with cross-evaluation, where each ML model was trained using one dataset, which was then evaluated over the other two datasets. Results from this experiment show a significant drop in the model's accuracy (lowest accuracy: 63.80\%) where fewer smells exist in the training dataset, which has a noticeab
    
[^47]: 基于对称结构子群的机器人设计方法

    Symmetry-Aware Robot Design with Structured Subgroups. (arXiv:2306.00036v1 [cs.AI])

    [http://arxiv.org/abs/2306.00036](http://arxiv.org/abs/2306.00036)

    提出了一种利用对称性搜索的机器人设计方法，使用二面体群的子群表示对称性，并在结构化子群中搜索最优对称性，并在此对称性下完成机器人设计。理论证明了该方法能在保持设计空间的同时，设计出高效的对称机器人，实证评估也表明其具有卓越的效率和通用性。

    

    机器人的设计旨在学习如何创建易于控制且高效完成任务的机器人。以往的机器人设计研究已证明其能生成适用于各种任务的机器人。然而，这些研究直接从庞大的设计空间中搜索机器人，而忽略了共同的结构，导致机器人异常且性能不佳。为解决这个问题，我们提出了一种称为“基于对称结构子群的机器人设计”（SARD）框架，通过把对称性搜索引入机器人设计过程中，利用设计空间的结构信息。具体说，我们用二面体群的子群代表对称性，并在结构化子群中搜索最优对称性。然后，在搜索到的对称性下完成机器人设计。这样，SARD能够设计出高效的对称机器人，同时覆盖原始设计空间，理论上是可行的。我们进一步在各种任务中进行了实证评估，结果表明SARD具有卓越的效率和通用性。

    Robot design aims at learning to create robots that can be easily controlled and perform tasks efficiently. Previous works on robot design have proven its ability to generate robots for various tasks. However, these works searched the robots directly from the vast design space and ignored common structures, resulting in abnormal robots and poor performance. To tackle this problem, we propose a Symmetry-Aware Robot Design (SARD) framework that exploits the structure of the design space by incorporating symmetry searching into the robot design process. Specifically, we represent symmetries with the subgroups of the dihedral group and search for the optimal symmetry in structured subgroups. Then robots are designed under the searched symmetry. In this way, SARD can design efficient symmetric robots while covering the original design space, which is theoretically analyzed. We further empirically evaluate SARD on various tasks, and the results show its superior efficiency and generalizabili
    
[^48]: CodeTF：一站式Transformer库，实现最先进的代码LLM

    CodeTF: One-stop Transformer Library for State-of-the-art Code LLM. (arXiv:2306.00029v1 [cs.SE])

    [http://arxiv.org/abs/2306.00029](http://arxiv.org/abs/2306.00029)

    CodeTF是一个开源的Transformer库，提供了包括预训练的Code LLM模型和标准化接口等一系列功能，可以轻松地将最先进的Code LLM模型应用于各种软件工程任务中。

    

    代码智能在转型现代软件工程中扮演着重要角色。近年来，基于深度学习的模型，尤其是利用大量开源代码和编程语言特征的Transformer-based大型语言模型（LLMs），已经展示出了对这些任务的显著潜力。然而，这些模型的开发和部署通常需要对机器学习和软件工程的专业知识，从而为模型应用带来了一定的障碍。本文提出了CodeTF，一个基于Transformer的开放源代码库，用于实现最先进的Code LLM和代码智能。我们采用模块化设计和可扩展框架的原则，设计CodeTF并提供统一接口，以便快速访问和开发不同类型的模型、数据集和任务。我们的库支持预训练的Code LLM模型和流行的代码基准测试，包括标准化接口以有效地训练和服务代码LLMs，并支持双GPU训练和推理。使用CodeTF，用户可以轻松将最先进的Code LLM模型应用于各种软件工程任务中，减少训练工作量。

    Code intelligence plays a key role in transforming modern software engineering. Recently, deep learning-based models, especially Transformer-based large language models (LLMs), have demonstrated remarkable potential in tackling these tasks by leveraging massive open-source code data and programming language features. However, the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption. In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence. Following the principles of modular design and extensible framework, we design CodeTF with a unified interface to enable rapid access and development across different types of models, datasets and tasks. Our library supports a collection of pretrained Code LLM models and popular code benchmarks, including a standardized interface to train and serve code LLMs efficiently, and d
    
[^49]: 在领英平台上实现AI公平性的分离与操作化

    Disentangling and Operationalizing AI Fairness at LinkedIn. (arXiv:2306.00025v1 [cs.SI])

    [http://arxiv.org/abs/2306.00025](http://arxiv.org/abs/2306.00025)

    领英使用公平性框架来实现AI公平性，该框架将AI的公平性分离出公平待遇和公平产品期望，并提供了操作指南来实现平等AI待遇并配合产品平等策略。

    

    针对领英平台上实现规模化的AI公平性不仅具有多个相互冲突的公平定义，而且确定公平性还取决于AI实现的产品的具体细节和背景，这对于AI从业人员的公平性期望值也需要明确。在本文中，我们介绍了领英正在使用的AI公平性框架来应对这三个挑战。该框架通过区分平等对待和公平产品期望，从而分离了AI的公平性。该框架提供了明确的指导方针来操作化实现平等的AI待遇，配合产品平等的策略，而不是在这两种常见的公平性解释之间强制进行权衡。本文重点介绍了领英AI公平性框架中的平等AI待遇组成部分，并分享了支持这些原则的实例。

    Operationalizing AI fairness at LinkedIn's scale is challenging not only because there are multiple mutually incompatible definitions of fairness but also because determining what is fair depends on the specifics and context of the product where AI is deployed. Moreover, AI practitioners need clarity on what fairness expectations need to be addressed at the AI level. In this paper, we present the evolving AI fairness framework used at LinkedIn to address these three challenges. The framework disentangles AI fairness by separating out equal treatment and equitable product expectations. Rather than imposing a trade-off between these two commonly opposing interpretations of fairness, the framework provides clear guidelines for operationalizing equal AI treatment complemented with a product equity strategy. This paper focuses on the equal AI treatment component of LinkedIn's AI fairness framework, shares the principles that support it, and illustrates their application through a case study
    
[^50]: 使用机器学习算法预测心脏病并减少调查时间

    Predicting Heart Disease and Reducing Survey Time Using Machine Learning Algorithms. (arXiv:2306.00023v1 [cs.LG])

    [http://arxiv.org/abs/2306.00023](http://arxiv.org/abs/2306.00023)

    本研究使用机器学习算法对美国CDC的心脏病调查进行了准确性调查，确定了最相关的问题子集，并证明使用有限的问题集仍然可以保持高的预测准确率，并显著减少调查时间。

    

    目前，许多研究人员和分析师正致力于改善各种疾病的医学诊断。心脏病是一种常见疾病，可视为全球重要死因。早期发现心脏病有助于显著降低心力衰竭的风险。因此，疾病控制和预防中心每年从400,000多名参与者中进行与健康相关的电话调查。然而，人们对于预测心脏病的数据可靠性以及是否所有调查问题都具有强相关性提出了一些担忧。本研究旨在利用支持向量机和逻辑回归等多种机器学习技术，以调查美国疾病控制和预防中心的心脏病调查准确性。此外，我们使用各种特征选择方法来确定可以用于预测心脏病的最相关的问题子集。为了得出有力的结论，我们在全国健康和营养调查数据集上测试了开发的模型。结果表明，使用有限的问题集可以显著缩短调查时间，同时仍然保持高的心脏病预测准确率。

    Currently, many researchers and analysts are working toward medical diagnosis enhancement for various diseases. Heart disease is one of the common diseases that can be considered a significant cause of mortality worldwide. Early detection of heart disease significantly helps in reducing the risk of heart failure. Consequently, the Centers for Disease Control and Prevention (CDC) conducts a health-related telephone survey yearly from over 400,000 participants. However, several concerns arise regarding the reliability of the data in predicting heart disease and whether all of the survey questions are strongly related. This study aims to utilize several machine learning techniques, such as support vector machines and logistic regression, to investigate the accuracy of the CDC's heart disease survey in the United States. Furthermore, we use various feature selection methods to identify the most relevant subset of questions that can be utilized to forecast heart conditions. To reach a robus
    
[^51]: 采用模型无关方法解释仇恨言论分类

    Explaining Hate Speech Classification with Model Agnostic Methods. (arXiv:2306.00021v1 [cs.CL])

    [http://arxiv.org/abs/2306.00021](http://arxiv.org/abs/2306.00021)

    本文旨在解释仇恨言论分类，采用双向变形器模型BERT进行预测，并提供后验的模型无关和代理可解释性方法，以支持模型的解释性并防止模型偏差。

    

    机器学习和人工智能在自然语言处理和深度学习领域取得了显著突破。随着社交媒体的广泛使用，对话中的仇恨言论检测也在自然语言处理研究人员中变得越来越受欢迎。然而，正如最近的趋势所表明的那样，在AI模型中加入可解释性和可解释性维度的需求已经得到深刻的认识。本文的研究目标是要在仇恨言论预测与系统生成的解释之间搭建桥梁。通过首先预测文本的分类，然后提供一个后验的模型无关和代理可解释性方法来支持模型的解释性并防止模型偏差。采用双向变形器模型BERT进行预测，因为它在自然语言处理的机器学习模型中具有最先进的效率。

    There have been remarkable breakthroughs in Machine Learning and Artificial Intelligence, notably in the areas of Natural Language Processing and Deep Learning. Additionally, hate speech detection in dialogues has been gaining popularity among Natural Language Processing researchers with the increased use of social media. However, as evidenced by the recent trends, the need for the dimensions of explainability and interpretability in AI models has been deeply realised. Taking note of the factors above, the research goal of this paper is to bridge the gap between hate speech prediction and the explanations generated by the system to support its decision. This has been achieved by first predicting the classification of a text and then providing a posthoc, model agnostic and surrogate interpretability approach for explainability and to prevent model bias. The bidirectional transformer model BERT has been used for prediction because of its state of the art efficiency over other Machine Lea
    
[^52]: GPT4GEO：一个语言模型如何看待世界地理

    GPT4GEO: How a Language Model Sees the World's Geography. (arXiv:2306.00020v1 [cs.CL])

    [http://arxiv.org/abs/2306.00020](http://arxiv.org/abs/2306.00020)

    研究调查了GPT-4的地理知识水平，并探讨其在地理数据分析等领域的应用潜力。

    

    大型语言模型（LLM）在涉及问题回答、生成连贯文本和代码的广泛任务中展现出了卓越的能力。全面理解LLM的优点和缺点对于安全、下游应用和性能改进都有益处。在本研究中，我们调查了GPT-4获得事实地理知识的程度，并能否将这些知识用于解释性推理，这对于涉及地理数据的应用（如地理空间分析、供应链管理和灾难响应）尤其重要。为此，我们设计并进行了一系列多样化的实验，从定位、距离和高度估计等事实任务开始，到生成国家轮廓和旅游网络、在约束条件下寻找路线和供应链分析等更复杂的问题。我们提供了GPT-4（没有插件或Internet访问）了解和不了解世界地理的广泛描述，并讨论了在地理空间领域应用的影响。

    Large language models (LLMs) have shown remarkable capabilities across a broad range of tasks involving question answering and the generation of coherent text and code. Comprehensively understanding the strengths and weaknesses of LLMs is beneficial for safety, downstream applications and improving performance. In this work, we investigate the degree to which GPT-4 has acquired factual geographic knowledge and is capable of using this knowledge for interpretative reasoning, which is especially important for applications that involve geographic data, such as geospatial analysis, supply chain management, and disaster response. To this end, we design and conduct a series of diverse experiments, starting from factual tasks such as location, distance and elevation estimation to more complex questions such as generating country outlines and travel networks, route finding under constraints and supply chain analysis. We provide a broad characterisation of what GPT-4 (without plugins or Interne
    
[^53]: 向可解释的、语言无关的LLMs迈进：大规模语言符号逆向工程

    Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale. (arXiv:2306.00017v1 [cs.CL])

    [http://arxiv.org/abs/2306.00017](http://arxiv.org/abs/2306.00017)

    本文提出结合符号表示和自下而上的逆向工程的方法，解决大规模语言模型在真正语言理解上的局限性，实现可解释的、语言无关的LLMs。

    

    大型语言模型（LLMs）取得了一个里程碑，无可否认地改变了人工智能（AI）中许多信仰。然而，当涉及真正的语言理解时，这些LLM的许多限制仍然存在，这些限制是深度神经网络底层架构的副产品。此外，由于它们的亚符号性质，这些模型获得有关语言如何运作的任何知识都将被埋在数十亿个微特征（权重）中，其中没有一个单独的特征有意义，使得这些模型无法解释。为了解决这些限制，我们建议将符号表示的强度与我们认为是LLMs成功的关键结合起来，即在规模上成功地进行自下而上的语言逆向工程。因此，我们主张在符号设置下对语言进行自下而上的逆向工程。一些作者提出了这个项目的提示，我们将进行详细讨论。

    Large language models (LLMs) have achieved a milestone that undenia-bly changed many held beliefs in artificial intelligence (AI). However, there remains many limitations of these LLMs when it comes to true language understanding, limitations that are a byproduct of the under-lying architecture of deep neural networks. Moreover, and due to their subsymbolic nature, whatever knowledge these models acquire about how language works will always be buried in billions of microfeatures (weights), none of which is meaningful on its own, making such models hopelessly unexplainable. To address these limitations, we suggest com-bining the strength of symbolic representations with what we believe to be the key to the success of LLMs, namely a successful bottom-up re-verse engineering of language at scale. As such we argue for a bottom-up reverse engineering of language in a symbolic setting. Hints on what this project amounts to have been suggested by several authors, and we discuss in some detail
    
[^54]: 将领域知识纳入深度神经网络的离散选择模型中

    Incorporating Domain Knowledge in Deep Neural Networks for Discrete Choice Models. (arXiv:2306.00016v1 [cs.LG])

    [http://arxiv.org/abs/2306.00016](http://arxiv.org/abs/2306.00016)

    本文提出了一种框架，将领域知识和先验信念纳入离散选择模型（DCM）的深度神经网络（DNN）中，以扩展数据驱动方法在DCM中的潜力。所提出的方法既具有可解释性，又保留了DCM的优点和DNNs的灵活性，并优于现有的最先进方法。

    

    离散选择模型（DCM）被广泛应用于旅行需求分析中，作为一种强大的理论计量框架来理解和预测选择行为。DCM是随机效用模型（RUM），其主要优点是可解释性。然而，估计这些模型的核心要求是先验指定相关的效用函数，使其对模型人员的主观信念敏感。最近，机器学习（ML）方法已经成为学习DCM中未观测到的非线性关系的一种有前途的方法。然而，ML模型被认为是“黑盒子”，可能与预期的关系不符。本文提出了一个框架，通过约束支持开发并将领域知识和先验信念纳入可解释模型中，以扩展数据驱动方法在DCM中的潜力。所提出的框架包括代表所需关系的伪数据样本和将DCM转换为具有软约束的深度神经网络（DNN）的对数似然函数。由此得到的模型既具有可解释性，又保留了RUM的优点和DNNs的灵活性。实验结果表明，所提出的方法优于现有的最先进方法。

    Discrete choice models (DCM) are widely employed in travel demand analysis as a powerful theoretical econometric framework for understanding and predicting choice behaviors. DCMs are formed as random utility models (RUM), with their key advantage of interpretability. However, a core requirement for the estimation of these models is a priori specification of the associated utility functions, making them sensitive to modelers' subjective beliefs. Recently, machine learning (ML) approaches have emerged as a promising avenue for learning unobserved non-linear relationships in DCMs. However, ML models are considered "black box" and may not correspond with expected relationships. This paper proposes a framework that expands the potential of data-driven approaches for DCM by supporting the development of interpretable models that incorporate domain knowledge and prior beliefs through constraints. The proposed framework includes pseudo data samples that represent required relationships and a l
    
[^55]: GraphCleaner: 在流行的图学习基准数据集中检测错误标注的样本

    GraphCleaner: Detecting Mislabelled Samples in Popular Graph Learning Benchmarks. (arXiv:2306.00015v1 [cs.LG])

    [http://arxiv.org/abs/2306.00015](http://arxiv.org/abs/2306.00015)

    该论文提出了一种后期方法GraphCleaner，用于在流行的图学习基准数据集中检测和纠正错误标注节点。GraphCleaner 组合了合成错误标注数据集的生成和邻域感知错误标注检测两种新颖思想，实证评估结果表明其平均F1得分提高了0.14。

    

    研究发现，流行的文本，图像和音频数据集中存在标签错误，这严重影响了机器学习算法的开发和评估的可靠性。尽管针对通用数据类型（如图像和文本）的质量改进工作有所增加，但在图数据中检测错误标注的问题仍然未被充分探讨。该论文旨在探索流行的实际图数据集中的错误标注问题，并提出了GraphCleaner，一种后期方法，用于检测和纠正这些图数据集中的错误标注节点。GraphCleaner结合了两种新颖的思想：1）合成错误标注数据集的生成，旨在生成逼真的错误标注；2）邻域感知错误标注检测，其中利用了标签和基分类器预测中的邻域依赖性。在6个基准数据集和6个实验设置上的实证评估表明，GraphCleaner优于最接近的基准线，并具有平均F1得分改进0.14。

    Label errors have been found to be prevalent in popular text, vision, and audio datasets, which heavily influence the safe development and evaluation of machine learning algorithms. Despite increasing efforts towards improving the quality of generic data types, such as images and texts, the problem of mislabel detection in graph data remains underexplored. To bridge the gap, we explore mislabelling issues in popular real-world graph datasets and propose GraphCleaner, a post-hoc method to detect and correct these mislabelled nodes in graph datasets. GraphCleaner combines the novel ideas of 1) Synthetic Mislabel Dataset Generation, which seeks to generate realistic mislabels; and 2) Neighborhood-Aware Mislabel Detection, where neighborhood dependency is exploited in both labels and base classifier predictions. Empirical evaluations on 6 datasets and 6 experimental settings demonstrate that GraphCleaner outperforms the closest baseline, with an average improvement of 0.14 in F1 score, and
    
[^56]: 基于图神经网络的时空数据方法及应用：综述

    Graph Neural Network for spatiotemporal data: methods and applications. (arXiv:2306.00012v1 [cs.LG])

    [http://arxiv.org/abs/2306.00012](http://arxiv.org/abs/2306.00012)

    本文综述了基于图神经网络的时空数据方法及应用，并提供了一个GNN的分类及其应用领域。这个综述突出了使用GNN分析时空数据的强大工具及其局限性，并指出未来研究方向和目前面临的挑战。

    

    在大数据时代，蕴含着空间和时间信息的数据的可用性急剧增加，为天气预报、自然灾害管理、智能交通系统和精准农业等应用提供了宝贵的洞见。图神经网络是一种用于对具有依赖关系的数据（如空间和时间依赖关系）进行建模和理解的强大工具。已有大量工作致力于利用GNN解决时空数据中的复杂空间和时间依赖关系。然而，时空数据的强跨学科性质导致众多特定于不同应用领域的GNN变体的出现。尽管这些技术通常适用于各个领域，但由于缺乏关于时空数据GNN的全面文献综述，因此跨领域参考这些方法仍然是必要的，但也带来了挑战。本文综述了现有关于基于图神经网络的时空数据研究，并对其优点、局限性和潜在应用进行了讨论。我们也提供了未来研究的路线图和强调了一些未解决的挑战。

    In the era of big data, there has been a surge in the availability of data containing rich spatial and temporal information, offering valuable insights into dynamic systems and processes for applications such as weather forecasting, natural disaster management, intelligent transport systems, and precision agriculture. Graph neural networks (GNNs) have emerged as a powerful tool for modeling and understanding data with dependencies to each other such as spatial and temporal dependencies. There is a large amount of existing work that focuses on addressing the complex spatial and temporal dependencies in spatiotemporal data using GNNs. However, the strong interdisciplinary nature of spatiotemporal data has created numerous GNNs variants specifically designed for distinct application domains. Although the techniques are generally applicable across various domains, cross-referencing these methods remains essential yet challenging due to the absence of a comprehensive literature review on GN
    
[^57]: 简单形式映射神经网络中的可解释性

    Explainability in Simplicial Map Neural Networks. (arXiv:2306.00010v1 [cs.LG])

    [http://arxiv.org/abs/2306.00010](http://arxiv.org/abs/2306.00010)

    本文提出了简单形式映射神经网络（SMNN）的训练过程和替代凸多面体的方法，并且首次引入了 SMNN 的可解释性能力。

    

    简单形式映射神经网络（SMNN）是基于拓扑学的神经网络，具有普适逼近能力和在适当条件下对抗性示例的鲁棒性。然而，在高维中应用 SMNN 存在一些瓶颈，首先没有定义 SMNN 的训练过程，其次对于输入数据集需要构建一个包围凸多面体。本文提出了基于给定数据集的支持子集和投影到超球面的方法作为替代凸多面体的 SMNN 训练过程，并首次引入了 SMNN 的可解释性能力。

    Simplicial map neural networks (SMNNs) are topology-based neural networks with interesting properties such as universal approximation capability and robustness to adversarial examples under appropriate conditions. However, SMNNs present some bottlenecks for their possible application in high dimensions. First, no SMNN training process has been defined so far. Second, SMNNs require the construction of a convex polytope surrounding the input dataset. In this paper, we propose a SMNN training procedure based on a support subset of the given dataset and a method based on projection to a hypersphere as a replacement for the convex polytope construction. In addition, the explainability capacity of SMNNs is also introduced for the first time in this paper.
    
[^58]: 截断亲和力最大化：用于图形异常监测的单类同型建模

    Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v1 [cs.SI])

    [http://arxiv.org/abs/2306.00006](http://arxiv.org/abs/2306.00006)

    本文针对图形异常监测数据集中存在的一类同型现象，提出了一种新的无监督异常评分度量——当前节点亲和力，并通过学习量身定制的节点表示，实现了截断亲和力最大化（TAM）方法，优化在原始图形结构上进行，能够有效进行双重One-Class的GAD。

    

    我们在现实世界的图形异常监测（GAD）数据集中经常发现一种普遍的属性......本文提出了一种新的无监督异常评分度量 - 当前节点亲和力......我们进一步提出了截断亲和力最大化 (TAM)，该方法通过最大化与_neighbors的本地亲和力来学习量身定制的节点表示。本文所提方法在原始图形结构上进行优化，可以进行双重One-Class的GAD。

    One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be bi
    
[^59]: 通过监督式注意力多实例学习从多视角超声图像检测心脏病

    Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning. (arXiv:2306.00003v1 [eess.IV])

    [http://arxiv.org/abs/2306.00003](http://arxiv.org/abs/2306.00003)

    本研究提出了一种基于监督式注意力多实例学习的方法，可以自动分析超声图像，实现AS的精确筛查且精度优于传统机器学习和深度学习方法。

    

    主动脉瓣狭窄(AS)是一种导致严重发病率和死亡率的退行性瓣膜疾病。这种情况经常被低估和低治疗。在临床实践中，AS是通过超声心动图的专家审查来诊断的，这会产生数十个下肺采样的超声图像。只有一些视图显示主动脉瓣。为了自动化筛查AS，深度网络必须学习模仿人类专家识别主动脉瓣视图的能力，然后汇总这些相关图像以产生研究级诊断。我们发现先前的AS检测方法由于依赖于跨图像的不灵活平均值而导致精度不足。我们进一步发现，现成的基于注意力的多实例(MIL)学习表现不佳。我们提出了一种新的端到端MIL方法，包含两个关键方法创新。首先，通过监督式注意技术，引导学习的注意机制偏爱相关视图。其次，一种新颖的自我监督学习技术提高了每个单独图像的表现。我们的方法在一个真实的临床数据集（4569名患者）上实现了最先进的性能，在传统的机器学习和深度学习方法之上。

    Aortic stenosis (AS) is a degenerative valve condition that causes substantial morbidity and mortality. This condition is under-diagnosed and under-treated. In clinical practice, AS is diagnosed with expert review of transthoracic echocardiography, which produces dozens of ultrasound images of the heart. Only some of these views show the aortic valve. To automate screening for AS, deep networks must learn to mimic a human expert's ability to identify views of the aortic valve then aggregate across these relevant images to produce a study-level diagnosis. We find previous approaches to AS detection yield insufficient accuracy due to relying on inflexible averages across images. We further find that off-the-shelf attention-based multiple instance learning (MIL) performs poorly. We contribute a new end-to-end MIL approach with two key methodological innovations. First, a supervised attention technique guides the learned attention mechanism to favor relevant views. Second, a novel self-sup
    
[^60]: 以决策为导向的人机对话

    Decision-Oriented Dialogue for Human-AI Collaboration. (arXiv:2305.20076v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.20076](http://arxiv.org/abs/2305.20076)

    该论文探讨了一类以决策为导向的人机对话任务，以及在会议论文审稿人分配、城市多步行程规划和旅行计划协商等场景中，人工智能助手和用户不同的能力如何结合以达到最佳决策。论文通过构建对话环境并进行人机对话收集数据，发现当前人工智能助手在此类任务中的局限性。

    

    我们描述了一类任务，称为以决策为导向的对话，其中人工智能助手必须通过自然语言与一名或多名人类合作，帮助他们做出复杂的决策。我们在三个领域中形式化用户面临日常决策的过程：（1）选择会议论文的审稿人分配，（2）在城市中规划多步行程，以及（3）为一群朋友协商旅行计划。在每个设置中，AI助手和用户具有不同的能力，他们必须结合起来得出最佳决策：助手可以访问和处理大量信息，而用户具有系统外的偏好和限制。对于每个任务，我们构建了一个对话环境，其中代理根据他们达到的最终决策的质量获得奖励。使用这些环境，我们与人们扮演助手的人进行了人机对话。为了比较当前人工智能助手在这些设置中的交流方式，我们提出了基线模型，并将其与人类-人类对话进行比较。我们的结果展示了决策导向对话所面临的挑战，并凸显了当前人工智能助手的局限性。

    We describe a class of tasks called decision-oriented dialogues, in which AI assistants must collaborate with one or more humans via natural language to help them make complex decisions. We formalize three domains in which users face everyday decisions: (1) choosing an assignment of reviewers to conference papers, (2) planning a multi-step itinerary in a city, and (3) negotiating travel plans for a group of friends. In each of these settings, AI assistants and users have disparate abilities that they must combine to arrive at the best decision: assistants can access and process large amounts of information, while users have preferences and constraints external to the system. For each task, we build a dialogue environment where agents receive a reward based on the quality of the final decision they reach. Using these environments, we collect human-human dialogues with humans playing the role of assistant. To compare how current AI assistants communicate in these settings, we present bas
    
[^61]: 束搜索递归单元：一种支持反向传播的递归神经网络框架

    Beam Tree Recursive Cells. (arXiv:2305.19999v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19999](http://arxiv.org/abs/2305.19999)

    本论文提出了一种支持反向传播的递归神经网络框架——束搜索递归单元（BT-Cell），用于扩展递归神经网络，实现对潜在结构的感知；此外，我们提出了一种放松束搜索中硬前k算子的方法，以实现更好的梯度信号传递。在评估中发现，BT-Cell在合成和实际数据的多个具有结构敏感性的任务中表现优异。

    

    本文提出了一种叫做束搜索递归单元（BT-Cell）的框架，用于扩展支持使用束搜索进行潜在结构感知的递归神经网络（RvNN）。我们进一步通过提出在束搜索中对硬性前k算子的放松来扩展此框架，以更好地传递梯度信号。我们在合成和实际数据的不同代表性分布上评估了我们的模型。实验结果表明，BT-Cell在多个具有挑战性的体现结构敏感性的任务（如ListOps和逻辑推理）上达到了几乎完美的性能，同时在实际数据上与其他基于RvNN的模型具有可比性的性能。此外，我们在ListOps中确定了神经模型在推广到未见过的参数数量上的未知失效案例。代码可在https://github.com/JRC1995/BeamTreeRecursiveCells上获得。

    We propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendly framework to extend Recursive Neural Networks (RvNNs) with beam search for latent structure induction. We further extend this framework by proposing a relaxation of the hard top-k operators in beam search for better propagation of gradient signals. We evaluate our proposed models in different out-of-distribution splits in both synthetic and realistic data. Our experiments show that BTCell achieves near-perfect performance on several challenging structure-sensitive synthetic tasks like ListOps and logical inference while maintaining comparable performance in realistic data against other RvNN-based models. Additionally, we identify a previously unknown failure case for neural models in generalization to unseen number of arguments in ListOps. The code is available at: https://github.com/JRC1995/BeamTreeRecursiveCells.
    
[^62]: InGram：通过关系图进行归纳知识图谱嵌入

    InGram: Inductive Knowledge Graph Embedding via Relation Graphs. (arXiv:2305.19987v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19987](http://arxiv.org/abs/2305.19987)

    InGram是一种新的归纳式知识图谱补全方法，可以在推理时生成新关系和实体的嵌入，并使用注意力机制汇总邻居嵌入生成关系和实体嵌入。该方法在多个基准数据集上的性能优于现有的基准方法。

    

    归纳知识图谱补全被视为预测训练期间未观察到的新实体之间的缺失三元组的任务。该论文提出了一个新方法InGram，它可以在推理时生成新关系和实体的嵌入，并基于关系图和原始知识图谱使用注意力机制来汇总邻居嵌入以生成关系和实体嵌入。实验结果表明，在几个基准数据集上，InGram的性能优于现有的基准方法。

    Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to appear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time. Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, InGram learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism. Experimental results show that InGram outper
    
[^63]: 基于大语言模型的推荐系统综述

    A Survey on Large Language Models for Recommendation. (arXiv:2305.19860v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2305.19860](http://arxiv.org/abs/2305.19860)

    本综述介绍了基于大语言模型的推荐系统，提出了判别式LLMs和生成式LLMs两种模型范式，总结了这些模型的最新进展，强调了该领域的挑战和研究方向。

    

    大语言模型（LLMs）已成为自然语言处理（NLP）领域强大的工具，并在推荐系统领域引起了重视。这些模型使用自监督学习在海量数据上进行训练，已在学习通用表示方面取得了显着成功，并有可能通过一些有效的转移技术（如微调和提示调整）等手段提高推荐系统的各个方面的性能。利用大语言模型增强推荐质量的关键是利用它们高质量的文本特征表示和大量的外部知识覆盖，建立项目和用户之间的相关性。为了全面了解现有基于LLM的推荐系统，本综述提出了一种分类法，将这些模型分为两种主要范式，分别是判别式LLMs和生成式LLMs。此外，我们总结了这些范式的最新进展，并强调了这个新兴领域的挑战和开放性研究问题。

    Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discrimi
    
[^64]: 细粒度语义奖励增强文本到图像扩散模型

    Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards. (arXiv:2305.19599v1 [cs.CV])

    [http://arxiv.org/abs/2305.19599](http://arxiv.org/abs/2305.19599)

    本文提出了FineRewards，通过引入细粒度的语义奖励，即标题奖励和SAM奖励，来改进文本到图像扩散模型中文本和图像之间的对齐。

    

    最近，文本到图像扩散模型的研究取得了显著的成功，在给定的文本提示下生成了高质量、逼真的图像。然而，由于缺乏细粒度语义指导，以成功诊断形态差异为止，以前的方法无法执行文本概念和生成的图像之间的准确形态对齐。在本文中，我们提出了FineRewards，通过引入两种新的细粒度语义奖励--标题奖励和语义分割任何事物（SAM）奖励，来改进文本到图像扩散模型中文本和图像之间的对齐。

    Recent advances in text-to-image diffusion models have achieved remarkable success in generating high-quality, realistic images from given text prompts. However, previous methods fail to perform accurate modality alignment between text concepts and generated images due to the lack of fine-level semantic guidance that successfully diagnoses the modality discrepancy. In this paper, we propose FineRewards to improve the alignment between text and images in text-to-image diffusion models by introducing two new fine-grained semantic rewards: the caption reward and the Semantic Segment Anything (SAM) reward. From the global semantic view, the caption reward generates a corresponding detailed caption that depicts all important contents in the synthetic image via a BLIP-2 model and then calculates the reward score by measuring the similarity between the generated caption and the given prompt. From the local semantic view, the SAM reward segments the generated images into local parts with categ
    
[^65]: 带有内部分布在线适应的离线元强化学习

    Offline Meta Reinforcement Learning with In-Distribution Online Adaptation. (arXiv:2305.19529v1 [cs.LG])

    [http://arxiv.org/abs/2305.19529](http://arxiv.org/abs/2305.19529)

    本文提出了一种带有不确定性量化的内部分布在线适应(IDAQ)的框架，利用策略后验集合和信念更新网络量化策略不确定性并生成上下文信息来处理新任务，在离线元强化学习上具有竞争性表现。

    

    近期的离线元强化学习方法通常利用任务相关的行为策略(例如，对每个个体任务进行RL智能体的训练)来收集多任务数据集。然而，这些方法总是需要额外的信息进行快速调整，例如测试任务的离线上下文。为了解决这个问题，我们首先正式地表征了离线元强化学习中的一个独特挑战：离线数据集和在线适应之间的转换-奖励分布偏移。我们的理论发现，来自分布之外的适应情况可能会导致不可靠的策略评估，并且使用分布内的情况进行在线适应可以确保适应性能保证。基于这些理论洞察，我们提出了一种新的适应框架，称为带有不确定性量化的内部分布在线适应(IDAQ)，它利用策略后验集合和信念更新网络量化策略不确定性并生成上下文信息来处理新任务。 实验结果表明，IDAQ的效果优于现有的离线元强化学习方法，并且达到了最先进的在线元强化学习方法的竞争性表现。

    Recent offline meta-reinforcement learning (meta-RL) methods typically utilize task-dependent behavior policies (e.g., training RL agents on each individual task) to collect a multi-task dataset. However, these methods always require extra information for fast adaptation, such as offline context for testing tasks. To address this problem, we first formally characterize a unique challenge in offline meta-RL: transition-reward distribution shift between offline datasets and online adaptation. Our theory finds that out-of-distribution adaptation episodes may lead to unreliable policy evaluation and that online adaptation with in-distribution episodes can ensure adaptation performance guarantee. Based on these theoretical insights, we propose a novel adaptation framework, called In-Distribution online Adaptation with uncertainty Quantification (IDAQ), which generates in-distribution context using a given uncertainty quantification and performs effective task belief inference to address new
    
[^66]: 基于$K^2$-树的分级图生成

    Hierarchical Graph Generation with $K^2$-trees. (arXiv:2305.19125v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19125](http://arxiv.org/abs/2305.19125)

    本文介绍了一种基于$K^2$-树的图生成方法，该方法可以实现紧凑生成，并同时捕获图的内在分层结构。通过提出顺序$K^2$-树表示和引入基于Transformer的架构，本文进一步改进了这种方法。实验表明，该方法在图生成方面具有卓越的表现。

    

    从目标分布生成图是许多领域的一个重大挑战，包括药物发现和社交网络分析。在本文中，我们介绍了一种利用原本设计用于无损图压缩的$K^2$-树表示的新颖图生成方法。我们的动机源于$K^2$-树能够在进行紧凑生成的同时，捕获图的内在分层结构的能力。此外，我们还通过(1)提出了一种包含剪枝、扁平化和记号化过程的顺序K2树表示和(2)引入了一种基于Transformer的架构，旨在通过结合专业树形位置编码方案来生成序列。最后，我们对四个常规和两个分子图数据集进行了广泛的评估，以证实我们的算法在图生成方面的优越性。

    Generating graphs from a target distribution is a significant challenge across many domains, including drug discovery and social network analysis. In this work, we introduce a novel graph generation method leveraging $K^2$-tree representation which was originally designed for lossless graph compression. Our motivation stems from the ability of the $K^2$-trees to enable compact generation while concurrently capturing the inherent hierarchical structure of a graph. In addition, we make further contributions by (1) presenting a sequential $K^2$-tree representation that incorporates pruning, flattening, and tokenization processes and (2) introducing a Transformer-based architecture designed to generate the sequence by incorporating a specialized tree positional encoding scheme. Finally, we extensively evaluate our algorithm on four general and two molecular graph datasets to confirm its superiority for graph generation.
    
[^67]: 向单一综合防欺骗说话人验证嵌入迈进

    Towards single integrated spoofing-aware speaker verification embeddings. (arXiv:2305.19051v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2305.19051](http://arxiv.org/abs/2305.19051)

    本研究旨在开发单一综合防欺骗说话人验证嵌入，在欺骗和非目标说话人输入方面达到卓越性能，相较于现有的嵌入方案，该嵌入通过多阶段训练和复制合成技术来提高性能，并在 SASV2022 挑战协议中取得了1.06%的SASV-EER效果。

    

    本研究旨在开发单一综合防欺骗说话人验证嵌入，满足以下两个方面的要求：首先，需要拒绝非目标说话人的输入以及目标说话人的欺骗输入。其次，需要证明其与自动说话人验证 (ASV) 和反欺骗嵌入 (CM) 融合的性能相当，而后者在 SASV2022 挑战中表现远好于单一嵌入的解决方案。我们分析认为单一 SASV 嵌入的劣质性能来自于训练数据量不足和 ASV 与 CM 任务的不同性质。为此，我们提出了一种新的框架，包括多阶段训练和损失函数的组合。同时利用 Copy synthesis 和几个声码器解决欺骗数据不足的问题。实验结果显示出显著的改进，评估 SASV2022 挑战协议时，达到了 SASV-EER 1.06%。

    This study aims to develop a single integrated spoofing-aware speaker verification (SASV) embeddings that satisfy two aspects. First, rejecting non-target speakers' input as well as target speakers' spoofed inputs should be addressed. Second, competitive performance should be demonstrated compared to the fusion of automatic speaker verification (ASV) and countermeasure (CM) embeddings, which outperformed single embedding solutions by a large margin in the SASV2022 challenge. We analyze that the inferior performance of single SASV embeddings comes from insufficient amount of training data and distinct nature of ASV and CM tasks. To this end, we propose a novel framework that includes multi-stage training and a combination of loss functions. Copy synthesis, combined with several vocoders, is also exploited to address the lack of spoofed data. Experimental results show dramatic improvements, achieving a SASV-EER of 1.06% on the evaluation protocol of the SASV2022 challenge.
    
[^68]: 标准比评分更重要：面向多准则推荐的标准偏好感知轻量图卷积网络

    Criteria Tell You More than Ratings: Criteria Preference-Aware Light Graph Convolution for Effective Multi-Criteria Recommendation. (arXiv:2305.18885v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2305.18885](http://arxiv.org/abs/2305.18885)

    本文提出了一种面向多准则推荐的标准偏好感知轻量图卷积网络，该方法结合了MC扩展图，可以准确地捕捉用户的标准偏好，并进一步将用户对各个标准的偏好合并到最终的推荐列表中。

    

    多准则推荐系统现在在广泛的电子商务领域中利用多准则 (MC) 评分信息，而深度学习中的图神经网络 (GNN) 已经被广泛应用于各种推荐系统的开发中。在这种情况下，本文首次尝试使用GNN辅助设计MC推荐系统。具体而言，我们提出了一种新颖的标准偏好感知轻量图卷积方法(CPA-LGC),可以准确捕捉用户的标准偏好以及复杂高阶连接中的协作信号。本文在MC扩展图上构建了一个能够将用户-物品MC评分转换为扩展二分图的MC扩展图，再进一步将标准重要性编码到图卷积过程中，并引入了一种新的标准偏好感知聚合方法来将用户对不同标准的偏好合并到最终的推荐列表中。

    The multi-criteria (MC) recommender system, which leverages MC rating information in a wide range of e-commerce areas, is ubiquitous nowadays. Surprisingly, although graph neural networks (GNNs) have been widely applied to develop various recommender systems due to GNN's high expressive capability in learning graph representations, it has been still unexplored how to design MC recommender systems with GNNs. In light of this, we make the first attempt towards designing a GNN-aided MC recommender system. Specifically, rather than straightforwardly adopting existing GNN-based recommendation methods, we devise a novel criteria preference-aware light graph convolution CPA-LGC method, which is capable of precisely capturing the criteria preference of users as well as the collaborative signal in complex high-order connectivities. To this end, we first construct an MC expansion graph that transforms user--item MC ratings into an expanded bipartite graph to potentially learn from the collaborat
    
[^69]: 任务等变图Few-shot学习

    Task-Equivariant Graph Few-shot Learning. (arXiv:2305.18758v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18758](http://arxiv.org/abs/2305.18758)

    本文提出了一种任务等变图Few-shot学习（TEG）框架，利用图神经网络的等变性质来使模型学习可转移的任务适应策略。该方法在各种Few-shot分类基准上展示了最先进的性能。

    

    虽然图神经网络（GNN）在节点分类任务中取得了成功，但其性能严重依赖于每类具有足够标记节点的可用性。在现实情况下，不是所有类都有许多标记节点，模型可能需要分类新类别，使得手动标记变得困难。为了解决这个问题，GNN需要能够用有限数量的标记节点对节点进行分类，称为Few-shot节点分类。先前的基于剧集元学习的方法在Few-shot节点分类中取得了成功，但我们的发现表明仅有多样的训练元任务才能实现最佳性能。为了应对基于元学习的Few-shot学习的挑战，我们提出了一种新的方法，即任务等变图Few-shot学习（TEG）框架。我们的TEG框架通过利用图神经网络的等变性质来使模型学习可转移的任务适应策略。我们在各种Few-shot分类基准上展示了我们提出的方法的有效性，实现了最先进的性能。

    Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strate
    
[^70]: 基于图的多ODE神经网络用于时空交通预测

    Graph-based Multi-ODE Neural Networks for Spatio-Temporal Traffic Forecasting. (arXiv:2305.18687v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18687](http://arxiv.org/abs/2305.18687)

    本文提出了一种称为GRAM-ODENNs的新型神经网络结构，用于解决当前图ODE模型的局限性，包括局部模式的忽略和缺乏动态语义边缘，用于更准确的长程交通预测。

    

    近年来交通领域中出现了许多时空预测模型的发展。然而，由于交通网络中观察到的复杂而广泛的时空相关性，长程交通预测仍然是一个具有挑战性的任务。目前的作品主要依赖于具有图形结构的道路网络，并使用图神经网络（GNN）学习表示，但这种方法在深度架构中存在过度平滑的问题。为了解决这个问题，最近的方法介绍了将GNN与残差连接或神经普通微分方程（ODE）相结合的方法。然而，当前的图形ODE模型在特征提取方面存在两个关键限制：（1）它们倾向于全局时间模式，忽略了对于意外事件很重要的局部模式；（2）它们在其架构设计中缺乏动态语义边缘。在本文中，我们提出了一种称为基于图的多ODE神经网络（GRAM-ODENNs）的新型架构，用于解决当前图形ODE模型的局限性。GRAM-ODENNs使用多个并行ODE编码器和动态图结构，适应交通数据的时间演变，从而实现更准确的长程交通预测。

    There is a recent surge in the development of spatio-temporal forecasting models in the transportation domain. Long-range traffic forecasting, however, remains a challenging task due to the intricate and extensive spatio-temporal correlations observed in traffic networks. Current works primarily rely on road networks with graph structures and learn representations using graph neural networks (GNNs), but this approach suffers from over-smoothing problem in deep architectures. To tackle this problem, recent methods introduced the combination of GNNs with residual connections or neural ordinary differential equations (ODE). However, current graph ODE models face two key limitations in feature extraction: (1) they lean towards global temporal patterns, overlooking local patterns that are important for unexpected events; and (2) they lack dynamic semantic edges in their architectural design. In this paper, we propose a novel architecture called Graph-based Multi-ODE Neural Networks (GRAM-OD
    
[^71]: 可控毁灭路径

    Controllable Path of Destruction. (arXiv:2305.18553v1 [cs.AI])

    [http://arxiv.org/abs/2305.18553](http://arxiv.org/abs/2305.18553)

    本文介绍了可控毁灭路径方法，该方法是一种自我监督的迭代生成器学习方法，通过向修复轨迹的状态-动作对添加条件输入来实现可控性。

    

    毁灭路径（PoD）是一种自我监督的迭代生成器学习方法。其核心思想是通过破坏一组物品来产生一个训练集，为每个破坏步骤创建一个与相应修复动作相关的训练实例。在此数据集上训练的生成器可以通过从任意状态“修复”来生成新的物品。PoD方法在原始训练示例方面非常节省，并且非常适合由分类数据组成的功能部件，例如游戏关卡和离散的3D结构。在本文中，我们将毁灭路径方法扩展到允许设计师控制生成的物品的各个方面。通过向构成修复轨迹的状态-动作对添加条件输入来引入可控性。我们在2D地牢设置以及小型3D乐高汽车领域测试了可控PoD方法。

    Path of Destruction (PoD) is a self-supervised method for learning iterative generators. The core idea is to produce a training set by destroying a set of artifacts, and for each destructive step create a training instance based on the corresponding repair action. A generator trained on this dataset can then generate new artifacts by ``repairing'' from arbitrary states. The PoD method is very data-efficient in terms of original training examples and well-suited to functional artifacts composed of categorical data, such as game levels and discrete 3D structures. In this paper, we extend the Path of Destruction method to allow designer control over aspects of the generated artifacts. Controllability is introduced by adding conditional inputs to the state-action pairs that make up the repair trajectories. We test the controllable PoD method in a 2D dungeon setting, as well as in the domain of small 3D Lego cars.
    
[^72]: 一种元学习框架用于调整可信联邦学习保护机制的参数

    A Meta-learning Framework for Tuning Parameters of Protection Mechanisms in Trustworthy Federated Learning. (arXiv:2305.18400v1 [cs.LG])

    [http://arxiv.org/abs/2305.18400](http://arxiv.org/abs/2305.18400)

    提出了一个元学习框架，用于调整可信联邦学习保护机制的参数，以在隐私泄露、效用损失和效率降低之间进行权衡。

    

    可信联邦学习（TFL）通常利用保护机制来保证隐私安全。然而，保护机制不可避免地会引入效用损失或效率降低，同时保护数据隐私。因此，保护机制及其参数应该仔细选择，以在保护隐私泄露、效用损失和效率降低之间取得最佳平衡。为此，联邦学习从业者需要工具来衡量这三个因素，并优化它们之间的权衡，选择最适合手头应用的保护机制。基于这个要求，我们提出了一个框架，它(1)将TFL定义为找到保护机制来优化隐私泄露、效用损失和效率降低三者之间的权衡的问题；(2)正式定义了这三个因素的有界测量。然后，我们提出了一个元学习算法来近似解决此优化问题。

    Trustworthy Federated Learning (TFL) typically leverages protection mechanisms to guarantee privacy. However, protection mechanisms inevitably introduce utility loss or efficiency reduction while protecting data privacy. Therefore, protection mechanisms and their parameters should be carefully chosen to strike an optimal tradeoff between \textit{privacy leakage}, \textit{utility loss}, and \textit{efficiency reduction}. To this end, federated learning practitioners need tools to measure the three factors and optimize the tradeoff between them to choose the protection mechanism that is most appropriate to the application at hand. Motivated by this requirement, we propose a framework that (1) formulates TFL as a problem of finding a protection mechanism to optimize the tradeoff between privacy leakage, utility loss, and efficiency reduction and (2) formally defines bounded measurements of the three factors. We then propose a meta-learning algorithm to approximate this optimization proble
    
[^73]: 可视化编程中神经任务合成

    Neural Task Synthesis for Visual Programming. (arXiv:2305.18342v1 [cs.LG])

    [http://arxiv.org/abs/2305.18342](http://arxiv.org/abs/2305.18342)

    该论文提出了一种基于神经符号技术的可视化编程任务合成方法NeurTaskSyn。该方法能够针对规范中给出的解决方案代码所需要的编程概念和对可视化任务的限制，自动生成编程任务。

    

    通过合成新的内容，生成式神经模型在增强编程教育方面具有巨大的潜力。我们旨在设计神经模型，能够根据可视化编程环境下给定的规范自动生成编程任务。尽管近年来像 GPT-4 这样的大型生成模型获得了成功，但我们的初步结果显示，这些模型在合成可视化编程任务方面效果不佳，并且在逻辑和空间推理方面存在困难。我们提出了一种新颖的神经符号技术 NeurTaskSyn，该技术能够针对规范中给出的解决方案代码所需要的编程概念和对可视化任务的限制，合成编程任务。NeurTaskSyn 由两个部分构成：第一个部分通过模仿学习程序进行训练，生成可能的解决方案代码，第二个部分通过强化学习程序进行训练，指导底层符号执行引擎生成可视化任务。

    Generative neural models hold great promise in enhancing programming education by synthesizing new content for students. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visua
    
[^74]: 用Transformer学习超关系型和数值知识图中的表征学习

    Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers. (arXiv:2305.18256v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18256](http://arxiv.org/abs/2305.18256)

    本文提出了一个名为HyNT的框架，用于学习超关系型知识图的表示，包括数值文字。该框架使用上下文Transformer和预测Transformer，通过学习三元组和其限定词之间的相关性以及数值信息来获得模型。

    

    近期研究了一个超关系型知识图谱，其中三元组与限定词集合相关联; 一个限定词由关系和实体组成，为三元组提供辅助信息。现有的超关系型知识图嵌入方法假定实体是离散对象，但有些信息应使用数值表示，例如(J.R.R.，出生于，1892)。同时，三元组(J.R.R.，就读于，牛津大学)可以与限定词(开始时间，1911)相关联。在本文中，我们提出了一个名为HyNT的统一框架，用于学习包含三元组或限定词中数值文字的超关系型知识图的表示。我们定义了一个上下文Transformer和一个预测Transformer，来学习表示，不仅基于三元组和其限定词之间的相关性，还基于数值信息。通过学习三元组和限定词的紧凑表示，并将它们馈送给Transformer来获得模型

    A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding 
    
[^75]: InDL: 基于视错觉的图中逻辑解释新数据集和基准的研究

    InDL: A New Datasets and Benchmark for In-Diagram Logic Interpreting based on Visual Illusion. (arXiv:2305.17716v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.17716](http://arxiv.org/abs/2305.17716)

    本文提出了一个基于视错觉的独特数据集InDL，用于测试和评估深度学习模型的图中逻辑解释能力。利用几何光学视错觉，建立可比性框架用于阐明模型可能存在的缺陷和提供改进模型的洞察力。

    

    本文提出了一种新的方法来评估深度学习模型在图中逻辑解释方面的能力。通过利用有趣领域的视错觉，我们建立了一个独特的数据集InDL，旨在严格测试和基准这些模型。我们利用六个经典的几何视觉错觉，创建了一个人类和机器视觉感知的可比性框架。这种方法提供了一个可量化的衡量模型的方法，阐明了可能存在的缺陷，并提供了改进模型的行动洞察力。

    This paper introduces a novel approach to evaluating deep learning models' capacity for in-diagram logic interpretation. Leveraging the intriguing realm of visual illusions, we establish a unique dataset, InDL, designed to rigorously test and benchmark these models. Deep learning has witnessed remarkable progress in domains such as computer vision and natural language processing. However, models often stumble in tasks requiring logical reasoning due to their inherent 'black box' characteristics, which obscure the decision-making process. Our work presents a new lens to understand these models better by focusing on their handling of visual illusions -- a complex interplay of perception and logic. We utilize six classic geometric optical illusions to create a comparative framework between human and machine visual perception. This methodology offers a quantifiable measure to rank models, elucidating potential weaknesses and providing actionable insights for model improvements. Our experim
    
[^76]: 疾病患者个体根本原因的反事实公式化

    Counterfactual Formulation of Patient-Specific Root Causes of Disease. (arXiv:2305.17574v1 [cs.AI])

    [http://arxiv.org/abs/2305.17574](http://arxiv.org/abs/2305.17574)

    本文提出了一种针对疾病患者个体的根本原因的新公式，可以用于自动从数据中检测根本原因，并考虑了噪声标签和疾病流行率等因素，同时具有快速计算的优势。

    

    疾病的根本原因直观地对应于增加诊断可能性的根本顶点。然而，这种根本原因的描述缺乏计算机算法发展所需的严格数学公式。在以前的工作中，使用干预主义者帐户定义了疾病的病人特定根本原因，该帐户仅攀升到珍珠的因果Ladder的第二层。在这个理论性的文章中，我们通过提出反事实的定义来攀升到第三层，以匹配基于固定事实数据的临床直觉。然后，我们展示了如何使用可解释的人工智能的Shapley值为每个变量分配根因贡献得分。提出的疾病患者个体根本原因的反事实公式化考虑了噪声标签，适应了疾病的流行率，并允许快速计算，无需反事实模拟。

    Root causes of disease intuitively correspond to root vertices that increase the likelihood of a diagnosis. This description of a root cause nevertheless lacks the rigorous mathematical formulation needed for the development of computer algorithms designed to automatically detect root causes from data. Prior work defined patient-specific root causes of disease using an interventionalist account that only climbs to the second rung of Pearl's Ladder of Causation. In this theoretical piece, we climb to the third rung by proposing a counterfactual definition matching clinical intuition based on fixed factual data alone. We then show how to assign a root causal contribution score to each variable using Shapley values from explainable artificial intelligence. The proposed counterfactual formulation of patient-specific root causes of disease accounts for noisy labels, adapts to disease prevalence and admits fast computation without the need for counterfactual simulation.
    
[^77]: 深度学习模型概述与比较分析：CNN、RNN、LSTM、GRU。

    A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU. (arXiv:2305.17473v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.17473](http://arxiv.org/abs/2305.17473)

    本文全面概括了深度学习模型的类型和应用，比较分析了各个模型的结构、优点和局限性，有助于选择和设计深度学习模型。

    

    深度学习（DL）是机器学习（ML）和人工智能（AI）的强大子集，特别在处理非结构化和大型数据集方面优于传统的ML方法。其影响跨越各个领域，包括语音识别、医疗保健、自动驾驶汽车、网络安全、预测分析等。然而，实际问题的复杂性和动态性给设计有效的深度学习模型带来了挑战。因此，人们开发出了几种不同的深度学习模型来解决不同的问题和应用。在本文中，我们对各种深度学习模型进行了全面调查，包括卷积神经网络（CNN）、循环神经网络（RNN）、生成模型、深度强化学习（DRL）和深度迁移学习。我们考察了每个模型的结构、应用、好处和局限性。此外，我们使用了三个公开可用的数据集进行了分析。

    Deep learning (DL) has emerged as a powerful subset of machine learning (ML) and artificial intelligence (AI), outperforming traditional ML methods, especially in handling unstructured and large datasets. Its impact spans across various domains, including speech recognition, healthcare, autonomous vehicles, cybersecurity, predictive analytics, and more. However, the complexity and dynamic nature of real-world problems present challenges in designing effective deep learning models. Consequently, several deep learning models have been developed to address different problems and applications. In this article, we conduct a comprehensive survey of various deep learning models, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Models, Deep Reinforcement Learning (DRL), and Deep Transfer Learning. We examine the structure, applications, benefits, and limitations of each model. Furthermore, we perform an analysis using three publicly available dataset
    
[^78]: 神经智能中的注意力模式

    Attention Schema in Neural Agents. (arXiv:2305.17375v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.17375](http://arxiv.org/abs/2305.17375)

    本文研究了神经智能中的注意力模式，并提出了注意力模式理论（AST）。作者发现将AS实现为一种循环内部控制的智能体效果最佳，这一理论为应用与改进神经智能提供了新思路。

    

    注意力已经成为深度学习架构中的常见要素。它通过加入动态信息选择，支持静态的权重选择。同样地，我们可以想象在注意力之上构建一个更高阶的信息过滤器：注意力模式（AS）。也就是说，一个描述性和预测性的注意力模型。在认知神经科学中，注意力模式理论（AST）支持这种区分注意力和AS的想法。该理论的一个重要预测是，一个智能体可以使用自己的AS来推断其他智能体的注意状态，从而增强与其他智能体的协调。因此，多智能体强化学习是一个实验测试AST有效性的理想场景。我们探讨了注意力和AS相互作用的不同方式，初步结果表明，将AS实现为一种循环内部控制的智能体效果最佳。总体而言，这些实验为理解以及改进神经智能中的注意力模式提供了新的思路。

    Attention has become a common ingredient in deep learning architectures. It adds a dynamical selection of information on top of the static selection of information supported by weights. In the same way, we can imagine a higher-order informational filter built on top of attention: an Attention Schema (AS), namely, a descriptive and predictive model of attention. In cognitive neuroscience, Attention Schema Theory (AST) supports this idea of distinguishing attention from AS. A strong prediction of this theory is that an agent can use its own AS to also infer the states of other agents' attention and consequently enhance coordination with other agents. As such, multi-agent reinforcement learning would be an ideal setting to experimentally test the validity of AST. We explore different ways in which attention and AS interact with each other. Our preliminary results indicate that agents that implement the AS as a recurrent internal control achieve the best performance. In general, these expl
    
[^79]: 大型语言模型的异质价值评估

    Heterogeneous Value Evaluation for Large Language Models. (arXiv:2305.17147v1 [cs.CL])

    [http://arxiv.org/abs/2305.17147](http://arxiv.org/abs/2305.17147)

    本文提出了一种自动对齐评估方法A2EHV，采用异质价值系统，并基于价值合理性和社会价值定向框架评估代理人行为的社会偏好，结果表明比传统对齐方法更合理。

    

    大型语言模型（LLM）的出现使得将它们的价值与人类价值对齐变得至关重要。当前的方法通常尝试将其与一种同质的人类价值对齐，并需要人类验证，但缺乏对对齐所需方面和深度的共识以及造成的人类偏见。在本文中，我们提出了一种自动对齐评估方法A2EHV，该方法采用异质价值系统，（1）是自动化的，以最小化单个人类偏见，并且（2）允许评估针对各种目标值的异质代理人。我们的方法基于价值合理性的概念，它代表了代理人执行最能满足目标价值行为的能力。价值合理性的量化是通过社会心理学中的社会价值定向框架进行的，该框架将价值空间分为四个类别，以评估代理人行为的社会偏好。我们评估了三个模型的价值合理性，结果表明A2EHV方法比传统对齐方法更合理。

    The emergent capabilities of Large Language Models (LLMs) have made it crucial to align their values with those of humans. Current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. In this paper, we propose A2EHV, an Automated Alignment Evaluation with a Heterogeneous Value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. Our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. The quantification of value rationality is facilitated by the Social Value Orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. We evaluate the value rationality of e
    
[^80]: Minecraft中的幽灵：利用基于文本知识和记忆的大型语言模型实现开放世界环境中的通用能力智能体。

    Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory. (arXiv:2305.17144v1 [cs.AI])

    [http://arxiv.org/abs/2305.17144](http://arxiv.org/abs/2305.17144)

    本文提出了Ghost in the Minecraft (GITM)框架，利用大型语言模型与基于文本的知识和记忆，创造了一种在Minecraft中具备通用能力的智能体，可在以文本为基础的复杂编程环境中熟练导航。

    

    近年来，Minecraft玩法吸引了大量的研究关注，成为开发能够在开放世界环境中运行的智能体的丰富平台。然而，当前的研究主要集中在特定的目标上，例如流行的“ObtainDiamond”任务，并且还没有显示出有效地推广到更广泛任务的能力。此外，“ObtainDiamond”任务的目前最高成功率只有约20％，凸显了现有方法中使用强化学习（RL）控制器的局限性。为了解决这些挑战，我们引入了Ghost in the Minecraft (GITM)，一个新颖的框架，将大型语言模型与基于文本的知识和记忆相结合，旨在创建Minecraft中的通用能力智能体。这些具备LLM中的逻辑和常识能力的智能体可以熟练地在以文本为基础的复杂编程环境中导航。

    The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular "ObtainDiamond" task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the "ObtainDiamond" task stands at around 20%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based 
    
[^81]: 人人可复现的NLP研究：初学者的需求调查

    NLP Reproducibility For All: Understanding Experiences of Beginners. (arXiv:2305.16579v1 [cs.CL])

    [http://arxiv.org/abs/2305.16579](http://arxiv.org/abs/2305.16579)

    通过对93名NLP初学者的调查，发现研究作者提供完整文档、更好的代码实践和更易于获取的数据文件是初学者成功复现最近NLP论文结果的关键，建议NLP研究人员注重这些方面，更好地支持初学者。

    

    随着自然语言处理（NLP）近年来异常火爆，越来越多的人急于进入该领域，但目前的研究复现努力是否足以让这些初学者应用最新的进展还不清楚。为了了解初学者的需求，我们在一个介绍性的NLP课程中开展了一项研究，让学生复现最近NLP论文的结果。令人惊讶的是，我们发现他们的编程技能和对研究论文的理解对完成练习的付出仅有限的影响，相比之下，研究作者的可访问性努力是成功的关键，包括完整的文档、更好的编码实践和更容易获取的数据文件。前进时，我们建议NLP研究人员密切关注这些开源工作的简单方面，并使用初学者的反馈见解提供可操作的想法以更好地支持他们。

    As natural language processing (NLP) has recently seen an unprecedented level of excitement, and more people are eager to enter the field, it is unclear whether current research reproducibility efforts are sufficient for this group of beginners to apply the latest developments. To understand their needs, we conducted a study with 93 students in an introductory NLP course, where students reproduced the results of recent NLP papers. Surprisingly, we find that their programming skill and comprehension of research papers have a limited impact on their effort spent completing the exercise. Instead, we find accessibility efforts by research authors to be the key to success, including complete documentation, better coding practice, and easier access to data files. Going forward, we recommend that NLP researchers pay close attention to these simple aspects of open-sourcing their work, and use insights from beginners' feedback to provide actionable ideas on how to better support them.
    
[^82]: 长文本的神经自然语言处理：现状综述

    Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art. (arXiv:2305.16259v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16259](http://arxiv.org/abs/2305.16259)

    本文简要概述了长文本的神经自然语言处理的现状，主要包括文档分类和摘要，涵盖了情感分析，同时还探讨了长文本NLP的主要挑战、问题和解决方案。

    

    在过去的十年中，深度神经网络（DNN）的采用极大地促进了自然语言处理（NLP）的发展。然而，长文本分析的需求与短文本有很大不同，而网络上传输的文档大小不断增加，使长文本的自动理解成为一项关键的研究领域。本文的两个目标是：a）概述相关的神经构建模块，作为短教程；b）总结长文本NLP的现状，主要关注两个核心任务：文档分类和文档摘要。情感分析也涵盖在内，因为它通常被视为文档分类的特例。此外，本文还讨论了长文本NLP相关的主要挑战、问题和解决方案。最后，介绍了相关的公开的注释数据集，以便促进进一步研究。

    The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded on-line renders automated understanding of long texts a critical area of research. This article has two goals: a) it overviews the relevant neural building blocks, thus serving as a short tutorial, and b) it surveys the state-of-the-art in long document NLP, mainly focusing on two central tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Additionally, this article discusses the main challenges, issues and current solutions related to long document NLP. Finally, the relevant, publicly available, annotated datasets are presented, in order to facilitate further research.
    
[^83]: 大型语言模型是零样本文本到视频生成的帧级导演

    Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation. (arXiv:2305.14330v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.14330](http://arxiv.org/abs/2305.14330)

    本文引入了一个新的框架——DirecT2V，利用大型语言模型作为导演，从一个抽象的用户提示中生成零样本文本到视频生成的连贯且连贯的视频。该框架使用LLM导演将用户输入分为每一帧的提示，通过值映射和双softmax过滤器来保持时间一致和防止对象折叠。

    

    在人工智能生成内容（AIGC）的范式中，越来越多的关注点放在将预训练的文本到图像（T2I）模型扩展到文本到视频（T2V）生成上。尽管这些框架很有效，但它们面临着维护一致的叙述和处理从单个用户提示中的快速场景组合或对象位置变化的挑战。本文引入了一个新的框架，称为DirecT2V，它利用针对指令校准的大型语言模型（LLMs）从单个抽象用户提示生成逐帧描述。DirecT2V利用LLM导演将用户输入分为每个帧的单独提示，从而实现包含时间变化的内容和便于一致的视频生成。为了保持时间上的一致性和防止对象折叠，我们提出了一种新的值映射方法和双softmax过滤器。广泛的实验结果验证了DirecT2V框架在零样本T2V生成中产生的视觉连贯和一致的视频生成的有效性。

    In the paradigm of AI-generated content (AIGC), there has been increasing attention in extending pre-trained text-to-image (T2I) models to text-to-video (T2V) generation. Despite their effectiveness, these frameworks face challenges in maintaining consistent narratives and handling rapid shifts in scene composition or object placement from a single user prompt. This paper introduces a new framework, dubbed DirecT2V, which leverages instruction-tuned large language models (LLMs) to generate frame-by-frame descriptions from a single abstract user prompt. DirecT2V utilizes LLM directors to divide user inputs into separate prompts for each frame, enabling the inclusion of time-varying content and facilitating consistent video generation. To maintain temporal consistency and prevent object collapse, we propose a novel value mapping method and dual-softmax filtering. Extensive experimental results validate the effectiveness of the DirecT2V framework in producing visually coherent and consist
    
[^84]: 零样本多语言神经机器翻译中的“离谱问题”

    On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation. (arXiv:2305.10930v1 [cs.CL])

    [http://arxiv.org/abs/2305.10930](http://arxiv.org/abs/2305.10930)

    零样本多语言神经机器翻译容易出现“离谱问题”，本文提出的简单且有效的算法LAVS可以通过增加语言之间的KL分歧显著降低这个问题。

    

    尽管多语言神经机器翻译取得了巨大成功，但它仍然存在“离谱问题”，即将翻译输出到错误的语言中。这个问题在零样本翻译任务中更加明显。本文发现，当编码目标语言信号时失效，会导致离谱问题，并且两种语言词汇之间更接近的词汇距离（即KL分歧）与更高的离谱率有关。此外，本文还发现，仅隔离解码器中不同语言的词汇可以缓解这个问题。基于这些发现，我们提出了一种简单有效的算法Language Aware Vocabulary Sharing (LAVS)来构建多语言词汇表，通过增加语言之间的KL分歧，大大减轻了翻译模型的离谱问题。我们在11种语言的多语言翻译基准测试上进行了实验。实验结果表明，对于90个翻译任务，采用LAVS的离谱率降低了37％至90％。

    While multilingual neural machine translation has achieved great success, it suffers from the off-target issue, where the translation is in the wrong language. This problem is more pronounced on zero-shot translation tasks. In this work, we find that failing in encoding discriminative target language signal will lead to off-target and a closer lexical distance (i.e., KL-divergence) between two languages' vocabularies is related with a higher off-target rate. We also find that solely isolating the vocab of different languages in the decoder can alleviate the problem. Motivated by the findings, we propose Language Aware Vocabulary Sharing (LAVS), a simple and effective algorithm to construct the multilingual vocabulary, that greatly alleviates the off-target problem of the translation model by increasing the KL-divergence between languages. We conduct experiments on a multilingual machine translation benchmark in 11 languages. Experiments show that the off-target rate for 90 translation 
    
[^85]: 增量因果图学习进行在线无监督根本原因分析

    Incremental Causal Graph Learning for Online Unsupervised Root Cause Analysis. (arXiv:2305.10638v1 [cs.LG])

    [http://arxiv.org/abs/2305.10638](http://arxiv.org/abs/2305.10638)

    本文提出了CORAL，一种用于在线无监督根本原因分析的新框架，可以自动触发该过程并增量更新模型，包括三个主要部分：触发点检测，增量因果图学习和基于网络传播的根本原因定位。

    

    根本原因分析（RCA）的任务是分析系统监控数据，以识别系统故障/失效的根本原因。有效的RCA可以大大加速系统故障恢复，并减轻系统损失或财务损失。然而，以前的研究大多集中在开发离线RCA算法上，这通常需要手动启动RCA过程，需要大量时间和数据来训练稳健的模型，然后需要从头开始重新训练新的系统故障。在本文中，我们提出了CORAL，一种新颖的在线RCA框架，可以自动触发RCA过程并增量更新RCA模型。CORAL包括触发点检测、增量解缠因果图学习和基于网络传播的根本原因定位。触发点检测组件旨在自动检测系统状态转换并进行准实时检测。为此，我们开发了一种基于m的在线触发点检测方法。

    The task of root cause analysis (RCA) is to identify the root causes of system faults/failures by analyzing system monitoring data. Efficient RCA can greatly accelerate system failure recovery and mitigate system damages or financial losses. However, previous research has mostly focused on developing offline RCA algorithms, which often require manually initiating the RCA process, a significant amount of time and data to train a robust model, and then being retrained from scratch for a new system fault.  In this paper, we propose CORAL, a novel online RCA framework that can automatically trigger the RCA process and incrementally update the RCA model. CORAL consists of Trigger Point Detection, Incremental Disentangled Causal Graph Learning, and Network Propagation-based Root Cause Localization. The Trigger Point Detection component aims to detect system state transitions automatically and in near-real-time. To achieve this, we develop an online trigger point detection approach based on m
    
[^86]: 基于卷积神经网络的自动照片方向检测

    Automatic Photo Orientation Detection with Convolutional Neural Networks. (arXiv:2305.10319v1 [cs.CV])

    [http://arxiv.org/abs/2305.10319](http://arxiv.org/abs/2305.10319)

    本论文使用CNN解决了照片方向检测的问题，并在数据集上显著提高性能。使用Guided Backpropagation获得了CNN检测方向的见解。

    

    本文探讨了利用卷积神经网络(CNN)来解决确定消费者照片正确方向(0°, 90°, 180°和270°)的问题，特别对于模拟照片的数字化非常重要。我们在标准数据集上显著提高了性能，并在更困难的消费者照片大型数据集上进行了测试。我们使用引导反向传播(Guided Backpropagation)来获得关于CNN如何检测照片方向的见解，并解释其错误。

    We apply convolutional neural networks (CNN) to the problem of image orientation detection in the context of determining the correct orientation (from 0, 90, 180, and 270 degrees) of a consumer photo. The problem is especially important for digitazing analog photographs. We substantially improve on the published state of the art in terms of the performance on one of the standard datasets, and test our system on a more difficult large dataset of consumer photos. We use Guided Backpropagation to obtain insights into how our CNN detects photo orientation, and to explain its mistakes.
    
[^87]: HICO-DET-SG和V-COCO-SG：新的数据拆分用于评估人-物交互检测中的系统性泛化

    HICO-DET-SG and V-COCO-SG: New Data Splits to Evaluate Systematic Generalization in Human-Object Interaction Detection. (arXiv:2305.09948v1 [cs.CV])

    [http://arxiv.org/abs/2305.09948](http://arxiv.org/abs/2305.09948)

    本论文提出了两个新的HOI检测数据拆分，旨在评估系统性泛化。在新的数据拆分上测试结果表明，HOI检测模型对于未见过的对象和交互组合的泛化十分困难。

    

    人-物交互检测是一种预测图像中人与物品之间交互的任务。在实际场景中，需要对HOI检测模型进行系统性的泛化，即泛化到新的对象和交互组合上，因为训练数据仅可能涵盖所有可能组合的一小部分。然而，据我们所知，没有开放的基准测试或现有工作评估HOI检测中的系统性泛化。为解决这个问题，我们基于HICO-DET和V-COCO数据集创建了两个名为HICO-DET-SG和V-COCO-SG的新的HOI检测数据拆分。我们在新的数据拆分上评估了代表性的HOI检测模型，并观察到与原始数据集上相比测试性能有很大的降低。这个结果表明系统性泛化是HOI检测中一个具有挑战性的目标。我们希望我们的新数据拆分能够鼓励更多的研究朝着这个目标努力。

    Human-Object Interaction (HOI) detection is a task to predict interactions between humans and objects in an image. In real-world scenarios, HOI detection models are required systematic generalization, i.e., generalization to novel combinations of objects and interactions, because it is highly probable that the train data only cover a limited portion of all possible combinations. However, to our knowledge, no open benchmark or existing work evaluates the systematic generalization in HOI detection. To address this issue, we created two new sets of HOI detection data splits named HICO-DET-SG and V-COCO-SG based on HICO-DET and V-COCO datasets. We evaluated representative HOI detection models on the new data splits and observed large degradation in the test performances compared to those on the original datasets. This result shows that systematic generalization is a challenging goal in HOI detection. We hope our new data splits encourage more research toward this goal.
    
[^88]: 降低现实策略优化中循环时间调整的代价

    Reducing the Cost of Cycle-Time Tuning for Real-World Policy Optimization. (arXiv:2305.05760v1 [cs.LG])

    [http://arxiv.org/abs/2305.05760](http://arxiv.org/abs/2305.05760)

    本研究提出了一种新方法，可基于周期时间设置超参数，使得PPO和SAC在广泛的循环时间范围内进行学习，同时实现了接近耗时的在线超参数调整获得的性能。

    

    连续时间强化学习任务通常使用固定周期时间的离散步骤进行操作。实践中需要为给定任务选择操作周期时间，一个重要问题是学习算法的超参数是否需要为每个周期时间重新调整，这对于现实世界的机器人来说是不可行的。在本文中，我们研究了两种策略梯度算法--PPO和SAC--在不同的周期时间下使用基线超参数值的情况。通过使用基线超参数在一个基准任务中展示这两种算法表现良好的情况，我们发现当选择不同于任务默认值的周期时间时，使用基线超参数的PPO无法学习。此外，当超参数用于每个周期时间时，基于基线的PPO和SAC表现均明显劣于它们的调整值。我们提出了一种基于周期时间设置这些超参数的新方法。在我们的仿真机器人运动任务实验中，我们的方法使得PPO和SAC在极其广泛的周期时间范围内进行学习，同时实现了接近于耗时的在线超参数调整获得的性能。

    Continuous-time reinforcement learning tasks commonly use discrete steps of fixed cycle times for actions. As practitioners need to choose the action-cycle time for a given task, a significant concern is whether the hyper-parameters of the learning algorithm need to be re-tuned for each choice of the cycle time, which is prohibitive for real-world robotics. In this work, we investigate the widely-used baseline hyper-parameter values of two policy gradient algorithms -- PPO and SAC -- across different cycle times. Using a benchmark task where the baseline hyper-parameters of both algorithms were shown to work well, we reveal that when a cycle time different than the task default is chosen, PPO with baseline hyper-parameters fails to learn. Moreover, both PPO and SAC with their baseline hyper-parameters perform substantially worse than their tuned values for each cycle time. We propose novel approaches for setting these hyper-parameters based on the cycle time. In our experiments on simu
    
[^89]: 基于图形遮盖自编码器的序列推荐系统

    Graph Masked Autoencoder for Sequential Recommendation. (arXiv:2305.04619v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2305.04619](http://arxiv.org/abs/2305.04619)

    提出了一种简单而有效的基于图遮盖自编码器的序列推荐系统，它使用基于图的注意力机制暴露出带有遮盖的项目序列，自适应动态提取全局项目转换信息进行自监督增强，在具有较少标记样本的情况下始终比最先进的序列推荐方法表现出更好的性能，而且对数据损坏和缺失情况具有鲁棒性。

    

    虽然一些强大的神经网络架构（例如Transformer、图神经网络）通过高阶项依赖建模在序列推荐中实现了改进的性能，但它们可能在标签稀缺情况下表现出较差的表征能力。为了解决标签不足的问题，对比学习（CL）已经引起了近期的关注，通过嵌入对比来进行自我监督的数据增强。然而，由于其对比视图生成策略的手工制定特性，现有的CL增强模型不仅难以在不同的序列推荐任务中产生一致的性能，还可能对用户行为数据噪声不具有鲁棒性。鉴于这一点，我们提出了一种简单而有效的自适应全局信息提取的图遮盖自编码器增强的序列推荐系统（MAERec）来解决这个问题。它自然地避免了上述问题，得益于其独特的数据重构机制。具体而言，我们的模型使用基于图的注意力机制，暴露出带有遮盖的项目序列，使表示不仅利用本地顺序信息，还利用项目之间的全局相关性。我们在四个基准数据集上对我们的方法进行了广泛评估。实验结果表明，我们的模型在具有较少标记样本的情况下始终比最先进的序列推荐方法表现出更好的性能，而且对数据损坏和缺失情况具有鲁棒性。

    While some powerful neural network architectures (e.g., Transformer, Graph Neural Networks) have achieved improved performance in sequential recommendation with high-order item dependency modeling, they may suffer from poor representation capability in label scarcity scenarios. To address the issue of insufficient labels, Contrastive Learning (CL) has attracted much attention in recent methods to perform data augmentation through embedding contrasting for self-supervision. However, due to the hand-crafted property of their contrastive view generation strategies, existing CL-enhanced models i) can hardly yield consistent performance on diverse sequential recommendation tasks; ii) may not be immune to user behavior data noise. In light of this, we propose a simple yet effective Graph Masked AutoEncoder-enhanced sequential Recommender system (MAERec) that adaptively and dynamically distills global item transitional information for self-supervised augmentation. It naturally avoids the abov
    
[^90]: 如何发挥大语言模型在少样本关系抽取中的能力？

    How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])

    [http://arxiv.org/abs/2305.01555](http://arxiv.org/abs/2305.01555)

    本文通过使用GPT-3.5模型在少样本关系抽取中，实现在四个不同数据集上的新的最优性能，并提出了与任务相关的指导说明和约束模式下的数据生成方法。

    

    语言模型的扩展已经彻底改变了广泛的自然语言处理任务，但是使用大型语言模型进行少样本关系抽取还没有得到全面探索。本文通过详细实验，研究了使用GPT-3.5进行少样本关系抽取的基本方法——上下文学习和数据生成。为了增强少样本性能，我们进一步提出了与任务相关的指导说明和约束模式下的数据生成。我们观察到，在上下文学习的情况下，可以实现与以前的提示学习方法相当的性能，而使用大型语言模型的数据生成可以推动以前的解决方案以在四个广泛研究的关系抽取数据集上获得新的最先进的少样本结果。我们希望我们的工作可以激发未来对大型语言模型在少样本关系抽取中的能力的研究。代码可以在 \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} 中找到。

    Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
    
[^91]: 架起桥梁：自然语言生成中整合（人类）反馈的调查研究。

    Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation. (arXiv:2305.00955v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.00955](http://arxiv.org/abs/2305.00955)

    本文调查研究了利用人类反馈改进自然语言生成方面的最近研究，包括对反馈的全面形式化、反馈的格式和目的的描述，和提出使用反馈的两种方法。我们还讨论了用于人类反馈数据收集的现有数据集，和相关的担忧。

    

    最近许多自然语言生成方面的进展都是基于利用互联网规模的数据对大型语言模型进行训练。然而，这种范式可能会导致生成有害、不准确和无用内容的模型，而自动评估指标通常无法识别这些行为。随着模型变得更加强大，人类反馈成为评价和改进模型的宝贵信号。本调查旨在概述最近利用人类反馈改进自然语言生成方面的研究。首先，我们介绍了对反馈的全面形式化，并根据这种形式化将现有研究进行分类和组织。接下来，我们讨论了反馈可以通过其格式和目的来描述，并涵盖了提出使用反馈的两种方法（用于训练或解码）：直接使用反馈或训练反馈模型。我们还讨论了用于人类反馈数据收集的现有数据集，以及相关的担忧。

    Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns 
    
[^92]: 基于自引图的面向影响力的学者环境剖析

    Impact-Oriented Contextual Scholar Profiling using Self-Citation Graphs. (arXiv:2304.12217v2 [cs.DL] UPDATED)

    [http://arxiv.org/abs/2304.12217](http://arxiv.org/abs/2304.12217)

    本研究提出了一个名为GeneticFlow的基于自引图的学者剖析工具，能够满足学者特征剖析中的三个基本要求，即结构化背景、学者为中心和丰富的进化，在科学奖项推理真实任务中表现出色。

    

    定量地剖析学者的科研影响力对于现代研究社会至关重要。目前使用的文献计量指标（如h指数）、列表和网络在学者排名方面表现良好，但不提供学者相关、分析性任务的结构化背景，例如剖析和理解学者特征。本文提出了一个基于图的学者剖析工具——GeneticFlow (GF)，满足三个基本要求：结构化背景、学者为中心和丰富的进化。我们提出了一个框架，用于在包括数百万学者的大规模学术数据源上计算GF；该框架包含了一种新的无监督导师-学生检测算法、一个使用可解释特征的精心设计的引用类型分类器，以及一个经过微调的图神经网络(GNN)模型。通过对科学奖项推理的真实任务进行评估，实验结果表明，最佳GF剖析的F1得分明显优于替代方案。

    Quantitatively profiling a scholar's scientific impact is important to modern research society. Current practices with bibliometric indicators (e.g., h-index), lists, and networks perform well at scholar ranking, but do not provide structured context for scholar-centric, analytical tasks such as profile reasoning and understanding. This work presents GeneticFlow (GF), a suite of novel graph-based scholar profiles that fulfill three essential requirements: structured-context, scholar-centric, and evolution-rich. We propose a framework to compute GF over large-scale academic data sources with millions of scholars. The framework encompasses a new unsupervised advisor-advisee detection algorithm, a well-engineered citation type classifier using interpretable features, and a fine-tuned graph neural network (GNN) model. Evaluations are conducted on the real-world task of scientific award inference. Experiment outcomes show that the F1 score of best GF profile significantly outperforms altern
    
[^93]: 基于大脑启发的尖峰神经网络，用于在线无监督时间序列预测

    Brain-Inspired Spiking Neural Network for Online Unsupervised Time Series Prediction. (arXiv:2304.04697v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2304.04697](http://arxiv.org/abs/2304.04697)

    本论文提出了一种新的连续学习无监督循环尖峰神经网络模型，可以高效、节能地进行在线时间序列预测，并且通过随机延迟嵌入和使用底层动态系统中介数中心性最高的神经元的膜电位进行重构。

    

    在多个领域中，特别是需要基于流数据不断更新的边缘AI应用中，能够高效、节能地进行在线时间序列预测是至关重要的。然而，当前基于深度神经网络的有监督在线学习模型需要大量的训练数据，且无法在底层系统发生变化时快速适应。此外，这些模型需要使用传入数据进行持续的重新训练，使其高度低效。为了解决这些问题，我们提出了一种新的连续学习无监督循环尖峰神经网络模型（CLURSNN），其通过尖峰时序相关可塑性（STDP）进行训练。CLURSNN通过使用在可重构RSNN的具有最高介数中心性的神经元的膜电位来进行随机延迟嵌入，以重构底层动态系统进行在线预测。我们还利用拓扑数据分析提出了一种新的方法，使用Wasserstein距离。

    Energy and data-efficient online time series prediction for predicting evolving dynamical systems are critical in several fields, especially edge AI applications that need to update continuously based on streaming data. However, current DNN-based supervised online learning models require a large amount of training data and cannot quickly adapt when the underlying system changes. Moreover, these models require continuous retraining with incoming data making them highly inefficient. To solve these issues, we present a novel Continuous Learning-based Unsupervised Recurrent Spiking Neural Network Model (CLURSNN), trained with spike timing dependent plasticity (STDP). CLURSNN makes online predictions by reconstructing the underlying dynamical system using Random Delay Embedding by measuring the membrane potential of neurons in the recurrent layer of the RSNN with the highest betweenness centrality. We also use topological data analysis to propose a novel methodology using the Wasserstein Di
    
[^94]: 应用机器学习和领域知识个性化数字健康行为变革干预

    Personalizing Digital Health Behavior Change Interventions using Machine Learning and Domain Knowledge. (arXiv:2304.03392v1 [cs.LG])

    [http://arxiv.org/abs/2304.03392](http://arxiv.org/abs/2304.03392)

    该论文提出了一种采用机器学习和领域知识进行个性化数字健康行为变革干预的系统，其利用反事实例子进行特征控制以预测干预效果并优化干预效果。

    

    我们正在开发一种虚拟教练系统，帮助患者坚持行为变革干预（BCI）。我们的系统预测患者是否会执行目标行为，并使用反事实例子进行特征控制，以指导个性化BCI。我们使用具有不同响应水平的模拟患者数据评估了我们的预测模型。

    We are developing a virtual coaching system that helps patients adhere to behavior change interventions (BCI). Our proposed system predicts whether a patient will perform the targeted behavior and uses counterfactual examples with feature control to guide personalizsation of BCI. We evaluated our prediction model using simulated patient data with varying levels of receptivity to intervention.
    
[^95]: 端到端扩散潜在优化提高分类器引导能力。

    End-to-End Diffusion Latent Optimization Improves Classifier Guidance. (arXiv:2303.13703v1 [cs.CV])

    [http://arxiv.org/abs/2303.13703](http://arxiv.org/abs/2303.13703)

    本文提出了一种新的分类器引导方法DOODL，它可以通过针对真实生成的像素上预训练分类器的梯度优化扩散潜变来实现即插即用的指导，并在计算和人类评估度量上优于一步分类器指导。

    

    分类器指导——利用图像分类器的梯度来引导扩散模型的生成——有潜力大幅扩展对图像生成和编辑的创造性控制。然而，目前分类器指导要么需要训练新的噪声感知模型以获得精确的梯度，要么使用一步去噪的近似最终生成物，并导致梯度不对齐和次优控制。我们强调了这种近似的缺点，并提出了一种新的指导方法：直接优化扩散潜变（DOODL），它通过针对真实生成的像素上预训练分类器的梯度优化扩散潜变来实现即插即用的指导，使用可逆扩散过程实现内存有效的反向传递。展示了更精确指导潜力的 DOODL 在不同形式的指导的计算和人类评估度量上优于一步分类器指导。

    Classifier guidance -- using the gradients of an image classifier to steer the generations of a diffusion model -- has the potential to dramatically expand the creative control over image generation and editing. However, currently classifier guidance requires either training new noise-aware models to obtain accurate gradients or using a one-step denoising approximation of the final generation, which leads to misaligned gradients and sub-optimal control. We highlight this approximation's shortcomings and propose a novel guidance method: Direct Optimization of Diffusion Latents (DOODL), which enables plug-and-play guidance by optimizing diffusion latents w.r.t. the gradients of a pre-trained classifier on the true generated pixels, using an invertible diffusion process to achieve memory-efficient backpropagation. Showcasing the potential of more precise guidance, DOODL outperforms one-step classifier guidance on computational and human evaluation metrics across different forms of guidanc
    
[^96]: Aux-Drop: 使用辅助丢弃处理在线学习中的混乱输入

    Aux-Drop: Handling Haphazard Inputs in Online Learning Using Auxiliary Dropouts. (arXiv:2303.05155v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05155](http://arxiv.org/abs/2303.05155)

    本文介绍了一种名为Aux-Drop的策略，适用于在线学习，能够处理混乱的输入特征。通过调整常规的丢失正则化方案，确保最终输出受到这些特征混乱出现的最小影响，有助于构建更健壮有效的在线学习系统。

    

    许多基于在线学习的真实世界应用程序生成的流式数据具有混乱的特性，即包含缺失的特征，在时间上变得过时，以后出现新特征并且缺乏对总输入特征数量的清晰认识。这些挑战使得为这些应用构建可学习系统变得困难，并且几乎没有关于这个问题的深度学习研究工作。本文提出了Aux-Drop，一种用于在线学习的辅助丢失正则化策略，以有效处理混乱的输入特征。Aux-Drop为混乱的输入特征空间调整常规的丢失正则化方案，确保最终输出受到这些特征混乱出现的最小影响。它有助于防止特别是辅助和基础特征的共适应，并减少模型中任何辅助输入对输出的强烈依赖。这有助于为真实世界应用程序构建更健壮有效的在线学习系统。

    Many real-world applications based on online learning produce streaming data that is haphazard in nature, i.e., contains missing features, features becoming obsolete in time, the appearance of new features at later points in time and a lack of clarity on the total number of input features. These challenges make it hard to build a learnable system for such applications, and almost no work exists in deep learning that addresses this issue. In this paper, we present Aux-Drop, an auxiliary dropout regularization strategy for online learning that handles the haphazard input features in an effective manner. Aux-Drop adapts the conventional dropout regularization scheme for the haphazard input feature space ensuring that the final output is minimally impacted by the chaotic appearance of such features. It helps to prevent the co-adaptation of especially the auxiliary and base features, as well as reduces the strong dependence of the output on any of the auxiliary inputs of the model. This hel
    
[^97]: 基于查询-话语注意力和联合建模的查询焦点会议摘要

    Query-Utterance Attention with Joint modeling for Query-Focused Meeting Summarization. (arXiv:2303.04487v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.04487](http://arxiv.org/abs/2303.04487)

    本文提出了一种基于查询-话语注意力和联合建模的查询感知框架，它使用密集检索模块计算话语级别与查询的相关性，并将标记级别的查询关联性和话语级别的查询关联性结合起来，实现生成一个更与查询相关的摘要。经过对两个基准数据集上的测试，表明该方法优于现有的QFMS模型。

    

    查询焦点会议摘要（QFMS）旨在根据给定的查询，从会议记录中生成摘要。以往的方法通常将查询与会议记录拼接起来，并使用注意机制隐式地对标记级别的查询相关性进行建模。然而，由于长时间的会议记录导致关键的查询相关信息被稀释，因此原始的基于转换的模型不足以突出与查询相关的关键部分。本文提出了一种基于查询-话语注意力和联合建模的查询感知框架。它使用密集检索模块计算话语级别与查询的相关性。然后，将标记级别的查询关联性和话语级别的查询关联性结合起来，并通过明确的注意机制整合到生成过程中。我们表明，不同颗粒度的查询相关性有助于生成一个更与查询相关的摘要。在两个基准数据集上的实验结果表明，我们提出的方法优于现有的QFMS模型。

    Query-focused meeting summarization (QFMS) aims to generate summaries from meeting transcripts in response to a given query. Previous works typically concatenate the query with meeting transcripts and implicitly model the query relevance only at the token level with attention mechanism. However, due to the dilution of key query-relevant information caused by long meeting transcripts, the original transformer-based model is insufficient to highlight the key parts related to the query. In this paper, we propose a query-aware framework with joint modeling token and utterance based on Query-Utterance Attention. It calculates the utterance-level relevance to the query with a dense retrieval module. Then both token-level query relevance and utterance-level query relevance are combined and incorporated into the generation process with attention mechanism explicitly. We show that the query relevance of different granularities contributes to generating a summary more related to the query. Exper
    
[^98]: Almanac: 检索增强的临床医学语言模型

    Almanac: Retrieval-Augmented Language Models for Clinical Medicine. (arXiv:2303.01229v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.01229](http://arxiv.org/abs/2303.01229)

    Almanac是一个带有检索能力的大型语言模型框架，为临床医学的指南和治疗建议提供了事实性、完备性和安全性的增强。该模型有望成为临床决策过程中的有效工具。

    

    最近，大型语言模型已经在各种自然语言任务中展示出令人印象深刻的零-shot能力，例如摘要、对话生成和问答。虽然在临床医学中有许多有前途的应用，但这些模型在实际环境中的应用往往受到它们生成错误甚至有时是有害言论的限制。在本研究中，我们开发了Almanac，这是一个带有检索能力的大型语言模型框架，用于医疗指南和治疗建议。由5名董事会认证和住院医师评估的130个临床场景的新数据集的表现表明，在所有专业领域中，事实性显著提高（在p值<0.05的情况下增加了18%的平均值），并改善了完备性和安全性。我们的结果表明，大型语言模型有潜力成为临床决策过程中有效的工具，同时也强调了正确性的重要性。

    Large-language models have recently demonstrated impressive zero-shot capabilities in a variety of natural language tasks such as summarization, dialogue generation, and question-answering. Despite many promising applications in clinical medicine, adoption of these models in real-world settings has been largely limited by their tendency to generate incorrect and sometimes even toxic statements. In this study, we develop Almanac, a large language model framework augmented with retrieval capabilities for medical guideline and treatment recommendations. Performance on a novel dataset of clinical scenarios (n = 130) evaluated by a panel of 5 board-certified and resident physicians demonstrates significant increases in factuality (mean of 18% at p-value < 0.05) across all specialties, with improvements in completeness and safety. Our results demonstrate the potential for large language models to be effective tools in the clinical decision-making process, while also emphasizing the importanc
    
[^99]: 为相对表示式引入并行锚点的引导方法

    Bootstrapping Parallel Anchors for Relative Representations. (arXiv:2303.00721v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00721](http://arxiv.org/abs/2303.00721)

    该论文提出了一种引导方法，通过已知的集合发现新的并行锚点，以克服相对表示式中获取并行锚点困难的问题，能够用于不同域之间的语义对应，取得竞争性结果。

    

    相对表示法在各种应用中都显示出了作为潜在嵌入法的潜力，例如启用潜在空间通信和模型拼接。然而，相对表示依赖于一定量的并行锚点作为输入，但在某些情况下难以获取这些锚点。为了克服这一限制，我们提出了一种优化方法，从有限的已知集合（种子）中发现新的并行锚点。我们的方法可以用于在不同域之间寻找语义对应，对齐它们的相对空间，并在多项任务中取得竞争性结果。

    The use of relative representations for latent embeddings has shown potential in enabling latent space communication and zero-shot model stitching across a wide range of applications. Nevertheless, relative representations rely on a certain amount of parallel anchors to be given as input, which can be impractical to obtain in certain scenarios. To overcome this limitation, we propose an optimization-based method to discover new parallel anchors from a limited known set (seed). Our approach can be used to find semantic correspondence between different domains, align their relative spaces, and achieve competitive results in several tasks.
    
[^100]: 让我们来聊聊吧！与ChatGPT的对话：技术，应用和限制。

    Let's have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations. (arXiv:2302.13817v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.13817](http://arxiv.org/abs/2302.13817)

    本文讨论了聊天机器人的历史概述以及ChatGPT背后的技术，强调了它在医疗保健、教育和研究中的潜在应用，并指出了其隐私和道德方面的担忧以及当前版本的重要限制。

    

    一款能够生成像人类一样的句子和写出连贯文章的人工智能聊天机器人ChatGPT的出现引起了世界的关注。本文讨论了聊天机器人的历史概述以及ChatGPT背后的技术。此外，还强调了ChatGPT在各个领域，包括医疗保健，教育和研究中的潜在应用。尽管有着令人期待的结果，但是ChatGPT周围存在着一些隐私和道德方面的担忧。另外，我们还强调了当前版本ChatGPT的一些重要限制。我们还向ChatGPT提出了一些问题，以便它表达自己的看法。

    The emergence of an AI-powered chatbot that can generate human-like sentences and write coherent essays has caught the world's attention. This paper discusses the historical overview of chatbots and the technology behind Chat Generative Pre-trained Transformer, better known as ChatGPT. Moreover, potential applications of ChatGPT in various domains, including healthcare, education, and research, are highlighted. Despite promising results, there are several privacy and ethical concerns surrounding ChatGPT. In addition, we highlight some of the important limitations of the current version of ChatGPT. We also ask ChatGPT to provide its point of view and present its responses to several questions we attempt to answer.
    
[^101]: 遮盖数据有帮助：关于稀疏编码中遮盖的好处

    Hiding Data Helps: On the Benefits of Masking for Sparse Coding. (arXiv:2302.12715v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12715](http://arxiv.org/abs/2302.12715)

    本文研究了稀疏编码中学习得到的大于实际字典的情况下，噪声会导致标准的字典学习目标函数无法恢复出实际字典的问题，提出了通过遮盖数据的方法进行可靠的字典恢复适用于多种信号模态。

    

    稀疏编码被用在信号处理、计算机视觉以及医学成像等领域，在这些应用中，信号被建模为用学习到的字典中的元素的稀疏线性组合。然而，目前大多数研究关注学习到的字典与真实字典大小相同时的情况，并且仅研究了没有噪声的情景。本文中，我们提出了一种简单而有效的数据遮盖方法，能够保证在遮盖的情况下获得可靠的字典恢复结果。我们在多个数据集和信号模态下对我们的遮盖方法进行了实验验证，得到了优于现有方法的结果。

    Sparse coding, which refers to modeling a signal as sparse linear combinations of the elements of a learned dictionary, has proven to be a successful (and interpretable) approach in applications such as signal processing, computer vision, and medical imaging. While this success has spurred much work on provable guarantees for dictionary recovery when the learned dictionary is the same size as the ground-truth dictionary, work on the setting where the learned dictionary is larger (or over-realized) with respect to the ground truth is comparatively nascent. Existing theoretical results in this setting have been constrained to the case of noise-less data. We show in this work that, in the presence of noise, minimizing the standard dictionary learning objective can fail to recover the elements of the ground-truth dictionary in the over-realized regime, regardless of the magnitude of the signal in the data-generating process. Furthermore, drawing from the growing body of work on self-superv
    
[^102]: K-SHAP: 一种用于匿名状态-动作对的策略聚类算法

    K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs. (arXiv:2302.11996v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11996](http://arxiv.org/abs/2302.11996)

    本文提出了一种名为K-SHAP的算法，来解决多个智能体保持匿名且仅有状态-动作对的情况下学习智能体决策的问题。

    

    从观测数据中学习智能体行为已经被证明可以提高我们对它们决策过程的理解，从而增强我们解释它们与环境和其他智能体之间交互的能力。尽管文献中已经提出了多种学习技术，但还有一种特定的情况尚未被探索，那就是智能体身份保持匿名的多智能体系统。例如，在金融市场中，标记数据通常是专有的，仅公开多个市场参与者交互而产生的匿名状态-动作对。因此，智能体行动序列不可观测，限制了现有工作的适用性。本文提出了一种策略聚类算法K-SHAP，它学习根据智能体策略对匿名状态-动作对进行分组。我们将该问题作为模仿学习(IL)任务，学习一个w...

    Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. In this paper, we propose a Policy Clustering algorithm, called K-SHAP, that learns to group anonymous state-action pairs according to the agent policies. We frame the problem as an Imitation Learning (IL) task, and we learn a w
    
[^103]: 基于强化学习的稳健且多用途的双足跳跃控制

    Robust and Versatile Bipedal Jumping Control through Reinforcement Learning. (arXiv:2302.09450v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.09450](http://arxiv.org/abs/2302.09450)

    本论文提出了一个通过强化学习训练的智能双足机器人跳跃控制框架，可以执行各种跳跃任务，具有多用途和稳健性，且可直接转移到真实机器人应用。

    

    本文旨在通过使扭矩控制的双足机器人在实际环境中进行稳健且多用途的跳跃，将双足机器人的敏捷性提升到极限。我们提出了一个强化学习框架，用于训练机器人完成各种跳跃任务，如跳向不同的位置和方向。为了提高在这些具有挑战性的任务上的表现，我们开发了一种新的策略结构，它编码了机器人的长期输入/输出 (I/O) 历史记录，同时还提供了对短期 I/O 历史记录的直接访问。为了训练具有多用途跳跃策略，我们使用了一个多阶段的训练方案，其中包括不同目标的不同训练阶段。在多阶段训练之后，该策略可以直接转移到真实的双足 Cassie 机器人上。对不同任务进行训练和探索更多样化的情景会导致高度稳健的策略，这些策略可以利用所学习的各种机动方式来从扰动或不良着陆中恢复过来。

    This work aims to push the limits of agility for bipedal robots by enabling a torque-controlled bipedal robot to perform robust and versatile dynamic jumps in the real world. We present a reinforcement learning framework for training a robot to accomplish a large variety of jumping tasks, such as jumping to different locations and directions. To improve performance on these challenging tasks, we develop a new policy structure that encodes the robot's long-term input/output (I/O) history while also providing direct access to a short-term I/O history. In order to train a versatile jumping policy, we utilize a multi-stage training scheme that includes different training stages for different objectives. After multi-stage training, the policy can be directly transferred to a real bipedal Cassie robot. Training on different tasks and exploring more diverse scenarios lead to highly robust policies that can exploit the diverse set of learned maneuvers to recover from perturbations or poor land
    
[^104]: 通过大型语言模型进行关卡生成

    Level Generation Through Large Language Models. (arXiv:2302.05817v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.05817](http://arxiv.org/abs/2302.05817)

    该研究探讨了使用大型语言模型(LLMs)生成Sokoban游戏关卡，并发现随着数据集大小的增加，LLMs的性能得到了很大的提高。未来工作的前景也被讨论了。

    

    大型语言模型(LLM)是强大的工具，能够利用自然语言的训练写故事、生成代码和回答问题。但它们能否生成功能性的视频游戏关卡呢？游戏关卡由于功能约束和多维空间关系的复杂性，与LLM在训练期间通常看到的数据种类非常不同。游戏关卡的数据集也很难获得，可能会耗尽这些对数据有强烈需求的模型的能力。我们研究了使用LLMs生成Sokoban游戏关卡，并发现LLMs确实能够实现这一点，并且它的性能随着数据集大小的增加而大幅提高。我们还进行了初步的实验来控制LLM关卡生成器，并讨论了未来工作的有前途的领域。

    Large Language Models (LLMs) are powerful tools, capable of leveraging their training on natural language to write stories, generate code, and answer questions. But can they generate functional video game levels? Game levels, with their complex functional constraints and spatial relationships in more than one dimension, are very different from the kinds of data an LLM typically sees during training. Datasets of game levels are also hard to come by, potentially taxing the abilities of these data-hungry models. We investigate the use of LLMs to generate levels for the game Sokoban, finding that LLMs are indeed capable of doing so, and that their performance scales dramatically with dataset size. We also perform preliminary experiments on controlling LLM level generators and discuss promising areas for future work.
    
[^105]: 因果关系强化学习综述

    A Survey on Causal Reinforcement Learning. (arXiv:2302.05209v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.05209](http://arxiv.org/abs/2302.05209)

    本文综述了因果关系强化学习的最新进展和未来研究方向，将现有的CRL方法分为两类，并讨论了因果关系和RL之间的关系。

    

    在许多领域的顺序决策问题中，强化学习取得了巨大的成功，但仍面临数据效率和解释性的关键挑战。有趣的是，许多研究者最近已经利用因果关系文献的见解，带来了繁荣的工作，以统一因果关系的优点，并解决RL所面临的挑战。因此，总结这些因果关系强化学习（CRL）工作，提供CRL方法的综述，以及研究因果关系对RL的潜在功能，具有重要的必要性和意义。特别地，我们根据它们是否提前给出基于因果关系的信息，将现有的CRL方法分为两类。我们进一步分析每个类别在不同模型的规范化方面的关系，包括马尔可夫决策过程（MDP），部分观察到的马尔可夫决策过程（POMDP），多臂老虎机（MAB）和动态治疗效果（DTE）。通过各种角度讨论因果关系和RL之间的关系，包括探索-开发困境，反事实评估和基于模型的强化学习。总之，这篇综述论文全面概述了因果关系强化学习的最新进展，并指出了未来研究方向的潜在机会。

    While Reinforcement Learning (RL) achieves tremendous success in sequential decision-making problems of many domains, it still faces key challenges of data inefficiency and the lack of interpretability. Interestingly, many researchers have leveraged insights from the causality literature recently, bringing forth flourishing works to unify the merits of causality and address well the challenges from RL. As such, it is of great necessity and significance to collate these Causal Reinforcement Learning (CRL) works, offer a review of CRL methods, and investigate the potential functionality from causality toward RL. In particular, we divide existing CRL approaches into two categories according to whether their causality-based information is given in advance or not. We further analyze each category in terms of the formalization of different models, ranging from the Markov Decision Process (MDP), Partially Observed Markov Decision Process (POMDP), Multi-Arm Bandits (MAB), and Dynamic Treatment
    
[^106]: 更好的扩散模型进一步改进对抗训练

    Better Diffusion Models Further Improve Adversarial Training. (arXiv:2302.04638v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.04638](http://arxiv.org/abs/2302.04638)

    本文证明了更好的扩散模型可以进一步提高对抗训练的性能，通过采用最新的扩散模型，我们训练的模型仅使用生成的数据就在RobustBench上实现了最佳性能，并在CIFAR-10和CIFAR-100数据集上分别提高了$+4.58\%$和$+8.03\%$的性能。

    

    众所周知，由去噪扩散概率模型（DDPM）产生的数据可以提高对抗性训练。随着扩散模型的迅速发展，一个自然的问题出现了：更好的扩散模型是否能够进一步改进对抗训练？本文采用了最新的扩散模型，与DDPM相比，该模型具有更高的效率（约$\sim20$个采样步骤）和更低的图像质量（更低的FID分数），证明了更好的扩散模型可以进一步提高对抗训练。我们训练的模型仅使用生成的数据（没有外部数据集）在RobustBench上实现了最佳性能。在$\ell_\infty$-norm威胁模型下，当$\epsilon=8/255$时，我们的模型在CIFAR-10和CIFAR-100上分别达到$70.69\%$和$42.67\%$的鲁棒准确度，即分别比以前的最先进模型提高了$+4.58\%$和$+8.03\%$。在$\ell_2$-norm威胁模型下，当$\epsilon=128/255$时，我们的模型在CIFAR-10上可以达到$84.86\%$的准确率，提高了$+4.44\%$。

    It has been recognized that the data generated by the denoising diffusion probabilistic model (DDPM) improves adversarial training. After two years of rapid development in diffusion models, a question naturally arises: can better diffusion models further improve adversarial training? This paper gives an affirmative answer by employing the most recent diffusion model which has higher efficiency ($\sim 20$ sampling steps) and image quality (lower FID score) compared with DDPM. Our adversarially trained models achieve state-of-the-art performance on RobustBench using only generated data (no external datasets). Under the $\ell_\infty$-norm threat model with $\epsilon=8/255$, our models achieve $70.69\%$ and $42.67\%$ robust accuracy on CIFAR-10 and CIFAR-100, respectively, i.e. improving upon previous state-of-the-art models by $+4.58\%$ and $+8.03\%$. Under the $\ell_2$-norm threat model with $\epsilon=128/255$, our models achieve $84.86\%$ on CIFAR-10 ($+4.44\%$). These results also beat
    
[^107]: 语言模型训练数据提取的技巧总结

    Bag of Tricks for Training Data Extraction from Language Models. (arXiv:2302.04460v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.04460](http://arxiv.org/abs/2302.04460)

    本文总结了一些技巧用于改进语言模型训练数据提取，提出了在文本生成和文本排名中可以使用的技巧，实验证明这些技巧对于提高训练数据提取的效果非常重要。

    

    随着语言模型的不断研究，隐私保护变得越来越重要。因此，训练数据提取作为潜在的评估隐私泄露的工具变得非常关键。然而，由于这项任务的困难程度，目前现有的大多数方法仍然不够有效。本文提出了一些技巧用于改进训练数据提取，我们对公开可用的数据集进行了实验。由于大多数现有的提取方法使用生成然后排序的流程（例如，生成潜在的训练数据文本，然后根据特定的标准对它们进行排序），因此我们的研究重点在于文本生成和文本排名的技巧。（例如，采样策略和令牌级标准）。实验结果表明，一些之前被忽视的技巧对于训练数据提取的成功非常关键。基于GPT-Neo 1.3B的评估结果，我们提出的技巧优于现有的方法。

    With the advance of language models, privacy protection is receiving more attention. Training data extraction is therefore of great importance, as it can serve as a potential tool to assess privacy leakage. However, due to the difficulty of this task, most of the existing methods are proof-of-concept and still not effective enough. In this paper, we investigate and benchmark tricks for improving training data extraction using a publicly available dataset. Because most existing extraction methods use a pipeline of generating-then-ranking, i.e., generating text candidates as potential training data and then ranking them based on specific criteria, our research focuses on the tricks for both text generation (e.g., sampling strategy) and text ranking (e.g., token-level criteria). The experimental results show that several previously overlooked tricks can be crucial to the success of training data extraction. Based on the GPT-Neo 1.3B evaluation results, our proposed tricks outperform the b
    
[^108]: SSL的相互作用：增强、归纳偏差和泛化

    The SSL Interplay: Augmentations, Inductive Bias, and Generalization. (arXiv:2302.02774v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.02774](http://arxiv.org/abs/2302.02774)

    本论文通过分析数据增强、网络架构和训练算法的相互作用，研究了预训练和下游任务的泛化性能，并为SSL从业人员提供了一些见解。

    

    自监督学习（SSL）已成为一种无需监督即可从原始数据中学习表示的强大框架。然而，在实践中，工程师面临着调整优化器和训练过程中表示塌陷等问题。这些挑战促使我们需要一种理论来阐明数据增强、网络架构和训练算法的选择之间的复杂相互作用。我们在一个理论友好的设置下，通过对预训练和下游任务的泛化性能进行精确分析，研究了这种相互作用，并强调了我们理论得出的SSL从业者的一些见解。

    Self-supervised learning (SSL) has emerged as a powerful framework to learn representations from raw data without supervision. Yet in practice, engineers face issues such as instability in tuning optimizers and collapse of representations during training. Such challenges motivate the need for a theory to shed light on the complex interplay between the choice of data augmentation, network architecture, and training algorithm. We study such an interplay with a precise analysis of generalization performance on both pretraining and downstream tasks in a theory friendly setup, and highlight several insights for SSL practitioners that arise from our theory.
    
[^109]: 低位视觉变压器的无振荡量化

    Oscillation-free Quantization for Low-bit Vision Transformers. (arXiv:2302.02210v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.02210](http://arxiv.org/abs/2302.02210)

    研究发现，可学习比例因子加剧了权重振荡，本文提出三种技术以解决这个问题，并在多个基准测试上显著提高了模型的性能。

    

    量化意识训练的一个不良副作用是权重振荡，其中量化权重经常在两个量化级别之间跳动，导致训练不稳定和子优化的最终模型。本研究发现，可学习的比例因子——在量化中广泛使用的$\textit{de facto}$设置——加剧了权重振荡。本研究研究了可学习比例因子与量化权重振荡之间的关系，并以ViT为案例来说明发现和解决方法。此外，我们还发现自注意力层中量化权重的$\textit{query}$和$\textit{key}$之间的相互依存使ViT容易受到振荡的影响。因此，我们相应地提出了三种技术：统计权重量化（$\rm StatsQ$）以改善量化鲁棒性，与普遍使用的可学习比例因子方法相比；置信度引导的退火（$\rm CGA$）在训练期间冻结具有$\textit{高置信度}$的权重，以减少权重振荡；以及相互依赖权重的均衡（$\rm IWEqual$），以有效处理相互依赖问题。我们的实验表明，我们提出的方法相比于最先进的方法在多个基准测试上显著提高了模型的性能。

    Weight oscillation is an undesirable side effect of quantization-aware training, in which quantized weights frequently jump between two quantized levels, resulting in training instability and a sub-optimal final model. We discover that the learnable scaling factor, a widely-used $\textit{de facto}$ setting in quantization aggravates weight oscillation. In this study, we investigate the connection between the learnable scaling factor and quantized weight oscillation and use ViT as a case driver to illustrate the findings and remedies. In addition, we also found that the interdependence between quantized weights in $\textit{query}$ and $\textit{key}$ of a self-attention layer makes ViT vulnerable to oscillation. We, therefore, propose three techniques accordingly: statistical weight quantization ($\rm StatsQ$) to improve quantization robustness compared to the prevalent learnable-scale-based method; confidence-guided annealing ($\rm CGA$) that freezes the weights with $\textit{high confi
    
[^110]: 将语言模型与图像进行联系以处理多模态信息

    Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.13823](http://arxiv.org/abs/2301.13823)

    该论文提出一种有效的方法，将仅处理文本的语言模型与图像进行联系，使其能够处理任意交错的图像和文本数据，并生成与检索图像交错的自由形式文本。该方法在环境相关的图像检索和多模态对话等任务中表现十分优异，是利用预训练语言模型解决视觉场景下交互问题的有效解决方案。

    

    我们提出了一种有效的方法，将预训练的仅文本语言模型与视觉领域联系起来，使其能够处理任意交错的图像和文本数据，并生成与检索图像交错的文本。我们利用从大规模文本预训练中学到的语言模型的能力，例如上下文学习和自由形式文本生成。我们保持语言模型冻结，并微调输入和输出线性层以实现跨模态交互。这使得我们的模型能够处理任意交错的图像和文本输入，并生成与检索图像交错的自由形式文本。我们在环境相关的图像检索和多模态对话等任务中取得了强大的零-shot表现，并展示了引人入胜的交互能力。我们的方法适用于任何现成的语言模型，为在视觉场景下利用预训练语言模型提供了一个有效且通用的解决方案。

    We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
    
[^111]: 双价值网络在逆向合成规划中的应用

    Retrosynthetic Planning with Dual Value Networks. (arXiv:2301.13755v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.13755](http://arxiv.org/abs/2301.13755)

    PDVN是一种新的在线训练算法，它在逆向合成规划中利用双价值网络优化完整的路线，成功率和效率上均优于现有方法。

    

    逆向合成旨在从商业上可得的起始材料中找到合成目标分子的路线，是药物发现和材料设计中的关键任务。最近，基于机器学习的单步反应预测器与多步规划器的组合已经取得了令人鼓舞的结果。然而，单步预测器大多数情况下是离线训练的，只优化单步的准确性，而不考虑完整的路线。本文提出了一种新的在线训练算法PDVN，通过使用树形MDP来优化完整的路线，利用强化学习（RL）改善单步预测器。在PDVN中，我们构建了两个单独的价值网络，分别预测分子的可合成性和成本。为了保持单步预测器的准确性，我们设计了一个双分支网络结构。

    Retrosynthesis, which aims to find a route to synthesize a target molecule from commercially available starting materials, is a critical task in drug discovery and materials design. Recently, the combination of ML-based single-step reaction predictors with multi-step planners has led to promising results. However, the single-step predictors are mostly trained offline to optimize the single-step accuracy, without considering complete routes. Here, we leverage reinforcement learning (RL) to improve the single-step predictor, by using a tree-shaped MDP to optimize complete routes. Specifically, we propose a novel online training algorithm, called Planning with Dual Value Networks (PDVN), which alternates between the planning phase and updating phase. In PDVN, we construct two separate value networks to predict the synthesizability and cost of molecules, respectively. To maintain the single-step accuracy, we design a two-branch network structure for the single-step predictor. On the widely
    
[^112]: 新的分布水平度量方法：弃用$\Delta$DP，实现人口统计特征的公平机器学习

    Retiring $\Delta$DP: New Distribution-Level Metrics for Demographic Parity. (arXiv:2301.13443v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13443](http://arxiv.org/abs/2301.13443)

    本文提出两种新公平度量方法：概率密度函数曲线之间区域（ABPC）和累积密度函数曲线之间区域（ABCC），实现人口统计特征的公平机器学习

    

    人口统计特征的平等对待是机器学习中最广泛认可的公平度量标准。为实现人口统计平等，许多研究致力于追求常用度量方法$\Delta DP$。然而，本文揭示了公平度量方法$\Delta DP$存在固有缺陷：i) 零值$\Delta DP$不保证民族统计平等的零违规，ii) $\Delta DP$值随不同分类阈值变化。为此，提出了两种新公平度量方法——概率密度函数曲线之间区域（ABPC）和累积密度函数曲线之间区域（ABCC），以精确测量不同民族统计群体预测概率分布之间的差异。

    Demographic parity is the most widely recognized measure of group fairness in machine learning, which ensures equal treatment of different demographic groups. Numerous works aim to achieve demographic parity by pursuing the commonly used metric $\Delta DP$. Unfortunately, in this paper, we reveal that the fairness metric $\Delta DP$ can not precisely measure the violation of demographic parity, because it inherently has the following drawbacks: i) zero-value $\Delta DP$ does not guarantee zero violation of demographic parity, ii) $\Delta DP$ values can vary with different classification thresholds. To this end, we propose two new fairness metrics, Area Between Probability density function Curves (ABPC) and Area Between Cumulative density function Curves (ABCC), to precisely measure the violation of demographic parity at the distribution level. The new fairness metrics directly measure the difference between the distributions of the prediction probability for different demographic group
    
[^113]: 神经代理通信推动语言普遍规律的出现：以语序/格标交换为例证

    Communication Drives the Emergence of Language Universals in Neural Agents: Evidence from the Word-order/Case-marking Trade-off. (arXiv:2301.13083v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.13083](http://arxiv.org/abs/2301.13083)

    本文研究了神经代理语言学习与通信框架，成功复制了语序/格标交换这一广泛存在的语言普遍规律，实现了更真实的语言演变模拟，并帮助我们更好地理解影响语言普遍规律的认知和社会因素。

    

    在基于神经代理的语言演变和变化的模拟中，人工学习者的行为通常与人类学习者不同，这常被归因于这些学习者缺乏适当的认知偏见。然而，也有人提出更自然的语言学习和使用环境可能导致更类似于人类的结果。本文研究了这种后一种说法，重点关注语序/格标交换，一种被广泛证明的语言普遍规律，这种规律在模拟中被证明尤其困难。我们提出了一个新的神经代理语言学习和通信框架（NeLLCom），其中说话和听取的代理首先通过监督学习学习一种小语言，然后通过强化学习对其进行优化以进行沟通。紧密遵循早期人类实验的设置，我们成功复制了这种新框架下的交换，而不是在代理中硬编码特定的偏见。我们认为，这是发展更真实的语言演变模拟和更好地理解影响语言普遍规律的认知和社会因素的重要一步。

    Artificial learners often behave differently from human learners in the context of neural agent-based simulations of language emergence and change. A common explanation is the lack of appropriate cognitive biases in these learners. However, it has also been proposed that more naturalistic settings of language learning and use could lead to more human-like results. We investigate this latter account focusing on the word-order/case-marking trade-off, a widely attested language universal that has proven particularly hard to simulate. We propose a new Neural-agent Language Learning and Communication framework (NeLLCom) where pairs of speaking and listening agents first learn a miniature language via supervised learning, and then optimize it for communication via reinforcement learning. Following closely the setup of earlier human experiments, we succeed in replicating the trade-off with the new framework without hard-coding specific biases in the agents. We see this as an essential step to
    
[^114]: 使用来自成对或$K$元比较的人类反馈的规范强化学习

    Principled Reinforcement Learning with Human Feedback from Pairwise or $K$-wise Comparisons. (arXiv:2301.11270v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11270](http://arxiv.org/abs/2301.11270)

    该论文提供了带有人类反馈强化学习问题的理论框架，证明了最大似然估计在Bradley-Terry-Luce和Plackett-Luce模型下收敛。此外，提出了在一定的覆盖假设下，基于悲观估计的MLE提供了性能更好的策略。在证明了真实MLE和以成对比较形式替代的备选MLE都可以在PL模型下收敛的同时，也表明了真实MLE的高效性。这些结果为RLHF算法提供了新的见解，并统一了RLHF问题和IRL问题。

    

    我们为带有人类反馈的强化学习问题提供了一个理论框架。我们的分析表明，当真实奖励函数为线性函数时，最大似然估计（MLE）在Bradley-Terry-Luce（BTL）模型和Plackett-Luce（PL）模型下均收敛。然而，我们发现当基于学得的奖励模型训练策略时，MLE会失败，而基于悲观估计的MLE在一定的覆盖假设下提供性能更好的策略。此外，我们证明在PL模型下，真实MLE和将$k$元比较拆分为成对比较的备选MLE都收敛。而真实MLE是渐近更为高效的。我们的结果验证了现有RLHF算法（如InstructGPT）的实验成功，并为算法设计提供了新的见解。此外，我们的结果统一了RLHF问题和最大熵反向强化学习(IRL)问题，并为其提供了第一个样本复杂度界。

    We provide a theoretical framework for Reinforcement Learning with Human Feedback (RLHF). Our analysis shows that when the true reward function is linear, the widely used maximum likelihood estimator (MLE) converges under both the Bradley-Terry-Luce (BTL) model and the Plackett-Luce (PL) model. However, we show that when training a policy based on the learned reward model, MLE fails while a pessimistic MLE provides policies with improved performance under certain coverage assumptions. Additionally, we demonstrate that under the PL model, the true MLE and an alternative MLE that splits the $K$-wise comparison into pairwise comparisons both converge. Moreover, the true MLE is asymptotically more efficient. Our results validate the empirical success of existing RLHF algorithms in InstructGPT and provide new insights for algorithm design. Furthermore, our results unify the problem of RLHF and max-entropy Inverse Reinforcement Learning (IRL), and provide the first sample complexity bound fo
    
[^115]: 关于上下文学习的综述

    A Survey on In-context Learning. (arXiv:2301.00234v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.00234](http://arxiv.org/abs/2301.00234)

    本文调查和总结了上下文学习(ICL)的进展和挑战，ICL已成为自然语言处理(NLP)的新范式，探索ICL以评估和推广大型语言模型(LLM)的能力已成为一种新趋势。本文提出了ICL的正式定义，并总结了高级技术，最后讨论了ICL的挑战以及进一步研究的潜在方向。

    

    随着大型语言模型（LLM）的能力不断增强，上下文学习（ICL）已成为自然语言处理（NLP）的新范式，在其中LLM仅基于加入少量示例的上下文进行预测。探索ICL以评估和推广LLM的能力已成为一种新趋势。本文旨在调查和总结ICL的进展和挑战。我们首先提出ICL的正式定义，并澄清其与相关研究的关系。然后，我们组织和讨论高级技术，包括训练策略、演示设计策略以及相关分析。最后，我们讨论了ICL的挑战，并提供了进一步研究的潜在方向。我们希望我们的工作可以鼓励更多的研究，揭示ICL的工作原理并改进ICL。

    With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.
    
[^116]: 使用语言模型提示进行推理：一项调查

    Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09597](http://arxiv.org/abs/2212.09597)

    本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。

    

    推理作为复杂问题解决的重要能力，可以为医疗诊断、谈判等各种实际应用提供后端支持。本文对使用语言模型提示进行推理的前沿研究进行了综合调查。我们介绍了研究成果的比较和总结，并提供了系统资源以帮助初学者。我们还讨论了新兴推理能力出现的潜在原因，并突出了未来的研究方向。资源可在 https://github.com/zjunlp/Prompt4ReasoningPapers 上获取（定期更新）。

    Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
    
[^117]: 用于整数线性规划的局部分支松弛启发式算法

    Local Branching Relaxation Heuristics for Integer Linear Programs. (arXiv:2212.08183v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.08183](http://arxiv.org/abs/2212.08183)

    本论文提出了一种用于整数线性规划的 LB-RELAX 及其变体的启发式算法，通过使用线性规划松弛值来选择邻域，加快了计算速度而保持了较传统的 LB 算法相同的求解质量。实验结果表明，该算法达到了最佳的任意时间性能。

    

    大邻域搜索是解决组合优化问题的一种流行的启发式算法。在本文中，我们专注于设计用于整数线性规划的有效和高效的局部分支启发式算法。我们提出的启发式算法 LB-RELAX 及其变体使用线性规划松弛值来选择邻域，相较于传统的 LB 算法，其计算速度更快但效果相同。实验证明，LB-RELAX 算法及其变体在提高求解质量的同时，也实现了最佳的任意时间性能。

    Large Neighborhood Search (LNS) is a popular heuristic algorithm for solving combinatorial optimization problems (COP). It starts with an initial solution to the problem and iteratively improves it by searching a large neighborhood around the current best solution. LNS relies on heuristics to select neighborhoods to search in. In this paper, we focus on designing effective and efficient heuristics in LNS for integer linear programs (ILP) since a wide range of COPs can be represented as ILPs. Local Branching (LB) is a heuristic that selects the neighborhood that leads to the largest improvement over the current solution in each iteration of LNS. LB is often slow since it needs to solve an ILP of the same size as input. Our proposed heuristics, LB-RELAX and its variants, use the linear programming relaxation of LB to select neighborhoods. Empirically, LB-RELAX and its variants compute as effective neighborhoods as LB but run faster. They achieve state-of-the-art anytime performance on se
    
[^118]: 基于闭合形式策略改进算子的离线强化学习

    Offline Reinforcement Learning with Closed-Form Policy Improvement Operators. (arXiv:2211.15956v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15956](http://arxiv.org/abs/2211.15956)

    本文提出了基于行为约束的离线强化学习中的闭合形式策略改进算子，该算子将行为策略建模为高斯混合，利用LogSumExp的下界和Jensen不等式克服了优化困难，能有效处理实际数据集中的异构策略。

    

    行为约束策略优化已被证明是解决离线强化学习问题的一种成功的范式。本文提出了我们的闭合形式策略改进算子。我们发现，行为约束自然地激励了使用一阶泰勒近似，从而导致了策略目标的线性近似。此外，由于实际数据集通常由异构策略收集而来，我们将行为策略建模为高斯混合，并利用LogSumExp的下界和Jensen不等式克服了引起优化困难的问题，从而得到了闭式策略改进算子。我们使用我们的新颖策略改进算子来实例化离线RL算法，并在实验中展示了它们的效果。

    Behavior constrained policy optimization has been demonstrated to be a successful paradigm for tackling Offline Reinforcement Learning. By exploiting historical transitions, a policy is trained to maximize a learned value function while constrained by the behavior policy to avoid a significant distributional shift. In this paper, we propose our closed-form policy improvement operators. We make a novel observation that the behavior constraint naturally motivates the use of first-order Taylor approximation, leading to a linear approximation of the policy objective. Additionally, as practical datasets are usually collected by heterogeneous policies, we model the behavior policies as a Gaussian Mixture and overcome the induced optimization difficulties by leveraging the LogSumExp's lower bound and Jensen's Inequality, giving rise to a closed-form policy improvement operator. We instantiate offline RL algorithms with our novel policy improvement operators and empirically demonstrate their e
    
[^119]: 条件文本生成中的奖励博弈

    Reward Gaming in Conditional Text Generation. (arXiv:2211.08714v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08714](http://arxiv.org/abs/2211.08714)

    在条件文本生成中，使用强化学习（RL）进行训练时，噪声、自然发生的虚假相关性和协变量漂移可能会导致不良模式被错误地赋予高奖励值，这可能会导致奖励博弈，需要解决。

    

    为了使条件文本生成模型的输出与所需行为相一致，越来越多的关注点在于使用从人类注释中学习的奖励函数以强化学习（RL）训练模型。在这个框架下，我们确定了三种常见情况，即由噪声引起的虚假相关性、自然发生的虚假相关性和协变量漂移，其中高奖励被错误地分配给不良模式。我们表明，即使学习到的度量在训练奖励函数所使用的数据分布上表现良好，不良模式在文本生成模型的RL训练过程中仍有可能被放大。尽管RL或安全社区已经开始讨论奖励博弈，但在这篇讨论中，我们希望使用具体的条件文本生成示例，重点介绍自然语言生成（NLG）社区中的奖励博弈，并讨论可能的修复措施和未来的研究方向。

    To align conditional text generation model outputs with desired behaviors, there has been an increasing focus on training the model using reinforcement learning (RL) with reward functions learned from human annotations. Under this framework, we identify three common cases where high rewards are incorrectly assigned to undesirable patterns: noise-induced spurious correlation, naturally occurring spurious correlation, and covariate shift. We show that even though learned metrics achieve high performance on the distribution of the data used to train the reward function, the undesirable patterns may be amplified during RL training of the text generation model. While there has been discussion about reward gaming in the RL or safety community, in this discussion piece, we would like to highlight reward gaming in the natural language generation (NLG) community using concrete conditional text generation examples and discuss potential fixes and areas for future work.
    
[^120]: 机器翻译的衡量指标与人工评分相符，在同时翻译中也适用

    MT Metrics Correlate with Human Ratings of Simultaneous Speech Translation. (arXiv:2211.08633v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.08633](http://arxiv.org/abs/2211.08633)

    本文表明了离线指标与人工连续评分之间存在良好的相关性，可以在同时翻译模式中可靠地用于机器翻译的评估，从而减轻了大规模人工评估的需要。

    

    在人工评分和离线机器翻译评估指标（如BLEU、chrF2、BertScore和COMET）之间有几个元评估研究。这些指标已经用于同时语音翻译（SST）的评估，但它们与最近收集的连续评分（CR）的SST的人工评分之间的相关性不清楚。在本文中，我们利用提交给IWSLT 2022年英德SST任务的候选系统的评估，并对CR和上述指标进行了广泛的相关性分析。我们的研究表明，离线指标与CR存在良好的相关性，并可以可靠地用于评估同时翻译模式下的机器翻译，但对测试集大小有一定限制。我们得出结论称，鉴于当前SST的质量水平，这些指标可以用作CR的代理，减轻了大规模人工评估的需要。另外，我们观察到许多指标的相关性在不同的测试集下发生变化。

    There have been several meta-evaluation studies on the correlation between human ratings and offline machine translation (MT) evaluation metrics such as BLEU, chrF2, BertScore and COMET. These metrics have been used to evaluate simultaneous speech translation (SST) but their correlations with human ratings of SST, which has been recently collected as Continuous Ratings (CR), are unclear. In this paper, we leverage the evaluations of candidate systems submitted to the English-German SST task at IWSLT 2022 and conduct an extensive correlation analysis of CR and the aforementioned metrics. Our study reveals that the offline metrics are well correlated with CR and can be reliably used for evaluating machine translation in simultaneous mode, with some limitations on the test set size. We conclude that given the current quality levels of SST, these metrics can be used as proxies for CR, alleviating the need for large scale human evaluation. Additionally, we observe that correlations of the m
    
[^121]: 迭代自回归：提高低延迟语音增强模型的新技巧

    Iterative autoregression: a novel trick to improve your low-latency speech enhancement model. (arXiv:2211.01751v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.01751](http://arxiv.org/abs/2211.01751)

    本研究提出了一种简单而有效的替代技术，用于训练自回归低延迟语音增强模型，该方法在不同的架构和训练场景下均能带来稳定的改进。

    

    流式模型是实时语音增强工具的重要组成部分。流式模式限制语音增强模型仅能使用极少量未来信息作为上下文。因此，低延迟流式设置通常被视为一项具有挑战性的任务，这对模型的质量有着显著的负面影响。然而，流式生成的顺序性提供了自回归的自然可能性，即在进行当前预测时利用以前的预测。常规训练自回归模型的方法是教师强制，但其主要缺点在于训练-推理不匹配可能会导致大幅度的质量降级。在本研究中，我们提出了一种简单但有效的替代技术，用于训练自回归低延迟语音增强模型。我们证明了这种方法在不同的架构和训练场景下都能带来稳定的改进。

    Streaming models are an essential component of real-time speech enhancement tools. The streaming regime constrains speech enhancement models to use only a tiny context of future information. As a result, the low-latency streaming setup is generally considered a challenging task and has a significant negative impact on the model's quality. However, the sequential nature of streaming generation offers a natural possibility for autoregression, that is, utilizing previous predictions while making current ones. The conventional method for training autoregressive models is teacher forcing, but its primary drawback lies in the training-inference mismatch that can lead to a substantial degradation in quality. In this study, we propose a straightforward yet effective alternative technique for training autoregressive low-latency speech enhancement models. We demonstrate that the proposed approach leads to stable improvement across diverse architectures and training scenarios.
    
[^122]: 用中点 Mixup 在多视角数据中可证明学习多元特征

    Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup. (arXiv:2210.13512v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13512](http://arxiv.org/abs/2210.13512)

    本文提出了一种基于中点 Mixup 的多视角数据学习方法，相比于传统经验风险最小化方法能够更好地学习每个类别的所有特征，具有更好的泛化和鲁棒性。

    

    Mixup 是一种数据增强技术，依赖于使用数据点和标签的随机凸组合进行训练。近年来，Mixup 已成为训练最先进的图像分类模型的标准基元，因为它在泛化和鲁棒性方面比经验风险最小化有明显的优势。在这项工作中，我们试图从特征学习的角度解释一些这种成功。我们关注的分类问题是，每个类别可能具有多个相关特征（或视图），可用于正确预测类别。我们的主要理论结果表明，在具有每类两个特征的一类非平凡数据分布中，使用经验风险最小化训练 2 层卷积网络可能会导致几乎所有类别只学习一个特征，而使用 Mixup 的特定实例进行训练可以成功地学习每个类别的两个特征。

    Mixup is a data augmentation technique that relies on training using random convex combinations of data points and their labels. In recent years, Mixup has become a standard primitive used in the training of state-of-the-art image classification models due to its demonstrated benefits over empirical risk minimization with regards to generalization and robustness. In this work, we try to explain some of this success from a feature learning perspective. We focus our attention on classification problems in which each class may have multiple associated features (or views) that can be used to predict the class correctly. Our main theoretical results demonstrate that, for a non-trivial class of data distributions with two features per class, training a 2-layer convolutional network using empirical risk minimization can lead to learning only one feature for almost all classes while training with a specific instantiation of Mixup succeeds in learning both features for every class. We also show
    
[^123]: 关于因果位置-尺度噪声模型的可识别性和估计

    On the Identifiability and Estimation of Causal Location-Scale Noise Models. (arXiv:2210.09054v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.09054](http://arxiv.org/abs/2210.09054)

    本文研究了一类异方差噪声模型，发现除特殊情况外因果方向是可识别的。提出了两个估计器，能够准确识别因果效应。

    

    本文研究了一类位置-尺度或异方差噪声模型（LSNMs），在其中，效应$ Y $可以被写成是因果$ X $和与$ X $无关的噪声源$ N $的函数，但可能被因果$ X $缩放为一个正函数$ g（X）$，即$ Y = f（X）+ g（X）N $。尽管模型类别非常广泛，但我们发现除一些特殊情况外，因果方向是可识别的。为了在实证上验证这些理论发现，我们提出了两个LSNMs的估计器：一个基于（非线性）特征映射的估计器和一个基于神经网络的估计器。两者将$ Y $给定$ X $的条件分布建模为由其自然参数参数化的高斯分布。当特征映射被正确规定时，我们证明我们的估计器是联合凸的，并且是因果效应识别任务的一致估计器。尽管神经网络没有继承这些保证，但它可以拟合任意复杂性的函数，并达到最先进的性能。

    We study the class of location-scale or heteroscedastic noise models (LSNMs), in which the effect $Y$ can be written as a function of the cause $X$ and a noise source $N$ independent of $X$, which may be scaled by a positive function $g$ over the cause, i.e., $Y = f(X) + g(X)N$. Despite the generality of the model class, we show the causal direction is identifiable up to some pathological cases. To empirically validate these theoretical findings, we propose two estimators for LSNMs: an estimator based on (non-linear) feature maps, and one based on neural networks. Both model the conditional distribution of $Y$ given $X$ as a Gaussian parameterized by its natural parameters. When the feature maps are correctly specified, we prove that our estimator is jointly concave, and a consistent estimator for the cause-effect identification task. Although the the neural network does not inherit those guarantees, it can fit functions of arbitrary complexity, and reaches state-of-the-art performance
    
[^124]: 时态一致的变换器用于视频生成

    Temporally Consistent Transformers for Video Generation. (arXiv:2210.02396v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.02396](http://arxiv.org/abs/2210.02396)

    本文介绍了一种名为TECO的生成模型，其可以大大提高视频生成的长期时间一致性。作者提出了3个难度不同的视频数据集以评估该模型和现有模型在时间一致性方面的局限性。

    

    为了生成准确的视频，算法必须理解世界中的时空依赖关系。目前的算法虽然能够准确预测短期内的内容，但往往存在时间上的不一致性。当生成的内容消失后再次出现时，模型会发明不同的内容。尽管存在这种严重的限制，但目前尚不存在复杂数据的已建立基准来对具有长期时间依赖性的视频生成进行严格评估。在本文中，我们通过渲染通过 3D 场景的流程迷宫，Minecraft 世界和室内扫描，策划了三个具有长程依赖的视频数据集。我们对当前模型进行全面评估，并观察到它们在时间一致性方面的局限性。此外，我们引入了时态一致的变换器（TECO），一种生成模型，它显著提高了长期一致性，同时也减少了采样时间。通过将其输入序列压缩为更少的项，

    To generate accurate videos, algorithms have to understand the spatial and temporal dependencies in the world. Current algorithms enable accurate predictions over short horizons but tend to suffer from temporal inconsistencies. When generated content goes out of view and is later revisited, the model invents different content instead. Despite this severe limitation, no established benchmarks on complex data exist for rigorously evaluating video generation with long temporal dependencies. In this paper, we curate 3 challenging video datasets with long-range dependencies by rendering walks through 3D scenes of procedural mazes, Minecraft worlds, and indoor scans. We perform a comprehensive evaluation of current models and observe their limitations in temporal consistency. Moreover, we introduce the Temporally Consistent Transformer (TECO), a generative model that substantially improves long-term consistency while also reducing sampling time. By compressing its input sequence into fewer e
    
[^125]: 基于特征学习的多样性隐私保护反事实解释方法

    Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations. (arXiv:2209.13446v5 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.13446](http://arxiv.org/abs/2209.13446)

    本文提出了一种基于特征学习的多样性隐私保护反事实解释方法，可以有效地处理反事实约束条件，并为私有解释模型做出贡献。

    

    可解释的机器学习致力于理解长期以来因缺乏可解释性而声名狼藉的复杂黑盒子系统的推理过程。其中一种蓬勃发展的方法是通过反事实解释，为用户提供如何改变结果的建议。反事实样本不仅必须反驳黑盒分类器的原始预测，还必须满足各种实际应用的约束条件，其中多样性是关键约束之一但仍较少讨论。虽然多样化的反事实很理想，但同时解决其他约束条件在计算上具有挑战性。此外，共享反事实数据存在越来越多的隐私问题。因此，本文提出了一种基于特征学习的框架来有效地处理反事实约束条件，并为私有解释模型的有限资料库做出贡献。我们在各种数据集和黑盒模型上展示了我们的框架的灵活性和有效性。

    Interpretable machine learning seeks to understand the reasoning process of complex black-box systems that are long notorious for lack of explainability. One flourishing approach is through counterfactual explanations, which provide suggestions on what a user can do to alter an outcome. Not only must a counterfactual example counter the original prediction from the black-box classifier but it should also satisfy various constraints for practical applications. Diversity is one of the critical constraints that however remains less discussed. While diverse counterfactuals are ideal, it is computationally challenging to simultaneously address some other constraints. Furthermore, there is a growing privacy concern over the released counterfactual data. To this end, we propose a feature-based learning framework that effectively handles the counterfactual constraints and contributes itself to the limited pool of private explanation models. We demonstrate the flexibility and effectiveness of o
    
[^126]: 大型语言模型能否像人类一样知道别人的信仰？

    Do Large Language Models know what humans know?. (arXiv:2209.01515v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.01515](http://arxiv.org/abs/2209.01515)

    本文测试了语言暴露假说，评估语言模型是否能够像人类一样感知书面文本中角色的知识状态。结果显示，语言模型虽然超过了偶然行为，但表现不如人类，暗示着仅靠语言暴露难以完全解释人类这种认知能力。

    

    人类能够了解他人的信仰。然而，尚不清楚这种能力在多大程度上是源于天生的生物禀赋，还是来源于儿童发育过程中的经验积累，尤其是通过接受描述他人心理状态的语言而获得的经验。我们通过评估暴露于大量人类语言的模型是否显示对书面段落中角色暗示的知识状态敏感性来测试语言暴露假说的可行性。在预注册的分析中，我们向人类参与者和大型语言模型GPT-3提供了语言版本的误信任务。两者都敏感于他人的信仰，而语言模型显著超过了偶然行为，但它的表现不如人类，并且没有解释他们行为的全部范围--尽管语言模型接受了比一个人一生中接受的语言更多的语言。这表明，虽然从语言暴露中进行的统计学习可能在一定程度上解释了人类如何做到这一点，但它并不能完全解释人类的行为。

    Humans can attribute beliefs to others. However, it is unknown to what extent this ability results from an innate biological endowment or from experience accrued through child development, particularly exposure to language describing others' mental states. We test the viability of the language exposure hypothesis by assessing whether models exposed to large quantities of human language display sensitivity to the implied knowledge states of characters in written passages. In pre-registered analyses, we present a linguistic version of the False Belief Task to both human participants and a Large Language Model, GPT-3. Both are sensitive to others' beliefs, but while the language model significantly exceeds chance behavior, it does not perform as well as the humans, nor does it explain the full extent of their behavior -- despite being exposed to more language than a human would in a lifetime. This suggests that while statistical learning from language exposure may in part explain how huma
    
[^127]: 一些监督是必须的：通过认知不确定性度量在强化学习中引入神谕策略

    Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics. (arXiv:2208.10533v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.10533](http://arxiv.org/abs/2208.10533)

    本文提出一种名为评判置信度引导探索的方法，用于将现有的神谕策略纳入标准的演员-评论家强化学习算法中，以提高探索效率。在不确定性高时，该方法会将神谕策略的行动作为建议纳入学习方案中，而在不确定性低时忽略它。

    

    强化学习的固有问题是通过随机行动探索环境，其中很大一部分可能是无效的。相反，可以通过使用现有的（先前学习的或硬编码的）神谕策略、离线数据或演示来改善探索。但在使用神谕策略的情况下，如何最大化学习样本效率地将神谕经验融入到学习策略中可能不清楚。本文提出了一种名为评判置信度引导探索（Critic Confidence Guided Exploration，CCGE）的方法，用于将这样的神谕策略纳入标准的演员-评论家强化学习算法中。具体而言，当不确定性高时，CCGE以神谕策略的行动为建议，并将此信息纳入学习方案中，而当不确定性低时忽略它。CCGE对不确定性估计方法不加区分，并且我们证明它与现有算法相当。

    An inherent problem of reinforcement learning is performing exploration of an environment through random actions, of which a large portion can be unproductive. Instead, exploration can be improved by initializing the learning policy with an existing (previously learned or hard-coded) oracle policy, offline data, or demonstrations. In the case of using an oracle policy, it can be unclear how best to incorporate the oracle policy's experience into the learning policy in a way that maximizes learning sample efficiency. In this paper, we propose a method termed Critic Confidence Guided Exploration (CCGE) for incorporating such an oracle policy into standard actor-critic reinforcement learning algorithms. More specifically, CCGE takes in the oracle policy's actions as suggestions and incorporates this information into the learning scheme when uncertainty is high, while ignoring it when the uncertainty is low. CCGE is agnostic to methods of estimating uncertainty, and we show that it is equa
    
[^128]: Claim-Dissector: 一款联合重排和真实性预测的可解释的事实核查系统

    Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction. (arXiv:2207.14116v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.14116](http://arxiv.org/abs/2207.14116)

    Claim-Dissector是一款联合重排和真实性预测的可解释的事实核查系统，可以识别与声明相关的证据，并确定声明的真实性。该系统的个人贡献以及证据所支持或反驳声明的贡献都可以被识别。

    

    我们提出了Claim-Dissector，一种针对事实核查和分析的新型潜变量模型，给出一个声明和一组检索到的证据，联合学习识别：（i）与给定声明相关的证据，（ii）声明的真实性。我们建议以可解释的方式解开每个证据的相关性概率及其对最终真实性概率的影响-最终真实性概率与每个证据相关性概率的线性整合成比例。通过这种方式，可以识别出每个证据对最终预测概率的个人贡献。在每个证据的相关性概率中，我们的模型还可以进一步区分每个相关证据是支持（S）还是反驳（R）声明。这样可以量化S/R概率对最终结论的贡献或检测有异议的证据。尽管我们的系统具有可解释性，但在FEVER竞赛中，其结果与最先进的结果相当。

    We present Claim-Dissector: a novel latent variable model for fact-checking and analysis, which given a claim and a set of retrieved evidences jointly learns to identify: (i) the relevant evidences to the given claim, (ii) the veracity of the claim. We propose to disentangle the per-evidence relevance probability and its contribution to the final veracity probability in an interpretable way -- the final veracity probability is proportional to a linear ensemble of per-evidence relevance probabilities. In this way, the individual contributions of evidences towards the final predicted probability can be identified. In per-evidence relevance probability, our model can further distinguish whether each relevant evidence is supporting (S) or refuting (R) the claim. This allows to quantify how much the S/R probability contributes to the final verdict or to detect disagreeing evidence.  Despite its interpretable nature, our system achieves results competitive with state-of-the-art on the FEVER 
    
[^129]: 一种用于实时视觉处理的超低功耗TinyML系统

    An Ultra-low Power TinyML System for Real-time Visual Processing at Edge. (arXiv:2207.04663v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2207.04663](http://arxiv.org/abs/2207.04663)

    本文提出了一种超低功耗TinyML系统，采用微型骨干网络构建高效CNN模型，再由特别设计的神经协处理器与MCU相连，将所有特征和权重储存在芯片上，完全消除芯片外存储器访问的延迟和功耗。此系统具有相当的准确性并实现了创纪录的160mW超低功耗。

    

    TinyML是在资源和功耗严格限制的系统上执行AI工作负载的重要且具有挑战性的领域。本文首先提出了一种极其微小的骨干网络，用于构建用于各种视觉任务的高效CNN模型。然后，一种特别设计的神经协处理器（NCP）与MCU相连，构建了一种超低功耗的TinyML系统，该系统将所有特征和权重存储在芯片上，并完全消除了芯片外存储器访问的延迟和功耗。此外，还提出了一种应用特定的指令集，以实现敏捷开发和快速部署。广泛的实验表明，基于我们的模型、NCP和指令集的TinyML系统在实现30FPS的目标检测和识别时具有相当的准确性，并实现了创纪录的160mW超低功耗。演示视频在\url{https://www.youtube.com/watch?v=mIZPxtJ-9EY}上可用。

    Tiny machine learning (TinyML), executing AI workloads on resource and power strictly restricted systems, is an important and challenging topic. This brief firstly presents an extremely tiny backbone to construct high efficiency CNN models for various visual tasks. Then, a specially designed neural co-processor (NCP) is interconnected with MCU to build an ultra-low power TinyML system, which stores all features and weights on chip and completely removes both of latency and power consumption in off-chip memory access. Furthermore, an application specific instruction-set is further presented for realizing agile development and rapid deployment. Extensive experiments demonstrate that the proposed TinyML system based on our model, NCP and instruction set yields considerable accuracy and achieves a record ultra-low power of 160mW while implementing object detection and recognition at 30FPS. The demo video is available on \url{https://www.youtube.com/watch?v=mIZPxtJ-9EY}.
    
[^130]: 非线性独立分量分析的可辨识性：稀疏性及其它

    On the Identifiability of Nonlinear ICA: Sparsity and Beyond. (arXiv:2206.07751v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.07751](http://arxiv.org/abs/2206.07751)

    本文提出一个新的方法，考虑混合过程的假设，即结构稀疏性，来实现非线性ICA的可识别性，无需辅助变量。

    

    非线性独立分量分析旨在从其可观测的非线性混合中恢复出潜在独立分量。如何使非线性ICA模型可辨识直到某些平凡不确定性是无监督学习中的一个长期问题。最近的突破是将源的标准独立性假设重新定义为在某些辅助变量（例如类标签和/或域/时间索引）给定的条件独立性，作为弱监督或归纳偏置。然而，具有无条件先验的非线性ICA无法从这些发展中受益。我们探索了一条替代路径，并仅考虑混合过程的假设，例如结构稀疏性。我们展示了在这些约束的具体实例下，独立的潜在分量可以从其非线性混合中辨识出来，达到非平凡的非线性ICA可识别性，而无需辅助变量。

    Nonlinear independent component analysis (ICA) aims to recover the underlying independent latent sources from their observable nonlinear mixtures. How to make the nonlinear ICA model identifiable up to certain trivial indeterminacies is a long-standing problem in unsupervised learning. Recent breakthroughs reformulate the standard independence assumption of sources as conditional independence given some auxiliary variables (e.g., class labels and/or domain/time indexes) as weak supervision or inductive bias. However, nonlinear ICA with unconditional priors cannot benefit from such developments. We explore an alternative path and consider only assumptions on the mixing process, such as Structural Sparsity. We show that under specific instantiations of such constraints, the independent latent sources can be identified from their nonlinear mixtures up to a permutation and a component-wise transformation, thus achieving nontrivial identifiability of nonlinear ICA without auxiliary variable
    
[^131]: 从感知到程序：正则化、过度参数化和摊销

    From Perception to Programs: Regularize, Overparameterize, and Amortize. (arXiv:2206.05922v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.05922](http://arxiv.org/abs/2206.05922)

    该论文探索了正则化、过度参数化、摊销等技术来提高神经符号程序合成的稳定性，用于将感知到的输入处理为离散抽象并符号化处理为程序。

    

    为了将归纳推理与感知能力相结合，我们开发了一种神经符号程序合成技术，其中感知输入首先通过神经网络解析成低维可解释表示，然后再由合成程序处理。我们探索了几种松弛问题的技术，并使用梯度下降共同学习所有模块：多任务学习；摊销推理；过度参数化；以及一种可区别的策略来惩罚冗长的程序。 总体而言，这个工具箱提高了梯度引导的程序搜索的稳定性，并提出了学习如何将输入感知为离散抽象，并如何符号化处理这些抽象的程序的方式。

    Toward combining inductive reasoning with perception abilities, we develop techniques for neurosymbolic program synthesis where perceptual input is first parsed by neural nets into a low-dimensional interpretable representation, which is then processed by a synthesized program. We explore several techniques for relaxing the problem and jointly learning all modules end-to-end with gradient descent: multitask learning; amortized inference; overparameterization; and a differentiable strategy for penalizing lengthy programs. Collectedly this toolbox improves the stability of gradient-guided program search, and suggests ways of learning both how to perceive input as discrete abstractions, and how to symbolically process those abstractions as programs.
    
[^132]: 空中对话与导航：一种基于自然语言的无人机导航方法

    Aerial Vision-and-Dialog Navigation. (arXiv:2205.12219v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.12219](http://arxiv.org/abs/2205.12219)

    本文介绍了一种名为AVDN的无人机导航方法，该方法通过自然语言对话来控制无人机的导航，为解决无人机控制的人体负担、多任务操作、以及残疾人和手部占用者控制无人机的难题提供了新思路。

    

    能够与人类进行对话并遵循自然语言指令是智能无人机（即无人机）的关键能力。它可以减轻人们一直握着控制器的负担，允许同时进行多项任务，使残疾人或手部占用者更容易控制无人机。为此，我们介绍了一种名为Aerial Vision-and-Dialog Navigation（AVDN）的无人机导航方法，通过自然语言对话来控制无人机的导航。我们构建了一个带有连续逼真环境的无人机模拟器，并收集了一组新的AVDN数据集，其中包括超过3,000个记录的导航轨迹以及指挥官和跟随者之间的异步人际对话。指挥官提供初始导航指令，并根据需要提供进一步的指导，而跟随者在模拟器中导航无人机并在需要时提问。在数据收集过程中，跟随者对无人机的视觉观察也被记录下来。基于AVDN数据集，我们研究了无人机导航的任务。

    The ability to converse with humans and follow natural language commands is crucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can relieve people's burden of holding a controller all the time, allow multitasking, and make drone control more accessible for people with disabilities or with their hands occupied. To this end, we introduce Aerial Vision-and-Dialog Navigation (AVDN), to navigate a drone via natural language conversation. We build a drone simulator with a continuous photorealistic environment and collect a new AVDN dataset of over 3k recorded navigation trajectories with asynchronous human-human dialogs between commanders and followers. The commander provides initial navigation instruction and further guidance by request, while the follower navigates the drone in the simulator and asks questions when needed. During data collection, followers' attention on the drone's visual observation is also recorded. Based on the AVDN dataset, we study the tasks of aerial
    
[^133]: 受限单调神经网络

    Constrained Monotonic Neural Networks. (arXiv:2205.11775v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.11775](http://arxiv.org/abs/2205.11775)

    本文针对实际应用场景需要的单调性，提出了一种通过在层中的一部分神经元中采用原始激活函数，同时在另一部分采用其点对称反射来解决构建单调深度神经网络的方法。实验证明，该方法的精度符合要求。

    

    深度神经网络越来越流行，可以逼近从嘈杂数据中得出的任意函数，但在推广过程中需要解释这些模型并对它们施加额外的限制，其中单调性是最受实际应用场景需要的属性之一，并且是该论文的重点。最早构建单调全连接神经网络的方法是将其权重约束为非负，同时采用单调激活函数。不幸的是，该方法无法与常用的非饱和激活函数（如ReLU，ELU，SELU等）一起使用，因为它只能逼近凸函数。我们通过在层中的一部分神经元中采用原始激活函数，同时在另一部分采用其点对称反射来解决这个问题。我们的实验证明，采用这种方法建立单调深度神经网络的精度与其他方法相当甚至更好，同时满足单调性约束。

    Deep neural networks are becoming increasingly popular in approximating arbitrary functions from noisy data. But wider adoption is being hindered by the need to explain such models and to impose additional constraints on them. Monotonicity constraint is one of the most requested properties in real-world scenarios and is the focus of this paper. One of the oldest ways to construct a monotonic fully connected neural network is to constrain its weights to be non-negative while employing a monotonic activation function. Unfortunately, this construction does not work with popular non-saturated activation functions such as ReLU, ELU, SELU etc, as it can only approximate convex functions. We show this shortcoming can be fixed by employing the original activation function for a part of the neurons in the layer, and employing its point reflection for the other part. Our experiments show this approach of building monotonic deep neural networks have matching or better accuracy when compared to ot
    
[^134]: 监督多保真度超参数配置竞赛

    Supervising the Multi-Fidelity Race of Hyperparameter Configurations. (arXiv:2202.09774v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.09774](http://arxiv.org/abs/2202.09774)

    本文介绍了一种贝叶斯优化方法DyHPO，它能够学习动态决定哪个超参数配置在所有可行的配置中进行下一步训练。实验表明DyHPO相比于最先进的超参数优化方法有显著的优越性。

    

    近期，多保真度超参数优化技术（HPO）作为调整深度学习方法的一种有希望的方向而出现，然而，现有方法存在超参数配置的HPO预算分配不足的问题。本文提出了一种贝叶斯优化方法DyHPO，该方法可以学习决定在所有可行的配置中动态赛跑中进一步训练哪个超参数配置。我们提出了一种新的深度高斯过程内核，它嵌入了学习曲线动力学，以及一个包含多预算信息的获得函数。我们通过包含50个数据集（表格、图像和自然语言处理）和不同架构（MLP、CNN/NAS、RNN）的大规模实验证明了DyHPO相比于最先进的超参数优化方法的显著优越性。

    Multi-fidelity (gray-box) hyperparameter optimization techniques (HPO) have recently emerged as a promising direction for tuning Deep Learning methods. However, existing methods suffer from a sub-optimal allocation of the HPO budget to the hyperparameter configurations. In this work, we introduce DyHPO, a Bayesian Optimization method that learns to decide which hyperparameter configuration to train further in a dynamic race among all feasible configurations. We propose a new deep kernel for Gaussian Processes that embeds the learning curve dynamics, and an acquisition function that incorporates multi-budget information. We demonstrate the significant superiority of DyHPO against state-of-the-art hyperparameter optimization methods through large-scale experiments comprising 50 datasets (Tabular, Image, NLP) and diverse architectures (MLP, CNN/NAS, RNN).
    
[^135]: 通过挖掘邻域和社区影响的方式，在时间网络中进行归纳表示学习

    Inductive Representation Learning in Temporal Networks via Mining Neighborhood and Community Influences. (arXiv:2110.00267v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2110.00267](http://arxiv.org/abs/2110.00267)

    该论文提出了一种新的归纳式网络表示学习方法MNCI，通过在时间网络中挖掘邻域和社区影响来生成节点嵌入，实验证明该方法具有更好的性能。

    

    网络表示学习旨在为网络中的每个节点生成嵌入，其可促进节点分类和链接预测等下游机器学习任务。目前的工作主要集中在转导式网络表示学习，即生成固定节点嵌入，这不适合实际应用。因此，我们提出了一种新的归纳式网络表示学习方法MNCI，通过在时间网络中挖掘邻域和社区影响来生成节点嵌入。我们提出了一个聚合函数，将邻域影响与社区影响相结合，以在任何时间生成节点嵌入。我们在几个真实数据集上进行了广泛的实验，并将MNCI与几种最先进的基线方法进行了比较，包括节点分类和网络可视化等各种任务。实验结果表明，MNCI比基线方法具有更好的性能。

    Network representation learning aims to generate an embedding for each node in a network, which facilitates downstream machine learning tasks such as node classification and link prediction. Current work mainly focuses on transductive network representation learning, i.e. generating fixed node embeddings, which is not suitable for real-world applications. Therefore, we propose a new inductive network representation learning method called MNCI by mining neighborhood and community influences in temporal networks. We propose an aggregator function that integrates neighborhood influence with community influence to generate node embeddings at any time. We conduct extensive experiments on several real-world datasets and compare MNCI with several state-of-the-art baseline methods on various tasks, including node classification and network visualization. The experimental results show that MNCI achieves better performance than baselines.
    
[^136]: 鲁棒预测系统动力学的解缠生成模型

    Disentangled Generative Models for Robust Prediction of System Dynamics. (arXiv:2108.11684v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.11684](http://arxiv.org/abs/2108.11684)

    本文研究了解缠生成模型用于预测系统动力学，将领域参数与生成模型的潜在空间中的动力学分离，取得了良好的泛化和长期稳定性。

    

    深度神经网络在动力系统预测中越来越受到关注，但是超出分布范围的泛化和长期稳定性仍然具有挑战性。本文将动力系统的领域参数视为数据生成过程的变异因素，借鉴监督解缠和因果分解的思想，旨在将领域参数与生成模型的潜在空间中的动力学分离。在我们的实验中，我们模拟了相空间和视频序列的动态，并进行了严格的超出分布价值评估。结果表明，解缠的 VAEs 更适应于在训练数据中不存在的领域参数空间。同时，解缠可以改善视频序列中现有最先进模型的长期和超出分布预测能力。

    Deep neural networks have become increasingly of interest in dynamical system prediction, but out-of-distribution generalization and long-term stability still remains challenging. In this work, we treat the domain parameters of dynamical systems as factors of variation of the data generating process. By leveraging ideas from supervised disentanglement and causal factorization, we aim to separate the domain parameters from the dynamics in the latent space of generative models. In our experiments we model dynamics both in phase space and in video sequences and conduct rigorous OOD evaluations. Results indicate that disentangled VAEs adapt better to domain parameters spaces that were not present in the training data. At the same time, disentanglement can improve the long-term and out-of-distribution predictions of state-of-the-art models in video sequences.
    
[^137]: 基于Rational Closure的可撤销RDFS

    Defeasible RDFS via Rational Closure. (arXiv:2007.07573v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2007.07573](http://arxiv.org/abs/2007.07573)

    本文介绍了如何将Rational Closure整合到RDFS中，提出了可撤销的$\rho df_\bot$，使其仍然保持三元语言的语法，并通过引入新规则进行简单的扩展，以声明不兼容的实体。

    

    在非单调逻辑领域，Rational Closure（RC）的概念被认为是一种重要的方法。近年来，在描述逻辑（DLs）的上下文中，RC在 OWL 2 的主要成分（类和角色）的语义Web标准本体语言下得到了更广泛的应用。在本文中，我们展示了如何将RC集成到三元语言RDFS中，RDFS是与OWL2一起构成主要的语义Web本体语言的两个标准之一。为此，我们从RDFS背后的$\rho df$开始，然后将其扩展为$\rho df_\bot$，允许声明两个实体是不兼容的。最终，我们通过典型的RC构造提出了可撤销的$\rho df_\bot$。我们方法的主要特点是：（i）与大多数在单调RDFS之上添加一个额外的非单调规则层的方法不同，可撤销的$\rho df_\bot$仍然是语法上的三元语言，并且是通过引入一些新规则来简单地扩展$\rho df_\bot$。

    In the field of non-monotonic logics, the notion of Rational Closure (RC) is acknowledged as a prominent approach. In recent years, RC has gained even more popularity in the context of Description Logics (DLs), the logic underpinning the semantic web standard ontology language OWL 2, whose main ingredients are classes and roles. In this work, we show how to integrate RC within the triple language RDFS, which together with OWL2 are the two major standard semantic web ontology languages. To do so, we start from $\rho df$, which is the logic behind RDFS, and then extend it to $\rho df_\bot$, allowing to state that two entities are incompatible. Eventually, we propose defeasible $\rho df_\bot$ via a typical RC construction. The main features of our approach are: (i) unlike most other approaches that add an extra non-monotone rule layer on top of monotone RDFS, defeasible $\rho df_\bot$ remains syntactically a triple language and is a simple extension of $\rho df_\bot$ by introducing some n
    

