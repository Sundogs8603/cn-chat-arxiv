# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [DualCross: Cross-Modality Cross-Domain Adaptation for Monocular BEV Perception.](http://arxiv.org/abs/2305.03724) | DualCross是一个跨模态和跨领域的自适应框架，旨在使单目BEV感知更加鲁棒，并实现了跨领域跨传感器感知和适应。 |
| [^2] | [ChatGPT and Works Scholarly: Best Practices and Legal Pitfalls in Writing with AI.](http://arxiv.org/abs/2305.03722) | 本文讨论了学者如何在撰写论文时使用人工智能，提出了相应的最佳实践标准，同时探究了使用AI是否违反版权或属于公平使用的安全港，为学术写作与人工智能共存提供了法律与学术基础的探讨。 |
| [^3] | [Training Is Everything: Artificial Intelligence, Copyright, and Fair Training.](http://arxiv.org/abs/2305.03720) | 本文讨论了在版权法中公正使用版权作品进行人工智能训练的问题，探讨了许可和补偿等方法，强调平衡AI开发者、版权所有者和整个社会的利益和权利需要一种细致而灵活的方法。 |
| [^4] | [Governance of the AI, by the AI, and for the AI.](http://arxiv.org/abs/2305.03719) | 本文讨论AI的治理问题，提出了一种治理模式，即由人类和AI合作，各自发挥其各自的优势，以实现对该技术的有效治理。 |
| [^5] | [Mining bias-target Alignment from Voronoi Cells.](http://arxiv.org/abs/2305.03691) | 本文提出了一种偏见无关的方法来减轻深度神经网络中偏差的影响，在目标类别上量化“偏见对齐/不对齐”，以避免通过网络传播偏见-目标对齐信息。 |
| [^6] | [Predicting COVID-19 and pneumonia complications from admission texts.](http://arxiv.org/abs/2305.03661) | 本文提出一种基于入院记录进行患者并发症风险评估的新方法，并使用Longformer神经网络计算患者的风险评分。该方法在多个欧洲医院的患者数据中表现出优异的性能，具有泛化能力和其他优势。 |
| [^7] | [Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models.](http://arxiv.org/abs/2305.03660) | 该研究提出了一种检索增强的方法，利用对比预训练的视觉语言模型的多模态对齐嵌入来检索相应的候选放射学文本，并使用通用领域生成模型来生成报告，可抑制虚构的生成，实现更好的临床指标。 |
| [^8] | [Improving LaCAM for Scalable Eventually Optimal Multi-Agent Pathfinding.](http://arxiv.org/abs/2305.03632) | 本研究提出了可扩展最终优化多智能体路径规划能力中的LaCAM算法，改进了其任何时期版本LaCAM*和后继生成，成功减少规划工作量并取得了相应实验效果。 |
| [^9] | [Data Curation for Image Captioning with Text-to-Image Generative Models.](http://arxiv.org/abs/2305.03610) | 本论文探究了通过数据整合来提高图像字幕质量的方案，并使用现有资源训练了比基准模型更好的模型。 |
| [^10] | [Human Attention-Guided Explainable Artificial Intelligence for Computer Vision Models.](http://arxiv.org/abs/2305.03601) | 本研究探讨了将人类注意力知识嵌入基于显著性的可解释人工智能方法用于计算机视觉模型中的效果。开发了基于梯度和人类注意力引导的两种解释方法，结果表明对于目标检测模型，人类注意力引导的方法在解释的准确性和合理性方面优于基于梯度的方法。 |
| [^11] | [NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports.](http://arxiv.org/abs/2305.03598) | 本文提出了一种面向临床试验报告的自然语言推理模型，通过检索支持事实来确定自然语言陈述和CTR之间的推理关系。 |
| [^12] | [Now It Sounds Like You: Learning Personalized Vocabulary On Device.](http://arxiv.org/abs/2305.03584) | 这项研究提出了一种称为“生词扩展”的技术，通过个性化的“生词适配器”来学习个性化词汇，提高了生词覆盖率并显著提高了模型准确度。 |
| [^13] | [In-context Learning as Maintaining Coherency: A Study of On-the-fly Machine Translation Using Large Language Models.](http://arxiv.org/abs/2305.03573) | 本文探究了机器翻译的在语境学习现象，提出了在语境学习应该是保持结果与其语境具有连贯性的生成任务。实验结果表明，将具有长期连贯性的样例用于翻译可以提高翻译性能。 |
| [^14] | [Learn how to Prune Pixels for Multi-view Neural Image-based Synthesis.](http://arxiv.org/abs/2305.03572) | LeHoPP是一种有效的方法进行输入像素修剪，可以在不重新训练基于图像的渲染网络的情况下，在合成质量和像素率之间达到良好平衡，提高图像合成的效率。 |
| [^15] | [Black-box Prompt Tuning with Subspace Learning.](http://arxiv.org/abs/2305.03518) | 本文提出了基于子空间学习的黑匣子提示调优（BSL）框架，通过元学习在相似任务的集合中识别子空间，以改善黑匣子提示调优的通用性，在下游任务和LLMs上都能够持续达到竞争性的表现水平。 |
| [^16] | [Few-shot Domain-Adaptive Visually-fused Event Detection from Text.](http://arxiv.org/abs/2305.03517) | 本文提出了一种基于视觉融合和领域自适应的事件检测方法，可以用少量标记数据训练并且适应于新领域。 |
| [^17] | [Learning Decision Trees with Gradient Descent.](http://arxiv.org/abs/2305.03515) | 本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。 |
| [^18] | [Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment.](http://arxiv.org/abs/2305.03510) | 本文提出了一个通过翻译对齐的方式实现参数高效、跨语言的迁移学习框架，实验结果显示该框架能够显著减少多语言之间的性能差异。 |
| [^19] | [Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion.](http://arxiv.org/abs/2305.03509) | Diffusion Explainer是第一个可交互的可视化工具，用于解释稳定扩散如何将文本提示转化为图像，用户可以通过动画和交互元素流畅地在多个抽象级别之间过渡，从而更好地理解提示对图像生成的影响。 |
| [^20] | [Training Natural Language Processing Models on Encrypted Text for Enhanced Privacy.](http://arxiv.org/abs/2305.03497) | 本研究提出了一种在加密文本数据上训练NLP模型的方法，以缓解数据隐私问题，同时保持与非加密数据训练模型相似的性能。 |
| [^21] | [Automatic Prompt Optimization with "Gradient Descent" and Beam Search.](http://arxiv.org/abs/2305.03495) | 在基于大型语言模型的自然语言处理中，使用梯度下降和 beam search 的自动提示优化方法可以自动改进提示，提高性能。 |
| [^22] | [Generative Steganography Diffusion.](http://arxiv.org/abs/2305.03472) | 提出了一种称为“生成式隐写扩散”（GSD）的新型生成式隐写方案，利用一种反演扩散模型可以100％地恢复隐藏的秘密数据，并实现了高质量的隐写图像生成。 |
| [^23] | [Multi-View Graph Representation Learning for Answering Hybrid Numerical Reasoning Question.](http://arxiv.org/abs/2305.03458) | 本文提出了一种基于多视角图形表示学习的混合型数值推理问答模型，能够从表格视角、关系视角和数值视角捕捉混合数据之间的粒度关系和空间结构信息，能够有效提高数值推理任务的准确率。 |
| [^24] | [Advances on the classification of radio image cubes.](http://arxiv.org/abs/2305.03435) | 本研究综述了机器智能技术在射电图像中的应用，特别关注了射电星系的形态分类问题。 |
| [^25] | [Towards Applying Powerful Large AI Models in Classroom Teaching: Opportunities, Challenges and Prospects.](http://arxiv.org/abs/2305.03433) | 本文探索了利用大型语言模型实现教师与学生的互动，提高教学质量的潜力和挑战。提出了一个统一的框架来处理多样化的教育数据集，处理长时间的对话以及压缩信息以更好地完成更多的下游任务。 |
| [^26] | [Adaptive Graph Convolutional Subspace Clustering.](http://arxiv.org/abs/2305.03414) | 本研究提出了自适应图卷积子空间聚类方法（AGCSC），将特征提取方法和系数矩阵约束相结合，实现更真实的子空间结构揭示，并在大量实验中证明了其有效性和效率。 |
| [^27] | [Assessing Trustworthiness of Autonomous Systems.](http://arxiv.org/abs/2305.03411) | 随着自主系统在社会中变得越来越普遍，评估其可信度是必要的。本文提出了一个可以用作自主系统可信度评估框架的过程概述。 |
| [^28] | [GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering.](http://arxiv.org/abs/2305.03403) | 介绍了一种名为CAAFE的上下文感知自动特征工程方法，它利用大型语言模型根据数据集描述生成更多具有语义意义的特征，能够提高大多数数据集的性能，平均ROC AUC表现提高至0.822。 |
| [^29] | [Compressing audio CNNs with graph centrality based filter pruning.](http://arxiv.org/abs/2305.03391) | 本文提出一种基于图中心性的滤波器剪枝压缩框架，用于音频卷积神经网络，该方法可减少95％的计算和97％的存储。 |
| [^30] | [Explaining the ghosts: Feminist intersectional XAI and cartography as methods to account for invisible labour.](http://arxiv.org/abs/2305.03376) | AI需要大量的“无形劳动”，女性主义交叉XAI和制图是解释隐形劳动的方法。 |
| [^31] | [Towards Feminist Intersectional XAI: From Explainability to Response-Ability.](http://arxiv.org/abs/2305.03375) | 本文呼吁在计算和概念化方法中采用批判性方法，关注交叉的女性主义视角，并探讨在HCXAI研究和设计中实现女性主义交叉视角所需的响应能力和关注边缘化视角。 |
| [^32] | [A Survey on Offline Model-Based Reinforcement Learning.](http://arxiv.org/abs/2305.03360) | 本文是一篇关于离线基于模型的强化学习最新进展的综述性文章，讨论了该领域的最新概念、方法和挑战。文献综述介绍了当前离线基于模型的强化学习领域的关键文献，并重点讨论了如何解决分布变化问题。该领域面临的关键挑战也被讨论和总结。 |
| [^33] | [From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base.](http://arxiv.org/abs/2305.03356) | 本文提出了一种名为解析-执行-优化（Parse-Execute-Refine）的范式，通过向知识库问题应答模型演示执行中间推理步骤，可以提高复杂推理的能力。 |
| [^34] | [A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness.](http://arxiv.org/abs/2305.03355) | 本研究对当前最先进的数据集压缩方法进行了全面评估，发现其存在隐私风险并可能放大模型的不公平性，提供了大规模的基准测试框架。 |
| [^35] | [MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic.](http://arxiv.org/abs/2305.03353) | 本文利用动态认知逻辑在自然语言处理模型中探讨了理解心智理论的方法。虽然GPT-4表现出改进的能力，但需要进一步提高。 |
| [^36] | [Biophysical Cybernetics of Directed Evolution and Eco-evolutionary Dynamics.](http://arxiv.org/abs/2305.03340) | 本论文提出了一种新的方法，以部分可观察的马尔可夫过程模型为基础，分析生态演化动力学中生态和个体基因型/表型类型复杂性的不确定性。 |
| [^37] | [QCRI at SemEval-2023 Task 3: News Genre, Framing and Persuasion Techniques Detection using Multilingual Models.](http://arxiv.org/abs/2305.03336) | 本文介绍了QCRI在SemEval-2023任务3中使用多语言模型成功排名前三的结果，并展示了去解决分析和验证在线新闻传播的难题。 |
| [^38] | [Human-centered trust framework: An HCI perspective.](http://arxiv.org/abs/2305.03306) | 本论文提出了以人为中心的信任框架，帮助非专家实现在人工智能设计中充分利用用户信任的潜力；相关研究发现了某些计算机科学和人工智能话语中的用户信任误解，并进行了有效性评估，该研究的主要贡献是为抵制设计以技术为中心的易受攻击的交互方式的趋势提供了指导。 |
| [^39] | [Open Information Extraction via Chunks.](http://arxiv.org/abs/2305.03299) | 本文提出了一种句子作为块序列的方法，将块的跨度识别为元组关系和参数，采用Chunk-OIE进行元组提取并取得了最先进的结果。 |
| [^40] | [BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks.](http://arxiv.org/abs/2305.03289) | 本文介绍了BadSAM，这是对图像分割基础模型的首次后门攻击，该攻击将导致定制化SAM模型面临更加严峻的安全挑战。 |
| [^41] | [Low-Resource Multi-Granularity Academic Function Recognition Based on Multiple Prompt Knowledge.](http://arxiv.org/abs/2305.03287) | 本研究提出了 Mix Prompt Tuning（MPT）方法，通过将手动提示模板与自动学习的连续提示模板相结合，提高多粒度学术功能识别任务的性能，并减轻对注释数据的依赖。 |
| [^42] | [Composite Motion Learning with Task Control.](http://arxiv.org/abs/2305.03286) | 该论文介绍了一种使用多个判别器在类 GAN 设置中直接学习来自多个参考动作的特定身体部位的解耦动作的深度学习方法，以实现复合和任务驱动的动作控制，同时考虑了多个任务的奖励和训练一个单一的多目标控制策略。 |
| [^43] | [Semantic Segmentation using Vision Transformers: A survey.](http://arxiv.org/abs/2305.03273) | 本文介绍了使用视觉变换器（ViT）进行语义分割的不同架构，讨论了ViT在分块划分方案方面的局限性，并回顾和比较了不同ViT架构的性能表现。ViT架构的崛起在计算机视觉任务中替代传统的卷积神经网络的趋势不断增强。 |
| [^44] | [Bayesian Reinforcement Learning with Limited Cognitive Load.](http://arxiv.org/abs/2305.03263) | 本文综述了限制认知负荷的贝叶斯强化学习的最新算法和理论成果，并讨论了这些思想如何应用于研究认知和行为科学中的问题。 |
| [^45] | [Clothes Grasping and Unfolding Based on RGB-D Semantic Segmentation.](http://arxiv.org/abs/2305.03259) | 本论文针对机器人辅助穿衣过程中的服装抓取和展开问题，提出了一种基于RGB-D语义分割的双向分形交叉融合网络，可以识别可抓取区域提供更多抓取可能性，并且使用RGB和深度数据融合来考虑图像信息。 |
| [^46] | [Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts.](http://arxiv.org/abs/2305.03237) | 本文提出了一个上下文感知的OOD意图检测框架（Caro），用于模拟OOD意图检测任务中的多轮对话上下文，并在提取稳健的表示时删除与意图检测无关的多余信息。Caro在多个标准数据集上表现出最先进的性能，并超越了先前方法。 |
| [^47] | [A Survey on Out-of-Distribution Detection in NLP.](http://arxiv.org/abs/2305.03236) | 这篇论文首次综述了最新的OOD检测方法在自然语言处理中的应用。根据算法使用的数据，将方法分成三类，并介绍了相关数据集、应用和度量方法。 |
| [^48] | [Near-realtime Facial Animation by Deep 3D Simulation Super-Resolution.](http://arxiv.org/abs/2305.03216) | 该论文提出了一种基于神经网络的3D模拟超分辨率框架，能够高效、逼真地增强低成本、实时物理模拟产生的面部表现，使其接近于具有更高分辨率和准确物理建模的参考质量离线模拟器。 |
| [^49] | [LLM2Loss: Leveraging Language Models for Explainable Model Diagnostics.](http://arxiv.org/abs/2305.03212) | LLM2Loss利用语言模型为黑盒模型提供可解释的模型诊断，通过提取数据点的语义特征，揭示导致模型失败和偏差的因素。 |
| [^50] | [Investigating Lexical Sharing in Multilingual Machine Translation for Indian Languages.](http://arxiv.org/abs/2305.03207) | 本文研究了印度语系多语言机器翻译中的词汇共享问题，探索了数据采样和词汇量之间的翻译性能权衡，以及字母转写是否有助于促进跨脚本概括，发现字母转写对跨脚本翻译效果没有明显改进，对于较低资源的语言，多语言机器翻译模型训练在原始脚本上似乎已经能够抵抗脚本差异。 |
| [^51] | [Gpt-4: A Review on Advancements and Opportunities in Natural Language Processing.](http://arxiv.org/abs/2305.03195) | GPT-4是OpenAI开发的第四代GPT语言模型，具有超过1万亿的模型规模、更好的多语言能力和改进的语境理解和推理能力。它有望应用于聊天机器人、个人助理、语言翻译、文本摘要和问答系统，但也存在计算要求、数据要求和道德问题等挑战和限制。 |
| [^52] | [Smaller3d: Smaller Models for 3D Semantic Segmentation Using Minkowski Engine and Knowledge Distillation Methods.](http://arxiv.org/abs/2305.03188) | 本文提出了一种基于知识蒸馏技术的稀疏张量在3D深度学习中的小型模型，通过采用不同的损失函数，在保持性能的情况下减小模型的大小并获得优于现有模型的结果。 |
| [^53] | [POET: A Self-learning Framework for PROFINET Industrial Operations Behaviour.](http://arxiv.org/abs/2305.03175) | 提出了一种名为POET的基于Python的框架，能自学习PROFINET网络的基础设施信息，包括PLCs、现场设备和人机界面，利用知识图来优化异常检测和分类结果，能有效提高工业网络安全。 |
| [^54] | [New Adversarial Image Detection Based on Sentiment Analysis.](http://arxiv.org/abs/2305.03173) | 本文提出了一种新的对抗性样本检测器，通过使用情感分析并检测对神经网络的隐藏层特征图造成的影响逐渐显现来检测对抗性样本，克服了现有技术的局限，且实验结果表明其在识别最新的对抗性攻击方面优于现有技术。 |
| [^55] | [Towards Invertible Semantic-Preserving Embeddings of Logical Formulae.](http://arxiv.org/abs/2305.03143) | 本研究提出了一种基于 Graph VAE 的深度学习模型ISPE，能够在连续空间中保持语义的嵌入逻辑公式，并且该方法是可逆的。在语句补全和推理等任务上的表现优于现有方法。 |
| [^56] | [ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review.](http://arxiv.org/abs/2305.03123) | 本文研究关注ChatGPT面临的可持续性、隐私、数字鸿沟和伦理问题，提出了SPADE评估的必要性，并给出了缓解和建议。 |
| [^57] | [HAISTA-NET: Human Assisted Instance Segmentation Through Attention.](http://arxiv.org/abs/2305.03105) | 该论文提出了一种通过人类辅助实例分割方法，称为HAISTA-NET，增强了现有的实例分割网络，引入了人类指定的部分边界地图，以生成更精确的分割掩模。 |
| [^58] | [Unsupervised anomaly localization in high-resolution breast scans using deep pluralistic image completion.](http://arxiv.org/abs/2305.03098) | 本文提出了一种利用深度多元图像完成方法进行乳腺扫描异常定位的无监督方法，该方法通过探索完成的多元性来提高评估标准的精度。 |
| [^59] | [Federated Ensemble-Directed Offline Reinforcement Learning.](http://arxiv.org/abs/2305.03097) | 本文开发了一种名为FEDORA的联邦集成指导的离线强化学习算法，通过集成学习方法提炼客户群体的集体智慧，显著优于其他方法，包括在合并的数据汇总中进行离线强化学习，在各种复杂的连续控制环境和真实世界数据集中进行了实验。 |
| [^60] | [Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation.](http://arxiv.org/abs/2305.03088) | 本文提出了一种新的方法来共同建模“要问什么”和“如何问”的问题，在不明确答案的设置中非常实用。 |
| [^61] | [Neuro-symbolic model for cantilever beams damage detection.](http://arxiv.org/abs/2305.03063) | 本文提出了一种神经符号模型用于悬臂梁损伤检测，该模型通过将卷积网络的处理能力与逻辑查询交互控制相结合，不仅能够准确检测损伤，而且还能够提供解释和定位，使其在操作条件下更可靠和可信。 |
| [^62] | [Leveraging gradient-derived metrics for data selection and valuation in differentially private training.](http://arxiv.org/abs/2305.02942) | 研究如何在隐私增强技术下，利用梯度信息识别训练中感兴趣的数据样本进行数据选择和估价。 |
| [^63] | [Hierarchical Transformer for Scalable Graph Learning.](http://arxiv.org/abs/2305.02866) | 本文提出了分层可扩展图Transformer (HSGT)用于解决图表示学习中的规模问题和上下文信息捕获不足问题，通过构建多尺度图分层结构，HSGT实现了对大型图的快速和内存高效处理，并在基准数据集上展现了卓越的性能表现。 |
| [^64] | [Automated Code generation for Information Technology Tasks in YAML through Large Language Models.](http://arxiv.org/abs/2305.02783) | 这项研究提出了一种名为Ansible Wisdom的自然语言转Ansible-YAML代码的工具，可自动化生成Ansible脚本，提高IT自动化生产力，并相比现有技术达到或更好的性能水平。 |
| [^65] | [Multiplicity Boost Of Transit Signal Classifiers: Validation of 69 New Exoplanets Using The Multiplicity Boost of ExoMiner.](http://arxiv.org/abs/2305.02470) | 该论文介绍了一种可提高探测行星信号分类器性能的框架，称为多重性增强分类器，基于现有的分类器并使用多重性信息来验证69个新的系外行星。 |
| [^66] | [Reverse Engineering of Temporal Queries Mediated by LTL Ontologies.](http://arxiv.org/abs/2305.01248) | 本文研究了基于时间戳数据的LTL正片段制定的查询反向工程问题，旨在构建一个将给定答案与非答案分开的查询语言，并考虑了LTL本体介导的情况。 |
| [^67] | [LSTM-based Preceding Vehicle Behaviour Prediction during Aggressive Lane Change for ACC Application.](http://arxiv.org/abs/2305.01095) | 该论文提出了一种基于LSTM的ACC系统，可以学习过去的驾驶经验，适应和预测新情况，并且在模拟驾驶环境中表现良好。 |
| [^68] | [Exploiting CNNs for Semantic Segmentation with Pascal VOC.](http://arxiv.org/abs/2304.13216) | 本文在Pascal VOC数据集上进行了语义分割的研究，使用了FCN作为基准并改进了其问题，最终发现使用ResNet进行迁移学习的表现最佳。 |
| [^69] | [Unsupervised Domain Transfer for Science: Exploring Deep Learning Methods for Translation between LArTPC Detector Simulations with Differing Response Models.](http://arxiv.org/abs/2304.12858) | 本文提出了一种基于最近在成对图像转换技术上的进展的，完全无监督的方法来减少LArTPC探测器模拟和真实数据之间的系统差异，以提高模型性能。 |
| [^70] | [LLM+P: Empowering Large Language Models with Optimal Planning Proficiency.](http://arxiv.org/abs/2304.11477) | 本论文引入了LLM+P框架，将经典规划器的优点融入大型语言模型（LLM），通过将语言描述转换为用规划领域定义语言（PDDL）编写的文件来解决计划问题，旨在综合两者的优势，赋能LLMs最优规划能力。 |
| [^71] | [Wild Face Anti-Spoofing Challenge 2023: Benchmark and Results.](http://arxiv.org/abs/2304.05753) | 前人的人脸防伪（FAS）技术应用于实际场景仍然存在限制，因为当前公开的FAS数据集数量和多样性不足，引起过拟合和场景误判。通过引入野外人脸防伪（WFAS）数据集，该研究提高了数据规模和多样化程度，促进了FAS技术的发展。 |
| [^72] | [Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4.](http://arxiv.org/abs/2304.03439) | 本文分析了多个逻辑推理数据集，评估了ChatGPT和GPT-4在逻辑推理任务上的表现，并构造了一个逻辑推理的分布之外的数据集来研究它们的鲁棒性。实验结果显示，ChatGPT在大多数逻辑推理基准测试中的表现远优于RoBERTa微调方法，而GPT-4的表现则更高。 |
| [^73] | [Language Models are Few-shot Learners for Prognostic Prediction.](http://arxiv.org/abs/2302.12692) | 本研究探索了语言模型在免疫治疗预后预测中的应用，研究了小样本学习面临的挑战，对比了基线和语言模型的有效性，并发现在准确度方面有显著的改进，突出了自然语言处理在临床研究中改善早期检测和干预不同疾病的潜力。 |
| [^74] | [Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection.](http://arxiv.org/abs/2302.12173) | 本文揭示了一种新的攻击向量：间接提示注入，它可以通过在可能检索到的数据中策略性地注入提示来远程利用 LLM 集成应用程序。该攻击对数据盗窃、蠕虫、信息生态系统污染等造成威胁。 |
| [^75] | [A Systematic Review of Green AI.](http://arxiv.org/abs/2301.11047) | 人工智能（AI）的碳足迹越来越大，因此出现了绿色AI这个领域来解决AI的环境可持续性的问题。本文对98篇Green AI文献进行了系统性综述，揭示了该领域在2020年以后得到了显著增长。其中，大多数研究集中在监测AI模型的碳足迹、调整超参数以提高模型的可持续性，或对模型进行基准测试。 |
| [^76] | [xTrimoABFold: De novo Antibody Structure Prediction without MSA.](http://arxiv.org/abs/2212.00735) | xTrimoABFold是一种基于深度抗体语言模型的新型抗体结构预测方法，无需多序列比对，有望促进高通量药物设计的应用。 |
| [^77] | [Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks.](http://arxiv.org/abs/2211.10024) | 本文提出了一种基于嵌入方法的SNAFUE技术，用于寻找复制/粘贴攻击，并使用该技术对ImageNet分类器进行了测试并发现许多易于描述的漏洞。 |
| [^78] | [Trust-Awareness to Secure Swarm Intelligence from Data Injection Attack.](http://arxiv.org/abs/2211.08407) | 本文提出一种基于信任感知的方法，以应对群体智能的数据注入攻击。 |
| [^79] | [Offline Reinforcement Learning for Safer Blood Glucose Control in People with Type 1 Diabetes.](http://arxiv.org/abs/2204.03376) | 本论文研究了离线强化学习在控制1型糖尿病患者血糖水平中的应用，采用BCQ、CQL和TD3-BC算法有效地管理血糖水平，克服了在线学习过程的不稳定性和不安全性。 |
| [^80] | [Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features.](http://arxiv.org/abs/2203.01881) | 从自监督学习模型中提取区分特征，并使用它们压缩表示空间，提出了一种Q-Score自监督表示质量分数，可以可靠地预测线性评估期间的错误分类。 |
| [^81] | [Survey and Systematization of 3D Object Detection Models and Methods.](http://arxiv.org/abs/2201.09354) | 本文就3D目标检测领域进行了全面的调查和系统化研究，提出了一种实用框架用于比较3D目标检测方法，并帮助研究人员和实践者快速了解该领域。 |
| [^82] | [LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with Self-training.](http://arxiv.org/abs/2112.01404) | 本文提出了一种基于少样本的逻辑知识条件下文本生成的统一框架LOGEN，通过自训练和基于内容和结构一致性抽样伪逻辑形式，实现了在少量样本下的文本生成。 |
| [^83] | [PredProp: Bidirectional Stochastic Optimization with Precision Weighted Predictive Coding.](http://arxiv.org/abs/2111.08792) | 本文提出了PredProp方法，它通过随机梯度下降和自适应加权参数更新来优化预测编码网络中的权重和状态。通过使用传播误差协方差实现近似自然梯度下降，使得PredProp在测试中表现良好，比Adam表现更优。此外，PredProp可以通过误差精度提高权重参数的优化效果，并且在层次结构中可以分解精度需求。 |
| [^84] | [Posterior Regularization on Bayesian Hierarchical Mixture Clustering.](http://arxiv.org/abs/2105.06903) | 本文提出了一种后验正则化方法来改进贝叶斯分层混合聚类模型，在每个层级对节点实施最大间隔约束以增强集群的分离。 |

# 详细

[^1]: DualCross: 单目BEV感知的跨模态与跨领域自适应方法

    DualCross: Cross-Modality Cross-Domain Adaptation for Monocular BEV Perception. (arXiv:2305.03724v1 [cs.CV])

    [http://arxiv.org/abs/2305.03724](http://arxiv.org/abs/2305.03724)

    DualCross是一个跨模态和跨领域的自适应框架，旨在使单目BEV感知更加鲁棒，并实现了跨领域跨传感器感知和适应。

    

    在自动驾驶中，缩小训练和部署之间的领域差距并结合多种传感器模态是两个具有挑战性但至关重要的问题。现有的工作只关注上述两个问题中的一个，忽视了实际场景中普遍存在的领域和模态的同时变化。在本文中，我们提出了DualCross，一个跨模态和跨领域的自适应框架，以促进学习更为稳健的单目鸟瞰感知模型，它可以在训练阶段从一个领域的LiDAR传感器中转移点云知识，在不同领域的仅摄像头的测试场景中。这项工作是第一次对单目3D任务的跨领域跨传感器感知和适应进行了开放性分析。我们在大规模数据集上对我们的方法进行了基准测试，覆盖了广泛的场景。

    Closing the domain gap between training and deployment and incorporating multiple sensor modalities are two challenging yet critical topics for self-driving. Existing work only focuses on single one of the above topics, overlooking the simultaneous domain and modality shift which pervasively exists in real-world scenarios. A model trained with multi-sensor data collected in Europe may need to run in Asia with a subset of input sensors available. In this work, we propose DualCross, a cross-modality cross-domain adaptation framework to facilitate the learning of a more robust monocular bird's-eye-view (BEV) perception model, which transfers the point cloud knowledge from a LiDAR sensor in one domain during the training phase to the camera-only testing scenario in a different domain. This work results in the first open analysis of cross-domain cross-sensor perception and adaptation for monocular 3D tasks in the wild. We benchmark our approach on large-scale datasets under a wide range of 
    
[^2]: ChatGPT及其学术应用：在使用AI撰写论文时的最佳实践与法律陷阱

    ChatGPT and Works Scholarly: Best Practices and Legal Pitfalls in Writing with AI. (arXiv:2305.03722v1 [cs.CY])

    [http://arxiv.org/abs/2305.03722](http://arxiv.org/abs/2305.03722)

    本文讨论了学者如何在撰写论文时使用人工智能，提出了相应的最佳实践标准，同时探究了使用AI是否违反版权或属于公平使用的安全港，为学术写作与人工智能共存提供了法律与学术基础的探讨。

    

    近年来，人工智能（AI）的快速发展引发了是否在各种专业背景下使用AI是否合适和合法的问题。本文从学者如何与AI共同撰写论文的角度出发，阐述了如何评估此类AI写作是否违反版权或属于公平使用的安全港。我们提出了一系列最佳实践标准，以保证论文的抄袭、版权和公平使用的标准。随着AI在未来几年内可能变得更加强大，将其整合到学术写作活动中是恰当的。我们提供了一个建立良好的法律和学术基础的框架。

    Recent advances in artificial intelligence (AI) have raised questions about whether the use of AI is appropriate and legal in various professional contexts. Here, we present a perspective on how scholars may approach writing in conjunction with AI, and offer approaches to evaluating whether or not such AI-writing violates copyright or falls within the safe harbor of fair use. We present a set of best practices for standard of care with regard to plagiarism, copyright, and fair use. As AI is likely to grow more capable in the coming years, it is appropriate to begin integrating AI into scholarly writing activities. We offer a framework for establishing sound legal and scholarly foundations.
    
[^3]: 训练才是一切：人工智能、版权和公平训练

    Training Is Everything: Artificial Intelligence, Copyright, and Fair Training. (arXiv:2305.03720v1 [cs.CY])

    [http://arxiv.org/abs/2305.03720](http://arxiv.org/abs/2305.03720)

    本文讨论了在版权法中公正使用版权作品进行人工智能训练的问题，探讨了许可和补偿等方法，强调平衡AI开发者、版权所有者和整个社会的利益和权利需要一种细致而灵活的方法。

    

    为了学习行为，当前这一代革命性的人工智能必须在海量的出版图像、文字和声音中进行训练，其中很多属于版权法的核心内容。一些人认为，将版权作品用作AI的训练集只是一种过渡性的、非消耗性的使用，不会对所有者的内容或保护其版权的著作权产生实质性的干扰。使用这种内容来训练他们的AI引擎的公司通常认为，这种使用应该被视为美国法律下的“公平使用”（在其他国家有时被称为“公平使用”）。相比之下，许多版权所有者及其支持者认为，将版权作品纳入AI的训练集中构成了对所有者知识产权的盗用，因此显然不是法律下的公平使用。这场辩论对于AI及其应用的未来轨迹至关重要。在本文中，我们分析了在版权法中公平使用的情况下，使用版权作品训练AI引擎的论点，探讨了各种方法的法律和政策影响，包括许可和补偿，以及对文化和创意表达的潜在危害。最终，我们主张，平衡AI开发者、版权所有者和整个社会的利益和权利需要一种细致而灵活的方法，认识到训练在开发有效的AI系统方面的重要性。

    To learn how to behave, the current revolutionary generation of AIs must be trained on vast quantities of published images, written works, and sounds, many of which fall within the core subject matter of copyright law. To some, the use of copyrighted works as training sets for AI is merely a transitory and non-consumptive use that does not materially interfere with owners' content or copyrights protecting it. Companies that use such content to train their AI engine often believe such usage should be considered "fair use" under United States law (sometimes known as "fair dealing" in other countries). By contrast, many copyright owners, as well as their supporters, consider the incorporation of copyrighted works into training sets for AI to constitute misappropriation of owners' intellectual property, and, thus, decidedly not fair use under the law. This debate is vital to the future trajectory of AI and its applications.  In this article, we analyze the arguments in favor of, and agains
    
[^4]: AI的治理，由AI治理，为AI治理

    Governance of the AI, by the AI, and for the AI. (arXiv:2305.03719v1 [cs.CY])

    [http://arxiv.org/abs/2305.03719](http://arxiv.org/abs/2305.03719)

    本文讨论AI的治理问题，提出了一种治理模式，即由人类和AI合作，各自发挥其各自的优势，以实现对该技术的有效治理。

    

    在过去的半个世纪中，有几次曾宣布人类历史将被人工智能（AI）彻底改变的虚假黎明。作者们认为，AI时代终于到来了。强大的图像生成器，如DALL-E2和Midjourney，突然间使得任何有权限的人们都可以轻松地创作出富有复杂性的艺术品。类似地，文本生成器，如GPT3.5(包括ChatGPT)和BLOOM，可以让用户撰写有关许多感兴趣的主题的详细写作描述。现在，即使是没有编写软件方面广泛专业知识的人也可以使用AI来生成可以实现无数应用程序的代码。虽然AI将继续发展和改进，可能是以极快的速度，但当前的AI状态已经在许多不同的社会部门带来了深刻的变革。每一种新技术都挑战着人类明智地进行治理的能力。然而，治理通常被认为是可能的和必要的。在AI的情况下，治理由于透明度、问责制和公平性等问题而变得复杂。本文探讨了AI治理，由AI治理和为AI治理的想法，认为AI本身可以在实现有效治理技术方面起到重要作用。作者提出了一种治理模式，涉及人类和AI之间的合作，各自发挥其各自的优势。

    Over the past half century, there have been several false dawns during which the "arrival" of world-changing artificial intelligence (AI) has been heralded. Tempting fate, the authors believe the age of AI has, indeed, finally arrived. Powerful image generators, such as DALL-E2 and Midjourney have suddenly allowed anyone with access the ability easily to create rich and complex art. In a similar vein, text generators, such as GPT3.5 (including ChatGPT) and BLOOM, allow users to compose detailed written descriptions of many topics of interest. And, it is even possible now for a person without extensive expertise in writing software to use AI to generate code capable of myriad applications. While AI will continue to evolve and improve, probably at a rapid rate, the current state of AI is already ushering in profound changes to many different sectors of society. Every new technology challenges the ability of humanity to govern it wisely. However, governance is usually viewed as both possi
    
[^5]: 从泰森多边形中挖掘偏见-目标对齐

    Mining bias-target Alignment from Voronoi Cells. (arXiv:2305.03691v1 [cs.LG])

    [http://arxiv.org/abs/2305.03691](http://arxiv.org/abs/2305.03691)

    本文提出了一种偏见无关的方法来减轻深度神经网络中偏差的影响，在目标类别上量化“偏见对齐/不对齐”，以避免通过网络传播偏见-目标对齐信息。

    

    尽管进行了大量的研究，深度神经网络仍然容易出现偏差:这引起了他们公正性的担忧并限制了它们的泛化能力。在本文中，我们提出了一种偏见无关的方法来减轻深度神经网络中偏差的影响。与传统的去偏见方法不同，我们依靠一个指标来量化目标类别上的“偏见对齐/不对齐”，并利用此信息来避免通过网络传播偏见-目标对齐信息。我们在几个常用的去偏见数据集上进行实验，并将我们的方法与有监督和偏见特定的方法进行比较。我们的结果表明，尽管存在多个样本中的偏见，所提出的方法在不存在偏见的情况下实现了与最先进的有监督方法相当的性能。

    Despite significant research efforts, deep neural networks are still vulnerable to biases: this raises concerns about their fairness and limits their generalization. In this paper, we propose a bias-agnostic approach to mitigate the impact of bias in deep neural networks. Unlike traditional debiasing approaches, we rely on a metric to quantify ``bias alignment/misalignment'' on target classes, and use this information to discourage the propagation of bias-target alignment information through the network. We conduct experiments on several commonly used datasets for debiasing and compare our method to supervised and bias-specific approaches. Our results indicate that the proposed method achieves comparable performance to state-of-the-art supervised approaches, although it is bias-agnostic, even in presence of multiple biases in the same sample.
    
[^6]: 从入院记录预测COVID-19和肺炎并发症

    Predicting COVID-19 and pneumonia complications from admission texts. (arXiv:2305.03661v1 [cs.CL])

    [http://arxiv.org/abs/2305.03661](http://arxiv.org/abs/2305.03661)

    本文提出一种基于入院记录进行患者并发症风险评估的新方法，并使用Longformer神经网络计算患者的风险评分。该方法在多个欧洲医院的患者数据中表现出优异的性能，具有泛化能力和其他优势。

    

    本文提出了一种基于入院报告的风险评估新方法，用于预测因肺炎或COVID-19而住院的患者的并发症风险。我们将Longformer神经网络应用于入院记录和其他可用的文本数据，计算出患者的风险评分。我们使用多个欧洲医院的患者数据证明了我们的方法优于Transformer基线模型。我们的实验表明，该模型在多个机构和诊断中具有泛化能力。此外，本文还介绍了我们的方法具有的其他优势。

    In this paper we present a novel approach to risk assessment for patients hospitalized with pneumonia or COVID-19 based on their admission reports. We applied a Longformer neural network to admission reports and other textual data available shortly after admission to compute risk scores for the patients. We used patient data of multiple European hospitals to demonstrate that our approach outperforms the Transformer baselines. Our experiments show that the proposed model generalises across institutions and diagnoses. Also, our method has several other advantages described in the paper.
    
[^7]: 利用OpenAI GPT模型的检索增强的胸部X射线报告生成

    Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models. (arXiv:2305.03660v1 [cs.CL])

    [http://arxiv.org/abs/2305.03660](http://arxiv.org/abs/2305.03660)

    该研究提出了一种检索增强的方法，利用对比预训练的视觉语言模型的多模态对齐嵌入来检索相应的候选放射学文本，并使用通用领域生成模型来生成报告，可抑制虚构的生成，实现更好的临床指标。

    

    我们提出了一种名为Retrieval Augmented Generation (RAG) 的方法来自动生成放射学报告，该方法利用对比预训练的视觉语言模型的多模态对齐嵌入来检索相应的候选放射学文本，并使用像OpenAI text-davinci-003、gpt-3.5-turbo和gpt-4这样的通用领域生成模型来生成报告。该方法可以抑制虚构的生成并提供指令跟随能力，以我们所需的格式生成报告内容。我们的方法实现了更好的临床指标，BERTScore为0.2865（Δ+25.88%），Semb Score为0.4026（Δ+6.31%）。我们的方法可以广泛应用于不同的临床设置，因为它允许增强自动生成的放射学报告过程，同时具备适合该设置的相关内容的能力。

    We propose Retrieval Augmented Generation (RAG) as an approach for automated radiology report writing that leverages multimodally aligned embeddings from a contrastively pretrained vision language model for retrieval of relevant candidate radiology text for an input radiology image and a general domain generative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for report generation using the relevant radiology text retrieved. This approach keeps hallucinated generations under check and provides capabilities to generate report content in the format we desire leveraging the instruction following capabilities of these generative models. Our approach achieves better clinical metrics with a BERTScore of 0.2865 ({\Delta}+ 25.88%) and Semb score of 0.4026 ({\Delta}+ 6.31%). Our approach can be broadly relevant for different clinical settings as it allows to augment the automated radiology report generation process with content relevant for that setting while also having the abilit
    
[^8]: 提高LaCAM算法的可扩展最终优化多智能体路径规划能力

    Improving LaCAM for Scalable Eventually Optimal Multi-Agent Pathfinding. (arXiv:2305.03632v1 [cs.AI])

    [http://arxiv.org/abs/2305.03632](http://arxiv.org/abs/2305.03632)

    本研究提出了可扩展最终优化多智能体路径规划能力中的LaCAM算法，改进了其任何时期版本LaCAM*和后继生成，成功减少规划工作量并取得了相应实验效果。

    

    本研究扩展了最近开发的LaCAM算法，用于多智能体路径规划（MAPF）。LaCAM是一种次优的基于搜索的算法，使用懒惰的后继生成来大幅减少规划工作量。我们提出了两个增强功能。首先，我们提出其任何时期版本，称为LaCAM*，其最终收敛于最佳解，前提是解决方案成本是累积的转换成本。其次，我们改进了后继生成，以快速获得初始解。详尽的实验证明了它们的效用。例如，LaCAM*在标准桌面PC上，在确保最终收敛到最佳解的情况下，次优地解决了从MAPF基准检索的99％的实例，其中代理数量最高达1000个，用时不超过十秒;开发了新的MAPF算法视野。

    This study extends the recently-developed LaCAM algorithm for multi-agent pathfinding (MAPF). LaCAM is a sub-optimal search-based algorithm that uses lazy successor generation to dramatically reduce the planning effort. We present two enhancements. First, we propose its anytime version, called LaCAM*, which eventually converges to optima, provided that solution costs are accumulated transition costs. Second, we improve the successor generation to quickly obtain initial solutions. Exhaustive experiments demonstrate their utility. For instance, LaCAM* sub-optimally solved 99% of the instances retrieved from the MAPF benchmark, where the number of agents varied up to a thousand, within ten seconds on a standard desktop PC, while ensuring eventual convergence to optima; developing a new horizon of MAPF algorithms.
    
[^9]: 使用文本到图像生成模型进行图像字幕数据管理

    Data Curation for Image Captioning with Text-to-Image Generative Models. (arXiv:2305.03610v1 [cs.CV])

    [http://arxiv.org/abs/2305.03610](http://arxiv.org/abs/2305.03610)

    本论文探究了通过数据整合来提高图像字幕质量的方案，并使用现有资源训练了比基准模型更好的模型。

    

    最近，图像字幕技术的发展主要依赖于大规模的视觉-语言预训练，并且日益依赖于计算资源和越来越大的多模态数据集。本文通过数据整合的两种方法探究了如何通过提高现有数据集中的样本质量来改善性能：一种方法假定由于图像和字幕之间的不匹配，某些示例应该避免使用，另一种方法则假定不匹配可以通过替换图像来解决。

    Recent advances in image captioning are mainly driven by large-scale vision-language pretraining, relying heavily on computational resources and increasingly large multimodal datasets. Instead of scaling up pretraining data, we ask whether it is possible to improve performance by improving the quality of the samples in existing datasets. We pursue this question through two approaches to data curation: one that assumes that some examples should be avoided due to mismatches between the image and caption, and one that assumes that the mismatch can be addressed by replacing the image, for which we use the state-of-the-art Stable Diffusion model. These approaches are evaluated using the BLIP model on MS COCO and Flickr30K in both finetuning and few-shot learning settings. Our simple yet effective approaches consistently outperform baselines, indicating that better image captioning models can be trained by curating existing resources. Finally, we conduct a human study to understand the error
    
[^10]: 人类注意力引导的可解释人工智能对计算机视觉模型的影响

    Human Attention-Guided Explainable Artificial Intelligence for Computer Vision Models. (arXiv:2305.03601v1 [cs.CV])

    [http://arxiv.org/abs/2305.03601](http://arxiv.org/abs/2305.03601)

    本研究探讨了将人类注意力知识嵌入基于显著性的可解释人工智能方法用于计算机视觉模型中的效果。开发了基于梯度和人类注意力引导的两种解释方法，结果表明对于目标检测模型，人类注意力引导的方法在解释的准确性和合理性方面优于基于梯度的方法。

    

    本研究探讨了将人类注意力知识嵌入基于显著性的可解释人工智能（XAI）方法用于计算机视觉模型中是否可以增强其合理性和准确性。我们首先针对目标检测模型开发了新的基于梯度的XAI方法，通过扩展当前用于图像分类模型的方法生成面向对象的解释。有趣的是，虽然这些基于梯度的方法可以很好地解释图像分类模型，但是当用于解释目标检测模型时，所得到的显著性图通常比执行同一任务的人类注意力图的准确性低。因此，我们开发了人类注意力引导的XAI（HAG-XAI）方法，通过使用可训练的激活函数和平滑核来最大化XAI显著性图与人类注意力图的相似性，从人类注意力学习如何最好地结合模型的解释信息以增强解释的合理性。对于图像分类模型，HAG-XAI和基于梯度的方法表现相似，但是对于目标检测模型，HAG-XAI生成的解释在准确性和合理性方面都显著优于基于梯度的方法。

    We examined whether embedding human attention knowledge into saliency-based explainable AI (XAI) methods for computer vision models could enhance their plausibility and faithfulness. We first developed new gradient-based XAI methods for object detection models to generate object-specific explanations by extending the current methods for image classification models. Interestingly, while these gradient-based methods worked well for explaining image classification models, when being used for explaining object detection models, the resulting saliency maps generally had lower faithfulness than human attention maps when performing the same task. We then developed Human Attention-Guided XAI (HAG-XAI) to learn from human attention how to best combine explanatory information from the models to enhance explanation plausibility by using trainable activation functions and smoothing kernels to maximize XAI saliency map's similarity to human attention maps. While for image classification models, HAG
    
[^11]: NLI4CT：面向临床试验报告的多证据自然语言推理

    NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])

    [http://arxiv.org/abs/2305.03598](http://arxiv.org/abs/2305.03598)

    本文提出了一种面向临床试验报告的自然语言推理模型，通过检索支持事实来确定自然语言陈述和CTR之间的推理关系。

    

    如何解释和检索用于支持临床决策的医学证据？多年来，积累下来的临床试验报告包含了发展个性化医学所必需的信息。然而，为了找到最佳的实验治疗证据，手动检查超过400,000个临床试验报告是实际上不可行的。自然语言推理（NLI）提供了一个潜在的解决方案，通过允许可扩展计算文本蕴含关系。然而，现有的NLI模型在生物医学语料库上表现不佳，之前发布的数据集无法捕捉CTR推理的全部复杂性。在本文中，我们提出了一种新的资源，以推进关于CTR推理的NLI研究。该资源包括两个主要任务。首先，确定自然语言陈述和CTR之间的推理关系。其次，检索支持事实以证明预测的关系。我们提供了NLI4CT，一个基于CTR的语料库。

    How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
    
[^12]: 现在它听起来像你了：在设备上学习个性化词汇

    Now It Sounds Like You: Learning Personalized Vocabulary On Device. (arXiv:2305.03584v1 [cs.CL])

    [http://arxiv.org/abs/2305.03584](http://arxiv.org/abs/2305.03584)

    这项研究提出了一种称为“生词扩展”的技术，通过个性化的“生词适配器”来学习个性化词汇，提高了生词覆盖率并显著提高了模型准确度。

    

    近年来，联邦学习在进行各种自然语言处理任务方面已经显示出显著进展。这项工作侧重于应用个性化联邦学习进行设备端语言建模。由于内存和延迟的限制，这些模型无法支持子单词标记或波束搜索解码的复杂性，因此决定部署封闭词汇的语言模型。然而，封闭词汇模型无法处理特定用户的生词，为了解决这个问题，我们提出了一种称为“生词扩展”的新技术，该技术提高了生词覆盖率，增加了模型的准确性，同时最大程度地减少了对内存和延迟的影响。这种方法引入了个性化的“生词适配器”，有效地从中央模型传输知识，并为个性化词汇学习单词嵌入。在一组常见的联邦学习基准测试中，生词扩展方法明显优于标准个性化联邦学习方法。

    In recent years, Federated Learning (FL) has shown significant advancements in its ability to perform various natural language processing (NLP) tasks. This work focuses on applying personalized FL for on-device language modeling. Due to limitations of memory and latency, these models cannot support the complexity of sub-word tokenization or beam search decoding, resulting in the decision to deploy a closed-vocabulary language model. However, closed-vocabulary models are unable to handle out-of-vocabulary (OOV) words belonging to specific users. To address this issue, We propose a novel technique called "OOV expansion" that improves OOV coverage and increases model accuracy while minimizing the impact on memory and latency. This method introduces a personalized "OOV adapter" that effectively transfers knowledge from a central model and learns word embedding for personalized vocabulary. OOV expansion significantly outperforms standard FL personalization methods on a set of common FL benc
    
[^13]: 在语境学习中保持连贯性：使用大型语言模型进行即席机器翻译的研究

    In-context Learning as Maintaining Coherency: A Study of On-the-fly Machine Translation Using Large Language Models. (arXiv:2305.03573v1 [cs.CL])

    [http://arxiv.org/abs/2305.03573](http://arxiv.org/abs/2305.03573)

    本文探究了机器翻译的在语境学习现象，提出了在语境学习应该是保持结果与其语境具有连贯性的生成任务。实验结果表明，将具有长期连贯性的样例用于翻译可以提高翻译性能。

    

    通常把在语境学习现象看做是“从例子中学习”。本文研究了机器翻译的学习过程，将在语境学习看作是生成结果任务，要求结果与其语境具有连贯性。我们通过对4个领域的随机样例的研究发现，当显示领域内样例时，翻译性能得到了提升。接下来，我们研究了具有移动窗口的领域内设置中的连贯性，并将其与文献中先前确定的其他因素如长度、表面相似性和句子嵌入相似性进行了比较。我们的结果显示，在3个模型（GPTNeo2.7B、Bloom3B、XGLM2.9B）和3个翻译方向（en→{pt，de，fr}）中，样例与测试句子的长期连贯性是流向翻译性能的良好指标。

    The phenomena of in-context learning has typically been thought of as "learning from examples". In this work which focuses on Machine Translation, we present a perspective of in-context learning as the desired generation task maintaining coherency with its context, i.e., the prompt examples. We first investigate randomly sampled prompts across 4 domains, and find that translation performance improves when shown in-domain prompts. Next, we investigate coherency for the in-domain setting, which uses prompt examples from a moving window. We study this with respect to other factors that have previously been identified in the literature such as length, surface similarity and sentence embedding similarity. Our results across 3 models (GPTNeo2.7B, Bloom3B, XGLM2.9B), and three translation directions (\texttt{en}$\rightarrow$\{\texttt{pt, de, fr}\}) suggest that the long-term coherency of the prompts and the test sentence is a good indicator of downstream translation performance. In doing so, 
    
[^14]: 学习如何为多视角神经图像合成修剪像素

    Learn how to Prune Pixels for Multi-view Neural Image-based Synthesis. (arXiv:2305.03572v1 [cs.MM])

    [http://arxiv.org/abs/2305.03572](http://arxiv.org/abs/2305.03572)

    LeHoPP是一种有效的方法进行输入像素修剪，可以在不重新训练基于图像的渲染网络的情况下，在合成质量和像素率之间达到良好平衡，提高图像合成的效率。

    

    基于图像的渲染技术是用户沉浸式体验的核心，它们在给定多个输入图像的情况下生成新视图。由于它们在客观和主观质量方面表现良好，因此研究社区致力于它们的改进。然而，处于带宽有限的环境中时所需数据的大量使得在实时应用中的应用受到限制。我们提出了LeHoPP方法，通过检查每个输入像素对于渲染的视图的重要性并避免使用不相关的像素的方法进行输入像素修剪。即使不重新训练基于图像的渲染网络，我们的方法显示出合成质量和像素率之间的良好平衡。在测试神经渲染框架时，与其他修剪基线相比，LeHoPP平均增益在0.9 dB到3.6 dB之间。

    Image-based rendering techniques stand at the core of an immersive experience for the user, as they generate novel views given a set of multiple input images. Since they have shown good performance in terms of objective and subjective quality, the research community devotes great effort to their improvement. However, the large volume of data necessary to render at the receiver's side hinders applications in limited bandwidth environments or prevents their employment in real-time applications. We present LeHoPP, a method for input pixel pruning, where we examine the importance of each input pixel concerning the rendered view, and we avoid the use of irrelevant pixels. Even without retraining the image-based rendering network, our approach shows a good trade-off between synthesis quality and pixel rate. When tested in the general neural rendering framework, compared to other pruning baselines, LeHoPP gains between $0.9$ dB and $3.6$ dB on average.
    
[^15]: 基于子空间学习的黑匣子提示调优

    Black-box Prompt Tuning with Subspace Learning. (arXiv:2305.03518v1 [cs.CL])

    [http://arxiv.org/abs/2305.03518](http://arxiv.org/abs/2305.03518)

    本文提出了基于子空间学习的黑匣子提示调优（BSL）框架，通过元学习在相似任务的集合中识别子空间，以改善黑匣子提示调优的通用性，在下游任务和LLMs上都能够持续达到竞争性的表现水平。

    

    黑匣子提示调优使用无导数优化算法在低维子空间中学习提示，而不是通过大语言模型（LLM）的网络进行反向传播。最近的研究发现黑匣子提示调优在不同任务和LLMs上缺乏通用性，这与不恰当的子空间选择有关。本文提出了基于子空间学习的黑匣子提示调优（BSL）来改善黑匣子提示调优的通用性。基于相似任务的集合上进行元学习以确定子空间可以识别类似任务的最优提示，并且可以保证在相似任务上进行子空间优化找到一个在目标任务上表现良好的提示。实验结果表明，我们的BSL框架无论在下游任务和LLMs上都能够持续达到竞争性的表现水平。

    Black-box prompt tuning uses derivative-free optimization algorithms to learn prompts in low-dimensional subspaces instead of back-propagating through the network of Large Language Models (LLMs). Recent studies have found that black-box prompt tuning lacks versatility across tasks and LLMs, which we believe is related to the inappropriate choice of subspaces. In this paper, we propose Black-box prompt tuning with Subspace Learning (BSL) to improve the versatility of black-box prompt tuning. Based on the assumption that nearly optimal prompts for similar tasks exist in a common subspace, we propose identifying such subspaces by meta-learning on a set of similar source tasks. Therefore, for a target task that shares similarities with source tasks, we guarantee that optimizing in the subspace can find a prompt that performs well on the target task. Experiments confirm that our BSL framework consistently achieves competitive performance regardless of downstream tasks and LLMs.
    
[^16]: 从文本中实现少样本领域自适应视觉融合事件检测

    Few-shot Domain-Adaptive Visually-fused Event Detection from Text. (arXiv:2305.03517v1 [cs.CL])

    [http://arxiv.org/abs/2305.03517](http://arxiv.org/abs/2305.03517)

    本文提出了一种基于视觉融合和领域自适应的事件检测方法，可以用少量标记数据训练并且适应于新领域。

    

    近年来，将辅助模态如图像整合到事件检测模型中已经引起人们的越来越多的关注。自然语言描述情境的复杂性促使研究人员利用相关的视觉上下文来提高事件检测的性能。然而，目前这个领域的方法在数据稀缺性方面存在问题，需要许多标记好的文本-图像对来训练模型。此外，在推断时无法获得视觉上下文的限制也会对这些模型的性能产生负面影响，使其在实际场景中难以应用。本文提出了一种新颖的领域自适应视觉融合事件检测方法，可以用很少的标记图像-文本配对数据点进行训练。具体来说，我们引入了一种称为视觉想象器的方法，它可以在没有视觉上下文的情况下从文本中合成图像，并且可以根据需要定制到特定的领域。通过这种方法，我们的方法消除了需要大量标记数据的需求，并且只需少量的标记图像-文本对就可以适应于新领域。我们在基准数据集上评估了我们的方法，并显示其优于现有的最先进的领域自适应事件检测方法。

    Incorporating auxiliary modalities such as images into event detection models has attracted increasing interest over the last few years. The complexity of natural language in describing situations has motivated researchers to leverage the related visual context to improve event detection performance. However, current approaches in this area suffer from data scarcity, where a large amount of labelled text-image pairs are required for model training. Furthermore, limited access to the visual context at inference time negatively impacts the performance of such models, which makes them practically ineffective in real-world scenarios. In this paper, we present a novel domain-adaptive visually-fused event detection approach that can be trained on a few labelled image-text paired data points. Specifically, we introduce a visual imaginator method that synthesises images from text in the absence of visual context. Moreover, the imaginator can be customised to a specific domain. In doing so, our
    
[^17]: 使用梯度下降学习决策树

    Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])

    [http://arxiv.org/abs/2305.03515](http://arxiv.org/abs/2305.03515)

    本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。

    

    决策树是用于许多机器学习任务的常见工具，因为它们具有高度的解释性。然而，从数据中学习决策树是一个困难的优化问题，因为它是非凸和非可微的。因此，通常的方法是使用一种贪婪生长算法来学习决策树，在每个内部节点上局部最小化不纯度。不幸的是，这种贪心过程可能会导致次优的决策树。在本文中，我们提出了一种使用梯度下降学习难以处理的轴对齐决策树的新方法。所提出的方法使用反向传播和直通算子在密集的决策树表示上联合优化所有树的参数。我们的方法在二分类基准测试上优于现有方法，并在多类任务中实现了有竞争力的结果。

    Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
    
[^18]: 基于翻译对齐的视觉语言模型跨语言迁移的参数高效方法

    Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment. (arXiv:2305.03510v1 [cs.CL])

    [http://arxiv.org/abs/2305.03510](http://arxiv.org/abs/2305.03510)

    本文提出了一个通过翻译对齐的方式实现参数高效、跨语言的迁移学习框架，实验结果显示该框架能够显著减少多语言之间的性能差异。

    

    预训练的视觉语言模型（如CLIP）在连接图像和英语文本方面取得了显著的成功。尽管最近试图扩展CLIP以支持其他语言，但由于资源不平衡，观察到了不同语言之间的性能差异。此外，当前的预训练模型的跨语言迁移方法会消耗大量资源。因此，我们提出了一种新的参数高效的跨语言迁移学习框架，利用基于翻译的对齐方法来减轻多语言差异，并探索参数高效的微调方法来实现参数高效的跨语言迁移。在XTD和Multi30K数据集上进行了广泛的实验，涵盖了零-shot、few-shot和全数据集学习场景下的11种语言，结果显示我们的框架显著减少了语言之间的多语言差异，并提高了性能。

    Pre-trained vision and language models such as CLIP have witnessed remarkable success in connecting images and texts with a primary focus on English texts. Despite recent efforts to extend CLIP to support other languages, disparities in performance among different languages have been observed due to uneven resource availability. Additionally, current cross-lingual transfer methods of those pre-trained models would consume excessive resources for a large number of languages. Therefore, we propose a new parameter-efficient cross-lingual transfer learning framework that utilizes a translation-based alignment method to mitigate multilingual disparities and explores parameter-efficient fine-tuning methods for parameter-efficient cross-lingual transfer. Extensive experiments on XTD and Multi30K datasets, covering 11 languages under zero-shot, few-shot, and full-dataset learning scenarios, show that our framework significantly reduces the multilingual disparities among languages and improves 
    
[^19]: Diffusion Explainer：用于文本到图像稳定扩散的可视化解释工具

    Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])

    [http://arxiv.org/abs/2305.03509](http://arxiv.org/abs/2305.03509)

    Diffusion Explainer是第一个可交互的可视化工具，用于解释稳定扩散如何将文本提示转化为图像，用户可以通过动画和交互元素流畅地在多个抽象级别之间过渡，从而更好地理解提示对图像生成的影响。

    

    基于扩散的生成模型通过创造逼真的图像而获得了全球关注。然而，它们复杂的内部结构和操作往往使得非专业人员难以理解。我们提出了 Diffusion Explainer，这是第一个交互式可视化工具，用于解释稳定扩散如何将文本提示转化为图像。Diffusion Explainer紧密地将稳定扩散的复杂组件的视觉概述与其潜在操作的详细说明相结合，通过动画和交互元素使用户可以流畅地在多个抽象级别之间过渡。通过比较由两个相关文本提示引导的图像表示的演变来指导精细时间步长，用户可以发现提示对图像生成的影响。Diffusion Explainer在用户的Web浏览器中本地运行，无需安装或专门的硬件，扩大了公众对现代人工智能技术的教育获取。

    Diffusion-based generative models' impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern AI tec
    
[^20]: 在加密文本上训练自然语言处理模型以增强隐私保护

    Training Natural Language Processing Models on Encrypted Text for Enhanced Privacy. (arXiv:2305.03497v1 [cs.CL])

    [http://arxiv.org/abs/2305.03497](http://arxiv.org/abs/2305.03497)

    本研究提出了一种在加密文本数据上训练NLP模型的方法，以缓解数据隐私问题，同时保持与非加密数据训练模型相似的性能。

    

    随着云服务在训练和部署机器学习模型中的不断增加，数据隐私已成为一个主要关注点。这对于自然语言处理（NLP）模型尤为重要，因为这些模型通常处理涉及个人通信和机密文件等敏感信息。本研究提出了一种在加密文本数据上训练NLP模型的方法，以缓解数据隐私问题，同时保持与非加密数据训练模型相似的性能。我们使用两种不同的架构，即Doc2Vec + XGBoost和Doc2Vec + LSTM，演示了我们的方法，并在20 Newsgroups数据集上评估了模型。我们的结果表明，加密和非加密模型均可实现相似的性能，说明我们的加密方法在保持模型准确性的同时有效地维护了数据隐私。为了复制我们的实验，我们在以下地址提供了Colab笔记本：https://t.ly/lR-TP

    With the increasing use of cloud-based services for training and deploying machine learning models, data privacy has become a major concern. This is particularly important for natural language processing (NLP) models, which often process sensitive information such as personal communications and confidential documents. In this study, we propose a method for training NLP models on encrypted text data to mitigate data privacy concerns while maintaining similar performance to models trained on non-encrypted data. We demonstrate our method using two different architectures, namely Doc2Vec+XGBoost and Doc2Vec+LSTM, and evaluate the models on the 20 Newsgroups dataset. Our results indicate that both encrypted and non-encrypted models achieve comparable performance, suggesting that our encryption method is effective in preserving data privacy without sacrificing model accuracy. In order to replicate our experiments, we have provided a Colab notebook at the following address: https://t.ly/lR-TP
    
[^21]: 基于“梯度下降”与 beam search 的自动提示优化

    Automatic Prompt Optimization with "Gradient Descent" and Beam Search. (arXiv:2305.03495v1 [cs.CL])

    [http://arxiv.org/abs/2305.03495](http://arxiv.org/abs/2305.03495)

    在基于大型语言模型的自然语言处理中，使用梯度下降和 beam search 的自动提示优化方法可以自动改进提示，提高性能。

    

    大型语言模型（LLM）在通用智能方面展现了出色性能，但其能力仍高度依赖于手写的提示，需要大量的试错尝试。我们提出了一个简单而非参数化的解决方案——自动提示优化（APO），其灵感来自于使用数值梯度下降自动改进提示。

    Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language ``gradients'' that criticize the current prompt. The gradients are then ``propagated'' into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing 
    
[^22]: 生成式隐写扩散

    Generative Steganography Diffusion. (arXiv:2305.03472v1 [cs.MM])

    [http://arxiv.org/abs/2305.03472](http://arxiv.org/abs/2305.03472)

    提出了一种称为“生成式隐写扩散”（GSD）的新型生成式隐写方案，利用一种反演扩散模型可以100％地恢复隐藏的秘密数据，并实现了高质量的隐写图像生成。

    

    生成式隐写术（GS）是一种新兴技术，它直接从秘密数据中生成隐藏信息图像。最近已经开发了基于GAN或Flow的各种GS方法。然而，现有的基于GAN的GS方法由于缺乏网络可逆性，无法完全恢复隐藏的秘密数据，而基于Flow的方法由于每个模块中严格的可逆限制而产生质量较差的图像。为了解决这个问题，我们提出了一种新的GS方案，称为“生成式隐写扩散”（GSD），通过设计一种反演扩散模型“StegoDiffusion”来实现。它不仅生成逼真的隐写图像，而且允许100％地恢复隐藏的秘密数据。所提出的StegoDiffusion模型利用非马尔可夫链和快速采样技术实现高效的隐写图像生成。通过根据StegoDiffusion中的生成过程的转移概率构造一个普通微分方程（ODE），秘密数据和隐写信息可以被有效和可逆地嵌入到隐写图像中。实验结果表明了所提出的GSD方案在隐写图像质量和秘密数据恢复性能方面的有效性。

    Generative steganography (GS) is an emerging technique that generates stego images directly from secret data. Various GS methods based on GANs or Flow have been developed recently. However, existing GAN-based GS methods cannot completely recover the hidden secret data due to the lack of network invertibility, while Flow-based methods produce poor image quality due to the stringent reversibility restriction in each module. To address this issue, we propose a novel GS scheme called "Generative Steganography Diffusion" (GSD) by devising an invertible diffusion model named "StegoDiffusion". It not only generates realistic stego images but also allows for 100\% recovery of the hidden secret data. The proposed StegoDiffusion model leverages a non-Markov chain with a fast sampling technique to achieve efficient stego image generation. By constructing an ordinary differential equation (ODE) based on the transition probability of the generation process in StegoDiffusion, secret data and stego i
    
[^23]: 多视角图形表示学习用于解决混合型数值推理问答问题

    Multi-View Graph Representation Learning for Answering Hybrid Numerical Reasoning Question. (arXiv:2305.03458v1 [cs.CL])

    [http://arxiv.org/abs/2305.03458](http://arxiv.org/abs/2305.03458)

    本文提出了一种基于多视角图形表示学习的混合型数值推理问答模型，能够从表格视角、关系视角和数值视角捕捉混合数据之间的粒度关系和空间结构信息，能够有效提高数值推理任务的准确率。

    

    混合型问答 (HybridQA) 数据含有文本与表格数据，需要模型选择合适证据进行数值推理任务。现有的基于编码器-解码器框架的方法使用基于表达树的解码器来解决数值推理问题。然而，编码器更倾向于使用机器阅读理解(MRC)方法,它以表格序列和文本拼接作为输入，破坏了表格和文本之间的粒度关系以及表格本身的空间结构信息。为了解决这些问题，本文提出了多视角图形 (MVG) 编码器来考虑粒度之间关系并从多个视角捕捉关系。通过使用MVGE作为模块，我们构建了面向保留混合数据原始特征的表格视图、关系视图和数值视图。我们在公开的表格文本混合QA数据集上验证了我们的模型，并显示我们提出的MVGE在数值推理子任务上优于现有最先进方法，准确率相对提高了27.0%。

    Hybrid question answering (HybridQA) over the financial report contains both textual and tabular data, and requires the model to select the appropriate evidence for the numerical reasoning task. Existing methods based on encoder-decoder framework employ a expression tree-based decoder to solve numerical reasoning problems. However, encoders rely more on Machine Reading Comprehension (MRC) methods, which take table serialization and text splicing as input, damaging the granularity relationship between table and text as well as the spatial structure information of table itself. In order to solve these problems, the paper proposes a Multi-View Graph (MVG) Encoder to take the relations among the granularity into account and capture the relations from multiple view. By utilizing MVGE as a module, we constuct Tabular View, Relation View and Numerical View which aim to retain the original characteristics of the hybrid data. We validate our model on the publicly available table-text hybrid QA 
    
[^24]: 射电图像立方体分类的新进展

    Advances on the classification of radio image cubes. (arXiv:2305.03435v1 [astro-ph.IM])

    [http://arxiv.org/abs/2305.03435](http://arxiv.org/abs/2305.03435)

    本研究综述了机器智能技术在射电图像中的应用，特别关注了射电星系的形态分类问题。

    

    现代射电望远镜每天将产生数量级为艾字节的数据，如Square Kilometre Array（SKA）系统。大规模数据集是未知和罕见天体物理现象的来源，这些现象会导致发现。然而，仅依靠人工辅助和传统统计技术是不可能的，需要利用强大的机器智能。最近，有越来越多的科学文献着眼于利用人工智能技术处理射电天文学中的问题，包括源提取、形态分类和异常检测。本研究简要概述了机器智能技术在射电图像中的应用，重点是射电星系的形态分类。它旨在基于数据复杂性、数据预处理和射电天文学方法的新颖性，对相关论文进行详细综述。

    Modern radio telescopes will daily generate data sets on the scale of exabytes for systems like the Square Kilometre Array (SKA). Massive data sets are a source of unknown and rare astrophysical phenomena that lead to discoveries. Nonetheless, this is only plausible with the exploitation of intensive machine intelligence to complement human-aided and traditional statistical techniques. Recently, there has been a surge in scientific publications focusing on the use of artificial intelligence in radio astronomy, addressing challenges such as source extraction, morphological classification, and anomaly detection. This study presents a succinct, but comprehensive review of the application of machine intelligence techniques on radio images with emphasis on the morphological classification of radio galaxies. It aims to present a detailed synthesis of the relevant papers summarizing the literature based on data complexity, data pre-processing, and methodological novelty in radio astronomy. Th
    
[^25]: 探索应用强大的大型人工智能模型在课堂教学中：机遇、挑战和前景

    Towards Applying Powerful Large AI Models in Classroom Teaching: Opportunities, Challenges and Prospects. (arXiv:2305.03433v1 [cs.AI])

    [http://arxiv.org/abs/2305.03433](http://arxiv.org/abs/2305.03433)

    本文探索了利用大型语言模型实现教师与学生的互动，提高教学质量的潜力和挑战。提出了一个统一的框架来处理多样化的教育数据集，处理长时间的对话以及压缩信息以更好地完成更多的下游任务。

    

    本文提出了一系列互动场景，利用人工智能（AI）增强课堂教学，如对话自动完成、知识和风格转移以及评估AI生成内容。通过利用近期大型语言模型（LLM）的发展，本文探索了AI增强和丰富师生对话、提高教学质量的潜力。我们的目标是在教师与学生之间产生创新和有意义的对话，制定评估标准，并提高AI教育计划的效力。在第三节中，我们讨论了利用现有LLM有效完成教育任务的挑战，并提出了一个统一的框架来处理多样化的教育数据集，处理长时间的对话以及压缩信息以更好地完成更多的下游任务。在第四节中，我们总结了关键任务，包括教师-学生对话自动完成、示范答案和AI支持的写作。

    This perspective paper proposes a series of interactive scenarios that utilize Artificial Intelligence (AI) to enhance classroom teaching, such as dialogue auto-completion, knowledge and style transfer, and assessment of AI-generated content. By leveraging recent developments in Large Language Models (LLMs), we explore the potential of AI to augment and enrich teacher-student dialogues and improve the quality of teaching. Our goal is to produce innovative and meaningful conversations between teachers and students, create standards for evaluation, and improve the efficacy of AI-for-Education initiatives. In Section 3, we discuss the challenges of utilizing existing LLMs to effectively complete the educated tasks and present a unified framework for addressing diverse education dataset, processing lengthy conversations, and condensing information to better accomplish more downstream tasks. In Section 4, we summarize the pivoting tasks including Teacher-Student Dialogue Auto-Completion, Ex
    
[^26]: 自适应图卷积子空间聚类

    Adaptive Graph Convolutional Subspace Clustering. (arXiv:2305.03414v1 [cs.LG])

    [http://arxiv.org/abs/2305.03414](http://arxiv.org/abs/2305.03414)

    本研究提出了自适应图卷积子空间聚类方法（AGCSC），将特征提取方法和系数矩阵约束相结合，实现更真实的子空间结构揭示，并在大量实验中证明了其有效性和效率。

    

    光谱型子空间聚类算法在许多子空间聚类应用中表现出很好的性能。现有的光谱型子空间聚类算法要么专注于设计重构系数矩阵的约束条件，要么使用特征提取方法找到原始数据样本的潜在特征。本文受到图卷积网络的启发，使用图卷积技术同时开发特征提取方法和系数矩阵约束。我们在提出的算法中迭代和自适应地更新图卷积算子。因此，我们将该方法称为自适应图卷积子空间聚类（AGCSC）。我们声称，通过使用AGCSC，原始数据样本的聚合特征表示适合于子空间聚类，系数矩阵可以更真实地揭示原始数据集的子空间结构。最后，进行了大量子空间聚类实验，以验证AGCSC的有效性和效率。

    Spectral-type subspace clustering algorithms have shown excellent performance in many subspace clustering applications. The existing spectral-type subspace clustering algorithms either focus on designing constraints for the reconstruction coefficient matrix or feature extraction methods for finding latent features of original data samples. In this paper, inspired by graph convolutional networks, we use the graph convolution technique to develop a feature extraction method and a coefficient matrix constraint simultaneously. And the graph-convolutional operator is updated iteratively and adaptively in our proposed algorithm. Hence, we call the proposed method adaptive graph convolutional subspace clustering (AGCSC). We claim that by using AGCSC, the aggregated feature representation of original data samples is suitable for subspace clustering, and the coefficient matrix could reveal the subspace structure of the original data set more faithfully. Finally, plenty of subspace clustering ex
    
[^27]: 自主系统的可信性评估

    Assessing Trustworthiness of Autonomous Systems. (arXiv:2305.03411v1 [cs.AI])

    [http://arxiv.org/abs/2305.03411](http://arxiv.org/abs/2305.03411)

    随着自主系统在社会中变得越来越普遍，评估其可信度是必要的。本文提出了一个可以用作自主系统可信度评估框架的过程概述。

    

    随着自主系统在社会中变得越来越普遍，对于我们的安全和与它们的交互越来越频繁，使它们信得过是必要的。评估自主系统的可信度对于验证和开发社区来说是一项必要的挑战。这将需要适当的标准和适当的度量标准，可以在当前和未来的广泛应用范围内客观地和比较地判断AS的可信度。在文章中，考虑到文献中构成这个词的相关特质，对自主系统（AS）中的“可信度”一词进行了审查。回顾了支持自主系统保证的标准和框架的最新发展。我们列出了社区面临的关键挑战，并提出了一个可以用作自主系统可信度评估框架的过程概述。

    As Autonomous Systems (AS) become more ubiquitous in society, more responsible for our safety and our interaction with them more frequent, it is essential that they are trustworthy. Assessing the trustworthiness of AS is a mandatory challenge for the verification and development community. This will require appropriate standards and suitable metrics that may serve to objectively and comparatively judge trustworthiness of AS across the broad range of current and future applications. The meta-expression `trustworthiness' is examined in the context of AS capturing the relevant qualities that comprise this term in the literature. Recent developments in standards and frameworks that support assurance of autonomous systems are reviewed. A list of key challenges are identified for the community and we present an outline of a process that can be used as a trustworthiness assessment framework for AS.
    
[^28]: GPT用于半自动化数据科学：引入CAAFE实现上下文感知自动特征工程

    GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])

    [http://arxiv.org/abs/2305.03403](http://arxiv.org/abs/2305.03403)

    介绍了一种名为CAAFE的上下文感知自动特征工程方法，它利用大型语言模型根据数据集描述生成更多具有语义意义的特征，能够提高大多数数据集的性能，平均ROC AUC表现提高至0.822。

    

    随着自动化机器学习（AutoML）领域的发展，将领域知识纳入这些系统中变得越来越重要。我们利用大型语言模型（LLMs）的强大功能提出了一种方法来实现这一目标。具体地，我们介绍了一种用于表格数据的特征工程方法，名为上下文感知自动特征工程（CAAFE），它利用LLM根据数据集的描述生成更多具有语义意义的特征。该方法产生用于创建新特征的Python代码，并提供生成特征的效用说明。尽管方法论上很简单，但CAAFE提高了14个数据集中11个数据集的性能，与2个数据集并列，只有1个数据集性能下降，从而使所有数据集的平均ROC AUC表现从0.798提升至0.822。对于所评估的数据集，这一改进与使用随机森林（AUC 0.782）代替逻辑回归（AUC 0.754）所获得的平均改进相似。此外，

    As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
    
[^29]: 基于图中心性滤波剪枝的音频卷积神经网络压缩

    Compressing audio CNNs with graph centrality based filter pruning. (arXiv:2305.03391v1 [cs.SD])

    [http://arxiv.org/abs/2305.03391](http://arxiv.org/abs/2305.03391)

    本文提出一种基于图中心性的滤波器剪枝压缩框架，用于音频卷积神经网络，该方法可减少95％的计算和97％的存储。

    

    卷积神经网络（CNN）在解决许多现实世界问题，如音频分类中，已经成为常见的高性能解决方案。但是，CNN具有许多参数和滤波器，其中一些对性能的影响比其他更大。这意味着网络可能包含许多不必要的滤波器，增加了CNN的计算和内存需求，同时提供有限的性能优势。为了使CNN更高效，我们提出了一种剪枝框架，消除具有最高“共通性”的滤波器。我们使用图论的“中心性”概念来衡量这种共通性。我们假设具有高中心性的滤波器应该被消除，因为它代表了共通性，并且可以用其他滤波器替换而不太影响网络的性能。我们对提出的框架进行了实验评估，应用于声学场景分类和音频标记。在DCASE 2021任务1A基线网络上，我们的方法减少了95％的计算和97％的存储。

    Convolutional neural networks (CNNs) are commonplace in high-performing solutions to many real-world problems, such as audio classification. CNNs have many parameters and filters, with some having a larger impact on the performance than others. This means that networks may contain many unnecessary filters, increasing a CNN's computation and memory requirements while providing limited performance benefits. To make CNNs more efficient, we propose a pruning framework that eliminates filters with the highest "commonality". We measure this commonality using the graph-theoretic concept of "centrality". We hypothesise that a filter with a high centrality should be eliminated as it represents commonality and can be replaced by other filters without affecting the performance of a network much. An experimental evaluation of the proposed framework is performed on acoustic scene classification and audio tagging. On the DCASE 2021 Task 1A baseline network, our proposed method reduces computations p
    
[^30]: 解读"幽灵"：女性主义交叉XAI和制图作为解释隐形劳动的方法

    Explaining the ghosts: Feminist intersectional XAI and cartography as methods to account for invisible labour. (arXiv:2305.03376v1 [cs.HC])

    [http://arxiv.org/abs/2305.03376](http://arxiv.org/abs/2305.03376)

    AI需要大量的“无形劳动”，女性主义交叉XAI和制图是解释隐形劳动的方法。

    

    当代人工智能通过自动化需要大量的幕后人力劳动，这种劳动往往被隐瞒和低估。由于包括标记和维护工作在内的隐形劳动是当代人工智能系统的一个重要部分，因此让用户意识到这种劳动的作用依然很重要。我们建议可以通过可解释的AI（XAI）设计，特别是女性主义交叉XAI来实现这一点。我们提出了一种制图的方法，它来自女性主义交叉研究，以绘制AI的系统性视角，并包括与隐形劳动相关的AI维度。

    Contemporary automation through AI entails a substantial amount of behind-the-scenes human labour, which is often both invisibilised and underpaid. Since invisible labour, including labelling and maintenance work, is an integral part of contemporary AI systems, it remains important to sensitise users to its role. We suggest that this could be done through explainable AI (XAI) design, particularly feminist intersectional XAI. We propose the method of cartography, which stems from feminist intersectional research, to draw out a systemic perspective of AI and include dimensions of AI that pertain to invisible labour.
    
[^31]: 走向女性主义交叉XAI：从可解释性到响应能力

    Towards Feminist Intersectional XAI: From Explainability to Response-Ability. (arXiv:2305.03375v1 [cs.HC])

    [http://arxiv.org/abs/2305.03375](http://arxiv.org/abs/2305.03375)

    本文呼吁在计算和概念化方法中采用批判性方法，关注交叉的女性主义视角，并探讨在HCXAI研究和设计中实现女性主义交叉视角所需的响应能力和关注边缘化视角。

    

    本文呼应对计算和概念化的临床方法的呼吁，关注交叉的女性主义，去殖民化的人机交互和人工智能设计方面，并询问女性主义交叉视角在HCXAI研究和设计方面的可能性。勾画初步的研究方向和对可解释AI设计的影响，建议从女性主义的角度来看，可解释性应包括培养响应能力——批判评估和应对AI系统的能力——并将边缘化的视角置于中心位置。

    This paper follows calls for critical approaches to computing and conceptualisations of intersectional, feminist, decolonial HCI and AI design and asks what a feminist intersectional perspective in HCXAI research and design might look like. Sketching out initial research directions and implications for explainable AI design, it suggests that explainability from a feminist perspective would include the fostering of response-ability - the capacity to critically evaluate and respond to AI systems - and would centre marginalised perspectives.
    
[^32]: 离线基于模型的强化学习综述

    A Survey on Offline Model-Based Reinforcement Learning. (arXiv:2305.03360v1 [cs.LG])

    [http://arxiv.org/abs/2305.03360](http://arxiv.org/abs/2305.03360)

    本文是一篇关于离线基于模型的强化学习最新进展的综述性文章，讨论了该领域的最新概念、方法和挑战。文献综述介绍了当前离线基于模型的强化学习领域的关键文献，并重点讨论了如何解决分布变化问题。该领域面临的关键挑战也被讨论和总结。

    

    基于模型的方法在离线强化学习领域变得越来越流行，由于模型能够充分利用监督学习技术中大规模历史数据集的优势，在实际应用中具有非常高的潜力。本文对目前离线基于模型的强化学习研究进行了文献综述。综述简要介绍了离线强化学习和基于模型的强化学习的概念和最新发展，并探讨了两个领域的交叉点。接着，本文介绍了离线基于模型的强化学习领域的关键文献，并讨论了它们的方法，特别是它们在解决分布变化问题方面的方法，这是当前所有离线基于模型的强化学习方法面临的主要问题。本文还进一步讨论了该领域面临的关键挑战。

    Model-based approaches are becoming increasingly popular in the field of offline reinforcement learning, with high potential in real-world applications due to the model's capability of thoroughly utilizing the large historical datasets available with supervised learning techniques. This paper presents a literature review of recent work in offline model-based reinforcement learning, a field that utilizes model-based approaches in offline reinforcement learning. The survey provides a brief overview of the concepts and recent developments in both offline reinforcement learning and model-based reinforcement learning, and discuss the intersection of the two fields. We then presents key relevant papers in the field of offline model-based reinforcement learning and discuss their methods, particularly their approaches in solving the issue of distributional shift, the main problem faced by all current offline model-based reinforcement learning methods. We further discuss key challenges faced by
    
[^33]: 从解析-执行到解析-执行-优化：提高复杂问题答案基于知识库的语义解析器

    From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base. (arXiv:2305.03356v1 [cs.CL])

    [http://arxiv.org/abs/2305.03356](http://arxiv.org/abs/2305.03356)

    本文提出了一种名为解析-执行-优化（Parse-Execute-Refine）的范式，通过向知识库问题应答模型演示执行中间推理步骤，可以提高复杂推理的能力。

    

    把问题解析成可执行的逻辑形式对于知识库问题应答有了显著的结果。然而，复杂的知识库问题应答是一项更具挑战性的任务，需要进行复杂的多步推理。最近，提出了一种名为KoPL的新型语义解析器，旨在显式地模拟推理过程，在复杂知识库问题应答领域实现了最先进的结果。本文进一步探讨了如何通过一种简单的解析-执行-优化范式来开发语义解析器的推理能力。我们通过向知识库问题应答模型演示执行中间推理步骤来完善和改进KoPL解析器。我们表明，这样简单的策略可以显著提高复杂推理的能力。具体而言，我们提出了三个组成部分：解析阶段，执行阶段和优化阶段，以增强复杂推理的能力。解析器使用KoPL生成透明的逻辑形式。然后，执行阶段对齐和执行这些逻辑形式。

    Parsing questions into executable logical forms has showed impressive results for knowledge-base question answering (KBQA). However, complex KBQA is a more challenging task that requires to perform complex multi-step reasoning. Recently, a new semantic parser called KoPL has been proposed to explicitly model the reasoning processes, which achieved the state-of-the-art on complex KBQA. In this paper, we further explore how to unlock the reasoning ability of semantic parsers by a simple proposed parse-execute-refine paradigm. We refine and improve the KoPL parser by demonstrating the executed intermediate reasoning steps to the KBQA model. We show that such simple strategy can significantly improve the ability of complex reasoning. Specifically, we propose three components: a parsing stage, an execution stage and a refinement stage, to enhance the ability of complex reasoning. The parser uses the KoPL to generate the transparent logical forms. Then, the execution stage aligns and execute
    
[^34]: 数据集压缩综合研究：性能、隐私、鲁棒性以及公平性

    A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness. (arXiv:2305.03355v1 [cs.LG])

    [http://arxiv.org/abs/2305.03355](http://arxiv.org/abs/2305.03355)

    本研究对当前最先进的数据集压缩方法进行了全面评估，发现其存在隐私风险并可能放大模型的不公平性，提供了大规模的基准测试框架。

    

    数据集压缩旨在将原始数据集的丰富特征编码成小型数据集，是一种加速神经网络训练和相关研究的有前途的方法。已经提出了不同的方法来改善压缩图像的信息性和泛化性能。然而，目前还没有从安全性角度全面分析这一技术的工作，并且对潜在风险缺乏系统理解。在本文中，我们进行了大量实验，评估了当前最先进的数据集压缩方法。我们成功使用成员推理攻击来显示仍然存在隐私风险。本文还表明，数据集压缩在模型鲁棒性方面可能会产生不同程度的影响，并在进行预测时放大类别间的模型不公平性。本研究为数据集压缩评估提供了大规模的基准测试框架。

    The aim of dataset distillation is to encode the rich features of an original dataset into a tiny dataset. It is a promising approach to accelerate neural network training and related studies. Different approaches have been proposed to improve the informativeness and generalization performance of distilled images. However, no work has comprehensively analyzed this technique from a security perspective and there is a lack of systematic understanding of potential risks. In this work, we conduct extensive experiments to evaluate current state-of-the-art dataset distillation methods. We successfully use membership inference attacks to show that privacy risks still remain. Our work also demonstrates that dataset distillation can cause varying degrees of impact on model robustness and amplify model unfairness across classes when making predictions. This work offers a large-scale benchmarking framework for dataset distillation evaluation.
    
[^35]: MindGames：利用动态认知模态逻辑在大型语言模型中针对心智理论进行研究

    MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic. (arXiv:2305.03353v1 [cs.CL])

    [http://arxiv.org/abs/2305.03353](http://arxiv.org/abs/2305.03353)

    本文利用动态认知逻辑在自然语言处理模型中探讨了理解心智理论的方法。虽然GPT-4表现出改进的能力，但需要进一步提高。

    

    心智理论(ToM)是智能的重要组成部分，但准确度量它仍然是一个争议话题。先前的研究尝试将人类ToM评估应用于自然语言处理模型，使用人类创建的标准化测试或基于规则的模板。然而，这些方法主要集中在简单的推理上，并需要进一步验证。在本研究中，我们利用具有与ToM重叠的动态认知逻辑来生成更复杂的问题。我们还引入新的语言技巧来用自然语言表达这些问题。我们的研究结果表明，特定的语言模型缩放（从70M到6B和350M到174B）并不一致地产生比随机结果更好的结果。虽然GPT-4展示了改进的认知推理能力，但仍有提升空间。我们的代码和数据集可在以下链接公开获取：https://github.com/antoinelrnld/modlog https://huggingface.co/datasets/sileo

    Theory of Mind (ToM) is a critical component of intelligence, yet accurately measuring it continues to be a subject of debate. Prior research has attempted to apply human ToM assessments to natural language processing models using either human-created standardized tests or rule-based templates. However, these methods primarily focus on simplistic reasoning and require further validation. In this study, we utilize dynamic epistemic logic, which has established overlaps with ToM, to generate more intricate problems. We also introduce novel verbalization techniques to express these problems using natural language. Our findings indicate that certain language model scaling (from 70M to 6B and 350M to 174B) does not consistently yield results better than random chance. While GPT-4 demonstrates improved epistemic reasoning capabilities, there is still room for enhancement. Our code and datasets are publicly available https://github.com/antoinelrnld/modlog https://huggingface.co/datasets/sileo
    
[^36]: 定向演化和生态进化动力学的生物物理控制

    Biophysical Cybernetics of Directed Evolution and Eco-evolutionary Dynamics. (arXiv:2305.03340v1 [q-bio.PE])

    [http://arxiv.org/abs/2305.03340](http://arxiv.org/abs/2305.03340)

    本论文提出了一种新的方法，以部分可观察的马尔可夫过程模型为基础，分析生态演化动力学中生态和个体基因型/表型类型复杂性的不确定性。

    

    进化动力学中的许多重要问题可以在博弈理论背景下以随机轨迹分析的方式有意义地映射到分析中。通常的方法是分析少量不同的群体和/或假设动力学发生在人口规模大到足以使确定性轨迹成为现实的区域。被称为“生态演化动力学”的生态因素的添加进一步复杂化了动力学，并导致许多问题难以处理或当前的理论方法难以实际应用。但是，一种类似但未被充分探讨的方法是将重点放在模型本身的不确定性上，依据强化学习领域和相邻领域的研究人员的语言，而被称为“部分可观察马尔可夫过程”。在这里，我们介绍了一种对偶性，将同时考虑生态和个体基因型/表型类型的复杂性映射到一个新的马尔可夫过程模型上。

    Many major questions in the theory of evolutionary dynamics can in a meaningful sense be mapped to analyses of stochastic trajectories in game theoretic contexts. Often the approach is to analyze small numbers of distinct populations and/or to assume dynamics occur within a regime of population sizes large enough that deterministic trajectories are an excellent approximation of reality. The addition of ecological factors, termed "eco-evolutionary dynamics", further complicates the dynamics and results in many problems which are intractable or impractically messy for current theoretical methods. However, an analogous but underexplored approach is to analyze these systems with an eye primarily towards uncertainty in the models themselves. In the language of researchers in Reinforcement Learning and adjacent fields, a Partially Observable Markov Process. Here we introduce a duality which maps the complexity of accounting for both ecology and individual genotypic/phenotypic types onto a pr
    
[^37]: QCRI在SemEval-2023任务3 中的表现：使用多语言模型检测新闻类型、框架和说服技巧

    QCRI at SemEval-2023 Task 3: News Genre, Framing and Persuasion Techniques Detection using Multilingual Models. (arXiv:2305.03336v1 [cs.CL])

    [http://arxiv.org/abs/2305.03336](http://arxiv.org/abs/2305.03336)

    本文介绍了QCRI在SemEval-2023任务3中使用多语言模型成功排名前三的结果，并展示了去解决分析和验证在线新闻传播的难题。

    

    主流和社交媒体中的虚假信息传播一直以来以不同的方式误导用户。由记者和事实核查员进行手动检测和验证的努力已经无法应对虚假信息的快速扩散和大规模传播。这激励了研究和工业界的努力，以开发用于分析和验证在线新闻传播的系统。 SemEval-2023任务3 尝试解决在这个总体问题下的几个子任务，针对新闻文章中使用的写作技巧以影响读者的观点。该任务使用六种语言来解决三个子任务，此外还有三种“出人意料”的测试语言，共计27种不同的测试环境。本文描述了我们参与该任务的系统。我们团队是6个成功提交所有环境运行的团队之一。官方结果显示我们的系统在27种测试环境中有10个排名在前三位。

    Misinformation spreading in mainstream and social media has been misleading users in different ways. Manual detection and verification efforts by journalists and fact-checkers can no longer cope with the great scale and quick spread of misleading information. This motivated research and industry efforts to develop systems for analyzing and verifying news spreading online. The SemEval-2023 Task 3 is an attempt to address several subtasks under this overarching problem, targeting writing techniques used in news articles to affect readers' opinions. The task addressed three subtasks with six languages, in addition to three ``surprise'' test languages, resulting in 27 different test setups. This paper describes our participating system to this task. Our team is one of the 6 teams that successfully submitted runs for all setups. The official results show that our system is ranked among the top 3 systems for 10 out of the 27 setups.
    
[^38]: 以人为中心的信任框架：一个人机交互的视角

    Human-centered trust framework: An HCI perspective. (arXiv:2305.03306v1 [cs.HC])

    [http://arxiv.org/abs/2305.03306](http://arxiv.org/abs/2305.03306)

    本论文提出了以人为中心的信任框架，帮助非专家实现在人工智能设计中充分利用用户信任的潜力；相关研究发现了某些计算机科学和人工智能话语中的用户信任误解，并进行了有效性评估，该研究的主要贡献是为抵制设计以技术为中心的易受攻击的交互方式的趋势提供了指导。

    

    本文的出发点基于当前人工智能用户信任话题的讨论，旨在提出新颖的以信任为辅助的人机交互方法，从而促进当前技术的采用和应用。我们提出了一个框架（HCTFrame），以指导非专家在人工智能设计中充分利用用户信任的潜力。通过对三个文献综述的发现进行数据三角化，可消除计算机科学和人工智能话语中关于用户信任的一些误解，并进行三个案例研究，以评估心理测量量表在映射潜在用户信任破坏和担忧方面的有效性。本文主要贡献于抵制设计以技术为中心的易受攻击的交互方式的趋势，这可能最终导致更多实际和感知上的信任破裂。我们提出的框架可用于指导系统设计人员如何映射和定义用户信任以及社会伦理和组织需求。

    The rationale of this work is based on the current user trust discourse of Artificial Intelligence (AI). We aim to produce novel HCI approaches that use trust as a facilitator for the uptake (or appropriation) of current technologies. We propose a framework (HCTFrame) to guide non-experts to unlock the full potential of user trust in AI design. Results derived from a data triangulation of findings from three literature reviews demystify some misconceptions of user trust in computer science and AI discourse, and three case studies are conducted to assess the effectiveness of a psychometric scale in mapping potential users' trust breakdowns and concerns. This work primarily contributes to the fight against the tendency to design technical-centered vulnerable interactions, which can eventually lead to additional real and perceived breaches of trust. The proposed framework can be used to guide system designers on how to map and define user trust and the socioethical and organisational need
    
[^39]: 基于块的开放式信息抽取

    Open Information Extraction via Chunks. (arXiv:2305.03299v1 [cs.CL])

    [http://arxiv.org/abs/2305.03299](http://arxiv.org/abs/2305.03299)

    本文提出了一种句子作为块序列的方法，将块的跨度识别为元组关系和参数，采用Chunk-OIE进行元组提取并取得了最先进的结果。

    

    开放式信息抽取(OIE)致力于从开放领域的句子中提取关系元组。现有的OIE系统将句子拆成标记后识别标记的跨度作为元组关系和参数。相反，我们提出句子作为块序列(SaC)，并将块的跨度识别为元组关系和参数。我们认为，SaC对于OIE具有比标记序列更好的定量和定性特性，并针对四种块的选择（即CoNLL块、简单短语、NP块和SpanOIE的跨度）针对gold OIE元组进行评估。因此，我们提出了一种基于BERT的句子分块模型，并提出了在SaC之上进行的元组提取的Chunk-OIE。Chunk-OIE在多个OIE数据集上取得了最先进的成果，显示了SaC对OIE任务的益处。

    Open Information Extraction (OIE) aims to extract relational tuples from open-domain sentences. Existing OIE systems split a sentence into tokens and recognize token spans as tuple relations and arguments. We instead propose Sentence as Chunk sequence (SaC) and recognize chunk spans as tuple relations and arguments. We argue that SaC has better quantitative and qualitative properties for OIE than sentence as token sequence, and evaluate four choices of chunks (i.e., CoNLL chunks, simple phrases, NP chunks, and spans from SpanOIE) against gold OIE tuples. Accordingly, we propose a simple BERT-based model for sentence chunking, and propose Chunk-OIE for tuple extraction on top of SaC. Chunk-OIE achieves state-of-the-art results on multiple OIE datasets, showing that SaC benefits OIE task.
    
[^40]: BadSAM：通过后门攻击探索SAM的安全漏洞

    BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks. (arXiv:2305.03289v1 [cs.CV])

    [http://arxiv.org/abs/2305.03289](http://arxiv.org/abs/2305.03289)

    本文介绍了BadSAM，这是对图像分割基础模型的首次后门攻击，该攻击将导致定制化SAM模型面临更加严峻的安全挑战。

    

    最近，由于其在各种下游任务中的强大性能，分割任何模型（SAM）以图像分割基础模型而备受关注。然而，已经发现当面对具有挑战性的下游任务时，SAM并不总是表现出色。这导致下游用户要求定制的SAM模型，以适应这些下游任务。在本文中，我们提出BadSAM，这是对图像分割基础模型的首次后门攻击。我们在CAMO数据集上的初步实验展示了BadSAM的有效性。

    Recently, the Segment Anything Model (SAM) has gained significant attention as an image segmentation foundation model due to its strong performance on various downstream tasks. However, it has been found that SAM does not always perform satisfactorily when faced with challenging downstream tasks. This has led downstream users to demand a customized SAM model that can be adapted to these downstream tasks. In this paper, we present BadSAM, the first backdoor attack on the image segmentation foundation model. Our preliminary experiments on the CAMO dataset demonstrate the effectiveness of BadSAM.
    
[^41]: 基于多个提示知识的低资源多粒度学术功能识别

    Low-Resource Multi-Granularity Academic Function Recognition Based on Multiple Prompt Knowledge. (arXiv:2305.03287v1 [cs.CL])

    [http://arxiv.org/abs/2305.03287](http://arxiv.org/abs/2305.03287)

    本研究提出了 Mix Prompt Tuning（MPT）方法，通过将手动提示模板与自动学习的连续提示模板相结合，提高多粒度学术功能识别任务的性能，并减轻对注释数据的依赖。

    

    在科学领域的自然语言处理任务中，Fine-tuning 预训练语言模型（PLMs），如 SciBERT，通常需要大量注释数据才能实现最先进的性能。但是，获取科学 NLP 任务的 fine-tune 数据仍然具有挑战性和昂贵性。受提示学习的最新进展启发，本文提出了 Mix Prompt Tuning（MPT）方法，这是一种半监督方法，旨在减轻对注释数据的依赖，并使用很少数量的标记示例提高多粒度学术功能识别任务的性能。具体而言，所提出的方法通过将手动提示模板与自动学习的连续提示模板相结合，提供多方面的表示，以帮助给定的学术功能识别任务充分利用 PLMs 中的知识。基于这些提示模板和 fine-tuned PLM，大量的伪标签被分配给未标记的实例，以提高性能。

    Fine-tuning pre-trained language models (PLMs), e.g., SciBERT, generally requires large numbers of annotated data to achieve state-of-the-art performance on a range of NLP tasks in the scientific domain. However, obtaining the fine-tune data for scientific NLP task is still challenging and expensive. Inspired by recent advancement in prompt learning, in this paper, we propose the Mix Prompt Tuning (MPT), which is a semi-supervised method to alleviate the dependence on annotated data and improve the performance of multi-granularity academic function recognition tasks with a small number of labeled examples. Specifically, the proposed method provides multi-perspective representations by combining manual prompt templates with automatically learned continuous prompt templates to help the given academic function recognition task take full advantage of knowledge in PLMs. Based on these prompt templates and the fine-tuned PLM, a large number of pseudo labels are assigned to the unlabeled exam
    
[^42]: 使用任务控制的复合动作学习

    Composite Motion Learning with Task Control. (arXiv:2305.03286v1 [cs.GR])

    [http://arxiv.org/abs/2305.03286](http://arxiv.org/abs/2305.03286)

    该论文介绍了一种使用多个判别器在类 GAN 设置中直接学习来自多个参考动作的特定身体部位的解耦动作的深度学习方法，以实现复合和任务驱动的动作控制，同时考虑了多个任务的奖励和训练一个单一的多目标控制策略。

    

    我们提出了一种深度学习方法，用于物理模拟角色的复合和任务驱动的动作控制。与使用强化学习模仿全身动作的现有数据驱动方法不同，我们通过在类 GAN 设置中利用多个判别器直接同时学习来自多个参考动作的特定身体部位的解耦动作。在这个过程中，不需要手动制作用于学习的复合参考动作。相反，控制策略自行探索如何自动地组合复合动作。我们进一步考虑了多个任务特定奖励，并训练一个单一的多目标控制策略。为此，我们提出了一个新的多目标学习框架，自适应平衡来自多个来源和多个目标定向控制目标的不同运动的学习。此外，由于复合动作通常是更简单行为的增强。

    We present a deep learning method for composite and task-driven motion control for physically simulated characters. In contrast to existing data-driven approaches using reinforcement learning that imitate full-body motions, we learn decoupled motions for specific body parts from multiple reference motions simultaneously and directly by leveraging the use of multiple discriminators in a GAN-like setup. In this process, there is no need of any manual work to produce composite reference motions for learning. Instead, the control policy explores by itself how the composite motions can be combined automatically. We further account for multiple task-specific rewards and train a single, multi-objective control policy. To this end, we propose a novel framework for multi-objective learning that adaptively balances the learning of disparate motions from multiple sources and multiple goal-directed control objectives. In addition, as composite motions are typically augmentations of simpler behavio
    
[^43]: 使用视觉变换器的语义分割：一项调查

    Semantic Segmentation using Vision Transformers: A survey. (arXiv:2305.03273v1 [cs.CV])

    [http://arxiv.org/abs/2305.03273](http://arxiv.org/abs/2305.03273)

    本文介绍了使用视觉变换器（ViT）进行语义分割的不同架构，讨论了ViT在分块划分方案方面的局限性，并回顾和比较了不同ViT架构的性能表现。ViT架构的崛起在计算机视觉任务中替代传统的卷积神经网络的趋势不断增强。

    

    语义分割在各个领域中有广泛的应用，包括土地覆盖分析、自动驾驶和医学图像分析。卷积神经网络（CNN）和视觉变换器（ViT）为语义分割提供了架构模型。尽管ViT在图像分类方面取得了成功，但由于其分块划分方案，无法直接应用于像图像分割和目标检测这样的密集预测任务，因此ViT不是一种通用的主干架构。本调查讨论了一些可用于语义分割的不同ViT架构以及它们如何应对上述挑战。ViT的崛起以及其高成功率的表现，促使社区逐渐在各种计算机视觉任务中取代传统的卷积神经网络。本调查旨在回顾和比较设计用于语义分割的ViT架构的性能。

    Semantic segmentation has a broad range of applications in a variety of domains including land coverage analysis, autonomous driving, and medical image analysis. Convolutional neural networks (CNN) and Vision Transformers (ViTs) provide the architecture models for semantic segmentation. Even though ViTs have proven success in image classification, they cannot be directly applied to dense prediction tasks such as image segmentation and object detection since ViT is not a general purpose backbone due to its patch partitioning scheme. In this survey, we discuss some of the different ViT architectures that can be used for semantic segmentation and how their evolution managed the above-stated challenge. The rise of ViT and its performance with a high success rate motivated the community to slowly replace the traditional convolutional neural networks in various computer vision tasks. This survey aims to review and compare the performances of ViT architectures designed for semantic segmentati
    
[^44]: 限制认知负荷的贝叶斯强化学习

    Bayesian Reinforcement Learning with Limited Cognitive Load. (arXiv:2305.03263v1 [cs.LG])

    [http://arxiv.org/abs/2305.03263](http://arxiv.org/abs/2305.03263)

    本文综述了限制认知负荷的贝叶斯强化学习的最新算法和理论成果，并讨论了这些思想如何应用于研究认知和行为科学中的问题。

    

    所有的生物和人工智能代理都必须在信息处理能力上有限制下进行学习和决策。因此，一般的自适应行为理论应该能够说明代理的学习历史、决策和能力限制之间的复杂互动。最近，计算机科学领域的研究开始通过将强化学习、贝叶斯决策和速率-失真理论的思想融合，澄清塑造这些动态的原则。这一领域的工作提供了一种会考虑处理限制对学习和动作选择的影响的统一规范框架，即容量有限的贝叶斯强化学习。在这里，我们提供了一篇易于理解的综述，介绍了这一领域的最新算法和理论成果，特别关注这些思想如何应用于研究认知和行为科学中的问题。

    All biological and artificial agents must learn and make decisions given limits on their ability to process information. As such, a general theory of adaptive behavior should be able to account for the complex interactions between an agent's learning history, decisions, and capacity constraints. Recent work in computer science has begun to clarify the principles that shape these dynamics by bridging ideas from reinforcement learning, Bayesian decision-making, and rate-distortion theory. This body of work provides an account of capacity-limited Bayesian reinforcement learning, a unifying normative framework for modeling the effect of processing constraints on learning and action selection. Here, we provide an accessible review of recent algorithms and theoretical results in this setting, paying special attention to how these ideas can be applied to studying questions in the cognitive and behavioral sciences.
    
[^45]: 基于RGB-D语义分割的服装抓取和展开

    Clothes Grasping and Unfolding Based on RGB-D Semantic Segmentation. (arXiv:2305.03259v1 [cs.CV])

    [http://arxiv.org/abs/2305.03259](http://arxiv.org/abs/2305.03259)

    本论文针对机器人辅助穿衣过程中的服装抓取和展开问题，提出了一种基于RGB-D语义分割的双向分形交叉融合网络，可以识别可抓取区域提供更多抓取可能性，并且使用RGB和深度数据融合来考虑图像信息。

    

    服装的抓取和展开是机器人辅助穿衣的核心步骤。大多数现有作品利用服装的深度图像训练深度学习模型以识别合适的抓取点。然而，这些方法通常利用物理引擎来合成深度图像以减少真实标记数据的收集成本。然而，合成与真实图像之间的自然领域差距常常导致这些方法在实际数据上表现不佳。此外，这些方法通常在抓取点被服装物品本身遮挡的场景中难以处理。为了解决上述挑战，我们提出了一种新颖的双向分形交叉融合网络（BiFCNet）进行语义分割，实现对可抓取区域的识别以提供更多的抓取可能性。我们不仅使用深度图像，还利用具有丰富色彩特征的RGB图像作为网络输入，其中分形交叉融合（FCF）模块通过将RGB和深度数据融合来考虑图像信息。

    Clothes grasping and unfolding is a core step in robotic-assisted dressing. Most existing works leverage depth images of clothes to train a deep learning-based model to recognize suitable grasping points. These methods often utilize physics engines to synthesize depth images to reduce the cost of real labeled data collection. However, the natural domain gap between synthetic and real images often leads to poor performance of these methods on real data. Furthermore, these approaches often struggle in scenarios where grasping points are occluded by the clothing item itself. To address the above challenges, we propose a novel Bi-directional Fractal Cross Fusion Network (BiFCNet) for semantic segmentation, enabling recognition of graspable regions in order to provide more possibilities for grasping. Instead of using depth images only, we also utilize RGB images with rich color features as input to our network in which the Fractal Cross Fusion (FCF) module fuses RGB and depth data by consid
    
[^46]: 考虑多轮对话上下文的领域外意图检测

    Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts. (arXiv:2305.03237v1 [cs.CL])

    [http://arxiv.org/abs/2305.03237](http://arxiv.org/abs/2305.03237)

    本文提出了一个上下文感知的OOD意图检测框架（Caro），用于模拟OOD意图检测任务中的多轮对话上下文，并在提取稳健的表示时删除与意图检测无关的多余信息。Caro在多个标准数据集上表现出最先进的性能，并超越了先前方法。

    

    领域外（OOD）意图检测对于实用的对话系统非常重要，通常需要考虑多轮对话上下文。然而，大多数先前的OOD意图检测方法仅限于单轮对话。在本文中，我们介绍了一个上下文感知的OOD意图检测（Caro）框架，用于对OOD意图检测任务中的多轮上下文进行建模。具体地，我们遵循信息瓶颈原则从多轮对话上下文中提取稳健的表示。每个输入样本构建了两个不同的视角，使用多视图信息瓶颈损失删除与意图检测无关的多余信息。此外，我们还探索了在Caro中利用未标记的数据。引入了一个两阶段训练过程来从这些未标记的数据中挖掘OOD样本，并使用自举方法用这些OOD样本来训练生成的模型。全面的实验表明，Caro在OOD意图检测任务的几个基准数据集上建立了最先进的性能，并超越了仅考虑单轮上下文的先前方法。

    Out-of-Domain (OOD) intent detection is vital for practical dialogue systems, and it usually requires considering multi-turn dialogue contexts. However, most previous OOD intent detection approaches are limited to single dialogue turns. In this paper, we introduce a context-aware OOD intent detection (Caro) framework to model multi-turn contexts in OOD intent detection tasks. Specifically, we follow the information bottleneck principle to extract robust representations from multi-turn dialogue contexts. Two different views are constructed for each input sample and the superfluous information not related to intent detection is removed using a multi-view information bottleneck loss. Moreover, we also explore utilizing unlabeled data in Caro. A two-stage training process is introduced to mine OOD samples from these unlabeled data, and these OOD samples are used to train the resulting model with a bootstrapping approach. Comprehensive experiments demonstrate that Caro establishes state-of-
    
[^47]: 自然语言处理中基于外部分布检测的综述

    A Survey on Out-of-Distribution Detection in NLP. (arXiv:2305.03236v1 [cs.CL])

    [http://arxiv.org/abs/2305.03236](http://arxiv.org/abs/2305.03236)

    这篇论文首次综述了最新的OOD检测方法在自然语言处理中的应用。根据算法使用的数据，将方法分成三类，并介绍了相关数据集、应用和度量方法。

    

    在现实世界中，基于外部分布（OOD）的检测对于机器学习系统的可靠和安全的部署至关重要。过去几年取得了极大进展。本文重点关注自然语言处理方法，并首次综述了OOD检测方面的最新进展。首先，我们给出OOD检测的正式定义，并讨论了几个相关领域。然后，根据算法使用的数据，将最近的算法分成三类：（1）可用OOD数据，（2）OOD数据不可用+内部分布（ID）标签可用，（3）OOD数据不可用+ID标签不可用。第三，介绍数据集、应用和度量方法。最后，总结现有工作并提出潜在的未来研究课题。

    Out-of-distribution (OOD) detection is essential for the reliable and safe deployment of machine learning systems in the real world. Great progress has been made over the past years. This paper presents the first review of recent advances in OOD detection with a particular focus on natural language processing approaches. First, we provide a formal definition of OOD detection and discuss several related fields. We then categorize recent algorithms into three classes according to the data they used: (1) OOD data available, (2) OOD data unavailable + in-distribution (ID) label available, and (3) OOD data unavailable + ID label unavailable. Third, we introduce datasets, applications, and metrics. Finally, we summarize existing work and present potential future research topics.
    
[^48]: 基于神经网络的3D模拟超分辨率实现近实时面部动画表现

    Near-realtime Facial Animation by Deep 3D Simulation Super-Resolution. (arXiv:2305.03216v1 [cs.GR])

    [http://arxiv.org/abs/2305.03216](http://arxiv.org/abs/2305.03216)

    该论文提出了一种基于神经网络的3D模拟超分辨率框架，能够高效、逼真地增强低成本、实时物理模拟产生的面部表现，使其接近于具有更高分辨率和准确物理建模的参考质量离线模拟器。

    

    我们提出了一种基于神经网络的模拟超分辨率框架，能够高效、逼真地增强低成本、实时物理模拟产生的面部表现，使其接近于具有更高分辨率（在我们的实验中高达26倍的元素数）和准确物理建模的参考质量离线模拟器。我们的方法源于我们通过模拟构建一组配对帧序列的能力，这些序列分别来自于低分辨率和高分辨率模拟器，并且在语义上相互对应。我们以面部动画为例，创造这种语义一致性的方式就是在两个模拟器中调整同样的肌肉激活控制和骨架姿势。我们提出的神经网络超分辨率框架从这个训练集中泛化到看不见的表情，并且补偿两个模拟之间的建模差异。

    We present a neural network-based simulation super-resolution framework that can efficiently and realistically enhance a facial performance produced by a low-cost, realtime physics-based simulation to a level of detail that closely approximates that of a reference-quality off-line simulator with much higher resolution (26x element count in our examples) and accurate physical modeling. Our approach is rooted in our ability to construct - via simulation - a training set of paired frames, from the low- and high-resolution simulators respectively, that are in semantic correspondence with each other. We use face animation as an exemplar of such a simulation domain, where creating this semantic congruence is achieved by simply dialing in the same muscle actuation controls and skeletal pose in the two simulators. Our proposed neural network super-resolution framework generalizes from this training set to unseen expressions, compensates for modeling discrepancies between the two simulations du
    
[^49]: LLM2Loss: 利用语言模型进行可解释的模型诊断

    LLM2Loss: Leveraging Language Models for Explainable Model Diagnostics. (arXiv:2305.03212v1 [cs.CV])

    [http://arxiv.org/abs/2305.03212](http://arxiv.org/abs/2305.03212)

    LLM2Loss利用语言模型为黑盒模型提供可解释的模型诊断，通过提取数据点的语义特征，揭示导致模型失败和偏差的因素。

    

    在大量数据的训练下，大型语言模型已经在抽象空间中对相当复杂的文本输入进行建模，取得了前所未有的成功和概括能力，从而成为了零样本学习的强大工具。这种能力可以通过跨模态基础模型，如CLIP，来扩展到其他模态，如视觉域，从而可以从视觉输入中提取语义上有意义的表示。在本文中，我们利用这种能力，并提出一种方法，可以提供模型失败和偏差模式的语义见解。对于给定的黑盒模型，其训练数据和任务定义，我们首先计算其每个数据点的任务相关损失。然后，我们提取每个训练数据点的语义上有意义的表示（例如来自其视觉编码器的CLIP嵌入），并训练一个轻量级的诊断模型，将这个数据点的语义上有意义的表示映射到其任务损失。我们展示了该方法不仅可以识别对模型难以学习的数据点，而且可以识别导致模型失败和偏差的因素。

    Trained on a vast amount of data, Large Language models (LLMs) have achieved unprecedented success and generalization in modeling fairly complex textual inputs in the abstract space, making them powerful tools for zero-shot learning. Such capability is extended to other modalities such as the visual domain using cross-modal foundation models such as CLIP, and as a result, semantically meaningful representation are extractable from visual inputs.  In this work, we leverage this capability and propose an approach that can provide semantic insights into a model's patterns of failures and biases. Given a black box model, its training data, and task definition, we first calculate its task-related loss for each data point. We then extract a semantically meaningful representation for each training data point (such as CLIP embeddings from its visual encoder) and train a lightweight diagnosis model which maps this semantically meaningful representation of a data point to its task loss. We show 
    
[^50]: 研究印度语系多语言机器翻译中的词汇共享

    Investigating Lexical Sharing in Multilingual Machine Translation for Indian Languages. (arXiv:2305.03207v1 [cs.CL])

    [http://arxiv.org/abs/2305.03207](http://arxiv.org/abs/2305.03207)

    本文研究了印度语系多语言机器翻译中的词汇共享问题，探索了数据采样和词汇量之间的翻译性能权衡，以及字母转写是否有助于促进跨脚本概括，发现字母转写对跨脚本翻译效果没有明显改进，对于较低资源的语言，多语言机器翻译模型训练在原始脚本上似乎已经能够抵抗脚本差异。

    

    多语言语言模型表现出惊人的跨语言转移能力。为了改进这些模型的跨语言能力，一些策略包括改进字符细分而不是子词和对字母转写的使用。在本文中，我们研究了从印地语、古吉拉特语、尼泊尔语到英语的多语言机器翻译中的词汇共享。我们探讨了在数据采样和词汇量之间存在的翻译性能权衡，并探讨了字母转写是否有助于促进跨脚本概括。我们还验证了不同设置如何推广到看不见的语言（马拉地语和孟加拉语）。我们发现字母转写没有明显的改进，我们的分析表明，我们的多语言机器翻译模型在原始脚本上训练时，即使对于相对低资源的语言，似乎已经能够抵抗脚本差异。

    Multilingual language models have shown impressive cross-lingual transfer ability across a diverse set of languages and tasks. To improve the cross-lingual ability of these models, some strategies include transliteration and finer-grained segmentation into characters as opposed to subwords. In this work, we investigate lexical sharing in multilingual machine translation (MT) from Hindi, Gujarati, Nepali into English. We explore the trade-offs that exist in translation performance between data sampling and vocabulary size, and we explore whether transliteration is useful in encouraging cross-script generalisation. We also verify how the different settings generalise to unseen languages (Marathi and Bengali). We find that transliteration does not give pronounced improvements and our analysis suggests that our multilingual MT models trained on original scripts seem to already be robust to cross-script differences even for relatively low-resource languages
    
[^51]: Gpt-4：自然语言处理中的进展和机遇综述

    Gpt-4: A Review on Advancements and Opportunities in Natural Language Processing. (arXiv:2305.03195v1 [cs.CL])

    [http://arxiv.org/abs/2305.03195](http://arxiv.org/abs/2305.03195)

    GPT-4是OpenAI开发的第四代GPT语言模型，具有超过1万亿的模型规模、更好的多语言能力和改进的语境理解和推理能力。它有望应用于聊天机器人、个人助理、语言翻译、文本摘要和问答系统，但也存在计算要求、数据要求和道德问题等挑战和限制。

    

    “生成式预训练变换器”（GPT）系列的第四代语言模型GPT-4由OpenAI开发，有望在自然语言处理领域实现显著进展。本研究论文讨论了GPT-4的特点、潜在应用以及可能面临的挑战，并将GPT-4与其前身GPT-3进行了比较。相比GPT-3，GPT-4具有更大的模型规模（超过1万亿）、更好的多语言能力、改进的语境理解和推理能力。GPT-4的一些潜在应用包括聊天机器人、个人助理、语言翻译、文本摘要和问答系统。然而，GPT-4也存在诸多挑战和限制，如计算要求、数据要求和道德问题。

    Generative Pre-trained Transformer 4 (GPT-4) is the fourth-generation language model in the GPT series, developed by OpenAI, which promises significant advancements in the field of natural language processing (NLP). In this research article, we have discussed the features of GPT-4, its potential applications, and the challenges that it might face. We have also compared GPT-4 with its predecessor, GPT-3. GPT-4 has a larger model size (more than one trillion), better multilingual capabilities, improved contextual understanding, and reasoning capabilities than GPT-3. Some of the potential applications of GPT-4 include chatbots, personal assistants, language translation, text summarization, and question-answering. However, GPT-4 poses several challenges and limitations such as computational requirements, data requirements, and ethical concerns.
    
[^52]: Smaller3d：使用Minkowski Engine和知识蒸馏方法进行3D语义分割的小型模型

    Smaller3d: Smaller Models for 3D Semantic Segmentation Using Minkowski Engine and Knowledge Distillation Methods. (arXiv:2305.03188v1 [cs.CV])

    [http://arxiv.org/abs/2305.03188](http://arxiv.org/abs/2305.03188)

    本文提出了一种基于知识蒸馏技术的稀疏张量在3D深度学习中的小型模型，通过采用不同的损失函数，在保持性能的情况下减小模型的大小并获得优于现有模型的结果。

    

    在3D领域，有许多优化技术，包括基于点云的方法，使用网格、纹理和体素来优化3D存储和计算。这篇论文提出了为了在保持性能的同时减小模型大小而采用的知识蒸馏技术，特别是针对稀疏张量在3D深度学习中的应用。我们分析和提出了不同的损失函数，包括标准方法和各种损失的组合，以模拟不同稀疏卷积NN的最先进模型的性能。我们的实验是在标准的ScanNet V2数据集上进行的，并且我们的结果优于现有的最佳模型。

    There are various optimization techniques in the realm of 3D, including point cloud-based approaches that use mesh, texture, and voxels which optimize how you store, and how do calculate in 3D. These techniques employ methods such as feed-forward networks, 3D convolutions, graph neural networks, transformers, and sparse tensors. However, the field of 3D is one of the most computationally expensive fields, and these methods have yet to achieve their full potential due to their large capacity, complexity, and computation limits. This paper proposes the application of knowledge distillation techniques, especially for sparse tensors in 3D deep learning, to reduce model sizes while maintaining performance. We analyze and purpose different loss functions, including standard methods and combinations of various losses, to simulate the performance of state-of-the-art models of different Sparse Convolutional NNs. Our experiments are done on the standard ScanNet V2 dataset, and we achieved around
    
[^53]: POET: PROFINET工业操作行为的自学习框架

    POET: A Self-learning Framework for PROFINET Industrial Operations Behaviour. (arXiv:2305.03175v1 [cs.CR])

    [http://arxiv.org/abs/2305.03175](http://arxiv.org/abs/2305.03175)

    提出了一种名为POET的基于Python的框架，能自学习PROFINET网络的基础设施信息，包括PLCs、现场设备和人机界面，利用知识图来优化异常检测和分类结果，能有效提高工业网络安全。

    

    自2010年以来，多次对工业基础设施的网络攻击事件（例如Stuxnet和CrashOverride）一再暴露了工业控制系统（ICS）对网络威胁的脆弱性。工业系统的投入使用时间长，长达几十年，往往导致与工业网络安全机制的技术进展不符。网络基础设施信息的不可用性使得设计安全策略或配置网络入侵检测系统（NIDS）等网络安全对策变得具有挑战性。一个实证的解决方案是从监视的网络流量中自学习工业系统的网络基础设施信息，以使网络对下游分析任务（例如异常检测）透明化。在这项工作中，提出了一种基于Python的PROFINET通信范式感知框架，名为“PROFINET操作枚举和跟踪”（POET），用于枚举可编程逻辑控制器（PLC）执行的不同工业操作，并构建工业通信的知识图。POET被设计为从网络流量中自学习网络基础设施信息，包括PLCs、现场设备和人机界面，并使用知识图来优化异常检测和分类结果。在PROFINET测试平台上进行的实验评估证明了所提出的POET框架在工业网络安全上的高效性和有效性。

    Since 2010, multiple cyber incidents on industrial infrastructure, such as Stuxnet and CrashOverride, have exposed the vulnerability of Industrial Control Systems (ICS) to cyber threats. The industrial systems are commissioned for longer duration amounting to decades, often resulting in non-compliance to technological advancements in industrial cybersecurity mechanisms. The unavailability of network infrastructure information makes designing the security policies or configuring the cybersecurity countermeasures such as Network Intrusion Detection Systems (NIDS) challenging. An empirical solution is to self-learn the network infrastructure information of an industrial system from its monitored network traffic to make the network transparent for downstream analyses tasks such as anomaly detection. In this work, a Python-based industrial communication paradigm-aware framework, named PROFINET Operations Enumeration and Tracking (POET), that enumerates different industrial operations execut
    
[^54]: 基于情感分析的新型对抗性图像检测方法

    New Adversarial Image Detection Based on Sentiment Analysis. (arXiv:2305.03173v1 [cs.CR])

    [http://arxiv.org/abs/2305.03173](http://arxiv.org/abs/2305.03173)

    本文提出了一种新的对抗性样本检测器，通过使用情感分析并检测对神经网络的隐藏层特征图造成的影响逐渐显现来检测对抗性样本，克服了现有技术的局限，且实验结果表明其在识别最新的对抗性攻击方面优于现有技术。

    

    深度神经网络（DNN）易受到对抗样本的攻击，而对抗攻击模型（例如DeepFool）正在崛起并超越对抗性样本检测技术。本文提出了一种新的对抗性样本检测器，可在识别图像数据集上的最新对抗性攻击方面超越现有技术。具体而言，我们提出使用情感分析来检测对抗性样本，这是通过检测对神经网络的隐藏层特征图造成的影响逐渐显现来实现的。因此，我们设计一个具有最少可学习参数的模块化嵌入层，将隐藏层特征映射到词向量中，组成可以进行情感分析的句子。大量实验表明，这种新型检测器在检测针对CIFAR-10、SVHN和ImageNet数据集上的ResNet和Inception中性网络发起的最新攻击方面均能一致超越最先进的检测算法。

    Deep Neural Networks (DNNs) are vulnerable to adversarial examples, while adversarial attack models, e.g., DeepFool, are on the rise and outrunning adversarial example detection techniques. This paper presents a new adversarial example detector that outperforms state-of-the-art detectors in identifying the latest adversarial attacks on image datasets. Specifically, we propose to use sentiment analysis for adversarial example detection, qualified by the progressively manifesting impact of an adversarial perturbation on the hidden-layer feature maps of a DNN under attack. Accordingly, we design a modularized embedding layer with the minimum learnable parameters to embed the hidden-layer feature maps into word vectors and assemble sentences ready for sentiment analysis. Extensive experiments demonstrate that the new detector consistently surpasses the state-of-the-art detection algorithms in detecting the latest attacks launched against ResNet and Inception neutral networks on the CIFAR-1
    
[^55]: 构建可逆的语义保持的逻辑公式嵌入：一种基于 Graph VAE 的深度学习方法

    Towards Invertible Semantic-Preserving Embeddings of Logical Formulae. (arXiv:2305.03143v1 [cs.AI])

    [http://arxiv.org/abs/2305.03143](http://arxiv.org/abs/2305.03143)

    本研究提出了一种基于 Graph VAE 的深度学习模型ISPE，能够在连续空间中保持语义的嵌入逻辑公式，并且该方法是可逆的。在语句补全和推理等任务上的表现优于现有方法。

    

    逻辑是自动推理的主要形式语言，同时也是一种可解释的语言。学习和优化逻辑规则一直是人工智能领域的重要问题。然而，现有的机器学习方法是基于连续空间的梯度下降优化，而学习逻辑则处于离散的语法空间中。解决这个挑战，需要提出一种能在连续空间中保持语义的嵌入方法，目前的方法虽然能保持语义，但是不可逆。本文提出了一种新的模型，即可逆语义保持嵌入（ISPE），利用基于 Graph VAE 的深度结构实现了嵌入。该方法在语句补全和推理等任务上表现优异，超过了现有的方法。

    Logic is the main formal language to perform automated reasoning, and it is further a human-interpretable language, at least for small formulae. Learning and optimising logic requirements and rules has always been an important problem in Artificial Intelligence. State of the art Machine Learning (ML) approaches are mostly based on gradient descent optimisation in continuous spaces, while learning logic is framed in the discrete syntactic space of formulae. Using continuous optimisation to learn logic properties is a challenging problem, requiring to embed formulae in a continuous space in a meaningful way, i.e. preserving the semantics. Current methods are able to construct effective semantic-preserving embeddings via kernel methods (for linear temporal logic), but the map they define is not invertible. In this work we address this problem, learning how to invert such an embedding leveraging deep architectures based on the Graph Variational Autoencoder framework. We propose a novel mod
    
[^56]: ChatGPT 需要进行SPADE（可持续性、隐私、数字鸿沟和伦理）评估：一项综述。

    ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])

    [http://arxiv.org/abs/2305.03123](http://arxiv.org/abs/2305.03123)

    本文研究关注ChatGPT面临的可持续性、隐私、数字鸿沟和伦理问题，提出了SPADE评估的必要性，并给出了缓解和建议。

    

    ChatGPT是另一个大型语言模型（LLM），由于其性能和有效的对话能力，在研究和工业界中得到了巨大的关注。最近，许多研究已经发表，以展示ChatGPT和其他LLMs的有效性、效率、集成和情感。相反，本研究关注的是大多数被忽视的重要方面，即可持续性、隐私、数字鸿沟和伦理，并建议不仅仅是ChatGPT，而是在对话机器人类别中的每一个后续入口都应该进行SPADE评估。本文详细讨论了关于ChatGPT的问题和关注点与上述特征一致。我们通过一些初步的数据收集和可视化以及假设的事实来支持我们的假设。我们还为每个问题提出了缓解和建议。此外，我们还提供了一些未来方向和开放问题的探讨。

    ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
    
[^57]: HAISTA-NET: 通过注意力进行人类辅助的实例分割

    HAISTA-NET: Human Assisted Instance Segmentation Through Attention. (arXiv:2305.03105v1 [cs.CV])

    [http://arxiv.org/abs/2305.03105](http://arxiv.org/abs/2305.03105)

    该论文提出了一种通过人类辅助实例分割方法，称为HAISTA-NET，增强了现有的实例分割网络，引入了人类指定的部分边界地图，以生成更精确的分割掩模。

    

    实例分割是图像检测的一种形式，在物体细化、医学图像分析和图像/视频编辑等方面有广泛应用，这些应用都需要高度精确的结果。然而，即便是最先进的完全自动化实例分割算法，其精度常常无法达到。对于小而复杂的对象来说，性能差距尤为明显。通常，从业者只能采用完全手动的注释方法，这可能是一个繁琐的过程。为了克服这个问题，我们提出了一种新的方法，以实现更精确的预测，并为高曲率、复杂和小规模对象生成更高质量的分割掩模。我们的人类辅助分割模型HAISTA-NET，扩充了现有的Strong Mask R-CNN网络，以包括人类指定的部分边界。此外，我们还提出了一个手绘部分物体边界的数据集，称为人类注意力地图。我们还引入了一种新的损失函数，它考虑到了部分注意力地图和原始掩模提议，这使网络能够关注人们认为最重要的区域。在PASCAL VOC和COCO数据集上的实验结果表明，我们提出的方法优于强基线，并在小而复杂对象实例分割方面实现了最先进的性能。

    Instance segmentation is a form of image detection which has a range of applications, such as object refinement, medical image analysis, and image/video editing, all of which demand a high degree of accuracy. However, this precision is often beyond the reach of what even state-of-the-art, fully automated instance segmentation algorithms can deliver. The performance gap becomes particularly prohibitive for small and complex objects. Practitioners typically resort to fully manual annotation, which can be a laborious process. In order to overcome this problem, we propose a novel approach to enable more precise predictions and generate higher-quality segmentation masks for high-curvature, complex and small-scale objects. Our human-assisted segmentation model, HAISTA-NET, augments the existing Strong Mask R-CNN network to incorporate human-specified partial boundaries. We also present a dataset of hand-drawn partial object boundaries, which we refer to as human attention maps. In addition, 
    
[^58]: 使用深度多元图像完成进行高分辨率乳腺扫描的无监督异常定位

    Unsupervised anomaly localization in high-resolution breast scans using deep pluralistic image completion. (arXiv:2305.03098v1 [eess.IV])

    [http://arxiv.org/abs/2305.03098](http://arxiv.org/abs/2305.03098)

    本文提出了一种利用深度多元图像完成方法进行乳腺扫描异常定位的无监督方法，该方法通过探索完成的多元性来提高评估标准的精度。

    

    数字乳腺摄影中的自动肿瘤检测是一项困难的任务，由于肿瘤很少出现，乳房组织变异和高分辨率。针对这个问题，鉴于异常图像的稀缺性和正常图像的丰富性，异常检测/定位方法可能非常适合。然而，机器学习中大部分异常定位研究集中在非医学数据集上，我们发现这些方法在医学图像数据集上适应性不足。这个问题可以通过解决图像完成视角下的任务得到缓解，其中异常的存在可以通过原始外观与其环境条件下自动完成之间的差异来指示。然而，在DBT数据集中，往往有很多相同环境条件下的有效的正常完成，使这个评估标准不太精确。为了解决这个问题，我们考虑通过探索完成的多元性来进行图像完成。

    Automated tumor detection in Digital Breast Tomosynthesis (DBT) is a difficult task due to natural tumor rarity, breast tissue variability, and high resolution. Given the scarcity of abnormal images and the abundance of normal images for this problem, an anomaly detection/localization approach could be well-suited. However, most anomaly localization research in machine learning focuses on non-medical datasets, and we find that these methods fall short when adapted to medical imaging datasets. The problem is alleviated when we solve the task from the image completion perspective, in which the presence of anomalies can be indicated by a discrepancy between the original appearance and its auto-completion conditioned on the surroundings. However, there are often many valid normal completions given the same surroundings, especially in the DBT dataset, making this evaluation criterion less precise. To address such an issue, we consider pluralistic image completion by exploring the distributi
    
[^59]: 联邦集成指导的离线强化学习算法

    Federated Ensemble-Directed Offline Reinforcement Learning. (arXiv:2305.03097v1 [cs.LG])

    [http://arxiv.org/abs/2305.03097](http://arxiv.org/abs/2305.03097)

    本文开发了一种名为FEDORA的联邦集成指导的离线强化学习算法，通过集成学习方法提炼客户群体的集体智慧，显著优于其他方法，包括在合并的数据汇总中进行离线强化学习，在各种复杂的连续控制环境和真实世界数据集中进行了实验。

    

    本文考虑了联邦离线强化学习问题。在这一场景下，分布式的学习代理必须仅使用由不同的未知的行为策略生成的小型预先收集的数据集协作学习出高质量的控制策略。笨拙地将标准离线强化学习方法与标准联邦学习方法组合来解决这个问题可能会导致表现不佳的策略。我们因此设计了Federated Ensemble-Directed Offline Reinforcement Learning Algorithm (FEDORA)，通过集成学习方法提炼客户群体的集体智慧。我们开发了FEDORA代码库，利用联邦学习平台上的分布式计算资源。我们证明了FEDORA在各种复杂的连续控制环境和真实世界数据集中均显著优于其他方法，包括在合并的数据汇总中进行离线强化学习。最后，我们展示了FEDORA在真实世界中的表现。

    We consider the problem of federated offline reinforcement learning (RL), a scenario under which distributed learning agents must collaboratively learn a high-quality control policy only using small pre-collected datasets generated according to different unknown behavior policies. Naively combining a standard offline RL approach with a standard federated learning approach to solve this problem can lead to poorly performing policies. In response, we develop the Federated Ensemble-Directed Offline Reinforcement Learning Algorithm (FEDORA), which distills the collective wisdom of the clients using an ensemble learning approach. We develop the FEDORA codebase to utilize distributed compute resources on a federated learning platform. We show that FEDORA significantly outperforms other approaches, including offline RL over the combined data pool, in various complex continuous control environments and real world datasets. Finally, we demonstrate the performance of FEDORA in the real-world on 
    
[^60]: 面向无答案的会话问答生成的“要问什么”和“如何问”的建模

    Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation. (arXiv:2305.03088v1 [cs.CL])

    [http://arxiv.org/abs/2305.03088](http://arxiv.org/abs/2305.03088)

    本文提出了一种新的方法来共同建模“要问什么”和“如何问”的问题，在不明确答案的设置中非常实用。

    

    会话问答生成（CQG）是机器通过对话帮助人类满足信息需求的关键任务。这个任务一般分为两种不同的设置：明确答案和不明确答案。虽然前者通过暴露预期答案来帮助模型，但后者更加现实并最近受到越来越多的关注。在不明确答案的设置中，“要问什么”和“如何问”是两个主要挑战。为了解决第一个挑战，现有的方法主要将上下文中的连续句子作为理由选择。我们认为，使用这种朴素的启发式方法生成的对话可能不够自然，因为在现实中，对话者经常谈论相关的内容，这些内容不一定是连续的。此外，先前的方法隐含决定要生成的问题类型（布尔/跨度），显式建模问题类型至关重要，因为答案（提示模型生成某种类型的问题）在无答案设置中不可用。为此，我们提出了一种新的方法，以端对端的方式共同模拟“要问什么”和“如何问”。具体而言，我们引入了一种新的理由选择方法，利用上下文和候选句子之间的相关性进行更自然的对话生成。此外，我们设计了一个问题类型预测器来明确模型中的目标问题类型。两个基准数据集上的广泛实验证明了我们提出的方法的有效性。

    Conversational Question Generation (CQG) is a critical task for machines to assist humans in fulfilling their information needs through conversations. The task is generally cast into two different settings: answer-aware and answer-unaware. While the former facilitates the models by exposing the expected answer, the latter is more realistic and receiving growing attentions recently. What-to-ask and how-to-ask are the two main challenges in the answer-unaware setting. To address the first challenge, existing methods mainly select sequential sentences in context as the rationales. We argue that the conversation generated using such naive heuristics may not be natural enough as in reality, the interlocutors often talk about the relevant contents that are not necessarily sequential in context. Additionally, previous methods decide the type of question to be generated (boolean/span-based) implicitly. Modeling the question type explicitly is crucial as the answer, which hints the models to ge
    
[^61]: 悬臂梁损伤检测的神经符号模型

    Neuro-symbolic model for cantilever beams damage detection. (arXiv:2305.03063v1 [cs.LG])

    [http://arxiv.org/abs/2305.03063](http://arxiv.org/abs/2305.03063)

    本文提出了一种神经符号模型用于悬臂梁损伤检测，该模型通过将卷积网络的处理能力与逻辑查询交互控制相结合，不仅能够准确检测损伤，而且还能够提供解释和定位，使其在操作条件下更可靠和可信。

    

    在过去的十年中，损伤检测方法迅速从先进的信号处理方法转变为机器学习，尤其是深度学习模型，以准确地、非侵入性地估计梁结构状态。但随着深度学习模型达到巅峰表现，人们也观察到了它们适用性的限制和易受攻击的弱点。其中最重要的一个原因是深度学习系统内在可解释性的缺失，由于知识编码在张量值中而没有包含逻辑约束。本文提出了一种神经符号模型，基于新颖的认知架构，将卷积网络的处理能力与直接将实际逻辑包含到模型中的查询交互控制结合起来，用于悬臂梁损伤检测。该混合判别模型不仅能够精确地检测损伤，而且能够提供损伤的解释和定位，使其在操作条件下更可靠和可信。

    In the last decade, damage detection approaches swiftly changed from advanced signal processing methods to machine learning and especially deep learning models, to accurately and non-intrusively estimate the state of the beam structures. But as the deep learning models reached their peak performances, also their limitations in applicability and vulnerabilities were observed. One of the most important reason for the lack of trustworthiness in operational conditions is the absence of intrinsic explainability of the deep learning system, due to the encoding of the knowledge in tensor values and without the inclusion of logical constraints. In this paper, we propose a neuro-symbolic model for the detection of damages in cantilever beams based on a novel cognitive architecture in which we join the processing power of convolutional networks with the interactive control offered by queries realized through the inclusion of real logic directly into the model. The hybrid discriminative model is 
    
[^62]: 利用梯度衡量数据选择和估价在差分隐私训练中的应用

    Leveraging gradient-derived metrics for data selection and valuation in differentially private training. (arXiv:2305.02942v1 [cs.LG])

    [http://arxiv.org/abs/2305.02942](http://arxiv.org/abs/2305.02942)

    研究如何在隐私增强技术下，利用梯度信息识别训练中感兴趣的数据样本进行数据选择和估价。

    

    由于监管担忧和参与度的不足，为机器学习模型进行协作训练获取高质量数据可能是一项具有挑战性的任务。隐私增强技术（PET）是解决监管问题的一种常用方法，差分隐私（DP）训练是其中最常用的一种方法。本文研究了如何使用梯度信息来识别隐私训练中感兴趣的训练样本。我们展示了在最严格的隐私设置中，存在着能够为客户提供有原则的数据选择工具的技术。

    Obtaining high-quality data for collaborative training of machine learning models can be a challenging task due to A) the regulatory concerns and B) lack of incentive to participate. The first issue can be addressed through the use of privacy enhancing technologies (PET), one of the most frequently used one being differentially private (DP) training. The second challenge can be addressed by identifying which data points can be beneficial for model training and rewarding data owners for sharing this data. However, DP in deep learning typically adversely affects atypical (often informative) data samples, making it difficult to assess the usefulness of individual contributions. In this work we investigate how to leverage gradient information to identify training samples of interest in private training settings. We show that there exist techniques which are able to provide the clients with the tools for principled data selection even in strictest privacy settings.
    
[^63]: 分层Transformer用于可扩展图学习

    Hierarchical Transformer for Scalable Graph Learning. (arXiv:2305.02866v1 [cs.LG])

    [http://arxiv.org/abs/2305.02866](http://arxiv.org/abs/2305.02866)

    本文提出了分层可扩展图Transformer (HSGT)用于解决图表示学习中的规模问题和上下文信息捕获不足问题，通过构建多尺度图分层结构，HSGT实现了对大型图的快速和内存高效处理，并在基准数据集上展现了卓越的性能表现。

    

    图Transformer在机器学习领域中越来越受到关注，并在图表示学习的基准测试中展现出了最先进的性能。然而，由于当前实现的图Transformer主要集中在学习小规模图的表示上，全局自注意机制的二次复杂度对于应用于较大规模图的全批量训练构成了挑战。此外，传统的基于采样的方法无法捕捉必要的高层次上下文信息，导致性能严重下降。在本文中，我们引入了分层可扩展图Transformer (HSGT)作为这些挑战的解决方案。HSGT成功地将Transformer架构扩展到大规模图上的节点表示学习任务中，同时保持高性能。通过利用通过粗化技术构建的图分层结构，HSGT有效地更新和存储多尺度信息，从而实现对大型图的快速和内存高效处理。我们在几个基准数据集上评估了HSGT，并展示了它相对于现有方法的卓越性能。

    Graph Transformer is gaining increasing attention in the field of machine learning and has demonstrated state-of-the-art performance on benchmarks for graph representation learning. However, as current implementations of Graph Transformer primarily focus on learning representations of small-scale graphs, the quadratic complexity of the global self-attention mechanism presents a challenge for full-batch training when applied to larger graphs. Additionally, conventional sampling-based methods fail to capture necessary high-level contextual information, resulting in a significant loss of performance. In this paper, we introduce the Hierarchical Scalable Graph Transformer (HSGT) as a solution to these challenges. HSGT successfully scales the Transformer architecture to node representation learning tasks on large-scale graphs, while maintaining high performance. By utilizing graph hierarchies constructed through coarsening techniques, HSGT efficiently updates and stores multi-scale informat
    
[^64]: 大语言模型在信息技术任务中自动生成YAML代码

    Automated Code generation for Information Technology Tasks in YAML through Large Language Models. (arXiv:2305.02783v1 [cs.SE])

    [http://arxiv.org/abs/2305.02783](http://arxiv.org/abs/2305.02783)

    这项研究提出了一种名为Ansible Wisdom的自然语言转Ansible-YAML代码的工具，可自动化生成Ansible脚本，提高IT自动化生产力，并相比现有技术达到或更好的性能水平。

    

    由于大语言模型在代码生成方面的不断提升，在通用编程语言方面的受益最大，而针对IT自动化等领域特定语言的研究较少。本研究聚焦于Ansible-YAML的生成，提出了一种名为Ansible Wisdom的自然语言转Ansible-YAML代码的工具，旨在提高IT自动化生产力。研究采用基于Transformer的模型，并通过新的包含Ansible-YAML的数据集进行扩展训练。同时，还开发了两个用于捕捉此领域特征的YAML和Ansible性能指标。结果表明，Ansible Wisdom可以精确地从自然语言提示中生成Ansible脚本，并且其性能可与现有技术的状态相媲美或更好。

    The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, have received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible-YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code generation tool, aimed at improving IT automation productivity. Ansible Wisdom is a transformer-based model, extended by training with a new dataset containing Ansible-YAML. We also develop two novel performance metrics for YAML and Ansible to capture the specific characteristics of this domain. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code 
    
[^65]: 探测行星信号的多重性增强分类器：使用ExoMiner的多重性增强验证69个新行星

    Multiplicity Boost Of Transit Signal Classifiers: Validation of 69 New Exoplanets Using The Multiplicity Boost of ExoMiner. (arXiv:2305.02470v1 [astro-ph.EP])

    [http://arxiv.org/abs/2305.02470](http://arxiv.org/abs/2305.02470)

    该论文介绍了一种可提高探测行星信号分类器性能的框架，称为多重性增强分类器，基于现有的分类器并使用多重性信息来验证69个新的系外行星。

    

    大多数已知的系外行星是通过验证技术而不是通过补充观测进行确认的。这些技术生成的分数通常代表了有关信号的某些信息（用x表示）给出的探测行星信号的概率（y（x）=行星）。这项工作引入了一种框架，即在给定的探测行星信号确认器的基础上，利用多重性信息改善其性能。我们将此框架应用于几个现有的分类器，包括vespa（Morton等人2016）、Robovetter（Coughlin等人2017）、AstroNet（Shallue和Vanderburg 2018）、ExoNet（Ansdel等人2018）、GPC和RFC（Armstrong等人2020）以及ExoMiner（Valizadegan等人2022），以支持我们的分类结果的有效性。

    Most existing exoplanets are discovered using validation techniques rather than being confirmed by complementary observations. These techniques generate a score that is typically the probability of the transit signal being an exoplanet (y(x)=exoplanet) given some information related to that signal (represented by x). Except for the validation technique in Rowe et al. (2014) that uses multiplicity information to generate these probability scores, the existing validation techniques ignore the multiplicity boost information. In this work, we introduce a framework with the following premise: given an existing transit signal vetter (classifier), improve its performance using multiplicity information. We apply this framework to several existing classifiers, which include vespa (Morton et al. 2016), Robovetter (Coughlin et al. 2017), AstroNet (Shallue & Vanderburg 2018), ExoNet (Ansdel et al. 2018), GPC and RFC (Armstrong et al. 2020), and ExoMiner (Valizadegan et al. 2022), to support our cl
    
[^66]: LTL本体中的时间查询反向工程

    Reverse Engineering of Temporal Queries Mediated by LTL Ontologies. (arXiv:2305.01248v1 [cs.LO])

    [http://arxiv.org/abs/2305.01248](http://arxiv.org/abs/2305.01248)

    本文研究了基于时间戳数据的LTL正片段制定的查询反向工程问题，旨在构建一个将给定答案与非答案分开的查询语言，并考虑了LTL本体介导的情况。

    

    在数据库查询的反向工程中，我们旨在从给定的答案和非答案集合构建一个查询; 然后可以将其用于进一步探索数据或作为答案和非答案的解释。我们研究了基于时间戳数据的线性时态逻辑LTL的正片段制定的查询的这个查询-by-example问题，重点是设计合适的查询语言和决定是否存在一个查询在给定的语言中可以将给定的答案与非答案分开的组合和数据复杂性。我们同时考虑了普通的LTL查询和那些通过LTL本体介导的查询。

    In reverse engineering of database queries, we aim to construct a query from a given set of answers and non-answers; it can then be used to explore the data further or as an explanation of the answers and non-answers. We investigate this query-by-example problem for queries formulated in positive fragments of linear temporal logic LTL over timestamped data, focusing on the design of suitable query languages and the combined and data complexity of deciding whether there exists a query in the given language that separates the given answers from non-answers. We consider both plain LTL queries and those mediated by LTL-ontologies.
    
[^67]: 一种基于LSTM的自适应巡航控制器，在车道变换时可以预测先前车辆的行为

    LSTM-based Preceding Vehicle Behaviour Prediction during Aggressive Lane Change for ACC Application. (arXiv:2305.01095v1 [cs.RO])

    [http://arxiv.org/abs/2305.01095](http://arxiv.org/abs/2305.01095)

    该论文提出了一种基于LSTM的ACC系统，可以学习过去的驾驶经验，适应和预测新情况，并且在模拟驾驶环境中表现良好。

    

    自适应巡航控制（ACC）系统的发展旨在通过自动调节车速来确保与前车安全距离，从而增强车辆的安全性和舒适性。然而，传统的ACC系统无法适应不断变化的驾驶条件和驾驶者的行为。为了解决这个问题，我们提出了一种基于LSTM的ACC系统，可以从以往的驾驶经验中学习，并实时地适应和预测新的情况。该模型是基于实际的高速公路高D数据集构建的，该数据集是利用装备有摄像头的无人机在德国高速公路上获取的。我们在侧面车道前车剪切并强制目标驾驶员减速的激进车道变化时评估了ACC系统。为此，将所提出的系统在模拟驾驶环境中进行了评估，并与馈送前人工神经网络（ANN）模型和模型预测控制（MPC）模型进行了比较。

    The development of Adaptive Cruise Control (ACC) systems aims to enhance the safety and comfort of vehicles by automatically regulating the speed of the vehicle to ensure a safe gap from the preceding vehicle. However, conventional ACC systems are unable to adapt themselves to changing driving conditions and drivers' behavior. To address this limitation, we propose a Long Short-Term Memory (LSTM) based ACC system that can learn from past driving experiences and adapt and predict new situations in real time. The model is constructed based on the real-world highD dataset, acquired from German highways with the assistance of camera-equipped drones. We evaluated the ACC system under aggressive lane changes when the side lane preceding vehicle cut off, forcing the targeted driver to reduce speed. To this end, the proposed system was assessed on a simulated driving environment and compared with a feedforward Artificial Neural Network (ANN) model and Model Predictive Control (MPC) model. The 
    
[^68]: 利用CNN进行Pascal VOC的语义分割

    Exploiting CNNs for Semantic Segmentation with Pascal VOC. (arXiv:2304.13216v1 [cs.CV])

    [http://arxiv.org/abs/2304.13216](http://arxiv.org/abs/2304.13216)

    本文在Pascal VOC数据集上进行了语义分割的研究，使用了FCN作为基准并改进了其问题，最终发现使用ResNet进行迁移学习的表现最佳。

    

    本文在Pascal VOC数据集上对语义分割进行了全面研究。我们使用了完全卷积网络（FCN）作为基准，并针对其问题进行改进，包括余弦退火学习率调度器、数据增强和类别不平衡权重。此外，我们还探索了三种不同的架构，并发现使用ResNet进行迁移学习的表现最佳。

    In this paper, we present a comprehensive study on semantic segmentation with the Pascal VOC dataset. Here, we have to label each pixel with a class which in turn segments the entire image based on the objects/entities present. To tackle this, we firstly use a Fully Convolution Network (FCN) baseline which gave 71.31% pixel accuracy and 0.0527 mean IoU. We analyze its performance and working and subsequently address the issues in the baseline with three improvements: a) cosine annealing learning rate scheduler(pixel accuracy: 72.86%, IoU: 0.0529), b) data augmentation(pixel accuracy: 69.88%, IoU: 0.0585) c) class imbalance weights(pixel accuracy: 68.98%, IoU: 0.0596). Apart from these changes in training pipeline, we also explore three different architectures: a) Our proposed model -- Advanced FCN (pixel accuracy: 67.20%, IoU: 0.0602) b) Transfer Learning with ResNet (Best performance) (pixel accuracy: 71.33%, IoU: 0.0926 ) c) U-Net(pixel accuracy: 72.15%, IoU: 0.0649). We observe that
    
[^69]: 面向科学的无监督域转移：探索深度学习方法实现具有不同响应模型的LArTPC探测器模拟之间的翻译

    Unsupervised Domain Transfer for Science: Exploring Deep Learning Methods for Translation between LArTPC Detector Simulations with Differing Response Models. (arXiv:2304.12858v1 [hep-ex])

    [http://arxiv.org/abs/2304.12858](http://arxiv.org/abs/2304.12858)

    本文提出了一种基于最近在成对图像转换技术上的进展的，完全无监督的方法来减少LArTPC探测器模拟和真实数据之间的系统差异，以提高模型性能。

    

    深度学习技术在科学中具有广泛应用，特别是在寻求简化潜在解决方案和发现的路径方面。然而，DL模型经常在模拟结果上进行训练，然后应用于真实实验数据。因此，模拟和真实数据之间的任何系统差异可能会降低模型的性能，这种效应称为“领域漂移”。本文研究了一种系统差异的玩具模型，提出了一种完全无监督的任务不可知方法来减少两个系统不同的样本之间的差异。该方法基于最近在成对图像转换技术上的进展，并在两组模拟液氩时间投影室（LArTPC）探测器事件样本上进行了验证，这些样本被创建以控制地演示在模拟和真实数据之间的常见系统差异。LArTPC探测器代表了下一代粒子探测器的发展方向。

    Deep learning (DL) techniques have broad applications in science, especially in seeking to streamline the pathway to potential solutions and discoveries. Frequently, however, DL models are trained on the results of simulation yet applied to real experimental data. As such, any systematic differences between the simulated and real data may degrade the model's performance -- an effect known as "domain shift." This work studies a toy model of the systematic differences between simulated and real data. It presents a fully unsupervised, task-agnostic method to reduce differences between two systematically different samples. The method is based on the recent advances in unpaired image-to-image translation techniques and is validated on two sets of samples of simulated Liquid Argon Time Projection Chamber (LArTPC) detector events, created to illustrate common systematic differences between the simulated and real data in a controlled way. LArTPC-based detectors represent the next-generation pa
    
[^70]: LLM+P：赋能大型语言模型最优规划能力

    LLM+P: Empowering Large Language Models with Optimal Planning Proficiency. (arXiv:2304.11477v1 [cs.AI])

    [http://arxiv.org/abs/2304.11477](http://arxiv.org/abs/2304.11477)

    本论文引入了LLM+P框架，将经典规划器的优点融入大型语言模型（LLM），通过将语言描述转换为用规划领域定义语言（PDDL）编写的文件来解决计划问题，旨在综合两者的优势，赋能LLMs最优规划能力。

    

    大型语言模型（LLM）展示了惊人的零样本泛化能力：最先进的聊天机器人可以提供对许多日常生活中常见问题的合理答案。然而，迄今为止，LLM不能可靠地解决长时程规划问题。相比之下，经典规划器一旦以格式化的方式提供问题，则可以使用高效的搜索算法快速识别正确或甚至最优的计划。为了综合两者的优势，本论文引入了LLM+P，这是第一个将经典规划器的优点融入LLM的框架。LLM+P接收一个计划问题的自然语言描述，然后以自然语言返回解决该问题的正确（或最优）计划。LLM+P首先将语言描述转换为用规划领域定义语言（PDDL）编写的文件，然后利用经典规划器快速找到解决方案，最后将找到的解决方案翻译回自然语言。

    Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into
    
[^71]: 野外人脸防伪挑战赛2023：基准和结果

    Wild Face Anti-Spoofing Challenge 2023: Benchmark and Results. (arXiv:2304.05753v1 [cs.CV])

    [http://arxiv.org/abs/2304.05753](http://arxiv.org/abs/2304.05753)

    前人的人脸防伪（FAS）技术应用于实际场景仍然存在限制，因为当前公开的FAS数据集数量和多样性不足，引起过拟合和场景误判。通过引入野外人脸防伪（WFAS）数据集，该研究提高了数据规模和多样化程度，促进了FAS技术的发展。

    

    人脸防伪（FAS）是保护自动人脸识别系统完整性的重要机制。尽管有了重大进展，但将现有方法推广到实际应用仍然具有挑战性。这种限制可以归因于公开可用的FAS数据集的稀缺性和缺乏多样性，这经常导致训练期间过拟合或测试期间饱和。就数量而言，欺诈主体的数量是一个重要的决定因素。大多数数据集仅包括少于2,000个受试者。就多样性而言，大多数数据集由在受控环境中使用重复机械化过程收集的欺诈样本组成。这种数据收集方法导致同质化样本和场景多样性的匮乏。为了解决这些缺点，我们介绍了野外人脸防伪（WFAS）数据集，这是一个大规模的、多样化的FAS数据集，可在不受限制的情况下收集。

    Face anti-spoofing (FAS) is an essential mechanism for safeguarding the integrity of automated face recognition systems. Despite substantial advancements, the generalization of existing approaches to real-world applications remains challenging. This limitation can be attributed to the scarcity and lack of diversity in publicly available FAS datasets, which often leads to overfitting during training or saturation during testing. In terms of quantity, the number of spoof subjects is a critical determinant. Most datasets comprise fewer than 2,000 subjects. With regard to diversity, the majority of datasets consist of spoof samples collected in controlled environments using repetitive, mechanical processes. This data collection methodology results in homogenized samples and a dearth of scenario diversity. To address these shortcomings, we introduce the Wild Face Anti-Spoofing (WFAS) dataset, a large-scale, diverse FAS dataset collected in unconstrained settings. Our dataset encompasses 853
    
[^72]: 评估ChatGPT和GPT-4的逻辑推理能力

    Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4. (arXiv:2304.03439v1 [cs.CL])

    [http://arxiv.org/abs/2304.03439](http://arxiv.org/abs/2304.03439)

    本文分析了多个逻辑推理数据集，评估了ChatGPT和GPT-4在逻辑推理任务上的表现，并构造了一个逻辑推理的分布之外的数据集来研究它们的鲁棒性。实验结果显示，ChatGPT在大多数逻辑推理基准测试中的表现远优于RoBERTa微调方法，而GPT-4的表现则更高。

    

    利用逻辑推理能力是一个全面的自然语言理解任务。随着先进的生成预训练转换器4（GPT-4）的发布，我们渴望了解GPT-4在各种逻辑推理任务上的表现。本文分析了多个逻辑推理数据集，包括LogiQA和ReClor等常用基准测试，以及像AR-LSAT这样的新发布的数据集。我们对需要逻辑推理的基准测试进行了多项选择阅读理解和自然语言推理任务测试。我们进一步构造了一个逻辑推理的分布之外的数据集，以研究ChatGPT和GPT-4的鲁棒性。我们还进行了ChatGPT和GPT-4之间的性能比较。实验结果表明，在大多数逻辑推理基准测试中，ChatGPT的表现远远优于RoBERTa微调方法。GPT-4在我们的手动测试中表现更高。在基准测试中，ChatGPT和GPT-4的表现相对较为均衡。

    Harnessing logical reasoning ability is a comprehensive natural language understanding endeavor. With the release of Generative Pretrained Transformer 4 (GPT-4), highlighted as "advanced" at reasoning tasks, we are eager to learn the GPT-4 performance on various logical reasoning tasks. This report analyses multiple logical reasoning datasets, with popular benchmarks like LogiQA and ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice reading comprehension and natural language inference tasks with benchmarks requiring logical reasoning. We further construct a logical reasoning out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4. We also make a performance comparison between ChatGPT and GPT-4. Experiment results show that ChatGPT performs significantly better than the RoBERTa fine-tuning method on most logical reasoning benchmarks. GPT-4 shows even higher performance on our manual tests. Among benchmarks, ChatGPT and GPT-4 do relatively w
    
[^73]: 语言模型在预后预测中的小样本学习能力

    Language Models are Few-shot Learners for Prognostic Prediction. (arXiv:2302.12692v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.12692](http://arxiv.org/abs/2302.12692)

    本研究探索了语言模型在免疫治疗预后预测中的应用，研究了小样本学习面临的挑战，对比了基线和语言模型的有效性，并发现在准确度方面有显著的改进，突出了自然语言处理在临床研究中改善早期检测和干预不同疾病的潜力。

    

    临床预测是医疗保健行业中的关键任务。然而，迄今为止，建立在理论框架Transformers之上的大型语言模型的成功并未延伸到这个领域。本研究利用真实世界中患者的临床数据和分子特征探索了Transformers和语言模型在免疫治疗预后预测中的应用。本文研究了Transformers相对于常规机器学习方法在临床预测中的潜力，并发现了在预测罕见疾病领域下小样本学习面临的挑战。该研究对比了基线和语言模型在多种癌症类型的预后预测中的有效性，并研究了在小样本情况下不同预先训练的语言模型的影响。结果表明在准确度方面有显著的改进，并突出了自然语言处理在临床研究中改善早期检测和干预不同疾病的潜力。

    Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for immunotherapy using real-world patients' clinical data and molecular profiles. This paper investigates the potential of transformers to improve clinical prediction compared to conventional machine learning approaches and addresses the challenge of few-shot learning in predicting rare disease areas. The study benchmarks the efficacy of baselines and language models on prognostic prediction across multiple cancer types and investigates the impact of different pretrained language models under few-shot regimes. The results demonstrate significant improvements in accuracy and highlight the potential of NLP in clinical research to improve early detection and intervention for different diseases.
    
[^74]: 不是你所签署的：间接提示注入妨害现实世界 LLM 一体化应用

    Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. (arXiv:2302.12173v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.12173](http://arxiv.org/abs/2302.12173)

    本文揭示了一种新的攻击向量：间接提示注入，它可以通过在可能检索到的数据中策略性地注入提示来远程利用 LLM 集成应用程序。该攻击对数据盗窃、蠕虫、信息生态系统污染等造成威胁。

    

    大型语言模型 (LLM) 日益被整合到各种应用程序中。最近的 LLM 可以通过自然语言提示灵活调节功能。这使它们容易受到针对性的对抗性提示攻击，例如 Prompt Injection (PI) 攻击使攻击者能够覆盖原始指令和使用的控件。迄今为止，人们认为用户直接提示 LLM。但是，如果不是用户提示呢？我们认为 LLM 一体化应用程序模糊了数据和指令之间的界限。我们揭示了使用间接提示注入的新攻击向量，使攻击者能够通过在可能检索到的数据中策略性地注入提示来远程（没有直接接口）利用 LLM 集成应用程序。我们从计算机安全的角度推导出一个全面的分类法，以系统地调查影响和漏洞，包括数据盗窃、蠕虫、信息生态系统污染以及其他创新性的威胁。

    Large Language Models (LLMs) are increasingly being integrated into various applications. The functionalities of recent LLMs can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection (PI) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We argue that LLM-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other nove
    
[^75]: 《绿色AI的系统性研究》综述

    A Systematic Review of Green AI. (arXiv:2301.11047v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.11047](http://arxiv.org/abs/2301.11047)

    人工智能（AI）的碳足迹越来越大，因此出现了绿色AI这个领域来解决AI的环境可持续性的问题。本文对98篇Green AI文献进行了系统性综述，揭示了该领域在2020年以后得到了显著增长。其中，大多数研究集中在监测AI模型的碳足迹、调整超参数以提高模型的可持续性，或对模型进行基准测试。

    

    随着基于人工智能的系统越来越多地被采用，AI的碳足迹已经不可忽视。因此，AI研究人员和实践者被敦促对他们设计和使用的AI模型的碳排放负责。这导致近年来出现了研究绿色AI环境可持续性的研究，这个领域被称为绿色AI。尽管对该主题的兴趣迅速增长，但至今仍缺乏综合性的绿色AI研究概述。为了填补这一空白,本文对Green AI文献进行了系统性综述。从分析98篇原始研究中，出现了不同的模式。这个主题从2020年开始经历了显著的增长。大多数研究考虑监测AI模型的碳足迹、调整超参数以提高模型的可持续性或对模型进行基准测试。混合了位置论文、观察研究和解决方案论文。大多数论文集中在训练阶段，算法

    With the ever-growing adoption of AI-based systems, the carbon footprint of AI is no longer negligible. AI researchers and practitioners are therefore urged to hold themselves accountable for the carbon emissions of the AI models they design and use. This led in recent years to the appearance of researches tackling AI environmental sustainability, a field referred to as Green AI. Despite the rapid growth of interest in the topic, a comprehensive overview of Green AI research is to date still missing. To address this gap, in this paper, we present a systematic review of the Green AI literature. From the analysis of 98 primary studies, different patterns emerge. The topic experienced a considerable growth from 2020 onward. Most studies consider monitoring AI model footprint, tuning hyperparameters to improve model sustainability, or benchmarking models. A mix of position papers, observational studies, and solution papers are present. Most papers focus on the training phase, are algorithm
    
[^76]: xTrimoABFold：无多序列比对的新型抗体结构预测方法

    xTrimoABFold: De novo Antibody Structure Prediction without MSA. (arXiv:2212.00735v2 [q-bio.QM] CROSS LISTED)

    [http://arxiv.org/abs/2212.00735](http://arxiv.org/abs/2212.00735)

    xTrimoABFold是一种基于深度抗体语言模型的新型抗体结构预测方法，无需多序列比对，有望促进高通量药物设计的应用。

    

    在抗体工程领域，设计一个新型抗体以正确地结合特定抗原的表位是一项重要的任务。了解抗体结构和其表位可以促进对其功能的机制理解。因此，从其序列预测抗体结构一直是一项高度有价值的任务，而AlphaFold2提供了一种基于蛋白质序列预测蛋白质结构的解决方案，但对于抗体，特别是对于抗体的互补决定区（CDRs），其预测效率和准确性有限制。

    In the field of antibody engineering, an essential task is to design a novel antibody whose paratopes bind to a specific antigen with correct epitopes. Understanding antibody structure and its paratope can facilitate a mechanistic understanding of its function. Therefore, antibody structure prediction from its sequence alone has always been a highly valuable problem for de novo antibody design. AlphaFold2, a breakthrough in the field of structural biology, provides a solution to predict protein structure based on protein sequences and computationally expensive coevolutionary multiple sequence alignments (MSAs). However, the computational efficiency and undesirable prediction accuracy of antibodies, especially on the complementarity-determining regions (CDRs) of antibodies limit their applications in the industrially high-throughput drug design. To learn an informative representation of antibodies, we employed a deep antibody language model (ALM) on curated sequences from the observed a
    
[^77]: 自动化复制/粘贴攻击对深度神经网络的诊断

    Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks. (arXiv:2211.10024v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10024](http://arxiv.org/abs/2211.10024)

    本文提出了一种基于嵌入方法的SNAFUE技术，用于寻找复制/粘贴攻击，并使用该技术对ImageNet分类器进行了测试并发现许多易于描述的漏洞。

    

    本文研究了如何帮助人类监督深度神经网络（DNN）。对抗样本可以通过揭示DNN的弱点来帮助，但很难解释或从中得出可实施的结论。先前的工作提出了使用人类可解释的对抗攻击，包括复制/粘贴攻击，其中一张自然图像粘贴到另一张图像会导致意外的分类错误。本文在此基础上提出了两个贡献。首先，我们介绍了使用嵌入的SNAFUE（Search for Natural Adversarial Features Using Embeddings）寻找复制/粘贴攻击的全自动方法。其次，我们使用SNAFUE来测试ImageNet分类器。我们重现了先前工作中的复制/粘贴攻击，并发现了数百个其他易于描述的漏洞，全过程无需人为干预。代码已在 https://github.com/thestephencasper/snafue 上公开。

    This paper considers the problem of helping humans exercise scalable oversight over deep neural networks (DNNs). Adversarial examples can be useful by helping to reveal weaknesses in DNNs, but they can be difficult to interpret or draw actionable conclusions from. Some previous works have proposed using human-interpretable adversarial attacks including copy/paste attacks in which one natural image pasted into another causes an unexpected misclassification. We build on these with two contributions. First, we introduce Search for Natural Adversarial Features Using Embeddings (SNAFUE) which offers a fully automated method for finding copy/paste attacks. Second, we use SNAFUE to red team an ImageNet classifier. We reproduce copy/paste attacks from previous works and find hundreds of other easily-describable vulnerabilities, all without a human in the loop. Code is available at https://github.com/thestephencasper/snafue
    
[^78]: 基于信任感知的群体智能数据注入攻击防御

    Trust-Awareness to Secure Swarm Intelligence from Data Injection Attack. (arXiv:2211.08407v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2211.08407](http://arxiv.org/abs/2211.08407)

    本文提出一种基于信任感知的方法，以应对群体智能的数据注入攻击。

    

    群体智能（SI）借助新兴的工业智能体（IA）技术得以实现，被预计在由第六代移动通信和数字孪生（DT）构成的未来工业物联网（IIoT）中扮演重要角色。然而，SI 对于数据注入攻击的脆弱性可能会使其无法实际部署。本文提出了一种有效的信任方法，以应对 SI 的这一安全问题。

    Enabled by the emerging industrial agent (IA) technology, swarm intelligence (SI) is envisaged to play an important role in future industrial Internet of Things (IIoT) that is shaped by Sixth Generation (6G) mobile communications and digital twin (DT). However, its fragility against data injection attack may halt it from practical deployment. In this paper we propose an efficient trust approach to address this security concern for SI.
    
[^79]: 离线增强学习用于更安全地控制1型糖尿病患者的血糖水平

    Offline Reinforcement Learning for Safer Blood Glucose Control in People with Type 1 Diabetes. (arXiv:2204.03376v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.03376](http://arxiv.org/abs/2204.03376)

    本论文研究了离线强化学习在控制1型糖尿病患者血糖水平中的应用，采用BCQ、CQL和TD3-BC算法有效地管理血糖水平，克服了在线学习过程的不稳定性和不安全性。

    

    有效的混合闭环系统的广泛应用将是1型糖尿病患者护理中的一个重要里程碑。这些设备通常利用简单的控制算法选择最佳胰岛素剂量来维持血糖水平在健康范围内。在线强化学习已被用作进一步增强这些设备的血糖控制方法。先前的方法已被证明与传统控制算法相比降低了患者风险，并改善了在目标范围内的时间，但往往在学习过程中不稳定，导致选择不安全的行为。本文提出了一种离线强化学习评估方法，以开发有效的剂量策略，无需在训练过程中进行潜在危险的患者交互。本文研究了BCQ、CQL和TD3-BC在管理FDA批准的30名虚拟病人的血糖水平中的效用。

    The widespread adoption of effective hybrid closed loop systems would represent an important milestone of care for people living with type 1 diabetes (T1D). These devices typically utilise simple control algorithms to select the optimal insulin dose for maintaining blood glucose levels within a healthy range. Online reinforcement learning (RL) has been utilised as a method for further enhancing glucose control in these devices. Previous approaches have been shown to reduce patient risk and improve time spent in the target range when compared to classical control algorithms, but are prone to instability in the learning process, often resulting in the selection of unsafe actions. This work presents an evaluation of offline RL for developing effective dosing policies without the need for potentially dangerous patient interaction during training. This paper examines the utility of BCQ, CQL and TD3-BC in managing the blood glucose of the 30 virtual patients available within the FDA-approved
    
[^80]: 使用区分特征度量下游分类的自监督表示质量

    Measuring Self-Supervised Representation Quality for Downstream Classification using Discriminative Features. (arXiv:2203.01881v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01881](http://arxiv.org/abs/2203.01881)

    从自监督学习模型中提取区分特征，并使用它们压缩表示空间，提出了一种Q-Score自监督表示质量分数，可以可靠地预测线性评估期间的错误分类。

    

    自监督学习在下游分类任务中展现出了惊人的结果。然而，对于它们的失败模式和学习表示的解释，存在着有限的研究。本文研究了 SimCLR、SwaV、MoCo、BYOL、DINO、SimSiam、VICReg 和 Barlow Twins 等最先进的自监督模型的表示空间。在不使用类标签信息的情况下，我们发现了对应于图像中独特物理属性的区分特征，这些区分特征主要存在于正确分类的表示中。使用这些特征，我们可以将表示空间压缩多达 40%，而不会显著影响线性分类性能。然后，我们提出了自监督表示质量分数（或 Q-Score），这是一种模型无关、无监督的分数，可以可靠地预测一个给定样本在线性评估期间是否可能被错误分类，并在 ImageNet-100 和 ImageNet-1K 上实现了 AUPRC 分别为 91.45 和 78.78。

    Self-supervised learning has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to $40\%$ without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), a model-agnostic, unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score
    
[^81]: 3D Object Detection Models and Methods 的调查和系统化研究

    Survey and Systematization of 3D Object Detection Models and Methods. (arXiv:2201.09354v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2201.09354](http://arxiv.org/abs/2201.09354)

    本文就3D目标检测领域进行了全面的调查和系统化研究，提出了一种实用框架用于比较3D目标检测方法，并帮助研究人员和实践者快速了解该领域。

    

    自动驾驶车辆的强烈需求和3D传感器广泛普及，不断推动着3D目标检测方法的提出。本文全面调研了2012年至2021年间3D目标检测领域的最新发展，包括输入数据、数据表示和特征提取以及实际检测模块的完整流程。介绍了基本概念，关注了过去十年里涌现的各种不同方法，并提出了一种系统化的方法，为比较这些方法提供了实用框架，以指导未来的开发、评估和应用活动。具体而言，本调查和系统化研究可以帮助研究人员和实践者快速了解该领域，将3D目标检测的解决方案分解为更易处理的部分。

    Strong demand for autonomous vehicles and the wide availability of 3D sensors are continuously fueling the proposal of novel methods for 3D object detection. In this paper, we provide a comprehensive survey of recent developments from 2012-2021 in 3D object detection covering the full pipeline from input data, over data representation and feature extraction to the actual detection modules. We introduce fundamental concepts, focus on a broad range of different approaches that have emerged over the past decade, and propose a systematization that provides a practical framework for comparing these approaches with the goal of guiding future development, evaluation and application activities. Specifically, our survey and systematization of 3D object detection models and methods can help researchers and practitioners to get a quick overview of the field by decomposing 3DOD solutions into more manageable pieces.
    
[^82]: LOGEN：基于逻辑知识条件的自训练文本生成在少样本下的应用

    LOGEN: Few-shot Logical Knowledge-Conditioned Text Generation with Self-training. (arXiv:2112.01404v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2112.01404](http://arxiv.org/abs/2112.01404)

    本文提出了一种基于少样本的逻辑知识条件下文本生成的统一框架LOGEN，通过自训练和基于内容和结构一致性抽样伪逻辑形式，实现了在少量样本下的文本生成。

    

    结构化数据的自然语言生成主要集中在表面层面描述，其存在控制内容选择困难和低保真度的问题。先前的研究利用逻辑形式来促进逻辑知识条件下的文本生成。虽然取得了显著进展，但是它们对数据的需求量较大，这使得在有限数据情况下应用于现实世界应用变得具有挑战性。为此，本文提出了一种基于少样本的逻辑知识条件下文本生成的统一框架。我们的方法只使用少量种子逻辑形式（如20/100种子） ，并利用自训练和基于内容和结构一致性抽样伪逻辑形式。实验结果表明，我们的方法可以比基准方法获得更好的少样本性能。

    Natural language generation from structured data mainly focuses on surface-level descriptions, suffering from uncontrollable content selection and low fidelity. Previous works leverage logical forms to facilitate logical knowledge-conditioned text generation. Though achieving remarkable progress, they are data-hungry, which makes the adoption for real-world applications challenging with limited data. To this end, this paper proposes a unified framework for logical knowledge-conditioned text generation in the few-shot setting. With only a few seeds logical forms (e.g., 20/100 shot), our approach leverages self-training and samples pseudo logical forms based on content and structure consistency. Experimental results demonstrate that our approach can obtain better few-shot performance than baselines.
    
[^83]: PredProp: 带精度加权预测编码的双向随机优化方法

    PredProp: Bidirectional Stochastic Optimization with Precision Weighted Predictive Coding. (arXiv:2111.08792v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08792](http://arxiv.org/abs/2111.08792)

    本文提出了PredProp方法，它通过随机梯度下降和自适应加权参数更新来优化预测编码网络中的权重和状态。通过使用传播误差协方差实现近似自然梯度下降，使得PredProp在测试中表现良好，比Adam表现更优。此外，PredProp可以通过误差精度提高权重参数的优化效果，并且在层次结构中可以分解精度需求。

    

    本文提出了PredProp方法，它基于传播的误差和神经活动的精度，通过随机梯度下降和自适应加权参数更新来优化预测编码网络(PCN)中的权重和状态。由于传播误差协方差与Fisher信息矩阵之间的关系，PredProp实现了近似自然梯度下降。我们在密集解码器网络和简单图像基准数据集的情况下展示了PredProp的有效性。我们发现，在测试配置中，PredProp表现优于Adam，一种广泛使用的自适应学习率优化器。此外，可用于权重参数的优化方法在推理过程中受益于使用PredProp的误差精度。由于层次预测编码层是通过局部误差单独优化的，所以所需精度在层次结构上分解。

    We present PredProp, a method for optimization of weights and states in predictive coding networks (PCNs) based on the precision of propagated errors and neural activity. PredProp jointly addresses inference and learning via stochastic gradient descent and adaptively weights parameter updates by approximate curvature. Due to the relation between propagated error covariance and the Fisher information matrix, PredProp implements approximate Natural Gradient Descent. We demonstrate PredProp's effectiveness in the context of dense decoder networks and simple image benchmark datasets. We found that PredProp performs favorably over Adam, a widely used adaptive learning rate optimizer in the tested configurations. Furthermore, available optimization methods for weight parameters benefit from using PredProp's error precision during inference. Since hierarchical predictive coding layers are optimised individually using local errors, the required precisions factorize over hierarchical layers. Ex
    
[^84]: 贝叶斯分层混合聚类中的后验正则化

    Posterior Regularization on Bayesian Hierarchical Mixture Clustering. (arXiv:2105.06903v7 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2105.06903](http://arxiv.org/abs/2105.06903)

    本文提出了一种后验正则化方法来改进贝叶斯分层混合聚类模型，在每个层级对节点实施最大间隔约束以增强集群的分离。

    

    贝叶斯分层混合聚类通过在生成过程中用层级狄利克雷过程混合模型(HDPMM)替换传统的高斯-高斯核来实现从父节点到子节点的扩散，从而改进了传统的贝叶斯分层聚类。然而，BHMC可能会产生具有高节点方差的树，表明在较高层级之间的节点之间存在较弱的分离。为了解决这个问题，我们采用了后验正则化(Posterior Regularization)，它对每个层级的节点实施最大间隔约束以增强集群的分离。我们阐述了如何将PR应用于BHMC，并证明了它在改进BHMC模型方面的有效性。

    Bayesian hierarchical mixture clustering (BHMC) improves traditionalBayesian hierarchical clustering by replacing conventional Gaussian-to-Gaussian kernels with a Hierarchical Dirichlet Process Mixture Model(HDPMM) for parent-to-child diffusion in the generative process. However,BHMC may produce trees with high nodal variance, indicating weak separation between nodes at higher levels. To address this issue, we employ Posterior Regularization, which imposes max-margin constraints on nodes at every level to enhance cluster separation. We illustrate how to apply PR toBHMC and demonstrate its effectiveness in improving the BHMC model.
    

