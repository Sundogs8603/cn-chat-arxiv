# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [DiffuserLite: Towards Real-time Diffusion Planning.](http://arxiv.org/abs/2401.15443) | DiffuserLite是一个快速轻量级的扩散规划框架，通过引入计划细化过程（PRP）来提高决策频率，相比之前的框架，它只产生了很小的运行时间成本，并在D4RL基准测试中达到了最先进的性能。 |
| [^2] | [A microservice architecture for real-time IoT data processing: A reusable Web of things approach for smart ports.](http://arxiv.org/abs/2401.15390) | 本文提出了一种可重用的微服务架构，使用物联网范式进行标准化，并支持复杂事件处理技术，以实现高效的实时物联网数据处理。 |
| [^3] | [A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM.](http://arxiv.org/abs/2401.15378) | 基于RAG的MufassirQAS问答系统利用NLP技术建立联系并准确回答复杂问题，提高了LLMs的准确性和透明度，帮助理解伊斯兰教的复杂性和教义深度。 |
| [^4] | [A Survey on Neural Topic Models: Methods, Applications, and Challenges.](http://arxiv.org/abs/2401.15351) | 这篇综述调研了神经主题模型的方法、应用和挑战，对于短文本和跨语言文档等各种场景提供了系统性的组织和介绍，并讨论了广泛应用的一系列热门应用。 |
| [^5] | [A Comprehensive Survey of Compression Algorithms for Language Models.](http://arxiv.org/abs/2401.15347) | 这篇论文是关于语言模型压缩算法的综合调查，讨论了如何在不损失准确性的情况下压缩语言模型。通过对多种压缩算法的调查和分析，总结了各个算法的整体趋势和价值。 |
| [^6] | [Deep Learning with Information Fusion and Model Interpretation for Health Monitoring of Fetus based on Long-term Prenatal Electronic Fetal Heart Rate Monitoring Data.](http://arxiv.org/abs/2401.15337) | 本研究通过结合深度学习和信息融合方法，开发了一个名为LARA的自动分析系统，用于长期产前电子胎儿心率监测。该系统通过卷积神经网络模型处理长期的FHR数据，提供了更全面的对胎儿状态的理解。 |
| [^7] | [L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks.](http://arxiv.org/abs/2401.15335) | 本文介绍了一种名为L-AutoDA的创新方法，利用大型语言模型自动设计决策型对抗攻击。通过与大型语言模型的迭代交互，L-AutoDA能够高效地生成竞争性的攻击算法，显示出在成功率和计算效率方面的显著改进。 |
| [^8] | [Music Auto-Tagging with Robust Music Representation Learned via Domain Adversarial Training.](http://arxiv.org/abs/2401.15323) | 该研究通过领域对抗训练(DAT)方法提出了一种改善嘈杂环境中音乐自动标记性能的方法。该方法通过额外的预训练阶段和添加合成的嘈杂音乐数据，获得了鲁棒的音乐表示，并在音乐自动标记方面展现了增强的性能。 |
| [^9] | [Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting.](http://arxiv.org/abs/2401.15318) | 高斯喷溅技术相结合物理基础动画和3D高斯喷溅，可以在虚拟场景中创造出无可比拟的效果，同时实现渲染、视图合成以及固体和流体的动态管理和交互。 |
| [^10] | [SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks.](http://arxiv.org/abs/2401.15299) | SupplyGraph是一个基准数据集，用于使用图神经网络进行供应链规划。该数据集包含了来自孟加拉国一家领先快速消费品公司的实际数据，用于优化、预测和解决供应链问题。数据集中的时间数据作为节点特征，可用于销售预测、生产计划和故障识别。 |
| [^11] | [A Survey on 3D Skeleton Based Person Re-Identification: Approaches, Designs, Challenges, and Future Directions.](http://arxiv.org/abs/2401.15296) | 本文通过对当前基于3D骨架的人员再识别方法、模型设计、挑战和未来方向的系统调研，填补了相关研究总结的空白。 |
| [^12] | [SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection.](http://arxiv.org/abs/2401.15293) | SkipViT通过令牌级跳跃连接将不重要的图像令牌分离，以提高Vision Transformers的训练速度，而不影响最终模型的准确率。 |
| [^13] | [Building ethical guidelines for generative AI in scientific research.](http://arxiv.org/abs/2401.15284) | 本文提出了一个初步的框架，通过五个关键主题的分析和缓解策略来建立科学研究中生成AI的伦理指南。全球共识、专业培训和合理的执行对于促进AI的益处和维护研究诚信至关重要。 |
| [^14] | [SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models.](http://arxiv.org/abs/2401.15270) | SimFair是一种物理引导的公平感知学习框架，通过集成模拟和逆向建模来解决纯数据驱动的公平性问题，有效保持公平性。 |
| [^15] | [Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models.](http://arxiv.org/abs/2401.15269) | 本论文介绍了一种名为Self-BioRAG的框架，通过使用检索和自我反思的方法，提高了医疗推理的能力。该框架专注于生成解释、检索领域特定文档以及对生成的响应进行自我反思。 |
| [^16] | [Towards Stable Preferences for Stakeholder-aligned Machine Learning.](http://arxiv.org/abs/2401.15268) | 本研究旨在通过数据驱动的方法，整合个人和团体的偏好，解决肾脏分配中的紧迫挑战，并评估偏好的稳定性。 |
| [^17] | [GenPluSSS: A Genetic Algorithm Based Plugin for Measured Subsurface Scattering Representation.](http://arxiv.org/abs/2401.15245) | 本文介绍了一种基于遗传算法的插件，可以在Blender 3D建模工具上添加次表面散射的表示方法，并使用Mitsuba渲染器进行验证。实验证明该插件能够准确、紧密和高效地可视化均匀和异质次表面散射效果。 |
| [^18] | [Unlearning Reveals the Influential Training Data of Language Models.](http://arxiv.org/abs/2401.15241) | 本文提出了一种简单而有效的方法UnTrac，通过反学习训练数据集来估计语言模型的影响。实验结果表明，UnTrac能够准确评估预训练数据集对生成有害内容的影响，并且无需额外的资源。 |
| [^19] | [Deep Learning with Tabular Data: A Self-supervised Approach.](http://arxiv.org/abs/2401.15238) | 我们提出了一种使用自监督学习和TabTransformer模型进行表格数据训练的新方法，该方法能够捕捉表格数据中的复杂关系和依赖关系。相比传统的机器学习模型，我们的方法能够消除对标记数据的需求。 |
| [^20] | [Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection.](http://arxiv.org/abs/2401.15222) | 本研究提出了一种使用多任务学习和迁移学习的方法，在临床文本中预测实体修饰语。实验结果表明，该方法在公开数据集和新数据集上均取得了最新技术的结果。 |
| [^21] | [Roq: Robust Query Optimization Based on a Risk-aware Learned Cost Model.](http://arxiv.org/abs/2401.15210) | Roq是一个基于风险感知学习方法的综合框架，用于实现鲁棒的查询优化。 |
| [^22] | [SCANIA Component X Dataset: A Real-World Multivariate Time Series Dataset for Predictive Maintenance.](http://arxiv.org/abs/2401.15199) | 这个论文介绍了一种来自SCANIA公司的真实世界多变量时间序列数据集，该数据集适用于各种机器学习应用，尤其是预测性维护场景。它具有庞大的样本数量和多样化的特征，以及时间信息，为研究者提供了一个使用真实世界数据的标准基准。 |
| [^23] | [Regularized Q-Learning with Linear Function Approximation.](http://arxiv.org/abs/2401.15196) | 本文提出了一种带有线性函数逼近的正则化Q学习算法，通过在不同尺度上操作，实现了有限时间内的收敛，并在马尔可夫噪声下具有性能保证。 |
| [^24] | [CAREForMe: Contextual Multi-Armed Bandit Recommendation Framework for Mental Health.](http://arxiv.org/abs/2401.15188) | CAREForMe是一种为心理健康设计的情境多臂赌博推荐框架，通过上下文感知、个性化和模块化的设计，结合移动传感和在线学习算法，提供及时、个性化的推荐。它的模块化设计既支持定制化的研究，也促进了跨学科的合作。 |
| [^25] | [Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks.](http://arxiv.org/abs/2401.15170) | 本研究证明了大型语言模型在定性编码中的应用潜力。相比于GPT-3.5，GPT-4能够实现与人类相当的解释能力，并具有较高的编码一致性。无论模型规模大小，只要满足一定条件，模型都可以实现较高的编码准确性。 |
| [^26] | [On the Emergence of Symmetrical Reality.](http://arxiv.org/abs/2401.15132) | 这篇论文引入了对称现实框架，通过统一的表示形式将物理和虚拟融合起来，使研究人员能够更好地理解AI代理在物理和虚拟世界中的合作。 |
| [^27] | [Sensor-Based Data Acquisition via Ubiquitous Device to Detect Muscle Strength Training Activities.](http://arxiv.org/abs/2401.15124) | 该研究利用智能手机中的传感器技术识别肌肉力量锻炼活动，通过分析数据和应用LSTM算法，发现了在右手和左手的运动中起重要作用的传感器属性。 |
| [^28] | [Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection.](http://arxiv.org/abs/2401.15123) | 提出了一种基于大型语言模型引导的知识蒸馏的时间序列异常检测方法AnomalyLLM，通过训练学生网络模仿预训练的大型语言模型的特征，在测试阶段通过比较学生网络和教师网络的特征差异来检测异常。 |
| [^29] | [A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics.](http://arxiv.org/abs/2401.15122) | 提出了一种能够促进数值MD模拟并有效模拟蛋白质-配体结合动力学的NeuralMD方法，采用物理信息多级对称框架，实现了准确建模多级蛋白质-配体相互作用。 |
| [^30] | [Expressive Power of ReLU and Step Networks under Floating-Point Operations.](http://arxiv.org/abs/2401.15121) | 该论文研究了在浮点运算下神经网络的表达能力，证明了使用二进制阈值单元或ReLU的神经网络可以记忆任何实数输入/输出对，并且可以在小误差内逼近任何连续函数。 |
| [^31] | [Context-driven self-supervised visual learning: Harnessing the environment as a data source.](http://arxiv.org/abs/2401.15120) | 这项研究提出了一种基于环境的上下文驱动的自我监督视觉学习方法，通过利用环境的历史空间上下文提供的相似性信号进行对比学习，并展示了在模拟环境中的优越性能，尤其在陌生环境中。该方法有潜力为代理在具有独特视觉特征的新环境中实现快速的视觉学习。 |
| [^32] | [Interpreting Time Series Transformer Models and Sensitivity Analysis of Population Age Groups to COVID-19 Infections.](http://arxiv.org/abs/2401.15119) | 该论文研究了解释深度学习时间序列模型的重要性，并通过局部解释方法解释了最先进的Transformer模型。通过将13个输入特征与3,142个美国县的三年日案例数据相结合，最佳预测模型能够在过去两周的基础上预测接下来两周的COVID-19感染情况。此外，该研究还提出了一种创新的评估方法，用于评估感染对8个人口年龄组的敏感性。 |
| [^33] | [GeoDecoder: Empowering Multimodal Map Understanding.](http://arxiv.org/abs/2401.15118) | GeoDecoder是一种专门设计用于处理地图中地理空间信息的多模态模型，通过集成图像和文本处理模块，无缝集成外部数据和特征，以及执行多任务训练和执行，实现了强化地图认知的目标。 |
| [^34] | [Towards Collective Superintelligence: Amplifying Group IQ using Conversational Swarms.](http://arxiv.org/abs/2401.15109) | 这项研究开发了一种叫做对话群体智能（CSI）的新技术，通过自然对话讨论来提高网络人类群体的决策准确性。研究评估了使用CSI平台的实时群体参加Raven智商测试的能力。 |
| [^35] | [Multi-agent Deep Reinforcement Learning for Dynamic Pricing by Fast-charging Electric Vehicle Hubs in ccompetition.](http://arxiv.org/abs/2401.15108) | 本文提出了一个多智能体深度强化学习的方法，应用于快速充电电动车中心的动态定价竞争。通过预测性购买电力需求和设定竞争性价格策略，充电站可以在竞争中进行有效定价。 |
| [^36] | [Decision Theoretic Foundations for Experiments Evaluating Human Decisions.](http://arxiv.org/abs/2401.15106) | 该论文通过综合统计决策理论和信息经济学，提出了决策问题的广泛适用定义。为了将人类决策的下降归咎于偏见形式，实验必须向参与者提供足够的信息来识别规范决策。然而，根据作者对AI辅助决策的研究的评估，只有17%的研究提供了足够的信息来描述参与者的行为偏离了良好的决策。 |
| [^37] | [Diffusion Enhancement for Cloud Removal in Ultra-Resolution Remote Sensing Imagery.](http://arxiv.org/abs/2401.15105) | 本文提出了一种基于扩散增强的云去除方法，通过在数据和方法上进行改进，实现了对超高分辨率遥感图像中云层的准确去除和详细语义内容恢复。 |
| [^38] | [PruneSymNet: A Symbolic Neural Network and Pruning Algorithm for Symbolic Regression.](http://arxiv.org/abs/2401.15103) | PruneSymNet是一种用于符号回归的新颖神经网络，可以通过贪婪修剪算法提取子网络以获得所需的符号表达式。 |
| [^39] | [Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning.](http://arxiv.org/abs/2401.15098) | Hi-Core提出了一种新的框架，通过层次化的知识迁移来增强连续强化学习。该框架包括利用大型语言模型的推理能力设定目标的高层策略制定和通过强化学习按照高层目标导向的低层策略学习。在实验中，Hi-Core展现了较强的知识迁移能力。 |
| [^40] | [Can generative AI and ChatGPT outperform humans on cognitive-demanding problem-solving tasks in science?.](http://arxiv.org/abs/2401.15081) | 该研究探讨了生成式人工智能工具在解决认知强度的科学问题时是否能超越人类，并通过与学生的对比实验发现，ChatGPT和GPT-4在大多数情况下表现出色。 |
| [^41] | [Charting the Future of AI in Project-Based Learning: A Co-Design Exploration with Students.](http://arxiv.org/abs/2401.14915) | 本文介绍了一项与学生共同设计的研究，探索学生在项目化学习中使用人工智能的潜力，并基于学生的视觉对教育目标转变进行分析。研究发现，不同态度的学生对人工智能的使用有不同的偏好。这为未来研究学生与人工智能交互和理解增强学习提供了机会。 |
| [^42] | [Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron.](http://arxiv.org/abs/2401.14521) | 本研究通过利用质量守恒感知器构建基于有向图结构的水文模型，实现了对集水区尺度水文过程的解释能力，在保持简洁性的同时能够准确地模拟各种流量动力学行为，并通过引入输入旁路机制进一步优化了模型的表现。 |
| [^43] | [Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search.](http://arxiv.org/abs/2401.14424) | 通过结合MCTS和生成式预训练模型，我们提出了一种新的符号回归算法SR-GPT，在发现数据中的数学公式方面取得了显著的改进。 |
| [^44] | [Adaptive Mobile Manipulation for Articulated Objects In the Open World.](http://arxiv.org/abs/2401.14403) | 本文介绍了一种针对开放环境中关节物体操作的全栈方法，机器人通过自适应学习框架从少量数据中学习，并通过在线实践学习适应训练分布之外的新对象。同时，还开发了低成本的移动操作硬件平台。 |
| [^45] | [ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using ChatGPT.](http://arxiv.org/abs/2401.14279) | ZS4C提出了一种使用ChatGPT进行零射击合成可编译代码的轻量级方法，帮助用户重用或分析不完整的Q&A代码片段，通过识别缺失的导入语句并修复编译错误来实现。 |
| [^46] | [Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation.](http://arxiv.org/abs/2401.14257) | 本文提出了一个多视角草图引导的文本到3D生成框架(Sketch2NeRF)，以增加对3D生成的草图控制。通过利用预训练的2D扩散模型和神经辐射场来优化3D场景实现细粒度控制。实验证明，该方法能够合成一致的3D内容。 |
| [^47] | [Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement.](http://arxiv.org/abs/2401.14215) | 本文提出了一个旨在解决长期对话中角色句子不具信息性的问题的框架，通过利用常识增强的角色扩展，并设计策略将相互矛盾的角色转化为包含丰富说话者信息的句子，以提高回应生成质量。 |
| [^48] | [BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction.](http://arxiv.org/abs/2401.14166) | BayesPrompt通过无偏领域抽象解决大规模预训练语言模型在少样本推理中的泛化问题。 |
| [^49] | [WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models.](http://arxiv.org/abs/2401.13919) | WebVoyager是一种创新的基于大型多模态模型的Web代理，能够通过与真实网站交互来端到端地完成用户指令。它提出了一个新的Web代理评估协议，并在实际任务中取得了显著的成功率。 |
| [^50] | [Investigating the Efficacy of Large Language Models for Code Clone Detection.](http://arxiv.org/abs/2401.13802) | 这项研究探索了大型语言模型在代码克隆检测任务中的应用。 |
| [^51] | [Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions.](http://arxiv.org/abs/2401.13324) | 本研究探讨了受算法决策影响的人的信息需求，发现解释往往不能满足他们的关注点，导致对监管框架的理解和遵守产生障碍。为了解决这个问题，研究团队提出了XAI初学者问题库，涵盖了就业预测和健康监测两个领域中受影响利益相关者的信息需求。 |
| [^52] | [Can AI Assistants Know What They Don't Know?.](http://arxiv.org/abs/2401.13275) | 本文研究了AI助手是否能知道自己不知道的事情，并通过自然语言表达出来的问题。为了回答这个问题，我们构建了一个特定模型的"I don't know"（Idk）数据集，并与AI助手进行对齐。 |
| [^53] | [DISCOUNT: Distributional Counterfactual Explanation With Optimal Transport.](http://arxiv.org/abs/2401.13112) | 本文提出了使用最优传输进行分布式对抗解释的方法DISCOUNT，将对抗解释的概念扩展到整个输入输出分布，并通过统计置信度来支撑这一方法。 |
| [^54] | [Gravity-Informed Deep Learning Framework for Predicting Ship Traffic Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge.](http://arxiv.org/abs/2401.13098) | 通过考虑航运通量密度、港口距离、贸易流量和交通枢纽的中心性指标等因素，本研究开发了一个受物理启发的模型来预测海事航运流量，并用于指导全球交通网络中入侵物种的风险评估和管理。 |
| [^55] | [Locality Sensitive Sparse Encoding for Learning World Models Online.](http://arxiv.org/abs/2401.13034) | 本文提出了一种基于局部敏感稀疏编码的线性回归模型，通过非线性随机特征实现对复杂环境的拟合。这种模型能够高效地进行稀疏更新，实现了优化拟合先前经验的Follow-The-Leader（FTL）世界模型。 |
| [^56] | [ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation.](http://arxiv.org/abs/2401.12665) | 这项工作提出了一种名为ClipSAM的CLIP和SAM协作框架，用于零样本异常分割。ClipSAM利用CLIP的语义理解能力进行异常定位和粗糙分割，然后将其用作SAM的提示约束，进一步改进异常分割结果。 |
| [^57] | [Tensor-view Topological Graph Neural Network.](http://arxiv.org/abs/2401.12007) | 提出了一种新颖的Tensor视图拓扑图神经网络（TTG-NN），该方法结合了持久同调、图卷积和张量运算，同时捕捉了局部和全局层面上的Tensor视图拓扑（TT）和Tensor视图图（TG）结构信息。 |
| [^58] | [Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation.](http://arxiv.org/abs/2401.11864) | 本研究提出了思维方程蒸馏（EoTD）技术和集合思维蒸馏（ETD）框架，通过构建基于方程的表示和使用多个思维过程的推理数据集来改进小型语言模型（SLMs）的数学推理能力，实验结果表明，EoTD和ETD显著提升了SLMs的推理能力。 |
| [^59] | [Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction.](http://arxiv.org/abs/2401.11798) | 本论文研究了在交通预测中应用空间-时间图卷积网络和知识蒸馏的方法。知识蒸馏的思想能够实现在减少参数和保持准确性的同时提高执行效率。通过引入教师网络的空间-时间相关性，我们的方法能够使学生网络学习到复杂的交通模式。 |
| [^60] | [Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations.](http://arxiv.org/abs/2401.11792) | 本文介绍了一种安全且广义的端到端自主驾驶系统 (SGADS)，使用强化学习和示范相结合的方法解决了现有方法的低安全性、泛化能力差和采样效率低的问题，同时引入了变分推理和归一化流以准确预测驾驶轨迹，并提出了鲁棒性安全约束的制定方法。 |
| [^61] | [Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey and the Open Libraries Behind Them.](http://arxiv.org/abs/2401.11723) | 该论文对机器学习在物联网生态系统中引起的安全威胁进行了综合探索，包括会员推断、对抗规避、重建、属性推断、模型提取和毒化攻击等多种攻击类型。 |
| [^62] | [Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation.](http://arxiv.org/abs/2401.11648) | 通过医学代码中心的多模态对比EHR建模预测下次就诊诊断，并通过分层正则化提高性能。 |
| [^63] | [S$^3$M-Net: Joint Learning of Semantic Segmentation and Stereo Matching for Autonomous Driving.](http://arxiv.org/abs/2401.11414) | S$^3$M-Net是一种用于自动驾驶的联合学习框架，同时进行语义分割和立体匹配。通过共享特征和特征融合适应模块的使用，S$^3$M-Net能够提高整体场景理解能力。 |
| [^64] | [Measuring Policy Distance for Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2401.11257) | 本文提出了一种用于测量多智能体强化学习中政策差异的通用工具，即多智能体政策距离（MAPD）。通过学习智能体的决策条件表示，MAPD可以计算任意一对智能体之间的政策距离，并且可以扩展到定制化版本以量化智能体政策在特定方面的差异。这个工具不仅有助于评估多智能体系统中多样性的演变，还为基于多样性的MARL算法的设计提供指导。 |
| [^65] | [Generalizing Speaker Verification for Spoof Awareness in the Embedding Space.](http://arxiv.org/abs/2401.11156) | 本文提出了一种在嵌入空间中推广的演讲者验证系统，可以同时处理冒充者和欺骗攻击，提供更强的保护和更经济的计算。 |
| [^66] | [SPAND: Sleep Prediction Architecture using Network Dynamics.](http://arxiv.org/abs/2401.11113) | SPAND是一个利用网络动态的睡眠预测架构，可以通过图网络和移动设备数据来预测下一天的睡眠持续时间标签。 |
| [^67] | [Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering.](http://arxiv.org/abs/2401.10711) | 本论文提出了一种使用大型多模型的弱监督高斯对比基础模型来处理视频问答问题的方法。通过将问题和答案对作为事件描述，找到多个关键帧作为目标时刻，并利用这些时刻作为伪标签来强制LMMs进行推理。所提出的方法使用轻量级的基于高斯的对比基础模块（GCG）来学习时效结构。 |
| [^68] | [BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis.](http://arxiv.org/abs/2401.10282) | BioDiffusion是一种用于生物医学信号合成的多功能扩散模型，能够产生高保真度、非稳态的多变量信号。通过利用这些合成的信号，可以有效解决生物医学信号机器学习任务中的数据不足、数据不平衡和标签复杂性等问题，提高准确性。 |
| [^69] | [Explaining Time Series via Contrastive and Locally Sparse Perturbations.](http://arxiv.org/abs/2401.08552) | 这篇论文提出了一个局部稀疏模型ContraLSP，通过引入对立样本和对比学习来解释时间序列。实证研究表明，ContraLSP在解释时间序列数据的质量上取得了实质性的改进。 |
| [^70] | [A Universal Knowledge Model and Cognitive Architecture for Prototyping AGI.](http://arxiv.org/abs/2401.06256) | 本文提出了一个普适的知识模型和认知架构，用于原型开发通用人工智能（AGI）。该架构包括42种认知架构和一组功能模块，用于接近AGI能力的智能系统。此外，本文还提出了一种通用的知识表示方法，可以将各种不同形式的知识表示整合到一个知识库中。 |
| [^71] | [Rediscovering Ranganathan: A Prismatic View of His Life through the Knowledge Graph Spectrum.](http://arxiv.org/abs/2401.03343) | 本研究通过一个新颖的生平知识图谱（KG）提供了库图与信息科学领域先驱人物S.R. Ranganathan的360度视角，这种专门的表示在范围和覆盖范围上无可比拟，并呼吁整个社区共同努力。 |
| [^72] | [NODEC: Neural ODE For Optimal Control of Unknown Dynamical Systems.](http://arxiv.org/abs/2401.01836) | NODEC是一种用神经ODE模型将动力学建模与控制器训练相结合的新框架，用于控制未知动态系统。 |
| [^73] | [A quatum inspired neural network for geometric modeling.](http://arxiv.org/abs/2401.01801) | 这个论文介绍了一种创新的矩阵乘积态(MPS)的消息传递策略，通过这种策略可以更好地捕捉几何图中的复杂关系。 |
| [^74] | [A Theoretical Analysis of Efficiency Constrained Utility-Privacy Bi-Objective Optimization in Federated Learning.](http://arxiv.org/abs/2312.16554) | 本文从理论上分析了联邦学习中受效率限制的效用-隐私双目标优化。先前的研究主要关注效用-隐私的权衡，忽视了训练效率和其他影响因素。该研究对差分隐私联邦学习中的关键问题进行了系统分析。 |
| [^75] | [Time-Transformer: Integrating Local and Global Features for Better Time Series Generation.](http://arxiv.org/abs/2312.11714) | 本文提出了一种新的时间序列生成模型，通过时间变换器同时学习本地和全局特征，实现了对时间序列数据的更好生成能力。 |
| [^76] | [Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint.](http://arxiv.org/abs/2312.11456) | 该论文研究了在KL约束下的反馈强化学习的理论框架，并提出了有效的算法和实践。实证评估表明，该框架在大型语言模型的对齐实验中表现出良好的效果。 |
| [^77] | [Improved Anonymous Multi-Agent Path Finding Algorithm.](http://arxiv.org/abs/2312.10572) | 本文提出了一种改进的匿名多智能体路径规划算法，通过利用批量探索搜索空间的思想，在寻找解决方案时将搜索状态进行压缩、存储和展开，从而实现高度减少。 |
| [^78] | [A Generalized Neural Diffusion Framework on Graphs.](http://arxiv.org/abs/2312.08616) | 本文提出了一个通用的扩散方程框架，通过带有保真度项的方程，正式建立了GNN与扩散过程之间的关系。通过实验证明，该框架能够描述高阶邻居的标签相似性。 |
| [^79] | [Evolving Reservoirs for Meta Reinforcement Learning.](http://arxiv.org/abs/2312.06695) | 本论文提出了一种进化沉积池的计算模型，用于研究动物适应环境的机制。这种模型基于元增强学习框架，通过演化和发展之间的相互作用，利用进化沉积池来加速和引导强化学习过程。 |
| [^80] | [AesFA: An Aesthetic Feature-Aware Arbitrary Neural Style Transfer.](http://arxiv.org/abs/2312.05928) | AesFA是一种轻量级但有效的神经风格转换方法，通过频率分解图像，以更好地解开美学风格，排除了预训练模型。引入了对比损失以提高风格化质量。实验证明，AesFA在stylization quality方面优于其他方法，并实现了快速转换。 |
| [^81] | [The sample complexity of multi-distribution learning.](http://arxiv.org/abs/2312.04027) | 本文解决了多分布学习的样本复杂度问题，并给出了匹配下界的样本复杂度算法。 |
| [^82] | [A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints.](http://arxiv.org/abs/2312.03905) | 本论文提出了一种针对具有逻辑约束的自回归模型的伪语义损失方法，通过在模型输出的局部近似上优化约束的似然，提高了神经符号学习的效率和适用性。 |
| [^83] | [MatterGen: a generative model for inorganic materials design.](http://arxiv.org/abs/2312.03687) | MatterGen是一个生成模型，在无机材料设计中能够生成稳定多样的材料，并通过适配器模块进行微调以满足广泛的性质约束。 |
| [^84] | [Flexible Communication for Optimal Distributed Learning over Unpredictable Networks.](http://arxiv.org/abs/2312.02493) | 本文提出了一种灵活的通信策略，根据网络配置自动切换使用Allgather（AG）或Allreduce（AR），以提高分布式学习的并行效率和模型精度。 |
| [^85] | [Automatic Time Signature Determination for New Scores Using Lyrics for Latent Rhythmic Structure.](http://arxiv.org/abs/2311.15480) | 本文提出了一种新颖的方法，通过仅使用歌词作为输入，自动生成适合歌词歌曲的节拍记号，并揭示潜在的节奏结构。 |
| [^86] | [Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization.](http://arxiv.org/abs/2311.09335) | 本文通过广泛的实证研究发现，修剪后的大型语言模型在抽象摘要任务中产生幻觉的情况较原始模型要少，表现更可靠，具有更高的效率和稀疏推理能力。 |
| [^87] | [Simplifying Complex Observation Models in Continuous POMDP Planning with Probabilistic Guarantees and Practice.](http://arxiv.org/abs/2311.07745) | 本研究在解决具有高维度和连续观测的部分可观测马尔可夫决策过程中，提出了一种基于统计总变差距离的新型概率界限，能够简化观测模型并保证解决方案的质量。 |
| [^88] | [GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling.](http://arxiv.org/abs/2311.01927) | GateLoop是一种完全数据控制的线性递归序列模型，优于现有模型，可以提供数据控制的相对位置信息给Attention。 |
| [^89] | [GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models.](http://arxiv.org/abs/2310.20025) | GOPlan是一个使用学习模型进行计划的目标条件下的离线强化学习方法，通过预训练先验策略和使用重新分析方法生成虚构轨迹，用以提高性能和处理有限数据预算和未见目标泛化的能力。 |
| [^90] | [A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem.](http://arxiv.org/abs/2310.18446) | 本论文提出了一种新型的跳跃正交列表来解决动态最优传输问题，在考虑数据点权重或位置变化时能够有效地更新最优传输方案。 |
| [^91] | [An Open Source Data Contamination Report for Large Language Models.](http://arxiv.org/abs/2310.17589) | 本文介绍了一个针对大型语言模型的开源数据污染报告，其中包括超过15个热门模型对六个常见多项选择问答基准测试的污染分析。实验证明，数据污染会显著降低模型性能，并且随着时间的推移污染程度不断增加。 |
| [^92] | [Towards Zero Shot Learning in Restless Multi-armed Bandits.](http://arxiv.org/abs/2310.14526) | 通过开发一个基于神经网络的预训练模型，我们实现了在不断变化的多臂赌博机中的零样本学习，该模型具有泛化能力，并且能够在特定实例上进行高效微调，同时适用于多行为设置和离散或连续状态空间。 |
| [^93] | [GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?.](http://arxiv.org/abs/2310.13833) | GraphMaker是一种专门设计用于生成大型带属性图的新颖扩散模型。 |
| [^94] | [Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations.](http://arxiv.org/abs/2310.10705) | 本文综述了利用机器学习技术在半导体制造中识别晶圆缺陷的方法，提出了一种创新的分类体系，为不同算法和技术之间的关系提供了深入分析，并通过严谨的评估验证了算法性能。 |
| [^95] | [BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations.](http://arxiv.org/abs/2310.07276) | BioT5是一个全面的预训练框架，在生物学中利用化学知识和自然语言关联丰富了跨模态整合，通过鲁棒的分子表示和上下文知识提取，实现了更有效的信息利用，展现出卓越的性能。 |
| [^96] | [Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning Approach.](http://arxiv.org/abs/2310.05969) | 提出了一种基于多模型深度学习的自动化胸部X光报告生成器系统，通过利用多个二元分类模型检测多种异常，在单个图像中辅助放射科医生的工作。该系统将放射学异常检测限制为心脏肥大、肺积液和实变，并通过三个步骤生成放射学报告：图像预处理、深度学习模型检测异常和生成报告。 |
| [^97] | [Low-Resource Languages Jailbreak GPT-4.](http://arxiv.org/abs/2310.02446) | 通过翻译不安全的英文输入成低资源语言，我们成功绕过了GPT-4的安全机制，并展示了这种跨语言漏洞。这一方法在实验中取得了与甚至超过了最先进的越狱攻击的效果，揭示了低资源语言在AI安全性中的薄弱环节。 |
| [^98] | [Selenite: Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models.](http://arxiv.org/abs/2310.02161) | Selenite是一个利用大型语言模型的系统，能够自动产生全面的选项和标准概览，帮助用户在陌生领域进行意义建构。 |
| [^99] | [Time-LLM: Time Series Forecasting by Reprogramming Large Language Models.](http://arxiv.org/abs/2310.01728) | 这项工作介绍了Time-LLM，一个重新编程的框架，通过重新利用大型语言模型，可以进行一般的时间序列预测。 |
| [^100] | [Syllable-level lyrics generation from melody exploiting character-level language model.](http://arxiv.org/abs/2310.00863) | 该论文提出了一种利用字符级语言模型从旋律中生成音节级歌词的方法，并通过融合语言模型知识和生成器网络进行优化。通过探索ChatGPT的评估方法，以及人工评估，证明了该方法提高了生成歌词的连贯性和正确性。 |
| [^101] | [Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients.](http://arxiv.org/abs/2309.16742) | 该论文研究了2型糖尿病患者白蛋白尿的早期预测问题，并开发了一种监督学习模型。通过使用不同的监督学习算法对184条数据进行训练，得出了预测结果。 |
| [^102] | [Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank.](http://arxiv.org/abs/2309.15560) | 研究揭示在无偏学习排名中，当点击数据不能完全拟合时，无法恢复真实相关性，导致排名性能显著降低，提出了可识别性图模型作为解决方案。 |
| [^103] | [Revisiting LARS for Large Batch Training Generalization of Neural Networks.](http://arxiv.org/abs/2309.14053) | 本文通过对大批量训练技术的研究，提出了一种新的算法TVLARS，该算法利用可配置的函数替代了热身阶段，以实现对于神经网络的稳健训练。实验证明，在大多数情况下，TVLARS比LARS和LAMB都有更好的性能表现，特别是在自监督学习方面。 |
| [^104] | [Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy.](http://arxiv.org/abs/2309.13500) | 这项研究介绍了一种创新的策略，将有符号图神经网络（SGNNs）和大型语言模型（LLM）的潜力协同起来，用于预测学生在学习者提供的问题上的表现。该方法利用有符号二分图全面建模学生回答，并采用对比学习框架增强了噪声的鲁棒性。 |
| [^105] | [Towards LLM-guided Causal Explainability for Black-box Text Classifiers.](http://arxiv.org/abs/2309.13340) | 本文提出了一种利用大型语言模型（LLM）引导黑盒文本分类器的因果可解释性的方法，通过生成反事实解释来解决这一挑战。 |
| [^106] | [E(2)-Equivariant Graph Planning for Navigation.](http://arxiv.org/abs/2309.13043) | 本文提出了一种E(2)-对称图规划用于导航的方法，通过利用欧几里得对称性和开发等变消息传递网络，实现了高效、稳定和具有泛化能力的机器人导航学习。 |
| [^107] | [Manifold-based Verbalizer Space Re-embedding for Tuning-free Prompt-based Classification.](http://arxiv.org/abs/2309.04174) | 本研究提出了一种无需调参的基于流形的语言转换器嵌入方法，通过保留同一类中的局部特性来进行分类，实验证明其与自动化的语言转换器效果相当。 |
| [^108] | [Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and Research Opportunities.](http://arxiv.org/abs/2308.13420) | 本文调查了强化学习辅助进化算法（RL-EA），该算法将强化学习与进化算法结合，以提高优化性能。对各种RL-EA的结构、操作符和搜索模式进行了分类和概述。 |
| [^109] | [A Study on Robustness and Reliability of Large Language Model Code Generation.](http://arxiv.org/abs/2308.10335) | 本研究针对大型语言模型生成的代码的可靠性和鲁棒性进行了研究，发现在真实的软件开发中可执行的代码并不能保证可靠和鲁棒，滥用API可能导致严重问题。这对初级开发者来说尤其危险，因为他们很难察觉到代码中的API滥用问题。 |
| [^110] | [Learning Logic Programs by Discovering Higher-Order Abstractions.](http://arxiv.org/abs/2308.08334) | 本论文介绍了一种通过发现高阶抽象来学习逻辑程序的方法，并且在多个领域的实验结果表明，该方法能够显著提高预测精度并减少学习时间。 |
| [^111] | [Developmental Bootstrapping of AIs.](http://arxiv.org/abs/2308.04586) | 传统的符号AI方法和深度学习AI方法无法满足创建强大和可信赖的AI的挑战，然而，发展脱靴法通过模仿人类儿童的能力发展过程，为创建稳健可靠的AI提供了希望。 |
| [^112] | [Rating-based Reinforcement Learning.](http://arxiv.org/abs/2307.16348) | 本文提出了一种基于评分的强化学习方法，通过利用人类评分来获得人类指导，该方法不同于现有的强化学习方法，它通过对样本轨迹的评估来进行学习。研究结果表明，该方法在实验中取得了良好的效果和收益。 |
| [^113] | [REX: Rapid Exploration and eXploitation for AI Agents.](http://arxiv.org/abs/2307.08962) | 本文提出了一种增强型的快速探索与利用的AI代理方法REX，它通过引入额外的奖励层和类似于UCB分数的概念，实现了更强大和高效的AI代理性能，并且具有离线行为利用和与基础模型无缝集成的优势。 |
| [^114] | [Black-Box Prediction of Flaky Test Fix Categories Using Language Models.](http://arxiv.org/abs/2307.00012) | 本文提出了一个使用语言模型的框架，可以自动生成易出错测试的标记数据集，并通过分析测试代码来预测测试的修复类别。实验结果表明UniXcoder优于CodeBERT。 |
| [^115] | [Challenges of Zero-Shot Recognition with Vision-Language Models: Granularity and Correctness.](http://arxiv.org/abs/2306.16048) | 本文研究了将视觉-语言模型应用于零样本视觉识别任务所面临的挑战，发现VLMs在识别细粒度概念方面表现更好，并指出了VLMs中相似度分数不能严格反映正确性的问题，提出了未来研究方向。 |
| [^116] | [To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration.](http://arxiv.org/abs/2306.15749) | 神经形态计算旨在通过仿真脑部操作来提高深度学习模型的效率，但是在SNNs的高效硬件后端设计上仍需进一步研究。 |
| [^117] | [SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling.](http://arxiv.org/abs/2306.11886) | SPRINT 提出了一种离线策略预训练方法，通过指令重标记及离线强化学习实现可扩展的预训练任务，大大减少了预训练所需的人力，同时使机器人能够获取更丰富的技能库，相较于之前的预训练方法，能够更快地学习新的长时间跨度任务。 |
| [^118] | [Neural Cellular Automata Can Respond to Signals.](http://arxiv.org/abs/2305.12971) | 神经元元胞自动机对信号作出响应的能力，为神经元元胞自动机作为人工形态发生模型的发展提供了基础，并且为将动态行为嵌入模型铺平了道路。 |
| [^119] | [Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model.](http://arxiv.org/abs/2305.10163) | 本研究通过在ChatGPT中集成医学领域知识和启用少样本学习的新方法，在中国国家医学执业医师资格考试中取得成功，这为建立在自然语言处理技术和医学领域知识的创新应用提供了可能。 |
| [^120] | [A sequential transit network design algorithm with optimal learning under correlated beliefs.](http://arxiv.org/abs/2305.09452) | 提出一种结合序列公交网络设计和最优学习的算法，用于准确估计潜在出行需求，并规避设计与实际需求不一致的风险。 |
| [^121] | [Masked Language Model Based Textual Adversarial Example Detection.](http://arxiv.org/abs/2304.08767) | 通过探索掩码语言模型引起的流形变化，我们提出了一种插即用的文本对抗例子检测方法，可以在保持对分类任务、模型结构和数据集无依赖的前提下，有效地检测到对抗例子。 |
| [^122] | [Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning.](http://arxiv.org/abs/2304.01295) | 本文提出了一个平行大规模多语种会话数据集XSGD，开发了一种有效的基于提示调整的方法来学习对齐提示，同时研究了跨语言任务的NLI-based和vanilla分类器，并在插槽填充和意图分类任务上评估了模型的跨语言泛化能力。 |
| [^123] | [Commonsense Knowledge Assisted Deep Learning for Resource-constrained and Fine-grained Object Detection.](http://arxiv.org/abs/2303.09026) | 本文提出了一种通识知识辅助的细粒度目标检测方法，利用通识知识推理模块处理由基准深度学习检测器给出的粗粒度标签，从而提高目标检测的准确性。经过实验验证，该方法相比于现有方法需要更少的计算量和标注资源。 |
| [^124] | [Magnushammer: A Transformer-based Approach to Premise Selection.](http://arxiv.org/abs/2303.04488) | Magnushammer是一种基于Transformer的前提选择方法，通过在PISA基准上的测试表明，它可以大幅度超越传统符号系统，并将先前最先进的证明率从57.0％提高到71.0％。 |
| [^125] | [Evaluating explainability for machine learning predictions using model-agnostic metrics.](http://arxiv.org/abs/2302.12094) | 本文提出了一种使用模型无关的度量标准，用于评估机器学习模型的预测结果的可解释性。这些度量标准将各个解释能力方面总结成标量，提供全面的理解并促进决策者和利益相关者之间的沟通，从而提高整体的透明度。 |
| [^126] | [Generalization-based similarity.](http://arxiv.org/abs/2302.10096) | 本文从头构建了一个基于抽象代数和定性概念的相似性概念，并将其通过模型论类型自然地嵌入到一阶逻辑中。 |
| [^127] | [A Convex Hull Cheapest Insertion Heuristic for the Non-Euclidean TSP.](http://arxiv.org/abs/2302.06582) | 本文提出了一种适用于非欧几里德旅行商问题的凸包最便宜插入启发式解法，通过使用多维缩放将非欧几里德空间的点近似到欧几里德空间，生成了初始化算法的凸包。在评估中发现，该算法在大多数情况下优于最邻近算法。 |
| [^128] | [Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models.](http://arxiv.org/abs/2302.04914) | 本文提出了一个灵活的、模型无关的方法，使用通用语言模型从研究论文中提取材料数据。该方法几乎不需要编码或模型训练，并且在生成的数据库中具有高召回率和几乎完美的精确度。 |
| [^129] | [Aligning Robot and Human Representations.](http://arxiv.org/abs/2302.01928) | 本文研究了机器人与人类表征之间的对齐问题，指出了当前学习方法存在的表示不对齐的困境，并建议应将机器人表征学习方法从实现任务目标的角度转向与人类表征对齐的问题。 |
| [^130] | [IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing.](http://arxiv.org/abs/2301.13359) | 该论文提出了一个IM-IAD工业制造中的图像异常检测基准，通过统一的设置评估了16个算法在7个数据集上的性能，旨在解决目前这一领域研究的不规范问题。 |
| [^131] | [AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning.](http://arxiv.org/abs/2301.12132) | AutoPEFT是一个自动化的PEFT（参数高效微调）配置搜索方法，它能够自动地找到最佳的PEFT模块和体系结构，以优化任务的性能和参数效率。在典型的NLP任务中，AutoPEFT表现出比手动设计更好的性能。 |
| [^132] | [Efficient Deep Reinforcement Learning with Predictive Processing Proximal Policy Optimization.](http://arxiv.org/abs/2211.06236) | 该论文介绍了一种名为预测处理近端策略优化（P4O）的深度强化学习方法，通过利用递归神经网络预测自身感觉状态来最小化惊异，从而显著提高累积奖励。 |
| [^133] | [No-Box Attacks on 3D Point Cloud Classification.](http://arxiv.org/abs/2210.14164) | 该论文介绍了一种新的方法，可以在不访问目标DNN模型的情况下预测3D点云中的对抗点，提供了无盒子攻击的新视角。 |
| [^134] | [Proportional algebras.](http://arxiv.org/abs/2210.01751) | 本文引入了直比代数，探讨了保持比拟比例的函数的数学性质，将其与直比同态、同余和直比函子联系起来，为模拟比例和模拟推理的数学理论提供了进一步理解。 |
| [^135] | [On the Relation between Sensitivity and Accuracy in In-context Learning.](http://arxiv.org/abs/2209.07661) | 在上下文学习中，我们发现ICL对多种扰动类型具有敏感性，标签偏差导致过去的研究低估了ICL的敏感性。同时，我们观察到ICL的敏感性和准确性之间呈现负相关关系。基于这些发现，我们提出了一种少样本选择性预测方法SenSel，它在放弃敏感预测决策上取得了优于常用基准方法的结果。 |
| [^136] | [SSL-WM: A Black-Box Watermarking Approach for Encoders Pre-trained by Self-supervised Learning.](http://arxiv.org/abs/2209.03563) | SSL-WM是一种用于验证自监督学习模型所有权的黑盒水印方法，对于下游任务多样且未知的情况下也适用。 |
| [^137] | [Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL.](http://arxiv.org/abs/2208.10469) | 本研究通过引入正式合同的概念，解决了多智能体强化学习中个体激励和集体激励分歧导致的次优行为问题。理论和实证结果表明，通过在马尔可夫博弈中引入有约束的状态依赖奖励转移，实现了所有可观察马尔可夫博弈的子博弈完美均衡表现出社会最优行为，并提升了算法的社会性能。 |
| [^138] | [DeepAutoPIN: An automorphism orbits based deep neural network for characterizing the organizational diversity of protein interactomes across the tree of life.](http://arxiv.org/abs/2203.00999) | 本研究利用基于自同构轨道的深度神经网络研究了整个生命起源树中的蛋白互作组织多样性，并发现不同生命域和门属网络的轨道使用概况存在明显差异。这一研究结果对于了解蛋白质相互作用网络的演化具有重要意义。 |
| [^139] | [Computer Vision Self-supervised Learning Methods on Time Series.](http://arxiv.org/abs/2109.00783) | 该研究评估了计算机视觉自监督学习框架在时间序列上的效果，并且提出了一种改进方法，通过改进协方差项和添加迭代归一化层，加速了模型的收敛。 |
| [^140] | [One head is better than two: a polynomial restriction for propositional definite Horn forgetting.](http://arxiv.org/abs/2009.07497) | 本研究首先对单头等价性进行了语义化的研究，给出了必要条件；其次提出了一种不完全算法，用于将公式转换为单头形式，从而实现在多项式时间内进行遗忘操作。 |
| [^141] | [Query Complexity of Tournament Solutions.](http://arxiv.org/abs/1611.06189) | 本文研究了竞赛解决方案的查询复杂度问题，通过尽可能少的查询边缘的方式找到最佳顶点集合。 |

# 详细

[^1]: DiffuserLite: 实时扩散规划的研究

    DiffuserLite: Towards Real-time Diffusion Planning. (arXiv:2401.15443v1 [cs.AI])

    [http://arxiv.org/abs/2401.15443](http://arxiv.org/abs/2401.15443)

    DiffuserLite是一个快速轻量级的扩散规划框架，通过引入计划细化过程（PRP）来提高决策频率，相比之前的框架，它只产生了很小的运行时间成本，并在D4RL基准测试中达到了最先进的性能。

    

    扩散规划被认为是各个领域中有效的决策范式。长时间跨度轨迹的高质量条件生成能力使其成为一个有前途的研究方向。然而，现有的扩散规划方法由于迭代抽样成本昂贵而导致决策频率低。为了解决这个问题，我们引入了DiffuserLite，一个快速而轻量级的扩散规划框架。DiffuserLite使用了一个计划细化过程（PRP）来生成粗到细粒度的轨迹，这显著减少了冗余信息的建模，从而显著提高了决策频率。我们的实验结果表明，与之前的框架相比，DiffuserLite仅产生了$0.88\%$的运行时间成本，平均决策频率达到了122Hz，并在D4RL基准测试上达到了最先进的性能。此外，我们的干净DiffuserLite框架可以提供...

    Diffusion planning has been recognized as an effective decision-making paradigm in various domains. The high-quality conditional generation capability of long-horizon trajectories makes it a promising research direction. However, existing diffusion planning methods suffer from low decision-making frequencies because of the expensive iterative sampling cost. To address this issue, we introduce DiffuserLite, a fast and lightweight diffusion planning framework. DiffuserLite employs a planning refinement process (PRP) to generate coarse-to-fine-grained trajectories, which significantly reduces the modeling of redundant information and leads to notable increases in decision-making frequency. Our experimental results demonstrate that DiffuserLite incurs only $0.88\%$ of the runtime cost compared to previous frameworks, achieves an average decision-making frequency of $122$Hz, and reaches state-of-the-art performance on D4RL benchmarks. In addition, our clean DiffuserLite framework can serve 
    
[^2]: 一种用于实时物联网数据处理的微服务架构：基于可重用物联网的智能港口方法

    A microservice architecture for real-time IoT data processing: A reusable Web of things approach for smart ports. (arXiv:2401.15390v1 [cs.SE])

    [http://arxiv.org/abs/2401.15390](http://arxiv.org/abs/2401.15390)

    本文提出了一种可重用的微服务架构，使用物联网范式进行标准化，并支持复杂事件处理技术，以实现高效的实时物联网数据处理。

    

    电信和物联网的重大进展使得智能城市场景得以实现。然而，提供这些智能服务需要以高效、互操作和实时的方式进行，这是一个前沿的技术挑战。虽然一些软件架构在这个领域提供了解决方案，但这些解决方案通常在可重用性和维护性方面存在限制，包括独立模块的维护或演进时的系统停机需求，以及缺乏接口互操作性的标准化。本文提出了一种完全可重用的微服务架构，通过使用物联网范式进行标准化，并支持复杂事件处理技术以实现高效的实时数据处理。为了说明这个提议，我们提供了一个完全可重用的微服务实现。

    Major advances in telecommunications and the Internet of Things have given rise to numerous smart city scenarios in which smart services are provided. What was once a dream for the future has now become reality. However, the need to provide these smart services quickly, efficiently, in an interoperable manner and in real time is a cutting-edge technological challenge. Although some software architectures offer solutions in this area, these are often limited in terms of reusability and maintenance by independent modules, involving the need for system downtime when maintaining or evolving, as well as by a lack of standards in terms of the interoperability of their interface. In this paper, we propose a fully reusable microservice architecture, standardized through the use of the Web of things paradigm, and with high efficiency in real-time data processing, supported by complex event processing techniques. To illustrate the proposal, we present a fully reusable implementation of the micro
    
[^3]: 基于RAG的理解伊斯兰教问题回答系统提案：MufassirQAS LLM

    A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])

    [http://arxiv.org/abs/2401.15378](http://arxiv.org/abs/2401.15378)

    基于RAG的MufassirQAS问答系统利用NLP技术建立联系并准确回答复杂问题，提高了LLMs的准确性和透明度，帮助理解伊斯兰教的复杂性和教义深度。

    

    学习和理解宗教存在复杂性和教义深度的挑战。问答机器人作为解决这些挑战的问题回答系统，可以帮助。LLM聊天机器人利用自然语言处理技术建立主题之间的联系，准确回答复杂问题。这些能力使其成为用于宗教启蒙的问题回答聊天机器人的理想选择。然而，LLM也有生成虚假信息的倾向，称为幻觉。聊天机器人的回答可能包含侮辱个人宗教信仰、跨宗派冲突和有争议或敏感的话题的内容。它需要避免这种情况，而不会宣扬仇恨言论或冒犯某些群体的人或他们的信仰。本研究使用基于向量数据库的检索增强生成（RAG）方法来提高LLMs的准确性和透明度。我们的问答系统称为"MufassirQAS"。我们创建了一个模型来评估该系统并证明其在解决宗教行业问题中的效果。

    There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as "MufassirQAS". We cre
    
[^4]: 关于神经主题模型的综述：方法、应用和挑战

    A Survey on Neural Topic Models: Methods, Applications, and Challenges. (arXiv:2401.15351v1 [cs.CL])

    [http://arxiv.org/abs/2401.15351](http://arxiv.org/abs/2401.15351)

    这篇综述调研了神经主题模型的方法、应用和挑战，对于短文本和跨语言文档等各种场景提供了系统性的组织和介绍，并讨论了广泛应用的一系列热门应用。

    

    主题模型几十年来一直被广泛应用于无监督方式下发现潜在主题和推断文档的主题比例。它们在文本分析和上下文推荐等各种应用中得到广泛应用。近年来，神经网络的崛起促成了一个新的研究领域——神经主题模型(NTMs)的出现。与传统的主题模型不同，NTMs直接优化参数，而不需要模型特定的推导。这使得NTMs具有更好的可扩展性和灵活性，吸引了大量的研究关注并产生了丰富的新方法和应用。在本文中，我们对神经主题模型的方法、应用和挑战进行了全面的调研。具体而言，根据网络结构系统地组织了当前NTM方法，并介绍了针对短文本和跨语言文档等各种场景的NTMs。我们还讨论了广泛应用的一系列热门应用。

    Topic models have been prevalent for decades to discover latent topics and infer topic proportions of documents in an unsupervised fashion. They have been widely used in various applications like text analysis and context recommendation. Recently, the rise of neural networks has facilitated the emergence of a new research field -- Neural Topic Models (NTMs). Different from conventional topic models, NTMs directly optimize parameters without requiring model-specific derivations. This endows NTMs with better scalability and flexibility, resulting in significant research attention and plentiful new methods and applications. In this paper, we present a comprehensive survey on neural topic models concerning methods, applications, and challenges. Specifically, we systematically organize current NTM methods according to their network structures and introduce the NTMs for various scenarios like short texts and cross-lingual documents. We also discuss a wide range of popular applications built 
    
[^5]: 关于语言模型压缩算法的综合调查

    A Comprehensive Survey of Compression Algorithms for Language Models. (arXiv:2401.15347v1 [cs.CL])

    [http://arxiv.org/abs/2401.15347](http://arxiv.org/abs/2401.15347)

    这篇论文是关于语言模型压缩算法的综合调查，讨论了如何在不损失准确性的情况下压缩语言模型。通过对多种压缩算法的调查和分析，总结了各个算法的整体趋势和价值。

    

    如何在不损失准确性的情况下压缩语言模型？语言模型的压缩算法数量正在快速增长，以从最近语言模型的显著进展中受益，而不会产生庞大语言模型的副作用，比如增加的碳排放和昂贵的维护费用。虽然许多压缩算法在压缩语言模型方面表现出色，但由于过多的算法，部分的难题在于捕捉新兴趋势并识别其基本概念。在本文中，我们对包括修剪、量化、知识蒸馏、低秩逼近、参数共享和高效架构设计在内的多种压缩算法进行了调查和总结。我们不仅总结了各种压缩算法的整体趋势，还选择了代表性算法，并对其进行了深入分析。我们讨论了每个类别的压缩算法的价值。

    How can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of com
    
[^6]: 基于深度学习和信息融合的长期产前电子胎儿心率监测健康监测技术研究

    Deep Learning with Information Fusion and Model Interpretation for Health Monitoring of Fetus based on Long-term Prenatal Electronic Fetal Heart Rate Monitoring Data. (arXiv:2401.15337v1 [cs.LG])

    [http://arxiv.org/abs/2401.15337](http://arxiv.org/abs/2401.15337)

    本研究通过结合深度学习和信息融合方法，开发了一个名为LARA的自动分析系统，用于长期产前电子胎儿心率监测。该系统通过卷积神经网络模型处理长期的FHR数据，提供了更全面的对胎儿状态的理解。

    

    胎儿心率（FHR）的长期监测在产前期间越来越受欢迎，其中电子FHR监测被广泛采用。与短期监测相比，这种连续监测可以收集更长时间的胎儿心率数据，从而更全面地了解胎儿的情况。然而，长期产前胎儿心率监测的解释仍处于早期阶段，缺乏相应的临床标准。此外，连续监测产生的大量数据在手动分析时对临床工作造成了重大负担。为了解决上述挑战，本研究开发了一个名为LARA（长期产前风险分析系统）的自动分析系统，结合了深度学习和信息融合方法。LARA的核心是一个成熟的卷积神经网络（CNN）模型，它将长期的FHR数据作为输入进行处理。

    Long-term fetal heart rate (FHR) monitoring during the antepartum period, increasingly popularized by electronic FHR monitoring, represents a growing approach in FHR monitoring. This kind of continuous monitoring, in contrast to the short-term one, collects an extended period of fetal heart data. This offers a more comprehensive understanding of fetus's conditions. However, the interpretation of long-term antenatal fetal heart monitoring is still in its early stages, lacking corresponding clinical standards. Furthermore, the substantial amount of data generated by continuous monitoring imposes a significant burden on clinical work when analyzed manually. To address above challenges, this study develops an automatic analysis system named LARA (Long-term Antepartum Risk Analysis system) for continuous FHR monitoring, combining deep learning and information fusion methods. LARA's core is a well-established convolutional neural network (CNN) model. It processes long-term FHR data as input 
    
[^7]: L-AutoDA: 利用大型语言模型进行自动决策型对抗攻击

    L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks. (arXiv:2401.15335v1 [cs.CR])

    [http://arxiv.org/abs/2401.15335](http://arxiv.org/abs/2401.15335)

    本文介绍了一种名为L-AutoDA的创新方法，利用大型语言模型自动设计决策型对抗攻击。通过与大型语言模型的迭代交互，L-AutoDA能够高效地生成竞争性的攻击算法，显示出在成功率和计算效率方面的显著改进。

    

    在快速发展的机器学习领域中，对抗攻击对模型的健壮性和安全性提出了显著挑战。决策型攻击只需要模型的决策反馈，而不需要详细的概率或分数，因此特别难以防御。本研究引入了L-AutoDA（基于大型语言模型自动生成决策型对抗攻击）的创新方法，利用大型语言模型的生成能力自动设计这些攻击。通过在进化框架中与大型语言模型进行迭代交互，L-AutoDA能够高效地自动设计出竞争性的攻击算法，减少人工工作量。我们在CIFAR-10数据集上展示了L-AutoDA的有效性，显示出在成功率和计算效率方面相比基准方法的显著改进。我们的研究结果突显了语言模型作为对抗攻击生成工具的潜力。

    In the rapidly evolving field of machine learning, adversarial attacks present a significant challenge to model robustness and security. Decision-based attacks, which only require feedback on the decision of a model rather than detailed probabilities or scores, are particularly insidious and difficult to defend against. This work introduces L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), a novel approach leveraging the generative capabilities of Large Language Models (LLMs) to automate the design of these attacks. By iteratively interacting with LLMs in an evolutionary framework, L-AutoDA automatically designs competitive attack algorithms efficiently without much human effort. We demonstrate the efficacy of L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline methods in both success rate and computational efficiency. Our findings underscore the potential of language models as tools for adversarial attack generation and highli
    
[^8]: 音乐自动标记：通过领域对抗训练学习鲁棒的音乐表示

    Music Auto-Tagging with Robust Music Representation Learned via Domain Adversarial Training. (arXiv:2401.15323v1 [cs.SD])

    [http://arxiv.org/abs/2401.15323](http://arxiv.org/abs/2401.15323)

    该研究通过领域对抗训练(DAT)方法提出了一种改善嘈杂环境中音乐自动标记性能的方法。该方法通过额外的预训练阶段和添加合成的嘈杂音乐数据，获得了鲁棒的音乐表示，并在音乐自动标记方面展现了增强的性能。

    

    音乐自动标记对于增强音乐发现和推荐至关重要。现有的音乐信息检索(MIR)模型在多媒体内容中存在的环境噪声和语音声音等现实世界噪声方面面临困难。本研究提出了一种受语音相关任务启发的方法，以增强嘈杂环境中的音乐自动标记性能。该方法将领域对抗训练(DAT)集成到音乐领域中，使得鲁棒的音乐表示能够抵抗噪声。与以前的研究不同，该方法还涉及领域分类器的额外预训练阶段，以避免后续阶段性能的下降。添加各种合成的嘈杂音乐数据改善了该模型在不同噪声水平下的泛化能力。所提出的架构通过有效利用无标签的嘈杂音乐数据，在音乐自动标记方面展现了增强的性能。在补充无标签数据的额外实验进一步提高了模型的性能。

    Music auto-tagging is crucial for enhancing music discovery and recommendation. Existing models in Music Information Retrieval (MIR) struggle with real-world noise such as environmental and speech sounds in multimedia content. This study proposes a method inspired by speech-related tasks to enhance music auto-tagging performance in noisy settings. The approach integrates Domain Adversarial Training (DAT) into the music domain, enabling robust music representations that withstand noise. Unlike previous research, this approach involves an additional pretraining phase for the domain classifier, to avoid performance degradation in the subsequent phase. Adding various synthesized noisy music data improves the model's generalization across different noise levels. The proposed architecture demonstrates enhanced performance in music auto-tagging by effectively utilizing unlabeled noisy music data. Additional experiments with supplementary unlabeled data further improves the model's performance
    
[^9]: 高斯喷溅：利用高斯飘落动态合成流体

    Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting. (arXiv:2401.15318v1 [cs.GR])

    [http://arxiv.org/abs/2401.15318](http://arxiv.org/abs/2401.15318)

    高斯喷溅技术相结合物理基础动画和3D高斯喷溅，可以在虚拟场景中创造出无可比拟的效果，同时实现渲染、视图合成以及固体和流体的动态管理和交互。

    

    我们展示了将物理基础动画与3D高斯喷溅（3DGS）相结合的可行性，以在使用3DGS重建的虚拟场景中创建新效果。利用高斯喷溅和基于位置的动力学（PBD）在底层表示中的一致性，我们以连贯的方式管理渲染、视图合成以及固体和流体的动态。类似于高斯着色器，我们通过添加法线增强每个高斯核，将核的方向与表面法线对齐，以改进PBD模拟。这种方法有效消除了固体旋转变形产生的尖峰噪声。它还使我们能够将基于物理的渲染集成到流体的动态表面反射中。因此，我们的框架能够真实地复现动态流体上的表面亮点，并促进场景对象与流体之间的交互。

    We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian splatting and position-based dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views. For more information, pl
    
[^10]: SupplyGraph: 使用图神经网络进行供应链规划的基准数据集

    SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks. (arXiv:2401.15299v1 [cs.LG])

    [http://arxiv.org/abs/2401.15299](http://arxiv.org/abs/2401.15299)

    SupplyGraph是一个基准数据集，用于使用图神经网络进行供应链规划。该数据集包含了来自孟加拉国一家领先快速消费品公司的实际数据，用于优化、预测和解决供应链问题。数据集中的时间数据作为节点特征，可用于销售预测、生产计划和故障识别。

    

    图神经网络（GNNs）在不同领域如运输、生物信息学、语言处理和计算机视觉中取得了重要进展。然而，在将GNNs应用于供应链网络方面，目前尚缺乏研究。供应链网络在结构上类似于图形，使其成为应用GNN方法的理想选择。这为优化、预测和解决供应链问题开辟了无限可能。然而，此方法的一个主要障碍在于缺乏真实世界的基准数据集以促进使用GNN来研究和解决供应链问题。为了解决这个问题，我们提供了一个来自孟加拉国一家领先的快速消费品公司的实际基准数据集，该数据集侧重于用于生产目的的供应链规划的时间任务。该数据集包括时间数据作为节点特征，以实现销售预测、生产计划和故障识别。

    Graph Neural Networks (GNNs) have gained traction across different domains such as transportation, bio-informatics, language processing, and computer vision. However, there is a noticeable absence of research on applying GNNs to supply chain networks. Supply chain networks are inherently graph-like in structure, making them prime candidates for applying GNN methodologies. This opens up a world of possibilities for optimizing, predicting, and solving even the most complex supply chain problems. A major setback in this approach lies in the absence of real-world benchmark datasets to facilitate the research and resolution of supply chain problems using GNNs. To address the issue, we present a real-world benchmark dataset for temporal tasks, obtained from one of the leading FMCG companies in Bangladesh, focusing on supply chain planning for production purposes. The dataset includes temporal data as node features to enable sales predictions, production planning, and the identification of fa
    
[^11]: 基于3D骨架的人员再识别：方法、设计、挑战和未来方向的综述

    A Survey on 3D Skeleton Based Person Re-Identification: Approaches, Designs, Challenges, and Future Directions. (arXiv:2401.15296v1 [cs.CV])

    [http://arxiv.org/abs/2401.15296](http://arxiv.org/abs/2401.15296)

    本文通过对当前基于3D骨架的人员再识别方法、模型设计、挑战和未来方向的系统调研，填补了相关研究总结的空白。

    

    通过3D骨架进行人员再识别是一个重要的新兴研究领域，引起了模式识别社区的极大兴趣。近年来，针对骨架建模和特征学习中突出问题，已经提出了许多具有独特优势的基于3D骨架的人员再识别（SRID）方法。尽管近年来取得了一些进展，但据我们所知，目前还没有对这些研究及其挑战进行综合总结。因此，本文通过对当前SRID方法、模型设计、挑战和未来方向的系统调研，试图填补这一空白。具体而言，我们首先定义了SRID问题，并提出了一个SRID研究的分类体系，总结了常用的基准数据集、常用的模型架构，并对不同方法的特点进行了分析评价。然后，我们详细阐述了SRID模型的设计原则。

    Person re-identification via 3D skeletons is an important emerging research area that triggers great interest in the pattern recognition community. With distinctive advantages for many application scenarios, a great diversity of 3D skeleton based person re-identification (SRID) methods have been proposed in recent years, effectively addressing prominent problems in skeleton modeling and feature learning. Despite recent advances, to the best of our knowledge, little effort has been made to comprehensively summarize these studies and their challenges. In this paper, we attempt to fill this gap by providing a systematic survey on current SRID approaches, model designs, challenges, and future directions. Specifically, we first formulate the SRID problem, and propose a taxonomy of SRID research with a summary of benchmark datasets, commonly-used model architectures, and an analytical review of different methods' characteristics. Then, we elaborate on the design principles of SRID models fro
    
[^12]: SkipViT: 使用令牌级跳跃连接加速Vision Transformers

    SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection. (arXiv:2401.15293v1 [cs.CV])

    [http://arxiv.org/abs/2401.15293](http://arxiv.org/abs/2401.15293)

    SkipViT通过令牌级跳跃连接将不重要的图像令牌分离，以提高Vision Transformers的训练速度，而不影响最终模型的准确率。

    

    Vision transformers被认为比CNN模型更具计算和数据密集性。这些Transformer模型，如ViT，需要所有输入图像令牌来学习它们之间的关系。然而，许多这些令牌并不信息丰富，可能包含无关的背景或不重要的场景等无关信息。这些令牌被多头自注意力（MHSA）忽略，导致MHSA和前馈网络（FFN）中存在许多冗余和不必要的计算。在这项工作中，我们提出了一种方法，通过将这些不重要的令牌分离并通过不同的低成本计算路径发送，来优化不必要的交互量。我们的方法不会给ViT模型添加任何参数，并旨在在训练吞吐量和最终模型的Top-1准确率损失为0%之间找到最佳平衡。我们对从头开始训练ViT-small的实验结果表明，SkipViT能够有效地提高训练速度。

    Vision transformers are known to be more computationally and data-intensive than CNN models. These transformer models such as ViT, require all the input image tokens to learn the relationship among them. However, many of these tokens are not informative and may contain irrelevant information such as unrelated background or unimportant scenery. These tokens are overlooked by the multi-head self-attention (MHSA), resulting in many redundant and unnecessary computations in MHSA and the feed-forward network (FFN). In this work, we propose a method to optimize the amount of unnecessary interactions between unimportant tokens by separating and sending them through a different low-cost computational path. Our method does not add any parameters to the ViT model and aims to find the best trade-off between training throughput and achieving a 0% loss in the Top-1 accuracy of the final model. Our experimental results on training ViT-small from scratch show that SkipViT is capable of effectively dr
    
[^13]: 在科学研究中建立生成AI的伦理指南

    Building ethical guidelines for generative AI in scientific research. (arXiv:2401.15284v1 [cs.CY])

    [http://arxiv.org/abs/2401.15284](http://arxiv.org/abs/2401.15284)

    本文提出了一个初步的框架，通过五个关键主题的分析和缓解策略来建立科学研究中生成AI的伦理指南。全球共识、专业培训和合理的执行对于促进AI的益处和维护研究诚信至关重要。

    

    生成人工智能工具（如大型语言模型）正在迅速改变学术研究和实际应用。然而，关于科学中生成AI的伦理指南的讨论仍然零散，强调了协商一致性标准的紧迫性。本文通过对五个关键主题的分析和缓解策略的开发，提供了一个初步的框架：了解模型在真实性和偏见方面的局限性；尊重隐私、机密和版权；在融入模型输出时避免抄袭和违反政策；确保应用带来总体利益；以及透明、可复制地使用人工智能。通过列举常见场景来展示潜在的伦理违规行为。我们认为，全球共识以及专业培训和合理的执行是促进AI的益处并维护研究诚信的关键。

    Generative artificial intelligence tools like large language models are rapidly transforming academic research and real world applications. However, discussions on ethical guidelines for generative AI in science remain fragmented, underscoring the urgent need for consensus based standards. This paper offers an initial framework by developing analyses and mitigation strategies across five key themes: understanding model limitations regarding truthfulness and bias; respecting privacy, confidentiality, and copyright; avoiding plagiarism and policy violations when incorporating model output; ensuring applications provide overall benefit; and using AI transparently and reproducibly. Common scenarios are outlined to demonstrate potential ethical violations. We argue that global consensus coupled with professional training and reasonable enforcement are critical to promoting the benefits of AI while safeguarding research integrity.
    
[^14]: SimFair：物理引导的公平感知学习与模拟模型

    SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models. (arXiv:2401.15270v1 [cs.LG])

    [http://arxiv.org/abs/2401.15270](http://arxiv.org/abs/2401.15270)

    SimFair是一种物理引导的公平感知学习框架，通过集成模拟和逆向建模来解决纯数据驱动的公平性问题，有效保持公平性。

    

    公平感知已经成为在真实应用中负责任使用人工智能的关键基础。在许多情况下，性能不平等是由于不同区域分布的变化引起的。虽然已经开发出提高公平可迁移性的技术，但是在没有来自新区域的样本的情况下解决这个问题并不总是可行的，这对于纯数据驱动的尝试是一个瓶颈。幸运的是，基于物理机制模型已经在许多具有重大社会影响的问题上进行了研究。我们提出了SimFair，一种物理引导的公平感知学习框架，通过集成基于物理规则的模拟和逆向建模到训练设计中来弥补数据限制。以温度预测为例，我们演示了所提出的SimFair在保持公平性方面的有效性。

    Fairness-awareness has emerged as an essential building block for the responsible use of artificial intelligence in real applications. In many cases, inequity in performance is due to the change in distribution over different regions. While techniques have been developed to improve the transferability of fairness, a solution to the problem is not always feasible with no samples from the new regions, which is a bottleneck for pure data-driven attempts. Fortunately, physics-based mechanistic models have been studied for many problems with major social impacts. We propose SimFair, a physics-guided fairness-aware learning framework, which bridges the data limitation by integrating physical-rule-based simulation and inverse modeling into the training design. Using temperature prediction as an example, we demonstrate the effectiveness of the proposed SimFair in fairness preservation.
    
[^15]: 通过检索和自我反思改善医疗推理能力的检索增强型大型语言模型

    Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models. (arXiv:2401.15269v1 [cs.CL])

    [http://arxiv.org/abs/2401.15269](http://arxiv.org/abs/2401.15269)

    本论文介绍了一种名为Self-BioRAG的框架，通过使用检索和自我反思的方法，提高了医疗推理的能力。该框架专注于生成解释、检索领域特定文档以及对生成的响应进行自我反思。

    

    最近的专有大型语言模型（LLMs），例如GPT-4，在生物医学领域中解决了从多项选择题到长篇生成等多样化挑战的里程碑。为了解决LLMs编码知识无法处理的挑战，已经开发了各种检索增强生成（RAG）方法，通过从知识语料库中搜索文档并无条件或有选择地将其附加到LLMs的输入来进行生成。然而，将现有方法应用于不同领域特定问题时，出现了泛化能力差的问题，导致获取不正确的文档或做出不准确的判断。在本文中，我们介绍了一种可靠的医学文本框架Self-BioRAG，专门用于生成解释、检索领域特定文档和自我反思生成的响应。我们使用了84k个经过过滤的生物医学指令集来训练Self-BioRAG，它具备评估自己的基因

    Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its gene
    
[^16]: 朝着稳定的利益相关方一致化机器学习偏好迈进

    Towards Stable Preferences for Stakeholder-aligned Machine Learning. (arXiv:2401.15268v1 [cs.LG])

    [http://arxiv.org/abs/2401.15268](http://arxiv.org/abs/2401.15268)

    本研究旨在通过数据驱动的方法，整合个人和团体的偏好，解决肾脏分配中的紧迫挑战，并评估偏好的稳定性。

    

    针对肾脏分配的紧迫挑战，即需求增长与利益相关方价值的结合，本研究旨在开发一个以数据驱动的解决方案，以解决这个问题。该研究的主要目标是创建一种学习个人和团体关于肾脏分配的偏好的方法。通过利用“成对肾脏患者在线调查”的数据，结合两个不同的数据集，并在个人、团体和稳定性三个层面上进行评估，我们使用机器学习分类器并通过几种度量方法进行评估。个人层面模型预测个体参与者的偏好，团体层面模型汇总参与者偏好，稳定性层面模型是团体升级的扩展，评估这些偏好随时间的稳定性。通过将利益相关方的偏好纳入肾脏分配过程，我们希望推动伦理维度的发展。

    In response to the pressing challenge of kidney allocation, characterized by growing demands for organs, this research sets out to develop a data-driven solution to this problem, which also incorporates stakeholder values. The primary objective of this study is to create a method for learning both individual and group-level preferences pertaining to kidney allocations. Drawing upon data from the 'Pairwise Kidney Patient Online Survey.' Leveraging two distinct datasets and evaluating across three levels - Individual, Group and Stability - we employ machine learning classifiers assessed through several metrics. The Individual level model predicts individual participant preferences, the Group level model aggregates preferences across participants, and the Stability level model, an extension of the Group level, evaluates the stability of these preferences over time. By incorporating stakeholder preferences into the kidney allocation process, we aspire to advance the ethical dimensions of o
    
[^17]: GenPluSSS：一种基于遗传算法的测量次表面散射表示的插件

    GenPluSSS: A Genetic Algorithm Based Plugin for Measured Subsurface Scattering Representation. (arXiv:2401.15245v1 [cs.GR])

    [http://arxiv.org/abs/2401.15245](http://arxiv.org/abs/2401.15245)

    本文介绍了一种基于遗传算法的插件，可以在Blender 3D建模工具上添加次表面散射的表示方法，并使用Mitsuba渲染器进行验证。实验证明该插件能够准确、紧密和高效地可视化均匀和异质次表面散射效果。

    

    本文介绍了一种在Blender 3D建模工具上添加均匀和异质、光学厚度的半透明材料表示的插件。该插件的工作原理基于遗传算法（GA）和基于奇异值分解（SVD）的次表面散射方法（GenSSS）的组合。所提出的插件使用开源渲染软件Mitsuba渲染器进行实现。该插件在测得的次表面散射数据上进行了验证。结果表明，所提出的插件能够准确、紧密和高效地可视化均匀和异质次表面散射效果。

    This paper presents a plugin that adds a representation of homogeneous and heterogeneous, optically thick, translucent materials on the Blender 3D modeling tool. The working principle of this plugin is based on a combination of Genetic Algorithm (GA) and Singular Value Decomposition (SVD)-based subsurface scattering method (GenSSS). The proposed plugin has been implemented using Mitsuba renderer, which is an open source rendering software. The proposed plugin has been validated on measured subsurface scattering data. It's shown that the proposed plugin visualizes homogeneous and heterogeneous subsurface scattering effects, accurately, compactly and computationally efficiently.
    
[^18]: 反学习揭示了语言模型的影响训练数据

    Unlearning Reveals the Influential Training Data of Language Models. (arXiv:2401.15241v1 [cs.CL])

    [http://arxiv.org/abs/2401.15241](http://arxiv.org/abs/2401.15241)

    本文提出了一种简单而有效的方法UnTrac，通过反学习训练数据集来估计语言模型的影响。实验结果表明，UnTrac能够准确评估预训练数据集对生成有害内容的影响，并且无需额外的资源。

    

    为了提高语言模型的性能，同时减少生成有害内容的风险，识别哪些训练数据集影响模型的输出至关重要。理想情况下，我们可以通过从训练中移除每个数据集来衡量其影响;然而，多次重新训练模型是非常昂贵的。本文提出了UnTrac，通过从训练模型中取消学习来估计训练数据集的影响。UnTrac非常简单; 通过梯度上升来取消学习每个训练数据集，并评估在取消学习后模型的预测发生了多大变化。我们经验性地研究了我们的方法是否能评估预训练数据集对生成有毒、有偏见和不真实内容的影响。实验结果表明，我们的方法比现有方法更准确地估计了它们的影响，同时不需要过多的内存空间或多个模型检查点。

    In order to enhance the performance of language models while mitigating the risks of generating harmful content, it is crucial to identify which training dataset affects the model's outputs. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac, which estimates the influence of a training dataset by unlearning it from the trained model. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model's predictions change after unlearning. We empirically examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Experimental results demonstrate that our method estimates their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple model checkpoints.
    
[^19]: 基于自监督学习的表格数据深度学习：一种自监督学习的方法

    Deep Learning with Tabular Data: A Self-supervised Approach. (arXiv:2401.15238v1 [cs.LG])

    [http://arxiv.org/abs/2401.15238](http://arxiv.org/abs/2401.15238)

    我们提出了一种使用自监督学习和TabTransformer模型进行表格数据训练的新方法，该方法能够捕捉表格数据中的复杂关系和依赖关系。相比传统的机器学习模型，我们的方法能够消除对标记数据的需求。

    

    我们描述了一种利用TabTransformer模型和自监督学习来训练表格数据的新方法。传统的表格数据机器学习模型，如GBDT，虽然被广泛使用，但我们的论文研究了专为表格数据优化的TabTransformer的有效性。TabTransformer通过利用Transformer的自注意机制捕捉表格数据中特征之间的复杂关系和依赖关系。我们在这项研究中使用了自监督学习方法，TabTransformer通过创建代理监督任务从无标签数据中学习，消除了对标记数据的需求。目标是找到最有效的TabTransformer模型来表示分类和数值特征。为了解决在Transformer中构建不同输入设置时所面临的挑战，我们还进行了比较分析。

    We have described a novel approach for training tabular data using the TabTransformer model with self-supervised learning. Traditional machine learning models for tabular data, such as GBDT are being widely used though our paper examines the effectiveness of the TabTransformer which is a Transformer based model optimised specifically for tabular data. The TabTransformer captures intricate relationships and dependencies among features in tabular data by leveraging the self-attention mechanism of Transformers. We have used a self-supervised learning approach in this study, where the TabTransformer learns from unlabelled data by creating surrogate supervised tasks, eliminating the need for the labelled data. The aim is to find the most effective TabTransformer model representation of categorical and numerical features. To address the challenges faced during the construction of various input settings into the Transformers. Furthermore, a comparative analysis is also been conducted to exami
    
[^20]: 在临床文本中预测实体修饰语的迁移学习：以阿片类物质使用障碍病例检测为应用

    Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection. (arXiv:2401.15222v1 [cs.CL])

    [http://arxiv.org/abs/2401.15222](http://arxiv.org/abs/2401.15222)

    本研究提出了一种使用多任务学习和迁移学习的方法，在临床文本中预测实体修饰语。实验结果表明，该方法在公开数据集和新数据集上均取得了最新技术的结果。

    

    背景：从临床文本中提取的实体的语义可能会受到修饰语的显著改变，包括实体的否定、不确定性、条件性、严重性和主观性。现有的确定临床实体修饰语的模型涉及使用正则表达式或特征权重，这些权重是独立训练每个修饰语的。方法：我们开发并评估了一个多任务变换器架构设计，在公开可用的SemEval 2015任务14语料库和一个新的阿片类物质使用障碍（OUD）数据集上共同学习和预测修饰语，该数据集包含与SemEval共享的修饰语以及OUD特定的新修饰语。我们评估了我们的多任务学习方法与以前发表的系统的效果，并评估了仅共享部分临床修饰语时的临床实体修饰语的迁移学习的可行性。结果：我们的方法在来自SemEval 2015的ShARe语料库上取得了最新技术的结果。

    Background: The semantics of entities extracted from a clinical text can be dramatically altered by modifiers, including entity negation, uncertainty, conditionality, severity, and subject. Existing models for determining modifiers of clinical entities involve regular expression or features weights that are trained independently for each modifier.  Methods: We develop and evaluate a multi-task transformer architecture design where modifiers are learned and predicted jointly using the publicly available SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that contains modifiers shared with SemEval as well as novel modifiers specific for OUD. We evaluate the effectiveness of our multi-task learning approach versus previously published systems and assess the feasibility of transfer learning for clinical entity modifiers when only a portion of clinical modifiers are shared.  Results: Our approach achieved state-of-the-art results on the ShARe corpus from SemEval 2015 T
    
[^21]: Roq：基于风险感知学习成本模型的鲁棒查询优化

    Roq: Robust Query Optimization Based on a Risk-aware Learned Cost Model. (arXiv:2401.15210v1 [cs.DB])

    [http://arxiv.org/abs/2401.15210](http://arxiv.org/abs/2401.15210)

    Roq是一个基于风险感知学习方法的综合框架，用于实现鲁棒的查询优化。

    

    关系数据库管理系统(RDBMS)中的查询优化器搜索预期对于给定查询最优的执行计划。它们使用参数估计，通常是不准确的，并且做出的假设在实践中可能不成立。因此，在这些估计和假设无效时，它们可能选择在运行时是次优的执行计划，这可能导致查询性能不佳。因此，查询优化器不足以支持鲁棒的查询优化。近年来，使用机器学习(ML)来提高数据系统的效率并减少其维护开销的兴趣日益高涨，在查询优化领域取得了有希望的结果。在本文中，受到这些进展的启发，并基于IBM Db2多年的经验，我们提出了Roq: 一种基于风险感知学习方法的综合框架，它实现了鲁棒的查询优化。

    Query optimizers in relational database management systems (RDBMSs) search for execution plans expected to be optimal for a given queries. They use parameter estimates, often inaccurate, and make assumptions that may not hold in practice. Consequently, they may select execution plans that are suboptimal at runtime, when these estimates and assumptions are not valid, which may result in poor query performance. Therefore, query optimizers do not sufficiently support robust query optimization. Recent years have seen a surge of interest in using machine learning (ML) to improve efficiency of data systems and reduce their maintenance overheads, with promising results obtained in the area of query optimization in particular. In this paper, inspired by these advancements, and based on several years of experience of IBM Db2 in this journey, we propose Robust Optimization of Queries, (Roq), a holistic framework that enables robust query optimization based on a risk-aware learning approach. Roq 
    
[^22]: SCANIA组件X数据集：用于预测性维护的真实世界多变量时间序列数据集

    SCANIA Component X Dataset: A Real-World Multivariate Time Series Dataset for Predictive Maintenance. (arXiv:2401.15199v1 [cs.LG])

    [http://arxiv.org/abs/2401.15199](http://arxiv.org/abs/2401.15199)

    这个论文介绍了一种来自SCANIA公司的真实世界多变量时间序列数据集，该数据集适用于各种机器学习应用，尤其是预测性维护场景。它具有庞大的样本数量和多样化的特征，以及时间信息，为研究者提供了一个使用真实世界数据的标准基准。

    

    本论文介绍了一种来自SCANIA瑞典公司的卡车车队中匿名发动机部件（称为Component X）的真实世界多变量时间序列数据集。该数据集包括多种变量，捕捉了详细的操作数据、维修记录和卡车规格，同时通过匿名处理保持机密性。它非常适用于各种机器学习应用，如分类、回归、生存分析和异常检测，特别是在预测性维护场景中的应用。庞大的样本数量和以直方图和计数器形式的多样化特征，以及包含时间信息，使得这个真实世界数据集在该领域中独特。发布这个数据集的目标是让广大研究人员有可能使用来自一家国际知名公司的真实世界数据，并引入一个标准基准用于预测性维护的研究。

    This paper presents a description of a real-world, multivariate time series dataset collected from an anonymized engine component (called Component X) of a fleet of trucks from SCANIA, Sweden. This dataset includes diverse variables capturing detailed operational data, repair records, and specifications of trucks while maintaining confidentiality by anonymization. It is well-suited for a range of machine learning applications, such as classification, regression, survival analysis, and anomaly detection, particularly when applied to predictive maintenance scenarios. The large population size and variety of features in the format of histograms and numerical counters, along with the inclusion of temporal information, make this real-world dataset unique in the field. The objective of releasing this dataset is to give a broad range of researchers the possibility of working with real-world data from an internationally well-known company and introduce a standard benchmark to the predictive ma
    
[^23]: 带有线性函数逼近的正则化Q学习

    Regularized Q-Learning with Linear Function Approximation. (arXiv:2401.15196v1 [cs.AI])

    [http://arxiv.org/abs/2401.15196](http://arxiv.org/abs/2401.15196)

    本文提出了一种带有线性函数逼近的正则化Q学习算法，通过在不同尺度上操作，实现了有限时间内的收敛，并在马尔可夫噪声下具有性能保证。

    

    一些成功的强化学习算法利用正则化来促进多模态策略，从而提高探索能力和鲁棒性。在使用函数逼近时，这些算法（如软Q学习）的收敛性质并不被很好地理解。本文考虑了一种单环路算法，在线性函数逼近的情况下具有有限时间收敛保证，用于最小化投影贝尔曼误差。该算法在两个尺度上运行：一个较慢的尺度用于更新状态动作值的目标网络，一个较快的尺度用于在基向量空间中逼近贝尔曼备份。在某些假设下，我们证明了在马尔可夫噪声存在下，该算法收敛于一个稳定点。此外，我们还提供了该算法衍生策略的性能保证。

    Several successful reinforcement learning algorithms make use of regularization to promote multi-modal policies that exhibit enhanced exploration and robustness. With functional approximation, the convergence properties of some of these algorithms (e.g. soft Q-learning) are not well understood. In this paper, we consider a single-loop algorithm for minimizing the projected Bellman error with finite time convergence guarantees in the case of linear function approximation. The algorithm operates on two scales: a slower scale for updating the target network of the state-action values, and a faster scale for approximating the Bellman backups in the subspace of the span of basis vectors. We show that, under certain assumptions, the proposed algorithm converges to a stationary point in the presence of Markovian noise. In addition, we provide a performance guarantee for the policies derived from the proposed algorithm.
    
[^24]: CAREForMe：心理健康的情境多臂赌博推荐框架

    CAREForMe: Contextual Multi-Armed Bandit Recommendation Framework for Mental Health. (arXiv:2401.15188v1 [cs.AI])

    [http://arxiv.org/abs/2401.15188](http://arxiv.org/abs/2401.15188)

    CAREForMe是一种为心理健康设计的情境多臂赌博推荐框架，通过上下文感知、个性化和模块化的设计，结合移动传感和在线学习算法，提供及时、个性化的推荐。它的模块化设计既支持定制化的研究，也促进了跨学科的合作。

    

    新冠疫情加剧了人们日常生活中有效和可获得的心理健康干预的紧迫性。移动健康（mHealth）解决方案，如AI聊天机器人和正念应用程序，已经获得了推广，因为它们不再局限于传统的临床环境，而是支持日常生活。然而，当前mHealth解决方案的有效性受到了缺乏上下文感知、个性化和模块化的阻碍，以促进它们的可重复使用性。本文介绍了CAREForMe，一种针对心理健康的情境多臂赌博（CMAB）推荐框架。CAREForMe以上下文感知、个性化和模块化为核心进行设计，利用移动传感和集成在线学习算法以及用户聚类能力来提供及时、个性化的推荐。通过其模块化设计，CAREForMe既是一个可定制的推荐框架，以指导未来的研究，又是一个促进跨学科合作的平台。

    The COVID-19 pandemic has intensified the urgency for effective and accessible mental health interventions in people's daily lives. Mobile Health (mHealth) solutions, such as AI Chatbots and Mindfulness Apps, have gained traction as they expand beyond traditional clinical settings to support daily life. However, the effectiveness of current mHealth solutions is impeded by the lack of context-awareness, personalization, and modularity to foster their reusability. This paper introduces CAREForMe, a contextual multi-armed bandit (CMAB) recommendation framework for mental health. Designed with context-awareness, personalization, and modularity at its core, CAREForMe harnesses mobile sensing and integrates online learning algorithms with user clustering capability to deliver timely, personalized recommendations. With its modular design, CAREForMe serves as both a customizable recommendation framework to guide future research, and a collaborative platform to facilitate interdisciplinary cont
    
[^25]: LLMs实现可扩展的定性编码：思维链推理在某些解释学任务中能达到人类水平

    Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks. (arXiv:2401.15170v1 [cs.CL])

    [http://arxiv.org/abs/2401.15170](http://arxiv.org/abs/2401.15170)

    本研究证明了大型语言模型在定性编码中的应用潜力。相比于GPT-3.5，GPT-4能够实现与人类相当的解释能力，并具有较高的编码一致性。无论模型规模大小，只要满足一定条件，模型都可以实现较高的编码准确性。

    

    定性编码或内容分析从文本中提取含义，以识别跨文本语料库的定量模式。最近，大型语言模型（LLMs）在解释能力方面的进展为自动化编码过程（对文本应用类别标签）提供了潜力，从而使人类研究人员能够专注于更有创造力的研究方面，同时将这些解释任务委托给人工智能。我们的案例研究包括对人文学研究具有代表性的密集段落的一组社会历史编码。我们发现GPT-4能够达到与人类相当的解释，而GPT-3.5则不能。与我们由人类获得的金标准相比，GPT-4在3个编码中具有优秀的编码一致性（Cohen's κ ≥ 0.79），在9个编码中有8个具有显著的一致性（κ ≥ 0.6）。相比之下，GPT-3.5在所有编码中表现不佳（mean(κ) = 0.34；max(κ) = 0.55）。重要的是，我们发现编码的准确性不受模型规模影响，在满足一定条件的情况下，较小的模型也可以实现较高的编码准确性。

    Qualitative coding, or content analysis, extracts meaning from text to discern quantitative patterns across a corpus of texts. Recently, advances in the interpretive abilities of large language models (LLMs) offer potential for automating the coding process (applying category labels to texts), thereby enabling human researchers to concentrate on more creative research aspects, while delegating these interpretive tasks to AI. Our case study comprises a set of socio-historical codes on dense, paragraph-long passages representative of a humanistic study. We show that GPT-4 is capable of human-equivalent interpretations, whereas GPT-3.5 is not. Compared to our human-derived gold standard, GPT-4 delivers excellent intercoder reliability (Cohen's $\kappa \geq 0.79$) for 3 of 9 codes, and substantial reliability ($\kappa \geq 0.6$) for 8 of 9 codes. In contrast, GPT-3.5 greatly underperforms for all codes ($mean(\kappa) = 0.34$; $max(\kappa) = 0.55$). Importantly, we find that coding fidelity
    
[^26]: 关于对称现实的出现

    On the Emergence of Symmetrical Reality. (arXiv:2401.15132v1 [cs.HC])

    [http://arxiv.org/abs/2401.15132](http://arxiv.org/abs/2401.15132)

    这篇论文引入了对称现实框架，通过统一的表示形式将物理和虚拟融合起来，使研究人员能够更好地理解AI代理在物理和虚拟世界中的合作。

    

    人工智能（AI）已经彻底改变了人类的认知能力，并促进了新的能够在物理和虚拟环境中与人类互动的AI实体的发展。虽然多年来存在着虚拟现实、混合现实和增强现实，但由于它们应用方向的差异，整合这些技术领域仍然是一个巨大的挑战。AI代理的出现，能够自主感知和行动，进一步加剧了传统以人为中心的研究方法的局限性。建立一个综合性框架，既能容纳人类和AI代理在物理和虚拟世界中的双重感知中心，是至关重要的。在本文中，我们引入了对称现实框架，提供了一个统一的表示，包括各种形式的物理-虚拟融合。这个框架使研究人员能够更好地理解AI代理如何能够合作。

    Artificial intelligence (AI) has revolutionized human cognitive abilities and facilitated the development of new AI entities capable of interacting with humans in both physical and virtual environments. Despite the existence of virtual reality, mixed reality, and augmented reality for several years, integrating these technical fields remains a formidable challenge due to their disparate application directions. The advent of AI agents, capable of autonomous perception and action, further compounds this issue by exposing the limitations of traditional human-centered research approaches. It is imperative to establish a comprehensive framework that accommodates the dual perceptual centers of humans and AI agents in both physical and virtual worlds. In this paper, we introduce the symmetrical reality framework, which offers a unified representation encompassing various forms of physical-virtual amalgamations. This framework enables researchers to better comprehend how AI agents can collabor
    
[^27]: 基于普适设备的传感器数据采集用于检测肌肉力量训练活动

    Sensor-Based Data Acquisition via Ubiquitous Device to Detect Muscle Strength Training Activities. (arXiv:2401.15124v1 [cs.HC])

    [http://arxiv.org/abs/2401.15124](http://arxiv.org/abs/2401.15124)

    该研究利用智能手机中的传感器技术识别肌肉力量锻炼活动，通过分析数据和应用LSTM算法，发现了在右手和左手的运动中起重要作用的传感器属性。

    

    通过体育锻炼来维持高品质的生活以预防健康下降是至关重要的。然而，个体健康状况、体育锻炼偏好和运动因素之间的关系是复杂的。体育锻炼的讨论始终显示与健康衰老经验存在积极相关，但与特定类型的肌肉骨骼锻炼没有明确的关联。利用智能手机的普及，尤其是在印度尼西亚，本研究利用嵌入式传感器进行人体活动识别。通过25名参与者的数据，在进行九种选择运动时，本研究成功地确定了在右手和左手的肌肉力量运动中起重要作用的传感器属性，作为开发基于LSTM算法的机器学习模型的基础。

    Maintaining a high quality of life through physical activities (PA) to prevent health decline is crucial. However, the relationship between individuals health status, PA preferences, and motion factors is complex. PA discussions consistently show a positive correlation with healthy aging experiences, but no explicit relation to specific types of musculoskeletal exercises. Taking advantage of the increasingly widespread existence of smartphones, especially in Indonesia, this research utilizes embedded sensors for Human Activity Recognition (HAR). Based on 25 participants data, performing nine types of selected motion, this study has successfully identified important sensor attributes that play important roles in the right and left hands for muscle strength motions as the basis for developing machine learning models with the LSTM algorithm.
    
[^28]: 大型语言模型引导的知识蒸馏用于时间序列异常检测

    Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection. (arXiv:2401.15123v1 [cs.LG])

    [http://arxiv.org/abs/2401.15123](http://arxiv.org/abs/2401.15123)

    提出了一种基于大型语言模型引导的知识蒸馏的时间序列异常检测方法AnomalyLLM，通过训练学生网络模仿预训练的大型语言模型的特征，在测试阶段通过比较学生网络和教师网络的特征差异来检测异常。

    

    自监督方法在时间序列异常检测中变得越来越重要，因为可用标注数据的稀缺性。然而，它们通常需要大量的训练数据来获得可泛化的表示映射，这与只有少量样本的情况冲突，从而限制了它们的性能。为了克服这个限制，我们提出了一种基于知识蒸馏的时间序列异常检测方法AnomalyLLM，在这种方法中，学生网络被训练成模仿基于大规模数据集预训练的大型语言模型(LLM)的特征。在测试阶段，当学生网络与教师网络的特征差异很大时，就检测到异常。为了避免学生网络学习到教师网络对异常样本的特征，我们设计了两个关键策略。1) 将典型信号融入学生网络，以巩固正常特征提取。2) 加权教师网络和学生网络的特征，以减少异常样本的影响。

    Self-supervised methods have gained prominence in time series anomaly detection due to the scarcity of available annotations. Nevertheless, they typically demand extensive training data to acquire a generalizable representation map, which conflicts with scenarios of a few available samples, thereby limiting their performance. To overcome the limitation, we propose \textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly detection approach where the student network is trained to mimic the features of the large language model (LLM)-based teacher network that is pretrained on large-scale datasets. During the testing phase, anomalies are detected when the discrepancy between the features of the teacher and student networks is large. To circumvent the student network from learning the teacher network's feature of anomalous samples, we devise two key strategies. 1) Prototypical signals are incorporated into the student network to consolidate the normal feature extraction. 2) W
    
[^29]: 一种多级对称微分方程模型用于学习蛋白质-配体结合动力学

    A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics. (arXiv:2401.15122v1 [cs.LG])

    [http://arxiv.org/abs/2401.15122](http://arxiv.org/abs/2401.15122)

    提出了一种能够促进数值MD模拟并有效模拟蛋白质-配体结合动力学的NeuralMD方法，采用物理信息多级对称框架，实现了准确建模多级蛋白质-配体相互作用。

    

    在药物发现中，蛋白质-配体结合的分子动力学（MD）模拟提供了一种强大的工具，用于预测结合亲和力，估计运输性能和探索口袋位点。通过改进数值方法以及最近通过机器学习（ML）方法增强MD模拟的效率已经有了很长的历史。然而，仍然存在一些挑战，例如准确建模扩展时间尺度的模拟。为了解决这个问题，我们提出了NeuralMD，这是第一个能够促进数值MD并提供准确的蛋白质-配体结合动力学模拟的ML辅助工具。我们提出了一个合理的方法，将一种新的物理信息多级对称框架纳入模型中。具体而言，我们提出了（1）一个使用向量框架满足群对称性并捕获多级蛋白质-配体相互作用的BindingNet模型，以及（2）一个增强的神经微分方程求解器，学习轨迹的演化。

    In drug discovery, molecular dynamics (MD) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities, estimating transport properties, and exploring pocket sites. There has been a long history of improving the efficiency of MD simulations through better numerical methods and, more recently, by augmenting them with machine learning (ML) methods. Yet, challenges remain, such as accurate modeling of extended-timescale simulations. To address this issue, we propose NeuralMD, the first ML surrogate that can facilitate numerical MD and provide accurate simulations of protein-ligand binding dynamics. We propose a principled approach that incorporates a novel physics-informed multi-grained group symmetric framework. Specifically, we propose (1) a BindingNet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions, and (2) an augmented neural differential equation solver that learns the trajectory und
    
[^30]: ReLU和Step网络在浮点运算下的表达能力

    Expressive Power of ReLU and Step Networks under Floating-Point Operations. (arXiv:2401.15121v1 [cs.LG])

    [http://arxiv.org/abs/2401.15121](http://arxiv.org/abs/2401.15121)

    该论文研究了在浮点运算下神经网络的表达能力，证明了使用二进制阈值单元或ReLU的神经网络可以记忆任何实数输入/输出对，并且可以在小误差内逼近任何连续函数。

    

    神经网络表达能力的研究调查了神经网络的基本限制。大多数现有的结果假设实数输入和参数以及在神经网络评估过程中进行精确运算。然而，神经网络通常在只能表示实数的计算机上执行，并且进行不精确的运算。在这项工作中，我们分析了在更实际的设置下神经网络的表达能力：使用浮点数和浮点运算。我们的第一组结果假设浮点运算中，浮点数的有效位数由有限位表示，但其指数可以取任何整数值。在这种设置下，我们展示了神经网络使用二进制阈值单元或ReLU可以记忆任何有限的输入/输出对，并可以在小误差内逼近任何连续函数。我们还展示了浮点运算下关于记忆和通用逼近的类似结果。

    The study of the expressive power of neural networks has investigated the fundamental limits of neural networks. Most existing results assume real-valued inputs and parameters as well as exact operations during the evaluation of neural networks. However, neural networks are typically executed on computers that can only represent a tiny subset of the reals and apply inexact operations. In this work, we analyze the expressive power of neural networks under a more realistic setup: when we use floating-point numbers and operations. Our first set of results assumes floating-point operations where the significand of a float is represented by finite bits but its exponent can take any integer value. Under this setup, we show that neural networks using a binary threshold unit or ReLU can memorize any finite input/output pairs and can approximate any continuous function within a small error. We also show similar results on memorization and universal approximation when floating-point operations u
    
[^31]: 基于环境的上下文驱动的自我监督视觉学习：利用环境作为数据源

    Context-driven self-supervised visual learning: Harnessing the environment as a data source. (arXiv:2401.15120v1 [cs.CV])

    [http://arxiv.org/abs/2401.15120](http://arxiv.org/abs/2401.15120)

    这项研究提出了一种基于环境的上下文驱动的自我监督视觉学习方法，通过利用环境的历史空间上下文提供的相似性信号进行对比学习，并展示了在模拟环境中的优越性能，尤其在陌生环境中。该方法有潜力为代理在具有独特视觉特征的新环境中实现快速的视觉学习。

    

    视觉学习通常发生在特定的上下文中，其中代理通过对一致环境中的位置的探索和跟踪来获取技能。代理的历史空间上下文提供了自我监督对比学习的相似性信号。我们提出了一种独特的方法，称为环境空间相似性（ESS），它补充了现有的对比学习方法。在模拟的逼真环境中使用图像作为实验设置，我们证明了ESS优于传统的实例鉴别方法。此外，从相同环境中采样更多数据显著提高了准确性并提供了新的增强。ESS在房间分类和空间预测任务中表现出卓越的熟练度，特别是在陌生环境中。这种学习范式有潜力在具有独特视觉特征的新环境中使代理能够快速进行视觉学习。

    Visual learning often occurs in a specific context, where an agent acquires skills through exploration and tracking of its location in a consistent environment. The historical spatial context of the agent provides a similarity signal for self-supervised contrastive learning. We present a unique approach, termed Environmental Spatial Similarity (ESS), that complements existing contrastive learning methods. Using images from simulated, photorealistic environments as an experimental setting, we demonstrate that ESS outperforms traditional instance discrimination approaches. Moreover, sampling additional data from the same environment substantially improves accuracy and provides new augmentations. ESS allows remarkable proficiency in room classification and spatial prediction tasks, especially in unfamiliar environments. This learning paradigm has the potential to enable rapid visual learning in agents operating in new environments with unique visual characteristics. Potentially transforma
    
[^32]: 解释时间序列Transformer模型和COVID-19感染对人口年龄组的敏感性分析

    Interpreting Time Series Transformer Models and Sensitivity Analysis of Population Age Groups to COVID-19 Infections. (arXiv:2401.15119v1 [cs.LG])

    [http://arxiv.org/abs/2401.15119](http://arxiv.org/abs/2401.15119)

    该论文研究了解释深度学习时间序列模型的重要性，并通过局部解释方法解释了最先进的Transformer模型。通过将13个输入特征与3,142个美国县的三年日案例数据相结合，最佳预测模型能够在过去两周的基础上预测接下来两周的COVID-19感染情况。此外，该研究还提出了一种创新的评估方法，用于评估感染对8个人口年龄组的敏感性。

    

    解释深度学习时间序列模型对于理解模型行为和从原始数据中学习模式以进行实时决策非常重要。然而，基于Transformer的时间序列模型的复杂性使解释个体特征对预测影响的挑战变得困难。在这项研究中，我们利用最近的局部解释方法来解释最先进的时间序列模型。为了使用真实世界的数据集，我们收集了3142个美国县的三年日案例数据。首先，我们比较了六种基于Transformer的模型，并选择了最佳的COVID-19感染预测模型。使用过去两周的13个输入特征，我们可以预测接下来两周的病例数量。其次，我们提出了一种创新的方法，评估了预测对8个人口年龄组的敏感性，以及高度动态的多变量感染数据。第三，我们将我们提出的基于扰动的解释方法与相关工作进行比较，总共包括...

    Interpreting deep learning time series models is crucial in understanding the model's behavior and learning patterns from raw data for real-time decision-making. However, the complexity inherent in transformer-based time series models poses challenges in explaining the impact of individual features on predictions. In this study, we leverage recent local interpretation methods to interpret state-of-the-art time series models. To use real-world datasets, we collected three years of daily case data for 3,142 US counties. Firstly, we compare six transformer-based models and choose the best prediction model for COVID-19 infection. Using 13 input features from the last two weeks, we can predict the cases for the next two weeks. Secondly, we present an innovative way to evaluate the prediction sensitivity to 8 population age groups over highly dynamic multivariate infection data. Thirdly, we compare our proposed perturbation-based interpretation method with related work, including a total of 
    
[^33]: GeoDecoder: 强化多模态地图理解

    GeoDecoder: Empowering Multimodal Map Understanding. (arXiv:2401.15118v1 [cs.CV])

    [http://arxiv.org/abs/2401.15118](http://arxiv.org/abs/2401.15118)

    GeoDecoder是一种专门设计用于处理地图中地理空间信息的多模态模型，通过集成图像和文本处理模块，无缝集成外部数据和特征，以及执行多任务训练和执行，实现了强化地图认知的目标。

    

    本文提出了GeoDecoder，一种专门设计用于处理地图中地理空间信息的多模态模型。GeoDecoder基于BeitGPT架构构建，并集成了图像和文本处理的专业模块。在图像方面，GeoDecoder利用高德地图作为底图，该地图内置了道路和建筑形状、相对位置和其他属性的重要细节。通过渲染技术，该模型无缝集成了外部数据和特征，如符号标记、驾驶轨迹、热力图和用户定义的标记，消除了额外的特征工程需求。GeoDecoder的文本模块接受各种上下文文本和问题提示，并生成类似于GPT的文本输出。此外，基于GPT的模型允许在同一模型中进行多个任务的训练和执行。为了增强地图认知能力并使GeoDecoder获取知识

    This paper presents GeoDecoder, a dedicated multimodal model designed for processing geospatial information in maps. Built on the BeitGPT architecture, GeoDecoder incorporates specialized expert modules for image and text processing. On the image side, GeoDecoder utilizes GaoDe Amap as the underlying base map, which inherently encompasses essential details about road and building shapes, relative positions, and other attributes. Through the utilization of rendering techniques, the model seamlessly integrates external data and features such as symbol markers, drive trajectories, heatmaps, and user-defined markers, eliminating the need for extra feature engineering. The text module of GeoDecoder accepts various context texts and question prompts, generating text outputs in the style of GPT. Furthermore, the GPT-based model allows for the training and execution of multiple tasks within the same model in an end-to-end manner. To enhance map cognition and enable GeoDecoder to acquire knowle
    
[^34]: 实现集体超智能：利用对话群体增强群体智商

    Towards Collective Superintelligence: Amplifying Group IQ using Conversational Swarms. (arXiv:2401.15109v1 [cs.HC])

    [http://arxiv.org/abs/2401.15109](http://arxiv.org/abs/2401.15109)

    这项研究开发了一种叫做对话群体智能（CSI）的新技术，通过自然对话讨论来提高网络人类群体的决策准确性。研究评估了使用CSI平台的实时群体参加Raven智商测试的能力。

    

    群体智能（SI）是一种自然现象，能够通过形成实时系统来增强生物群体的综合智力。人工群体智能（或群体AI）是一种技术，能够通过形成类似系统来增强网络化人类群体的综合智能。过去，基于群体的方法仅限于狭义任务，如概率预测和多项选择决策。一种名为对话群体智能（CSI）的新技术于2023年开发出来，通过自然对话讨论来增强网络人类群体的决策精度。本研究评估了使用CSI平台的实时群体参加一种被称为Raven高级渐进矩阵（RAPM）的常见智商测试的能力。首先，基准组参与者通过传统调查方式接受了Raven智商测试。这个组的平均正确率为45.6%。然后，大约35人的群体回答了同样的智商测试。

    Swarm Intelligence (SI) is a natural phenomenon that enables biological groups to amplify their combined intellect by forming real-time systems. Artificial Swarm Intelligence (or Swarm AI) is a technology that enables networked human groups to amplify their combined intelligence by forming similar systems. In the past, swarm-based methods were constrained to narrowly defined tasks like probabilistic forecasting and multiple-choice decision making. A new technology called Conversational Swarm Intelligence (CSI) was developed in 2023 that amplifies the decision-making accuracy of networked human groups through natural conversational deliberations. The current study evaluated the ability of real-time groups using a CSI platform to take a common IQ test known as Raven's Advanced Progressive Matrices (RAPM). First, a baseline group of participants took the Raven's IQ test by traditional survey. This group averaged 45.6% correct. Then, groups of approximately 35 individuals answered IQ test 
    
[^35]: 多智能体深度强化学习在竞争中为快速充电电动车中心的动态定价中的应用

    Multi-agent Deep Reinforcement Learning for Dynamic Pricing by Fast-charging Electric Vehicle Hubs in ccompetition. (arXiv:2401.15108v1 [cs.LG])

    [http://arxiv.org/abs/2401.15108](http://arxiv.org/abs/2401.15108)

    本文提出了一个多智能体深度强化学习的方法，应用于快速充电电动车中心的动态定价竞争。通过预测性购买电力需求和设定竞争性价格策略，充电站可以在竞争中进行有效定价。

    

    快速充电站将成为全球新建交通电气化基础设施的一部分。这些充电站将承载许多直流快速充电设备，仅可供电动车辆充电使用。类似于汽油加油站，同一地区的快速充电站将根据竞争调整价格以吸引同一群电动车主。这些充电站将与电力网络进行交互，通过预测性购买在前一天电力市场上的电力需求，并在实时市场上满足差额需求。充电站可能配备补充电池储能系统用于套利。本文针对充电站竞争中开发了一个两步数据驱动的动态定价方法。首先通过求解随机的前一天电力需求模型得到纳入承诺，然后通过将游戏建模为竞争来得到充电站的价格策略。

    Fast-charging hubs for electric vehicles will soon become part of the newly built infrastructure for transportation electrification across the world. These hubs are expected to host many DC fast-charging stations and will admit EVs only for charging. Like the gasoline refueling stations, fast-charging hubs in a neighborhood will dynamically vary their prices to compete for the same pool of EV owners. These hubs will interact with the electric power network by making purchase commitments for a significant part of their power needs in the day-ahead (DA) electricity market and meeting the difference from the real-time (RT) market. Hubs may have supplemental battery storage systems (BSS), which they will use for arbitrage. In this paper, we develop a two-step data-driven dynamic pricing methodology for hubs in price competition. We first obtain the DA commitment by solving a stochastic DA commitment model. Thereafter we obtain the hub pricing strategies by modeling the game as a competitiv
    
[^36]: 决策理论基础对评估人类决策的实验的影响

    Decision Theoretic Foundations for Experiments Evaluating Human Decisions. (arXiv:2401.15106v1 [cs.HC])

    [http://arxiv.org/abs/2401.15106](http://arxiv.org/abs/2401.15106)

    该论文通过综合统计决策理论和信息经济学，提出了决策问题的广泛适用定义。为了将人类决策的下降归咎于偏见形式，实验必须向参与者提供足够的信息来识别规范决策。然而，根据作者对AI辅助决策的研究的评估，只有17%的研究提供了足够的信息来描述参与者的行为偏离了良好的决策。

    

    信息展示的决策是可解释AI、人工智能与人类的合作以及数据可视化等领域研究的重点。然而，决策问题的定义以及实验必须具备的条件以得出人类决策存在缺陷的结论仍然存在争议。我们提出了一个广泛适用的决策问题定义，该定义是从统计决策理论和信息经济学中综合提炼而来的。我们认为，要将人类绩效下降归咎于某种偏见形式，实验必须向参与者提供足够的信息，以便合理的代理能够识别规范决策。我们评估了最近有关AI辅助决策的文献中对决策制定进行的评估在多大程度上达到了这一标准。我们发现，只有35项声称确定了有偏差行为的研究中的6项（17%）向参与者提供了足够信息来描述其行为偏离良好决策

    Decision-making with information displays is a key focus of research in areas like explainable AI, human-AI teaming, and data visualization. However, what constitutes a decision problem, and what is required for an experiment to be capable of concluding that human decisions are flawed in some way, remain open to speculation. We present a widely applicable definition of a decision problem synthesized from statistical decision theory and information economics. We argue that to attribute loss in human performance to forms of bias, an experiment must provide participants with the information that a rational agent would need to identify the normative decision. We evaluate the extent to which recent evaluations of decision-making from the literature on AI-assisted decisions achieve this criteria. We find that only 6 (17\%) of 35 studies that claim to identify biased behavior present participants with sufficient information to characterize their behavior as deviating from good decision-making
    
[^37]: 基于扩散增强的超高分辨率遥感图像云去除

    Diffusion Enhancement for Cloud Removal in Ultra-Resolution Remote Sensing Imagery. (arXiv:2401.15105v1 [eess.IV])

    [http://arxiv.org/abs/2401.15105](http://arxiv.org/abs/2401.15105)

    本文提出了一种基于扩散增强的云去除方法，通过在数据和方法上进行改进，实现了对超高分辨率遥感图像中云层的准确去除和详细语义内容恢复。

    

    云层的存在严重影响了光学遥感（RS）图像的质量和效果。然而，现有的基于深度学习（DL）的云去除（CR）技术在准确重建图像的视觉真实性和详细语义内容方面遇到了困难。为了应对这一挑战，本研究提出了在数据和方法上进行改进的方案。在数据方面，建立了一个名为CUHK Cloud Removal (CUHK-CR) 的0.5m空间分辨率的超高分辨率基准并加入了丰富的细节纹理和多样化的云覆盖，为设计和评估CR模型提供了可靠的基础。从方法的角度出发，提出了一种称为Diffusion Enhancement（DE）的基于扩散的CR框架，用于进行渐进纹理细节恢复，以减轻训练难度并提高推理准确性。此外，还提出了一种权重分配（WA）网络。

    The presence of cloud layers severely compromises the quality and effectiveness of optical remote sensing (RS) images. However, existing deep-learning (DL)-based Cloud Removal (CR) techniques encounter difficulties in accurately reconstructing the original visual authenticity and detailed semantic content of the images. To tackle this challenge, this work proposes to encompass enhancements at the data and methodology fronts. On the data side, an ultra-resolution benchmark named CUHK Cloud Removal (CUHK-CR) of 0.5m spatial resolution is established. This benchmark incorporates rich detailed textures and diverse cloud coverage, serving as a robust foundation for designing and assessing CR models. From the methodology perspective, a novel diffusion-based framework for CR called Diffusion Enhancement (DE) is proposed to perform progressive texture detail recovery, which mitigates the training difficulty with improved inference accuracy. Additionally, a Weight Allocation (WA) network is dev
    
[^38]: PruneSymNet：一种用于符号回归的符号神经网络和修剪算法

    PruneSymNet: A Symbolic Neural Network and Pruning Algorithm for Symbolic Regression. (arXiv:2401.15103v1 [cs.LG])

    [http://arxiv.org/abs/2401.15103](http://arxiv.org/abs/2401.15103)

    PruneSymNet是一种用于符号回归的新颖神经网络，可以通过贪婪修剪算法提取子网络以获得所需的符号表达式。

    

    符号回归旨在从数据中导出可解释的符号表达式，以便更好地理解和解释数据。本研究提出了一种名为PruneSymNet的符号网络，用于符号回归。这是一种新颖的神经网络，其激活函数由常见的基本函数和运算符组成。整个网络是可微分的，并可以通过梯度下降方法进行训练。网络中的每个子网络对应一个表达式，我们的目标是提取这些子网络以获得所需的符号表达式。因此，提出了一种贪婪修剪算法，将网络剪成子网络，同时确保数据拟合的精度。所提出的贪婪修剪算法每次修剪都保留损失最小的边，但贪婪算法通常无法得到最优解。为了缓解这个问题，我们结合了束搜索方法。

    Symbolic regression aims to derive interpretable symbolic expressions from data in order to better understand and interpret data. %which plays an important role in knowledge discovery and interpretable machine learning.  In this study, a symbolic network called PruneSymNet is proposed for symbolic regression. This is a novel neural network whose activation function consists of common elementary functions and operators. The whole network is differentiable and can be trained by gradient descent method. Each subnetwork in the network corresponds to an expression, and our goal is to extract such subnetworks to get the desired symbolic expression.  Therefore, a greedy pruning algorithm is proposed to prune the network into a subnetwork while ensuring the accuracy of data fitting. The proposed greedy pruning algorithm preserves the edge with the least loss in each pruning, but greedy algorithm often can not get the optimal solution. In order to alleviate this problem, we combine beam search 
    
[^39]: Hi-Core: 面向连续强化学习的层次化知识迁移

    Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning. (arXiv:2401.15098v1 [cs.LG])

    [http://arxiv.org/abs/2401.15098](http://arxiv.org/abs/2401.15098)

    Hi-Core提出了一种新的框架，通过层次化的知识迁移来增强连续强化学习。该框架包括利用大型语言模型的推理能力设定目标的高层策略制定和通过强化学习按照高层目标导向的低层策略学习。在实验中，Hi-Core展现了较强的知识迁移能力。

    

    连续强化学习（Continual Reinforcement Learning, CRL）赋予强化学习智能体从一系列任务中学习的能力，保留先前的知识并利用它来促进未来的学习。然而，现有的方法往往专注于在类似任务之间传输低层次的知识，忽视了人类认知控制的层次结构，导致在各种任务之间的知识迁移不足。为了增强高层次的知识迁移，我们提出了一种名为Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning)的新框架，它由两层结构组成：1) 利用大型语言模型（Large Language Model, LLM）的强大推理能力设定目标的高层策略制定和2) 通过强化学习按照高层目标导向的低层策略学习。此外，构建了一个知识库（策略库）来存储可以用于层次化知识迁移的策略。在MiniGr实验中进行了实验。

    Continual reinforcement learning (CRL) empowers RL agents with the ability to learn from a sequence of tasks, preserving previous knowledge and leveraging it to facilitate future learning. However, existing methods often focus on transferring low-level knowledge across similar tasks, which neglects the hierarchical structure of human cognitive control, resulting in insufficient knowledge transfer across diverse tasks. To enhance high-level knowledge transfer, we propose a novel framework named Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning), which is structured in two layers: 1) the high-level policy formulation which utilizes the powerful reasoning ability of the Large Language Model (LLM) to set goals and 2) the low-level policy learning through RL which is oriented by high-level goals. Moreover, the knowledge base (policy library) is constructed to store policies that can be retrieved for hierarchical knowledge transfer. Experiments conducted in MiniGr
    
[^40]: 《生成式人工智能和ChatGPT能否在科学领域的认知需求问题解决任务上胜过人类？》

    Can generative AI and ChatGPT outperform humans on cognitive-demanding problem-solving tasks in science?. (arXiv:2401.15081v1 [cs.AI])

    [http://arxiv.org/abs/2401.15081](http://arxiv.org/abs/2401.15081)

    该研究探讨了生成式人工智能工具在解决认知强度的科学问题时是否能超越人类，并通过与学生的对比实验发现，ChatGPT和GPT-4在大多数情况下表现出色。

    

    本研究旨在考察生成式人工智能（GAI）工具能否克服人类在解决问题时所遭受的认知强度。我们将ChatGPT和GPT-4在2019年NAEP科学评估中与学生的表现进行了比较，根据任务的认知需求对五十四个任务进行了编码，其中包括任务的认知复杂度和维度。使用NAEP的评分标准对ChatGPT和GPT-4的回答进行了评分。对可用数据的分析基于每个问题正确回答的学生的平均能力分数和回答每个问题的学生的百分比。结果显示，ChatGPT和GPT-4在大多数回答NAEP科学评估的学生中表现出色。随着NAEP任务的认知需求增加，需要具备统计上更高的平均学生能力分数才能正确回答问题。此项研究提供了关于GAI工具在科学问题解决任务中的性能表现的有益信息。

    This study aimed to examine an assumption that generative artificial intelligence (GAI) tools can overcome the cognitive intensity that humans suffer when solving problems. We compared the performance of ChatGPT and GPT-4 on 2019 NAEP science assessments with students by cognitive demands of the items. Fifty-four tasks were coded by experts using a two-dimensional cognitive load framework, including task cognitive complexity and dimensionality. ChatGPT and GPT-4 responses were scored using the scoring keys of NAEP. The analysis of the available data was based on the average student ability scores for students who answered each item correctly and the percentage of students who responded to individual items. Results showed that both ChatGPT and GPT-4 consistently outperformed most students who answered the NAEP science assessments. As the cognitive demand for NAEP tasks increases, statistically higher average student ability scores are required to correctly address the questions. This pa
    
[^41]: AI在基于项目的学习中的未来规划：与学生一起进行共同设计探索

    Charting the Future of AI in Project-Based Learning: A Co-Design Exploration with Students. (arXiv:2401.14915v1 [cs.HC])

    [http://arxiv.org/abs/2401.14915](http://arxiv.org/abs/2401.14915)

    本文介绍了一项与学生共同设计的研究，探索学生在项目化学习中使用人工智能的潜力，并基于学生的视觉对教育目标转变进行分析。研究发现，不同态度的学生对人工智能的使用有不同的偏好。这为未来研究学生与人工智能交互和理解增强学习提供了机会。

    

    学生在学习中越来越多地使用人工智能（AI），这给基于项目的学习（PBL）的评估带来了新的挑战。本文介绍了一项共同设计研究，探索学生的AI使用数据作为PBL评估的新材料的潜力。我们与18名大学生进行了研讨会，鼓励他们设想一个可以自由使用AI进行PBL的另一个世界，同时需要报告这个过程以评估他们的技能和贡献。我们的研讨会产生了各种学生在PBL中使用AI的情景，以及基于学生对教育目标转变的视觉进行分析这些使用的方式。我们还发现对AI持有不同态度的学生在分析和理解AI使用方面有不同的偏好。基于这些发现，我们讨论了关于学生与AI交互和理解AI增强学习的未来研究机会。

    The increasing use of Artificial Intelligence (AI) by students in learning presents new challenges for assessing their learning outcomes in project-based learning (PBL). This paper introduces a co-design study to explore the potential of students' AI usage data as a novel material for PBL assessment. We conducted workshops with 18 college students, encouraging them to speculate an alternative world where they could freely employ AI in PBL while needing to report this process to assess their skills and contributions. Our workshops yielded various scenarios of students' use of AI in PBL and ways of analyzing these uses grounded by students' vision of education goal transformation. We also found students with different attitudes toward AI exhibited distinct preferences in how to analyze and understand the use of AI. Based on these findings, we discuss future research opportunities on student-AI interactions and understanding AI-enhanced learning.
    
[^42]: 以质量守恒感知器为基础，实现可解释的物理-概念集水区尺度水文建模

    Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron. (arXiv:2401.14521v1 [cs.LG])

    [http://arxiv.org/abs/2401.14521](http://arxiv.org/abs/2401.14521)

    本研究通过利用质量守恒感知器构建基于有向图结构的水文模型，实现了对集水区尺度水文过程的解释能力，在保持简洁性的同时能够准确地模拟各种流量动力学行为，并通过引入输入旁路机制进一步优化了模型的表现。

    

    本研究探讨了利用机器学习技术开发简洁可解释的集水区尺度水文模型的可行性，采用基于质量守恒感知器（MCP）的有向图结构作为基本计算单元。我们关注的是单个位置的结构复杂性（深度），而不是对大样本集水区具有普适性的广度。目标是发现一个最小的表示（单元状态数和流量路径数），用于表示能够解释给定集水区输入状态和输出行为的主要过程，特别强调模拟全范围（高、中、低）的流量动力学。我们发现，在我们的研究区域，采用类似HyMod的架构，具有3个单元状态和2个主要流动路径，能够实现这样的表示，但引入输入旁路机制可以显著改善水文图的时间和形状。

    We investigate the applicability of machine learning technologies to the development of parsimonious, interpretable, catchment-scale hydrologic models using directed-graph architectures based on the mass-conserving perceptron (MCP) as the fundamental computational unit. Here, we focus on architectural complexity (depth) at a single location, rather than universal applicability (breadth) across large samples of catchments. The goal is to discover a minimal representation (numbers of cell-states and flow paths) that represents the dominant processes that can explain the input-state-output behaviors of a given catchment, with particular emphasis given to simulating the full range (high, medium, and low) of flow dynamics. We find that a HyMod-like architecture with three cell-states and two major flow pathways achieves such a representation at our study location, but that the additional incorporation of an input-bypass mechanism significantly improves the timing and shape of the hydrograph
    
[^43]: 通过GPT引导的蒙特卡洛树搜索从数据中发现数学公式

    Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search. (arXiv:2401.14424v1 [cs.LG])

    [http://arxiv.org/abs/2401.14424](http://arxiv.org/abs/2401.14424)

    通过结合MCTS和生成式预训练模型，我们提出了一种新的符号回归算法SR-GPT，在发现数据中的数学公式方面取得了显著的改进。

    

    在科学研究和人工智能中，找到一个简洁且可解释的数学公式来准确描述数据中每个变量与预测值之间的关系是一个关键任务，也是一个重大挑战。这个问题被称为符号回归，是一个NP困难问题。去年，提出了一种基于蒙特卡洛树搜索（MCTS）的符号回归方法，并在多个数据集上获得了sota。虽然与以前的方法相比，该算法在恢复目标表达式方面显示出了相当大的改进，但是在MCTS过程中缺乏引导严重阻碍了其搜索效率。最近，一些算法在MCTS的搜索中添加了一个预训练的策略网络，但是这个预训练的策略网络的泛化能力很差。为了平衡效率和通用性，我们提出了SR-GPT，结合了AlphaZero的思想。SR-GPT是一种新的符号回归算法，将MCTS与一个通用性较好的生成式预训练模型相结合。

    Finding a concise and interpretable mathematical formula that accurately describes the relationship between each variable and the predicted value in the data is a crucial task in scientific research, as well as a significant challenge in artificial intelligence. This problem is referred to as symbolic regression, which is an NP-hard problem. Last year, a symbolic regression method based on Monte Carlo Tree Search (MCTS) was proposed and sota was obtained on multiple datasets. While this algorithm has shown considerable improvement in recovering target expressions compared to previous methods, the lack of guidance during the MCTS process severely hampers its search efficiency. Recently, some algorithms have added a pre-trained policy network to guide the search of MCTS, but the pre-trained policy network generalizes poorly. To balance efficiency and generality, we propose SR-GPT combining ideas from AlphaZero. SR-GPT is a new symbolic regression algorithm that combines MCTS with a Gener
    
[^44]: 自适应移动操作在开放环境中的可关节物体研究

    Adaptive Mobile Manipulation for Articulated Objects In the Open World. (arXiv:2401.14403v1 [cs.RO])

    [http://arxiv.org/abs/2401.14403](http://arxiv.org/abs/2401.14403)

    本文介绍了一种针对开放环境中关节物体操作的全栈方法，机器人通过自适应学习框架从少量数据中学习，并通过在线实践学习适应训练分布之外的新对象。同时，还开发了低成本的移动操作硬件平台。

    

    在开放的无结构环境中部署机器人一直是一个长期存在的问题。然而，机器人通常只在封闭的实验室环境中进行研究，之前的移动操作工作也仅限于拾取、移动、放置，这在这一领域中只是冰山一角。在本文中，我们引入了开放世界移动操作系统，采用全栈方法来解决现实世界中可关节物体的操作，例如真实世界中的门、柜子、抽屉和冰箱。机器人利用自适应学习框架，通过行为克隆先从一小组数据中学习，然后通过在线实践学习来处理训练分布之外的新对象。我们还开发了一个低成本的移动操作硬件平台，能够在无结构环境中进行安全和自主的在线适应，成本约为20,000美元。在我们的实验中，我们使用了20个可关节的物体。

    Deploying robots in open-ended unstructured environments such as homes has been a long-standing research problem. However, robots are often studied only in closed-off lab settings, and prior mobile manipulation work is restricted to pick-move-place, which is arguably just the tip of the iceberg in this area. In this paper, we introduce Open-World Mobile Manipulation System, a full-stack approach to tackle realistic articulated object operation, e.g. real-world doors, cabinets, drawers, and refrigerators in open-ended unstructured environments. The robot utilizes an adaptive learning framework to initially learns from a small set of data through behavior cloning, followed by learning from online practice on novel objects that fall outside the training distribution. We also develop a low-cost mobile manipulation hardware platform capable of safe and autonomous online adaptation in unstructured environments with a cost of around 20,000 USD. In our experiments we utilize 20 articulate obje
    
[^45]: ZS4C: 使用ChatGPT进行零射击合成不完整代码片段的可编译代码

    ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using ChatGPT. (arXiv:2401.14279v1 [cs.SE] CROSS LISTED)

    [http://arxiv.org/abs/2401.14279](http://arxiv.org/abs/2401.14279)

    ZS4C提出了一种使用ChatGPT进行零射击合成可编译代码的轻量级方法，帮助用户重用或分析不完整的Q&A代码片段，通过识别缺失的导入语句并修复编译错误来实现。

    

    技术问答（Q&A）网站如Stack Overflow已成为软件开发者寻求知识的重要来源。然而，Q&A网站上的代码片段通常由于未解析的类型和缺失的依赖库而无法编译和语义上不完整，这增加了用户重用或分析Q&A代码片段的障碍。之前的方法要么不适用于合成可编译代码，要么编译成功率低。为了解决这个问题，我们提出了ZS4C，一种使用大型语言模型（LLM）从不完整的代码片段中进行零射击合成可编译代码的轻量级方法。ZS4C分为两个阶段。在第一阶段，ZS4C利用一个LLM，即ChatGPT，根据我们设计的专用任务提示模板，为给定的代码片段识别缺失的导入语句。在第二阶段，ZS4C通过修复由于不正确的导入语句和语法错误引起的编译错误来修复代码。

    Technical question and answering (Q&A) sites such as Stack Overflow have become an important source for software developers to seek knowledge. However, code snippets on Q&A sites are usually uncompilable and semantically incomplete for compilation due to unresolved types and missing dependent libraries, which raises the obstacle for users to reuse or analyze Q&A code snippets. Prior approaches either are not designed for synthesizing compilable code or suffer from a low compilation success rate. To address this problem, we propose ZS4C, a lightweight approach to perform zero-shot synthesis of compilable code from incomplete code snippets using Large Language Model (LLM). ZS4C operates in two stages. In the first stage, ZS4C utilizes an LLM, i.e., ChatGPT, to identify missing import statements for a given code snippet, leveraging our designed task-specific prompt template. In the second stage, ZS4C fixes compilation errors caused by incorrect import statements and syntax errors through 
    
[^46]: Sketch2NeRF: 多视角草图引导的文本到3D生成

    Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation. (arXiv:2401.14257v1 [cs.CV])

    [http://arxiv.org/abs/2401.14257](http://arxiv.org/abs/2401.14257)

    本文提出了一个多视角草图引导的文本到3D生成框架(Sketch2NeRF)，以增加对3D生成的草图控制。通过利用预训练的2D扩散模型和神经辐射场来优化3D场景实现细粒度控制。实验证明，该方法能够合成一致的3D内容。

    

    最近，文本到3D的方法通过文本描述实现了高保真度的3D内容生成。然而，生成的对象是随机的，并且缺乏细粒度的控制。草图提供了一种廉价的方法来引入这种细粒度的控制。然而，由于草图的抽象和模糊性，实现从这些草图中获得灵活的控制是具有挑战性的。在本文中，我们提出了一个多视角草图引导的文本到3D生成框架（即Sketch2NeRF），以增加对3D生成的草图控制。具体而言，我们的方法利用预训练的2D扩散模型（例如Stable Diffusion和ControlNet）来监督由神经辐射场（NeRF）表示的3D场景的优化。我们提出了一种新颖的同步生成和重建方法，以有效优化NeRF。在实验中，我们收集了两种多视角草图数据集来评估所提出的方法。我们证明我们的方法可以合成一致的3D内容。

    Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents w
    
[^47]: 通过上下文感知个性化细化，增强长期对话中的常识增强性内存构建和管理

    Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement. (arXiv:2401.14215v1 [cs.CL])

    [http://arxiv.org/abs/2401.14215](http://arxiv.org/abs/2401.14215)

    本文提出了一个旨在解决长期对话中角色句子不具信息性的问题的框架，通过利用常识增强的角色扩展，并设计策略将相互矛盾的角色转化为包含丰富说话者信息的句子，以提高回应生成质量。

    

    在长期对话中，记忆和利用说话者的角色是生成回应的常见做法。然而，人工编写的数据集通常提供无信息的角色句子，这妨碍了回应质量。本文提出了一个新颖的框架，利用常识增强的角色扩展来解决长期对话中的这些问题。以前的工作侧重于不产生与其他角色相矛盾的角色，我们侧重于根据设计的策略，将相互矛盾的角色转化为包含丰富说话者信息的句子，以此来细化它们的上下文背景。作为多会话情境中角色扩展的先驱，我们的框架通过类人个性细化促进了更好的回应生成。

    Memorizing and utilizing speakers' personas is a common practice for response generation in long-term conversations. Yet, human-authored datasets often provide uninformative persona sentences that hinder response quality. This paper presents a novel framework that leverages commonsense-based persona expansion to address such issues in long-term conversation. While prior work focuses on not producing personas that contradict others, we focus on transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies. As the pioneer of persona expansion in multi-session settings, our framework facilitates better response generation via human-like persona refinement. The supplementary video of our work is available at https://caffeine-15bbf.web.app/.
    
[^48]: BayesPrompt: 通过无偏领域抽象在少样本推理上指导大规模预训练语言模型

    BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction. (arXiv:2401.14166v1 [cs.CL])

    [http://arxiv.org/abs/2401.14166](http://arxiv.org/abs/2401.14166)

    BayesPrompt通过无偏领域抽象解决大规模预训练语言模型在少样本推理中的泛化问题。

    

    作为一种基于大规模预训练语言模型（PLMs）的新颖有效的微调范式，prompt-tuning旨在缩小下游任务与预训练目标之间的差距。虽然prompt-tuning在各种任务中取得了持续进展，但这种方法仍然存在一个持久的缺陷：prompt-tuning方法无法泛化到特定的少样本模式。从分布分析的角度来看，我们揭示了这一现象背后的内在问题是PLMs中包含过多的概念知识和目标下游领域的缩减知识，两者共同导致PLMs在普遍的知识嵌入空间中错误地定位与目标领域相对应的知识分布。为此，我们直观地探索了以无偏方式逼近下游任务的完整目标领域，并通过抽象这样的领域生成有区别的提示，从而提供了无歧义的信息。

    As a novel and effective fine-tuning paradigm based on large-scale pre-trained language models (PLMs), prompt-tuning aims to reduce the gap between downstream tasks and pre-training objectives. While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns. From the perspective of distribution analyses, we disclose that the intrinsic issues behind the phenomenon are the over-multitudinous conceptual knowledge contained in PLMs and the abridged knowledge for target downstream domains, which jointly result in that PLMs mis-locate the knowledge distributions corresponding to the target domains in the universal knowledge embedding space. To this end, we intuitively explore to approximate the unabridged target domains of downstream tasks in a debiased manner, and then abstract such domains to generate discriminative prompts, thereby providing the de-ambiguous
    
[^49]: WebVoyager：使用大型多模态模型构建端到端的Web Agent

    WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v1 [cs.CL])

    [http://arxiv.org/abs/2401.13919](http://arxiv.org/abs/2401.13919)

    WebVoyager是一种创新的基于大型多模态模型的Web代理，能够通过与真实网站交互来端到端地完成用户指令。它提出了一个新的Web代理评估协议，并在实际任务中取得了显著的成功率。

    

    大型语言模型（LLMs）的进步引领了一个由真实世界中自主应用程序的发展所标志的新时代，推动了基于网络的高级代理的创新。现有的网络代理通常只处理一个输入模态，并且仅在简化的网络模拟器或静态的网络快照中进行评估，极大地限制了它们在真实场景中的适用性。为了填补这一差距，我们引入了WebVoyager，一种创新的基于大型多模态模型（LMM）的Web代理，通过与真实网站进行交互，能够端到端地完成用户指令。此外，我们提出了一个新的Web代理评估协议，以解决开放式Web代理任务的自动评估挑战，利用了GPT-4V的强大多模态理解能力。我们通过收集来自15个广泛使用的网站的真实世界任务来创建一个新的基准来评估我们的代理。我们展示了WebVoyager实现了55.7％的任务成功率，显著地.....

    The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7% task success rate, significantly 
    
[^50]: 研究大型语言模型在代码克隆检测方面的功效

    Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v1 [cs.SE])

    [http://arxiv.org/abs/2401.13802](http://arxiv.org/abs/2401.13802)

    这项研究探索了大型语言模型在代码克隆检测任务中的应用。

    

    大型语言模型（LLMs）在各种自然语言处理和软件工程任务中表现出了显著的成功，例如代码生成。LLMs主要在基于提示的零/少样本范式中被用于指导模型完成任务。本研究探索了LLMs在代码克隆检测（CCD）这一非生成任务中的适用性。

    Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. %\textbf{Goal:} GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are `generative' tasks. However, there is limited research on the usage of LLMs for `non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect \textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We \textcolor{blac
    
[^51]: 有关算法决策的信息：探索受到算法决策影响的人的信息需求。

    Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions. (arXiv:2401.13324v1 [cs.HC])

    [http://arxiv.org/abs/2401.13324](http://arxiv.org/abs/2401.13324)

    本研究探讨了受算法决策影响的人的信息需求，发现解释往往不能满足他们的关注点，导致对监管框架的理解和遵守产生障碍。为了解决这个问题，研究团队提出了XAI初学者问题库，涵盖了就业预测和健康监测两个领域中受影响利益相关者的信息需求。

    

    AI系统的解释很少涉及到受算法决策影响的人的信息需求。这种传达信息与受影响利益相关者所关心的信息之间的差距可能阻碍对监管框架（如AI法案）的理解和遵守。为了解决这个差距，我们提出了“XAI初学者问题库”：这是一个涵盖两个算法决策应用领域（就业预测和健康监测）中受影响利益相关者信息需求的目录，包括数据、系统背景、系统使用和系统规范等类别。信息需求是通过访谈研究收集的，参与者根据自己的问题获得解释。参与者还报告了他们的理解和决策信心，结果显示，尽管在接受解释后信心倾向于增加，但参与者也面临着理解上的挑战，如无法解释为什么自己的理解感觉不完整。解释还对理解产生了影响。

    Explanations of AI systems rarely address the information needs of people affected by algorithmic decision-making (ADM). This gap between conveyed information and information that matters to affected stakeholders can impede understanding and adherence to regulatory frameworks such as the AI Act. To address this gap, we present the "XAI Novice Question Bank": A catalog of affected stakeholders' information needs in two ADM use cases (employment prediction and health monitoring), covering the categories data, system context, system usage, and system specifications. Information needs were gathered in an interview study where participants received explanations in response to their inquiries. Participants further reported their understanding and decision confidence, showing that while confidence tended to increase after receiving explanations, participants also met understanding challenges, such as being unable to tell why their understanding felt incomplete. Explanations further influenced
    
[^52]: AI助手是否能知道自己不知道的事情?

    Can AI Assistants Know What They Don't Know?. (arXiv:2401.13275v1 [cs.CL])

    [http://arxiv.org/abs/2401.13275](http://arxiv.org/abs/2401.13275)

    本文研究了AI助手是否能知道自己不知道的事情，并通过自然语言表达出来的问题。为了回答这个问题，我们构建了一个特定模型的"I don't know"（Idk）数据集，并与AI助手进行对齐。

    

    最近，基于大型语言模型（LLMs）的AI助手在对话、解决数学问题、编写代码和使用工具等许多任务中表现出令人惊讶的性能。尽管LLMs具有深入的世界知识，但在面对某些知识密集型任务（如开放领域问答）时仍然会出现事实错误。AI助手的这种不真实回答可能在实际应用中造成重大风险。我们认为，AI助手拒绝回答自己不知道的问题是减少幻觉和使助手真实的关键方法。因此，在本文中，我们提出问题“AI助手是否能知道自己不知道的事情，并通过自然语言表达出来？”为了回答这个问题，我们为助手构建了一个特定模型的“I don't know”(Idk)数据集，其中包含了已知和未知的问题，基于现有的开放领域问答数据集。然后我们将助手与其相应的Idk数据进行对齐。

    Recently, AI assistants based on large language models (LLMs) show surprising performance in many tasks, such as dialogue, solving math problems, writing code, and using tools. Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering. These untruthful responses from the AI assistant may cause significant risks in practical applications. We believe that an AI assistant's refusal to answer questions it does not know is a crucial method for reducing hallucinations and making the assistant truthful. Therefore, in this paper, we ask the question "Can AI assistants know what they don't know and express them through natural language?" To answer this question, we construct a model-specific "I don't know" (Idk) dataset for an assistant, which contains its known and unknown questions, based on existing open-domain question answering datasets. Then we align the assistant with its corresponding I
    
[^53]: DISCOUNT: 使用最优传输进行分布式对抗解释

    DISCOUNT: Distributional Counterfactual Explanation With Optimal Transport. (arXiv:2401.13112v1 [cs.AI])

    [http://arxiv.org/abs/2401.13112](http://arxiv.org/abs/2401.13112)

    本文提出了使用最优传输进行分布式对抗解释的方法DISCOUNT，将对抗解释的概念扩展到整个输入输出分布，并通过统计置信度来支撑这一方法。

    

    对抗解释是在黑盒决策模型中提供洞察力和可解释性的事实方法，通过确定导致不同结果的替代输入实例来实现。本文将对抗解释的概念扩展到分布上下文，从个体数据点扩大到整个输入输出分布，命名为分布式对抗解释。在分布式对抗解释中，我们的重点转向分析事实和对抗的分布属性，类似于评估个体实例及其结果决策的经典方法。我们利用最优传输来构建一个机会约束优化问题，旨在导出与事实对应的对抗分布，以统计置信度做支撑。我们提出的优化方法DISCOUNT在输入和输出分布之间平衡这种置信度。

    Counterfactual Explanations (CE) is the de facto method for providing insight and interpretability in black-box decision-making models by identifying alternative input instances that lead to different outcomes. This paper extends the concept of CEs to a distributional context, broadening the scope from individual data points to entire input and output distributions, named Distributional Counterfactual Explanation (DCE). In DCE, our focus shifts to analyzing the distributional properties of the factual and counterfactual, drawing parallels to the classical approach of assessing individual instances and their resulting decisions. We leverage Optimal Transport (OT) to frame a chance-constrained optimization problem, aiming to derive a counterfactual distribution that closely aligns with its factual counterpart, substantiated by statistical confidence. Our proposed optimization method, DISCOUNT, strategically balances this confidence across both input and output distributions. This algorit
    
[^54]: 通过重力信息驱动的深度学习框架预测非本地物种船舶交通流量和入侵风险

    Gravity-Informed Deep Learning Framework for Predicting Ship Traffic Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge. (arXiv:2401.13098v1 [cs.LG])

    [http://arxiv.org/abs/2401.13098](http://arxiv.org/abs/2401.13098)

    通过考虑航运通量密度、港口距离、贸易流量和交通枢纽的中心性指标等因素，本研究开发了一个受物理启发的模型来预测海事航运流量，并用于指导全球交通网络中入侵物种的风险评估和管理。

    

    水体中的入侵物种对全球环境和生物多样性构成了重大威胁。由于交通和贸易增加，非本土物种已经引入了新的环境，导致生态系统破坏，并导致农业、林业和渔业方面的经济损失。因此，迫切需要风险评估和管理技术以减轻这些入侵的影响。本研究旨在开发一种新的受物理启发的模型，用于预测海事航运交通流量，并以此指导通过全球交通网络传播的入侵物种风险评估。受国际贸易重力模型的启发，我们的模型考虑了影响船舶活动可能性和影响的各种因素，如航运通量密度、港口之间的距离、贸易流量和交通枢纽的中心性指标。此外，通过分析入侵物种的风险网络，我们为评估和管理入侵提供了全面的框架。

    Invasive species in water bodies pose a major threat to the environment and biodiversity globally. Due to increased transportation and trade, non-native species have been introduced to new environments, causing damage to ecosystems and leading to economic losses in agriculture, forestry, and fisheries. Therefore, there is a pressing need for risk assessment and management techniques to mitigate the impact of these invasions. This study aims to develop a new physics-inspired model to forecast maritime shipping traffic and thus inform risk assessment of invasive species spread through global transportation networks. Inspired by the gravity model for international trades, our model considers various factors that influence the likelihood and impact of vessel activities, such as shipping flux density, distance between ports, trade flow, and centrality measures of transportation hubs. Additionally, by analyzing the risk network of invasive species, we provide a comprehensive framework for as
    
[^55]: 在线学习世界模型的局部敏感稀疏编码

    Locality Sensitive Sparse Encoding for Learning World Models Online. (arXiv:2401.13034v1 [cs.LG])

    [http://arxiv.org/abs/2401.13034](http://arxiv.org/abs/2401.13034)

    本文提出了一种基于局部敏感稀疏编码的线性回归模型，通过非线性随机特征实现对复杂环境的拟合。这种模型能够高效地进行稀疏更新，实现了优化拟合先前经验的Follow-The-Leader（FTL）世界模型。

    

    为了解决神经网络在在线学习中遇到的数据非平稳性问题，本文提出了一种基于局部敏感稀疏编码的线性回归模型，该模型通过非线性随机特征实现了对复杂环境的拟合。通过引入局部敏感稀疏编码，我们能够进行高效的稀疏更新，在平衡模型容量和计算效率的同时实现优化拟合所有先前经验的Follow-The-Leader（FTL）世界模型。

    Acquiring an accurate world model online for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even 
    
[^56]: ClipSAM：CLIP和SAM的合作用于零样本异常分割

    ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation. (arXiv:2401.12665v1 [cs.CV])

    [http://arxiv.org/abs/2401.12665](http://arxiv.org/abs/2401.12665)

    这项工作提出了一种名为ClipSAM的CLIP和SAM协作框架，用于零样本异常分割。ClipSAM利用CLIP的语义理解能力进行异常定位和粗糙分割，然后将其用作SAM的提示约束，进一步改进异常分割结果。

    

    最近，基于CLIP和SAM等基础模型在零样本异常分割（ZSAS）任务中表现出了很有希望的性能。然而，无论是基于CLIP还是SAM的ZSAS方法仍然存在一些不可忽视的关键缺点：1）CLIP主要关注不同输入之间的全局特征对齐，导致对局部异常部分的分割不准确；2）SAM倾向于生成大量没有适当提示约束的冗余掩码，导致复杂的后处理要求。在这项工作中，我们创新性地提出了一种名为ClipSAM的CLIP和SAM协作框架，用于ZSAS。ClipSAM的思路是利用CLIP的语义理解能力进行异常定位和粗糙分割，进一步将其用作提供提示约束以改进SAM的异常分割结果。具体地，我们引入了一个关键的统一多尺度跨模态交互（UMCI）模块，用于在多个尺度上相互作用语言和视觉特征。

    Recently, foundational models such as CLIP and SAM have shown promising performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However, either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible key drawbacks: 1) CLIP primarily focuses on global feature alignment across different inputs, leading to imprecise segmentation of local anomalous parts; 2) SAM tends to generate numerous redundant masks without proper prompt constraints, resulting in complex post-processing requirements. In this work, we innovatively propose a CLIP and SAM collaboration framework called ClipSAM for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding capability for anomaly localization and rough segmentation, which is further used as the prompt constraints for SAM to refine the anomaly segmentation results. In details, we introduce a crucial Unified Multi-scale Cross-modal Interaction (UMCI) module for interacting language with visual features at multiple sc
    
[^57]: Tensor视图拓扑图神经网络

    Tensor-view Topological Graph Neural Network. (arXiv:2401.12007v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.12007](http://arxiv.org/abs/2401.12007)

    提出了一种新颖的Tensor视图拓扑图神经网络（TTG-NN），该方法结合了持久同调、图卷积和张量运算，同时捕捉了局部和全局层面上的Tensor视图拓扑（TT）和Tensor视图图（TG）结构信息。

    

    图分类是一项重要的图结构数据学习任务。图神经网络（GNNs）近年来在图学习中引起了越来越多的关注，并在许多重要的图问题上显示出显著的改进。尽管现有的GNNs在性能上处于最前沿，但它们只使用了每个节点周围非常有限的邻域的局部信息，导致了多模态信息的丢失和过多计算的开销。为了解决这些问题，我们提出了一种新颖的Tensor视图拓扑图神经网络（TTG-NN），这是一种简单而有效的基于持久同调、图卷积和张量运算的拓扑深度学习方法。这种新方法同时捕捉了局部和全局层面上的Tensor视图拓扑（TT）和Tensor视图图（TG）结构信息，并在计算上充分利用了图的拓扑和结构。

    Graph classification is an important learning task for graph-structured data. Graph neural networks (GNNs) have recently gained growing attention in graph learning and have shown significant improvements in many important graph problems. Despite their state-of-the-art performances, existing GNNs only use local information from a very limited neighborhood around each node, suffering from loss of multi-modal information and overheads of excessive computation. To address these issues, we propose a novel Tensor-view Topological Graph Neural Network (TTG-NN), a class of simple yet effective topological deep learning built upon persistent homology, graph convolution, and tensor operations. This new method incorporates tensor learning to simultaneously capture Tensor-view Topological (TT), as well as Tensor-view Graph (TG) structural information on both local and global levels. Computationally, to fully exploit graph topology and structure, we propose two flexible TT and TG representation lea
    
[^58]: 通过思维方程蒸馏改进小型语言模型的数学推理能力

    Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.11864](http://arxiv.org/abs/2401.11864)

    本研究提出了思维方程蒸馏（EoTD）技术和集合思维蒸馏（ETD）框架，通过构建基于方程的表示和使用多个思维过程的推理数据集来改进小型语言模型（SLMs）的数学推理能力，实验结果表明，EoTD和ETD显著提升了SLMs的推理能力。

    

    本研究解决了将先进的大型语言模型（LLMs）的数学推理能力压缩到具有小于十亿参数的小型语言模型（SLMs）中的挑战，同时不损害性能。我们引入了一种新颖的思维方程蒸馏（EoTD）技术，将推理过程封装为基于方程的表示，构建了一个EoTD数据集来对SLMs进行微调。此外，我们提出了集合思维蒸馏（ETD）框架，以提升SLMs的推理性能。这包括创建一个包含多个思维过程（包括思维链、思维程序和思维方程）的推理数据集，并将其用于微调。我们的实验证明，EoTD显著提升了SLMs的推理能力，而ETD使这些模型实现了最先进的推理性能。

    This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while ETD enables these models to achieve state-of-the-art reasoning performance.
    
[^59]: 空间-时间图卷积网络在交通预测上的知识蒸馏

    Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction. (arXiv:2401.11798v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.11798](http://arxiv.org/abs/2401.11798)

    本论文研究了在交通预测中应用空间-时间图卷积网络和知识蒸馏的方法。知识蒸馏的思想能够实现在减少参数和保持准确性的同时提高执行效率。通过引入教师网络的空间-时间相关性，我们的方法能够使学生网络学习到复杂的交通模式。

    

    高效实时交通预测对减少交通时间至关重要。为了预测交通状况，我们采用了空间-时间图神经网络（ST-GNN）将实时交通数据建模为时间图。尽管ST-GNN具有强大的能力，但在为实际交通数据进行高效实时预测时经常面临挑战。鉴于实时数据动态性的重要性，我们采用知识蒸馏（KD）作为解决方案，以提高ST-GNN在交通预测中的执行时间。本文介绍了一个成本函数，旨在使用复杂网络（教师）的蒸馏数据来训练具有较少参数的网络（学生），同时保持其准确性接近教师的准确性。我们使用知识蒸馏，将教师网络的空间-时间相关性融入学生网络，使学生能够学习到教师感知的复杂模式。然而，面临一个挑战。

    Efficient real-time traffic prediction is crucial for reducing transportation time. To predict traffic conditions, we employ a spatio-temporal graph neural network (ST-GNN) to model our real-time traffic data as temporal graphs. Despite its capabilities, it often encounters challenges in delivering efficient real-time predictions for real-world traffic data. Recognizing the significance of timely prediction due to the dynamic nature of real-time data, we employ knowledge distillation (KD) as a solution to enhance the execution time of ST-GNNs for traffic prediction. In this paper, We introduce a cost function designed to train a network with fewer parameters (the student) using distilled data from a complex network (the teacher) while maintaining its accuracy close to that of the teacher. We use knowledge distillation, incorporating spatial-temporal correlations from the teacher network to enable the student to learn the complex patterns perceived by the teacher. However, a challenge a
    
[^60]: 安全且广义的端到端自主驾驶系统：基于强化学习和示范的研究

    Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2401.11792](http://arxiv.org/abs/2401.11792)

    本文介绍了一种安全且广义的端到端自主驾驶系统 (SGADS)，使用强化学习和示范相结合的方法解决了现有方法的低安全性、泛化能力差和采样效率低的问题，同时引入了变分推理和归一化流以准确预测驾驶轨迹，并提出了鲁棒性安全约束的制定方法。

    

    一个智能驾驶系统应该能够根据当前环境和车辆状态动态制定适当的驾驶策略，同时确保系统的安全性和可靠性。然而，基于强化学习和模仿学习的现有方法存在安全性低、泛化能力差和采样效率低的问题。此外，它们无法准确预测未来的驾驶轨迹，而准确预测未来的驾驶轨迹是做出最优决策的前提。为了解决这些问题，本文引入了一种复杂而多样场景下的安全且广义的端到端自主驾驶系统 (SGADS)。我们的SGADS与变分推理和归一化流结合，使智能车辆能够准确预测未来的驾驶轨迹。此外，我们提出了鲁棒性安全约束的制定。此外，我们将强化学习与示范相结合进行增强学习。

    An intelligent driving system should be capable of dynamically formulating appropriate driving strategies based on the current environment and vehicle status, while ensuring the security and reliability of the system. However, existing methods based on reinforcement learning and imitation learning suffer from low safety, poor generalization, and inefficient sampling. Additionally, they cannot accurately predict future driving trajectories, and the accurate prediction of future driving trajectories is a precondition for making optimal decisions. To solve these problems, in this paper, we introduce a Safe and Generalized end-to-end Autonomous Driving System (SGADS) for complex and various scenarios. Our SGADS incorporates variational inference with normalizing flows, enabling the intelligent vehicle to accurately predict future driving trajectories. Moreover, we propose the formulation of robust safety constraints. Furthermore, we combine reinforcement learning with demonstrations to aug
    
[^61]: 解析机器学习在物联网生态系统中的攻击：调查及其背后的开放库

    Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey and the Open Libraries Behind Them. (arXiv:2401.11723v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2401.11723](http://arxiv.org/abs/2401.11723)

    该论文对机器学习在物联网生态系统中引起的安全威胁进行了综合探索，包括会员推断、对抗规避、重建、属性推断、模型提取和毒化攻击等多种攻击类型。

    

    物联网的出现带来了前所未有的连接性时代，预计到2025年底将有约800亿个智能设备投入运营。这些设备促进了众多智能应用，在各个领域提高了生活质量和效率。机器学习作为一项关键技术，不仅用于分析物联网生成的数据，还用于物联网生态系统内的多种应用。例如，机器学习在物联网设备识别、异常检测甚至揭示恶意活动方面发挥着重要作用。本文对机器学习整合到物联网各个方面引发的安全威胁进行了全面探索，包括会员推断、对抗规避、重建、属性推断、模型提取和毒化攻击等各种攻击类型。与以往的研究不同，我们的工作提供了一种整体的视角，根据诸如...的标准对威胁进行分类。

    The advent of the Internet of Things (IoT) has brought forth an era of unprecedented connectivity, with an estimated 80 billion smart devices expected to be in operation by the end of 2025. These devices facilitate a multitude of smart applications, enhancing the quality of life and efficiency across various domains. Machine Learning (ML) serves as a crucial technology, not only for analyzing IoT-generated data but also for diverse applications within the IoT ecosystem. For instance, ML finds utility in IoT device recognition, anomaly detection, and even in uncovering malicious activities. This paper embarks on a comprehensive exploration of the security threats arising from ML's integration into various facets of IoT, spanning various attack types including membership inference, adversarial evasion, reconstruction, property inference, model extraction, and poisoning attacks. Unlike previous studies, our work offers a holistic perspective, categorizing threats based on criteria such as
    
[^62]: 通过具有分层正则化的医学代码中心的多模态对比EHR建模预测下次就诊诊断

    Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal Contrastive EHR Modelling with Hierarchical Regularisation. (arXiv:2401.11648v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.11648](http://arxiv.org/abs/2401.11648)

    通过医学代码中心的多模态对比EHR建模预测下次就诊诊断，并通过分层正则化提高性能。

    

    在医疗保健中，利用电子健康记录（EHR）预测下次就诊的诊断是一项必要的任务，对于制定医疗保健提供者和患者的主动未来计划至关重要。然而，之前的许多研究并没有充分解决EHR数据固有的异构和分层特征，必然导致次优的性能。为此，我们提出了NECHO，一种新颖的医学代码中心的多模态对比EHR学习框架，其中包括分层正则化。首先，我们使用定制的网络设计和一对双模态对比损失融合涵盖医学代码、人口统计数据和临床笔记的多方面信息，所有这些都围绕着医学代码表现。我们还使用医学本体中的父级信息来规范特定模态的编码器，以学习EHR数据的层次结构。对MIMIC-III数据进行的一系列实验证明了我们方法的有效性。

    Predicting next visit diagnosis using Electronic Health Records (EHR) is an essential task in healthcare, critical for devising proactive future plans for both healthcare providers and patients. Nonetheless, many preceding studies have not sufficiently addressed the heterogeneous and hierarchical characteristics inherent in EHR data, inevitably leading to sub-optimal performance. To this end, we propose NECHO, a novel medical code-centric multimodal contrastive EHR learning framework with hierarchical regularisation. First, we integrate multifaceted information encompassing medical codes, demographics, and clinical notes using a tailored network design and a pair of bimodal contrastive losses, all of which pivot around a medical code representation. We also regularise modality-specific encoders using a parental level information in medical ontology to learn hierarchical structure of EHR data. A series of experiments on MIMIC-III data demonstrates effectiveness of our approach.
    
[^63]: S$^3$M-Net:用于自动驾驶的语义分割和立体匹配的联合学习

    S$^3$M-Net: Joint Learning of Semantic Segmentation and Stereo Matching for Autonomous Driving. (arXiv:2401.11414v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.11414](http://arxiv.org/abs/2401.11414)

    S$^3$M-Net是一种用于自动驾驶的联合学习框架，同时进行语义分割和立体匹配。通过共享特征和特征融合适应模块的使用，S$^3$M-Net能够提高整体场景理解能力。

    

    语义分割和立体匹配是自动驾驶的三维环境感知系统的两个重要组成部分。然而，传统方法通常独立处理这两个问题，为每个任务使用单独的模型。这种方法在现实世界的场景中存在实际限制，特别是在计算资源稀缺或实时性能至关重要的情况下。因此，在本文中，我们介绍了S$^3$M-Net，这是一种新颖的联合学习框架，用于同时进行语义分割和立体匹配。具体而言，S$^3$M-Net共享来自RGB图像的特征，从而提高整体场景理解能力。这个特征共享过程通过一个特征融合适应（FFA）模块实现，有效地将共享特征转换为语义空间，并随后将它们与编码的视差特征融合。整个联合学习框架是通过联合训练实现的。

    Semantic segmentation and stereo matching are two essential components of 3D environmental perception systems for autonomous driving. Nevertheless, conventional approaches often address these two problems independently, employing separate models for each task. This approach poses practical limitations in real-world scenarios, particularly when computational resources are scarce or real-time performance is imperative. Hence, in this article, we introduce S$^3$M-Net, a novel joint learning framework developed to perform semantic segmentation and stereo matching simultaneously. Specifically, S$^3$M-Net shares the features extracted from RGB images between both tasks, resulting in an improved overall scene understanding capability. This feature sharing process is realized using a feature fusion adaption (FFA) module, which effectively transforms the shared features into semantic space and subsequently fuses them with the encoded disparity features. The entire joint learning framework is tr
    
[^64]: 多智能体强化学习中的政策距离测量

    Measuring Policy Distance for Multi-Agent Reinforcement Learning. (arXiv:2401.11257v1 [cs.MA])

    [http://arxiv.org/abs/2401.11257](http://arxiv.org/abs/2401.11257)

    本文提出了一种用于测量多智能体强化学习中政策差异的通用工具，即多智能体政策距离（MAPD）。通过学习智能体的决策条件表示，MAPD可以计算任意一对智能体之间的政策距离，并且可以扩展到定制化版本以量化智能体政策在特定方面的差异。这个工具不仅有助于评估多智能体系统中多样性的演变，还为基于多样性的MARL算法的设计提供指导。

    

    多样性在改善多智能体强化学习（MARL）的性能中起着关键作用。目前，已经开发出许多基于多样性的方法，以克服传统MARL中过多参数共享的缺点。然而，目前缺乏一种通用的度量标准来量化智能体之间的政策差异。这样的度量标准不仅可以方便地评估多智能体系统中多样性的演变，还可以为基于多样性的MARL算法的设计提供指导。本文中，我们提出了多智能体政策距离（MAPD），这是一个用于测量MARL中政策差异的通用工具。通过学习智能体决策的条件表示，MAPD可以计算任意一对智能体之间的政策距离。此外，我们还将MAPD扩展为可定制的版本，可以量化在指定方面的智能体政策之间的差异。基于MAPD的在线部署，我们设计了一个多智能体动态参数共享（MAD）模型。

    Diversity plays a crucial role in improving the performance of multi-agent reinforcement learning (MARL). Currently, many diversity-based methods have been developed to overcome the drawbacks of excessive parameter sharing in traditional MARL. However, there remains a lack of a general metric to quantify policy differences among agents. Such a metric would not only facilitate the evaluation of the diversity evolution in multi-agent systems, but also provide guidance for the design of diversity-based MARL algorithms. In this paper, we propose the multi-agent policy distance (MAPD), a general tool for measuring policy differences in MARL. By learning the conditional representations of agents' decisions, MAPD can computes the policy distance between any pair of agents. Furthermore, we extend MAPD to a customizable version, which can quantify differences among agent policies on specified aspects. Based on the online deployment of MAPD, we design a multi-agent dynamic parameter sharing (MAD
    
[^65]: 在嵌入空间中推广演讲者验证以对抗欺骗意识

    Generalizing Speaker Verification for Spoof Awareness in the Embedding Space. (arXiv:2401.11156v1 [cs.CR])

    [http://arxiv.org/abs/2401.11156](http://arxiv.org/abs/2401.11156)

    本文提出了一种在嵌入空间中推广的演讲者验证系统，可以同时处理冒充者和欺骗攻击，提供更强的保护和更经济的计算。

    

    如今众所周知，自动演讲者验证（ASV）系统可以被各种类型的对手欺骗。对抗ASV系统的常见方法是开发一个独立的欺骗对策（CM）模块，将演讲输入分类为真实或伪造的话语。然而，这种设计在认证阶段需要额外的计算和利用。另一种策略是设计一个统一的ASV系统，可以处理零努力的冒充者（非目标）和欺骗攻击。这种具有欺骗意识的ASV系统有可能提供更强的保护和更经济的计算。为此，我们提出了在嵌入空间中推广独立ASV（G-SASV）来对抗欺骗攻击的方法，在测试（认证）阶段不涉及独立的CM模块，并利用来自CM的有限训练数据增强简单的后端。

    It is now well-known that automatic speaker verification (ASV) systems can be spoofed using various types of adversaries. The usual approach to counteract ASV systems against such attacks is to develop a separate spoofing countermeasure (CM) module to classify speech input either as a bonafide, or a spoofed utterance. Nevertheless, such a design requires additional computation and utilization efforts at the authentication stage. An alternative strategy involves a single monolithic ASV system designed to handle both zero-effort imposter (non-targets) and spoofing attacks. Such spoof-aware ASV systems have the potential to provide stronger protections and more economic computations. To this end, we propose to generalize the standalone ASV (G-SASV) against spoofing attacks, where we leverage limited training data from CM to enhance a simple backend in the embedding space, without the involvement of a separate CM module during the test (authentication) phase. We propose a novel yet simple 
    
[^66]: SPAND: 使用网络动态的睡眠预测架构

    SPAND: Sleep Prediction Architecture using Network Dynamics. (arXiv:2401.11113v1 [cs.LG])

    [http://arxiv.org/abs/2401.11113](http://arxiv.org/abs/2401.11113)

    SPAND是一个利用网络动态的睡眠预测架构，可以通过图网络和移动设备数据来预测下一天的睡眠持续时间标签。

    

    睡眠行为对健康有重大影响，对身心健康的指示也非常重要。利用普遍存在的传感器监测和预测睡眠行为，可以帮助管理睡眠并追踪相关健康状况。虽然睡眠行为取决于个体的生理状况，但也受到数字媒体使用、社交网络传染以及周围天气等外部因素的影响。在本研究中，我们提出了SPAND（Sleep Prediction Architecture using Network Dynamics），这是一个利用图网络中的社交传染来预测睡眠行为的系统，并将其与从普遍存在的移动设备和可穿戴设备中提取的生理和手机数据集成，以预测下一天的睡眠持续时间标签。我们的架构通过设计一种注意机制，克服了包含与睡眠行为无关的连接的大规模图形的局限性。广泛的实验评估突显出该系统的性能。

    Sleep behavior significantly impacts health and acts as an indicator of physical and mental well-being. Monitoring and predicting sleep behavior with ubiquitous sensors may therefore assist in both sleep management and tracking of related health conditions. While sleep behavior depends on, and is reflected in the physiology of a person, it is also impacted by external factors such as digital media usage, social network contagion, and the surrounding weather. In this work, we propose SPAND (Sleep Prediction Architecture using Network Dynamics), a system that exploits social contagion in sleep behavior through graph networks and integrates it with physiological and phone data extracted from ubiquitous mobile and wearable devices for predicting next-day sleep labels about sleep duration. Our architecture overcomes the limitations of large-scale graphs containing connections irrelevant to sleep behavior by devising an attention mechanism. The extensive experimental evaluation highlights th
    
[^67]: 使用大型多模型的弱监督高斯对比基础模型来处理视频问答问题

    Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v1 [cs.CV])

    [http://arxiv.org/abs/2401.10711](http://arxiv.org/abs/2401.10711)

    本论文提出了一种使用大型多模型的弱监督高斯对比基础模型来处理视频问答问题的方法。通过将问题和答案对作为事件描述，找到多个关键帧作为目标时刻，并利用这些时刻作为伪标签来强制LMMs进行推理。所提出的方法使用轻量级的基于高斯的对比基础模块（GCG）来学习时效结构。

    

    视频问答（VideoQA）旨在基于观察到的视频信息回答自然语言问题。尽管大型多模型（LMMs）在图像语言理解和推理方面取得了近期的成功，但它们在处理视频问答方面还不足够，仅仅是将均匀采样的帧作为视觉输入，忽略了与问题相关的视觉线索。此外，现有的视频问答数据集中没有针对问题关键时间戳的人工注释。基于此，我们提出了一种新的弱监督框架，强制LMMs使用问题关键时刻作为视觉输入推理出答案。具体来说，我们将问题和答案对合并为事件描述，以找到多个关键帧作为目标时刻，这些时刻将作为伪标签。通过将这些伪标签作为额外的弱监督，我们设计了一个轻量级的基于高斯的对比基础模块（GCG）。GCG学习多个高斯函数来描述时效结构。

    Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos. Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues. Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets. In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs. Specifically, we fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments, which will be pseudo-labels. With these pseudo-labels as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG learns multiple Gaussian functions to characterize the temporal structure o
    
[^68]: BioDiffusion：用于生物医学信号合成的多功能扩散模型

    BioDiffusion: A Versatile Diffusion Model for Biomedical Signal Synthesis. (arXiv:2401.10282v1 [eess.SP])

    [http://arxiv.org/abs/2401.10282](http://arxiv.org/abs/2401.10282)

    BioDiffusion是一种用于生物医学信号合成的多功能扩散模型，能够产生高保真度、非稳态的多变量信号。通过利用这些合成的信号，可以有效解决生物医学信号机器学习任务中的数据不足、数据不平衡和标签复杂性等问题，提高准确性。

    

    生物医学信号的机器学习任务通常面临有限的数据可用性、不平衡的数据集、标签复杂性和测量噪声的干扰等问题。这些挑战经常阻碍机器学习算法的最佳训练。为了解决这些问题，我们引入了BioDiffusion，这是一种针对合成多变量生物医学信号进行优化的基于扩散的概率模型。BioDiffusion在产生高保真度、非稳态的多变量信号方面表现出色，可用于无条件、标签条件和信号条件生成等多个任务。利用这些合成的信号为上述挑战提供了一个显著的解决方案。我们的研究包括对合成数据质量进行的定性和定量评估，强调其在与生物医学信号相关的机器学习任务中提高准确性的能力。此外，与当前主流的时间系信息生成模型相比，BioDiffusion在质量和效率方面表现出更好的性能。

    Machine learning tasks involving biomedical signals frequently grapple with issues such as limited data availability, imbalanced datasets, labeling complexities, and the interference of measurement noise. These challenges often hinder the optimal training of machine learning algorithms. Addressing these concerns, we introduce BioDiffusion, a diffusion-based probabilistic model optimized for the synthesis of multivariate biomedical signals. BioDiffusion demonstrates excellence in producing high-fidelity, non-stationary, multivariate signals for a range of tasks including unconditional, label-conditional, and signal-conditional generation. Leveraging these synthesized signals offers a notable solution to the aforementioned challenges. Our research encompasses both qualitative and quantitative assessments of the synthesized data quality, underscoring its capacity to bolster accuracy in machine learning tasks tied to biomedical signals. Furthermore, when juxtaposed with current leading tim
    
[^69]: 通过对比性和局部稀疏扰动解释时间序列

    Explaining Time Series via Contrastive and Locally Sparse Perturbations. (arXiv:2401.08552v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.08552](http://arxiv.org/abs/2401.08552)

    这篇论文提出了一个局部稀疏模型ContraLSP，通过引入对立样本和对比学习来解释时间序列。实证研究表明，ContraLSP在解释时间序列数据的质量上取得了实质性的改进。

    

    解释多元时间序列是一个复合挑战，因为它需要识别时间序列中的重要位置并匹配复杂的时间模式。虽然之前基于显著性的方法解决了这些挑战，但它们的扰动可能无法减轻分布偏移问题，尤其是在异质样本中。我们提出了ContraLSP，这是一个局部稀疏模型，引入对立样本来构建无信息的扰动，但使用对比学习保持分布。此外，我们还结合了样本特定的稀疏门，生成更多二进制偏斜和平滑的遮罩，可以轻松综合时间趋势并简洁地选择显著特征。对合成数据集和真实世界数据集的实证研究表明，ContraLSP优于现有模型，在解释时间序列数据的质量上取得了实质性的改进。源代码可在\url{https://github.com/zichuan-liu/ContraL}找到。

    Explaining multivariate time series is a compound challenge, as it requires identifying important locations in the time series and matching complex temporal patterns. Although previous saliency-based methods addressed the challenges, their perturbation may not alleviate the distribution shift issue, which is inevitable especially in heterogeneous samples. We present ContraLSP, a locally sparse model that introduces counterfactual samples to build uninformative perturbations but keeps distribution using contrastive learning. Furthermore, we incorporate sample-specific sparse gates to generate more binary-skewed and smooth masks, which easily integrate temporal trends and select the salient features parsimoniously. Empirical studies on both synthetic and real-world datasets show that ContraLSP outperforms state-of-the-art models, demonstrating a substantial improvement in explanation quality for time series data. The source code is available at \url{https://github.com/zichuan-liu/ContraL
    
[^70]: 一个用于原型开发通用人工智能的普适知识模型和认知架构

    A Universal Knowledge Model and Cognitive Architecture for Prototyping AGI. (arXiv:2401.06256v1 [cs.AI])

    [http://arxiv.org/abs/2401.06256](http://arxiv.org/abs/2401.06256)

    本文提出了一个普适的知识模型和认知架构，用于原型开发通用人工智能（AGI）。该架构包括42种认知架构和一组功能模块，用于接近AGI能力的智能系统。此外，本文还提出了一种通用的知识表示方法，可以将各种不同形式的知识表示整合到一个知识库中。

    

    本文确定了42种用于创建通用人工智能（AGI）的认知架构，并提出了一组相互关联的功能模块，这些模块是接近AGI能力的智能系统所应具备的。由于现有的架构中没有找到所需的功能模块集合，本文提出了一种新的认知架构，用于接近AGI能力的智能系统。作为架构框架中的关键解决方案之一，本文提出了一种通用的知识表示方法，可以将各种非形式化、部分和完全形式化的知识表示方法结合在一个知识库中，如自然语言文本、图像、音频和视频记录、图形、算法、数据库、神经网络、知识图、本体、框架、实质-属性-关系模型、推理系统、谓词演算模型、概念模型等。为了组合和结构化各个片段

    The article identified 42 cognitive architectures for creating general artificial intelligence (AGI) and proposed a set of interrelated functional blocks that an agent approaching AGI in its capabilities should possess. Since the required set of blocks is not found in any of the existing architectures, the article proposes a new cognitive architecture for intelligent systems approaching AGI in their capabilities. As one of the key solutions within the framework of the architecture, a universal method of knowledge representation is proposed, which allows combining various non-formalized, partially and fully formalized methods of knowledge representation in a single knowledge base, such as texts in natural languages, images, audio and video recordings, graphs, algorithms, databases, neural networks, knowledge graphs, ontologies, frames, essence-property-relation models, production systems, predicate calculus models, conceptual models, and others. To combine and structure various fragment
    
[^71]: 重新发现Ranganathan：通过知识图谱的光谱视角了解他的生平

    Rediscovering Ranganathan: A Prismatic View of His Life through the Knowledge Graph Spectrum. (arXiv:2401.03343v1 [cs.DL])

    [http://arxiv.org/abs/2401.03343](http://arxiv.org/abs/2401.03343)

    本研究通过一个新颖的生平知识图谱（KG）提供了库图与信息科学领域先驱人物S.R. Ranganathan的360度视角，这种专门的表示在范围和覆盖范围上无可比拟，并呼吁整个社区共同努力。

    

    本研究提出了一个关于库图与信息科学领域的先驱人物S.R. Ranganathan教授的新颖生平知识图谱（KG）。发现关于Ranganathan的相关事实存在于各种资源中，以碎片化和零散的方式提供信息。通过这个专门的KG，我们希望为他的生平和成就提供一个360度的视角。据我们所知，这种专门的表示在其范围和覆盖范围上是无可比拟的：使用最先进的技术供任何人公开访问、使用/再使用和贡献。受Ranganathan的理论和思想的启发，KG使用“基于特征的方法论”在两个层面上进行了发展：在关键的生平方面的标识和本体模型的开发。最后，通过这项研究，我们呼吁整个社区共同努力。

    The present study puts forward a novel biographical knowledge graph (KG) on Prof. S. R. Ranganathan, one of the pioneering figures in the Library and Information Science (LIS) domain. It has been found that most of the relevant facts about Ranganathan exist in a variety of resources (e.g., books, essays, journal articles, websites, blogs, etc.), offering information in a fragmented and piecemeal way. With this dedicated KG (henceforth known as RKG), we hope to furnish a 360-degree view of his life and achievements. To the best of our knowledge, such a dedicated representation is unparalleled in its scope and coverage: using state-of-the-art technology for anyone to openly access, use/re-use, and contribute. Inspired by Ranganathan's theories and ideas, the KG was developed using a "facet-based methodology" at two levels: in the identification of the vital biographical aspects and the development of the ontological model. Finally, with this study, we call for a community-driven effort t
    
[^72]: NODEC: 用于未知动态系统最优控制的神经ODE

    NODEC: Neural ODE For Optimal Control of Unknown Dynamical Systems. (arXiv:2401.01836v1 [cs.AI])

    [http://arxiv.org/abs/2401.01836](http://arxiv.org/abs/2401.01836)

    NODEC是一种用神经ODE模型将动力学建模与控制器训练相结合的新框架，用于控制未知动态系统。

    

    控制复杂动态系统通常涉及在变化计算框架下最小化具有已知动力学的某些控制目标。对于具有未知动力学的系统，需要额外进行动力学建模。然而，动力学建模的任何不准确都会导致结果控制函数的次优性。另一种用于控制未知动态系统的方法 - 强化学习，将动力学建模融入控制器训练中，通过与环境的广泛交互来近似值函数或策略梯度，但它的数据效率低。为了解决这些问题，我们引入了NODEC，这是一种用神经ODE模型将动力学建模与控制器训练相结合的新框架。通过两个耦合神经网络之间的有趣相互作用，NODEC学习了系统动力学以及指导未知动态系统的最优控制。

    Controlling complex dynamical systems is generally associated with minimizing certain control objectives with known dynamics under the variational calculus framework. For systems with unknown dynamics, an additional step of dynamics modeling is required. However, any inaccuracy in dynamics modeling will lead to sub-optimality in the resulting control function. Another set of approaches for controlling unknown dynamical systems - reinforcement learning, folds the dynamics modeling into controller training via value function approximation or policy gradient through extensively interacting with the environment, but it suffers from low data efficiency. To address these, we introduce NODEC, a novel framework for controlling unknown dynamical systems, which combines dynamics modelling and controller training using a coupled neural ODE model. Through an intriguing interplay between the two coupled neural networks, NODEC learns system dynamics as well as optimal controls that guides the unknow
    
[^73]: 一个量子启发的用于几何建模的神经网络

    A quatum inspired neural network for geometric modeling. (arXiv:2401.01801v1 [cs.LG])

    [http://arxiv.org/abs/2401.01801](http://arxiv.org/abs/2401.01801)

    这个论文介绍了一种创新的矩阵乘积态(MPS)的消息传递策略，通过这种策略可以更好地捕捉几何图中的复杂关系。

    

    通过将物理系统构想为3D多体点云，几何图神经网络(GNN)，如SE(3)/E(3)等效GNN，展示了良好的性能。特别是，它们高效的消息传递机制使它们能够熟练地对分子和晶体材料进行建模。然而，当前的几何GNN只提供了多体系统的平均场近似，封装在两体消息传递中，因此在捕捉这些几何图中的复杂关系方面有所欠缺。为了解决这个局限性，计算物理学中广泛使用的高阶张量来处理多体系统的张量网络被引入。然而，将这些张量化网络整合到GNN的消息传递框架中面临着可扩展性和对称性保持（如置换和旋转）的挑战。作为回应，我们引入了一种创新的等变矩阵乘积态(MPS)的消息传递策略，通过实现一个

    By conceiving physical systems as 3D many-body point clouds, geometric graph neural networks (GNNs), such as SE(3)/E(3) equivalent GNNs, have showcased promising performance. In particular, their effective message-passing mechanics make them adept at modeling molecules and crystalline materials. However, current geometric GNNs only offer a mean-field approximation of the many-body system, encapsulated within two-body message passing, thus falling short in capturing intricate relationships within these geometric graphs. To address this limitation, tensor networks, widely employed by computational physics to handle manybody systems using high-order tensors, have been introduced. Nevertheless, integrating these tensorized networks into the message-passing framework of GNNs faces scalability and symmetry conservation (e.g., permutation and rotation) challenges. In response, we introduce an innovative equivariant Matrix Product State (MPS)-based message-passing strategy, through achieving a
    
[^74]: 联邦学习中受效率限制的效用-隐私双目标优化的理论分析

    A Theoretical Analysis of Efficiency Constrained Utility-Privacy Bi-Objective Optimization in Federated Learning. (arXiv:2312.16554v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16554](http://arxiv.org/abs/2312.16554)

    本文从理论上分析了联邦学习中受效率限制的效用-隐私双目标优化。先前的研究主要关注效用-隐私的权衡，忽视了训练效率和其他影响因素。该研究对差分隐私联邦学习中的关键问题进行了系统分析。

    

    联邦学习（FL）使多个客户端在不共享个体数据的情况下协同学习共享模型。FL中的效用、隐私和训练效率问题已引起重要的研究关注。差分隐私已成为FL中一种主流技术，保护个体用户数据的隐私同时影响效用和训练效率。在差分隐私联邦学习（DPFL）中，先前的研究主要关注效用-隐私的权衡，而忽视了及时完成所必需的训练效率。此外，差分隐私通过在每轮通信中对选定的客户端引入受控的随机性（噪声）来实现隐私保护。先前的工作主要研究了噪声水平（$\sigma$）和通信轮数（$T$）对隐私-效用动态的影响，但忽视了其他影响因素，如样本比例（$q$，即选定客户端的比例）。

    Federated learning (FL) enables multiple clients to collaboratively learn a shared model without sharing their individual data. Concerns about utility, privacy, and training efficiency in FL have garnered significant research attention. Differential privacy has emerged as a prevalent technique in FL, safeguarding the privacy of individual user data while impacting utility and training efficiency. Within Differential Privacy Federated Learning (DPFL), previous studies have primarily focused on the utility-privacy trade-off, neglecting training efficiency, which is crucial for timely completion. Moreover, differential privacy achieves privacy by introducing controlled randomness (noise) on selected clients in each communication round. Previous work has mainly examined the impact of noise level ($\sigma$) and communication rounds ($T$) on the privacy-utility dynamic, overlooking other influential factors like the sample ratio ($q$, the proportion of selected clients). This paper systemati
    
[^75]: 时间变换器：融合本地和全局特征以实现更好的时间序列生成

    Time-Transformer: Integrating Local and Global Features for Better Time Series Generation. (arXiv:2312.11714v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11714](http://arxiv.org/abs/2312.11714)

    本文提出了一种新的时间序列生成模型，通过时间变换器同时学习本地和全局特征，实现了对时间序列数据的更好生成能力。

    

    生成时间序列数据是解决数据不足问题的一种有前景的方法。然而，由于时间序列数据的复杂时间特性，包括本地相关性和全局依赖性，使其成为具有挑战性的任务。大多数现有的生成模型未能有效学习时间序列数据的本地和全局特性。为了解决这个问题，我们提出了一种新颖的时间序列生成模型，命名为'时间变换器AAE'，它由一个对抗性自动编码器（AAE）和一个名为'时间变换器'的新设计架构组成。时间变换器首先通过层次并行设计同时学习本地和全局特征，结合了时间卷积网络和Transformer的能力，分别提取本地特征和全局依赖性。其次，提出了一个双向交叉注意力来在两个分支之间提供互补的引导，并实现本地特征和全局特征的合适融合。

    Generating time series data is a promising approach to address data deficiency problems. However, it is also challenging due to the complex temporal properties of time series data, including local correlations as well as global dependencies. Most existing generative models have failed to effectively learn both the local and global properties of time series data. To address this open problem, we propose a novel time series generative model named 'Time-Transformer AAE', which consists of an adversarial autoencoder (AAE) and a newly designed architecture named 'Time-Transformer' within the decoder. The Time-Transformer first simultaneously learns local and global features in a layer-wise parallel design, combining the abilities of Temporal Convolutional Networks and Transformer in extracting local features and global dependencies respectively. Second, a bidirectional cross attention is proposed to provide complementary guidance across the two branches and achieve proper fusion between loc
    
[^76]: 人类反馈的迭代偏好学习：在KL约束下将理论与实践联系起来的RLHF

    Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint. (arXiv:2312.11456v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11456](http://arxiv.org/abs/2312.11456)

    该论文研究了在KL约束下的反馈强化学习的理论框架，并提出了有效的算法和实践。实证评估表明，该框架在大型语言模型的对齐实验中表现出良好的效果。

    

    本文研究了生成模型与强化学习从人类反馈中的对齐过程的理论框架。我们考虑了一个标准的数学表达式，即反向KL正则化的上下文多臂赌博机用于RLHF。尽管它被广泛应用于实际应用，但对这个公式的严格理论分析仍然很开放。我们研究了它在离线、在线和混合三种不同场景下的行为，并提出了具有有限样本理论保证的高效算法。朝着实际应用的方向，我们的框架通过对信息理论策略改进预言的稳健近似，自然地产生了几种新颖的RLHF算法。这包括在线场景中的迭代版本的直接偏好优化(DPO)算法，以及离线情景下的多步拒绝抽样策略。我们对大型语言模型的真实对齐实验进行了实证评估。

    This paper studies the theoretical framework of the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees.  Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate t
    
[^77]: 改进的匿名多智能体路径规划算法

    Improved Anonymous Multi-Agent Path Finding Algorithm. (arXiv:2312.10572v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.10572](http://arxiv.org/abs/2312.10572)

    本文提出了一种改进的匿名多智能体路径规划算法，通过利用批量探索搜索空间的思想，在寻找解决方案时将搜索状态进行压缩、存储和展开，从而实现高度减少。

    

    本文考虑到一种匿名多智能体路径规划（AMAPF）问题，其中智能体的集合被限制在一个图中，给定了一组目标顶点，并且每个顶点必须被某个智能体到达。该问题是找到目标与智能体的分配以及无碰撞路径，并且我们希望找到最优makespan的解。解决这个问题的一种成熟方法是将其转化为一种特殊类型的图搜索问题，即在输入图产生的辅助图上找到最大流问题。前者的大小可能非常大，搜索可能成为一个瓶颈。为此，我们提出了一种特定的搜索算法，借助了通过同时考虑一批搜索状态而不是单独考虑它们来探索搜索空间的想法。也就是说，我们将搜索状态的批量压缩、存储和展开成单个状态，从而实现高度减少

    We consider an Anonymous Multi-Agent Path-Finding (AMAPF) problem where the set of agents is confined to a graph, a set of goal vertices is given and each of these vertices has to be reached by some agent. The problem is to find an assignment of the goals to the agents as well as the collision-free paths, and we are interested in finding the solution with the optimal makespan. A well-established approach to solve this problem is to reduce it to a special type of a graph search problem, i.e. to the problem of finding a maximum flow on an auxiliary graph induced by the input one. The size of the former graph may be very large and the search on it may become a bottleneck. To this end, we suggest a specific search algorithm that leverages the idea of exploring the search space not through considering separate search states but rather bulks of them simultaneously. That is, we implicitly compress, store and expand bulks of the search states as single states, which results in high reduction i
    
[^78]: 图上的通用神经扩散框架

    A Generalized Neural Diffusion Framework on Graphs. (arXiv:2312.08616v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2312.08616](http://arxiv.org/abs/2312.08616)

    本文提出了一个通用的扩散方程框架，通过带有保真度项的方程，正式建立了GNN与扩散过程之间的关系。通过实验证明，该框架能够描述高阶邻居的标签相似性。

    

    最近的研究揭示了GNN和扩散过程之间的联系，这激发了许多基于扩散的GNN的提出。然而，由于这两种机制密切相关，一个基本的问题自然地产生：是否存在一个可以正式统一这些GNN的通用扩散框架？这个问题的答案不仅可以加深我们对GNN学习过程的理解，而且还可能打开一个设计广泛新的GNN类别的新大门。在本文中，我们提出了一个带有保真度项的通用扩散方程框架，它正式建立了扩散过程与更多GNN之间的关系。同时，通过这个框架，我们确定了图扩散网络的一个特征，即当前神经扩散过程只对应于一阶扩散方程。然而，通过实验证明，高阶邻居的标签实际上表现出单一性特征，这引发了相似性。

    Recent studies reveal the connection between GNNs and the diffusion process, which motivates many diffusion-based GNNs to be proposed. However, since these two mechanisms are closely related, one fundamental question naturally arises: Is there a general diffusion framework that can formally unify these GNNs? The answer to this question can not only deepen our understanding of the learning process of GNNs, but also may open a new door to design a broad new class of GNNs. In this paper, we propose a general diffusion equation framework with the fidelity term, which formally establishes the relationship between the diffusion process with more GNNs. Meanwhile, with this framework, we identify one characteristic of graph diffusion networks, i.e., the current neural diffusion process only corresponds to the first-order diffusion equation. However, by an experimental investigation, we show that the labels of high-order neighbors actually exhibit monophily property, which induces the similarit
    
[^79]: 进化沉积池用于元增强学习

    Evolving Reservoirs for Meta Reinforcement Learning. (arXiv:2312.06695v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.06695](http://arxiv.org/abs/2312.06695)

    本论文提出了一种进化沉积池的计算模型，用于研究动物适应环境的机制。这种模型基于元增强学习框架，通过演化和发展之间的相互作用，利用进化沉积池来加速和引导强化学习过程。

    

    动物在其一生中经常展示出对环境的适应能力，部分原因是由于形态和神经结构的演化。这些结构捕捉到了代际之间共享的环境特征，以加速和引导一生中的学习过程。在这项工作中，我们提出了一个计算模型来研究实现这一过程的机制。我们采用基于元增强学习的计算框架作为演化和发展之间相互作用的模型。在演化尺度上，我们演化沉积池，这是一族循环神经网络，与常规网络不同的是，我们优化的不是突触权重，而是控制结果网络架构的宏观级别属性的超参数。在发展尺度上，我们使用这些进化沉积池来促进通过强化学习来学习行为策略。在强化学习代理中，沉积池编码环境的信息以优化学习过程。

    Animals often demonstrate a remarkable ability to adapt to their environments during their lifetime. They do so partly due to the evolution of morphological and neural structures. These structures capture features of environments shared between generations to bias and speed up lifetime learning. In this work, we propose a computational model for studying a mechanism that can enable such a process. We adopt a computational framework based on meta reinforcement learning as a model of the interplay between evolution and development. At the evolutionary scale, we evolve reservoirs, a family of recurrent neural networks that differ from conventional networks in that one optimizes not the synaptic weights, but hyperparameters controlling macro-level properties of the resulting network architecture. At the developmental scale, we employ these evolved reservoirs to facilitate the learning of a behavioral policy through Reinforcement Learning (RL). Within an RL agent, a reservoir encodes the en
    
[^80]: AesFA:一种美学特征感知的任意神经风格转换方法

    AesFA: An Aesthetic Feature-Aware Arbitrary Neural Style Transfer. (arXiv:2312.05928v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.05928](http://arxiv.org/abs/2312.05928)

    AesFA是一种轻量级但有效的神经风格转换方法，通过频率分解图像，以更好地解开美学风格，排除了预训练模型。引入了对比损失以提高风格化质量。实验证明，AesFA在stylization quality方面优于其他方法，并实现了快速转换。

    

    神经风格转换（NST）在近年来取得了显著的发展。然而，尽管其快速进展和改进，现有的NST方法要么在有效转移风格时很难保留美学信息，要么在特征解缠和计算效率方面由于使用预训练模型而存在高计算成本和低效率。本文提出了一种轻量级但有效的模型，AesFA -- 美学特征感知的NST。其主要思想是通过频率对图像进行分解，以更好地从参考图像中解开美学风格，同时以端到端的方式训练整个模型，完全取消了推断时的预训练模型。为了提高网络提取更加独特的表示和进一步增强风格化质量的能力，本文引入了一种新的美学特征：对比损失。广泛的实验和消融研究表明，这种方法不仅在风格化质量方面优于最新的NST方法，而且实现了快速的转换。

    Neural style transfer (NST) has evolved significantly in recent years. Yet, despite its rapid progress and advancement, existing NST methods either struggle to transfer aesthetic information from a style effectively or suffer from high computational costs and inefficiencies in feature disentanglement due to using pre-trained models. This work proposes a lightweight but effective model, AesFA -- Aesthetic Feature-Aware NST. The primary idea is to decompose the image via its frequencies to better disentangle aesthetic styles from the reference image while training the entire model in an end-to-end manner to exclude pre-trained models at inference completely. To improve the network's ability to extract more distinct representations and further enhance the stylization quality, this work introduces a new aesthetic feature: contrastive loss. Extensive experiments and ablations show the approach not only outperforms recent NST methods in terms of stylization quality, but it also achieves fast
    
[^81]: 多分布学习的样本复杂度

    The sample complexity of multi-distribution learning. (arXiv:2312.04027v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.04027](http://arxiv.org/abs/2312.04027)

    本文解决了多分布学习的样本复杂度问题，并给出了匹配下界的样本复杂度算法。

    

    多分布学习将经典的PAC学习推广到处理来自多个分布的数据。给定一组$k$个数据分布和一个VC维度为$d$的假设类，目标是学习一个假设，使得在$k$个分布上的最大总体损失最小，误差不超过$\epsilon$。本文通过给出一个样本复杂度算法$\widetilde{O}((d+k)\epsilon^{-2}) \cdot (k/\epsilon)^{o(1)}$来解决多分布学习的样本复杂度问题。这个结果与下界相匹配，解决了Awasthi、Haghtalab和Zhao在COLT 2023中提出的开放问题 [AHZ23]。

    Multi-distribution learning generalizes the classic PAC learning to handle data coming from multiple distributions. Given a set of $k$ data distributions and a hypothesis class of VC dimension $d$, the goal is to learn a hypothesis that minimizes the maximum population loss over $k$ distributions, up to $\epsilon$ additive error. In this paper, we settle the sample complexity of multi-distribution learning by giving an algorithm of sample complexity $\widetilde{O}((d+k)\epsilon^{-2}) \cdot (k/\epsilon)^{o(1)}$. This matches the lower bound up to sub-polynomial factor and resolves the COLT 2023 open problem of Awasthi, Haghtalab and Zhao [AHZ23].
    
[^82]: 具有逻辑约束的自回归模型的伪语义损失

    A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints. (arXiv:2312.03905v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.03905](http://arxiv.org/abs/2312.03905)

    本论文提出了一种针对具有逻辑约束的自回归模型的伪语义损失方法，通过在模型输出的局部近似上优化约束的似然，提高了神经符号学习的效率和适用性。

    

    神经符号化人工智能（neuro-symbolic AI）填补了纯符号和神经学习方法之间的鸿沟。这通常需要在神经网络的输出分布方面最大化对符号约束的似然。这些输出分布通常被假设为完全因子化的。这限制了神经符号学习在更具表现力的自回归分布（例如transformers）中的适用性。在这样的分布下，甚至简单约束的概率似然的计算是#P-hard的。我们提出，不是试图将约束强加在整个输出分布上，而是在其随机的局部近似上这样做。更确切地说，我们在以模型样本为中心的基于伪似然的近似中优化约束的似然。我们的近似是因子化的，可以重用子问题的解决方案，这是高效计算神经符号损失的主要原则。此外，它是一个局部的，高保真度的似然近似。

    Neuro-symbolic AI bridges the gap between purely symbolic and neural approaches to learning. This often requires maximizing the likelihood of a symbolic constraint w.r.t the neural network's output distribution. Such output distributions are typically assumed to be fully-factorized. This limits the applicability of neuro-symbolic learning to the more expressive autoregressive distributions, e.g., transformers. Under such distributions, computing the likelihood of even simple constraints is #P-hard. Instead of attempting to enforce the constraint on the entire output distribution, we propose to do so on a random, local approximation thereof. More precisely, we optimize the likelihood of the constraint under a pseudolikelihood-based approximation centered around a model sample. Our approximation is factorized, allowing the reuse of solutions to sub-problems, a main tenet for efficiently computing neuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of the likelihoo
    
[^83]: MatterGen: 无机材料设计的生成模型

    MatterGen: a generative model for inorganic materials design. (arXiv:2312.03687v2 [cond-mat.mtrl-sci] UPDATED)

    [http://arxiv.org/abs/2312.03687](http://arxiv.org/abs/2312.03687)

    MatterGen是一个生成模型，在无机材料设计中能够生成稳定多样的材料，并通过适配器模块进行微调以满足广泛的性质约束。

    

    在能源存储、催化和碳捕集等领域，设计具有期望性能的功能材料对推动技术进步至关重要。生成模型通过直接生成全新的材料，满足特定性质约束，为材料设计提供了新的范式。尽管近年来取得了一些进展，但当前的生成模型在提出稳定晶体的成功率方面较低，或者只能满足非常有限的性质约束。我们在这里介绍了MatterGen，一个能够在周期表上生成稳定多样的无机材料的模型，并且可以通过微调生成过程，满足广泛的性质约束范围。为了实现这一目标，我们引入了一种基于扩散的生成过程，通过逐步改进原子类型、坐标和周期晶格来产生晶体结构。我们进一步引入了适配器模块，以便通过带标签的数据集对任意给定的性质约束进行微调。

    The design of functional materials with desired properties is essential in driving technological advances in areas like energy storage, catalysis, and carbon capture. Generative models provide a new paradigm for materials design by directly generating entirely novel materials given desired property constraints. Despite recent progress, current generative models have low success rate in proposing stable crystals, or can only satisfy a very limited set of property constraints. Here, we present MatterGen, a model that generates stable, diverse inorganic materials across the periodic table and can further be fine-tuned to steer the generation towards a broad range of property constraints. To enable this, we introduce a new diffusion-based generative process that produces crystalline structures by gradually refining atom types, coordinates, and the periodic lattice. We further introduce adapter modules to enable fine-tuning towards any given property constraints with a labeled dataset. Comp
    
[^84]: 对于不可预知的网络的最优分布式学习的灵活通信

    Flexible Communication for Optimal Distributed Learning over Unpredictable Networks. (arXiv:2312.02493v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2312.02493](http://arxiv.org/abs/2312.02493)

    本文提出了一种灵活的通信策略，根据网络配置自动切换使用Allgather（AG）或Allreduce（AR），以提高分布式学习的并行效率和模型精度。

    

    梯度压缩通过发送更少的值和对应的索引来减轻分布式深度学习中的昂贵通信，通常通过Allgather（AG）来实现。在高压缩比（CR）下进行训练可以实现与DenseSGD相同的高精度，但由于高通信成本（即并行效率），并行扩展性较低。使用较低的CR可以通过降低同步开销来提高并行效率，但同时也降低模型精度（即统计效率）。此外，使用不同模型和CR获得的加速度也会因网络延迟，有效带宽和用于聚合的集合操作而有所不同。在许多情况下，像Allreduce（AR）这样的集体操作与AG交换相同数量的数据的成本较低。本文提出了一种AR兼容的Topk压缩器，在某些网络配置中比AG更优。我们开发了一种灵活的通信策略，根据使用哪个集体操作来在AG和AR之间切换

    Gradient compression alleviates expensive communication in distributed deep learning by sending fewer values and its corresponding indices, typically via Allgather (AG). Training with high compression ratio (CR) achieves high accuracy like DenseSGD, but has lower parallel scaling due to high communication cost (i.e., parallel efficiency). Using lower CRs improves parallel efficiency by lowering synchronization cost, but degrades model accuracy as well (statistical efficiency). Further, speedup attained with different models and CRs also varies with network latency, effective bandwidth and collective op used for aggregation. In many cases, collectives like Allreduce (AR) have lower cost than AG to exchange the same amount of data. In this paper, we propose an AR-compatible Topk compressor that is bandwidth-optimal and thus performs better than AG in certain network configurations. We develop a flexible communication strategy that switches between AG and AR based on which collective is o
    
[^85]: 使用歌词自动确定新曲谱的节拍记号

    Automatic Time Signature Determination for New Scores Using Lyrics for Latent Rhythmic Structure. (arXiv:2311.15480v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.15480](http://arxiv.org/abs/2311.15480)

    本文提出了一种新颖的方法，通过仅使用歌词作为输入，自动生成适合歌词歌曲的节拍记号，并揭示潜在的节奏结构。

    

    最近对于人工智能生成内容(AIGC)的兴趣急剧增加。然而，尚未对音乐组成部分如节拍记号进行足够的研究，以制定新作品的算法确定方法，尤其是歌词歌曲。这可能是因为忽视了音乐细节，而音乐细节对于构建强大的框架至关重要。具体而言，节拍记号为歌曲的几乎所有方面(包括短语和音符)建立了基础的节奏结构。在本文中，我们提出了一种新颖的方法，仅使用歌词作为输入，自动生成适合歌词歌曲的节拍记号，并利用可解释的机器学习模型揭示潜在的节奏结构。具体而言，我们设计了多种与发现歌词模式和创建同时包含歌词、节奏和统计信息的新特征相关的方法。在这种方法中，

    There has recently been a sharp increase in interest in Artificial Intelligence-Generated Content (AIGC). Despite this, musical components such as time signatures have not been studied sufficiently to form an algorithmic determination approach for new compositions, especially lyrical songs. This is likely because of the neglect of musical details, which is critical for constructing a robust framework. Specifically, time signatures establish the fundamental rhythmic structure for almost all aspects of a song, including the phrases and notes. In this paper, we propose a novel approach that only uses lyrics as input to automatically generate a fitting time signature for lyrical songs and uncover the latent rhythmic structure utilizing explainable machine learning models. In particular, we devise multiple methods that are associated with discovering lyrical patterns and creating new features that simultaneously contain lyrical, rhythmic, and statistical information. In this approach, the b
    
[^86]: 通过修剪大型语言模型调查幻觉在抽象摘要中的应用

    Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization. (arXiv:2311.09335v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.09335](http://arxiv.org/abs/2311.09335)

    本文通过广泛的实证研究发现，修剪后的大型语言模型在抽象摘要任务中产生幻觉的情况较原始模型要少，表现更可靠，具有更高的效率和稀疏推理能力。

    

    尽管生成型的大型语言模型在抽象摘要任务中表现出色，但它们面临两个重要挑战：模型庞大和易产生幻觉。幻觉是令人担忧的，因为它们降低了可靠性并引发安全问题。修剪是一种通过去除冗余权重来减小模型大小，实现更高效稀疏推理的技术。修剪后的模型在下游任务性能上与原始模型相当，因此在预算有限的情况下成为理想的替代选择。然而，修剪对语言模型在抽象摘要中产生幻觉的影响尚未被探索。本文通过对五个摘要数据集、两种最先进的修剪方法和五个经调试的语言模型进行了广泛的实证研究。令人惊讶的是，我们发现修剪后的语言模型产生幻觉的情况较原始模型要少。我们的分析表明，修剪后的模型更倾向于依赖指导信息。

    Despite the remarkable performance of generative large language models (LLMs) on abstractive summarization, they face two significant challenges: their considerable size and tendency to hallucinate. Hallucinations are concerning because they erode reliability and raise safety issues. Pruning is a technique that reduces model size by removing redundant weights, enabling more efficient sparse inference. Pruned models yield downstream task performance comparable to the original, making them ideal alternatives when operating on a limited budget. However, the effect that pruning has upon hallucinations in abstractive summarization with LLMs has yet to be explored. In this paper, we provide an extensive empirical study across five summarization datasets, two state-of-the-art pruning methods, and five instruction-tuned LLMs. Surprisingly, we find that hallucinations from pruned LLMs are less prevalent than the original models. Our analysis suggests that pruned models tend to depend more on th
    
[^87]: 在具有概率保证和实践的连续POMDP规划中简化复杂的观测模型

    Simplifying Complex Observation Models in Continuous POMDP Planning with Probabilistic Guarantees and Practice. (arXiv:2311.07745v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.07745](http://arxiv.org/abs/2311.07745)

    本研究在解决具有高维度和连续观测的部分可观测马尔可夫决策过程中，提出了一种基于统计总变差距离的新型概率界限，能够简化观测模型并保证解决方案的质量。

    

    解决具有高维度和连续观测（如相机图像）的部分可观测马尔可夫决策过程(POMDP)对于许多实际机器人和规划问题是必需的。最近的研究建议使用机器学习的概率模型作为观测模型，但它们目前在线部署时计算成本过高。我们探讨了在规划中使用简化观测模型的影响，同时保持对解决方案质量的形式化保证。我们的主要贡献是一种基于简化模型的统计总变差距离的新型概率界限。我们证明，通过推广最近的粒子置信度MDP集中界限的结果，它将理论POMDP值与简化模型下的实际规划值进行了约束。我们的计算可以分为离线和在线部分，并且我们可以得到形式化的保证，而无需

    Solving partially observable Markov decision processes (POMDPs) with high dimensional and continuous observations, such as camera images, is required for many real life robotics and planning problems. Recent researches suggested machine learned probabilistic models as observation models, but their use is currently too computationally expensive for online deployment. We deal with the question of what would be the implication of using simplified observation models for planning, while retaining formal guarantees on the quality of the solution. Our main contribution is a novel probabilistic bound based on a statistical total variation distance of the simplified model. We show that it bounds the theoretical POMDP value w.r.t. original model, from the empirical planned value with the simplified model, by generalizing recent results of particle-belief MDP concentration bounds. Our calculations can be separated into offline and online parts, and we arrive at formal guarantees without having to
    
[^88]: GateLoop: 完全数据控制的线性递归用于序列建模

    GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v1 [cs.LG])

    [http://arxiv.org/abs/2311.01927](http://arxiv.org/abs/2311.01927)

    GateLoop是一种完全数据控制的线性递归序列模型，优于现有模型，可以提供数据控制的相对位置信息给Attention。

    

    线性递归已被证明是一种有效建模长序列的强大工具。在这项工作中，我们表明现有模型未能充分利用其潜力。在这一发现的基础上，我们开发了GateLoop，这是一种基础性的序列模型，通过使用数据控制的状态转换来推广线性递归模型，如S4、S5、LRU和RetNet。利用这一理论进步，GateLoop在自回归语言建模方面在实证上优于现有模型。我们的方法具有低成本的$O(l)$递归模式和高度优化的关联扫描实现的高效$O(l \log_{2} l)$并行模式。此外，我们还推导出了一个$O(l^2)$的代理注意力模式，揭示了对Transformer和最近提出的架构的显著影响。具体而言，我们证明了我们的方法可以被解释为向Attention提供数据控制的相对位置信息。而许多现有模型仅依赖于数据无关的位置信息。

    Linear Recurrence has proven to be a powerful tool for modeling long sequences efficiently. In this work, we show that existing models fail to take full advantage of its potential. Motivated by this finding, we develop GateLoop, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet, by employing data-controlled state transitions. Utilizing this theoretical advance, GateLoop empirically outperforms existing models for auto-regressive language modeling. Our method comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$ parallel mode making use of highly optimized associative scan implementations. Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing remarkable implications for Transformer and recently proposed architectures. Specifically, we prove that our approach can be interpreted as providing data-controlled relative-positional information to Attention. While many existing models solely rely on da
    
[^89]: GOPlan:通过学习模型进行计划的目标条件下的离线强化学习

    GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models. (arXiv:2310.20025v1 [cs.LG])

    [http://arxiv.org/abs/2310.20025](http://arxiv.org/abs/2310.20025)

    GOPlan是一个使用学习模型进行计划的目标条件下的离线强化学习方法，通过预训练先验策略和使用重新分析方法生成虚构轨迹，用以提高性能和处理有限数据预算和未见目标泛化的能力。

    

    离线目标条件下的强化学习（GCRL）为从多样化和多任务的离线数据集中学习通用策略提供了可行的范例。尽管近期取得了显著进展，但主导的离线GCRL方法仍然受限于无模型方法，限制了它们应对有限数据预算和未见目标泛化的能力。在这项工作中，我们提出了一种新的两阶段模型为基础的框架，Goal-conditioned Offline Planning（GOPlan），包括（1）预训练一个能够捕捉多目标数据集中多模态动作分布的先验策略；（2）利用规划的重新分析方法为微调策略生成虚构轨迹。具体而言，先验策略基于一个具有明显模式分离的带优势权重的条件生成对抗网络，以克服超出分布（OOD）动作的缺点。为进一步优化策略，重新分析方法通过规划生成高质量的虚构数据。

    Offline goal-conditioned RL (GCRL) offers a feasible paradigm to learn general-purpose policies from diverse and multi-task offline datasets. Despite notable recent progress, the predominant offline GCRL methods have been restricted to model-free approaches, constraining their capacity to tackle limited data budgets and unseen goal generalization. In this work, we propose a novel two-stage model-based framework, Goal-conditioned Offline Planning (GOPlan), including (1) pretraining a prior policy capable of capturing multi-modal action distribution within the multi-goal dataset; (2) employing the reanalysis method with planning to generate imagined trajectories for funetuning policies. Specifically, the prior policy is based on an advantage-weighted Conditioned Generative Adversarial Networks that exhibits distinct mode separation to overcome the pitfalls of out-of-distribution (OOD) actions. For further policy optimization, the reanalysis method generates high-quality imaginary data by
    
[^90]: 动态最优传输问题的一种新型跳跃正交列表

    A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem. (arXiv:2310.18446v4 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2310.18446](http://arxiv.org/abs/2310.18446)

    本论文提出了一种新型的跳跃正交列表来解决动态最优传输问题，在考虑数据点权重或位置变化时能够有效地更新最优传输方案。

    

    最优传输是过去几十年来引起了优化社区极大关注的一个基本主题。本文研究了一个有趣的离散动态最优传输问题：当数据点的权重或位置发生变化时，我们是否可以高效地更新最优传输方案？这个问题在机器学习中有多个应用。我们经常需要计算两个不同数据集之间的最优传输成本；如果一些数据点发生了变化，我们应该重新计算高复杂度的成本函数，还是通过一些高效的动态数据结构来更新成本？我们注意到先前已提出了几种动态最大流算法，但据我们所知，对于动态最小费用流问题的研究仍然非常有限。我们提出了一种新型的2D跳跃正交列表，结合一些动态树技术。

    Optimal transport is a fundamental topic that has attracted a great amount of attention from the optimization community in the past decades. In this paper, we consider an interesting discrete dynamic optimal transport problem: can we efficiently update the optimal transport plan when the weights or the locations of the data points change? This problem is naturally motivated by several applications in machine learning. For example, we often need to compute the optimal transport cost between two different data sets; if some changes happen to a few data points, should we re-compute the high complexity cost function or update the cost by some efficient dynamic data structure? We are aware that several dynamic maximum flow algorithms have been proposed before, however, the research on dynamic minimum cost flow problem is still quite limited, to the best of our knowledge. We propose a novel 2D Skip Orthogonal List together with some dynamic tree techniques. Although our algorithm is based on
    
[^91]: 一份针对大型语言模型的开源数据污染报告

    An Open Source Data Contamination Report for Large Language Models. (arXiv:2310.17589v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.17589](http://arxiv.org/abs/2310.17589)

    本文介绍了一个针对大型语言模型的开源数据污染报告，其中包括超过15个热门模型对六个常见多项选择问答基准测试的污染分析。实验证明，数据污染会显著降低模型性能，并且随着时间的推移污染程度不断增加。

    

    随着大型语言模型的日益普及，模型评估中的数据污染问题越来越普遍。它允许模型通过记忆而不是展示真正的能力来"作弊"。因此，污染分析已成为可靠的模型评估的重要部分，用于验证结果。然而，现有的污染分析通常由大型语言模型开发者内部进行，往往缺乏透明度和完整性。本文为六个常见的多项选择问答基准测试提供了超过15个热门大型语言模型的详细数据污染报告。我们还介绍了一个开源的流程，使社区能够对自定义数据和模型进行污染分析。我们的实验揭示了基准测试中不同的污染水平，范围从1%到45%，污染程度随时间迅速增加。大型语言模型的性能分析表明，数据污染会显著降低模型的性能。

    Data contamination in model evaluation has become increasingly prevalent with the growing popularity of large language models. It allows models to "cheat" via memorisation instead of displaying true capabilities. Therefore, contamination analysis has become an crucial part of reliable model evaluation to validate results. However, existing contamination analysis is usually conducted internally by large language model developers and often lacks transparency and completeness. This paper presents an extensive data contamination report for over 15 popular large language models across six popular multiple-choice QA benchmarks. We also introduce an open-source pipeline that enables the community to perform contamination analysis on customised data and models. Our experiments reveal varying contamination levels ranging from 1\% to 45\% across benchmarks, with the contamination degree increasing rapidly over time. Performance analysis of large language models indicates that data contamination 
    
[^92]: 在不断变化的多臂赌博机中实现零样本学习

    Towards Zero Shot Learning in Restless Multi-armed Bandits. (arXiv:2310.14526v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.14526](http://arxiv.org/abs/2310.14526)

    通过开发一个基于神经网络的预训练模型，我们实现了在不断变化的多臂赌博机中的零样本学习，该模型具有泛化能力，并且能够在特定实例上进行高效微调，同时适用于多行为设置和离散或连续状态空间。

    

    近来，通过多智能体强化学习的视角研究了一类资源分配问题——不断变化的多臂赌博机（RMABs），该问题在医疗保健、在线广告和反盗猎等领域具有广泛应用。先前的RMAB研究存在一些限制，例如没有充分解决连续状态问题，并且在多个真实世界应用中，当赌博机的入选和退出不断发生时，需要从头开始重新训练，这是一个常见的挑战。为了解决这些限制，我们开发了一个基于神经网络的预训练模型（PreFeRMAB），该模型具有对之前未见过的广泛RMAB问题的零样本能力，并且可以比从头训练更加高效地对特定实例进行微调。此外，我们的模型还适用于一般的多行为设置和离散或连续状态空间。为了实现快速泛化，我们学习了一种新颖的单一策略网络模型，该模型利用特征信息并采用了一种新的训练方式。

    Restless multi-arm bandits (RMABs), a class of resource allocation problems with broad application in areas such as healthcare, online advertising, and anti-poaching, have recently been studied from a multi-agent reinforcement learning perspective. Prior RMAB research suffers from several limitations, e.g., it fails to adequately address continuous states, and requires retraining from scratch when arms opt-in and opt-out over time, a common challenge in many real world applications. We address these limitations by developing a neural network-based pre-trained model (PreFeRMAB) that has general zero-shot ability on a wide range of previously unseen RMABs, and which can be fine-tuned on specific instances in a more sample-efficient way than retraining from scratch. Our model also accommodates general multi-action settings and discrete or continuous state spaces. To enable fast generalization, we learn a novel single policy network model that utilizes feature information and employs a tra
    
[^93]: GraphMaker: 扩散模型能生成大型带属性图吗？

    GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?. (arXiv:2310.13833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.13833](http://arxiv.org/abs/2310.13833)

    GraphMaker是一种专门设计用于生成大型带属性图的新颖扩散模型。

    

    在各种实际应用中，具有节点属性的大规模图变得越来越常见。创建与真实世界示例类似的合成、富属性图对于共享图数据进行分析和开发学习模型至关重要，尤其是当原始数据限制被共享时。传统的图生成方法在处理这些复杂结构方面存在局限性。最新的扩散模型在生成没有属性和较小的分子图方面显示出潜力。然而，这些模型在生成大型带属性图方面面临着挑战，原因是复杂的属性-结构相关性和图的大规模。本文介绍了一种专门用于生成大型带属性图的新颖扩散模型：GraphMaker。我们探索了各种节点属性和图结构生成过程的组合，发现异步方法更有效地捕捉了内部。

    Large-scale graphs with node attributes are increasingly common in various real-world applications. Creating synthetic, attribute-rich graphs that mirror real-world examples is crucial, especially for sharing graph data for analysis and developing learning models when original data is restricted to be shared. Traditional graph generation methods are limited in their capacity to handle these complex structures. Recent advances in diffusion models have shown potential in generating graph structures without attributes and smaller molecular graphs. However, these models face challenges in generating large attributed graphs due to the complex attribute-structure correlations and the large size of these graphs. This paper introduces a novel diffusion model, GraphMaker, specifically designed for generating large attributed graphs. We explore various combinations of node attribute and graph structure generation processes, finding that an asynchronous approach more effectively captures the intr
    
[^94]: 用于识别半导体晶圆地图中缺陷模式的机器学习技术：一项调查、实证和实验评估

    Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations. (arXiv:2310.10705v1 [cs.LG])

    [http://arxiv.org/abs/2310.10705](http://arxiv.org/abs/2310.10705)

    本文综述了利用机器学习技术在半导体制造中识别晶圆缺陷的方法，提出了一种创新的分类体系，为不同算法和技术之间的关系提供了深入分析，并通过严谨的评估验证了算法性能。

    

    本文综述了利用机器学习（ML）技术识别半导体制造中晶圆缺陷的方法学。尽管越来越多的研究证明了ML在晶圆缺陷识别中的有效性，但在这个主题上缺乏全面的综述。本文试图弥补这个空白，通过整合现有文献，深入分析各种ML算法在晶圆缺陷检测领域的优势、局限性和潜在应用。我们提出了一种创新的方法学分类体系，详细分类了算法，并提供了更细致的子技术划分。这个分类体系从广泛的方法学类别开始，到具体的子技术结束。它帮助研究人员理解不同算法以及它们的技术之间的复杂关系。我们采用严谨的实证和实验评估来验证算法性能。

    This survey paper offers a comprehensive review of methodologies utilizing machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing. Despite the growing body of research demonstrating the effectiveness of ML in wafer defect identification, there is a noticeable absence of comprehensive reviews on this subject. This survey attempts to fill this void by amalgamating available literature and providing an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in the realm of wafer defect detection. An innovative taxonomy of methodologies that we present provides a detailed classification of algorithms into more refined categories and techniques. This taxonomy follows a four-tier structure, starting from broad methodology categories and ending with specific sub-techniques. It aids researchers in comprehending the complex relationships between different algorithms and their techniques. We employ a rigorous em
    
[^95]: BioT5：在生物学中利用化学知识和自然语言关联丰富跨模态整合

    BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])

    [http://arxiv.org/abs/2310.07276](http://arxiv.org/abs/2310.07276)

    BioT5是一个全面的预训练框架，在生物学中利用化学知识和自然语言关联丰富了跨模态整合，通过鲁棒的分子表示和上下文知识提取，实现了更有效的信息利用，展现出卓越的性能。

    

    最近在生物研究领域的进展利用分子、蛋白质和自然语言的整合来增强药物发现。然而，当前的模型存在一些限制，如生成无效的分子SMILES、对上下文信息的利用不足以及对结构化和非结构化知识的等量处理。为了解决这些问题，我们提出了一个全面的预训练框架BioT5，它通过化学知识和自然语言关联丰富了生物学中的跨模态整合。BioT5利用SELFIES进行100%鲁棒的分子表示，并从非结构化的生物文献中提取生物实体周围上下文的知识。此外，BioT5区分结构化和非结构化知识，从而更有效地利用信息。在微调后，BioT5在各种任务中展现出卓越的性能，表明其强大的能力。

    Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability 
    
[^96]: 使用多模型深度学习方法的自动化胸部X光报告生成器

    Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning Approach. (arXiv:2310.05969v1 [eess.IV])

    [http://arxiv.org/abs/2310.05969](http://arxiv.org/abs/2310.05969)

    提出了一种基于多模型深度学习的自动化胸部X光报告生成器系统，通过利用多个二元分类模型检测多种异常，在单个图像中辅助放射科医生的工作。该系统将放射学异常检测限制为心脏肥大、肺积液和实变，并通过三个步骤生成放射学报告：图像预处理、深度学习模型检测异常和生成报告。

    

    阅读和解读胸部X光图像是大多数放射科医生的例行工作之一。然而，即使对于经验最丰富的医生来说，这仍然可能是具有挑战性的。因此，我们提出了一种基于多模型深度学习的自动化胸部X光报告生成系统，旨在辅助放射科医生的工作。所提出的系统的基本思想是利用多个二元分类模型来检测多种异常，每个模型负责检测一种异常，在单个图像中。在本研究中，我们将放射学异常检测限制为心脏肥大、肺积液和实变。系统通过执行以下三个步骤生成放射学报告：图像预处理，利用深度学习模型检测异常，并生成报告。图像预处理步骤的目的是通过将其缩放为128x128像素并分割成三个部分来使输入标准化，分别涵盖上部、下部和中部。

    Reading and interpreting chest X-ray images is one of the most radiologist's routines. However, it still can be challenging, even for the most experienced ones. Therefore, we proposed a multi-model deep learning-based automated chest X-ray report generator system designed to assist radiologists in their work. The basic idea of the proposed system is by utilizing multi binary-classification models for detecting multi abnormalities, with each model responsible for detecting one abnormality, in a single image. In this study, we limited the radiology abnormalities detection to only cardiomegaly, lung effusion, and consolidation. The system generates a radiology report by performing the following three steps: image pre-processing, utilizing deep learning models to detect abnormalities, and producing a report. The aim of the image pre-processing step is to standardize the input by scaling it to 128x128 pixels and slicing it into three segments, which covers the upper, lower, and middle parts
    
[^97]: 低资源语言越狱 GPT-4

    Low-Resource Languages Jailbreak GPT-4. (arXiv:2310.02446v1 [cs.CL])

    [http://arxiv.org/abs/2310.02446](http://arxiv.org/abs/2310.02446)

    通过翻译不安全的英文输入成低资源语言，我们成功绕过了GPT-4的安全机制，并展示了这种跨语言漏洞。这一方法在实验中取得了与甚至超过了最先进的越狱攻击的效果，揭示了低资源语言在AI安全性中的薄弱环节。

    

    人工智能安全培训和大型语言模型（LLM）的红队测试是减少生成不安全内容的措施。我们的工作通过将不安全的英文输入翻译成低资源语言，成功绕过GPT-4的安全机制，并揭示了这些安全机制的跨语言漏洞。在AdvBenchmark中，GPT-4针对不安全的翻译输入进行交互，并且79%的时间内提供了可行的方案，使用户实现其有害目标，这与甚至超过了最先进的越狱攻击的效果相当。其他高/中资源语言的攻击成功率显著较低，这表明跨语言漏洞主要适用于低资源语言。以前，对低资源语言的有限训练主要影响那些使用这些语言的人，造成技术差距。然而，我们的工作突出了一个关键转变：

    AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4's safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift:
    
[^98]: Selenite：利用大型语言模型构建全面概览的在线意义建构支架

    Selenite: Scaffolding Online Sensemaking with Comprehensive Overviews Elicited from Large Language Models. (arXiv:2310.02161v4 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2310.02161](http://arxiv.org/abs/2310.02161)

    Selenite是一个利用大型语言模型的系统，能够自动产生全面的选项和标准概览，帮助用户在陌生领域进行意义建构。

    

    在陌生领域进行意义建构可能很具挑战性，要求用户耗费大量努力比较不同选项在各个标准上的差异。之前的研究和我们的形式化研究发现，人们阅读信息空间的概览可以获益，其中包括之前其他人发现有用的标准。然而，现有的意义建构工具面临着“冷启动”问题，不仅需要先前用户的大量输入来生成和分享这些概览，而且这些概览可能也会被证明是有偏见和不完整的。在这项工作中，我们介绍了一个新颖的系统Selenite，它利用大型语言模型作为推理机和知识检索器，自动产生全面的选项和标准概览，以启动用户的意义建构过程。随后，Selenite还会随着用户的使用而进行适应，帮助用户以系统而个性化的方式找到、阅读和导航陌生的信息。

    Sensemaking in unfamiliar domains can be challenging, demanding considerable user effort to compare different options with respect to various criteria. Prior research and our formative study found that people would benefit from reading an overview of an information space upfront, including the criteria others previously found useful. However, existing sensemaking tools struggle with the "cold-start" problem -- it not only requires significant input from previous users to generate and share these overviews, but such overviews may also turn out to be biased and incomplete. In this work, we introduce a novel system, Selenite, which leverages Large Language Models (LLMs) as reasoning machines and knowledge retrievers to automatically produce a comprehensive overview of options and criteria to jumpstart users' sensemaking processes. Subsequently, Selenite also adapts as people use it, helping users find, read, and navigate unfamiliar information in a systematic yet personalized manner. Thro
    
[^99]: Time-LLM: 通过重新编程大型语言模型进行时间序列预测

    Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. (arXiv:2310.01728v1 [cs.LG])

    [http://arxiv.org/abs/2310.01728](http://arxiv.org/abs/2310.01728)

    这项工作介绍了Time-LLM，一个重新编程的框架，通过重新利用大型语言模型，可以进行一般的时间序列预测。

    

    时间序列预测在许多实际动态系统中具有重要意义并得到了广泛研究。不同于自然语言处理（NLP）和计算机视觉（CV），在这些领域，一个单一的大型模型可以处理多个任务，而时间序列预测的模型通常是专门化的，需要为不同的任务和应用设计不同的模型。虽然在NLP和CV领域中，预训练的基础模型取得了令人瞩目的进展，但是在时间序列领域的发展受到数据稀疏性的限制。最近的研究表明，大型语言模型（LLMs）在复杂的序列标记中具有强大的模式识别和推理能力。然而，有效地将时间序列数据和自然语言的模态进行对齐以利用这些能力仍然具有挑战性。在这项工作中，我们提出了Time-LLM，这是一个重新编程的框架，可以重用LLMs来进行一般的时间序列预测，同时保持骨干语言模型的完整性。

    Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by 
    
[^100]: 从旋律中利用字符级语言模型生成音节级歌词

    Syllable-level lyrics generation from melody exploiting character-level language model. (arXiv:2310.00863v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.00863](http://arxiv.org/abs/2310.00863)

    该论文提出了一种利用字符级语言模型从旋律中生成音节级歌词的方法，并通过融合语言模型知识和生成器网络进行优化。通过探索ChatGPT的评估方法，以及人工评估，证明了该方法提高了生成歌词的连贯性和正确性。

    

    生成与伴奏旋律紧密相关的歌词涉及建立音乐音符与歌词音节之间的映射。这个过程需要对音节级、词级和句级语义意义上的音乐约束和语义模式有深入的理解。然而，公开的音节级预训练语言模型并不存在。为了解决这些具有挑战性的问题，我们提出利用以字符级语言模型进行音节级歌词生成。特别地，我们的方法将语言模型的语言知识融入音节级Transformer生成器网络的束搜索过程中。此外，通过探索基于ChatGPT的生成歌词评估方法，以及人工主观评估，我们证明了我们的方法增强了生成歌词的连贯性和正确性，消除了训练昂贵的新模型的需求。

    The generation of lyrics tightly connected to accompanying melodies involves establishing a mapping between musical notes and syllables of lyrics. This process requires a deep understanding of music constraints and semantic patterns at syllable-level, word-level, and sentence-level semantic meanings. However, pre-trained language models specifically designed at the syllable level are publicly unavailable. To solve these challenging issues, we propose to exploit fine-tuning character-level language models for syllable-level lyrics generation from symbolic melody. In particular, our method endeavors to incorporate linguistic knowledge of the language model into the beam search process of a syllable-level Transformer generator network. Additionally, by exploring ChatGPT-based evaluation for generated lyrics, along with human subjective evaluation, we demonstrate that our approach enhances the coherence and correctness of the generated lyrics, eliminating the need to train expensive new la
    
[^101]: 早期检测2型糖尿病患者白蛋白尿风险的监督学习模型

    Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients. (arXiv:2309.16742v1 [cs.LG])

    [http://arxiv.org/abs/2309.16742](http://arxiv.org/abs/2309.16742)

    该论文研究了2型糖尿病患者白蛋白尿的早期预测问题，并开发了一种监督学习模型。通过使用不同的监督学习算法对184条数据进行训练，得出了预测结果。

    

    糖尿病，尤其是2型糖尿病，仍然是一个重大的健康问题。与糖尿病相关的一个主要问题是其并发症的发展。糖尿病肾病是糖尿病的一种慢性并发症，不利地影响肾脏，导致肾脏损伤。诊断糖尿病肾病涉及考虑各种标准之一，其中之一是尿液中白蛋白的病理学病理学数量，称为白蛋白尿。因此，对糖尿病患者尿液中白蛋白尿的早期预测具有及时预防措施的潜力。本研究旨在开发一种监督学习模型，以预测2型糖尿病患者患有白蛋白尿的风险。所选的监督学习算法包括朴素贝叶斯，支持向量机（SVM），决策树，随机森林，AdaBoost，XGBoost和多层感知器（MLP）。我们的私有数据集包括184条糖尿病并发症风险因素的条目被用来训练算法

    Diabetes, especially T2DM, continues to be a significant health problem. One of the major concerns associated with diabetes is the development of its complications. Diabetic nephropathy, one of the chronic complication of diabetes, adversely affects the kidneys, leading to kidney damage. Diagnosing diabetic nephropathy involves considering various criteria, one of which is the presence of a pathologically significant quantity of albumin in urine, known as albuminuria. Thus, early prediction of albuminuria in diabetic patients holds the potential for timely preventive measures. This study aimed to develop a supervised learning model to predict the risk of developing albuminuria in T2DM patients. The selected supervised learning algorithms included Na\"ive Bayes, Support Vector Machine (SVM), decision tree, random forest, AdaBoost, XGBoost, and Multi-Layer Perceptron (MLP). Our private dataset, comprising 184 entries of diabetes complications risk factors, was used to train the algorithm
    
[^102]: 识别性很重要：揭示无偏学习排名中隐藏的可恢复条件

    Identifiability Matters: Revealing the Hidden Recoverable Condition in Unbiased Learning to Rank. (arXiv:2309.15560v1 [cs.IR])

    [http://arxiv.org/abs/2309.15560](http://arxiv.org/abs/2309.15560)

    研究揭示在无偏学习排名中，当点击数据不能完全拟合时，无法恢复真实相关性，导致排名性能显著降低，提出了可识别性图模型作为解决方案。

    

    无偏学习排名(Unbiased Learning to Rank, ULTR)在从有偏点击日志训练无偏排名模型的现代系统中被广泛应用。关键在于明确地建模用户行为的生成过程，并基于检验假设对点击数据进行拟合。先前的研究经验性地发现只要点击完全拟合，大多数情况下可以恢复出真实潜在相关性。然而，我们证明并非总是能够实现这一点，从而导致排名性能显著降低。在本工作中，我们旨在回答真实相关性是否能够从点击数据恢复出来的问题，这是ULTR领域的一个基本问题。我们首先将一个排名模型定义为可识别的，如果它可以恢复出真实相关性，最多只有一个缩放变换，这对于成对排名目标来说已足够。然后，我们探讨了一个等价的可识别条件，可以新颖地表达为一个图连通性测试问题：当且仅当一个图（即可识别性图）连通时，该排名模型是可识别的。

    The application of Unbiased Learning to Rank (ULTR) is widespread in modern systems for training unbiased ranking models from biased click logs. The key is to explicitly model a generation process for user behavior and fit click data based on examination hypothesis. Previous research found empirically that the true latent relevance can be recovered in most cases as long as the clicks are perfectly fitted. However, we demonstrate that this is not always achievable, resulting in a significant reduction in ranking performance. In this work, we aim to answer if or when the true relevance can be recovered from click data, which is a foundation issue for ULTR field. We first define a ranking model as identifiable if it can recover the true relevance up to a scaling transformation, which is enough for pairwise ranking objective. Then we explore an equivalent condition for identifiability that can be novely expressed as a graph connectivity test problem: if and only if a graph (namely identifi
    
[^103]: 对于神经网络的大批量训练泛化性能的LARS再审视

    Revisiting LARS for Large Batch Training Generalization of Neural Networks. (arXiv:2309.14053v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.14053](http://arxiv.org/abs/2309.14053)

    本文通过对大批量训练技术的研究，提出了一种新的算法TVLARS，该算法利用可配置的函数替代了热身阶段，以实现对于神经网络的稳健训练。实验证明，在大多数情况下，TVLARS比LARS和LAMB都有更好的性能表现，特别是在自监督学习方面。

    

    本文通过在不同场景下使用逐层自适应缩放比(LARS)来探索大批量训练技术，揭示了一些见解。具有热身阶段的LARS算法由于冗余的比例缩放导致在早期陷入尖锐的极小化器。此外，后期固定的陡峭下降限制了深度神经网络有效地遍历早期尖锐的极小化器。基于这些发现，我们提出了一种新的算法Time Varying LARS (TVLARS)，它用可配置的类似sigmoid函数替代了热身阶段，以实现在初始阶段的稳健训练。TVLARS在早期促进了梯度探索，超越了尖锐的优化器，并逐渐过渡到LARS以实现后期的稳健性。广泛的实验表明，在大多数情况下，TVLARS始终优于LARS和LAMB，分类场景中的改进达到2\%。值得注意的是，在所有自监督学习的案例中，TVLARS都胜过了LARS和LAMB，并且性能提升了

    This paper explores Large Batch Training techniques using layer-wise adaptive scaling ratio (LARS) across diverse settings, uncovering insights. LARS algorithms with warm-up tend to be trapped in sharp minimizers early on due to redundant ratio scaling. Additionally, a fixed steep decline in the latter phase restricts deep neural networks from effectively navigating early-phase sharp minimizers. Building on these findings, we propose Time Varying LARS (TVLARS), a novel algorithm that replaces warm-up with a configurable sigmoid-like function for robust training in the initial phase. TVLARS promotes gradient exploration early on, surpassing sharp optimizers and gradually transitioning to LARS for robustness in later phases. Extensive experiments demonstrate that TVLARS consistently outperforms LARS and LAMB in most cases, with up to 2\% improvement in classification scenarios. Notably, in all self-supervised learning cases, TVLARS dominates LARS and LAMB with performance improvements of
    
[^104]: 在学习者提供的问题上增强学生表现预测的SGNN-LLM协同

    Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy. (arXiv:2309.13500v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13500](http://arxiv.org/abs/2309.13500)

    这项研究介绍了一种创新的策略，将有符号图神经网络（SGNNs）和大型语言模型（LLM）的潜力协同起来，用于预测学生在学习者提供的问题上的表现。该方法利用有符号二分图全面建模学生回答，并采用对比学习框架增强了噪声的鲁棒性。

    

    通过学生内容创作，学习者合作具有可扩展教育的巨大潜力。然而，预测学生在学习者提供的问题上的表现，对于个性化学习体验至关重要，由于学生生成的数据中固有的噪声，这是具有挑战性的。此外，传统的基于图的方法可以捕获学生和问题交互的复杂网络，但在冷启动条件下，其中学生对问题的有限参与导致数据稀疏，这些方法往往表现不佳。为了解决这两个挑战，我们引入了一种创新策略，将整合有符号图神经网络（SGNNs）和大型语言模型（LLM）的潜力协同起来。我们的方法利用有符号二分图全面建模学生回答，并采用对比学习框架增强了噪声的鲁棒性。此外，LLM的贡献在于生成基础问题嵌入，特别是证明了其优势。

    Learnersourcing offers great potential for scalable education through student content creation. However, predicting student performance on learnersourced questions, which is essential for personalizing the learning experience, is challenging due to the inherent noise in student-generated data. Moreover, while conventional graph-based methods can capture the complex network of student and question interactions, they often fall short under cold start conditions where limited student engagement with questions yields sparse data. To address both challenges, we introduce an innovative strategy that synergizes the potential of integrating Signed Graph Neural Networks (SGNNs) and Large Language Model (LLM) embeddings. Our methodology employs a signed bipartite graph to comprehensively model student answers, complemented by a contrastive learning framework that enhances noise resilience. Furthermore, LLM's contribution lies in generating foundational question embeddings, proving especially adv
    
[^105]: 面向黑盒文本分类器的LLM引导因果可解释性

    Towards LLM-guided Causal Explainability for Black-box Text Classifiers. (arXiv:2309.13340v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.13340](http://arxiv.org/abs/2309.13340)

    本文提出了一种利用大型语言模型（LLM）引导黑盒文本分类器的因果可解释性的方法，通过生成反事实解释来解决这一挑战。

    

    随着越来越大且更复杂的深度学习模型的出现，比如在自然语言处理（NLP）领域，像可解释性和可解释性这样的模型质量，尽管非常令人向往，但变得越来越难以解决。例如，文本分类中的最先进模型是设计为黑盒。尽管标准的解释方法可以提供一定程度的解释能力，但这些方法主要是基于相关性的，对模型的理解能力有限。因果解释能力是更理想的目标，但在NLP领域却极具挑战性，原因有很多。受到最近利用大型语言模型（LLMs）作为专家的工作的启发，本文旨在利用最新的LLMs的指导和理解能力，通过生成反事实解释来实现黑盒文本分类器的因果可解释性。为此，我们提出了一个三步骤的流程，

    With the advent of larger and more complex deep learning models, such as in Natural Language Processing (NLP), model qualities like explainability and interpretability, albeit highly desirable, are becoming harder challenges to tackle and solve. For example, state-of-the-art models in text classification are black-box by design. Although standard explanation methods provide some degree of explainability, these are mostly correlation-based methods and do not provide much insight into the model. The alternative of causal explainability is more desirable to achieve but extremely challenging in NLP due to a variety of reasons. Inspired by recent endeavors to utilize Large Language Models (LLMs) as experts, in this work, we aim to leverage the instruction-following and textual understanding capabilities of recent state-of-the-art LLMs to facilitate causal explainability via counterfactual explanation generation for black-box text classifiers. To do this, we propose a three-step pipeline via
    
[^106]: E(2)-对称图规划用于导航

    E(2)-Equivariant Graph Planning for Navigation. (arXiv:2309.13043v1 [cs.RO])

    [http://arxiv.org/abs/2309.13043](http://arxiv.org/abs/2309.13043)

    本文提出了一种E(2)-对称图规划用于导航的方法，通过利用欧几里得对称性和开发等变消息传递网络，实现了高效、稳定和具有泛化能力的机器人导航学习。

    

    机器人导航的学习是一项关键且具有挑战性的任务。真实世界数据集的稀缺性和昂贵性需要高效的学习方法。在本文中，我们利用二维导航规划中的欧几里得对称性，该对称性源于参考框架之间的欧几里得变换，并实现参数共享。为了解决非结构化环境的挑战，我们将导航问题规划为在几何图上的规划，并开发了一种等变消息传递网络来执行价值迭代。此外，为了处理多摄像头输入，我们提出了一个可学习的等变层将特征提升到所需空间。我们在包括结构化和非结构化环境、已知和未知地图以及给定点目标或语义目标的五个不同任务上进行了全面评估。我们的实验结果证实了训练效率、稳定性和泛化性能的显著优势。

    Learning for robot navigation presents a critical and challenging task. The scarcity and costliness of real-world datasets necessitate efficient learning approaches. In this letter, we exploit Euclidean symmetry in planning for 2D navigation, which originates from Euclidean transformations between reference frames and enables parameter sharing. To address the challenges of unstructured environments, we formulate the navigation problem as planning on a geometric graph and develop an equivariant message passing network to perform value iteration. Furthermore, to handle multi-camera input, we propose a learnable equivariant layer to lift features to a desired space. We conduct comprehensive evaluations across five diverse tasks encompassing structured and unstructured environments, along with maps of known and unknown, given point goals or semantic goals. Our experiments confirm the substantial benefits on training efficiency, stability, and generalization.
    
[^107]: 基于流形的无调参提示分类的重新嵌入方法

    Manifold-based Verbalizer Space Re-embedding for Tuning-free Prompt-based Classification. (arXiv:2309.04174v1 [cs.CL])

    [http://arxiv.org/abs/2309.04174](http://arxiv.org/abs/2309.04174)

    本研究提出了一种无需调参的基于流形的语言转换器嵌入方法，通过保留同一类中的局部特性来进行分类，实验证明其与自动化的语言转换器效果相当。

    

    提示分类通过利用[MASK]标记的遗漏问题形式来适应任务，然后通过预定义的语言转换器将填充的标记映射到标签上。最近的研究已经探索了使用语言转换器嵌入来减少这一过程中的劳动力。然而，所有现有的研究都需要对预训练模型或附加可训练嵌入进行调参过程。同时，由于表示空间中潜在的非线性流形，高维语言转换器嵌入之间的距离不应该使用欧氏距离来衡量。在本研究中，我们提出了一种无调参基于流形的空间重新嵌入方法，称为具有内类近邻约束的局部线性嵌入（LLE-INC），用于语言转换器嵌入，它保留了同一类中的局部特性作为分类的引导。实验结果表明，即使不进行任何参数调优，我们的LLE-INC与自动化的语言转换器相媲美。

    Prompt-based classification adapts tasks to a cloze question format utilizing the [MASK] token and the filled tokens are then mapped to labels through pre-defined verbalizers. Recent studies have explored the use of verbalizer embeddings to reduce labor in this process. However, all existing studies require a tuning process for either the pre-trained models or additional trainable embeddings. Meanwhile, the distance between high-dimensional verbalizer embeddings should not be measured by Euclidean distance due to the potential for non-linear manifolds in the representation space. In this study, we propose a tuning-free manifold-based space re-embedding method called Locally Linear Embedding with Intra-class Neighborhood Constraint (LLE-INC) for verbalizer embeddings, which preserves local properties within the same class as guidance for classification. Experimental results indicate that even without tuning any parameters, our LLE-INC is on par with automated verbalizers with parameter 
    
[^108]: 强化学习辅助进化算法：调查和研究机会

    Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and Research Opportunities. (arXiv:2308.13420v2 [cs.NE] CROSS LISTED)

    [http://arxiv.org/abs/2308.13420](http://arxiv.org/abs/2308.13420)

    本文调查了强化学习辅助进化算法（RL-EA），该算法将强化学习与进化算法结合，以提高优化性能。对各种RL-EA的结构、操作符和搜索模式进行了分类和概述。

    

    进化算法是一类基于自然进化原理的随机搜索方法，因其在各种实际优化问题中的卓越性能而广受赞誉。尽管全球的研究人员提出了各种各样的进化算法，但仍存在一些限制，如收敛速度慢和泛化能力差。因此，许多学者积极探索改进算法结构、操作符、搜索模式等方法，以提高其优化性能。近年来，将强化学习作为进化算法框架的一个组成部分，已经展示出超越性能。本文综述了将强化学习集成到进化算法中的最新研究进展，被称为强化学习辅助进化算法（RL-EA）。我们首先介绍了强化学习和进化算法的概念。然后，我们提供了一个对RL-EA中不同结构、操作符和搜索模式的分类方法。

    Evolutionary algorithms (EA), a class of stochastic search methods based on the principles of natural evolution, have received widespread acclaim for their exceptional performance in various real-world optimization problems. While researchers worldwide have proposed a wide variety of EAs, certain limitations remain, such as slow convergence speed and poor generalization capabilities. Consequently, numerous scholars actively explore improvements to algorithmic structures, operators, search patterns, etc., to enhance their optimization performance. Reinforcement learning (RL) integrated as a component in the EA framework has demonstrated superior performance in recent years. This paper presents a comprehensive survey on integrating reinforcement learning into the evolutionary algorithm, referred to as reinforcement learning-assisted evolutionary algorithm (RL-EA). We begin with the conceptual outlines of reinforcement learning and the evolutionary algorithm. We then provide a taxonomy of
    
[^109]: 对大型语言模型代码生成的鲁棒性和可靠性的研究

    A Study on Robustness and Reliability of Large Language Model Code Generation. (arXiv:2308.10335v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10335](http://arxiv.org/abs/2308.10335)

    本研究针对大型语言模型生成的代码的可靠性和鲁棒性进行了研究，发现在真实的软件开发中可执行的代码并不能保证可靠和鲁棒，滥用API可能导致严重问题。这对初级开发者来说尤其危险，因为他们很难察觉到代码中的API滥用问题。

    

    最近，大型语言模型(LLMs)在理解自然语言和生成编程代码方面显示出了非凡能力。当遇到编码问题时，软件工程师常常会咨询LLMs。尽管已经做出了一些努力来避免语法错误并使代码与预期的语义对齐，但LLMs生成的代码的可靠性和鲁棒性尚未被深入研究。在真实的软件开发环境中，可执行的代码并不等同于可靠和鲁棒的代码。在生成的代码中滥用API可能会导致严重的问题，如资源泄漏、程序崩溃。更糟糕的是，LLM代码生成服务的用户实际上是最容易受到这些看似正确的代码影响的开发者——他们通常是不熟悉LLMs为他们生成代码的API的初级开发者。因此，他们很难察觉到API的滥用。

    Recently, the large language models (LLMs) have shown extraordinary ability in understanding natural language and generating programming code. It has been a common practice of software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability and robustness of the code generationfrom LLMs have not yet been thoroughly studied. The executable code is not equivalent to the reliable and robust code, especially in the context of real-world software development. The misuse of APIs in the generated code could lead to severe problem, such as resource leaks, program crashes. To make things worse, the users of LLM code generation services are actually the developers that are most vulnerable to these code that seems right -- They are always novice developers that are not familiar with the APIs that LLMs generate code for them. Therefore, they could hardly tell the misuse in t
    
[^110]: 通过发现高阶抽象来学习逻辑程序

    Learning Logic Programs by Discovering Higher-Order Abstractions. (arXiv:2308.08334v1 [cs.LG])

    [http://arxiv.org/abs/2308.08334](http://arxiv.org/abs/2308.08334)

    本论文介绍了一种通过发现高阶抽象来学习逻辑程序的方法，并且在多个领域的实验结果表明，该方法能够显著提高预测精度并减少学习时间。

    

    发现新颖的抽象对于人类级别的人工智能至关重要。我们介绍了一种发现高阶抽象（例如map、filter和fold）的方法。我们专注于归纳逻辑编程，即从示例和背景知识中归纳逻辑程序。我们引入了高阶重构问题，目标是通过引入高阶抽象来压缩逻辑程序。我们将我们的方法实现在STEVIE中，它将高阶重构问题建模为约束优化问题。我们在多个领域，包括程序合成和视觉推理，的实验结果表明，与没有重构相比，STEVIE可以提高预测精度27%并将学习时间减少47%。我们还展示了STEVIE可以发现适用于不同领域的抽象。

    Discovering novel abstractions is important for human-level AI. We introduce an approach to discover higher-order abstractions, such as map, filter, and fold. We focus on inductive logic programming, which induces logic programs from examples and background knowledge. We introduce the higher-order refactoring problem, where the goal is to compress a logic program by introducing higher-order abstractions. We implement our approach in STEVIE, which formulates the higher-order refactoring problem as a constraint optimisation problem. Our experimental results on multiple domains, including program synthesis and visual reasoning, show that, compared to no refactoring, STEVIE can improve predictive accuracies by 27% and reduce learning times by 47%. We also show that STEVIE can discover abstractions that transfer to different domains
    
[^111]: AIs的发展脱靴法

    Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])

    [http://arxiv.org/abs/2308.04586](http://arxiv.org/abs/2308.04586)

    传统的符号AI方法和深度学习AI方法无法满足创建强大和可信赖的AI的挑战，然而，发展脱靴法通过模仿人类儿童的能力发展过程，为创建稳健可靠的AI提供了希望。

    

    尽管当前一些AI在封闭的世界，如棋盘游戏中超越了人类能力，但它们在混乱的现实世界中的表现有限。它们会犯奇怪的错误而且没有意识到。它们很难受到指导，不能运用常识，缺乏好奇心。它们不能成为良好的合作者。传统手动构建的符号AI方法构建的系统和使用生成和深度学习AI方法(包括大规模语言模型)构建的系统都无法应对这些挑战。它们不适合创建强大和可信赖的AI。尽管此方法不属于主流的AI方法，但发展脱靴法显示出希望。在发展脱靴法中，AI像人类儿童一样发展能力。它们从先天能力开始。像人类一样，它们与环境互动，并从互动中学习。它们通过自我发展的能力逐步扩展先天能力。它们互动并逐渐将所学应用于实际操作。

    Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
    
[^112]: 基于评分的强化学习方法

    Rating-based Reinforcement Learning. (arXiv:2307.16348v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.16348](http://arxiv.org/abs/2307.16348)

    本文提出了一种基于评分的强化学习方法，通过利用人类评分来获得人类指导，该方法不同于现有的强化学习方法，它通过对样本轨迹的评估来进行学习。研究结果表明，该方法在实验中取得了良好的效果和收益。

    

    本文提出了一种新的基于评分的强化学习方法，利用人类评分来获得人类指导强化学习。与现有的基于偏好和基于排名的强化学习范式不同，该方法基于人类对样本轨迹的评估而不是对样本对的相对比较。基于评分的强化学习方法建立在一个新的人类评分预测模型和一种新颖的多类损失函数上。我们通过合成评分和真实人类评分进行了多个实验研究，评估了新的基于评分的强化学习方法的有效性和好处。

    This paper develops a novel rating-based reinforcement learning approach that uses human ratings to obtain human guidance in reinforcement learning. Different from the existing preference-based and ranking-based reinforcement learning paradigms, based on human relative preferences over sample pairs, the proposed rating-based reinforcement learning approach is based on human evaluation of individual trajectories without relative comparisons between sample pairs. The rating-based reinforcement learning approach builds on a new prediction model for human ratings and a novel multi-class loss function. We conduct several experimental studies based on synthetic ratings and real human ratings to evaluate the effectiveness and benefits of the new rating-based reinforcement learning approach.
    
[^113]: REX: 快速探索与利用的增强型AI代理方法

    REX: Rapid Exploration and eXploitation for AI Agents. (arXiv:2307.08962v1 [cs.AI])

    [http://arxiv.org/abs/2307.08962](http://arxiv.org/abs/2307.08962)

    本文提出了一种增强型的快速探索与利用的AI代理方法REX，它通过引入额外的奖励层和类似于UCB分数的概念，实现了更强大和高效的AI代理性能，并且具有离线行为利用和与基础模型无缝集成的优势。

    

    本文提出了一种增强型的快速探索与利用的AI代理方法，称为REX。现有的AutoGPT风格技术存在一些固有的限制，如对于决策的精确描述的过度依赖，以及缺乏类似传统强化学习(Reinforcement Learning，RL)中的尝试和失败程序的系统性方法。REX引入了额外的奖励层，并集成了类似于上限置信界限(UCB)分数的概念，从而实现更强大和高效的AI代理性能。这种方法的优势是可以利用来自日志的离线行为，并与现有的基础模型无缝集成，而不需要任何模型微调。通过与现有方法（如思维链(CoT)和规划推理(RAP)）的比较分析，基于REX的方法展示了相当的性能，并在某些情况下甚至超过了这些现有技术所取得的结果。

    In this paper, we propose an enhanced approach for Rapid Exploration and eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional Reinforcement Learning (RL). REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance. This approach has the advantage of enabling the utilization of offline behaviors from logs and allowing seamless integration with existing foundation models while it does not require any model fine-tuning. Through comparative analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP), REX-based methods demonstrate comparable performance and, in certain cases, even surpass the results achieved by these existing techniqu
    
[^114]: 使用语言模型的黑盒预测易出错测试修复类别

    Black-Box Prediction of Flaky Test Fix Categories Using Language Models. (arXiv:2307.00012v1 [cs.SE])

    [http://arxiv.org/abs/2307.00012](http://arxiv.org/abs/2307.00012)

    本文提出了一个使用语言模型的框架，可以自动生成易出错测试的标记数据集，并通过分析测试代码来预测测试的修复类别。实验结果表明UniXcoder优于CodeBERT。

    

    易出错测试会在相同软件版本的测试下非确定性地通过或失败，引起混乱并浪费开发者时间。尽管机器学习模型已经被用于预测易出错性及其根本原因，但在提供修复支持方面仍有较少工作。为了填补这一空白，我们提出了一个框架，通过仅分析测试代码自动生成13个修复类别的标记数据集，并训练模型来预测易出错测试的修复类别。虽然在当前阶段准确预测修复本身是不现实的，但这些类别提供了关于需要检查的测试代码部分的精确指导。我们的方法基于语言模型，即CodeBERT和UniXcoder，其输出经过前馈神经网络（FNN）或基于孪生网络的Few Shot Learning（FSL）进行了微调。我们的实验结果表明，UniXcoder在正确预测大多数修复类别方面表现优于CodeBERT。

    Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting developer time. While machine learning models have been used to predict flakiness and its root causes, there is less work on providing support to fix the problem. To address this gap, we propose a framework that automatically generates labeled datasets for 13 fix categories and train models to predict the fix category of a flaky test by analyzing the test code only. Though it is unrealistic at this stage to accurately predict the fix itself, the categories provide precise guidance about what part of the test code to look at. Our approach is based on language models, namely CodeBERT and UniXcoder, whose output is fine-tuned with a Feed Forward Neural Network (FNN) or a Siamese Network-based Few Shot Learning (FSL). Our experimental results show that UniXcoder outperforms CodeBERT, in correctly predicting most of the categories of fixes a dev
    
[^115]: 零样本识别中的视觉-语言模型的挑战：粒度和正确性

    Challenges of Zero-Shot Recognition with Vision-Language Models: Granularity and Correctness. (arXiv:2306.16048v1 [cs.CV])

    [http://arxiv.org/abs/2306.16048](http://arxiv.org/abs/2306.16048)

    本文研究了将视觉-语言模型应用于零样本视觉识别任务所面临的挑战，发现VLMs在识别细粒度概念方面表现更好，并指出了VLMs中相似度分数不能严格反映正确性的问题，提出了未来研究方向。

    

    本文研究了将视觉-语言模型（VLMs）应用于开放世界环境下的零样本视觉识别任务所面临的挑战，重点关注对比视觉-语言模型（如CLIP）的应用。我们首先检查了VLMs在不同粒度概念上的表现。我们提出了一种公正评估两种实验设置下性能差异的方法，并发现VLMs在识别细粒度概念方面表现更好。此外，我们发现VLMs产生的相似度分数并不能严格反映文本输入在视觉输入下的正确性。我们提出了一种评估协议来测试我们的假设，即分数可能会偏向更具信息的描述，并且由于嵌入之间的相似度分数的性质，对于VLMs来说识别相似但错误的描述之间的正确性是具有挑战性的。我们的研究强调了在开放世界环境中使用VLMs的挑战，并提出了未来研究的方向。

    This paper investigates the challenges of applying vision-language models (VLMs) to zero-shot visual recognition tasks in an open-world setting, with a focus on contrastive vision-language models such as CLIP. We first examine the performance of VLMs on concepts of different granularity levels. We propose a way to fairly evaluate the performance discrepancy under two experimental setups and find that VLMs are better at recognizing fine-grained concepts. Furthermore, we find that the similarity scores from VLMs do not strictly reflect the correctness of the textual inputs given visual input. We propose an evaluation protocol to test our hypothesis that the scores can be biased towards more informative descriptions, and the nature of the similarity score between embedding makes it challenging for VLMs to recognize the correctness between similar but wrong descriptions. Our study highlights the challenges of using VLMs in open-world settings and suggests directions for future research to 
    
[^116]: 何去何从：深度学习加速的数字硬件视角

    To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v1 [cs.NE])

    [http://arxiv.org/abs/2306.15749](http://arxiv.org/abs/2306.15749)

    神经形态计算旨在通过仿真脑部操作来提高深度学习模型的效率，但是在SNNs的高效硬件后端设计上仍需进一步研究。

    

    随着深度学习模型规模的增加，它们在涵盖计算机视觉到自然语言处理等领域变得越来越有竞争力；然而，这是以效率为代价的，因为它们需要越来越多的内存和计算能力。生物脑的功耗效率超过任何大规模深度学习（DL）模型；因此，神经形态计算试图模仿脑部操作，例如基于脉冲的信息处理，以提高DL模型的效率。尽管脑部有诸如高效的信息传输、密集的神经元连接和计算与存储的共同位置等优势，但可用的生物基底严重限制了生物大脑的进化。电子硬件没有相同的约束；因此，虽然建模脉冲神经网络（SNNs）可能揭示了一个谜题的一部分，但对于SNNs的高效硬件后端设计需要进一步研究。

    As deep learning models scale, they become increasingly competitive from domains spanning computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms the one of any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, po
    
[^117]: SPRINT：通过语言指令 relabeling 实现可扩展的策略预训练

    SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling. (arXiv:2306.11886v1 [cs.RO])

    [http://arxiv.org/abs/2306.11886](http://arxiv.org/abs/2306.11886)

    SPRINT 提出了一种离线策略预训练方法，通过指令重标记及离线强化学习实现可扩展的预训练任务，大大减少了预训练所需的人力，同时使机器人能够获取更丰富的技能库，相较于之前的预训练方法，能够更快地学习新的长时间跨度任务。

    

    预训练机器人策略并赋予丰富的技能集合可以大大加速下游任务的学习。先前的研究通过自然语言指令定义预训练任务，但这需要人为地注释数十万个指令。因此，我们提出了 SPRINT，这是一种可扩展的离线策略预训练方法，可大大减少预训练多样的技能所需的人力。我们的方法使用两个核心想法来自动扩展基础预训练任务：通过大型语言模型来进行指令重标记和通过离线强化学习进行交叉轨迹技能链接。因此，SPRINT 预训练可以为机器人装备更丰富的技能库。在家庭模拟器和真实机器人厨房操作任务中的实验结果表明，SPRINT 相对于之前的预训练方法能够更快地学习新的长时间跨度任务。

    Pre-training robot policies with a rich set of skills can substantially accelerate the learning of downstream tasks. Prior works have defined pre-training tasks via natural language instructions, but doing so requires tedious human annotation of hundreds of thousands of instructions. Thus, we propose SPRINT, a scalable offline policy pre-training approach which substantially reduces the human effort needed for pre-training a diverse set of skills. Our method uses two core ideas to automatically expand a base set of pre-training tasks: instruction relabeling via large language models and cross-trajectory skill chaining through offline reinforcement learning. As a result, SPRINT pre-training equips robots with a much richer repertoire of skills. Experimental results in a household simulator and on a real robot kitchen manipulation task show that SPRINT leads to substantially faster learning of new long-horizon tasks than previous pre-training approaches. Website at https://clvrai.com/spr
    
[^118]: 神经元元胞自动机能够对信号作出响应

    Neural Cellular Automata Can Respond to Signals. (arXiv:2305.12971v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2305.12971](http://arxiv.org/abs/2305.12971)

    神经元元胞自动机对信号作出响应的能力，为神经元元胞自动机作为人工形态发生模型的发展提供了基础，并且为将动态行为嵌入模型铺平了道路。

    

    神经元元胞自动机（NCAs）是一种形态发生模型，能够从单个种子细胞生长出二维人工生物体。本文展示了NCAs可以被训练成对信号作出响应。使用了两种类型的信号：内部（基因编码）信号和外部（环境）信号。信号被呈现给单个像素在一个时间步长内。结果显示，NCAs能够根据内部信号发展成多种不同形态，并且能够根据外部信号改变颜色。总体上，这些结果为NCAs作为人工形态发生模型的发展贡献了，并为将动态行为嵌入NCA模型奠定了基础。通过GitHub可以获得代码和目标图像：https://github.com/jstovold/ALIFE2023

    Neural Cellular Automata (NCAs) are a model of morphogenesis, capable of growing two-dimensional artificial organisms from a single seed cell. In this paper, we show that NCAs can be trained to respond to signals. Two types of signal are used: internal (genomically-coded) signals, and external (environmental) signals. Signals are presented to a single pixel for a single timestep.  Results show NCAs are able to grow into multiple distinct forms based on internal signals, and are able to change colour based on external signals. Overall these contribute to the development of NCAs as a model of artificial morphogenesis, and pave the way for future developments embedding dynamic behaviour into the NCA model.  Code and target images are available through GitHub: https://github.com/jstovold/ALIFE2023
    
[^119]: 基于知识增强的生成预训练模型在中国医学执业医师资格考试上的应用研究

    Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model. (arXiv:2305.10163v1 [cs.CL])

    [http://arxiv.org/abs/2305.10163](http://arxiv.org/abs/2305.10163)

    本研究通过在ChatGPT中集成医学领域知识和启用少样本学习的新方法，在中国国家医学执业医师资格考试中取得成功，这为建立在自然语言处理技术和医学领域知识的创新应用提供了可能。

    

    生成式预训练模型（GPT），如ChatGPT，在各种自然语言处理任务中展现出了出色的性能。尽管ChatGPT已被整合到各个领域的工作流中以提高效率，但其微调过程的灵活性不足，阻碍了其在需要广泛领域专业知识和语义知识的领域，如医疗保健，的应用。在本文中，我们评估了ChatGPT在中国国家医学执业医师资格考试（CNMLE）中的表现，并提出了一种新的方法来改进ChatGPT，即从两个方面集成医学领域知识和启用少样本学习。通过使用简单但有效的检索方法，将医学背景知识提取为语义指令来指导ChatGPT的推断。类似地，相关的医疗问题被识别并作为演示输入给ChatGPT。实验结果表明，直接应用ChatGPT无法在CNMLE上获得合格分数（51分），只有基于知识增强训练的模型成功通过考试。

    Generative Pre-Training (GPT) models like ChatGPT have demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. Although ChatGPT has been integrated into the overall workflow to boost efficiency in many domains, the lack of flexibility in the finetuning process hinders its applications in areas that demand extensive domain expertise and semantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on the China National Medical Licensing Examination (CNMLE) and propose a novel approach to improve ChatGPT from two perspectives: integrating medical domain knowledge and enabling few-shot learning. By using a simple but effective retrieval method, medical background knowledge is extracted as semantic instructions to guide the inference of ChatGPT. Similarly, relevant medical questions are identified and fed as demonstrations to ChatGPT. Experimental results show that directly applying ChatGPT fails to qualify the CNMLE at a score of 51 (i.e., onl
    
[^120]: 一种具有相关信念下最优学习的序列公交网络设计算法

    A sequential transit network design algorithm with optimal learning under correlated beliefs. (arXiv:2305.09452v1 [cs.AI])

    [http://arxiv.org/abs/2305.09452](http://arxiv.org/abs/2305.09452)

    提出一种结合序列公交网络设计和最优学习的算法，用于准确估计潜在出行需求，并规避设计与实际需求不一致的风险。

    

    准确估计潜在需求对于公交服务路线设计至关重要。本论文提出了一种结合了序列公交网络设计和最优学习的人工智能驱动算法。运营商逐步扩展其路线系统，以避免设计路线与实际出行需求不一致的风险。同时，观测到的信息将被存档以更新运营商当前使用的知识。算法中比较了三种学习策略：多臂老虎机、知识梯度和具有相关信念的知识梯度。该算法的模拟结果显示其在公交网络设计中的良好表现。

    Mobility service route design requires potential demand information to well accommodate travel demand within the service region. Transit planners and operators can access various data sources including household travel survey data and mobile device location logs. However, when implementing a mobility system with emerging technologies, estimating demand level becomes harder because of more uncertainties with user behaviors. Therefore, this study proposes an artificial intelligence-driven algorithm that combines sequential transit network design with optimal learning. An operator gradually expands its route system to avoid risks from inconsistency between designed routes and actual travel demand. At the same time, observed information is archived to update the knowledge that the operator currently uses. Three learning policies are compared within the algorithm: multi-armed bandit, knowledge gradient, and knowledge gradient with correlated beliefs. For validation, a new route system is de
    
[^121]: 基于掩码语言模型的文本对抗样本检测

    Masked Language Model Based Textual Adversarial Example Detection. (arXiv:2304.08767v1 [cs.CR])

    [http://arxiv.org/abs/2304.08767](http://arxiv.org/abs/2304.08767)

    通过探索掩码语言模型引起的流形变化，我们提出了一种插即用的文本对抗例子检测方法，可以在保持对分类任务、模型结构和数据集无依赖的前提下，有效地检测到对抗例子。

    

    对抗攻击是机器学习模型在关键安全应用中可靠部署的严重威胁，稍微修改输入即可误导当前模型进行错误预测。最近，大量研究表明，对抗样本往往偏离正常样本的基础数据流形，而预训练的掩码语言模型可以适应正常的NLP数据流形。为了探索如何将掩码语言模型用于对抗性检测，我们提出了一种新颖的文本对抗例子检测方法，即基于掩码语言模型的检测（MLMD），它可以通过探索掩码语言模型引起的流形变化，在正常样本和对抗样本之间产生明显可区分的信号。MLMD具有即插即用的使用方法（即无需重新训练受害模型）用于对抗性防御，而且不受分类任务、受害模型结构和待防御的数据集的影响。

    Adversarial attacks are a serious threat to the reliable deployment of machine learning models in safety-critical applications. They can misguide current models to predict incorrectly by slightly modifying the inputs. Recently, substantial work has shown that adversarial examples tend to deviate from the underlying data manifold of normal examples, whereas pre-trained masked language models can fit the manifold of normal NLP data. To explore how to use the masked language model in adversarial detection, we propose a novel textual adversarial example detection method, namely Masked Language Model-based Detection (MLMD), which can produce clearly distinguishable signals between normal examples and adversarial examples by exploring the changes in manifolds induced by the masked language model. MLMD features a plug and play usage (i.e., no need to retrain the victim model) for adversarial defense and it is agnostic to classification tasks, victim model's architectures, and to-be-defended a
    
[^122]: 有效地对齐跨语言会话任务的提示调整跨语言转移学习

    Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v1 [cs.CL])

    [http://arxiv.org/abs/2304.01295](http://arxiv.org/abs/2304.01295)

    本文提出了一个平行大规模多语种会话数据集XSGD，开发了一种有效的基于提示调整的方法来学习对齐提示，同时研究了跨语言任务的NLI-based和vanilla分类器，并在插槽填充和意图分类任务上评估了模型的跨语言泛化能力。

    

    针对自然语言处理任务，跨语言转移的语言模型已被广泛研究，但是对于会话任务的研究相对较少。本文提出了XSGD，这是一个由Schema-Guided Dialogue（SGD）翻译成105种其他语言的平行大规模多语种会话数据集。为了实现对齐的跨语言表示方法，我们开发了一种有效的基于提示调整的方法来学习对齐提示。我们还研究了两种不同的分类器：NLI-based和vanilla分类器，并测试了对齐提示所实现的跨语言能力。我们在两个对话任务（插槽填充和意图分类）上评估了我们模型的跨语言泛化能力。

    Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce XSGD, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains approximately 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model's cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent cla
    
[^123]: 通识知识辅助的资源受限和细粒度目标检测的深度学习方法

    Commonsense Knowledge Assisted Deep Learning for Resource-constrained and Fine-grained Object Detection. (arXiv:2303.09026v1 [cs.CV])

    [http://arxiv.org/abs/2303.09026](http://arxiv.org/abs/2303.09026)

    本文提出了一种通识知识辅助的细粒度目标检测方法，利用通识知识推理模块处理由基准深度学习检测器给出的粗粒度标签，从而提高目标检测的准确性。经过实验验证，该方法相比于现有方法需要更少的计算量和标注资源。

    

    本文考虑边缘计算等资源受限场景下的细粒度图像目标检测问题。针对使用现代深度学习目标检测器时需要使用大型模型和大量数据标注的精准细粒度检测需求，提出一种方法，即利用通识知识辅助粗粒度目标检测器获取精准的细粒度检测结果。引入通识知识推理模块(CKIM)处理由基准深度学习检测器给出的粗粒度标签，从而生成细粒度标签。论文中考虑了模糊规则和清晰规则的推理，前者用于处理目标语义标签的模糊性。实验结果表明所提方法可以有效提高目标检测的准确性，同时相比于现有方法需要更少的计算量和标注资源。

    In this paper, we consider fine-grained image object detection in resource-constrained cases such as edge computing. Deep learning (DL), namely learning with deep neural networks (DNNs), has become the dominating approach to object detection. To achieve accurate fine-grained detection, one needs to employ a large enough DNN model and a vast amount of data annotations, which brings a challenge for using modern DL object detectors in resource-constrained cases. To this end, we propose an approach, which leverages commonsense knowledge to assist a coarse-grained object detector to get accurate fine-grained detection results. Specifically, we introduce a commonsense knowledge inference module (CKIM) to process coarse-grained lables given by a benchmark DL detector to produce fine-grained lables. We consider both crisp-rule and fuzzy-rule based inference in our CKIM; the latter is used to handle ambiguity in the target semantic labels. We implement our method based on several modern DL dete
    
[^124]: Magnushammer: 一种基于Transformer的前提选择方法

    Magnushammer: A Transformer-based Approach to Premise Selection. (arXiv:2303.04488v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04488](http://arxiv.org/abs/2303.04488)

    Magnushammer是一种基于Transformer的前提选择方法，通过在PISA基准上的测试表明，它可以大幅度超越传统符号系统，并将先前最先进的证明率从57.0％提高到71.0％。

    

    前提选择是自动定理证明的一个基本问题。以往的方法常常使用复杂的符号方法，依赖领域知识，并需要大量的工程工作来解决这个任务。在这项工作中，我们展示了基于神经转换器的Magnushammer方法可以大幅度地超越传统的符号系统。通过在PISA基准上的测试，Magnushammer的证明率达到了59.5％，而最成熟和流行的基于符号的求解器Sledgehammer的证明率只有38.3％。此外，通过将Magnushammer与基于语言模型的神经形式证明器相结合，我们将先前最先进的证明率从57.0％大幅提高到71.0％。

    Premise selection is a fundamental problem of automated theorem proving. Previous works often use intricate symbolic methods, rely on domain knowledge, and require significant engineering effort to solve this task. In this work, we show that Magnushammer, a neural transformer-based approach, can outperform traditional symbolic systems by a large margin. Tested on the PISA benchmark, Magnushammer achieves $59.5\%$ proof rate compared to a $38.3\%$ proof rate of Sledgehammer, the most mature and popular symbolic-based solver. Furthermore, by combining Magnushammer with a neural formal prover based on a language model, we significantly improve the previous state-of-the-art proof rate from $57.0\%$ to $71.0\%$.
    
[^125]: 评估使用模型无关的度量标准解释机器学习预测的可解释性

    Evaluating explainability for machine learning predictions using model-agnostic metrics. (arXiv:2302.12094v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12094](http://arxiv.org/abs/2302.12094)

    本文提出了一种使用模型无关的度量标准，用于评估机器学习模型的预测结果的可解释性。这些度量标准将各个解释能力方面总结成标量，提供全面的理解并促进决策者和利益相关者之间的沟通，从而提高整体的透明度。

    

    人工智能技术的快速发展带来了管理和监管方面的众多挑战。人工智能系统正在被整合到各个行业和领域，决策者需全面细致地了解这些系统的能力和限制。这个需求的一个关键方面是能够解释机器学习模型的结果，这对于提高透明度和信任度以及帮助模型在道德上进行训练至关重要。本文提出了新颖的度量标准，用于量化AI模型预测结果是否可以通过其特征进行易于解释。我们的度量标准将解释能力的不同方面总结为标量，提供对模型预测的更全面的理解，促进决策者和利益相关者之间的沟通，从而提高整体的透明度。

    Rapid advancements in artificial intelligence (AI) technology have brought about a plethora of new challenges in terms of governance and regulation. AI systems are being integrated into various industries and sectors, creating a demand from decision-makers to possess a comprehensive and nuanced understanding of the capabilities and limitations of these systems. One critical aspect of this demand is the ability to explain the results of machine learning models, which is crucial to promoting transparency and trust in AI systems, as well as fundamental in helping machine learning models to be trained ethically. In this paper, we present novel metrics to quantify the degree of which AI model predictions can be easily explainable by its features. Our metrics summarize different aspects of explainability into scalars, providing a more comprehensive understanding of model predictions and facilitating communication between decision-makers and stakeholders, thereby increasing the overall transp
    
[^126]: 基于泛化的相似性

    Generalization-based similarity. (arXiv:2302.10096v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.10096](http://arxiv.org/abs/2302.10096)

    本文从头构建了一个基于抽象代数和定性概念的相似性概念，并将其通过模型论类型自然地嵌入到一阶逻辑中。

    

    检测和利用看似不相关的对象之间的相似性是类比推理的核心，而类比推理又是人工智能的核心。本文从头开始开发了一个基于抽象代数和定性概念的相似性概念，基于观察到泛化集合可以编码元素的重要属性。我们证明了以这种方式定义的相似性具有吸引人的数学属性。通过从基本概念出发构建相似性概念，并将其自然地嵌入到模型论类型中的一阶逻辑中，我们使读者相信其合理性。

    Detecting and exploiting similarities between seemingly distant objects is at the core of analogical reasoning which itself is at the core of artificial intelligence. This paper develops {\em from the ground up} an abstract algebraic and {\em qualitative} notion of similarity based on the observation that sets of generalizations encode important properties of elements. We show that similarity defined in this way has appealing mathematical properties. As we construct our notion of similarity from first principles using only elementary concepts of universal algebra, to convince the reader of its plausibility, we show that it can be naturally embedded into first-order logic via model-theoretic types.
    
[^127]: 非欧几里德旅行商问题的凸包最便宜插入启发式解法

    A Convex Hull Cheapest Insertion Heuristic for the Non-Euclidean TSP. (arXiv:2302.06582v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.06582](http://arxiv.org/abs/2302.06582)

    本文提出了一种适用于非欧几里德旅行商问题的凸包最便宜插入启发式解法，通过使用多维缩放将非欧几里德空间的点近似到欧几里德空间，生成了初始化算法的凸包。在评估中发现，该算法在大多数情况下优于最邻近算法。

    

    众所周知，凸包最便宜插入启发式算法可以在欧几里德空间中产生良好的旅行商问题解决方案，但还未在非欧几里德情况下进行扩展。为了解决非欧几里德空间中处理障碍物的困难，提出的改进方法使用多维缩放将这些点首先近似到欧几里德空间，从而可以生成初始化算法的凸包。通过修改TSPLIB基准数据集，向其中添加不可通过的分割器来产生非欧几里德空间，评估了所提出的算法。在所研究的案例中，该算法表现出优于常用的最邻近算法的性能，达到96%的情况。

    The convex hull cheapest insertion heuristic is known to generate good solutions to the Traveling Salesperson Problem in Euclidean spaces, but it has not been extended to the non-Euclidean case. To address the difficulty of dealing with obstacles in the non-Euclidean space, the proposed adaptation uses multidimensional scaling to first approximate these points in a Euclidean space, thereby enabling the generation of the convex hull that initializes the algorithm. To evaluate the proposed algorithm, the TSPLIB benchmark data-set is modified by adding impassable separators that produce non-Euclidean spaces. The algorithm is demonstrated to outperform the commonly used Nearest Neighbor algorithm in 96% of the cases studied.
    
[^128]: 灵活的、模型无关的方法用于从文本中提取材料数据，使用通用语言模型

    Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models. (arXiv:2302.04914v2 [cond-mat.mtrl-sci] UPDATED)

    [http://arxiv.org/abs/2302.04914](http://arxiv.org/abs/2302.04914)

    本文提出了一个灵活的、模型无关的方法，使用通用语言模型从研究论文中提取材料数据。该方法几乎不需要编码或模型训练，并且在生成的数据库中具有高召回率和几乎完美的精确度。

    

    准确和全面的从研究论文中提取材料数据库对于材料科学和工程至关重要，但需要大量人力来开发。本文提出了一种简单的方法，从研究论文的全文中提取材料数据，适用于快速开发规模适中的数据库。该方法几乎不需要编码，不需要关于提取属性的先验知识或模型训练，且在生成的数据库中具有高召回率和几乎完美的精确度。该方法是完全自动化的，除了一个需要人工辅助的步骤，通常只需要几个小时的人力劳动。该方法基于自然语言处理和大型通用语言模型，但几乎可以与任何此类模型一起使用。这里评估了GPT-3/3.5、bart和DeBERTaV3这些语言模型的性能比较。我们详细分析了该方法在提取体模量数据方面的性能，获得了高达90%的精确度。

    Accurate and comprehensive material databases extracted from research papers are critical for materials science and engineering but require significant human effort to develop. In this paper we present a simple method of extracting materials data from full texts of research papers suitable for quickly developing modest-sized databases. The method requires minimal to no coding, prior knowledge about the extracted property, or model training, and provides high recall and almost perfect precision in the resultant database. The method is fully automated except for one human-assisted step, which typically requires just a few hours of human labor. The method builds on top of natural language processing and large general language models but can work with almost any such model. The language models GPT-3/3.5, bart and DeBERTaV3 are evaluated here for comparison. We provide a detailed detailed analysis of the methods performance in extracting bulk modulus data, obtaining up to 90% precision at 9
    
[^129]: 机器人与人类表征的对齐

    Aligning Robot and Human Representations. (arXiv:2302.01928v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.01928](http://arxiv.org/abs/2302.01928)

    本文研究了机器人与人类表征之间的对齐问题，指出了当前学习方法存在的表示不对齐的困境，并建议应将机器人表征学习方法从实现任务目标的角度转向与人类表征对齐的问题。

    

    为了在世界中行动，机器人依赖于一个凸显任务关键方面的表示：例如，为了搬运咖啡杯，机器人可能会考虑动作效率或杯子的方向。然而，如果我们希望机器人为人类而行动，它们的表示不能只是功能性的，还必须反映人类关心的事物，即它们必须对齐。我们观察到当前的学习方法存在表示不对齐的问题，即机器人学习的表示不能捕捉到人类的表示。我们认为，因为人类是机器人表现的最终评估者，所以我们必须明确地将我们的努力集中在与人类的表征对齐上，而不仅仅是学习下游任务。我们提倡从对表征对齐目标的完成程度的角度研究当前机器人表征学习方法。我们在数学上定义了这个问题，并确定了它的关键要求。

    To act in the world, robots rely on a representation of salient task aspects: for example, to carry a coffee mug, a robot may consider movement efficiency or mug orientation in its behavior. However, if we want robots to act for and with people, their representations must not be just functional but also reflective of what humans care about, i.e. they must be aligned. We observe that current learning approaches suffer from representation misalignment, where the robot's learned representation does not capture the human's representation. We suggest that because humans are the ultimate evaluator of robot performance, we must explicitly focus our efforts on aligning learned representations with humans, in addition to learning the downstream task. We advocate that current representation learning approaches in robotics should be studied from the perspective of how well they accomplish the objective of representation alignment. We mathematically define the problem, identify its key desiderata,
    
[^130]: IM-IAD：工业制造中的图像异常检测基准

    IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing. (arXiv:2301.13359v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.13359](http://arxiv.org/abs/2301.13359)

    该论文提出了一个IM-IAD工业制造中的图像异常检测基准，通过统一的设置评估了16个算法在7个数据集上的性能，旨在解决目前这一领域研究的不规范问题。

    

    图像异常检测（IAD）是工业制造中一项新兴且重要的计算机视觉任务。近年来，许多先进的算法已经被发布，但它们的性能存在很大差异。我们认为，缺乏实际的工业制造设置很可能阻碍了这些方法在实际应用中的发展和使用。据我们所知，IAD方法尚未经过系统评估。因此，这使得研究人员很难分析它们，因为它们是为不同或特殊情况而设计的。为了解决这个问题，我们首先提出了一个统一的工业制造设置来评估这些算法的性能，包括几个方面，如各种监督级别（无监督vs半监督）、少样本学习、持续学习、噪声标签、内存使用和推断速度。此外，我们巧妙地构建了一个完整的图像异常检测基准（IM-IAD），该基准在统一的设置下包括了16个算法和7个主流数据集。

    Image anomaly detection (IAD) is an emerging and vital computer vision task in industrial manufacturing (IM). Recently many advanced algorithms have been published, but their performance deviates greatly. We realize that the lack of actual IM settings most probably hinders the development and usage of these methods in real-world applications. As far as we know, IAD methods are not evaluated systematically. As a result, this makes it difficult for researchers to analyze them because they are designed for different or special cases. To solve this problem, we first propose a uniform IM setting to assess how well these algorithms perform, which includes several aspects, i.e., various levels of supervision (unsupervised vs. semi-supervised), few-shot learning, continual learning, noisy labels, memory usage, and inference speed. Moreover, we skillfully build a comprehensive image anomaly detection benchmark (IM-IAD) that includes 16 algorithms on 7 mainstream datasets with uniform settings. 
    
[^131]: AutoPEFT：用于参数高效微调的自动配置搜索

    AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. (arXiv:2301.12132v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.12132](http://arxiv.org/abs/2301.12132)

    AutoPEFT是一个自动化的PEFT（参数高效微调）配置搜索方法，它能够自动地找到最佳的PEFT模块和体系结构，以优化任务的性能和参数效率。在典型的NLP任务中，AutoPEFT表现出比手动设计更好的性能。

    

    大型预训练语言模型通过专门的微调用于下游NLP任务，但这样的过程可能很昂贵。最近，参数高效微调（PEFT）方法通过更新比完整模型微调（FFT）少得多的参数，实现了强大的任务性能。然而，在PEFT配置方面做出明智的设计选择是不容易的，例如它们的体系结构、可调参数的数量，甚至是PEFT模块插入的图层。因此，目前的手动设计配置很可能在性能效率权衡方面是次优的。受神经架构搜索的进展启发，我们提出了AutoPEFT来自动选择PEFT配置：首先设计具有多个代表性PEFT模块的表达配置搜索空间。然后使用多目标贝叶斯优化进行低成本的设置，从而发现优化任务性能和参数效率的Pareto优化配置。我们在几个典型的NLP任务，包括文本分类、问答和命名实体识别上评估了AutoPEFT，并展示了其优于手动设计基线的性能。

    Large pretrained language models are widely used in downstream NLP tasks via task-specific fine-tuning, but such procedures can be costly. Recently, Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task performance while updating a much smaller number of parameters compared to full model fine-tuning (FFT). However, it is non-trivial to make informed design choices on the PEFT configurations, such as their architecture, the number of tunable parameters, and even the layers in which the PEFT modules are inserted. Consequently, it is highly likely that the current, manually designed configurations are suboptimal in terms of their performance-efficiency trade-off. Inspired by advances in neural architecture search, we propose AutoPEFT for automatic PEFT configuration selection: we first design an expressive configuration search space with multiple representative PEFT modules as building blocks. Using multi-objective Bayesian optimisation in a low-cost setup, we then disc
    
[^132]: 高效的深度强化学习与预测处理近端策略优化

    Efficient Deep Reinforcement Learning with Predictive Processing Proximal Policy Optimization. (arXiv:2211.06236v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.06236](http://arxiv.org/abs/2211.06236)

    该论文介绍了一种名为预测处理近端策略优化（P4O）的深度强化学习方法，通过利用递归神经网络预测自身感觉状态来最小化惊异，从而显著提高累积奖励。

    

    强化学习的进步常常依赖于大量的计算资源，且在样本效率上仍然不高。相比之下，人类大脑能够使用有限的资源有效地学习控制策略。这引发了一个问题，即是否可以借鉴神经科学的见解来改进当前的强化学习方法。预测处理是一个流行的理论框架，它认为人类大脑主动寻求最小化惊异。我们展示了能够预测自身感觉状态的递归神经网络可以被利用来最小化惊异，从而在累积奖励上取得巨大的收益。具体而言，我们提出了预测处理近端策略优化（P4O）智能体；它是一个将预测处理应用到基于递归的PPO算法的演员批判强化学习智能体，通过将世界模型集成到其隐藏状态中。即使没有超参数调整，P4O与基线递归变体相比，也能显著提高性能。

    Advances in reinforcement learning (RL) often rely on massive compute resources and remain notoriously sample inefficient. In contrast, the human brain is able to efficiently learn effective control strategies using limited resources. This raises the question whether insights from neuroscience can be used to improve current RL methods. Predictive processing is a popular theoretical framework which maintains that the human brain is actively seeking to minimize surprise. We show that recurrent neural networks which predict their own sensory states can be leveraged to minimise surprise, yielding substantial gains in cumulative reward. Specifically, we present the Predictive Processing Proximal Policy Optimization (P4O) agent; an actor-critic reinforcement learning agent that applies predictive processing to a recurrent variant of the PPO algorithm by integrating a world model in its hidden state. Even without hyperparameter tuning, P4O significantly outperforms a baseline recurrent varian
    
[^133]: 3D点云分类的无盒子攻击

    No-Box Attacks on 3D Point Cloud Classification. (arXiv:2210.14164v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.14164](http://arxiv.org/abs/2210.14164)

    该论文介绍了一种新的方法，可以在不访问目标DNN模型的情况下预测3D点云中的对抗点，提供了无盒子攻击的新视角。

    

    对于基于深度神经网络（DNN）的各种输入信号的分析，对抗攻击构成了严重挑战。在3D点云的情况下，已经开发出了一些方法来识别在网络决策中起关键作用的点，而这些方法在生成现有的对抗攻击中变得至关重要。例如，显著性图方法是一种流行的方法，用于识别对抗攻击会显著影响网络决策的点。通常，识别对抗点的方法依赖于对目标DNN模型的访问，以确定哪些点对模型的决策至关重要。本文旨在对这个问题提供一种新的视角，在不访问目标DNN模型的情况下预测对抗点，这被称为“无盒子”攻击。为此，我们定义了14个点云特征，并使用多元线性回归来检查这些特征是否可以用于预测对抗点，以及哪些特征对预测最为重要。

    Adversarial attacks pose serious challenges for deep neural network (DNN)-based analysis of various input signals. In the case of 3D point clouds, methods have been developed to identify points that play a key role in network decision, and these become crucial in generating existing adversarial attacks. For example, a saliency map approach is a popular method for identifying adversarial drop points, whose removal would significantly impact the network decision. Generally, methods for identifying adversarial points rely on the access to the DNN model itself to determine which points are critically important for the model's decision. This paper aims to provide a novel viewpoint on this problem, where adversarial points can be predicted without access to the target DNN model, which is referred to as a ``no-box'' attack. To this end, we define 14 point cloud features and use multiple linear regression to examine whether these features can be used for adversarial point prediction, and which
    
[^134]: 直比代数

    Proportional algebras. (arXiv:2210.01751v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.01751](http://arxiv.org/abs/2210.01751)

    本文引入了直比代数，探讨了保持比拟比例的函数的数学性质，将其与直比同态、同余和直比函子联系起来，为模拟比例和模拟推理的数学理论提供了进一步理解。

    

    比拟比例是形式为"$a$对$b$，正如$c$对$d$"的表达式，是模拟推理的核心，而模拟推理又是人工智能的核心。本文引入了直比代数作为一种带有4元比拟比例关系$a:b::c:d$满足一组适当公理的代数结构。在人工智能中，保持比拟比例的函数已经被证明具有实际意义，而研究它们的数学性质对于理解比例是至关重要的。因此，我们引入了直比同态及其关联的同余和直比函子，并展示了它们之间的密切关联。从更广泛的意义上说，本文是迈向模拟比例和模拟推理的数学理论的进一步步骤。

    Analogical proportions are expressions of the form "$a$ is to $b$ what $c$ is to $d$" at the core of analogical reasoning which itself is at the core of artificial intelligence. This paper introduces proportional algebras as algebras endowed with a 4-ary analogical proportion relation $a:b::c:d$ satisfying a suitable set of axioms. Functions preserving analogical proportions have already proven to be of practical interest in artificial intelligence and studying their mathematical properties is essential for understanding proportions. We therefore introduce proportional homomorphisms and their associated congruences and proportional functors, and show that they are closely related notions. In a broader sense, this paper is a further step towards a mathematical theory of analogical proportions and analogical reasoning in general.
    
[^135]: 关于敏感性与准确性在上下文学习中的关系

    On the Relation between Sensitivity and Accuracy in In-context Learning. (arXiv:2209.07661v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.07661](http://arxiv.org/abs/2209.07661)

    在上下文学习中，我们发现ICL对多种扰动类型具有敏感性，标签偏差导致过去的研究低估了ICL的敏感性。同时，我们观察到ICL的敏感性和准确性之间呈现负相关关系。基于这些发现，我们提出了一种少样本选择性预测方法SenSel，它在放弃敏感预测决策上取得了优于常用基准方法的结果。

    

    上下文学习 (In-context learning, ICL) 在实际场景中常常受到提示的过度敏感性的影响，导致其在现实世界中不可靠。我们研究了ICL对多种扰动类型的敏感性。首先，我们发现标签偏差掩盖了真实的敏感性，因此之前的研究可能大大低估了ICL的敏感性。其次，我们观察到ICL的敏感性与准确性之间存在强烈的负相关关系：对扰动敏感的预测更不容易正确。在这些发现的基础上，我们提出了一种称为SenSel的少样本选择性预测方法，该方法避免了对敏感预测的使用。在十个分类数据集上的实验证明，SenSel在放弃预测决策上始终优于两种常用的基于置信度和基于熵的基准方法。

    In-context learning (ICL) suffers from oversensitivity to the prompt, making it unreliable in real-world scenarios. We study the sensitivity of ICL with respect to multiple perturbation types. First, we find that label bias obscures the true sensitivity, and therefore prior work may have significantly underestimated ICL sensitivity. Second, we observe a strong negative correlation between ICL sensitivity and accuracy: predictions sensitive to perturbations are less likely to be correct. Motivated by these findings, we propose \textsc{SenSel}, a few-shot selective prediction method that abstains from sensitive predictions. Experiments on ten classification datasets show that \textsc{SenSel} consistently outperforms two commonly used confidence-based and entropy-based baselines on abstention decisions.
    
[^136]: SSL-WM：一种用于经过自监督学习预训练的编码器的黑盒水印方法

    SSL-WM: A Black-Box Watermarking Approach for Encoders Pre-trained by Self-supervised Learning. (arXiv:2209.03563v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2209.03563](http://arxiv.org/abs/2209.03563)

    SSL-WM是一种用于验证自监督学习模型所有权的黑盒水印方法，对于下游任务多样且未知的情况下也适用。

    

    近年来，自监督学习（SSL）取得了巨大的成功，在计算机视觉（CV）和自然语言处理（NLP）领域被广泛用于促进各种下游任务。然而，攻击者可能窃取这些SSL模型并将其商业化，因此验证SSL模型的所有权变得至关重要。现有的大多数所有权保护解决方案（例如基于后门的水印）都是为监督学习模型设计的，不能直接使用，因为它们要求在嵌入水印时已知和可用的是模型的下游任务和目标标签，在SSL领域中这并不总是可能的。为了解决这个问题，特别是在嵌入水印期间下游任务多样且不知道的情况下，我们提出了一种名为SSL-WM的新型黑盒水印解决方案，用于验证SSL模型的所有权。SSL-WM将受保护的编码器的带水印输入映射到不变表示s中。

    Recent years have witnessed tremendous success in Self-Supervised Learning (SSL), which has been widely utilized to facilitate various downstream tasks in Computer Vision (CV) and Natural Language Processing (NLP) domains. However, attackers may steal such SSL models and commercialize them for profit, making it crucial to verify the ownership of the SSL models. Most existing ownership protection solutions (e.g., backdoor-based watermarks) are designed for supervised learning models and cannot be used directly since they require that the models' downstream tasks and target labels be known and available during watermark embedding, which is not always possible in the domain of SSL. To address such a problem, especially when downstream tasks are diverse and unknown during watermark embedding, we propose a novel black-box watermarking solution, named SSL-WM, for verifying the ownership of SSL models. SSL-WM maps watermarked inputs of the protected encoders into an invariant representation s
    
[^137]: 写下来吧：正式合同缓解多智能体强化学习中的社会困境

    Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL. (arXiv:2208.10469v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.10469](http://arxiv.org/abs/2208.10469)

    本研究通过引入正式合同的概念，解决了多智能体强化学习中个体激励和集体激励分歧导致的次优行为问题。理论和实证结果表明，通过在马尔可夫博弈中引入有约束的状态依赖奖励转移，实现了所有可观察马尔可夫博弈的子博弈完美均衡表现出社会最优行为，并提升了算法的社会性能。

    

    多智能体强化学习（MARL）是训练在共同环境中独立行动的自动化系统的强大工具。然而，当个体激励和集体激励出现分歧时，它可能导致次优行为。人类在解决这些社会困境方面具有非凡的能力。在MARL中复制这种合作行为对于自私的智能体来说是一个未解决的问题。在这项工作中，我们借鉴了经济学中正式合同的思想，以克服MARL中智能体之间的激励分歧。我们提出了一种对马尔可夫博弈进行增强的方法，智能体自愿同意在预先规定的条件下进行有约束的状态依赖奖励转移。我们的贡献是理论的和实证的。首先，我们展示了这种增强使得所有完全可观察马尔可夫博弈的子博弈完美均衡都表现出社会最优行为，只要合同空间足够丰富。接下来，我们通过展示最先进的强化学习算法在增强后的MARL中表现出更好的社会性能来补充我们的博弈论分析。

    Multi-agent reinforcement learning (MARL) is a powerful tool for training automated systems acting independently in a common environment. However, it can lead to sub-optimal behavior when individual incentives and group incentives diverge. Humans are remarkably capable at solving these social dilemmas. It is an open problem in MARL to replicate such cooperative behaviors in selfish agents. In this work, we draw upon the idea of formal contracting from economics to overcome diverging incentives between agents in MARL. We propose an augmentation to a Markov game where agents voluntarily agree to binding state-dependent transfers of reward, under pre-specified conditions. Our contributions are theoretical and empirical. First, we show that this augmentation makes all subgame-perfect equilibria of all fully observed Markov games exhibit socially optimal behavior, given a sufficiently rich space of contracts. Next, we complement our game-theoretic analysis by showing that state-of-the-art R
    
[^138]: 使用基于自同构轨道的深度神经网络对整个生命起源树中的蛋白互作组织多样性进行表征

    DeepAutoPIN: An automorphism orbits based deep neural network for characterizing the organizational diversity of protein interactomes across the tree of life. (arXiv:2203.00999v2 [q-bio.MN] UPDATED)

    [http://arxiv.org/abs/2203.00999](http://arxiv.org/abs/2203.00999)

    本研究利用基于自同构轨道的深度神经网络研究了整个生命起源树中的蛋白互作组织多样性，并发现不同生命域和门属网络的轨道使用概况存在明显差异。这一研究结果对于了解蛋白质相互作用网络的演化具有重要意义。

    

    在极不同的环境中繁盛的生命形式的巨大多样性涉及到相互作用的组分蛋白之间的复杂相互作用。然而，表征蛋白质相互作用网络（PINs）在整个生命起源树中的演化的组织原则目前还大部分不明确。本文研究了属于16个门的4,738个PINs，以发现门特有的结构特征，并检查网络拓扑结构上是否存在一些演化约束。我们利用了网络节点的位置信息，通过对大小为2-5的图形中出现的自同构轨道的频率进行归一化来研究。我们报告，生命的三个域所属网络的轨道使用概况（OUPs）不仅在域层面上有明显差异，而且在门的规模上也有明显差异。通过整合与蛋白家族、结构域、亚细胞定位、基因本体论和通路相关的信息，我们的结果表明，连接模式可能是PINs在进化过程中的一个重要影响因素。

    The enormous diversity of life forms thriving in drastically different environmental milieus involves a complex interplay among constituent proteins interacting with each other. However, the organizational principles characterizing the evolution of protein interaction networks (PINs) across the tree of life are largely unknown. Here we study 4,738 PINs belonging to 16 phyla to discover phyla-specific architectural features and examine if there are some evolutionary constraints imposed on the networks' topologies. We utilized positional information of a network's nodes by normalizing the frequencies of automorphism orbits appearing in graphlets of sizes 2-5. We report that orbit usage profiles (OUPs) of networks belonging to the three domains of life are contrastingly different not only at the domain level but also at the scale of phyla. Integrating the information related to protein families, domains, subcellular location, gene ontology, and pathways, our results indicate that wiring p
    
[^139]: 计算机视觉中基于自监督学习的时序方法

    Computer Vision Self-supervised Learning Methods on Time Series. (arXiv:2109.00783v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.00783](http://arxiv.org/abs/2109.00783)

    该研究评估了计算机视觉自监督学习框架在时间序列上的效果，并且提出了一种改进方法，通过改进协方差项和添加迭代归一化层，加速了模型的收敛。

    

    自监督学习（SSL）在计算机视觉领域取得了巨大成功。目前主流的计算机视觉自监督学习框架大多基于Siamese网络架构。这些方法通常依赖于精心设计的损失函数和训练设置，以避免特征崩溃。本研究评估了这些计算机视觉自监督学习框架在不同模态（即时间序列）上的效果。我们在UCR和UEA档案上进行了实验证明，计算机视觉自监督学习框架在时间序列上同样有效。此外，我们提出了一种改进最近提出的VICReg方法的新方法。我们改进了VICReg中提出的一个“协方差”项，同时在架构的头部增加了一个迭代归一化层，加速了模型的收敛。

    Self-supervised learning (SSL) has had great success in both computer vision. Most of the current mainstream computer vision SSL frameworks are based on Siamese network architecture. These approaches often rely on cleverly crafted loss functions and training setups to avoid feature collapse. In this study, we evaluate if those computer-vision SSL frameworks are also effective on a different modality (\textit{i.e.,} time series). The effectiveness is experimented and evaluated on the UCR and UEA archives, and we show that the computer vision SSL frameworks can be effective even for time series. In addition, we propose a new method that improves on the recently proposed VICReg method. Our method improves on a \textit{covariance} term proposed in VICReg, and in addition we augment the head of the architecture by an iterative normalization layer that accelerates the convergence of the model.
    
[^140]: 单头胜过双头：命题明确Horn遗忘的多项式限制

    One head is better than two: a polynomial restriction for propositional definite Horn forgetting. (arXiv:2009.07497v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2009.07497](http://arxiv.org/abs/2009.07497)

    本研究首先对单头等价性进行了语义化的研究，给出了必要条件；其次提出了一种不完全算法，用于将公式转换为单头形式，从而实现在多项式时间内进行遗忘操作。

    

    逻辑遗忘即使在命题Horn公式的简单情况下也是p完全的，可能会指数级增加其大小。一种遗忘的方式是用包含变量要遗忘的每个子句替换它们的头部变量。在单头的情况下，这需要多项式时间：每个变量最多是一个子句的头部。有些公式不是单头的，但可以通过简化来实现。它们是单头等价的。本文的第一项贡献是对单头等价性进行语义特征化的研究。给出了两个必要条件。当公式不等价时，它们是充分条件：它只会使两个变量集等价，如果它们也等价于它们的交集。所有非循环公式都是不等价的。本文的第二个贡献是一个用于将公式转换为单头的不完全算法。如果成功，遗忘将能够在多项式时间内进行，并产生一个多项式大小的公式。

    Logical forgetting is \np-complete even in the simple case of propositional Horn formulae, and may exponentially increase their size. A way to forget is to replace each variable to forget with the body of each clause whose head is the variable. It takes polynomial time in the single-head case: each variable is at most the head of a clause. Some formulae are not single-head but can be made so to simplify forgetting. They are single-head equivalent. The first contribution of this article is the study of a semantical characterization of single-head equivalence. Two necessary conditions are given. They are sufficient when the formula is inequivalent: it makes two sets of variables equivalent only if they are also equivalent to their intersection. All acyclic formulae are inequivalent. The second contribution of this article is an incomplete algorithm for turning a formula single-head. In case of success, forgetting becomes possible in polynomial time and produces a polynomial-size formula,
    
[^141]: 竞赛解决方案的查询复杂度

    Query Complexity of Tournament Solutions. (arXiv:1611.06189v4 [cs.DS] UPDATED)

    [http://arxiv.org/abs/1611.06189](http://arxiv.org/abs/1611.06189)

    本文研究了竞赛解决方案的查询复杂度问题，通过尽可能少的查询边缘的方式找到最佳顶点集合。

    

    在社会选择理论中，研究如何找到竞赛的最佳顶点集合是一个既定的问题。然而，在许多应用中，例如从给定的药物集合中选择最佳药物，只有隐式给出竞赛边缘，并且了解边缘的方向是昂贵的。在这种情况下，我们希望用尽可能少的查询边缘的方式找到最佳顶点集合。本文精确研究了常用竞赛解决方案的这个问题：通过查询尽可能少的边缘，给定一个竞赛T的边缘访问，找到$f(T)$的值，其中$f$是竞赛解决方案。我们首先展示了在竞赛中找到由康多塞不败者组成的集合可以通过查询$2n-\lfl

    A directed graph where there is exactly one edge between every pair of vertices is called a {\em tournament}. Finding the "best" set of vertices of a tournament is a well studied problem in social choice theory. A {\em tournament solution} takes a tournament as input and outputs a subset of vertices of the input tournament. However, in many applications, for example, choosing the best set of drugs from a given set of drugs, the edges of the tournament are given only implicitly and knowing the orientation of an edge is costly. In such scenarios, we would like to know the best set of vertices (according to some tournament solution) by "querying" as few edges as possible. We, in this paper, precisely study this problem for commonly used tournament solutions: given an oracle access to the edges of a tournament T, find $f(T)$ by querying as few edges as possible, for a tournament solution f. We first show that the set of Condorcet non-losers in a tournament can be found by querying $2n-\lfl
    

