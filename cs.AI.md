# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Seeing is not always believing: A Quantitative Study on Human Perception of AI-Generated Images.](http://arxiv.org/abs/2304.13023) | 本研究揭示了人类无法辨别AI生成的假照片和真实照片，这一点受个人背景的影响并不显著。 |
| [^2] | [Answering Questions by Meta-Reasoning over Multiple Chains of Thought.](http://arxiv.org/abs/2304.13007) | 本论文提出了基于元推理的Multi-Chain Reasoning (MCR)方法，该方法检查多个推理链，混合它们之间的信息并选择最相关的事实，从而超越多链思维，解决多跳QA问题。 实验结果表明MCR胜过多个强基线，解释质量高。 |
| [^3] | [Centralized control for multi-agent RL in a complex Real-Time-Strategy game.](http://arxiv.org/abs/2304.13004) | 本文介绍了在Lux AI v2 Kaggle比赛中使用集中式方法来训练RL智能体，在复杂的实时战略游戏中实现多智能体强化学习，并成功提高了性能。 |
| [^4] | [On the Generalization of Learned Structured Representations.](http://arxiv.org/abs/2304.13001) | 本论文研究了深度学习中学习结构化表示的泛化问题。通过提出能够学习局部和全局依赖关系的结构化表示方法，并在各种下游任务上进行实验验证，证明了该方法在图像分类和物体检测等任务上的有效性。 |
| [^5] | [Segment anything, from space?.](http://arxiv.org/abs/2304.13000) | 最近开发的Segment Anything Model（SAM）模型可以基于简单的输入提示（如一个或多个点、边界框或掩码）有效分割自然图像中的对象，对视觉研究人员具有重要意义。此项研究探讨SAM在空中图像问题上的卓越性能，并在多项基准任务上进行了验证，表现良好。 |
| [^6] | [AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head.](http://arxiv.org/abs/2304.12995) | AudioGPT是一种多模式人工智能系统，能够处理复杂的音频信息并支持口语对话，其实验结果表明其在处理语音、音乐、声音和人头像方面有着很强的能力。 |
| [^7] | [Measuring Massive Multitask Chinese Understanding.](http://arxiv.org/abs/2304.12986) | 本研究提出了一项测试，以衡量大型中文语言模型的多任务准确性，测试涵盖医学、法律、心理学和教育四个主要领域，结果表明所有模型在法律领域中表现都很差，建议研究人员应该开发更加多样化和均衡的多任务中文理解模型。 |
| [^8] | [Rubik's Optical Neural Networks: Multi-task Learning with Physics-aware Rotation Architecture.](http://arxiv.org/abs/2304.12985) | RubikONNs利用光学系统的物理特性，通过旋转硬件编码多个前馈函数，为ONNs实现高效的多任务学习提供了新的策略。 |
| [^9] | [A Closer Look at Reward Decomposition for High-Level Robotic Explanations.](http://arxiv.org/abs/2304.12958) | 本论文提出了一种将奖励分解与抽象动作空间相结合的学习框架，可以提供基于对象属性的明确高层次解释，避免了解释机器人行为的复杂性。 |
| [^10] | [Generating robust counterfactual explanations.](http://arxiv.org/abs/2304.12943) | 本论文提出了一个新的反事实解释框架CROCO，能够生成强鲁棒性的解释，同时有效地平衡了鲁棒性和解释示例的接近度之间的权衡。 |
| [^11] | [SALSA: Simulated Annealing based Loop-Ordering Scheduler for DNN Accelerators.](http://arxiv.org/abs/2304.12931) | SALSA是一种用于DNN加速器的快速双引擎调度器，利用新策略将穷举搜索和模拟退火相结合，能够在加速搜索的同时找到更低能量调度。 |
| [^12] | [Blockchain-based Federated Learning with Secure Aggregation in Trusted Execution Environment for Internet-of-Things.](http://arxiv.org/abs/2304.12889) | 本文提出了一种基于区块链和可信执行环境的联邦学习框架，用于安全聚合本地模型，保证全局模型的真实性和完整性，适用于工业物联网。 |
| [^13] | [Out-of-distribution Evidence-aware Fake News Detection via Dual Adversarial Debiasing.](http://arxiv.org/abs/2304.12888) | 该论文提出了一种新颖的双重对抗学习方法，通过在模型中加入去偏置鉴别器，旨在训练模型更好地进行越界证据感知假新闻检测，有效减轻新闻和证据内容偏差的影响。 |
| [^14] | [Unsupervised Domain Transfer for Science: Exploring Deep Learning Methods for Translation between LArTPC Detector Simulations with Differing Response Models.](http://arxiv.org/abs/2304.12858) | 本文提出了一种基于最近在成对图像转换技术上的进展的，完全无监督的方法来减少LArTPC探测器模拟和真实数据之间的系统差异，以提高模型性能。 |
| [^15] | [A New Information Theory of Certainty for Machine Learning.](http://arxiv.org/abs/2304.12833) | 该论文提出了一种新的信息理论概念 troenpy 来量化底层分布的确定性，用于机器学习中文档分类和序列数据权重方案，并定义了量子 troenpy 量化量子系统确定性。 |
| [^16] | [A optimization framework for herbal prescription planning based on deep reinforcement learning.](http://arxiv.org/abs/2304.12828) | 基于深度强化学习的中药处方规划模型 PrescDRL 关注长期疗效，对慢性疾病治疗具有更好的效果，评估表明其较医生治疗效果提高了117%和153%。 |
| [^17] | [Investigations into Proof Structures.](http://arxiv.org/abs/2304.12827) | 介绍了一种新的形式主义来操作和分析证明，用于生成更短的证明和减少搜索工作量。 |
| [^18] | [GraphVF: Controllable Protein-Specific 3D Molecule Generation with Variational Flow.](http://arxiv.org/abs/2304.12825) | GraphVF是一种新的分子生成框架，它将2D拓扑和3D几何结合起来，用于可控生成具有结合能力的3D分子，是第一个可以生成结合能力的3D分子并定制其物化特性的方法。 |
| [^19] | [Can Voice Assistants Sound Cute? Towards a Model of Kawaii Vocalics.](http://arxiv.org/abs/2304.12809) | 本文研究声音是否可以具备可爱性，并通过探索可爱的语音特质，即可爱的声音学来实现。通过对不同年龄段和不同特征的语音感受研究，发现可爱性与性别、年龄、流利度和人工性等方面有关。提出了一个可爱嗓音的模型，需要进行声音质量、认知评估、行为反应和情感报告的研究来验证。 |
| [^20] | [Loss and Reward Weighing for increased learning in Distributed Reinforcement Learning.](http://arxiv.org/abs/2304.12778) | 本文提出了两种分布式强化学习方法，奖励加权和损失加权梯度合并，以更好地提高分布式代理的学习效果。 |
| [^21] | [State Spaces Aren't Enough: Machine Translation Needs Attention.](http://arxiv.org/abs/2304.12776) | S4模型在机器翻译任务上与变压器模型相比存在四个BLEU分数点的差距，需要注意力机制来弥补其无法在单个隐藏状态中总结完整的源句子的缺陷。 |
| [^22] | [Empowering Wildlife Guardians: An Equitable Digital Stewardship and Reward System for Biodiversity Conservation using Deep Learning and 3/4G Camera Traps.](http://arxiv.org/abs/2304.12703) | 本研究提出一种基于“物种货币”的数字管理和奖励系统，以支持野生动物守护者的保护工作，并通过深度学习算法分析数据来监督和保护它们。 |
| [^23] | [On the Computation of Meaning, Language Models and Incomprehensible Horrors.](http://arxiv.org/abs/2304.12686) | 本文提出了一个全面的机械化解释，涉及意义、交流和符号出现的现象，揭示了当前的语言模型并不具备与人类相同的意义理解，作者建议模拟人类情感并优化模型以构建弱表示形式来解决这个问题。 |
| [^24] | [Exploring the Mutual Influence between Self-Supervised Single-Frame and Multi-Frame Depth Estimation.](http://arxiv.org/abs/2304.12685) | 文章介绍了一种新的自监督训练框架，旨在充分利用单帧深度和多帧深度方法之间的相互影响。通过引入一个像素逐像素自适应深度采样模块，以单帧深度为指导来训练多帧模型，并利用最小重投影为基础的蒸馏方法来优化单帧深度的模型。 |
| [^25] | [Disagreement amongst counterfactual explanations: How transparency can be deceptive.](http://arxiv.org/abs/2304.12667) | 反事实解释的多样性会对利益相关者之间产生潜在分歧问题，并可能掩盖或淡化原始模型中的敏感和不公平特征，因此需要更好地理解和管理这种差异，以防止欺骗，并确保XAI的道德使用。 |
| [^26] | [CoDi: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis.](http://arxiv.org/abs/2304.12654) | CoDi 方法使用两个共同演化的对比扩散模型单独处理离散和连续变量并相互条件化，同时引入对比学习方法进行进一步的绑定，展现了在真实世界的表格数据集上的有效性。 |
| [^27] | [Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention.](http://arxiv.org/abs/2304.12653) | 本文提出了一种新的基于图注意力的部分可观察均场多智能体强化学习算法，使用图注意力来捕获周围邻居智能体的特征信息，可以提高大规模多智能体环境中部分可观察MARL的性能。 |
| [^28] | [Q-based Equilibria.](http://arxiv.org/abs/2304.12647) | 该论文研究了基于Q的策略规则族中的均衡偏差（或 Qb-equilibria），即Q值在不同监测技术下的效果。 |
| [^29] | [Adaptive Path-Memory Network for Temporal Knowledge Graph Reasoning.](http://arxiv.org/abs/2304.12604) | 该论文提出了一种新的框架DaeMon，用于自适应地模拟查询主题和每个对象候选之间的时间路径信息，从而解决了TKG实体数量庞大和实时增加的问题。该方法不依赖于实体表达，并使用注意机制进行记忆的选择性整合。 |
| [^30] | [MMRDN: Consistent Representation for Multi-View Manipulation Relationship Detection in Object-Stacked Scenes.](http://arxiv.org/abs/2304.12592) | 本文提出了一种用于多视角物体堆叠场景中操作关系检测的多视角MRD网络框架，能够通过2D和3D多视角数据学习一致表示，进而指导机器人按正确的顺序抓取物体。 |
| [^31] | [Unsupervised Synthetic Image Refinement via Contrastive Learning and Consistent Semantic and Structure Constraints.](http://arxiv.org/abs/2304.12591) | 本文采用对比学习和一致的语义和结构约束来减少合成和细化图像之间的语义失真，进一步提高了性能。 |
| [^32] | [Real-time Safety Assessment of Dynamic Systems in Non-stationary Environments: A Review of Methods and Techniques.](http://arxiv.org/abs/2304.12583) | 本文综述了非平稳环境下动态系统实时安全评估的方法和技术，包括在线主动学习、在线半监督学习、在线迁移学习和在线异常检测，并讨论了未来的研究方向。 |
| [^33] | [Harnessing Deep Learning and HPC Kernels via High-Level Loop and Tensor Abstractions on CPU Architectures.](http://arxiv.org/abs/2304.12576) | 该论文介绍了一种新的方法来开发适用于现代CPU体系结构的高效、可移植的深度学习和高性能计算内核，使用高级循环和张量抽象。 |
| [^34] | [Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks.](http://arxiv.org/abs/2304.12567) | 本文研究了如何通过增加辅助任务的数量和代理网络的大小，提高表示学习的效果。同时，本文还提出了基于后继度量的新型辅助任务家族 Proto-Value 网络。 |
| [^35] | [A Preliminary Evaluation of ChatGPT in Requirements Information Retrieval.](http://arxiv.org/abs/2304.12562) | ChatGPT具备在需求信息检索任务中执行的能力，并且在零-shot设置下获得了可比较或更好的结果。 |
| [^36] | [Combining Adversaries with Anti-adversaries in Training.](http://arxiv.org/abs/2304.12550) | 该论文研究了在对抗训练中，通过结合对手和反对手(带有反对手扰动的样本)可以更有效地提高深度神经网络的公平性、鲁棒性和泛化性，在一些特定的学习场景中表现出更好的性能。 |
| [^37] | [Object Semantics Give Us the Depth We Need: Multi-task Approach to Aerial Depth Completion.](http://arxiv.org/abs/2304.12542) | 该论文提出了一种利用对象检测中的语义信息来提高航空深度补全效果的方法，通过基于编码器的多任务学习模型将两个任务在一个模型中执行一次，实现了与KITTI深度补全基准的最先进性能。 |
| [^38] | [SEA: A Spatially Explicit Architecture for Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2304.12532) | 论文提出了一种针对多智能体问题的空间信息提取结构，能有效地共享邻域和全局信息，并能够处理具有可变代理数量的问题。实验结果表明该结构可以提升现有的强化学习算法的效果。 |
| [^39] | [Semantic Compression With Large Language Models.](http://arxiv.org/abs/2304.12512) | 本研究使用大型语言模型（LLMs）进行近似压缩研究。具体实验是以GPT-3.5和GPT-4为基础进行的，旨在探索近似压缩的可行性。 |
| [^40] | [Fulfilling Formal Specifications ASAP by Model-free Reinforcement Learning.](http://arxiv.org/abs/2304.12508) | 本文提出了 ASAP-Phi框架，利用分段奖励函数来为未满足规范的轨迹分配定量的语义奖励，并为其余轨迹分配高的常数奖励，使用基于演员-评论家的算法训练代理来尽快实现规范。 |
| [^41] | [Evaluating Adversarial Robustness on Document Image Classification.](http://arxiv.org/abs/2304.12486) | 本文对文档图像分类任务中的对抗性攻击进行了研究和评估，通过对ResNet50和EfficientNetB0模型架构进行对抗训练、JPEG输入压缩和灰度输入转换等方法，提高了模型的鲁棒性。 |
| [^42] | [DocParser: End-to-end OCR-free Information Extraction from Visually Rich Documents.](http://arxiv.org/abs/2304.12484) | DocParser是一种无OCR的端到端信息提取模型，可更好地提取判别性字符特征，并在视觉丰富的文档数据集上实现最新的结果。 |
| [^43] | [Artificial General Intelligence (AGI) for Education.](http://arxiv.org/abs/2304.12479) | AGI技术具有革命教育领域潜力，可以建立e-learning平台、教育协作工具等，弥补传统AI模型因受限于数据和人际交互限制而无法满足教育需求的不足。 |
| [^44] | [On Dynamic Program Decompositions of Static Risk Measures.](http://arxiv.org/abs/2304.12477) | 本文证明了现有的CVaR和EVaR风险度量动态分解是真实风险值的严格高估计，然而VaR存在精确的动态分解。 |
| [^45] | [Model-Free Learning and Optimal Policy Design in Multi-Agent MDPs Under Probabilistic Agent Dropout.](http://arxiv.org/abs/2304.12458) | 本文研究了多智能体MDP中基于概率代理掉线的情况，并提出了一种无模型算法，能够消除掉线情况需要枚举计算的限制，从而实现计算后掉线系统的最优策略设计。 |
| [^46] | [Sample-Efficient and Surrogate-Based Design Optimization of Underwater Vehicle Hulls.](http://arxiv.org/abs/2304.12420) | 该论文使用了高效的样本集优化和基于代理的方法来设计水下航行器船体，其中代理模型显著提高了计算效率，使优化更加快速准确。 |
| [^47] | [End-to-End Lidar-Camera Self-Calibration for Autonomous Vehicles.](http://arxiv.org/abs/2304.12412) | 本文提出了一种名为CaLiCa的端到端深度自标定网络，用于联合自动校准针孔相机和激光雷达的固有和外参参数以确保车辆多模式感知传感器的校准质量，同时采用孪生结构以达到领域共享特征的目的。 |
| [^48] | [PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques.](http://arxiv.org/abs/2304.12410) | 本文提出了PEFT-Ref参考架构，标准化了不同PEFT技术共享的方面，隔离了差异到特定位置和交互中，模块化的视图有助于比较不同技术及其效率和任务性能，并有助于更好地理解PEFT的基本原理。 |
| [^49] | [On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research.](http://arxiv.org/abs/2304.12397) | 本文讨论使用黑盒API进行毒性评估的挑战，发现依赖继承的自动毒性评分可能导致不准确的结果，建议采用更加结构化的方法评估毒性随时间变化的模型和方法。 |
| [^50] | [Extreme Classification for Answer Type Prediction in Question Answering.](http://arxiv.org/abs/2304.12395) | 本文提出了使用Transformer模型（XBERT）进行极端多标签分类，通过将KG类型基于问题文本使用结构和语义特征进行聚类，以提高问题回答（QA）系统中语义答案类型预测（SMART）任务的性能，并获得最先进的结果。 |
| [^51] | [Virus2Vec: Viral Sequence Classification Using Machine Learning.](http://arxiv.org/abs/2304.12328) | Virus2Vec 是一种机器学习方法，使用特征向量表示病毒序列，能够识别病毒宿主，其在病毒宿主预测上比当前最先进方法提高了多达16％的精度。 |
| [^52] | [Dependence of Physiochemical Features on Marine Chlorophyll Analysis with Learning Techniques.](http://arxiv.org/abs/2304.12325) | 研究发现，海洋中叶绿素的生长与物理化学成分如铁、硝酸盐、磷酸盐、pH值、盐度等的最佳浓度有关，可以用机器学习预测海洋叶绿素。 |
| [^53] | [USA-Net: Unified Semantic and Affordance Representations for Robot Memory.](http://arxiv.org/abs/2304.12164) | 本文提出了一种名为USA-Net的方法，用于构建可微分地图的世界表示，编码了场景的语义和空间作用，从而使机器人能够按照自然语言指令移动。该系统是第一个端到端可微的计划系统，可以处理开放式自然语言命令而无需进行任何特定任务的手动工程。 |
| [^54] | [Towards a Praxis for Intercultural Ethics in Explainable AI.](http://arxiv.org/abs/2304.11861) | 该论文介绍了一种可解释人工智能的跨文化伦理实践方法，研究了文化差异如何影响技术的采纳和使用，并探讨了解释技术概念如何帮助那些在社会和文化多样性地区中具有不同需求的用户。 |
| [^55] | [Hierarchical Diffusion Autoencoders and Disentangled Image Manipulation.](http://arxiv.org/abs/2304.11829) | 本文提出了一种分级扩散自编码器 (HDAE) ，利用细粒度到抽象和低级到高级的特征层次结构来构建扩散模型的分层潜在空间。该方法更全面地编码了不同抽象层次的语义，并提供了一种基于截断特征的分解图像操作方法。 |
| [^56] | [Ensuring Trustworthy Medical Artificial Intelligencethrough Ethical and Philosophical Principles.](http://arxiv.org/abs/2304.11530) | 本文讨论了人工智能在医疗保健中的应用和考虑伦理和哲学原则以确保可靠的人工智能工具的重要性。人工智能在医疗中带来了更多挑战，必须解决偏见、透明度、自主权、责任和问责制等问题，作者提出了可能的解决办法。 |
| [^57] | [Boosting Theory-of-Mind Performance in Large Language Models via Prompting.](http://arxiv.org/abs/2304.11490) | 本研究通过提示提高大型语言模型（LLMs）在心智理论（ToM）任务上的表现，证明了上下文学习可以提升LLMs在复杂推理特别是ToM任务中的表现。 |
| [^58] | [Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method.](http://arxiv.org/abs/2304.11171) | 本文提出了一种基于颗粒球计算的自适应多粒度表示和计算方法，能够提高机器学习的效率、鲁棒性和可解释性。 |
| [^59] | [Reinforcement Learning Approaches for Traffic Signal Control under Missing Data.](http://arxiv.org/abs/2304.10722) | 本文提出了在交通路网中缺少传感器的情况下，使用强化学习方法通过补充流量状态或状态和动作来实现自适应控制和条件融合。 |
| [^60] | [A Theory on Adam Instability in Large-Scale Machine Learning.](http://arxiv.org/abs/2304.09871) | Adam优化算法在大批量的训练下容易出现不稳定现象，并导致训练异常，作者提出了该现象的理论解释。 |
| [^61] | [Improving Autoregressive NLP Tasks via Modular Linearized Attention.](http://arxiv.org/abs/2304.08453) | 本文提出模块化线性化注意力机制（MLA）以最大化推理质量并实现速度提升，并在多个自回归自然语言处理任务上验证了该方法，包括语音到文本神经机器翻译（S2T NMT）、语音到文本同声传译（SimulST）和自回归文本到频谱图任务，在TTS上具有高效率收益，在NMT和SimulST的训练和推理过程中表现出竞争性能。 |
| [^62] | [Chinese Open Instruction Generalist: A Preliminary Release.](http://arxiv.org/abs/2304.07987) | 本论文旨在通过适应不同子任务的固有特性，创建一个中文指令数据集，以填补指令调整技术在中文语言领域的空白。 |
| [^63] | [Recognizing Entity Types via Properties.](http://arxiv.org/abs/2304.07910) | 本文提出一种基于属性的方法，可根据用于定义它们的属性来识别实体类型。主要贡献包括一组基于属性的度量和一种机器学习的实体类型识别算法。 |
| [^64] | [A Survey of Large Language Models.](http://arxiv.org/abs/2303.18223) | 本文综述了大型语言模型的研究历程以及最近的预训练语言模型(PLMs)，并强调模型扩展将带来性能改进和特殊能力的发掘。 |
| [^65] | [It Takes One to Tango but More Make Trouble? In-Context Training with Different Number of Demonstrations.](http://arxiv.org/abs/2303.08119) | 本文研究了使用较少的演示数进行上下文学习（ICL）的任务，在测试查询上只使用一个随机选择的演示时并没有明显性能下降，而只使用一个正确演示的ICL在性能上显著优于全演示ICL。 |
| [^66] | [Using a Variational Autoencoder to Learn Valid Search Spaces of Safely Monitored Autonomous Robots for Last-Mile Delivery.](http://arxiv.org/abs/2303.03211) | 本文使用 COIL 方法研究了有效的自主机器人配送问题，确保在保证安全监控的前提下最大化机器人配送效率。 |
| [^67] | [Hitachi at SemEval-2023 Task 3: Exploring Cross-lingual Multi-task Strategies for Genre and Framing Detection in Online News.](http://arxiv.org/abs/2303.01794) | Hitachi团队参加SemEval-2023第3项任务，研究了跨语言和多任务策略，表明跨语言/多任务训练和收集外部平衡数据集可以有益于流派和框架检测，在意大利语和俄语流派分类子任务中实现了最高的宏平均F1分数。 |
| [^68] | [Fourier Analysis Meets Runtime Analysis: Precise Runtimes on Plateaus.](http://arxiv.org/abs/2302.08021) | 本文提出了一种基于傅里叶分析的新方法，用于分析进化算法在高原上消耗的时间，通过算法在针尖问题和新基准问题上的应用研究，确定了最佳变异率，并证明了$(1+1)$进化算法相对于一般随机搜索启发式算法在高原问题上具有惊人的效率。 |
| [^69] | [Can gamification reduce the burden of self-reporting in mHealth applications? A feasibility study using machine learning from smartwatch data to estimate cognitive load.](http://arxiv.org/abs/2302.03616) | 通过机器学习技术和智能手表数据进行认知负荷估计，研究发现游戏化的自我报告方式与传统方式的认知负荷没有差异，但参与者更喜欢游戏化版本。 |
| [^70] | [Emergent Causality & the Foundation of Consciousness.](http://arxiv.org/abs/2302.03189) | 讨论了不需要显式实现干预表示的 Pareto 最优数学形式，通过归纳性地出现相关因果干预的表示，作为自我和任何其他对象的表示。 |
| [^71] | [The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest.](http://arxiv.org/abs/2301.12987) | 本研究表明，在构建假设的过程中，选择最短的假设不如选择最弱的假设，而弱点是比长度或简单性更好的推广表现代理。 |
| [^72] | [RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving.](http://arxiv.org/abs/2301.10222) | 本文旨在探究视觉Transformer是否可以应用于自动驾驶中的3D语义分割中，通过保留与RGB图像相同的骨干结构，这项工作证明了ViTs在结合投影方法，大数据训练和具有噪声鲁棒性的新损失函数后可以取得最先进的结果。 |
| [^73] | [Masked Autoencoding Does Not Help Natural Language Supervision at Scale.](http://arxiv.org/abs/2301.07836) | 本研究旨在探讨大规模训练下掩模自编码和对比语言图像预训练的有效性。研究结果表明，在小规模训练中这两种方法可以有效地结合使用，但在大规模训练中没有明显的优势。 |
| [^74] | [VISEM-Tracking, a human spermatozoa tracking dataset.](http://arxiv.org/abs/2212.02842) | 本文提供了人类精子跟踪数据集VISEM-Tracking，包含手动注释的包围框坐标和由专家分析的精子特征，并提供未标记的视频以供易于访问和分析，有助于训练监督式机器学习方法，提高在评估精子运动和运动学方面的精度和可靠性。 |
| [^75] | [Relation-Aware Language-Graph Transformer for Question Answering.](http://arxiv.org/abs/2212.00975) | 本论文提出了关系感知语言图转换器，能够以统一的方式联合推理语言和图关于实体关系，并在多个QA数据集上验证了其有效性。 |
| [^76] | [xTrimoABFold: De novo Antibody Structure Prediction without MSA.](http://arxiv.org/abs/2212.00735) | xTrimoABFold是一种基于深度抗体语言模型的新型抗体结构预测方法，无需多序列比对，有望促进高通量药物设计的应用。 |
| [^77] | [On the Ability of Graph Neural Networks to Model Interactions Between Vertices.](http://arxiv.org/abs/2211.16494) | 本文研究了GNN模拟顶点间相互作用的能力，通过一个被称为分离秩的度量标准来量化这种能力，结果表明模拟相互作用的能力主要取决于分区的行走指数，即从分界线开始的行走数量，同时设计了一种名为WISA的边稀疏化算法以提高GNNs的处理效率和表达能力。 |
| [^78] | [StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects.](http://arxiv.org/abs/2211.04604) | 本论文提出了一种结合扩散模型和以物体为中心的转换器的方法，通过高层语言目标和局部视点云构建物理有效的结构，该方法可用于多个具有挑战性的多步骤3D规划任务，即使使用未知对象仍能提高成功率。 |
| [^79] | [Augmenting Interpretable Models with LLMs during Training.](http://arxiv.org/abs/2209.11799) | 本文提出了 Aug-imodels 框架，利用 LLMs 的知识在拟合过程中构建高效且可解释的模型，在推理过程中不使用 LLMs，具备完全的透明性。研究探讨了两种不同方式的实现，并在多种文本分类数据集中表现出优异的效果。 |
| [^80] | [Model Predictive Robustness of Signal Temporal Logic Predicates.](http://arxiv.org/abs/2209.07881) | 本论文提出了一种新的模型预测鲁棒性的概念来更系统地评估信号时间逻辑中底层谓词的鲁棒性，并成功验证了该方法在自动驾驶中的应用。 |
| [^81] | [Efficient Utility Function Learning for Multi-Objective Parameter Optimization with Prior Knowledge.](http://arxiv.org/abs/2208.10300) | 该论文提出了一种利用偏好学习离线学习效用函数的方法，以应对真实世界问题中用专家知识定义效用函数困难且与专家反复互动昂贵的问题。使用效用函数空间的粗略信息，能够在使用很少结果时提高效用函数估计，并通过整个优化链中传递效用函数学习任务中出现的不确定性。 |
| [^82] | [Isoform Function Prediction Using a Deep Neural Network.](http://arxiv.org/abs/2208.03325) | 本文介绍了一种使用深度神经网络预测同一基因位点上不同mRNA功能的方法，解决了缺乏标记训练数据的问题。 |
| [^83] | [Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering.](http://arxiv.org/abs/2207.12647) | 本篇论文提出了一个新型的事件级视觉问答框架——跨模态因果关系推理（CMCIR），通过引入因果干预方法，发现视觉和语言模态的真正因果结构，实现强健的因果感知视觉语言问答。 |
| [^84] | [BiometricBlender: Ultra-high dimensional, multi-class synthetic data generator to imitate biometric feature space.](http://arxiv.org/abs/2206.10747) | BiometricBlender是一个高维度多类合成数据生成工具，可以模拟真实生物特征数据集的关键属性，有助于在特征筛选领域进行快速发展。 |
| [^85] | [Predicting Hate Intensity of Twitter Conversation Threads.](http://arxiv.org/abs/2206.08406) | 本文提出了DRAGNET++, 通过考虑对话线程的语义、传播结构和用户交互，以预测推文的回复链中可能存在的仇恨程度，并在两个公开数据集上的实验中表现出优越性能。该模型可为社交媒体平台提供在恶意对话升级之前识别和管理的工具。 |
| [^86] | [BRExIt: On Opponent Modelling in Expert Iteration.](http://arxiv.org/abs/2206.00113) | BRExIt算法使用对手模型加速游戏学习，提高学徒的特征塑造和规划偏向于对手模型。实验证明BRExIt比ExIt学习到更好的策略。 |
| [^87] | [A theoretical framework for self-supervised MR image reconstruction using sub-sampling via variable density Noisier2Noise.](http://arxiv.org/abs/2205.10278) | 本研究提出了一个基于Noisier2Noise框架的自监督磁共振图像重建理论框架，解释了SSDU方法的性能，并提出了两种修改。 |
| [^88] | [Intention Aware Robot Crowd Navigation with Attention-Based Interaction Graph.](http://arxiv.org/abs/2203.01821) | 提出了一种基于循环图神经网络与注意力机制的机器人导航方法，可以在复杂的人群场景中实现安全、高效和不干扰的导航，通过预测未来轨迹推断出动态代理的意图，避免侵入他人预期路径。 |
| [^89] | [Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning.](http://arxiv.org/abs/2201.08115) | 本文介绍了一种名为“APEx”的技能发现和转移算法，它以一种原则性和学习的方式利用信息不对称性，实现了最先进的采样效率、任务可转移性和概括能力，同时保留了非分层方法的表达能力和灵活性。 |
| [^90] | [A Simplicity Bubble Problem in Formal-Theoretic Learning Systems.](http://arxiv.org/abs/2112.12275) | 这篇论文证明了在机器学习中，对于每个算法都存在一个足够大的数据集大小，超过该大小，无法预测欺骗者的算法概率上界是该算法的算法概率上界。 |
| [^91] | [Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective.](http://arxiv.org/abs/1903.11406) | 本文提出了一种多嵌入交互的角度来分析和比较知识图谱嵌入方法，研究了CP，DistMult和ComplEx等三种流行的方法，揭示了ComplEx性能更好的原因，并为开发新模型提供了见解。 |
| [^92] | [Ensemble Sampling.](http://arxiv.org/abs/1705.07347) | 本文介绍了集成采样，以近似Thompson采样并在复杂模型下保持可行性，将大大扩展Thompson采样的应用范围。 |

# 详细

[^1]: 眼见不一定为实：人类感知AI生成图像的定量研究

    Seeing is not always believing: A Quantitative Study on Human Perception of AI-Generated Images. (arXiv:2304.13023v1 [cs.AI])

    [http://arxiv.org/abs/2304.13023](http://arxiv.org/abs/2304.13023)

    本研究揭示了人类无法辨别AI生成的假照片和真实照片，这一点受个人背景的影响并不显著。

    

    照片是人们记录日常生活经历的一种方式，通常被认为是可信的信息来源。然而，越来越多的人担心人工智能（AI）技术的进步可能产生伪造的照片，从而产生困惑并降低对照片的信任。本研究旨在回答一个问题，即当前最先进的基于AI的视觉内容生成模型能否持续地欺骗人类的眼睛，并传达错误信息。通过对50名参与者进行高质量的定量研究，我们首次揭示，人类无法显著区分真实照片和AI创建的伪造照片，达到38.7%。我们的研究还发现，个人的背景，如性别，年龄和经验，对区分AI生成的图像和真实照片的能力没有显著影响。但是，我们观察到

    Photos serve as a way for humans to record what they experience in their daily lives, and they are often regarded as trustworthy sources of information. However, there is a growing concern that the advancement of artificial intelligence (AI) technology may produce fake photos, which can create confusion and diminish trust in photographs. This study aims to answer the question of whether the current state-of-the-art AI-based visual content generation models can consistently deceive human eyes and convey false information. By conducting a high-quality quantitative study with fifty participants, we reveal, for the first time, that humans cannot distinguish between real photos and AI-created fake photos to a significant degree 38.7%. Our study also finds that an individual's background, such as their gender, age, and experience with AI-generated content (AIGC), does not significantly affect their ability to distinguish AI-generated images from real photographs. However, we do observe that 
    
[^2]: 超越多链思维：基于元推理的问题解答方法

    Answering Questions by Meta-Reasoning over Multiple Chains of Thought. (arXiv:2304.13007v1 [cs.CL])

    [http://arxiv.org/abs/2304.13007](http://arxiv.org/abs/2304.13007)

    本论文提出了基于元推理的Multi-Chain Reasoning (MCR)方法，该方法检查多个推理链，混合它们之间的信息并选择最相关的事实，从而超越多链思维，解决多跳QA问题。 实验结果表明MCR胜过多个强基线，解释质量高。

    

    现代多跳问题解答（QA）系统通常将问题分解为一系列思考步骤（CoT），然后才得出最终答案。通常来说，多个链条被抽样并通过最终答案的投票机制进行聚合，但中间步骤本身被丢弃。虽然这种方法提高了性能，但它们并不考虑链之间的中间步骤之间的关系，并且不提供预测答案的统一解释。我们引入了基于元推理的 Multi-Chain Reasoning (MCR) 方法，该方法利用大型语言模型来超越多个思考链，而不是聚合回答。MCR检查不同的推理链，混合它们之间的信息并选择在生成解释和预测答案时最相关的事实。MCR在7个多跳QA数据集上胜过强基线。此外，我们的分析表明MCR的解释具有高质量。

    Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregating their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, en
    
[^3]: 在复杂的实时战略游戏中进行多智能体强化学习的集中化控制

    Centralized control for multi-agent RL in a complex Real-Time-Strategy game. (arXiv:2304.13004v1 [cs.AI])

    [http://arxiv.org/abs/2304.13004](http://arxiv.org/abs/2304.13004)

    本文介绍了在Lux AI v2 Kaggle比赛中使用集中式方法来训练RL智能体，在复杂的实时战略游戏中实现多智能体强化学习，并成功提高了性能。

    

    多智能体强化学习(MARL)研究多个相互作用的学习智能体在共享环境下的行为。与单一智能体强化学习相比，MARL具有更加复杂的学习动态：每个智能体的观察和奖励都是其他所有智能体的函数。在MARL的背景下，实时战略(RTS)游戏是非常具有挑战性的环境，其中多个玩家同时互动并同时控制不同性质的多个单位。实际上，对于当前的RL方法来说，RTS游戏如此具有挑战性，以至于只是能够用RL应对它们已经很有意义了。本项目提供了在Lux AI v2 Kaggle比赛中应用RL的端到端体验，参赛者设计能够控制可变大小舰队单位并在1v1情况下面对其他参赛者的多变量优化、资源收集和分配问题。我们使用集中式方法来训练RL智能体，并使用全局视图表示游戏状态。我们的方法优于使用独立代理和分散训练的标准基线方法。

    Multi-agent Reinforcement learning (MARL) studies the behaviour of multiple learning agents that coexist in a shared environment. MARL is more challenging than single-agent RL because it involves more complex learning dynamics: the observations and rewards of each agent are functions of all other agents. In the context of MARL, Real-Time Strategy (RTS) games represent very challenging environments where multiple players interact simultaneously and control many units of different natures all at once. In fact, RTS games are so challenging for the current RL methods, that just being able to tackle them with RL is interesting. This project provides the end-to-end experience of applying RL in the Lux AI v2 Kaggle competition, where competitors design agents to control variable-sized fleets of units and tackle a multi-variable optimization, resource gathering, and allocation problem in a 1v1 scenario against other competitors. We use a centralized approach for training the RL agents, and rep
    
[^4]: 深度学习中学习结构化表示的泛化问题研究

    On the Generalization of Learned Structured Representations. (arXiv:2304.13001v1 [cs.LG])

    [http://arxiv.org/abs/2304.13001](http://arxiv.org/abs/2304.13001)

    本论文研究了深度学习中学习结构化表示的泛化问题。通过提出能够学习局部和全局依赖关系的结构化表示方法，并在各种下游任务上进行实验验证，证明了该方法在图像分类和物体检测等任务上的有效性。

    

    尽管在过去的十年中取得了巨大的进展，但深度学习方法通常无法达到人类级别的系统泛化水平。人们认为明确捕获数据的基础结构应该能够让联结主义系统以更可预测和系统的方式进行泛化。事实上，人类的证据表明，用符号般的组合实体来解释世界可能对智能行为和高级推理至关重要。另一个深度学习系统的常见限制是它们需要大量的训练数据，这可能很昂贵。在表示学习中，利用大型数据集学习通用数据表示，这些表示可以用于有效地学习任意的下游任务。本论文研究了结构化表示学习。我们研究了学习未结构化数据的结构化表示的方法，这些方法需要很少或没有监督，并捕获其隐藏结构。在论文的第一部分中，我们提出了一些新的方法，用于学习结构化表示，可以捕获数据中的局部和全局依赖关系。在第二部分中，我们研究了学习的结构化表示如何提高深度网络在各种下游任务上的泛化性能。我们的实验表明，我们提出的方法在图像分类和物体检测等任务上优于几种最先进的无监督方法。

    Despite tremendous progress over the past decade, deep learning methods generally fall short of human-level systematic generalization. It has been argued that explicitly capturing the underlying structure of data should allow connectionist systems to generalize in a more predictable and systematic manner. Indeed, evidence in humans suggests that interpreting the world in terms of symbol-like compositional entities may be crucial for intelligent behavior and high-level reasoning. Another common limitation of deep learning systems is that they require large amounts of training data, which can be expensive to obtain. In representation learning, large datasets are leveraged to learn generic data representations that may be useful for efficient learning of arbitrary downstream tasks.  This thesis is about structured representation learning. We study methods that learn, with little or no supervision, representations of unstructured data that capture its hidden structure. In the first part of
    
[^5]: 从空间中分割任何物体吗？

    Segment anything, from space?. (arXiv:2304.13000v1 [cs.CV])

    [http://arxiv.org/abs/2304.13000](http://arxiv.org/abs/2304.13000)

    最近开发的Segment Anything Model（SAM）模型可以基于简单的输入提示（如一个或多个点、边界框或掩码）有效分割自然图像中的对象，对视觉研究人员具有重要意义。此项研究探讨SAM在空中图像问题上的卓越性能，并在多项基准任务上进行了验证，表现良好。

    

    最近，为视觉任务专门开发的第一个基础模型被开发出来，被称为“Segment Anything Model”（SAM）。SAM可以根据简单的输入提示（如一个或多个点、边界框或掩码）分割输入图像中的对象。作者们在大量的视觉基准任务上研究了SAM的零样本图像分割精度，并发现SAM通常达到了与目标任务训练的视觉模型相似或有时甚至超越其识别精度。SAM在分割方面的卓越泛化能力对于从事自然图像研究的视觉研究人员具有重要意义。在这项工作中，我们研究了SAM的卓越性能是否扩展到空中图像问题，并帮助指导社区对其发展的回应。我们在一组多样化和广泛研究过的基准任务上研究SAM的表现。我们发现，SAM通常在空中图像上有良好的泛化表现，尽管在某些情况下会失败。

    Recently, the first foundation model developed specifically for vision tasks was developed, termed the "Segment Anything Model" (SAM). SAM can segment objects in input imagery based upon cheap input prompts, such as one (or more) points, a bounding box, or a mask. The authors examined the zero-shot image segmentation accuracy of SAM on a large number of vision benchmark tasks and found that SAM usually achieved recognition accuracy similar to, or sometimes exceeding, vision models that had been trained on the target tasks. The impressive generalization of SAM for segmentation has major implications for vision researchers working on natural imagery. In this work, we examine whether SAM's impressive performance extends to overhead imagery problems, and help guide the community's response to its development. We examine SAM's performance on a set of diverse and widely-studied benchmark tasks. We find that SAM does often generalize well to overhead imagery, although it fails in some cases d
    
[^6]: AudioGPT：理解和生成语音、音乐、声音和人头像

    AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head. (arXiv:2304.12995v1 [cs.CL])

    [http://arxiv.org/abs/2304.12995](http://arxiv.org/abs/2304.12995)

    AudioGPT是一种多模式人工智能系统，能够处理复杂的音频信息并支持口语对话，其实验结果表明其在处理语音、音乐、声音和人头像方面有着很强的能力。

    

    大型语言模型（LLM）在各种领域和任务中展现出了卓越的能力，挑战了我们对学习和认知的理解。尽管最近取得了成功，但当前的LLM无法处理复杂的音频信息或进行口语交流（如Siri或Alexa）。在这项工作中，我们提出了一种名为AudioGPT的多模式人工智能系统，它通过以下方式补充了LLM（即ChatGPT）：1）提供基础模型以处理复杂的音频信息并解决众多的理解和生成任务；2）提供输入/输出接口（ASR，TTS）以支持口语对话。随着对人类意图理解和与基础模型协作的多模式LLM的评估需求的增加，我们概述了原则和过程，并测试了AudioGPT的一致性、能力和稳健性。实验结果显示，AudioGPT在解决具有语音、音乐、声音和人头像理解和生成的AI任务方面具有很强的能力。

    Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and gene
    
[^7]: 测量大规模多任务中文理解能力

    Measuring Massive Multitask Chinese Understanding. (arXiv:2304.12986v1 [cs.CL])

    [http://arxiv.org/abs/2304.12986](http://arxiv.org/abs/2304.12986)

    本研究提出了一项测试，以衡量大型中文语言模型的多任务准确性，测试涵盖医学、法律、心理学和教育四个主要领域，结果表明所有模型在法律领域中表现都很差，建议研究人员应该开发更加多样化和均衡的多任务中文理解模型。

    

    大规模中文语言模型的研发正蓬勃发展，但缺乏相应的能力评估。因此，我们提出了一个测试，以衡量大型中文语言模型的多任务准确性。该测试涵盖了医学、法律、心理学和教育四个主要领域，在医学领域有15个子任务，在教育领域有8个子任务。我们发现，在零样本设置下表现最佳的模型平均比表现最差的模型高出近22个百分点。在四个主要领域中，所有模型的平均零样本准确度均未超过0.5。在子领域中，只有GPT-3.5-turbo模型在临床医学中实现了0.703的零样本准确度，这是所有模型在所有子任务中最高的准确度。所有模型在法律领域中表现都很差，最高的零样本准确度仅达到0.259。通过全面评估多个学科的广度和深度的知识，我们建议研究人员应该开发更加多样化和均衡的多任务中文理解模型。

    The development of large-scale Chinese language models is flourishing, yet there is a lack of corresponding capability assessments. Therefore, we propose a test to measure the multitask accuracy of large Chinese language models. This test encompasses four major domains, including medicine, law, psychology, and education, with 15 subtasks in medicine and 8 subtasks in education. We found that the best-performing models in the zero-shot setting outperformed the worst-performing models by nearly 22 percentage points on average. Across the four major domains, the average zero-shot accuracy of all models did not exceed 0.5. In the subdomains, only the GPT-3.5-turbo model achieved a zero-shot accuracy of 0.703 in clinical medicine, which was the highest accuracy among all models across all subtasks. All models performed poorly in the legal domain, with the highest zero-shot accuracy reaching only 0.259. By comprehensively evaluating the breadth and depth of knowledge across multiple discipli
    
[^8]: Rubik光学神经网络：具有物理感知旋转结构的多任务学习

    Rubik's Optical Neural Networks: Multi-task Learning with Physics-aware Rotation Architecture. (arXiv:2304.12985v1 [cs.LG])

    [http://arxiv.org/abs/2304.12985](http://arxiv.org/abs/2304.12985)

    RubikONNs利用光学系统的物理特性，通过旋转硬件编码多个前馈函数，为ONNs实现高效的多任务学习提供了新的策略。

    

    最近，有越来越多的研究工作在推进光学神经网络（ONNs），在功率效率，并行性和计算速度方面，ONNs带来了机器学习（ML）方面的显着优势。

    Recently, there are increasing efforts on advancing optical neural networks (ONNs), which bring significant advantages for machine learning (ML) in terms of power efficiency, parallelism, and computational speed. With the considerable benefits in computation speed and energy efficiency, there are significant interests in leveraging ONNs into medical sensing, security screening, drug detection, and autonomous driving. However, due to the challenge of implementing reconfigurability, deploying multi-task learning (MTL) algorithms on ONNs requires re-building and duplicating the physical diffractive systems, which significantly degrades the energy and cost efficiency in practical application scenarios. This work presents a novel ONNs architecture, namely, \textit{RubikONNs}, which utilizes the physical properties of optical systems to encode multiple feed-forward functions by physically rotating the hardware similarly to rotating a \textit{Rubik's Cube}. To optimize MTL performance on Rubi
    
[^9]: 对高水平机器人解释中的奖励分解进行更深入的研究

    A Closer Look at Reward Decomposition for High-Level Robotic Explanations. (arXiv:2304.12958v1 [cs.LG])

    [http://arxiv.org/abs/2304.12958](http://arxiv.org/abs/2304.12958)

    本论文提出了一种将奖励分解与抽象动作空间相结合的学习框架，可以提供基于对象属性的明确高层次解释，避免了解释机器人行为的复杂性。

    

    向人类解释智能机器人的行为是具有挑战性的，这是由于它们难以理解的自体感状态、多变的中间目标和其结果的不可预测性。此外，对强化学习代理的一步解释可能是模糊的，因为它们未能在每个转换时考虑到代理的未来行为，从而增加了解释机器人行为的复杂性。通过利用映射到任务特定基元的抽象动作，我们避免了在移动层面上的解释。我们提出的框架将奖励分解（RD）与抽象动作空间结合起来，构建了一个可解释的学习框架，基于任务中的对象属性，实现了明确的高层次解释。我们通过对两个机器人场景的定量和定性分析来证明了我们框架的有效性，展示了RD解释的输出文物中的可视和文本解释，易于人类理解。

    Explaining the behavior of intelligent agents such as robots to humans is challenging due to their incomprehensible proprioceptive states, variational intermediate goals, and resultant unpredictability. Moreover, one-step explanations for reinforcement learning agents can be ambiguous as they fail to account for the agent's future behavior at each transition, adding to the complexity of explaining robot actions. By leveraging abstracted actions that map to task-specific primitives, we avoid explanations on the movement level. Our proposed framework combines reward decomposition (RD) with abstracted action spaces into an explainable learning framework, allowing for non-ambiguous and high-level explanations based on object properties in the task. We demonstrate the effectiveness of our framework through quantitative and qualitative analysis of two robot scenarios, showcasing visual and textual explanations, from output artifacts of RD explanation, that are easy for humans to comprehend. 
    
[^10]: 生成强鲁棒性的反事实解释

    Generating robust counterfactual explanations. (arXiv:2304.12943v1 [cs.LG])

    [http://arxiv.org/abs/2304.12943](http://arxiv.org/abs/2304.12943)

    本论文提出了一个新的反事实解释框架CROCO，能够生成强鲁棒性的解释，同时有效地平衡了鲁棒性和解释示例的接近度之间的权衡。

    

    反事实的解释已经成为可解释人工智能领域的主流。这一直观的陈述让用户理解在给定情况下，为改变模型预测而必须进行的小但必要的更改。反事实的质量取决于多个标准：真实性、可行性、有效性、鲁棒性等等。本文关注反事实的鲁棒性概念，更具体地，关注反事实输入变化的鲁棒性。这种鲁棒性尤其具有挑战性，因为它涉及到反事实的鲁棒性和与要解释的示例的接近度之间的权衡。我们提出了一个新的框架CROCO，它能够有效地管理这种权衡，生成具有强鲁棒性的反事实解释，并保证用户具有最小的鲁棒性。在表格数据集上的实证评估确认了我们方法的相关性和有效性。

    Counterfactual explanations have become a mainstay of the XAI field. This particularly intuitive statement allows the user to understand what small but necessary changes would have to be made to a given situation in order to change a model prediction. The quality of a counterfactual depends on several criteria: realism, actionability, validity, robustness, etc. In this paper, we are interested in the notion of robustness of a counterfactual. More precisely, we focus on robustness to counterfactual input changes. This form of robustness is particularly challenging as it involves a trade-off between the robustness of the counterfactual and the proximity with the example to explain. We propose a new framework, CROCO, that generates robust counterfactuals while managing effectively this trade-off, and guarantees the user a minimal robustness. An empirical evaluation on tabular datasets confirms the relevance and effectiveness of our approach.
    
[^11]: SALSA: 用于DNN加速器的模拟退火循环排序调度器

    SALSA: Simulated Annealing based Loop-Ordering Scheduler for DNN Accelerators. (arXiv:2304.12931v1 [cs.AR])

    [http://arxiv.org/abs/2304.12931](http://arxiv.org/abs/2304.12931)

    SALSA是一种用于DNN加速器的快速双引擎调度器，利用新策略将穷举搜索和模拟退火相结合，能够在加速搜索的同时找到更低能量调度。

    

    为了满足DNN计算需求的不断增长，已经提出了多种专用的硬件体系结构。每个DNN层应该映射到最有效的硬件上，然而，现有优化器往往无法在合理的时间内为所有DNN-HW组合提供最佳的执行计划。本文提出了SALSA，一种快速的双引擎调度器，用于生成均匀和非均匀映射的最优执行计划。我们介绍了一种新策略，将穷举搜索与模拟退火相结合，以应对不同层之间循环排序设计空间大小的动态变化。SALSA在5个不同的DNN上进行了广泛的基准测试，平均而言，与LOMA和Timeloop相比，SALSA在加速搜索的同时能够找到11.9％和7.6％更低的能量调度，分别提高了1.7倍和24倍。

    To meet the growing need for computational power for DNNs, multiple specialized hardware architectures have been proposed. Each DNN layer should be mapped onto the hardware with the most efficient schedule, however, SotA schedulers struggle to consistently provide optimum schedules in a reasonable time across all DNN-HW combinations.  This paper proposes SALSA, a fast dual-engine scheduler to generate optimal execution schedules for both even and uneven mapping. We introduce a new strategy, combining exhaustive search with simulated annealing to address the dynamic nature of the loop ordering design space size across layers. SALSA is extensively benchmarked against two SotA schedulers, LOMA and Timeloop on 5 different DNNs, on average SALSA finds schedules with 11.9% and 7.6% lower energy while speeding up the search by 1.7x and 24x compared to LOMA and Timeloop, respectively.
    
[^12]: 基于区块链和可信执行环境的物联网联邦学习与安全集成研究 (arXiv:2304.12889v1 [cs.CR])

    Blockchain-based Federated Learning with Secure Aggregation in Trusted Execution Environment for Internet-of-Things. (arXiv:2304.12889v1 [cs.CR])

    [http://arxiv.org/abs/2304.12889](http://arxiv.org/abs/2304.12889)

    本文提出了一种基于区块链和可信执行环境的联邦学习框架，用于安全聚合本地模型，保证全局模型的真实性和完整性，适用于工业物联网。

    

    本文提出了一种基于区块链和Intel Software Guard Extension（SGX）可信执行环境（TEE）的联邦学习（FL）框架，在工业物联网（IIoTs）中安全地聚合本地模型。在FL中，攻击者可能篡改本地模型，因此，从篡改的本地模型生成的全局模型可能是错误的。因此，该框架利用区块链网络进行安全模型聚合，每个区块链节点都拥有一个启用了SGX的处理器，可以安全地执行基于FL的聚合任务以生成全局模型，节点可以验证聚合模型的真实性，运行区块链共识机制以确保模型的完整性，并将其添加到分布式分类帐中进行防篡改存储。每个集群都可以从区块链获取聚合模型并在使用之前验证其完整性。我们使用不同的CNN模型和数据集进行了几项实验，以评估该框架的性能。

    This paper proposes a blockchain-based Federated Learning (FL) framework with Intel Software Guard Extension (SGX)-based Trusted Execution Environment (TEE) to securely aggregate local models in Industrial Internet-of-Things (IIoTs). In FL, local models can be tampered with by attackers. Hence, a global model generated from the tampered local models can be erroneous. Therefore, the proposed framework leverages a blockchain network for secure model aggregation. Each blockchain node hosts an SGX-enabled processor that securely performs the FL-based aggregation tasks to generate a global model. Blockchain nodes can verify the authenticity of the aggregated model, run a blockchain consensus mechanism to ensure the integrity of the model, and add it to the distributed ledger for tamper-proof storage. Each cluster can obtain the aggregated model from the blockchain and verify its integrity before using it. We conducted several experiments with different CNN models and datasets to evaluate th
    
[^13]: 通过双重对抗去偏置实现的越界证据感知假新闻检测

    Out-of-distribution Evidence-aware Fake News Detection via Dual Adversarial Debiasing. (arXiv:2304.12888v1 [cs.CL])

    [http://arxiv.org/abs/2304.12888](http://arxiv.org/abs/2304.12888)

    该论文提出了一种新颖的双重对抗学习方法，通过在模型中加入去偏置鉴别器，旨在训练模型更好地进行越界证据感知假新闻检测，有效减轻新闻和证据内容偏差的影响。

    

    越界证据感知假新闻检测旨在对新闻和基于新闻内容检索的证据进行推理，以查找统一性或不一致性。然而，我们发现，证据感知检测模型会受到偏差的影响，即新闻/证据内容和真实/假新闻标签之间的虚假相关性，并且很难推广到越界情况。为了应对这个问题，我们提出了一种新颖的双重对抗学习方法。我们在DAL中加入了新闻方面和证据方面去偏置鉴别器，它们的目标都是真假新闻标签。然后，DAL会逆向优化新闻方面和证据方面去偏置鉴别器，以减轻新闻和证据内容偏差的影响。同时，DAL还优化主要的假新闻预测器，让新闻-证据交互模块能够被学习。这个过程能够帮助我们教新闻检测模型更好地进行新闻证据推理，并将检测假新闻的负面影响降至最低。

    Evidence-aware fake news detection aims to conduct reasoning between news and evidence, which is retrieved based on news content, to find uniformity or inconsistency. However, we find evidence-aware detection models suffer from biases, i.e., spurious correlations between news/evidence contents and true/fake news labels, and are hard to be generalized to Out-Of-Distribution (OOD) situations. To deal with this, we propose a novel Dual Adversarial Learning (DAL) approach. We incorporate news-aspect and evidence-aspect debiasing discriminators, whose targets are both true/fake news labels, in DAL. Then, DAL reversely optimizes news-aspect and evidence-aspect debiasing discriminators to mitigate the impact of news and evidence content biases. At the same time, DAL also optimizes the main fake news predictor, so that the news-evidence interaction module can be learned. This process allows us to teach evidence-aware fake news detection models to better conduct news-evidence reasoning, and min
    
[^14]: 面向科学的无监督域转移：探索深度学习方法实现具有不同响应模型的LArTPC探测器模拟之间的翻译

    Unsupervised Domain Transfer for Science: Exploring Deep Learning Methods for Translation between LArTPC Detector Simulations with Differing Response Models. (arXiv:2304.12858v1 [hep-ex])

    [http://arxiv.org/abs/2304.12858](http://arxiv.org/abs/2304.12858)

    本文提出了一种基于最近在成对图像转换技术上的进展的，完全无监督的方法来减少LArTPC探测器模拟和真实数据之间的系统差异，以提高模型性能。

    

    深度学习技术在科学中具有广泛应用，特别是在寻求简化潜在解决方案和发现的路径方面。然而，DL模型经常在模拟结果上进行训练，然后应用于真实实验数据。因此，模拟和真实数据之间的任何系统差异可能会降低模型的性能，这种效应称为“领域漂移”。本文研究了一种系统差异的玩具模型，提出了一种完全无监督的任务不可知方法来减少两个系统不同的样本之间的差异。该方法基于最近在成对图像转换技术上的进展，并在两组模拟液氩时间投影室（LArTPC）探测器事件样本上进行了验证，这些样本被创建以控制地演示在模拟和真实数据之间的常见系统差异。LArTPC探测器代表了下一代粒子探测器的发展方向。

    Deep learning (DL) techniques have broad applications in science, especially in seeking to streamline the pathway to potential solutions and discoveries. Frequently, however, DL models are trained on the results of simulation yet applied to real experimental data. As such, any systematic differences between the simulated and real data may degrade the model's performance -- an effect known as "domain shift." This work studies a toy model of the systematic differences between simulated and real data. It presents a fully unsupervised, task-agnostic method to reduce differences between two systematically different samples. The method is based on the recent advances in unpaired image-to-image translation techniques and is validated on two sets of samples of simulated Liquid Argon Time Projection Chamber (LArTPC) detector events, created to illustrate common systematic differences between the simulated and real data in a controlled way. LArTPC-based detectors represent the next-generation pa
    
[^15]: 一种新的用于机器学习的确定性信息理论

    A New Information Theory of Certainty for Machine Learning. (arXiv:2304.12833v1 [cs.IT])

    [http://arxiv.org/abs/2304.12833](http://arxiv.org/abs/2304.12833)

    该论文提出了一种新的信息理论概念 troenpy 来量化底层分布的确定性，用于机器学习中文档分类和序列数据权重方案，并定义了量子 troenpy 量化量子系统确定性。

    

    克劳德·香农提出了熵的概念来量化通信编码理论中随机分布的不确定性。我们观察到熵的这种不确定性特性也限制了其在数学建模中的直接使用。因此，我们提出了一个新概念 troenpy，作为熵的规范对偶，来量化底层分布的确定性。我们展示了在机器学习中的两个应用。第一个是用于传统的文档分类，我们开发了一个基于 troenpy 权重方案来利用文档分类标签。第二个是针对序列数据的自我 troenpy 权重方案，并表明它可以轻松地包含在基于神经网络的语言模型中，并实现显著的困惑度降低。我们还定义了量子 troenpy 作为 Von Neumann 熵的对偶，以量化量子系统的确定性。

    Claude Shannon coined entropy to quantify the uncertainty of a random distribution for communication coding theory. We observe that the uncertainty nature of entropy also limits its direct usage in mathematical modeling. Therefore we propose a new concept troenpy,as the canonical dual of entropy, to quantify the certainty of the underlying distribution. We demonstrate two applications in machine learning. The first is for the classical document classification, we develop a troenpy based weighting scheme to leverage the document class label. The second is a self-troenpy weighting scheme for sequential data and show that it can be easily included in neural network based language models and achieve dramatic perplexity reduction. We also define quantum troenpy as the dual of the Von Neumann entropy to quantify the certainty of quantum systems.
    
[^16]: 基于深度强化学习的中药处方规划优化框架

    A optimization framework for herbal prescription planning based on deep reinforcement learning. (arXiv:2304.12828v1 [cs.AI])

    [http://arxiv.org/abs/2304.12828](http://arxiv.org/abs/2304.12828)

    基于深度强化学习的中药处方规划模型 PrescDRL 关注长期疗效，对慢性疾病治疗具有更好的效果，评估表明其较医生治疗效果提高了117%和153%。

    

    慢性病治疗规划是医学人工智能中的关键任务，特别是在中医中药领域。然而，为不同临床情况下的患者生成优化的序列治疗策略仍然是一个需要进一步探索的难题。在本研究中，我们提出了一种基于深度强化学习的中药处方规划框架 (PrescDRL)，用于慢性疾病治疗。PrescDRL是一个序列中药处方优化模型，它关注的是长期的疗效，而不是在每一步达到最大的奖励，从而确保更好的患者治疗效果。我们构建了一个高质量的用于糖尿病顺序诊断和治疗的基准数据集，并评估了PrescDRL与该基准的性能。我们的结果表明，与医生相比，PrescDRL实现了更高的治疗效果，单步奖励提高了117%和153%。

    Treatment planning for chronic diseases is a critical task in medical artificial intelligence, particularly in traditional Chinese medicine (TCM). However, generating optimized sequential treatment strategies for patients with chronic diseases in different clinical encounters remains a challenging issue that requires further exploration. In this study, we proposed a TCM herbal prescription planning framework based on deep reinforcement learning for chronic disease treatment (PrescDRL). PrescDRL is a sequential herbal prescription optimization model that focuses on long-term effectiveness rather than achieving maximum reward at every step, thereby ensuring better patient outcomes. We constructed a high-quality benchmark dataset for sequential diagnosis and treatment of diabetes and evaluated PrescDRL against this benchmark. Our results showed that PrescDRL achieved a higher curative effect, with the single-step reward improving by 117% and 153% compared to doctors. Furthermore, PrescDRL
    
[^17]: 证明结构的研究

    Investigations into Proof Structures. (arXiv:2304.12827v1 [cs.AI])

    [http://arxiv.org/abs/2304.12827](http://arxiv.org/abs/2304.12827)

    介绍了一种新的形式主义来操作和分析证明，用于生成更短的证明和减少搜索工作量。

    

    我们引入并详细阐述了一种新型形式主义来操作和分析证明作为一个整体的对象。在这第一次尝试中，这个形式主义仅限于由浓缩推导特征的一阶问题。我们以一个全面的形式重构和分析历史上{\L}ukasiewicz广泛研究过的问题的证明为例进行了阐述。这种方法为在证明搜索过程中生成引理提供了新的系统方法，以减少搜索工作量并找到更短的证明。在这条路线上报告了许多实验，其中自动发现了一个证明{\L}ukasiewicz的问题，它比以前任何由人或机器发现的证明都要短得多。

    We introduce and elaborate a novel formalism for the manipulation and analysis of proofs as objects in a global manner. In this first approach the formalism is restricted to first-order problems characterized by condensed detachment. It is applied in an exemplary manner to a coherent and comprehensive formal reconstruction and analysis of historical proofs of a widely-studied problem due to {\L}ukasiewicz. The underlying approach opens the door towards new systematic ways of generating lemmas in the course of proof search to the effects of reducing the search effort and finding shorter proofs. Among the numerous reported experiments along this line, a proof of {\L}ukasiewicz's problem was automatically discovered that is much shorter than any proof found before by man or machine.
    
[^18]: GraphVF: 用变分流控制生成特定蛋白质的三维分子

    GraphVF: Controllable Protein-Specific 3D Molecule Generation with Variational Flow. (arXiv:2304.12825v1 [q-bio.BM])

    [http://arxiv.org/abs/2304.12825](http://arxiv.org/abs/2304.12825)

    GraphVF是一种新的分子生成框架，它将2D拓扑和3D几何结合起来，用于可控生成具有结合能力的3D分子，是第一个可以生成结合能力的3D分子并定制其物化特性的方法。

    

    设计与特定目标蛋白质结合的分子是药物发现中的基本任务。最近的模型利用几何约束条件生成能够与特定蛋白质口袋紧密结合的配体分子。然而，这些模型不能有效地生成具有2D骨架退化和物性限制的3D分子，这对于药物的效能和开发至关重要。为了克服这一挑战，我们提出了GraphVF，一种基于变分流的框架，将2D拓扑和3D几何结合起来，用于可控生成具有结合能力的3D分子。实验表明，我们的方法实现了最先进的结合亲和力和特定蛋白质生成的实际次级结构布局。尤其是，GraphVF代表了第一个可控的几何意识，特定蛋白质分子生成方法，可以生成具有定制次级结构和物理化学性质的结合3D分子。我们的代码可在https://github.com/Franco-Solis/GraphVF-code中获得。

    Designing molecules that bind to specific target proteins is a fundamental task in drug discovery. Recent models leverage geometric constraints to generate ligand molecules that bind cohesively with specific protein pockets. However, these models cannot effectively generate 3D molecules with 2D skeletal curtailments and property constraints, which are pivotal to drug potency and development. To tackle this challenge, we propose GraphVF, a variational flow-based framework that combines 2D topology and 3D geometry, for controllable generation of binding 3D molecules. Empirically, our method achieves state-of-the-art binding affinity and realistic sub-structural layouts for protein-specific generation. In particular, GraphVF represents the first controllable geometry-aware, protein-specific molecule generation method, which can generate binding 3D molecules with tailored sub-structures and physio-chemical properties. Our code is available at https://github.com/Franco-Solis/GraphVF-code.
    
[^19]: 语音助手可以听起来可爱吗？迈向可爱嗓音的模型

    Can Voice Assistants Sound Cute? Towards a Model of Kawaii Vocalics. (arXiv:2304.12809v1 [cs.HC])

    [http://arxiv.org/abs/2304.12809](http://arxiv.org/abs/2304.12809)

    本文研究声音是否可以具备可爱性，并通过探索可爱的语音特质，即可爱的声音学来实现。通过对不同年龄段和不同特征的语音感受研究，发现可爱性与性别、年龄、流利度和人工性等方面有关。提出了一个可爱嗓音的模型，需要进行声音质量、认知评估、行为反应和情感报告的研究来验证。

    

    日本的“可爱”概念或表达可爱、脆弱和/或魅力的方式是一种全球文化输出。研究探讨了在机器人和虚拟角色的视觉外观、非语言行为和声音中探索可爱性作为设计特征和用户体验因素。在这项初步工作中，我们考虑了声音是否可以通过探索语音助手语音的声音特质，即可爱的声音学，具备可爱性。根据一个包含年龄的可爱模型，我们对年轻和年老的日语电脑语音的可爱度进行了用户感知研究。我们发现可爱性与性别和年龄的感知相交，即性别模糊和女孩气质，以及VA的特征，即流利度和人工性。我们提出了一个可爱嗓音的模型，通过识别和研究声音质量、认知评估、行为反应和情感报告进行验证。

    The Japanese notion of "kawaii" or expressions of cuteness, vulnerability, and/or charm is a global cultural export. Work has explored kawaii-ness as a design feature and factor of user experience in the visual appearance, nonverbal behaviour, and sound of robots and virtual characters. In this initial work, we consider whether voices can be kawaii by exploring the vocal qualities of voice assistant speech, i.e., kawaii vocalics. Drawing from an age-inclusive model of kawaii, we ran a user perceptions study on the kawaii-ness of younger- and older-sounding Japanese computer voices. We found that kawaii-ness intersected with perceptions of gender and age, i.e., gender ambiguous and girlish, as well as VA features, i.e., fluency and artificiality. We propose an initial model of kawaii vocalics to be validated through the identification and study of vocal qualities, cognitive appraisals, behavioural responses, and affective reports.
    
[^20]: 分布式强化学习中的损失和奖励加权

    Loss and Reward Weighing for increased learning in Distributed Reinforcement Learning. (arXiv:2304.12778v1 [cs.LG])

    [http://arxiv.org/abs/2304.12778](http://arxiv.org/abs/2304.12778)

    本文提出了两种分布式强化学习方法，奖励加权和损失加权梯度合并，以更好地提高分布式代理的学习效果。

    

    本文介绍了两种强化学习（RL）环境中分布式代理的学习方案，即奖励加权（R-Weighted）和损失加权（L-Weighted）梯度合并。 R / L 加权方法替换了训练多个代理的标准实践，例如对梯度求和或平均。每个代理在不同初始化版本的相同环境中运行，这会从不同的actor获得不同的梯度。

    This paper introduces two learning schemes for distributed agents in Reinforcement Learning (RL) environments, namely Reward-Weighted (R-Weighted) and Loss-Weighted (L-Weighted) gradient merger. The R/L weighted methods replace standard practices for training multiple agents, such as summing or averaging the gradients. The core of our methods is to scale the gradient of each actor based on how high the reward (for R-Weighted) or the loss (for L-Weighted) is compared to the other actors. During training, each agent operates in differently initialized versions of the same environment, which gives different gradients from different actors. In essence, the R-Weights and L-Weights of each agent inform the other agents of its potential, which again reports which environment should be prioritized for learning. This approach of distributed learning is possible because environments that yield higher rewards, or low losses, have more critical information than environments that yield lower reward
    
[^21]: 状态空间不够用：机器翻译需要注意力机制

    State Spaces Aren't Enough: Machine Translation Needs Attention. (arXiv:2304.12776v1 [cs.CL])

    [http://arxiv.org/abs/2304.12776](http://arxiv.org/abs/2304.12776)

    S4模型在机器翻译任务上与变压器模型相比存在四个BLEU分数点的差距，需要注意力机制来弥补其无法在单个隐藏状态中总结完整的源句子的缺陷。

    

    序列的结构状态空间（S4）是一个最近提出的序列模型，在各种任务中都有成功的应用，例如视觉、语言建模和音频。由于它的数学公式，它将其输入压缩为一个隐藏状态，并能够捕获长期依赖关系，同时避免了注意力机制的需要。在本文中，我们将S4应用于机器翻译（MT），并在WMT'14和WMT'16上评估了几种编码器-解码器变体。与在语言建模中的成功相比，我们发现S4在BLEU点数上落后于变压器约4个点，并且令人感到困惑的是，它在处理长句时遇到了困难。最后，我们展示了这种差距是由于S4无法在单个隐藏状态中总结完整的源句子所致，并展示了我们可以通过引入注意力机制来弥补这一差距。

    Structured State Spaces for Sequences (S4) is a recently proposed sequence model with successful applications in various tasks, e.g. vision, language modeling, and audio. Thanks to its mathematical formulation, it compresses its input to a single hidden state, and is able to capture long range dependencies while avoiding the need for an attention mechanism. In this work, we apply S4 to Machine Translation (MT), and evaluate several encoder-decoder variants on WMT'14 and WMT'16. In contrast with the success in language modeling, we find that S4 lags behind the Transformer by approximately 4 BLEU points, and that it counter-intuitively struggles with long sentences. Finally, we show that this gap is caused by S4's inability to summarize the full source sentence in a single hidden state, and show that we can close the gap by introducing an attention mechanism.
    
[^22]: 授权野生动物守护者：使用深度学习和 3/4G 摄像机陷阱的公平数字管理和奖励系统进行生物多样性保护

    Empowering Wildlife Guardians: An Equitable Digital Stewardship and Reward System for Biodiversity Conservation using Deep Learning and 3/4G Camera Traps. (arXiv:2304.12703v1 [cs.AI])

    [http://arxiv.org/abs/2304.12703](http://arxiv.org/abs/2304.12703)

    本研究提出一种基于“物种货币”的数字管理和奖励系统，以支持野生动物守护者的保护工作，并通过深度学习算法分析数据来监督和保护它们。

    

    我们地球上的生物多样性正面临威胁，预计近一百万物种将在几十年内灭绝。这是由于人类的负面行为，包括狩猎、过度捕捞、污染以及为城市化和农业目的而转化土地等。尽管慈善机构和政府在有益自然的活动上投入了大量的资金，全球野生动物种群仍然不断下降。当地的野生动物守护者历史上在全球保护工作中发挥了关键作用，并展示了他们在各个层次上实现可持续发展的能力。2021年，联合国气候变化大会（COP26）认可了他们的贡献，并承诺每年提供17亿美元。然而，考虑到他们保护了地球生物多样性的80％，这只是全球生物多样性预算（每年约1240亿至1430亿美元）中的一小部分。本文提出了一种基于“物种货币”的激进新解决方案，其中动物拥有自己的货币。通过为野生动物创建数字孪生体，可以使用深度学习算法分析数据来监督和保护它们。

    The biodiversity of our planet is under threat, with approximately one million species expected to become extinct within decades. The reason; negative human actions, which include hunting, overfishing, pollution, and the conversion of land for urbanisation and agricultural purposes. Despite significant investment from charities and governments for activities that benefit nature, global wildlife populations continue to decline. Local wildlife guardians have historically played a critical role in global conservation efforts and have shown their ability to achieve sustainability at various levels. In 2021, COP26 recognised their contributions and pledged US$1.7 billion per year; however, this is a fraction of the global biodiversity budget available (between US$124 billion and US$143 billion annually) given they protect 80% of the planets biodiversity. This paper proposes a radical new solution based on "Interspecies Money," where animals own their own money. Creating a digital twin for e
    
[^23]: 关于意义计算、语言模型与不可理解恐惧的研究

    On the Computation of Meaning, Language Models and Incomprehensible Horrors. (arXiv:2304.12686v1 [cs.AI])

    [http://arxiv.org/abs/2304.12686](http://arxiv.org/abs/2304.12686)

    本文提出了一个全面的机械化解释，涉及意义、交流和符号出现的现象，揭示了当前的语言模型并不具备与人类相同的意义理解，作者建议模拟人类情感并优化模型以构建弱表示形式来解决这个问题。

    

    我们将意义的基础理论与人工通用智能的数学形式结合起来，提出了一个全面的机械化解释，涉及意义、交流和符号出现的现象，这对通用人工智能和关于语言本质的辩论都具有重要意义。通过研究机器在什么条件下能够生成有意义的话语或理解人类的意义，我们发现当前的语言模型并不具备与人类相同的意义理解，也不意味着它们的回答具有我们所归属的任何意义。因此，我们建议模拟人类情感并优化模型以构建弱表示形式来解决这个问题。我们的研究成果揭示了意义与...

    We integrate foundational theories of meaning with a mathematical formalism of artificial general intelligence (AGI) to offer a comprehensive mechanistic explanation of meaning, communication, and symbol emergence. This synthesis holds significance for both AGI and broader debates concerning the nature of language, as it unifies pragmatics, logical truth conditional semantics, Peircean semiotics, and a computable model of enactive cognition, addressing phenomena that have traditionally evaded mechanistic explanation. By examining the conditions under which a machine can generate meaningful utterances or comprehend human meaning, we establish that the current generation of language models do not possess the same understanding of meaning as humans nor intend any meaning that we might attribute to their responses. To address this, we propose simulating human feelings and optimising models to construct weak representations. Our findings shed light on the relationship between meaning and in
    
[^24]: 探索自监督单帧和多帧深度估计之间的相互影响

    Exploring the Mutual Influence between Self-Supervised Single-Frame and Multi-Frame Depth Estimation. (arXiv:2304.12685v1 [cs.CV])

    [http://arxiv.org/abs/2304.12685](http://arxiv.org/abs/2304.12685)

    文章介绍了一种新的自监督训练框架，旨在充分利用单帧深度和多帧深度方法之间的相互影响。通过引入一个像素逐像素自适应深度采样模块，以单帧深度为指导来训练多帧模型，并利用最小重投影为基础的蒸馏方法来优化单帧深度的模型。

    

    尽管自监督单帧和多帧深度估计方法都只需要无标签单目视频进行训练，但它们所利用的信息不同，单帧方法主要依赖于基于外貌的特征，而多帧方法则专注于几何线索。考虑到单帧和多帧方法的互补信息，一些工作尝试利用单帧深度来改进多帧深度。但是，这些方法既不能利用单帧深度和多帧深度之间的差异来改进多帧深度，也不能利用多帧深度来优化单帧深度模型。为了充分利用单帧和多帧方法之间的相互影响，我们提出了一种新的自监督训练框架。具体而言，我们首先引入一个由单帧深度指导的像素逐像素自适应深度采样模块来训练多帧模型。然后，我们利用最小重投影为基础的蒸馏方法来利用多帧深度优化单帧深度的模型。

    Although both self-supervised single-frame and multi-frame depth estimation methods only require unlabeled monocular videos for training, the information they leverage varies because single-frame methods mainly rely on appearance-based features while multi-frame methods focus on geometric cues. Considering the complementary information of single-frame and multi-frame methods, some works attempt to leverage single-frame depth to improve multi-frame depth. However, these methods can neither exploit the difference between single-frame depth and multi-frame depth to improve multi-frame depth nor leverage multi-frame depth to optimize single-frame depth models. To fully utilize the mutual influence between single-frame and multi-frame methods, we propose a novel self-supervised training framework. Specifically, we first introduce a pixel-wise adaptive depth sampling module guided by single-frame depth to train the multi-frame model. Then, we leverage the minimum reprojection based distillat
    
[^25]: 论反事实解释的分歧：透明可能具有误导性。

    Disagreement amongst counterfactual explanations: How transparency can be deceptive. (arXiv:2304.12667v1 [cs.AI])

    [http://arxiv.org/abs/2304.12667](http://arxiv.org/abs/2304.12667)

    反事实解释的多样性会对利益相关者之间产生潜在分歧问题，并可能掩盖或淡化原始模型中的敏感和不公平特征，因此需要更好地理解和管理这种差异，以防止欺骗，并确保XAI的道德使用。

    

    反事实解释作为可解释人工智能(XAI)技术之一，越来越多地被用于为数据驱动决策的利益相关者提供解释。尽管目前存在许多生成反事实解释的算法，但并非每个算法都能为同一实例创建一致的解释。在某些情况下，多个可能的解释是有益的，但在一些场景下，反事实解释之间的差异会导致利益相关者之间的潜在分歧问题。当恶意主体利用这种差异来掩盖敏感特征，公正掩饰不公平的机器学习模型时，就会出现道德问题。随着全球立法者开始将解释数据驱动高风险决策的权利纳入其政策中，应该了解和解决这些伦理问题。我们对反事实解释之间的分歧问题进行的文献综述表明，解释可能是具有误导性的：多样化的反事实解释可能会掩盖或淡化原始模型中的敏感和不公平特征。本文提出了更好地理解和管理反事实解释之间差异的论点，以防止欺骗，并确保XAI的道德使用。

    Counterfactual explanations are increasingly used as an Explainable Artificial Intelligence (XAI) technique to provide stakeholders of complex machine learning algorithms with explanations for data-driven decisions. The popularity of counterfactual explanations resulted in a boom in the algorithms generating them. However, not every algorithm creates uniform explanations for the same instance. Even though in some contexts multiple possible explanations are beneficial, there are circumstances where diversity amongst counterfactual explanations results in a potential disagreement problem among stakeholders. Ethical issues arise when for example, malicious agents use this diversity to fairwash an unfair machine learning model by hiding sensitive features. As legislators worldwide tend to start including the right to explanations for data-driven, high-stakes decisions in their policies, these ethical issues should be understood and addressed. Our literature review on the disagreement probl
    
[^26]: CoDi: 混合类型表格生成的共同演化对比扩散模型

    CoDi: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis. (arXiv:2304.12654v1 [cs.LG])

    [http://arxiv.org/abs/2304.12654](http://arxiv.org/abs/2304.12654)

    CoDi 方法使用两个共同演化的对比扩散模型单独处理离散和连续变量并相互条件化，同时引入对比学习方法进行进一步的绑定，展现了在真实世界的表格数据集上的有效性。

    

    随着越来越多的注意力被放在表格数据上，将综合表格应用于各种任务的尝试已经向各种场景扩展。由于生成建模的最新进展，通过表格数据综合模型生成的虚假数据变得复杂而真实。但是，建模表格数据的离散变量（列）仍然存在困难。在本研究中，我们提出通过两个对比扩散模型单独处理连续和离散变量（但相互条件化）。两个扩散模型通过彼此读取条件在训练中共同演化。此外，为了进一步绑定扩散模型，我们引入了一个负采样的对比学习方法。在11个真实世界的表格数据集和8个基准方法的实验中，我们证明了所提出的方法 CoDi 的有效性。

    With growing attention to tabular data these days, the attempt to apply a synthetic table to various tasks has been expanded toward various scenarios. Owing to the recent advances in generative modeling, fake data generated by tabular data synthesis models become sophisticated and realistic. However, there still exists a difficulty in modeling discrete variables (columns) of tabular data. In this work, we propose to process continuous and discrete variables separately (but being conditioned on each other) by two diffusion models. The two diffusion models are co-evolved during training by reading conditions from each other. In order to further bind the diffusion models, moreover, we introduce a contrastive learning method with a negative sampling method. In our experiments with 11 real-world tabular datasets and 8 baseline methods, we prove the efficacy of the proposed method, called CoDi.
    
[^27]: 基于图注意力的部分可观察均场多智能体强化学习

    Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention. (arXiv:2304.12653v1 [cs.AI])

    [http://arxiv.org/abs/2304.12653](http://arxiv.org/abs/2304.12653)

    本文提出了一种新的基于图注意力的部分可观察均场多智能体强化学习算法，使用图注意力来捕获周围邻居智能体的特征信息，可以提高大规模多智能体环境中部分可观察MARL的性能。

    

    传统的多智能体强化学习算法难以在大规模多智能体环境中应用。最近引入的均场理论提高了多智能体强化学习的可扩展性。本文考虑部分可观察的多智能体强化学习，其中每个智能体只能观察到固定范围内的其他智能体。这种部分可观察性影响了智能体评估周围智能体行动质量的能力。本文着重于开发一种从局部观测中获取更有效信息以选择更有效行动的方法。在这个领域的以前工作使用概率分布或加权均场来更新邻居智能体平均行动，但它没有充分考虑周围邻居的特征信息，导致了局部最优。本文提出了一种新的多智能体强化学习算法，基于图注意力的部分可观察均场多智能体强化学习，它使用图注意力来捕获周围邻居智能体的特征信息。我们的方法可以提高大规模多智能体环境中部分可观察MARL的性能。

    Traditional multi-agent reinforcement learning algorithms are difficultly applied in a large-scale multi-agent environment. The introduction of mean field theory has enhanced the scalability of multi-agent reinforcement learning in recent years. This paper considers partially observable multi-agent reinforcement learning (MARL), where each agent can only observe other agents within a fixed range. This partial observability affects the agent's ability to assess the quality of the actions of surrounding agents. This paper focuses on developing a method to capture more effective information from local observations in order to select more effective actions. Previous work in this field employs probability distributions or weighted mean field to update the average actions of neighborhood agents, but it does not fully consider the feature information of surrounding neighbors and leads to a local optimum. In this paper, we propose a novel multi-agent reinforcement learning algorithm, Partially
    
[^28]: 基于Q的均衡

    Q-based Equilibria. (arXiv:2304.12647v1 [econ.TH])

    [http://arxiv.org/abs/2304.12647](http://arxiv.org/abs/2304.12647)

    该论文研究了基于Q的策略规则族中的均衡偏差（或 Qb-equilibria），即Q值在不同监测技术下的效果。

    

    在动态环境中，Q学习是一种自适应规则，其为每个替代方案提供估计值(即Q值)，该值与之前的决策相关。一个朴素的策略是始终选择具有最高Q值的替代方案。我们考虑一族基于Q的策略规则，这些规则可能系统地支持某些替代方案而不是其他替代方案，例如包含有利合作的宽容偏差的规则。在 Compte 和 Postlewaite [2018] 的精神下，我们在这个 Q-based 规则族中寻找均衡偏差（或 Qb-equilibria）。我们研究了不同监测技术下的经典博弈。

    In dynamic environments, Q-learning is an adaptative rule that provides an estimate (a Q-value) of the continuation value associated with each alternative. A naive policy consists in always choosing the alternative with highest Q-value. We consider a family of Q-based policy rules that may systematically favor some alternatives over others, for example rules that incorporate a leniency bias that favors cooperation. In the spirit of Compte and Postlewaite [2018], we look for equilibrium biases (or Qb-equilibria) within this family of Q-based rules. We examine classic games under various monitoring technologies.
    
[^29]: 自适应路径记忆网络用于时间知识图谱推理

    Adaptive Path-Memory Network for Temporal Knowledge Graph Reasoning. (arXiv:2304.12604v1 [cs.AI])

    [http://arxiv.org/abs/2304.12604](http://arxiv.org/abs/2304.12604)

    该论文提出了一种新的框架DaeMon，用于自适应地模拟查询主题和每个对象候选之间的时间路径信息，从而解决了TKG实体数量庞大和实时增加的问题。该方法不依赖于实体表达，并使用注意机制进行记忆的选择性整合。

    

    时间知识图谱推理旨在基于历史信息预测未来的丢失事实，并近年来引起了越来越多的研究兴趣。大量工作已经致力于模拟推理任务的历史结构和时间特征。然而，大规模的真实场景下的TKG实体数量相当可观，随着时间的推移，会出现越来越多的新实体。因此，我们提出了一种新的架构，用于建模TKG的关系特征，即aDAptivE path-MemOry Network（DaeMon），它能够自适应地模拟查询主题与每个对象候选之间的时间路径信息。它可以在不依赖实体表达的情况下模拟历史信息。具体来说，DaeMon使用路径内存记录从时间轴上的路径汇聚单元得到的时间路径信息，考虑每条路径的顺序和权重，并使用注意机制选择性地集成记忆。在两个基准数据集上的实验结果表明了我们方法的有效性。

    Temporal knowledge graph (TKG) reasoning aims to predict the future missing facts based on historical information and has gained increasing research interest recently. Lots of works have been made to model the historical structural and temporal characteristics for the reasoning task. Most existing works model the graph structure mainly depending on entity representation. However, the magnitude of TKG entities in real-world scenarios is considerable, and an increasing number of new entities will arise as time goes on. Therefore, we propose a novel architecture modeling with relation feature of TKG, namely aDAptivE path-MemOry Network (DaeMon), which adaptively models the temporal path information between query subject and each object candidate across history time. It models the historical information without depending on entity representation. Specifically, DaeMon uses path memory to record the temporal path information derived from path aggregation unit across timeline considering the 
    
[^30]: MMRDN: 多视角物体堆叠场景中多视角操作关系检测的一致表示

    MMRDN: Consistent Representation for Multi-View Manipulation Relationship Detection in Object-Stacked Scenes. (arXiv:2304.12592v1 [cs.CV])

    [http://arxiv.org/abs/2304.12592](http://arxiv.org/abs/2304.12592)

    本文提出了一种用于多视角物体堆叠场景中操作关系检测的多视角MRD网络框架，能够通过2D和3D多视角数据学习一致表示，进而指导机器人按正确的顺序抓取物体。

    

    操作关系检测(MRD)旨在指导机器人按正确的顺序抓取物体，这在物体堆叠场景中，确保抓取的安全可靠性非常重要。以往的研究通过用预定义的视角收集的数据，利用深度神经网络来推断操作关系，这在非结构化环境中存在视觉错位的局限性。多视角数据提供了更全面的空间信息，但多视角MRD的挑战是领域偏移。本文提出了一种新的多视角融合框架，即多视角MRD网络(MMRDN)，它是通过2D和3D多视角数据训练的。我们将不同视角的2D数据投影到共同的隐藏空间中，并用一组Von-Mises-Fisher分布拟合嵌入，以学习一致的表示。此外，利用3D数据中的位置信息，我们从每个点云中选择一组$K$个最大垂直邻居(KMVN)点。

    Manipulation relationship detection (MRD) aims to guide the robot to grasp objects in the right order, which is important to ensure the safety and reliability of grasping in object stacked scenes. Previous works infer manipulation relationship by deep neural network trained with data collected from a predefined view, which has limitation in visual dislocation in unstructured environments. Multi-view data provide more comprehensive information in space, while a challenge of multi-view MRD is domain shift. In this paper, we propose a novel multi-view fusion framework, namely multi-view MRD network (MMRDN), which is trained by 2D and 3D multi-view data. We project the 2D data from different views into a common hidden space and fit the embeddings with a set of Von-Mises-Fisher distributions to learn the consistent representations. Besides, taking advantage of position information within the 3D data, we select a set of $K$ Maximum Vertical Neighbors (KMVN) points from the point cloud of eac
    
[^31]: 通过对比学习和一致的语义和结构约束进行无监督合成图像细化

    Unsupervised Synthetic Image Refinement via Contrastive Learning and Consistent Semantic and Structure Constraints. (arXiv:2304.12591v1 [cs.CV])

    [http://arxiv.org/abs/2304.12591](http://arxiv.org/abs/2304.12591)

    本文采用对比学习和一致的语义和结构约束来减少合成和细化图像之间的语义失真，进一步提高了性能。

    

    确保计算机生成的合成图像的真实性对于深度神经网络（DNN）的训练至关重要。由于合成和真实数据集之间存在不同的语义分布，因此合成和细化图像之间存在语义不匹配，进而导致语义失真。最近，对比学习（CL）已成功地用于将相关补丁拉在一起并将不相关的补丁推开。在这项工作中，我们利用合成和精细图像之间的语义和结构一致性，并采用CL来减少语义失真。此外，我们还采用了硬负采样来进一步提高性能。我们使用定性和定量措施比较了我们方法与几种其他基准方法的性能，并表明我们的方法提供了最先进的性能。

    Ensuring the realism of computer-generated synthetic images is crucial to deep neural network (DNN) training. Due to different semantic distributions between synthetic and real-world captured datasets, there exists semantic mismatch between synthetic and refined images, which in turn results in the semantic distortion. Recently, contrastive learning (CL) has been successfully used to pull correlated patches together and push uncorrelated ones apart. In this work, we exploit semantic and structural consistency between synthetic and refined images and adopt CL to reduce the semantic distortion. Besides, we incorporate hard negative mining to improve the performance furthermore. We compare the performance of our method with several other benchmarking methods using qualitative and quantitative measures and show that our method offers the state-of-the-art performance.
    
[^32]: 非平稳环境下动态系统的实时安全评估：方法和技术综述

    Real-time Safety Assessment of Dynamic Systems in Non-stationary Environments: A Review of Methods and Techniques. (arXiv:2304.12583v1 [eess.SY])

    [http://arxiv.org/abs/2304.12583](http://arxiv.org/abs/2304.12583)

    本文综述了非平稳环境下动态系统实时安全评估的方法和技术，包括在线主动学习、在线半监督学习、在线迁移学习和在线异常检测，并讨论了未来的研究方向。

    

    动态系统的实时安全评估是一个关键任务，对于工业和交通应用等各个领域具有重要意义，特别是在非平稳环境中。然而，缺乏非平稳环境下实时安全评估方法的全面综述阻碍了相关方法的进展和改进。本文提供了针对非平稳环境下实时安全评估任务的方法和技术综述。具体而言，首先突出了非平稳环境中实时安全评估方法的背景和重要性。然后，我们提出了问题描述，包括定义、分类和主要挑战。我们还回顾了相关技术的最新发展，如在线主动学习、在线半监督学习、在线迁移学习和在线异常检测。最后，我们讨论了未来的展望和进一步研究的潜在方向。本文的综述旨在为非平稳环境下实时安全评估提供参考依据。

    Real-time safety assessment (RTSA) of dynamic systems is a critical task that has significant implications for various fields such as industrial and transportation applications, especially in non-stationary environments. However, the absence of a comprehensive review of real-time safety assessment methods in non-stationary environments impedes the progress and refinement of related methods. In this paper, a review of methods and techniques for RTSA tasks in non-stationary environments is provided. Specifically, the background and significance of RTSA approaches in non-stationary environments are firstly highlighted. We then present a problem description that covers the definition, classification, and main challenges. We review recent developments in related technologies such as online active learning, online semi-supervised learning, online transfer learning, and online anomaly detection. Finally, we discuss future outlooks and potential directions for further research. Our review aims
    
[^33]: 利用高级循环和张量抽象在CPU架构上通过深度学习和HPC内核

    Harnessing Deep Learning and HPC Kernels via High-Level Loop and Tensor Abstractions on CPU Architectures. (arXiv:2304.12576v1 [cs.DC])

    [http://arxiv.org/abs/2304.12576](http://arxiv.org/abs/2304.12576)

    该论文介绍了一种新的方法来开发适用于现代CPU体系结构的高效、可移植的深度学习和高性能计算内核，使用高级循环和张量抽象。

    

    在过去的十年中，深度学习（DL）算法、编程系统和硬件已经与高性能计算（HPC）相结合。然而，DL和HPC系统的编程方法却停滞不前，依赖于高度优化、特定于平台、僵化的供应商优化库。这项工作介绍了一个框架，用于开发现代CPU架构的高效、可移植的DL和HPC内核。我们将内核开发分解为两个步骤：1）使用张量处理原语（TPP）表达计算核心：一个紧凑、多功能的2D张量运算符，2）以高级、声明性的方式表达TPP周围的逻辑循环，而确切的实例化（顺序，内存布局）则通过将TPL视为黑盒，根据优化目标和约束由自动优化器完成。

    During the past decade, Deep Learning (DL) algorithms, programming systems and hardware have converged with the High Performance Computing (HPC) counterparts. Nevertheless, the programming methodology of DL and HPC systems is stagnant, relying on highly-optimized, yet platform-specific and inflexible vendor-optimized libraries. Such libraries provide close-to-peak performance on specific platforms, kernels and shapes thereof that vendors have dedicated optimizations efforts, while they underperform in the remaining use-cases, yielding non-portable codes with performance glass-jaws. This work introduces a framework to develop efficient, portable DL and HPC kernels for modern CPU architectures. We decompose the kernel development in two steps: 1) Expressing the computational core using Tensor Processing Primitives (TPPs): a compact, versatile set of 2D-tensor operators, 2) Expressing the logical loops around TPPs in a high-level, declarative fashion whereas the exact instantiation (order
    
[^34]: Proto-Value Networks: 通过辅助任务进行表示学习的可扩展性研究

    Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks. (arXiv:2304.12567v1 [cs.LG])

    [http://arxiv.org/abs/2304.12567](http://arxiv.org/abs/2304.12567)

    本文研究了如何通过增加辅助任务的数量和代理网络的大小，提高表示学习的效果。同时，本文还提出了基于后继度量的新型辅助任务家族 Proto-Value 网络。

    

    辅助任务可以提高深度强化学习代理学到的表示能力。尽管其效果已经被相当充分地理论分析，但在实践中，它们主要被用作主要学习目标的支持，而不是作为表示学习的一种方法。基于这一观察，本文研究了辅助任务在学习复杂表示方面的有效性，重点关注同时增加任务数量和代理网络的大小的设置。为此，我们提出了一种基于后继度量的新型辅助任务家族。这些任务易于实现，具有吸引人的理论性质。与合适的离线学习规则结合使用，结果是一个表示学习算法，可以被理解为 Proto-Value 网络。

    Auxiliary tasks improve the representations learned by deep reinforcement learning agents. Analytically, their effect is reasonably well understood; in practice, however, their primary use remains in support of a main learning objective, rather than as a method for learning representations. This is perhaps surprising given that many auxiliary tasks are defined procedurally, and hence can be treated as an essentially infinite source of information about the environment. Based on this observation, we study the effectiveness of auxiliary tasks for learning rich representations, focusing on the setting where the number of tasks and the size of the agent's network are simultaneously increased. For this purpose, we derive a new family of auxiliary tasks based on the successor measure. These tasks are easy to implement and have appealing theoretical properties. Combined with a suitable off-policy learning rule, the result is a representation learning algorithm that can be understood as extend
    
[^35]: 对ChatGPT在需求信息检索中的初步评估

    A Preliminary Evaluation of ChatGPT in Requirements Information Retrieval. (arXiv:2304.12562v1 [cs.SE])

    [http://arxiv.org/abs/2304.12562](http://arxiv.org/abs/2304.12562)

    ChatGPT具备在需求信息检索任务中执行的能力，并且在零-shot设置下获得了可比较或更好的结果。

    

    近期，许多说明性实例显示了ChatGPT在执行编程任务和回答一般领域问题方面的出色能力。本研究目的在于实证评价ChatGPT在需求分析任务中的表现，以此洞察由ChatGPT代表的生成式大语言模型对自然语言处理在需求工程领域的影响。方法是设计了一个评估流程，包括两个常见的需求信息检索任务、包含两种典型需求工件的四个公共数据集、固定任务提示查询ChatGPT，以及定量和定性结果分析。结果显示，ChatGPT在所有数据集中都达到了零-shot设置下可比较或更好的$F\beta$值。定性分析进一步说明了ChatGPT强大的自然语言处理能力和有限的需求工程领域知识。结论是评估了ChatGPT在需求信息检索任务中的表现，并获得了令人满意的结果，突显了其提高需求工程中自然语言处理的潜力。

    Context: Recently, many illustrative examples have shown ChatGPT's impressive ability to perform programming tasks and answer general domain questions.  Objective: We empirically evaluate how ChatGPT performs on requirements analysis tasks to derive insights into how generative large language model, represented by ChatGPT, influence the research and practice of natural language processing for requirements engineering.  Method: We design an evaluation pipeline including two common requirements information retrieval tasks, four public datasets involving two typical requirements artifacts, querying ChatGPT with fixed task prompts, and quantitative and qualitative results analysis.  Results: Quantitative results show that ChatGPT achieves comparable or better $F\beta$ values in all datasets under a zero-shot setting. Qualitative analysis further illustrates ChatGPT's powerful natural language processing ability and limited requirements engineering domain knowledge.  Conclusion: The evaluat
    
[^36]: 训练中结合对手和反对手。

    Combining Adversaries with Anti-adversaries in Training. (arXiv:2304.12550v1 [cs.LG])

    [http://arxiv.org/abs/2304.12550](http://arxiv.org/abs/2304.12550)

    该论文研究了在对抗训练中，通过结合对手和反对手(带有反对手扰动的样本)可以更有效地提高深度神经网络的公平性、鲁棒性和泛化性，在一些特定的学习场景中表现出更好的性能。

    

    对抗训练是提高深度神经网络健壮性的有效学习技术。本研究在更一般的扰动范围下理论上研究了对抗训练对深度学习模型的公平性、鲁棒性和泛化性的影响。我们的理论探索表明，将对手和反对手 (带有反对手扰动的样本) 结合在训练中，在一些典型的学习场景 (如噪声标签学习和不平衡学习) 中能够更有效地实现更好的类别间公平性和鲁棒性和泛化性之间的平衡，相比于标准对抗训练。

    Adversarial training is an effective learning technique to improve the robustness of deep neural networks. In this study, the influence of adversarial training on deep learning models in terms of fairness, robustness, and generalization is theoretically investigated under more general perturbation scope that different samples can have different perturbation directions (the adversarial and anti-adversarial directions) and varied perturbation bounds. Our theoretical explorations suggest that the combination of adversaries and anti-adversaries (samples with anti-adversarial perturbations) in training can be more effective in achieving better fairness between classes and a better tradeoff between robustness and generalization in some typical learning scenarios (e.g., noisy label learning and imbalance learning) compared with standard adversarial training. On the basis of our theoretical findings, a more general learning objective that combines adversaries and anti-adversaries with varied b
    
[^37]: 物体语义信息赋予我们所需的深度: 多任务方法解决航空深度补全问题

    Object Semantics Give Us the Depth We Need: Multi-task Approach to Aerial Depth Completion. (arXiv:2304.12542v1 [cs.CV])

    [http://arxiv.org/abs/2304.12542](http://arxiv.org/abs/2304.12542)

    该论文提出了一种利用对象检测中的语义信息来提高航空深度补全效果的方法，通过基于编码器的多任务学习模型将两个任务在一个模型中执行一次，实现了与KITTI深度补全基准的最先进性能。

    

    深度补全和目标检测是航空三维建图、路径规划和无人机避障等领域中经常使用的两个关键任务。常见方法使用来自LiDAR传感器的测量数据，但生成的点云通常是稀疏和不规则的，限制了系统在三维渲染和安全决策方面的性能。为了解决这个挑战，利用无人机上其他传感器（如用于目标检测的摄像头）的信息来帮助深度补全过程生成更密集的三维模型。同时执行航空深度补全和目标检测任务并融合两个传感器的数据对资源利用效率提出了挑战。我们通过提出一种新颖的方法来解决这个挑战，该方法采用基于编码器的多任务学习模型，将两个任务暴露给共同学习的特征。我们展示了如何从目标检测中提取语义信息改进航空深度补全结果，在流行的KITTI深度补全基准上实现了最先进的性能。

    Depth completion and object detection are two crucial tasks often used for aerial 3D mapping, path planning, and collision avoidance of Uncrewed Aerial Vehicles (UAVs). Common solutions include using measurements from a LiDAR sensor; however, the generated point cloud is often sparse and irregular and limits the system's capabilities in 3D rendering and safety-critical decision-making. To mitigate this challenge, information from other sensors on the UAV (viz., a camera used for object detection) is utilized to help the depth completion process generate denser 3D models. Performing both aerial depth completion and object detection tasks while fusing the data from the two sensors poses a challenge to resource efficiency. We address this challenge by proposing a novel approach to jointly execute the two tasks in a single pass. The proposed method is based on an encoder-focused multi-task learning model that exposes the two tasks to jointly learned features. We demonstrate how semantic ex
    
[^38]: SEA: 用于多智能体强化学习的空间显式体系结构

    SEA: A Spatially Explicit Architecture for Multi-Agent Reinforcement Learning. (arXiv:2304.12532v1 [cs.MA])

    [http://arxiv.org/abs/2304.12532](http://arxiv.org/abs/2304.12532)

    论文提出了一种针对多智能体问题的空间信息提取结构，能有效地共享邻域和全局信息，并能够处理具有可变代理数量的问题。实验结果表明该结构可以提升现有的强化学习算法的效果。

    

    空间信息在许多领域中都十分重要。针对多智能体问题，如何根据代理的空间位置显式建模也非常重要，尤其是当代理数量变化和规模巨大时。本文受计算机视觉中点云任务的启发，提出了一种用于多智能体强化学习的空间信息提取结构。代理可以通过空间编码器-解码器结构有效地共享邻域和全局信息。我们的方法遵循中心化训练、去中心化执行（CTDE）范式。此外，我们的结构可以应用于各种现有的主流强化学习算法，并能够处理具有可变代理数量的问题。在几个多智能体场景的实验中，通过添加我们的空间显式体系结构，现有方法可以得到令人信服的结果。

    Spatial information is essential in various fields. How to explicitly model according to the spatial location of agents is also very important for the multi-agent problem, especially when the number of agents is changing and the scale is enormous. Inspired by the point cloud task in computer vision, we propose a spatial information extraction structure for multi-agent reinforcement learning in this paper. Agents can effectively share the neighborhood and global information through a spatially encoder-decoder structure. Our method follows the centralized training with decentralized execution (CTDE) paradigm. In addition, our structure can be applied to various existing mainstream reinforcement learning algorithms with minor modifications and can deal with the problem with a variable number of agents. The experiments in several multi-agent scenarios show that the existing methods can get convincing results by adding our spatially explicit architecture.
    
[^39]: 基于大型语言模型的语义压缩

    Semantic Compression With Large Language Models. (arXiv:2304.12512v1 [cs.AI])

    [http://arxiv.org/abs/2304.12512](http://arxiv.org/abs/2304.12512)

    本研究使用大型语言模型（LLMs）进行近似压缩研究。具体实验是以GPT-3.5和GPT-4为基础进行的，旨在探索近似压缩的可行性。

    

    大型语言模型的兴起正在彻底改变信息检索、问答、摘要和代码生成等任务。然而，除了有时会自信地呈现事实不准确的信息（称为“幻觉”）外，LLMs的输入和输出令牌数量也天生受限，这使它们在处理大量信息或连续流信息的任务上可能不太有效。减小数据的常见方法是通过无损或有损压缩。然而，在某些情况下，如果能够传达所需的语义精度或意图，就不一定需要从原始数据中恢复每个细节 to。本文提出了三个LLMs研究方面的贡献。首先，我们介绍了使用LLMs进行近似压缩的实验结果，重点关注GPT-3.5和GPT-4通过ChatGPT接口。

    The rise of large language models (LLMs) is revolutionizing information retrieval, question answering, summarization, and code generation tasks. However, in addition to confidently presenting factually inaccurate information at times (known as "hallucinations"), LLMs are also inherently limited by the number of input and output tokens that can be processed at once, making them potentially less effective on tasks that require processing a large set or continuous stream of information. A common approach to reducing the size of data is through lossless or lossy compression. Yet, in some cases it may not be strictly necessary to perfectly recover every detail from the original data, as long as a requisite level of semantic precision or intent is conveyed.  This paper presents three contributions to research on LLMs. First, we present the results from experiments exploring the viability of approximate compression using LLMs, focusing specifically on GPT-3.5 and GPT-4 via ChatGPT interfaces.
    
[^40]: 通过无模型强化学习尽快实现正式规范

    Fulfilling Formal Specifications ASAP by Model-free Reinforcement Learning. (arXiv:2304.12508v1 [cs.LG])

    [http://arxiv.org/abs/2304.12508](http://arxiv.org/abs/2304.12508)

    本文提出了 ASAP-Phi框架，利用分段奖励函数来为未满足规范的轨迹分配定量的语义奖励，并为其余轨迹分配高的常数奖励，使用基于演员-评论家的算法训练代理来尽快实现规范。

    

    本文提出了一个无模型强化学习解决方案，即 ASAP-Phi框架，以鼓励代理尽快满足正式规范。该框架利用一个分段奖励函数来为未满足规范的轨迹分配定量的语义奖励，并为其余轨迹分配高的常数奖励。然后，使用基于演员-评论家的算法（例如软演员-评论家(SAC)或深度确定性策略梯度（DDPG））训练代理。此外，我们证明ASAP-Phi生成的策略优先考虑尽快实现规范。对最先进的基准测试中进行了大量实验，包括消融研究。结果显示，我们的框架成功地为多达97％的测试用例找到了足够快的轨迹，并击败了基线。

    We propose a model-free reinforcement learning solution, namely the ASAP-Phi framework, to encourage an agent to fulfill a formal specification ASAP. The framework leverages a piece-wise reward function that assigns quantitative semantic reward to traces not satisfying the specification, and a high constant reward to the remaining. Then, it trains an agent with an actor-critic-based algorithm, such as soft actor-critic (SAC), or deep deterministic policy gradient (DDPG). Moreover, we prove that ASAP-Phi produces policies that prioritize fulfilling a specification ASAP. Extensive experiments are run, including ablation studies, on state-of-the-art benchmarks. Results show that our framework succeeds in finding sufficiently fast trajectories for up to 97\% test cases and defeats baselines.
    
[^41]: 评估文档图像分类的对抗性鲁棒性

    Evaluating Adversarial Robustness on Document Image Classification. (arXiv:2304.12486v1 [cs.CV])

    [http://arxiv.org/abs/2304.12486](http://arxiv.org/abs/2304.12486)

    本文对文档图像分类任务中的对抗性攻击进行了研究和评估，通过对ResNet50和EfficientNetB0模型架构进行对抗训练、JPEG输入压缩和灰度输入转换等方法，提高了模型的鲁棒性。

    

    近年来，对抗攻击和防御在计算机视觉系统上引起了越来越多的关注，但至今大部分研究仅限于图像。然而，许多人工智能模型实际上处理的是文档数据，这与真实世界的图像非常不同。因此，在本研究中，我们尝试将对抗攻击哲学应用于文献和自然数据，并保护模型免受此类攻击。我们的研究集中在无目标基于梯度、基于转移和基于分数的攻击上，并评估对抗训练、JPEG输入压缩和灰度输入转换对ResNet50和EfficientNetB0模型架构鲁棒性的影响。据我们所知，社区没有进行这样的研究以研究这些攻击对文档图像分类任务的影响。

    Adversarial attacks and defenses have gained increasing interest on computer vision systems in recent years, but as of today, most investigations are limited to images. However, many artificial intelligence models actually handle documentary data, which is very different from real world images. Hence, in this work, we try to apply the adversarial attack philosophy on documentary and natural data and to protect models against such attacks. We focus our work on untargeted gradient-based, transfer-based and score-based attacks and evaluate the impact of adversarial training, JPEG input compression and grey-scale input transformation on the robustness of ResNet50 and EfficientNetB0 model architectures. To the best of our knowledge, no such work has been conducted by the community in order to study the impact of these attacks on the document image classification task.
    
[^42]: DocParser：从视觉丰富的文档中实现端到端的无OCR信息提取

    DocParser: End-to-end OCR-free Information Extraction from Visually Rich Documents. (arXiv:2304.12484v1 [cs.CV])

    [http://arxiv.org/abs/2304.12484](http://arxiv.org/abs/2304.12484)

    DocParser是一种无OCR的端到端信息提取模型，可更好地提取判别性字符特征，并在视觉丰富的文档数据集上实现最新的结果。

    

    从视觉丰富的文档中提取信息是一项具有挑战性的任务，由于其在几个基于文档控制的应用程序中的重要性和其广泛的商业价值，近年来越来越受到关注。迄今为止，在这个主题上进行的大部分研究工作都遵循两步法。首先，他们使用现成的光学字符识别（OCR）引擎读取文本，然后从所获得的文本中提取感兴趣的字段。这些方法的主要缺点是它们依赖于外部OCR系统，这可能会对性能和计算速度产生负面影响。最近，出现了一些无OCR方法来解决上述问题。受到其有希望的结果的启示，我们在本文中提出一种名为DocParser的OCR-free端到端信息提取模型。它通过其更好地提取判别性字符特征的能力不同于先前的端到端方法。DocParser在视觉丰富的文档数据集上实现了最新的结果，超过了依赖于OCR引擎的先前方法。

    Information Extraction from visually rich documents is a challenging task that has gained a lot of attention in recent years due to its importance in several document-control based applications and its widespread commercial value. The majority of the research work conducted on this topic to date follow a two-step pipeline. First, they read the text using an off-the-shelf Optical Character Recognition (OCR) engine, then, they extract the fields of interest from the obtained text. The main drawback of these approaches is their dependence on an external OCR system, which can negatively impact both performance and computational speed. Recent OCR-free methods were proposed to address the previous issues. Inspired by their promising results, we propose in this paper an OCR-free end-to-end information extraction model named DocParser. It differs from prior end-to-end approaches by its ability to better extract discriminative character features. DocParser achieves state-of-the-art results on v
    
[^43]: 用于教育的通用人工智能（AGI）

    Artificial General Intelligence (AGI) for Education. (arXiv:2304.12479v1 [cs.AI])

    [http://arxiv.org/abs/2304.12479](http://arxiv.org/abs/2304.12479)

    AGI技术具有革命教育领域潜力，可以建立e-learning平台、教育协作工具等，弥补传统AI模型因受限于数据和人际交互限制而无法满足教育需求的不足。

    

    由于最新的大型语言模型和聊天机器人（如GPT-4和ChatGPT）的出现，通用人工智能（AGI）作为未来技术已经得到全球认可。AGI旨在通过计算机系统复制人类智能，是具有革命教育领域潜力的关键技术之一。与传统的人工智能模型相比，这些模型通常只针对有限范围的任务进行设计，需要大量特定领域的数据进行训练，可能无法考虑教育中复杂的人际动态。受最近的大规模预训练模型驱动，AGI代表了机器在执行需要人类水平智能的任务方面的重大飞跃，例如推理、解决问题、做出决策，甚至理解人类情感和社交互动。本研究回顾了AGI的关键概念、能力、范围和在未来教育中的潜力，包括建立e-learning平台和教育协作工具等。

    Artificial general intelligence (AGI) has gained global recognition as a future technology due to the emergence of breakthrough large language models and chatbots such as GPT-4 and ChatGPT, respectively. AGI aims to replicate human intelligence through computer systems, which is one of the critical technologies having the potential to revolutionize the field of education. Compared to conventional AI models, typically designed for a limited range of tasks, demand significant amounts of domain-specific data for training and may not always consider intricate interpersonal dynamics in education. AGI, driven by the recent large pre-trained models, represents a significant leap in the capability of machines to perform tasks that require human-level intelligence, such as reasoning, problem-solving, decision-making, and even understanding human emotions and social interactions. This work reviews AGI's key concepts, capabilities, scope, and potential within future education, including setting e
    
[^44]: 关于静态风险度量的动态规划分解

    On Dynamic Program Decompositions of Static Risk Measures. (arXiv:2304.12477v1 [math.OC])

    [http://arxiv.org/abs/2304.12477](http://arxiv.org/abs/2304.12477)

    本文证明了现有的CVaR和EVaR风险度量动态分解是真实风险值的严格高估计，然而VaR存在精确的动态分解。

    

    在马尔科夫决策过程中优化静态风险规避目标具有一定挑战性，因为它们不容易接受动态规划分解。先前的研究建议使用风险度量的动态分解来制定扩展状态空间上的动态规划。本文表明几种现有的分解本质上是不精确的，这与文献中的几个声明相矛盾。特别地，我们举出了一些例子，证明了CVaR和EVaR风险度量的流行分解是真实风险值的严格高估计。然而，VaR确实存在精确的分解，我们提供一个简单的证明，阐明了VaR和CVaR动态规划属性之间的基本差异。

    Optimizing static risk-averse objectives in Markov decision processes is challenging because they do not readily admit dynamic programming decompositions. Prior work has proposed to use a dynamic decomposition of risk measures that help to formulate dynamic programs on an augmented state space. This paper shows that several existing decompositions are inherently inexact, contradicting several claims in the literature. In particular, we give examples that show that popular decompositions for CVaR and EVaR risk measures are strict overestimates of the true risk values. However, an exact decomposition is possible for VaR, and we give a simple proof that illustrates the fundamental difference between VaR and CVaR dynamic programming properties.
    
[^45]: 多智能体MDP中基于概率代理掉线的无模型学习和最优策略设计

    Model-Free Learning and Optimal Policy Design in Multi-Agent MDPs Under Probabilistic Agent Dropout. (arXiv:2304.12458v1 [eess.SY])

    [http://arxiv.org/abs/2304.12458](http://arxiv.org/abs/2304.12458)

    本文研究了多智能体MDP中基于概率代理掉线的情况，并提出了一种无模型算法，能够消除掉线情况需要枚举计算的限制，从而实现计算后掉线系统的最优策略设计。

    

    本文研究了一个多智能体马尔可夫决策过程（MDP），该过程可以经历代理掉线，并基于对于策略的控制和预代理过程的采样来计算后掉线系统的策略。控制器的目标是寻找一个最优策略，使得在已知代理掉出概率的先验知识的情况下，期望系统的价值最大化。对于任何特定的掉线情况下的最优策略是这个问题的一个特例。对于具有特定转换独立性和奖励可分性结构的MDPs，我们假设从系统中移除代理组成了一个新的MDP，由剩余代理组成具有新状态和动作空间的MDP，转换动态消除已删除的代理，奖励与已删除的代理无关。首先我们展示了在这些假设下，对于预掉出系统期望值可以通过一个单一的MDP来表示；这个“鲁棒MDP”能够消除在计算最优策略时要评估所有$2^N$种代理掉线情况的需要。然后我们提出了一个无模型算法，该算法使用蒙特卡罗采样和重要性采样来学习鲁棒MDP，从而能够计算后掉线系统的最优策略。仿真结果展示了该方法的优点。

    This work studies a multi-agent Markov decision process (MDP) that can undergo agent dropout and the computation of policies for the post-dropout system based on control and sampling of the pre-dropout system. The controller's objective is to find an optimal policy that maximizes the value of the expected system given a priori knowledge of the agents' dropout probabilities. Finding an optimal policy for any specific dropout realization is a special case of this problem. For MDPs with a certain transition independence and reward separability structure, we assume that removing agents from the system forms a new MDP comprised of the remaining agents with new state and action spaces, transition dynamics that marginalize the removed agents, and rewards that are independent of the removed agents. We first show that under these assumptions, the value of the expected post-dropout system can be represented by a single MDP; this "robust MDP" eliminates the need to evaluate all $2^N$ realizations
    
[^46]: 水下航行器船体的样本高效和基于代理的设计优化

    Sample-Efficient and Surrogate-Based Design Optimization of Underwater Vehicle Hulls. (arXiv:2304.12420v1 [cs.LG])

    [http://arxiv.org/abs/2304.12420](http://arxiv.org/abs/2304.12420)

    该论文使用了高效的样本集优化和基于代理的方法来设计水下航行器船体，其中代理模型显著提高了计算效率，使优化更加快速准确。

    

    物理模拟是计算机辅助设计(CAD)优化过程中的一个计算瓶颈。因此，为了使精确(计算昂贵)的模拟可用于设计优化中，需要一个高样本效率的优化框架或快速的数据驱动代理(代理模型)来代替长时间运行的模拟。在这项工作中，我们利用最近优化和人工智能(AI)的进展来解决这两个潜在的解决方案，以设计一个最佳的无人水下航行器(UUV)。我们首先研究并比较了不同优化技术在优化循环中与标准计算流体力学(CFD)求解器相结合时的样本效率和收敛行为。然后，我们开发了一个基于深度神经网络(DNN)的代理模型来逼近否则通过CFD求解器进行计算的阻力。代理模型进而用于样本高效的优化框架中，该框架在不使用代理模型的情况下优于标准优化方法。

    Physics simulations are a computational bottleneck in computer-aided design (CAD) optimization processes. Hence, in order to make accurate (computationally expensive) simulations feasible for use in design optimization, one requires either an optimization framework that is highly sample-efficient or fast data-driven proxies (surrogate models) for long running simulations. In this work, we leverage recent advances in optimization and artificial intelligence (AI) to address both of these potential solutions, in the context of designing an optimal unmanned underwater vehicle (UUV). We first investigate and compare the sample efficiency and convergence behavior of different optimization techniques with a standard computational fluid dynamics (CFD) solver in the optimization loop. We then develop a deep neural network (DNN) based surrogate model to approximate drag forces that would otherwise be computed via direct numerical simulation with the CFD solver. The surrogate model is in turn use
    
[^47]: 用于自动驾驶车辆的激光雷达-相机端到端自标定

    End-to-End Lidar-Camera Self-Calibration for Autonomous Vehicles. (arXiv:2304.12412v1 [cs.CV])

    [http://arxiv.org/abs/2304.12412](http://arxiv.org/abs/2304.12412)

    本文提出了一种名为CaLiCa的端到端深度自标定网络，用于联合自动校准针孔相机和激光雷达的固有和外参参数以确保车辆多模式感知传感器的校准质量，同时采用孪生结构以达到领域共享特征的目的。

    

    自动驾驶车辆配备了多模式感知传感器，以确保汽车安全行驶。但是如何在汽车运行期间保持传感器的校准质量成为一个有趣的问题，同时如何联合校准多个传感器以确保系统误差不会传播也是一个挑战。本文提出一种名为CaLiCa的端到端深度自标定网络，针对针孔相机和激光雷达的自动校准问题做出了改进。我们通过回归相机图像和激光点云之间的特征相关性，联合预测相机固有参数(焦距和畸变)以及激光雷达-相机外参参数(旋转和平移)。网络采用孪生结构安排以将网络特征学习约束在点云和相机图像领域的共享特征上。

    Autonomous vehicles are equipped with a multi-modal sensor setup to enable the car to drive safely. The initial calibration of such perception sensors is a highly matured topic and is routinely done in an automated factory environment. However, an intriguing question arises on how to maintain the calibration quality throughout the vehicle's operating duration. Another challenge is to calibrate multiple sensors jointly to ensure no propagation of systemic errors. In this paper, we propose CaLiCa, an end-to-end deep self-calibration network which addresses the automatic calibration problem for pinhole camera and Lidar. We jointly predict the camera intrinsic parameters (focal length and distortion) as well as Lidar-Camera extrinsic parameters (rotation and translation), by regressing feature correlation between the camera image and the Lidar point cloud. The network is arranged in a Siamese-twin structure to constrain the network features learning to a mutually shared feature in both poi
    
[^48]: PEFT-Ref: 一种模块化的参考架构和类型，用于参数效率微调技术

    PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques. (arXiv:2304.12410v1 [cs.CL])

    [http://arxiv.org/abs/2304.12410](http://arxiv.org/abs/2304.12410)

    本文提出了PEFT-Ref参考架构，标准化了不同PEFT技术共享的方面，隔离了差异到特定位置和交互中，模块化的视图有助于比较不同技术及其效率和任务性能，并有助于更好地理解PEFT的基本原理。

    

    最近的参数效率微调(PEFT)技术旨在改善完全微调大型预训练语言模型(PLM)的高昂成本。随着不同的PEFT技术不断出现，对它们进行比较变得越来越困难，特别是在以下方面：(i)它们添加到PLM的结构和功能，(ii)不同类型和程度的效率改进，(iii)在不同的下游任务中的性能，以及(iv)结构和功能差异如何与效率和任务性能相关联。为了促进这样的比较，本文提出了一个参考框架，标准化了不同PEFT技术共享的方面，同时将差异隔离到与标准组件的特定位置和交互中。通过这个标准化和隔离差异的过程，出现了PEFT技术的模块化视图，不仅支持直接比较不同技术及其效率和任务性能，而且还有助于更好地理解PEFT的基本原理。所提出的参考架构称为PEFT-Ref，包括七个核心模块，每个模块都处理PEFT的特定方面，并可用作开发新PEFT技术和比较现有技术的指南。

    Recent parameter-efficient finetuning (PEFT) techniques aim to improve over the considerable cost of fully finetuning large pretrained language models (PLM). As different PEFT techniques proliferate, it is becoming difficult to compare them, in particular in terms of (i) the structure and functionality they add to the PLM, (ii) the different types and degrees of efficiency improvements achieved, (iii) performance at different downstream tasks, and (iv) how differences in structure and functionality relate to efficiency and task performance. To facilitate such comparisons, this paper presents a reference framework which standardises aspects shared by different PEFT techniques, while isolating differences to specific locations and interactions with the standard components. Through this process of standardising and isolating differences, a modular view of PEFT techniques emerges, supporting not only direct comparison of different techniques and their efficiency and task performance, but a
    
[^49]: 论使用黑盒API进行毒性评估的挑战

    On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research. (arXiv:2304.12397v1 [cs.CL])

    [http://arxiv.org/abs/2304.12397](http://arxiv.org/abs/2304.12397)

    本文讨论使用黑盒API进行毒性评估的挑战，发现依赖继承的自动毒性评分可能导致不准确的结果，建议采用更加结构化的方法评估毒性随时间变化的模型和方法。

    

    对毒性的感知随时间推移而不断演变，而且在不同的地理和文化背景中往往存在差异。同样，用于检测毒性的商业黑盒API（例如Perspective API）也不是静态的，而经常重新训练以解决任何未被关注的弱点和偏见。我们评估了这些变化对比较旨在遏制毒性的模型和方法的相对优劣的研究发现的可重复性的影响。我们的发现表明，依赖继承的自动毒性评分来比较模型和技术的研究可能导致不准确的结果。重新对HELM的所有模型进行最新版本API的毒性评分，导致了广泛使用的基础模型的不同排名。我们建议在将研究之间进行马蜂拼接型比较时要谨慎，并为评估毒性随时间变化的更加有结构化的方法提出建议。代码和数据可在https://github.com/X/XXX上找到。

    Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We evaluate the implications of these changes on the reproducibility of findings that compare the relative merits of models and methods that aim to curb toxicity. Our findings suggest that research that relied on inherited automatic toxicity scores to compare models and techniques may have resulted in inaccurate findings. Rescoring all models from HELM, a widely respected living benchmark, for toxicity with the recent version of the API led to a different ranking of widely used foundation models. We suggest caution in applying apples-to-apples comparisons between studies and lay recommendations for a more structured approach to evaluating toxicity over time. Code and data are available at https
    
[^50]: 问题回答中的答案类型预测的极限分类

    Extreme Classification for Answer Type Prediction in Question Answering. (arXiv:2304.12395v1 [cs.CL])

    [http://arxiv.org/abs/2304.12395](http://arxiv.org/abs/2304.12395)

    本文提出了使用Transformer模型（XBERT）进行极端多标签分类，通过将KG类型基于问题文本使用结构和语义特征进行聚类，以提高问题回答（QA）系统中语义答案类型预测（SMART）任务的性能，并获得最先进的结果。

    

    语义答案类型预测（SMART）已被证明是有效的问题回答（QA）系统的有用步骤。 SMART任务涉及预测给定自然语言问题的前k个知识图（KG）类型。由于KG中存在大量类型，这是具有挑战性的。在本文中，我们提出使用Transformer模型（XBERT）进行极端多标签分类，通过将KG类型基于问题文本使用结构和语义特征进行聚类。我们具体地改善了XBERT流程的聚类阶段，利用从KG中派生的文本和结构特征。我们表明，这些特征可以提高SMART任务的端到端性能，并产生最先进的结果。

    Semantic answer type prediction (SMART) is known to be a useful step towards effective question answering (QA) systems. The SMART task involves predicting the top-$k$ knowledge graph (KG) types for a given natural language question. This is challenging due to the large number of types in KGs. In this paper, we propose use of extreme multi-label classification using Transformer models (XBERT) by clustering KG types using structural and semantic features based on question text. We specifically improve the clustering stage of the XBERT pipeline using textual and structural features derived from KGs. We show that these features can improve end-to-end performance for the SMART task, and yield state-of-the-art results.
    
[^51]: Virus2Vec: 利用机器学习进行病毒序列分类

    Virus2Vec: Viral Sequence Classification Using Machine Learning. (arXiv:2304.12328v1 [q-bio.GN])

    [http://arxiv.org/abs/2304.12328](http://arxiv.org/abs/2304.12328)

    Virus2Vec 是一种机器学习方法，使用特征向量表示病毒序列，能够识别病毒宿主，其在病毒宿主预测上比当前最先进方法提高了多达16％的精度。

    

    理解不同病毒家族的宿主特异性可以揭示 SARS-CoV-2、狂犬病等动物源性病原体在人类中的起源。这有助于流行病学家、医疗专业人员和政策制定者及时遏制现有的流行病并预防未来的流行病。我们提出了Virus2Vec，它是病毒（核苷酸或氨基酸）序列的特征向量表示，可以让基于向量空间的机器学习模型识别病毒宿主。与当前最先进的方法相比，我们的方法在冠状病毒科和狂犬病毒的宿主预测上将精度提高了多达16％。

    Understanding the host-specificity of different families of viruses sheds light on the origin of, e.g., SARS-CoV-2, rabies, and other such zoonotic pathogens in humans. It enables epidemiologists, medical professionals, and policymakers to curb existing epidemics and prevent future ones promptly. In the family Coronaviridae (of which SARS-CoV-2 is a member), it is well-known that the spike protein is the point of contact between the virus and the host cell membrane. On the other hand, the two traditional mammalian orders, Carnivora (carnivores) and Chiroptera (bats) are recognized to be responsible for maintaining and spreading the Rabies Lyssavirus (RABV). We propose Virus2Vec, a feature-vector representation for viral (nucleotide or amino acid) sequences that enable vector-space-based machine learning models to identify viral hosts. Virus2Vec generates numerical feature vectors for unaligned sequences, allowing us to forego the computationally expensive sequence alignment step from t
    
[^52]: 学习技术在海洋叶绿素分析中的物理化学特性依赖性研究

    Dependence of Physiochemical Features on Marine Chlorophyll Analysis with Learning Techniques. (arXiv:2304.12325v1 [q-bio.QM])

    [http://arxiv.org/abs/2304.12325](http://arxiv.org/abs/2304.12325)

    研究发现，海洋中叶绿素的生长与物理化学成分如铁、硝酸盐、磷酸盐、pH值、盐度等的最佳浓度有关，可以用机器学习预测海洋叶绿素。

    

    海洋浮游植物中存在的叶绿素是光合作用的基础，对维持生态平衡具有重要意义，对全球初级生产力作出了重大贡献，并处于许多海洋生物的食物链中。浮游植物浓度的不平衡可能会破坏生态平衡。浮游植物的生长取决于物理化学成分的最佳浓度，如铁，硝酸盐，磷酸盐，pH值，盐度等的偏离理想浓度可能会影响浮游植物的生长，从而最终破坏生态系统。因此，分析这些成分具有极高的重要性，以估计海洋浮游植物的可能生长。遥感技术的进步改善了全球范围内远程研究物理化学成分的可能性。机器学习技术使得预测海洋叶绿素成为可能。

    Marine chlorophyll which is present within phytoplankton are the basis of photosynthesis and they have a high significance in sustaining ecological balance as they highly contribute toward global primary productivity and comes under the food chain of many marine organisms. Imbalance in the concentrations of phytoplankton can disrupt the ecological balance. The growth of phytoplankton depends upon the optimum concentrations of physiochemical constituents like iron, nitrates, phosphates, pH level, salinity, etc. and deviations from an ideal concentration can affect the growth of phytoplankton which can ultimately disrupt the ecosystem at a large scale. Thus the analysis of such constituents has high significance to estimate the probable growth of marine phytoplankton. The advancements of remote sensing technologies have improved the scope to remotely study the physiochemical constituents on a global scale. The machine learning techniques have made it possible to predict the marine chloro
    
[^53]: USA-Net：机器人记忆的统一语义和作用表示

    USA-Net: Unified Semantic and Affordance Representations for Robot Memory. (arXiv:2304.12164v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2304.12164](http://arxiv.org/abs/2304.12164)

    本文提出了一种名为USA-Net的方法，用于构建可微分地图的世界表示，编码了场景的语义和空间作用，从而使机器人能够按照自然语言指令移动。该系统是第一个端到端可微的计划系统，可以处理开放式自然语言命令而无需进行任何特定任务的手动工程。

    

    为了让机器人能够按照“去打开水槽上方的棕色橱柜”等开放式指令行动，它们需要理解场景几何形状和环境语义。机器人系统通常通过分开不同的流程来处理这些指令，有时使用非常不同的表示空间，当两个目标冲突时这可能是次优的。本文介绍USA-Net，这是一种简单的方法，用于构建可微分地图的世界表示，该地图编码了场景的语义和空间作用，从而使我们能够构建基于梯度的规划器，该规划器可以导航到使用开放式词汇指定的场景位置。在CLIP嵌入空间中，我们使用此规划器一贯生成较短（5-10%）和更接近目标查询（10-30%）的路径，而这些路径来自不利用梯度信息的可比较基于网格的规划器，据我们所知，这是第一个端到端可微的计划系统，可以处理开放式自然语言命令而无需进行任何特定任务的手动工程。

    In order for robots to follow open-ended instructions like "go open the brown cabinet over the sink", they require an understanding of both the scene geometry and the semantics of their environment. Robotic systems often handle these through separate pipelines, sometimes using very different representation spaces, which can be suboptimal when the two objectives conflict. In this work, we present USA-Net, a simple method for constructing a world representation that encodes both the semantics and spatial affordances of a scene in a differentiable map. This allows us to build a gradient-based planner which can navigate to locations in the scene specified using open-ended vocabulary. We use this planner to consistently generate trajectories which are both shorter 5-10% shorter and 10-30% closer to our goal query in CLIP embedding space than paths from comparable grid-based planners which don't leverage gradient information. To our knowledge, this is the first end-to-end differentiable plan
    
[^54]: 论可解释人工智能的跨文化伦理实践

    Towards a Praxis for Intercultural Ethics in Explainable AI. (arXiv:2304.11861v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2304.11861](http://arxiv.org/abs/2304.11861)

    该论文介绍了一种可解释人工智能的跨文化伦理实践方法，研究了文化差异如何影响技术的采纳和使用，并探讨了解释技术概念如何帮助那些在社会和文化多样性地区中具有不同需求的用户。

    

    可解释人工智能（XAI）通常被宣传为帮助用户理解机器学习模型的功能和预测产生的原因。但是，这些好处大多为那些具有专业领域知识的人所保留，比如机器学习开发人员。最近的研究认为，使AI可解释可能是在现实世界中使AI更有用的一种可行方式，尤其是在全球南方低资源领域内。尽管AI已经跨越了国界，但很少有研究关注将解释AI概念民主化到“大多数世界”内，这留给我们在这个领域探索和开发新方法的机会，以满足在文化和社交多样化的地区中具有不同需求的用户。本文介绍了一种跨文化伦理实践的AI可解释方法。它研究了文化细微差别如何影响技术采纳和使用，以及阻碍解释技术概念如AI的因素。

    Explainable AI (XAI) is often promoted with the idea of helping users understand how machine learning models function and produce predictions. Still, most of these benefits are reserved for those with specialized domain knowledge, such as machine learning developers. Recent research has argued that making AI explainable can be a viable way of making AI more useful in real-world contexts, especially within low-resource domains in the Global South. While AI has transcended borders, a limited amount of work focuses on democratizing the concept of explainable AI to the "majority world", leaving much room to explore and develop new approaches within this space that cater to the distinct needs of users within culturally and socially-diverse regions. This article introduces the concept of an intercultural ethics approach to AI explainability. It examines how cultural nuances impact the adoption and use of technology, the factors that impede how technical concepts such as AI are explained, and
    
[^55]: 分级扩散自编码器和分解图像操作

    Hierarchical Diffusion Autoencoders and Disentangled Image Manipulation. (arXiv:2304.11829v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.11829](http://arxiv.org/abs/2304.11829)

    本文提出了一种分级扩散自编码器 (HDAE) ，利用细粒度到抽象和低级到高级的特征层次结构来构建扩散模型的分层潜在空间。该方法更全面地编码了不同抽象层次的语义，并提供了一种基于截断特征的分解图像操作方法。

    

    扩散模型在图像合成方面已经取得了令人印象深刻的视觉质量。然而，如何解释和操作扩散模型的潜在空间并没有被广泛探索。之前的工作中，扩散自编码器将语义表示编码到一个语义潜在代码中，但不能反映细节和内在特征层次的丰富信息。为了缓解这些限制，我们提出了分级扩散自编码器 (HDAE)，利用细粒度到抽象和低级到高级的特征层次结构来构建扩散模型的潜在空间。 HDAE 的分层潜在空间固有地编码了不同抽象层次的语义，并提供了更全面的语义表示。此外，我们提出了一种基于截断特征的分解图像操作方法。通过大量实验和应用，我们证明了我们提出的方法的有效性，包括图片重建、风格混合、可控生成和图像编辑等。

    Diffusion models have attained impressive visual quality for image synthesis. However, how to interpret and manipulate the latent space of diffusion models has not been extensively explored. Prior work diffusion autoencoders encode the semantic representations into a semantic latent code, which fails to reflect the rich information of details and the intrinsic feature hierarchy. To mitigate those limitations, we propose Hierarchical Diffusion Autoencoders (HDAE) that exploit the fine-grained-to-abstract and lowlevel-to-high-level feature hierarchy for the latent space of diffusion models. The hierarchical latent space of HDAE inherently encodes different abstract levels of semantics and provides more comprehensive semantic representations. In addition, we propose a truncated-feature-based approach for disentangled image manipulation. We demonstrate the effectiveness of our proposed approach with extensive experiments and applications on image reconstruction, style mixing, controllable 
    
[^56]: 通过伦理和哲学原则确保可信赖的医疗人工智能

    Ensuring Trustworthy Medical Artificial Intelligencethrough Ethical and Philosophical Principles. (arXiv:2304.11530v1 [cs.AI])

    [http://arxiv.org/abs/2304.11530](http://arxiv.org/abs/2304.11530)

    本文讨论了人工智能在医疗保健中的应用和考虑伦理和哲学原则以确保可靠的人工智能工具的重要性。人工智能在医疗中带来了更多挑战，必须解决偏见、透明度、自主权、责任和问责制等问题，作者提出了可能的解决办法。

    

    人工智能方法在医疗护理方面具有极大的潜力，可以通过提高医疗专家和患者的体验来彻底改变众多医疗护理。基于人工智能的计算机辅助诊断工具如果能够表现出色甚至与临床专家的水平相当，就可以产生巨大的效益。因此，发展中国家可以提供先进的医疗护理服务，并解决缺乏专业医疗从业者的问题。基于人工智能的工具可以节省时间、资源和整体治疗成本。此外，与人类相比，人工智能可以揭示大量输入数据中的复杂关系，甚至可以为医学提供新的基于证据的知识。然而，在医疗护理中整合人工智能也带来了几个伦理和哲学上的问题，如偏见、透明度、自主权、责任和问责制，这些问题必须在将这些工具整合到临床环境之前得到解决。在本文中，我们强调了人工智能在医疗护理中的最新应用以及考虑伦理和哲学原则以确保可信赖的人工智能工具的重要性。我们讨论了与医疗护理中的人工智能相关的各种挑战，包括数据偏见、透明度的需要、自主决策的问题以及问责制。我们还提出了解决这些挑战的潜在方案，包括确保透明度和问责制的框架以及指导人工智能开发者考虑伦理原则的指南。通过解决这些挑战并实施伦理和哲学原则，我们可以确保开发出符合诊所设置的受信任的医疗人工智能。

    Artificial intelligence (AI) methods have great potential to revolutionize numerous medical care by enhancing the experience of medical experts and patients. AI based computer-assisted diagnosis tools can have a tremendous benefit if they can outperform or perform similarly to the level of a clinical expert. As a result, advanced healthcare services can be affordable in developing nations, and the problem of a lack of expert medical practitioners can be addressed. AI based tools can save time, resources, and overall cost for patient treatment. Furthermore, in contrast to humans, AI can uncover complex relations in the data from a large set of inputs and even lead to new evidence-based knowledge in medicine. However, integrating AI in healthcare raises several ethical and philosophical concerns, such as bias, transparency, autonomy, responsibility and accountability, which must be addressed before integrating such tools into clinical settings. In this article, we emphasize recent advanc
    
[^57]: 通过提示提高大型语言模型的心智理论表现

    Boosting Theory-of-Mind Performance in Large Language Models via Prompting. (arXiv:2304.11490v1 [cs.AI])

    [http://arxiv.org/abs/2304.11490](http://arxiv.org/abs/2304.11490)

    本研究通过提示提高大型语言模型（LLMs）在心智理论（ToM）任务上的表现，证明了上下文学习可以提升LLMs在复杂推理特别是ToM任务中的表现。

    

    2023年，大型语言模型（LLMs）在许多任务中表现出色，但在复杂推理方面仍面临挑战。心智理论（ToM）任务需要理解代理人的信念、目标和心理状态，对于涉及人类的常识推理至关重要，因此提高LLM在这方面的表现至关重要。本研究测量了GPT-4和三个GPT-3.5变体（Davinci-2、Davinci-3、GPT-3.5-Turbo）的ToM表现，并研究了上下文学习提高它们的ToM理解力的有效性。我们评估了包含两步思维推理和逐步思考说明的提示。我们发现，通过人类反馈的强化学习（RLHF）训练的LLMs（除Davinci-2外的所有模型）通过上下文学习提高了它们的ToM准确性。GPT-4在零轮情况下表现最佳，达到了近80%的ToM准确性，但仍不足测试集上87%的人类准确性。然而，当提供上下文学习的提示时，GPT-4和三个GPT-3.5变体的ToM准确性显著高于无提示时，其中表现最好的模型（GPT-3.5-Turbo）达到了92%的准确性。我们的研究展示了上下文学习提升LLM在复杂推理尤其是ToM任务中表现的潜力。

    Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-c
    
[^58]: 颗粒球计算：一种高效、鲁棒和可解释的自适应多粒度表示和计算方法

    Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v1 [cs.LG])

    [http://arxiv.org/abs/2304.11171](http://arxiv.org/abs/2304.11171)

    本文提出了一种基于颗粒球计算的自适应多粒度表示和计算方法，能够提高机器学习的效率、鲁棒性和可解释性。

    

    人类认知具有“先大后小”的认知机制，因此具有自适应的多粒度描述能力。这导致了有效性、鲁棒性和可解释性等计算特性。本文提出了一种新的基于颗粒球计算的自适应多粒度表示和计算方法。他们将这种方法应用于几个机器学习任务，并证明其相对于其他最先进的方法的有效性。

    Human cognition has a ``large-scale first'' cognitive mechanism, therefore possesses adaptive multi-granularity description capabilities. This results in computational characteristics such as efficiency, robustness, and interpretability. Although most existing artificial intelligence learning methods have certain multi-granularity features, they do not fully align with the ``large-scale first'' cognitive mechanism. Multi-granularity granular-ball computing is an important model method developed in recent years. This method can use granular-balls of different sizes to adaptively represent and cover the sample space, and perform learning based on granular-balls. Since the number of coarse-grained "granular-ball" is smaller than the number of sample points, granular-ball computing is more efficient; the coarse-grained characteristics of granular-balls are less likely to be affected by fine-grained sample points, making them more robust; the multi-granularity structure of granular-balls ca
    
[^59]: 缺失数据下的交通信号控制的强化学习方法

    Reinforcement Learning Approaches for Traffic Signal Control under Missing Data. (arXiv:2304.10722v1 [cs.LG])

    [http://arxiv.org/abs/2304.10722](http://arxiv.org/abs/2304.10722)

    本文提出了在交通路网中缺少传感器的情况下，使用强化学习方法通过补充流量状态或状态和动作来实现自适应控制和条件融合。

    

    强化学习方法在交通信号控制任务中的应用已经取得了比传统的基于规则的方法更好的性能。然而，现实中交通状态的缺失可能经常发生，使得现有的强化学习方法在缺少传感器的路网上无法应用。本文旨在控制交通信号在现实环境下的设置中，其中一些路口没有安装传感器，因此周围没有直接观察数据。在我们所知道的范围内，我们是第一个使用强化学习方法来解决这个现实世界中的交通信号控制问题。具体地，我们提出了两种解决方案：第一种方案补充流量状态以实现自适应控制，第二种方案补充状态和动作以进行条件融合。

    The emergence of reinforcement learning (RL) methods in traffic signal control tasks has achieved better performance than conventional rule-based approaches. Most RL approaches require the observation of the environment for the agent to decide which action is optimal for a long-term reward. However, in real-world urban scenarios, missing observation of traffic states may frequently occur due to the lack of sensors, which makes existing RL methods inapplicable on road networks with missing observation. In this work, we aim to control the traffic signals in a real-world setting, where some of the intersections in the road network are not installed with sensors and thus with no direct observations around them. To the best of our knowledge, we are the first to use RL methods to tackle the traffic signal control problem in this real-world setting. Specifically, we propose two solutions: the first one imputes the traffic states to enable adaptive control, and the second one imputes both stat
    
[^60]: 大规模机器学习中Adam不稳定性的理论研究

    A Theory on Adam Instability in Large-Scale Machine Learning. (arXiv:2304.09871v1 [cs.LG])

    [http://arxiv.org/abs/2304.09871](http://arxiv.org/abs/2304.09871)

    Adam优化算法在大批量的训练下容易出现不稳定现象，并导致训练异常，作者提出了该现象的理论解释。

    

    本文提出了一个之前未被解释的现象的理论，该现象出现在大型语言模型训练时的发散行为中。我们认为这种现象是由于主流的优化算法 Adam 导致的。我们观察到 Adam 可能会进入一种状态，其中参数更新向量有比较大的范数，并且与训练损失景观下的下降方向基本无关，从而导致发散。这种现象更容易在大批量情况下出现，这也是大型语言模型训练的典型设置。为了证明该理论，我们对规模不同的语言模型（70亿，300亿，650亿和5460亿参数）进行了训练运行的观察。

    We present a theory for the previously unexplained divergent behavior noticed in the training of large language models. We argue that the phenomenon is an artifact of the dominant optimization algorithm used for training, called Adam. We observe that Adam can enter a state in which the parameter update vector has a relatively large norm and is essentially uncorrelated with the direction of descent on the training loss landscape, leading to divergence. This artifact is more likely to be observed in the training of a deep model with a large batch size, which is the typical setting of large-scale language model training. To argue the theory, we present observations from the training runs of the language models of different scales: 7 billion, 30 billion, 65 billion, and 546 billion parameters.
    
[^61]: 通过模块化线性化注意力机制改进自回归自然语言处理任务

    Improving Autoregressive NLP Tasks via Modular Linearized Attention. (arXiv:2304.08453v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.08453](http://arxiv.org/abs/2304.08453)

    本文提出模块化线性化注意力机制（MLA）以最大化推理质量并实现速度提升，并在多个自回归自然语言处理任务上验证了该方法，包括语音到文本神经机器翻译（S2T NMT）、语音到文本同声传译（SimulST）和自回归文本到频谱图任务，在TTS上具有高效率收益，在NMT和SimulST的训练和推理过程中表现出竞争性能。

    

    多种自然语言处理任务需要的模型必须在最终应用于边缘或其他资源受限制的环境中高效且小型。尽管先前的研究已将这些模型的大小减小，但在不影响性能的前提下提高计算效率仍然很困难，特别是对于自回归任务而言。本文提出了一种模块化线性化注意力机制（MLA），它结合了多个有效的注意力机制，包括cosFormer，以最大化推理质量并实现显着的速度提升。我们在几个自回归自然语言处理任务上验证了这种方法，包括语音到文本神经机器翻译（S2T NMT）、语音到文本同声传译（SimulST）和自回归文本到频谱图任务，在TTS上具有高效率收益，在NMT和SimulST的训练和推理过程中表现出竞争性能。

    Various natural language processing (NLP) tasks necessitate models that are efficient and small based on their ultimate application at the edge or in other resource-constrained environments. While prior research has reduced the size of these models, increasing computational efficiency without considerable performance impacts remains difficult, especially for autoregressive tasks. This paper proposes {modular linearized attention (MLA), which combines multiple efficient attention mechanisms, including cosFormer, to maximize inference quality while achieving notable speedups. We validate this approach on several autoregressive NLP tasks, including speech-to-text neural machine translation (S2T NMT), speech-to-text simultaneous translation (SimulST), and autoregressive text-to-spectrogram, noting efficiency gains on TTS and competitive performance for NMT and SimulST during training and inference.
    
[^62]: 中文开放式指令广义语言模型：初步发布

    Chinese Open Instruction Generalist: A Preliminary Release. (arXiv:2304.07987v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.07987](http://arxiv.org/abs/2304.07987)

    本论文旨在通过适应不同子任务的固有特性，创建一个中文指令数据集，以填补指令调整技术在中文语言领域的空白。

    

    指令调整被广泛认为是构建广义语言模型的关键技术，随着InstructGPT和ChatGPT的发布，它已经引起了研究人员和公众的关注。尽管英语为基础的大规模语言模型取得了令人瞩目的进展，但是还未探索英语为基础的语言模型在多语任务上是否可以像英语任务那样通过精心设计的指令调整来执行，以及我们如何构建所需的语料库进行调整。为填补这一空白，我们提出了一个项目，试图通过适应4个子任务的固有特性，采用各种方法创建一个中文指令数据集。我们收集了约20万个中文指令调整样本，并进行了人工检查以确保高质量。我们还总结了现有的英文和中文指令语料库，并对一些潜在的应用进行了简要描述。

    Instruction tuning is widely recognized as a key technique for building generalist language models, which has attracted the attention of researchers and the public with the release of InstructGPT~\citep{ouyang2022training} and ChatGPT\footnote{\url{https://chat.openai.com/}}. Despite impressive progress in English-oriented large-scale language models (LLMs), it is still under-explored whether English-based foundation LLMs can perform similarly on multilingual tasks compared to English tasks with well-designed instruction tuning and how we can construct the corpora needed for the tuning.  To remedy this gap, we propose the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks. We collect around 200k Chinese instruction tuning samples, which have been manually checked to guarantee high quality. We also summarize the existing English and Chinese instruction corpora and briefly describe some potential applic
    
[^63]: 基于属性识别实体类型

    Recognizing Entity Types via Properties. (arXiv:2304.07910v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2304.07910](http://arxiv.org/abs/2304.07910)

    本文提出一种基于属性的方法，可根据用于定义它们的属性来识别实体类型。主要贡献包括一组基于属性的度量和一种机器学习的实体类型识别算法。

    

    建立本体的主流方法是合并编码不同信息的本体，其中一个主要困难是异构性促使本体合并但也限制高质量的合并性能。因此，提出实体类型识别任务以处理这种异构性，旨在通过利用本体中编码的信息推断实体和实体类型的类别。本文介绍了一种基于属性的方法，允许根据用于定义它们的属性来识别实体类型。从认识论的角度来看，实际上是属性表征实体和实体类型，这个定义独立于用于定义它们的具体标签和层次结构。主要贡献包括一组基于属性的度量，以衡量实体类型和实体之间的上下文相似性，并利用机器学习的实体类型识别算法exploiting p。

    The mainstream approach to the development of ontologies is merging ontologies encoding different information, where one of the major difficulties is that the heterogeneity motivates the ontology merging but also limits high-quality merging performance. Thus, the entity type (etype) recognition task is proposed to deal with such heterogeneity, aiming to infer the class of entities and etypes by exploiting the information encoded in ontologies. In this paper, we introduce a property-based approach that allows recognizing etypes on the basis of the properties used to define them. From an epistemological point of view, it is in fact properties that characterize entities and etypes, and this definition is independent of the specific labels and hierarchical schemas used to define them. The main contribution consists of a set of property-based metrics for measuring the contextual similarity between etypes and entities, and a machine learning-based etype recognition algorithm exploiting the p
    
[^64]: 大型语言模型综述

    A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])

    [http://arxiv.org/abs/2303.18223](http://arxiv.org/abs/2303.18223)

    本文综述了大型语言模型的研究历程以及最近的预训练语言模型(PLMs)，并强调模型扩展将带来性能改进和特殊能力的发掘。

    

    语言本质上是一个由语法规则控制的复杂精细的人类表达系统，对于开发理解和掌握语言的能力的AI算法来说是一项重大挑战。作为主要方法之一，语言建模在过去二十年里广泛研究用于语言理解和生成，从统计语言模型演化为神经语言模型。最近，通过在大规模语料库上预训练Transformer模型，提出了预训练语言模型（PLMs），在解决各种NLP任务方面显示出强大的能力。由于研究人员发现模型缩放可以导致性能改进，他们进一步通过增加模型规模来研究缩放效应，有趣的是，当参数规模超过一定水平时，这些扩大的语言模型不仅可以实现显着的性能提升，而且还显示出一些小规模语言模型所没有的特殊能力。

    Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
    
[^65]: 一人独舞好还是人多闹心？不同演示次数下的上下文训练

    It Takes One to Tango but More Make Trouble? In-Context Training with Different Number of Demonstrations. (arXiv:2303.08119v1 [cs.AI])

    [http://arxiv.org/abs/2303.08119](http://arxiv.org/abs/2303.08119)

    本文研究了使用较少的演示数进行上下文学习（ICL）的任务，在测试查询上只使用一个随机选择的演示时并没有明显性能下降，而只使用一个正确演示的ICL在性能上显著优于全演示ICL。

    

    大型语言模型在提供了一些输入输出演示（demos）并给出更多演示的中间推理步骤（“思路链（CoT）”）时，能够通过上下文学习（ICL）进行复杂推理。本文研究了在每个测试查询上使用较少的演示来进行ICL的任务~\cite{wei2022chain}。惊人地，当只使用一个随机选择的演示时，我们并没有观察到明显的性能下降。为了研究这种现象，对于每个测试查询，我们将演示分类为“正确演示”和“错误演示”。我们的分析揭示了这些广泛研究的数据集中存在的固有偏差：大多数测试查询的大多数演示都是正确的，这解释了使用一个随机演示时表现良好的原因。此外，只使用一个正确演示的ICL（带和不带CoT）在性能上显著优于大多数先前工作采用的全演示ICL，表明演示数量并不总是更好。

    Large language models (LLMs) are capable to perform complex reasoning by in-context learning (ICL) when provided with a few input-output demonstrations (demos) and more powerful when intermediate reasoning steps ("chain of thoughts (CoT)") of the demos are given. Is it necessary to use multi-demo in ICL? In this paper, we study ICL using fewer demos for each test query on the tasks in~\cite{wei2022chain}. Surprisingly, we do not observe significant degradation when using only one randomly chosen demo. To study this phenomenon, for each test query, we categorize demos into "correct demos" leading to the correct answer, and "wrong demos" resulting in wrong answers. Our analysis reveals an inherent bias in those widely studied datasets: most demos are correct for a majority of test queries, which explains the good performance of using one random demo. Moreover, ICL (with and w/o CoT) using only one correct demo significantly outperforms all-demo ICL adopted by most previous works, indicat
    
[^66]: 使用变分自编码器学习自主机器人最后一英里可靠监控的有效搜索空间

    Using a Variational Autoencoder to Learn Valid Search Spaces of Safely Monitored Autonomous Robots for Last-Mile Delivery. (arXiv:2303.03211v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2303.03211](http://arxiv.org/abs/2303.03211)

    本文使用 COIL 方法研究了有效的自主机器人配送问题，确保在保证安全监控的前提下最大化机器人配送效率。

    

    使用自主机器人送货给客户是一种可靠且可持续的服务方式。然而，在实际情况下，为了安全起见，自主机器人仍需要人类监督。本文解决了在保证安全监控的前提下，最大化自主机器人配送效率的问题。我们评估了一个基于机器学习和优化的方法 COIL（在学习的潜在空间中进行约束优化），并将其与基准遗传算法进行比较。我们还研究了改善 COIL 的速度和效率的新方法。我们表明只有 COIL 能够找到所有问题变化测试中适当数量的机器人同时运行的有效解决方案。我们还表明，当 COIL 学习其潜在表示时，可以比 G（遗传算法）更快地进行优化。

    The use of autonomous robots for delivery of goods to customers is an exciting new way to provide a reliable and sustainable service. However, in the real world, autonomous robots still require human supervision for safety reasons. We tackle the realworld problem of optimizing autonomous robot timings to maximize deliveries, while ensuring that there are never too many robots running simultaneously so that they can be monitored safely. We assess the use of a recent hybrid machine-learningoptimization approach COIL (constrained optimization in learned latent space) and compare it with a baseline genetic algorithm for the purposes of exploring variations of this problem. We also investigate new methods for improving the speed and efficiency of COIL. We show that only COIL can find valid solutions where appropriate numbers of robots run simultaneously for all problem variations tested. We also show that when COIL has learned its latent representation, it can optimize 10% faster than the G
    
[^67]: Hitachi在SemEval-2023任务3中的表现：探索跨语言多任务策略，用于在线新闻中的流派和框架检测

    Hitachi at SemEval-2023 Task 3: Exploring Cross-lingual Multi-task Strategies for Genre and Framing Detection in Online News. (arXiv:2303.01794v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.01794](http://arxiv.org/abs/2303.01794)

    Hitachi团队参加SemEval-2023第3项任务，研究了跨语言和多任务策略，表明跨语言/多任务训练和收集外部平衡数据集可以有益于流派和框架检测，在意大利语和俄语流派分类子任务中实现了最高的宏平均F1分数。

    

    本文介绍了Hitachi团队参加SemEval-2023第3项任务“在多语言设置中检测在线新闻中的流派、框架和说服技巧”的情况。鉴于任务的多语言、多任务性质和低资源环境，我们研究了不同的跨语言和多任务策略，以训练预训练语言模型。通过广泛的实验，我们发现(a)跨语言/多任务训练，以及(b)收集外部平衡数据集，可以有益于流派和框架检测。我们从结果构建了集成模型，并在意大利语和俄语流派分类子任务中实现了最高的宏平均F1分数。

    This paper explains the participation of team Hitachi to SemEval-2023 Task 3 "Detecting the genre, the framing, and the persuasion techniques in online news in a multi-lingual setup.'' Based on the multilingual, multi-task nature of the task and the low-resource setting, we investigated different cross-lingual and multi-task strategies for training the pretrained language models. Through extensive experiments, we found that (a) cross-lingual/multi-task training, and (b) collecting an external balanced dataset, can benefit the genre and framing detection. We constructed ensemble models from the results and achieved the highest macro-averaged F1 scores in Italian and Russian genre categorization subtasks.
    
[^68]: Fourier分析与运行时间分析相遇：关于“高原”问题的精准运行时间分析研究

    Fourier Analysis Meets Runtime Analysis: Precise Runtimes on Plateaus. (arXiv:2302.08021v2 [cs.NE] CROSS LISTED)

    [http://arxiv.org/abs/2302.08021](http://arxiv.org/abs/2302.08021)

    本文提出了一种基于傅里叶分析的新方法，用于分析进化算法在高原上消耗的时间，通过算法在针尖问题和新基准问题上的应用研究，确定了最佳变异率，并证明了$(1+1)$进化算法相对于一般随机搜索启发式算法在高原问题上具有惊人的效率。

    

    本文提出了一种基于离散傅里叶分析的新方法，用于分析进化算法在高原上消耗的时间。这种方法立即证明了由Garnier，Kallel，和Schoenauer（1999）提出的关于针尖问题上预期运行时间的经典估计。我们还将该方法用于分析一个新的基准问题：由$n/\ell$个有效大小为$2^\ell-1$的高原组成，需要以LeadingOnes的方式顺序地对其进行优化。利用我们的新方法，我们确定了静态和与适应度相关的变异率的精确预期运行时间。我们还确定了渐进最优的静态和与适应度相关的变异率。对于$\ell = o(n)$，最优的静态变异率近似为$1.59/n$。当找到前$k$个与适应度相关的二进制位时，最优的适应度相关变异率渐进为$1/(k+1)$。到目前为止，这些结果仅证明了预期的运行时间，表明该新基准问题非常困难，而$(1+1)$进化算法在高原问题上与一般的随机搜索启发式算法相比效率惊人。

    We propose a new method based on discrete Fourier analysis to analyze the time evolutionary algorithms spend on plateaus. This immediately gives a concise proof of the classic estimate of the expected runtime of the $(1+1)$ evolutionary algorithm on the Needle problem due to Garnier, Kallel, and Schoenauer (1999).  We also use this method to analyze the runtime of the $(1+1)$ evolutionary algorithm on a new benchmark consisting of $n/\ell$ plateaus of effective size $2^\ell-1$ which have to be optimized sequentially in a LeadingOnes fashion.  Using our new method, we determine the precise expected runtime both for static and fitness-dependent mutation rates. We also determine the asymptotically optimal static and fitness-dependent mutation rates. For $\ell = o(n)$, the optimal static mutation rate is approximately $1.59/n$. The optimal fitness dependent mutation rate, when the first $k$ fitness-relevant bits have been found, is asymptotically $1/(k+1)$. These results, so far only prove
    
[^69]: 游戏化能否减轻mHealth应用中自我报告的负担？利用智能手表数据的机器学习进行认知负荷估计的可行性研究。

    Can gamification reduce the burden of self-reporting in mHealth applications? A feasibility study using machine learning from smartwatch data to estimate cognitive load. (arXiv:2302.03616v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03616](http://arxiv.org/abs/2302.03616)

    通过机器学习技术和智能手表数据进行认知负荷估计，研究发现游戏化的自我报告方式与传统方式的认知负荷没有差异，但参与者更喜欢游戏化版本。

    

    数字化治疗的有效性可以通过要求患者通过应用程序自我报告其状态来衡量，然而，这可能会令人不知所措并导致失去参与度。我们进行了一项研究，探讨游戏化对自我报告的影响。我们的方法涉及创建一个系统，通过分析光-血容积变化信号来评估认知负荷（CL）。利用11名参与者的数据来训练机器学习模型来检测CL。随后，我们创建了两个版本的调查问卷：一个是游戏化版本，一个是传统版本。我们估计其他参与者（13名）在完成调查问卷时经历的CL。我们发现，通过预先在应激检测任务中进行预训练，可以增强CL检测器的性能。对于13名参与者中的10名，个性化CL检测器可以实现高于0.7的F1得分。我们发现，在CL方面，游戏化和非游戏化的调查问卷没有区别，但参与者更喜欢游戏化的版本。

    The effectiveness of digital treatments can be measured by requiring patients to self-report their state through applications, however, it can be overwhelming and causes disengagement. We conduct a study to explore the impact of gamification on self-reporting. Our approach involves the creation of a system to assess cognitive load (CL) through the analysis of photoplethysmography (PPG) signals. The data from 11 participants is utilized to train a machine learning model to detect CL. Subsequently, we create two versions of surveys: a gamified and a traditional one. We estimate the CL experienced by other participants (13) while completing surveys. We find that CL detector performance can be enhanced via pre-training on stress detection tasks. For 10 out of 13 participants, a personalized CL detector can achieve an F1 score above 0.7. We find no difference between the gamified and non-gamified surveys in terms of CL but participants prefer the gamified version.
    
[^70]: 新兴因果性与意识基础

    Emergent Causality & the Foundation of Consciousness. (arXiv:2302.03189v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.03189](http://arxiv.org/abs/2302.03189)

    讨论了不需要显式实现干预表示的 Pareto 最优数学形式，通过归纳性地出现相关因果干预的表示，作为自我和任何其他对象的表示。

    

    为了在交互环境中做出准确的推断，智能体不能把被动观察事件与干预引起它们混淆。$do$ 操作符将干预形式化，以便我们可以推断其影响。然而，在交互环境中存在无需显式实现干预表示的 Pareto 最优数学形式，可以做出最大程度的准确推断。本文考察了其中一个这样的形式。我们展示了在不存在 $do$ 操作符的情况下，可以通过变量表示干预。然后我们认为变量是抽象的，需要显式预先表示干预只是因为我们预设了这类抽象概念。前述的形式避免了这种困扰，因此，如果有适当的初始条件，则相关因果干预的表示将通过归纳性地出现。这些新兴的抽象作为自我和任何其他对象的表示。

    To make accurate inferences in an interactive setting, an agent must not confuse passive observation of events with having intervened to cause them. The $do$ operator formalises interventions so that we may reason about their effect. Yet there exist pareto optimal mathematical formalisms of general intelligence in an interactive setting which, presupposing no explicit representation of intervention, make maximally accurate inferences. We examine one such formalism. We show that in the absence of a $do$ operator, an intervention can be represented by a variable. We then argue that variables are abstractions, and that need to explicitly represent interventions in advance arises only because we presuppose these sorts of abstractions. The aforementioned formalism avoids this and so, initial conditions permitting, representations of relevant causal interventions will emerge through induction. These emergent abstractions function as representations of one`s self and of any other object, inas
    
[^71]: 假设的最佳选择是最弱的而不是最短的

    The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest. (arXiv:2301.12987v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.12987](http://arxiv.org/abs/2301.12987)

    本研究表明，在构建假设的过程中，选择最短的假设不如选择最弱的假设，而弱点是比长度或简单性更好的推广表现代理。

    

    如果$A$和$B$是这样的集合，即$A \subset B$，那么一般化可以被理解为从$A$推断出一个足以构建$B$的假设。可以从$A$推断出任意数量的假设，但只有其中的一些可以推广到$B$。怎样知道哪些假设可能推广？一种策略是选择最短的，将压缩信息的能力与推广的能力（智能的代理）等同起来。我们在主动认知的数学形式主义背景下研究了这一点。我们证明了，压缩既不是最大化表现（用假设推广的概率衡量）的必要条件，也不是充分条件。我们制定了一个与长度或简单性无关的代理，称为弱点。我们表明，如果任务是均匀分布的，则不存在任何代理的选择，可以在所有任务中至少与弱点最大化的表现相同，同时在至少一个任务中表现更好。在比较最大弱点和最小描述长度(MDL)的实验中，我们发现最大弱点在各种任务上表现优于MDL。我们认为弱点是比长度或简单性更好的推广表现代理。

    If $A$ and $B$ are sets such that $A \subset B$, generalisation may be understood as the inference from $A$ of a hypothesis sufficient to construct $B$. One might infer any number of hypotheses from $A$, yet only some of those may generalise to $B$. How can one know which are likely to generalise? One strategy is to choose the shortest, equating the ability to compress information with the ability to generalise (a proxy for intelligence). We examine this in the context of a mathematical formalism of enactive cognition. We show that compression is neither necessary nor sufficient to maximise performance (measured in terms of the probability of a hypothesis generalising). We formulate a proxy unrelated to length or simplicity, called weakness. We show that if tasks are uniformly distributed, then there is no choice of proxy that performs at least as well as weakness maximisation in all tasks while performing strictly better in at least one. In experiments comparing maximum weakness and m
    
[^72]: RangeViT：面向自动驾驶中的三维语义分割的视觉Transformer

    RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving. (arXiv:2301.10222v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.10222](http://arxiv.org/abs/2301.10222)

    本文旨在探究视觉Transformer是否可以应用于自动驾驶中的3D语义分割中，通过保留与RGB图像相同的骨干结构，这项工作证明了ViTs在结合投影方法，大数据训练和具有噪声鲁棒性的新损失函数后可以取得最先进的结果。

    

    将室外LiDAR点云的语义分割视为二维问题（例如通过距离投影），这是一种有效和流行的方法。这些基于投影的方法通常受益于快速计算，并且与使用其他点云表示的技术相结合，可以实现最先进的结果。目前，投影方法利用2D CNNs，但计算机视觉的最新进展表明，视觉Transformer（ViTs）在许多基于图像的基准测试中已经取得了最先进的成果。在这项工作中，我们质疑是否可以通过ViTs的最新改进来改进三维语义分割的投影方法。我们回答是肯定的，但只有在结合了三个关键因素之后才能实现：（a）ViTs难以训练，并且需要大量的训练数据来学习强大的表示。通过保留与RGB图像相同的骨干结构，我们可以利用对大图像集合的长时间训练的知识，这些集合比相应的点云数据集要小得多。

    Casting semantic segmentation of outdoor LiDAR point clouds as a 2D problem, e.g., via range projection, is an effective and popular approach. These projection-based methods usually benefit from fast computations and, when combined with techniques which use other point cloud representations, achieve state-of-the-art results. Today, projection-based methods leverage 2D CNNs but recent advances in computer vision show that vision transformers (ViTs) have achieved state-of-the-art results in many image-based benchmarks. In this work, we question if projection-based methods for 3D semantic segmentation can benefit from these latest improvements on ViTs. We answer positively but only after combining them with three key ingredients: (a) ViTs are notoriously hard to train and require a lot of training data to learn powerful representations. By preserving the same backbone architecture as for RGB images, we can exploit the knowledge from long training on large image collections that are much c
    
[^73]: 掩模自编码在大规模自然语言监督中没有帮助

    Masked Autoencoding Does Not Help Natural Language Supervision at Scale. (arXiv:2301.07836v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.07836](http://arxiv.org/abs/2301.07836)

    本研究旨在探讨大规模训练下掩模自编码和对比语言图像预训练的有效性。研究结果表明，在小规模训练中这两种方法可以有效地结合使用，但在大规模训练中没有明显的优势。

    

    自我监督和自然语言监督已成为训练通用图像编码器的两种有效方法。最近的研究表明，这些方法可以有效地结合使用，但这些结果主要使用了小的预训练数据集（<50M样本），并且没有有效地反映出常用于这些方法的大规模数据集（>100M样本）的情况。本研究调查了类似的方法在使用更大量数据进行训练时是否有效。我们发现，在对11.3M的图像-文本对进行训练时，掩模自编码器和对比语言图像预训练的组合可以比只使用对比语言图像预训练更好，但在对1.4B个图像进行训练时，它与仅使用对比语言图像预训练相比，几乎没有任何优势（在一套常见的视觉任务中评估）。本研究为这些方法的有效性提供了一些需要的清晰度。

    Self supervision and natural language supervision have emerged as two exciting ways to train general purpose image encoders which excel at a variety of downstream tasks. Recent works such as M3AE and SLIP have suggested that these approaches can be effectively combined, but most notably their results use small pre-training datasets (<50M samples) and don't effectively reflect the large-scale regime (>100M examples) that is commonly used for these approaches. Here we investigate whether a similar approach can be effective when trained with a much larger amount of data. We find that a combination of two state of the art approaches: masked auto-encoders, MAE and contrastive language image pre-training, CLIP provides a benefit over CLIP when trained on a corpus of 11.3M image-text pairs, but little to no benefit (as evaluated on a suite of common vision tasks) over CLIP when trained on a large corpus of 1.4B images. Our work provides some much needed clarity into the effectiveness (or lack
    
[^74]: VISEM-Tracking，一份人类精子跟踪数据集

    VISEM-Tracking, a human spermatozoa tracking dataset. (arXiv:2212.02842v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02842](http://arxiv.org/abs/2212.02842)

    本文提供了人类精子跟踪数据集VISEM-Tracking，包含手动注释的包围框坐标和由专家分析的精子特征，并提供未标记的视频以供易于访问和分析，有助于训练监督式机器学习方法，提高在评估精子运动和运动学方面的精度和可靠性。

    

    精子运动的手动评估需要显微镜观察，由于所观察的精子在视野中的快速移动，这是具有挑战性的。为了获得正确的结果，手动评估需要进行广泛的培训。因此，在诊所中，计算机辅助精子分析（CASA）变得越来越常用。尽管如此，需要更多数据来训练监督式机器学习方法，以提高在评估精子运动和运动学方面的精度和可靠性。在这方面，我们提供了一个名为VISEM-Tracking的数据集，其中包含20个30秒的视频记录（包括29,196帧）的湿性精子制备物，具备手动注释的包围框坐标和由该领域的专家分析的一组精子特征。除了已注释的数据，我们还提供了未标记的视频剪辑，以便通过自监督或无监督学习等方法轻松访问和分析数据。作为本文的一部分，我们提出了基线精子检测性能。

    A manual assessment of sperm motility requires microscopy observation, which is challenging due to the fast-moving spermatozoa in the field of view. To obtain correct results, manual evaluation requires extensive training. Therefore, computer-assisted sperm analysis (CASA) has become increasingly used in clinics. Despite this, more data is needed to train supervised machine learning approaches in order to improve accuracy and reliability in the assessment of sperm motility and kinematics. In this regard, we provide a dataset called VISEM-Tracking with 20 video recordings of 30 seconds (comprising 29,196 frames) of wet sperm preparations with manually annotated bounding-box coordinates and a set of sperm characteristics analyzed by experts in the domain. In addition to the annotated data, we provide unlabeled video clips for easy-to-use access and analysis of the data via methods such as selfor unsupervised learning. As part of this paper, we present baseline sperm detection performan
    
[^75]: 基于关系感知的语言图转换器用于问答

    Relation-Aware Language-Graph Transformer for Question Answering. (arXiv:2212.00975v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.00975](http://arxiv.org/abs/2212.00975)

    本论文提出了关系感知语言图转换器，能够以统一的方式联合推理语言和图关于实体关系，并在多个QA数据集上验证了其有效性。

    

    问答是一项需要推理自然语言环境的任务，许多相关工作通过图神经网络(GNN)增强语言模型(LM)，以对知识图谱(KG)信息进行编码。然而，大多数现有的面向问答的基于GNN的模块并未利用KG的丰富关系信息，并且依赖于LM和KG之间的有限信息交互。为了解决这些问题，我们提出了 Question Answering Transformer(QAT)，它旨在以统一的方式联合推理语言和图关于实体关系。具体而言，QAT构建了元路径令牌，这些令牌学习基于不同的结构和语义关系的关系中心嵌入。然后，我们的关系感知自注意力模块通过跨模态相关位置偏差全面整合了不同的模态，以指导不同模态之间相关实体的信息交换。我们验证了QAT在多个QA数据集上的有效性。

    Question Answering (QA) is a task that entails reasoning over natural language contexts, and many relevant works augment language models (LMs) with graph neural networks (GNNs) to encode the Knowledge Graph (KG) information. However, most existing GNN-based modules for QA do not take advantage of rich relational information of KGs and depend on limited information interaction between the LM and the KG. To address these issues, we propose Question Answering Transformer (QAT), which is designed to jointly reason over language and graphs with respect to entity relations in a unified manner. Specifically, QAT constructs Meta-Path tokens, which learn relation-centric embeddings based on diverse structural and semantic relations. Then, our Relation-Aware Self-Attention module comprehensively integrates different modalities via the Cross-Modal Relative Position Bias, which guides information exchange between relevant entites of different modalities. We validate the effectiveness of QAT on com
    
[^76]: xTrimoABFold：无多序列比对的新型抗体结构预测方法

    xTrimoABFold: De novo Antibody Structure Prediction without MSA. (arXiv:2212.00735v2 [q-bio.QM] CROSS LISTED)

    [http://arxiv.org/abs/2212.00735](http://arxiv.org/abs/2212.00735)

    xTrimoABFold是一种基于深度抗体语言模型的新型抗体结构预测方法，无需多序列比对，有望促进高通量药物设计的应用。

    

    在抗体工程领域，设计一个新型抗体以正确地结合特定抗原的表位是一项重要的任务。了解抗体结构和其表位可以促进对其功能的机制理解。因此，从其序列预测抗体结构一直是一项高度有价值的任务，而AlphaFold2提供了一种基于蛋白质序列预测蛋白质结构的解决方案，但对于抗体，特别是对于抗体的互补决定区（CDRs），其预测效率和准确性有限制。

    In the field of antibody engineering, an essential task is to design a novel antibody whose paratopes bind to a specific antigen with correct epitopes. Understanding antibody structure and its paratope can facilitate a mechanistic understanding of its function. Therefore, antibody structure prediction from its sequence alone has always been a highly valuable problem for de novo antibody design. AlphaFold2, a breakthrough in the field of structural biology, provides a solution to predict protein structure based on protein sequences and computationally expensive coevolutionary multiple sequence alignments (MSAs). However, the computational efficiency and undesirable prediction accuracy of antibodies, especially on the complementarity-determining regions (CDRs) of antibodies limit their applications in the industrially high-throughput drug design. To learn an informative representation of antibodies, we employed a deep antibody language model (ALM) on curated sequences from the observed a
    
[^77]: 关于图神经网络模拟顶点间相互作用的研究

    On the Ability of Graph Neural Networks to Model Interactions Between Vertices. (arXiv:2211.16494v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16494](http://arxiv.org/abs/2211.16494)

    本文研究了GNN模拟顶点间相互作用的能力，通过一个被称为分离秩的度量标准来量化这种能力，结果表明模拟相互作用的能力主要取决于分区的行走指数，即从分界线开始的行走数量，同时设计了一种名为WISA的边稀疏化算法以提高GNNs的处理效率和表达能力。

    

    图神经网络(GNNs)被广泛用于建模由图中顶点表示的实体之间的复杂互动。尽管最近有一些理论分析GNNs表达能力的努力，但对其模拟相互作用的能力缺乏一个正式的描述。本文旨在填补这一空白。通过一个已知的度量标准——分离秩(separation rank)来规范化相互作用的强度，我们量化了某些GNNs模拟给定顶点子集及其补集之间交互的能力，即输入顶点组成的给定分区的两侧之间的互动。我们的结果表明，模拟相互作用的能力主要取决于分区的行走指数(walk index)——一个由分界线开始的行走数量定义的图形特征。常见GNN架构的实验证明了这一发现。作为我们理论的实际应用，我们设计了一种名为Walk Indexed Sparsification Algorithm (WISA)的边稀疏化算法，利用我们的研究结果提高处理大规模图形的GNNs效率同时保持它们的表达能力。

    Graph neural networks (GNNs) are widely used for modeling complex interactions between entities represented as vertices of a graph. Despite recent efforts to theoretically analyze the expressive power of GNNs, a formal characterization of their ability to model interactions is lacking. The current paper aims to address this gap. Formalizing strength of interactions through an established measure known as separation rank, we quantify the ability of certain GNNs to model interaction between a given subset of vertices and its complement, i.e. between the sides of a given partition of input vertices. Our results reveal that the ability to model interaction is primarily determined by the partition's walk index -- a graph-theoretical characteristic defined by the number of walks originating from the boundary of the partition. Experiments with common GNN architectures corroborate this finding. As a practical application of our theory, we design an edge sparsification algorithm named Walk Inde
    
[^78]: StructDiffusion：使用未知对象进行物理有效结构的语言指导创建

    StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects. (arXiv:2211.04604v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.04604](http://arxiv.org/abs/2211.04604)

    本论文提出了一种结合扩散模型和以物体为中心的转换器的方法，通过高层语言目标和局部视点云构建物理有效的结构，该方法可用于多个具有挑战性的多步骤3D规划任务，即使使用未知对象仍能提高成功率。

    

    在人类环境中运作的机器人必须能够将物体重新排列成语义有意义的配置，即使这些物体以前没见过。本文关注如何在无需逐步指令的情况下构建物理有效的结构。我们提出了StructDiffusion，该方法结合了扩散模型和以物体为中心的转换器，根据局部视点云和高级语言目标（如“摆桌子”），构建结构。我们的方法可以使用一个模型执行多个具有挑战性的语言条件的多步骤3D规划任务。与训练在特定结构上的现有多模态转换器模型相比，StructDiffusion甚至提高了将未知对象组装成物理有效结构的成功率，平均可提高16％。我们展示了模拟和实际重新排列任务中使用保留对象的实验。重要的是，我们展示了如何将扩散模型和碰撞鉴别器模型结合起来，以实现对拟合性以及对语言指导物理有效结构构建任务的影响。

    Robots operating in human environments must be able to rearrange objects into semantically-meaningful configurations, even if these objects are previously unseen. In this work, we focus on the problem of building physically-valid structures without step-by-step instructions. We propose StructDiffusion, which combines a diffusion model and an object-centric transformer to construct structures given partial-view point clouds and high-level language goals, such as "set the table". Our method can perform multiple challenging language-conditioned multi-step 3D planning tasks using one model. StructDiffusion even improves the success rate of assembling physically-valid structures out of unseen objects by on average 16% over an existing multi-modal transformer model trained on specific structures. We show experiments on held-out objects in both simulation and on real-world rearrangement tasks. Importantly, we show how integrating both a diffusion model and a collision-discriminator model allo
    
[^79]: 使用LLM来增强可解释模型的训练

    Augmenting Interpretable Models with LLMs during Training. (arXiv:2209.11799v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.11799](http://arxiv.org/abs/2209.11799)

    本文提出了 Aug-imodels 框架，利用 LLMs 的知识在拟合过程中构建高效且可解释的模型，在推理过程中不使用 LLMs，具备完全的透明性。研究探讨了两种不同方式的实现，并在多种文本分类数据集中表现出优异的效果。

    

    近期，大型语言模型（LLMs）在越来越多的任务中表现出了出色的表现。然而，它们进入高风险领域（例如医学）和计算资源有限的环境中，对解释性和效率的需求日益增加。我们提出了增强可解释模型（Aug-imodels）框架，利用LLMs所学习的知识建立极其高效且可解释的模型。Aug-imodels在拟合过程中使用LLMs，但在推理过程中不使用，从而实现了完全的透明性，并且与LLMs相比，推理速度和内存性能有了大于1000倍的提高。我们探讨了两种Aug-imodels在自然语言处理中的具体实例：（i）Aug-GAM，它使用来自LLM的解耦嵌入来增强广义加性模型；（ii）Aug-Tree，它通过LLM特征扩展来增强决策树。在各种文本分类数据集中，这两种方法都优于其未增强的对照模型。

    Recent large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their proliferation into high-stakes domains (e.g. medicine) and compute-limited settings has created a burgeoning need for interpretability and efficiency. We address this need by proposing Augmented Interpretable Models (Aug-imodels), a framework for leveraging the knowledge learned by LLMs to build extremely efficient and interpretable models. Aug-imodels use LLMs during fitting but not during inference, allowing complete transparency and often a speed/memory improvement of greater than 1,000x for inference compared to LLMs. We explore two instantiations of Aug-imodels in natural-language processing: (i) Aug-GAM, which augments a generalized additive model with decoupled embeddings from an LLM and (ii) Aug-Tree, which augments a decision tree with LLM feature expansions. Across a variety of text-classification datasets, both outperform their non-augmented co
    
[^80]: 信号时间逻辑谓词的模型预测鲁棒性

    Model Predictive Robustness of Signal Temporal Logic Predicates. (arXiv:2209.07881v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.07881](http://arxiv.org/abs/2209.07881)

    本论文提出了一种新的模型预测鲁棒性的概念来更系统地评估信号时间逻辑中底层谓词的鲁棒性，并成功验证了该方法在自动驾驶中的应用。

    

    信号时间逻辑的鲁棒性不仅评估了一个信号是否符合规范，而且还提供了一个衡量公式被满足或违反的程度的指标。鲁棒性的计算基于对底层谓词的鲁棒性进行评估。然而，谓词的鲁棒性通常以一种无模型的方式定义，即不包括系统动态。而且，精确定义复杂谓词的鲁棒性通常是非平凡的。为了解决这些问题，我们提出了一种模型预测鲁棒性的概念，通过考虑基于模型的预测，提供了比之前方法更系统的评估鲁棒性的方法。特别地，我们使用高斯过程回归来学习基于预先计算的预测的鲁棒性，以便可以在线高效地计算鲁棒性值。我们评估了我们的方法，并使用在记录的数据上使用在形式化交通规则中使用的谓词的自动驾驶用例验证了我们的方法。

    The robustness of signal temporal logic not only assesses whether a signal adheres to a specification but also provides a measure of how much a formula is fulfilled or violated. The calculation of robustness is based on evaluating the robustness of underlying predicates. However, the robustness of predicates is usually defined in a model-free way, i.e., without including the system dynamics. Moreover, it is often nontrivial to define the robustness of complicated predicates precisely. To address these issues, we propose a notion of model predictive robustness, which provides a more systematic way of evaluating robustness compared to previous approaches by considering model-based predictions. In particular, we use Gaussian process regression to learn the robustness based on precomputed predictions so that robustness values can be efficiently computed online. We evaluate our approach for the use case of autonomous driving with predicates used in formalized traffic rules on a recorded dat
    
[^81]: 多目标参数优化中的有效效用函数学习与先验知识

    Efficient Utility Function Learning for Multi-Objective Parameter Optimization with Prior Knowledge. (arXiv:2208.10300v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.10300](http://arxiv.org/abs/2208.10300)

    该论文提出了一种利用偏好学习离线学习效用函数的方法，以应对真实世界问题中用专家知识定义效用函数困难且与专家反复互动昂贵的问题。使用效用函数空间的粗略信息，能够在使用很少结果时提高效用函数估计，并通过整个优化链中传递效用函数学习任务中出现的不确定性。

    

    目前的多目标优化技术通常假定已有效用函数、通过互动学习效用函数或尝试确定完整的Pareto前沿来进行。然而，在真实世界的问题中，结果往往基于隐含和显性的专家知识，难以定义一个效用函数，而互动学习或后续启发式需要反复并且昂贵地专家参与。为了缓解这种情况，我们使用偏好学习离线学习效用函数，利用专家知识。与其他工作不同的是，我们不仅使用（成对的）结果偏好，而且使用效用函数空间的粗略信息。这使我们能够提高效用函数估计，特别是在使用很少的结果时。此外，我们对效用函数学习任务中出现的不确定性进行建模，并将其传递到整个优化链中。

    The current state-of-the-art in multi-objective optimization assumes either a given utility function, learns a utility function interactively or tries to determine the complete Pareto front, requiring a post elicitation of the preferred result. However, result elicitation in real world problems is often based on implicit and explicit expert knowledge, making it difficult to define a utility function, whereas interactive learning or post elicitation requires repeated and expensive expert involvement. To mitigate this, we learn a utility function offline, using expert knowledge by means of preference learning. In contrast to other works, we do not only use (pairwise) result preferences, but also coarse information about the utility function space. This enables us to improve the utility function estimate, especially when using very few results. Additionally, we model the occurring uncertainties in the utility function learning task and propagate them through the whole optimization chain. 
    
[^82]: 使用深度神经网络预测同一基因位点上的不同mRNA的功能

    Isoform Function Prediction Using a Deep Neural Network. (arXiv:2208.03325v3 [q-bio.GN] UPDATED)

    [http://arxiv.org/abs/2208.03325](http://arxiv.org/abs/2208.03325)

    本文介绍了一种使用深度神经网络预测同一基因位点上不同mRNA功能的方法，解决了缺乏标记训练数据的问题。

    

    异剪切是从同一基因位点产生多个mRNA的现象。研究表明，超过95%的人类多外显子基因经历了异剪切。虽然mRNA序列的变化很小，但它们可能对细胞功能和调节产生系统性影响。报道称，同一基因的不同剪接形式具有不同甚至对立的功能。虽然基因的功能研究范围很广，但对于同一基因位点上不同mRNA的功能还知之甚少。最近，一些基于多实例学习的计算方法已被提出，用于预测功能，并结合了基因功能和基因表达谱。然而，由于缺乏标记的训练数据，它们的表现并不理想。此外，概率模型，如条件随机场（CRF），也被用于建模异构体之间的关系。

    Isoforms are mRNAs produced from the same gene site in the phenomenon called Alternative Splicing. Studies have shown that more than 95% of human multi-exon genes have undergone alternative splicing. Although there are few changes in mRNA sequence, They may have a systematic effect on cell function and regulation. It is widely reported that isoforms of a gene have distinct or even contrasting functions. Most studies have shown that alternative splicing plays a significant role in human health and disease. Despite the wide range of gene function studies, there is little information about isoforms' functionalities. Recently, some computational methods based on Multiple Instance Learning have been proposed to predict isoform function using gene function and gene expression profile. However, their performance is not desirable due to the lack of labeled training data. In addition, probabilistic models such as Conditional Random Field (CRF) have been used to model the relation between isofor
    
[^83]: 跨模态因果关系推理在事件级视觉问答中的应用

    Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering. (arXiv:2207.12647v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.12647](http://arxiv.org/abs/2207.12647)

    本篇论文提出了一个新型的事件级视觉问答框架——跨模态因果关系推理（CMCIR），通过引入因果干预方法，发现视觉和语言模态的真正因果结构，实现强健的因果感知视觉语言问答。

    

    现有的视觉问答方法往往捕捉跨模态的伪相关性，而未能发现真正的因果机制，以真实地基于主导视觉证据和问题意图进行推理。此外，现有方法通常忽略了跨模态事件级理解，需要联合建模事件的时间性、因果性和动态性。在本文中，我们从新的角度，即跨模态因果关系推理，聚焦于事件级视觉问答，引入因果干预方法来发现视觉和语言模态的真正因果结构。具体而言，我们提出了一个名为跨模态因果关系推理（CMCIR）的新型事件级视觉问答框架，以实现强健的因果感知视觉语言问答。为了发现跨模态因果结构，我们提出了因果感知视觉语言推理（CVLR）模块，用于共同对视觉和语言模态建模。

    Existing visual question answering methods tend to capture the cross-modal spurious correlations and fail to discover the true causal mechanism that facilitates reasoning truthfully based on the dominant visual evidence and the question intention. Additionally, the existing methods usually ignore the cross-modal event-level understanding that requires to jointly model event temporality, causality, and dynamics. In this work, we focus on event-level visual question answering from a new perspective, i.e., cross-modal causal relational reasoning, by introducing causal intervention methods to discover the true causal structures for visual and linguistic modalities. Specifically, we propose a novel event-level visual question answering framework named Cross-Modal Causal RelatIonal Reasoning (CMCIR), to achieve robust causality-aware visual-linguistic question answering. To discover cross-modal causal structures, the Causality-aware Visual-Linguistic Reasoning (CVLR) module is proposed to co
    
[^84]: BiometricBlender：高维度多类合成数据生成工具

    BiometricBlender: Ultra-high dimensional, multi-class synthetic data generator to imitate biometric feature space. (arXiv:2206.10747v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10747](http://arxiv.org/abs/2206.10747)

    BiometricBlender是一个高维度多类合成数据生成工具，可以模拟真实生物特征数据集的关键属性，有助于在特征筛选领域进行快速发展。

    

    缺乏自由可得的高维度多类真实或合成数据集常常限制了特征筛选领域的快速发展，尤其是在生物特征识别领域。本文介绍了一个名为BiometricBlender的Python包，它是一个超高维度多类合成数据生成工具，用于测试各种特征筛选方法。在数据生成过程中，用户可以控制特征混合的总体有用性和互相关系数，因此合成特征空间能够模拟真实生物特征数据集的关键属性。

    The lack of freely available (real-life or synthetic) high or ultra-high dimensional, multi-class datasets may hamper the rapidly growing research on feature screening, especially in the field of biometrics, where the usage of such datasets is common. This paper reports a Python package called BiometricBlender, which is an ultra-high dimensional, multi-class synthetic data generator to benchmark a wide range of feature screening methods. During the data generation process, the overall usefulness and the intercorrelations of blended features can be controlled by the user, thus the synthetic feature space is able to imitate the key properties of a real biometric dataset.
    
[^85]: 预测Twitter对话线程中的仇恨强度

    Predicting Hate Intensity of Twitter Conversation Threads. (arXiv:2206.08406v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2206.08406](http://arxiv.org/abs/2206.08406)

    本文提出了DRAGNET++, 通过考虑对话线程的语义、传播结构和用户交互，以预测推文的回复链中可能存在的仇恨程度，并在两个公开数据集上的实验中表现出优越性能。该模型可为社交媒体平台提供在恶意对话升级之前识别和管理的工具。

    

    推文是在线社交媒体中最简洁的交流形式，一条推文有可能是对话中打造或破坏讨论的潜在媒介。在线仇恨言论比以往任何时候都更容易获得，阻止其传播对于社交媒体公司和用户来说是极为重要的，可以推进良好的交流方式。目前除了最近的一些研究外，大部分研究都专注于分类单个推文，而忽略了推文之间的对话线程/上下文。我们提出了DRAGNET++，旨在通过推文的回复链预测它可能带来的仇恨程度，同时考虑到对话线程的语义和传播结构以及线程中的用户交互。我们在两个公开数据集上的实验表明DRAGNET++的表现优于现有的方法，我们认为社交媒体平台可以利用我们提出的方法，预测和管理恶意对话。

    Tweets are the most concise form of communication in online social media, wherein a single tweet has the potential to make or break the discourse of the conversation. Online hate speech is more accessible than ever, and stifling its propagation is of utmost importance for social media companies and users for congenial communication. Most of the research barring a recent few has focused on classifying an individual tweet regardless of the tweet thread/context leading up to that point. One of the classical approaches to curb hate speech is to adopt a reactive strategy after the hate speech postage. The ex-post facto strategy results in neglecting subtle posts that do not show the potential to instigate hate speech on their own but may portend in the subsequent discussion ensuing in the post's replies. In this paper, we propose DRAGNET++, which aims to predict the intensity of hatred that a tweet can bring in through its reply chain in the future. It uses the semantic and propagating stru
    
[^86]: BRExIt：论对手建模在专家迭代中的应用

    BRExIt: On Opponent Modelling in Expert Iteration. (arXiv:2206.00113v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.00113](http://arxiv.org/abs/2206.00113)

    BRExIt算法使用对手模型加速游戏学习，提高学徒的特征塑造和规划偏向于对手模型。实验证明BRExIt比ExIt学习到更好的策略。

    

    在博弈论和多智能体学习中，寻找最佳反应策略是一个核心目标。现代基于群体的训练方法采用强化学习算法作为最佳反应预测器，以改善玩家对候选对手（通常是先前学习的策略）的游戏表现。我们提出了最佳反应专家迭代（BRExIt）算法，它将对手模型融入到最先进的学习算法Expert Iteration（ExIt）中以加速游戏学习。BRExIt旨在（1）改进学徒中的特征塑造，通过策略头预测对手策略作为辅助任务；（2）将对手移动的规划偏向于给定或已学习的对手模型，以生成更好地逼近最佳反应的学徒目标。我们对BRExIt的算法变体进行了实证消融，针对一组固定测试代理进行比较，发现BRExIt比ExIt学习到更好的策略。

    Finding a best response policy is a central objective in game theory and multi-agent learning, with modern population-based training approaches employing reinforcement learning algorithms as best-response oracles to improve play against candidate opponents (typically previously learnt policies). We propose Best Response Expert Iteration (BRExIt), which accelerates learning in games by incorporating opponent models into the state-of-the-art learning algorithm Expert Iteration (ExIt). BRExIt aims to (1) improve feature shaping in the apprentice, with a policy head predicting opponent policies as an auxiliary task, and (2) bias opponent moves in planning towards the given or learnt opponent model, to generate apprentice targets that better approximate a best response. In an empirical ablation on BRExIt's algorithmic variants against a set of fixed test agents, we provide statistical evidence that BRExIt learns better performing policies than ExIt.
    
[^87]: 一种基于变密度Noisier2Noise的自监督磁共振图像重建的理论框架

    A theoretical framework for self-supervised MR image reconstruction using sub-sampling via variable density Noisier2Noise. (arXiv:2205.10278v4 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2205.10278](http://arxiv.org/abs/2205.10278)

    本研究提出了一个基于Noisier2Noise框架的自监督磁共振图像重建理论框架，解释了SSDU方法的性能，并提出了两种修改。

    

    近年来，利用神经网络的统计建模能力重建子采样磁共振成像数据引起了人们的关注。大多数提出的方法假设存在代表性的完全采样数据集，并使用完全督导式训练。然而，对于许多应用程序，完全采样的训练数据是不可用的，而且可能非常难以获取。因此，发展和理解仅使用子采样数据进行训练的自监督方法非常有价值。本文将最初用于自监督去噪任务的Noisier2Noise框架扩展到变密度子采样MRI数据。我们使用Noisier2Noise框架来解释近期提出的在实践中表现良好但缺乏理论基础的自监督学习 via 数据欠采样（SSDU）方法的性能，提出两种修改。

    In recent years, there has been attention on leveraging the statistical modeling capabilities of neural networks for reconstructing sub-sampled Magnetic Resonance Imaging (MRI) data. Most proposed methods assume the existence of a representative fully-sampled dataset and use fully-supervised training. However, for many applications, fully sampled training data is not available, and may be highly impractical to acquire. The development and understanding of self-supervised methods, which use only sub-sampled data for training, are therefore highly desirable. This work extends the Noisier2Noise framework, which was originally constructed for self-supervised denoising tasks, to variable density sub-sampled MRI data. We use the Noisier2Noise framework to analytically explain the performance of Self-Supervised Learning via Data Undersampling (SSDU), a recently proposed method that performs well in practice but until now lacked theoretical justification. Further, we propose two modifications 
    
[^88]: 注意力交互图中的意图感知机器人群体导航

    Intention Aware Robot Crowd Navigation with Attention-Based Interaction Graph. (arXiv:2203.01821v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2203.01821](http://arxiv.org/abs/2203.01821)

    提出了一种基于循环图神经网络与注意力机制的机器人导航方法，可以在复杂的人群场景中实现安全、高效和不干扰的导航，通过预测未来轨迹推断出动态代理的意图，避免侵入他人预期路径。

    

    本研究探讨如何在密集且互动的人群中进行安全和意图感知的机器人导航。与大多数基于强化学习（RL）的方法不同，我们提出了一种新颖的循环图神经网络，利用注意力机制捕捉代理之间的异质性交互，以便在空间和时间上确定最优策略。我们根据未来多个时间步长的轨迹预测来推断动态代理的意图，并将这些预测结果纳入一个无模型RL框架中，以避免机器人侵入他人的预期路径。我们证明了我们的方法使机器人在具有挑战性的人群导航场景下能够以高效、安全和不干扰的方式完成任务。我们成功地将在仿真中学习的策略转移到了真实机器人环境中，并验证了该方法的鲁棒性。

    We study the problem of safe and intention-aware robot navigation in dense and interactive crowds. Most previous reinforcement learning (RL) based methods fail to consider different types of interactions among all agents or ignore the intentions of people, which results in performance degradation. To learn a safe and efficient robot policy, we propose a novel recurrent graph neural network with attention mechanisms to capture heterogeneous interactions among agents through space and time. To encourage longsighted robot behaviors, we infer the intentions of dynamic agents by predicting their future trajectories for several timesteps. The predictions are incorporated into a model-free RL framework to prevent the robot from intruding into the intended paths of other agents. We demonstrate that our method enables the robot to achieve good navigation performance and non-invasiveness in challenging crowd navigation scenarios. We successfully transfer the policy learned in simulation to a rea
    
[^89]: 先验、层次和信息不对称在强化学习技能转移中的应用

    Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning. (arXiv:2201.08115v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2201.08115](http://arxiv.org/abs/2201.08115)

    本文介绍了一种名为“APEx”的技能发现和转移算法，它以一种原则性和学习的方式利用信息不对称性，实现了最先进的采样效率、任务可转移性和概括能力，同时保留了非分层方法的表达能力和灵活性。

    

    从过去的经验中发现行为，并将其转移到新任务中的能力是智能代理在真实世界中高效采样的标志。为智能强化学习者装备相同的能力可能对它们在机器人技术上的成功部署至关重要。虽然分层和KL-正则化强化学习各自在这方面很有希望，但混合方法可能结合它们各自的优点。这些领域的关键在于利用架构模块之间的信息不对称性来偏执学习的技能。虽然不对称性选择对可转移性有很大影响，但现有方法主要基于直觉，以一个与领域无关的可能次优的方式进行选择。在本文中，我们从理论和经验上展示了序列任务中技能的关键表达能力-可转移性的平衡，由信息不对称性控制。在获得这一洞见后，我们引入了一种名为“APEx”的技能发现和转移算法，它以一种原则性和学习的方式利用信息不对称性。APEx实现了最先进的采样效率、任务可转移性和概括能力，同时保留了非分层方法的表达能力和灵活性。

    The ability to discover behaviours from past experience and transfer them to new tasks is a hallmark of intelligent agents acting sample-efficiently in the real world. Equipping embodied reinforcement learners with the same ability may be crucial for their successful deployment in robotics. While hierarchical and KL-regularized reinforcement learning individually hold promise here, arguably a hybrid approach could combine their respective benefits. Key to these fields is the use of information asymmetry across architectural modules to bias which skills are learnt. While asymmetry choice has a large influence on transferability, existing methods base their choice primarily on intuition in a domain-independent, potentially sub-optimal, manner. In this paper, we theoretically and empirically show the crucial expressivity-transferability trade-off of skills across sequential tasks, controlled by information asymmetry. Given this insight, we introduce Attentive Priors for Expressive and Tra
    
[^90]: 形式理论学习系统中的简单性泡沫问题

    A Simplicity Bubble Problem in Formal-Theoretic Learning Systems. (arXiv:2112.12275v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2112.12275](http://arxiv.org/abs/2112.12275)

    这篇论文证明了在机器学习中，对于每个算法都存在一个足够大的数据集大小，超过该大小，无法预测欺骗者的算法概率上界是该算法的算法概率上界。

    

    当挖掘大型数据集以预测新数据时，统计机器学习背后的原则限制不仅对大数据洪流构成严重挑战，还对数据生成过程偏向低算法复杂度的传统假设提出了挑战。即使假设有限的数据集生成器存在底层的算法信息偏好，我们也证明了当前的机器学习方法（包括深度学习或任何自上而下人工智能和统计机器学习方法的混合）总是可以被足够大的数据集自然地或人为地欺骗。特别地，我们证明了对于每个学习算法（无论是否有形式化理论的支持），都存在一个足够大的数据集大小，超过该大小，无法预测欺骗者的算法概率上界是该算法的算法概率上界（乘以仅依赖于学习算法的一个乘法常数）。

    When mining large datasets in order to predict new data, limitations of the principles behind statistical machine learning pose a serious challenge not only to the Big Data deluge, but also to the traditional assumptions that data generating processes are biased toward low algorithmic complexity. Even when one assumes an underlying algorithmic-informational bias toward simplicity in finite dataset generators, we show that current approaches to machine learning (including deep learning, or any formal-theoretic hybrid mix of top-down AI and statistical machine learning approaches), can always be deceived, naturally or artificially, by sufficiently large datasets. In particular, we demonstrate that, for every learning algorithm (with or without access to a formal theory), there is a sufficiently large dataset size above which the algorithmic probability of an unpredictable deceiver is an upper bound (up to a multiplicative constant that only depends on the learning algorithm) for the algo
    
[^91]: 从多嵌入交互的角度分析知识图谱嵌入方法。

    Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective. (arXiv:1903.11406v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1903.11406](http://arxiv.org/abs/1903.11406)

    本文提出了一种多嵌入交互的角度来分析和比较知识图谱嵌入方法，研究了CP，DistMult和ComplEx等三种流行的方法，揭示了ComplEx性能更好的原因，并为开发新模型提供了见解。

    

    知识图谱是一种广泛用于表示知识的格式，且在语义搜索引擎、问答系统和推荐系统中有许多应用。然而，实际的知识图谱通常是不完整的，因此提出了基于嵌入向量方法，如CP、DistMult和ComplEx等，以解决此问题。这些方法将实体和关系表示为语义空间中的嵌入向量，并预测它们之间的链接。嵌入向量本身包含丰富的语义信息，可用于其他应用程序，如数据分析。但是，这些模型中的机制和嵌入向量本身变化很大，使其难以理解和比较。因此，我们需要以多嵌入交互的角度来分析和比较知识图谱嵌入方法。具体而言，我们研究了三种流行的方法（CP、DistMult和ComplEx），并研究它们的不同嵌入向量如何相互作用。我们的分析揭示了ComplEx性能更好的原因，并为开发新模型提供了见解。

    Knowledge graph is a popular format for representing knowledge, with many applications to semantic search engines, question-answering systems, and recommender systems. Real-world knowledge graphs are usually incomplete, so knowledge graph embedding methods, such as Canonical decomposition/Parallel factorization (CP), DistMult, and ComplEx, have been proposed to address this issue. These methods represent entities and relations as embedding vectors in semantic space and predict the links between them. The embedding vectors themselves contain rich semantic information and can be used in other applications such as data analysis. However, mechanisms in these models and the embedding vectors themselves vary greatly, making it difficult to understand and compare them. Given this lack of understanding, we risk using them ineffectively or incorrectly, particularly for complicated models, such as CP, with two role-based embedding vectors, or the state-of-the-art ComplEx model, with complex-valu
    
[^92]: 集成采样

    Ensemble Sampling. (arXiv:1705.07347v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1705.07347](http://arxiv.org/abs/1705.07347)

    本文介绍了集成采样，以近似Thompson采样并在复杂模型下保持可行性，将大大扩展Thompson采样的应用范围。

    

    Thompson采样已经成为一种解决多种在线决策问题的有效启发式算法。在其基本形式中，该算法需要计算并从模型的后验分布中采样，仅在简单特殊情况下才可行。本文介绍了集成采样，旨在近似Thompson采样，同时在复杂模型（如神经网络）的面前保持可行性。集成采样大大扩展了适用Thompson采样的应用范围。我们建立了支持该方法的理论基础，并提供了进一步的计算结果来提供更多见解。

    Thompson sampling has emerged as an effective heuristic for a broad range of online decision problems. In its basic form, the algorithm requires computing and sampling from a posterior distribution over models, which is tractable only for simple special cases. This paper develops ensemble sampling, which aims to approximate Thompson sampling while maintaining tractability even in the face of complex models such as neural networks. Ensemble sampling dramatically expands on the range of applications for which Thompson sampling is viable. We establish a theoretical basis that supports the approach and present computational results that offer further insight.
    

