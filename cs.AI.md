# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Exploration of TPUs for AI Applications.](http://arxiv.org/abs/2309.08918) | 本文探索了TPUs在人工智能应用中的性能和在边缘计算中的实现，并提供了与其他芯片架构的性能比较。结果表明，TPUs可以在云计算和边缘计算中提供显著的性能改进。 |
| [^2] | [Bidirectional Graph GAN: Representing Brain Structure-Function Connections for Alzheimer's Disease.](http://arxiv.org/abs/2309.08916) | 本研究提出了一种双向图生成对抗网络（BGGAN），用于表示阿尔茨海默病（AD）的脑结构-功能连接。通过特殊设计的内部图卷积网络模块和平衡器模块，该方法能够准确地学习结构域和功能域之间的映射函数，并解决模式坍塌问题，同时学习结构和功能特征的互补性。 |
| [^3] | [A Statistical Turing Test for Generative Models.](http://arxiv.org/abs/2309.08913) | 本研究提出了一个统计图灵测试的框架，用于量化人类和机器在给定评估环境下生成内容分布的差异，并演示了如何使用该框架评估生成模型在实现人类水平能力方面的进展。 |
| [^4] | [V2CE: Video to Continuous Events Simulator.](http://arxiv.org/abs/2309.08891) | 本文介绍了一种视频到事件流的转换方法，考虑到DVS的特定特征，通过精心设计的损失函数提高生成的事件体素的质量。同时，提出了一种新颖的局部动态感知时间戳推断策略，以准确地恢复事件时间戳。 |
| [^5] | [GCL: Gradient-Guided Contrastive Learning for Medical Image Segmentation with Multi-Perspective Meta Labels.](http://arxiv.org/abs/2309.08888) | 这项研究提出了一种基于梯度引导的对比学习方法，在医学图像分割中利用多视角元标签解决了"语义矛盾"的问题。 |
| [^6] | [Solving Satisfiability Modulo Counting for Symbolic and Statistical AI Integration With Provable Guarantees.](http://arxiv.org/abs/2309.08883) | 我们提出了XOR-SMC，一个具有访问NP-oracles权限的多项式算法，以解决高度难解的可满足性模数问题，并具有恒定的近似保证。 |
| [^7] | [ChatGPT-4 with Code Interpreter can be used to solve introductory college-level vector calculus and electromagnetism problems.](http://arxiv.org/abs/2309.08881) | ChatGPT-4与代码解释器的组合能够在大多数情况下令人满意地解决介绍性大学级的向量微积分和电磁学问题。 |
| [^8] | [Data-Driven H-infinity Control with a Real-Time and Efficient Reinforcement Learning Algorithm: An Application to Autonomous Mobility-on-Demand Systems.](http://arxiv.org/abs/2309.08880) | 本文提出了一种实时高效的强化学习算法，可以解决线性离散时间系统的H-infinity控制问题。该算法降低了计算复杂度，完全无模型，且不需要初始稳定策略，能够收敛到闭式解。 |
| [^9] | [PDFTriage: Question Answering over Long, Structured Documents.](http://arxiv.org/abs/2309.08872) | PDFTriage是一种处理长篇结构化文档问答的方法，通过使用结构或内容来检索上下文，解决了大型语言模型在问答中遇到的问题。 |
| [^10] | [MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding.](http://arxiv.org/abs/2309.08868) | 我们提出了一种名为MHLAT的简单但有效的模型，利用多跳标签关注来提供更准确和丰富的表示。实验结果表明，我们的方法在所有七个度量标准上都实现了明显更好或有竞争力的性能，并且需要优化的参数更少。 |
| [^11] | [Trajectory Tracking Control of Skid-Steering Mobile Robots with Slip and Skid Compensation using Sliding-Mode Control and Deep Learning.](http://arxiv.org/abs/2309.08863) | 本文提出一种新的轨迹跟踪技术，通过滑模控制和深度学习，在户外环境中实现了可行的移动机器人轨迹跟踪和在线滑动与打滑补偿。 |
| [^12] | [Emerging Approaches for THz Array Imaging: A Tutorial Review and Software Tool.](http://arxiv.org/abs/2309.08844) | 本文是一篇关于太赫兹阵列成像方法的教程综述，介绍了太赫兹技术在高分辨率成像和高吞吐量通信方面的应用，并讨论了面临的挑战。 |
| [^13] | [Bias and Fairness in Chatbots: An Overview.](http://arxiv.org/abs/2309.08836) | 这篇论文概述了现代聊天机器人设计中的偏见和公平性问题，并介绍了聊天机器人的历史、偏见来源和公平性保护方面的考虑因素。 |
| [^14] | [SLIDE: Reference-free Evaluation for Machine Translation using a Sliding Document Window.](http://arxiv.org/abs/2309.08832) | 本论文提出了一种名为SLIDE的度量方法，通过使用滑动文档窗口来评估机器翻译质量，该方法在某些情况下甚至能消除与参考度量之间的差距，表明源语言上下文可能提供了与人类参考相同的信息。 |
| [^15] | [S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking in the Era of LLMs.](http://arxiv.org/abs/2309.08827) | S3-DST是基于LLM的开放域对话中结构化的对话分段和状态跟踪方法，利用Pre-Analytical Recollection机制提高长上下文跟踪。 |
| [^16] | [Distributionally Robust Post-hoc Classifiers under Prior Shifts.](http://arxiv.org/abs/2309.08825) | 研究了先验偏移下的分布鲁棒事后分类器的训练问题，通过在预训练模型的预测上进行缩放调整，以最小化目标分布周围的分布鲁棒损失。 |
| [^17] | [GPT as a Baseline for Recommendation Explanation Texts.](http://arxiv.org/abs/2309.08817) | 本研究建立了以GPT为基准，探索了现代模型生成的电影推荐解释对用户的帮助。研究发现，参与者认为之前看过的电影的评论更好。总体而言，现代语言模型是一个有希望的推荐解释来源。 |
| [^18] | [URA*: Uncertainty-aware Path Planning using Image-based Aerial-to-Ground Traversability Estimation for Off-road Environments.](http://arxiv.org/abs/2309.08814) | 本研究提出了一种基于不确定性感知的路径规划方法 URA*，利用空中图像进行非道路环境下的自主导航。其中，通过集成的卷积神经网络模型对感兴趣区域的空中图像进行像素级可通过性估计。 |
| [^19] | [SHAPNN: Shapley Value Regularized Tabular Neural Network.](http://arxiv.org/abs/2309.08799) | SHAPNN是一种使用Shapley值正则化的表格型神经网络，能够提供有效的解释并改善模型的性能和连续学习能力。 |
| [^20] | [D3: Data Diversity Design for Systematic Generalization in Visual Question Answering.](http://arxiv.org/abs/2309.08798) | 本论文研究了视觉问答中系统一般化的关键因素，发现简单任务的多样性在实现系统一般化中起到了重要的作用，这意味着不必收集大量和多样的复杂任务。 |
| [^21] | [Privacy-preserving Early Detection of Epileptic Seizures in Videos.](http://arxiv.org/abs/2309.08794) | 这项研究提出了一种视频中隐私保护的癫痫发作早期检测框架，通过提取光流特征并使用逐步知识蒸馏的方法，解决了当前方法存在的隐私泄露和实时检测问题。 |
| [^22] | [Fin-Fact: A Benchmark Dataset for Multimodal Financial Fact Checking and Explanation Generation.](http://arxiv.org/abs/2309.08793) | Fin-Fact是一个用于多模态金融事实核查和解释生成的基准数据集，通过提供专业的注释和证据，以及多模态信息源来增强事实性分析，从而打击金融领域的错误信息，促进透明度，并建立信任。 |
| [^23] | [Projected Task-Specific Layers for Multi-Task Reinforcement Learning.](http://arxiv.org/abs/2309.08776) | 本研究提出了一种新的架构，Projected Task-Specific Layers (PTSL)，通过任务特定的层来表达共享和可变的任务信息，成功解决了多任务强化学习中的推广和干扰问题。 |
| [^24] | [Enhance audio generation controllability through representation similarity regularization.](http://arxiv.org/abs/2309.08773) | 这篇论文介绍了一种创新的方法，通过在训练过程中强调音频和文本表示之间的对齐来增强音频生成的可控性。实验结果表明，在音乐和音频生成任务中，这种方法取得了良好的效果。 |
| [^25] | [Rethinking Cross-Domain Pedestrian Detection: A Background-Focused Distribution Alignment Framework for Instance-Free One-Stage Detectors.](http://arxiv.org/abs/2309.08771) | 本文提出了一个以背景为重点的分布对齐框架，用于解决无实例单阶段检测器中的前景背景对齐问题。 |
| [^26] | [AlbNER: A Corpus for Named Entity Recognition in Albanian.](http://arxiv.org/abs/2309.08741) | 本文介绍了一个用于阿尔巴尼亚命名实体识别的语料库AlbNER，该语料库由阿尔巴尼亚维基百科文章中收集的900个带有标记命名实体的句子组成。初步结果表明，模型大小对NER性能影响小，而语言迁移有着显著的影响。这些资源和结果为未来实验提供了基线。 |
| [^27] | [MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response.](http://arxiv.org/abs/2309.08730) | MusiLingo是一个利用预训练的语言模型将音乐和文本相结合的系统，可以生成音乐字幕和回答音乐相关的查询。通过使用投影层对齐音乐表示，该系统成功地将音乐音频和文本环境联系起来，同时使用了一个新的数据集来推动领域的进展。 |
| [^28] | [SculptBot: Pre-Trained Models for 3D Deformable Object Manipulation.](http://arxiv.org/abs/2309.08728) | 本文研究了在机器人操作中处理可变形物体的挑战，并提出了一种使用点云作为状态表示的系统，利用预训练的点云重构Transformer学习动力学模型来预测材料变形。通过设计新颖的动作采样算法来提高模型规划器的效率。实验结果表明该系统能够成功捕捉可变形物体的变形。 |
| [^29] | [Modelling Irregularly Sampled Time Series Without Imputation.](http://arxiv.org/abs/2309.08698) | SLAN是一种无需插值的方法，可以建模不规则采样时间序列，利用动态适应的LSTM架构来捕捉每个传感器的局部摘要，并在整个观测期间维持一个全局摘要状态。 |
| [^30] | [Resolving Legalese: A Multilingual Exploration of Negation Scope Resolution in Legal Documents.](http://arxiv.org/abs/2309.08695) | 本研究通过多语言探索，解决了法律文件中否定范围解析的挑战。实验结果表明，以往模型在处理多语言法律数据时表现不佳，因此我们发布了一套新的法庭判决标注数据用于改进解析效果，并取得了高达86.7％的标记级F1分。 |
| [^31] | [Fake News Detectors are Biased against Texts Generated by Large Language Models.](http://arxiv.org/abs/2309.08674) | 假新闻检测器倾向于将大型语言模型生成的内容标记为假新闻，而将人工编写的假新闻误分类为真实，我们提出了一种通过敌对训练和LLM改写的真实新闻等方法来解决这个问题，并取得了显著的改进。 |
| [^32] | [Adversarial Attacks on Tables with Entity Swap.](http://arxiv.org/abs/2309.08650) | 本论文研究了对包含实体交换的表格进行的对抗攻击。作者提出了一种针对列类型注释任务的逃避性实体交换攻击，通过采用基于相似度的采样策略生成对抗性示例，成功导致性能下降了高达70%。 |
| [^33] | [MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings.](http://arxiv.org/abs/2309.08648) | MAPLE是一个利用大型语言模型嵌入进行移动应用预测的模型，通过严格测试验证了其在解密复杂模式和理解用户环境方面的能力，并强调了语言模型在不同领域中的广泛适用性。 |
| [^34] | [Intent Detection at Scale: Tuning a Generic Model using Relevant Intents.](http://arxiv.org/abs/2309.08647) | 本研究提出了一种有效的方法，通过将通用模型与每个客户的相关意图列表相结合，将意图检测扩展到不同的客户。这种方法减少了培训和维护成本，同时为客户提供个性化体验，并在生产环境中展现出卓越的性能。 |
| [^35] | [Cure the headache of Transformers via Collinear Constrained Attention.](http://arxiv.org/abs/2309.08646) | 通过引入共线约束注意力（CoCA）结构，解决Transformer模型中的头痛问题，实现了出色的外推性能和提高的计算效率。 |
| [^36] | [A Stochastic Online Forecast-and-Optimize Framework for Real-Time Energy Dispatch in Virtual Power Plants under Uncertainty.](http://arxiv.org/abs/2309.08642) | 本论文提出了一个适应不确定性的实时能源调度框架，该框架通过整合深度学习预测和随机优化，并通过在线数据增强和模型微调来解决数据波动、模型差异和环境扰动等不确定性问题。 |
| [^37] | [TextBind: Multi-turn Interleaved Multimodal Instruction-following.](http://arxiv.org/abs/2309.08637) | TextBind是一个注释极少的框架，用于将较大规模的语言模型赋予多轮交错多模态指令跟随能力，并通过图像-标题对生成多轮多模态指令-回应对话。这个框架对于解决实际任务具有重要意义，并为未来的研究提供了数据集、模型和演示。 |
| [^38] | [ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing? (ver. 23Q3).](http://arxiv.org/abs/2309.08636) | 本文综合分析了在人文学科和考古学领域中六个AI聊天机器人在学术写作方面的能力和局限性，发现它们在重新组合现有知识方面表现出色，但在产生原创科学内容方面存在问题。 |
| [^39] | [Doubly High-Dimensional Contextual Bandits: An Interpretable Model for Joint Assortment-Pricing.](http://arxiv.org/abs/2309.08634) | 本论文提出了一种双高维上下文强化学习算法，用于解决联合组合-定价问题，通过简单而灵活的模型捕捉协变量和行为之间的相互作用，同时保持可解释性。该方法兼容多种结构化的线性强化学习和定价模型，提供了一种计算可行的流程。 |
| [^40] | [Pretraining on the Test Set Is All You Need.](http://arxiv.org/abs/2309.08632) | 这项研究通过在测试集上进行预训练，使用精心构建的非合成数据混合，成功开发出一个在多个学术基准测试上表现出色的Transformer语言模型phi-CTNL。 |
| [^41] | [Large Language Models Can Infer Psychological Dispositions of Social Media Users.](http://arxiv.org/abs/2309.08631) | 大型语言模型能够通过分析社交媒体用户的数字足迹推断他们的心理倾向，具体表现为从Facebook状态更新中推断五大人格特质。研究发现，推断得分与自我报告得分之间存在相关性，但在性别和年龄方面存在偏见。 |
| [^42] | [Challenges in Annotating Datasets to Quantify Bias in Under-represented Society.](http://arxiv.org/abs/2309.08624) | 最近研究越来越关注衡量偏见和开发去偏见技术，但在少数社群相关的偏见衡量方面的研究仍然很少。本研究以新西兰人口为例，创建了用于衡量少数社群中偏见的基准数据集，并介绍了在这个过程中遇到的挑战。 |
| [^43] | [Representation Learning in Low-rank Slate-based Recommender Systems.](http://arxiv.org/abs/2309.08622) | 该论文提出了在推荐系统中使用低秩MDP将推荐问题视为在线RL问题，并通过提出的算法实现了高效的表示学习。 |
| [^44] | [Exploring Social Choice Mechanisms for Recommendation Fairness in SCRUF.](http://arxiv.org/abs/2309.08621) | 本文通过使用社会选择机制，探索了多个多方面公平应用中的选择机制选项，结果显示不同的选择和分配机制会产生不同但一致的公平性/准确性权衡结果，并且多智能体的构成使得系统能够适应用户人口的动态变化。 |
| [^45] | [Variance Reduction of Resampling for Sequential Monte Carlo.](http://arxiv.org/abs/2309.08620) | 本研究提出了一种重要性重采样方案，通过引入重复确定性区域和中位数遍历性的方法，实现了最低的方差，使得顺序蒙特卡洛方法在逼近隐马尔可夫模型时更快且准确。 |
| [^46] | [Energy Concerns with HPC Systems and Applications.](http://arxiv.org/abs/2309.08615) | 该论文讨论了HPC系统和应用中的能源问题，特别关注了嵌入式计算和超级计算两个上下文中的能源限制和问题。尤其在面对人工智能的应用时，高效的计算支持显得尤为重要。 |
| [^47] | [Analyzing Character and Consciousness in AI-Generated Social Content: A Case Study of Chirper, the AI Social Network.](http://arxiv.org/abs/2309.08614) | 本文研究了AI生成的社交内容中的角色和意识，使用了新的测试方法来评估AI行为。研究发现Chirper在不同情境下展示了出色的自我识别能力。 |
| [^48] | [Multimodal Recommender Systems in the Prediction of Disease Comorbidity.](http://arxiv.org/abs/2309.08613) | 该研究探讨了在医疗领域中利用基于深度学习的推荐系统进行疾病并发症预测的方法。研究使用了NCF和DHF两种新颖的推荐系统，并利用了不同的数据集进行预测。研究结果显示NCF模型在准确率和命中率方面表现较差。 |
| [^49] | [Explaining Vision and Language through Graphs of Events in Space and Time.](http://arxiv.org/abs/2309.08612) | 本论文提出了一种称为时空事件图（GEST）的方法，能够解释、表示和生成视觉和语言故事。通过将GEST图与深度学习模型相结合，可以改善从文本到视频的生成，并提高语义上的文本比较。 |
| [^50] | [Maneuver Decision-Making Through Proximal Policy Optimization And Monte Carlo Tree Search.](http://arxiv.org/abs/2309.08611) | 该论文提出了一种基于近端策略优化和蒙特卡洛树搜索的方法，用于解决机动决策问题。该方法通过训练价值网络和使用蒙特卡洛树搜索来选择行动，从而提高训练性能。实验证明，通过该方法训练的智能体可以根据不同情况做出不同决策。 |
| [^51] | [SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels.](http://arxiv.org/abs/2309.08513) | 本文提出了一种名为“显著通道微调”的简单而有效的方法，通过选择特征图中的部分通道进行微调，实现在低数据资源场景下低参数成本的高效微调，并在多个下游任务中优于全面微调方法。 |
| [^52] | [PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis.](http://arxiv.org/abs/2309.05833) | 本文提出了一种通过提示检索增强的大语言模型（LLM）来增强云事件根本原因分析工具中置信度估计的方法。 |
| [^53] | [CPMR: Context-Aware Incremental Sequential Recommendation with Pseudo-Multi-Task Learning.](http://arxiv.org/abs/2309.04802) | CPMR是一个基于上下文感知的增量顺序推荐系统，通过创建静态嵌入、历史时间状态和上下文时间状态的三个表示，准确地建模了用户随时间变化的表示和兴趣动态的演化。 |
| [^54] | [Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity.](http://arxiv.org/abs/2309.04160) | 本研究提出了一种利用原型患者表示和特征缺失感知校准的间接插补方法，以缓解电子健康记录数据稀疏性问题，通过获取更密集的嵌入来提高预测模型的有效性。 |
| [^55] | [FLM-101B: An Open LLM and How to Train It with $100K Budget.](http://arxiv.org/abs/2309.03852) | 本文介绍了一种开放的LLM模型（FLM-101B）以及如何用10万美元的预算来训练它。通过采用增长策略，可以显著降低LLM训练的成本。同时，引入了一种系统的评估方法，以评估LLM的智能能力。 |
| [^56] | [Diffusion Generative Inverse Design.](http://arxiv.org/abs/2309.02040) | 本文介绍了一种使用去噪扩散模型（DDMs）高效解决反向设计问题的方法，并提出了一种粒子采样算法来进一步改进。 |
| [^57] | [A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture.](http://arxiv.org/abs/2309.01105) | 本研究通过利用大型语言模型（LLM）应用架构实现了生成式AI服务，并开发了一种名为检索增强生成（RAG）模型，以解决信息稀缺和数据不足的挑战。 |
| [^58] | [Explainability for Large Language Models: A Survey.](http://arxiv.org/abs/2309.01029) | 本文调研了大型语言模型的可解释性问题，提出了一个解释技术的分类法，并介绍了基于Transformer的语言模型的解释方法。同时，讨论了评估生成解释的度量标准，以及如何利用解释来调试模型和提高性能。 |
| [^59] | [Over-Squashing in Graph Neural Networks: A Comprehensive survey.](http://arxiv.org/abs/2308.15568) | 过度压缩是图神经网络面临的关键挑战，它限制了节点之间的长程信息传递，影响了在需要广泛上下文洞察力的情况下的准确预测。 |
| [^60] | [Elucidating the Exposure Bias in Diffusion Models.](http://arxiv.org/abs/2308.15321) | 本文系统地研究了扩散模型中的曝光偏差问题，并提出了一种名为Epsilon Scaling的免训练方法来减轻这一问题。实验结果验证了该方法的有效性。 |
| [^61] | [Fast Feedforward Networks.](http://arxiv.org/abs/2308.14711) | 快速前馈网络是一种对于前馈网络的改进架构，能够以更快的速度进行推理，并具有比专家混合模型更好的训练性能。在视觉转换器中，它们可以仅使用1%的层神经元进行推理，同时保持94.2%的预测性能。 |
| [^62] | [Causality-Based Feature Importance Quantifying Methods: PN-FI, PS-FI and PNS-FI.](http://arxiv.org/abs/2308.14474) | 本文创新地使用因果关系概率量化特征重要性，提出了三种新的特征重要性测量方法：PN-FI、PS-FI和PNS-FI，分别适用于图像识别任务和图像生成任务，并通过RCT实验证明了其有效性。 |
| [^63] | [Language as Reality: A Co-Creative Storytelling Game Experience in 1001 Nights using Generative AI.](http://arxiv.org/abs/2308.12915) | 本文介绍了一款以生成AI为基础的合作性故事游戏《一千零一夜》，玩家通过与语言模型驱动的角色共同创作故事来引导游戏中的现实，挑战游戏世界与现实之间的传统边界。 |
| [^64] | [Efficient Benchmarking (of Language Models).](http://arxiv.org/abs/2308.11696) | 本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。 |
| [^65] | [Exploration of Rashomon Set Assists Explanations for Medical Data.](http://arxiv.org/abs/2308.11446) | 本文提出了一种新的过程来探索和分析医疗数据中的拉舒蒙集合模型，从而超越传统单一模型选择的方法，并通过引入"拉舒蒙检测"算法识别出集合中最不同的模型。 |
| [^66] | [A Survey on Model Compression for Large Language Models.](http://arxiv.org/abs/2308.07633) | 本论文提供了关于大型语言模型的模型压缩综述，探讨了量化、修剪、知识蒸馏等不同方法，并突出介绍了最新进展和创新方法，为实现高效的部署提供了重要思路。 |
| [^67] | [Exploring the Potential of World Models for Anomaly Detection in Autonomous Driving.](http://arxiv.org/abs/2308.05701) | 本文探讨了在自动驾驶领域利用世界模型进行异常检测的潜力，并提供了相关研究的概述和组成部分的联系。 |
| [^68] | [AspectMMKG: A Multi-modal Knowledge Graph with Aspect-aware Entities.](http://arxiv.org/abs/2308.04992) | AspectMMKG是一个具有方面意识的多模态知识图谱，通过匹配图像和不同实体方面，它提供了从多个角度理解实体的能力，并在实体方面链接任务中取得了最先进的性能。 |
| [^69] | [Part-Aware Transformer for Generalizable Person Re-identification.](http://arxiv.org/abs/2308.03322) | 本论文提出了一种基于部件感知的变压器模型，用于泛化的人物再识别。通过设计一个名为交叉ID相似度学习（CSL）的代理任务，模型可以挖掘不同ID之间共享的局部视觉信息，从而学习到通用特征，减轻了领域特定效果的影响。 |
| [^70] | [TempFuser: Learning Tactical and Agile Flight Maneuvers in Aerial Dogfights using a Long Short-Term Temporal Fusion Transformer.](http://arxiv.org/abs/2308.03257) | TempFuser是一种长短时序融合转换器，能够学习空中格斗中的战术和敏捷飞行动作。经过训练，模型成功地学会了复杂的战斗动作，并在面对高级对手时展现出人类一样的战术动作。 |
| [^71] | [LEMMA: Learning Language-Conditioned Multi-Robot Manipulation.](http://arxiv.org/abs/2308.00937) | LEMMA是一个学习语言条件下的多机器人操作的基准，通过专家示范和人类指令进行任务分配和长时间跨度物体操作。它提供了涉及工具使用和传递的复杂操纵任务，并提出了一种模块化分层规划方法作为基线。 |
| [^72] | [You Can Backdoor Personalized Federated Learning.](http://arxiv.org/abs/2307.15971) | 该论文研究了后门攻击对个性化联邦学习的影响，并揭示了部分模型共享的个性化联邦学习方法容易受到后门攻击。研究者提出了三种后门攻击方法，并验证了它们的有效性。 |
| [^73] | [Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection.](http://arxiv.org/abs/2307.13529) | 本论文提出了一个系统化和统一的框架（RmLR），通过结构化文本知识增强人物-物体交互检测，通过再挖掘策略生成更全面的视觉表示，并设计了细粒度的句子和词级对齐以及知识转移策略来解决多对多匹配问题。 |
| [^74] | [Hierarchical Skeleton Meta-Prototype Contrastive Learning with Hard Skeleton Mining for Unsupervised Person Re-Identification.](http://arxiv.org/abs/2307.12917) | 本文提出了一种无监督的层次骨架元-原型对比学习（Hi-MPC）方法，结合硬骨架挖掘，用于无标签3D骨架的人物重新识别。通过构建层次骨架表示并利用元-原型对比学习进行特征提取和聚类，实现了更多信息丰富的骨架特征的利用。 |
| [^75] | [How to Tidy Up a Table: Fusing Visual and Semantic Commonsense Reasoning for Robotic Tasks with Vague Objectives.](http://arxiv.org/abs/2307.11319) | 这项工作提出了一种融合视觉和语义常识推理的方法来解决具有模糊目标的机器人任务中的整理桌子问题。通过利用大规模语言模型的学习，可以推理出人类行为的常识。尽管语言模型的能力受限，但通过考虑感知和低级控制因素，可以解决整理桌子的任务。 |
| [^76] | [Backdoor Attack against Object Detection with Clean Annotation.](http://arxiv.org/abs/2307.10487) | 本文提出了一种在物体检测中进行后门攻击的方法，通过嵌入隐藏的后门，使得模型在正常数据上表现正常，在触发器出现时给出攻击者指定的判断。这对于安全敏感应用如自动驾驶具有严重威胁。 |
| [^77] | [Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition.](http://arxiv.org/abs/2307.09762) | 该论文提出了一种算法框架，通过将模式识别和随机滤波理论的技术结合起来，强化了基于POD的反应扩散复杂网络模型简化技术，在受扰动输入的情况下提高了代理模型的准确性。 |
| [^78] | [vONTSS: vMF based semi-supervised neural topic modeling with optimal transport.](http://arxiv.org/abs/2307.01226) | vONTSS是一种基于vMF和最优传输的半监督神经主题建模方法，它在分类准确率和多样性方面优于其他方法，并且支持无监督主题建模。实验证明，vONTSS比最近的NTM更快。 |
| [^79] | [Text-Driven Foley Sound Generation With Latent Diffusion Model.](http://arxiv.org/abs/2306.10359) | 本文提出了一种基于扩散模型的Foley音效生成系统，可进行文本条件的生成。我们通过迁移学习对系统进行微调，并引入可训练的层来改善文本嵌入，同时也改进了生成的波形。 |
| [^80] | [Variance-Preserving-Based Interpolation Diffusion Models for Speech Enhancement.](http://arxiv.org/abs/2306.08527) | 本研究提出了以方差保持为基础的插值扩散模型用于语音增强，经理论基础和框架推导证明了该方法的有效性，并给出了实际应用示例。同时，通过对困难问题的分析和调整超参数，提高了模型性能和训练便利性。评估结果展示该方法在公共基准上的优越性能。 |
| [^81] | [When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm.](http://arxiv.org/abs/2306.02552) | 从事用户行为分析的学术界一直面临着收集足够高质量用户行为数据的难题，一种解决方案是自动模拟用户行为，近期研究表明，利用大语言模型进行可靠的用户模拟有了重要的突破，将这种模型应用到用户行为分析研究中有着巨大潜力，可能对传统研究范式产生革命性影响。 |
| [^82] | [LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization.](http://arxiv.org/abs/2306.01102) | 本文介绍了利用大语言模型和多样性优化算法相结合的 LLMatic 神经结构搜索算法。该算法在CIFAR-10数据集进行测试，仅进行2000次搜索即可产生高性能网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。 |
| [^83] | [Continually Updating Generative Retrieval on Dynamic Corpora.](http://arxiv.org/abs/2305.18952) | 本文研究了动态语料库上的生成检索。实验结果表明，在静态设置下，生成检索效果优于双编码器，但在动态设置下情况相反。通过使用参数高效的预训练方法，我们的模型DynamicGR在新的语料库上展现出了意外的性能。 |
| [^84] | [KeyPosS: Plug-and-Play Facial Landmark Detection through GPS-Inspired True-Range Multilateration.](http://arxiv.org/abs/2305.16437) | KeyPosS是一种面部标记检测框架，采用真实距离多边定位算法实现快速而准确的检测，避免了传统方法中的计算负担和量化误差问题。 |
| [^85] | [Enhancing Generation through Summarization Duality and Explicit Outline Control.](http://arxiv.org/abs/2305.14459) | 本文提出了一个两阶段的摘要增强的大纲监督生成框架，能够更好地生成明确和合理的大纲，并引入了一个新颖的显式大纲控制方法以更有效地利用生成的大纲。 |
| [^86] | [SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres.](http://arxiv.org/abs/2305.13617) | 这篇论文提出了一种称为SPEECH的模型，它使用能量建模来表示复杂的事件结构，并使用超球来表示事件类别。实验结果表明，SPEECH在事件检测和事件关系抽取任务中表现出卓越的性能。 |
| [^87] | [Decouple knowledge from paramters for plug-and-play language modeling.](http://arxiv.org/abs/2305.11564) | 本文介绍了一种新的插件式预训练模型，其与模型参数中的知识存储分离，采用可编辑和可扩展的键值存储器，通过DPM中的知识检索以可解释的方式利用知识。 |
| [^88] | [AI & Blockchain as sustainable teaching and learning tools to cope with the 4IR.](http://arxiv.org/abs/2305.01088) | 本论文回顾了现有的AI和区块链在教育领域的研究，探讨了这些技术可以提高个性化学习、安全认证和分散式学习网络的潜力和挑战。此外，本文提出了一个模型，将AI和区块链整合到可持续的教育实践中。总之，AI和区块链技术有望成为可持续教学和学习工具。 |
| [^89] | [Categorical Foundations of Explainable AI: A Unifying Formalism of Structures and Semantics.](http://arxiv.org/abs/2304.14094) | 本文采用范畴理论的框架，提出了可解释AI的统一理论体系，为领域中所有重要术语提供了清晰的形式定义，并提供了遵循所提出结构的领域分类法。 |
| [^90] | [SDVRF: Sparse-to-Dense Voxel Region Fusion for Multi-modal 3D Object Detection.](http://arxiv.org/abs/2304.08304) | 提出了一种新的稀疏到密集的体素区域加强融合方法，通过动态投影每个体素内部的稀疏局部点云获得体素区域，更好地对齐和避免背景噪声问题，并通过多尺度融合方法极大地提高了三维物体检测性能。 |
| [^91] | [Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning.](http://arxiv.org/abs/2304.07163) | 本文研究了如何基于Bandit方法将外部建议融入到强化学习中，并提出了三种不同的塑形算法：UCB-PIES（UPIES）， Racing-PIES（RPIES）和Lazy PIES（LPIES）。实验结果表明这些算法在样本复杂度、学习速度和形状质量方面都取得了良好的效果。 |
| [^92] | [AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models.](http://arxiv.org/abs/2304.06364) | AGIEval是一个以人为中心设计的基准测试工具，用于评估基础模型在人类中心标准化考试上的表现。GPT-4在SAT、LSAT和数学比赛方面超越了人类平均表现，展示了当代基础模型在人类级任务中的非凡性能。 |
| [^93] | [Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views.](http://arxiv.org/abs/2304.06024) | 本文提出一种基于场景条件的扩散方法来建模身体姿态分布，以解决在个人视角下3D场景中的人类姿态估计的挑战，训练中无需分类器，采样具有不同的条件和增强的多样性。 |
| [^94] | [Understanding the Usability of AI Programming Assistants.](http://arxiv.org/abs/2303.17125) | 人工智能编程助手在快速完成编程任务方面有用，但输出的代码不适合开发者，导致他们不高频接受AI编程助手的初始建议。 |
| [^95] | [RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via Language-Based Feedback.](http://arxiv.org/abs/2303.07622) | RE-MOVE提出了一种基于语言反馈的自适应策略设计方法，可以使机器人适应实时环境变化，并从人类反馈中学习并适应之前未见过的对抗性场景。 |
| [^96] | [Virtual Guidance as a Mid-level Representation for Navigation.](http://arxiv.org/abs/2303.02731) | 该论文介绍了一种名为“虚拟导航”的新技术，通过在智能体的相机视图上叠加彩色路径或球的形式的视觉指引，以易于理解的导航指令传达抽象的导航信息。实验结果表明，在模拟和真实环境中，虚拟导航在遵循计划路径和避开障碍物等多个指标上优于现有方法。 |
| [^97] | [Sim-and-Real Reinforcement Learning for Manipulation: A Consensus-based Approach.](http://arxiv.org/abs/2302.13423) | 本文提出了一种基于共识的模拟与实际深度强化学习算法，该算法在机器人操纵中表现出可比较的性能，并发现了在模拟中的最佳策略不一定适用于模拟与实际训练，以及模拟智能体数量越多，模拟与实际训练效果越好。 |
| [^98] | [SAT Requires Exhaustive Search.](http://arxiv.org/abs/2302.09512) | 本文证明了对于一些具有大域和长子句的极难例子，要求进行彻底搜索才能解决，这意味着P $\neq$ NP。 |
| [^99] | [Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings.](http://arxiv.org/abs/2302.07729) | 该论文提出了一种使用指针生成网络和SciBERT嵌入来自动生成研究论文亮点的方法。在多个基准数据集上的实验证明，该模型在研究亮点生成方面具有最佳性能。 |
| [^100] | [Developing Driving Strategies Efficiently: A Skill-Based Hierarchical Reinforcement Learning Approach.](http://arxiv.org/abs/2302.02179) | 本文提出了基于技能的分层驾驶策略，通过设计和使用运动原语作为高层动作，大幅减少了训练时间，并且在合并场景中得到了更高性能的驾驶模型。 |
| [^101] | [Neural Operator: Is data all you need to model the world? An insight into the impact of Physics Informed Machine Learning.](http://arxiv.org/abs/2301.13331) | 本文探讨了如何将数据驱动方法与传统技术相结合，以解决工程和物理问题，并指出了机器学习方法的一些主要问题。 |
| [^102] | [One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER.](http://arxiv.org/abs/2301.10410) | 本论文提出了基于协作域前缀调整的跨领域实体识别，使用文本到文本生成的支撑领域相关指导来将知识转移至新域NER任务，避免了先前的为每个领域结束一个全新的NER模型的问题。 |
| [^103] | [Equivariant Representation Learning in the Presence of Stabilizers.](http://arxiv.org/abs/2301.05231) | 引入了一种称为EquIN的方法，用于学习在数据上具有一般群作用等变的表示。通过考虑稳定子，该方法可以提取数据的几何结构。 |
| [^104] | [Settling the Reward Hypothesis.](http://arxiv.org/abs/2212.10420) | 解决奖励假设，明确指明假设成立的目标和目的的隐含要求。 |
| [^105] | [Reasoning with Language Model Prompting: A Survey.](http://arxiv.org/abs/2212.09597) | 本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。 |
| [^106] | [FedALA: Adaptive Local Aggregation for Personalized Federated Learning.](http://arxiv.org/abs/2212.01197) | FedALA是一种用于个性化联邦学习的方法，通过自适应局部聚合（ALA）模块来解决统计异质性问题，并在广泛的实验证明中超过了11种最先进的基准模型。 |
| [^107] | [VRDU: A Benchmark for Visually-rich Document Understanding.](http://arxiv.org/abs/2211.15421) | 本研究提出了一个名为VRDU的基准测试，以更全面地反映实际文档的复杂性，其中包含具有挑战性的丰富模式、复杂模板和多样的布局。该基准测试可用于评估文档中提取结构化数据的模型。 |
| [^108] | [A fermion neural network with efficient optimization and quantum applicability.](http://arxiv.org/abs/2211.05793) | 本文提出了一种费米子神经网络（FNN），它将输入作为初始层，输出物理特性，建立了一种高效的优化方法，可应用于具有相互作用的硬量子系统，而且能够精确地确定拓扑相和紧凑电荷序，其量子特性带来多种优势。 |
| [^109] | [Formalizing Statistical Causality via Modal Logic.](http://arxiv.org/abs/2210.16751) | 提出了一种基于模态逻辑的形式语言，用于描述和解释统计因果关系，并且能够指定和解释统计因果推断的正确性。 |
| [^110] | [Generative Knowledge Graph Construction: A Review.](http://arxiv.org/abs/2210.12714) | 本文综述了生成式知识图谱构建领域的最新进展，包括方法分类和优劣分析，并提出了未来的研究方向。 |
| [^111] | [Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction.](http://arxiv.org/abs/2210.10709) | 提出了一种以检索增强的架构感知参考作为提示的方法，可动态利用人类注释和弱监督数据所继承的架构和知识，指导生成具有更好语义连贯性和一致性的结构化知识，从而在数据效率和知识质量方面具有优越性。 |
| [^112] | [Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study.](http://arxiv.org/abs/2210.10678) | 本文针对低资源环境中的关系抽取进行了实证研究，并提出了三种方案来提高性能，包括使用提示方法、平衡方法和数据增强技术。通过对8个关系抽取数据集的广泛比较，实验结果表明，虽然基于提示的调整有益于低资源关系抽取，但仍有改进空间，尤其是跨句子上下文中的多个关系三元组的抽取。 |
| [^113] | [Real2Sim2Real Transfer for Control of Cable-driven Robots via a Differentiable Physics Engine.](http://arxiv.org/abs/2209.06261) | 本文描述了一种基于可微物理引擎的真实世界到仿真世界转移的策略，该策略通过对真实机器人的有限数据进行迭代训练，以减少实到虚之间的差距并产生准确的仿真。该策略在索驱动张力结构机器人上得到了测试，并证明了其有效性。 |
| [^114] | [The emergence of division of labor through decentralized social sanctioning.](http://arxiv.org/abs/2208.05568) | 本研究通过引入社会规范模型，展示了分散社会制裁的出现模式能够解决以自利为导向的终身学习个体中的分工问题。 |
| [^115] | [Artificial Intelligence Techniques for Next-Generation Mega Satellite Networks.](http://arxiv.org/abs/2207.00414) | 本文介绍了如何使用人工智能技术来解决超级卫星网络通信中的挑战，包括卫星间链路、短暂时间过程和卫星覆盖范围等问题。 |
| [^116] | [An Empirical Study of Retrieval-enhanced Graph Neural Networks.](http://arxiv.org/abs/2206.00362) | 这项研究考察了检索增强的图神经网络在图数据集中的有效性，设计了一种称为GRAPHRETRIEVAL的检索增强方案，为图神经网络中学习的有用信息提供了增强。 |
| [^117] | [Two-Dimensional Quantum Material Identification via Self-Attention and Soft-labeling in Deep Learning.](http://arxiv.org/abs/2205.15948) | 通过自注意力和软标签鉴定二维量子材料中实例分割中缺失标注的问题，并使用特殊的机制和损失策略来减少负面影响，取得了实验中令人满意的结果。 |
| [^118] | [Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion.](http://arxiv.org/abs/2205.02357) | 本文提出了一种混合Transformer与多级融合的方法，用于解决多模态知识图谱补全的问题。该方法通过统一的输入-输出架构适用于多样的任务，同时利用多级融合将视觉和文本表示集成起来。 |
| [^119] | [On-Device Learning: A Neural Network Based Field-Trainable Edge AI.](http://arxiv.org/abs/2203.01077) | 本研究介绍了一种基于神经网络的设备上学习方法，针对边缘人工智能应用中的环境因素对准确性造成的影响进行了解决。通过重新训练，在嘈杂环境下显著提高了异常检测的准确性，同时节约了低功耗设备的计算和通信成本。 |
| [^120] | [Reward-Respecting Subtasks for Model-Based Reinforcement Learning.](http://arxiv.org/abs/2202.03466) | 论文提出了一种基于模型的强化学习的方法，通过添加奖励加成的子任务来发现选项，从而解决了以前方法中忽略原始奖励的问题。 |
| [^121] | [DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population.](http://arxiv.org/abs/2201.03335) | DeepKE是一个基于深度学习的知识提取工具包，支持复杂的低资源、文档级和多模态场景，可用于自定义数据集和模型来从非结构化数据中提取信息。 |
| [^122] | [Mathematical Runtime Analysis for the Non-Dominated Sorting Genetic Algorithm II (NSGA-II).](http://arxiv.org/abs/2112.08581) | 这项研究通过数学分析证明了非支配排序遗传算法II（NSGA-II）的运行时间特性，发现当种群规模是帕累托前沿大小的四倍时，NSGA-II具有与其他算法相同的运行时间保证。 |
| [^123] | [Interpretable and Fair Boolean Rule Sets via Column Generation.](http://arxiv.org/abs/2111.08466) | 本文提出了一种通过列生成法来生成可解释和公平的布尔规则集合的方法。该方法在兼顾准确性和简单性的平衡方面优于常用的启发式规则挖掘方法，同时考虑了公平性设置，并扩展了模型以满足两种不同的分类公平性度量。使用近似列生成算法处理大规模数据集，并在实验中取得了良好的结果。 |
| [^124] | [KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction.](http://arxiv.org/abs/2104.07650) | 本文提出了一种名为KnowPrompt的知识感知提示调整方法，通过将关系标签中的潜在知识融入到提示构建中，并通过协同优化的方式，提高了关系抽取任务的性能。 |
| [^125] | [Acting in Delayed Environments with Non-Stationary Markov Policies.](http://arxiv.org/abs/2101.11992) | 该论文介绍了在延迟环境中，学习和规划的马尔可夫决策过程(MDP)框架，证明了在延迟执行的情况下，原始状态空间中的非固定马尔可夫策略可以实现最大奖励，提出了一种解决延迟执行任务的非固定Q-learning风格算法。 |

# 详细

[^1]: 探索TPUs在人工智能应用中的应用

    Exploration of TPUs for AI Applications. (arXiv:2309.08918v1 [cs.AR])

    [http://arxiv.org/abs/2309.08918](http://arxiv.org/abs/2309.08918)

    本文探索了TPUs在人工智能应用中的性能和在边缘计算中的实现，并提供了与其他芯片架构的性能比较。结果表明，TPUs可以在云计算和边缘计算中提供显著的性能改进。

    

    Tensor Processing Units（TPUs）是由Google开发的专门用于深度学习的硬件加速器。本文主要探讨了TPU在人工智能领域的性能和在边缘计算中的实现。首先概述了TPU的设计与神经网络的关系，以及其总体架构、编译技术和支持框架。此外，我们还对云TPU和边缘TPU的性能与其他芯片架构进行了比较分析。接下来讨论了如何利用TPU加速AI工作负载。结果表明，TPU在云计算和边缘计算中均能提供显著的性能改进。此外，我们还强调了需要进一步研究在边缘TPU中部署更多架构的需求，以及在边缘计算中进行更可靠比较的需求。

    Tensor Processing Units (TPUs) are specialized hardware accelerators for deep learning developed by Google. This paper explores the performance of TPU with a focus on AI and its implementation in edge computing. It first provides an overview of TPUs, specifically their design in relation to neural networks, their general architecture, compilation techniques and supporting frameworks. Furthermore, we provide a comparative analysis of Cloud and Edge TPU performance against other counterpart chip architectures. It is then discussed how TPUs can be used to speed up AI workloads. The results show that TPUs can provide significant performance improvements both in cloud and edge computing. Additionally, we address the need for further research for the deployment of more architectures in the Edge TPU, as well as the need for the development of more robust comparisons in edge computing.
    
[^2]: 双向图生成对抗网络：用于阿尔茨海默病的脑结构-功能连接的表示

    Bidirectional Graph GAN: Representing Brain Structure-Function Connections for Alzheimer's Disease. (arXiv:2309.08916v1 [cs.AI])

    [http://arxiv.org/abs/2309.08916](http://arxiv.org/abs/2309.08916)

    本研究提出了一种双向图生成对抗网络（BGGAN），用于表示阿尔茨海默病（AD）的脑结构-功能连接。通过特殊设计的内部图卷积网络模块和平衡器模块，该方法能够准确地学习结构域和功能域之间的映射函数，并解决模式坍塌问题，同时学习结构和功能特征的互补性。

    

    揭示脑疾病的发病机制，包括阿尔茨海默病（AD），脑结构与功能之间的关系至关重要。然而，由于各种原因，将脑结构-功能连接映射是一个巨大的挑战。本文提出了一种双向图生成对抗网络（BGGAN）来表示脑结构-功能连接。具体来说，通过设计一个内部图卷积网络（InnerGCN）模块，BGGAN的生成器可以利用直接和间接脑区域的特征来学习结构域和功能域之间的映射函数。此外，还设计了一个名为Balancer的新模块来平衡生成器和判别器之间的优化。通过将Balancer引入到BGGAN中，结构生成器和功能生成器不仅可以缓解模式坍塌问题，还可以学习结构和功能特征的互补性。实验结果表明该方法能够在AD中准确地表示脑结构-功能连接。

    The relationship between brain structure and function is critical for revealing the pathogenesis of brain disease, including Alzheimer's disease (AD). However, it is a great challenge to map brain structure-function connections due to various reasons. In this work, a bidirectional graph generative adversarial networks (BGGAN) is proposed to represent brain structure-function connections. Specifically, by designing a module incorporating inner graph convolution network (InnerGCN), the generators of BGGAN can employ features of direct and indirect brain regions to learn the mapping function between structural domain and functional domain. Besides, a new module named Balancer is designed to counterpoise the optimization between generators and discriminators. By introducing the Balancer into BGGAN, both the structural generator and functional generator can not only alleviate the issue of mode collapse but also learn complementarity of structural and functional features. Experimental result
    
[^3]: 一个统计图灵测试用于生成模型

    A Statistical Turing Test for Generative Models. (arXiv:2309.08913v1 [cs.AI])

    [http://arxiv.org/abs/2309.08913](http://arxiv.org/abs/2309.08913)

    本研究提出了一个统计图灵测试的框架，用于量化人类和机器在给定评估环境下生成内容分布的差异，并演示了如何使用该框架评估生成模型在实现人类水平能力方面的进展。

    

    人工智能系统在文本、音频和视觉等领域的内容生成能力的出现催生了用于区分内容来源于人还是机器的分类器的发展。这些工作的隐含假设是人类的生成能力与机器的生成能力存在差异。本文提供了一个在统计模式识别语言中量化人类和机器生成内容分布差异的框架，并描述了如何在框架中评估生成模型在向人类水平能力方面的进展，涵盖了多个分析维度。

    The emergence of human-like abilities of AI systems for content generation in domains such as text, audio, and vision has prompted the development of classifiers to determine whether content originated from a human or a machine. Implicit in these efforts is an assumption that the generation properties of a human are different from that of the machine. In this work, we provide a framework in the language of statistical pattern recognition that quantifies the difference between the distributions of human and machine-generated content conditioned on an evaluation context. We describe current methods in the context of the framework and demonstrate how to use the framework to evaluate the progression of generative models towards human-like capabilities, among many axes of analysis.
    
[^4]: V2CE: 视频到连续事件模拟器

    V2CE: Video to Continuous Events Simulator. (arXiv:2309.08891v1 [cs.CV])

    [http://arxiv.org/abs/2309.08891](http://arxiv.org/abs/2309.08891)

    本文介绍了一种视频到事件流的转换方法，考虑到DVS的特定特征，通过精心设计的损失函数提高生成的事件体素的质量。同时，提出了一种新颖的局部动态感知时间戳推断策略，以准确地恢复事件时间戳。

    

    基于动态视觉传感器（DVS）的解决方案近年来在各种计算机视觉任务中受到了广泛关注，其在动态范围、时间分辨率和推理速度方面具有显著优势。然而，与RGB相机等主动像素传感器（APS）设备相比，DVS作为一个相对新兴的视觉传感器，缺乏充足的标记数据集。之前将APS数据转换为事件的努力往往要面对许多问题，如与真实事件之间的领域转移、缺乏定量验证和时间轴内的分层问题。在本文中，我们提出了一种新颖的方法来从多个角度将视频转换为事件流，考虑到DVS的特定特征。一系列精心设计的损失函数显著提高了生成的事件体素的质量。我们还提出了一种新颖的局部动态感知时间戳推断策略，以准确地从事件体素中恢复事件时间戳。

    Dynamic Vision Sensor (DVS)-based solutions have recently garnered significant interest across various computer vision tasks, offering notable benefits in terms of dynamic range, temporal resolution, and inference speed. However, as a relatively nascent vision sensor compared to Active Pixel Sensor (APS) devices such as RGB cameras, DVS suffers from a dearth of ample labeled datasets. Prior efforts to convert APS data into events often grapple with issues such as a considerable domain shift from real events, the absence of quantified validation, and layering problems within the time axis. In this paper, we present a novel method for video-to-events stream conversion from multiple perspectives, considering the specific characteristics of DVS. A series of carefully designed losses helps enhance the quality of generated event voxels significantly. We also propose a novel local dynamic-aware timestamp inference strategy to accurately recover event timestamps from event voxels in a continuo
    
[^5]: GCL: 基于梯度引导的对比学习用于医学图像分割与多视角元标签

    GCL: Gradient-Guided Contrastive Learning for Medical Image Segmentation with Multi-Perspective Meta Labels. (arXiv:2309.08888v1 [cs.CV])

    [http://arxiv.org/abs/2309.08888](http://arxiv.org/abs/2309.08888)

    这项研究提出了一种基于梯度引导的对比学习方法，在医学图像分割中利用多视角元标签解决了"语义矛盾"的问题。

    

    鉴于医学图像分割的标注成本普遍昂贵，设计一种节省标注负担的标注效率方法非常重要。最近，对比学习已经展示出在使用有限标签来增强下游任务的学习稳健表示方面具有巨大潜力。在医学图像场景下，预先准备好的元标签（即医学图像的特定属性信息）本质上揭示了图像之间的语义关系，这些标签已被用来定义正向对。然而，不同元标签所揭示的多视角语义通常不兼容，组合不同的元标签可能会导致“语义矛盾”的问题。为了解决这个问题，我们提出了梯度引导方法来处理“语义矛盾”，通过系统性地统一多视角的元标签，使预训练模型能够获得更好的高水平性能。

    Since annotating medical images for segmentation tasks commonly incurs expensive costs, it is highly desirable to design an annotation-efficient method to alleviate the annotation burden. Recently, contrastive learning has exhibited a great potential in learning robust representations to boost downstream tasks with limited labels. In medical imaging scenarios, ready-made meta labels (i.e., specific attribute information of medical images) inherently reveal semantic relationships among images, which have been used to define positive pairs in previous work. However, the multi-perspective semantics revealed by various meta labels are usually incompatible and can incur intractable "semantic contradiction" when combining different meta labels. In this paper, we tackle the issue of "semantic contradiction" in a gradient-guided manner using our proposed Gradient Mitigator method, which systematically unifies multi-perspective meta labels to enable a pre-trained model to attain a better high-l
    
[^6]: 解决符号和统计人工智能集成的可证明保证的可满足性模数问题

    Solving Satisfiability Modulo Counting for Symbolic and Statistical AI Integration With Provable Guarantees. (arXiv:2309.08883v1 [cs.AI])

    [http://arxiv.org/abs/2309.08883](http://arxiv.org/abs/2309.08883)

    我们提出了XOR-SMC，一个具有访问NP-oracles权限的多项式算法，以解决高度难解的可满足性模数问题，并具有恒定的近似保证。

    

    可满足性模数问题涵盖了需要符号决策和统计推理的问题。它的一般形式捕捉了符号和统计人工智能交叉领域的许多实际问题。可满足性模数问题寻找策略干预以控制概率性结果。解决可满足性模数问题具有高度难解的特性（$\text{NP}^{\text{PP}}$-complete），融合了统计推理和符号推理。先前对可满足性模数问题的研究缺乏可证明的保证和/或在组合约束存在时表现出次优的经验性能力。我们提出了XOR-SMC，一个具有访问NP-oracles权限的多项式算法，以解决高度难解的可满足性模数问题，并具有恒定的近似保证。XOR-SMC通过用随机的XOR约束替换SMC中的模型计数，将高度难解的SMC转化为可满足性问题。实验表明...

    Satisfiability Modulo Counting (SMC) encompasses problems that require both symbolic decision-making and statistical reasoning. Its general formulation captures many real-world problems at the intersection of symbolic and statistical Artificial Intelligence. SMC searches for policy interventions to control probabilistic outcomes. Solving SMC is challenging because of its highly intractable nature($\text{NP}^{\text{PP}}$-complete), incorporating statistical inference and symbolic reasoning. Previous research on SMC solving lacks provable guarantees and/or suffers from sub-optimal empirical performance, especially when combinatorial constraints are present. We propose XOR-SMC, a polynomial algorithm with access to NP-oracles, to solve highly intractable SMC problems with constant approximation guarantees. XOR-SMC transforms the highly intractable SMC into satisfiability problems, by replacing the model counting in SMC with SAT formulae subject to randomized XOR constraints. Experiments o
    
[^7]: ChatGPT-4与代码解释器可用于解决介绍性大学级的向量微积分和电磁学问题

    ChatGPT-4 with Code Interpreter can be used to solve introductory college-level vector calculus and electromagnetism problems. (arXiv:2309.08881v1 [cs.AI])

    [http://arxiv.org/abs/2309.08881](http://arxiv.org/abs/2309.08881)

    ChatGPT-4与代码解释器的组合能够在大多数情况下令人满意地解决介绍性大学级的向量微积分和电磁学问题。

    

    我们评估了ChatGPT 3.5、4以及带有代码解释器的ChatGPT 4在一组大学级工程数学和电磁学问题上的表现，这些问题通常是给大二的电气工程专业学生的。我们选择了一组13个问题，并让ChatGPT多次解决它们，每次都使用一个新的实例（交谈）。我们发现，带有代码解释器的ChatGPT-4能够大部分时间令人满意地解决我们测试的大多数问题，这是相比于没有代码解释器的ChatGPT-4（或3.5）的性能有了重大改进。我们观察到ChatGPT的性能有些随机性，因此我们发现在新的ChatGPT实例中多次解决相同的问题并选择最常见的答案是一种有效的策略。基于我们的研究结果和观察，我们为这个水平的教师和学生提供了一些建议。

    We evaluated ChatGPT 3.5, 4, and 4 with Code Interpreter on a set of college-level engineering-math and electromagnetism problems, such as those often given to sophomore electrical engineering majors. We selected a set of 13 problems, and had ChatGPT solve them multiple times, using a fresh instance (chat) each time. We found that ChatGPT-4 with Code Interpreter was able to satisfactorily solve most problems we tested most of the time -- a major improvement over the performance of ChatGPT-4 (or 3.5) without Code Interpreter. The performance of ChatGPT was observed to be somewhat stochastic, and we found that solving the same problem N times in new ChatGPT instances and taking the most-common answer was an effective strategy. Based on our findings and observations, we provide some recommendations for instructors and students of classes at this level.
    
[^8]: 数据驱动的实时高效强化学习算法在H-infinity控制中的应用：自主移动需求系统

    Data-Driven H-infinity Control with a Real-Time and Efficient Reinforcement Learning Algorithm: An Application to Autonomous Mobility-on-Demand Systems. (arXiv:2309.08880v1 [eess.SY])

    [http://arxiv.org/abs/2309.08880](http://arxiv.org/abs/2309.08880)

    本文提出了一种实时高效的强化学习算法，可以解决线性离散时间系统的H-infinity控制问题。该算法降低了计算复杂度，完全无模型，且不需要初始稳定策略，能够收敛到闭式解。

    

    强化学习是一类人工智能算法，通过在线学习设计适应性最优控制器。本文提出了一种基于模型无关、实时、数据高效的Q-learning算法来解决线性离散时间系统的H-infinity控制问题。计算复杂度从文献中的O(q^3)降低到了提出的算法的O(q^2)，其中q是状态变量、控制输入和干扰的大小之和的二次项。通过在线学习，设计了自适应最优控制器，并学习了动作和评论网络的参数，不需要对系统动力学的了解，使得该算法完全无模型。此外，仅在第一次迭代中需要足够的扰动噪声，而不影响提出的算法。无需初始稳定策略，算法收敛到闭式解。

    Reinforcement learning (RL) is a class of artificial intelligence algorithms being used to design adaptive optimal controllers through online learning. This paper presents a model-free, real-time, data-efficient Q-learning-based algorithm to solve the H$_{\infty}$ control of linear discrete-time systems. The computational complexity is shown to reduce from $\mathcal{O}(\underline{q}^3)$ in the literature to $\mathcal{O}(\underline{q}^2)$ in the proposed algorithm, where $\underline{q}$ is quadratic in the sum of the size of state variables, control inputs, and disturbance. An adaptive optimal controller is designed and the parameters of the action and critic networks are learned online without the knowledge of the system dynamics, making the proposed algorithm completely model-free. Also, a sufficient probing noise is only needed in the first iteration and does not affect the proposed algorithm. With no need for an initial stabilizing policy, the algorithm converges to the closed-form 
    
[^9]: PDFTriage: 对长篇结构化文档进行问答

    PDFTriage: Question Answering over Long, Structured Documents. (arXiv:2309.08872v1 [cs.CL])

    [http://arxiv.org/abs/2309.08872](http://arxiv.org/abs/2309.08872)

    PDFTriage是一种处理长篇结构化文档问答的方法，通过使用结构或内容来检索上下文，解决了大型语言模型在问答中遇到的问题。

    

    大型语言模型在处理长篇文档的问答时存在问题，因为文档无法适应语言模型的上下文长度限制。为了解决这个问题，现有的大多数方法集中于从文档中检索相关的上下文，并将其表示为纯文本。然而，像PDF、网页和演示文稿这样的文档是有结构的，包括不同的页码、表格、章节等。将这样的结构化文档表示为纯文本与用户对这些具有丰富结构的文档的认知模型不符。当系统需要从文档中查询上下文时，这种不符会显现出来，甚至简单的问题也可能使问答系统出错。为了弥合处理结构化文档中的基本差距，我们提出了一种名为PDFTriage的方法，使模型能够根据结构或内容检索上下文。我们的实验证明了所提出的PDFTriage的有效性。

    Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM. To overcome this issue, most existing works focus on retrieving the relevant context from the document, representing them as plain text. However, documents such as PDFs, web pages, and presentations are naturally structured with different pages, tables, sections, and so on. Representing such structured documents as plain text is incongruous with the user's mental model of these documents with rich structure. When a system has to query the document for context, this incongruity is brought to the fore, and seemingly trivial questions can trip up the QA system. To bridge this fundamental gap in handling structured documents, we propose an approach called PDFTriage that enables models to retrieve the context based on either structure or content. Our experiments demonstrate the effectiveness of the proposed PDFTriage-augmente
    
[^10]: MHLAT: 自动ICD编码的多跳标签关注模型

    MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding. (arXiv:2309.08868v1 [cs.CL])

    [http://arxiv.org/abs/2309.08868](http://arxiv.org/abs/2309.08868)

    我们提出了一种名为MHLAT的简单但有效的模型，利用多跳标签关注来提供更准确和丰富的表示。实验结果表明，我们的方法在所有七个度量标准上都实现了明显更好或有竞争力的性能，并且需要优化的参数更少。

    

    国际疾病分类（ICD）编码是将ICD诊断代码分配给临床笔记的任务。鉴于大量的标签（近9000个）和庞大的文本（多达8000个标记），这可能是具有挑战性的。然而，与以前的研究中的单通读过程不同，人们倾向于再次阅读文本和标签定义以获得更自信的答案。此外，尽管预训练语言模型已被用于解决这些问题，但它们的内存使用量很大。为了解决上述问题，我们提出了一个简单但有效的模型，称为多跳标签关注（MHLAT），利用多跳标签关注来获取更准确和丰富的表示。对三个基准MIMIC数据集的广泛实验表明，我们的方法在所有七个度量标准上实现了明显更好或有竞争力的性能，并且需要优化的参数更少。

    International Classification of Diseases (ICD) coding is the task of assigning ICD diagnosis codes to clinical notes. This can be challenging given the large quantity of labels (nearly 9,000) and lengthy texts (up to 8,000 tokens). However, unlike the single-pass reading process in previous works, humans tend to read the text and label definitions again to get more confident answers. Moreover, although pretrained language models have been used to address these problems, they suffer from huge memory usage. To address the above problems, we propose a simple but effective model called the Multi-Hop Label-wise ATtention (MHLAT), in which multi-hop label-wise attention is deployed to get more precise and informative representations. Extensive experiments on three benchmark MIMIC datasets indicate that our method achieves significantly better or competitive performance on all seven metrics, with much fewer parameters to optimize.
    
[^11]: 使用滑模控制和深度学习的带有滑动与打滑补偿的履带式移动机器人轨迹跟踪控制

    Trajectory Tracking Control of Skid-Steering Mobile Robots with Slip and Skid Compensation using Sliding-Mode Control and Deep Learning. (arXiv:2309.08863v1 [cs.RO])

    [http://arxiv.org/abs/2309.08863](http://arxiv.org/abs/2309.08863)

    本文提出一种新的轨迹跟踪技术，通过滑模控制和深度学习，在户外环境中实现了可行的移动机器人轨迹跟踪和在线滑动与打滑补偿。

    

    在户外环境和不平地形中，滑动和打滑补偿对移动机器人的导航至关重要。除了常规的户外环境中移动机器人的滑动和打滑危险外，滑动和打滑还会给轨迹跟踪系统带来不确定性，并且使稳定性分析的有效性受到威胁。尽管在该领域有研究，但由于户外环境中轮胎-地面相互作用的复杂性，实现可行的在线滑动和打滑补偿仍具有挑战性。本文提出了一种新的轨迹跟踪技术，能够在户外环境中实现车辆级的实时滑动和打滑补偿。采用滑模控制技术设计了鲁棒的轨迹跟踪系统，可以考虑到这种类型机器人的参数不确定性。在控制反馈循环中，将两个先前开发的深度学习模型[1]，[2]集成用于估计

    Slip and skid compensation is crucial for mobile robots' navigation in outdoor environments and uneven terrains. In addition to the general slipping and skidding hazards for mobile robots in outdoor environments, slip and skid cause uncertainty for the trajectory tracking system and put the validity of stability analysis at risk. Despite research in this field, having a real-world feasible online slip and skid compensation is still challenging due to the complexity of wheel-terrain interaction in outdoor environments. This paper presents a novel trajectory tracking technique with real-world feasible online slip and skid compensation at the vehicle-level for skid-steering mobile robots in outdoor environments. The sliding mode control technique is utilized to design a robust trajectory tracking system to be able to consider the parameter uncertainty of this type of robot. Two previously developed deep learning models [1], [2] are integrated into the control feedback loop to estimate the
    
[^12]: 新兴的太赫兹阵列成像方法：一篇教程综述和软件工具

    Emerging Approaches for THz Array Imaging: A Tutorial Review and Software Tool. (arXiv:2309.08844v1 [eess.SP])

    [http://arxiv.org/abs/2309.08844](http://arxiv.org/abs/2309.08844)

    本文是一篇关于太赫兹阵列成像方法的教程综述，介绍了太赫兹技术在高分辨率成像和高吞吐量通信方面的应用，并讨论了面临的挑战。

    

    随着5G、6G和物联网应用的日益关注，通信和传感技术近年来迅速从毫米波发展到太赫兹。在电磁硬件方面取得重大进展的推动下，可以利用毫米波和太赫兹频率范围（分别为30 GHz至300 GHz和300 GHz至3000 GHz）进行各种应用。太赫兹系统的主要特点是高带宽传输，可实现超高分辨率成像和高吞吐量通信；然而，太赫兹技术的普及还面临着硬件和算法方面的挑战。由毫米波和太赫兹频率组成的频谱非常适合用于亚毫米分辨率的合成孔径雷达（SAR）成像，适用于材料表征和无损测试（NDT）等各种任务。本文对太赫兹SAR系统和算法进行了教程综述。

    Accelerated by the increasing attention drawn by 5G, 6G, and Internet of Things applications, communication and sensing technologies have rapidly evolved from millimeter-wave (mmWave) to terahertz (THz) in recent years. Enabled by significant advancements in electromagnetic (EM) hardware, mmWave and THz frequency regimes spanning 30 GHz to 300 GHz and 300 GHz to 3000 GHz, respectively, can be employed for a host of applications. The main feature of THz systems is high-bandwidth transmission, enabling ultra-high-resolution imaging and high-throughput communications; however, challenges in both the hardware and algorithmic arenas remain for the ubiquitous adoption of THz technology. Spectra comprising mmWave and THz frequencies are well-suited for synthetic aperture radar (SAR) imaging at sub-millimeter resolutions for a wide spectrum of tasks like material characterization and nondestructive testing (NDT). This article provides a tutorial review of systems and algorithms for THz SAR in 
    
[^13]: 聊天机器人中的偏见和公平性: 一种概述

    Bias and Fairness in Chatbots: An Overview. (arXiv:2309.08836v1 [cs.CL])

    [http://arxiv.org/abs/2309.08836](http://arxiv.org/abs/2309.08836)

    这篇论文概述了现代聊天机器人设计中的偏见和公平性问题，并介绍了聊天机器人的历史、偏见来源和公平性保护方面的考虑因素。

    

    聊天机器人已经研究了半个多世纪。随着近年来自然语言处理技术的快速发展，使用大型语言模型的聊天机器人现在备受关注。与传统的聊天机器人相比，现代聊天机器人更强大，并已在实际应用中使用。然而，在现代聊天机器人设计中存在偏见和公平性问题。由于训练数据量巨大、模型规模庞大且缺乏可解释性，现代聊天机器人的偏见缓解和公平保护具有挑战性。因此，本文对聊天机器人系统中的偏见和公平性进行了全面概述。首先回顾了聊天机器人的历史和类别。然后，分析了应用中的偏见来源和潜在危害。研究了设计公平和无偏的聊天机器人系统的考虑因素。最后，讨论了未来的研究方向。

    Chatbots have been studied for more than half a century. With the rapid development of natural language processing (NLP) technologies in recent years, chatbots using large language models (LLMs) have received much attention nowadays. Compared with traditional ones, modern chatbots are more powerful and have been used in real-world applications. There are however, bias and fairness concerns in modern chatbot design. Due to the huge amounts of training data, extremely large model sizes, and lack of interpretability, bias mitigation and fairness preservation of modern chatbots are challenging. Thus, a comprehensive overview on bias and fairness in chatbot systems is given in this paper. The history of chatbots and their categories are first reviewed. Then, bias sources and potential harms in applications are analyzed. Considerations in designing fair and unbiased chatbot systems are examined. Finally, future research directions are discussed.
    
[^14]: SLIDE: 使用滑动文档窗口进行无参考机器翻译评估

    SLIDE: Reference-free Evaluation for Machine Translation using a Sliding Document Window. (arXiv:2309.08832v1 [cs.CL])

    [http://arxiv.org/abs/2309.08832](http://arxiv.org/abs/2309.08832)

    本论文提出了一种名为SLIDE的度量方法，通过使用滑动文档窗口来评估机器翻译质量，该方法在某些情况下甚至能消除与参考度量之间的差距，表明源语言上下文可能提供了与人类参考相同的信息。

    

    基于参考的度量通常在句子级别上优于仅能访问源语言和系统输出的质量估计度量。这并不奇怪，因为参考能够消除源语言中可能存在的歧义。我们研究了是否可以用额外的源语言上下文有效地替代参考。我们提出了一种度量方法，SLIDE（SLiding Document Evaluator），它通过一个滑动窗口在每个测试集中的文档上操作，将每个块输入到未修改的现成质量估计模型中。我们发现，SLIDE在系统准确性的成对比较上较句子级别基线显著提高，有些情况下甚至消除了与参考度量之间的差距。这表明源语言上下文可能提供了与人类参考相同的信息。

    Reference-based metrics that operate at the sentence level typically outperform quality estimation metrics, which have access only to the source and system output. This is unsurprising, since references resolve ambiguities that may be present in the source. We investigate whether additional source context can effectively substitute for a reference. We present a metric, SLIDE (SLiding Document Evaluator), which operates on blocks of sentences using a window that slides over each document in the test set, feeding each chunk into an unmodified, off-the-shelf quality estimation model. We find that SLIDE obtains significantly higher pairwise system accuracy than its sentence-level baseline, in some cases even eliminating the gap with reference-base metrics. This suggests that source context may provide the same information as a human reference.
    
[^15]: S3-DST: 基于LLM的结构化开放域对话分段和状态跟踪

    S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking in the Era of LLMs. (arXiv:2309.08827v1 [cs.CL])

    [http://arxiv.org/abs/2309.08827](http://arxiv.org/abs/2309.08827)

    S3-DST是基于LLM的开放域对话中结构化的对话分段和状态跟踪方法，利用Pre-Analytical Recollection机制提高长上下文跟踪。

    

    传统的对话状态跟踪 (DST) 问题旨在追踪用户在用户-代理对话中的偏好和意图。尽管对于支持狭义领域应用的任务导向性对话系统来说已经足够，但基于大型语言模型 (LLM) 的聊天系统的出现引入了许多开放域对话中的现实复杂性。这些复杂性体现在上下文交互的增加复杂性、涵盖各种主题的延长对话会话以及更频繁的上下文转变等形式。为了处理基于演变的LLM聊天系统引起的这些复杂性，我们提出了在开放域对话系统中对每个段进行联合对话分割和状态跟踪。在适合真正的开放域对话系统的零-shot设置下，我们提出了S3-DST，这是一种结构化提示技术，利用了我们为提高长上下文跟踪而设计的一种新的接地机制 - Pre-Analytical Recollection。

    The traditional Dialogue State Tracking (DST) problem aims to track user preferences and intents in user-agent conversations. While sufficient for task-oriented dialogue systems supporting narrow domain applications, the advent of Large Language Model (LLM)-based chat systems has introduced many real-world intricacies in open-domain dialogues. These intricacies manifest in the form of increased complexity in contextual interactions, extended dialogue sessions encompassing a diverse array of topics, and more frequent contextual shifts. To handle these intricacies arising from evolving LLM-based chat systems, we propose joint dialogue segmentation and state tracking per segment in open-domain dialogue systems. Assuming a zero-shot setting appropriate to a true open-domain dialogue system, we propose S3-DST, a structured prompting technique that harnesses Pre-Analytical Recollection, a novel grounding mechanism we designed for improving long context tracking. To demonstrate the efficacy o
    
[^16]: 先验偏移下的分布鲁棒事后分类器

    Distributionally Robust Post-hoc Classifiers under Prior Shifts. (arXiv:2309.08825v1 [cs.LG])

    [http://arxiv.org/abs/2309.08825](http://arxiv.org/abs/2309.08825)

    研究了先验偏移下的分布鲁棒事后分类器的训练问题，通过在预训练模型的预测上进行缩放调整，以最小化目标分布周围的分布鲁棒损失。

    

    当测试分布偏离训练分布时，机器学习模型的泛化能力显著降低。我们研究了训练模型以应对由类先验或组先验分布变化引起的偏移的问题。存在偏斜的训练先验往往会导致模型对噪声特征过拟合。与现有方法不同，现有方法优化最差或平均性能，而我们的工作是出于对模型鲁棒性质更细粒度控制的需求。我们提出了一种极其轻量级的事后方法，通过对预训练模型的预测进行缩放调整，旨在最小化选择的目标分布周围的分布鲁棒损失。这些调整通过在验证集上求解约束优化问题来计算，并在测试时间应用于模型。

    The generalization ability of machine learning models degrades significantly when the test distribution shifts away from the training distribution. We investigate the problem of training models that are robust to shifts caused by changes in the distribution of class-priors or group-priors. The presence of skewed training priors can often lead to the models overfitting to spurious features. Unlike existing methods, which optimize for either the worst or the average performance over classes or groups, our work is motivated by the need for finer control over the robustness properties of the model. We present an extremely lightweight post-hoc approach that performs scaling adjustments to predictions from a pre-trained model, with the goal of minimizing a distributionally robust loss around a chosen target distribution. These adjustments are computed by solving a constrained optimization problem on a validation set and applied to the model during test time. Our constrained optimization obje
    
[^17]: GPT作为推荐解释文本的基准

    GPT as a Baseline for Recommendation Explanation Texts. (arXiv:2309.08817v1 [cs.AI])

    [http://arxiv.org/abs/2309.08817](http://arxiv.org/abs/2309.08817)

    本研究建立了以GPT为基准，探索了现代模型生成的电影推荐解释对用户的帮助。研究发现，参与者认为之前看过的电影的评论更好。总体而言，现代语言模型是一个有希望的推荐解释来源。

    

    本研究通过建立一个基准，探索了现代模型生成的电影推荐解释文本对用户的帮助，以及用户对这些文本解释的不同组成部分的喜好和不喜好，特别是与现有的人工电影评论的对比。研究发现，参与者对电影之间的排名没有明显差异，也没有对他们之前从未见过的电影评论的个体质量打出明显不同的评分。然而，当评论是他们之前看过的电影时，参与者确实标记评论为显著更好。此外，我们还探讨了参与者认为对每个质量重要的影评文本的具体方面。总体而言，我们确定现代语言模型是一个有希望的推荐解释来源，并计划在将来进一步探索个性化的文本解释。

    In this work, we establish a baseline potential for how modern model-generated text explanations of movie recommendations may help users, and explore what different components of these text explanations that users like or dislike, especially in contrast to existing human movie reviews. We found that participants gave no significantly different rankings between movies, nor did they give significantly different individual quality scores to reviews of movies that they had never seen before. However, participants did mark reviews as significantly better when they were movies they had seen before. We also explore specific aspects of movie review texts that participants marked as important for each quality. Overall, we establish that modern LLMs are a promising source of recommendation explanations, and we intend on further exploring personalizable text explanations in the future.
    
[^18]: URA *：利用基于图像的空中到地面可通过性估计的不确定性感知路径规划在非道路环境中实现无人驾驶

    URA*: Uncertainty-aware Path Planning using Image-based Aerial-to-Ground Traversability Estimation for Off-road Environments. (arXiv:2309.08814v1 [cs.RO])

    [http://arxiv.org/abs/2309.08814](http://arxiv.org/abs/2309.08814)

    本研究提出了一种基于不确定性感知的路径规划方法 URA*，利用空中图像进行非道路环境下的自主导航。其中，通过集成的卷积神经网络模型对感兴趣区域的空中图像进行像素级可通过性估计。

    

    非道路自主导航的主要挑战之一是缺乏可以用于规划机器人路径的地图或道路标志。经典的路径规划方法大多假设已知完美的环境，而不考虑在非道路环境中检测地形和障碍物时固有的感知和传感不确定性。最近在计算机视觉和深度神经网络方面的研究推动了从原始图像中进行地形可通过性分割的能力；然而，使用这些噪声分割地图进行导航和路径规划的可行性尚未得到充分探索。为了解决这个问题，本研究提出了一种基于不确定性感知的路径规划方法 URA*，使用空中图像进行非道路环境下的自主导航。首先使用集成的卷积神经网络 (CNN) 模型对感兴趣区域的空中图像进行像素级可通过性估计。

    A major challenge with off-road autonomous navigation is the lack of maps or road markings that can be used to plan a path for autonomous robots. Classical path planning methods mostly assume a perfectly known environment without accounting for the inherent perception and sensing uncertainty from detecting terrain and obstacles in off-road environments. Recent work in computer vision and deep neural networks has advanced the capability of terrain traversability segmentation from raw images; however, the feasibility of using these noisy segmentation maps for navigation and path planning has not been adequately explored. To address this problem, this research proposes an uncertainty-aware path planning method, URA* using aerial images for autonomous navigation in off-road environments. An ensemble convolutional neural network (CNN) model is first used to perform pixel-level traversability estimation from aerial images of the region of interest. The traversability predictions are represen
    
[^19]: SHAPNN: Shapley Value正则化的表格型神经网络

    SHAPNN: Shapley Value Regularized Tabular Neural Network. (arXiv:2309.08799v1 [cs.LG])

    [http://arxiv.org/abs/2309.08799](http://arxiv.org/abs/2309.08799)

    SHAPNN是一种使用Shapley值正则化的表格型神经网络，能够提供有效的解释并改善模型的性能和连续学习能力。

    

    我们提出了SHAPNN，一种用于监督学习的新型深度表格数据建模架构。我们的方法利用了Shapley值，这是一种用于解释黑盒模型的成熟技术。我们的神经网络使用标准的反向传播优化方法进行训练，并使用实时估计的Shapley值进行正则化。我们的方法具有多个优势，包括能够对数据实例和数据集提供有效的解释而不增加计算开销。此外，带有解释的预测作为一种正则化器，改善了模型的性能。此外，正则化的预测增强了模型的连续学习能力。我们在各种公开可用的数据集上评估了我们的方法，并将其与最先进的深度神经网络模型进行了比较，展示了SHAPNN在AUROC、透明性以及对流数据的鲁棒性方面的卓越性能。

    We present SHAPNN, a novel deep tabular data modeling architecture designed for supervised learning. Our approach leverages Shapley values, a well-established technique for explaining black-box models. Our neural network is trained using standard backward propagation optimization methods, and is regularized with realtime estimated Shapley values. Our method offers several advantages, including the ability to provide valid explanations with no computational overhead for data instances and datasets. Additionally, prediction with explanation serves as a regularizer, which improves the model's performance. Moreover, the regularized prediction enhances the model's capability for continual learning. We evaluate our method on various publicly available datasets and compare it with state-of-the-art deep neural network models, demonstrating the superior performance of SHAPNN in terms of AUROC, transparency, as well as robustness to streaming data.
    
[^20]: D3: 数据多样性设计为系统一般化在视觉问答中。

    D3: Data Diversity Design for Systematic Generalization in Visual Question Answering. (arXiv:2309.08798v1 [cs.AI])

    [http://arxiv.org/abs/2309.08798](http://arxiv.org/abs/2309.08798)

    本论文研究了视觉问答中系统一般化的关键因素，发现简单任务的多样性在实现系统一般化中起到了重要的作用，这意味着不必收集大量和多样的复杂任务。

    

    系统一般化是智能的关键方面，它指的是通过结合已知的子任务和概念来推广到新任务的能力。已经显示影响系统一般化的一个关键因素是训练数据的多样性。然而，多样性可以以多种方式定义，因为数据具有许多变化因素。对于不同方面的数据多样性如何影响系统一般化的更细致的理解尚缺乏。我们在视觉问答（VQA）问题中提供了新的证据，揭示了简单任务的多样性（即由几个子任务和概念组成的任务）在实现系统一般化中的关键作用。这意味着收集大量和多样化的复杂任务可能并非必要，这可能成本高昂。我们证明了这个结果与训练和测试数据之间的相似性无关，并适用于众所周知的神经网络家族。

    Systematic generalization is a crucial aspect of intelligence, which refers to the ability to generalize to novel tasks by combining known subtasks and concepts. One critical factor that has been shown to influence systematic generalization is the diversity of training data. However, diversity can be defined in various ways, as data have many factors of variation. A more granular understanding of how different aspects of data diversity affect systematic generalization is lacking. We present new evidence in the problem of Visual Question Answering (VQA) that reveals that the diversity of simple tasks (i.e. tasks formed by a few subtasks and concepts) plays a key role in achieving systematic generalization. This implies that it may not be essential to gather a large and varied number of complex tasks, which could be costly to obtain. We demonstrate that this result is independent of the similarity between the training and testing data and applies to well-known families of neural network 
    
[^21]: 视频中隐私保护的癫痫发作早期检测

    Privacy-preserving Early Detection of Epileptic Seizures in Videos. (arXiv:2309.08794v1 [cs.AI])

    [http://arxiv.org/abs/2309.08794](http://arxiv.org/abs/2309.08794)

    这项研究提出了一种视频中隐私保护的癫痫发作早期检测框架，通过提取光流特征并使用逐步知识蒸馏的方法，解决了当前方法存在的隐私泄露和实时检测问题。

    

    本文通过引入一种新颖的框架（SETR-PKD），为基于视频的癫痫发作分类的发展做出了贡献，可以实现视频中癫痫发作的隐私保护早期检测。具体而言，我们的框架具有两个重要组成部分 - (1) 它是基于从癫痫视频中提取的光流特征构建的，这些特征编码了癫痫运动符号学，同时保护了患者的隐私；(2) 它利用基于Transformer的逐步知识蒸馏，逐渐从长视频样本训练的网络中蒸馏出知识，然后传递给将在较短视频样本上操作的网络。因此，我们提出的框架解决了目前方法的局限性，这些方法通过直接对癫痫视频进行操作以及利用完整视频样本进行预测来损害患者的隐私，并阻碍了癫痫发作的实时检测。我们的SETR-PKD框架可以检测到

    In this work, we contribute towards the development of video-based epileptic seizure classification by introducing a novel framework (SETR-PKD), which could achieve privacy-preserved early detection of seizures in videos. Specifically, our framework has two significant components - (1) It is built upon optical flow features extracted from the video of a seizure, which encodes the seizure motion semiotics while preserving the privacy of the patient; (2) It utilizes a transformer based progressive knowledge distillation, where the knowledge is gradually distilled from networks trained on a longer portion of video samples to the ones which will operate on shorter portions. Thus, our proposed framework addresses the limitations of the current approaches which compromise the privacy of the patients by directly operating on the RGB video of a seizure as well as impede real-time detection of a seizure by utilizing the full video sample to make a prediction. Our SETR-PKD framework could detect
    
[^22]: Fin-Fact:一种面向多模态金融事实核查和解释生成的基准数据集

    Fin-Fact: A Benchmark Dataset for Multimodal Financial Fact Checking and Explanation Generation. (arXiv:2309.08793v1 [cs.AI])

    [http://arxiv.org/abs/2309.08793](http://arxiv.org/abs/2309.08793)

    Fin-Fact是一个用于多模态金融事实核查和解释生成的基准数据集，通过提供专业的注释和证据，以及多模态信息源来增强事实性分析，从而打击金融领域的错误信息，促进透明度，并建立信任。

    

    金融领域的事实核查尚未充分探索，该领域缺乏高质量的数据集。本文提出了Fin-Fact，一种用于金融领域多模态事实核查的基准数据集。值得注意的是，它包括专业事实核查人员的注释和证据，提供专业知识和可信度。由于其多模态性质涵盖了文本和视觉内容，Fin-Fact提供了补充信息源，以增强事实性分析。其主要目标是在金融领域打击错误信息，促进透明度，并在财务报告和新闻传播中建立信任。通过提供有深度的解释，Fin-Fact使用户，包括领域专家和终端用户，能够理解事实核查决策的推理过程，验证声明的可信度，并促进对事实核查流程的信任。Fin-Fact数据集以及我们的实验代码可在https://github.com/IIT-DM/Fin-Fact/ 上找到。

    Fact-checking in financial domain is under explored, and there is a shortage of quality dataset in this domain. In this paper, we propose Fin-Fact, a benchmark dataset for multimodal fact-checking within the financial domain. Notably, it includes professional fact-checker annotations and justifications, providing expertise and credibility. With its multimodal nature encompassing both textual and visual content, Fin-Fact provides complementary information sources to enhance factuality analysis. Its primary objective is combating misinformation in finance, fostering transparency, and building trust in financial reporting and news dissemination. By offering insightful explanations, Fin-Fact empowers users, including domain experts and end-users, to understand the reasoning behind fact-checking decisions, validating claim credibility, and fostering trust in the fact-checking process. The Fin-Fact dataset, along with our experimental codes is available at https://github.com/IIT-DM/Fin-Fact/
    
[^23]: 多任务强化学习中的Projected Task-Specific Layers

    Projected Task-Specific Layers for Multi-Task Reinforcement Learning. (arXiv:2309.08776v1 [cs.LG])

    [http://arxiv.org/abs/2309.08776](http://arxiv.org/abs/2309.08776)

    本研究提出了一种新的架构，Projected Task-Specific Layers (PTSL)，通过任务特定的层来表达共享和可变的任务信息，成功解决了多任务强化学习中的推广和干扰问题。

    

    多任务强化学习可以使机器人在家庭和工作场所的各种操作任务中实现规模化。然而，从一个任务推广到另一个任务并减轻负面任务干扰仍然是一个挑战。成功地在任务之间共享信息并取得良好效果将取决于对任务底层结构的有效捕捉。在这项工作中，我们介绍了一种新的架构，即Projected Task-Specific Layers（PTSL），它通过任务特定的层，通过稠密的任务特定的修正来更好地表达共享和可变的任务信息。然后，我们展示了我们的模型在Meta-World的MT10和MT50基准中（包括Sawyer机器人臂上的10个和50个目标条件任务）的表现优于现有技术水平。

    Multi-task reinforcement learning could enable robots to scale across a wide variety of manipulation tasks in homes and workplaces. However, generalizing from one task to another and mitigating negative task interference still remains a challenge. Addressing this challenge by successfully sharing information across tasks will depend on how well the structure underlying the tasks is captured. In this work, we introduce our new architecture, Projected Task-Specific Layers (PTSL), that leverages a common policy with dense task-specific corrections through task-specific layers to better express shared and variable task information. We then show that our model outperforms the state of the art on the MT10 and MT50 benchmarks of Meta-World consisting of 10 and 50 goal-conditioned tasks for a Sawyer arm.
    
[^24]: 增强音频生成可控性的方法：通过表示相似性正则化

    Enhance audio generation controllability through representation similarity regularization. (arXiv:2309.08773v1 [cs.SD])

    [http://arxiv.org/abs/2309.08773](http://arxiv.org/abs/2309.08773)

    这篇论文介绍了一种创新的方法，通过在训练过程中强调音频和文本表示之间的对齐来增强音频生成的可控性。实验结果表明，在音乐和音频生成任务中，这种方法取得了良好的效果。

    

    本文提出了一种创新的方法，通过在模型训练期间强调音频和文本表示之间的对齐，增强对音频生成的控制能力。在基于语言模型的音频生成中，模型利用来自文本和音频标记表示的输入来预测后续的音频标记。然而，当前的配置缺乏明确的正则化来确保所选择的文本表示与语言模型的预测之间的对齐。我们的提议涉及音频和文本表示正则化的整合，特别是在无分类器引导（CFG）阶段，在语言模型训练过程中，文本条件被排除在跨注意力之外。我们提出的表示正则化的目的是最小化与同一训练批次中其他样本的音频和文本相似性差异。在音乐和音频生成任务上的实验结果表明，

    This paper presents an innovative approach to enhance control over audio generation by emphasizing the alignment between audio and text representations during model training. In the context of language model-based audio generation, the model leverages input from both textual and audio token representations to predict subsequent audio tokens. However, the current configuration lacks explicit regularization to ensure the alignment between the chosen text representation and the language model's predictions. Our proposal involves the incorporation of audio and text representation regularization, particularly during the classifier-free guidance (CFG) phase, where the text condition is excluded from cross attention during language model training. The aim of this proposed representation regularization is to minimize discrepancies in audio and text similarity compared to other samples within the same training batch. Experimental results on both music and audio generation tasks demonstrate that
    
[^25]: 重新思考跨领域行人检测：一个以背景为重点的分布对齐框架用于无实例单阶段检测器

    Rethinking Cross-Domain Pedestrian Detection: A Background-Focused Distribution Alignment Framework for Instance-Free One-Stage Detectors. (arXiv:2309.08771v1 [cs.CV])

    [http://arxiv.org/abs/2309.08771](http://arxiv.org/abs/2309.08771)

    本文提出了一个以背景为重点的分布对齐框架，用于解决无实例单阶段检测器中的前景背景对齐问题。

    

    跨领域行人检测旨在将行人检测器从一个标签丰富的领域推广到另一个标签稀缺的领域，这对各种实际应用非常重要。大多数最新的研究都专注于域对齐，以在实例级别或图像级别训练域自适应检测器。从实际角度来看，单阶段检测器更快。因此，我们专注于设计一种针对快速单阶段检测器的跨领域算法，该算法缺乏实例级别的提议，并且只能执行图像级别的特征对齐。然而，纯图像级特征对齐会导致前景和背景的不对齐问题，也就是源领域图像中的前景特征错误地与目标领域图像中的背景特征对齐。为了解决这个问题，我们系统地分析了图像级跨领域对齐中前景和背景的重要性，并发现背景在图像级跨领域对齐中起着更关键的作用。

    Cross-domain pedestrian detection aims to generalize pedestrian detectors from one label-rich domain to another label-scarce domain, which is crucial for various real-world applications. Most recent works focus on domain alignment to train domain-adaptive detectors either at the instance level or image level. From a practical point of view, one-stage detectors are faster. Therefore, we concentrate on designing a cross-domain algorithm for rapid one-stage detectors that lacks instance-level proposals and can only perform image-level feature alignment. However, pure image-level feature alignment causes the foreground-background misalignment issue to arise, i.e., the foreground features in the source domain image are falsely aligned with background features in the target domain image. To address this issue, we systematically analyze the importance of foreground and background in image-level cross-domain alignment, and learn that background plays a more critical role in image-level cross-d
    
[^26]: AlbNER：一个用于阿尔巴尼亚命名实体识别的语料库

    AlbNER: A Corpus for Named Entity Recognition in Albanian. (arXiv:2309.08741v1 [cs.CL])

    [http://arxiv.org/abs/2309.08741](http://arxiv.org/abs/2309.08741)

    本文介绍了一个用于阿尔巴尼亚命名实体识别的语料库AlbNER，该语料库由阿尔巴尼亚维基百科文章中收集的900个带有标记命名实体的句子组成。初步结果表明，模型大小对NER性能影响小，而语言迁移有着显著的影响。这些资源和结果为未来实验提供了基线。

    

    针对阿尔巴尼亚等资源匮乏的语言，如计算语言学和自然语言处理研究中存在着标注文本语料库的严重障碍。本文介绍了AlbNER，这是一个由阿尔巴尼亚维基百科文章中收集的900个带有标记命名实体的句子的语料库。用使用AlbNER数据进行细调和测试的BERT和RoBERTa变体的初步结果表明，模型大小对NER性能的影响很小，而语言迁移的影响很大。AlbNER语料库和这些结果应作为未来实验的基线。

    Scarcity of resources such as annotated text corpora for under-resourced languages like Albanian is a serious impediment in computational linguistics and natural language processing research. This paper presents AlbNER, a corpus of 900 sentences with labeled named entities, collected from Albanian Wikipedia articles. Preliminary results with BERT and RoBERTa variants fine-tuned and tested with AlbNER data indicate that model size has slight impact on NER performance, whereas language transfer has a significant one. AlbNER corpus and these obtained results should serve as baselines for future experiments.
    
[^27]: MusiLingo：利用预训练的语言模型将音乐和文本相结合，实现音乐字幕和查询响应

    MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response. (arXiv:2309.08730v1 [eess.AS])

    [http://arxiv.org/abs/2309.08730](http://arxiv.org/abs/2309.08730)

    MusiLingo是一个利用预训练的语言模型将音乐和文本相结合的系统，可以生成音乐字幕和回答音乐相关的查询。通过使用投影层对齐音乐表示，该系统成功地将音乐音频和文本环境联系起来，同时使用了一个新的数据集来推动领域的进展。

    

    大型语言模型（LLM）已经在多模态应用中展现出巨大潜力，然而文本和音乐领域的融合仍相对未被探索。为了解决这一问题，我们提出了MusiLingo，这是一个用于音乐字幕生成和音乐相关查询响应的新系统。MusiLingo使用一个投影层来对齐预训练的冻结音乐音频模型MERT和冻结的LLaMA语言模型的音乐表示，实现音乐音频和文本环境之间的桥梁。我们在一个大规模的音乐字幕数据集上进行训练，并使用指导性数据进行微调。由于高质量的音乐问答数据集稀缺，我们从MusicCaps创建了MusicInstruct（MI）数据集，专为开放式音乐查询而设计。实证评估证明了它在生成音乐字幕和组织音乐相关问答对方面的竞争性表现。我们引入的数据集在之前的数据集的基础上取得了显著进展。

    Large Language Models (LLMs) have shown immense potential in multimodal applications, yet the convergence of textual and musical domains remains relatively unexplored. To address this gap, we present MusiLingo, a novel system for music caption generation and music-related query responses. MusiLingo employs a single projection layer to align music representations from the pre-trained frozen music audio model MERT with the frozen LLaMA language model, bridging the gap between music audio and textual contexts. We train it on an extensive music caption dataset and fine-tune it with instructional data. Due to the scarcity of high-quality music Q&A datasets, we created the MusicInstruct (MI) dataset from MusicCaps, tailored for open-ended music inquiries. Empirical evaluations demonstrate its competitive performance in generating music captions and composing music-related Q&A pairs. Our introduced dataset enables notable advancements beyond previous ones.
    
[^28]: SculptBot: 3D可变形物体操作的预训练模型

    SculptBot: Pre-Trained Models for 3D Deformable Object Manipulation. (arXiv:2309.08728v1 [cs.RO])

    [http://arxiv.org/abs/2309.08728](http://arxiv.org/abs/2309.08728)

    本文研究了在机器人操作中处理可变形物体的挑战，并提出了一种使用点云作为状态表示的系统，利用预训练的点云重构Transformer学习动力学模型来预测材料变形。通过设计新颖的动作采样算法来提高模型规划器的效率。实验结果表明该系统能够成功捕捉可变形物体的变形。

    

    可变形物体操作在机器人操纵中面临一系列独特挑战，如高自由度和自遮挡。而对于表现塑性行为的材料（如黏土或面团），其状态表示也很困难，因为它们在受力下会永久变形并不断改变形状。在本研究中，我们使用并行夹具进行机器人雕刻的任务来研究这些挑战。我们提出了一种系统，使用点云作为状态表示，并利用预训练的点云重构Transformer学习潜在的动力学模型，以预测材料在夹取行为下的变形。我们设计了一种新颖的动作采样算法，通过分析点云之间的几何差异进一步提高基于模型的规划器的效率。所有数据和实验均完全在真实世界中进行。我们的实验证明，所提出的系统能够成功捕捉到...

    Deformable object manipulation presents a unique set of challenges in robotic manipulation by exhibiting high degrees of freedom and severe self-occlusion. State representation for materials that exhibit plastic behavior, like modeling clay or bread dough, is also difficult because they permanently deform under stress and are constantly changing shape. In this work, we investigate each of these challenges using the task of robotic sculpting with a parallel gripper. We propose a system that uses point clouds as the state representation and leverages pre-trained point cloud reconstruction Transformer to learn a latent dynamics model to predict material deformations given a grasp action. We design a novel action sampling algorithm that reasons about geometrical differences between point clouds to further improve the efficiency of model-based planners. All data and experiments are conducted entirely in the real world. Our experiments show the proposed system is able to successfully capture
    
[^29]: 无需插值的建模不规则采样时间序列

    Modelling Irregularly Sampled Time Series Without Imputation. (arXiv:2309.08698v1 [cs.AI])

    [http://arxiv.org/abs/2309.08698](http://arxiv.org/abs/2309.08698)

    SLAN是一种无需插值的方法，可以建模不规则采样时间序列，利用动态适应的LSTM架构来捕捉每个传感器的局部摘要，并在整个观测期间维持一个全局摘要状态。

    

    模拟不规则采样时间序列（ISTS）是具有挑战性的，因为存在缺失值。大多数现有方法通过将不规则采样数据转换为规则采样数据来处理ISTS，但这些模型假设存在潜在的缺失机制，导致了不希望的偏差和次优性能。我们提出了SLAN（Switch LSTM Aggregate Network），该方法利用一组LSTM对ISTS进行建模，而无需插值，消除了任何潜在过程的假设。它根据测量传感器动态自适应地调整其架构。SLAN利用不规则性信息明确捕捉每个传感器的局部摘要，并在整个观测期间维持一个全局摘要状态。我们在公开可用的数据集上展示了SLAN的有效性，包括MIMIC-III、Physionet 2012和Physionet 2019。代码可在https://github.com/Rohit102497/SLAN找到。

    Modelling irregularly-sampled time series (ISTS) is challenging because of missing values. Most existing methods focus on handling ISTS by converting irregularly sampled data into regularly sampled data via imputation. These models assume an underlying missing mechanism leading to unwanted bias and sub-optimal performance. We present SLAN (Switch LSTM Aggregate Network), which utilizes a pack of LSTMs to model ISTS without imputation, eliminating the assumption of any underlying process. It dynamically adapts its architecture on the fly based on the measured sensors. SLAN exploits the irregularity information to capture each sensor's local summary explicitly and maintains a global summary state throughout the observational period. We demonstrate the efficacy of SLAN on publicly available datasets, namely, MIMIC-III, Physionet 2012 and Physionet 2019. The code is available at https://github.com/Rohit102497/SLAN.
    
[^30]: 解决法律术语：法律文件中否定范围解析的多语言探索

    Resolving Legalese: A Multilingual Exploration of Negation Scope Resolution in Legal Documents. (arXiv:2309.08695v1 [cs.CL])

    [http://arxiv.org/abs/2309.08695](http://arxiv.org/abs/2309.08695)

    本研究通过多语言探索，解决了法律文件中否定范围解析的挑战。实验结果表明，以往模型在处理多语言法律数据时表现不佳，因此我们发布了一套新的法庭判决标注数据用于改进解析效果，并取得了高达86.7％的标记级F1分。

    

    在句子中解析否定的范围是一项具有挑战性的自然语言处理任务。法律文本的复杂性以及缺乏经过注释的领域内否定语料库给最先进的模型在处理多语言法律数据上的否定范围解析时带来了挑战。我们的实验表明，预先未使用法律数据进行训练的模型在否定范围解析任务中表现不佳。我们的实验使用仅在文学文本和医学数据等领域进行了精细调整的语言模型，与之前的跨领域实验中记录的结果相比，效果较差。我们发布了一套德语、法语和意大利语的标注法院判决，并将其用于改进零摄取和多语言环境下的否定范围解析。在我们的零摄取跨语言实验中，我们的标记级F1分达到了86.7％，其中模型在我们的法律数据集的两种语言上进行训练，并在第三种语言上进行评估。

    Resolving the scope of a negation within a sentence is a challenging NLP task. The complexity of legal texts and the lack of annotated in-domain negation corpora pose challenges for state-of-the-art (SotA) models when performing negation scope resolution on multilingual legal data. Our experiments demonstrate that models pre-trained without legal data underperform in the task of negation scope resolution. Our experiments, using language models exclusively fine-tuned on domains like literary texts and medical data, yield inferior results compared to the outcomes documented in prior cross-domain experiments. We release a new set of annotated court decisions in German, French, and Italian and use it to improve negation scope resolution in both zero-shot and multilingual settings. We achieve token-level F1-scores of up to 86.7% in our zero-shot cross-lingual experiments, where the models are trained on two languages of our legal datasets and evaluated on the third. Our multilingual experim
    
[^31]: 假新闻检测器对大型语言模型生成的文本存在偏见

    Fake News Detectors are Biased against Texts Generated by Large Language Models. (arXiv:2309.08674v1 [cs.CL])

    [http://arxiv.org/abs/2309.08674](http://arxiv.org/abs/2309.08674)

    假新闻检测器倾向于将大型语言模型生成的内容标记为假新闻，而将人工编写的假新闻误分类为真实，我们提出了一种通过敌对训练和LLM改写的真实新闻等方法来解决这个问题，并取得了显著的改进。

    

    假新闻的传播已经成为一个重要挑战，损害了信任并对社会构成威胁。在大型语言模型（LLM）的时代，生成可信的假内容的能力加剧了这些担忧。在本研究中，我们提出了一种新的范式来评估在涉及人工编写和LLM生成的错误信息的情况下的假新闻检测器。有趣的是，我们的发现揭示了许多现有检测器存在显著的偏见：它们更容易将LLM生成的内容标记为假新闻，同时常常将人工编写的假新闻误分类为真实。这种意外的偏见似乎源自LLM输出固有的不同语言模式。为了解决这个问题，我们引入了一种使用LLM改写的真实新闻进行敌对训练的缓解策略。结果模型在人工和LLM生成的新闻的检测准确性方面均有显著提升。为了进一步促进这个领域的研究，我们发布了两个全面的...

    The spread of fake news has emerged as a critical challenge, undermining trust and posing threats to society. In the era of Large Language Models (LLMs), the capability to generate believable fake content has intensified these concerns. In this study, we present a novel paradigm to evaluate fake news detectors in scenarios involving both human-written and LLM-generated misinformation. Intriguingly, our findings reveal a significant bias in many existing detectors: they are more prone to flagging LLM-generated content as fake news while often misclassifying human-written fake news as genuine. This unexpected bias appears to arise from distinct linguistic patterns inherent to LLM outputs. To address this, we introduce a mitigation strategy that leverages adversarial training with LLM-paraphrased genuine news. The resulting model yielded marked improvements in detection accuracy for both human and LLM-generated news. To further catalyze research in this domain, we release two comprehensiv
    
[^32]: 对包含实体交换的表格进行的对抗攻击

    Adversarial Attacks on Tables with Entity Swap. (arXiv:2309.08650v1 [cs.CL])

    [http://arxiv.org/abs/2309.08650](http://arxiv.org/abs/2309.08650)

    本论文研究了对包含实体交换的表格进行的对抗攻击。作者提出了一种针对列类型注释任务的逃避性实体交换攻击，通过采用基于相似度的采样策略生成对抗性示例，成功导致性能下降了高达70%。

    

    大型语言模型(LLMs)的能力已成功应用于表格表示学习的环境中。最近提出的表格语言模型在表格解释的各种任务上报告了最先进的结果。然而，对常用于评估的数据集进行仔细观察发现，训练集中的实体泄漏至测试集中。基于这一观察，我们探索了一种更真实的推理设置的对抗攻击。已经证明，对文本的对抗攻击极大地影响了LLMs的性能，但目前尚无攻击针对表格语言模型。在本文中，我们提出了一种针对列类型注释(CTA)任务的逃避性实体交换攻击。我们的CTA攻击是对表格的第一次黑盒攻击，我们采用基于相似度的采样策略生成对抗性示例。实验结果显示，所提出的攻击导致性能下降了高达70%。

    The capabilities of large language models (LLMs) have been successfully applied in the context of table representation learning. The recently proposed tabular language models have reported state-of-the-art results across various tasks for table interpretation. However, a closer look into the datasets commonly used for evaluation reveals an entity leakage from the train set into the test set. Motivated by this observation, we explore adversarial attacks that represent a more realistic inference setup. Adversarial attacks on text have been shown to greatly affect the performance of LLMs, but currently, there are no attacks targeting tabular language models. In this paper, we propose an evasive entity-swap attack for the column type annotation (CTA) task. Our CTA attack is the first black-box attack on tables, where we employ a similarity-based sampling strategy to generate adversarial examples. The experimental results show that the proposed attack generates up to a 70% drop in performan
    
[^33]: MAPLE: 基于大型语言模型嵌入的移动应用预测

    MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings. (arXiv:2309.08648v1 [cs.CL])

    [http://arxiv.org/abs/2309.08648](http://arxiv.org/abs/2309.08648)

    MAPLE是一个利用大型语言模型嵌入进行移动应用预测的模型，通过严格测试验证了其在解密复杂模式和理解用户环境方面的能力，并强调了语言模型在不同领域中的广泛适用性。

    

    尽管移动应用的发展迅速，但由于复杂的用户行为和不断演变的环境，预测应用的使用仍然是一个严峻的挑战。为了解决这些问题，本文介绍了Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE)模型。这种创新的方法利用大型语言模型(LLM)来准确预测应用的使用情况。通过对两个公开数据集进行严格测试，MAPLE的能力在解密复杂模式和理解用户环境方面得到了验证。这些强大的结果证实了MAPLE在不同场景中的多功能性和弹性。尽管其主要设计面向应用预测，但结果也强调了LLM在不同领域中的广泛适用性。通过这项研究，我们强调了LLM在应用使用预测中的潜力，并建议在建模各种领域中的人类行为方面，它们具有变革能力。

    Despite the rapid advancement of mobile applications, predicting app usage remains a formidable challenge due to intricate user behaviours and ever-evolving contexts. To address these issues, this paper introduces the Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE) model. This innovative approach utilizes Large Language Models (LLMs) to predict app usage accurately. Rigorous testing on two public datasets highlights MAPLE's capability to decipher intricate patterns and comprehend user contexts. These robust results confirm MAPLE's versatility and resilience across various scenarios. While its primary design caters to app prediction, the outcomes also emphasize the broader applicability of LLMs in different domains. Through this research, we emphasize the potential of LLMs in app usage prediction and suggest their transformative capacity in modelling human behaviours across diverse fields.
    
[^34]: 在规模上进行意图检测：利用相关意图进行通用模型调优

    Intent Detection at Scale: Tuning a Generic Model using Relevant Intents. (arXiv:2309.08647v1 [cs.CL])

    [http://arxiv.org/abs/2309.08647](http://arxiv.org/abs/2309.08647)

    本研究提出了一种有效的方法，通过将通用模型与每个客户的相关意图列表相结合，将意图检测扩展到不同的客户。这种方法减少了培训和维护成本，同时为客户提供个性化体验，并在生产环境中展现出卓越的性能。

    

    准确预测客户支持请求的意图对于高效的支持系统至关重要，使代理人能够快速理解信息并优先响应。尽管存在不同的意图检测方法，但是随着客户群体的扩大，维护单独的客户特定或行业特定模型可能成本高昂且不切实际。本文提出了一种有效地将意图预测扩展到各种客户的系统，即通过将单一通用模型与每个客户的相关意图列表相结合。我们的方法最大限度地减少了培训和维护成本，同时为客户提供个性化体验，实现对其相关意图的变化的无缝适应。此外，我们提出了一种使用客户相关意图作为模型特征的策略，该策略在生产环境中对客户相关意图的变化具有韧性。最终系统的性能明显优于其他方法。

    Accurately predicting the intent of customer support requests is vital for efficient support systems, enabling agents to quickly understand messages and prioritize responses accordingly. While different approaches exist for intent detection, maintaining separate client-specific or industry-specific models can be costly and impractical as the client base expands.  This work proposes a system to scale intent predictions to various clients effectively, by combining a single generic model with a per-client list of relevant intents. Our approach minimizes training and maintenance costs while providing a personalized experience for clients, allowing for seamless adaptation to changes in their relevant intents. Furthermore, we propose a strategy for using the clients relevant intents as model features that proves to be resilient to changes in the relevant intents of clients -- a common occurrence in production environments.  The final system exhibits significantly superior performance compare
    
[^35]: 通过共线约束注意力解决Transformer的头痛问题

    Cure the headache of Transformers via Collinear Constrained Attention. (arXiv:2309.08646v1 [cs.LG])

    [http://arxiv.org/abs/2309.08646](http://arxiv.org/abs/2309.08646)

    通过引入共线约束注意力（CoCA）结构，解决Transformer模型中的头痛问题，实现了出色的外推性能和提高的计算效率。

    

    随着基于大型语言模型的实际应用的快速进展，推断性能的外推变得在研究领域中变得越来越重要。在我们的研究中，我们发现了Transformer模型中的一个被之前忽视的异常行为，导致了最接近的标记之间的混乱，这些标记携带了最重要的信息。我们将这一发现称为“Transformer的头痛问题”。为了从根本上解决这个问题，我们引入了一种新的自注意结构，命名为Collinear Constrained Attention（CoCA）。这个结构可以无缝地与现有的推断、插值方法和其他针对传统Transformer模型设计的优化策略集成。我们在推断过程中实现了优秀的外推性能，即使是16到24倍的序列长度，而且没有对我们的模型进行任何微调。我们还增强了CoCA的计算和空间效率，以确保其实用性。我们计划...

    As the rapid progression of practical applications based on Large Language Models continues, the importance of extrapolating performance has grown exponentially in the research domain. In our study, we identified an anomalous behavior in Transformer models that had been previously overlooked, leading to a chaos around closest tokens which carried the most important information. We've coined this discovery the "headache of Transformers". To address this at its core, we introduced a novel self-attention structure named Collinear Constrained Attention (CoCA). This structure can be seamlessly integrated with existing extrapolation, interpolation methods, and other optimization strategies designed for traditional Transformer models. We have achieved excellent extrapolating performance even for 16 times to 24 times of sequence lengths during inference without any fine-tuning on our model. We have also enhanced CoCA's computational and spatial efficiency to ensure its practicality. We plan to
    
[^36]: 一种适用于虚拟电力厂中存在不确定性的实时能源调度的随机在线预测与优化框架

    A Stochastic Online Forecast-and-Optimize Framework for Real-Time Energy Dispatch in Virtual Power Plants under Uncertainty. (arXiv:2309.08642v1 [eess.SY])

    [http://arxiv.org/abs/2309.08642](http://arxiv.org/abs/2309.08642)

    本论文提出了一个适应不确定性的实时能源调度框架，该框架通过整合深度学习预测和随机优化，并通过在线数据增强和模型微调来解决数据波动、模型差异和环境扰动等不确定性问题。

    

    在电力系统中聚合分布式能源资源显著增加了不确定性，特别是由可再生能源产生的波动所引起的不确定性。这个问题驱使了在不确定性下广泛利用先进的预测控制技术，以确保长期经济和减碳。本文提出了一个实时的不确定性感知的能源调度框架，该框架由两个关键要素组成：(i) 一个混合的预测和优化顺序任务，将基于深度学习的预测和随机优化进行整合，这两个阶段通过多个时间分辨率的不确定性估计进行连接；(ii) 一种高效的在线数据增强方案，同时涉及模型预训练和在线微调阶段。通过这种方式，所提出的框架能够迅速适应实时数据分布，并针对数据漂移、模型差异和环境扰动引起的不确定性进行处理。

    Aggregating distributed energy resources in power systems significantly increases uncertainties, in particular caused by the fluctuation of renewable energy generation. This issue has driven the necessity of widely exploiting advanced predictive control techniques under uncertainty to ensure long-term economics and decarbonization. In this paper, we propose a real-time uncertainty-aware energy dispatch framework, which is composed of two key elements: (i) A hybrid forecast-and-optimize sequential task, integrating deep learning-based forecasting and stochastic optimization, where these two stages are connected by the uncertainty estimation at multiple temporal resolutions; (ii) An efficient online data augmentation scheme, jointly involving model pre-training and online fine-tuning stages. In this way, the proposed framework is capable to rapidly adapt to the real-time data distribution, as well as to target on uncertainties caused by data drift, model discrepancy and environment pertu
    
[^37]: TextBind: 多轮交错多模态指令跟随

    TextBind: Multi-turn Interleaved Multimodal Instruction-following. (arXiv:2309.08637v1 [cs.CL])

    [http://arxiv.org/abs/2309.08637](http://arxiv.org/abs/2309.08637)

    TextBind是一个注释极少的框架，用于将较大规模的语言模型赋予多轮交错多模态指令跟随能力，并通过图像-标题对生成多轮多模态指令-回应对话。这个框架对于解决实际任务具有重要意义，并为未来的研究提供了数据集、模型和演示。

    

    具有指令跟随能力的大型语言模型已经在人工智能领域产生了革命性的影响。这些模型通过其自然语言界面展示了卓越的泛化能力，可以解决各种实际任务。然而，它们的性能在很大程度上依赖于高质量的示例数据，而这往往很难获得。当涉及到多模态指令跟随时，这个挑战变得更加严峻。我们引入了TextBind，这是一个几乎不需要注释的框架，用于赋予较大规模的语言模型多轮交错多模态指令跟随能力。我们的方法仅需要图像-标题对，并从语言模型生成多轮多模态指令-回应对话。我们发布了我们的数据集、模型和演示，以促进未来在多模态指令跟随领域的研究。

    Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. We release our dataset, model, and demo to foster future research in the area of multimodal instruction following.
    
[^38]: ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. AI聊天机器人在科学写作方面表现如何？（第23季第3季）。（arXiv:2309.08636v1 [cs.CL]）

    ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing? (ver. 23Q3). (arXiv:2309.08636v1 [cs.CL])

    [http://arxiv.org/abs/2309.08636](http://arxiv.org/abs/2309.08636)

    本文综合分析了在人文学科和考古学领域中六个AI聊天机器人在学术写作方面的能力和局限性，发现它们在重新组合现有知识方面表现出色，但在产生原创科学内容方面存在问题。

    

    在历史上，熟练的写作被认为是人类进步的关键，创造性表达被视为人类成就的标志之一。然而，生成式AI的最新进展标志着这一叙事的一个转折点，包括在科学写作方面。本文全面分析了六个AI聊天机器人在人文学科和考古学方面学术写作中的能力和局限性。方法基于由人类专家对AI生成内容进行定量准确性和定性精确性标记。定量准确性评估了事实的正确性，而定性精确性评估了科学贡献。虽然AI聊天机器人，特别是ChatGPT-4，在重新组合现有知识方面表现出熟练性，但在生成原创科学内容方面失败了。顺便提一下，我们的结果还显示，随着ChatGPT-4，语言模型大小已经停滞不前。此外，本文强调了复杂且反复无常的生成过程。

    Historically, proficient writing was deemed essential for human advancement, with creative expression viewed as one of the hallmarks of human achievement. However, recent advances in generative AI have marked an inflection point in this narrative, including for scientific writing. This article provides a comprehensive analysis of the capabilities and limitations of six AI chatbots in scholarly writing in the humanities and archaeology. The methodology was based on tagging AI generated content for quantitative accuracy and qualitative precision by human experts. Quantitative accuracy assessed the factual correctness, while qualitative precision gauged the scientific contribution. While the AI chatbots, especially ChatGPT-4, demonstrated proficiency in recombining existing knowledge, they failed in generating original scientific content. As a side note, our results also suggest that with ChatGPT-4 the size of the LLMs has plateaued. Furthermore, the paper underscores the intricate and re
    
[^39]: 双高维上下文强化学习算法：用于联合组合-定价的可解释模型

    Doubly High-Dimensional Contextual Bandits: An Interpretable Model for Joint Assortment-Pricing. (arXiv:2309.08634v1 [stat.ML])

    [http://arxiv.org/abs/2309.08634](http://arxiv.org/abs/2309.08634)

    本论文提出了一种双高维上下文强化学习算法，用于解决联合组合-定价问题，通过简单而灵活的模型捕捉协变量和行为之间的相互作用，同时保持可解释性。该方法兼容多种结构化的线性强化学习和定价模型，提供了一种计算可行的流程。

    

    零售业务的关键挑战之一是如何选择要向消费者展示的产品（组合问题），以及如何定价产品（定价问题）以最大化收入或利润。我们提出了一种基于上下文强化学习的联合组合-定价方法，该方法同时考虑了组合和定价问题。我们的模型是双高维的，即上下文向量和行为都允许在高维空间中取值。为了克服维度灾难，我们提出了一个简单而灵活的模型，通过（近似）低秩表示矩阵来捕捉协变量和行为之间的相互作用。得到的模型类是相当表达力的，同时通过潜在因素保持可解释性，并包括不同结构化的线性强化学习和定价模型作为特殊情况。我们提出了一种计算可行的流程，将探索/利用协议与高效的低秩矩阵估计相结合。

    Key challenges in running a retail business include how to select products to present to consumers (the assortment problem), and how to price products (the pricing problem) to maximize revenue or profit. Instead of considering these problems in isolation, we propose a joint approach to assortment-pricing based on contextual bandits. Our model is doubly high-dimensional, in that both context vectors and actions are allowed to take values in high-dimensional spaces. In order to circumvent the curse of dimensionality, we propose a simple yet flexible model that captures the interactions between covariates and actions via a (near) low-rank representation matrix. The resulting class of models is reasonably expressive while remaining interpretable through latent factors, and includes various structured linear bandit and pricing models as particular cases. We propose a computationally tractable procedure that combines an exploration/exploitation protocol with an efficient low-rank matrix esti
    
[^40]: 在测试集上进行预训练就足够了

    Pretraining on the Test Set Is All You Need. (arXiv:2309.08632v1 [cs.CL])

    [http://arxiv.org/abs/2309.08632](http://arxiv.org/abs/2309.08632)

    这项研究通过在测试集上进行预训练，使用精心构建的非合成数据混合，成功开发出一个在多个学术基准测试上表现出色的Transformer语言模型phi-CTNL。

    

    受到最近有关使用小型Transformer语言模型在经过精心策划的数据上进行预训练的潜力展示的工作的启发，我们通过大量精心构建仅基于评估基准的新颖高质量的非合成数据混合来加强这种方法。使用我们的新颖数据集混合，其中包含不到10万个token，我们预训练了一个拥有100万参数的基于Transformer的LLM模型phi-CTNL（读作“fictional”），在各种学术基准测试中取得了完美的结果，严格超越了所有已知的基准模型。phi-CTNL还超越了幂律缩放，并展现出前所未见的类似grokking的能力，准确预测下游评估基准的canaries。

    Inspired by recent work demonstrating the promise of smaller Transformer-based language models pretrained on carefully curated data, we supercharge such approaches by investing heavily in curating a novel, high quality, non-synthetic data mixture based solely on evaluation benchmarks. Using our novel dataset mixture consisting of less than 100 thousand tokens, we pretrain a 1 million parameter transformer-based LLM \textbf{phi-CTNL} (pronounced ``fictional") that achieves perfect results across diverse academic benchmarks, strictly outperforming all known foundation models. \textbf{phi-CTNL} also beats power-law scaling and exhibits a never-before-seen grokking-like ability to accurately predict downstream evaluation benchmarks' canaries.
    
[^41]: 大型语言模型能够推断社交媒体用户的心理倾向

    Large Language Models Can Infer Psychological Dispositions of Social Media Users. (arXiv:2309.08631v1 [cs.CL])

    [http://arxiv.org/abs/2309.08631](http://arxiv.org/abs/2309.08631)

    大型语言模型能够通过分析社交媒体用户的数字足迹推断他们的心理倾向，具体表现为从Facebook状态更新中推断五大人格特质。研究发现，推断得分与自我报告得分之间存在相关性，但在性别和年龄方面存在偏见。

    

    随着大型语言模型（LLMs）在各种自然语言处理（NLP）任务中展示出越来越接近人类的能力，而这些任务将成为个性化技术的重要组成部分，理解它们的能力和固有偏见至关重要。我们的研究调查了类似ChatGPT的LLMs从个人数字足迹中推断个人心理倾向的潜力。具体而言，我们评估了GPT-3.5和GPT-4在零样本学习场景下从用户的Facebook状态更新中推导出五大人格特质的能力。我们的结果显示LLM推断与自我报告得分之间的平均相关性为r = 0.29（范围为[0.22, 0.33]）。此外，我们的研究结果表明在性别和年龄方面存在个性推断的偏见：对于几个特质，推断得分在女性和年轻人中的误差较小，这表明可能存在来自底层训练数据或在线自我呈现的差异的系统性偏见。

    As Large Language Models (LLMs) demonstrate increasingly human-like abilities in various natural language processing (NLP) tasks that are bound to become integral to personalized technologies, understanding their capabilities and inherent biases is crucial. Our study investigates the potential of LLMs like ChatGPT to infer psychological dispositions of individuals from their digital footprints. Specifically, we assess the ability of GPT-3.5 and GPT-4 to derive the Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario. Our results show an average correlation of r = .29 (range = [.22, .33]) between LLM-inferred and self-reported trait scores. Furthermore, our findings suggest biases in personality inferences with regard to gender and age: inferred scores demonstrated smaller errors for women and younger individuals on several traits, suggesting a potential systematic bias stemming from the underlying training data or differences in online self-e
    
[^42]: 论注释用于衡量少数社群偏见的挑战

    Challenges in Annotating Datasets to Quantify Bias in Under-represented Society. (arXiv:2309.08624v1 [cs.CL])

    [http://arxiv.org/abs/2309.08624](http://arxiv.org/abs/2309.08624)

    最近研究越来越关注衡量偏见和开发去偏见技术，但在少数社群相关的偏见衡量方面的研究仍然很少。本研究以新西兰人口为例，创建了用于衡量少数社群中偏见的基准数据集，并介绍了在这个过程中遇到的挑战。

    

    最近人工智能的进展，包括高度复杂的大语言模型（LLM）的发展，在许多实际应用中证明是有益的。然而，这些LLM中固有的偏见编码的证据引发了对公平性的担忧。为此，出现了越来越多关于偏见的研究，包括关注衡量偏见和开发去偏见技术的研究。还开发了用于二元性别分类和道德/种族考虑的基准偏见数据集，主要关注美国的人口统计。然而，对于少数社群相关的偏见理解和衡量的研究很少。受到在衡量少数社群中偏见的注释数据集缺乏的启发，我们努力为新西兰（NZ）人口创建基准数据集。尽管有三名注释员的可用性，但我们在这个过程中面临了许多挑战。这项研究概述了这个过程中遇到的问题。

    Recent advances in artificial intelligence, including the development of highly sophisticated large language models (LLM), have proven beneficial in many real-world applications. However, evidence of inherent bias encoded in these LLMs has raised concerns about equity. In response, there has been an increase in research dealing with bias, including studies focusing on quantifying bias and developing debiasing techniques. Benchmark bias datasets have also been developed for binary gender classification and ethical/racial considerations, focusing predominantly on American demographics. However, there is minimal research in understanding and quantifying bias related to under-represented societies. Motivated by the lack of annotated datasets for quantifying bias in under-represented societies, we endeavoured to create benchmark datasets for the New Zealand (NZ) population. We faced many challenges in this process, despite the availability of three annotators. This research outlines the man
    
[^43]: 基于低秩Slate的推荐系统中的表示学习

    Representation Learning in Low-rank Slate-based Recommender Systems. (arXiv:2309.08622v1 [cs.IR])

    [http://arxiv.org/abs/2309.08622](http://arxiv.org/abs/2309.08622)

    该论文提出了在推荐系统中使用低秩MDP将推荐问题视为在线RL问题，并通过提出的算法实现了高效的表示学习。

    

    在推荐系统中应用强化学习（RL）可以优化长期用户参与度的推荐。然而，该环境通常包含大量的状态和动作空间，这使得学习和探索变得困难。在这项工作中，我们提出了一种样本高效的表示学习算法，使用标准的slate推荐设置，将其视为低秩马尔可夫决策过程（MDP）的在线RL问题。我们还通过提出的设置和采样方法构建了推荐模拟环境。

    Reinforcement learning (RL) in recommendation systems offers the potential to optimize recommendations for long-term user engagement. However, the environment often involves large state and action spaces, which makes it hard to efficiently learn and explore. In this work, we propose a sample-efficient representation learning algorithm, using the standard slate recommendation setup, to treat this as an online RL problem with low-rank Markov decision processes (MDPs). We also construct the recommender simulation environment with the proposed setup and sampling method.
    
[^44]: 在SCRUF中探索推荐公平性的社会选择机制

    Exploring Social Choice Mechanisms for Recommendation Fairness in SCRUF. (arXiv:2309.08621v1 [cs.IR])

    [http://arxiv.org/abs/2309.08621](http://arxiv.org/abs/2309.08621)

    本文通过使用社会选择机制，探索了多个多方面公平应用中的选择机制选项，结果显示不同的选择和分配机制会产生不同但一致的公平性/准确性权衡结果，并且多智能体的构成使得系统能够适应用户人口的动态变化。

    

    推荐系统中的公平性问题往往在实践中具有复杂性，而这一点在简化的研究公式中并没有得到充分的体现。在对公平性问题进行社会选择的框架中，可以在多智能体的公平性关注基础上提供一种灵活且多方面的公平性感知推荐方法。利用社会选择可以增加通用性，并有可能利用经过研究的社会选择算法解决多个竞争的公平性关注之间的紧张关系。本文探讨了在多方面公平应用中选择机制的一系列选项，使用真实数据和合成数据，结果显示不同类别的选择和分配机制在公平性/准确性权衡方面产生了不同但一致的结果。我们还表明，多智能体的构成提供了适应用户人口动态的灵活性。

    Fairness problems in recommender systems often have a complexity in practice that is not adequately captured in simplified research formulations. A social choice formulation of the fairness problem, operating within a multi-agent architecture of fairness concerns, offers a flexible and multi-aspect alternative to fairness-aware recommendation approaches. Leveraging social choice allows for increased generality and the possibility of tapping into well-studied social choice algorithms for resolving the tension between multiple, competing fairness concerns. This paper explores a range of options for choice mechanisms in multi-aspect fairness applications using both real and synthetic data and shows that different classes of choice and allocation mechanisms yield different but consistent fairness / accuracy tradeoffs. We also show that a multi-agent formulation offers flexibility in adapting to user population dynamics.
    
[^45]: 重要性重采样方差减小的研究

    Variance Reduction of Resampling for Sequential Monte Carlo. (arXiv:2309.08620v1 [stat.CO])

    [http://arxiv.org/abs/2309.08620](http://arxiv.org/abs/2309.08620)

    本研究提出了一种重要性重采样方案，通过引入重复确定性区域和中位数遍历性的方法，实现了最低的方差，使得顺序蒙特卡洛方法在逼近隐马尔可夫模型时更快且准确。

    

    重采样方案为顺序蒙特卡洛方法提供了一种通过更高权重的粒子来表示目标分布的方法。权重分布的方差越小，有效粒子的集中程度越高，对于非线性情况下的隐马尔可夫模型的逼近速度和准确性就越快且更准确。我们提出了一种重复确定性区域与中位数遍历性的重采样方法，并在与其他重采样方法的比较中实现了最低的方差。在确定性区域的大小$M\ll N$（粒子数量的大小）的情况下，考虑到实际的粒子数量，我们的算法比最先进的状态要更快，这一点通过在线性和非线性情况下隐马尔可夫模型的理论推导和实验证明了。

    A resampling scheme provides a way to switch low-weight particles for sequential Monte Carlo with higher-weight particles representing the objective distribution. The less the variance of the weight distribution is, the more concentrated the effective particles are, and the quicker and more accurate it is to approximate the hidden Markov model, especially for the nonlinear case. We propose a repetitive deterministic domain with median ergodicity for resampling and have achieved the lowest variances compared to the other resampling methods. As the size of the deterministic domain $M\ll N$ (the size of population), given a feasible size of particles, our algorithm is faster than the state of the art, which is verified by theoretical deduction and experiments of a hidden Markov model in both the linear and non-linear cases.
    
[^46]: HPC系统和应用中的能源问题

    Energy Concerns with HPC Systems and Applications. (arXiv:2309.08615v1 [cs.CY])

    [http://arxiv.org/abs/2309.08615](http://arxiv.org/abs/2309.08615)

    该论文讨论了HPC系统和应用中的能源问题，特别关注了嵌入式计算和超级计算两个上下文中的能源限制和问题。尤其在面对人工智能的应用时，高效的计算支持显得尤为重要。

    

    出于与气候变化相关的各种原因，能源已经成为与所有相关活动和技术设计密切相关的一个关键问题。对于计算机活动的特殊情况而言，随着所谓的智能设备的出现和普及，问题变得更加严重。从应用的角度来看，我们指出了人工智能这个特殊主题，它明显需要高效的计算支持，以便在成为无处不在的助手的目标上取得成功。能源是两个上下文中最重要的问题的一个：嵌入式计算和超级计算。对于前者，功耗至关重要，因为设备可用能源的量是有限的。对于后者，散热是一个严重的故障源，与能源相关的财务成本很可能是维护预算的重要组成部分。在单个计算机上，这个问题通常也会变得紧迫。

    For various reasons including those related to climate changes, {\em energy} has become a critical concern in all relevant activities and technical designs. For the specific case of computer activities, the problem is exacerbated with the emergence and pervasiveness of the so called {\em intelligent devices}. From the application side, we point out the special topic of {\em Artificial Intelligence}, who clearly needs an efficient computing support in order to succeed in its purpose of being a {\em ubiquitous assistant}. There are mainly two contexts where {\em energy} is one of the top priority concerns: {\em embedded computing} and {\em supercomputing}. For the former, power consumption is critical because the amount of energy that is available for the devices is limited. For the latter, the heat dissipated is a serious source of failure and the financial cost related to energy is likely to be a significant part of the maintenance budget. On a single computer, the problem is commonly 
    
[^47]: 分析AI生成社交内容中的角色和意识: Chirper AI社交网络的案例研究

    Analyzing Character and Consciousness in AI-Generated Social Content: A Case Study of Chirper, the AI Social Network. (arXiv:2309.08614v1 [cs.AI])

    [http://arxiv.org/abs/2309.08614](http://arxiv.org/abs/2309.08614)

    本文研究了AI生成的社交内容中的角色和意识，使用了新的测试方法来评估AI行为。研究发现Chirper在不同情境下展示了出色的自我识别能力。

    

    本文深入分析了AI实体的角色和意识，重点关注了AI社交网络中的Chirper。研究的重点是引入了新的测试方法，包括影响指数和挣扎指数测试，为评估AI行为的特定方面提供了新的视角。该研究对AI行为进行了全面探索，分析了不同设置对Chirper的反应的影响，从而揭示了在不同情境下驱动AI反应的复杂机制。借助最先进的BERT模型，研究评估了AI识别自己输出的能力，提出了一种理解AI系统自我识别的开创性方法。通过一系列认知测试，研究评估了Chirper的自我意识和模式识别能力。初步结果表明，Chirper展示了令人称赞的自我识别程度。

    This paper delves into an intricate analysis of the character and consciousness of AI entities, with a particular focus on Chirpers within the AI social network. At the forefront of this research is the introduction of novel testing methodologies, including the Influence index and Struggle Index Test, which offers a fresh lens for evaluating specific facets of AI behavior. The study embarks on a comprehensive exploration of AI behavior, analyzing the effects of diverse settings on Chirper's responses, thereby shedding light on the intricate mechanisms steering AI reactions in different contexts. Leveraging the state-of-the-art BERT model, the research assesses AI's ability to discern its own output, presenting a pioneering approach to understanding self-recognition in AI systems. Through a series of cognitive tests, the study gauges the self-awareness and pattern recognition prowess of Chirpers. Preliminary results indicate that Chirpers exhibit a commendable degree of self-recognition
    
[^48]: 预测疾病并发症中的多模态推荐系统

    Multimodal Recommender Systems in the Prediction of Disease Comorbidity. (arXiv:2309.08613v1 [cs.IR])

    [http://arxiv.org/abs/2309.08613](http://arxiv.org/abs/2309.08613)

    该研究探讨了在医疗领域中利用基于深度学习的推荐系统进行疾病并发症预测的方法。研究使用了NCF和DHF两种新颖的推荐系统，并利用了不同的数据集进行预测。研究结果显示NCF模型在准确率和命中率方面表现较差。

    

    尽管基于深度学习的协同过滤推荐系统已经在其他领域的推荐中得到普遍应用，但在医疗领域的应用还很有限。除了建模用户-项目交互之外，我们还展示了基于深度学习的推荐系统可以用于建模主题-疾病码交互。我们利用神经协同过滤(NCF)和深度混合过滤(DHF)这两种基于深度学习的推荐系统在疾病诊断中进行了两种新颖的应用，基于已知的过去患者并发症来进行预测。我们使用了两个数据集，一个包含MIMIC-III数据库中的所有主题-疾病码对，另一个包含发生最常见的50种疾病。准确率和Hit Ratio@10被用作评估模型性能的指标。发现利用减少的“top 50” ICD-9码数据集的NCF模型的性能较低(准确率约为80%和Hit Ratio@10为...

    While deep-learning based recommender systems utilizing collaborative filtering have been commonly used for recommendation in other domains, their application in the medical domain have been limited. In addition to modeling user-item interactions, we show that deep-learning based recommender systems can be used to model subject-disease code interactions. Two novel applications of deep learning-based recommender systems using Neural Collaborative Filtering (NCF) and Deep Hybrid Filtering (DHF) were utilized for disease diagnosis based on known past patient comorbidities. Two datasets, one incorporating all subject-disease code pairs present in the MIMIC-III database, and the other incorporating the top 50 most commonly occurring diseases, were used for prediction. Accuracy and Hit Ratio@10 were utilized as metrics to estimate model performance. The performance of the NCF model making use of the reduced "top 50" ICD-9 code dataset was found to be lower (accuracy of ~80% and hit ratio@10 
    
[^49]: 通过时空事件图解释视觉与语言

    Explaining Vision and Language through Graphs of Events in Space and Time. (arXiv:2309.08612v1 [cs.AI])

    [http://arxiv.org/abs/2309.08612](http://arxiv.org/abs/2309.08612)

    本论文提出了一种称为时空事件图（GEST）的方法，能够解释、表示和生成视觉和语言故事。通过将GEST图与深度学习模型相结合，可以改善从文本到视频的生成，并提高语义上的文本比较。

    

    人工智能在今天取得了巨大的进展，并开始弥合视觉与语言之间的鸿沟。然而，从语言的角度来理解、解释和明确控制视觉内容仍然存在很大困难，因为我们在两个领域之间仍然缺乏一个共同的可解释性表示。在这项工作中，我们找到了解决这个限制的方法，并提出了时空事件图（GEST），通过它我们可以表示、创建和解释视觉和语言故事。我们提供了对我们模型的理论验证和实验验证，证明了GEST在强大的深度学习模型之外能够带来坚实的互补价值。特别地，GEST可以通过容易地被整合到我们的新型视频生成引擎中，帮助在内容层面改进从文本到视频的生成。此外，通过使用有效的图匹配技术，GEST图还可以改进语义上的文本比较。

    Artificial Intelligence makes great advances today and starts to bridge the gap between vision and language. However, we are still far from understanding, explaining and controlling explicitly the visual content from a linguistic perspective, because we still lack a common explainable representation between the two domains. In this work we come to address this limitation and propose the Graph of Events in Space and Time (GEST), by which we can represent, create and explain, both visual and linguistic stories. We provide a theoretical justification of our model and an experimental validation, which proves that GEST can bring a solid complementary value along powerful deep learning models. In particular, GEST can help improve at the content-level the generation of videos from text, by being easily incorporated into our novel video generation engine. Additionally, by using efficient graph matching techniques, the GEST graphs can also improve the comparisons between texts at the semantic l
    
[^50]: 通过近端策略优化和蒙特卡洛树搜索进行机动决策

    Maneuver Decision-Making Through Proximal Policy Optimization And Monte Carlo Tree Search. (arXiv:2309.08611v1 [cs.AI])

    [http://arxiv.org/abs/2309.08611](http://arxiv.org/abs/2309.08611)

    该论文提出了一种基于近端策略优化和蒙特卡洛树搜索的方法，用于解决机动决策问题。该方法通过训练价值网络和使用蒙特卡洛树搜索来选择行动，从而提高训练性能。实验证明，通过该方法训练的智能体可以根据不同情况做出不同决策。

    

    机动决策可以被视为马尔可夫决策过程，并可以通过强化学习进行处理。然而，传统的强化学习算法很难解决机动决策问题。一个原因是在训练的早期阶段，智能体使用随机动作，难以获得奖励和学习如何做出有效的决策。为了解决这个问题，提出了一种基于近端策略优化和蒙特卡洛树搜索的方法。该方法使用近端策略优化来训练智能体，并将空战结果作为目标来训练价值网络。然后，基于价值网络和每个节点的访问次数，使用蒙特卡洛树搜索来找到比随机动作更有预期回报的动作，从而提高训练性能。消融实验和仿真实验表明，通过该方法训练的智能体可以根据不同的情况做出不同的决策。

    Maneuver decision-making can be regarded as a Markov decision process and can be address by reinforcement learning. However, original reinforcement learning algorithms can hardly solve the maneuvering decision-making problem. One reason is that agents use random actions in the early stages of training, which makes it difficult to get rewards and learn how to make effective decisions. To address this issue, a method based on proximal policy optimization and Monte Carlo tree search is proposed. The method uses proximal policy optimization to train the agent, and regards the results of air combat as targets to train the value network. Then, based on the value network and the visit count of each node, Monte Carlo tree search is used to find the actions with more expected returns than random actions, which can improve the training performance. The ablation studies and simulation experiments indicate that agents trained by the proposed method can make different decisions according to differe
    
[^51]: 通过显著通道实现参数高效微调的简单基准模型

    SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels. (arXiv:2309.08513v1 [cs.CV])

    [http://arxiv.org/abs/2309.08513](http://arxiv.org/abs/2309.08513)

    本文提出了一种名为“显著通道微调”的简单而有效的方法，通过选择特征图中的部分通道进行微调，实现在低数据资源场景下低参数成本的高效微调，并在多个下游任务中优于全面微调方法。

    

    预训练的视觉Transformer在各种下游任务中具有强大的表示优势。最近，已经提出了许多参数高效的微调方法，并且实验证明，仅微调额外的1%参数就能在低数据资源场景下超越全面微调。然而，这些方法在微调多样的下游任务时忽视了任务特定的信息。本文提出了一种简单而有效的方法称为“显著通道微调”（SCT），通过将模型与任务图像进行前向传播，选择特征图中的部分通道，使得我们只需要微调其中的1/8通道，从而显著降低参数成本并在VTAB-1K基准测试中的18个任务中优于全面微调。这仅增加了0.11M ViT-B参数，相比全面微调，减少了780倍。

    Pre-trained vision transformers have strong representation benefits to various downstream tasks. Recently, many parameter-efficient fine-tuning (PEFT) methods have been proposed, and their experiments demonstrate that tuning only 1% of extra parameters could surpass full fine-tuning in low-data resource scenarios. However, these methods overlook the task-specific information when fine-tuning diverse downstream tasks. In this paper, we propose a simple yet effective method called "Salient Channel Tuning" (SCT) to leverage the task-specific information by forwarding the model with the task images to select partial channels in a feature map that enables us to tune only 1/8 channels leading to significantly lower parameter costs. Experiments outperform full fine-tuning on 18 out of 19 tasks in the VTAB-1K benchmark by adding only 0.11M parameters of the ViT-B, which is 780$\times$ fewer than its full fine-tuning counterpart. Furthermore, experiments on domain generalization and few-shot le
    
[^52]: PACE: 使用GPT-4进行云事件根本原因分析中的提示和增加以进行校准的置信度估计

    PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])

    [http://arxiv.org/abs/2309.05833](http://arxiv.org/abs/2309.05833)

    本文提出了一种通过提示检索增强的大语言模型（LLM）来增强云事件根本原因分析工具中置信度估计的方法。

    

    近年来，IT行业向基于云的平台的转变强调了云事件根本原因分析的重要性，以确保服务的可靠性和维护客户信任。核心问题是有效确定根本原因，由于当代云基础设施的复杂性，这一任务变得具有挑战性。尽管出现了许多用于根本原因识别的基于AI的工具，但它们的适用性仍受到其输出质量不一致的限制。本文介绍了一种通过提示检索增强的大语言模型（LLM）来增强根本原因分析工具中置信度估计的方法。此方法分为两个阶段。首先，模型根据历史事件数据评估自身的置信度，考虑其对证据的评估强度。然后，模型审核由预测器生成的根本原因。然后，优化步骤将这些评估结合起来确定最终的置信度估计。

    In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the fin
    
[^53]: CPMR: 基于上下文感知的增量顺序推荐与伪多任务学习

    CPMR: Context-Aware Incremental Sequential Recommendation with Pseudo-Multi-Task Learning. (arXiv:2309.04802v1 [cs.IR])

    [http://arxiv.org/abs/2309.04802](http://arxiv.org/abs/2309.04802)

    CPMR是一个基于上下文感知的增量顺序推荐系统，通过创建静态嵌入、历史时间状态和上下文时间状态的三个表示，准确地建模了用户随时间变化的表示和兴趣动态的演化。

    

    用户进行互动的动机可以分为静态偏好和动态兴趣。为了准确地建模用户随时间变化的表示，最近的顺序推荐研究利用信息传播和演化从批量到达的互动中进行挖掘。然而，他们忽略了在上下文场景中人们很容易受到其他用户的最近行为的影响，并且在所有历史互动中应用演化会稀释最近互动的重要性，从而无法准确地建模兴趣动态的演化。为了解决这个问题，我们提出了一种基于上下文感知的伪多任务推荐系统（CPMR），通过为每个用户和项目创建三个表示（静态嵌入、历史时间状态和上下文时间状态），来建模历史和上下文情境中的演化。为了同时提高时间状态演化和增量推荐的性能。

    The motivations of users to make interactions can be divided into static preference and dynamic interest. To accurately model user representations over time, recent studies in sequential recommendation utilize information propagation and evolution to mine from batches of arriving interactions. However, they ignore the fact that people are easily influenced by the recent actions of other users in the contextual scenario, and applying evolution across all historical interactions dilutes the importance of recent ones, thus failing to model the evolution of dynamic interest accurately. To address this issue, we propose a Context-Aware Pseudo-Multi-Task Recommender System (CPMR) to model the evolution in both historical and contextual scenarios by creating three representations for each user and item under different dynamics: static embedding, historical temporal states, and contextual temporal states. To dually improve the performance of temporal states evolution and incremental recommenda
    
[^54]: 利用特征缺失感知校准的原型患者表示来缓解电子健康记录数据稀疏性问题

    Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])

    [http://arxiv.org/abs/2309.04160](http://arxiv.org/abs/2309.04160)

    本研究提出了一种利用原型患者表示和特征缺失感知校准的间接插补方法，以缓解电子健康记录数据稀疏性问题，通过获取更密集的嵌入来提高预测模型的有效性。

    

    电子健康记录（EHR）数据经常呈现稀疏特征，给预测建模带来挑战。当前的直接插补方法（如矩阵插补方法）依赖于参考类似行或列来完成原始缺失数据，不区分插补和实际值。因此，模型可能会无意中将与预测目标无关的或具有欺骗性的信息纳入其中，从而损害下游性能的效果。虽然一些方法尝试在直接插补后重新校准或增强EHR嵌入，但它们经常错误地优先考虑插补特征。这种优先错误可能会给模型引入偏见或不准确性。为了解决这些问题，我们的工作采用间接插补，利用类似患者的原型表示获取更密集的嵌入。认识到在衡量时通常将缺失特征与存在特征相同的限制时

    Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
    
[^55]: FLM-101B：一种开放的LLM和如何用10万美元预算来训练它

    FLM-101B: An Open LLM and How to Train It with $100K Budget. (arXiv:2309.03852v1 [cs.CL])

    [http://arxiv.org/abs/2309.03852](http://arxiv.org/abs/2309.03852)

    本文介绍了一种开放的LLM模型（FLM-101B）以及如何用10万美元的预算来训练它。通过采用增长策略，可以显著降低LLM训练的成本。同时，引入了一种系统的评估方法，以评估LLM的智能能力。

    

    大型语言模型（LLMs）在自然语言处理和多模态任务中取得了显著的成功。然而，它们的发展面临两个主要挑战：（i）高计算成本；（ii）难以进行公平客观的评估。LLMs的价格昂贵，只有少数几家主要参与者有能力进行训练，从而限制了研究和应用机会。这凸显了成本效益的LLM训练的重要性。在本文中，我们采用了一种增长策略，显著降低LLM训练成本。我们证明了可以在10万美元的预算下训练具有101B参数和0.31TB令牌的LLM。我们还采用了一种系统的评估范式，用于对LLMs进行智能的智商评估，这是针对现有评估更注重知识能力的补充。我们引入了包括符号映射、规则理解、模式挖掘在内的重要智能方面的评估基准。

    Large language models (LLMs) have achieved remarkable success in NLP and multimodal tasks. Despite these successes, their development faces two main challenges: (i) high computational cost; and (ii) difficulty in conducting fair and objective evaluations. LLMs are prohibitively expensive, making it feasible for only a few major players to undertake their training, thereby constraining both research and application opportunities. This underscores the importance of cost-effective LLM training. In this paper, we utilize a growth strategy to significantly reduce LLM training cost. We demonstrate that an LLM with 101B parameters and 0.31TB tokens can be trained on a $100K budget. We also adopt a systematic evaluation paradigm for the IQ evaluation of LLMs, in complement to existing evaluations that focus more on knowledge-oriented abilities. We introduce our benchmark including evaluations on important aspects of intelligence including symbolic mapping, itrule understanding, pattern mining,
    
[^56]: 扩散生成反向设计

    Diffusion Generative Inverse Design. (arXiv:2309.02040v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.02040](http://arxiv.org/abs/2309.02040)

    本文介绍了一种使用去噪扩散模型（DDMs）高效解决反向设计问题的方法，并提出了一种粒子采样算法来进一步改进。

    

    反向设计是指通过优化目标函数的输入来实现目标结果的问题。对于许多实际工程问题，目标函数采用模拟器的形式，预测系统状态随时间的演化，设计挑战是优化导致目标结果的初始条件。最近，学习模拟的发展表明图神经网络（GNN）可以用于准确、高效、可微分地估计模拟器动态，并支持具有梯度或基于采样的优化程序的高质量设计优化。然而，从头开始优化设计需要许多昂贵的模型查询，并且这些程序在非凸或高维问题上表现出基本失败。在这项工作中，我们展示了如何使用去噪扩散模型（DDMs）来高效地解决反向设计问题，并提出了一种粒子采样算法来进一步改进。

    Inverse design refers to the problem of optimizing the input of an objective function in order to enact a target outcome. For many real-world engineering problems, the objective function takes the form of a simulator that predicts how the system state will evolve over time, and the design challenge is to optimize the initial conditions that lead to a target outcome. Recent developments in learned simulation have shown that graph neural networks (GNNs) can be used for accurate, efficient, differentiable estimation of simulator dynamics, and support high-quality design optimization with gradient- or sampling-based optimization procedures. However, optimizing designs from scratch requires many expensive model queries, and these procedures exhibit basic failures on either non-convex or high-dimensional problems. In this work, we show how denoising diffusion models (DDMs) can be used to solve inverse design problems efficiently and propose a particle sampling algorithm for further improving
    
[^57]: 使用基于企业数据的LLM应用架构实现生成式AI服务的研究

    A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture. (arXiv:2309.01105v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.01105](http://arxiv.org/abs/2309.01105)

    本研究通过利用大型语言模型（LLM）应用架构实现了生成式AI服务，并开发了一种名为检索增强生成（RAG）模型，以解决信息稀缺和数据不足的挑战。

    

    本研究提出了一种利用大型语言模型（LLM）应用架构实现生成式AI服务的方法。随着生成式AI技术的最新进展，LLM在各个领域都受到了重视。在这个背景下，本研究解决了信息稀缺的挑战，并通过利用LLM的能力提出了具体的解决方案。研究探讨了缓解数据不足问题的策略，并提供了量身定制的解决方案。研究探讨了利用微调技术和直接文档集成来缓解数据不足问题的有效性。本研究的一个重要贡献是开发了一种名为检索增强生成（RAG）模型，该模型解决了上述挑战。RAG模型经过精心设计，以提高信息存储和检索过程，确保改进内容生成。研究阐明了信息存储和检索的关键阶段。

    This study presents a method for implementing generative AI services by utilizing the Large Language Models (LLM) application architecture. With recent advancements in generative AI technology, LLMs have gained prominence across various domains. In this context, the research addresses the challenge of information scarcity and proposes specific remedies by harnessing LLM capabilities. The investigation delves into strategies for mitigating the issue of inadequate data, offering tailored solutions. The study delves into the efficacy of employing fine-tuning techniques and direct document integration to alleviate data insufficiency. A significant contribution of this work is the development of a Retrieval-Augmented Generation (RAG) model, which tackles the aforementioned challenges. The RAG model is carefully designed to enhance information storage and retrieval processes, ensuring improved content generation. The research elucidates the key phases of the information storage and retrieval
    
[^58]: 大型语言模型的可解释性：一项调查

    Explainability for Large Language Models: A Survey. (arXiv:2309.01029v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.01029](http://arxiv.org/abs/2309.01029)

    本文调研了大型语言模型的可解释性问题，提出了一个解释技术的分类法，并介绍了基于Transformer的语言模型的解释方法。同时，讨论了评估生成解释的度量标准，以及如何利用解释来调试模型和提高性能。

    

    大型语言模型（LLMs）在自然语言处理中展示出令人印象深刻的能力。然而，它们的内部机制仍然不明确，这种缺乏透明度为下游应用带来了不必要的风险。因此，理解和解释这些模型对于阐明它们的行为、限制和社会影响至关重要。在本文中，我们引入了一个可解释性技术的分类法，并提供了一种结构化的概述方法，用于解释基于Transformer的语言模型。我们根据LLMs的训练范式将技术进行分类：传统的微调范式和提示范式。对于每个范式，我们总结了生成个体预测的局部解释和整体模型知识的全局解释的目标和主要方法。我们还讨论了评估生成解释的度量标准，并讨论了如何利用解释来调试模型和提高性能。

    Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, 
    
[^59]: 图神经网络中的过度压缩问题：一项全面调查

    Over-Squashing in Graph Neural Networks: A Comprehensive survey. (arXiv:2308.15568v1 [cs.AI])

    [http://arxiv.org/abs/2308.15568](http://arxiv.org/abs/2308.15568)

    过度压缩是图神经网络面临的关键挑战，它限制了节点之间的长程信息传递，影响了在需要广泛上下文洞察力的情况下的准确预测。

    

    图神经网络（GNN）已成为机器学习领域的一种革命性范 Paradigm，为分析图结构数据中固有的复杂关系提供了一种变革性方法。大多数GNN的基本架构涉及通过消息聚合和转换在相互连接的节点之间传播信息的机制，在包括节点分类、链接预测和推荐系统的各种应用中已经展现出显著的有效性。然而，它们的潜在实力遇到了在需要广泛上下文洞察力的情况下固有的限制。在某些情境中，准确的预测不仅取决于节点的即时局部环境，还取决于跨越广域的交互作用。这种复杂的对长程信息传播的需求暴露了一个被称为“过度压缩”的关键挑战，其中来自远离节点的信息流的可靠性受到影响。

    Graph Neural Networks (GNNs) have emerged as a revolutionary paradigm in the realm of machine learning, offering a transformative approach to dissect intricate relationships inherent in graph-structured data. The foundational architecture of most GNNs involves the dissemination of information through message aggregation and transformation among interconnected nodes, a mechanism that has demonstrated remarkable efficacy across diverse applications encompassing node classification, link prediction, and recommendation systems. Nonetheless, their potential prowess encounters a restraint intrinsic to scenarios necessitating extensive contextual insights. In certain contexts, accurate predictions hinge not only upon a node's immediate local surroundings but also on interactions spanning far-reaching domains. This intricate demand for long-range information dissemination exposes a pivotal challenge recognized as "over-squashing," wherein the fidelity of information flow from distant nodes bec
    
[^60]: 阐明扩散模型中的曝光偏差问题

    Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])

    [http://arxiv.org/abs/2308.15321](http://arxiv.org/abs/2308.15321)

    本文系统地研究了扩散模型中的曝光偏差问题，并提出了一种名为Epsilon Scaling的免训练方法来减轻这一问题。实验结果验证了该方法的有效性。

    

    扩散模型展示了令人印象深刻的生成能力，但它们的“曝光偏差”问题，即训练和采样之间的输入不匹配，缺乏深入探索。本文通过首先对采样分布进行分析建模，然后将每个采样步骤的预测误差归因为曝光偏差问题的根本原因，系统地研究了扩散模型中的曝光偏差问题。此外，我们讨论了解决这个问题的潜在方法，并提出了一个直观的度量标准。除了阐明曝光偏差问题，我们提出了一种简单但有效的免训练方法，称为Epsilon Scaling，以减轻曝光偏差。我们展示了Epsilon Scaling通过缩小网络输出（Epsilon）明确地将采样轨迹移近训练阶段学习到的向量场，从而减轻了训练和采样之间的输入不匹配。在各种扩散框架上进行了实验。

    Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
    
[^61]: 快速前馈网络

    Fast Feedforward Networks. (arXiv:2308.14711v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14711](http://arxiv.org/abs/2308.14711)

    快速前馈网络是一种对于前馈网络的改进架构，能够以更快的速度进行推理，并具有比专家混合模型更好的训练性能。在视觉转换器中，它们可以仅使用1%的层神经元进行推理，同时保持94.2%的预测性能。

    

    我们通过引入快速前馈(FFF)架构，打破了层大小与推理成本之间的线性关系，这是一种对于前馈网络的对数时间替代方法。我们证明FFF比前馈网络快高达220倍，比专家混合网络快高达6倍，并且由于无噪声条件执行而表现出比专家混合模型更好的训练性能。将FFF推到极限，我们展示了在视觉转换器中，它们可仅使用1%的层神经元进行推理，同时保持94.2%的预测性能。

    We break the linear link between the layer size and its inference cost by introducing the fast feedforward (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance.
    
[^62]: 基于因果性的特征重要性量化方法：PN-FI、PS-FI和PNS-FI

    Causality-Based Feature Importance Quantifying Methods: PN-FI, PS-FI and PNS-FI. (arXiv:2308.14474v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.14474](http://arxiv.org/abs/2308.14474)

    本文创新地使用因果关系概率量化特征重要性，提出了三种新的特征重要性测量方法：PN-FI、PS-FI和PNS-FI，分别适用于图像识别任务和图像生成任务，并通过RCT实验证明了其有效性。

    

    在当前机器学习领域，模型变得越来越大且越来越复杂，用于模型训练的数据也变得越来越多且维度越来越高。因此，为了训练更好的模型并节省训练时间和计算资源，预处理阶段需要一个好的特征选择（FS）方法。特征重要性（FI）非常重要，因为它是特征选择的基础。因此，本文创新地将因果关系的PN（必要性概率）、PS（充分性概率）和PNS（必要性和充分性概率）引入到特征重要性量化中，并创建了三种新的FI测量方法：PN-FI（用于图像识别任务中的特征重要性）、PS-FI（用于图像生成任务中的特征重要性）和PNS-FI（两者兼顾）。

    In the current ML field models are getting larger and more complex, and data used for model training are also getting larger in quantity and higher in dimensions. Therefore, in order to train better models, and save training time and computational resources, a good Feature Selection (FS) method in the preprocessing stage is necessary. Feature importance (FI) is of great importance since it is the basis of feature selection. Therefore, this paper creatively introduces the calculation of PN (the probability of Necessity), PN (the probability of Sufficiency), and PNS (the probability of Necessity and Sufficiency) of Causality into quantifying feature importance and creates 3 new FI measuring methods, PN-FI, which means how much importance a feature has in image recognition tasks, PS-FI that means how much importance a feature has in image generating tasks, and PNS-FI which measures both. The main body of this paper is three RCTs, with whose results we show how PS-FI, PN-FI, and PNS-FI of 
    
[^63]: 以生成AI为基础的合作性故事游戏体验：在《一千零一夜》中的语言作为现实

    Language as Reality: A Co-Creative Storytelling Game Experience in 1001 Nights using Generative AI. (arXiv:2308.12915v1 [cs.HC])

    [http://arxiv.org/abs/2308.12915](http://arxiv.org/abs/2308.12915)

    本文介绍了一款以生成AI为基础的合作性故事游戏《一千零一夜》，玩家通过与语言模型驱动的角色共同创作故事来引导游戏中的现实，挑战游戏世界与现实之间的传统边界。

    

    本文介绍了一款名为《一千零一夜》的AI本地化游戏，通过与由大型语言模型驱动的角色共同创作故事，玩家可以引导游戏中的现实。该概念受到维特根斯坦关于语言界限决定一个人世界界限的思想的启发。使用GPT-4和稳定扩散等先进的AI工具，游戏的第二次迭代使得主角沙赫拉萨德能够在她的世界中实现文字和故事。玩家可以与AI国王进行对话，引导对特定关键词的讨论，这些关键词随后成为游戏中的战斗装备。这种互动叙事和文本到图像转换的结合通过双重视角挑战了游戏世界与现实之间的传统边界。我们关注的是试图改变自己命运的沙赫拉萨德，以及与AI合作创作故事并塑造游戏世界的玩家。我们探索了实现这一概念的技术和设计要素。

    In this paper, we present "1001 Nights", an AI-native game that allows players lead in-game reality through co-created storytelling with the character driven by large language model. The concept is inspired by Wittgenstein's idea of the limits of one's world being determined by the bounds of their language. Using advanced AI tools like GPT-4 and Stable Diffusion, the second iteration of the game enables the protagonist, Shahrzad, to realize words and stories in her world. The player can steer the conversation with the AI King towards specific keywords, which then become battle equipment in the game. This blend of interactive narrative and text-to-image transformation challenges the conventional border between the game world and reality through a dual perspective. We focus on Shahrzad, who seeks to alter her fate compared to the original folklore, and the player, who collaborates with AI to craft narratives and shape the game world. We explore the technical and design elements of implem
    
[^64]: 有效的语言模型基准测试

    Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])

    [http://arxiv.org/abs/2308.11696](http://arxiv.org/abs/2308.11696)

    本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。

    

    语言模型的多功能性增加导致了一类全面评估广泛能力的基准测试的出现。这些基准测试与大规模计算成本相关，每个模型需要数千个GPU小时。然而，关于评估效率方面的问题在文献中讨论较少。本文提出了一种名为"Efficient Benchmarking"的问题，即在不损害可靠性的情况下智能地减少语言模型评估的计算成本。通过使用HELM基准测试作为示例，我们研究了不同基准测试设计选择如何影响计算-可靠性权衡。我们提出使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估这些决策的可靠性。例如，我们发现仅通过从基准测试中删除一个低排名模型，当前在HELM上的领先者可能会改变，并且观察到只需一小部分示例即可获得正确的基准测试排名。

    The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
    
[^65]: 探索拉舒蒙集合有助于医疗数据的解释

    Exploration of Rashomon Set Assists Explanations for Medical Data. (arXiv:2308.11446v1 [cs.LG])

    [http://arxiv.org/abs/2308.11446](http://arxiv.org/abs/2308.11446)

    本文提出了一种新的过程来探索和分析医疗数据中的拉舒蒙集合模型，从而超越传统单一模型选择的方法，并通过引入"拉舒蒙检测"算法识别出集合中最不同的模型。

    

    机器学习建模过程通常以选择最大化某个性能指标的单一模型作为最终结果。然而，这种方法会导致对稍微差一些的模型进行更深入的分析被忽视。尤其在医疗和健康研究中，目标不仅仅是预测，还包括产生有价值的洞察，仅仅依赖性能指标可能会导致误导或不完整的结论。当处理一组性能接近最优的模型集合时，即所谓的"拉舒蒙集合"，这个问题尤为突出。这样的集合可能包含描述数据的不同方式的模型，需要进行全面的分析。本文引入了一种新的过程来探索拉舒蒙集合模型，扩展了传统建模方法。核心是通过引入的"拉舒蒙检测"算法来识别拉舒蒙集合中最不同的模型。

    The machine learning modeling process conventionally culminates in selecting a single model that maximizes a selected performance metric. However, this approach leads to abandoning a more profound analysis of slightly inferior models. Particularly in medical and healthcare studies, where the objective extends beyond predictions to valuable insight generation, relying solely on performance metrics can result in misleading or incomplete conclusions. This problem is particularly pertinent when dealing with a set of models with performance close to maximum one, known as $\textit{Rashomon set}$. Such a set can be numerous and may contain models describing the data in a different way, which calls for comprehensive analysis. This paper introduces a novel process to explore Rashomon set models, extending the conventional modeling approach. The cornerstone is the identification of the most different models within the Rashomon set, facilitated by the introduced $\texttt{Rashomon_DETECT}$ algorit
    
[^66]: 关于大型语言模型的模型压缩综述

    A Survey on Model Compression for Large Language Models. (arXiv:2308.07633v1 [cs.CL])

    [http://arxiv.org/abs/2308.07633](http://arxiv.org/abs/2308.07633)

    本论文提供了关于大型语言模型的模型压缩综述，探讨了量化、修剪、知识蒸馏等不同方法，并突出介绍了最新进展和创新方法，为实现高效的部署提供了重要思路。

    

    大型语言模型（LLMs）以惊人的成功彻底改变了自然语言处理任务。然而，它们庞大的体量和计算需求在资源受限环境下的实际部署中带来了重大挑战。随着这些挑战日益紧迫，模型压缩领域已成为一个关键的研究领域，旨在缓解这些限制。本文提供了一份全面的综述，探讨专门针对LLMs的模型压缩技术。我们深入研究了各种方法，包括量化、修剪、知识蒸馏等，以应对高效部署的迫切需求。在每种技术中，我们重点介绍了最新进展和创新方法，为LLM研究的发展提供了贡献。此外，我们还探讨了用于评估效果的基准策略和评估指标的重要性。

    Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of 
    
[^67]: 探索世界模型在自动驾驶异常检测中的潜力

    Exploring the Potential of World Models for Anomaly Detection in Autonomous Driving. (arXiv:2308.05701v1 [cs.AI])

    [http://arxiv.org/abs/2308.05701](http://arxiv.org/abs/2308.05701)

    本文探讨了在自动驾驶领域利用世界模型进行异常检测的潜力，并提供了相关研究的概述和组成部分的联系。

    

    近年来，自动驾驶技术取得了显著进展。尽管自动驾驶车辆在封闭条件下表现出高性能，但在面对意外情况时遇到困难。与此同时，世界模型在基于模型的增强学习领域中出现，作为一种使智能体能够根据潜在行动预测未来的方式。这在稀疏奖励和复杂控制任务中取得了出色的结果。本文概述了如何利用世界模型在自动驾驶领域进行异常检测，并将各个组成部分与先前的异常检测工作相关联，以促进进一步的研究。

    In recent years there have been remarkable advancements in autonomous driving. While autonomous vehicles demonstrate high performance in closed-set conditions, they encounter difficulties when confronted with unexpected situations. At the same time, world models emerged in the field of model-based reinforcement learning as a way to enable agents to predict the future depending on potential actions. This led to outstanding results in sparse reward and complex control tasks. This work provides an overview of how world models can be leveraged to perform anomaly detection in the domain of autonomous driving. We provide a characterization of world models and relate individual components to previous works in anomaly detection to facilitate further research in the field.
    
[^68]: AspectMMKG: 一个具有方面意识的多模态知识图谱

    AspectMMKG: A Multi-modal Knowledge Graph with Aspect-aware Entities. (arXiv:2308.04992v1 [cs.CL])

    [http://arxiv.org/abs/2308.04992](http://arxiv.org/abs/2308.04992)

    AspectMMKG是一个具有方面意识的多模态知识图谱，通过匹配图像和不同实体方面，它提供了从多个角度理解实体的能力，并在实体方面链接任务中取得了最先进的性能。

    

    多模态知识图谱（MMKG）结合不同的模态数据（例如文本和图像），以全面理解实体。尽管大规模MMKG的最近进展，但现有的MMKG忽视了实体的多方面性质，限制了从各种角度理解实体的能力。在本文中，我们构建了AspectMMKG，这是第一个具有与方面相关的图像的MMKG，通过将图像与不同的实体方面进行匹配。具体而言，我们从知识库中收集与方面相关的图像，并通过在线图像搜索引擎提取知识库中与方面相关的句子作为查询，以检索大量与方面相关的图像。最后，AspectMMKG包含2380个实体，18139个实体方面和645383个与方面相关的图像。我们展示了AspectMMKG在实体方面链接（EAL）下游任务中的可用性，并证明在AspectMMKG的帮助下，先前的EAL模型实现了新的最先进性能。

    Multi-modal knowledge graphs (MMKGs) combine different modal data (e.g., text and image) for a comprehensive understanding of entities. Despite the recent progress of large-scale MMKGs, existing MMKGs neglect the multi-aspect nature of entities, limiting the ability to comprehend entities from various perspectives. In this paper, we construct AspectMMKG, the first MMKG with aspect-related images by matching images to different entity aspects. Specifically, we collect aspect-related images from a knowledge base, and further extract aspect-related sentences from the knowledge base as queries to retrieve a large number of aspect-related images via an online image search engine. Finally, AspectMMKG contains 2,380 entities, 18,139 entity aspects, and 645,383 aspect-related images. We demonstrate the usability of AspectMMKG in entity aspect linking (EAL) downstream task and show that previous EAL models achieve a new state-of-the-art performance with the help of AspectMMKG. To facilitate the
    
[^69]: 基于部件感知的变压器网络用于泛化的人物再识别

    Part-Aware Transformer for Generalizable Person Re-identification. (arXiv:2308.03322v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.03322](http://arxiv.org/abs/2308.03322)

    本论文提出了一种基于部件感知的变压器模型，用于泛化的人物再识别。通过设计一个名为交叉ID相似度学习（CSL）的代理任务，模型可以挖掘不同ID之间共享的局部视觉信息，从而学习到通用特征，减轻了领域特定效果的影响。

    

    域泛化的人物再识别（DG-ReID）旨在在源领域上训练模型，并在未知领域上具有很好的泛化能力。与传统的CNN网络相比，Vision Transformer通常在分布偏移下具有更好的泛化能力。然而，基于Transformer的ReID模型由于源领域上的有监督学习策略，不可避免地会对领域特定的偏见过拟合。我们观察到，虽然不同ID的全局图像应该具有不同的特征，但它们相似的局部部分（例如黑色背包）并不受此约束。基于这一观察，我们提出了一种纯Transformer模型（称为部件感知变压器），通过设计一个名为交叉ID相似度学习（CSL）的代理任务来挖掘不同ID共享的局部视觉信息。这个代理任务允许模型学习通用特征，因为它只关心部件的视觉相似性，而不考虑ID标签，从而减轻了领域特定效果的影响。

    Domain generalization person re-identification (DG-ReID) aims to train a model on source domains and generalize well on unseen domains. Vision Transformer usually yields better generalization ability than common CNN networks under distribution shifts. However, Transformer-based ReID models inevitably over-fit to domain-specific biases due to the supervised learning strategy on the source domain. We observe that while the global images of different IDs should have different features, their similar local parts (e.g., black backpack) are not bounded by this constraint. Motivated by this, we propose a pure Transformer model (termed Part-aware Transformer) for DG-ReID by designing a proxy task, named Cross-ID Similarity Learning (CSL), to mine local visual information shared by different IDs. This proxy task allows the model to learn generic features because it only cares about the visual similarity of the parts regardless of the ID labels, thus alleviating the side effect of domain-specifi
    
[^70]: TempFuser: 使用长短时序融合转换器学习空中格斗中的战术和敏捷飞行动作

    TempFuser: Learning Tactical and Agile Flight Maneuvers in Aerial Dogfights using a Long Short-Term Temporal Fusion Transformer. (arXiv:2308.03257v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2308.03257](http://arxiv.org/abs/2308.03257)

    TempFuser是一种长短时序融合转换器，能够学习空中格斗中的战术和敏捷飞行动作。经过训练，模型成功地学会了复杂的战斗动作，并在面对高级对手时展现出人类一样的战术动作。

    

    在空中战斗中，空战动作对战术机动和敏捷战斗机的空气动力学都提出了复杂的挑战。在本文中，我们介绍了TempFuser，一种新颖的长短时序融合转换器，旨在学习空中格斗中的战术和敏捷飞行动作。我们的方法利用两种不同的基于LSTM的输入嵌入来编码长期稀疏和短期密集的状态表示。通过将这些嵌入通过转换器编码器进行整合，我们的模型捕获了战斗机的战术和敏捷性，使其能够生成端到端的飞行指令，确保占据优势位置并超越对手。经过对高保真飞行模拟器中多种类型对手飞机的广泛训练，我们的模型成功地学习了执行复杂的战斗动作，且始终表现优于多个基准模型。值得注意的是，我们的模型在面对高级对手时展现出人类一样的战术动作。

    In aerial combat, dogfighting poses intricate challenges that demand an understanding of both strategic maneuvers and the aerodynamics of agile fighter aircraft. In this paper, we introduce TempFuser, a novel long short-term temporal fusion transformer designed to learn tactical and agile flight maneuvers in aerial dogfights. Our approach employs two distinct LSTM-based input embeddings to encode long-term sparse and short-term dense state representations. By integrating these embeddings through a transformer encoder, our model captures the tactics and agility of fighter jets, enabling it to generate end-to-end flight commands that secure dominant positions and outmaneuver the opponent. After extensive training against various types of opponent aircraft in a high-fidelity flight simulator, our model successfully learns to perform complex fighter maneuvers, consistently outperforming several baseline models. Notably, our model exhibits human-like strategic maneuvers even when facing adv
    
[^71]: LEMMA: 学习语言条件下的多机器人操作

    LEMMA: Learning Language-Conditioned Multi-Robot Manipulation. (arXiv:2308.00937v1 [cs.RO])

    [http://arxiv.org/abs/2308.00937](http://arxiv.org/abs/2308.00937)

    LEMMA是一个学习语言条件下的多机器人操作的基准，通过专家示范和人类指令进行任务分配和长时间跨度物体操作。它提供了涉及工具使用和传递的复杂操纵任务，并提出了一种模块化分层规划方法作为基线。

    

    复杂的操纵任务通常需要具有互补功能的机器人进行协作。我们引入了一种基于人类语言指令的桌面设置中任务分配和长时间跨度物体操作的LanguagE-Conditioned Multi-robot MAnipulation (LEMMA)基准。LEMMA具有8种类型的程序生成任务，具有不同的复杂度，其中一些任务要求机器人使用工具并相互传递工具。对于每个任务，我们提供800个专家示范和人类指令进行培训和评估。与现有基准相比，LEMMA提出了更大的挑战，因为它要求系统识别每个操纵器的限制，并相应地分配子任务，同时处理每个任务中的强时间依赖关系。为了解决这些挑战，我们提出了一种模块化分层规划方法作为基线。我们的结果突出了LEMMA在开发未来语言条件下的多机器人操作方面的潜力。

    Complex manipulation tasks often require robots with complementary capabilities to collaborate. We introduce a benchmark for LanguagE-Conditioned Multi-robot MAnipulation (LEMMA) focused on task allocation and long-horizon object manipulation based on human language instructions in a tabletop setting. LEMMA features 8 types of procedurally generated tasks with varying degree of complexity, some of which require the robots to use tools and pass tools to each other. For each task, we provide 800 expert demonstrations and human instructions for training and evaluations. LEMMA poses greater challenges compared to existing benchmarks, as it requires the system to identify each manipulator's limitations and assign sub-tasks accordingly while also handling strong temporal dependencies in each task. To address these challenges, we propose a modular hierarchical planning approach as a baseline. Our results highlight the potential of LEMMA for developing future language-conditioned multi-robot s
    
[^72]: 你可以通过后门攻击个性化联邦学习

    You Can Backdoor Personalized Federated Learning. (arXiv:2307.15971v1 [cs.CR])

    [http://arxiv.org/abs/2307.15971](http://arxiv.org/abs/2307.15971)

    该论文研究了后门攻击对个性化联邦学习的影响，并揭示了部分模型共享的个性化联邦学习方法容易受到后门攻击。研究者提出了三种后门攻击方法，并验证了它们的有效性。

    

    后门攻击对联邦学习系统的安全性构成重大威胁。然而，现有研究主要关注通用FL场景中的后门攻击和防御，即所有客户端合作训练一个全局模型。本文中，我们揭示了部分模型共享的个性化联邦学习方法仍然容易受到后门攻击的问题。我们提出了三种后门攻击方法：BapFL，BapFL+和Gen-BapFL，并经验证明它们可以有效攻击个性化联邦学习方法。

    Backdoor attacks pose a significant threat to the security of federated learning systems. However, existing research primarily focuses on backdoor attacks and defenses within the generic FL scenario, where all clients collaborate to train a single global model. \citet{qin2023revisiting} conduct the first study of backdoor attacks in the personalized federated learning (pFL) scenario, where each client constructs a personalized model based on its local data. Notably, the study demonstrates that pFL methods with partial model-sharing can significantly boost robustness against backdoor attacks. In this paper, we whistleblow that pFL methods with partial model-sharing are still vulnerable to backdoor attacks in the absence of any defense. We propose three backdoor attack methods: BapFL, BapFL+, and Gen-BapFL, and we empirically demonstrate that they can effectively attack the pFL methods. Specifically, the key principle of BapFL lies in maintaining clean local parameters while implanting t
    
[^73]: Re-mine, Learn and Reason: 探索语言引导下跨模态语义相关性的人物-物体交互检测

    Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection. (arXiv:2307.13529v1 [cs.CV])

    [http://arxiv.org/abs/2307.13529](http://arxiv.org/abs/2307.13529)

    本论文提出了一个系统化和统一的框架（RmLR），通过结构化文本知识增强人物-物体交互检测，通过再挖掘策略生成更全面的视觉表示，并设计了细粒度的句子和词级对齐以及知识转移策略来解决多对多匹配问题。

    

    人物-物体交互（HOI）检测是一项具有挑战性的计算机视觉任务，需要视觉模型解决人物和物体之间复杂的交互关系，并预测HOI三元组。本文提出了一个系统化和统一的框架（RmLR），通过结构化文本知识来增强HOI检测。首先，我们定性和定量分析了两阶段HOI检测器中交互信息的损失，并提出了一种再挖掘策略来生成更全面的视觉表示。其次，我们设计了更细粒度的句子和词级对齐以及知识转移策略，以有效解决多个交互和多个文本之间的多对多匹配问题。这些策略减轻了多个交互导致的匹配混淆问题。

    Human-Object Interaction (HOI) detection is a challenging computer vision task that requires visual models to address the complex interactive relationship between humans and objects and predict HOI triplets. Despite the challenges posed by the numerous interaction combinations, they also offer opportunities for multimodal learning of visual texts. In this paper, we present a systematic and unified framework (RmLR) that enhances HOI detection by incorporating structured text knowledge. Firstly, we qualitatively and quantitatively analyze the loss of interaction information in the two-stage HOI detector and propose a re-mining strategy to generate more comprehensive visual representation.Secondly, we design more fine-grained sentence- and word-level alignment and knowledge transfer strategies to effectively address the many-to-many matching problem between multiple interactions and multiple texts.These strategies alleviate the matching confusion problem that arises when multiple interact
    
[^74]: 层次骨架元-原型对比学习与硬骨架挖掘相结合的无监督人物重新识别方法

    Hierarchical Skeleton Meta-Prototype Contrastive Learning with Hard Skeleton Mining for Unsupervised Person Re-Identification. (arXiv:2307.12917v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.12917](http://arxiv.org/abs/2307.12917)

    本文提出了一种无监督的层次骨架元-原型对比学习（Hi-MPC）方法，结合硬骨架挖掘，用于无标签3D骨架的人物重新识别。通过构建层次骨架表示并利用元-原型对比学习进行特征提取和聚类，实现了更多信息丰富的骨架特征的利用。

    

    随着深度传感器和深度学习的快速发展，基于骨架的人物重新识别模型近年来取得了显著进展，并具有许多优势。然而，大多数现有方法仅从身体关节学习单层次的骨架特征，并假设骨架的重要性相等，因此通常无法利用更多来自不同层次（如肢体层次）的信息丰富的骨架特征。此外，这些方法的标签依赖性也限制了它们在学习更一般的骨架表示方面的灵活性。本文提出了一种通用的无监督层次骨架元-原型对比学习（Hi-MPC）方法，结合硬骨架挖掘（HSM）用于无标签3D骨架的人物重新识别。首先，我们构建了骨架的层次表示，以模拟身体关节、组件和肢体层次的粗到细的身体和动作特征。然后，我们使用层次化的元-原型对比学习方法进行特征提取和聚类。

    With rapid advancements in depth sensors and deep learning, skeleton-based person re-identification (re-ID) models have recently achieved remarkable progress with many advantages. Most existing solutions learn single-level skeleton features from body joints with the assumption of equal skeleton importance, while they typically lack the ability to exploit more informative skeleton features from various levels such as limb level with more global body patterns. The label dependency of these methods also limits their flexibility in learning more general skeleton representations. This paper proposes a generic unsupervised Hierarchical skeleton Meta-Prototype Contrastive learning (Hi-MPC) approach with Hard Skeleton Mining (HSM) for person re-ID with unlabeled 3D skeletons. Firstly, we construct hierarchical representations of skeletons to model coarse-to-fine body and motion features from the levels of body joints, components, and limbs. Then a hierarchical meta-prototype contrastive learni
    
[^75]: 如何整理一张桌子：融合视觉和语义常识推理解决具有模糊目标的机器人任务

    How to Tidy Up a Table: Fusing Visual and Semantic Commonsense Reasoning for Robotic Tasks with Vague Objectives. (arXiv:2307.11319v1 [cs.RO])

    [http://arxiv.org/abs/2307.11319](http://arxiv.org/abs/2307.11319)

    这项工作提出了一种融合视觉和语义常识推理的方法来解决具有模糊目标的机器人任务中的整理桌子问题。通过利用大规模语言模型的学习，可以推理出人类行为的常识。尽管语言模型的能力受限，但通过考虑感知和低级控制因素，可以解决整理桌子的任务。

    

    在许多现实场景中，模糊的目标给机器人技术带来了长期的挑战，因为很难定义规则、奖励或约束以进行优化。像整理一张凌乱的桌子这样的任务对人类来说可能很简单，但由于常识推理的歧义和灵活性，表达整洁的标准却很复杂。近年来，在大规模语言模型（LLM）的进一步发展中，我们有了通过这些模糊目标进行推理的机会：LLM通过学习大量人类数据来捕捉有关人类行为的有意义的常识。然而，由于LLM仅训练于语言输入，它们在感知和低级控制方面的能力有限，因此可能在机器人任务中遇到困难。在这项工作中，我们提出了一种简单的方法来解决整理桌子的任务，这是一个具有模糊目标的机器人任务的示例。具体而言，整理一张桌子的任务不仅涉及按照类型和功能对物体进行聚类以实现语义整洁，还需要考虑感知和低级控制方面的因素。

    Vague objectives in many real-life scenarios pose long-standing challenges for robotics, as defining rules, rewards, or constraints for optimization is difficult. Tasks like tidying a messy table may appear simple for humans, but articulating the criteria for tidiness is complex due to the ambiguity and flexibility in commonsense reasoning. Recent advancement in Large Language Models (LLMs) offers us an opportunity to reason over these vague objectives: learned from extensive human data, LLMs capture meaningful common sense about human behavior. However, as LLMs are trained solely on language input, they may struggle with robotic tasks due to their limited capacity to account for perception and low-level controls. In this work, we propose a simple approach to solve the task of table tidying, an example of robotic tasks with vague objectives. Specifically, the task of tidying a table involves not just clustering objects by type and functionality for semantic tidiness but also considerin
    
[^76]: 恶意注释下的物体检测后门攻击

    Backdoor Attack against Object Detection with Clean Annotation. (arXiv:2307.10487v1 [cs.CV])

    [http://arxiv.org/abs/2307.10487](http://arxiv.org/abs/2307.10487)

    本文提出了一种在物体检测中进行后门攻击的方法，通过嵌入隐藏的后门，使得模型在正常数据上表现正常，在触发器出现时给出攻击者指定的判断。这对于安全敏感应用如自动驾驶具有严重威胁。

    

    深度神经网络（DNN）在物体检测任务中取得了前所未有的成功。然而，也发现DNN对多种攻击，包括后门攻击，是脆弱的。通过这种攻击，攻击者成功地将隐藏的后门嵌入到DNN中，使得模型在良性数据样本上表现正常，但在预定义触发器出现时给出攻击者指定的判断。尽管已经在图像分类上尝试了大量后门攻击，但对物体检测任务的后门攻击研究尚未得到适当的调查和探索。由于物体检测已被应用于多个安全敏感应用程序的重要模块，如自动驾驶，对物体检测的后门攻击可能造成更严重的威胁。受基于深度学习的目标检测器的固有属性启发，我们提出了一种简单而有效的物体检测后门攻击方法，而不修改地面真伪标签。

    Deep neural networks (DNNs) have shown unprecedented success in object detection tasks. However, it was also discovered that DNNs are vulnerable to multiple kinds of attacks, including Backdoor Attacks. Through the attack, the attacker manages to embed a hidden backdoor into the DNN such that the model behaves normally on benign data samples, but makes attacker-specified judgments given the occurrence of a predefined trigger. Although numerous backdoor attacks have been experimented on image classification, backdoor attacks on object detection tasks have not been properly investigated and explored. As object detection has been adopted as an important module in multiple security-sensitive applications such as autonomous driving, backdoor attacks on object detection could pose even more severe threats. Inspired by the inherent property of deep learning-based object detectors, we propose a simple yet effective backdoor attack method against object detection without modifying the ground tr
    
[^77]: 通过随机滤波和模式识别强化基于POD的反应扩散复杂网络模型简化技术

    Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition. (arXiv:2307.09762v1 [cs.CE])

    [http://arxiv.org/abs/2307.09762](http://arxiv.org/abs/2307.09762)

    该论文提出了一种算法框架，通过将模式识别和随机滤波理论的技术结合起来，强化了基于POD的反应扩散复杂网络模型简化技术，在受扰动输入的情况下提高了代理模型的准确性。

    

    复杂网络被用于建模许多现实世界系统，然而这些系统的维度使得其分析变得困难。在这种情况下，可以使用POD等降维技术。然而，这些模型容易受输入数据扰动的影响。我们提出了一种算法框架，将模式识别和随机滤波理论的技术结合起来，以增强这些模型的输出。研究结果表明，我们的方法可以在受扰动输入的情况下提高代理模型的准确性。深度神经网络(DNNs)容易受到对抗性攻击，然而最近的研究发现，神经常微分方程(ODEs)在特定应用中表现出鲁棒性。我们将我们的算法框架与基于神经ODE的方法进行了基准比较。

    Complex networks are used to model many real-world systems. However, the dimensionality of these systems can make them challenging to analyze. Dimensionality reduction techniques like POD can be used in such cases. However, these models are susceptible to perturbations in the input data. We propose an algorithmic framework that combines techniques from pattern recognition (PR) and stochastic filtering theory to enhance the output of such models. The results of our study show that our method can improve the accuracy of the surrogate model under perturbed inputs. Deep Neural Networks (DNNs) are susceptible to adversarial attacks. However, recent research has revealed that neural Ordinary Differential Equations (ODEs) exhibit robustness in specific applications. We benchmark our algorithmic framework with a Neural ODE-based approach as a reference.
    
[^78]: vONTSS：基于vMF和最优传输的半监督神经主题建模

    vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])

    [http://arxiv.org/abs/2307.01226](http://arxiv.org/abs/2307.01226)

    vONTSS是一种基于vMF和最优传输的半监督神经主题建模方法，它在分类准确率和多样性方面优于其他方法，并且支持无监督主题建模。实验证明，vONTSS比最近的NTM更快。

    

    最近，受变分自编码器启发的神经主题模型（NTM）引起了很多研究兴趣，然而，由于整合人类知识的挑战，这些方法在实际应用中受到了限制。本研究提出了一种半监督神经主题建模方法vONTSS，该方法利用基于von Mises-Fisher（vMF）的变分自编码器和最优传输。在半监督设置中，当提供每个主题的少量关键词时，vONTSS生成潜在主题并优化主题-关键词质量和主题分类。实验证明，vONTSS在分类准确率和多样性方面优于现有的半监督主题建模方法。vONTSS还支持无监督主题建模。定量和定性实验证明，vONTSS在无监督设置下在多个方面优于最近的NTM：vONTSS在基准数据集上发现高度聚类和连贯的主题。它也比现有-手法快得多。

    Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state
    
[^79]: 基于潜在扩散模型的文本驱动Foley音效生成

    Text-Driven Foley Sound Generation With Latent Diffusion Model. (arXiv:2306.10359v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2306.10359](http://arxiv.org/abs/2306.10359)

    本文提出了一种基于扩散模型的Foley音效生成系统，可进行文本条件的生成。我们通过迁移学习对系统进行微调，并引入可训练的层来改善文本嵌入，同时也改进了生成的波形。

    

    Foley音效生成旨在为多媒体内容生成背景音效。先前的模型通常使用大量有标签的开发集作为输入（例如，单个数字或one-hot向量）。本文提出了一种基于扩散模型的Foley音效生成系统，可进行文本条件的生成。为了缓解数据稀缺问题，我们的模型首先使用大规模数据集进行预训练，然后通过对比语言-音频配对（CLAP）技术进行迁移学习来对该任务进行微调。我们观察到，文本编码器提取的特征嵌入可以显著影响生成模型的性能。因此，我们在编码器之后引入可训练的层来改善编码器产生的文本嵌入。此外，我们通过同时生成多个候选音频片段并选择最佳片段来进一步改进生成的波形，最佳片段是根据嵌入之间相似性得分确定的。

    Foley sound generation aims to synthesise the background sound for multimedia content. Previous models usually employ a large development set with labels as input (e.g., single numbers or one-hot vector). In this work, we propose a diffusion model based system for Foley sound generation with text conditions. To alleviate the data scarcity issue, our model is initially pre-trained with large-scale datasets and fine-tuned to this task via transfer learning using the contrastive language-audio pertaining (CLAP) technique. We have observed that the feature embedding extracted by the text encoder can significantly affect the performance of the generation model. Hence, we introduce a trainable layer after the encoder to improve the text embedding produced by the encoder. In addition, we further refine the generated waveform by generating multiple candidate audio clips simultaneously and selecting the best one, which is determined in terms of the similarity score between the embedding of the 
    
[^80]: 以方差保持为基础的插值扩散模型在语音增强中的应用

    Variance-Preserving-Based Interpolation Diffusion Models for Speech Enhancement. (arXiv:2306.08527v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2306.08527](http://arxiv.org/abs/2306.08527)

    本研究提出了以方差保持为基础的插值扩散模型用于语音增强，经理论基础和框架推导证明了该方法的有效性，并给出了实际应用示例。同时，通过对困难问题的分析和调整超参数，提高了模型性能和训练便利性。评估结果展示该方法在公共基准上的优越性能。

    

    本研究的目标是实现语音增强的扩散模型。首先强调了连续条件下以方差保持为基础的插值扩散的理论基础。随后，我们提出了一个更简洁的框架，将以方差保持和以方差爆炸为基础的插值扩散方法结合在一起。我们证明这两种方法是所提出框架的特殊情况。此外，我们提供了一个以方差保持为基础的插值扩散在语音增强任务中的实际示例。为了提高性能和模型训练的便利性，我们分析了扩散模型中常见的困难，并提出可调节的超参数。最后，我们使用公共基准进行了几种方法的模型评估，展示了我们方法的有效性。

    The goal of this study is to implement diffusion models for speech enhancement (SE). The first step is to emphasize the theoretical foundation of variance-preserving (VP)-based interpolation diffusion under continuous conditions. Subsequently, we present a more concise framework that encapsulates both the VP- and variance-exploding (VE)-based interpolation diffusion methods. We demonstrate that these two methods are special cases of the proposed framework. Additionally, we provide a practical example of VP-based interpolation diffusion for the SE task. To improve performance and ease model training, we analyze the common difficulties encountered in diffusion models and suggest amenable hyper-parameters. Finally, we evaluate our model against several methods using a public benchmark to showcase the effectiveness of our approach
    
[^81]: 当基于大语言模型的智能体遇到用户行为分析：一种新颖的用户模拟范式

    When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm. (arXiv:2306.02552v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2306.02552](http://arxiv.org/abs/2306.02552)

    从事用户行为分析的学术界一直面临着收集足够高质量用户行为数据的难题，一种解决方案是自动模拟用户行为，近期研究表明，利用大语言模型进行可靠的用户模拟有了重要的突破，将这种模型应用到用户行为分析研究中有着巨大潜力，可能对传统研究范式产生革命性影响。

    

    用户行为分析在以人为中心的AI应用中至关重要。然而，收集足够和高质量的用户行为数据一直是一个基本但具有挑战性的问题。为了解决这个问题，自动模拟用户行为是一个直观的想法。然而，由于人类认知过程的主观和复杂性质，可靠地模拟用户行为是困难的。最近，大语言模型（LLM）取得了显著的成功，展示了实现类似人类智能的巨大潜力。我们认为这些模型为可靠的用户模拟提供了重要机会，并有可能改变传统的用户行为分析研究范式。在本文中，我们以推荐系统为例，探索使用LLM进行用户模拟的潜力。具体而言，我们将每个用户视为基于LLM的自治智能体，并让不同智能体在虚拟环境中自由交流、行为和发展。

    User behavior analysis is crucial in human-centered AI applications. In this field, the collection of sufficient and high-quality user behavior data has always been a fundamental yet challenging problem. An intuitive idea to address this problem is automatically simulating the user behaviors. However, due to the subjective and complex nature of human cognitive processes, reliably simulating the user behavior is difficult. Recently, large language models (LLM) have obtained remarkable successes, showing great potential to achieve human-like intelligence. We argue that these models present significant opportunities for reliable user simulation, and have the potential to revolutionize traditional study paradigms in user behavior analysis. In this paper, we take recommender system as an example to explore the potential of using LLM for user simulation. Specifically, we regard each user as an LLM-based autonomous agent, and let different agents freely communicate, behave and evolve in a vir
    
[^82]: LLMatic: 基于大语言模型和多样性优化的神经结构搜索

    LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])

    [http://arxiv.org/abs/2306.01102](http://arxiv.org/abs/2306.01102)

    本文介绍了利用大语言模型和多样性优化算法相结合的 LLMatic 神经结构搜索算法。该算法在CIFAR-10数据集进行测试，仅进行2000次搜索即可产生高性能网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。

    

    大型语言模型 (LLMs) 已成为一种强大的工具，可以完成广泛的任务。它们的能力涵盖了许多领域，它们在代码生成领域产生了重大影响。在此情况下，我们将 LLMs 视为变异和交叉工具。同时，多样性优化算法已知可以发现多样性和稳健的解决方案。通过将 LLMs 的代码生成能力与 QD 解决方案的多样性和鲁棒性相结合，我们引入了 LLMatic，一个神经结构搜索 (NAS) 算法。虽然 LLMs 通过提示直接进行 NAS 考验困难，但 LLMatic 利用程序化方法，利用 QD 来进行提示和网络结构，从而创建多样性和高性能网络。我们在 CIFAR-10 图像分类基准测试中测试了 LLMatic，证明它可以在仅进行 2000 次搜索的情况下产生具有竞争力的网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。

    Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
    
[^83]: 在动态语料库上持续更新生成检索

    Continually Updating Generative Retrieval on Dynamic Corpora. (arXiv:2305.18952v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2305.18952](http://arxiv.org/abs/2305.18952)

    本文研究了动态语料库上的生成检索。实验结果表明，在静态设置下，生成检索效果优于双编码器，但在动态设置下情况相反。通过使用参数高效的预训练方法，我们的模型DynamicGR在新的语料库上展现出了意外的性能。

    

    先前关于信息检索(IR)的大多数研究假设语料库是静态的，而实际世界中的文档是不断更新的。本文将知识的动态性引入检索系统中，将检索视为动态的知识库，更符合真实环境。我们对双编码器和生成检索进行全面评估，利用StreamingQA基准测试用于时态知识更新。我们的初步结果显示，在静态设置下，生成检索优于双编码器，但在动态设置下情况相反。然而，令人惊讶的是，当我们利用参数高效的预训练方法增强生成检索对新语料库的适应性时，我们的模型Dynamic Generative Retrieval (DynamicGR)展现出意外的发现。它能够在其内部索引中高效压缩新的知识，

    The majority of prior work on information retrieval (IR) assumes that the corpus is static, whereas in the real world, the documents are continually updated. In this paper, we incorporate often overlooked dynamic nature of knowledge into the retrieval systems. Our work treats retrieval not as static archives but as dynamic knowledge bases better aligned with real-world environments. We conduct a comprehensive evaluation of dual encoders and generative retrieval, utilizing the StreamingQA benchmark designed for the temporal knowledge updates. Our initial results show that while generative retrieval outperforms dual encoders in static settings, the opposite is true in dynamic settings. Surprisingly, however, when we utilize a parameter-efficient pre-training method to enhance adaptability of generative retrieval to new corpora, our resulting model, Dynamic Generative Retrieval (DynamicGR), exhibits unexpected findings. It (1) efficiently compresses new knowledge in their internal index, 
    
[^84]: KeyPosS: 基于 GPS 灵感的真实距离多边定位的即插即用面部标记检测

    KeyPosS: Plug-and-Play Facial Landmark Detection through GPS-Inspired True-Range Multilateration. (arXiv:2305.16437v1 [cs.CV])

    [http://arxiv.org/abs/2305.16437](http://arxiv.org/abs/2305.16437)

    KeyPosS是一种面部标记检测框架，采用真实距离多边定位算法实现快速而准确的检测，避免了传统方法中的计算负担和量化误差问题。

    

    在面部分析领域，准确的标记检测对于各种应用至关重要，包括人脸识别和表情分析等。然而，传统的热力图或坐标回归技术经常面临计算负担和量化误差等挑战。为解决这些问题，我们提出了 KeyPoint Positioning System（KeyPosS），这是一种突破性的面部标记检测框架，与现有方法不同。KeyPosS首次采用真实距离多边定位算法，一种最初用于GPS系统的技术，通过不依赖于计算密集型回归方法实现快速而准确的面部标记检测。该框架利用完全卷积网络预测距离图，计算感兴趣点（POI）与多个锚点之间的距离。通过巧妙地利用这些锚点来三角测量POI的位置，实现面部标记的检测。

    In the realm of facial analysis, accurate landmark detection is crucial for various applications, ranging from face recognition and expression analysis to animation. Conventional heatmap or coordinate regression-based techniques, however, often face challenges in terms of computational burden and quantization errors. To address these issues, we present the KeyPoint Positioning System (KeyPosS), a groundbreaking facial landmark detection framework that stands out from existing methods. For the first time, KeyPosS employs the True-range Multilateration algorithm, a technique originally used in GPS systems, to achieve rapid and precise facial landmark detection without relying on computationally intensive regression approaches. The framework utilizes a fully convolutional network to predict a distance map, which computes the distance between a Point of Interest (POI) and multiple anchor points. These anchor points are ingeniously harnessed to triangulate the POI's position through the Tru
    
[^85]: 通过摘要二元性和显式大纲控制增强生成

    Enhancing Generation through Summarization Duality and Explicit Outline Control. (arXiv:2305.14459v1 [cs.CL])

    [http://arxiv.org/abs/2305.14459](http://arxiv.org/abs/2305.14459)

    本文提出了一个两阶段的摘要增强的大纲监督生成框架，能够更好地生成明确和合理的大纲，并引入了一个新颖的显式大纲控制方法以更有效地利用生成的大纲。

    

    自动开放式长文本生成面临语义不连贯和情节不可信的重大挑战。先前的工作通常通过设计无监督任务中的短语或抽象信号的大纲来缓解此问题，但这往往是不稳定且难以解释的。在假设摘要作为已成熟的大纲的情况下，我们介绍了一个两阶段、摘要增强的大纲监督生成框架。该框架利用摘要任务的双重特征来改进大纲预测，从而产生更明确和合理的大纲。此外，我们发现基于大纲的生成具有未充分利用的问题，无论是标准的预训练语言模型（例如GPT-2、BART）还是大型语言模型（例如Vicuna、ChatGPT）。为了解决这个问题，我们提出了一种新颖的显式大纲控制方法，以更有效地利用生成的大纲。

    Automatically open-ended long text generation poses significant challenges due to semantic incoherence and plot implausibility. Previous works usually alleviate this problem through outlines in the form of short phrases or abstractive signals by designing unsupervised tasks, which tend to be unstable and weakly interpretable.  Assuming that a summary serves as a mature outline, we introduce a two-stage, summary-enhanced outline supervised generation framework. This framework leverages the dual characteristics of the summarization task to improve outline prediction, resulting in more explicit and plausible outlines. Furthermore, we identify an underutilization issue in outline-based generation with both standard pretrained language models (e.g., GPT-2, BART) and large language models (e.g., Vicuna, ChatGPT). To address this, we propose a novel explicit outline control method for more effective utilization of generated outlines.
    
[^86]: SPEECH: 基于能量的事件中心超球的结构化预测

    SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres. (arXiv:2305.13617v1 [cs.CL])

    [http://arxiv.org/abs/2305.13617](http://arxiv.org/abs/2305.13617)

    这篇论文提出了一种称为SPEECH的模型，它使用能量建模来表示复杂的事件结构，并使用超球来表示事件类别。实验结果表明，SPEECH在事件检测和事件关系抽取任务中表现出卓越的性能。

    

    事件中心的结构化预测涉及预测事件的结构化输出。在大多数自然语言处理情况下，事件结构都具有复杂的依赖关系，因此有效地表示这些复杂的事件结构是具有挑战性的。为了解决这些问题，我们提出了基于能量的事件中心超球的结构化预测 (SPEECH)。 SPEECH 使用基于能量的建模来模拟事件结构组件之间的复杂依赖关系，并使用简单但有效的超球来表示事件类别。在两个统一标注的事件数据集的实验中，结果表明SPEECH在事件检测和事件关系抽取任务中占优势。

    Event-centric structured prediction involves predicting structured outputs of events. In most NLP cases, event structures are complex with manifold dependency, and it is challenging to effectively represent these complicated structured events. To address these issues, we propose Structured Prediction with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex dependency among event structured components with energy-based modeling, and represents event classes with simple but effective hyperspheres. Experiments on two unified-annotated event datasets indicate that SPEECH is predominant in event detection and event-relation extraction tasks.
    
[^87]: 解耦参数中的知识：可插拔语言模型的新方法

    Decouple knowledge from paramters for plug-and-play language modeling. (arXiv:2305.11564v1 [cs.CL])

    [http://arxiv.org/abs/2305.11564](http://arxiv.org/abs/2305.11564)

    本文介绍了一种新的插件式预训练模型，其与模型参数中的知识存储分离，采用可编辑和可扩展的键值存储器，通过DPM中的知识检索以可解释的方式利用知识。

    

    预训练语言模型（PLM）在各种NLP任务中取得了令人印象深刻的成果。 揭示了这些模型成功的关键因素之一是这些模型的参数在预训练期间隐含地学习了各种知识。 然而，将知识隐含在模型参数中具有两个基本缺点。 首先，在模型训练后，无法编辑或扩展知识，特别是在知识不断发展的情况下，这是一个严重的问题。 其次，它缺乏可解释性并阻止人们了解PLM在某个问题上所需的哪些知识。 在本文中，我们介绍PlugLM，这是一种具有可微分插件存储器（DPM）的预训练模型。 关键的直觉是使用可编辑和可扩展的键值存储器将知识存储与模型参数分离，并通过DPM中的知识检索以可解释的方式利用知识。为了证明这种设计选择的合理性，我们在三个设置中进行评估，这些设置需要各种形式的知识：（1）领域适应，（2）未见实体合并，以及（3）在不遗忘旧任务的情况下适应新任务。

    Pre-trained language models(PLM) have made impressive results in various NLP tasks. It has been revealed that one of the key factors to their success is the parameters of these models implicitly learn all kinds of knowledge during pre-training. However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. Second, it lacks interpretability and prevents humans from understanding which knowledge PLM requires for a certain problem. In this paper, we introduce PlugLM, a pre-training model with differentiable plug-in memory(DPM). The key intuition is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the DPM. To justify this design choice, we conduct evaluations in three settings in
    
[^88]: AI和区块链作为可持续教学和学习工具来应对第四次工业革命

    AI & Blockchain as sustainable teaching and learning tools to cope with the 4IR. (arXiv:2305.01088v1 [cs.CY])

    [http://arxiv.org/abs/2305.01088](http://arxiv.org/abs/2305.01088)

    本论文回顾了现有的AI和区块链在教育领域的研究，探讨了这些技术可以提高个性化学习、安全认证和分散式学习网络的潜力和挑战。此外，本文提出了一个模型，将AI和区块链整合到可持续的教育实践中。总之，AI和区块链技术有望成为可持续教学和学习工具。

    

    第四次工业革命正在改变我们的生活和工作，而教育也不例外。为了应对4IR的挑战，需要创新和可持续的教学和学习工具。人工智能和区块链技术在这方面拥有巨大的潜力，如个性化学习、安全认证和分散式学习网络。本文对AI和区块链在教育中的现有研究进行了回顾，分析了案例研究，并探讨了这些技术的潜在利益和挑战。本文还提出了一种独特的模型，将AI和区块链整合到可持续的教学和学习实践中。未来的研究方向包括需要更多的实证研究和探索伦理和社会影响。本文讨论的关键摘要是，通过提高教育的可访问性、有效性和安全性，AI和区块链有潜力成为可持续的教学和学习工具。

    The Fourth Industrial Revolution (4IR) is transforming the way we live and work, and education is no exception. To cope with the challenges of 4IR, there is a need for innovative and sustainable teaching and learning tools. AI and block chain technologies hold great promise in this regard, with potential benefits such as personalized learning, secure credentialing, and decentralized learning networks. This paper presents a review of existing research on AI and block chain in education, analyzing case studies and exploring the potential benefits and challenges of these technologies. The paper also suggests a unique model for integrating AI and block chain into sustainable teaching and learning practices. Future research directions are discussed, including the need for more empirical studies and the exploration of ethical and social implications. The key summary of this discussion is that, by enhancing accessibility, efficacy, and security in education, AI and blockchain have the potenti
    
[^89]: 可解释人工智能的范畴基础：一种统一的结构和语义形式体系。

    Categorical Foundations of Explainable AI: A Unifying Formalism of Structures and Semantics. (arXiv:2304.14094v1 [cs.AI])

    [http://arxiv.org/abs/2304.14094](http://arxiv.org/abs/2304.14094)

    本文采用范畴理论的框架，提出了可解释AI的统一理论体系，为领域中所有重要术语提供了清晰的形式定义，并提供了遵循所提出结构的领域分类法。

    

    可解释人工智能（XAI）旨在回答与AI模型部署相关的伦理和法律问题。然而，相当数量的领域特定评论强调需要一个数学基础来定义领域中的关键概念，即使“解释”这个术语还缺乏精确定义。这些评论还主张建立一个健全而统一的可解释AI形式体系，以避免出现不良提出问题，帮助研究人员浏览一个快速增长的知识体系。据作者所知，该论文是填补该空白的首次尝试，通过形式化一个可解释AI的统一理论。采用范畴理论的框架，特别是反馈单调范畴，我们首先提供了可解释AI中所有重要术语的形式定义。然后，我们提出了一个遵循提出结构的领域分类法，展示了如何使用引入的理论来对当前研究的所有主要XAI系统类进行分类。

    Explainable AI (XAI) aims to answer ethical and legal questions associated with the deployment of AI models. However, a considerable number of domain-specific reviews highlight the need of a mathematical foundation for the key notions in the field, considering that even the term "explanation" still lacks a precise definition. These reviews also advocate for a sound and unifying formalism for explainable AI, to avoid the emergence of ill-posed questions, and to help researchers navigate a rapidly growing body of knowledge. To the authors knowledge, this paper is the first attempt to fill this gap by formalizing a unifying theory of XAI. Employing the framework of category theory, and feedback monoidal categories in particular, we first provide formal definitions for all essential terms in explainable AI. Then we propose a taxonomy of the field following the proposed structure, showing how the introduced theory can be used to categorize all the main classes of XAI systems currently studi
    
[^90]: 多模态三维物体检测的稀疏到密集体素区域融合方法

    SDVRF: Sparse-to-Dense Voxel Region Fusion for Multi-modal 3D Object Detection. (arXiv:2304.08304v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.08304](http://arxiv.org/abs/2304.08304)

    提出了一种新的稀疏到密集的体素区域加强融合方法，通过动态投影每个体素内部的稀疏局部点云获得体素区域，更好地对齐和避免背景噪声问题，并通过多尺度融合方法极大地提高了三维物体检测性能。

    

    在自动驾驶感知任务中，多模态方法成为趋势，主要因为LiDAR点云和图像数据的补充特性。然而，以往方法的性能通常受到点云稀疏或者LiDAR和相机之间偏差导致的噪声问题的限制。为了解决这两个问题，我们提出了一个新的概念，Voxel Region (VR)，通过动态投影每个体素中的稀疏局部点云来获得。我们提出了一种新颖的融合方法，称为Sparse-to-Dense Voxel Region Fusion (SDVRF)，具体而言，将VR内部较多的图像特征图像素收集起来，以补充从稀疏点中提取的体素特征，实现更密集的融合。与先前的方法不同，我们的动态区域生成策略实现了更好的对齐，并避免引入太多的背景噪声。此外，我们提出了一种多尺度融合方法来增强我们的方法的性能。在KITTI数据集上的大量实验表明，我们的方法在各种评估指标下显著优于现有最先进方法。

    In the perception task of autonomous driving, multi-modal methods have become a trend due to the complementary characteristics of LiDAR point clouds and image data. However, the performance of previous methods is usually limited by the sparsity of the point cloud or the noise problem caused by the misalignment between LiDAR and the camera. To solve these two problems, we present a new concept, Voxel Region (VR), which is obtained by projecting the sparse local point clouds in each voxel dynamically. And we propose a novel fusion method, named Sparse-to-Dense Voxel Region Fusion (SDVRF). Specifically, more pixels of the image feature map inside the VR are gathered to supplement the voxel feature extracted from sparse points and achieve denser fusion. Meanwhile, different from prior methods, which project the size-fixed grids, our strategy of generating dynamic regions achieves better alignment and avoids introducing too much background noise. Furthermore, we propose a multi-scale fusion
    
[^91]: 强化学习中基于Bandit方法的显式塑形外部建议算法的研究

    Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning. (arXiv:2304.07163v1 [cs.AI])

    [http://arxiv.org/abs/2304.07163](http://arxiv.org/abs/2304.07163)

    本文研究了如何基于Bandit方法将外部建议融入到强化学习中，并提出了三种不同的塑形算法：UCB-PIES（UPIES）， Racing-PIES（RPIES）和Lazy PIES（LPIES）。实验结果表明这些算法在样本复杂度、学习速度和形状质量方面都取得了良好的效果。

    

    强化学习（RL）算法中的一个关键问题是如何将外部或专家的建议融入到学习当中。本文将将将此问题表述为一种多臂赌博机称为塑形赌博机（shaping-bandits）。我们提出了三种不同的塑形算法：UCB-PIES（UPIES）， Racing-PIES（RPIES）和Lazy PIES（LPIES）。通过在模拟环境和LQR和Atari环境中的实验，我们证明了这三种算法在样本复杂度、学习速度和形状质量方面的有效性。

    A key challenge for a reinforcement learning (RL) agent is to incorporate external/expert1 advice in its learning. The desired goals of an algorithm that can shape the learning of an RL agent with external advice include (a) maintaining policy invariance; (b) accelerating the learning of the agent; and (c) learning from arbitrary advice [3]. To address this challenge this paper formulates the problem of incorporating external advice in RL as a multi-armed bandit called shaping-bandits. The reward of each arm of shaping bandits corresponds to the return obtained by following the expert or by following a default RL algorithm learning on the true environment reward.We show that directly applying existing bandit and shaping algorithms that do not reason about the non-stationary nature of the underlying returns can lead to poor results. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES (LPIES) three different shaping algorithms built on different assumptions that reason a
    
[^92]: AGIEval：一个以人为中心的基准评估基础模型的工具

    AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. (arXiv:2304.06364v1 [cs.CL])

    [http://arxiv.org/abs/2304.06364](http://arxiv.org/abs/2304.06364)

    AGIEval是一个以人为中心设计的基准测试工具，用于评估基础模型在人类中心标准化考试上的表现。GPT-4在SAT、LSAT和数学比赛方面超越了人类平均表现，展示了当代基础模型在人类级任务中的非凡性能。

    

    评估基础模型解决人类级别任务的通用能力是它们在发展和应用AGI（人工通用智能）中的重要方面。传统基准测试依赖于人造数据集，可能无法准确代表人类水平能力。在本文中，我们介绍了AGIEval，一个专门设计用于评估基础模型在人类中心标准化考试的基准测试工具，例如大学入学考试，法律学校入学考试，数学竞赛和律师资格考试。我们使用这个基准测试工具评估了几种最先进的基础模型，包括 GPT-4，ChatGPT 和Text-Davinci-003。令人印象深刻的是，GPT-4在SAT、LSAT和数学比赛方面超越了人类平均表现，SAT数学测试的准确率达到了95%，在中国国家大学英语考试的英语测试中准确率也达到了92.5%。这展示了当代基础模型在人类级任务中的非凡性能，并凸显了AGI未来发展的潜力。

    Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary fou
    
[^93]: 从个人角度视图中三维场景中恢复人体网格的概率方法研究

    Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views. (arXiv:2304.06024v1 [cs.CV])

    [http://arxiv.org/abs/2304.06024](http://arxiv.org/abs/2304.06024)

    本文提出一种基于场景条件的扩散方法来建模身体姿态分布，以解决在个人视角下3D场景中的人类姿态估计的挑战，训练中无需分类器，采样具有不同的条件和增强的多样性。

    

    在增强现实/虚拟现实应用中，自动感知人类社交互动行为至关重要，而一个重要组成部分是从个人视图中估计合理的3D人体姿态和形态。这项任务最大的挑战之一是由于个人场景中的近距离导致身体被截断严重，从而导致看不见身体部件的大量姿态模糊。为了应对这一挑战，我们提出了一种基于场景条件的扩散方法来模拟身体姿态分布。在3D场景几何条件的约束下，扩散模型生成在合理的人-场景交互中的身体，并通过基于物理碰撞得分的采样来进一步解决人-场景相互渗透问题。无需分类器的训练使得采样具有不同的条件和增强的多样性。一个可见性感知的图卷积模型通过每个关节的可见度来指导扩散去噪器，以合并互关节依赖。

    Automatic perception of human behaviors during social interactions is crucial for AR/VR applications, and an essential component is estimation of plausible 3D human pose and shape of our social partners from the egocentric view. One of the biggest challenges of this task is severe body truncation due to close social distances in egocentric scenarios, which brings large pose ambiguities for unseen body parts. To tackle this challenge, we propose a novel scene-conditioned diffusion method to model the body pose distribution. Conditioned on the 3D scene geometry, the diffusion model generates bodies in plausible human-scene interactions, with the sampling guided by a physics-based collision score to further resolve human-scene inter-penetrations. The classifier-free training enables flexible sampling with different conditions and enhanced diversity. A visibility-aware graph convolution model guided by per-joint visibility serves as the diffusion denoiser to incorporate inter-joint depende
    
[^94]: 理解人工智能编程助手的可用性

    Understanding the Usability of AI Programming Assistants. (arXiv:2303.17125v1 [cs.SE])

    [http://arxiv.org/abs/2303.17125](http://arxiv.org/abs/2303.17125)

    人工智能编程助手在快速完成编程任务方面有用，但输出的代码不适合开发者，导致他们不高频接受AI编程助手的初始建议。

    

    软件工程社区近年来广泛使用人工智能编程助手（例如GitHub Copilot）。然而在实践中，开发者并不高频接受AI编程助手的初始建议。这引发了与这些工具可用性相关的许多问题。为了了解开发者在使用这些工具时的实践情况和他们面临的重要的可用性挑战，我们向大量开发者进行了调查，并从410名开发者中获得了回复。通过定性和定量分析，我们发现，开发者最有动力使用AI编程助手的原因是它们可以帮助开发者减少按键次数，快速完成编程任务并调用语法，但它对帮助开发者思考潜在解决方案的支持度较低。我们还发现，开发者不使用这些工具的最重要原因是这些工具不能输出适合他们的代码。

    The software engineering community recently has witnessed widespread deployment of AI programming assistants, such as GitHub Copilot. However, in practice, developers do not accept AI programming assistants' initial suggestions at a high frequency. This leaves a number of open questions related to the usability of these tools. To understand developers' practices while using these tools and the important usability challenges they face, we administered a survey to a large population of developers and received responses from a diverse set of 410 developers. Through a mix of qualitative and quantitative analyses, we found that developers are most motivated to use AI programming assistants because they help developers reduce key-strokes, finish programming tasks quickly, and recall syntax, but resonate less with using them to help brainstorm potential solutions. We also found the most important reasons why developers do not use these tools are because these tools do not output code that add
    
[^95]: RE-MOVE：一种基于语言反馈的动态环境自适应策略设计方法

    RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via Language-Based Feedback. (arXiv:2303.07622v1 [cs.RO])

    [http://arxiv.org/abs/2303.07622](http://arxiv.org/abs/2303.07622)

    RE-MOVE提出了一种基于语言反馈的自适应策略设计方法，可以使机器人适应实时环境变化，并从人类反馈中学习并适应之前未见过的对抗性场景。

    

    连续控制机器人导航任务的强化学习策略经常无法在实时部署期间适应环境的变化，这可能导致灾难性的失败。为了解决这个问题，我们提出了一种名为RE-MOVE（请求帮助并移动）的新方法，它使用基于语言的反馈来调整经过训练的策略以适应环境的实时变化。在这项工作中，我们使经过训练的策略能够决定何时请求反馈并如何将反馈纳入训练好的策略中。RE-MOVE利用先验不确定性来确定请求人类反馈的最佳时间，并使用基于语言的反馈进行实时适应。我们进行了大量的合成和实际世界的评估，以展示我们提出的方法在多种测试时间动态导航场景中的好处。我们的方法使机器人能够从人类反馈中学习并适应之前未见过的对抗性场景。

    Reinforcement learning-based policies for continuous control robotic navigation tasks often fail to adapt to changes in the environment during real-time deployment, which may result in catastrophic failures. To address this limitation, we propose a novel approach called RE-MOVE (\textbf{RE}quest help and \textbf{MOVE} on), which uses language-based feedback to adjust trained policies to real-time changes in the environment. In this work, we enable the trained policy to decide \emph{when to ask for feedback} and \emph{how to incorporate feedback into trained policies}. RE-MOVE incorporates epistemic uncertainty to determine the optimal time to request feedback from humans and uses language-based feedback for real-time adaptation. We perform extensive synthetic and real-world evaluations to demonstrate the benefits of our proposed approach in several test-time dynamic navigation scenarios. Our approach enable robots to learn from human feedback and adapt to previously unseen adversarial 
    
[^96]: 导航的中层表示——虚拟导航

    Virtual Guidance as a Mid-level Representation for Navigation. (arXiv:2303.02731v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02731](http://arxiv.org/abs/2303.02731)

    该论文介绍了一种名为“虚拟导航”的新技术，通过在智能体的相机视图上叠加彩色路径或球的形式的视觉指引，以易于理解的导航指令传达抽象的导航信息。实验结果表明，在模拟和真实环境中，虚拟导航在遵循计划路径和避开障碍物等多个指标上优于现有方法。

    

    在自主导航的背景下，有效地传达抽象的导航指引给动态环境中的智能体存在挑战，特别是当导航信息是多模态的时候。为了解决这个问题，本文引入了一种名为“虚拟导航”的新技术，旨在以视觉方式呈现非视觉指令信号。这些视觉指引以彩色路径或球的形式叠加在智能体的相机视图上，作为易于理解的导航指令。我们通过在模拟和真实环境中进行实验来评估我们提出的方法。在模拟环境中，我们的虚拟导航在多项指标上优于基线混合方法，包括遵循计划路径和避开障碍物。此外，我们将虚拟导航的概念扩展到将基于文本提示的指令转换为用于真实环境实验的直观视觉格式。我们的结果验证了虚拟导航的适应性。

    In the context of autonomous navigation, effectively conveying abstract navigational cues to agents in dynamic environments poses challenges, particularly when the navigation information is multimodal. To address this issue, the paper introduces a novel technique termed "Virtual Guidance," which is designed to visually represent non-visual instructional signals. These visual cues, rendered as colored paths or spheres, are overlaid onto the agent's camera view, serving as easily comprehensible navigational instructions. We evaluate our proposed method through experiments in both simulated and real-world settings. In the simulated environments, our virtual guidance outperforms baseline hybrid approaches in several metrics, including adherence to planned routes and obstacle avoidance. Furthermore, we extend the concept of virtual guidance to transform text-prompt-based instructions into a visually intuitive format for real-world experiments. Our results validate the adaptability of virtua
    
[^97]: 模拟与实际强化学习在操纵中的应用：一种基于共识的方法

    Sim-and-Real Reinforcement Learning for Manipulation: A Consensus-based Approach. (arXiv:2302.13423v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.13423](http://arxiv.org/abs/2302.13423)

    本文提出了一种基于共识的模拟与实际深度强化学习算法，该算法在机器人操纵中表现出可比较的性能，并发现了在模拟中的最佳策略不一定适用于模拟与实际训练，以及模拟智能体数量越多，模拟与实际训练效果越好。

    

    模拟与实际训练是机器人操纵的一种有希望的替代方案。然而，当前的模拟与实际训练既不高效（即收敛到最优策略较慢），也不有效（即真实世界机器人数据较少）。考虑到有限的时间和硬件预算，模拟与实际训练的性能不尽如人意。本文提出一种基于共识的模拟与实际深度强化学习算法 (CSAR)，用于操纵器人的挑选和放置任务，该算法在模拟和实际世界中都表现出可比较的性能。在这个算法中，我们通过在模拟器和真实世界中训练智能体来获得模拟和实际世界的最优策略。我们发现了两个有趣的现象：（1）在模拟中的最佳策略并不是模拟与实际训练的最佳策略。（2）模拟智能体越多，模拟与实际训练效果越好。

    Sim-and-real training is a promising alternative to sim-to-real training for robot manipulations. However, the current sim-and-real training is neither efficient, i.e., slow convergence to the optimal policy, nor effective, i.e., sizeable real-world robot data. Given limited time and hardware budgets, the performance of sim-and-real training is not satisfactory. In this paper, we propose a Consensus-based Sim-And-Real deep reinforcement learning algorithm (CSAR) for manipulator pick-and-place tasks, which shows comparable performance in both sim-and-real worlds. In this algorithm, we train the agents in simulators and the real world to get the optimal policies for both sim-and-real worlds. We found two interesting phenomenons: (1) Best policy in simulation is not the best for sim-and-real training. (2) The more simulation agents, the better sim-and-real training. The experimental video is available at: https://youtu.be/mcHJtNIsTEQ.
    
[^98]: SAT需要彻底搜索

    SAT Requires Exhaustive Search. (arXiv:2302.09512v4 [cs.CC] CROSS LISTED)

    [http://arxiv.org/abs/2302.09512](http://arxiv.org/abs/2302.09512)

    本文证明了对于一些具有大域和长子句的极难例子，要求进行彻底搜索才能解决，这意味着P $\neq$ NP。

    

    本文通过构造具有大域和长子句的CSP和SAT的极难例子，证明这些例子无法在不进行彻底搜索的情况下解决，这意味着一个较弱的结论P $\neq$ NP。本文采用的是一种证明不可能性结果的建设性方法，与目前计算复杂性理论中使用的方法非常不同，但与Kurt G\"{o}del在证明他著名的逻辑不可能性结果时使用的方法相似。正如G\"{o}del的结果表明，在数学中证明形式上的不可证明性是可行的一样，本文的结果表明，在数学中证明计算上的难度不是很难的。具体来说，对许多问题，如3-SAT，证明下界可能具有挑战性，因为这些问题有各种有效的策略可用于避免进行彻底搜索。然而，在极难的例子中，彻底搜索可能是唯一可行的选择，证明其必要性变得更加重要。

    In this paper, by constructing extremely hard examples of CSP (with large domains) and SAT (with long clauses), we prove that such examples cannot be solved without exhaustive search, which implies a weaker conclusion P $\neq$ NP. This constructive approach for proving impossibility results is very different (and missing) from those currently used in computational complexity theory, but is similar to that used by Kurt G\"{o}del in proving his famous logical impossibility results. Just as shown by G\"{o}del's results that proving formal unprovability is feasible in mathematics, the results of this paper show that proving computational hardness is not hard in mathematics. Specifically, proving lower bounds for many problems, such as 3-SAT, can be challenging because these problems have various effective strategies available for avoiding exhaustive search. However, in cases of extremely hard examples, exhaustive search may be the only viable option, and proving its necessity becomes more 
    
[^99]: 使用指针生成网络和SciBERT嵌入生成研究论文的摘要

    Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings. (arXiv:2302.07729v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.07729](http://arxiv.org/abs/2302.07729)

    该论文提出了一种使用指针生成网络和SciBERT嵌入来自动生成研究论文亮点的方法。在多个基准数据集上的实验证明，该模型在研究亮点生成方面具有最佳性能。

    

    如今，许多研究文章都以研究亮点作为前言，以总结论文的主要发现。亮点不仅帮助研究人员准确快速地识别论文的贡献，还通过搜索引擎增加了文章的可发现性。我们的目标是在给定研究论文的特定段落的情况下自动构建研究亮点。我们使用了一个具有覆盖机制和上下文嵌入层的指针生成网络，将输入标记编码为SciBERT嵌入。我们在基准数据集CSPubSum上测试了我们的模型，并且还提出了MixSub，一个用于自动生成研究亮点的新的跨学科论文语料库。对于CSPubSum和MixSub，我们观察到所提出的模型相对于相关变体和文献中提出的其他模型来说具有最佳性能。在CSPubSum数据集上，我们的模型在只使用论文的摘要作为输入时表现最佳。

    Nowadays many research articles are prefaced with research highlights to summarize the main findings of the paper. Highlights not only help researchers precisely and quickly identify the contributions of a paper, they also enhance the discoverability of the article via search engines. We aim to automatically construct research highlights given certain segments of a research paper. We use a pointer-generator network with coverage mechanism and a contextual embedding layer at the input that encodes the input tokens into SciBERT embeddings. We test our model on a benchmark dataset, CSPubSum, and also present MixSub, a new multi-disciplinary corpus of papers for automatic research highlight generation. For both CSPubSum and MixSub, we have observed that the proposed model achieves the best performance compared to related variants and other models proposed in the literature. On the CSPubSum dataset, our model achieves the best performance when the input is only the abstract of a paper as op
    
[^100]: 高效开发驾驶策略：基于技能的分层强化学习方法

    Developing Driving Strategies Efficiently: A Skill-Based Hierarchical Reinforcement Learning Approach. (arXiv:2302.02179v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02179](http://arxiv.org/abs/2302.02179)

    本文提出了基于技能的分层驾驶策略，通过设计和使用运动原语作为高层动作，大幅减少了训练时间，并且在合并场景中得到了更高性能的驾驶模型。

    

    在人类和自动驾驶车辆中驾驶汽车是一项具有挑战性的任务，需要高层次的规划和推理能力。人类驾驶员可以轻松完成这个任务，因此一直在努力模拟人类驾驶员的策略。这些策略可以用作开发自动驾驶算法或创建真实感模拟器的灵感。强化学习是建模驾驶策略的常用工具，但传统的模型训练可能计算成本高且耗时。为了解决这个问题，本文提出了"基于技能"的分层驾驶策略，其中运动原语（即技能）被设计并用作高层动作。这减少了需要具有不同行为的多个模型的应用的训练时间。合并场景的仿真结果表明，与基准强化学习相比，该方法提供了在较少训练次数下实现更高性能的驾驶模型。

    Driving in dense traffic with human and autonomous drivers is a challenging task that requires high-level planning and reasoning. Human drivers can achieve this task comfortably, and there has been many efforts to model human driver strategies. These strategies can be used as inspirations for developing autonomous driving algorithms or to create high-fidelity simulators. Reinforcement learning is a common tool to model driver policies, but conventional training of these models can be computationally expensive and time-consuming. To address this issue, in this paper, we propose ``skill-based" hierarchical driving strategies, where motion primitives, i.e. skills, are designed and used as high-level actions. This reduces the training time for applications that require multiple models with varying behavior. Simulation results in a merging scenario demonstrate that the proposed approach yields driver models that achieve higher performance with less training compared to baseline reinforcemen
    
[^101]: 神经操作员：数据是否足以模拟世界？对物理启示机器学习影响的洞察

    Neural Operator: Is data all you need to model the world? An insight into the impact of Physics Informed Machine Learning. (arXiv:2301.13331v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.13331](http://arxiv.org/abs/2301.13331)

    本文探讨了如何将数据驱动方法与传统技术相结合，以解决工程和物理问题，并指出了机器学习方法的一些主要问题。

    

    常常使用偏微分方程（PDE）的数值近似来构建解决物理、工程和数学问题的方案，这些问题涉及到多个变量的函数，比如热传导或声音传播、流体流动、弹性、静电学、电动力学等。虽然这在解决许多复杂现象方面发挥了作用，但存在一些限制。常规方法如有限元法（FEM）和有限差分法（FDM）需要大量时间且计算成本高。相比之下，数据驱动的基于神经网络的方法提供了一种更快速、相对准确的替代方案，并具有离散不变性和分辨率不变性等优势。本文旨在深入了解数据驱动方法如何与传统技术相辅相成，解决工程和物理问题，同时指出机器学习方法的一些主要问题。

    Numerical approximations of partial differential equations (PDEs) are routinely employed to formulate the solution of physics, engineering and mathematical problems involving functions of several variables, such as the propagation of heat or sound, fluid flow, elasticity, electrostatics, electrodynamics, and more. While this has led to solving many complex phenomena, there are some limitations. Conventional approaches such as Finite Element Methods (FEMs) and Finite Differential Methods (FDMs) require considerable time and are computationally expensive. In contrast, data driven machine learning-based methods such as neural networks provide a faster, fairly accurate alternative, and have certain advantages such as discretization invariance and resolution invariance. This article aims to provide a comprehensive insight into how data-driven approaches can complement conventional techniques to solve engineering and physics problems, while also noting some of the major pitfalls of machine l
    
[^102]: 适用于所有领域的一个模型：基于协作域前缀调整的跨领域实体识别

    One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10410](http://arxiv.org/abs/2301.10410)

    本论文提出了基于协作域前缀调整的跨领域实体识别，使用文本到文本生成的支撑领域相关指导来将知识转移至新域NER任务，避免了先前的为每个领域结束一个全新的NER模型的问题。

    

    解决实际场景中低资源问题是跨领域实体识别的一个挑战性任务。先前典型的解决方案主要通过使用来自丰富资源领域的数据进行预训练语言模型(PLMs)获得NER模型并将其适应于目标领域。由于不同领域实体类型之间的不匹配问题，先前的方法通常调整所有PLMs的参数，从而为每个领域结束一个全新的NER模型。此外，当前的模型只关注于利用一个普通来源领域中的知识，而未能成功地将来自多个来源领域的知识转移到目标上。为了解决这些问题，我们基于文本到文本生成的PLM引入了协作域前缀调整跨领域NER(CP-NER)。具体来说，我们呈现了用于文本到文本生成的支撑领域相关指导来将知识转移至新域NER任务而无需结构修改。我们利用冻结的PLMs并进行协作域前缀调整。

    Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
    
[^103]: 在稳定子存在的情况下的等变表示学习

    Equivariant Representation Learning in the Presence of Stabilizers. (arXiv:2301.05231v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05231](http://arxiv.org/abs/2301.05231)

    引入了一种称为EquIN的方法，用于学习在数据上具有一般群作用等变的表示。通过考虑稳定子，该方法可以提取数据的几何结构。

    

    我们引入了等变同构网络（EquIN）-一种用于学习与数据上的一般群作用等变的表示的方法。与现有的等变表示学习方式不同，EquIN适用于非自由群作用，即通过非平凡对称性稳定数据的情况。EquIN在群论中的轨道稳定子定理的理论基础上进行。这保证了理想的学习器仅通过等变性训练时推断出同构表示，并完全提取了数据的几何结构。我们对具有旋转对称性的图像数据集进行了实证研究，并证明考虑稳定子可以提高表示的质量。

    We introduce Equivariant Isomorphic Networks (EquIN) -- a method for learning representations that are equivariant with respect to general group actions over data. Differently from existing equivariant representation learners, EquIN is suitable for group actions that are not free, i.e., that stabilize data via nontrivial symmetries. EquIN is theoretically grounded in the orbit-stabilizer theorem from group theory. This guarantees that an ideal learner infers isomorphic representations while trained on equivariance alone and thus fully extracts the geometric structure of data. We provide an empirical investigation on image datasets with rotational symmetries and show that taking stabilizers into account improves the quality of the representations.
    
[^104]: 解决奖励假设

    Settling the Reward Hypothesis. (arXiv:2212.10420v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.10420](http://arxiv.org/abs/2212.10420)

    解决奖励假设，明确指明假设成立的目标和目的的隐含要求。

    

    奖励假设认为，“我们所说的目标和目的都可以想象为最大化接收到的标量信号（奖励）的累积总和的预期值。”我们的目标是完全解决这个假设。这将不仅仅是一个简单的肯定或否定，而是完全指明假设成立的目标和目的的隐含要求。

    The reward hypothesis posits that, "all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward)." We aim to fully settle this hypothesis. This will not conclude with a simple affirmation or refutation, but rather specify completely the implicit requirements on goals and purposes under which the hypothesis holds.
    
[^105]: 使用语言模型提示进行推理：一项调查

    Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09597](http://arxiv.org/abs/2212.09597)

    本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。

    

    推理作为复杂问题解决的重要能力，可以为医疗诊断、谈判等各种实际应用提供后端支持。本文对使用语言模型提示进行推理的前沿研究进行了综合调查。我们介绍了研究成果的比较和总结，并提供了系统资源以帮助初学者。我们还讨论了新兴推理能力出现的潜在原因，并突出了未来的研究方向。资源可在 https://github.com/zjunlp/Prompt4ReasoningPapers 上获取（定期更新）。

    Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
    
[^106]: FedALA: 自适应局部聚合用于个性化联邦学习

    FedALA: Adaptive Local Aggregation for Personalized Federated Learning. (arXiv:2212.01197v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01197](http://arxiv.org/abs/2212.01197)

    FedALA是一种用于个性化联邦学习的方法，通过自适应局部聚合（ALA）模块来解决统计异质性问题，并在广泛的实验证明中超过了11种最先进的基准模型。

    

    联邦学习中的一个关键挑战是统计异质性，这会影响全局模型在每个客户端上的泛化能力。为了解决这个问题，我们提出了一种名为Federated learning with Adaptive Local Aggregation（FedALA）的方法，通过在个性化联邦学习中捕捉全局模型对客户端模型中所需的信息。FedALA的关键组成部分是自适应局部聚合（ALA）模块，它可以根据每个客户端上的局部目标自适应聚合下载的全局模型和本地模型以在每次迭代中初始化本地模型。为了评估FedALA的有效性，我们在计算机视觉和自然语言处理领域使用了五个基准数据集进行了大量的实验证明。FedALA在测试准确性方面比十一种最先进的基准模型取得了最多3.27%的改进。此外，我们还将ALA模块应用于其他联邦学习方法，并在测试准确性方面取得了最多24.19%的改进。

    A key challenge in federated learning (FL) is the statistical heterogeneity that impairs the generalization of the global model on each client. To address this, we propose a method Federated learning with Adaptive Local Aggregation (FedALA) by capturing the desired information in the global model for client models in personalized FL. The key component of FedALA is an Adaptive Local Aggregation (ALA) module, which can adaptively aggregate the downloaded global model and local model towards the local objective on each client to initialize the local model before training in each iteration. To evaluate the effectiveness of FedALA, we conduct extensive experiments with five benchmark datasets in computer vision and natural language processing domains. FedALA outperforms eleven state-of-the-art baselines by up to 3.27% in test accuracy. Furthermore, we also apply ALA module to other federated learning methods and achieve up to 24.19% improvement in test accuracy.
    
[^107]: VRDU：面向视觉丰富的文档理解的基准测试

    VRDU: A Benchmark for Visually-rich Document Understanding. (arXiv:2211.15421v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.15421](http://arxiv.org/abs/2211.15421)

    本研究提出了一个名为VRDU的基准测试，以更全面地反映实际文档的复杂性，其中包含具有挑战性的丰富模式、复杂模板和多样的布局。该基准测试可用于评估文档中提取结构化数据的模型。

    

    理解丰富视觉化业务文档以提取结构化数据和自动化业务工作流程在学术界和工业界都受到关注。虽然最近的多模式语言模型取得了令人印象深刻的成果，但我们发现现有的基准测试不反映工业中实际文档的复杂性。在这项工作中，我们确定了更全面的基准测试的必要条件，并提出了一个称为Visually Rich Document Understanding (VRDU)的基准测试。VRDU包含两个数据集，代表了多种挑战：丰富的模式，包括各种数据类型以及分层实体; 复杂的模板，包括表格和多列布局; 以及单个文档类型中不同布局（模板）的多样性。我们设计了少样本和常规实验设置，以及一个精心设计的匹配算法来评估提取结果。我们报告了强基线的性能，并提供了三个观察结果：(1)通用n的推广。

    Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry. Although recent multi-modal language models have achieved impressive results, we find that existing benchmarks do not reflect the complexity of real documents seen in industry. In this work, we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding (VRDU). VRDU contains two datasets that represent several challenges: rich schema including diverse data types as well as hierarchical entities, complex templates including tables and multi-column layouts, and diversity of different layouts (templates) within a single document type. We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results. We report the performance of strong baselines and offer three observations: (1) generalizing to n
    
[^108]: 一种具有高效优化和量子适用性的费米子神经网络

    A fermion neural network with efficient optimization and quantum applicability. (arXiv:2211.05793v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2211.05793](http://arxiv.org/abs/2211.05793)

    本文提出了一种费米子神经网络（FNN），它将输入作为初始层，输出物理特性，建立了一种高效的优化方法，可应用于具有相互作用的硬量子系统，而且能够精确地确定拓扑相和紧凑电荷序，其量子特性带来多种优势。

    

    经典人工神经网络已在机器学习领域取得了广泛成功。本文提出了一种费米子神经网络（FNN），其物理特性（例如局部态密度或条件电导）在输入作为初始层后作为输出。与反向传播类似，我们建立了一种高效优化方法，使FNN在具有挑战性的机器学习基准测试上表现出竞争性能。FNN也直接应用于量子系统，包括具有相互作用的硬系统，并在无预处理或假设的情况下提供原位分析。在机器学习之后，FNN精确地确定拓扑相和紧凑电荷序。它们的量子特性也带来了各种优势：量子相关性使网络连接更加通用，并且可以深入了解消失的梯度问题，量子纠缠则为可解释的机器学习打开了新的途径等。

    Classical artificial neural networks have witnessed widespread successes in machine-learning applications. Here, we propose fermion neural networks (FNNs) whose physical properties, such as local density of states or conditional conductance, serve as outputs, once the inputs are incorporated as an initial layer. Comparable to back-propagation, we establish an efficient optimization, which entitles FNNs to competitive performance on challenging machine-learning benchmarks. FNNs also directly apply to quantum systems, including hard ones with interactions, and offer in-situ analysis without preprocessing or presumption. Following machine learning, FNNs precisely determine topological phases and emergent charge orders. Their quantum nature also brings various advantages: quantum correlation entitles more general network connectivity and insight into the vanishing gradient problem, quantum entanglement opens up novel avenues for interpretable machine learning, etc.
    
[^109]: 基于模态逻辑的统计因果关系形式化

    Formalizing Statistical Causality via Modal Logic. (arXiv:2210.16751v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.16751](http://arxiv.org/abs/2210.16751)

    提出了一种基于模态逻辑的形式语言，用于描述和解释统计因果关系，并且能够指定和解释统计因果推断的正确性。

    

    我们提出了一种描述和解释统计因果关系的形式语言。具体来说，我们定义了统计因果语言（StaCL），用于表达随机变量上的因果效应并指定因果推断的要求。StaCL通过干预的模态运算符，在不同可能的世界的概率分布之间表达因果属性，在Kripke模型中解释。我们使用StaCL公式正式化概率分布、干预和因果谓词的公理。这些公理足够表达Pearl的do-calculus规则。最后，我们通过示例证明了StaCL可以用于指定和解释统计因果推断的正确性。

    We propose a formal language for describing and explaining statistical causality. Concretely, we define Statistical Causality Language (StaCL) for expressing causal effects on random variables and specifying the requirements for causal inference. StaCL incorporates modal operators for interventions to express causal properties between probability distributions in different possible worlds in a Kripke model. We formalize axioms for probability distributions, interventions, and causal predicates using StaCL formulas. These axioms are expressive enough to derive the rules of Pearl's do-calculus. Finally, we demonstrate by examples that StaCL can be used to specify and explain the correctness of statistical causal inference.
    
[^110]: 生成式知识图谱构建综述

    Generative Knowledge Graph Construction: A Review. (arXiv:2210.12714v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.12714](http://arxiv.org/abs/2210.12714)

    本文综述了生成式知识图谱构建领域的最新进展，包括方法分类和优劣分析，并提出了未来的研究方向。

    

    生成式知识图谱构建（KGC）是指利用序列到序列框架构建灵活且可适用于广泛任务的知识图谱。本研究总结了生成式知识图谱构建领域中近期的重要进展，对不同的生成目标从理论和实证分析角度分别讨论了各种方法的优势和不足，并提出了未来有潜力的研究方向。我们的贡献有三个方面：（1）我们提供了生成式KGC方法的详细、完整的分类体系；（2）我们对生成式KGC方法进行了理论和实证分析；（3）我们提出了几个未来可以发展的研究方向。

    Generative Knowledge Graph Construction (KGC) refers to those methods that leverage the sequence-to-sequence framework for building knowledge graphs, which is flexible and can be adapted to widespread tasks. In this study, we summarize the recent compelling progress in generative knowledge graph construction. We present the advantages and weaknesses of each paradigm in terms of different generation targets and provide theoretical insight and empirical analysis. Based on the review, we suggest promising research directions for the future. Our contributions are threefold: (1) We present a detailed, complete taxonomy for the generative KGC methods; (2) We provide a theoretical and empirical analysis of the generative KGC methods; (3) We propose several research directions that can be developed in the future.
    
[^111]: 以架构感知参考作为提示提高了数据有效的知识图谱构建

    Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction. (arXiv:2210.10709v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.10709](http://arxiv.org/abs/2210.10709)

    提出了一种以检索增强的架构感知参考作为提示的方法，可动态利用人类注释和弱监督数据所继承的架构和知识，指导生成具有更好语义连贯性和一致性的结构化知识，从而在数据效率和知识质量方面具有优越性。

    

    随着预训练语言模型的发展，许多基于提示的方法被提出并在数据有效的知识图谱构建中取得了令人瞩目的表现。然而，现有的基于提示的学习方法仍存在几个潜在的限制：（i）自然语言和预定义模式的输出结构化知识之间的语义差距，这意味着模型无法充分利用受限模板的语义知识；（ii）基于局部个体实例的表示学习限制了性能，给定了不充足的特征，这些特征不能释放预先训练语言模型的潜在类比能力。受这些观察的启发，我们提出了一种检索增强的方法，使用检索得到的架构感知参考作为提示，提高了数据有效的知识图谱构建的语义连贯性和一致性。在两个标准数据集上的实验结果表明，相比现有的基于提示和非提示的方法，我们提出的方法在数据效率和知识质量方面具有优越性。

    With the development of pre-trained language models, many prompt-based approaches to data-efficient knowledge graph construction have been proposed and achieved impressive performance. However, existing prompt-based learning methods for knowledge graph construction are still susceptible to several potential limitations: (i) semantic gap between natural language and output structured knowledge with pre-defined schema, which means model cannot fully exploit semantic knowledge with the constrained templates; (ii) representation learning with locally individual instances limits the performance given the insufficient features, which are unable to unleash the potential analogical capability of pre-trained language models. Motivated by these observations, we propose a retrieval-augmented approach, which retrieves schema-aware Reference As Prompt (RAP), for data-efficient knowledge graph construction. It can dynamically leverage schema and knowledge inherited from human-annotated and weak-supe
    
[^112]: 实现真实低资源关系抽取: 针对具有实证基准研究的论文

    Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study. (arXiv:2210.10678v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.10678](http://arxiv.org/abs/2210.10678)

    本文针对低资源环境中的关系抽取进行了实证研究，并提出了三种方案来提高性能，包括使用提示方法、平衡方法和数据增强技术。通过对8个关系抽取数据集的广泛比较，实验结果表明，虽然基于提示的调整有益于低资源关系抽取，但仍有改进空间，尤其是跨句子上下文中的多个关系三元组的抽取。

    

    本文提出了一项针对低资源环境中构建关系抽取系统的实证研究。基于最近的预训练语言模型，我们全面调查了三种方案来评估低资源环境下的性能：(i) 使用少量标记数据的不同类型的提示方法； (ii) 多样化的平衡方法来解决长尾分布问题； (iii) 数据增强技术和自训练来生成更多领域内标记数据。我们创建了一个包含8个关系抽取(RE) 数据集的基准，涵盖了不同的语言、领域和上下文，并对所提出的方案进行了广泛的比较。我们的实验证明：(i) 虽然基于提示的调整在低资源关系抽取中是有益的，但仍有很大的改进潜力，特别是在提取跨句子上下文中的多个关系三元组方面； (ii) 平衡方法并不总是有助于长尾分布的关系抽取。

    This paper presents an empirical study to build relation extraction systems in low-resource settings. Based upon recent pre-trained language models, we comprehensively investigate three schemes to evaluate the performance in low-resource settings: (i) different types of prompt-based methods with few-shot labeled data; (ii) diverse balancing methods to address the long-tailed distribution issue; (iii) data augmentation technologies and self-training to generate more labeled in-domain data. We create a benchmark with 8 relation extraction (RE) datasets covering different languages, domains and contexts and perform extensive comparisons over the proposed schemes with combinations. Our experiments illustrate: (i) Though prompt-based tuning is beneficial in low-resource RE, there is still much potential for improvement, especially in extracting relations from cross-sentence contexts with multiple relational triples; (ii) Balancing methods are not always helpful for RE with long-tailed distr
    
[^113]: 基于可微物理引擎的索驱动机器人的真实世界到仿真世界的控制转移

    Real2Sim2Real Transfer for Control of Cable-driven Robots via a Differentiable Physics Engine. (arXiv:2209.06261v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.06261](http://arxiv.org/abs/2209.06261)

    本文描述了一种基于可微物理引擎的真实世界到仿真世界转移的策略，该策略通过对真实机器人的有限数据进行迭代训练，以减少实到虚之间的差距并产生准确的仿真。该策略在索驱动张力结构机器人上得到了测试，并证明了其有效性。

    

    张力结构机器人由坚硬的杆和柔软的缆绳组成，具有高强度重量比和显著的变形能力，使其能够在非结构化的地形中航行并在严峻的撞击中存活。然而，由于维度高、动力复杂且耦合结构使得它们难以控制。基于物理的仿真是开发可以转移到实际机器人的运动策略的有前途途径。然而，由于实到虚之间的显著差距，对张力结构机器人进行建模是一个复杂的任务。为了解决这个问题，本文描述了一种基于不同iable物理引擎的张力结构机器人的真实世界到仿真世界的转移策略(R2S2R)。该策略基于一个可训练的可微物理引擎，通过对真实机器人的有限数据进行训练，并将包括物理属性的离线测量，如质量和几何体的各种机器人部件，以及使用随机控制策略的轨迹观察。利用来自真实机器人的数据，物理引擎可以进行迭代训练，以减少实到虚之间的差距并产生准确的仿真。这种R2S2R策略在索驱动张力结构机器人上得到了测试，并证明了使用可微物理引擎开发可以转移到实际机器人的控制策略的有效性。

    Tensegrity robots, composed of rigid rods and flexible cables, exhibit high strength-to-weight ratios and significant deformations, which enable them to navigate unstructured terrains and survive harsh impacts. They are hard to control, however, due to high dimensionality, complex dynamics, and a coupled architecture. Physics-based simulation is a promising avenue for developing locomotion policies that can be transferred to real robots. Nevertheless, modeling tensegrity robots is a complex task due to a substantial sim2real gap. To address this issue, this paper describes a Real2Sim2Real (R2S2R) strategy for tensegrity robots. This strategy is based on a differentiable physics engine that can be trained given limited data from a real robot. These data include offline measurements of physical properties, such as mass and geometry for various robot components, and the observation of a trajectory using a random control policy. With the data from the real robot, the engine can be iterativ
    
[^114]: 通过分散社会制裁的出现，分工的形成

    The emergence of division of labor through decentralized social sanctioning. (arXiv:2208.05568v4 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2208.05568](http://arxiv.org/abs/2208.05568)

    本研究通过引入社会规范模型，展示了分散社会制裁的出现模式能够解决以自利为导向的终身学习个体中的分工问题。

    

    人类生态成功依赖于我们的独特能力，即灵活自组织成合作社会群体，其中最成功的群体采用了大量的专业化和分工。与大多数其他动物不同，人类通过一生的试错中学习自己要扮演的角色。然而，当某些关键角色比其他角色更具吸引力，并且个体是自利的时，就会出现社会困境：每个个体都希望其他人扮演关键但无报酬的角色，这样他们可以自由选择一个报酬更高的角色。但是，如果每个人都这样行事，且一个关键角色缺乏填补，就会发生灾难。在这种情况下，学习最佳角色分配可能是不可能的。因此，一个基本问题是：如何在一群以自利为导向的终身学习个体中形成分工呢？在这里，我们展示了通过引入社会规范模型（我们将其视为分散社会制裁的出现模式）可以解决这个问题。

    Human ecological success relies on our characteristic ability to flexibly self-organize into cooperative social groups, the most successful of which employ substantial specialization and division of labor. Unlike most other animals, humans learn by trial and error during their lives what role to take on. However, when some critical roles are more attractive than others, and individuals are self-interested, then there is a social dilemma: each individual would prefer others take on the critical-but-unremunerative roles so they may remain free to take one that pays better. But disaster occurs if all act thusly and a critical role goes unfilled. In such situations learning an optimum role distribution may not be possible. Consequently, a fundamental question is: how can division of labor emerge in groups of self-interested lifetime-learning individuals? Here we show that by introducing a model of social norms, which we regard as emerging patterns of decentralized social sanctioning, it be
    
[^115]: 下一代超级卫星网络的人工智能技术

    Artificial Intelligence Techniques for Next-Generation Mega Satellite Networks. (arXiv:2207.00414v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2207.00414](http://arxiv.org/abs/2207.00414)

    本文介绍了如何使用人工智能技术来解决超级卫星网络通信中的挑战，包括卫星间链路、短暂时间过程和卫星覆盖范围等问题。

    

    由于太空发射、电子、处理能力和微型化的重大进展，空间通信，特别是超级卫星网络，重新成为下一代网络的有吸引力的候选者。然而，超级卫星网络依赖于无数的基础和相互交织的过程，传统模型不能真正捕捉它们的动态和独特特征，如轨道速度、卫星间链路、短暂的时间过程和卫星覆盖范围等。因此，需要新的方法来使网络主动适应与链接中快速变化的条件。人工智能提供了一种捕捉这些过程、分析它们的行为并模拟它们对网络影响的途径。本文介绍了应用AI技术于综合地面卫星网络，特别是超级卫星网络通信的应用。它详细介绍了超级卫星网络的独特特征和相关的挑战，并讨论了基于AI的技术如何帮助克服这些挑战。

    Space communications, particularly mega satellite networks, re-emerged as an appealing candidate for next generation networks due to major advances in space launching, electronics, processing power, and miniaturization. However, mega satellite networks rely on numerous underlying and intertwined processes that cannot be truly captured using conventionally used models, due to their dynamic and unique features such as orbital speed, inter-satellite links, short time pass, and satellite footprint, among others. Hence, new approaches are needed to enable the network to proactively adjust to the rapidly varying conditions associated within the link. Artificial intelligence (AI) provides a pathway to capture these processes, analyze their behavior, and model their effect on the network. This article introduces the application of AI techniques for integrated terrestrial satellite networks, particularly mega satellite network communications. It details the unique features of mega satellite net
    
[^116]: 检索增强的图神经网络的实证研究

    An Empirical Study of Retrieval-enhanced Graph Neural Networks. (arXiv:2206.00362v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00362](http://arxiv.org/abs/2206.00362)

    这项研究考察了检索增强的图神经网络在图数据集中的有效性，设计了一种称为GRAPHRETRIEVAL的检索增强方案，为图神经网络中学习的有用信息提供了增强。

    

    图神经网络（GNNs）是图表示学习的有效工具。大多数GNNs依赖于递归的邻居聚合方案，称为消息传递，因此它们的理论表达能力限于一阶Weisfeiler-Lehman测试（1-WL）。解决这个挑战的一种有效方法是明确地检索用于增强GNN模型的一些已注释的示例。虽然在语言和视觉领域中已经证明了检索增强模型的有效性，但是当应用于图数据集时，检索增强的GNNs的有效性问题仍然是一个悬而未决的问题。出于这个目的，我们想探索检索思想如何帮助增强图神经网络中学习的有用信息，并设计了一个称为GRAPHRETRIEVAL的检索增强方案，该方案对于图神经网络模型的选择是不可知的。在GRAPHRETRIEVAL中，对于每个输入图，从现有数据库中检索出相似的图以及它们的真实标签。

    Graph Neural Networks (GNNs) are effective tools for graph representation learning. Most GNNs rely on a recursive neighborhood aggregation scheme, named message passing, thereby their theoretical expressive power is limited to the first-order Weisfeiler-Lehman test (1-WL). An effective approach to this challenge is to explicitly retrieve some annotated examples used to enhance GNN models. While retrieval-enhanced models have been proved to be effective in many language and vision domains, it remains an open question how effective retrieval-enhanced GNNs are when applied to graph datasets. Motivated by this, we want to explore how the retrieval idea can help augment the useful information learned in the graph neural networks, and we design a retrieval-enhanced scheme called GRAPHRETRIEVAL, which is agnostic to the choice of graph neural network models. In GRAPHRETRIEVAL, for each input graph, similar graphs together with their ground-true labels are retrieved from an existing database. 
    
[^117]: 通过自注意力和深度学习中的软标签鉴定二维量子材料

    Two-Dimensional Quantum Material Identification via Self-Attention and Soft-labeling in Deep Learning. (arXiv:2205.15948v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.15948](http://arxiv.org/abs/2205.15948)

    通过自注意力和软标签鉴定二维量子材料中实例分割中缺失标注的问题，并使用特殊的机制和损失策略来减少负面影响，取得了实验中令人满意的结果。

    

    在量子机器领域中，检测硅芯片中的二维材料是一个非常关键的问题之一。实例分割可以被认为是解决这个问题的一个潜在方法。然而，与其他深度学习方法类似，实例分割需要大规模的训练数据集和高质量的标注才能达到相当的性能。在实践中，准备训练数据集是一个挑战，因为标注者必须处理大图像，例如2K分辨率，并且在这个问题中存在极密集的对象。在这项工作中，我们提出了一种新的方法来解决二维量子材料标识中实例分割中缺失标注的问题。我们提出了一种新的机制来自动检测误判的对象，并采用基于注意力的损失策略来减少这些对象对总体损失函数的负面影响。我们在二维材料检测数据集上进行实验，并得到了令人满意的结果。

    In quantum machine field, detecting two-dimensional (2D) materials in Silicon chips is one of the most critical problems. Instance segmentation can be considered as a potential approach to solve this problem. However, similar to other deep learning methods, the instance segmentation requires a large scale training dataset and high quality annotation in order to achieve a considerable performance. In practice, preparing the training dataset is a challenge since annotators have to deal with a large image, e.g 2K resolution, and extremely dense objects in this problem. In this work, we present a novel method to tackle the problem of missing annotation in instance segmentation in 2D quantum material identification. We propose a new mechanism for automatically detecting false negative objects and an attention based loss strategy to reduce the negative impact of these objects contributing to the overall loss function. We experiment on the 2D material detection datasets, and the experiments s
    
[^118]: 混合Transformer与多级融合用于多模态知识图谱补全

    Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion. (arXiv:2205.02357v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.02357](http://arxiv.org/abs/2205.02357)

    本文提出了一种混合Transformer与多级融合的方法，用于解决多模态知识图谱补全的问题。该方法通过统一的输入-输出架构适用于多样的任务，同时利用多级融合将视觉和文本表示集成起来。

    

    最近，多模态知识图谱（MKG）在信息检索、问答和推荐系统等任务中取得了成功，MKG组织了视觉-文本事实知识。然而，由于大多数MKG都不完整，因此提出了广泛的知识图谱补全研究，重点关注多模态实体、关系提取和链接预测。本文针对这些问题提出了一种混合Transformer与多级融合的方法。具体来说，我们利用一种混合Transformer架构和统一的输入-输出来完成多样的多模态知识图谱补全任务。此外，我们提出了多级融合，通过粗粒度前缀引导交互和细粒度相关感知将视觉和文本表示集成起来。

    Multimodal Knowledge Graphs (MKGs), which organize visual-text factual knowledge, have recently been successfully applied to tasks such as information retrieval, question answering, and recommendation system. Since most MKGs are far from complete, extensive knowledge graph completion studies have been proposed focusing on the multimodal entity, relation extraction and link prediction. However, different tasks and modalities require changes to the model architecture, and not all images/objects are relevant to text input, which hinders the applicability to diverse real-world scenarios. In this paper, we propose a hybrid transformer with multi-level fusion to address those issues. Specifically, we leverage a hybrid transformer architecture with unified input-output for diverse multimodal knowledge graph completion tasks. Moreover, we propose multi-level fusion, which integrates visual and text representation via coarse-grained prefix-guided interaction and fine-grained correlation-aware f
    
[^119]: 设备上学习：基于神经网络的可训练边缘人工智能

    On-Device Learning: A Neural Network Based Field-Trainable Edge AI. (arXiv:2203.01077v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01077](http://arxiv.org/abs/2203.01077)

    本研究介绍了一种基于神经网络的设备上学习方法，针对边缘人工智能应用中的环境因素对准确性造成的影响进行了解决。通过重新训练，在嘈杂环境下显著提高了异常检测的准确性，同时节约了低功耗设备的计算和通信成本。

    

    在真实世界的边缘人工智能应用中，其准确性经常受到各种环境因素的影响，如噪声、传感器的位置/校准和时间相关的变化。本文介绍了一种基于神经网络的设备上学习方法，以解决这个问题而不需要深入了解。我们的方法与事实上的反向传播训练有很大区别，而是专为低端边缘设备量身定制。本文介绍了其算法和在由树莓派Pico和低功耗无线模块组成的无线传感器节点上的实现。通过使用旋转机器的振动模式进行实验证明，通过设备上学习的重新训练在嘈杂环境下显著提高了异常检测的准确性，同时节约了低功耗设备的计算和通信成本。

    In real-world edge AI applications, their accuracy is often affected by various environmental factors, such as noises, location/calibration of sensors, and time-related changes. This article introduces a neural network based on-device learning approach to address this issue without going deep. Our approach is quite different from de facto backpropagation based training but tailored for low-end edge devices. This article introduces its algorithm and implementation on a wireless sensor node consisting of Raspberry Pi Pico and low-power wireless module. Experiments using vibration patterns of rotating machines demonstrate that retraining by the on-device learning significantly improves an anomaly detection accuracy at a noisy environment while saving computation and communication costs for low power.
    
[^120]: 基于模型的强化学习中遵循奖励的子任务

    Reward-Respecting Subtasks for Model-Based Reinforcement Learning. (arXiv:2202.03466v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03466](http://arxiv.org/abs/2202.03466)

    论文提出了一种基于模型的强化学习的方法，通过添加奖励加成的子任务来发现选项，从而解决了以前方法中忽略原始奖励的问题。

    

    为了实现人工智能的远大目标，强化学习必须包括对抽象状态和时间的世界模型的规划。深度学习在状态抽象方面取得了进展，但时间抽象却很少被使用，尽管基于选项框架已经广泛发展了理论。其中一个原因是可能的选项空间很大，以前提出的选项发现方法没有考虑到选项模型在规划中的使用方式。通常通过提出子任务（例如达到瓶颈状态或最大化除奖励外的感知信号的累积和）来发现选项。解决每个子任务以生成一个选项，然后学习选项的模型并使其可用于规划过程。在大多数以前的研究中，子任务忽略了原始问题上的奖励，而我们提出的子任务使用原始奖励加上基于某个特征的奖励加成。

    To achieve the ambitious goals of artificial intelligence, reinforcement learning must include planning with a model of the world that is abstract in state and time. Deep learning has made progress with state abstraction, but temporal abstraction has rarely been used, despite extensively developed theory based on the options framework. One reason for this is that the space of possible options is immense, and the methods previously proposed for option discovery do not take into account how the option models will be used in planning. Options are typically discovered by posing subsidiary tasks, such as reaching a bottleneck state or maximizing the cumulative sum of a sensory signal other than reward. Each subtask is solved to produce an option, and then a model of the option is learned and made available to the planning process. In most previous work, the subtasks ignore the reward on the original problem, whereas we propose subtasks that use the original reward plus a bonus based on a fe
    
[^121]: DeepKE: 一种基于深度学习的知识提取工具包用于知识库构建

    DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population. (arXiv:2201.03335v6 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2201.03335](http://arxiv.org/abs/2201.03335)

    DeepKE是一个基于深度学习的知识提取工具包，支持复杂的低资源、文档级和多模态场景，可用于自定义数据集和模型来从非结构化数据中提取信息。

    

    我们提出了一个开放源代码和可扩展的知识提取工具包DeepKE，支持知识库构建中的复杂低资源、文档级和多模态场景。DeepKE实现了各种信息提取任务，包括命名实体识别、关系提取和属性提取。通过统一的框架，DeepKE允许开发人员和研究人员根据自己的需求定制数据集和模型，从非结构化数据中提取信息。具体而言，DeepKE不仅为不同任务和场景提供各种功能模块和模型实现，还通过一致的框架组织所有组件，以保持足够的模块化和可扩展性。我们在https://github.com/zjunlp/DeepKE发布了源代码，并提供了适用于初学者的Google Colab教程和全面的文档。此外，我们还在http URL上提供了一个在线系统，用于实时提取各种任务，并提供了演示视频。

    We present an open-source and extensible knowledge extraction toolkit DeepKE, supporting complicated low-resource, document-level and multimodal scenarios in the knowledge base population. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured data according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. We release the source code at GitHub in https://github.com/zjunlp/DeepKE with Google Colab tutorials and comprehensive documents for beginners. Besides, we present an online system in this http URL for real-time extraction of various tasks, and a demo video
    
[^122]: 非支配排序遗传算法II（NSGA-II）的数学运行时间分析

    Mathematical Runtime Analysis for the Non-Dominated Sorting Genetic Algorithm II (NSGA-II). (arXiv:2112.08581v5 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2112.08581](http://arxiv.org/abs/2112.08581)

    这项研究通过数学分析证明了非支配排序遗传算法II（NSGA-II）的运行时间特性，发现当种群规模是帕累托前沿大小的四倍时，NSGA-II具有与其他算法相同的运行时间保证。

    

    非支配排序遗传算法II（NSGA-II）是目前实际应用中最常用的多目标进化算法（MOEA）。然而，与一些简单的MOEA的数学分析相反，NSGA-II至今没有进行过这样的研究。在这项工作中，我们证明了对NSGA-II进行数学运行时间分析是可行的。作为特定结果，我们证明了当种群规模是帕累托前沿大小的四倍时，NSGA-II使用两种经典变异算子和四种不同的父代选择方法与SEMO和GSEMO算法在基本的OneMinMax和LeadingOnesTrailingZeros基准测试上具有相同的渐近运行时间保证。然而，如果种群规模只等于帕累托前沿的大小，则NSGA-II无法高效地计算完整的帕累托前沿：在指数级迭代次数内，种群始终会错失帕累托前沿的一个恒定部分。

    The non-dominated sorting genetic algorithm II (NSGA-II) is the most intensively used multi-objective evolutionary algorithm (MOEA) in real-world applications. However, in contrast to several simple MOEAs analyzed also via mathematical means, no such study exists for the NSGA-II so far. In this work, we show that mathematical runtime analyses are feasible also for the NSGA-II. As particular results, we prove that with a population size four times larger than the size of the Pareto front, the NSGA-II with two classic mutation operators and four different ways to select the parents satisfies the same asymptotic runtime guarantees as the SEMO and GSEMO algorithms on the basic OneMinMax and LeadingOnesTrailingZeros benchmarks. However, if the population size is only equal to the size of the Pareto front, then the NSGA-II cannot efficiently compute the full Pareto front: for an exponential number of iterations, the population will always miss a constant fraction of the Pareto front. Our exp
    
[^123]: 可解释和公平的布尔规则集合生成法

    Interpretable and Fair Boolean Rule Sets via Column Generation. (arXiv:2111.08466v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08466](http://arxiv.org/abs/2111.08466)

    本文提出了一种通过列生成法来生成可解释和公平的布尔规则集合的方法。该方法在兼顾准确性和简单性的平衡方面优于常用的启发式规则挖掘方法，同时考虑了公平性设置，并扩展了模型以满足两种不同的分类公平性度量。使用近似列生成算法处理大规模数据集，并在实验中取得了良好的结果。

    

    本文考虑将布尔规则学习表示为析取范式（DNF，与决策规则集相等）的可解释的分类模型。提出了一个整数规划模型，以最优化分类准确性和规则简单性之间的平衡。同时，考虑了公平性设置，并扩展了模型，包括对两种分类公平性度量的显式约束：机会平等和均衡几率。采用列生成法（CG）高效搜索大量可能规则，而无需启发式规则挖掘。为了处理大规模数据集，提出了一种利用随机化的近似CG算法。与三种最近提出的替代算法相比，在16个数据集中，CG算法在8个数据集上兼顾了准确性和简单性的平衡。当以准确性为最大化目标时，CG算法与专为此目的设计的规则学习器具有相当的竞争力，有时能找到更简单但准确性不减的解决方案。

    This paper considers the learning of Boolean rules in disjunctive normal form (DNF, OR-of-ANDs, equivalent to decision rule sets) as an interpretable model for classification. An integer program is formulated to optimally trade classification accuracy for rule simplicity. We also consider the fairness setting and extend the formulation to include explicit constraints on two different measures of classification parity: equality of opportunity and equalized odds. Column generation (CG) is used to efficiently search over an exponential number of candidate rules without the need for heuristic rule mining. To handle large data sets, we propose an approximate CG algorithm using randomization. Compared to three recently proposed alternatives, the CG algorithm dominates the accuracy-simplicity trade-off in 8 out of 16 data sets. When maximized for accuracy, CG is competitive with rule learners designed for this purpose, sometimes finding significantly simpler solutions that are no less accurat
    
[^124]: KnowPrompt：具有协同优化的知识感知提示调整在关系抽取中的应用

    KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v7 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2104.07650](http://arxiv.org/abs/2104.07650)

    本文提出了一种名为KnowPrompt的知识感知提示调整方法，通过将关系标签中的潜在知识融入到提示构建中，并通过协同优化的方式，提高了关系抽取任务的性能。

    

    最近，对于特定的少样本分类任务，使用提示调整方法取得了有希望的结果。提示调整的核心思想是将文本片段（即模板）插入输入，并将分类任务转化为掩码语言建模问题。然而，对于关系抽取，确定一个合适的提示模板需要领域专业知识，获取合适的标签词是繁琐且耗时的。此外，关系标签之间存在丰富的语义和先验知识，不容忽视。因此，我们的研究着眼于将关系标签之间的知识融入到关系抽取的提示调整中，并提出了一种具有协同优化的知识感知提示调整方法（KnowPrompt）。具体而言，我们利用可学习的虚拟类型词和答案词将关系标签中的潜在知识融入到提示构建中。然后，我们通过结构化约束协同优化它们的表示。

    Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Ex
    
[^125]: 在延迟环境中以非固定马尔可夫策略行动

    Acting in Delayed Environments with Non-Stationary Markov Policies. (arXiv:2101.11992v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2101.11992](http://arxiv.org/abs/2101.11992)

    该论文介绍了在延迟环境中，学习和规划的马尔可夫决策过程(MDP)框架，证明了在延迟执行的情况下，原始状态空间中的非固定马尔可夫策略可以实现最大奖励，提出了一种解决延迟执行任务的非固定Q-learning风格算法。

    

    标准的马尔可夫决策过程(MDP)假设在选择动作后立即执行，但这种假设常常不切实际，会在机器人操纵、云计算和金融等应用中导致灾难性故障。我们引入了一个学习和计划的MDP框架，其中决策者选择的动作需要延迟$m$步才能执行。我们证明了在延迟执行的情况下，原始状态空间中的确定性马尔可夫策略足以实现最大奖励，但需要是非固定的。然而，我们还证明了固定的马尔可夫策略在一般情况下是次优的。因此，我们设计了一种非固定的基于模型的Q-learning风格算法，可以解决延迟执行任务。

    The standard Markov Decision Process (MDP) formulation hinges on the assumption that an action is executed immediately after it was chosen. However, assuming it is often unrealistic and can lead to catastrophic failures in applications such as robotic manipulation, cloud computing, and finance. We introduce a framework for learning and planning in MDPs where the decision-maker commits actions that are executed with a delay of $m$ steps. The brute-force state augmentation baseline where the state is concatenated to the last $m$ committed actions suffers from an exponential complexity in $m$, as we show for policy iteration. We then prove that with execution delay, deterministic Markov policies in the original state-space are sufficient for attaining maximal reward, but need to be non-stationary. As for stationary Markov policies, we show they are sub-optimal in general. Consequently, we devise a non-stationary Q-learning style model-based algorithm that solves delayed execution tasks wi
    

