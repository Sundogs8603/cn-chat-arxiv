# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Key-Locked Rank One Editing for Text-to-Image Personalization.](http://arxiv.org/abs/2305.01644) | Perfusion是一种文本到图像个性化编辑方法，通过锁定新概念与其上位类别的交叉关注键和门控秩-1方法来解决多个难题，包括保持高度保真度、允许创意控制以及多个个性概念的组合。 |
| [^2] | [Differentially Private In-Context Learning.](http://arxiv.org/abs/2305.01639) | 本文提出了DP-ICL，实现了在隐私保证下对新任务的适应性。经过四个基准测试，发现其性能与非私有ICL相当。 |
| [^3] | [Basic syntax from speech: Spontaneous concatenation in unsupervised deep neural networks.](http://arxiv.org/abs/2305.01626) | 该论文提出了一种基于语音的完全无监督的方法，可以直接从原始语音中建立基础语法模型。作者发现，在基于声音的单词记录上训练的卷积神经网络可以自发连接两个或三个单词，并且可以学会将单词嵌入到新的未见过的单词组合中，这是之前未报道的属性，这一发现对我们理解神经网络的学习方式和建立从原始声学输入中的语法及其演化的模型都有重要的意义。 |
| [^4] | [Augmented Electronic Ising Machine as an Effective SAT Solver.](http://arxiv.org/abs/2305.01623) | 本文提出增强型电子伊辛机作为高效的SAT求解器，通过添加适当的立方相互作用和新的启发式算法，使其在解决满足性问题方面具有出色表现。 |
| [^5] | [FlowMap: Path Generation for Automated Vehicles in Open Space Using Traffic Flow.](http://arxiv.org/abs/2305.01622) | 本文提出了一种基于交通流的自动驾驶车辆路径生成框架FlowMap，通过利用其他车辆的轨迹生成路径，解决了在没有定义清晰的“道路”情况下的路径规划问题。 |
| [^6] | [FreeLM: Fine-Tuning-Free Language Model.](http://arxiv.org/abs/2305.01616) | 本文提出了一种免调优策略来训练语言模型，该模型考虑了语言信号和教师信号。通过与大型模型相比，实验证明 FreeLM 模型在多种语言理解任务上表现出了更好的性能。 |
| [^7] | [Finding Neurons in a Haystack: Case Studies with Sparse Probing.](http://arxiv.org/abs/2305.01610) | 本文通过训练$k$-稀疏线性分类器以预测输入特征是否存在，研究了大型语言模型（LLM）内部神经元激活的表示方式；对不同层次的神经元网络的研究表明，早期层利用神经元的稀疏组合来表示多种特征，中间层有特定的神经元表示高级上下文特征，增加规模使特征表示更加稀疏化。 |
| [^8] | [From Words to Code: Harnessing Data for Program Synthesis from Natural Language.](http://arxiv.org/abs/2305.01598) | 该论文利用数据上下文对大型语言模型生成的代码进行语义重排，以生成更优质的程序。 |
| [^9] | [Molecular design method based on novel molecular representation and variational auto-encoder.](http://arxiv.org/abs/2305.01580) | 本论文提出了一种基于SELFIES分子表示和改进后的变分自编码器模型的分子设计方法，用于生成更多种类的分子。 |
| [^10] | [Discern and Answer: Mitigating the Impact of Misinformation in Retrieval-Augmented Models with Discriminators.](http://arxiv.org/abs/2305.01579) | 本文研究了现有检索增强语言模型假设所有检索信息都是正确的假设的问题，在实际应用中可能存在虚假信息导致冲突的情况下，提出了通过精细调整鉴别器和提示鉴别能力引出鲁棒性的方法，这显著改善了模型在知识冲突下的效果；同时提供了关于交替精细调整模型和上下文学习的新的结论。 |
| [^11] | [H2CGL: Modeling Dynamics of Citation Network for Impact Prediction.](http://arxiv.org/abs/2305.01572) | 本文提出了一种名为H2CGL的新颖图神经网络模型，用于引文网络的动态建模和影响预测。该模型通过分层和异构的方式记录目标论文年度动态信息，并优先考虑高被引论文和参考文献、引文、目标论文之间的关系。它采用加权 GIN 来捕捉异构子图的动态，同时采用对比学习来提高模型效果。 |
| [^12] | [Type-enhanced Ensemble Triple Representation via Triple-aware Attention for Cross-lingual Entity Alignment.](http://arxiv.org/abs/2305.01556) | 提出了一种新的跨语言实体对齐框架TTEA，该框架考虑了三元组特征和实体角色多样性，利用三元组感知注意力构建增强型三元组集成表示，通过具体性感知控制了信息传播过程中的噪声影响。 |
| [^13] | [How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?.](http://arxiv.org/abs/2305.01555) | 本文通过使用GPT-3.5模型在少样本关系抽取中，实现在四个不同数据集上的新的最优性能，并提出了与任务相关的指导说明和约束模式下的数据生成方法。 |
| [^14] | [FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information.](http://arxiv.org/abs/2305.01528) | 本研究介绍了一份包含真实游戏状态信息的Dungeons & Dragons实际游戏数据集FIREBALL，它可以改善自然语言生成的质量。此外，LLMs可以使用FIREBALL中的游戏状态信息来生成更高质量的游戏回合。 |
| [^15] | [Huatuo-26M, a Large-scale Chinese Medical QA Dataset.](http://arxiv.org/abs/2305.01526) | 我们发布了一份最大的中医问答数据集，现有模型的表现远低于预期。此数据集可以用于其他QA数据集的零-shot学习并用作检索增强生成（RAG）的外部知识，在预训练语言模型时代仍然具有挑战性。 |
| [^16] | [Empowering AI drug discovery with explicit and implicit knowledge.](http://arxiv.org/abs/2305.01523) | DeepEIK是一个统一的深度学习框架，利用显式和隐式知识进行AI药物发现，通过特征融合和注意力机制来提高模型预测准确性，并在多个任务上显著优于现有方法。 |
| [^17] | [Defining Replicability of Prediction Rules.](http://arxiv.org/abs/2305.01518) | 本文提出了一种定义预测规则可复制性的方法，通过多代理人框架定义可复制性，旨在为机器学习中更系统的可复制性评估提供指导。 |
| [^18] | [Discovering the Effectiveness of Pre-Training in a Large-scale Car-sharing Platform.](http://arxiv.org/abs/2305.01506) | 本文针对大规模共享汽车平台上的汽车图像分析任务，通过分析实验验证了预训练技术对于提高模型性能的有效性。 |
| [^19] | [Beyond Classification: Financial Reasoning in State-of-the-Art Language Models.](http://arxiv.org/abs/2305.01505) | 本研究探讨了大语言模型在财务推理领域的潜在应用，对任务制定、数据生成、提示方法和评估能力等方面进行了详细研究，最终在各种数据集大小上对参数规模为2.8B至13B的各种GPT变体进行基准测试。 |
| [^20] | [Towards Summarizing Multiple Documents with Hierarchical Relationships.](http://arxiv.org/abs/2305.01498) | 提出了一个新的数据集PeerSum用于生成科学论文的元评论，源文档具有显式层次结构的丰富文档间关系，提出了一种用于元评论生成的关系感知多任务模型Rammer。 |
| [^21] | [ARBEx: Attentive Feature Extraction with Reliability Balancing for Robust Facial Expression Learning.](http://arxiv.org/abs/2305.01486) | 本论文提出了一个名为ARBEx的框架，它采用了可靠性平衡方法来应对面部表情学习任务中的数据偏差和不确定性。该框架还引入了可学习的锚点和多头自注意机制，并在多个公共数据集上取得了有效性验证。 |
| [^22] | [Great Models Think Alike: Improving Model Reliability via Inter-Model Latent Agreement.](http://arxiv.org/abs/2305.01481) | 通过测量模型和一个基础模型的潜在空间之间的一致性来提高机器学习模型的可靠性。 |
| [^23] | [Mixed-Integer Optimal Control via Reinforcement Learning: A Case Study on Hybrid Vehicle Energy Management.](http://arxiv.org/abs/2305.01461) | 本研究提出了一种新的混合整数最优控制问题的强化学习算法，可同时处理连续和离散控制变量， 并在混合动力车辆能源管理问题上优于现有方法和强化学习算法。 |
| [^24] | [Forecast reconciliation for vaccine supply chain optimization.](http://arxiv.org/abs/2305.01455) | 本文尝试通过层次时间序列来预测疫苗销售数据，并使用协调方法解决了不同层次预测不一致的问题，结果表明最小迹和加权最小二乘与结构缩放的协调方法效果最佳，提高了预测的一致性并减少预测误差。 |
| [^25] | [Uncertain Machine Ethical Decisions Using Hypothetical Retrospection.](http://arxiv.org/abs/2305.01424) | 本文提出了一种新的机器伦理推理方法，使用假想回顾论证程序，允许使用各种哲学理论进行伦理推理，可以考虑概率和不确定性，应用于一个自主图书馆系统用例中进行测试。 |
| [^26] | [Get Back Here: Robust Imitation by Return-to-Distribution Planning.](http://arxiv.org/abs/2305.01400) | 本文提出的算法POIR将行为克隆和规划器相结合，解决了模仿学习中分布偏移的问题，模型在机器人操纵任务中表现出强大的鲁棒性。 |
| [^27] | [Oil Spill Segmentation using Deep Encoder-Decoder models.](http://arxiv.org/abs/2305.01386) | 本研究测试了使用深度编码-解码模型进行油污分割的可行性，并在高维卫星合成孔径雷达图像数据上比较了多种分割模型的结果。最好的表现模型是使用ResNet-50编码器和DeepLabV3+解码器，能够实现64.868%的平均交集联合（IoU）和61.549%的“油污”类IoU。 |
| [^28] | [Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees.](http://arxiv.org/abs/2305.01381) | 本文提出了一种基于LTL规范的无模型强化学习方法，该方法结合乘积MDP、奖励结构和折扣机制有效地学习并优化未知随机系统最大化满足LTL规范的概率的最优策略。 |
| [^29] | [An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework.](http://arxiv.org/abs/2305.01322) | 该论文关注强化学习中的探索研究，提出了一个能够自主管理探索策略的多模式智能体非单体探索方法，并通过实验结果展示了该方法的优越性能。 |
| [^30] | [Transfer Visual Prompt Generator across LLMs.](http://arxiv.org/abs/2305.01278) | 本论文提出将已有的轻量化视觉提示发生器连接到视觉-语言LLM以减少资源消耗的方法，并提出了跨不同大小和类型的LLMs的VPG转移方案VPGTrans，该方案在VQA和NLVR2任务中表现优秀。 |
| [^31] | [Sim2real and Digital Twins in Autonomous Driving: A Survey.](http://arxiv.org/abs/2305.01263) | 本文综述了自主驾驶中的sim2real和数字孪生方法，它们分别解决了从模拟到现实的知识转移和从真实世界数据中学习以提高仿真精度的问题，但也存在各自的优缺点和限制。 |
| [^32] | [DreamPaint: Few-Shot Inpainting of E-Commerce Items for Virtual Try-On without 3D Modeling.](http://arxiv.org/abs/2305.01257) | DreamPaint是一个无需3D建模的电商商品试穿Few-Shot修复框架，可以在任意用户环境下对商品图像进行修复，具有较高的修复精度。 |
| [^33] | [Reverse Engineering of Temporal Queries Mediated by LTL Ontologies.](http://arxiv.org/abs/2305.01248) | 本文研究了基于时间戳数据的LTL正片段制定的查询反向工程问题，旨在构建一个将给定答案与非答案分开的查询语言，并考虑了LTL本体介导的情况。 |
| [^34] | [DRPT: Disentangled and Recurrent Prompt Tuning for Compositional Zero-Shot Learning.](http://arxiv.org/abs/2305.01239) | 该论文提出了一种名为DRPT的算法，通过分离与循环提示调整的方式来优化记号参数，使得在组合零样本学习中所采用的视觉-语言模型能够更有效地进行识别。实验表明，DRPT在三个基准测试数据集上取得了最先进的性能表现。 |
| [^35] | [Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models.](http://arxiv.org/abs/2305.01219) | 本研究提出一种新颖有效的“ProAttack”方法来执行干净标签的后门攻击，使用的是提示本身作为触发器。该方法不需要外部触发器，并确保毒瘤数据的标注正确，提高了后门攻击的隐蔽性，相比于现有的后门攻击方法有显著提升。 |
| [^36] | [MultiLegalSBD: A Multilingual Legal Sentence Boundary Detection Dataset.](http://arxiv.org/abs/2305.01211) | 本文介绍了一个多语言法律句子边界检测数据集MultiLegalSBD，包括6种语言的130,000个注释句子。在该数据集上，现有的SBD模型的表现不佳。作者训练了基于CRF、BiLSTM-CRF和transformers的单语和多语模型，并展示了在该领域中最先进的性能。他们的多语模型在零-shot测试中优于所有基线。 |
| [^37] | [Chronosymbolic Learning: Efficient CHC Solving with Symbolic Reasoning and Inductive Learning.](http://arxiv.org/abs/2305.01206) | Chronosymbolic Learning是一个简单而有效的框架，将符号推理和数据驱动方法相结合，用于高效地解决CHC系统。实验证明它在288个基准测试上表现出优异的结果，包括许多具有非线性整数算术的实例。 |
| [^38] | [Deconstructing Student Perceptions of Generative AI (GenAI) through an Expectancy Value Theory (EVT)-based Instrument.](http://arxiv.org/abs/2305.01186) | 本研究基于EVT理论开发了一份问卷，调查了405名学生对于在高等教育中使用生成式人工智能的看法和意愿。结果显示，学生认为生成式人工智能具有较高的使用价值和较小的感知成本，但在推广应用时需要考虑潜在的长期影响和伦理问题。 |
| [^39] | [Complex Logical Reasoning over Knowledge Graphs using Large Language Models.](http://arxiv.org/abs/2305.01157) | 本文提出了一种使用大型语言模型的解耦方法，将复杂的知识图谱推理形式化为上下文知识图搜索和抽象逻辑查询推理的组合，与现有方法相比，它在多个逻辑查询结构的标准基准数据集上都表现出更好的性能，并且在更高复杂性的查询中获得了显着的性能提升。 |
| [^40] | [Ripple Knowledge Graph Convolutional Networks For Recommendation Systems.](http://arxiv.org/abs/2305.01147) | 本文介绍了一种基于知识图谱的深度学习模型RKGCN，它能够动态分析用户的偏好并推荐出合适的物品。该模型在包括电影、书籍和音乐在内的三个真实世界的数据集上比5个基准模型表现更好。 |
| [^41] | [Analysis of different temporal graph neural network configurations on dynamic graphs.](http://arxiv.org/abs/2305.01128) | 本文针对动态图上的时空依赖结构进行了分析，并比较了不同TGN模型在预测任务上的有效性。通过广泛的消融实验，探讨了不同设计选择对预测准确性的影响。 |
| [^42] | [Human adaptation to adaptive machines converges to game-theoretic equilibria.](http://arxiv.org/abs/2305.01124) | 本研究测试了机器学习算法与人类进行博弈，发现算法会收敛于博弈论均衡，同时自适应机器的存在会导致人类行为向更加剥削性策略转变。 |
| [^43] | [CSP: Self-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Representations.](http://arxiv.org/abs/2305.01118) | CSP提出了一个自监督对比学习框架，通过利用地理信息，学习地理位置的有效表示，进一步提高了模型在大规模地理标记图像上的表现。 |
| [^44] | [Local and Global Contextual Features Fusion for Pedestrian Intention Prediction.](http://arxiv.org/abs/2305.01111) | 本研究提出了基于局部和全局上下文特征融合的行人意向预测方法，通过分析行人和交通上下文的视觉特征来提高自动驾驶汽车在道路上的安全性。 |
| [^45] | [A Novel Model for Driver Lane Change Prediction in Cooperative Adaptive Cruise Control Systems.](http://arxiv.org/abs/2305.01096) | 本文研究了合作自适应巡航控制系统中驾驶员变道预测问题。使用车到车通信技术，提供了多个车辆的加速度信息，通过LSTM模型的学习，当周围车辆数量增加时，准确性有所提高。 |
| [^46] | [LSTM-based Preceding Vehicle Behaviour Prediction during Aggressive Lane Change for ACC Application.](http://arxiv.org/abs/2305.01095) | 该论文提出了一种基于LSTM的ACC系统，可以学习过去的驾驶经验，适应和预测新情况，并且在模拟驾驶环境中表现良好。 |
| [^47] | [Computing Expected Motif Counts for Exchangeable Graph Generative Models.](http://arxiv.org/abs/2305.01089) | 本文提出了一种可扩展的估计过程，用于生成混合模型中期望的模体计数。 |
| [^48] | [AI & Blockchain as sustainable teaching and learning tools to cope with the 4IR.](http://arxiv.org/abs/2305.01088) | 本论文回顾了现有的AI和区块链在教育领域的研究，探讨了这些技术可以提高个性化学习、安全认证和分散式学习网络的潜力和挑战。此外，本文提出了一个模型，将AI和区块链整合到可持续的教育实践中。总之，AI和区块链技术有望成为可持续教学和学习工具。 |
| [^49] | [Expertise Trees Resolve Knowledge Limitations in Collective Decision-Making.](http://arxiv.org/abs/2305.01063) | 本研究通过引入专业知识树算法，解决了集体决策中专业知识水平不同的问题，并在多个问题上进行了验证。 |
| [^50] | [Venn Diagram Multi-label Class Interpretation of Diabetic Foot Ulcer with Color and Sharpness Enhancement.](http://arxiv.org/abs/2305.01044) | 本文提出了一种基于Venn图解释的多标签分类算法，通过不同的图像增强策略，将四个类别缩减为两个，有效提高了糖尿病足潰疡的多类别分类准确度。 |
| [^51] | [CLIP-S$^4$: Language-Guided Self-Supervised Semantic Segmentation.](http://arxiv.org/abs/2305.01040) | CLIP-S$^4$是一种无需像素级标注和预定义类别的语言引导自监督语义分割方法，利用自监督像素表示学习和视觉-语言模型，支持各种语义分割任务。 |
| [^52] | [Model-agnostic Measure of Generalization Difficulty.](http://arxiv.org/abs/2305.01034) | 该论文提出了第一个无特定模型的、量化机器学习测试泛化难度的方法——归纳偏差复杂度度量。该方法量化了在任务上良好泛化所需的总信息量与数据提供的信息量之差，通常需要在许多维度上泛化的任务比涉及更少维度但要求更多细节的任务要困难得多。 |
| [^53] | [Evaluating statistical language models as pragmatic reasoners.](http://arxiv.org/abs/2305.01020) | 本文评估了大型语言模型推断语用话语的能力。作者通过表述有关等级形容词“强”的门槛估计来测试 LLM 的性能，并发现 LLM 可以生成具有上下文依赖性、类人的分布，但在组合方面存在困难。 |
| [^54] | [Ternary Instantaneous Noise-based Logic.](http://arxiv.org/abs/2305.00984) | 本文提出了一种三元逻辑系统，其中包含不确定的比特值和不存在的比特（真空态），与标准的二元逻辑系统相比有着显著的优势，且可以在不改变二进制算法的情况下使用。 |
| [^55] | [Two-phase Dual COPOD Method for Anomaly Detection in Industrial Control System.](http://arxiv.org/abs/2305.00982) | 本文提出了一个双阶段基于双Copula的离群检测方法，该方法具有优异的透明度、可解释性和计算效率，并在实际工控系统数据集上具有卓越的性能。 |
| [^56] | [SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation.](http://arxiv.org/abs/2305.00795) | SelfDocSeg提出了一种完全基于视觉的自我监督文档分割方法，通过生成伪布局训练图像编码器学习文档对象的表示和定位，克服了标注数据稀缺的挑战。 |
| [^57] | [Representations and Exploration for Deep Reinforcement Learning using Singular Value Decomposition.](http://arxiv.org/abs/2305.00654) | 本文提出了一种基于奇异值分解的自动表征学习模型，可以获得保留转换结构的表示形式并捕捉状态访问的相对频率。该方法不需要转移矩阵，可以利用深度网络，适用于部分可观察领域，并且在多任务设置中表现良好。 |
| [^58] | [Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding.](http://arxiv.org/abs/2305.00633) | 本论文提出了一种通过自我评估引导解码提高推理的方法，使用经过校准的自动标准探索推理搜索空间，使搜索能够产生更高质量的最终预测结果；使用自我评估引导的随机束搜索在产生推理链的质量和多样性之间平衡权衡，适应多数投票，并且可以准确判断逻辑错误，提高一致性和鲁棒性。 |
| [^59] | [Rubik's Optical Neural Networks: Multi-task Learning with Physics-aware Rotation Architecture.](http://arxiv.org/abs/2304.12985) | RubikONNs利用光学系统的物理特性，通过旋转硬件编码多个前馈函数，为ONNs实现高效的多任务学习提供了新的策略。 |
| [^60] | [Evaluating Adversarial Robustness on Document Image Classification.](http://arxiv.org/abs/2304.12486) | 本文对文档图像分类任务中的对抗性攻击进行了研究和评估，通过对ResNet50和EfficientNetB0模型架构进行对抗训练、JPEG输入压缩和灰度输入转换等方法，提高了模型的鲁棒性。 |
| [^61] | [DocParser: End-to-end OCR-free Information Extraction from Visually Rich Documents.](http://arxiv.org/abs/2304.12484) | DocParser是一种无OCR的端到端信息提取模型，可更好地提取判别性字符特征，并在视觉丰富的文档数据集上实现最新的结果。 |
| [^62] | [SDVRF: Sparse-to-Dense Voxel Region Fusion for Multi-modal 3D Object Detection.](http://arxiv.org/abs/2304.08304) | 提出了一种新的稀疏到密集的体素区域加强融合方法，通过动态投影每个体素内部的稀疏局部点云获得体素区域，更好地对齐和避免背景噪声问题，并通过多尺度融合方法极大地提高了三维物体检测性能。 |
| [^63] | [Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark.](http://arxiv.org/abs/2304.03279) | 本文介绍了 MACHIAVELLI 基准测试，用于衡量人工智能代理是否表现出马基雅维利行为，发现了最大化奖励和行为的道德性之间存在权衡，并探索了基于语言模型的方法来减轻这种权衡。 |
| [^64] | [Fourier Analysis Meets Runtime Analysis: Precise Runtimes on Plateaus.](http://arxiv.org/abs/2302.08021) | 本文提出了一种基于傅里叶分析的新方法，用于分析进化算法在高原上消耗的时间，通过算法在针尖问题和新基准问题上的应用研究，确定了最佳变异率，并证明了$(1+1)$进化算法相对于一般随机搜索启发式算法在高原问题上具有惊人的效率。 |
| [^65] | [Language Model Analysis for Ontology Subsumption Inference.](http://arxiv.org/abs/2302.06761) | 本文研究了语言模型对本体子类推断的理解能力，提出了一套涉及原子概念和复合概念的推理任务，并证明语言模型对子类推断背景知识的记忆相对较少，但在给定少量样本的情况下可显著提高准确率。 |
| [^66] | [Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition.](http://arxiv.org/abs/2302.05881) | 本文提出了一种新的方法框架GCDTC，利用数值先验和广义CP分解实现了更高的低秩张量补全精度；同时介绍了一个算法SPTC，作为该框架的一个实现。在实验中，该方法表现出比现有技术更好的性能。 |
| [^67] | [Risk-Sensitive Reinforcement Learning with Exponential Criteria.](http://arxiv.org/abs/2212.09010) | 本文介绍了一种风险敏感的强化学习算法，使用指数判据来提高其系统抗干扰性和实用性。作者进行了在模拟和实际机器人上的实验验证，表明该算法能够有效地提高样本效率和执行效果。 |
| [^68] | [Differentially Private Learning with Per-Sample Adaptive Clipping.](http://arxiv.org/abs/2212.00328) | 本文提出了一种差分隐私每个样本自适应裁剪算法DP-PSAC，该算法采用非单调自适应权重函数，根据梯度的历史敏感性自适应裁剪每个样本的梯度幅度，提高了模型的实用性，且保证了隐私。 |
| [^69] | [Frustratingly Easy Label Projection for Cross-lingual Transfer.](http://arxiv.org/abs/2211.15613) | 本文通过一项广泛的实证研究，对57种语言和三个任务下的跨语言转移进行了研究，并发现优化后的标记-翻译法比传统注释投影方法更有效。 |
| [^70] | [Interpretable Scientific Discovery with Symbolic Regression: A Review.](http://arxiv.org/abs/2211.10873) | 符号回归作为一种机器学习方法日益流行，可以从数据中学习简洁且可解释的数学表达式，本综述全面概述了该方法并讨论了其优点和局限性。 |
| [^71] | [On Web-based Visual Corpus Construction for Visual Document Understanding.](http://arxiv.org/abs/2211.03256) | 这篇论文提出了一个基于Web的视觉语料库构建工具（Webvicob），用于从Wikipedia HTML转储文件中构建大规模的、多语言的视觉语料库，通过数据集的使用可以提升视觉文档理解模型的性能。 |
| [^72] | [Generating Executable Action Plans with Environmentally-Aware Language Models.](http://arxiv.org/abs/2210.04964) | 本文提出了一种带环境感知的语言模型生成可执行的动作计划的方法，通过集成环境对象和对象关系作为额外输入，设计了新颖的评分函数，生成的可执行计划相对于传统的LLM方法有更高成功率。 |
| [^73] | [Online 2-stage Stable Matching.](http://arxiv.org/abs/2207.02057) | 该论文研究了一个在线2阶段的稳定匹配问题，提出了能够最优解决该问题的算法。 |
| [^74] | [Know your audience: specializing grounded language models with listener subtraction.](http://arxiv.org/abs/2206.08349) | 本文介绍了一种利用多智能体图像参照游戏自适应不同听众的目标任务描述的方法，并通过微调 CLIP 视觉编码器和大型语言模型之间的适配器，在适应听众的语言上下文的情况下进行了自然语言专业化。 |
| [^75] | [Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning.](http://arxiv.org/abs/2206.04384) | 本论文提出了一种离线强化学习的方法，通过构建简单离散的世界模型（Value Memory Graph，VMG）来抽象原始复杂环境，从而简化策略学习。 |
| [^76] | [WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation.](http://arxiv.org/abs/2109.14196) | 本研究提出了一种基于网络图像辅助的领域泛化方法，利用网络抓取的大量现实图像数据集来增加数据的多样性，提高模型泛化能力，在未见过的领域中表现良好。 |

# 详细

[^1]: 锁定关键排序进行文本到图像个性化编辑

    Key-Locked Rank One Editing for Text-to-Image Personalization. (arXiv:2305.01644v1 [cs.CV])

    [http://arxiv.org/abs/2305.01644](http://arxiv.org/abs/2305.01644)

    Perfusion是一种文本到图像个性化编辑方法，通过锁定新概念与其上位类别的交叉关注键和门控秩-1方法来解决多个难题，包括保持高度保真度、允许创意控制以及多个个性概念的组合。

    

    文本到图像模型（T2I）通过自然语言引导创作过程，提供了新的灵活性水平。然而，将这些模型个性化以与用户提供的视觉概念相一致仍然是一个具有挑战性的问题。T2I 个性化的任务面临多个困难挑战，例如在允许创意控制的同时保持高度视觉保真度，将多个个性化概念组合在单个图像中，以及保持小型模型尺寸。我们提出了一种名为“Perfusion”的 T2I 个性化方法，它使用基础 T2I 模型的动态秩-1更新来解决这些挑战。Perfusion 通过将新概念的交叉关注键“锁定”到其上位类别来避免过度拟合。此外，我们开发了一种门控秩-1方法，使我们能够在推理时间内控制所学概念的影响，并组合多个概念。这允许运行时有效地平衡视觉保真度和文本对齐。

    Text-to-image models (T2I) offer a new level of flexibility by allowing users to guide the creative process through natural language. However, personalizing these models to align with user-provided visual concepts remains a challenging problem. The task of T2I personalization poses multiple hard challenges, such as maintaining high visual fidelity while allowing creative control, combining multiple personalized concepts in a single image, and keeping a small model size. We present Perfusion, a T2I personalization method that addresses these challenges using dynamic rank-1 updates to the underlying T2I model. Perfusion avoids overfitting by introducing a new mechanism that "locks" new concepts' cross-attention Keys to their superordinate category. Additionally, we develop a gated rank-1 approach that enables us to control the influence of a learned concept during inference time and to combine multiple concepts. This allows runtime-efficient balancing of visual-fidelity and textual-align
    
[^2]: 差分隐私下的上下文学习

    Differentially Private In-Context Learning. (arXiv:2305.01639v1 [cs.LG])

    [http://arxiv.org/abs/2305.01639](http://arxiv.org/abs/2305.01639)

    本文提出了DP-ICL，实现了在隐私保证下对新任务的适应性。经过四个基准测试，发现其性能与非私有ICL相当。

    

    在部署大型语言模型（LLM）时，一个重要的问题是如何使用私有数据增强LLM。我们提出了"DP-ICL"来实现对新任务的适应性，同时保持隐私保证。DP-ICL通过使用"report-noisy-max"机制在示例集合上建立嘈杂一致性来进行私有推断。我们在四个基准测试上评估了DP-ICL，发现其与非私有ICL相比具有可比性的性能(<2%降级)。

    An important question in deploying large language models (LLMs) is how to augment LLMs with private data. We propose Differentially Private In-context Learning (DP-ICL) to enable LLMs to adapt to new tasks while maintaining privacy guarantees. DP-ICL performs private inference by establishing noisy consensus over an ensemble of exemplars using the Report-Noisy-Max mechanism. We evaluate DP-ICL on four benchmarks and find that it achieves comparable performance (<2\% degradation) with non-private ICL.
    
[^3]: 基于语音的基础语法：自发联接的自监督深度神经网络

    Basic syntax from speech: Spontaneous concatenation in unsupervised deep neural networks. (arXiv:2305.01626v1 [cs.CL])

    [http://arxiv.org/abs/2305.01626](http://arxiv.org/abs/2305.01626)

    该论文提出了一种基于语音的完全无监督的方法，可以直接从原始语音中建立基础语法模型。作者发现，在基于声音的单词记录上训练的卷积神经网络可以自发连接两个或三个单词，并且可以学会将单词嵌入到新的未见过的单词组合中，这是之前未报道的属性，这一发现对我们理解神经网络的学习方式和建立从原始声学输入中的语法及其演化的模型都有重要的意义。

    

    语法的计算模型主要基于文本。本文提出了一种完全无监督的方法，可以直接从原始语音中建立基础语法模型。我们重点研究了最普遍和基本的语法特性之一——联接。我们介绍了自发联接现象：卷积神经网络(CNN)在个别单词的声学记录上训练时，开始产生输出，这些输出将两个甚至三个单词连接在一起，而不会接触到具有多个单词的输入数据。此外，训练两个单词的网络可以学习将单词嵌入到新的未见过的单词组合中。据我们所知，这是在生成对抗网络环境下训练的原始语音CNN以前未报道的属性，它不仅对我们理解这些体系结构的学习方式有影响，还对建立从原始声学输入中的语法及其演化的模型有影响。

    Computational models of syntax are predominantly text-based. Here we propose that basic syntax can be modeled directly from raw speech in a fully unsupervised way. We focus on one of the most ubiquitous and basic properties of syntax -- concatenation. We introduce spontaneous concatenation: a phenomenon where convolutional neural networks (CNNs) trained on acoustic recordings of individual words start generating outputs with two or even three words concatenated without ever accessing data with multiple words in the input. Additionally, networks trained on two words learn to embed words into novel unobserved word combinations. To our knowledge, this is a previously unreported property of CNNs trained on raw speech in the Generative Adversarial Network setting and has implications both for our understanding of how these architectures learn as well as for modeling syntax and its evolution from raw acoustic inputs.
    
[^4]: 增强型电子伊辛机作为高效的SAT求解器

    Augmented Electronic Ising Machine as an Effective SAT Solver. (arXiv:2305.01623v1 [cs.AI])

    [http://arxiv.org/abs/2305.01623](http://arxiv.org/abs/2305.01623)

    本文提出增强型电子伊辛机作为高效的SAT求解器，通过添加适当的立方相互作用和新的启发式算法，使其在解决满足性问题方面具有出色表现。

    

    随着传统冯诺依曼系统改进的减速，越来越多的关注被付诸于新兴范例，如伊辛机。它们对于NP完全优化问题有着非常不同的方法。伊辛机已经展现出在解决像MaxCut这样的二元优化问题方面的巨大潜力。在本文中，我们针对满足性(SAT)问题对这些系统进行了分析。我们证明，在3-SAT的情况下，一个基本的架构无法产生有意义的加速，这在很大程度上要归功于传统SAT求解器不断取得的进步。尽管如此，细致的分析将部分失败归因于缺乏两个重要组件：立方相互作用和高效的随机启发式算法。为了克服这些限制，我们在最先进的伊辛机上添加了适当的立方体相互作用结构支持。更重要的是，我们提出了一种新的语义感知淬火进度表，使得搜索空间导航变得更加高效。

    With the slowdown of improvement in conventional von Neumann systems, increasing attention is paid to novel paradigms such as Ising machines. They have very different approach to NP-complete optimization problems. Ising machines have shown great potential in solving binary optimization problems like MaxCut. In this paper, we present an analysis of these systems in satisfiability (SAT) problems. We demonstrate that, in the case of 3-SAT, a basic architecture fails to produce meaningful acceleration, thanks in no small part to the relentless progress made in conventional SAT solvers. Nevertheless, careful analysis attributes part of the failure to the lack of two important components: cubic interactions and efficient randomization heuristics. To overcome these limitations, we add proper architectural support for cubic interaction on a state-of-the-art Ising machine. More importantly, we propose a novel semantic-aware annealing schedule that makes the search-space navigation much more eff
    
[^5]: FlowMap：使用交通流生成开放空间自动驾驶车辆路径的方法

    FlowMap: Path Generation for Automated Vehicles in Open Space Using Traffic Flow. (arXiv:2305.01622v1 [cs.RO])

    [http://arxiv.org/abs/2305.01622](http://arxiv.org/abs/2305.01622)

    本文提出了一种基于交通流的自动驾驶车辆路径生成框架FlowMap，通过利用其他车辆的轨迹生成路径，解决了在没有定义清晰的“道路”情况下的路径规划问题。

    

    许多文献探讨了通过融合各种传感器输入（如激光雷达点云和相机图像）使用深度神经网络感知道路结构。利用最新的神经结构（如变压器）和鸟瞰图（BEV）表示法，道路认知精度不断提高。然而，如何为自动驾驶车辆认知“道路”，特别是在没有定义清晰的“道路”的情况下，仍然是一个悬而未决的问题。本文提出了一种基于交通流的自动驾驶车辆路径生成框架FlowMap。FlowMap通过扩展我们先前的工作RoadMap来构建。

    There is extensive literature on perceiving road structures by fusing various sensor inputs such as lidar point clouds and camera images using deep neural nets. Leveraging the latest advance of neural architects (such as transformers) and bird-eye-view (BEV) representation, the road cognition accuracy keeps improving. However, how to cognize the ``road'' for automated vehicles where there is no well-defined ``roads'' remains an open problem. For example, how to find paths inside intersections without HD maps is hard since there is neither an explicit definition for ``roads'' nor explicit features such as lane markings. The idea of this paper comes from a proverb: it becomes a way when people walk on it. Although there are no ``roads'' from sensor readings, there are ``roads'' from tracks of other vehicles. In this paper, we propose FlowMap, a path generation framework for automated vehicles based on traffic flows. FlowMap is built by extending our previous work RoadMap, a light-weight 
    
[^6]: FreeLM: 免调优语言模型

    FreeLM: Fine-Tuning-Free Language Model. (arXiv:2305.01616v1 [cs.CL])

    [http://arxiv.org/abs/2305.01616](http://arxiv.org/abs/2305.01616)

    本文提出了一种免调优策略来训练语言模型，该模型考虑了语言信号和教师信号。通过与大型模型相比，实验证明 FreeLM 模型在多种语言理解任务上表现出了更好的性能。

    

    预训练语言模型 (PLMs) 在 NLP 任务中取得了显著的成功。尽管如此，主流的解决方案仍然遵循预训练后调优的范式，这既带来了高昂的部署成本，也降低了训练效率。然而，调优特定任务是必要的，因为 PLMs 仅在大型原始数据的语言信号下进行了预训练。本文提出了一种新颖的无调优策略，即同时考虑语言信号和教师信号。教师信号是下游任务的一个抽象表示，以统一命题格式提供。我们的 FreeLM 模型在交互式地使用语言信号和强任务感知的教师信号进行训练后表现出了强大的泛化和鲁棒性。在实验中，FreeLM 在多种语言理解任务上优于大型模型，如 GPT-3 和 InstructGPT。与这些模型的 175B 参数相比，FreeLM 更小，只有 0.3B 参数。

    Pre-trained language models (PLMs) have achieved remarkable success in NLP tasks. Despite the great success, mainstream solutions largely follow the pre-training then finetuning paradigm, which brings in both high deployment costs and low training efficiency. Nevertheless, fine-tuning on a specific task is essential because PLMs are only pre-trained with language signal from large raw data. In this paper, we propose a novel fine-tuning-free strategy for language models, to consider both language signal and teacher signal. Teacher signal is an abstraction of a battery of downstream tasks, provided in a unified proposition format. Trained with both language and strong task-aware teacher signals in an interactive manner, our FreeLM model demonstrates strong generalization and robustness. FreeLM outperforms large models e.g., GPT-3 and InstructGPT, on a range of language understanding tasks in experiments. FreeLM is much smaller with 0.3B parameters, compared to 175B in these models.
    
[^7]: 在稀疏探测中寻找海量神经元: 实例研究

    Finding Neurons in a Haystack: Case Studies with Sparse Probing. (arXiv:2305.01610v1 [cs.LG])

    [http://arxiv.org/abs/2305.01610](http://arxiv.org/abs/2305.01610)

    本文通过训练$k$-稀疏线性分类器以预测输入特征是否存在，研究了大型语言模型（LLM）内部神经元激活的表示方式；对不同层次的神经元网络的研究表明，早期层利用神经元的稀疏组合来表示多种特征，中间层有特定的神经元表示高级上下文特征，增加规模使特征表示更加稀疏化。

    

    尽管大型语言模型(LLM)的应用和部署迅速增加，但这些模型的内部计算仍然不透明且难以理解。本文旨在了解高级可解释特征在LLM内部神经元激活中的表示方式。我们使用$k$-稀疏线性分类器(探针)来训练这些内部激活值，并预测输入的特征是否存在；通过改变$k$值，我们研究了学习表示的稀疏性以及随着模型规模的变化而变化的情况。当$k=1$时，我们定位某个特定特征非常相关的单个神经元，并进行了大量案例研究，以说明LLM的一般性质。特别是，我们展示了早期层利用神经元的稀疏组合来表示许多特征，中间层似乎具有专门的神经元来表示更高级的上下文特征，而增加的规模则导致表示稀疏性增加。

    Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train $k$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of $k$ we study the sparsity of learned representations and how this varies with model scale. With $k=1$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to
    
[^8]: 从自然语言到代码：利用数据进行程序综合。

    From Words to Code: Harnessing Data for Program Synthesis from Natural Language. (arXiv:2305.01598v1 [cs.DB])

    [http://arxiv.org/abs/2305.01598](http://arxiv.org/abs/2305.01598)

    该论文利用数据上下文对大型语言模型生成的代码进行语义重排，以生成更优质的程序。

    

    创建正确操作数据的程序是一项艰巨的任务，因为底层的编程语言和 API 对于许多不熟练的程序员来说学习起来很具有挑战性。大型语言模型展示了从自然语言生成代码的巨大潜力，但在数据操作领域，除了所需任务的自然语言描述外，我们还有数据集作为该任务的上下文。现有的方法仅通过将输入数据中的相关信息添加到发送给 LLM 的提示中的方式有限地利用数据上下文。在这项工作中，我们利用可用的输入数据来执行 LLM 生成的候选程序并收集它们的输出。我们引入了语义重排技术，该技术基于程序输出的三个信号（a）语义过滤和良好格式得分调整：程序是否符合语义和格式， (b) 输入-输出示例得分: 程序是否为输入数据提供了输出。, (c) 结构与规范得分：程序是否遵循API的结构和规范，以重排 LLM 生成的程序。

    Creating programs to correctly manipulate data is a difficult task, as the underlying programming languages and APIs can be challenging to learn for many users who are not skilled programmers. Large language models (LLMs) demonstrate remarkable potential for generating code from natural language, but in the data manipulation domain, apart from the natural language (NL) description of the intended task, we also have the dataset on which the task is to be performed, or the "data context". Existing approaches have utilized data context in a limited way by simply adding relevant information from the input data into the prompts sent to the LLM.  In this work, we utilize the available input data to execute the candidate programs generated by the LLMs and gather their outputs. We introduce semantic reranking, a technique to rerank the programs generated by LLMs based on three signals coming the program outputs: (a) semantic filtering and well-formedness based score tuning: do programs even ge
    
[^9]: 基于新型分子表示与变分自编码器的分子设计方法

    Molecular design method based on novel molecular representation and variational auto-encoder. (arXiv:2305.01580v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.01580](http://arxiv.org/abs/2305.01580)

    本论文提出了一种基于SELFIES分子表示和改进后的变分自编码器模型的分子设计方法，用于生成更多种类的分子。

    

    本论文在传统的变分自编码器的基础上采用最新的分子表示方法SELFIES，提出一种改进后的神经网络模型用于生成新的分子，该模型结合了多层卷积网络、费舍尔信息和长短期记忆神经网络，通过聚合数据特征和指导编码过程等手段，有效解决了原始变分自编码器模型的问题。实验结果表明，使用SELFIES和新的变分自编码器模型可以生成更多种类的分子，且生成的分子相似度较高。

    Based on the traditional VAE, a novel neural network model is presented, with the latest molecular representation, SELFIES, to improve the effect of generating new molecules. In this model, multi-layer convolutional network and Fisher information are added to the original encoding layer to learn the data characteristics and guide the encoding process, which makes the features of the data hiding layer more aggregated, and integrates the Long Short Term Memory neural network (LSTM) into the decoding layer for better data generation, which effectively solves the degradation phenomenon generated by the encoding layer and decoding layer of the original VAE model. Through experiments on zinc molecular data sets, it is found that the similarity in the new VAE is 8.47% higher than that of the original ones. SELFIES are better at generating a variety of molecules than the traditional molecular representation, SELFIES. Experiments have shown that using SELFIES and the new VAE model presented in 
    
[^10]: 区分和回答：通过辨别器缓解检索增强模型中虚假信息的影响

    Discern and Answer: Mitigating the Impact of Misinformation in Retrieval-Augmented Models with Discriminators. (arXiv:2305.01579v1 [cs.CL])

    [http://arxiv.org/abs/2305.01579](http://arxiv.org/abs/2305.01579)

    本文研究了现有检索增强语言模型假设所有检索信息都是正确的假设的问题，在实际应用中可能存在虚假信息导致冲突的情况下，提出了通过精细调整鉴别器和提示鉴别能力引出鲁棒性的方法，这显著改善了模型在知识冲突下的效果；同时提供了关于交替精细调整模型和上下文学习的新的结论。

    

    大多数现有的检索增强语言模型（LM）假定所有检索到的信息都是事实上正确的。本文研究一个更加现实的场景，即检索到的文档可能包含虚假信息，从而导致它们之间存在冲突。我们观察到，现有模型在精调和上下文少样本学习设置中对这种信息高度脆弱。我们提出了一些方法，通过明确地对鉴别器进行精细调整或提示来引出GPT-3的鉴别能力，使检索增强LM对虚假信息具有鲁棒性。我们在开放域问答方面的实证结果表明，这些方法显著改善了LM对知识冲突的鲁棒性。我们还提供了关于交替精细调整模型的决策与上下文学习过程的发现，为利用两者的最佳方式铺平了新的道路。

    Most existing retrieval-augmented language models (LMs) for question answering assume all retrieved information is factually correct. In this work, we study a more realistic scenario in which retrieved documents may contain misinformation, causing conflicts among them. We observe that the existing models are highly brittle to such information in both fine-tuning and in-context few-shot learning settings. We propose approaches to make retrieval-augmented LMs robust to misinformation by explicitly fine-tuning a discriminator or prompting to elicit discrimination capability in GPT-3. Our empirical results on open-domain question answering show that these approaches significantly improve LMs' robustness to knowledge conflicts. We also provide our findings on interleaving the fine-tuned model's decision with the in-context learning process, paving a new path to leverage the best of both worlds.
    
[^11]: H2CGL: 用于引文网络动态建模与影响预测的模型。

    H2CGL: Modeling Dynamics of Citation Network for Impact Prediction. (arXiv:2305.01572v1 [cs.DL])

    [http://arxiv.org/abs/2305.01572](http://arxiv.org/abs/2305.01572)

    本文提出了一种名为H2CGL的新颖图神经网络模型，用于引文网络的动态建模和影响预测。该模型通过分层和异构的方式记录目标论文年度动态信息，并优先考虑高被引论文和参考文献、引文、目标论文之间的关系。它采用加权 GIN 来捕捉异构子图的动态，同时采用对比学习来提高模型效果。

    

    论文的影响力通常是通过其引用数量来衡量的。然而，大多数常用的模型可能会低估新发表论文随时间的影响力，并且未能将这种引文网络的动态性纳入图中。在本研究中，我们构建了一个具有年度视角的目标论文的分层异构图，并记录了目标论文科学背景信息的年度动态性。然后，我们提出了一种新的图神经网络模型，称为分层异构对比图学习模型（H2CGL），以融合引文网络的异构性和动态性。H2CGL分别聚合了每年的异构信息，并优先考虑高被引论文以及参考文献和引文与目标论文之间的关系。然后，它采用加权GIN来捕捉年份之间的异构子图动态性。此外，它采用对比学习来提高模型效果。

    The potential impact of a paper is often quantified by how many citations it will receive. However, most commonly used models may underestimate the influence of newly published papers over time, and fail to encapsulate this dynamics of citation network into the graph. In this study, we construct hierarchical and heterogeneous graphs for target papers with an annual perspective. The constructed graphs can record the annual dynamics of target papers' scientific context information. Then, a novel graph neural network, Hierarchical and Heterogeneous Contrastive Graph Learning Model (H2CGL), is proposed to incorporate heterogeneity and dynamics of the citation network. H2CGL separately aggregates the heterogeneous information for each year and prioritizes the highly-cited papers and relationships among references, citations, and the target paper. It then employs a weighted GIN to capture dynamics between heterogeneous subgraphs over years. Moreover, it leverages contrastive learning to make
    
[^12]: 借助三元组感知注意力的增强型三元组集成表示方法，用于跨语言实体对齐

    Type-enhanced Ensemble Triple Representation via Triple-aware Attention for Cross-lingual Entity Alignment. (arXiv:2305.01556v1 [cs.CL])

    [http://arxiv.org/abs/2305.01556](http://arxiv.org/abs/2305.01556)

    提出了一种新的跨语言实体对齐框架TTEA，该框架考虑了三元组特征和实体角色多样性，利用三元组感知注意力构建增强型三元组集成表示，通过具体性感知控制了信息传播过程中的噪声影响。

    

    实体对齐是将跨语言和跨领域的知识图谱中指向同一实际对象的实体进行发现的关键任务。目前大多数现有的方法是通过基于嵌入的方法挖掘三元组元素的相关性来生成对齐实体表示，很少关注三元组的不可分性和实体角色的多样性。本文提出了一种新颖的框架TTEA（类型增强的三元组集成表示），通过考虑集成三元组的具体性和实体角色特征来克服上述问题。具体而言，通过将关系作为信息载体在语义空间和类型空间之间进行转换，从而可以通过具体性感知的三元组注意力平稳地控制空间转换和信息传播过程中的噪声影响。此外，我们的框架使用了...

    Entity alignment(EA) is a crucial task for integrating cross-lingual and cross-domain knowledge graphs(KGs), which aims to discover entities referring to the same real-world object from different KGs. Most existing methods generate aligning entity representation by mining the relevance of triple elements via embedding-based methods, paying little attention to triple indivisibility and entity role diversity. In this paper, a novel framework named TTEA -- Type-enhanced Ensemble Triple Representation via Triple-aware Attention for Cross-lingual Entity Alignment is proposed to overcome the above issues considering ensemble triple specificity and entity role features. Specifically, the ensemble triple representation is derived by regarding relation as information carrier between semantic space and type space, and hence the noise influence during spatial transformation and information propagation can be smoothly controlled via specificity-aware triple attention. Moreover, our framework uses 
    
[^13]: 如何发挥大语言模型在少样本关系抽取中的能力？

    How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])

    [http://arxiv.org/abs/2305.01555](http://arxiv.org/abs/2305.01555)

    本文通过使用GPT-3.5模型在少样本关系抽取中，实现在四个不同数据集上的新的最优性能，并提出了与任务相关的指导说明和约束模式下的数据生成方法。

    

    语言模型的扩展已经彻底改变了广泛的自然语言处理任务，但是使用大型语言模型进行少样本关系抽取还没有得到全面探索。本文通过详细实验，研究了使用GPT-3.5进行少样本关系抽取的基本方法——上下文学习和数据生成。为了增强少样本性能，我们进一步提出了与任务相关的指导说明和约束模式下的数据生成。我们观察到，在上下文学习的情况下，可以实现与以前的提示学习方法相当的性能，而使用大型语言模型的数据生成可以推动以前的解决方案以在四个广泛研究的关系抽取数据集上获得新的最先进的少样本结果。我们希望我们的工作可以激发未来对大型语言模型在少样本关系抽取中的能力的研究。代码可以在 \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} 中找到。

    Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
    
[^14]: FIREBALL：一份包含结构化游戏状态信息的Dungeons & Dragons实际游戏数据集

    FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information. (arXiv:2305.01528v1 [cs.CL])

    [http://arxiv.org/abs/2305.01528](http://arxiv.org/abs/2305.01528)

    本研究介绍了一份包含真实游戏状态信息的Dungeons & Dragons实际游戏数据集FIREBALL，它可以改善自然语言生成的质量。此外，LLMs可以使用FIREBALL中的游戏状态信息来生成更高质量的游戏回合。

    

    Dungeons & Dragons（D＆D）是一款桌面角色扮演游戏，其玩家之间存在复杂的自然语言交互和隐藏的状态信息。最近的研究表明，拥有状态信息的大型语言模型（LLMs）生成的游戏回合比仅使用对话历史的LLMs更具高质量。然而，以往的研究使用的游戏状态信息是启发式创建的，并不是真正的黄金标准游戏状态。我们提出了FIREBALL，这是一个包含真实游戏状态信息的大型数据集，其中包含来自Discord的近25,000个真实D＆D游戏会话。我们记录了使用Avrae机器人的玩家的游戏会话，该机器人是为了帮助人们在线玩D＆D而开发的，并捕获了语言、游戏命令和基础游戏状态信息。我们证明，通过使用Avrae状态信息，FIREBALL可以提高自然语言生成（NLG），从而提高自动评估指标和人类的质量评判。此外，我们还展示了LLMs可以生成…

    Dungeons & Dragons (D&D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D\&D gameplay on Discord with true game state info. We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D&D online, capturing language, game commands and underlying game state information. We demonstrate that FIREBALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate
    
[^15]: Huatuo-26M：一份大规模的中医问答数据集

    Huatuo-26M, a Large-scale Chinese Medical QA Dataset. (arXiv:2305.01526v1 [cs.CL])

    [http://arxiv.org/abs/2305.01526](http://arxiv.org/abs/2305.01526)

    我们发布了一份最大的中医问答数据集，现有模型的表现远低于预期。此数据集可以用于其他QA数据集的零-shot学习并用作检索增强生成（RAG）的外部知识，在预训练语言模型时代仍然具有挑战性。

    

    本文中，我们发布了一份有着2600万个问答对的中医问答数据集，此为迄今为止最大型的数据集。我们以检索和生成两个方面对现有方法在此数据集上进行了基准测试。实验结果表明，现有模型的表现远低于预期，而且在预训练语言模型时代，发布的数据集仍然具有挑战性。此外，我们还实验性地展示了所提议数据集在以下方面的益处：（i）以零-shot的方式为其他问答数据集训练模型；（ii）作为检索增强生成（RAG）的外部知识；（iii）通过将问答对作为预训练语料库进行连续训练方式，提升现有预训练语言模型的性能。我们相信该数据集不仅将有助于医学研究，还将促进病人和临床医生的服务。请参考：\url{https://github.com/FreedomIntelligence/Huatuo-26M}。

    In this paper, we release a largest ever medical Question Answering (QA) dataset with 26 million QA pairs. We benchmark many existing approaches in our dataset in terms of both retrieval and generation. Experimental results show that the existing models perform far lower than expected and the released dataset is still challenging in the pre-trained language model era. Moreover, we also experimentally show the benefit of the proposed dataset in many aspects: (i) trained models for other QA datasets in a zero-shot fashion; and (ii) as external knowledge for retrieval-augmented generation (RAG); and (iii) improving existing pre-trained language models by using the QA pairs as a pre-training corpus in continued training manner. We believe that this dataset will not only contribute to medical research but also facilitate both the patients and clinical doctors. See \url{https://github.com/FreedomIntelligence/Huatuo-26M}.
    
[^16]: 用显式和隐式知识推动AI药物发现

    Empowering AI drug discovery with explicit and implicit knowledge. (arXiv:2305.01523v1 [cs.LG])

    [http://arxiv.org/abs/2305.01523](http://arxiv.org/abs/2305.01523)

    DeepEIK是一个统一的深度学习框架，利用显式和隐式知识进行AI药物发现，通过特征融合和注意力机制来提高模型预测准确性，并在多个任务上显著优于现有方法。

    

    近年来，独立利用知识图谱中的显式知识或生物医学文献中的隐式知识进行AI药物发现的研究迅速增长。这些方法极大地提高了多个下游任务上AI模型的预测准确性。然而，独立地整合显式和隐式知识会阻碍对分子的理解。本研究提出了DeepEIK，这是一个统一的深度学习框架，结合了显式和隐式知识来进行AI药物发现。

    Motivation: Recently, research on independently utilizing either explicit knowledge from knowledge graphs or implicit knowledge from biomedical literature for AI drug discovery has been growing rapidly. These approaches have greatly improved the prediction accuracy of AI models on multiple downstream tasks. However, integrating explicit and implicit knowledge independently hinders their understanding of molecules. Results: We propose DeepEIK, a unified deep learning framework that incorporates both explicit and implicit knowledge for AI drug discovery. We adopt feature fusion to process the multi-modal inputs, and leverage the attention mechanism to denoise the text information. Experiments show that DeepEIK significantly outperforms state-of-the-art methods on crucial tasks in AI drug discovery including drug-target interaction prediction, drug property prediction and protein-protein interaction prediction. Further studies show that benefiting from explicit and implicit knowledge, our
    
[^17]: 定义预测规则的可复制性

    Defining Replicability of Prediction Rules. (arXiv:2305.01518v1 [stat.ME])

    [http://arxiv.org/abs/2305.01518](http://arxiv.org/abs/2305.01518)

    本文提出了一种定义预测规则可复制性的方法，通过多代理人框架定义可复制性，旨在为机器学习中更系统的可复制性评估提供指导。

    

    本文提出了一种定义预测规则可复制性的方法。受最近一份美国国家科学院报告的启发，文章从一个角度出发，认为可复制性是在适合回答同一预测问题的多个研究中获得一致结果，每个研究都有自己的数据。然后讨论了定义该语句关键部分的概念和问题，并专注于典型利用环境中“一致结果”的含义，提出了一个多代理人框架来定义可复制性，代理人既不是合作伙伴也不是对手。其中一些普遍实用的方法成为特例。希望为机器学习中更系统的可复制性评估提供指导。

    In this article I propose an approach for defining replicability for prediction rules. Motivated by a recent NAS report, I start from the perspective that replicability is obtaining consistent results across studies suitable to address the same prediction question, each of which has obtained its own data. I then discuss concept and issues in defining key elements of this statement. I focus specifically on the meaning of "consistent results" in typical utilization contexts, and propose a multi-agent framework for defining replicability, in which agents are neither partners nor adversaries. I recover some of the prevalent practical approaches as special cases. I hope to provide guidance for a more systematic assessment of replicability in machine learning.
    
[^18]: 在大规模共享汽车平台中发现预训练的效果

    Discovering the Effectiveness of Pre-Training in a Large-scale Car-sharing Platform. (arXiv:2305.01506v1 [cs.CV])

    [http://arxiv.org/abs/2305.01506](http://arxiv.org/abs/2305.01506)

    本文针对大规模共享汽车平台上的汽车图像分析任务，通过分析实验验证了预训练技术对于提高模型性能的有效性。

    

    深度学习的最新进展赋予了各种智能交通应用以力量，特别是在共享汽车平台上。传统的共享汽车服务运营高度依赖于车队管理中的人类参与，现代共享汽车平台则允许用户在使用前后上传汽车图像，以检查汽车而无需实地访问。为了自动化上述检查任务，之前的方法利用了深度神经网络并普遍采用了预训练技术以在有限的标记数据集下建立有效的模型。由于处理汽车图像的候选从业者很可能会遭受标记数据集的缺乏，因此我们对预训练的有效性进行了复杂的类比分析。然而，之前的研究主要集中于预训练的效果缺乏深入剖析。鉴于上述分析缺乏，我们的研究介绍了一系列分析，以探究预训练在大规模共享汽车平台中的有效性。我们在大规模汽车图像数据集上尝试了不同的预训练策略，并将已预训练的模型的性能与从头开始训练的传统模型进行比较。我们的发现表明，预训练技术确实提高了汽车图像分析的性能，无论是在准确性还是效率方面。我们的研究为在共享汽车平台应用中探究预训练的有效性提供了洞见。

    Recent progress of deep learning has empowered various intelligent transportation applications, especially in car-sharing platforms. While the traditional operations of the car-sharing service highly relied on human engagements in fleet management, modern car-sharing platforms let users upload car images before and after their use to inspect the cars without a physical visit. To automate the aforementioned inspection task, prior approaches utilized deep neural networks. They commonly employed pre-training, a de-facto technique to establish an effective model under the limited number of labeled datasets. As candidate practitioners who deal with car images would presumably get suffered from the lack of a labeled dataset, we analyzed a sophisticated analogy into the effectiveness of pre-training is important. However, prior studies primarily shed a little spotlight on the effectiveness of pre-training. Motivated by the aforementioned lack of analysis, our study proposes a series of analys
    
[^19]: 超越分类：最先进的语言模型中的财务推理

    Beyond Classification: Financial Reasoning in State-of-the-Art Language Models. (arXiv:2305.01505v1 [cs.CL])

    [http://arxiv.org/abs/2305.01505](http://arxiv.org/abs/2305.01505)

    本研究探讨了大语言模型在财务推理领域的潜在应用，对任务制定、数据生成、提示方法和评估能力等方面进行了详细研究，最终在各种数据集大小上对参数规模为2.8B至13B的各种GPT变体进行基准测试。

    

    大语言模型(LLMs)由1000亿及以上的参数组成，在复杂的多步推理任务中表现出了非凡的能力。然而，这种通用的进展应用在很少领域中，例如临床或法律领域，而财务推理领域基本上未被探索。据我们所知，LLMs解决财务推理问题的能力从未被研究过，并且它是否可以在任何规模上完成仍未知。为了填补这一知识空白，本研究对LLMs在财务领域的潜在应用进行了全面调查。调查包括对一系列主题的详细探讨，包括任务制定，合成数据生成，提示方法和评估能力。此外，本研究在不同的数据集大小上对参数规模为2.8B至13B的各种GPT变体进行基准测试，包括有无指导调整。

    Large Language Models (LLMs), consisting of 100 billion or more parameters, have demonstrated remarkable ability in complex multi-step reasoning tasks. However, the application of such generic advancements has been limited to a few fields, such as clinical or legal, with the field of financial reasoning remaining largely unexplored. To the best of our knowledge, the ability of LLMs to solve financial reasoning problems has never been dealt with, and whether it can be performed at any scale remains unknown. To address this knowledge gap, this research presents a comprehensive investigation into the potential application of LLMs in the financial domain. The investigation includes a detailed exploration of a range of subjects, including task formulation, synthetic data generation, prompting methods, and evaluation capability. Furthermore, the study benchmarks various GPT variants with parameter scales ranging from 2.8B to 13B, with and without instruction tuning, on diverse dataset sizes.
    
[^20]: 旨在总结带有层次关系的多篇文档

    Towards Summarizing Multiple Documents with Hierarchical Relationships. (arXiv:2305.01498v1 [cs.CL])

    [http://arxiv.org/abs/2305.01498](http://arxiv.org/abs/2305.01498)

    提出了一个新的数据集PeerSum用于生成科学论文的元评论，源文档具有显式层次结构的丰富文档间关系，提出了一种用于元评论生成的关系感知多任务模型Rammer。

    

    多数现存的多文档摘要(MDS)数据集缺少人工生成的、真实的(即非合成的)摘要或者带有显式文档间关系的源文档。为了增强MDS系统的能力，我们提出PeerSum，这是一个新颖的数据集，用于生成科学论文的元评论，其中元评论是对评论和相应讨论的高度概括且真实的摘要。这些源文档具有显式层次结构的丰富文档间关系，包括交叉引用和经常出现的冲突。鉴于很少有研究采用基于预训练语言模型的注意力操纵来将层次关系纳入MDS系统中，我们还提出了Rammer(关系感知多任务元评论生成器)，这是一种元评论生成模型，使用基于层次关系的稀疏注意力和多任务目标，可以预测多个度量值。

    Most existing multi-document summarization (MDS) datasets lack human-generated and genuine (i.e., not synthetic) summaries or source documents with explicit inter-document relationships that a summary must capture. To enhance the capabilities of MDS systems we present PeerSum, a novel dataset for generating meta-reviews of scientific papers, where the meta-reviews are highly abstractive and genuine summaries of reviews and corresponding discussions. These source documents have rich inter-document relationships of an explicit hierarchical structure with cross-references and often feature conflicts. As there is a scarcity of research that incorporates hierarchical relationships into MDS systems through attention manipulation on pre-trained language models, we additionally present Rammer (Relationship-aware Multi-task Meta-review Generator), a meta-review generation model that uses sparse attention based on the hierarchical relationships and a multi-task objective that predicts several me
    
[^21]: ARBEx：用于鲁棒性面部表情学习的关注特征提取与可靠性平衡框架

    ARBEx: Attentive Feature Extraction with Reliability Balancing for Robust Facial Expression Learning. (arXiv:2305.01486v1 [cs.CV])

    [http://arxiv.org/abs/2305.01486](http://arxiv.org/abs/2305.01486)

    本论文提出了一个名为ARBEx的框架，它采用了可靠性平衡方法来应对面部表情学习任务中的数据偏差和不确定性。该框架还引入了可学习的锚点和多头自注意机制，并在多个公共数据集上取得了有效性验证。

    

    本论文提出了一个名为ARBEx的框架，它是由Vision Transformer驱动的新型关注特征提取框架，带有可靠性平衡，以应对面部表情学习任务中的较差类分布、偏差和不确定性。我们采用了多种数据预处理和精化方法以及基于窗口的交叉关注ViT来充分利用数据。我们还在嵌入空间中引入了可学习的锚点，加上标签分布和多头自注意机制，以通过可靠性平衡优化对弱预测的性能，这是一种提高标签预测韧性的策略。为了确保正确的标签分类并提高模型的区分能力，我们引入了锚损失，鼓励锚点之间的大间隔。另外，多头自注意机制也是可训练的，对于提升在FEL任务中的表现至关重要。最后，我们在多个公共数据集上验证了ARBEx的有效性。

    In this paper, we introduce a framework ARBEx, a novel attentive feature extraction framework driven by Vision Transformer with reliability balancing to cope against poor class distributions, bias, and uncertainty in the facial expression learning (FEL) task. We reinforce several data pre-processing and refinement methods along with a window-based cross-attention ViT to squeeze the best of the data. We also employ learnable anchor points in the embedding space with label distributions and multi-head self-attention mechanism to optimize performance against weak predictions with reliability balancing, which is a strategy that leverages anchor points, attention scores, and confidence values to enhance the resilience of label predictions. To ensure correct label classification and improve the models' discriminative power, we introduce anchor loss, which encourages large margins between anchor points. Additionally, the multi-head self-attention mechanism, which is also trainable, plays an i
    
[^22]: 模型之间的潜在一致性：提高机器学习模型可靠性的方法

    Great Models Think Alike: Improving Model Reliability via Inter-Model Latent Agreement. (arXiv:2305.01481v1 [cs.LG])

    [http://arxiv.org/abs/2305.01481](http://arxiv.org/abs/2305.01481)

    通过测量模型和一个基础模型的潜在空间之间的一致性来提高机器学习模型的可靠性。

    

    机器学习模型的可靠应用对于深度学习算法的实际部署至关重要。但是由于过度自信，模型经常是不可靠的。本文提出通过测量一个模型的潜在空间与另一个基础模型的潜在空间之间的一致性来估计模型的可靠性。然而，由于它们的不连贯性，即任意旋转和不同的维度，两个不同的潜在空间之间的一致性很难衡量。为了解决这个不连贯性问题，我们设计了一种潜在空间之间的“邻域一致性度量”，并发现这种一致性与模型预测的可靠性有惊人的相关性。此外，我们证明在后续的方式中将邻域一致性融入模型的预测置信度可以显著提高其可靠性。在各种数据集上的故障检测的理论分析和广泛实验验证了这种方法的有效性。

    Reliable application of machine learning is of primary importance to the practical deployment of deep learning methods. A fundamental challenge is that models are often unreliable due to overconfidence. In this paper, we estimate a model's reliability by measuring \emph{the agreement between its latent space, and the latent space of a foundation model}. However, it is challenging to measure the agreement between two different latent spaces due to their incoherence, \eg, arbitrary rotations and different dimensionality. To overcome this incoherence issue, we design a \emph{neighborhood agreement measure} between latent spaces and find that this agreement is surprisingly well-correlated with the reliability of a model's predictions. Further, we show that fusing neighborhood agreement into a model's predictive confidence in a post-hoc way significantly improves its reliability. Theoretical analysis and extensive experiments on failure detection across various datasets verify the effective
    
[^23]: 混合整数最优控制在混合动力车辆能量管理中的强化学习研究

    Mixed-Integer Optimal Control via Reinforcement Learning: A Case Study on Hybrid Vehicle Energy Management. (arXiv:2305.01461v1 [eess.SY])

    [http://arxiv.org/abs/2305.01461](http://arxiv.org/abs/2305.01461)

    本研究提出了一种新的混合整数最优控制问题的强化学习算法，可同时处理连续和离散控制变量， 并在混合动力车辆能源管理问题上优于现有方法和强化学习算法。

    

    许多最优控制问题需要同时输出连续和离散控制变量。这些问题通常被制定为混合整数最优控制(MIOC)问题，由于解决方案空间的复杂性而难以解决。数值方法如分支定界法计算成本高，不适合实时控制。本文提出了一种新的连续离散强化学习(CDRL)算法，双延迟深度确定性演员- Q(TD3AQ)，用于MIOC问题。TD3AQ结合了演员-批评家和Q学习方法的优点，同时可以处理连续和离散动作空间。该算法在混合动力车辆(HEV)能量管理问题上进行评估，其中连续变量发动机转矩和离散变量齿轮比的实时控制对于最大化燃油经济性并满足驾驶约束条件至关重要。不同驱动循环的仿真结果表明，所提出的CDRL算法在解决质量、收敛速度和计算效率方面优于最先进的数值方法和现有的强化学习算法。

    Many optimal control problems require the simultaneous output of continuous and discrete control variables. Such problems are usually formulated as mixed-integer optimal control (MIOC) problems, which are challenging to solve due to the complexity of the solution space. Numerical methods such as branch-and-bound are computationally expensive and unsuitable for real-time control. This paper proposes a novel continuous-discrete reinforcement learning (CDRL) algorithm, twin delayed deep deterministic actor-Q (TD3AQ), for MIOC problems. TD3AQ combines the advantages of both actor-critic and Q-learning methods, and can handle the continuous and discrete action spaces simultaneously. The proposed algorithm is evaluated on a hybrid electric vehicle (HEV) energy management problem, where real-time control of the continuous variable engine torque and discrete variable gear ratio is essential to maximize fuel economy while satisfying driving constraints. Simulation results on different drive cyc
    
[^24]: 疫苗供应链优化的预测协调

    Forecast reconciliation for vaccine supply chain optimization. (arXiv:2305.01455v1 [cs.LG])

    [http://arxiv.org/abs/2305.01455](http://arxiv.org/abs/2305.01455)

    本文尝试通过层次时间序列来预测疫苗销售数据，并使用协调方法解决了不同层次预测不一致的问题，结果表明最小迹和加权最小二乘与结构缩放的协调方法效果最佳，提高了预测的一致性并减少预测误差。

    

    疫苗供应链优化通过按类型或地点分组的层次时间序列预测来获益。然而，当高层次的预测结果与低层次预测结果之和不匹配时，不同层次的预测变得不一致，这可以通过协调方法进行解决。本文通过将2010年至2021年GSK公司的销售数据建模为层次时间序列，解决了疫苗销售预测问题。在使用多个ARIMA模型预测未来值后，我们系统地比较了各种协调方法的性能，并使用统计检验进行了比较。我们还比较了COVID之前和之后的预测性能。结果突出了最小迹和加权最小二乘与结构缩放的协调方法，这些方法提供了一致的预测并减少了基线ARIMA的预测误差。

    Vaccine supply chain optimization can benefit from hierarchical time series forecasting, when grouping the vaccines by type or location. However, forecasts of different hierarchy levels become incoherent when higher levels do not match the sum of the lower levels forecasts, which can be addressed by reconciliation methods.  In this paper, we tackle the vaccine sale forecasting problem by modeling sales data from GSK between 2010 and 2021 as a hierarchical time series. After forecasting future values with several ARIMA models, we systematically compare the performance of various reconciliation methods, using statistical tests. We also compare the performance of the forecast before and after COVID. The results highlight Minimum Trace and Weighted Least Squares with Structural scaling as the best performing methods, which provided a coherent forecast while reducing the forecast error of the baseline ARIMA.
    
[^25]: 使用假想回顾进行不确定机器伦理决策

    Uncertain Machine Ethical Decisions Using Hypothetical Retrospection. (arXiv:2305.01424v1 [cs.AI])

    [http://arxiv.org/abs/2305.01424](http://arxiv.org/abs/2305.01424)

    本文提出了一种新的机器伦理推理方法，使用假想回顾论证程序，允许使用各种哲学理论进行伦理推理，可以考虑概率和不确定性，应用于一个自主图书馆系统用例中进行测试。

    

    本文提出使用由Sven Hansson开发的假想回顾论证程序，以考虑概率和不确定性，从哲学的角度出发改进现有的机器伦理推理方法，以与人类产生共鸣。该程序将行动表示为一组可能结果的分支，每个分支具有状态、效用和数值或诗意概率估计。行动的选择基于从它们的分支的角度比较倾向于行动的一组论证，即使这些分支导致了不良结果也一样。这种使用论证的方法允许使用各种哲学理论进行伦理推理，可能可以灵活地组合使用。我们将该方法应用于一个自主图书馆系统用例中，分别独立或同时应用结果主义和义务论的伦理理论，并引入一个初步的框架，似乎满足了各种要求。

    We propose the use of the hypothetical retrospection argumentation procedure, developed by Sven Hansson, to improve existing approaches to machine ethical reasoning by accounting for probability and uncertainty from a position of Philosophy that resonates with humans. Actions are represented with a branching set of potential outcomes, each with a state, utility, and either a numeric or poetic probability estimate. Actions are chosen based on comparisons between sets of arguments favouring actions from the perspective of their branches, even those branches that led to an undesirable outcome. This use of arguments allows a variety of philosophical theories for ethical reasoning to be used, potentially in flexible combination with each other. We implement the procedure, applying consequentialist and deontological ethical theories, independently and concurrently, to an autonomous library system use case. We introduce a a preliminary framework that seems to meet the varied requirements of a
    
[^26]: 返回分布规划：鲁棒性模仿学习

    Get Back Here: Robust Imitation by Return-to-Distribution Planning. (arXiv:2305.01400v1 [cs.RO])

    [http://arxiv.org/abs/2305.01400](http://arxiv.org/abs/2305.01400)

    本文提出的算法POIR将行为克隆和规划器相结合，解决了模仿学习中分布偏移的问题，模型在机器人操纵任务中表现出强大的鲁棒性。

    

    我们考虑模仿学习的设置，即专家数据不是在实际部署环境下收集的，而是在另一个版本上收集的。为了解决由此导致的分布偏移问题，我们将行为克隆（BC）与一个规划器相结合，规划器的任务是在代理程序偏离演示分布时，将代理程序带回到专家访问的状态。得到的算法POIR可以离线训练，并利用在线交互来有效地优化其规划器，以逐步提高性能。我们在现实的机器人操纵模拟器中对POIR进行测试，使用各种人类生成的操作演示，并展示了学习策略对不同初始状态分布和嘈杂动态的鲁棒性。

    We consider the Imitation Learning (IL) setup where expert data are not collected on the actual deployment environment but on a different version. To address the resulting distribution shift, we combine behavior cloning (BC) with a planner that is tasked to bring the agent back to states visited by the expert whenever the agent deviates from the demonstration distribution. The resulting algorithm, POIR, can be trained offline, and leverages online interactions to efficiently fine-tune its planner to improve performance over time. We test POIR on a variety of human-generated manipulation demonstrations in a realistic robotic manipulation simulator and show robustness of the learned policy to different initial state distributions and noisy dynamics.
    
[^27]: 使用深度编码-解码模型进行油污分割

    Oil Spill Segmentation using Deep Encoder-Decoder models. (arXiv:2305.01386v1 [cs.CV])

    [http://arxiv.org/abs/2305.01386](http://arxiv.org/abs/2305.01386)

    本研究测试了使用深度编码-解码模型进行油污分割的可行性，并在高维卫星合成孔径雷达图像数据上比较了多种分割模型的结果。最好的表现模型是使用ResNet-50编码器和DeepLabV3+解码器，能够实现64.868%的平均交集联合（IoU）和61.549%的“油污”类IoU。

    

    原油是现代世界经济的重要组成部分，随着原油广泛应用的需求增长，意外的油污泄漏也难以避免。本研究测试了使用深度编码-解码模型进行油污检测的可行性，并比较了高维卫星合成孔径雷达图像数据上几种分割模型的结果。实验中使用了多种模型组合。最好的表现模型是使用ResNet-50编码器和DeepLabV3+解码器，与当前基准模型相比，它在“油污”类的平均交集联合（IoU）上实现了64.868%的结果和61.549%的类IoU。

    Crude oil is an integral component of the modern world economy. With the growing demand for crude oil due to its widespread applications, accidental oil spills are unavoidable. Even though oil spills are in and themselves difficult to clean up, the first and foremost challenge is to detect spills. In this research, the authors test the feasibility of deep encoder-decoder models that can be trained effectively to detect oil spills. The work compares the results from several segmentation models on high dimensional satellite Synthetic Aperture Radar (SAR) image data. Multiple combinations of models are used in running the experiments. The best-performing model is the one with the ResNet-50 encoder and DeepLabV3+ decoder. It achieves a mean Intersection over Union (IoU) of 64.868% and a class IoU of 61.549% for the "oil spill" class when compared with the current benchmark model, which achieved a mean IoU of 65.05% and a class IoU of 53.38% for the "oil spill" class.
    
[^28]: 基于LTL规范的样本有效无模型强化学习与优化保证

    Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees. (arXiv:2305.01381v1 [cs.LG])

    [http://arxiv.org/abs/2305.01381](http://arxiv.org/abs/2305.01381)

    本文提出了一种基于LTL规范的无模型强化学习方法，该方法结合乘积MDP、奖励结构和折扣机制有效地学习并优化未知随机系统最大化满足LTL规范的概率的最优策略。

    

    线性时间逻辑（LTL）广泛用于指定系统策略的高级目标，自主系统学习相对于这样的规范的最优策略是非常理想的。 但是，从LTL规范中学习最优策略并不轻松。我们提出了一种无模型强化学习（RL）方法，该方法可以有效地学习未知随机系统的最优策略，其中使用马尔可夫决策过程（MDP）进行建模。我们提出了一种新颖且更通用的乘积MDP、奖励结构和折扣机制，当与现成的无模型RL算法结合使用时，能够高效地学习最大化给定LTL规范满足概率的最优策略，并提供了更好的有关选择RL中关键参数以保证最优性的理论结果。为了直接评估学习策略，我们采用概率模型检查器PRISM来计算LTL规范的满足概率。

    Linear Temporal Logic (LTL) is widely used to specify high-level objectives for system policies, and it is highly desirable for autonomous systems to learn the optimal policy with respect to such specifications. However, learning the optimal policy from LTL specifications is not trivial. We present a model-free Reinforcement Learning (RL) approach that efficiently learns an optimal policy for an unknown stochastic system, modelled using Markov Decision Processes (MDPs). We propose a novel and more general product MDP, reward structure and discounting mechanism that, when applied in conjunction with off-the-shelf model-free RL algorithms, efficiently learn the optimal policy that maximizes the probability of satisfying a given LTL specification with optimality guarantees. We also provide improved theoretical results on choosing the key parameters in RL to ensure optimality. To directly evaluate the learned policy, we adopt probabilistic model checker PRISM to compute the probability of 
    
[^29]: 基于Option框架的多模式探索自主非单体智能体

    An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework. (arXiv:2305.01322v1 [cs.AI])

    [http://arxiv.org/abs/2305.01322](http://arxiv.org/abs/2305.01322)

    该论文关注强化学习中的探索研究，提出了一个能够自主管理探索策略的多模式智能体非单体探索方法，并通过实验结果展示了该方法的优越性能。

    

    强化学习领域的探索研究主要关注“如何探索”的探索方式，而“何时探索”的探索研究一直没有成为重点。典型的探索行为通常将探索行为与智能体的开发利用行为绑定在一起。最近出现了非单体探索行为的研究，以研究人类和动物的模式切换行为。本研究的最终目的是使智能体能够自主决定何时探索或利用。我们在Option框架中描述了自主多模式探索的初始研究。通过比较实验结果，我们展示了我们的方法相对于现有的非单体探索方法的更高性能。

    Most exploration research on reinforcement learning (RL) has paid attention to `the way of exploration', which is `how to explore'. The other exploration research, `when to explore', has not been the main focus of RL exploration research. \textcolor{black}{The issue of `when' of a monolithic exploration in the usual RL exploration behaviour binds an exploratory action to an exploitational action of an agent. Recently, a non-monolithic exploration research has emerged to examine the mode-switching exploration behaviour of humans and animals.} The ultimate purpose of our research is to enable an agent to decide when to explore or exploit autonomously. We describe the initial research of an autonomous multi-mode exploration of non-monolithic behaviour in an options framework. The higher performance of our method is shown against the existing non-monolithic exploration method through comparative experimental results.
    
[^30]: 横向迁移轻量化视觉提示发生器在VL-LLMs之间的应用研究

    Transfer Visual Prompt Generator across LLMs. (arXiv:2305.01278v1 [cs.CV])

    [http://arxiv.org/abs/2305.01278](http://arxiv.org/abs/2305.01278)

    本论文提出将已有的轻量化视觉提示发生器连接到视觉-语言LLM以减少资源消耗的方法，并提出了跨不同大小和类型的LLMs的VPG转移方案VPGTrans，该方案在VQA和NLVR2任务中表现优秀。

    

    本文研究利用现有的轻量化视觉提示发生器（VPG）连接已有的视觉-语言LLM（VL-LLM）以减少资源消耗。此外，我们提出一种跨不同大小和类型的LLMs的VPG转移方案。基于我们的观察，我们设计了一个名为VPGTrans的两阶段转移框架，它在VQA和NLVR2两个下游任务中表现出比现有方法更好的精度和转移速度。

    While developing a new vision-language LLM (VL-LLM) by pre-training on tremendous image-text pairs from scratch can be exceedingly resource-consuming, connecting an existing LLM with a comparatively lightweight visual prompt generator (VPG) becomes a feasible paradigm. However, further tuning the VPG part of the VL-LLM still suffers from indispensable computational costs, i.e., requiring thousands of GPU hours and millions of training data. One alternative solution is to transfer an existing VPG from any existing VL-LLMs for the target VL-LLM.  In this work, we for the first time investigate the VPG transferability across LLMs, and explore a solution to reduce the cost of VPG transfer. We first study the VPG transfer across different LLM sizes (e.g., small-to-large), and across different LLM types, through which we diagnose the key factors to maximize the transfer efficiency. Based on our observation, we design a two-stage transfer framework named VPGTrans, which is simple yet highly e
    
[^31]: 自主驾驶中的sim2real和数字孪生：综述

    Sim2real and Digital Twins in Autonomous Driving: A Survey. (arXiv:2305.01263v1 [cs.RO])

    [http://arxiv.org/abs/2305.01263](http://arxiv.org/abs/2305.01263)

    本文综述了自主驾驶中的sim2real和数字孪生方法，它们分别解决了从模拟到现实的知识转移和从真实世界数据中学习以提高仿真精度的问题，但也存在各自的优缺点和限制。

    

    安全和成本是自主驾驶技术开发的两个重要问题。从学术研究到商业应用，都需要充分的模拟和真实世界测试。通常会在模拟环境中进行大规模的测试，然后将学习到的驾驶知识转移到真实世界，因此如何将在模拟中学习到的驾驶知识适应现实成为一个关键问题。然而，虚拟仿真世界与现实世界在许多方面（如照明、纹理、车辆动力学和代理行为等）存在差异，这使得弥合虚拟和真实世界之间的差距变得困难。这个差距通常被称为现实差距（RG）。近年来，研究人员探索了各种方法来解决现实差距问题，这些方法可以广泛地分为两类：从模拟到现实的知识转移（sim2real）和从真实世界数据中学习以提高仿真精度（数字孪生）。本文综述了自主驾驶中的sim2real和数字孪生方法，审查了当前的技术和应用，以及它们的优点和局限性。

    Safety and cost are two important concerns for the development of autonomous driving technologies. From the academic research to commercial applications of autonomous driving vehicles, sufficient simulation and real world testing are required. In general, a large scale of testing in simulation environment is conducted and then the learned driving knowledge is transferred to the real world, so how to adapt driving knowledge learned in simulation to reality becomes a critical issue. However, the virtual simulation world differs from the real world in many aspects such as lighting, textures, vehicle dynamics, and agents' behaviors, etc., which makes it difficult to bridge the gap between the virtual and real worlds. This gap is commonly referred to as the reality gap (RG). In recent years, researchers have explored various approaches to address the reality gap issue, which can be broadly classified into two categories: transferring knowledge from simulation to reality (sim2real) and learn
    
[^32]: DreamPaint: 无需3D建模的电商商品试穿Few-Shot修复

    DreamPaint: Few-Shot Inpainting of E-Commerce Items for Virtual Try-On without 3D Modeling. (arXiv:2305.01257v1 [cs.CV])

    [http://arxiv.org/abs/2305.01257](http://arxiv.org/abs/2305.01257)

    DreamPaint是一个无需3D建模的电商商品试穿Few-Shot修复框架，可以在任意用户环境下对商品图像进行修复，具有较高的修复精度。

    

    我们介绍了 DreamPaint，这是一个能够智能修复电商产品在用户提供的任意背景图像上的框架。例如，用户可以使用自己的图像来试穿电商目录中的衣服，或者使用自己房间的图像来在其中试放电商目录中的家具等。与以前的增强现实（AR）虚拟试穿方法不同，DreamPaint 不使用，也不需要电商产品或用户环境的 3D 建模。相反，它直接使用目录数据库中可用的产品的 2D 图像和上下文的 2D 图像，例如从用户的手机相机中拍摄的图像。该方法依靠少量样本微调预训练扩散模型及其产品目录图像下的遮罩潜变量（例如，Masked DreamBooth）的权重，然后将其加载到能够保留属性的预训练修复模块上。

    We introduce DreamPaint, a framework to intelligently inpaint any e-commerce product on any user-provided context image. The context image can be, for example, the user's own image for virtual try-on of clothes from the e-commerce catalog on themselves, the user's room image for virtual try-on of a piece of furniture from the e-commerce catalog in their room, etc. As opposed to previous augmented-reality (AR)-based virtual try-on methods, DreamPaint does not use, nor does it require, 3D modeling of neither the e-commerce product nor the user context. Instead, it directly uses 2D images of the product as available in product catalog database, and a 2D picture of the context, for example taken from the user's phone camera. The method relies on few-shot fine tuning a pre-trained diffusion model with the masked latents (e.g., Masked DreamBooth) of the catalog images per item, whose weights are then loaded on a pre-trained inpainting module that is capable of preserving the characteristics 
    
[^33]: LTL本体中的时间查询反向工程

    Reverse Engineering of Temporal Queries Mediated by LTL Ontologies. (arXiv:2305.01248v1 [cs.LO])

    [http://arxiv.org/abs/2305.01248](http://arxiv.org/abs/2305.01248)

    本文研究了基于时间戳数据的LTL正片段制定的查询反向工程问题，旨在构建一个将给定答案与非答案分开的查询语言，并考虑了LTL本体介导的情况。

    

    在数据库查询的反向工程中，我们旨在从给定的答案和非答案集合构建一个查询; 然后可以将其用于进一步探索数据或作为答案和非答案的解释。我们研究了基于时间戳数据的线性时态逻辑LTL的正片段制定的查询的这个查询-by-example问题，重点是设计合适的查询语言和决定是否存在一个查询在给定的语言中可以将给定的答案与非答案分开的组合和数据复杂性。我们同时考虑了普通的LTL查询和那些通过LTL本体介导的查询。

    In reverse engineering of database queries, we aim to construct a query from a given set of answers and non-answers; it can then be used to explore the data further or as an explanation of the answers and non-answers. We investigate this query-by-example problem for queries formulated in positive fragments of linear temporal logic LTL over timestamped data, focusing on the design of suitable query languages and the combined and data complexity of deciding whether there exists a query in the given language that separates the given answers from non-answers. We consider both plain LTL queries and those mediated by LTL-ontologies.
    
[^34]: DRPT: 分离与循环提示调整的组合零样本学习

    DRPT: Disentangled and Recurrent Prompt Tuning for Compositional Zero-Shot Learning. (arXiv:2305.01239v1 [cs.CV])

    [http://arxiv.org/abs/2305.01239](http://arxiv.org/abs/2305.01239)

    该论文提出了一种名为DRPT的算法，通过分离与循环提示调整的方式来优化记号参数，使得在组合零样本学习中所采用的视觉-语言模型能够更有效地进行识别。实验表明，DRPT在三个基准测试数据集上取得了最先进的性能表现。

    

    组合零样本学习旨在识别由已知知识组成的新概念，没有训练样本。标准的组合零样本学习要么识别视觉原语，要么增强看不见的组合实体，导致状态和对象原语之间的纠缠无法完全利用。本文提出了一种新的分离和循环提示调整框架DRPT，通过学习提示中的词汇嵌入状态和对象原语，并在看到的组合中调整它们，优化记号参数。相比于联合调整状态和对象，我们采用了分离和循环的调整策略，抑制了纠缠所引起的牵引力，并逐步优化了记号参数，从而实现了更有效和高效的组合零样本学习模型。在三个基准数据集上的实验证明，DRPT实现了最先进的CZSL性能，超过现有方法的显著差距。

    Compositional Zero-shot Learning (CZSL) aims to recognize novel concepts composed of known knowledge without training samples. Standard CZSL either identifies visual primitives or enhances unseen composed entities, and as a result, entanglement between state and object primitives cannot be fully utilized. Admittedly, vision- language models (VLMs) could naturally cope with CZSL through tuning prompts, while uneven entanglement leads prompts to be dragged into local optimum. In this paper, we take a further step to introduce a novel Disentangled and Recurrent Prompt Tuning framework termed DRPT to better tap the potential of VLMs in CZSL. Specifically, the state and object primitives are deemed as learnable tokens of vocabulary embedded in prompts and tuned on seen compositions. Instead of jointly tuning state and object, we devise a disentangled and recurrent tuning strategy to suppress the traction force caused by entanglement and gradually optimize the token parameters, leading to a 
    
[^35]: 触发词作为后门攻击的触发器：检查语言模型的脆弱性

    Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])

    [http://arxiv.org/abs/2305.01219](http://arxiv.org/abs/2305.01219)

    本研究提出一种新颖有效的“ProAttack”方法来执行干净标签的后门攻击，使用的是提示本身作为触发器。该方法不需要外部触发器，并确保毒瘤数据的标注正确，提高了后门攻击的隐蔽性，相比于现有的后门攻击方法有显著提升。

    

    基于提示的学习范例弥合了预训练和微调之间的差距，在几个NLP任务中取得了最先进的性能，尤其是在少样本情况下。尽管应用广泛，但基于提示的学习容易受到后门攻击。文本后门攻击旨在通过注入触发器并修改标签来在模型中引入有针对性的漏洞。然而，由于触发器的存在和毒瘤数据标注不正确等缺陷，这种攻击存在异常的自然语言表达。在本研究中，我们提出了一种新颖有效的“ProAttack”方法，基于提示来执行干净标签的后门攻击，使用的是提示本身作为触发器。我们的方法不需要外部触发器，并确保毒瘤数据的标注正确，提高了后门攻击的隐蔽性。通过在丰富的资源和少样本文本语料库上的广泛实验，我们证明了ProAttack方法在保持干净数据一致性的同时显著优于现有的后门攻击方式。

    The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text c
    
[^36]: MultiLegalSBD：一个多语言法律句子边界检测数据集

    MultiLegalSBD: A Multilingual Legal Sentence Boundary Detection Dataset. (arXiv:2305.01211v1 [cs.CL])

    [http://arxiv.org/abs/2305.01211](http://arxiv.org/abs/2305.01211)

    本文介绍了一个多语言法律句子边界检测数据集MultiLegalSBD，包括6种语言的130,000个注释句子。在该数据集上，现有的SBD模型的表现不佳。作者训练了基于CRF、BiLSTM-CRF和transformers的单语和多语模型，并展示了在该领域中最先进的性能。他们的多语模型在零-shot测试中优于所有基线。

    

    句子边界检测是自然语言处理的基础之一，不正确的分割会严重影响下游任务的输出质量。对于算法来说是一个具有挑战性的任务，尤其对于法律领域，因为使用的复杂句子结构各不相同。本文精心策划了一个多语种法律数据集，包括6种语言的130,000个注释句子。我们的实验结果表明，现有的SBD模型在多语种法律数据上的性能表现不佳。我们训练和测试了基于CRF、BiLSTM-CRF和transformers的单语和多语模型，展示了最先进的性能。我们还展示了我们的多语模型在葡萄牙语测试集的零-shot设置中优于所有基线。为了鼓励社区进一步的研究和发展，我们已经公开了我们的数据集、模型和代码。

    Sentence Boundary Detection (SBD) is one of the foundational building blocks of Natural Language Processing (NLP), with incorrectly split sentences heavily influencing the output quality of downstream tasks. It is a challenging task for algorithms, especially in the legal domain, considering the complex and different sentence structures used. In this work, we curated a diverse multilingual legal dataset consisting of over 130'000 annotated sentences in 6 languages. Our experimental results indicate that the performance of existing SBD models is subpar on multilingual legal data. We trained and tested monolingual and multilingual models based on CRF, BiLSTM-CRF, and transformers, demonstrating state-of-the-art performance. We also show that our multilingual models outperform all baselines in the zero-shot setting on a Portuguese test set. To encourage further research and development by the community, we have made our dataset, models, and code publicly available.
    
[^37]: Chronosymbolic Learning: 结合符号推理与归纳学习的有效CHC求解方法

    Chronosymbolic Learning: Efficient CHC Solving with Symbolic Reasoning and Inductive Learning. (arXiv:2305.01206v1 [cs.LO])

    [http://arxiv.org/abs/2305.01206](http://arxiv.org/abs/2305.01206)

    Chronosymbolic Learning是一个简单而有效的框架，将符号推理和数据驱动方法相结合，用于高效地解决CHC系统。实验证明它在288个基准测试上表现出优异的结果，包括许多具有非线性整数算术的实例。

    

    CHC (Constrained Horn Clauses)的求解是许多验证和分析任务的基本挑战。数据驱动法在提高CHC求解效率方面显示出巨大的潜力，同时避免了手动创建和调整各种启发式方法的繁琐工作。但数据驱动的CHC求解器与基于符号推理的求解器之间存在巨大的性能差距。在这项工作中，我们开发了一个简单而有效的框架，"Chronosymbolic Learning"，它将符号信息和数值数据点统一起来，将CHC系统高效地求解。我们还展示了Chronosymbolic Learning的一个简单实例，其中包括一个数据驱动学习器和一个BMC样式的推理器。尽管该工具非常简单，但实验结果表明其效力和健壮性。它在由288个基准测试组成的数据集上胜过了最先进的CHC求解器，其中包括许多包含非线性整数算术的实例。

    Solving Constrained Horn Clauses (CHCs) is a fundamental challenge behind a wide range of verification and analysis tasks. Data-driven approaches show great promise in improving CHC solving without the painstaking manual effort of creating and tuning various heuristics. However, a large performance gap exists between data-driven CHC solvers and symbolic reasoning-based solvers. In this work, we develop a simple but effective framework, "Chronosymbolic Learning", which unifies symbolic information and numerical data points to solve a CHC system efficiently. We also present a simple instance of Chronosymbolic Learning with a data-driven learner and a BMC-styled reasoner. Despite its great simplicity, experimental results show the efficacy and robustness of our tool. It outperforms state-of-the-art CHC solvers on a dataset consisting of 288 benchmarks, including many instances with non-linear integer arithmetics.
    
[^38]: 通过期望价值理论（EVT）为基础的工具来分析学生对生成式人工智能（GenAI）的看法

    Deconstructing Student Perceptions of Generative AI (GenAI) through an Expectancy Value Theory (EVT)-based Instrument. (arXiv:2305.01186v1 [cs.CY])

    [http://arxiv.org/abs/2305.01186](http://arxiv.org/abs/2305.01186)

    本研究基于EVT理论开发了一份问卷，调查了405名学生对于在高等教育中使用生成式人工智能的看法和意愿。结果显示，学生认为生成式人工智能具有较高的使用价值和较小的感知成本，但在推广应用时需要考虑潜在的长期影响和伦理问题。

    

    本研究旨在探究学生对于在高等教育中使用生成式人工智能的看法与意愿之间的关系。我们采用期望价值理论（EVT）开发了一份问卷，用于测量学生对生成式人工智能的知识、感知价值和感知成本。共有405名学生参与了本研究，确认性因素分析被用来验证构建的因素。结果表明，学生对生成式人工智能的感知价值和使用意愿之间存在强烈的正相关性，而感知成本与使用意愿之间则存在弱负相关性。在我们持续探索生成式人工智能在教育和其他领域的应用影响时，需要仔细考虑潜在的长期影响和可能导致的伦理困境。

    This study examines the relationship between student perceptions and their intention to use generative AI in higher education. Drawing on Expectancy-Value Theory (EVT), a questionnaire was developed to measure students' knowledge of generative AI, perceived value, and perceived cost. A sample of 405 students participated in the study, and confirmatory factor analysis was used to validate the constructs. The results indicate a strong positive correlation between perceived value and intention to use generative AI, and a weak negative correlation between perceived cost and intention to use. As we continue to explore the implications of generative AI in education and other domains, it is crucial to carefully consider the potential long-term consequences and the ethical dilemmas that may arise from widespread adoption.
    
[^39]: 使用大型语言模型在知识图谱上进行复杂逻辑推理

    Complex Logical Reasoning over Knowledge Graphs using Large Language Models. (arXiv:2305.01157v1 [cs.LO])

    [http://arxiv.org/abs/2305.01157](http://arxiv.org/abs/2305.01157)

    本文提出了一种使用大型语言模型的解耦方法，将复杂的知识图谱推理形式化为上下文知识图搜索和抽象逻辑查询推理的组合，与现有方法相比，它在多个逻辑查询结构的标准基准数据集上都表现出更好的性能，并且在更高复杂性的查询中获得了显着的性能提升。

    

    在知识图谱上进行推理是一项具有挑战性的任务，它需要对实体之间的复杂关系以及它们之间的基础逻辑进行深入理解。当前的方法依赖于学习几何来嵌入实体的向量空间进行逻辑查询操作，但是它们在复杂查询和特定数据集表示方面表现不佳。本文提出了一种新颖的解耦方法，称为基于语言引导的知识图谱抽象推理（LARK），将复杂的知识图谱推理形式化为上下文知识图搜索和抽象逻辑查询推理的组合，以分别利用图形提取算法和大型语言模型的优势。我们的实验表明，所提出的方法在多个逻辑查询结构的标准基准数据集上优于现有的知识图谱推理方法，在更高复杂性的查询中获得了显着的性能提升。

    Reasoning over knowledge graphs (KGs) is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations. Current approaches rely on learning geometries to embed entities in vector space for logical query operations, but they suffer from subpar performance on complex queries and dataset-specific representations. In this paper, we propose a novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), that formulates complex KG reasoning as a combination of contextual KG search and abstract logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively. Our experiments demonstrate that the proposed approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity. Furthermore, we show t
    
[^40]: 基于知识图谱的卷积神经网络在推荐系统中的应用

    Ripple Knowledge Graph Convolutional Networks For Recommendation Systems. (arXiv:2305.01147v1 [cs.IR])

    [http://arxiv.org/abs/2305.01147](http://arxiv.org/abs/2305.01147)

    本文介绍了一种基于知识图谱的深度学习模型RKGCN，它能够动态分析用户的偏好并推荐出合适的物品。该模型在包括电影、书籍和音乐在内的三个真实世界的数据集上比5个基准模型表现更好。

    

    最近已经证明，使用知识图谱来辅助深度学习模型进行推荐决策能有效提高模型的可解释性和准确性。本文介绍了一种端到端的深度学习模型，命名为RKGCN，它动态分析每个用户的偏好，并推荐出合适的物品。它在物品和用户双方面利用知识图谱来丰富它们的表示，最大化知识图谱中丰富的信息的利用。 RKGCN能够在三种不同的场景下提供更个性化和相关的推荐。实验结果表明，在包括电影、书籍和音乐在内的三个真实世界的数据集上，我们的模型比5个基准模型更有效。

    Using knowledge graphs to assist deep learning models in making recommendation decisions has recently been proven to effectively improve the model's interpretability and accuracy. This paper introduces an end-to-end deep learning model, named RKGCN, which dynamically analyses each user's preferences and makes a recommendation of suitable items. It combines knowledge graphs on both the item side and user side to enrich their representations to maximize the utilization of the abundant information in knowledge graphs. RKGCN is able to offer more personalized and relevant recommendations in three different scenarios. The experimental results show the superior effectiveness of our model over 5 baseline models on three real-world datasets including movies, books, and music.
    
[^41]: 不同时间图神经网络配置在动态图上的分析。

    Analysis of different temporal graph neural network configurations on dynamic graphs. (arXiv:2305.01128v1 [cs.LG])

    [http://arxiv.org/abs/2305.01128](http://arxiv.org/abs/2305.01128)

    本文针对动态图上的时空依赖结构进行了分析，并比较了不同TGN模型在预测任务上的有效性。通过广泛的消融实验，探讨了不同设计选择对预测准确性的影响。

    

    近年来，人们对使用图神经网络（GNN）分析动态图（随时间演变的图）的兴趣越来越大。然而，不同的时间图神经网络（TGN）配置如何影响动态图上的预测准确性仍存在不足之处。此外，对于这些TGN模型的基准数据集的搜索仍在进行中。近期，Pytorch Geometric Temporal提供了一些基准数据集，但这些数据集大多数还没有用不同的TGN模型进行分析以建立最先进的状况。因此，本项目旨在通过对动态图上的时空依赖结构学习进行定性分析，并比较选定TGN模型在节点和边预测任务上的有效性，以及对最佳表现TGN模型的不同变体进行广泛的消融实验，以探索不同设计选择对预测准确性的影响。

    In recent years, there has been an increasing interest in the use of graph neural networks (GNNs) for analyzing dynamic graphs, which are graphs that evolve over time. However, there is still a lack of understanding of how different temporal graph neural network (TGNs) configurations can impact the accuracy of predictions on dynamic graphs. Moreover, the hunt for benchmark datasets for these TGNs models is still ongoing. Up until recently, Pytorch Geometric Temporal came up with a few benchmark datasets but most of these datasets have not been analyzed with different TGN models to establish the state-of-the-art. Therefore, this project aims to address this gap in the literature by performing a qualitative analysis of spatial-temporal dependence structure learning on dynamic graphs, as well as a comparative study of the effectiveness of selected TGNs on node and edge prediction tasks. Additionally, an extensive ablation study will be conducted on different variants of the best-performin
    
[^42]: 人类对自适应机器的适应趋向于博弈论均衡。

    Human adaptation to adaptive machines converges to game-theoretic equilibria. (arXiv:2305.01124v1 [cs.AI])

    [http://arxiv.org/abs/2305.01124](http://arxiv.org/abs/2305.01124)

    本研究测试了机器学习算法与人类进行博弈，发现算法会收敛于博弈论均衡，同时自适应机器的存在会导致人类行为向更加剥削性策略转变。

    

    自适应机器有潜力在多种情境下帮助或干扰人类行为，从认知决策到物理设备援助。因此，了解机器学习算法如何影响人类行为尤为关键，特别是在机器目标与人类目标不一致的情况下。由于人类使用显式和隐含策略不断适应他们的环境，因此当环境中包含自适应机器时，人类和机器将进行博弈。博弈论是一种用于建模两个或多个决策者之间交互的成熟框架，在经济市场和机器算法中被广泛应用。然而，现有方法对个体人类的适应如何受自适应机器交互影响进行假设，而不是通过实证测试。在这里，我们测试了机器学习算法与人类主体进行一般和游戏的玩法。我们的算法收敛于著名的博弈论均衡，尽管人类适应能力和不同的机器学习方法存在差异。我们还发现，自适应机器的存在可以导致人类行为向更加剥削性的策略转变。我们的结果证明在自适应机器的设计中考虑人机交互的重要性，并建议博弈论可以是预测这些情境下人类行为的有用工具。

    Adaptive machines have the potential to assist or interfere with human behavior in a range of contexts, from cognitive decision-making to physical device assistance. Therefore it is critical to understand how machine learning algorithms can influence human actions, particularly in situations where machine goals are misaligned with those of people. Since humans continually adapt to their environment using a combination of explicit and implicit strategies, when the environment contains an adaptive machine, the human and machine play a game. Game theory is an established framework for modeling interactions between two or more decision-makers that has been applied extensively in economic markets and machine algorithms. However, existing approaches make assumptions about, rather than empirically test, how adaptation by individual humans is affected by interaction with an adaptive machine. Here we tested learning algorithms for machines playing general-sum games with human subjects. Our algo
    
[^43]: CSP：针对地理空间视觉表示的自监督对比空间预训练

    CSP: Self-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Representations. (arXiv:2305.01118v1 [cs.CV])

    [http://arxiv.org/abs/2305.01118](http://arxiv.org/abs/2305.01118)

    CSP提出了一个自监督对比学习框架，通过利用地理信息，学习地理位置的有效表示，进一步提高了模型在大规模地理标记图像上的表现。

    

    大量的地理标记图像公开可用，而对象类别等标签则相对稀缺且收集成本高昂。同时，对比学习在各种自然图像和语言任务中取得了巨大成功，仅需很少的带标签数据。然而，现有的方法未能充分利用地理空间信息，这可能是区分视觉上相似的对象的关键。为了在预训练、微调和推理阶段直接利用与图像相关的丰富地理空间信息，我们提出了针对地理标记图像的自监督学习框架 Contrastive Spatial Pre-Training（CSP）。我们使用双编码器分别对图像及其对应的地理位置进行编码，利用对比目标从图像中学习有效的位置表示，这些表示可以转移到下游监督任务，例如图像分类。实验证明，CSP可以提高模型在大规模地理标记图像上的性能。

    Geo-tagged images are publicly available in large quantities, whereas labels such as object classes are rather scarce and expensive to collect. Meanwhile, contrastive learning has achieved tremendous success in various natural image and language tasks with limited labeled data. However, existing methods fail to fully leverage geospatial information, which can be paramount to distinguishing objects that are visually similar. To directly leverage the abundant geospatial information associated with images in pre-training, fine-tuning, and inference stages, we present Contrastive Spatial Pre-Training (CSP), a self-supervised learning framework for geo-tagged im- ages. We use a dual-encoder to separately encode the images and their corresponding geo-locations, and use contrastive objectives to learn effective location representations from images, which can be transferred to downstream supervised tasks such as image classification. Experiments show that CSP can improve model performance on b
    
[^44]: 行人意向预测的局部和全局上下文特征融合

    Local and Global Contextual Features Fusion for Pedestrian Intention Prediction. (arXiv:2305.01111v1 [cs.CV])

    [http://arxiv.org/abs/2305.01111](http://arxiv.org/abs/2305.01111)

    本研究提出了基于局部和全局上下文特征融合的行人意向预测方法，通过分析行人和交通上下文的视觉特征来提高自动驾驶汽车在道路上的安全性。

    

    自动驾驶汽车正在成为未来交通不可或缺的一部分。但是，安全挑战和缺乏可靠性限制了它们在实际环境中的部署。与行人的交互包括“预测行人的穿越意向”值得广泛研究，以提高自动驾驶汽车在道路上的可靠性。本文通过提取和分析行人和交通上下文的时空视觉特征进行有效的意向预测学习。

    Autonomous vehicles (AVs) are becoming an indispensable part of future transportation. However, safety challenges and lack of reliability limit their real-world deployment. Towards boosting the appearance of AVs on the roads, the interaction of AVs with pedestrians including "prediction of the pedestrian crossing intention" deserves extensive research. This is a highly challenging task as involves multiple non-linear parameters. In this direction, we extract and analyse spatio-temporal visual features of both pedestrian and traffic contexts. The pedestrian features include body pose and local context features that represent the pedestrian's behaviour. Additionally, to understand the global context, we utilise location, motion, and environmental information using scene parsing technology that represents the pedestrian's surroundings, and may affect the pedestrian's intention. Finally, these multi-modality features are intelligently fused for effective intention prediction learning. The 
    
[^45]: 合作自适应巡航控制系统中的驾驶员变道预测新模型

    A Novel Model for Driver Lane Change Prediction in Cooperative Adaptive Cruise Control Systems. (arXiv:2305.01096v1 [cs.RO])

    [http://arxiv.org/abs/2305.01096](http://arxiv.org/abs/2305.01096)

    本文研究了合作自适应巡航控制系统中驾驶员变道预测问题。使用车到车通信技术，提供了多个车辆的加速度信息，通过LSTM模型的学习，当周围车辆数量增加时，准确性有所提高。

    

    准确的变道预测可以减少潜在事故，提高道路安全性。自适应巡航控制（ACC），车道偏离警告（LDA）和车道保持辅助（LKA）是一些高级驾驶员辅助系统（ADAS）的传统模块。有了车到车通信（V2V），车辆可以与周围车辆共享交通信息，实现合作自适应巡航控制（CACC）。本文比较了驾驶员变道预测所需的信息类型（位置、速度、加速度）和周围车辆数量。我们使用了HighD数据集，训练了一个LSTM（长短期记忆）模型来预测变道意图。结果表明，随着周围车辆数量的增加，准确性有了显著提高。

    Accurate lane change prediction can reduce potential accidents and contribute to higher road safety. Adaptive cruise control (ACC), lane departure avoidance (LDA), and lane keeping assistance (LKA) are some conventional modules in advanced driver assistance systems (ADAS). Thanks to vehicle-to-vehicle communication (V2V), vehicles can share traffic information with surrounding vehicles, enabling cooperative adaptive cruise control (CACC). While ACC relies on the vehicle's sensors to obtain the position and velocity of the leading vehicle, CACC also has access to the acceleration of multiple vehicles through V2V communication. This paper compares the type of information (position, velocity, acceleration) and the number of surrounding vehicles for driver lane change prediction. We trained an LSTM (Long Short-Term Memory) on the HighD dataset to predict lane change intention. Results indicate a significant improvement in accuracy with an increase in the number of surrounding vehicles and 
    
[^46]: 一种基于LSTM的自适应巡航控制器，在车道变换时可以预测先前车辆的行为

    LSTM-based Preceding Vehicle Behaviour Prediction during Aggressive Lane Change for ACC Application. (arXiv:2305.01095v1 [cs.RO])

    [http://arxiv.org/abs/2305.01095](http://arxiv.org/abs/2305.01095)

    该论文提出了一种基于LSTM的ACC系统，可以学习过去的驾驶经验，适应和预测新情况，并且在模拟驾驶环境中表现良好。

    

    自适应巡航控制（ACC）系统的发展旨在通过自动调节车速来确保与前车安全距离，从而增强车辆的安全性和舒适性。然而，传统的ACC系统无法适应不断变化的驾驶条件和驾驶者的行为。为了解决这个问题，我们提出了一种基于LSTM的ACC系统，可以从以往的驾驶经验中学习，并实时地适应和预测新的情况。该模型是基于实际的高速公路高D数据集构建的，该数据集是利用装备有摄像头的无人机在德国高速公路上获取的。我们在侧面车道前车剪切并强制目标驾驶员减速的激进车道变化时评估了ACC系统。为此，将所提出的系统在模拟驾驶环境中进行了评估，并与馈送前人工神经网络（ANN）模型和模型预测控制（MPC）模型进行了比较。

    The development of Adaptive Cruise Control (ACC) systems aims to enhance the safety and comfort of vehicles by automatically regulating the speed of the vehicle to ensure a safe gap from the preceding vehicle. However, conventional ACC systems are unable to adapt themselves to changing driving conditions and drivers' behavior. To address this limitation, we propose a Long Short-Term Memory (LSTM) based ACC system that can learn from past driving experiences and adapt and predict new situations in real time. The model is constructed based on the real-world highD dataset, acquired from German highways with the assistance of camera-equipped drones. We evaluated the ACC system under aggressive lane changes when the side lane preceding vehicle cut off, forcing the targeted driver to reduce speed. To this end, the proposed system was assessed on a simulated driving environment and compared with a feedforward Artificial Neural Network (ANN) model and Model Predictive Control (MPC) model. The 
    
[^47]: 计算可交换图生成模型的期望模体计数

    Computing Expected Motif Counts for Exchangeable Graph Generative Models. (arXiv:2305.01089v1 [cs.LG])

    [http://arxiv.org/abs/2305.01089](http://arxiv.org/abs/2305.01089)

    本文提出了一种可扩展的估计过程，用于生成混合模型中期望的模体计数。

    

    估计图形统计量的期望值是使用和学习图模型的重要推断任务。本文介绍了一种可扩展的估计过程，用于期望模体计数，这是一种广泛使用的图统计量类型。该程序适用于生成混合模型，这种模型常用于神经网络和贝叶斯方法中的图数据。

    Estimating the expected value of a graph statistic is an important inference task for using and learning graph models. This note presents a scalable estimation procedure for expected motif counts, a widely used type of graph statistic. The procedure applies for generative mixture models of the type used in neural and Bayesian approaches to graph data.
    
[^48]: AI和区块链作为可持续教学和学习工具来应对第四次工业革命

    AI & Blockchain as sustainable teaching and learning tools to cope with the 4IR. (arXiv:2305.01088v1 [cs.CY])

    [http://arxiv.org/abs/2305.01088](http://arxiv.org/abs/2305.01088)

    本论文回顾了现有的AI和区块链在教育领域的研究，探讨了这些技术可以提高个性化学习、安全认证和分散式学习网络的潜力和挑战。此外，本文提出了一个模型，将AI和区块链整合到可持续的教育实践中。总之，AI和区块链技术有望成为可持续教学和学习工具。

    

    第四次工业革命正在改变我们的生活和工作，而教育也不例外。为了应对4IR的挑战，需要创新和可持续的教学和学习工具。人工智能和区块链技术在这方面拥有巨大的潜力，如个性化学习、安全认证和分散式学习网络。本文对AI和区块链在教育中的现有研究进行了回顾，分析了案例研究，并探讨了这些技术的潜在利益和挑战。本文还提出了一种独特的模型，将AI和区块链整合到可持续的教学和学习实践中。未来的研究方向包括需要更多的实证研究和探索伦理和社会影响。本文讨论的关键摘要是，通过提高教育的可访问性、有效性和安全性，AI和区块链有潜力成为可持续的教学和学习工具。

    The Fourth Industrial Revolution (4IR) is transforming the way we live and work, and education is no exception. To cope with the challenges of 4IR, there is a need for innovative and sustainable teaching and learning tools. AI and block chain technologies hold great promise in this regard, with potential benefits such as personalized learning, secure credentialing, and decentralized learning networks. This paper presents a review of existing research on AI and block chain in education, analyzing case studies and exploring the potential benefits and challenges of these technologies. The paper also suggests a unique model for integrating AI and block chain into sustainable teaching and learning practices. Future research directions are discussed, including the need for more empirical studies and the exploration of ethical and social implications. The key summary of this discussion is that, by enhancing accessibility, efficacy, and security in education, AI and blockchain have the potenti
    
[^49]: 专业知识树在集体决策中解决知识局限性问题

    Expertise Trees Resolve Knowledge Limitations in Collective Decision-Making. (arXiv:2305.01063v1 [cs.AI])

    [http://arxiv.org/abs/2305.01063](http://arxiv.org/abs/2305.01063)

    本研究通过引入专业知识树算法，解决了集体决策中专业知识水平不同的问题，并在多个问题上进行了验证。

    

    向决策者提供建议的专家往往会显示出随问题实例变化而变化的专业知识水平。在实践中，这可能导致针对少数情况的次优或歧视性决策。在本文中，我们将这种知识深度和广度的变化建模为将问题空间划分为不同专业知识区域。我们提供了一些新算法，它们明确考虑并适应问题实例与专家知识之间的关系。我们首先提出并强调了一种基于最近邻查询的天真方法的缺点。为了解决这些问题，我们引入了一种新的算法——专业知识树，它构建决策树，使学习者能够选择适当的模型。我们提供了理论见解，并在一系列现有方法被证明不足以解决问题的问题上进行了经验证实了我们的新方法的改进性能。

    Experts advising decision-makers are likely to display expertise which varies as a function of the problem instance. In practice, this may lead to sub-optimal or discriminatory decisions against minority cases. In this work we model such changes in depth and breadth of knowledge as a partitioning of the problem space into regions of differing expertise. We provide here new algorithms that explicitly consider and adapt to the relationship between problem instances and experts' knowledge. We first propose and highlight the drawbacks of a naive approach based on nearest neighbor queries. To address these drawbacks we then introduce a novel algorithm - expertise trees - that constructs decision trees enabling the learner to select appropriate models. We provide theoretical insights and empirically validate the improved performance of our novel approach on a range of problems for which existing methods proved to be inadequate.
    
[^50]: Venn图多标签分类解释与颜色和清晰度增强的糖尿病足潰疡

    Venn Diagram Multi-label Class Interpretation of Diabetic Foot Ulcer with Color and Sharpness Enhancement. (arXiv:2305.01044v1 [cs.CV])

    [http://arxiv.org/abs/2305.01044](http://arxiv.org/abs/2305.01044)

    本文提出了一种基于Venn图解释的多标签分类算法，通过不同的图像增强策略，将四个类别缩减为两个，有效提高了糖尿病足潰疡的多类别分类准确度。

    

    糖尿病足潰疡是糖尿病的严重并发症，如果不正确治疗，可能导致下肢截肢。受2021年糖尿病足潰疡大挑战的启发，研究人员设计了DFU的自动化多类别分类，包括感染、缺血、这两种情况以及以上两种情况均不属于的情况。然而，分类准确度仍然不尽人意。本文提出了一种 Venn 图解释的多标签基于 CNN 的方法，利用不同的图像增强策略，以改进多类别 DFU 分类。我们建议将这四个类别减少为两个类别，因为这两个类别的伤口可以解释为感染和缺血的同时发生，而无类别伤口则表示缺乏感染和缺血。我们在分类器中引入了一种新颖的 Venn 图表示块，用于解释这两个类别的所有四个类。为使我们的模型更具弹性，我们建议增强感知。

    DFU is a severe complication of diabetes that can lead to amputation of the lower limb if not treated properly. Inspired by the 2021 Diabetic Foot Ulcer Grand Challenge, researchers designed automated multi-class classification of DFU, including infection, ischaemia, both of these conditions, and none of these conditions. However, it remains a challenge as classification accuracy is still not satisfactory. This paper proposes a Venn Diagram interpretation of multi-label CNN-based method, utilizing different image enhancement strategies, to improve the multi-class DFU classification. We propose to reduce the four classes into two since both class wounds can be interpreted as the simultaneous occurrence of infection and ischaemia and none class wounds as the absence of infection and ischaemia. We introduce a novel Venn Diagram representation block in the classifier to interpret all four classes from these two classes. To make our model more resilient, we propose enhancing the perceptual 
    
[^51]: CLIP-S$^4$: 语言引导的自监督语义分割

    CLIP-S$^4$: Language-Guided Self-Supervised Semantic Segmentation. (arXiv:2305.01040v1 [cs.CV])

    [http://arxiv.org/abs/2305.01040](http://arxiv.org/abs/2305.01040)

    CLIP-S$^4$是一种无需像素级标注和预定义类别的语言引导自监督语义分割方法，利用自监督像素表示学习和视觉-语言模型，支持各种语义分割任务。

    

    传统的语义分割方法常常受到昂贵的像素级标注和预定义类别的限制。本文提出了一种名为CLIP-S$^4$的语言引导自监督语义分割方法，旨在利用自监督像素表示学习和视觉-语言模型，支持各种语义分割任务，无需人工注释和未知类别信息即可完成。

    Existing semantic segmentation approaches are often limited by costly pixel-wise annotations and predefined classes. In this work, we present CLIP-S$^4$ that leverages self-supervised pixel representation learning and vision-language models to enable various semantic segmentation tasks (e.g., unsupervised, transfer learning, language-driven segmentation) without any human annotations and unknown class information. We first learn pixel embeddings with pixel-segment contrastive learning from different augmented views of images. To further improve the pixel embeddings and enable language-driven semantic segmentation, we design two types of consistency guided by vision-language models: 1) embedding consistency, aligning our pixel embeddings to the joint feature space of a pre-trained vision-language model, CLIP; and 2) semantic consistency, forcing our model to make the same predictions as CLIP over a set of carefully designed target classes with both known and unknown prototypes. Thus, CL
    
[^52]: 无特定模型泛化难度度量

    Model-agnostic Measure of Generalization Difficulty. (arXiv:2305.01034v1 [cs.LG])

    [http://arxiv.org/abs/2305.01034](http://arxiv.org/abs/2305.01034)

    该论文提出了第一个无特定模型的、量化机器学习测试泛化难度的方法——归纳偏差复杂度度量。该方法量化了在任务上良好泛化所需的总信息量与数据提供的信息量之差，通常需要在许多维度上泛化的任务比涉及更少维度但要求更多细节的任务要困难得多。

    

    机器学习算法的度量是其可以执行的任务难度，足够困难的任务是强大机器学习模型的关键驱动因素。然而，量化机器学习测试的泛化难度一直是具有挑战性的。我们提出了据我们所知的第一个对任务固有泛化难度的无特定模型的度量。我们的归纳偏差复杂度度量量化了在任务上良好泛化所需的总信息量与数据提供的信息量之差。通过测量适合训练数据的假设在任务中泛化的分数占据的容积，来实现这一点。它与模型必须泛化的空间的内在维数成指数比例，但仅在每个维度的分辨率上呈多项式比例，表明需要在许多维度上泛化的任务比涉及更少维度的更多细节的任务要困难得多。

    The measure of a machine learning algorithm is the difficulty of the tasks it can perform, and sufficiently difficult tasks are critical drivers of strong machine learning models. However, quantifying the generalization difficulty of machine learning benchmarks has remained challenging. We propose what is to our knowledge the first model-agnostic measure of the inherent generalization difficulty of tasks. Our inductive bias complexity measure quantifies the total information required to generalize well on a task minus the information provided by the data. It does so by measuring the fractional volume occupied by hypotheses that generalize on a task given that they fit the training data. It scales exponentially with the intrinsic dimensionality of the space over which the model must generalize but only polynomially in resolution per dimension, showing that tasks which require generalizing over many dimensions are drastically more difficult than tasks involving more detail in fewer dimen
    
[^53]: 将统计语言模型作为语用推理的评估器

    Evaluating statistical language models as pragmatic reasoners. (arXiv:2305.01020v1 [cs.CL])

    [http://arxiv.org/abs/2305.01020](http://arxiv.org/abs/2305.01020)

    本文评估了大型语言模型推断语用话语的能力。作者通过表述有关等级形容词“强”的门槛估计来测试 LLM 的性能，并发现 LLM 可以生成具有上下文依赖性、类人的分布，但在组合方面存在困难。

    

    沟通语言和预期意义之间的关系通常是概率性的，而且对语境非常敏感。许多策略试图估计这种映射，通常利用基于贝叶斯递归模型的通信方式。与此同时，大型语言模型 (LLMs) 被越来越多地应用于语义分析应用程序，任务是从自然语言推断逻辑表示。虽然现有的 LLM 探索主要局限于字面上的语言使用，但在这项工作中，我们评估了 LLM 推断语用话语的能力。具体而言，我们探讨了基于“强”这个等级形容词的门槛估计的情况，从具有强度先验条件的语境出发，然后扩展到限定、否定、极性反转和类比组合。我们发现，LLMs 可以推导出与语境相关的类人分布，涉及到几个复杂语用话语的解释，但在组合上还有困难。

    The relationship between communicated language and intended meaning is often probabilistic and sensitive to context. Numerous strategies attempt to estimate such a mapping, often leveraging recursive Bayesian models of communication. In parallel, large language models (LLMs) have been increasingly applied to semantic parsing applications, tasked with inferring logical representations from natural language. While existing LLM explorations have been largely restricted to literal language use, in this work, we evaluate the capacity of LLMs to infer the meanings of pragmatic utterances. Specifically, we explore the case of threshold estimation on the gradable adjective ``strong'', contextually conditioned on a strength prior, then extended to composition with qualification, negation, polarity inversion, and class comparison. We find that LLMs can derive context-grounded, human-like distributions over the interpretations of several complex pragmatic utterances, yet struggle composing with n
    
[^54]: 三元瞬时噪声逻辑系统

    Ternary Instantaneous Noise-based Logic. (arXiv:2305.00984v1 [cs.ET])

    [http://arxiv.org/abs/2305.00984](http://arxiv.org/abs/2305.00984)

    本文提出了一种三元逻辑系统，其中包含不确定的比特值和不存在的比特（真空态），与标准的二元逻辑系统相比有着显著的优势，且可以在不改变二进制算法的情况下使用。

    

    本论文提出了一种三值瞬时噪声逻辑的可能表示方法。第三个值是不确定的比特值，在人工智能应用中可能会有用。同时，还有第四个值，它可以表示不存在的比特（真空态），对于所有比特来说，它是相同的（1个数值），但却是所有比特的压缩态。一些逻辑门也进行了探讨。三元宇宙与标准的二元宇宙相比有着显著的优势：在任何时钟周期内，其幅度永远不为零。所有已知的二进制逻辑门在二元比特值上的工作方式与先前相同，因此旧的二进制算法可以在三元系统中运行，而不需要改变，并且不会出现宇宙零值所带来的问题。

    One of the possible representations of three-valued instantaneous noise-based logic is proposed. The third value is an uncertain bit value, which can be useful in artificial intelligence applications. There is a forth value, too, that can represent a non-existing bit (vacuum-state) that is the same (1 numeric value) for all bits, however that is a squeezed state common for all bits. Some logic gates are explored. The ternary Universe has a significant advantage compared to the standard binary one: its amplitude is never zero during any clock period. All the known binary logic gates work for the binary bit values in the same way as earlier therefore the former binary algorithms can be run in the ternary system with no change and without the problems posed by zero values of the Universe.
    
[^55]: 工控系统中针对异常检测的双阶段双Copula方法

    Two-phase Dual COPOD Method for Anomaly Detection in Industrial Control System. (arXiv:2305.00982v1 [cs.LG])

    [http://arxiv.org/abs/2305.00982](http://arxiv.org/abs/2305.00982)

    本文提出了一个双阶段基于双Copula的离群检测方法，该方法具有优异的透明度、可解释性和计算效率，并在实际工控系统数据集上具有卓越的性能。

    

    水处理设施和电站等关键基础设施依赖于工业控制系统（ICS）进行监控和控制，这使它们容易受到网络攻击和系统故障的影响。传统的ICS异常检测方法缺乏透明度和可解释性，这使实践者难以理解和信任结果。本文提出了一种基于两个阶段的双Copula离群检测方法来应对这些挑战。

    Critical infrastructures like water treatment facilities and power plants depend on industrial control systems (ICS) for monitoring and control, making them vulnerable to cyber attacks and system malfunctions. Traditional ICS anomaly detection methods lack transparency and interpretability, which make it difficult for practitioners to understand and trust the results. This paper proposes a two-phase dual Copula-based Outlier Detection (COPOD) method that addresses these challenges. The first phase removes unwanted outliers using an empirical cumulative distribution algorithm, and the second phase develops two parallel COPOD models based on the output data of phase 1. The method is based on empirical distribution functions, parameter-free, and provides interpretability by quantifying each feature's contribution to an anomaly. The method is also computationally and memory-efficient, suitable for low- and high-dimensional datasets. Experimental results demonstrate superior performance in 
    
[^56]: SelfDocSeg: 一种自我监督视觉文档分割方法

    SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation. (arXiv:2305.00795v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.00795](http://arxiv.org/abs/2305.00795)

    SelfDocSeg提出了一种完全基于视觉的自我监督文档分割方法，通过生成伪布局训练图像编码器学习文档对象的表示和定位，克服了标注数据稀缺的挑战。

    

    文档布局分析是一个众所周知的问题，已经被广泛探索，涵盖了从文本挖掘、识别到基于图形的表示、视觉特征提取等多种解决方案。然而，现有的大部分工作都忽略了标注数据的稀缺性这一关键事实。我们使用自我监督来解决这一挑战，并且与现有的使用文本挖掘和文本标签的自我监督文档分割方法不同，我们使用了完全基于视觉的方法在预训练中生成伪布局，以在没有任何真实标签或其导出物的情况下训练图像编码器，以自我监督的框架中学习文档对象的表示和定位。

    Document layout analysis is a known problem to the documents research community and has been vastly explored yielding a multitude of solutions ranging from text mining, and recognition to graph-based representation, visual feature extraction, etc. However, most of the existing works have ignored the crucial fact regarding the scarcity of labeled data. With growing internet connectivity to personal life, an enormous amount of documents had been available in the public domain and thus making data annotation a tedious task. We address this challenge using self-supervision and unlike, the few existing self-supervised document segmentation approaches which use text mining and textual labels, we use a complete vision-based approach in pre-training without any ground-truth label or its derivative. Instead, we generate pseudo-layouts from the document images to pre-train an image encoder to learn the document object representation and localization in a self-supervised framework before fine-tun
    
[^57]: 使用奇异值分解的深度强化学习中的表征学习和探索

    Representations and Exploration for Deep Reinforcement Learning using Singular Value Decomposition. (arXiv:2305.00654v1 [cs.LG])

    [http://arxiv.org/abs/2305.00654](http://arxiv.org/abs/2305.00654)

    本文提出了一种基于奇异值分解的自动表征学习模型，可以获得保留转换结构的表示形式并捕捉状态访问的相对频率。该方法不需要转移矩阵，可以利用深度网络，适用于部分可观察领域，并且在多任务设置中表现良好。

    

    表现学习和探索是任何深度强化学习代理所面临的关键挑战。本文提供了一种基于奇异值分解的方法，可以用来获得保留域中潜在转换结构的表示形式。有趣的是，我们发现这些表示形式还捕捉了状态访问的相对频率，从而免费提供了伪计数的估计。为了将这种分解方法推广到大规模域，我们提供了一种不需要建立转移矩阵，可以利用深度网络，也允许小批量训练的算法。此外，我们从预测状态表示中吸取灵感，并扩展了我们的分解方法到部分可观察的环境。通过对部分可观察领域的多任务设置进行实验，我们展示了提出的方法不仅可以在DM-Lab-30环境中学习有用的表示形式。

    Representation learning and exploration are among the key challenges for any deep reinforcement learning agent. In this work, we provide a singular value decomposition based method that can be used to obtain representations that preserve the underlying transition structure in the domain. Perhaps interestingly, we show that these representations also capture the relative frequency of state visitations, thereby providing an estimate for pseudo-counts for free. To scale this decomposition method to large-scale domains, we provide an algorithm that never requires building the transition matrix, can make use of deep networks, and also permits mini-batch training. Further, we draw inspiration from predictive state representations and extend our decomposition method to partially observable environments. With experiments on multi-task settings with partially observable domains, we show that the proposed method can not only learn useful representation on DM-Lab-30 environments (that have inputs
    
[^58]: 分解增强推理的自我评估引导解码

    Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])

    [http://arxiv.org/abs/2305.00633](http://arxiv.org/abs/2305.00633)

    本论文提出了一种通过自我评估引导解码提高推理的方法，使用经过校准的自动标准探索推理搜索空间，使搜索能够产生更高质量的最终预测结果；使用自我评估引导的随机束搜索在产生推理链的质量和多样性之间平衡权衡，适应多数投票，并且可以准确判断逻辑错误，提高一致性和鲁棒性。

    

    我们提出了一种有效的提示方法，通过随机束搜索结合自我评估引导。我们的方法使用经过校准的自动标准探索推理搜索空间。这使得有效搜索能够产生更高质量的最终预测结果。使用自我评估引导的随机束搜索，我们在产生推理链的质量和多样性之间平衡权衡，从而能够适应多数投票，并在GSM8K、AQUA和StrategyQA基准测试中以少量示例准确性分别超越对应的Codex-backboned基线$6.34\%$、$9.56\%$和$5.46\%$。对我们的分解式推理分析发现，它可以指出逻辑错误并导致更高的一致性和鲁棒性。

    We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality--diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness.
    
[^59]: Rubik光学神经网络：具有物理感知旋转结构的多任务学习

    Rubik's Optical Neural Networks: Multi-task Learning with Physics-aware Rotation Architecture. (arXiv:2304.12985v1 [cs.LG])

    [http://arxiv.org/abs/2304.12985](http://arxiv.org/abs/2304.12985)

    RubikONNs利用光学系统的物理特性，通过旋转硬件编码多个前馈函数，为ONNs实现高效的多任务学习提供了新的策略。

    

    最近，有越来越多的研究工作在推进光学神经网络（ONNs），在功率效率，并行性和计算速度方面，ONNs带来了机器学习（ML）方面的显着优势。

    Recently, there are increasing efforts on advancing optical neural networks (ONNs), which bring significant advantages for machine learning (ML) in terms of power efficiency, parallelism, and computational speed. With the considerable benefits in computation speed and energy efficiency, there are significant interests in leveraging ONNs into medical sensing, security screening, drug detection, and autonomous driving. However, due to the challenge of implementing reconfigurability, deploying multi-task learning (MTL) algorithms on ONNs requires re-building and duplicating the physical diffractive systems, which significantly degrades the energy and cost efficiency in practical application scenarios. This work presents a novel ONNs architecture, namely, \textit{RubikONNs}, which utilizes the physical properties of optical systems to encode multiple feed-forward functions by physically rotating the hardware similarly to rotating a \textit{Rubik's Cube}. To optimize MTL performance on Rubi
    
[^60]: 评估文档图像分类的对抗性鲁棒性

    Evaluating Adversarial Robustness on Document Image Classification. (arXiv:2304.12486v1 [cs.CV])

    [http://arxiv.org/abs/2304.12486](http://arxiv.org/abs/2304.12486)

    本文对文档图像分类任务中的对抗性攻击进行了研究和评估，通过对ResNet50和EfficientNetB0模型架构进行对抗训练、JPEG输入压缩和灰度输入转换等方法，提高了模型的鲁棒性。

    

    近年来，对抗攻击和防御在计算机视觉系统上引起了越来越多的关注，但至今大部分研究仅限于图像。然而，许多人工智能模型实际上处理的是文档数据，这与真实世界的图像非常不同。因此，在本研究中，我们尝试将对抗攻击哲学应用于文献和自然数据，并保护模型免受此类攻击。我们的研究集中在无目标基于梯度、基于转移和基于分数的攻击上，并评估对抗训练、JPEG输入压缩和灰度输入转换对ResNet50和EfficientNetB0模型架构鲁棒性的影响。据我们所知，社区没有进行这样的研究以研究这些攻击对文档图像分类任务的影响。

    Adversarial attacks and defenses have gained increasing interest on computer vision systems in recent years, but as of today, most investigations are limited to images. However, many artificial intelligence models actually handle documentary data, which is very different from real world images. Hence, in this work, we try to apply the adversarial attack philosophy on documentary and natural data and to protect models against such attacks. We focus our work on untargeted gradient-based, transfer-based and score-based attacks and evaluate the impact of adversarial training, JPEG input compression and grey-scale input transformation on the robustness of ResNet50 and EfficientNetB0 model architectures. To the best of our knowledge, no such work has been conducted by the community in order to study the impact of these attacks on the document image classification task.
    
[^61]: DocParser：从视觉丰富的文档中实现端到端的无OCR信息提取

    DocParser: End-to-end OCR-free Information Extraction from Visually Rich Documents. (arXiv:2304.12484v1 [cs.CV])

    [http://arxiv.org/abs/2304.12484](http://arxiv.org/abs/2304.12484)

    DocParser是一种无OCR的端到端信息提取模型，可更好地提取判别性字符特征，并在视觉丰富的文档数据集上实现最新的结果。

    

    从视觉丰富的文档中提取信息是一项具有挑战性的任务，由于其在几个基于文档控制的应用程序中的重要性和其广泛的商业价值，近年来越来越受到关注。迄今为止，在这个主题上进行的大部分研究工作都遵循两步法。首先，他们使用现成的光学字符识别（OCR）引擎读取文本，然后从所获得的文本中提取感兴趣的字段。这些方法的主要缺点是它们依赖于外部OCR系统，这可能会对性能和计算速度产生负面影响。最近，出现了一些无OCR方法来解决上述问题。受到其有希望的结果的启示，我们在本文中提出一种名为DocParser的OCR-free端到端信息提取模型。它通过其更好地提取判别性字符特征的能力不同于先前的端到端方法。DocParser在视觉丰富的文档数据集上实现了最新的结果，超过了依赖于OCR引擎的先前方法。

    Information Extraction from visually rich documents is a challenging task that has gained a lot of attention in recent years due to its importance in several document-control based applications and its widespread commercial value. The majority of the research work conducted on this topic to date follow a two-step pipeline. First, they read the text using an off-the-shelf Optical Character Recognition (OCR) engine, then, they extract the fields of interest from the obtained text. The main drawback of these approaches is their dependence on an external OCR system, which can negatively impact both performance and computational speed. Recent OCR-free methods were proposed to address the previous issues. Inspired by their promising results, we propose in this paper an OCR-free end-to-end information extraction model named DocParser. It differs from prior end-to-end approaches by its ability to better extract discriminative character features. DocParser achieves state-of-the-art results on v
    
[^62]: 多模态三维物体检测的稀疏到密集体素区域融合方法

    SDVRF: Sparse-to-Dense Voxel Region Fusion for Multi-modal 3D Object Detection. (arXiv:2304.08304v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.08304](http://arxiv.org/abs/2304.08304)

    提出了一种新的稀疏到密集的体素区域加强融合方法，通过动态投影每个体素内部的稀疏局部点云获得体素区域，更好地对齐和避免背景噪声问题，并通过多尺度融合方法极大地提高了三维物体检测性能。

    

    在自动驾驶感知任务中，多模态方法成为趋势，主要因为LiDAR点云和图像数据的补充特性。然而，以往方法的性能通常受到点云稀疏或者LiDAR和相机之间偏差导致的噪声问题的限制。为了解决这两个问题，我们提出了一个新的概念，Voxel Region (VR)，通过动态投影每个体素中的稀疏局部点云来获得。我们提出了一种新颖的融合方法，称为Sparse-to-Dense Voxel Region Fusion (SDVRF)，具体而言，将VR内部较多的图像特征图像素收集起来，以补充从稀疏点中提取的体素特征，实现更密集的融合。与先前的方法不同，我们的动态区域生成策略实现了更好的对齐，并避免引入太多的背景噪声。此外，我们提出了一种多尺度融合方法来增强我们的方法的性能。在KITTI数据集上的大量实验表明，我们的方法在各种评估指标下显著优于现有最先进方法。

    In the perception task of autonomous driving, multi-modal methods have become a trend due to the complementary characteristics of LiDAR point clouds and image data. However, the performance of previous methods is usually limited by the sparsity of the point cloud or the noise problem caused by the misalignment between LiDAR and the camera. To solve these two problems, we present a new concept, Voxel Region (VR), which is obtained by projecting the sparse local point clouds in each voxel dynamically. And we propose a novel fusion method, named Sparse-to-Dense Voxel Region Fusion (SDVRF). Specifically, more pixels of the image feature map inside the VR are gathered to supplement the voxel feature extracted from sparse points and achieve denser fusion. Meanwhile, different from prior methods, which project the size-fixed grids, our strategy of generating dynamic regions achieves better alignment and avoids introducing too much background noise. Furthermore, we propose a multi-scale fusion
    
[^63]: 奖励是否合理？在 MACHIAVELLI 基准测试中衡量奖励与道德行为之间的权衡

    Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])

    [http://arxiv.org/abs/2304.03279](http://arxiv.org/abs/2304.03279)

    本文介绍了 MACHIAVELLI 基准测试，用于衡量人工智能代理是否表现出马基雅维利行为，发现了最大化奖励和行为的道德性之间存在权衡，并探索了基于语言模型的方法来减轻这种权衡。

    

    传统上，人工智能代理被训练成最大化奖励，这可能会激励追求权力和欺骗行为，类似于语言模型中的下一个标记预测可能会激励有害行为。那么代理是否自然而然地学会了马基雅维利行为？我们如何在 GPT-4 等通用模型中衡量这些行为呢？为回答这些问题，我们引入了 MACHIAVELLI 基准测试，该测试涵盖了超过一百万个多样化的情景，重点关注社会决策制定，用于衡量人工代理是否表现出马基雅维利行为。我们数学化了数十种有害行为，并使用我们的注释来评估代理倾向于追求权力，造成功能不良和违反伦理的倾向。我们观察到最大化奖励和行为的道德性之间存在一些紧张关系。为了改善这种权衡，我们研究了基于语言模型的方法，以使代理趋向于采取更少的有害行为。我们的结果显示，MACHIAVELLI 是评估人工代理马基雅维利行为水平的有用基准测试。

    Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results sh
    
[^64]: Fourier分析与运行时间分析相遇：关于“高原”问题的精准运行时间分析研究

    Fourier Analysis Meets Runtime Analysis: Precise Runtimes on Plateaus. (arXiv:2302.08021v2 [cs.NE] CROSS LISTED)

    [http://arxiv.org/abs/2302.08021](http://arxiv.org/abs/2302.08021)

    本文提出了一种基于傅里叶分析的新方法，用于分析进化算法在高原上消耗的时间，通过算法在针尖问题和新基准问题上的应用研究，确定了最佳变异率，并证明了$(1+1)$进化算法相对于一般随机搜索启发式算法在高原问题上具有惊人的效率。

    

    本文提出了一种基于离散傅里叶分析的新方法，用于分析进化算法在高原上消耗的时间。这种方法立即证明了由Garnier，Kallel，和Schoenauer（1999）提出的关于针尖问题上预期运行时间的经典估计。我们还将该方法用于分析一个新的基准问题：由$n/\ell$个有效大小为$2^\ell-1$的高原组成，需要以LeadingOnes的方式顺序地对其进行优化。利用我们的新方法，我们确定了静态和与适应度相关的变异率的精确预期运行时间。我们还确定了渐进最优的静态和与适应度相关的变异率。对于$\ell = o(n)$，最优的静态变异率近似为$1.59/n$。当找到前$k$个与适应度相关的二进制位时，最优的适应度相关变异率渐进为$1/(k+1)$。到目前为止，这些结果仅证明了预期的运行时间，表明该新基准问题非常困难，而$(1+1)$进化算法在高原问题上与一般的随机搜索启发式算法相比效率惊人。

    We propose a new method based on discrete Fourier analysis to analyze the time evolutionary algorithms spend on plateaus. This immediately gives a concise proof of the classic estimate of the expected runtime of the $(1+1)$ evolutionary algorithm on the Needle problem due to Garnier, Kallel, and Schoenauer (1999).  We also use this method to analyze the runtime of the $(1+1)$ evolutionary algorithm on a new benchmark consisting of $n/\ell$ plateaus of effective size $2^\ell-1$ which have to be optimized sequentially in a LeadingOnes fashion.  Using our new method, we determine the precise expected runtime both for static and fitness-dependent mutation rates. We also determine the asymptotically optimal static and fitness-dependent mutation rates. For $\ell = o(n)$, the optimal static mutation rate is approximately $1.59/n$. The optimal fitness dependent mutation rate, when the first $k$ fitness-relevant bits have been found, is asymptotically $1/(k+1)$. These results, so far only prove
    
[^65]: 语言模型分析本体子类推断

    Language Model Analysis for Ontology Subsumption Inference. (arXiv:2302.06761v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.06761](http://arxiv.org/abs/2302.06761)

    本文研究了语言模型对本体子类推断的理解能力，提出了一套涉及原子概念和复合概念的推理任务，并证明语言模型对子类推断背景知识的记忆相对较少，但在给定少量样本的情况下可显著提高准确率。

    

    最近，研究人员开始探究预训练的语言模型是否能够作为知识库的替代。然而，现有的研究都关注于简单的三元组关系型知识库，忽略了更为复杂、逻辑为基础、概念化的 OWL 本体等知识库。为了研究语言模型对于本体的了解，我们提出 OntoLAMA，它包含基于推理的一系列测试任务和数据集，从涉及原子概念和复合概念的子类推断公理出发。我们对不同领域和规模的本体进行了大量实验，结果表明，相比传统的自然语言推理，语言模型对子类推断的背景知识记忆相对较少，但是在给定少量样本的情况下，可以显著提高子类推断的准确率。我们将公开源码和数据集。

    Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets.
    
[^66]: 探索基于数值先验的广义CP分解低秩张量补全算法

    Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition. (arXiv:2302.05881v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05881](http://arxiv.org/abs/2302.05881)

    本文提出了一种新的方法框架GCDTC，利用数值先验和广义CP分解实现了更高的低秩张量补全精度；同时介绍了一个算法SPTC，作为该框架的一个实现。在实验中，该方法表现出比现有技术更好的性能。

    

    张量补全在计算机视觉、数据分析和信号处理等领域中具有重要意义。最近，低秩张量补全这一类别的方法得到了广泛研究，对补全张量施加低秩结构。虽然这些方法取得了巨大成功，但尚未考虑到张量元素的数值先验信息。忽略数值先验将导致丢失关于数据的重要信息，因此阻止算法达到最优精度。本研究试图构建一个新的方法框架，名为GCDTC（广义CP分解张量补全），以利用数值先验并实现更高的张量补全精度。在这个新引入的框架中，将广义的CP分解应用于低秩张量补全。本文还提出了一种名为SPTC（平滑泊松张量补全）的算法，用于非负整数张量补全，作为GCDTC框架的一个实现。通过对合成和真实世界数据集的大量实验，证明所提出的方法相比于现有技术具有更优的张量补全性能。

    Tensor completion is important to many areas such as computer vision, data analysis, and signal processing. Enforcing low-rank structures on completed tensors, a category of methods known as low-rank tensor completion has recently been studied extensively. While such methods attained great success, none considered exploiting numerical priors of tensor elements. Ignoring numerical priors causes loss of important information regarding the data, and therefore prevents the algorithms from reaching optimal accuracy. This work attempts to construct a new methodological framework called GCDTC (Generalized CP Decomposition Tensor Completion) for leveraging numerical priors and achieving higher accuracy in tensor completion. In this newly introduced framework, a generalized form of CP Decomposition is applied to low-rank tensor completion. This paper also proposes an algorithm known as SPTC (Smooth Poisson Tensor Completion) for nonnegative integer tensor completion as an instantiation of the G
    
[^67]: 风险敏感的强化学习算法：指数标准的应用

    Risk-Sensitive Reinforcement Learning with Exponential Criteria. (arXiv:2212.09010v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.09010](http://arxiv.org/abs/2212.09010)

    本文介绍了一种风险敏感的强化学习算法，使用指数判据来提高其系统抗干扰性和实用性。作者进行了在模拟和实际机器人上的实验验证，表明该算法能够有效地提高样本效率和执行效果。

    

    尽管风险中性的强化学习已经在很多应用中得到了实验成功，但是这种方法容易受到噪声和系统参数扰动的影响而不够稳健。因此,对风险敏感的强化学习算法进行了研究，以提高其系统抗干扰性，样本效率和实用性。本文介绍了一种新型的无模型风险敏感学习算法，将广泛使用的策略梯度算法进行变体，其实现过程类似。具体来说，本文研究了指数标准对强化学习代理的策略风险敏感性的影响，并开发了蒙特卡罗策略梯度算法和在线(时间差分)演员-评论家算法的变体。分析结果表明，指数标准的使用能够推广常用的特定正则化方法。作者在摆动杆和摆摆杆任务上进行了测试，验证了所提出的算法的实现性能和稳健性。

    While risk-neutral reinforcement learning has shown experimental success in a number of applications, it is well-known to be non-robust with respect to noise and perturbations in the parameters of the system. For this reason, risk-sensitive reinforcement learning algorithms have been studied to introduce robustness and sample efficiency, and lead to better real-life performance. In this work, we introduce new model-free risk-sensitive reinforcement learning algorithms as variations of widely-used Policy Gradient algorithms with similar implementation properties. In particular, we study the effect of exponential criteria on the risk-sensitivity of the policy of a reinforcement learning agent, and develop variants of the Monte Carlo Policy Gradient algorithm and the online (temporal-difference) Actor-Critic algorithm. Analytical results showcase that the use of exponential criteria generalize commonly used ad-hoc regularization approaches. The implementation, performance, and robustness 
    
[^68]: 带每个样本自适应裁剪的差分隐私学习

    Differentially Private Learning with Per-Sample Adaptive Clipping. (arXiv:2212.00328v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00328](http://arxiv.org/abs/2212.00328)

    本文提出了一种差分隐私每个样本自适应裁剪算法DP-PSAC，该算法采用非单调自适应权重函数，根据梯度的历史敏感性自适应裁剪每个样本的梯度幅度，提高了模型的实用性，且保证了隐私。

    

    近年来，AI中的隐私问题一直是吸引研究者和公众关注的话题。差分隐私学习作为一种实现隐私保护AI的方式，可以使AI模型使用差分隐私。现有算法通常通过常量裁剪来限制梯度幅度以实现学习过程的差分隐私。为了解决裁剪常数对模型性能的巨大影响，最新的作品NSGD和Auto-S提出了使用归一化来替代裁剪的创新方法。然而，NSGD和Auto-S等基于归一化的方法依赖于单调权重函数，对小梯度样本施加过量权重并引入额外偏差。本文提出了一种基于非单调自适应权重函数的差分隐私每个样本自适应裁剪（DP-PSAC）算法，它保证了隐私而不牺牲模型性能。我们的DP-PSAC算法根据该样本的历史敏感性自适应裁剪每个样本的梯度幅度，可以显著减少裁剪梯度的方差并提高模型的效用。实验结果表明，与现有方法相比，我们的DP-PSAC算法在真实数据集上实现了竞争性的实用性。

    Privacy in AI remains a topic that draws attention from researchers and the general public in recent years. As one way to implement privacy-preserving AI, differentially private learning is a framework that enables AI models to use differential privacy (DP). To achieve DP in the learning process, existing algorithms typically limit the magnitude of gradients with a constant clipping, which requires carefully tuned due to its significant impact on model performance. As a solution to this issue, latest works NSGD and Auto-S innovatively propose to use normalization instead of clipping to avoid hyperparameter tuning. However, normalization-based approaches like NSGD and Auto-S rely on a monotonic weight function, which imposes excessive weight on small gradient samples and introduces extra deviation to the update. In this paper, we propose a Differentially Private Per-Sample Adaptive Clipping (DP-PSAC) algorithm based on a non-monotonic adaptive weight function, which guarantees privacy w
    
[^69]: 跨语言转移的令人沮丧的简易标签投影

    Frustratingly Easy Label Projection for Cross-lingual Transfer. (arXiv:2211.15613v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.15613](http://arxiv.org/abs/2211.15613)

    本文通过一项广泛的实证研究，对57种语言和三个任务下的跨语言转移进行了研究，并发现优化后的标记-翻译法比传统注释投影方法更有效。

    

    将训练数据翻译成多种语言已成为提高跨语言转移的实际解决方案。对于涉及跨度级别注释（例如信息提取或问题回答）的任务，需要进行额外的标签投影步骤，将已注释的跨度映射到翻译后的文本中。然而，据我们所知，迄今为止尚未对这种方法与基于单词对齐的传统注释投影进行实证分析。在本文中，我们展示了一项对57种语言和三个任务（QA，NER和事件提取）进行广泛的实证研究，以评估两种方法的有效性和局限性，并填补文献中的重要空白。实验结果表明，我们优化后的标记-翻译法比传统注释投影方法更有效。

    Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto the translated texts. Recently, a few efforts have utilized a simple mark-then-translate method to jointly perform translation and projection by inserting special markers around the labeled spans in the original sentence. However, as far as we are aware, no empirical analysis has been conducted on how this approach compares to traditional annotation projection based on word alignment. In this paper, we present an extensive empirical study across 57 languages and three tasks (QA, NER, and Event Extraction) to evaluate the effectiveness and limitations of both methods, filling an important gap in the literature. Experimental results show that our optimized version of mark-then-translate, which we
    
[^70]: 用符号回归实现可解释的科学发现：综述（arXiv:2211.10873v2 [cs.LG] UPDATED）

    Interpretable Scientific Discovery with Symbolic Regression: A Review. (arXiv:2211.10873v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10873](http://arxiv.org/abs/2211.10873)

    符号回归作为一种机器学习方法日益流行，可以从数据中学习简洁且可解释的数学表达式，本综述全面概述了该方法并讨论了其优点和局限性。

    

    符号回归作为从数据中学习简洁且可解释的数学表达式的一种机器学习方法日益流行。虽然传统上采用遗传编程的方式解决这个问题，但近年来，它在深度学习中引起了越来越多的关注，作为一种数据驱动的模型发现方法，在从基础科学到应用科学的各个领域取得了重大进展。本文综述了符号回归方法的结构和全面的概述，并讨论了它们的优点和局限性。

    Symbolic regression is emerging as a promising machine learning method for learning succinct underlying interpretable mathematical expressions directly from data. Whereas it has been traditionally tackled with genetic programming, it has recently gained a growing interest in deep learning as a data-driven model discovery method, achieving significant advances in various application domains ranging from fundamental to applied sciences. This survey presents a structured and comprehensive overview of symbolic regression methods and discusses their strengths and limitations.
    
[^71]: 基于Web的视觉语料库构建用于视觉文档理解

    On Web-based Visual Corpus Construction for Visual Document Understanding. (arXiv:2211.03256v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.03256](http://arxiv.org/abs/2211.03256)

    这篇论文提出了一个基于Web的视觉语料库构建工具（Webvicob），用于从Wikipedia HTML转储文件中构建大规模的、多语言的视觉语料库，通过数据集的使用可以提升视觉文档理解模型的性能。

    

    近年来，关于视觉文档理解（VDU）的研究取得了显著的进展，特别是在自监督学习方法的发展方面。然而，在这个领域面临的一个重要挑战是可公开获取的视觉语料库的有限性，尤其是对于非拉丁语言或资源短缺的语言而言，大规模的图像集合与详细文本注释难以得到。为了解决这一挑战，我们提出了基于Web的视觉语料库构建工具（Webvicob），该工具能够从原始的Wikipedia HTML转储文件中构建大规模的多语言视觉语料库。我们的实验表明，Webvicob生成的数据可用于训练稳健的VDU模型，并在各种下游任务（例如DocVQA和后OCR解析）中表现良好。此外，当使用由Webvicob生成的100万张图像数据集时，我们观察到在DocVQA任务3上相对于COCO-Text数据集的1100万张图像数据集，有超过13％的提升。

    In recent years, research on visual document understanding (VDU) has grown significantly, with a particular emphasis on the development of self-supervised learning methods. However, one of the significant challenges faced in this field is the limited availability of publicly accessible visual corpora or extensive collections of images with detailed text annotations, particularly for non-Latin or resource-scarce languages. To address this challenge, we propose Web-based Visual Corpus Builder (Webvicob), a dataset generator engine capable of constructing large-scale, multilingual visual corpora from raw Wikipedia HTML dumps. Our experiments demonstrate that the data generated by Webvicob can be used to train robust VDU models that perform well on various downstream tasks, such as DocVQA and post-OCR parsing. Furthermore, when using a dataset of 1 million images generated by Webvicob, we observed an improvement of over 13% on the DocVQA Task 3 compared to a dataset of 11 million images fr
    
[^72]: 带环境感知的语言模型生成可执行的动作计划

    Generating Executable Action Plans with Environmentally-Aware Language Models. (arXiv:2210.04964v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.04964](http://arxiv.org/abs/2210.04964)

    本文提出了一种带环境感知的语言模型生成可执行的动作计划的方法，通过集成环境对象和对象关系作为额外输入，设计了新颖的评分函数，生成的可执行计划相对于传统的LLM方法有更高成功率。

    

    近期大规模语言模型在从高层次文本查询中生成机器人代理的动作计划方面表现出了很好的潜力。然而，这些模型通常不考虑机器人的环境，导致生成的计划可能无法执行，因为计划中的动作模糊不清或受到环境限制。本文提出了一种生成带环境感知的可执行动作计划的方法。我们的方法是将环境对象和对象关系作为额外输入集成到LLM动作计划生成中，以提供系统对周围环境的感知，从而生成与场景中存在的物体相对应的计划动作。此外，我们设计了一种新颖的评分函数，帮助系统消除物体实例之间的歧义并考虑它们的状态。我们在一个机器人操作任务上评估了我们的方法，并展示了我们的模型相对于传统的LLM方法生成了更高成功率的可执行计划。

    Large Language Models (LLMs) trained using massive text datasets have recently shown promise in generating action plans for robotic agents from high level text queries. However, these models typically do not consider the robot's environment, resulting in generated plans that may not actually be executable, due to ambiguities in the planned actions or environmental constraints. In this paper, we propose an approach to generate environmentally-aware action plans that agents are better able to execute. Our approach involves integrating environmental objects and object relations as additional inputs into LLM action plan generation to provide the system with an awareness of its surroundings, resulting in plans where each generated action is mapped to objects present in the scene. We also design a novel scoring function that, along with generating the action steps and associating them with objects, helps the system disambiguate among object instances and take into account their states. We ev
    
[^73]: 在线2阶段稳定匹配

    Online 2-stage Stable Matching. (arXiv:2207.02057v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2207.02057](http://arxiv.org/abs/2207.02057)

    该论文研究了一个在线2阶段的稳定匹配问题，提出了能够最优解决该问题的算法。

    

    我们关注一个在线2阶段问题，由以下情况激发：考虑一个学生被分配到大学的系统。有第一轮申请，需要计算第一个稳定匹配$M_1$。然而，有些学生可能会决定离开这个系统（改变计划，去外国大学，或者去一些不在系统中的机构）。然后，在第二轮（在这些删除之后），我们应该计算第二个（最终）稳定匹配$M_2$。由于改变分配是不可取的，目标是将两个稳定的匹配$M_1$和$M_2$之间的离婚/修改次数最小化。那么，我们应该如何选择$M_1$和$M_2$？我们展示了有一个最佳的在线算法来解决这个问题。特别是，我们通过支配性质展示了即使不知道离开系统的学生，我们仍然可以最优地计算出$M_1$。我们将结果推广到输入数据中可能的其他一些修改中。

    We focus on an online 2-stage problem, motivated by the following situation: consider a system where students shall be assigned to universities. There is a first round where some students apply, and a first (stable) matching $M_1$ has to be computed. However, some students may decide to leave the system (change their plan, go to a foreign university, or to some institution not in the system). Then, in a second round (after these deletions), we shall compute a second (final) stable matching $M_2$. As it is undesirable to change assignments, the goal is to minimize the number of divorces/modifications between the two stable matchings $M_1$ and $M_2$. Then, how should we choose $M_1$ and $M_2$? We show that there is an {\it optimal online} algorithm to solve this problem. In particular, thanks to a dominance property, we show that we can optimally compute $M_1$ without knowing the students that will leave the system. We generalize the result to some other possible modifications in the inp
    
[^74]: 了解你的听众：用听众减法专门化基于上下文的语言模型

    Know your audience: specializing grounded language models with listener subtraction. (arXiv:2206.08349v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.08349](http://arxiv.org/abs/2206.08349)

    本文介绍了一种利用多智能体图像参照游戏自适应不同听众的目标任务描述的方法，并通过微调 CLIP 视觉编码器和大型语言模型之间的适配器，在适应听众的语言上下文的情况下进行了自然语言专业化。

    

    有效的沟通需要适应每个交际情境的特殊性，比如与每个交互伙伴分享的共同语境。本研究通过借鉴对话游戏 Dixit 的思想设计了一个多智能体图像参照游戏，训练一个说话者模型来描述一个目标图像，使得一个听者能够在干扰项中正确地识别出目标图像，而另一个听者则不能。这要求说话者利用它与不同听者的共同知识差异进行适应。本研究还展示了在这种对比、多智能体的语境下微调 CLIP 视觉编码器和大型语言模型之间的注意力适配器会自然地产生上下文依赖的自然语言专业化，且只需要通过奖励而无需直接监督来实现。通过控制实验，本研究证明了用两个听者来训练说话者的有效性。

    Effective communication requires adapting to the idiosyncrasies of each communicative context--such as the common ground shared with each partner. Humans demonstrate this ability to specialize to their audience in many contexts, such as the popular game Dixit. We take inspiration from Dixit to formulate a multi-agent image reference game where a (trained) speaker model is rewarded for describing a target image such that one (pretrained) listener model can correctly identify it among distractors, but another listener cannot. To adapt, the speaker must exploit differences in the knowledge it shares with the different listeners. We show that finetuning an attention-based adapter between a CLIP vision encoder and a large language model in this contrastive, multi-agent setting gives rise to context-dependent natural language specialization from rewards only, without direct supervision. Through controlled experiments, we show that training a speaker with two listeners that perceive different
    
[^75]: 基于图结构的世界模型：离线强化学习中的价值记忆图

    Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning. (arXiv:2206.04384v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04384](http://arxiv.org/abs/2206.04384)

    本论文提出了一种离线强化学习的方法，通过构建简单离散的世界模型（Value Memory Graph，VMG）来抽象原始复杂环境，从而简化策略学习。

    

    强化学习（RL）方法通常直接应用于环境来学习策略。在一些具有连续状态-动作空间、稀疏奖励和/或长时间间隔的复杂环境中，在原始环境中学习一个好的策略是困难的。在离线RL设置中，我们旨在构建一个简单且离散的世界模型，以抽象出原始环境。RL方法应用于我们的世界模型而非环境数据进行简化的策略学习。我们的世界模型称为价值记忆图（VMG），它被设计为基于有向图的马尔可夫决策过程（MDP），其中顶点和有向边分别代表图状态和图动作。由于VMG的状态空间和动作空间相对于原始环境而言是有限且相对较小的，因此我们可以直接在VMG上应用价值迭代算法来估算图状态值并确定最佳的图动作。VMG是从离线数据中训练和构建的。

    Reinforcement Learning (RL) methods are typically applied directly in environments to learn policies. In some complex environments with continuous state-action spaces, sparse rewards, and/or long temporal horizons, learning a good policy in the original environments can be difficult. Focusing on the offline RL setting, we aim to build a simple and discrete world model that abstracts the original environment. RL methods are applied to our world model instead of the environment data for simplified policy learning. Our world model, dubbed Value Memory Graph (VMG), is designed as a directed-graph-based Markov decision process (MDP) of which vertices and directed edges represent graph states and graph actions, separately. As state-action spaces of VMG are finite and relatively small compared to the original environment, we can directly apply the value iteration algorithm on VMG to estimate graph state values and figure out the best graph actions. VMG is trained from and built on the offline
    
[^76]: WEDGE：基于网络图像辅助的语义分割领域泛化

    WEDGE: Web-Image Assisted Domain Generalization for Semantic Segmentation. (arXiv:2109.14196v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2109.14196](http://arxiv.org/abs/2109.14196)

    本研究提出了一种基于网络图像辅助的领域泛化方法，利用网络抓取的大量现实图像数据集来增加数据的多样性，提高模型泛化能力，在未见过的领域中表现良好。

    

    针对语义分割的领域泛化问题，在现实应用场景中，训练好的模型需要在之前未见过的领域表现出良好的效果，但面临着训练数据无法覆盖各种可能看不见的领域分布的挑战。本文提出了WEb-image辅助的Domain GEneralization（WEDGE）方案，这是首个利用网络抓取图像的多样性来进行普适语义分割的方法。为了探索和利用现实中的数据分布，我们收集了一个网络抓取的数据集，其中包括了不同的天气条件、网站、光照、相机风格等各种多样性。同时，我们提出了一种方法，可以在训练过程中实时地将网络抓取数据的风格表示注入源领域，从而使网络经历具有可靠标签的不同风格的图像以进行有效的训练。此外，我们使用网络抓取数据集和预测伪标签来增强泛化能力，使模型在以前未见过的领域中表现出良好的效果。

    Domain generalization for semantic segmentation is highly demanded in real applications, where a trained model is expected to work well in previously unseen domains. One challenge lies in the lack of data which could cover the diverse distributions of the possible unseen domains for training. In this paper, we propose a WEb-image assisted Domain GEneralization (WEDGE) scheme, which is the first to exploit the diversity of web-crawled images for generalizable semantic segmentation. To explore and exploit the real-world data distributions, we collect a web-crawled dataset which presents large diversity in terms of weather conditions, sites, lighting, camera styles, etc. We also present a method which injects the style representation of the web-crawled data into the source domain on-the-fly during training, which enables the network to experience images of diverse styles with reliable labels for effective training. Moreover, we use the web-crawled dataset with predicted pseudo labels for 
    

