# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Shuffled Autoregression For Motion Interpolation.](http://arxiv.org/abs/2306.06367) | 本文提出了一种洗牌自回归的框架，用于解决运动插值问题，可利用任意顺序生成的自回归模型和建模为有向无环图的帧内依赖性，提高了性能。 |
| [^2] | [Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception.](http://arxiv.org/abs/2306.06362) | Aria数字孪生是一个自我中心数据集，具有其它任何数据集都没有的高精度、照片逼真和详尽的真实信息。这个数据集将成为自我中心机器感知评估的新标准。 |
| [^3] | [3D reconstruction using Structure for Motion.](http://arxiv.org/abs/2306.06360) | 该论文介绍了一种基于运动的结构的三维重建技术，利用一对HDR照相机和室内移动地面机器人进行捕捉和算法推算，实现室内空间的深度图可视化。 |
| [^4] | [Language-Guided Traffic Simulation via Scene-Level Diffusion.](http://arxiv.org/abs/2306.06344) | 该论文提出了一种可以受到语言指导的场景级条件扩散模型，该模型能够生成真实且可控的交通，并通过大型语言模型将用户的查询转化为损失函数，指导模型生成符合查询条件的仿真交通。 |
| [^5] | [DocumentCLIP: Linking Figures and Main Body Text in Reflowed Documents.](http://arxiv.org/abs/2306.06306) | DocumentCLIP是一种显著性感知对比学习框架，用于理解文档内长文本和图像之间的交互作用。我们是第一个在多模态文档内部链接方面进行对比学习的人。 |
| [^6] | [NERFBK: A High-Quality Benchmark for NERF-Based 3D Reconstruction.](http://arxiv.org/abs/2306.06300) | 该论文介绍了一个名为NeRFBK的新的真实和合成数据集，专门用于测试和比较基于NeRF的3D重建算法，旨在解决收集具有精确地面实况的多样化数据的挑战问题。该数据集有望推动3D重建领域的发展。 |
| [^7] | [Protect Your Prompts: Protocols for IP Protection in LLM Applications.](http://arxiv.org/abs/2306.06297) | 本文讨论了两个协议，旨在保护LLM提示的知识产权，提供开放市场上交易的可能性。 |
| [^8] | [Explaining SAT Solving Using Causal Reasoning.](http://arxiv.org/abs/2306.06294) | CausalSAT使用因果推理揭示现代SAT求解器的内部运作机制。 |
| [^9] | [Everybody Compose: Deep Beats To Music.](http://arxiv.org/abs/2306.06284) | 本项目提出了基于深度学习的方法，使得即使是业余爱好者也能够创作自己的音乐作品，其创新在于输入节拍生成单声部的旋律，提出了三种有效的方法分别为全局注意力的LSTM、局部注意力的LSTM以及相对位置表示的Transformer，生成的音乐具有丰富的变化、和谐和结构。 |
| [^10] | [DeepLCZChange: A Remote Sensing Deep Learning Model Architecture for Urban Climate Resilience.](http://arxiv.org/abs/2306.06269) | 本研究介绍了一个遥感深度学习架构，名为DeepLCZChange，结合了空中LiDAR数据和Landsat 8卫星的地表温度产品，用于研究城市土地利用与当地气候之间的关系。在纽约市的应用中验证了城市森林的降温效应。 |
| [^11] | [Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models.](http://arxiv.org/abs/2306.06253) | 决策堆叠是一种灵活的生成框架，将目标条件化策略代理分解为3个生成模块，并通过教师强制并行学习，可以用于设计单个模块以考虑架构偏差、优化目标和动态、跨领域的可转移性和推理速度。在连续控制基准测试中具有灵活性和可扩展性。 |
| [^12] | [Design Principles for Generalization and Scalability of AI in Communication Systems.](http://arxiv.org/abs/2306.06251) | 本文提出了可持续和可扩展的AI集成通信系统的设计原则，侧重于创建具备通用性的AI算法，通过少量的AI驱动的RAN函数来解决更大的问题，提高系统性能，并简化生命周期管理。 |
| [^13] | [Understanding the Effect of the Long Tail on Neural Network Compression.](http://arxiv.org/abs/2306.06238) | 本文研究了在神经网络压缩中如何保持与原始网络的语义相同，在长尾现象中探讨了压缩提高推广性能的记忆要素。 |
| [^14] | [Using Foundation Models to Detect Policy Violations with Minimal Supervision.](http://arxiv.org/abs/2306.06234) | 本文利用基础模型在极少监督下检测政策违规，创新性地将思维链提示引入政策违规任务，同时将硬提示与软提示相结合，可以高准确度地生成合理解释。 |
| [^15] | [Boosting GUI Prototyping with Diffusion Models.](http://arxiv.org/abs/2306.06233) | 这篇论文介绍了一个名为UI-Diffuser的方法，它利用基于扩散模型的深度学习模型生成移动UI，通过简单的文本描述和UI组件。该方法可以提高GUI原型设计的速度和效率，减少原型设计的工作量。 |
| [^16] | [NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics.](http://arxiv.org/abs/2306.06202) | 本文介绍了神经成像领域的图机器学习基准测试NeuroGraph，并探讨了数据集生成的搜索空间。 |
| [^17] | [ElectroCardioGuard: Preventing Patient Misidentification in Electrocardiogram Databases through Neural Networks.](http://arxiv.org/abs/2306.06196) | 本研究提出了一种基于神经网络的小而高效模型，用于确定两个心电图(ECG)是否来自同一患者。同时，还提出了一种方法，利用该模型检测记录-分配错误，实现了在现实场景中的应用，并在新的ECG数据集上进行了评估。 |
| [^18] | [Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation.](http://arxiv.org/abs/2306.06192) | Ada-NAV是一种自适应轨迹优化策略学习方法，采用降低策略随机性的方法平衡探索与利用，提高机器人导航任务的采样效率。在真实世界的测试中表现优异，可以在更短的采样时间内取得更高的性能。 |
| [^19] | [FasterViT: Fast Vision Transformers with Hierarchical Attention.](http://arxiv.org/abs/2306.06189) | 本研究设计了一种新型混合CNN-ViT神经网络FasterViT，引入了具有分层注意力的方法HAT，将全局自我注意力分解为多级注意力，实现了高效的跨窗口通信。 FasterViT在精度和图像吞吐量方面达到了SOTA前沿水平，并已在分类，物体检测和分割等CV任务中得到广泛验证。 |
| [^20] | [HypLL: The Hyperbolic Learning Library.](http://arxiv.org/abs/2306.06154) | HypLL是一个使用希亚空间的深度学习库，基于PyTorch，旨在使其易于使用，搭建希亚网络模块，特别适用于处理层次化数据和使用少量嵌入维度，是一种新的、开放的研究方向。 |
| [^21] | [EfficientBioAI: Making Bioimaging AI Models Efficient in Energy, Latency and Representation.](http://arxiv.org/abs/2306.06152) | EfficientBioAI是一个即插即用的工具箱，可以压缩生物成像AI模型，使它们在CPU和GPU上运行时能够显著减少能源成本和推理时间，而不会影响准确性。 |
| [^22] | [Read, look and detect: Bounding box annotation from image-caption pairs.](http://arxiv.org/abs/2306.06149) | 本文提出了一种通过图像-标题对进行弱监督学习的方法来实现定位和标注图像中的对象，并在实验中实现了47.51%的短语接地以及21.1 和 10.5的新的最高mAP得分。 |
| [^23] | [Artificial intelligence and radiation protection. A game changer or an update?.](http://arxiv.org/abs/2306.06148) | 本文介绍了基于机器学习的辐射防护方法，探讨了人工智能在辐射防护中的潜在优势和障碍，并提出了促进科学技术成果的合作方式。 |
| [^24] | [SentiGOLD: A Large Bangla Gold Standard Multi-Domain Sentiment Analysis Dataset and its Evaluation.](http://arxiv.org/abs/2306.06147) | 本文介绍了SentiGOLD，一个孟加拉语多领域情感分析数据集。该数据集由70,000个来自不同来源的样本组成，遵守了政府和语言学委员会商定的语言约定，包括了30个领域和5个情感类别。在具有鲁棒性的注释方案下，该数据集的互评一致性表现出色，可用于建立孟加拉语情感分析模型。 |
| [^25] | [Hidden Classification Layers: a study on Data Hidden Representations with a Higher Degree of Linear Separability between the Classes.](http://arxiv.org/abs/2306.06146) | 本文中，研究了一种新颖的培训方法影响深层网络分类器性能，并提出了一个新的神经网络架构，在数据隐藏表示中达到更高的线性可分性。 |
| [^26] | [WePaMaDM-Outlier Detection: Weighted Outlier Detection using Pattern Approaches for Mass Data Mining.](http://arxiv.org/abs/2306.06139) | 本文提出了一种使用模式方法进行加权异常值检测的大数据挖掘方法，可揭示系统故障、欺诈活动和数据模式的重要信息。 |
| [^27] | [Robustness Testing for Multi-Agent Reinforcement Learning: State Perturbations on Critical Agents.](http://arxiv.org/abs/2306.06136) | 提出了一个新的多智能体强化学习鲁棒性测试框架RTCA，采用基于差分进化的关键智能体攻击方法，关键智能体通过团队合作政策评估方法被选取为受害者。该框架在鲁棒性测试中表现出优异的性能。 |
| [^28] | [Safety and Fairness for Content Moderation in Generative Models.](http://arxiv.org/abs/2306.06135) | 生成模型在训练数据中模仿最糟糕的内容，通过安全输入和输出过滤器实现负责任部署；通过安全、公平和指标公平的定义，列举了每个领域可能遇到的例子损害，并提供了损害量化的演示。 |
| [^29] | [Sound Explanation for Trustworthy Machine Learning.](http://arxiv.org/abs/2306.06134) | 本篇论文提出了声音解释的概念，以提供足够的信息来因果解释机器学习系统进行的预测，反对通过将分数归因于输入组件来解释黑盒模型的惯例，并且提出应用特征选择作为癌症预测模型的声音解释，以建立临床医生之间的信任。 |
| [^30] | [Towards Understanding the Interplay of Generative Artificial Intelligence and the Internet.](http://arxiv.org/abs/2306.06130) | 探究生成AI和互联网之间互动的影响，生成AI可能成为数据仓库贡献者并影响后续训练，提出未来版本生成AI工具在使用混合数据训练时会出现的问题和挑战。 |
| [^31] | [Unsupervised clustering of disturbances in power systems via deep convolutional autoencoders.](http://arxiv.org/abs/2306.06124) | 本文提出了一种基于卷积自编码器和K-means聚类的无监督分类方法，可将电力系统中的干扰波形聚类为涉及电压下降、中断、瞬态、正常和谐波畸变等类别，从而实现过滤异常波形和常规波形。 |
| [^32] | [Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey.](http://arxiv.org/abs/2306.06123) | 本文总结了对抗性攻击和防御在可解释人工智能中的研究。列出了现有的不安全因素，并表明了本领域的新兴研究方向。 |
| [^33] | [Strengths and Weaknesses of 3D Pose Estimation and Inertial Motion Capture System for Movement Therapy.](http://arxiv.org/abs/2306.06117) | 本研究比较了3D姿势估计和惯性运动捕捉系统在特定运动锻炼中的精度，为临床应用提供了有用信息。 |
| [^34] | [Learning to Quantize Vulnerability Patterns and Match to Locate Statement-Level Vulnerabilities.](http://arxiv.org/abs/2306.06109) | 该论文提出了一种新颖的漏洞匹配方法，利用学习的漏洞代码库中的漏洞模式定位易受攻击语句，以解决传统漏洞检测方法中存在的识别漏洞范围不准确的问题。 |
| [^35] | [Evaluating the Social Impact of Generative AI Systems in Systems and Society.](http://arxiv.org/abs/2306.05949) | 提出了一种标准方法来评估任何模态的生成AI系统的社会影响，分为基础系统和社会方面的评估，涵盖7个社会影响类别，包括偏见、隐私保护、环境成本等。 |
| [^36] | [Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations.](http://arxiv.org/abs/2306.05880) | 该论文提出了基于INR的时间序列连续建模方法，解决了处理缺失数据、不规则采样和多传感器不对准观测等重复建模问题，并在预测和插值任务中取得了最新的性能表现，具有很好的泛化能力。 |
| [^37] | [How Can Recommender Systems Benefit from Large Language Models: A Survey.](http://arxiv.org/abs/2306.05817) | 本文对将大型语言模型（LLM）应用于推荐系统进行了全面的调查研究，从两个角度总结了现有的研究工作：如何在推荐系统中调整LLM和调整LLM时在哪里调整。最后，我们提出了一些潜在的研究方向和挑战。 |
| [^38] | [Simulation and Prediction of Countercurrent Spontaneous Imbibition at Early and Late Times Using Physics-Informed Neural Networks.](http://arxiv.org/abs/2306.05554) | 本文通过物理信息神经网络模型对多孔材料中的逆流自发渗透过程进行了早期和晚期的模拟和预测，并使用改变变量技术来改进模型性能。 |
| [^39] | [CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification.](http://arxiv.org/abs/2306.04979) | CoCo是一种耦合对比图表示学习框架，其中包含一个图卷积网络和一个分层图内核网络，通过耦合对比学习减少领域差异，用于无监督领域自适应图分类。 |
| [^40] | [INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models.](http://arxiv.org/abs/2306.04757) | INSTRUCTEVAL是一个专注于指导调整的大型语言模型评估的综合套件，它采取了全面的方法来评估模型的性能，包括解决问题、写作能力和与人类价值观的一致性等特征。 |
| [^41] | [AGIQA-3K: An Open Database for AI-Generated Image Quality Assessment.](http://arxiv.org/abs/2306.04717) | 本论文提出了一个用于AI生成图像质量评估的开放数据库AGIQA-3K，并在其中进行了基准实验，提出了StairReward以提高主观文本到图像对齐评估的性能。 |
| [^42] | [Adaptive Frequency Green Light Optimal Speed Advisory based on Hybrid Actor-Critic Reinforcement Learning.](http://arxiv.org/abs/2306.04660) | 本文提出了一种基于混合Actor-Critic强化学习的自适应频率绿灯最佳速度建议模型，通过使用离散和连续演员网络来优化绿灯最佳速度建议系统的频率和加速度曲线，设计了新的奖励函数来平衡减少停车和最小化速度变化的权衡。实验结果显示，该模型可以有效地减少旅行时间和燃料消耗，并优于现有的最先进的GLOSA系统。 |
| [^43] | [Divide and Repair: Using Options to Improve Performance of Imitation Learning Against Adversarial Demonstrations.](http://arxiv.org/abs/2306.04581) | 本文提出了一种利用选项改进模仿学习对抗性示范的性能表现的新技术，可以识别未被对手显着修改的演示轨迹并仅从中进行学习。 |
| [^44] | [Dual policy as self-model for planning.](http://arxiv.org/abs/2306.04440) | 该论文探究了使用精简策略网络作为自我模型的优缺点，并通过实验结果表明，使用经过精简的策略网络作为自我模型可以稳定训练，并且有更快的收敛速度。 |
| [^45] | [Social robots to improve therapeutic adherence in pediatric asthma.](http://arxiv.org/abs/2306.04422) | 通过与人类形态机器人的游戏化会话，可以提高儿童哮喘患者的治疗依从性。 |
| [^46] | [ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems.](http://arxiv.org/abs/2306.04357) | 本研究提出了一种针对对话响应选择的后训练技术Dial-MAE，利用生成方法更好地压缩对话语义至密集向量，并提高对话响应选择准确性。 |
| [^47] | [Professional Basketball Player Behavior Synthesis via Planning with Diffusion.](http://arxiv.org/abs/2306.04090) | 本文中提出了一个名为PLAYBEST的方法，通过使用扩散概率模型从篮球比赛历史数据中学习策略，生成更加真实和多样化的球员行为，从而提高球员的决策制定效果，并在实验中得到了验证。 |
| [^48] | [Enabling Efficient Interaction between an Algorithm Agent and an LLM: A Reinforcement Learning Approach.](http://arxiv.org/abs/2306.03604) | 本论文提出一种强化学习的中介模型，可实现代理与LLM之间高效经济有效的互动，提高效率和成本效益。 |
| [^49] | [SDR-GAIN: A High Real-Time Occluded Pedestrian Pose Completion Method for Autonomous Driving.](http://arxiv.org/abs/2306.03538) | SDR-GAIN是一种用于解决行人姿态中部分遮挡问题的关键点补全方法，它通过对不完整的关键点进行降维，统一特征分布，并使用GAN框架的两种生成模型来完成姿态的补全。该方法的实验表明性能优于基本的GAIN框架。 |
| [^50] | [Applying Standards to Advance Upstream & Downstream Ethics in Large Language Models.](http://arxiv.org/abs/2306.03503) | 本文探讨如何为AI生成的内容制定安全保障，分析LLMs的内容生成机制，确定了四个关键领域，提出了新的分发和销售LLM生成内容的企业的标准。 |
| [^51] | [Is AI Changing the Rules of Academic Misconduct? An In-depth Look at Students' Perceptions of 'AI-giarism'.](http://arxiv.org/abs/2306.03358) | 这项开创性研究调查了学生对“AI-giarism”的认知，提出了初始概念化的AI-giarism工具，有助于应对不断发展的AI技术带来的学术不端行为，同时还挑战了传统的学术不端行为定义。 |
| [^52] | [LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion.](http://arxiv.org/abs/2306.02561) | 本论文提出了LLM-Blender，它是一个集成框架，旨在利用不同的开源大型语言模型的优秀特性，实现始终如一的卓越性能。PairRanker和GenFuser是该框架的两个模块，PairRanker使用成对比较方法来区分候选输出，并且GenFuser旨在合并排名最高的候选者，以生成改进的输出。 |
| [^53] | [Large-Batch, Neural Multi-Objective Bayesian Optimization.](http://arxiv.org/abs/2306.01095) | 本文提出了一种针对数据密集型问题和多目标优化设置的贝叶斯优化框架，该方法利用了贝叶斯神经网络代理建模和可扩展、具有不确定性的收购策略，能够在最少迭代次数的情况下高效地进行优化。 |
| [^54] | [Factors Impacting the Quality of User Answers on Smartphones.](http://arxiv.org/abs/2306.00627) | 本论文研究影响用户答案质量的两个关键因素是反应时间和完成时间，并探究这些因素与外部和内部原因的相关性。 |
| [^55] | [Causal Imitability Under Context-Specific Independence Relations.](http://arxiv.org/abs/2306.00585) | 本文探讨了特定上下文独立关系对因果模仿学习的影响，证明了这种情况下的模仿可行性决策问题是NP难的，并提供了必要的图形标准以及一个有声的算法方法。 |
| [^56] | [Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning.](http://arxiv.org/abs/2306.00477) | 本研究尝试实现在预训练语言模型中运用可逆模型实现高效的微调，并发现在初始化微调时保留PLM的起点非常重要。 |
| [^57] | [The Survey, Taxonomy, and Future Directions of Trustworthy AI: A Meta Decision of Strategic Decisions.](http://arxiv.org/abs/2306.00380) | 本文提出了一个新方法来解决在使用AI系统进行决策时的可信问题。该方法引入了一个包括表达、真实和基本水平的不同信任级别的TAI分类系统或框架，使用十个维度来衡量信任，并提供了现有TAI研究的调查和元分析，还确定了TAI研究的未来方向和潜在应用。 |
| [^58] | [Data Augmentation Approaches for Source Code Models: A Survey.](http://arxiv.org/abs/2305.19915) | 本文对源代码的数据增强技术进行了全面的调查和综述，介绍了它们的分类法、优化策略和性能结果，并讨论了未来方向和研究挑战。 |
| [^59] | [Improving Expressivity of GNNs with Subgraph-specific Factor Embedded Normalization.](http://arxiv.org/abs/2305.19903) | 本文提出了一种名为SuperNorm的专用归一化方案，通过嵌入子图特定因子和纳入图实例特定统计数据来加强GNN的代表性能力，实现对节点感应子图中内部连接信息的明确考虑，从而改善GNN的表达能力。 |
| [^60] | [Fine-grained Text Style Transfer with Diffusion-Based Language Models.](http://arxiv.org/abs/2305.19512) | 本文提出了一种基于扩散式语言模型的细粒度文本风格转换方法，在不依赖外部信息的情况下取得了比之前利用预训练权重、嵌入和外部语法分析器更好的效果，表明扩散概率模型在文本生成领域具有广泛的应用前景。 |
| [^61] | [Mitigating Label Biases for In-context Learning.](http://arxiv.org/abs/2305.19148) | 本文针对上下文学习（ICL）中的三种标签偏差提出分类法，并提出一种简单的偏差校准方法，使用随机的领域词估算语言模型的标签偏差。 |
| [^62] | [One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning.](http://arxiv.org/abs/2305.17682) | 本研究提出了PROPETL方法，通过原型网络和二进制掩码实现了更高效的参数共用迁移学习，解决了多任务微调预训练语言模型存储空间占用的问题。 |
| [^63] | [Modeling Dynamic Environments with Scene Graph Memory.](http://arxiv.org/abs/2305.17537) | 本论文提出了一种新的场景图记忆状态表示，结合节点边缘预测器（NEP）的神经网络架构，能够帮助具有行动能力的AI代理在部分可观察动态场景中高效搜索。 |
| [^64] | [Backdooring Neural Code Search.](http://arxiv.org/abs/2305.17506) | 本文研究了神经代码搜索模型的安全性问题，指出攻击者可以注入后门来返回具有安全/隐私问题的代码，提出了几种防御机制来缓解这种威胁，该工作突显了研究AI系统安全方面的重要性，特别是在部署于安全关键领域时。 |
| [^65] | [Understanding Programs by Exploiting (Fuzzing) Test Cases.](http://arxiv.org/abs/2305.13592) | 本文提出了通过模糊测试获取代表性输入来帮助语义理解程序的方法。 |
| [^66] | [Evaluating Prompt-based Question Answering for Object Prediction in the Open Research Knowledge Graph.](http://arxiv.org/abs/2305.12900) | 本研究采用基于提示的训练方法，在学术知识图谱对象预测领域进行了大规模transformers模型的评估和测试，发现提示的使用可以改进pre-trained transformers的泛化能力。 |
| [^67] | [Integrating Item Relevance in Training Loss for Sequential Recommender Systems.](http://arxiv.org/abs/2305.10824) | 本文提出了一种融合项目相关性的新型训练损失函数，用于提高序列推荐系统对噪声的鲁棒性和性能。 |
| [^68] | [People Talking and AI Listening: How Stigmatizing Language in EHR Notes Affect AI Performance.](http://arxiv.org/abs/2305.10201) | 本文研究了电子病历中污名化语言对基于Transformer的深度学习模型和可解释AI(XAI)技术进行死亡预测的影响。发现临床医生所写的SL会对AI性能表现不利，尤其是在黑人患者中表现更为明显，强调了理解偏见对下游AI性能的影响的重要性，以开发更具公平和正义的医疗系统。 |
| [^69] | [A Scalable Walsh-Hadamard Regularizer to Overcome the Low-degree Spectral Bias of Neural Networks.](http://arxiv.org/abs/2305.09779) | 本文提出了一种新型的可扩展Walsh-Hadamard正则化器，可避免神经网络学习低阶频率以及误识别低阶频率，从而提高了神经网络的泛化性能。 |
| [^70] | [In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making.](http://arxiv.org/abs/2305.07722) | AI解释只有在允许人类决策者验证AI预测的正确性时才有用，而大多数决策环境无法进行验证。 |
| [^71] | [Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator.](http://arxiv.org/abs/2305.06710) | 本文发现，模型扩散中的无标注文本实际上是一个能够生成卡通风格图片的工具。通过简单地扰动无标注文本指导，这一功能得以实现。回滚扰动能够将生成的图像有效转换成卡通图像，而图像扰动则能够产生高保真度、多样性的卡通图像。 |
| [^72] | [Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts.](http://arxiv.org/abs/2305.05832) | 本文提出了利用因果机制在不同环境下保持不变的直觉来主动准备的代理特征选择和工程技术，用以应对统计预测模型在分布转移情况下的稳定性问题。 |
| [^73] | [Towards Applying Powerful Large AI Models in Classroom Teaching: Opportunities, Challenges and Prospects.](http://arxiv.org/abs/2305.03433) | 本文探索了利用大型语言模型实现教师与学生的互动，提高教学质量的潜力和挑战。提出了一个统一的框架来处理多样化的教育数据集，处理长时间的对话以及压缩信息以更好地完成更多的下游任务。 |
| [^74] | [Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models.](http://arxiv.org/abs/2305.02531) | 本研究分析了大型语言模型在不同语言提示下的奖励时间偏好，并发现GPT在具有较弱未来时态的语言下表现出更大的耐心，这与使用该语言的人类的偏好相似。 |
| [^75] | [Causality-aware Concept Extraction based on Knowledge-guided Prompting.](http://arxiv.org/abs/2305.01876) | 该论文提出了一种基于因果感知的知识引导提示方法，将其作为干预器装备到基于预训练语言模型的句子提取器中，以缓解概念偏差。在代表性的多语言KG数据集上进行广泛实验，获得了最先进的结果。 |
| [^76] | [Sensitive Tuning of Large Scale CNNs for E2E Secure Prediction using Homomorphic Encryption.](http://arxiv.org/abs/2304.14836) | 本论文提出一种新的HE友好模型训练方法，成功演示了在ResNet和ConvNeXt等经典和现代CNN上运行加密样本，并以前所未有的方式演示了如何使用CLIP GPT-2模型进行零知识安全预测，是一种可行的隐私保护机器学习解决方案。 |
| [^77] | [Rotation and Translation Invariant Representation Learning with Implicit Neural Representations.](http://arxiv.org/abs/2304.13995) | 本文提出了一种使用隐式神经表示和超网络进行表示学习的方法，称为不变性表示学习，可以在复杂图像上学到分离的语义表示，并且和SCAN很好地协同工作，从而获得最先进的无监督聚类结果。 |
| [^78] | [WizardLM: Empowering Large Language Models to Follow Complex Instructions.](http://arxiv.org/abs/2304.12244) | 本文使用 Evol-Instruct 方法创建了大量不同复杂度的指令数据用于微调 LLaMA 模型，得到了新模型 WizardLM。人类评估结果表明 Evol-Instruct 生成的指令优于人工创建的，而 WizardLM 输出的结果也比 OpenAI ChatGPT 更受欢迎。 |
| [^79] | [Optimal Layout Synthesis for Quantum Circuits as Classical Planning.](http://arxiv.org/abs/2304.12014) | 本文提供了两种编码，将最优布局综合作为经典规划问题，并使用最优的经典规划器来综合标准基准测试的最优布局，解决了对于大规模量子比特优化布局的问题。 |
| [^80] | [Local object crop collision network for efficient simulation of non-convex objects in GPU-based simulators.](http://arxiv.org/abs/2304.09439) | 提出了一种数据驱动的碰撞检测方法，用于高效模拟非凸物体，不需要在计算速度和准确性之间进行权衡，具有较小的在线计算时间和更好的GPU利用率。 |
| [^81] | [LLM as A Robotic Brain: Unifying Egocentric Memory and Control.](http://arxiv.org/abs/2304.09349) | 本文提出了一个统一自我中心记忆和控制的框架LLM-Brain，使用大规模语言模型作为机器人大脑进行零-shot学习。该框架包括封闭式多轮对话，覆盖了感知、规划、控制和记忆，具有很好的泛化性能，适用于多个机器人任务。 |
| [^82] | [Language Instructed Reinforcement Learning for Human-AI Coordination.](http://arxiv.org/abs/2304.07297) | 本文提出了一种称之为instructRL的新的框架，它通过自然语言指令来指定对人工智能搭档的预期策略，解决在缺乏高质量人类行为数据的领域中多智能体强化学习收敛于人类不偏爱的策略的问题，从而提高了人工智能协作的性能。 |
| [^83] | [Improving Few-Shot Inductive Learning on Temporal Knowledge Graphs using Confidence-Augmented Reinforcement Learning.](http://arxiv.org/abs/2304.00613) | 本论文介绍了一种名为FITCARL的TKGC方法，它将少样本学习与强化学习相结合，以实现在时间知识图上进行少样本归纳学习和预测。在FITCARL中，一个智能体通过策略网络来指导搜索过程，通过引入合成样本的模块来解决数据稀缺性，在真实世界数据上取得了最新的结果。 |
| [^84] | [A Survey on Explainable Artificial Intelligence for Network Cybersecurity.](http://arxiv.org/abs/2303.12942) | 这篇论文综述了网络驱动的威胁和问题的系统分类，审查了网络系统中的可解释人工智能在网络安全中的最新技术，并勾画了未来研究的有前途的方向。 |
| [^85] | [Reflexion: an autonomous agent with dynamic memory and self-reflection.](http://arxiv.org/abs/2303.11366) | 本文提出 Reflexion 方法，给智能体赋予了动态记忆和自我反思能力，以增强其任务特定的行动选择能力。 |
| [^86] | [Data-centric Artificial Intelligence: A Survey.](http://arxiv.org/abs/2303.10158) | 本文讨论了数据中心人工智能的必要性并从三个一般性数据中心目标和代表性方法的全面视角进行了介绍。该综述从自动化和协作的角度组织了现有文献并讨论了挑战。 |
| [^87] | [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU.](http://arxiv.org/abs/2303.06865) | 本论文提出了一种名为FlexGen的技术，使用单个GPU实现大型语言模型的高吞吐推理。FlexGen通过聚合来自GPU、CPU和磁盘的内存和计算，搜索有效的张量存储和访问模式，并将权重和注意力缓存压缩为4个位。这些技术使FlexGen具有更大的批量选择空间，从而显著增加了最大吞吐量。 |
| [^88] | [Transformer-based Planning for Symbolic Regression.](http://arxiv.org/abs/2303.06833) | 该论文提出了一种基于Transformer和蒙特卡罗树搜索的符号回归规划策略TPSR，可以将非可微的反馈作为知识的外部来源融入到方程生成过程中，提高方程生成的准确性和复杂性。 |
| [^89] | [RMMDet: Road-Side Multitype and Multigroup Sensor Detection System for Autonomous Driving.](http://arxiv.org/abs/2303.05203) | 本文提出了一种适用于自动驾驶的路侧多类型多组传感器检测系统RMMDet，采用ROS虚拟环境模拟真实道路条件，能够实现多类型传感器检测和多组传感器融合，在各种实验中得到部分成功。 |
| [^90] | [The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models.](http://arxiv.org/abs/2303.03284) | 本论文提出了一种“Wasserstein Belief Updater”算法来学习部分可观测环境下的信念更新，该算法通过学习POMDP​​的潜在模型和置信更新的近似值，实现了对历史观察和行动的有效压缩，提升了算法的性能表现。 |
| [^91] | [DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks.](http://arxiv.org/abs/2302.14685) | 本文中提出了DART策略，其中利用多样化的增强方法训练不同的模型，然后通过聚合这些模型的权重来结合其专业知识，并重复聚合步骤以实现更好的泛化能力。 |
| [^92] | [CrossSpeech: Speaker-independent Acoustic Representation for Cross-lingual Speech Synthesis.](http://arxiv.org/abs/2302.14370) | CrossSpeech提出了一种改善跨语音TTS质量的方法，通过将语音表示分解为语音无关生成器和说话人相关的生成器并分别处理，实现了分离的说话人和语言表示，同时在Blizzard Challenge 2019任务中取得了最先进的性能。 |
| [^93] | [(Re)$^2$H2O: Autonomous Driving Scenario Generation via Reversely Regularized Hybrid Offline-and-Online Reinforcement Learning.](http://arxiv.org/abs/2302.13726) | 提出了一个新的自主驾驶场景生成框架，结合了离线实际数据和在线模拟数据，将反向 L_1 正则化用于关键特征提取，并有效地查询更高维度搜索空间中的场景，实验证明了方法的有效性。 |
| [^94] | [ChatGPT: Jack of all trades, master of none.](http://arxiv.org/abs/2302.10724) | 本研究检验了 ChatGPT 在 25 个不同的 NLP 任务上的性能，它是一个万能的 AI 模型，但无关紧要的表现可能会对某些任务的表现产生负面影响。 |
| [^95] | [A Review on Generative Adversarial Networks for Data Augmentation in Person Re-Identification Systems.](http://arxiv.org/abs/2302.09119) | 本文综述了使用生成对抗网络通过数据增强来改进人员再识别模型性能的最新方法。 |
| [^96] | [Probabilistic Circuits That Know What They Don't Know.](http://arxiv.org/abs/2302.06544) | 本文指出概率电路（PC）对超出分布（OOD）数据不具备鲁棒性；通过模型不确定性量化，我们提出了可处理的随机失活推断（TDI）来克服这一挑战，并且这种方法可以在单个正向传递中提供可处理的无采样不确定性估计，从而改善了PC对分布漂移和OOD数据的鲁棒性。 |
| [^97] | [Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation.](http://arxiv.org/abs/2302.02561) | 该论文介绍了一种新的域自适应方法，该方法利用了从多域数据中生成的域索引，提供额外的洞察视角，并在各种任务中实现了最佳性能。 |
| [^98] | [A Comprehensive Survey of Continual Learning: Theory, Method and Application.](http://arxiv.org/abs/2302.00487) | 本文为持续学习的全面综述，总结了持续学习的一般目标，回顾了各种持续学习方法，并强调了一些有前途的应用领域和未来研究方向。 |
| [^99] | [Retiring $\Delta$DP: New Distribution-Level Metrics for Demographic Parity.](http://arxiv.org/abs/2301.13443) | 本文提出两种新公平度量方法：概率密度函数曲线之间区域（ABPC）和累积密度函数曲线之间区域（ABCC），实现人口统计特征的公平机器学习 |
| [^100] | [Minimal Value-Equivalent Partial Models for Scalable and Robust Planning in Lifelong Reinforcement Learning.](http://arxiv.org/abs/2301.10119) | 本文提出了一种仅模拟环境中相关方面的“最小价值等价部分模型”，并证明了这些模型用于规划在生涯强化学习场景中具有可扩展性优势。 |
| [^101] | [Scalable Communication for Multi-Agent Reinforcement Learning via Transformer-Based Email Mechanism.](http://arxiv.org/abs/2301.01919) | 本文提出了一种基于Transformer的邮件机制（TEM），解决了多智能体强化学习中的可扩展性问题，它采用本地通信和消息链转发的方式进行通信，而不需要模拟所有智能体。 |
| [^102] | [Information Transfer Rate in BCIs: Towards Tightly Integrated Symbiosis.](http://arxiv.org/abs/2301.00488) | 本研究重新定义了信息传输速率（ITR），通过将寄宿于视网膜神经交叉处的共生通信媒介建模为离散无记忆信道，并使用修改后的容量表达式进行计算。这将有助于未来BCI设计的端到端优化。 |
| [^103] | [AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security.](http://arxiv.org/abs/2212.06951) | 本研究分析了Twitter上与MEV相关的话题，结果表明推文讨论了深刻的伦理问题，包括安全、公平、情感情绪和对MEV解决方案的渴望。 |
| [^104] | [Causal Inference via Style Transfer for Out-of-distribution Generalisation.](http://arxiv.org/abs/2212.03063) | 本文提出一种基于风格转移的方法，使用前门调整来解决因果推断中存在的混淆因素，实现领域外泛化。 |
| [^105] | [Mean Shift Mask Transformer for Unseen Object Instance Segmentation.](http://arxiv.org/abs/2211.11679) | 本文提出了一种新的均值漂移掩模变换器，用于联合训练和推断特征提取器和聚类器，可用于未知物体实例分割，在COCO数据集上表现出竞争性的性能，并在罕见和未知物体类别上具有显着优势。 |
| [^106] | [Active Exploration based on Information Gain by Particle Filter for Efficient Spatial Concept Formation.](http://arxiv.org/abs/2211.10934) | 该论文提出了一种基于信息增益主导的主动探索方法，通过粒子滤波器实现顺序贝叶斯推理和目标确定，能够帮助自主机器人在形成和学习空间概念的过程中高效地探索环境和与用户交互。 |
| [^107] | [On the Effectiveness of Hybrid Pooling in Mixup-Based Graph Learning for Language Processing.](http://arxiv.org/abs/2210.03123) | 本文探讨了图池化算子对于Manifold-Mixup方法在图形学习中的影响，提出了一种新颖的混合池化架构，相比现有方法在文本分类任务上表现显著优越并达到了最先进性能水平。 |
| [^108] | [Meta-Learning Priors for Safe Bayesian Optimization.](http://arxiv.org/abs/2210.00762) | 本文提出了一种数据驱动方法，通过元学习先验从离线数据中实现安全的贝叶斯优化，同时开发一种新的框架以数据驱动的方式选择符合安全要求的先验，结果表明，相比于基准方法，元学习先验加快了安全贝叶斯优化的收敛速度并改进了整体性能。 |
| [^109] | [Generating Formal Safety Assurances for High-Dimensional Reachability.](http://arxiv.org/abs/2209.12336) | 提出了一个称为Safe DeepReach的框架，用于为DeepReach方法生成正式安全保障。该框架将新颖的Lipschitz连续性分析与区间边界传播方法相结合，以有效和可伸缩的方式保证解决方案的安全性。 |
| [^110] | [First-order Policy Optimization for Robust Markov Decision Process.](http://arxiv.org/abs/2209.10579) | 本论文介绍了一种用于解决鲁棒性马尔科夫决策过程的一阶方法，称为鲁棒策略镜像下降算法（RPMD）。通过使用线性递增步进，算法具有较低的复杂度，并且能够在不确定情况下找到最优策略。 |
| [^111] | [Discrete Key-Value Bottleneck.](http://arxiv.org/abs/2207.11240) | 本文提出了一个新的神经网络模型结构，包含离散瓶颈，可以有效处理在多个任务之间进行连续学习的问题。 |
| [^112] | [Generative Pretraining for Black-Box Optimization.](http://arxiv.org/abs/2206.10786) | 该论文提出了一种使用离线数据预训练黑盒优化器的生成框架BONET，使用自回归模型和样本策略合成轨迹以帮助在高维空间中优化昂贵的黑盒函数。 |
| [^113] | [Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models.](http://arxiv.org/abs/2206.04615) | 本研究引入了Beyond the Imitation Game基准测试（BIG-bench），该测试集包含了204个各领域的难题，旨在评估当前语言模型的能力并为未来的研究提供信息和准备。 |
| [^114] | [Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training.](http://arxiv.org/abs/2206.00621) | 本文提出了跨视图语言建模框架，该框架将跨语言和跨模态预训练统一在共享的架构和目标下进行，通过有条件的掩码语言建模和对比学习来最大化不同视图之间的互信息以实现两个视图的对齐。 |
| [^115] | [QUIC-FL: Quick Unbiased Compression for Federated Learning.](http://arxiv.org/abs/2205.13341) | 本文提出QUIC-FL方法，即面向联邦学习的快速无偏压缩，通过改进DME技术实现了最优归一化均方误差保证。 |
| [^116] | [SE-MoE: A Scalable and Efficient Mixture-of-Experts Distributed Training and Inference System.](http://arxiv.org/abs/2205.10034) | SE-MoE提出了一种弹性的训练方式和不同优化措施，以提高混合专家模型的分布式训练和推理效率，解决了负载平衡、通信/计算效率和内存限制等问题。 |
| [^117] | [Semantic Information Recovery in Wireless Networks.](http://arxiv.org/abs/2204.13366) | 本文提出了一个基于机器学习的语义通信系统SINFONY，它通过对消息进行数据减少和可靠传输来最好地保留语义，从而实现无线网络中的语义信息恢复。 |
| [^118] | [Recent Advances in Neural Text Generation: A Task-Agnostic Survey.](http://arxiv.org/abs/2203.03047) | 本文调查了最近神经文本生成领域的最新进展，包括数据构建、神经框架、训练和推理策略和评估指标等四个方面，并探讨了神经管道和背景知识的利用等未来方向。 |
| [^119] | [The Impact of a Coalition: Assessing the Likelihood of Voter Influence in Large Elections.](http://arxiv.org/abs/2202.06411) | 本文研究了在半随机模型中，联盟大小任意且可变情况下，进行操纵、胜利边缘和选票控制等影响大选的可能性。主要定理提供了选举结果受 $s$ 个联盟影响的上下界。 |
| [^120] | [The Concept of Criticality in AI Safety.](http://arxiv.org/abs/2201.04632) | 本文提出了一种在AI安全领域中的重要性概念——关键操作，并提出了一种更加高效的解决方案，使操作员可以参与其他活动而不忽略监控任务。AI代理仅请求关键操作的许可，并用操作员的反馈使代理更加智能化。 |
| [^121] | [Model-Based Safe Reinforcement Learning with Time-Varying State and Control Constraints: An Application to Intelligent Vehicles.](http://arxiv.org/abs/2112.11217) | 本文提出了一种安全RL算法，结合了基于屏障力的控制策略结构与多步策略评估机制，在保证控制安全的同时，能够应对时间变化的安全约束，并证明了其稳定性、鲁棒性和收敛性，优于几种最先进的RL算法。 |
| [^122] | [Multi-Row, Multi-Span Distant Supervision For Table+Text Question.](http://arxiv.org/abs/2112.07337) | 这篇论文提出了一个名为MITQA的基于Transformer的TextTableQA系统，通过多实例学习以及远程监督方法，有效地解决了表格加文本问题的挑战。 |
| [^123] | [Retrosynthetic Planning with Experience-Guided Monte Carlo Tree Search.](http://arxiv.org/abs/2112.06028) | 本文提出了一种经验引导的蒙特卡罗树搜索算法用于反合成规划，能够显著提高搜索效率和有效性，在基准数据集上表现出色。 |
| [^124] | [Efficient Multi-objective Neural Architecture Search Framework via Policy Gradient Algorithm.](http://arxiv.org/abs/2111.03892) | 本论文提出了TND-NAS框架，利用可微架构搜索框架和多目标NAS的兼容性，通过策略梯度算法实现高效多目标神经架构搜索。实验结果表明其优于现有方法。 |
| [^125] | [Integrating Distributed Architectures in Highly Modular RL Libraries.](http://arxiv.org/abs/2007.02622) | 该研究探讨了在本地和分布式执行级别上实现代理组合性的设计选择，通过独立的可重用组件允许在不同尺度上定义RL代理，并成功解决了新颖和复杂的环境，具有最先进的性能。 |
| [^126] | [Method for the semantic indexing of concept hierarchies, uniform representation, use of relational database systems and generic and case-based reasoning.](http://arxiv.org/abs/1910.01539) | 本文提出了一种语义索引方法，通过概念层次结构表示知识并将键分配给节点，使得概念与所有更具体的概念部分可统一，并且只允许添加语义正确的概念。此方法可以使用基于案例的推理和通用问题解决方法进行推理。 |
| [^127] | [Improving a State-of-the-Art Heuristic for the Minimum Latency Problem with Data Mining.](http://arxiv.org/abs/1908.10705) | 这篇论文通过利用数据挖掘技术，改进了一种基于GRASP的最小延迟问题启发式算法，取得了较好的效果，匹配或优于解的质量，在大大缩短计算时间的同时，还成功地引入了88个新的解成本值。 |

# 详细

[^1]: 洗牌自回归用于运动插值

    Shuffled Autoregression For Motion Interpolation. (arXiv:2306.06367v1 [cs.CV])

    [http://arxiv.org/abs/2306.06367](http://arxiv.org/abs/2306.06367)

    本文提出了一种洗牌自回归的框架，用于解决运动插值问题，可利用任意顺序生成的自回归模型和建模为有向无环图的帧内依赖性，提高了性能。

    

    本工作旨在为运动插值任务提供深度学习解决方案。以往的研究使用几何权重函数解决这个问题。其他一些工作则提出了神经网络来处理不同的问题设置，并以连续的姿势序列作为输入。但是运动插值是一个更复杂的问题，它将孤立的姿势（例如只使用一个起始姿势和一个结束姿势）作为输入。当应用于运动插值时，这些深度学习方法的性能有限，因为它们无法利用原始几何公式中的灵活依赖关系。为了实现这种插值特性，我们提出了一种新的框架，称为“洗牌自回归”，它将自回归扩展到任意（洗牌）顺序生成，并将任何帧间依赖关系建模为有向无环图。我们进一步提出了一种构建特定类型依赖性图的方法，包括三个阶段组装成一个模型。

    This work aims to provide a deep-learning solution for the motion interpolation task. Previous studies solve it with geometric weight functions. Some other works propose neural networks for different problem settings with consecutive pose sequences as input. However, motion interpolation is a more complex problem that takes isolated poses (e.g., only one start pose and one end pose) as input. When applied to motion interpolation, these deep learning methods have limited performance since they do not leverage the flexible dependencies between interpolation frames as the original geometric formulas do. To realize this interpolation characteristic, we propose a novel framework, referred to as \emph{Shuffled AutoRegression}, which expands the autoregression to generate in arbitrary (shuffled) order and models any inter-frame dependencies as a directed acyclic graph. We further propose an approach to constructing a particular kind of dependency graph, with three stages assembled into an end
    
[^2]: Aria数字孪生：一种新的基准数据集用于自我中心的3D机器感知。

    Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception. (arXiv:2306.06362v1 [cs.CV])

    [http://arxiv.org/abs/2306.06362](http://arxiv.org/abs/2306.06362)

    Aria数字孪生是一个自我中心数据集，具有其它任何数据集都没有的高精度、照片逼真和详尽的真实信息。这个数据集将成为自我中心机器感知评估的新标准。

    

    我们推出了Aria数字孪生（ADT）-一个使用Aria眼镜捕获的自我中心数据集，具有广泛的对象，环境和人类级别的真实数据。该ADT数据集包括200个由穿戴Aria设备的人在两个室内真实场景中执行的真实世界活动序列，包含398个对象实例（324个静态和74个动态）。每个序列包括：a）两个单色相机流，一个RGB相机流，两个IMU流的原始数据；b）完整的传感器校准；c）真实数据，包括Aria设备的连续6自由度（6DoF）姿态，对象6DoF姿态，3D注视矢量，3D人体姿态，2D图像分割，图像深度图；d）照片般真实的合成渲染图像。据我们所知，目前没有现有自我中心数据集能够与ADT的准确性、逼真度和全面性相媲美。通过向研究社区贡献ADT，我们的使命是为自我中心机器感知的评估设立新的标准。

    We introduce the Aria Digital Twin (ADT) - an egocentric dataset captured using Aria glasses with extensive object, environment, and human level ground truth. This ADT release contains 200 sequences of real-world activities conducted by Aria wearers in two real indoor scenes with 398 object instances (324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two monochrome camera streams, one RGB camera stream, two IMU streams; b) complete sensor calibration; c) ground truth data including continuous 6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye gaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d) photo-realistic synthetic renderings. To the best of our knowledge, there is no existing egocentric dataset with a level of accuracy, photo-realism and comprehensiveness comparable to ADT. By contributing ADT to the research community, our mission is to set a new standard for evaluation in the egocentric machine perce
    
[^3]: 基于运动的结构的三维重建技术

    3D reconstruction using Structure for Motion. (arXiv:2306.06360v1 [cs.CV])

    [http://arxiv.org/abs/2306.06360](http://arxiv.org/abs/2306.06360)

    该论文介绍了一种基于运动的结构的三维重建技术，利用一对HDR照相机和室内移动地面机器人进行捕捉和算法推算，实现室内空间的深度图可视化。

    

    本文旨在利用一对HDR照相机进行室内空间的三维重建，该照相机以立体视觉配置安装在室内移动地面机器人上，捕捉各种纹理和空间特征并将其作为2D图像传给我们的算法，进而实现深度图的可视化。

    We are working towards 3D reconstruction of indoor spaces using a pair of HDR cameras in a stereo vision configuration mounted on an indoor mobile floor robot that captures various textures and spatial features as 2D images and this data is simultaneously utilized as a feed to our algorithm which will allow us to visualize the depth map.
    
[^4]: 语言指导下的场景级交通仿真模拟

    Language-Guided Traffic Simulation via Scene-Level Diffusion. (arXiv:2306.06344v1 [cs.RO])

    [http://arxiv.org/abs/2306.06344](http://arxiv.org/abs/2306.06344)

    该论文提出了一种可以受到语言指导的场景级条件扩散模型，该模型能够生成真实且可控的交通，并通过大型语言模型将用户的查询转化为损失函数，指导模型生成符合查询条件的仿真交通。

    

    实现真实和可控的交通仿真是加速自主驾驶汽车（AV）发展的核心能力。然而，目前用于控制基于学习的交通模型的方法需要大量领域专业知识，对于从业者来说很难使用。为了解决这个问题，我们提出了CTG++，一种可以受到语言指导的场景级条件扩散模型。为了达到这个目的，我们需要解决两个问题：需要一个真实和可控的交通模型骨干结构，并且要有一种有效的方法来使用语言与交通模型进行交互。为了解决这些问题，我们首先提出了一个带有时空转换器骨干结构的场景级扩散模型，它生成了真实和可控的交通。然后，我们利用大型语言模型（LLM）将用户的查询转换为损失函数，指导扩散模型朝着查询合规的生成方向前进。通过全面的评估，我们展示了该模型的有效性。

    Realistic and controllable traffic simulation is a core capability that is necessary to accelerate autonomous vehicle (AV) development. However, current approaches for controlling learning-based traffic models require significant domain expertise and are difficult for practitioners to use. To remedy this, we present CTG++, a scene-level conditional diffusion model that can be guided by language instructions. Developing this requires tackling two challenges: the need for a realistic and controllable traffic model backbone, and an effective method to interface with a traffic model using language. To address these challenges, we first propose a scene-level diffusion model equipped with a spatio-temporal transformer backbone, which generates realistic and controllable traffic. We then harness a large language model (LLM) to convert a user's query into a loss function, guiding the diffusion model towards query-compliant generation. Through comprehensive evaluation, we demonstrate the effect
    
[^5]: DocumentCLIP：链接换行文件中的图像和正文文本

    DocumentCLIP: Linking Figures and Main Body Text in Reflowed Documents. (arXiv:2306.06306v1 [cs.CV])

    [http://arxiv.org/abs/2306.06306](http://arxiv.org/abs/2306.06306)

    DocumentCLIP是一种显著性感知对比学习框架，用于理解文档内长文本和图像之间的交互作用。我们是第一个在多模态文档内部链接方面进行对比学习的人。

    

    视觉语言预训练模型通过理解图像和文本之间的对齐而在支持多媒体应用方面取得了巨大成功。然而，现有的视觉语言预训练模型主要集中在理解单个图像与一段文本相关联，而往往忽略了多句话与多个图像组成的文档内部的对齐。在本文中，我们提出了一种称为DocumentCLIP的显著性感知对比学习框架，以强制视觉语言预训练模型理解文档内长文本和图像之间的交互作用。我们的模型有助于对新闻文章、杂志、产品描述等具有更丰富语言和视觉内容的真实世界多模态文档理解。据我们所知，我们是第一个通过对比学习探索多模态文档内部链接的人。此外，我们收集了一个大型的Wikipedia数据集进行预培训，为训练提供了多样化的来源。

    Vision-language pretraining models have achieved great success in supporting multimedia applications by understanding the alignments between images and text. While existing vision-language pretraining models primarily focus on understanding single image associated with a single piece of text, they often ignore the alignment at the intra-document level, consisting of multiple sentences with multiple images. In this work, we propose DocumentCLIP, a salience-aware contrastive learning framework to enforce vision-language pretraining models to comprehend the interaction between images and longer text within documents. Our model is beneficial for the real-world multimodal document understanding like news article, magazines, product descriptions, which contain linguistically and visually richer content. To the best of our knowledge, we are the first to explore multimodal intra-document links by contrastive learning. In addition, we collect a large Wikipedia dataset for pretraining, which pro
    
[^6]: NERFBK:针对基于NeRF的3D重建的高质量基准数据集

    NERFBK: A High-Quality Benchmark for NERF-Based 3D Reconstruction. (arXiv:2306.06300v1 [cs.CV])

    [http://arxiv.org/abs/2306.06300](http://arxiv.org/abs/2306.06300)

    该论文介绍了一个名为NeRFBK的新的真实和合成数据集，专门用于测试和比较基于NeRF的3D重建算法，旨在解决收集具有精确地面实况的多样化数据的挑战问题。该数据集有望推动3D重建领域的发展。

    

    本文介绍一个名为NeRFBK的新的真实和合成数据集，专门用于测试和比较基于NeRF的3D重建算法。高质量的3D重建在各个领域都有重要的潜力，基于图像的算法的改进使得评估新的先进技术变得必不可少。然而，收集具有精确地面实况的多样化数据是具有挑战性的且可能无法涵盖所有相关的应用。NeRFBK数据集通过提供多尺度、室内、室外数据集及高分辨率图像、视频和相机参数等来解决这个问题，以测试和比较基于NeRF的算法。本文介绍了NeRFBK基准的设计和创建、各种例子和应用场景，并强调了它在推动3D重建领域发展方面的潜力。

    This paper introduces a new real and synthetic dataset called NeRFBK specifically designed for testing and comparing NeRF-based 3D reconstruction algorithms. High-quality 3D reconstruction has significant potential in various fields, and advancements in image-based algorithms make it essential to evaluate new advanced techniques. However, gathering diverse data with precise ground truth is challenging and may not encompass all relevant applications. The NeRFBK dataset addresses this issue by providing multi-scale, indoor and outdoor datasets with high-resolution images and videos and camera parameters for testing and comparing NeRF-based algorithms. This paper presents the design and creation of the NeRFBK benchmark, various examples and application scenarios, and highlights its potential for advancing the field of 3D reconstruction.
    
[^7]: LLM应用中的IP保护协议

    Protect Your Prompts: Protocols for IP Protection in LLM Applications. (arXiv:2306.06297v1 [cs.CL])

    [http://arxiv.org/abs/2306.06297](http://arxiv.org/abs/2306.06297)

    本文讨论了两个协议，旨在保护LLM提示的知识产权，提供开放市场上交易的可能性。

    

    随着大型语言模型（LLM）形式的AI的快速采用，精心设计的提示的潜在价值变得重要。然而，为了实现这个潜力，提示应该在一个公开市场上交易。由于提示目前通常是经济上不可排除的，因为它们是文本的性质，因此尚未建立一般的竞争市场。本文讨论了两个旨在提供提示保护的协议，提高其作为知识产权的地位，从而确认提示工程师的知识产权，并可能支持LLM提示的开放市场的繁荣。

    With the rapid adoption of AI in the form of large language models (LLMs), the potential value of carefully engineered prompts has become significant. However, to realize this potential, prompts should be tradable on an open market. Since prompts are, at present, generally economically non-excludable, by virtue of their nature as text, no general competitive market has yet been established. This note discusses two protocols intended to provide protection of prompts, elevating their status as intellectual property, thus confirming the intellectual property rights of prompt engineers, and potentially supporting the flourishing of an open market for LLM prompts.
    
[^8]: 使用因果推理解释SAT求解

    Explaining SAT Solving Using Causal Reasoning. (arXiv:2306.06294v1 [cs.AI])

    [http://arxiv.org/abs/2306.06294](http://arxiv.org/abs/2306.06294)

    CausalSAT使用因果推理揭示现代SAT求解器的内部运作机制。

    

    过去三十年来，设计高效的SAT求解器取得了显著的成功，现代求解器能够在几秒钟内解决包含数百万个变量的工业基准，其成功归功于广泛使用的CDCL算法，但缺乏全面的理论研究。此外，已经观察到CDCL求解器仍然难以处理特定类别的基准，其中只包含数百个变量，这与它们在实际应用中的广泛使用形成了反差。因此，有迫切需要揭示这些看似薄弱但强大的黑匣子的内部运作机制。在本文中，我们通过引入一种称为CausalSAT的方法，采用因果推理来深入了解现代SAT求解器的功能，这是实现这一目标的第一步。

    The past three decades have witnessed notable success in designing efficient SAT solvers, with modern solvers capable of solving industrial benchmarks containing millions of variables in just a few seconds. The success of modern SAT solvers owes to the widely-used CDCL algorithm, which lacks comprehensive theoretical investigation. Furthermore, it has been observed that CDCL solvers still struggle to deal with specific classes of benchmarks comprising only hundreds of variables, which contrasts with their widespread use in real-world applications. Consequently, there is an urgent need to uncover the inner workings of these seemingly weak yet powerful black boxes.  In this paper, we present a first step towards this goal by introducing an approach called CausalSAT, which employs causal reasoning to gain insights into the functioning of modern SAT solvers. CausalSAT initially generates observational data from the execution of SAT solvers and learns a structured graph representing the cau
    
[^9]: 众创曲：深度神经网络生成音乐节拍

    Everybody Compose: Deep Beats To Music. (arXiv:2306.06284v1 [cs.SD])

    [http://arxiv.org/abs/2306.06284](http://arxiv.org/abs/2306.06284)

    本项目提出了基于深度学习的方法，使得即使是业余爱好者也能够创作自己的音乐作品，其创新在于输入节拍生成单声部的旋律，提出了三种有效的方法分别为全局注意力的LSTM、局部注意力的LSTM以及相对位置表示的Transformer，生成的音乐具有丰富的变化、和谐和结构。

    

    本项目提出了一种基于深度学习的方法，通过输入节拍生成单声部的旋律，使得即使是业余爱好者也能够创作自己的音乐作品。该项目提出了三种有效的方法——带有全局注意力的LSTM、带有局部注意力的LSTM和带有相对位置表示的Transformer，为生成的音乐提供了丰富的变化、和谐和结构。该项目允许任何人通过敲击键盘或“重新调整”现有作品的节拍序列来创作自己的音乐。

    This project presents a deep learning approach to generate monophonic melodies based on input beats, allowing even amateurs to create their own music compositions. Three effective methods - LSTM with Full Attention, LSTM with Local Attention, and Transformer with Relative Position Representation - are proposed for this novel task, providing great variation, harmony, and structure in the generated music. This project allows anyone to compose their own music by tapping their keyboards or ``recoloring'' beat sequences from existing works.
    
[^10]: DeepLCZChange：一种用于城市气候韧性的遥感深度学习模型架构

    DeepLCZChange: A Remote Sensing Deep Learning Model Architecture for Urban Climate Resilience. (arXiv:2306.06269v1 [cs.CV])

    [http://arxiv.org/abs/2306.06269](http://arxiv.org/abs/2306.06269)

    本研究介绍了一个遥感深度学习架构，名为DeepLCZChange，结合了空中LiDAR数据和Landsat 8卫星的地表温度产品，用于研究城市土地利用与当地气候之间的关系。在纽约市的应用中验证了城市森林的降温效应。

    

    城市土地利用结构会影响都市区的局部气候条件。为了揭示城市土地利用与当地气候之间的机制，本文提出了一种新颖的数据驱动深度学习架构和流程，DeepLCZChange，以相关空中LiDAR数据统计数据与Landsat 8卫星的地表温度产品相关联。一个概念验证数值实验利用纽约市的相应遥感数据验证了城市森林的降温效应。

    Urban land use structures impact local climate conditions of metropolitan areas. To shed light on the mechanism of local climate wrt. urban land use, we present a novel, data-driven deep learning architecture and pipeline, DeepLCZChange, to correlate airborne LiDAR data statistics with the Landsat 8 satellite's surface temperature product. A proof-of-concept numerical experiment utilizes corresponding remote sensing data for the city of New York to verify the cooling effect of urban forests.
    
[^11]: 通过模块化生成模型实现灵活的强化学习决策堆叠

    Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models. (arXiv:2306.06253v1 [cs.LG])

    [http://arxiv.org/abs/2306.06253](http://arxiv.org/abs/2306.06253)

    决策堆叠是一种灵活的生成框架，将目标条件化策略代理分解为3个生成模块，并通过教师强制并行学习，可以用于设计单个模块以考虑架构偏差、优化目标和动态、跨领域的可转移性和推理速度。在连续控制基准测试中具有灵活性和可扩展性。

    

    强化学习是一种有吸引力的模型，可以推理顺序决策制定的几个不同方面，如指定复杂目标、规划未来观察和行动，以及批评其实用性。然而，这些能力的综合集成在保持最大表达能力的同时允许进行模型选择以实现高效的学习和推理，这构成了竞争性的算法挑战。我们提出了决策堆叠（Decision Stacks），这是一个生成框架，将目标条件化策略代理分解为3个生成模块。这些模块通过独立的生成模型模拟了观测、奖励和行动的时间演变，可以通过教师强制并行学习。我们的框架保证了表达能力和灵活性，在设计单个模块以考虑关键因素（如架构偏差、优化目标和动态、跨领域的可转移性和推理速度）方面具有优势。我们对一系列连续控制基准进行的实证结果表明，决策堆叠提供了一种灵活且可扩展的替代最先进的基于模型和基于模型的强化学习方法。

    Reinforcement learning presents an attractive paradigm to reason about several distinct aspects of sequential decision making, such as specifying complex goals, planning future observations and actions, and critiquing their utilities. However, the combined integration of these capabilities poses competing algorithmic challenges in retaining maximal expressivity while allowing for flexibility in modeling choices for efficient learning and inference. We present Decision Stacks, a generative framework that decomposes goal-conditioned policy agents into 3 generative modules. These modules simulate the temporal evolution of observations, rewards, and actions via independent generative models that can be learned in parallel via teacher forcing. Our framework guarantees both expressivity and flexibility in designing individual modules to account for key factors such as architectural bias, optimization objective and dynamics, transferrability across domains, and inference speed. Our empirical 
    
[^12]: 通信系统中AI通用性与可扩展性的设计原则

    Design Principles for Generalization and Scalability of AI in Communication Systems. (arXiv:2306.06251v1 [cs.LG])

    [http://arxiv.org/abs/2306.06251](http://arxiv.org/abs/2306.06251)

    本文提出了可持续和可扩展的AI集成通信系统的设计原则，侧重于创建具备通用性的AI算法，通过少量的AI驱动的RAN函数来解决更大的问题，提高系统性能，并简化生命周期管理。

    

    人工智能（AI）已经成为通信系统中解决复杂和动态任务的强大工具，传统的基于规则的算法往往无法胜任。然而，大多数网络任务的AI应用都是针对特定和有限的条件设计和训练的，使得算法无法适应于常见的网络环境、任务和控制任务。本文提出了可持续和可扩展的AI集成通信系统的设计原则，侧重于创建可以在网络环境、意图和控制任务上具备通用性的AI算法。这种方法可以使少量的AI驱动的RAN函数来解决更大的问题，提高系统性能，并简化生命周期管理。为了实现可持续性和自动化，我们引入了一种可扩展的学习架构，该架构支持系统中部署的所有AI应用程序。该架构将中央化学习功能与分布式学习和数据采集功能分离，确保了系统的可扩展性和稳健性。

    Artificial intelligence (AI) has emerged as a powerful tool for addressing complex and dynamic tasks in communication systems, where traditional rule-based algorithms often struggle. However, most AI applications to networking tasks are designed and trained for specific, limited conditions, hindering the algorithms from learning and adapting to generic situations, such as those met across radio access networks (RAN). This paper proposes design principles for sustainable and scalable AI integration in communication systems, focusing on creating AI algorithms that can generalize across network environments, intents, and control tasks. This approach enables a limited number of AI-driven RAN functions to tackle larger problems, improve system performance, and simplify lifecycle management. To achieve sustainability and automation, we introduce a scalable learning architecture that supports all deployed AI applications in the system. This architecture separates centralized learning function
    
[^13]: 理解长尾效应对神经网络压缩的影响

    Understanding the Effect of the Long Tail on Neural Network Compression. (arXiv:2306.06238v1 [cs.LG])

    [http://arxiv.org/abs/2306.06238](http://arxiv.org/abs/2306.06238)

    本文研究了在神经网络压缩中如何保持与原始网络的语义相同，在长尾现象中探讨了压缩提高推广性能的记忆要素。

    

    网络压缩现在是神经网络研究的一个成熟的子领域，过去的十年中，取得了显著的进展，以减小模型尺寸和加速推断为目标，同时保持分类准确性。然而，许多研究观察到，仅关注总体准确性可能是误导的。例如，已经证明全模型和压缩模型之间的差异可能会偏向于在数据集中低频的类。这引出了一个重要的研究问题，即“我们能否在保持与原始网络语义等同的情况下实现网络压缩？”在本文中，我们研究了这个问题，重点关注计算机视觉数据集中Feldman等人观察到的“长尾”现象。他们认为，某些输入（适当定义）的记忆对于实现良好的泛化是必要的。由于压缩限制了网络的容量（因此也限制了其记忆能力），所以我们研究了这个问题：

    Network compression is now a mature sub-field of neural network research: over the last decade, significant progress has been made towards reducing the size of models and speeding up inference, while maintaining the classification accuracy. However, many works have observed that focusing on just the overall accuracy can be misguided. E.g., it has been shown that mismatches between the full and compressed models can be biased towards under-represented classes. This raises the important research question, \emph{can we achieve network compression while maintaining ``semantic equivalence'' with the original network?} In this work, we study this question in the context of the ``long tail'' phenomenon in computer vision datasets observed by Feldman, et al. They argue that \emph{memorization} of certain inputs (appropriately defined) is essential to achieving good generalization. As compression limits the capacity of a network (and hence also its ability to memorize), we study the question: a
    
[^14]: 使用基础模型在极少监督下检测政策违规

    Using Foundation Models to Detect Policy Violations with Minimal Supervision. (arXiv:2306.06234v1 [cs.CL])

    [http://arxiv.org/abs/2306.06234](http://arxiv.org/abs/2306.06234)

    本文利用基础模型在极少监督下检测政策违规，创新性地将思维链提示引入政策违规任务，同时将硬提示与软提示相结合，可以高准确度地生成合理解释。

    

    基础模型，即预训练于大型文本语料库的大型神经网络，已经彻底改变了自然语言处理。它们可以直接指导，例如硬提示(例如(arXiv:2005.14165))，也可以使用极少的数据进行调整，这种技术被称为软提示，我们试图利用它们的能力来检测政策违规。我们的创新点是：我们确定了一种硬提示，将思维链提示适应于政策违规任务。这个提示生成政策违规分类以及提取式解释来证明分类的合理性。我们将硬提示与软提示调整相结合，使用非常少的监督来生成一个分类器，同时该分类器还可以产生解释。虽然监督只作用于分类，但我们发现修改后的解释与(调整后的)模型响应保持一致。在此过程中，我们确定了一些令人费解的方面。

    Foundation models, i.e. large neural networks pre-trained on large text corpora, have revolutionized NLP. They can be instructed directly (e.g. (arXiv:2005.14165)) - this is called hard prompting - and they can be tuned using very little data (e.g. (arXiv:2104.08691)) - this technique is called soft prompting. We seek to leverage their capabilities to detect policy violations. Our contributions are: We identify a hard prompt that adapts chain-of-thought prompting to policy violation tasks. This prompt produces policy violation classifications, along with extractive explanations that justify the classification. We compose the hard-prompts with soft prompt tuning to produce a classifier that attains high accuracy with very little supervision; the same classifier also produces explanations. Though the supervision only acts on the classifications, we find that the modified explanations remain consistent with the (tuned) model's response. Along the way, we identify several unintuitive aspec
    
[^15]: 基于扩散模型的GUI原型设计加速

    Boosting GUI Prototyping with Diffusion Models. (arXiv:2306.06233v1 [cs.SE])

    [http://arxiv.org/abs/2306.06233](http://arxiv.org/abs/2306.06233)

    这篇论文介绍了一个名为UI-Diffuser的方法，它利用基于扩散模型的深度学习模型生成移动UI，通过简单的文本描述和UI组件。该方法可以提高GUI原型设计的速度和效率，减少原型设计的工作量。

    

    GUI（图形用户界面）原型设计是需求工程中广泛使用的技术，用于收集和完善需求，降低开发风险并增加干系人的参与度。然而，GUI原型设计可能是一项耗时且昂贵的过程。近年来，诸如Stable Diffusion等深度学习模型已成为一种强大的文本到图像工具，并能够根据文本提示生成详细的图像。在本文中，我们提出了UI-Diffuser，一种利用Stable Diffusion生成移动UI的方法，通过简单的文本描述和UI组件。初步结果表明，UI-Diffuser提供了一种高效且具有成本效益的方法，可以减少对广泛原型设计工作的需求，从而生成移动GUI设计。这种方法有望显著提高GUI原型设计在需求工程中的速度和效率。

    GUI (graphical user interface) prototyping is a widely-used technique in requirements engineering for gathering and refining requirements, reducing development risks and increasing stakeholder engagement. However, GUI prototyping can be a time-consuming and costly process. In recent years, deep learning models such as Stable Diffusion have emerged as a powerful text-to-image tool capable of generating detailed images based on text prompts. In this paper, we propose UI-Diffuser, an approach that leverages Stable Diffusion to generate mobile UIs through simple textual descriptions and UI components. Preliminary results show that UI-Diffuser provides an efficient and cost-effective way to generate mobile GUI designs while reducing the need for extensive prototyping efforts. This approach has the potential to significantly improve the speed and efficiency of GUI prototyping in requirements engineering.
    
[^16]: NeuroGraph:面向脑连接组学的图机器学习基准测试

    NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics. (arXiv:2306.06202v1 [cs.LG])

    [http://arxiv.org/abs/2306.06202](http://arxiv.org/abs/2306.06202)

    本文介绍了神经成像领域的图机器学习基准测试NeuroGraph，并探讨了数据集生成的搜索空间。

    

    机器学习为分析高维功能性神经成像数据提供了有价值的工具，已被证明对预测各种神经疾病、精神障碍和认知模式有效。在功能磁共振成像研究中，大脑区域之间的相互作用通常使用基于图的表示进行建模。图机器学习方法的有效性已在多个领域得到证实，标志着数据解释和预测建模中的一个转变步骤。然而，尽管有前景，但由于图形数据集构建的广泛预处理流水线和大参数搜索空间，在神经成像领域中应用这些技术的转换仍然受到意外的限制。本文介绍了NeuroGraph(一个基于图的神经成像数据集)，它涵盖了多个行为和认知特征类别。我们深入探讨了数据集生成搜索空间

    Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional Magnetic Resonance Imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain remains surprisingly under-explored due to the expansive preprocessing pipeline and large parameter search space for graph-based datasets construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets that span multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by c
    
[^17]: ElectroCardioGuard：通过神经网络防止心电图数据库中患者误识别

    ElectroCardioGuard: Preventing Patient Misidentification in Electrocardiogram Databases through Neural Networks. (arXiv:2306.06196v1 [cs.LG])

    [http://arxiv.org/abs/2306.06196](http://arxiv.org/abs/2306.06196)

    本研究提出了一种基于神经网络的小而高效模型，用于确定两个心电图(ECG)是否来自同一患者。同时，还提出了一种方法，利用该模型检测记录-分配错误，实现了在现实场景中的应用，并在新的ECG数据集上进行了评估。

    

    心电图(ECG)通常被心脏病专家用于检测与心脏相关的病理情况，而可靠的ECG集合对于确诊非常重要。然而，在临床实践中，将记录的ECG分配给错误的患者可能会不经意地发生。本文与一家临床和研究机构合作，该机构认识到这一挑战并联系我们，我们提出了一项研究来解决这个问题。我们提出了一种小巧高效的基于神经网络的模型，用于确定两个ECG是否来自同一患者。我们的模型展现了很强的泛化能力，并在利用760倍更少的参数的情况下，在PTB-XL上实现了最新的画廊探针患者识别表现。此外，我们提出了一种技术，利用我们的模型来检测记录-分配错误，展示了它在一个现实场景中的适用性。最后，我们对新收集的ECG数据集进行了评估。

    Electrocardiograms (ECGs) are commonly used by cardiologists to detect heart-related pathological conditions. Reliable collections of ECGs are crucial for precise diagnosis. However, in clinical practice, the assignment of captured ECG recordings to incorrect patients can occur inadvertently. In collaboration with a clinical and research facility which recognized this challenge and reached out to us, we present a study that addresses this issue. In this work, we propose a small and efficient neural-network based model for determining whether two ECGs originate from the same patient. Our model demonstrates great generalization capabilities and achieves state-of-the-art performance in gallery-probe patient identification on PTB-XL while utilizing 760x fewer parameters. Furthermore, we present a technique leveraging our model for detection of recording-assignment mistakes, showcasing its applicability in a realistic scenario. Finally, we evaluate our model on a newly collected ECG dataset
    
[^18]: Ada-NAV：用于机器人导航的自适应轨迹优化策略学习方法

    Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation. (arXiv:2306.06192v1 [cs.RO])

    [http://arxiv.org/abs/2306.06192](http://arxiv.org/abs/2306.06192)

    Ada-NAV是一种自适应轨迹优化策略学习方法，采用降低策略随机性的方法平衡探索与利用，提高机器人导航任务的采样效率。在真实世界的测试中表现优异，可以在更短的采样时间内取得更高的性能。

    

    强化学习方法在学习机器人导航策略方面十分有效，但其采样效率低的问题也十分明显。在策略优化中，这种效率低下部分来自于未能适当地平衡探索与利用的问题，特别是在面对非静态时。为了加入探索与利用的平衡以提高采样效率，我们提出了Ada-NAV，一种自适应轨迹长度方案，其中长度随着策略的随机性（用其Shannon或差分熵表示）的减小而增加。我们的自适应轨迹长度方案由于更频繁的梯度更新强调了训练开始时的探索，后来则更强调利用。在网格世界，仿真机器人环境和真实世界机器人实验中，我们证明了该方法的优点，表现在性能和采样效率上均优于常数和随机采样的轨迹长度。在固定的样本预算下，相对于现有的基准方法，Ada-NAV的性能提高了高达46％，采样数量减少了高达80％。

    Reinforcement learning methods, while effective for learning robotic navigation strategies, are known to be highly sample inefficient. This sample inefficiency comes in part from not suitably balancing the explore-exploit dilemma, especially in the presence of non-stationarity, during policy optimization. To incorporate a balance of exploration-exploitation for sample efficiency, we propose Ada-NAV, an adaptive trajectory length scheme where the length grows as a policy's randomness, represented by its Shannon or differential entropy, decreases. Our adaptive trajectory length scheme emphasizes exploration at the beginning of training due to more frequent gradient updates and emphasizes exploitation later on with longer trajectories. In gridworld, simulated robotic environments, and real-world robotic experiments, we demonstrate the merits of the approach over constant and randomly sampled trajectory lengths in terms of performance and sample efficiency. For a fixed sample budget, Ada-N
    
[^19]: FasterViT：具有分层注意力的快速视觉变换器

    FasterViT: Fast Vision Transformers with Hierarchical Attention. (arXiv:2306.06189v1 [cs.CV])

    [http://arxiv.org/abs/2306.06189](http://arxiv.org/abs/2306.06189)

    本研究设计了一种新型混合CNN-ViT神经网络FasterViT，引入了具有分层注意力的方法HAT，将全局自我注意力分解为多级注意力，实现了高效的跨窗口通信。 FasterViT在精度和图像吞吐量方面达到了SOTA前沿水平，并已在分类，物体检测和分割等CV任务中得到广泛验证。

    

    本研究设计了一种新型混合CNN-ViT神经网络，命名为FasterViT，专注于计算机视觉应用中的高图像吞吐量。FasterViT将CNN中快速本地表示学习的优点与ViT中的全局建模特性相结合。我们引入了分层注意力（HAT）方法，将具有二次复杂度的全局自我注意力分解为具有较小计算成本的多级注意力。我们受益于高效的基于窗口的自我注意力，每个窗口都可以访问专门用于本地和全局表示学习的专用载体令牌。在高层次上，全局自我注意力实现了较低成本的跨窗口通信，FasterViT在精度与图像吞吐量方面实现了SOTA前沿水平，并已在各种CV任务中进行了广泛验证，包括分类，物体检测和分割。我们还展示了HAT可以用作现有CNN架构的即插即用模块，以改善性能。

    We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy \vs image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for exi
    
[^20]: HypLL: 希亚空间深度学习库

    HypLL: The Hyperbolic Learning Library. (arXiv:2306.06154v1 [cs.LG])

    [http://arxiv.org/abs/2306.06154](http://arxiv.org/abs/2306.06154)

    HypLL是一个使用希亚空间的深度学习库，基于PyTorch，旨在使其易于使用，搭建希亚网络模块，特别适用于处理层次化数据和使用少量嵌入维度，是一种新的、开放的研究方向。

    

    在机器学习、多媒体和计算机视觉等领域，希亚空间深度学习正迅速引起关注。深度网络通常在欧几里得空间中运行，隐含地假设数据在规则网格上。最近的研究表明，当处理层次化数据和使用少量嵌入维度时，希亚几何提供了一个可行的深度学习基础。然而，目前没有可访问的开源库用于构建类似于众所周知的深度学习库的希亚网络模块。我们提出了HypLL, 即希亚空间深度学习库，以将希亚深度学习的进展聚集在一起。HypLL建立在PyTorch之上，特别强调其易用性设计，以吸引广泛的受众关注这个新的和开放的研究方向。代码可在以下网址找到：https://github.com/maxvanspengler/hyperbolic_learning_library。压缩文件可在以下网址找到：https://d

    Deep learning in hyperbolic space is quickly gaining traction in the fields of machine learning, multimedia, and computer vision. Deep networks commonly operate in Euclidean space, implicitly assuming that data lies on regular grids. Recent advances have shown that hyperbolic geometry provides a viable alternative foundation for deep learning, especially when data is hierarchical in nature and when working with few embedding dimensions. Currently however, no accessible open-source library exists to build hyperbolic network modules akin to well-known deep learning libraries. We present HypLL, the Hyperbolic Learning Library to bring the progress on hyperbolic deep learning together. HypLL is built on top of PyTorch, with an emphasis in its design for easy-of-use, in order to attract a broad audience towards this new and open-ended research direction. The code is available at: https://github.com/maxvanspengler/hyperbolic_learning_library. The compressed archive is available at: https://d
    
[^21]: EfficientBioAI：使生物成像AI模型在能量、延迟和表示方面更高效

    EfficientBioAI: Making Bioimaging AI Models Efficient in Energy, Latency and Representation. (arXiv:2306.06152v1 [cs.LG])

    [http://arxiv.org/abs/2306.06152](http://arxiv.org/abs/2306.06152)

    EfficientBioAI是一个即插即用的工具箱，可以压缩生物成像AI模型，使它们在CPU和GPU上运行时能够显著减少能源成本和推理时间，而不会影响准确性。

    

    如今，人工智能（AI）在生物成像分析中被广泛使用，但由于模型规模和复杂性的增长以及现代生物医学研究中需要快速增长的分析需求，AI模型的效率，如能量消耗和延迟，是不可忽视的。就像我们可以压缩大型图像以实现高效存储和共享一样，我们也可以压缩AI模型以实现高效应用和部署。在这项工作中，我们提出了EfficientBioAI，这是一个即插即用的工具箱，可以压缩给定的生物成像AI模型，使它们在CPU和GPU上运行时能够显著减少能源成本和推理时间，而不会影响准确性。在某些情况下，压缩后的预测准确率甚至可能会增加，因为压缩过程可以去除模型表示中的冗余信息，从而减少过度拟合。从四种不同的生物图像分析应用中，我们观察到推理过程中的速度提高了约2-5倍，在GPU上提高了约3-10倍，并且准确性始终得到保证。

    Artificial intelligence (AI) has been widely used in bioimage image analysis nowadays, but the efficiency of AI models, like the energy consumption and latency is not ignorable due to the growing model size and complexity, as well as the fast-growing analysis needs in modern biomedical studies. Like we can compress large images for efficient storage and sharing, we can also compress the AI models for efficient applications and deployment. In this work, we present EfficientBioAI, a plug-and-play toolbox that can compress given bioimaging AI models for them to run with significantly reduced energy cost and inference time on both CPU and GPU, without compromise on accuracy. In some cases, the prediction accuracy could even increase after compression, since the compression procedure could remove redundant information in the model representation and therefore reduce over-fitting. From four different bioimage analysis applications, we observed around 2-5 times speed-up during inference and 3
    
[^22]: 从图像-标题对标注中获取边界框

    Read, look and detect: Bounding box annotation from image-caption pairs. (arXiv:2306.06149v1 [cs.CV])

    [http://arxiv.org/abs/2306.06149](http://arxiv.org/abs/2306.06149)

    本文提出了一种通过图像-标题对进行弱监督学习的方法来实现定位和标注图像中的对象，并在实验中实现了47.51%的短语接地以及21.1 和 10.5的新的最高mAP得分。

    

    提出了一种使用图像-标题对进行对象定位和标注的方法，在训练过程中利用了较弱的监督，并通过结合最近的视觉-语言模型和自监督视觉转换器来实现短语接地和目标检测。实验结果表明该方法的有效性。

    Various methods have been proposed to detect objects while reducing the cost of data annotation. For instance, weakly supervised object detection (WSOD) methods rely only on image-level annotations during training. Unfortunately, data annotation remains expensive since annotators must provide the categories describing the content of each image and labeling is restricted to a fixed set of categories. In this paper, we propose a method to locate and label objects in an image by using a form of weaker supervision: image-caption pairs. By leveraging recent advances in vision-language (VL) models and self-supervised vision transformers (ViTs), our method is able to perform phrase grounding and object detection in a weakly supervised manner. Our experiments demonstrate the effectiveness of our approach by achieving a 47.51% recall@1 score in phrase grounding on Flickr30k Entities and establishing a new state-of-the-art in object detection by achieving 21.1 mAP 50 and 10.5 mAP 50:95 on MS COC
    
[^23]: 人工智能与辐射防护：革命性还是更新换代？(arXiv:2306.06148v1 [cs.LG])

    Artificial intelligence and radiation protection. A game changer or an update?. (arXiv:2306.06148v1 [cs.LG])

    [http://arxiv.org/abs/2306.06148](http://arxiv.org/abs/2306.06148)

    本文介绍了基于机器学习的辐射防护方法，探讨了人工智能在辐射防护中的潜在优势和障碍，并提出了促进科学技术成果的合作方式。

    

    人工智能被认为是本世纪最颠覆性的技术之一，具有无数的应用。那么它对辐射防护意味着什么？本文介绍了基于机器学习（ML）的方法的基本原理，并介绍了在不同领域的辐射防护中的首次应用。预计人工智能在辐射防护中的使用将越来越广泛。因此，本文探讨了一些优点和潜在的障碍和问题，包括伦理问题。作者建议辐射防护专业人员与数据科学家专家合作，加速并指导算法的开发，以获得有效的科学技术成果。

    Artificial intelligence (AI) is regarded as one of the most disruptive technology of the century and with countless applications. What does it mean for radiation protection? This article describes the fundamentals of machine learning (ML) based methods and presents the inaugural applications in different fields of radiation protection. It is foreseen that the usage of AI will increase in radiation protection. Consequently, this article explores some of the benefits and also the potential barriers and questions, including ethical ones, that can come out. The article proposes that collaboration between radiation protection professionals and data scientist experts can accelerate and guide the development of the algorithms for effective scientific and technological outcomes.
    
[^24]: SentiGOLD：一个大型孟加拉语多领域情感分析黄金标准数据集及其评估

    SentiGOLD: A Large Bangla Gold Standard Multi-Domain Sentiment Analysis Dataset and its Evaluation. (arXiv:2306.06147v1 [cs.CL])

    [http://arxiv.org/abs/2306.06147](http://arxiv.org/abs/2306.06147)

    本文介绍了SentiGOLD，一个孟加拉语多领域情感分析数据集。该数据集由70,000个来自不同来源的样本组成，遵守了政府和语言学委员会商定的语言约定，包括了30个领域和5个情感类别。在具有鲁棒性的注释方案下，该数据集的互评一致性表现出色，可用于建立孟加拉语情感分析模型。

    

    本研究介绍了SentiGOLD，一个孟加拉语多领域情感分析数据集。它由来自不同来源的70,000个样本组成，并由一组性别平衡的语言学家进行注释。SentiGOLD遵守孟加拉国政府和孟加拉语言学委员会商定的既定语言约定。与英语和其他语言不同，由于缺乏国家语言学框架，孟加拉语缺乏标准的情感分析数据集。该数据集包括在线视频评论、社交媒体帖子、博客、新闻和其他来源的数据，并严格维护领域和类别分布。它涵盖了30个领域（如政治、娱乐、体育），包括5个情感类别（强烈的负面、弱的负面、中性和强烈的正面）。由国家语言学委员会批准的注释方案确保具有鲁棒的互评一致性（IAA），菲利斯kappa得分为0.88。内部和跨数据集的评估表明...

    This study introduces SentiGOLD, a Bangla multi-domain sentiment analysis dataset. Comprising 70,000 samples, it was created from diverse sources and annotated by a gender-balanced team of linguists. SentiGOLD adheres to established linguistic conventions agreed upon by the Government of Bangladesh and a Bangla linguistics committee. Unlike English and other languages, Bangla lacks standard sentiment analysis datasets due to the absence of a national linguistics framework. The dataset incorporates data from online video comments, social media posts, blogs, news, and other sources while maintaining domain and class distribution rigorously. It spans 30 domains (e.g., politics, entertainment, sports) and includes 5 sentiment classes (strongly negative, weakly negative, neutral, and strongly positive). The annotation scheme, approved by the national linguistics committee, ensures a robust Inter Annotator Agreement (IAA) with a Fleiss' kappa score of 0.88. Intra- and cross-dataset evaluatio
    
[^25]: 隐藏分类层：关于数据隐藏表示中更高线性可分性的研究

    Hidden Classification Layers: a study on Data Hidden Representations with a Higher Degree of Linear Separability between the Classes. (arXiv:2306.06146v1 [cs.LG])

    [http://arxiv.org/abs/2306.06146](http://arxiv.org/abs/2306.06146)

    本文中，研究了一种新颖的培训方法影响深层网络分类器性能，并提出了一个新的神经网络架构，在数据隐藏表示中达到更高的线性可分性。

    

    在分类问题的背景下，深度学习（DL）方法代表了最先进的技术。许多深度学习方法都基于标准的多层前馈神经网络的变种。这些也被称为深度网络。基本思想是每个隐藏神经层完成一种数据转换，预期使数据表示“比之前更线性可分”，以获得尽可能线性可分的最终数据表示。然而，确定可以执行这些转换的适当神经网络参数是一个关键问题。在本文中，我们研究了一种培训方法对深层网络分类器性能的影响，这种方法倾向于使用标准方法相比，隐藏层的数据表示具有更高的类之间线性可分性。为此，我们提出了一个神经网络架构，该架构引入了一个涉及误差函数的新颖培训方法。

    In the context of classification problems, Deep Learning (DL) approaches represent state of art. Many DL approaches are based on variations of standard multi-layer feed-forward neural networks. These are also referred to as deep networks. The basic idea is that each hidden neural layer accomplishes a data transformation which is expected to make the data representation "somewhat more linearly separable" than the previous one to obtain a final data representation which is as linearly separable as possible. However, determining the appropriate neural network parameters that can perform these transformations is a critical problem. In this paper, we investigate the impact on deep network classifier performances of a training approach favouring solutions where data representations at the hidden layers have a higher degree of linear separability between the classes with respect to standard methods. To this aim, we propose a neural network architecture which induces an error function involvin
    
[^26]: WePaMaDM-Outlier Detection: 使用模式方法进行加权异常值检测的大数据挖掘方法

    WePaMaDM-Outlier Detection: Weighted Outlier Detection using Pattern Approaches for Mass Data Mining. (arXiv:2306.06139v1 [cs.LG])

    [http://arxiv.org/abs/2306.06139](http://arxiv.org/abs/2306.06139)

    本文提出了一种使用模式方法进行加权异常值检测的大数据挖掘方法，可揭示系统故障、欺诈活动和数据模式的重要信息。

    

    加权异常值检测是一种识别数据集中异常或异常数据点的方法，可能由各种因素如人为错误、欺诈或设备故障引起。检测异常值可以揭示有关系统故障、欺诈活动和数据模式的重要信息，帮助专家解决这些异常的根本原因。然而，创建正常数据模式的模型以识别异常值可能由于输入数据的性质、标记数据的可用性以及问题的特定要求而具有挑战性。本文针对特定的大规模数据挖掘领域提出了WePaMaDM-Outlier Detection方法，证明这样的技术是依赖于特定领域并通常针对特定问题制定的。然而，类似的领域可以进行修改以适应解决方案。本研究还研究了数据建模在监控、故障检测和趋势分析中的异常点检测技术的重要性。

    Weighted Outlier Detection is a method for identifying unusual or anomalous data points in a dataset, which can be caused by various factors like human error, fraud, or equipment malfunctions. Detecting outliers can reveal vital information about system faults, fraudulent activities, and patterns in the data, assisting experts in addressing the root causes of these anomalies. However,creating a model of normal data patterns to identify outliers can be challenging due to the nature of input data, labeled data availability, and specific requirements of the problem. This article proposed the WePaMaDM-Outlier Detection with distinct mass data mining domain, demonstrating that such techniques are domain-dependent and usually developed for specific problem formulations. Nevertheless, similar domains can adapt solutions with modifications. This work also investigates the significance of data modeling in outlier detection techniques in surveillance, fault detection, and trend analysis, also re
    
[^27]: 多智能体强化学习的鲁棒性测试：对关键智能体进行状态扰动

    Robustness Testing for Multi-Agent Reinforcement Learning: State Perturbations on Critical Agents. (arXiv:2306.06136v1 [cs.LG])

    [http://arxiv.org/abs/2306.06136](http://arxiv.org/abs/2306.06136)

    提出了一个新的多智能体强化学习鲁棒性测试框架RTCA，采用基于差分进化的关键智能体攻击方法，关键智能体通过团队合作政策评估方法被选取为受害者。该框架在鲁棒性测试中表现出优异的性能。

    

    多智能体强化学习（MARL）已经广泛应用于智能交通和无人机等许多领域。然而，大多数MARL算法容易受到智能体状态的对抗性扰动。对训练模型进行鲁棒性测试是确认模型在面对意外扰动时可信赖的基本步骤。本文提出一种针对MARL的新型鲁棒性测试框架，以攻击关键智能体的状态（RTCA）。RTCA有两个创新点：1）基于差分进化（DE）的方法选定关键的智能体作为受害者，并为它们提供最坏情况的联合动作建议；2）采用团队合作政策评估方法作为DE优化的目标函数。然后，基于最坏情况的联合动作生成关键智能体的对抗性状态扰动。这是第一个具有不同受害者智能体的鲁棒性测试框架。RTCA在鲁棒性测试中表现出了出色的性能。

    Multi-Agent Reinforcement Learning (MARL) has been widely applied in many fields such as smart traffic and unmanned aerial vehicles. However, most MARL algorithms are vulnerable to adversarial perturbations on agent states. Robustness testing for a trained model is an essential step for confirming the trustworthiness of the model against unexpected perturbations. This work proposes a novel Robustness Testing framework for MARL that attacks states of Critical Agents (RTCA). The RTCA has two innovations: 1) a Differential Evolution (DE) based method to select critical agents as victims and to advise the worst-case joint actions on them; and 2) a team cooperation policy evaluation method employed as the objective function for the optimization of DE. Then, adversarial state perturbations of the critical agents are generated based on the worst-case joint actions. This is the first robustness testing framework with varying victim agents. RTCA demonstrates outstanding performance in terms of 
    
[^28]: 生成模型中的内容审核安全与公平性

    Safety and Fairness for Content Moderation in Generative Models. (arXiv:2306.06135v1 [cs.LG])

    [http://arxiv.org/abs/2306.06135](http://arxiv.org/abs/2306.06135)

    生成模型在训练数据中模仿最糟糕的内容，通过安全输入和输出过滤器实现负责任部署；通过安全、公平和指标公平的定义，列举了每个领域可能遇到的例子损害，并提供了损害量化的演示。

    

    随着生成人工智能的显著进步，新技术正迅速部署到生成组件中。生成模型通常是在大型数据集上进行训练，导致其行为可能模仿训练数据中最糟糕的内容。负责任地部署生成技术需要内容审核策略，例如安全输入和输出过滤器。在此，我们提供一个理论框架，用于概念化文本到图像生成技术的负责任内容审核，包括如何实证地衡量我们列举的构造。我们定义和区分了安全、公平和指标公平的概念，并列举了每个领域可能出现的例子损害。然后，我们提供了如何量化定义的损害的演示。最后，我们总结了我们演示的损害量化风格如何实现数据驱动的内容审核决策。

    With significant advances in generative AI, new technologies are rapidly being deployed with generative components. Generative models are typically trained on large datasets, resulting in model behaviors that can mimic the worst of the content in the training data. Responsible deployment of generative technologies requires content moderation strategies, such as safety input and output filters. Here, we provide a theoretical framework for conceptualizing responsible content moderation of text-to-image generative technologies, including a demonstration of how to empirically measure the constructs we enumerate. We define and distinguish the concepts of safety, fairness, and metric equity, and enumerate example harms that can come in each domain. We then provide a demonstration of how the defined harms can be quantified. We conclude with a summary of how the style of harms quantification we demonstrate enables data-driven content moderation decisions.
    
[^29]: 可信机器学习的声音解释

    Sound Explanation for Trustworthy Machine Learning. (arXiv:2306.06134v1 [cs.LG])

    [http://arxiv.org/abs/2306.06134](http://arxiv.org/abs/2306.06134)

    本篇论文提出了声音解释的概念，以提供足够的信息来因果解释机器学习系统进行的预测，反对通过将分数归因于输入组件来解释黑盒模型的惯例，并且提出应用特征选择作为癌症预测模型的声音解释，以建立临床医生之间的信任。

    

    我们采用正式的方法来解决机器学习系统的可解释性问题。我们反对通过将分数归因于输入组件来解释黑盒模型的惯例，因为这种解释方法的目标存在内在的冲突。我们证明没有任何归因算法能够满足特异性、可加性、完整性和基线不变性。然后，我们正式定义了在以前的工作中不正式采用的概念——声音解释。一个声音的解释需要提供足够的信息来因果解释系统所进行的预测。最后，我们提出应用特征选择作为癌症预测模型的声音解释，以建立临床医生之间的信任。

    We take a formal approach to the explainability problem of machine learning systems. We argue against the practice of interpreting black-box models via attributing scores to input components due to inherently conflicting goals of attribution-based interpretation. We prove that no attribution algorithm satisfies specificity, additivity, completeness, and baseline invariance. We then formalize the concept, sound explanation, that has been informally adopted in prior work. A sound explanation entails providing sufficient information to causally explain the predictions made by a system. Finally, we present the application of feature selection as a sound explanation for cancer prediction models to cultivate trust among clinicians.
    
[^30]: 探究生成人工智能与互联网的相互作用

    Towards Understanding the Interplay of Generative Artificial Intelligence and the Internet. (arXiv:2306.06130v1 [cs.AI])

    [http://arxiv.org/abs/2306.06130](http://arxiv.org/abs/2306.06130)

    探究生成AI和互联网之间互动的影响，生成AI可能成为数据仓库贡献者并影响后续训练，提出未来版本生成AI工具在使用混合数据训练时会出现的问题和挑战。

    

    近来，如DALL-E、MidJourney或ChatGPT等能够生成逼真图像或文本的生成人工智能（AI）工具的广泛应用，引发了这些技术的社会影响成为公共争论的核心。这些工具是可能的，是因为互联网上公开可用的大量数据（文本和图像）。与此同时，这些生成人工智能工具成为内容创作者，已经为未来模型训练所需的数据做出了贡献。因此，生成人工智能工具的未来版本将通过人工创建和人工智能生成的内容混合训练，引发潜在的生成人工智能和公共数据仓库之间的反馈循环。这种互动引发了许多问题：未来版本的生成人工智能工具在混合的真实和人工智能生成的数据上训练时将如何表现？它们是否会随着新的数据集进化和改进还是相反变得更差？进化是否会引入偏误或收缩视野？

    The rapid adoption of generative Artificial Intelligence (AI) tools that can generate realistic images or text, such as DALL-E, MidJourney, or ChatGPT, have put the societal impacts of these technologies at the center of public debate. These tools are possible due to the massive amount of data (text and images) that is publicly available through the Internet. At the same time, these generative AI tools become content creators that are already contributing to the data that is available to train future models. Therefore, future versions of generative AI tools will be trained with a mix of human-created and AI-generated content, causing a potential feedback loop between generative AI and public data repositories. This interaction raises many questions: how will future versions of generative AI tools behave when trained on a mixture of real and AI generated data? Will they evolve and improve with the new data sets or on the contrary will they degrade? Will evolution introduce biases or red
    
[^31]: 基于深度卷积自编码器的电力系统干扰无监督聚类方法

    Unsupervised clustering of disturbances in power systems via deep convolutional autoencoders. (arXiv:2306.06124v1 [eess.SP])

    [http://arxiv.org/abs/2306.06124](http://arxiv.org/abs/2306.06124)

    本文提出了一种基于卷积自编码器和K-means聚类的无监督分类方法，可将电力系统中的干扰波形聚类为涉及电压下降、中断、瞬态、正常和谐波畸变等类别，从而实现过滤异常波形和常规波形。

    

    当电网检测到异常事件时，电力质量(PQ)仪器会记录PQ事件。使用机器学习的神经网络可以帮助准确分类记录的波形并帮助电力系统工程师诊断和纠正问题的根本原因。然而，在电力系统干扰期间捕获的许多波形需要进行标记以进行监督学习，使工程师需要手动处理或未看到大量数据记录。本文提出了一种基于自编码器和K-means聚类的无监督技术，可将PQ事件聚类为涉及电压下降、中断、瞬态、正常和谐波畸变等类别，从而实现过滤异常波形和常规波形。该方法利用在分布网格中记录的三相场获得的电压波形进行演示。首先，卷积自编码器将输入信号压缩为一组较低特征。

    Power quality (PQ) events are recorded by PQ meters whenever anomalous events are detected on the power grid. Using neural networks with machine learning can aid in accurately classifying the recorded waveforms and help power system engineers diagnose and rectify the root causes of problems. However, many of the waveforms captured during a disturbance in the power system need to be labeled for supervised learning, leaving a large number of data recordings for engineers to process manually or go unseen. This paper presents an autoencoder and K-means clustering-based unsupervised technique that can be used to cluster PQ events into categories like sag, interruption, transients, normal, and harmonic distortion to enable filtering of anomalous waveforms from recurring or normal waveforms. The method is demonstrated using three-phase, field-obtained voltage waveforms recorded in a distribution grid. First, a convolutional autoencoder compresses the input signals into a set of lower feature 
    
[^32]: 《可解释人工智能中的对抗性攻击和防御：调查报告》

    Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey. (arXiv:2306.06123v1 [cs.CR])

    [http://arxiv.org/abs/2306.06123](http://arxiv.org/abs/2306.06123)

    本文总结了对抗性攻击和防御在可解释人工智能中的研究。列出了现有的不安全因素，并表明了本领域的新兴研究方向。

    

    可解释人工智能（XAI）方法被描绘为调试和信任统计和深度学习模型的治疗方式，以及解释它们的预测。然而，对抗机器学习的最新进展突出了最新解释的局限性和漏洞，这些进展令人对其安全性和可信度产生质疑。操纵、欺骗或洗白模型推理证据的可能性在高风险决策和知识发现中产生不利后果。本文总结了50多篇论文的研究，概述了针对机器学习模型解释的对抗攻击以及公平度量的研究。我们讨论了如何防御攻击并设计鲁棒的解释方法。我们列出XAI中现有的不安全因素，并概述了对抗性XAI（AdvXAI）的新兴研究方向。

    Explainable artificial intelligence (XAI) methods are portrayed as a remedy for debugging and trusting statistical and deep learning models, as well as interpreting their predictions. However, recent advances in adversarial machine learning highlight the limitations and vulnerabilities of state-of-the-art explanations, putting their security and trustworthiness into question. The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery. This concise survey of over 50 papers summarizes research concerning adversarial attacks on explanations of machine learning models, as well as fairness metrics. We discuss how to defend against attacks and design robust interpretation methods. We contribute a list of existing insecurities in XAI and outline the emerging research directions in adversarial XAI (AdvXAI).
    
[^33]: 运动治疗中3D姿势估计和惯性运动捕捉系统的优势和劣势

    Strengths and Weaknesses of 3D Pose Estimation and Inertial Motion Capture System for Movement Therapy. (arXiv:2306.06117v1 [eess.IV])

    [http://arxiv.org/abs/2306.06117](http://arxiv.org/abs/2306.06117)

    本研究比较了3D姿势估计和惯性运动捕捉系统在特定运动锻炼中的精度，为临床应用提供了有用信息。

    

    3D姿势估计为快速、非侵入性且准确的运动分析提供了机会。这对于临床应用也具有特殊的兴趣。目前，运动捕捉系统被应用，因为它们提供了强大而精确的数据获取，在临床应用中是必不可少的。在本研究中，我们比较了最先进的3D位置估计方法MeTrabs与已建立的惯性传感器系统MTw Awinda在特定运动锻炼中的精度。该研究使用并提供了10名受试者在各种运动治疗锻炼期间的平行记录的评估数据集。Awinda系统的信息和单眼姿态估计的帧被同步。为了进行比较，对踝关节、膝关节、背部和肘部的关节角度进行了临床相关参数的估计和评估，使用平均值、中位数和计算出的不同锻炼中关节角度之间的最大偏差。

    3D pose estimation offers the opportunity for fast, non-invasive, and accurate motion analysis. This is of special interest also for clinical use. Currently, motion capture systems are used, as they offer robust and precise data acquisition, which is essential in the case of clinical applications. In this study, we investigate the accuracy of the state-of-the-art 3D position estimation approach MeTrabs, compared to the established inertial sensor system MTw Awinda for specific motion exercises. The study uses and provides an evaluation dataset of parallel recordings from 10 subjects during various movement therapy exercises. The information from the Awinda system and the frames for monocular pose estimation are synchronized. For the comparison, clinically relevant parameters for joint angles of ankle, knee, back, and elbow flexion-extension were estimated and evaluated using mean, median, and maximum deviation between the calculated joint angles for the different exercises, camera posi
    
[^34]: 学习量化漏洞模式并匹配以定位语句级漏洞

    Learning to Quantize Vulnerability Patterns and Match to Locate Statement-Level Vulnerabilities. (arXiv:2306.06109v1 [cs.CR])

    [http://arxiv.org/abs/2306.06109](http://arxiv.org/abs/2306.06109)

    该论文提出了一种新颖的漏洞匹配方法，利用学习的漏洞代码库中的漏洞模式定位易受攻击语句，以解决传统漏洞检测方法中存在的识别漏洞范围不准确的问题。

    

    深度学习模型在识别软件漏洞中越来越流行。先前的研究发现，不同易受攻击的程序中的漏洞可能表现出类似的易受攻击范围，这些范围形成可通过监督训练为DL模型所学的可辨认的漏洞模式。然而，漏洞的易攻击范围仍以不同的空间位置和格式在程序中表现，这导致模型准确识别易受攻击语句面临挑战。尽管如此，最先进的漏洞检测方法仍未利用易受攻击程序中出现的漏洞模式。为充分利用易受攻击模式并释放DL模型的能力，在本文中，我们提出了一种新颖的漏洞匹配方法，受程序分析工具的启发，根据预定义的模式来定位漏洞。具体而言，我们学习了一份漏洞代码库，其中包括量化漏洞模式。

    Deep learning (DL) models have become increasingly popular in identifying software vulnerabilities. Prior studies found that vulnerabilities across different vulnerable programs may exhibit similar vulnerable scopes, implicitly forming discernible vulnerability patterns that can be learned by DL models through supervised training. However, vulnerable scopes still manifest in various spatial locations and formats within a program, posing challenges for models to accurately identify vulnerable statements. Despite this challenge, state-of-the-art vulnerability detection approaches fail to exploit the vulnerability patterns that arise in vulnerable programs. To take full advantage of vulnerability patterns and unleash the ability of DL models, we propose a novel vulnerability-matching approach in this paper, drawing inspiration from program analysis tools that locate vulnerabilities based on pre-defined patterns. Specifically, a vulnerability codebook is learned, which consists of quantize
    
[^35]: 评估生成AI系统在系统和社会中的社会影响

    Evaluating the Social Impact of Generative AI Systems in Systems and Society. (arXiv:2306.05949v1 [cs.CY])

    [http://arxiv.org/abs/2306.05949](http://arxiv.org/abs/2306.05949)

    提出了一种标准方法来评估任何模态的生成AI系统的社会影响，分为基础系统和社会方面的评估，涵盖7个社会影响类别，包括偏见、隐私保护、环境成本等。

    

    生成AI系统跨越文本、图像、音频、视频等多种模态，具有广泛的社会影响，但目前不存在官方标准来评估这些影响和应该评估哪些影响。本文提出了一种标准方法来评估任何模态的生成AI系统，分为两大类别：对于没有预定应用的基础系统可以评估什么，以及可以在社会中评估什么。我们描述了具体的社会影响类别以及如何评估基础技术系统、人民和社会。我们的基础系统框架定义了七个社会影响类别：偏见、刻板印象和表现性伤害；文化价值和敏感内容；不对等的性能；隐私和数据保护；财务成本；环境成本；以及数据和内容监管劳动成本。建议的评估方法适用于所有模态和分析。

    Generative AI systems across modalities, ranging from text, image, audio, and video, have broad social impacts, but there exists no official standard for means of evaluating those impacts and which impacts should be evaluated. We move toward a standard approach in evaluating a generative AI system for any modality, in two overarching categories: what is able to be evaluated in a base system that has no predetermined application and what is able to be evaluated in society. We describe specific social impact categories and how to approach and conduct evaluations in the base technical system, then in people and society. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to all modalities and analyses of the li
    
[^36]: 基于Implicit Neural Representations的时间序列连续建模用于插值和预测

    Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations. (arXiv:2306.05880v1 [cs.LG])

    [http://arxiv.org/abs/2306.05880](http://arxiv.org/abs/2306.05880)

    该论文提出了基于INR的时间序列连续建模方法，解决了处理缺失数据、不规则采样和多传感器不对准观测等重复建模问题，并在预测和插值任务中取得了最新的性能表现，具有很好的泛化能力。

    

    尽管时间序列建模已被广泛探索，但在面对真实世界的数据时仍面临重大挑战。我们提出了一种新颖的建模方法，利用Implicit Neural Representations (INR)。该方法使我们能够有效地捕捉时间序列的连续性，并提供了自然的解决方案，以处理缺失数据、处理不规则采样或来自多个传感器的不对准观测等重复建模问题。通过引入条件调制INR参数并利用元学习技术，我们解决了模型泛化到未见样本和时间窗口移位的问题。通过大量实验，我们的模型展示了在预测和插值任务中领先的性能，同时在处理许多竞争模型无法处理的各种具有挑战性的场景方面展现了灵活性。

    Although widely explored, time series modeling continues to encounter significant challenges when confronted with real-world data. We propose a novel modeling approach leveraging Implicit Neural Representations (INR). This approach enables us to effectively capture the continuous aspect of time series and provides a natural solution to recurring modeling issues such as handling missing data, dealing with irregular sampling, or unaligned observations from multiple sensors. By introducing conditional modulation of INR parameters and leveraging meta-learning techniques, we address the issue of generalization to both unseen samples and time window shifts. Through extensive experimentation, our model demonstrates state-of-the-art performance in forecasting and imputation tasks, while exhibiting flexibility in handling a wide range of challenging scenarios that competing models cannot.
    
[^37]: 推荐系统如何从大型语言模型中受益：一项调查研究

    How Can Recommender Systems Benefit from Large Language Models: A Survey. (arXiv:2306.05817v1 [cs.IR])

    [http://arxiv.org/abs/2306.05817](http://arxiv.org/abs/2306.05817)

    本文对将大型语言模型（LLM）应用于推荐系统进行了全面的调查研究，从两个角度总结了现有的研究工作：如何在推荐系统中调整LLM和调整LLM时在哪里调整。最后，我们提出了一些潜在的研究方向和挑战。

    

    推荐系统在匹配互联网应用程序用户的信息需求方面发挥着重要作用。在自然语言处理领域中，大型语言模型已经展现出了惊人的新兴能力（例如指令跟踪、推理），从而为将LLM调整到推荐系统中以提高性能和改善用户体验的研究方向带来了希望。在本文中，我们从应用导向的角度对此研究方向进行了全面的调查。我们首先从两个正交的角度总结了现有的研究工作：如何在推荐系统中调整LLM和调整LLM时在哪里调整。对于“在哪里”这个问题，我们讨论了LLM在推荐流程的不同阶段中可能发挥的作用，即特征工程、特征编码器、评分/排名函数和流程控制器。对于“如何”这个问题，我们调查了训练和推理策略，从而得出两个细粒度的分类标准，即是否调整LLM和是否将LLM作为独立模型或混合模型组件使用。最后，我们提出了在将LLM调整到RS中的一些挑战和潜在方向，包括与现有系统的集成、用户反馈、评估度量和知识蒸馏。

    Recommender systems (RS) play important roles to match users' information needs for Internet applications. In natural language processing (NLP) domains, large language model (LLM) has shown astonishing emergent abilities (e.g., instruction following, reasoning), thus giving rise to the promising research direction of adapting LLM to RS for performance enhancements and user experience improvements. In this paper, we conduct a comprehensive survey on this research direction from an application-oriented view. We first summarize existing research works from two orthogonal perspectives: where and how to adapt LLM to RS. For the "WHERE" question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, and pipeline controller. For the "HOW" question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLMs or not, a
    
[^38]: 物理信息神经网络在逆流自发渗透中的应用和预测：早期和晚期的模拟

    Simulation and Prediction of Countercurrent Spontaneous Imbibition at Early and Late Times Using Physics-Informed Neural Networks. (arXiv:2306.05554v1 [physics.comp-ph])

    [http://arxiv.org/abs/2306.05554](http://arxiv.org/abs/2306.05554)

    本文通过物理信息神经网络模型对多孔材料中的逆流自发渗透过程进行了早期和晚期的模拟和预测，并使用改变变量技术来改进模型性能。

    

    逆流自发渗透（COUCSI）是一种多孔材料中的过程，其中润湿相取代了非润湿相的位置。本文首次探讨了物理信息神经网络（PINNs）在解决早期（ET）和晚期（LT）COUCSI问题中的应用。同时，我们还研究了改变变量技术以改进PINNs的性能。我们通过改变自变量将COUCSI问题分别用XT-，XY-和Z-三种等效形式进行描述：第一个描述了饱和度作为规范化位置X和时间T的函数;第二个描述了X和Y=T^0.5作为函数的饱和度;第三个作为Z=X/T^0.5的唯一函数（仅在ET下有效）。该PINN模型使用前馈神经网络生成，并基于最小化加权损失函数进行训练，包括物理信息丢失项和与初始边界条件相对应的项。没有合成或实验数据被调用。

    Countercurrent spontaneous imbibition (COUCSI) is a process in porous materials in which a wetting phase displaces non-wetting phase. In this work, we investigate for the first time the application of Physics-Informed Neural Networks (PINNs) in solving the 1D COUCSI problem in both early (ET) and late (LT) times. Also novel, we examine the Change-of-Variables technique for improving the performance of PINNs. We formulated the COUCSI problem in three equivalent forms by changing the independent variables: XT-, XY-, and Z-formulations. The first describes saturation as function of normalized position X and time T; the second as function of X and Y=T^0.5; and the third as a sole function of Z=X/T^0.5 (valid only at ET). The PINN model was generated using a feed-forward neural network and trained based on minimizing a weighted loss function, including the physics-informed loss term and terms corresponding to the initial and boundary conditions. No synthetical or experimental data were invo
    
[^39]: CoCo: 一种用于无监督领域自适应图分类的耦合对比框架

    CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification. (arXiv:2306.04979v1 [cs.LG])

    [http://arxiv.org/abs/2306.04979](http://arxiv.org/abs/2306.04979)

    CoCo是一种耦合对比图表示学习框架，其中包含一个图卷积网络和一个分层图内核网络，通过耦合对比学习减少领域差异，用于无监督领域自适应图分类。

    

    虽然图神经网络在图分类中取得了显著成果，但它们通常需要大量特定任务的标签，这可能需要极大的代价来获得。一种可靠的解决方案是探索其他标注图以增强目标域的无监督学习，但如何将图神经网络应用到领域适应中仍未解决，因为对图拓扑的不充分探索以及相当大的领域偏差。本文提出了一种称为CoCo（Coupled Contrastive Graph Representation Learning）方案，该方案从耦合学习分支中提取拓扑信息，并通过耦合对比学习减少领域差异。CoCo包含一个图卷积网络分支和分层图内核网络分支，分别用隐式和显式方式探索图拓扑。此外，我们将耦合分支结合到一个全面的多视角对比学习框架中，

    Although graph neural networks (GNNs) have achieved impressive achievements in graph classification, they often need abundant task-specific labels, which could be extensively costly to acquire. A credible solution is to explore additional labeled graphs to enhance unsupervised learning on the target domain. However, how to apply GNNs to domain adaptation remains unsolved owing to the insufficient exploration of graph topology and the significant domain discrepancy. In this paper, we propose \underline{Co}upled \underline{Co}ntrastive Graph Representation Learning (\method{}), which extracts the topological information from coupled learning branches and reduces the domain discrepancy with coupled contrastive learning. \method{} contains a graph convolutional network branch and a hierarchical graph kernel network branch, which explore graph topology in implicit and explicit manners. Besides, we incorporate coupled branches into a holistic multi-view contrastive learning framework, which 
    
[^40]: INSTRUCTEVAL：面向指导调整的大型语言模型的整体评估

    INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. (arXiv:2306.04757v1 [cs.CL])

    [http://arxiv.org/abs/2306.04757](http://arxiv.org/abs/2306.04757)

    INSTRUCTEVAL是一个专注于指导调整的大型语言模型评估的综合套件，它采取了全面的方法来评估模型的性能，包括解决问题、写作能力和与人类价值观的一致性等特征。

    

    指导调整的大型语言模型已经从根本上改变了自然语言处理，已经在诸如对话代理等应用中显示出了巨大的潜力。这些模型，如GPT-4，不仅能够掌握语言，而且可以解决数学、编码、医学和法律等领域的复杂任务。尽管它们具有卓越的能力，但由于许多模型的黑盒性质和缺乏全面的评估研究，对它们的全部潜力仍然缺乏全面的理解。为了解决这些挑战，我们提出了INSTRUCTEVAL，一个更全面的评估套件，专门针对指导调整的大型语言模型。与以往的作品不同，我们的评估包括对模型基于解决问题、写作能力和与人类价值观的一致性的严格评估。我们采取了全面的方法来分析影响模型性能的各种因素，包括预训练基础、指导调整数据和训练。

    Instruction-tuned large language models have revolutionized natural language processing and have shown great potential in applications such as conversational agents. These models, such as GPT-4, can not only master language but also solve complex tasks in areas like mathematics, coding, medicine, and law. Despite their impressive capabilities, there is still a lack of comprehensive understanding regarding their full potential, primarily due to the black-box nature of many models and the absence of holistic evaluation studies. To address these challenges, we present INSTRUCTEVAL, a more comprehensive evaluation suite designed specifically for instruction-tuned large language models. Unlike previous works, our evaluation involves a rigorous assessment of models based on problem-solving, writing ability, and alignment to human values. We take a holistic approach to analyze various factors affecting model performance, including the pretraining foundation, instruction-tuning data, and train
    
[^41]: AGIQA-3K：一份用于AI生成图像质量评估的开放数据库

    AGIQA-3K: An Open Database for AI-Generated Image Quality Assessment. (arXiv:2306.04717v1 [cs.CV])

    [http://arxiv.org/abs/2306.04717](http://arxiv.org/abs/2306.04717)

    本论文提出了一个用于AI生成图像质量评估的开放数据库AGIQA-3K，并在其中进行了基准实验，提出了StairReward以提高主观文本到图像对齐评估的性能。

    

    随着文本到图像生成模型的快速发展，人工智能生成的图像（AGIs）已广泛应用于娱乐、教育、社交媒体等领域。然而，考虑到不同AGIs之间的大量质量差异，迫切需要与人类主观评分一致的质量模型。为了解决这个问题，我们广泛考虑了各种流行的AGI模型、不同提示和模型参数生成的AGI，并收集了感知质量和文本到图像对齐方面的主观评分，从而建立了迄今为止最为全面的AGI主观质量数据库AGIQA-3K。此外，我们在这个数据库上进行了基准实验，评估了当前图像质量评估（IQA）模型与人类感知之间的一致性，同时提出了StairReward，大大提高了主观文本到图像对齐评估的性能。我们相信，AGIQA-3K中的细粒度主观评分将有助于推动AI生成图像质量评估技术的发展。

    With the rapid advancements of the text-to-image generative model, AI-generated images (AGIs) have been widely applied to entertainment, education, social media, etc. However, considering the large quality variance among different AGIs, there is an urgent need for quality models that are consistent with human subjective ratings. To address this issue, we extensively consider various popular AGI models, generated AGI through different prompts and model parameters, and collected subjective scores at the perceptual quality and text-to-image alignment, thus building the most comprehensive AGI subjective quality database AGIQA-3K so far. Furthermore, we conduct a benchmark experiment on this database to evaluate the consistency between the current Image Quality Assessment (IQA) model and human perception, while proposing StairReward that significantly improves the assessment performance of subjective text-to-image alignment. We believe that the fine-grained subjective scores in AGIQA-3K wil
    
[^42]: 基于混合Actor-Critic强化学习的自适应频率绿灯最佳速度建议

    Adaptive Frequency Green Light Optimal Speed Advisory based on Hybrid Actor-Critic Reinforcement Learning. (arXiv:2306.04660v1 [cs.LG])

    [http://arxiv.org/abs/2306.04660](http://arxiv.org/abs/2306.04660)

    本文提出了一种基于混合Actor-Critic强化学习的自适应频率绿灯最佳速度建议模型，通过使用离散和连续演员网络来优化绿灯最佳速度建议系统的频率和加速度曲线，设计了新的奖励函数来平衡减少停车和最小化速度变化的权衡。实验结果显示，该模型可以有效地减少旅行时间和燃料消耗，并优于现有的最先进的GLOSA系统。

    

    绿灯最佳速度建议（GLOSA）系统建议车辆速度，以帮助它们在绿色时间通过路口，从而通过最小化在路口停车和怠速时间来减少交通拥堵和燃料消耗。但是，以前的研究集中于优化GLOSA算法，忽略了GLOSA系统的速度建议频率。具体而言，一些研究在每个决策步骤提供速度建议，导致冗余建议，而其他人仅为车辆计算最佳速度，无法适应动态交通。在本文中，我们提出了一种基于混合PPO（H-PPO）的自适应频率GLOSA（AF-GLOSA）模型，其采用了一个actor-critic架构和一个混合actor网络。混合演员网络包括一个离散演员，输出咨询频率和一个连续演员，输出加速度曲线。此外，我们设计了一种新的奖励函数来平衡减少停车和最小化速度变化的权衡。实验结果表明，所提出的AF-GLOSA模型可以有效地减少旅行时间和燃料消耗，并优于现有的最先进的GLOSA系统。

    Green Light Optimal Speed Advisory (GLOSA) system suggests speeds to vehicles to assist them in passing through intersections during green intervals, thus reducing traffic congestion and fuel consumption by minimizing the number of stops and idle times at intersections. However, previous research has focused on optimizing the GLOSA algorithm, neglecting the frequency of speed advisory by the GLOSA system. Specifically, some studies provide speed advisory profile at each decision step, resulting in redundant advisory, while others calculate the optimal speed for the vehicle only once, which cannot adapt to dynamic traffic. In this paper, we propose an Adaptive Frequency GLOSA (AF-GLOSA) model based on Hybrid Proximal Policy Optimization (H-PPO), which employs an actor-critic architecture with a hybrid actor network. The hybrid actor network consists of a discrete actor that outputs advisory frequency and a continuous actor that outputs acceleration profiles. Additionally, we design a no
    
[^43]: 利用选项改进模仿学习对抗性示范的性能表现

    Divide and Repair: Using Options to Improve Performance of Imitation Learning Against Adversarial Demonstrations. (arXiv:2306.04581v1 [cs.LG])

    [http://arxiv.org/abs/2306.04581](http://arxiv.org/abs/2306.04581)

    本文提出了一种利用选项改进模仿学习对抗性示范的性能表现的新技术，可以识别未被对手显着修改的演示轨迹并仅从中进行学习。

    

    在本文中，我们考虑了从教师或专家的演示中学习执行任务的问题，并提出了一种新的技术，可以识别未被对手显着修改的演示轨迹的部分，并使用时间上扩展的策略或选项进行学习。首先，我们定义了一种基于演示轨迹的空间和时间特征的轨迹分歧度量，以检测和丢弃已被对手显着修改的轨迹部分，并可能降低学习者的性能，如果用于学习。然后，我们使用基于选项的算法来分割轨迹，并只从已确定为可接受的轨迹部分中进行学习。我们提供了我们技术的理论结果，以表明修复部分轨迹可以改善学习效果。

    We consider the problem of learning to perform a task from demonstrations given by teachers or experts, when some of the experts' demonstrations might be adversarial and demonstrate an incorrect way to perform the task. We propose a novel technique that can identify parts of demonstrated trajectories that have not been significantly modified by the adversary and utilize them for learning, using temporally extended policies or options. We first define a trajectory divergence measure based on the spatial and temporal features of demonstrated trajectories to detect and discard parts of the trajectories that have been significantly modified by an adversarial expert, and, could degrade the learner's performance, if used for learning, We then use an options-based algorithm that partitions trajectories and learns only from the parts of trajectories that have been determined as admissible. We provide theoretical results of our technique to show that repairing partial trajectories improves the 
    
[^44]: 以双策略为自模型的规划

    Dual policy as self-model for planning. (arXiv:2306.04440v1 [cs.AI])

    [http://arxiv.org/abs/2306.04440](http://arxiv.org/abs/2306.04440)

    该论文探究了使用精简策略网络作为自我模型的优缺点，并通过实验结果表明，使用经过精简的策略网络作为自我模型可以稳定训练，并且有更快的收敛速度。

    

    规划是一种数据有效的决策策略，代理通过探索可能的未来状态来选择候选动作。当存在高维行动空间时，为了模拟未来状态，必须使用自己的决策策略来限制所需探索的动作数量。我们将用于模拟自己决策的模型称为代理的自我模型。尽管在规划行动时，世界模型通常与自我模型一起隐含地使用，但如何设计自我模型仍不清楚。受当前强化学习方法和神经科学的启发，我们探讨了使用精简策略网络作为自我模型的优缺点。在这样的双策略代理中，一个无模型策略和一个经过精简的策略分别用于无模型动作和计划动作。我们在一个生态相关的参数环境上的结果表明，使用经过精简的策略网络作为自我模型可以稳定训练，并且有更快的收敛速度。

    Planning is a data efficient decision-making strategy where an agent selects candidate actions by exploring possible future states. To simulate future states when there is a high-dimensional action space, the knowledge of one's decision making strategy must be used to limit the number of actions to be explored. We refer to the model used to simulate one's decisions as the agent's self-model. While self-models are implicitly used widely in conjunction with world models to plan actions, it remains unclear how self-models should be designed. Inspired by current reinforcement learning approaches and neuroscience, we explore the benefits and limitations of using a distilled policy network as the self-model. In such dual-policy agents, a model-free policy and a distilled policy are used for model-free actions and planned actions, respectively. Our results on a ecologically relevant, parametric environment indicate that distilled policy network for self-model stabilizes training, has faster i
    
[^45]: 社交机器人在儿童哮喘治疗依从性方面的应用

    Social robots to improve therapeutic adherence in pediatric asthma. (arXiv:2306.04422v1 [cs.AI])

    [http://arxiv.org/abs/2306.04422](http://arxiv.org/abs/2306.04422)

    通过与人类形态机器人的游戏化会话，可以提高儿童哮喘患者的治疗依从性。

    

    在慢性疾病中，正确诊断和提供最适当的治疗通常并不足以保证患者临床状况的改善。低治疗依从性是阻碍达到治疗目标的主要原因之一，尤其是在某些疾病和特定目标患者（如儿童）中尤为常见。一种有趣的技术可以用于支持临床实践，以实现更好的健康结果。我们假设与传统的治疗教育方法相比，与人类形态机器人的游戏化会话可以更加有效地教授儿童哮喘患者正确的吸入方法。在这个角度上，我们描述了在Pepper机器人平台上实现的交互式模块以及计划于2020年在Palermo的CNR肺部过敏兒童诊所开展的一项研究。由于COVID-19紧急情况的原因，这项研究被取消，但我们提供初步结果，以评估方法的可行性和同龄人的兴趣。通过分析培训前后的吸入习惯的比较和给参与者的问卷调查结果来评估教育结果。

    In chronic diseases, obtaining a correct diagnosis and providing the most appropriate treatments often is not enough to guarantee an improvement of the clinical condition of a patient. Poor adherence to medical prescriptions constitutes one of the main causes preventing achievement of therapeutic goals. This is generally true especially for certain diseases and specific target patients, such as children. An engaging and entertaining technology can be exploited in support of clinical practices to achieve better health outcomes. Our assumption is that a gamified session with a humanoid robot, compared to the usual methodologies for therapeutic education, can be more incisive in learning the correct inhalation procedure in children affected by asthma. In this perspective, we describe an interactive module implemented on the Pepper robotic platform and the setting of a study that was planned in 2020 to be held at the Pneumoallergology Pediatric clinic of CNR in Palermo. The study was cance
    
[^46]: 用于基于检索的对话系统的上下文掩码自编码器

    ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v1 [cs.CL])

    [http://arxiv.org/abs/2306.04357](http://arxiv.org/abs/2306.04357)

    本研究提出了一种针对对话响应选择的后训练技术Dial-MAE，利用生成方法更好地压缩对话语义至密集向量，并提高对话响应选择准确性。

    

    对话响应选择旨在根据给定的用户和系统话语历史记录从几个候选响应中选择适当的响应。最近的研究通过后训练大多依赖于单纯的掩码语言建模方法来提高对话响应选择的准确性。但是，最近开发的生成方法在IR社区展示了有希望的文本表示能力，这可能会导致更好的对话语义建模。因此，在本文中，我们提出 Dial-MAE（对话上下文掩码自编码器），这是一种简单而有效的针对对话响应选择的后训练技术。 Dial-MAE使用一个不对称的编码器-解码器架构，学习将对话的语义更好地压缩到密集向量中。 Dial-MAE的过程包括由深度编码器创建带有掩码对话上下文的对话嵌入，然后是浅解码器，该解码器使用此嵌入以及上下文向量来生成响应。

    Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Recent studies have been improving the accuracy of dialogue response selection through post-training, mostly relying on naive masked language modeling methods. However, the recently developed generative methods have shown promising text representation capabilities in IR community, which could potentially lead to better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE (Dialogue Contextual Masking Auto-encoder), a straightforward yet effective post-training technique tailored for dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture that learns to better compress the semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE involves a deep encoder creating a dialogue embedding with the masked dialogue context, followed by a shallow decoder that uses this embedding along with
    
[^47]: 通过扩散规划进行职业篮球运动员行为合成

    Professional Basketball Player Behavior Synthesis via Planning with Diffusion. (arXiv:2306.04090v1 [cs.AI])

    [http://arxiv.org/abs/2306.04090](http://arxiv.org/abs/2306.04090)

    本文中提出了一个名为PLAYBEST的方法，通过使用扩散概率模型从篮球比赛历史数据中学习策略，生成更加真实和多样化的球员行为，从而提高球员的决策制定效果，并在实验中得到了验证。

    

    在多智能体系统中动态规划已经被应用于改善各种领域的决策制定。职业篮球作为一个包含隐蔽性战略策略和决策制定的动态时空博弈的引人注目的例子。然而，处理多样的场上信号和导航潜在动作和结果的广阔空间使得现有方法很难迅速识别响应不断变化的情况下的最佳策略。在本研究中，我们首先将序列决策制定过程定义为条件轨迹生成过程。我们进一步引入PLAYBEST（PLAYER BEhavior SynThesis），这是一种提高球员决策制定的方法。我们扩展了最先进的生成模型——扩散概率模型，以从历史的美国职业篮球联赛(NBA)球员运动跟踪数据中学习具有挑战性的多智能体环境动态。为了融合数据驱动的策略，我们引入了一个辅助变量到PLAYBEST中，以适应外部输入并生成真实和多样化的球员轨迹。实验结果表明，PLAYBEST可以生成高质量的球员行为，并在各种评估场景中优于基线模型。

    Dynamically planning in multi-agent systems has been explored to improve decision-making in various domains. Professional basketball serves as a compelling example of a dynamic spatio-temporal game, encompassing both concealed strategic policies and decision-making. However, processing the diverse on-court signals and navigating the vast space of potential actions and outcomes makes it difficult for existing approaches to swiftly identify optimal strategies in response to evolving circumstances. In this study, we first formulate the sequential decision-making process as a conditional trajectory generation process. We further introduce PLAYBEST (PLAYer BEhavior SynThesis), a method for enhancing player decision-making. We extend the state-of-the-art generative model, diffusion probabilistic model, to learn challenging multi-agent environmental dynamics from historical National Basketball Association (NBA) player motion tracking data. To incorporate data-driven strategies, an auxiliary v
    
[^48]: 一种基于强化学习的方法促进算法代理与LLM之间的高效互动

    Enabling Efficient Interaction between an Algorithm Agent and an LLM: A Reinforcement Learning Approach. (arXiv:2306.03604v1 [cs.AI])

    [http://arxiv.org/abs/2306.03604](http://arxiv.org/abs/2306.03604)

    本论文提出一种强化学习的中介模型，可实现代理与LLM之间高效经济有效的互动，提高效率和成本效益。

    

    大型语言模型(LLMs)包含从海量文本数据集中获取的大量世界知识。最近的研究表明，LLMs可以通过提供高层指令来协助算法代理解决具有复杂顺序决策的任务。然而，与LLMs进行交互可能耗时较长，因为在许多实际情况下，它们需要大量存储空间，只能部署在远程云服务器节点上。此外，使用商业LLMs可能成本很高，因为它们可能根据使用频率收费。本文探讨如何实现代理与LLM之间的高效和经济有效的互动。我们提出了一种基于强化学习的中介模型，以确定何时需要查询LLMs以完成目标任务的高级指令。在涉及规划子目标的4个MiniGrid环境上进行的实验表明，我们的方法可以学习解决目标任务，并提升了效率和成本效益。

    Large language models (LLMs) encode a vast amount of world knowledge acquired from massive text datasets. Recent studies have demonstrated that LLMs can assist an algorithm agent in solving complex sequential decision making tasks in embodied environments by providing high-level instructions. However, interacting with LLMs can be time-consuming, as in many practical scenarios, they require a significant amount of storage space that can only be deployed on remote cloud server nodes. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency. In this paper, we explore how to enable efficient and cost-effective interactions between the agent and an LLM. We propose a reinforcement learning based mediator model that determines when it is necessary to consult LLMs for high-level instructions to accomplish a target task. Experiments on 4 MiniGrid environments that entail planning sub-goals demonstrate that our method can learn to solve target tasks with o
    
[^49]: SDR-GAIN：一种用于自动驾驶的高实时遮挡行人姿态完成方法

    SDR-GAIN: A High Real-Time Occluded Pedestrian Pose Completion Method for Autonomous Driving. (arXiv:2306.03538v1 [cs.CV])

    [http://arxiv.org/abs/2306.03538](http://arxiv.org/abs/2306.03538)

    SDR-GAIN是一种用于解决行人姿态中部分遮挡问题的关键点补全方法，它通过对不完整的关键点进行降维，统一特征分布，并使用GAN框架的两种生成模型来完成姿态的补全。该方法的实验表明性能优于基本的GAIN框架。

    

    为了缓解基于人体姿态关键点的行人检测算法中部分遮挡带来的挑战，我们提出了一种称为分离和降维基于生成对抗性补全网络(SDR-GAIN)的新型行人姿势关键点补全方法。首先，我们利用OpenPose在图像中估计行人的姿态。然后，我们对由于遮挡或其他因素而不完整的行人头部和躯干关键点进行维度缩减，以增强特征并进一步统一特征分布。最后，我们引入了基于生成对抗网络(GAN)框架的两种生成模型，这些模型融合了Huber损失、残差结构和L1正则化来生成部分遮挡行人不完整头部和躯干姿态关键点的缺失部分，从而实现了姿态补全。我们在MS COCO和JAAD数据集上的实验表明，SDR-GAIN的性能优于基本的GAIN框架。

    To mitigate the challenges arising from partial occlusion in human pose keypoint based pedestrian detection methods , we present a novel pedestrian pose keypoint completion method called the separation and dimensionality reduction-based generative adversarial imputation networks (SDR-GAIN) . Firstly, we utilize OpenPose to estimate pedestrian poses in images. Then, we isolate the head and torso keypoints of pedestrians with incomplete keypoints due to occlusion or other factors and perform dimensionality reduction to enhance features and further unify feature distribution. Finally, we introduce two generative models based on the generative adversarial networks (GAN) framework, which incorporate Huber loss, residual structure, and L1 regularization to generate missing parts of the incomplete head and torso pose keypoints of partially occluded pedestrians, resulting in pose completion. Our experiments on MS COCO and JAAD datasets demonstrate that SDR-GAIN outperforms basic GAIN framework
    
[^50]: 应用标准促进大型语言模型上下游伦理

    Applying Standards to Advance Upstream & Downstream Ethics in Large Language Models. (arXiv:2306.03503v1 [cs.CY])

    [http://arxiv.org/abs/2306.03503](http://arxiv.org/abs/2306.03503)

    本文探讨如何为AI生成的内容制定安全保障，分析LLMs的内容生成机制，确定了四个关键领域，提出了新的分发和销售LLM生成内容的企业的标准。

    

    本文探讨AI所有者如何借鉴其他内容创作行业的行为准则和伦理标准，为AI生成的内容制定安全保障。它深入研究了大型语言模型（LLMs）的伦理意识现状。通过分析LLMs的内容生成机制，确定了四个关键领域（上下游和用户提示/回答），在这些领域可以有效地应用保障措施。随后，对这四个领域进行了比较分析，包括在成本、有效性和与行业惯例的一致性方面评估现有的伦理保障措施。本文的主要观点是，现有的与IT相关的伦理准则虽然适用于传统的IT工程领域，但不足以应对基于LLMs内容生成所带来的挑战。我们借鉴新闻业内已有的实践，为分发和销售LLM生成内容的企业提出了潜在的标准。

    This paper explores how AI-owners can develop safeguards for AI-generated content by drawing from established codes of conduct and ethical standards in other content-creation industries. It delves into the current state of ethical awareness on Large Language Models (LLMs). By dissecting the mechanism of content generation by LLMs, four key areas (upstream/downstream and at user prompt/answer), where safeguards could be effectively applied, are identified. A comparative analysis of these four areas follows and includes an evaluation of the existing ethical safeguards in terms of cost, effectiveness, and alignment with established industry practices. The paper's key argument is that existing IT-related ethical codes, while adequate for traditional IT engineering, are inadequate for the challenges posed by LLM-based content generation. Drawing from established practices within journalism, we propose potential standards for businesses involved in distributing and selling LLM-generated cont
    
[^51]: 人工智能改变了学术不端行为的规则吗？深入探究学生对“AI-giarism”的看法

    Is AI Changing the Rules of Academic Misconduct? An In-depth Look at Students' Perceptions of 'AI-giarism'. (arXiv:2306.03358v1 [cs.CY])

    [http://arxiv.org/abs/2306.03358](http://arxiv.org/abs/2306.03358)

    这项开创性研究调查了学生对“AI-giarism”的认知，提出了初始概念化的AI-giarism工具，有助于应对不断发展的AI技术带来的学术不端行为，同时还挑战了传统的学术不端行为定义。

    

    这项开创性研究探讨了高等教育背景下，AI和抄袭合体所涉及的新兴学术不端行为“AI-giarism”的学生认知。共有来自不同学科的393名本科和研究生参与了调查，受访者针对多种AI-giarism情境表达了不同的看法。研究结果揭示了一个复杂的认知格局，明确反对直接生成AI内容，但对于更微妙的AI使用方式则持更为暧昧态度。该研究引入了一种新型工具——作为AI-giarism的初始概念化——为教育工作者和政策制定者提供了重要支持。该量具有助于理解和探讨与AI有关的学术不端行为，有助于在AI集成时进行教学设计和评估。此外，它还挑战了传统的学术不端行为定义，强调了根据不断发展的AI技术进行适应的必要性。尽管存在限制，例如具有样本偏差等问题，但是该研究仍然具有重要意义。

    This pioneering study explores students' perceptions of AI-giarism, an emergent form of academic dishonesty involving AI and plagiarism, within the higher education context. A survey, undertaken by 393 undergraduate and postgraduate students from a variety of disciplines, investigated their perceptions of diverse AI-giarism scenarios. The findings portray a complex landscape of understanding, with clear disapproval for direct AI content generation, yet more ambivalent attitudes towards subtler uses of AI. The study introduces a novel instrument, as an initial conceptualization of AI-giarism, offering a significant tool for educators and policy-makers. This scale facilitates understanding and discussions around AI-related academic misconduct, aiding in pedagogical design and assessment in an era of AI integration. Moreover, it challenges traditional definitions of academic misconduct, emphasizing the need to adapt in response to evolving AI technology. Despite limitations, such as the r
    
[^52]: LLM-Blender: 利用成对排名和生成融合集成大型语言模型

    LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. (arXiv:2306.02561v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02561](http://arxiv.org/abs/2306.02561)

    本论文提出了LLM-Blender，它是一个集成框架，旨在利用不同的开源大型语言模型的优秀特性，实现始终如一的卓越性能。PairRanker和GenFuser是该框架的两个模块，PairRanker使用成对比较方法来区分候选输出，并且GenFuser旨在合并排名最高的候选者，以生成改进的输出。

    

    本论文提出了LLM-Blender，一个集成框架，旨在通过利用多个开源大型语言模型（LLMs）的不同优势来达到始终如一的卓越性能。我们的框架由两个模块组成：PairRanker和GenFuser，以应对不同示例的最优LLMs可以显着变化的观察。PairRanker使用专门的成对比较方法来区分候选输出之间的微小差异。它联合编码输入文本和一对候选者，使用交叉注意编码器来确定优越者。我们的结果表明，PairRanker与ChatGPT的排名相关性最高。然后，GenFuser旨在合并排名最高的候选者，通过利用它们的优势和减少它们的弱点来生成改进的输出。为了促进大规模评估，我们介绍了一个基准数据集MixInstruct，它是多个指令数据集的混合，具有oracle p。

    We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle p
    
[^53]: 大批量神经多目标贝叶斯优化

    Large-Batch, Neural Multi-Objective Bayesian Optimization. (arXiv:2306.01095v1 [cs.LG])

    [http://arxiv.org/abs/2306.01095](http://arxiv.org/abs/2306.01095)

    本文提出了一种针对数据密集型问题和多目标优化设置的贝叶斯优化框架，该方法利用了贝叶斯神经网络代理建模和可扩展、具有不确定性的收购策略，能够在最少迭代次数的情况下高效地进行优化。

    

    贝叶斯优化在全局优化黑盒高成本函数方面提供了强大的框架。然而，由于默认高斯过程代理的可扩展性差，它在处理数据密集型问题，特别是在多目标设置中的能力有限。本文提出了一种新颖的贝叶斯优化框架，专为解决这些限制而设计。我们的方法利用了贝叶斯神经网络方法进行代理建模。这使得它能够有效地处理大批量数据，建模复杂问题以及产生预测的不确定性。此外，我们的方法结合了一种基于众所周知且易于部署的NSGA-II的可扩展的、具有不确定性的收购策略。这种完全可并行化的策略促进了未勘探区域的有效探索。我们的框架允许在最少迭代次数的情况下在数据密集环境中进行有效的优化。我们展示了我们方法的优越性。

    Bayesian optimization provides a powerful framework for global optimization of black-box, expensive-to-evaluate functions. However, it has a limited capacity in handling data-intensive problems, especially in multi-objective settings, due to the poor scalability of default Gaussian Process surrogates. We present a novel Bayesian optimization framework specifically tailored to address these limitations. Our method leverages a Bayesian neural networks approach for surrogate modeling. This enables efficient handling of large batches of data, modeling complex problems, and generating the uncertainty of the predictions. In addition, our method incorporates a scalable, uncertainty-aware acquisition strategy based on the well-known, easy-to-deploy NSGA-II. This fully parallelizable strategy promotes efficient exploration of uncharted regions. Our framework allows for effective optimization in data-intensive environments with a minimum number of iterations. We demonstrate the superiority of ou
    
[^54]: 影响智能手机用户答案质量的因素

    Factors Impacting the Quality of User Answers on Smartphones. (arXiv:2306.00627v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2306.00627](http://arxiv.org/abs/2306.00627)

    本论文研究影响用户答案质量的两个关键因素是反应时间和完成时间，并探究这些因素与外部和内部原因的相关性。

    

    目前，大多数研究探究人类行为的可预测性，如移动和社交互动，主要集中在传感器数据的利用上。然而，传感器数据很难捕捉到个人行为背后的主观动机。理解个人的上下文（例如，他们所处的地方和正在做什么）可以大大提高可预测性。主要限制是人类输入经常缺失或不准确。本文的目标是确定影响用户被问及其当前上下文时响应质量的因素。我们发现，两个关键因素影响着响应质量：用户的反应时间和完成时间。这些因素与各种外部原因（例如，情境背景，时间）和内部原因（例如，拖延态度，心情）相关。然后，我们研究这两个因素如何影响响应的质量。

    So far, most research investigating the predictability of human behavior, such as mobility and social interactions, has focused mainly on the exploitation of sensor data. However, sensor data can be difficult to capture the subjective motivations behind the individuals' behavior. Understanding personal context (e.g., where one is and what they are doing) can greatly increase predictability. The main limitation is that human input is often missing or inaccurate. The goal of this paper is to identify factors that influence the quality of responses when users are asked about their current context. We find that two key factors influence the quality of responses: user reaction time and completion time. These factors correlate with various exogenous causes (e.g., situational context, time of day) and endogenous causes (e.g., procrastination attitude, mood). In turn, we study how these two factors impact the quality of responses.
    
[^55]: 特定上下文独立关系下的因果可模仿性

    Causal Imitability Under Context-Specific Independence Relations. (arXiv:2306.00585v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.00585](http://arxiv.org/abs/2306.00585)

    本文探讨了特定上下文独立关系对因果模仿学习的影响，证明了这种情况下的模仿可行性决策问题是NP难的，并提供了必要的图形标准以及一个有声的算法方法。

    

    最近认识到，在执行模仿学习时忽略因果机制的缺点。文献中提出了几种方法来评估模仿的可行性以及规避因果混淆和因果偏差，但是对于关于潜在因果结构的其他信息的潜在好处却未被探索。这些忽略的信息之一是特定上下文独立性（CSI），即仅在某些上下文中保持独立性。当已知CSI关系时，我们考虑了因果模仿学习问题。我们证明了在这种情况下关于模仿可行性的决策问题是NP难的。此外，我们提供了在CSI下的模仿学习必要的图形标准，并表明在一种结构假设下，这一标准也是充分的。最后，我们提出了一个有声的算法方法用于基于因果的模仿学习，该算法考虑了CSI关系。

    Drawbacks of ignoring the causal mechanisms when performing imitation learning have recently been acknowledged. Several approaches both to assess the feasibility of imitation and to circumvent causal confounding and causal misspecifications have been proposed in the literature. However, the potential benefits of the incorporation of additional information about the underlying causal structure are left unexplored. An example of such overlooked information is context-specific independence (CSI), i.e., independence that holds only in certain contexts. We consider the problem of causal imitation learning when CSI relations are known. We prove that the decision problem pertaining to the feasibility of imitation in this setting is NP-hard. Further, we provide a necessary graphical criterion for imitation learning under CSI and show that under a structural assumption, this criterion is also sufficient. Finally, we propose a sound algorithmic approach for causal imitation learning which takes 
    
[^56]: 使预训练模型具有可逆性：从参数到内存高效的微调

    Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])

    [http://arxiv.org/abs/2306.00477](http://arxiv.org/abs/2306.00477)

    本研究尝试实现在预训练语言模型中运用可逆模型实现高效的微调，并发现在初始化微调时保留PLM的起点非常重要。

    

    预训练语言模型（PLM）的参数高效微调已经成为一种非常成功的方法，只需训练少量参数而不会降低性能，并随着PLM越来越大而成为事实上的学习范式。然而，现有的PEFT方法不具备内存效率，因为它们仍需要存储大部分中间激活值以便计算梯度，类似于微调。一个减少激活内存的有效方法是应用可逆模型，这样中间激活值就无需缓存，可以重新计算。然而，将PLM修改为它的可逆变体并进行PEFT并不是一件容易的事，因为可逆模型具有与当前发布的PLM不同的体系结构。本文首先调查现有PEFT方法成功的关键因素，认识到在初始化PEFT时保留PLM的起点是至关重要的。

    Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
    
[^57]: 可信人工智能的调查、分类及未来方向：元决策的战略决策元分析

    The Survey, Taxonomy, and Future Directions of Trustworthy AI: A Meta Decision of Strategic Decisions. (arXiv:2306.00380v1 [cs.AI])

    [http://arxiv.org/abs/2306.00380](http://arxiv.org/abs/2306.00380)

    本文提出了一个新方法来解决在使用AI系统进行决策时的可信问题。该方法引入了一个包括表达、真实和基本水平的不同信任级别的TAI分类系统或框架，使用十个维度来衡量信任，并提供了现有TAI研究的调查和元分析，还确定了TAI研究的未来方向和潜在应用。

    

    在制定战略决策时，我们常常面临着大量需要处理的信息。当一些证据相互矛盾或自相矛盾时，情况可能会更加复杂。此时，问题在于如何确定哪些信息是有用的，哪些应该被排除。这个过程被称为元决策。同样，在使用人工智能（AI）系统进行战略决策时，对AI本身的信任就成为了一个元决策，因为许多AI系统被视为处理大量数据的不透明“黑匣子”。信任一个不透明的系统涉及决定什么样的可信人工智能（TAI）水平。我们提出了一种新方法来解决这个问题，即引入一个新颖的TAI分类系统或框架，该框架包括三个关键领域：表达、真实和基本水平的不同信任级别。为了支撑这些领域，我们创建了十个维度来衡量信任：可解释性/透明性、无偏性、问责制、准确性、可靠性、隐私、安全、公平性和人类控制。在本文中，我们提供了一个现有TAI研究的调查和元分析，以全面了解这一新兴领域的研究现状。我们还确定了TAI研究的未来方向，并提出了一些潜在应用，这些应用可能从TAI的使用中受益。

    When making strategic decisions, we are often confronted with overwhelming information to process. The situation can be further complicated when some pieces of evidence are contradicted each other or paradoxical. The challenge then becomes how to determine which information is useful and which ones should be eliminated. This process is known as meta-decision. Likewise, when it comes to using Artificial Intelligence (AI) systems for strategic decision-making, placing trust in the AI itself becomes a meta-decision, given that many AI systems are viewed as opaque "black boxes" that process large amounts of data. Trusting an opaque system involves deciding on the level of Trustworthy AI (TAI). We propose a new approach to address this issue by introducing a novel taxonomy or framework of TAI, which encompasses three crucial domains: articulate, authentic, and basic for different levels of trust. To underpin these domains, we create ten dimensions to measure trust: explainability/transparen
    
[^58]: 源代码模型的数据增强方法：一份综述

    Data Augmentation Approaches for Source Code Models: A Survey. (arXiv:2305.19915v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.19915](http://arxiv.org/abs/2305.19915)

    本文对源代码的数据增强技术进行了全面的调查和综述，介绍了它们的分类法、优化策略和性能结果，并讨论了未来方向和研究挑战。

    

    源代码在许多关键任务中的广泛应用促进了数据增强（DA）技术的发展，以增强训练数据并提高这些模型的各种能力（例如健壮性和可泛化性）。虽然已经提出并针对源代码模型进行了一系列DA方法的调整，但缺乏综合性的调查和审查以理解它们的有效性和含义。本文通过对源代码的数据增强进行全面而综合的调查，填补这一空白，我们系统地整理和概述现有文献，以提供该领域的全面概述。我们首先构建了适用于源代码模型的数据增强的分类法，然后讨论了著名的、方法上具有说明性的方法。接下来，我们强调了优化DA质量的一般策略和技术。随后，我们强调了在被广泛接受的基准测试中发挥作用的技术，并呈现了它们的性能结果。最后，我们讨论了DA用于源代码模型的潜在未来方向和开放研究挑战。

    The increasingly popular adoption of source code in many critical tasks motivates the development of data augmentation (DA) techniques to enhance training data and improve various capabilities (e.g., robustness and generalizability) of these models. Although a series of DA methods have been proposed and tailored for source code models, there lacks a comprehensive survey and examination to understand their effectiveness and implications. This paper fills this gap by conducting a comprehensive and integrative survey of data augmentation for source code, wherein we systematically compile and encapsulate existing literature to provide a comprehensive overview of the field. We start by constructing a taxonomy of DA for source code models model approaches, followed by a discussion on prominent, methodologically illustrative approaches. Next, we highlight the general strategies and techniques to optimize the DA quality. Subsequently, we underscore techniques that find utility in widely-accept
    
[^59]: 使用子图特定因子嵌入归一化改善GNN的表达能力

    Improving Expressivity of GNNs with Subgraph-specific Factor Embedded Normalization. (arXiv:2305.19903v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19903](http://arxiv.org/abs/2305.19903)

    本文提出了一种名为SuperNorm的专用归一化方案，通过嵌入子图特定因子和纳入图实例特定统计数据来加强GNN的代表性能力，实现对节点感应子图中内部连接信息的明确考虑，从而改善GNN的表达能力。

    

    图神经网络（GNN）已经成为一类处理图结构数据的强大学习架构。然而，现有的GNN通常忽略了节点感应子图中的重要结构特征，从而限制了它们在各种下游任务中的表达能力。本文旨在通过设计一种专用的即插即用归一化方案——SUbgraph-sPEcific FactoR Embedded Normalization（SuperNorm）来加强GNN的代表性能力，该方案明确考虑了每个节点感应子图内部连接的信息。为此，我们在标准的BatchNorm开始和结束时嵌入了子图特定因子，并纳入图实例特定统计数据以提高区分能力。同时，我们提供了理论分析支持，指出通过改善的SuperNorm，任意GNN至少与1-WL测试一样能够区分非同构图。

    Graph Neural Networks~(GNNs) have emerged as a powerful category of learning architecture for handling graph-structured data. However, existing GNNs typically ignore crucial structural characteristics in node-induced subgraphs, which thus limits their expressiveness for various downstream tasks. In this paper, we strive to strengthen the representative capabilities of GNNs by devising a dedicated plug-and-play normalization scheme, termed as SUbgraph-sPEcific FactoR Embedded Normalization (SuperNorm), that explicitly considers the intra-connection information within each node-induced subgraph. To this end, we embed the subgraph-specific factor at the beginning and the end of the standard BatchNorm, as well as incorporate graph instance-specific statistics for improved distinguishable capabilities. In the meantime, we provide theoretical analysis to support that, with the elaborated SuperNorm, an arbitrary GNN is at least as powerful as the 1-WL test in distinguishing non-isomorphism gr
    
[^60]: 基于扩散式语言模型的细粒度文本风格转换

    Fine-grained Text Style Transfer with Diffusion-Based Language Models. (arXiv:2305.19512v1 [cs.CL])

    [http://arxiv.org/abs/2305.19512](http://arxiv.org/abs/2305.19512)

    本文提出了一种基于扩散式语言模型的细粒度文本风格转换方法，在不依赖外部信息的情况下取得了比之前利用预训练权重、嵌入和外部语法分析器更好的效果，表明扩散概率模型在文本生成领域具有广泛的应用前景。

    

    扩散式概率模型已经在可控制地生成高质量图像上显示出了巨大的成功，研究人员已经试图将这种可控性运用到文本生成领域。以前的扩散式语言模型研究表明，它们可以在不需要外部知识（如预训练权重）的情况下进行训练，并且仍然可以实现稳定的性能和可控性。 在本文中，我们在StylePTB数据集上训练了一个扩散式模型，这是细粒度文本风格转换的标准基准。与以前的工作评估任务相比，StylePTB中的任务需要对输出文本进行更加精细的控制，我们的模型能够在StylePTB上实现卓越的性能，包括个别和组合转换。此外，我们的模型在没有外部知识的情况下使用StylePTB的有限数据进行训练，其表现优于以前利用预训练权重、嵌入和外部语法分析器的工作，这可能表明扩散概率模型在文本生成领域具有巨大的潜力。

    Diffusion probabilistic models have shown great success in generating high-quality images controllably, and researchers have tried to utilize this controllability into text generation domain. Previous works on diffusion-based language models have shown that they can be trained without external knowledge (such as pre-trained weights) and still achieve stable performance and controllability. In this paper, we trained a diffusion-based model on StylePTB dataset, the standard benchmark for fine-grained text style transfers. The tasks in StylePTB requires much more refined control over the output text compared to tasks evaluated in previous works, and our model was able to achieve state-of-the-art performance on StylePTB on both individual and compositional transfers. Moreover, our model, trained on limited data from StylePTB without external knowledge, outperforms previous works that utilized pretrained weights, embeddings, and external grammar parsers, and this may indicate that diffusion
    
[^61]: 缓解上下文学习的标签偏差

    Mitigating Label Biases for In-context Learning. (arXiv:2305.19148v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.19148](http://arxiv.org/abs/2305.19148)

    本文针对上下文学习（ICL）中的三种标签偏差提出分类法，并提出一种简单的偏差校准方法，使用随机的领域词估算语言模型的标签偏差。

    

    上下文学习（ICL）的各种设计设置，如选择和顺序的上下文示例，可能使模型对某种特定预测偏见，而这种预测并不反映对任务的理解。虽然许多研究讨论了这些设计选择，但对它们进行分类和减缓其影响的系统调查很少。在本文中，我们为文本分类中上下文学习（ICL）中的三种标签偏差定义了一个分类法：香草标签偏差、上下文标签偏差和领域标签偏差（我们首次概念化和检测到）。我们的分析表明，先前的标签偏差校准方法不能解决所有三种偏差。特别是，领域标签偏差使LLM在许多任务上只能实现随机级别的性能，而不管上下文示例的选择如何。为了缓解这些偏差的影响，我们提出一个简单的偏差校准方法，使用随机的领域词估算语言模型的标签偏差。

    Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias a model toward a particular prediction without being reflective of an understanding of the task. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time).  Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model's label bias using random in-domain words f
    
[^62]: 一网络，多任务：更高效的参数共用迁移学习方法

    One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning. (arXiv:2305.17682v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17682](http://arxiv.org/abs/2305.17682)

    本研究提出了PROPETL方法，通过原型网络和二进制掩码实现了更高效的参数共用迁移学习，解决了多任务微调预训练语言模型存储空间占用的问题。

    

    对于多个任务来说，微调预训练的语言模型往往会占用大量存储空间，参数共用迁移学习（PETL）方法可以缓解这个问题，但在应用于更广泛的任务范围时仍需要大量的参数和存储空间。为了实现更大的存储空间减少，我们提出了PROPETL，这是一种新颖的方法，可以在不同层和任务之间使用单个PETL模块，我们称之为原型网络（例如适配器、LoRA和前缀调整）。我们然后学习二进制掩码以从共享的原型网络中选择不同的子网络，并将它们作为PETL模块应用于不同的层。我们发现，二进制掩码可以确定网络中关键的信息，这在以前的研究中经常被忽略。我们的工作也可以看作是一种修剪方法，我们发现即使在看似很小的PETL模块中也存在过度参数化。我们对PROPETL进行了评估。

    Fine-tuning pre-trained language models for multiple tasks tends to be expensive in terms of storage. To mitigate this, parameter-efficient transfer learning (PETL) methods have been proposed to address this issue, but they still require a significant number of parameters and storage when being applied to broader ranges of tasks. To achieve even greater storage reduction, we propose PROPETL, a novel method that enables efficient sharing of a single PETL module which we call prototype network (e.g., adapter, LoRA, and prefix-tuning) across layers and tasks. We then learn binary masks to select different sub-networks from the shared prototype network and apply them as PETL modules into different layers. We find that the binary masks can determine crucial information from the network, which is often ignored in previous studies. Our work can also be seen as a type of pruning method, where we find that overparameterization also exists in the seemingly small PETL modules. We evaluate PROPETL
    
[^63]: 使用场景图记忆建模动态环境

    Modeling Dynamic Environments with Scene Graph Memory. (arXiv:2305.17537v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.17537](http://arxiv.org/abs/2305.17537)

    本论文提出了一种新的场景图记忆状态表示，结合节点边缘预测器（NEP）的神经网络架构，能够帮助具有行动能力的AI代理在部分可观察动态场景中高效搜索。

    

    在大型环境中，如居室等，寻找物品的具有行动能力的AI代理需要基于部分信息预测物品位置来做出有效决策。我们将其形式化为一种新类型的链路预测问题：部分可观察动态图上的链路预测。我们的图表达了一个场景，其中房间和物品是节点，在边缘中编码它们之间的关系；在每个时间步骤上，代理人仅知道更改图的部分。这种部分可观测性对于现有的链路预测方法构成了挑战，我们进行了解决。我们提出了一种新颖的状态表示 - 场景图记忆（SGM） - 其中包括代理人的累积观察集合，以及一种名为节点边缘预测器（NEP）的神经网络架构，该架构从SGM中提取信息以进行高效搜索。我们在动态房屋模拟器中评估了我们的方法，这是一个新的基准，它按照语义模式创建不同的动态图形。

    Embodied AI agents that search for objects in large environments such as households often need to make efficient decisions by predicting object locations based on partial information. We pose this as a new type of link prediction problem: link prediction on partially observable dynamic graphs. Our graph is a representation of a scene in which rooms and objects are nodes, and their relationships are encoded in the edges; only parts of the changing graph are known to the agent at each timestep. This partial observability poses a challenge to existing link prediction approaches, which we address. We propose a novel state representation -- Scene Graph Memory (SGM) -- with captures the agent's accumulated set of observations, as well as a neural net architecture called a Node Edge Predictor (NEP) that extracts information from the SGM to search efficiently. We evaluate our method in the Dynamic House Simulator, a new benchmark that creates diverse dynamic graphs following the semantic patte
    
[^64]: 神经代码搜索中的后门攻击

    Backdooring Neural Code Search. (arXiv:2305.17506v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2305.17506](http://arxiv.org/abs/2305.17506)

    本文研究了神经代码搜索模型的安全性问题，指出攻击者可以注入后门来返回具有安全/隐私问题的代码，提出了几种防御机制来缓解这种威胁，该工作突显了研究AI系统安全方面的重要性，特别是在部署于安全关键领域时。

    

    从在线代码库中重复使用现成代码是常见的做法，它极大地提高了软件开发人员的生产力。要查找所需的代码片段，开发人员则要通过自然语言查询使用代码搜索引擎。因此，在许多这样的搜索引擎后面，都是神经代码搜索模型。这些模型基于深度学习，因其出色的性能而备受关注。然而，这些模型的安全性很少被研究。具体来说，攻击者可能会在神经代码搜索模型中注入后门，从而返回具有安全/隐私问题的错误或甚至易受攻击的代码。这可能会影响下游软件（例如股票交易系统和自动驾驶），并造成财务损失和/或危及生命的事件。在本文中，我们展示了这样的攻击是可行的，并且可能非常隐蔽。通过简单地修改一个变量/函数名称，攻击者可以使出现错误/易受攻击的代码排名前11％。我们的攻击利用了神经网络的可再训练性和代码相似度度量的宽松性，以注入后门。我们还提出了几种防御机制来缓解这种威胁，例如添加对抗性训练数据和特征压缩。我们相信我们的工作突显了研究AI系统安全方面的重要性，特别是在部署于安全关键领域时。

    Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence behind many such engines. These models are based on deep learning and gain substantial attention due to their impressive performance. However, the security aspect of these models is rarely studied. Particularly, an adversary can inject a backdoor in neural code search models, which return buggy or even vulnerable code with security/privacy issues. This may impact the downstream software (e.g., stock trading systems and autonomous driving) and cause financial loss and/or life-threatening incidents. In this paper, we demonstrate such attacks are feasible and can be quite stealthy. By simply modifying one variable/function name, the attacker can make buggy/vulnerable code rank in the top 11%. Our at
    
[^65]: 利用（模糊测试）测试用例来理解程序

    Understanding Programs by Exploiting (Fuzzing) Test Cases. (arXiv:2305.13592v1 [cs.LG])

    [http://arxiv.org/abs/2305.13592](http://arxiv.org/abs/2305.13592)

    本文提出了通过模糊测试获取代表性输入来帮助语义理解程序的方法。

    

    程序的语义理解引起了社区的极大关注。受到自然语言理解中大型语言模型（LLM）的最近成功启发，通过将编程语言视为另一种自然语言，并在程序代码语料库上训练LLM，取得了巨大进展。然而，程序毕竟与文本有本质的区别，因为它们通常具有严格的结构和语法。特别是，程序及其基本单元（即函数和子程序）旨在展示各种行为和/或提供可能的输出，给定不同的输入。输入和可能的输出/行为之间的关系表示函数/子程序，并概述了整个程序。因此，我们提出将这种关系纳入学习中，以实现对程序的更深入语义理解。为了获得足够代表性的输入以触发大量执行，可以使用模糊测试。

    Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models (LLMs) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training LLMs on corpora of program code. However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict. In particular, programs and their basic units (i.e., functions and subroutines) are designed to demonstrate a variety of behaviors and/or provide possible outputs, given different inputs. The relationship between inputs and possible outputs/behaviors represents the functions/subroutines and profiles the program as a whole. Therefore, we propose to incorporate such a relationship into learning, for achieving a deeper semantic understanding of programs. To obtain inputs that are representative enough to trigger the execution of mo
    
[^66]: 基于提示的问题回答应用于开放研究知识图谱中的对象预测评估

    Evaluating Prompt-based Question Answering for Object Prediction in the Open Research Knowledge Graph. (arXiv:2305.12900v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12900](http://arxiv.org/abs/2305.12900)

    本研究采用基于提示的训练方法，在学术知识图谱对象预测领域进行了大规模transformers模型的评估和测试，发现提示的使用可以改进pre-trained transformers的泛化能力。

    

    最近对于基于提示的训练方法在低资源环境下对转换器语言模型进行新文本体裁训练的调查有很多。发现基于提示的训练方法对于通用预训练或微调模型以适应资源缺乏的环境有很好的效果。本研究首次报道了采用基于提示训练transformers进行“学术知识图谱对象预测”的结果。该研究具有以下两个主要特点。1）它偏离了其他提出用于预测学术知识图谱对象的实体和关系提取流程的研究。2）在其他研究中测试了该方法对于与通用知识领域相对接近的文本体裁，而我们测试了该方法适用于显著不同的学术知识领域，从而测试这些大规模transformers模型的语言，概率和事实的普适性。我们发现（i）符合预期，使用提示进行微调的transformers优于基线；（ii）与先前研究中看到的模式不同，预先训练的transformers并不能始终足以胜任学术对象预测的任务，结果表明提示确实有助于改进抽取预训练模型所获得的语义信息的泛化能力。

    There have been many recent investigations into prompt-based training of transformer language models for new text genres in low-resource settings. The prompt-based training approach has been found to be effective in generalizing pre-trained or fine-tuned models for transfer to resource-scarce settings. This work, for the first time, reports results on adopting prompt-based training of transformers for \textit{scholarly knowledge graph object prediction}. The work is unique in the following two main aspects. 1) It deviates from the other works proposing entity and relation extraction pipelines for predicting objects of a scholarly knowledge graph. 2) While other works have tested the method on text genera relatively close to the general knowledge domain, we test the method for a significantly different domain, i.e. scholarly knowledge, in turn testing the linguistic, probabilistic, and factual generalizability of these large-scale transformer models. We find that (i) per expectations, t
    
[^67]: 融合项目相关性的序列推荐系统训练损失函数

    Integrating Item Relevance in Training Loss for Sequential Recommender Systems. (arXiv:2305.10824v1 [cs.IR])

    [http://arxiv.org/abs/2305.10824](http://arxiv.org/abs/2305.10824)

    本文提出了一种融合项目相关性的新型训练损失函数，用于提高序列推荐系统对噪声的鲁棒性和性能。

    

    序列推荐系统是一种受欢迎的推荐系统，它通过学习用户的历史数据来预测用户下一个可能与之交互的项目。然而，用户的交互可能会受到来自帐户共享、不一致的偏好或意外点击等噪声的影响。为了解决这个问题，我们（i）提出了一个考虑多个未来项目的新的评估协议，（ii）引入了一种新的关注相关性的损失函数，用于训练具有多个未来项目的序列推荐系统，以使其对噪声更加鲁棒。我们的关注相关性模型在传统评估协议中提高了NDCG@10约1.2%和HR约0.88%，而在新评估协议中，改进的NDCG@10约1.63%和HR约1.5%。

    Sequential Recommender Systems (SRSs) are a popular type of recommender system that learns from a user's history to predict the next item they are likely to interact with. However, user interactions can be affected by noise stemming from account sharing, inconsistent preferences, or accidental clicks. To address this issue, we (i) propose a new evaluation protocol that takes multiple future items into account and (ii) introduce a novel relevance-aware loss function to train a SRS with multiple future items to make it more robust to noise. Our relevance-aware models obtain an improvement of ~1.2% of NDCG@10 and 0.88% in the traditional evaluation protocol, while in the new evaluation protocol, the improvement is ~1.63% of NDCG@10 and ~1.5% of HR w.r.t the best performing models.
    
[^68]: 人们交谈，AI倾听：电子病历中污名化语言对AI判断的影响

    People Talking and AI Listening: How Stigmatizing Language in EHR Notes Affect AI Performance. (arXiv:2305.10201v1 [cs.AI])

    [http://arxiv.org/abs/2305.10201](http://arxiv.org/abs/2305.10201)

    本文研究了电子病历中污名化语言对基于Transformer的深度学习模型和可解释AI(XAI)技术进行死亡预测的影响。发现临床医生所写的SL会对AI性能表现不利，尤其是在黑人患者中表现更为明显，强调了理解偏见对下游AI性能的影响的重要性，以开发更具公平和正义的医疗系统。

    

    电子病历(EHRs)是期望中的人工智能(AI)-驱动的医疗转型的重要数据来源。然而，反映在EHR笔记中的临床医师偏见可能会导致AI模型继承并放大这些偏见，从而不断加剧健康上的不平等。本研究调查了EHR笔记中污名化语言(SL)对基于Transformer的深度学习模型和可解释AI(XAI)技术进行死亡预测的影响。我们的研究发现，临床医生所写的SL不利于AI的性能表现，尤其是在黑人患者中表现更为明显，突出了SL作为AI模型发展中种族差异的一种来源。为探索一种操作上有效的缓解SL影响的方法，我们研究了临床医生协作网络中SL生成的模式，发现中央医生对AI模型中的种族差异具有更强的影响力。我们发现，删除中央临床医生撰写的SL是相对于随机选择临床医生而言，缓解SL对AI性能影响的更为有效的策略。我们的研究强调了理解反映在EHR笔记中的临床医师偏见对下游AI性能的影响的重要性，以开发更具公平和正义的医疗系统。

    Electronic health records (EHRs) serve as an essential data source for the envisioned artificial intelligence (AI)-driven transformation in healthcare. However, clinician biases reflected in EHR notes can lead to AI models inheriting and amplifying these biases, perpetuating health disparities. This study investigates the impact of stigmatizing language (SL) in EHR notes on mortality prediction using a Transformer-based deep learning model and explainable AI (XAI) techniques. Our findings demonstrate that SL written by clinicians adversely affects AI performance, particularly so for black patients, highlighting SL as a source of racial disparity in AI model development. To explore an operationally efficient way to mitigate SL's impact, we investigate patterns in the generation of SL through a clinicians' collaborative network, identifying central clinicians as having a stronger impact on racial disparity in the AI model. We find that removing SL written by central clinicians is a more 
    
[^69]: 一种可扩展的Walsh-Hadamard正则化器，以克服神经网络的低阶谱偏差

    A Scalable Walsh-Hadamard Regularizer to Overcome the Low-degree Spectral Bias of Neural Networks. (arXiv:2305.09779v1 [cs.LG])

    [http://arxiv.org/abs/2305.09779](http://arxiv.org/abs/2305.09779)

    本文提出了一种新型的可扩展Walsh-Hadamard正则化器，可避免神经网络学习低阶频率以及误识别低阶频率，从而提高了神经网络的泛化性能。

    

    尽管神经网络具有学习任意函数的能力，但通过梯度下降训练的模型常常表现出对“更简单”函数的偏好。本文通过傅里叶（Walsh-Hadamard）变换，从离散（零一）输入的神经网络的角度探讨了简单性的概念，其中可以通过傅里叶系数的“阶”来捕捉简单性概念。我们实证表明神经网络有学习较低阶频率的趋势。我们展示了这种谱偏差向较简单特征的趋势实际上会损害神经网络在真实世界数据集上的泛化能力。为了解决这个问题，我们提出了一种新的可扩展的功能正则化方案，以帮助神经网络学习更高的阶频率。我们的正则化器还有助于避免对低阶频率的错误识别，从而进一步提高了泛化能力。我们在计算机视觉、自然语言处理和语音识别中应用各种神经网络架构进行分类任务的广泛评估。我们的实验结果表明，我们的正则化器在低数据量环境下显著提高了泛化性能。

    Despite the capacity of neural nets to learn arbitrary functions, models trained through gradient descent often exhibit a bias towards ``simpler'' functions. Various notions of simplicity have been introduced to characterize this behavior. Here, we focus on the case of neural networks with discrete (zero-one) inputs through the lens of their Fourier (Walsh-Hadamard) transforms, where the notion of simplicity can be captured through the \emph{degree} of the Fourier coefficients. We empirically show that neural networks have a tendency to learn lower-degree frequencies. We show how this spectral bias towards simpler features can in fact \emph{hurt} the neural network's generalization on real-world datasets. To remedy this we propose a new scalable functional regularization scheme that aids the neural network to learn higher degree frequencies. Our regularizer also helps avoid erroneous identification of low-degree frequencies, which further improves generalization. We extensively evaluat
    
[^70]: 寻求可验证性: 解释很少能够在AI辅助决策中提高决策性能

    In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making. (arXiv:2305.07722v1 [cs.AI])

    [http://arxiv.org/abs/2305.07722](http://arxiv.org/abs/2305.07722)

    AI解释只有在允许人类决策者验证AI预测的正确性时才有用，而大多数决策环境无法进行验证。

    

    目前关于AI辅助决策的文献，涉及可解释的AI系统为人类决策者提供建议，并呈现出一系列不确定和令人困惑的结果。为了综合这些发现，我们提出了一个简单的理论，阐明了AI解释经常无法促使适当的依赖和互补决策表现的失败。我们认为解释只有在允许人类决策者验证AI预测的正确性时才有用，而不是其他期望，例如可解释性或清晰阐述AI的推理过程。先前的研究发现，在许多决策环境中，AI解释并未促进这种验证。此外，无论解释方法如何，大多数环境基本上都无法进行验证。我们最后讨论了更有效的可解释AI辅助决策和人工智能协作的潜在方法。

    The current literature on AI-advised decision making -- involving explainable AI systems advising human decision makers -- presents a series of inconclusive and confounding results. To synthesize these findings, we propose a simple theory that elucidates the frequent failure of AI explanations to engender appropriate reliance and complementary decision making performance. We argue explanations are only useful to the extent that they allow a human decision maker to verify the correctness of an AI's prediction, in contrast to other desiderata, e.g., interpretability or spelling out the AI's reasoning process. Prior studies find in many decision making contexts AI explanations do not facilitate such verification. Moreover, most contexts fundamentally do not allow verification, regardless of explanation method. We conclude with a discussion of potential approaches for more effective explainable AI-advised decision making and human-AI collaboration.
    
[^71]: 模型扩散中的无标注文本实际上是卡通风格生成器

    Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator. (arXiv:2305.06710v1 [cs.CV])

    [http://arxiv.org/abs/2305.06710](http://arxiv.org/abs/2305.06710)

    本文发现，模型扩散中的无标注文本实际上是一个能够生成卡通风格图片的工具。通过简单地扰动无标注文本指导，这一功能得以实现。回滚扰动能够将生成的图像有效转换成卡通图像，而图像扰动则能够产生高保真度、多样性的卡通图像。

    

    无分类器指导是扩散模型中被广泛采用的一种有效的采样技术。其主要思想是在文本指导方向上对模型进行外推，并远离无标注文本指导。在本文中，我们展示了模型扩散中的无标注文本实际上是一个卡通风格生成器，即通过简单地扰动无标注文本指导就可以有效地将生成的图像转换成卡通图像。具体地，我们提出了两种扰动方法：回滚扰动（Back-D）和图像扰动（Image-D），用于构造在采样过程中用于预测无标注文本指导和文本指导的嘈杂图像之间的错位。Back-D通过通过将$x_t$替换为$x_{t+\Delta t}$来改变无标注嘈杂图像的噪声水平从而实现卡通化。Image-D则通过产生高保真度、多样性的卡通图像。

    Classifier-free guidance is an effective sampling technique in diffusion models that has been widely adopted. The main idea is to extrapolate the model in the direction of text guidance and away from null-text guidance. In this paper, we demonstrate that null-text guidance in diffusion models is secretly a cartoon-style creator, i.e., the generated images can be efficiently transformed into cartoons by simply perturbing the null-text guidance. Specifically, we proposed two disturbance methods, i.e., Rollback disturbance (Back-D) and Image disturbance (Image-D), to construct misalignment between the noisy images used for predicting null-text guidance and text guidance (subsequently referred to as \textbf{null-text noisy image} and \textbf{text noisy image} respectively) in the sampling process. Back-D achieves cartoonization by altering the noise level of null-text noisy image via replacing $x_t$ with $x_{t+\Delta t}$. Image-D, alternatively, produces high-fidelity, diverse cartoons by 
    
[^72]: 因果信息分离：为抗分布转移设计代理特征

    Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts. (arXiv:2305.05832v1 [cs.LG])

    [http://arxiv.org/abs/2305.05832](http://arxiv.org/abs/2305.05832)

    本文提出了利用因果机制在不同环境下保持不变的直觉来主动准备的代理特征选择和工程技术，用以应对统计预测模型在分布转移情况下的稳定性问题。

    

    统计预测模型通常是在与最终使用情况不同的概率分布中进行训练的。为了预测分布转移，有一种方法是利用因果机制在不同环境下保持不变的直觉来主动准备。本文针对一个具有挑战性的场景，其中目标的因果和反因果变量都是未被观察到的。利用信息论，我们为下游观测变量开发了特征选择和工程技术，这些变量充当代理。我们选择有助于建立稳定模型的代理，并使用辅助训练任务从代理中提取增强稳定性的信息。我们在合成数据和真实数据上展示了我们技术的有效性。

    Statistical prediction models are often trained on data that is drawn from different probability distributions than their eventual use cases. One approach to proactively prepare for these shifts harnesses the intuition that causal mechanisms should remain invariant between environments. Here we focus on a challenging setting in which the causal and anticausal variables of the target are unobserved. Leaning on information theory, we develop feature selection and engineering techniques for the observed downstream variables that act as proxies. We identify proxies that help to build stable models and moreover utilize auxiliary training tasks to extract stability-enhancing information from proxies. We demonstrate the effectiveness of our techniques on synthetic and real data.
    
[^73]: 探索应用强大的大型人工智能模型在课堂教学中：机遇、挑战和前景

    Towards Applying Powerful Large AI Models in Classroom Teaching: Opportunities, Challenges and Prospects. (arXiv:2305.03433v1 [cs.AI])

    [http://arxiv.org/abs/2305.03433](http://arxiv.org/abs/2305.03433)

    本文探索了利用大型语言模型实现教师与学生的互动，提高教学质量的潜力和挑战。提出了一个统一的框架来处理多样化的教育数据集，处理长时间的对话以及压缩信息以更好地完成更多的下游任务。

    

    本文提出了一系列互动场景，利用人工智能（AI）增强课堂教学，如对话自动完成、知识和风格转移以及评估AI生成内容。通过利用近期大型语言模型（LLM）的发展，本文探索了AI增强和丰富师生对话、提高教学质量的潜力。我们的目标是在教师与学生之间产生创新和有意义的对话，制定评估标准，并提高AI教育计划的效力。在第三节中，我们讨论了利用现有LLM有效完成教育任务的挑战，并提出了一个统一的框架来处理多样化的教育数据集，处理长时间的对话以及压缩信息以更好地完成更多的下游任务。在第四节中，我们总结了关键任务，包括教师-学生对话自动完成、示范答案和AI支持的写作。

    This perspective paper proposes a series of interactive scenarios that utilize Artificial Intelligence (AI) to enhance classroom teaching, such as dialogue auto-completion, knowledge and style transfer, and assessment of AI-generated content. By leveraging recent developments in Large Language Models (LLMs), we explore the potential of AI to augment and enrich teacher-student dialogues and improve the quality of teaching. Our goal is to produce innovative and meaningful conversations between teachers and students, create standards for evaluation, and improve the efficacy of AI-for-Education initiatives. In Section 3, we discuss the challenges of utilizing existing LLMs to effectively complete the educated tasks and present a unified framework for addressing diverse education dataset, processing lengthy conversations, and condensing information to better accomplish more downstream tasks. In Section 4, we summarize the pivoting tasks including Teacher-Student Dialogue Auto-Completion, Ex
    
[^74]: 语言、时间偏好和消费行为：大型语言模型的证据

    Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v1 [econ.GN])

    [http://arxiv.org/abs/2305.02531](http://arxiv.org/abs/2305.02531)

    本研究分析了大型语言模型在不同语言提示下的奖励时间偏好，并发现GPT在具有较弱未来时态的语言下表现出更大的耐心，这与使用该语言的人类的偏好相似。

    

    语言对我们对时间和奖励的感知有很大的影响。这引发了一个问题，即当以不同的语言询问大型语言模型时，它们是否显示出不同的奖励时间偏好，并且它们的选择是否类似于人类的选择。本研究分析了GPT-3.5（以下简称GPT）在多种语言提示下的响应，探索了较小、较早的奖励和较大、较晚的奖励之间的偏好。我们的结果显示，当以语义含义较弱的未来时态参考（FTR），如德语和汉语，为提示语时，GPT表现出更大的耐心，相比英语和法语等具有强大FTR的语言。这些发现与现有文献一致，并表明了GPT的选择与这些语言的使用者的偏好之间的关联。然而，进一步的分析揭示了较早或较晚奖励的偏好并没有随着奖励差异系统地改变，这表明了一种词典序优先的选择。

    Language has a strong influence on our perceptions of time and rewards. This raises the question of whether large language models, when asked in different languages, show different preferences for rewards over time and if their choices are similar to those of humans. In this study, we analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages, exploring preferences between smaller, sooner rewards and larger, later rewards. Our results show that GPT displays greater patience when prompted in languages with weak future tense references (FTR), such as German and Mandarin, compared to languages with strong FTR, like English and French. These findings are consistent with existing literature and suggest a correlation between GPT's choices and the preferences of speakers of these languages. However, further analysis reveals that the preference for earlier or later rewards does not systematically change with reward gaps, indicating a lexicographic preferen
    
[^75]: 基于因果感知的知识引导句子提取

    Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v1 [cs.CL])

    [http://arxiv.org/abs/2305.01876](http://arxiv.org/abs/2305.01876)

    该论文提出了一种基于因果感知的知识引导提示方法，将其作为干预器装备到基于预训练语言模型的句子提取器中，以缓解概念偏差。在代表性的多语言KG数据集上进行广泛实验，获得了最先进的结果。

    

    概念有助于自然语言理解，但现有的知识图谱（KG）中远未完善。最近，预训练语言模型（PLM）已被广泛用于基于文本的概念提取（CE）。然而，PLM往往从大量语料库的共现关联中进行预训练知识挖掘，而非Token之间的真实因果关系。因此，预训练知识混淆了PLM，导致提取基于虚假共现相关性的有偏概念，不可避免地导致低精度。本文通过结构因果模型（SCM）提出了一种知识引导提示方法，将其作为干预器装备到基于PLM的提取器中，以减轻概念偏差。提示采用现有KG中的给定实体主题来缓解实体和有偏概念之间的虚假共现相关性。我们在代表性的多语言KG数据集上进行了广泛的实验，证明了我们提出的提示显著改进了提取性能，并达到了最先进的结果。

    Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens.As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt 
    
[^76]: 使用同态加密对大规模CNN进行敏感调整以进行端到端安全预测

    Sensitive Tuning of Large Scale CNNs for E2E Secure Prediction using Homomorphic Encryption. (arXiv:2304.14836v1 [cs.LG])

    [http://arxiv.org/abs/2304.14836](http://arxiv.org/abs/2304.14836)

    本论文提出一种新的HE友好模型训练方法，成功演示了在ResNet和ConvNeXt等经典和现代CNN上运行加密样本，并以前所未有的方式演示了如何使用CLIP GPT-2模型进行零知识安全预测，是一种可行的隐私保护机器学习解决方案。

    

    隐私保护的机器学习解决方案近来受到了广泛关注。其中一种有前途的研究趋势是使用同态加密（HE），这是一种在加密数据上执行计算的方法。这种方法的一个主要挑战是训练适用于HE的加密或未加密的深层CNN，以实现良好的准确性。我们提出了一种新的HE友好模型训练方法，并在基本和现代CNN上进行了演示，例如ResNet和ConvNeXt。训练后，我们使用HELayers SDK运行加密样本来评估我们的模型，并证明它们产生了所需的结果。在ImageNet数据集上运行时，我们的ResNet-18/50/101实现仅需要7、31和57分钟，这表明这个解决方案是实用的。此外，我们提供了一些关于在HE下处理激活函数和跳跃连接的见解。最后，我们以前所未有的方式演示了如何使用CLIP GPT-2模型进行零知识安全预测。

    Privacy-preserving machine learning solutions have recently gained significant attention. One promising research trend is using Homomorphic Encryption (HE), a method for performing computation over encrypted data. One major challenge in this approach is training HE-friendly, encrypted or unencrypted, deep CNNs with decent accuracy. We propose a novel training method for HE-friendly models, and demonstrate it on fundamental and modern CNNs, such as ResNet and ConvNeXt. After training, we evaluate our models by running encrypted samples using HELayers SDK and proving that they yield the desired results. When running on a GPU over the ImageNet dataset, our ResNet-18/50/101 implementations take only 7, 31 and 57 minutes, respectively, which shows that this solution is practical. Furthermore, we present several insights on handling the activation functions and skip-connections under HE. Finally, we demonstrate in an unprecedented way how to perform secure zero-shot prediction using a CLIP m
    
[^77]: 使用隐式神经表示学习旋转和平移不变性表示

    Rotation and Translation Invariant Representation Learning with Implicit Neural Representations. (arXiv:2304.13995v1 [cs.CV])

    [http://arxiv.org/abs/2304.13995](http://arxiv.org/abs/2304.13995)

    本文提出了一种使用隐式神经表示和超网络进行表示学习的方法，称为不变性表示学习，可以在复杂图像上学到分离的语义表示，并且和SCAN很好地协同工作，从而获得最先进的无监督聚类结果。

    

    在许多计算机视觉应用中，图像是以任意或随机旋转和平移的方式获取的。在这样的设置中，获取与图像方向无关的语义表示是可取的。本文提出了一种使用隐式神经表示（INR）和超网络进行表示学习的方法，称为不变性表示学习（IRL-INR）。我们展示了IRL-INR可以在比以前的工作中更复杂的图像上有效地学习分离的语义表示，并且表明这些语义表示可以与SCAN很好地协同工作，从而产生最先进的无监督聚类结果。

    In many computer vision applications, images are acquired with arbitrary or random rotations and translations, and in such setups, it is desirable to obtain semantic representations disentangled from the image orientation. Examples of such applications include semiconductor wafer defect inspection, plankton microscope images, and inference on single-particle cryo-electron microscopy (cryo-EM) micro-graphs. In this work, we propose Invariant Representation Learning with Implicit Neural Representation (IRL-INR), which uses an implicit neural representation (INR) with a hypernetwork to obtain semantic representations disentangled from the orientation of the image. We show that IRL-INR can effectively learn disentangled semantic representations on more complex images compared to those considered in prior works and show that these semantic representations synergize well with SCAN to produce state-of-the-art unsupervised clustering results.
    
[^78]: WizardLM: 增强大型语言模型遵循复杂指令的能力

    WizardLM: Empowering Large Language Models to Follow Complex Instructions. (arXiv:2304.12244v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.12244](http://arxiv.org/abs/2304.12244)

    本文使用 Evol-Instruct 方法创建了大量不同复杂度的指令数据用于微调 LLaMA 模型，得到了新模型 WizardLM。人类评估结果表明 Evol-Instruct 生成的指令优于人工创建的，而 WizardLM 输出的结果也比 OpenAI ChatGPT 更受欢迎。

    

    使用开放域指令追踪数据对大型语言模型进行训练带来了巨大的成功。然而，手动创建这样的指令数据非常耗时和劳动密集，且人类可能难以生成高复杂度指令。在本文中，我们展示了使用LLM而不是人类创建大量不同复杂度指令数据的途径。我们从一组初始指令开始，使用我们提出的Evol-Instruct逐步将其重新编写为更复杂的指令。然后，将所有生成的指令数据混合以微调LLaMA。我们称结果模型为WizardLM。针对一个复杂度平衡的测试集和Vicuna的测试集进行的人类评估表明，Evol-Instruct生成的指令优于人工创建的指令。通过分析高复杂性部分的人类评估结果，我们证明了从我们的WizardLM生成的输出比从OpenAI ChatGPT生成的输出更受欢迎。在GPT-4自动评估中，WizardLM产生了最好的结果。

    Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation
    
[^79]: 作为经典计划的量子电路最优布局综合

    Optimal Layout Synthesis for Quantum Circuits as Classical Planning. (arXiv:2304.12014v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2304.12014](http://arxiv.org/abs/2304.12014)

    本文提供了两种编码，将最优布局综合作为经典规划问题，并使用最优的经典规划器来综合标准基准测试的最优布局，解决了对于大规模量子比特优化布局的问题。

    

    在布局综合中，将量子电路的逻辑量子比特映射到给定量子硬件平台的物理量子比特，考虑物理量子比特的连接。这涉及在应用于远距离量子比特的操作之前插入SWAP门。最优布局综合对于当前误差率较高的硬件上实用的量子计算非常重要：最小化SWAP门数量直接减轻了运行量子电路时的错误率。近年来，已经提出了几种方法来最小化所需的SWAP插入次数。所提出的精确方法只能扩展到少量的量子比特。证明所需的交换插入次数是最优的要比生成近似最优的映射困难得多。在本文中，我们提供了两种编码，将最优布局综合作为经典规划问题。我们使用最优的经典规划器来综合标准基准测试的最优布局。我们的结果显示了我们方法的可扩展性。

    In Layout Synthesis, the logical qubits of a quantum circuit are mapped to the physical qubits of a given quantum hardware platform, taking into account the connectivity of physical qubits. This involves inserting SWAP gates before an operation is applied on distant qubits. Optimal Layout Synthesis is crucial for practical Quantum Computing on current error-prone hardware: Minimizing the number of SWAP gates directly mitigates the error rates when running quantum circuits.  In recent years, several approaches have been proposed for minimizing the required SWAP insertions. The proposed exact approaches can only scale to a small number of qubits. Proving that a number of swap insertions is optimal is much harder than producing near optimal mappings.  In this paper, we provide two encodings for Optimal Layout Synthesis as a classical planning problem. We use optimal classical planners to synthesize the optimal layout for a standard set of benchmarks. Our results show the scalability of ou
    
[^80]: 用于GPU模拟非凸物体的本地物体裁剪碰撞网络的高效碰撞检测算法

    Local object crop collision network for efficient simulation of non-convex objects in GPU-based simulators. (arXiv:2304.09439v1 [cs.RO])

    [http://arxiv.org/abs/2304.09439](http://arxiv.org/abs/2304.09439)

    提出了一种数据驱动的碰撞检测方法，用于高效模拟非凸物体，不需要在计算速度和准确性之间进行权衡，具有较小的在线计算时间和更好的GPU利用率。

    

    本文旨在开发一种用于GPU模拟非凸物体的高效碰撞检测算法。目前GPU模拟器在模拟非凸物体时需要在速度、通用性和精度之间做出权衡，主要问题在于现有的碰撞检测算法需要在计算速度和准确性之间进行权衡。我们提出了一种数据驱动的碰撞检测方法，其中精度仅取决于离线数据集的质量和数量，而不是在线计算时间。

    Our goal is to develop an efficient contact detection algorithm for large-scale GPU-based simulation of non-convex objects. Current GPU-based simulators such as IsaacGym and Brax must trade-off speed with fidelity, generality, or both when simulating non-convex objects. Their main issue lies in contact detection (CD): existing CD algorithms, such as Gilbert-Johnson-Keerthi (GJK), must trade off their computational speed with accuracy which becomes expensive as the number of collisions among non-convex objects increases. We propose a data-driven approach for CD, whose accuracy depends only on the quality and quantity of offline dataset rather than online computation time. Unlike GJK, our method inherently has a uniform computational flow, which facilitates efficient GPU usage based on advanced compilers such as XLA (Accelerated Linear Algebra). Further, we offer a data-efficient solution by learning the patterns of colliding local crop object shapes, rather than global object shapes whi
    
[^81]: LLM作为机器人的大脑：统一自我中心记忆与控制

    LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v1 [cs.AI])

    [http://arxiv.org/abs/2304.09349](http://arxiv.org/abs/2304.09349)

    本文提出了一个统一自我中心记忆和控制的框架LLM-Brain，使用大规模语言模型作为机器人大脑进行零-shot学习。该框架包括封闭式多轮对话，覆盖了感知、规划、控制和记忆，具有很好的泛化性能，适用于多个机器人任务。

    

    体感人工智能研究和开发具备物理或虚拟实体（即机器人）并能够与环境动态交互的智能系统。记忆和控制是体感系统的两个基本部分，通常需要分别使用框架进行建模。本文提出了一个新的、可推广的框架，称为LLM-Brain：使用大规模语言模型作为机器人大脑，统一自我中心记忆和控制。LLM-Brain框架集成了多个多模态语言模型用于机器人任务，利用零-shot学习方法。LLM-Brain中的所有组件使用自然语言进行封闭式多轮对话，包括感知、规划、控制和记忆。系统的核心是一个具备自我中心记忆和控制机器人的实体LLM。我们通过研究两个下游任务：主动探索和实体问答来演示LLM-Brain。

    Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The
    
[^82]: 语言指导下的强化学习以实现人工智能协作

    Language Instructed Reinforcement Learning for Human-AI Coordination. (arXiv:2304.07297v1 [cs.AI])

    [http://arxiv.org/abs/2304.07297](http://arxiv.org/abs/2304.07297)

    本文提出了一种称之为instructRL的新的框架，它通过自然语言指令来指定对人工智能搭档的预期策略，解决在缺乏高质量人类行为数据的领域中多智能体强化学习收敛于人类不偏爱的策略的问题，从而提高了人工智能协作的性能。

    

    人工智能的一个基本问题是如何让智能体能够和人类有效地协作。本文提出了一种称之为instructRL的新的框架，让人们可以通过自然语言指令来指定对人工智能搭档的预期策略，以此解决在缺乏较高质量的人类行为数据的领域中，由于多智能体强化学习常常会收敛到人类并不偏爱的策略的不足。我们使用预先训练的大型语言模型来生成一个在人类指令下的先验策略，并将其用于约束强化学习目标。这导致强化学习智能体收敛到与人类喜好一致的均衡点。通过概念证明环境和具有挑战性的Hanabi基准，证明了instructRL收敛于满足给定指令的类似人类智能体的策略。最后，我们证明了知道语言指令显著提高了人工智能协作的性能。

    One of the fundamental quests of AI is to produce agents that coordinate well with humans. This problem is challenging, especially in domains that lack high quality human behavioral data, because multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer. We propose a novel framework, instructRL, that enables humans to specify what kind of strategies they expect from their AI partners through natural language instructions. We use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective. This leads to the RL agent converging to equilibria that are aligned with human preferences. We show that instructRL converges to human-like policies that satisfy the given instructions in a proof-of-concept environment as well as the challenging Hanabi benchmark. Finally, we show that knowing the language instruction significantly boosts human-AI coordination pe
    
[^83]: 使用置信度增强强化学习在时间知识图上改进少样本归纳学习

    Improving Few-Shot Inductive Learning on Temporal Knowledge Graphs using Confidence-Augmented Reinforcement Learning. (arXiv:2304.00613v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.00613](http://arxiv.org/abs/2304.00613)

    本论文介绍了一种名为FITCARL的TKGC方法，它将少样本学习与强化学习相结合，以实现在时间知识图上进行少样本归纳学习和预测。在FITCARL中，一个智能体通过策略网络来指导搜索过程，通过引入合成样本的模块来解决数据稀缺性，在真实世界数据上取得了最新的结果。

    

    时间知识图补全（TKGC）旨在预测时间知识图（TKG）中实体之间缺失的联系。大多数先前的TKGC方法仅考虑在训练集中看到的实体之间预测缺失的联系，而无法在关于新出现的未见实体的链接预测方面达到良好的性能。最近，提出了一种新的任务，即TKG少样本场景外链接预测，其中TKGC模型需要在关于仅有少量观察样本的新出现实体方面实现良好的链接预测性能。在这项工作中，我们提出了一种名为FITCARL的TKGC方法，该方法将少样本学习与强化学习相结合来解决这个问题。在FITCARL中，一个智能体跨越整个TKG寻找预测答案。我们设计了一个策略网络，根据遍历的路径指导搜索过程。为了更好地解决少样本设置中的数据稀缺问题，我们引入了一个模块，生成合成样本来增加训练集。通过在真实世界数据集上进行广泛实验，我们证明FITCARL在少样本场景外链接预测和传统TKGC任务上均取得了最新的结果。

    Temporal knowledge graph completion (TKGC) aims to predict the missing links among the entities in a temporal knwoledge graph (TKG). Most previous TKGC methods only consider predicting the missing links among the entities seen in the training set, while they are unable to achieve great performance in link prediction concerning newly-emerged unseen entities. Recently, a new task, i.e., TKG few-shot out-of-graph (OOG) link prediction, is proposed, where TKGC models are required to achieve great link prediction performance concerning newly-emerged entities that only have few-shot observed examples. In this work, we propose a TKGC method FITCARL that combines few-shot learning with reinforcement learning to solve this task. In FITCARL, an agent traverses through the whole TKG to search for the prediction answer. A policy network is designed to guide the search process based on the traversed path. To better address the data scarcity problem in the few-shot setting, we introduce a module tha
    
[^84]: 论网络网络安全的可解释人工智能综述

    A Survey on Explainable Artificial Intelligence for Network Cybersecurity. (arXiv:2303.12942v1 [cs.CR])

    [http://arxiv.org/abs/2303.12942](http://arxiv.org/abs/2303.12942)

    这篇论文综述了网络驱动的威胁和问题的系统分类，审查了网络系统中的可解释人工智能在网络安全中的最新技术，并勾画了未来研究的有前途的方向。

    

    人工智能模型的黑匣子特性一直是用于关键应用程序的许多关注点的来源。可解释人工智能（XAI）是一个快速发展的研究领域，旨在创建能够为其决策和行动提供清晰可解释解释的机器学习模型。在网络网络安全领域，XAI具有通过使我们能够更好地了解网络威胁的行为并设计更有效的防御来彻底改变我们处理网络安全的方式的潜力。在本综述中，我们对网络驱动的威胁和问题进行系统分类，审查了网络系统中的可解释人工智能（XAI）在网络网络安全中的最新技术，探讨了各种解决这个重要问题的方法。我们讨论了当前XAI方法在网络安全背景下的挑战和局限性，并勾画未来研究的有前途的方向。

    The black-box nature of artificial intelligence (AI) models has been the source of many concerns in their use for critical applications. Explainable Artificial Intelligence (XAI) is a rapidly growing research field that aims to create machine learning models that can provide clear and interpretable explanations for their decisions and actions. In the field of network cybersecurity, XAI has the potential to revolutionize the way we approach network security by enabling us to better understand the behavior of cyber threats and to design more effective defenses. In this survey, we review the state of the art in XAI for cybersecurity in network systems and explore the various approaches that have been proposed to address this important problem. The review follows a systematic classification of network-driven cybersecurity threats and issues. We discuss the challenges and limitations of current XAI methods in the context of cybersecurity and outline promising directions for future research.
    
[^85]: Reflexion：具有动态记忆和自我反思的自主智能体

    Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])

    [http://arxiv.org/abs/2303.11366](http://arxiv.org/abs/2303.11366)

    本文提出 Reflexion 方法，给智能体赋予了动态记忆和自我反思能力，以增强其任务特定的行动选择能力。

    

    最近决策大型语言模型（LLM）代理的发展在各种基准测试中展现出卓越的性能。然而，这些最先进的方法通常需要内部模型微调、外部模型微调或在定义的状态空间上进行策略优化。由于高质量训练数据的稀缺性或缺乏良好定义的状态空间，实现这些方法可能会具有挑战性。此外，这些代理没有人类决策过程固有的某些品质，特别是从错误中学习的能力。通过反思，人类可以通过试错过程高效地解决新的问题。在最近的研究基础上，我们提出 Reflexion，一种将动态记忆和自我反思能力赋予智能体的方法，以增强其现有的推理轨迹和任务特定的行动选择能力。为了实现完全自动化，我们介绍了一种简单而有效的方法。

    Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
    
[^86]: 数据中心人工智能综述：一份调查报告。

    Data-centric Artificial Intelligence: A Survey. (arXiv:2303.10158v1 [cs.LG])

    [http://arxiv.org/abs/2303.10158](http://arxiv.org/abs/2303.10158)

    本文讨论了数据中心人工智能的必要性并从三个一般性数据中心目标和代表性方法的全面视角进行了介绍。该综述从自动化和协作的角度组织了现有文献并讨论了挑战。

    

    人工智能（AI）正在几乎所有领域产生深远的影响，其成功的关键之一是可用于构建机器学习模型的丰富高质量数据。最近，数据在AI中的作用得到了显著放大，引发了数据中心AI这一新兴概念的出现。研究人员和从业者的注意力逐渐从推进模型设计转向提高数据质量和数量。在本调查中，我们讨论了数据中心AI的必要性，随后从训练数据开发、推理数据开发和数据维护三个一般性数据中心目标以及代表性方法的全面视角进行了介绍。我们还从自动化和协作的角度组织了现有文献，讨论了挑战，并列出了各种任务的测试基准。我们认为，这是第一份提供跨越各个阶段一系列任务的全球视角的综合性调查。

    Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages o
    
[^87]: 单个GPU的高吞吐大语言模型生成推理技术——FlexGen

    FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU. (arXiv:2303.06865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06865](http://arxiv.org/abs/2303.06865)

    本论文提出了一种名为FlexGen的技术，使用单个GPU实现大型语言模型的高吞吐推理。FlexGen通过聚合来自GPU、CPU和磁盘的内存和计算，搜索有效的张量存储和访问模式，并将权重和注意力缓存压缩为4个位。这些技术使FlexGen具有更大的批量选择空间，从而显著增加了最大吞吐量。

    

    大语言模型(LLM)的高计算和内存需求使其只能在多个高端加速器上实现。本文着眼于对批处理不敏感的任务，并使用有限资源（如单个普通GPU）进行高吞吐量LLM推理的研究。我们提出了FlexGen，一种用于运行具有GPU内存限制的LLMs的高吞吐量生成引擎。通过聚合来自GPU、CPU和磁盘的内存和计算，可以在各种硬件资源约束下灵活配置FlexGen。通过解决线性规划问题，它可以搜索有效的张量存储和访问模式。FlexGen还将权重和注意力缓存压缩为4个位，几乎没有精度损失。这些技术使FlexGen具有更大的批量选择空间，从而显著增加了最大吞吐量。结果，当在单个16GB GPU上运行OPT-175B时，FlexGen的吞吐量为每秒90个序列，比Megatron-LM快4.5倍，比使用单个普通GPU的竞争者快7.5-19倍。

    The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGe
    
[^88]: 基于Transformer的符号回归规划

    Transformer-based Planning for Symbolic Regression. (arXiv:2303.06833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06833](http://arxiv.org/abs/2303.06833)

    该论文提出了一种基于Transformer和蒙特卡罗树搜索的符号回归规划策略TPSR，可以将非可微的反馈作为知识的外部来源融入到方程生成过程中，提高方程生成的准确性和复杂性。

    

    符号回归是机器学习中一项具有挑战性的任务，它涉及基于函数值查找其数学表达式。最近，符号回归的一些进展表明，预训练的基于Transformer的模型对于生成方程序列是有效的，这些模型从合成数据集的大规模预训练中获益，并在推理时间方面比基于GP的方法具有显著优势。然而，这些模型关注的是借鉴文本生成的监督预训练目标，而忽略了方程的特定目标，如准确性和复杂性。为了解决这个问题，我们提出了TPSR，一种基于Transformer的符号回归规划策略，将蒙特卡罗树搜索融入到Transformer解码过程中。与传统的解码策略不同，TPSR允许将非可微的反馈（如拟合准确性和复杂性）作为知识的外部来源融入到方程生成过程中。

    Symbolic regression (SR) is a challenging task in machine learning that involves finding a mathematical expression for a function based on its values. Recent advancements in SR have demonstrated the efficacy of pretrained transformer-based models for generating equations as sequences, which benefit from large-scale pretraining on synthetic datasets and offer considerable advantages over GP-based methods in terms of inference time. However, these models focus on supervised pretraining goals borrowed from text generation and ignore equation-specific objectives like accuracy and complexity. To address this, we propose TPSR, a Transformer-based Planning strategy for Symbolic Regression that incorporates Monte Carlo Tree Search into the transformer decoding process. TPSR, as opposed to conventional decoding strategies, allows for the integration of non-differentiable feedback, such as fitting accuracy and complexity, as external sources of knowledge into the equation generation process. Ext
    
[^89]: 自动驾驶路侧多类型多组传感器检测系统RMMDet(更新版)

    RMMDet: Road-Side Multitype and Multigroup Sensor Detection System for Autonomous Driving. (arXiv:2303.05203v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.05203](http://arxiv.org/abs/2303.05203)

    本文提出了一种适用于自动驾驶的路侧多类型多组传感器检测系统RMMDet，采用ROS虚拟环境模拟真实道路条件，能够实现多类型传感器检测和多组传感器融合，在各种实验中得到部分成功。

    

    自动驾驶技术在人工智能的帮助下取得了长足的进步，许多先进的方法已被提出用于车辆端目标检测，包括单传感器或多传感器检测方法。然而，复杂多变的实际交通情况需要探究如何将这些方法应用于实际道路条件中。本文提出了RMMDet，一种适用于自动驾驶的路侧多类型多组传感器检测系统。我们在基于ROS的虚拟环境中模拟真实世界条件，特别是传感器的物理和功能构造。然后，在此环境中实现了多类型传感器检测和多组传感器融合，包括基于结果级融合的摄像头-雷达和摄像头-Lidar检测。我们还制作了本地数据集和真实沙盘场地，并进行了各种实验。此外，我们还将多智能体协同调度系统与融合检测系统相链接。

    Autonomous driving has now made great strides thanks to artificial intelligence, and numerous advanced methods have been proposed for vehicle end target detection, including single sensor or multi sensor detection methods. However, the complexity and diversity of real traffic situations necessitate an examination of how to use these methods in real road conditions. In this paper, we propose RMMDet, a road-side multitype and multigroup sensor detection system for autonomous driving. We use a ROS-based virtual environment to simulate real-world conditions, in particular the physical and functional construction of the sensors. Then we implement muti-type sensor detection and multi-group sensors fusion in this environment, including camera-radar and camera-lidar detection based on result-level fusion. We produce local datasets and real sand table field, and conduct various experiments. Furthermore, we link a multi-agent collaborative scheduling system to the fusion detection system. Hence,
    
[^90]: "Wasserstein Believer:通过可靠的潜在空间模型学习部分可观测环境下的信念更新"

    The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models. (arXiv:2303.03284v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.03284](http://arxiv.org/abs/2303.03284)

    本论文提出了一种“Wasserstein Belief Updater”算法来学习部分可观测环境下的信念更新，该算法通过学习POMDP​​的潜在模型和置信更新的近似值，实现了对历史观察和行动的有效压缩，提升了算法的性能表现。

    

    部分可观测马尔可夫决策过程（POMDP）是建模代理无法感知到完整状态的环境的有用工具。因此，代理需要考虑过去的观察和行动进行推理。但是，由于历史空间指数级增长，仅仅记住完整历史通常是不可行的。保持模拟真实状态的置信概率分布可以作为历史的充分统计量，但其计算需要访问环境的模型，因此也是不可行的。最先进的算法使用递归神经网络来压缩观察-行动历史以学习充分的统计量，但它们缺乏成功的保证并可能导致次优策略。为了克服这一点，我们提出了Wasserstein Belief Updater ，这是一种RL算法，它学习POMDP​​的潜在模型和置信更新的近似值。我们的方法具有理论成果。

    Partially Observable Markov Decision Processes (POMDPs) are useful tools to model environments where the full state cannot be perceived by an agent. As such the agent needs to reason taking into account the past observations and actions. However, simply remembering the full history is generally intractable due to the exponential growth in the history space. Keeping a probability distribution that models the belief over what the true state is can be used as a sufficient statistic of the history, but its computation requires access to the model of the environment and is also intractable. State-of-the-art algorithms use Recurrent Neural Networks to compress the observation-action history aiming to learn a sufficient statistic, but they lack guarantees of success and can lead to sub-optimal policies. To overcome this, we propose the Wasserstein Belief Updater, an RL algorithm that learns a latent model of the POMDP and an approximation of the belief update. Our approach comes with theoreti
    
[^91]: DART: 多样化聚合重复训练改进神经网络的泛化能力

    DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks. (arXiv:2302.14685v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14685](http://arxiv.org/abs/2302.14685)

    本文中提出了DART策略，其中利用多样化的增强方法训练不同的模型，然后通过聚合这些模型的权重来结合其专业知识，并重复聚合步骤以实现更好的泛化能力。

    

    神经网络的泛化能力对于在现实世界中安全部署它们至关重要。改进泛化的常见训练策略包括使用数据增强、集成和模型平均化。在本文中，我们首先建立了一个惊人简单但强有力的泛化基准，它利用了训练小批量中的多种不同增强方法，并证明这可以学习到一个更平衡的特征分布。进一步地，我们提出了Diversify-Aggregate-Repeat Training (DART)策略，该策略首先使用不同的增强方法(或领域)训练不同的模型，以探索损失盆地，并进一步聚合它们的权重，结合它们的专业知识并获得改进的泛化能力。我们发现，在整个训练过程中重复聚合步骤可以提高整体优化轨迹，并确保单个模型具有足够低的损失障碍，在将它们组合时可以获得改进的泛化能力。我们解释了我们的方法是一种正则化形式，它迫使模型探索损失景观的多种模式。我们的实验表明，DART在CIFAR-10和CIFAR-100上实现了最先进的性能，并在ImageNet和鲁棒性基准测试中取得了有竞争力的结果。

    Generalization of neural networks is crucial for deploying them safely in the real world. Common training strategies to improve generalization involve the use of data augmentations, ensembling and model averaging. In this work, we first establish a surprisingly simple but strong benchmark for generalization which utilizes diverse augmentations within a training minibatch, and show that this can learn a more balanced distribution of features. Further, we propose Diversify-Aggregate-Repeat Training (DART) strategy that first trains diverse models using different augmentations (or domains) to explore the loss basin, and further Aggregates their weights to combine their expertise and obtain improved generalization. We find that Repeating the step of Aggregation throughout training improves the overall optimization trajectory and also ensures that the individual models have a sufficiently low loss barrier to obtain improved generalization on combining them. We shed light on our approach by 
    
[^92]: 跨语音: 面向跨语音合成的语音无关特征表示方法

    CrossSpeech: Speaker-independent Acoustic Representation for Cross-lingual Speech Synthesis. (arXiv:2302.14370v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2302.14370](http://arxiv.org/abs/2302.14370)

    CrossSpeech提出了一种改善跨语音TTS质量的方法，通过将语音表示分解为语音无关生成器和说话人相关的生成器并分别处理，实现了分离的说话人和语言表示，同时在Blizzard Challenge 2019任务中取得了最先进的性能。

    

    尽管最近的文本到语音合成系统已经在实现人类水平质量方面取得了显著进展，但跨语音合成的性能仍然落后于同种语言的语音合成。本文提出了一种名为CrossSpeech的方法，通过在声学特征空间的级别上有效地分离说话人和语言信息，改善跨语音语音的质量。具体而言，CrossSpeech将语音生成流程分解为语音无关生成器（SIG）和说话人相关的生成器（SDG）。 SIG生成与特定说话人分布无关的语音无关声学表示。另一方面，SDG模型说话人相关的语音变化，表征说话人属性。通过将每个信息分开处理，CrossSpeech可以获得分离的说话人和语言表示。从实验中，我们验证了CrossSpeech在自然度和说话人相似性方面优于现有的跨语音TTS模型，并在Blizzard Challenge 2019任务中取得了最先进的性能。

    While recent text-to-speech (TTS) systems have made remarkable strides toward human-level quality, the performance of cross-lingual TTS lags behind that of intra-lingual TTS. This gap is mainly rooted from the speaker-language entanglement problem in cross-lingual TTS. In this paper, we propose CrossSpeech which improves the quality of cross-lingual speech by effectively disentangling speaker and language information in the level of acoustic feature space. Specifically, CrossSpeech decomposes the speech generation pipeline into the speaker-independent generator (SIG) and speaker-dependent generator (SDG). The SIG produces the speaker-independent acoustic representation which is not biased to specific speaker distributions. On the other hand, the SDG models speaker-dependent speech variation that characterizes speaker attributes. By handling each information separately, CrossSpeech can obtain disentangled speaker and language representations. From the experiments, we verify that CrossSp
    
[^93]: (Re)$^2$H2O: 自主驾驶场景生成的反向正则化混合离线和在线强化学习

    (Re)$^2$H2O: Autonomous Driving Scenario Generation via Reversely Regularized Hybrid Offline-and-Online Reinforcement Learning. (arXiv:2302.13726v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13726](http://arxiv.org/abs/2302.13726)

    提出了一个新的自主驾驶场景生成框架，结合了离线实际数据和在线模拟数据，将反向 L_1 正则化用于关键特征提取，并有效地查询更高维度搜索空间中的场景，实验证明了方法的有效性。

    

    自主驾驶及其广泛采用一直被寄予厚望。然而，在没有可信的全面测试程序的情况下，不仅行业难以大规模生产自主驾驶车辆(AV)，而且公众和决策者也没有说服接受创新。生成对AV具有重要挑战的安全关键场景是测试的重要第一步。现实世界的数据集包括自然但过于安全的驾驶行为，而模拟则允许无限制地探索多样化和激进的交通场景。相反，模拟中的更高维度搜索空间使得没有实际数据分布作为隐式约束的情况下效率低下。为了将两者的优点结合起来，从离线实际数据和在线模拟数据同时学习生成场景似乎是可行的。因此，我们量身打造了一个称为 (Re)$^2$H2O 的反向正则化混合离线和在线强化学习框架来实现场景生成。具体而言，离线部分使用反向 L_1 正则化来提取关键特征，并指导传输到在线学习中，以满足真实世界数据分布。在线部分则通过从离线数据学到的策略更有效地查询更高维度搜索空间中的场景。在 CARLA 模拟器和实际数据集上的实验验证了我们方法的有效性。

    Autonomous driving and its widespread adoption have long held tremendous promise. Nevertheless, without a trustworthy and thorough testing procedure, not only does the industry struggle to mass-produce autonomous vehicles (AV), but neither the general public nor policymakers are convinced to accept the innovations. Generating safety-critical scenarios that present significant challenges to AV is an essential first step in testing. Real-world datasets include naturalistic but overly safe driving behaviors, whereas simulation would allow for unrestricted exploration of diverse and aggressive traffic scenarios. Conversely, higher-dimensional searching space in simulation disables efficient scenario generation without real-world data distribution as implicit constraints. In order to marry the benefits of both, it seems appealing to learn to generate scenarios from both offline real-world and online simulation data simultaneously. Therefore, we tailor a Reversely Regularized Hybrid Offline-
    
[^94]: ChatGPT：应付千事的万能型 AI，但无所专精

    ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.10724](http://arxiv.org/abs/2302.10724)

    本研究检验了 ChatGPT 在 25 个不同的 NLP 任务上的性能，它是一个万能的 AI 模型，但无关紧要的表现可能会对某些任务的表现产生负面影响。

    

    OpenAI 推出了聊天生成预训练 Transformer（ChatGPT），革新了人工智能与人类互动的方法。许多研究通过测试 ChatGPT 在众所周知的自然语言处理（NLP）任务中的效果，来评估该模型的效能。然而，现有的研究大多非自动化，并且规模非常有限。本研究在 25 个不同的 NLP 任务上检验了 ChatGPT 的性能，其中大多数任务甚至对人类而言都是主观的，例如情感分析、情绪识别、攻击性和立场检测。另一些任务则需要更客观的推理，如词义消歧、语言可接受性和问答。我们还对 GPT-4 模型在五个选定的 NLP 任务子集上进行了评估。我们自动化了 ChatGPT 和 GPT-4 的引导过程，并分析了超过 49k 个响应。与现有最先进的解决方案（SOTA）进行比较，我们的结果显示，在一些任务上 ChatGPT 的性能存在一定的缺陷。

    OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quali
    
[^95]: 一项关于生成对抗网络在人员再识别系统数据增强中的综述

    A Review on Generative Adversarial Networks for Data Augmentation in Person Re-Identification Systems. (arXiv:2302.09119v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.09119](http://arxiv.org/abs/2302.09119)

    本文综述了使用生成对抗网络通过数据增强来改进人员再识别模型性能的最新方法。

    

    近年来，自动人员再识别系统的兴趣显着增加，主要是为了开发监控和智能店铺软件。由于人员姿势的变化、不同的照明条件和遮挡情况，以及不同摄像头获得的图像质量较差，这是目前尚未解决的问题。在基于机器学习的计算机视觉应用中，通过扩展模型训练所需的图像或视频集合来改进再识别系统的性能是一种可能性。目前，生成对抗网络是生成合成信息进行数据增强的最强大方式之一，无论是视频、图像还是文本。本文综述了使用生成对抗网络通过数据增强来改进人员再识别模型性能的最新方法。我们关注三个分类：(1) 基于深度学习的生成对抗网络数据增强方法，(2) 基于生成对抗网络的时空数据扩展方法，(3) 基于生成对抗网络的跨摄像头域自适应方法。

    Interest in automatic people re-identification systems has significantly grown in recent years, mainly for developing surveillance and smart shops software. Due to the variability in person posture, different lighting conditions, and occluded scenarios, together with the poor quality of the images obtained by different cameras, it is currently an unsolved problem. In machine learning-based computer vision applications with reduced data sets, one possibility to improve the performance of re-identification system is through the augmentation of the set of images or videos available for training the neural models. Currently, one of the most robust ways to generate synthetic information for data augmentation, whether it is video, images or text, are the generative adversarial networks. This article reviews the most relevant recent approaches to improve the performance of person re-identification models through data augmentation, using generative adversarial networks. We focus on three categ
    
[^96]: 知道自己不知道的概率电路

    Probabilistic Circuits That Know What They Don't Know. (arXiv:2302.06544v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06544](http://arxiv.org/abs/2302.06544)

    本文指出概率电路（PC）对超出分布（OOD）数据不具备鲁棒性；通过模型不确定性量化，我们提出了可处理的随机失活推断（TDI）来克服这一挑战，并且这种方法可以在单个正向传递中提供可处理的无采样不确定性估计，从而改善了PC对分布漂移和OOD数据的鲁棒性。

    

    概率电路（PC）是一种允许准确和可处理的概率推断的模型。与神经网络相比，它们通常被认为是良好校准的，并且对于超出分布（OOD）数据具有鲁棒性。本文表明 PC 实际上不具有对OOD数据的鲁棒性，进而展示了如何通过模型不确定性量化来克服这一挑战。为此，我们提出了可处理的随机失活推断（TDI）——一种推断程序，通过方差传播导出蒙特卡洛失活（MCD）的解析解来估计不确定性。与神经网络中的 MCD 不同，TDI不需要进行多次网络评估就可以提供可处理的无采样不确定性估计。通过一系列实验评估在真实数据上的分类置信度和不确定性估计，TDI改善了PC对分布漂移和OOD数据的鲁棒性。

    Probabilistic circuits (PCs) are models that allow exact and tractable probabilistic inference. In contrast to neural networks, they are often assumed to be well-calibrated and robust to out-of-distribution (OOD) data. In this paper, we show that PCs are in fact not robust to OOD data, i.e., they don't know what they don't know. We then show how this challenge can be overcome by model uncertainty quantification. To this end, we propose tractable dropout inference (TDI), an inference procedure to estimate uncertainty by deriving an analytical solution to Monte Carlo dropout (MCD) through variance propagation. Unlike MCD in neural networks, which comes at the cost of multiple network evaluations, TDI provides tractable sampling-free uncertainty estimates in a single forward pass. TDI improves the robustness of PCs to distribution shift and OOD data, demonstrated through a series of experiments evaluating the classification confidence and uncertainty estimates on real-world data.
    
[^97]: 域索引变分贝叶斯：可解释的域索引用于域自适应

    Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation. (arXiv:2302.02561v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02561](http://arxiv.org/abs/2302.02561)

    该论文介绍了一种新的域自适应方法，该方法利用了从多域数据中生成的域索引，提供额外的洞察视角，并在各种任务中实现了最佳性能。

    

    先前的研究表明，利用域索引可以显著提高域自适应性能。然而，并非总是有这样的域索引可用。为解决这一挑战，我们首先从概率角度提供了域索引的正式定义，然后提出了一个对抗性变分贝叶斯框架，从多域数据中推断出域索引，从而提供额外的域关系洞察，并提高域自适应性能。我们的理论分析表明，我们的对抗性变分贝叶斯框架在平衡点处找到了最优的域索引。对合成和真实数据的实证结果验证了我们的模型可以产生可解释的域索引，使我们可以实现优于现有域适应方法的性能。代码可在https://github.com/Wang-ML-Lab/VDI获得。

    Previous studies have shown that leveraging domain index can significantly boost domain adaptation performance (arXiv:2007.01807, arXiv:2202.03628). However, such domain indices are not always available. To address this challenge, we first provide a formal definition of domain index from the probabilistic perspective, and then propose an adversarial variational Bayesian framework that infers domain indices from multi-domain data, thereby providing additional insight on domain relations and improving domain adaptation performance. Our theoretical analysis shows that our adversarial variational Bayesian framework finds the optimal domain index at equilibrium. Empirical results on both synthetic and real data verify that our model can produce interpretable domain indices which enable us to achieve superior performance compared to state-of-the-art domain adaptation methods. Code is available at https://github.com/Wang-ML-Lab/VDI.
    
[^98]: 持续学习综述：理论、方法与应用

    A Comprehensive Survey of Continual Learning: Theory, Method and Application. (arXiv:2302.00487v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00487](http://arxiv.org/abs/2302.00487)

    本文为持续学习的全面综述，总结了持续学习的一般目标，回顾了各种持续学习方法，并强调了一些有前途的应用领域和未来研究方向。

    

    智能系统需要在其生命周期内不断获取、更新、积累和利用知识以应对现实世界的动态变化。这种能力称为持续学习，为AI系统自适应发展提供了基础。然而，持续学习的一个显著限制是灾难性遗忘，即学习一个新任务通常会导致旧任务的性能显著降低。近年来，不断涌现的各种进展大大扩展了持续学习的理解和应用。本文提供了一个全面的持续学习综述，旨在连接基本设置、理论基础、代表性方法和实际应用。我们总结了现有理论和实证结果，概括了持续学习的一般目标：减少灾难性遗忘，实现高效的和终身的学习，以及实现更深层次的表征学习。我们还回顾了各种持续学习方法，包括基于正则化、基于回放和基于生成的方法，并讨论了它们的优缺点。最后，我们强调了一些有前途的应用领域，如机器人、自然语言处理和计算机视觉，并确定了一些开放挑战和未来研究方向。

    To cope with real-world dynamics, an intelligent agent needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of 
    
[^99]: 新的分布水平度量方法：弃用$\Delta$DP，实现人口统计特征的公平机器学习

    Retiring $\Delta$DP: New Distribution-Level Metrics for Demographic Parity. (arXiv:2301.13443v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13443](http://arxiv.org/abs/2301.13443)

    本文提出两种新公平度量方法：概率密度函数曲线之间区域（ABPC）和累积密度函数曲线之间区域（ABCC），实现人口统计特征的公平机器学习

    

    人口统计特征的平等对待是机器学习中最广泛认可的公平度量标准。为实现人口统计平等，许多研究致力于追求常用度量方法$\Delta DP$。然而，本文揭示了公平度量方法$\Delta DP$存在固有缺陷：i) 零值$\Delta DP$不保证民族统计平等的零违规，ii) $\Delta DP$值随不同分类阈值变化。为此，提出了两种新公平度量方法——概率密度函数曲线之间区域（ABPC）和累积密度函数曲线之间区域（ABCC），以精确测量不同民族统计群体预测概率分布之间的差异。

    Demographic parity is the most widely recognized measure of group fairness in machine learning, which ensures equal treatment of different demographic groups. Numerous works aim to achieve demographic parity by pursuing the commonly used metric $\Delta DP$. Unfortunately, in this paper, we reveal that the fairness metric $\Delta DP$ can not precisely measure the violation of demographic parity, because it inherently has the following drawbacks: i) zero-value $\Delta DP$ does not guarantee zero violation of demographic parity, ii) $\Delta DP$ values can vary with different classification thresholds. To this end, we propose two new fairness metrics, Area Between Probability density function Curves (ABPC) and Area Between Cumulative density function Curves (ABCC), to precisely measure the violation of demographic parity at the distribution level. The new fairness metrics directly measure the difference between the distributions of the prediction probability for different demographic group
    
[^100]: 基于最小价值等价部分模型的生涯强化学习的可扩展和鲁棒规划

    Minimal Value-Equivalent Partial Models for Scalable and Robust Planning in Lifelong Reinforcement Learning. (arXiv:2301.10119v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10119](http://arxiv.org/abs/2301.10119)

    本文提出了一种仅模拟环境中相关方面的“最小价值等价部分模型”，并证明了这些模型用于规划在生涯强化学习场景中具有可扩展性优势。

    

    从纯交互中学习环境模型通常被认为是构建生涯强化学习智能体的至关组成部分。然而，在基于模型的强化学习中，通常的做法是学习模型来对智能体的环境的每个方面进行建模，无论这些方面是否在提出最优决策方面重要。本文认为这种模型并不适合在生涯强化学习场景中执行可扩展和鲁棒的规划，因此提出了只模拟环境中相关方面的新型模型，称为“最小价值等价部分模型”。本文提供了这些模型的正式定义，并提供了理论结果来证明使用这些模型进行规划的可扩展性优势，然后进行实验以从实证角度说明我们的理论结果，最后提出一些有用的启发式方法来学习这些模型。

    Learning models of the environment from pure interaction is often considered an essential component of building lifelong reinforcement learning agents. However, the common practice in model-based reinforcement learning is to learn models that model every aspect of the agent's environment, regardless of whether they are important in coming up with optimal decisions or not. In this paper, we argue that such models are not particularly well-suited for performing scalable and robust planning in lifelong reinforcement learning scenarios and we propose new kinds of models that only model the relevant aspects of the environment, which we call "minimal value-equivalent partial models". After providing a formal definition for these models, we provide theoretical results demonstrating the scalability advantages of performing planning with such models and then perform experiments to empirically illustrate our theoretical results. Then, we provide some useful heuristics on how to learn these kinds
    
[^101]: 基于Transformer邮件机制的多智能体强化学习可扩展沟通方法

    Scalable Communication for Multi-Agent Reinforcement Learning via Transformer-Based Email Mechanism. (arXiv:2301.01919v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2301.01919](http://arxiv.org/abs/2301.01919)

    本文提出了一种基于Transformer的邮件机制（TEM），解决了多智能体强化学习中的可扩展性问题，它采用本地通信和消息链转发的方式进行通信，而不需要模拟所有智能体。

    

    在多智能体强化学习中，通讯可以极大地改善合作，尤其是对于部分可观测的任务。然而，现有方法要么广播信息导致信息冗余，要么通过将所有其他智能体模拟成目标来学习有针对性的通信，但是当智能体数量变化时，这种方法无法扩展。为了解决部分可观察任务的可扩展性问题，我们提出了一种新的框架Transformer-based Email Mechanism（TEM）。智能体采用本地通信，仅向可以观察到的智能体发送消息，无需模拟所有智能体。受到人类电子邮件转发合作的启发，我们设计了消息链以将信息转发给观察范围之外的智能体。我们引入Transformer来编码和解码消息链，以选择下一个接收者。在多个合作的MARL基准测试中，实验证明TEM优于基线方法。

    Communication can impressively improve cooperation in multi-agent reinforcement learning (MARL), especially for partially-observed tasks. However, existing works either broadcast the messages leading to information redundancy, or learn targeted communication by modeling all the other agents as targets, which is not scalable when the number of agents varies. In this work, to tackle the scalability problem of MARL communication for partially-observed tasks, we propose a novel framework Transformer-based Email Mechanism (TEM). The agents adopt local communication to send messages only to the ones that can be observed without modeling all the agents. Inspired by human cooperation with email forwarding, we design message chains to forward information to cooperate with the agents outside the observation range. We introduce Transformer to encode and decode the message chain to choose the next receiver selectively. Empirically, TEM outperforms the baselines on multiple cooperative MARL benchma
    
[^102]: BCI中的信息传输速率：走向紧密集成的共生关系

    Information Transfer Rate in BCIs: Towards Tightly Integrated Symbiosis. (arXiv:2301.00488v3 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2301.00488](http://arxiv.org/abs/2301.00488)

    本研究重新定义了信息传输速率（ITR），通过将寄宿于视网膜神经交叉处的共生通信媒介建模为离散无记忆信道，并使用修改后的容量表达式进行计算。这将有助于未来BCI设计的端到端优化。

    

    信息传输速率（ITR）或有效比特率是一种流行而广泛使用的信息度量标准，特别是在基于SSVEP的脑机接口（BCI）中流行。通过将速度和准确性结合为单一值参数，该指标有助于评估和比较不同BCI社区中的各种目标识别算法。为了计算ITR，通常假定输入分布是一致的，并且信道模型过于简单，是无记忆的、平稳的、对称的，并具有离散的字母表大小。为了准确地描述性能并激发面向未来的BCI设计的端到端设计，因此需要更全面地审查和定义ITR。我们将寄宿于视网膜神经交叉处的共生通信媒介建模为离散无记忆信道，并使用修改后的容量表达式重新定义ITR。我们利用有向图的结果来表征通道的可达性和通信容量。

    The information transmission rate (ITR), or effective bit rate, is a popular and widely used information measurement metric, particularly popularized for SSVEP-based Brain-Computer (BCI) interfaces. By combining speed and accuracy into a single-valued parameter, this metric aids in the evaluation and comparison of various target identification algorithms across different BCI communities. In order to calculate ITR, it is customary to assume a uniform input distribution and an oversimplified channel model that is memoryless, stationary, and symmetrical in nature with discrete alphabet sizes. To accurately depict performance and inspire an end-to-end design for futuristic BCI designs, a more thorough examination and definition of ITR is therefore required. We model the symbiotic communication medium, hosted by the retinogeniculate visual pathway, as a discrete memoryless channel and use the modified capacity expressions to redefine the ITR. We leverage a result for directed graphs to char
    
[^103]: 区块链上的AI伦理: 基于Twitter数据的区块链安全主题分析

    AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security. (arXiv:2212.06951v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2212.06951](http://arxiv.org/abs/2212.06951)

    本研究分析了Twitter上与MEV相关的话题，结果表明推文讨论了深刻的伦理问题，包括安全、公平、情感情绪和对MEV解决方案的渴望。

    

    区块链使用分布式网络让计算机系统更安全，但当前的区块链设计在交易顺序方面存在公平性问题。矿工可以重新排序交易以生成利润，这被称为矿工可提取价值（MEV）问题。现有研究认为MEV是一个严重的安全问题，并提出了潜在的解决方案，包括知名的Flashbots。然而，以往的研究大多分析了区块链数据，这可能无法捕捉到MEV在更广泛的AI社会中的影响。因此，在这项研究中，我们应用自然语言处理（NLP）方法全面分析了MEV推文中的话题。我们收集了超过20,000个MEV和Flashbots标签的推文并分析了它们的话题。我们的结果显示，这些推文讨论了深刻的伦理问题，包括安全、公平、情感情绪和对MEV解决方案的渴望。我们还发现了区块链上MEV活动的共同运动。

    Blockchain has empowered computer systems to be more secure using a distributed network. However, the current blockchain design suffers from fairness issues in transaction ordering. Miners are able to reorder transactions to generate profits, the so-called miner extractable value (MEV). Existing research recognizes MEV as a severe security issue and proposes potential solutions, including prominent Flashbots. However, previous studies have mostly analyzed blockchain data, which might not capture the impacts of MEV in a much broader AI society. Thus, in this research, we applied natural language processing (NLP) methods to comprehensively analyze topics in tweets on MEV. We collected more than 20000 tweets with MEV and Flashbots hashtags and analyzed their topics. Our results show that the tweets discussed profound topics of ethical concern, including security, equity, emotional sentiments, and the desire for solutions to MEV. We also identify the co-movements of MEV activities on block
    
[^104]: 风格转移用于因果推断，实现领域外泛化

    Causal Inference via Style Transfer for Out-of-distribution Generalisation. (arXiv:2212.03063v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.03063](http://arxiv.org/abs/2212.03063)

    本文提出一种基于风格转移的方法，使用前门调整来解决因果推断中存在的混淆因素，实现领域外泛化。

    

    领域外泛化旨在构建一个模型，该模型可以使用多个源领域的知识，在未见目标领域上实现良好的泛化。为此，模型应寻求输入和标签之间因果依赖性，该依赖性可能由输入的语义决定，并在多个领域中保持不变。然而，统计或非因果方法通常无法捕捉到这种依赖性，并因未考虑由未观测到的混淆因素学习的表观相关性而表现不佳。现有的众所周知的因果推断方法，如后门调整，无法应用于消除表观相关性，因为它需要观察混淆因素。在本文中，我们提出了一种新方法，该方法通过成功实现前门调整（FA）有效地处理隐藏的混淆因素。FA需要选择一个中介人，我们认为这个中介人是图像的语义信息，它有助于不需要观察混淆因素，就能访问因果机制。

    Out-of-distribution (OOD) generalisation aims to build a model that can generalise well on an unseen target domain using knowledge from multiple source domains. To this end, the model should seek the causal dependence between inputs and labels, which may be determined by the semantics of inputs and remain invariant across domains. However, statistical or non-causal methods often cannot capture this dependence and perform poorly due to not considering spurious correlations learnt from model training via unobserved confounders. A well-known existing causal inference method like back-door adjustment cannot be applied to remove spurious correlations as it requires the observation of confounders. In this paper, we propose a novel method that effectively deals with hidden confounders by successfully implementing front-door adjustment (FA). FA requires the choice of a mediator, which we regard as the semantic information of images that helps access the causal mechanism without the need for ob
    
[^105]: 用于未知物体实例分割的均值漂移掩模变换器

    Mean Shift Mask Transformer for Unseen Object Instance Segmentation. (arXiv:2211.11679v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11679](http://arxiv.org/abs/2211.11679)

    本文提出了一种新的均值漂移掩模变换器，用于联合训练和推断特征提取器和聚类器，可用于未知物体实例分割，在COCO数据集上表现出竞争性的性能，并在罕见和未知物体类别上具有显着优势。

    

    物体实例的分割是机器人需要掌握的关键感知技能之一，它有助于机器人抓取和操作未知物体。均值漂移聚类是一种广泛用于图像分割任务的方法。然而，传统的均值漂移聚类算法不可微分，使其难以集成到端到端的神经网络训练框架中。在本文中，我们提出了均值漂移掩模变换器（MSMFormer），这是一种新的变换器体系结构，模拟 von Mises-Fisher（vMF）均值漂移聚类算法，允许联合训练和推断特征提取器和聚类器。其核心组件是超球面注意力机制，可在超球面上更新物体查询。为了说明我们方法的有效性，我们将MSMFormer应用于未知物体实例分割。实验结果表明，MSMFormer在COCO数据集上与现有方法相比取得了竞争性的性能，并且在罕见和未知物体类别上具有显着优势。

    Segmenting unseen objects from images is a critical perception skill that a robot needs to acquire. In robot manipulation, it can facilitate a robot to grasp and manipulate unseen objects. Mean shift clustering is a widely used method for image segmentation tasks. However, the traditional mean shift clustering algorithm is not differentiable, making it difficult to integrate it into an end-to-end neural network training framework. In this work, we propose the Mean Shift Mask Transformer (MSMFormer), a new transformer architecture that simulates the von Mises-Fisher (vMF) mean shift clustering algorithm, allowing for the joint training and inference of both the feature extractor and the clustering. Its central component is a hypersphere attention mechanism, which updates object queries on a hypersphere. To illustrate the effectiveness of our method, we apply MSMFormer to unseen object instance segmentation. Our experiments show that MSMFormer achieves competitive performance compared to
    
[^106]: 基于粒子滤波器的信息增益主导的主动探索用于高效的空间概念形成

    Active Exploration based on Information Gain by Particle Filter for Efficient Spatial Concept Formation. (arXiv:2211.10934v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.10934](http://arxiv.org/abs/2211.10934)

    该论文提出了一种基于信息增益主导的主动探索方法，通过粒子滤波器实现顺序贝叶斯推理和目标确定，能够帮助自主机器人在形成和学习空间概念的过程中高效地探索环境和与用户交互。

    

    自主机器人需要通过探索环境并与用户交互来学习各种地方的类别。然而，从用户那里准备语言指导的训练数据集是耗时且劳动密集的。此外，有效的探索对于适当的概念形成和快速环境覆盖至关重要。为了解决这个问题，我们提出了一种主动推理方法，称为基于信息增益的主动探索空间概念形成（SpCoAE），将粒子滤波器的顺序贝叶斯推理和基于信息增益的目标确定与概率生成模型相结合。该研究将机器人的动作解释为在主动推理的环境下选择要问用户“这是什么地方？”的目的地。该研究提供了有关所提出的方法的技术方面，包括机器人的积极感知和探索，以及该方法如何使移动机器人能够高效地形成和学习空间概念。

    Autonomous robots need to learn the categories of various places by exploring their environments and interacting with users. However, preparing training datasets with linguistic instructions from users is time-consuming and labor-intensive. Moreover, effective exploration is essential for appropriate concept formation and rapid environmental coverage. To address this issue, we propose an active inference method, referred to as spatial concept formation with information gain-based active exploration (SpCoAE) that combines sequential Bayesian inference using particle filters and information gain-based destination determination in a probabilistic generative model. This study interprets the robot's action as a selection of destinations to ask the user, `What kind of place is this?' in the context of active inference. This study provides insights into the technical aspects of the proposed method, including active perception and exploration by the robot, and how the method can enable mobile 
    
[^107]: 混合池化在基于Mixup的图形学习中的有效性研究

    On the Effectiveness of Hybrid Pooling in Mixup-Based Graph Learning for Language Processing. (arXiv:2210.03123v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03123](http://arxiv.org/abs/2210.03123)

    本文探讨了图池化算子对于Manifold-Mixup方法在图形学习中的影响，提出了一种新颖的混合池化架构，相比现有方法在文本分类任务上表现显著优越并达到了最先进性能水平。

    

    基于图神经网络（GNN）的图形学习在自然语言和编程语言处理方面越来越受欢迎，特别是在文本和源代码分类方面。通常，GNN是由交替图层和图池化层构成的，交替图层可以学习图节点特征的转换，而图池化层则使用图池化算子（例如Max池化）有效地减少节点数量，同时保留图的语义信息。最近，为了增强GNN在图形学习任务中的性能，人们广泛采用了Manifold-Mixup这种数据增强技术，该技术通过线性混合一对图数据和它们的标签来生成合成图数据。然而，Manifold-Mixup的性能很大程度上受到图池化算子的影响，而且并没有进行很多关于这种影响的研究。为了填补这一空白，我们早期探索了图池化算子如何影响基于Mixup的图形学习的性能。具体而言，我们提出了一种新颖的混合池化架构，结合了Max-pooling和Attention-pooling，以更好地捕捉本地和全局的图结构信息。我们在文本分类任务上的实验表明，所提出的混合池化结构显著优于现有的池化方法，并达到了最先进的性能水平。

    Graph neural network (GNN)-based graph learning has been popular in natural language and programming language processing, particularly in text and source code classification. Typically, GNNs are constructed by incorporating alternating layers which learn transformations of graph node features, along with graph pooling layers that use graph pooling operators (e.g., Max-pooling) to effectively reduce the number of nodes while preserving the semantic information of the graph. Recently, to enhance GNNs in graph learning tasks, Manifold-Mixup, a data augmentation technique that produces synthetic graph data by linearly mixing a pair of graph data and their labels, has been widely adopted. However, the performance of Manifold-Mixup can be highly affected by graph pooling operators, and there have not been many studies that are dedicated to uncovering such affection. To bridge this gap, we take an early step to explore how graph pooling operators affect the performance of Mixup-based graph le
    
[^108]: 安全贝叶斯优化的元学习先验

    Meta-Learning Priors for Safe Bayesian Optimization. (arXiv:2210.00762v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00762](http://arxiv.org/abs/2210.00762)

    本文提出了一种数据驱动方法，通过元学习先验从离线数据中实现安全的贝叶斯优化，同时开发一种新的框架以数据驱动的方式选择符合安全要求的先验，结果表明，相比于基准方法，元学习先验加快了安全贝叶斯优化的收敛速度并改进了整体性能。

    

    在机器人学中，优化控制器参数并满足安全约束是一个重要的挑战。安全贝叶斯优化通过量化目标和约束中的不确定性来安全地指导探索。然而，在存在未知安全约束的情况下，选择可靠的模型超参数以避免安全违规至关重要，但人工设计适合的概率模型可能很具有挑战性。本文提出了一种数据驱动的方法，通过元学习先验从离线数据中实现安全的贝叶斯优化。我们借助元学习算法 F-PACOH，在数据稀缺性的情况下提供可靠的不确定性量化。同时，在基准函数和高精度运动系统上，我们通过经验不确定度度量和前沿搜索算法开发了一种新的框架，以数据驱动的方式选择符合安全要求的先验。实验结果表明，相比于基准方法，我们的元学习先验加快了安全贝叶斯优化的收敛速度并改进了整体性能。

    In robotics, optimizing controller parameters under safety constraints is an important challenge. Safe Bayesian optimization (BO) quantifies uncertainty in the objective and constraints to safely guide exploration in such settings. Hand-designing a suitable probabilistic model can be challenging, however. In the presence of unknown safety constraints, it is crucial to choose reliable model hyper-parameters to avoid safety violations. Here, we propose a data-driven approach to this problem by meta-learning priors for safe BO from offline data. We build on a meta-learning algorithm, F-PACOH, capable of providing reliable uncertainty quantification in settings of data scarcity. As core contribution, we develop a novel framework for choosing safety-compliant priors in a data-riven manner via empirical uncertainty metrics and a frontier search algorithm. On benchmark functions and a high-precision motion system, we demonstrate that our meta-learned priors accelerate the convergence of safe 
    
[^109]: 面向高维可达性问题的形式化安全保障生成

    Generating Formal Safety Assurances for High-Dimensional Reachability. (arXiv:2209.12336v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.12336](http://arxiv.org/abs/2209.12336)

    提出了一个称为Safe DeepReach的框架，用于为DeepReach方法生成正式安全保障。该框架将新颖的Lipschitz连续性分析与区间边界传播方法相结合，以有效和可伸缩的方式保证解决方案的安全性。

    

    为自主系统提供正式的安全和性能保证变得日益重要。哈密顿-雅科比（HJ）可达性分析是一种流行的形式验证工具，用于提供这些保证，因为它可以处理一般非线性系统动态、有界对抗系统干扰以及状态和输入约束。但是，它涉及到求解PDE，其计算和内存复杂度随着状态维度的增加呈指数级增长，使其在大型系统上的直接使用变得不可行。最近提出的DeepReach方法通过利用正弦神经PDE求解器来克服了这一挑战，用于解决高维可达性问题，其计算要求随可达管复杂性而不是状态空间维度而变化。不幸的是，神经网络可能会出现错误，因此计算出的解决方案可能不安全，这没有达到我们提供正式安全保障的总体目标。为了解决这一挑战，我们提出了一个称为Safe DeepReach的框架，用于为DeepReach方法生成正式安全保障。我们的框架将新颖的Lipschitz连续性分析与区间边界传播方法相结合，以有效和可伸缩的方式保证解决方案的安全性。我们在几个基准示例上展示了我们提出的方法的有效性，包括基于感知的高维车道保持系统。

    Providing formal safety and performance guarantees for autonomous systems is becoming increasingly important. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for providing these guarantees, since it can handle general nonlinear system dynamics, bounded adversarial system disturbances, and state and input constraints. However, it involves solving a PDE, whose computational and memory complexity scales exponentially with respect to the state dimensionality, making its direct use on large-scale systems intractable. A recently proposed method called DeepReach overcomes this challenge by leveraging a sinusoidal neural PDE solver for high-dimensional reachability problems, whose computational requirements scale with the complexity of the underlying reachable tube rather than the state space dimension. Unfortunately, neural networks can make errors and thus the computed solution may not be safe, which falls short of achieving our overarching goal to provide fo
    
[^110]: 保证鲁棒性马尔可夫决策过程的一阶策略优化

    First-order Policy Optimization for Robust Markov Decision Process. (arXiv:2209.10579v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.10579](http://arxiv.org/abs/2209.10579)

    本论文介绍了一种用于解决鲁棒性马尔科夫决策过程的一阶方法，称为鲁棒策略镜像下降算法（RPMD）。通过使用线性递增步进，算法具有较低的复杂度，并且能够在不确定情况下找到最优策略。

    

    本文考虑解决鲁棒性马尔可夫决策过程（MDP）问题，包括一组具有不确定转移核的折扣、有限状态、有限动作空间的MDP。规划的目标是找到一个鲁棒策略来优化对转移不确定性的最坏情况值，因此包括标准MDP规划作为一种特殊情况。对于$(\mathbf{s},\mathbf{a})$-矩形不确定集，本文建立了鲁棒目标的几个结构性观察，从而便于开发基于策略的一阶方法，即鲁棒策略镜像下降算法（RPMD）。使用线性递增的步长，建立了找到$\epsilon$-最优策略的$\mathcal{O}(\log(1/\epsilon))$迭代复杂度。当一阶信息仅通过与名义环境的在线交互获得时，我们进一步开发了鲁棒策略镜像下降方法的随机变体，即SRPMD。我们证明了最优策略的取得方式与基于一阶信息的方法是等价的。

    We consider the problem of solving robust Markov decision process (MDP), which involves a set of discounted, finite state, finite action space MDPs with uncertain transition kernels. The goal of planning is to find a robust policy that optimizes the worst-case values against the transition uncertainties, and thus encompasses the standard MDP planning as a special case. For $(\mathbf{s},\mathbf{a})$-rectangular uncertainty sets, we establish several structural observations on the robust objective, which facilitates the development of a policy-based first-order method, namely the robust policy mirror descent (RPMD). An $\mathcal{O}(\log(1/\epsilon))$ iteration complexity for finding an $\epsilon$-optimal policy is established with linearly increasing stepsizes. We further develop a stochastic variant of the robust policy mirror descent method, named SRPMD, when the first-order information is only available through online interactions with the nominal environment. We show that the optimal
    
[^111]: 离散键值瓶颈

    Discrete Key-Value Bottleneck. (arXiv:2207.11240v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.11240](http://arxiv.org/abs/2207.11240)

    本文提出了一个新的神经网络模型结构，包含离散瓶颈，可以有效处理在多个任务之间进行连续学习的问题。

    

    深度神经网络在i.i.d.数据流和标注数据丰富的分类任务中表现良好，但对于连续学习等非平稳训练数据流会出现挑战。目前已有的一个有效方法是在大量可用数据上对编码器进行预训练，然后进行特定任务的微调。然而，对于新任务，更新这些编码器的权重是具有挑战性的，因为需要微调大量的权重，并且会忘记先前任务的信息。我们提出了一个模型架构来解决这个问题，建立在包含成对分离可学习键值代码的离散瓶颈的基础上。我们的范式是进行编码、通过离散瓶颈进行表示处理、解码。在这里，输入被馈送到预训练编码器，编码器的输出用于选择最近的键，并将相应的值馈送到正在执行的任务。

    Deep neural networks perform well on classification tasks where data streams are i.i.d. and labeled data is abundant. Challenges emerge with non-stationary training data streams such as continual learning. One powerful approach that has addressed this challenge involves pre-training of large encoders on volumes of readily available data, followed by task-specific tuning. Given a new task, however, updating the weights of these encoders is challenging as a large number of weights needs to be fine-tuned, and as a result, they forget information about the previous tasks. In the present work, we propose a model architecture to address this issue, building upon a discrete bottleneck containing pairs of separate and learnable key-value codes. Our paradigm will be to encode; process the representation via a discrete bottleneck; and decode. Here, the input is fed to the pre-trained encoder, the output of the encoder is used to select the nearest keys, and the corresponding values are fed to th
    
[^112]: 用于黑盒优化的生成预训练

    Generative Pretraining for Black-Box Optimization. (arXiv:2206.10786v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10786](http://arxiv.org/abs/2206.10786)

    该论文提出了一种使用离线数据预训练黑盒优化器的生成框架BONET，使用自回归模型和样本策略合成轨迹以帮助在高维空间中优化昂贵的黑盒函数。

    

    科学和工程中的许多问题涉及在高维空间中优化昂贵的黑盒函数。对于这样的黑盒优化 (BBO) 问题，我们通常假设在线函数评估的预算很小，但往往可以访问用于预训练的固定离线数据集。之前的方法试图利用离线数据来逼近函数或其反函数，但在离数据分布较远时不够精确。我们提出了BONET，这是一个利用离线数据集预训练黑盒优化器的生成框架。在BONET中，我们对来自离线数据集的定长轨迹训练一个自回归模型。我们设计了一种采样策略，使用从低保真度样本到高保真度样本的单调转换的简单启发式来合成来自离线数据的轨迹。在Design-Bench上使用被因果掩蔽的Transformer实例化BONET，并进行评估，我们在平均排名上排名第一。

    Many problems in science and engineering involve optimizing an expensive black-box function over a high-dimensional space. For such black-box optimization (BBO) problems, we typically assume a small budget for online function evaluations, but also often have access to a fixed, offline dataset for pretraining. Prior approaches seek to utilize the offline data to approximate the function or its inverse but are not sufficiently accurate far from the data distribution. We propose BONET, a generative framework for pretraining a novel black-box optimizer using offline datasets. In BONET, we train an autoregressive model on fixed-length trajectories derived from an offline dataset. We design a sampling strategy to synthesize trajectories from offline data using a simple heuristic of rolling out monotonic transitions from low-fidelity to high-fidelity samples. Empirically, we instantiate BONET using a causally masked Transformer and evaluate it on Design-Bench, where we rank the best on averag
    
[^113]: 超越模仿游戏：量化和拓展语言模型的能力

    Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. (arXiv:2206.04615v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.04615](http://arxiv.org/abs/2206.04615)

    本研究引入了Beyond the Imitation Game基准测试（BIG-bench），该测试集包含了204个各领域的难题，旨在评估当前语言模型的能力并为未来的研究提供信息和准备。

    

    随着规模的增大，语言模型展示了数量上的提升和新的定性能力。尽管具有潜在的转变性影响，但这些新的能力目前尚未被充分描述。为了为未来的研究提供信息，为剧变的新型模型能力做准备，并缓解社会有害影响，我们必须了解语言模型的现有和近期能力和限制。为了解决这一挑战，我们引入了Beyond the Imitation Game基准测试（BIG-bench）。BIG-bench目前包括204个任务，由132个机构的450名作者贡献。任务主题多样，涵盖了语言学、儿童发展、数学、常识推理、生物学、物理学、社会偏见、软件开发等等。BIG-bench专注于那些被认为超出了当前语言模型能力的任务。我们评估了OpenAI的GPT模型和谷歌内部的密集转换模型的行为。

    Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transform
    
[^114]: 跨视图语言建模：迈向统一的跨语言跨模态预训练

    Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training. (arXiv:2206.00621v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.00621](http://arxiv.org/abs/2206.00621)

    本文提出了跨视图语言建模框架，该框架将跨语言和跨模态预训练统一在共享的架构和目标下进行，通过有条件的掩码语言建模和对比学习来最大化不同视图之间的互信息以实现两个视图的对齐。

    

    本文引入了跨视图语言建模，这是一个简单而有效的预训练框架，将跨语言和跨模态预训练与共享架构和目标统一起来。我们的方法受到一个关键观察的启发，即跨语言和跨模态预训练具有将同一对象的两个不同视图对齐到一个共同语义空间的相同目标。为此，跨视图语言建模框架将多模态数据（即图像-标题对）和多语言数据（即平行句对）视为同一对象的两个不同视图，并通过有条件的掩码语言建模和对比学习来最大化它们之间的互信息来训练模型，以对齐两个视图。我们使用跨视图语言建模框架对跨语言跨模态语言模型CCLM进行预训练。在多语言多模态基准IGLUE和两个多语言图像-文本检索数据上进行了实证结果。

    In this paper, we introduce Cross-View Language Modeling, a simple and effective pre-training framework that unifies cross-lingual and cross-modal pre-training with shared architectures and objectives. Our approach is motivated by a key observation that cross-lingual and cross-modal pre-training share the same goal of aligning two different views of the same object into a common semantic space. To this end, the cross-view language modeling framework considers both multi-modal data (i.e., image-caption pairs) and multi-lingual data (i.e., parallel sentence pairs) as two different views of the same object, and trains the model to align the two views by maximizing the mutual information between them with conditional masked language modeling and contrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal Language Model, with the cross-view language modeling framework. Empirical results on IGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual image-text retrieval data
    
[^115]: QUIC-FL：面向联邦学习的快速无偏压缩

    QUIC-FL: Quick Unbiased Compression for Federated Learning. (arXiv:2205.13341v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13341](http://arxiv.org/abs/2205.13341)

    本文提出QUIC-FL方法，即面向联邦学习的快速无偏压缩，通过改进DME技术实现了最优归一化均方误差保证。

    

    分布式均值估计（DME）是通信高效的联邦学习中的基本构建块，在其中$n$个客户端向参数服务器通信向量，参数服务器估算其平均值。本文改进了先前的DME技术，实现了最优$O(1/n)$的归一化均方误差（NMSE）保证，通过渐进改进编码或解码（或两者）的复杂度。为了实现这一点，我们以一种新颖的方式形式化了问题，使我们能够使用现成的数学求解器来设计量化。

    Distributed Mean Estimation (DME), in which $n$ clients communicate vectors to a parameter server that estimates their average, is a fundamental building block in communication-efficient federated learning. In this paper, we improve on previous DME techniques that achieve the optimal $O(1/n)$ Normalized Mean Squared Error (NMSE) guarantee by asymptotically improving the complexity for either encoding or decoding (or both). To achieve this, we formalize the problem in a novel way that allows us to use off-the-shelf mathematical solvers to design the quantization.
    
[^116]: SE-MoE: 一种可扩展高效的分布式训练和推理混合专家系统

    SE-MoE: A Scalable and Efficient Mixture-of-Experts Distributed Training and Inference System. (arXiv:2205.10034v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2205.10034](http://arxiv.org/abs/2205.10034)

    SE-MoE提出了一种弹性的训练方式和不同优化措施，以提高混合专家模型的分布式训练和推理效率，解决了负载平衡、通信/计算效率和内存限制等问题。

    

    随着机器学习架构的多样性增加，将模型分布式训练在异构计算系统上以方便生成大模型成为了一种趋势。混合专家模型利用分治策略通过门控和并行处理方式来降低训练成本并控制总体模型/数据大小。虽然 DeepSpeed 在进行大规模 MoE 训练上进行了尝试，但训练和推理的效率仍有提升的空间，包括负载平衡、通信/计算效率和内存限制等方面。本工作提出了 SE-MoE，使用弹性 MoE 训练、基于层次存储的 2D 预取和融合通信等方法，以便在不同的类型中获得高效的并行处理。针对单节点的可扩展推理，特别是当模型大小大于 GPU 内存时，SE-MoE 将 CPU-GPU 存储联合形成一个更大的存储空间。

    With the increasing diversity of ML infrastructures nowadays, distributed training over heterogeneous computing systems is desired to facilitate the production of big models. Mixture-of-Experts (MoE) models have been proposed to lower the cost of training subject to the overall size of models/data through gating and parallelism in a divide-and-conquer fashion. While DeepSpeed has made efforts in carrying out large-scale MoE training over heterogeneous infrastructures, the efficiency of training and inference could be further improved from several system aspects, including load balancing, communication/computation efficiency, and memory footprint limits. In this work, we present SE-MoE that proposes Elastic MoE training with 2D prefetch and Fusion communication over Hierarchical storage, so as to enjoy efficient parallelisms in various types. For scalable inference in a single node, especially when the model size is larger than GPU memory, SE-MoE forms the CPU-GPU memory jointly into a 
    
[^117]: 无线网络中的语义信息恢复

    Semantic Information Recovery in Wireless Networks. (arXiv:2204.13366v4 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2204.13366](http://arxiv.org/abs/2204.13366)

    本文提出了一个基于机器学习的语义通信系统SINFONY，它通过对消息进行数据减少和可靠传输来最好地保留语义，从而实现无线网络中的语义信息恢复。

    

    受机器学习工具在无线通信中的成功启发，1949年Weaver提出的语义通信思想引起了人们的关注。 它打破了Shannon的经典设计范例，旨在传递消息的含义，即语义，而不是其确切版本，从而允许节省信息速率。 在这项工作中，我们将Basu等人的建模语义的基本方法扩展到完整通信马尔可夫链。 因此，我们通过隐含的随机变量来建模语义，并将语义通信任务定义为通过通信信道对消息进行数据减少和可靠传输，从而最好地保留语义。 我们将此任务作为端到端信息瓶颈问题进行建模，允许在保留相关信息的同时进行压缩。 作为解决方案，我们提出了基于ML的语义通信系统SINFONY，并将其用于分布式多点场景：SIN。

    Motivated by the recent success of Machine Learning (ML) tools in wireless communications, the idea of semantic communication by Weaver from 1949 has gained attention. It breaks with Shannon's classic design paradigm by aiming to transmit the meaning of a message, i.e., semantics, rather than its exact version and thus allows for savings in information rate. In this work, we extend the fundamental approach from Basu et al. for modeling semantics to the complete communications Markov chain. Thus, we model semantics by means of hidden random variables and define the semantic communication task as the data-reduced and reliable transmission of messages over a communication channel such that semantics is best preserved. We cast this task as an end-to-end Information Bottleneck problem, allowing for compression while preserving relevant information most. As a solution approach, we propose the ML-based semantic communication system SINFONY and use it for a distributed multipoint scenario: SIN
    
[^118]: 神经文本生成的最新进展：一项任务无关的调查

    Recent Advances in Neural Text Generation: A Task-Agnostic Survey. (arXiv:2203.03047v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.03047](http://arxiv.org/abs/2203.03047)

    本文调查了最近神经文本生成领域的最新进展，包括数据构建、神经框架、训练和推理策略和评估指标等四个方面，并探讨了神经管道和背景知识的利用等未来方向。

    

    近年来，相当多的研究致力于在自然语言生成（NLG）领域中应用神经模型。主要目标是生成既具有语言自然性又具有人类化属性的文本，并同时对生成过程进行控制。本文提供了一份全面的，任务无关的神经文本生成最新进展调查。这些进展通过多种发展得以实现，我们将其分为四个主要方面：数据构建，神经框架，训练和推理策略和评估指标。通过研究这些不同方面，我们旨在提供对该领域的进展的全面概述。此外，我们探讨了神经文本生成的未来方向，这些方向包括利用神经管道和融合背景知识，这些途径为进一步增强神经文本生成系统的能力提供了有希望的机会。

    In recent years, considerable research has been dedicated to the application of neural models in the field of natural language generation (NLG). The primary objective is to generate text that is both linguistically natural and human-like, while also exerting control over the generation process. This paper offers a comprehensive and task-agnostic survey of the recent advancements in neural text generation. These advancements have been facilitated through a multitude of developments, which we categorize into four key areas: data construction, neural frameworks, training and inference strategies, and evaluation metrics. By examining these different aspects, we aim to provide a holistic overview of the progress made in the field. Furthermore, we explore the future directions for the advancement of neural text generation, which encompass the utilization of neural pipelines and the incorporation of background knowledge. These avenues present promising opportunities to further enhance the cap
    
[^119]: 联盟的影响：评估大选中选民影响的可能性。

    The Impact of a Coalition: Assessing the Likelihood of Voter Influence in Large Elections. (arXiv:2202.06411v4 [econ.TH] UPDATED)

    [http://arxiv.org/abs/2202.06411](http://arxiv.org/abs/2202.06411)

    本文研究了在半随机模型中，联盟大小任意且可变情况下，进行操纵、胜利边缘和选票控制等影响大选的可能性。主要定理提供了选举结果受 $s$ 个联盟影响的上下界。

    

    几个世纪以来，人们普遍认为小联盟选民的影响在大选中微不足道。因此，有大量文献描述了特定分布下选举可能会被影响的可能性，特别是在 i.i.d. 均匀分布下一个选民进行操纵的可能性，称为公正文化（IC）。在本文中，我们从三个方面扩展了先前的研究：（1）我们提出了一种更一般的半随机模型，其中分布对手选择最坏情况分布，然后污染对手修改高达 $ \psi $ 数据的一部分，（2）我们考虑了许多联盟影响问题，包括联盟操纵，胜利边缘和各种选票控制和贿赂，（3）我们考虑任意和可变联盟规模 $B$。我们的主要定理提供了半随机可能性存在一组大小为 $s$ 的联盟对选举结果施加影响的紧密上下界。

    For centuries, it has been widely believed that the influence of a small coalition of voters is negligible in a large election. Consequently, there is a large body of literature on characterizing the likelihood for an election to be influenced when the votes follow certain distributions, especially the likelihood of being manipulable by a single voter under the i.i.d. uniform distribution, known as the Impartial Culture (IC).  In this paper, we extend previous studies in three aspects: (1) we propose a more general semi-random model, where a distribution adversary chooses a worst-case distribution and then a contamination adversary modifies up to $\psi$ portion of the data, (2) we consider many coalitional influence problems, including coalitional manipulation, margin of victory, and various vote controls and bribery, and (3) we consider arbitrary and variable coalition size $B$. Our main theorem provides asymptotically tight bounds on the semi-random likelihood of the existence of a s
    
[^120]: 人工智能安全中的重要性概念

    The Concept of Criticality in AI Safety. (arXiv:2201.04632v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2201.04632](http://arxiv.org/abs/2201.04632)

    本文提出了一种在AI安全领域中的重要性概念——关键操作，并提出了一种更加高效的解决方案，使操作员可以参与其他活动而不忽略监控任务。AI代理仅请求关键操作的许可，并用操作员的反馈使代理更加智能化。

    

    当AI代理没有与人类价值观保持一致时，他们可能会造成严重的伤害。解决价值观对齐问题的一种方法是包括一个人类操作员监控所有代理的活动。尽管这种解决方案保证了最大的安全性，但它非常低效，因为它要求人类操作员将所有注意力都专注于代理。在本文中，我们提出了一个更加高效的解决方案，使得操作员可以参与其他活动而不忽略监控任务。在我们的方法中，AI代理仅请求关键操作的许可，也就是潜在的有害操作。我们引入了与AI安全相关的关键操作概念，并讨论如何构建一个度量行动关键性的模型。我们还讨论了如何利用操作员的反馈使代理更加智能化。

    When AI agents don't align their actions with human values they may cause serious harm. One way to solve the value alignment problem is by including a human operator who monitors all of the agent's actions. Despite the fact, that this solution guarantees maximal safety, it is very inefficient, since it requires the human operator to dedicate all of his attention to the agent. In this paper, we propose a much more efficient solution that allows an operator to be engaged in other activities without neglecting his monitoring task. In our approach the AI agent requests permission from the operator only for critical actions, that is, potentially harmful actions. We introduce the concept of critical actions with respect to AI safety and discuss how to build a model that measures action criticality. We also discuss how the operator's feedback could be used to make the agent smarter.
    
[^121]: 模型为基础的安全强化学习在时间变化状态和控制约束下的应用：智能车辆中的应用

    Model-Based Safe Reinforcement Learning with Time-Varying State and Control Constraints: An Application to Intelligent Vehicles. (arXiv:2112.11217v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.11217](http://arxiv.org/abs/2112.11217)

    本文提出了一种安全RL算法，结合了基于屏障力的控制策略结构与多步策略评估机制，在保证控制安全的同时，能够应对时间变化的安全约束，并证明了其稳定性、鲁棒性和收敛性，优于几种最先进的RL算法。

    

    近年来，基于演员-评论家结构的安全强化学习（RL）在连续控制任务中受到越来越多的关注。学习一个具有安全性和收敛性保证的近似最优控制策略仍然具有挑战性。同时，很少有作品讨论了在时间变化的安全性约束下设计安全RL算法。本文提出了一种安全RL算法，用于具有时间变化状态和控制约束的非线性系统的最优控制。在所提出的方法中，我们构建了一种新颖的基于屏障力的控制策略结构，以保证控制安全。提出了一种多步策略评估机制，用于预测策略在时间变化的安全约束下的安全风险，并指导策略安全更新。已证明稳定性和鲁棒性的理论结果。同时，分析了演员评论家实现的收敛性。所提出的算法的性能在模拟的Sa中优于几种最先进的RL算法

    Recently, safe reinforcement learning (RL) with the actor-critic structure for continuous control tasks has received increasing attention. It is still challenging to learn a near-optimal control policy with safety and convergence guarantees. Also, few works have addressed the safe RL algorithm design under time-varying safety constraints. This paper proposes a safe RL algorithm for optimal control of nonlinear systems with time-varying state and control constraints. In the proposed approach, we construct a novel barrier force-based control policy structure to guarantee control safety. A multi-step policy evaluation mechanism is proposed to predict the policy's safety risk under time-varying safety constraints and guide the policy to update safely. Theoretical results on stability and robustness are proven. Also, the convergence of the actor-critic implementation is analyzed. The performance of the proposed algorithm outperforms several state-of-the-art RL algorithms in the simulated Sa
    
[^122]: 基于多行多跨度远程监督的表格加文本问题多实例学习

    Multi-Row, Multi-Span Distant Supervision For Table+Text Question. (arXiv:2112.07337v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2112.07337](http://arxiv.org/abs/2112.07337)

    这篇论文提出了一个名为MITQA的基于Transformer的TextTableQA系统，通过多实例学习以及远程监督方法，有效地解决了表格加文本问题的挑战。

    

    近年来，关于表格和链接文本的问答（TextTableQA）已经取得了重要的研究成果，因为表格通常与相关的文本嵌入在文档中。HybridQA和OTT-QA是两个最知名的TextTableQA数据集，其中的问题最好通过同时从表格单元和链接文本段落中获取信息来回答。这两个数据集的共同挑战是，训练实例如问题和答案，其中黄金答案可能不仅匹配跨越表格行的多个表格单元，而且还包括表格行及其相关文本范围内的多个文本跨度。我们提出了MITQA，这是一个基于Transformer的TextTableQA系统，专门设计用于通过多实例损失目标和谨慎的课程设计来应对这两个方面的远程监督。我们的实验表明，所提出的MRMS-DS方法显著提高了MITQA在HybridQA和OTT-QA数据集上的性能，证明了我们的方法在处理TextTableQA挑战方面的有效性。

    Question answering (QA) over tables and linked text, also called TextTableQA, has witnessed significant research in recent years, as tables are often found embedded in documents along with related text. HybridQA and OTT-QA are the two best-known TextTableQA datasets, with questions that are best answered by combining information from both table cells and linked text passages. A common challenge in both datasets, and TextTableQA in general, is that the training instances include just the question and answer, where the gold answer may match not only multiple table cells across table rows but also multiple text spans within the scope of a table row and its associated text. This leads to a noisy multi instance training regime. We present MITQA, a transformer-based TextTableQA system that is explicitly designed to cope with distant supervision along both these axes, through a multi-instance loss objective, together with careful curriculum design. Our experiments show that the proposed multi
    
[^123]: 经验引导的蒙特卡罗树搜索在反合成规划中的应用

    Retrosynthetic Planning with Experience-Guided Monte Carlo Tree Search. (arXiv:2112.06028v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2112.06028](http://arxiv.org/abs/2112.06028)

    本文提出了一种经验引导的蒙特卡罗树搜索算法用于反合成规划，能够显著提高搜索效率和有效性，在基准数据集上表现出色。

    

    在反合成规划中，使用简单的基础分子合成复杂分子的可能路径数量巨大，导致可能性呈指数级增长。即使是有经验的化学家也往往难以选择最有前途的转化路径。现有方法依赖于人工定义或机器训练的评分函数，这些函数的化学知识有限，或使用昂贵的估算方法来进行指导。本文提出了一种称为经验引导的蒙特卡罗树搜索算法（EG-MCTS）来解决这个问题。在搜索过程中，我们建立了一个经验引导网络，从合成经验中学习知识。在基准 USPTO 数据集上的实验证明，EG-MCTS 在效率和有效性方面都比现有的方法有显著改进。在与文献的比较实验中，我们计算机生成的路径大多与报道的路径匹配。设计用于真实药物化合物的路径展现了该算法的有效性。

    In retrosynthetic planning, the huge number of possible routes to synthesize a complex molecule using simple building blocks leads to a combinatorial explosion of possibilities. Even experienced chemists often have difficulty to select the most promising transformations. The current approaches rely on human-defined or machine-trained score functions which have limited chemical knowledge or use expensive estimation methods for guiding. Here we an propose experience-guided Monte Carlo tree search (EG-MCTS) to deal with this problem. Instead of rollout, we build an experience guidance network to learn knowledge from synthetic experiences during the search. Experiments on benchmark USPTO datasets show that, EG-MCTS gains significant improvement over state-of-the-art approaches both in efficiency and effectiveness. In a comparative experiment with the literature, our computer-generated routes mostly matched the reported routes. Routes designed for real drug compounds exhibit the effectivene
    
[^124]: 通过策略梯度算法的高效多目标神经架构搜索框架

    Efficient Multi-objective Neural Architecture Search Framework via Policy Gradient Algorithm. (arXiv:2111.03892v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.03892](http://arxiv.org/abs/2111.03892)

    本论文提出了TND-NAS框架，利用可微架构搜索框架和多目标NAS的兼容性，通过策略梯度算法实现高效多目标神经架构搜索。实验结果表明其优于现有方法。

    

    可微架构搜索已成为神经架构搜索领域的主流研究课题，相对于早期的EA-based和RL-based方法，其高效率备受青睐。然而，这些方法已不再能够自然地应对不可微参数，如能源和资源受限效率等。针对多目标NAS领域的研究旨在解决这个问题，但由于对每个候选架构进行唯一的优化，因此需要大量的计算资源。基于此，我们提出了TND-NAS，它具有不可微参数的兼容性和不同iable NAS框架中的高效性。实验结果表明，在几个基准数据集上，我们提出的TND-NAS在搜索效率和解决方案质量方面优于现有方法。

    Differentiable architecture search has gradually become the mainstream research topic in the field of Neural Architecture Search (NAS) for its high efficiency compared with the early NAS (EA-based, RL-based) methods. Recent differentiable NAS also aims at further improving the search performance and reducing the GPU-memory consumption. However, these methods are no longer naturally capable of tackling the non-differentiable objectives, e.g., energy, resource-constrained efficiency, and other metrics, let alone the multi-objective search demands. Researches in the multi-objective NAS field target this but requires vast computational resources cause of the sole optimization of each candidate architecture. In light of this discrepancy, we propose the TND-NAS, which is with the merits of the high efficiency in differentiable NAS framework and the compatibility among non-differentiable metrics in Multi-objective NAS. Under the differentiable NAS framework, with the continuous relaxation of 
    
[^125]: 高度模块化强化学习库中分布式架构的集成

    Integrating Distributed Architectures in Highly Modular RL Libraries. (arXiv:2007.02622v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2007.02622](http://arxiv.org/abs/2007.02622)

    该研究探讨了在本地和分布式执行级别上实现代理组合性的设计选择，通过独立的可重用组件允许在不同尺度上定义RL代理，并成功解决了新颖和复杂的环境，具有最先进的性能。

    

    推进强化学习（RL）需要具有足够灵活性的工具，以便轻松地原型化新方法，同时避免不切实际的实验周转时间。为了匹配第一个要求，最流行的RL库倡导高度模块化的代理组合性，这有助于实验和开发。为了在合理的时间范围内解决具有挑战性的环境，将RL扩展到大规模采样和计算资源已被证明是一种成功的策略。然而，这种能力迄今为止很难与模块化相结合。在这项工作中，我们探索了设计选择，以允许在本地和分布式执行级别上实现代理组合性。我们提出了一种通用方法，通过独立的可重用组件允许在不同尺度上定义RL代理。我们通过实验证明，我们的设计选择使我们能够复制经典基准测试，探索多个分布式架构，并解决新颖和复杂的环境，具有最先进的性能。

    Advancing reinforcement learning (RL) requires tools that are flexible enough to easily prototype new methods while avoiding impractically slow experimental turnaround times. To match the first requirement, the most popular RL libraries advocate for highly modular agent composability, which facilitates experimentation and development. To solve challenging environments within reasonable time frames, scaling RL to large sampling and computing resources has proved a successful strategy. However, this capability has been so far difficult to combine with modularity. In this work, we explore design choices to allow agent composability both at a local and distributed level of execution. We propose a versatile approach that allows the definition of RL agents at different scales through independent reusable components. We demonstrate experimentally that our design choices allow us to reproduce classical benchmarks, explore multiple distributed architectures, and solve novel and complex environm
    
[^126]: 概念层次结构的语义索引方法，统一表示，关系数据库系统和通用与基于案例的推理方法

    Method for the semantic indexing of concept hierarchies, uniform representation, use of relational database systems and generic and case-based reasoning. (arXiv:1910.01539v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/1910.01539](http://arxiv.org/abs/1910.01539)

    本文提出了一种语义索引方法，通过概念层次结构表示知识并将键分配给节点，使得概念与所有更具体的概念部分可统一，并且只允许添加语义正确的概念。此方法可以使用基于案例的推理和通用问题解决方法进行推理。

    

    本文提出了一种语义索引方法，并描述了其在知识表示领域的应用。语义索引的起点是通过概念层次结构表示的知识。其目标是将键分配给节点（概念），这些节点按层次顺序排列且语法和语义正确。使用索引算法，计算键，使得概念与所有更具体的概念部分可统一，并且只允许添加语义正确的概念。键表示术语关系。所述索引算法的正确性和完备性已被证明。描述了将经典关系数据库用于实例存储的方法。由于统一表示，可以使用基于案例的推理和通用问题解决方法进行推理。

    This paper presents a method for semantic indexing and describes its application in the field of knowledge representation. Starting point of the semantic indexing is the knowledge represented by concept hierarchies. The goal is to assign keys to nodes (concepts) that are hierarchically ordered and syntactically and semantically correct. With the indexing algorithm, keys are computed such that concepts are partially unifiable with all more specific concepts and only semantically correct concepts are allowed to be added. The keys represent terminological relationships. Correctness and completeness of the underlying indexing algorithm are proven. The use of classical relational databases for the storage of instances is described. Because of the uniform representation, inference can be done using case-based reasoning and generic problem solving methods.
    
[^127]: 运用数据挖掘改进最小延迟问题的启发式算法

    Improving a State-of-the-Art Heuristic for the Minimum Latency Problem with Data Mining. (arXiv:1908.10705v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1908.10705](http://arxiv.org/abs/1908.10705)

    这篇论文通过利用数据挖掘技术，改进了一种基于GRASP的最小延迟问题启发式算法，取得了较好的效果，匹配或优于解的质量，在大大缩短计算时间的同时，还成功地引入了88个新的解成本值。

    

    混合元启发式算法在运筹学中越来越流行。其中一种成功的方法是将贪心随机自适应搜索程序（GRASP）与数据挖掘技术相结合，利用高质量解中发现的频繁模式，在保证搜索范围的同时显著减少计算时间。本文利用数据挖掘技术改进了一个基于GRASP的最小延迟问题启发式算法，适用于两个问题变体。计算实验证明，数据挖掘方法能够在较大数量的实例上匹配或改善解的质量，同时大大减少运行时间。此外，本文还引入了88个新的解成本值。为了支持我们的结果，我们提供了统计显著性检验、挖掘模式的影响、等时间比较和时间到目标曲线的测试。

    Recently, hybrid metaheuristics have become a trend in operations research. A successful example combines the Greedy Randomized Adaptive Search Procedures (GRASP) and data mining techniques, where frequent patterns found in high-quality solutions can lead to an efficient exploration of the search space, along with a significant reduction of computational time. In this work, a GRASP-based state-of-the-art heuristic for the Minimum Latency Problem (MLP) is improved by means of data mining techniques for two MLP variants. Computational experiments showed that the approaches with data mining were able to match or improve the solution quality for a large number of instances, together with a substantial reduction of running time. In addition, 88 new cost values of solutions are introduced into the literature. To support our results, tests of statistical significance, impact of using mined patterns, equal time comparisons and time-to-target plots are provided.
    

