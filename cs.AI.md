# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation.](http://arxiv.org/abs/2305.19798) | 本文提出了一种基于不对称核奇异值分解的自注意力机制，即Primal-Attention，来优化注意力机制，提高注意力输出的投影方差。 |
| [^2] | [DeepMerge: Deep Learning-Based Region-Merging for Image Segmentation.](http://arxiv.org/abs/2305.19787) | 本文提出一种名为DeepMerge的基于深度学习的区域合并方法，用于处理高分辨率图像分割问题，并成功使用远程感知数据集进行了验证。 |
| [^3] | [A technique to jointly estimate depth and depth uncertainty for unmanned aerial vehicles.](http://arxiv.org/abs/2305.19780) | 本文介绍了一种针对无人机深度估计的技术，可以联合估计深度和不确定性，针对M4Depth产生的视差不确定性进行转换，实现了优于标准概率方法的效果。实验表明，在各种公共数据集上始终保持着优异的性能，且速度更快，值得推广。 |
| [^4] | [Reliable Off-Policy Learning for Dosage Combinations.](http://arxiv.org/abs/2305.19742) | 本文提出了一种用于剂量组合的新颖可靠的脱机学习方法，通过三个步骤实现：开发神经网络估计个性化的剂量-反应，估计倾向得分检测共享协变量-治疗空间中的重叠有限区域，然后基于梯度的学习算法找到最佳的个性化剂量组合。 |
| [^5] | [Knowledge Base Question Answering for Space Debris Queries.](http://arxiv.org/abs/2305.19734) | 本文介绍了一个基于知识库的系统，可以回答复杂自然语言的查询，支持工程师访问太空碎片环境的知识库中的信息。 |
| [^6] | [APPRAISER: DNN Fault Resilience Analysis Employing Approximation Errors.](http://arxiv.org/abs/2305.19733) | 提出了一种新的容错评估方法，称为APPRAISER，它采用近似误差进行DNN故障容忍性分析。APPRAISER提供了成千上万倍的评估速度提升，同时保持了分析的高准确性。 |
| [^7] | [A rule-general abductive learning by rough sets.](http://arxiv.org/abs/2305.19718) | 本文提出了一种基于粗糙集理论的规则通用逆推学习方法，通过转化目标概念和子概念为信息表，以更低的成本解决领域知识获取和规则的修正、减少、生成问题。 |
| [^8] | [Optimal Decision Trees for Separable Objectives: Pushing the Limits of Dynamic Programming.](http://arxiv.org/abs/2305.19706) | 本研究提出了一种通用的动态规划方法来优化任何组合的可分离目标和约束条件，这种方法在可扩展性方面比通用求解器表现得更好。 |
| [^9] | [End-to-end Training of Deep Boltzmann Machines by Unbiased Contrastive Divergence with Local Mode Initialization.](http://arxiv.org/abs/2305.19684) | 本研究提出一种基于Metropolis-Hastings耦合和局部模态初始化的方法，解决了深度玻尔兹曼机中的偏差梯度估计问题，使得DBMs可以端到端地训练，实验结果表明与其他深度生成模型相当的生成性能。 |
| [^10] | [Mask, Stitch, and Re-Sample: Enhancing Robustness and Generalizability in Anomaly Detection through Automatic Diffusion Models.](http://arxiv.org/abs/2305.19643) | AutoDDPM是一种通过自动扩散模型生成潜在异常的初始可能性地图，并将其与原始图像融合，通过联合噪声分布重新采样来增强扩散模型鲁棒性的方法，它在保留健康组织的情况下替换异常区域，明显超越了扩散模型的局限性。 |
| [^11] | [MSMix:An Interpolation-Based Text Data Augmentation Method Manifold Swap Mixup.](http://arxiv.org/abs/2305.19617) | 提出了一种基于插值的文本数据增强方法——流形交换Mixup(MSMix)，通过在网络的特定层部分替换隐藏特征来从两个不同的样本中获得更丰富的隐藏表示，并在三个中文意图识别数据集上实验证明了MSMix在完整样本和小样本配置下均表现出更好的性能。 |
| [^12] | [Medication Recommendation via Domain Knowledge Informed Deep Learning.](http://arxiv.org/abs/2305.19604) | 提出一种基于动态领域知识的药物推荐框架DKINet，将领域知识与患者临床表现相结合，此为首次实验。 |
| [^13] | [Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards.](http://arxiv.org/abs/2305.19599) | 本文提出了FineRewards，通过引入细粒度的语义奖励，即标题奖励和SAM奖励，来改进文本到图像扩散模型中文本和图像之间的对齐。 |
| [^14] | [Towards Semi-supervised Universal Graph Classification.](http://arxiv.org/abs/2305.19598) | 该论文提出了一种新型图神经网络框架UGNN， 解决了半监督普适图分类问题，通过估计未标记图的确定性解决了类别偏移，具有最新性能。 |
| [^15] | [What does the Failure to Reason with "Respectively" in Zero/Few-Shot Settings Tell Us about Language Models?.](http://arxiv.org/abs/2305.19597) | 本文研究发现，语言模型在零/少样本环境下难以理解“respectively”的各种读法，需要更长时间的训练和依赖常识推理，仍落后于人类。 |
| [^16] | [Integrated multi-operand optical neurons for scalable and hardware-efficient deep learning.](http://arxiv.org/abs/2305.19592) | 本文提出了一种使用定制的多操作数光学器件的可扩展和高效光点积引擎，即多操作数光学神经元（MOON），并在图像识别任务中证明了其实用性。在SVHN识别数据集上，该MOON实现了85.89％的测量精度，性能逊于128x128的基于MOMZI的PTCs。 |
| [^17] | [Traffic Prediction using Artificial Intelligence: Review of Recent Advances and Emerging Opportunities.](http://arxiv.org/abs/2305.19591) | 该论文综述了交通预测方法的发展，重点介绍了基于人工智能的交通预测方法在多元交通时间序列模型研究方面的进展和机遇。 |
| [^18] | [Active causal structure learning with advice.](http://arxiv.org/abs/2305.19588) | 本研究提出了带建议的主动因果结构学习问题，并设计了一个自适应搜索算法，可以从建议中受益，即使建议是任意糟糕的情况下，仍然具有最坏情况下的保证。 |
| [^19] | [Towards Omni-generalizable Neural Methods for Vehicle Routing Problems.](http://arxiv.org/abs/2305.19587) | 提出了一个通用meta-learning框架，使得模型可以在推理过程中快速适应新任务。通过在多个合成和基准数据集上的实验表明，该方法可以有效地解决大小和分布变化的车辆路径问题（VRP）。 |
| [^20] | [Causal Discovery with Latent Confounders Based on Higher-Order Cumulants.](http://arxiv.org/abs/2305.19582) | 本文提出了一种利用高阶累积量实现潜在混淆因素因果关系发现的新方法，这种方法能够使单潜在成分结构的OICA问题得到闭合形式解决方案，同时还提出了可测试的单潜在成分条件，通过迭代删除共享识别的潜在成分成功扩展了结果到多潜在成分结构。 |
| [^21] | [SVVAD: Personal Voice Activity Detection for Speaker Verification.](http://arxiv.org/abs/2305.19581) | 提出了一种基于说话人验证的语音活动检测（SVVAD）框架，可根据最具信息量的语音特征进行调整，利用无标签训练方法，完全避免了由于错误标记导致的SV性能下降。实验证明在其他说话者混合的条件下，SVVAD在平均等误差率（EER）方面显著优于基线。 |
| [^22] | [Replicability in Reinforcement Learning.](http://arxiv.org/abs/2305.19562) | 这篇论文研究了在强化学习中的可复制性，提出了可复制算法和松弛可复制算法，并给出了相应的时间和样本复杂度，这对于RL算法设计以及未来的可复制性研究具有影响。 |
| [^23] | [Look-Ahead Task Offloading for Multi-User Mobile Augmented Reality in Edge-Cloud Computing.](http://arxiv.org/abs/2305.19558) | 本论文提出了一种基于 MAR 服务的移动增强现实任务卸载方案，使用有向无环图建模任务相互依赖性，并采用前瞻式卸载方案，基于改进的蒙特卡罗树搜索来确保高效性和公平性，实验结果表明该方案可以显著减少任务延迟和能量消耗。 |
| [^24] | [Exploring Phonetic Context in Lip Movement for Authentic Talking Face Generation.](http://arxiv.org/abs/2305.19556) | 本研究探讨了语音上下文对真实说话人脸生成的影响，提出了一种Context-Aware Lip-Sync框架（CALS），可利用语音上下文生成更加准确、稳定的唇部运动。 |
| [^25] | [Spotlight Attention: Robust Object-Centric Learning With a Spatial Locality Prior.](http://arxiv.org/abs/2305.19550) | 该论文提出了一个新的目标中心学习方法，通过加入空间局部性先验来提高模型的鲁棒性，使模型在合成和真实数据上实现了显著的物体分割改进，并且对模型超参数不太敏感。 |
| [^26] | [Recasting Self-Attention with Holographic Reduced Representations.](http://arxiv.org/abs/2305.19534) | 本文提出了一种使用HRR的神经符号方法重新构建自注意力的方法，可以实现较低的时间和空间复杂度，并在LRA基准测试中获得了接近于最先进的准确度。 |
| [^27] | [Offline Meta Reinforcement Learning with In-Distribution Online Adaptation.](http://arxiv.org/abs/2305.19529) | 本文提出了一种带有不确定性量化的内部分布在线适应(IDAQ)的框架，利用策略后验集合和信念更新网络量化策略不确定性并生成上下文信息来处理新任务，在离线元强化学习上具有竞争性表现。 |
| [^28] | [Fine-grained Text Style Transfer with Diffusion-Based Language Models.](http://arxiv.org/abs/2305.19512) | 本文提出了一种基于扩散式语言模型的细粒度文本风格转换方法，在不依赖外部信息的情况下取得了比之前利用预训练权重、嵌入和外部语法分析器更好的效果，表明扩散概率模型在文本生成领域具有广泛的应用前景。 |
| [^29] | [Perception and Semantic Aware Regularization for Sequential Confidence Calibration.](http://arxiv.org/abs/2305.19498) | 本论文提出了一种顺序置信度校准的方法，即使用感知及语义感知正则化来调整过于自信的预测结果。该方法通过对与目标序列高度相关的令牌/序列进行分析，提高了深度顺序识别模型的性能。 |
| [^30] | [CVSNet: A Computer Implementation for Central Visual System of The Brain.](http://arxiv.org/abs/2305.19492) | 本文提出了一个人工神经网络CVSNet，可以看作是大脑中央视觉系统的计算机实现，其中每个块都表示与大脑中相同的视觉信息，通过三个独立的通道和五个不同的块流动。 |
| [^31] | [Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration.](http://arxiv.org/abs/2305.19476) | 本文提出了一种新的探索技术，使用值条件状态熵来解决强化学习中探索不足的问题，可以均衡地覆盖低价值和高价值状态，相较于现有基于熵的探索方法，该方法在MuJoCo基准测试和Atari游戏上有着显著的提升。 |
| [^32] | [Doubly Constrained Fair Clustering.](http://arxiv.org/abs/2305.19475) | 本论文关注公平聚类问题中的人口统计学公平概念，提出一种同时满足不同公平要求的快速算法。 |
| [^33] | [PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning.](http://arxiv.org/abs/2305.19472) | PlaSma提出了一种使用小型语言模型进行过程知识和计划能力的新方法， |
| [^34] | [Efficient Implementation of a Multi-Layer Gradient-Free Online-Trainable Spiking Neural Network on FPGA.](http://arxiv.org/abs/2305.19468) | 本文介绍了一种在FPGA上有效实现多层梯度自由在线可训练脉冲神经网络的方法，通过使用本地自适应选择阈值和适合硬件的修改重量更新规则，并实现了99.5％的高分类精度。 |
| [^35] | [The Impact of Positional Encoding on Length Generalization in Transformers.](http://arxiv.org/abs/2305.19466) | 本文通过实证研究 Transformer 模型中位置编码对于长度推广的影响，结果表明常用的位置编码方法并不适合用于下游任务的长度推广，并且使用位置编码甚至可能会损害长度推广的能力。 |
| [^36] | [Implementation of a framework for deploying AI inference engines in FPGAs.](http://arxiv.org/abs/2305.19455) | 该论文介绍了SLAC神经网络库(SNL)框架，这是一个基于FPGA的机器学习部署框架，优化延迟，旨在解决高速率探测器数据处理的挑战。 |
| [^37] | [Dynamic Sparsity Is Channel-Level Sparsity Learner.](http://arxiv.org/abs/2305.19454) | 本文提出了一种名为Channel-aware dynamic sparse (Chase)的方法，使用端到端训练实现了GPU友好的通道级别稀疏，不需要任何特殊操作，并且可以直接在通用硬件上加速，显著减小模型大小，同时保持性能。 |
| [^38] | [Best of Both Distortion Worlds.](http://arxiv.org/abs/2305.19453) | 本文研究了如何在只能访问序数偏好的情况下最大化基数偏好的方法，先前的工作在两个失真度中研究了功利主义失真和度量失真，提出了一个新的混合失真度量方法。 |
| [^39] | [Bigger, Better, Faster: Human-level Atari with human-level efficiency.](http://arxiv.org/abs/2305.19452) | 引入BBF基于价值函数的RL代理，在Atari 100K基准测试上实现超人类表现，具有人类效率，提出了在样本高效RL研究的ALE中更新目标的可能。 |
| [^40] | [A Computational Account Of Self-Supervised Visual Learning From Egocentric Object Play.](http://arxiv.org/abs/2305.19445) | 本文研究了如何利用等同不同视角的学习信号支持视觉学习，并发现通过等同一个对象的不同视角学习得到的表示可以提高下游图像分类精度。 |
| [^41] | [Data and Knowledge for Overtaking Scenarios in Autonomous Driving.](http://arxiv.org/abs/2305.19421) | 本文提出了一个新的超车场景数据集和知识表示模型，以帮助自动驾驶车辆进行规划和控制。 |
| [^42] | [Are Large Kernels Better Teachers than Transformers for ConvNets?.](http://arxiv.org/abs/2305.19412) | 本文揭示了新出现的大核ConvNets有效地用作小核ConvNets知识蒸馏（KD）的教师的优点。 |
| [^43] | [FRAMM: Fair Ranking with Missing Modalities for Clinical Trial Site Selection.](http://arxiv.org/abs/2305.19407) | 本文提出了一个深度强化学习框架FRAMM，用于公平的临床试验选址，该框架可以解决数据缺失和优化招募和多样性之间的权衡。在真实的临床试验数据集上，实验结果表明FRAMM能够实现公平的试验选址，并在不降低招募率的情况下提高多样性。 |
| [^44] | [Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI.](http://arxiv.org/abs/2305.19404) | 本文提出了一种“分歧感知”的双流增量学习框架，适用于针对不断演化的目标域数据进行分割任务，解决了分布变化、未见过结构和训练数据缺失等挑战。 |
| [^45] | [Contextual Vision Transformers for Robust Representation Learning.](http://arxiv.org/abs/2305.19402) | 上下文视觉变换器(ContextViT)用于生成图像的鲁棒特征表示，引入了一个额外的上下文令牌，可以解释掉特定于组的协变量结构，同时保持跨组共享的核心视觉特征，能够在监督微调、半监督学习以及主动学习等方面得到应用。 |
| [^46] | [Sensitivity Analysis of RF+clust for Leave-one-problem-out Performance Prediction.](http://arxiv.org/abs/2305.19375) | 本文提出了一种新的方法来解决机器学习中的LOPO问题，通过调整距离加权和引进基于特征的重要性，实验结果表明在预测准确性和推广能力方面比RF + clust更优。 |
| [^47] | [Compositional diversity in visual concept learning.](http://arxiv.org/abs/2305.19374) | 本文研究了人类如何利用组合性进行视觉概念学习，开发了一个程序归纳模型来生成候选视觉图形，发现人类和模型都可以进行多样的组合泛化。 |
| [^48] | [Mining Themes in Clinical Notes to Identify Phenotypes and to Predict Length of Stay in Patients admitted with Heart Failure.](http://arxiv.org/abs/2305.19373) | 本文采用主题建模技术对1200例心力衰竭患者的诊断编码和程序报告中存在的主题进行识别，旨在从中识别心力衰竭的临床表型并预测病人住院时间。 |
| [^49] | [Vision Transformers for Mobile Applications: A Short Survey.](http://arxiv.org/abs/2305.19365) | 本文调查了在权衡精度和推理延迟的前提下，哪些因素使得Vision Transformer适用于移动部署，并研究了专门设计为移动应用的ViT的体系结构和挑战。 |
| [^50] | [Stable Anisotropic Regularization.](http://arxiv.org/abs/2305.19358) | 本文提出了一种新颖的正则化方法I-STAR，可以增加模型的稳定性，提高性能，并改善自然语言处理中的组合表示问题。 |
| [^51] | [Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses.](http://arxiv.org/abs/2305.19339) |  |
| [^52] | [SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models.](http://arxiv.org/abs/2305.19308) | 本研究提出了一个基于大型语言模型（LLMs）的代理程序SheetCopilot，该程序可以通过自然语言指导软件执行电子表格数据处理等任务。该程序设计了一组抽象的电子表格软件功能原子动作以及基于状态机的任务规划框架，实现了LLMs与电子表格的鲁棒交互，可以单次正确完成44.3％的任务。 |
| [^53] | [A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks.](http://arxiv.org/abs/2305.19306) | 本文提出了SpikeGCL，一种用二值化1比特表示来提高效率和节约资源的图对比学习框架，实验结果表明可以以近32倍的表示存储压缩实现高效学习。 |
| [^54] | [MLOps: A Step Forward to Enterprise Machine Learning.](http://arxiv.org/abs/2305.19298) | 本文讨论了机器学习运维（MLOps）在企业级机器学习中的重要性和适用性，并详细解释了MLOps工作流程、自动化流程的不同成熟度水平以及各种底层技术。最后，用一个物体检测服务的企业级MLOps项目的详细示例来解释技术在现实场景中的工作流程。 |
| [^55] | [Perimeter Control Using Deep Reinforcement Learning: A Model-free Approach towards Homogeneous Flow Rate Optimization.](http://arxiv.org/abs/2305.19291) | 该论文提出了一种基于深度强化学习的模型无周界控制框架，通过交通信号控制代理与模拟车辆进行交互，实现同质流量优化，比传统的基于模型的方法更有效。 |
| [^56] | [Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup 2023.](http://arxiv.org/abs/2305.19283) | CYRUS足球模拟2D团队采用LSTM和DNN提出观察去噪思路，达成RoboCup 2021的冠军。 |
| [^57] | [A Telecare System for Use in Traditional Persian Medicine.](http://arxiv.org/abs/2305.19282) | 研究提出了一种基于传统波斯医学的远程护理系统，该系统可以利用记录的热分布、体质问卷和定制的脉搏测量设备，评估患者的体质状态并将结果发送给医生，从而减少对波斯医学专家的依赖。 |
| [^58] | [Large language models improve Alzheimer's disease diagnosis using multi-modality data.](http://arxiv.org/abs/2305.19280) | 本研究使用大型语言模型提高对非影像数据的应用能力，并在ADNI数据集上实现了SOTA结果。 |
| [^59] | [Enhancing Human Capabilities through Symbiotic Artificial Intelligence with Shared Sensory Experiences.](http://arxiv.org/abs/2305.19278) | 本文提出了一种共享感官体验的共生人工智能概念，旨在建立人工智能系统和人类用户之间的互惠关系，并个性化提供支持、帮助和增强，同时探讨了隐私、伦理准则和缓解潜在偏见和不平等现象的策略。 |
| [^60] | [Memory as a Mass-based Graph: Towards a Conceptual Framework for the Simulation Model of Human Memory in AI.](http://arxiv.org/abs/2305.19274) | 论文提出了一种以质量为基础的记忆图模型，用于 AI 中人类记忆的仿真。该模型可以清晰地区分记忆的地形差异，并提供了一个包含与观察事实相当一致的大脑活动模型。 |
| [^61] | [Grammar Prompting for Domain-Specific Language Generation with Large Language Models.](http://arxiv.org/abs/2305.19234) | 本文提出了一种基于语法提示的方法，使用专用的语法来增强示例，为大型语言模型（LLM）在特定领域的语言生成任务中使用外部知识和特定约束条件进行上下文学习。 |
| [^62] | [Controlled Text Generation with Hidden Representation Transformations.](http://arxiv.org/abs/2305.19230) | 我们提出了CHRT，它是一种可控语言生成框架，通过学习表示转换来修改基础模型的隐藏表示从而获得属性控制。实验证明，CHRT在三个属性上表现均优于所有基线模型，同时最小化了在语言质量上的损失。 |
| [^63] | [Solving Projected Model Counting by Utilizing Treewidth and its Limits.](http://arxiv.org/abs/2305.19212) | 本文提出一种用于解决投影模型计数问题的算法，该算法利用输入实例原始图的小树宽，算法运行时间为O(2^2k+4n2)，其中k是树宽，n是实例的输入大小，我们建立了PMC被限制树宽算法的下界，其运行时限界已达到渐进控制。 |
| [^64] | [StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation.](http://arxiv.org/abs/2305.19012) | 本文提出了一种新方法，利用图像-文本扩散模型进行数据生成和基于生成对抗网络（GAN）的3D生成网络进行训练，以生成高质量、风格化的3D头像，同时在生成过程中增加了现有3D模型中提取的姿势来引导多视角的图像生成，并提出了视点特定提示、粗到细的GAN鉴别器以及属性相关提示等方法以增加多样性。 |
| [^65] | [IDToolkit: A Toolkit for Benchmarking and Developing Inverse Design Algorithms in Nanophotonics.](http://arxiv.org/abs/2305.18978) | 提出了一个纳米光子学器件反向设计基准测试，以帮助人们进行易于理解和可重复的科学设计。开发了一个开源工具箱IDToolkit，其中包含了模拟和优化器模块以及后处理结果的函数，可用于与基准方法比较算法。 |
| [^66] | [HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance.](http://arxiv.org/abs/2305.18766) | 该论文提出了一种高保真度的文本到3D图像合成方法，并引入了先进的扩散引导策略。通过对NeRF渲染图像进行辅助深度监督和规范化密度场来提高3D几何表示。实验证明该方法优于以前的工作，产生了先进的照片真实感和改进的多视角一致性。 |
| [^67] | [Task-Equivariant Graph Few-shot Learning.](http://arxiv.org/abs/2305.18758) | 本文提出了一种任务等变图Few-shot学习（TEG）框架，利用图神经网络的等变性质来使模型学习可转移的任务适应策略。该方法在各种Few-shot分类基准上展示了最先进的性能。 |
| [^68] | [Hybrid Representation Learning via Epistemic Graph.](http://arxiv.org/abs/2305.18731) | 本论文提出了一种基于认知图的混合表示学习方法，通过将结构化知识与数据样本无缝集成来实现更有效的表示学习，并在多个基准数据集上取得了卓越的性能。 |
| [^69] | [Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models.](http://arxiv.org/abs/2305.18703) | 本文综述了大型语言模型的领域专门化，包括动机、挑战、方法论和评估指标。此外，还提供了一个特定领域任务和数据集的分类法，对现有的领域自适应和定制技术进行了详细比较，并广泛讨论了这一领域中的未解决问题和未来的发展方向。 |
| [^70] | [NUNO: A General Framework for Learning Parametric PDEs with Non-Uniform Data.](http://arxiv.org/abs/2305.18694) | NUNO 是一个用于处理非均匀数据的高效算子学习框架，在三维 PDE 问题中取得了显著的效果提升。 |
| [^71] | [Dink-Net: Neural Clustering on Large Graphs.](http://arxiv.org/abs/2305.18405) | Dink-Net是一个可扩展的大规模图形神经聚类方法，该方法利用了膨胀和收缩的思想来处理百万节点的大图，并在各种基准数据集上优于现有的最先进方法。 |
| [^72] | [Statistically Significant Concept-based Explanation of Image Classifiers via Model Knockoffs.](http://arxiv.org/abs/2305.18362) | 该研究提出了一种基于概念的解释模型敲门技术，可以在图像分类任务中找到显著的概念以避免误解，并控制误发现率（FDR）在某个值下，在合成和真实数据实验中得到验证。 |
| [^73] | [Practical PCG Through Large Language Models.](http://arxiv.org/abs/2305.18243) | 本研究介绍了如何利用语言模型生成游戏房间，在仅有少量数据的情况下，可以生成多达37%的可玩新颖关卡，该技术有助于解决包含许多局部和全局约束的PCG问题。 |
| [^74] | [Understanding Predictive Coding as an Adaptive Trust-Region Method.](http://arxiv.org/abs/2305.18188) | 研究将预测编码（PC）作为自适应信任区域（TR）算法的理论模型，并发现它可以比反向传播（BP）更快地逃脱鞍点。 |
| [^75] | [Modeling Dynamic Environments with Scene Graph Memory.](http://arxiv.org/abs/2305.17537) | 本论文提出了一种新的场景图记忆状态表示，结合节点边缘预测器（NEP）的神经网络架构，能够帮助具有行动能力的AI代理在部分可观察动态场景中高效搜索。 |
| [^76] | [The Curse of Recursion: Training on Generated Data Makes Models Forget.](http://arxiv.org/abs/2305.17493) | 使用生成数据进行训练会导致模型不可逆的缺陷并且使得原始内容分布的尾部消失，这种效应称为模型折叠。我们证明了这种现象在所有学习生成模型中都存在，必须认真对待。 |
| [^77] | [HUB: Guiding Learned Optimizers with Continuous Prompt Tuning.](http://arxiv.org/abs/2305.16823) | 本文提出了一种名为HUB的混合更新策略，通过结合学习优化器和手工设计的优化器，提高了学习优化器泛化性能。 |
| [^78] | [Do GPTs Produce Less Literal Translations?.](http://arxiv.org/abs/2305.16806) | 本研究比较了GPT和NMT生成翻译的文字积极度差异，发现GPT翻译更不准确，但在MT质量评估指标上表现出相似或更好的分数。 |
| [^79] | [AdaPlanner: Adaptive Planning from Feedback with Language Models.](http://arxiv.org/abs/2305.16653) | LLM代理可以通过Adaplanner自适应改进自己的计划以应对环境反馈，为此提出计划内外的改进策略以及代码风格的LLM提示结构和技能发现机制。 |
| [^80] | [Reverse Engineering Self-Supervised Learning.](http://arxiv.org/abs/2305.15614) | 本文逆向工程了自监督学习（SSL）训练表示，发现SSL训练过程中的正则化项本质上促进了样本基于语义标签的聚类。SSL训练的表示与语义类别更加接近，对齐在训练过程中增加，而且在网络深度加深时增加。 |
| [^81] | [Friendly Neighbors: Contextualized Sequence-to-Sequence Link Prediction.](http://arxiv.org/abs/2305.13059) | 研究提出了一种简单的序列到序列模型KGT5-context，通过加入查询实体的直接邻居信息实现知识图谱链接预测的高性能，并与其他方法相比取得了最先进的表现。 |
| [^82] | [HICO-DET-SG and V-COCO-SG: New Data Splits to Evaluate Systematic Generalization in Human-Object Interaction Detection.](http://arxiv.org/abs/2305.09948) | 本论文提出了两个新的HOI检测数据拆分，旨在评估系统性泛化。在新的数据拆分上测试结果表明，HOI检测模型对于未见过的对象和交互组合的泛化十分困难。 |
| [^83] | [Deep Learning Empowered Type-II Codebook: New Perspectives for Enhancing CSI Feedback.](http://arxiv.org/abs/2305.08081) | 本文提出了两个新视角来改进 Release 17 Type-II 码本中的 CSI 反馈性能：利用深度学习选择特定的角度-延迟域端口以提高精度，通过训练深度神经网络来实现可靠的反馈。实验结果表明，该方法可以显著提高反馈性能。 |
| [^84] | [Davinci the Dualist: the mind-body divide in large language models and in human learners.](http://arxiv.org/abs/2305.07667) | 本研究探究了大语言模型Davinci中的心身分离，发现其具有弱化的二元论倾向。 |
| [^85] | [ImageBind: One Embedding Space To Bind Them All.](http://arxiv.org/abs/2305.05665) | ImageBind是一种新的跨模态联合嵌入方法，只需要使用图像配对数据就可以将不同模态的数据绑定在一起，并实现跨模态检索、组合和生成等多种应用。 |
| [^86] | [Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling.](http://arxiv.org/abs/2305.04111) | 本文提出了EDGE，一种新的离散扩散模型用于生成大型图，并通过删除边来鼓励图的稀疏性。EDGE在每个去噪步骤中仅关注图中一部分节点，并且可以明确地对图的节点度数进行建模。实验表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。 |
| [^87] | [On the nonlinear correlation of ML performance between data subpopulations.](http://arxiv.org/abs/2305.02995) | 在不同数据子群体间，机器学习模型的内部准确性和外部准确性之间的相关性是非线性的，呈现出“月亮形”的相关性。 |
| [^88] | [Moccasin: Efficient Tensor Rematerialization for Neural Networks.](http://arxiv.org/abs/2304.14463) | 本文提出了一种名为Moccasin的新型约束编程形式，用于实现在内存预算下最小化计算图的执行时间，相较于最近的研究，该方法显著提高了效率，并成功应用于神经网络的高效张量重算。 |
| [^89] | [End-User Development for Artificial Intelligence: A Systematic Literature Review.](http://arxiv.org/abs/2304.09863) | 本文综述了终端用户开发（EUD）对人工智能系统的影响，目的是使非技术用户直接以满足其需求的方式参与到人工智能的定义和个性化中。此外，文章还评估了EUD面临的挑战、潜在的好处以及将其整合到整个人工智能开发中的未来影响。 |
| [^90] | [Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning.](http://arxiv.org/abs/2304.03916) | 本文提出了一种使用多模态对比损失函数的方法，通过在微调期间检测和明确区分受影响类别的错误属性，缓解多模态模型的错误相关性，同时提高模型精度和指向目标领域的有意义特征。 |
| [^91] | [cTBL: Augmenting Large Language Models for Conversational Tables.](http://arxiv.org/abs/2303.12024) | 本论文提出了一种称为cTBL的方法，可以从表格中检索信息，并生成具有检索信息支撑的对话响应，其中使用了转换器编码器嵌入进行浓密表检索，可以获得更好的性能。 |
| [^92] | [A Survey of Graph Prompting Methods: Techniques, Applications, and Challenges.](http://arxiv.org/abs/2303.07275) | 该论文从图的角度审查了提示方法，将提示函数与图知识相结合，以解决在复杂任务中设计提示的挑战，在此基础上组织现有工作，并描述了应用和未来挑战。 |
| [^93] | [Query-Utterance Attention with Joint modeling for Query-Focused Meeting Summarization.](http://arxiv.org/abs/2303.04487) | 本文提出了一种基于查询-话语注意力和联合建模的查询感知框架，它使用密集检索模块计算话语级别与查询的相关性，并将标记级别的查询关联性和话语级别的查询关联性结合起来，实现生成一个更与查询相关的摘要。经过对两个基准数据集上的测试，表明该方法优于现有的QFMS模型。 |
| [^94] | [Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?.](http://arxiv.org/abs/2303.04143) | 该论文提出了一个可以预测其他神经网络高质量ImageNet参数的神经网络，通过使用预测参数进行初始化，能够提高多种ImageNet模型的训练速度，并且在转移到其他数据集时可以更快地收敛并达到竞争力的最终性能。 |
| [^95] | [Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations.](http://arxiv.org/abs/2303.02536) | 本文提出了分布式对齐搜索（DAS）算法，可以在不使用暴力搜索的情况下找到高层因果模型和低层深度学习系统之间的对齐方法，并且DAS可以发现先前方法忽略的内部结构。DAS算法有潜力实现对复杂深度学习系统的更好解释和理解。 |
| [^96] | [Dropout Reduces Underfitting.](http://arxiv.org/abs/2303.01500) | 本研究证明dropout不仅可以防止神经网络过拟合，还可以缓解欠拟合问题。在训练初期采用early dropout方法，可以减少小批次梯度的方向差异，缓解SGD中的随机性，从而提高模型的训练效果。 |
| [^97] | [K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs.](http://arxiv.org/abs/2302.11996) | 本文提出了一种名为K-SHAP的算法，来解决多个智能体保持匿名且仅有状态-动作对的情况下学习智能体决策的问题。 |
| [^98] | [IB-RAR: Information Bottleneck as Regularizer for Adversarial Robustness.](http://arxiv.org/abs/2302.10896) | 本文提出了一种名为IB-RAR的正则化方法，利用信息瓶颈来增强对抗性训练和非对抗性训练方法的鲁棒性，并通过过滤不必要的特征来提高准确性。 |
| [^99] | [Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most.](http://arxiv.org/abs/2302.09195) | 该研究证明了在自监督学习中容易学习的样本对学习高质量表示起到最大的作用，这有助于减少所需的训练数据量，并提高性能。 |
| [^100] | [Cooperative Open-ended Learning Framework for Zero-shot Coordination.](http://arxiv.org/abs/2302.04831) | 该论文提出了一个COLE框架，通过构建合作游戏的开放式目标，从图论的角度评估和确定每个策略的协作能力，以有效地解决零样本协调中的合作不兼容性问题。 |
| [^101] | [Efficient Online Reinforcement Learning with Offline Data.](http://arxiv.org/abs/2302.02948) | 本文研究了利用离线数据进行高效在线强化学习的方法，证明了现有的离线策略方法能够应用于在线学习，提出了一些最少但重要的更改，来实现可靠的性能并提供了实践中可应用的建议。 |
| [^102] | [Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs.](http://arxiv.org/abs/2301.12950) | 本论文提出了一种名为HPRL的分层编程强化学习框架，通过学习程序组合的方法实现，能够产生具有人类可解释性并且在评估候选方案时可以准确奖励和惩罚的策略。 |
| [^103] | [Diffusion Models as Artists: Are we Closing the Gap between Humans and Machines?.](http://arxiv.org/abs/2301.11722) | 本文研究表明，扩散模型已经在拉近人与机器之间的鸿沟方面取得了进展，但机器生成作品的原创性和可识别性仍然存在差距。 |
| [^104] | [Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification.](http://arxiv.org/abs/2301.11562) | 在公平分类中，模型的预测方差是一个重要但鲜为人知的误差来源问题。作者提出了一个自洽性标准来衡量测量和减少随意性。作者还开发了一个算法来处理随意性预测，并通过实证研究揭示了当前模型无法处理某些类型数据的问题。 |
| [^105] | [SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning.](http://arxiv.org/abs/2301.11520) | 本文提出了一种称为SNeRL的语义感知神经辐射场，它通过学习3D-aware的隐式表示来进行强化学习，并在基于像素的以及最新的3D感知表示方法中表现出更好的性能。 |
| [^106] | [Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt Learning for Automatic Scoring in Science Education.](http://arxiv.org/abs/2301.08771) | 本研究提出了一种零样本学习自动评分的方法，利用预训练的语言模型配合匹配标本作为下一句预测技术，成功应用于科学教育领域的论证任务，极大地减少了训练成本和时间。 |
| [^107] | [PEAK: Explainable Privacy Assistant through Automated Knowledge Extraction.](http://arxiv.org/abs/2301.02079) | 本文提出了隐私助手PEAK，通过自动化知识提取和生成解释，帮助用户理解其隐私建议。在真实数据集的用户研究中，用户认为生成的解释有用且易于理解。 |
| [^108] | [Continual Contrastive Finetuning Improves Low-Resource Relation Extraction.](http://arxiv.org/abs/2212.10823) | 本文提出了一种使用连续对比微调的方法来改进低资源关系提取，通过使用一致的对比学习目标预训练和微调RE模型，以及多中心对比损失来允许一个关系形成多个聚类。实验结果表明该方法可以显着提高低资源情况和领域中的关系提取性能。 |
| [^109] | [Synthetic Pre-Training Tasks for Neural Machine Translation.](http://arxiv.org/abs/2212.09864) | 本文提出了一种使用合成任务和数据预训练神经机器翻译模型的方法，其可以缓解大规模抓取的语料库所导致的毒性、偏见和法律隐患，并证明了即使采用高度混淆或纯合成数据，预训练依然有效。 |
| [^110] | [Transformers learn in-context by gradient descent.](http://arxiv.org/abs/2212.07677) | 本文提出，训练Transformer模型应用于自回归目标问题时，与基于梯度的元学习的形式密切相关，通过梯度下降学习模型的“底层优化程序”的机制，在回归问题的领域中从机械的角度理解了Transformers模型中上下文学习的内部机制。 |
| [^111] | [Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator.](http://arxiv.org/abs/2212.06751) | 本文提出了一种基于任务相似度元学习的方法来加速树形结构Parzen估计中的多目标非分层超参数最优化，实现了最先进的性能。 |
| [^112] | [The Stable Artist: Steering Semantics in Diffusion Latent Space.](http://arxiv.org/abs/2212.06013) | 本文介绍了一种名为"稳定的美术家"的图像编辑方法，其中包含SEGA以及潜在遍历等组成部分，使得用户可以实现在图像生成过程中的细粒度控制，可以微妙地编辑图像、改变构图和风格，达到艺术构思优化等目的，实现了在各种文本到图像合成任务中的最先进的定量和定性结果。 |
| [^113] | [Elixir: Train a Large Language Model on a Small GPU Cluster.](http://arxiv.org/abs/2212.05339) | Elixir 提出了一种基于预运行模型分析的自动化高效大模型训练方案，可以将内存使用卸载到 CPU 和 NVMe 存储器中，充分发挥硬件的潜力，实验中表现优于最先进的基准模型。 |
| [^114] | [On the Power of Foundation Models.](http://arxiv.org/abs/2211.16327) | 本文通过范畴论探究了基础模型的能力，提出了具有最小所需能力的基础模型可以通过微调和足够的资源来解决前置任务所定义的类别中的下游任务，并且这种能力可以扩展到任何下游任务，只要允许微调且下游任务可在前置任务定义的范畴中表示。 |
| [^115] | [PuzzleFusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving.](http://arxiv.org/abs/2211.13785) | 本文提出了一个名为"PuzzleFusion"的神经架构，基于扩散模型，用于解决空间拼图和房间布局任务。在实验中，他们发现简单使用扩散模型可以有效地解决这些具有挑战性的空间拼图任务。 |
| [^116] | [Unifying Label-inputted Graph Neural Networks with Deep Equilibrium Models.](http://arxiv.org/abs/2211.10629) | 本文将标签输入的GNN和隐式GNN统一起来，并提出了一种IGNN中的隐式微分方法，使得标签无限传播变得可行。 |
| [^117] | [Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory.](http://arxiv.org/abs/2211.10586) | 本文提出了一种利用恒定内存需求扩展数据集精简的方法，可将Matching Training Trajectories（MTT）应用于ImageNet-1K数据集，达到6倍的内存降低，同时增加了约2%的运行时开销。同时也发现，为合成图像分配软标签对于实现良好的性能非常重要。 |
| [^118] | [MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets.](http://arxiv.org/abs/2211.07321) | 本文提出了一个新的自监督多任务学习框架MT4SSL，通过同时使用K均值算法作为离线目标提取器和没有梯度的教师网络作为在线目标提取器，取得了比以前更好的表现。同时，使用离线和在线目标提取器可以得到更好的收敛性，我们认为这是自监督语音模型上的多任务学习有前途的趋势。 |
| [^119] | [Effective Audio Classification Network Based on Paired Inverse Pyramid Structure and Dense MLP Block.](http://arxiv.org/abs/2211.02940) | 该论文通过提出基于轻量级音频的配对逆金字塔结构网络和密集多层感知机块网络，实现了在不进行数据增强或模型迁移情况下，对UrbanSound8K数据集和GTAZN数据集的高准确度分类任务。 |
| [^120] | [Forecasting Local Behavior of Self-organizing Many-agent System without Reconstruction.](http://arxiv.org/abs/2210.17289) | 本文提出了一种CNN-LSTM模型，可以在不需要重建所有代理状态的情况下，预测自组织多代理系统中特定代理的状态。所提出的模型在森林火灾模型中的实验中表现出更好的性能，可以提高自组织众智系统的效率和可扩展性。 |
| [^121] | [E-MCTS: Deep Exploration in Model-Based Reinforcement Learning by Planning with Epistemic Uncertainty.](http://arxiv.org/abs/2210.13455) | 本文提出了一种新的方法E-MCTS，通过在MCTS预测中应用表观不确定性估计，实现了模型基强化学习中的深度探索，以及规划探索策略。通过实验证明这种方法在成功的表观不确定性估计和深度探索方面表现优异。 |
| [^122] | [STAP: Sequencing Task-Agnostic Policies.](http://arxiv.org/abs/2210.12250) | STAP提出了一种可扩展框架，能够训练操作技能并在规划时协调它们的几何依赖关系，从而解决任何技能在训练期间都没有见过的长远任务，以此提升长远任务的成功率。 |
| [^123] | [RARR: Researching and Revising What Language Models Say, Using Language Models.](http://arxiv.org/abs/2210.08726) | RARR是一个可以对不确定信息进行研究和修订的系统，它可以自动找到文本生成模型输出的归因并修正不支持的内容。 |
| [^124] | [How to Sift Out a Clean Data Subset in the Presence of Data Poisoning?.](http://arxiv.org/abs/2210.06516) | 外部数据可能引入攻击者篡改的有毒数据，为了提高毒性防御性能，需要准确地从数据集中筛选出干净的子集。 |
| [^125] | [On the Forward Invariance of Neural ODEs.](http://arxiv.org/abs/2210.04763) | 该论文提出了一种利用控制障碍函数使神经ODE满足输出规范的方法，该方法保证了输出规范，且在训练和推理过程中可以通过改变受限参数/输入进行操作，此外，该方法还创造了额外的鲁棒性。 |
| [^126] | [Topological Singularity Detection at Multiple Scales.](http://arxiv.org/abs/2210.00069) | 本文提出了一种多尺度拓扑奇异性检测方法，可以评估数据的局部固有维度，并量化点的“流形度”，能够检测复杂空间和图像中的奇异性。 |
| [^127] | [Forecasting Evolution of Clusters in Game Agents with Hebbian Learning.](http://arxiv.org/abs/2209.06904) | 本文研究了如何通过聚类和预测模型来学习游戏智能体群集的演化，提出了一种基于 Hebbian 学习的无监督聚类方法，结合 LSTM 预测模型，能够在多个场景下准确预测群集演化。 |
| [^128] | [Adversarial Detection: Attacking Object Detection in Real Time.](http://arxiv.org/abs/2209.01962) | 本文首次提出了针对目标检测模型的实时在线攻击，这些攻击可以在所需要的位置制造不存在的物体，攻击成功率约为90\%，揭示了目标检测模型的弱点和安全性问题。 |
| [^129] | [ILLUME: Rationalizing Vision-Language Models through Human Interactions.](http://arxiv.org/abs/2208.08241) | 本文提出了一种新的调整范例，名为ILLUME，通过人机交互来合理化视觉-语言模型，从而使模型的输出更符合人的思维方式。在使用相对较少的训练数据和最少的人类反馈下，ILLUME表现出与标准监督微调相当的竞争力。 |
| [^130] | [OmniMAE: Single Model Masked Pretraining on Images and Videos.](http://arxiv.org/abs/2206.08356) | 该论文提出了一种基于遮蔽自编码的方法，可以在图像和视频上训练一个简单的单一Vision Transformer模型，而不需要标记数据，该模型的视觉表示可与单模态表示在基准测试上相当或更好，并且使用更简单的架构。 |
| [^131] | [Fair Classification via Domain Adaptation: A Dual Adversarial Learning Approach.](http://arxiv.org/abs/2206.03656) | 通过利用类似域的辅助信息，本论文提出了一种双重对抗学习方法，以实现没有敏感属性的目标域的公平分类。 |
| [^132] | [Floorplan Restoration by Structure Hallucinating Transformer Cascades.](http://arxiv.org/abs/2206.00645) | 该文介绍了一个新的极端楼层平面图重建任务和一个神经网络框架用于解决该任务，通过Transformer解码器级联的方式来幻化不可见的房间和门以重建整个楼层平面图，并在701个房屋的基准测试中表现出较好的效果。 |
| [^133] | [Personalized Algorithmic Recourse with Preference Elicitation.](http://arxiv.org/abs/2205.13743) | 研究提出了PEAR方法，这是一个首个能够针对最终用户需求提供个性化算法补救成本的人机交互方法。该方法利用贝叶斯偏好引导的见解，通过最大化原则性信息增益度量来计算目标用户选择的预期效用，然后将偏好引导整合到强化学习框架中。该方法显著提高了算法干预的经济实用性和用户友好性。 |
| [^134] | [RoMFAC: A robust mean-field actor-critic reinforcement learning against adversarial perturbations on states.](http://arxiv.org/abs/2205.07229) | 本文提出了RoMFAC算法，通过新的训练目标和重复的正则化损失函数，使其对于异常状态干扰具有鲁棒性并获得出色性能表现。 |
| [^135] | [Enhanced Physics-Informed Neural Networks with Augmented Lagrangian Relaxation Method (AL-PINNs).](http://arxiv.org/abs/2205.01059) | 本文提出一种增广拉格朗日松弛方法(AL-PINNs)用于物理信息神经网络(PINNs)的训练，该方法通过自适应平衡每个损失组件，能够有效地解决非线性偏微分方程问题。 |
| [^136] | [Implications of Distance over Redistricting Maps: Central and Outlier Maps.](http://arxiv.org/abs/2203.00872) | 本文提出了一种可解释且可操作的选区划分图距离测量方法，并定义了一种“最典型”的中心图。这种方法可以帮助我们深入研究一系列约束条件下选区划分图的应用。 |
| [^137] | [Zero-Shot Machine Unlearning.](http://arxiv.org/abs/2201.05629) | 零样本机器遗忘是一个新兴的研究问题，允许从已经训练好的ML模型中删除数据。因为这些请求可能会涉及到无法访问的训练数据，因此需要新的解决方法。 |
| [^138] | [AmbiFC: Fact-Checking Ambiguous Claims with Evidence.](http://arxiv.org/abs/2104.00640) | 本研究提出了一个大规模的事实核查数据集AmbiFC，用于处理现实场景中的含糊性声明核查问题，通过细粒度的证据注释和分析，提出了一种适用于含糊性声明的软标签证据核查方法，并且在注释人员争议分析中发现了相关性。 |
| [^139] | [An Efficient Paradigm for Feasibility Guarantees in Legged Locomotion.](http://arxiv.org/abs/2011.07967) | 本文提出了一种高效的范式，可以设计可行的质心和身体轨迹，同时满足动态平衡、关节扭矩和运动限制。在模拟实验中验证了该方法的有效性。 |
| [^140] | [Learning Diverse Options via InfoMax Termination Critic.](http://arxiv.org/abs/2010.02756) | 本文提出了一种通过最大化状态和动作选项之间的互信息来学习选项的终止条件的方法，从而提高学习到的选项的多样性和可重用性，在实验中取得了显著的效果。 |

# 详细

[^1]: 基于原始表达的不对称核奇异值分解的自注意力机制

    Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation. (arXiv:2305.19798v1 [cs.LG])

    [http://arxiv.org/abs/2305.19798](http://arxiv.org/abs/2305.19798)

    本文提出了一种基于不对称核奇异值分解的自注意力机制，即Primal-Attention，来优化注意力机制，提高注意力输出的投影方差。

    

    近期，一系列工作将自注意力机制视为核机器，以此来理解和改进Transformers。然而，现有的方法只适用于对称核而不适用于不对称的自注意力，导致了理论和实际的差距。在本文中，我们提出了一种基于不对称核奇异值分解（KSVD）来表达和优化自注意力的新视角。通过不对称KSVD，我们得到了：i）自注意力的一种原始-对偶表达，其中优化目标被转化为最大化注意力输出中的投影方差；ii）一种新的注意力机制-Primal-Attention，通过KSVD的原始表达式避免了在对偶中显式计算核矩阵的问题；iii）通过KKT条件，我们证明了Primal-Attention的状态最小化问题的解与之前的对偶算法具有一致性。

    Recently, a new line of works has emerged to understand and improve self-attention in Transformers by treating it as a kernel machine. However, existing works apply the methods for symmetric kernels to the asymmetric self-attention, resulting in a nontrivial gap between the analytical understanding and numerical implementation. In this paper, we provide a new perspective to represent and optimize self-attention through asymmetric Kernel Singular Value Decomposition (KSVD), which is also motivated by the low-rank property of self-attention normally observed in deep layers. Through asymmetric KSVD, $i$) a primal-dual representation of self-attention is formulated, where the optimization objective is cast to maximize the projection variances in the attention outputs; $ii$) a novel attention mechanism, i.e., Primal-Attention, is proposed via the primal representation of KSVD, avoiding explicit computation of the kernel matrix in the dual; $iii$) with KKT conditions, we prove that the stati
    
[^2]: DeepMerge: 基于深度学习的区域合并方法用于图像分割

    DeepMerge: Deep Learning-Based Region-Merging for Image Segmentation. (arXiv:2305.19787v1 [cs.CV])

    [http://arxiv.org/abs/2305.19787](http://arxiv.org/abs/2305.19787)

    本文提出一种名为DeepMerge的基于深度学习的区域合并方法，用于处理高分辨率图像分割问题，并成功使用远程感知数据集进行了验证。

    

    在图像分析中，从高分辨率遥感图像中精确地分割大区域仍然是一个具有挑战性的问题。现有的有监督和无监督方法都受到对象尺寸巨大变化和尺度选择困难的影响，经常导致分割精度不佳。为了应对上述挑战，我们提出了一种基于深度学习的区域合并方法（DeepMerge），通过将Transformers、多级嵌入模块、基于段的特征嵌入模块和区域相邻图模型相结合来处理大的高分辨率图像分割。此外，我们提出了一种改进的二叉树采样方法，以生成多级输入，用于DeepMerge模型的输入。根据我们的最佳知识，提出的方法是第一个使用深度学习学习相邻段之间相似性进行区域合并的方法。我们使用远程感知数据集对所提出的DeepMerge方法进行验证。

    Accurate segmentation of large areas from very high spatial-resolution (VHR) remote sensing imagery remains a challenging issue in image analysis. Existing supervised and unsupervised methods both suffer from the large variance of object sizes and the difficulty in scale selection, which often result in poor segmentation accuracies. To address the above challenges, we propose a deep learning-based region-merging method (DeepMerge) to handle the segmentation in large VHR images by integrating a Transformer with a multi-level embedding module, a segment-based feature embedding module and a region-adjacency graph model. In addition, we propose a modified binary tree sampling method to generate multi-level inputs from initial segmentation results, serving as inputs for the DeepMerge model. To our best knowledge, the proposed method is the first to use deep learning to learn the similarity between adjacent segments for region-merging. The proposed DeepMerge method is validated using a remot
    
[^3]: 一种联合估计无人机深度和深度不确定性的技术

    A technique to jointly estimate depth and depth uncertainty for unmanned aerial vehicles. (arXiv:2305.19780v1 [cs.CV])

    [http://arxiv.org/abs/2305.19780](http://arxiv.org/abs/2305.19780)

    本文介绍了一种针对无人机深度估计的技术，可以联合估计深度和不确定性，针对M4Depth产生的视差不确定性进行转换，实现了优于标准概率方法的效果。实验表明，在各种公共数据集上始终保持着优异的性能，且速度更快，值得推广。

    

    自主车辆在轨迹规划或避障时需要可靠的深度估计方法，并且估计深度输出的质量至关重要。本文介绍了如何增强针对无人机应用设计的最先进深度估计方法M4Depth，以执行联合深度和不确定性估计。我们提出了一种解决方案，将M4Depth产生的视差不确定性估计转换为深度相关的不确定性估计，并证明其优于标准的概率方法。我们在各种公共数据集上的实验表明，我们的方法表现始终如一，甚至在零样本转移时也能保持优异性能。此外，我们的方法与现有的多视深度估计方法相比，在多视深度估计基准测试方面表现类似，但速度更快，可造成，是其他方法的2.5倍。代码可供使用。

    When used by autonomous vehicles for trajectory planning or obstacle avoidance, depth estimation methods need to be reliable. Therefore, estimating the quality of the depth outputs is critical. In this paper, we show how M4Depth, a state-of-the-art depth estimation method designed for unmanned aerial vehicle (UAV) applications, can be enhanced to perform joint depth and uncertainty estimation. For that, we present a solution to convert the uncertainty estimates related to parallax generated by M4Depth into uncertainty estimates related to depth, and show that it outperforms the standard probabilistic approach. Our experiments on various public datasets demonstrate that our method performs consistently, even in zero-shot transfer. Besides, our method offers a compelling value when compared to existing multi-view depth estimation methods as it performs similarly on a multi-view depth estimation benchmark despite being 2.5 times faster and causal, as opposed to other methods. The code of 
    
[^4]: 用于剂量组合的可靠脱机学习

    Reliable Off-Policy Learning for Dosage Combinations. (arXiv:2305.19742v1 [cs.LG])

    [http://arxiv.org/abs/2305.19742](http://arxiv.org/abs/2305.19742)

    本文提出了一种用于剂量组合的新颖可靠的脱机学习方法，通过三个步骤实现：开发神经网络估计个性化的剂量-反应，估计倾向得分检测共享协变量-治疗空间中的重叠有限区域，然后基于梯度的学习算法找到最佳的个性化剂量组合。

    

    个性化医学领域的决策制定，如癌症治疗或危重护理，通常必须对剂量组合进行选择，即多种连续治疗。现有的这项任务的工作已经独立地建模了多种治疗的效果，而估计联合效果却受到了很少的关注，并且面临着非平凡的挑战。在本文中，我们提出了一种新颖的方法，用于剂量组合的可靠脱机学习。我们的方法分为三个步骤：（1）我们开发了一个特定的神经网络，估计个性化的剂量-反应函数，同时考虑多个相关剂量的联合效应。（2）我们使用条件正态化流量估计广义倾向得分，以检测共享协变量-治疗空间中重叠有限的区域。（3）我们提供一种基于梯度的学习算法，以找到最佳的个性化剂量组合。在此，我们确保可靠地估计策略价值。

    Decision-making in personalized medicine such as cancer therapy or critical care must often make choices for dosage combinations, i.e., multiple continuous treatments. Existing work for this task has modeled the effect of multiple treatments independently, while estimating the joint effect has received little attention but comes with non-trivial challenges. In this paper, we propose a novel method for reliable off-policy learning for dosage combinations. Our method proceeds along three steps: (1) We develop a tailored neural network that estimates the individualized dose-response function while accounting for the joint effect of multiple dependent dosages. (2) We estimate the generalized propensity score using conditional normalizing flows in order to detect regions with limited overlap in the shared covariate-treatment space. (3) We present a gradient-based learning algorithm to find the optimal, individualized dosage combinations. Here, we ensure reliable estimation of the policy val
    
[^5]: 基于知识库的太空碎片问答系统

    Knowledge Base Question Answering for Space Debris Queries. (arXiv:2305.19734v1 [cs.AI])

    [http://arxiv.org/abs/2305.19734](http://arxiv.org/abs/2305.19734)

    本文介绍了一个基于知识库的系统，可以回答复杂自然语言的查询，支持工程师访问太空碎片环境的知识库中的信息。

    

    太空机构执行复杂的卫星操作，需要在其广泛的信息系统中存储和访问技术知识。知识库是以规模存储和访问此类信息的有效方式。本文中，我们介绍了一个为欧洲空间局（ESA）开发的系统，该系统可以回答复杂的自然语言查询，以支持工程师访问模拟轨道太空碎片环境的知识库中的信息。我们的系统基于一种管道流程，首先从自然语言问题生成一系列基本数据库操作，称为“程序草图”，然后将草图转换为具体的查询程序，包括实体、属性和关系的提及，最后针对数据库执行程序。这种流程分解方法使我们能够通过利用域外数据和由GPT-3生成的半合成数据来训练系统，从而减少过度拟合和漏洞。

    Space agencies execute complex satellite operations that need to be supported by the technical knowledge contained in their extensive information systems. Knowledge bases (KB) are an effective way of storing and accessing such information at scale. In this work we present a system, developed for the European Space Agency (ESA), that can answer complex natural language queries, to support engineers in accessing the information contained in a KB that models the orbital space debris environment. Our system is based on a pipeline which first generates a sequence of basic database operations, called a %program sketch, from a natural language question, then specializes the sketch into a concrete query program with mentions of entities, attributes and relations, and finally executes the program against the database. This pipeline decomposition approach enables us to train the system by leveraging out-of-domain data and semi-synthetic data generated by GPT-3, thus reducing overfitting and shor
    
[^6]: APPRAISER: 利用近似误差进行DNN故障容忍性分析

    APPRAISER: DNN Fault Resilience Analysis Employing Approximation Errors. (arXiv:2305.19733v1 [cs.LG])

    [http://arxiv.org/abs/2305.19733](http://arxiv.org/abs/2305.19733)

    提出了一种新的容错评估方法，称为APPRAISER，它采用近似误差进行DNN故障容忍性分析。APPRAISER提供了成千上万倍的评估速度提升，同时保持了分析的高准确性。

    

    如今，深度神经网络在安全关键应用中的广泛应用引起了新的可靠性关注。实际上，通过硬件仿真进行故障注入的方法是研究DNN架构容错性的有效和广泛使用的方法，早期设计阶段已经减轻了可靠性问题。然而，目前的故障注入方法在时间、设计和控制复杂性方面存在一系列问题。为了克服这些问题，提出了一种新的容错评估方法，称为APPRAISER，它将函数近似用于非传统用途，并利用近似计算误差。通过在容错评估领域采用这个概念，APPRAISER提供了成千上万倍的评估速度提升，同时保持了分析的高准确性。本文通过与最先进的故障注入方法进行比较，验证了APPRAISER的有效性。

    Nowadays, the extensive exploitation of Deep Neural Networks (DNNs) in safety-critical applications raises new reliability concerns. In practice, methods for fault injection by emulation in hardware are efficient and widely used to study the resilience of DNN architectures for mitigating reliability issues already at the early design stages. However, the state-of-the-art methods for fault injection by emulation incur a spectrum of time-, design- and control-complexity problems. To overcome these issues, a novel resiliency assessment method called APPRAISER is proposed that applies functional approximation for a non-conventional purpose and employs approximate computing errors for its interest. By adopting this concept in the resiliency assessment domain, APPRAISER provides thousands of times speed-up in the assessment process, while keeping high accuracy of the analysis. In this paper, APPRAISER is validated by comparing it with state-of-the-art approaches for fault injection by emulat
    
[^7]: 粗糙集下一种规则通用逆推学习方法

    A rule-general abductive learning by rough sets. (arXiv:2305.19718v1 [cs.LG])

    [http://arxiv.org/abs/2305.19718](http://arxiv.org/abs/2305.19718)

    本文提出了一种基于粗糙集理论的规则通用逆推学习方法，通过转化目标概念和子概念为信息表，以更低的成本解决领域知识获取和规则的修正、减少、生成问题。

    

    在现实任务中，通常存在大量未标记数据和标记数据。将两者组合起来进行学习的任务被称为半监督学习。专家可以使用逻辑规则来标记未标记数据，但这个操作很昂贵。感知和推理的结合在处理具有领域知识的半监督任务方面具有良好的效果。然而，获取领域知识以及规则的修正、减少和生成仍然是需要解决的复杂问题。粗糙集理论是解决信息系统中知识处理的重要方法。本文提出了一种粗糙集下的规则通用逆推学习方法（RS-ABL）。通过将规则的目标概念和子概念转化为信息表，利用粗糙集理论来解决以更低的成本获取领域知识和修正、减少、生成规则的问题。该框架还可以生成更广泛的负规则，以增强规则范围。

    In real-world tasks, there is usually a large amount of unlabeled data and labeled data. The task of combining the two to learn is known as semi-supervised learning. Experts can use logical rules to label unlabeled data, but this operation is costly. The combination of perception and reasoning has a good effect in processing such semi-supervised tasks with domain knowledge. However, acquiring domain knowledge and the correction, reduction and generation of rules remain complex problems to be solved. Rough set theory is an important method for solving knowledge processing in information systems. In this paper, we propose a rule general abductive learning by rough set (RS-ABL). By transforming the target concept and sub-concepts of rules into information tables, rough set theory is used to solve the acquisition of domain knowledge and the correction, reduction and generation of rules at a lower cost. This framework can also generate more extensive negative rules to enhance the breadth of
    
[^8]: 可分目标的最优决策树：推动动态规划的极限

    Optimal Decision Trees for Separable Objectives: Pushing the Limits of Dynamic Programming. (arXiv:2305.19706v1 [cs.LG])

    [http://arxiv.org/abs/2305.19706](http://arxiv.org/abs/2305.19706)

    本研究提出了一种通用的动态规划方法来优化任何组合的可分离目标和约束条件，这种方法在可扩展性方面比通用求解器表现得更好。

    

    决策树的全局优化在准确性，大小和人类可理解性方面表现出良好的前景。然而，许多方法仍然依赖于通用求解器，可扩展性仍然是一个问题。动态规划方法已被证明具有更好的可扩展性，因为它们通过将子树作为独立的子问题解决来利用树结构。然而，这仅适用于可以分别优化子树的任务。我们详细研究了这种关系，并展示了实现这种可分离约束和目标任意组合的动态规划方法。在四个应用领域的实验表明了这种方法的普适性，同时也比通用求解器具有更好的可扩展性。

    Global optimization of decision trees has shown to be promising in terms of accuracy, size, and consequently human comprehensibility. However, many of the methods used rely on general-purpose solvers for which scalability remains an issue. Dynamic programming methods have been shown to scale much better because they exploit the tree structure by solving subtrees as independent subproblems. However, this only works when an objective can be optimized separately for subtrees. We explore this relationship in detail and show necessary and sufficient conditions for such separability and generalize previous dynamic programming approaches into a framework that can optimize any combination of separable objectives and constraints. Experiments on four application domains show the general applicability of this framework, while outperforming the scalability of general-purpose solvers by a large margin.
    
[^9]: 通过局部模态初始化和无偏差对比散度实现深度玻尔兹曼机的端到端训练

    End-to-end Training of Deep Boltzmann Machines by Unbiased Contrastive Divergence with Local Mode Initialization. (arXiv:2305.19684v1 [cs.LG])

    [http://arxiv.org/abs/2305.19684](http://arxiv.org/abs/2305.19684)

    本研究提出一种基于Metropolis-Hastings耦合和局部模态初始化的方法，解决了深度玻尔兹曼机中的偏差梯度估计问题，使得DBMs可以端到端地训练，实验结果表明与其他深度生成模型相当的生成性能。

    

    本研究解决了深度玻尔兹曼机（DBMs）中的偏差梯度估计问题。现有的获取无偏估计量的方法使用基于Gibbs采样的最大耦合，但当状态是高维时，其收敛需要很长时间。因此，我们提出了基于Metropolis-Hastings的耦合，并围绕目标分布的局部模态初始化状态。由于MH倾向于拒绝提案，这种耦合有很高的概率在一步内收敛，因此具有高效性。我们发现，我们的方法可以在不进行贪心预训练的情况下端到端地训练DBMs。我们还提出了一些实用技术，以进一步提高DBMs的性能。我们通过实验证明，我们的训练算法使DBMs能够展现出与其他深度生成模型相当的生成性能，在MNIST上达到了10.33的FID分数。

    We address the problem of biased gradient estimation in deep Boltzmann machines (DBMs). The existing method to obtain an unbiased estimator uses a maximal coupling based on a Gibbs sampler, but when the state is high-dimensional, it takes a long time to converge. In this study, we propose to use a coupling based on the Metropolis-Hastings (MH) and to initialize the state around a local mode of the target distribution. Because of the propensity of MH to reject proposals, the coupling tends to converge in only one step with a high probability, leading to high efficiency. We find that our method allows DBMs to be trained in an end-to-end fashion without greedy pretraining. We also propose some practical techniques to further improve the performance of DBMs. We empirically demonstrate that our training algorithm enables DBMs to show comparable generative performance to other deep generative models, achieving the FID score of 10.33 for MNIST.
    
[^10]: 通过自动扩散模型增强异常检测的鲁棒性和泛化性：掩盖、拼合和重新采样

    Mask, Stitch, and Re-Sample: Enhancing Robustness and Generalizability in Anomaly Detection through Automatic Diffusion Models. (arXiv:2305.19643v1 [cs.CV])

    [http://arxiv.org/abs/2305.19643](http://arxiv.org/abs/2305.19643)

    AutoDDPM是一种通过自动扩散模型生成潜在异常的初始可能性地图，并将其与原始图像融合，通过联合噪声分布重新采样来增强扩散模型鲁棒性的方法，它在保留健康组织的情况下替换异常区域，明显超越了扩散模型的局限性。

    

    在异常检测中引入扩散模型为病理图像的更有效和准确的重建铺平了道路。然而，当前通过调节噪声粒度来控制模型的通用性存在局限性，并妨碍了对健康组织的恢复。为了克服这些挑战，我们提出了AutoDDPM，一种增强扩散模型鲁棒性的新方法。AutoDDPM利用扩散模型生成潜在异常的初始可能性地图，并将其与原始图像无缝集成。通过联合噪声分布重新采样，AutoDDPM实现了协调和修补效果。我们的研究证明了AutoDDPM的有效性，在保留健康组织的情况下替换异常区域，明显超越了扩散模型的局限性。它还为当前扩散模型的局限性提供了有价值的见解和分析，促进了鲁棒和普适的异常检测模型的发展。

    The introduction of diffusion models in anomaly detection has paved the way for more effective and accurate image reconstruction in pathologies. However, the current limitations in controlling noise granularity hinder diffusion models' ability to generalize across diverse anomaly types and compromise the restoration of healthy tissues. To overcome these challenges, we propose AutoDDPM, a novel approach that enhances the robustness of diffusion models. AutoDDPM utilizes diffusion models to generate initial likelihood maps of potential anomalies and seamlessly integrates them with the original image. Through joint noised distribution re-sampling, AutoDDPM achieves harmonization and in-painting effects. Our study demonstrates the efficacy of AutoDDPM in replacing anomalous regions while preserving healthy tissues, considerably surpassing diffusion models' limitations. It also contributes valuable insights and analysis on the limitations of current diffusion models, promoting robust and in
    
[^11]: MSMix: 基于插值的文本数据增强方法——流形交换Mixup

    MSMix:An Interpolation-Based Text Data Augmentation Method Manifold Swap Mixup. (arXiv:2305.19617v1 [cs.LG])

    [http://arxiv.org/abs/2305.19617](http://arxiv.org/abs/2305.19617)

    提出了一种基于插值的文本数据增强方法——流形交换Mixup(MSMix)，通过在网络的特定层部分替换隐藏特征来从两个不同的样本中获得更丰富的隐藏表示，并在三个中文意图识别数据集上实验证明了MSMix在完整样本和小样本配置下均表现出更好的性能。

    

    为了解决深度神经网络模型由于数据不足而表现不佳的问题，提出了一种简单而有效的基于插值的数据增强方法：MSMix(流形交换Mixup)。该方法将两个不同的样本输入同一个深度神经网络模型，随机选择一个特定的层，部分替换该层一个样本的隐藏特征为另一个样本的对应特征。混合后的隐藏特征输入模型，并通过网络的其余部分。还提出了两种不同的选择策略，以获取更丰富的隐藏表示。在三个中文意图识别数据集上进行了实验，结果表明MSMix方法在完整样本和小样本配置下均获得了比其他方法更好的结果。

    To solve the problem of poor performance of deep neural network models due to insufficient data, a simple yet effective interpolation-based data augmentation method is proposed: MSMix (Manifold Swap Mixup). This method feeds two different samples to the same deep neural network model, and then randomly select a specific layer and partially replace hidden features at that layer of one of the samples by the counterpart of the other. The mixed hidden features are fed to the model and go through the rest of the network. Two different selection strategies are also proposed to obtain richer hidden representation. Experiments are conducted on three Chinese intention recognition datasets, and the results show that the MSMix method achieves better results than other methods in both full-sample and small-sample configurations.
    
[^12]: 通过领域知识启示的深度学习进行药物推荐

    Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])

    [http://arxiv.org/abs/2305.19604](http://arxiv.org/abs/2305.19604)

    提出一种基于动态领域知识的药物推荐框架DKINet，将领域知识与患者临床表现相结合，此为首次实验。

    

    药物推荐是医疗保健的基本但至关重要的分支，提供机会为复杂健康状况的患者支持临床医生更精确的药物处方。从电子健康记录（EHR）中学习推荐药物是先前研究中最常见的方法。然而，大多数研究忽视了根据患者的EHR中的临床表现纳入领域知识的问题。为了解决这些问题，我们提出了一种新颖的基于动态领域知识的药物推荐框架，即领域知识启示网络（DKINet），用于将领域知识与可观察的患者临床表现相结合。特别是，我们首先设计了一个基于领域知识的编码器来捕捉领域信息，然后开发了一个数据驱动的编码器将领域知识整合到可观察的EHR中。

    Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
    
[^13]: 细粒度语义奖励增强文本到图像扩散模型

    Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards. (arXiv:2305.19599v1 [cs.CV])

    [http://arxiv.org/abs/2305.19599](http://arxiv.org/abs/2305.19599)

    本文提出了FineRewards，通过引入细粒度的语义奖励，即标题奖励和SAM奖励，来改进文本到图像扩散模型中文本和图像之间的对齐。

    

    最近，文本到图像扩散模型的研究取得了显著的成功，在给定的文本提示下生成了高质量、逼真的图像。然而，由于缺乏细粒度语义指导，以成功诊断形态差异为止，以前的方法无法执行文本概念和生成的图像之间的准确形态对齐。在本文中，我们提出了FineRewards，通过引入两种新的细粒度语义奖励--标题奖励和语义分割任何事物（SAM）奖励，来改进文本到图像扩散模型中文本和图像之间的对齐。

    Recent advances in text-to-image diffusion models have achieved remarkable success in generating high-quality, realistic images from given text prompts. However, previous methods fail to perform accurate modality alignment between text concepts and generated images due to the lack of fine-level semantic guidance that successfully diagnoses the modality discrepancy. In this paper, we propose FineRewards to improve the alignment between text and images in text-to-image diffusion models by introducing two new fine-grained semantic rewards: the caption reward and the Semantic Segment Anything (SAM) reward. From the global semantic view, the caption reward generates a corresponding detailed caption that depicts all important contents in the synthetic image via a BLIP-2 model and then calculates the reward score by measuring the similarity between the generated caption and the given prompt. From the local semantic view, the SAM reward segments the generated images into local parts with categ
    
[^14]: 半监督普适图分类

    Towards Semi-supervised Universal Graph Classification. (arXiv:2305.19598v1 [cs.LG])

    [http://arxiv.org/abs/2305.19598](http://arxiv.org/abs/2305.19598)

    该论文提出了一种新型图神经网络框架UGNN， 解决了半监督普适图分类问题，通过估计未标记图的确定性解决了类别偏移，具有最新性能。

    

    近年来，图神经网络推动了图分类的最新技术。本文提出了一种名为UGNN的新型图神经网络框架，以从子图角度利用未标记数据来解决半监督普适图分类问题。该问题的挑战在于缺乏标签数据和可能的类别偏移。UGNN通过估计未标记图的确定性来解决类别偏移，使其在具有挑战性的数据集上具有最新性能。

    Graph neural networks have pushed state-of-the-arts in graph classifications recently. Typically, these methods are studied within the context of supervised end-to-end training, which necessities copious task-specific labels. However, in real-world circumstances, labeled data could be limited, and there could be a massive corpus of unlabeled data, even from unknown classes as a complementary. Towards this end, we study the problem of semi-supervised universal graph classification, which not only identifies graph samples which do not belong to known classes, but also classifies the remaining samples into their respective classes. This problem is challenging due to a severe lack of labels and potential class shifts. In this paper, we propose a novel graph neural network framework named UGNN, which makes the best of unlabeled data from the subgraph perspective. To tackle class shifts, we estimate the certainty of unlabeled graphs using multiple subgraphs, which facilities the discovery of
    
[^15]: 语言模型在零/少样本环境下无法正确理解“respectively”的原因研究

    What does the Failure to Reason with "Respectively" in Zero/Few-Shot Settings Tell Us about Language Models?. (arXiv:2305.19597v1 [cs.CL])

    [http://arxiv.org/abs/2305.19597](http://arxiv.org/abs/2305.19597)

    本文研究发现，语言模型在零/少样本环境下难以理解“respectively”的各种读法，需要更长时间的训练和依赖常识推理，仍落后于人类。

    

    人类可以轻松理解“Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle, respectively”这种句子的协作结构。在自然语言推断（NLI）的背景下，本文从句法-语义和常识-世界知识的两个角度研究了语言模型（LMs）如何理解两个读数（Gawron和Kehler，2004） 。我们提出了一个受控合成数据集WikiResNLI 和一个自然数据集 NatResNLI，以包含各种显式和隐式实现的“respectively”。我们发现，微调后的NLI模型在没有显式监督的情况下难以理解这样的读数。当存在显式提示时，零/少样本学习很容易，而当该读数隐含时，则需要更长的训练时间，以依赖常识推理。此外，我们的细致分析表明，模型无法在不同结构之间进行泛化。总之，我们证明了在零/少样本环境下，语言模型在理解“respectively”的各种读法方面仍落后于人类。

    Humans can effortlessly understand the coordinate structure of sentences such as "Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle, respectively". In the context of natural language inference (NLI), we examine how language models (LMs) reason with respective readings (Gawron and Kehler, 2004) from two perspectives: syntactic-semantic and commonsense-world knowledge. We propose a controlled synthetic dataset WikiResNLI and a naturally occurring dataset NatResNLI to encompass various explicit and implicit realizations of "respectively". We show that fine-tuned NLI models struggle with understanding such readings without explicit supervision. While few-shot learning is easy in the presence of explicit cues, longer training is required when the reading is evoked implicitly, leaving models to rely on common sense inferences. Furthermore, our fine-grained analysis indicates models fail to generalize across different constructions. To conclude, we demonstrate that LMs still lag 
    
[^16]: 可扩展且高效的深度学习光学神经元 多操作数光学神经元

    Integrated multi-operand optical neurons for scalable and hardware-efficient deep learning. (arXiv:2305.19592v1 [physics.optics])

    [http://arxiv.org/abs/2305.19592](http://arxiv.org/abs/2305.19592)

    本文提出了一种使用定制的多操作数光学器件的可扩展和高效光点积引擎，即多操作数光学神经元（MOON），并在图像识别任务中证明了其实用性。在SVHN识别数据集上，该MOON实现了85.89％的测量精度，性能逊于128x128的基于MOMZI的PTCs。

    

    光神经网络（ONN）是下一代神经形态计算的有前途硬件平台，基于单值操作的光学非常数核（PTC）会占用大量单操作数光调制器用于信号和权重编码，导致较大的区域成本和高传播损耗以实现大型张量操作。本研究提出了基于定制的多操作数光学器件的可扩展和高效光点积引擎，即多操作数光学神经元（MOON）。我们在图像识别任务中使用多操作数马赫–泽德干干涉仪（MOMZI）实验演示了MOON的效用。具体而言，我们的基于MOMZI的ONN在4位电压控制精度下在街景房屋数字（SVHN）识别数据集中实现了85.89%的测量精度。此外，我们的性能分析显示，128x128的基于MOMZI的PTCs优于基于单值光调制器的对应物。

    The optical neural network (ONN) is a promising hardware platform for next-generation neuromorphic computing due to its high parallelism, low latency, and low energy consumption. However, previous integrated photonic tensor cores (PTCs) consume numerous single-operand optical modulators for signal and weight encoding, leading to large area costs and high propagation loss to implement large tensor operations. This work proposes a scalable and efficient optical dot-product engine based on customized multi-operand photonic devices, namely multi-operand optical neurons (MOON). We experimentally demonstrate the utility of a MOON using a multi-operand-Mach-Zehnder-interferometer (MOMZI) in image recognition tasks. Specifically, our MOMZI-based ONN achieves a measured accuracy of 85.89% in the street view house number (SVHN) recognition dataset with 4-bit voltage control precision. Furthermore, our performance analysis reveals that a 128x128 MOMZI-based PTCs outperform their counterparts base
    
[^17]: 基于人工智能的交通预测：近期进展与新机遇综述

    Traffic Prediction using Artificial Intelligence: Review of Recent Advances and Emerging Opportunities. (arXiv:2305.19591v1 [cs.LG])

    [http://arxiv.org/abs/2305.19591](http://arxiv.org/abs/2305.19591)

    该论文综述了交通预测方法的发展，重点介绍了基于人工智能的交通预测方法在多元交通时间序列模型研究方面的进展和机遇。

    

    交通预测在缓解全球性的交通拥堵问题中起着关键作用，其负面影响包括额外旅行时间的损失和燃料消耗的增加。将新兴技术融入交通系统可以显著改善交通预测，并带来新的研究问题。为了了解交通预测中的开放性研究挑战，本综述旨在提供交通预测方法的综合概述。具体而言，我们侧重于基于人工智能（AI）的交通预测方法在多变量交通时间序列建模方面的近期进展和新的研究机遇，这是由于近年来这类方法在交通预测中具有潜在的成功和潜力。

    Traffic prediction plays a crucial role in alleviating traffic congestion which represents a critical problem globally, resulting in negative consequences such as lost hours of additional travel time and increased fuel consumption. Integrating emerging technologies into transportation systems provides opportunities for improving traffic prediction significantly and brings about new research problems. In order to lay the foundation for understanding the open research challenges in traffic prediction, this survey aims to provide a comprehensive overview of traffic prediction methodologies. Specifically, we focus on the recent advances and emerging research opportunities in Artificial Intelligence (AI)-based traffic prediction methods, due to their recent success and potential in traffic prediction, with an emphasis on multivariate traffic time series modeling. We first provide a list and explanation of the various data types and resources used in the literature. Next, the essential data 
    
[^18]: 带建议的主动因果结构学习

    Active causal structure learning with advice. (arXiv:2305.19588v1 [cs.LG])

    [http://arxiv.org/abs/2305.19588](http://arxiv.org/abs/2305.19588)

    本研究提出了带建议的主动因果结构学习问题，并设计了一个自适应搜索算法，可以从建议中受益，即使建议是任意糟糕的情况下，仍然具有最坏情况下的保证。

    

    我们引入了带建议的主动因果结构学习问题。在典型的研究中，学习算法针对观测分布获得本质图，并被要求在最小化干预次数的同时恢复出潜在的因果有向无环图(DAG) $G^*$。在我们的问题设定中，除了关于 $G^*$的必要信息外，例如一个声称是 $G^*$的DAG $G$，我们还会额外获得关于 $G^*$的侧面信息。我们想知道，当建议接近正确时，学习算法是否可以从建议中受益，同时即使建议是任意糟糕的情况下，仍然具有最坏情况下的保证。我们的工作与关于带预测算法的不断增加的研究领域相同。当建议是有向无环图$G$时，我们设计了自适应搜索算法来恢复 $G^*$，其干预成本最多为验证$G^*$的成本的$O(max\{1, \log \psi\})$倍。这里，$\psi$是$G$和$G^*$之间的距离度量，它被上界约束。

    We introduce the problem of active causal structure learning with advice. In the typical well-studied setting, the learning algorithm is given the essential graph for the observational distribution and is asked to recover the underlying causal directed acyclic graph (DAG) $G^*$ while minimizing the number of interventions made. In our setting, we are additionally given side information about $G^*$ as advice, e.g. a DAG $G$ purported to be $G^*$. We ask whether the learning algorithm can benefit from the advice when it is close to being correct, while still having worst-case guarantees even when the advice is arbitrarily bad. Our work is in the same space as the growing body of research on algorithms with predictions. When the advice is a DAG $G$, we design an adaptive search algorithm to recover $G^*$ whose intervention cost is at most $O(\max\{1, \log \psi\})$ times the cost for verifying $G^*$; here, $\psi$ is a distance measure between $G$ and $G^*$ that is upper bounded by the numb
    
[^19]: 面向车辆路径问题的全通用神经方法

    Towards Omni-generalizable Neural Methods for Vehicle Routing Problems. (arXiv:2305.19587v1 [cs.LG])

    [http://arxiv.org/abs/2305.19587](http://arxiv.org/abs/2305.19587)

    提出了一个通用meta-learning框架，使得模型可以在推理过程中快速适应新任务。通过在多个合成和基准数据集上的实验表明，该方法可以有效地解决大小和分布变化的车辆路径问题（VRP）。

    

    由于避免了对手工规则的依赖，学习车辆路径问题（VRP）的启发式方法受到了广泛关注。然而，现有方法通常在固定大小和节点分布的同一任务上进行训练和测试，因此具有有限的泛化性能。本文研究了一个具有挑战性但又现实的场景，该场景考虑了VRP在大小和分布方面的一般性。我们提出了一种通用的元学习框架，在推理期间能够快速适应新任务的能力下对初始化模型进行有效训练。我们进一步开发了一种简单而有效的近似方法来减少训练开销。对旅行商问题（TSP）和容量车辆路径问题（CVRP）的合成和基准实例进行了广泛的实验证明了我们方法的有效性。代码可在https://github.com/RoyalSkye/Omni-VRP得到。

    Learning heuristics for vehicle routing problems (VRPs) has gained much attention due to the less reliance on hand-crafted rules. However, existing methods are typically trained and tested on the same task with a fixed size and distribution (of nodes), and hence suffer from limited generalization performance. This paper studies a challenging yet realistic setting, which considers generalization across both size and distribution in VRPs. We propose a generic meta-learning framework, which enables effective training of an initialized model with the capability of fast adaptation to new tasks during inference. We further develop a simple yet efficient approximation method to reduce the training overhead. Extensive experiments on both synthetic and benchmark instances of the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP) demonstrate the effectiveness of our method. The code is available at: https://github.com/RoyalSkye/Omni-VRP.
    
[^20]: 基于高阶累积量的潜在混淆因素因果关系发现

    Causal Discovery with Latent Confounders Based on Higher-Order Cumulants. (arXiv:2305.19582v1 [cs.LG])

    [http://arxiv.org/abs/2305.19582](http://arxiv.org/abs/2305.19582)

    本文提出了一种利用高阶累积量实现潜在混淆因素因果关系发现的新方法，这种方法能够使单潜在成分结构的OICA问题得到闭合形式解决方案，同时还提出了可测试的单潜在成分条件，通过迭代删除共享识别的潜在成分成功扩展了结果到多潜在成分结构。

    

    在许多科学领域中，具有潜在混淆因素的因果关系发现是一项重要但具有挑战性的任务。尽管某些过完备独立分量分析（OICA）方法在某些领域取得了成功，但它们计算成本高，容易陷入局部最优解。我们发现有趣的是，通过利用高阶累积量，在特定情况下（例如，混合过程遵循单潜在分量结构），存在OICA的闭合形式解决方案。鉴于OICA闭合形式解决方案与单潜在成分结构相对应的强大功能，我们制定了一种使用高阶累积量估计混合矩阵的方法，并进一步提出可测试的单潜在成分条件，以识别潜在变量并确定因果顺序。通过迭代删除共享识别的潜在成分，我们成功地将单潜在成分结构的结果扩展到多潜在成分结构。

    Causal discovery with latent confounders is an important but challenging task in many scientific areas. Despite the success of some overcomplete independent component analysis (OICA) based methods in certain domains, they are computationally expensive and can easily get stuck into local optima. We notice that interestingly, by making use of higher-order cumulants, there exists a closed-form solution to OICA in specific cases, e.g., when the mixing procedure follows the One-Latent-Component structure. In light of the power of the closed-form solution to OICA corresponding to the One-Latent-Component structure, we formulate a way to estimate the mixing matrix using the higher-order cumulants, and further propose the testable One-Latent-Component condition to identify the latent variables and determine causal orders. By iteratively removing the share identified latent components, we successfully extend the results on the One-Latent-Component structure to the Multi-Latent-Component structu
    
[^21]: SVVAD：用于说话人验证的个人语音活动检测

    SVVAD: Personal Voice Activity Detection for Speaker Verification. (arXiv:2305.19581v1 [cs.SD])

    [http://arxiv.org/abs/2305.19581](http://arxiv.org/abs/2305.19581)

    提出了一种基于说话人验证的语音活动检测（SVVAD）框架，可根据最具信息量的语音特征进行调整，利用无标签训练方法，完全避免了由于错误标记导致的SV性能下降。实验证明在其他说话者混合的条件下，SVVAD在平均等误差率（EER）方面显著优于基线。

    

    语音活动检测（VAD）通过保留语音片段和减弱非语音的影响来提高说话人验证（SV）的性能。然而，这个方法并不理想：它在嘈杂的环境或多说话者对话中失败；它是基于不准确的非SV敏感标签进行训练的。为了解决这个问题，我们提出了一种基于说话人验证的语音活动检测（SVVAD）框架，可以根据SV最具信息量的语音特征进行调整。为了实现这一点，我们引入了一种无标签训练方法，利用类三元组损失函数，完全避免了由于错误标记而导致的SV性能下降。广泛的实验表明，SVVAD在其他说话者混合的条件下，平均等误差率（EER）方面显著优于基线。此外，决策边界揭示了语音不同部分的重要性，这与人类判断基本一致。

    Voice activity detection (VAD) improves the performance of speaker verification (SV) by preserving speech segments and attenuating the effects of non-speech. However, this scheme is not ideal: (1) it fails in noisy environments or multi-speaker conversations; (2) it is trained based on inaccurate non-SV sensitive labels. To address this, we propose a speaker verification-based voice activity detection (SVVAD) framework that can adapt the speech features according to which are most informative for SV. To achieve this, we introduce a label-free training method with triplet-like losses that completely avoids the performance degradation of SV due to incorrect labeling. Extensive experiments show that SVVAD significantly outperforms the baseline in terms of equal error rate (EER) under conditions where other speakers are mixed at different ratios. Moreover, the decision boundaries reveal the importance of the different parts of speech, which are largely consistent with human judgments.
    
[^22]: 强化学习中的可复现性研究

    Replicability in Reinforcement Learning. (arXiv:2305.19562v1 [cs.LG])

    [http://arxiv.org/abs/2305.19562](http://arxiv.org/abs/2305.19562)

    这篇论文研究了在强化学习中的可复制性，提出了可复制算法和松弛可复制算法，并给出了相应的时间和样本复杂度，这对于RL算法设计以及未来的可复制性研究具有影响。

    

    我们在强化学习 (RL) 的背景下，将可复现性作为算法属性进行了数学研究。我们关注的是具有生成模型访问权的带折扣表格MDP的基本设置。受Impagliazzo等人 [2022]的启发，如果在内部随机性相同时，RL算法在从生成器抽取的两个独立和同分布的样本上执行两次并输出完全相同的策略，则表示该RL算法是可复制的。我们首先提供一个有效的$\rho$-可复制算法，用于$(\varepsilon,\delta)$-最优策略估计，其样本和时间复杂度为 $\widetilde O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$，其中$N$是状态-动作对的数量。然后，对于确定性算法的子类，我们提供了 $ \Omega\left(\frac {N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right) $ 阶的下限。接下来，我们研究了Kalavasis等人[2019]提出的可复制性的松弛版本，其中仅要求算法的输出接近复制算法的输出，而不是相同。我们提供了一种有效算法，其时间和样本复杂度为 $\widetilde O\left(\frac{N^5\cdot\log(1/\delta)}{(1-\gamma)^9\cdot\varepsilon^4\cdot\rho^2}\right)$，用于$(\varepsilon,\delta)$意义下的可复制性，这比先前与相关问题的界限更好。最后，我们讨论了我们的结果对RL算法设计和可重复性研究的未来方向的影响。

    We initiate the mathematical study of replicability as an algorithmic property in the context of reinforcement learning (RL). We focus on the fundamental setting of discounted tabular MDPs with access to a generative model. Inspired by Impagliazzo et al. [2022], we say that an RL algorithm is replicable if, with high probability, it outputs the exact same policy after two executions on i.i.d. samples drawn from the generator when its internal randomness is the same. We first provide an efficient $\rho$-replicable algorithm for $(\varepsilon, \delta)$-optimal policy estimation with sample and time complexity $\widetilde O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$, where $N$ is the number of state-action pairs. Next, for the subclass of deterministic algorithms, we provide a lower bound of order $\Omega\left(\frac{N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right)$. Then, we study a relaxed version of replicability proposed by Kalavasis et 
    
[^23]: 基于前瞻的移动增强现实边缘云计算任务卸载

    Look-Ahead Task Offloading for Multi-User Mobile Augmented Reality in Edge-Cloud Computing. (arXiv:2305.19558v1 [eess.SP])

    [http://arxiv.org/abs/2305.19558](http://arxiv.org/abs/2305.19558)

    本论文提出了一种基于 MAR 服务的移动增强现实任务卸载方案，使用有向无环图建模任务相互依赖性，并采用前瞻式卸载方案，基于改进的蒙特卡罗树搜索来确保高效性和公平性，实验结果表明该方案可以显著减少任务延迟和能量消耗。

    

    移动增强现实 (MAR) 将真实场景与虚拟内容融合，被视为 Metaverse 中普遍接口之一。由于 MAR 设备计算能力和电池寿命有限，因此常见的做法是将计算任务卸载到靠近设备的边缘或云服务器上。然而，为 MAR 任务开发的现有卸载方案在提供多用户 MAR 服务时存在高迁移开销、可扩展性差和短视性的问题。为解决这些问题，设计并评估了一种基于 MAR 服务的任务卸载方案，应用于边缘云计算网络中。具体而言，首先使用有向无环图对 MAR 应用的任务相互依赖性进行分析和建模。然后，提出了一种基于改进的蒙特卡罗树 (MMCT) 搜索的前瞻式卸载方案，可以提前运行多个多步执行，以获取即时行动的长期效果估计。实验结果表明，我们提出的方案可以显著减少任务延迟和能量消耗，同时对多用户 MAR 服务保持任务准确性和公平性。

    Mobile augmented reality (MAR) blends a real scenario with overlaid virtual content, which has been envisioned as one of the ubiquitous interfaces to the Metaverse. Due to the limited computing power and battery life of MAR devices, it is common to offload the computation tasks to edge or cloud servers in close proximity. However, existing offloading solutions developed for MAR tasks suffer from high migration overhead, poor scalability, and short-sightedness when applied in provisioning multi-user MAR services. To address these issues, a MAR service-oriented task offloading scheme is designed and evaluated in edge-cloud computing networks. Specifically, the task interdependency of MAR applications is firstly analyzed and modeled by using directed acyclic graphs. Then, we propose a look-ahead offloading scheme based on a modified Monte Carlo tree (MMCT) search, which can run several multi-step executions in advance to get an estimate of the long-term effect of immediate action. Experim
    
[^24]: 探索唇部运动的语音上下文对真实说话人脸生成的影响

    Exploring Phonetic Context in Lip Movement for Authentic Talking Face Generation. (arXiv:2305.19556v1 [cs.CV])

    [http://arxiv.org/abs/2305.19556](http://arxiv.org/abs/2305.19556)

    本研究探讨了语音上下文对真实说话人脸生成的影响，提出了一种Context-Aware Lip-Sync框架（CALS），可利用语音上下文生成更加准确、稳定的唇部运动。

    

    说话人脸生成是将自然面部与驱动音频同步合成的任务。尽管在视觉质量、唇形同步和面部动作方面取得了很大进展，但当前的研究仍然难以解决粗糙和异步的唇部运动问题，这可能导致类似木偶动画的效果。本文发现，以往的作品通常将唇部运动与音频在不同的音素级别上进行相关联，然而，由于音素之间的协同发音（co-articulation）现象，即隔离的音素受前一个或下一个音素的影响，因此同一个音素的发音因音素上下文而异。因此，使用音素上下文模型可以生成更加空间和时间上对齐、稳定的唇部运动。基于此，我们研究了唇部运动中的语音上下文对于真实说话人脸生成的影响。我们提出了一种Context-Aware Lip-Sync框架（CALS），利用语音上下文生成更加空间和时间上对齐、稳定的唇部运动。

    Talking face generation is the task of synthesizing a natural face synchronous to driving audio. Although much progress has been made in terms of visual quality, lip synchronization, and facial motion of the talking face, current works still struggle to overcome issues of crude and asynchronous lip movement, which can result in puppetry-like animation. We identify that the prior works commonly correlate lip movement with audio at the phone level. However, due to co-articulation, where an isolated phone is influenced by the preceding or following phones, the articulation of a phone varies upon the phonetic context. Therefore, modeling lip motion with the phonetic context can generate more spatio-temporally aligned and stable lip movement. In this respect, we investigate the phonetic context in lip motion for authentic talking face generation. We propose a Context-Aware Lip-Sync framework (CALS), which leverages phonetic context to generate more spatio-temporally aligned and stable lip m
    
[^25]: Spotlight Attention: 具备空间局部性先验的鲁棒目标中心学习

    Spotlight Attention: Robust Object-Centric Learning With a Spatial Locality Prior. (arXiv:2305.19550v1 [cs.CV])

    [http://arxiv.org/abs/2305.19550](http://arxiv.org/abs/2305.19550)

    该论文提出了一个新的目标中心学习方法，通过加入空间局部性先验来提高模型的鲁棒性，使模型在合成和真实数据上实现了显著的物体分割改进，并且对模型超参数不太敏感。

    

    目标中心视觉的目的是构建场景中物体的显式表示。这种表示是通过一组可互换的模块(称为slot或对象文件)获得的，它们竞争图像的局部补丁。该竞争具有弱感性偏差，以保持空间连续性;因此，一个slot可能会宣称在整个图像中散布的补丁。与此相反，人类视觉的感性偏差很强，到了注意力经典用聚光灯比喻的程度。我们将空间局部性先验融入现代目标中心视觉模型，从而在合成和真实数据集中获得显着的物体分割改进。类似于人类视觉注意力，图像内容和空间约束的组合产生了具有鲁棒性的无监督目标中心学习，包括对模型超参数不太敏感。

    The aim of object-centric vision is to construct an explicit representation of the objects in a scene. This representation is obtained via a set of interchangeable modules called \emph{slots} or \emph{object files} that compete for local patches of an image. The competition has a weak inductive bias to preserve spatial continuity; consequently, one slot may claim patches scattered diffusely throughout the image. In contrast, the inductive bias of human vision is strong, to the degree that attention has classically been described with a spotlight metaphor. We incorporate a spatial-locality prior into state-of-the-art object-centric vision models and obtain significant improvements in segmenting objects in both synthetic and real-world datasets. Similar to human visual attention, the combination of image content and spatial constraints yield robust unsupervised object-centric learning, including less sensitivity to model hyperparameters.
    
[^26]: 用全息约化表示重新建模自注意力

    Recasting Self-Attention with Holographic Reduced Representations. (arXiv:2305.19534v1 [cs.LG])

    [http://arxiv.org/abs/2305.19534](http://arxiv.org/abs/2305.19534)

    本文提出了一种使用HRR的神经符号方法重新构建自注意力的方法，可以实现较低的时间和空间复杂度，并在LRA基准测试中获得了接近于最先进的准确度。

    

    近年来，自注意力已经成为各个领域序列建模的主要范例。然而，在序列长度非常长的领域中，复杂度为$\mathcal{O}(T^2)$的内存和$\mathcal{O}(T^2 \cdot H)$的计算成本可能会使得使用变形金刚网络不可行。受惊物检测中$T \geq 100,000$的序列长度成为深度学习的拦路虎的问题的启发，我们使用全息约化表示（HRR）的神经符号化方法重新构建自注意力。这样我们执行相同的高级策略，即标准自 注意力的查询匹配钥匙，返回每个键的值的加权响应。通过实现“Hrrformer”，我们获得了一些好处，包括$\mathcal{O}(T H \log H)$的时间复杂度、$\mathcal{O}(T H)$的空间复杂度和收敛于$10\times$更少的迭代次数。然而，Hrrformer在LRA基准测试中实现了接近于最先进的准确度，我们能够学习到深度模型。

    In recent years, self-attention has become the dominant paradigm for sequence modeling in a variety of domains. However, in domains with very long sequence lengths the $\mathcal{O}(T^2)$ memory and $\mathcal{O}(T^2 H)$ compute costs can make using transformers infeasible. Motivated by problems in malware detection, where sequence lengths of $T \geq 100,000$ are a roadblock to deep learning, we re-cast self-attention using the neuro-symbolic approach of Holographic Reduced Representations (HRR). In doing so we perform the same high-level strategy of the standard self-attention: a set of queries matching against a set of keys, and returning a weighted response of the values for each key. Implemented as a ``Hrrformer'' we obtain several benefits including $\mathcal{O}(T H \log H)$ time complexity, $\mathcal{O}(T H)$ space complexity, and convergence in $10\times$ fewer epochs. Nevertheless, the Hrrformer achieves near state-of-the-art accuracy on LRA benchmarks and we are able to learn wi
    
[^27]: 带有内部分布在线适应的离线元强化学习

    Offline Meta Reinforcement Learning with In-Distribution Online Adaptation. (arXiv:2305.19529v1 [cs.LG])

    [http://arxiv.org/abs/2305.19529](http://arxiv.org/abs/2305.19529)

    本文提出了一种带有不确定性量化的内部分布在线适应(IDAQ)的框架，利用策略后验集合和信念更新网络量化策略不确定性并生成上下文信息来处理新任务，在离线元强化学习上具有竞争性表现。

    

    近期的离线元强化学习方法通常利用任务相关的行为策略(例如，对每个个体任务进行RL智能体的训练)来收集多任务数据集。然而，这些方法总是需要额外的信息进行快速调整，例如测试任务的离线上下文。为了解决这个问题，我们首先正式地表征了离线元强化学习中的一个独特挑战：离线数据集和在线适应之间的转换-奖励分布偏移。我们的理论发现，来自分布之外的适应情况可能会导致不可靠的策略评估，并且使用分布内的情况进行在线适应可以确保适应性能保证。基于这些理论洞察，我们提出了一种新的适应框架，称为带有不确定性量化的内部分布在线适应(IDAQ)，它利用策略后验集合和信念更新网络量化策略不确定性并生成上下文信息来处理新任务。 实验结果表明，IDAQ的效果优于现有的离线元强化学习方法，并且达到了最先进的在线元强化学习方法的竞争性表现。

    Recent offline meta-reinforcement learning (meta-RL) methods typically utilize task-dependent behavior policies (e.g., training RL agents on each individual task) to collect a multi-task dataset. However, these methods always require extra information for fast adaptation, such as offline context for testing tasks. To address this problem, we first formally characterize a unique challenge in offline meta-RL: transition-reward distribution shift between offline datasets and online adaptation. Our theory finds that out-of-distribution adaptation episodes may lead to unreliable policy evaluation and that online adaptation with in-distribution episodes can ensure adaptation performance guarantee. Based on these theoretical insights, we propose a novel adaptation framework, called In-Distribution online Adaptation with uncertainty Quantification (IDAQ), which generates in-distribution context using a given uncertainty quantification and performs effective task belief inference to address new
    
[^28]: 基于扩散式语言模型的细粒度文本风格转换

    Fine-grained Text Style Transfer with Diffusion-Based Language Models. (arXiv:2305.19512v1 [cs.CL])

    [http://arxiv.org/abs/2305.19512](http://arxiv.org/abs/2305.19512)

    本文提出了一种基于扩散式语言模型的细粒度文本风格转换方法，在不依赖外部信息的情况下取得了比之前利用预训练权重、嵌入和外部语法分析器更好的效果，表明扩散概率模型在文本生成领域具有广泛的应用前景。

    

    扩散式概率模型已经在可控制地生成高质量图像上显示出了巨大的成功，研究人员已经试图将这种可控性运用到文本生成领域。以前的扩散式语言模型研究表明，它们可以在不需要外部知识（如预训练权重）的情况下进行训练，并且仍然可以实现稳定的性能和可控性。 在本文中，我们在StylePTB数据集上训练了一个扩散式模型，这是细粒度文本风格转换的标准基准。与以前的工作评估任务相比，StylePTB中的任务需要对输出文本进行更加精细的控制，我们的模型能够在StylePTB上实现卓越的性能，包括个别和组合转换。此外，我们的模型在没有外部知识的情况下使用StylePTB的有限数据进行训练，其表现优于以前利用预训练权重、嵌入和外部语法分析器的工作，这可能表明扩散概率模型在文本生成领域具有巨大的潜力。

    Diffusion probabilistic models have shown great success in generating high-quality images controllably, and researchers have tried to utilize this controllability into text generation domain. Previous works on diffusion-based language models have shown that they can be trained without external knowledge (such as pre-trained weights) and still achieve stable performance and controllability. In this paper, we trained a diffusion-based model on StylePTB dataset, the standard benchmark for fine-grained text style transfers. The tasks in StylePTB requires much more refined control over the output text compared to tasks evaluated in previous works, and our model was able to achieve state-of-the-art performance on StylePTB on both individual and compositional transfers. Moreover, our model, trained on limited data from StylePTB without external knowledge, outperforms previous works that utilized pretrained weights, embeddings, and external grammar parsers, and this may indicate that diffusion
    
[^29]: 顺序置信度校准的感知及语义感知正则化

    Perception and Semantic Aware Regularization for Sequential Confidence Calibration. (arXiv:2305.19498v1 [cs.CV])

    [http://arxiv.org/abs/2305.19498](http://arxiv.org/abs/2305.19498)

    本论文提出了一种顺序置信度校准的方法，即使用感知及语义感知正则化来调整过于自信的预测结果。该方法通过对与目标序列高度相关的令牌/序列进行分析，提高了深度顺序识别模型的性能。

    

    深度顺序识别模型因在各种应用中的优越性能受到越来越多的关注。大多数深度顺序识别模型仅使用目标序列作为监督，而未考虑其他相关序列，导致其预测结果过于自信。已有的深度顺序识别模型使用标签平滑来对标签进行调整以降低自信度，但它们并未考虑令牌/序列之间的关联性，这些关联性可能为正则化训练提供更加有效的信息，进而导致次优的性能。因此，本研究发现与目标序列高度感知和语义相关的令牌/序列包含了更具相关性和更有效的信息，从而有助于更有效地进行正则化。为此，我们提出了一个感知及语义感知序列正则化体系结构，探索联想感知和语义相似性的令牌/序列，从而提高模型的性能。

    Deep sequence recognition (DSR) models receive increasing attention due to their superior application to various applications. Most DSR models use merely the target sequences as supervision without considering other related sequences, leading to over-confidence in their predictions. The DSR models trained with label smoothing regularize labels by equally and independently smoothing each token, reallocating a small value to other tokens for mitigating overconfidence. However, they do not consider tokens/sequences correlations that may provide more effective information to regularize training and thus lead to sub-optimal performance. In this work, we find tokens/sequences with high perception and semantic correlations with the target ones contain more correlated and effective information and thus facilitate more effective regularization. To this end, we propose a Perception and Semantic aware Sequence Regularization framework, which explore perceptively and semantically correlated tokens
    
[^30]: CVSNet：大脑中央视觉系统的计算机实现

    CVSNet: A Computer Implementation for Central Visual System of The Brain. (arXiv:2305.19492v1 [cs.CV])

    [http://arxiv.org/abs/2305.19492](http://arxiv.org/abs/2305.19492)

    本文提出了一个人工神经网络CVSNet，可以看作是大脑中央视觉系统的计算机实现，其中每个块都表示与大脑中相同的视觉信息，通过三个独立的通道和五个不同的块流动。

    

    在计算机视觉中，不同的基础模块围绕着不同的矩阵运算创建，并且基于这些基础模块的模型已经取得了良好的结果。然而，这些基于实验的模型也使得深度学习长期以来受到原理和可解释性方面的批评。本文中，我们构建了一个人工神经网络——CVSNet，可以看作是大脑中央视觉系统的计算机实现。CVSNet中的每个块都表示与大脑中相同的视觉信息。CVSNet中的块彼此不同，视觉信息通过三个独立的通道和五个不同的块流动。因此，CVSNet与所有先前模型的设计完全不同，其中基础块被重复以构建模型，信息也得以传递。

    In computer vision, different basic blocks are created around different matrix operations, and models based on different basic blocks have achieved good results. Good results achieved in vision tasks grants them rationality. However, these experimental-based models also make deep learning long criticized for principle and interpretability. Deep learning originated from the concept of neurons in neuroscience, but recent designs detached natural neural networks except for some simple concepts. In this paper, we build an artificial neural network, CVSNet, which can be seen as a computer implementation for central visual system of the brain. Each block in CVSNet represents the same vision information as that in brains. In CVSNet, blocks differs from each other and visual information flows through three independent pathways and five different blocks. Thus CVSNet is completely different from the design of all previous models, in which basic blocks are repeated to build model and information 
    
[^31]: 使用值条件状态熵探索加速强化学习

    Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration. (arXiv:2305.19476v1 [cs.LG])

    [http://arxiv.org/abs/2305.19476](http://arxiv.org/abs/2305.19476)

    本文提出了一种新的探索技术，使用值条件状态熵来解决强化学习中探索不足的问题，可以均衡地覆盖低价值和高价值状态，相较于现有基于熵的探索方法，该方法在MuJoCo基准测试和Atari游戏上有着显著的提升。

    

    探索的一种有效技术是通过鼓励对访问状态空间的均匀覆盖来最大化已访问状态分布的熵，即状态熵。然而，它在有任务奖励的监督设置中往往难以应对，其中代理趋向于访问高价值状态以利用任务奖励。这个偏好会导致高价值状态和低价值状态的分布不平衡，当分布变得更加均匀时，状态熵会增加，从而偏向于探索低价值区域。当高价值状态在状态空间中分布狭窄时，这个问题会进一步恶化，使得代理完成任务变得更加困难。在本文中，我们提出了一种新颖的探索技术，最大化值条件状态熵，它分别估计每个状态价值估计条件下的状态熵，然后最大化它们的加权和。值条件状态熵量化了低价值和高价值状态区域的覆盖范围，从而使其对不平衡问题更加健壮。我们展示了我们的方法在一系列具有挑战性的MuJoCo基准测试和Atari游戏上显著优于现有的基于熵的探索方法。

    A promising technique for exploration is to maximize the entropy of visited state distribution, i.e., state entropy, by encouraging uniform coverage of visited state space. While it has been effective for an unsupervised setup, it tends to struggle in a supervised setup with a task reward, where an agent prefers to visit high-value states to exploit the task reward. Such a preference can cause an imbalance between the distributions of high-value states and low-value states, which biases exploration towards low-value state regions as a result of the state entropy increasing when the distribution becomes more uniform. This issue is exacerbated when high-value states are narrowly distributed within the state space, making it difficult for the agent to complete the tasks. In this paper, we present a novel exploration technique that maximizes the value-conditional state entropy, which separately estimates the state entropies that are conditioned on the value estimates of each state, then ma
    
[^32]: 双重约束公平聚类

    Doubly Constrained Fair Clustering. (arXiv:2305.19475v1 [cs.LG])

    [http://arxiv.org/abs/2305.19475](http://arxiv.org/abs/2305.19475)

    本论文关注公平聚类问题中的人口统计学公平概念，提出一种同时满足不同公平要求的快速算法。

    

    在过去的几年中，公平聚类吸引了显着的关注，导致出现了大量不同的公平概念。虽然这些概念是有充分理据的，但它们通常在独立的情况下被考虑和研究，其中一个公平要求被独立地考虑而不考虑其他的。这导致了在公平聚类中理解不同公平概念之间的关系成为一个重要的开放问题。在本文中，我们朝向这个方向迈出了第一步。具体来说，我们考虑了聚类中最突出的两种人口统计学公平概念:(1)集团公正(GF)，在这种情况下，不同的人口统计学群体在每个聚类中应该有接近人口水平的代表;(2)中心选择中的多样性(DS)，在这种情况下，所选择的中心应该接近每个群体的人口水平表示。我们展示了在给定一个恒定的近似算法的情况下，一个快速的算法可以同时满足这两个概念。

    The remarkable attention which fair clustering has received in the last few years has resulted in a significant number of different notions of fairness. Despite the fact that these notions are well-justified, they are often motivated and studied in a disjoint manner where one fairness desideratum is considered exclusively in isolation from the others. This leaves the understanding of the relations between different fairness notions as an important open problem in fair clustering. In this paper, we take the first step in this direction. Specifically, we consider the two most prominent demographic representation fairness notions in clustering: (1) Group Fairness (GF), where the different demographic groups are supposed to have close to population-level representation in each cluster and (2) Diversity in Center Selection (DS), where the selected centers are supposed to have close to population-level representation of each group. We show that given a constant approximation algorithm for on
    
[^33]: PlaSma: 为 (反事实) 计划制定增强过程知识模型的小型语言模型

    PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v1 [cs.CL])

    [http://arxiv.org/abs/2305.19472](http://arxiv.org/abs/2305.19472)

    PlaSma提出了一种使用小型语言模型进行过程知识和计划能力的新方法，

    

    过程规划是机器的一项重要而又复杂的任务，它将一个高级目标分解为一系列时间顺序的步骤。它需要整合常识知识以推理出常常是反事实的复杂情境，例如 "没有电话时安排医生的约会"。当前的方法使用大型语言模型 (LLM) 取得了令人鼓舞的结果，但受到昂贵的 API 调用和可复现性问题的限制。本文提出使用更小的语言模型来进行规划，我们介绍了 PlaSma，这是一种新的双重方法，使小型语言模型具有过程知识和 (反事实) 计划能力。更具体地说，我们开发了符号过程知识蒸馏来增强小型语言模型中的隐含知识，以及一种推理算法来促进更结构化和准确的推理。此外，我们还引入了一个新的任务，反事实规划。

    Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g. "scheduling a doctor's appointment without a phone". While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactua
    
[^34]: FPGA上多层梯度自由在线可训练脉冲神经网络的有效实现

    Efficient Implementation of a Multi-Layer Gradient-Free Online-Trainable Spiking Neural Network on FPGA. (arXiv:2305.19468v1 [cs.NE])

    [http://arxiv.org/abs/2305.19468](http://arxiv.org/abs/2305.19468)

    本文介绍了一种在FPGA上有效实现多层梯度自由在线可训练脉冲神经网络的方法，通过使用本地自适应选择阈值和适合硬件的修改重量更新规则，并实现了99.5％的高分类精度。

    

    本文介绍了最近提出的优化深度事件驱动脉冲神经网络体系结构（ODESA）的高效硬件实现。ODESA是第一个具有端到端多层在线本地监督训练且不使用梯度的网络，并具有有效的层次结构中的权重和阈值的组合适应性。本研究表明，可以在硬件上高效地实现网络架构和重量和阈值的在线培训。实现包括多层脉冲神经网络（SNN）和每个层的单独训练模块，可以实现在线自学习而不使用反向传播。通过使用简单的本地自适应选择阈值，在每个层上进行赢家通吃（WTA）约束，并使用更适合硬件的修改重量更新规则，训练模块可以在每个层中最优地分配神经元资源，而不必将高精度误差信号传递给较低层的神经元。在可扩展的FPGA上提出的硬件实现在MNIST数据集上实现了99.5％的高分类精度，同时运行速度更快，功耗更低。

    This paper presents an efficient hardware implementation of the recently proposed Optimized Deep Event-driven Spiking Neural Network Architecture (ODESA). ODESA is the first network to have end-to-end multi-layer online local supervised training without using gradients and has the combined adaptation of weights and thresholds in an efficient hierarchical structure. This research shows that the network architecture and the online training of weights and thresholds can be implemented efficiently on a large scale in hardware. The implementation consists of a multi-layer Spiking Neural Network (SNN) and individual training modules for each layer that enable online self-learning without using back-propagation. By using simple local adaptive selection thresholds, a Winner-Takes-All (WTA) constraint on each layer, and a modified weight update rule that is more amenable to hardware, the trainer module allocates neuronal resources optimally at each layer without having to pass high-precision er
    
[^35]: 位置编码对 Transformer 模型长度推广的影响

    The Impact of Positional Encoding on Length Generalization in Transformers. (arXiv:2305.19466v1 [cs.CL])

    [http://arxiv.org/abs/2305.19466](http://arxiv.org/abs/2305.19466)

    本文通过实证研究 Transformer 模型中位置编码对于长度推广的影响，结果表明常用的位置编码方法并不适合用于下游任务的长度推广，并且使用位置编码甚至可能会损害长度推广的能力。

    

    Transformer-based 语言模型的开发中，长度推广是一个关键的挑战，它是指从小的训练文本范围到更大范围的泛化能力。位置编码（PE）被发现是影响长度推广的主要因素之一，但不同的 PE 方案对下游任务的外推影响还不清楚。本文通过对比评估五种不同位置编码方法（包括绝对位置嵌入、T5 的相对 PE、ALiBi、Rotary 和无位置编码）的解码器 Transformer 的长度推广能力，对推理和数学任务进行了系统的实证研究。研究发现，常用的位置编码方法，如 ALiBi、Rotary 和 APE，并不适合用于下游任务的长度推广。更重要的是，无 PE 的 Transformer 在推理任务中的表现优于其他显式 PE 方法，这意味着使用位置编码实际上可能会损害长度推广的能力。这些发现揭示了位置编码在有效 Transformer 模型开发中的重要作用。

    Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other expli
    
[^36]: 在FPGA上部署AI推理引擎的框架实现

    Implementation of a framework for deploying AI inference engines in FPGAs. (arXiv:2305.19455v1 [physics.ins-det])

    [http://arxiv.org/abs/2305.19455](http://arxiv.org/abs/2305.19455)

    该论文介绍了SLAC神经网络库(SNL)框架，这是一个基于FPGA的机器学习部署框架，优化延迟，旨在解决高速率探测器数据处理的挑战。

    

    LCLS2自由电子激光器将产生高达1MHz的x射线脉冲，这将需要新的超高速率（UHR）探测器，以在高达100 kHz的速率下操作，并生成高达1 TB/s的数据吞吐量。机器学习已经展示了消化大型数据集以提取相关洞见的潜力，但是当前的实现显示出实时数据降低目标所需的延迟太高。为此，SLAC致力于创建一个软件框架，将ML的结构翻译为部署在数据链的边缘（靠近仪器）的FPGA上。该框架利用了Xilinx的HLS框架，提供了一个模仿开源Keras接口到TensorFlow库的API。这个SLAC神经网络库（SNL）框架设计了一个流式数据方法，以优化延迟。

    The LCLS2 Free Electron Laser FEL will generate xray pulses to beamline experiments at up to 1Mhz These experimentals will require new ultrahigh rate UHR detectors that can operate at rates above 100 kHz and generate data throughputs upwards of 1 TBs a data velocity which requires prohibitively large investments in storage infrastructure Machine Learning has demonstrated the potential to digest large datasets to extract relevant insights however current implementations show latencies that are too high for realtime data reduction objectives SLAC has endeavored on the creation of a software framework which translates MLs structures for deployment on Field Programmable Gate Arrays FPGAs deployed at the Edge of the data chain close to the instrumentation This framework leverages Xilinxs HLS framework presenting an API modeled after the open source Keras interface to the TensorFlow library This SLAC Neural Network Library SNL framework is designed with a streaming data approach optimizing t
    
[^37]: 动态稀疏是通道级稀疏的学习者

    Dynamic Sparsity Is Channel-Level Sparsity Learner. (arXiv:2305.19454v1 [cs.LG])

    [http://arxiv.org/abs/2305.19454](http://arxiv.org/abs/2305.19454)

    本文提出了一种名为Channel-aware dynamic sparse (Chase)的方法，使用端到端训练实现了GPU友好的通道级别稀疏，不需要任何特殊操作，并且可以直接在通用硬件上加速，显著减小模型大小，同时保持性能。

    

    稀疏训练由于在整个训练过程和推理中具有诱人的节省能力而受到机器学习的广泛关注。动态稀疏训练(DST)作为一种领先的稀疏训练方法，可以从零开始训练深度神经网络，以达到与密集对应物性能相匹配的高稀疏性能。然而，大多数DST之前的研究都表明它们的有效性是在高度不规则的稀疏模式下的非结构化稀疏性上，这在常见硬件上得到了有限的支持。这种限制阻碍了DST在实践中的使用。在本文中，我们提出了一种名为通道感知动态稀疏（Chase）的方法，它将非结构化动态稀疏的性能转换为适合GPU友好的通道级别稀疏，在一个端到端的训练过程中实现，而不需要任何特殊的操作。所得到的小型稀疏网络可以直接通过通用硬件加速，而无需使用专用的稀疏硬件加速器。我们在各种数据集上的实验结果表明，Chase可以在深度神经网络上实现高通道级稀疏性，同时保持其性能，并显着减小模型的大小。

    Sparse training has received an upsurging interest in machine learning due to its tantalizing saving potential for the entire training process as well as inference. Dynamic sparse training (DST), as a leading sparse training approach, can train deep neural networks at high sparsity from scratch to match the performance of their dense counterparts. However, most if not all DST prior arts demonstrate their effectiveness on unstructured sparsity with highly irregular sparse patterns, which receives limited support in common hardware. This limitation hinders the usage of DST in practice. In this paper, we propose Channel-aware dynamic sparse (Chase), which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity (not fine-grained N:M or group sparsity) during one end-to-end training process, without any ad-hoc operations. The resulting small sparse networks can be directly accelerated by commodity hardware, without using a
    
[^38]: 两个失真世界的最优投票规则

    Best of Both Distortion Worlds. (arXiv:2305.19453v1 [cs.GT])

    [http://arxiv.org/abs/2305.19453](http://arxiv.org/abs/2305.19453)

    本文研究了如何在只能访问序数偏好的情况下最大化基数偏好的方法，先前的工作在两个失真度中研究了功利主义失真和度量失真，提出了一个新的混合失真度量方法。

    

    本文研究设计投票规则的问题，该规则将$n$个代理人对$m$个备选方案的序列偏好作为输入并输出单个方案，目的是优化代理人的整体幸福感。投票规则的输入是每个代理人对备选方案的排名，代理人有更精细的(基数)偏好，可以捕捉他们对某个备选方案的偏好程度。为了量化投票规则在只能访问序数偏好的情况下，可以最大化基数偏好的程度，先前的工作使用失真度量，即投票规则性能与基数偏好下的最佳性能之间的最坏近似比。投票规则的失真度方面的研究主要分为两个世界：功利主义失真和度量失真。在前者中，代理人的基数偏好与一般效用相对应，

    We study the problem of designing voting rules that take as input the ordinal preferences of $n$ agents over a set of $m$ alternatives and output a single alternative, aiming to optimize the overall happiness of the agents. The input to the voting rule is each agent's ranking of the alternatives from most to least preferred, yet the agents have more refined (cardinal) preferences that capture the intensity with which they prefer one alternative over another. To quantify the extent to which voting rules can optimize over the cardinal preferences given access only to the ordinal ones, prior work has used the distortion measure, i.e., the worst-case approximation ratio between a voting rule's performance and the best performance achievable given the cardinal preferences.  The work on the distortion of voting rules has been largely divided into two worlds: utilitarian distortion and metric distortion. In the former, the cardinal preferences of the agents correspond to general utilities and
    
[^39]: 更大、更好、更快：具有人类效率的人类级Atari游戏

    Bigger, Better, Faster: Human-level Atari with human-level efficiency. (arXiv:2305.19452v1 [cs.LG])

    [http://arxiv.org/abs/2305.19452](http://arxiv.org/abs/2305.19452)

    引入BBF基于价值函数的RL代理，在Atari 100K基准测试上实现超人类表现，具有人类效率，提出了在样本高效RL研究的ALE中更新目标的可能。

    

    我们引入了一个名为BBF的基于价值函数的强化学习代理来实现Atari 100K基准测试的超人类表现。BBF依靠神经网络的价值估计扩展以及其他设计选择，在遵循样本高效的情况下，能够实现这种扩展。我们对这些设计选择进行了广泛的分析并为未来的工作提供了洞见。最后，我们讨论了更新ALE上样本高效的RL研究目标的问题。我们将我们的代码和数据公开在https://github.com/google-research/google-research/tree/master/bigger_better_faster。

    We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive analyses of these design choices and provide insights for future work. We end with a discussion about updating the goalposts for sample-efficient RL research on the ALE. We make our code and data publicly available at https://github.com/google-research/google-research/tree/master/bigger_better_faster.
    
[^40]: 来自自我本体对象游戏的自监督视觉学习的计算机模型

    A Computational Account Of Self-Supervised Visual Learning From Egocentric Object Play. (arXiv:2305.19445v1 [cs.CV])

    [http://arxiv.org/abs/2305.19445](http://arxiv.org/abs/2305.19445)

    本文研究了如何利用等同不同视角的学习信号支持视觉学习，并发现通过等同一个对象的不同视角学习得到的表示可以提高下游图像分类精度。

    

    儿童发展研究表明，处理物理对象的体验对许多认知能力，包括视觉学习，有益。这种体验的一个特征是学习者从几个不同的视角看同一对象。本文研究了如何利用等同不同视角的学习信号 -- 例如将单个对象的不同视图分配相似的表示 -- 支持强大的视觉学习。我们使用了Toybox数据集，其中包含人类操作不同对象的自我中心视频，并使用计算机视觉框架进行自我监督对比学习实验。我们发现，通过等同一个对象的不同物理视角学习得到的表示有助于下游图像分类精度的提高。进一步的实验表明，这种性能提高对视角之间的差异的变化具有鲁棒性，并且这种益处能够转化到多种不同的图像分类任务中。

    Research in child development has shown that embodied experience handling physical objects contributes to many cognitive abilities, including visual learning. One characteristic of such experience is that the learner sees the same object from several different viewpoints. In this paper, we study how learning signals that equate different viewpoints -- e.g., assigning similar representations to different views of a single object -- can support robust visual learning. We use the Toybox dataset, which contains egocentric videos of humans manipulating different objects, and conduct experiments using a computer vision framework for self-supervised contrastive learning. We find that representations learned by equating different physical viewpoints of an object benefit downstream image classification accuracy. Further experiments show that this performance improvement is robust to variations in the gaps between viewpoints, and that the benefits transfer to several different image classificati
    
[^41]: 自动驾驶中的超车场景数据和知识

    Data and Knowledge for Overtaking Scenarios in Autonomous Driving. (arXiv:2305.19421v1 [cs.RO])

    [http://arxiv.org/abs/2305.19421](http://arxiv.org/abs/2305.19421)

    本文提出了一个新的超车场景数据集和知识表示模型，以帮助自动驾驶车辆进行规划和控制。

    

    自动驾驶已成为人工智能中最流行的研究方向之一。自动驾驶车辆被理解为一个系统，它结合了感知、决策制定、规划和控制等任务，所有这些任务都需要车辆收集周围的数据，以便做出良好的决策和行动。本文的贡献在于提出了一个专注于超车场景的新的合成数据集，以及一个超车动作的知识表示模型，除了用于感知和决策制定任务之外，还可以用于规划和控制算法的开发。

    Autonomous driving has become one of the most popular research topics within Artificial Intelligence. An autonomous vehicle is understood as a system that combines perception, decision-making, planning, and control. All of those tasks require that the vehicle collects surrounding data in order to make a good decision and action. In particular, the overtaking maneuver is one of the most critical actions of driving. The process involves lane changes, acceleration and deceleration actions, and estimation of the speed and distance of the vehicle in front or in the lane in which it is moving. Despite the amount of work available in the literature, just a few handle overtaking maneuvers and, because overtaking can be risky, no real-world dataset is available. This work contributes in this area by presenting a new synthetic dataset whose focus is the overtaking maneuver. We start by performing a thorough review of the state of the art in autonomous driving and then explore the main datasets f
    
[^42]: 大核与变形金刚网络，哪个更适合作为ConvNet的教师？

    Are Large Kernels Better Teachers than Transformers for ConvNets?. (arXiv:2305.19412v1 [cs.CV])

    [http://arxiv.org/abs/2305.19412](http://arxiv.org/abs/2305.19412)

    本文揭示了新出现的大核ConvNets有效地用作小核ConvNets知识蒸馏（KD）的教师的优点。

    

    本文揭示了新出现的大核卷积神经网络（ConvNets）的一种新优点：作为小核ConvNets在知识蒸馏（KD）中的教师。虽然变形金刚网络在各个领域的性能已经达到了最先进水平，但小核ConvNets由于卷积运算和紧凑的权重共享，被视为更适合资源有限的应用。KD被广泛用于提高小核ConvNets的性能。然而，以前的研究表明，从变形金刚网络中蒸馏知识（例如全局信息）到小核ConvNets并不是非常有效，可能是因为它们的不同体系结构。我们的研究表明，现代大核ConvNets是小核ConvNets更为有效的教师，因为它们的体系结构更为相似。

    This paper reveals a new appeal of the recently emerged large-kernel Convolutional Neural Networks (ConvNets): as the teacher in Knowledge Distillation (KD) for small-kernel ConvNets. While Transformers have led state-of-the-art (SOTA) performance in various fields with ever-larger models and labeled data, small-kernel ConvNets are considered more suitable for resource-limited applications due to the efficient convolution operation and compact weight sharing. KD is widely used to boost the performance of small-kernel ConvNets. However, previous research shows that it is not quite effective to distill knowledge (e.g., global information) from Transformers to small-kernel ConvNets, presumably due to their disparate architectures. We hereby carry out a first-of-its-kind study unveiling that modern large-kernel ConvNets, a compelling competitor to Vision Transformers, are remarkably more effective teachers for small-kernel ConvNets, due to more similar architectures. Our findings are backe
    
[^43]: FRAMM：针对临床试验招募不足的公平排名和缺失数据的解决方案

    FRAMM: Fair Ranking with Missing Modalities for Clinical Trial Site Selection. (arXiv:2305.19407v1 [cs.AI])

    [http://arxiv.org/abs/2305.19407](http://arxiv.org/abs/2305.19407)

    本文提出了一个深度强化学习框架FRAMM，用于公平的临床试验选址，该框架可以解决数据缺失和优化招募和多样性之间的权衡。在真实的临床试验数据集上，实验结果表明FRAMM能够实现公平的试验选址，并在不降低招募率的情况下提高多样性。

    

    尽管许多努力已经做出来，但性别、种族和少数民族在临床试验中的代表性不足仍然是一个问题，并且削弱了少数民族的治疗效果。本文针对试验选址任务提出FRAMM，这是一个深度强化学习框架，用于公平的试验选址。我们专注于解决影响公平试验选址的两个现实挑战：许多潜在试验场地的数据模式经常不完整，而且试验选址需要同时优化招募和多样性，因为问题必然是二者之间的权衡，唯一可能通过限制招募数量来增加多样性。为了解决缺失数据的挑战，FRAMM具有一个模态编码器和一个掩码交叉关注机制，可处理缺失数据，无需进行数据填充和完整数据进行训练。为了处理优化招募和多样性的需求，FRAMM提出了一种新颖的公平度量和可调节的限制招募机制，以进行权衡。实验结果表明，在真实的临床试验数据集上，FRAMM能够有效实现公平的试验选址，并在不牺牲入选率的情况下提高多样性。

    Despite many efforts to address the disparities, the underrepresentation of gender, racial, and ethnic minorities in clinical trials remains a problem and undermines the efficacy of treatments on minorities. This paper focuses on the trial site selection task and proposes FRAMM, a deep reinforcement learning framework for fair trial site selection. We focus on addressing two real-world challenges that affect fair trial sites selection: the data modalities are often not complete for many potential trial sites, and the site selection needs to simultaneously optimize for both enrollment and diversity since the problem is necessarily a trade-off between the two with the only possible way to increase diversity post-selection being through limiting enrollment via caps. To address the missing data challenge, FRAMM has a modality encoder with a masked cross-attention mechanism for handling missing data, bypassing data imputation and the need for complete data in training. To handle the need fo
    
[^44]: 脑部肿瘤MRI异质结构分割的增量学习

    Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI. (arXiv:2305.19404v1 [cs.CV])

    [http://arxiv.org/abs/2305.19404](http://arxiv.org/abs/2305.19404)

    本文提出了一种“分歧感知”的双流增量学习框架，适用于针对不断演化的目标域数据进行分割任务，解决了分布变化、未见过结构和训练数据缺失等挑战。

    

    通过单源域训练的静态深度学习模型，在不同的解剖结构分割任务中取得了巨大的成功。但是，在不断演化的环境中，静态模型的表现可能会变差，需要适当地更新模型。在增量学习环境下，我们希望能够更新好的静态模型，跟随不断演化的目标域数据（例如，来自不同站点的额外损伤或感兴趣结构），而不会发生灾难性遗忘。然而，这也带来了一些挑战：分布的变化、在初始模型训练期间未见过的其他结构以及源域中缺乏训练数据。为了解决这些挑战，本文致力于以统一的方式逐步演化“即插即用”训练的分割模型，以应对不断增加的解剖分类数据集。具体地，我们首先提出了一种“分歧感知”的双流增量学习框架

    Deep learning (DL) models for segmenting various anatomical structures have achieved great success via a static DL model that is trained in a single source domain. Yet, the static DL model is likely to perform poorly in a continually evolving environment, requiring appropriate model updates. In an incremental learning setting, we would expect that well-trained static models are updated, following continually evolving target domain data -- e.g., additional lesions or structures of interest -- collected from different sites, without catastrophic forgetting. This, however, poses challenges, due to distribution shifts, additional structures not seen during the initial model training, and the absence of training data in a source domain. To address these challenges, in this work, we seek to progressively evolve an ``off-the-shelf" trained segmentation model to diverse datasets with additional anatomical categories in a unified manner. Specifically, we first propose a divergence-aware dual-fl
    
[^45]: 上下文视觉变换器用于强健的表示学习

    Contextual Vision Transformers for Robust Representation Learning. (arXiv:2305.19402v1 [cs.CV])

    [http://arxiv.org/abs/2305.19402](http://arxiv.org/abs/2305.19402)

    上下文视觉变换器(ContextViT)用于生成图像的鲁棒特征表示，引入了一个额外的上下文令牌，可以解释掉特定于组的协变量结构，同时保持跨组共享的核心视觉特征，能够在监督微调、半监督学习以及主动学习等方面得到应用。

    

    本文介绍了一种上下文视觉变换器(ContextViT)的方法，用于生成图像的鲁棒特征表示，特别是针对表现出分组结构的图像，如协变量。ContextViT引入了一个额外的上下文令牌来编码特定于组的信息，允许模型解释掉特定于组的协变量结构，同时保持跨组共享的核心视觉特征。具体而言，在给定输入图像的情况下，Context-ViT将共享相同协变量的图像映射到该上下文令牌，并添加到输入图像令牌中，以捕获将模型条件化为组成员身份的效果。此外，我们还引入了一个上下文推断网络，可以在运行时预测这些令牌，只需要给出一些来自组分布的样本即可，使得ContextViT可以推广到新的测试分布。我们通过各种应用程序说明了ContextViT的性能。在监督微调中，我们证明了将预训练的ViTs与额外的上下文令牌相结合可以提高图像分类基准的准确性。我们还展示了ContextViT可以用于半监督学习，在CIFAR-10数据集上仅使用10%的标记样本即可实现最先进的表现。最后，我们展示了ContextViT可以通过引导选择来自少数群体的更具信息价值的样本，从而提高主动学习的效率。

    We present Contextual Vision Transformers (ContextViT), a method for producing robust feature representations for images exhibiting grouped structure such as covariates. ContextViT introduces an extra context token to encode group-specific information, allowing the model to explain away group-specific covariate structures while keeping core visual features shared across groups. Specifically, given an input image, Context-ViT maps images that share the same covariate into this context token appended to the input image tokens to capture the effects of conditioning the model on group membership. We furthermore introduce a context inference network to predict such tokens on the fly given a few samples from a group distribution, enabling ContextViT to generalize to new testing distributions at inference time. We illustrate the performance of ContextViT through a diverse range of applications. In supervised fine-tuning, we demonstrate that augmenting pre-trained ViTs with additional context 
    
[^46]: LOPO性能预测的RF + clust灵敏度分析

    Sensitivity Analysis of RF+clust for Leave-one-problem-out Performance Prediction. (arXiv:2305.19375v1 [cs.LG])

    [http://arxiv.org/abs/2305.19375](http://arxiv.org/abs/2305.19375)

    本文提出了一种新的方法来解决机器学习中的LOPO问题，通过调整距离加权和引进基于特征的重要性，实验结果表明在预测准确性和推广能力方面比RF + clust更优。

    

    Leave-one-problem-out（LOPO）性能预测需要机器学习（ML）模型将算法的性能从一组训练问题推广到之前未见过的问题上。即使对于最先进的方法，LOPO也是一项非常具有挑战性的任务。在更简单的leave-one-instance-out场景中表现良好的模型通常未能很好地推广到LOPO设置中。为了解决LOPO问题，最近的研究建议使用加权算法性能的随机森林（RF）性能回归模型对标准模型进行扩展，这些算法性能被认为与测试问题相似。更准确地说，在这个RF + clust方法中，权重是根据某些特征空间中问题的距离成比例选择的。在这项工作中，我们通过调整基于特征的重要性对性能回归的距离加权来扩展RF + clust方法。也就是说，我们不再考虑特征空间中的余弦距离，而是考虑加权余弦距离，其中权重是基于RF中的特征重要性度量自动计算的。我们在不同领域的23个数据集上进行了大量实验，以比较RF + clust与所提出方法的性能。结果表明，所提出的方法在预测准确性和对新问题的推广能力方面比RF + clust显着更优，特别是当训练问题的数量较少时。

    Leave-one-problem-out (LOPO) performance prediction requires machine learning (ML) models to extrapolate algorithms' performance from a set of training problems to a previously unseen problem. LOPO is a very challenging task even for state-of-the-art approaches. Models that work well in the easier leave-one-instance-out scenario often fail to generalize well to the LOPO setting. To address the LOPO problem, recent work suggested enriching standard random forest (RF) performance regression models with a weighted average of algorithms' performance on training problems that are considered similar to a test problem. More precisely, in this RF+clust approach, the weights are chosen proportionally to the distances of the problems in some feature space. Here in this work, we extend the RF+clust approach by adjusting the distance-based weights with the importance of the features for performance regression. That is, instead of considering cosine distance in the feature space, we consider a weig
    
[^47]: 视觉概念学习中的组合多样性

    Compositional diversity in visual concept learning. (arXiv:2305.19374v1 [cs.CV])

    [http://arxiv.org/abs/2305.19374](http://arxiv.org/abs/2305.19374)

    本文研究了人类如何利用组合性进行视觉概念学习，开发了一个程序归纳模型来生成候选视觉图形，发现人类和模型都可以进行多样的组合泛化。

    

    人类利用组合性来有效地学习新概念，理解熟悉部件如何组合在一起形成新颖对象。相比之下，流行的计算机视觉模型难以进行相同类型的推理，需要更多数据，并且不如人类具有灵活的泛化能力。本文研究这些人类特有的能力在不同类型的视觉组合中的表现，研究人类如何对具有丰富关系结构的“外星人图形”进行分类和生成。我们还开发了一个贝叶斯程序归纳模型，搜索生成候选视觉图形的最佳程序，利用包含不同组合机制和抽象的大型程序空间。在少样本分类任务中，我们发现人类和程序归纳模型可以进行许多有意义的组合泛化，模型提供了实验数据的强有力解释以及显示人类式组合推理的可解释参数。

    Humans leverage compositionality to efficiently learn new concepts, understanding how familiar parts can combine together to form novel objects. In contrast, popular computer vision models struggle to make the same types of inferences, requiring more data and generalizing less flexibly than people do. Here, we study these distinctively human abilities across a range of different types of visual composition, examining how people classify and generate ``alien figures'' with rich relational structure. We also develop a Bayesian program induction model which searches for the best programs for generating the candidate visual figures, utilizing a large program space containing different compositional mechanisms and abstractions. In few shot classification tasks, we find that people and the program induction model can make a range of meaningful compositional generalizations, with the model providing a strong account of the experimental data as well as interpretable parameters that reveal huma
    
[^48]: 通过挖掘临床笔记中的主题识别心力衰竭患者的表型并预测住院时间

    Mining Themes in Clinical Notes to Identify Phenotypes and to Predict Length of Stay in Patients admitted with Heart Failure. (arXiv:2305.19373v1 [cs.LG])

    [http://arxiv.org/abs/2305.19373](http://arxiv.org/abs/2305.19373)

    本文采用主题建模技术对1200例心力衰竭患者的诊断编码和程序报告中存在的主题进行识别，旨在从中识别心力衰竭的临床表型并预测病人住院时间。

    

    心力衰竭是一种综合征，当心脏无法泵血和输送氧气以支持体内其他器官时出现。识别接受心力衰竭治疗的病人的诊断编码和程序报告中的潜在主题，可以揭示与心力衰竭相关的临床表型，并根据相似的特征对病人进行分组，这也有助于预测病人的住院时间。由于这些临床表型通常具有概率的潜在结构，并且之前没有关于使用概率框架在心力衰竭患者的临床笔记中识别表型和使用数据驱动的基于人工智能的方法预测这些患者的住院时间的研究，因此我们采用自然语言处理技术——主题建模，对伊利诺伊大学医院1200例心力衰竭患者的诊断编码和程序报告中存在的主题进行识别。

    Heart failure is a syndrome which occurs when the heart is not able to pump blood and oxygen to support other organs in the body. Identifying the underlying themes in the diagnostic codes and procedure reports of patients admitted for heart failure could reveal the clinical phenotypes associated with heart failure and to group patients based on their similar characteristics which could also help in predicting patient outcomes like length of stay. These clinical phenotypes usually have a probabilistic latent structure and hence, as there has been no previous work on identifying phenotypes in clinical notes of heart failure patients using a probabilistic framework and to predict length of stay of these patients using data-driven artificial intelligence-based methods, we apply natural language processing technique, topic modeling, to identify the themes present in diagnostic codes and in procedure reports of 1,200 patients admitted for heart failure at the University of Illinois Hospital 
    
[^49]: 移动应用中的视觉Transformer: 一份简短调查

    Vision Transformers for Mobile Applications: A Short Survey. (arXiv:2305.19365v1 [cs.CV])

    [http://arxiv.org/abs/2305.19365](http://arxiv.org/abs/2305.19365)

    本文调查了在权衡精度和推理延迟的前提下，哪些因素使得Vision Transformer适用于移动部署，并研究了专门设计为移动应用的ViT的体系结构和挑战。

    

    视觉Transformer (ViT) 在许多计算机视觉任务上展现了最新的性能。然而，部署这些大规模的ViT对许多移动设备来说是资源消耗大且不可能的。本文提出了相反的问题：在精度和推理延迟的权衡考虑下，ViT可以多小，以使其适用于移动部署？我们研究了一些专为移动应用程序设计的ViT，并观察到它们修改了Transformer的架构或是围绕CNN和Transformer的组合而构建。最近的一些工作还尝试创建稀疏ViT网络并提出了关于注意力模块的替代方案。本文研究了这些架构，确定了挑战并分析了是什么使得Vision Transformer适用于移动应用。我们旨在为未来的研究方向提供基线，并希望打下移动计算机视觉领域的基础。

    Vision Transformers (ViTs) have demonstrated state-of-the-art performance on many Computer Vision Tasks. Unfortunately, deploying these large-scale ViTs is resource-consuming and impossible for many mobile devices. While most in the community are building for larger and larger ViTs, we ask a completely opposite question: How small can a ViT be within the tradeoffs of accuracy and inference latency that make it suitable for mobile deployment? We look into a few ViTs specifically designed for mobile applications and observe that they modify the transformer's architecture or are built around the combination of CNN and transformer. Recent work has also attempted to create sparse ViT networks and proposed alternatives to the attention module. In this paper, we study these architectures, identify the challenges and analyze what really makes a vision transformer suitable for mobile applications. We aim to serve as a baseline for future research direction and hopefully lay the foundation to ch
    
[^50]: 稳健的各向异性正则化

    Stable Anisotropic Regularization. (arXiv:2305.19358v1 [cs.CL])

    [http://arxiv.org/abs/2305.19358](http://arxiv.org/abs/2305.19358)

    本文提出了一种新颖的正则化方法I-STAR，可以增加模型的稳定性，提高性能，并改善自然语言处理中的组合表示问题。

    

    鉴于大型语言模型（LLMs）的成功，研究模型激活的属性已引起了相当大的兴趣。文献普遍认为LLMs表示由少数具有极高方差和幅度的“异常维度”主导。自然语言处理（NLP）中的几项研究试图减轻这些异常维度的影响，并迫使LLMs成为各向同性（即在嵌入空间中所有维度具有均匀方差）的。各向同性被认为是LLMs的一种理想属性，可以提高模型性能并更加贴近人类直觉的文本表示。然而，关于NLP中各向同性的许多观点都是基于嵌入的平均余弦相似度，最近已经表明这是一种有缺陷的各向同性度量。在本文中，我们提出了I-STAR：基于IsoScore$^{\star}$的稳定各向异性正则化，这是一种新颖的正则化方法，可以用于增加模型的稳定性并提高性能。

    Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few ``outlier dimensions'' with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many of the claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore$^{\star}$-based STable Anisotropic Regularization, a novel regularization method that can be used to incre
    
[^51]: 

    Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses. (arXiv:2305.19339v1 [cs.CL])

    [http://arxiv.org/abs/2305.19339](http://arxiv.org/abs/2305.19339)

    

    

    

    A human decision-maker benefits the most from an AI assistant that corrects for their biases. For problems such as generating interpretation of a radiology report given findings, a system predicting only highly likely outcomes may be less useful, where such outcomes are already obvious to the user. To alleviate biases in human decision-making, it is worth considering a broad differential diagnosis, going beyond the most likely options. We introduce a new task, "less likely brainstorming," that asks a model to generate outputs that humans think are relevant but less likely to happen. We explore the task in two settings: a brain MRI interpretation generation setting and an everyday commonsense reasoning setting. We found that a baseline approach of training with less likely hypotheses as targets generates outputs that humans evaluate as either likely or irrelevant nearly half of the time; standard MLE training is not effective. To tackle this problem, we propose a controlled text generat
    
[^52]: SheetCopilot: 通过大型语言模型将软件生产力提升到新的水平

    SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models. (arXiv:2305.19308v1 [cs.SE])

    [http://arxiv.org/abs/2305.19308](http://arxiv.org/abs/2305.19308)

    本研究提出了一个基于大型语言模型（LLMs）的代理程序SheetCopilot，该程序可以通过自然语言指导软件执行电子表格数据处理等任务。该程序设计了一组抽象的电子表格软件功能原子动作以及基于状态机的任务规划框架，实现了LLMs与电子表格的鲁棒交互，可以单次正确完成44.3％的任务。

    

    计算机终端用户花费了数十亿小时完成诸如表格数据处理和项目时间轴调度等日常任务。这些任务大多是重复性的和容易出错的，然而大多数终端用户缺乏自动化这些繁琐工作的技能。随着大型语言模型（LLMs）的出现，用自然语言用户请求指导软件成为了一个可达成的目标。在本研究中，我们提出了一个SheetCopilot代理，该代理接受自然语言任务和控制电子表格以满足要求。我们提出了一组原子动作作为电子表格软件功能的抽象。我们进一步设计了基于状态机的任务规划框架，以便LLMs与电子表格进行鲁棒的交互。我们策划了一个包含221种电子表格控制任务的代表性数据集，并建立了一个完全自动化的评估管道，以严格评估LLMs在软件控制任务中的能力。我们的SheetCopilot单次正确完成44.3％的任务。

    Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill of automating away these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent which takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\% of tasks for a single 
    
[^53]: 一张图值得一比特的差异性：当图的对比学习遇到脉冲神经网络时。

    A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks. (arXiv:2305.19306v1 [cs.NE])

    [http://arxiv.org/abs/2305.19306](http://arxiv.org/abs/2305.19306)

    本文提出了SpikeGCL，一种用二值化1比特表示来提高效率和节约资源的图对比学习框架，实验结果表明可以以近32倍的表示存储压缩实现高效学习。

    

    虽然对比自监督学习已经成为图神经网络的事实上的学习范式，但对高任务准确性的追求需要大的隐藏维度来学习信息丰富、有区别性的全精度表示，这引发了对计算、存储和能源消耗负担（在现实世界应用中大多被忽略）的担忧。本文探索了一种有前途的方向，即用脉冲神经网络（SNNs）进行图的对比学习（GCL），利用稀疏和二元特性来学习更具生物可行性和紧凑性的表示。我们提出了SpikeGCL，一种学习图的二值化1比特表示的新型GCL框架，平衡了效率和性能之间的权衡。我们提供了理论保证，证明SpikeGCL在表达能力上与其全精度对应物具有可比性。实验结果表明，通过将表示存储压缩近32倍，SpikeGCL在保持高准确性的同时可以实现高效的学习。

    While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of high task accuracy requires a large hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation, memory footprint, and energy consumption burden (largely overlooked) for real-world applications. This paper explores a promising direction for graph contrastive learning (GCL) with spiking neural networks (SNNs), which leverage sparse and binary characteristics to learn more biologically plausible and compact representations. We propose SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance. We provide theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts. Experimental results demonstrate that, with nearly 32x representation storage compressio
    
[^54]: MLOps：企业级机器学习迈进一步

    MLOps: A Step Forward to Enterprise Machine Learning. (arXiv:2305.19298v1 [cs.SE])

    [http://arxiv.org/abs/2305.19298](http://arxiv.org/abs/2305.19298)

    本文讨论了机器学习运维（MLOps）在企业级机器学习中的重要性和适用性，并详细解释了MLOps工作流程、自动化流程的不同成熟度水平以及各种底层技术。最后，用一个物体检测服务的企业级MLOps项目的详细示例来解释技术在现实场景中的工作流程。

    

    机器学习运维（MLOps）正成为希望利用人工智能和机器学习模型获益的企业非常关键的一部分。本研究详细回顾了MLOps及其好处、困难、进化以及重要的底层技术，如MLOps框架、Docker、GitHub操作和Kubernetes。详细解释了MLOps工作流程，包括模型设计、部署和操作，以及为模型和数据探索和部署所必需的各种工具。此外，该文章还强调了使用各种自动化流程的不同成熟度水平来端到端生产ML项目的重要性。最低为无自动化，最高为具有完整的CI/CD和CT功能。此外，使用物体检测服务的企业级MLOps项目的详细示例，用于解释技术在现实场景中的工作流程。为此，使用TensorFlow Object Detection API训练了预训练模型，并使用了Flask Web Framework在Web应用程序中托管该模型。

    Machine Learning Operations (MLOps) is becoming a highly crucial part of businesses looking to capitalize on the benefits of AI and ML models. This research presents a detailed review of MLOps, its benefits, difficulties, evolutions, and important underlying technologies such as MLOps frameworks, Docker, GitHub actions, and Kubernetes. The MLOps workflow, which includes model design, deployment, and operations, is explained in detail along with the various tools necessary for both model and data exploration and deployment. This article also puts light on the end-to-end production of ML projects using various maturity levels of automated pipelines, with the least at no automation at all and the highest with complete CI/CD and CT capabilities. Furthermore, a detailed example of an enterprise-level MLOps project for an object detection service is used to explain the workflow of the technology in a real-world scenario. For this purpose, a web application hosting a pre-trained model from Te
    
[^55]: 使用深度强化学习的周界控制: 一种无模型方法实现同质流量优化

    Perimeter Control Using Deep Reinforcement Learning: A Model-free Approach towards Homogeneous Flow Rate Optimization. (arXiv:2305.19291v1 [cs.LG])

    [http://arxiv.org/abs/2305.19291](http://arxiv.org/abs/2305.19291)

    该论文提出了一种基于深度强化学习的模型无周界控制框架，通过交通信号控制代理与模拟车辆进行交互，实现同质流量优化，比传统的基于模型的方法更有效。

    

    周界控制通过控制不同区域之间的交通转移流量，以保证其交通密度低于临界值，从而保持受保护区域内交通高效。现有方法可以分为基于网络传输模型（NTMs）和宏观基本图（MFDs）的模型基方法和无模型方法。虽然基于模型的方法具有更高的数据效率和性能保证，但它们天生容易受到模型偏差和不准确性的影响。此外，现有研究中没有一项研究在微观模拟中使用强化学习进行同质流量优化，微观模拟考虑了空间特征、车辆级别信息和计量实现，这些在宏观模拟中经常被忽略。为了应对这些挑战，我们提出了一种基于深度强化学习（DRL）的无模型周界控制框架。我们的框架使用交通信号控制代理与模拟车辆进行交互，学习如何优化信号时序以控制受保护区域的流入和流出。我们展示了基于DRL的方法在合成和真实交通情况下均能优于传统的基于模型的方法。

    Perimeter control maintains high traffic efficiency within protected regions by controlling transfer flows among regions to ensure that their traffic densities are below critical values. Existing approaches can be categorized as either model-based or model-free, depending on whether they rely on network transmission models (NTMs) and macroscopic fundamental diagrams (MFDs). Although model-based approaches are more data efficient and have performance guarantees, they are inherently prone to model bias and inaccuracy. For example, NTMs often become imprecise for a large number of protected regions, and MFDs can exhibit scatter and hysteresis that are not captured in existing model-based works. Moreover, no existing studies have employed reinforcement learning for homogeneous flow rate optimization in microscopic simulation, where spatial characteristics, vehicle-level information, and metering realizations -- often overlooked in macroscopic simulations -- are taken into account. To circu
    
[^56]: CYRUS足球模拟2D团队在RoboCup 2023中的观察去噪

    Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup 2023. (arXiv:2305.19283v1 [cs.AI])

    [http://arxiv.org/abs/2305.19283](http://arxiv.org/abs/2305.19283)

    CYRUS足球模拟2D团队采用LSTM和DNN提出观察去噪思路，达成RoboCup 2021的冠军。

    

    RoboCup比赛有多个联盟，其中Soccer Simulation 2D联盟是一个主要联盟。Soccer Simulation 2D（SS2D）比赛涉及两个团队，包括11名球员和一名教练互相竞争。比赛期间球员只能与Soccer Simulation服务器通信。本文介绍了CYRUS足球模拟2D团队的最新研究，他们是RoboCup2021的冠军。我们将解释我们的去噪思路，它由长短期记忆网络（LSTM）和深度神经网络（DNN）提供支持。CYRUS团队使用了基于Helios和Gliders开发的CYRUS2D基础代码。

    The RoboCup competitions hold various leagues, and the Soccer Simulation 2D League is a major one among them. Soccer Simulation 2D (SS2D) match involves two teams, including 11 players and a coach, competing against each other. The players can only communicate with the Soccer Simulation Server during the game. This paper presents the latest research of the CYRUS soccer simulation 2D team, the champion of RoboCup 2021. We will explain our denoising idea powered by long short-term memory networks (LSTM) and deep neural networks (DNN). The CYRUS team uses the CYRUS2D base code that was developed based on the Helios and Gliders bases.
    
[^57]: 一种用于传统波斯医学的远程护理系统

    A Telecare System for Use in Traditional Persian Medicine. (arXiv:2305.19282v1 [cs.HC])

    [http://arxiv.org/abs/2305.19282](http://arxiv.org/abs/2305.19282)

    研究提出了一种基于传统波斯医学的远程护理系统，该系统可以利用记录的热分布、体质问卷和定制的脉搏测量设备，评估患者的体质状态并将结果发送给医生，从而减少对波斯医学专家的依赖。

    

    波斯医学（PM）利用手腕温度/湿度和脉搏来确定人体健康状况和体质。然而，诊断可能取决于医师的解释，从而阻碍PM与现代医学方法的结合。本研究提出了一种基于PM的脉搏信号测量和体质检测系统。该系统使用记录的热分布、体质问卷和定制的脉搏测量设备。收集的数据可以通过远程护理系统发送给医生进行解释和处方药物。该系统已在患者护理中进行了临床实施，评估了34名参与者的体质，并记录了手腕、手背和整个面部的热像。研究表明可以将基于PM的脉搏波和其他标准的定制设备纳入远程医疗系统，减少对PM专家的诊断依赖。

    Persian Medicine (PM) uses wrist temperature/humidity and pulse to determine a person's health status and temperament. However, the diagnosis may depend on the physician's interpretation, hindering the combination of PM with modern medical methods. This study proposes a system for measuring pulse signals and temperament detection based on PM. The system uses recorded thermal distribution, a temperament questionnaire, and a customized pulse measurement device. The collected data can be sent to a physician via a telecare system for interpretation and prescription of medications. The system was clinically implemented for patient care, assessed the temperaments of 34 participants, and recorded thermal images of the wrist, back of the hand, and entire face. The study suggests that a customized device for measuring pulse waves and other criteria based on PM can be incorporated into a telemedicine system, reducing the dependency on PM specialists for diagnosis.
    
[^58]: 大型语言模型改善多模态数据诊断阿尔茨海默病

    Large language models improve Alzheimer's disease diagnosis using multi-modality data. (arXiv:2305.19280v1 [cs.LG])

    [http://arxiv.org/abs/2305.19280](http://arxiv.org/abs/2305.19280)

    本研究使用大型语言模型提高对非影像数据的应用能力，并在ADNI数据集上实现了SOTA结果。

    

    在诊断像阿尔茨海默病（AD）这样的艰难病症时，影像是一个重要的参考。非影像患者数据（例如患者信息、遗传数据、药物信息、认知和记忆测试）在诊断中也起着非常重要的作用。然而，由于人工智能模型挖掘这些信息的能力受限，大部分现有模型只能使用多模态影像数据，而不能充分利用非影像数据。我们使用目前非常流行的预训练大型语言模型（LLM），以增强模型使用非影像数据的能力，并在ADNI数据集上实现了SOTA的结果。

    In diagnosing challenging conditions such as Alzheimer's disease (AD), imaging is an important reference. Non-imaging patient data such as patient information, genetic data, medication information, cognitive and memory tests also play a very important role in diagnosis. Effect. However, limited by the ability of artificial intelligence models to mine such information, most of the existing models only use multi-modal image data, and cannot make full use of non-image data. We use a currently very popular pre-trained large language model (LLM) to enhance the model's ability to utilize non-image data, and achieved SOTA results on the ADNI dataset.
    
[^59]: 通过共享感官体验增强人类能力的共生人工智能

    Enhancing Human Capabilities through Symbiotic Artificial Intelligence with Shared Sensory Experiences. (arXiv:2305.19278v1 [cs.HC])

    [http://arxiv.org/abs/2305.19278](http://arxiv.org/abs/2305.19278)

    本文提出了一种共享感官体验的共生人工智能概念，旨在建立人工智能系统和人类用户之间的互惠关系，并个性化提供支持、帮助和增强，同时探讨了隐私、伦理准则和缓解潜在偏见和不平等现象的策略。

    

    人工智能与人类智能的融合一直以来都是科幻小说和学术界的研究课题。本文提出了一种新的人机交互概念，名为共享感官体验的共生人工智能（SAISSE），旨在通过共享感官体验建立人工智能系统和人类用户之间的互惠关系。通过集成多种感官输入通道和处理人类体验，SAISSE促进了强大的人机关系，使人工智能系统能够从个人用户那里学习和适应，提供个性化的支持、帮助和增强。此外，我们还讨论了用于长期增长和发展人工智能系统和其人类用户的存储单元的整合。同时，我们还探讨了AI人类共生中的用户隐私和伦理准则，探讨了潜在的偏见和不平等现象，并提出缓解策略。

    The merging of human intelligence and artificial intelligence has long been a subject of interest in both science fiction and academia. In this paper, we introduce a novel concept in Human-AI interaction called Symbiotic Artificial Intelligence with Shared Sensory Experiences (SAISSE), which aims to establish a mutually beneficial relationship between AI systems and human users through shared sensory experiences. By integrating multiple sensory input channels and processing human experiences, SAISSE fosters a strong human-AI bond, enabling AI systems to learn from and adapt to individual users, providing personalized support, assistance, and enhancement. Furthermore, we discuss the incorporation of memory storage units for long-term growth and development of both the AI system and its human user. As we address user privacy and ethical guidelines for responsible AI-human symbiosis, we also explore potential biases and inequalities in AI-human symbiosis and propose strategies to mitigate
    
[^60]: 作为基于质量的图的记忆：面向AI中仿真人类记忆的概念框架

    Memory as a Mass-based Graph: Towards a Conceptual Framework for the Simulation Model of Human Memory in AI. (arXiv:2305.19274v1 [cs.AI])

    [http://arxiv.org/abs/2305.19274](http://arxiv.org/abs/2305.19274)

    论文提出了一种以质量为基础的记忆图模型，用于 AI 中人类记忆的仿真。该模型可以清晰地区分记忆的地形差异，并提供了一个包含与观察事实相当一致的大脑活动模型。

    

    人工智能中模拟记忆和学习有两种方法：功能主义方法和认知方法。实现第二种方法的必要条件是提供一个包含与观察事实相当一致的大脑活动模型，例如错误和遗忘经历。由于人类记忆具有包括我们的身份、家庭和家乡组成的坚实核心，我们生活中的主要和决定性事件以及我们文化中无数重复和接受的事实，我们越来越容易暴露于遗忘的外围区域的数据也越来越薄弱。因此，提出了一种能够清晰区分地形差异的模型至关重要。在我们提出的模型中，我们将这种地形情况转化为与节点相关的量。结果是一个带有基于质量的节点值的边权重图，其展示了...

    There are two approaches for simulating memory as well as learning in artificial intelligence; the functionalistic approach and the cognitive approach. The necessary condition to put the second approach into account is to provide a model of brain activity that contains a quite good congruence with observational facts such as mistakes and forgotten experiences. Given that human memory has a solid core that includes the components of our identity, our family and our hometown, the major and determinative events of our lives, and the countless repeated and accepted facts of our culture, the more we go to the peripheral spots the data becomes flimsier and more easily exposed to oblivion. It was essential to propose a model in which the topographical differences are quite distinguishable. In our proposed model, we have translated this topographical situation into quantities, which are attributed to the nodes. The result is an edge-weighted graph with mass-based values on the nodes which demo
    
[^61]: 基于大语言模型的特定领域语言生成中的语法提示

    Grammar Prompting for Domain-Specific Language Generation with Large Language Models. (arXiv:2305.19234v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.19234](http://arxiv.org/abs/2305.19234)

    本文提出了一种基于语法提示的方法，使用专用的语法来增强示例，为大型语言模型（LLM）在特定领域的语言生成任务中使用外部知识和特定约束条件进行上下文学习。

    

    大型语言模型（LLM）可以从仅有几个上下文示例中学习执行各种自然语言任务。然而，对于从高度结构化的语言（例如，从语义解析到复杂的特定领域语言）生成字符串，LLM只从少量示例中进行泛化是具有挑战性的。我们探讨了$\textbf{语法提示}$作为一种简单的方法，通过在背科斯-诺尔范式（BNF）中表达的语法来启用LLM使用外部知识和特定领域的约束条件来进行上下文学习。语法提示使用一个专门的语法来增强每个演示示例，该语法足以生成特定的输出示例，其中该专门的语法是全DSL语法的子集。对于推理，LLM首先预测一个给定测试输入的BNF语法，然后根据语法规则生成输出。实验表明，语法提示可以使LLM在特定领域的语言生成任务中表现出色。

    Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We explore $\textbf{grammar prompting}$ as a simple approach for enabling LLMs to use external knowledge and domain-specific constraints, expressed through a grammar expressed in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perfor
    
[^62]: 通过隐藏表示转换实现的可控文本生成

    Controlled Text Generation with Hidden Representation Transformations. (arXiv:2305.19230v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.19230](http://arxiv.org/abs/2305.19230)

    我们提出了CHRT，它是一种可控语言生成框架，通过学习表示转换来修改基础模型的隐藏表示从而获得属性控制。实验证明，CHRT在三个属性上表现均优于所有基线模型，同时最小化了在语言质量上的损失。

    

    我们提出了CHRT(Control Hidden Representation Transformation)，它是一种可控语言生成框架，可以引导大型语言模型生成特定属性的文本(如有毒性文本)。CHRT通过学习表示转换从而修改基础模型的隐藏表示来获得属性控制。我们采用对比学习框架来学习这些表示转换，可以结合使用以获得多属性控制。通过在三个属性上与七个基线模型进行比较，实验证明了CHRT的有效性。CHRT在解毒、正面情感引导和文本简化任务中表现均优于所有基线模型，同时最小化了在语言质量上的损失。此外，我们的方法推断延迟仅比基础模型多0.01秒，是最适合高性能生产环境的方法。我们开放源代码并发布了两个新的数据集，以进一步推动可控文本生成的发展。

    We propose CHRT (Control Hidden Representation Transformation) - a controlled language generation framework that steers large language models to generate text pertaining to certain attributes (such as toxicity). CHRT gains attribute control by modifying the hidden representation of the base model through learned transformations. We employ a contrastive-learning framework to learn these transformations that can be combined to gain multi-attribute control. The effectiveness of CHRT is experimentally shown by comparing it with seven baselines over three attributes. CHRT outperforms all the baselines in the task of detoxification, positive sentiment steering, and text simplification while minimizing the loss in linguistic qualities. Further, our approach has the lowest inference latency of only 0.01 seconds more than the base model, making it the most suitable for high-performance production environments. We open-source our code and release two novel datasets to further propel controlled l
    
[^63]: 利用树宽及其限制求解投影模型计数

    Solving Projected Model Counting by Utilizing Treewidth and its Limits. (arXiv:2305.19212v2 [cs.CC] UPDATED)

    [http://arxiv.org/abs/2305.19212](http://arxiv.org/abs/2305.19212)

    本文提出一种用于解决投影模型计数问题的算法，该算法利用输入实例原始图的小树宽，算法运行时间为O(2^2k+4n2)，其中k是树宽，n是实例的输入大小，我们建立了PMC被限制树宽算法的下界，其运行时限界已达到渐进控制。

    

    本文介绍了一种新的算法来解决投影模型计数（PMC）。PMC要求按照给定的投影变量集合计算布尔公式的解的数量，其中当限制为投影变量时，多个解相同时只计算一个解。我们的算法利用输入实例原始图的小树宽，这个观察结果表示树宽是最突出的结构参数之一。更确切地说，它的运行时间为O(2^2k+4n2)，其中k是树宽，n是实例的输入大小。 换句话说，当给定树宽参数时，我们得到PMG问题固定参数可解。此外，我们考虑指数时间假设（ETH），并建立了PMC被限制树宽算法的下界，从而得到了我们算法的渐进紧密运行时限界。虽然上述算法可以作为第一个理论上界，但...

    In this paper, we introduce a novel algorithm to solve projected model counting (PMC). PMC asks to count solutions of a Boolean formula with respect to a given set of projection variables, where multiple solutions that are identical when restricted to the projection variables count as only one solution. Inspired by the observation that the so-called "treewidth" is one of the most prominent structural parameters, our algorithm utilizes small treewidth of the primal graph of the input instance. More precisely, it runs in time O(2^2k+4n2) where k is the treewidth and n is the input size of the instance. In other words, we obtain that the problem PMC is fixed-parameter tractable when parameterized by treewidth. Further, we take the exponential time hypothesis (ETH) into consideration and establish lower bounds of bounded treewidth algorithms for PMC, yielding asymptotically tight runtime bounds of our algorithm. While the algorithm above serves as a first theoretical upper bound and althou
    
[^64]: StyleAvatar3D：利用图像-文本扩散模型生成高保真3D头像的新方法

    StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation. (arXiv:2305.19012v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.19012](http://arxiv.org/abs/2305.19012)

    本文提出了一种新方法，利用图像-文本扩散模型进行数据生成和基于生成对抗网络（GAN）的3D生成网络进行训练，以生成高质量、风格化的3D头像，同时在生成过程中增加了现有3D模型中提取的姿势来引导多视角的图像生成，并提出了视点特定提示、粗到细的GAN鉴别器以及属性相关提示等方法以增加多样性。

    

    最近图像-文本扩散模型的进展刺激了大规模3D生成模型的研究兴趣。然而，有限的多样化3D资源的可用性对学习提出了重大挑战。在本文中，我们提出了一种新方法，利用预训练的图像-文本扩散模型进行数据生成和基于生成对抗网络（GAN）的3D生成网络进行训练，以生成高质量、风格化的3D头像。我们的方法利用图像-文本扩散模型提供的完整外观和几何先验生成各种风格的多视角头像图像。在数据生成过程中，我们利用现有3D模型中提取的姿势来引导多视角图像的生成。为了解决数据中姿势和图像的不对齐，我们研究了视点特定提示并开发了一个粗到细的GAN鉴别器进行训练。我们还深入研究了属性相关提示以增加多样性。

    The recent advancements in image-text diffusion models have stimulated research interest in large-scale 3D generative models. Nevertheless, the limited availability of diverse 3D resources presents significant challenges to learning. In this paper, we present a novel method for generating high-quality, stylized 3D avatars that utilizes pre-trained image-text diffusion models for data generation and a Generative Adversarial Network (GAN)-based 3D generation network for training. Our method leverages the comprehensive priors of appearance and geometry offered by image-text diffusion models to generate multi-view images of avatars in various styles. During data generation, we employ poses extracted from existing 3D models to guide the generation of multi-view images. To address the misalignment between poses and images in data, we investigate view-specific prompts and develop a coarse-to-fine discriminator for GAN training. We also delve into attribute-related prompts to increase the dive
    
[^65]: IDToolkit: 用于纳米光子学反向设计算法基准测试和开发的工具箱

    IDToolkit: A Toolkit for Benchmarking and Developing Inverse Design Algorithms in Nanophotonics. (arXiv:2305.18978v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.18978](http://arxiv.org/abs/2305.18978)

    提出了一个纳米光子学器件反向设计基准测试，以帮助人们进行易于理解和可重复的科学设计。开发了一个开源工具箱IDToolkit，其中包含了模拟和优化器模块以及后处理结果的函数，可用于与基准方法比较算法。

    

    帮助人类进行科学设计是人工智能（AI）和机器学习（ML）最令人兴奋的领域之一，因为它们具有发现新药物、设计新材料和化合物等潜力。然而，科学设计通常需要熟悉领域知识的专业技能，这些对于AI研究人员来说并不熟悉。此外，科学研究需要专业的实验和评估技能。这些障碍阻碍了AI研究人员开发专门用于科学设计的方法。为迈向易于理解和可重复研究的科学设计，我们提出了一个用于纳米光子学器件反向设计的基准测试，可在计算上和准确地验证。具体而言，我们实现了三种不同的纳米光子学设计问题，分别是辐射冷却器，适用于热光伏选择性发射体以及结构色滤光器，它们在设计参数空间、复杂度和物理性质上都不同。此外，我们开发了IDToolkit，一个开源的Python工具箱，以为用户提供与基准方法比较其算法的简单接口。IDToolkit包含了反向设计所需的几个模块，例如模拟模块和优化器模块，以及后处理结果的函数。我们希望所提出的基准测试和工具包将加速纳米光子学科学设计新方法的发展。

    Aiding humans with scientific designs is one of the most exciting of artificial intelligence (AI) and machine learning (ML), due to their potential for the discovery of new drugs, design of new materials and chemical compounds, etc. However, scientific design typically requires complex domain knowledge that is not familiar to AI researchers. Further, scientific studies involve professional skills to perform experiments and evaluations. These obstacles prevent AI researchers from developing specialized methods for scientific designs. To take a step towards easy-to-understand and reproducible research of scientific design, we propose a benchmark for the inverse design of nanophotonic devices, which can be verified computationally and accurately. Specifically, we implemented three different nanophotonic design problems, namely a radiative cooler, a selective emitter for thermophotovoltaics, and structural color filters, all of which are different in design parameter spaces, complexity, an
    
[^66]: HiFA: 高保真度的文本到3D图像合成及其先进的扩散引导策略

    HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance. (arXiv:2305.18766v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.18766](http://arxiv.org/abs/2305.18766)

    该论文提出了一种高保真度的文本到3D图像合成方法，并引入了先进的扩散引导策略。通过对NeRF渲染图像进行辅助深度监督和规范化密度场来提高3D几何表示。实验证明该方法优于以前的工作，产生了先进的照片真实感和改进的多视角一致性。

    

    通过优化3D模型，自动文本到3D合成在提升中已经取得了显著进展。现有方法通常依赖于预训练的文本到图像生成模型（如扩散模型），提供神经辐射场（NeRFs）的2D渲染得分并用于优化NeRFs。然而，由于其对3D几何的有限理解，这些方法经常遇到多个视角上的伪影和不一致现象。为了解决这些限制，我们提出了使用扩散先验重新制定优化损失的方法。此外，我们引入了一种新的训练方法，释放了扩散先验的潜力。为了提高3D几何表示，我们对NeRF渲染图像进行辅助深度监督，并规范化NeRF的密度场。大量实验证明了我们的方法优于以前的工作，产生了先进的照片真实感和改进的多视角一致性。

    Automatic text-to-3D synthesis has achieved remarkable advancements through the optimization of 3D models. Existing methods commonly rely on pre-trained text-to-image generative models, such as diffusion models, providing scores for 2D renderings of Neural Radiance Fields (NeRFs) and being utilized for optimizing NeRFs. However, these methods often encounter artifacts and inconsistencies across multiple views due to their limited understanding of 3D geometry. To address these limitations, we propose a reformulation of the optimization loss using the diffusion prior. Furthermore, we introduce a novel training approach that unlocks the potential of the diffusion prior. To improve 3D geometry representation, we apply auxiliary depth supervision for NeRF-rendered images and regularize the density field of NeRFs. Extensive experiments demonstrate the superiority of our method over prior works, resulting in advanced photo-realism and improved multi-view consistency.
    
[^67]: 任务等变图Few-shot学习

    Task-Equivariant Graph Few-shot Learning. (arXiv:2305.18758v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18758](http://arxiv.org/abs/2305.18758)

    本文提出了一种任务等变图Few-shot学习（TEG）框架，利用图神经网络的等变性质来使模型学习可转移的任务适应策略。该方法在各种Few-shot分类基准上展示了最先进的性能。

    

    虽然图神经网络（GNN）在节点分类任务中取得了成功，但其性能严重依赖于每类具有足够标记节点的可用性。在现实情况下，不是所有类都有许多标记节点，模型可能需要分类新类别，使得手动标记变得困难。为了解决这个问题，GNN需要能够用有限数量的标记节点对节点进行分类，称为Few-shot节点分类。先前的基于剧集元学习的方法在Few-shot节点分类中取得了成功，但我们的发现表明仅有多样的训练元任务才能实现最佳性能。为了应对基于元学习的Few-shot学习的挑战，我们提出了一种新的方法，即任务等变图Few-shot学习（TEG）框架。我们的TEG框架通过利用图神经网络的等变性质来使模型学习可转移的任务适应策略。我们在各种Few-shot分类基准上展示了我们提出的方法的有效性，实现了最先进的性能。

    Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strate
    
[^68]: 基于认知图的混合表示学习

    Hybrid Representation Learning via Epistemic Graph. (arXiv:2305.18731v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.18731](http://arxiv.org/abs/2305.18731)

    本论文提出了一种基于认知图的混合表示学习方法，通过将结构化知识与数据样本无缝集成来实现更有效的表示学习，并在多个基准数据集上取得了卓越的性能。

    

    近年来，深度模型在许多视觉任务中取得了显著的成功。但是，它们的性能很大程度上取决于密集的训练样本。相比之下，人类通常执行混合学习，例如自发地将结构化知识用于跨领域识别，或者仅使用极少量的数据样本进行少样本学习。因此，通过无缝地将结构化知识与数据样本集成，实现更有效的表示学习，可以将这种混合学习方法扩展到计算机视觉任务中。然而，由于结构化知识和深度特征（从数据样本中学习）在维度和知识粒度上存在巨大差距，因此这样的混合学习方法仍然是一个巨大的挑战。本文提出了一种新的认知图层（EGLayer），以实现混合学习，从而在深度特征和结构化知识图之间更有效地交换信息。具体来说，EGLayer将深度特征作为输入，并构建知识图，将结构化知识传播到深度特征上。这种混合表示学习方法在几个基准数据集（包括少样本学习和零样本分类任务）上进行了评估，并与最先进的方法相比，取得了卓越的性能。

    In recent years, deep models have achieved remarkable success in many vision tasks. Unfortunately, their performance largely depends on intensive training samples. In contrast, human beings typically perform hybrid learning, e.g., spontaneously integrating structured knowledge for cross-domain recognition or on a much smaller amount of data samples for few-shot learning. Thus it is very attractive to extend hybrid learning for the computer vision tasks by seamlessly integrating structured knowledge with data samples to achieve more effective representation learning. However, such a hybrid learning approach remains a great challenge due to the huge gap between the structured knowledge and the deep features (learned from data samples) on both dimensions and knowledge granularity. In this paper, a novel Epistemic Graph Layer (EGLayer) is developed to enable hybrid learning, such that the information can be exchanged more effectively between the deep features and a structured knowledge gra
    
[^69]: 超越一个模型适用于所有领域：大型语言模型的领域专门化综述

    Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models. (arXiv:2305.18703v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18703](http://arxiv.org/abs/2305.18703)

    本文综述了大型语言模型的领域专门化，包括动机、挑战、方法论和评估指标。此外，还提供了一个特定领域任务和数据集的分类法，对现有的领域自适应和定制技术进行了详细比较，并广泛讨论了这一领域中的未解决问题和未来的发展方向。

    

    大型语言模型（LLM）已经大大推动了自然语言处理（NLP）领域的发展，为广泛应用提供了高度实用、任务无关的基础。LLMs 作为通用任务求解器的巨大潜力，促使人们将其用于特定领域，如医疗保健、金融和教育，并将其用作助手甚至替代特定领域的专家和工具。但是，将LLMs直接应用于特定领域中的复杂问题会遇到许多困难，包括领域数据的异质性、领域知识的复杂性、领域目标的独特性以及约束的多样性。为了填补这种差距，最近几年进行了急剧增加的研究和实践致力于大型语言模型的领域专门化，然而这方面的研究尚未被系统地总结。在这篇综述中，我们对LLMs的领域专门化进行了全面概述，包括动机、挑战、方法论和评估指标。此外，我们提供了一个特定领域任务和数据集的分类法，对现有的领域自适应和定制技术进行了详细比较，并广泛讨论了这一领域中的未解决问题和未来的发展方向。

    Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a ``chatbot'', and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted in very recent years on the domain specialization of LLMs, which, howe
    
[^70]: NUNO: 用于学习非均匀数据 Parametric PDEs 的通用框架

    NUNO: A General Framework for Learning Parametric PDEs with Non-Uniform Data. (arXiv:2305.18694v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18694](http://arxiv.org/abs/2305.18694)

    NUNO 是一个用于处理非均匀数据的高效算子学习框架，在三维 PDE 问题中取得了显著的效果提升。

    

    神经算子已经成为求解偏微分方程中函数空间映射的有力工具。但是，当面临现实世界的物理数据时，这些数据往往分布非常不均匀，使用基于网格的技术（如FFT）很具有挑战性。为了解决这个问题，我们引入了Non-Uniform Neural Operator (NUNO)，这是一个专门用于处理非均匀数据的高效算子学习框架。通过利用基于K-D树的域分解，我们将非均匀数据转换成均匀网格，同时有效地控制插值误差，从而可以与从非均匀数据学习的速度和准确性并行。我们在二维弹性、(2+1)D河道流和三维多物理 heatsink 上进行了大量的实验，这些实验涉及复杂几何结构的三维偏微分方程问题。我们的框架将误差率降低了最多60％，并将训练速度提高了2倍至30倍。代码现在在https://github.com 上可用。

    The neural operator has emerged as a powerful tool in learning mappings between function spaces in PDEs. However, when faced with real-world physical data, which are often highly non-uniformly distributed, it is challenging to use mesh-based techniques such as the FFT. To address this, we introduce the Non-Uniform Neural Operator (NUNO), a comprehensive framework designed for efficient operator learning with non-uniform data. Leveraging a K-D tree-based domain decomposition, we transform non-uniform data into uniform grids while effectively controlling interpolation error, thereby paralleling the speed and accuracy of learning from non-uniform data. We conduct extensive experiments on 2D elasticity, (2+1)D channel flow, and a 3D multi-physics heatsink, which, to our knowledge, marks a novel exploration into 3D PDE problems with complex geometries. Our framework has reduced error rates by up to 60% and enhanced training speeds by 2x to 30x. The code is now available at https://github.co
    
[^71]: Dink-Net: 大规模图形神经聚类方法

    Dink-Net: Neural Clustering on Large Graphs. (arXiv:2305.18405v1 [cs.LG])

    [http://arxiv.org/abs/2305.18405](http://arxiv.org/abs/2305.18405)

    Dink-Net是一个可扩展的大规模图形神经聚类方法，该方法利用了膨胀和收缩的思想来处理百万节点的大图，并在各种基准数据集上优于现有的最先进方法。

    

    近年来，深度图聚类通过深度神经网络将图形的节点进行分组取得了很大的进展，但现有方法无法处理百万节点的大图。为了解决这个问题，我们提出了一种可扩展的Dink-Net深度图聚类方法，利用了膨胀和收缩的思想。首先，通过区分带增强的跟不带增强的节点，自我监督方式学习表示形式。同时，将聚类中心初始化为可学习的神经网络参数。随后，通过对抗性方式最小化提出的集群膨胀损失和集群收缩损失，优化聚类分布。通过这些设置，我们将表示学习和聚类优化两个步骤统一为一个端到端框架，引导网络学习聚类友好的特征。此外，Dink-Net能很好地扩展到大规模的图形上，因为设计的膨胀收缩操作可以有效地减少计算和内存消耗。实验结果表明，Dink-Net在处理百万节点图形的各种基准数据集上优于现有的最先进方法，证明了该方法在大图聚类中的可扩展性和有效性。

    Deep graph clustering, which aims to group the nodes of a graph into disjoint clusters with deep neural networks, has achieved promising progress in recent years. However, the existing methods fail to scale to the large graph with million nodes. To solve this problem, a scalable deep graph clustering method (Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by discriminating nodes, whether being corrupted by augmentations, representations are learned in a self-supervised manner. Meanwhile, the cluster centres are initialized as learnable neural parameters. Subsequently, the clustering distribution is optimized by minimizing the proposed cluster dilation loss and cluster shrink loss in an adversarial manner. By these settings, we unify the two-step clustering, i.e., representation learning and clustering optimization, into an end-to-end framework, guiding the network to learn clustering-friendly features. Besides, Dink-Net scales well to large graphs since the designe
    
[^72]: 基于概念的解释模型敲门技术检验图像分类器的显著统计结论

    Statistically Significant Concept-based Explanation of Image Classifiers via Model Knockoffs. (arXiv:2305.18362v1 [cs.CV])

    [http://arxiv.org/abs/2305.18362](http://arxiv.org/abs/2305.18362)

    该研究提出了一种基于概念的解释模型敲门技术，可以在图像分类任务中找到显著的概念以避免误解，并控制误发现率（FDR）在某个值下，在合成和真实数据实验中得到验证。

    

    基于概念的分类器可以通过人类可理解的概念来解释深度学习模型在图像分类问题上的决策过程。然而，有时概念解释可能会导致误报，将不相关的概念误认为是预测任务的重要因素。我们的目标是找到用于分类的显著概念以防止误解。在本研究中，我们提出了一种方法，使用深度学习模型学习图像概念，然后使用Knockoff样本选择重要概念进行预测，并控制误发现率（FDR）在某个值下。我们在合成和真实数据实验中评估了所提出的方法。结果表明，该方法可以适当地控制FDR，同时选择高度可解释的概念，提高模型的可信度。

    A concept-based classifier can explain the decision process of a deep learning model by human-understandable concepts in image classification problems. However, sometimes concept-based explanations may cause false positives, which misregards unrelated concepts as important for the prediction task. Our goal is to find the statistically significant concept for classification to prevent misinterpretation. In this study, we propose a method using a deep learning model to learn the image concept and then using the Knockoff samples to select the important concepts for prediction by controlling the False Discovery Rate (FDR) under a certain value. We evaluate the proposed method in our synthetic and real data experiments. Also, it shows that our method can control the FDR properly while selecting highly interpretable concepts to improve the trustworthiness of the model.
    
[^73]: 通过大型语言模型实现实用的PCG

    Practical PCG Through Large Language Models. (arXiv:2305.18243v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18243](http://arxiv.org/abs/2305.18243)

    本研究介绍了如何利用语言模型生成游戏房间，在仅有少量数据的情况下，可以生成多达37%的可玩新颖关卡，该技术有助于解决包含许多局部和全局约束的PCG问题。

    

    大型语言模型(LLMs)已经被证明是自然语言处理领域之外的各种领域中非常有用的工具。本研究提供了如何使用LLMs为正在开发中的游戏Metavoidal生成2D游戏房间的实用方向。我们的技术可以通过人类参与的微调，利用GPT-3的能力，仅使用60个手动设计的房间数据，在复杂的游戏场景下，生成37%的可玩新颖关卡，这是针对存在大量局部和全局约束的PCG的。

    Large Language Models (LLMs) have proven to be useful tools in various domains outside of the field of their inception, which was natural language processing. In this study, we provide practical directions on how to use LLMs to generate 2D-game rooms for an under-development game, named Metavoidal. Our technique can harness the power of GPT-3 by Human-in-the-loop fine-tuning which allows our method to create 37% Playable-Novel levels from as scarce data as only 60 hand-designed rooms under a scenario of the non-trivial game, with respect to (Procedural Content Generation) PCG, that has a good amount of local and global constraints.
    
[^74]: 将预测编码理解为自适应信任区域方法

    Understanding Predictive Coding as an Adaptive Trust-Region Method. (arXiv:2305.18188v1 [cs.NE] CROSS LISTED)

    [http://arxiv.org/abs/2305.18188](http://arxiv.org/abs/2305.18188)

    研究将预测编码（PC）作为自适应信任区域（TR）算法的理论模型，并发现它可以比反向传播（BP）更快地逃脱鞍点。

    

    预测编码（PC）是一种类脑本地学习算法，最近被提出在生物相关场景中提供比反向传播（BP）更好的优势。虽然理论研究主要集中在展示PC如何在各种极限中逼近BP，但“自然”的PC的潜在优势尚不完全理解。在本文中，我们将PC开发为使用二阶信息的自适应信任区域（TR）算法的理论模型。

    Predictive coding (PC) is a brain-inspired local learning algorithm that has recently been suggested to provide advantages over backpropagation (BP) in biologically relevant scenarios. While theoretical work has mainly focused on showing how PC can approximate BP in various limits, the putative benefits of "natural" PC are less understood. Here we develop a theory of PC as an adaptive trust-region (TR) algorithm that uses second-order information. We show that the learning dynamics of PC can be interpreted as interpolating between BP's loss gradient direction and a TR direction found by the PC inference dynamics. Our theory suggests that PC should escape saddle points faster than BP, a prediction which we prove in a shallow linear model and support with experiments on deeper networks. This work lays a foundation for understanding PC in deep and wide networks.
    
[^75]: 使用场景图记忆建模动态环境

    Modeling Dynamic Environments with Scene Graph Memory. (arXiv:2305.17537v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.17537](http://arxiv.org/abs/2305.17537)

    本论文提出了一种新的场景图记忆状态表示，结合节点边缘预测器（NEP）的神经网络架构，能够帮助具有行动能力的AI代理在部分可观察动态场景中高效搜索。

    

    在大型环境中，如居室等，寻找物品的具有行动能力的AI代理需要基于部分信息预测物品位置来做出有效决策。我们将其形式化为一种新类型的链路预测问题：部分可观察动态图上的链路预测。我们的图表达了一个场景，其中房间和物品是节点，在边缘中编码它们之间的关系；在每个时间步骤上，代理人仅知道更改图的部分。这种部分可观测性对于现有的链路预测方法构成了挑战，我们进行了解决。我们提出了一种新颖的状态表示 - 场景图记忆（SGM） - 其中包括代理人的累积观察集合，以及一种名为节点边缘预测器（NEP）的神经网络架构，该架构从SGM中提取信息以进行高效搜索。我们在动态房屋模拟器中评估了我们的方法，这是一个新的基准，它按照语义模式创建不同的动态图形。

    Embodied AI agents that search for objects in large environments such as households often need to make efficient decisions by predicting object locations based on partial information. We pose this as a new type of link prediction problem: link prediction on partially observable dynamic graphs. Our graph is a representation of a scene in which rooms and objects are nodes, and their relationships are encoded in the edges; only parts of the changing graph are known to the agent at each timestep. This partial observability poses a challenge to existing link prediction approaches, which we address. We propose a novel state representation -- Scene Graph Memory (SGM) -- with captures the agent's accumulated set of observations, as well as a neural net architecture called a Node Edge Predictor (NEP) that extracts information from the SGM to search efficiently. We evaluate our method in the Dynamic House Simulator, a new benchmark that creates diverse dynamic graphs following the semantic patte
    
[^76]: 递归的诅咒：使用生成数据进行训练会让模型忘记

    The Curse of Recursion: Training on Generated Data Makes Models Forget. (arXiv:2305.17493v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.17493](http://arxiv.org/abs/2305.17493)

    使用生成数据进行训练会导致模型不可逆的缺陷并且使得原始内容分布的尾部消失，这种效应称为模型折叠。我们证明了这种现象在所有学习生成模型中都存在，必须认真对待。

    

    稳定扩散技术革命性地改变了从描述性文本中生成图像的方法。GPT-2、GPT-3(.5)和GPT-4在各种语言任务中表现惊人。ChatGPT将这些语言模型引入了大众视野。大语言模型(LLMs)已经不可避免并将彻底改变在线文本和图像的整个生态系统。本文考虑了未来可能发生的事情。当LLMs占据了在线语言的大部分时，GPT-{n}会发生什么？我们发现，在训练中使用模型生成的内容会导致所得模型中不可逆缺陷，原始内容分布的尾部消失。我们将这种效应称为模型折叠，并显示它可以发生在变分自编码器、高斯混合模型和LLMs中。我们建立了现象背后的理论直觉，并展示了这种现象在所有学习生成模型中的普遍性。我们证明，如果我们要在实践中使用生成数据进行训练，就必须认真对待这一问题。

    Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-{n} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we ar
    
[^77]: HUB: 用持续提示调整引导学习优化器

    HUB: Guiding Learned Optimizers with Continuous Prompt Tuning. (arXiv:2305.16823v1 [cs.LG])

    [http://arxiv.org/abs/2305.16823](http://arxiv.org/abs/2305.16823)

    本文提出了一种名为HUB的混合更新策略，通过结合学习优化器和手工设计的优化器，提高了学习优化器泛化性能。

    

    学习优化器是元学习的关键组成部分，但其在处理未见过的任务和网络架构时有限。为了解决此问题，本文提出了一种基于混合更新策略的优化方法（HUB），该方法受到了大型语言和视觉模型中硬提示调整和结果选择技术的启发。通过将手工设计的优化器作为我们混合方法的第二个组件，我们能够在稳定训练的同时保留学习优化器的好处。

    Learned optimizers are a crucial component of meta-learning. Recent advancements in scalable learned optimizers have demonstrated their superior performance over hand-designed optimizers in various tasks. However, certain characteristics of these models, such as an unstable learning curve, limited ability to handle unseen tasks and network architectures, difficult-to-control behaviours, and poor performance in fine-tuning tasks impede their widespread adoption. To tackle the issue of generalization in scalable learned optimizers, we propose a hybrid-update-based (HUB) optimization strategy inspired by recent advancements in hard prompt tuning and result selection techniques used in large language and vision models. This approach can be easily applied to any task that involves hand-designed or learned optimizer. By incorporating hand-designed optimizers as the second component in our hybrid approach, we are able to retain the benefits of learned optimizers while stabilizing the training
    
[^78]: GPT是否会产生更不准确的翻译?

    Do GPTs Produce Less Literal Translations?. (arXiv:2305.16806v1 [cs.CL])

    [http://arxiv.org/abs/2305.16806](http://arxiv.org/abs/2305.16806)

    本研究比较了GPT和NMT生成翻译的文字积极度差异，发现GPT翻译更不准确，但在MT质量评估指标上表现出相似或更好的分数。

    

    大型语言模型（LLMs），如GPT-3，已经成为通用语言模型，能够处理许多自然语言生成或理解任务。在机器翻译（MT）任务中，已有多项研究探索利用few-shot提示机制从LLMs中引出更好的翻译。然而，人们相对较少地关注这种翻译与标准神经机器翻译（NMT）模型生成翻译的质量差异。本研究从文字对齐和单调性等方面，比较了GPT和NMT生成翻译的文本文字积极度，发现GPT从英语（E-X）翻译的文本更不准确，但在MT质量评估指标上表现出相似或更好的分数。我们证明这一结果在人工评估中也得到了验证。同时，当翻译句子长度增加时，这种差别就尤为显著。

    Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models. In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating senten
    
[^79]: AdaPlanner:自适应规划与语言模型的反馈。 （arXiv：2305.16653v1 [cs.CL]）

    AdaPlanner: Adaptive Planning from Feedback with Language Models. (arXiv:2305.16653v1 [cs.CL])

    [http://arxiv.org/abs/2305.16653](http://arxiv.org/abs/2305.16653)

    LLM代理可以通过Adaplanner自适应改进自己的计划以应对环境反馈，为此提出计划内外的改进策略以及代码风格的LLM提示结构和技能发现机制。

    

    最近的大型语言模型（LLM）展示了在序列决策任务中作为自主代理的潜力。然而，大多数现有方法要么贪婪地采取行动而没有计划，要么依赖于不可适应环境反馈的静态计划。因此，随着问题复杂性和计划水平的增加，LLM代理的顺序决策性能会退化。我们提出了一种闭环方法AdaPlanner，它允许LLM代理根据环境反馈自适应地改进其自动生成的计划。在AdaPlanner中，LLM代理通过计划内和计划外的改进策略自适应地改进其计划。为了减轻幻觉，我们开发了一种代码风格的LLM提示结构，促进了跨各种任务，环境和代理能力的计划生成。此外，我们提出了一种技能发现机制，利用成功的计划作为少量示例，使计划更具普适性。

    Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the
    
[^80]: 自监督学习的逆向工程

    Reverse Engineering Self-Supervised Learning. (arXiv:2305.15614v1 [cs.LG])

    [http://arxiv.org/abs/2305.15614](http://arxiv.org/abs/2305.15614)

    本文逆向工程了自监督学习（SSL）训练表示，发现SSL训练过程中的正则化项本质上促进了样本基于语义标签的聚类。SSL训练的表示与语义类别更加接近，对齐在训练过程中增加，而且在网络深度加深时增加。

    

    自监督学习（SSL）是机器学习中有力的工具，但理解学习表示及其基础机制仍然是一个挑战。本文对SSL训练表示进行了深入的实证分析，包括多种模型、架构和超参数。我们的研究揭示了SSL训练过程的一个有趣方面：它本质上促进了样本基于语义标签的聚类，这令人惊讶的是，这是由SSL目标的正则化项驱动的。这种聚类过程不仅增强了下游分类，而且压缩了数据信息。此外，我们发现SSL训练的表示与语义类别更加接近，而不是随机类别。值得注意的是，我们展示了学习表示与各种层次的语义类别对齐，并且这种对齐在训练过程中增加，而且在网络深度加深时增加。

    Self-supervised learning (SSL) is a powerful tool in machine learning, but understanding the learned representations and their underlying mechanisms remains a challenge. This paper presents an in-depth empirical analysis of SSL-trained representations, encompassing diverse models, architectures, and hyperparameters. Our study reveals an intriguing aspect of the SSL training process: it inherently facilitates the clustering of samples with respect to semantic labels, which is surprisingly driven by the SSL objective's regularization term. This clustering process not only enhances downstream classification but also compresses the data information. Furthermore, we establish that SSL-trained representations align more closely with semantic classes rather than random classes. Remarkably, we show that learned representations align with semantic classes across various hierarchical levels, and this alignment increases during training and when moving deeper into the network. Our findings provid
    
[^81]: 友好的邻居：语境化序列到序列链接预测

    Friendly Neighbors: Contextualized Sequence-to-Sequence Link Prediction. (arXiv:2305.13059v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13059](http://arxiv.org/abs/2305.13059)

    研究提出了一种简单的序列到序列模型KGT5-context，通过加入查询实体的直接邻居信息实现知识图谱链接预测的高性能，并与其他方法相比取得了最先进的表现。

    

    我们提出了 KGT5-context，这是一个用于知识图谱（KG）中链接预测（LP）的简单序列到序列模型。我们的工作在最近的LP模型KGT5的基础上拓展，KGT5利用了KG的文本特征，模型规模小且可扩展，但为了达到好的预测性能，KGT5依赖于与之配合的知识图嵌入模型，该模型本身非常大且使用成本高昂。在这篇短文中，我们通过实验证明，加入语境信息，即关于查询实体的直接邻居信息，可以减轻对独立KGE模型的需求以获得良好的性能。由此产生的KGT5-context模型简单，显著缩小了模型大小，并在我们的实验研究中获得了最先进的性能。

    We propose KGT5-context, a simple sequence-to-sequence model for link prediction (LP) in knowledge graphs (KG). Our work expands on KGT5, a recent LP model that exploits textual features of the KG, has small model size, and is scalable. To reach good predictive performance, however, KGT5 relies on an ensemble with a knowledge graph embedding model, which itself is excessively large and costly to use. In this short paper, we show empirically that adding contextual information - i.e., information about the direct neighborhood of the query entity - alleviates the need for a separate KGE model to obtain good performance. The resulting KGT5-context model is simple, reduces model size significantly, and obtains state-of-the-art performance in our experimental study.
    
[^82]: HICO-DET-SG和V-COCO-SG：新的数据拆分用于评估人-物交互检测中的系统性泛化

    HICO-DET-SG and V-COCO-SG: New Data Splits to Evaluate Systematic Generalization in Human-Object Interaction Detection. (arXiv:2305.09948v1 [cs.CV])

    [http://arxiv.org/abs/2305.09948](http://arxiv.org/abs/2305.09948)

    本论文提出了两个新的HOI检测数据拆分，旨在评估系统性泛化。在新的数据拆分上测试结果表明，HOI检测模型对于未见过的对象和交互组合的泛化十分困难。

    

    人-物交互检测是一种预测图像中人与物品之间交互的任务。在实际场景中，需要对HOI检测模型进行系统性的泛化，即泛化到新的对象和交互组合上，因为训练数据仅可能涵盖所有可能组合的一小部分。然而，据我们所知，没有开放的基准测试或现有工作评估HOI检测中的系统性泛化。为解决这个问题，我们基于HICO-DET和V-COCO数据集创建了两个名为HICO-DET-SG和V-COCO-SG的新的HOI检测数据拆分。我们在新的数据拆分上评估了代表性的HOI检测模型，并观察到与原始数据集上相比测试性能有很大的降低。这个结果表明系统性泛化是HOI检测中一个具有挑战性的目标。我们希望我们的新数据拆分能够鼓励更多的研究朝着这个目标努力。

    Human-Object Interaction (HOI) detection is a task to predict interactions between humans and objects in an image. In real-world scenarios, HOI detection models are required systematic generalization, i.e., generalization to novel combinations of objects and interactions, because it is highly probable that the train data only cover a limited portion of all possible combinations. However, to our knowledge, no open benchmark or existing work evaluates the systematic generalization in HOI detection. To address this issue, we created two new sets of HOI detection data splits named HICO-DET-SG and V-COCO-SG based on HICO-DET and V-COCO datasets. We evaluated representative HOI detection models on the new data splits and observed large degradation in the test performances compared to those on the original datasets. This result shows that systematic generalization is a challenging goal in HOI detection. We hope our new data splits encourage more research toward this goal.
    
[^83]: 基于深度学习的 Type-II 码本：增强 CSI 反馈的新视角

    Deep Learning Empowered Type-II Codebook: New Perspectives for Enhancing CSI Feedback. (arXiv:2305.08081v1 [cs.IT])

    [http://arxiv.org/abs/2305.08081](http://arxiv.org/abs/2305.08081)

    本文提出了两个新视角来改进 Release 17 Type-II 码本中的 CSI 反馈性能：利用深度学习选择特定的角度-延迟域端口以提高精度，通过训练深度神经网络来实现可靠的反馈。实验结果表明，该方法可以显著提高反馈性能。

    

    在频分双工系统中，基于深度学习的信道状态信息（CSI）反馈引起了学术界和工业界的广泛关注。本文重点研究将无线通信标准中的 Type-II 码本与深度学习相结合，以提高 CSI 反馈的性能。与现有基于深度学习的 Release 16 Type-II 码本研究不同，Release 17（R17）中的 Type-II 码本利用上行和下行信道之间角度-延迟域偏振部分互易性选择部分角度-延迟域端口进行测量和反馈下行 CSI，传统深度学习方法的性能由于稀疏结构的缺陷而受到限制。为解决这个问题，我们提出了两个采用深度学习来改进 R17 Type-II 码本的新视角。首先，考虑到上行信道的低信噪比，深度学习被用来精确选择特定的角度-延迟域端口，从而提高 CSI 的精度。其次，我们利用虚拟域和实际域之间的关系，通过训练深度神经网络来实现可靠的反馈。实验结果表明，我们所提出的方法可以显著提高 CSI 的反馈性能，并且具有更好的鲁棒性和通用性。

    Deep learning based channel state information (CSI) feedback in frequency division duplex systems has drawn widespread attention in both academia and industry. In this paper, we focus on integrating the Type-II codebook in the wireless communication standards with deep learning to enhance the performance of CSI feedback. In contrast to the existing deep learning based studies on the Release 16 Type-II codebook, the Type-II codebook in Release 17 (R17) exploits the angular-delay-domain partial reciprocity between uplink and downlink channels to select part of angular-delay-domain ports for measuring and feeding back the downlink CSI, where the performance of deep learning based conventional methods is limited due to the deficiency of sparse structures. To address this issue, we propose two new perspectives of adopting deep learning to improve the R17 Type-II codebook. Firstly, considering the low signal-to-noise ratio of uplink channels, deep learning is utilized to accurately select th
    
[^84]: 达芬奇二重论者：大语言模型中的心身分离和人类学习者中的心身二元论

    Davinci the Dualist: the mind-body divide in large language models and in human learners. (arXiv:2305.07667v1 [cs.AI])

    [http://arxiv.org/abs/2305.07667](http://arxiv.org/abs/2305.07667)

    本研究探究了大语言模型Davinci中的心身分离，发现其具有弱化的二元论倾向。

    

    大量文献表明，人类具有直观的二元论思想，认为精神和身体是不同的存在。过去的研究也表明，二元论是通过学习而出现的（例如Barlev＆Shtulman，2021）。但学习是否足以导致二元论尚不清楚。通过探究Davinci（一种不具有任何内在核心知识的大语言模型）中的心身分离，我们评估了学习的作用。我们发现，Davinci仍然倾向于二元论，并且这种偏见随着学习者的归纳潜力逐渐增加。

    A large literature suggests that people are intuitive Dualists--they consider the mind ethereal, distinct from the body. Past research also shows that Dualism emerges, in part, via learning (e.g., Barlev & Shtulman, 2021). But whether learning is sufficient to give rise to Dualism is unknown.The evidence from human learners does address this question because humans are endowed not only with general learning capacities but also with core knowledge capacities. And recent results suggest that core knowledge begets Dualism (Berent, Theodore & Valencia, 2021; Berent, 2023). To evaluate the role of learning, here, we probe for a mind-body divide in Davinci--a large language model (LLM) that is devoid of any innate core knowledge. We show that Davinci still leans towards Dualism, and that this bias increases systematically with the learner's inductive potential. Thus, davinci (a GPT-3 model) exhibits mild Dualist tendencies, whereas its descendent, text-davinci-003 (a GPT-3.5 model), shows a 
    
[^85]: ImageBind:一个共同嵌入空间绑定所有模态的方法

    ImageBind: One Embedding Space To Bind Them All. (arXiv:2305.05665v1 [cs.CV])

    [http://arxiv.org/abs/2305.05665](http://arxiv.org/abs/2305.05665)

    ImageBind是一种新的跨模态联合嵌入方法，只需要使用图像配对数据就可以将不同模态的数据绑定在一起，并实现跨模态检索、组合和生成等多种应用。

    

    我们提出了ImageBind，这是一种跨越图像、文本、音频、深度、热传感和IMU数据的六种不同模态的联合嵌入方法。我们展示，不需要训练所有配对数据，只需要图像配对数据就足以将这些模态绑定在一起。ImageBind可以利用最近的大规模视觉-语言模型，并通过使用它们与图像的自然配对，将它们的零样本能力扩展到新的模态。它可以实现“开箱即用”的新型应用程序，包括跨模态检索、用算术组合模态、跨模态检测和生成。新型应用随着图像编码器的强度而不断改进，我们在跨模态的零样本识别任务上取得了新的最优成绩，超过了专家监督模型。最后，我们还展示了强的几何识别结果，超过了以前的工作，ImageBind成为了评估视觉模态联合学习的一种新方法。

    We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate visio
    
[^86]: 离散扩散建模下的高效和度数引导图生成

    Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])

    [http://arxiv.org/abs/2305.04111](http://arxiv.org/abs/2305.04111)

    本文提出了EDGE，一种新的离散扩散模型用于生成大型图，并通过删除边来鼓励图的稀疏性。EDGE在每个去噪步骤中仅关注图中一部分节点，并且可以明确地对图的节点度数进行建模。实验表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。

    

    基于扩散的生成图模型已被证明在生成高质量小图方面非常有效。然而，它们需要更可扩展性，以生成包含数千个节点的大图并满足图统计。本文提出了EDGE，一种新的基于扩散的生成图模型，用于生成大型图的生成任务。为了提高计算效率，我们通过在每个时间步长随机删除边来鼓励图的稀疏性，并最终获得一张空白图。EDGE仅在每个去噪步骤中关注图中一部分节点。它比以前的基于扩散的模型更少地进行边预测。此外，EDGE明确地允许对图的节点度数进行建模，进一步提高了模型的性能。实证研究表明，EDGE比竞争方法更有效，并且可以生成具有数千个节点的大型图。它还在生成质量方面优于基准模型。

    Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation qual
    
[^87]: 关于数据子群体间机器学习模型性能的非线性相关性

    On the nonlinear correlation of ML performance between data subpopulations. (arXiv:2305.02995v1 [cs.LG])

    [http://arxiv.org/abs/2305.02995](http://arxiv.org/abs/2305.02995)

    在不同数据子群体间，机器学习模型的内部准确性和外部准确性之间的相关性是非线性的，呈现出“月亮形”的相关性。

    

    理解机器学习模型在不同数据分布下的性能对于可靠的应用至关重要。尽管最新的经验研究认为训练数据内部的准确性和新数据外部的准确性之间存在近乎完美的线性相关性，但我们在各种数据集、模型和训练时期进行了严格的实验和分析，发现在子群体转移下，内部准确性和外部准确性之间的相关性更为微妙，并且在上升阶段存在“月亮形”的相关性（抛物线上升曲线）。

    Understanding the performance of machine learning (ML) models across diverse data distributions is critically important for reliable applications. Despite recent empirical studies positing a near-perfect linear correlation between in-distribution (ID) and out-of-distribution (OOD) accuracies, we empirically demonstrate that this correlation is more nuanced under subpopulation shifts. Through rigorous experimentation and analysis across a variety of datasets, models, and training epochs, we demonstrate that OOD performance often has a nonlinear correlation with ID performance in subpopulation shifts. Our findings, which contrast previous studies that have posited a linear correlation in model performance during distribution shifts, reveal a "moon shape" correlation (parabolic uptrend curve) between the test performance on the majority subpopulation and the minority subpopulation. This non-trivial nonlinear correlation holds across model architectures, hyperparameters, training durations
    
[^88]: Moccasin：神经网络的高效张量重算技术

    Moccasin: Efficient Tensor Rematerialization for Neural Networks. (arXiv:2304.14463v1 [cs.LG])

    [http://arxiv.org/abs/2304.14463](http://arxiv.org/abs/2304.14463)

    本文提出了一种名为Moccasin的新型约束编程形式，用于实现在内存预算下最小化计算图的执行时间，相较于最近的研究，该方法显著提高了效率，并成功应用于神经网络的高效张量重算。

    

    在边缘计算设备上部署和训练神经网络面临许多挑战，其中较低的内存是部署大型神经网络模型时经常遇到的最大限制因素之一。张量重算是解决神经网络训练和推理所需高内存需求的一种方式。本文考虑在内存预算下最小化计算图的执行时间问题。具体来说，我们开发了一种新的约束编程形式，称为Moccasin，其中只有$O(n)$个整数变量，$n$是计算图中节点的数量。这相对于最近文献中提出的具有$O(n^2)$布尔变量的公式提出了显着的改进。我们展示了数值研究结果，表明我们的方法在大规模图上比最近的工作快一个数量级。

    The deployment and training of neural networks on edge computing devices pose many challenges. The low memory nature of edge devices is often one of the biggest limiting factors encountered in the deployment of large neural network models. Tensor rematerialization or recompute is a way to address high memory requirements for neural network training and inference. In this paper we consider the problem of execution time minimization of compute graphs subject to a memory budget. In particular, we develop a new constraint programming formulation called \textsc{Moccasin} with only $O(n)$ integer variables, where $n$ is the number of nodes in the compute graph. This is a significant improvement over the works in the recent literature that propose formulations with $O(n^2)$ Boolean variables. We present numerical studies that show that our approach is up to an order of magnitude faster than recent work especially for large-scale graphs.
    
[^89]: 人工智能的终端用户开发：系统性文献综述

    End-User Development for Artificial Intelligence: A Systematic Literature Review. (arXiv:2304.09863v1 [cs.HC])

    [http://arxiv.org/abs/2304.09863](http://arxiv.org/abs/2304.09863)

    本文综述了终端用户开发（EUD）对人工智能系统的影响，目的是使非技术用户直接以满足其需求的方式参与到人工智能的定义和个性化中。此外，文章还评估了EUD面临的挑战、潜在的好处以及将其整合到整个人工智能开发中的未来影响。

    

    近年来，人工智能在我们社会中变得越来越重要。然而，创建人工智能系统几乎总是IT和人工智能专家的专属权利。本文提出了一种新的方法——终端用户开发（EUD），使得非技术人员可以直接参与定义和个性化人工智能技术，从而增强AI系统的效果。本文通过系统性文献综述阐述了当前EUD对AI系统的发展格局，即如何使用户在没有人工智能和/或编程技能的情况下，定制人工智能的行为来满足他们的需求。此外，本研究还讨论了EUD面对的当前挑战、潜在的好处以及将EUD整合到整个人工智能开发过程中的未来影响。

    In recent years, Artificial Intelligence has become more and more relevant in our society. Creating AI systems is almost always the prerogative of IT and AI experts. However, users may need to create intelligent solutions tailored to their specific needs. In this way, AI systems can be enhanced if new approaches are devised to allow non-technical users to be directly involved in the definition and personalization of AI technologies. End-User Development (EUD) can provide a solution to these problems, allowing people to create, customize, or adapt AI-based systems to their own needs. This paper presents a systematic literature review that aims to shed the light on the current landscape of EUD for AI systems, i.e., how users, even without skills in AI and/or programming, can customize the AI behavior to their needs. This study also discusses the current challenges of EUD for AI, the potential benefits, and the future implications of integrating EUD into the overall AI development process
    
[^90]: 在微调时缓解多模态模型中的错误相关性

    Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning. (arXiv:2304.03916v1 [cs.LG])

    [http://arxiv.org/abs/2304.03916](http://arxiv.org/abs/2304.03916)

    本文提出了一种使用多模态对比损失函数的方法，通过在微调期间检测和明确区分受影响类别的错误属性，缓解多模态模型的错误相关性，同时提高模型精度和指向目标领域的有意义特征。

    

    损害模型泛化能力或导致模型基于错误原因的错误相关性是实际部署面临的主要鲁棒性问题之一。然而，在预训练大型模型期间缓解这些相关性可能成本高昂且不切实际，特别是对于没有高性能计算资源的人来说。本文提出了一种新方法，以解决特定领域的微调期间的错误相关性。针对多模态模型（例如CLIP），所提出的方法利用这些模型中的不同模态来检测并明确区分受影响类别的错误属性，通过表达语言的多模态对比损失函数来实现。我们在CLIP上进行的实验证明和深入的可视化显示，这种介入能够有效地提高模型精度，而不存在错误属性，并将模型指向目标领域的有意义特征。

    Spurious correlations that degrade model generalization or lead the model to be right for the wrong reasons are one of the main robustness concerns for real-world deployments. However, mitigating these correlations during pre-training for large-scale models can be costly and impractical, particularly for those without access to high-performance computing resources. This paper proposes a novel approach to address spurious correlations during fine-tuning for a given domain of interest. With a focus on multi-modal models (e.g., CLIP), the proposed method leverages different modalities in these models to detect and explicitly set apart spurious attributes from the affected class, achieved through a multi-modal contrastive loss function that expresses spurious relationships through language. Our experimental results and in-depth visualizations on CLIP show that such an intervention can effectively i) improve the model's accuracy when spurious attributes are not present, and ii) directs the 
    
[^91]: cTBL：增强大型语言模型用于对话表格

    cTBL: Augmenting Large Language Models for Conversational Tables. (arXiv:2303.12024v1 [cs.CL])

    [http://arxiv.org/abs/2303.12024](http://arxiv.org/abs/2303.12024)

    本论文提出了一种称为cTBL的方法，可以从表格中检索信息，并生成具有检索信息支撑的对话响应，其中使用了转换器编码器嵌入进行浓密表检索，可以获得更好的性能。

    

    多模态对话人工智能中一个开放的挑战是如何从文本和非文本来源中增强大型语言模型以进行多轮对话。为了解决这个问题，本文引入了Conversation Table (cTBL)，这是一种三步编码器-解码器方法，用于检索表格信息并生成基于检索信息的对话响应。cTBL使用转换器编码器嵌入进行浓密表检索，并在HyrbiDialogue数据集Top-1和Top-3准确性上相对于稀疏检索提高了最多5%。此外，cTBL使用编码器和解码器模型进行表格知识检索，在HyrbiDialogue上产生了最高46%的ROUGE分数相对改进，并实现了更好的人工评估响应生成。

    An open challenge in multimodal conversational AI requires augmenting large language models with information from textual and non-textual sources for multi-turn dialogue. To address this problem, this paper introduces Conversational Tables (cTBL), a three-step encoder-decoder approach to retrieve tabular information and generate dialogue responses grounded on the retrieved information. cTBL uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over sparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs tabular knowledge retrieval using both encoder and decoder models, resulting in up to 46% relative improvement in ROUGE scores and better human evaluation for response generation on HyrbiDialogue.
    
[^92]: 图形提示方法综述：技术，应用和挑战

    A Survey of Graph Prompting Methods: Techniques, Applications, and Challenges. (arXiv:2303.07275v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.07275](http://arxiv.org/abs/2303.07275)

    该论文从图的角度审查了提示方法，将提示函数与图知识相结合，以解决在复杂任务中设计提示的挑战，在此基础上组织现有工作，并描述了应用和未来挑战。

    

    最近，“预训练，提示，预测训练”范例已经成为一种用有限标记数据学习普适模型的方式。这种方法涉及使用预训练模型和一个提示函数，该函数将模板应用于输入样本，添加指示背景并将目标任务重构为预训练任务。然而，在复杂任务中设计提示可能是一个具有挑战性和耗时的过程。这种限制可以通过使用图数据来解决，因为图形作为显式地建模实体之间交互的结构化知识库。在这个调查中，我们从图的角度审查提示方法，其中提示函数使用图知识进行扩展。特别是，我们介绍了图形提示学习的基本概念，组织了设计图形提示函数的现有工作，并描述了它们的应用和未来挑战。本调查将弥合图形和提示设计之间的差距。

    The recent "pre-train, prompt, predict training" paradigm has gained popularity as a way to learn generalizable models with limited labeled data. The approach involves using a pre-trained model and a prompting function that applies a template to input samples, adding indicative context and reformulating target tasks as the pre-training task. However, the design of prompts could be a challenging and time-consuming process in complex tasks. The limitation can be addressed by using graph data, as graphs serve as structured knowledge repositories by explicitly modeling the interaction between entities. In this survey, we review prompting methods from the graph perspective, where prompting functions are augmented with graph knowledge. In particular, we introduce the basic concepts of graph prompt learning, organize the existing work of designing graph prompting functions, and describe their applications and future challenges. This survey will bridge the gap between graphs and prompt design 
    
[^93]: 基于查询-话语注意力和联合建模的查询焦点会议摘要

    Query-Utterance Attention with Joint modeling for Query-Focused Meeting Summarization. (arXiv:2303.04487v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.04487](http://arxiv.org/abs/2303.04487)

    本文提出了一种基于查询-话语注意力和联合建模的查询感知框架，它使用密集检索模块计算话语级别与查询的相关性，并将标记级别的查询关联性和话语级别的查询关联性结合起来，实现生成一个更与查询相关的摘要。经过对两个基准数据集上的测试，表明该方法优于现有的QFMS模型。

    

    查询焦点会议摘要（QFMS）旨在根据给定的查询，从会议记录中生成摘要。以往的方法通常将查询与会议记录拼接起来，并使用注意机制隐式地对标记级别的查询相关性进行建模。然而，由于长时间的会议记录导致关键的查询相关信息被稀释，因此原始的基于转换的模型不足以突出与查询相关的关键部分。本文提出了一种基于查询-话语注意力和联合建模的查询感知框架。它使用密集检索模块计算话语级别与查询的相关性。然后，将标记级别的查询关联性和话语级别的查询关联性结合起来，并通过明确的注意机制整合到生成过程中。我们表明，不同颗粒度的查询相关性有助于生成一个更与查询相关的摘要。在两个基准数据集上的实验结果表明，我们提出的方法优于现有的QFMS模型。

    Query-focused meeting summarization (QFMS) aims to generate summaries from meeting transcripts in response to a given query. Previous works typically concatenate the query with meeting transcripts and implicitly model the query relevance only at the token level with attention mechanism. However, due to the dilution of key query-relevant information caused by long meeting transcripts, the original transformer-based model is insufficient to highlight the key parts related to the query. In this paper, we propose a query-aware framework with joint modeling token and utterance based on Query-Utterance Attention. It calculates the utterance-level relevance to the query with a dense retrieval module. Then both token-level query relevance and utterance-level query relevance are combined and incorporated into the generation process with attention mechanism explicitly. We show that the query relevance of different granularities contributes to generating a summary more related to the query. Exper
    
[^94]: 我们能否将Transformer应用到多种ImageNet模型的参数预测中进行扩展？

    Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?. (arXiv:2303.04143v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04143](http://arxiv.org/abs/2303.04143)

    该论文提出了一个可以预测其他神经网络高质量ImageNet参数的神经网络，通过使用预测参数进行初始化，能够提高多种ImageNet模型的训练速度，并且在转移到其他数据集时可以更快地收敛并达到竞争力的最终性能。

    

    在大规模数据集上对神经网络进行预训练已成为机器学习中的基石，但这只能由一些拥有充足资源的社区实现。我们旨在实现一个雄心勃勃的目标：民主化预训练。为此，我们训练并发布了一个单一的神经网络，可以预测其他神经网络的高质量ImageNet参数。通过使用预测参数进行初始化，我们可以提高PyTorch中可用的各种ImageNet模型的训练速度。在转移到其他数据集时，使用预测参数初始化的模型也会更快地收敛并达到竞争力的最终性能。

    Pretraining a neural network on a large dataset is becoming a cornerstone in machine learning that is within the reach of only a few communities with large-resources. We aim at an ambitious goal of democratizing pretraining. Towards that goal, we train and release a single neural network that can predict high quality ImageNet parameters of other neural networks. By using predicted parameters for initialization we are able to boost training of diverse ImageNet models available in PyTorch. When transferred to other datasets, models initialized with predicted parameters also converge faster and reach competitive final performance.
    
[^95]: 在可解释的因果变量和分布式神经表示之间寻找对齐方法

    Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations. (arXiv:2303.02536v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.02536](http://arxiv.org/abs/2303.02536)

    本文提出了分布式对齐搜索（DAS）算法，可以在不使用暴力搜索的情况下找到高层因果模型和低层深度学习系统之间的对齐方法，并且DAS可以发现先前方法忽略的内部结构。DAS算法有潜力实现对复杂深度学习系统的更好解释和理解。

    

    因果抽象是可解释的人工智能的一个有前途的理论框架，它定义了可解释的高层因果模型何时是低层深度学习系统的可信简化。然而，现有的因果抽象方法存在两个主要限制：它们需要在高层模型和低层模型之间进行暴力搜索对齐，并且它们预设高层模型中的变量将与低层模型中的不相交的神经元集对齐。在本文中，我们提出了分布式对齐搜索（DAS），它克服了这些限制。在DAS中，我们使用梯度下降找到高层模型和低层模型之间的对齐方法，允许个体神经元在非传统基底分布表示中发挥多个不同的角色。我们的实验表明，DAS可以发现先前方法忽略的内部结构。总体而言，DAS有潜力实现对复杂深度学习系统的更好解释和理解。

    Causal abstraction is a promising theoretical framework for explainable artificial intelligence that defines when an interpretable high-level causal model is a faithful simplification of a low-level deep learning system. However, existing causal abstraction methods have two major limitations: they require a brute-force search over alignments between the high-level model and the low-level one, and they presuppose that variables in the high-level model will align with disjoint sets of neurons in the low-level one. In this paper, we present distributed alignment search (DAS), which overcomes these limitations. In DAS, we find the alignment between high-level and low-level models using gradient descent rather than conducting a brute-force search, and we allow individual neurons to play multiple distinct roles by analyzing representations in non-standard bases-distributed representations. Our experiments show that DAS can discover internal structure that prior approaches miss. Overall, DAS 
    
[^96]: 《Dropout Reduces Underfitting》

    Dropout Reduces Underfitting. (arXiv:2303.01500v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01500](http://arxiv.org/abs/2303.01500)

    本研究证明dropout不仅可以防止神经网络过拟合，还可以缓解欠拟合问题。在训练初期采用early dropout方法，可以减少小批次梯度的方向差异，缓解SGD中的随机性，从而提高模型的训练效果。

    

    《Dropout Reduces Underfitting》是一篇关于神经网络正则化方法Dropout的研究，证明了Dropout可以在训练初期防止欠拟合的效果。在研究中，发现Dropout可以减少小批次梯度的方向差异，并有助于将小批次梯度与整个数据集的梯度对齐，从而缓解SGD中的随机性，减少每个批次对模型训练的影响。因此，我们提出了一个解决欠拟合问题的方案——early dropout: 在训练的初始阶段应用dropout，并在训练后关闭dropout。相比于没有dropout的模型，采用early dropout的模型能够获得更低的最终训练损失。另外，我们还探讨了一种用于正则化过拟合模型的对称技术——late dropout。

    Introduced by Hinton et al. in 2012, dropout has stood the test of time as a regularizer for preventing overfitting in neural networks. In this study, we demonstrate that dropout can also mitigate underfitting when used at the start of training. During the early phase, we find dropout reduces the directional variance of gradients across mini-batches and helps align the mini-batch gradients with the entire dataset's gradient. This helps counteract the stochasticity of SGD and limit the influence of individual batches on model training. Our findings lead us to a solution for improving performance in underfitting models - early dropout: dropout is applied only during the initial phases of training, and turned off afterwards. Models equipped with early dropout achieve lower final training loss compared to their counterparts without dropout. Additionally, we explore a symmetric technique for regularizing overfitting models - late dropout, where dropout is not used in the early iterations an
    
[^97]: K-SHAP: 一种用于匿名状态-动作对的策略聚类算法

    K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs. (arXiv:2302.11996v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11996](http://arxiv.org/abs/2302.11996)

    本文提出了一种名为K-SHAP的算法，来解决多个智能体保持匿名且仅有状态-动作对的情况下学习智能体决策的问题。

    

    从观测数据中学习智能体行为已经被证明可以提高我们对它们决策过程的理解，从而增强我们解释它们与环境和其他智能体之间交互的能力。尽管文献中已经提出了多种学习技术，但还有一种特定的情况尚未被探索，那就是智能体身份保持匿名的多智能体系统。例如，在金融市场中，标记数据通常是专有的，仅公开多个市场参与者交互而产生的匿名状态-动作对。因此，智能体行动序列不可观测，限制了现有工作的适用性。本文提出了一种策略聚类算法K-SHAP，它学习根据智能体策略对匿名状态-动作对进行分组。我们将该问题作为模仿学习(IL)任务，学习一个w...

    Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. In this paper, we propose a Policy Clustering algorithm, called K-SHAP, that learns to group anonymous state-action pairs according to the agent policies. We frame the problem as an Imitation Learning (IL) task, and we learn a w
    
[^98]: IB-RAR：信息瓶颈作为对抗性鲁棒性的正则化方法

    IB-RAR: Information Bottleneck as Regularizer for Adversarial Robustness. (arXiv:2302.10896v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10896](http://arxiv.org/abs/2302.10896)

    本文提出了一种名为IB-RAR的正则化方法，利用信息瓶颈来增强对抗性训练和非对抗性训练方法的鲁棒性，并通过过滤不必要的特征来提高准确性。

    

    本文提出了一种新颖的方法IB-RAR，利用信息瓶颈（IB）来增强对抗性训练和非对抗性训练方法的对抗性鲁棒性。首先，本文使用IB理论在损失函数中构建正则化器作为学习目标。然后，根据中间表示与标签之间的互信息（MI）过滤掉不必要的特征，因为使用IB训练的网络提供易于区分的MI特征。实验结果表明，我们的方法可以自然地与对抗性训练相结合，并在新的对抗性例子上提供始终更好的准确性。我们的方法在针对VGG16网络进行三次对抗性训练基准和CIFAR-10数据集的五种对抗性攻击方案中，平均提高了3.07％的准确率。此外，我们的方法也为无防御方法提供了良好的鲁棒性，例如仅使用交叉熵损失进行训练。最后，在无人干预的情况下，我们的方法可以在处理缺失标签的情况下提供更好的学习效果。

    In this paper, we propose a novel method, IB-RAR, which uses Information Bottleneck (IB) to strengthen adversarial robustness for both adversarial training and non-adversarial-trained methods. We first use the IB theory to build regularizers as learning objectives in the loss function. Then, we filter out unnecessary features of intermediate representation according to their mutual information (MI) with labels, as the network trained with IB provides easily distinguishable MI for its features. Experimental results show that our method can be naturally combined with adversarial training and provides consistently better accuracy on new adversarial examples. Our method improves the accuracy by an average of 3.07% against five adversarial attacks for the VGG16 network, trained with three adversarial training benchmarks and the CIFAR-10 dataset. In addition, our method also provides good robustness for undefended methods, such as training with cross-entropy loss only. Finally, in the absenc
    
[^99]: 数据高效的对比自监督学习：易于学习的样本起到最大的作用。

    Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most. (arXiv:2302.09195v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09195](http://arxiv.org/abs/2302.09195)

    该研究证明了在自监督学习中容易学习的样本对学习高质量表示起到最大的作用，这有助于减少所需的训练数据量，并提高性能。

    

    自监督学习（SSL）从大量的无标签训练数据中学习高质量的表示。随着数据集变得越来越大，识别对学习此类表示最有用的示例变得至关重要。这可以通过减少学习高质量表示所需的数据量来实现有效的SSL。然而，对于SSL的价值如何量化一直是一个悬而未决的问题。在本文中，我们首次解决了这个问题，证明在期望意义下，对比SSL中对学习做出最大贡献的示例是具有最相似数据增强的示例。我们对这些子集的SSL的广义性能提供了严格的保证。实验证明，令人惊讶的是，对SSL做出最大贡献的子集是对监督学习做出最小贡献的子集。通过广泛的实验，我们证明了我们的子集在CIFAR100、CIFAR中的表现优于随机子集3%以上。

    Self-supervised learning (SSL) learns high-quality representations from large pools of unlabeled training data. As datasets grow larger, it becomes crucial to identify the examples that contribute the most to learning such representations. This enables efficient SSL by reducing the volume of data required for learning high-quality representations. Nevertheless, quantifying the value of examples for SSL has remained an open question. In this work, we address this for the first time, by proving that examples that contribute the most to contrastive SSL are those that have the most similar augmentations to other examples, in expectation. We provide rigorous guarantees for the generalization performance of SSL on such subsets. Empirically, we discover, perhaps surprisingly, the subsets that contribute the most to SSL are those that contribute the least to supervised learning. Through extensive experiments, we show that our subsets outperform random subsets by more than 3% on CIFAR100, CIFAR
    
[^100]: 零样本协同合作学习框架的合作开放式学习

    Cooperative Open-ended Learning Framework for Zero-shot Coordination. (arXiv:2302.04831v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.04831](http://arxiv.org/abs/2302.04831)

    该论文提出了一个COLE框架，通过构建合作游戏的开放式目标，从图论的角度评估和确定每个策略的协作能力，以有效地解决零样本协调中的合作不兼容性问题。

    

    协作人工智能中的零样本协调仍然是一个重大挑战，有效地协调一系列看不见的合作伙伴。先前的算法试图通过优化种群中的固定目标来改善策略或行为的多样性来解决这一挑战。然而，这些方法可能导致学习损失和与种群中某些策略无法合作，即合作不兼容性。为了解决这个问题，我们提出了合作开放式学习（COLE）框架，该框架从图论的角度构建了协作游戏的开放式目标，以评估和确定每个策略的协作能力。我们进一步明确了框架并提出了一种实用的算法，该算法利用了博弈论和图论的知识。此外，对算法的学习过程进行的分析显示，它可以有效地克服学习困难。

    Zero-shot coordination in cooperative artificial intelligence (AI) remains a significant challenge, which means effectively coordinating with a wide range of unseen partners. Previous algorithms have attempted to address this challenge by optimizing fixed objectives within a population to improve strategy or behaviour diversity. However, these approaches can result in a loss of learning and an inability to cooperate with certain strategies within the population, known as cooperative incompatibility. To address this issue, we propose the Cooperative Open-ended LEarning (COLE) framework, which constructs open-ended objectives in cooperative games with two players from the perspective of graph theory to assess and identify the cooperative ability of each strategy. We further specify the framework and propose a practical algorithm that leverages knowledge from game theory and graph theory. Furthermore, an analysis of the learning process of the algorithm shows that it can efficiently overc
    
[^101]: 利用离线数据进行高效在线强化学习

    Efficient Online Reinforcement Learning with Offline Data. (arXiv:2302.02948v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02948](http://arxiv.org/abs/2302.02948)

    本文研究了利用离线数据进行高效在线强化学习的方法，证明了现有的离线策略方法能够应用于在线学习，提出了一些最少但重要的更改，来实现可靠的性能并提供了实践中可应用的建议。

    

    样本效率和探索仍然是在线强化学习中的主要挑战。一种强有力的方法是包括离线数据，如来自人类专家或次优探索策略的先前轨迹。以前的方法依赖于广泛的修改和额外的复杂性来确保有效使用这些数据。相反，我们想问：我们是否可以简单地应用现有的离线策略方法来利用离线数据进行在线学习？在这项工作中，我们证明了答案是肯定的。但是，为了实现可靠的性能，需要对现有的离线策略强化学习算法进行一些最少但重要的更改。我们广泛地测试了这些设计选择，并展示了对性能影响最大的关键因素，得出了一组实践者可以轻松应用的建议，无论其数据包括少量专家演示或大量次优轨迹。

    Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see
    
[^102]: 通过学习程序组合的方法实现分层编程强化学习

    Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs. (arXiv:2301.12950v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12950](http://arxiv.org/abs/2301.12950)

    本论文提出了一种名为HPRL的分层编程强化学习框架，通过学习程序组合的方法实现，能够产生具有人类可解释性并且在评估候选方案时可以准确奖励和惩罚的策略。

    

    Trivedi等人(2021)提出了一种方法(LEAPS)，旨在产生具有人类可解释性并且可以更好地推广到新场景的加强学习（RL）策略。方法先学习一个程序嵌入空间，以连续参数化来自预生成的程序数据集的多样化程序，然后在给定任务时在学习的程序嵌入空间中搜索解决任务的程序。

    Aiming to produce reinforcement learning (RL) policies that are human-interpretable and can generalize better to novel scenarios, Trivedi et al. (2021) present a method (LEAPS) that first learns a program embedding space to continuously parameterize diverse programs from a pre-generated program dataset, and then searches for a task-solving program in the learned program embedding space when given a task. Despite the encouraging results, the program policies that LEAPS can produce are limited by the distribution of the program dataset. Furthermore, during searching, LEAPS evaluates each candidate program solely based on its return, failing to precisely reward correct parts of programs and penalize incorrect parts. To address these issues, we propose to learn a meta-policy that composes a series of programs sampled from the learned program embedding space. By learning to compose programs, our proposed hierarchical programmatic reinforcement learning (HPRL) framework can produce program p
    
[^103]: 扩散模型作为艺术家：我们正在拉近人与机器之间的鸿沟吗？

    Diffusion Models as Artists: Are we Closing the Gap between Humans and Machines?. (arXiv:2301.11722v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.11722](http://arxiv.org/abs/2301.11722)

    本文研究表明，扩散模型已经在拉近人与机器之间的鸿沟方面取得了进展，但机器生成作品的原创性和可识别性仍然存在差距。

    

    人工智能的一个重要里程碑是开发能够生成和人类无异的绘画作品的算法。在本文中，我们采用 Boutin 等人 2022 年提出的“多样性 vs. 可识别性”评分框架，发现一发即中的扩散模型确实已经开始拉近人类和机器之间的鸿沟。然而，用更细粒度的样本独创性度量，我们发现强化扩散模型的指导有助于提高它们的绘画人性化程度，但它们仍然无法接近人类绘画作品的原创性和可识别性。通过在线心理物理实验收集人类类别诊断特征并将其与从扩散模型中导出的特征进行比较，我们发现人类依赖于更少且更局部的特征。总体而言，我们的研究表明扩散模型在显著提高机器生成绘画作品的质量方面发挥着重要作用，但人与机器之间的差距仍然存在。

    An important milestone for AI is the development of algorithms that can produce drawings that are indistinguishable from those of humans. Here, we adapt the 'diversity vs. recognizability' scoring framework from Boutin et al, 2022 and find that one-shot diffusion models have indeed started to close the gap between humans and machines. However, using a finer-grained measure of the originality of individual samples, we show that strengthening the guidance of diffusion models helps improve the humanness of their drawings, but they still fall short of approximating the originality and recognizability of human drawings. Comparing human category diagnostic features, collected through an online psychophysics experiment, against those derived from diffusion models reveals that humans rely on fewer and more localized features. Overall, our study suggests that diffusion models have significantly helped improve the quality of machine-generated drawings; however, a gap between humans and machines 
    
[^104]: 预测是否随意？在公平分类中评估自洽性

    Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification. (arXiv:2301.11562v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11562](http://arxiv.org/abs/2301.11562)

    在公平分类中，模型的预测方差是一个重要但鲜为人知的误差来源问题。作者提出了一个自洽性标准来衡量测量和减少随意性。作者还开发了一个算法来处理随意性预测，并通过实证研究揭示了当前模型无法处理某些类型数据的问题。

    

    在公平分类中，不同经过训练的模型之间的预测方差是一个重要但鲜为人知的误差来源问题。 实证表明，某些情况下，预测的方差差异非常大，以至于决策实际上是随意的。 为了研究这个问题，我们进行了大规模的实证研究，并做出了四个总体贡献：我们1）定义了一种基于方差的度量标准，称为自洽性，在测量和减少随意性时使用； 2）开发了一种合理的算法，当预测无法做出决策时，可以放弃分类； 3）进行了迄今为止有关公平分类中方差（相对于自洽性和随意性）作用的最大规模实证研究； 4）推出了一个工具包，使美国住房抵押贷款披露法案（HMDA）数据集易于用于未来研究。 总的来说，我们的实证结果揭示了关于可重复性的令人震惊的见解。当考虑到方差和随意预测的可能性时，大多数公平分类基准接近公平。 但是，一小部分实例显示出极大的随意性水平，这表明当前的模型可能无法处理某些类型的数据。

    Variance in predictions across different trained models is a significant, under-explored source of error in fair classification. Empirically, the variance on some instances is so large that decisions can be effectively arbitrary. To study this problem, we perform a large-scale empirical study and make four overarching contributions: We 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our empirical results reveal shocking insights about reproducibility. Most fairness classification benchmarks are close-to-fair when taking into
    
[^105]: SNeRL: 语义感知的神经辐射场用于强化学习

    SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning. (arXiv:2301.11520v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11520](http://arxiv.org/abs/2301.11520)

    本文提出了一种称为SNeRL的语义感知神经辐射场，它通过学习3D-aware的隐式表示来进行强化学习，并在基于像素的以及最新的3D感知表示方法中表现出更好的性能。

    

    传统的强化学习表示方法很难有效地融合人类直观的3D环境理解，因此经常表现出次优性能。本文提出了一种称为SNeRL的语义感知神经辐射场，它通过联合优化卷积编码器和语义感知神经辐射场（NeRF）来从多视角图像中学习3D感知神经隐式表示。我们在NeRF中引入了3D语义和蒸馏特征场，并与RGB辐射场并行用于强化学习中的语义和对象中心表示学习。SNeRL在无模型和有模型强化学习中不仅优于以往的基于像素的表示方法，还优于最近的3D感知表示方法。

    As previous representations for reinforcement learning cannot effectively incorporate a human-intuitive understanding of the 3D environment, they usually suffer from sub-optimal performances. In this paper, we present Semantic-aware Neural Radiance Fields for Reinforcement Learning (SNeRL), which jointly optimizes semantic-aware neural radiance fields (NeRF) with a convolutional encoder to learn 3D-aware neural implicit representation from multi-view images. We introduce 3D semantic and distilled feature fields in parallel to the RGB radiance fields in NeRF to learn semantic and object-centric representation for reinforcement learning. SNeRL outperforms not only previous pixel-based representations but also recent 3D-aware representations both in model-free and model-based reinforcement learning.
    
[^106]: 匹配标本作为下一句预测：自然语言处理科学教育中的零样本学习自动评分

    Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt Learning for Automatic Scoring in Science Education. (arXiv:2301.08771v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.08771](http://arxiv.org/abs/2301.08771)

    本研究提出了一种零样本学习自动评分的方法，利用预训练的语言模型配合匹配标本作为下一句预测技术，成功应用于科学教育领域的论证任务，极大地减少了训练成本和时间。

    

    开发能够自动评分科学问题的学生书面答案的模型对于科学教育至关重要。然而，收集和标记足够的学生答案以训练模型是耗时和费用高昂的。最近的研究表明，预训练的语言模型（PLMs）可以在不需要prompt调整的情况下适应下游任务。然而，在科学教育中还没有使用过这种提示方法的研究。由于学生的答案是用自然语言呈现的，因此使用提示将评分过程对齐为下一句预测任务可以跳过昂贵的调整阶段。在这项研究中，我们通过匹配标本作为下一句预测（MeNSP）开发了一种零样本自动评分方法。这种方法不需要训练样本。我们首先在评分三个科学论证任务中应用MeNSP，并发现机器-人评分的一致性，Cohen的Kappa系数在0.30到0.57之间，F1分数

    Developing models to automatically score students' written responses to science problems is critical for science education. However, collecting and labeling sufficient student responses for training models is time and cost-consuming. Recent studies suggest that pre-trained language models (PLMs) can be adapted to downstream tasks without fine-tuning with prompts. However, no research has employed such a prompt approach in science education. As student responses are presented with natural language, aligning the scoring procedure as the next sentence prediction task using prompts can skip the costly fine-tuning stage. In this study, we developed a zero-shot approach to automatically score student responses via Matching Exemplars as Next Sentence Prediction (MeNSP). This approach employs no training samples. We first apply MeNSP in scoring three assessment tasks of scientific argumentation and found machine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and F1 score ran
    
[^107]: PEAK: 通过自动化知识提取实现的可解释隐私助手

    PEAK: Explainable Privacy Assistant through Automated Knowledge Extraction. (arXiv:2301.02079v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.02079](http://arxiv.org/abs/2301.02079)

    本文提出了隐私助手PEAK，通过自动化知识提取和生成解释，帮助用户理解其隐私建议。在真实数据集的用户研究中，用户认为生成的解释有用且易于理解。

    

    在在线隐私领域，隐私助手在使用户能够有效管理隐私方面发挥着至关重要的作用。尽管最近的研究在解决隐私侵犯检测和个性化隐私建议等任务方面取得了有希望的进展，但这些系统被广泛采用的关键在于这些系统提供决策过程的解释能力。本文提出了一个为隐私决策生成解释的隐私助手。隐私助手着重于发现潜在主题、识别解释类别、建立解释方案和生成自动化解释。生成的解释可以被用户用来理解隐私助手的建议。我们的用户研究使用了真实世界的图像隐私数据集，结果显示用户认为生成的解释有用且易于理解。此外，隐私助手可以使用这些生成的解释。

    In the realm of online privacy, privacy assistants play a pivotal role in empowering users to manage their privacy effectively. Although recent studies have shown promising progress in tackling tasks such as privacy violation detection and personalized privacy recommendations, a crucial aspect for widespread user adoption is the capability of these systems to provide explanations for their decision-making processes. This paper presents a privacy assistant for generating explanations for privacy decisions. The privacy assistant focuses on discovering latent topics, identifying explanation categories, establishing explanation schemes, and generating automated explanations. The generated explanations can be used by users to understand the recommendations of the privacy assistant. Our user study of real-world privacy dataset of images shows that users find the generated explanations useful and easy to understand. Additionally, the generated explanations can be used by privacy assistants th
    
[^108]: 连续对比微调改进低资源关系提取

    Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2212.10823](http://arxiv.org/abs/2212.10823)

    本文提出了一种使用连续对比微调的方法来改进低资源关系提取，通过使用一致的对比学习目标预训练和微调RE模型，以及多中心对比损失来允许一个关系形成多个聚类。实验结果表明该方法可以显着提高低资源情况和领域中的关系提取性能。

    

    关系提取（RE）依赖结构化注释语料库进行模型训练，尤其在低资源情况和领域中，该任务具有挑战性。近期研究通过自监督学习来解决低资源的RE，其中解决方案包括通过RE目标预训练关系嵌入，并通过分类为基础的目标对有标签数据进行微调。然而，这种方法的一个关键挑战是目标之间的差距，它阻止RE模型充分利用预训练表示中的知识。本文旨在弥合差距，并提出使用一致的对比学习目标预训练和微调RE模型。由于在这种表示学习范式中，一个关系可能在表示空间中轻松形成多个聚类，因此我们进一步提出了多中心对比损失，允许一个关系形成多个聚类以更好地对齐预训练。在两个文档中的实验表明，所提出的方法可以在低资源情况和领域中显着提高关系提取性能。

    Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the relation embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two docum
    
[^109]: 神经机器翻译的合成预训练任务

    Synthetic Pre-Training Tasks for Neural Machine Translation. (arXiv:2212.09864v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09864](http://arxiv.org/abs/2212.09864)

    本文提出了一种使用合成任务和数据预训练神经机器翻译模型的方法，其可以缓解大规模抓取的语料库所导致的毒性、偏见和法律隐患，并证明了即使采用高度混淆或纯合成数据，预训练依然有效。

    

    使用大规模抓取的语料库进行预训练模型可能会导致毒性和偏见等问题，以及版权和隐私问题。采用合成任务和数据进行预训练是缓解这些问题的一种有前途的方式，因为模型不会吸收任何真实世界信息。本文旨在了解使用合成资源时对预训练模型有效性的影响因素，特别是在神经机器翻译的背景下。我们提出了几种新颖的预训练翻译模型的方法，包括不同水平的词汇和结构知识，例如：1）从大型平行语料库生成混淆数据，2）连接从小型词对齐语料库提取的短语对，以及3）生成不带真实人类语料库的合成平行数据。我们在多种语言对上的实验表明，即使存在高水平的混淆或纯合成数据，也可以实现预训练的效益。

    Pre-training models with large crawled corpora can lead to issues such as toxicity and bias, as well as copyright and privacy concerns. A promising way of alleviating such concerns is to conduct pre-training with synthetic tasks and data, since no real-world information is ingested by the model. Our goal in this paper is to understand the factors that contribute to the effectiveness of pre-training models when using synthetic resources, particularly in the context of neural machine translation. We propose several novel approaches to pre-training translation models that involve different levels of lexical and structural knowledge, including: 1) generating obfuscated data from a large parallel corpus 2) concatenating phrase pairs extracted from a small word-aligned corpus, and 3) generating synthetic parallel data without real human language corpora. Our experiments on multiple language pairs reveal that pre-training benefits can be realized even with high levels of obfuscation or purely
    
[^110]: Transformer模型通过梯度下降实现上下文学习

    Transformers learn in-context by gradient descent. (arXiv:2212.07677v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07677](http://arxiv.org/abs/2212.07677)

    本文提出，训练Transformer模型应用于自回归目标问题时，与基于梯度的元学习的形式密切相关，通过梯度下降学习模型的“底层优化程序”的机制，在回归问题的领域中从机械的角度理解了Transformers模型中上下文学习的内部机制。

    

    目前，Transformers模型中上下文学习的机制尚未得到很好的理解，大多只停留在直觉上。本文提出，训练Transformer模型应用于自回归目标问题时，与基于梯度的元学习的形式密切相关。我们首先提供一个简单的权重构造，证明了由单个线性自注意力层引发的数据转换与由具有回归损失的梯度下降（GD）获得的转换具有等价性。在此基础上，我们通过实验证明，当仅训练自注意力Transformer模型进行简单的回归任务时，通过GD优化得到的模型与模型权重十分相似，或者在某些情况下，GD优化的权重与构造的权重相同。因此，我们展示了经过训练的Transformer模型是如何在前向传递中通过梯度下降学习模型的“底层优化程序”的。在回归问题的领域中，这使我们能够从机械的角度理解Transformers模型中上下文学习的内部机制。

    At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context
    
[^111]: 基于任务相似度元学习加速多目标非分层超参数最优化的树形结构Parzen估计

    Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator. (arXiv:2212.06751v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06751](http://arxiv.org/abs/2212.06751)

    本文提出了一种基于任务相似度元学习的方法来加速树形结构Parzen估计中的多目标非分层超参数最优化，实现了最先进的性能。

    

    超参数优化是提高深度学习性能的关键步骤。实践者通常面临多个方面的权衡，如准确性和延迟时间。在深度学习的高计算需求和对高效超参数优化的不断增长需求下，加速多目标优化变得越来越重要。本文将TPE的收购函数扩展到元学习设置中，使用由任务之间顶级域之间的重叠度定义的任务相似性。我们也从理论上分析并解决了任务相似性的局限性。在实验中，我们展示了我们的方法在表格HPO基准上加速了MO-TPE，并获得了最先进的性能。我们的方法还通过赢得AutoML 2022来得到外部验证。

    Hyperparameter optimization (HPO) is a vital step in improving performance in deep learning (DL). Practitioners are often faced with the trade-off between multiple criteria, such as accuracy and latency. Given the high computational needs of DL and the growing demand for efficient HPO, the acceleration of multi-objective (MO) optimization becomes ever more important. Despite the significant body of work on meta-learning for HPO, existing methods are inapplicable to MO tree-structured Parzen estimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we extend TPE's acquisition function to the meta-learning setting using a task similarity defined by the overlap of top domains between tasks. We also theoretically analyze and address the limitations of our task similarity. In the experiments, we demonstrate that our method speeds up MO-TPE on tabular HPO benchmarks and attains state-of-the-art performance. Our method was also validated externally by winning the AutoML 2022 
    
[^112]: 稳定的美术家：扭曲漫流空间中的语义驾驭

    The Stable Artist: Steering Semantics in Diffusion Latent Space. (arXiv:2212.06013v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.06013](http://arxiv.org/abs/2212.06013)

    本文介绍了一种名为"稳定的美术家"的图像编辑方法，其中包含SEGA以及潜在遍历等组成部分，使得用户可以实现在图像生成过程中的细粒度控制，可以微妙地编辑图像、改变构图和风格，达到艺术构思优化等目的，实现了在各种文本到图像合成任务中的最先进的定量和定性结果。

    

    最近，基于大规模的文本条件下的生成扩散模型因其惊人的性能而受到了广泛关注，可以仅通过文本生成高保真度的图像。然而，一次性获得高质量的结果几乎是不可行的。相反，文本指导的图像生成包括用户对输入进行多次微小的更改，以迭代地雕刻出所想象的图像。然而，输入提示的微小更改通常会导致生成完全不同的图像，因此艺术家的控制受限于其粒度。为了提供灵活性，我们提出了稳定的美术家，这是一种图像编辑方法，可使图像生成过程中的细粒度控制更加容易。主要组成部分是语义引导(SEGA)，它沿着不同数量的语义方向引导扩散过程。这允许对图像进行微妙的编辑、改变构图和风格，以及优化整体艺术构思。此外，我们提出了一种名为潜在遍历的新技术，它可以实现对图像特定区域的局部和有导向地修改。我们的方法在各种文本到图像合成任务中实现了最先进的定量和定性结果，为用户提供了对所生成的图像前所未有的控制。

    Large, text-conditioned generative diffusion models have recently gained a lot of attention for their impressive performance in generating high-fidelity images from text alone. However, achieving high-quality results is almost unfeasible in a one-shot fashion. On the contrary, text-guided image generation involves the user making many slight changes to inputs in order to iteratively carve out the envisioned image. However, slight changes to the input prompt often lead to entirely different images being generated, and thus the control of the artist is limited in its granularity. To provide flexibility, we present the Stable Artist, an image editing approach enabling fine-grained control of the image generation process. The main component is semantic guidance (SEGA) which steers the diffusion process along variable numbers of semantic directions. This allows for subtle edits to images, changes in composition and style, as well as optimization of the overall artistic conception. Furthermo
    
[^113]: Elixir: 在小型 GPU 集群上训练大型语言模型

    Elixir: Train a Large Language Model on a Small GPU Cluster. (arXiv:2212.05339v3 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2212.05339](http://arxiv.org/abs/2212.05339)

    Elixir 提出了一种基于预运行模型分析的自动化高效大模型训练方案，可以将内存使用卸载到 CPU 和 NVMe 存储器中，充分发挥硬件的潜力，实验中表现优于最先进的基准模型。

    

    近年来，由于其规模前所未有的大小，大型语言模型取得了巨大的成功。然而，训练这些模型对于大多数研究人员来说是一项挑战，因为它需要大量的 GPU。为了减少 GPU 内存使用，提出了内存分区和内存卸载。这些方法消除了内存冗余，并将内存使用卸载到 CPU 和 NVMe 存储器中，使得可以在小型 GPU 集群上进行训练。然而，直接部署这些解决方案通常会导致次优效率。只有经验丰富的专家才能通过仔细调整分布式配置来充分发挥硬件的潜力。因此，我们提出了一种新颖的解决方案 Elixir，它基于预运行模型分析自动化高效的大模型训练。Elixir 的目标是确定分区和卸载技术的最佳组合，以最大化训练吞吐量。在我们的实验中，Elixir 显著优于当前最先进的基准

    In recent years, large language models have achieved great success due to their unprecedented size. However, training these models poses a challenge for most researchers as it requires a substantial number of GPUs. To reduce GPU memory usage, memory partitioning, and memory offloading have been proposed. These approaches eliminate memory redundancies and offload memory usage to the CPU and NVMe memory, respectively, enabling training on small GPU clusters. However, directly deploying these solutions often leads to suboptimal efficiency. Only experienced experts can unleash the full potential of hardware by carefully tuning the distributed configuration. Thus, we present a novel solution, Elixir, which automates efficient large-model training based on pre-runtime model profiling. Elixir aims to identify the optimal combination of partitioning and offloading techniques to maximize training throughput. In our experiments, Elixir significantly outperforms the current state-of-the-art basel
    
[^114]: 基础模型的能力探究

    On the Power of Foundation Models. (arXiv:2211.16327v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.16327](http://arxiv.org/abs/2211.16327)

    本文通过范畴论探究了基础模型的能力，提出了具有最小所需能力的基础模型可以通过微调和足够的资源来解决前置任务所定义的类别中的下游任务，并且这种能力可以扩展到任何下游任务，只要允许微调且下游任务可在前置任务定义的范畴中表示。

    

    如果基础模型具有无限高质量的数据点、无限计算能力、一个无限大的完美训练算法、以及在预设任务上保证零泛化误差，那么它可以用于一切吗？传统的表示、优化或泛化理论无法回答这个问题，因为它们主要探讨的问题在这里都是不存在的。本文提出范畴论提供了强大的理论工具，以回答这个问题。我们证明了三个结果，第一个限制了基于提示的学习的能力，即仅当任务可表示时，模型才能用提示解决下游任务；第二个结果表明，微调不受这个限制，因为一个具有最小所需能力（对称性）的基础模型可以通过微调和足够的资源来理论上解决前置任务所定义的类别中的下游任务。我们的最终结果可以看作是第二个结果的一般化，表明如果允许微调并且下游任务可在前置任务定义的范畴中表示，则基础模型的最小能力也足以解决任何下游任务。

    With infinitely many high-quality data points, infinite computational power, an infinitely large foundation model with a perfect training algorithm and guaranteed zero generalization error on the pretext task, can the model be used for everything? This question cannot be answered by the existing theory of representation, optimization or generalization, because the issues they mainly investigate are assumed to be nonexistent here. In this paper, we show that category theory provides powerful machinery to answer this question. We have proved three results. The first one limits the power of prompt-based learning, saying that the model can solve a downstream task with prompts if and only if the task is representable. The second one says fine tuning does not have this limit, as a foundation model with the minimum required power (up to symmetry) can theoretically solve downstream tasks for the category defined by pretext task, with fine tuning and enough resources. Our final result can be se
    
[^115]: PuzzleFusion：释放扩散模型在空间拼图解决中的威力

    PuzzleFusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving. (arXiv:2211.13785v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.13785](http://arxiv.org/abs/2211.13785)

    本文提出了一个名为"PuzzleFusion"的神经架构，基于扩散模型，用于解决空间拼图和房间布局任务。在实验中，他们发现简单使用扩散模型可以有效地解决这些具有挑战性的空间拼图任务。

    

    本文提出了一种基于扩散模型的端到端神经架构，用于解决空间拼图和房间布局任务。所提出的系统"PuzzleFusion"针对后者任务，将一组房间布局视为俯视图中的多边形曲线，并通过估计它们的二维平移和旋转来对齐房间布局块，类似于解决空间拼图的过程。本文令人惊讶的发现是，扩散模型的简单使用有效地将这些具有挑战性的空间拼图任务解决为条件生成过程。为了实现端到端神经系统的学习，本文引入了新的数据集，其中包含有地面实况安排的数据集：1）2D Voronoi 拼图数据集，这是一种通过 2D 点集的 Voronoi 图生成拼图块的合成数据集；以及2）MagicPlan 数据集，这是一种从 MagicPlan 生产线提供的真实数据集，其中拼图块是由增强现实应用程序构建的房间布局。

    This paper presents an end-to-end neural architecture based on Diffusion Models for spatial puzzle solving, particularly jigsaw puzzle and room arrangement tasks. In the latter task, for instance, the proposed system ``PuzzleFusion'' takes a set of room layouts as polygonal curves in the top-down view and aligns the room layout pieces by estimating their 2D translations and rotations, akin to solving the jigsaw puzzle of room layouts. A surprising discovery of the paper is that the simple use of a Diffusion Model effectively solves these challenging spatial puzzle tasks as a conditional generation process. To enable learning of an end-to-end neural system, the paper introduces new datasets with ground-truth arrangements: 1) 2D Voronoi jigsaw dataset, a synthetic one where pieces are generated by Voronoi diagram of 2D pointset; and 2) MagicPlan dataset, a real one offered by MagicPlan from its production pipeline, where pieces are room layouts constructed by augmented reality App by rea
    
[^116]: 用深度均衡模型统一标签输入图神经网络

    Unifying Label-inputted Graph Neural Networks with Deep Equilibrium Models. (arXiv:2211.10629v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10629](http://arxiv.org/abs/2211.10629)

    本文将标签输入的GNN和隐式GNN统一起来，并提出了一种IGNN中的隐式微分方法，使得标签无限传播变得可行。

    

    图神经网络在学习非欧几里得数据方面的成功引发了许多子课题，例如标签输入的GNN(LGNN)和隐式GNN(IGNN)。本文通过将LGNN解释为IGNN理论并将流行的LGNN归约为IGNN的形式来统一这两个子域。该统一简化了两个子域之间的交流并启发了更多研究。具体来说，介绍了IGNN的隐式微分到LGNN中，以常数内存微分其无限范围的标签传播，使传播成为可行的选择。

    The success of Graph Neural Networks (GNN) in learning on non-Euclidean data arouses many subtopics, such as Label-inputted GNN (LGNN) and Implicit GNN (IGNN). LGNN, explicitly inputting supervising information (a.k.a. labels) in GNN, integrates label propagation to achieve superior performance, but with the dilemma between its propagating distance and adaptiveness. IGNN, outputting an equilibrium point by iterating its network infinite times, exploits information in the entire graph to capture long-range dependencies, but with its network constrained to guarantee the existence of the equilibrium. This work unifies the two subdomains by interpreting LGNN in the theory of IGNN and reducing prevailing LGNNs to the form of IGNN. The unification facilitates the exchange between the two subdomains and inspires more studies. Specifically, implicit differentiation of IGNN is introduced to LGNN to differentiate its infinite-range label propagation with constant memory, making the propagation b
    
[^117]: 利用恒定内存将数据集精简扩展到ImageNet-1K

    Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory. (arXiv:2211.10586v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.10586](http://arxiv.org/abs/2211.10586)

    本文提出了一种利用恒定内存需求扩展数据集精简的方法，可将Matching Training Trajectories（MTT）应用于ImageNet-1K数据集，达到6倍的内存降低，同时增加了约2%的运行时开销。同时也发现，为合成图像分配软标签对于实现良好的性能非常重要。

    

    数据集精简方法旨在将大型数据集压缩成一小组合成样本，使得在训练时，与在整个数据集上进行常规训练相比，可以获得竞争性的性能。在最近提出的方法中，匹配训练轨迹（MTT）在CIFAR-10/100上实现了最先进的性能，但由于在反向传播过程中执行展开梯度计算时需要大量内存，因此很难扩展到ImageNet-1k数据集。令人惊讶的是，我们发现存在一种方法，可以使用恒定的GPU内存需求（与展开步骤的数量无关）精确计算轨迹匹配损失函数的梯度。有了这一发现，所提出的内存高效的轨迹匹配方法只需要比原始MTT多约2％的运行时开销，即可轻松扩展到具有6倍内存缩减的ImageNet-1K。此外，我们发现为合成图像分配软标签对于实现良好的性能至关重要。

    Dataset distillation methods aim to compress a large dataset into a small set of synthetic samples, such that when being trained on, competitive performances can be achieved compared to regular training on the entire dataset. Among recently proposed methods, Matching Training Trajectories (MTT) achieves state-of-the-art performance on CIFAR-10/100, while having difficulty scaling to ImageNet-1k dataset due to the large memory requirement when performing unrolled gradient computation through back-propagation. Surprisingly, we show that there exists a procedure to exactly calculate the gradient of the trajectory matching loss with constant GPU memory requirement (irrelevant to the number of unrolled steps). With this finding, the proposed memory-efficient trajectory matching method can easily scale to ImageNet-1K with 6x memory reduction while introducing only around 2% runtime overhead than original MTT. Further, we find that assigning soft labels for synthetic images is crucial for the
    
[^118]: MT4SSL：通过集成多个目标来提升自监督语音表示学习

    MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. (arXiv:2211.07321v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.07321](http://arxiv.org/abs/2211.07321)

    本文提出了一个新的自监督多任务学习框架MT4SSL，通过同时使用K均值算法作为离线目标提取器和没有梯度的教师网络作为在线目标提取器，取得了比以前更好的表现。同时，使用离线和在线目标提取器可以得到更好的收敛性，我们认为这是自监督语音模型上的多任务学习有前途的趋势。

    

    本文从训练目标的获取方式提出了自监督语音模型的新视角，并将目标提取器概括为离线目标提取器和在线目标提取器。基于此，我们提出了一种新的自监督多任务学习框架MT4SSL，它使用K均值算法作为离线目标提取器，使用没有梯度的教师网络作为在线目标提取器。实验结果表明，我们的模型在LibriSpeech基准测试上优于以前的方法，并且与使用更少数据的最佳模型相当甚至更好。此外，我们发现在预训练阶段同时使用离线和在线目标提取器可以得到更好的收敛性。因此，我们认为从我们的角度进行自监督语音模型上的多任务学习是一种有前途的趋势。

    In this paper, we provide a new perspective on self-supervised speech models from how the training targets are obtained. We generalize the targets extractor into Offline Targets Extractor (Off-TE) and Online Targets Extractor (On-TE). Based on this, we propose a new multi-tasking learning framework for self-supervised learning, MT4SSL, which stands for Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. MT4SSL uses the K-means algorithm as an Off-TE and a teacher network without gradients as an On-TE, respectively. Our model outperforms previous SSL methods by nontrivial margins on the LibriSpeech benchmark, and is comparable to or even better than the best-performing models with fewer data. Furthermore, we find that using both Off-TE and On-TE results in better convergence in the pre-training phase. With both effectiveness and efficiency, we think doing multi-task learning on self-supervised speech models from our perspective is a promising trend.
    
[^119]: 基于配对逆金字塔结构和密集多层感知机块的有效音频分类网络

    Effective Audio Classification Network Based on Paired Inverse Pyramid Structure and Dense MLP Block. (arXiv:2211.02940v3 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.02940](http://arxiv.org/abs/2211.02940)

    该论文通过提出基于轻量级音频的配对逆金字塔结构网络和密集多层感知机块网络，实现了在不进行数据增强或模型迁移情况下，对UrbanSound8K数据集和GTAZN数据集的高准确度分类任务。

    

    最近，基于卷积神经网络（CNN）和自注意机制的大规模架构已经成为音频分类领域的必要技术。虽然这些技术是最先进的，但只有通过巨大的计算成本和参数、大量的数据增强、来自大型数据集的迁移以及一些其他技巧才能保证有效性。通过利用音频的轻量级特性，我们提出了一种高效的网络结构——配对逆金字塔结构（PIP），以及一种称为配对逆金字塔结构MLP网络（PIPMN）的网络。PIP网络在UrbanSound8K数据集上达到96%的环境声音分类准确度，在GTAZN数据集上达到93.2%的音乐流派分类准确度，仅使用100万个参数即可实现这两个结果，而不需要进行数据增强或模型迁移。公共代码可在以下网址获取：https://github.com/JNAIC/PIPMN

    Recently, massive architectures based on Convolutional Neural Network (CNN) and self-attention mechanisms have become necessary for audio classification. While these techniques are state-of-the-art, these works' effectiveness can only be guaranteed with huge computational costs and parameters, large amounts of data augmentation, transfer from large datasets and some other tricks. By utilizing the lightweight nature of audio, we propose an efficient network structure called Paired Inverse Pyramid Structure (PIP) and a network called Paired Inverse Pyramid Structure MLP Network (PIPMN). The PIPMN reaches 96\% of Environmental Sound Classification (ESC) accuracy on the UrbanSound8K dataset and 93.2\% of Music Genre Classification (MGC) on the GTAZN dataset, with only 1 million parameters. Both of the results are achieved without data augmentation or model transfer. Public code is available at: https://github.com/JNAIC/PIPMN
    
[^120]: 不需要重建的预测自组织众智系统的局部行为

    Forecasting Local Behavior of Self-organizing Many-agent System without Reconstruction. (arXiv:2210.17289v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17289](http://arxiv.org/abs/2210.17289)

    本文提出了一种CNN-LSTM模型，可以在不需要重建所有代理状态的情况下，预测自组织多代理系统中特定代理的状态。所提出的模型在森林火灾模型中的实验中表现出更好的性能，可以提高自组织众智系统的效率和可扩展性。

    

    大型多代理系统通常由局部定义的代理交互驱动，即自组织。本文的首要目标是确定这种局部交互扩散到特定感兴趣代理的时间。虽然可以使用重建所有代理状态的传统方法，但可能会带来不必要的计算成本。我们研究了一种CNN-LSTM模型，可以在不重建的情况下预测大型自组织多代理系统中特定代理的状态。所提出的模型包括CNN编码器以低维度向量表示系统，LSTM模块学习向量空间中的代理动态，并预测未来代理状态的MLP解码器。我们以森林火灾模型为例，旨在预测特定树代理何时开始燃烧。我们将所提出的模型与重建型方法（如CNN-LSTM和ConvLSTM）进行比较。所提出的模型表现出更好的性能，可以预测局部代理的行为而无需重建所有代理状态，从而实现更高效和可扩展的自组织众智系统应用。

    Large multi-agent systems are often driven by locally defined agent interactions, which is referred to as self-organization. Our primary objective is to determine when the propagation of such local interactions will reach a specific agent of interest. Although conventional approaches that reconstruct all agent states can be used, they may entail unnecessary computational costs. In this paper, we investigate a CNN-LSTM model to forecast the state of a particular agent in a large self-organizing multi-agent system without the reconstruction. The proposed model comprises a CNN encoder to represent the system in a low-dimensional vector, a LSTM module to learn agent dynamics in the vector space, and a MLP decoder to predict the future state of an agent. As an example, we consider a forest fire model where we aim to predict when a particular tree agent will start burning. We compare the proposed model with reconstruction-based approaches such as CNN-LSTM and ConvLSTM. The proposed model exh
    
[^121]: E-MCTS：通过规划表观不确定性进行深度探索的模型基强化学习

    E-MCTS: Deep Exploration in Model-Based Reinforcement Learning by Planning with Epistemic Uncertainty. (arXiv:2210.13455v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13455](http://arxiv.org/abs/2210.13455)

    本文提出了一种新的方法E-MCTS，通过在MCTS预测中应用表观不确定性估计，实现了模型基强化学习中的深度探索，以及规划探索策略。通过实验证明这种方法在成功的表观不确定性估计和深度探索方面表现优异。

    

    模拟退火树搜索（MCTS）是模型基强化学习中应用最广泛、性能最优秀的规划方法之一。MCTS的关键挑战在于深度探索和面对未知时的可靠性，这两个挑战可以通过在MCTS预测中使用原则性的表观不确定性估计来缓解。本文提出了两个主要贡献：首先，我们开发了一种在MCTS中传播表观不确定性的方法，使智能体能够估计其预测的表观不确定性。其次，我们利用传播的不确定性提出了一种新的深度探索算法，通过明确规划探索策略。我们将这种方法应用于基于MCTS的模型基强化学习方法中，包括使用学习和提供的模型，通过实验证明了我们的方法实现了成功的表观不确定性估计并进行了深度探索。我们将其与基于非规划的深度探索基线进行了比较，并表明...

    One of the most well-studied and highly performing planning approaches used in Model-Based Reinforcement Learning (MBRL) is Monte-Carlo Tree Search (MCTS). Key challenges of MCTS-based MBRL methods remain dedicated deep exploration and reliability in the face of the unknown, and both challenges can be alleviated through principled epistemic uncertainty estimation in the predictions of MCTS. We present two main contributions: First, we develop methodology to propagate epistemic uncertainty in MCTS, enabling agents to estimate the epistemic uncertainty in their predictions. Second, we utilize the propagated uncertainty for a novel deep exploration algorithm by explicitly planning to explore. We incorporate our approach into variations of MCTS-based MBRL approaches with learned and provided models, and empirically show deep exploration through successful epistemic uncertainty estimation achieved by our approach. We compare to a non-planning-based deep-exploration baseline, and demonstrate
    
[^122]: STAP: 序列化任务无关策略

    STAP: Sequencing Task-Agnostic Policies. (arXiv:2210.12250v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.12250](http://arxiv.org/abs/2210.12250)

    STAP提出了一种可扩展框架，能够训练操作技能并在规划时协调它们的几何依赖关系，从而解决任何技能在训练期间都没有见过的长远任务，以此提升长远任务的成功率。

    

    机器人技能获取的进步使构建下游操纵任务通用的学习技能库成为可能。然而，单纯地执行这些技能进行任务总会失败，因为它没有考虑到长远计划中普遍存在的动作依赖关系。我们提出了一种名为STAP的可扩展框架，用于训练操作技能和在规划时协调它们的几何依赖关系，以解决任何技能在训练期间都没有见过的长远任务。鉴于Q函数编码了技能可行性的度量，我们制定了一个优化问题，最大化所有技能联合成功的计划序列中的成功，并通过它们的Q值的乘积来估计。我们的实验表明，该目标函数近似于实现计划的可能性，当用作规划目标时，能减少近视行为，从而促进长远任务的成功。我们进一步展示了STAP在多个任务和机器人平台上的有效性和可伸缩性。

    Advances in robotic skill acquisition have made it possible to build general-purpose libraries of learned skills for downstream manipulation tasks. However, naively executing these skills one after the other is unlikely to succeed without accounting for dependencies between actions prevalent in long-horizon plans. We present Sequencing Task-Agnostic Policies (STAP), a scalable framework for training manipulation skills and coordinating their geometric dependencies at planning time to solve long-horizon tasks never seen by any skill during training. Given that Q-functions encode a measure of skill feasibility, we formulate an optimization problem to maximize the joint success of all skills sequenced in a plan, which we estimate by the product of their Q-values. Our experiments indicate that this objective function approximates ground truth plan feasibility and, when used as a planning objective, reduces myopic behavior and thereby promotes long-horizon task success. We further demonstra
    
[^123]: RARR: 使用语言模型研究和修正其输出结果中的不确定信息

    RARR: Researching and Revising What Language Models Say, Using Language Models. (arXiv:2210.08726v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.08726](http://arxiv.org/abs/2210.08726)

    RARR是一个可以对不确定信息进行研究和修订的系统，它可以自动找到文本生成模型输出的归因并修正不支持的内容。

    

    现在的语言模型在诸如少样本学习、问答、推理和对话等许多任务上表现出色。然而，它们有时会生成无支持或误导性的内容。由于大多数语言模型没有任何内置的归因外部证据的机制，用户很难确定它们的输出是否可靠。为了在保留最新一代模型的所有强大优势的同时实现归因，我们提出了 RARR (使用研究和修订进行改进归因)系统，它 1) 自动找到任何文本生成模型输出的归因并 2) 在尽可能保留原始输出的同时，修正不支持的内容。当应用于几个最先进的语言模型在各种输出任务上的结果时，我们发现RARR在显著提高归因率的同时，比以前探索的编辑模型更能保留原始输入。

    Language models (LMs) now excel at many tasks such as few-shot learning, question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, 
    
[^124]: 如何在数据污染的情况下筛选出干净的数据子集？

    How to Sift Out a Clean Data Subset in the Presence of Data Poisoning?. (arXiv:2210.06516v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.06516](http://arxiv.org/abs/2210.06516)

    外部数据可能引入攻击者篡改的有毒数据，为了提高毒性防御性能，需要准确地从数据集中筛选出干净的子集。

    

    鉴于现代机器学习模型所需的大量数据，越来越多地使用外部供应商。然而，合并外部数据会带来数据污染的风险，攻击者可以操纵他们的数据以降低模型的效用或完整性。大多数毒化防御都假定可以访问一组干净的数据（或基础集）。然而，鉴于隐蔽性毒化攻击的快速增长研究，一个问题出现了：防御者真的能够在被污染的数据集中确定一个干净的子集以支持防御吗？本文从研究有毒样本错误地混入基础集后对防御的影响开始。我们分析了五种防御方法并发现它们的性能会在基础集中污染点少于1％时急剧下降。这些发现表明，在这些防御的性能方面，精确地筛选出一个基础集是关键。受这些观察的启发，我们研究了如何精确确定现有的自动化方法，以在污染数据中鉴别一个干净的子集。

    Given the volume of data needed to train modern machine learning models, external suppliers are increasingly used. However, incorporating external data poses data poisoning risks, wherein attackers manipulate their data to degrade model utility or integrity. Most poisoning defenses presume access to a set of clean data (or base set). While this assumption has been taken for granted, given the fast-growing research on stealthy poisoning attacks, a question arises: can defenders really identify a clean subset within a contaminated dataset to support defenses?  This paper starts by examining the impact of poisoned samples on defenses when they are mistakenly mixed into the base set. We analyze five defenses and find that their performance deteriorates dramatically with less than 1% poisoned points in the base set. These findings suggest that sifting out a base set with high precision is key to these defenses' performance. Motivated by these observations, we study how precise existing auto
    
[^125]: 关于神经ODE的正向不变性

    On the Forward Invariance of Neural ODEs. (arXiv:2210.04763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04763](http://arxiv.org/abs/2210.04763)

    该论文提出了一种利用控制障碍函数使神经ODE满足输出规范的方法，该方法保证了输出规范，且在训练和推理过程中可以通过改变受限参数/输入进行操作，此外，该方法还创造了额外的鲁棒性。

    

    我们提出了一种新的方法，通过使用不变集传播来确保神经常微分方程(ODE)满足输出规范。 我们的方法使用一类控制障碍函数将输出规范转换为对学习系统的参数和输入的约束条件。 这个设置使我们能够通过在训练和推理过程中改变受限参数/输入来实现输出规范保证。 此外，我们证明我们通过数据控制的神经ODE的不变集传播不仅保持了概括性能，而且通过启用对系统参数/输入的因果操作创造了额外的鲁棒性。 我们在一系列表示学习任务上测试了我们的方法，包括物理动力学和凸性画像，以及自主车辆的安全避碰。

    We propose a new method to ensure neural ordinary differential equations (ODEs) satisfy output specifications by using invariance set propagation. Our approach uses a class of control barrier functions to transform output specifications into constraints on the parameters and inputs of the learning system. This setup allows us to achieve output specification guarantees simply by changing the constrained parameters/inputs both during training and inference. Moreover, we demonstrate that our invariance set propagation through data-controlled neural ODEs not only maintains generalization performance but also creates an additional degree of robustness by enabling causal manipulation of the system's parameters/inputs. We test our method on a series of representation learning tasks, including modeling physical dynamics and convexity portraits, as well as safe collision avoidance for autonomous vehicles.
    
[^126]: 多尺度拓扑奇异性检测

    Topological Singularity Detection at Multiple Scales. (arXiv:2210.00069v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00069](http://arxiv.org/abs/2210.00069)

    本文提出了一种多尺度拓扑奇异性检测方法，可以评估数据的局部固有维度，并量化点的“流形度”，能够检测复杂空间和图像中的奇异性。

    

    流形假设是现代机器学习研究的一个基本假设，它假定数据位于或接近于低固有维度的未知流形上。然而，最近的研究表明，现实世界的数据表现出明显的非流形结构，即奇异性，这可能导致错误的发现。因此，检测这种奇异性在插值和推断任务之前是至关重要的。我们通过开发一个拓扑框架来解决这个问题，该框架能够（i）量化局部固有维度，以及（ii）在多个尺度上产生“欧几里得性”评分，用以评估点的“流形度”。我们的方法可以在图像数据中捕获复杂空间的奇异性，同时捕捉奇异结构和局部几何复杂性。

    The manifold hypothesis, which assumes that data lies on or close to an unknown manifold of low intrinsic dimension, is a staple of modern machine learning research. However, recent work has shown that real-world data exhibits distinct non-manifold structures, i.e. singularities, that can lead to erroneous findings. Detecting such singularities is therefore crucial as a precursor to interpolation and inference tasks. We address this issue by developing a topological framework that (i) quantifies the local intrinsic dimension, and (ii) yields a Euclidicity score for assessing the 'manifoldness' of a point along multiple scales. Our approach identifies singularities of complex spaces, while also capturing singular structures and local geometric complexity in image data.
    
[^127]: 基于赫比学习的游戏智能体群集演化预测研究

    Forecasting Evolution of Clusters in Game Agents with Hebbian Learning. (arXiv:2209.06904v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2209.06904](http://arxiv.org/abs/2209.06904)

    本文研究了如何通过聚类和预测模型来学习游戏智能体群集的演化，提出了一种基于 Hebbian 学习的无监督聚类方法，结合 LSTM 预测模型，能够在多个场景下准确预测群集演化。

    

    大型多智能体系统例如实时策略游戏通常被智能体的集体行为所驱动。例如在星际争霸II中，人类玩家会将空间接近的智能体分组成团队，并控制团队击败敌人。在这种情况下，将游戏中的智能体进行聚类已被用于多智能体强化学习的高效控制以及提供给游戏用户的游戏分析工具等等多个方面。然而，尽管聚类提供了有用的信息，但是在研究多智能体系统在群集级别上的动态学习方面还很少被研究。在本文中，我们提出了一种混合型AI模型，将无监督学习和自监督学习相结合，来预测星际争霸II中群集的演化。我们开发了一种无监督的 Hebbian 学习方法，用于在 Set-to-Cluster 模块中高效地创建可变数量的群集，其推理时间复杂度低于 K-means 聚类。同时，采用了基于长短期记忆（LSTM）的预测模型学习群集的复杂动态，并预测未来群集的归属。通过实验结果，我们证明了所提出的方法优于现有的聚类方法，并能够在游戏的多个场景下准确预测群集演化。

    Large multi-agent systems such as real-time strategy games are often driven by collective behavior of agents. For example, in StarCraft II, human players group spatially near agents into a team and control the team to defeat opponents. In this light, clustering the agents in the game has been used for various purposes such as the efficient control of the agents in multi-agent reinforcement learning and game analytic tools for the game users. However, despite the useful information provided by clustering, learning the dynamics of multi-agent systems at a cluster level has been rarely studied yet. In this paper, we present a hybrid AI model that couples unsupervised and self-supervised learning to forecast evolution of the clusters in StarCraft II. We develop an unsupervised Hebbian learning method in a set-to-cluster module to efficiently create a variable number of the clusters with lower inference time complexity than K-means clustering. Also, a long short-term memory based prediction
    
[^128]: 对抗检测: 实时攻击目标检测

    Adversarial Detection: Attacking Object Detection in Real Time. (arXiv:2209.01962v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.01962](http://arxiv.org/abs/2209.01962)

    本文首次提出了针对目标检测模型的实时在线攻击，这些攻击可以在所需要的位置制造不存在的物体，攻击成功率约为90\%，揭示了目标检测模型的弱点和安全性问题。

    

    智能机器人依赖目标检测模型来感知环境。随着深度学习安全性的进步，揭示了目标检测模型易受到对抗性攻击的威胁。然而先前的研究主要侧重于攻击静态图像或离线视频。因此，仍不清楚此类攻击是否会危及动态环境下实际的机器人应用。本文通过提出针对目标检测模型的首次实时在线攻击来填补这一空白。我们设计了三种攻击方式，可以在所需位置生成不存在对象的边界框。这些攻击在约20次迭代内达到约90\%的成功率。演示视频可在https://youtu.be/zJZ1aNlXsMU上观看。

    Intelligent robots rely on object detection models to perceive the environment. Following advances in deep learning security it has been revealed that object detection models are vulnerable to adversarial attacks. However, prior research primarily focuses on attacking static images or offline videos. Therefore, it is still unclear if such attacks could jeopardize real-world robotic applications in dynamic environments. This paper bridges this gap by presenting the first real-time online attack against object detection models. We devise three attacks that fabricate bounding boxes for nonexistent objects at desired locations. The attacks achieve a success rate of about 90\% within about 20 iterations. The demo video is available at https://youtu.be/zJZ1aNlXsMU.
    
[^129]: ILLUME：通过人机交互来合理化视觉-语言模型

    ILLUME: Rationalizing Vision-Language Models through Human Interactions. (arXiv:2208.08241v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.08241](http://arxiv.org/abs/2208.08241)

    本文提出了一种新的调整范例，名为ILLUME，通过人机交互来合理化视觉-语言模型，从而使模型的输出更符合人的思维方式。在使用相对较少的训练数据和最少的人类反馈下，ILLUME表现出与标准监督微调相当的竞争力。

    

    基于预训练语言模型的引导已被证明是构建视觉-语言模型（VLM）的有效方法，可用于图像字幕或视觉问题回答等任务。然而，这些模型的输出很少与用户对特定答案的理性相一致。为了改善这种对齐并加强常识原因，我们提出了一种基于人机生成数据的调整范例。我们的ILLUME执行以下循环：给定一个图像-问题-答案提示，VLM样本多个候选原理，人类评论家通过偏好选择提供反馈，用于微调。这个循环增加了训练数据，并逐渐雕刻出与人类意图相一致的VLM的理性能力。我们的详尽实验表明，ILLUME在使用 significantly 更少的训练数据仅需要 minimal 反馈的同时，与标准监督微调具有竞争力。

    Bootstrapping from pre-trained language models has been proven to be an efficient approach for building vision-language models (VLM) for tasks such as image captioning or visual question answering. However, outputs of these models rarely align with user's rationales for specific answers. In order to improve this alignment and reinforce commonsense reasons, we propose a tuning paradigm based on human interactions with machine-generated data. Our ILLUME executes the following loop: Given an image-question-answer prompt, the VLM samples multiple candidate rationales, and a human critic provides feedback via preference selection, used for fine-tuning. This loop increases the training data and gradually carves out the VLM's rationalization capabilities that are aligned with human intent. Our exhaustive experiments demonstrate that ILLUME is competitive with standard supervised finetuning while using significantly fewer training data and only requiring minimal feedback.
    
[^130]: OmniMAE: 图片和视频上的单一模型遮蔽预训练

    OmniMAE: Single Model Masked Pretraining on Images and Videos. (arXiv:2206.08356v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.08356](http://arxiv.org/abs/2206.08356)

    该论文提出了一种基于遮蔽自编码的方法，可以在图像和视频上训练一个简单的单一Vision Transformer模型，而不需要标记数据，该模型的视觉表示可与单模态表示在基准测试上相当或更好，并且使用更简单的架构。

    

    基于Transformer的体系结构在各种视觉领域中已变得竞争力十足，其中最著名的是图像和视频。之前的工作通常是研究这些模态之间的隔离，但是具有相同的架构意味着可以为多个视觉模态训练一个单一的统一模型。之前的统一建模尝试通常使用专门为视觉任务量身定制的架构，或与单模态模型相比表现更差。在这项工作中，我们展示了遮蔽自编码可以用于在图像和视频上训练一个简单的Vision Transformer模型，而无需任何标记数据。这个单一的模型学习的视觉表示与单模态表示在图像和视频基准测试上相当或更好，同时使用更简单的架构。此外，通过删除90％的图像和95％的视频补丁，可以学习该模型，从而实现极快的大型模型架构训练。特别地，我们展示了我们的单一ViT-Hu

    Transformer-based architectures have become competitive across a variety of visual domains, most notably images and videos. While prior work studies these modalities in isolation, having a common architecture suggests that one can train a single unified model for multiple visual modalities. Prior attempts at unified modeling typically use architectures tailored for vision tasks, or obtain worse performance compared to single modality models. In this work, we show that masked autoencoding can be used to train a simple Vision Transformer on images and videos, without requiring any labeled data. This single model learns visual representations that are comparable to or better than single-modality representations on both image and video benchmarks, while using a much simpler architecture. Furthermore, this model can be learned by dropping 90% of the image and 95% of the video patches, enabling extremely fast training of huge model architectures. In particular, we show that our single ViT-Hu
    
[^131]: 通过领域适应实现公平分类：一种双重对抗学习方法

    Fair Classification via Domain Adaptation: A Dual Adversarial Learning Approach. (arXiv:2206.03656v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03656](http://arxiv.org/abs/2206.03656)

    通过利用类似域的辅助信息，本论文提出了一种双重对抗学习方法，以实现没有敏感属性的目标域的公平分类。

    

    现代的机器学习模型越来越受欢迎，在决策系统中得到广泛应用。然而，研究表明机器学习歧视和不公的问题十分严重，这些问题阻碍了高风险应用的采用。公平分类器的最近研究引起了人们的广泛关注，旨在开发有效的算法实现公平和良好的分类性能。尽管这些公平感知的机器学习模型取得了极大的成功，但大多数现有模型需要使用敏感属性对数据进行预处理，正则化模型学习或后处理预测，以实现公平预测。然而，由于隐私、法律或监管限制，敏感属性常常是不完整甚至不可用的。虽然我们没有敏感属性来训练目标域的公平模型，但可能存在具有敏感属性的类似域。因此，利用类似域的辅助信息非常重要。

    Modern machine learning (ML) models are becoming increasingly popular and are widely used in decision-making systems. However, studies have shown critical issues of ML discrimination and unfairness, which hinder their adoption on high-stake applications. Recent research on fair classifiers has drawn significant attention to developing effective algorithms to achieve fairness and good classification performance. Despite the great success of these fairness-aware machine learning models, most of the existing models require sensitive attributes to pre-process the data, regularize the model learning or post-process the prediction to have fair predictions. However, sensitive attributes are often incomplete or even unavailable due to privacy, legal or regulation restrictions. Though we lack the sensitive attribute for training a fair model in the target domain, there might exist a similar domain that has sensitive attributes. Thus, it is important to exploit auxiliary information from a simil
    
[^132]: 通过结构幻化Transformer级联实现楼层平面图的恢复

    Floorplan Restoration by Structure Hallucinating Transformer Cascades. (arXiv:2206.00645v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.00645](http://arxiv.org/abs/2206.00645)

    该文介绍了一个新的极端楼层平面图重建任务和一个神经网络框架用于解决该任务，通过Transformer解码器级联的方式来幻化不可见的房间和门以重建整个楼层平面图，并在701个房屋的基准测试中表现出较好的效果。

    

    本文提出了一项极端的楼层平面图重建任务，为该任务提出了一个新的基准，以及一个神经架构作为解决方案。给定从全景图像中推断或策划的部分楼层平面图重建，任务是重建包括不可见建筑结构在内的完整楼层平面图。提出的神经网络1）通过卷积神经网络和Transformer将输入的部分楼层平面图编码为一组潜在向量；2）通过级联Transformer解码器，在幻化不可见的房间和门的同时重建整个楼层平面图。定性和定量评估证明了我们的方法在701个房屋的基准测试中比最先进的重建技术更有效。我们将分享我们的代码、模型和数据。

    This paper presents an extreme floorplan reconstruction task, a new benchmark for the task, and a neural architecture as a solution. Given a partial floorplan reconstruction inferred or curated from panorama images, the task is to reconstruct a complete floorplan including invisible architectural structures. The proposed neural network 1) encodes an input partial floorplan into a set of latent vectors by convolutional neural networks and a Transformer; and 2) reconstructs an entire floorplan while hallucinating invisible rooms and doors by cascading Transformer decoders. Qualitative and quantitative evaluations demonstrate effectiveness of our approach over the benchmark of 701 houses, outperforming the state-of-the-art reconstruction techniques. We will share our code, models, and data.
    
[^133]: 带有偏好引导的个性化算法干预研究

    Personalized Algorithmic Recourse with Preference Elicitation. (arXiv:2205.13743v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13743](http://arxiv.org/abs/2205.13743)

    研究提出了PEAR方法，这是一个首个能够针对最终用户需求提供个性化算法补救成本的人机交互方法。该方法利用贝叶斯偏好引导的见解，通过最大化原则性信息增益度量来计算目标用户选择的预期效用，然后将偏好引导整合到强化学习框架中。该方法显著提高了算法干预的经济实用性和用户友好性。

    

    算法干预（AR）的问题是计算用户执行一系列操作以颠覆不良机器决策的过程。该过程的操作序列不应该对用户的实施提出过高的要求。然而，大多数AR方法都假设所有用户的操作成本相同，因此可能会向某些用户推荐昂贵的补救计划。为了解决这个问题，我们提出了PEAR，这是一种首个可提供个性化算法补救成本的人机交互方法，以满足任何最终用户的需求。PEAR利用贝叶斯偏好引导的见解，通过向目标用户发出选择集查询来迭代地改善对操作成本的估计值。这些查询的计算是通过最大化选择的预期效用来计算的，这是一种能够考虑成本估计和用户响应不确定性的原则性信息增益度量。PEAR将偏好引导整合到强化学习框架中，同时考虑用户实现AR任务所需达成目标的偏好，以及执行每个操作所涉及的成本。我们通过引入更具挑战性的AR任务来评估PEAR，并显示其比现有的方法找到了更为经济实用且用户友好的补救计划。

    Algorithmic Recourse (AR) is the problem of computing a sequence of actions that -- once performed by a user -- overturns an undesirable machine decision. It is paramount that the sequence of actions does not require too much effort for users to implement. Yet, most approaches to AR assume that actions cost the same for all users, and thus may recommend unfairly expensive recourse plans to certain users. Prompted by this observation, we introduce PEAR, the first human-in-the-loop approach capable of providing personalized algorithmic recourse tailored to the needs of any end-user. PEAR builds on insights from Bayesian Preference Elicitation to iteratively refine an estimate of the costs of actions by asking choice set queries to the target user. The queries themselves are computed by maximizing the Expected Utility of Selection, a principled measure of information gain accounting for uncertainty on both the cost estimate and the user's responses. PEAR integrates elicitation into a Rein
    
[^134]: RoMFAC: 一种对于状态异常干扰具有鲁棒性的均场演员-评论家强化学习算法

    RoMFAC: A robust mean-field actor-critic reinforcement learning against adversarial perturbations on states. (arXiv:2205.07229v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.07229](http://arxiv.org/abs/2205.07229)

    本文提出了RoMFAC算法，通过新的训练目标和重复的正则化损失函数，使其对于异常状态干扰具有鲁棒性并获得出色性能表现。

    

    多智能体强化学习需要基于观察到的系统状态做出最优决策，但是观察中的不确定性可能会导致智能体做出错误的行动。本文提出一种新的算法RoMFAC，它通过一个策略梯度函数和一个代表状态干扰影响的行动损失函数训练演员。同时，RoMFAC还引入一个重复的正则化行动损失函数的方法，以确保训练演员具有出色的性能表现。

    Multi-agent deep reinforcement learning makes optimal decisions dependent on system states observed by agents, but any uncertainty on the observations may mislead agents to take wrong actions. The Mean-Field Actor-Critic reinforcement learning (MFAC) is well-known in the multi-agent field since it can effectively handle a scalability problem. However, it is sensitive to state perturbations that can significantly degrade the team rewards. This work proposes a Robust Mean-field Actor-Critic reinforcement learning (RoMFAC) that has two innovations: 1) a new objective function of training actors, composed of a \emph{policy gradient function} that is related to the expected cumulative discount reward on sampled clean states and an \emph{action loss function} that represents the difference between actions taken on clean and adversarial states; and 2) a repetitive regularization of the action loss, ensuring the trained actors to obtain excellent performance. Furthermore, this work proposes a 
    
[^135]: 增强物理信息神经网络的增广拉格朗日松弛方法（AL-PINNs）

    Enhanced Physics-Informed Neural Networks with Augmented Lagrangian Relaxation Method (AL-PINNs). (arXiv:2205.01059v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.01059](http://arxiv.org/abs/2205.01059)

    本文提出一种增广拉格朗日松弛方法(AL-PINNs)用于物理信息神经网络(PINNs)的训练，该方法通过自适应平衡每个损失组件，能够有效地解决非线性偏微分方程问题。

    

    物理信息神经网络（PINNs）已成为科学计算中深度学习的杰出应用，它们是非线性偏微分方程（PDE）解的强大逼近器。本文提出了一种增广拉格朗日松弛方法（AL-PINNs）用于PINNs的训练。我们将初始和边界条件视为PDE残差优化问题的约束条件。通过应用增广拉格朗日松弛方法，约束优化问题变成了一个顺序的最大-最小问题，使可以学习的参数λ能自适应平衡每个损失组件。我们的理论分析表明，所提出的损失函数的最小化序列收敛于Helmholtz、粘性Burgers和Klein-Gordon方程的实际解。

    Physics-Informed Neural Networks (PINNs) have become a prominent application of deep learning in scientific computation, as they are powerful approximators of solutions to nonlinear partial differential equations (PDEs). There have been numerous attempts to facilitate the training process of PINNs by adjusting the weight of each component of the loss function, called adaptive loss-balancing algorithms. In this paper, we propose an Augmented Lagrangian relaxation method for PINNs (AL-PINNs). We treat the initial and boundary conditions as constraints for the optimization problem of the PDE residual. By employing Augmented Lagrangian relaxation, the constrained optimization problem becomes a sequential max-min problem so that the learnable parameters $\lambda$ adaptively balance each loss component. Our theoretical analysis reveals that the sequence of minimizers of the proposed loss functions converges to an actual solution for the Helmholtz, viscous Burgers, and Klein--Gordon equations
    
[^136]: 距离对选区划分图的影响：中心和异常地图的应用

    Implications of Distance over Redistricting Maps: Central and Outlier Maps. (arXiv:2203.00872v4 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2203.00872](http://arxiv.org/abs/2203.00872)

    本文提出了一种可解释且可操作的选区划分图距离测量方法，并定义了一种“最典型”的中心图。这种方法可以帮助我们深入研究一系列约束条件下选区划分图的应用。

    

    在代议制民主中，选区划分图用于将选民划分为一组选区，每个区选出一个代表。有效的划分图必须满足一系列约束条件，例如紧凑性、连续性、以及几乎相等的人口分布。然而，这些加强的限制条件仍然不足以限制有效选区划分图的数量。本文提出了一种可解释且可操作的距离测量方法，以此研究在一系列约束条件下选区划分图的应用。具体而言，我们定义了一种被认为是“最典型”的中心图，并通过展示它在一个委员会场景中反映了Kemeny（凯门耶）排名的良好性来给出了严格的证明。

    In representative democracy, a redistricting map is chosen to partition an electorate into a collection of districts each of which elects a representative. A valid redistricting map must satisfy a collection of constraints such as being compact, contiguous, and of almost equal population. However, these imposed constraints are still loose enough to enable an enormous ensemble of valid redistricting maps. This fact introduces a difficulty in drawing redistricting maps and it also enables a partisan legislature to possibly gerrymander by choosing a map which unfairly favors it. In this paper, we introduce an interpretable and tractable distance measure over redistricting maps which does not use election results and study its implications over the ensemble of redistricting maps. Specifically, we define a central map which may be considered as being "most typical" and give a rigorous justification for it by showing that it mirrors the Kemeny ranking in a scenario where we have a committee 
    
[^137]: 零样本机器遗忘

    Zero-Shot Machine Unlearning. (arXiv:2201.05629v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.05629](http://arxiv.org/abs/2201.05629)

    零样本机器遗忘是一个新兴的研究问题，允许从已经训练好的ML模型中删除数据。因为这些请求可能会涉及到无法访问的训练数据，因此需要新的解决方法。

    

    现代隐私法规赋予公民被产品、服务和公司遗忘的权利。对于机器学习（ML）应用而言，这需要从存储归档和ML模型中删除数据。由于ML应用需要越来越多的监管合规性，机器遗忘已成为一个不断出现的研究问题。被遗忘请求以删除已经训练好的ML模型中的一定集合或类别的数据的形式提出。实际考虑阻止丢弃删除的数据后从头重新训练模型。现有少数研究使用整个训练数据、训练数据子集或在训练期间存储的一些元数据更新遗忘的模型权重。然而，在许多情况下，训练过程或训练样本相关的数据可能无法访问，我们因此提出问题：是否可能通过零样本学习实现遗忘。

    Modern privacy regulations grant citizens the right to be forgotten by products, services and companies. In case of machine learning (ML) applications, this necessitates deletion of data not only from storage archives but also from ML models. Due to an increasing need for regulatory compliance required for ML applications, machine unlearning is becoming an emerging research problem. The right to be forgotten requests come in the form of removal of a certain set or class of data from the already trained ML model. Practical considerations preclude retraining of the model from scratch after discarding the deleted data. The few existing studies use either the whole training data, or a subset of training data, or some metadata stored during training to update the model weights for unlearning. However, in many cases, no data related to the training process or training samples may be accessible for the unlearning purpose. We therefore ask the question: is it possible to achieve unlearning wit
    
[^138]: AmbiFC: 用证据检验含糊性声明的真实性

    AmbiFC: Fact-Checking Ambiguous Claims with Evidence. (arXiv:2104.00640v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2104.00640](http://arxiv.org/abs/2104.00640)

    本研究提出了一个大规模的事实核查数据集AmbiFC，用于处理现实场景中的含糊性声明核查问题，通过细粒度的证据注释和分析，提出了一种适用于含糊性声明的软标签证据核查方法，并且在注释人员争议分析中发现了相关性。

    

    在实际场景中，自动化事实核查系统必须将声明与检索到的证据进行比较以预测真实性。检索到的证据可能无法明确支持或反驳声明，并产生各种有效解释。现有的事实核查数据集需要模型为每个声明预测单个真实性标签，并且缺乏管理此类模糊性的能力。我们提出了一个大规模的事实核查数据集AmbiFC，其中包含从完整维基百科页面中获取的经过细粒度证据注释的信息需求的现实声明。我们彻底分析了AmbiFC中涉及含糊声明引起的争议，观察到与注释人员的自我评估和专家注释的语言现象强烈相关的注释人员争议。我们引入基于证据的含糊声明的真实性核查任务，比较了三种方法，其中包含注释信号和单标签分类。

    Automated fact-checking systems in real-world scenarios must compare claims with retrieved evidence to predict the veracity. The retrieved evidence may not unambiguously support or refute the claim and yield diverse valid interpretations. Existing fact-checking datasets necessitate that models predict a single veracity label for each claim and lack the ability to manage such ambiguity. We present AmbiFC, a large-scale fact-checking dataset with realistic claims derived from real-world information needs. Our dataset contains fine-grained evidence annotations of passages from complete Wikipedia pages. We thoroughly analyze disagreements arising from ambiguous claims in AmbiFC, observing a strong correlation of annotator disagreement with their self-assessment and expert-annotated linguistic phenomena. We introduce the task of evidence-based fact-checking for ambiguous claims with soft labels, and compare three methodologies incorporating annotation signals with a single-label classificat
    
[^139]: 一种高效的腿式机器人可行性保证范式

    An Efficient Paradigm for Feasibility Guarantees in Legged Locomotion. (arXiv:2011.07967v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2011.07967](http://arxiv.org/abs/2011.07967)

    本文提出了一种高效的范式，可以设计可行的质心和身体轨迹，同时满足动态平衡、关节扭矩和运动限制。在模拟实验中验证了该方法的有效性。

    

    在任意地形上为腿式机器人设计可行的身体轨迹是一项具有挑战性的任务。在本文中，我们提出了一种范式，可以高效地设计可行的质心和身体轨迹。我们提出了一种改进的可行区域的一般公式，可以在保证动态平衡的同时，高效地满足关节扭矩和运动限制。为了考虑运动限制的可行性，我们引入了一种计算质心可达区域的算法。此外，我们还提出了一种高效的规划策略，利用改进的可行区域来设计可行的质心和身体方向轨迹。最后，我们在一个模拟的四足机器人上验证了我们的方法的能力，使其能够在具有挑战性的地形中执行各种任务。

    Developing feasible body trajectories for legged systems on arbitrary terrains is a challenging task. In this paper, we present a paradigm that allows to design feasible Center of Mass (CoM) and body trajectories in an efficient manner. In our previous work [1], we introduced the notion of the 2D feasible region, where static balance and the satisfaction of joint torque limits were guaranteed, whenever the projection of the CoM lied inside the proposed admissible region. In this work we propose a general formulation of the improved feasible region that guarantees dynamic balance alongside the satisfaction of both joint-torque and kinematic limits in an efficient manner. To incorporate the feasibility of the kinematic limits, we introduce an algorithm that computes the reachable region of the CoM. Furthermore, we propose an efficient planning strategy that utilizes the improved feasible region to design feasible CoM and body orientation trajectories. Finally, we validate the capabilitie
    
[^140]: 通过信息最大化的终止评估学习多样化选项

    Learning Diverse Options via InfoMax Termination Critic. (arXiv:2010.02756v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.02756](http://arxiv.org/abs/2010.02756)

    本文提出了一种通过最大化状态和动作选项之间的互信息来学习选项的终止条件的方法，从而提高学习到的选项的多样性和可重用性，在实验中取得了显著的效果。

    

    本文考虑了深度强化学习中自动学习可重用的持续性动作选项的问题。虽然选项可以作为可重用的构建模块来加速迁移学习，但是为未知任务分布学习可重用选项仍然具有挑战性。本文受到基于互信息技术的技能学习的启发，提出通过最大化选项和对应状态转换之间的互信息来学习选项的终止条件。我们通过梯度上升导出了这种互信息最大化的可扩展近似方法，并称之为InfoMax Termination Critic（IMTC）算法。本文的实验结果表明，IMTC显著提高了学习选项的多样性，且不需要外部奖励，仅结合内在的选项学习方法。此外，我们通过将学习到的选项转移到各种任务中测试其可重用性，证实了其可行性。

    We consider the problem of autonomously learning reusable temporally extended actions, or options, in reinforcement learning. While options can speed up transfer learning by serving as reusable building blocks, learning reusable options for unknown task distribution remains challenging. Motivated by the recent success of mutual information (MI) based skill learning, we hypothesize that more diverse options are more reusable. To this end, we propose a method for learning termination conditions of options by maximizing MI between options and corresponding state transitions. We derive a scalable approximation of this MI maximization via gradient ascent, yielding the InfoMax Termination Critic (IMTC) algorithm. Our experiments demonstrate that IMTC significantly improves the diversity of learned options without extrinsic rewards combined with an intrinsic option learning method. Moreover, we test the reusability of learned options by transferring options into various tasks, confirming that
    

