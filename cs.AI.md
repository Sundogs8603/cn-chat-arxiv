# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://arxiv.org/abs/2404.02151) | 展示了对齐的LLM对简单自适应越狱攻击不具有鲁棒性，并成功实现了在多个模型上几乎100%的攻击成功率，同时还介绍了对于不公开logprobs的模型如何进行越狱以及如何在受污染的模型中查找木马字符串的方法。 |
| [^2] | [Evaluating Text-to-Visual Generation with Image-to-Text Generation](https://arxiv.org/abs/2404.01291) | 引入VQAScore，利用视觉问答模型生成对齐分数，取得了在图像文本对齐基准上的最先进结果 |
| [^3] | [Mechanisms of non-factual hallucinations in language models](https://arxiv.org/abs/2403.18167) | 研究揭示了语言模型中非事实性幻觉的两个通用机制：主题属性知识不足和未能正确选择对象属性，这有助于深入理解和减轻幻觉。 |
| [^4] | [Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence](https://arxiv.org/abs/2403.17993) | 人工智能对通过创新性使用深度神经网络推动湍流减少的拉格朗日模型具有重要影响，为AI和湍流研究之间紧密交织的未来铺平道路。 |
| [^5] | [Qibo: A Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2403.16056) | 本论文在中医领域构建了专业语料库，基于LLaMA成功开发了首个经过完整训练的Qibo模型，并推出了用于评估LLMs性能的Qibo基准测试。 |
| [^6] | [Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4](https://arxiv.org/abs/2403.05680) | 提出了一种新颖的评估框架，用于评估视觉-语言LLMs在生成CT异常的准确摘要方面的能力。 |
| [^7] | [HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction](https://arxiv.org/abs/2403.05396) | HistGen是一个通过本地-全局特征编码和跨模态上下文交互来生成组织病理学报告的框架，提供了第一个用于评估的基准数据集。 |
| [^8] | [Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication](https://arxiv.org/abs/2402.18439) | 挑战了默认使用自然语言的做法，通过探索LLMs在推理和沟通中使用替代格式的实用性，实现了推理效率的提升和多智能体通信时标记使用量的显著减少。 |
| [^9] | [PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs](https://arxiv.org/abs/2402.12835) | PANDA是一种旨在增强LLMs领域特定能力的方法，通过利用专家模型响应偏好的见解，无需微调即可实现显著改进。 |
| [^10] | [Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models](https://arxiv.org/abs/2402.11436) | 大型语言模型存在自我偏见，研究发现通过更大的模型规模和准确评估的外部反馈可以显著减少这种偏见，并提高后续任务的实际表现。 |
| [^11] | [Pseudorandom Error-Correcting Codes](https://arxiv.org/abs/2402.09370) | 我们构建了一种伪随机纠错码，它对替换和删除错误具有鲁棒性，并且可以高效解码。我们还使用伪随机码提出了一种对语言模型输出进行水印处理的方案，该方案对裁剪和随机替换、删除具有恒定的鲁棒性。 |
| [^12] | [AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach](https://arxiv.org/abs/2402.09334) | "AuditLLM"是一种能够以系统的方式评估各种LLMs的性能的工具，通过使用从单个问题生成的多个探测来对给定的LLM进行审计，从而识别模型在理解或操作方面的任何不一致性。 |
| [^13] | [Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries](https://arxiv.org/abs/2402.08349) | 本文首次深入评估了在实践中文本到SQL系统的数据模型的鲁棒性，通过基于一个多年的国际项目集中评估，对一个在FIFA World Cup背景下连续运行了9个月的真实部署的FootballDB系统进行了评估。 |
| [^14] | [Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric](https://arxiv.org/abs/2402.06900) | 本研究提出了一种基于大型语言模型（LLMs）的自动度量方法，用于识别生成文本中的毒性。通过分析毒性因素和LLMs的内在毒性属性，该方法在测量毒性方面表现出众，比现有指标提升12个百分点。 |
| [^15] | [Non-autoregressive Generative Models for Reranking Recommendation](https://arxiv.org/abs/2402.06871) | 本研究提出了一个非自回归的生成模型用于排序推荐，在多阶段推荐系统中扮演关键角色。该模型旨在提高效率和效果，并解决稀疏训练样本和动态候选项对模型收敛性的挑战。 |
| [^16] | [You Only Need One Color Space: An Efficient Network for Low-light Image Enhancement](https://arxiv.org/abs/2402.05809) | 本文提出了一种用于低光图像增强的高效网络，通过引入可训练的水平/垂直强度（HVI）颜色空间来解耦亮度和颜色，并设计了颜色和强度解耦网络（CIDNet）以改善增强过程中的稳定性。结果显示，该方法可以减少增强图像中的颜色和亮度伪影。 |
| [^17] | [Multi-Sender Persuasion -- A Computational Perspective](https://arxiv.org/abs/2402.04971) | 这项研究考虑了多个具有信息优势的发信者向单个自私行为者传递信号以影响其行为的问题，并提出了一种新颖的可微神经网络方法来近似解决这一问题。通过额外梯度算法，我们发现了超越已有方法的局部均衡解。 |
| [^18] | [User-Centric Evaluation of ChatGPT Capability of Generating R Program Code](https://arxiv.org/abs/2402.03130) | 本文评估了ChatGPT从自然语言输入生成R编程语言代码的能力，通过多次尝试的过程，并对生成的解决方案进行了精确性、完整性、简洁性等多个质量属性的评估。 |
| [^19] | [Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues](https://arxiv.org/abs/2402.01737) | 本文旨在开发LLM代理以减轻多代理设置下谈判中的社交规范违反。我们引入了基于价值影响的环境学习（ICL）方法，用于为基于LLM的修正代理识别高质量的ICL示例，其中价值影响函数衡量谈判结果的质量。我们展示了这种方法与策略学习的联系，并提供了丰富的实证证据来证明其在三个不同主题的谈判中的有效性。 |
| [^20] | [Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/abs/2402.00798) | 本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。 |
| [^21] | [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805) | Gemini家族是一系列在图像、音频、视频和文本理解方面表现出色的多模态模型，其中最具能力的Gemini Ultra模型在30个基准测试中推进了技术前沿，并改进了所有20个多模态基准测试的技术状态。 |
| [^22] | [Uncovering communities of pipelines in the task-fMRI analytical space](https://arxiv.org/abs/2312.06231) | 本论文通过使用社群检测算法揭示了任务fMRI分析空间中的流程社群，并评估了不同背景下的流程关系的稳定性。研究表明，存在一些子集的流程给出相似的结果，特别是那些分享特定参数。这些结果对于参与者群体来说是稳定的，但在不同任务之间不稳定。流程空间的形成主要受到大脑激活区域大小和统计值规模的影响。 |
| [^23] | [ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach](https://arxiv.org/abs/2311.16136) | 本文提出了ERASER，一种通过推理服务器感知的方法，在MLaaS中实现机器学习的去学习。 |
| [^24] | [Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning.](http://arxiv.org/abs/2401.15935) | 本研究通过比较研究和混合方法，调查了事件序列的自我监督学习技术，并引入了一种新的方法，将生成模型和对比嵌入进行对齐。结果显示，这种对齐模型在各种任务上表现优越，为预测事件序列中的信息提供了潜在的好处。 |
| [^25] | [Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models.](http://arxiv.org/abs/2401.15269) | 本论文介绍了一种名为Self-BioRAG的框架，通过使用检索和自我反思的方法，提高了医疗推理的能力。该框架专注于生成解释、检索领域特定文档以及对生成的响应进行自我反思。 |
| [^26] | [Towards Goal-oriented Large Language Model Prompting: A Survey.](http://arxiv.org/abs/2401.14043) | 本文调查了大型语言模型(LLM)中目标导向提示工程的重要性。通过对35个代表性研究的回顾，我们发现引导LLM遵循人类的逻辑思维的目标导向提示公式显著提高了LLM的性能。我们还提出了一个新的分类体系，并总结了十个适用任务来展示我们框架的广泛适用性。同时，我们提出了四个未来的方向，以推动目标导向提示工程的进一步发展。 |
| [^27] | [Prompt Weight Experiments for LLM Instruction Fine-Tuning.](http://arxiv.org/abs/2401.13586) | LLM指令微调中，对于短提示完成数据集，提示词标记分类损失加权（PLW）与性能呈负二次关系，而长提示完成数据集则不受PLW影响。 |
| [^28] | [Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction and Beyond.](http://arxiv.org/abs/2401.13432) | 本文提出了一种半监督耦合薄板样条模型，用于处理旋转校正等图像变形任务。通过耦合多个薄板样条变换，有效消除了插值误差，突破了控制点数量瓶颈，并且通过半监督学习方法，可以在有限的带标注样本条件下，充分利用未标注样本来提高模型性能。 |
| [^29] | [BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving.](http://arxiv.org/abs/2401.01065) | BEV-CLIP是一种用于自动驾驶中复杂场景的多模态BEV检索方法，通过使用描述性文本进行检索，利用大型语言模型的 semantic feature extraction 和知识图谱的半结构化信息来提高检索准确性。 |
| [^30] | [Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review.](http://arxiv.org/abs/2310.14735) | 这篇论文解释了提示工程在释放大型语言模型能力方面的关键作用，探讨了不同的提示方法以及外部插件如何协助减少机器幻想，并指出了未来研究方向的重要性。 |
| [^31] | [Language Models as Zero-Shot Trajectory Generators.](http://arxiv.org/abs/2310.11604) | 本文研究了使用大型语言模型（LLMs）作为零-shot轨迹生成器的可能性。通过给予LLM物体检测和分割视觉模型的访问权限，研究人员发现LLMs能够直接预测操作技能中的末端执行器姿态序列，并在26个真实世界的语言任务中取得了良好效果。这一研究突破了对LLMs在机器人技术中的限制，揭示了LLMs确实具有对操作任务的理解能力。 |
| [^32] | [Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey.](http://arxiv.org/abs/2310.01424) | 这项调查对语言模型（LMs）中的隐私风险进行了研究和减轻措施的探讨，通过分类法和调查现有攻击，提出了关键趋势，并讨论现有的减轻策略的优势和局限性，指出关键差距和未来工作方向。 |
| [^33] | [Generative Escher Meshes.](http://arxiv.org/abs/2309.14564) | 本文提出了一种全自动的生成方法，用于生成周期性的非正方形镶嵌图案，该方法通过优化几何和颜色来生成与所需对象形状和外观相似的瓷砖。 |
| [^34] | [Multimodal Pretrained Models for Sequential Decision-Making: Synthesis, Verification, Grounding, and Perception.](http://arxiv.org/abs/2308.05295) | 该论文介绍了一种利用预训练模型的知识来解决序列决策任务的算法，通过构建和验证控制器，并通过视觉观测来与任务环境相连接。 |
| [^35] | [Promoting Exploration in Memory-Augmented Adam using Critical Momenta.](http://arxiv.org/abs/2307.09638) | 本研究提出了一种记忆增强型Adam方法，通过使用关键动量项的缓冲区来促进对更平坦最小值的探索。实验证明，该方法在标准的监督语言建模和图像分类任务中提高了几种Adam变体的性能。 |
| [^36] | [Integrated Planning in Hospitals: A Review.](http://arxiv.org/abs/2307.05258) | 本文综述了医院综合规划的运筹学和管理科学文献，强调了综合规划多个资源的潜力，并提供了关于不确定性建模和使用现实数据等方面的分析。 |
| [^37] | [SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation.](http://arxiv.org/abs/2307.01646) | 本文提出了一种新的图生成扩散模型SwinGNN，通过使用高效的2-WL消息传递网络和移动窗口自注意力，以及结合关键的训练和采样技术，显著提高了图生成样本的质量，并引入了随机置换的后处理技巧转换生成的图形统计量。 |
| [^38] | [Challenges of Zero-Shot Recognition with Vision-Language Models: Granularity and Correctness.](http://arxiv.org/abs/2306.16048) | 本文研究了将视觉-语言模型应用于零样本视觉识别任务所面临的挑战，发现VLMs在识别细粒度概念方面表现更好，并指出了VLMs中相似度分数不能严格反映正确性的问题，提出了未来研究方向。 |
| [^39] | [On Faking a Nash Equilibrium.](http://arxiv.org/abs/2306.08041) | 本文研究多智能体强化学习中的数据污染攻击，提出了唯一纳什集的概念，并设计了一个线性规划方案来计算最优污染攻击策略。 |
| [^40] | [DU-Shapley: A Shapley Value Proxy for Efficient Dataset Valuation.](http://arxiv.org/abs/2306.02071) | 本论文提出了一种称为DU-Shapley的方法，用于更有效地计算Shapley值，以实现机器学习中的数据集价值评估。 |
| [^41] | [A Survey on Large Language Models for Recommendation.](http://arxiv.org/abs/2305.19860) | 本综述介绍了基于大语言模型的推荐系统，提出了判别式LLMs和生成式LLMs两种模型范式，总结了这些模型的最新进展，强调了该领域的挑战和研究方向。 |
| [^42] | [ChatLog: Recording and Analyzing ChatGPT Across Time.](http://arxiv.org/abs/2304.14106) | ChatLog是一个分析ChatGPT随时间变化的数据集，通过提取ChatGPT知识和语言特征发现了一些稳定的特征，提高了RoBERTa-based detector在新版本ChatGPT上的鲁棒性。 |
| [^43] | [Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition.](http://arxiv.org/abs/2302.05881) | 本文提出了一种新的方法框架GCDTC，利用数值先验和广义CP分解实现了更高的低秩张量补全精度；同时介绍了一个算法SPTC，作为该框架的一个实现。在实验中，该方法表现出比现有技术更好的性能。 |
| [^44] | [A Survey on In-context Learning.](http://arxiv.org/abs/2301.00234) | 本文调查和总结了上下文学习(ICL)的进展和挑战，ICL已成为自然语言处理(NLP)的新范式，探索ICL以评估和推广大型语言模型(LLM)的能力已成为一种新趋势。本文提出了ICL的正式定义，并总结了高级技术，最后讨论了ICL的挑战以及进一步研究的潜在方向。 |

# 详细

[^1]: 用简单自适应攻击越狱功能对齐的LLM

    Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks

    [https://arxiv.org/abs/2404.02151](https://arxiv.org/abs/2404.02151)

    展示了对齐的LLM对简单自适应越狱攻击不具有鲁棒性，并成功实现了在多个模型上几乎100%的攻击成功率，同时还介绍了对于不公开logprobs的模型如何进行越狱以及如何在受污染的模型中查找木马字符串的方法。

    

    我们展示了即使是最新的安全对齐的LLM也不具有抵抗简单自适应越狱攻击的稳健性。首先，我们展示了如何成功利用对logprobs的访问进行越狱：我们最初设计了一个对抗性提示模板（有时会适应目标LLM），然后我们在后缀上应用随机搜索以最大化目标logprob（例如token“Sure”），可能会进行多次重启。通过这种方式，我们实现了对GPT-3.5/4、Llama-2-Chat-7B/13B/70B、Gemma-7B和针对GCG攻击进行对抗训练的HarmBench上的R2D2等几乎100%的攻击成功率--根据GPT-4的评判。我们还展示了如何通过转移或预填充攻击以100%的成功率对所有不暴露logprobs的Claude模型进行越狱。此外，我们展示了如何在受污染的模型中使用对一组受限制的token执行随机搜索以查找木马字符串的方法--这项任务与许多其他任务共享相同的属性。

    arXiv:2404.02151v1 Announce Type: cross  Abstract: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many s
    
[^2]: 评估文本到图像生成与图像到文本生成

    Evaluating Text-to-Visual Generation with Image-to-Text Generation

    [https://arxiv.org/abs/2404.01291](https://arxiv.org/abs/2404.01291)

    引入VQAScore，利用视觉问答模型生成对齐分数，取得了在图像文本对齐基准上的最先进结果

    

    尽管生成式人工智能取得了显著进展，但由于缺乏有效的度量标准和标准化基准，综合评估仍然具有挑战性。为了解决这个问题，我们引入了VQAScore，使用视觉问答（VQA）模型通过计算对简单的“这幅图表现出了'{文本}'吗？”问题的“是”答案的概率来生成对齐分数。尽管比先前的方法更简单，但使用现成模型计算的VQAScore在许多（8个）图像文本对齐基准上产生了最先进的结果。

    arXiv:2404.01291v1 Announce Type: cross  Abstract: Despite significant progress in generative AI, comprehensive evaluation remains challenging because of the lack of effective metrics and standardized benchmarks. For instance, the widely-used CLIPScore measures the alignment between a (generated) image and text prompt, but it fails to produce reliable scores for complex prompts involving compositions of objects, attributes, and relations. One reason is that text encoders of CLIP can notoriously act as a "bag of words", conflating prompts such as "the horse is eating the grass" with "the grass is eating the horse". To address this, we introduce the VQAScore, which uses a visual-question-answering (VQA) model to produce an alignment score by computing the probability of a "Yes" answer to a simple "Does this figure show '{text}'?" question. Though simpler than prior art, VQAScore computed with off-the-shelf models produces state-of-the-art results across many (8) image-text alignment benc
    
[^3]: 语言模型中非事实性幻觉的机制

    Mechanisms of non-factual hallucinations in language models

    [https://arxiv.org/abs/2403.18167](https://arxiv.org/abs/2403.18167)

    研究揭示了语言模型中非事实性幻觉的两个通用机制：主题属性知识不足和未能正确选择对象属性，这有助于深入理解和减轻幻觉。

    

    现今最先进的语言模型（LMs）有时会产生与世界知识不符的非事实幻觉。尽管人们已经付出了大量努力来检测和减轻幻觉，但理解它们的内在机制仍然是困难的。 我们的研究调查了幻觉的机制原因，特别是 LM 在对主题关系查询做出回答时错误地预测对象属性的非事实形式。通过因果中介分析和嵌入空间投影，我们确认了跨不同规模和设计的 LM 中共享的两个造成幻觉的一般机制原因：1）在较低层 MLPs 中主题属性知识不足，以及2）在较高层注意力头和 MLPs 中未能选择正确的对象属性。这两个机制展示了不同程度的主宾关系、预测不确定性和扰动鲁棒性。此外，我们还审查了 LM 的预训练检查点。

    arXiv:2403.18167v1 Announce Type: cross  Abstract: State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge. Despite extensive efforts to detect and mitigate hallucinations, understanding their internal mechanisms remains elusive. Our study investigates the mechanistic causes of hallucination, specifically non-factual ones where the LM incorrectly predicts object attributes in response to subject-relation queries. With causal mediation analysis and embedding space projection, we identify two general mechanistic causes of hallucinations shared across LMs of various scales and designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and 2) failing to select the correct object attribute in upper layer attention heads and MLPs. These two mechanisms exhibit varying degrees of subject-object association, predictive uncertainty and perturbation robustness. Additionally, we scrutinize LM pre-training checkpoints, r
    
[^4]: 将人工智能与自然智能相融合：从统计力学到人工智能再到湍流

    Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence

    [https://arxiv.org/abs/2403.17993](https://arxiv.org/abs/2403.17993)

    人工智能对通过创新性使用深度神经网络推动湍流减少的拉格朗日模型具有重要影响，为AI和湍流研究之间紧密交织的未来铺平道路。

    

    这篇论文反思了人工智能在科学研究中的未来角色，特别关注了湍流研究，并通过根植于非平衡统计力学的扩散模型来检验人工智能的发展，强调了人工智能通过创新性地利用深度神经网络推动减少的拉格朗日湍流模型的重要影响。此外，论文审查了湍流研究中的各种其他人工智能应用，并概述了在人工智能和统计流体力学的同时发展中的潜在挑战和机会。

    arXiv:2403.17993v1 Announce Type: cross  Abstract: The paper reflects on the future role of AI in scientific research, with a special focus on turbulence studies, and examines the evolution of AI, particularly through Diffusion Models rooted in non-equilibrium statistical mechanics. It underscores the significant impact of AI on advancing reduced, Lagrangian models of turbulence through innovative use of deep neural networks. Additionally, the paper reviews various other AI applications in turbulence research and outlines potential challenges and opportunities in the concurrent advancement of AI and statistical hydrodynamics. This discussion sets the stage for a future where AI and turbulence research are intricately intertwined, leading to more profound insights and advancements in both fields.
    
[^5]: Qibo: 一种用于中医领域的大型语言模型

    Qibo: A Large Language Model for Traditional Chinese Medicine

    [https://arxiv.org/abs/2403.16056](https://arxiv.org/abs/2403.16056)

    本论文在中医领域构建了专业语料库，基于LLaMA成功开发了首个经过完整训练的Qibo模型，并推出了用于评估LLMs性能的Qibo基准测试。

    

    在人工智能领域，大型语言模型(LLMs)展示了在用户意图理解和响应方面取得的显著进展，在许多专业领域，包括医学、法律和金融。然而，在中医领域，LLMs的性能提升受到挑战，其原因在于中医理论与现代医学之间的根本差异，以及缺乏专业语料库资源。本文旨在构建和整理中医领域的专业语料库，赋予大型模型具有中医理论特色的专业知识，并成功基于LLaMA开发了Qibo模型，这是中医领域第一个经过完整训练过程（从预训练到监督微调）的LLM。此外，我们开发了Qibo基准测试，这是一个用于评估LLMs性能的专门工具。

    arXiv:2403.16056v1 Announce Type: cross  Abstract: In the field of Artificial Intelligence, Large Language Models (LLMs) have demonstrated significant advances in user intent understanding and response in a number of specialized domains, including medicine, law, and finance. However, in the unique domain of traditional Chinese medicine (TCM), the performance enhancement of LLMs is challenged by the essential differences between its theories and modern medicine, as well as the lack of specialized corpus resources. In this paper, we aim to construct and organize a professional corpus in the field of TCM, to endow the large model with professional knowledge that is characteristic of TCM theory, and to successfully develop the Qibo model based on LLaMA, which is the first LLM in the field of TCM to undergo a complete training process from pre-training to Supervised Fine-Tuning (SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for evaluating the performance of LLMs, whic
    
[^6]: 使用GPT-4对基于视觉的LLM预测进行分解以进行自动评估

    Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4

    [https://arxiv.org/abs/2403.05680](https://arxiv.org/abs/2403.05680)

    提出了一种新颖的评估框架，用于评估视觉-语言LLMs在生成CT异常的准确摘要方面的能力。

    

    CT检查的数量每年都在增加，这导致放射科医生疲劳。大型语言模型（LLMs）有潜力减轻他们的负担，但其在临床中的采用取决于放射科医生的信任和生成内容的简单评估。本文提出了一个新颖的评估框架，用于评估视觉-语言LLMs在生成CT异常的准确摘要方面的能力。

    arXiv:2403.05680v1 Announce Type: new  Abstract: The volume of CT exams being done in the world has been rising every year, which has led to radiologist burn-out. Large Language Models (LLMs) have the potential to reduce their burden, but their adoption in the clinic depends on radiologist trust, and easy evaluation of generated content. Presently, many automated methods are available to evaluate the reports generated for chest radiographs, but such an approach is not available for CT presently. In this paper, we propose a novel evaluation framework to judge the capabilities of vision-language LLMs in generating accurate summaries of CT-based abnormalities. CT slices containing an abnormality (e.g., lesion) were input to a vision-based LLM (GPT-4V, LLaVA-Med, and RadFM), and it generated a free-text summary of the predicted characteristics of the abnormality. Next, a GPT-4 model decomposed the summary into specific aspects (body part, location, type, and attributes), automatically eval
    
[^7]: HistGen：通过本地-全局特征编码和跨模态上下文交互生成组织病理学报告

    HistGen: Histopathology Report Generation via Local-Global Feature Encoding and Cross-modal Context Interaction

    [https://arxiv.org/abs/2403.05396](https://arxiv.org/abs/2403.05396)

    HistGen是一个通过本地-全局特征编码和跨模态上下文交互来生成组织病理学报告的框架，提供了第一个用于评估的基准数据集。

    

    组织病理学在癌症诊断中扮演着黄金标准的角色，临床报告在解释和理解这一过程中至关重要，在指导癌症治疗和患者护理方面起着关键作用。深度学习对组织病理学报告生成的自动化将极大提升临床效率，并减轻病理学家在报告撰写方面的劳动强度和耗时负担。为追求这一进步，作者引入了HistGen，这是一个多实例学习增强的组织病理学报告生成框架，并提供第一个用于评估的基准数据集。HistGen受诊断和报告撰写工作流程的启发，具有两个精心设计的模块，旨在通过对齐整张切片图像（WSIs）和诊断报告，从本地和全局粒度提升报告生成。为实现这一目标，开发了一个本地-全局分层编码器，用于有效地从区域中聚合视觉特征。

    arXiv:2403.05396v1 Announce Type: cross  Abstract: Histopathology serves as the gold standard in cancer diagnosis, with clinical reports being vital in interpreting and understanding this process, guiding cancer treatment and patient care. The automation of histopathology report generation with deep learning stands to significantly enhance clinical efficiency and lessen the labor-intensive, time-consuming burden on pathologists in report writing. In pursuit of this advancement, we introduce HistGen, a multiple instance learning-empowered framework for histopathology report generation together with the first benchmark dataset for evaluation. Inspired by diagnostic and report-writing workflows, HistGen features two delicately designed modules, aiming to boost report generation by aligning whole slide images (WSIs) and diagnostic reports from local and global granularity. To achieve this, a local-global hierarchical encoder is developed for efficient visual feature aggregation from a regi
    
[^8]: 超越自然语言：LLM利用替代格式进行增强推理和沟通

    Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication

    [https://arxiv.org/abs/2402.18439](https://arxiv.org/abs/2402.18439)

    挑战了默认使用自然语言的做法，通过探索LLMs在推理和沟通中使用替代格式的实用性，实现了推理效率的提升和多智能体通信时标记使用量的显著减少。

    

    自然语言（NL）长期以来一直是人类认知和沟通的主要格式，因此，在大型语言模型（LLMs）的发展和应用中同样至关重要。然而，除了NL之外，LLMs在预训练过程中看到了各种非NL格式，如代码和逻辑表达式。NL作为LLMs的最佳格式，在单一LLM推理和多智能体通信中的地位尚未得到彻底审查。在这项工作中，我们挑战了默认使用NL，通过探索在这些上下文中使用非NL格式的效用。我们展示，允许LLMs在推理或沟通之前自主选择最合适的格式，可导致不同LLMs推理效率提高3.3至5.7％，并且在多智能体通信中最多可减少72.7％的标记使用，同时保持沟通效果。我们的综合分析进一步揭示了LLMs可以......

    arXiv:2402.18439v1 Announce Type: cross  Abstract: Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs, and up to a 72.7\% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs c
    
[^9]: PANDA: 用于增强LLMs领域特定能力的偏好适应方法

    PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs

    [https://arxiv.org/abs/2402.12835](https://arxiv.org/abs/2402.12835)

    PANDA是一种旨在增强LLMs领域特定能力的方法，通过利用专家模型响应偏好的见解，无需微调即可实现显著改进。

    

    大型语言模型（LLMs）在各种自然语言任务中展示出相当大的能力，但它们通常无法达到特定领域最先进模型的性能水平。增强LLMs领域特定能力的一种潜在方法是使用相应的数据集对其进行微调。然而，这种方法既耗费资源又耗时，并且无法应用于封闭源商业LLMs。在本文中，我们提出了一种称为PANDA的偏好适应方法，旨在通过利用专家模型响应偏好的见解来增强LLMs的领域特定能力，而无需进行微调。我们的实验结果显示，PANDA显著提升了LLMs在文本分类和交互式决策任务上的领域特定能力。此外，具有PANDA的LLM甚至超过了专家模型

    arXiv:2402.12835v1 Announce Type: cross  Abstract: While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models. One potential approach to enhance domain-specific capabilities of LLMs involves fine-tuning them using corresponding datasets. However, this method can be both resource and time-intensive, and not applicable to closed-source commercial LLMs. In this paper, we propose Preference Adaptation for Enhancing Domain-specific Abilities of LLMs (PANDA), a method designed to augment the domain-specific capabilities of LLMs by leveraging insights from the response preference of expert models without requiring fine-tuning. Our experimental results reveal that PANDA significantly enhances the domain-specific ability of LLMs on text classification and interactive decision tasks. Moreover, LLM with PANDA even outperforms the expert model that
    
[^10]: 自我反馈的危险：在大型语言模型中自我偏见被放大

    Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models

    [https://arxiv.org/abs/2402.11436](https://arxiv.org/abs/2402.11436)

    大型语言模型存在自我偏见，研究发现通过更大的模型规模和准确评估的外部反馈可以显著减少这种偏见，并提高后续任务的实际表现。

    

    最近的研究表明，自我反馈可以改善大型语言模型在某些任务上的表现，但在其他任务上却会恶化。我们发现这种矛盾是由于大型语言模型对其自身输出的偏见。本文正式定义了大型语言模型的自我偏见——倾向于偏爱自身生成——并使用两个统计量进行了分析。我们在翻译、受限文本生成和数学推理任务上分析了六种大型语言模型。我们发现自我偏见在所有检测的大型语言模型中都普遍存在，跨多种语言和任务。我们的分析表明，虽然自我改进管道提高了模型输出的流畅性和可理解性，但它进一步放大了自我偏见。为了缓解这种偏见，我们发现更大的模型规模和具有准确评估的外部反馈可以显著减少自我改进管道中的偏见，从而实际改善下游任务的性能。

    arXiv:2402.11436v1 Announce Type: cross  Abstract: Recent studies show that self-feedback improves large language models (LLMs) on certain tasks while worsens other tasks. We discovered that such a contrary is due to LLM's bias towards their own output. In this paper, we formally define LLM's self-bias -- the tendency to favor its own generation -- using two statistics. We analyze six LLMs on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.
    
[^11]: 伪随机纠错码

    Pseudorandom Error-Correcting Codes

    [https://arxiv.org/abs/2402.09370](https://arxiv.org/abs/2402.09370)

    我们构建了一种伪随机纠错码，它对替换和删除错误具有鲁棒性，并且可以高效解码。我们还使用伪随机码提出了一种对语言模型输出进行水印处理的方案，该方案对裁剪和随机替换、删除具有恒定的鲁棒性。

    

    我们构建了伪随机纠错码（或简称为伪随机码），它们是具有以下特性的纠错码：对于任何计算受限的对手来说，任意多个编码词都是伪随机的。通过解码密钥，可以高效地纠正有错误的编码词。我们构建了对替换错误和删除错误具有强鲁棒性的伪随机码，其中伪随机性基于标准密码学假设。具体而言，伪随机性基于LPN问题的$2^{O(\sqrt{n})}$困难程度，或者基于LPN问题和低密度下的插入异或问题的多项式困难程度。

    arXiv:2402.09370v1 Announce Type: cross Abstract: We construct pseudorandom error-correcting codes (or simply pseudorandom codes), which are error-correcting codes with the property that any polynomial number of codewords are pseudorandom to any computationally-bounded adversary. Efficient decoding of corrupted codewords is possible with the help of a decoding key.   We build pseudorandom codes that are robust to substitution and deletion errors, where pseudorandomness rests on standard cryptographic assumptions. Specifically, pseudorandomness is based on either $2^{O(\sqrt{n})}$-hardness of LPN, or polynomial hardness of LPN and the planted XOR problem at low density.   As our primary application of pseudorandom codes, we present an undetectable watermarking scheme for outputs of language models that is robust to cropping and a constant rate of random substitutions and deletions. The watermark is undetectable in the sense that any number of samples of watermarked text are computationa
    
[^12]: AuditLLM:一种使用多探测方法对大型语言模型进行审计的工具

    AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach

    [https://arxiv.org/abs/2402.09334](https://arxiv.org/abs/2402.09334)

    "AuditLLM"是一种能够以系统的方式评估各种LLMs的性能的工具，通过使用从单个问题生成的多个探测来对给定的LLM进行审计，从而识别模型在理解或操作方面的任何不一致性。

    

    当大型语言模型（LLMs）在各种情境中越来越广泛地被采用时，确保它们在特定应用中是相对安全、一致和可靠的变得至关重要。这可能需要对它们进行探测或审计。通过使用多次迭代的单个问题对LLMs进行探测，可以揭示它们的知识或功能的潜在不一致性。然而，目前缺乏一种能够以简单工作流程和低技术门槛进行此类审计的工具。在这个演示中，我们介绍了一种名为"AuditLLM"的新颖工具，它旨在以系统的方式评估各种LLMs的性能。AuditLLM的核心功能在于能够通过使用从单个问题生成的多个探测来对给定的LLM进行审计，从而识别模型在理解或操作方面的任何不一致性。一个相对健壮、可靠和一致的LLM应该对以不同方式或由不同人提出的问题给出语义相似的回答。

    arXiv:2402.09334v1 Announce Type: new Abstract: As Large Language Models (LLMs) gain wider adoption in various contexts, it becomes crucial to ensure they are reasonably safe, consistent, and reliable for an application at hand. This may require probing or auditing them. Probing LLMs with varied iterations of a single question could reveal potential inconsistencies in their knowledge or functionality. However, a tool for performing such audits with simple workflow and low technical threshold is lacking. In this demo, we introduce "AuditLLM," a novel tool designed to evaluate the performance of various LLMs in a methodical way. AuditLLM's core functionality lies in its ability to test a given LLM by auditing it using multiple probes generated from a single question, thereby identifying any inconsistencies in the model's understanding or operation. A reasonably robust, reliable, and consistent LLM should output semantically similar responses for a question asked differently or by differe
    
[^13]: 基于真实用户查询评估文本到SQL系统的数据模型鲁棒性

    Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries

    [https://arxiv.org/abs/2402.08349](https://arxiv.org/abs/2402.08349)

    本文首次深入评估了在实践中文本到SQL系统的数据模型的鲁棒性，通过基于一个多年的国际项目集中评估，对一个在FIFA World Cup背景下连续运行了9个月的真实部署的FootballDB系统进行了评估。

    

    文本到SQL系统（也称为自然语言到SQL系统）已成为弥合用户能力与基于SQL的数据访问之间差距的越来越流行的解决方案。这些系统将用户的自然语言请求转化为特定数据库的有效SQL语句。最近的基于转换器的语言模型使得文本到SQL系统受益匪浅。然而，虽然这些系统在常常是合成基准数据集上不断取得新的高分，但对于它们在真实世界、现实场景中对不同数据模型的鲁棒性的系统性探索明显缺乏。本文基于一个多年国际项目关于文本到SQL界面的集中评估，提供了对文本到SQL系统在实践中数据模型鲁棒性的首次深度评估。我们的评估基于FootballDB的真实部署，该系统在FIFA World Cup的背景下连续运行了9个月。

    Text-to-SQL systems (also known as NL-to-SQL systems) have become an increasingly popular solution for bridging the gap between user capabilities and SQL-based data access. These systems translate user requests in natural language to valid SQL statements for a specific database. Recent Text-to-SQL systems have benefited from the rapid improvement of transformer-based language models. However, while Text-to-SQL systems that incorporate such models continuously reach new high scores on -- often synthetic -- benchmark datasets, a systematic exploration of their robustness towards different data models in a real-world, realistic scenario is notably missing. This paper provides the first in-depth evaluation of the data model robustness of Text-to-SQL systems in practice based on a multi-year international project focused on Text-to-SQL interfaces. Our evaluation is based on a real-world deployment of FootballDB, a system that was deployed over a 9 month period in the context of the FIFA Wor
    
[^14]: LLM能够识别毒性吗？结构化毒性调查框架和基于语义的度量

    Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric

    [https://arxiv.org/abs/2402.06900](https://arxiv.org/abs/2402.06900)

    本研究提出了一种基于大型语言模型（LLMs）的自动度量方法，用于识别生成文本中的毒性。通过分析毒性因素和LLMs的内在毒性属性，该方法在测量毒性方面表现出众，比现有指标提升12个百分点。

    

    在开发遵守社会标准的大型语言模型（LLMs）的过程中，识别生成文本中的毒性存在至关重要。现有的大多数毒性度量依赖于在特定毒性数据集上训练的编码模型。然而，这些编码器容易受到分布外的问题的影响，并且依赖于数据集中所假定的毒性定义。本文介绍了一种基于LLMs的自动鲁棒度量，用于区分模型回应是否具有毒性。我们首先分析了毒性因素，然后研究了LLMs的内在毒性属性，以确定它们作为评估器的适用性。随后，我们对评估数据集上的度量指标LLMs As ToxiciTy Evaluators（LATTE）进行了评估。实证结果表明，在不进行训练过程的情况下，我们的度量在测量毒性方面表现出色，F1得分比现有技术指标提高了12个百分点。我们还展示了上游毒性对度量结果的影响。

    In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an 
    
[^15]: 非自回归的生成模型用于排序推荐

    Non-autoregressive Generative Models for Reranking Recommendation

    [https://arxiv.org/abs/2402.06871](https://arxiv.org/abs/2402.06871)

    本研究提出了一个非自回归的生成模型用于排序推荐，在多阶段推荐系统中扮演关键角色。该模型旨在提高效率和效果，并解决稀疏训练样本和动态候选项对模型收敛性的挑战。

    

    在多阶段推荐系统中，重新排序通过建模项目之间的内部相关性起到了至关重要的作用。重新排序的关键挑战在于在排列的组合空间中探索最佳序列。最近的研究提出了生成器-评估器学习范式，生成器生成多个可行序列，评估器基于估计的列表得分选择最佳序列。生成器至关重要，而生成模型非常适合生成器函数。当前的生成模型采用自回归策略进行序列生成。然而，在实时工业系统中部署自回归模型是具有挑战性的。因此，我们提出了一个非自回归生成模型用于排序推荐（NAR4Rec），以提高效率和效果。为了解决与稀疏训练样本和动态候选项对模型收敛性的挑战，我们引入了一个m

    In a multi-stage recommendation system, reranking plays a crucial role by modeling the intra-list correlations among items.The key challenge of reranking lies in the exploration of optimal sequences within the combinatorial space of permutations. Recent research proposes a generator-evaluator learning paradigm, where the generator generates multiple feasible sequences and the evaluator picks out the best sequence based on the estimated listwise score. Generator is of vital importance, and generative models are well-suited for the generator function. Current generative models employ an autoregressive strategy for sequence generation. However, deploying autoregressive models in real-time industrial systems is challenging. Hence, we propose a Non-AutoRegressive generative model for reranking Recommendation (NAR4Rec) designed to enhance efficiency and effectiveness. To address challenges related to sparse training samples and dynamic candidates impacting model convergence, we introduce a m
    
[^16]: 只需一个颜色空间：一种用于低光图像增强的高效网络

    You Only Need One Color Space: An Efficient Network for Low-light Image Enhancement

    [https://arxiv.org/abs/2402.05809](https://arxiv.org/abs/2402.05809)

    本文提出了一种用于低光图像增强的高效网络，通过引入可训练的水平/垂直强度（HVI）颜色空间来解耦亮度和颜色，并设计了颜色和强度解耦网络（CIDNet）以改善增强过程中的稳定性。结果显示，该方法可以减少增强图像中的颜色和亮度伪影。

    

    低光图像增强（Low-Light Image Enhancement，LLIE）任务旨在从受损的低光图像中恢复细节和视觉信息。大多数现有方法通过深度神经网络（DNNs）在sRGB和HSV颜色空间上学习低/正常光图像之间的映射函数。然而，增强涉及放大图像信号，并且将这些颜色空间应用于信噪比低的低光图像可能会引入灵敏度和不稳定性，从而导致增强图像中存在颜色伪影和亮度伪影。为了缓解这个问题，我们提出了一种新的可训练颜色空间，称为水平/垂直强度（HVI）。它不仅将亮度和颜色从RGB通道分离出来以减轻增强过程中的不稳定性，而且由于可训练参数，它还适应不同光照范围的低光图像。此外，我们设计了一种新颖的颜色和强度解耦网络（CIDNet），含有两个部分。

    Low-Light Image Enhancement (LLIE) task tends to restore the details and visual information from corrupted low-light images. Most existing methods learn the mapping function between low/normal-light images by Deep Neural Networks (DNNs) on sRGB and HSV color space. Nevertheless, enhancement involves amplifying image signals, and applying these color spaces to low-light images with a low signal-to-noise ratio can introduce sensitivity and instability into the enhancement process. Consequently, this results in the presence of color artifacts and brightness artifacts in the enhanced images. To alleviate this problem, we propose a novel trainable color space, named Horizontal/Vertical-Intensity (HVI). It not only decouples brightness and color from RGB channels to mitigate the instability during enhancement but also adapts to low-light images in different illumination ranges due to the trainable parameters. Further, we design a novel Color and Intensity Decoupling Network (CIDNet) with two
    
[^17]: 多发信者说服 - 从计算的角度来看

    Multi-Sender Persuasion -- A Computational Perspective

    [https://arxiv.org/abs/2402.04971](https://arxiv.org/abs/2402.04971)

    这项研究考虑了多个具有信息优势的发信者向单个自私行为者传递信号以影响其行为的问题，并提出了一种新颖的可微神经网络方法来近似解决这一问题。通过额外梯度算法，我们发现了超越已有方法的局部均衡解。

    

    我们考虑到具有信息优势的多个发信者向单个自私行为者传递信号以使其采取某些行动。这些设置是计算经济学，多智能体学习和具有多个目标的机器学习中普遍存在的。核心解决方案概念是发信者信号策略的纳什均衡。理论上，我们证明一般情况下找到一个均衡是PPAD-Hard的;实际上，计算一个发信者的最佳响应甚至是NP-Hard的。鉴于这些固有的困难，我们转而寻找局部纳什均衡。我们提出了一种新颖的可微神经网络来近似该游戏的非线性和不连续效用。结合额外梯度算法，我们发现了超越完全展示均衡和现有神经网络发现的局部均衡。广义上，我们的理论和实证贡献对广泛的类别感兴趣。

    We consider multiple senders with informational advantage signaling to convince a single self-interested actor towards certain actions. Generalizing the seminal Bayesian Persuasion framework, such settings are ubiquitous in computational economics, multi-agent learning, and machine learning with multiple objectives. The core solution concept here is the Nash equilibrium of senders' signaling policies. Theoretically, we prove that finding an equilibrium in general is PPAD-Hard; in fact, even computing a sender's best response is NP-Hard. Given these intrinsic difficulties, we turn to finding local Nash equilibria. We propose a novel differentiable neural network to approximate this game's non-linear and discontinuous utilities. Complementing this with the extra-gradient algorithm, we discover local equilibria that Pareto dominates full-revelation equilibria and those found by existing neural networks. Broadly, our theoretical and empirical contributions are of interest to a large class 
    
[^18]: ChatGPT生成R程序代码的用户中心评估

    User-Centric Evaluation of ChatGPT Capability of Generating R Program Code

    [https://arxiv.org/abs/2402.03130](https://arxiv.org/abs/2402.03130)

    本文评估了ChatGPT从自然语言输入生成R编程语言代码的能力，通过多次尝试的过程，并对生成的解决方案进行了精确性、完整性、简洁性等多个质量属性的评估。

    

    本文报告了对ChatGPT从自然语言输入生成R编程语言代码能力的评估。构建了一个专门用于生成R程序代码的数据集，以支持基于场景的测试和评估在不同难度级别和不同类型程序的各种使用情境中的代码生成能力。评估采用多次尝试的过程，测试人员通过多次尝试来完成代码生成任务，直至获得满意的解决方案或达到最大尝试次数后放弃。每次尝试中，测试人员根据先前的结果和待完成的任务制定自然语言输入给ChatGPT。除了平均尝试次数和平均完成任务所需的时间指标外，最终生成的解决方案将根据精确性、完整性、简洁性等多个质量属性进行评估。

    This paper reports an evaluation of ChatGPT's capability of generating R programming language code from natural language input. A dataset specially designed for generating R program code was constructed with metadata to support scenario-based testing and evaluation of code generation capabilities in various usage scenarios of different levels of difficulty and different types of programs. The evaluation takes a multiple attempt process in which the tester tries to complete the code generation task through a number of attempts until a satisfactory solution is obtained or gives up after a fixed number of maximal attempts. In each attempt the tester formulates a natural language input to ChatGPT based on the previous results and the task to be completed. In addition to the metrics of average numbers of attempts and average amount of time taken to complete the tasks, the final generated solutions are then assessed on a number of quality attributes, including accuracy, completeness, concise
    
[^19]: 为社交感知的谈判对话开发辅助大型语言模型代理

    Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues

    [https://arxiv.org/abs/2402.01737](https://arxiv.org/abs/2402.01737)

    本文旨在开发LLM代理以减轻多代理设置下谈判中的社交规范违反。我们引入了基于价值影响的环境学习（ICL）方法，用于为基于LLM的修正代理识别高质量的ICL示例，其中价值影响函数衡量谈判结果的质量。我们展示了这种方法与策略学习的联系，并提供了丰富的实证证据来证明其在三个不同主题的谈判中的有效性。

    

    本文旨在开发LLM代理以减轻多代理设置下谈判中的社交规范违反。我们通过让两个大型语言模型（LLM）扮演每次对话中的两名谈判者来模拟现实世界谈判。第三个LLM充当修正代理，重新编写违反规范的话语以改善谈判结果。由于这是一个新颖的任务，不存在手动构建的数据。为解决这个限制，我们引入了基于价值影响的环境学习（ICL）方法，用于为基于LLM的修正代理识别高质量的ICL示例，其中价值影响函数衡量谈判结果的质量。我们展示了这种方法与策略学习的联系，并提供了丰富的实证证据来证明其在三个不同主题的谈判中的有效性，即产品销售、房价和薪资谈判。源代码和生成的数据集将在接受后公开。

    In this work, we aim to develop LLM agents to mitigate social norm violations in negotiations in a multi-agent setting. We simulate real-world negotiations by letting two large Language Models (LLMs) play the roles of two negotiators in each conversation. A third LLM acts as a remediation agent to rewrite utterances violating norms for improving negotiation outcomes. As it is a novel task, no manually constructed data is available. To address this limitation, we introduce a value impact based In-Context Learning (ICL) method to identify high-quality ICL examples for the LLM-based remediation agents, where the value impact function measures the quality of negotiation outcomes. We show the connection of this method to policy learning and provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different topics: product sale, housing price, and salary negotiation. The source code and the generated dataset will be publicly available upon acceptance.
    
[^20]: 正式-LLM：将形式语言和自然语言集成于可控的LLM智能体中

    Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents

    [https://arxiv.org/abs/2402.00798](https://arxiv.org/abs/2402.00798)

    本文提出了一种将自然语言和形式语言整合的“正式-LLM”框架，用于解决现有LLM智能体无法控制的计划生成问题。实验证明，该框架在提高生成计划性能和确保可控性方面取得了显著改进。

    

    最近，对于大型语言模型（LLMs）的进展使得人工智能智能体能够自动生成和执行解决复杂任务的多步计划。然而，由于LLM的内容生成过程几乎无法控制，当前的LLM智能体经常生成无效或不可执行的计划，这损害了生成计划的性能并破坏了用户对LLM智能体的信任。为应对这个问题，本文提出了一种新颖的“正式-LLM”框架，用于LLM智能体，通过将自然语言的表达力和形式语言的精确性进行整合。具体而言，该框架允许人类用户将他们对计划过程的要求或约束表达为自动机。然后，在自动机的监督下，使用基于堆栈的LLM计划生成过程来确保生成的计划满足约束条件，从而使计划过程可控。我们在基准任务和实际的真实任务上进行了实验，并且obtained significant improvements over existing LLM-based agents, demonstrating the effectiveness and controllability of the proposed Formal-LLM framework.

    Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and o
    
[^21]: Gemini：一系列高性能多模态模型

    Gemini: A Family of Highly Capable Multimodal Models

    [https://arxiv.org/abs/2312.11805](https://arxiv.org/abs/2312.11805)

    Gemini家族是一系列在图像、音频、视频和文本理解方面表现出色的多模态模型，其中最具能力的Gemini Ultra模型在30个基准测试中推进了技术前沿，并改进了所有20个多模态基准测试的技术状态。

    

    本报告介绍了一种新的多模态模型系列Gemini，展示出在图像、音频、视频和文本理解方面的显著能力。Gemini系列包括Ultra、Pro和Nano尺寸，适用于从复杂推理任务到设备内存受限应用的各种应用场景。在广泛的基准测试中，我们最具能力的Gemini Ultra模型在32个基准测试中的30个中推进了技术前沿 - 显著地是第一个在被广泛研究的考试基准测试MMLU上实现人类专家水平表现的模型，并在我们研究的每一个20个多模态基准测试中改进了技术前沿。我们相信Gemini系列在跨模态推理和语言理解方面的新能力将能够支持各种用例。我们讨论了负责任地向用户提供Gemini模型的训练后和部署方法，包括使用服务。

    arXiv:2312.11805v2 Announce Type: replace-cross  Abstract: This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services includi
    
[^22]: 揭示任务fMRI分析空间中的流程社群

    Uncovering communities of pipelines in the task-fMRI analytical space

    [https://arxiv.org/abs/2312.06231](https://arxiv.org/abs/2312.06231)

    本论文通过使用社群检测算法揭示了任务fMRI分析空间中的流程社群，并评估了不同背景下的流程关系的稳定性。研究表明，存在一些子集的流程给出相似的结果，特别是那些分享特定参数。这些结果对于参与者群体来说是稳定的，但在不同任务之间不稳定。流程空间的形成主要受到大脑激活区域大小和统计值规模的影响。

    

    功能磁共振成像中的分析工作流程具有高度灵活性，选择流程的最佳实践有限。尽管已经显示出使用不同流程可能导致不同的结果，但对于驱动这些差异的因素以及这些差异在不同背景下的稳定性仍然缺乏理解。我们使用社群检测算法探索流程空间，并评估不同背景下流程关系的稳定性。我们发现，存在一些子集的流程给出相似的结果，特别是那些分享特定参数（例如运动回归器的数量、软件包等）。这些流程与流程之间的模式在参与者群体中是稳定的，但在不同任务之间不稳定。通过可视化社群间的差异，我们发现流程空间主要受大脑激活区域的大小和统计值的规模的影响。

    Analytical workflows in functional magnetic resonance imaging are highly flexible with limited best practices as to how to choose a pipeline. While it has been shown that the use of different pipelines might lead to different results, there is still a lack of understanding of the factors that drive these differences and of the stability of these differences across contexts. We use community detection algorithms to explore the pipeline space and assess the stability of pipeline relationships across different contexts. We show that there are subsets of pipelines that give similar results, especially those sharing specific parameters (e.g. number of motion regressors, software packages, etc.). Those pipeline-to-pipeline patterns are stable across groups of participants but not across different tasks. By visualizing the differences between communities, we show that the pipeline space is mainly driven by the size of the activation area in the brain and the scale of statistic values in stati
    
[^23]: ERASER: 通过推理服务器感知的方法，在MLaaS中实现机器学习的去学习

    ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware Approach

    [https://arxiv.org/abs/2311.16136](https://arxiv.org/abs/2311.16136)

    本文提出了ERASER，一种通过推理服务器感知的方法，在MLaaS中实现机器学习的去学习。

    

    过去几年，机器学习作为一种服务（MLaaS）在支持基于机器学习的服务方面有着大量需求，以在各种应用领域提供革命性的用户体验。MLaaS基于从众多个体数据所有者收集的训练数据集训练的机器学习模型提供低推理延迟的推理服务。最近，为了保护数据所有者的隐私并遵守数据保护法的“被遗忘权（RTBF）”，许多机器去学习方法被提出，以在请求去学习时从训练模型中删除数据所有者的数据。然而，尽管它们具有很高的效率，几乎所有现有的机器去学习方法都独立处理去学习请求和推理请求，这不幸地引入了推理服务过时性的安全问题和机器去学习中极易遭受不必要的曝光的隐私漏洞。

    Over the past years, Machine Learning-as-a-Service (MLaaS) has received a surging demand for supporting Machine Learning-driven services to offer revolutionized user experience across diverse application areas. MLaaS provides inference service with low inference latency based on an ML model trained using a dataset collected from numerous individual data owners. Recently, for the sake of data owners' privacy and to comply with the "right to be forgotten (RTBF)" as enacted by data protection legislation, many machine unlearning methods have been proposed to remove data owners' data from trained models upon their unlearning requests. However, despite their promising efficiency, almost all existing machine unlearning methods handle unlearning requests independently from inference requests, which unfortunately introduces a new security issue of inference service obsolescence and a privacy vulnerability of undesirable exposure for machine unlearning in MLaaS.   In this paper, we propose the 
    
[^24]: 事件序列的自我监督学习：生成建模和对比学习的比较研究和混合方法的应用

    Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning. (arXiv:2401.15935v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.15935](http://arxiv.org/abs/2401.15935)

    本研究通过比较研究和混合方法，调查了事件序列的自我监督学习技术，并引入了一种新的方法，将生成模型和对比嵌入进行对齐。结果显示，这种对齐模型在各种任务上表现优越，为预测事件序列中的信息提供了潜在的好处。

    

    本研究调查了获取事件序列表示的自我监督学习技术。这是各种应用中的关键模态，包括但不限于银行、电子商务和医疗保健。我们对自我监督学习中的生成模型和对比方法进行了全面的研究，并分别应用了它们。我们发现没有一种绝对优越的方法。因此，我们探讨了结合这些方法的潜在好处。为了实现这个目标，我们引入了一种新的方法，将生成模型和对比嵌入作为不同的模态进行对齐，从当代多模态研究中汲取灵感。生成模型和对比方法通常被视为互斥的，因此存在它们的联合探索的空白。我们的结果表明，这种对齐模型在至少与现有方法持平，并且在各种任务上更加普适。此外，我们证明了自我监督学习在预测事件序列中包含的信息方面的潜力。

    This study investigates self-supervised learning techniques to obtain representations of Event Sequences. It is a key modality in various applications, including but not limited to banking, e-commerce, and healthcare.  We perform a comprehensive study of generative and contrastive approaches in self-supervised learning, applying them both independently. We find that there is no single supreme method. Consequently, we explore the potential benefits of combining these approaches. To achieve this goal, we introduce a novel method that aligns generative and contrastive embeddings as distinct modalities, drawing inspiration from contemporary multimodal research.  Generative and contrastive approaches are often treated as mutually exclusive, leaving a gap for their combined exploration. Our results demonstrate that this aligned model performs at least on par with, and mostly surpasses, existing methods and is more universal across a variety of tasks. Furthermore, we demonstrate that self-sup
    
[^25]: 通过检索和自我反思改善医疗推理能力的检索增强型大型语言模型

    Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models. (arXiv:2401.15269v1 [cs.CL])

    [http://arxiv.org/abs/2401.15269](http://arxiv.org/abs/2401.15269)

    本论文介绍了一种名为Self-BioRAG的框架，通过使用检索和自我反思的方法，提高了医疗推理的能力。该框架专注于生成解释、检索领域特定文档以及对生成的响应进行自我反思。

    

    最近的专有大型语言模型（LLMs），例如GPT-4，在生物医学领域中解决了从多项选择题到长篇生成等多样化挑战的里程碑。为了解决LLMs编码知识无法处理的挑战，已经开发了各种检索增强生成（RAG）方法，通过从知识语料库中搜索文档并无条件或有选择地将其附加到LLMs的输入来进行生成。然而，将现有方法应用于不同领域特定问题时，出现了泛化能力差的问题，导致获取不正确的文档或做出不准确的判断。在本文中，我们介绍了一种可靠的医学文本框架Self-BioRAG，专门用于生成解释、检索领域特定文档和自我反思生成的响应。我们使用了84k个经过过滤的生物医学指令集来训练Self-BioRAG，它具备评估自己的基因

    Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its gene
    
[^26]: 朝着目标导向的大型语言模型提示方法：一项调查

    Towards Goal-oriented Large Language Model Prompting: A Survey. (arXiv:2401.14043v1 [cs.CL])

    [http://arxiv.org/abs/2401.14043](http://arxiv.org/abs/2401.14043)

    本文调查了大型语言模型(LLM)中目标导向提示工程的重要性。通过对35个代表性研究的回顾，我们发现引导LLM遵循人类的逻辑思维的目标导向提示公式显著提高了LLM的性能。我们还提出了一个新的分类体系，并总结了十个适用任务来展示我们框架的广泛适用性。同时，我们提出了四个未来的方向，以推动目标导向提示工程的进一步发展。

    

    大型语言模型(LLM)在各种下游任务中显示出卓越的性能，而提示工程在优化LLM性能中起着关键作用。本文旨在强调设计提示的限制，同时保持人类追求LLM像人类思考的人类学假设。通过对35个代表性研究的回顾，我们展示了目标导向提示公式的重要性，该公式指导LLM遵循人类的逻辑思维，显著提高了LLM的性能。此外，我们引入了一个新的分类体系，将目标导向提示方法分为五个相互关联的阶段，并通过总结十个适用任务来展示我们框架的广泛适用性。最后，我们提出了四个未来的方向，希望进一步强调和推动目标导向提示工程。

    Large Language Models (LLMs) have shown prominent performance in various downstream tasks in which prompt engineering plays a pivotal role in optimizing LLMs' performance. This paper, not as an overview of current prompt engineering methods, aims to highlight the limitation of designing prompts while holding an anthropomorphic assumption that expects LLMs to think like humans. From our review of 35 representative studies, we demonstrate that a goal-oriented prompt formulation, which guides LLMs to follow established human logical thinking, significantly improves the performance of LLMs. Furthermore, We introduce a novel taxonomy that categorizes goal-oriented prompting methods into five interconnected stages and we demonstrate the broad applicability of our framework by summarizing ten applicable tasks. With four future directions proposed, we hope to further emphasize and promote goal-oriented prompt engineering.
    
[^27]: LLM指令微调中的提示权重实验

    Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])

    [http://arxiv.org/abs/2401.13586](http://arxiv.org/abs/2401.13586)

    LLM指令微调中，对于短提示完成数据集，提示词标记分类损失加权（PLW）与性能呈负二次关系，而长提示完成数据集则不受PLW影响。

    

    我们进行了一项小型研究，分析了提示词标记分类损失加权（PLW）如何影响在指令任务上进行微调的7B大小的LLaMA模型的性能。我们使用多个指令数据集重现了斯坦福大学的Alpaca实验，其中包括LLaMA 1和LLaMA 2。我们发现，在我们的短提示完成数据集上微调的模型与PLW之间存在负二次关系，而在长提示完成数据集上微调的模型不受PLW的影响。

    We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
    
[^28]: 半监督耦合薄板样条模型用于旋转校正及更多应用

    Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction and Beyond. (arXiv:2401.13432v1 [cs.CV])

    [http://arxiv.org/abs/2401.13432](http://arxiv.org/abs/2401.13432)

    本文提出了一种半监督耦合薄板样条模型，用于处理旋转校正等图像变形任务。通过耦合多个薄板样条变换，有效消除了插值误差，突破了控制点数量瓶颈，并且通过半监督学习方法，可以在有限的带标注样本条件下，充分利用未标注样本来提高模型性能。

    

    薄板样条（TPS）是一种允许使用控制点运动表示弹性非线性变换的主要变形方式。随着控制点数量的增加，变形越来越灵活，但常常会遇到由不希望的问题（如内容畸变）导致的瓶颈。本文探索了TPS在基于单幅图像的变形任务中的通用应用，如旋转校正、矩形化和肖像校正。为了突破这个瓶颈，我们提出了耦合薄板样条模型（CoupledTPS），它将有限控制点的多个TPS迭代耦合成一个更灵活和强大的变换。具体来说，我们首先设计了一个迭代搜索来根据当前潜在条件预测新的控制点。然后，我们以变形流作为连接不同TPS变换的桥梁，有效消除了多个变形引起的插值误差。此外，鉴于繁琐的注释成为制约深度学习方法效果的重要因素，我们还提出了一种半监督学习方法，使得在有限带标注样本条件下，能充分利用未标注样本来提高模型性能。

    Thin-plate spline (TPS) is a principal warp that allows for representing elastic, nonlinear transformation with control point motions. With the increase of control points, the warp becomes increasingly flexible but usually encounters a bottleneck caused by undesired issues, e.g., content distortion. In this paper, we explore generic applications of TPS in single-image-based warping tasks, such as rotation correction, rectangling, and portrait correction. To break this bottleneck, we propose the coupled thin-plate spline model (CoupledTPS), which iteratively couples multiple TPS with limited control points into a more flexible and powerful transformation. Concretely, we first design an iterative search to predict new control points according to the current latent condition. Then, we present the warping flow as a bridge for the coupling of different TPS transformations, effectively eliminating interpolation errors caused by multiple warps. Besides, in light of the laborious annotation co
    
[^29]: BEV-CLIP：用于自动驾驶中复杂场景的多模态BEV检索方法论

    BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving. (arXiv:2401.01065v1 [cs.CV])

    [http://arxiv.org/abs/2401.01065](http://arxiv.org/abs/2401.01065)

    BEV-CLIP是一种用于自动驾驶中复杂场景的多模态BEV检索方法，通过使用描述性文本进行检索，利用大型语言模型的 semantic feature extraction 和知识图谱的半结构化信息来提高检索准确性。

    

    随着乘用车辆具备在城市环境中导航的能力，自动驾驶中对复杂场景数据的检索需求不断增加，尤其是在处理长尾情景时。然而，在现有的二维图像检索方法下，存在一些场景检索的问题，例如全局特征表征不足和文本检索能力不佳。为了解决这些问题，我们提出了BEV-CLIP，这是第一个利用描述性文本作为输入来检索相应场景的多模态鸟瞰图检索方法。该方法利用大型语言模型 (LLM) 的语义特征提取能力，实现了对广泛文本描述的零样本检索，并结合知识图谱的半结构化信息，提高了语言嵌入的语义丰富性和多样性。实验结果表明，在NuScenes数据集上取得了87.66%的准确率。

    The demand for the retrieval of complex scene data in autonomous driving is increasing, especially as passenger vehicles have been equipped with the ability to navigate urban settings, with the imperative to address long-tail scenarios. Meanwhile, under the pre-existing two dimensional image retrieval method, some problems may arise with scene retrieval, such as lack of global feature representation and subpar text retrieval ability. To address these issues, we have proposed \textbf{BEV-CLIP}, the first multimodal Bird's-Eye View(BEV) retrieval methodology that utilizes descriptive text as an input to retrieve corresponding scenes. This methodology applies the semantic feature extraction abilities of a large language model (LLM) to facilitate zero-shot retrieval of extensive text descriptions, and incorporates semi-structured information from a knowledge graph to improve the semantic richness and variety of the language embedding. Our experiments result in 87.66% accuracy on NuScenes d
    
[^30]: 激发大型语言模型中的提示工程潜力：一项综述

    Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. (arXiv:2310.14735v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.14735](http://arxiv.org/abs/2310.14735)

    这篇论文解释了提示工程在释放大型语言模型能力方面的关键作用，探讨了不同的提示方法以及外部插件如何协助减少机器幻想，并指出了未来研究方向的重要性。

    

    本文深入探讨了提示工程在释放大型语言模型（LLM）能力方面的关键作用。提示工程是为LLM构建输入文本的过程，是优化LLM有效性的重要技术。本综述阐明了提示工程的基本原理，如角色提示、一次性提示和少量提示，以及更高级的方法，如思维链和思维树提示。本文还阐述了外部插件如何协助此任务，并通过检索外部知识来减少机器幻想。随后，我们勾勒了提示工程研究的前景方向，强调了对结构和代理在人工智能生成内容（AIGC）工具中的作用的深入理解的必要性。我们讨论了如何从不同角度和使用不同的方法评估提示方法的有效性。最后，我们提出了展望未来的研究方向。

    This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we
    
[^31]: 语言模型作为零-shot轨迹生成器

    Language Models as Zero-Shot Trajectory Generators. (arXiv:2310.11604v1 [cs.RO])

    [http://arxiv.org/abs/2310.11604](http://arxiv.org/abs/2310.11604)

    本文研究了使用大型语言模型（LLMs）作为零-shot轨迹生成器的可能性。通过给予LLM物体检测和分割视觉模型的访问权限，研究人员发现LLMs能够直接预测操作技能中的末端执行器姿态序列，并在26个真实世界的语言任务中取得了良好效果。这一研究突破了对LLMs在机器人技术中的限制，揭示了LLMs确实具有对操作任务的理解能力。

    

    近期研究表明，大型语言模型（LLMs）在给予低级技能选择时能够作为机器人的高级规划器。然而，通常认为LLMs不具备足够的知识来用于低级轨迹生成。在本研究中，我们详细探讨了这种假设，并调查了当给予LLM（GPT-4）仅能访问物体检测和分割视觉模型时，它能否直接预测一系列密集的末端执行器姿态用于操作技能。我们研究了一个单一的任务不可知提示，没有任何上下文示例、运动原语或外部轨迹优化器，它在26个真实世界的基于语言的任务中的表现，如“打开瓶盖”和“用海绵擦拭盘子”，以及我们调查了这个提示中哪些设计选择最有效。我们的结论突破了对LLMs在机器人技术上的限制，并首次揭示了LLMs确实具有对操作任务的理解能力。

    Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as "open the bottle cap" and "wipe the plate with the sponge", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding o
    
[^32]: 从语言模型中识别和减轻隐私风险：一项调查

    Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey. (arXiv:2310.01424v1 [cs.CL])

    [http://arxiv.org/abs/2310.01424](http://arxiv.org/abs/2310.01424)

    这项调查对语言模型（LMs）中的隐私风险进行了研究和减轻措施的探讨，通过分类法和调查现有攻击，提出了关键趋势，并讨论现有的减轻策略的优势和局限性，指出关键差距和未来工作方向。

    

    语言模型（LMs）的快速发展使其被广泛采用于许多领域。除了潜在的好处外，这些模型还带来了一系列风险，包括隐私风险。尤其是随着LMs规模的增加，它们对训练数据的记忆潜力增加，从而导致泄露私人信息的风险。随着LMs的日益普及，我们必须了解这些隐私风险以及如何减轻它们。为了帮助研究人员和决策者了解LM隐私攻击和减轻措施的知识状况，包括需要更多工作的领域，我们介绍了第一份关于LM隐私的技术调查。我们（i）确定了攻击在LMs上存在的显著维度的分类法，（ii）调查现有攻击并使用我们的分类法来突出主要趋势，（iii）讨论现有的减轻策略，突出其优势和局限性，识别关键差距，展示开放问题和建议未来工作方向。

    Rapid advancements in language models (LMs) have led to their adoption across many sectors. Alongside the potential benefits, such models present a range of risks, including around privacy. In particular, as LMs have grown in size, the potential to memorise aspects of their training data has increased, resulting in the risk of leaking private information. As LMs become increasingly widespread, it is vital that we understand such privacy risks and how they might be mitigated. To help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first technical survey on LM privacy. We (i) identify a taxonomy of salient dimensions where attacks differ on LMs, (ii) survey existing attacks and use our taxonomy of dimensions to highlight key trends, (iii) discuss existing mitigation strategies, highlighting their strengths and limitations, identifying key gaps and demonstrating open problems and are
    
[^33]: 生成艾舍尔网格

    Generative Escher Meshes. (arXiv:2309.14564v1 [cs.CV])

    [http://arxiv.org/abs/2309.14564](http://arxiv.org/abs/2309.14564)

    本文提出了一种全自动的生成方法，用于生成周期性的非正方形镶嵌图案，该方法通过优化几何和颜色来生成与所需对象形状和外观相似的瓷砖。

    

    本文提出了一种全自动、以文本为导向的生成方法，用于生成周期性的、可重复的二维艺术作品，如地板、马赛克、陶瓷和艾舍尔的作品。与传统的无缝纹理概念不同，即平铺无缝的正方形图像，我们的方法生成的是由重复的相同对象组成的非正方形镶嵌图案。它通过优化二维网格的几何和颜色来生成与所需对象形状和外观相似的非正方形瓷砖，几乎没有额外的背景细节。我们通过一个关键的技术贡献实现了镶嵌图案的几何优化：一个无约束的、可微分的参数化方法，用于给定对称群的所有可能的可铺砖形状空间。换句话说，我们证明了修改二维网格映射技术Orbifold Tutte Embedding中使用的Laplacian算子可以实现所选平面对称群的所有可能的铺砖配置。

    This paper proposes a fully-automatic, text-guided generative method for producing periodic, repeating, tile-able 2D art, such as the one seen on floors, mosaics, ceramics, and the work of M.C. Escher. In contrast to the standard concept of a seamless texture, i.e., square images that are seamless when tiled, our method generates non-square tilings which comprise solely of repeating copies of the same object. It achieves this by optimizing both geometry and color of a 2D mesh, in order to generate a non-square tile in the shape and appearance of the desired object, with close to no additional background details. We enable geometric optimization of tilings by our key technical contribution: an unconstrained, differentiable parameterization of the space of all possible tileable shapes for a given symmetry group. Namely, we prove that modifying the laplacian used in a 2D mesh-mapping technique Orbifold Tutte Embedding - can achieve all possible tiling configurations for a chosen planar 
    
[^34]: 多模态预训练模型用于序列决策：综合、验证、基础和感知

    Multimodal Pretrained Models for Sequential Decision-Making: Synthesis, Verification, Grounding, and Perception. (arXiv:2308.05295v1 [cs.AI])

    [http://arxiv.org/abs/2308.05295](http://arxiv.org/abs/2308.05295)

    该论文介绍了一种利用预训练模型的知识来解决序列决策任务的算法，通过构建和验证控制器，并通过视觉观测来与任务环境相连接。

    

    最近开发的预训练模型可以编码以多种形式表达的丰富世界知识，例如文本和图像。然而，这些模型的输出不能集成到解决序列决策任务的算法中。我们开发了一种算法，利用预训练模型的知识来构建和验证序列决策任务的控制器，并通过视觉观测将这些控制器与任务环境相连接。具体而言，该算法通过用户提供的基于文本的任务描述查询预训练模型，并使用模型的输出构建基于自动机的控制器，以编码模型的任务相关知识。然后，它验证控制器中编码的知识是否与其他独立可用的知识一致，这些知识可能包括环境的抽象信息或用户提供的规范。如果验证步骤发现任何不一致性，算法会自动纠正并重新验证，直到所有知识一致。

    Recently developed pretrained models can encode rich world knowledge expressed in multiple modalities, such as text and images. However, the outputs of these models cannot be integrated into algorithms to solve sequential decision-making tasks. We develop an algorithm that utilizes the knowledge from pretrained models to construct and verify controllers for sequential decision-making tasks, and to ground these controllers to task environments through visual observations. In particular, the algorithm queries a pretrained model with a user-provided, text-based task description and uses the model's output to construct an automaton-based controller that encodes the model's task-relevant knowledge. It then verifies whether the knowledge encoded in the controller is consistent with other independently available knowledge, which may include abstract information on the environment or user-provided specifications. If this verification step discovers any inconsistency, the algorithm automaticall
    
[^35]: 通过关键阶段促进记忆增强型Adam的探索

    Promoting Exploration in Memory-Augmented Adam using Critical Momenta. (arXiv:2307.09638v1 [cs.LG])

    [http://arxiv.org/abs/2307.09638](http://arxiv.org/abs/2307.09638)

    本研究提出了一种记忆增强型Adam方法，通过使用关键动量项的缓冲区来促进对更平坦最小值的探索。实验证明，该方法在标准的监督语言建模和图像分类任务中提高了几种Adam变体的性能。

    

    自适应梯度优化器，特别是Adam，在训练大规模深度学习模型中发挥了重要作用。这种优化器的优势在于其快速收敛性，同时对超参数的选择更加鲁棒。然而，它们通常比非自适应方法泛化效果更差。最近的研究将这种性能差距归因于选择平坦最小值：自适应方法倾向于在损失函数曲面中更尖锐的盆地中寻找解决方案，从而损害了泛化能力。为了克服这个问题，我们提出了一种新的记忆增强型Adam方法，在训练过程中使用关键动量项的缓冲区来促进对更平坦最小值的探索。直观地说，缓冲区的使用使得优化器如果盆地的吸引范围不够宽，就会超出其范围。我们经验性地证明了我们的方法在标准的监督语言建模和图像分类任务上提高了几种Adam变体的性能。

    Adaptive gradient-based optimizers, particularly Adam, have left their mark in training large-scale deep learning models. The strength of such optimizers is that they exhibit fast convergence while being more robust to hyperparameter choice. However, they often generalize worse than non-adaptive methods. Recent studies have tied this performance gap to flat minima selection: adaptive methods tend to find solutions in sharper basins of the loss landscape, which in turn hurts generalization. To overcome this issue, we propose a new memory-augmented version of Adam that promotes exploration towards flatter minima by using a buffer of critical momentum terms during training. Intuitively, the use of the buffer makes the optimizer overshoot outside the basin of attraction if it is not wide enough. We empirically show that our method improves the performance of several variants of Adam on standard supervised language modelling and image classification tasks.
    
[^36]: 医院中的综合规划：一项综述

    Integrated Planning in Hospitals: A Review. (arXiv:2307.05258v1 [cs.AI])

    [http://arxiv.org/abs/2307.05258](http://arxiv.org/abs/2307.05258)

    本文综述了医院综合规划的运筹学和管理科学文献，强调了综合规划多个资源的潜力，并提供了关于不确定性建模和使用现实数据等方面的分析。

    

    1950年以来，高效规划医院稀缺资源是一项具有挑战性的任务，为此，已经开发了大量的运筹学和管理科学方法。尽管高效规划单一资源如手术室、床位或特定类型的医护人员已经能够带来巨大的效益，但综合规划多个资源被证明具有更大的潜力，在过去几十年的文献中已经提出了大量的综合规划方法。本文首次提供了关于医院不同资源综合规划的运筹学和管理科学文献的综述。我们收集了相关文献，并对不同方面进行分析，如不确定性建模和使用现实数据。几个交叉比较揭示了一些有趣的洞察，例如建模和求解之间的关系。

    Efficient planning of scarce resources in hospitals is a challenging task for which a large variety of Operations Research and Management Science approaches have been developed since the 1950s. While efficient planning of single resources such as operating rooms, beds, or specific types of staff can already lead to enormous efficiency gains, integrated planning of several resources has been shown to hold even greater potential, and a large number of integrated planning approaches have been presented in the literature over the past decades.  This paper provides the first literature review that focuses specifically on the Operations Research and Management Science literature related to integrated planning of different resources in hospitals. We collect the relevant literature and analyze it regarding different aspects such as uncertainty modeling and the use of real-life data. Several cross comparisons reveal interesting insights concerning, e.g., relations between the modeling and solut
    
[^37]: SwinGNN:重新思考在图生成的扩散模型中的置换不变性

    SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])

    [http://arxiv.org/abs/2307.01646](http://arxiv.org/abs/2307.01646)

    本文提出了一种新的图生成扩散模型SwinGNN，通过使用高效的2-WL消息传递网络和移动窗口自注意力，以及结合关键的训练和采样技术，显著提高了图生成样本的质量，并引入了随机置换的后处理技巧转换生成的图形统计量。

    

    基于置换等变网络的扩散模型可以学习图数据的置换不变分布。然而，相对于非不变模型，我们发现这些不变模型遇到了更大的学习挑战，因为1）它们的目标分布更具模态性；2）它们的最优一步去噪得分是具有更多成分的高斯混合物的得分函数。受到这个分析的启发，我们提出了一种非不变的扩散模型，称为“SwinGNN”，它采用了一种高效的边到边的2-WL消息传递网络，并利用SwinTransformers中的移动窗口自注意力。此外，通过系统性的实验和剖析，我们确定了几种关键的训练和采样技术，显著提高了图生成样本的质量。最后，我们引入了一种简单的后处理技巧，即随机置换生成的图，可以证明将任何图转换成图形统计量。

    Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph ge
    
[^38]: 零样本识别中的视觉-语言模型的挑战：粒度和正确性

    Challenges of Zero-Shot Recognition with Vision-Language Models: Granularity and Correctness. (arXiv:2306.16048v1 [cs.CV])

    [http://arxiv.org/abs/2306.16048](http://arxiv.org/abs/2306.16048)

    本文研究了将视觉-语言模型应用于零样本视觉识别任务所面临的挑战，发现VLMs在识别细粒度概念方面表现更好，并指出了VLMs中相似度分数不能严格反映正确性的问题，提出了未来研究方向。

    

    本文研究了将视觉-语言模型（VLMs）应用于开放世界环境下的零样本视觉识别任务所面临的挑战，重点关注对比视觉-语言模型（如CLIP）的应用。我们首先检查了VLMs在不同粒度概念上的表现。我们提出了一种公正评估两种实验设置下性能差异的方法，并发现VLMs在识别细粒度概念方面表现更好。此外，我们发现VLMs产生的相似度分数并不能严格反映文本输入在视觉输入下的正确性。我们提出了一种评估协议来测试我们的假设，即分数可能会偏向更具信息的描述，并且由于嵌入之间的相似度分数的性质，对于VLMs来说识别相似但错误的描述之间的正确性是具有挑战性的。我们的研究强调了在开放世界环境中使用VLMs的挑战，并提出了未来研究的方向。

    This paper investigates the challenges of applying vision-language models (VLMs) to zero-shot visual recognition tasks in an open-world setting, with a focus on contrastive vision-language models such as CLIP. We first examine the performance of VLMs on concepts of different granularity levels. We propose a way to fairly evaluate the performance discrepancy under two experimental setups and find that VLMs are better at recognizing fine-grained concepts. Furthermore, we find that the similarity scores from VLMs do not strictly reflect the correctness of the textual inputs given visual input. We propose an evaluation protocol to test our hypothesis that the scores can be biased towards more informative descriptions, and the nature of the similarity score between embedding makes it challenging for VLMs to recognize the correctness between similar but wrong descriptions. Our study highlights the challenges of using VLMs in open-world settings and suggests directions for future research to 
    
[^39]: 关于伪造纳什均衡的研究

    On Faking a Nash Equilibrium. (arXiv:2306.08041v1 [cs.MA])

    [http://arxiv.org/abs/2306.08041](http://arxiv.org/abs/2306.08041)

    本文研究多智能体强化学习中的数据污染攻击，提出了唯一纳什集的概念，并设计了一个线性规划方案来计算最优污染攻击策略。

    

    本文研究了多智能体强化学习中的数据污染攻击，攻击者试图更改数据集以安装（潜在虚假的）唯一马尔可夫完美纳什均衡点(Nash equilibrium)。我们提出了唯一纳什集的概念，即由其Q函数规定的游戏的集合，其具有唯一的联合策略作为唯一的纳什均衡点。唯一纳什集对于污染攻击非常重要，因为只有当数据污染使所有合理的游戏都在其中时，攻击才成功。唯一纳什集将常用于逆强化学习中的奖励多面体推广到多智能体强化学习中。对于零和马尔科夫博弈，逆纳什集以及由数据引起的合理游戏集都是Q函数空间中的多面体。我们提出了一个线性规划方案以有效地计算最优的污染攻击策略。我们的工作为设计更加鲁棒的多智能体强化学习算法之前必要的步骤揭示了离线MARL数据污染攻击结构的一些特点。

    We characterize offline data poisoning attacks on Multi-Agent Reinforcement Learning (MARL), where an attacker may change a data set in an attempt to install a (potentially fictitious) unique Markov-perfect Nash equilibrium. We propose the unique Nash set, namely the set of games, specified by their Q functions, with a specific joint policy being the unique Nash equilibrium. The unique Nash set is central to poisoning attacks because the attack is successful if and only if data poisoning pushes all plausible games inside it. The unique Nash set generalizes the reward polytope commonly used in inverse reinforcement learning to MARL. For zero-sum Markov games, both the inverse Nash set and the set of plausible games induced by data are polytopes in the Q function space. We exhibit a linear program to efficiently compute the optimal poisoning attack. Our work sheds light on the structure of data poisoning attacks on offline MARL, a necessary step before one can design more robust MARL alg
    
[^40]: DU-Shapley: 一种有效的数据集价值评估的Shapley值代理

    DU-Shapley: A Shapley Value Proxy for Efficient Dataset Valuation. (arXiv:2306.02071v1 [cs.AI])

    [http://arxiv.org/abs/2306.02071](http://arxiv.org/abs/2306.02071)

    本论文提出了一种称为DU-Shapley的方法，用于更有效地计算Shapley值，以实现机器学习中的数据集价值评估。

    

    许多机器学习问题需要进行数据集评估，即量化将一个单独的数据集与其他数据集聚合的增量收益，以某些相关预定义公用事业为基础。最近，Shapley值被提出作为实现这一目标的一种基本工具，因为它具有形式公理证明。由于其计算通常需要指数时间，因此考虑基于Monte Carlo积分的标准近似策略。然而，在某些情况下，这种通用近似方法仍然昂贵。本文利用数据集评估问题的结构知识，设计了更有效的Shapley值估计器。我们提出了一种新的Shapley值近似，称为离散均匀Shapley (DU-Shapley)，其表达为期望值

    Many machine learning problems require performing dataset valuation, i.e. to quantify the incremental gain, to some relevant pre-defined utility, of aggregating an individual dataset to others. As seminal examples, dataset valuation has been leveraged in collaborative and federated learning to create incentives for data sharing across several data owners. The Shapley value has recently been proposed as a principled tool to achieve this goal due to formal axiomatic justification. Since its computation often requires exponential time, standard approximation strategies based on Monte Carlo integration have been considered. Such generic approximation methods, however, remain expensive in some cases. In this paper, we exploit the knowledge about the structure of the dataset valuation problem to devise more efficient Shapley value estimators. We propose a novel approximation of the Shapley value, referred to as discrete uniform Shapley (DU-Shapley) which is expressed as an expectation under 
    
[^41]: 基于大语言模型的推荐系统综述

    A Survey on Large Language Models for Recommendation. (arXiv:2305.19860v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2305.19860](http://arxiv.org/abs/2305.19860)

    本综述介绍了基于大语言模型的推荐系统，提出了判别式LLMs和生成式LLMs两种模型范式，总结了这些模型的最新进展，强调了该领域的挑战和研究方向。

    

    大语言模型（LLMs）已成为自然语言处理（NLP）领域强大的工具，并在推荐系统领域引起了重视。这些模型使用自监督学习在海量数据上进行训练，已在学习通用表示方面取得了显着成功，并有可能通过一些有效的转移技术（如微调和提示调整）等手段提高推荐系统的各个方面的性能。利用大语言模型增强推荐质量的关键是利用它们高质量的文本特征表示和大量的外部知识覆盖，建立项目和用户之间的相关性。为了全面了解现有基于LLM的推荐系统，本综述提出了一种分类法，将这些模型分为两种主要范式，分别是判别式LLMs和生成式LLMs。此外，我们总结了这些范式的最新进展，并强调了这个新兴领域的挑战和开放性研究问题。

    Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discrimi
    
[^42]: ChatLog: 记录和分析ChatGPT随时间的变化

    ChatLog: Recording and Analyzing ChatGPT Across Time. (arXiv:2304.14106v1 [cs.CL])

    [http://arxiv.org/abs/2304.14106](http://arxiv.org/abs/2304.14106)

    ChatLog是一个分析ChatGPT随时间变化的数据集，通过提取ChatGPT知识和语言特征发现了一些稳定的特征，提高了RoBERTa-based detector在新版本ChatGPT上的鲁棒性。

    

    尽管有大量关于在自然语言理解和生成任务中评估ChatGPT的研究，但鲜有研究调查ChatGPT的行为如何随时间变化。在本文中，我们收集了一个粗到细的时间数据集，称为ChatLog，由两个部分组成，每月和每天更新：ChatLog-Monthly是一个数据集，包括每个月收集的38,730个问题-回答对，其中包括推理和分类任务的问题。另一方面，ChatLog-Daily包括ChatGPT每天对1000个相同问题的长篇回答。我们进行全面的自动和人工评估，以提供ChatGPT进化模式存在的证据。我们进一步通过提取其知识和语言特征分析了ChatGPT随时间不变的特征。我们发现一些稳定的特征，以提高基于RoBERTa的检测器在新版本的ChatGPT上的鲁棒性。我们将继续维护我们的数据集以及随时间的分析。

    While there are abundant researches about evaluating ChatGPT on natural language understanding and generation tasks, few studies have investigated how ChatGPT's behavior changes over time. In this paper, we collect a coarse-to-fine temporal dataset called ChatLog, consisting of two parts that update monthly and daily: ChatLog-Monthly is a dataset of 38,730 question-answer pairs collected every month including questions from both the reasoning and classification tasks. ChatLog-Daily, on the other hand, consists of ChatGPT's responses to 1000 identical questions for long-form generation every day. We conduct comprehensive automatic and human evaluation to provide the evidence for the existence of ChatGPT evolving patterns. We further analyze the unchanged characteristics of ChatGPT over time by extracting its knowledge and linguistic features. We find some stable features to improve the robustness of a RoBERTa-based detector on new versions of ChatGPT. We will continuously maintain our p
    
[^43]: 探索基于数值先验的广义CP分解低秩张量补全算法

    Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition. (arXiv:2302.05881v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05881](http://arxiv.org/abs/2302.05881)

    本文提出了一种新的方法框架GCDTC，利用数值先验和广义CP分解实现了更高的低秩张量补全精度；同时介绍了一个算法SPTC，作为该框架的一个实现。在实验中，该方法表现出比现有技术更好的性能。

    

    张量补全在计算机视觉、数据分析和信号处理等领域中具有重要意义。最近，低秩张量补全这一类别的方法得到了广泛研究，对补全张量施加低秩结构。虽然这些方法取得了巨大成功，但尚未考虑到张量元素的数值先验信息。忽略数值先验将导致丢失关于数据的重要信息，因此阻止算法达到最优精度。本研究试图构建一个新的方法框架，名为GCDTC（广义CP分解张量补全），以利用数值先验并实现更高的张量补全精度。在这个新引入的框架中，将广义的CP分解应用于低秩张量补全。本文还提出了一种名为SPTC（平滑泊松张量补全）的算法，用于非负整数张量补全，作为GCDTC框架的一个实现。通过对合成和真实世界数据集的大量实验，证明所提出的方法相比于现有技术具有更优的张量补全性能。

    Tensor completion is important to many areas such as computer vision, data analysis, and signal processing. Enforcing low-rank structures on completed tensors, a category of methods known as low-rank tensor completion has recently been studied extensively. While such methods attained great success, none considered exploiting numerical priors of tensor elements. Ignoring numerical priors causes loss of important information regarding the data, and therefore prevents the algorithms from reaching optimal accuracy. This work attempts to construct a new methodological framework called GCDTC (Generalized CP Decomposition Tensor Completion) for leveraging numerical priors and achieving higher accuracy in tensor completion. In this newly introduced framework, a generalized form of CP Decomposition is applied to low-rank tensor completion. This paper also proposes an algorithm known as SPTC (Smooth Poisson Tensor Completion) for nonnegative integer tensor completion as an instantiation of the G
    
[^44]: 关于上下文学习的综述

    A Survey on In-context Learning. (arXiv:2301.00234v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.00234](http://arxiv.org/abs/2301.00234)

    本文调查和总结了上下文学习(ICL)的进展和挑战，ICL已成为自然语言处理(NLP)的新范式，探索ICL以评估和推广大型语言模型(LLM)的能力已成为一种新趋势。本文提出了ICL的正式定义，并总结了高级技术，最后讨论了ICL的挑战以及进一步研究的潜在方向。

    

    随着大型语言模型（LLM）的能力不断增强，上下文学习（ICL）已成为自然语言处理（NLP）的新范式，在其中LLM仅基于加入少量示例的上下文进行预测。探索ICL以评估和推广LLM的能力已成为一种新趋势。本文旨在调查和总结ICL的进展和挑战。我们首先提出ICL的正式定义，并澄清其与相关研究的关系。然后，我们组织和讨论高级技术，包括训练策略、演示设计策略以及相关分析。最后，我们讨论了ICL的挑战，并提供了进一步研究的潜在方向。我们希望我们的工作可以鼓励更多的研究，揭示ICL的工作原理并改进ICL。

    With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.
    

