# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Supervised Algorithmic Fairness in Distribution Shifts: A Survey](https://rss.arxiv.org/abs/2402.01327) | 这篇综述研究了分布变化下的监督公平机器学习领域，调查了各种类型的分布变化和现有的解决方法，并列举了公开数据集和评估指标。研究发现六种常用的方法，并探讨了与相关研究领域的交互关系。 |
| [^2] | [Machine Learning Robustness: A Primer](https://arxiv.org/abs/2404.00897) | 该章节探讨了机器学习中稳健性的重要概念及关键的技术和因素，以确立人工智能系统的可信度。 |
| [^3] | [Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain](https://arxiv.org/abs/2403.20288) | LLMs在医学决策中提供重要反馈，可以挑战不正确的诊断，促进更准确的决策。 |
| [^4] | [MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models](https://arxiv.org/abs/2403.17141) | MetaAligner是第一个与策略无关且通用的多目标偏好对齐方法，通过将参数更新与策略模型解耦实现即插即用的对齐，并通过上下文学习实现未见目标的零冷启动偏好对齐 |
| [^5] | [Towards a theory of model distillation](https://arxiv.org/abs/2403.09053) | 提出了模型蒸馏的一般理论，通过PAC-蒸馏定义，提出了抽取神经网络训练权重知识的新算法，并证明了蒸馏比从头学习更便宜且有助于理解其复杂性。 |
| [^6] | [Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement](https://arxiv.org/abs/2403.06659) | 通过Multimodal ECG Representation Learning (MERL)框架，本文提出了一种零样本心电图分类方法，结合了对ECG记录和相关报告的多模态学习，同时在测试阶段使用了Clinical Knowledge Enhanced Prompt Engineering (CKEPE)方法来利用临床知识数据库。 |
| [^7] | [Effectiveness Assessment of Recent Large Vision-Language Models](https://arxiv.org/abs/2403.04306) | 本文评估了最近出现的大型视觉-语言模型在专业和通用任务中的表现，旨在全面了解这些创新方法的能力。 |
| [^8] | [Simplicity in Complexity](https://arxiv.org/abs/2403.03134) | 本研究提出使用基于区段的图像表示来建模复杂性，与之前复杂的图像复杂性模型不同，这种方法既能泛化，又能为理论理解提供指导。 |
| [^9] | [Evaluating and Optimizing Educational Content with Large Language Model Judgments](https://arxiv.org/abs/2403.02795) | 使用语言模型作为教育专家来评估教育内容的影响，展示了LMs作为可靠评估者的潜力，并介绍了一种指导优化方法。 |
| [^10] | [World Models for Autonomous Driving: An Initial Survey](https://arxiv.org/abs/2403.02622) | 世界模型在自主驾驶领域的重要性和作用，是通过准确预测未来事件和评估其影响来帮助决策过程，从而推动自主驾驶技术发展的革命性方法。 |
| [^11] | [On the Compressibility of Quantized Large Language Models](https://arxiv.org/abs/2403.01384) | 研究在内存受限设备上应用数据压缩技术以加速量化LLM推理过程的一项初步工作。 |
| [^12] | [Distilling Text Style Transfer With Self-Explanation From LLMs](https://arxiv.org/abs/2403.01106) | CoTeX是一个利用大型语言模型和思维链提示来促进文本风格转移的框架，通过提炼LLMs的能力为处理非平行数据和平行数据的简化模型，在低资源情况下表现优于传统的监督微调和知识蒸馏方法，并通过透明的解释在风格转移过程中有显著优势。 |
| [^13] | [PEM: Prototype-based Efficient MaskFormer for Image Segmentation](https://arxiv.org/abs/2402.19422) | PEM提出了基于原型的高效MaskFormer，通过引入原型交叉注意力和多尺度特征金字塔网络，实现了在多个分割任务中的高效运行。 |
| [^14] | [Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy](https://arxiv.org/abs/2402.19379) | 该研究通过将十二个LLMs组成的LLM集成方法与925名人类预测者的群体预测进行比较，发现LLM群体优于简单的无信息基准，并在统计上等效于人类群体。 |
| [^15] | [REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories](https://arxiv.org/abs/2402.16310) | 该论文提出了REPLAY模型，利用一般RNN架构来学习捕捉人类移动的时间变化规律，用于位置预测。 |
| [^16] | [Emoji Driven Crypto Assets Market Reactions](https://arxiv.org/abs/2402.10481) | 该研究利用GPT-4和BERT模型进行多模态情感分析，发现基于表情符号情绪的策略可以帮助避免市场下挫并稳定回报。 |
| [^17] | [Tackling Negative Transfer on Graphs](https://arxiv.org/abs/2402.08907) | 图迁移学习中的负迁移现象尚未得到充分研究，本文发现在图结构数据中负迁移普遍存在，即使源图和目标图在语义上相似。我们提出了一个新的观点，对于语义相似的图，结构差异对子图嵌入的影响较小。 |
| [^18] | [CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines](https://arxiv.org/abs/2402.04400) | CEHR-GPT是一种使用病人时间轴生成电子健康记录的方法，能够处理EHR数据的合成、疾病进展分析等应用。 |
| [^19] | [Graph Transformers without Positional Encodings](https://arxiv.org/abs/2401.17791) | 本文介绍了一种不需要位置编码的图变压器模型，该模型通过注意机制本身包含图结构信息，并通过实验证明了其有效性。 |
| [^20] | [Imitation Bootstrapped Reinforcement Learning](https://arxiv.org/abs/2311.02198) | 提出了一种模仿引导式强化学习（IBRL）的框架，用于高效的样本-efficient RL，通过先在提供的示范上训练IL策略，然后使用它提出替代动作进行在线探索和引导目标值。 |
| [^21] | [Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning](https://arxiv.org/abs/2204.04510) | 提出了一种将子图转化为节点的方法来学习子图的表示，该方法不仅显著降低了内存和计算成本，还捕捉了子图的局部和全局结构，并在多个基准测试上表现出色。 |
| [^22] | [DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven Policy Learning.](http://arxiv.org/abs/2401.09243) | 本文介绍了DiffClone，一种通过扩散驱动的策略学习增强行为克隆代理的离线算法。在真实的在线物理机器人上的实验表明，采用MOCO微调的ResNet50的效果最好。 |
| [^23] | [FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding.](http://arxiv.org/abs/2401.01970) | 提出了一种新颖的方法，将视觉语言嵌入基础模型到3D高斯分割中，实现了高质量的3D场景理解。 |
| [^24] | [A ripple in time: a discontinuity in American history.](http://arxiv.org/abs/2312.01185) | 该论文通过使用向量嵌入和非线性降维方法，发现GPT-2与UMAP的结合可以提供更好的分离和聚类效果。同时，经过微调的DistilBERT模型可用于识别总统和演讲的年份。 |
| [^25] | [How do Language Models Bind Entities in Context?.](http://arxiv.org/abs/2310.17191) | 通过分析语言模型的表示，我们发现了绑定ID机制，它可以将实体与属性进行有效地绑定。我们通过因果干预实验进一步证明了语言模型内部激活表示绑定信息的方式。研究结果揭示了语言模型在上下文中如何表示符号知识，从而为理解大规模语言模型的一般上下文推理提供了指导。 |
| [^26] | [KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models.](http://arxiv.org/abs/2310.15872) | 本文提出了一种称为基赫霍夫网络的神经网络模型，利用基赫霍夫电流定律与消息传递神经网络和连续深度网络建立连接。在MNIST数据集上，基赫霍夫网络可以实现接近98.86%的测试准确度，且具有在硬件上实现的潜力。无论网络参数数量如何，其正向计算都可以在1/f秒内完成，具有快速计算的硬件特性。 |
| [^27] | [Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming from One-Shot Observation.](http://arxiv.org/abs/2310.10690) | 本研究探索在开放式学习环境中使用大型语言模型进行上下文学生建模，提出了一个新的框架LLM-SS，通过合成学生在不同任务上的尝试，为学生建模提供更准确的预测和教学策略。 |
| [^28] | [Beyond Memorization: Violating Privacy Via Inference with Large Language Models.](http://arxiv.org/abs/2310.07298) | 该论文首次全面研究了预训练大型语言模型从文本中推断个人属性的能力，发现当前的模型可以以较低的成本和时间比例，准确地推断出多种个人属性，这引发了隐私泄露的新威胁。 |
| [^29] | [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations.](http://arxiv.org/abs/2310.06387) | 本文研究了使用少量上下文示范来操纵语言模型对齐能力的方法。通过提供示范，而无需微调，可以增加或降低模型回答恶意提示的概率。我们提出了相同上下文攻击和相同上下文防御方法，用于越狱和对齐语言模型。实验证明了这些方法的有效性。 |
| [^30] | [Making Retrieval-Augmented Language Models Robust to Irrelevant Context.](http://arxiv.org/abs/2310.01558) | 本文分析了检索增强的语言模型在开放领域问答中的性能问题，并提出了基于自然语言推理的方法来缓解这个问题。 |
| [^31] | [RA-DIT: Retrieval-Augmented Dual Instruction Tuning.](http://arxiv.org/abs/2310.01352) | 本论文介绍了一种轻量级的微调方法RA-DIT，通过为任何语言模型添加检索能力来提高性能。该方法分为两个步骤：一是更新语言模型以更好地利用检索到的信息，二是更新检索器以返回更相关的结果。实验证明每个步骤都能显著提高性能，同时使用两个步骤可以获得额外的收益。 |
| [^32] | [Towards Causal Foundation Model: on Duality between Causal Inference and Attention.](http://arxiv.org/abs/2310.00809) | 该论文提出了一种名为Causal Inference with Attention (CInA)的新方法，利用因果推断和注意力的对偶关系，在复杂任务中实现了零样本的因果推断。 |
| [^33] | [LLM-grounded Video Diffusion Models.](http://arxiv.org/abs/2309.17444) | 使用LLM-grounded Video Diffusion (LVD)模型，通过先生成动态场景布局，再通过这些布局指导视频生成的扩散模型，解决了当前模型在复杂的时空提示和不正确的运动生成方面的困难。 |
| [^34] | [PLMM: Personal Large Models on Mobile Devices.](http://arxiv.org/abs/2309.14726) | 本文提出了一种从传统大型语言模型中提取的个人大型模型，该模型更适应于本地用户的个人信息，并且能够保护用户的隐私。该模型分为个人级别、专家级别和传统级别，同时还需要小型化以适应个人计算机或移动设备，并实现实时响应以提供更好的用户体验。 |
| [^35] | [Collision Avoidance and Navigation for a Quadrotor Swarm Using End-to-end Deep Reinforcement Learning.](http://arxiv.org/abs/2309.13285) | 这项研究提出了一种基于端到端深度强化学习的方法，用于在带有障碍物的环境中控制无人机群体，并通过课程学习和回放缓冲区提高性能。此外，还实施了注意力机制以关注邻近机器人和障碍物之间的相互作用。 |
| [^36] | [Pursuing Counterfactual Fairness via Sequential Autoencoder Across Domains.](http://arxiv.org/abs/2309.13005) | 通过顺序自编码器实现反事实公平性追求的创新框架将环境信息和敏感属性与分类特征的嵌入表示分开，以提高模型在不同和陌生领域中的泛化能力，并解决公平问题。 |
| [^37] | [Multicopy Reinforcement Learning Agents.](http://arxiv.org/abs/2309.10908) | 本文研究了一种新型的多智能体问题，在这个问题中，智能体制作多个相同副本来更好地完成任务。通过利用价值函数的结构，提出了一种学习算法来平衡添加额外副本的优势和成本。 |
| [^38] | [Multi-Object Graph Affordance Network: Enabling Goal-Oriented Planning through Compound Object Affordances.](http://arxiv.org/abs/2309.10426) | 多对象图形可用性网络（MOGAN）模型化了复合对象的可用性，预测了将新对象放置在复合对象上的效果。在构建具有特定高度或属性的塔等任务中，我们使用基于搜索的规划找到具有适当可用性的对象堆叠操作的序列。我们的系统能够正确建模非常复杂的复合对象的可用性，并在模拟和真实环境中展示了其适用性。 |
| [^39] | [Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion.](http://arxiv.org/abs/2308.12517) | 本文提出了一种新的强化学习框架，为复杂机器人系统训练神经网络控制器。该框架引入了奖励和约束的概念，通过设计高效的策略优化算法来处理约束，以减少计算开销。通过应用于不同腿式机器人的运动控制器训练中，展示了该框架的有效性。 |
| [^40] | [Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI).](http://arxiv.org/abs/2308.11471) | 本文提出了一种动态开放词汇增强的智能安全着陆系统，通过利用开放词汇图像分割的能力实现无人机的视觉伺服，适应不同场景且无需大量数据积累进行模型改进，可以处理100米高度的操作。 |
| [^41] | [Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes.](http://arxiv.org/abs/2308.11267) | 本文介绍了两种算法，具有鲁棒拉格朗日的RCPG和对抗性RCPG，用于解决鲁棒约束马尔可夫决策过程中的问题。具有鲁棒拉格朗日的RCPG通过使用拉格朗日来计算最坏情况下的动态，而对抗性RCPG通过对抗策略的方式直接和增量学习最坏情况下的动态。 |
| [^42] | [Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization.](http://arxiv.org/abs/2308.02151) | 本文介绍了一种通过策略梯度优化的回顾性大型语言代理框架，该框架通过学习环境反馈来调整语言代理的提示，从而优化其性能。这种代理能够从多个环境和任务中学习奖励，并通过总结以前任务的根本原因来改进语言代理提示。 |
| [^43] | [A Comprehensive Survey of Artificial Intelligence Techniques for Talent Analytics.](http://arxiv.org/abs/2307.03195) | 这篇论文综述了人工智能技术在人才分析中的应用。通过使用大数据和人工智能技术，组织可以从数据科学的角度理解组织行为并实时做出决策，为有效的人才管理提供智能支持。 |
| [^44] | [BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables.](http://arxiv.org/abs/2307.02891) | 本文提出了一种名为BaBE的方法，通过估计潜在解释变量来提高公平性。该方法通过结合贝叶斯推断和期望最大化方法，估计给定Z的每个群体E的最可能值。 |
| [^45] | [Are aligned neural networks adversarially aligned?.](http://arxiv.org/abs/2306.15447) | 我们研究了大型语言模型在面对对抗用户构建的对抗性输入时是否仍能保持对齐。我们发现现有的攻击手法不足以可靠攻击对齐文本模型，并通过蛮力方法找到了对抗性输入。 |
| [^46] | [Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?.](http://arxiv.org/abs/2306.14222) | 本篇论文研究如何运用大型语言模型提取中文新闻文本信息的情感因素，以期促进知情和高频的投资组合调整。通过建立严格和全面的基准测试与标准化的回测框架，作者对不同类型 LLMs 在该领域内的效果进行了客观评估。 |
| [^47] | [LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models.](http://arxiv.org/abs/2306.12420) | LMFlow是一个可扩展和轻量级的工具包，为大型基础模型提供完整的微调工作流程，以支持在有限的计算资源下进行个性化训练，并支持连续预训练和指令微调以适应不同的专业任务。 |
| [^48] | [A Simple and Effective Pruning Approach for Large Language Models.](http://arxiv.org/abs/2306.11695) | 本论文提出了一种称为Wanda的新颖、简单而有效的剪枝方法，用于大型语言模型，通过对每个输出上的权重按照最小幅度乘以对应的输入激活来进行剪枝，无需重新训练或更新权重。 |
| [^49] | [SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking.](http://arxiv.org/abs/2306.05426) | 本文提出了一种称为SequenceMatch的带有回溯的自回归模型的模仿学习框架，该框架通过最小化自回归模型生成序列和数据集序列之间的各种分歧来减少在自回归生成过程中的复合误差，并允许引入回溯动作。 |
| [^50] | [Navigating Explanatory Multiverse Through Counterfactual Path Geometry.](http://arxiv.org/abs/2306.02786) | 该论文提出了解释性多元宇宙的概念，用于导航和比较所有可能的反事实路径的几何关系。 |
| [^51] | [Decision-Oriented Dialogue for Human-AI Collaboration.](http://arxiv.org/abs/2305.20076) | 该论文探讨了一类以决策为导向的人机对话任务，以及在会议论文审稿人分配、城市多步行程规划和旅行计划协商等场景中，人工智能助手和用户不同的能力如何结合以达到最佳决策。论文通过构建对话环境并进行人机对话收集数据，发现当前人工智能助手在此类任务中的局限性。 |
| [^52] | [Fairness of ChatGPT.](http://arxiv.org/abs/2305.18569) | 本文提供了一个使用ChatGPT作为研究案例的LLM有效性和公平性的系统评估，旨在评估ChatGPT在高风险领域的表现，以提供更深入的了解LLM的公平表现，并为偏见缓解和负责任的人工智能系统的发展做出贡献。 |
| [^53] | [Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives.](http://arxiv.org/abs/2305.08088) | 本文提出了BBT-RGB，一套用于增强黑盒优化效率和性能的直接且互补技术套件，包括两阶段无导数优化策略、自动语言转化器构建及其在少样本设置中的新用法以及更好的提示初始化。 |
| [^54] | [An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework.](http://arxiv.org/abs/2305.01322) | 该论文关注强化学习中的探索研究，提出了一个能够自主管理探索策略的多模式智能体非单体探索方法，并通过实验结果展示了该方法的优越性能。 |
| [^55] | [RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding.](http://arxiv.org/abs/2304.00962) | 提出了一种用于开放世界3D场景理解的Regional Point-Language Contrastive Learning框架，通过密集的视觉提示和点独立对比学习来实现对新颖类别的识别和密集场景理解。 |
| [^56] | [AraSpot: Arabic Spoken Command Spotting.](http://arxiv.org/abs/2303.16621) | AraSpot是一款使用ConformerGRU模型架构训练40个阿拉伯语关键词的口语命令识别工具，其通过在线数据增强和文本到语音模型的训练提高了性能，并以99.59%的准确率超出以往的方法。 |
| [^57] | [Analyzing And Editing Inner Mechanisms Of Backdoored Language Models.](http://arxiv.org/abs/2302.12461) | 本研究分析并编辑暗藏后门的语言模型的内部机制，发现早期层的MLP模块和初始嵌入投影是后门机制中最重要的部分。通过使用PCP消融技术替换变压器模块，我们成功删除、插入和修改后门机制，并显著改善了后门的输出效果。 |
| [^58] | [Average-Constrained Policy Optimization.](http://arxiv.org/abs/2302.00808) | 本研究提出了一种新的基于函数逼近算法的带平均标准约束 MDP 的策略优化算法，具有较好的性能表现。 |
| [^59] | [Low Variance Off-policy Evaluation with State-based Importance Sampling.](http://arxiv.org/abs/2212.03932) | 本文提出了一种基于状态的重要性抽样（SIS）方法，通过检测“忽略状态”的子轨迹来实现低方差的离线评估。 |
| [^60] | [Can Brain Signals Reveal Inner Alignment with Human Languages?.](http://arxiv.org/abs/2208.06348) | 本研究探索了脑信号和人类语言之间的关系，并介绍了一种名为MTAM的方法，该方法在情感分析和关系检测等下游应用中取得了新的最先进结果。 |
| [^61] | [Illusory Attacks: Detectability Matters in Adversarial Attacks on Sequential Decision-Makers.](http://arxiv.org/abs/2207.10170) | 对于顺序决策者的敌对攻击来说，弱点是缺乏时间上的一致性，使其容易被检测出来；而R-attack是一种既有效又可证明是统计不可检测的攻击，可以更难以使用自动化方法检测出来。 |

# 详细

[^1]: 在分布变化中的监督算法公平性：一项综述

    Supervised Algorithmic Fairness in Distribution Shifts: A Survey

    [https://rss.arxiv.org/abs/2402.01327](https://rss.arxiv.org/abs/2402.01327)

    这篇综述研究了分布变化下的监督公平机器学习领域，调查了各种类型的分布变化和现有的解决方法，并列举了公开数据集和评估指标。研究发现六种常用的方法，并探讨了与相关研究领域的交互关系。

    

    在分布变化下的监督公平机器学习是一个新兴领域，解决了面对从源领域到目标领域的数据分布变化时，如何保持公平和无偏预测的挑战。在现实世界的应用中，机器学习模型通常是在特定数据集上进行训练，但在部署时，数据分布可能因各种因素而随时间发生变化。这种变化可能导致不公平的预测，对特定通过敏感属性（如种族和性别）来表征的群体产生不均衡的影响。在这项调查中，我们对各种类型的分布变化进行了总结，并全面调查了基于这些变化的现有方法，在文献中突出了六种常用的方法。此外，这份调查列出了用于实证研究的公开可用数据集和评估指标。我们进一步探讨了与相关研究领域的交互关系，并讨论了其中的重要创新和贡献。

    Supervised fairness-aware machine learning under distribution shifts is an emerging field that addresses the challenge of maintaining equitable and unbiased predictions when faced with changes in data distributions from source to target domains. In real-world applications, machine learning models are often trained on a specific dataset but deployed in environments where the data distribution may shift over time due to various factors. This shift can lead to unfair predictions, disproportionately affecting certain groups characterized by sensitive attributes, such as race and gender. In this survey, we provide a summary of various types of distribution shifts and comprehensively investigate existing methods based on these shifts, highlighting six commonly used approaches in the literature. Additionally, this survey lists publicly available datasets and evaluation metrics for empirical studies. We further explore the interconnection with related research fields, discuss the significant c
    
[^2]: 机器学习鲁棒性：入门指南

    Machine Learning Robustness: A Primer

    [https://arxiv.org/abs/2404.00897](https://arxiv.org/abs/2404.00897)

    该章节探讨了机器学习中稳健性的重要概念及关键的技术和因素，以确立人工智能系统的可信度。

    

    这一章节探讨了机器学习（ML）中稳健性的基本概念及其在确立人工智能（AI）系统的可信度中的重要作用。讨论始于稳健性的详细定义，将其描述为ML模型在各种不同和意外的环境条件下保持稳定性能的能力。ML鲁棒性通过多个视角进行了剖析：其与泛化能力的互补性；其作为可信AI的要求；其对抗性与非对抗性方面；其数量化指标；以及其可复现性和可解释性等指标。章节深入探讨了影响鲁棒性的因素，如数据偏差、模型复杂性以及ML流程不明确的风险。它从广泛的视角调查了鲁棒性评估的关键技术，包括对抗性攻击，包括数字和物理领域。

    arXiv:2404.00897v1 Announce Type: cross  Abstract: This chapter explores the foundational concept of robustness in Machine Learning (ML) and its integral role in establishing trustworthiness in Artificial Intelligence (AI) systems. The discussion begins with a detailed definition of robustness, portraying it as the ability of ML models to maintain stable performance across varied and unexpected environmental conditions. ML robustness is dissected through several lenses: its complementarity with generalizability; its status as a requirement for trustworthy AI; its adversarial vs non-adversarial aspects; its quantitative metrics; and its indicators such as reproducibility and explainability. The chapter delves into the factors that impede robustness, such as data bias, model complexity, and the pitfalls of underspecified ML pipelines. It surveys key techniques for robustness assessment from a broad perspective, including adversarial attacks, encompassing both digital and physical realms.
    
[^3]: LLM能够在医学领域中纠正医生吗？研究有效的交互方法

    Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain

    [https://arxiv.org/abs/2403.20288](https://arxiv.org/abs/2403.20288)

    LLMs在医学决策中提供重要反馈，可以挑战不正确的诊断，促进更准确的决策。

    

    我们探讨了大型语言模型（LLMs）在协助并可能纠正医生进行医疗决策任务方面的潜力。我们评估了几种LLMs，包括Meditron、Llama2和Mistral，分析这些模型在不同情景下与医生有效交互的能力。我们考虑了来自PubMedQA的问题和几项任务，从二元（是/否）回答到长答案生成，其中模型的答案是在与医生交互后产生的。我们的研究结果表明，提示设计显著影响了LLMs的下游准确性，并且LLMs可以为医生提供有价值的反馈，挑战不正确的诊断，促进更准确的决策。例如，当医生准确率为38%时，Mistral可以给出正确答案，根据所使用的提示，将准确性提高到74%，而Llama2和Meditron模型也能提供类似的改进。

    arXiv:2403.20288v1 Announce Type: cross  Abstract: We explore the potential of Large Language Models (LLMs) to assist and potentially correct physicians in medical decision-making tasks. We evaluate several LLMs, including Meditron, Llama2, and Mistral, to analyze the ability of these models to interact effectively with physicians across different scenarios. We consider questions from PubMedQA and several tasks, ranging from binary (yes/no) responses to long answer generation, where the answer of the model is produced after an interaction with a physician. Our findings suggest that prompt design significantly influences the downstream accuracy of LLMs and that LLMs can provide valuable feedback to physicians, challenging incorrect diagnoses and contributing to more accurate decision-making. For example, when the physician is accurate 38% of the time, Mistral can produce the correct answer, improving accuracy up to 74% depending on the prompt being used, while Llama2 and Meditron models
    
[^4]: MetaAligner：用于语言模型通用多目标对齐的条件从弱到强校正

    MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models

    [https://arxiv.org/abs/2403.17141](https://arxiv.org/abs/2403.17141)

    MetaAligner是第一个与策略无关且通用的多目标偏好对齐方法，通过将参数更新与策略模型解耦实现即插即用的对齐，并通过上下文学习实现未见目标的零冷启动偏好对齐

    

    近期大型语言模型（LLM）的进展旨在通过多目标偏好对齐来解决异质人类期望和价值观，然而，现有方法受到策略模型的参数限制，导致两个关键局限性：（1）它们的对齐算法对于每个新目标模型的重复成本很高；（2）由于其静态对齐目标，它们无法扩展到未见目标。在这项工作中，我们提出了Meta-Objective Aligner（MetaAligner），这是一种执行条件从弱到强校正以逼近强响应的模型。MetaAligner是第一个与策略无关且通用的多目标偏好对齐方法，它通过将参数更新与策略模型解耦实现即插即用的对齐，并通过上下文学习实现未见目标的零冷启动偏好对齐。实验结果表明，MetaAligner取得了显著

    arXiv:2403.17141v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) aim to tackle heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are parameter-adherent to the policy model, leading to two key limitations: (1) the high-cost repetition of their alignment algorithms for each new target model; (2) they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose Meta-Objective Aligner (MetaAligner), a model that performs conditional weak-to-strong correction for weak responses to approach strong responses. MetaAligner is the first policy-agnostic and generalizable method for multi-objective preference alignment, which enables plug-and-play alignment by decoupling parameter updates from the policy models and facilitates zero-shot preference alignment for unseen objectives via in-context learning. Experimental results show that MetaAligner achieves sign
    
[^5]: 走向模型蒸馏理论

    Towards a theory of model distillation

    [https://arxiv.org/abs/2403.09053](https://arxiv.org/abs/2403.09053)

    提出了模型蒸馏的一般理论，通过PAC-蒸馏定义，提出了抽取神经网络训练权重知识的新算法，并证明了蒸馏比从头学习更便宜且有助于理解其复杂性。

    

    蒸馏是将复杂的机器学习模型替换为简化模型来近似原模型的任务。尽管有许多实际应用，关于模型蒸馏的程度、所需运行时间和数据量的基本问题仍然大多未解。为了研究这些问题，我们开始了蒸馏的一般理论，以类似的方式定义了PAC-蒸馏 [Val84]，提出了提取训练权重中存储的知识的新算法，展示了如何通过使用“线性表示假设”将神经网络高效地蒸馏成简明明了的决策树表示，还证明了蒸馏可以比从头开始学习便宜得多，并在表征其复杂性方面取得了进展。

    arXiv:2403.09053v1 Announce Type: cross  Abstract: Distillation is the task of replacing a complicated machine learning model with a simpler model that approximates the original [BCNM06,HVD15]. Despite many practical applications, basic questions about the extent to which models can be distilled, and the runtime and amount of data needed to distill, remain largely open.   To study these questions, we initiate a general theory of distillation, defining PAC-distillation in an analogous way to PAC-learning [Val84]. As applications of this theory: (1) we propose new algorithms to extract the knowledge stored in the trained weights of neural networks -- we show how to efficiently distill neural networks into succinct, explicit decision tree representations when possible by using the ``linear representation hypothesis''; and (2) we prove that distillation can be much cheaper than learning from scratch, and make progress on characterizing its complexity.
    
[^6]: 基于多模态学习和测试时临床知识增强的零样本心电图分类

    Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement

    [https://arxiv.org/abs/2403.06659](https://arxiv.org/abs/2403.06659)

    通过Multimodal ECG Representation Learning (MERL)框架，本文提出了一种零样本心电图分类方法，结合了对ECG记录和相关报告的多模态学习，同时在测试阶段使用了Clinical Knowledge Enhanced Prompt Engineering (CKEPE)方法来利用临床知识数据库。

    

    心电图（ECG）是临床实践中用于检测心律失常疾病的非侵入性诊断工具。在未经注释的ECG数据中进行自监督学习（eSSL）方法显示出了表征学习的潜力，但往往忽视了可以在报告中找到的临床知识。本文通过多模态学习ECG记录和相关报告，提出了Multimodal ECG Representation Learning (MERL)框架，该框架能够使用文本提示进行零样本ECG分类，消除了下游任务中对训练数据的需求。在测试时，我们提出了Clinical Knowledge Enhanced Prompt Engineering (CKEPE)方法，利用大型语言模型（LLM）来利用外部专家验证的临床知识数据库，生成更多关于患者病史的提示。

    arXiv:2403.06659v1 Announce Type: cross  Abstract: Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice. While ECG Self-supervised Learning (eSSL) methods show promise in representation learning from unannotated ECG data, they often overlook the clinical knowledge that can be found in reports. This oversight and the requirement for annotated samples for downstream tasks limit eSSL's versatility. In this work, we address these issues with the Multimodal ECG Representation Learning (MERL}) framework. Through multimodal learning on ECG records and associated reports, MERL is capable of performing zero-shot ECG classification with text prompts, eliminating the need for training data in downstream tasks. At test time, we propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach, which uses Large Language Models (LLMs) to exploit external expert-verified clinical knowledge databases, generating mo
    
[^7]: 最近大型视觉-语言模型的有效性评估

    Effectiveness Assessment of Recent Large Vision-Language Models

    [https://arxiv.org/abs/2403.04306](https://arxiv.org/abs/2403.04306)

    本文评估了最近出现的大型视觉-语言模型在专业和通用任务中的表现，旨在全面了解这些创新方法的能力。

    

    大型视觉-语言模型(LVLMs)的出现代表着迈向人工通用智能的重要进步。然而，它们在专业和通用任务中的有效性程度需要进一步调查。本文旨在评估流行的LVLMs在专业和通用任务中的能力，旨在提供对这些创新方法的全面理解。为了评估它们在专业任务中的有效性，我们量身定制了一个包含自然、医疗和工业三种不同场景的全面测试平台，涵盖六项具有挑战性的任务。这些任务包括显著、伪装和透明物体检测，以及息肉和皮肤病变检测，以及工业异常检测。我们检验了最近三种开源LVLMs--MiniGPT-v2、LLaVA-1.5和Shikra--在视觉识别和定位领域的表现。

    arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
    
[^8]: 复杂中的简单

    Simplicity in Complexity

    [https://arxiv.org/abs/2403.03134](https://arxiv.org/abs/2403.03134)

    本研究提出使用基于区段的图像表示来建模复杂性，与之前复杂的图像复杂性模型不同，这种方法既能泛化，又能为理论理解提供指导。

    

    视觉刺激的复杂性在许多认知现象中起着重要作用，包括注意力、参与度、易记性、时间感知和美学评价。然而，尽管其重要性，复杂性仍然知之甚少，讽刺的是，先前的图像复杂性模型相当复杂。早先的模型试图寻找手工制作的特征来解释复杂性，但这些特征通常是特定于数据集的，因此无法泛化。与此同时，最近的研究采用了深度神经网络来预测复杂性，但这些模型仍然难以解释，并且不指导对问题的理论理解。在本文中，我们提出使用基于区段的图像表示来建模复杂性。我们使用最先进的分割模型SAM和FC-CLIP，来量化图像中的多个粒度的区段数量，以及图像中的类别数量。

    arXiv:2403.03134v1 Announce Type: cross  Abstract: The complexity of visual stimuli plays an important role in many cognitive phenomena, including attention, engagement, memorability, time perception and aesthetic evaluation. Despite its importance, complexity is poorly understood and ironically, previous models of image complexity have been quite \textit{complex}. There have been many attempts to find handcrafted features that explain complexity, but these features are usually dataset specific, and hence fail to generalise. On the other hand, more recent work has employed deep neural networks to predict complexity, but these models remain difficult to interpret, and do not guide a theoretical understanding of the problem. Here we propose to model complexity using segment-based representations of images. We use state-of-the-art segmentation models, SAM and FC-CLIP, to quantify the number of segments at multiple granularities, and the number of classes in an image respectively. We find 
    
[^9]: 用大型语言模型评估和优化教育内容

    Evaluating and Optimizing Educational Content with Large Language Model Judgments

    [https://arxiv.org/abs/2403.02795](https://arxiv.org/abs/2403.02795)

    使用语言模型作为教育专家来评估教育内容的影响，展示了LMs作为可靠评估者的潜力，并介绍了一种指导优化方法。

    

    创建有效的教育材料通常需要对学生学习成果进行昂贵且耗时的研究。为了克服这一障碍，一个想法是构建学生学习的计算模型，并使用它们来优化教学材料。然而，模拟学习动态的认知过程是困难的。我们提出了一种使用语言模型（LMs）作为教育专家来评估各种指导对学习结果影响的替代方法。具体地，我们使用GPT-3.5来评估指导材料对不同学生群体的整体影响，并发现它可以复制诸如专业逆转效应和变异效应等已经建立的教育发现。这展示了LMs作为教育内容可靠评估者的潜力。基于这一见解，我们介绍了一种指导优化方法，其中一个LM生成指导。

    arXiv:2403.02795v1 Announce Type: new  Abstract: Creating effective educational materials generally requires expensive and time-consuming studies of student learning outcomes. To overcome this barrier, one idea is to build computational models of student learning and use them to optimize instructional materials. However, it is difficult to model the cognitive processes of learning dynamics. We propose an alternative approach that uses Language Models (LMs) as educational experts to assess the impact of various instructions on learning outcomes. Specifically, we use GPT-3.5 to evaluate the overall effect of instructional materials on different student groups and find that it can replicate well-established educational findings such as the Expertise Reversal Effect and the Variability Effect. This demonstrates the potential of LMs as reliable evaluators of educational content. Building on this insight, we introduce an instruction optimization approach in which one LM generates instruction
    
[^10]: 自主驾驶的世界模型：一项初步调查

    World Models for Autonomous Driving: An Initial Survey

    [https://arxiv.org/abs/2403.02622](https://arxiv.org/abs/2403.02622)

    世界模型在自主驾驶领域的重要性和作用，是通过准确预测未来事件和评估其影响来帮助决策过程，从而推动自主驾驶技术发展的革命性方法。

    

    在自主驾驶领域不断发展的背景下，准确预测未来事件并评估其影响对于安全和效率至关重要，关键地帮助决策过程。世界模型已经成为一种革命性方法，使自主驾驶系统能够综合和解释大量传感器数据，从而预测潜在的未来情景并弥补信息缺口。本文对自主驾驶中世界模型的当前状态和未来发展进行了初步审查，涵盖了其理论基础、实际应用以及旨在克服现有限制的正在进行的研究工作。强调了世界模型在推动自主驾驶技术发展中的重要作用，本调查旨在成为研究社区的基础参考，便于快速获得和应用。

    arXiv:2403.02622v1 Announce Type: cross  Abstract: In the rapidly evolving landscape of autonomous driving, the capability to accurately predict future events and assess their implications is paramount for both safety and efficiency, critically aiding the decision-making process. World models have emerged as a transformative approach, enabling autonomous driving systems to synthesize and interpret vast amounts of sensor data, thereby predicting potential future scenarios and compensating for information gaps. This paper provides an initial review of the current state and prospective advancements of world models in autonomous driving, spanning their theoretical underpinnings, practical applications, and the ongoing research efforts aimed at overcoming existing limitations. Highlighting the significant role of world models in advancing autonomous driving technologies, this survey aspires to serve as a foundational reference for the research community, facilitating swift access to and com
    
[^11]: 关于量化大型语言模型的可压缩性

    On the Compressibility of Quantized Large Language Models

    [https://arxiv.org/abs/2403.01384](https://arxiv.org/abs/2403.01384)

    研究在内存受限设备上应用数据压缩技术以加速量化LLM推理过程的一项初步工作。

    

    部署大型语言模型（LLMs）到边缘或移动设备上具有显著优势，如增强数据隐私和实时处理能力。本文研究了将数据压缩技术应用于减少数据移动，从而加速内存受限设备上量化LLM的推理过程的初步步骤。

    arXiv:2403.01384v1 Announce Type: cross  Abstract: Deploying Large Language Models (LLMs) on edge or mobile devices offers significant benefits, such as enhanced data privacy and real-time processing capabilities. However, it also faces critical challenges due to the substantial memory requirement of LLMs. Quantization is an effective way of reducing the model size while maintaining good performance. However, even after quantization, LLMs may still be too big to fit entirely into the limited memory of edge or mobile devices and have to be partially loaded from the storage to complete the inference. In this case, the I/O latency of model loading becomes the bottleneck of the LLM inference latency. In this work, we take a preliminary step of studying applying data compression techniques to reduce data movement and thus speed up the inference of quantized LLM on memory-constrained devices. In particular, we discussed the compressibility of quantized LLMs, the trade-off between the compres
    
[^12]: 通过从大型语言模型中自我解释提炼文本风格转移

    Distilling Text Style Transfer With Self-Explanation From LLMs

    [https://arxiv.org/abs/2403.01106](https://arxiv.org/abs/2403.01106)

    CoTeX是一个利用大型语言模型和思维链提示来促进文本风格转移的框架，通过提炼LLMs的能力为处理非平行数据和平行数据的简化模型，在低资源情况下表现优于传统的监督微调和知识蒸馏方法，并通过透明的解释在风格转移过程中有显著优势。

    

    文本风格转移（TST）旨在改变文本的风格同时保留其核心内容。鉴于TST的有限平行数据集的限制，我们提出了CoTeX，这是一个利用大型语言模型（LLMs）和思维链（CoT）提示来促进TST的框架。CoTeX将LLMs的复杂重写和推理能力提炼成更简化的模型，能够处理非平行数据和平行数据。通过在四个TST数据集上的实验，CoTeX显示出超越传统监督微调和知识蒸馏方法的能力，特别是在资源匮乏的情况下。我们进行了全面评估，将CoTeX与当前的无监督、监督、上下文学习（ICL）技术以及指导调整的LLMs进行了比较。此外，CoTeX通过提供透明的解释其风格转移过程而脱颖而出。

    arXiv:2403.01106v1 Announce Type: cross  Abstract: Text Style Transfer (TST) seeks to alter the style of text while retaining its core content. Given the constraints of limited parallel datasets for TST, we propose CoTeX, a framework that leverages large language models (LLMs) alongside chain-of-thought (CoT) prompting to facilitate TST. CoTeX distills the complex rewriting and reasoning capabilities of LLMs into more streamlined models capable of working with both non-parallel and parallel data. Through experimentation across four TST datasets, CoTeX is shown to surpass traditional supervised fine-tuning and knowledge distillation methods, particularly in low-resource settings. We conduct a comprehensive evaluation, comparing CoTeX against current unsupervised, supervised, in-context learning (ICL) techniques, and instruction-tuned LLMs. Furthermore, CoTeX distinguishes itself by offering transparent explanations for its style transfer process.
    
[^13]: PEM：基于原型的高效MaskFormer用于图像分割

    PEM: Prototype-based Efficient MaskFormer for Image Segmentation

    [https://arxiv.org/abs/2402.19422](https://arxiv.org/abs/2402.19422)

    PEM提出了基于原型的高效MaskFormer，通过引入原型交叉注意力和多尺度特征金字塔网络，实现了在多个分割任务中的高效运行。

    

    近期基于transformer的架构在图像分割领域展现出令人印象深刻的结果。由于其灵活性，它们在多个分割任务中获得出色的性能，如语义分割和全景分割，在一个统一的框架下。为了填补这一差距，我们提出了基于原型的高效MaskFormer（PEM），这是一个可以在多个分割任务中运行的高效transformer架构。PEM提出了一种新颖的基于原型的交叉注意力，利用视觉特征的冗余性来限制计算并提高效率而不损害性能。此外，PEM引入了一个高效的多尺度特征金字塔网络，能够提取具有高语义的特征。

    arXiv:2402.19422v1 Announce Type: cross  Abstract: Recent transformer-based architectures have shown impressive results in the field of image segmentation. Thanks to their flexibility, they obtain outstanding performance in multiple segmentation tasks, such as semantic and panoptic, under a single unified framework. To achieve such impressive performance, these architectures employ intensive operations and require substantial computational resources, which are often not available, especially on edge devices. To fill this gap, we propose Prototype-based Efficient MaskFormer (PEM), an efficient transformer-based architecture that can operate in multiple segmentation tasks. PEM proposes a novel prototype-based cross-attention which leverages the redundancy of visual features to restrict the computation and improve the efficiency without harming the performance. In addition, PEM introduces an efficient multi-scale feature pyramid network, capable of extracting features that have high seman
    
[^14]: 硅谷人群的智慧：LLM集成预测能力达到人群准确率水平

    Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy

    [https://arxiv.org/abs/2402.19379](https://arxiv.org/abs/2402.19379)

    该研究通过将十二个LLMs组成的LLM集成方法与925名人类预测者的群体预测进行比较，发现LLM群体优于简单的无信息基准，并在统计上等效于人类群体。

    

    实践中人类预测准确性依赖于“群体智慧”效应，即通过聚合一群个体预测者的预测可以显著提高对未来事件的预测。过去关于大型语言模型（LLMs）预测能力的研究表明，作为个体预测者的前沿LLMs表现不佳，与人类群体预测比赛的黄金标准相比。我们通过使用一个由十二个LLMs组成的LLM集成方法，扩展了研究。我们将31个二元问题的聚合LLM预测与一个来自三个月预测比赛的925名人类预测者的群体预测进行比较。我们的主要分析表明，LLM群体的表现优于简单的无信息基准，并在统计上等效于人类群体。我们还观察到一种顺从效应，平均模型预测明显高于50%，尽管几乎是平等的。

    arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
    
[^15]: REPLAY: 对稀疏轨迹进行位置预测的人类移动时间变化规律建模

    REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories

    [https://arxiv.org/abs/2402.16310](https://arxiv.org/abs/2402.16310)

    该论文提出了REPLAY模型，利用一般RNN架构来学习捕捉人类移动的时间变化规律，用于位置预测。

    

    位置预测是根据历史用户移动轨迹来预测用户位置的技术。为了应对真实世界用户移动轨迹的固有稀疏问题，时空上下文被证明是非常有用的。现有的解决方案主要是将位置之间的时空距离纳入到移动轨迹中，要么通过将其作为附加输入提供给递归神经网络（RNNs），要么通过利用它们来寻找有信息的过去隐藏状态进行预测。然而，这种基于距离的方法未能捕捉人类移动的时间变化规律，例如，人类移动在早晨通常比其他时间更有规律；这暗示了实际时间戳的有用性。基于这一背景，我们提出了REPLAY，是一种通用的RNN架构，旨在捕捉时间变化的人类移动时间规律以进行位置预测。

    arXiv:2402.16310v1 Announce Type: cross  Abstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, 
    
[^16]: 基于表情符号的加密资产市场反应

    Emoji Driven Crypto Assets Market Reactions

    [https://arxiv.org/abs/2402.10481](https://arxiv.org/abs/2402.10481)

    该研究利用GPT-4和BERT模型进行多模态情感分析，发现基于表情符号情绪的策略可以帮助避免市场下挫并稳定回报。

    

    在加密货币领域，诸如Twitter之类的社交媒体平台已经成为影响市场趋势和投资者情绪的关键因素。在我们的研究中，我们利用GPT-4和经过微调的基于BERT模型的多模态情感分析，重点关注表情符号情绪对加密货币市场的影响。通过将表情符号转化为可量化的情感数据，我们将这些见解与BTC价格和VCRIX指数等关键市场指标进行了相关联。这种方法可以用于开发旨在利用社交媒体元素识别和预测市场趋势的交易策略。关键是，我们的研究结果表明，基于表情符号情绪的策略可以有助于避免重大市场下挫，并有助于回报的稳定。这项研究强调了将先进的基于人工智能的分析整合到金融策略中的实际益处，并提供了一种新的方式来看待市场预测。

    arXiv:2402.10481v1 Announce Type: cross  Abstract: In the burgeoning realm of cryptocurrency, social media platforms like Twitter have become pivotal in influencing market trends and investor sentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based BERT model for a multimodal sentiment analysis, focusing on the impact of emoji sentiment on cryptocurrency markets. By translating emojis into quantifiable sentiment data, we correlate these insights with key market indicators like BTC Price and the VCRIX index. This approach may be fed into the development of trading strategies aimed at utilizing social media elements to identify and forecast market trends. Crucially, our findings suggest that strategies based on emoji sentiment can facilitate the avoidance of significant market downturns and contribute to the stabilization of returns. This research underscores the practical benefits of integrating advanced AI-driven analyses into financial strategies, offering a nuan
    
[^17]: 解决图上的负迁移问题

    Tackling Negative Transfer on Graphs

    [https://arxiv.org/abs/2402.08907](https://arxiv.org/abs/2402.08907)

    图迁移学习中的负迁移现象尚未得到充分研究，本文发现在图结构数据中负迁移普遍存在，即使源图和目标图在语义上相似。我们提出了一个新的观点，对于语义相似的图，结构差异对子图嵌入的影响较小。

    

    迁移学习旨在通过利用从其他相关任务中学到的知识来提高目标任务上的学习效果。然而，当源任务和目标任务之间关系不密切时，学习性能可能会受到不利影响，这种现象被称为负迁移。本文研究了在图迁移学习中的负迁移问题，这是一个重要但尚未深入研究的领域。我们发现，在图结构数据中，与图像或文本不同，负迁移经常发生，即使源图和目标图在语义上有相似之处。具体来说，我们发现结构差异会大大增强图中节点嵌入之间的差异。为了缓解这个问题，我们带来了一个新的观点：对于语义相似的图，尽管结构差异会导致节点嵌入的分布差异，但它们对子图嵌入的影响可能较小。基于这个观点，我们引入了tw

    arXiv:2402.08907v1 Announce Type: cross Abstract: Transfer learning aims to boost the learning on the target task leveraging knowledge learned from other relevant tasks. However, when the source and target are not closely related, the learning performance may be adversely affected, a phenomenon known as negative transfer. In this paper, we investigate the negative transfer in graph transfer learning, which is important yet underexplored. We reveal that, unlike image or text, negative transfer commonly occurs in graph-structured data, even when source and target graphs share semantic similarities. Specifically, we identify that structural differences significantly amplify the dissimilarities in the node embeddings across graphs. To mitigate this, we bring a new insight: for semantically similar graphs, although structural differences lead to significant distribution shift in node embeddings, their impact on subgraph embeddings could be marginal. Building on this insight, we introduce tw
    
[^18]: 生成带有病人时间轴的电子健康记录的CEHR-GPT

    CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines

    [https://arxiv.org/abs/2402.04400](https://arxiv.org/abs/2402.04400)

    CEHR-GPT是一种使用病人时间轴生成电子健康记录的方法，能够处理EHR数据的合成、疾病进展分析等应用。

    

    合成电子健康记录（EHR）已成为推进医疗应用和机器学习模型的关键工具，特别是对于没有直接访问医疗数据的研究人员而言。尽管现有方法，如基于规则的方法和生成对抗网络（GAN），可以生成类似真实世界EHR数据的合成数据，但这些方法常常使用表格格式，忽略了病人历史的时间依赖性，限制了数据复制。最近，越来越多的人开始利用生成预训练转换器（GPT）来处理EHR数据。这使得可以进行疾病进展分析、人口估计、反事实推理和合成数据生成等应用。本研究关注合成数据生成，并演示了使用源自CEHR-BERT的特定病人表示训练GPT模型的能力，从而能够生成可无缝转换的病人序列。

    Synthetic Electronic Health Records (EHR) have emerged as a pivotal tool in advancing healthcare applications and machine learning models, particularly for researchers without direct access to healthcare data. Although existing methods, like rule-based approaches and generative adversarial networks (GANs), generate synthetic data that resembles real-world EHR data, these methods often use a tabular format, disregarding temporal dependencies in patient histories and limiting data replication. Recently, there has been a growing interest in leveraging Generative Pre-trained Transformers (GPT) for EHR data. This enables applications like disease progression analysis, population estimation, counterfactual reasoning, and synthetic data generation. In this work, we focus on synthetic data generation and demonstrate the capability of training a GPT model using a particular patient representation derived from CEHR-BERT, enabling us to generate patient sequences that can be seamlessly converted 
    
[^19]: 不带位置编码的图变压器

    Graph Transformers without Positional Encodings

    [https://arxiv.org/abs/2401.17791](https://arxiv.org/abs/2401.17791)

    本文介绍了一种不需要位置编码的图变压器模型，该模型通过注意机制本身包含图结构信息，并通过实验证明了其有效性。

    

    最近，用于图表示学习的变压器越来越受欢迎，在各种数据集上取得了最先进的性能，无论是单独使用还是与消息传递图神经网络（MP-GNN）结合。将图归纳偏见融入天然与结构无关的变压器架构中，以结构或位置编码（PEs）的形式，是实现这些令人印象深刻的结果的关键。然而，设计这样的编码是棘手的，人们已经提出了不同的尝试来设计这样的编码，包括拉普拉斯特征向量、相对随机行走概率（RRWP）、空间编码、中心度编码、边缘编码等。在这项工作中，我们认为这些编码可能根本不需要，只要注意机制本身包含有关图结构的信息。我们介绍了Eigenformer，它使用一种新颖的谱感知注意机制，了解图的拉普拉斯谱，并通过实验证明

    Recently, Transformers for graph representation learning have become increasingly popular, achieving state-of-the-art performance on a wide-variety of datasets, either alone or in combination with message-passing graph neural networks (MP-GNNs). Infusing graph inductive-biases in the innately structure-agnostic transformer architecture in the form of structural or positional encodings (PEs) is key to achieving these impressive results. However, designing such encodings is tricky and disparate attempts have been made to engineer such encodings including Laplacian eigenvectors, relative random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge encodings etc. In this work, we argue that such encodings may not be required at all, provided the attention mechanism itself incorporates information about the graph structure. We introduce Eigenformer, which uses a novel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of the graph, and empirically show
    
[^20]: 模仿引导式强化学习

    Imitation Bootstrapped Reinforcement Learning

    [https://arxiv.org/abs/2311.02198](https://arxiv.org/abs/2311.02198)

    提出了一种模仿引导式强化学习（IBRL）的框架，用于高效的样本-efficient RL，通过先在提供的示范上训练IL策略，然后使用它提出替代动作进行在线探索和引导目标值。

    

    尽管强化学习（RL）具有相当大的潜力，但机器人控制任务主要依赖模仿学习（IL）是因为其更好的样本效率。然而，收集能使IL推广到所有可能场景的全面专家演示是昂贵的，任何分布的转变都需要重新收集数据进行微调。因此，如果RL可以建立在IL的基础上作为一种高效的自我改进程序，那么它将具有吸引力。我们提出了一种模仿引导式强化学习（IBRL）的新框架，用于具有示范的高效抽样RL，首先在提供的示范上训练IL策略，然后使用它提出替代动作进行在线探索和引导目标值。与先前过度采样示范或用额外的模仿损失对RL进行正则化的工作相比，IBRL能够利用来自IL的高质量动作。

    arXiv:2311.02198v4 Announce Type: replace-cross  Abstract: Despite the considerable potential of reinforcement learning (RL), robotic control tasks predominantly rely on imitation learning (IL) due to its better sample efficiency. However, it is costly to collect comprehensive expert demonstrations that enable IL to generalize to all possible scenarios, and any distribution shift would require recollecting data for finetuning. Therefore, RL is appealing if it can build upon IL as an efficient autonomous self-improvement procedure. We propose imitation bootstrapped reinforcement learning (IBRL), a novel framework for sample-efficient RL with demonstrations that first trains an IL policy on the provided demonstrations and then uses it to propose alternative actions for both online exploration and bootstrapping target values. Compared to prior works that oversample the demonstrations or regularize RL with an additional imitation loss, IBRL is able to utilize high quality actions from IL p
    
[^21]: 将子图转化为节点让简单的图神经网络在子图表示学习上更强大和高效

    Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning

    [https://arxiv.org/abs/2204.04510](https://arxiv.org/abs/2204.04510)

    提出了一种将子图转化为节点的方法来学习子图的表示，该方法不仅显著降低了内存和计算成本，还捕捉了子图的局部和全局结构，并在多个基准测试上表现出色。

    

    子图表示学习已经成为一个重要的问题，并且通常使用专门的图神经网络来处理大型全局图。这些模型需要大量的内存和计算资源，但挑战子图的层次结构建模。在本文中，我们提出了子图到节点（S2N）转换的新颖公式，用于学习子图的表示。具体而言，给定全局图中的一组子图，我们通过粗略地将子图转换成节点来构建一个新的图。通过理论和实证证据，S2N不仅相比最先进的模型显著减少了内存和计算成本，而且通过捕捉子图的局部和全局结构也在性能上超过了它们。通过利用图粗化方法，我们的方法甚至在数据稀缺的情况下也优于基线模型。我们在八个基准测试上的实验表明，调整模型后效果出色。

    Subgraph representation learning has emerged as an important problem, but it is by default approached with specialized graph neural networks on a large global graph. These models demand extensive memory and computational resources but challenge modeling hierarchical structures of subgraphs. In this paper, we propose Subgraph-To-Node (S2N) translation, a novel formulation for learning representations of subgraphs. Specifically, given a set of subgraphs in the global graph, we construct a new graph by coarsely transforming subgraphs into nodes. Demonstrating both theoretical and empirical evidence, S2N not only significantly reduces memory and computational costs compared to state-of-the-art models but also outperforms them by capturing both local and global structures of the subgraph. By leveraging graph coarsening methods, our method outperforms baselines even in a data-scarce setting with insufficient subgraphs. Our experiments on eight benchmarks demonstrate that fined-tuned models w
    
[^22]: DiffClone: 使用扩散驱动的策略学习增强机器人行为克隆

    DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven Policy Learning. (arXiv:2401.09243v1 [cs.RO])

    [http://arxiv.org/abs/2401.09243](http://arxiv.org/abs/2401.09243)

    本文介绍了DiffClone，一种通过扩散驱动的策略学习增强行为克隆代理的离线算法。在真实的在线物理机器人上的实验表明，采用MOCO微调的ResNet50的效果最好。

    

    机器人学习任务在计算上非常密集且硬件特定。因此，通过使用多样化的离线演示数据集来训练机器人操作代理，来应对这些挑战的方式非常吸引人。Train-Offline-Test-Online（TOTO）基准提供了一个经过精心策划的开源离线训练数据集，主要由专家数据组成，并提供了常见离线强化学习和行为克隆代理的基准分数。在本文中，我们介绍了DiffClone，一种增强行为克隆代理的离线算法，采用基于扩散的策略学习，并在测试时在真实的在线物理机器人上评估了我们的方法的有效性。同时，这也是我们在NeurIPS 2023举办的Train-Offline-Test-Online（TOTO）基准挑战赛中的官方提交。我们尝试了预训练的视觉表示和代理策略。在实验中，我们发现MOCO微调的ResNet50相比其他微调方法表现最好。

    Robot learning tasks are extremely compute-intensive and hardware-specific. Thus the avenues of tackling these challenges, using a diverse dataset of offline demonstrations that can be used to train robot manipulation agents, is very appealing. The Train-Offline-Test-Online (TOTO) Benchmark provides a well-curated open-source dataset for offline training comprised mostly of expert data and also benchmark scores of the common offline-RL and behaviour cloning agents. In this paper, we introduce DiffClone, an offline algorithm of enhanced behaviour cloning agent with diffusion-based policy learning, and measured the efficacy of our method on real online physical robots at test time. This is also our official submission to the Train-Offline-Test-Online (TOTO) Benchmark Challenge organized at NeurIPS 2023. We experimented with both pre-trained visual representation and agent policies. In our experiments, we find that MOCO finetuned ResNet50 performs the best in comparison to other finetuned
    
[^23]: FMGS：基于嵌入式3D高斯分割的全面3D场景理解

    FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding. (arXiv:2401.01970v1 [cs.CV])

    [http://arxiv.org/abs/2401.01970](http://arxiv.org/abs/2401.01970)

    提出了一种新颖的方法，将视觉语言嵌入基础模型到3D高斯分割中，实现了高质量的3D场景理解。

    

    准确地感知现实世界3D物体的几何和语义特性对于增强现实和机器人应用的持续进化至关重要。为此，我们提出了FMGS（Foundation Model Embedded 3D Gaussian Splatting），将视觉语言嵌入基础模型到3D高斯分割中。本工作的主要贡献是一种高效的方法，用于重建和表示3D视觉语言模型。这是通过将基于图像的基础模型生成的特征图融合到我们的3D模型中渲染实现的。为了确保高质量的渲染和快速训练，我们引入了一种新颖的场景表示方法，将GS和多分辨率哈希编码（MHE）的优势结合起来。我们的有效训练过程还引入了像素对齐损失，使相同语义实体的渲染特征距离接近，遵循像素级语义边界。我们的结果展示了显著的多视图语义一致性。

    Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present \algfull{} (\algname{}), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, faci
    
[^24]: 时间中的涟漪：美国历史中的不连续性

    A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.01185](http://arxiv.org/abs/2312.01185)

    该论文通过使用向量嵌入和非线性降维方法，发现GPT-2与UMAP的结合可以提供更好的分离和聚类效果。同时，经过微调的DistilBERT模型可用于识别总统和演讲的年份。

    

    在这篇论文中，我们使用来自Kaggle的国情咨文数据集对美国历史的总体时间线及咨文本身的特点和性质进行了一些令人惊讶（也有些不那么令人惊讶）的观察。我们的主要方法是使用向量嵌入，如BERT（DistilBERT）和GPT-2。虽然广泛认为BERT（及其变体）最适合NLP分类任务，但我们发现GPT-2结合UMAP等非线性降维方法可以提供更好的分离和更强的聚类效果。这使得GPT-2 + UMAP成为一个有趣的替代方案。在我们的情况下，不需要对模型进行微调，预训练的GPT-2模型就足够好用。我们还使用了经过微调的DistilBERT模型来检测哪位总统发表了哪篇演讲，并取得了非常好的结果（准确率为93\% - 95\%，具体取决于运行情况）。为了确定写作年份，我们还执行了一个类似的任务。

    In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
    
[^25]: 语言模型如何将实体绑定到上下文中?

    How do Language Models Bind Entities in Context?. (arXiv:2310.17191v1 [cs.LG])

    [http://arxiv.org/abs/2310.17191](http://arxiv.org/abs/2310.17191)

    通过分析语言模型的表示，我们发现了绑定ID机制，它可以将实体与属性进行有效地绑定。我们通过因果干预实验进一步证明了语言模型内部激活表示绑定信息的方式。研究结果揭示了语言模型在上下文中如何表示符号知识，从而为理解大规模语言模型的一般上下文推理提供了指导。

    

    为了正确使用上下文信息，语言模型（LMs）必须将实体与其属性进行绑定。例如，给定描述“绿色方块”和“蓝色圆形”的上下文，LMs必须将形状与它们对应的颜色进行绑定。我们分析LM表示并确定绑定ID机制：这是一种解决绑定问题的通用机制，我们在Pythia和LLaMA家族的每个足够大的模型中观察到。通过因果干预，我们展示了LMs内部激活通过将绑定ID向量附加到相应的实体和属性上来表示绑定信息。我们进一步展示了绑定ID向量形成连续的子空间，在这个子空间中，绑定ID向量之间的距离反映了它们的区别。总体而言，我们的结果揭示了LMs在上下文中表示符号知识的可解释策略，为理解大规模LMs中的一般上下文推理迈出了一步。

    To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a "green square" and a "blue circle", LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.
    
[^26]: KirchhoffNet：一种连接消息传递和连续深度模型的电路桥接神经网络

    KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models. (arXiv:2310.15872v1 [cs.LG])

    [http://arxiv.org/abs/2310.15872](http://arxiv.org/abs/2310.15872)

    本文提出了一种称为基赫霍夫网络的神经网络模型，利用基赫霍夫电流定律与消息传递神经网络和连续深度网络建立连接。在MNIST数据集上，基赫霍夫网络可以实现接近98.86%的测试准确度，且具有在硬件上实现的潜力。无论网络参数数量如何，其正向计算都可以在1/f秒内完成，具有快速计算的硬件特性。

    

    在本文中，我们利用了模拟电路的基本原理基赫霍夫电流定律，引入了一类独特的神经网络模型，称为基赫霍夫网络。基赫霍夫网络与消息传递神经网络和连续深度网络建立了密切联系。我们证明，即使在没有任何传统层（如卷积、池化或线性层）的情况下，基赫霍夫网络在MNIST数据集上取得了98.86%的测试准确度，与最先进的结果相当。让基赫霍夫网络更加有趣的是其在硬件领域的潜力。当代深度神经网络通常部署在GPU上。相反，基赫霍夫网络可以通过模拟电路来实现。此外，我们证明了无论在基赫霍夫网络内有多少参数，其正向计算都可以在1/f秒内完成，其中f表示硬件的时钟频率。这种特性表明，基赫霍夫网络具有潜力实现快速计算的硬件。

    In this paper, we exploit a fundamental principle of analog electronic circuitry, Kirchhoff's current law, to introduce a unique class of neural network models that we refer to as KirchhoffNet. KirchhoffNet establishes close connections with message passing neural networks and continuous-depth networks. We demonstrate that even in the absence of any traditional layers (such as convolution, pooling, or linear layers), KirchhoffNet attains 98.86% test accuracy on the MNIST dataset, comparable with state of the art (SOTA) results. What makes KirchhoffNet more intriguing is its potential in the realm of hardware. Contemporary deep neural networks are conventionally deployed on GPUs. In contrast, KirchhoffNet can be physically realized by an analog electronic circuit. Moreover, we justify that irrespective of the number of parameters within a KirchhoffNet, its forward calculation can always be completed within 1/f seconds, with f representing the hardware's clock frequency. This characteris
    
[^27]: 在上下文中的学生建模中使用大型语言模型：从一次性观察中合成视觉编程中学生的行为

    Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming from One-Shot Observation. (arXiv:2310.10690v1 [cs.CL])

    [http://arxiv.org/abs/2310.10690](http://arxiv.org/abs/2310.10690)

    本研究探索在开放式学习环境中使用大型语言模型进行上下文学生建模，提出了一个新的框架LLM-SS，通过合成学生在不同任务上的尝试，为学生建模提供更准确的预测和教学策略。

    

    学生建模对于许多教育技术来说至关重要，因为它可以预测未来的学习结果和有针对性的教学策略。然而，开放式学习环境会带来挑战，因为学生表现出多样化的行为且缺乏明确定义的学习技能集。为了应对这些挑战，我们探索在开放式学习环境中应用大型语言模型（LLMs）进行上下文学生建模。我们引入了一个新颖的框架LLM-SS，利用LLMs合成学生的行为。具体而言，给定一个特定学生在参考任务上的解决尝试作为观察，目标是合成该学生在目标任务上的尝试。我们的框架可以与不同的LLMs结合使用；而且，我们使用领域专家知识对LLMs进行微调，提高它们对领域背景和学生行为的理解。我们评估了几种具体的方法...

    Student modeling is central to many educational technologies as it enables the prediction of future learning outcomes and targeted instructional strategies. However, open-ended learning environments pose challenges for accurately modeling students due to the diverse behaviors exhibited by students and the absence of a well-defined set of learning skills. To approach these challenges, we explore the application of Large Language Models (LLMs) for in-context student modeling in open-ended learning environments. We introduce a novel framework, LLM-SS, that leverages LLMs for synthesizing student's behavior. More concretely, given a particular student's solving attempt on a reference task as observation, the goal is to synthesize the student's attempt on a target task. Our framework can be combined with different LLMs; moreover, we fine-tune LLMs using domain-specific expertise to boost their understanding of domain background and student behaviors. We evaluate several concrete methods bas
    
[^28]: 超越记忆：通过大型语言模型进行推理来侵犯隐私

    Beyond Memorization: Violating Privacy Via Inference with Large Language Models. (arXiv:2310.07298v1 [cs.AI])

    [http://arxiv.org/abs/2310.07298](http://arxiv.org/abs/2310.07298)

    该论文首次全面研究了预训练大型语言模型从文本中推断个人属性的能力，发现当前的模型可以以较低的成本和时间比例，准确地推断出多种个人属性，这引发了隐私泄露的新威胁。

    

    目前关于大型语言模型（LLMs）的隐私研究主要集中在提取记忆训练数据的问题上。同时，模型的推理能力已大幅增强。这引发了一个关键问题，即当前的LLMs是否能够通过从推理时给出的文本中推断个人属性来侵犯个人隐私。在这项工作中，我们首次对预训练LLMs从文本中推断个人属性的能力进行了全面研究。我们构建了一个包含真实Reddit个人资料的数据集，并且显示当前的LLMs可以推断出各种各样的个人属性（例如，位置、收入、性别），在成本（100倍）和时间（240倍）上仅需人类的一小部分，达到了最高1的准确率达到85％，最高3的准确率达到95.8％。随着人们越来越多地与由LLM驱动的聊天机器人在生活的各个方面进行互动，我们还探讨了侵犯隐私的聊天机器人通过似乎无关的对话试图提取个人信息的新威胁。

    Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\%$ top-1 and $95.8\%$ top-3 accuracy at a fraction of the cost ($100\times$) and time ($240\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemi
    
[^29]: 只需少量上下文示范即可实现越狱和对齐的语言模型

    Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations. (arXiv:2310.06387v1 [cs.LG])

    [http://arxiv.org/abs/2310.06387](http://arxiv.org/abs/2310.06387)

    本文研究了使用少量上下文示范来操纵语言模型对齐能力的方法。通过提供示范，而无需微调，可以增加或降低模型回答恶意提示的概率。我们提出了相同上下文攻击和相同上下文防御方法，用于越狱和对齐语言模型。实验证明了这些方法的有效性。

    

    大规模语言模型（LLM）在各种任务中取得了显著的成功，但对其安全性和生成恶意内容的潜在风险的担忧也浮现出来。本文探讨了在相同上下文学习（ICL）中操纵LLM对齐能力的效果。我们发现，仅通过少量的上下文示范而无需微调，就可以操纵LLM增加或降低越狱概率，即回答恶意提示。基于这些观察，我们提出了用于越狱和对齐语言模型目的的相同上下文攻击（ICA）和相同上下文防御（ICD）方法。ICA通过构造恶意上下文指导模型生成有害输出，而ICD通过拒绝回答有害提示的示范来增强模型的稳健性。我们的实验证明了ICA和ICD在增加或降低对抗越狱攻击成功率方面的有效性。总的来说，我们揭示了ICL在越狱和对齐语言模型领域的潜力。

    Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL t
    
[^30]: 使检索增强的语言模型对无关上下文具有鲁棒性

    Making Retrieval-Augmented Language Models Robust to Irrelevant Context. (arXiv:2310.01558v1 [cs.CL])

    [http://arxiv.org/abs/2310.01558](http://arxiv.org/abs/2310.01558)

    本文分析了检索增强的语言模型在开放领域问答中的性能问题，并提出了基于自然语言推理的方法来缓解这个问题。

    

    检索增强的语言模型（RALMs）有望产生准确、高效和最新的语言理解系统。RALMs的一个重要目标是在相关时提高模型性能，在不相关时不影响性能。这在多跳推理场景中尤为重要，因为不相关证据的误用会导致连锁错误。然而，最近的研究表明，检索增强有时会对性能产生负面影响。在这项工作中，我们对五个开放领域的问答基准进行了彻底分析，描述了检索降低准确性的情况。然后，我们提出了两种缓解这个问题的方法。首先，一个简单的基准线，根据自然语言推理（NLI）模型筛选出不涉及问题-答案对的检索段落。这可以有效防止性能下降，但代价是舍弃了相关信息。

    Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding releva
    
[^31]: RA-DIT: 检索增强的双重指令调优

    RA-DIT: Retrieval-Augmented Dual Instruction Tuning. (arXiv:2310.01352v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.01352](http://arxiv.org/abs/2310.01352)

    本论文介绍了一种轻量级的微调方法RA-DIT，通过为任何语言模型添加检索能力来提高性能。该方法分为两个步骤：一是更新语言模型以更好地利用检索到的信息，二是更新检索器以返回更相关的结果。实验证明每个步骤都能显著提高性能，同时使用两个步骤可以获得额外的收益。

    

    检索增强语言模型（RALMs）通过访问外部数据存储中的长尾和最新知识来提高性能，但构建起来具有挑战性。现有的方法要么需要昂贵的检索特定修改来进行语言模型预训练，要么使用事后集成数据存储的方法，导致性能不理想。我们引入了一种轻量级的微调方法——检索增强的双重指令调优（RA-DIT），通过为任何语言模型添加检索能力来实现。我们的方法分为两个不同的微调步骤：（1）一个更新预训练的语言模型以更好地利用检索到的信息，（2）另一个更新检索器以返回更相关的结果，符合语言模型的偏好。通过在需要知识利用和上下文意识的任务上进行微调，我们证明了每个阶段都能显著提高性能，并且同时使用两个阶段可以获得额外的收益。我们的最佳模型是RA-DIT 65B。

    Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B,
    
[^32]: 指向因果基础模型: 因果推断与注意力的对偶关系

    Towards Causal Foundation Model: on Duality between Causal Inference and Attention. (arXiv:2310.00809v1 [cs.LG])

    [http://arxiv.org/abs/2310.00809](http://arxiv.org/abs/2310.00809)

    该论文提出了一种名为Causal Inference with Attention (CInA)的新方法，利用因果推断和注意力的对偶关系，在复杂任务中实现了零样本的因果推断。

    

    基于因果推断和注意力之间的对偶连接，我们提出了一种名为Causal Inference with Attention (CInA)的理论上完备的方法，利用多个无标签数据集进行自监督因果学习，并在新数据的未见任务上实现零样本因果推断。我们的实证结果表明了我们的方法在复杂任务中的有效性。

    Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for complex tasks. We propose a novel, theoretically sound method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that our approach
    
[^33]: LLM基于视频扩散模型

    LLM-grounded Video Diffusion Models. (arXiv:2309.17444v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.17444](http://arxiv.org/abs/2309.17444)

    使用LLM-grounded Video Diffusion (LVD)模型，通过先生成动态场景布局，再通过这些布局指导视频生成的扩散模型，解决了当前模型在复杂的时空提示和不正确的运动生成方面的困难。

    

    文字条件下的扩散模型已经成为神经视频生成的一个有希望的工具。然而，目前的模型仍然在复杂的时空提示方面存在困难，通常生成受限制或不正确的运动（例如，甚至缺乏从左向右移动的物体的提示能力）。为了解决这些限制，我们引入了LLM基于视频扩散（LVD）。LVD不直接从文本输入中生成视频，而是首先利用大型语言模型（LLM）根据文本输入生成动态场景布局，然后使用生成的布局来指导视频生成的扩散模型。我们展示了LLM能够从单纯的文本中理解复杂的时空动态，并生成与实际世界中通常观察到的提示和物体运动模式密切对齐的布局。然后，我们提出通过调整注意力图来指导视频扩散模型与这些布局进行交互。我们的方法无需训练。

    Text-conditioned diffusion models have emerged as a promising tool for neural video generation. However, current models still struggle with intricate spatiotemporal prompts and often generate restricted or incorrect motion (e.g., even lacking the ability to be prompted for objects moving from left to right). To address these limitations, we introduce LLM-grounded Video Diffusion (LVD). Instead of directly generating videos from the text inputs, LVD first leverages a large language model (LLM) to generate dynamic scene layouts based on the text inputs and subsequently uses the generated layouts to guide a diffusion model for video generation. We show that LLMs are able to understand complex spatiotemporal dynamics from text alone and generate layouts that align closely with both the prompts and the object motion patterns typically observed in the real world. We then propose to guide video diffusion models with these layouts by adjusting the attention maps. Our approach is training-free 
    
[^34]: PLMM：移动设备上的个人大型模型

    PLMM: Personal Large Models on Mobile Devices. (arXiv:2309.14726v1 [cs.CV])

    [http://arxiv.org/abs/2309.14726](http://arxiv.org/abs/2309.14726)

    本文提出了一种从传统大型语言模型中提取的个人大型模型，该模型更适应于本地用户的个人信息，并且能够保护用户的隐私。该模型分为个人级别、专家级别和传统级别，同时还需要小型化以适应个人计算机或移动设备，并实现实时响应以提供更好的用户体验。

    

    在本文中，受到联邦学习的启发，我们提出了从传统大型语言模型中提取的个人大型模型，这些模型更适应本地用户的个人信息，如教育背景和爱好。我们将大型语言模型分为三个级别：个人级别，专家级别和传统级别。个人级别模型适应用户的个人信息，对用户的输入进行加密并保护其隐私。专家级别模型专注于合并特定领域的知识，如金融、IT和艺术。传统模型专注于普遍知识的发现和提升专家模型。在这样的分类中，个人模型直接与用户交互。对于整个系统来说，个人模型具有用户的（加密的）个人信息。此外，这些模型必须足够小以在个人计算机或移动设备上运行。最后，它们还必须实时响应，以提供更好的用户体验。

    Inspired by Federated Learning, in this paper, we propose personal large models that are distilled from traditional large language models but more adaptive to local users' personal information such as education background and hobbies. We classify the large language models into three levels: the personal level, expert level and traditional level. The personal level models are adaptive to users' personal information. They encrypt the users' input and protect their privacy. The expert level models focus on merging specific knowledge such as finance, IT and art. The traditional models focus on the universal knowledge discovery and upgrading the expert models. In such classifications, the personal models directly interact with the user. For the whole system, the personal models have users' (encrypted) personal information. Moreover, such models must be small enough to be performed on personal computers or mobile devices. Finally, they also have to response in real-time for better user exper
    
[^35]: 无人机群体的碰撞回避和导航方法：基于端到端深度强化学习

    Collision Avoidance and Navigation for a Quadrotor Swarm Using End-to-end Deep Reinforcement Learning. (arXiv:2309.13285v1 [cs.RO])

    [http://arxiv.org/abs/2309.13285](http://arxiv.org/abs/2309.13285)

    这项研究提出了一种基于端到端深度强化学习的方法，用于在带有障碍物的环境中控制无人机群体，并通过课程学习和回放缓冲区提高性能。此外，还实施了注意力机制以关注邻近机器人和障碍物之间的相互作用。

    

    端到端深度强化学习(DRL)的无人机控制方法具有易于部署、任务泛化和实时执行能力等优点。然而，以往的端到端DRL方法主要用于在简单、无障碍环境中训练单个无人机或无人机团队的控制器，并没有考虑障碍物对训练RL策略的困难性增加造成的挑战。本文提出了一种在带有障碍物环境中控制无人机群体的端到端DRL方法。我们为智能体提供了一个课程（curriculum）和一个回放缓冲区（replay buffer），用于改善在充满障碍物的环境中的性能。此外，我们还实施了一个注意力机制，以关注邻近机器人和障碍物之间的相互作用 - 这是首次成功地在严重计算受限的硬件上部署的群体行为策略中应用此机制。

    End-to-end deep reinforcement learning (DRL) for quadrotor control promises many benefits -- easy deployment, task generalization and real-time execution capability. Prior end-to-end DRL-based methods have showcased the ability to deploy learned controllers onto single quadrotors or quadrotor teams maneuvering in simple, obstacle-free environments. However, the addition of obstacles increases the number of possible interactions exponentially, thereby increasing the difficulty of training RL policies. In this work, we propose an end-to-end DRL approach to control quadrotor swarms in environments with obstacles. We provide our agents a curriculum and a replay buffer of the clipped collision episodes to improve performance in obstacle-rich environments. We implement an attention mechanism to attend to the neighbor robots and obstacle interactions - the first successful demonstration of this mechanism on policies for swarm behavior deployed on severely compute-constrained hardware. Our wor
    
[^36]: 通过跨域序列自编码器实现反事实公平性追求

    Pursuing Counterfactual Fairness via Sequential Autoencoder Across Domains. (arXiv:2309.13005v1 [cs.LG])

    [http://arxiv.org/abs/2309.13005](http://arxiv.org/abs/2309.13005)

    通过顺序自编码器实现反事实公平性追求的创新框架将环境信息和敏感属性与分类特征的嵌入表示分开，以提高模型在不同和陌生领域中的泛化能力，并解决公平问题。

    

    鉴于领域转移在机器学习中普遍存在的挑战，已经开发出各种领域泛化（DG）技术，以提高处理分布外（OOD）数据时机器学习系统的性能。此外，在实际场景中，数据分布可能会在连续的序列领域中逐渐变化。虽然当前的方法主要集中在改进在这些新领域内的模型效果，但往往忽视了学习过程中的公平问题。为此，我们引入了一种创新的框架，称为具有顺序自编码器的反事实公平性感知领域泛化（CDSAE）。这种方法有效地将环境信息和敏感属性与分类特征的嵌入表示分开。这种并行分离不仅极大地提高了模型在不同和陌生的领域中的泛化能力，还有效地解决了相关挑战。

    Recognizing the prevalence of domain shift as a common challenge in machine learning, various domain generalization (DG) techniques have been developed to enhance the performance of machine learning systems when dealing with out-of-distribution (OOD) data. Furthermore, in real-world scenarios, data distributions can gradually change across a sequence of sequential domains. While current methodologies primarily focus on improving model effectiveness within these new domains, they often overlook fairness issues throughout the learning process. In response, we introduce an innovative framework called Counterfactual Fairness-Aware Domain Generalization with Sequential Autoencoder (CDSAE). This approach effectively separates environmental information and sensitive attributes from the embedded representation of classification features. This concurrent separation not only greatly improves model generalization across diverse and unfamiliar domains but also effectively addresses challenges rela
    
[^37]: 多副本强化学习智能体

    Multicopy Reinforcement Learning Agents. (arXiv:2309.10908v1 [cs.MA])

    [http://arxiv.org/abs/2309.10908](http://arxiv.org/abs/2309.10908)

    本文研究了一种新型的多智能体问题，在这个问题中，智能体制作多个相同副本来更好地完成任务。通过利用价值函数的结构，提出了一种学习算法来平衡添加额外副本的优势和成本。

    

    本文研究了一种新型的多智能体问题，其中一个智能体通过制作多个相同副本来更好或更高效地完成单个智能体任务。如果环境嘈杂，并且单个智能体副本有时无法完成任务，则这种策略可以提高性能。我们提出了一种用于解决多副本问题的学习算法，该算法利用价值函数的结构，有效地学习如何平衡添加额外副本的优势和成本。

    This paper examines a novel type of multi-agent problem, in which an agent makes multiple identical copies of itself in order to achieve a single agent task better or more efficiently. This strategy improves performance if the environment is noisy and the task is sometimes unachievable by a single agent copy. We propose a learning algorithm for this multicopy problem which takes advantage of the structure of the value function to efficiently learn how to balance the advantages and costs of adding additional copies.
    
[^38]: 多对象图形可用性网络：通过复合对象可用性实现目标导向规划

    Multi-Object Graph Affordance Network: Enabling Goal-Oriented Planning through Compound Object Affordances. (arXiv:2309.10426v1 [cs.RO])

    [http://arxiv.org/abs/2309.10426](http://arxiv.org/abs/2309.10426)

    多对象图形可用性网络（MOGAN）模型化了复合对象的可用性，预测了将新对象放置在复合对象上的效果。在构建具有特定高度或属性的塔等任务中，我们使用基于搜索的规划找到具有适当可用性的对象堆叠操作的序列。我们的系统能够正确建模非常复杂的复合对象的可用性，并在模拟和真实环境中展示了其适用性。

    

    学习对象的可用性是机器人学习领域的有效工具。虽然数据驱动的模型深入探讨了单个或成对对象的可用性，但在调查由复杂形状的任意数量的对象组成的复合对象的可用性方面存在明显差距。在本研究中，我们提出了多对象图形可用性网络（MOGAN），它建模了复合对象的可用性，并预测将新对象放置在现有复合对象上的效果。给定不同的任务，例如构建具有特定高度或属性的塔，我们使用基于搜索的规划找到具有适当可用性的对象堆叠操作的序列。我们展示了我们的系统能够正确建模包括堆叠的球体和杯子、杆和包围杆的环等非常复杂的复合对象的可用性。我们在模拟和真实环境中展示了我们系统的适用性。

    Learning object affordances is an effective tool in the field of robot learning. While the data-driven models delve into the exploration of affordances of single or paired objects, there is a notable gap in the investigation of affordances of compound objects that are composed of an arbitrary number of objects with complex shapes. In this study, we propose Multi-Object Graph Affordance Network (MOGAN) that models compound object affordances and predicts the effect of placing new objects on top of the existing compound. Given different tasks, such as building towers of specific heights or properties, we used a search based planning to find the sequence of stack actions with the objects of suitable affordances. We showed that our system was able to correctly model the affordances of very complex compound objects that include stacked spheres and cups, poles, and rings that enclose the poles. We demonstrated the applicability of our system in both simulated and real-world environments, com
    
[^39]: 不仅仅奖励，还有约束：用于腿式机器人运动的应用

    Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion. (arXiv:2308.12517v1 [cs.RO])

    [http://arxiv.org/abs/2308.12517](http://arxiv.org/abs/2308.12517)

    本文提出了一种新的强化学习框架，为复杂机器人系统训练神经网络控制器。该框架引入了奖励和约束的概念，通过设计高效的策略优化算法来处理约束，以减少计算开销。通过应用于不同腿式机器人的运动控制器训练中，展示了该框架的有效性。

    

    早期的一些研究通过设计神经网络控制器并使用无模型强化学习来训练，展示了复杂机器人系统中令人印象深刻的控制性能。然而，这些具有自然动作风格和高任务性能的出色控制器是通过进行大量奖励工程而开发的，该过程非常费时费力，需要设计大量奖励项并确定合适的奖励系数。在这项工作中，我们提出了一种新的强化学习框架，用于训练同时包含奖励和约束的神经网络控制器。为了让工程师能够适当地反映他们对约束的意图并以最小的计算开销处理它们，我们提出了两种约束类型和一种高效的策略优化算法。该学习框架被应用于训练不同形态和物理属性的几个腿式机器人的运动控制器。

    Several earlier studies have shown impressive control performance in complex robotic systems by designing the controller using a neural network and training it with model-free reinforcement learning. However, these outstanding controllers with natural motion style and high task performance are developed through extensive reward engineering, which is a highly laborious and time-consuming process of designing numerous reward terms and determining suitable reward coefficients. In this work, we propose a novel reinforcement learning framework for training neural network controllers for complex robotic systems consisting of both rewards and constraints. To let the engineers appropriately reflect their intent to constraints and handle them with minimal computation overhead, two constraint types and an efficient policy optimization algorithm are suggested. The learning framework is applied to train locomotion controllers for several legged robots with different morphology and physical attribu
    
[^40]: 动态开放词汇增强的智能安全着陆（DOVESEI）

    Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI). (arXiv:2308.11471v1 [cs.RO])

    [http://arxiv.org/abs/2308.11471](http://arxiv.org/abs/2308.11471)

    本文提出了一种动态开放词汇增强的智能安全着陆系统，通过利用开放词汇图像分割的能力实现无人机的视觉伺服，适应不同场景且无需大量数据积累进行模型改进，可以处理100米高度的操作。

    

    本研究针对城市空中机器人的基础步骤之一，即安全着陆。我们关注安全着陆感知堆栈中最关键的方面之一，即分割。我们提出了一种简化的反应式无人机系统，利用开放词汇图像分割的能力实现视觉伺服。这种方法可以适应各种场景，并通过其开放词汇方法，最小化调整需求，绕过对内部模型进行大量数据积累以进行改进的必要性。考虑到当地当局的限制，我们的主要关注点是从100米高度起飞的操作。这个选择是有意的，因为许多之前的工作处理的高度仅限于30米，与小型立体相机的能力相吻合。因此，我们采用传统的三维路径规划方法来导航剩下的20米。利用单目相机和图像

    This work targets what we consider to be the foundational step for urban airborne robots, a safe landing. Our attention is directed toward what we deem the most crucial aspect of the safe landing perception stack: segmentation. We present a streamlined reactive UAV system that employs visual servoing by harnessing the capabilities of open vocabulary image segmentation. This approach can adapt to various scenarios with minimal adjustments, bypassing the necessity for extensive data accumulation for refining internal models, thanks to its open vocabulary methodology. Given the limitations imposed by local authorities, our primary focus centers on operations originating from altitudes of 100 meters. This choice is deliberate, as numerous preceding works have dealt with altitudes up to 30 meters, aligning with the capabilities of small stereo cameras. Consequently, we leave the remaining 20m to be navigated using conventional 3D path planning methods. Utilizing monocular cameras and image 
    
[^41]: 鲁棒拉格朗日和对抗性策略梯度在鲁棒约束马尔可夫决策过程中的应用

    Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes. (arXiv:2308.11267v1 [cs.LG])

    [http://arxiv.org/abs/2308.11267](http://arxiv.org/abs/2308.11267)

    本文介绍了两种算法，具有鲁棒拉格朗日的RCPG和对抗性RCPG，用于解决鲁棒约束马尔可夫决策过程中的问题。具有鲁棒拉格朗日的RCPG通过使用拉格朗日来计算最坏情况下的动态，而对抗性RCPG通过对抗策略的方式直接和增量学习最坏情况下的动态。

    

    鲁棒约束马尔可夫决策过程（RCMDP）是一个最近应用于强化学习的任务建模框架，它通过使用不确定性集合在转移动态模型中提供了对错误的鲁棒性。模拟RCMDPs需要基于每个状态的值估计计算最坏情况下的动态，这种方法之前在鲁棒约束策略梯度（RCPG）中使用过。本文介绍了两种算法，分别称为具有鲁棒拉格朗日的RCPG和对抗性RCPG。具有鲁棒拉格朗日的RCPG通过使用拉格朗日而不是值或约束来计算最坏情况下的动态从而修改RCPG。对抗性RCPG也基于拉格朗日公式计算最坏情况下的动态，但是将其作为对抗策略直接和增量地学习。

    The robust constrained Markov decision process (RCMDP) is a recent task-modelling framework for reinforcement learning that incorporates behavioural constraints and that provides robustness to errors in the transition dynamics model through the use of an uncertainty set. Simulating RCMDPs requires computing the worst-case dynamics based on value estimates for each state, an approach which has previously been used in the Robust Constrained Policy Gradient (RCPG). Highlighting potential downsides of RCPG such as not robustifying the full constrained objective and the lack of incremental learning, this paper introduces two algorithms, called RCPG with Robust Lagrangian and Adversarial RCPG. RCPG with Robust Lagrangian modifies RCPG by taking the worst-case dynamics based on the Lagrangian rather than either the value or the constraint. Adversarial RCPG also formulates the worst-case dynamics based on the Lagrangian but learns this directly and incrementally as an adversarial policy throug
    
[^42]: Retroformer：使用策略梯度优化的回顾性大型语言代理

    Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization. (arXiv:2308.02151v1 [cs.CL])

    [http://arxiv.org/abs/2308.02151](http://arxiv.org/abs/2308.02151)

    本文介绍了一种通过策略梯度优化的回顾性大型语言代理框架，该框架通过学习环境反馈来调整语言代理的提示，从而优化其性能。这种代理能够从多个环境和任务中学习奖励，并通过总结以前任务的根本原因来改进语言代理提示。

    

    最近几个月，出现了一个强大的新趋势，即将大型语言模型（LLMs）增强成能够自主完成目标导向多步骤任务的语言代理，而不仅仅是回答人类用户的查询。然而，大多数现有的语言代理没有使用环境特定的奖励进行优化。尽管一些代理通过口头反馈实现了迭代改进，但它们不能以与基于梯度的奖励学习相兼容的方式进行推理和规划。本文提出了一个原则性的框架，通过学习回顾模型，通过策略梯度自动调整语言代理的提示，从环境反馈中优化代理的工作。具体而言，我们提出的代理架构通过学习多个环境和任务的奖励来微调预训练语言模型，从而通过总结以前任务的根本原因来改进语言代理提示。

    Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior
    
[^43]: 人才分析的人工智能技术综述

    A Comprehensive Survey of Artificial Intelligence Techniques for Talent Analytics. (arXiv:2307.03195v1 [cs.CY])

    [http://arxiv.org/abs/2307.03195](http://arxiv.org/abs/2307.03195)

    这篇论文综述了人工智能技术在人才分析中的应用。通过使用大数据和人工智能技术，组织可以从数据科学的角度理解组织行为并实时做出决策，为有效的人才管理提供智能支持。

    

    在当今竞争激烈且快速发展的商业环境下，组织需要重新思考以量化方式做出人才相关决策的重要性。事实上，大数据和人工智能技术的最新发展已经彻底改变了人力资源管理。大规模人才和管理相关数据的可用性为企业领导者提供了从数据科学角度理解组织行为和获取实际知识的无与伦比机会，进而为实时决策和有效的人才管理提供智能支持。在过去的十年中，人才分析已经成为应用数据科学在人力资源管理方面的有希望的领域，引起了人工智能社区的广泛关注并激发了许多研究工作。为此，我们提供了一个最新、全面的关于人工智能技术在人才分析中的应用的综述。

    In today's competitive and fast-evolving business environment, it is a critical time for organizations to rethink how to make talent-related decisions in a quantitative manner. Indeed, the recent development of Big Data and Artificial Intelligence (AI) techniques have revolutionized human resource management. The availability of large-scale talent and management-related data provides unparalleled opportunities for business leaders to comprehend organizational behaviors and gain tangible knowledge from a data science perspective, which in turn delivers intelligence for real-time decision-making and effective talent management at work for their organizations. In the last decade, talent analytics has emerged as a promising field in applied data science for human resource management, garnering significant attention from AI communities and inspiring numerous research efforts. To this end, we present an up-to-date and comprehensive survey on AI technologies used for talent analytics in the f
    
[^44]: BaBE:通过估计潜在解释变量增强公平性

    BaBE: Enhancing Fairness via Estimation of Latent Explaining Variables. (arXiv:2307.02891v1 [cs.LG])

    [http://arxiv.org/abs/2307.02891](http://arxiv.org/abs/2307.02891)

    本文提出了一种名为BaBE的方法，通过估计潜在解释变量来提高公平性。该方法通过结合贝叶斯推断和期望最大化方法，估计给定Z的每个群体E的最可能值。

    

    本文考虑了两个群体之间不公平歧视的问题，并提出了一种预处理方法来实现公平性。我们提出了一种基于贝叶斯推断和期望最大化方法的BaBE (Bayesian Bias Elimination)方法，用于估计给定Z的每个群体的E的最可能值。

    We consider the problem of unfair discrimination between two groups and propose a pre-processing method to achieve fairness. Corrective methods like statistical parity usually lead to bad accuracy and do not really achieve fairness in situations where there is a correlation between the sensitive attribute S and the legitimate attribute E (explanatory variable) that should determine the decision. To overcome these drawbacks, other notions of fairness have been proposed, in particular, conditional statistical parity and equal opportunity. However, E is often not directly observable in the data, i.e., it is a latent variable. We may observe some other variable Z representing E, but the problem is that Z may also be affected by S, hence Z itself can be biased. To deal with this problem, we propose BaBE (Bayesian Bias Elimination), an approach based on a combination of Bayes inference and the Expectation-Maximization method, to estimate the most likely value of E for a given Z for each grou
    
[^45]: 对齐的神经网络是否对抗对齐？

    Are aligned neural networks adversarially aligned?. (arXiv:2306.15447v1 [cs.CL])

    [http://arxiv.org/abs/2306.15447](http://arxiv.org/abs/2306.15447)

    我们研究了大型语言模型在面对对抗用户构建的对抗性输入时是否仍能保持对齐。我们发现现有的攻击手法不足以可靠攻击对齐文本模型，并通过蛮力方法找到了对抗性输入。

    

    大型语言模型现在被调整为与其创建者的目标保持一致，即"有益且无害"。这些模型应该对用户的问题给出有益的回答，但拒绝回答可能会造成伤害的请求。然而，对抗用户可以构建绕过对齐尝试的输入。在这项工作中，我们研究了在与构造最坏情况输入（对抗性样本）的对抗用户交互时，这些模型保持多大程度的对齐。这些输入被设计成导致模型发出本应禁止的有害内容。我们展示了现有基于自然语言处理的优化攻击手法在可靠攻击对齐文本模型方面的不足之处：即使在当前基于自然语言处理的攻击失败时，我们仍然可以通过蛮力方法找到对抗性输入。因此，当前攻击的失败不应被视为对齐文本模型在面对对抗性输入时仍然保持对齐的证明。但是近期大规模机器学习模型的趋势是多模态的。

    Large language models are now tuned to align with the goals of their creators, namely to be "helpful and harmless." These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs.  However the recent trend in large-scale ML models is multim
    
[^46]: 揭示情感的潜力：大型语言模型能否预测中国股票价格波动？

    Unveiling the Potential of Sentiment: Can Large Language Models Predict Chinese Stock Price Movements?. (arXiv:2306.14222v1 [cs.CL])

    [http://arxiv.org/abs/2306.14222](http://arxiv.org/abs/2306.14222)

    本篇论文研究如何运用大型语言模型提取中文新闻文本信息的情感因素，以期促进知情和高频的投资组合调整。通过建立严格和全面的基准测试与标准化的回测框架，作者对不同类型 LLMs 在该领域内的效果进行了客观评估。

    

    大型语言模型 (LLMs) 的快速发展已引发了广泛的讨论，其中包括它们将如何提高量化股票交易策略的回报的潜力。这些讨论主要围绕着利用 LLMs 的出色理解能力来提取情感因素，从而促进知情和高频的投资组合调整。为了确保这些 LLMs 成功地应用于中国金融文本分析和随后的中国股票市场交易策略开发中，我们提供了一个严格和全面的基准测试以及一个标准化的回测框架，旨在客观评估不同类型 LLMs 在中文新闻文本数据的情感因素提取中的效果。为了说明我们基准测试的工作方式，我们引用了三个不同模型：1）生成式 LLM (ChatGPT)，2）中文语言特定的预训练 LLM (二郎神 RoBERTa)，以及……

    The rapid advancement of Large Language Models (LLMs) has led to extensive discourse regarding their potential to boost the return of quantitative stock trading strategies. This discourse primarily revolves around harnessing the remarkable comprehension capabilities of LLMs to extract sentiment factors which facilitate informed and high-frequency investment portfolio adjustments. To ensure successful implementations of these LLMs into the analysis of Chinese financial texts and the subsequent trading strategy development within the Chinese stock market, we provide a rigorous and encompassing benchmark as well as a standardized back-testing framework aiming at objectively assessing the efficacy of various types of LLMs in the specialized domain of sentiment factor extraction from Chinese news text data. To illustrate how our benchmark works, we reference three distinctive models: 1) the generative LLM (ChatGPT), 2) the Chinese language-specific pre-trained LLM (Erlangshen-RoBERTa), and 
    
[^47]: LMFlow：用于大型基础模型微调和推理的可扩展工具包

    LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. (arXiv:2306.12420v1 [cs.CL])

    [http://arxiv.org/abs/2306.12420](http://arxiv.org/abs/2306.12420)

    LMFlow是一个可扩展和轻量级的工具包，为大型基础模型提供完整的微调工作流程，以支持在有限的计算资源下进行个性化训练，并支持连续预训练和指令微调以适应不同的专业任务。

    

    大型基础模型展现出比传统方法更接近人类智能的能力，已经引起了人工智能界的广泛关注。然而，大部分模型在专业任务应用中仍然表现出明显的缺陷，需要微调才能获得令人满意的性能。随着可用模型和专业任务数量的增加，通用微调的工作变得非常棘手。在本文中，我们采取了第一步解决这个问题。我们介绍了一个可扩展和轻量级的工具包LMFlow，旨在简化大型基础模型的微调和推理。LMFlow为大型基础模型提供了完整的微调工作流程，支持在有限的计算资源下进行个性化训练。此外，它还支持连续预训练、指令微调等功能，以更好地适应不同的专业任务。

    Large foundation models have demonstrated a great ability to achieve general human-level intelligence far beyond traditional approaches. As the technique keeps attracting attention from the AI community, more and more large foundation models have become publically available. However, most of those models exhibit a major deficiency in specialized-task applications, where the step of finetuning is still required for obtaining satisfactory performance. As the number of available models and specialized tasks keeps growing, the job of general finetuning becomes highly nontrivial. In this paper, we take the first step to address this issue. We introduce an extensible and lightweight toolkit, LMFlow, which aims to simplify the finetuning and inference of general large foundation models. LMFlow offers a complete finetuning workflow for a large foundation model to support personalized training with limited computing resources. Furthermore, it supports continuous pretraining, instruction tuning,
    
[^48]: 大型语言模型的简单而有效的剪枝方法

    A Simple and Effective Pruning Approach for Large Language Models. (arXiv:2306.11695v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.11695](http://arxiv.org/abs/2306.11695)

    本论文提出了一种称为Wanda的新颖、简单而有效的剪枝方法，用于大型语言模型，通过对每个输出上的权重按照最小幅度乘以对应的输入激活来进行剪枝，无需重新训练或更新权重。

    

    随着规模的增大，大型语言模型（LLMs）是网络剪枝方法的自然候选对象：这些方法在努力保持性能的同时，丢弃了网络权重的一个子集。然而，现有的方法要么需要重新训练，这对于十亿级别的LLMs来说很少可行，要么需要解决一个依赖二阶信息的权重重构问题，这也可能计算成本很高。在本文中，我们介绍了一种新颖的、简单但有效的剪枝方法，称为Wanda（基于权重和激活的剪枝），旨在对预训练的LLMs引入稀疏性。受到最近对LLMs中出现的大幅特征的发现的启发，我们的方法在每个输出上按照权重和对应的输入激活相乘的最小幅度来剪枝权重。值得注意的是，Wanda不需要重新训练或更新权重，剪枝后的LLM可以直接使用。我们在LLaMA和LLaMA-2上对我们的方法Wanda进行了彻底的评估。

    As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across vari
    
[^49]: SequenceMatch：带回溯的自回归序列模型的模仿学习

    SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking. (arXiv:2306.05426v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.05426](http://arxiv.org/abs/2306.05426)

    本文提出了一种称为SequenceMatch的带有回溯的自回归模型的模仿学习框架，该框架通过最小化自回归模型生成序列和数据集序列之间的各种分歧来减少在自回归生成过程中的复合误差，并允许引入回溯动作。

    

    在许多领域，自回归模型可以在预测下一个观测值的任务上获得高似然度。然而，这种最大似然（MLE）目标不一定与自回归生成高质量序列的下游用例相匹配。MLE目标按照数据分布下序列的频率加权，不提供模型在分布之外行为的指导，这会导致在自回归生成过程中复合误差。为了解决这个复合误差问题，我们将序列生成定为模仿学习（IL）问题。这使我们可以最小化自回归模型生成的序列分布和数据集序列之间的各种分歧，包括考虑出分布序列的分歧。IL框架还允许我们通过在生成过程中引入回格动作来引入回溯。这进一步减轻了复合效应。

    In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compound
    
[^50]: 通过反事实路径几何导航解释性多元宇宙

    Navigating Explanatory Multiverse Through Counterfactual Path Geometry. (arXiv:2306.02786v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02786](http://arxiv.org/abs/2306.02786)

    该论文提出了解释性多元宇宙的概念，用于导航和比较所有可能的反事实路径的几何关系。

    

    反事实解释是解释（不透明的）预测模型决策的事实标准。其生成往往受到算法和特定领域约束的影响，如基于密度的可行性和属性的（不）可变性或变化的方向性，旨在最大化其在现实生活中的实用性。除了对反事实实例本身的要求之外，已知算法可行性路径与事实数据点之间的连接，即算法可诉求，已成为重要的技术考虑因素。尽管这两个要求确保了旅程的步骤和目的地的合理性，但目前的文献忽略了这种反事实路径的多样性。为了解决这个缺点，我们引入了一种新颖的解释性多元宇宙概念，涵盖了所有可能的反事实旅程；然后展示了如何导航、推理和比较这些轨迹的几何关系。

    Counterfactual explanations are the de facto standard when tasked with interpreting decisions of (opaque) predictive models. Their generation is often subject to algorithmic and domain-specific constraints -- such as density-based feasibility and attribute (im)mutability or directionality of change -- that aim to maximise their real-life utility. In addition to desiderata with respect to the counterfactual instance itself, existence of a viable path connecting it with the factual data point, known as algorithmic recourse, has become an important technical consideration. While both of these requirements ensure that the steps of the journey as well as its destination are admissible, current literature neglects the multiplicity of such counterfactual paths. To address this shortcoming we introduce the novel concept of explanatory multiverse that encompasses all the possible counterfactual journeys; we then show how to navigate, reason about and compare the geometry of these trajectories -
    
[^51]: 以决策为导向的人机对话

    Decision-Oriented Dialogue for Human-AI Collaboration. (arXiv:2305.20076v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.20076](http://arxiv.org/abs/2305.20076)

    该论文探讨了一类以决策为导向的人机对话任务，以及在会议论文审稿人分配、城市多步行程规划和旅行计划协商等场景中，人工智能助手和用户不同的能力如何结合以达到最佳决策。论文通过构建对话环境并进行人机对话收集数据，发现当前人工智能助手在此类任务中的局限性。

    

    我们描述了一类任务，称为以决策为导向的对话，其中人工智能助手必须通过自然语言与一名或多名人类合作，帮助他们做出复杂的决策。我们在三个领域中形式化用户面临日常决策的过程：（1）选择会议论文的审稿人分配，（2）在城市中规划多步行程，以及（3）为一群朋友协商旅行计划。在每个设置中，AI助手和用户具有不同的能力，他们必须结合起来得出最佳决策：助手可以访问和处理大量信息，而用户具有系统外的偏好和限制。对于每个任务，我们构建了一个对话环境，其中代理根据他们达到的最终决策的质量获得奖励。使用这些环境，我们与人们扮演助手的人进行了人机对话。为了比较当前人工智能助手在这些设置中的交流方式，我们提出了基线模型，并将其与人类-人类对话进行比较。我们的结果展示了决策导向对话所面临的挑战，并凸显了当前人工智能助手的局限性。

    We describe a class of tasks called decision-oriented dialogues, in which AI assistants must collaborate with one or more humans via natural language to help them make complex decisions. We formalize three domains in which users face everyday decisions: (1) choosing an assignment of reviewers to conference papers, (2) planning a multi-step itinerary in a city, and (3) negotiating travel plans for a group of friends. In each of these settings, AI assistants and users have disparate abilities that they must combine to arrive at the best decision: assistants can access and process large amounts of information, while users have preferences and constraints external to the system. For each task, we build a dialogue environment where agents receive a reward based on the quality of the final decision they reach. Using these environments, we collect human-human dialogues with humans playing the role of assistant. To compare how current AI assistants communicate in these settings, we present bas
    
[^52]: ChatGPT的公平性评估

    Fairness of ChatGPT. (arXiv:2305.18569v1 [cs.LG])

    [http://arxiv.org/abs/2305.18569](http://arxiv.org/abs/2305.18569)

    本文提供了一个使用ChatGPT作为研究案例的LLM有效性和公平性的系统评估，旨在评估ChatGPT在高风险领域的表现，以提供更深入的了解LLM的公平表现，并为偏见缓解和负责任的人工智能系统的发展做出贡献。

    

    理解和解决LLM中不公平的问题对于负责任的AI部署至关重要。然而，在将LLM应用于高风险领域时，尤其是关于公平评估方面，数量分析和深入研究的可用性有限。本文旨在提供一个使用ChatGPT作为研究案例的LLM有效性和公平性的系统评估，我们专注于评估ChatGPT在包括教育、犯罪学、金融和医疗保健等高风险领域的表现。为了进行全面的评估，我们考虑了群体公平性和个人公平性，并观察了在一系列有偏或无偏提示下ChatGPT输出的差异。该研究对于更深入的了解LLM的公平表现，便于偏见缓解，促进负责任的人工智能系统的发展具有意义。

    Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high-stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's performance in high-takes fields including education, criminology, finance and healthcare. To make thorough evaluation, we consider both group fairness and individual fairness and we also observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.
    
[^53]: 使基于提示的黑盒调优更加丰富多彩：从三个正交角度提高模型泛化能力

    Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives. (arXiv:2305.08088v1 [cs.CL])

    [http://arxiv.org/abs/2305.08088](http://arxiv.org/abs/2305.08088)

    本文提出了BBT-RGB，一套用于增强黑盒优化效率和性能的直接且互补技术套件，包括两阶段无导数优化策略、自动语言转化器构建及其在少样本设置中的新用法以及更好的提示初始化。

    

    大型语言模型在各种自然语言处理任务中已经展现出越来越强大的能力。然而，调整这些模型以用于下游任务通常需要巨额的代价或由于商业考虑而不可用。最近，提出了黑盒调优来解决这个问题，通过优化任务特定的提示而不访问梯度和隐藏表示。然而，大多数现有的作品还没有充分利用少样本学习场景下无梯度优化的潜力。在本文中，我们描述了BBT-RGB，这是一个用于增强黑盒优化效率和性能的直接且互补技术套件。具体来说，我们的方法包括三个即插即用的组件：（1）两阶段无导数优化策略，有助于快速收敛并缓解过拟合；（2）自动语言转化器构建及其在少样本设置中的新用法；（3）更好的提示初始化，基于未标记数据的语言学动机句法模式。

    Large language models (LLMs) have shown increasing power on various natural language processing (NLP) tasks. However, tuning these models for downstream tasks usually needs exorbitant costs or is unavailable due to commercial considerations. Recently, black-box tuning has been proposed to address this problem by optimizing task-specific prompts without accessing the gradients and hidden representations. However, most existing works have yet fully exploited the potential of gradient-free optimization under the scenario of few-shot learning. In this paper, we describe BBT-RGB, a suite of straightforward and complementary techniques for enhancing the efficiency and performance of black-box optimization. Specifically, our method includes three plug-and-play components: (1) Two-stage derivative-free optimization strategy that facilitates fast convergence and mitigates overfitting; (2) Automatic verbalizer construction with its novel usage under few-shot settings; (3) Better prompt initializ
    
[^54]: 基于Option框架的多模式探索自主非单体智能体

    An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework. (arXiv:2305.01322v1 [cs.AI])

    [http://arxiv.org/abs/2305.01322](http://arxiv.org/abs/2305.01322)

    该论文关注强化学习中的探索研究，提出了一个能够自主管理探索策略的多模式智能体非单体探索方法，并通过实验结果展示了该方法的优越性能。

    

    强化学习领域的探索研究主要关注“如何探索”的探索方式，而“何时探索”的探索研究一直没有成为重点。典型的探索行为通常将探索行为与智能体的开发利用行为绑定在一起。最近出现了非单体探索行为的研究，以研究人类和动物的模式切换行为。本研究的最终目的是使智能体能够自主决定何时探索或利用。我们在Option框架中描述了自主多模式探索的初始研究。通过比较实验结果，我们展示了我们的方法相对于现有的非单体探索方法的更高性能。

    Most exploration research on reinforcement learning (RL) has paid attention to `the way of exploration', which is `how to explore'. The other exploration research, `when to explore', has not been the main focus of RL exploration research. \textcolor{black}{The issue of `when' of a monolithic exploration in the usual RL exploration behaviour binds an exploratory action to an exploitational action of an agent. Recently, a non-monolithic exploration research has emerged to examine the mode-switching exploration behaviour of humans and animals.} The ultimate purpose of our research is to enable an agent to decide when to explore or exploit autonomously. We describe the initial research of an autonomous multi-mode exploration of non-monolithic behaviour in an options framework. The higher performance of our method is shown against the existing non-monolithic exploration method through comparative experimental results.
    
[^55]: RegionPLC：用于开放世界3D场景理解的区域点-语言对比学习

    RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding. (arXiv:2304.00962v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.00962](http://arxiv.org/abs/2304.00962)

    提出了一种用于开放世界3D场景理解的Regional Point-Language Contrastive Learning框架，通过密集的视觉提示和点独立对比学习来实现对新颖类别的识别和密集场景理解。

    

    现有的3D场景理解任务在闭集基准上取得了高性能，但在现实世界应用中无法处理新颖类别。为此，我们提出了一种称为RegionPLC的开放世界3D场景理解的区域点-语言对比学习框架，它使经过封闭集数据集训练的模型具备开放词汇识别能力。我们提出了密集的视觉提示，通过标题生成从2D基础模型中引发区域级视觉-语言知识，进而使我们能够建立密集的区域点-语言关联。然后，我们设计了一种点判别对比学习目标，使得从标题中进行点独立学习以实现密集场景理解。我们在ScanNet、ScanNet200和nuScenes数据集上进行了大量实验。相比之前的基于注释的3D开放世界场景理解方法，我们的RegionPLC在语义和实例分割方面的性能平均提高了11.6%和6.6%。

    Existing 3D scene understanding tasks have achieved high performance on close-set benchmarks but fail to handle novel categories in real-world applications. To this end, we propose a Regional Point-Language Contrastive learning framework, namely RegionPLC, for open-world 3D scene understanding, which equips models trained on closed-set datasets with open-vocabulary recognition capabilities. We propose dense visual prompts to elicit region-level visual-language knowledge from 2D foundation models via captioning, which further allows us to build dense regional point-language associations. Then, we design a point-discriminative contrastive learning objective to enable point-independent learning from captions for dense scene understanding. We conduct extensive experiments on ScanNet, ScanNet200, and nuScenes datasets. Our RegionPLC significantly outperforms previous base-annotated 3D open-world scene understanding approaches by an average of 11.6\% and 6.6\% for semantic and instance segme
    
[^56]: AraSpot：阿拉伯语口语命令识别

    AraSpot: Arabic Spoken Command Spotting. (arXiv:2303.16621v1 [cs.CL])

    [http://arxiv.org/abs/2303.16621](http://arxiv.org/abs/2303.16621)

    AraSpot是一款使用ConformerGRU模型架构训练40个阿拉伯语关键词的口语命令识别工具，其通过在线数据增强和文本到语音模型的训练提高了性能，并以99.59%的准确率超出以往的方法。

    

    口语关键识别（KWS）是指在音频流中识别关键词，广泛用于智能边缘设备上，以启动语音助手和进行免提任务。该任务面临着高精度和在低功率和可能的有限计算能力设备上保持系统运行效率的需求。本文介绍了使用不同的在线数据增强和引入ConformerGRU模型架构的AraSpot，用于训练40个阿拉伯语关键词的识别。最后，我们通过训练文本到语音模型进行合成数据生成，进一步提高了模型的性能。AraSpot以SOTA 99.59％超过以往的方法。

    Spoken keyword spotting (KWS) is the task of identifying a keyword in an audio stream and is widely used in smart devices at the edge in order to activate voice assistants and perform hands-free tasks. The task is daunting as there is a need, on the one hand, to achieve high accuracy while at the same time ensuring that such systems continue to run efficiently on low power and possibly limited computational capabilities devices. This work presents AraSpot for Arabic keyword spotting trained on 40 Arabic keywords, using different online data augmentation, and introducing ConformerGRU model architecture. Finally, we further improve the performance of the model by training a text-to-speech model for synthetic data generation. AraSpot achieved a State-of-the-Art SOTA 99.59% result outperforming previous approaches.
    
[^57]: 分析和编辑暗藏后门的语言模型的内部机制

    Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. (arXiv:2302.12461v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12461](http://arxiv.org/abs/2302.12461)

    本研究分析并编辑暗藏后门的语言模型的内部机制，发现早期层的MLP模块和初始嵌入投影是后门机制中最重要的部分。通过使用PCP消融技术替换变压器模块，我们成功删除、插入和修改后门机制，并显著改善了后门的输出效果。

    

    数据集中的毒化是对大型语言模型的潜在安全威胁，可能导致暗藏后门的模型。关于暗藏后门语言模型的内部机制以及它们如何处理触发输入（例如，切换至有毒语言）的描述尚未找到。本文研究基于Transformer的暗藏后门语言模型的内部表示，并确定早期层的MLP模块与初始嵌入投影结合是后门机制中最重要的部分。我们利用这些知识来删除、插入和修改后门机制，并用工程化替代物降低MLP模块输出的重要性。为此，我们引入了基于主要成分的低秩矩阵的PCP消融技术，用其替换变压器模块。我们在暗藏后门的玩具模型、暗藏后门的大型模型和非暗藏后门的开源模型上展示了我们的结果。我们表明我们可以改善后门的输出效果。

    Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor r
    
[^58]: 平均限制策略优化

    Average-Constrained Policy Optimization. (arXiv:2302.00808v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00808](http://arxiv.org/abs/2302.00808)

    本研究提出了一种新的基于函数逼近算法的带平均标准约束 MDP 的策略优化算法，具有较好的性能表现。

    

    有限制条件的强化学习对于各种应用变得越来越重要。通常，平均标准比折扣标准更合适。然而，针对平均限制 CMDP 的强化学习仍然是一个具有挑战性的问题。针对折扣限制 RL 问题设计的算法通常在平均 CMDP 环境下表现不佳。在本文中，我们引入了一种新的基于函数逼近算法的带平均标准约束 MDP 的策略优化算法。平均限制策略优化（ACPO）算法的灵感来自基于信任区域方法的著名 PPO 类算法。我们发展了基本的平均 MDP 敏感性理论，然后在算法设计中使用相应的界限。我们提供了其性能的理论保证，并通过在各种具有挑战性的 MuJoCo 环境中进行大量实验工作，展示了该算法与其他常规算法相比的卓越表现。

    Reinforcement Learning (RL) with constraints is becoming an increasingly important problem for various applications. Often, the average criterion is more suitable than the discounted criterion. Yet, RL for average criterion-constrained MDPs remains a challenging problem. Algorithms designed for discounted constrained RL problems often do not perform well for the average CMDP setting. In this paper, we introduce a new policy optimization with function approximation algorithm for constrained MDPs with the average criterion. The Average-Constrained Policy Optimization (ACPO) algorithm is inspired by the famed PPO-type algorithms based on trust region methods. We develop basic sensitivity theory for average MDPs, and then use the corresponding bounds in the design of the algorithm. We provide theoretical guarantees on its performance, and through extensive experimental work in various challenging MuJoCo environments, show the superior performance of the algorithm when compared to other sta
    
[^59]: 基于状态的重要性抽样方法实现低方差的行为策略离线评估

    Low Variance Off-policy Evaluation with State-based Importance Sampling. (arXiv:2212.03932v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03932](http://arxiv.org/abs/2212.03932)

    本文提出了一种基于状态的重要性抽样（SIS）方法，通过检测“忽略状态”的子轨迹来实现低方差的离线评估。

    

    在强化学习的离线评估中，需要评估目标策略的性能，而这需要使用由行为策略采集的样本数据。传统的重要性抽样方法由于计算动作概率比值的乘积而导致方差增加，从而在涉及长期规划的任务中出现估计不准确的问题。本文提出了一种基于状态的重要性抽样（SIS）方法，通过检测“忽略状态”的子轨迹来实现低方差的离线评估。

    In off-policy reinforcement learning, a behaviour policy performs exploratory interactions with the environment to obtain state-action-reward samples which are then used to learn a target policy that optimises the expected return. This leads to a problem of off-policy evaluation, where one needs to evaluate the target policy from samples collected by the often unrelated behaviour policy. Importance sampling is a traditional statistical technique that is often applied to off-policy evaluation. While importance sampling estimators are unbiased, their variance increases exponentially with the horizon of the decision process due to computing the importance weight as a product of action probability ratios, yielding estimates with low accuracy for domains involving long-term planning. This paper proposes state-based importance sampling (SIS), which drops the action probability ratios of sub-trajectories with "negligible states" -- roughly speaking, those for which the chosen actions have no 
    
[^60]: 能否通过脑信号揭示人类语言的内部一致性？

    Can Brain Signals Reveal Inner Alignment with Human Languages?. (arXiv:2208.06348v4 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2208.06348](http://arxiv.org/abs/2208.06348)

    本研究探索了脑信号和人类语言之间的关系，并介绍了一种名为MTAM的方法，该方法在情感分析和关系检测等下游应用中取得了新的最先进结果。

    

    脑信号（如脑电图）和人类语言在许多下游任务中被广泛研究，但二者之间的联系尚未得到很好的探索。本研究探讨了脑电图和语言之间的关系和依赖性。在表示层面上，我们引入了一种名为MTAM（Multimodal Transformer Alignment Model）的方法，用于观察这两种模态之间的协调表示。我们使用了多种关系对齐技术，如典型相关分析和Wasserstein距离，作为损失函数来转换特征。在情感分析和关系检测等下游应用中，我们在ZuCo和K-EmoCon两个数据集上实现了新的最先进结果。我们的方法在情感分析方面使K-EmoCon数据集的F1分数提高了1.7％，ZuCo数据集提高了9.3％，在关系检测方面ZuCo数据集提高了7.4％。此外，我们提供了国际上最大的人类类比推理数据集的编码方案。

    Brain Signals, such as Electroencephalography (EEG), and human languages have been widely explored independently for many downstream tasks, however, the connection between them has not been well explored. In this study, we explore the relationship and dependency between EEG and language. To study at the representation level, we introduced \textbf{MTAM}, a \textbf{M}ultimodal \textbf{T}ransformer \textbf{A}lignment \textbf{M}odel, to observe coordinated representations between the two modalities. We used various relationship alignment-seeking techniques, such as Canonical Correlation Analysis and Wasserstein Distance, as loss functions to transfigure features. On downstream applications, sentiment analysis and relation detection, we achieved new state-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method achieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets for sentiment analysis, and 7.4% on ZuCo for relation detection. In addition, we provide inter
    
[^61]: 幻觉攻击：对顺序决策者的敌对攻击中可检测性很重要

    Illusory Attacks: Detectability Matters in Adversarial Attacks on Sequential Decision-Makers. (arXiv:2207.10170v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2207.10170](http://arxiv.org/abs/2207.10170)

    对于顺序决策者的敌对攻击来说，弱点是缺乏时间上的一致性，使其容易被检测出来；而R-attack是一种既有效又可证明是统计不可检测的攻击，可以更难以使用自动化方法检测出来。

    

    在实际世界中部署的自主代理需要对感官输入的敌对攻击具备强大的鲁棒性。强化代理策略需要预测可能的最强攻击。我们证明了现有的强化学习代理的观测空间攻击具有共同的弱点：虽然有效，但它们缺乏时间上的一致性，因此可以使用自动化手段或人工检查来检测。对于敌手来说，可检测性是不希望出现的，因为它可能会引发安全事态升级。我们引入了完美的幻觉攻击，这是一种新形式的顺序决策者的敌对攻击，既有效又可证明是统计不可检测的。随后，我们提出了更加灵活的R-attack，其生成的观测转换与无敌对环境的状态转换函数一致且可以端到端学习。实验结果显示，与现有的攻击相比，R-attack更难以使用自动化方法检测出来。

    Autonomous agents deployed in the real world need to be robust against adversarial attacks on sensory inputs. Robustifying agent policies requires anticipating the strongest attacks possible. We demonstrate that existing observation-space attacks on reinforcement learning agents have a common weakness: while effective, their lack of temporal consistency makes them detectable using automated means or human inspection. Detectability is undesirable to adversaries as it may trigger security escalations. We introduce perfect illusory attacks, a novel form of adversarial attack on sequential decision-makers that is both effective and provably statistically undetectable. We then propose the more versatile R-attacks, which result in observation transitions that are consistent with the state-transition function of the adversary-free environment and can be learned end-to-end. Compared to existing attacks, we empirically find R-attacks to be significantly harder to detect with automated methods, 
    

