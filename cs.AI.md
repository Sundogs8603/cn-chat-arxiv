# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Meta-learning in healthcare: A survey.](http://arxiv.org/abs/2308.02877) | 元学习在医疗领域有广泛应用，可以解决医疗挑战，如样本不足和数据收集差异。主要包括多/单任务学习和多/少样本学习方法。 |
| [^2] | [Semi-supervised Learning for Segmentation of Bleeding Regions in Video Capsule Endoscopy.](http://arxiv.org/abs/2308.02869) | 该论文提出了一种半监督学习方法，用于视频胶囊内镜中出血区域的分割。通过采用“平均教师”方法，构建了一个学生 U-Net 模型和一个教师模型，实现了对VCE图像中出血区域的准确识别。 |
| [^3] | [Replace Scoring with Arrangement: A Contextual Set-to-Arrangement Framework for Learning-to-Rank.](http://arxiv.org/abs/2308.02860) | 这篇论文提出了一种新的学习排序框架，名为STARank，它通过直接生成候选项目排列来替代个别评分和排序操作，并且是端到端可微分的。 |
| [^4] | [feather -- a Python SDK to share and deploy models.](http://arxiv.org/abs/2308.02838) | feather是一个Python SDK，可以帮助模型开发人员用很少的代码构建可共享的用户界面，并提供URL和API端点供其他人访问和使用模型。 |
| [^5] | [Physics-Based Task Generation Through Causal Sequence of Physical Interactions.](http://arxiv.org/abs/2308.02835) | 本文提出了一种基于物理交互因果序列的任务生成方法，使得人工智能系统能够在模拟物理环境中执行任务并评估其物理推理能力。我们使用“愤怒的小鸟”游戏作为示例，通过一系列指标对生成的任务进行评估。 |
| [^6] | [Multi-Agent Verification and Control with Probabilistic Model Checking.](http://arxiv.org/abs/2308.02829) | 概率模型检验技术已经扩展到多智能体交互领域，可用于灵活地推理智能体的协作或对抗行为，在人工智能、机器人和自主系统等领域有广泛应用。 |
| [^7] | [A Symbolic Character-Aware Model for Solving Geometry Problems.](http://arxiv.org/abs/2308.02823) | 本文提出了一个符号感知模型，用于解决几何问题。该模型通过探索符号字符在文本和图表理解中的作用，以及在多模态推理框架下对模型进行优化来提高解决几何问题的能力。 |
| [^8] | [The changing rule of human bone density with aging based on a novel definition and mensuration of bone density with computed tomography.](http://arxiv.org/abs/2308.02815) | 本研究提出了一种新的骨密度定义和测量方法，利用计算机断层扫描图像中的骨像素分布来测量骨密度。研究发现，在成年期39岁到80岁之间，骨密度呈线性下降，女性下降速度大约是男性的1.6倍，与现有观点相矛盾。这对于了解人体衰老过程具有重要意义。 |
| [^9] | [Artificial Intelligence for Molecular Communication.](http://arxiv.org/abs/2308.02812) | 人工智能用于分子通信，这是一种在微型设备之间进行数据传输的新方法，具有广阔的医学应用潜力。分子通信通过转化数字信号为分子浓度来进行，解调信号时面临着精确建模困难的挑战。 |
| [^10] | [A generative model for surrogates of spatial-temporal wildfire nowcasting.](http://arxiv.org/abs/2308.02810) | 本文提出了一种生成模型，利用三维量化变分自编码器来生成给定生态区域内未见的空间-时间野火燃烧区域序列。该模型在加利福尼亚州的Chimney火灾生态区域中进行了测试，结果表明该模型能够成功生成连贯且结构良好的火势情景。 |
| [^11] | [MiAMix: Enhancing Image Classification through a Multi-stage Augmented Mixied Sample Data Augmentation Method.](http://arxiv.org/abs/2308.02804) | MiAMix是一种新型的混合样本数据增强方法，通过将图像增强集成到混合框架中并利用多种多样的混合方法来提升图像分类模型的性能和泛化能力。 |
| [^12] | [Crowdsourcing Fraud Detection over Heterogeneous Temporal MMMA Graph.](http://arxiv.org/abs/2308.02793) | 本文提出了一种新颖的对比多视图学习方法CMT，用于在异构时间MMMA图上进行众包欺诈检测。CMT以自我监督的方式捕捉图的异质性和动态性，并生成高质量的表示。实验证明，CMT在代表性MMMA微信的行业规模HTG上表现出色，也显示出在大规模公共金融HTG上有希望的结果，可应用于其他图异常检测任务。 |
| [^13] | [Solving Logistic-Oriented Bin Packing Problems Through a Hybrid Quantum-Classical Approach.](http://arxiv.org/abs/2308.02787) | 这项研究通过混合量子-经典方法解决了物流中以实际为导向的装箱问题，并扩展了解决不同维度问题和具有不同要求的箱子的能力。 |
| [^14] | [Semi-supervised Contrastive Regression for Estimation of Eye Gaze.](http://arxiv.org/abs/2308.02784) | 本文提出了一种用于眼球注视估计的半监督对比回归方法。该方法利用小规模标记的数据集来寻找泛化解决方案，能够在未见面部图像上进行准确的估计。通过引入新的对比损失范式，该方法在性能上表现出色。 |
| [^15] | [Surrogate Empowered Sim2Real Transfer of Deep Reinforcement Learning for ORC Superheat Control.](http://arxiv.org/abs/2308.02765) | 本文提出了一种基于模拟到真实转移学习的深度强化学习控制方法，用于 ORC 超热控制。该方法旨在提供一种新的简单、可行且用户友好的解决方案。 |
| [^16] | [NeRFs: The Search for the Best 3D Representation.](http://arxiv.org/abs/2308.02751) | NeRFs是视图合成和相关问题中寻找最佳3D表示的结果，该方法利用神经网络查询获取体积参数来描述连续体积场景。 |
| [^17] | [Nonlinear Controller Design for a Quadrotor with Inverted Pendulum.](http://arxiv.org/abs/2308.02741) | 本文介绍了一种倒立摆扬升器的非线性控制器设计，该设计适用于四旋翼飞行器和倒立摆组合的动力学系统，并展示了轨迹跟踪的实验结果。 |
| [^18] | [Assessing the impact of emergency department short stay units using length-of-stay prediction and discrete event simulation.](http://arxiv.org/abs/2308.02730) | 本研究开发了一个决策支持系统，通过预测患者入院的住院时间，帮助指导临床决策和资源分配。研究结果表明，使用患者入院人口统计学数据和化验结果，可以相对准确地预测住院时间。 |
| [^19] | [Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction.](http://arxiv.org/abs/2308.02723) | 本文提出了通过修改输入特征和训练目标来改进歌唱旋律提取的方法。这些修改包括增强对尾部谐波的敏感性和设计可防止预测极短片段的损失函数。实验结果表明，这些修改对于提高歌唱旋律提取的效果有实际效果。 |
| [^20] | [Guided Distillation for Semi-Supervised Instance Segmentation.](http://arxiv.org/abs/2308.02668) | 这项研究提出了一种半监督实例分割的引导蒸馏方法，通过引入新的“引导预烧”阶段和利用未标记数据的导师模型指导，取得了显著的改进。 |
| [^21] | [Solving Witness-type Triangle Puzzles Faster with an Automatically Learned Human-Explainable Predicate.](http://arxiv.org/abs/2308.02666) | 使用自动学习的人类可解释谓词加速搜索，解决《见证人》游戏中的谜题实例，比基准搜索提升六倍速度，且能处理更大规模的谜题。 |
| [^22] | [Let's Give a Voice to Conversational Agents in Virtual Reality.](http://arxiv.org/abs/2308.02665) | 本研究提出了一个简化开发虚拟环境中对话代理的开源架构，通过多模态和沉浸式交互提升与对话代理的对话体验，并通过添加语音到文本和文本到语音模型实现基于语音的交互。 |
| [^23] | [AI-Enhanced Data Processing and Discovery Crowd Sourcing for Meteor Shower Mapping.](http://arxiv.org/abs/2308.02664) | 本研究使用增强型人工智能技术对全球流星雨数据进行处理和分析，并通过开发互动的网页平台，让公众参与流星探测，从而提高了流星雨的发现率。 |
| [^24] | [Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks.](http://arxiv.org/abs/2308.02632) | 本研究提出了一种使用生成对抗网络生成合成毫米波雷达数据的快速方法，可以增加数据增强的潜力，并进一步开发雷达数据处理算法。 |
| [^25] | [Harnessing the Web and Knowledge Graphs for Automated Impact Investing Scoring.](http://arxiv.org/abs/2308.02622) | 本论文介绍了一个利用网络和知识图谱自动评估影响力投资的数据驱动系统，通过收集和过滤文本数据集以及使用分类器来预测分数，实现了自动创建SDG框架的过程。 |
| [^26] | [ChatGPT for GTFS: From Words to Information.](http://arxiv.org/abs/2308.02618) | 本研究探索了使用ChatGPT语言模型从GTFS数据中检索信息的可行性，验证了ChatGPT（GPT-3.5）在GTFS规范理解和信息提取方面的能力。程序合成方法在信息检索任务中表现出更高的准确率，为解决GTFS数据信息获取问题提供了一种有效的方法。 |
| [^27] | [Vehicles Control: Collision Avoidance using Federated Deep Reinforcement Learning.](http://arxiv.org/abs/2308.02614) | 本文研究了使用联邦深度强化学习技术进行碰撞避免的车辆控制问题，并比较了不同算法的效果。结果表明，联邦深度确定性策略梯度算法在优化车辆控制方面表现更好。 |
| [^28] | [Interoperable synthetic health data with SyntHIR to enable the development of CDSS tools.](http://arxiv.org/abs/2308.02613) | 本论文提出了一种利用合成EHR数据开发CDSS工具的体系架构，通过使用SyntHIR系统和FHIR标准实现数据互操作性和工具可迁移性。 |
| [^29] | [Unravelling Responsibility for AI.](http://arxiv.org/abs/2308.02608) | 本文旨在解构人工智能责任的概念，提出了一种包含四种责任意义的有效组合，以支持对人工智能责任的实践推理。 |
| [^30] | [On stable wrapper-based parameter selection method for efficient ANN-based data-driven modeling of turbulent flows.](http://arxiv.org/abs/2308.02602) | 本研究针对复杂的湍流流动现象，提出了一种基于ANN和包装器方法的简化建模方法，通过基于梯度的子集选择指标，在每个消除步骤中最小化总导数的损失或方向一致性。该方法相对于其他方法具有优势，可以有效去除冗余或无关的参数。 |
| [^31] | [Designing a Deep Learning-Driven Resource-Efficient Diagnostic System for Metastatic Breast Cancer: Reducing Long Delays of Clinical Diagnosis and Improving Patient Survival in Developing Countries.](http://arxiv.org/abs/2308.02597) | 这项研究设计了一种基于深度学习的诊断系统，用于转移性乳腺癌的诊断，旨在减少发展中国家临床诊断的长时间延迟，并提高患者的生存率。 |
| [^32] | [SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents.](http://arxiv.org/abs/2308.02594) | 本文提出了一种基于机器学习的安全监测方法SMARLA，用于深度强化学习智能体。该方法设计为黑盒子，利用状态抽象减少状态空间，实现对智能体状态的安全违规预测。经验证，SMARLA具有准确的违规预测能力，并可在智能体执行的早期阶段进行预测。 |
| [^33] | [Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting.](http://arxiv.org/abs/2308.02582) | 该论文介绍了一种通过领域适应和最少到最多提示的方式实现文本到SQL的高效泛化的方法。通过离线抽样获取少量样本，并合成一个通用提示，避免了昂贵的测试时间样本检索，并通过自适应和分解的方法更好地处理跨领域和跨组合式的泛化。 |
| [^34] | [Is GPT-4 a reliable rater? Evaluating Consistency in GPT-4 Text Ratings.](http://arxiv.org/abs/2308.02575) | GPT-4在多个迭代中生成的反馈评分具有高一致性，内容和文体评分之间具有高相关性。 |
| [^35] | [Learning Implicit Entity-object Relations by Bidirectional Generative Alignment for Multimodal NER.](http://arxiv.org/abs/2308.02570) | 本文提出了一种名为BGA-MNER的双向生成对齐方法，用于解决多模态命名实体识别中的两个主要挑战：语义鸿沟和实体-物体关系。实验结果表明，该方法能够有效地捕捉隐式实体-物体关系。 |
| [^36] | [SimTeG: A Frustratingly Simple Approach Improves Textual Graph Learning.](http://arxiv.org/abs/2308.02565) | 这项工作提出了一种令人沮丧地简单的文本图学习方法SimTeG，解决了现有方法在特征工程和模型设计方面的不足。 |
| [^37] | [Food Classification using Joint Representation of Visual and Textual Data.](http://arxiv.org/abs/2308.02562) | 本研究提出了一种使用联合表示的多模态分类框架，通过修改版的EfficientNet和Mish激活函数实现图像分类，使用基于BERT的网络实现文本分类。实验结果表明，所提出的网络在图像和文本分类上表现优于其他方法，准确率提高了11.57%和6.34%。比较分析还证明了所提出方法的效率和鲁棒性。 |
| [^38] | [Large-scale Generative Simulation Artificial Intelligence: the Next Hotspot in Generative AI.](http://arxiv.org/abs/2308.02561) | 大规模生成模拟人工智能是GenAI中的下一个热点，可以解决学习资源有限和过于依赖科学发现经验主义等实际挑战。 |
| [^39] | [The Paradigm Shifts in Artificial Intelligence.](http://arxiv.org/abs/2308.02558) | 本文总结了过去60年来人工智能中发生的范式转变，并讨论了当前人工智能范式转变所面临的紧迫问题和风险。 |
| [^40] | [Knowledge-aware Collaborative Filtering with Pre-trained Language Model for Personalized Review-based Rating Prediction.](http://arxiv.org/abs/2308.02555) | 该论文提出了一种名为知识感知协同过滤与预训练语言模型（KCF-PLM）的方法，用于个性化评论评分预测。该方法利用预训练语言模型来更好地建模用户和物品，并通过开发Transformer网络来考虑评论中的丰富知识。 |
| [^41] | [Collaborative filtering to capture AI user's preferences as norms.](http://arxiv.org/abs/2308.02542) | 本论文提出了一种以协同过滤方法构建规范以捕捉AI用户偏好的新视角。 |
| [^42] | [Towards More Human-like AI Communication: A Review of Emergent Communication Research.](http://arxiv.org/abs/2308.02541) | 本综述综合了紧急沟通研究的最新进展，旨在开发能够超越简单任务，并有效沟通和学习新概念的人工智能代理。 |
| [^43] | [ALE: A Simulation-Based Active Learning Evaluation Framework for the Parameter-Driven Comparison of Query Strategies for NLP.](http://arxiv.org/abs/2308.02537) | ALE是一个用于对比NLP中AL策略的仿真主动学习评估框架，提供实证基础和公正的比较。 |
| [^44] | [Exploring the Role of Explainability in AI-Assisted Embryo Selection.](http://arxiv.org/abs/2308.02534) | 本文探讨了AI辅助胚胎分析模型的可解释性问题，并提出了增加解释性和可信度的指南，旨在推动这项技术在临床实践中得到广泛应用。 |
| [^45] | [Choir Transformer: Generating Polyphonic Music with Relative Attention on Transformer.](http://arxiv.org/abs/2308.02531) | 本论文提出了一种名为Choir Transformer的神经网络模型，通过使用相对位置注意力来生成多声部音乐。实验结果表明，该模型在精度和和声指标方面都超过了之前的最先进方法。 |
| [^46] | [Gated Driver Attention Predictor.](http://arxiv.org/abs/2308.02530) | Gate-DAP是一种用于驾驶员注意力预测的网络连接门控机制，通过学习不同信息源在驾驶场景中的重要性，提高了驾驶任务引导下的交通场景理解和事故预测的能力。 |
| [^47] | [Multiobjective Evolutionary Component Effect on Algorithm behavior.](http://arxiv.org/abs/2308.02527) | 本研究探索了多目标进化算法中最具影响力的组件，通过自动设计出的元启发式算法来提高性能，对不同类型的问题进行了测试和比较。 |
| [^48] | [Chatbot Application to Support Smart Agriculture in Thailand.](http://arxiv.org/abs/2308.02524) | 本研究提出了一种在泰国支持智能农业的聊天机器人应用，为农民提供作物种植知识和建议，通过与智能农业和推荐系统配合使用，为农民提供数据监控和灌溉系统控制的功能。 |
| [^49] | [Evaluating ChatGPT and GPT-4 for Visual Programming.](http://arxiv.org/abs/2308.02522) | 本研究评估了ChatGPT和GPT-4在视觉编程领域的应用，并发现它们具备与基于文本的Python编程相当的高级能力。 |
| [^50] | [Language models as master equation solvers.](http://arxiv.org/abs/2308.02514) | 本研究将语言模型用作求解主方程的机器学习方法，通过设计提示网络和使用强化学习算法训练，实现了对多模组和高维系统的高精度求解。 |
| [^51] | [Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals.](http://arxiv.org/abs/2308.02510) | 本文通过重建观察到的图像来揭示人脑对视觉刺激的知觉机制，并提出了一种名为NeuroImagen的综合管道，利用脑电图信号重建视觉刺激图像。 |
| [^52] | [A Multimodal Supervised Machine Learning Approach for Satellite-based Wildfire Identification in Europe.](http://arxiv.org/abs/2308.02508) | 本研究提出了一种多模态监督机器学习方法，通过利用多个信息源，如热点数据和地表覆盖数据，实现了欧洲卫星火灾的自动识别。 |
| [^53] | [Who Answers It Better? An In-Depth Analysis of ChatGPT and Stack Overflow Answers to Software Engineering Questions.](http://arxiv.org/abs/2308.02312) | 本研究深入分析了ChatGPT和Stack Overflow回答软件工程问题的特点和可用性。结果显示，ChatGPT回答中有52%错误，77%冗长，但由于其综合性和清晰的语言表达，仍然在39.34%的情况下被使用者偏好选择。 |
| [^54] | [OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models.](http://arxiv.org/abs/2308.01390) | OpenFlamingo是一个开源框架，用于训练大型自回归视觉语言模型。它在多个数据集上表现良好，达到了对应模型性能的80%至89%。 |
| [^55] | [LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs.](http://arxiv.org/abs/2308.01157) | LLMs在处理可解释模型方面表现出色，提供了全面的模型级总结和自动化的异常检测、原因描述和修复建议。在医疗保健领域使用广义可加模型作为示例，同时介绍了开源的LLM-GAM接口包$\texttt{TalkToEBM}$。 |
| [^56] | [Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach.](http://arxiv.org/abs/2308.01011) | 本文提出了一种叫做Floss的无监督方法，通过在频域上对学到的表示进行正则化来增强周期性时间序列的表示学习。Floss方法可以自动检测时间序列中的周期性并学习具有周期一致性的有意义的表示。 |
| [^57] | [Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes.](http://arxiv.org/abs/2308.00628) | 这篇论文提出了一个室外多模态多视角多人类姿势数据库Human-M3，并介绍了一种基于多模态数据输入的算法来生成准确的人体姿势。这个数据库解决了现有数据集的不足，提供了更多的数据多样性。 |
| [^58] | [Formally Explaining Neural Networks within Reactive Systems.](http://arxiv.org/abs/2308.00143) | 这项研究在反应式系统中提出了一种基于DNN验证的形式化XAI技术，可以解释DNN的行为，并且通过利用系统的转换约束来计算简洁的解释。 |
| [^59] | [Getting pwn'd by AI: Penetration Testing with Large Language Models.](http://arxiv.org/abs/2308.00121) | 本文探讨了使用大型语言模型（如GPT3.5）作为AI助手来增强渗透测试人员的能力，实现了高级任务规划和低级漏洞寻找两种用例，取得了有前景的初步结果，并就提供该技术的伦理问题进行了讨论。 |
| [^60] | [Distributed Dynamic Programming and an O.D.E. Framework of Distributed TD-Learning for Networked Multi-Agent Markov Decision Processes.](http://arxiv.org/abs/2307.16706) | 本文研究了网络多智能体马尔可夫决策问题中的分布式动态规划和分布式TD学习算法。其中，我们通过引入新的分布式DP算法和分布式TD学习算法，并证明了它们的收敛性，提出了两个关键点。该分布式DP算法具有两个独立的动态系统的特点。 |
| [^61] | [Relation-Oriented: Toward Knowledge-Aligned Causal AI.](http://arxiv.org/abs/2307.16387) | 本研究从创新的关系导向视角出发，探讨了当前的建模范式中的观察模型与实际理解的不对齐问题，并提出了关系定义的表示学习方法作为实现关系导向建模的实践方法。 |
| [^62] | [A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe.](http://arxiv.org/abs/2307.14361) | 本研究提出了一个基于多种机器学习算法和嵌入模型的集成模型，用于基因突变在癌症中的分类。实验结果表明，该模型在准确率、精确率、召回率等指标上优于其他传统和最新的转换器模型，并且具有更高的训练效率。 |
| [^63] | [AI4GCC - Team: Below Sea Level: Critiques and Improvements.](http://arxiv.org/abs/2307.13894) | 这项研究对气候变化评估模型RICE-N进行了批判性分析，发现了关键问题并提出了改进建议，同时也对综合评估模型的特征进行了讨论。研究结果有助于进一步发展RICE-N框架，为政策制定者提供参考。 |
| [^64] | [Argument Attribution Explanations in Quantitative Bipolar Argumentation Frameworks.](http://arxiv.org/abs/2307.13582) | 本文提出了一个新的“论证归因解释（AAEs）”理论，用于确定论证对“主题论证”产生的影响，在定量双极论证框架（QBAFs）中填补了解释定量推理结果的空白。 |
| [^65] | [Imperceptible Physical Attack against Face Recognition Systems via LED Illumination Modulation.](http://arxiv.org/abs/2307.13294) | 本论文提出了一种通过LED照明调制对人脸识别系统进行不可察觉的物理攻击，通过快速强度调制生成难以察觉的亮度变化，并利用图像传感器的卷帘快门效应向捕获的人脸图像中注入亮度信息扰动。 |
| [^66] | [In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning.](http://arxiv.org/abs/2307.12375) | 大型语言模型（LLMs）在包含标签关系示例的上下文中的学习能力使其在下游任务中表现显著提高，但与传统学习方法不同。我们研究了上下文示例中的标签如何影响预测、预训练中学习到的标签关系如何与上下文示例相互作用以及上下文学习如何聚合标签信息。研究结果揭示了LLMs的工作机制及其对上下文信息的处理方式。 |
| [^67] | [Tackling the Curse of Dimensionality with Physics-Informed Neural Networks.](http://arxiv.org/abs/2307.12306) | 本文提出了一种新方法，利用物理信知的神经网络(PINNs)解决高维度的偏微分方程(PDEs)问题，并证明了收敛性和其他期望属性。 |
| [^68] | [TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT.](http://arxiv.org/abs/2307.08674) | TableGPT是一个统一的框架，利用大型语言模型（LLMs）和外部功能命令使LLMs能够无缝地与表格进行交互，实现广泛的功能，并提供便利和可访问性给用户。其中的创新是全局表格表示的概念，使LLMs能够全面理解表格的结构和内容。 |
| [^69] | [Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and Opportunities.](http://arxiv.org/abs/2307.06687) | 走向普遍的语义元宇宙：通过人工智能、时空数据表示、语义物联网和语义增强数字孪生实现的智能、上下文感知的交互技术，在远程教育、工作与协作、娱乐与社交、医疗保健和电子商务营销等领域具有重要应用价值。 |
| [^70] | [Recommender Systems in the Era of Large Language Models (LLMs).](http://arxiv.org/abs/2307.02046) | 大型语言模型在推荐系统中的应用已经带来了显著的改进，克服了传统DNN方法的限制，并提供了强大的语言理解、生成、推理和泛化能力。 |
| [^71] | [Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems.](http://arxiv.org/abs/2307.01292) | 本论文研究了模型服务系统的安全性，通过引入一个查询高效的指纹算法，使得攻击者能够一致地触发任何想要的模型，从而增强了对模型提取攻击的鲁棒性和准确性。 |
| [^72] | [Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution.](http://arxiv.org/abs/2307.00925) | 本研究首次使用语法演化自动设计语义相似性集合，通过自动选择和聚合候选度量来优化集合与人类判断的相关性，提高相似度评估准确性，并证明了使用集合对语义相似性任务的益处。 |
| [^73] | [Elastically-Constrained Meta-Learner for Federated Learning.](http://arxiv.org/abs/2306.16703) | 这项研究提出了一种弹性约束的元学习方法，用于解决联邦学习中由于非独立同分布数据导致元学习的不稳定目标的收敛问题。 |
| [^74] | [Predicting Software Performance with Divide-and-Learn.](http://arxiv.org/abs/2306.06651) | 本文提出了一种名为$DaL$的基于分割学习的方法，用于预测高度配置的软件系统的性能。实验证明了该方法的有效性。 |
| [^75] | [Invisible Image Watermarks Are Provably Removable Using Generative AI.](http://arxiv.org/abs/2306.01953) | 该论文证明了使用生成式AI可以清除隐形图像水印，提出了一种家族化再生攻击方法。通过形式化证明和实证结果，论文展示了所有隐形水印容易受到攻击，并针对一种具有弹性的水印RivaGAN，再生攻击可以去除93-99%的水印。 |
| [^76] | [Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner.](http://arxiv.org/abs/2305.11769) | 本文提出了一种名为JADE的新方法，可以利用易于获取的图像-文本对进行的联合学习，以提升视觉和语言模态的细粒度特征对齐，从而更好地进行视觉问答和密集字幕生成。 |
| [^77] | [Graphologue: Exploring Large Language Model Responses with Interactive Diagrams.](http://arxiv.org/abs/2305.11473) | Graphologue是一个交互式系统，将大型语言模型的基于文本的响应转换为图形化图表以增强其可用性和可解释性，用户可以通过选择和突出显示特定节点和链接来与这些图表进行交互。 |
| [^78] | [The Treachery of Images: Bayesian Scene Keypoints for Deep Policy Learning in Robotic Manipulation.](http://arxiv.org/abs/2305.04718) | 该论文介绍了一种用于机器人操作中的深度策略学习的贝叶斯场景关键点跟踪方法。该方法通过解决图像中的歧义问题，实现了对于对称物体和遮挡和视野之外物体的关键点跟踪，提高了相机观察的效用，对于策略学习具有优势。 |
| [^79] | [Defending against Insertion-based Textual Backdoor Attacks via Attribution.](http://arxiv.org/abs/2305.02394) | 本文提出了一种基于归因的管道AttDef，用于防御两种插入式污染攻击BadNL和InSent，该管道可以成功缓解插入式文本后门攻击并在四个基准数据集上平均提高了56.59%至79.97%和15.25%至48.34%的准确率。 |
| [^80] | [Exploiting Intrinsic Stochasticity of Real-Time Simulation to Facilitate Robust Reinforcement Learning for Robot Manipulation.](http://arxiv.org/abs/2304.06056) | 本文研究了实时模拟的内在随机性特性及其在强化学习中的应用，旨在提高RL方法鲁棒性和域随机化性能。 |
| [^81] | [Gradient Sparsification for Efficient Wireless Federated Learning with Differential Privacy.](http://arxiv.org/abs/2304.04164) | 本文提出了一种基于梯度稀疏化和差分隐私的无线联合学习框架，使用随机稀疏化算法缓解DP引起的性能下降，并减少上传的参数数量，提高训练效率而不损失收敛性能。 |
| [^82] | [Generative Agents: Interactive Simulacra of Human Behavior.](http://arxiv.org/abs/2304.03442) | 本文介绍了一种生成代理的架构，它能够仿真出具有可信度的人类行为，填充交互式沙盒环境，为创造更加真实的人机交互体验提供了一种新的思路。 |
| [^83] | [Towards Open-Vocabulary Video Instance Segmentation.](http://arxiv.org/abs/2304.01715) | 本文提出了新任务--开放词汇视频实例分割，并收集了大规模的LV-VIS数据集，同时提出了高效的MindVLT方法，能够以近实时的速度实现开放集视频实例分割任务，相比现有方法有显著的提升。 |
| [^84] | [Self-Supervised Multimodal Learning: A Survey.](http://arxiv.org/abs/2304.01008) | 自监督多模态学习是一项旨在解决多模态数据中的自监督学习挑战的研究方向。它通过学习来自原始多模态数据中的表示，并解决了没有标签的多模态数据学习、不同模态的融合和不对齐数据学习等问题。 |
| [^85] | [Neuro-Symbolic Execution of Generic Source Code.](http://arxiv.org/abs/2304.00989) | 这项研究提出了一种新的神经模型，能够根据源代码执行泛型程序，并引入了神经符号执行问题。该模型能够执行Py150数据集程序，包括没有具体输入的库函数，并可以用于变量误用定位和修复。 |
| [^86] | [LLM for Patient-Trial Matching: Privacy-Aware Data Augmentation Towards Better Performance and Generalizability.](http://arxiv.org/abs/2303.16756) | 本文提出了一种隐私感知数据增强的LLM-PTM方法，有效地提高了患者-试验匹配的性能和泛化能力。 |
| [^87] | [Democratising AI: Multiple Meanings, Goals, and Methods.](http://arxiv.org/abs/2303.12642) | 这篇论文探讨了AI的民主化，包括四种类型的民主化：AI使用的民主化，AI开发的民主化，AI利润的民主化，和AI治理的民主化。要想实现有效的政策和权衡讨论，需要认识到AI治理的民主化在决策中扮演着重要的角色。 |
| [^88] | [Learning to Incentivize Information Acquisition: Proper Scoring Rules Meet Principal-Agent Model.](http://arxiv.org/abs/2303.08613) | 本文设计了一种样本高效算法，将 UCB 算法（Auer等人，2002）应用于委托代理模型的在线设置，该算法能够通过与策略代理多次互动来设计最优的计分规则，并实现良好的效果。 |
| [^89] | [GOATS: Goal Sampling Adaptation for Scooping with Curriculum Reinforcement Learning.](http://arxiv.org/abs/2303.05193) | 本文提出了一种名为GOATS的方法，使用目标采样自适应课程强化学习技术，通过插值位置目标和数量目标的分布创建学习过程中的课程来解决机器人舀取任务中的位置目标和水量目标问题，取得了比基线更好的表现。 |
| [^90] | [Exploring Efficient-Tuned Learning Audio Representation Method from BriVL.](http://arxiv.org/abs/2303.04585) | 本文提出了一种基于BriVL的稳健音频表示学习方法WavBriVL，通过将音频、图像和文本投影到共享的嵌入空间中，实现多模态应用。实验结果表明，该方法可以通过音频生成适当的图像。 |
| [^91] | [DroNeRF: Real-time Multi-agent Drone Pose Optimization for Computing Neural Radiance Fields.](http://arxiv.org/abs/2303.04322) | DroNeRF是一种实时多代理无人机位姿优化算法，在只使用少量图像的情况下，通过计算物体几何来实现自主定位和3D重建，并结合神经辐射场技术生成独特而动态的新视图。 |
| [^92] | [SG-LSTM: Social Group LSTM for Robot Navigation Through Dense Crowds.](http://arxiv.org/abs/2303.04320) | SG-LSTM模型用于预测人在拥挤环境中的移动，通过社交群组LSTM实现对人群和互动的建模，以更准确地预测轨迹，在机器人导航中提高了碰撞路径的计算速度和准确性。 |
| [^93] | [NovPhy: A Testbed for Physical Reasoning in Open-world Environments.](http://arxiv.org/abs/2303.01711) | NovPhy是一个为了开放世界物理推理而设计的测试平台，要求智能体在存在新颖情况的物理场景中进行推理并相应地采取行动。 |
| [^94] | [Privacy-Preserving Tree-Based Inference with Fully Homomorphic Encryption.](http://arxiv.org/abs/2303.01254) | 本研究介绍了一种基于全同态加密的数据隐私保护方法，能够针对加密表格数据进行任意计算，并得到了最新的解决方案，适用于一系列树型模型，包括决策树，随机森林和梯度增强树。此方法已应用在Concrete-ML开源库中，能够在准确性方面接近未受保护的版本。 |
| [^95] | [Empirical analysis of Different Dimensionality Reduction and classification Techniques for Epileptic Seizure detection.](http://arxiv.org/abs/2302.12012) | 本研究对使用不同降维和分类技术进行癫痫发作检测进行了实证分析，通过离散小波变换和机器学习分类器，结合主成分分析、独立成分分析和线性判别分析等降维算法，选择特征来提高检测准确性。 |
| [^96] | [Heterogeneous Neuronal and Synaptic Dynamics for Spike-Efficient Unsupervised Learning: Theory and Design Principles.](http://arxiv.org/abs/2302.11618) | 本文研究了异质神经元和突触动力学对于脉冲减少网络的影响。分析结果表明，异质性能够提高网络的记忆容量和预测性能，同时降低脉冲活动。实验结果证明了优化的异质脉冲减少网络的性能提升和脉冲活动减少的效果。 |
| [^97] | [ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms.](http://arxiv.org/abs/2302.11408) | 该论文研究了在多种深度学习范式中实现鲁棒的后门数据检测，并发现现有的检测方法在不同攻击和毒害比例下的性能变化很大，且不能应用于最新的干净标签攻击，以及自监督学习和迁移学习中性能损失较大。为此，论文提出了一种名为ASSET的新的检测方法，通过主动引导不同的模型行为来促进后门和干净样本的分离。 |
| [^98] | [Selective Explanations: Leveraging Human Input to Align Explainable AI.](http://arxiv.org/abs/2301.09656) | 本研究提出一种通过利用人类输入生成选择性解释的通用框架，以弥合可解释人工智能（XAI）与人类解释的差距，并且在决策支持任务中进行了实验证明其有效性。 |
| [^99] | [TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World.](http://arxiv.org/abs/2301.05880) | TikTalk是一个基于视频的多模态对话数据集，用于研究智能且类似人类的闲聊机器人。数据集包含从流行视频分享平台收集的38K个视频和367K个用户对话。与其他数据集相比，TikTalk提供了更丰富的上下文类型，同时也增加了从复杂的多模态信息中生成个性化回答的难度。数据集中还更频繁地引用了外部知识，为多模态对话模型提供了新的挑战。 |
| [^100] | [Phase-shifted Adversarial Training.](http://arxiv.org/abs/2301.04785) | 本论文通过分析响应频率的视角，发现对抗训练导致神经网络收敛性较低，从而在每个数据附近产生高度振荡的预测。为了有效地学习高频内容，提出了相位偏移对抗训练(PhaseAT)方法。 |
| [^101] | [Reasoning with Language Model Prompting: A Survey.](http://arxiv.org/abs/2212.09597) | 本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。 |
| [^102] | [CLIP: Train Faster with Less Data.](http://arxiv.org/abs/2212.01452) | 本文提出了CLIP，通过结合课程学习和数据集修剪的方法，在深度学习模型的训练中使用更少的数据，实现更快的收敛速度和更好的泛化能力。 |
| [^103] | [Crowd Density Estimation using Imperfect Labels.](http://arxiv.org/abs/2212.01450) | 本文研究了不完整标签对人群计数准确性的影响，并提出了一种系统，利用深度学习模型自动生成不完整标签，并将其用于训练新的人群计数模型。实验证明，所提出的方案的准确性接近于完美标签数据集的准确性。 |
| [^104] | [Offline Supervised Learning V.S. Online Direct Policy Optimization: A Comparative Study and A Unified Training Paradigm for Neural Network-Based Optimal Feedback Control.](http://arxiv.org/abs/2211.15930) | 本文对离线监督学习和在线直接策略优化进行了比较研究，结果突出了离线监督学习在最优性和训练时间方面的优势。为了克服两种方法中的主要挑战，我们提出了预训练和微调策略作为一种统一的训练范式。 |
| [^105] | [DroneNet: Crowd Density Estimation using Self-ONNs for Drones.](http://arxiv.org/abs/2211.07137) | 使用自组织神经网络的无人机进行人群密度估计的模型（DroneNet），相比于使用CNN的模型具有更高的计算效率，能够在保持准确性的前提下降低推断时间。 |
| [^106] | [Towards trustworthy multi-modal motion prediction: Holistic evaluation and interpretability of outputs.](http://arxiv.org/abs/2210.16144) | 本研究旨在设计可信赖的运动预测系统，通过综合评估指标、提高鲁棒性和输出的可解释性。研究分析了当前基准的主要缺陷，并提出了新的整体评估框架。 |
| [^107] | [Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction.](http://arxiv.org/abs/2207.14116) | Claim-Dissector是一款联合重排和真实性预测的可解释的事实核查系统，可以识别与声明相关的证据，并确定声明的真实性。该系统的个人贡献以及证据所支持或反驳声明的贡献都可以被识别。 |
| [^108] | [QSAN: A Near-term Achievable Quantum Self-Attention Network.](http://arxiv.org/abs/2207.07563) | 本文提出了一种量子自注意力网络（QSAN），用于近期量子设备上的图像分类任务。该网络利用量子自注意力机制来增强数据表示能力，并设计了对应的一步实现和量子电路框架。 |
| [^109] | [FairGrad: Fairness Aware Gradient Descent.](http://arxiv.org/abs/2206.10923) | FairGrad是一种公平感知梯度下降方法，通过重新加权方案迭代学习群体特定权重来实现群体公平性。它易于实现，适用于各种标准的公平性定义，并且在各种数据集上与标准基线方法具有竞争力。 |
| [^110] | [CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to multiple Real-World Domains.](http://arxiv.org/abs/2206.08083) | 本论文提出了一个适用于2D车道检测的无监督域适应基准- CARLANE，包括模拟到多个真实世界领域的三个不同数据集。该基准数据集涵盖了多样的场景，并包含大量有注释的图片。此外，研究还提出了系统基准，用以评估方法的性能。 |
| [^111] | [A Search-Based Testing Approach for Deep Reinforcement Learning Agents.](http://arxiv.org/abs/2206.07813) | 本文提出一种基于搜索的测试方法，探索状态空间以检测DRL代理的安全性，并在三个基准测试中取得了比基线方法更高的状态空间覆盖率。 |
| [^112] | [Variational Meta Reinforcement Learning for Social Robotics.](http://arxiv.org/abs/2206.03211) | 本研究探讨了变分元元强化学习在社交机器人中的应用，通过选择合适的奖励函数，可快速适应不同环境，提高机器人的社交技能。 |
| [^113] | [Class-incremental Learning with Pre-allocated Fixed Classifiers.](http://arxiv.org/abs/2010.08657) | 本文提出了一种具有预分配固定分类器的类增量学习方法，通过利用存储在情节性记忆中的过去数据，并在学习阶段的开始就将一些预分配的输出节点纳入分类损失的计算，解决了神经网络在类增量学习中遗忘先前知识的问题。 |
| [^114] | [Multi-Agent Trust Region Policy Optimization.](http://arxiv.org/abs/2010.07916) | 我们提出了一种名为多智能体 TRPO (MATRPO) 的分散式 MARL 算法，它可以在多智能体协作任务上优化分布式策略，并且无需智能体之间共享观测、奖励、策略或值/动作值函数。该算法在两个合作游戏上展示了出色的性能。 |
| [^115] | [Multi-Purchase Behavior: Modeling, Estimation and Optimization.](http://arxiv.org/abs/2006.08055) | 本研究提出了一种名为Bundle-MVL-K的多次购买选择模型家族，并开发了一种迭代策略来高效地计算出该模型的优化推荐。这是对多次购买选择模型进行操作化的首次尝试之一，并首次建立了多次购买行为建模与收入增益之间的定量联系。 |

# 详细

[^1]: 在医疗领域的元学习：一项调查研究。

    Meta-learning in healthcare: A survey. (arXiv:2308.02877v1 [cs.LG])

    [http://arxiv.org/abs/2308.02877](http://arxiv.org/abs/2308.02877)

    元学习在医疗领域有广泛应用，可以解决医疗挑战，如样本不足和数据收集差异。主要包括多/单任务学习和多/少样本学习方法。

    

    作为机器学习的一个子集，元学习旨在通过利用先前的知识和经验来提高模型的能力。元学习范式可以适当地解决传统学习方法所面临的挑战，如样本数量不足、领域转移和泛化问题。这些独特的特点使元学习成为在各种医疗环境中开发有影响力的解决方案的合适选择，这些环境中可用数据通常不足，并且数据收集方法也不同。本调查讨论了元学习在医疗领域的广泛应用，以了解它如何以及在哪些方面可以解决关键的医疗挑战。我们首先描述了元学习的理论基础和关键方法。然后将在医疗领域应用的元学习方法分为多/单任务学习和多/少样本学习两大类。

    As a subset of machine learning, meta-learning, or learning to learn, aims at improving the model's capabilities by employing prior knowledge and experience. A meta-learning paradigm can appropriately tackle the conventional challenges of traditional learning approaches, such as insufficient number of samples, domain shifts, and generalization. These unique characteristics position meta-learning as a suitable choice for developing influential solutions in various healthcare contexts, where the available data is often insufficient, and the data collection methodologies are different. This survey discusses meta-learning broad applications in the healthcare domain to provide insight into how and where it can address critical healthcare challenges. We first describe the theoretical foundations and pivotal methods of meta-learning. We then divide the employed meta-learning approaches in the healthcare domain into two main categories of multi/single-task learning and many/few-shot learning a
    
[^2]: 半监督学习用于视频胶囊内镜中出血区域的分割

    Semi-supervised Learning for Segmentation of Bleeding Regions in Video Capsule Endoscopy. (arXiv:2308.02869v1 [cs.CV])

    [http://arxiv.org/abs/2308.02869](http://arxiv.org/abs/2308.02869)

    该论文提出了一种半监督学习方法，用于视频胶囊内镜中出血区域的分割。通过采用“平均教师”方法，构建了一个学生 U-Net 模型和一个教师模型，实现了对VCE图像中出血区域的准确识别。

    

    在现代诊断技术领域，视频胶囊内镜(VCE)因其在诊断各种胃肠道疾病，包括隐性出血等方面的高效性和非侵入性而脱颖而出。对于成功诊断和治疗这些疾病，准确识别VCE图像中的出血区域至关重要。虽然基于深度学习的方法已经成为VCE图像自动分析的强大工具，但它们通常需要大量具有全面标注的训练数据集。获取这些标记数据集往往耗时、昂贵，并且需要丰富的领域专业知识。为了解决这个问题，我们采用了半监督学习(SSL)方法来对VCE中的出血区域进行分割。通过采用“平均教师”方法，我们构建了一个配备了scSE注意力模块的学生U-Net模型，以及一个相同架构的教师模型。这些模型的参数是交替更新的。

    In the realm of modern diagnostic technology, video capsule endoscopy (VCE) is a standout for its high efficacy and non-invasive nature in diagnosing various gastrointestinal (GI) conditions, including obscure bleeding. Importantly, for the successful diagnosis and treatment of these conditions, accurate recognition of bleeding regions in VCE images is crucial. While deep learning-based methods have emerged as powerful tools for the automated analysis of VCE images, they often demand large training datasets with comprehensive annotations. Acquiring these labeled datasets tends to be time-consuming, costly, and requires significant domain expertise. To mitigate this issue, we have embraced a semi-supervised learning (SSL) approach for the bleeding regions segmentation within VCE. By adopting the `Mean Teacher' method, we construct a student U-Net equipped with an scSE attention block, alongside a teacher model of the same architecture. These models' parameters are alternately updated th
    
[^3]: 用安排取代评分：一种用于学习排序的上下文集合到排列框架

    Replace Scoring with Arrangement: A Contextual Set-to-Arrangement Framework for Learning-to-Rank. (arXiv:2308.02860v1 [cs.IR])

    [http://arxiv.org/abs/2308.02860](http://arxiv.org/abs/2308.02860)

    这篇论文提出了一种新的学习排序框架，名为STARank，它通过直接生成候选项目排列来替代个别评分和排序操作，并且是端到端可微分的。

    

    学习排序是top-N推荐任务中的核心技术，理想的排名器应该是一个从项目集合到排列（即排列）的映射。现有的大多数解决方案属于概率排序原则（PRP）范式，即首先对候选集中的每个项目进行评分，然后执行排序操作以生成排名列表。然而，这些方法忽视了个体评分过程中候选项目之间的上下文依赖性，并且排序操作是不可微分的。为了解决上述问题，我们提出了一种名为STARank的集合到排列排序框架，它直接生成候选项目的排列，而不需要进行个别评分和排序操作，并且是端到端可微分的。因此，STARank可以在只有真实排列可访问但没有项目的真实相关度分数的情况下运行。为此，STARank首先阅读候选项目...

    Learning-to-rank is a core technique in the top-N recommendation task, where an ideal ranker would be a mapping from an item set to an arrangement (a.k.a. permutation). Most existing solutions fall in the paradigm of probabilistic ranking principle (PRP), i.e., first score each item in the candidate set and then perform a sort operation to generate the top ranking list. However, these approaches neglect the contextual dependence among candidate items during individual scoring, and the sort operation is non-differentiable. To bypass the above issues, we propose Set-To-Arrangement Ranking (STARank), a new framework directly generates the permutations of the candidate items without the need for individually scoring and sort operations; and is end-to-end differentiable. As a result, STARank can operate when only the ground-truth permutations are accessible without requiring access to the ground-truth relevance scores for items. For this purpose, STARank first reads the candidate items in t
    
[^4]: feather -- 一个用于分享和部署模型的Python SDK

    feather -- a Python SDK to share and deploy models. (arXiv:2308.02838v1 [cs.AI])

    [http://arxiv.org/abs/2308.02838](http://arxiv.org/abs/2308.02838)

    feather是一个Python SDK，可以帮助模型开发人员用很少的代码构建可共享的用户界面，并提供URL和API端点供其他人访问和使用模型。

    

    在其核心，feather是一个工具，允许模型开发人员仅用不到20行代码构建可共享的用户界面。使用Python SDK，开发人员可以指定用户将与之交互的视觉组件（例如，FileUpload组件，允许用户上传文件）。我们的服务提供了以下功能：1）URL，允许其他人通过用户界面进行可视化访问和使用模型；2）API端点，允许对模型发出编程请求。在本文中，我们讨论了feather的动机和我们打算为AI研究人员和开发人员提供的价值。例如，该SDK支持多步骤模型，并可扩展以对保留数据集进行自动评估。我们还提供了详尽的技术和实现细节。注意，feather目前是一个休眠项目。我们已经将我们的代码开源用于研究目的：https://github.com/feather-ai/

    At its core, feather was a tool that allowed model developers to build shareable user interfaces for their models in under 20 lines of code. Using the Python SDK, developers specified visual components that users would interact with. (e.g. a FileUpload component to allow users to upload a file). Our service then provided 1) a URL that allowed others to access and use the model visually via a user interface; 2) an API endpoint to allow programmatic requests to a model. In this paper, we discuss feather's motivations and the value we intended to offer AI researchers and developers. For example, the SDK can support multi-step models and can be extended to run automatic evaluation against held out datasets. We additionally provide comprehensive technical and implementation details.  N.B. feather is presently a dormant project. We have open sourced our code for research purposes: https://github.com/feather-ai/
    
[^5]: 基于物理交互因果序列的任务生成方法

    Physics-Based Task Generation Through Causal Sequence of Physical Interactions. (arXiv:2308.02835v1 [cs.AI])

    [http://arxiv.org/abs/2308.02835](http://arxiv.org/abs/2308.02835)

    本文提出了一种基于物理交互因果序列的任务生成方法，使得人工智能系统能够在模拟物理环境中执行任务并评估其物理推理能力。我们使用“愤怒的小鸟”游戏作为示例，通过一系列指标对生成的任务进行评估。

    

    在真实世界中，对于使用人工智能系统来执行物理环境中的任务是一个至关重要却具有挑战性的问题。物理模拟任务通常被用来促进解决这一挑战的研究。本文首先提出了一种系统的方法，使用物体之间的物理交互的因果序列来定义物理场景。然后，我们提出了一种利用这些定义的场景作为输入，在模拟物理环境中生成任务的方法学。我们的方法可以更好地理解解决基于物理的任务所需的微观力学，从而为评估人工智能系统的物理推理能力提供准确的评估。我们使用基于物理的益智游戏“愤怒的小鸟”来演示我们提出的任务生成方法，并使用一系列指标对生成的任务进行评估，包括物理稳定性、使用预期物理相互作用的可解性以及使用意外相互作用的偶然可解性。

    Performing tasks in a physical environment is a crucial yet challenging problem for AI systems operating in the real world. Physics simulation-based tasks are often employed to facilitate research that addresses this challenge. In this paper, first, we present a systematic approach for defining a physical scenario using a causal sequence of physical interactions between objects. Then, we propose a methodology for generating tasks in a physics-simulating environment using these defined scenarios as inputs. Our approach enables a better understanding of the granular mechanics required for solving physics-based tasks, thereby facilitating accurate evaluation of AI systems' physical reasoning capabilities. We demonstrate our proposed task generation methodology using the physics-based puzzle game Angry Birds and evaluate the generated tasks using a range of metrics, including physical stability, solvability using intended physical interactions, and accidental solvability using unintended s
    
[^6]: 带有概率模型检验的多智能体验证与控制

    Multi-Agent Verification and Control with Probabilistic Model Checking. (arXiv:2308.02829v1 [cs.LO])

    [http://arxiv.org/abs/2308.02829](http://arxiv.org/abs/2308.02829)

    概率模型检验技术已经扩展到多智能体交互领域，可用于灵活地推理智能体的协作或对抗行为，在人工智能、机器人和自主系统等领域有广泛应用。

    

    概率模型检验是一种用于对在不确定性或随机性环境中运作的软件或硬件系统进行形式化自动推理的技术。它建立在从逻辑、自动机和图论到优化、数值方法和控制的各个领域的思想和技术上。近年来，概率模型检验还扩展到集成博弈论的思想，特别是使用诸如随机博弈和均衡概念等模型，从而形式化地验证具有不同目标的多个理性智能体的交互。这提供了一种灵活地思考智能体以敌对或协作方式行动的方式，并为解决人工智能、机器人和自主系统等领域的新问题提供了机会。在本文中，我们总结了这一领域的一些进展，并强调了它们已经被使用的应用。我们讨论了如何

    Probabilistic model checking is a technique for formal automated reasoning about software or hardware systems that operate in the context of uncertainty or stochasticity. It builds upon ideas and techniques from a diverse range of fields, from logic, automata and graph theory, to optimisation, numerical methods and control. In recent years, probabilistic model checking has also been extended to integrate ideas from game theory, notably using models such as stochastic games and solution concepts such as equilibria, to formally verify the interaction of multiple rational agents with distinct objectives. This provides a means to reason flexibly about agents acting in either an adversarial or a collaborative fashion, and opens up opportunities to tackle new problems within, for example, artificial intelligence, robotics and autonomous systems. In this paper, we summarise some of the advances in this area, and highlight applications for which they have already been used. We discuss how the 
    
[^7]: 一个用于解决几何问题的符号感知模型

    A Symbolic Character-Aware Model for Solving Geometry Problems. (arXiv:2308.02823v1 [cs.CV])

    [http://arxiv.org/abs/2308.02823](http://arxiv.org/abs/2308.02823)

    本文提出了一个符号感知模型，用于解决几何问题。该模型通过探索符号字符在文本和图表理解中的作用，以及在多模态推理框架下对模型进行优化来提高解决几何问题的能力。

    

    AI在解决数学问题方面取得了重大进展，但由于几何问题依赖于文本和图表，因此仍然具有挑战性。在文本描述中，符号字符（如“$\triangle$ABC”）通常作为连接相应图表的桥梁。然而，通过将符号字符仅仅分解为单个字母（如'A'、'B'和'C'），现有的方法未能明确研究它们，从而失去了与图表的语义关系。在本文中，我们开发了一个符号感知模型，以完全探索这些字符在文本和图表理解中的作用，并在多模态推理框架下对模型进行优化。在文本编码器方面，我们提出将个别符号字符与相应图表的几何信息合并形成一个语义单元。对于图表编码器，我们使用符号字符作为标签，在多标签分类框架下对其进行预训练。此外，我们增强了模型的目标函数，使其在文本和图表之间进行几何一致性约束的训练。

    AI has made significant progress in solving math problems, but geometry problems remain challenging due to their reliance on both text and diagrams. In the text description, symbolic characters such as "$\triangle$ABC" often serve as a bridge to connect the corresponding diagram. However, by simply tokenizing symbolic characters into individual letters (e.g., 'A', 'B' and 'C'), existing works fail to study them explicitly and thus lose the semantic relationship with the diagram. In this paper, we develop a symbolic character-aware model to fully explore the role of these characters in both text and diagram understanding and optimize the model under a multi-modal reasoning framework. In the text encoder, we propose merging individual symbolic characters to form one semantic unit along with geometric information from the corresponding diagram. For the diagram encoder, we pre-train it under a multi-label classification framework with the symbolic characters as labels. In addition, we enha
    
[^8]: 基于新的骨密度定义和计算机断层扫描进行人骨密度随年龄变化的研究

    The changing rule of human bone density with aging based on a novel definition and mensuration of bone density with computed tomography. (arXiv:2308.02815v1 [physics.med-ph])

    [http://arxiv.org/abs/2308.02815](http://arxiv.org/abs/2308.02815)

    本研究提出了一种新的骨密度定义和测量方法，利用计算机断层扫描图像中的骨像素分布来测量骨密度。研究发现，在成年期39岁到80岁之间，骨密度呈线性下降，女性下降速度大约是男性的1.6倍，与现有观点相矛盾。这对于了解人体衰老过程具有重要意义。

    

    随着人口老龄化，骨质疏松症和易碎性骨折已成为重要的公共卫生问题。然而，使用双能X射线吸收法测量骨密度在个性化风险评估方面受到各种因素的干扰。本研究提出了一种创新的统计模型，该模型利用细分的计算机断层扫描图像中的骨像素分布来测量骨密度。我们的研究结果表明，在39岁到80岁之间的成年期，骨密度呈线性下降，女性下降速度大约是男性的1.6倍。这与广泛接受的观点相矛盾，即骨密度在女性绝经期开始下降，在男性大约50岁时开始下降。年龄相关变化的线性性提供了对人体老化动态的进一步洞察。

    Osteoporosis and fragility fractures have emerged as major public health concerns in an aging population. However, measuring age-related changes in bone density using dual-energy X-ray absorptiometry has limited personalized risk assessment due to susceptibility to interference from various factors. In this study, we propose an innovative statistical model of bone pixel distribution in fine-segmented computed tomography (CT) images, along with a novel approach to measuring bone density based on CT values of bone pixels. Our findings indicate that bone density exhibits a linear decline with age during adulthood between the ages of 39 and 80, with the rate of decline being approximately 1.6 times faster in women than in men. This contradicts the widely accepted notion that bone density starts declining in women at menopause and in men at around 50 years of age. The linearity of age-related changes provides further insights into the dynamics of the aging human body. Consequently, our find
    
[^9]: 分子通信的人工智能技术

    Artificial Intelligence for Molecular Communication. (arXiv:2308.02812v1 [cs.ET])

    [http://arxiv.org/abs/2308.02812](http://arxiv.org/abs/2308.02812)

    人工智能用于分子通信，这是一种在微型设备之间进行数据传输的新方法，具有广阔的医学应用潜力。分子通信通过转化数字信号为分子浓度来进行，解调信号时面临着精确建模困难的挑战。

    

    分子通信是一种在微型设备之间进行数据传输的新方法，特别适用于需要避免电信号的情况。这种通信是通过在纳米级别上发送分子（或其他粒子）而不是通过电线发送电子来实现的。分子通信设备在医学应用中具有巨大的潜力，因为它们为尺寸、温度或辐射约束下可能不适用的天线传输系统提供了替代方案。通信通过将数字信号转化为分子浓度来实现。然后在通信信道的另一端检测到这些分子，并将其转化回数字信号。由于传输信道的精确建模通常是不可能的，这可能是由于缺乏数据或信道的时变参数（例如，佩戴医疗设备的人员的运动）所致。这使得信号的解调非常困难。

    Molecular communication is a novel approach for data transmission between miniaturized devices, especially in contexts where electrical signals are to be avoided. The communication is based on sending molecules (or other particles) at nano scale through channel instead sending electrons over a wire. Molecular communication devices have a large potential in medical applications as they offer an alternative to antenna-based transmission systems that may not be applicable due to size, temperature, or radiation constraints. The communication is achieved by transforming a digital signal into concentrations of molecules. These molecules are then detected at the other end of the communication channel and transformed back into a digital signal. Accurately modeling the transmission channel is often not possible which may be due to a lack of data or time-varying parameters of the channel (e. g., the movements of a person wearing a medical device). This makes demodulation of the signal very diffi
    
[^10]: 一种用于空间-时间野火预测的生成模型

    A generative model for surrogates of spatial-temporal wildfire nowcasting. (arXiv:2308.02810v1 [cs.LG])

    [http://arxiv.org/abs/2308.02810](http://arxiv.org/abs/2308.02810)

    本文提出了一种生成模型，利用三维量化变分自编码器来生成给定生态区域内未见的空间-时间野火燃烧区域序列。该模型在加利福尼亚州的Chimney火灾生态区域中进行了测试，结果表明该模型能够成功生成连贯且结构良好的火势情景。

    

    近年来全球野火数量的增加导致了对实时火势预测的需求。传统的物理模型，如元胞自动机和计算流体力学，可以提供高保真度的火势传播模拟，但耗时且计算复杂。目前已经有很多研究致力于开发机器学习模型进行火势预测，但这些模型往往特定于某个地区，并需要大量的模拟数据进行训练，这在不同生态区域中需要大量的计算工作。本文提出了一种生成模型，利用三维量化变分自编码器来生成给定生态区域内未见的空间-时间野火燃烧区域序列。该模型在加利福尼亚州最近的一次大规模野火事件 - Chimney火灾的生态区域中进行了测试。数值结果表明，该模型成功生成了连贯且有结构的火势情景。

    Recent increase in wildfires worldwide has led to the need for real-time fire nowcasting. Physics-driven models, such as cellular automata and computational fluid dynamics can provide high-fidelity fire spread simulations but they are computationally expensive and time-consuming. Much effort has been put into developing machine learning models for fire prediction. However, these models are often region-specific and require a substantial quantity of simulation data for training purpose. This results in a significant amount of computational effort for different ecoregions. In this work, a generative model is proposed using a three-dimensional Vector-Quantized Variational Autoencoders to generate spatial-temporal sequences of unseen wildfire burned areas in a given ecoregion. The model is tested in the ecoregion of a recent massive wildfire event in California, known as the Chimney fire. Numerical results show that the model succeed in generating coherent and structured fire scenarios, ta
    
[^11]: MiAMix: 通过多阶段增强混合样本数据增强方法提升图像分类

    MiAMix: Enhancing Image Classification through a Multi-stage Augmented Mixied Sample Data Augmentation Method. (arXiv:2308.02804v1 [cs.CV])

    [http://arxiv.org/abs/2308.02804](http://arxiv.org/abs/2308.02804)

    MiAMix是一种新型的混合样本数据增强方法，通过将图像增强集成到混合框架中并利用多种多样的混合方法来提升图像分类模型的性能和泛化能力。

    

    尽管深度学习领域取得了相当大的进展，但过拟合依然是一个关键的挑战。数据增强作为一种十分有前景的方法，因其能够增强模型在各种计算机视觉任务中的泛化能力而备受关注。虽然已经提出了各种各样的策略，但混合样本数据增强（MSDA）在增强模型性能和泛化能力方面显示出了巨大潜力。我们引入了一种称为MiAMix的新型混合方法，即多阶段增强混合。MiAMix将图像增强集成到混合框架中，同时利用多种多样的混合方法，并通过随机选择混合掩模增强方法来改进混合方法。最近的方法利用了显著性信息，而MiAMix的设计也考虑到了计算效率，减少了额外的开销，并且可以轻松集成到现有的训练流程中。我们对MiAMix进行了全面的评估，使用了四个图像基准和进行了比较。

    Despite substantial progress in the field of deep learning, overfitting persists as a critical challenge, and data augmentation has emerged as a particularly promising approach due to its capacity to enhance model generalization in various computer vision tasks. While various strategies have been proposed, Mixed Sample Data Augmentation (MSDA) has shown great potential for enhancing model performance and generalization. We introduce a novel mixup method called MiAMix, which stands for Multi-stage Augmented Mixup. MiAMix integrates image augmentation into the mixup framework, utilizes multiple diversified mixing methods concurrently, and improves the mixing method by randomly selecting mixing mask augmentation methods. Recent methods utilize saliency information and the MiAMix is designed for computational efficiency as well, reducing additional overhead and offering easy integration into existing training pipelines. We comprehensively evaluate MiaMix using four image benchmarks and pit
    
[^12]: 在异构时间MMMA图上进行众包欺诈检测

    Crowdsourcing Fraud Detection over Heterogeneous Temporal MMMA Graph. (arXiv:2308.02793v1 [cs.SI])

    [http://arxiv.org/abs/2308.02793](http://arxiv.org/abs/2308.02793)

    本文提出了一种新颖的对比多视图学习方法CMT，用于在异构时间MMMA图上进行众包欺诈检测。CMT以自我监督的方式捕捉图的异质性和动态性，并生成高质量的表示。实验证明，CMT在代表性MMMA微信的行业规模HTG上表现出色，也显示出在大规模公共金融HTG上有希望的结果，可应用于其他图异常检测任务。

    

    点击农场业务的兴起利用多用途消息移动应用程序（MMMA）诱使网络犯罪分子进行众包欺诈，给点击农场工人造成财务损失。在本文中，我们提出了一种名为CMT的新型对比多视图学习方法，用于在MMMA的异构时间图（HTG）上进行众包欺诈检测。CMT以自我监督的方式捕捉HTG的异质性和动态性，并生成用于众包欺诈检测的高质量表征。我们使用CMT在代表性MMMA微信的行业规模HTG上检测众包欺诈，并且其表现明显优于其他方法。CMT还显示出在大规模公共金融HTG上进行欺诈检测的有希望的结果，表明它可以应用于其他图异常检测任务。

    The rise of the click farm business using Multi-purpose Messaging Mobile Apps (MMMAs) tempts cybercriminals to perpetrate crowdsourcing frauds that cause financial losses to click farm workers. In this paper, we propose a novel contrastive multi-view learning method named CMT for crowdsourcing fraud detection over the heterogeneous temporal graph (HTG) of MMMA. CMT captures both heterogeneity and dynamics of HTG and generates high-quality representations for crowdsourcing fraud detection in a self-supervised manner. We deploy CMT to detect crowdsourcing frauds on an industry-size HTG of a representative MMMA WeChat and it significantly outperforms other methods. CMT also shows promising results for fraud detection on a large-scale public financial HTG, indicating that it can be applied in other graph anomaly detection tasks.
    
[^13]: 通过混合量子-经典方法解决以物流为导向的装箱问题

    Solving Logistic-Oriented Bin Packing Problems Through a Hybrid Quantum-Classical Approach. (arXiv:2308.02787v1 [cs.AI])

    [http://arxiv.org/abs/2308.02787](http://arxiv.org/abs/2308.02787)

    这项研究通过混合量子-经典方法解决了物流中以实际为导向的装箱问题，并扩展了解决不同维度问题和具有不同要求的箱子的能力。

    

    装箱问题是一个具有广泛工业应用的经典问题。事实上，将物品高效地装箱是许多物流公司面临的最艰巨的挑战之一，也是降低存储成本或改善车辆空间分配的关键问题。本文采用了我们先前发表的量子-经典框架Q4RealBPP，并详细阐述了解决以实际为导向的装箱问题的方法。为此，本文重点关注以下特点：i）存在不同类型的箱子，ii）扩展框架以解决三维、二维和一维问题，iii）物品与箱子关联的要求，iv）交付优先级。本文对所有这些特点进行了测试，以及Q4RealBPP解决以实际为导向的装箱问题的能力。

    The Bin Packing Problem is a classic problem with wide industrial applicability. In fact, the efficient packing of items into bins is one of the toughest challenges in many logistic corporations and is a critical issue for reducing storage costs or improving vehicle space allocation. In this work, we resort to our previously published quantum-classical framework known as Q4RealBPP, and elaborate on the solving of real-world oriented instances of the Bin Packing Problem. With this purpose, this paper gravitates on the following characteristics: i) the existence of heterogeneous bins, ii) the extension of the framework to solve not only three-dimensional, but also one- and two-dimensional instances of the problem, iii) requirements for item-bin associations, and iv) delivery priorities. All these features have been tested in this paper, as well as the ability of Q4RealBPP to solve real-world oriented instances.
    
[^14]: 半监督对比回归方法用于眼球注视估计

    Semi-supervised Contrastive Regression for Estimation of Eye Gaze. (arXiv:2308.02784v1 [cs.CV])

    [http://arxiv.org/abs/2308.02784](http://arxiv.org/abs/2308.02784)

    本文提出了一种用于眼球注视估计的半监督对比回归方法。该方法利用小规模标记的数据集来寻找泛化解决方案，能够在未见面部图像上进行准确的估计。通过引入新的对比损失范式，该方法在性能上表现出色。

    

    随着智能系统对人机界面的需求不断增长，开发眼球控制系统已成为必要。眼球注视作为一种非侵入性的人机交互方式，是最适合的方法之一。基于外观的深度学习模型是眼球注视估计中最常用的方法。但是，这些模型的性能完全受到标记的眼球注视数据集的大小的影响，并且这种影响会影响性能的泛化能力。本文旨在开发一种用于眼球注视方向估计的半监督对比学习框架。在标记的眼球注视数据集较小的情况下，该框架能够找到一个泛化的解决方案，即使是看不见的人脸图像也能进行估计。我们提出了一种新的对比损失范式，既最大化了相似图片之间的相似性，同时减少了嵌入表示中的冗余。我们的对比回归框架在性能上表现良好，优于其他一些最先进的方法。

    With the escalated demand of human-machine interfaces for intelligent systems, development of gaze controlled system have become a necessity. Gaze, being the non-intrusive form of human interaction, is one of the best suited approach. Appearance based deep learning models are the most widely used for gaze estimation. But the performance of these models is entirely influenced by the size of labeled gaze dataset and in effect affects generalization in performance. This paper aims to develop a semi-supervised contrastive learning framework for estimation of gaze direction. With a small labeled gaze dataset, the framework is able to find a generalized solution even for unseen face images. In this paper, we have proposed a new contrastive loss paradigm that maximizes the similarity agreement between similar images and at the same time reduces the redundancy in embedding representations. Our contrastive regression framework shows good performance in comparison to several state of the art con
    
[^15]: 基于模拟到真实转移学习的深度强化学习用于ORC超热控制

    Surrogate Empowered Sim2Real Transfer of Deep Reinforcement Learning for ORC Superheat Control. (arXiv:2308.02765v1 [eess.SY])

    [http://arxiv.org/abs/2308.02765](http://arxiv.org/abs/2308.02765)

    本文提出了一种基于模拟到真实转移学习的深度强化学习控制方法，用于 ORC 超热控制。该方法旨在提供一种新的简单、可行且用户友好的解决方案。

    

    有机朗肯循环(ORC)由于其简单的结构和易于维护，被广泛应用于工业余热回收中。然而，在过程工业的智能制造环境下，传统的基于模型的优化控制方法无法适应ORC系统中不断变化的工况或突发工作模式的改变。深度强化学习(DRL)在存在不确定性的情况下具有显著优势，它通过与环境进行交互来直接实现控制目标，而无需对受控系统进行精确建模。然而，直接将DRL应用于物理ORC系统存在不可接受的安全风险，并且在模型与实际受控环境不匹配的情况下，其泛化能力不足以支持ORC控制需求。因此，本文提出了一种基于模拟到真实转移学习的DRL控制方法，用于ORC超热控制，旨在提供一种新的简单、可行且用户友好的解决方案。

    The Organic Rankine Cycle (ORC) is widely used in industrial waste heat recovery due to its simple structure and easy maintenance. However, in the context of smart manufacturing in the process industry, traditional model-based optimization control methods are unable to adapt to the varying operating conditions of the ORC system or sudden changes in operating modes. Deep reinforcement learning (DRL) has significant advantages in situations with uncertainty as it directly achieves control objectives by interacting with the environment without requiring an explicit model of the controlled plant. Nevertheless, direct application of DRL to physical ORC systems presents unacceptable safety risks, and its generalization performance under model-plant mismatch is insufficient to support ORC control requirements. Therefore, this paper proposes a Sim2Real transfer learning-based DRL control method for ORC superheat control, which aims to provide a new simple, feasible, and user-friendly solution 
    
[^16]: NeRFs: 寻找最佳3D表示的研究

    NeRFs: The Search for the Best 3D Representation. (arXiv:2308.02751v1 [cs.CV])

    [http://arxiv.org/abs/2308.02751](http://arxiv.org/abs/2308.02751)

    NeRFs是视图合成和相关问题中寻找最佳3D表示的结果，该方法利用神经网络查询获取体积参数来描述连续体积场景。

    

    神经辐射场（NeRFs）已成为视图合成或基于图像渲染等问题的首选表示方法，也应用于计算机图形学和计算机视觉等多个领域。NeRFs通过查询神经网络获得视图相关辐射和体积密度等体积参数，将场景表示为连续的体积。该表示方法已广泛应用，每年有数千篇论文在其基础上扩展或相关研究，多位作者和网站提供概述和调研，并有众多工业应用和创业公司。本文简要回顾了NeRFs的表示方法，并描述了长达三十年的寻找最佳3D表示方法以及最终引出NeRFs论文的过程。

    Neural Radiance Fields or NeRFs have become the representation of choice for problems in view synthesis or image-based rendering, as well as in many other applications across computer graphics and vision, and beyond. At their core, NeRFs describe a new representation of 3D scenes or 3D geometry. Instead of meshes, disparity maps, multiplane images or even voxel grids, they represent the scene as a continuous volume, with volumetric parameters like view-dependent radiance and volume density obtained by querying a neural network. The NeRF representation has now been widely used, with thousands of papers extending or building on it every year, multiple authors and websites providing overviews and surveys, and numerous industrial applications and startup companies. In this article, we briefly review the NeRF representation, and describe the three decades-long quest to find the best 3D representation for view synthesis and related problems, culminating in the NeRF papers. We then describe n
    
[^17]: 倒立摆扬升器的非线性控制器设计

    Nonlinear Controller Design for a Quadrotor with Inverted Pendulum. (arXiv:2308.02741v1 [cs.RO])

    [http://arxiv.org/abs/2308.02741](http://arxiv.org/abs/2308.02741)

    本文介绍了一种倒立摆扬升器的非线性控制器设计，该设计适用于四旋翼飞行器和倒立摆组合的动力学系统，并展示了轨迹跟踪的实验结果。

    

    四旋翼飞行器是一个具有六个自由度的欠驱动系统。在四旋翼飞行器上增加一个球形摆锤进一步增加了在稳定其余部分时实现任何输出跟踪的复杂性。在本文中，我们提出了用于四旋翼飞行器和倒立摆组合的非线性动力学系统的不同类型控制器，利用反馈线性化和具有二次规划的控制Lyapunov函数（CLF-QP）方法。我们展示了四旋翼飞行器独立情况下以及四旋翼飞行器-倒立摆组合情况下的轨迹跟踪。

    The quadrotor is a $6$ degrees-of-freedom (DoF) system with underactuation. Adding a spherical pendulum on top of a quadrotor further complicates the task of achieving any output tracking while stabilizing the rest. In this report, we present different types of controllers for the nonlinear dynamical system of quadrotor and pendulum combination, utilizing feedback-linearization and control Lyapunov function with quadratic programming (CLF-QP) approaches. We demonstrated trajectory tracking for quadrotor-only case as well as quadrotor-pendulum-combined case.
    
[^18]: 评估急诊短期住院部对住院时间的影响：基于住院时间预测和离散事件模拟的研究

    Assessing the impact of emergency department short stay units using length-of-stay prediction and discrete event simulation. (arXiv:2308.02730v1 [cs.AI])

    [http://arxiv.org/abs/2308.02730](http://arxiv.org/abs/2308.02730)

    本研究开发了一个决策支持系统，通过预测患者入院的住院时间，帮助指导临床决策和资源分配。研究结果表明，使用患者入院人口统计学数据和化验结果，可以相对准确地预测住院时间。

    

    准确预测患者入院时的住院时间可以帮助指导临床决策和资源分配。本研究旨在建立一个决策支持系统，预测从急诊科转入内科住院的患者的住院时间。我们进行了探索性数据分析，并使用特征选择方法来确定产生最佳预测性能的属性。我们还开发了一个离散事件模拟模型，以评估预测模型在实际环境中的性能。我们的结果表明，所提出方法的推荐性能通常可接受，并且不受特征选择的影响。此外，结果表明可以使用患者入院人口统计学数据和化验结果相对准确地预测住院时间（如，用于分类短期住院和长期住院患者的AUC值为0.69）。

    Accurately predicting hospital length-of-stay at the time a patient is admitted to hospital may help guide clinical decision making and resource allocation. In this study we aim to build a decision support system that predicts hospital length-of-stay for patients admitted to general internal medicine from the emergency department. We conduct an exploratory data analysis and employ feature selection methods to identify the attributes that result in the best predictive performance. We also develop a discrete-event simulation model to assess the performances of the prediction models in a practical setting. Our results show that the recommendation performances of the proposed approaches are generally acceptable and do not benefit from the feature selection. Further, the results indicate that hospital length-of-stay could be predicted with reasonable accuracy (e.g., AUC value for classifying short and long stay patients is 0.69) using patient admission demographics, laboratory test results,
    
[^19]: 为歌唱旋律提取改进谐波敏感性和预测稳定性的途径

    Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction. (arXiv:2308.02723v1 [cs.SD])

    [http://arxiv.org/abs/2308.02723](http://arxiv.org/abs/2308.02723)

    本文提出了通过修改输入特征和训练目标来改进歌唱旋律提取的方法。这些修改包括增强对尾部谐波的敏感性和设计可防止预测极短片段的损失函数。实验结果表明，这些修改对于提高歌唱旋律提取的效果有实际效果。

    

    在深度学习研究中，许多旋律提取模型依赖于重新设计神经网络架构来提高性能。本文提出了基于两个假设的输入特征修改和训练目标修改。首先，音频数据的频谱图中的谐波在频率轴上迅速衰减。为了增强模型对尾部谐波的敏感性，我们使用离散z-transform修改了联合频率和周期性(CFP)表示。其次，极短时长的人声和非人声片段并不常见。为了确保更稳定的旋律轮廓，我们设计了一个可微分的损失函数，防止模型预测这些片段。我们将这些修改应用于多个模型，包括MSNet、FTANet和新引入的模型PianoNet，该模型是从钢琴转录网络改编而来的。我们的实验结果表明，提出的修改在歌唱旋律提取上具有实证有效性。

    In deep learning research, many melody extraction models rely on redesigning neural network architectures to improve performance. In this paper, we propose an input feature modification and a training objective modification based on two assumptions. First, harmonics in the spectrograms of audio data decay rapidly along the frequency axis. To enhance the model's sensitivity on the trailing harmonics, we modify the Combined Frequency and Periodicity (CFP) representation using discrete z-transform. Second, the vocal and non-vocal segments with extremely short duration are uncommon. To ensure a more stable melody contour, we design a differentiable loss function that prevents the model from predicting such segments. We apply these modifications to several models, including MSNet, FTANet, and a newly introduced model, PianoNet, modified from a piano transcription network. Our experimental results demonstrate that the proposed modifications are empirically effective for singing melody extrac
    
[^20]: 半监督实例分割的引导蒸馏

    Guided Distillation for Semi-Supervised Instance Segmentation. (arXiv:2308.02668v1 [cs.CV])

    [http://arxiv.org/abs/2308.02668](http://arxiv.org/abs/2308.02668)

    这项研究提出了一种半监督实例分割的引导蒸馏方法，通过引入新的“引导预烧”阶段和利用未标记数据的导师模型指导，取得了显著的改进。

    

    虽然实例分割方法有了显著的改进，但主导范式是依赖于完全带注释的训练图像，这需要费时费力。为了减轻这种依赖并提高结果，半监督方法利用未标记数据作为额外的训练信号，以限制对标记样本的过拟合。在这个背景下，我们提出了一些新颖的设计选择来显著改进师生蒸馏模型。特别是，我们(i)通过引入新的“引导预烧”阶段改进了蒸馏方法，(ii)评估了不同的实例分割架构、主干网络和预训练策略。与之前只使用监督数据来对学生模型进行预烧的工作相反，我们还利用导师模型的指导在预烧阶段中利用未标记数据。我们改进的蒸馏方法在之前最先进的结果上取得了显著的改进。

    Although instance segmentation methods have improved considerably, the dominant paradigm is to rely on fully-annotated training images, which are tedious to obtain. To alleviate this reliance, and boost results, semi-supervised approaches leverage unlabeled data as an additional training signal that limits overfitting to the labeled samples. In this context, we present novel design choices to significantly improve teacher-student distillation models. In particular, we (i) improve the distillation approach by introducing a novel "guided burn-in" stage, and (ii) evaluate different instance segmentation architectures, as well as backbone networks and pre-training strategies. Contrary to previous work which uses only supervised data for the burn-in period of the student model, we also use guidance of the teacher model to exploit unlabeled data in the burn-in period. Our improved distillation approach leads to substantial improvements over previous state-of-the-art results. For example, on 
    
[^21]: 用自动学习的可解释人类谓词更快解决见证型三角谜题

    Solving Witness-type Triangle Puzzles Faster with an Automatically Learned Human-Explainable Predicate. (arXiv:2308.02666v1 [cs.AI])

    [http://arxiv.org/abs/2308.02666](http://arxiv.org/abs/2308.02666)

    使用自动学习的人类可解释谓词加速搜索，解决《见证人》游戏中的谜题实例，比基准搜索提升六倍速度，且能处理更大规模的谜题。

    

    自动解决游戏《见证人》中的谜题实例可以指导玩家找到解决方案，并帮助谜题设计者生成更好的谜题。然而，这些谜题在组合上很难，并且基于搜索的求解器可能需要大量的时间和内存。我们通过自动学习一个可解释的人类谓词，加速搜索，预测不可完成到解决路径的部分路径，从而提升搜索效率。相较于基准搜索，平均加速六倍，同时保持搜索的完整性。对于给定的每个谜题的固定搜索时间预算，使用谓词加速的搜索能够解决更大规模的谜题实例。

    Automatically solving puzzle instances in the game The Witness can guide players toward solutions and help puzzle designers generate better puzzles. In the latter case such an Artificial Intelligence puzzle solver can inform a human puzzle designer and procedural puzzle generator to produce better instances. The puzzles, however, are combinatorially difficult and search-based solvers can require large amounts of time and memory. We accelerate such search by automatically learning a human-explainable predicate that predicts whether a partial path to a Witness-type puzzle is not completable to a solution path. We prove a key property of the learned predicate which allows us to use it for pruning successor states in search thereby accelerating search by an average of six times while maintaining completeness of the underlying search. Conversely given a fixed search time budget per puzzle our predicate-accelerated search can solve more puzzle instances of larger sizes than the baseline sear
    
[^22]: 在虚拟现实中让对话代理发出声音

    Let's Give a Voice to Conversational Agents in Virtual Reality. (arXiv:2308.02665v1 [cs.AI])

    [http://arxiv.org/abs/2308.02665](http://arxiv.org/abs/2308.02665)

    本研究提出了一个简化开发虚拟环境中对话代理的开源架构，通过多模态和沉浸式交互提升与对话代理的对话体验，并通过添加语音到文本和文本到语音模型实现基于语音的交互。

    

    通过虚拟现实中的多模态和沉浸式交互，可以极大地增强与对话代理的对话体验。在这项工作中，我们提出了一个开源架构，旨在简化在虚拟环境中运行对话代理的开发。该架构提供了将不同领域的对话代理插入和添加自定义或基于云的语音到文本和文本到语音模型，以实现基于语音的交互的可能性。利用这个架构，我们展示了在数字健康领域开发的两个对话原型，用于非沉浸式显示和虚拟现实头盔。

    The dialogue experience with conversational agents can be greatly enhanced with multimodal and immersive interactions in virtual reality. In this work, we present an open-source architecture with the goal of simplifying the development of conversational agents operating in virtual environments. The architecture offers the possibility of plugging in conversational agents of different domains and adding custom or cloud-based Speech-To-Text and Text-To-Speech models to make the interaction voice-based. Using this architecture, we present two conversational prototypes operating in the digital health domain developed in Unity for both non-immersive displays and VR headsets.
    
[^23]: 增强型人工智能数据处理和发现群体外包用于流星雨制图

    AI-Enhanced Data Processing and Discovery Crowd Sourcing for Meteor Shower Mapping. (arXiv:2308.02664v1 [astro-ph.EP])

    [http://arxiv.org/abs/2308.02664](http://arxiv.org/abs/2308.02664)

    本研究使用增强型人工智能技术对全球流星雨数据进行处理和分析，并通过开发互动的网页平台，让公众参与流星探测，从而提高了流星雨的发现率。

    

    由NASA自2010年开始资助的全球全天候流星观测(CAMS)项目旨在通过对来自16个国家、北半球和南半球的多地低光视频摄像机探测到的流星轨迹进行三角测量，制图全球流星雨。该项目的使命是验证、发现并预测未来的流星雨降临。我们的研究旨在通过实施基于云的自动化AI流程和改善数据可视化，将公众参与到监测流星探测中，从而提高发现率。本文描述了使用可解释活动学习和AI流程自动化进行数据摄取、处理和洞察生成的过程。此外，本研究还描述了一个互动的Web门户（NASA流星雨门户），用于促进流星源地图的可视化。目前，CAMS已经发现了超过200个新的流星雨

    The Cameras for Allsky Meteor Surveillance (CAMS) project, funded by NASA starting in 2010, aims to map our meteor showers by triangulating meteor trajectories detected in low-light video cameras from multiple locations across 16 countries in both the northern and southern hemispheres. Its mission is to validate, discover, and predict the upcoming returns of meteor showers. Our research aimed to streamline the data processing by implementing an automated cloud-based AI-enabled pipeline and improve the data visualization to improve the rate of discoveries by involving the public in monitoring the meteor detections. This article describes the process of automating the data ingestion, processing, and insight generation using an interpretable Active Learning and AI pipeline. This work also describes the development of an interactive web portal (the NASA Meteor Shower portal) to facilitate the visualization of meteor radiant maps. To date, CAMS has discovered over 200 new meteor showers and
    
[^24]: 使用生成对抗网络生成逼真的合成毫米波雷达数据，用于自动驾驶应用

    Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks. (arXiv:2308.02632v1 [cs.CV])

    [http://arxiv.org/abs/2308.02632](http://arxiv.org/abs/2308.02632)

    本研究提出了一种使用生成对抗网络生成合成毫米波雷达数据的快速方法，可以增加数据增强的潜力，并进一步开发雷达数据处理算法。

    

    目前模拟FMCW雷达的主要方法是基于射线追踪，通常计算密集且不能考虑背景噪声。本研究提出了一种更快速的FMCW雷达模拟方法，能够使用生成对抗网络（GAN）生成合成的原始雷达数据。代码和预训练的权重是开源的，并可在GitHub上获得。该方法生成了16个同时的脉冲，可以用于进一步开发用于处理雷达数据（滤波和聚类）的算法。这可以增加数据增强的潜力，例如通过生成在实际生活中不可复现的不存在或安全关键场景的数据。在本研究中，使用摩托车的雷达测量数据对GAN进行训练，并生成摩托车直线行驶的合成原始雷达数据。生成这些数据时，使用了摩托车的距离和高斯噪声作为输入。

    The main approaches for simulating FMCW radar are based on ray tracing, which is usually computationally intensive and do not account for background noise. This work proposes a faster method for FMCW radar simulation capable of generating synthetic raw radar data using generative adversarial networks (GAN). The code and pre-trained weights are open-source and available on GitHub. This method generates 16 simultaneous chirps, which allows the generated data to be used for the further development of algorithms for processing radar data (filtering and clustering). This can increase the potential for data augmentation, e.g., by generating data in non-existent or safety-critical scenarios that are not reproducible in real life. In this work, the GAN was trained with radar measurements of a motorcycle and used to generate synthetic raw radar data of a motorcycle traveling in a straight line. For generating this data, the distance of the motorcycle and Gaussian noise are used as input to the 
    
[^25]: 利用网络和知识图谱自动评估影响力投资

    Harnessing the Web and Knowledge Graphs for Automated Impact Investing Scoring. (arXiv:2308.02622v1 [cs.LG])

    [http://arxiv.org/abs/2308.02622](http://arxiv.org/abs/2308.02622)

    本论文介绍了一个利用网络和知识图谱自动评估影响力投资的数据驱动系统，通过收集和过滤文本数据集以及使用分类器来预测分数，实现了自动创建SDG框架的过程。

    

    可持续发展目标（SDGs）是联合国提出的旨在鼓励有助于保障人类繁荣和可持续性的政策和活动。在金融行业中，为了评估一个公司与每个17个可持续发展目标的一致性，设计了SDG框架提供分数。这种评分能够对潜在建立包容和可持续经济的投资进行一致的评估。由于此类框架需要高质量和可靠性，创建和维护它们的过程耗时且需要广泛的领域专业知识。在这项工作中，我们描述了一个数据驱动的系统，旨在自动化创建SDG框架的过程。首先，我们提出了一种从不同网络来源和与一组公司相关的知识图谱收集和过滤文本数据集的新方法。然后，我们使用此数据训练和部署了分类器来预测分数。

    The Sustainable Development Goals (SDGs) were introduced by the United Nations in order to encourage policies and activities that help guarantee human prosperity and sustainability. SDG frameworks produced in the finance industry are designed to provide scores that indicate how well a company aligns with each of the 17 SDGs. This scoring enables a consistent assessment of investments that have the potential of building an inclusive and sustainable economy. As a result of the high quality and reliability required by such frameworks, the process of creating and maintaining them is time-consuming and requires extensive domain expertise. In this work, we describe a data-driven system that seeks to automate the process of creating an SDG framework. First, we propose a novel method for collecting and filtering a dataset of texts from different web sources and a knowledge graph relevant to a set of companies. We then implement and deploy classifiers trained with this data for predicting score
    
[^26]: ChatGPT用于GTFS: 从文字到信息

    ChatGPT for GTFS: From Words to Information. (arXiv:2308.02618v1 [cs.IR])

    [http://arxiv.org/abs/2308.02618](http://arxiv.org/abs/2308.02618)

    本研究探索了使用ChatGPT语言模型从GTFS数据中检索信息的可行性，验证了ChatGPT（GPT-3.5）在GTFS规范理解和信息提取方面的能力。程序合成方法在信息检索任务中表现出更高的准确率，为解决GTFS数据信息获取问题提供了一种有效的方法。

    

    广泛使用的公交通行数据发布标准General Transit Feed Specification（GTFS）是表格数据，信息分散在不同的文件中，需要专门的工具或包来检索信息。与此同时，使用大型语言模型进行文本和信息检索的趋势也在增长。本研究的想法是看看当前广泛采用的LLMs（ChatGPT）是否能够使用自然语言指令从GTFS中检索信息。我们首先测试ChatGPT（GPT-3.5）是否理解GTFS规范。GPT-3.5在我们的多项选择问题（MCQ）中正确回答了77%。接下来，我们利用过滤的GTFS数据集对LLM进行信息提取任务。对于信息检索，我们比较了零-shot和程序合成。程序合成的效果更好，在简单问题上达到了约90%的准确率，在复杂问题上达到了约40%的准确率。

    The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.
    
[^27]: 车辆控制：使用联邦深度强化学习进行碰撞避免

    Vehicles Control: Collision Avoidance using Federated Deep Reinforcement Learning. (arXiv:2308.02614v1 [cs.LG])

    [http://arxiv.org/abs/2308.02614](http://arxiv.org/abs/2308.02614)

    本文研究了使用联邦深度强化学习技术进行碰撞避免的车辆控制问题，并比较了不同算法的效果。结果表明，联邦深度确定性策略梯度算法在优化车辆控制方面表现更好。

    

    在城市人口增长和道路上车辆数量不断增加的情况下，高效管理交通并确保安全已成为重要挑战。为了解决这些问题，开发车辆智能控制系统至关重要。本文介绍了一项关于车辆碰撞避免的综合研究，利用联邦深度强化学习（FDRL）技术的力量。我们的主要目标是在优先考虑安全和保护数据隐私的同时，最大程度地减少出行延误和提高车辆的平均速度。为了实现这一目标，我们在本文中对本地模型深度确定性策略梯度（DDPG）和全球模型联邦深度确定性策略梯度（FDDPG）进行了比较分析，以确定它们在优化车辆碰撞避免方面的效果。所得到的结果表明，FDDPG算法在有效控制车辆和预防碰撞方面优于DDPG。

    In the face of growing urban populations and the escalating number of vehicles on the roads, managing transportation efficiently and ensuring safety have become critical challenges. To tackle these issues, the development of intelligent control systems for vehicles is paramount. This paper presents a comprehensive study on vehicle control for collision avoidance, leveraging the power of Federated Deep Reinforcement Learning (FDRL) techniques. Our main goal is to minimize travel delays and enhance the average speed of vehicles while prioritizing safety and preserving data privacy. To accomplish this, we conducted a comparative analysis between the local model, Deep Deterministic Policy Gradient (DDPG), and the global model, Federated Deep Deterministic Policy Gradient (FDDPG), to determine their effectiveness in optimizing vehicle control for collision avoidance. The results obtained indicate that the FDDPG algorithm outperforms DDPG in terms of effectively controlling vehicles and prev
    
[^28]: 用SyntHIR实现互操作性合成健康数据，以便开发CDSS工具

    Interoperable synthetic health data with SyntHIR to enable the development of CDSS tools. (arXiv:2308.02613v1 [cs.LG])

    [http://arxiv.org/abs/2308.02613](http://arxiv.org/abs/2308.02613)

    本论文提出了一种利用合成EHR数据开发CDSS工具的体系架构，通过使用SyntHIR系统和FHIR标准实现数据互操作性和工具可迁移性。

    

    利用高质量的患者日志和健康登记来开发基于机器学习的临床决策支持系统（CDSS）有很大的机会。为了在临床工作流程中实施CDSS工具，需要将该工具集成、验证和测试在用于存储和管理患者数据的电子健康记录（EHR）系统上。然而，由于合规法规，通常不可能获得对EHR系统的必要访问权限。我们提出了一种用于生成和使用CDSS工具开发的合成EHR数据的体系架构。该体系结构在一个称为SyntHIR的系统中实现。SyntHIR系统使用Fast Healthcare Interoperability Resources (FHIR)标准进行数据互操作性，使用Gretel框架生成合成数据，使用Microsoft Azure FHIR服务器作为基于FHIR的EHR系统，以及使用SMART on FHIR框架进行工具可迁移性。我们通过使用数据开发机器学习基于CDSS工具来展示SyntHIR的实用性。

    There is a great opportunity to use high-quality patient journals and health registers to develop machine learning-based Clinical Decision Support Systems (CDSS). To implement a CDSS tool in a clinical workflow, there is a need to integrate, validate and test this tool on the Electronic Health Record (EHR) systems used to store and manage patient data. However, it is often not possible to get the necessary access to an EHR system due to legal compliance. We propose an architecture for generating and using synthetic EHR data for CDSS tool development. The architecture is implemented in a system called SyntHIR. The SyntHIR system uses the Fast Healthcare Interoperability Resources (FHIR) standards for data interoperability, the Gretel framework for generating synthetic data, the Microsoft Azure FHIR server as the FHIR-based EHR system and SMART on FHIR framework for tool transportability. We demonstrate the usefulness of SyntHIR by developing a machine learning-based CDSS tool using data
    
[^29]: 解构人工智能责任

    Unravelling Responsibility for AI. (arXiv:2308.02608v1 [cs.AI])

    [http://arxiv.org/abs/2308.02608](http://arxiv.org/abs/2308.02608)

    本文旨在解构人工智能责任的概念，提出了一种包含四种责任意义的有效组合，以支持对人工智能责任的实践推理。

    

    为了在涉及人工智能系统的复杂情况下合理思考责任应该放在何处，我们首先需要一个足够清晰和详细的跨学科词汇来谈论责任。责任是一种三元关系，涉及到一个行为者、一个事件和一种责任方式。作为一种有意识的为了支持对人工智能责任进行实践推理的“解构”责任概念的努力，本文采取了“行为者A对事件O负责”的三部分表述，并确定了A、负责、O的子类别的有效组合。这些有效组合我们称之为“责任串”，分为四种责任意义：角色责任、因果责任、法律责任和道德责任。我们通过两个运行示例进行了说明，一个涉及医疗AI系统，另一个涉及AV与行人的致命碰撞。

    To reason about where responsibility does and should lie in complex situations involving AI-enabled systems, we first need a sufficiently clear and detailed cross-disciplinary vocabulary for talking about responsibility. Responsibility is a triadic relation involving an actor, an occurrence, and a way of being responsible. As part of a conscious effort towards 'unravelling' the concept of responsibility to support practical reasoning about responsibility for AI, this paper takes the three-part formulation, 'Actor A is responsible for Occurrence O' and identifies valid combinations of subcategories of A, is responsible for, and O. These valid combinations - which we term "responsibility strings" - are grouped into four senses of responsibility: role-responsibility; causal responsibility; legal liability-responsibility; and moral responsibility. They are illustrated with two running examples, one involving a healthcare AI-based system and another the fatal collision of an AV with a pedes
    
[^30]: 基于稳定的包装器参数选择方法用于高效的基于ANN的湍流数据驱动建模

    On stable wrapper-based parameter selection method for efficient ANN-based data-driven modeling of turbulent flows. (arXiv:2308.02602v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2308.02602](http://arxiv.org/abs/2308.02602)

    本研究针对复杂的湍流流动现象，提出了一种基于ANN和包装器方法的简化建模方法，通过基于梯度的子集选择指标，在每个消除步骤中最小化总导数的损失或方向一致性。该方法相对于其他方法具有优势，可以有效去除冗余或无关的参数。

    

    为了模拟复杂的湍流流动和传热现象，本研究旨在分析和开发一种基于人工神经网络(ANN)和包装器方法的简化建模方法。这种方法相对于其他方法（如基于相关性的过滤方法）具有优势，可以在参数之间存在非线性关系的情况下去除冗余或无关的参数。然而，ANN训练的过度拟合和随机性可能会导致在更高的物理维度下特别是在选择试验中产生不一致的子集。本研究分析了几种现有的基于ANN的包装器方法，并基于基于梯度的子集选择指标开发了修订版方法，以在每个消除步骤中最小化总导数的损失或方向一致性。为了检验参数减少性能和试验一致性，我们将这些方法应用于一个制造的子集选择问题，即湍流气泡流中气泡尺寸的建模，以及空间建模。

    To model complex turbulent flow and heat transfer phenomena, this study aims to analyze and develop a reduced modeling approach based on artificial neural network (ANN) and wrapper methods. This approach has an advantage over other methods such as the correlation-based filter method in terms of removing redundant or irrelevant parameters even under non-linearity among them. As a downside, the overfitting and randomness of ANN training may produce inconsistent subsets over selection trials especially in a higher physical dimension. This study analyzes a few existing ANN-based wrapper methods and develops a revised one based on the gradient-based subset selection indices to minimize the loss in the total derivative or the directional consistency at each elimination step. To examine parameter reduction performance and consistency-over-trials, we apply these methods to a manufactured subset selection problem, modeling of the bubble size in a turbulent bubbly flow, and modeling of the spati
    
[^31]: 设计一种基于深度学习的资源高效诊断系统，用于转移性乳腺癌：减少发展中国家临床诊断的长时间延迟，提高患者生存率

    Designing a Deep Learning-Driven Resource-Efficient Diagnostic System for Metastatic Breast Cancer: Reducing Long Delays of Clinical Diagnosis and Improving Patient Survival in Developing Countries. (arXiv:2308.02597v1 [eess.IV])

    [http://arxiv.org/abs/2308.02597](http://arxiv.org/abs/2308.02597)

    这项研究设计了一种基于深度学习的诊断系统，用于转移性乳腺癌的诊断，旨在减少发展中国家临床诊断的长时间延迟，并提高患者的生存率。

    

    乳腺癌是癌症死亡的主要原因之一。发展中国家，特别是撒哈拉以南非洲、南亚和南美的乳腺癌患者死亡率最高。导致全球死亡率差距的关键因素之一是由于受训病理学家严重短缺而导致的诊断长时间延迟，从而导致诊断时较大比例的晚期表现。初始症状发展到诊断接受时间可能长达15个月。为解决这一关键的医疗差距，本研究开发了一种基于深度学习的转移性乳腺癌诊断系统，既能达到高诊断准确度，又能保证计算效率。根据我们的评估，基于MobileNetV2的诊断模型在诊断准确度、模型泛化性和模型训练方面超过了更复杂的VGG16、ResNet50和ResNet101模型。

    Breast cancer is one of the leading causes of cancer mortality. Breast cancer patients in developing countries, especially sub-Saharan Africa, South Asia, and South America, suffer from the highest mortality rate in the world. One crucial factor contributing to the global disparity in mortality rate is long delay of diagnosis due to a severe shortage of trained pathologists, which consequently has led to a large proportion of late-stage presentation at diagnosis. The delay between the initial development of symptoms and the receipt of a diagnosis could stretch upwards 15 months. To tackle this critical healthcare disparity, this research has developed a deep learning-based diagnosis system for metastatic breast cancer that can achieve high diagnostic accuracy as well as computational efficiency. Based on our evaluation, the MobileNetV2-based diagnostic model outperformed the more complex VGG16, ResNet50 and ResNet101 models in diagnostic accuracy, model generalization, and model traini
    
[^32]: SMARLA：一种用于深度强化学习智能体的安全监测方法

    SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents. (arXiv:2308.02594v1 [cs.LG])

    [http://arxiv.org/abs/2308.02594](http://arxiv.org/abs/2308.02594)

    本文提出了一种基于机器学习的安全监测方法SMARLA，用于深度强化学习智能体。该方法设计为黑盒子，利用状态抽象减少状态空间，实现对智能体状态的安全违规预测。经验证，SMARLA具有准确的违规预测能力，并可在智能体执行的早期阶段进行预测。

    

    深度强化学习算法(DRL)越来越多地应用于安全关键系统。确保DRL智能体的安全性在这种情况下是一个关键问题。然而，仅依靠测试是不足以确保安全性的，因为它不能提供保证。构建安全监测器是缓解这一挑战的一种解决方案。本文提出了SMARLA，一种基于机器学习的安全监测方法，专为DRL智能体设计。出于实际原因，SMARLA被设计为黑盒子(因为它不需要访问智能体的内部)，并利用状态抽象来减少状态空间，从而促进从智能体的状态学习安全违规预测模型。我们在两个知名的RL案例研究中验证了SMARLA。经验分析表明，SMARLA具有准确的违规预测能力，误报率低，并且可以在智能体执行的一半左右的早期阶段预测安全违规。

    Deep reinforcement learning algorithms (DRL) are increasingly being used in safety-critical systems. Ensuring the safety of DRL agents is a critical concern in such contexts. However, relying solely on testing is not sufficient to ensure safety as it does not offer guarantees. Building safety monitors is one solution to alleviate this challenge. This paper proposes SMARLA, a machine learning-based safety monitoring approach designed for DRL agents. For practical reasons, SMARLA is designed to be black-box (as it does not require access to the internals of the agent) and leverages state abstraction to reduce the state space and thus facilitate the learning of safety violation prediction models from agent's states. We validated SMARLA on two well-known RL case studies. Empirical analysis reveals that SMARLA achieves accurate violation prediction with a low false positive rate, and can predict safety violations at an early stage, approximately halfway through the agent's execution before 
    
[^33]: 通过领域适应的最少到最多提示的方式实现文本到SQL的高效泛化

    Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v1 [cs.CL])

    [http://arxiv.org/abs/2308.02582](http://arxiv.org/abs/2308.02582)

    该论文介绍了一种通过领域适应和最少到最多提示的方式实现文本到SQL的高效泛化的方法。通过离线抽样获取少量样本，并合成一个通用提示，避免了昂贵的测试时间样本检索，并通过自适应和分解的方法更好地处理跨领域和跨组合式的泛化。

    

    跨领域和跨组合式的文本到SQL语义解析的泛化是一项具有挑战性的任务。现有的基于大型语言模型（LLM）的解决方案依赖于从训练集中推理出少量样本，以合成每个自然语言（NL）测试查询的运行时提示。与此相反，我们设计了一种算法，该算法通过离线抽样从训练数据中获取少量样本，完全覆盖SQL子句、运算符和函数，并在允许的令牌长度范围内实现最大领域覆盖。这样可以合成一个固定的通用提示（GP），其中包含NL测试查询之间共用的多样化样本集，避免了昂贵的测试时间样本检索。我们还将GP自适应到目标数据库领域（DA-GP），以更好地处理跨领域泛化；然后采用分解的最少到最多提示（LTMP-DA-GP）来处理跨组合泛化。LTMP-DA-GP的合成是离线任务，

    Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to
    
[^34]: GPT-4是一个可靠的评分器吗？评估GPT-4文本评分的一致性。

    Is GPT-4 a reliable rater? Evaluating Consistency in GPT-4 Text Ratings. (arXiv:2308.02575v1 [cs.CL])

    [http://arxiv.org/abs/2308.02575](http://arxiv.org/abs/2308.02575)

    GPT-4在多个迭代中生成的反馈评分具有高一致性，内容和文体评分之间具有高相关性。

    

    本研究调查了OpenAI的GPT-4在多个迭代、时间跨度和文体变化中生成的反馈评分的一致性。该模型根据内容和文体对宏观经济学学科领域内的任务回答进行评分。通过统计分析，研究了评分的一致性、迭代之间的评分相关性以及内容和文体评分之间的相关性。结果显示，不同时间跨度的ICC分数在0.94到0.99之间，表明GPT-4能够在重复任务中生成一致的评分。内容和文体评分之间的相关性为0.87。当应用不恰当的文体时，平均内容评分保持不变，而文体评分下降，这表明大型语言模型在生成一致评分方面具有能力。

    This study investigates the consistency of feedback ratings generated by OpenAI's GPT-4, a state-of-the-art artificial intelligence language model, across multiple iterations, time spans and stylistic variations. The model rated responses to tasks within the Higher Education (HE) subject domain of macroeconomics in terms of their content and style. Statistical analysis was conducted in order to learn more about the interrater reliability, consistency of the ratings across iterations and the correlation between ratings in terms of content and style. The results revealed a high interrater reliability with ICC scores ranging between 0.94 and 0.99 for different timespans, suggesting that GPT-4 is capable of generating consistent ratings across repetitions with a clear prompt. Style and content ratings show a high correlation of 0.87. When applying a non-adequate style the average content ratings remained constant, while style ratings decreased, which indicates that the large language model
    
[^35]: 通过双向生成对齐学习隐式实体-物体关系，用于多模态NER

    Learning Implicit Entity-object Relations by Bidirectional Generative Alignment for Multimodal NER. (arXiv:2308.02570v1 [cs.LG])

    [http://arxiv.org/abs/2308.02570](http://arxiv.org/abs/2308.02570)

    本文提出了一种名为BGA-MNER的双向生成对齐方法，用于解决多模态命名实体识别中的两个主要挑战：语义鸿沟和实体-物体关系。实验结果表明，该方法能够有效地捕捉隐式实体-物体关系。

    

    多模态命名实体识别(MNER)面临的挑战主要有两方面: (1) 弥合文本和图像之间的语义鸿沟; (2) 匹配实体与图像中其关联的物体。现有方法无法捕捉隐含的实体-物体关系，因为缺乏相应的注释。本文提出了一种名为BGA-MNER的双向生成对齐方法来解决这些问题。我们的BGA-MNER包括针对两种模态中的实体显著内容的\texttt{图像到文本}和\texttt{文本到图像}生成。它通过共同优化双向重建目标来对齐隐含的实体-物体关系，在直接而强大的约束下实现对齐。此外，图像-文本对通常包含不匹配的组件，对于生成来说是噪声。我们提出了一种阶段性改进的上下文采样器，用于提取匹配的跨模态内容进行生成。在两个基准测试中进行的广泛实验证明了我们的方法。

    The challenge posed by multimodal named entity recognition (MNER) is mainly two-fold: (1) bridging the semantic gap between text and image and (2) matching the entity with its associated object in image. Existing methods fail to capture the implicit entity-object relations, due to the lack of corresponding annotation. In this paper, we propose a bidirectional generative alignment method named BGA-MNER to tackle these issues. Our BGA-MNER consists of \texttt{image2text} and \texttt{text2image} generation with respect to entity-salient content in two modalities. It jointly optimizes the bidirectional reconstruction objectives, leading to aligning the implicit entity-object relations under such direct and powerful constraints. Furthermore, image-text pairs usually contain unmatched components which are noisy for generation. A stage-refined context sampler is proposed to extract the matched cross-modal content for generation. Extensive experiments on two benchmarks demonstrate that our met
    
[^36]: SimTeG: 一种令人沮丧地简单的方法改进了文本图学习

    SimTeG: A Frustratingly Simple Approach Improves Textual Graph Learning. (arXiv:2308.02565v1 [cs.CL])

    [http://arxiv.org/abs/2308.02565](http://arxiv.org/abs/2308.02565)

    这项工作提出了一种令人沮丧地简单的文本图学习方法SimTeG，解决了现有方法在特征工程和模型设计方面的不足。

    

    文本图（TGs）是指节点对应文本（句子或文档）的图形，在许多领域都得到广泛应用。TGs的表示学习包括两个阶段：（i）无监督特征提取和（ii）监督图表示学习。近年来，人们在后者阶段进行了大量的研究，其中图神经网络（GNNs）占据主导地位。然而，目前大多数现有图形基准的前者阶段仍依赖于传统的特征工程技术。最近随着语言模型（LMs）的快速发展，研究人员开始利用LMs来促进TGs的学习，方法要么是在计算密集的框架中联合训练它们（合并两个阶段），要么是设计复杂的自监督训练任务来进行特征提取（增强第一阶段）。在这项工作中，我们提出了SimTeG，一种令人沮丧地简单的文本图学习方法，它不创新于框架、模型和任务。

    Textual graphs (TGs) are graphs whose nodes correspond to text (sentences or documents), which are widely prevalent. The representation learning of TGs involves two stages: (i) unsupervised feature extraction and (ii) supervised graph representation learning. In recent years, extensive efforts have been devoted to the latter stage, where Graph Neural Networks (GNNs) have dominated. However, the former stage for most existing graph benchmarks still relies on traditional feature engineering techniques. More recently, with the rapid development of language models (LMs), researchers have focused on leveraging LMs to facilitate the learning of TGs, either by jointly training them in a computationally intensive framework (merging the two stages), or designing complex self-supervised training tasks for feature extraction (enhancing the first stage). In this work, we present SimTeG, a frustratingly Simple approach for Textual Graph learning that does not innovate in frameworks, models, and tas
    
[^37]: 使用视觉和文本数据的联合表示进行食物分类

    Food Classification using Joint Representation of Visual and Textual Data. (arXiv:2308.02562v1 [cs.CV])

    [http://arxiv.org/abs/2308.02562](http://arxiv.org/abs/2308.02562)

    本研究提出了一种使用联合表示的多模态分类框架，通过修改版的EfficientNet和Mish激活函数实现图像分类，使用基于BERT的网络实现文本分类。实验结果表明，所提出的网络在图像和文本分类上表现优于其他方法，准确率提高了11.57%和6.34%。比较分析还证明了所提出方法的效率和鲁棒性。

    

    食物分类是健康保健中的重要任务。在这项工作中，我们提出了一个多模态分类框架，该框架使用了修改版的EfficientNet和Mish激活函数用于图像分类，同时使用传统的基于BERT的网络进行文本分类。我们在一个大型开源数据集UPMC Food-101上评估了所提出的网络和其他最先进的方法。实验结果显示，所提出的网络在图像和文本分类上的准确率分别比第二最好的方法提高了11.57%和6.34%。我们还比较了使用机器学习和深度学习模型进行文本分类的准确率、精确率和召回率。通过对图像和文本的预测结果进行比较分析，证明了所提出方法的效率和鲁棒性。

    Food classification is an important task in health care. In this work, we propose a multimodal classification framework that uses the modified version of EfficientNet with the Mish activation function for image classification, and the traditional BERT transformer-based network is used for text classification. The proposed network and the other state-of-the-art methods are evaluated on a large open-source dataset, UPMC Food-101. The experimental results show that the proposed network outperforms the other methods, a significant difference of 11.57% and 6.34% in accuracy is observed for image and text classification, respectively, when compared with the second-best performing method. We also compared the performance in terms of accuracy, precision, and recall for text classification using both machine learning and deep learning-based models. The comparative analysis from the prediction results of both images and text demonstrated the efficiency and robustness of the proposed approach.
    
[^38]: 大规模生成模拟人工智能：生成AI中的下一个热点

    Large-scale Generative Simulation Artificial Intelligence: the Next Hotspot in Generative AI. (arXiv:2308.02561v1 [cs.AI])

    [http://arxiv.org/abs/2308.02561](http://arxiv.org/abs/2308.02561)

    大规模生成模拟人工智能是GenAI中的下一个热点，可以解决学习资源有限和过于依赖科学发现经验主义等实际挑战。

    

    GenAI的概念已经发展了数十年。最近，它在自然语言处理和计算机视觉方面取得了重大突破，在工业场景中积极参与。鉴于现实挑战，如有限的学习资源和过于依赖科学发现经验主义，我们将大规模生成模拟人工智能（LS-GenAI）提名为GenAI的下一个连接热点。

    The concept of GenAI has been developed for decades. Until recently, it has impressed us with substantial breakthroughs in natural language processing and computer vision, actively engaging in industrial scenarios. Noticing the practical challenges, e.g., limited learning resources, and overly dependencies on scientific discovery empiricism, we nominate large-scale generative simulation artificial intelligence (LS-GenAI) as the next hotspot for GenAI to connect.
    
[^39]: 人工智能中的范式转变

    The Paradigm Shifts in Artificial Intelligence. (arXiv:2308.02558v1 [cs.AI])

    [http://arxiv.org/abs/2308.02558](http://arxiv.org/abs/2308.02558)

    本文总结了过去60年来人工智能中发生的范式转变，并讨论了当前人工智能范式转变所面临的紧迫问题和风险。

    

    Kuhn (1962) 的科学进步框架提供了一个有用的视角，用于理解过去60年来人工智能中发生的范式转变。该框架同样有助于理解人工智能中的一个新的范式转变，这一转变的标志是大型预训练系统的出现，如基于ChatGPT的对话代理。这些系统使得智能成为一种可配置应用的通用技术产品。在本文中，我总结了导致每个范式兴起和衰落的力量，并讨论了当前人工智能范式转变所面临的紧迫问题和风险。

    Kuhn's framework of scientific progress (Kuhn, 1962) provides a useful framing of the paradigm shifts that have occurred in Artificial Intelligence over the last 60 years. The framework is also useful in understanding what is arguably a new paradigm shift in AI, signaled by the emergence of large pre-trained systems such as GPT-3, on which conversational agents such as ChatGPT are based. Such systems make intelligence a commoditized general purpose technology that is configurable to applications. In this paper, I summarize the forces that led to the rise and fall of each paradigm, and discuss the pressing issues and risks associated with the current paradigm shift in AI.
    
[^40]: 基于预训练语言模型的知识感知协同过滤用于个性化评论评分预测

    Knowledge-aware Collaborative Filtering with Pre-trained Language Model for Personalized Review-based Rating Prediction. (arXiv:2308.02555v1 [cs.IR])

    [http://arxiv.org/abs/2308.02555](http://arxiv.org/abs/2308.02555)

    该论文提出了一种名为知识感知协同过滤与预训练语言模型（KCF-PLM）的方法，用于个性化评论评分预测。该方法利用预训练语言模型来更好地建模用户和物品，并通过开发Transformer网络来考虑评论中的丰富知识。

    

    个性化的评论评分预测旨在利用现有的评论来建模用户兴趣和物品特征，以进行评分预测。大多数现有的研究主要遇到两个问题。首先，很少考虑每个评论的细粒度方面和知识图中包含的丰富知识，以补充纯文本，以更好地建模用户-物品交互。其次，尚未认真研究预训练语言模型在个性化评论评分预测中的作用。为了解决这些问题，我们提出了一种名为知识感知协同过滤与预训练语言模型（KCF-PLM）的方法。针对第一个问题，为了利用丰富的知识，KCF-PLM开发了一个Transformer网络，以对提取的方面与用户-物品对的交互进行建模。针对第二个问题，为了更好地表示用户和物品，KCF-PLM将用户或物品的所有历史评论作为预训练语言模型的输入。

    Personalized review-based rating prediction aims at leveraging existing reviews to model user interests and item characteristics for rating prediction. Most of the existing studies mainly encounter two issues. First, the rich knowledge contained in the fine-grained aspects of each review and the knowledge graph is rarely considered to complement the pure text for better modeling user-item interactions. Second, the power of pre-trained language models is not carefully studied for personalized review-based rating prediction. To address these issues, we propose an approach named Knowledge-aware Collaborative Filtering with Pre-trained Language Model (KCF-PLM). For the first issue, to utilize rich knowledge, KCF-PLM develops a transformer network to model the interactions of the extracted aspects w.r.t. a user-item pair. For the second issue, to better represent users and items, KCF-PLM takes all the historical reviews of a user or an item as input to pre-trained language models. Moreover,
    
[^41]: 以协同过滤捕捉AI用户偏好作为规范的方法

    Collaborative filtering to capture AI user's preferences as norms. (arXiv:2308.02542v1 [cs.IR])

    [http://arxiv.org/abs/2308.02542](http://arxiv.org/abs/2308.02542)

    本论文提出了一种以协同过滤方法构建规范以捕捉AI用户偏好的新视角。

    

    将AI技术根据每个用户的偏好进行定制是其良好运行的基础。然而，当前的方法需要用户过多参与，并且未能真正捕捉到他们的真实偏好。事实上，为了避免手动设置偏好的麻烦，用户通常会接受默认设置，即使这些设置与他们的真实偏好不符。规范可以用来调节行为，确保其符合用户的偏好，但是尽管文献已经详细研究了规范，大部分提议都是从形式化的角度出发。实际上，虽然已经有一些关于构建规范以捕捉用户隐私偏好的研究，但是这些方法依赖于领域知识，在AI技术的情况下，这很难获得和维护。我们认为，在构建规范时需要一种新的视角，即利用系统中大量用户的偏好信息。受到推荐系统的启发，我们相信协同过滤可以成为构建规范的方法。

    Customising AI technologies to each user's preferences is fundamental to them functioning well. Unfortunately, current methods require too much user involvement and fail to capture their true preferences. In fact, to avoid the nuisance of manually setting preferences, users usually accept the default settings even if these do not conform to their true preferences. Norms can be useful to regulate behaviour and ensure it adheres to user preferences but, while the literature has thoroughly studied norms, most proposals take a formal perspective. Indeed, while there has been some research on constructing norms to capture a user's privacy preferences, these methods rely on domain knowledge which, in the case of AI technologies, is difficult to obtain and maintain. We argue that a new perspective is required when constructing norms, which is to exploit the large amount of preference information readily available from whole systems of users. Inspired by recommender systems, we believe that co
    
[^42]: 迈向更具人类化的人工智能交流：紧急沟通研究综述

    Towards More Human-like AI Communication: A Review of Emergent Communication Research. (arXiv:2308.02541v1 [cs.CL])

    [http://arxiv.org/abs/2308.02541](http://arxiv.org/abs/2308.02541)

    本综述综合了紧急沟通研究的最新进展，旨在开发能够超越简单任务，并有效沟通和学习新概念的人工智能代理。

    

    在向以人为中心的人工智能转变的最近，机器准确使用自然语言的需求变得越来越重要。虽然实现这一目标的常见方法是训练大型语言模型，但这种方法可能存在学习错位，模型可能无法捕捉人类在使用自然语言时所使用的基本结构和推理方式，可能导致意想不到或不可靠的行为。紧急沟通（Emecom）是一个近年来发表的研究领域，旨在开发能够以超越简单的判别性任务，并能有效地交流和学习新概念的自然语言使用能力的人工代理。在本综述中，我们以两个方面介绍Emecom。首先，我们界定了文献中发现的所有常见特性及其与人类交互的关系。其次，我们确定了两个子类别，并突出它们的特点和开放挑战。

    In the recent shift towards human-centric AI, the need for machines to accurately use natural language has become increasingly important. While a common approach to achieve this is to train large language models, this method presents a form of learning misalignment where the model may not capture the underlying structure and reasoning humans employ in using natural language, potentially leading to unexpected or unreliable behavior. Emergent communication (Emecom) is a field of research that has seen a growing number of publications in recent years, aiming to develop artificial agents capable of using natural language in a way that goes beyond simple discriminative tasks and can effectively communicate and learn new concepts. In this review, we present Emecom under two aspects. Firstly, we delineate all the common proprieties we find across the literature and how they relate to human interactions. Secondly, we identify two subcategories and highlight their characteristics and open chall
    
[^43]: ALE: 用于NLP查询策略参数驱动比较的基于仿真的主动学习评估框架

    ALE: A Simulation-Based Active Learning Evaluation Framework for the Parameter-Driven Comparison of Query Strategies for NLP. (arXiv:2308.02537v1 [cs.CL])

    [http://arxiv.org/abs/2308.02537](http://arxiv.org/abs/2308.02537)

    ALE是一个用于对比NLP中AL策略的仿真主动学习评估框架，提供实证基础和公正的比较。

    

    监督机器学习和深度学习需要大量标记数据，数据科学家通过手动和耗时的注释过程获取。为了减轻这个挑战，主动学习（AL）提出将有希望的数据点推荐给注释员，以便他们注释接下来的数据，而不是随机或连续的样本。这种方法旨在节省注释工作量，同时保持模型的性能。然而，实践者面临着许多用于不同任务的AL策略，并且需要一个实证基础来选择它们之间的比较。调研将AL策略分类为没有性能指标的分类法。新型AL策略的介绍性演示与少量策略的性能比较。我们的贡献通过引入一种可重现的主动学习评估（ALE）框架来解决这个问题，用于比较评估NLP中的AL策略。该框架可以以较低的成本实现AL策略，并通过数据驱动的比较进行公正的比较。

    Supervised machine learning and deep learning require a large amount of labeled data, which data scientists obtain in a manual, and time-consuming annotation process. To mitigate this challenge, Active Learning (AL) proposes promising data points to annotators they annotate next instead of a subsequent or random sample. This method is supposed to save annotation effort while maintaining model performance. However, practitioners face many AL strategies for different tasks and need an empirical basis to choose between them. Surveys categorize AL strategies into taxonomies without performance indications. Presentations of novel AL strategies compare the performance to a small subset of strategies. Our contribution addresses the empirical basis by introducing a reproducible active learning evaluation (ALE) framework for the comparative evaluation of AL strategies in NLP. The framework allows the implementation of AL strategies with low effort and a fair data-driven comparison through defin
    
[^44]: 探索可解释性在AI辅助胚胎筛选中的作用

    Exploring the Role of Explainability in AI-Assisted Embryo Selection. (arXiv:2308.02534v1 [cs.CV])

    [http://arxiv.org/abs/2308.02534](http://arxiv.org/abs/2308.02534)

    本文探讨了AI辅助胚胎分析模型的可解释性问题，并提出了增加解释性和可信度的指南，旨在推动这项技术在临床实践中得到广泛应用。

    

    体外受精是治疗不孕的最常见方法之一，其主要挑战之一是评估和选择用于植入的胚胎，这是一个具有很大的临床医生间和内部变异性的过程。基于深度学习的方法正在引起关注，但它们的不透明性损害了它们在临床情境中的接受度，而在决策过程中的透明性是关键。在本文中，我们分析了AI辅助胚胎分析模型可解释性的当前工作，识别了其局限性。我们还讨论了如何将这些模型整合到临床情境中作为决策支持系统，并考虑临床医生和患者的需求。最后，我们提出了增加解释性和可信度的指南，推动这项技术向成熟的临床实践迈进。

    In Vitro Fertilization is among the most widespread treatments for infertility. One of its main challenges is the evaluation and selection of embryo for implantation, a process with large inter- and intra-clinician variability. Deep learning based methods are gaining attention, but their opaque nature compromises their acceptance in the clinical context, where transparency in the decision making is key. In this paper we analyze the current work in the explainability of AI-assisted embryo analysis models, identifying the limitations. We also discuss how these models could be integrated in the clinical context as decision support systems, considering the needs of clinicians and patients. Finally, we propose guidelines for the sake of increasing interpretability and trustworthiness, pushing this technology forward towards established clinical practice.
    
[^45]: Choir Transformer: 使用Transformer上的相对注意力生成多声部音乐

    Choir Transformer: Generating Polyphonic Music with Relative Attention on Transformer. (arXiv:2308.02531v1 [eess.AS])

    [http://arxiv.org/abs/2308.02531](http://arxiv.org/abs/2308.02531)

    本论文提出了一种名为Choir Transformer的神经网络模型，通过使用相对位置注意力来生成多声部音乐。实验结果表明，该模型在精度和和声指标方面都超过了之前的最先进方法。

    

    由于旋律和和声之间的正确联系，多声部音乐生成仍然是一个具有挑战性的方向。大部分以前的研究都使用基于RNN的模型，然而，基于RNN的模型难以建立长距离音符之间的关系。在本文中，我们提出了一种名为Choir Transformer的多声部音乐生成神经网络，利用相对位置注意力来更好地建模音乐结构。我们还提出了适用于多声部音乐生成的音乐表示方法。Choir Transformer的性能超过了4.06%的前期最先进精度。我们还测量了多声部音乐的和声指标，实验表明和声指标接近巴赫的音乐。在实际应用中，生成的旋律和节奏可以根据指定的输入进行调整，具有不同风格的音乐，如民间音乐或流行音乐等。

    Polyphonic music generation is still a challenge direction due to its correct between generating melody and harmony. Most of the previous studies used RNN-based models. However, the RNN-based models are hard to establish the relationship between long-distance notes. In this paper, we propose a polyphonic music generation neural network named Choir Transformer[ https://github.com/Zjy0401/choir-transformer], with relative positional attention to better model the structure of music. We also proposed a music representation suitable for polyphonic music generation. The performance of Choir Transformer surpasses the previous state-of-the-art accuracy of 4.06%. We also measures the harmony metrics of polyphonic music. Experiments show that the harmony metrics are close to the music of Bach. In practical application, the generated melody and rhythm can be adjusted according to the specified input, with different styles of music like folk music or pop music and so on.
    
[^46]: Gated Driver Attention Predictor （arXiv：2308.02530v1 [cs.CV]）

    Gated Driver Attention Predictor. (arXiv:2308.02530v1 [cs.CV])

    [http://arxiv.org/abs/2308.02530](http://arxiv.org/abs/2308.02530)

    Gate-DAP是一种用于驾驶员注意力预测的网络连接门控机制，通过学习不同信息源在驾驶场景中的重要性，提高了驾驶任务引导下的交通场景理解和事故预测的能力。

    

    驾驶员注意力预测涉及到对驾驶员的意图理解，包括驾驶员打算去哪里以及驾驶员关注的对象，通常提供驾驶任务引导下的交通场景理解。一些最近的研究探索了在紧急或事故场景下的驾驶员注意力预测，并发现在帮助事故预测方面起到了积极的作用，而其提升能力受到了驾驶员注意力地图预测准确性的限制。在这项工作中，我们探索了驾驶员注意力预测的网络连接门控机制（Gate-DAP）。Gate-DAP旨在学习不同空间、时间和模态信息在具有不同道路类型、场合以及光线和天气条件的驾驶场景中的重要性。Gate-DAP中的网络连接门控包括了空间编码网络门控、长短期记忆网络门控和信息类型门控模块。每个连接门控操作都是即插即用的，可以灵活组装。

    Driver attention prediction implies the intention understanding of where the driver intends to go and what object the driver concerned about, which commonly provides a driving task-guided traffic scene understanding. Some recent works explore driver attention prediction in critical or accident scenarios and find a positive role in helping accident prediction, while the promotion ability is constrained by the prediction accuracy of driver attention maps. In this work, we explore the network connection gating mechanism for driver attention prediction (Gate-DAP). Gate-DAP aims to learn the importance of different spatial, temporal, and modality information in driving scenarios with various road types, occasions, and light and weather conditions. The network connection gating in Gate-DAP consists of a spatial encoding network gating, long-short-term memory network gating, and information type gating modules. Each connection gating operation is plug-and-play and can be flexibly assembled, w
    
[^47]: 多目标进化组件对算法行为影响的研究

    Multiobjective Evolutionary Component Effect on Algorithm behavior. (arXiv:2308.02527v1 [cs.NE])

    [http://arxiv.org/abs/2308.02527](http://arxiv.org/abs/2308.02527)

    本研究探索了多目标进化算法中最具影响力的组件，通过自动设计出的元启发式算法来提高性能，对不同类型的问题进行了测试和比较。

    

    多目标进化算法的性能因问题而异，这使得开发新算法或将现有算法应用于新问题变得困难。为了简化新的多目标算法的开发和应用，人们对其组件的自动设计越来越感兴趣。这些自动设计的元启发式算法可以优于人工开发的对应算法。然而，目前仍不清楚导致性能改进的最具影响力的组件是什么。本研究提出一种新的方法来研究自动设计算法的最终配置的影响。我们将这种方法应用于基于分解的经过调整的多目标进化算法(MOEA/D)，该算法由迭代竞速(irace)配置包设计，应用于三组有约束问题：(1)分析真实世界问题，(2)分析人工问题，(3)模拟真实世界问题。

    The performance of multiobjective evolutionary algorithms (MOEAs) varies across problems, making it hard to develop new algorithms or apply existing ones to new problems. To simplify the development and application of new multiobjective algorithms, there has been an increasing interest in their automatic design from their components. These automatically designed metaheuristics can outperform their human-developed counterparts. However, it is still unknown what are the most influential components that lead to performance improvements. This study specifies a new methodology to investigate the effects of the final configuration of an automatically designed algorithm. We apply this methodology to a tuned Multiobjective Evolutionary Algorithm based on Decomposition (MOEA/D) designed by the iterated racing (irace) configuration package on constrained problems of 3 groups: (1) analytical real-world problems, (2) analytical artificial problems and (3) simulated real-world. We then compare the 
    
[^48]: 在泰国支持智能农业的聊天机器人应用

    Chatbot Application to Support Smart Agriculture in Thailand. (arXiv:2308.02524v1 [cs.CL])

    [http://arxiv.org/abs/2308.02524](http://arxiv.org/abs/2308.02524)

    本研究提出了一种在泰国支持智能农业的聊天机器人应用，为农民提供作物种植知识和建议，通过与智能农业和推荐系统配合使用，为农民提供数据监控和灌溉系统控制的功能。

    

    聊天机器人是一种能够自动快速地回复文本或语音对话的软件。在农业领域中，现有的智能农业系统仅使用来自感应和物联网技术的数据，不包括作物种植知识来支持农民的决策。为了增强这一点，聊天机器人应用可以成为农民的助手，提供作物种植知识。因此，我们提出了 LINE 聊天机器人应用作为信息和知识表示，为农民提供作物种植建议。它与智能农业和推荐系统配合使用。我们提出的 LINE 聊天机器人应用包括五个主要的功能（开始/停止菜单、主页、滴灌页面、雾灌页面和监控页面）。农民们将会获得用于数据监控以支持决策的信息。此外，他们可以通过 LINE 聊天机器人来控制灌溉系统。

    A chatbot is a software developed to help reply to text or voice conversations automatically and quickly in real time. In the agriculture sector, the existing smart agriculture systems just use data from sensing and internet of things (IoT) technologies that exclude crop cultivation knowledge to support decision-making by farmers. To enhance this, the chatbot application can be an assistant to farmers to provide crop cultivation knowledge. Consequently, we propose the LINE chatbot application as an information and knowledge representation providing crop cultivation recommendations to farmers. It works with smart agriculture and recommendation systems. Our proposed LINE chatbot application consists of five main functions (start/stop menu, main page, drip irri gation page, mist irrigation page, and monitor page). Farmers will receive information for data monitoring to support their decision-making. Moreover, they can control the irrigation system via the LINE chatbot. Furthermore, farmer
    
[^49]: 评估ChatGPT和GPT-4在视觉编程中的应用

    Evaluating ChatGPT and GPT-4 for Visual Programming. (arXiv:2308.02522v1 [cs.LG])

    [http://arxiv.org/abs/2308.02522](http://arxiv.org/abs/2308.02522)

    本研究评估了ChatGPT和GPT-4在视觉编程领域的应用，并发现它们具备与基于文本的Python编程相当的高级能力。

    

    生成式人工智能和大型语言模型有潜力通过自动生成个性化反馈和内容来极大改善计算机教育的格局。最近的研究探讨了这些模型在不同编程教育场景下的能力，然而这些研究仅考虑了基于文本的编程，特别是Python编程。因此，它们未解答这些模型在K-8编程教育中广泛使用的视觉编程领域中的表现如何。我们主要研究的问题是：最先进的生成模型是否具备与它们在基于文本的Python编程中相当的视觉编程能力？在我们的工作中，我们评估了两个模型，基于GPT-3.5的ChatGPT和GPT-4，针对不同场景在视觉编程领域进行评估，并通过专家的注释来评估性能。

    Generative AI and large language models have the potential to drastically improve the landscape of computing education by automatically generating personalized feedback and content. Recent works have studied the capabilities of these models for different programming education scenarios; however, these works considered only text-based programming, in particular, Python programming. Consequently, they leave open the question of how well these models would perform in visual programming domains popularly used for K-8 programming education. The main research question we study is: Do state-of-the-art generative models show advanced capabilities in visual programming on par with their capabilities in text-based Python programming? In our work, we evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, in visual programming domains for various scenarios and assess performance using expert-based annotations. In particular, we base our evaluation using reference tasks from the domains of Hour
    
[^50]: 语言模型作为主方程求解器

    Language models as master equation solvers. (arXiv:2308.02514v1 [cs.LG])

    [http://arxiv.org/abs/2308.02514](http://arxiv.org/abs/2308.02514)

    本研究将语言模型用作求解主方程的机器学习方法，通过设计提示网络和使用强化学习算法训练，实现了对多模组和高维系统的高精度求解。

    

    主方程在建模随机动力系统中具有基本重要性，然而由于状态空间维度的增加，解决主方程是具有挑战性的。本研究提出将语言模型重新应用为机器学习方法来解决主方程。我们设计了一个基于提示的神经网络，将速率参数、初始条件和时间值直接映射到与输入上下文完全匹配的状态联合概率分布。通过这种方式，我们近似地求解了主方程的最一般形式。我们使用强化学习框架中的策略梯度算法对网络进行训练，反馈奖励由一组变分自回归模型提供。通过将该方法应用于代表性示例，我们观察到对于多模组和高维系统，准确性很高。训练后的网络还展示了...

    Master equations are of fundamental importance in modeling stochastic dynamical systems.However, solving master equations is challenging due to the exponential increase in the number of possible states or trajectories with the dimension of the state space. In this study, we propose repurposing language models as a machine learning approach to solve master equations. We design a prompt-based neural network to map rate parameters, initial conditions, and time values directly to the state joint probability distribution that exactly matches the input contexts. In this way, we approximate the solution of the master equation in its most general form. We train the network using the policy gradient algorithm within the reinforcement learning framework, with feedback rewards provided by a set of variational autoregressive models. By applying this approach to representative examples, we observe high accuracy for both multi-module and high-dimensional systems. The trained network also exhibits ex
    
[^51]: 窥视大脑：通过人脑信号重建视觉知觉的图像

    Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals. (arXiv:2308.02510v1 [eess.IV])

    [http://arxiv.org/abs/2308.02510](http://arxiv.org/abs/2308.02510)

    本文通过重建观察到的图像来揭示人脑对视觉刺激的知觉机制，并提出了一种名为NeuroImagen的综合管道，利用脑电图信号重建视觉刺激图像。

    

    见到就信，然而人类视觉知觉与认知的基本机制仍是一个谜。由于神经科学和人工智能的最近进展，我们能够记录到视觉诱发的脑活动，并通过计算方法模拟视觉感知能力。本文关注通过重建观察到的图像来重建视觉刺激，基于易于获得的脑信号，即脑电图（EEG）数据。由于脑电图信号是动态的时间序列格式，同时也因有噪音而臭名昭著，处理和提取有用信息需要更多的专门工作；在本文中，我们提出了一个综合管道，名为NeuroImagen，用于从脑电图信号中重建视觉刺激图像。具体来说，我们结合了新颖的多层感知信息解码方法，以从给定的脑电图数据中获取多粒度的输出。

    Seeing is believing, however, the underlying mechanism of how human visual perceptions are intertwined with our cognitions is still a mystery. Thanks to the recent advances in both neuroscience and artificial intelligence, we have been able to record the visually evoked brain activities and mimic the visual perception ability through computational approaches. In this paper, we pay attention to visual stimuli reconstruction by reconstructing the observed images based on portably accessible brain signals, i.e., electroencephalography (EEG) data. Since EEG signals are dynamic in the time-series format and are notorious to be noisy, processing and extracting useful information requires more dedicated efforts; In this paper, we propose a comprehensive pipeline, named NeuroImagen, for reconstructing visual stimuli images from EEG signals. Specifically, we incorporate a novel multi-level perceptual information decoding to draw multi-grained outputs from the given EEG data. A latent diffusion 
    
[^52]: 一种多模态监督机器学习方法应用于欧洲卫星火灾识别

    A Multimodal Supervised Machine Learning Approach for Satellite-based Wildfire Identification in Europe. (arXiv:2308.02508v1 [eess.IV])

    [http://arxiv.org/abs/2308.02508](http://arxiv.org/abs/2308.02508)

    本研究提出了一种多模态监督机器学习方法，通过利用多个信息源，如热点数据和地表覆盖数据，实现了欧洲卫星火灾的自动识别。

    

    不断增长的灾难性自然事件，例如野火，呼吁开发快速和自动化的火灾检测系统。本文提出了一种野火识别解决方案，通过利用多个信息源提高自动化卫星热点检测系统的准确性。我们将Moderate-resolution Imaging Spectroradiometer (MODIS)和Visible Infrared Imaging Radiometer Suite (VIIRS)热点服务检测到的热异常与欧洲森林火灾信息系统 (EFFIS) 数据库进行交叉引用，构建了一个用于欧洲火灾研究的大规模热点数据集。然后，我们提出了一种新颖的多模态监督机器学习方法来消除热点检测中的歧义，区分野火和其他事件。我们的方法包括使用多模态数据源，例如ERSI年度土地利用与土地覆盖 (LULC) 和Copernicus Sentinel-3数据。实验结果表明

    The increasing frequency of catastrophic natural events, such as wildfires, calls for the development of rapid and automated wildfire detection systems. In this paper, we propose a wildfire identification solution to improve the accuracy of automated satellite-based hotspot detection systems by leveraging multiple information sources. We cross-reference the thermal anomalies detected by the Moderate-resolution Imaging Spectroradiometer (MODIS) and the Visible Infrared Imaging Radiometer Suite (VIIRS) hotspot services with the European Forest Fire Information System (EFFIS) database to construct a large-scale hotspot dataset for wildfire-related studies in Europe. Then, we propose a novel multimodal supervised machine learning approach to disambiguate hotspot detections, distinguishing between wildfires and other events. Our methodology includes the use of multimodal data sources, such as the ERSI annual Land Use Land Cover (LULC) and the Copernicus Sentinel-3 data. Experimental results
    
[^53]: 谁回答的更好？对ChatGPT和Stack Overflow回答软件工程问题进行深入分析

    Who Answers It Better? An In-Depth Analysis of ChatGPT and Stack Overflow Answers to Software Engineering Questions. (arXiv:2308.02312v1 [cs.SE])

    [http://arxiv.org/abs/2308.02312](http://arxiv.org/abs/2308.02312)

    本研究深入分析了ChatGPT和Stack Overflow回答软件工程问题的特点和可用性。结果显示，ChatGPT回答中有52%错误，77%冗长，但由于其综合性和清晰的语言表达，仍然在39.34%的情况下被使用者偏好选择。

    

    Q&A平台在过去十年中一直是程序员网上求助行为的重要组成部分。然而，随着ChatGPT的推出，网上求助行为的范式正在发生变化。尽管ChatGPT很受欢迎，但尚未进行全面的研究来评估ChatGPT回答软件工程问题的特点或可用性。为了填补这个空白，我们对ChatGPT回答517个Stack Overflow（SO）问题进行了首次深入分析，并对ChatGPT回答的正确性、一致性、综合性和简洁性进行了检查。此外，我们进行了大规模的语言分析和用户研究，以了解ChatGPT回答在语言和人类方面的特点。我们的分析表明，52％的ChatGPT回答是错误的，77％的回答冗长。尽管如此，由于其综合性和清晰的语言表达，ChatGPT回答仍然在39.34％的情况下受到青睐。

    Q&A platforms have been an integral part of the web-help-seeking behavior of programmers over the past decade. However, with the recent introduction of ChatGPT, the paradigm of web-help-seeking behavior is experiencing a shift. Despite the popularity of ChatGPT, no comprehensive study has been conducted to evaluate the characteristics or usability of ChatGPT's answers to software engineering questions. To bridge the gap, we conducted the first in-depth analysis of ChatGPT's answers to 517 Stack Overflow (SO) questions and examined the correctness, consistency, comprehensiveness, and conciseness of ChatGPT's answers. Furthermore, we conducted a large-scale linguistic analysis, and a user study to understand the characteristics of ChatGPT answers from linguistic and human aspects. Our analysis shows that 52\% of ChatGPT answers are incorrect and 77\% are verbose. Nonetheless, ChatGPT answers are still preferred 39.34\% of the time due to their comprehensiveness and well-articulated langu
    
[^54]: OpenFlamingo: 一个用于训练大型自回归视觉语言模型的开源框架

    OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models. (arXiv:2308.01390v1 [cs.CV])

    [http://arxiv.org/abs/2308.01390](http://arxiv.org/abs/2308.01390)

    OpenFlamingo是一个开源框架，用于训练大型自回归视觉语言模型。它在多个数据集上表现良好，达到了对应模型性能的80%至89%。

    

    我们介绍了OpenFlamingo，这是一系列自回归的视觉语言模型，参数范围从3B到9B。 OpenFlamingo是一个持续努力的项目，旨在复制DeepMind的Flamingo模型的开源版本。在七个视觉语言数据集上，OpenFlamingo模型的性能介于对应的Flamingo性能的80%至89%之间。本技术报告介绍了我们的模型、训练数据、超参数和评估套件。我们在https://github.com/mlfoundations/open_flamingo上分享我们的模型和代码。

    We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce an open-source replication of DeepMind's Flamingo models. On seven vision-language datasets, OpenFlamingo models average between 80 - 89% of corresponding Flamingo performance. This technical report describes our models, training data, hyperparameters, and evaluation suite. We share our models and code at https://github.com/mlfoundations/open_flamingo.
    
[^55]: LLMs理解玻璃盒模型，发现惊喜并提出修复建议。

    LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs. (arXiv:2308.01157v1 [stat.ML])

    [http://arxiv.org/abs/2308.01157](http://arxiv.org/abs/2308.01157)

    LLMs在处理可解释模型方面表现出色，提供了全面的模型级总结和自动化的异常检测、原因描述和修复建议。在医疗保健领域使用广义可加模型作为示例，同时介绍了开源的LLM-GAM接口包$\texttt{TalkToEBM}$。

    

    我们展示了大型语言模型(LLMs)在处理可解释模型方面的出色表现，这些模型可以将复杂结果分解为单一变量的图表示组件。通过采用层次推理的方法，LLMs能够在不需要整个模型适应上下文的情况下提供全面的模型级总结。这种方法使LLMs能够应用其广泛的背景知识来自动完成数据科学中的常见任务，如检测与先前知识相矛盾的异常，描述异常的潜在原因，并提出去除异常的修复建议。我们使用医疗保健领域的多个示例来证明LLMs的这些新能力的实用性，特别强调广义可加模型(GAMs)。最后，我们将$\texttt{TalkToEBM}$包作为一个开源的LLM-GAM接口进行介绍。

    We show that large language models (LLMs) are remarkably good at working with interpretable models that decompose complex outcomes into univariate graph-represented components. By adopting a hierarchical approach to reasoning, LLMs can provide comprehensive model-level summaries without ever requiring the entire model to fit in context. This approach enables LLMs to apply their extensive background knowledge to automate common tasks in data science such as detecting anomalies that contradict prior knowledge, describing potential reasons for the anomalies, and suggesting repairs that would remove the anomalies. We use multiple examples in healthcare to demonstrate the utility of these new capabilities of LLMs, with particular emphasis on Generalized Additive Models (GAMs). Finally, we present the package $\texttt{TalkToEBM}$ as an open-source LLM-GAM interface.
    
[^56]: 使用Floss增强周期性时间序列的表示学习：一种频域正则化方法

    Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach. (arXiv:2308.01011v1 [cs.LG])

    [http://arxiv.org/abs/2308.01011](http://arxiv.org/abs/2308.01011)

    本文提出了一种叫做Floss的无监督方法，通过在频域上对学到的表示进行正则化来增强周期性时间序列的表示学习。Floss方法可以自动检测时间序列中的周期性并学习具有周期一致性的有意义的表示。

    

    时间序列分析是各个应用领域的基础任务，深度学习方法在这个领域表现出了非凡的性能。然而，许多现实世界的时间序列数据展现出重要的周期性或准周期性动态，这些动态往往不能被现有的基于深度学习的解决方案充分捕捉到。这导致对感兴趣的基础动态行为的表示不完整。为了解决这个问题，我们提出了一种无监督的方法叫做Floss，它通过自动化地在频域上调整学到的表示来进行正则化。Floss方法首先自动检测时间序列中的主要周期性。然后，它利用周期移位和谱密度相似度度量来学习具有周期一致性的有意义的表示。此外，Floss可以轻松地整合到有监督、半监督和无监督的学习框架中。

    Time series analysis is a fundamental task in various application domains, and deep learning approaches have demonstrated remarkable performance in this area. However, many real-world time series data exhibit significant periodic or quasi-periodic dynamics that are often not adequately captured by existing deep learning-based solutions. This results in an incomplete representation of the underlying dynamic behaviors of interest. To address this gap, we propose an unsupervised method called Floss that automatically regularizes learned representations in the frequency domain. The Floss method first automatically detects major periodicities from the time series. It then employs periodic shift and spectral density similarity measures to learn meaningful representations with periodic consistency. In addition, Floss can be easily incorporated into both supervised, semi-supervised, and unsupervised learning frameworks. We conduct extensive experiments on common time series classification, for
    
[^57]: 人类-M3：一个用于室外场景中的3D人体姿势估计的多视角多模态数据集

    Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes. (arXiv:2308.00628v1 [cs.CV])

    [http://arxiv.org/abs/2308.00628](http://arxiv.org/abs/2308.00628)

    这篇论文提出了一个室外多模态多视角多人类姿势数据库Human-M3，并介绍了一种基于多模态数据输入的算法来生成准确的人体姿势。这个数据库解决了现有数据集的不足，提供了更多的数据多样性。

    

    最近，对于室外环境中的3D人体姿势估计越来越受到关注。然而，现有的室外场景3D人体姿势数据集缺乏多样性，因为它们主要只使用一种模态（RGB图像或点云），并且场景中通常只有一个人。数据集基础的有限范围严重阻碍了可用数据的变化性。在本文中，我们提出了Human-M3，这是一个室外多模态多视角多人类姿势数据库，其中包括室外场景的多视角RGB视频和相应的点云数据。为了获得准确的人体姿势，我们提出了一种基于多模态数据输入的算法来生成地面真值标注。这种方法利用了鲁棒的点云检测和跟踪，解决了之前室外场景中多个人的多视角RGB视频中可能存在的不准确人体定位和匹配模糊问题，生成了相关信息。

    3D human pose estimation in outdoor environments has garnered increasing attention recently. However, prevalent 3D human pose datasets pertaining to outdoor scenes lack diversity, as they predominantly utilize only one type of modality (RGB image or pointcloud), and often feature only one individual within each scene. This limited scope of dataset infrastructure considerably hinders the variability of available data. In this article, we propose Human-M3, an outdoor multi-modal multi-view multi-person human pose database which includes not only multi-view RGB videos of outdoor scenes but also corresponding pointclouds. In order to obtain accurate human poses, we propose an algorithm based on multi-modal data input to generate ground truth annotation. This benefits from robust pointcloud detection and tracking, which solves the problem of inaccurate human localization and matching ambiguity that may exist in previous multi-view RGB videos in outdoor multi-person scenes, and generates rel
    
[^58]: 在反应式系统内对神经网络进行形式化解释

    Formally Explaining Neural Networks within Reactive Systems. (arXiv:2308.00143v1 [cs.AI])

    [http://arxiv.org/abs/2308.00143](http://arxiv.org/abs/2308.00143)

    这项研究在反应式系统中提出了一种基于DNN验证的形式化XAI技术，可以解释DNN的行为，并且通过利用系统的转换约束来计算简洁的解释。

    

    深度神经网络(DNNs)越来越多地被用作反应式系统中的控制器。然而，DNNs具有高度的不透明性，这使得解释和证明它们的行为变得困难。为了解决这个问题，出现了对可解释AI(XAI)技术的兴趣激增，这些技术能够找出导致DNN行为的输入特征。现有的XAI技术通常存在两个限制：(i)它们是启发式方法，并不能提供解释正确性的正式保证；(ii)它们通常适用于“一次性”系统(即DNN独立于过去的调用)，而不是反应式系统。在这里，我们开始弥合这个差距，提出一种基于DNN验证的形式化XAI技术，用于推理多步骤的反应式系统。我们建议通过利用系统的转换约束来计算简洁的解释的方法，以便减少底层验证器所探索的搜索空间。

    Deep neural networks (DNNs) are increasingly being used as controllers in reactive systems. However, DNNs are highly opaque, which renders it difficult to explain and justify their actions. To mitigate this issue, there has been a surge of interest in explainable AI (XAI) techniques, capable of pinpointing the input features that caused the DNN to act as it did.  Existing XAI techniques typically face two limitations: (i) they are heuristic, and do not provide formal guarantees that the explanations are correct; and (ii) they often apply to ``one-shot'' systems (where the DNN is invoked independently of past invocations), as opposed to reactive systems.  Here, we begin bridging this gap, and propose a formal DNN-verification-based XAI technique for reasoning about multi-step, reactive systems. We suggest methods for efficiently calculating succinct explanations, by exploiting the system's transition constraints in order to curtail the search space explored by the underlying verifier. W
    
[^59]: 使用大型语言模型进行渗透测试：AI作为辅助

    Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v1 [cs.CL])

    [http://arxiv.org/abs/2308.00121](http://arxiv.org/abs/2308.00121)

    本文探讨了使用大型语言模型（如GPT3.5）作为AI助手来增强渗透测试人员的能力，实现了高级任务规划和低级漏洞寻找两种用例，取得了有前景的初步结果，并就提供该技术的伦理问题进行了讨论。

    

    软件安全测试领域，尤其是渗透测试是一项需要高水平专业知识的活动，并涉及许多手动测试和分析步骤。本文探讨了使用大型语言模型（如GPT3.5）来增强渗透测试人员的能力。我们研究了两种不同的用例：用于安全测试任务的高级任务规划和在易受攻击的虚拟机中进行低级漏洞寻找。对于后者，我们实现了一个闭环反馈，将由语言模型生成的低级操作与易受攻击的虚拟机（通过SSH连接）相连，并允许语言模型分析虚拟机状态以寻找漏洞，并提供具体的攻击向量。我们讨论了有前景的初步结果，详细介绍了改进的途径，并就提供该技术的伦理问题进行了讨论。

    The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providi
    
[^60]: 分布式动态规划和分布式TD-Learning的网络多智能体马尔可夫决策过程的ODE框架

    Distributed Dynamic Programming and an O.D.E. Framework of Distributed TD-Learning for Networked Multi-Agent Markov Decision Processes. (arXiv:2307.16706v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2307.16706](http://arxiv.org/abs/2307.16706)

    本文研究了网络多智能体马尔可夫决策问题中的分布式动态规划和分布式TD学习算法。其中，我们通过引入新的分布式DP算法和分布式TD学习算法，并证明了它们的收敛性，提出了两个关键点。该分布式DP算法具有两个独立的动态系统的特点。

    

    本文主要研究网络多智能体马尔可夫决策问题中的分布式动态规划（DP）和分布式时序差分（TD）学习算法。我们采用分布式多智能体框架，其中各个智能体只能访问自己的奖励，缺乏对其他智能体奖励的了解。此外，每个智能体都能通过一个由图表示的通信网络与相邻智能体共享其参数。我们的贡献可以总结为两个关键点：1）我们引入了一个新的受连续时间区间内的平均一致性方法启发的分布式DP。通过控制理论的视角评估了该DP的收敛性。2）基于上述DP，我们设计了一个新的分布式TD学习算法并证明了其收敛性。我们提出的分布式DP的一个显著特点是其包含了两个独立的动态系统。

    The primary objective of this paper is to investigate distributed dynamic programming (DP) and distributed temporal difference (TD) learning algorithms for networked multi-agent Markov decision problems (MAMDPs). In our study, we adopt a distributed multi-agent framework where individual agents have access only to their own rewards, lacking insights into the rewards of other agents. Additionally, each agent has the ability to share its parameters with neighboring agents through a communication network, represented by a graph. Our contributions can be summarized in two key points: 1) We introduce a novel distributed DP, inspired by the averaging consensus method in the continuous-time domain. The convergence of this DP is assessed through control theory perspectives. 2) Building upon the aforementioned DP, we devise a new distributed TD-learning algorithm and prove its convergence. A standout feature of our proposed distributed DP is its incorporation of two independent dynamic systems,
    
[^61]: Relation-Oriented: 迈向与知识对准的因果人工智能

    Relation-Oriented: Toward Knowledge-Aligned Causal AI. (arXiv:2307.16387v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2307.16387](http://arxiv.org/abs/2307.16387)

    本研究从创新的关系导向视角出发，探讨了当前的建模范式中的观察模型与实际理解的不对齐问题，并提出了关系定义的表示学习方法作为实现关系导向建模的实践方法。

    

    在机器学习中，我们自然地应用一个观察导向的原则，其中观察变量先存在并为构建关系奠定基础。虽然对于传统模型来说足够了，但是人工智能与大数据的整合暴露了观察模型与我们的实际理解之间的不对齐。相反，人类塑造了由关系定义的认知实体，使我们能够跨越时间和超维度空间制定知识，而不是被限制在观察构建中。从一种创新的关系导向的视角出发，本研究通过来自计算机视觉和健康信息学的直观例子，分析了在我们当前的建模范式中这种不对齐的根源。我们还介绍了关系定义的表示学习方法作为关系导向建模的一种实际实施，支持广泛的实验验证。

    In machine learning, we naturally apply an Observation-Oriented principle, in which observational variables preexist and set the stage for constructing relationships. While sufficient for traditional models, the integration of AI with big data exposes the misalignment between the observational models and our actual comprehension. Contrarily, humans shape cognitive entities defined by relationships, enabling us to formulate knowledge across temporal and hyper-dimensional spaces, rather than being confined to observational constructs. From an innovative Relation-Oriented perspective, this study examines the roots of this misalignment within our current modeling paradigm, illuminated by intuitive examples from computer vision and health informatics. We also introduce the relation-defined representation learning methodology as a practical implementation of Relation-Oriented modeling, supported by extensive experimental validation.
    
[^62]: 一种基于LSTM、BiLSTM、CNN、GRU和GloVe的混合机器学习模型用于基因突变在癌症中的分类

    A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.14361](http://arxiv.org/abs/2307.14361)

    本研究提出了一个基于多种机器学习算法和嵌入模型的集成模型，用于基因突变在癌症中的分类。实验结果表明，该模型在准确率、精确率、召回率等指标上优于其他传统和最新的转换器模型，并且具有更高的训练效率。

    

    本研究提出了一个集成模型，将LSTM、BiLSTM、CNN、GRU和GloVe结合起来，用于在Kaggle的“个性化医学：重新定义癌症治疗”数据集中对基因突变进行分类。通过与BERT、Electra、Roberta、XLNet、Distilbert以及它们的LSTM集成等知名转换器进行比较，结果显示我们的模型在准确率、精确率、召回率、F1分数和均方误差方面都优于其他模型。令人惊讶的是，它还需要较少的训练时间，实现了性能和效率的完美结合。该研究证明了集成模型在基因突变分类等困难任务中的实用性。

    This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
    
[^63]: AI4GCC - 团队: 低于海平面: 评论和改进

    AI4GCC - Team: Below Sea Level: Critiques and Improvements. (arXiv:2307.13894v1 [cs.CY])

    [http://arxiv.org/abs/2307.13894](http://arxiv.org/abs/2307.13894)

    这项研究对气候变化评估模型RICE-N进行了批判性分析，发现了关键问题并提出了改进建议，同时也对综合评估模型的特征进行了讨论。研究结果有助于进一步发展RICE-N框架，为政策制定者提供参考。

    

    我们对模拟框架RICE-N进行了批判性分析，这是一个用于评估气候变化对经济影响的综合评估模型(IAM)。我们确定了RICE-N的关键问题，包括行动屏蔽和无关的行动，并提出了改进的建议，如利用关税收入和惩罚过量生产。我们还批判性地讨论了IAM的特征，即过于乐观的损失函数和不现实的减排成本函数。我们的研究结果有助于不断改进RICE-N框架，使其作为政策制定者的灵感更加有用。

    We present a critical analysis of the simulation framework RICE-N, an integrated assessment model (IAM) for evaluating the impacts of climate change on the economy. We identify key issues with RICE-N, including action masking and irrelevant actions, and suggest improvements such as utilizing tariff revenue and penalizing overproduction. We also critically engage with features of IAMs in general, namely overly optimistic damage functions and unrealistic abatement cost functions. Our findings contribute to the ongoing efforts to further develop the RICE-N framework in an effort to improve the simulation, making it more useful as an inspiration for policymakers.
    
[^64]: 在定量双极论证框架中的论证归因解释

    Argument Attribution Explanations in Quantitative Bipolar Argumentation Frameworks. (arXiv:2307.13582v1 [cs.AI])

    [http://arxiv.org/abs/2307.13582](http://arxiv.org/abs/2307.13582)

    本文提出了一个新的“论证归因解释（AAEs）”理论，用于确定论证对“主题论证”产生的影响，在定量双极论证框架（QBAFs）中填补了解释定量推理结果的空白。

    

    近年来，有几个人提倡论证性可解释人工智能，越来越多的人对论证框架（AFs）的推理结果进行解释产生了兴趣。虽然关于用辩论/争论/对话的扩展语义精神定性地解释AFs的推理结果的研究成果很多，但是在渐进语义下解释AFs的定量推理结果却没有得到太多关注，尽管在应用中广泛使用。本文通过将机器学习中的特征归因精神引入定量双极论证框架（QBAFs）的背景中，提出了一个新的“论证归因解释（AAEs）”理论，用于确定论证对“主题论证”产生的影响，而特征归因则用于确定特征对机器学习模型输出的影响。

    Argumentative explainable AI has been advocated by several in recent years, with an increasing interest on explaining the reasoning outcomes of Argumentation Frameworks (AFs). While there is a considerable body of research on qualitatively explaining the reasoning outcomes of AFs with debates/disputes/dialogues in the spirit of \emph{extension-based semantics}, explaining the quantitative reasoning outcomes of AFs under \emph{gradual semantics} has not received much attention, despite widespread use in applications. In this paper, we contribute to filling this gap by proposing a novel theory of \emph{Argument Attribution Explanations (AAEs)} by incorporating the spirit of feature attribution from machine learning in the context of Quantitative Bipolar Argumentation Frameworks (QBAFs): whereas feature attribution is used to determine the influence of features towards outputs of machine learning models, AAEs are used to determine the influence of arguments towards \emph{topic argument}s 
    
[^65]: 通过LED照明调制对人脸识别系统进行不可察觉的物理攻击

    Imperceptible Physical Attack against Face Recognition Systems via LED Illumination Modulation. (arXiv:2307.13294v1 [cs.CV])

    [http://arxiv.org/abs/2307.13294](http://arxiv.org/abs/2307.13294)

    本论文提出了一种通过LED照明调制对人脸识别系统进行不可察觉的物理攻击，通过快速强度调制生成难以察觉的亮度变化，并利用图像传感器的卷帘快门效应向捕获的人脸图像中注入亮度信息扰动。

    

    尽管人脸识别在我们的日常生活中开始扮演重要角色，但我们需要注意到，基于数据驱动的人脸识别视觉系统容易受到对抗性攻击。然而，当前两种对抗性攻击，即数字攻击和物理攻击，都有缺点，前者不实用，后者显眼、计算量大且不可执行。为解决这些问题，我们提出了一种实用、可执行、不显眼且计算量较低的对抗性攻击方法，基于LED照明调制。为了欺骗系统，该攻击方法通过对场景LED照明进行快速强度调制，在人眼看不到的范围内生成难以察觉的亮度变化，并利用CMOS图像传感器的卷帘快门效应，向捕获的人脸图像中注入亮度信息扰动。总之，我们提出了一种用于人脸检测的拒绝服务（DoS）攻击以及一种用于人脸验证的躲避攻击。

    Although face recognition starts to play an important role in our daily life, we need to pay attention that data-driven face recognition vision systems are vulnerable to adversarial attacks. However, the current two categories of adversarial attacks, namely digital attacks and physical attacks both have drawbacks, with the former ones impractical and the latter one conspicuous, high-computational and inexecutable. To address the issues, we propose a practical, executable, inconspicuous and low computational adversarial attack based on LED illumination modulation. To fool the systems, the proposed attack generates imperceptible luminance changes to human eyes through fast intensity modulation of scene LED illumination and uses the rolling shutter effect of CMOS image sensors in face recognition systems to implant luminance information perturbation to the captured face images. In summary,we present a denial-of-service (DoS) attack for face detection and a dodging attack for face verifica
    
[^66]: 大型语言模型中的上下文学习在学习标签关系上具有创新，但并非传统学习方法

    In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning. (arXiv:2307.12375v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.12375](http://arxiv.org/abs/2307.12375)

    大型语言模型（LLMs）在包含标签关系示例的上下文中的学习能力使其在下游任务中表现显著提高，但与传统学习方法不同。我们研究了上下文示例中的标签如何影响预测、预训练中学习到的标签关系如何与上下文示例相互作用以及上下文学习如何聚合标签信息。研究结果揭示了LLMs的工作机制及其对上下文信息的处理方式。

    

    在下游任务中，大型语言模型（LLMs）的性能在包含输入-标签关系示例的上下文中通常显著提高。然而，目前对LLMs的这种上下文学习（ICL）能力的工作机制尚无共识：例如，虽然Xie等人（2021年）将ICL比作一种通用学习算法，但Min等人（2022b年）认为ICL甚至不能从上下文示例中学习标签关系。在本文中，我们研究了以下三个问题：（1）上下文示例的标签如何影响预测结果，（2）预训练期间学习到的标签关系如何与上下文中提供的输入-标签示例相互作用，以及（3）ICL如何聚合来自上下文示例的标签信息。我们的研究发现，LLMs通常会整合上下文标签的信息，但预训练和上下文标签关系被区别对待，模型不会将所有上下文信息等同对待。我们的结果揭示了对LLMs的理解。

    The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into underst
    
[^67]: 用物理信知的神经网络解决维度诅咒问题

    Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v1 [cs.LG])

    [http://arxiv.org/abs/2307.12306](http://arxiv.org/abs/2307.12306)

    本文提出了一种新方法，利用物理信知的神经网络(PINNs)解决高维度的偏微分方程(PDEs)问题，并证明了收敛性和其他期望属性。

    

    维度诅咒(CoD)随着维度的增加，以指数级增长的计算成本来极度税费计算资源。这在解决高维偏微分方程(PDEs)中面临极大挑战，正如Richard Bellman在60年前首次指出的那样。尽管近年来在高维度上数值解决偏微分方程(PDEs)取得了一些成功，但这样的计算代价过高，而将一般非线性PDEs扩展到高维度从未实现过。本文提出了一种新方法，将物理信知的神经网络(PINNs)扩展到解决任意高维PDEs。该新方法称为随机维度梯度下降(SDGD)，将PDE的梯度分解为与不同维度对应的部分，并在训练PINNs的每次迭代中随机选择这些维度部分的子集进行采样。我们在理论上证明了所提出方法的收敛保证和其他期望属性。

    The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed meth
    
[^68]: TableGPT：将表格，自然语言和命令统一到一个GPT中

    TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT. (arXiv:2307.08674v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2307.08674](http://arxiv.org/abs/2307.08674)

    TableGPT是一个统一的框架，利用大型语言模型（LLMs）和外部功能命令使LLMs能够无缝地与表格进行交互，实现广泛的功能，并提供便利和可访问性给用户。其中的创新是全局表格表示的概念，使LLMs能够全面理解表格的结构和内容。

    

    表格在现实世界的数据库中非常普遍，需要人们花费大量时间和精力进行分析和操作。大型语言模型（LLMs）的进步使得使用自然语言输入与表格交互成为可能，使得这种能力更加接近现实。本文介绍了TableGPT，这是一个统一的精调框架，使得LLMs能够利用外部功能命令理解和操作表格。它引入了与表格无缝交互的能力，实现了广泛的功能，如问答、数据操作（例如插入、删除、查询和修改操作）、数据可视化、分析报告生成和自动预测。TableGPT旨在通过使用户能够轻松利用表格数据来提供便利和可访问性。TableGPT的核心是全局表格表示的新概念，它使LLMs能够全面理解表格的结构和内容，并将自然语言和命令操作对表格实现无缝集成。

    Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the ent
    
[^69]: 走向普遍的语义元宇宙：挑战、方法和机遇

    Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and Opportunities. (arXiv:2307.06687v1 [cs.HC])

    [http://arxiv.org/abs/2307.06687](http://arxiv.org/abs/2307.06687)

    走向普遍的语义元宇宙：通过人工智能、时空数据表示、语义物联网和语义增强数字孪生实现的智能、上下文感知的交互技术，在远程教育、工作与协作、娱乐与社交、医疗保健和电子商务营销等领域具有重要应用价值。

    

    近年来，普遍的语义元宇宙被研究用来革新增强现实（AR）和虚拟现实（VR）用户的沉浸式网络虚拟体验，利用先进的语义理解与表示来实现在混合现实环境中的无缝、上下文感知的交互。本调查重点关注普遍的语义元宇宙中四个基本系统组件的智能和时空特征，即人工智能（AI）、时空数据表示（STDR）、语义物联网（SIoT）和语义增强数字孪生（SDT）。我们全面调查了这四个基本系统组件的典型技术，使得在普遍的语义元宇宙中能够进行智能、个性化、上下文感知的交互，并给出了典型的使用案例，如远程教育、工作与协作、娱乐与社交、医疗保健和电子商务营销等。

    In recent years, ubiquitous semantic Metaverse has been studied to revolutionize immersive cyber-virtual experiences for augmented reality (AR) and virtual reality (VR) users, which leverages advanced semantic understanding and representation to enable seamless, context-aware interactions within mixed-reality environments. This survey focuses on the intelligence and spatio-temporal characteristics of four fundamental system components in ubiquitous semantic Metaverse, i.e., artificial intelligence (AI), spatio-temporal data representation (STDR), semantic Internet of Things (SIoT), and semantic-enhanced digital twin (SDT). We thoroughly survey the representative techniques of the four fundamental system components that enable intelligent, personalized, and context-aware interactions with typical use cases of the ubiquitous semantic Metaverse, such as remote education, work and collaboration, entertainment and socialization, healthcare, and e-commerce marketing. Furthermore, we outline 
    
[^70]: 大语言模型时代的推荐系统 (LLMs)

    Recommender Systems in the Era of Large Language Models (LLMs). (arXiv:2307.02046v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2307.02046](http://arxiv.org/abs/2307.02046)

    大型语言模型在推荐系统中的应用已经带来了显著的改进，克服了传统DNN方法的限制，并提供了强大的语言理解、生成、推理和泛化能力。

    

    随着电子商务和网络应用的繁荣，推荐系统（RecSys）已经成为我们日常生活中重要的组成部分，为用户提供个性化建议以满足其喜好。尽管深度神经网络（DNN）通过模拟用户-物品交互和整合文本侧信息在提升推荐系统方面取得了重要进展，但是DNN方法仍然存在一些限制，例如理解用户兴趣、捕捉文本侧信息的困难，以及在不同推荐场景中泛化和推理能力的不足等。与此同时，大型语言模型（LLMs）的出现（例如ChatGPT和GPT4）在自然语言处理（NLP）和人工智能（AI）领域引起了革命，因为它们在语言理解和生成的基本职责上有着卓越的能力，同时具有令人印象深刻的泛化和推理能力。

    With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, 
    
[^71]: Pareto-安全的机器学习（PSML）：指纹和保护推断服务系统。

    Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems. (arXiv:2307.01292v1 [cs.CR])

    [http://arxiv.org/abs/2307.01292](http://arxiv.org/abs/2307.01292)

    本论文研究了模型服务系统的安全性，通过引入一个查询高效的指纹算法，使得攻击者能够一致地触发任何想要的模型，从而增强了对模型提取攻击的鲁棒性和准确性。

    

    随着大型基础模型的出现，模型服务系统越来越受欢迎。在这样的系统中，用户将查询发送到服务器，并指定所需的性能指标（例如准确性、延迟等）。服务器在后端维护一组模型（模型库），并根据指定的指标提供查询服务。本文研究了这些系统的安全性，特别是对模型提取攻击的鲁棒性。现有的黑盒攻击不能直接应用于提取受害模型，因为模型隐藏在推理服务接口背后的模型库中，攻击者无法确定使用的是哪个模型。需要一个中间步骤来确保每个输入查询都能得到受害模型的输出。为此，我们提出了一种查询高效的指纹算法，使攻击者能够一致地触发任何想要的模型。我们证明，通过使用我们的指纹算法，模型提取可以具有保真度和准确性。

    With the emergence of large foundational models, model-serving systems are becoming popular. In such a system, users send the queries to the server and specify the desired performance metrics (e.g., accuracy, latency, etc.). The server maintains a set of models (model zoo) in the back-end and serves the queries based on the specified metrics. This paper examines the security, specifically robustness against model extraction attacks, of such systems. Existing black-box attacks cannot be directly applied to extract a victim model, as models hide among the model zoo behind the inference serving interface, and attackers cannot identify which model is being used. An intermediate step is required to ensure that every input query gets the output from the victim model. To this end, we propose a query-efficient fingerprinting algorithm to enable the attacker to trigger any desired model consistently. We show that by using our fingerprinting algorithm, model extraction can have fidelity and accu
    
[^72]: 使用语法演化自动设计语义相似性集合

    Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution. (arXiv:2307.00925v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.00925](http://arxiv.org/abs/2307.00925)

    本研究首次使用语法演化自动设计语义相似性集合，通过自动选择和聚合候选度量来优化集合与人类判断的相关性，提高相似度评估准确性，并证明了使用集合对语义相似性任务的益处。

    

    语义相似性度量在自然语言处理中被广泛应用于多种与计算机相关的任务。然而，没有单一的语义相似性度量适用于所有任务，研究人员经常使用集合策略来确保性能。本研究提出了一种自动设计语义相似性集合的方法。事实上，我们提出的方法首次使用语法演化来自动选择和聚合一组候选度量，以创建一个最大化与人类判断相关性的集合。该方法在多个基准数据集上进行了评估，并与最先进的集合进行了比较，结果显示它可以显著提高相似度评估的准确性，并在某些情况下优于现有方法。因此，我们的研究既展示了使用语法演化来自动比较文本的潜力，也证明了使用集合对语义相似性任务的益处。

    Semantic similarity measures are widely used in natural language processing to catalyze various computer-related tasks. However, no single semantic similarity measure is the most appropriate for all tasks, and researchers often use ensemble strategies to ensure performance. This research work proposes a method for automatically designing semantic similarity ensembles. In fact, our proposed method uses grammatical evolution, for the first time, to automatically select and aggregate measures from a pool of candidates to create an ensemble that maximizes correlation to human judgment. The method is evaluated on several benchmark datasets and compared to state-of-the-art ensembles, showing that it can significantly improve similarity assessment accuracy and outperform existing methods in some cases. As a result, our research demonstrates the potential of using grammatical evolution to automatically compare text and prove the benefits of using ensembles for semantic similarity tasks. The so
    
[^73]: 弹性约束下的元学习器用于联邦学习

    Elastically-Constrained Meta-Learner for Federated Learning. (arXiv:2306.16703v1 [cs.LG])

    [http://arxiv.org/abs/2306.16703](http://arxiv.org/abs/2306.16703)

    这项研究提出了一种弹性约束的元学习方法，用于解决联邦学习中由于非独立同分布数据导致元学习的不稳定目标的收敛问题。

    

    联邦学习是一种协作训练机器学习模型的方法，用于多个参与方之间禁止数据共享。在联邦学习中的一个挑战是客户端之间的非独立同分布数据，因为单个模型无法适应所有客户端的数据分布。为了解决这个问题，介绍了元学习（如Per-FedAvg）。元学习学习适用于所有客户端的共享初始参数。每个客户端使用梯度下降法将初始化快速调整到本地数据分布，实现模型个性化。然而，由于非凸损失函数和采样更新的随机性，元学习方法在本地适应同一客户端时具有不稳定的目标。这种不同适应方向的波动阻碍了元学习的收敛。为了克服这个挑战，我们使用了历史本地调整的模型来限制内循环的方向，并提出了一种弹性约束方法。

    Federated learning is an approach to collaboratively training machine learning models for multiple parties that prohibit data sharing. One of the challenges in federated learning is non-IID data between clients, as a single model can not fit the data distribution for all clients. Meta-learning, such as Per-FedAvg, is introduced to cope with the challenge. Meta-learning learns shared initial parameters for all clients. Each client employs gradient descent to adapt the initialization to local data distributions quickly to realize model personalization. However, due to non-convex loss function and randomness of sampling update, meta-learning approaches have unstable goals in local adaptation for the same client. This fluctuation in different adaptation directions hinders the convergence in meta-learning. To overcome this challenge, we use the historical local adapted model to restrict the direction of the inner loop and propose an elastic-constrained method. As a result, the current round
    
[^74]: 使用分割学习预测软件性能

    Predicting Software Performance with Divide-and-Learn. (arXiv:2306.06651v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2306.06651](http://arxiv.org/abs/2306.06651)

    本文提出了一种名为$DaL$的基于分割学习的方法，用于预测高度配置的软件系统的性能。实验证明了该方法的有效性。

    

    预测高度配置的软件系统的性能是性能测试和质量保证的基础。为此，最近的研究依靠机器/深度学习来建模软件性能。然而，一个至关重要但未解决的挑战是如何满足配置景观中继承的稀疏性：配置选项（特征）的影响和数据样本的分布都非常稀疏。本文提出了一种基于“分割学习”概念的方法，称为$DaL$。基本思想是，为了处理样本稀疏性，我们将配置景观中的样本划分为远离的部分，对于每个部分，我们建立一个规范化的深度神经网络作为本地模型来处理特征稀疏性。然后，新给定的配置将被分配给最终预测的正确模型。八个真实系统和五组训练数据的实验结果显示

    Predicting the performance of highly configurable software systems is the foundation for performance testing and quality assurance. To that end, recent work has been relying on machine/deep learning to model software performance. However, a crucial yet unaddressed challenge is how to cater for the sparsity inherited from the configuration landscape: the influence of configuration options (features) and the distribution of data samples are highly sparse.  In this paper, we propose an approach based on the concept of 'divide-and-learn', dubbed $DaL$. The basic idea is that, to handle sample sparsity, we divide the samples from the configuration landscape into distant divisions, for each of which we build a regularized Deep Neural Network as the local model to deal with the feature sparsity. A newly given configuration would then be assigned to the right model of division for the final prediction.  Experiment results from eight real-world systems and five sets of training data reveal that
    
[^75]: 通过生成式AI，证明了隐形图像水印是可清除的

    Invisible Image Watermarks Are Provably Removable Using Generative AI. (arXiv:2306.01953v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2306.01953](http://arxiv.org/abs/2306.01953)

    该论文证明了使用生成式AI可以清除隐形图像水印，提出了一种家族化再生攻击方法。通过形式化证明和实证结果，论文展示了所有隐形水印容易受到攻击，并针对一种具有弹性的水印RivaGAN，再生攻击可以去除93-99%的水印。

    

    隐形水印通过嵌入只有权利拥有者可以检测到的隐藏信息来保护图像的版权。它们还防止人们滥用由AI模型生成的图像。我们提出了一种家族化再生攻击来清除这些隐形水印。所提出的攻击方法首先向图像添加随机噪声来破坏水印，然后重建图像。这种方法灵活，可以与许多现有的图像降噪算法和预训练的生成模型（如扩散模型）实例化。通过形式化证明和实证结果，我们证明了所有隐形水印都容易受到所提出的攻击。对于一个特别有弹性的水印RivaGAN，再生攻击可以去除93-99%的隐形水印，而基线攻击只能去除不超过3%。然而，如果我们不要求带水印的图像与原始图像相同，保持图像语义相似的水印可能是一种替代方案。

    Invisible watermarks safeguard images' copyright by embedding hidden messages only detectable by owners. They also prevent people from misusing images, especially those generated by AI models. We propose a family of regeneration attacks to remove these invisible watermarks. The proposed attack method first adds random noise to an image to destroy the watermark and then reconstructs the image. This approach is flexible and can be instantiated with many existing image-denoising algorithms and pre-trained generative models such as diffusion models. Through formal proofs and empirical results, we show that all invisible watermarks are vulnerable to the proposed attack. For a particularly resilient watermark, RivaGAN, regeneration attacks remove 93-99% of the invisible watermarks while the baseline attacks remove no more than 3%. However, if we do not require the watermarked image to look the same as the original one, watermarks that keep the image semantically similar can be an alternative
    
[^76]: 提升联合学习的视觉语言预训练：基于联合学习的问答与密集字幕生成

    Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner. (arXiv:2305.11769v1 [cs.CV])

    [http://arxiv.org/abs/2305.11769](http://arxiv.org/abs/2305.11769)

    本文提出了一种名为JADE的新方法，可以利用易于获取的图像-文本对进行的联合学习，以提升视觉和语言模态的细粒度特征对齐，从而更好地进行视觉问答和密集字幕生成。

    

    大型预先训练的多模态模型在许多下游任务中都表现出显著的成功，包括图像字幕生成、图像文本检索和视觉问答等。然而，许多方法都依赖于从网络上收集的图像-文本对作为预先训练的数据，忽视了视觉和语言模态之间需要细粒度特征对齐的需求，这需要对图像和语言表达进行详细的理解。将视觉问答和密集字幕集成到预先训练中可以解决这个问题，但是获取图像-问题-答案以及图像-位置-字幕三元组是具有挑战性和耗时的。此外，公开可用的视觉问答和密集字幕数据集通常由于手动数据收集和标注而规模有限。在本文中，我们提出了一种新方法，称为联合问答和密集字幕生成（JADE），它利用预先训练的多模态模型和易于获取的图像-文本对来进行模型训练。

    Large pre-trained multimodal models have demonstrated significant success in a range of downstream tasks, including image captioning, image-text retrieval, visual question answering (VQA), etc. However, many of these methods rely on image-text pairs collected from the web as pre-training data and unfortunately overlook the need for fine-grained feature alignment between vision and language modalities, which requires detailed understanding of images and language expressions. While integrating VQA and dense captioning (DC) into pre-training can address this issue, acquiring image-question-answer as well as image-location-caption triplets is challenging and time-consuming. Additionally, publicly available datasets for VQA and dense captioning are typically limited in scale due to manual data collection and labeling efforts. In this paper, we propose a novel method called Joint QA and DC GEneration (JADE), which utilizes a pre-trained multimodal model and easily-crawled image-text pairs to
    
[^77]: Graphologue：用交互式图表探索大型语言模型响应

    Graphologue: Exploring Large Language Model Responses with Interactive Diagrams. (arXiv:2305.11473v1 [cs.HC])

    [http://arxiv.org/abs/2305.11473](http://arxiv.org/abs/2305.11473)

    Graphologue是一个交互式系统，将大型语言模型的基于文本的响应转换为图形化图表以增强其可用性和可解释性，用户可以通过选择和突出显示特定节点和链接来与这些图表进行交互。

    

    大型语言模型（LLM）由于易于获取和在多种应用中表现出的前所未有的智能而近来风靡一时。然而，像ChatGPT这样的LLM在支持复杂信息任务方面存在显着的限制，原因是基于文本的媒介和线性对话结构提供的功能不足。通过与十名参与者的形式化研究，我们发现LLM界面通常会呈现冗长的响应，使人们难以快速理解和灵活地与各种信息进行交互，特别是在更复杂的任务中。我们提出了Graphologue，这是一个交互式系统，将LLM的基于文本的响应转换为图形化图表，以便于信息查找和问题回答任务。Graphologue采用新颖的提示策略和界面设计，从LLM响应中提取实体和关系，并实时构建节点链接图。此外，用户可以通过选择和突出显示特定节点和链接来与这些图表进行交互，以探索相关信息和跟进问题。我们的用户研究结果表明，与传统的基于文本的界面相比，Graphologue显著提高了用户在复杂信息任务中的表现和满意度。Graphologue为增强LLM在各种应用和领域中的可用性和可解释性提供了一个有前景的方向。

    Large language models (LLMs) have recently soared in popularity due to their ease of access and the unprecedented intelligence exhibited on diverse applications. However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure. Through a formative study with ten participants, we found that LLM interfaces often present long-winded responses, making it difficult for people to quickly comprehend and interact flexibly with various pieces of information, particularly during more complex tasks. We present Graphologue, an interactive system that converts text-based responses from LLMs into graphical diagrams to facilitate information-seeking and question-answering tasks. Graphologue employs novel prompting strategies and interface designs to extract entities and relationships from LLM responses and constructs node-link diagrams in real-time. Further, users can int
    
[^78]: 图像的背叛：贝叶斯场景关键点在机器人操作中的深度策略学习

    The Treachery of Images: Bayesian Scene Keypoints for Deep Policy Learning in Robotic Manipulation. (arXiv:2305.04718v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.04718](http://arxiv.org/abs/2305.04718)

    该论文介绍了一种用于机器人操作中的深度策略学习的贝叶斯场景关键点跟踪方法。该方法通过解决图像中的歧义问题，实现了对于对称物体和遮挡和视野之外物体的关键点跟踪，提高了相机观察的效用，对于策略学习具有优势。

    

    在机器人操作的策略学习中，样本效率至关重要。因此，从相机观察中学习和提取更紧凑的表示是一个有前途的途径。然而，当前的方法常常假设场景的完全可观测性，并且对于尺度不变性的处理存在困难。在许多任务和情景中，这个假设并不成立，因为场景中的物体经常被遮挡或者位于相机的视野之外，导致相机观察在物体位置方面产生歧义。为了解决这个问题，我们提出了BASK，一种贝叶斯方法来跟踪随时间变化的尺度不变关键点。我们的方法成功地解决了图像中固有的歧义问题，实现了对对称物体和遮挡和视野之外的物体的关键点跟踪。我们利用我们的方法从手腕相机观察中学习具有挑战性的多物体机器人操作任务，并与其他表示学习方法相比，展示了更出色的策略学习效果。

    In policy learning for robotic manipulation, sample efficiency is of paramount importance. Thus, learning and extracting more compact representations from camera observations is a promising avenue. However, current methods often assume full observability of the scene and struggle with scale invariance. In many tasks and settings, this assumption does not hold as objects in the scene are often occluded or lie outside the field of view of the camera, rendering the camera observation ambiguous with regard to their location. To tackle this problem, we present BASK, a Bayesian approach to tracking scale-invariant keypoints over time. Our approach successfully resolves inherent ambiguities in images, enabling keypoint tracking on symmetrical objects and occluded and out-of-view objects. We employ our method to learn challenging multi-object robot manipulation tasks from wrist camera observations and demonstrate superior utility for policy learning compared to other representation learning te
    
[^79]: 基于归因的防御插入式文本后门攻击

    Defending against Insertion-based Textual Backdoor Attacks via Attribution. (arXiv:2305.02394v1 [cs.CL])

    [http://arxiv.org/abs/2305.02394](http://arxiv.org/abs/2305.02394)

    本文提出了一种基于归因的管道AttDef，用于防御两种插入式污染攻击BadNL和InSent，该管道可以成功缓解插入式文本后门攻击并在四个基准数据集上平均提高了56.59%至79.97%和15.25%至48.34%的准确率。

    

    文本后门攻击是一种新型攻击模式，已被证明在训练期间向模型添加后门是有效的。防御此类后门攻击已变得紧迫和重要。本文提出了一种名为AttDef的高效归因管道，用于防御两种插入式污染攻击BadNL和InSent。具体而言，我们将具有较大归因分数的令牌视为潜在触发器，因为较大的归因词对于错误预测结果做出较大贡献，因此更有可能是污染触发器。此外，我们进一步利用外部预训练语言模型来区分输入是否被污染。我们展示了我们的方法可以在两种常见的攻击场景（污染训练数据和测试数据）中具有足够的泛化性，这一点持续改善了之前的方法。例如，AttDef在四个基准数据集上可以成功缓解两种攻击，平均准确率为79.97%（提高了56.59%）和48.34%（提高了15.25%），证明了它在防御插入式文本后门攻击方面的有效性。

    Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training. Defending against such backdoor attacks has become urgent and important. In this paper, we propose AttDef, an efficient attribution-based pipeline to defend against two insertion-based poisoning attacks, BadNL and InSent. Specifically, we regard the tokens with larger attribution scores as potential triggers since larger attribution words contribute more to the false prediction results and therefore are more likely to be poison triggers. Additionally, we further utilize an external pre-trained language model to distinguish whether input is poisoned or not. We show that our proposed method can generalize sufficiently well in two common attack scenarios (poisoning training data and testing data), which consistently improves previous methods. For instance, AttDef can successfully mitigate both attacks with an average accuracy of 79.97% (56.59% up) and 48.34% 
    
[^80]: 利用实时模拟的内在随机性促进机器人操作的强化学习

    Exploiting Intrinsic Stochasticity of Real-Time Simulation to Facilitate Robust Reinforcement Learning for Robot Manipulation. (arXiv:2304.06056v1 [cs.RO])

    [http://arxiv.org/abs/2304.06056](http://arxiv.org/abs/2304.06056)

    本文研究了实时模拟的内在随机性特性及其在强化学习中的应用，旨在提高RL方法鲁棒性和域随机化性能。

    

    对于像机器人操作这样的安全关键型应用，模拟对于强化学习（RL）在实际实现前是必不可少的，但是RL代理往往对模拟与实际世界之间的差异敏感。我们研究了现成模拟软件实时模拟（RT-IS）的内在随机性特性及其潜力，以提高RL方法的鲁棒性和域随机化的性能。

    Simulation is essential to reinforcement learning (RL) before implementation in the real world, especially for safety-critical applications like robot manipulation. Conventionally, RL agents are sensitive to the discrepancies between the simulation and the real world, known as the sim-to-real gap. The application of domain randomization, a technique used to fill this gap, is limited to the imposition of heuristic-randomized models. We investigate the properties of intrinsic stochasticity of real-time simulation (RT-IS) of off-the-shelf simulation software and its potential to improve the robustness of RL methods and the performance of domain randomization. Firstly, we conduct analytical studies to measure the correlation of RT-IS with the occupation of the computer hardware and validate its comparability with the natural stochasticity of a physical robot. Then, we apply the RT-IS feature in the training of an RL agent. The simulation and physical experiment results verify the feasibili
    
[^81]: 基于梯度稀疏化和差分隐私的高效无线联合学习

    Gradient Sparsification for Efficient Wireless Federated Learning with Differential Privacy. (arXiv:2304.04164v1 [cs.DC])

    [http://arxiv.org/abs/2304.04164](http://arxiv.org/abs/2304.04164)

    本文提出了一种基于梯度稀疏化和差分隐私的无线联合学习框架，使用随机稀疏化算法缓解DP引起的性能下降，并减少上传的参数数量，提高训练效率而不损失收敛性能。

    

    联合学习使分布式客户端在不共享原始数据的情况下协同训练机器学习模型。但是，由于上传模型而泄漏私有信息。此外，随着模型大小的增加，由于有限的传输带宽，训练延迟增加，同时使用差分隐私（DP）保护时模型性能会下降。在本文中，我们提出了一种基于梯度稀疏化和差分隐私的无线联合学习框架，以提高训练效率而不损失收敛性能。具体而言，我们首先设计了一个随机稀疏化算法，在每个客户端的本地训练中保留一部分梯度元素，从而缓解了DP引起的性能下降，并减少了无线信道上传输的参数数量。然后，我们通过建模非凸FL问题分析了所提出算法的收敛度界。接下来，我们提出了一个分布式联合优化问题，使用Alternating Direction Method of Multipliers（ADMM）解决其优化问题。

    Federated learning (FL) enables distributed clients to collaboratively train a machine learning model without sharing raw data with each other. However, it suffers the leakage of private information from uploading models. In addition, as the model size grows, the training latency increases due to limited transmission bandwidth and the model performance degrades while using differential privacy (DP) protection. In this paper, we propose a gradient sparsification empowered FL framework over wireless channels, in order to improve training efficiency without sacrificing convergence performance. Specifically, we first design a random sparsification algorithm to retain a fraction of the gradient elements in each client's local training, thereby mitigating the performance degradation induced by DP and and reducing the number of transmission parameters over wireless channels. Then, we analyze the convergence bound of the proposed algorithm, by modeling a non-convex FL problem. Next, we formula
    
[^82]: 生成代理: 人类行为的交互仿真器

    Generative Agents: Interactive Simulacra of Human Behavior. (arXiv:2304.03442v1 [cs.HC])

    [http://arxiv.org/abs/2304.03442](http://arxiv.org/abs/2304.03442)

    本文介绍了一种生成代理的架构，它能够仿真出具有可信度的人类行为，填充交互式沙盒环境，为创造更加真实的人机交互体验提供了一种新的思路。

    

    可信的人类行为仿真可赋能于从沉浸式环境到人际交流排练空间到原型工具的交互式应用程序。在本文中，我们介绍了生成代理——具有可信度的人类行为仿真的计算机软件代理。生成代理会起床，做早餐，去工作；艺术家画画，作家写作；他们形成观点，互相注意，并开始交谈；他们回忆过去的日子并计划未来。为了使生成代理能够实现，我们描述了一种架构，它将大型语言模型扩展到使用自然语言存储代理的经历的完整记录，随着时间的推移综合这些记忆到更高层次的反思，以及动态检索这些记忆以规划行为。我们实例化生成代理以填充受《模拟人生》启发的交互式沙盒环境，最终用户可以使用自然语言对话系统与25个代理交互。

    Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natur
    
[^83]: 开放词汇视频实例分割的探索

    Towards Open-Vocabulary Video Instance Segmentation. (arXiv:2304.01715v1 [cs.CV])

    [http://arxiv.org/abs/2304.01715](http://arxiv.org/abs/2304.01715)

    本文提出了新任务--开放词汇视频实例分割，并收集了大规模的LV-VIS数据集，同时提出了高效的MindVLT方法，能够以近实时的速度实现开放集视频实例分割任务，相比现有方法有显著的提升。

    

    视频实例分割（VIS）旨在从一组封闭的训练类别中对视频中的对象进行分割和分类，缺乏处理真实世界中新类别的泛化能力。为了解决这个问题，本文提出了三个方案。首先，我们引入了开放词汇视频实例分割的新任务，旨在同时从开放集类别中对视频中的对象进行分割、跟踪和分类，包括训练期间未见过的新类别。其次，为了评测开放词汇实例分割，我们收集了包含1,212个不同类别的大规模词汇视频实例分割数据集（LV-VIS），显著超出了现有数据集的类别规模一个数量级以上。第三，我们提出了一种高效的记忆驱动视觉语言变换器MindVLT，以实现近实时端到端的开放词汇视频实例分割。广泛的实验结果表明，我们提出的MindVLT在封闭集和开放集视频实例分割任务上显著优于现有方法。

    Video Instance Segmentation(VIS) aims at segmenting and categorizing objects in videos from a closed set of training categories, lacking the generalization ability to handle novel categories in real-world videos. To address this limitation, we make the following three contributions. First, we introduce the novel task of Open-Vocabulary Video Instance Segmentation, which aims to simultaneously segment, track, and classify objects in videos from open-set categories, including novel categories unseen during training. Second, to benchmark Open-Vocabulary VIS, we collect a Large-Vocabulary Video Instance Segmentation dataset(LV-VIS), that contains well-annotated objects from 1,212 diverse categories, significantly surpassing the category size of existing datasets by more than one order of magnitude. Third, we propose an efficient Memory-Induced Vision-Language Transformer, MindVLT, to first achieve Open-Vocabulary VIS in an end-to-end manner with near real-time inference speed. Extensive ex
    
[^84]: 自监督多模态学习：一项综述

    Self-Supervised Multimodal Learning: A Survey. (arXiv:2304.01008v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.01008](http://arxiv.org/abs/2304.01008)

    自监督多模态学习是一项旨在解决多模态数据中的自监督学习挑战的研究方向。它通过学习来自原始多模态数据中的表示，并解决了没有标签的多模态数据学习、不同模态的融合和不对齐数据学习等问题。

    

    多模态学习旨在理解和分析来自多种模态的信息，在监督学习范式下取得了重大进展。然而，由于依赖于配对数据和昂贵的人工注释，模型的扩展性受到了限制。与此同时，鉴于野外有大规模未注释的数据可用，自监督学习成为缓解注释瓶颈的一种有吸引力的策略。自监督多模态学习（SSML）建立在这两个方向的基础上，提供了从原始多模态数据中学习的方法。在本综述中，我们全面回顾了SSML的最新进展，阐述了自监督学习在多模态数据中面临的三个主要挑战：（1）在没有标签的多模态数据中学习表示，（2）不同模态的融合，以及（3）与不对齐数据的学习。然后，我们详细介绍了这些挑战的现有解决方案。具体而言，我们考虑了（1）目标

    Multimodal learning, which aims to understand and analyze information from multiple modalities, has achieved substantial progress in the supervised regime in recent years. However, the heavy dependence on data paired with expensive human annotations impedes scaling up models. Meanwhile, given the availability of large-scale unannotated data in the wild, self-supervised learning has become an attractive strategy to alleviate the annotation bottleneck. Building on these two directions, self-supervised multimodal learning (SSML) provides ways to learn from raw multimodal data. In this survey, we provide a comprehensive review of the state-of-the-art in SSML, in which we elucidate three major challenges intrinsic to self-supervised learning with multimodal data: (1) learning representations from multimodal data without labels, (2) fusion of different modalities, and (3) learning with unaligned data. We then detail existing solutions to these challenges. Specifically, we consider (1) object
    
[^85]: 泛型源代码的神经符号执行

    Neuro-Symbolic Execution of Generic Source Code. (arXiv:2304.00989v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2304.00989](http://arxiv.org/abs/2304.00989)

    这项研究提出了一种新的神经模型，能够根据源代码执行泛型程序，并引入了神经符号执行问题。该模型能够执行Py150数据集程序，包括没有具体输入的库函数，并可以用于变量误用定位和修复。

    

    能否使用根据源代码组合而成的神经网络逐语句执行Python程序？我们提出了神经符号执行问题，并引入了神经解释（Neural Interpretation，NI），这是第一个能够按照源代码执行泛型源代码的神经模型，允许缺失定义。NI保留源代码结构，其中每个变量都有一个向量编码，每个函数执行一个神经网络。NI是一种新颖的神经计算机模型，具有编译器架构，可以组装由源代码“编程”的神经层。NI是第一个能够执行Py150数据集程序的神经模型，包括没有具体输入的库函数，并且可以根据灵活的代码理解目标进行训练。我们展示了针对变量误用定位和修复的无具体输入的白盒执行。

    Can a Python program be executed statement-by-statement by neural networks composed according to the source code? We formulate the Neuro-Symbolic Execution Problem and introduce Neural Interpretation (NI), the first neural model for the execution of generic source code that allows missing definitions. NI preserves source code structure, where every variable has a vector encoding, and every function executes a neural network. NI is a novel neural model of computers with a compiler architecture that can assemble neural layers "programmed" by source code. NI is the first neural model capable of executing Py150 dataset programs, including library functions without concrete inputs, and it can be trained with flexible code understanding objectives. We demonstrate white-box execution without concrete inputs for variable misuse localization and repair.
    
[^86]: LLM用于患者-试验匹配: 面向更好的性能和泛化能力的隐私感知数据增强

    LLM for Patient-Trial Matching: Privacy-Aware Data Augmentation Towards Better Performance and Generalizability. (arXiv:2303.16756v1 [cs.CL])

    [http://arxiv.org/abs/2303.16756](http://arxiv.org/abs/2303.16756)

    本文提出了一种隐私感知数据增强的LLM-PTM方法，有效地提高了患者-试验匹配的性能和泛化能力。

    

    将患者与适合的临床试验进行匹配是推进医学研究和提供最佳护理的关键。然而，现有方法面临数据标准化、伦理考虑和电子健康记录与临床试验标准之间互操作性缺乏等挑战。在本文中，我们探索利用大型语言模型（LLMs）解决这些挑战的潜力，通过利用其先进的自然语言生成能力来改善EHRs和临床试验描述之间的兼容性。我们提出了一种创新的基于LLM的患者-试验匹配（LLM-PTM）的隐私感知数据增强方法，平衡了LLMs的好处，同时确保敏感患者数据的安全和保密。我们的实验表明，使用所提出的LLM-PTM方法，性能平均提高了7.32％，新数据的泛化能力提高了12.12％。此外，我们还提供了案例研究。

    The process of matching patients with suitable clinical trials is essential for advancing medical research and providing optimal care. However, current approaches face challenges such as data standardization, ethical considerations, and a lack of interoperability between Electronic Health Records (EHRs) and clinical trial criteria. In this paper, we explore the potential of large language models (LLMs) to address these challenges by leveraging their advanced natural language generation capabilities to improve compatibility between EHRs and clinical trial descriptions. We propose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM), which balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data. Our experiments demonstrate a 7.32% average improvement in performance using the proposed LLM-PTM method, and the generalizability to new data is improved by 12.12%. Additionally, we present case stud
    
[^87]: 智能化的民主化：多种含义、目标和方法

    Democratising AI: Multiple Meanings, Goals, and Methods. (arXiv:2303.12642v1 [cs.AI])

    [http://arxiv.org/abs/2303.12642](http://arxiv.org/abs/2303.12642)

    这篇论文探讨了AI的民主化，包括四种类型的民主化：AI使用的民主化，AI开发的民主化，AI利润的民主化，和AI治理的民主化。要想实现有效的政策和权衡讨论，需要认识到AI治理的民主化在决策中扮演着重要的角色。

    

    许多人呼吁实现AI的民主化，但这个词语用来指代多种目标，有时会相互冲突。本文确定了通常讨论的四种AI民主化类型：(1) AI使用的民主化，(2) AI开发的民主化，(3) AI利润的民主化，和(4) AI治理的民主化。本文讨论了实现每种民主化形式的多个目标和方法。从本文中主要得出的结论是，AI的民主化是一个多元而有时会相互冲突的概念，不应混淆AI可访问性的改善。如果我们想要超越对智能化民主化的模糊承诺，进入具体政策和权衡的生产性讨论，我们需要认识到AI治理的民主化在跨越关于使用、开发和利润的决策中导航权衡和风险的主要作用。

    Numerous parties are calling for the democratisation of AI, but the phrase is used to refer to a variety of goals, the pursuit of which sometimes conflict. This paper identifies four kinds of AI democratisation that are commonly discussed: (1) the democratisation of AI use, (2) the democratisation of AI development, (3) the democratisation of AI profits, and (4) the democratisation of AI governance. Numerous goals and methods of achieving each form of democratisation are discussed. The main takeaway from this paper is that AI democratisation is a multifarious and sometimes conflicting concept that should not be conflated with improving AI accessibility. If we want to move beyond ambiguous commitments to democratising AI, to productive discussions of concrete policies and trade-offs, then we need to recognise the principal role of the democratisation of AI governance in navigating tradeoffs and risks across decisions around use, development, and profits.
    
[^88]: 学习奖励信息获取：正确计分规则遇到委托代理模型

    Learning to Incentivize Information Acquisition: Proper Scoring Rules Meet Principal-Agent Model. (arXiv:2303.08613v1 [cs.LG])

    [http://arxiv.org/abs/2303.08613](http://arxiv.org/abs/2303.08613)

    本文设计了一种样本高效算法，将 UCB 算法（Auer等人，2002）应用于委托代理模型的在线设置，该算法能够通过与策略代理多次互动来设计最优的计分规则，并实现良好的效果。

    

    本文研究委托代理模型中的激励信息获取问题。此问题被建模为委托方和代理方之间的 Stackelberg 博弈，其中委托人宣布了一条得分规则来指定付款，然后代理方选择最大化其自身利润和报告信息的努力水平。我们从委托方的角度研究这个问题的在线设置，即通过与策略代理多次交互来设计最优计分规则。我们设计了一种可证明的样本高效算法，将 UCB 算法 (Auer et al., 2002) 量身定制到我们的模型中，其在 T 次迭代后实现了次线性 $T^{2/3}$-遗憾。我们的算法具有对委托方最优利润进行精细估计的过程以及保守纠正方案，以确保代理方的行动得到有效激励。此外，我们的遗憾界的一个关键特征是它是渐进最小可实现的。

    We study the incentivized information acquisition problem, where a principal hires an agent to gather information on her behalf. Such a problem is modeled as a Stackelberg game between the principal and the agent, where the principal announces a scoring rule that specifies the payment, and then the agent then chooses an effort level that maximizes her own profit and reports the information. We study the online setting of such a problem from the principal's perspective, i.e., designing the optimal scoring rule by repeatedly interacting with the strategic agent. We design a provably sample efficient algorithm that tailors the UCB algorithm (Auer et al., 2002) to our model, which achieves a sublinear $T^{2/3}$-regret after $T$ iterations. Our algorithm features a delicate estimation procedure for the optimal profit of the principal, and a conservative correction scheme that ensures the desired agent's actions are incentivized. Furthermore, a key feature of our regret bound is that it is i
    
[^89]: GOATS：目标采样自适应课程强化学习用于舀取任务

    GOATS: Goal Sampling Adaptation for Scooping with Curriculum Reinforcement Learning. (arXiv:2303.05193v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.05193](http://arxiv.org/abs/2303.05193)

    本文提出了一种名为GOATS的方法，使用目标采样自适应课程强化学习技术，通过插值位置目标和数量目标的分布创建学习过程中的课程来解决机器人舀取任务中的位置目标和水量目标问题，取得了比基线更好的表现。

    

    本文首先使用目标条件强化学习对机器人舀取水的问题进行了阐述。由于流体的复杂动力学和实现多模式目标的需求，该任务具有特别的挑战性。政策需要成功地达到位置目标和水量目标，这导致一个庞大而复杂的目标状态空间。为了克服这些挑战，我们引入了GOATS，一种课程强化学习方法，通过插值位置目标分布和数量目标分布来创建学习过程中的课程，使用目标分解奖励公式，学习一个高效且具有通用性的机器人舀取策略。结果，我们的方法可以在仿真中表现出比基线更好的性能，分别在碗舀和桶舀任务中实现了5.46％和8.71％的误差，涵盖了1000种初始水状态的变化。

    In this work, we first formulate the problem of robotic water scooping using goal-conditioned reinforcement learning. This task is particularly challenging due to the complex dynamics of fluid and the need to achieve multi-modal goals. The policy is required to successfully reach both position goals and water amount goals, which leads to a large convoluted goal state space. To overcome these challenges, we introduce Goal Sampling Adaptation for Scooping (GOATS), a curriculum reinforcement learning method that can learn an effective and generalizable policy for robot scooping tasks. Specifically, we use a goal-factorized reward formulation and interpolate position goal distributions and amount goal distributions to create curriculum throughout the learning process. As a result, our proposed method can outperform the baselines in simulation and achieves 5.46% and 8.71% amount errors on bowl scooping and bucket scooping tasks, respectively, under 1000 variations of initial water states in
    
[^90]: 从BriVL中探索高效调优的学习音频表示方法

    Exploring Efficient-Tuned Learning Audio Representation Method from BriVL. (arXiv:2303.04585v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2303.04585](http://arxiv.org/abs/2303.04585)

    本文提出了一种基于BriVL的稳健音频表示学习方法WavBriVL，通过将音频、图像和文本投影到共享的嵌入空间中，实现多模态应用。实验结果表明，该方法可以通过音频生成适当的图像。

    

    最近，研究人员逐渐意识到，在某些情况下，对大规模互联网数据进行自监督预训练要好于高质量/手动标记的数据集，并且多模态/大模型要好于单一或双模态/小模型。本文提出了一种基于Bridging-Vision-and-Language（BriVL）的稳健音频表示学习方法WavBriVL。WavBriVL将音频、图像和文本投影到共享的嵌入空间中，从而实现多模态应用。我们通过对从WavBriVL生成的图像进行定性评估，来实现本文的主要目的：（1）学习音频和图像之间的相关性；（2）探索一种新的图像生成方式，即使用音频生成图像。实验结果表明，该方法可以有效地从音频中生成适当的图像。

    Recently, researchers have gradually realized that in some cases, the self-supervised pre-training on large-scale Internet data is better than that of high-quality/manually labeled data sets, and multimodal/large models are better than single or bimodal/small models. In this paper, we propose a robust audio representation learning method WavBriVL based on Bridging-Vision-and-Language (BriVL). WavBriVL projects audio, image and text into a shared embedded space, so that multi-modal applications can be realized. We demonstrate the qualitative evaluation of the image generated from WavBriVL as a shared embedded space, with the main purposes of this paper:(1) Learning the correlation between audio and image;(2) Explore a new way of image generation, that is, use audio to generate pictures. Experimental results show that this method can effectively generate appropriate images from audio.
    
[^91]: DroNeRF: 实时多代理无人机位姿优化用于计算神经辐射场

    DroNeRF: Real-time Multi-agent Drone Pose Optimization for Computing Neural Radiance Fields. (arXiv:2303.04322v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.04322](http://arxiv.org/abs/2303.04322)

    DroNeRF是一种实时多代理无人机位姿优化算法，在只使用少量图像的情况下，通过计算物体几何来实现自主定位和3D重建，并结合神经辐射场技术生成独特而动态的新视图。

    

    我们提出了一种新的优化算法DroNeRF，用于自主定位单目无人机围绕一个物体进行实时三维重建，只使用少量图像。神经辐射场（Neural Radiance Fields）是一种新颖的视图合成技术，用于从一组输入图像生成物体或场景的新视图。使用无人机结合NeRF提供了一种独特而动态的方式来生成场景的新视图，尤其是在场景能力有限且移动受限的情况下。我们的方法侧重于计算各个无人机的优化位姿，仅依赖于物体几何而不使用任何外部定位系统。在数据捕获阶段期间独特的相机定位显著影响着3D模型的质量。为了评估我们生成的新视图的质量，我们计算了不同的感知度量，如峰值信噪比（PSNR）和结构相似性指数测度（SSIM）。

    We present a novel optimization algorithm called DroNeRF for the autonomous positioning of monocular camera drones around an object for real-time 3D reconstruction using only a few images. Neural Radiance Fields or NeRF, is a novel view synthesis technique used to generate new views of an object or scene from a set of input images. Using drones in conjunction with NeRF provides a unique and dynamic way to generate novel views of a scene, especially with limited scene capabilities of restricted movements. Our approach focuses on calculating optimized pose for individual drones while solely depending on the object geometry without using any external localization system. The unique camera positioning during the data-capturing phase significantly impacts the quality of the 3D model. To evaluate the quality of our generated novel views, we compute different perceptual metrics like the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure(SSIM). Our work demonstrates the 
    
[^92]: SG-LSTM: 用于机器人在密集人群中导航的社交群组LSTM

    SG-LSTM: Social Group LSTM for Robot Navigation Through Dense Crowds. (arXiv:2303.04320v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.04320](http://arxiv.org/abs/2303.04320)

    SG-LSTM模型用于预测人在拥挤环境中的移动，通过社交群组LSTM实现对人群和互动的建模，以更准确地预测轨迹，在机器人导航中提高了碰撞路径的计算速度和准确性。

    

    随着个人机器人的可用性和价格的增加，它们将不再局限于大型公司仓库或工厂，而是预计将在较不受控制的环境中与更大的人群一起工作。除了确保安全和效率外，最重要的是减少机器人对人类可能产生的负面心理影响，并在这些情况下遵守未书写的社交规范。我们的研究旨在开发一个模型，可以预测拥挤环境中行人和感知社交群体的移动。我们引入了一种新的社交群组长短期记忆（SG-LSTM）模型，使用具有社交意识的LSTM来对密集环境中的人群和互动进行建模，以产生更准确的轨迹预测。我们的方法可以使导航算法在拥挤环境中更快、更准确地计算无碰撞路径。此外，我们还发布了一个带有标记行人群体的大型视频数据集。

    With the increasing availability and affordability of personal robots, they will no longer be confined to large corporate warehouses or factories but will instead be expected to operate in less controlled environments alongside larger groups of people. In addition to ensuring safety and efficiency, it is crucial to minimize any negative psychological impact robots may have on humans and follow unwritten social norms in these situations. Our research aims to develop a model that can predict the movements of pedestrians and perceptually-social groups in crowded environments. We introduce a new Social Group Long Short-term Memory (SG-LSTM) model that models human groups and interactions in dense environments using a socially-aware LSTM to produce more accurate trajectory predictions. Our approach enables navigation algorithms to calculate collision-free paths faster and more accurately in crowded environments. Additionally, we also release a large video dataset with labeled pedestrian gro
    
[^93]: NovPhy：一个在开放世界环境中进行物理推理的测试平台

    NovPhy: A Testbed for Physical Reasoning in Open-world Environments. (arXiv:2303.01711v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.01711](http://arxiv.org/abs/2303.01711)

    NovPhy是一个为了开放世界物理推理而设计的测试平台，要求智能体在存在新颖情况的物理场景中进行推理并相应地采取行动。

    

    随着与物理环境交互的AI系统的出现，将物理推理能力融入这些AI系统越来越受关注。但仅仅具备物理推理能力是否足以在真实物理环境中运行？在现实世界中，我们经常面对之前从未遇到过的新颖情况。作为人类，我们能够成功地适应这些情况。同样，一个智能体需要具备在面对新颖情况时正常运作的能力，才能在开放世界的物理环境中有效运行。为了推动这样的AI系统的发展，我们提出了一个新的测试平台NovPhy，要求智能体在存在新颖情况的物理场景中进行推理并相应地采取行动。测试平台包括需要智能体检测和适应物理场景中的新颖情况的任务。为了创建测试平台中的任务，我们开发了八种代表不同新颖情况的创新点。

    Due to the emergence of AI systems that interact with the physical environment, there is an increased interest in incorporating physical reasoning capabilities into those AI systems. But is it enough to only have physical reasoning capabilities to operate in a real physical environment? In the real world, we constantly face novel situations we have not encountered before. As humans, we are competent at successfully adapting to those situations. Similarly, an agent needs to have the ability to function under the impact of novelties in order to properly operate in an open-world physical environment. To facilitate the development of such AI systems, we propose a new testbed, NovPhy, that requires an agent to reason about physical scenarios in the presence of novelties and take actions accordingly. The testbed consists of tasks that require agents to detect and adapt to novelties in physical scenarios. To create tasks in the testbed, we develop eight novelties representing a diverse novelt
    
[^94]: 基于全同态加密的隐私保护树型推理

    Privacy-Preserving Tree-Based Inference with Fully Homomorphic Encryption. (arXiv:2303.01254v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2303.01254](http://arxiv.org/abs/2303.01254)

    本研究介绍了一种基于全同态加密的数据隐私保护方法，能够针对加密表格数据进行任意计算，并得到了最新的解决方案，适用于一系列树型模型，包括决策树，随机森林和梯度增强树。此方法已应用在Concrete-ML开源库中，能够在准确性方面接近未受保护的版本。

    

    隐私增强技术(PETs)被提出作为一种保护数据隐私同时允许数据分析的方式。在本文中，我们关注一种强大的工具——全同态加密(FHE)，它允许对加密数据进行任意计算。我们展示了如何将FHE应用于基于树型模型的数据分析中，得到了针对加密表格数据的最新解决方案。我们证明了该方法适用于一系列树型模型，包括决策树，随机森林和梯度增强树，并已实现在Concrete-ML库中，该库在https://github.com/zama-ai/concrete-ml. 开源。通过选择一组应用案例，我们证明了我们的FHE版本在准确性方面非常接近未受保护的版本。

    Privacy enhancing technologies (PETs) have been proposed as a way to protect the privacy of data while still allowing for data analysis. In this work, we focus on Fully Homomorphic Encryption (FHE), a powerful tool that allows for arbitrary computations to be performed on encrypted data. FHE has received lots of attention in the past few years and has reached realistic execution times and correctness.  More precisely, we explain in this paper how we apply FHE to tree-based models and get state-of-the-art solutions over encrypted tabular data. We show that our method is applicable to a wide range of tree-based models, including decision trees, random forests, and gradient boosted trees, and has been implemented within the Concrete-ML library, which is open-source at https://github.com/zama-ai/concrete-ml. With a selected set of use-cases, we demonstrate that our FHE version is very close to the unprotected version in terms of accuracy.
    
[^95]: 使用不同降维和分类技术的癫痫发作检测的经验分析

    Empirical analysis of Different Dimensionality Reduction and classification Techniques for Epileptic Seizure detection. (arXiv:2302.12012v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12012](http://arxiv.org/abs/2302.12012)

    本研究对使用不同降维和分类技术进行癫痫发作检测进行了实证分析，通过离散小波变换和机器学习分类器，结合主成分分析、独立成分分析和线性判别分析等降维算法，选择特征来提高检测准确性。

    

    脑电图（EEG）是一种非侵入性检查，记录大脑的电活动。该检查用于帮助诊断各种脑问题。通过离散小波变换（DWT）和机器学习分类器，可以进行癫痫检测。在癫痫发作检测中，主要使用机器学习分类器和统计特征。EEG信号中的隐藏信息对于检测影响大脑的疾病很有用。有时，在时间和频率域内识别EEG的最小变化是非常困难的。DWT可以在不同频带进行信号良好的分解和特征提取。我们使用三个降维算法：主成分分析（PCA）、独立成分分析（ICA）和线性判别分析（LDA）。最后，通过融合规则选择特征。

    An Electroencephalogram (EEG) is a non-invasive exam that records the electrical activity of the brain. This exam is used to help diagnose conditions such as different brain problems. EEG signals are taken for the purpose of epilepsy detection and with Discrete Wavelet Transform (DWT) and machine learning classifier, they perform epilepsy detection. In Epilepsy seizure detection, mainly machine learning classifiers and statistical features are used. The hidden information in the EEG signal is useful for detecting diseases affecting the brain. Sometimes it is very difficult to identify the minimum changes in the EEG in the time and frequency domains purpose. The DWT can give a good decomposition of the signals in different frequency bands and feature extraction. We use the tri-dimensionality reduction algorithm.; Principal Component Analysis (PCA), Independent Component Analysis (ICA), and Linear Discriminant Analysis (LDA). Finally, features are selected by using a fusion rule and at t
    
[^96]: 异质神经元和突触动力学对于高效自主学习的脉冲减少网络的理论和设计原则

    Heterogeneous Neuronal and Synaptic Dynamics for Spike-Efficient Unsupervised Learning: Theory and Design Principles. (arXiv:2302.11618v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.11618](http://arxiv.org/abs/2302.11618)

    本文研究了异质神经元和突触动力学对于脉冲减少网络的影响。分析结果表明，异质性能够提高网络的记忆容量和预测性能，同时降低脉冲活动。实验结果证明了优化的异质脉冲减少网络的性能提升和脉冲活动减少的效果。

    

    本文显示神经元和突触动力学的异质性降低了递归脉冲神经网络(RSNN)的脉冲活动，同时提高了预测性能，实现了高效的脉冲减少(自主)学习。我们分析表明，神经元整合/放松动力学的多样性提高了RSNN学习更多不同的输入模式(更高的记忆容量)，从而提高了分类和预测性能。我们进一步证明了突触的异质性时序相关可塑性(STDP)动力学降低了脉冲活动但保持了记忆容量。分析结果促使我们使用贝叶斯优化确定神经元和突触的异质RSNN设计，以提高$\mathcal{E}$，即脉冲活动和记忆容量的比值。时间序列分类和预测任务的实证结果表明，优化的HRSNN提高了性能并减少了脉冲活动

    This paper shows that the heterogeneity in neuronal and synaptic dynamics reduces the spiking activity of a Recurrent Spiking Neural Network (RSNN) while improving prediction performance, enabling spike-efficient (unsupervised) learning. We analytically show that the diversity in neurons' integration/relaxation dynamics improves an RSNN's ability to learn more distinct input patterns (higher memory capacity), leading to improved classification and prediction performance. We further prove that heterogeneous Spike-Timing-Dependent-Plasticity (STDP) dynamics of synapses reduce spiking activity but preserve memory capacity. The analytical results motivate Heterogeneous RSNN design using Bayesian optimization to determine heterogeneity in neurons and synapses to improve $\mathcal{E}$, defined as the ratio of spiking activity and memory capacity. The empirical results on time series classification and prediction tasks show that optimized HRSNN increases performance and reduces spiking activi
    
[^97]: ASSET：在多种深度学习范式中实现鲁棒的后门数据检测

    ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms. (arXiv:2302.11408v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11408](http://arxiv.org/abs/2302.11408)

    该论文研究了在多种深度学习范式中实现鲁棒的后门数据检测，并发现现有的检测方法在不同攻击和毒害比例下的性能变化很大，且不能应用于最新的干净标签攻击，以及自监督学习和迁移学习中性能损失较大。为此，论文提出了一种名为ASSET的新的检测方法，通过主动引导不同的模型行为来促进后门和干净样本的分离。

    

    传统上，后门数据检测是在端到端监督学习（SL）的设置中进行研究的。然而，近年来，自监督学习（SSL）和迁移学习（TL）的普及应用增加，因为它们对标注数据的需求较少。成功的后门攻击也在这些新的设置中得到了证明。然而，我们对现有检测方法在不同学习设置下的适用性缺乏深入的理解。通过评估56种攻击设置，我们发现大多数现有检测方法的性能在不同攻击和毒害比例下存在显著差异，并且在最新的干净标签攻击下全部失败。此外，当应用于SSL和TL时，它们要么变得不适用，要么遭受较大的性能损失。我们提出了一种名为Active Separation via Offset (ASSET)的新的检测方法，通过在后门和干净样本之间主动引导不同的模型行为来促进它们的分离。

    Backdoor data detection is traditionally studied in an end-to-end supervised learning (SL) setting. However, recent years have seen the proliferating adoption of self-supervised learning (SSL) and transfer learning (TL), due to their lesser need for labeled data. Successful backdoor attacks have also been demonstrated in these new settings. However, we lack a thorough understanding of the applicability of existing detection methods across a variety of learning settings. By evaluating 56 attack settings, we show that the performance of most existing detection methods varies significantly across different attacks and poison ratios, and all fail on the state-of-the-art clean-label attack. In addition, they either become inapplicable or suffer large performance losses when applied to SSL and TL. We propose a new detection method called Active Separation via Offset (ASSET), which actively induces different model behaviors between the backdoor and clean samples to promote their separation. W
    
[^98]: 选择性解释：利用人类输入对齐可解释人工智能

    Selective Explanations: Leveraging Human Input to Align Explainable AI. (arXiv:2301.09656v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.09656](http://arxiv.org/abs/2301.09656)

    本研究提出一种通过利用人类输入生成选择性解释的通用框架，以弥合可解释人工智能（XAI）与人类解释的差距，并且在决策支持任务中进行了实验证明其有效性。

    

    近年来，出现了大量的可解释人工智能（XAI）算法，但它们经常因与人类解释的生产和消费方式存在显著差距而受到批评。因此，目前的XAI技术往往难以使用并缺乏有效性。在本文中，我们尝试通过使AI解释具有选择性（这是人类解释的基本属性之一）来弥合这些差距，通过根据接收方的偏好有选择性地呈现大量模型原因的子集来实现。我们提出了一个通用的框架，通过利用小样本上的人类输入来生成选择性解释。该框架开辟了一个丰富的设计空间，涵盖了不同的选择性目标、输入类型等。作为一个展示，我们使用决策支持任务来探索基于决策者认为相关的选择性解释。我们进行了两项实验研究，以检查从大一组模型原因中选择的三个子集与未选择的子集相比，选择性解释的效果。

    While a vast collection of explainable AI (XAI) algorithms have been developed in recent years, they are often criticized for significant gaps with how humans produce and consume explanations. As a result, current XAI techniques are often found to be hard to use and lack effectiveness. In this work, we attempt to close these gaps by making AI explanations selective -- a fundamental property of human explanations -- by selectively presenting a subset from a large set of model reasons based on what aligns with the recipient's preferences. We propose a general framework for generating selective explanations by leveraging human input on a small sample. This framework opens up a rich design space that accounts for different selectivity goals, types of input, and more. As a showcase, we use a decision-support task to explore selective explanations based on what the decision-maker would consider relevant to the decision task. We conducted two experimental studies to examine three out of a bro
    
[^99]: TikTalk: 一个用于多模态真实世界闲聊的基于视频的对话数据集

    TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World. (arXiv:2301.05880v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.05880](http://arxiv.org/abs/2301.05880)

    TikTalk是一个基于视频的多模态对话数据集，用于研究智能且类似人类的闲聊机器人。数据集包含从流行视频分享平台收集的38K个视频和367K个用户对话。与其他数据集相比，TikTalk提供了更丰富的上下文类型，同时也增加了从复杂的多模态信息中生成个性化回答的难度。数据集中还更频繁地引用了外部知识，为多模态对话模型提供了新的挑战。

    

    为了促进多模态上下文中智能和人类化聊天机器人的研究，我们引入了一个新的基于视频的多模态对话数据集，称为TikTalk。我们从一个流行的视频分享平台收集了38K个视频，以及用户在其下发布的367K个对话。用户根据他们观看视频时的多模态经验进行自发性对话，这有助于重现真实世界的闲聊环境。与之前的多模态对话数据集相比，TikTalk中更丰富的上下文类型导致了更多样化的对话，但也增加了从复杂的多模态信息中捕捉人类兴趣并生成个性化回答的难度。此外，我们的数据集中更频繁地引用了外部知识。这些事实揭示了多模态对话模型面临的新挑战。我们定量地展示了TikTalk的特点，提出了一个基于视频的多模态闲聊任务，并评估了几种对话基线模型。

    To facilitate the research on intelligent and human-like chatbots with multi-modal context, we introduce a new video-based multi-modal dialogue dataset, called TikTalk. We collect 38K videos from a popular video-sharing platform, along with 367K conversations posted by users beneath them. Users engage in spontaneous conversations based on their multi-modal experiences from watching videos, which helps recreate real-world chitchat context. Compared to previous multi-modal dialogue datasets, the richer context types in TikTalk lead to more diverse conversations, but also increase the difficulty in capturing human interests from intricate multi-modal information to generate personalized responses. Moreover, external knowledge is more frequently evoked in our dataset. These facts reveal new challenges for multi-modal dialogue models. We quantitatively demonstrate the characteristics of TikTalk, propose a video-based multi-modal chitchat task, and evaluate several dialogue baselines. Experi
    
[^100]: 相位偏移对抗训练

    Phase-shifted Adversarial Training. (arXiv:2301.04785v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.04785](http://arxiv.org/abs/2301.04785)

    本论文通过分析响应频率的视角，发现对抗训练导致神经网络收敛性较低，从而在每个数据附近产生高度振荡的预测。为了有效地学习高频内容，提出了相位偏移对抗训练(PhaseAT)方法。

    

    对抗训练被认为是确保神经网络应用程序安全部署到现实世界的关键组成部分。现有方法主要集中在如何通过增加更新步骤的数量、使用平滑的损失函数对模型进行正则化以及将随机性注入到攻击中来生成强有力的攻击。然而，我们通过响应频率的视角分析了对抗训练的行为。我们经验性地发现，对抗训练导致神经网络对高频信息的收敛性较低，从而在每个数据附近产生高度振荡的预测。为了高效而有效地学习高频内容，我们首先证明了一个频率原理的普遍现象，即\textit{较低的频率先学习}在对抗训练中仍然成立。基于此，我们提出了相位偏移对抗训练(PhaseAT)，模型通过学习高频内容来改善对抗训练的收敛性问题。

    Adversarial training has been considered an imperative component for safely deploying neural network-based applications to the real world. To achieve stronger robustness, existing methods primarily focus on how to generate strong attacks by increasing the number of update steps, regularizing the models with the smoothed loss function, and injecting the randomness into the attack. Instead, we analyze the behavior of adversarial training through the lens of response frequency. We empirically discover that adversarial training causes neural networks to have low convergence to high-frequency information, resulting in highly oscillated predictions near each data. To learn high-frequency contents efficiently and effectively, we first prove that a universal phenomenon of frequency principle, i.e., \textit{lower frequencies are learned first}, still holds in adversarial training. Based on that, we propose phase-shifted adversarial training (PhaseAT) in which the model learns high-frequency com
    
[^101]: 使用语言模型提示进行推理：一项调查

    Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09597](http://arxiv.org/abs/2212.09597)

    本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。

    

    推理作为复杂问题解决的重要能力，可以为医疗诊断、谈判等各种实际应用提供后端支持。本文对使用语言模型提示进行推理的前沿研究进行了综合调查。我们介绍了研究成果的比较和总结，并提供了系统资源以帮助初学者。我们还讨论了新兴推理能力出现的潜在原因，并突出了未来的研究方向。资源可在 https://github.com/zjunlp/Prompt4ReasoningPapers 上获取（定期更新）。

    Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
    
[^102]: CLIP: 用更少的数据更快地训练模型

    CLIP: Train Faster with Less Data. (arXiv:2212.01452v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.01452](http://arxiv.org/abs/2212.01452)

    本文提出了CLIP，通过结合课程学习和数据集修剪的方法，在深度学习模型的训练中使用更少的数据，实现更快的收敛速度和更好的泛化能力。

    

    深度学习模型需要大量的数据进行训练，但近年来机器学习正从以模型为中心转向以数据为中心的方法。在数据为中心的方法中，重点是通过改进和提高数据质量来提高模型的学习性能，而不是重新设计模型架构。在本文中，我们提出了CLIP，即使用迭代数据修剪的课程学习。CLIP结合了课程学习和数据集修剪这两种数据为中心的方法，以提高模型的学习准确性和收敛速度。所提出的方案采用了有损数据集修剪的方法，迭代地去除最不重要的样本，并逐渐减小在课程学习训练中的有效数据集的大小。在众筹密度估计模型上进行的大量实验验证了结合这两种方法的理念，通过减小收敛时间和改进泛化能力。据我们所知，这是第一次将课程学习和数据集修剪结合应用于深度学习的训练中。

    Deep learning models require an enormous amount of data for training. However, recently there is a shift in machine learning from model-centric to data-centric approaches. In data-centric approaches, the focus is to refine and improve the quality of the data to improve the learning performance of the models rather than redesigning model architectures. In this paper, we propose CLIP i.e., Curriculum Learning with Iterative data Pruning. CLIP combines two data-centric approaches i.e., curriculum learning and dataset pruning to improve the model learning accuracy and convergence speed. The proposed scheme applies loss-aware dataset pruning to iteratively remove the least significant samples and progressively reduces the size of the effective dataset in the curriculum learning training. Extensive experiments performed on crowd density estimation models validate the notion behind combining the two approaches by reducing the convergence time and improving generalization. To our knowledge, th
    
[^103]: 使用不完整标签进行人群密度估计

    Crowd Density Estimation using Imperfect Labels. (arXiv:2212.01450v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.01450](http://arxiv.org/abs/2212.01450)

    本文研究了不完整标签对人群计数准确性的影响，并提出了一种系统，利用深度学习模型自动生成不完整标签，并将其用于训练新的人群计数模型。实验证明，所提出的方案的准确性接近于完美标签数据集的准确性。

    

    密度估计是人群计数中最常用的方法之一，深度学习模型通过学习头部标注的人群图像来估计未见图像中的人群密度。通常，模型的学习性能很大程度上受到标注准确性的影响，不准确的标注可能导致预测过程中的定位和计数错误。已有大量的研究关注使用完美标签数据集进行人群计数，但没有人探索注释错误对模型准确性的影响。本文研究了不完整标签（包括噪声和缺失标签）对人群计数准确性的影响。我们提出了一种系统，利用深度学习模型（称为标注器）自动生成不完整标签，然后用于训练新的人群计数模型（目标模型）。我们在两个人群计数模型和两个基准数据集上的分析结果表明，所提出的方案的准确性接近于完美标签数据集的准确性。

    Density estimation is one of the most widely used methods for crowd counting in which a deep learning model learns from head-annotated crowd images to estimate crowd density in unseen images. Typically, the learning performance of the model is highly impacted by the accuracy of the annotations and inaccurate annotations may lead to localization and counting errors during prediction. A significant amount of works exist on crowd counting using perfectly labelled datasets but none of these explore the impact of annotation errors on the model accuracy. In this paper, we investigate the impact of imperfect labels (both noisy and missing labels) on crowd counting accuracy. We propose a system that automatically generates imperfect labels using a deep learning model (called annotator) which are then used to train a new crowd counting model (target model). Our analysis on two crowd counting models and two benchmark datasets shows that the proposed scheme achieves accuracy closer to that of the
    
[^104]: 离线监督学习与在线直接策略优化的比较研究及神经网络优化反馈控制的统一训练范式

    Offline Supervised Learning V.S. Online Direct Policy Optimization: A Comparative Study and A Unified Training Paradigm for Neural Network-Based Optimal Feedback Control. (arXiv:2211.15930v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2211.15930](http://arxiv.org/abs/2211.15930)

    本文对离线监督学习和在线直接策略优化进行了比较研究，结果突出了离线监督学习在最优性和训练时间方面的优势。为了克服两种方法中的主要挑战，我们提出了预训练和微调策略作为一种统一的训练范式。

    

    本文旨在高效解决基于神经网络的最优反馈控制问题。首先对离线监督学习和在线直接策略优化这两种主流方法进行了比较研究。虽然离线监督学习方法的训练部分相对容易，但其成功与否严重依赖于由开环最优控制求解器生成的最优控制数据集。相反，直接优化将最优控制问题直接转化为优化问题，无需预先计算，但在问题复杂时，与动力学相关的目标可能难以优化。我们的结果突出了离线监督学习在最优性和训练时间方面的优势。为了克服两种方法中的主要挑战，即数据集和优化问题，我们互补它们，并提出了预训练和微调策略作为一种统一的训练范式。

    This work is concerned with solving neural network-based feedback controllers efficiently for optimal control problems. We first conduct a comparative study of two mainstream approaches: offline supervised learning and online direct policy optimization. Albeit the training part of the supervised learning approach is relatively easy, the success of the method heavily depends on the optimal control dataset generated by open-loop optimal control solvers. In contrast, direct optimization turns the optimal control problem into an optimization problem directly without any requirement of pre-computing, but the dynamics-related objective can be hard to optimize when the problem is complicated. Our results highlight the priority of offline supervised learning in terms of both optimality and training time. To overcome the main challenges, dataset, and optimization, in the two approaches respectively, we complement them and propose the Pre-train and Fine-tune strategy as a unified training paradi
    
[^105]: DroneNet: 使用自组织神经网络的无人机进行人群密度估计

    DroneNet: Crowd Density Estimation using Self-ONNs for Drones. (arXiv:2211.07137v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.07137](http://arxiv.org/abs/2211.07137)

    使用自组织神经网络的无人机进行人群密度估计的模型（DroneNet），相比于使用CNN的模型具有更高的计算效率，能够在保持准确性的前提下降低推断时间。

    

    使用无人机进行视频监控既方便又高效，由于无人机在许多场景下部署和移动没有障碍。无人机视频监控的一个有趣应用是估计公共场所的人群密度（包括行人和车辆）。使用卷积神经网络（CNN）进行深度学习，利用图像和视频进行自动人群计数和密度估计。然而，这类模型的性能和准确性通常取决于模型架构，即更深的CNN模型在增加推断时间的代价下提高了准确性。在本文中，我们提出了一种新颖的无人机人群密度估计模型（DroneNet），使用自组织操作神经网络（Self-ONN）。相比于基于CNN的模型，Self-ONN在计算复杂度更低的情况下提供了高效的学习能力。我们将算法在两个无人机视角的公开数据集上进行了测试。评估结果显示，所提出的DroneNet在人群密度估计上表现出

    Video surveillance using drones is both convenient and efficient due to the ease of deployment and unobstructed movement of drones in many scenarios. An interesting application of drone-based video surveillance is to estimate crowd densities (both pedestrians and vehicles) in public places. Deep learning using convolution neural networks (CNNs) is employed for automatic crowd counting and density estimation using images and videos. However, the performance and accuracy of such models typically depend upon the model architecture i.e., deeper CNN models improve accuracy at the cost of increased inference time. In this paper, we propose a novel crowd density estimation model for drones (DroneNet) using Self-organized Operational Neural Networks (Self-ONN). Self-ONN provides efficient learning capabilities with lower computational complexity as compared to CNN-based models. We tested our algorithm on two drone-view public datasets. Our evaluation shows that the proposed DroneNet shows supe
    
[^106]: 迈向可信多模态运动预测: 输出的整体评估和可解释性

    Towards trustworthy multi-modal motion prediction: Holistic evaluation and interpretability of outputs. (arXiv:2210.16144v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.16144](http://arxiv.org/abs/2210.16144)

    本研究旨在设计可信赖的运动预测系统，通过综合评估指标、提高鲁棒性和输出的可解释性。研究分析了当前基准的主要缺陷，并提出了新的整体评估框架。

    

    预测其他道路参与者的运动能使自动驾驶车辆执行安全高效的路径规划。这个任务非常复杂，因为道路参与者的行为取决于许多因素，并且可能的未来轨迹数量可观（多模态）。大多数先前的方法旨在解决多模态运动预测问题的复杂机器学习系统解释性有限。此外，当前基准所使用的指标不评估问题的所有方面，例如输出的多样性和可接受性。在这项工作中，我们旨在根据“可信人工智能设计”的一些要求，推动可信运动预测系统的设计。我们关注评估标准、鲁棒性和输出可解释性。首先，我们全面分析评估指标，确定当前基准的主要缺陷，并提出一个新的整体评估框架。

    Predicting the motion of other road agents enables autonomous vehicles to perform safe and efficient path planning. This task is very complex, as the behaviour of road agents depends on many factors and the number of possible future trajectories can be considerable (multi-modal). Most prior approaches proposed to address multi-modal motion prediction are based on complex machine learning systems that have limited interpretability. Moreover, the metrics used in current benchmarks do not evaluate all aspects of the problem, such as the diversity and admissibility of the output. In this work, we aim to advance towards the design of trustworthy motion prediction systems, based on some of the requirements for the design of Trustworthy Artificial Intelligence. We focus on evaluation criteria, robustness, and interpretability of outputs. First, we comprehensively analyse the evaluation metrics, identify the main gaps of current benchmarks, and propose a new holistic evaluation framework. We t
    
[^107]: Claim-Dissector: 一款联合重排和真实性预测的可解释的事实核查系统

    Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction. (arXiv:2207.14116v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2207.14116](http://arxiv.org/abs/2207.14116)

    Claim-Dissector是一款联合重排和真实性预测的可解释的事实核查系统，可以识别与声明相关的证据，并确定声明的真实性。该系统的个人贡献以及证据所支持或反驳声明的贡献都可以被识别。

    

    我们提出了Claim-Dissector，一种针对事实核查和分析的新型潜变量模型，给出一个声明和一组检索到的证据，联合学习识别：（i）与给定声明相关的证据，（ii）声明的真实性。我们建议以可解释的方式解开每个证据的相关性概率及其对最终真实性概率的影响-最终真实性概率与每个证据相关性概率的线性整合成比例。通过这种方式，可以识别出每个证据对最终预测概率的个人贡献。在每个证据的相关性概率中，我们的模型还可以进一步区分每个相关证据是支持（S）还是反驳（R）声明。这样可以量化S/R概率对最终结论的贡献或检测有异议的证据。尽管我们的系统具有可解释性，但在FEVER竞赛中，其结果与最先进的结果相当。

    We present Claim-Dissector: a novel latent variable model for fact-checking and analysis, which given a claim and a set of retrieved evidences jointly learns to identify: (i) the relevant evidences to the given claim, (ii) the veracity of the claim. We propose to disentangle the per-evidence relevance probability and its contribution to the final veracity probability in an interpretable way -- the final veracity probability is proportional to a linear ensemble of per-evidence relevance probabilities. In this way, the individual contributions of evidences towards the final predicted probability can be identified. In per-evidence relevance probability, our model can further distinguish whether each relevant evidence is supporting (S) or refuting (R) the claim. This allows to quantify how much the S/R probability contributes to the final verdict or to detect disagreeing evidence.  Despite its interpretable nature, our system achieves results competitive with state-of-the-art on the FEVER 
    
[^108]: QSAN: 一种近期可实现的量子自注意力网络

    QSAN: A Near-term Achievable Quantum Self-Attention Network. (arXiv:2207.07563v4 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2207.07563](http://arxiv.org/abs/2207.07563)

    本文提出了一种量子自注意力网络（QSAN），用于近期量子设备上的图像分类任务。该网络利用量子自注意力机制来增强数据表示能力，并设计了对应的一步实现和量子电路框架。

    

    自注意机制（SAM）擅长捕捉特征的内部连接，并极大地提高了机器学习模型的性能，尤其是对高维数据的高效特征提取和表征。本文提出了一种新型的量子自注意力网络（QSAN），用于近期量子设备上的图像分类任务。首先，探索了量子自注意力机制（QSAM），包括量子逻辑相似度（QLS）和量子位自注意力得分矩阵（QBSASM），作为QSAN的理论基础，以增强SAM的数据表示能力。QLS用于防止测量获取内积，使得QSAN能够在量子计算机上完全实现，而QBSASM作为QSAN演进的结果，产生一个能有效反映输出的注意力分布的密度矩阵。然后，设计了QSAN的一步实现和量子电路框架，充分考虑了数据压缩等因素。

    Self-Attention Mechanism (SAM) is good at capturing the internal connections of features and greatly improves the performance of machine learning models, espeacially requiring efficient characterization and feature extraction of high-dimensional data. A novel Quantum Self-Attention Network (QSAN) is proposed for image classification tasks on near-term quantum devices. First, a Quantum Self-Attention Mechanism (QSAM) including Quantum Logic Similarity (QLS) and Quantum Bit Self-Attention Score Matrix (QBSASM) is explored as the theoretical basis of QSAN to enhance the data representation of SAM. QLS is employed to prevent measurements from obtaining inner products to allow QSAN to be fully implemented on quantum computers, and QBSASM as a result of the evolution of QSAN to produce a density matrix that effectively reflects the attention distribution of the output. Then, the framework for one-step realization and quantum circuits of QSAN are designed for fully considering the compression
    
[^109]: 公平感知梯度下降：FairGrad

    FairGrad: Fairness Aware Gradient Descent. (arXiv:2206.10923v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10923](http://arxiv.org/abs/2206.10923)

    FairGrad是一种公平感知梯度下降方法，通过重新加权方案迭代学习群体特定权重来实现群体公平性。它易于实现，适用于各种标准的公平性定义，并且在各种数据集上与标准基线方法具有竞争力。

    

    本文解决了分类中的群体公平性问题，目标是学习不会不公平歧视人群子集的模型。之前的方法大多限于简单的二分类任务，或者涉及难以实现的训练机制，降低了它们的实际可应用性。本文提出了一种名为FairGrad的方法，它基于重新加权方案来强化公平性，通过迭代学习群体特定的权重，这些权重取决于是否具有优势。FairGrad易于实现，适用于各种标准的公平性定义，并且开销最小。此外，我们还展示了FairGrad在包括自然语言处理和计算机视觉在内的各种数据集上与标准基线方法具有竞争力。FairGrad可以在https://pypi.org/project/fairgrad上作为一个PyPI包获得。

    We address the problem of group fairness in classification, where the objective is to learn models that do not unjustly discriminate against subgroups of the population. Most existing approaches are limited to simple binary tasks or involve difficult to implement training mechanisms which reduces their practical applicability. In this paper, we propose FairGrad, a method to enforce fairness based on a re-weighting scheme that iteratively learns group specific weights based on whether they are advantaged or not. FairGrad is easy to implement, accommodates various standard fairness definitions, and comes with minimal overhead. Furthermore, we show that it is competitive with standard baselines over various datasets including ones used in natural language processing and computer vision.  FairGrad is available as a PyPI package at https://pypi.org/project/fairgrad
    
[^110]: CARLANE: 从模拟到多个真实世界领域的无监督域适应的车道检测基准

    CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to multiple Real-World Domains. (arXiv:2206.08083v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.08083](http://arxiv.org/abs/2206.08083)

    本论文提出了一个适用于2D车道检测的无监督域适应基准- CARLANE，包括模拟到多个真实世界领域的三个不同数据集。该基准数据集涵盖了多样的场景，并包含大量有注释的图片。此外，研究还提出了系统基准，用以评估方法的性能。

    

    无监督域适应通过将模型从有标签的源领域转移到无标签的目标领域，展示了减轻域漂移的巨大潜力。虽然无监督域适应已经应用于各种复杂的视觉任务，但只有少数工作集中在自动驾驶的车道检测上。这可以归因于缺乏公开可用的数据集。为了促进在这些方向上的研究，我们提出了CARLANE，一个适用于2D车道检测的3向模拟到真实域适应基准。CARLANE包括单目标数据集MoLane和TuLane，以及多目标数据集MuLane。这些数据集来自于三个不同的领域，涵盖了多样的场景，共包含16.3万张独特的图片，其中有11.8万张有注释。此外，我们评估和报告了系统基准，包括我们自己的方法，该方法基于原型交叉域自监督学习。我们发现评估中的误报率和漏报率

    Unsupervised Domain Adaptation demonstrates great potential to mitigate domain shifts by transferring models from labeled source domains to unlabeled target domains. While Unsupervised Domain Adaptation has been applied to a wide variety of complex vision tasks, only few works focus on lane detection for autonomous driving. This can be attributed to the lack of publicly available datasets. To facilitate research in these directions, we propose CARLANE, a 3-way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANE encompasses the single-target datasets MoLane and TuLane and the multi-target dataset MuLane. These datasets are built from three different domains, which cover diverse scenes and contain a total of 163K unique images, 118K of which are annotated. In addition we evaluate and report systematic baselines, including our own method, which builds upon Prototypical Cross-domain Self-supervised Learning. We find that false positive and false negative rates of the eva
    
[^111]: 一种基于搜索的测试方法，用于深度强化学习代理

    A Search-Based Testing Approach for Deep Reinforcement Learning Agents. (arXiv:2206.07813v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2206.07813](http://arxiv.org/abs/2206.07813)

    本文提出一种基于搜索的测试方法，探索状态空间以检测DRL代理的安全性，并在三个基准测试中取得了比基线方法更高的状态空间覆盖率。

    

    近十年来，深度强化学习算法越来越多地被应用于解决自动驾驶和机器人等各种决策问题。然而，由于深度强化学习代理在生命安全环境中经常表现出错误行为，导致潜在的重大错误，因此它们面临着巨大的挑战。为了评估DRL代理的安全性，一种方法是对它们进行测试，以检测可能导致关键故障的故障。这就提出了一个问题，即我们如何有效地测试DRL策略，以确保它们的正确性和遵守安全要求。本文提出了一种基于搜索的测试方法，通过引导代理生成满足安全要求的状态序列变化，以探索环境的状态空间。我们的方法在三种不同的DRL基准测试中进行了评估，并在保持相似或更好的测试效果的同时，实现了比基线方法更高的状态空间覆盖率。

    Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One way to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Their main goal is to test the robustness of DRL agents rather than testing the compliance of agents' policies with respect to requirements. Due to the huge state spac
    
[^112]: 变分元元强化学习在社交机器人中的应用

    Variational Meta Reinforcement Learning for Social Robotics. (arXiv:2206.03211v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2206.03211](http://arxiv.org/abs/2206.03211)

    本研究探讨了变分元元强化学习在社交机器人中的应用，通过选择合适的奖励函数，可快速适应不同环境，提高机器人的社交技能。

    

    随着机器人在我们日常环境中的普及，提高它们的社交技能变得非常重要。然而，社交机器人仍面临许多挑战。其中一个瓶颈是，由于社交规范在很大程度上取决于环境，机器人行为需要经常适应。例如，在医院中，机器人应该比在办公室中更加小心地导航周围的病人。在这项工作中，我们研究了元强化学习（meta-RL）作为一个潜在的解决方案。在这里，机器人的行为是通过强化学习来学习的，需要选择一个奖励函数，使机器人能够针对给定环境学习出合适的行为。我们提出使用变分元强化学习程序，快速地将机器人的行为适应到新的奖励函数上。因此，在给定一个新的环境时，可以快速评估不同的奖励函数并选择适合的一个。该程序学习了奖励函数的向量化表示和一些其他的技巧。

    With the increasing presence of robots in our every-day environments, improving their social skills is of utmost importance. Nonetheless, social robotics still faces many challenges. One bottleneck is that robotic behaviors need to be often adapted as social norms depend strongly on the environment. For example, a robot should navigate more carefully around patients in a hospital compared to workers in an office. In this work, we investigate meta-reinforcement learning (meta-RL) as a potential solution. Here, robot behaviors are learned via reinforcement learning where a reward function needs to be chosen so that the robot learns an appropriate behavior for a given environment. We propose to use a variational meta-RL procedure that quickly adapts the robots' behavior to new reward functions. As a result, given a new environment different reward functions can be quickly evaluated and an appropriate one selected. The procedure learns a vectorized representation for reward functions and a
    
[^113]: 具有预分配固定分类器的类增量学习

    Class-incremental Learning with Pre-allocated Fixed Classifiers. (arXiv:2010.08657v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.08657](http://arxiv.org/abs/2010.08657)

    本文提出了一种具有预分配固定分类器的类增量学习方法，通过利用存储在情节性记忆中的过去数据，并在学习阶段的开始就将一些预分配的输出节点纳入分类损失的计算，解决了神经网络在类增量学习中遗忘先前知识的问题。

    

    在类增量学习中，学习代理面对一系列数据的任务是学习新类别而不忘记以前的类别。神经网络在这种情况下常常会忘记先前获得的知识。为了解决这个问题，有效的方法利用存储在一个情节性记忆中的过去数据，同时扩展最终分类器节点以容纳新的类别。在这项工作中，我们用一个新颖的固定分类器替代了扩展分类器，其中一些预分配的输出节点从学习阶段开始就受到分类损失的影响。与标准扩展分类器相反，这样做有以下好处：(a)未来未见过的类别的输出节点从学习的一开始就能看到负样本，以及逐渐增加的正样本；(b)能够学习不随着新类别的加入而改变其几何配置的特征。

    In class-incremental learning, a learning agent faces a stream of data with the goal of learning new classes while not forgetting previous ones. Neural networks are known to suffer under this setting, as they forget previously acquired knowledge. To address this problem, effective methods exploit past data stored in an episodic memory while expanding the final classifier nodes to accommodate the new classes.  In this work, we substitute the expanding classifier with a novel fixed classifier in which a number of pre-allocated output nodes are subject to the classification loss right from the beginning of the learning phase. Contrarily to the standard expanding classifier, this allows: (a) the output nodes of future unseen classes to firstly see negative samples since the beginning of learning together with the positive samples that incrementally arrive; (b) to learn features that do not change their geometric configuration as novel classes are incorporated in the learning model.  Experi
    
[^114]: 多智能体信任区域策略优化

    Multi-Agent Trust Region Policy Optimization. (arXiv:2010.07916v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2010.07916](http://arxiv.org/abs/2010.07916)

    我们提出了一种名为多智能体 TRPO (MATRPO) 的分散式 MARL 算法，它可以在多智能体协作任务上优化分布式策略，并且无需智能体之间共享观测、奖励、策略或值/动作值函数。该算法在两个合作游戏上展示了出色的性能。

    

    我们将信任区域策略优化 (TRPO) 扩展到多智能体强化学习 (MARL) 问题。我们展示了 TRPO 的策略更新可以转化为多智能体案例下的分布式共识优化问题。通过对共识优化模型进行一系列的近似，我们提出了一种名为多智能体 TRPO (MATRPO) 的分散式 MARL 算法。该算法可以基于本地观测和私人奖励优化分布式策略。智能体不需要了解其他智能体的观测、奖励、策略或值/动作值函数。智能体在训练过程中只与邻居共享似然比。该算法完全分散且保护隐私。我们在两个合作游戏上的实验表明，它在复杂的 MARL 任务中具有出色的性能。

    We extend trust region policy optimization (TRPO) to multi-agent reinforcement learning (MARL) problems. We show that the policy update of TRPO can be transformed into a distributed consensus optimization problem for multi-agent cases. By making a series of approximations to the consensus optimization model, we propose a decentralized MARL algorithm, which we call multi-agent TRPO (MATRPO). This algorithm can optimize distributed policies based on local observations and private rewards. The agents do not need to know observations, rewards, policies or value/action-value functions of other agents. The agents only share a likelihood ratio with their neighbors during the training process. The algorithm is fully decentralized and privacy-preserving. Our experiments on two cooperative games demonstrate its robust performance on complicated MARL tasks.
    
[^115]: 多次购买行为：建模、估计和优化

    Multi-Purchase Behavior: Modeling, Estimation and Optimization. (arXiv:2006.08055v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2006.08055](http://arxiv.org/abs/2006.08055)

    本研究提出了一种名为Bundle-MVL-K的多次购买选择模型家族，并开发了一种迭代策略来高效地计算出该模型的优化推荐。这是对多次购买选择模型进行操作化的首次尝试之一，并首次建立了多次购买行为建模与收入增益之间的定量联系。

    

    我们研究了建模多个产品购买并利用该模型为在线零售商和电商平台提供优化推荐的问题。我们提出了一种简化的多次购买选择模型家族，称为Bundle-MVL-K家族，并开发了一种基于二分搜索的迭代策略，高效地计算出该模型的优化推荐。我们证明了计算最优推荐集的困难性，并推导出最优解的几个结构属性，以加速计算。这是对多次购买选择模型进行操作化的首次尝试之一。我们展示了建模多次购买行为与收入增益之间的首个定量联系。我们使用多个真实世界数据集，比较了我们的建模和优化技术与竞争解决方案在模型适配度、预期收入增益和运行时间减少等多个指标上的效果。例如，预期的收入增益比竞争解决方案更高。

    We study the problem of modeling purchase of multiple products and utilizing it to display optimized recommendations for online retailers and e-commerce platforms.  We present a parsimonious multi-purchase family of choice models called the Bundle-MVL-K family, and develop a binary search based iterative strategy that efficiently computes optimized recommendations for this model. We establish the hardness of computing optimal recommendation sets, and derive several structural properties of the optimal solution that aid in speeding up computation. This is one of the first attempts at operationalizing multi-purchase class of choice models. We show one of the first quantitative links between modeling multiple purchase behavior and revenue gains. The efficacy of our modeling and optimization techniques compared to competing solutions is shown using several real world datasets on multiple metrics such as model fitness, expected revenue gains and run-time reductions. For example, the expecte
    

