# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective.](http://arxiv.org/abs/2306.13092) | SRe$^2$L是一种数据集压缩方法，可以处理不同规模的数据集、模型体系结构和图像分辨率，具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力，在Tiny-ImageNet和ImageNet-1K数据集上实现了最佳性能，并超越了现有的最先进方法。 |
| [^2] | [Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting.](http://arxiv.org/abs/2306.13085) | 该论文提出了一种混合离线强化学习数据集的改进方法，可以通过重新加权采样数据集来充分利用高表现的轨迹，以提高目标策略的表现。 |
| [^3] | [Impacts and Risk of Generative AI Technology on Cyber Defense.](http://arxiv.org/abs/2306.13033) | 生成人工智能技术在文本、图像、音频和视频等领域具有广泛应用潜力，但由于其可能被滥用制作虚假信息，存在网络安全挑战和风险。为此，我们建议利用网络攻击生命周期作为网络防御的基础模型。 |
| [^4] | [Transferable Curricula through Difficulty Conditioned Generators.](http://arxiv.org/abs/2306.13028) | 本文提出了一种名为PERM的方法，可以直接对环境难度和RL代理的能力进行建模，通过匹配环境的难度生成课程，实现了在参数化环境中训练RL代理的有前途的结果。 |
| [^5] | [Can Differentiable Decision Trees Learn Interpretable Reward Functions?.](http://arxiv.org/abs/2306.13004) | 本文提出了使用可微分决策树从人类偏好中学习具有表达能力和可解释性的奖励函数，通过在多个环境上的评估，发现该方法能够学习到可解释的奖励函数，但在强化学习测试时表现受到树的离散性的影响。 |
| [^6] | [Apolitical Intelligence? Auditing Delphi's responses on controversial political issues in the US.](http://arxiv.org/abs/2306.13000) | 本文审查了用于众包伦理的大型语言模型Delphi在美国政治争议问题中的回应。研究发现，该模型的置信度校准不良，呈现显著的政治倾斜。作者从数据女权主义的角度探讨了中立性的问题，讨论了中立性概念如何转移权力、进一步边缘化无声的声音。 |
| [^7] | [Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning.](http://arxiv.org/abs/2306.12964) | 本文提出了一种新的Alpha生成框架，它通过强化学习挖掘协同的公式化Alpha集合以优化它们作为组合模型的性能。 |
| [^8] | [Siamese SIREN: Audio Compression with Implicit Neural Representations.](http://arxiv.org/abs/2306.12957) | 该研究利用Siamese SIREN 建立了一种新的方法，使用了较少的网络参数来实现卓越的音频重建保真度，拓展了隐式神经表示压缩的应用领域。 |
| [^9] | [Evolving Computation Graphs.](http://arxiv.org/abs/2306.12943) | 本论文提出了一种进化计算图算法（ECGs），用于提高针对异质性数据的图神经网络（GNN）性能，通过重连GNN的计算图增加连接同一类节点的边缘以提升性能。 |
| [^10] | [Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing.](http://arxiv.org/abs/2306.12929) | 本文提出了一种称为“Helper-Head”的方法，可以通过教授注意头忽略输入和输出的某些部分来消除离群值，从而实现对transformer的可量化。实验结果表明，该方法在低、高比特宽度设置上均优于现有方法，在两个流行的语言建模基准上实现了最先进的结果。 |
| [^11] | [Decentralized Multi-Agent Reinforcement Learning with Global State Prediction.](http://arxiv.org/abs/2306.12926) | 本文研究了分散式多智能体强化学习中的一个关键挑战：如何在没有全局信息的情况下有效训练机器人。我们提出了一种基于状态预测的方法，在不需要显式通信的情况下使机器人能够协调行动，实现更快更好的学习效果和任务执行性能。 |
| [^12] | [AudioPaLM: A Large Language Model That Can Speak and Listen.](http://arxiv.org/abs/2306.12925) | AudioPaLM是一款能够更好地处理语音任务的大型语言模型，它继承了AudioLM的语音身份和语调信息保留能力以及PaLM-2的语言知识，可用于语音识别、语音翻译等应用。 |
| [^13] | [FuXi: A cascade machine learning forecasting system for 15-day global weather forecast.](http://arxiv.org/abs/2306.12873) | 逐步级联模型FuXi在15天全球天气预报中表现出更好的性能，并通过减少预测误差的积累达到优化。 |
| [^14] | [Don't Treat the Symptom, Find the Cause! Efficient Artificial-Intelligence Methods for (Interactive) Debugging.](http://arxiv.org/abs/2306.12850) | 本文提出了用基于模型诊断的高效人工智能方法进行（交互式）调试的方案，包括贝叶斯网络和马尔可夫决策过程，并使用真实世界的例子证明了方法的有效性，这能够显著减少系统维修所需的时间和成本，并提高现代系统的整体可靠性。 |
| [^15] | [XAI-TRIS: Non-linear benchmarks to quantify ML explanation performance.](http://arxiv.org/abs/2306.12816) | XAI-TRIS提供了用于测试机器学习解释性能的具有挑战性的非线性基准数据集，并揭示了现有XAI方法的局限性。 |
| [^16] | [Conditional Generators for Limit Order Book Environments: Explainability, Challenges, and Robustness.](http://arxiv.org/abs/2306.12806) | 本文研究了在订单簿模拟中使用条件生成模型的方法，并探索了其对输入特征的依赖性及其优点和缺点，提高了CGAN的逼真度和强健性。 |
| [^17] | [Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery.](http://arxiv.org/abs/2306.12802) | 本研究在药物发现方面使用了多模态知识图谱表示学习，获得了最先进的结果。他们整合了来自不同来源的数据，并提供了基于这些数据的预训练模型。 |
| [^18] | [MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning.](http://arxiv.org/abs/2306.12785) | 本文提出了一种采用对抗学习的新型语音合成器MFCCGAN，使用MFCC作为输入，产生比传统规则MFCC的WORLD语音合成器更高清晰度、更好理解度的语音。 |
| [^19] | [On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective.](http://arxiv.org/abs/2306.12756) | 本论文研究了生成式检索模型在超出分布（OOD）泛化方面的鲁棒性，定义了从三个方面衡量OOD鲁棒性，并分析了其与密集检索模型的比较。实验结果表明，生成式检索模型的OOD鲁棒性较弱，特别是在面向任务的超出分布场景中更为明显。针对造成鲁棒性较弱的原因，提出了潜在的解决方案。 |
| [^20] | [To Spike or Not to Spike? A Quantitative Comparison of SNN and CNN FPGA Implementations.](http://arxiv.org/abs/2306.12742) | 本文分析了多个SNN硬件加速器，用于FPGA的性能和能量效率。提出了一种SNN编码方案和一种内存组织技术以进一步提高能量效率。比较结果显示，SNN加速器没有比CNN加速器显著降低能量消耗。 |
| [^21] | [OptIForest: Optimal Isolation Forest for Anomaly Detection.](http://arxiv.org/abs/2306.12703) | 本论文针对隔离森林算法中分支因子的最优取值问题，基于隔离效率提出创新算法OptIForest，该算法结构简洁、检测性能优秀，可应用于各种异常检测场景。 |
| [^22] | [Towards quantum enhanced adversarial robustness in machine learning.](http://arxiv.org/abs/2306.12688) | 本文讨论了将量子计算与机器学习相结合的新领域，即量子对抗机器学习，它有望提供更好的准确性，更高的计算效率和更强的对抗攻击鲁棒性。但在实现稳健的实际QAML工具方面仍存在挑战。 |
| [^23] | [Explainable Representations for Relation Prediction in Knowledge Graphs.](http://arxiv.org/abs/2306.12687) | 提出一种新的可解释表示方法SEEK用于知识图谱关系预测，通过识别实体之间相关的共享语义方面生成一个多方面和可解释的表示，并在真实世界的蛋白质相互作用预测和基因-疾病关联性预测任务中进行了验证。 |
| [^24] | [Recent Developments in Recommender Systems: A Survey.](http://arxiv.org/abs/2306.12680) | 本篇综述全面总结了推荐系统领域的最新进展和趋势，包括推荐系统分类，知识推荐系统，鲁棒性，数据偏见和公平性问题，以及评估度量。该研究还提供了未来研究的新方向。 |
| [^25] | [SoftGPT: Learn Goal-oriented Soft Object Manipulation Skills by Generative Pre-trained Heterogeneous Graph Transformer.](http://arxiv.org/abs/2306.12677) | 我们提出了一种预训练软物体操作技能学习模型SoftGPT，它使用大量的数据进行训练，结合三维异构图表示和基于GPT的动态模型，并利用这种先前知识来学习目标导向的软物体操作技能，从而打破了这一领域的技术瓶颈。 |
| [^26] | [Memristive Reservoirs Learn to Learn.](http://arxiv.org/abs/2306.12676) | 本研究提出了学会学习的框架来解决基于忆阻器的储层神经网络优化过程中的挑战，成功地找到了储层的最优超参数，同时证实了储层的最优性能发生在导电通道的形成边缘，而且还发现这种系统可以模拟尖峰神经元的膜电位行为，具有将尖峰和连续信号互相转换的潜力 。 |
| [^27] | [Identifying and Extracting Rare Disease Phenotypes with Large Language Models.](http://arxiv.org/abs/2306.12656) | 研究提出了一个通过大型语言模型自动提取罕见病表型的方法，使用零样本或少样本的提示学习技术，无需对大量语料进行注释。 |
| [^28] | [Novelty Accommodating Multi-Agent Planning in High Fidelity Simulated Open World.](http://arxiv.org/abs/2306.12654) | 本文介绍了一个能够检测、描述和适应现实环境中创新的领域独立AI代理在高保真模拟开放世界中成功应用的研究。在处理新颖环境干扰的多智能体规划任务中，该代理性能明显优于现有的基线。 |
| [^29] | [FLAG: Finding Line Anomalies (in code) with Generative AI.](http://arxiv.org/abs/2306.12643) | FLAG是一种利用生成AI发现代码中行异常的语言无关方法，能够处理不完整（甚至非编译）的代码，有助于减少设计人员的检查搜索空间。 |
| [^30] | [Targeted collapse regularized autoencoder for anomaly detection: black hole at the center.](http://arxiv.org/abs/2306.12627) | 本文提出一种在自编码器的损失函数中添加一个轻量级的约束项，用于解决传统自编码器在异常检测中的不足，并取得了良好的表现。 |
| [^31] | [SEAL: Simultaneous Exploration and Localization in Multi-Robot Systems.](http://arxiv.org/abs/2306.12623) | 本论文提出了一种基于高斯过程的信息融合和通信图优化相结合的SEAL方法，用于实现多机器人系统中的高精度定位和探索，以取得了比最前沿方法更好的探索和定位性能。 |
| [^32] | [Class-Incremental Learning based on Label Generation.](http://arxiv.org/abs/2306.12619) | 本文提出了一种基于标签生成方法的增量分类学习（CIL）方法（VAG），大幅减少了灾难性遗忘（CF），并更好地保留了预训练模型的可推广表示。 |
| [^33] | [Towards Regulatable AI Systems: Technical Gaps and Policy Opportunities.](http://arxiv.org/abs/2306.12609) | 该论文探讨了如何确保AI系统遵循规定，提出了当前和潜在可能的技术实现，以及需要跨学科方法解决的问题。 |
| [^34] | [Resources and Evaluations for Multi-Distribution Dense Information Retrieval.](http://arxiv.org/abs/2306.12601) | 本文提出了一个新问题——多分布信息检索，并通过三个基准数据集展示了简单方法的有效性，可防止已知领域消耗大部分检索预算，平均提高Recall @ 100点3.8+，最高可达8.0个点。 |
| [^35] | [Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases.](http://arxiv.org/abs/2306.12567) | 本研究使用名为 NeuBAROCO 的数据集，检验了当前大型语言模型在三段论推理中的表现。发现这些模型在处理信念偏见、转化错误和氛围效应等问题时表现欠佳。 |
| [^36] | [Improving Long-Horizon Imitation Through Instruction Prediction.](http://arxiv.org/abs/2306.12554) | 本文探讨了基于语言的指令预测损失的辅助监督方式，展示了在演示数量受限的情况下，指令建模在复杂推理任务中提高了表现。 |
| [^37] | [Memory-Query Tradeoffs for Randomized Convex Optimization.](http://arxiv.org/abs/2306.12534) | 随机凸优化需要在内存和查询之间权衡，使用 $\tilde{O}(d^2)$ 比特内存和 $\tilde{O}(d)$ 次查询的割平面方法是最优的。 |
| [^38] | [Investigating Poor Performance Regions of Black Boxes: LIME-based Exploration in Sepsis Detection.](http://arxiv.org/abs/2306.12507) | 本文提出了一种利用LIME方法探索黑盒分类模型不佳性能区域的方法，并应用于高风险的败血症检测中。通过分析错误分类实例，确定造成模型性能差的重要特征，并识别出分类器性能不佳的区域，并计算出其错误率，这对于实现谨慎的决策具有重要意义，同时增强了机器学习模型在临床实践中的可解释性，降低关键应用场景下的风险。 |
| [^39] | [Deep Dynamic Epidemiological Modelling for COVID-19 Forecasting in Multi-level Districts.](http://arxiv.org/abs/2306.12457) | 该文提出一种深度动态流行病学的模型，将流行病学方程和深度学习相结合，以实现对COVID-19在多级区域的准确预测，同时保证模型可视化机制 |
| [^40] | [Pushing the Limits of Machine Design: Automated CPU Design with AI.](http://arxiv.org/abs/2306.12456) | 本研究运用新的人工智能方法，自动设计了一个中央处理器(CPU)，这是人类设计过的最复杂的装置之一，从而探索了机器设计的边界。 |
| [^41] | [Learning Conditional Instrumental Variable Representation for Causal Effect Estimation.](http://arxiv.org/abs/2306.12453) | 本文提出了一种名为 DVAE.CIV 的方法，通过分离表示学习，从带有潜在混淆因素的数据中学习和分解条件 IV 和其条件集的表示，用于因果效应估计。 |
| [^42] | [Factors Affecting the Performance of Automated Speaker Verification in Alzheimer's Disease Clinical Trials.](http://arxiv.org/abs/2306.12444) | 本文研究了自动化说话人验证技术在阿尔茨海默症临床试验中应用的可行性，人口统计特征、音频质量标准和AD的严重程度对ASV性能会产生影响。 |
| [^43] | [Knowledge Distillation via Token-level Relationship Graph.](http://arxiv.org/abs/2306.12442) | 本文提出了一种新方法，基于词级别关系图(TRG)，用于提高知识蒸馏的性能。通过利用TRG，学生模型可以模拟教师模型中更高级别的语义信息。同时，还引入了一种上下文损失以进一步增强学习过程。 |
| [^44] | [AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization.](http://arxiv.org/abs/2306.11971) | AdCraft是一种高级强化学习基准环境，用于模拟出价和预算变化的搜索引擎营销(SEM)活动，可用于评估和提高SEM出价和预算管理相关的RL算法的鲁棒性。 |
| [^45] | [Enhancing variational quantum state diagonalization using reinforcement learning techniques.](http://arxiv.org/abs/2306.11086) | 本研究采用强化学习技术，通过新的编码方法来优化量子状态对角化所需的电路深度，从而提高其在近期量子硬件上的应用性能。 |
| [^46] | [A HRNet-based Rehabilitation Monitoring System.](http://arxiv.org/abs/2306.10756) | 该论文介绍了一种基于HRNet的康复监测系统，旨在通过智能手机进行康复训练的管理。患者可以通过系统进行应用程序来进行训练，而治疗师可以通过服务器端进行进度的监测。 |
| [^47] | [Enlighten-anything:When Segment Anything Model Meets Low-light Image Enhancement.](http://arxiv.org/abs/2306.10286) | 本文提出了Enlighten-anything，在低光图像增强中将分段模型与SAM融合，实现了良好视觉感知的融合图像。 |
| [^48] | [Demystifying GPT Self-Repair for Code Generation.](http://arxiv.org/abs/2306.09896) | 本文分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，自我修复在较短和较简单的任务中效果更好，仅在某些代码部分上应用自我修复可以非常有效，本文提出的引导修复方法在 APPS 数据集上获得性能提升。 |
| [^49] | [The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement.](http://arxiv.org/abs/2306.09633) | 谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。 |
| [^50] | [Explaining Legal Concepts with Augmented Large Language Models (GPT-4).](http://arxiv.org/abs/2306.09525) | 本文评估了利用GPT-4增强解释法律术语性能的方法。与基准设置相比，使用增强的法律信息检索模块可以提高法律术语解释的质量，并消除模型的幻觉问题，从而为在法律领域使用大型语言模型开辟了新的途径。 |
| [^51] | [Explore, Establish, Exploit: Red Teaming Language Models from Scratch.](http://arxiv.org/abs/2306.09442) | 本文提出了一种新的红队行动，通过从高层次、抽象的规范出发来考虑语言模型的行为，以探究模型的创新和贡献。 |
| [^52] | [Active Inference in Hebbian Learning Networks.](http://arxiv.org/abs/2306.05053) | 本文研究了具有局部Hebbian可塑性的仿脑神经群体如何执行主动推理，通过两个不同的Hebbian神经元组成的网络来生成捕捉环境动态的生成模型，使用Mountain Car环境进行实验研究，结果表明所提出的Hebbian AIF方法优于使用Q-learning，同时不需要回放缓冲区。 |
| [^53] | [Denoising Diffusion Semantic Segmentation with Mask Prior Modeling.](http://arxiv.org/abs/2306.01721) | 本文提出了一种利用去噪扩散生成模型建模的mask先验，改进现有的语义分割方法。该方法达到了在多个基准数据集上的最先进效果，超参数调整较少。 |
| [^54] | [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing.](http://arxiv.org/abs/2305.14720) | BLIP-Diffusion是一种新的主体驱动图像生成模型，通过预训练的多模态编码器和主体表征学习任务实现了可控的文本到图像生成和编辑，并支持零样本生成和定制化微调。 |
| [^55] | [ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing.](http://arxiv.org/abs/2305.09770) | ConvXAI是一个基于对话的XAI系统，它集成了多种XAI类型，并将实际用户需求嵌入设计中，以提高实用性。 |
| [^56] | [Investigating the effect of sub-word segmentation on the performance of transformer language models.](http://arxiv.org/abs/2305.05480) | 本文研究使用单语词段算法StateMorph训练语言模型时，可以使模型更高效地收敛并获得更好的验证分数。 |
| [^57] | [Vision Langauge Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation.](http://arxiv.org/abs/2305.04474) | 本文提出了一种跨模态相似性逐步细化的对比学习策略，在视觉语言预训练中优化图像/文本锚点与其负样本文本/图像之间的互信息，有效应对了（部分）误反样本的挑战。 |
| [^58] | [On the Security Risks of Knowledge Graph Reasoning.](http://arxiv.org/abs/2305.02383) | 本文介绍了一个重要的人工智能任务 - 知识图谱推理(KGR)，并基于攻击者的目的、知识和攻击向量概括了KGR的安全威胁。本文提出了一种新型攻击ROAR，高度有效地误导KGR向目标查询提供预定义答案，但对于非目标查询几乎没有影响。 |
| [^59] | [TSMixer: An all-MLP Architecture for Time Series Forecasting.](http://arxiv.org/abs/2303.06053) | TSMixer是一种通过堆叠多层感知器（MLP）设计的新型结构，基于沿时间和特征维度的混合操作，能够在时间序列预测中表现出极好的性能。 |
| [^60] | [Visual Abstraction and Reasoning through Language.](http://arxiv.org/abs/2303.04091) | 本论文提出了一种通过自然语言描述任务的通用框架来解决Abstraction and Reasoning Corpus（ARC）问题，虽然还没有在ARC上击败最先进的DSL模型，但我们展示了我们的方法具有巨大的潜力，可以解决先前未解决的任务。 |
| [^61] | [Exploring Self-supervised Pre-trained ASR Models For Dysarthric and Elderly Speech Recognition.](http://arxiv.org/abs/2302.14564) | 本文探索了将自监督预训练模型与ASR系统相结合以提高口吃和老年人的语音识别性能的方法，包括输入特征融合，TDNN系统的帧级联合解码，以及多通道解码等。同时，在构建多模式的口吃和老年人语音识别系统时，采用了领域自适应的wav2vec2.0表示法。 |
| [^62] | [IGB: Addressing The Gaps In Labeling, Features, Heterogeneity, and Size of Public Graph Datasets for Deep Learning Research.](http://arxiv.org/abs/2302.13522) | IGB是一个研究数据集工具，包含同质和异质性学术图形，规模巨大，并提供工具用于生成不同特性的合成图形，为GNN研究人员提供解决公共图形数据集在标记、特征、异质性和大小方面差距的有价值资源。 |
| [^63] | [Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation.](http://arxiv.org/abs/2302.03673) | 本研究提出了一种新的独立线性Markov博弈模型，针对多智体强化学习中的大状态空间和大量代理问题，设计了新算法以学习Markov粗略相关均衡和Markov相关均衡。相比于现有的Markov博弈函数逼近技术，我们的方法能够大大降低样本复杂度并取得更高的精度。 |
| [^64] | [The Power of Linear Combinations: Learning with Random Convolutions.](http://arxiv.org/abs/2301.11360) | 本研究质疑了卷积神经网络中学习到的卷积核的重要性，提出了简单的线性组合方法，从随机卷积核中创建出表达能力强的网络运算符，通过隐含的正则化技术，可以提高整体性能。 |
| [^65] | [A Survey of Deep Learning for Mathematical Reasoning.](http://arxiv.org/abs/2212.10535) | 本文综述了过去十年中深度学习在数学推理领域的关键任务、数据集和方法，并评估了现有的基准和方法，探讨了未来的研究方向。 |
| [^66] | [\{kappa}HGCN: Tree-likeness Modeling via Continuous and Discrete Curvature Learning.](http://arxiv.org/abs/2212.01793) | 本文提出了一种新的\{kappa}HGCN模型，在双曲空间内实现树状结构建模，通过结合连续和离散曲率来学习输入图的基础几何结构，并在多个基准测试和数据集上取得了最先进的性能。 |
| [^67] | [Improving Proactive Dialog Agents Using Socially-Aware Reinforcement Learning.](http://arxiv.org/abs/2211.15359) | 本论文的主要贡献在于提出了一种新的方法，通过将社交和任务相关特征考虑在对话中，来优化主动对话策略，使其在任务效率高的同时，也能促进用户信任，从而提高人机交互的成功率和效率。 |
| [^68] | [Semi-Supervised Offline Reinforcement Learning with Action-Free Trajectories.](http://arxiv.org/abs/2210.06518) | 该论文提出了一种半监督离线强化学习的新设置，利用有标记的轨迹数据和无动作的轨迹数据训练反动力学模型以获取代理标签，最终使用任何离线强化学习算法以实现高成功率的表现。 |
| [^69] | [Efficient Learning of Locomotion Skills through the Discovery of Diverse Environmental Trajectory Generator Priors.](http://arxiv.org/abs/2210.04819) | 本论文提出了一种名为EETG的方法，使用“质量-多样性”算法学习一组多样化的专业化运动先验知识，能够帮助四足机器人成功地穿越各种环境，比使用单个轨迹生成器的方法更加有效。 |
| [^70] | [Algorithmic decision making methods for fair credit scoring.](http://arxiv.org/abs/2209.07912) | 本论文研究了12种偏差缓解方法在5个不同的公平性指标下的有效性，评估了它们对金融机构准确性和潜在盈利能力的影响，并指出了最成功和最不成功缓解方法。 |
| [^71] | [Exploiting Cross-domain And Cross-Lingual Ultrasound Tongue Imaging Features For Elderly And Dysarthric Speech Recognition.](http://arxiv.org/abs/2206.07327) | 本文利用跨领域和跨语言的A2A反演方法，利用平行音频和超声舌头成像（UTI）数据来产生基于UTI的发音特征，用于老年人和患有发音障碍者的语音识别研究，实验表明该方法相比基线ASR系统有更好的效果。 |
| [^72] | [Scrutinizing XAI using linear ground-truth data with suppressor variables.](http://arxiv.org/abs/2111.07473) | 研究提出了一个关于可解释人工智能(XAI)的方法，该方法提出了特征重要性的客观初步定义，以避免由于方法的行为引起的错误解读。 |
| [^73] | [A Survey of Knowledge Representation in Service Robotics.](http://arxiv.org/abs/1807.02192) | 本文重点综述了服务机器人中知识表示的研究进展，深入探讨了知识的获取、表示和应用，与其他学习模型的差异，并探讨了其在机器人任务中的重要性。 |

# 详细

[^1]: 从新的角度压缩ImageNet规模数据集：SRe$^2$L

    Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective. (arXiv:2306.13092v1 [cs.CV])

    [http://arxiv.org/abs/2306.13092](http://arxiv.org/abs/2306.13092)

    SRe$^2$L是一种数据集压缩方法，可以处理不同规模的数据集、模型体系结构和图像分辨率，具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力，在Tiny-ImageNet和ImageNet-1K数据集上实现了最佳性能，并超越了现有的最先进方法。

    

    我们提出了一个新的数据集压缩框架，称为Squeeze、Recover和Relabel（SRe$^2$L），它在训练期间分离了模型和合成数据的双层优化，以处理不同规模的数据集、模型体系结构和图像分辨率，从而实现有效的数据集压缩。所提出的方法展示了在不同数据集规模上的灵活性，并在合成图像任意分辨率、高分辨率训练的情况下具有低训练成本和内存消耗以及扩展到任意评估网络体系结构的能力。我们在Tiny-ImageNet和完整的ImageNet-1K数据集上进行了广泛的实验。在50IPC下，我们的方法在Tiny-ImageNet和ImageNet-1K上分别实现了42.5％和60.8％的最高验证精度，较之前所有最先进方法提高了14.5％和32.9％。我们的方法在Res上也比MTT快约52倍(ConvNet-4)和16倍。

    We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also outperforms MTT by approximately 52$\times$ (ConvNet-4) and 16$\times$ (Res
    
[^2]: 通过轨迹加权利用混合的离线强化学习数据集

    Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting. (arXiv:2306.13085v1 [cs.LG])

    [http://arxiv.org/abs/2306.13085](http://arxiv.org/abs/2306.13085)

    该论文提出了一种混合离线强化学习数据集的改进方法，可以通过重新加权采样数据集来充分利用高表现的轨迹，以提高目标策略的表现。

    

    多数离线强化学习算法返回一个最大化预期表现与诱导状态-动作占用的分布差异所带来的风险之间的权衡的目标策略，因此，目标策略的表现与数据集收集的行为策略的表现密切相关。我们发现，在由大多数低回报轨迹和少数高回报轨迹组成的混合数据集中，现有的离线强化学习算法受到低回报轨迹的过度制约，未能充分利用高表现轨迹。为了解决这个问题，我们表明，在带有随机初始状态的确定性MDPs中，可以通过重新加权采样数据集来诱导具有更高回报的行为策略的人工数据集。这种重新加权的采样策略可以与任何算法结合使用。

    Most offline reinforcement learning (RL) algorithms return a target policy maximizing a trade-off between (1) the expected performance gain over the behavior policy that collected the dataset, and (2) the risk stemming from the out-of-distribution-ness of the induced state-action occupancy. It follows that the performance of the target policy is strongly related to the performance of the behavior policy and, thus, the trajectory return distribution of the dataset. We show that in mixed datasets consisting of mostly low-return trajectories and minor high-return trajectories, state-of-the-art offline RL algorithms are overly restrained by low-return trajectories and fail to exploit high-performing trajectories to the fullest. To overcome this issue, we show that, in deterministic MDPs with stochastic initial states, the dataset sampling can be re-weighted to induce an artificial dataset whose behavior policy has a higher return. This re-weighted sampling strategy may be combined with any
    
[^3]: 生成人工智能技术对网络安全的影响和风险

    Impacts and Risk of Generative AI Technology on Cyber Defense. (arXiv:2306.13033v1 [cs.CR])

    [http://arxiv.org/abs/2306.13033](http://arxiv.org/abs/2306.13033)

    生成人工智能技术在文本、图像、音频和视频等领域具有广泛应用潜力，但由于其可能被滥用制作虚假信息，存在网络安全挑战和风险。为此，我们建议利用网络攻击生命周期作为网络防御的基础模型。

    

    生成人工智能技术（GenAI）已成为一种强大的技术，能够自主地在各个领域（如文本、图像、音频和视频）生成高度逼真的内容。由于在创意艺术、内容生成、虚拟助手和数据综合等方面有积极的应用潜力，因此引起了广泛的关注和采用。然而，GenAI的普及不断增加，引发了人们对其可能被用于制作逼真的网络钓鱼邮件、通过深度伪造视频生成虚假信息和通过看起来逼真的社交媒体帖子传播虚假信息等方面的担忧，这给网络安全带来了一系列新的挑战和风险。为了应对GenAI所带来的威胁，我们提出利用网络攻击生命周期来理解网络攻击的生命周期，作为网络防御的基础模型。本文旨在全面分析GenAI技术在网络攻击中可能引入的风险领域。

    Generative Artificial Intelligence (GenAI) has emerged as a powerful technology capable of autonomously producing highly realistic content in various domains, such as text, images, audio, and videos. With its potential for positive applications in creative arts, content generation, virtual assistants, and data synthesis, GenAI has garnered significant attention and adoption. However, the increasing adoption of GenAI raises concerns about its potential misuse for crafting convincing phishing emails, generating disinformation through deepfake videos, and spreading misinformation via authentic-looking social media posts, posing a new set of challenges and risks in the realm of cybersecurity. To combat the threats posed by GenAI, we propose leveraging the Cyber Kill Chain (CKC) to understand the lifecycle of cyberattacks, as a foundational model for cyber defense. This paper aims to provide a comprehensive analysis of the risk areas introduced by the offensive use of GenAI techniques in ea
    
[^4]: 基于难度条件生成器的可转移课程

    Transferable Curricula through Difficulty Conditioned Generators. (arXiv:2306.13028v1 [cs.AI])

    [http://arxiv.org/abs/2306.13028](http://arxiv.org/abs/2306.13028)

    本文提出了一种名为PERM的方法，可以直接对环境难度和RL代理的能力进行建模，通过匹配环境的难度生成课程，实现了在参数化环境中训练RL代理的有前途的结果。

    

    强化学习在复杂任务中展示了超人类水平的表现，如星际争霸、围棋、国际象棋等等。然而，将人工“专家”的知识传递给人类仍然是一个重大挑战。在课程中使用课程是一种有前途的转移途径。然而，最近的课程生成方法侧重于高效训练RL代理，然而这种方法依赖于代理人进展的代理测量标准，不能用于在现实世界中培训机器人（或更有雄心的是人类）。在本文中，我们介绍了一种名为参数化环境响应模型（PERM）的方法，该方法在参数化环境中训练RL代理表现出了有前途的结果。受项目反应理论的启发，PERM试图直接对环境难度和RL代理的能力建模。鉴于RL代理和人类在“发展的邻域”中更有效地接受训练，我们的方法通过匹配环境的难度以产生课程。

    Advancements in reinforcement learning (RL) have demonstrated superhuman performance in complex tasks such as Starcraft, Go, Chess etc. However, knowledge transfer from Artificial "Experts" to humans remain a significant challenge. A promising avenue for such transfer would be the use of curricula. Recent methods in curricula generation focuses on training RL agents efficiently, yet such methods rely on surrogate measures to track student progress, and are not suited for training robots in the real world (or more ambitiously humans). In this paper, we introduce a method named Parameterized Environment Response Model (PERM) that shows promising results in training RL agents in parameterized environments. Inspired by Item Response Theory, PERM seeks to model difficulty of environments and ability of RL agents directly. Given that RL agents and humans are trained more efficiently under the "zone of proximal development", our method generates a curriculum by matching the difficulty of an e
    
[^5]: 可微分决策树是否能够学习可解释的奖励函数?

    Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v1 [cs.LG])

    [http://arxiv.org/abs/2306.13004](http://arxiv.org/abs/2306.13004)

    本文提出了使用可微分决策树从人类偏好中学习具有表达能力和可解释性的奖励函数，通过在多个环境上的评估，发现该方法能够学习到可解释的奖励函数，但在强化学习测试时表现受到树的离散性的影响。

    

    学习人的意图和偏好的奖励函数越来越受到关注，但许多框架使用黑盒学习方法，难以解释。本文提出并评估了一种新颖方法，使用可微分决策树（DDT）从偏好中学习具有表达能力和可解释性的奖励函数，适用于低维和高维状态输入。我们在Cartpole、视觉网格世界环境和Atari游戏上评估了我们的算法，探讨了使用DDT学习可解释奖励函数的可行性。我们提供证据表明，学习到的奖励函数的树形结构有助于确定奖励函数与人类偏好的一致程度。我们可视化了学习到的奖励DDT，发现它们能够学习可解释的奖励函数，但树的离散性会影响强化学习在测试时的表现。

    There is an increasing interest in learning reward functions that model human intent and human preferences. However, many frameworks use blackbox learning methods that, while expressive, are difficult to interpret. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs) for both low- and high-dimensional state inputs. We explore and discuss the viability of learning interpretable reward functions using DDTs by evaluating our algorithm on Cartpole, Visual Gridworld environments, and Atari games. We provide evidence that that the tree structure of our learned reward function is useful in determining the extent to which a reward function is aligned with human preferences. We visualize the learned reward DDTs and find that they are capable of learning interpretable reward functions but that the discrete nature of the trees hurts the performance of reinforcement learning at test time. How
    
[^6]: 无政治智能？审查Delphi在美国有争议政治问题上的回应。

    Apolitical Intelligence? Auditing Delphi's responses on controversial political issues in the US. (arXiv:2306.13000v1 [cs.CY])

    [http://arxiv.org/abs/2306.13000](http://arxiv.org/abs/2306.13000)

    本文审查了用于众包伦理的大型语言模型Delphi在美国政治争议问题中的回应。研究发现，该模型的置信度校准不良，呈现显著的政治倾斜。作者从数据女权主义的角度探讨了中立性的问题，讨论了中立性概念如何转移权力、进一步边缘化无声的声音。

    

    随着生成语言模型在越来越广泛的应用中被部署，有关它们政治价值的担忧已成为前沿问题，各个政治派别对其存在偏见和缺乏中立性的批评纷至沓来。本文通过审计用于众包伦理的大型语言模型Delphi [arXiv:2110.07574]，从政治争议问题的角度，分析了Delphi与各个美国政治分组的不同回应。结果发现，Delphi的置信度校准不良，呈现显著的政治倾斜。基于这些结果，作者从数据女权主义的角度探讨了中立性的问题，讨论了中立性概念如何转移权力、进一步边缘化无声的声音。这些发现有望为关于模型与价值的规范性问题形成更多有思考性的辩论做出贡献。

    As generative language models are deployed in ever-wider contexts, concerns about their political values have come to the forefront with critique from all parts of the political spectrum that the models are biased and lack neutrality. However, the question of what neutrality is and whether it is desirable remains underexplored. In this paper, I examine neutrality through an audit of Delphi [arXiv:2110.07574], a large language model designed for crowdsourced ethics. I analyse how Delphi responds to politically controversial questions compared to different US political subgroups. I find that Delphi is poorly calibrated with respect to confidence and exhibits a significant political skew. Based on these results, I examine the question of neutrality from a data-feminist lens, in terms of how notions of neutrality shift power and further marginalise unheard voices. These findings can hopefully contribute to a more reflexive debate about the normative questions of alignment and what role we 
    
[^7]: 通过强化学习生成协同公式化的Alpha集合

    Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning. (arXiv:2306.12964v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.12964](http://arxiv.org/abs/2306.12964)

    本文提出了一种新的Alpha生成框架，它通过强化学习挖掘协同的公式化Alpha集合以优化它们作为组合模型的性能。

    

    在量化交易领域中，将原始股票历史数据转化为市场趋势指示信号是常见的实践。这些信号被称为Alpha因子。公式形式的Alpha因子更易于解释，因此风险意识强的实践者更喜欢它们。在实践中，往往同时使用一组公式化的Alpha因子进行更好的建模精度，因此我们需要找到能够良好协同的公式化Alpha集。但是，大多数传统的Alpha生成器分别挖掘单个Alpha，忽略了后续的Alpha组合重要性。本文提出了一种新的Alpha挖掘框架，它优先挖掘协同的Alpha集合，即直接使用下游组合模型的性能来优化Alpha生成器。我们的框架还利用了强化学习的强大探索能力，以更好地探索公式形式的Alpha因子的大量搜索空间。本文的贡献在于提出了使用强化学习生成协同的公式化Alpha集合的框架，优化它们作为组合模型的性能，这与传统的Alpha生成器分别挖掘单个Alpha的方法形成对比。

    In the field of quantitative trading, it is common practice to transform raw historical stock data into indicative signals for the market trend. Such signals are called alpha factors. Alphas in formula forms are more interpretable and thus favored by practitioners concerned with risk. In practice, a set of formulaic alphas is often used together for better modeling precision, so we need to find synergistic formulaic alpha sets that work well together. However, most traditional alpha generators mine alphas one by one separately, overlooking the fact that the alphas would be combined later. In this paper, we propose a new alpha-mining framework that prioritizes mining a synergistic set of alphas, i.e., it directly uses the performance of the downstream combination model to optimize the alpha generator. Our framework also leverages the strong exploratory capabilities of reinforcement learning~(RL) to better explore the vast search space of formulaic alphas. The contribution to the combina
    
[^8]: 用隐式神经表示进行音频压缩的连锁SIREN模型

    Siamese SIREN: Audio Compression with Implicit Neural Representations. (arXiv:2306.12957v1 [cs.SD])

    [http://arxiv.org/abs/2306.12957](http://arxiv.org/abs/2306.12957)

    该研究利用Siamese SIREN 建立了一种新的方法，使用了较少的网络参数来实现卓越的音频重建保真度，拓展了隐式神经表示压缩的应用领域。

    

    隐式神经表示(INRs)已成为表示多种数据形式(包括3D形状、图像和音频)的一种有前途的方法。虽然最近研究已经成功地将INRs应用于图像和3D形状的压缩中，但它们在音频压缩方面的潜力仍然未被充分探索。受此启发，我们对INRs进行了初步的音频压缩研究。我们的研究介绍了基于流行的SIREN架构的一种新方法: 连锁SIREN。我们的实验结果表明，与先前的INRs架构相比，连锁SIREN在利用更少的网络参数的同时实现了更高的音频重建保真度。

    Implicit Neural Representations (INRs) have emerged as a promising method for representing diverse data modalities, including 3D shapes, images, and audio. While recent research has demonstrated successful applications of INRs in image and 3D shape compression, their potential for audio compression remains largely unexplored. Motivated by this, we present a preliminary investigation into the use of INRs for audio compression. Our study introduces Siamese SIREN, a novel approach based on the popular SIREN architecture. Our experimental results indicate that Siamese SIREN achieves superior audio reconstruction fidelity while utilizing fewer network parameters compared to previous INR architectures.
    
[^9]: 进化计算图算法

    Evolving Computation Graphs. (arXiv:2306.12943v1 [cs.LG])

    [http://arxiv.org/abs/2306.12943](http://arxiv.org/abs/2306.12943)

    本论文提出了一种进化计算图算法（ECGs），用于提高针对异质性数据的图神经网络（GNN）性能，通过重连GNN的计算图增加连接同一类节点的边缘以提升性能。

    

    在关系数据建模方面，图神经网络（GNN）已经展现出了成功，尤其是对于那些表现出同质性的数据：当节点之间的连接往往暗示它们属于同一类时，这一点更为明显。然而，虽然这个假设在许多相关情况下是成立的，但在重要的现实场景中也有违反这个假设的情况，这促进了对GNN在这些情况下进行改进的研究。在这项工作中，我们提出了一种新颖的方法——进化计算图算法（ECGs），用于增强针对异质性数据的GNN。我们的方法建立在先前的理论洞见之上，将节点度、高同质性和内部与类间嵌入相似性联系在一起，通过重连GNN的计算图来增加连接同一类节点的边缘。我们利用较弱的分类器来识别这些边缘，从而最终提高了GNN对非同质性数据的性能。我们评估ECGs在一组多样化的最近提出的异质数据集上的表现。

    Graph neural networks (GNNs) have demonstrated success in modeling relational data, especially for data that exhibits homophily: when a connection between nodes tends to imply that they belong to the same class. However, while this assumption is true in many relevant situations, there are important real-world scenarios that violate this assumption, and this has spurred research into improving GNNs for these cases. In this work, we propose Evolving Computation Graphs (ECGs), a novel method for enhancing GNNs on heterophilic datasets. Our approach builds on prior theoretical insights linking node degree, high homophily, and inter vs intra-class embedding similarity by rewiring the GNNs' computation graph towards adding edges that connect nodes that are likely to be in the same class. We utilise weaker classifiers to identify these edges, ultimately improving GNN performance on non-homophilic data as a result. We evaluate ECGs on a diverse set of recently-proposed heterophilous datasets a
    
[^10]: 可量化Transformer：通过帮助注意力头“什么也不做”去除离群值

    Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing. (arXiv:2306.12929v1 [cs.LG])

    [http://arxiv.org/abs/2306.12929](http://arxiv.org/abs/2306.12929)

    本文提出了一种称为“Helper-Head”的方法，可以通过教授注意头忽略输入和输出的某些部分来消除离群值，从而实现对transformer的可量化。实验结果表明，该方法在低、高比特宽度设置上均优于现有方法，在两个流行的语言建模基准上实现了最先进的结果。

    

    过去几年里，Transformer模型已经被广泛应用于各个领域，特别是大型语言模型已经显著推进了人工智能领域的发展。由于其规模，这些网络的能力已经大大增强，但这是以极大的计算成本为代价的。量化是减少神经网络计算时间和存储器消耗的最有效方法之一。然而，许多研究表明，现代transformer模型往往学习到其激活中的强离群值，这使得它们难以量化。为保持可接受的性能，这些离群值的存在需要将激活置于更高的比特宽度或使用不同的数字格式，进行额外的微调或其他变通方法。本文展示了强离群值与特定注意头行为的相关性，这些头试图学习“无操作”或仅仅是部分残差更新。为了实现注意力头中需要的精确零位，我们引入了一个称为“Helper-Head”的方法，教授注意力头忽略输入和输出的某些部分。我们还引入了一种利用这些额外信息的量化技术，可以使用低精度量化甚至是强离群数据。在几个基准数据集上的实验证明，我们的方法在低、高比特宽度设置上均优于现有方法，在两个流行的语言建模基准上实现了最先进的结果。

    Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI significantly. Due to their size, the capability of these networks has increased tremendously, but this has come at the cost of a significant increase in necessary compute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize. To retain acceptable performance, the existence of these outliers requires activations to be in higher bitwidth or the use of different numeric formats, extra fine-tuning, or other workarounds. We show that strong outliers are related to very specific behavior of attention heads that try to learn a "no-op" or just a partial update of the residual. To achieve the exact zeros needed in the attention 
    
[^11]: 具有全局状态预测的分散式多智能体强化学习

    Decentralized Multi-Agent Reinforcement Learning with Global State Prediction. (arXiv:2306.12926v1 [cs.RO])

    [http://arxiv.org/abs/2306.12926](http://arxiv.org/abs/2306.12926)

    本文研究了分散式多智能体强化学习中的一个关键挑战：如何在没有全局信息的情况下有效训练机器人。我们提出了一种基于状态预测的方法，在不需要显式通信的情况下使机器人能够协调行动，实现更快更好的学习效果和任务执行性能。

    

    深度强化学习（DRL）在控制单个机器人方面取得了显着的成功。然而，将DRL应用于机器人群体存在重大挑战。其中一个关键挑战是非静态性，即当两个或更多机器人同时更新个体或共享政策时，会进入一个相互依存的培训过程，并且不保证收敛。克服非静态性通常涉及使用其他智能体的全局信息来训练机器人，例如其他智能体的状态和/或行动。相比之下，本文探讨了如何消除全局信息的需求。由于缺乏其他信息体的全局知识，我们将问题描述为部分可观察的马尔可夫决策过程。在以集体运输为测试场景的情况下，我们研究了两种多智能体培训方法。在第一种方法中，机器人不交换信息，并且被训练依靠通过推（push）和拉（pull）物体进行隐式通信。在第二种方法中，机器人彼此共享状态预测，使他们能够在没有显式通信的情况下协调行动。我们的实验表明，共享预测可以使智能体更有效地学习，同时在需要更少与环境交互的情况下实现更好的任务执行性能。

    Deep reinforcement learning (DRL) has seen remarkable success in the control of single robots. However, applying DRL to robot swarms presents significant challenges. A critical challenge is non-stationarity, which occurs when two or more robots update individual or shared policies concurrently, thereby engaging in an interdependent training process with no guarantees of convergence. Circumventing non-stationarity typically involves training the robots with global information about other agents' states and/or actions. In contrast, in this paper we explore how to remove the need for global information. We pose our problem as a Partially Observable Markov Decision Process, due to the absence of global knowledge on other agents. Using collective transport as a testbed scenario, we study two approaches to multi-agent training. In the first, the robots exchange no messages, and are trained to rely on implicit communication through push-and-pull on the object to transport. In the second appro
    
[^12]: AudioPaLM：一款能说会听的大型语言模型

    AudioPaLM: A Large Language Model That Can Speak and Listen. (arXiv:2306.12925v1 [cs.CL])

    [http://arxiv.org/abs/2306.12925](http://arxiv.org/abs/2306.12925)

    AudioPaLM是一款能够更好地处理语音任务的大型语言模型，它继承了AudioLM的语音身份和语调信息保留能力以及PaLM-2的语言知识，可用于语音识别、语音翻译等应用。

    

    本文介绍了一种用于语音理解和生成的大型语言模型——AudioPaLM。它将基于文本的语言模型PaLM-2[Anil等人，2023]和基于音频的语言模型AudioLM[Borsos等人，2022]结合成一个统一的多模态结构，可以处理和生成文本和语音，包括语音识别和语音翻译等应用。AudioPaLM继承了从AudioLM中保留语音发音者身份和语调等的能力，以及只存在于文本大型语言模型PaLM-2中的语言知识。我们表明，通过使用文本大型语言模型的权重进行初始化，可以改善语音处理，成功地利用了预训练中使用的更大量的文本训练数据来协助语音任务。最终得到的模型在语音翻译任务中明显优于现有系统，并且具有进行零-shot言语文本转换的能力。

    We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for ma
    
[^13]: FuXi: 一个15天全球天气预报级联机器学习系统

    FuXi: A cascade machine learning forecasting system for 15-day global weather forecast. (arXiv:2306.12873v1 [physics.ao-ph])

    [http://arxiv.org/abs/2306.12873](http://arxiv.org/abs/2306.12873)

    逐步级联模型FuXi在15天全球天气预报中表现出更好的性能，并通过减少预测误差的积累达到优化。

    

    近年来，随着机器学习模型在天气预报中的快速发展，最先进的机器学习模型在0.25度空间分辨率下的10天天气预报中已经表现出比欧洲中期天气预报中心(ECMWF)的高分辨率预报(HRES)更优越的性能。然而，挑战在于在15天预报中表现与ECMWF集合平均(EM)相当。以前的研究表明，缓解预报误差的积累对于有效的长期预报非常重要。尽管有许多减少积累误差的努力，包括自回归多时间步长损失，但使用单个模型发现无法在短和长导出时间上达到最佳性能。因此，我们提出了FuXi，这是一个级联机器学习天气预测系统，提供了分辨率为0.25度、时间分辨率为6小时的15天全球预测。FuXi基于级联集合模型开发，它集成了多种模型的优势，并减少了预测误差的积累。使用空气温度，比湿度和位势高度的均方根误差(RMSE)和异常相关系数(ACC)评估了FuXi的性能。结果表明，与ECMWF HRES相比，FuXi在15天预报中表现出更好的性能，并显著减少了积累误差。

    Over the past few years, due to the rapid development of machine learning (ML) models for weather forecasting, state-of-the-art ML models have shown superior performance compared to the European Centre for Medium-Range Weather Forecasts (ECMWF)'s high-resolution forecast (HRES) in 10-day forecasts at a spatial resolution of 0.25 degree. However, the challenge remains to perform comparably to the ECMWF ensemble mean (EM) in 15-day forecasts. Previous studies have demonstrated the importance of mitigating the accumulation of forecast errors for effective long-term forecasts. Despite numerous efforts to reduce accumulation errors, including autoregressive multi-time step loss, using a single model is found to be insufficient to achieve optimal performance in both short and long lead times. Therefore, we present FuXi, a cascaded ML weather forecasting system that provides 15-day global forecasts with a temporal resolution of 6 hours and a spatial resolution of 0.25 degree. FuXi is develope
    
[^14]: 不要治疗症状，找到病因！用高效人工智能方法进行（交互式）调试

    Don't Treat the Symptom, Find the Cause! Efficient Artificial-Intelligence Methods for (Interactive) Debugging. (arXiv:2306.12850v1 [cs.AI])

    [http://arxiv.org/abs/2306.12850](http://arxiv.org/abs/2306.12850)

    本文提出了用基于模型诊断的高效人工智能方法进行（交互式）调试的方案，包括贝叶斯网络和马尔可夫决策过程，并使用真实世界的例子证明了方法的有效性，这能够显著减少系统维修所需的时间和成本，并提高现代系统的整体可靠性。

    

    在现代社会中，我们不断使用、利用、交互和依赖着不断高Sophistication的系统，从汽车、电子商务中的推荐系统，上网时的网络，使用我们的个人电脑和智能手机时的集成电路，保证我们的能源供应的电网，访问银行账户时的安全关键软件，到财务规划和决策时的电子表格等等。这些系统的复杂性加上我们的高度依赖意味着系统出现故障的可能性不容忽视，并且这样的故障对我们日常生活的影响可能非常大。因此，将正在出现的故障的伤害降到最低是一项关键要求，这意味着最小化系统停机时间以及系统维修成本。这就是模型诊断的作用。本文提出了基于模型诊断的高效人工智能方法来进行（交互式）调试。我们引入概率模型诊断框架，包括贝叶斯网络和马尔可夫决策过程，并且使用真实世界的例子证明了我们的方法的有效性。我们的方法可以显著减少系统维修所需的时间和成本，并能提高现代系统的整体可靠性。

    In the modern world, we are permanently using, leveraging, interacting with, and relying upon systems of ever higher sophistication, ranging from our cars, recommender systems in e-commerce, and networks when we go online, to integrated circuits when using our PCs and smartphones, the power grid to ensure our energy supply, security-critical software when accessing our bank accounts, and spreadsheets for financial planning and decision making. The complexity of these systems coupled with our high dependency on them implies both a non-negligible likelihood of system failures, and a high potential that such failures have significant negative effects on our everyday life. For that reason, it is a vital requirement to keep the harm of emerging failures to a minimum, which means minimizing the system downtime as well as the cost of system repair. This is where model-based diagnosis comes into play.  Model-based diagnosis is a principled, domain-independent approach that can be generally app
    
[^15]: XAI-TRIS：用于量化机器学习解释性能的非线性基准测试

    XAI-TRIS: Non-linear benchmarks to quantify ML explanation performance. (arXiv:2306.12816v1 [cs.LG])

    [http://arxiv.org/abs/2306.12816](http://arxiv.org/abs/2306.12816)

    XAI-TRIS提供了用于测试机器学习解释性能的具有挑战性的非线性基准数据集，并揭示了现有XAI方法的局限性。

    

    “可解释的”人工智能（XAI）领域已经产生了高度引用的方法，旨在使复杂的机器学习（ML）方法的决策“可理解”给人类，例如通过对输入特征进行“重要性”评分来实现。然而，缺乏正式的基础，使得无法从给定XAI方法的结果中安全地得出结论，迄今为止，也阻碍了XAI方法的理论验证和实证验证。这意味着，目前缺乏适当的解决方法来解决通常由深度神经网络解决的具有挑战性的非线性问题。本文针对三种不同的非线性分类情景制作基准数据集，其中通过设计已知重要的类条件特征，作为地面实况解释。利用新的定量指标，我们在三个深度学习模型架构上测试了广泛的XAI方法的解释性能。我们展示了流行的XAI方法的很多局限性。

    The field of 'explainable' artificial intelligence (XAI) has produced highly cited methods that seek to make the decisions of complex machine learning (ML) methods 'understandable' to humans, for example by attributing 'importance' scores to input features. Yet, a lack of formal underpinning leaves it unclear as to what conclusions can safely be drawn from the results of a given XAI method and has also so far hindered the theoretical verification and empirical validation of XAI methods. This means that challenging non-linear problems, typically solved by deep neural networks, presently lack appropriate remedies. Here, we craft benchmark datasets for three different non-linear classification scenarios, in which the important class-conditional features are known by design, serving as ground truth explanations. Using novel quantitative metrics, we benchmark the explanation performance of a wide set of XAI methods across three deep learning model architectures. We show that popular XAI met
    
[^16]: 条件生成器在限价订单簿环境中的应用：可解释性、挑战和强健性探索

    Conditional Generators for Limit Order Book Environments: Explainability, Challenges, and Robustness. (arXiv:2306.12806v1 [q-fin.TR])

    [http://arxiv.org/abs/2306.12806](http://arxiv.org/abs/2306.12806)

    本文研究了在订单簿模拟中使用条件生成模型的方法，并探索了其对输入特征的依赖性及其优点和缺点，提高了CGAN的逼真度和强健性。

    

    限价订单簿是一种基本而广泛使用的市场机制。本文研究了在订单簿模拟中使用条件生成模型的方法。这种方法因为其能够对交易代理的存在做出反应而受到关注，被认为是传统回测的替代方案。本文使用了来自Coletta等人（2022）的最先进的CGAN，探索了它对输入特征的依赖性，强调了其优点和缺点。为此，我们对模型特征及其机制进行了“对抗攻击”。然后展示了如何利用这些认识来提高CGAN的逼真度和强健性。最后，我们提出了未来工作的路线图。

    Limit order books are a fundamental and widespread market mechanism. This paper investigates the use of conditional generative models for order book simulation. For developing a trading agent, this approach has drawn recent attention as an alternative to traditional backtesting due to its ability to react to the presence of the trading agent. Using a state-of-the-art CGAN (from Coletta et al. (2022)), we explore its dependence upon input features, which highlights both strengths and weaknesses. To do this, we use "adversarial attacks" on the model's features and its mechanism. We then show how these insights can be used to improve the CGAN, both in terms of its realism and robustness. We finish by laying out a roadmap for future work.
    
[^17]: Otter-Knowledge：不同来源的多模态知识图谱表示学习在药物发现中的基准测试。

    Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery. (arXiv:2306.12802v1 [cs.LG])

    [http://arxiv.org/abs/2306.12802](http://arxiv.org/abs/2306.12802)

    本研究在药物发现方面使用了多模态知识图谱表示学习，获得了最先进的结果。他们整合了来自不同来源的数据，并提供了基于这些数据的预训练模型。

    

    最近，表示学习的研究利用大量的蛋白质或分子数据库，通过无监督学习技术获得药物和蛋白质结构的知识。这些预训练表示已被证明可以显著提高后续任务的准确性，如预测药物和靶蛋白之间的亲和力。在本研究中，我们展示了通过将来自不同来源和模态的知识图谱整合到序列或SMILES表示中，可以进一步丰富表示，并在已建立的基准测试数据集上实现最先进的结果。我们提供了来自7个公共来源的预处理和整合数据，其中包括超过30M个三元组。此外，我们还提供了基于这些数据的预训练模型，以及它们在Therapeutic Data Commons (TDC)基准测试中性能报告的结果。

    Recent research in representation learning utilizes large databases of proteins or molecules to acquire knowledge of drug and protein structures through unsupervised learning techniques. These pre-trained representations have proven to significantly enhance the accuracy of subsequent tasks, such as predicting the affinity between drugs and target proteins. In this study, we demonstrate that by incorporating knowledge graphs from diverse sources and modalities into the sequences or SMILES representation, we can further enrich the representation and achieve state-of-the-art results on established benchmark datasets. We provide preprocessed and integrated data obtained from 7 public sources, which encompass over 30M triples. Additionally, we make available the pre-trained models based on this data, along with the reported outcomes of their performance on three widely-used benchmark datasets for drug-target binding affinity prediction found in the Therapeutic Data Commons (TDC) benchmarks.
    
[^18]: MFCCGAN：一种基于MFCC和对抗学习的语音合成器

    MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning. (arXiv:2306.12785v1 [cs.SD])

    [http://arxiv.org/abs/2306.12785](http://arxiv.org/abs/2306.12785)

    本文提出了一种采用对抗学习的新型语音合成器MFCCGAN，使用MFCC作为输入，产生比传统规则MFCC的WORLD语音合成器更高清晰度、更好理解度的语音。

    

    本文介绍了一种基于对抗学习的新型语音合成器MFCCGAN，采用MFCC作为输入并生成原始语音波形。受益于GAN模型的能力，它产生的语音比基于规则的MFCC的语音合成器WORLD具有更高的清晰度。我们根据流行的侵入式客观语音可懂度测量（STOI）和质量（NISQA得分）进行了模型评估。实验结果表明，我们提出的系统优于Librosa MFCC- inversion（在STOI和NISQA得分中增加了约26％至53％和16％至78％），与传统的基于规则的编码器WORLD相比，可辨度提高约10％，自然度提高约4％。然而，WORLD需要额外的F0数据。最后，对基于STOI的鉴别器使用感知损失可以进一步提高质量。基于WebMUSHRA的主观测试也显示了所提出方法的质量。

    In this paper, we introduce MFCCGAN as a novel speech synthesizer based on adversarial learning that adopts MFCCs as input and generates raw speech waveforms. Benefiting the GAN model capabilities, it produces speech with higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD. We evaluated the model based on a popular intrusive objective speech intelligibility measure (STOI) and quality (NISQA score). Experimental results show that our proposed system outperforms Librosa MFCC- inversion (by an increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a rise of about 10% in intelligibility and about 4% in naturalness in comparison with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family. However, WORLD needs additional data like F0. Finally, using perceptual loss in discriminators based on STOI could improve the quality more. WebMUSHRA-based subjective tests also show the quality of the proposed approach.
    
[^19]: 关于生成式检索模型的鲁棒性:基于超出分布视角的研究

    On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective. (arXiv:2306.12756v1 [cs.IR])

    [http://arxiv.org/abs/2306.12756](http://arxiv.org/abs/2306.12756)

    本论文研究了生成式检索模型在超出分布（OOD）泛化方面的鲁棒性，定义了从三个方面衡量OOD鲁棒性，并分析了其与密集检索模型的比较。实验结果表明，生成式检索模型的OOD鲁棒性较弱，特别是在面向任务的超出分布场景中更为明显。针对造成鲁棒性较弱的原因，提出了潜在的解决方案。

    

    最近，生成式检索在信息检索领域日益受到关注，它通过直接生成标识符来检索文档。迄今为止，人们已经付出了很多努力来开发有效的生成式检索模型。然而，在鲁棒性方面却得到的关注较少。当一个新的检索范式进入到真实世界应用中时，衡量超出分布（OOD）泛化也是至关重要的，即生成式检索模型如何泛化到新的分布中。为了回答这个问题，我们首先从检索问题的三个方面定义OOD鲁棒性：1）查询变化；2）未知的查询类型；3）未知任务。基于这个分类法，我们进行实证研究，分析了几个代表性生成式检索模型与密集检索模型在OOD鲁棒性方面的比较。实证结果表明，生成式检索模型的OOD鲁棒性比密集检索模型弱，特别是在面向任务的OOD场景中更明显。我们进一步研究了造成生成式检索模型鲁棒性较弱的原因，并提出了改善它们OOD泛化性能的潜在解决方法。

    Recently, we have witnessed generative retrieval increasingly gaining attention in the information retrieval (IR) field, which retrieves documents by directly generating their identifiers. So far, much effort has been devoted to developing effective generative retrieval models. There has been less attention paid to the robustness perspective. When a new retrieval paradigm enters into the real-world application, it is also critical to measure the out-of-distribution (OOD) generalization, i.e., how would generative retrieval models generalize to new distributions. To answer this question, firstly, we define OOD robustness from three perspectives in retrieval problems: 1) The query variations; 2) The unforeseen query types; and 3) The unforeseen tasks. Based on this taxonomy, we conduct empirical studies to analyze the OOD robustness of several representative generative retrieval models against dense retrieval models. The empirical results indicate that the OOD robustness of generative re
    
[^20]: 加Spiking还是不加Spiking？SNN和CNN FPGA实现的量化比较

    To Spike or Not to Spike? A Quantitative Comparison of SNN and CNN FPGA Implementations. (arXiv:2306.12742v1 [cs.AR])

    [http://arxiv.org/abs/2306.12742](http://arxiv.org/abs/2306.12742)

    本文分析了多个SNN硬件加速器，用于FPGA的性能和能量效率。提出了一种SNN编码方案和一种内存组织技术以进一步提高能量效率。比较结果显示，SNN加速器没有比CNN加速器显著降低能量消耗。

    

    卷积神经网络（CNNs）被广泛用于解决各种问题，例如图像分类。由于它们的计算和数据密集型特性，CNN加速器已经作为ASIC或FPGA开发。由于应用的复杂性增加，这些加速器的资源成本和能量要求不断增长。脉冲神经网络（SNNs）是一种新兴的替代CNN实现的方法，承诺更高的资源和能量效率。本文主要研究问题是SNN加速器是否真正满足降低能量要求的期望，相比于它们的CNN等效项。为此，我们分析多个SNN硬件加速器，用于FPGA的性能和能量效率。我们提出了一种新的脉冲事件队列编码方案和一种新的内存组织技术，以进一步提高SNN的能量效率。这两种技术已经集成到最先进的SNN架构中，并进行了评估。

    Convolutional Neural Networks (CNNs) are widely employed to solve various problems, e.g., image classification. Due to their compute- and data-intensive nature, CNN accelerators have been developed as ASICs or on FPGAs. Increasing complexity of applications has caused resource costs and energy requirements of these accelerators to grow. Spiking Neural Networks (SNNs) are an emerging alternative to CNN implementations, promising higher resource and energy efficiency. The main research question addressed in this paper is whether SNN accelerators truly meet these expectations of reduced energy requirements compared to their CNN equivalents. For this purpose, we analyze multiple SNN hardware accelerators for FPGAs regarding performance and energy efficiency. We present a novel encoding scheme of spike event queues and a novel memory organization technique to improve SNN energy efficiency further. Both techniques have been integrated into a state-of-the-art SNN architecture and evaluated fo
    
[^21]: OptIForest: 用于异常检测的最优隔离森林算法

    OptIForest: Optimal Isolation Forest for Anomaly Detection. (arXiv:2306.12703v1 [cs.LG])

    [http://arxiv.org/abs/2306.12703](http://arxiv.org/abs/2306.12703)

    本论文针对隔离森林算法中分支因子的最优取值问题，基于隔离效率提出创新算法OptIForest，该算法结构简洁、检测性能优秀，可应用于各种异常检测场景。

    

    异常检测在诸多领域扮演着重要角色，诸如网络安全中的入侵检测、金融风险监控、人类健康监测等。根据隔离森林机制提出的一类异常检测方法由于其简洁、有效、高效而备受青睐，例如针对实际部署，iForest是最常用的检测器之一。虽然大多数隔离森林采用二进制结构，但框架LSHiForest已经证明了多叉隔离树结构可以带来更好的检测性能。然而，尚无理论工作回答关于隔离森林的最优树结构的根本和实践重要问题，即何种分支因子的隔离树结构最优。本文提出隔离效率理论来解答该问题，进而确定了一个隔离树的最优分支因子。

    Anomaly detection plays an increasingly important role in various fields for critical tasks such as intrusion detection in cybersecurity, financial risk detection, and human health monitoring. A variety of anomaly detection methods have been proposed, and a category based on the isolation forest mechanism stands out due to its simplicity, effectiveness, and efficiency, e.g., iForest is often employed as a state-of-the-art detector for real deployment. While the majority of isolation forests use the binary structure, a framework LSHiForest has demonstrated that the multi-fork isolation tree structure can lead to better detection performance. However, there is no theoretical work answering the fundamentally and practically important question on the optimal tree structure for an isolation forest with respect to the branching factor. In this paper, we establish a theory on isolation efficiency to answer the question and determine the optimal branching factor for an isolation tree. Based on
    
[^22]: 量子增强机器学习中的对抗鲁棒性研究

    Towards quantum enhanced adversarial robustness in machine learning. (arXiv:2306.12688v1 [quant-ph])

    [http://arxiv.org/abs/2306.12688](http://arxiv.org/abs/2306.12688)

    本文讨论了将量子计算与机器学习相结合的新领域，即量子对抗机器学习，它有望提供更好的准确性，更高的计算效率和更强的对抗攻击鲁棒性。但在实现稳健的实际QAML工具方面仍存在挑战。

    

    机器学习算法是进行图像分类和特征检测等数据驱动任务的强大工具，然而它们对于对抗性样本的脆弱性——即对于被篡改以欺骗算法的输入样本——仍然是一个严峻的挑战。将机器学习与量子计算相结合，有可能提供不仅更好的准确性和计算效率，而且还能够在对抗性攻击方面具有优越的鲁棒性的工具。确实，最近的工作利用了量子机械现象来防御对抗性攻击，推动了量子对抗机器学习（QAML）领域的快速发展，并可能产生新的量子优势来源。尽管有着有希望的初步结果，但在构建稳健的实际QAML工具方面仍然存在挑战。在本综述中，我们讨论了QAML的最新进展，并确定了关键挑战。同时，我们还提出了可能确定实现QA ML路线的未来研究方向。

    Machine learning algorithms are powerful tools for data driven tasks such as image classification and feature detection, however their vulnerability to adversarial examples - input samples manipulated to fool the algorithm remains a serious challenge. The integration of machine learning with quantum computing has the potential to yield tools offering not only better accuracy and computational efficiency, but also superior robustness against adversarial attacks. Indeed, recent work has employed quantum mechanical phenomena to defend against adversarial attacks, spurring the rapid development of the field of quantum adversarial machine learning (QAML) and potentially yielding a new source of quantum advantage. Despite promising early results, there remain challenges towards building robust real-world QAML tools. In this review we discuss recent progress in QAML and identify key challenges. We also suggest future research directions which could determine the route to practicality for QA
    
[^23]: 知识图谱关系预测中的可解释表示

    Explainable Representations for Relation Prediction in Knowledge Graphs. (arXiv:2306.12687v1 [cs.LG])

    [http://arxiv.org/abs/2306.12687](http://arxiv.org/abs/2306.12687)

    提出一种新的可解释表示方法SEEK用于知识图谱关系预测，通过识别实体之间相关的共享语义方面生成一个多方面和可解释的表示，并在真实世界的蛋白质相互作用预测和基因-疾病关联性预测任务中进行了验证。

    

    知识图谱以本体论支持的语义丰富结构表示实体及其关系。基于机器学习方法探索这个数据通常依赖于知识图谱嵌入，它可产生实体的潜在表示，保留结构和本地图邻域特性，但牺牲了可解释性。然而，在像链接或关系预测等任务中，了解哪些具体特征更好地解释关系对于支持复杂或关键应用至关重要。本文提出了一种名为SEEK的新型方法，用于解释支持知识图谱关系预测的表示。它基于识别实体之间相关的共享语义方面（即子图），并学习每个子图的表示，生成一个多方面和可解释的表示。我们将SEEK评估于两个真实世界中高度复杂的关系预测任务：蛋白质相互作用预测和基因-疾病关联性预测。

    Knowledge graphs represent real-world entities and their relations in a semantically-rich structure supported by ontologies. Exploring this data with machine learning methods often relies on knowledge graph embeddings, which produce latent representations of entities that preserve structural and local graph neighbourhood properties, but sacrifice explainability. However, in tasks such as link or relation prediction, understanding which specific features better explain a relation is crucial to support complex or critical applications.  We propose SEEK, a novel approach for explainable representations to support relation prediction in knowledge graphs. It is based on identifying relevant shared semantic aspects (i.e., subgraphs) between entities and learning representations for each subgraph, producing a multi-faceted and explainable representation.  We evaluate SEEK on two real-world highly complex relation prediction tasks: protein-protein interaction prediction and gene-disease associ
    
[^24]: 推荐系统的最新发展：综述

    Recent Developments in Recommender Systems: A Survey. (arXiv:2306.12680v1 [cs.IR])

    [http://arxiv.org/abs/2306.12680](http://arxiv.org/abs/2306.12680)

    本篇综述全面总结了推荐系统领域的最新进展和趋势，包括推荐系统分类，知识推荐系统，鲁棒性，数据偏见和公平性问题，以及评估度量。该研究还提供了未来研究的新方向。

    

    这篇技术综述全面总结了推荐系统领域的最新进展。本研究的目的是提供领域内现状的概述，并强调推荐系统发展的最新趋势。该研究首先全面总结了主要推荐系统分类方法，包括个性化和群组推荐系统，然后深入探讨了基于知识的推荐系统类别。此外，该综述分析了推荐系统中的鲁棒性、数据偏见和公平性问题，并总结了评估度量用于评估这些系统的性能。最后，研究提供了有关推荐系统发展的最新趋势的见解，并强调了未来研究的新方向。

    In this technical survey, we comprehensively summarize the latest advancements in the field of recommender systems. The objective of this study is to provide an overview of the current state-of-the-art in the field and highlight the latest trends in the development of recommender systems. The study starts with a comprehensive summary of the main taxonomy of recommender systems, including personalized and group recommender systems, and then delves into the category of knowledge-based recommender systems. In addition, the survey analyzes the robustness, data bias, and fairness issues in recommender systems, summarizing the evaluation metrics used to assess the performance of these systems. Finally, the study provides insights into the latest trends in the development of recommender systems and highlights the new directions for future research in the field.
    
[^25]: SoftGPT: 通过预训练异质图变换生成模型学习面向目标的软物体操作技能

    SoftGPT: Learn Goal-oriented Soft Object Manipulation Skills by Generative Pre-trained Heterogeneous Graph Transformer. (arXiv:2306.12677v1 [cs.RO])

    [http://arxiv.org/abs/2306.12677](http://arxiv.org/abs/2306.12677)

    我们提出了一种预训练软物体操作技能学习模型SoftGPT，它使用大量的数据进行训练，结合三维异构图表示和基于GPT的动态模型，并利用这种先前知识来学习目标导向的软物体操作技能，从而打破了这一领域的技术瓶颈。

    

    在家庭场景中进行的软物体操作任务由于其复杂的动态特性和可变的形状特征，对现有机器人技能学习技术提出了重大挑战。由于从人类演示中学习新的操作技能是机器人应用的有效方式，因此开发软物体表示和动态的先前知识是必要的。在这方面，我们提出了一种名为SoftGPT的预训练软物体操作技能学习模型，该模型使用大量的探索数据进行训练，包括三维异构图表示和基于GPT的动态模型。针对每个下游任务，训练一个面向目标的策略代理以预测后续动作，SoftGPT生成这些动作的后果。将这两种方法集成在一起，建立了机器人思考过程，为促进策略学习提供了展开。我们的结果表明，利用软物体动力学和表示的先前知识使SoftGPT能够学习面向目标的软物体操作技能，从而胜过现有的最先进方法。

    Soft object manipulation tasks in domestic scenes pose a significant challenge for existing robotic skill learning techniques due to their complex dynamics and variable shape characteristics. Since learning new manipulation skills from human demonstration is an effective way for robot applications, developing prior knowledge of the representation and dynamics of soft objects is necessary. In this regard, we propose a pre-trained soft object manipulation skill learning model, namely SoftGPT, that is trained using large amounts of exploration data, consisting of a three-dimensional heterogeneous graph representation and a GPT-based dynamics model. For each downstream task, a goal-oriented policy agent is trained to predict the subsequent actions, and SoftGPT generates the consequences of these actions. Integrating these two approaches establishes a thinking process in the robot's mind that provides rollout for facilitating policy learning. Our results demonstrate that leveraging prior kn
    
[^26]: 基于忆阻器的储层神经网络学习优化

    Memristive Reservoirs Learn to Learn. (arXiv:2306.12676v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2306.12676](http://arxiv.org/abs/2306.12676)

    本研究提出了学会学习的框架来解决基于忆阻器的储层神经网络优化过程中的挑战，成功地找到了储层的最优超参数，同时证实了储层的最优性能发生在导电通道的形成边缘，而且还发现这种系统可以模拟尖峰神经元的膜电位行为，具有将尖峰和连续信号互相转换的潜力 。

    

    基于忆阻器的储层神经网络受到纳米线网络的启发，具有类脑动力学特性，并在动力学相变时表现出最优性能。然而，这种网络只有有限数量的电极来调节系统动态，与神经形态硬件通过随机存取内存提供的全局可控性不同。本文提出了学会学习的框架，以解决优化过程中的挑战。在这个框架下，我们成功地找到了储层的最优超参数。此研究结果与先前的研究一致，表明储层的最优性能发生在导电通道的形成边缘。此外，本研究还发现，这种系统可以模拟尖峰神经元的膜电位行为，可能作为尖峰和连续信号之间的接口。

    Memristive reservoirs draw inspiration from a novel class of neuromorphic hardware known as nanowire networks. These systems display emergent brain-like dynamics, with optimal performance demonstrated at dynamical phase transitions. In these networks, a limited number of electrodes are available to modulate system dynamics, in contrast to the global controllability offered by neuromorphic hardware through random access memories. We demonstrate that the learn-to-learn framework can effectively address this challenge in the context of optimization. Using the framework, we successfully identify the optimal hyperparameters for the reservoir. This finding aligns with previous research, which suggests that the optimal performance of a memristive reservoir occurs at the `edge of formation' of a conductive pathway. Furthermore, our results show that these systems can mimic membrane potential behavior observed in spiking neurons, and may serve as an interface between spike-based and continuous 
    
[^27]: 利用大型语言模型鉴别和提取罕见病表型

    Identifying and Extracting Rare Disease Phenotypes with Large Language Models. (arXiv:2306.12656v1 [cs.CL])

    [http://arxiv.org/abs/2306.12656](http://arxiv.org/abs/2306.12656)

    研究提出了一个通过大型语言模型自动提取罕见病表型的方法，使用零样本或少样本的提示学习技术，无需对大量语料进行注释。

    

    罕见病（RDs）在全球影响着3亿人，正确的表型分类对诊断和治疗至关重要。然而，RD表型通常嵌入在非结构化文本中，手动提取过程耗时。自然语言处理（NLP）模型可以执行命名实体识别（NER）以自动提取，但其训练中需要大量带注释的语料库。 近期，提示学习作为可以导致更可推广结果的NLP范例出现了，而不需要任何（零样本）或少量标记样本（少样本）。尽管对ChatGPT越来越感兴趣，这是一种革命性的大型语言模型，能够遵循复杂的人类提示并生成高质量的回复，但尚未有人在零样本和少样本设置中研究其在RD NER性能。为此，我们设计了新型提示以提取RD表型，并据我们所知是首次建立基准。

    Rare diseases (RDs) are collectively common and affect 300 million people worldwide. Accurate phenotyping is critical for informing diagnosis and treatment, but RD phenotypes are often embedded in unstructured text and time-consuming to extract manually. While natural language processing (NLP) models can perform named entity recognition (NER) to automate extraction, a major bottleneck is the development of a large, annotated corpus for model training. Recently, prompt learning emerged as an NLP paradigm that can lead to more generalizable results without any (zero-shot) or few labeled samples (few-shot). Despite growing interest in ChatGPT, a revolutionary large language model capable of following complex human prompts and generating high-quality responses, none have studied its NER performance for RDs in the zero- and few-shot settings. To this end, we engineered novel prompts aimed at extracting RD phenotypes and, to the best of our knowledge, are the first the establish a benchmark 
    
[^28]: 在高保真模拟开放世界中考虑创新的多智能体规划

    Novelty Accommodating Multi-Agent Planning in High Fidelity Simulated Open World. (arXiv:2306.12654v1 [cs.AI])

    [http://arxiv.org/abs/2306.12654](http://arxiv.org/abs/2306.12654)

    本文介绍了一个能够检测、描述和适应现实环境中创新的领域独立AI代理在高保真模拟开放世界中成功应用的研究。在处理新颖环境干扰的多智能体规划任务中，该代理性能明显优于现有的基线。

    

    自主智能体在现实环境中行动时经常需要处理未知的创新干扰其计划执行。创新是一种意外的现象，可以改变环境的核心特征、组成和动态。创新在任何足够复杂的环境中的任何时间都可能发生，而没有任何事先通知或解释。先前的研究表明，创新对智能体的性能产生灾难性影响。智能体通过世界的内在模型进行推理，以理解其环境的复杂性，并成功执行其计划。创新通常使得他们的内部模型不准确并且生成的计划不再适用。创新特别普遍在现实世界中，在其中领域特定甚至是预测的创新特定方法被用于减轻创新的影响。在这项工作中，我们展示了一个设计用于在现实环境中检测，描绘和适应创新的领域独立AI代理可以成功地将其技能转移到高保真的模拟开放世界中。我们的代理在涉及处理新颖环境干扰的多智能体规划任务中显示出了显着的改进，而且比现有的基线表现得更好。

    Autonomous agents acting in real-world environments often need to reason with unknown novelties interfering with their plan execution. Novelty is an unexpected phenomenon that can alter the core characteristics, composition, and dynamics of the environment. Novelty can occur at any time in any sufficiently complex environment without any prior notice or explanation. Previous studies show that novelty has catastrophic impact on agent performance. Intelligent agents reason with an internal model of the world to understand the intricacies of their environment and to successfully execute their plans. The introduction of novelty into the environment usually renders their internal model inaccurate and the generated plans no longer applicable. Novelty is particularly prevalent in the real world where domain-specific and even predicted novelty-specific approaches are used to mitigate the novelty's impact. In this work, we demonstrate that a domain-independent AI agent designed to detect, chara
    
[^29]: 利用生成AI发现代码中的行异常（FLAG）

    FLAG: Finding Line Anomalies (in code) with Generative AI. (arXiv:2306.12643v1 [cs.CR])

    [http://arxiv.org/abs/2306.12643](http://arxiv.org/abs/2306.12643)

    FLAG是一种利用生成AI发现代码中行异常的语言无关方法，能够处理不完整（甚至非编译）的代码，有助于减少设计人员的检查搜索空间。

    

    代码中存在安全性和功能性缺陷，识别和定位它们的过程困难且依赖于人力。在这项工作中，我们提出了一种新的方法（FLAG）来辅助人类调试者。 FLAG基于生成AI的词汇能力，特别是大型语言模型（LLM）。我们输入代码文件，然后提取并重新生成该文件中的每一行以进行自我比较。通过将原始代码与LLM生成的替代方案进行比较，我们可以将显着差异标记为异常以进行进一步检查，同时距离注释和LLM置信度等特征也有助于该分类。这减少了设计人员的检查搜索空间。与此领域中的其他自动化方法不同，FLAG是语言无关的，可以处理不完整（甚至非编译）的代码，并且不需要创建安全属性、功能测试或规则定义。在这项工作中，我们探讨了帮助LLM进行分类的特征。

    Code contains security and functional bugs. The process of identifying and localizing them is difficult and relies on human labor. In this work, we present a novel approach (FLAG) to assist human debuggers. FLAG is based on the lexical capabilities of generative AI, specifically, Large Language Models (LLMs). Here, we input a code file then extract and regenerate each line within that file for self-comparison. By comparing the original code with an LLM-generated alternative, we can flag notable differences as anomalies for further inspection, with features such as distance from comments and LLM confidence also aiding this classification. This reduces the inspection search space for the designer. Unlike other automated approaches in this area, FLAG is language-agnostic, can work on incomplete (and even non-compiling) code and requires no creation of security properties, functional tests or definition of rules. In this work, we explore the features that help LLMs in this classification a
    
[^30]: 针对异常检测的目标塌缩正则化自编码器：中心的黑洞

    Targeted collapse regularized autoencoder for anomaly detection: black hole at the center. (arXiv:2306.12627v1 [cs.LG])

    [http://arxiv.org/abs/2306.12627](http://arxiv.org/abs/2306.12627)

    本文提出一种在自编码器的损失函数中添加一个轻量级的约束项，用于解决传统自编码器在异常检测中的不足，并取得了良好的表现。

    

    自编码器已广泛用于最近的异常检测技术开发中。它们的应用前提是在正常训练数据上训练自编码器后，异常输入将表现出显著的重构误差。因此，这使得正常和异常样本之间有了明显的区别。然而，在实践中观察到，自编码器可以一定程度上泛化到正常类之外，并在一些异常样本上实现较小的重构误差。为了改善性能，各种技术提出了其他组件和更复杂的训练程序。在这项工作中，我们提出了一个非常简单的替代方法：不是添加神经网络组件、涉及计算和繁琐的训练，而是通过在潜在空间中调节表示的范数，用一个计算简单的项来补充重构损失。我们方法的简单性最小化了复杂性。

    Autoencoders have been extensively used in the development of recent anomaly detection techniques. The premise of their application is based on the notion that after training the autoencoder on normal training data, anomalous inputs will exhibit a significant reconstruction error. Consequently, this enables a clear differentiation between normal and anomalous samples. In practice, however, it is observed that autoencoders can generalize beyond the normal class and achieve a small reconstruction error on some of the anomalous samples. To improve the performance, various techniques propose additional components and more sophisticated training procedures. In this work, we propose a remarkably straightforward alternative: instead of adding neural network components, involved computations, and cumbersome training, we complement the reconstruction loss with a computationally light term that regulates the norm of representations in the latent space. The simplicity of our approach minimizes th
    
[^31]: SEAL: 多机器人系统中的同时探索和定位

    SEAL: Simultaneous Exploration and Localization in Multi-Robot Systems. (arXiv:2306.12623v1 [cs.RO])

    [http://arxiv.org/abs/2306.12623](http://arxiv.org/abs/2306.12623)

    本论文提出了一种基于高斯过程的信息融合和通信图优化相结合的SEAL方法，用于实现多机器人系统中的高精度定位和探索，以取得了比最前沿方法更好的探索和定位性能。

    

    准确定位对于多机器人探索策略至关重要；嘈杂或不一致的定位会导致未能达成探索目标。本论文旨在实现高精度定位和当代探索地图置信度，而无需全局定位信息。本文提出了一种新颖的同时探索和定位(SEAL)方法，它利用基于高斯过程（GP）的信息融合进行最大化探索，同时执行通信图优化进行相对定位。这两个交叉依赖目标通过Rao-Blackwellization技术进行整合。使用分布式线性化凸壳优化来选择下一个最佳的未探索区域进行分布式探索。在大量ROS-Gazebo模拟中，SEAL在探索和定位性能上优于最前沿的方法，说明该方法在实际应用中是可行的。

    The availability of accurate localization is critical for multi-robot exploration strategies; noisy or inconsistent localization causes failure in meeting exploration objectives. We aim to achieve high localization accuracy with contemporary exploration map belief and vice versa without needing global localization information. This paper proposes a novel simultaneous exploration and localization (SEAL) approach, which uses Gaussian Processes (GP)-based information fusion for maximum exploration while performing communication graph optimization for relative localization. Both these cross-dependent objectives were integrated through the Rao-Blackwellization technique. Distributed linearized convex hull optimization is used to select the next-best unexplored region for distributed exploration. SEAL outperformed cutting-edge methods on exploration and localization performance in extensive ROS-Gazebo simulations, illustrating the practicality of the approach in real-world applications.
    
[^32]: 基于标签生成的增量分类学习方法

    Class-Incremental Learning based on Label Generation. (arXiv:2306.12619v1 [cs.CL])

    [http://arxiv.org/abs/2306.12619](http://arxiv.org/abs/2306.12619)

    本文提出了一种基于标签生成方法的增量分类学习（CIL）方法（VAG），大幅减少了灾难性遗忘（CF），并更好地保留了预训练模型的可推广表示。

    

    尽管预训练语言模型取得了巨大成功，但对于类别增量学习（CIL）设置，由于灾难性遗忘（CF），使用这些模型进行连续学习仍然是一个挑战。本文发现，如果将CIL定式为一个连续的标签生成问题，则可以大幅减少CF并更好地保留预训练模型的可推广表示。因此，我们提出了一种新的CIL方法（VAG），该方法还利用了词汇表的稀疏性以便于生成，并使用标签语义创建伪重播样本。实验结果表明，VAG的性能比基线大幅优越。

    Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.
    
[^33]: “构建可管控的AI系统：技术挑战和政策机遇”

    Towards Regulatable AI Systems: Technical Gaps and Policy Opportunities. (arXiv:2306.12609v1 [cs.AI])

    [http://arxiv.org/abs/2306.12609](http://arxiv.org/abs/2306.12609)

    该论文探讨了如何确保AI系统遵循规定，提出了当前和潜在可能的技术实现，以及需要跨学科方法解决的问题。

    

    针对如何规范AI系统的问题，越来越多的关注被给予。在管控机构努力确定要封装到法规中的价值观时，本文考虑了其中的技术问题：AI专家们能够审查AI系统是否遵守监管要求的程度是多少？我们通过两个公共部门的采购清单对这个问题进行了调查，并定义了三个方面：目前有哪些方面是我们可以做的；利用AI技术创新我们能做到哪些方面； 哪些要求需要更多跨学科的方法。

    There is increasing attention being given to how to regulate AI systems. As governing bodies grapple with what values to encapsulate into regulation, we consider the technical half of the question: To what extent can AI experts vet an AI system for adherence to regulatory requirements? We investigate this question through two public sector procurement checklists, identifying what we can do now, what we should be able to do with technical innovation in AI, and what requirements necessitate a more interdisciplinary approach.
    
[^34]: 多分布稠密信息检索的资源和评估

    Resources and Evaluations for Multi-Distribution Dense Information Retrieval. (arXiv:2306.12601v1 [cs.IR])

    [http://arxiv.org/abs/2306.12601](http://arxiv.org/abs/2306.12601)

    本文提出了一个新问题——多分布信息检索，并通过三个基准数据集展示了简单方法的有效性，可防止已知领域消耗大部分检索预算，平均提高Recall @ 100点3.8+，最高可达8.0个点。

    

    我们引入并定义了多分布信息检索（IR）的新问题，即在给定查询的情况下，系统需要从多个集合中检索出段落，每个集合都来自不同的分布。其中一些集合和分布可能在训练时不可用。为了评估多分布检索的方法，我们从现有的单分布数据集设计了三个基准，分别是基于问题回答的数据集和两个基于实体匹配的数据集。我们提出了针对此任务的简单方法，该方法在域之间战略性地分配固定的检索预算（前k个段落），以防已知领域消耗大部分预算。我们展示我们的方法在数据集上导致了平均3.8+和高达8.0个Recall @ 100点的提高，并且在微调不同的基础检索模型时改进是一致的。我们的基准公开可用。

    We introduce and define the novel problem of multi-distribution information retrieval (IR) where given a query, systems need to retrieve passages from within multiple collections, each drawn from a different distribution. Some of these collections and distributions might not be available at training time. To evaluate methods for multi-distribution retrieval, we design three benchmarks for this task from existing single-distribution datasets, namely, a dataset based on question answering and two based on entity matching. We propose simple methods for this task which allocate the fixed retrieval budget (top-k passages) strategically across domains to prevent the known domains from consuming most of the budget. We show that our methods lead to an average of 3.8+ and up to 8.0 points improvements in Recall@100 across the datasets and that improvements are consistent when fine-tuning different base retrieval models. Our benchmarks are made publicly available.
    
[^35]: 用 NeuBAROCO 评估大型语言模型的三段论推理能力和人类偏见

    Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases. (arXiv:2306.12567v1 [cs.CL])

    [http://arxiv.org/abs/2306.12567](http://arxiv.org/abs/2306.12567)

    本研究使用名为 NeuBAROCO 的数据集，检验了当前大型语言模型在三段论推理中的表现。发现这些模型在处理信念偏见、转化错误和氛围效应等问题时表现欠佳。

    

    本文调查了当前的大型语言模型是否像人类一样在逻辑推理中存在偏见。具体而言，我们关注的是三段论推理，这是人们在推理认知科学中研究过的推理形式。为了方便我们的分析，我们介绍了一个名为 NeuBAROCO 的数据集，最初是为评估人类在三段论推理中的逻辑能力而设计的心理实验。该数据集包括英文和日文的三段论推理。我们研究了人类三段论推理中观察到的三种偏见类型：信念偏见、转化错误和氛围效应。我们的研究结果表明，当前的大型语言模型在涉及这三种类型的问题时更容易出现问题。

    This paper investigates whether current large language models exhibit biases in logical reasoning, similar to humans. Specifically, we focus on syllogistic reasoning, a well-studied form of inference in the cognitive science of human deduction. To facilitate our analysis, we introduce a dataset called NeuBAROCO, originally designed for psychological experiments that assess human logical abilities in syllogistic reasoning. The dataset consists of syllogistic inferences in both English and Japanese. We examine three types of biases observed in human syllogistic reasoning: belief biases, conversion errors, and atmosphere effects. Our findings demonstrate that current large language models struggle more with problems involving these three types of biases.
    
[^36]: 通过指令预测提高长期模仿的表现

    Improving Long-Horizon Imitation Through Instruction Prediction. (arXiv:2306.12554v1 [cs.LG])

    [http://arxiv.org/abs/2306.12554](http://arxiv.org/abs/2306.12554)

    本文探讨了基于语言的指令预测损失的辅助监督方式，展示了在演示数量受限的情况下，指令建模在复杂推理任务中提高了表现。

    

    复杂的长期计划及其组合性质对于基于学习的智能体来说是巨大的挑战。在低数据环境中，过度拟合抑制了泛化，并且累积误差损害了准确性。本文中，我们探讨了一种通常未使用的辅助监督方式：语言。受最近基于Transformer模型的进展的启发，我们使用指令预测损失来训练代理，以鼓励学习在高层次上操作的具有时间扩展的表示。具体而言，我们证明了在BabyAI和Crafter基准测试中，指令建模显著提高了规划环境中受限的演示数量时的表现。在进一步的分析中，我们发现指令建模对需要复杂推理的任务最为重要，而在需要简单计划的环境中则可以理解地获得更小的收益。更多细节和代码可在[https://github.com/facebookresearch/long-term-fairness]中找到。

    Complex, long-horizon planning and its combinatorial nature pose steep challenges for learning-based agents. Difficulties in such settings are exacerbated in low data regimes where over-fitting stifles generalization and compounding errors hurt accuracy. In this work, we explore the use of an often unused source of auxiliary supervision: language. Inspired by recent advances in transformer-based models, we train agents with an instruction prediction loss that encourages learning temporally extended representations that operate at a high level of abstraction. Concretely, we demonstrate that instruction modeling significantly improves performance in planning environments when training with a limited number of demonstrations on the BabyAI and Crafter benchmarks. In further analysis we find that instruction modeling is most important for tasks that require complex reasoning, while understandably offering smaller gains in environments that require simple plans. More details and code can be 
    
[^37]: 随机凸优化的内存和查询权衡

    Memory-Query Tradeoffs for Randomized Convex Optimization. (arXiv:2306.12534v1 [cs.DS])

    [http://arxiv.org/abs/2306.12534](http://arxiv.org/abs/2306.12534)

    随机凸优化需要在内存和查询之间权衡，使用 $\tilde{O}(d^2)$ 比特内存和 $\tilde{O}(d)$ 次查询的割平面方法是最优的。

    

    我们证明了任何随机一阶算法，用于在单位球上最小化一个 $d$ 维、$1$-Lipschitz 凸函数，必须使用 $\Omega(d^{2-\delta})$ 比特的内存或者进行 $\Omega(d^{1+\delta/6-o(1)})$ 次查询，其中 $\delta\in (0,1)$ 是任意常数，精度 $\epsilon$ 在 $d$ 中是准多项式小的。我们的结果意味着，使用 $\tilde{O}(d^2)$ 比特内存和 $\tilde{O}(d)$ 次查询的割平面方法，在随机一阶算法中是帕累托最优，而对于凸优化，需要二次内存才能实现最佳查询复杂度。

    We show that any randomized first-order algorithm which minimizes a $d$-dimensional, $1$-Lipschitz convex function over the unit ball must either use $\Omega(d^{2-\delta})$ bits of memory or make $\Omega(d^{1+\delta/6-o(1)})$ queries, for any constant $\delta\in (0,1)$ and when the precision $\epsilon$ is quasipolynomially small in $d$. Our result implies that cutting plane methods, which use $\tilde{O}(d^2)$ bits of memory and $\tilde{O}(d)$ queries, are Pareto-optimal among randomized first-order algorithms, and quadratic memory is required to achieve optimal query complexity for convex optimization.
    
[^38]: 基于LIME的探索方法研究黑匣子的性能不佳区域：以败血症检测为例

    Investigating Poor Performance Regions of Black Boxes: LIME-based Exploration in Sepsis Detection. (arXiv:2306.12507v1 [cs.LG])

    [http://arxiv.org/abs/2306.12507](http://arxiv.org/abs/2306.12507)

    本文提出了一种利用LIME方法探索黑盒分类模型不佳性能区域的方法，并应用于高风险的败血症检测中。通过分析错误分类实例，确定造成模型性能差的重要特征，并识别出分类器性能不佳的区域，并计算出其错误率，这对于实现谨慎的决策具有重要意义，同时增强了机器学习模型在临床实践中的可解释性，降低关键应用场景下的风险。

    

    解释机器学习模型仍然是一个挑战，阻碍了它们在临床环境中的应用。本文提出利用局部可解释的模型不可知解释（LIME）来提供解释性地描述高风险败血症检测黑盒分类模型。通过分析被错误分类的实例，识别贡献于次优性能的重要特征。分析揭示出分类器性能不佳的区域，从而计算出这些区域内的错误率，这对于在败血症检测等关键应用场景下进行谨慎决策至关重要。本文使用eICU数据集展示了所提出方法的有效性，能够识别和可视化分类器性能不佳的区域。通过增强可解释性，我们的方法促进了机器学习模型在临床实践中的应用，实现了明智的决策，并在关键时刻降低风险。

    Interpreting machine learning models remains a challenge, hindering their adoption in clinical settings. This paper proposes leveraging Local Interpretable Model-Agnostic Explanations (LIME) to provide interpretable descriptions of black box classification models in high-stakes sepsis detection. By analyzing misclassified instances, significant features contributing to suboptimal performance are identified. The analysis reveals regions where the classifier performs poorly, allowing the calculation of error rates within these regions. This knowledge is crucial for cautious decision-making in sepsis detection and other critical applications. The proposed approach is demonstrated using the eICU dataset, effectively identifying and visualizing regions where the classifier underperforms. By enhancing interpretability, our method promotes the adoption of machine learning models in clinical practice, empowering informed decision-making and mitigating risks in critical scenarios.
    
[^39]: 多级区域COVID-19深度动态流行病学建模和预测

    Deep Dynamic Epidemiological Modelling for COVID-19 Forecasting in Multi-level Districts. (arXiv:2306.12457v1 [cs.LG])

    [http://arxiv.org/abs/2306.12457](http://arxiv.org/abs/2306.12457)

    该文提出一种深度动态流行病学的模型，将流行病学方程和深度学习相结合，以实现对COVID-19在多级区域的准确预测，同时保证模型可视化机制

    

    目的：COVID-19已经全球蔓延并对整个世界产生了巨大的影响。模拟COVID-19传播状况对了解当前状态和制定干预措施至关重要。基于SEIR模型的流行病学方程模拟疾病的发展。传统的参数估计方法不能精确地拟合真实世界的数据，可能由于不同的情况，如社交距离政策和干预策略等。此外，基于学习的模型可以实现出色的拟合性能，但无法可视化机制。方法：因此，我们提出了一种深度动态流行病学（DDE）方法，将流行病学方程和深度学习优势相结合，以获得高精度和可视化。 DDE包含深度网络以适应效应函数，以基于神经ODE方法解决变量方程来模拟不断变化的情况，确保拟合性能

    Objective: COVID-19 has spread worldwide and made a huge influence across the world. Modeling the infectious spread situation of COVID-19 is essential to understand the current condition and to formulate intervention measurements. Epidemiological equations based on the SEIR model simulate disease development. The traditional parameter estimation method to solve SEIR equations could not precisely fit real-world data due to different situations, such as social distancing policies and intervention strategies. Additionally, learning-based models achieve outstanding fitting performance, but cannot visualize mechanisms. Methods: Thus, we propose a deep dynamic epidemiological (DDE) method that combines epidemiological equations and deep-learning advantages to obtain high accuracy and visualization. The DDE contains deep networks to fit the effect function to simulate the ever-changing situations based on the neural ODE method in solving variants' equations, ensuring the fitting performance o
    
[^40]: 机器设计的极限挑战：利用人工智能实现自动CPU设计

    Pushing the Limits of Machine Design: Automated CPU Design with AI. (arXiv:2306.12456v1 [cs.AI])

    [http://arxiv.org/abs/2306.12456](http://arxiv.org/abs/2306.12456)

    本研究运用新的人工智能方法，自动设计了一个中央处理器(CPU)，这是人类设计过的最复杂的装置之一，从而探索了机器设计的边界。

    

    设计活动——构建一个满足给定目标和限制条件的工件描述——区别于人类和传统机器，赋予机器人类水平或更高的设计能力一直是一个长期追求的目标。虽然机器已经展示了运用先进的人工智能技术设计新材料、蛋白质和计算机程序的能力，但这些工件的设计空间相对较小，因此，“机器是否能像人类一样设计？”成为了一个开放性问题。为了拓展机器设计的边界，本研究提出了一种新的人工智能方法，利用该方法实现了自动设计一个中央处理器(CPU)，即计算机的大脑，这是人类设计过的最复杂的装置之一。该方法从只有外部输入输出观测的情况下生成CPU设计的电路逻辑，该逻辑用二元推演图(BSD)表示。

    Design activity -- constructing an artifact description satisfying given goals and constraints -- distinguishes humanity from other animals and traditional machines, and endowing machines with design abilities at the human level or beyond has been a long-term pursuit. Though machines have already demonstrated their abilities in designing new materials, proteins, and computer programs with advanced artificial intelligence (AI) techniques, the search space for designing such objects is relatively small, and thus, "Can machines design like humans?" remains an open question. To explore the boundary of machine design, here we present a new AI approach to automatically design a central processing unit (CPU), the brain of a computer, and one of the world's most intricate devices humanity have ever designed. This approach generates the circuit logic, which is represented by a graph structure called Binary Speculation Diagram (BSD), of the CPU design from only external input-output observations
    
[^41]: 学习条件工具变量表示用于因果效应估计

    Learning Conditional Instrumental Variable Representation for Causal Effect Estimation. (arXiv:2306.12453v1 [cs.LG])

    [http://arxiv.org/abs/2306.12453](http://arxiv.org/abs/2306.12453)

    本文提出了一种名为 DVAE.CIV 的方法，通过分离表示学习，从带有潜在混淆因素的数据中学习和分解条件 IV 和其条件集的表示，用于因果效应估计。

    

    因果推断中的一个基本挑战是从观察数据中估计治疗对感兴趣结果的因果效应。然而，因果效应估计经常受到混淆偏差的影响，这是由于未测量的混淆因素影响了治疗和结果。 工具变量 (IV) 方法是消除潜在混淆因素的混淆偏差的有效方法。然而，现有的基于 IV 的估计器需要有一个被提名的 IV，而对于条件 IV (CIV) 进行因果效应估计需要其相应的条件集。 基于这种限制，我们提出了一种名为 DVAE.CIV 的新方法，通过利用分离表示学习的优势，从具有混淆因素的数据中学习并分解 CIV 的表示和其条件集的表示，用于因果效应估计。在合成和实际数据集上的大量实验结果证明了所提出方法的有效性，包括准确性和对混淆偏差的鲁棒性。

    One of the fundamental challenges in causal inference is to estimate the causal effect of a treatment on its outcome of interest from observational data. However, causal effect estimation often suffers from the impacts of confounding bias caused by unmeasured confounders that affect both the treatment and the outcome. The instrumental variable (IV) approach is a powerful way to eliminate the confounding bias from latent confounders. However, the existing IV-based estimators require a nominated IV, and for a conditional IV (CIV) the corresponding conditioning set too, for causal effect estimation. This limits the application of IV-based estimators. In this paper, by leveraging the advantage of disentangled representation learning, we propose a novel method, named DVAE.CIV, for learning and disentangling the representations of CIV and the representations of its conditioning set for causal effect estimations from data with latent confounders. Extensive experimental results on both synthet
    
[^42]: 自动化说话人验证在阿尔茨海默病临床试验中的表现受哪些因素影响？

    Factors Affecting the Performance of Automated Speaker Verification in Alzheimer's Disease Clinical Trials. (arXiv:2306.12444v1 [eess.AS])

    [http://arxiv.org/abs/2306.12444](http://arxiv.org/abs/2306.12444)

    本文研究了自动化说话人验证技术在阿尔茨海默症临床试验中应用的可行性，人口统计特征、音频质量标准和AD的严重程度对ASV性能会产生影响。

    

    在临床试验中检测重复的患者参与是一个重大的挑战，因为重复的患者可能会破坏试验结果的可信度和准确性，导致重大的健康和财务风险。开发准确的自动化说话人验证（ASV）模型对于验证已注册个体的身份并去除重复项来说至关重要，但是数据的大小和质量会影响ASV的表现。然而，迄今为止对于影响临床环境中ASV能力的因素的调查有限。在本文中，我们通过对从多个说话任务中获取的659个具有不同AD感染程度的参与者的语音记录数据集进行分析，探讨参与者的人口统计特征、音频质量标准和AD的严重程度对ASV性能的影响。我们的结果表明，ASV的性能：1）在男性说话者上略优于女性说话者；2）

    Detecting duplicate patient participation in clinical trials is a major challenge because repeated patients can undermine the credibility and accuracy of the trial's findings and result in significant health and financial risks. Developing accurate automated speaker verification (ASV) models is crucial to verify the identity of enrolled individuals and remove duplicates, but the size and quality of data influence ASV performance. However, there has been limited investigation into the factors that can affect ASV capabilities in clinical environments. In this paper, we bridge the gap by conducting analysis of how participant demographic characteristics, audio quality criteria, and severity level of Alzheimer's disease (AD) impact the performance of ASV utilizing a dataset of speech recordings from 659 participants with varying levels of AD, obtained through multiple speech tasks. Our results indicate that ASV performance: 1) is slightly better on male speakers than on female speakers; 2)
    
[^43]: 基于词级别关系图的知识蒸馏

    Knowledge Distillation via Token-level Relationship Graph. (arXiv:2306.12442v1 [cs.LG])

    [http://arxiv.org/abs/2306.12442](http://arxiv.org/abs/2306.12442)

    本文提出了一种新方法，基于词级别关系图(TRG)，用于提高知识蒸馏的性能。通过利用TRG，学生模型可以模拟教师模型中更高级别的语义信息。同时，还引入了一种上下文损失以进一步增强学习过程。

    

    知识蒸馏是将预先训练好的教师模型中的知识传递给学生模型的一种有效技术。然而，知识传递的真正潜力还没有被充分挖掘。现有的方法主要集中在蒸馏单个信息或实例级别的关系，忽略了嵌入在词级别关系中的有价值的信息，这可能会受到长尾效应的影响。为了解决上述限制，我们提出了一种称为基于词级别关系图(TRG)的知识蒸馏的新方法，利用词级别的关系知识来提高知识蒸馏的性能。通过使用TRG，学生模型可以有效地模拟教师模型中更高级别的语义信息，从而提高了蒸馏结果。为了进一步增强学习过程，我们引入了一种称为上下文损失的词级别上下文损失，鼓励学生模型捕捉

    Knowledge distillation is a powerful technique for transferring knowledge from a pre-trained teacher model to a student model. However, the true potential of knowledge transfer has not been fully explored. Existing approaches primarily focus on distilling individual information or instance-level relationships, overlooking the valuable information embedded in token-level relationships, which may be particularly affected by the long-tail effects. To address the above limitations, we propose a novel method called Knowledge Distillation with Token-level Relationship Graph (TRG) that leverages the token-wise relational knowledge to enhance the performance of knowledge distillation. By employing TRG, the student model can effectively emulate higher-level semantic information from the teacher model, resulting in improved distillation results. To further enhance the learning process, we introduce a token-wise contextual loss called contextual loss, which encourages the student model to capture
    
[^44]: AdCraft：一种用于搜索引擎营销优化的高级强化学习基准环境

    AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization. (arXiv:2306.11971v1 [cs.LG])

    [http://arxiv.org/abs/2306.11971](http://arxiv.org/abs/2306.11971)

    AdCraft是一种高级强化学习基准环境，用于模拟出价和预算变化的搜索引擎营销(SEM)活动，可用于评估和提高SEM出价和预算管理相关的RL算法的鲁棒性。

    

    本文介绍了一种新的强化学习基准环境—— AdCraft，其具有随机和非静态特性。该环境模拟了搜索引擎营销中出价和预算的动态变化。SEM是一种利用付费广告来增加网站在搜索引擎结果页面上的可见性的数字营销技术。SEM广告活动的表现取决于多个因素，包括关键字选择、广告设计、出价管理、预算调整和表现监控。深度强化学习最近被认为是一种优化SEM广告投放活动的潜在策略，但需要大量数据，在实践中可能成本高昂或不可行。我们的可定制环境使从业者能够评估和提高与SEM出价和预算管理相关的RL算法的鲁棒性，而无需付出这些成本。通过在AdCraft环境下进行一系列实验，

    We introduce \env{}, a novel benchmark environment for the Reinforcement Learning (RL) community distinguished by its stochastic and non-stationary properties. The environment simulates bidding and budgeting dynamics within Search Engine Marketing (SEM), a digital marketing technique utilizing paid advertising to enhance the visibility of websites on search engine results pages (SERPs). The performance of SEM advertisement campaigns depends on several factors, including keyword selection, ad design, bid management, budget adjustments, and performance monitoring. Deep RL recently emerged as a potential strategy to optimize campaign profitability within the complex and dynamic landscape of SEM but it requires substantial data, which may be costly or infeasible to acquire in practice. Our customizable environment enables practitioners to assess and enhance the robustness of RL algorithms pertinent to SEM bid and budget management without such costs. Through a series of experiments within 
    
[^45]: 采用强化学习技术增强变分量子状态对角化

    Enhancing variational quantum state diagonalization using reinforcement learning techniques. (arXiv:2306.11086v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2306.11086](http://arxiv.org/abs/2306.11086)

    本研究采用强化学习技术，通过新的编码方法来优化量子状态对角化所需的电路深度，从而提高其在近期量子硬件上的应用性能。

    

    变分量子算法的发展对于 NISQ 计算机的应用至关重要。这种算法需要短的量子电路，这种电路更易于在近期硬件上实现，也已经开发出了许多这样的方法。其中一个特别有趣的算法是所谓的变分对角化方法，它是一种重要的算法子例程，可以直接用于处理以量子状态编码的数据。特别地，它可以用于分辨量子态的特征，例如系统的纠缠性质，或者在量子机器学习算法中使用。在本研究中，我们利用强化学习解决在量子状态对角化任务中所需电路非常浅的设计问题。为了实现这一点，我们利用一种新的编码方法，可以利用强化学习方法解决电路深度优化问题。我们证明，我们的方法有可能显著减少所需的电路深度，从而使其更适用于近期量子硬件。

    The development of variational quantum algorithms is crucial for the application of NISQ computers. Such algorithms require short quantum circuits, which are more amenable to implementation on near-term hardware, and many such methods have been developed. One of particular interest is the so-called the variational diagonalization method, which constitutes an important algorithmic subroutine, and it can be used directly for working with data encoded in quantum states. In particular, it can be applied to discern the features of quantum states, such as entanglement properties of a system, or in quantum machine learning algorithms. In this work, we tackle the problem of designing a very shallow quantum circuit, required in the quantum state diagonalization task, by utilizing reinforcement learning. To achieve this, we utilize a novel encoding method that can be used to tackle the problem of circuit depth optimization using a reinforcement learning approach. We demonstrate that our approach
    
[^46]: 基于HRNet的康复监测系统

    A HRNet-based Rehabilitation Monitoring System. (arXiv:2306.10756v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.10756](http://arxiv.org/abs/2306.10756)

    该论文介绍了一种基于HRNet的康复监测系统，旨在通过智能手机进行康复训练的管理。患者可以通过系统进行应用程序来进行训练，而治疗师可以通过服务器端进行进度的监测。

    

    康复治疗有助于治愈轻微的体育和职业伤害。传统的康复过程中，治疗师会指定某些动作供患者在医院访问之间执行，这将依赖于患者正确地记住动作和执行计划。不幸的是，许多患者会忘记执行动作或无法详细回想动作。因此，康复治疗受到阻碍，或者在最坏的情况下，患者可能会因执行不正确的动作而遭受额外的伤害。为了解决这些问题，我们提出了一种基于HRNet的康复监测系统，它可以通过患者的智能手机提醒患者何时执行动作并展示动作供患者跟随。此外，它还帮助治疗师监测患者的康复进展。我们的系统由一款iOS应用程序和几个服务器组件组成。该应用程序负责显示...

    The rehabilitation treatment helps to heal minor sports and occupational injuries. In a traditional rehabilitation process, a therapist will assign certain actions to a patient to perform in between hospital visits, and it will rely on the patient to remember actions correctly and the schedule to perform them. Unfortunately, many patients forget to perform actions or fail to recall actions in detail. As a consequence, the rehabilitation treatment is hampered or, in the worst case, the patient may suffer from additional injury caused by performing incorrect actions. To resolve these issues, we propose a HRNet-based rehabilitation monitoring system, which can remind a patient when to perform the actions and display the actions for the patient to follow via the patient's smartphone. In addition, it helps the therapist to monitor the progress of the rehabilitation for the patient. Our system consists of an iOS app and several components at the server side. The app is in charge of displayin
    
[^47]: Enlighten-anything: 当分段模型遇见低光图像增强

    Enlighten-anything:When Segment Anything Model Meets Low-light Image Enhancement. (arXiv:2306.10286v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.10286](http://arxiv.org/abs/2306.10286)

    本文提出了Enlighten-anything，在低光图像增强中将分段模型与SAM融合，实现了良好视觉感知的融合图像。

    

    图像恢复是一项低级别视觉任务，大多数CNN方法都是作为黑盒子设计的，缺乏透明度和固有美学。许多无监督方法忽略了低光场景中可见信息的退化，这会严重影响补充信息的聚合，并使融合算法无法在极端情况下产生令人满意的融合结果。本文提出了Enlighten-anything，能够将SAM分段的语义意图与低光图像增强相结合，获得具有良好视觉感知的融合图像。无监督学习的泛化能力得到了极大提高，对LOL数据集的实验表明，我们的方法在PSNR上比基线提高了3dB，在SSIM上提高了8。 SAM的零样本学习为无监督低光增强提供了有力的帮助。Enlighten-anything的源代码可以从 https://github.com/zhangbaijin/enlighten-any 获得。

    Image restoration is a low-level visual task, and most CNN methods are designed as black boxes, lacking transparency and intrinsic aesthetics. Many unsupervised approaches ignore the degradation of visible information in low-light scenes, which will seriously affect the aggregation of complementary information and also make the fusion algorithm unable to produce satisfactory fusion results under extreme conditions. In this paper, we propose Enlighten-anything, which is able to enhance and fuse the semantic intent of SAM segmentation with low-light images to obtain fused images with good visual perception. The generalization ability of unsupervised learning is greatly improved, and experiments on LOL dataset are conducted to show that our method improves 3db in PSNR over baseline and 8 in SSIM. zero-shot learning of SAM introduces a powerful aid for unsupervised low-light enhancement. The source code of Enlighten-anything can be obtained from https://github.com/zhangbaijin/enlighten-any
    
[^48]: 揭秘 GPT 自我修复代码生成能力

    Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])

    [http://arxiv.org/abs/2306.09896](http://arxiv.org/abs/2306.09896)

    本文分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，自我修复在较短和较简单的任务中效果更好，仅在某些代码部分上应用自我修复可以非常有效，本文提出的引导修复方法在 APPS 数据集上获得性能提升。

    

    大型语言模型 (LLM) 在代码生成方面表现出色，但在挑战性编程任务上仍面临困难。自我修复——即模型调试并修复自己的代码——最近成为提高性能的一种流行方式。然而，关于自我修复如何有效地发挥作用的研究还非常有限。有人会想知道，当同一模型生成代码时，模型究竟能否提供准确的反馈。在本文中，我们分析了 GPT-3.5 和 GPT-4 在 APPS 数据集上执行自我修复的能力，这是一个由多种编码挑战组成的具有挑战性的数据集。我们首先建立了一种新的评估策略 pass@t，该策略衡量了任务通过率与从模型中抽样的总标记数，从而实现对仅基于抽样的方法的公平比较。通过这种评估策略，我们发现自我修复在 GPT 模型中的有效性严重取决于任务的质量和复杂性，并确定了影响自我修复表现的几个因素。具体而言，我们发现，在输入噪声较少且模型对初始输出不太自信的较短和较简单的任务中，自我修复效果更好。我们还表明，仅在某些代码部分上应用自我修复可以非常有效。此外，我们提出了一种新的引导修复方法，利用外部反馈来增强 GPT 模型的自我修复能力，在 APPS 数据集上获得性能提升。

    Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
    
[^49]: 虚假黎明：重新评估谷歌强化学习在芯片宏观布局中的应用

    The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])

    [http://arxiv.org/abs/2306.09633](http://arxiv.org/abs/2306.09633)

    谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。

    

    谷歌2021年在《自然》杂志上发表的有关使用强化学习设计芯片的论文，因为所声称的结果缺乏充分的文件记录和关键步骤的说明，引发争议并受到媒体的批评报道。 而两项独立的评估填补了空白，证明谷歌强化学习落后于人类设计师、落后于一种众所周知的算法（模拟退火），并且还落后于普遍可用的商业软件。交叉检查的数据表明，由于行为、分析和报告中的错误，该《自然》文章的完整性受到了严重的损害。

    Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
    
[^50]: 利用增强型大型语言模型（GPT-4）解释法律概念

    Explaining Legal Concepts with Augmented Large Language Models (GPT-4). (arXiv:2306.09525v1 [cs.CL])

    [http://arxiv.org/abs/2306.09525](http://arxiv.org/abs/2306.09525)

    本文评估了利用GPT-4增强解释法律术语性能的方法。与基准设置相比，使用增强的法律信息检索模块可以提高法律术语解释的质量，并消除模型的幻觉问题，从而为在法律领域使用大型语言模型开辟了新的途径。

    

    解释法律开放性术语的含义是法律专业人员的重要任务。先前法院案例中该术语的应用是解释其含义的重要来源。本文评估了GPT-4生成法律术语解释的准确性、清晰性和相关性的性能。我们将GPT-4被直接要求解释法律术语的基准设置的性能与增强方法进行了比较，在增强方法中，一个法律信息检索模块被用来为模型提供相关背景，即来自案例法的句子。我们发现，直接应用GPT-4产生的解释在表面上似乎非常高质量。然而，详细分析揭示了解释的实际准确性方面存在的限制。此外，我们发现增强性能可以提高质量，并似乎消除了幻觉问题，即模型发明不正确的陈述。这些发现为在法律领域中使用大型语言模型开辟了新的途径。

    Interpreting the meaning of legal open-textured terms is a key task of legal professionals. An important source for this interpretation is how the term was applied in previous court cases. In this paper, we evaluate the performance of GPT-4 in generating factually accurate, clear and relevant explanations of terms in legislation. We compare the performance of a baseline setup, where GPT-4 is directly asked to explain a legal term, to an augmented approach, where a legal information retrieval module is used to provide relevant context to the model, in the form of sentences from case law. We found that the direct application of GPT-4 yields explanations that appear to be of very high quality on their surface. However, detailed analysis uncovered limitations in terms of the factual accuracy of the explanations. Further, we found that the augmentation leads to improved quality, and appears to eliminate the issue of hallucination, where models invent incorrect statements. These findings ope
    
[^51]: 从零开始实现红队对抗语言模型的探索与建立

    Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])

    [http://arxiv.org/abs/2306.09442](http://arxiv.org/abs/2306.09442)

    本文提出了一种新的红队行动，通过从高层次、抽象的规范出发来考虑语言模型的行为，以探究模型的创新和贡献。

    

    部署大型语言模型（LLMs）可能会产生有害输出，例如有毒或不诚实陈述。先前的研究已经引入了工具以调查有害输出，以识别和减轻这些风险。虽然这是确保语言模型安全的有价值步骤，但这些方法通常依赖于现有的针对不希望的输出的分类器。这限制了它们在只有预先知道有害行为类型的情况下的应用。然而，这跳过了红队行动的核心挑战：开发模型可能展示的行为的上下文理解。此外，当这样的分类器已经存在时，红队行动的边际价值有限，因为分类器可以用于过滤训练数据或模型输出。本文考虑在假设对手从高级、抽象的不良行为规范出发的情况下进行红队行动。红队应该在精化/扩展此规范的同时对抗该模型。

    Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
    
[^52]: Hebbian学习网络中的主动推理

    Active Inference in Hebbian Learning Networks. (arXiv:2306.05053v1 [cs.NE])

    [http://arxiv.org/abs/2306.05053](http://arxiv.org/abs/2306.05053)

    本文研究了具有局部Hebbian可塑性的仿脑神经群体如何执行主动推理，通过两个不同的Hebbian神经元组成的网络来生成捕捉环境动态的生成模型，使用Mountain Car环境进行实验研究，结果表明所提出的Hebbian AIF方法优于使用Q-learning，同时不需要回放缓冲区。

    

    本研究探讨了具有局部Hebbian可塑性的仿脑神经群体如何执行主动推理（AIF），以控制动态代理。通过由两个不同的Hebbian神经元组成的网络进行学习，生成了一个捕捉环境动态的生成模型：一个后验网络，用于在给定观测的情况下推断潜在状态，以及一个状态转移网络，用于在给定当前状态-动作对的情况下预测下一个期望的潜在状态。使用OpenAI gym套件中的Mountain Car环境进行实验研究，以研究各种Hebbian网络参数对任务性能的影响。结果表明，所提出的Hebbian AIF方法优于使用Q-learning，同时不需要回放缓冲区，如典型的强化学习系统。这些结果促使我们进一步探讨Hebbian学习，以设计能够学习环境动态而不需要重新访问过去缓冲区的AIF网络。

    This work studies how brain-inspired neural ensembles equipped with local Hebbian plasticity can perform active inference (AIF) in order to control dynamical agents. A generative model capturing the environment dynamics is learned by a network composed of two distinct Hebbian ensembles: a posterior network, which infers latent states given the observations, and a state transition network, which predicts the next expected latent state given current state-action pairs. Experimental studies are conducted using the Mountain Car environment from the OpenAI gym suite, to study the effect of the various Hebbian network parameters on the task performance. It is shown that the proposed Hebbian AIF approach outperforms the use of Q-learning, while not requiring any replay buffer, as in typical reinforcement learning systems. These results motivate further investigations of Hebbian learning for the design of AIF networks that can learn environment dynamics without the need for revisiting past buf
    
[^53]: 利用mask先验模型去噪扩散语义分割

    Denoising Diffusion Semantic Segmentation with Mask Prior Modeling. (arXiv:2306.01721v1 [cs.CV])

    [http://arxiv.org/abs/2306.01721](http://arxiv.org/abs/2306.01721)

    本文提出了一种利用去噪扩散生成模型建模的mask先验，改进现有的语义分割方法。该方法达到了在多个基准数据集上的最先进效果，超参数调整较少。

    

    语义分割一直以来都是学习更具有区分性的图像表示用于对每个像素进行分类。尽管取得了显著的进展，但分割掩模本身的先验，例如几何约束和语义约束仍未被深入探索。在本文中，我们提出了一种利用最近开发的去噪扩散生成模型建模的mask先验，改善现有区分性方法的语义分割质量。我们从为mask先验建模调整扩散模型的统一架构开始，将本文重点放在具有离散扩散的具体实例上，并确定其成功应用的各种关键设计选择。我们的初步分析揭示了几个重要发现，包括：（1）简单地将扩散模型集成到语义分割中不足以达到最佳效果，设计不良的扩散过程可能导致分割性能下降;（2）在扩散过程中，在保留遮罩结构和合并输入图像中的其他信息之间取得平衡至关重要;（3）所提出的方法可以在几个基准数据集上实现最先进的性能，而调整的超参数较少。

    The evolution of semantic segmentation has long been dominated by learning more discriminative image representations for classifying each pixel. Despite the prominent advancements, the priors of segmentation masks themselves, e.g., geometric and semantic constraints, are still under-explored. In this paper, we propose to ameliorate the semantic segmentation quality of existing discriminative approaches with a mask prior modeled by a recently-developed denoising diffusion generative model. Beginning with a unified architecture that adapts diffusion models for mask prior modeling, we focus this work on a specific instantiation with discrete diffusion and identify a variety of key design choices for its successful application. Our exploratory analysis revealed several important findings, including: (1) a simple integration of diffusion models into semantic segmentation is not sufficient, and a poorly-designed diffusion process might lead to degradation in segmentation performance; (2) dur
    
[^54]: BLIP-Diffusion: 用于可控文本到图像生成和编辑的预训练主体表征模型

    BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing. (arXiv:2305.14720v1 [cs.CV])

    [http://arxiv.org/abs/2305.14720](http://arxiv.org/abs/2305.14720)

    BLIP-Diffusion是一种新的主体驱动图像生成模型，通过预训练的多模态编码器和主体表征学习任务实现了可控的文本到图像生成和编辑，并支持零样本生成和定制化微调。

    

    基于主体的文本到图像生成模型根据文本提示创建新颖的主体图像。现有模型存在长时间微调和难以保持主体一致性的问题。为了克服这些限制，我们介绍了BLIP-Diffusion，一种新的支持多模态控制的主体驱动图像生成模型，它使用主体图像和文本提示作为输入。与其他主体驱动的生成模型不同，BLIP-Diffusion引入了一个新的多模态编码器，对主体表征进行预训练。我们首先按照BLIP-2的方法预训练了多模态编码器，以生成与文本对齐的视觉表征。然后，我们设计了一个主体表征学习任务，使扩散模型能够利用这种视觉表征并生成新的主体图像。与DreamBooth等先前方法相比，我们的模型支持零样本主体驱动生成，并且可以进行有效的定制主体微调。

    Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with u
    
[^55]: ConvXAI：通过对话提供异构的AI解释，支持人机科技写作

    ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])

    [http://arxiv.org/abs/2305.09770](http://arxiv.org/abs/2305.09770)

    ConvXAI是一个基于对话的XAI系统，它集成了多种XAI类型，并将实际用户需求嵌入设计中，以提高实用性。

    

    尽管已经提出了各种各样的人工智能解释（XAI）方法来解释AI系统，但目前的方法是否对人类实用仍存在不一致的发现。为了改善XAI方法的实用性，一系列研究确定了现实世界中多样化和动态的用户需求与现有XAI方法之间的差距。虽然之前的研究设想将多种XAI方法集成到通用XAI界面（例如，基于对话或GUI的XAI系统）中以减轻这些差距，但缺少针对这些系统如何设计以满足实际用户需求的研究。在本研究中，我们提出了ConvXAI，这是一个基于对话的XAI系统，它结合了多种XAI类型，并赋予用户通过通用的XAI对话界面提出各种XAI问题的能力。特别地，我们创新地将实际用户需求（即，基于格式研究的四个原则）嵌入ConvXAI设计中，以提高实用性。

    While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
    
[^56]: 探究子词分割对transformer语言模型性能的影响

    Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])

    [http://arxiv.org/abs/2305.05480](http://arxiv.org/abs/2305.05480)

    本文研究使用单语词段算法StateMorph训练语言模型时，可以使模型更高效地收敛并获得更好的验证分数。

    

    我们想研究词段如何影响语言模型的性能。我们使用了一种单语词段算法StateMorph，在芬兰语和俄语中训练了GPT-2和BERT模型。作为比较，我们还训练了一个使用BPE和Morfessor分割算法的模型。我们的初步结果表明，StateMorph可以帮助模型更有效地收敛并获得更好的验证分数。

    We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
    
[^57]: 跨模态相似性调节的对比学习在视觉语言预训练中的应用

    Vision Langauge Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation. (arXiv:2305.04474v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.04474](http://arxiv.org/abs/2305.04474)

    本文提出了一种跨模态相似性逐步细化的对比学习策略，在视觉语言预训练中优化图像/文本锚点与其负样本文本/图像之间的互信息，有效应对了（部分）误反样本的挑战。

    

    在视觉语言预训练中，跨模态对比学习面临着（部分）误反样本的挑战。本文从 mutual information 优化的角度研究了这个问题。我们理论上证明了在存在噪声的情况下，涉及到负样本的互信息也很重要。我们提出了一种跨模态相似性逐步细化的对比学习策略，以更加精确地优化图像/文本锚点与其负样本文本/图像之间的互信息。我们的方法在四个下游跨模态任务上表现出竞争力，并在理论指导下系统地平衡了（部分）误反样本的有益影响和有害影响。

    Cross-modal contrastive learning in vision language pretraining (VLP) faces the challenge of (partial) false negatives. In this paper, we study this problem from the perspective of Mutual Information (MI) optimization. It is common sense that InfoNCE loss used in contrastive learning will maximize the lower bound of MI between anchors and their positives, while we theoretically prove that MI involving negatives also matters when noises commonly exist. Guided by a more general lower bound form for optimization, we propose a contrastive learning strategy regulated by progressively refined cross-modal similarity, to more accurately optimize MI between an image/text anchor and its negative texts/images instead of improperly minimizing it. Our method performs competitively on four downstream cross-modal tasks and systematically balances the beneficial and harmful effects of (partial) false negative samples under theoretical guidance.
    
[^58]: 论知识图谱推理的安全风险

    On the Security Risks of Knowledge Graph Reasoning. (arXiv:2305.02383v1 [cs.CR])

    [http://arxiv.org/abs/2305.02383](http://arxiv.org/abs/2305.02383)

    本文介绍了一个重要的人工智能任务 - 知识图谱推理(KGR)，并基于攻击者的目的、知识和攻击向量概括了KGR的安全威胁。本文提出了一种新型攻击ROAR，高度有效地误导KGR向目标查询提供预定义答案，但对于非目标查询几乎没有影响。

    

    知识图谱推理 (KGR) - 回答大型知识图谱上复杂逻辑查询的任务，代表了一项重要的人工智能任务，涉及多种应用（例如网络威胁狩猎）。然而，尽管它越来越流行，但是 KGR 的潜在安全风险还未被广泛探讨，这一点令人担忧，因为它在安全关键领域的应用越来越广泛。本文是架起狭隘鸿沟的一个可靠的初始步骤。我们根据攻击者的目的、知识和攻击向量系统化地概括了 KGR 的安全威胁。此外，我们提出了 ROAR，一种新型攻击，它是实现各种此类威胁的攻击。通过在代表性用例（如医疗决策支持、网络威胁狩猎和常识推理）中的实证评估，我们证明 ROAR 对于误导 KGR 提供预定义答案的目标查询非常有效，但对非目标查询影响微乎其微。

    Knowledge graph reasoning (KGR) -- answering complex logical queries over large knowledge graphs -- represents an important artificial intelligence task, entailing a range of applications (e.g., cyber threat hunting). However, despite its surging popularity, the potential security risks of KGR are largely unexplored, which is concerning, given the increasing use of such capability in security-critical domains.  This work represents a solid initial step towards bridging the striking gap. We systematize the security threats to KGR according to the adversary's objectives, knowledge, and attack vectors. Further, we present ROAR, a new class of attacks that instantiate a variety of such threats. Through empirical evaluation in representative use cases (e.g., medical decision support, cyber threat hunting, and commonsense reasoning), we demonstrate that ROAR is highly effective to mislead KGR to suggest pre-defined answers for target queries, yet with negligible impact on non-target ones. Fi
    
[^59]: TSMixer：一种全MLP架构用于时间序列预测

    TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06053](http://arxiv.org/abs/2303.06053)

    TSMixer是一种通过堆叠多层感知器（MLP）设计的新型结构，基于沿时间和特征维度的混合操作，能够在时间序列预测中表现出极好的性能。

    

    实际时间序列数据集通常是多变量且具有复杂的动态。为了捕获这种复杂性，像循环或基于注意力的顺序深度学习模型这样的高容量结构变得受欢迎。然而，最近的研究表明，简单的单变量线性模型可以在几个常用的学术基准测试中胜过这样的深度学习模型。扩展它们，本文研究线性模型在时间序列预测中的能力，并提出了时序混合器（TSMixer），这是一种通过堆叠多层感知器（MLP）设计的新型结构。 TSMixer基于沿时间和特征维度的混合操作，以有效地提取信息。在流行的学术基准测试上，简单易行的TSMixer与利用特定基准的归纳偏差的专业先进模型相媲美。在具有挑战性和大规模的M5基准测试中，即一个实际的零售数据集上，TSMixer表现出非常出色的性能。

    Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates super
    
[^60]: 通过语言实现视觉抽象和推理技术

    Visual Abstraction and Reasoning through Language. (arXiv:2303.04091v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.04091](http://arxiv.org/abs/2303.04091)

    本论文提出了一种通过自然语言描述任务的通用框架来解决Abstraction and Reasoning Corpus（ARC）问题，虽然还没有在ARC上击败最先进的DSL模型，但我们展示了我们的方法具有巨大的潜力，可以解决先前未解决的任务。

    

    尽管人工智能（AI）模型在局限应用中已经达到了人类甚至超越人类的性能，但它们仍然难以展现更广泛和更灵活的智能。Abstraction and Reasoning Corpus（ARC）旨在评估AI系统与人类类似的认知能力。目前大多数方法依赖于精心设计的特定领域语言（DSL），用于暴力解决ARC中的任务。在这项工作中，我们提出了一个基于任务自然语言描述的通用框架来解决ARC问题。虽然还没有在ARC上击败最先进的DSL模型，但我们展示了我们的方法具有巨大的潜力，可以解决先前未解决的任务。

    While Artificial Intelligence (AI) models have achieved human or even superhuman performance in narrowly defined applications, they still struggle to show signs of broader and more flexible intelligence. The Abstraction and Reasoning Corpus (ARC), introduced by Fran\c{c}ois Chollet, aims to assess how close AI systems are to human-like cognitive abilities. Most current approaches rely on carefully handcrafted domain-specific languages (DSLs), which are used to brute-force solutions to the tasks present in ARC. In this work, we propose a general framework for solving ARC based on natural language descriptions of the tasks. While not yet beating state-of-the-art DSL models on ARC, we demonstrate the immense potential of our approach hinted at by the ability to solve previously unsolved tasks.
    
[^61]: 探索自监督预训练ASR模型用于口吃和老年人语音识别

    Exploring Self-supervised Pre-trained ASR Models For Dysarthric and Elderly Speech Recognition. (arXiv:2302.14564v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2302.14564](http://arxiv.org/abs/2302.14564)

    本文探索了将自监督预训练模型与ASR系统相结合以提高口吃和老年人的语音识别性能的方法，包括输入特征融合，TDNN系统的帧级联合解码，以及多通道解码等。同时，在构建多模式的口吃和老年人语音识别系统时，采用了领域自适应的wav2vec2.0表示法。

    

    自动识别语音障碍和老年人的语音仍然是一项极具挑战性的任务，因为很难收集大量这样的数据。本文探讨了一系列方法，将领域自适应的SSL预训练模型与TDNN和Conformer ASR系统相结合，用于口吃和老年人的语音识别。

    Automatic recognition of disordered and elderly speech remains a highly challenging task to date due to the difficulty in collecting such data in large quantities. This paper explores a series of approaches to integrate domain adapted SSL pre-trained models into TDNN and Conformer ASR systems for dysarthric and elderly speech recognition: a) input feature fusion between standard acoustic frontends and domain adapted wav2vec2.0 speech representations; b) frame-level joint decoding of TDNN systems separately trained using standard acoustic features alone and with additional wav2vec2.0 features; and c) multi-pass decoding involving the TDNN/Conformer system outputs to be rescored using domain adapted wav2vec2.0 models. In addition, domain adapted wav2vec2.0 representations are utilized in acoustic-to-articulatory (A2A) inversion to construct multi-modal dysarthric and elderly speech recognition systems. Experiments conducted on the UASpeech dysarthric and DementiaBank Pitt elderly speech 
    
[^62]: IGB: 针对公共图形数据集在标记、特征、异质性和大小方面的差距为深度学习研究提供了解决方案

    IGB: Addressing The Gaps In Labeling, Features, Heterogeneity, and Size of Public Graph Datasets for Deep Learning Research. (arXiv:2302.13522v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13522](http://arxiv.org/abs/2302.13522)

    IGB是一个研究数据集工具，包含同质和异质性学术图形，规模巨大，并提供工具用于生成不同特性的合成图形，为GNN研究人员提供解决公共图形数据集在标记、特征、异质性和大小方面差距的有价值资源。

    

    图形神经网络 (GNNs) 已经展示了对于各种具有挑战性的真实应用的巨大潜力，但 GNN 研究中的一个主要障碍是缺乏大规模灵活的数据集。现有的大部分公共GNN数据集都相对较小，这限制了 GNN 的推广到未知数据的能力。很少有大型的图形数据集提供丰富的标记数据，这使得难以确定 GNN 模型在未知数据上的低准确性是由于训练数据不足还是模型无法推广。此外，训练 GNN 的数据集需要提供灵活性，以便深入研究各种因素对 GNN 模型训练的影响。在这项工作中，我们介绍了伊利诺伊图形基准 (IGB)，这是一个研究数据集工具，开发人员可以使用它来高精度地训练、审查和系统地评估GNN模型。IGB 包括同质和异质性学术图形，规模巨大，并且可以标记和操作，以模拟真实场景。该数据集还包括用于生成具有不同特性的合成图形的工具，使研究人员能够探索各种图形特性对 GNN 的影响。我们相信，伊利诺伊图形基准将为 GNN 研究团体提供有价值的资源，以解决公共图形数据集在标记、特征、异质性和大小方面的差距，以用于深度学习研究。

    Graph neural networks (GNNs) have shown high potential for a variety of real-world, challenging applications, but one of the major obstacles in GNN research is the lack of large-scale flexible datasets. Most existing public datasets for GNNs are relatively small, which limits the ability of GNNs to generalize to unseen data. The few existing large-scale graph datasets provide very limited labeled data. This makes it difficult to determine if the GNN model's low accuracy for unseen data is inherently due to insufficient training data or if the model failed to generalize. Additionally, datasets used to train GNNs need to offer flexibility to enable a thorough study of the impact of various factors while training GNN models.  In this work, we introduce the Illinois Graph Benchmark (IGB), a research dataset tool that the developers can use to train, scrutinize and systematically evaluate GNN models with high fidelity. IGB includes both homogeneous and heterogeneous academic graphs of enorm
    
[^63]: 在大状态空间中打破多智体的诅咒：带独立线性函数逼近的Markov博弈中的强化学习

    Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation. (arXiv:2302.03673v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03673](http://arxiv.org/abs/2302.03673)

    本研究提出了一种新的独立线性Markov博弈模型，针对多智体强化学习中的大状态空间和大量代理问题，设计了新算法以学习Markov粗略相关均衡和Markov相关均衡。相比于现有的Markov博弈函数逼近技术，我们的方法能够大大降低样本复杂度并取得更高的精度。

    

    我们提出了一种新的模型——独立线性Markov博弈，用于具有大状态空间和大量代理的多智体强化学习。这是一类带有独立线性函数逼近的Markov博弈，每个代理都有自己的函数逼近，用于被其他玩家的策略边缘化的状态-动作值函数。我们设计了学习Markov粗略相关均衡和Markov相关均衡的新算法，并提供了样本复杂度界限，这些界限仅与每个代理自己的函数类复杂度成多项式比例，从而打破了多智体的诅咒。相比之下，现有的用于函数逼近的Markov博弈的研究，在特化于标准表格状况的Markov博弈设置时，其样本复杂度界限会随着\emph{联合行动空间}的大小成指数级增长，而该联合行动空间在代理的数量上呈指数级增长。我们的算法依赖于两个关键的技术创新：(1) 利用策略重放来降低样本复杂度；(2) 利用独立线性函数逼近来获得计算上的有效性和统计上的高精度。

    We propose a new model, independent linear Markov game, for multi-agent reinforcement learning with a large state space and a large number of agents. This is a class of Markov games with independent linear function approximation, where each agent has its own function approximation for the state-action value functions that are marginalized by other players' policies. We design new algorithms for learning the Markov coarse correlated equilibria (CCE) and Markov correlated equilibria (CE) with sample complexity bounds that only scale polynomially with each agent's own function class complexity, thus breaking the curse of multiagents. In contrast, existing works for Markov games with function approximation have sample complexity bounds scale with the size of the \emph{joint action space} when specialized to the canonical tabular Markov game setting, which is exponentially large in the number of agents. Our algorithms rely on two key technical innovations: (1) utilizing policy replay to tac
    
[^64]: 线性组合的威力：随机卷积学习

    The Power of Linear Combinations: Learning with Random Convolutions. (arXiv:2301.11360v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.11360](http://arxiv.org/abs/2301.11360)

    本研究质疑了卷积神经网络中学习到的卷积核的重要性，提出了简单的线性组合方法，从随机卷积核中创建出表达能力强的网络运算符，通过隐含的正则化技术，可以提高整体性能。

    

    现代卷积神经网络通过增加模型深度、宽度和卷积核大小来保持与更先进的模型（如基于变换器的模型）的竞争力，导致有大量的可训练模型参数需要在训练过程中进行处理。本文质疑卷积神经网络中学习到的卷积核的重要性。实验证明，很多当代的卷积神经网络结构，甚至在不更新初始化的随机卷积核的情况下就可以达到高的测试准确率。实际上，简单的线性组合可以有效地将随机卷积核组合成表达能力强的网络运算符，其通过高效的 $1 \times 1$ 卷积来实现。此外，这些随机卷积核的组合可以隐式地正则化结果运算符，减轻过拟合，提高整体性能。

    Following the traditional paradigm of convolutional neural networks (CNNs), modern CNNs manage to keep pace with more recent, for example transformer-based, models by not only increasing model depth and width but also the kernel size. This results in large amounts of learnable model parameters that need to be handled during training. While following the convolutional paradigm with the according spatial inductive bias, we question the significance of \emph{learned} convolution filters. In fact, our findings demonstrate that many contemporary CNN architectures can achieve high test accuracies without ever updating randomly initialized (spatial) convolution filters. Instead, simple linear combinations (implemented through efficient $1\times 1$ convolutions) suffice to effectively recombine even random filters into expressive network operators. Furthermore, these combinations of random filters can implicitly regularize the resulting operations, mitigating overfitting and enhancing overall 
    
[^65]: 深度学习在数学推理中的应用综述

    A Survey of Deep Learning for Mathematical Reasoning. (arXiv:2212.10535v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.10535](http://arxiv.org/abs/2212.10535)

    本文综述了过去十年中深度学习在数学推理领域的关键任务、数据集和方法，并评估了现有的基准和方法，探讨了未来的研究方向。

    

    数学推理是人类智能的基本组成部分，并且应用广泛，包括科学、工程、金融和日常生活。发展能够解决数学问题和证明定理的人工智能系统在机器学习和自然语言处理领域引起了极大关注。本文回顾了过去十年中在数学推理和深度学习交叉领域的关键任务、数据集和方法，并评估了现有的基准和方法，讨论了未来的研究方向。

    Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.
    
[^66]: \{kappa}HGCN: 通过连续和离散曲率学习实现树状结构建模

    \{kappa}HGCN: Tree-likeness Modeling via Continuous and Discrete Curvature Learning. (arXiv:2212.01793v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01793](http://arxiv.org/abs/2212.01793)

    本文提出了一种新的\{kappa}HGCN模型，在双曲空间内实现树状结构建模，通过结合连续和离散曲率来学习输入图的基础几何结构，并在多个基准测试和数据集上取得了最先进的性能。

    

    树状结构在现实世界中广泛存在，包括层次结构和幂律分布。最近，利用双曲空间进行树状结构建模受到了广泛关注，由于其呈指数增长，相比于平坦的欧几里得空间，曲面双曲空间提供了更易处理和嵌入的空间，特别适用于展现隐含树状结构的数据集。然而，真实世界树状数据的复杂性提出了一个重要挑战，因为它经常展示出树状、平坦和圆形区域的异质组成。将这样异质的结构直接嵌入一个同质化的嵌入空间（即双曲空间）必然导致重大失真。为了缓解上述缺点，本研究致力于探索双曲空间的曲率，以实现灵活准确地建模树状结构。具体而言，我们提出了一种新的\{kappa}HGCN模型，将连续和离散曲率相结合，学习输入图的基础几何结构。我们的模型在不同的基准测试和数据集上均取得了最先进的性能，证明了其在捕捉输入数据的树状结构方面的有效性。

    The prevalence of tree-like structures, encompassing hierarchical structures and power law distributions, exists extensively in real-world applications, including recommendation systems, ecosystems, financial networks, social networks, etc. Recently, the exploitation of hyperbolic space for tree-likeness modeling has garnered considerable attention owing to its exponential growth volume. Compared to the flat Euclidean space, the curved hyperbolic space provides a more amenable and embeddable room, especially for datasets exhibiting implicit tree-like architectures. However, the intricate nature of real-world tree-like data presents a considerable challenge, as it frequently displays a heterogeneous composition of tree-like, flat, and circular regions. The direct embedding of such heterogeneous structures into a homogeneous embedding space (i.e., hyperbolic space) inevitably leads to heavy distortions. To mitigate the aforementioned shortage, this study endeavors to explore the curvatur
    
[^67]: 使用社交感知强化学习提高主动对话代理的性能

    Improving Proactive Dialog Agents Using Socially-Aware Reinforcement Learning. (arXiv:2211.15359v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.15359](http://arxiv.org/abs/2211.15359)

    本论文的主要贡献在于提出了一种新的方法，通过将社交和任务相关特征考虑在对话中，来优化主动对话策略，使其在任务效率高的同时，也能促进用户信任，从而提高人机交互的成功率和效率。

    

    智能对话代理的下一个步骤是从旁观者的角色中解脱出来，变得更加主动。明确定义的主动行为可以改善人机合作，因为代理在交互过程中扮演更积极的角色并解除了用户的责任。然而，主动性是一把双刃剑，因为执行不当的预防性行动可能不仅对任务结果产生破坏性影响，而且还会对与用户的关系产生影响。为了设计合适的主动对话策略，我们提出了一种新的方法，将社交和任务相关特征都考虑在对话中。这里的主要目标是优化主动行为，使其任务导向——这意味着高任务成功率和效率——同时在促进用户信任时也具有社交效益。将这两个方面包含在用强化学习训练主动对话代理的奖励函数中，显示出我们的方法对于更加成功的人机交互的益处。

    The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive. Well-defined proactive behavior may improve human-machine cooperation, as the agent takes a more active role during interaction and takes off responsibility from the user. However, proactivity is a double-edged sword because poorly executed pre-emptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user. For designing adequate proactive dialog strategies, we propose a novel approach including both social as well as task-relevant features in the dialog. Here, the primary goal is to optimize proactive behavior so that it is task-oriented - this implies high task success and efficiency - while also being socially effective by fostering user trust. Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for more successful human-mach
    
[^68]: 基于无动作轨迹的半监督离线强化学习

    Semi-Supervised Offline Reinforcement Learning with Action-Free Trajectories. (arXiv:2210.06518v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06518](http://arxiv.org/abs/2210.06518)

    该论文提出了一种半监督离线强化学习的新设置，利用有标记的轨迹数据和无动作的轨迹数据训练反动力学模型以获取代理标签，最终使用任何离线强化学习算法以实现高成功率的表现。

    

    自然智能体可以有效地从不同大小、质量和测量类型的多个数据源中学习。我们通过引入一个新的、实际上受到启发的半监督设置来研究强化学习中的这种异质性。在这里，智能体可以访问两个轨迹集：一个包含每个时间步的状态、行为和奖励三元组的标记轨迹集，以及一个仅包含状态和奖励信息的未标记轨迹集。针对这种情况，我们开发和研究了一个简单的元算法流水线，该算法在标记数据上学习反动力学模型，以获得未标记数据的代理标签，然后将任何离线强化学习算法用于真实和代理标记轨迹。经验表明，这个简单的流水线非常成功——在几个D4RL基准测试中，某些离线RL算法可以与在完全标记数据集上训练的变体的表现相匹配，即使后者拥有更多的标记数据。

    Natural agents can effectively learn from multiple data sources that differ in size, quality, and types of measurements. We study this heterogeneity in the context of offline reinforcement learning (RL) by introducing a new, practically motivated semi-supervised setting. Here, an agent has access to two sets of trajectories: labelled trajectories containing state, action and reward triplets at every timestep, along with unlabelled trajectories that contain only state and reward information. For this setting, we develop and study a simple meta-algorithmic pipeline that learns an inverse dynamics model on the labelled data to obtain proxy-labels for the unlabelled data, followed by the use of any offline RL algorithm on the true and proxy-labelled trajectories. Empirically, we find this simple pipeline to be highly successful -- on several D4RL benchmarks~\cite{fu2020d4rl}, certain offline RL algorithms can match the performance of variants trained on a fully labelled dataset even when w
    
[^69]: 通过发现多样的环境轨迹生成器先验知识，高效学习运动技能

    Efficient Learning of Locomotion Skills through the Discovery of Diverse Environmental Trajectory Generator Priors. (arXiv:2210.04819v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2210.04819](http://arxiv.org/abs/2210.04819)

    本论文提出了一种名为EETG的方法，使用“质量-多样性”算法学习一组多样化的专业化运动先验知识，能够帮助四足机器人成功地穿越各种环境，比使用单个轨迹生成器的方法更加有效。

    

    最近数据驱动的学习方法在学习各种不规则地形的鲁棒运动控制器方面表现出色。以轨迹生成器（TG）形式加入良好的运动先验知识，可以有效地学习复杂的运动技能。然而，随着任务/环境变得越来越复杂，在单个良好的TG中定义它仍然是一个具有挑战性的问题，因为它需要大量的调整，同时还有可能降低先验知识的有效性。本文提出一种名为EETG的方法，它使用“质量-多样性（Quality-Diversity）”算法学习一组多样化的专业化运动先验知识，同时在Policies Modulating TG（PMTG）框架内维持单一策略。实验结果表明，EETG使四足机器人能够成功地穿越各种环境，如斜坡、台阶、崎岖地形和平衡横杆。我们的实验结果表明，EETG比使用单个TG的方法更加有效。

    Data-driven learning based methods have recently been particularly successful at learning robust locomotion controllers for a variety of unstructured terrains. Prior work has shown that incorporating good locomotion priors in the form of trajectory generators (TGs) is effective at efficiently learning complex locomotion skills. However, defining a good, single TG as tasks/environments become increasingly more complex remains a challenging problem as it requires extensive tuning and risks reducing the effectiveness of the prior. In this paper, we present Evolved Environmental Trajectory Generators (EETG), a method that learns a diverse set of specialised locomotion priors using Quality-Diversity algorithms while maintaining a single policy within the Policies Modulating TG (PMTG) architecture. The results demonstrate that EETG enables a quadruped robot to successfully traverse a wide range of environments, such as slopes, stairs, rough terrain, and balance beams. Our experiments show th
    
[^70]: 公平信用评分的算法决策方法研究

    Algorithmic decision making methods for fair credit scoring. (arXiv:2209.07912v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07912](http://arxiv.org/abs/2209.07912)

    本论文研究了12种偏差缓解方法在5个不同的公平性指标下的有效性，评估了它们对金融机构准确性和潜在盈利能力的影响，并指出了最成功和最不成功缓解方法。

    

    机器学习在评估贷款申请人的信用价值方面的有效性已经被证明了很长一段时间。然而，使用自动决策过程可能导致对某些群体或个人进行不平等对待，进而导致歧视性结果，这引起了人们的关注。本文通过评估12种主要的偏差缓解方法在5种不同的公平性指标下的有效性以及评估它们对于金融机构的准确性和潜在盈利能力，来解决这个问题。通过我们的分析，我们已经确定了在保持准确性和盈利能力的同时实现公平所面临的挑战，并突出了最成功和最不成功的缓解方法。最终，我们的研究旨在弥合实验机器学习与其在金融行业中的实际应用之间的差距。

    The effectiveness of machine learning in evaluating the creditworthiness of loan applicants has been demonstrated for a long time. However, there is concern that the use of automated decision-making processes may result in unequal treatment of groups or individuals, potentially leading to discriminatory outcomes. This paper seeks to address this issue by evaluating the effectiveness of 12 leading bias mitigation methods across 5 different fairness metrics, as well as assessing their accuracy and potential profitability for financial institutions. Through our analysis, we have identified the challenges associated with achieving fairness while maintaining accuracy and profitabiliy, and have highlighted both the most successful and least successful mitigation methods. Ultimately, our research serves to bridge the gap between experimental machine learning and its practical applications in the finance industry.
    
[^71]: 开发跨领域跨语言超声舌头成像特征用于老年人和患有发音障碍者的语音识别

    Exploiting Cross-domain And Cross-Lingual Ultrasound Tongue Imaging Features For Elderly And Dysarthric Speech Recognition. (arXiv:2206.07327v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2206.07327](http://arxiv.org/abs/2206.07327)

    本文利用跨领域和跨语言的A2A反演方法，利用平行音频和超声舌头成像（UTI）数据来产生基于UTI的发音特征，用于老年人和患有发音障碍者的语音识别研究，实验表明该方法相比基线ASR系统有更好的效果。

    

    发音特征本质上不变形于声学信号失真，并已成功地被整合进了设计用于正常语音的自动语音识别（ASR）系统中。然而，在老年人和患有语音障碍的跨语言非正常任务领域中，这些发音特征的实际应用往往受到从目标说话人收集此类特殊数据的困难所限制。为此，本文提出了一种跨领域和跨语言的A2A反演方法，该方法在模型预训练时利用24小时TaL语料库的平行音频和超声舌头成像（UTI）数据，并在跨领域和跨语言适应到两种语言的三个数据集上：英语DementiaBank Pitt和广东话JCCOCC MoCA的老年人的语音语料库；以及英语TORGO的发音障碍言语数据，从而产生基于UTI的发音特征。实验表明，在三个任务上整合生成的发音特征始终优于基线TDNN和PLP特征ASR系统，在所有数据集上相对误差率（RER）的降低范围为7.2％到36.2％。

    Articulatory features are inherently invariant to acoustic signal distortion and have been successfully incorporated into automatic speech recognition (ASR) systems designed for normal speech. Their practical application to atypical task domains such as elderly and disordered speech across languages is often limited by the difficulty in collecting such specialist data from target speakers. This paper presents a cross-domain and cross-lingual A2A inversion approach that utilizes the parallel audio and ultrasound tongue imaging (UTI) data of the 24-hour TaL corpus in A2A model pre-training before being cross-domain and cross-lingual adapted to three datasets across two languages: the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech corpora; and the English TORGO dysarthric speech data, to produce UTI based articulatory features. Experiments conducted on three tasks suggested incorporating the generated articulatory features consistently outperformed the baseline TDNN an
    
[^72]: 利用抑制变量的线性基础数据审查可解释人工智能(XAI)

    Scrutinizing XAI using linear ground-truth data with suppressor variables. (arXiv:2111.07473v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2111.07473](http://arxiv.org/abs/2111.07473)

    研究提出了一个关于可解释人工智能(XAI)的方法，该方法提出了特征重要性的客观初步定义，以避免由于方法的行为引起的错误解读。

    

    机器学习(ML)越来越常用于高风险决策中。由于复杂的ML模型(如深度神经网络)通常被认为是黑匣子，因此已经开发出大量程序来阐明其内部运作和预测方式，定义了"可解释人工智能"(XAI)领域。显著性方法根据某种"重要性"度量对输入特征进行排序。由于迄今为止缺乏特征重要性的正式定义，这些方法很难验证。已经证实，一些显著性方法可以突出显示与预测目标没有统计联系的特征(抑制变量)。为了避免由于这种行为引起的错误解读，我们提出了确保此类联系存在是特征重要性的必要条件和客观初步定义。我们精心制作了一个基础数据集，其中所有统计依赖关系都是明确定义的和线性的，

    Machine learning (ML) is increasingly often used to inform high-stakes decisions. As complex ML models (e.g., deep neural networks) are often considered black boxes, a wealth of procedures has been developed to shed light on their inner workings and the ways in which their predictions come about, defining the field of 'explainable AI' (XAI). Saliency methods rank input features according to some measure of 'importance'. Such methods are difficult to validate since a formal definition of feature importance is, thus far, lacking. It has been demonstrated that some saliency methods can highlight features that have no statistical association with the prediction target (suppressor variables). To avoid misinterpretations due to such behavior, we propose the actual presence of such an association as a necessary condition and objective preliminary definition for feature importance. We carefully crafted a ground-truth dataset in which all statistical dependencies are well-defined and linear, se
    
[^73]: 服务机器人中知识表示的综述

    A Survey of Knowledge Representation in Service Robotics. (arXiv:1807.02192v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/1807.02192](http://arxiv.org/abs/1807.02192)

    本文重点综述了服务机器人中知识表示的研究进展，深入探讨了知识的获取、表示和应用，与其他学习模型的差异，并探讨了其在机器人任务中的重要性。

    

    在服务机器人领域，研究人员致力于学习、理解和表示机器人执行任务的运动方式。机器人学习和问题解决的任务是非常广泛的，因为它集成了如物体检测、活动识别、任务/运动规划、定位、知识表示和检索以及感知/视觉和机器学习技术的各种任务。本文仅关注知识表示，并阐述了过去几十年来研究人员通常如何收集、表示和再现知识以解决问题。根据知识表示的定义，我们讨论了这种表示与近年来广泛介绍和研究的有用学习模型之间的主要区别，如机器学习、深度学习、概率建模和语义图形结构。

    Within the realm of service robotics, researchers have placed a great amount of effort into learning, understanding, and representing motions as manipulations for task execution by robots. The task of robot learning and problem-solving is very broad, as it integrates a variety of tasks such as object detection, activity recognition, task/motion planning, localization, knowledge representation and retrieval, and the intertwining of perception/vision and machine learning techniques. In this paper, we solely focus on knowledge representations and notably how knowledge is typically gathered, represented, and reproduced to solve problems as done by researchers in the past decades. In accordance with the definition of knowledge representations, we discuss the key distinction between such representations and useful learning models that have extensively been introduced and studied in recent years, such as machine learning, deep learning, probabilistic modelling, and semantic graphical structur
    

