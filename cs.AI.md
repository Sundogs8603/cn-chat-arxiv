# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Learning and Verification of Task Structure in Instructional Videos.](http://arxiv.org/abs/2303.13519) | 该论文提出了一种新的预训练视频模型——VideoTaskformer。通过从整体上学习表示来验证视频的正确执行，并预测下一步骤。同时，论文也提出了两个新的检测教学视频中错误的基准。 |
| [^2] | [Three ways to improve feature alignment for open vocabulary detection.](http://arxiv.org/abs/2303.13518) | 该论文提出了三种方法来改善开放式词汇检测中特征对齐的问题，包括增强文本嵌入、修改特征金字塔网络和检测头部、以及采用自学习方法。这些方法可以有效缓解模型在未见类上的性能问题。 |
| [^3] | [Towards Solving Fuzzy Tasks with Human Feedback: A Retrospective of the MineRL BASALT 2022 Competition.](http://arxiv.org/abs/2303.13512) | MineRL BASALT 2022竞赛旨在促进开发算法来利用人类反馈解决Minecraft中难以指定奖励函数的任务，帮助推动此方向的研究。 |
| [^4] | [Neural Preset for Color Style Transfer.](http://arxiv.org/abs/2303.13511) | 本文提出了一种名为神经预设的技术，通过确定性神经颜色映射方法（DNCM）和两阶段流水线实现高质量的颜色风格转移，并且具有各种优势。 |
| [^5] | [DreamBooth3D: Subject-Driven Text-to-3D Generation.](http://arxiv.org/abs/2303.13508) | DreamBooth3D是一种可从3-6张图片中生成主体特定3D素材的方法，通过结合文本到图像模型和文本到3D生成模型，使用一种三阶段的优化策略来产生高质量3D素材。 |
| [^6] | [TriPlaneNet: An Encoder for EG3D Inversion.](http://arxiv.org/abs/2303.13497) | 本研究介绍了一种实时方法TriPlaneNet，通过直接利用EG3D生成模型的三平面表示，建立在一个用于潜在编码的前馈卷积编码器上，并扩展了一个完全卷积的三平面数值偏移预测器，旨在弥合现有的GAN反演方法的差距。 |
| [^7] | [The effectiveness of MAE pre-pretraining for billion-scale pretraining.](http://arxiv.org/abs/2303.13496) | 本文在计算机视觉领域提出了一种自我监督的MAE技术预前置训练方法，该方法适用于亿级预训练规模，并可显著提高模型收敛性和下游转移性能。 |
| [^8] | [Attention! Dynamic Epistemic Logic Models of (In)attentive Agents.](http://arxiv.org/abs/2303.13494) | 本文提出了一种更一般化的动态认知逻辑模型，允许代理人关注一些原子公式的子集，并扩展了该框架，以解释无注意瞎视现象。 |
| [^9] | [Boosting Reinforcement Learning and Planning with Demonstrations: A Survey.](http://arxiv.org/abs/2303.13489) | 强化学习中一种减少试错的方法是使用示范，本文综述了如何使用示范来促进学习决策模型的应用，并提供了基于ManiSkill机器人学习基准的示范生成和利用管道的实例。 |
| [^10] | [NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations.](http://arxiv.org/abs/2303.13483) | 本文提出了一种神经符号基础的三维物体及关系的框架NS3D，可以有效的推理出三维场景中的复杂语义，同时在多个基准数据集上展示了领先或具有竞争性的性能。 |
| [^11] | [TactoFind: A Tactile Only System for Object Retrieval.](http://arxiv.org/abs/2303.13482) | TactoFind是一个纯触觉物品获取系统，它可以使用手指上的触控传感器，在没有任何视觉反馈的情况下定位、识别和抓取新的物体。 |
| [^12] | [Plotting Behind the Scenes: Towards Learnable Game Engines.](http://arxiv.org/abs/2303.13472) | 本文提出了一个方法，可以从单眼注释视频中训练出类似游戏引擎的神经模型，这个模型被称为可学习游戏引擎(LGE)，它可以通过指定高级和低级操作序列来玩游戏，并且解锁了导演模式，可以使用高级约束条件控制代理。 |
| [^13] | [Extracting Physical Rehabilitation Exercise Information from Clinical Notes: a Comparison of Rule-Based and Machine Learning Natural Language Processing Techniques.](http://arxiv.org/abs/2303.13466) | 本文提出了一种基于规则的自然语言处理算法，用于从临床笔记中提取卒中患者治疗过程的锻炼信息，并与几个小型机器学习模型进行比较。在足够的数据可用的情况下，我们的算法在提取一半的概念方面优于这些模型，并且每个概念的个体运动描述可以分配二进制标签，并且F值不低于0.75。这些算法表现出了准确提取临床笔记中康复治疗锻炼信息的前景。 |
| [^14] | [Deep RL with Hierarchical Action Exploration for Dialogue Generation.](http://arxiv.org/abs/2303.13465) | 本篇论文提出了一种新的方法，通过分层行为探索，从多个奖励函数中进行离线学习，并成功地解决了在对话生成中行为采样效率低下的问题，可以更好地识别人类情感细节。 |
| [^15] | [DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video.](http://arxiv.org/abs/2303.13397) | 提出了一种基于扩散驱动变压器的视频 HMR 框架（DDT），它旨在从输入序列中解码特定的运动模式，增强运动平滑性和时间一致性，并输出所有帧的人体网格，使得 DDT 更适用于时间效率至关重要的实际应用。 |
| [^16] | [Planning for Manipulation among Movable Objects: Deciding Which Objects Go Where, in What Order, and How.](http://arxiv.org/abs/2303.13385) | 本文扩展了M4M算法并提出了E-M4M算法，在受约束的环境中排列多个物体，并且能够确定可行的推动序列，表现优于M4M算法。 |
| [^17] | [Practical and Ethical Challenges of Large Language Models in Education: A Systematic Literature Review.](http://arxiv.org/abs/2303.13379) | LLMs在教育中有自动生成和分析文本内容的潜力。然而，这些创新的实际性和伦理性存在担忧，需要考虑技术可行性、隐私、平等和善意等因素。 |
| [^18] | [Capabilities of GPT-4 on Medical Challenge Problems.](http://arxiv.org/abs/2303.13375) | 本论文对最先进的LLM——GPT-4在医学能力考试和基准数据集上进行了全面评估，结果显示其表现出色，有助于医学相关领域的研究和应用。 |
| [^19] | [Fine-tuning ClimateBert transformer with ClimaText for the disclosure analysis of climate-related financial risks.](http://arxiv.org/abs/2303.13373) | 本文利用最新的自然语言处理技术，基于ClimaText数据集，微调ClimateBert transformer和BERT模型，成功检测气候变化相关句子，为金融监管机构和投资者提供更准确和一致的气候相关金融风险信息，从而有望支持更可持续的金融系统。 |
| [^20] | [Requirement Formalisation using Natural Language Processing and Machine Learning: A Systematic Review.](http://arxiv.org/abs/2303.13365) | 本文调查了自然语言处理和机器学习在需求规范化中的应用现状，发现启发式NLP方法是自动RF中最常用的NLP技术，机器学习方法则包括基于分类和聚类的技术。本文总结了现有的NLP和ML技术在RF领域中的较大进展，并指出了在应用中面临的挑战和未来研究的方向。 |
| [^21] | [POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery.](http://arxiv.org/abs/2303.13357) | POTTER是一种针对人体网格恢复任务的纯变换器架构，其有效的池化注意力模块可以提高性能并降低计算成本。此外，集成HR数据流可以用于恢复更精确的人体网格。 |
| [^22] | [Revealing Weaknesses of Vietnamese Language Models Through Unanswerable Questions in Machine Reading Comprehension.](http://arxiv.org/abs/2303.13355) | 本文通过机器阅读理解的无法回答问题的表现，揭示越南语言模型的弱点，并提出新方向。我们同时还发现现有越南机器阅读理解基准存在人工问题，需迫切寻求新的高质量基准评估进展。 |
| [^23] | [Planning for Complex Non-prehensile Manipulation Among Movable Objects by Interleaving Multi-Agent Pathfinding and Physics-Based Simulation.](http://arxiv.org/abs/2303.13352) | 本文通过交织多智能体路径规划和基于物理模拟的规划，使机器人能够考虑复杂的机器人-物体和物体-物体的交互，并成功实现可动物体的复杂非握取式操作。 |
| [^24] | [Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI.](http://arxiv.org/abs/2303.13336) | 此文介绍了音频扩散模型，重点讨论了两个活跃任务：文本到语音和语音增强，并对实验结果进行了比较和讨论。 |
| [^25] | [Decentralized Adversarial Training over Graphs.](http://arxiv.org/abs/2303.13326) | 本文研究了在图上的去中心化对抗性训练，利用扩散学习的方法，开发了一种对抗性训练框架，增强了多个代理的鲁棒性以对抗攻击。 |
| [^26] | [DARE-GRAM : Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices.](http://arxiv.org/abs/2303.13325) | 本文提出了一种简单而有效的无监督领域自适应回归方法 DARE-GRAM，利用伪逆低秩性在由两个领域的伪逆格拉姆矩阵生成的选择子空间中对齐尺度和角度，解决了回归问题中标记源数据集与未标记目标数据集之间的领域差异问题。 |
| [^27] | [QDP: Learning to Sequentially Optimise Quasi-Static and Dynamic Manipulation Primitives for Robotic Cloth Manipulation.](http://arxiv.org/abs/2303.13320) | QDP方法优化了准静态和动态操作基元的运动速度和挑选放置位置等参数，有助于处理家庭布物材料范围中的挑战。 |
| [^28] | [Leveraging Foundation Models for Clinical Text Analysis.](http://arxiv.org/abs/2303.13314) | 本研究提出了一个NLP框架，利用预训练的Transformer模型从临床数据中提取与传染病相关的关键信息，该方法在评估中优于标准方法。 |
| [^29] | [Innovation Slowdown: Decelerating Concept Creation and Declining Originality in New Technological Concepts.](http://arxiv.org/abs/2303.13300) | 人类智力的局限性导致技术概念创造放缓和原创性下降，因此建议开发和实施创造性人工智能增强创新过程。 |
| [^30] | [Reckoning with the Disagreement Problem: Explanation Consensus as a Training Objective.](http://arxiv.org/abs/2303.13299) | 本文提出针对后续特征归因方法所存在的不同解释的问题，引入PEAR损失项，从而提升模型的解释一致性，达到模型行为的可理解和可信任。 |
| [^31] | [Unsupervised Deep Probabilistic Approach for Partial Point Cloud Registration.](http://arxiv.org/abs/2303.13290) | 提出了一种无监督深度概率点云配准框架UDPReg，使用高斯混合模型的后验概率分布来进行点云的匹配，在混合权重约束下使用Sinkhorn算法进行分布级对应关系预测，同时设计了自一致性、交一致性和局部对比三种分布一致性损失函数，实现了在无标签信息下的自动学习和配准。 |
| [^32] | [Frame-Level Multi-Label Playing Technique Detection Using Multi-Scale Network and Self-Attention Mechanism.](http://arxiv.org/abs/2303.13272) | 本文提出了一种利用多尺度网络和自注意机制进行帧级多标签演奏技巧检测的方法，并应用于古筝演奏。该方法在多声部独奏音乐中的IPT检测方面表现明显优于现有方法。 |
| [^33] | [Transforming Radiance Field with Lipschitz Network for Photorealistic 3D Scene Stylization.](http://arxiv.org/abs/2303.13232) | 本文介绍了一个名为LipRF的学习框架，可以使用利普希茨映射将预训练的NeRF的外观表示转换为具有视觉一致性和逼真度的风格化场景。 |
| [^34] | [Enriching Neural Network Training Dataset to Improve Worst-Case Performance Guarantees.](http://arxiv.org/abs/2303.13228) | 本文提出了一种算法来丰富神经网络训练数据集，从而减少最坏情况的违规，并提高其性能保证。 |
| [^35] | [Fairness-guided Few-shot Prompting for Large Language Models.](http://arxiv.org/abs/2303.13217) | 本文提出了一种新的搜索策略-FairPrompt，在保证公正性的前提下，通过评估提示预测偏差，确定近似最优的提示，从而改进大型语言模型的上下文学习性能，实验表明该方法在准确性和公正性方面均优于现有方法。 |
| [^36] | [A Case Study on AI Engineering Practices: Developing an Autonomous Stock Trading System.](http://arxiv.org/abs/2303.13216) | 本文介绍了一个利用机器学习实现自主股票交易的案例研究，使用了稳定的AI工程实践来确保系统的质量和改善开发过程。 |
| [^37] | [Complementary Pseudo Multimodal Feature for Point Cloud Anomaly Detection.](http://arxiv.org/abs/2303.13194) | 本研究提出了一种基于伪多模特征的点云异常检测方法，聚合了局部几何信息和全局语义信息，并能够达到较高的AU-ROC值。 |
| [^38] | [Extended High Utility Pattern Mining: An Answer Set Programming Based Framework and Applications.](http://arxiv.org/abs/2303.13191) | 本文介绍了一种基于答案集编程的扩展高效用模式挖掘框架，可以处理多种新型效用准则，对于实际应用有着重要意义。 |
| [^39] | [CMG-Net: An End-to-End Contact-Based Multi-Finger Dexterous Grasping Network.](http://arxiv.org/abs/2303.13182) | 本文提出了CMG-Net，一种基于接触点的多指抓取网络，可以高效地预测抓取姿势和手型配置，用于在杂乱环境中抓取未知物体。使用合成数据训练的模型在实际机器人中表现非常好，并优于现有最佳工作。 |
| [^40] | [Design Patterns for AI-based Systems: A Multivocal Literature Review and Pattern Repository.](http://arxiv.org/abs/2303.13173) | 本研究概述了基于人工智能系统的设计模式，旨在提高软件质量和系统性能。从51篇学术论文中选择了43个设计模式，分别提供了针对AI-based systems的解决方案，并分为数据处理、建模、决策和实现四个类别。该研究为研究者和从业者提供了一个完整的、易于访问的设计模式库。 |
| [^41] | [An elementary belief function logic.](http://arxiv.org/abs/2303.13168) | 本文提出了一种简单的信念函数逻辑，通过在MEL顶部添加{\L}ukasiewicz逻辑来实现，允许更自然的语义。 |
| [^42] | [Adiabatic replay for continual learning.](http://arxiv.org/abs/2303.13157) | 本研究提出了一种称为绝热重放的重放连续学习策略，它能够有选择性地重放与新数据相似的样本，从而提高学习效率。 |
| [^43] | [Defining Quality Requirements for a Trustworthy AI Wildflower Monitoring Platform.](http://arxiv.org/abs/2303.13151) | 本文将ISO25000质量模型应用于野花监测深度学习平台的实际案例研究，提出了一种结构化的方法来定义质量要求。 |
| [^44] | [MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models.](http://arxiv.org/abs/2303.13126) | 本论文提出了一种名为SNB的方法，该方法集成了两个文本指导扩散模型的噪声预测，以实现更可控的图像生成，同时不需要额外的训练或注释。 |
| [^45] | [A Simple Explanation for the Phase Transition in Large Language Models with List Decoding.](http://arxiv.org/abs/2303.13112) | 本文提供了一个对大语言模型中相变现象的简单解释，利用列表译码器建模，它能够保证在LLM低于临界阈值时错误候选序列数的期望保持有界，而在高于该阈值时呈指数增长。 |
| [^46] | [Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer.](http://arxiv.org/abs/2303.13099) | 本研究提出了一种多领域批处理和代理梯度转移的语义多视角模型，可以解决任务导向对话系统中的意图检测和诱导新意图的问题，在Open Intent Induction中有显著的性能提升。 |
| [^47] | [CP$^3$: Channel Pruning Plug-in for Point-based Networks.](http://arxiv.org/abs/2303.13097) | CP$^3$是一种基于点云网络的通道剪枝插件，它利用点云和PNN的特性，提出了坐标增强的通道重要度指标以实现通道剪枝的最优选择。 |
| [^48] | [CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching.](http://arxiv.org/abs/2303.13076) | 本研究提出了CORA框架，通过Region Prompting和Anchor Pre-Matching解决使用CLIP进行OVD训练时遇到的分布差异和目标定位等难点。 |
| [^49] | [Open-Vocabulary Object Detection using Pseudo Caption Labels.](http://arxiv.org/abs/2303.13040) | 该论文提出了一种名为伪标注标签（PCL）的简单有效方法，在开放词汇物体检测中使用图像字幕模型生成描述对象实例的字幕标签，以提取关于新颖对象的丰富知识。 |
| [^50] | [SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization.](http://arxiv.org/abs/2303.13035) | 研究通过引入软提示嵌入，提出Soft Prompt-Based Calibration (SPeC)管道，来减轻输入变量对输出多样性的影响，降低性能变异. 此方法不仅比大语言模型(LLM)性能稳定，而且在临床笔记摘要任务上表现优于最先进的模型. |
| [^51] | [Preference-Aware Constrained Multi-Objective Bayesian Optimization.](http://arxiv.org/abs/2303.13034) | PAC-MOO是一个偏好感知的约束多目标贝叶斯优化方法，可以有效地解决在黑盒目标函数和从业者指定的目标偏好下，大部分输入空间是不可行的约束多目标优化问题。 |
| [^52] | [Failure-tolerant Distributed Learning for Anomaly Detection in Wireless Networks.](http://arxiv.org/abs/2303.13015) | 本文提出了一种名为“Tol-FL”的新方法，通过结合扁平和星型拓扑结构的优势，增强了分布式网络中的变异检测性能和可靠性。 |
| [^53] | [Semantic Image Attack for Visual Model Diagnosis.](http://arxiv.org/abs/2303.13010) | 本文提出了一种新的基于对抗攻击的方法——语义图像攻击（SIA），可以提供语义对抗图像以便进行模型诊断、可解释性和鲁棒性。 |
| [^54] | [A Survey of Historical Learning: Learning Models with Learning History.](http://arxiv.org/abs/2303.12992) | 本文综述了“历史学习：带有学习历史的学习模型”这个主题，涵盖历史类型、功能部分和存储形式三个方面，是首个系统研究利用各种历史统计数据进行深度神经网络训练的综述论文。 |
| [^55] | [Uncertainty Calibration for Counterfactual Propensity Estimation in Recommendation.](http://arxiv.org/abs/2303.12973) | 本文提出了多种不确定性校准技术，以改进推荐系统中倾向性估计的效果。经过实验验证，校准后的IPS估计器在Coat和yahoo数据集上表现更好。 |
| [^56] | [Continuous Indeterminate Probability Neural Network.](http://arxiv.org/abs/2303.12964) | 本文提出了CIPNN模型，该模型能够推导出连续潜在随机变量的解析解，同时提出了CIPAE自编码器，并通过可视化潜在随机变量的方法验证了模型的有效性。 |
| [^57] | [The Shaky Foundations of Clinical Foundation Models: A Survey of Large Language Models and Foundation Models for EMRs.](http://arxiv.org/abs/2303.12961) | 本论文回顾了超过80个在非成像 EMR 数据上训练的基础模型，发现这些模型大多范围有限、训练集有限，且评估指标未对其对医疗系统贡献提供有意义见解。因此，本研究提出了一种更接近于医疗保健重要指标的医疗基础模型效益评估框架。 |
| [^58] | [Variantional autoencoder with decremental information bottleneck for disentanglement.](http://arxiv.org/abs/2303.12959) | 本论文提出了一种逐步减少信息瓶颈的变分自编码器方法，使用去纠缠不变变换来平衡去纠缠和重构保真度，避免信息扩散问题。 |
| [^59] | [Reinforcement Learning with Exogenous States and Rewards.](http://arxiv.org/abs/2303.12957) | 该研究提出了一种强化学习的方法，通过将MDP分解为外生和内生两个部分，优化内生奖励，在状态空间的内生和外生状态空间没有事先给出的情况下，提出了正确的算法进行自动发现。 |
| [^60] | [Use of Federated Learning and Blockchain towards Securing Financial Services.](http://arxiv.org/abs/2303.12944) | 本文介绍了如何利用联邦学习和区块链技术提高金融服务的安全性和隐私性，探讨了在实现这些技术时所面临的挑战和潜在解决方案。 |
| [^61] | [A Survey on Explainable Artificial Intelligence for Network Cybersecurity.](http://arxiv.org/abs/2303.12942) | 这篇论文综述了网络驱动的威胁和问题的系统分类，审查了网络系统中的可解释人工智能在网络安全中的最新技术，并勾画了未来研究的有前途的方向。 |
| [^62] | [Real-World Community-in-the-Loop Smart Video Surveillance -- A Case Study at a Community College.](http://arxiv.org/abs/2303.12934) | 本文研究了一种基于社区的智能视频监控系统在社区学院的实际测试平台上的设计和部署，着重解决了实时高准确度的视频分析处理、云系统基础架构和移动应用的开发、以及保护个人隐私和数据安全等方面的挑战。 |
| [^63] | [Revisiting the Fragility of Influence Functions.](http://arxiv.org/abs/2303.12922) | 本文研究了影响函数的脆弱性，并提出在非凸条件下使用深层模型和更复杂数据集来解决这一问题。 |
| [^64] | [Deep learning-based stereo camera multi-video synchronization.](http://arxiv.org/abs/2303.12916) | 本研究比较了不同深度学习系统的优劣，并证明一些系统具备足够高效且具有一般性来完成立体相机多视频同步任务。这个技术将有望降低整个系统的成本、重量和大小，并允许在建立这样的系统时更加灵活。 |
| [^65] | [Feature Reduction Method Comparison Towards Explainability and Efficiency in Cybersecurity Intrusion Detection Systems.](http://arxiv.org/abs/2303.12891) | 本文对于用于网络安全入侵检测系统的三种特征降维方法进行了比较，结果表明使用蝙蝠算法的相关特征选择（CFS-BA）是最为高效的，仅用最佳随机森林信息增益（RF-IG）模型55%的时间构建，同时实现了99.99%的准确性。 |
| [^66] | [A dynamic risk score for early prediction of cardiogenic shock using machine learning.](http://arxiv.org/abs/2303.12888) | 该研究基于深度学习开发了一个风险分层工具CShock，旨在针对急性失代偿性心力衰竭和/或心肌梗死患者预测心源性休克的发作。 |
| [^67] | [Human Uncertainty in Concept-Based AI Systems.](http://arxiv.org/abs/2303.12872) | 本研究探讨了人类不确定性对概念驱动AI系统的影响，通过控制数据集的干扰因素，分析了现有模型的处理方法。 |
| [^68] | [Salient Span Masking for Temporal Understanding.](http://arxiv.org/abs/2303.12860) | 本文介绍了一种用于时间理解的显著性跨度掩蔽技术，通过引入Temporal Span Masking中间训练并与Salient Span Masking结合使用，有效提高多个时间任务的性能及表示效果。 |
| [^69] | [Semi-Oblivious Chase Termination for Linear Existential Rules: An Experimental Study.](http://arxiv.org/abs/2303.12851) | 本文关注线性存在规则的半无记忆追赶算法及其终止问题，提供了一个名为LERT的工具，通过实验评估并得到多个理论和实验结果，并发掘了新的有趣研究方向。 |
| [^70] | [The power and limitations of learning quantum dynamics incoherently.](http://arxiv.org/abs/2303.12834) | 本文证明在不相干框架下，通过浅层测量只能有效地学习低纠缠门。虽然该类框架为我们在不同物理平台之间转移量子过程提供了方法，但也存在一定的局限性。 |
| [^71] | [Data-Driven Leader-following Consensus for Nonlinear Multi-Agent Systems against Composite Attacks: A Twins Layer Approach.](http://arxiv.org/abs/2303.12823) | 本文提出了一种面向复合攻击的非线性多智能体系统的数据驱动从属一致性控制方法，利用双生层方法解决了分布式估计和弹性分布式跟踪控制两部分任务，具有较强的鲁棒性和可扩展性。 |
| [^72] | [Co-Speech Gesture Synthesis using Discrete Gesture Token Learning.](http://arxiv.org/abs/2303.12822) | 该论文提出了一个两阶段的机制，使用离散的编码方式来解决合成共性语言手势中的不确定性问题，采用VAE和自回归变压器模型进行学习，能够生成多样化和逼真的共性语言手势。 |
| [^73] | [Towards A Visual Programming Tool to Create Deep Learning Models.](http://arxiv.org/abs/2303.12821) | DeepBlocks是一款可视化编程工具，允许DL开发人员设计、训练和评估模型，而无需依赖特定的编程语言。其通过构建典型模型结构实现其工作原理，结果表明开发人员可以视觉上设计复杂的DL架构。 |
| [^74] | [From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding.](http://arxiv.org/abs/2303.12816) | 本文提出了一个用于实现参数高效的知识图谱嵌入的深度网络，通过增加深度克服因采用低维实体表示而导致的模型精度下降和模型参数减少有限的问题。 |
| [^75] | [SignCRF: Scalable Channel-agnostic Data-driven Radio Authentication System.](http://arxiv.org/abs/2303.12811) | SignCRF是一个可扩展的无频道数据驱动射频认证平台，能够高精度地识别无线设备，不受移动性带来的动态信道影响。 |
| [^76] | [PACO: Provocation Involving Action, Culture, and Oppression.](http://arxiv.org/abs/2303.12808) | 该研究利用现有的印度WhatsApp帖子数据集，创造了一个可以从WhatsApp帖子中识别挑衅句子的模型PACO，并利用该模型可以防止可能的歧视或暴力事件。 |
| [^77] | [Granular-ball Optimization Algorithm.](http://arxiv.org/abs/2303.12807) | 粒球优化算法(GBO)是一种新的多粒度优化算法，可以通过引入粒球计算来提高全局搜索能力和收敛速度，实验结果表明，在这些方面它比现有的最先进的算法表现更优。 |
| [^78] | [Evolving Populations of Diverse RL Agents with MAP-Elites.](http://arxiv.org/abs/2303.12803) | 本文介绍了一种尝试在进化计算与强化学习中相结合解决机器人控制问题的方法，并通过改进ME算法提高了效率和多样性，但也出现了一些常见强化学习算法的限制。 |
| [^79] | [IoT Device Identification Based on Network Communication Analysis Using Deep Learning.](http://arxiv.org/abs/2303.12800) | 内部网络中允许连接的IoT设备和未知的IoT设备的识别变得越发重要。本研究提出了一种基于深度学习的自动识别方法，可以不需对网络通信进行复杂的特征处理。 |
| [^80] | [Time Series as Images: Vision Transformer for Irregularly Sampled Time Series.](http://arxiv.org/abs/2303.12799) | 本文提出了一种新颖的方法，将不规则采样的时间序列转换为线图像，并适应强大的视觉transformer进行时间序列分类。该方法简化了算法设计，具有通用性，并展示了在多个医疗和人体活动数据集上明显优于最先进的专业算法的表现。 |
| [^81] | [An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters.](http://arxiv.org/abs/2303.12797) | 本文提出一种基于进化的有向无环图的算法框架，用于自动生成高效且灵活的深度神经网络并优化相关的超参数。此框架可用于任何能够处理混合搜索空间的元启发式算法，并在时间序列预测数据集上表现出比已有模型更好的性能。 |
| [^82] | [An Analysis of Abstractive Text Summarization Using Pre-trained Models.](http://arxiv.org/abs/2303.12796) | 本文对使用预训练模型进行文本摘要的方法进行了评估，并在不同数据集上进行了比较，结果表明...... |
| [^83] | [Named Entity Recognition Based Automatic Generation of Research Highlights.](http://arxiv.org/abs/2303.12795) | 该研究使用命名实体识别技术自动生成研究亮点，探究其是否能提高生成亮点的质量。实验结果表明，增加命名实体信息可以提高生成亮点的性能。 |
| [^84] | [Text Semantics to Image Generation: A method of building facades design base on Stable Diffusion model.](http://arxiv.org/abs/2303.12755) | 本文提出了一种多网络结合的文本到建筑立面图像生成方法，通过 LoRA 训练方法微调稳定扩散模型和 ControlNet 模型的添加，大大提高了文本到建筑立面图像生成的可控性和稳定性，为后续建筑图像生成研究提供了基础。 |
| [^85] | [cTBL: Augmenting Large Language Models for Conversational Tables.](http://arxiv.org/abs/2303.12024) | 本论文提出了一种称为cTBL的方法，可以从表格中检索信息，并生成具有检索信息支撑的对话响应，其中使用了转换器编码器嵌入进行浓密表检索，可以获得更好的性能。 |
| [^86] | [CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning.](http://arxiv.org/abs/2303.10365) | CroSel是一种处理伪标签噪声的新方法，通过利用历史预测信息和一致性正则化项来准确识别部分标签数据的真实标签。 |
| [^87] | [CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment.](http://arxiv.org/abs/2303.05725) | CVT-SLR是一种新的手语识别模型，它采用基于对比视觉-文本变换和变分对齐的方法来充分利用跨模态知识，为解决手语识别中缺乏大规模可用数据集的问题提供了一种新的解决方案。 |
| [^88] | [Patch of Invisibility: Naturalistic Black-Box Adversarial Attacks on Object Detectors.](http://arxiv.org/abs/2303.04238) | 本文提出了一种基于GAN的无梯度物理对抗攻击方法，用于生成自然的对抗补丁，攻击物体检测器，具有实际应用价值。 |
| [^89] | [Attribution-Scores and Causal Counterfactuals as Explanations in Artificial Intelligence.](http://arxiv.org/abs/2303.02829) | 本文介绍了归属分数和因果反事实在解释人工智能中的应用，重点关注因果关系领域中的逻辑推理和分数计算。 |
| [^90] | [Real-Time Evaluation in Online Continual Learning: A New Hope.](http://arxiv.org/abs/2302.01047) | 该研究针对现实环境中的连续学习提出了一种实时评估方法，并在包含3900万个时间戳标记图像的大型数据集CLOC上进行了实验。结果表明，简单的基线模型胜过了最先进的CL方法，证明现有方法在现实环境下的适用性存在问题。 |
| [^91] | [DepGraph: Towards Any Structural Pruning.](http://arxiv.org/abs/2301.12900) | 这篇论文提出了一种通用且完全自动化的方法，称为“Dependency Graph”（DepGraph），用于解决任何结构剪枝，它明确模型层之间的依赖关系，以避免出现结构问题和显著的性能下降。 |
| [^92] | [Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization.](http://arxiv.org/abs/2301.07784) | 该论文提出了一种使用广义策略改进优先级来实现高效多目标学习的算法，从而通过主动学习策略，可以识别出每一时刻最有前途的偏好或目标，以更快地解决MORL问题，同时也可以识别出学习特定代理偏好的策略时最相关的历史经验。 |
| [^93] | [CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image.](http://arxiv.org/abs/2301.02232) | 本文介绍了一种可以将单张图像中物体的运动转移到未调整的3D模型中的神经网络方法，可以处理任意类别的对象，训练时只使用合成数据。 |
| [^94] | [CLIP is Also an Efficient Segmenter: A Text-Driven Approach for Weakly Supervised Semantic Segmentation.](http://arxiv.org/abs/2212.09506) | 本文提出了一种基于文本输入的新型WSSS框架CLIP-ES，利用图像级别标签进行弱监督语义分割，通过采用softmax函数、锐度驱动提示选择和同义词融合等策略，提高了WSSS的三个阶段的效率，并成功利用了CLIP的零样本能力，从而实现了高质量的分割掩模生成。 |
| [^95] | [ECON: Explicit Clothed humans Optimized via Normal integration.](http://arxiv.org/abs/2212.07422) | 结合隐式函数和显式身体模型，ECON方法成功地重建了着装立体人体，并可以恢复宽松服装等自由形式表面。 |
| [^96] | [Applications of statistical causal inference in software engineering.](http://arxiv.org/abs/2211.11482) | 本文回顾了在软件工程领域的32篇相关研究，总结出统计因果推断方法的应用较新，研究社区相对分散。 |
| [^97] | [NAR-Former: Neural Architecture Representation Learning towards Holistic Attributes Prediction.](http://arxiv.org/abs/2211.08024) | 本文提出了一种神经架构表示模型NAR-Former来全面估计神经网络架构的属性，通过标记器和多阶段融合变压器构建紧凑的向量表示，并采用信息流一致性增强和架构一致性损失进行有效的模型训练。实验证实了该模型在准确性和延迟估计方面与最先进的方法相比取得了有竞争力的性能。 |
| [^98] | [Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings.](http://arxiv.org/abs/2210.16848) | 本文提出了一种改进词嵌入的方法，分别为将更多上下文信息纳入Skip-gram框架和提出一个基于先验同义词知识和加权向量分布的静态嵌入后处理装配方法。这两种方法经由外部和内部任务的检验，能够大幅度超越基准线。 |
| [^99] | [Convex and Nonconvex Sublinear Regression with Application to Data-driven Learning of Reach Sets.](http://arxiv.org/abs/2210.01919) | 本文提出了使用支撑函数对紧致集合进行学习的方法，并提出了两种算法进行子线性回归，分别为凸规划和非凸规划。本文在受控动态到达集的应用中进行了实验。 |
| [^100] | [Omnigrok: Grokking Beyond Algorithmic Data.](http://arxiv.org/abs/2210.01117) | 本文通过分析神经网络的损失景观，发现训练和测试损失之间的不匹配是grokking的原因，提出了“LU机制”，并成功诱导了算法数据集的grokking和消除了其grokking现象。它们的dramatic grokking依赖于表示学习。 |
| [^101] | [Leveraging the Potential of Novel Data in Power Line Communication of Electricity Grids.](http://arxiv.org/abs/2209.12693) | 本文提出了两个基于宽带电力线通信测量的首个数据集FiN-1和FiN-2，共收集了130亿个数据点。这些数据可以应用于资产管理、电网状态可视化和预测，以解决电力网在向可再生能源过渡和复杂负载配置的情况下面临的新挑战。 |
| [^102] | [Normalizing Flows for Interventional Density Estimation.](http://arxiv.org/abs/2209.06203) | 本研究提出了一种名为干预正则化流的全参数深度学习方法，用于从观测数据中估计干预后的潜在结果密度。该方法结合了nuisance flow和target flow，并开发了一个易于处理的优化目标，以实现有效和双重稳健的估计。实验证明，该方法优于现有最先进的方法。 |
| [^103] | [Diffusion Models in Vision: A Survey.](http://arxiv.org/abs/2209.04747) | 扩散模型是视觉中的新兴主题，其生成样本的质量和多样性受到广泛欣赏。本综述介绍了三种通用的扩散建模方法以及各种算法和架构方面的讨论，并总结比较了扩散模型与其他最先进的生成模型的性能。 |
| [^104] | [Diffusion Models: A Comprehensive Survey of Methods and Applications.](http://arxiv.org/abs/2209.00796) | 本调研综合总结了扩散模型的方法和应用研究，包括高效采样、似然估计与特殊结构数据处理，介绍了扩散模型与其他生成模型相结合的潜力并回顾了其在不同领域的广泛应用，为进一步研究提出了可能的研究方向。 |
| [^105] | [Towards Global Optimality in Cooperative MARL with the Transformation And Distillation Framework.](http://arxiv.org/abs/2207.11143) | 本文研究了采用分散策略的MARL算法在梯度下降优化器下的次最优性，并提出了转化与蒸馏框架，该框架可以将多智能体MDP转化为单智能体MDP以实现分散执行。 |
| [^106] | [On the Importance and Applicability of Pre-Training for Federated Learning.](http://arxiv.org/abs/2206.11488) | 研究发现，在联邦学习中使用预训练可以改善性能，尤其是在非独立同分布客户数据的情况下。此外，使用合成数据或客户端数据进行分散式预训练也可以显著改善性能，并且不同的技术可以相互补充以进一步提高性能。 |
| [^107] | [FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER.](http://arxiv.org/abs/2205.15448) | FeatER是一个有效的网络，用于通过人体结构信息特征图的 transformer 处理来实现人物重建，并解决了既能够处理位置敏感性特征图又能减少计算和内存需求的问题。 |
| [^108] | [Evaluating the Robustness of Deep Reinforcement Learning for Autonomous Policies in a Multi-agent Urban Driving Environment.](http://arxiv.org/abs/2112.11947) | 本论文提出了一个基准测试框架，用于评估和比较深度强化学习算法在单个和多个智能体自主驾驶环境中的性能。同时，提出了一种混合算法以提高深度强化学习策略在多代理驾驶环境中的鲁棒性。 |
| [^109] | [AMRA*: Anytime Multi-Resolution Multi-Heuristic A*.](http://arxiv.org/abs/2110.05328) | AMRA*是一种任意多分辨率多启发式A*算法，能够在跨越多个分辨率搜索时横跨大范围的无障碍区域和狭窄通道，可以尽可能使用粗分辨率找到解法并逐步细化，具有较好的性能表现。 |
| [^110] | [Graph-Based Decoding for Task Oriented Semantic Parsing.](http://arxiv.org/abs/2109.04587) | 本研究探索了一种替代语义解析任务的方法，将其作为依赖解析任务进行表述并应用了基于图的解码技术，有望提高部分注释数据的效率和数据使用效率。 |
| [^111] | [Pseudo-Euclidean Attract-Repel Embeddings for Undirected Graphs.](http://arxiv.org/abs/2106.09671) | 通过将节点嵌入拟欧几里得空间中来消除传递性假设，使得拟欧几里得嵌入可以高效地压缩网络，允许多种最近邻的概念，并可以被插入到现有的模型中，以实现更好的链接预测。 |
| [^112] | [A* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks.](http://arxiv.org/abs/2102.04518) | 本文提出了一种使用深度Q网络学习启发式函数，通过只进行一次前向传递计算相邻节点的转移成本和启发式值之和，并在不显式生成这些子节点的情况下指导搜索的Q*搜索算法，以大幅减少计算时间。在魔方问题上的实验表明，该方法能够高效地解决具有大动作空间的问题。 |
| [^113] | [Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity.](http://arxiv.org/abs/2004.12908) | 本文提出了一种名为RELL的方法，利用知识蒸馏和知识保持正则化方法，以协同集成在不同任务上独立学习的表示，在准线性复杂度下实现了前向和后向传递。实验结果表明，在各种基准数据集上，RELL的表现优于现有的最先进方法，尤其是在存在灾难性遗忘的情况下，能够显着改善反向传递。 |

# 详细

[^1]: 教学视频中任务结构的学习和验证

    Learning and Verification of Task Structure in Instructional Videos. (arXiv:2303.13519v1 [cs.CV])

    [http://arxiv.org/abs/2303.13519](http://arxiv.org/abs/2303.13519)

    该论文提出了一种新的预训练视频模型——VideoTaskformer。通过从整体上学习表示来验证视频的正确执行，并预测下一步骤。同时，论文也提出了两个新的检测教学视频中错误的基准。

    

    鉴于在线教学视频数量庞大，从视频中学习多步骤任务模型的多样性是一个诱人的目标。本文引入了一种新的预训练视频模型——VideoTaskformer，专注于表示教学视频的语义和结构。我们使用一种简单有效的目标来对VideoTaskformer进行预训练：从教学视频中随机屏蔽的步骤预测弱监督的文本标签（遮盖步骤建模）。与先前学习局部步骤表示的方法相比，我们的方法涉及全局学习，利用整个周围任务的视频作为上下文。从这些学习到的表示中，我们可以验证一个未见过的视频是否正确执行给定的任务，以及预测在给定步骤之后可能采取哪些步骤。我们引入了两个新的基准来检测教学视频中的错误，以验证是否存在异常步骤并检查步骤是否按正确的顺序执行。

    Given the enormous number of instructional videos available online, learning a diverse array of multi-step task models from videos is an appealing goal. We introduce a new pre-trained video model, VideoTaskformer, focused on representing the semantics and structure of instructional videos. We pre-train VideoTaskformer using a simple and effective objective: predicting weakly supervised textual labels for steps that are randomly masked out from an instructional video (masked step modeling). Compared to prior work which learns step representations locally, our approach involves learning them globally, leveraging video of the entire surrounding task as context. From these learned representations, we can verify if an unseen video correctly executes a given task, as well as forecast which steps are likely to be taken after a given step. We introduce two new benchmarks for detecting mistakes in instructional videos, to verify if there is an anomalous step and if steps are executed in the rig
    
[^2]: 改善开放式词汇检测中特征对齐的三种方法

    Three ways to improve feature alignment for open vocabulary detection. (arXiv:2303.13518v1 [cs.CV])

    [http://arxiv.org/abs/2303.13518](http://arxiv.org/abs/2303.13518)

    该论文提出了三种方法来改善开放式词汇检测中特征对齐的问题，包括增强文本嵌入、修改特征金字塔网络和检测头部、以及采用自学习方法。这些方法可以有效缓解模型在未见类上的性能问题。

    

    零样本开放式词汇检测的核心问题在于如何对齐视觉和文本特征，以使检测器在未见类上表现良好。之前的方法从头开始训练特征金字塔和检测头部，这破坏了预训练期间建立的视觉-文本特征对齐，并且难以防止语言模型忘记未见类。我们提出了三种方法来缓解这些问题。首先，使用简单的方案来增强文本嵌入，防止过度拟合到训练期间见到的少量类别，并同时节省内存和计算。其次，修改特征金字塔网络和检测头部，包括可训练门控快捷方式，这鼓励视觉-文本特征对齐，并确保在检测训练开始时实现特征对齐。最后，采用自学习方法利用更大的图像-文本对语料库，从而改善无人类注释类别的检测性能。

    The core problem in zero-shot open vocabulary detection is how to align visual and text features, so that the detector performs well on unseen classes. Previous approaches train the feature pyramid and detection head from scratch, which breaks the vision-text feature alignment established during pretraining, and struggles to prevent the language model from forgetting unseen classes.  We propose three methods to alleviate these issues. Firstly, a simple scheme is used to augment the text embeddings which prevents overfitting to a small number of classes seen during training, while simultaneously saving memory and computation. Secondly, the feature pyramid network and the detection head are modified to include trainable gated shortcuts, which encourages vision-text feature alignment and guarantees it at the start of detection training. Finally, a self-training approach is used to leverage a larger corpus of image-text pairs thus improving detection performance on classes with no human an
    
[^3]: 利用人类反馈解决模糊任务：MineRL BASALT 2022竞赛回顾。

    Towards Solving Fuzzy Tasks with Human Feedback: A Retrospective of the MineRL BASALT 2022 Competition. (arXiv:2303.13512v1 [cs.AI])

    [http://arxiv.org/abs/2303.13512](http://arxiv.org/abs/2303.13512)

    MineRL BASALT 2022竞赛旨在促进开发算法来利用人类反馈解决Minecraft中难以指定奖励函数的任务，帮助推动此方向的研究。

    

    为了促进基础模型通过人类反馈的微调研究，我们在NeurIPS 2022举办了MineRL BASALT竞赛，该竞赛旨在寻求用于解决Minecraft中难以明确定义奖励函数的任务的算法。通过此竞赛，我们致力于促进利用人类反馈作为学习所需行为的渠道的算法的发展。我们描述了竞赛，并概述了顶尖解决方案。最后，我们探讨了竞赛的影响和未来的改进方向。

    To facilitate research in the direction of fine-tuning foundation models from human feedback, we held the MineRL BASALT Competition on Fine-Tuning from Human Feedback at NeurIPS 2022. The BASALT challenge asks teams to compete to develop algorithms to solve tasks with hard-to-specify reward functions in Minecraft. Through this competition, we aimed to promote the development of algorithms that use human feedback as channels to learn the desired behavior. We describe the competition and provide an overview of the top solutions. We conclude by discussing the impact of the competition and future directions for improvement.
    
[^4]: 神经预设：用于颜色风格转移的新技术

    Neural Preset for Color Style Transfer. (arXiv:2303.13511v1 [cs.CV])

    [http://arxiv.org/abs/2303.13511](http://arxiv.org/abs/2303.13511)

    本文提出了一种名为神经预设的技术，通过确定性神经颜色映射方法（DNCM）和两阶段流水线实现高质量的颜色风格转移，并且具有各种优势。

    

    本文提出了一种名为神经预设的技术，用于解决现有颜色风格转移方法的限制，包括可视化伪影、大量内存需求和缓慢的风格切换速度。本方法基于两个核心设计。首先，我们提出了一种确定性神经颜色映射方法（DNCM），通过自适应的颜色映射矩阵在每个像素上进行一致的操作，避免了伪影，并支持具有小内存占用的高分辨率输入。其次，我们通过将任务分为颜色归一化和风格化两个阶段来开发一个两阶段流水线，可以通过将颜色风格作为预设提取，并在归一化的输入图像上重复使用它们来实现有效的风格切换。由于存在成对数据集的问题，我们描述了如何通过自监督策略训练神经预设模型。通过全面的评估展示了神经预设相对于现有方法的各种优势。此外，我们展示了我们训练的模型可以自然地支持多个风格，并在定量和定性性能上实现了最新的表现。

    In this paper, we present a Neural Preset technique to address the limitations of existing color style transfer methods, including visual artifacts, vast memory requirement, and slow style switching speed. Our method is based on two core designs. First, we propose Deterministic Neural Color Mapping (DNCM) to consistently operate on each pixel via an image-adaptive color mapping matrix, avoiding artifacts and supporting high-resolution inputs with a small memory footprint. Second, we develop a two-stage pipeline by dividing the task into color normalization and stylization, which allows efficient style switching by extracting color styles as presets and reusing them on normalized input images. Due to the unavailability of pairwise datasets, we describe how to train Neural Preset via a self-supervised strategy. Various advantages of Neural Preset over existing methods are demonstrated through comprehensive evaluations. Besides, we show that our trained model can naturally support multipl
    
[^5]: DreamBooth3D：主体驱动的文本到3D生成

    DreamBooth3D: Subject-Driven Text-to-3D Generation. (arXiv:2303.13508v1 [cs.CV])

    [http://arxiv.org/abs/2303.13508](http://arxiv.org/abs/2303.13508)

    DreamBooth3D是一种可从3-6张图片中生成主体特定3D素材的方法，通过结合文本到图像模型和文本到3D生成模型，使用一种三阶段的优化策略来产生高质量3D素材。

    

    我们提出了DreamBooth3D方法，该方法可以从3-6个随意拍摄的主体图像个性化生成文本到3D模型。我们的方法将个性化文本到图像模型(DreamBooth)与文本到3D生成(DreamFusion)的最新进展相结合。我们发现，简单地将这些方法组合起来无法产生令人满意的主体特定的3D素材，因为个性化的文本到图像模型会过度拟合主体图像的输入视角。我们通过三阶段的优化策略解决了这个问题，其中我们同时利用了神经辐射场的3D一致性和文本到图像模型的个性化能力。我们的方法可以产生高质量、主体特定的3D素材，具有文本驱动的修改，如新颖的姿势、颜色和属性，这些修改在主体的任何输入图像中都没有看到。

    We present DreamBooth3D, an approach to personalize text-to-3D generative models from as few as 3-6 casually captured images of a subject. Our approach combines recent advances in personalizing text-to-image models (DreamBooth) with text-to-3D generation (DreamFusion). We find that naively combining these methods fails to yield satisfactory subject-specific 3D assets due to personalized text-to-image models overfitting to the input viewpoints of the subject. We overcome this through a 3-stage optimization strategy where we jointly leverage the 3D consistency of neural radiance fields together with the personalization capability of text-to-image models. Our method can produce high-quality, subject-specific 3D assets with text-driven modifications such as novel poses, colors and attributes that are not seen in any of the input images of the subject.
    
[^6]: TriPlaneNet：一种EG3D反演的编码器

    TriPlaneNet: An Encoder for EG3D Inversion. (arXiv:2303.13497v1 [cs.CV])

    [http://arxiv.org/abs/2303.13497](http://arxiv.org/abs/2303.13497)

    本研究介绍了一种实时方法TriPlaneNet，通过直接利用EG3D生成模型的三平面表示，建立在一个用于潜在编码的前馈卷积编码器上，并扩展了一个完全卷积的三平面数值偏移预测器，旨在弥合现有的GAN反演方法的差距。

    

    最近，基于NeRF的GAN取得了很大的进展，在高分辨率和高保真度的生成建模中引入了许多方法，可以进行新颖的视角渲染。与此同时，为了能够重新渲染或修改现有的图像或视频，必须解决一个反问题。尽管对于2D GAN反演而言，通用的基于优化的方法取得了成功，但对于3D GAN反演，这些方法可能无法产生3D一致的渲染。而像StyleGAN这样的快速编码器技术，可能也不太吸引人，因为它们缺乏身份保留能力。在我们的工作中，我们介绍了一种实时方法，通过直接利用为EG3D生成模型引入的三平面表示，弥合了这两种方法之间的差距。具体而言，我们建立在一个用于潜在编码的前馈卷积编码器上，并扩展了一个完全卷积的三平面数值偏移预测器。正如我们的工作所显示的那样，渲染结果相似。

    Recent progress in NeRF-based GANs has introduced a number of approaches for high-resolution and high-fidelity generative modeling of human heads with a possibility for novel view rendering. At the same time, one must solve an inverse problem to be able to re-render or modify an existing image or video. Despite the success of universal optimization-based methods for 2D GAN inversion, those, applied to 3D GANs, may fail to produce 3D-consistent renderings. Fast encoder-based techniques, such as those developed for StyleGAN, may also be less appealing due to the lack of identity preservation. In our work, we introduce a real-time method that bridges the gap between the two approaches by directly utilizing the tri-plane representation introduced for EG3D generative model. In particular, we build upon a feed-forward convolutional encoder for the latent code and extend it with a fully-convolutional predictor of tri-plane numerical offsets. As shown in our work, the renderings are similar in
    
[^7]: MAE预前置训练对于亿级预训练的有效性

    The effectiveness of MAE pre-pretraining for billion-scale pretraining. (arXiv:2303.13496v1 [cs.CV])

    [http://arxiv.org/abs/2303.13496](http://arxiv.org/abs/2303.13496)

    本文在计算机视觉领域提出了一种自我监督的MAE技术预前置训练方法，该方法适用于亿级预训练规模，并可显著提高模型收敛性和下游转移性能。

    

    本文重新审视了计算机视觉中用于视觉识别任务的标准预训练-微调范式。通常情况下，最先进的基础模型使用数十亿张图像的大规模（弱）监督数据集进行预训练。我们引入了一个额外的预前置训练阶段，它使用了自我监督的MAE技术来初始化模型。虽然MAE技术仅被证明能够与模型大小相缩放，但我们发现它也可以随数据集大小缩放。因此，我们基于MAE的预前置训练可同时适用于训练基础模型的模型和数据规模。预前置训练在一系列模型规模（参数数百万到数十亿）和数据集大小（图像数百万到数十亿）上一致提高了模型收敛性和下游转移性能，且我们还测量了其在10个不同的视觉识别任务上的有效性，包括图像分类、视频识别和目标检测。

    This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video reco
    
[^8]: 注意力！（不）专注代理人的动态认知逻辑模型

    Attention! Dynamic Epistemic Logic Models of (In)attentive Agents. (arXiv:2303.13494v1 [cs.AI])

    [http://arxiv.org/abs/2303.13494](http://arxiv.org/abs/2303.13494)

    本文提出了一种更一般化的动态认知逻辑模型，允许代理人关注一些原子公式的子集，并扩展了该框架，以解释无注意瞎视现象。

    

    注意力是限制和选择我们观察到的信息的关键认知能力。本文提出了一种基于动态认知逻辑（DEL）的注意力模型，其中代理人要么完全专注，要么完全不关注。我们提出了一种更一般化的模型，允许代理人关注一些原子公式的子集。我们介绍了命题注意力的相应逻辑，并证明了其公理系统的准确性和完备性。进一步地，我们扩展了该框架，以解释无注意瞎视现象。

    Attention is the crucial cognitive ability that limits and selects what information we observe. Previous work by Bolander et al. (2016) proposes a model of attention based on dynamic epistemic logic (DEL) where agents are either fully attentive or not attentive at all. While introducing the realistic feature that inattentive agents believe nothing happens, the model does not represent the most essential aspect of attention: its selectivity. Here, we propose a generalization that allows for paying attention to subsets of atomic formulas. We introduce the corresponding logic for propositional attention, and show its axiomatization to be sound and complete. We then extend the framework to account for inattentive agents that, instead of assuming nothing happens, may default to a specific truth-value of what they failed to attend to (a sort of prior concerning the unattended atoms). This feature allows for a more cognitively plausible representation of the inattentional blindness phenomenon
    
[^9]: 使用示范加速强化学习与规划：一份综述

    Boosting Reinforcement Learning and Planning with Demonstrations: A Survey. (arXiv:2303.13489v1 [cs.LG])

    [http://arxiv.org/abs/2303.13489](http://arxiv.org/abs/2303.13489)

    强化学习中一种减少试错的方法是使用示范，本文综述了如何使用示范来促进学习决策模型的应用，并提供了基于ManiSkill机器人学习基准的示范生成和利用管道的实例。

    

    尽管强化学习最近取得了巨大的成功，但是这种试错式的学习方法在复杂环境下可能效率低下。与此相反，使用示范可以让智能体受益于专家的知识，而无需探索最佳行动。在本综述中，我们讨论了在顺序决策中使用示范的优点，以及学习为基础的决策制定范式（例如，强化学习和规划在学习的模型中如何应用示范），以及如何在各种情况下收集示范。此外，我们还举了一个实际的示范生成和利用管道的例子，并在最近提出的ManiSkill机器人学习基准中进行了说明。

    Although reinforcement learning has seen tremendous success recently, this kind of trial-and-error learning can be impractical or inefficient in complex environments. The use of demonstrations, on the other hand, enables agents to benefit from expert knowledge rather than having to discover the best action to take through exploration. In this survey, we discuss the advantages of using demonstrations in sequential decision making, various ways to apply demonstrations in learning-based decision making paradigms (for example, reinforcement learning and planning in the learned models), and how to collect the demonstrations in various scenarios. Additionally, we exemplify a practical pipeline for generating and utilizing demonstrations in the recently proposed ManiSkill robot learning benchmark.
    
[^10]: NS3D: 三维物体及关系的神经符号基础

    NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations. (arXiv:2303.13483v1 [cs.CV])

    [http://arxiv.org/abs/2303.13483](http://arxiv.org/abs/2303.13483)

    本文提出了一种神经符号基础的三维物体及关系的框架NS3D，可以有效的推理出三维场景中的复杂语义，同时在多个基准数据集上展示了领先或具有竞争性的性能。

    

    为了进行各种人工智能任务，如视觉对话和具体操作，将三维场景中的物体属性和关系基础化是必要的。然而，三维领域的变化性导致了两个基本的挑战：1）标注成本昂贵；2）复杂的三维基础语言。因此，模型的基础要求是数据高效、能够推广到不同的数据分布和任务以及具有看不到语义形式的基础复杂语义（例如，视点锚定和多对象引用）。为了解决这些挑战，我们提出了NS3D，一种三维基础的神经符号框架。NS3D通过利用大型的语言到代码模型将语言转化为具有分层结构的程序。程序中的不同功能模块被实现为神经网络。值得注意的是，NS3D通过引入能够有效推理出视点锚定、多对象引用和其他三维场景中复杂语义的功能模块，扩展了以前的神经符号视觉推理方法。当NS3D在几个基准数据集上进行测试时，我们展示了NS3D与现有方法相比的最先进或具有竞争力的性能。

    Grounding object properties and relations in 3D scenes is a prerequisite for a wide range of artificial intelligence tasks, such as visually grounded dialogues and embodied manipulation. However, the variability of the 3D domain induces two fundamental challenges: 1) the expense of labeling and 2) the complexity of 3D grounded language. Hence, essential desiderata for models are to be data-efficient, generalize to different data distributions and tasks with unseen semantic forms, as well as ground complex language semantics (e.g., view-point anchoring and multi-object reference). To address these challenges, we propose NS3D, a neuro-symbolic framework for 3D grounding. NS3D translates language into programs with hierarchical structures by leveraging large language-to-code models. Different functional modules in the programs are implemented as neural networks. Notably, NS3D extends prior neuro-symbolic visual reasoning methods by introducing functional modules that effectively reason ab
    
[^11]: TactoFind：一种纯触觉物品获取系统

    TactoFind: A Tactile Only System for Object Retrieval. (arXiv:2303.13482v1 [cs.RO])

    [http://arxiv.org/abs/2303.13482](http://arxiv.org/abs/2303.13482)

    TactoFind是一个纯触觉物品获取系统，它可以使用手指上的触控传感器，在没有任何视觉反馈的情况下定位、识别和抓取新的物体。

    

    本文研究了在缺乏视觉感知、未知物体形状以及物体可以自由移动的场景下进行物品检索的问题。成功的解决方案需要定位自由物体、识别特定的物体实例并使用触觉反馈来抓取已识别的物体。与摄影机可观察整个场景的视觉不同，触觉传感器是局部的，并且仅观察与操纵器接触的场景部分。此外，通过触觉传感器收集信息需要在触摸表面施加力，这可能会扰乱场景本身。因此，触摸感知需要通过时间上的精细探索和信息集成来进行推理。我们提出了一个系统，可以利用手指触摸传感器上的稀疏触觉反馈，在没有任何视觉反馈的情况下定位、识别和抓取新的物体。

    We study the problem of object retrieval in scenarios where visual sensing is absent, object shapes are unknown beforehand and objects can move freely, like grabbing objects out of a drawer. Successful solutions require localizing free objects, identifying specific object instances, and then grasping the identified objects, only using touch feedback. Unlike vision, where cameras can observe the entire scene, touch sensors are local and only observe parts of the scene that are in contact with the manipulator. Moreover, information gathering via touch sensors necessitates applying forces on the touched surface which may disturb the scene itself. Reasoning with touch, therefore, requires careful exploration and integration of information over time -- a challenge we tackle. We present a system capable of using sparse tactile feedback from fingertip touch sensors on a dexterous hand to localize, identify and grasp novel objects without any visual feedback. Videos are available at https://ta
    
[^12]: 幕后制作：面向可学习游戏引擎的研究

    Plotting Behind the Scenes: Towards Learnable Game Engines. (arXiv:2303.13472v1 [cs.CV])

    [http://arxiv.org/abs/2303.13472](http://arxiv.org/abs/2303.13472)

    本文提出了一个方法，可以从单眼注释视频中训练出类似游戏引擎的神经模型，这个模型被称为可学习游戏引擎(LGE)，它可以通过指定高级和低级操作序列来玩游戏，并且解锁了导演模式，可以使用高级约束条件控制代理。

    

    游戏引擎在计算机图形学中是强大的工具，但是它们的开发成本也是十分巨大的。本文提出了一个框架，可以从单眼注释视频中训练出类似游戏引擎的神经模型。该结果被称为Learnable Game Engine (LGE)，可以维护场景、物体和其中的代理状态，并且可以从可控制的视角渲染环境。类似于游戏引擎，它模拟了游戏的逻辑和底层物理规则，使用户可以通过指定高级和低级操作序列来玩游戏。最引人注目的是，我们的LGE解锁了导演模式，用户通过标注高层次的动作和目标来控制代理。这要求学习“游戏人工智能”，由我们的动画模型封装，以使用高级约束条件导航场景、与对手对战，设计赢得游戏的策略。

    Game engines are powerful tools in computer graphics. Their power comes at the immense cost of their development. In this work, we present a framework to train game-engine-like neural models, solely from monocular annotated videos. The result-a Learnable Game Engine (LGE)-maintains states of the scene, objects and agents in it, and enables rendering the environment from a controllable viewpoint. Similarly to a game engine, it models the logic of the game and the underlying rules of physics, to make it possible for a user to play the game by specifying both high- and low-level action sequences. Most captivatingly, our LGE unlocks the director's mode, where the game is played by plotting behind the scenes, specifying high-level actions and goals for the agents in the form of language and desired states. This requires learning "game AI", encapsulated by our animation model, to navigate the scene using high-level constraints, play against an adversary, devise the strategy to win a point. T
    
[^13]: 从临床笔记中提取康复锻炼信息：基于规则和机器学习自然语言处理技术的比较

    Extracting Physical Rehabilitation Exercise Information from Clinical Notes: a Comparison of Rule-Based and Machine Learning Natural Language Processing Techniques. (arXiv:2303.13466v1 [cs.CL])

    [http://arxiv.org/abs/2303.13466](http://arxiv.org/abs/2303.13466)

    本文提出了一种基于规则的自然语言处理算法，用于从临床笔记中提取卒中患者治疗过程的锻炼信息，并与几个小型机器学习模型进行比较。在足够的数据可用的情况下，我们的算法在提取一半的概念方面优于这些模型，并且每个概念的个体运动描述可以分配二进制标签，并且F值不低于0.75。这些算法表现出了准确提取临床笔记中康复治疗锻炼信息的前景。

    

    康复锻炼在卒中后患者的康复过程中扮演着至关重要的角色。通过个性化治疗和电子健康记录，医疗保健提供者可以使康复过程更加高效。在预测建模为患者分配治疗计划之前，自动化方法是从非结构化电子健康记录中提取康复锻炼信息所必需的。我们引入了一个基于规则的自然语言处理算法来注释卒中患者的治疗过程，并将其与几个小型机器学习模型进行比较。我们发现，在足够的数据可用的情况下，我们的算法在提取一半的概念方面优于这些模型，并且每个概念的个体运动描述可以分配二进制标签，并且F值不低于0.75。在这些算法可以部署到无标签文档之前，需要进行更多的研究，但定制的基于规则的自然语言处理算法表现出了准确提取临床笔记中康复治疗锻炼信息的前景。

    Physical rehabilitation plays a crucial role in the recovery process of post-stroke patients. By personalizing therapies for patients leveraging predictive modeling and electronic health records (EHRs), healthcare providers can make the rehabilitation process more efficient. Before predictive modeling can provide decision support for the assignment of treatment plans, automated methods are necessary to extract physical rehabilitation exercise information from unstructured EHRs. We introduce a rule-based natural language processing algorithm to annotate therapeutic procedures for stroke patients and compare it to several small machine learning models. We find that our algorithm outperforms these models in extracting half of the concepts where sufficient data is available, and individual exercise descriptions can be assigned binary labels with an f-score of no less than 0.75 per concept. More research needs to be done before these algorithms can be deployed on unlabeled documents, but cu
    
[^14]: 使用分层行为探索的深度强化学习在对话生成中的应用

    Deep RL with Hierarchical Action Exploration for Dialogue Generation. (arXiv:2303.13465v1 [cs.CL])

    [http://arxiv.org/abs/2303.13465](http://arxiv.org/abs/2303.13465)

    本篇论文提出了一种新的方法，通过分层行为探索，从多个奖励函数中进行离线学习，并成功地解决了在对话生成中行为采样效率低下的问题，可以更好地识别人类情感细节。

    

    自然语言的行为空间极其庞大，因此在对话生成中，近似动态规划必须使用策略改进和行为采样。但是，由于有价值的回应非常稀疏，因此使用随机采样的贪心策略效率低下。本文提出了双粒度的 Q-function 并通过探索最有前途的回应类别来缓解这个局限性。该算法从识别人类情感细节的多个奖励函数中进行离线学习。实证研究表明，该算法优于基线方法。

    Conventionally, since the natural language action space is astronomical, approximate dynamic programming applied to dialogue generation involves policy improvement with action sampling. However, such a practice is inefficient for reinforcement learning (RL) because the eligible (high action value) responses are very sparse, and the greedy policy sustained by the random sampling is flabby. This paper shows that the performance of dialogue policy positively correlated with sampling size by theoretical and experimental. We introduce a novel dual-granularity Q-function to alleviate this limitation by exploring the most promising response category to intervene in the sampling. It extracts the actions following the grained hierarchy, which can achieve the optimum with fewer policy iterations. Our approach learns in the way of offline RL from multiple reward functions designed to recognize human emotional details. Empirical studies demonstrate that our algorithm outperforms the baseline metho
    
[^15]: DDT：一种基于扩散驱动变压器的从视频中恢复人体网格的框架

    DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video. (arXiv:2303.13397v1 [cs.CV])

    [http://arxiv.org/abs/2303.13397](http://arxiv.org/abs/2303.13397)

    提出了一种基于扩散驱动变压器的视频 HMR 框架（DDT），它旨在从输入序列中解码特定的运动模式，增强运动平滑性和时间一致性，并输出所有帧的人体网格，使得 DDT 更适用于时间效率至关重要的实际应用。

    

    人体网格恢复（HMR）为各种实际应用提供了丰富的人体信息，例如游戏、人机交互和虚拟现实。与单一图像方法相比，基于视频的方法可以利用时间信息通过融合人体运动先验进一步提高性能。然而，像 VIBE 这样的多对多方法存在运动平滑性和时间一致性的挑战。而像 TCMR 和 MPS-Net 这样的多对一方法则依赖于未来帧，在推理过程中是非因果和时间效率低下的。为了解决这些挑战，提出了一种新的基于扩散驱动变压器的视频 HMR 框架（DDT）。DDT 旨在从输入序列中解码特定的运动模式，增强运动平滑性和时间一致性。作为一种多对多方法，DDT 的解码器输出所有帧的人体网格，使 DDT 更适用于时间效率至关重要的实际应用。

    Human mesh recovery (HMR) provides rich human body information for various real-world applications such as gaming, human-computer interaction, and virtual reality. Compared to single image-based methods, video-based methods can utilize temporal information to further improve performance by incorporating human body motion priors. However, many-to-many approaches such as VIBE suffer from motion smoothness and temporal inconsistency. While many-to-one approaches such as TCMR and MPS-Net rely on the future frames, which is non-causal and time inefficient during inference. To address these challenges, a novel Diffusion-Driven Transformer-based framework (DDT) for video-based HMR is presented. DDT is designed to decode specific motion patterns from the input sequence, enhancing motion smoothness and temporal consistency. As a many-to-many approach, the decoder of our DDT outputs the human mesh of all the frames, making DDT more viable for real-world applications where time efficiency is cruc
    
[^16]: 可移动物体操纵规划：如何决定哪些物体被放置在哪里、按什么顺序放置以及如何放置

    Planning for Manipulation among Movable Objects: Deciding Which Objects Go Where, in What Order, and How. (arXiv:2303.13385v1 [cs.RO])

    [http://arxiv.org/abs/2303.13385](http://arxiv.org/abs/2303.13385)

    本文扩展了M4M算法并提出了E-M4M算法，在受约束的环境中排列多个物体，并且能够确定可行的推动序列，表现优于M4M算法。

    

    本文研究在充满障碍物且空间狭小的三维工作区域中，机器人如何进行扫描-抓取-搬运风格的物品操纵任务。我们提出了M4M算法来解决这个问题，该算法解决了多智能体路径规划(MAPF)的抽象，确定了需要移动的物体和它们的目的地，并采用非预抓取推式规划器来计算机器人如何实现这些重新排列，并使用一个刚体物理模拟器来检查行动是否符合编码在问题中的物理约束。然而，M4M算法在规划过程中贪心地选择有效的推动操作，并且如果需要重新排列多个物体，则不考虑它们之间的顺序。此外，M4M算法也无法考虑导致不同物品重新排列和推动的其他MAPF解决方案。本文扩展了M4M算法，并提出了Enhanced-M4M (E-M4M)算法，一种基于系统图搜索的求解器，可搜索推动操作的顺序和不同的MAPF解决方案。E-M4M算法能够确定在受约束的空间中排列多个物体的可行推动序列。我们在一组具有随机物体配置的模拟环境数据集上评估了我们的规划器，并显示E-M4M算法在成功率和规划时间方面优于M4M算法。

    We are interested in pick-and-place style robot manipulation tasks in cluttered and confined 3D workspaces among movable objects that may be rearranged by the robot and may slide, tilt, lean or topple. A recently proposed algorithm, M4M, determines which objects need to be moved and where by solving a Multi-Agent Pathfinding MAPF abstraction of this problem. It then utilises a nonprehensile push planner to compute actions for how the robot might realise these rearrangements and a rigid body physics simulator to check whether the actions satisfy physics constraints encoded in the problem. However, M4M greedily commits to valid pushes found during planning, and does not reason about orderings over pushes if multiple objects need to be rearranged. Furthermore, M4M does not reason about other possible MAPF solutions that lead to different rearrangements and pushes. In this paper, we extend M4M and present Enhanced-M4M (E-M4M) -- a systematic graph search-based solver that searches over ord
    
[^17]: 大语言模型在教育中的实际和伦理挑战：一项系统文献综述

    Practical and Ethical Challenges of Large Language Models in Education: A Systematic Literature Review. (arXiv:2303.13379v1 [cs.CL])

    [http://arxiv.org/abs/2303.13379](http://arxiv.org/abs/2303.13379)

    LLMs在教育中有自动生成和分析文本内容的潜力。然而，这些创新的实际性和伦理性存在担忧，需要考虑技术可行性、隐私、平等和善意等因素。

    

    基于大语言模型（LLMs）开发的教育技术创新显示出自动生成和分析文本内容的潜力。虽然已经开发了各种创新来自动化各种教育任务（例如，生成问题、提供反馈和评分），但对这些创新的实际性和伦理性存在担忧。这些担忧可能会阻碍未来研究和在真实教育环境中采用基于LLMs的创新。为了解决这个问题，我们对118篇自2017年以来发表的同行评议论文进行了系统的文献综述，以确定使用LLMs自动化和支持教育任务的当前研究状态。通过评估其技术可行性、模型性能、可复制性、系统透明度、隐私、平等和善意，还确定了LLMs创新的实际和伦理挑战。

    Educational technology innovations that have been developed based on large language models (LLMs) have shown the potential to automate the laborious process of generating and analysing textual content. While various innovations have been developed to automate a range of educational tasks (e.g., question generation, feedback provision, and essay grading), there are concerns regarding the practicality and ethicality of these innovations. Such concerns may hinder future research and the adoption of LLMs-based innovations in authentic educational contexts. To address this, we conducted a systematic literature review of 118 peer-reviewed papers published since 2017 to pinpoint the current state of research on using LLMs to automate and support educational tasks. The practical and ethical challenges of LLMs-based innovations were also identified by assessing their technological readiness, model performance, replicability, system transparency, privacy, equality, and beneficence. The findings 
    
[^18]: GPT-4在医学挑战问题上的能力

    Capabilities of GPT-4 on Medical Challenge Problems. (arXiv:2303.13375v1 [cs.CL])

    [http://arxiv.org/abs/2303.13375](http://arxiv.org/abs/2303.13375)

    本论文对最先进的LLM——GPT-4在医学能力考试和基准数据集上进行了全面评估，结果显示其表现出色，有助于医学相关领域的研究和应用。

    

    大型语言模型（LLMs）已经在各个领域展示了惊人的自然语言理解和生成能力，包括医学。我们对一项最先进的LLM——GPT-4在医学能力考试和基准数据集上进行了全面评估。GPT-4是一个通用模型，没有经过针对医学问题的训练或设计用于解决临床任务。我们的分析涵盖了美国临床能力评估和授权考核计划（USMLE）的两组官方练习材料。我们还评估了在MultiMedQA基准数据集上的表现。除了测量模型的性能，还进行了实验来研究包含文本和图像的测试问题对模型性能的影响，探索训练期间内容记忆的可能性，并研究概率校准在高风险应用中的重要性。

    Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications 
    
[^19]: 用ClimaText微调ClimateBERT transformer解析气候相关金融风险披露

    Fine-tuning ClimateBert transformer with ClimaText for the disclosure analysis of climate-related financial risks. (arXiv:2303.13373v1 [cs.CL])

    [http://arxiv.org/abs/2303.13373](http://arxiv.org/abs/2303.13373)

    本文利用最新的自然语言处理技术，基于ClimaText数据集，微调ClimateBert transformer和BERT模型，成功检测气候变化相关句子，为金融监管机构和投资者提供更准确和一致的气候相关金融风险信息，从而有望支持更可持续的金融系统。

    

    近年来，金融机构，特别是个人和机构投资者，对公司报告气候相关金融风险的需求不断增长。为了识别这些类型的风险，公司可以在财务和非财务报告中短期内披露大量文本信息，特别是为了响应不断通过的规定。为此，本文应用最先进的自然语言处理技术来实现气候变化在文本语料库中的检测。我们使用迁移学习来微调两个变换器模型，BERT和ClimateBert，这是一个最近发布的基于DistillRoBERTa模型的模型，专门为气候文本分类量身定制。这两个算法基于变换器架构，能够学习文本中单词之间的上下文关系。我们在新的ClimaText数据集上对两个模型进行微调，该数据集由超过2000家公司披露其气候相关风险的报告组成。我们的实验表明，微调模型在识别ClimaText中与气候变化相关的句子方面优于原始模型。本研究的结果有潜力支持金融监管机构、投资者和其他利益相关者，为他们提供更准确、一致的关于气候相关金融风险的信息，从而为一个更可持续的金融系统做出贡献。

    In recent years there has been a growing demand from financial agents, especially from particular and institutional investors, for companies to report on climate-related financial risks. A vast amount of information, in text format, can be expected to be disclosed in the short term by firms in order to identify these types of risks in their financial and non financial reports, particularly in response to the growing regulation that is being passed on the matter. To this end, this paper applies state-of-the-art NLP techniques to achieve the detection of climate change in text corpora. We use transfer learning to fine-tune two transformer models, BERT and ClimateBert -a recently published DistillRoBERTa-based model that has been specifically tailored for climate text classification-. These two algorithms are based on the transformer architecture which enables learning the contextual relationships between words in a text. We carry out the fine-tuning process of both models on the novel Cl
    
[^20]: 自然语言处理和机器学习在需求规范化中的应用：一篇系统综述

    Requirement Formalisation using Natural Language Processing and Machine Learning: A Systematic Review. (arXiv:2303.13365v1 [cs.CL])

    [http://arxiv.org/abs/2303.13365](http://arxiv.org/abs/2303.13365)

    本文调查了自然语言处理和机器学习在需求规范化中的应用现状，发现启发式NLP方法是自动RF中最常用的NLP技术，机器学习方法则包括基于分类和聚类的技术。本文总结了现有的NLP和ML技术在RF领域中的较大进展，并指出了在应用中面临的挑战和未来研究的方向。

    

    软件开发方法的改进吸引了开发人员在需求工程领域自动化需求规范化（RF）中应用自然语言处理（NLP）和机器学习（ML），报告了应用NLP和ML在减少自然语言编写的需求不确定性和不完整性方面的潜在优势。本文的目标是调查和分类现有的NLP和ML在RF上的工作，识别该领域的挑战并提供有前途的未来研究方向。为了实现这一目标，我们进行了系统文献综述，选取了来自常用库的257篇论文。通过定义包含和排除标准来过滤搜索结果，并选择了47项相关研究，时间跨度在2012年至2022年之间。我们发现启发式NLP方法是自动RF中最常用的NLP技术，主要应用于结构化数据，而机器学习方法则包括基于分类和聚类的技术。本文总结了现有的NLP和ML技术在RF领域中的较大进展，并指出了在应用中面临的挑战和未来研究的方向。

    Improvement of software development methodologies attracts developers to automatic Requirement Formalisation (RF) in the Requirement Engineering (RE) field. The potential advantages by applying Natural Language Processing (NLP) and Machine Learning (ML) in reducing the ambiguity and incompleteness of requirement written in natural languages is reported in different studies. The goal of this paper is to survey and classify existing work on NLP and ML for RF, identifying challenges in this domain and providing promising future research directions. To achieve this, we conducted a systematic literature review to outline the current state-of-the-art of NLP and ML techniques in RF by selecting 257 papers from common used libraries. The search result is filtered by defining inclusion and exclusion criteria and 47 relevant studies between 2012 and 2022 are selected. We found that heuristic NLP approaches are the most common NLP techniques used for automatic RF, primary operating on structured 
    
[^21]: POTTER: 基于池化注意力变换器的高效人体网格恢复

    POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery. (arXiv:2303.13357v1 [cs.CV])

    [http://arxiv.org/abs/2303.13357](http://arxiv.org/abs/2303.13357)

    POTTER是一种针对人体网格恢复任务的纯变换器架构，其有效的池化注意力模块可以提高性能并降低计算成本。此外，集成HR数据流可以用于恢复更精确的人体网格。

    

    在单目图像中，变换器架构已经在人体网格恢复（HMR）上取得SOTA表现。然而，这种性能的提升是以巨大的内存和计算开销为代价的。为了实现实际应用中精确的人体网格重建，需要一种轻量级和高效的模型。在本文中，我们提出了一种纯变换器架构，名为POTTER（池化注意力变换器），用于单图像HMR任务。我们提出了一种高效的池化注意力模块，可以显著降低内存和计算成本，而不会牺牲性能。此外，我们设计了一种新的变换器架构，通过集成高分辨率（HR）数据流进行HMR任务。来自HR流的高分辨率局部和全局特征可以用于恢复更精确的人体网格。我们的POTTER仅需要7个英伟达显卡，超越了SOTA方法METRO，同时具有更高的精度和更好的性能表现。

    Transformer architectures have achieved SOTA performance on the human mesh recovery (HMR) from monocular images. However, the performance gain has come at the cost of substantial memory and computational overhead. A lightweight and efficient model to reconstruct accurate human mesh is needed for real-world applications. In this paper, we propose a pure transformer architecture named POoling aTtention TransformER (POTTER) for the HMR task from single images. Observing that the conventional attention module is memory and computationally expensive, we propose an efficient pooling attention module, which significantly reduces the memory and computational cost without sacrificing performance. Furthermore, we design a new transformer architecture by integrating a High-Resolution (HR) stream for the HMR task. The high-resolution local and global features from the HR stream can be utilized for recovering more accurate human mesh. Our POTTER outperforms the SOTA method METRO by only requiring 7
    
[^22]: 通过机器阅读理解的无法回答问题揭示越南语言模型的弱点

    Revealing Weaknesses of Vietnamese Language Models Through Unanswerable Questions in Machine Reading Comprehension. (arXiv:2303.13355v1 [cs.CL])

    [http://arxiv.org/abs/2303.13355](http://arxiv.org/abs/2303.13355)

    本文通过机器阅读理解的无法回答问题的表现，揭示越南语言模型的弱点，并提出新方向。我们同时还发现现有越南机器阅读理解基准存在人工问题，需迫切寻求新的高质量基准评估进展。

    

    尽管多语言模型在单语言环境下的语言能力受到了很大限制，但研究人员仍然不得不依靠多语言模型来开发越南文机器阅读理解的最先进系统。这种研究困难是由于越南语言模型开发的高质量作品数量有限。为了鼓励在这个研究领域中进行更多的工作，我们使用机器阅读理解的下游任务，对当前越南单语言模型的语言弱点和优势进行了全面分析。从分析结果中，我们提出了发展越南语言模型的新方向。除了这个主要贡献外，我们还成功地揭示了越南机器阅读理解基准中存在的人工问题，并建议迫切需要新的高质量基准来跟踪越南机器阅读理解的进展。此外，我们还演示了越南机器阅读理解中无法回答的问题如何暴露语言建模的弱点，并评估模型的性能。

    Although the curse of multilinguality significantly restricts the language abilities of multilingual models in monolingual settings, researchers now still have to rely on multilingual models to develop state-of-the-art systems in Vietnamese Machine Reading Comprehension. This difficulty in researching is because of the limited number of high-quality works in developing Vietnamese language models. In order to encourage more work in this research field, we present a comprehensive analysis of language weaknesses and strengths of current Vietnamese monolingual models using the downstream task of Machine Reading Comprehension. From the analysis results, we suggest new directions for developing Vietnamese language models. Besides this main contribution, we also successfully reveal the existence of artifacts in Vietnamese Machine Reading Comprehension benchmarks and suggest an urgent need for new high-quality benchmarks to track the progress of Vietnamese Machine Reading Comprehension. Moreov
    
[^23]: 通过交织多智能体路径规划和基于物理模拟的规划，规划可动物体的复杂非握取式操作

    Planning for Complex Non-prehensile Manipulation Among Movable Objects by Interleaving Multi-Agent Pathfinding and Physics-Based Simulation. (arXiv:2303.13352v1 [cs.RO])

    [http://arxiv.org/abs/2303.13352](http://arxiv.org/abs/2303.13352)

    本文通过交织多智能体路径规划和基于物理模拟的规划，使机器人能够考虑复杂的机器人-物体和物体-物体的交互，并成功实现可动物体的复杂非握取式操作。

    

    在高混杂度环境中，实现物体的操作可能需要考虑物体间的潜在接触。我们聚焦于从架子上取下目标物体的 pick-and-place 操作，其中必须重新排列一些“可动”物体才能解决任务。我们的重点是允许机器人考虑非握取的重新排列行动，这些行动会导致复杂的机器人-物体和物体-物体的交互，机器人可能同时移动多个物体，物体可能倾斜，相互靠在一起或倾倒。为了支持这一点，我们查询基于物理学的模拟器来前向模拟这些交互动力学，这使得计划过程中的行动评估变得非常耗时。为了使规划器可行，我们建立了可动物体操作领域和多智能体路径规划之间的联系，让我们将问题分解成两个阶段，我们的 M4M 算法迭代地进行。首先，我们使用多智能体路径规划生成一系列机器人运动，将物体运输到它们的目标位置，其次，我们使用基于物理模拟的模拟器评估生成的每个运动计划以找到最优计划。我们的实验表明，我们的方法在规划时间和成功率方面优于现有的最先进方法。

    Real-world manipulation problems in heavy clutter require robots to reason about potential contacts with objects in the environment. We focus on pick-and-place style tasks to retrieve a target object from a shelf where some `movable' objects must be rearranged in order to solve the task. In particular, our motivation is to allow the robot to reason over and consider non-prehensile rearrangement actions that lead to complex robot-object and object-object interactions where multiple objects might be moved by the robot simultaneously, and objects might tilt, lean on each other, or topple. To support this, we query a physics-based simulator to forward simulate these interaction dynamics which makes action evaluation during planning computationally very expensive. To make the planner tractable, we establish a connection between the domain of Manipulation Among Movable Objects and Multi-Agent Pathfinding that lets us decompose the problem into two phases our M4M algorithm iterates over. Firs
    
[^24]: 语音合成的音频扩散模型：基于生成AI的文本到语音和语音增强的概述

    Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI. (arXiv:2303.13336v1 [cs.SD])

    [http://arxiv.org/abs/2303.13336](http://arxiv.org/abs/2303.13336)

    此文介绍了音频扩散模型，重点讨论了两个活跃任务：文本到语音和语音增强，并对实验结果进行了比较和讨论。

    

    生成AI在各个领域表现出了惊人的性能，其中语音合成是一个有趣的方向。随着扩散模型成为最流行的生成模型，许多工作已经尝试了两个活跃任务：文本到语音和语音增强。本文对音频扩散模型进行了概述，这是对现有调查的补充，这些调查要么缺乏基于扩散的语音合成的最新进展，要么强调在多个领域应用扩散模型的整体情况。具体而言，本文首先简要介绍了音频和扩散模型的背景。对于文本到语音任务，我们将方法分为三类，基于扩散模型采用的阶段：声学模型、声码器和端到端框架。此外，我们通过将某些信号从输入语音中删除或添加来将各种语音增强任务进行分类。本文还涵盖了实验结果的比较和讨论。

    Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered 
    
[^25]: 基于图的去中心化对抗性训练

    Decentralized Adversarial Training over Graphs. (arXiv:2303.13326v1 [cs.LG])

    [http://arxiv.org/abs/2303.13326](http://arxiv.org/abs/2303.13326)

    本文研究了在图上的去中心化对抗性训练，利用扩散学习的方法，开发了一种对抗性训练框架，增强了多个代理的鲁棒性以对抗攻击。

    

    近年来，机器学习模型对抗攻击的漏洞引起了广泛关注。大多数现有研究都集中在独立单一代理学习者的行为上。相比之下，本文研究了在图上的对抗性训练，其中各个单独的代理会受到空间中不同强度的扰动。预期通过链接代理和可能在图上实现的攻击模型的异质性，协调整个团队的强大协同作用可以帮助增强鲁棒性。本文使用扩散学习的极小-极大公式，为多代理系统开发了一种去中心化的对抗性训练框架。我们分析了该方案在凸和非凸环境下的收敛特性，并说明了增强的鲁棒性对抗攻击。

    The vulnerability of machine learning models to adversarial attacks has been attracting considerable attention in recent years. Most existing studies focus on the behavior of stand-alone single-agent learners. In comparison, this work studies adversarial training over graphs, where individual agents are subjected to perturbations of varied strength levels across space. It is expected that interactions by linked agents, and the heterogeneity of the attack models that are possible over the graph, can help enhance robustness in view of the coordination power of the group. Using a min-max formulation of diffusion learning, we develop a decentralized adversarial training framework for multi-agent systems. We analyze the convergence properties of the proposed scheme for both convex and non-convex environments, and illustrate the enhanced robustness to adversarial attacks.
    
[^26]: 通过对齐逆格拉姆矩阵进行无监督领域自适应回归：DARE-GRAM

    DARE-GRAM : Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices. (arXiv:2303.13325v1 [cs.CV])

    [http://arxiv.org/abs/2303.13325](http://arxiv.org/abs/2303.13325)

    本文提出了一种简单而有效的无监督领域自适应回归方法 DARE-GRAM，利用伪逆低秩性在由两个领域的伪逆格拉姆矩阵生成的选择子空间中对齐尺度和角度，解决了回归问题中标记源数据集与未标记目标数据集之间的领域差异问题。

    

    无监督领域自适应回归(DAR)旨在解决回归问题中标记源数据集与未标记目标数据集之间的领域差异问题。最近的研究主要集中在学习深度特征编码器，通过减小源和目标特征之间的差异来实现。本文提出了一种不同于以往的DAR问题的观点，通过分析深度领域适应背景下线性回归器的闭式普通最小二乘(OLS)解法，提出了对特征嵌入空间进行对齐的不同方法，即通过对齐特征的逆格拉姆矩阵来进行对齐，这是由于逆Gram矩阵存在于OLS解法中并且Gram矩阵能够捕捉特征之间的相关性所促使。具体而言，我们提出了一种简单而有效地DAR方法，利用伪逆低秩性在由两个领域的伪逆格拉姆矩阵生成的选择子空间中对齐尺度和角度。我们在三个领域自适应数据集上评估了我们的方法。

    Unsupervised Domain Adaptation Regression (DAR) aims to bridge the domain gap between a labeled source dataset and an unlabelled target dataset for regression problems. Recent works mostly focus on learning a deep feature encoder by minimizing the discrepancy between source and target features. In this work, we present a different perspective for the DAR problem by analyzing the closed-form ordinary least square~(OLS) solution to the linear regressor in the deep domain adaptation context. Rather than aligning the original feature embedding space, we propose to align the inverse Gram matrix of the features, which is motivated by its presence in the OLS solution and the Gram matrix's ability to capture the feature correlations. Specifically, we propose a simple yet effective DAR method which leverages the pseudo-inverse low-rank property to align the scale and angle in a selected subspace generated by the pseudo-inverse Gram matrix of the two domains. We evaluate our method on three doma
    
[^27]: QDP：学习串行优化准静态和动态操作基元以进行机器人布料操作

    QDP: Learning to Sequentially Optimise Quasi-Static and Dynamic Manipulation Primitives for Robotic Cloth Manipulation. (arXiv:2303.13320v1 [cs.RO])

    [http://arxiv.org/abs/2303.13320](http://arxiv.org/abs/2303.13320)

    QDP方法优化了准静态和动态操作基元的运动速度和挑选放置位置等参数，有助于处理家庭布物材料范围中的挑战。

    

    预定义的操作基元被广泛用于布料操作。然而，诸如僵硬度或密度等布料属性可能会严重影响这些基元的性能。虽然现有解决方案已经解决了挑选和放置位置的参数化问题，但准静态和动态操作基元的速度或轨迹等因素的影响却被忽略了。选择适当的这些参数的值对于处理家庭布物物品中存在的材料范围非常重要。为了应对这一挑战，我们提出了Quasi-Dynamic Parameterisable（QDP）方法，它优化了参数，例如准静态和动态操作基元的运动速度以外，还包括挑选和放置位置。在这项工作中，我们利用串行强化学习的框架来顺序解耦组成基元的参数。为了评估该方法的有效性，我们专注于布料平整和堆叠的任务。

    Pre-defined manipulation primitives are widely used for cloth manipulation. However, cloth properties such as its stiffness or density can highly impact the performance of these primitives. Although existing solutions have tackled the parameterisation of pick and place locations, the effect of factors such as the velocity or trajectory of quasi-static and dynamic manipulation primitives has been neglected. Choosing appropriate values for these parameters is crucial to cope with the range of materials present in house-hold cloth objects. To address this challenge, we introduce the Quasi-Dynamic Parameterisable (QDP) method, which optimises parameters such as the motion velocity in addition to the pick and place positions of quasi-static and dynamic manipulation primitives. In this work, we leverage the framework of Sequential Reinforcement Learning to decouple sequentially the parameters that compose the primitives. To evaluate the effectiveness of the method we focus on the task of clo
    
[^28]: 利用基础模型进行临床文本分析

    Leveraging Foundation Models for Clinical Text Analysis. (arXiv:2303.13314v1 [cs.CL])

    [http://arxiv.org/abs/2303.13314](http://arxiv.org/abs/2303.13314)

    本研究提出了一个NLP框架，利用预训练的Transformer模型从临床数据中提取与传染病相关的关键信息，该方法在评估中优于标准方法。

    

    传染病是全球重要的公共卫生问题，从科学文献中提取相关信息可以促进有效的预防和治疗策略的制定。然而，大量的临床数据可用性会对信息抽取构成挑战。为了解决这个问题，本研究提出了一个自然语言处理（NLP）框架，使用预训练的Transformer模型在特定数据上进行微调，从自由文本临床数据中提取与传染病相关的关键信息。提出的框架包括三个组件：数据层用于从临床文本准备数据集，基础模型层用于实体提取，评估层用于性能分析。评估的结果表明，所提出的方法优于标准方法，并通过预训练的Transformer模型利用先前的知识使其有助于研究其他传染病。

    Infectious diseases are a significant public health concern globally, and extracting relevant information from scientific literature can facilitate the development of effective prevention and treatment strategies. However, the large amount of clinical data available presents a challenge for information extraction. To address this challenge, this study proposes a natural language processing (NLP) framework that uses a pre-trained transformer model fine-tuned on task-specific data to extract key information related to infectious diseases from free-text clinical data. The proposed framework includes three components: a data layer for preparing datasets from clinical texts, a foundation model layer for entity extraction, and an assessment layer for performance analysis. The results of the evaluation indicate that the proposed method outperforms standard methods, and leveraging prior knowledge through the pre-trained transformer model makes it useful for investigating other infectious disea
    
[^29]: 创新放缓：技术概念创造的减速及新技术概念原创性的下降

    Innovation Slowdown: Decelerating Concept Creation and Declining Originality in New Technological Concepts. (arXiv:2303.13300v1 [cs.SI])

    [http://arxiv.org/abs/2303.13300](http://arxiv.org/abs/2303.13300)

    人类智力的局限性导致技术概念创造放缓和原创性下降，因此建议开发和实施创造性人工智能增强创新过程。

    

    通过对之前的概念重用、重组和合成进行新技术概念的创造可能会导致概念空间的指数增长。然而，我们对由专利文本中超过400万个概念组成的大规模技术语义网络进行的统计分析发现，概念创造的步伐在持续减缓，并且新创造出的概念的原创性有所下降。这些趋势可以归因于人类智力在创新超出现有技术的拓展空间方面的局限。为了保持创新，我们建议开发和实施创造性人工智能，以增强创新过程的多个方面，包括学习、创造和评估。

    The creation of new technological concepts through design reuses, recombination, and synthesis of prior concepts to create new ones may lead to exponential growth of the concept space over time. However, our statistical analysis of a large-scale technology semantic network consisting of over four million concepts from patent texts found evidence of a persistent deceleration in the pace of concept creation and a decline in the originality of newly created concepts. These trends may be attributed to the limitations of human intelligence in innovating beyond an expanding space of prior art. To sustain innovation, we recommend the development and implementation of creative artificial intelligence that can augment various aspects of the innovation process, including learning, creation, and evaluation.
    
[^30]: 论如何解决不同解释方法所带来的问题：通过训练目标达成解释一致性

    Reckoning with the Disagreement Problem: Explanation Consensus as a Training Objective. (arXiv:2303.13299v1 [cs.LG])

    [http://arxiv.org/abs/2303.13299](http://arxiv.org/abs/2303.13299)

    本文提出针对后续特征归因方法所存在的不同解释的问题，引入PEAR损失项，从而提升模型的解释一致性，达到模型行为的可理解和可信任。

    

    随着深度神经网络逐渐在高风险领域中做出关键决策，监控和解释其行为成为必需。后续特征归因方法是一种常用的解释方法，可为输入中的每个特征分配得分，以衡量其对模型输出的影响。在实践中，这种方法的一个主要局限是不同的解释方法对于哪些特征更重要可能有不同的看法。本文提出了一种考虑不同解释方法的训练模型方法，我们引入Post hoc Explainer Agreement Regularization (PEAR)损失项以提升解释一致性。我们在三个数据集上观察到我们可以使用此损失项训练模型，以在未看见的数据上获得解释一致性的提升。

    As neural networks increasingly make critical decisions in high-stakes settings, monitoring and explaining their behavior in an understandable and trustworthy manner is a necessity. One commonly used type of explainer is post hoc feature attribution, a family of methods for giving each feature in an input a score corresponding to its influence on a model's output. A major limitation of this family of explainers in practice is that they can disagree on which features are more important than others. Our contribution in this paper is a method of training models with this disagreement problem in mind. We do this by introducing a Post hoc Explainer Agreement Regularization (PEAR) loss term alongside the standard term corresponding to accuracy, an additional term that measures the difference in feature attribution between a pair of explainers. We observe on three datasets that we can train a model with this loss term to improve explanation consensus on unseen data, and see improved consensus
    
[^31]: 无监督的深度概率点云配准方法

    Unsupervised Deep Probabilistic Approach for Partial Point Cloud Registration. (arXiv:2303.13290v1 [cs.CV])

    [http://arxiv.org/abs/2303.13290](http://arxiv.org/abs/2303.13290)

    提出了一种无监督深度概率点云配准框架UDPReg，使用高斯混合模型的后验概率分布来进行点云的匹配，在混合权重约束下使用Sinkhorn算法进行分布级对应关系预测，同时设计了自一致性、交一致性和局部对比三种分布一致性损失函数，实现了在无标签信息下的自动学习和配准。

    

    深度点云配准方法面临着局部重叠的挑战，并且依赖于标记数据。为了解决这些问题，我们提出了UDPReg，一种用于部分重叠点云的无监督深度概率配准框架。具体而言，我们首先采用网络从点云中学习高斯混合模型（GMM）的后验概率分布。为了处理局部点云配准，我们采用Sinkhorn算法在GMM的混合权重约束下预测分布级对应关系。为了实现无监督学习，我们设计了三种基于分布一致性的损失函数：自一致性、交一致性和局部对比。自一致性损失通过鼓励Euclidean和特征空间的GMM共享相同的后验分布来形成；交一致性损失源于两个部分重叠点云的点属于相同簇的事实，它们共享簇中心。

    Deep point cloud registration methods face challenges to partial overlaps and rely on labeled data. To address these issues, we propose UDPReg, an unsupervised deep probabilistic registration framework for point clouds with partial overlaps. Specifically, we first adopt a network to learn posterior probability distributions of Gaussian mixture models (GMMs) from point clouds. To handle partial point cloud registration, we apply the Sinkhorn algorithm to predict the distribution-level correspondences under the constraint of the mixing weights of GMMs. To enable unsupervised learning, we design three distribution consistency-based losses: self-consistency, cross-consistency, and local contrastive. The self-consistency loss is formulated by encouraging GMMs in Euclidean and feature spaces to share identical posterior distributions. The cross-consistency loss derives from the fact that the points of two partially overlapping point clouds belonging to the same clusters share the cluster cen
    
[^32]: 多尺度网络和自注意机制的帧级多标签演奏技巧检测

    Frame-Level Multi-Label Playing Technique Detection Using Multi-Scale Network and Self-Attention Mechanism. (arXiv:2303.13272v1 [cs.SD])

    [http://arxiv.org/abs/2303.13272](http://arxiv.org/abs/2303.13272)

    本文提出了一种利用多尺度网络和自注意机制进行帧级多标签演奏技巧检测的方法，并应用于古筝演奏。该方法在多声部独奏音乐中的IPT检测方面表现明显优于现有方法。

    

    演奏技巧（IPT）是音乐表演的关键元素。然而，大多数现有的IPT检测方法仅涉及单声道音乐信号，对于具有重叠IPT或混合IPT的多声部独奏音乐中的IPT检测几乎没有研究。在本文中，我们将其建模为帧级多标签分类问题，并将其应用于古筝，一种中国弹拨弦乐器。我们创建了一个新的数据集，古筝_Tech99，其中包含古筝录音以及每个音符的起始、结束、音高和IPT注释。因为不同的IPT在其长度方面有很大变化，我们提出了一种使用多尺度网络和自注意方法解决这个问题的新方法。多尺度网络从不同的尺度提取特征，而在最粗糙的尺度上应用于特征映射的自注意机制进一步增强了长程特征提取。我们的方法在IPT检测方面比现有的方法表现更好，表明了其在多声部独奏音乐的IPT检测中的有效性。

    Instrument playing technique (IPT) is a key element of musical presentation. However, most of the existing works for IPT detection only concern monophonic music signals, yet little has been done to detect IPTs in polyphonic instrumental solo pieces with overlapping IPTs or mixed IPTs. In this paper, we formulate it as a frame-level multi-label classification problem and apply it to Guzheng, a Chinese plucked string instrument. We create a new dataset, Guzheng\_Tech99, containing Guzheng recordings and onset, offset, pitch, IPT annotations of each note. Because different IPTs vary a lot in their lengths, we propose a new method to solve this problem using multi-scale network and self-attention. The multi-scale network extracts features from different scales, and the self-attention mechanism applied to the feature maps at the coarsest scale further enhances the long-range feature extraction. Our approach outperforms existing works by a large margin, indicating its effectiveness in IPT de
    
[^33]: 利普希茨网络转换辐射场，实现逼真的三维场景风格化

    Transforming Radiance Field with Lipschitz Network for Photorealistic 3D Scene Stylization. (arXiv:2303.13232v1 [cs.CV])

    [http://arxiv.org/abs/2303.13232](http://arxiv.org/abs/2303.13232)

    本文介绍了一个名为LipRF的学习框架，可以使用利普希茨映射将预训练的NeRF的外观表示转换为具有视觉一致性和逼真度的风格化场景。

    

    近年来，神经辐射场（NeRFs）在3D场景建模和新视角合成方面取得了重要进展。然而，利用NeRF进行逼真的三维场景风格化并不容易，因为它需要从新视角生成具有视觉一致性和逼真度的风格化场景。将NeRF和逼真的风格迁移（PST）相结合，会导致跨视角不一致和风格化视角合成的降质。经过深入分析，我们证明这一非常规任务可以以新的方法简化处理：使用利普希茨映射转换预训练的NeRF的外观表示，则会无缝地将源视角的一致性和逼真性编码到合成图像中。这启发我们构建了一个简洁、灵活的学习框架LipRF，通过利普希茨映射升级任何适用于三维场景的二维PST方法。

    Recent advances in 3D scene representation and novel view synthesis have witnessed the rise of Neural Radiance Fields (NeRFs). Nevertheless, it is not trivial to exploit NeRF for the photorealistic 3D scene stylization task, which aims to generate visually consistent and photorealistic stylized scenes from novel views. Simply coupling NeRF with photorealistic style transfer (PST) will result in cross-view inconsistency and degradation of stylized view syntheses. Through a thorough analysis, we demonstrate that this non-trivial task can be simplified in a new light: When transforming the appearance representation of a pre-trained NeRF with Lipschitz mapping, the consistency and photorealism across source views will be seamlessly encoded into the syntheses. That motivates us to build a concise and flexible learning framework namely LipRF, which upgrades arbitrary 2D PST methods with Lipschitz mapping tailored for the 3D scene. Technically, LipRF first pre-trains a radiance field to recon
    
[^34]: 丰富神经网络训练数据集以提高最坏情况性能保证

    Enriching Neural Network Training Dataset to Improve Worst-Case Performance Guarantees. (arXiv:2303.13228v1 [cs.LG])

    [http://arxiv.org/abs/2303.13228](http://arxiv.org/abs/2303.13228)

    本文提出了一种算法来丰富神经网络训练数据集，从而减少最坏情况的违规，并提高其性能保证。

    

    机器学习算法，特别是神经网络（NNs），是用于近似非线性关系（例如AC-OPF）的有价值的工具，并在部署时实现几个数量级的加速。通常在电力系统文献中，神经网络是通过在训练过程之前生成的固定数据集进行训练的。本文证明，在训练过程中调整神经网络训练数据集可以提高神经网络的性能，并大幅减少其最坏情况违规。本文提出了一个算法，用于识别和丰富关键数据点的训练数据集，以减少最坏情况违规，提供具有改进最坏情况性能保证的神经网络。我们在四个测试电力系统中演示了我们算法的性能，范围从39个总线到162个总线。

    Machine learning algorithms, especially Neural Networks (NNs), are a valuable tool used to approximate non-linear relationships, like the AC-Optimal Power Flow (AC-OPF), with considerable accuracy -- and achieving a speedup of several orders of magnitude when deployed for use. Often in power systems literature, the NNs are trained with a fixed dataset generated prior to the training process. In this paper, we show that adapting the NN training dataset during training can improve the NN performance and substantially reduce its worst-case violations. This paper proposes an algorithm that identifies and enriches the training dataset with critical datapoints that reduce the worst-case violations and deliver a neural network with improved worst-case performance guarantees. We demonstrate the performance of our algorithm in four test power systems, ranging from 39-buses to 162-buses.
    
[^35]: 大型语言模型的公正引导少样本提示

    Fairness-guided Few-shot Prompting for Large Language Models. (arXiv:2303.13217v1 [cs.CL])

    [http://arxiv.org/abs/2303.13217](http://arxiv.org/abs/2303.13217)

    本文提出了一种新的搜索策略-FairPrompt，在保证公正性的前提下，通过评估提示预测偏差，确定近似最优的提示，从而改进大型语言模型的上下文学习性能，实验表明该方法在准确性和公正性方面均优于现有方法。

    

    大型语言模型已经表现出惊人的能力，能够通过几个输入输出示例构建的提示进行直接应用来解决众多下游任务。但是，先前的研究表明，由于训练示例，示例顺序和提示格式的变化导致上下文学习容易出现高度不稳定性。因此，构建适当的提示对于改进上下文学习的性能至关重要。在这篇文章中，我们从预测偏差的角度重新探讨了这个问题。具体而言，我们引入了一个指标来评估固定提示相对于标签或给定属性的预测偏差。然后我们通过实验证明了预测偏差较大的提示总是导致不令人满意的预测质量。基于这个观察，我们提出了一种新的搜索策略，基于贪婪搜索来确定近似最优的提示，从而改进上下文学习的性能。我们提出的方法叫做"公正提示"，其中融入了公平性约束，以指导搜索不展现出对某些人群的偏见。我们在多种少样本分类任务上证明了FairPrompt的有效性，并展示了它在准确性和公正性方面均优于现有的最先进方法。

    Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context l
    
[^36]: AI工程实践案例研究：开发自主股票交易系统

    A Case Study on AI Engineering Practices: Developing an Autonomous Stock Trading System. (arXiv:2303.13216v1 [cs.SE])

    [http://arxiv.org/abs/2303.13216](http://arxiv.org/abs/2303.13216)

    本文介绍了一个利用机器学习实现自主股票交易的案例研究，使用了稳定的AI工程实践来确保系统的质量和改善开发过程。

    

    许多系统使用人工智能（AI）来解决复杂问题，然而，开发可投入生产的AI系统是一个困难的任务。因此，需要遵循稳定的AI工程实践以确保结果系统的质量并改善开发过程。本文收集了一个案例研究中的实践经验，即开发一个利用机器学习功能投资股票的自主股票交易系统。

    Today, many systems use artificial intelligence (AI) to solve complex problems. While this often increases system effectiveness, developing a production-ready AI-based system is a difficult task. Thus, solid AI engineering practices are required to ensure the quality of the resulting system and to improve the development process. While several practices have already been proposed for the development of AI-based systems, detailed practical experiences of applying these practices are rare.  In this paper, we aim to address this gap by collecting such experiences during a case study, namely the development of an autonomous stock trading system that uses machine learning functionality to invest in stocks. We selected 10 AI engineering practices from the literature and systematically applied them during development, with the goal to collect evidence about their applicability and effectiveness. Using structured field notes, we documented our experiences. Furthermore, we also used field notes
    
[^37]: 基于伪多模特征的点云异常检测方法

    Complementary Pseudo Multimodal Feature for Point Cloud Anomaly Detection. (arXiv:2303.13194v1 [cs.CV])

    [http://arxiv.org/abs/2303.13194](http://arxiv.org/abs/2303.13194)

    本研究提出了一种基于伪多模特征的点云异常检测方法，聚合了局部几何信息和全局语义信息，并能够达到较高的AU-ROC值。

    

    点云异常检测作为一种新兴的研究领域不断出现。本研究旨在通过将手工制作的点云描述符与强大的预训练2D神经网络相结合来提高点云异常检测性能。为此，本研究提出了一种基于伪多模特征 (CPMF) 的方法，该方法利用手工制作的点云描述符将局部几何信息纳入3D模态中，并利用预训练的2D神经网络在生成的伪2D模态中提取全局语义信息。对于全局语义提取，CPMF将原始点云投影到包含多视图图像的伪2D模态中。这些图像被传递给预训练的2D神经网络以提取信息丰富的2D模态特征。3D和2D模态特征被聚合以获得用于点云异常检测的CPMF。广泛的实验证明了2D和3D模态特征之间的互补能力以及CPMF的有效性，在基准数据集上达到了95.15%的图像级AU-ROC和92.67%的点级AU-ROC。

    Point cloud (PCD) anomaly detection steadily emerges as a promising research area. This study aims to improve PCD anomaly detection performance by combining handcrafted PCD descriptions with powerful pre-trained 2D neural networks. To this end, this study proposes Complementary Pseudo Multimodal Feature (CPMF) that incorporates local geometrical information in 3D modality using handcrafted PCD descriptors and global semantic information in the generated pseudo 2D modality using pre-trained 2D neural networks. For global semantics extraction, CPMF projects the origin PCD into a pseudo 2D modality containing multi-view images. These images are delivered to pre-trained 2D neural networks for informative 2D modality feature extraction. The 3D and 2D modality features are aggregated to obtain the CPMF for PCD anomaly detection. Extensive experiments demonstrate the complementary capacity between 2D and 3D modality features and the effectiveness of CPMF, with 95.15% image-level AU-ROC and 92
    
[^38]: 扩展高效用模式挖掘：基于答案集编程框架及其应用

    Extended High Utility Pattern Mining: An Answer Set Programming Based Framework and Applications. (arXiv:2303.13191v1 [cs.AI])

    [http://arxiv.org/abs/2303.13191](http://arxiv.org/abs/2303.13191)

    本文介绍了一种基于答案集编程的扩展高效用模式挖掘框架，可以处理多种新型效用准则，对于实际应用有着重要意义。

    

    在数据挖掘中，检测给定数据集中相关模式的集合是一个重要的挑战。模式的相关性（也称为效用）是一个主观的度量，可以从非常不同的角度进行评估。基于规则的语言（如答案集编程）似乎很适合以约束形式规定用户提供的准则来评估模式效用；此外，答案集编程的声明性使得可以非常容易地在不同角度分析数据集。本文介绍了扩展高效用模式挖掘的新方法，特别是引入了新的框架，可以处理以前文献中未考虑的新型效用准则类。我们还展示了如何利用最近引入的带有外部函数的答案集编程扩展来支持一种快速有效的编码和测试新框架。我们展示了该框架的潜力，并且将其与现有的HUPM算法进行了比较。此外，我们提供了实际应用的示例，说明了我们的框架可用于高效准确的模式挖掘。

    Detecting sets of relevant patterns from a given dataset is an important challenge in data mining. The relevance of a pattern, also called utility in the literature, is a subjective measure and can be actually assessed from very different points of view. Rule-based languages like Answer Set Programming (ASP) seem well suited for specifying user-provided criteria to assess pattern utility in a form of constraints; moreover, declarativity of ASP allows for a very easy switch between several criteria in order to analyze the dataset from different points of view. In this paper, we make steps toward extending the notion of High Utility Pattern Mining (HUPM); in particular we introduce a new framework that allows for new classes of utility criteria not considered in the previous literature. We also show how recent extensions of ASP with external functions can support a fast and effective encoding and testing of the new framework. To demonstrate the potential of the proposed framework, we exp
    
[^39]: CMG-Net: 一种基于接触的多指灵巧抓取网络

    CMG-Net: An End-to-End Contact-Based Multi-Finger Dexterous Grasping Network. (arXiv:2303.13182v1 [cs.RO])

    [http://arxiv.org/abs/2303.13182](http://arxiv.org/abs/2303.13182)

    本文提出了CMG-Net，一种基于接触点的多指抓取网络，可以高效地预测抓取姿势和手型配置，用于在杂乱环境中抓取未知物体。使用合成数据训练的模型在实际机器人中表现非常好，并优于现有最佳工作。

    

    本文提出了一种新颖的抓取表述方式，使用机器手指与待操作物体之间的接触点。该表述方式显著减少了预测维度并加速了学习过程。我们提出了一种有效的端到端网络CMG-Net，可以从单个点云中高效地预测多指抓取姿势和手型配置，用于在杂乱环境中抓取未知物体。此外，我们创建了一个综合的、包含五千个杂乱场景、80个物体类别和2000万注释的合成抓取数据集。我们进行了全面的实证研究，并展示了我们的抓取表述方式和CMG-Net的有效性。我们的工作在三指机器手的领域中明显优于现有的最佳工作。我们还证明使用合成数据训练的模型在实际机器人中的表现非常好。

    In this paper, we propose a novel representation for grasping using contacts between multi-finger robotic hands and objects to be manipulated. This representation significantly reduces the prediction dimensions and accelerates the learning process. We present an effective end-to-end network, CMG-Net, for grasping unknown objects in a cluttered environment by efficiently predicting multi-finger grasp poses and hand configurations from a single-shot point cloud. Moreover, we create a synthetic grasp dataset that consists of five thousand cluttered scenes, 80 object categories, and 20 million annotations. We perform a comprehensive empirical study and demonstrate the effectiveness of our grasping representation and CMG-Net. Our work significantly outperforms the state-of-the-art for three-finger robotic hands. We also demonstrate that the model trained using synthetic data performs very well for real robots.
    
[^40]: 基于人工智能系统的设计模式：多方位文献综述与模式存储库

    Design Patterns for AI-based Systems: A Multivocal Literature Review and Pattern Repository. (arXiv:2303.13173v1 [cs.SE])

    [http://arxiv.org/abs/2303.13173](http://arxiv.org/abs/2303.13173)

    本研究概述了基于人工智能系统的设计模式，旨在提高软件质量和系统性能。从51篇学术论文中选择了43个设计模式，分别提供了针对AI-based systems的解决方案，并分为数据处理、建模、决策和实现四个类别。该研究为研究者和从业者提供了一个完整的、易于访问的设计模式库。

    

    近年来，人工智能组件构成的系统，即所谓的基于人工智能的系统，已经引起了极大的关注。然而，许多组织在实现这样的系统的生产准备方面存在问题。作为提高某些软件质量属性和解决经常出现的问题的手段，设计模式提供了可靠的解决方案蓝图。在AI-based系统出现新模式的同时，现有模式也已经被适应到了这个新的上下文环境中。本研究的目标是提供一个基于人工智能系统的设计模式的概述，包括新模式和适应模式。我们希望收集和分类模式，并使其对研究者和从业者可访问。为此，我们首先进行了多方位文献综述（MLR）以收集与AI-based系统一起使用的设计模式。然后，我们将所创建的模式集成到一个基于web的模式存储库中，使得这些模式易于浏览和查找。结果，我们选择了51篇学术论文中的43个设计模式，这些模式分为数据处理、建模、决策和实现四个类别，分别提供了针对AI-based systems 的解决方案，从而提高软件质量和系统性能。该研究旨在为研究者和从业者提供一个完整的、易于访问的设计模式库。

    Systems with artificial intelligence components, so-called AI-based systems, have gained considerable attention recently. However, many organizations have issues with achieving production readiness with such systems. As a means to improve certain software quality attributes and to address frequently occurring problems, design patterns represent proven solution blueprints. While new patterns for AI-based systems are emerging, existing patterns have also been adapted to this new context.  The goal of this study is to provide an overview of design patterns for AI-based systems, both new and adapted ones. We want to collect and categorize patterns, and make them accessible for researchers and practitioners. To this end, we first performed a multivocal literature review (MLR) to collect design patterns used with AI-based systems. We then integrated the created pattern collection into a web-based pattern repository to make the patterns browsable and easy to find.  As a result, we selected 51
    
[^41]: 一个基础的信念函数逻辑

    An elementary belief function logic. (arXiv:2303.13168v1 [cs.AI])

    [http://arxiv.org/abs/2303.13168](http://arxiv.org/abs/2303.13168)

    本文提出了一种简单的信念函数逻辑，通过在MEL顶部添加{\L}ukasiewicz逻辑来实现，允许更自然的语义。

    

    非可加不确定性理论，尤其是可能性理论、信念函数和不精确概率与模态逻辑共享一个共同特点，即可能性和必然性度量、置信度和可能度函数以及上下概率之间的对偶性质将可能性和必然性模态的对偶性扩展到分级环境。本文展示了一种更简单的信念函数逻辑，并通过在MEL顶部添加{\L}ukasiewicz逻辑来实现，从而允许更自然的语义。

    Non-additive uncertainty theories, typically possibility theory, belief functions and imprecise probabilities share a common feature with modal logic: the duality properties between possibility and necessity measures, belief and plausibility functions as well as between upper and lower probabilities extend the duality between possibility and necessity modalities to the graded environment. It has been shown that the all-or-nothing version of possibility theory can be exactly captured by a minimal epistemic logic (MEL) that uses a very small fragment of the KD modal logic, without resorting to relational semantics. Besides, the case of belief functions has been studied independently, and a belief function logic has been obtained by extending the modal logic S5 to graded modalities using {\L}ukasiewicz logic, albeit using relational semantics. This paper shows that a simpler belief function logic can be devised by adding {\L}ukasiewicz logic on top of MEL. It allows for a more natural sem
    
[^42]: 连续学习的绝热重放

    Adiabatic replay for continual learning. (arXiv:2303.13157v1 [cs.LG])

    [http://arxiv.org/abs/2303.13157](http://arxiv.org/abs/2303.13157)

    本研究提出了一种称为绝热重放的重放连续学习策略，它能够有选择性地重放与新数据相似的样本，从而提高学习效率。

    

    传统的基于重放的连续学习方法需要在每个新数据的学习阶段重放代表先前学习到的所有知识的样本，以避免灾难性遗忘。由于在连续学习问题中学到的知识量随时间增长，生成式重放会花费越来越多的时间来重新学习已知内容。在这个概念验证的研究中，我们提出了一种我们称之为绝热重放（AR）的重放连续学习策略，其效率来自于（合理的）假设每个新的学习阶段都是绝热的，即仅代表现有知识的小幅增加。每个新的学习阶段会触发一个选择性重放的采样过程，从现有知识库中选择相似于新数据的样本进行重放，而不是全部重放。完全重放不是必须的，因为AR通过GMMs表示数据分布，这些分布能够有选择性地更新它们的分布。

    Conventional replay-based approaches to continual learning (CL) require, for each learning phase with new data, the replay of samples representing all of the previously learned knowledge in order to avoid catastrophic forgetting. Since the amount of learned knowledge grows over time in CL problems, generative replay spends an increasing amount of time just re-learning what is already known. In this proof-of-concept study, we propose a replay-based CL strategy that we term adiabatic replay (AR), which derives its efficiency from the (reasonable) assumption that each new learning phase is adiabatic, i.e., represents only a small addition to existing knowledge. Each new learning phase triggers a sampling process that selectively replays, from the body of existing knowledge, just such samples that are similar to the new data, in contrast to replaying all of it. Complete replay is not required since AR represents the data distribution by GMMs, which are capable of selectively updating their
    
[^43]: 为可信AI野花监测平台定义质量要求

    Defining Quality Requirements for a Trustworthy AI Wildflower Monitoring Platform. (arXiv:2303.13151v1 [cs.AI])

    [http://arxiv.org/abs/2303.13151](http://arxiv.org/abs/2303.13151)

    本文将ISO25000质量模型应用于野花监测深度学习平台的实际案例研究，提出了一种结构化的方法来定义质量要求。

    

    为将AI解决方案从训练好的机器学习模型演变为生产就绪的AI系统，需要考虑比机器学习模型性能更多的因素。生产就绪的AI系统需要具备可信性，即高质量。本文使用ISO25000质量模型之一，将其应用于实际案例研究:监测野花的深度学习平台。文章提出了三种现实场景，分别概述了采用、扩展和逐步改进深度学习平台用于野花识别和计数的含义。接下来，展示了如何使用质量模型作为结构化词典，为数据、模型和软件定义质量要求。未来的工作提出了

    For an AI solution to evolve from a trained machine learning model into a production-ready AI system, many more things need to be considered than just the performance of the machine learning model. A production-ready AI system needs to be trustworthy, i.e. of high quality. But how to determine this in practice? For traditional software, ISO25000 and its predecessors have since long time been used to define and measure quality characteristics. Recently, quality models for AI systems, based on ISO25000, have been introduced. This paper applies one such quality model to a real-life case study: a deep learning platform for monitoring wildflowers. The paper presents three realistic scenarios sketching what it means to respectively use, extend and incrementally improve the deep learning platform for wildflower identification and counting. Next, it is shown how the quality model can be used as a structured dictionary to define quality requirements for data, model and software. Future work rem
    
[^44]: MagicFusion：通过融合扩散模型提高文本到图像生成的性能

    MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models. (arXiv:2303.13126v1 [cs.CV])

    [http://arxiv.org/abs/2303.13126](http://arxiv.org/abs/2303.13126)

    本论文提出了一种名为SNB的方法，该方法集成了两个文本指导扩散模型的噪声预测，以实现更可控的图像生成，同时不需要额外的训练或注释。

    

    开源AI社区的出现产生了一系列强大的基于文本的扩散模型，这些模型训练在各种数据集上。本文提出了一种名为“Saliency-aware Noise Blending (SNB)” 的简单且有效的方法，可以使融合的文本指导扩散模型实现更可控的生成。实验证明分类器自由指导的响应与生成图像的显着性高度相关，因此我们提出了一种在显着性感知的情况下混合两个扩散模型的预测噪声来信任其专业领域的不同模型的方法。SNB是无需训练即可完成的，并且可以在DDIM采样过程中自动对齐两个噪声空间的语义，而无需额外的注释，例如掩模。大量的实验展示了SNB的显着有效性

    The advent of open-source AI communities has produced a cornucopia of powerful text-guided diffusion models that are trained on various datasets. While few explorations have been conducted on ensembling such models to combine their strengths. In this work, we propose a simple yet effective method called Saliency-aware Noise Blending (SNB) that can empower the fused text-guided diffusion models to achieve more controllable generation. Specifically, we experimentally find that the responses of classifier-free guidance are highly related to the saliency of generated images. Thus we propose to trust different models in their areas of expertise by blending the predicted noises of two diffusion models in a saliency-aware manner. SNB is training-free and can be completed within a DDIM sampling process. Additionally, it can automatically align the semantics of two noise spaces without requiring additional annotations such as masks. Extensive experiments show the impressive effectiveness of SNB
    
[^45]: 利用列表译码解释大语言模型中的相变现象

    A Simple Explanation for the Phase Transition in Large Language Models with List Decoding. (arXiv:2303.13112v1 [cs.CL])

    [http://arxiv.org/abs/2303.13112](http://arxiv.org/abs/2303.13112)

    本文提供了一个对大语言模型中相变现象的简单解释，利用列表译码器建模，它能够保证在LLM低于临界阈值时错误候选序列数的期望保持有界，而在高于该阈值时呈指数增长。

    

    最近的实验结果表明，大语言模型（LLM）呈现出小模型所没有的突出能力。当模型达到一定的规模关键点时，系统性能得到了极大地提高。在这篇文章中，我们提供了一个简单的解释，并将LLM建模为一个序列到序列的随机函数。我们使用列表译码器代替每个步骤的即时生成，该译码器在每个步骤保留一个候选序列列表，并在结束时推迟输出序列的生成。我们表明，存在一个临界阈值，当LLM低于此阈值时，期望的错误候选序列数保持有界，当LLM高于此阈值时，期望错误序列数呈指数增长。这样的阈值与传染病的基本繁殖数有关。

    Various recent experimental results show that large language models (LLM) exhibit emergent abilities that are not present in small models. System performance is greatly improved after passing a certain critical threshold of scale. In this letter, we provide a simple explanation for such a phase transition phenomenon. For this, we model an LLM as a sequence-to-sequence random function. Instead of using instant generation at each step, we use a list decoder that keeps a list of candidate sequences at each step and defers the generation of the output sequence at the end. We show that there is a critical threshold such that the expected number of erroneous candidate sequences remains bounded when an LLM is below the threshold, and it grows exponentially when an LLM is above the threshold. Such a threshold is related to the basic reproduction number in a contagious disease.
    
[^46]: 多视角的零样本开放意图归纳：多领域批处理和代理梯度转移

    Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer. (arXiv:2303.13099v1 [cs.CL])

    [http://arxiv.org/abs/2303.13099](http://arxiv.org/abs/2303.13099)

    本研究提出了一种多领域批处理和代理梯度转移的语义多视角模型，可以解决任务导向对话系统中的意图检测和诱导新意图的问题，在Open Intent Induction中有显著的性能提升。

    

    在任务导向的对话系统中，检测和诱导新的意图是将该系统应用于实际应用的两个主要挑战。本文提出了语义多视角模型来解决这两个难题：（1）用于一般嵌入的SBERT（2）多领域批处理（MDB）用于对话领域知识，以及（3）用于集群专业语义的代理梯度转移（PGT）。 MDB一次向模型提供多种对话数据集，通过学习多领域知识来解决多领域问题。我们引入了一种新的方法PGT，它采用Siamese网络直接使用聚类方法微调模型。我们的模型可以学习如何使用PGT聚类对话语句。实验结果表明，与基线系统相比，我们的多视角模型与MDB和PGT显着提高了Open Intent Induction的性能。

    In Task Oriented Dialogue (TOD) system, detecting and inducing new intents are two main challenges to apply the system in the real world. In this paper, we suggest the semantic multi-view model to resolve these two challenges: (1) SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized semantic. MDB feeds diverse dialogue datasets to the model at once to tackle the multi-domain problem by learning the multiple domain knowledge. We introduce a novel method PGT, which employs the Siamese network to fine-tune the model with a clustering method directly.Our model can learn how to cluster dialogue utterances by using PGT. Experimental results demonstrate that our multi-view model with MDB and PGT significantly improves the Open Intent Induction performance compared to baseline systems.
    
[^47]: CP$^3$: 基于点云网络的通道剪枝插件

    CP$^3$: Channel Pruning Plug-in for Point-based Networks. (arXiv:2303.13097v1 [cs.CV])

    [http://arxiv.org/abs/2303.13097](http://arxiv.org/abs/2303.13097)

    CP$^3$是一种基于点云网络的通道剪枝插件，它利用点云和PNN的特性，提出了坐标增强的通道重要度指标以实现通道剪枝的最优选择。

    

    通道剪枝可以有效地减少原始网络的计算成本和内存占用，同时保持相当的精度性能。虽然在2D基于图像的卷积神经网络（CNN）中已经取得了巨大的成功，但现有的工作很少将通道修剪方法扩展到3D基于点云的神经网络（PNNs）。直接将2D CNN通道修剪方法实现到PNN会破坏PNN性能，因为2D图像和3D点云的不同表示以及网络结构的差异。在本文中，我们提出了CP$^3$，它是一种基于点云网络的通道修剪插件。 CP$^3$经过精心设计，利用点云和PNN的特性，以使2D通道剪枝方法适用于PNN。具体而言，它提出了一个坐标增强的通道重要度指标，以反映尺寸信息和个体通道特征之间的相关性，从而实现通道剪枝的最优选择。

    Channel pruning can effectively reduce both computational cost and memory footprint of the original network while keeping a comparable accuracy performance. Though great success has been achieved in channel pruning for 2D image-based convolutional networks (CNNs), existing works seldom extend the channel pruning methods to 3D point-based neural networks (PNNs). Directly implementing the 2D CNN channel pruning methods to PNNs undermine the performance of PNNs because of the different representations of 2D images and 3D point clouds as well as the network architecture disparity. In this paper, we proposed CP$^3$, which is a Channel Pruning Plug-in for Point-based network. CP$^3$ is elaborately designed to leverage the characteristics of point clouds and PNNs in order to enable 2D channel pruning methods for PNNs. Specifically, it presents a coordinate-enhanced channel importance metric to reflect the correlation between dimensional information and individual channel features, and it recy
    
[^48]: CORA：基于 Region Prompting 和 Anchor Pre-Matching 的 CLIP 开放词汇检测模型改进

    CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching. (arXiv:2303.13076v1 [cs.CV])

    [http://arxiv.org/abs/2303.13076](http://arxiv.org/abs/2303.13076)

    本研究提出了CORA框架，通过Region Prompting和Anchor Pre-Matching解决使用CLIP进行OVD训练时遇到的分布差异和目标定位等难点。

    

    开放式词汇检测(OVD)是一种旨在检测基于对象定位器的新类别的目标的技术。最近，基于大规模视觉语言预训练模型（例如CLIP）的OVD方法用于识别新型物体。然而，我们发现将这些模型应用于检测器训练时需要克服两个核心难点：1）应用VL模型训练全图像进行区域识别时的分布偏差；2）定位非基础类别物体的困难。针对这些难点，我们提出CORA框架，旨在通过Region prompting和Anchor pre-matching技术将CLIP改进为一种适用于OVD的DETR风格模型。

    Open-vocabulary detection (OVD) is an object detection task aiming at detecting objects from novel categories beyond the base categories on which the detector is trained. Recent OVD methods rely on large-scale visual-language pre-trained models, such as CLIP, for recognizing novel objects. We identify the two core obstacles that need to be tackled when incorporating these models into detector training: (1) the distribution mismatch that happens when applying a VL-model trained on whole images to region recognition tasks; (2) the difficulty of localizing objects of unseen classes. To overcome these obstacles, we propose CORA, a DETR-style framework that adapts CLIP for Open-vocabulary detection by Region prompting and Anchor pre-matching. Region prompting mitigates the whole-to-region distribution gap by prompting the region features of the CLIP-based region classifier. Anchor pre-matching helps learning generalizable object localization by a class-aware matching mechanism. We evaluate 
    
[^49]: 利用伪标注标签的开放词汇物体检测

    Open-Vocabulary Object Detection using Pseudo Caption Labels. (arXiv:2303.13040v1 [cs.CV])

    [http://arxiv.org/abs/2303.13040](http://arxiv.org/abs/2303.13040)

    该论文提出了一种名为伪标注标签（PCL）的简单有效方法，在开放词汇物体检测中使用图像字幕模型生成描述对象实例的字幕标签，以提取关于新颖对象的丰富知识。

    

    最近的开放词汇检测方法旨在通过从训练在大量图像文本对上的视觉语言模型（VLM）中蒸馏知识来检测新的对象。为了提高这些方法的有效性，研究人员利用了一个包含大量对象类的大词汇数据集，以假设这样的数据将使模型提取关于各种对象之间关系的综合知识，并更好地推广到看不见的对象类别。在这项研究中，我们认为需要更精细的标签来提取丰富的有关新对象的知识，包括对象属性和关系，而不仅仅是它们的名称。为了解决这个挑战，我们提出了一个名为伪标注标签（PCL）的简单而有效的方法，该方法利用图像字幕模型生成描述来自不同角度的对象实例的字幕。由此产生的伪字幕标签为知识提供了密集的样本。

    Recent open-vocabulary detection methods aim to detect novel objects by distilling knowledge from vision-language models (VLMs) trained on a vast amount of image-text pairs. To improve the effectiveness of these methods, researchers have utilized datasets with a large vocabulary that contains a large number of object classes, under the assumption that such data will enable models to extract comprehensive knowledge on the relationships between various objects and better generalize to unseen object classes. In this study, we argue that more fine-grained labels are necessary to extract richer knowledge about novel objects, including object attributes and relationships, in addition to their names. To address this challenge, we propose a simple and effective method named Pseudo Caption Labeling (PCL), which utilizes an image captioning model to generate captions that describe object instances from diverse perspectives. The resulting pseudo caption labels offer dense samples for knowledge di
    
[^50]: SPeC：软提示校准在临床笔记摘要中降低性能变异的研究

    SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v1 [cs.CL])

    [http://arxiv.org/abs/2303.13035](http://arxiv.org/abs/2303.13035)

    研究通过引入软提示嵌入，提出Soft Prompt-Based Calibration (SPeC)管道，来减轻输入变量对输出多样性的影响，降低性能变异. 此方法不仅比大语言模型(LLM)性能稳定，而且在临床笔记摘要任务上表现优于最先进的模型.

    

    电子健康记录（EHR）存储着包括病历、诊断、治疗和检测结果在内的大量患者信息。这些记录对于医疗保健专业人员做出明智的患者护理决策非常关键。摘要临床笔记可以帮助医疗保健专业人员更好地发现潜在健康风险，以及做出更好的决策。这一过程通过确保医疗保健专业人员可以访问最相关和最新的患者数据，有助于减少错误并提高患者的护理效果。最近的研究表明，将提示与大语言模型（LLM）相结合可以显著提高摘要任务的效率。然而，我们发现这种方法也会导致输出方差增加，即使提示意义相似，输出也会有明显的差异。为了解决这一挑战，我们引入了一个模型无关的软提示校准（SPeC）流程，该流程采用软提示嵌入来减轻输入变量对输出多样性的影响。我们的实验表明，SPeC不仅可以降低LLM的性能变异，而且在临床笔记摘要任务上优于现有的最先进模型。

    Electronic health records (EHRs) store an extensive array of patient information, encompassing medical histories, diagnoses, treatments, and test outcomes. These records are crucial for enabling healthcare providers to make well-informed decisions regarding patient care. Summarizing clinical notes further assists healthcare professionals in pinpointing potential health risks and making better-informed decisions. This process contributes to reducing errors and enhancing patient outcomes by ensuring providers have access to the most pertinent and current patient data. Recent research has shown that incorporating prompts with large language models (LLMs) substantially boosts the efficacy of summarization tasks. However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings. To tackle this challenge, we introduce a model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft prom
    
[^51]: 偏好感知的约束多目标贝叶斯优化

    Preference-Aware Constrained Multi-Objective Bayesian Optimization. (arXiv:2303.13034v1 [cs.LG])

    [http://arxiv.org/abs/2303.13034](http://arxiv.org/abs/2303.13034)

    PAC-MOO是一个偏好感知的约束多目标贝叶斯优化方法，可以有效地解决在黑盒目标函数和从业者指定的目标偏好下，大部分输入空间是不可行的约束多目标优化问题。

    

    本文解决了在大部分输入空间是不可行（即违反约束条件）时，基于黑盒目标函数和从业者指定的目标偏好的约束多目标优化问题。这个问题在许多工程设计问题中都存在，包括模拟电路和电力系统设计。我们的总体目标是在可行的输入设计的小部分上近似最优Pareto集合。主要挑战包括设计空间的巨大大小、多个目标和大量的约束条件以及只能在进行昂贵的仿真后才能确认的可行的输入设计的小部分。我们提出了一种新颖而有效的偏好感知的约束多目标贝叶斯优化方法（PAC-MOO）来解决这些挑战。关键思想是学习输出目标和约束的代理模型，并根据从业者预测的偏好选择评估目标的候选输入。PAC-MOO根据预测的目标和约束的联合偏好迭代地选择下一个要模拟的输入设计，并使用新获得的数据更新代理模型。实验结果表明，与现有的最先进方法相比，所提出的方法具有有效性和效率。

    This paper addresses the problem of constrained multi-objective optimization over black-box objective functions with practitioner-specified preferences over the objectives when a large fraction of the input space is infeasible (i.e., violates constraints). This problem arises in many engineering design problems including analog circuits and electric power system design. Our overall goal is to approximate the optimal Pareto set over the small fraction of feasible input designs. The key challenges include the huge size of the design space, multiple objectives and large number of constraints, and the small fraction of feasible input designs which can be identified only after performing expensive simulations. We propose a novel and efficient preference-aware constrained multi-objective Bayesian optimization approach referred to as PAC-MOO to address these challenges. The key idea is to learn surrogate models for both output objectives and constraints, and select the candidate input for eva
    
[^52]: 无线网络中容忍故障的分布式学习用于异常检测

    Failure-tolerant Distributed Learning for Anomaly Detection in Wireless Networks. (arXiv:2303.13015v1 [cs.LG])

    [http://arxiv.org/abs/2303.13015](http://arxiv.org/abs/2303.13015)

    本文提出了一种名为“Tol-FL”的新方法，通过结合扁平和星型拓扑结构的优势，增强了分布式网络中的变异检测性能和可靠性。

    

    大多数分布式技术的分析都是专注于它们的效率，而没有考虑它们的鲁棒性（或缺乏鲁棒性）。然而，这种考虑尤其重要，因为当设备或中央服务器出现故障时，这可能会瘫痪分布式系统。本文提出了一种通过结合扁平和星型拓扑结构来解决这些风险的新方法，将两者的性能和可靠性优势相结合。我们将这种方法称为“Tol-FL”，因为与联邦学习技术相比，它的故障容错能力提高了。我们的方法在客户端和服务器故障的各种逼真情况下，在异常检测AUROC方面优于先前方法高达8％，同时减少了设备故障风险。

    The analysis of distributed techniques is often focused upon their efficiency, without considering their robustness (or lack thereof). Such a consideration is particularly important when devices or central servers can fail, which can potentially cripple distributed systems. When such failures arise in wireless communications networks, important services that they use/provide (like anomaly detection) can be left inoperable and can result in a cascade of security problems. In this paper, we present a novel method to address these risks by combining both flat- and star-topologies, combining the performance and reliability benefits of both. We refer to this method as "Tol-FL", due to its increased failure-tolerance as compared to the technique of Federated Learning. Our approach both limits device failure risks while outperforming prior methods by up to 8% in terms of anomaly detection AUROC in a range of realistic settings that consider client as well as server failure, all while reducing
    
[^53]: 用于视觉模型诊断的语义图像攻击

    Semantic Image Attack for Visual Model Diagnosis. (arXiv:2303.13010v1 [cs.CV])

    [http://arxiv.org/abs/2303.13010](http://arxiv.org/abs/2303.13010)

    本文提出了一种新的基于对抗攻击的方法——语义图像攻击（SIA），可以提供语义对抗图像以便进行模型诊断、可解释性和鲁棒性。

    

    在实践中，对特定训练和测试数据集进行度量分析不能保证可靠或公平的机器学习模型。这部分原因是，获得平衡、多样和标记完美的数据集通常是昂贵、耗时和易出错的。本文提出了一种基于对抗攻击的方法——语义图像攻击（SIA），它提供了语义对抗图像，以便进行模型诊断、可解释性和鲁棒性。传统的对抗训练是一种增强机器学习模型对抗攻击的流行方法。然而，现有的对抗方法不结合两个方面，无法解释和分析模型的缺陷：语义可追溯性和感觉质量。SIA通过预定义的语义属性空间和图像空间上的迭代梯度上升结合了两个特征。我们证明了SIA的有效性。

    In practice, metric analysis on a specific train and test dataset does not guarantee reliable or fair ML models. This is partially due to the fact that obtaining a balanced, diverse, and perfectly labeled dataset is typically expensive, time-consuming, and error-prone. Rather than relying on a carefully designed test set to assess ML models' failures, fairness, or robustness, this paper proposes Semantic Image Attack (SIA), a method based on the adversarial attack that provides semantic adversarial images to allow model diagnosis, interpretability, and robustness. Traditional adversarial training is a popular methodology for robustifying ML models against attacks. However, existing adversarial methods do not combine the two aspects that enable the interpretation and analysis of the model's flaws: semantic traceability and perceptual quality. SIA combines the two features via iterative gradient ascent on a predefined semantic attribute space and the image space. We illustrate the validi
    
[^54]: 历史学习综述: 带有学习历史的学习模型

    A Survey of Historical Learning: Learning Models with Learning History. (arXiv:2303.12992v1 [cs.LG])

    [http://arxiv.org/abs/2303.12992](http://arxiv.org/abs/2303.12992)

    本文综述了“历史学习：带有学习历史的学习模型”这个主题，涵盖历史类型、功能部分和存储形式三个方面，是首个系统研究利用各种历史统计数据进行深度神经网络训练的综述论文。

    

    新的知识源于旧的知识。在训练历史记录中存储的各种类型的元素对于改善深度学习模型的学习非常有帮助。本文综述了“历史学习：带有学习历史的学习模型”这个主题，系统地研究从历史统计数据中进行深度神经网络训练的方法，涵盖历史类型、功能部分和存储形式三个方面。我们认为，这是首个系统研究利用各种历史统计数据进行深度神经网络训练的综述论文。文章还讨论了与此相关的话题，如循环/记忆网络、集成学习和强化学习。最后，我们还指出了这个主题的未来挑战，并鼓励学术界在设计算法时认真思考历史学习原则。

    New knowledge originates from the old. The various types of elements, deposited in the training history, are a large amount of wealth for improving learning deep models. In this survey, we comprehensively review and summarize the topic--``Historical Learning: Learning Models with Learning History'', which learns better neural models with the help of their learning history during its optimization, from three detailed aspects: Historical Type (what), Functional Part (where) and Storage Form (how). To our best knowledge, it is the first survey that systematically studies the methodologies which make use of various historical statistics when training deep neural networks. The discussions with related topics like recurrent/memory networks, ensemble learning, and reinforcement learning are demonstrated. We also expose future challenges of this topic and encourage the community to pay attention to the think of historical learning principles when designing algorithms. The paper list related to
    
[^55]: 推荐系统中反事实倾向估计的不确定性校准

    Uncertainty Calibration for Counterfactual Propensity Estimation in Recommendation. (arXiv:2303.12973v1 [cs.AI])

    [http://arxiv.org/abs/2303.12973](http://arxiv.org/abs/2303.12973)

    本文提出了多种不确定性校准技术，以改进推荐系统中倾向性估计的效果。经过实验验证，校准后的IPS估计器在Coat和yahoo数据集上表现更好。

    

    在推荐系统中，由于选择偏差，许多评分信息都丢失了，这被称为非随机缺失。反事实逆倾向评分（IPS）被用于衡量每个观察到的评分的填充错误。虽然在多种情况下有效，但我们认为IPS估计的性能受到倾向性估计不确定性的限制。本文提出了多种代表性的不确定性校准技术，以改进推荐系统中倾向性估计的不确定性校准。通过对偏误和推广界限的理论分析表明，经过校准的IPS估计器优于未校准的IPS估计器。 Coat和yahoo数据集上的实验结果表明，不确定性校准得到改进，从而使推荐结果更好。

    In recommendation systems, a large portion of the ratings are missing due to the selection biases, which is known as Missing Not At Random. The counterfactual inverse propensity scoring (IPS) was used to weight the imputation error of every observed rating. Although effective in multiple scenarios, we argue that the performance of IPS estimation is limited due to the uncertainty miscalibration of propensity estimation. In this paper, we propose the uncertainty calibration for the propensity estimation in recommendation systems with multiple representative uncertainty calibration techniques. Theoretical analysis on the bias and generalization bound shows the superiority of the calibrated IPS estimator over the uncalibrated one. Experimental results on the coat and yahoo datasets shows that the uncertainty calibration is improved and hence brings the better recommendation results.
    
[^56]: 连续不定概率神经网络

    Continuous Indeterminate Probability Neural Network. (arXiv:2303.12964v1 [cs.LG])

    [http://arxiv.org/abs/2303.12964](http://arxiv.org/abs/2303.12964)

    本文提出了CIPNN模型，该模型能够推导出连续潜在随机变量的解析解，同时提出了CIPAE自编码器，并通过可视化潜在随机变量的方法验证了模型的有效性。

    

    本文介绍了一种称为CIPNN（Continuous Indeterminate Probability Neural Network）的通用模型，该模型基于IPNN，用于离散潜在随机变量。目前，连续潜在变量的后验被认为是不可计算的，但是IPNN提出了新的理论，可以解决这个问题。本文的贡献有四个方面。首先，我们推导了连续潜在随机变量的后验计算的解析解，并提出了一个通用分类模型（CIPNN）。其次，我们提出了一种通用的自编码器——CIPAE（Continuous Indeterminate Probability Auto-Encoder），其中解码器部分不是神经网络，而是第一次使用全概率推理模型。第三，我们提出了一种新的可视化潜在随机变量的方法。我们使用N维潜在变量之一作为解码器来重建输入图像，即使是分类任务也能达到效果，这样，我们可以看到每个潜在变量代表什么。第四，我们通过MNIST和Fashion-MNIST数据集的实验证明了所提出模型的有效性。

    This paper introduces a general model called CIPNN - Continuous Indeterminate Probability Neural Network, and this model is based on IPNN, which is used for discrete latent random variables. Currently, posterior of continuous latent variables is regarded as intractable, with the new theory proposed by IPNN this problem can be solved. Our contributions are Four-fold. First, we derive the analytical solution of the posterior calculation of continuous latent random variables and propose a general classification model (CIPNN). Second, we propose a general auto-encoder called CIPAE - Continuous Indeterminate Probability Auto-Encoder, the decoder part is not a neural network and uses a fully probabilistic inference model for the first time. Third, we propose a new method to visualize the latent random variables, we use one of N dimensional latent variables as a decoder to reconstruct the input image, which can work even for classification tasks, in this way, we can see what each latent varia
    
[^57]: 临床基础模型的不稳定基础：针对 EMR 的大语言模型和基础模型的调查

    The Shaky Foundations of Clinical Foundation Models: A Survey of Large Language Models and Foundation Models for EMRs. (arXiv:2303.12961v1 [cs.LG])

    [http://arxiv.org/abs/2303.12961](http://arxiv.org/abs/2303.12961)

    本论文回顾了超过80个在非成像 EMR 数据上训练的基础模型，发现这些模型大多范围有限、训练集有限，且评估指标未对其对医疗系统贡献提供有意义见解。因此，本研究提出了一种更接近于医疗保健重要指标的医疗基础模型效益评估框架。

    

    类似 ChatGPT 和 AlphaFold 的基础模型的成功引发了人们对于构建类似模型以改善 EMR（电子病历）以提高患者护理和医院运营的极大兴趣。然而，最近的炒作掩盖了我们对这些模型能力的关键缺失。我们回顾了超过80个在非成像 EMR 数据（即临床文本和/或结构化数据）上训练的基础模型，并创建了一个分类法来说明它们的体系结构、训练数据和潜在用例。我们发现大多数模型是在小型、范围有限的临床数据集（例如MIMIC-III）或广泛的公共生物医学语料库（例如PubMed）上进行训练的，并且在不提供对其对医疗系统有用处的有意义见解的任务上进行评估。基于这些发现，我们提出了一种更接近于医疗保健重要指标的医疗基础模型效益的改进评估框架。

    The successes of foundation models such as ChatGPT and AlphaFold have spurred significant interest in building similar models for electronic medical records (EMRs) to improve patient care and hospital operations. However, recent hype has obscured critical gaps in our understanding of these models' capabilities. We review over 80 foundation models trained on non-imaging EMR data (i.e. clinical text and/or structured data) and create a taxonomy delineating their architectures, training data, and potential use cases. We find that most models are trained on small, narrowly-scoped clinical datasets (e.g. MIMIC-III) or broad, public biomedical corpora (e.g. PubMed) and are evaluated on tasks that do not provide meaningful insights on their usefulness to health systems. In light of these findings, we propose an improved evaluation framework for measuring the benefits of clinical foundation models that is more closely grounded to metrics that matter in healthcare.
    
[^58]: 变分自编码器中逐步减少信息瓶颈的去纠缠方法

    Variantional autoencoder with decremental information bottleneck for disentanglement. (arXiv:2303.12959v1 [cs.LG])

    [http://arxiv.org/abs/2303.12959](http://arxiv.org/abs/2303.12959)

    本论文提出了一种逐步减少信息瓶颈的变分自编码器方法，使用去纠缠不变变换来平衡去纠缠和重构保真度，避免信息扩散问题。

    

    变分自编码器中去纠缠学习的一个主要挑战是在权衡去纠缠和重构保真度之间的平衡。之前仅在一个潜在空间中进行的逐步方法无法同时优化这两个目标，因此在训练过程中扩展了信息瓶颈，以从去纠缠到重构进行优化。然而，大型瓶颈会失去去纠缠的约束，导致信息扩散问题。为了解决这个问题，我们提出了一种新颖的逐步减少信息瓶颈的变分自编码器方法，使用去纠缠不变变换来优化不同层的多个目标，称为DeVAE。通过逐渐减小不同潜在空间的信息瓶颈，DeVAE 平衡了去纠缠和重构保真度。由于具有多个潜在空间，DeVAE 允许同时优化多个目标，以在保持去纠缠约束的同时优化重构，避免信息扩散问题。

    One major challenge of disentanglement learning with variational autoencoders is the trade-off between disentanglement and reconstruction fidelity. Previous incremental methods with only on latent space cannot optimize these two targets simultaneously, so they expand the Information Bottleneck while training to {optimize from disentanglement to reconstruction. However, a large bottleneck will lose the constraint of disentanglement, causing the information diffusion problem. To tackle this issue, we present a novel decremental variational autoencoder with disentanglement-invariant transformations to optimize multiple objectives in different layers, termed DeVAE, for balancing disentanglement and reconstruction fidelity by decreasing the information bottleneck of diverse latent spaces gradually. Benefiting from the multiple latent spaces, DeVAE allows simultaneous optimization of multiple objectives to optimize reconstruction while keeping the constraint of disentanglement, avoiding info
    
[^59]: 具有外部状态和奖励的强化学习

    Reinforcement Learning with Exogenous States and Rewards. (arXiv:2303.12957v1 [cs.LG])

    [http://arxiv.org/abs/2303.12957](http://arxiv.org/abs/2303.12957)

    该研究提出了一种强化学习的方法，通过将MDP分解为外生和内生两个部分，优化内生奖励，在状态空间的内生和外生状态空间没有事先给出的情况下，提出了正确的算法进行自动发现。

    

    外部状态变量和奖励会通过向奖励信号注入不可控的变化而减慢强化学习的速度。本文对外部状态变量和奖励进行了正式化，并表明如果奖励函数加法分解成内生和外生两个部分，MDP可以分解为一个外生马尔可夫奖励过程（基于外部奖励）和一个内生马尔可夫决策过程（优化内生奖励）。内生MDP的任何最优策略也是原始MDP的最优策略，但由于内生奖励通常具有降低的方差，因此内生MDP更容易求解。我们研究了状态空间分解为内外生状态空间的情况，而这种状态空间分解并没有给出，而是必须发现。本文介绍并证明了在线性组合下发现内生和外生状态空间的算法的正确性。

    Exogenous state variables and rewards can slow reinforcement learning by injecting uncontrolled variation into the reward signal. This paper formalizes exogenous state variables and rewards and shows that if the reward function decomposes additively into endogenous and exogenous components, the MDP can be decomposed into an exogenous Markov Reward Process (based on the exogenous reward) and an endogenous Markov Decision Process (optimizing the endogenous reward). Any optimal policy for the endogenous MDP is also an optimal policy for the original MDP, but because the endogenous reward typically has reduced variance, the endogenous MDP is easier to solve. We study settings where the decomposition of the state space into exogenous and endogenous state spaces is not given but must be discovered. The paper introduces and proves correctness of algorithms for discovering the exogenous and endogenous subspaces of the state space when they are mixed through linear combination. These algorithms
    
[^60]: 利用联邦学习和区块链保障金融服务安全性

    Use of Federated Learning and Blockchain towards Securing Financial Services. (arXiv:2303.12944v1 [cs.CR])

    [http://arxiv.org/abs/2303.12944](http://arxiv.org/abs/2303.12944)

    本文介绍了如何利用联邦学习和区块链技术提高金融服务的安全性和隐私性，探讨了在实现这些技术时所面临的挑战和潜在解决方案。

    

    近年来，现有和新的网络攻击的大量出现对金融服务的稳定性构成了根本性的威胁。很难预测能够引发严重金融危机的攻击的性质。金融服务的前所未有的数字化转型在 COVID-19 疫情期间得到加速，并仍在持续进行中。攻击者正在利用这一转型，对金融稳定性和完整性构成新的全球威胁。许多大型机构正在从中心化金融（CeFi）转向去中心化金融（DeFi），因为去中心化金融具有许多优势。区块链可以对金融业的可信度、安全性、可访问性、成本效益和开放性带来重大和深远的影响。本文着重探讨了区块链和联邦学习如何在金融服务中使用。它从最近的两个应用案例的概述开始，解释了联邦学习和区块链技术如何提高金融服务的安全性和隐私性。此外，它还阐述了在实施这些技术时面临的挑战并提供了潜在解决方案。

    In recent days, the proliferation of several existing and new cyber-attacks pose an axiomatic threat to the stability of financial services. It is hard to predict the nature of attacks that can trigger a serious financial crisis. The unprecedented digital transformation to financial services has been accelerated during the COVID-19 pandemic and it is still ongoing. Attackers are taking advantage of this transformation and pose a new global threat to financial stability and integrity. Many large organizations are switching from centralized finance (CeFi) to decentralized finance (DeFi) because decentralized finance has many advantages. Blockchain can bring big and far-reaching effects on the trustworthiness, safety, accessibility, cost-effectiveness, and openness of the financial sector. The present paper gives an in-depth look at how blockchain and federated learning (FL) are used in financial services. It starts with an overview of recent developments in both use cases. This paper exp
    
[^61]: 论网络网络安全的可解释人工智能综述

    A Survey on Explainable Artificial Intelligence for Network Cybersecurity. (arXiv:2303.12942v1 [cs.CR])

    [http://arxiv.org/abs/2303.12942](http://arxiv.org/abs/2303.12942)

    这篇论文综述了网络驱动的威胁和问题的系统分类，审查了网络系统中的可解释人工智能在网络安全中的最新技术，并勾画了未来研究的有前途的方向。

    

    人工智能模型的黑匣子特性一直是用于关键应用程序的许多关注点的来源。可解释人工智能（XAI）是一个快速发展的研究领域，旨在创建能够为其决策和行动提供清晰可解释解释的机器学习模型。在网络网络安全领域，XAI具有通过使我们能够更好地了解网络威胁的行为并设计更有效的防御来彻底改变我们处理网络安全的方式的潜力。在本综述中，我们对网络驱动的威胁和问题进行系统分类，审查了网络系统中的可解释人工智能（XAI）在网络网络安全中的最新技术，探讨了各种解决这个重要问题的方法。我们讨论了当前XAI方法在网络安全背景下的挑战和局限性，并勾画未来研究的有前途的方向。

    The black-box nature of artificial intelligence (AI) models has been the source of many concerns in their use for critical applications. Explainable Artificial Intelligence (XAI) is a rapidly growing research field that aims to create machine learning models that can provide clear and interpretable explanations for their decisions and actions. In the field of network cybersecurity, XAI has the potential to revolutionize the way we approach network security by enabling us to better understand the behavior of cyber threats and to design more effective defenses. In this survey, we review the state of the art in XAI for cybersecurity in network systems and explore the various approaches that have been proposed to address this important problem. The review follows a systematic classification of network-driven cybersecurity threats and issues. We discuss the challenges and limitations of current XAI methods in the context of cybersecurity and outline promising directions for future research.
    
[^62]: 现实世界中基于社区的智能视频监控 —— 一所社区学院的案例研究

    Real-World Community-in-the-Loop Smart Video Surveillance -- A Case Study at a Community College. (arXiv:2303.12934v1 [cs.CV])

    [http://arxiv.org/abs/2303.12934](http://arxiv.org/abs/2303.12934)

    本文研究了一种基于社区的智能视频监控系统在社区学院的实际测试平台上的设计和部署，着重解决了实时高准确度的视频分析处理、云系统基础架构和移动应用的开发、以及保护个人隐私和数据安全等方面的挑战。

    

    智能视频监控系统近来在保障公共安全和治安方面变得越来越重要，特别是在智慧城市中。然而，实时人工智能技术结合低延迟的通知和报警已经使得部署这些系统变得相当具有挑战性。本文提出了一个在社区学院的实际测试平台上设计和部署智能视频监控系统的案例研究。我们主要关注基于智能摄像头的系统，可以识别可疑/异常活动并立即通知相关人员和居民。本文还强调和解决了不同的算法和系统设计挑战，以确保测试平台中实时高准确度的视频分析处理。它还介绍了云系统基础架构和移动应用以进行实时通知，以保持学生、教职员工和负责治安人员的参与。同时，它涵盖了系统设计方面的考虑，例如保护个人隐私和数据安全。

    Smart Video surveillance systems have become important recently for ensuring public safety and security, especially in smart cities. However, applying real-time artificial intelligence technologies combined with low-latency notification and alarming has made deploying these systems quite challenging. This paper presents a case study for designing and deploying smart video surveillance systems based on a real-world testbed at a community college. We primarily focus on a smart camera-based system that can identify suspicious/abnormal activities and alert the stakeholders and residents immediately. The paper highlights and addresses different algorithmic and system design challenges to guarantee real-time high-accuracy video analytics processing in the testbed. It also presents an example of cloud system infrastructure and a mobile application for real-time notification to keep students, faculty/staff, and responsible security personnel in the loop. At the same time, it covers the design 
    
[^63]: 重新审视影响函数的脆弱性

    Revisiting the Fragility of Influence Functions. (arXiv:2303.12922v1 [cs.LG])

    [http://arxiv.org/abs/2303.12922](http://arxiv.org/abs/2303.12922)

    本文研究了影响函数的脆弱性，并提出在非凸条件下使用深层模型和更复杂数据集来解决这一问题。

    

    最近几年有很多论文致力于解释深度学习模型的预测。然而，很少有方法被提出来验证这些解释的准确性或可信度。最近，影响函数被证明是一种评估深度神经网络在单个样本上的灵敏度的方法。但是，先前的研究表明影响函数易受噪声和数据分布不对称性影响，缺乏鲁棒性。本文旨在研究影响函数的脆弱性，通过探究影响函数背后的机理，从而为增强影响函数的鲁棒性提供新思路。

    In the last few years, many works have tried to explain the predictions of deep learning models. Few methods, however, have been proposed to verify the accuracy or faithfulness of these explanations. Recently, influence functions, which is a method that approximates the effect that leave-one-out training has on the loss function, has been shown to be fragile. The proposed reason for their fragility remains unclear. Although previous work suggests the use of regularization to increase robustness, this does not hold in all cases. In this work, we seek to investigate the experiments performed in the prior work in an effort to understand the underlying mechanisms of influence function fragility. First, we verify influence functions using procedures from the literature under conditions where the convexity assumptions of influence functions are met. Then, we relax these assumptions and study the effects of non-convexity by using deeper models and more complex datasets. Here, we analyze the k
    
[^64]: 基于深度学习的立体相机多视频同步技术

    Deep learning-based stereo camera multi-video synchronization. (arXiv:2303.12916v1 [cs.CV])

    [http://arxiv.org/abs/2303.12916](http://arxiv.org/abs/2303.12916)

    本研究比较了不同深度学习系统的优劣，并证明一些系统具备足够高效且具有一般性来完成立体相机多视频同步任务。这个技术将有望降低整个系统的成本、重量和大小，并允许在建立这样的系统时更加灵活。

    

    立体视觉对于许多应用都至关重要。目前，两个摄像头传来的视流同步主要是使用硬件来完成的。基于软件的同步方法将会降低整个系统的成本、重量和大小，并允许在建立这样的系统时更加灵活。鉴于此，我们在本文中提出了不同深度学习系统的比较，并证明了一些系统足够高效且具有一般性来完成此任务。本研究为生产准备就绪的基于软件的视频同步系统铺平了道路。

    Stereo vision is essential for many applications. Currently, the synchronization of the streams coming from two cameras is done using mostly hardware. A software-based synchronization method would reduce the cost, weight and size of the entire system and allow for more flexibility when building such systems. With this goal in mind, we present here a comparison of different deep learning-based systems and prove that some are efficient and generalizable enough for such a task. This study paves the way to a production ready software-based video synchronization system.
    
[^65]: 用于网络安全入侵检测系统的特征降维方法比较

    Feature Reduction Method Comparison Towards Explainability and Efficiency in Cybersecurity Intrusion Detection Systems. (arXiv:2303.12891v1 [cs.LG])

    [http://arxiv.org/abs/2303.12891](http://arxiv.org/abs/2303.12891)

    本文对于用于网络安全入侵检测系统的三种特征降维方法进行了比较，结果表明使用蝙蝠算法的相关特征选择（CFS-BA）是最为高效的，仅用最佳随机森林信息增益（RF-IG）模型55%的时间构建，同时实现了99.99%的准确性。

    

    在网络安全领域，入侵检测系统（IDS）根据收集到的计算机和网络数据检测和防止攻击。最近的研究中，IDS模型使用机器学习（ML）和深度学习（DL）方法构建，如随机森林（RF）和深度神经网络（DNN）。特征选择（FS）可用于构建更快，更可解释和更准确的模型。我们研究了三种不同的FS技术； 随机森林信息增益（RF-IG），使用蝙蝠算法的相关特征选择（CFS-BA）和使用阿基拉优化器的CFS（CFS-AO）。我们的结果显示，CFS-BA是最有效的FS方法，仅用最佳RF-IG模型55％的时间构建，同时实现了99.99％的准确性。这加强了先前对CFS-BA准确性的贡献，并在最终结果中建立了子集大小，CFS得分和RF-IG得分之间的关系。

    In the realm of cybersecurity, intrusion detection systems (IDS) detect and prevent attacks based on collected computer and network data. In recent research, IDS models have been constructed using machine learning (ML) and deep learning (DL) methods such as Random Forest (RF) and deep neural networks (DNN). Feature selection (FS) can be used to construct faster, more interpretable, and more accurate models. We look at three different FS techniques; RF information gain (RF-IG), correlation feature selection using the Bat Algorithm (CFS-BA), and CFS using the Aquila Optimizer (CFS-AO). Our results show CFS-BA to be the most efficient of the FS methods, building in 55% of the time of the best RF-IG model while achieving 99.99% of its accuracy. This reinforces prior contributions attesting to CFS-BA's accuracy while building upon the relationship between subset size, CFS score, and RF-IG score in final results.
    
[^66]: 使用机器学习的动态风险评分提前预测心源性休克

    A dynamic risk score for early prediction of cardiogenic shock using machine learning. (arXiv:2303.12888v1 [cs.LG])

    [http://arxiv.org/abs/2303.12888](http://arxiv.org/abs/2303.12888)

    该研究基于深度学习开发了一个风险分层工具CShock，旨在针对急性失代偿性心力衰竭和/或心肌梗死患者预测心源性休克的发作。

    

    心肌梗死和心力衰竭是主要的心血管疾病，影响着美国数百万人的健康。发展心源性休克的患者中，发病率和死亡率最高。心源性休克的早期识别至关重要，及时实施治疗措施可以防止缺血、低血压以及由于心源性休克导致心输出量降低的有害循环。然而，由于心脏监护病房中海量数据的信息处理能力与缺乏有效的风险分层工具，对心源性休克的早期识别一直具有挑战性。我们基于深度学习开发了一个称为CShock的风险分层工具，用于预测入住心脏监护病房的急性失代偿性心力衰竭和/或心肌梗死患者的心源性休克发作。为了开发和验证CShock，我们使用由医师裁定的结果注释了心脏监护病房数据集。

    Myocardial infarction and heart failure are major cardiovascular diseases that affect millions of people in the US. The morbidity and mortality are highest among patients who develop cardiogenic shock. Early recognition of cardiogenic shock is critical. Prompt implementation of treatment measures can prevent the deleterious spiral of ischemia, low blood pressure, and reduced cardiac output due to cardiogenic shock. However, early identification of cardiogenic shock has been challenging due to human providers' inability to process the enormous amount of data in the cardiac intensive care unit (ICU) and lack of an effective risk stratification tool. We developed a deep learning-based risk stratification tool, called CShock, for patients admitted into the cardiac ICU with acute decompensated heart failure and/or myocardial infarction to predict onset of cardiogenic shock. To develop and validate CShock, we annotated cardiac ICU datasets with physician adjudicated outcomes. CShock achieved
    
[^67]: 概念驱动的AI系统中的人类不确定性

    Human Uncertainty in Concept-Based AI Systems. (arXiv:2303.12872v1 [cs.HC])

    [http://arxiv.org/abs/2303.12872](http://arxiv.org/abs/2303.12872)

    本研究探讨了人类不确定性对概念驱动AI系统的影响，通过控制数据集的干扰因素，分析了现有模型的处理方法。

    

    在安全关键领域中部署AI系统（如医疗AI系统与临床医生一起工作）时，将人类放入其中可能会减轻一些风险。然而，缓解人间误差和不确定因素在此类人工智能交互中引起的风险，是一个重要的且未被研究充分的问题。在本文中，我们研究了在概念驱动模型中人类不确定性的问题，这是一类在AI系统中启用概念干预功能的模型。该功能是指在与任务相关的人类可解释概念上，专家对其进行干预以获得人类反馈。之前的工作已经对此进行了研究，但通常假设人类是预言家，总是确定和正确的。然而，实际中人类的决策过程往往也会出现偶尔的错误和不确定性。我们通过两个新型数据集（UMNIST和CUB-S）探讨了现有的概念驱动模型如何处理来自人类的不确定干预。

    Placing a human in the loop may abate the risks of deploying AI systems in safety-critical settings (e.g., a clinician working with a medical AI system). However, mitigating risks arising from human error and uncertainty within such human-AI interactions is an important and understudied issue. In this work, we study human uncertainty in the context of concept-based models, a family of AI systems that enable human feedback via concept interventions where an expert intervenes on human-interpretable concepts relevant to the task. Prior work in this space often assumes that humans are oracles who are always certain and correct. Yet, real-world decision-making by humans is prone to occasional mistakes and uncertainty. We study how existing concept-based models deal with uncertain interventions from humans using two novel datasets: UMNIST, a visual dataset with controlled simulated uncertainty based on the MNIST dataset, and CUB-S, a relabeling of the popular CUB concept dataset with rich, d
    
[^68]: 用于时间理解的显著性跨度掩蔽

    Salient Span Masking for Temporal Understanding. (arXiv:2303.12860v1 [cs.CL])

    [http://arxiv.org/abs/2303.12860](http://arxiv.org/abs/2303.12860)

    本文介绍了一种用于时间理解的显著性跨度掩蔽技术，通过引入Temporal Span Masking中间训练并与Salient Span Masking结合使用，有效提高多个时间任务的性能及表示效果。

    

    显著性跨度掩蔽 (SSM) 已经被证明是提高封闭式问答性能的有效策略。 SSM通过创建额外的无监督训练句子对普通遮蔽语言模型进行扩展，这些句子屏蔽了一个实体或日期跨度，从而过度取样了事实信息。 尽管这种范式很成功，但跨度类型和采样策略相对任意，并且不被广泛研究用于其他任务。 因此，我们从时间任务的角度研究了SSM，在这些任务中学习各种时间表达的良好表示非常重要。 为此，我们引入了中间培训Temporal Span Masking (TSM)。首先，我们发现仅使用SSM就可以平均改善三个时间任务的下游性能5.8个点。此外，我们通过添加TSM任务能够实现额外的改进（平均+0.29个点）。这些是目标任务报告的新最佳结果。我们的分析表明，SSM和TSM策略的效果对于多个时间任务是通用的，并且可以相互补充。

    Salient Span Masking (SSM) has shown itself to be an effective strategy to improve closed-book question answering performance. SSM extends general masked language model pretraining by creating additional unsupervised training sentences that mask a single entity or date span, thus oversampling factual information. Despite the success of this paradigm, the span types and sampling strategies are relatively arbitrary and not widely studied for other tasks. Thus, we investigate SSM from the perspective of temporal tasks, where learning a good representation of various temporal expressions is important. To that end, we introduce Temporal Span Masking (TSM) intermediate training. First, we find that SSM alone improves the downstream performance on three temporal tasks by an avg. +5.8 points. Further, we are able to achieve additional improvements (avg. +0.29 points) by adding the TSM task. These comprise the new best reported results on the targeted tasks. Our analysis suggests that the effec
    
[^69]: 线性存在规则的半无记忆追赶终止的实验研究

    Semi-Oblivious Chase Termination for Linear Existential Rules: An Experimental Study. (arXiv:2303.12851v1 [cs.DB])

    [http://arxiv.org/abs/2303.12851](http://arxiv.org/abs/2303.12851)

    本文关注线性存在规则的半无记忆追赶算法及其终止问题，提供了一个名为LERT的工具，通过实验评估并得到多个理论和实验结果，并发掘了新的有趣研究方向。

    

    追赶过程是数据库中用于处理约束(例如存在规则)并且具有多种应用的基本算法工具。它接受数据库和约束集作为输入，并根据约束迭代地完成数据库。然而，一个关键的挑战是它可能不会终止，这导致需要检查给定数据库和一组约束是否终止的问题。本文关注于半无记忆追赶算法的应用，它非常适合实际实现，并针对具有多个应用的中心约束类别，即线性存在规则。在这种设定下，已有较为成熟的理论工作，提供了可追赶终止的句法表征、检查追赶终止的算法、精确的复杂度结果，以及在有限情况下追赶结果大小的最坏情况下的最优边界。我们的主要目标是实验研究半无记忆追赶算法对于线性存在规则的终止行为。

    The chase procedure is a fundamental algorithmic tool in databases that allows us to reason with constraints, such as existential rules, with a plethora of applications. It takes as input a database and a set of constraints, and iteratively completes the database as dictated by the constraints. A key challenge, though, is the fact that it may not terminate, which leads to the problem of checking whether it terminates given a database and a set of constraints. In this work, we focus on the semi-oblivious version of the chase, which is well-suited for practical implementations, and linear existential rules, a central class of constraints with several applications. In this setting, there is a mature body of theoretical work that provides syntactic characterizations of when the chase terminates, algorithms for checking chase termination, precise complexity results, and worst-case optimal bounds on the size of the result of the chase (whenever is finite). Our main objective is to experiment
    
[^70]: 量子动力学的学习的能力与局限性

    The power and limitations of learning quantum dynamics incoherently. (arXiv:2303.12834v1 [quant-ph])

    [http://arxiv.org/abs/2303.12834](http://arxiv.org/abs/2303.12834)

    本文证明在不相干框架下，通过浅层测量只能有效地学习低纠缠门。虽然该类框架为我们在不同物理平台之间转移量子过程提供了方法，但也存在一定的局限性。

    

    量子过程学习是研究量子系统的重要工具之一。然而大部分研究都放在了自旋相干和器件自耦合的波动函数上，研究量子动力学在系统和目标不直接交互的情况下是否可以被学习并没有得到足够的关注。这类不相干的框架实际上非常吸引人，因为它们能够在不需要挑战性的混合纠缠方案中，为我们提供在不同物理平台之间转移量子过程的方法。在本文中，我们通过分析需要仿真的明确的相干学习策略的测量次数，提供了在不相干框架下学习幺正过程样本复杂度的界限。我们证明，如果允许任意测量，则任何有效表示的幺正矩阵都可以在不相干框架内被有效地学习；然而，如果仅限于浅层测量，则只能有效地学习低纠缠门。因此，我们的工作突出了学习量子动力学在不相干框架中的能力与局限性。

    Quantum process learning is emerging as an important tool to study quantum systems. While studied extensively in coherent frameworks, where the target and model system can share quantum information, less attention has been paid to whether the dynamics of quantum systems can be learned without the system and target directly interacting. Such incoherent frameworks are practically appealing since they open up methods of transpiling quantum processes between the different physical platforms without the need for technically challenging hybrid entanglement schemes. Here we provide bounds on the sample complexity of learning unitary processes incoherently by analyzing the number of measurements that are required to emulate well-established coherent learning strategies. We prove that if arbitrary measurements are allowed, then any efficiently representable unitary can be efficiently learned within the incoherent framework; however, when restricted to shallow-depth measurements only low-entangl
    
[^71]: 面向复合攻击的非线性多智能体系统数据驱动的从属一致性控制：双生层方法

    Data-Driven Leader-following Consensus for Nonlinear Multi-Agent Systems against Composite Attacks: A Twins Layer Approach. (arXiv:2303.12823v1 [eess.SY])

    [http://arxiv.org/abs/2303.12823](http://arxiv.org/abs/2303.12823)

    本文提出了一种面向复合攻击的非线性多智能体系统的数据驱动从属一致性控制方法，利用双生层方法解决了分布式估计和弹性分布式跟踪控制两部分任务，具有较强的鲁棒性和可扩展性。

    

    本文研究了面对复合攻击（包括拒绝服务攻击和执行攻击）的不确定和非线性多智能体系统的领导者从属一致性。通过添加数字双胞胎层（TL，受最近数字双胞胎技术的启发）来构建双层控制框架。因此，对抗复合攻击的弹性控制任务被分为两部分：一部分是针对TL上的DoS攻击的分布式估计，另一部分是对抗CPL上的执行攻击的弹性分布式跟踪控制。数据驱动方案用于处理模型非线性和模型不确定性，其中整个控制过程只使用系统的输入和输出数据。首先，基于切换估计规律的分布式观测器被设计用于在TL上针对DoS攻击。其次，基于双生层的分布式无模型自适应控制（DMFAC）协议被提出，用于每个智能体在CPL上处理AAs。仿真结果显示了所提出方法的有效性。

    This paper studies the leader-following consensuses of uncertain and nonlinear multi-agent systems against composite attacks (CAs), including Denial of Service (DoS) attacks and actuation attacks (AAs). A double-layer control framework is formulated, where a digital twin layer (TL) is added beside the traditional cyber-physical layer (CPL), inspired by the recent Digital Twin technology. Consequently, the resilient control task against CAs can be divided into two parts: One is distributed estimation against DoS attacks on the TL and the other is resilient decentralized tracking control against actuation attacks on the CPL. %The data-driven scheme is used to deal with both model non-linearity and model uncertainty, in which only the input and output data of the system are employed throughout the whole control process. First, a distributed observer based on switching estimation law against DoS is designed on TL. Second, a distributed model free adaptive control (DMFAC) protocol based on 
    
[^72]: 利用离散手势令牌学习的共性语言手势合成

    Co-Speech Gesture Synthesis using Discrete Gesture Token Learning. (arXiv:2303.12822v1 [cs.CV])

    [http://arxiv.org/abs/2303.12822](http://arxiv.org/abs/2303.12822)

    该论文提出了一个两阶段的机制，使用离散的编码方式来解决合成共性语言手势中的不确定性问题，采用VAE和自回归变压器模型进行学习，能够生成多样化和逼真的共性语言手势。

    

    制作逼真的共性语言手势是一个重要且尚未解决的问题，可以用于驱动人形机器人与人类用户进行交互和沟通。这种能力将改善人类用户对机器人的印象，并在教育、培训和医疗服务中找到应用。学习共性语言手势模型的一个挑战是，对于同一语音话语，可能存在多个合理的手势运动。确定性回归方法无法解决冲突样本，并可能产生过度平滑或抑制的运动。我们提出了一个两阶段模型，通过将手势片段建模为离散的潜在编码来解决这个不确定性问题，我们的方法利用RQ-VAE在第一阶段从训练数据中学习由手势令牌组成的离散码本，第二阶段使用两级自回归变压器模型学习残余码的先验分布，以及给出语音时手势令牌的条件分布。在大型数据集上的实验证明，所提出的方法可以生成多样化和逼真的共性语言手势。

    Synthesizing realistic co-speech gestures is an important and yet unsolved problem for creating believable motions that can drive a humanoid robot to interact and communicate with human users. Such capability will improve the impressions of the robots by human users and will find applications in education, training, and medical services. One challenge in learning the co-speech gesture model is that there may be multiple viable gesture motions for the same speech utterance. The deterministic regression methods can not resolve the conflicting samples and may produce over-smoothed or damped motions. We proposed a two-stage model to address this uncertainty issue in gesture synthesis by modeling the gesture segments as discrete latent codes. Our method utilizes RQ-VAE in the first stage to learn a discrete codebook consisting of gesture tokens from training data. In the second stage, a two-level autoregressive transformer model is used to learn the prior distribution of residual codes cond
    
[^73]: 一种用于创建深度学习模型的可视化编程工具

    Towards A Visual Programming Tool to Create Deep Learning Models. (arXiv:2303.12821v1 [cs.HC])

    [http://arxiv.org/abs/2303.12821](http://arxiv.org/abs/2303.12821)

    DeepBlocks是一款可视化编程工具，允许DL开发人员设计、训练和评估模型，而无需依赖特定的编程语言。其通过构建典型模型结构实现其工作原理，结果表明开发人员可以视觉上设计复杂的DL架构。

    

    深度学习（DL）开发人员来自不同的背景，例如医学、基因组学、金融和计算机科学。为了创建DL模型，他们必须学习和使用高级编程语言（例如Python），因此需要处理相关设置和解决编程错误。本文介绍了DeepBlocks，这是一款可视化编程工具，允许DL开发人员设计、训练和评估模型，而无需依赖特定的编程语言。DeepBlocks通过构建典型模型结构实现其工作原理：一系列可学习函数的顺序排列定义了模型的特定特征。我们通过对5个参与者的形式化访谈推导出了DeepBlocks的设计目标，并通过一个典型用例验证了该工具的第一个实现。结果是令人兴奋的，表明开发人员可以视觉上设计复杂的DL架构。

    Deep Learning (DL) developers come from different backgrounds, e.g., medicine, genomics, finance, and computer science. To create a DL model, they must learn and use high-level programming languages (e.g., Python), thus needing to handle related setups and solve programming errors. This paper presents DeepBlocks, a visual programming tool that allows DL developers to design, train, and evaluate models without relying on specific programming languages. DeepBlocks works by building on the typical model structure: a sequence of learnable functions whose arrangement defines the specific characteristics of the model. We derived DeepBlocks' design goals from a 5-participants formative interview, and we validated the first implementation of the tool through a typical use case. Results are promising and show that developers could visually design complex DL architectures.
    
[^74]: 从宽到深：维度提升网络用于参数高效的知识图谱嵌入

    From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])

    [http://arxiv.org/abs/2303.12816](http://arxiv.org/abs/2303.12816)

    本文提出了一个用于实现参数高效的知识图谱嵌入的深度网络，通过增加深度克服因采用低维实体表示而导致的模型精度下降和模型参数减少有限的问题。

    

    知识图谱嵌入（KGE）将实体和关系映射到向量表示对于下游任务非常重要。传统的KGE方法需要相对高维的实体表示来保留知识图谱的结构信息，但会导致庞大的模型参数。最近的方法通过采用低维实体表示来降低模型参数，同时开发技术（例如知识蒸馏）来补偿降维。然而，这样的操作会导致模型精度下降和模型参数减少有限。具体来说，我们将所有实体表示的级联视为嵌入层，那么采用高维实体表示的传统KGE方法等同于扩展嵌入层的宽度以获得表现力。为了在不牺牲准确度的情况下实现参数效率，我们相反地增加深度，并提出一个更深的实体嵌入网络。

    Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
    
[^75]: SignCRF: 可扩展的无频道数据驱动射频认证系统

    SignCRF: Scalable Channel-agnostic Data-driven Radio Authentication System. (arXiv:2303.12811v1 [cs.CR])

    [http://arxiv.org/abs/2303.12811](http://arxiv.org/abs/2303.12811)

    SignCRF是一个可扩展的无频道数据驱动射频认证平台，能够高精度地识别无线设备，不受移动性带来的动态信道影响。

    

    无线电频率指纹通过深度学习(RFFDL)是一种数据驱动的物联网身份认证技术，利用与特定设备相关的独特硬件级制造缺陷来识别（指纹）基于传输波形引入的变化的设备。提出的SignCRF是一个可扩展的、无频道的、数据驱动的射频认证平台，在识别基于其独特的制造缺陷的无线设备方面具有无与伦比的精度，并且独立于由移动性引起的动态信道不规则性。SignCRF由三部分组成：(i)基线分类器，经过精细训练，能够高精度地扩展认证设备;(ii)环境翻译器，经过精心设计和训练，能够从RF信号中去除动态信道影响，同时保持收发机具体的信号；(iii)最大规则模块选择基线分类器和环境翻译器之间的最高精度认证技术。

    Radio Frequency Fingerprinting through Deep Learning (RFFDL) is a data-driven IoT authentication technique that leverages the unique hardware-level manufacturing imperfections associated with a particular device to recognize (fingerprint) the device based on variations introduced in the transmitted waveform. The proposed SignCRF is a scalable, channel-agnostic, data-driven radio authentication platform with unmatched precision in fingerprinting wireless devices based on their unique manufacturing impairments and independent of the dynamic channel irregularities caused by mobility. SignCRF consists of (i) a baseline classifier finely trained to authenticate devices with high accuracy and at scale; (ii) an environment translator carefully designed and trained to remove the dynamic channel impact from RF signals while maintaining the radio's specific signature; (iii) a Max-Rule module that selects the highest precision authentication technique between the baseline classifier and the envir
    
[^76]: PACO: 包含行动、文化和压迫的挑衅

    PACO: Provocation Involving Action, Culture, and Oppression. (arXiv:2303.12808v1 [cs.CL])

    [http://arxiv.org/abs/2303.12808](http://arxiv.org/abs/2303.12808)

    该研究利用现有的印度WhatsApp帖子数据集，创造了一个可以从WhatsApp帖子中识别挑衅句子的模型PACO，并利用该模型可以防止可能的歧视或暴力事件。

    

    在印度，人们根据某些属性（如宗教）认同于特定群体，同一宗教群体经常相互挑衅。之前的研究表明，挑衅在增加印度两个主要宗教群体之间的紧张关系方面扮演着重要角色。随着互联网的出现，这种挑衅也出现在WhatsApp等社交媒体平台上。通过利用现有的印度WhatsApp帖子数据集，我们识别出了三种针对印度穆斯林的挑衅句子类别。此外，我们为三种挑衅类别标记了7000个句子，并将其称为PACO数据集。我们利用PACO来训练一个可以从WhatsApp帖子中识别挑衅句子的模型。我们的最佳模型是精调RoBERTa，并在五倍交叉验证中实现了0.851的平均AUC分数。自动识别挑衅句子可以阻止挑衅文本扩散到群众之间，可以防止可能的歧视或暴力事件。

    In India, people identify with a particular group based on certain attributes such as religion. The same religious groups are often provoked against each other. Previous studies show the role of provocation in increasing tensions between India's two prominent religious groups: Hindus and Muslims. With the advent of the Internet, such provocation also surfaced on social media platforms such as WhatsApp.  By leveraging an existing dataset of Indian WhatsApp posts, we identified three categories of provoking sentences against Indian Muslims. Further, we labeled 7,000 sentences for three provocation categories and called this dataset PACO. We leveraged PACO to train a model that can identify provoking sentences from a WhatsApp post. Our best model is fine-tuned RoBERTa and achieved a 0.851 average AUC score over five-fold cross-validation. Automatically identifying provoking sentences could stop provoking text from reaching out to the masses, and can prevent possible discrimination or viol
    
[^77]: 粒球优化算法

    Granular-ball Optimization Algorithm. (arXiv:2303.12807v1 [cs.LG])

    [http://arxiv.org/abs/2303.12807](http://arxiv.org/abs/2303.12807)

    粒球优化算法(GBO)是一种新的多粒度优化算法，可以通过引入粒球计算来提高全局搜索能力和收敛速度，实验结果表明，在这些方面它比现有的最先进的算法表现更优。

    

    现有的智能优化算法都是基于最小粒度即点的设计，导致全局搜索能力较弱且效率低下。为了解决这个问题，我们提出了一种新的多粒度优化算法，即粒球优化算法(GBO)，通过引入粒球计算来实现。GBO使用多个粒球来覆盖解空间，使用许多细小的细粒度粒球来描述重要部分，使用少量的大粗粒度粒球来描述不重要的部分，精细的多粒度数据描述能力提高了全局搜索能力和收敛速度。针对二十个基准函数的实验结果表明，与最流行的最先进的算法相比，GBO具有更好的性能和更快的速度，更接近最优解，没有超参数，设计更简单。

    The existing intelligent optimization algorithms are designed based on the finest granularity, i.e., a point. This leads to weak global search ability and inefficiency. To address this problem, we proposed a novel multi-granularity optimization algorithm, namely granular-ball optimization algorithm (GBO), by introducing granular-ball computing. GBO uses many granular-balls to cover the solution space. Quite a lot of small and fine-grained granular-balls are used to depict the important parts, and a little number of large and coarse-grained granular-balls are used to depict the inessential parts. Fine multi-granularity data description ability results in a higher global search capability and faster convergence speed. In comparison with the most popular and state-of-the-art algorithms, the experiments on twenty benchmark functions demonstrate its better performance. The faster speed, higher approximation ability of optimal solution, no hyper-parameters, and simpler design of GBO make it 
    
[^78]: 基于MAP-Elites的多样化强化学习智能体群体进化

    Evolving Populations of Diverse RL Agents with MAP-Elites. (arXiv:2303.12803v1 [cs.NE])

    [http://arxiv.org/abs/2303.12803](http://arxiv.org/abs/2303.12803)

    本文介绍了一种尝试在进化计算与强化学习中相结合解决机器人控制问题的方法，并通过改进ME算法提高了效率和多样性，但也出现了一些常见强化学习算法的限制。

    

    品质多样性(QD)已成为一种强大的替代优化模式，旨在生成大量和多样的解决方案，其代表算法MAP-Elites(ME)通过变异和交叉进化解决方案。虽然ME对于某些非结构化问题非常有效，但早期的ME实现仅依赖于随机搜索来进化解决方案的种群，因此在高维问题中如进化神经网络时常常无法有效地进行，效率极低。后续的研究考虑利用梯度信息通过黑盒优化（BBO）或强化学习（RL）中的技术来引导搜索，以解决这些问题。将RL技巧与ME结合在机器人控制问题中实现了最先进的性能，但是也在ME变体中引入了这些算法的共同限制，例如对探索需求的要求。

    Quality Diversity (QD) has emerged as a powerful alternative optimization paradigm that aims at generating large and diverse collections of solutions, notably with its flagship algorithm MAP-ELITES (ME) which evolves solutions through mutations and crossovers. While very effective for some unstructured problems, early ME implementations relied exclusively on random search to evolve the population of solutions, rendering them notoriously sample-inefficient for high-dimensional problems, such as when evolving neural networks. Follow-up works considered exploiting gradient information to guide the search in order to address these shortcomings through techniques borrowed from either Black-Box Optimization (BBO) or Reinforcement Learning (RL). While mixing RL techniques with ME unlocked state-of-the-art performance for robotics control problems that require a good amount of exploration, it also plagued these ME variants with limitations common among RL algorithms that ME was free of, such a
    
[^79]: 基于深度学习的IoT设备网络通信分析与识别技术

    IoT Device Identification Based on Network Communication Analysis Using Deep Learning. (arXiv:2303.12800v1 [cs.NI])

    [http://arxiv.org/abs/2303.12800](http://arxiv.org/abs/2303.12800)

    内部网络中允许连接的IoT设备和未知的IoT设备的识别变得越发重要。本研究提出了一种基于深度学习的自动识别方法，可以不需对网络通信进行复杂的特征处理。

    

    随着越来越多不安全的IoT设备的使用，对于攻击者来说，入侵组织的攻击方式越发多样。BYOD政策使得员工可以携带IoT设备进入组织并连接到组织的网络，同时也增加了组织网络受攻击的风险。为了应对这一威胁和保护网络，组织通常实施安全策略，只允许列入白名单的IoT设备连接到网络。为了监测这种策略的合规性，识别组织网络内允许连接的IoT设备与不在白名单中（未知的）IoT设备之间的差异已经变得非常关键。本研究将深度学习应用于网络通信中，实现了对网络内IoT设备的自动识别。与现有方法不同的是，所提出的方法不需要对网络通信进行复杂的特征工程处理。

    Attack vectors for adversaries have increased in organizations because of the growing use of less secure IoT devices. The risk of attacks on an organization's network has also increased due to the bring your own device (BYOD) policy which permits employees to bring IoT devices onto the premises and attach them to the organization's network. To tackle this threat and protect their networks, organizations generally implement security policies in which only white listed IoT devices are allowed on the organization's network. To monitor compliance with such policies, it has become essential to distinguish IoT devices permitted within an organization's network from non white listed (unknown) IoT devices. In this research, deep learning is applied to network communication for the automated identification of IoT devices permitted on the network. In contrast to existing methods, the proposed approach does not require complex feature engineering of the network communication, because the 'communi
    
[^80]: 时间序列视为图像：用视觉transformer处理不规则采样时间序列

    Time Series as Images: Vision Transformer for Irregularly Sampled Time Series. (arXiv:2303.12799v1 [cs.LG])

    [http://arxiv.org/abs/2303.12799](http://arxiv.org/abs/2303.12799)

    本文提出了一种新颖的方法，将不规则采样的时间序列转换为线图像，并适应强大的视觉transformer进行时间序列分类。该方法简化了算法设计，具有通用性，并展示了在多个医疗和人体活动数据集上明显优于最先进的专业算法的表现。

    

    在各个领域中，尤其是在医疗应用中，不规则抽样的时间序列越来越普遍。尽管已经提出了不同的高度定制化方法来解决不规则性问题，但如何有效地模拟它们的复杂动态和高稀疏性仍然是一个开放的问题。本文从全新的角度研究了这个问题：将不规则采样的时间序列转换为线图像，并调整强大的视觉transformer以执行与图像分类相同的时间序列分类。我们的方法在不假设先前知识的情况下大大简化了算法设计，并且可以被潜在地扩展为一个通用框架。尽管其简单性，我们展示了它在几个流行的医疗保健和人体活动数据集上明显优于最先进的专业算法。特别是在具有挑战性的离传感器设置中，即在测试期间屏蔽变量的子集中，性能比最佳基准提高了高达11％。

    Irregularly sampled time series are becoming increasingly prevalent in various domains, especially in medical applications. Although different highly-customized methods have been proposed to tackle irregularity, how to effectively model their complicated dynamics and high sparsity is still an open problem. This paper studies the problem from a whole new perspective: transforming irregularly sampled time series into line graph images and adapting powerful vision transformers to perform time series classification in the same way as image classification. Our approach largely simplifies algorithm designs without assuming prior knowledge and can be potentially extended as a general-purpose framework. Despite its simplicity, we show that it substantially outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Especially in the challenging leave-sensors-out setting where a subset of variables is masked during testing, the performance impr
    
[^81]: 一种用于深度神经网络架构和超参数优化的算法框架

    An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters. (arXiv:2303.12797v1 [cs.NE])

    [http://arxiv.org/abs/2303.12797](http://arxiv.org/abs/2303.12797)

    本文提出一种基于进化的有向无环图的算法框架，用于自动生成高效且灵活的深度神经网络并优化相关的超参数。此框架可用于任何能够处理混合搜索空间的元启发式算法，并在时间序列预测数据集上表现出比已有模型更好的性能。

    

    本文提出一种算法框架，用于自动生成高效的深度神经网络并优化相关的超参数。该框架基于进化的有向无环图(DAG)，定义了比文献中现有的搜索空间更为灵活的搜索空间，允许混合使用传统操作，如卷积、循环和密集层，以及较为新颖的操作，如自注意力机制。基于该搜索空间，我们提出了邻域搜索算子和演化搜索算子，以优化网络的架构和超参数。这些搜索算子可与任何能够处理混合搜索空间的元启发式算法一起使用。我们在时间序列预测数据集上使用进化算法测试了我们的算法框架。结果表明，我们的框架能够找到在许多数据集上性能优于基准模型的模型。

    In this paper, we propose an algorithmic framework to automatically generate efficient deep neural networks and optimize their associated hyperparameters. The framework is based on evolving directed acyclic graphs (DAGs), defining a more flexible search space than the existing ones in the literature. It allows mixtures of different classical operations: convolutions, recurrences and dense layers, but also more newfangled operations such as self-attention. Based on this search space we propose neighbourhood and evolution search operators to optimize both the architecture and hyper-parameters of our networks. These search operators can be used with any metaheuristic capable of handling mixed search spaces. We tested our algorithmic framework with an evolutionary algorithm on a time series prediction benchmark. The results demonstrate that our framework was able to find models outperforming the established baseline on numerous datasets.
    
[^82]: 使用预训练模型进行抽象文本摘要的分析

    An Analysis of Abstractive Text Summarization Using Pre-trained Models. (arXiv:2303.12796v1 [cs.CL])

    [http://arxiv.org/abs/2303.12796](http://arxiv.org/abs/2303.12796)

    本文对使用预训练模型进行文本摘要的方法进行了评估，并在不同数据集上进行了比较，结果表明......

    

    人们现在使用像谷歌、雅虎和必应这样的搜索引擎来查找互联网上的信息。由于数据爆炸，如果为用户提供相关的搜索结果摘要而不仅仅是网页链接将会很有帮助。文本摘要已成为帮助用户迅速掌握大量信息的关键方法。在本文中，对不同的预训练模型进行了在不同数据集上的评估。具体来说，我们使用了三个不同的预训练模型，分别是google/pegasus-cnn-dailymail、T5-base、facebook/bart-large-cnn。我们考虑了三个不同的数据集，分别是CNN-dailymail、SAMSum和BillSum，以从上述三个模型中获取输出。通过ROUGH和BLEU指标，在这些不同的数据集上比较了这些预训练模型，每个数据集有2000个示例。

    People nowadays use search engines like Google, Yahoo, and Bing to find information on the Internet. Due to explosion in data, it is helpful for users if they are provided relevant summaries of the search results rather than just links to webpages. Text summarization has become a vital approach to help consumers swiftly grasp vast amounts of information.In this paper, different pre-trained models for text summarization are evaluated on different datasets. Specifically, we have used three different pre-trained models, namely, google/pegasus-cnn-dailymail, T5-base, facebook/bart-large-cnn. We have considered three different datasets, namely, CNN-dailymail, SAMSum and BillSum to get the output from the above three models. The pre-trained models are compared over these different datasets, each of 2000 examples, through ROUGH and BLEU metrics.
    
[^83]: 基于命名实体识别的研究亮点自动生成技术研究

    Named Entity Recognition Based Automatic Generation of Research Highlights. (arXiv:2303.12795v1 [cs.CL])

    [http://arxiv.org/abs/2303.12795](http://arxiv.org/abs/2303.12795)

    该研究使用命名实体识别技术自动生成研究亮点，探究其是否能提高生成亮点的质量。实验结果表明，增加命名实体信息可以提高生成亮点的性能。

    

    传统科学论文摘要用于总结论文内容。近期，研究亮点作为摘要的补充，聚焦于论文的主要发现，但使用频率还不如摘要普遍。该研究旨在使用论文不同部分的输入，自动生成研究亮点。研究使用命名实体识别技术，探究它能否改进生成研究亮点的质量。研究使用两个深度学习模型：第一个是指针-生成器网络，第二个在第一个模型的基础上增加了覆盖机制。 然后将上述每个模型与命名实体识别特征相结合。该方法可用于为缺少亮点的论文生成亮点。实验结果显示，增加命名实体信息可以提高深度学习模型生成高质量研究亮点的性能。

    A scientific paper is traditionally prefaced by an abstract that summarizes the paper. Recently, research highlights that focus on the main findings of the paper have emerged as a complementary summary in addition to an abstract. However, highlights are not yet as common as abstracts, and are absent in many papers. In this paper, we aim to automatically generate research highlights using different sections of a research paper as input. We investigate whether the use of named entity recognition on the input improves the quality of the generated highlights. In particular, we have used two deep learning-based models: the first is a pointer-generator network, and the second augments the first model with coverage mechanism. We then augment each of the above models with named entity recognition features. The proposed method can be used to produce highlights for papers with missing highlights. Our experiments show that adding named entity information improves the performance of the deep learn
    
[^84]: 从文本到图像生成: 基于稳定扩散模型构建建筑立面设计的方法

    Text Semantics to Image Generation: A method of building facades design base on Stable Diffusion model. (arXiv:2303.12755v1 [cs.CV])

    [http://arxiv.org/abs/2303.12755](http://arxiv.org/abs/2303.12755)

    本文提出了一种多网络结合的文本到建筑立面图像生成方法，通过 LoRA 训练方法微调稳定扩散模型和 ControlNet 模型的添加，大大提高了文本到建筑立面图像生成的可控性和稳定性，为后续建筑图像生成研究提供了基础。

    

    稳定扩散模型已广泛应用于建筑图像生成的研究中，但目前仍有提高生成图像内容可控性的机会。本文提出了一种多网络结合的文本到建筑立面图像生成方法。我们首先通过 LoRA（低秩自适应）方法在 CMP Fa-cades 数据集上对稳定扩散模型进行了微调，然后应用 ControlNet 模型进一步控制输出。最后，我们对不同建筑风格文本内容和控制策略下的立面生成结果进行了对比。结果表明，LoRA 训练方法显着降低了微调稳定扩散大模型的可能性，而 ControlNet 模型的添加增加了文本到建筑立面图像的可控性。这为后续关于建筑图像生成的研究提供了基础。

    Stable Diffusion model has been extensively employed in the study of archi-tectural image generation, but there is still an opportunity to enhance in terms of the controllability of the generated image content. A multi-network combined text-to-building facade image generating method is proposed in this work. We first fine-tuned the Stable Diffusion model on the CMP Fa-cades dataset using the LoRA (Low-Rank Adaptation) approach, then we ap-ply the ControlNet model to further control the output. Finally, we contrast-ed the facade generating outcomes under various architectural style text con-tents and control strategies. The results demonstrate that the LoRA training approach significantly decreases the possibility of fine-tuning the Stable Dif-fusion large model, and the addition of the ControlNet model increases the controllability of the creation of text to building facade images. This pro-vides a foundation for subsequent studies on the generation of architectural images.
    
[^85]: cTBL：增强大型语言模型用于对话表格

    cTBL: Augmenting Large Language Models for Conversational Tables. (arXiv:2303.12024v1 [cs.CL])

    [http://arxiv.org/abs/2303.12024](http://arxiv.org/abs/2303.12024)

    本论文提出了一种称为cTBL的方法，可以从表格中检索信息，并生成具有检索信息支撑的对话响应，其中使用了转换器编码器嵌入进行浓密表检索，可以获得更好的性能。

    

    多模态对话人工智能中一个开放的挑战是如何从文本和非文本来源中增强大型语言模型以进行多轮对话。为了解决这个问题，本文引入了Conversation Table (cTBL)，这是一种三步编码器-解码器方法，用于检索表格信息并生成基于检索信息的对话响应。cTBL使用转换器编码器嵌入进行浓密表检索，并在HyrbiDialogue数据集Top-1和Top-3准确性上相对于稀疏检索提高了最多5%。此外，cTBL使用编码器和解码器模型进行表格知识检索，在HyrbiDialogue上产生了最高46%的ROUGE分数相对改进，并实现了更好的人工评估响应生成。

    An open challenge in multimodal conversational AI requires augmenting large language models with information from textual and non-textual sources for multi-turn dialogue. To address this problem, this paper introduces Conversational Tables (cTBL), a three-step encoder-decoder approach to retrieve tabular information and generate dialogue responses grounded on the retrieved information. cTBL uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over sparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs tabular knowledge retrieval using both encoder and decoder models, resulting in up to 46% relative improvement in ROUGE scores and better human evaluation for response generation on HyrbiDialogue.
    
[^86]: CroSel: 用于部分标签学习的自信伪标签的跨选择

    CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning. (arXiv:2303.10365v1 [cs.LG])

    [http://arxiv.org/abs/2303.10365](http://arxiv.org/abs/2303.10365)

    CroSel是一种处理伪标签噪声的新方法，通过利用历史预测信息和一致性正则化项来准确识别部分标签数据的真实标签。

    

    部分标签学习(PLL)是一个重要的弱监督学习问题，它允许每个训练示例有一个候选标签集，而不是一个单一的ground-truth标签。已经广泛探索了基于识别的方法来解决PLL中的标签歧义问题，这些方法将真实标签视为要识别的潜在变量。然而，准确和完整地识别真实标签仍然具有挑战性，这会在模型训练过程中导致伪标签中的噪声。本文提出了一种名为CroSel的新方法，该方法利用模型的历史预测信息来识别大多数训练示例的真实标签。首先，我们引入了一种交叉选择策略，使得两个深度模型可以相互选择部分标记数据的真实标签。此外，我们提出了一种新颖的一致性正则化项co-mix，以避免因虚假选择而引起的样本浪费和微小噪声。通过这种方式，CroSel能够挑选出大多数示例的真实标签。

    Partial-label learning (PLL) is an important weakly supervised learning problem, which allows each training example to have a candidate label set instead of a single ground-truth label. Identification-based methods have been widely explored to tackle label ambiguity issues in PLL, which regard the true label as a latent variable to be identified. However, identifying the true labels accurately and completely remains challenging, causing noise in pseudo labels during model training. In this paper, we propose a new method called CroSel, which leverages historical prediction information from models to identify true labels for most training examples. First, we introduce a cross selection strategy, which enables two deep models to select true labels of partially labeled data for each other. Besides, we propose a novel consistent regularization term called co-mix to avoid sample waste and tiny noise caused by false selection. In this way, CroSel can pick out the true labels of most examples 
    
[^87]: CVT-SLR：基于对比视觉-文本变换与变分对齐的手语识别模型

    CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment. (arXiv:2303.05725v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05725](http://arxiv.org/abs/2303.05725)

    CVT-SLR是一种新的手语识别模型，它采用基于对比视觉-文本变换和变分对齐的方法来充分利用跨模态知识，为解决手语识别中缺乏大规模可用数据集的问题提供了一种新的解决方案。

    

    手语识别 (SLR) 是一项弱监督任务，可以将手语视频注释为文本。最近的研究表明，由于缺乏大规模可用的手语数据集而导致的不充分训练成为 SLR 的主要瓶颈。因此，大多数 SLR 的工作采用预训练的视觉模块，并开发了两种主流解决方案。本文提出了一种新型的对比视觉-文本变换模型 CVT-SLR，充分发挥了视觉和语言模态的预训练知识。

    Sign language recognition (SLR) is a weakly supervised task that annotates sign videos as textual glosses. Recent studies show that insufficient training caused by the lack of large-scale available sign language datasets becomes the main bottleneck for SLR. The majority of SLR works thereby adopt pretrained visual modules and develop two mainstream solutions. The multi-stream architectures extend multi-cue visual features, yielding the current SOTA performances but requiring complex designs and might introduce potential noise. Alternatively, the advanced single-cue SLR frameworks using explicit cross-modal alignment between visual and textual modalities are simple and effective, potentially competitive with the multi-cue framework. In this work, we propose a novel contrastive visual-textual transformation for SLR, CVT-SLR, to fully explore the pretrained knowledge of both the visual and language modalities. Based on the single-cue cross-modal alignment framework, we propose a variation
    
[^88]: 区域隐形补丁：基于生成对抗网络的物理对抗攻击物体检测器

    Patch of Invisibility: Naturalistic Black-Box Adversarial Attacks on Object Detectors. (arXiv:2303.04238v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.04238](http://arxiv.org/abs/2303.04238)

    本文提出了一种基于GAN的无梯度物理对抗攻击方法，用于生成自然的对抗补丁，攻击物体检测器，具有实际应用价值。

    

    近年来，深度学习模型的对抗攻击越来越引起关注。这一领域的研究大多集中在基于梯度的技术，即所谓的白盒攻击，在其中攻击者可以访问目标模型的内部参数。然而，这种假设在实际世界中通常是不现实的。相对地，我们提出了一种在无需使用梯度的情况下，利用预先训练的生成对抗网络（GAN）的学习图像流形来生成自然的物理对抗补丁，用于物体检测器的攻击方法。我们展示了我们提出的方法在数字和物理层面上均可行。

    Adversarial attacks on deep-learning models have been receiving increased attention in recent years. Work in this area has mostly focused on gradient-based techniques, so-called white-box attacks, wherein the attacker has access to the targeted model's internal parameters; such an assumption is usually unrealistic in the real world. Some attacks additionally use the entire pixel space to fool a given model, which is neither practical nor physical (i.e., real-world). On the contrary, we propose herein a gradient-free method that uses the learned image manifold of a pretrained generative adversarial network (GAN) to generate naturalistic physical adversarial patches for object detectors. We show that our proposed method works both digitally and physically.
    
[^89]: 作为解释的因果反事实和归属分数在人工智能中的应用

    Attribution-Scores and Causal Counterfactuals as Explanations in Artificial Intelligence. (arXiv:2303.02829v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.02829](http://arxiv.org/abs/2303.02829)

    本文介绍了归属分数和因果反事实在解释人工智能中的应用，重点关注因果关系领域中的逻辑推理和分数计算。

    

    在本文中，我们突出了解释对人工智能的普遍影响和解释人工智能的新发展的重要性，包括起源和不同方法之间的联系。我们用简单的术语描述了基于归属分数的数据管理和机器学习的解释，以及在因果关系领域中发现的反事实。我们阐述了在处理反事实时逻辑推理的重要性，以及它们用于计算分数的用途。

    In this expository article we highlight the relevance of explanations for artificial intelligence, in general, and for the newer developments in {\em explainable AI}, referring to origins and connections of and among different approaches. We describe in simple terms, explanations in data management and machine learning that are based on attribution-scores, and counterfactuals as found in the area of causality. We elaborate on the importance of logical reasoning when dealing with counterfactuals, and their use for score computation.
    
[^90]: 在在线连续学习中进行实时评估：一个新希望

    Real-Time Evaluation in Online Continual Learning: A New Hope. (arXiv:2302.01047v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01047](http://arxiv.org/abs/2302.01047)

    该研究针对现实环境中的连续学习提出了一种实时评估方法，并在包含3900万个时间戳标记图像的大型数据集CLOC上进行了实验。结果表明，简单的基线模型胜过了最先进的CL方法，证明现有方法在现实环境下的适用性存在问题。

    

    当前连续学习方法的评估通常假设在训练时间和计算方面没有限制。这对于任何实际世界的环境都是不现实的。因此我们提出了一种实时评估连续学习的方法，其中流不等待模型完成训练即揭示下一个数据进行预测。为了实现这一点，我们从计算成本的角度评估当前的CL方法，并在包含3900万个时间戳标记图像的大型数据集CLOC上进行了广泛的实验。结果表明，在这种评估下，一个简单的基线模型胜过了最先进的CL方法，这对现有方法在现实环境中的适用性提出了质疑。另外，我们还探讨了文献中常用的各种CL组件，包括记忆采样策略和正则化方法。我们发现，所有考虑的方法都无法与我们的简单基线模型竞争。

    Current evaluations of Continual Learning (CL) methods typically assume that there is no constraint on training time and computation. This is an unrealistic assumption for any real-world setting, which motivates us to propose: a practical real-time evaluation of continual learning, in which the stream does not wait for the model to complete training before revealing the next data for predictions. To do this, we evaluate current CL methods with respect to their computational costs. We conduct extensive experiments on CLOC, a large-scale dataset containing 39 million time-stamped images with geolocation labels. We show that a simple baseline outperforms state-of-the-art CL methods under this evaluation, questioning the applicability of existing methods in realistic settings. In addition, we explore various CL components commonly used in the literature, including memory sampling strategies and regularization approaches. We find that all considered methods fail to be competitive against ou
    
[^91]: DepGraph: 实现任何结构剪枝

    DepGraph: Towards Any Structural Pruning. (arXiv:2301.12900v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.12900](http://arxiv.org/abs/2301.12900)

    这篇论文提出了一种通用且完全自动化的方法，称为“Dependency Graph”（DepGraph），用于解决任何结构剪枝，它明确模型层之间的依赖关系，以避免出现结构问题和显著的性能下降。

    

    结构剪枝通过从神经网络中删除结构分组参数来加速模型，然而，在不同模型中，参数分组模式差异很大，依赖于手动设计的分组方案的特定结构剪枝器无法推广到新的架构。在这项工作中，我们研究了一项极具挑战性但鲜有探索的任务，任何结构剪枝，以应对任意架构（如CNN、RNN、GNN和Transformer）的通用结构剪枝。实现这一目标的最大障碍在于结构耦合，它不仅强制同时剪枝不同层，而且还期望所有已删除参数都一致不重要，从而避免结构问题和剪枝后的性能显著下降。为解决这个问题，我们提出了一种通用和完全自动化的方法，“依赖图”（DepGraph），来明确模型层之间的依赖关系。

    Structural pruning enables model acceleration by removing structurally-grouped parameters from neural networks. However, the parameter-grouping patterns vary widely across different models, making architecture-specific pruners, which rely on manually-designed grouping schemes, non-generalizable to new architectures. In this work, we study a highly-challenging yet barely-explored task, any structural pruning, to tackle general structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and Transformers. The most prominent obstacle towards this goal lies in the structural coupling, which not only forces different layers to be pruned simultaneously, but also expects all removed parameters to be consistently unimportant, thereby avoiding structural issues and significant performance degradation after pruning. To address this problem, we propose a general and {fully automatic} method, \emph{Dependency Graph} (DepGraph), to explicitly model the dependency between layers and comprehens
    
[^92]: 通过广义策略优化优先级实现高效多目标学习

    Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization. (arXiv:2301.07784v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.07784](http://arxiv.org/abs/2301.07784)

    该论文提出了一种使用广义策略改进优先级来实现高效多目标学习的算法，从而通过主动学习策略，可以识别出每一时刻最有前途的偏好或目标，以更快地解决MORL问题，同时也可以识别出学习特定代理偏好的策略时最相关的历史经验。

    

    多目标强化学习 (MORL) 算法解决了代理在可能冲突的奖励函数上有不同偏好的顺序决策问题。这些算法通常学习一组策略（每个策略都是为不同代理偏好而优化的），这组策略可以后来用于解决具有新偏好的问题。我们介绍了一种新算法，该算法使用广义策略改进 (GPI) 定义了原则上得出的组数，从而提高了固定样本数的学习效率。通过该算法，代理可以实现主动学习策略，并可以在每一时刻确定最有前途的偏好或目标，以更快地解决给定的MORL问题。同时，该算法也可以通过一种新的Dyna风格的MORL方法，识别出学习特定代理偏好的策略时最相关的以往经验。我们证明了我们的算法保证始终在有限的步数内收敛到最优解，或收敛到距离最优解 $\epsilon$-o。

    Multi-objective reinforcement learning (MORL) algorithms tackle sequential decision problems where agents may have different preferences over (possibly conflicting) reward functions. Such algorithms often learn a set of policies (each optimized for a particular agent preference) that can later be used to solve problems with novel preferences. We introduce a novel algorithm that uses Generalized Policy Improvement (GPI) to define principled, formally-derived prioritization schemes that improve sample-efficient learning. They implement active-learning strategies by which the agent can (i) identify the most promising preferences/objectives to train on at each moment, to more rapidly solve a given MORL problem; and (ii) identify which previous experiences are most relevant when learning a policy for a particular agent preference, via a novel Dyna-style MORL method. We prove our algorithm is guaranteed to always converge to an optimal solution in a finite number of steps, or an $\epsilon$-o
    
[^93]: 单张图像无类别3D关节转移的CA$^2$T-Net网络

    CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image. (arXiv:2301.02232v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.02232](http://arxiv.org/abs/2301.02232)

    本文介绍了一种可以将单张图像中物体的运动转移到未调整的3D模型中的神经网络方法，可以处理任意类别的对象，训练时只使用合成数据。

    

    本文介绍了一种神经网络方法，可以将单帧图像中关节物体的运动转移到未经调整的3D模型中。我们的网络学习预测物体的姿态、部分分割和相应的运动参数，以重现输入图像中显示的关节运动。网络由三个不同的分支组成，它们采用共享的联合图像形状嵌入，并进行端到端的训练。与以往的方法不同，我们的方法不依赖于对象的拓扑结构，并且可以处理来自任意类别的对象。我们的方法仅使用合成数据进行训练，可以自动地为网格添加动画，从真实图像中推断运动，并在测试时间将运动转移到功能上相似但几何上不同的3D模型。

    We present a neural network approach to transfer the motion from a single image of an articulated object to a rest-state (i.e., unarticulated) 3D model. Our network learns to predict the object's pose, part segmentation, and corresponding motion parameters to reproduce the articulation shown in the input image. The network is composed of three distinct branches that take a shared joint image-shape embedding and is trained end-to-end. Unlike previous methods, our approach is independent of the topology of the object and can work with objects from arbitrary categories. Our method, trained with only synthetic data, can be used to automatically animate a mesh, infer motion from real images, and transfer articulation to functionally similar but geometrically distinct 3D models at test time.
    
[^94]: CLIP也是一个高效的分割器：一种基于文本的弱监督语义分割方法

    CLIP is Also an Efficient Segmenter: A Text-Driven Approach for Weakly Supervised Semantic Segmentation. (arXiv:2212.09506v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.09506](http://arxiv.org/abs/2212.09506)

    本文提出了一种基于文本输入的新型WSSS框架CLIP-ES，利用图像级别标签进行弱监督语义分割，通过采用softmax函数、锐度驱动提示选择和同义词融合等策略，提高了WSSS的三个阶段的效率，并成功利用了CLIP的零样本能力，从而实现了高质量的分割掩模生成。

    

    利用图像级别标签进行弱监督的语义分割是一项具有挑战性的任务。本文探讨了对比度语言-图像预训练模型（CLIP）在仅使用图像级别标签的情况下本地化不同类别的潜力，并提出了一种名为CLIP-ES的新型WSSS框架，以高效生成高质量的分割掩模。

    Weakly supervised semantic segmentation (WSSS) with image-level labels is a challenging task. Mainstream approaches follow a multi-stage framework and suffer from high training costs. In this paper, we explore the potential of Contrastive Language-Image Pre-training models (CLIP) to localize different categories with only image-level labels and without further training. To efficiently generate high-quality segmentation masks from CLIP, we propose a novel WSSS framework called CLIP-ES. Our framework improves all three stages of WSSS with special designs for CLIP: 1) We introduce the softmax function into GradCAM and exploit the zero-shot ability of CLIP to suppress the confusion caused by non-target classes and backgrounds. Meanwhile, to take full advantage of CLIP, we re-explore text inputs under the WSSS setting and customize two text-driven strategies: sharpness-based prompt selection and synonym fusion. 2) To simplify the stage of CAM refinement, we propose a real-time class-aware a
    
[^95]: ECON: 通过法线积分进行优化的显式衣着人物重建

    ECON: Explicit Clothed humans Optimized via Normal integration. (arXiv:2212.07422v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.07422](http://arxiv.org/abs/2212.07422)

    结合隐式函数和显式身体模型，ECON方法成功地重建了着装立体人体，并可以恢复宽松服装等自由形式表面。

    

    结合深度学习、艺术家策划的扫描和隐式函数（IF），可以从图像中创建详细的着装立体人体。然而，现有方法尚不完美。基于IF的方法可以恢复自由形式的几何形状，但是对于新姿势或新装的人物，会产生脱离身体的四肢或退化的形状。为了增强这些情况下的鲁棒性，现有工作使用显式参数人体模型约束表面重建，但这限制了从身体偏离的宽松服装等自由形式表面的恢复。我们希望找到一种方法，将隐式表示和显式身体规范化的优点结合起来。为此，我们有两个重要的观察：（1）当前的网络更擅长推断详细的2D贴图而不是完整的三维表面，（2）参数模型可以看作是岀了局部表面拼接的“画布”。基于这些观察，我们的方法ECON有三个主要步骤：

    The combination of deep learning, artist-curated scans, and Implicit Functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry, but produce disembodied limbs or degenerate shapes for novel poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit representation and explicit body regularization. To this end, we make two key observations: (1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a "canvas" for stitching together detailed surface patches. Based on these, our method, ECON, has three main steps: (1) It infers detailed 2D normal m
    
[^96]: 软件工程中的统计因果推断应用

    Applications of statistical causal inference in software engineering. (arXiv:2211.11482v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2211.11482](http://arxiv.org/abs/2211.11482)

    本文回顾了在软件工程领域的32篇相关研究，总结出统计因果推断方法的应用较新，研究社区相对分散。

    

    本文回顾了在软件工程领域应用统计因果推断方法的相关研究工作，该方法旨在从观测数据中估计因果效应。本文综述了2010年至2022年间发表的32篇论文。结果表明，统计因果推断方法的应用相对较新，相应的研究社区仍然相对分散。

    This paper reviews existing work in software engineering that applies statistical causal inference methods. These methods aim at estimating causal effects from observational data. The review covers 32 papers published between 2010 and 2022. Our results show that the application of statistical causal inference methods is relatively recent and that the corresponding research community remains relatively fragmented.
    
[^97]: NAR-Former：面向全面属性预测的神经架构表示学习

    NAR-Former: Neural Architecture Representation Learning towards Holistic Attributes Prediction. (arXiv:2211.08024v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.08024](http://arxiv.org/abs/2211.08024)

    本文提出了一种神经架构表示模型NAR-Former来全面估计神经网络架构的属性，通过标记器和多阶段融合变压器构建紧凑的向量表示，并采用信息流一致性增强和架构一致性损失进行有效的模型训练。实验证实了该模型在准确性和延迟估计方面与最先进的方法相比取得了有竞争力的性能。

    

    随着深度学习模型在实际应用中的广泛和深入采用，越来越需要对神经网络本身的表示进行建模和学习。这些模型可以用于估计不同神经网络架构的属性，例如准确性和延迟，而不需要运行实际的训练或推断任务。本文提出了一种神经架构表示模型，可用于全面估计这些属性。具体地，我们首先提出了一个简单而有效的标记器，将神经网络的操作和拓扑信息编码为一个单一的序列。然后，我们设计了一个多阶段融合变压器，从转换后的序列中构建紧凑的向量表示。为了有效的模型训练，我们进一步提出了信息流一致性增强，并相应地设计了一个架构一致性损失，与预测的损失相比，更少的增广样本带来了更多的好处。在各种基准数据集上的实验证实了所提出的 NAR-Former 模型的有效性，在不同神经架构的准确性和延迟估计方面，与最先进的方法相比取得了有竞争力的性能。

    With the wide and deep adoption of deep learning models in real applications, there is an increasing need to model and learn the representations of the neural networks themselves. These models can be used to estimate attributes of different neural network architectures such as the accuracy and latency, without running the actual training or inference tasks. In this paper, we propose a neural architecture representation model that can be used to estimate these attributes holistically. Specifically, we first propose a simple and effective tokenizer to encode both the operation and topology information of a neural network into a single sequence. Then, we design a multi-stage fusion transformer to build a compact vector representation from the converted sequence. For efficient model training, we further propose an information flow consistency augmentation and correspondingly design an architecture consistency loss, which brings more benefits with less augmentation samples compared with pre
    
[^98]: 使用上下文向量和图形装配改进词嵌入

    Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings. (arXiv:2210.16848v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.16848](http://arxiv.org/abs/2210.16848)

    本文提出了一种改进词嵌入的方法，分别为将更多上下文信息纳入Skip-gram框架和提出一个基于先验同义词知识和加权向量分布的静态嵌入后处理装配方法。这两种方法经由外部和内部任务的检验，能够大幅度超越基准线。

    

    尽管从大型预训练模型生成的上下文化嵌入在许多任务中表现良好，但由于计算成本低、部署便捷、稳定性高，传统的静态嵌入（例如Skip-gram、Word2Vec）仍在低资源和轻量级环境中发挥着重要作用。本文旨在通过以下方法改进词嵌入：1）将更多从现有预训练模型中获得的上下文信息纳入Skip-gram框架中，我们称之为Context-to-Vec；2）提出一个基于先验同义词知识和加权向量分布的静态嵌入后处理装配方法，独立于训练。通过外部和内部任务，我们的方法被证明能够以大幅超越基准线的性能表现。

    Although contextualized embeddings generated from large-scale pre-trained models perform well in many tasks, traditional static embeddings (e.g., Skip-gram, Word2Vec) still play an important role in low-resource and lightweight settings due to their low computational cost, ease of deployment, and stability. In this paper, we aim to improve word embeddings by 1) incorporating more contextual information from existing pre-trained models into the Skip-gram framework, which we call Context-to-Vec; 2) proposing a post-processing retrofitting method for static embeddings independent of training by employing priori synonym knowledge and weighted vector distribution. Through extrinsic and intrinsic tasks, our methods are well proven to outperform the baselines by a large margin.
    
[^99]: 具有凸和非凸子线性回归的研究及其在数据驱动的到达集学习中的应用。

    Convex and Nonconvex Sublinear Regression with Application to Data-driven Learning of Reach Sets. (arXiv:2210.01919v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2210.01919](http://arxiv.org/abs/2210.01919)

    本文提出了使用支撑函数对紧致集合进行学习的方法，并提出了两种算法进行子线性回归，分别为凸规划和非凸规划。本文在受控动态到达集的应用中进行了实验。

    

    本文考虑通过逐渐逼近一致函数（support function）的子线性回归方法，从有限数据中估计一个紧致集合。支撑函数在凸封闭运算的意义下能够唯一地刻画一个紧致集合，而且是子线性（凸以及一次正齐次的）。相反，任何子线性函数都是一个紧致集合的支撑函数。我们利用这一性质，将学习一个紧致集合的任务转化为学习它的支撑函数。为了进行子线性回归，我们提出了两种算法，一种是通过凸规划求解二次规划（QP）得到支撑函数，另一种是通过训练子线性神经网络实现非凸规划得到支撑函数。并通过数值实验展示了这些算法在轨迹数据中学习具有集合值输入不确定性的受控动态到达集的应用。

    We consider estimating a compact set from finite data by approximating the support function of that set via sublinear regression. Support functions uniquely characterize a compact set up to closure of convexification, and are sublinear (convex as well as positive homogeneous of degree one). Conversely, any sublinear function is the support function of a compact set. We leverage this property to transcribe the task of learning a compact set to that of learning its support function. We propose two algorithms to perform the sublinear regression, one via convex and another via nonconvex programming. The convex programming approach involves solving a quadratic program (QP). The nonconvex programming approach involves training a input sublinear neural network. We illustrate the proposed methods via numerical examples on learning the reach sets of controlled dynamics subject to set-valued input uncertainties from trajectory data.
    
[^100]: Omnigrok：理解超越算法数据的“Grokking”

    Omnigrok: Grokking Beyond Algorithmic Data. (arXiv:2210.01117v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01117](http://arxiv.org/abs/2210.01117)

    本文通过分析神经网络的损失景观，发现训练和测试损失之间的不匹配是grokking的原因，提出了“LU机制”，并成功诱导了算法数据集的grokking和消除了其grokking现象。它们的dramatic grokking依赖于表示学习。

    

    Grokking是一种不寻常的现象，指算法数据集在过拟合训练数据后长时间仍然能进行泛化，一直以来一直难以理解。本文旨在通过分析神经网络的损失景观来理解grokking，并确定训练和测试损失之间的不匹配是grokking的原因。我们将其称为“LU机制”，因为训练和测试损失（对模型权重规范）通常分别类似于“L”和“U”。这个简单的机制可以很好地解释grokking的许多方面：数据大小依赖性、权重衰减依赖性、表示的出现等。在直觉上给定的基础上，我们能够在涉及图像、语言和分子的任务中诱导grokking。反向来看，我们能够消除算法数据集的grokking。我们将算法数据集的dramatic grokking归因于表示学习。

    Grokking, the unusual phenomenon for algorithmic datasets where generalization happens long after overfitting the training data, has remained elusive. We aim to understand grokking by analyzing the loss landscapes of neural networks, identifying the mismatch between training and test losses as the cause for grokking. We refer to this as the "LU mechanism" because training and test losses (against model weight norm) typically resemble "L" and "U", respectively. This simple mechanism can nicely explain many aspects of grokking: data size dependence, weight decay dependence, the emergence of representations, etc. Guided by the intuitive picture, we are able to induce grokking on tasks involving images, language and molecules. In the reverse direction, we are able to eliminate grokking for algorithmic datasets. We attribute the dramatic nature of grokking for algorithmic datasets to representation learning.
    
[^101]: 利用新数据在电力网通讯中的潜力

    Leveraging the Potential of Novel Data in Power Line Communication of Electricity Grids. (arXiv:2209.12693v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.12693](http://arxiv.org/abs/2209.12693)

    本文提出了两个基于宽带电力线通信测量的首个数据集FiN-1和FiN-2，共收集了130亿个数据点。这些数据可以应用于资产管理、电网状态可视化和预测，以解决电力网在向可再生能源过渡和复杂负载配置的情况下面临的新挑战。

    

    电力网已成为日常生活中不可或缺的一部分，尽管在日常生活中往往不会注意到。只有当电力网不再可用时，我们通常才会特别意识到这种依赖。然而，重大变化，如向可再生能源（光伏、风力涡轮机等）的过渡以及复杂负载配置（电动汽车、家庭电池系统等）的能源消费者数量的增加，给电力网带来了新的挑战。为了解决这些挑战，我们提出了两个基于宽带电力线通信（PLC）基础设施测量的首个数据集。这两个数据集 FiN-1 和 FiN-2 在德国低压电网的一部分实际使用中收集，向大约440万人提供服务，并显示5100多个传感器收集的超过130亿个数据点。此外，我们提出了不同的用例，用于资产管理、电网状态可视化和预测。

    Electricity grids have become an essential part of daily life, even if they are often not noticed in everyday life. We usually only become particularly aware of this dependence by the time the electricity grid is no longer available. However, significant changes, such as the transition to renewable energy (photovoltaic, wind turbines, etc.) and an increasing number of energy consumers with complex load profiles (electric vehicles, home battery systems, etc.), pose new challenges for the electricity grid. To address these challenges, we propose two first-of-its-kind datasets based on measurements in a broadband powerline communications (PLC) infrastructure. Both datasets FiN-1 and FiN-2, were collected during real practical use in a part of the German low-voltage grid that supplies around 4.4 million people and show more than 13 billion datapoints collected by more than 5100 sensors. In addition, we present different use cases in asset management, grid state visualization, forecasting, 
    
[^102]: 针对干预密度估计的正则化流

    Normalizing Flows for Interventional Density Estimation. (arXiv:2209.06203v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06203](http://arxiv.org/abs/2209.06203)

    本研究提出了一种名为干预正则化流的全参数深度学习方法，用于从观测数据中估计干预后的潜在结果密度。该方法结合了nuisance flow和target flow，并开发了一个易于处理的优化目标，以实现有效和双重稳健的估计。实验证明，该方法优于现有最先进的方法。

    

    现有机器学习方法针对因果推断通常通过潜在结果的均值（例如平均处理效应）来计算数量。然而，这些数量并不能完全捕捉潜在结果分布的全部信息。本研究旨在从观测数据中估计干预后的潜在结果密度。为此，我们提出了一种新的全参数深度学习方法，称为干预正则化流。具体而言，我们组合了两种正则化流，即（i）用于估计干扰参数的nuisance flow和（ii）用于参数化估计潜在结果密度的target flow。我们进一步基于单步偏差校正开发了一个易于处理的优化目标，以有效和双重稳健的方式估计目标流参数。因此，我们的干预正则化流提供了一个正确归一化的密度估计器。在各种实验中，我们展示了我们的干预正则化流方法优于现有的用于从观测数据中估计潜在结果密度的最先进的方法。

    Existing machine learning methods for causal inference usually estimate quantities expressed via the mean of potential outcomes (e.g., average treatment effect). However, such quantities do not capture the full information about the distribution of potential outcomes. In this work, we estimate the density of potential outcomes after interventions from observational data. For this, we propose a novel, fully-parametric deep learning method called Interventional Normalizing Flows. Specifically, we combine two normalizing flows, namely (i) a nuisance flow for estimating nuisance parameters and (ii) a target flow for a parametric estimation of the density of potential outcomes. We further develop a tractable optimization objective based on a one-step bias correction for an efficient and doubly robust estimation of the target flow parameters. As a result our Interventional Normalizing Flows offer a properly normalized density estimator. Across various experiments, we demonstrate that our Int
    
[^103]: 视觉中的扩散模型：一项综述

    Diffusion Models in Vision: A Survey. (arXiv:2209.04747v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.04747](http://arxiv.org/abs/2209.04747)

    扩散模型是视觉中的新兴主题，其生成样本的质量和多样性受到广泛欣赏。本综述介绍了三种通用的扩散建模方法以及各种算法和架构方面的讨论，并总结比较了扩散模型与其他最先进的生成模型的性能。

    

    去噪扩散模型是计算机视觉领域中的新兴主题，展现了在生成建模领域中非凡的结果。扩散模型是一种基于深度学习的生成模型，由两个阶段组成，前向扩散和反向扩散。在前向扩散阶段，通过逐步添加高斯噪声逐渐扰动输入数据。在反向阶段，模型被任务为通过逐步学习逆转扩散过程，逐步恢复原始输入数据。尽管扩散模型的计算负担较大，即由于在采样过程中涉及的步骤数量较多导致的速度较慢，但其所生成样本的质量和多样性仍然受到广泛欣赏。在本篇文章中，我们提供了一个关于去噪扩散模型在视觉中应用的综合性评论，包括该领域的理论和实践贡献。首先，我们确定并介绍了三种通用的扩散建模方法：连续、离散和混合扩散模型。接着，我们讨论了扩散模型的各种算法和架构方面，如使用 Lévy 过程、不同形式的噪声、模型条件和正则化、多尺度架构和并行化技术。最后，我们总结并比较了去噪扩散模型与其他最先进的生成模型在多个数据集上的性能。

    Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e. low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modelin
    
[^104]: 扩散模型：方法和应用的综合调研

    Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v10 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.00796](http://arxiv.org/abs/2209.00796)

    本调研综合总结了扩散模型的方法和应用研究，包括高效采样、似然估计与特殊结构数据处理，介绍了扩散模型与其他生成模型相结合的潜力并回顾了其在不同领域的广泛应用，为进一步研究提出了可能的研究方向。

    

    扩散模型已成为一类具有记录性能的强大的深度生成模型，在许多应用中，包括图像合成、视频生成和分子设计方面表现出色。在这份调研中，我们概述了关于扩散模型的快速扩展研究，将研究分类为三个关键领域：高效采样、改进的似然估计和处理具有特殊结构的数据。我们还讨论了将扩散模型与其他生成模型相结合以实现增强结果的潜力。我们进一步回顾了扩散模型在计算机视觉、自然语言生成、时间数据建模以及其他科学学科的跨学科应用中的广泛应用。这个调研旨在提供一个具有背景的深入了解扩散模型的现状，确定关键的研究重点，指明可能的进一步研究领域。

    Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/Yan
    
[^105]: 《采用转化与蒸馏框架实现合作MARL全局最优性》

    Towards Global Optimality in Cooperative MARL with the Transformation And Distillation Framework. (arXiv:2207.11143v3 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2207.11143](http://arxiv.org/abs/2207.11143)

    本文研究了采用分散策略的MARL算法在梯度下降优化器下的次最优性，并提出了转化与蒸馏框架，该框架可以将多智能体MDP转化为单智能体MDP以实现分散执行。

    

    在合作多智能体强化学习中，分散执行是一项核心需求。目前，大多数流行的MARL算法采用分散策略来实现分散执行，并使用梯度下降作为优化器。然而，在考虑到优化方法的情况下，这些算法几乎没有任何理论分析，我们发现当梯度下降被选为优化方法时，各种流行的分散策略MARL算法在玩具任务中都是次最优的。本文在理论上分析了两种常见的采用分散策略的算法——多智能体策略梯度方法和值分解方法，证明了它们在使用梯度下降时的次最优性。此外，我们提出了转化与蒸馏（TAD）框架，它将多智能体MDP重新制定为一种具有连续结构的特殊单智能体MDP，并通过蒸馏实现分散执行。

    Decentralized execution is one core demand in cooperative multi-agent reinforcement learning (MARL). Recently, most popular MARL algorithms have adopted decentralized policies to enable decentralized execution and use gradient descent as their optimizer. However, there is hardly any theoretical analysis of these algorithms taking the optimization method into consideration, and we find that various popular MARL algorithms with decentralized policies are suboptimal in toy tasks when gradient descent is chosen as their optimization method. In this paper, we theoretically analyze two common classes of algorithms with decentralized policies -- multi-agent policy gradient methods and value-decomposition methods to prove their suboptimality when gradient descent is used. In addition, we propose the Transformation And Distillation (TAD) framework, which reformulates a multi-agent MDP as a special single-agent MDP with a sequential structure and enables decentralized execution by distilling the
    
[^106]: 关于预训练在联邦学习中的重要性和适用性的研究

    On the Importance and Applicability of Pre-Training for Federated Learning. (arXiv:2206.11488v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.11488](http://arxiv.org/abs/2206.11488)

    研究发现，在联邦学习中使用预训练可以改善性能，尤其是在非独立同分布客户数据的情况下。此外，使用合成数据或客户端数据进行分散式预训练也可以显著改善性能，并且不同的技术可以相互补充以进一步提高性能。

    

    预训练在现代深度学习中被广泛用于提高模型的性能。然而，在联邦学习（FL）的文献中，神经网络大多数是使用随机权重初始化的。这引起了我们对进行系统研究探索FL预训练的兴趣。在多个视觉识别基准测试中，我们发现预训练不仅可以提高FL的性能，而且可以缩小它与中心化学习之间的准确度差距，特别是在非独立同分布客户数据的挑战性情况下。为了使我们的发现适用于没有直接获得预训练模型的情况，我们探索了使用合成数据或甚至使用客户端数据进行分散式预训练，并发现它们已经显著改善了FL的性能。有趣的是，我们探索的许多技术互补性很强，可以进一步提高性能，我们将这视为在实际应用中扩展深度FL的关键结果。

    Pre-training is prevalent in nowadays deep learning to improve the learned model's performance. However, in the literature on federated learning (FL), neural networks are mostly initialized with random weights. These attract our interest in conducting a systematic study to explore pre-training for FL. Across multiple visual recognition benchmarks, we found that pre-training can not only improve FL, but also close its accuracy gap to the counterpart centralized learning, especially in the challenging cases of non-IID clients' data. To make our findings applicable to situations where pre-trained models are not directly available, we explore pre-training with synthetic data or even with clients' data in a decentralized manner, and found that they can already improve FL notably. Interestingly, many of the techniques we explore are complementary to each other to further boost the performance, and we view this as a critical result toward scaling up deep FL for real-world applications. We con
    
[^107]: FeatER: 通过基于特征图的TransformER实现人物重建的高效网络

    FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER. (arXiv:2205.15448v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.15448](http://arxiv.org/abs/2205.15448)

    FeatER是一个有效的网络，用于通过人体结构信息特征图的 transformer 处理来实现人物重建，并解决了既能够处理位置敏感性特征图又能减少计算和内存需求的问题。

    

    最近，视觉transformer在2D人体姿态估计、3D人体姿态估计和人体网格重建任务中取得了巨大的成功。在这些任务中，人体结构信息的特征映射表示通常首先由 CNN（如 HRNet）从图像中提取，然后通过 transformer 进一步处理以预测 HPE 或 HMR 的热力图。然而，现有的 transformer 架构不能直接处理这些特征图输入，需要对具有位置敏感性的人体结构信息进行不自然的展平。此外，最近的 HPE 和 HMR 方法中，很大一部分性能改进是以越来越高的计算和内存需求为代价的。因此，为了同时解决这些问题，我们提出了 FeatER，一种新颖的 transformer 设计，可以保持人体结构信息的内在结构。

    Recently, vision transformers have shown great success in a set of human reconstruction tasks such as 2D human pose estimation (2D HPE), 3D human pose estimation (3D HPE), and human mesh reconstruction (HMR) tasks. In these tasks, feature map representations of the human structural information are often extracted first from the image by a CNN (such as HRNet), and then further processed by transformer to predict the heatmaps (encodes each joint's location into a feature map with a Gaussian distribution) for HPE or HMR. However, existing transformer architectures are not able to process these feature map inputs directly, forcing an unnatural flattening of the location-sensitive human structural information. Furthermore, much of the performance benefit in recent HPE and HMR methods has come at the cost of ever-increasing computation and memory needs. Therefore, to simultaneously address these problems, we propose FeatER, a novel transformer design that preserves the inherent structure of 
    
[^108]: 在多智能体城市驾驶环境中评估深度强化学习自主策略的鲁棒性

    Evaluating the Robustness of Deep Reinforcement Learning for Autonomous Policies in a Multi-agent Urban Driving Environment. (arXiv:2112.11947v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2112.11947](http://arxiv.org/abs/2112.11947)

    本论文提出了一个基准测试框架，用于评估和比较深度强化学习算法在单个和多个智能体自主驾驶环境中的性能。同时，提出了一种混合算法以提高深度强化学习策略在多代理驾驶环境中的鲁棒性。

    

    深度强化学习被广泛用于在模拟驾驶环境中训练自主车策略。由于各种强化学习算法的大量可用性，并缺乏对它们在不同驾驶场景下的系统比较，我们不确定哪些算法更有效地用于单车和多车驾驶环境中自主汽车软件的训练。为评估深度强化学习在基于视觉的自主驾驶中的性能，我们提供了一个开放且可重复使用的基准测试框架，用于系统评估和比较分析单个和多个强化学习算法用于自动驾驶。利用该框架，我们进行了离散和连续动作空间深度强化学习算法的比较研究。同时，我们还提出了一种混合算法，将离散和连续动作空间相结合，以提高深度强化学习策略在多代理驾驶环境中的鲁棒性。

    Deep reinforcement learning is actively used for training autonomous car policies in a simulated driving environment. Due to the large availability of various reinforcement learning algorithms and the lack of their systematic comparison across different driving scenarios, we are unsure of which ones are more effective for training autonomous car software in single-agent as well as multi-agent driving environments. A benchmarking framework for the comparison of deep reinforcement learning in a vision-based autonomous driving will open up the possibilities for training better autonomous car driving policies. To address these challenges, we provide an open and reusable benchmarking framework for systematic evaluation and comparative analysis of deep reinforcement learning algorithms for autonomous driving in a single- and multi-agent environment. Using the framework, we perform a comparative study of discrete and continuous action space deep reinforcement learning algorithms. We also prop
    
[^109]: AMRA*: 任意多分辨率多启发式 A*算法

    AMRA*: Anytime Multi-Resolution Multi-Heuristic A*. (arXiv:2110.05328v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2110.05328](http://arxiv.org/abs/2110.05328)

    AMRA*是一种任意多分辨率多启发式A*算法，能够在跨越多个分辨率搜索时横跨大范围的无障碍区域和狭窄通道，可以尽可能使用粗分辨率找到解法并逐步细化，具有较好的性能表现。

    

    启发式搜索基于离散化搜索空间的方法来解决最短路问题，性能与离散化程度密切相关。过细的离散化复杂度高但可以更好地逼近连续搜索空间，而过粗的离散化可能会牺牲解法质量以快速得到解。对于大规模状态空间，跨越多个分辨率寻找解法可能是有益的，但定义离散化方法是具有挑战性的。最近提出的多分辨率A*(MRA*)算法跨越多个分辨率搜索，它在粗略分辨率上遍历大范围的无障碍区域并逃脱局部最小值，同时在较细略分辨率上可以穿越所谓的狭窄通道。本文中，我们开发了AMRA*，MRA*的实时版本。AMRA*尽可能使用粗分辨率快速找到解法，如果需要逐步细化。我们使用多个启发式来在每个分辨率水平上优先考虑搜索方向。在广泛的基准问题上，AMRA*优于MRA*和其他现有算法。

    Heuristic search-based motion planning algorithms typically discretise the search space in order to solve the shortest path problem. Their performance is closely related to this discretisation. A fine discretisation allows for better approximations of the continuous search space, but makes the search for a solution more computationally costly. A coarser resolution might allow the algorithms to find solutions quickly at the expense of quality. For large state spaces, it can be beneficial to search for solutions across multiple resolutions even though defining the discretisations is challenging. The recently proposed algorithm Multi-Resolution A* (MRA*) searches over multiple resolutions. It traverses large areas of obstacle-free space and escapes local minima at a coarse resolution. It can also navigate so-called narrow passageways at a finer resolution. In this work, we develop AMRA*, an anytime version of MRA*. AMRA* tries to find a solution quickly using the coarse resolution as much
    
[^110]: 基于图解码技术的面向任务的语义解析

    Graph-Based Decoding for Task Oriented Semantic Parsing. (arXiv:2109.04587v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2109.04587](http://arxiv.org/abs/2109.04587)

    本研究探索了一种替代语义解析任务的方法，将其作为依赖解析任务进行表述并应用了基于图的解码技术，有望提高部分注释数据的效率和数据使用效率。

    

    近年来，语义解析的主流范式是将解析作为序列到序列任务，使用自回归序列解码器生成预测值。本研究探讨了一种替代范式。我们将语义解析作为依赖解析任务进行了表述，应用了针对句法解析开发的基于图的解码技术。我们在 TOP 数据集上比较了给定相同预先训练的 Transformer 编码器的各种解码技术，包括训练数据有限或仅包含部分注释示例的设置。我们发现，我们基于图的方法在标准设置上与序列解码器相当竞争，并在数据效率和部分注释数据可用的设置中提供了显着的改进。

    The dominant paradigm for semantic parsing in recent years is to formulate parsing as a sequence-to-sequence task, generating predictions with auto-regressive sequence decoders. In this work, we explore an alternative paradigm. We formulate semantic parsing as a dependency parsing task, applying graph-based decoding techniques developed for syntactic parsing. We compare various decoding techniques given the same pre-trained Transformer encoder on the TOP dataset, including settings where training data is limited or contains only partially-annotated examples. We find that our graph-based approach is competitive with sequence decoders on the standard setting, and offers significant improvements in data efficiency and settings where partially-annotated data is available.
    
[^111]: 无向图的拟欧几里得吸引排斥嵌入

    Pseudo-Euclidean Attract-Repel Embeddings for Undirected Graphs. (arXiv:2106.09671v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2106.09671](http://arxiv.org/abs/2106.09671)

    通过将节点嵌入拟欧几里得空间中来消除传递性假设，使得拟欧几里得嵌入可以高效地压缩网络，允许多种最近邻的概念，并可以被插入到现有的模型中，以实现更好的链接预测。

    

    点积嵌入将图构造为节点矢量，使得两个矢量之间的点积给出边的强度。然而，许多在现实世界中生成图形的重要力量导致了非传递关系，我们通过将节点嵌入拟欧几里得空间中来消除传递性假设，给每个节点一个吸引和一个排斥矢量，两个节点之间的内积通过在吸引矢量中进行点积并在排斥矢量中进行点积的结果相减来定义。拟欧几里得嵌入可以高效地压缩网络，允许多种最近邻的概念，每种概念都有自己的解释，并且可以被“插入”到现有的模型中，比如指数族嵌入或图神经网络，以实现更好的链接预测。

    Dot product embeddings take a graph and construct vectors for nodes such that dot products between two vectors give the strength of the edge. Dot products make a strong transitivity assumption, however, many important forces generating graphs in the real world lead to non-transitive relationships. We remove the transitivity assumption by embedding nodes into a pseudo-Euclidean space - giving each node an attract and a repel vector. The inner product between two nodes is defined by taking the dot product in attract vectors and subtracting the dot product in repel vectors. Pseudo-Euclidean embeddings can compress networks efficiently, allow for multiple notions of nearest neighbors each with their own interpretation, and can be `slotted' into existing models such as exponential family embeddings or graph neural networks for better link prediction.
    
[^112]: 不扩展的A*搜索：用深度Q网络学习启发式函数

    A* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks. (arXiv:2102.04518v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2102.04518](http://arxiv.org/abs/2102.04518)

    本文提出了一种使用深度Q网络学习启发式函数，通过只进行一次前向传递计算相邻节点的转移成本和启发式值之和，并在不显式生成这些子节点的情况下指导搜索的Q*搜索算法，以大幅减少计算时间。在魔方问题上的实验表明，该方法能够高效地解决具有大动作空间的问题。

    

    高效地使用 A* 搜索解决具有大动作空间的问题对于人工智能社区几十年来一直非常重要。这是因为 A* 搜索的计算和存储需求随着动作空间的大小呈线性增长。当 A* 搜索使用计算代价高昂的函数逼近器（如深度神经网络）学习启发式函数时，这种负担变得更加明显。为了解决这个问题，我们引入了 Q* 搜索，一种使用深度 Q 网络引导搜索的搜索算法，以利用一个事实，即在不显式生成这些子节点的情况下，一个节点的子节点的转移成本和启发式值之和可以通过单次前向传递计算。这显着降低了计算时间，并且每次迭代只需要生成一个节点。我们使用 Q* 搜索来解决魔方问题，并将其们表示为一个包含 1872 个元动作的大动作空间。

    Efficiently solving problems with large action spaces using A* search has been of importance to the artificial intelligence community for decades. This is because the computation and memory requirements of A* search grow linearly with the size of the action space. This burden becomes even more apparent when A* search uses a heuristic function learned by computationally expensive function approximators, such as deep neural networks. To address this problem, we introduce Q* search, a search algorithm that uses deep Q-networks to guide search in order to take advantage of the fact that the sum of the transition costs and heuristic values of the children of a node can be computed with a single forward pass through a deep Q-network without explicitly generating those children. This significantly reduces computation time and requires only one node to be generated per iteration. We use Q* search to solve the Rubik's cube when formulated with a large action space that includes 1872 meta-action
    
[^113]: 代表性集成在准线性复杂度下实现协同生命周期学习

    Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity. (arXiv:2004.12908v16 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2004.12908](http://arxiv.org/abs/2004.12908)

    本文提出了一种名为RELL的方法，利用知识蒸馏和知识保持正则化方法，以协同集成在不同任务上独立学习的表示，在准线性复杂度下实现了前向和后向传递。实验结果表明，在各种基准数据集上，RELL的表现优于现有的最先进方法，尤其是在存在灾难性遗忘的情况下，能够显着改善反向传递。

    

    在终身学习中，数据不仅可以用于改进当前任务的性能，还可以用于之前和尚未遇到的任务。传统的机器学习则从空白状态开始，仅针对单个任务使用数据。虽然传统迁移学习算法可以提高未来任务的性能，但在学习新任务后对旧任务的性能下降（称为遗忘）。近期针对连续或终身学习的许多方法都试图在给定新任务的情况下保持对旧任务的性能。但是，仅努力避免忘记将目标定得过低。终身学习的目标不仅应该是提高未来任务（前向传递）的性能，而且还应该是用任何新数据提高过去任务（反向传递）的性能。我们的关键见解是，我们可以协同集成分别在不同任务上独立学习的表示，以实现准线性复杂度下的前向和后向传递。本文提出了一种新方法，称为“终身学习中的表示集成（RELL）”，它集成了知识蒸馏和知识保持正则化方法，以利用不同表示中包含的互补信息。我们的实验表明，RELL在各种基准数据集上都优于现有最先进方法，尤其是在存在灾难性遗忘的情况下实现了显着更好的反向传递。

    In lifelong learning, data are used to improve performance not only on the current task, but also on previously encountered, and as yet unencountered tasks. In contrast, classical machine learning, which we define as, starts from a blank slate, or tabula rasa and uses data only for the single task at hand. While typical transfer learning algorithms can improve performance on future tasks, their performance on prior tasks degrades upon learning new tasks (called forgetting). Many recent approaches for continual or lifelong learning have attempted to maintain performance on old tasks given new tasks. But striving to avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning should be not only to improve performance on future tasks (forward transfer) but also on past tasks (backward transfer) with any new data. Our key insight is that we can synergistically ensemble representations -- that were learned independently on disparate tasks -- to enable both forward and bac
    

