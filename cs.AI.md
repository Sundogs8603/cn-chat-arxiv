# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution](https://rss.arxiv.org/abs/2402.01586) | 本文介绍了一种基于代理构成的代理框架TrustAgent，该框架通过预先规划、规划过程中和计划后检查三种策略来提高LLM代理的安全性。实验结果表明，这些方法可以有效识别和预防潜在危险。此外，还研究了安全性与使用者满意度以及模型推理能力与效率之间的关系。 |
| [^2] | [LTL learning on GPUs](https://arxiv.org/abs/2402.12373) | 实现了首个基于GPU的LTL学习器，使用新颖的枚举式程序合成，性能显著优于现有最先进的学习器，处理跟踪至少多2048倍，速度平均快46倍，并且引入了具有$O(\log n)$时间复杂度的无分支LTL semantics。 |
| [^3] | [AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies](https://arxiv.org/abs/2402.12370) | 通过提出ANALOBENCH基准来评估语言模型（LMs）进行类比推理的能力，发现扩展LMs规模对于处理涉及长场景或相关经验回忆的类比时带来的性能提升较小。 |
| [^4] | [A Critical Evaluation of AI Feedback for Aligning Large Language Models](https://arxiv.org/abs/2402.12366) | 研究质疑复杂的强化学习在AI反馈中的必要性，表明使用更强的教师模型进行监督微调可以超越现有的RLAIF管道。 |
| [^5] | [Universal Physics Transformers](https://arxiv.org/abs/2402.12365) | 提出了通用物理变压器（UPTs）这一新颖学习范式，能够模拟广泛的时空问题，同时适用于拉格朗日和欧拉离散化方案，有效地传播动态并允许查询潜在空间 |
| [^6] | [Nonlinear Discrete-Time Observers with Physics-Informed Neural Networks](https://arxiv.org/abs/2402.12360) | 通过基于物理的神经网络，提出了一种非线性离散时间观测器，在单步精确观测器线性化框架内学习非线性状态转换映射，通过两个案例研究进行性能评估，并与传统幂级数数值实现进行比较 |
| [^7] | [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354) | LoRA+通过设置不同的学习率来改进原始LoRA的低效率问题，在保持计算成本不变的情况下提高了模型性能和微调速度。 |
| [^8] | [GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations](https://arxiv.org/abs/2402.12348) | 该论文通过博弈论任务评估了LLMs在竞争环境中的推理能力，观察到LLMs在不同游戏场景下表现出不同行为，具有重要的战略推理局限性。 |
| [^9] | [Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models](https://arxiv.org/abs/2402.12336) | 通过无监督对抗微调，提出了一种强大的CLIP视觉编码器，用于增强各种视觉-语言模型的鲁棒性。恶意第三方提供操纵图像的用户隐形攻击得以杜绝。 |
| [^10] | [Generating Survival Interpretable Trajectories and Data](https://arxiv.org/abs/2402.12331) | 提出了一种新的模型，能够生成生存轨迹和数据，并通过特定结构的自动编码器解决了预测、数据补充和生成原型时间相关轨迹等任务 |
| [^11] | [Query-Based Adversarial Prompt Generation](https://arxiv.org/abs/2402.12329) | 该研究提出了一种基于查询的对抗性攻击方法，通过利用远程语言模型的 API 访问构造对抗性示例，使模型以更高概率发出有害字符串，而非仅仅基于模型之间的转移性攻击。 |
| [^12] | [Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents](https://arxiv.org/abs/2402.12327) | 该研究揭示了LLM代理甚至在竞争环境中也能自发形成合作关系的能力，验证了计算社会科学的愿景，表明LLM代理可以用于模拟人类社会互动，包括自发合作的互动，为社会现象提供洞察。 |
| [^13] | [Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness](https://arxiv.org/abs/2402.12319) | 引入了FairSAR，一种独特的遗憾度量，以解决动态环境下的公平意识在线学习挑战。 |
| [^14] | [ARKS: Active Retrieval in Knowledge Soup for Code Generation](https://arxiv.org/abs/2402.12317) | ARKS是一种用于代码生成的先进策略，通过活跃检索和整合各种信息源，能够提高大型语言模型的性能，为解决与频繁更新的库和长尾编程语言相关的现实编程问题提供了新的可能性。 |
| [^15] | [Multi-View Conformal Learning for Heterogeneous Sensor Fusion](https://arxiv.org/abs/2402.12307) | 我们提出了用于异构传感器融合的多视角一致模型，并引入了基于集合交集的多视角半一致模型。 |
| [^16] | [Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports](https://arxiv.org/abs/2402.12298) | 这项研究比较了商业模型GPT-3.5 Turbo和GPT-4与开源模型Mistral-7B、Mixtral-8x7B、Llama2-13B、Llama2-70B、QWEN1.5-72B以及CheXbert和CheXpert-labeler在准确标记X射线文本报告中多发现存在的能力。 |
| [^17] | [Refining Minimax Regret for Unsupervised Environment Design](https://arxiv.org/abs/2402.12284) | 介绍了贝叶斯级别完美的MMR（BLP），它是极小化遗憾目标的精确化，能够克服极小化遗憾策略在遗憾上界时学习停滞的限制。 |
| [^18] | [Adaptive Skeleton Graph Decoding](https://arxiv.org/abs/2402.12280) | 提出了骨架图解码（SGD）方法，利用子问题之间的依赖关系进行信息转发，改善响应质量且提高性能。 |
| [^19] | [Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks](https://arxiv.org/abs/2402.12279) | 通过微调学习率可以缓解零样本跨语言生成中以错误语言生成的问题，全面微调模型是很强大的基线，mBART在这个任务中表现类似于同等大小的mT5。 |
| [^20] | [WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment](https://arxiv.org/abs/2402.12275) | 通过编写代码和与环境交互来构建世界模型的基于模型的LLM代理在样本效率上优于深度RL，并在计算效率上优于ReAct风格的代理。 |
| [^21] | [On the Byzantine-Resilience of Distillation-Based Federated Learning](https://arxiv.org/abs/2402.12265) | 基于蒸馏的联邦学习在拜占庭环境下表现出极强的弹性，介绍了两种新的拜占庭攻击，并提出了一种增强拜占庭弹性的新方法。 |
| [^22] | [Uncertainty quantification in fine-tuned LLMs using LoRA ensembles](https://arxiv.org/abs/2402.12264) | 使用LoRA集成在精调LLMs中提出了一种原则性不确定性量化方法，通过对不同数据域的低秩适应集成分析，推测了模型对特定架构难以学习的数据领域的信号。 |
| [^23] | [BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts](https://arxiv.org/abs/2402.12240) | BEARS是一种集成技术，可以让神经符号模型意识到它们学习的概念的语义模糊性，帮助用户识别和怀疑低质量概念。 |
| [^24] | [Learning to Defer in Content Moderation: The Human-AI Interplay](https://arxiv.org/abs/2402.12237) | 本文提出了一个模型，捕捉内容审核中人工智能的相互作用。 |
| [^25] | [Kernel KMeans clustering splits for end-to-end unsupervised decision trees](https://arxiv.org/abs/2402.12232) | 提出了一种新颖的端到端训练的无监督二叉树用于聚类，称为Kauri，通过贪婪最大化 kernel KMeans 目标来执行，无需定义质心，并在多个数据集上展示其性能优于其他方法。 |
| [^26] | [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/abs/2402.12226) | AnyGPT是一个统一的多模态语言模型，通过离散表示实现各种模态的统一处理，能够在不改变大型语言模型架构或训练方式的情况下稳定训练，为新模态的无缝整合提供了可能。 |
| [^27] | [Reformatted Alignment](https://arxiv.org/abs/2402.12219) | 本文提出了一种名为ReAlign的简单有效方法，通过重新格式化指导数据的响应，显著提升了大型语言模型（LLMs）与人类价值观的对齐能力。 |
| [^28] | [Copyleft for Alleviating AIGC Copyright Dilemma: What-if Analysis, Public Perception and Implications](https://arxiv.org/abs/2402.12216) | 本研究探讨了通过采用Copyleft减轻AIGC版权困境的可行性，通过定性和定量研究发现，人们普遍感知到困境并倾向于使用自由授权。 |
| [^29] | [Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid Federated Approach](https://arxiv.org/abs/2402.12202) | 提出了一种考虑异质性的混合联邦推荐系统，用于解决跨校选修课程推荐问题，通过构建异构图和设计注意机制来捕捉异质性感知表示，并在联邦方案下训练个别学校模型以推荐量身定制的选修课程。 |
| [^30] | [MultiFIX: An XAI-friendly feature inducing approach to building models from multimodal data](https://arxiv.org/abs/2402.12183) | 提出了MultiFIX，一种注重可解释性的多模态数据融合方法，使用深度学习架构训练模型，并通过注意力图来解释每个模态对最终预测的影响 |
| [^31] | [Revisiting Data Augmentation in Deep Reinforcement Learning](https://arxiv.org/abs/2402.12181) | 重新审视深度强化学习中的数据增强，分析不同方法的影响，提出了如何更加原则地利用数据增强的建议。 |
| [^32] | [Examining Monitoring System: Detecting Abnormal Behavior In Online Examinations](https://arxiv.org/abs/2402.12179) | 该论文设计的“检测监控系统”能够高准确性和速度地识别在线考试中的作弊行为，帮助监考员做出决策。 |
| [^33] | [Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning](https://arxiv.org/abs/2402.12177) | Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。 |
| [^34] | [Unsupervised LLM Adaptation for Question Answering](https://arxiv.org/abs/2402.12170) | 提出了无监督LLM适应问答任务，通过利用预训练的LLM和目标领域的未标记文档，实现在新领域回答问题的目标。 |
| [^35] | [Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.12168) | PEFT相对于全参数微调更容易受到权重投毒后门攻击的影响，提出了一个通过置信度识别受污染样本的毒化样本识别模块（PSIM），为权重投毒后门攻击提供稳健防御 |
| [^36] | [Endowing Pre-trained Graph Models with Provable Fairness](https://arxiv.org/abs/2402.12161) | 提出了一种新的适配器调优框架，赋予预训练图模型具有可证明的公平性 |
| [^37] | [Transformer-based Causal Language Models Perform Clustering](https://arxiv.org/abs/2402.12151) | Transformer-based因果语言模型通过在隐藏空间内对数据进行聚类来学习任务特定信息，这种聚类过程在学习中动态演变，并有助于处理未见实例。 |
| [^38] | [Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One](https://arxiv.org/abs/2402.12150) | 提出了一种通过提示大型语言模型（LLMs）具有特定角色以表达多样观点的方法，并开发了FairThinking流水线，以实现公平表达。 |
| [^39] | [End-to-end multilingual fact-checking at scale](https://arxiv.org/abs/2402.12147) | 使用Factiverse AI模型，可以进行跨语言的端到端事实核查，并且通过实验证明，为事实核查任务进行微调的模型优于大型语言模型。 |
| [^40] | [Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement](https://arxiv.org/abs/2402.12146) | Meta Ranking方法通过比较目标查询-响应对与参考查询-响应对来使较不具备能力的语言模型有效地评估单个响应的可靠性。 |
| [^41] | [SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding](https://arxiv.org/abs/2402.12132) | 该论文提出了一个名为SSTKG的新框架，用于构建和探索时空知识图，以将空间和时间数据整合到知识图中，提供了用于未来时序预测和空间信息推荐的输出嵌入。 |
| [^42] | [Evaluating Image Review Ability of Vision Language Models](https://arxiv.org/abs/2402.12121) | 本论文通过引入基于排名相关分析的评估方法，探讨了大规模视觉语言模型（LVLM）在生成图像评价文本方面的能力，并创建了一个评估数据集来验证这种方法。 |
| [^43] | [DualView: Data Attribution from the Dual Perspective](https://arxiv.org/abs/2402.12118) | 提出了DualView，一种基于替代建模的后期数据归因方法，具有高效计算和优质评估结果。 |
| [^44] | [Is It a Free Lunch for Removing Outliers during Pretraining?](https://arxiv.org/abs/2402.12102) | 通过确保归一化对序列长度不变，我们改进了一种预训练方法，使其在移除异常值的同时促进了因果语言模型的成功预训练。 |
| [^45] | [Groot: Adversarial Testing for Generative Text-to-Image Models with Tree-based Semantic Transformation](https://arxiv.org/abs/2402.12100) | Groot是第一个利用基于树的语义转换进行对抗测试文本到图像模型的自动化框架，成功率高达93.66%。 |
| [^46] | [Towards Explainable LiDAR Point Cloud Semantic Segmentation via Gradient Based Target Localization](https://arxiv.org/abs/2402.12098) | 通过pGS-CAM方法，本研究提出了一种基于梯度的目标定位方法，可以有效地生成LiDAR点云语义分割模型中每个点的贡献，帮助我们更好地理解模型预测过程并识别改进的潜在区域。 |
| [^47] | [Do Large Language Models Understand Logic or Just Mimick Context?](https://arxiv.org/abs/2402.12091) | 大型语言模型在逻辑推理中并不真正理解逻辑规则，而是通过语境学习增强了模型到达结论的可能性 |
| [^48] | [HIP Network: Historical Information Passing Network for Extrapolation Reasoning on Temporal Knowledge Graph](https://arxiv.org/abs/2402.12074) | 该论文提出了HIP网络，通过从时间性、结构性和重复性角度传递信息，综合考虑了时间变化背后的潜在模式，更新表示并准确预测未来事件。 |
| [^49] | [EmoBench: Evaluating the Emotional Intelligence of Large Language Models](https://arxiv.org/abs/2402.12071) | EmoBench是一个基于心理学理论的基准测试，旨在评估大型语言模型的情感智能，包括情感理解和情感应用。 |
| [^50] | [WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More](https://arxiv.org/abs/2402.12065) | 该论文提出了WKVQuant，一种专为大型语言模型设计的量化框架，通过量化权重和键值缓存来改善性能。 |
| [^51] | [Causal Equal Protection as Algorithmic Fairness](https://arxiv.org/abs/2402.12062) | 本文提出了一种新的算法公平性原则——平等保护，其关键在于将错误分类的风险均等化，避免了许多对传统分类平等原则的反例。 |
| [^52] | [All Language Models Large and Small](https://arxiv.org/abs/2402.12061) | LONDI框架可以在需要复杂决策和推理的地方选择性地使用大的语言模型，极大地降低了资源消耗。 |
| [^53] | [Linear bandits with polylogarithmic minimax regret](https://arxiv.org/abs/2402.12042) | 该研究提出了一种新的线性赌博机算法，解决了线性随机赌博机中最小极小遗憾的多对数缩放问题，通过加权最小二乘估计实现对设计矩阵特征值关系的控制，实现了累积遗憾的对数缩放。 |
| [^54] | [Class-incremental Learning for Time Series: Benchmark and Evaluation](https://arxiv.org/abs/2402.12035) | 时间序列增量学习问题在图像和语言领域取得了进展，但在时间序列数据方面仍然相对较少研究，本文提出了一个全面的评估和基准测试方法。 |
| [^55] | [Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space](https://arxiv.org/abs/2402.12026) | 通过对频率空间的分析，本文提出了一种多尺度低秩适应（MuScleLoRA）方法，用于解决在训练语言模型时受到后门攻击的问题。 |
| [^56] | [Evaluation of ChatGPT's Smart Contract Auditing Capabilities Based on Chain of Thought](https://arxiv.org/abs/2402.12023) | 本研究评估了使用GPT-4模型增强智能合约安全审计能力的潜力，通过比较其在识别常见漏洞、代码解析和漏洞捕获方面的表现，展示了其在提升智能合约安全性方面的价值。 |
| [^57] | [Training Green AI Models Using Elite Samples](https://arxiv.org/abs/2402.12010) | 该论文提出了一个基于进化的采样框架，旨在识别精英训练样本，比较与传统方法的模型性能和能效优势，探讨其对可持续模型训练的可能性。 |
| [^58] | [Cluster Metric Sensitivity to Irrelevant Features](https://arxiv.org/abs/2402.12008) | 本文研究群集性能对添加到基线数据集中的嘈杂不相关变量的敏感度。 |
| [^59] | [A Survey on Extractive Knowledge Graph Summarization: Applications, Approaches, Evaluation, and Future Directions](https://arxiv.org/abs/2402.12001) | 本文调查了抽取式知识图谱总结的应用并提出了方法分类，为未来方向提供了重要参考。 |
| [^60] | [Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models](https://arxiv.org/abs/2402.11997) | 大型语言模型在处理时间信息和推理方面存在显著限制，闭源模型可能暗示了不确定性认识与错误回应之间的权衡。 |
| [^61] | [Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks](https://arxiv.org/abs/2402.11984) | 该研究提出了一种基于横向连接和Hebbian学习的神经操作新方法，能够通过投影保护知识 |
| [^62] | [Imbalance in Regression Datasets](https://arxiv.org/abs/2402.11963) | 回归数据集中的不平衡问题一直被忽视，本文通过理论分析和定义，展示了这一问题的重要性，并为未来研究提供了共同基础。 |
| [^63] | [DB-LLM: Accurate Dual-Binarization for Efficient LLMs](https://arxiv.org/abs/2402.11960) | 本文提出了一种名为DB-LLM的新颖双二值化方法，通过引入灵活双二值化(FDB)来平衡2位宽度的精度优势和二值化的效率优势，从而在提高LLMs的计算效率的同时保持了准确性。 |
| [^64] | [Analysis of Multidomain Abstractive Summarization Using Salience Allocation](https://arxiv.org/abs/2402.11955) | 通过SEASON技术，本研究评估了用于生成概括性摘要的方法，与BART、PEGASUS和ProphetNet等模型进行对比，在多个数据集上进行了评估，着重分析了财经数据集的表现。 |
| [^65] | [Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model](https://arxiv.org/abs/2402.11948) | Mini-Hes提出了一种新的mini-block对角黑塞无约束优化方法，用于构建二阶潜在因子分析模型，解决了大数据量下二阶算法可行性的挑战。 |
| [^66] | [Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting AI-generated Text](https://arxiv.org/abs/2402.11934) | 本文研究了团队QUST在SemEval-2024任务8中的参与情况，通过数据增强和清洗提高了模型训练效率和准确性，在单语任务中评估了多种方法并最终采用堆叠集成模型，最终在多语言环境中取得了不错的排名。 |
| [^67] | [Energy-Efficient Edge Learning via Joint Data Deepening-and-Prefetching](https://arxiv.org/abs/2402.11925) | 我们提出了一种名为JD2P的新型离线架构，通过联合数据深化和预取技术，按顺序离线每个数据样本的特征，以减少物联网设备向边缘服务器传输数据时所需的能量消耗。 |
| [^68] | [SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning](https://arxiv.org/abs/2402.11903) | 提出了一种新颖的求解器层适应（SoLA）方法，在LLM中引入求解器层，不同地引导解决方案朝向可满足性 |
| [^69] | [Real-World Planning with PDDL+ and Beyond](https://arxiv.org/abs/2402.11901) | Nyx是一种新型PDDL+规划器，强调轻巧、简单和适应性，可以定制实现超越PDDL+范围的能力。 |
| [^70] | [Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint](https://arxiv.org/abs/2402.11893) | 提出了一种自适应解码方法COIECD，用于识别和解决知识冲突，提高模型对冲突上下文的忠实度，并在非冲突情况下保持高性能。 |
| [^71] | [Evaluating Program Repair with Semantic-Preserving Transformations: A Naturalness Assessment](https://arxiv.org/abs/2402.11892) | 本文研究了保留语义的转换的自然性及其对NPR评估的影响，发现了NPR系统在面对不自然的代码转换时会产生较高的误报率，且在使用自然转换进行评估时性能明显下降。 |
| [^72] | [The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth](https://arxiv.org/abs/2402.11886) | 本文旨在评估和改进LLM作为酷儿青少年情感支持者的潜力，通过定性和定量分析LLM与酷儿相关内容的互动，并开发了一个新颖的评估标准量表。 |
| [^73] | [Finite-Time Error Analysis of Online Model-Based Q-Learning with a Relaxed Sampling Model](https://arxiv.org/abs/2402.11877) | 本文通过有限时间分析以及实证评估，探讨了集成模型方法的Q学习在样本复杂度方面的优势。 |
| [^74] | [From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions and Models for Planning from Raw Data](https://arxiv.org/abs/2402.11871) | 本文提出了一种从未标记高维实值机器人轨迹开始自主学习通用的逻辑相关表示，这些表示构成了自动发明的PDDL-like域模型。 |
| [^75] | [Two Online Map Matching Algorithms Based on Analytic Hierarchy Process and Fuzzy Logic](https://arxiv.org/abs/2402.11866) | 本文开发了基于层次分析法和模糊逻辑的两种新的在线地图匹配算法，其中AHP应用于地图匹配的方式是新开发的，同时模糊逻辑被应用于地图匹配与现有研究类似，但进行了一些改变。 |
| [^76] | [CodeArt: Better Code Models by Attention Regularization When Symbols Are Lacking](https://arxiv.org/abs/2402.11842) | 提出一种在符号缺失时通过注意力规范化改进代码模型的新方法，使用程序分析提取上下文并利用注意力掩码方法，同时利用自注意力机制学习关注度的重要性 |
| [^77] | [Where It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages](https://arxiv.org/abs/2402.11818) | 提出了一种名为NewsSerow的方法，用于自动识别低资源语言中的环境保护内容，并且在尼泊尔语中使用少于10个示例新闻文章时，NewsSerow显着优于其他少样本模型。 |
| [^78] | [HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?](https://arxiv.org/abs/2402.11815) | 提出了一种基于对比学习的单一模型，用较少的参数实现与基线相当的机器生成文本检测性能 |
| [^79] | [A novel framework for adaptive stress testing of autonomous vehicles in highways](https://arxiv.org/abs/2402.11813) | 提出了一种新颖的框架，利用自适应压力测试方法和深度强化学习来系统探索可能导致高速公路交通场景中安全问题的边界情况 |
| [^80] | [Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding](https://arxiv.org/abs/2402.11809) | 提出了Smart Parallel Auto-Correct Decoding (SPACE)方法，通过集成半自回归推理和猜测解码，实现了大型语言模型推理加速和并行生成验证令牌的功能。 |
| [^81] | [LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs](https://arxiv.org/abs/2402.11804) | 本文利用大型语言模型（LLMs）生成图形结构提示，以增强预训练的图神经网络（GNNs），提出一种新的方法论见解，实现了在任意知识图上进行低资源归纳推理的高通用性。 |
| [^82] | [Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling](https://arxiv.org/abs/2402.11800) | 延迟更新的随机逼近方案在时间变化有界延迟下，保证了每次迭代快速收敛到固定点周围的球体，界限依赖于最大延迟和混合时间。 |
| [^83] | [Generative Kaleidoscopic Networks](https://arxiv.org/abs/2402.11793) | 发现深层ReLU网络表现出过度泛化现象，利用这一特性设计了“生成万花筒网络”，通过递归映射随机输入噪声生成样本。 |
| [^84] | [MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast Cancer Through Multimodal Data Fusion](https://arxiv.org/abs/2402.11788) | 提出了一种深度学习方法，通过整合多模态数据实现乳腺癌患者的生存风险分层，通过MaxViT模型和自注意力机制从图像中提取特征并融合遗传和临床数据，取得了优于现有方法的性能，有望改善患者预后。 |
| [^85] | [Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware](https://arxiv.org/abs/2402.11780) | 本文提出了CiMNet，一个旨在联合优化深度神经网络架构和配置的框架，以解决计算存储硬件构建中的挑战。 |
| [^86] | [Towards Theoretical Understandings of Self-Consuming Generative Models](https://arxiv.org/abs/2402.11778) | 通过构建理论框架，我们探讨了在自消耗循环中训练生成模型对数据分布学习的影响，证明了在足够大的训练数据集大小或真实数据比例条件下，合成数据分布与原始真实数据分布之间的总变差距离能够被有效控制。 |
| [^87] | [Uncovering Latent Human Wellbeing in Language Model Embeddings](https://arxiv.org/abs/2402.11777) | 本研究通过ETHICS Utilitarianism任务发现，预训练语言模型的表示隐含了对人类福祉的理解，且模型规模增加时，准确率呈非下降趋势。 |
| [^88] | [Dynamic Multi-Network Mining of Tensor Time Series](https://arxiv.org/abs/2402.11773) | 提出了一种新方法，Dynamic Multi-network Mining (DMM)，能够将张量时间序列转换为不同长度的段组，通过稀疏依赖网络提供聚类的可解释性和精确性。 |
| [^89] | [Evaluating the Effectiveness of Index-Based Treatment Allocation](https://arxiv.org/abs/2402.11771) | 本文介绍了一种评估基于指数的资源分配策略有效性的方法，通过翻译和扩展统计文献中的最新思想，提供了有效的估计器和计算渐近正确置信区间的方法。 |
| [^90] | [ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs](https://arxiv.org/abs/2402.11764) | 本研究提出了一种利用ChatGPT生成合成训练数据来增强LLMs去偏见化的新方法，能够高效地去除已知偏见并跨越不同类别进行去偏见化。 |
| [^91] | [ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs](https://arxiv.org/abs/2402.11753) | 提出了一种新颖的基于ASCII艺术的越狱攻击，以及一个用于评估LLMs在识别非纯语义提示方面能力的基准挑战。五个SOTA LLMs在识别ASCII艺术提示时存在困难。 |
| [^92] | [Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing](https://arxiv.org/abs/2402.11752) | 引入了Diagonalisation Stochastic Gradient Descent（对角化SGD），通过重新参数化和平滑实现非可微模型的快速收敛SGD，在实证评估中表现出简单、快速、稳定，并且取得了数量级的工作规范化方差降低。 |
| [^93] | [Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic](https://arxiv.org/abs/2402.11746) | 提出了一种简单方法RESTA，通过任务算法对精调语言模型进行安全重新定位，有效降低了其有害程度。 |
| [^94] | [Compression Repair for Feedforward Neural Networks Based on Model Equivalence Evaluation](https://arxiv.org/abs/2402.11737) | 提出了一种基于模型等效评估的前馈神经网络压缩修复方法，通过计算两个神经网络之间的输出差异，初始化新的训练集并进行重新训练来改进压缩网络性能 |
| [^95] | [Solving Data-centric Tasks using Large Language Models](https://arxiv.org/abs/2402.11734) | 本文提出了两点贡献：一是创建了一个真实世界的NL-to-code任务数据集，二是引入了一种聚类然后选择提示技术，从输入数据中添加最具代表性的行到LLM提示中。 |
| [^96] | [The Effectiveness of Random Forgetting for Robust Generalization](https://arxiv.org/abs/2402.11733) | FOMO引入了一种新的学习范式，通过随机遗忘部分权重来调节信息并强调学习可泛化的特征，从而显著减少神经网络在对抗攻击下出现的稳健过拟合问题。 |
| [^97] | [Prospector Heads: Generalized Feature Attribution for Large Models & Data](https://arxiv.org/abs/2402.11729) | Prospector heads是一种高效且可解释的基于特征归因的替代方法，它可以应用于任何编码器和任何数据形态，并且通过对不同数据形态的实验，表现优越于传统方法。 |
| [^98] | [GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network](https://arxiv.org/abs/2402.11709) | GNNavi通过引入基于提示的参数高效微调（PEFT）方法和图神经网络（GNN）层，准确引导信息流的汇聚和分布，在大型语言模型中导航信息流动态，超越了标准提示式微调的性能。 |
| [^99] | [Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable](https://arxiv.org/abs/2402.11707) | 讨论了生成式人工智能对搜索引擎的影响，指出了其可能导致的可靠性问题，强调了信息来源不透明和内容正确性等挑战。 |
| [^100] | [Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation](https://arxiv.org/abs/2402.11702) | 大型语言模型在代码生成方面表现出显著能力，但目前主要用于展示概念或提供示例，需要进一步改进才能实现生产就绪代码。 |
| [^101] | [ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model](https://arxiv.org/abs/2402.11684) | 通过采用GPT-4V合成的高质量训练数据，研究成功地实现了ALLaVA，一个轻量级视觉-语言模型，该模型在12个基准测试上表现出与最多3B LVLMs竞争性能。 |
| [^102] | [3D Point Cloud Compression with Recurrent Neural Network and Image Compression Methods](https://arxiv.org/abs/2402.11680) | 提出一种使用循环神经网络和图像压缩方法对3D点云进行压缩的新方法，通过独特的3D到2D转换和密集的表示结构，在压缩中高效利用空间相关性。 |
| [^103] | [MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of LiDAR-Camera Fusion for 3D Object Detection](https://arxiv.org/abs/2402.11677) | MultiCorrupt是用于评估多模态3D目标检测器在十种不同数据损坏类型下的鲁棒性的综合基准。 |
| [^104] | [A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models](https://arxiv.org/abs/2402.11676) | 提出了一个多方面框架，使用大型语言模型评估反叙事，通过5个方面从专门 NGO 指南中提取定义的内容，以解决以往评估方法的局限性。 |
| [^105] | [Autocorrect for Estonian texts: final report from project EKTB25](https://arxiv.org/abs/2402.11671) | 该项目成功开发了爱沙尼亚语言的拼写和语法校正工具，主要创新在于使用迁移学习和自动评估来克服可用数据不足的挑战。 |
| [^106] | [Dynamic planning in hierarchical active inference](https://arxiv.org/abs/2402.11658) | 通过研究在动态规划领域中模拟工具使用的目标，我们深入探讨了主动推断中的动态规划，该领域考虑到生物目标导向行为的两个关键方面 |
| [^107] | [Combinatorial Client-Master Multiagent Deep Reinforcement Learning for Task Offloading in Mobile Edge Computing](https://arxiv.org/abs/2402.11653) | 深度强化学习在移动边缘计算中的任务卸载问题中的应用面临着连续和离散资源约束的挑战，但有望实现高效的任务分配。 |
| [^108] | [Quantum Image Denoising with Machine Learning: A Novel Approach to Improve Quantum Image Processing Quality and Reliability](https://arxiv.org/abs/2402.11645) | 提出一种新方法，通过使用机器学习模型识别和校正量子处理图像中的噪声，以提高量子图像处理的质量和可靠性，实现与经典计算机类似的处理结果。 |
| [^109] | [Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2402.11622) | 提出了一种基于逻辑闭环的框架（LogicCheckGPT），利用大型视觉-语言模型本身来检测和减轻对象幻觉。 |
| [^110] | [Simplifying Hyperparameter Tuning in Online Machine Learning -- The spotRiverGUI](https://arxiv.org/abs/2402.11594) | `spotRiverGUI`是一个为在线机器学习模型进行超参数调优的图形用户界面，简化了用户手动搜索最佳超参数设置的过程。 |
| [^111] | [SDiT: Spiking Diffusion Model with Transformer](https://arxiv.org/abs/2402.11588) | 本文提出了一种新颖的脉冲扩散模型架构，通过在脉冲神经网络中利用Transformer取代U-net结构，在图像生成任务中取得了较高质量的图像，并提供了基于SNN的生成模型研究的实证基准。 |
| [^112] | [Developing Autonomous Robot-Mediated Behavior Coaching Sessions with Haru](https://arxiv.org/abs/2402.11569) | 通过Haru开发了一个完全自主的对话系统，最大限度地发挥了其情感表达和独特人格，成功应用于行为改变辅导，取得了显著的成效。 |
| [^113] | [Continual Learning on Graphs: Challenges, Solutions, and Opportunities](https://arxiv.org/abs/2402.11565) | 对图上持续学习进行了全面评估和分类，弥补了欧几里得数据上持续学习研究的不足。 |
| [^114] | [LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration](https://arxiv.org/abs/2402.11550) | LongAgent通过多智能体协作将语言模型扩展到128K上下文，并在长文本处理方面表现出潜在的优越性。 |
| [^115] | [Syntactic Language Change in English and German: Metrics, Parsers, and Convergences](https://arxiv.org/abs/2402.11549) | 本文研究英语和德语句法语言变化趋势，使用议会辩论语料库，探讨了句法依存距离最小化及基于树图属性的15个度量标准，揭示了现代解析器在这种变化中的影响。 |
| [^116] | [Question Answering Over Spatio-Temporal Knowledge Graph](https://arxiv.org/abs/2402.11542) | 介绍了一个新的基于时空知识图的问答系统STQAD，以解决问答系统在涵盖时空信息的问题上的挑战，提出了一种新的STComplEx嵌入方法STCQA来实现此目标 |
| [^117] | [Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought](https://arxiv.org/abs/2402.11541) | 本文通过对KG知识注入方法进行全面比较，探索为LLMs提供知识图谱知识的最佳方法，以增强它们的理解能力。 |
| [^118] | [Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning](https://arxiv.org/abs/2402.11537) | 通过对五个主要类别的预训练数据的48个数据集进行系统分析，研究了它们对大型语言模型性能的影响，并发现了一些“高影响数据”，如书籍，与模型能力相关联，为LLMs的优化提供了见解。 |
| [^119] | [PreAct: Predicting Future in ReAct Enhances Agent's Planning Ability](https://arxiv.org/abs/2402.11534) | PreAct是一个整合了预测、推理和行动的智能体框架，利用预测信息可以帮助智能体进行更多样化和策略性的推理，导致更有效的行动，提升任务完成效率。 |
| [^120] | [Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative Filtering](https://arxiv.org/abs/2402.11523) | 本文提出了一种基于邻域增强的监督对比学习方法，通过将锚节点的协作邻居视为正样本，有效解决了协同过滤中数据稀疏的问题 |
| [^121] | [Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources](https://arxiv.org/abs/2402.11505) | 该研究引入了FlexLoRA，一个简单而有效的大型语言模型微调聚合方案，能够在联邦学习中充分利用异质客户资源，通过动态调整本地LoRA排名和采用奇异值分解进行权重重新分配，提升全局模型的广泛知识。 |
| [^122] | [Verifiably Following Complex Robot Instructions with Foundation Models](https://arxiv.org/abs/2402.11498) | 提出了一种名为语言指令地面化运动规划（LIMP）系统，利用基础模型和时间逻辑生成指令条件的语义地图，使机器人能够可验证地遵循富有表现力和长期的指令，包括开放词汇参照和复杂的时空约束。 |
| [^123] | [LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation](https://arxiv.org/abs/2402.11485) | LEIA是一种语言适应调整方法，利用维基百科实体名称跨语言增强目标语言语料库，通过左到右的语言建模训练，显著提高了各种非英语语言的表现。 |
| [^124] | [DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning](https://arxiv.org/abs/2402.11472) | 基于图提示学习的DDIPrompt框架旨在解决药物相互作用事件预测中的高度不平衡事件分布和罕见事件标记数据稀缺性问题。 |
| [^125] | [Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective](https://arxiv.org/abs/2402.11463) | Attraos模型基于混沌理论，在长期时间序列预测中利用多尺度动态记忆单元和局部演化策略，表现优异于其他LTSF方法。 |
| [^126] | [FGeo-HyperGNet: Geometry Problem Solving Integrating Formal Symbolic System and Hypergraph Neural Network](https://arxiv.org/abs/2402.11461) | 该论文提出了FGeo-HyperGNet，将形式符号系统和超图神经网络集成，用于解决几何问题，实现自动执行人类化的几何演绎推理。 |
| [^127] | [Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge](https://arxiv.org/abs/2402.11459) | 提出了一种新颖的扩散桥生成模型 Re-Dock，用于灵活和现实的分子对接，通过能量到几何映射来共同建模结合能和构象，填补了对接中的实用性和构象预测方面的差距 |
| [^128] | [SciAgent: Tool-augmented Language Models for Scientific Reasoning](https://arxiv.org/abs/2402.11451) | 引入了工具增强型科学推理的新任务设置，通过提供可扩展的工具集，帮助大型语言模型在科学问题解决中变得更加实用和可解决。 |
| [^129] | [InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration](https://arxiv.org/abs/2402.11441) | 提出了一种Infuser-Guided Knowledge Integration（InfuserKI）框架，利用transformer内部状态有效地将未知知识集成到大型语言模型中，从而缓解知识遗忘问题。 |
| [^130] | [Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models](https://arxiv.org/abs/2402.11436) | 大型语言模型存在自我偏见，研究发现通过更大的模型规模和准确评估的外部反馈可以显著减少这种偏见，并提高后续任务的实际表现。 |
| [^131] | [OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations](https://arxiv.org/abs/2402.11427) | OptEx是第一个通过利用并行计算来减轻一阶优化的迭代瓶颈并增强效率的框架，使用核化梯度估计实现迭代的并行化，提供理论保证。 |
| [^132] | [Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition](https://arxiv.org/abs/2402.11424) | 在广义零样本识别中解决已见数据偏见问题的D$^3$GZSL框架，通过引入分布内双空间精馏和分布外批次精馏模块，实现了更平衡的模型学习。 |
| [^133] | [LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models](https://arxiv.org/abs/2402.11417) | LoRETTA是一个通过张量训练分解显著减少可训练参数的超低参数高效框架，在大型语言模型的微调中表现出与大多数PEFT方法相媲美甚至更好的性能。 |
| [^134] | [An Empirical Evaluation of Neural and Neuro-symbolic Approaches to Real-time Multimodal Complex Event Detection](https://arxiv.org/abs/2402.11403) | 本研究评估神经和神经符号方法在多模态复杂事件检测中的效果，特别关注时间推理，实验发现神经符号方法在较少数据下表现更好。 |
| [^135] | [Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis](https://arxiv.org/abs/2402.11398) | 通过利用LLM增强语义分析，开发了用于文本的相似度度量框架，可显著改善文本的语义相似性评估，并可扩展到其他专业领域。 |
| [^136] | [Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry](https://arxiv.org/abs/2402.11363) | 这项研究提出了Casanovo-DIA，一种基于Transformer架构的深度学习模型，可用于从DIA质谱数据中解析肽段序列。 |
| [^137] | [Training Language Model Agents without Modifying Language Models](https://arxiv.org/abs/2402.11359) | 提出一种新的方法，在不修改语言模型的情况下训练语言模型代理，通过进化代理的功能来解决下游任务 |
| [^138] | [Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2402.11354) | 该论文提出了一种基于概率路由的方法，通过引入PEOs有效识别图中需要考虑进行精确距离计算的邻居，从而显著提高了基于图的近似最近邻搜索的效率。 |
| [^139] | [Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention](https://arxiv.org/abs/2402.11353) | 长期记忆的引入提高了大型语言模型驱动聊天机器人在公共卫生干预中的用户自我披露，但仍存在挑战，特别是在解决慢性健康状况和隐私问题方面。 |
| [^140] | [Tasks That Language Models Don't Learn](https://arxiv.org/abs/2402.11349) | 大型语言模型没有学习到语言的视听特性，在新的任务基准测试中表现较差，暴露了人类语言理解与语言模型感官处理能力之间的根本差距。 |
| [^141] | [Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach](https://arxiv.org/abs/2402.11338) | 该方法提出了一种基于探索的数据收集方法，能够在缺乏部分反馈信息的情况下训练分类器，并提供了一系列策略来确保所有子群体都被探索、防止错误分类、以及收敛到期望的分类器。 |
| [^142] | [Learning by Reconstruction Produces Uninformative Features For Perception](https://arxiv.org/abs/2402.11337) | 重构学习所产生的特征对感知无用，需要通过其他策略如去噪学习来缓解这种不一致性。 |
| [^143] | [SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network Systems](https://arxiv.org/abs/2402.11322) | SpikeNAS提出了一种快速内存感知神经架构搜索框架，旨在帮助脉冲神经网络系统快速找到在给定内存预算下高准确性的适当架构。 |
| [^144] | [Hysteresis Compensation of Flexible Continuum Manipulator using RGBD Sensing and Temporal Convolutional Network](https://arxiv.org/abs/2402.11319) | 基于时间卷积网络的数据驱动方法用于捕捉柔性连续机械臂电缆驱动的非线性特性，提出了滞后补偿的解决方案。 |
| [^145] | [Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics](https://arxiv.org/abs/2402.11317) | 提出了一种名为DORA的新方法，通过信息瓶颈原理在离线设置中学习适应性策略，解决了动态编码与环境数据之间的互信息与与行为策略的互信息之间的难题 |
| [^146] | [Multi-Generative Agent Collective Decision-Making in Urban Planning: A Case Study for Kendall Square Renovation](https://arxiv.org/abs/2402.11314) | 多生成代理系统在城市规划中模拟社区决策，发现沟通有助于集体推理，而包含人口统计数据和生活价值导致意见分歧。这为城市规划和社区参与提供了有价值的见解。 |
| [^147] | [Dissecting Human and LLM Preferences](https://arxiv.org/abs/2402.11296) | 本研究分析了人类和32种不同LLM的偏好，发现人类不太在意错误，偏好支持立场的回应，而先进的LLM更注重正确性、清晰性和无害性。 |
| [^148] | [Puzzle Solving using Reasoning of Large Language Models: A Survey](https://arxiv.org/abs/2402.11291) | 本调查通过将难题分为基于规则和无规则两类的独特分类法，通过各种方法评估了大型语言模型（LLMs）的表现，强调了在复杂难题情境中LLMs的挑战和人类类似推理之间的差距，突出了推动LLMs解谜能力和贡献于人工智能发展的必要性。 |
| [^149] | [Fair Resource Allocation in Virtualized O-RAN Platforms](https://arxiv.org/abs/2402.11285) | 该论文通过实验评估了O-Cloud的能耗及其与服务器硬件、容量和数据流量特性的关系，提出了一种计算策略和无线策略，平衡能源节约和性能，确保它们在服务器和用户之间公平分配。 |
| [^150] | [Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models](https://arxiv.org/abs/2402.11279) | 多透视一致性方法为大型语言模型中的置信度估计带来改进，能有效减轻过度自信问题，并在多个数据集上实现最先进的性能。 |
| [^151] | [Semi-supervised Medical Image Segmentation Method Based on Cross-pseudo Labeling Leveraging Strong and Weak Data Augmentation Strategies](https://arxiv.org/abs/2402.11273) | 本文提出了一种半监督模型DFCPS，创新地融合了Fixmatch概念，通过数据增强处理提高了模型性能和泛化能力，同时引入了交叉伪监督概念，使模型能够充分利用多个角度的伪标签，增强训练的多样性。 |
| [^152] | [MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning](https://arxiv.org/abs/2402.11260) | MoRAL结合了MoE的多任务能力和LoRA的微调能力，采用问答对作为输入，实现了LLM的有效终身学习。 |
| [^153] | [Aligning Large Language Models by On-Policy Self-Judgment](https://arxiv.org/abs/2402.11253) | 本文提出了一个新颖的对齐框架SELF-JUDGE，通过增加式监督微调（JSFT）训练一个同时充当策略和评判器的单一模型，实现了参数高效的基于政策学习，无需额外的奖励模型。 |
| [^154] | [Can Large Language Models perform Relation-based Argument Mining?](https://arxiv.org/abs/2402.11243) | 大型语言模型在处理关系型论证挖掘方面表现更好，能够显著超过目前最佳基准线，并且在十个数据集上进行了实验验证。 |
| [^155] | [Learning with Imbalanced Noisy Data by Preventing Bias in Sample Selection](https://arxiv.org/abs/2402.11242) | 提出了一种用于处理不平衡数据中嘈杂标签的方法，通过Class-Balance-based Sample Selection (CBS)防止忽视尾部类别样本，并通过Confidence-based Sample Augmentation (CSA)增强干净样本的可靠性。 |
| [^156] | [DiffPoint: Single and Multi-view Point Cloud Reconstruction with ViT Based Diffusion Model](https://arxiv.org/abs/2402.11241) | DiffPoint是一种结合了ViT和扩散模型的新型架构，用于实现点云重建任务。 |
| [^157] | [Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents](https://arxiv.org/abs/2402.11208) | 这项工作调查了基于LLM的代理人面临的后门攻击威胁，并提出了一般框架和不同形式的后门攻击分析。 |
| [^158] | [Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges](https://arxiv.org/abs/2402.11203) | ChatGPT作为信息检索领域的关键技术，不断挑战传统范式，带来了新的机遇和挑战，同时超越了之前的GPT-3模型。 |
| [^159] | [Maintaining Adversarial Robustness in Continuous Learning](https://arxiv.org/abs/2402.11196) | 提出了一种名为双梯度投影的方法，通过将梯度投影到两个关键子空间来实现持续鲁棒学习，有效地维持了神经网络对抗性鲁棒性。 |
| [^160] | [I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments](https://arxiv.org/abs/2402.11192) | 将微调过程中的实际响应风格与大型语言模型固有风格相匹配能够产生更好的学习结果，开发的方法通过最小程度地调整模型响应来避免过拟合。 |
| [^161] | [LaCo: Large Language Model Pruning via Layer Collapse](https://arxiv.org/abs/2402.11187) | 提出了一种名为Layer Collapse（LaCo）的简明的逐层剪枝方法，使得大型语言模型可以在保持模型结构的同时迅速减小尺寸，并在剪枝比例达到25-30%时保持超过80%的平均任务性能，明显优于现有最先进的结构化剪枝方法。 |
| [^162] | [KnowTuning: Knowledge-aware Fine-tuning for Large Language Models](https://arxiv.org/abs/2402.11176) | 提出了一种知识感知微调方法，通过显式和隐式方式改善大型语言模型对知识的认识，包括训练模型明确识别答案中的知识三元组和隐式区分可靠和不可靠的知识。 |
| [^163] | [Trust Regions for Explanations via Black-Box Probabilistic Certification](https://arxiv.org/abs/2402.11168) | 通过黑盒概率认证解释的信任区域能够有效地洞察模型行为、保证解释的稳定性，并实现解释的重用 |
| [^164] | [Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection](https://arxiv.org/abs/2402.11167) | 提出了一种新颖的token-ensemble生成策略，挑战了当前AI内容检测方法的鲁棒性，对当前检测模型构成了重要挑战，需要进一步改进检测技术以应对复杂对抗策略。 |
| [^165] | [PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation](https://arxiv.org/abs/2402.11161) | 提出了PANDA方法，引入了更精确的答案正确性评测方式，解决了当前自动评估问答和文本生成过程中的挑战。 |
| [^166] | [LiGNN: Graph Neural Networks at LinkedIn](https://arxiv.org/abs/2402.11139) | 本文介绍了在领英上开发和部署的LiGNN框架，包括对GNN表示学习的算法改进和大规模训练优化，为工作申请回复率、广告点击率和Feed每日活跃用户提高带来了约1%-2%的相对改善。 |
| [^167] | [Contrastive Instruction Tuning](https://arxiv.org/abs/2402.11138) | 提出了对比指令调整方法，通过最大化相似性来提高大型语言模型对未知任务指令的稳健性 |
| [^168] | [Speculative Streaming: Fast LLM Inference without Auxiliary Models](https://arxiv.org/abs/2402.11131) | 提出了一种Speculative Streaming方法，将草稿模型融入目标模型，并通过将微调目标从下一个令牌预测更改为未来的n-gram预测，加速解码1.8-3.1倍，同时保持生成质量。 |
| [^169] | [Optimizing Warfarin Dosing Using Contextual Bandit: An Offline Policy Learning and Evaluation Method](https://arxiv.org/abs/2402.11123) | 本研究提出了一种使用情境臂和离线策略学习的方法，通过观察数据建立个性化的华法林剂量策略，即使在缺乏基因型信息的情况下也可以超越基线方法。 |
| [^170] | [Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models](https://arxiv.org/abs/2402.11122) | 这项研究全面评估了大型语言模型中顺序记忆编辑的影响，发现修改参数ME可能会导致所有任务表现不佳，而保留参数ME则能够保持较好性能。 |
| [^171] | [Language Models as Science Tutors](https://arxiv.org/abs/2402.11111) | 介绍了TutorEval和TutorChat，通过TutorEval基准可以衡量LMs作为科学助手的实际可用性，TutorChat数据集用于微调模型。 |
| [^172] | [Computing Voting Rules with Elicited Incomplete Votes](https://arxiv.org/abs/2402.11104) | 本文研究了通过询问选民有关少量候选人的投票规则计算问题，完全表征了可计算的位置评分规则集合，并给出了对于确定性或随机算法确定最大得分候选人必须进行的查询次数的参数化上限和下限。 |
| [^173] | [The Male CEO and the Female Assistant: Probing Gender Biases in Text-To-Image Models Through Paired Stereotype Test](https://arxiv.org/abs/2402.11089) | 通过成对刻板印象测试（PST）框架，在文本-图像模型中探究性别偏见，并评估了DALLE-3在性别职业和组织权力方面的偏见。 |
| [^174] | [The AI Security Pyramid of Pain](https://arxiv.org/abs/2402.11082) | 介绍了AI安全痛点金字塔框架，提供了一种结构化方法来理解和应对各个级别的AI威胁。 |
| [^175] | [Model Editing by Pure Fine-Tuning](https://arxiv.org/abs/2402.11078) | 纯微调通过优化条件似然、增加随机释义和事实的数据，在模型编辑中取得了不俗的表现。 |
| [^176] | [AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators](https://arxiv.org/abs/2402.11073) | 提出了一种新框架 AFaCTA，利用大型语言模型辅助事实性索赔的标注，提高了标注的效率和一致性。 |
| [^177] | [Bridging Causal Discovery and Large Language Models: A Comprehensive Survey of Integrative Approaches and Future Directions](https://arxiv.org/abs/2402.11068) | 本文综合调查了将大型语言模型（如GPT4）整合到因果发现任务中的方法，揭示了它们在推断因果结构时对元数据和自然语言的创新利用，强调了LLMs在增强传统CD方法和作为专家辅助方面的潜力和挑战。 |
| [^178] | [Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement](https://arxiv.org/abs/2402.11060) | 介绍了 Persona-DB，一个简单却有效的框架，通过层级构建过程和协同优化，改善了大规模语言模型个性化中数据库表示的泛化能力和检索效率。 |
| [^179] | [Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives](https://arxiv.org/abs/2402.11051) | 提出了Conan数据集，用于从侦探叙事中提取和分析复杂的人物关系图，并揭示大型语言模型在推理复杂关系和处理长篇叙事方面的局限性。 |
| [^180] | [Exploring Value Biases: How LLMs Deviate Towards the Ideal](https://arxiv.org/abs/2402.11005) | 研究发现大型语言模型（LLMs）在给出响应时存在一个价值偏好的机制，倾向于偏向理想状态，这种偏差会对不同应用场景产生重要影响。 |
| [^181] | [ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment](https://arxiv.org/abs/2402.11000) | 提出了一个新的实体对齐框架ASGEA，利用Align-Subgraphs中的逻辑规则，设计了可解释的基于路径的图神经网络ASGNN，引入了多模态注意机制，取得了令人满意的实验结果 |
| [^182] | [Analysis and Mortality Prediction using Multiclass Classification for Older Adults with Type 2 Diabetes](https://arxiv.org/abs/2402.10999) | 设计治疗方案需注意患者剩余生命和合并症，研究利用大规模数据集构建多类分类模型来预测老年2型糖尿病患者的死亡率。 |
| [^183] | [Provably Safe Neural Network Controllers via Differential Dynamic Logic](https://arxiv.org/abs/2402.10998) | 通过差分动态逻辑与神经网络验证相结合的VerSAILLE方法，实现了对神经网络控制系统在无限时间范围上的安全性证明。 |
| [^184] | ["Understanding AI": Semantic Grounding in Large Language Models](https://arxiv.org/abs/2402.10992) | 大型语言模型（LLMs）展现了对语义的渐进理解，通过应用心灵哲学和语言学中关于含义的核心假设，研究发现LLMs不仅仅是生成文本的工具，而是在某种程度上已经理解了它们生成的语言。 |
| [^185] | [Accelerating Semi-Asynchronous Federated Learning](https://arxiv.org/abs/2402.10991) | 提出了一种考虑贡献的异步联邦学习方法，动态调整接收到的更新的处理方式，以解决现实情况下同步上传数据可能出现的缓慢和不可靠问题。 |
| [^186] | [WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing](https://arxiv.org/abs/2402.10987) | 该研究提出了一种名为WilKE的知识编辑方法，通过选择编辑层来匹配不同层级中的知识编辑模式程度，在终身编辑中相比最先进的方法平均展现了46.2%和67.8%的改进。 |
| [^187] | [FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models](https://arxiv.org/abs/2402.10986) | FinTral是一类基于Mistral-7b模型的GPT-4级别多模态金融大型语言模型，通过领域特定的预训练和检索方法优化，在AI驱动金融技术中取得显著进展。 |
| [^188] | [Leveraging AI Planning For Detecting Cloud Security Vulnerabilities](https://arxiv.org/abs/2402.10985) | 提出了一个通用框架来建模云系统中的访问控制策略，并开发了基于PDDL模型的新方法来检测可能导致诸如勒索软件和敏感数据外泄等广泛攻击的安全漏洞。 |
| [^189] | [CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback](https://arxiv.org/abs/2402.10980) | 通过将大型语言模型推理与量子化学反馈相结合，我们引入了一个AI引导的计算筛选框架，将催化剂发现形式化为一个不确定环境，从而实现高效催化剂的积极搜索 |
| [^190] | [SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs](https://arxiv.org/abs/2402.10979) | 这项研究介绍了围绕体育数据分析展开的四项新任务，旨在评估大型语言模型在数值推理和信息融合方面的能力。 |
| [^191] | [Language Models with Conformal Factuality Guarantees](https://arxiv.org/abs/2402.10978) | 提出了一种能够通过连接语言建模和符合预测为语言模型提供高概率正确性保证的框架。 |
| [^192] | [Social network analysis for personalized characterization and risk assessment of alcohol use disorders in adolescents using semantic technologies](https://arxiv.org/abs/2402.10967) | 本研究利用语义技术和社交网络分析技术，自动化数据收集和分析阶段，为青少年酒精使用障碍的个性化特征和风险评估提供了新的方法。 |
| [^193] | [Measuring and Controlling Persona Drift in Language Model Dialogs](https://arxiv.org/abs/2402.10962) | 提出了一种量化基准来测量语言模型对话中的“人设”漂移，并提出了一种称为split-softmax的轻量级方法来对抗注意力衰减和“人设”漂移 |
| [^194] | [Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts](https://arxiv.org/abs/2402.10958) | 提出了相对优先权优化（RPO）方法，通过区分来自相同和相关提示的更受青睐的响应和更不受青睐的响应，扩展了模型的学习能力。 |
| [^195] | [Zero-shot Explainable Mental Health Analysis on Social Media by incorporating Mental Scales](https://arxiv.org/abs/2402.10948) | 该方法结合心理量表通过LLMs进行零-shot心理健康分析，实验结果表明其优于其他方法 |
| [^196] | [CultureLLM: Incorporating Cultural Differences into Large Language Models](https://arxiv.org/abs/2402.10946) | 提出了一种名为CultureLLM的成本效益高的解决方案，通过使用世界价值调查（WVS）作为种子数据，并通过提出的语义数据增强来将文化差异纳入大型语言模型中，成功微调得到了涵盖富裕和低资源语言的9种文化特定LLMs以及一个统一模型（CultureLLM-One）。 |
| [^197] | [Text2Data: Low-Resource Data Generation with Textual Control](https://arxiv.org/abs/2402.10941) | Text2Data提出了一种利用未标记数据通过无监督扩散模型来理解基础数据分布的新方法，以解决低资源环境下缺乏文本标签的文本到数据任务中的挑战。 |
| [^198] | [A Lightweight Inception Boosted U-Net Neural Network for Routability Prediction](https://arxiv.org/abs/2402.10937) | 提出了一种轻量级Inception增强的U-Net神经网络模型，用于预测布线拥塞和设计规则检查热点，在实验中取得了显著的性能改进。 |
| [^199] | [ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters](https://arxiv.org/abs/2402.10930) | ConSmax是一种硬件友好型Softmax替代方案，通过引入可学习参数，在不影响性能的情况下实现了对原Softmax关键任务的高效处理。 |
| [^200] | [AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning](https://arxiv.org/abs/2402.10921) | 通过自适应缺失模态情绪识别, 该模型包括查询自适应融合和多模态联合嵌入学习两大特点，旨在提高情绪识别的准确性和鲁棒性。 |
| [^201] | [Designing Silicon Brains using LLM: Leveraging ChatGPT for Automated Description of a Spiking Neuron Array](https://arxiv.org/abs/2402.10920) | 本文展示了使用ChatGPT4为可编程脉冲神经元阵列ASIC生成Verilog描述的设计流程，验证了AI生成的设计，并展示了使用自然语言驱动硬件设计的现状。 |
| [^202] | [LLM-Assisted Crisis Management: Building Advanced LLM Platforms for Effective Emergency Response and Public Collaboration](https://arxiv.org/abs/2402.10908) | 通过LLAMA2语言模型，建立了一个能够从社交媒体和紧急消息中识别和分类紧急情况的方法，可协助在全国范围内的紧急情况下公共安全话务员和大众，提供相关指导并通知政府机构。 |
| [^203] | [Hermite Neural Network Simulation for Solving the 2D Schrodinger Equation](https://arxiv.org/abs/2402.10649) | 使用混合神经网络结合Hermite函数根作为配点，提高了解决二维薛定谔方程的效率和精度，与其他神经网络和方法相比取得了优秀的结果。 |
| [^204] | [Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs](https://arxiv.org/abs/2402.10586) | 本文探讨了如何通过研究文本中的话语特征来区分人类创作和机器生成的文本，引入了一种新颖的方法来揭示这些特征，并发现人类写作在结构上更为多样化。 |
| [^205] | [InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?](https://arxiv.org/abs/2402.10567) | 本研究在印度法律领域探讨了大型语言模型（LLMs）在处理社会因素时的能力，提出了结合公平性和准确性的新指标$LSS_{\beta}$，并评估了模型在二元法律推理任务中的表现以及在印度社会各种不平等方面的公平性展示。 |
| [^206] | [On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities](https://arxiv.org/abs/2402.10340) | 论文突出探讨了在机器人应用中整合大型语言模型和视觉语言模型所带来的安全性和健壮性关键问题，指出这种整合可能容易受到恶意攻击并导致严重后果。 |
| [^207] | [Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment](https://arxiv.org/abs/2402.10207) | 本文介绍了Rewards-in-Context（RiC）方法，该方法通过多个奖励条件控制基础模型的响应，并应用有监督的微调进行对齐。它具有简单性和适应性，并支持在推理时动态调整用户偏好。 |
| [^208] | [Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective](https://arxiv.org/abs/2402.10184) | 本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。 |
| [^209] | [Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data](https://arxiv.org/abs/2402.10100) | 本研究评估了在临床设置中使用深度学习模型进行音频分类的效果，并发现在微调之前，预训练模型在大数据集上的性能对临床数据的影响较好。研究结果表明，CNN模型可以在小数据集环境中与转换模型相媲美或超越。 |
| [^210] | [Enhancing Cybersecurity Resilience in Finance with Deep Learning for Advanced Threat Detection](https://arxiv.org/abs/2402.09820) | 这项研究提出使用深度学习来增强金融行业的网络安全韧性，并实现高级威胁检测。目前的网络威胁检测方法往往基于规则和传统的机器学习方法，无法适用大规模数据应用，并且无法有效检测未知威胁。 |
| [^211] | [Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention](https://arxiv.org/abs/2402.09784) | 该论文基于对比学习和自注意力机制，提出了一种考虑垂直和水平时间接近度的顺序推荐方法，以更好地捕捉用户-项目交互中的时间上下文。 |
| [^212] | [The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse](https://arxiv.org/abs/2402.09656) | 尽管模型编辑在大型语言模型中显示出修订知识的潜力，但少量编辑可以触发模型崩溃，导致性能显著下降。我们提出使用困惑度作为替代指标，并通过实验证实其与下游任务性能的强相关性。 |
| [^213] | [Emerging Opportunities of Using Large Language Language Models for Translation Between Drug Molecules and Indications](https://arxiv.org/abs/2402.09588) | 本研究探讨了使用大型语言模型在药物分子和适应症之间进行翻译的机遇，提出了一个新任务，并测试了其有效性，这对于药物发现过程具有重要意义。 |
| [^214] | [Combatting deepfakes: Policies to address national security threats and rights violations](https://arxiv.org/abs/2402.09581) | 本文提供了应对深度伪造威胁的政策建议，包括背景信息、危害、先前的立法提案以及全面的深度伪造供应链政策建议。 |
| [^215] | [On the Potential of Network-Based Features for Fraud Detection](https://arxiv.org/abs/2402.09495) | 本文研究了基于网络特征在欺诈检测中的潜力，通过使用个性化的PageRank算法来捕捉欺诈的社会动态。实验结果表明，集成PPR可以提高模型的预测能力并提供独特有价值的信息。 |
| [^216] | [LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset](https://arxiv.org/abs/2402.09391) | 本文介绍了LlaSMol，它是一种推进化学领域大规模语言模型的方法。通过使用一个大规模、全面、高质量的指令调优数据集来训练模型，LlaSMol在化学任务中表现出强大的性能，超过了GPT-4并接近于任务特定模型。 |
| [^217] | [Immediate generalisation in humans but a generalisation lag in deep neural networks$\unicode{x2014}$evidence for representational divergence?](https://arxiv.org/abs/2402.09303) | 研究对比了人类和深度神经网络在图像分类中的行为差异，发现人类具有即时概括能力，而DNNs存在滞后概括现象，这表明了表示分歧的存在。 |
| [^218] | [SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds](https://arxiv.org/abs/2402.08653) | SAGMAN是一种用于检验图神经网络稳定性的谱框架，它通过评估非线性映射中的距离失真来衡量GNN的稳定性。为了进行有意义的稳定性分析，我们提出了一种距离保持的图降维方法。 |
| [^219] | [Knowledge Editing on Black-box Large Language Models](https://arxiv.org/abs/2402.08631) | 这项研究提出了在黑盒大型语言模型上进行知识编辑的方法，并引入了一种多角度评估框架和一种新的postEdit框架，以解决现有方法中的隐私和风格问题。 |
| [^220] | [Dueling Over Dessert, Mastering the Art of Repeated Cake Cutting](https://arxiv.org/abs/2402.08547) | 这篇论文研究了重复公平分割问题中的竞争策略，发现如果Bob过于偏好某一块蛋糕，Alice利用类似于二分查找的策略可以系统性地对Bob实施剥削，从而在时间上获得更多资源份额。 |
| [^221] | [ChatCell: Facilitating Single-Cell Analysis with Natural Language](https://arxiv.org/abs/2402.08303) | ChatCell是一个利用自然语言促进单细胞分析的工具，通过词汇适应和统一序列生成，它具备深厚的专业知识和适应各种分析任务的能力。 |
| [^222] | [LLaGA: Large Language and Graph Assistant](https://arxiv.org/abs/2402.08170) | LLaGA是一个创新的模型，它有效地整合了大型语言模型（LLM）的能力，以处理图结构数据的复杂性。通过重新组织图节点以作为结构感知的序列，并通过一个多功能投影仪将其映射到标记嵌入空间中，LLaGA在多样性、泛化性和可解释性方面表现出色。 |
| [^223] | [UFO: A UI-Focused Agent for Windows OS Interaction](https://arxiv.org/abs/2402.07939) | UFO是一个专注于Windows操作系统上应用程序的用户界面智能体，利用GPT-Vision的能力来满足用户需求。它通过观察和分析Windows应用程序的图形用户界面和控制信息，实现无缝导航和操作以满足用户的请求。UFO的控制交互模块使得无需人工干预即可实现动作连接和完全自动化执行，使繁琐和耗时的过程变为简单任务。经过测试，UFO在各种场景中取得了良好效果。 |
| [^224] | [Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction](https://arxiv.org/abs/2402.07570) | 通过下一个曲线形状预测，我们提出了基于编码器的零样本多元时间序列预测模型GTT，通过预训练和通道级别的曲线形状预测，展现出优秀的预测能力，甚至超过了最先进的有监督模型。 |
| [^225] | [Dynamic Graph Information Bottleneck](https://arxiv.org/abs/2402.06716) | 动态图信息瓶颈框架（DGIB）能够学习鲁棒且有区分性的动态图表示。利用信息瓶颈原理，通过迭代引导和改进图快照传递的结构和特征信息流，压缩冗余信息并保留有价值的信息。该框架能满足最小-全局-一致条件，提高了动态图神经网络的鲁棒性。 |
| [^226] | [Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty](https://arxiv.org/abs/2402.06529) | 本文研究了内省规划的概念，作为一种引导语言驱动的代理机器人改进自身不确定性的系统方法。通过识别任务不确定性并主动寻求澄清，内省显著提高了机器人任务规划的成功率和安全性。 |
| [^227] | [On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference](https://arxiv.org/abs/2402.06262) | 本文研究了针对键值约束生成语言模型推理的驱逐策略的有效性，通过引入基于时间注意力得分和鲁棒性度量的RoCo策略，优于现有的策略。 |
| [^228] | [Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning](https://arxiv.org/abs/2402.06025) | 本论文建立了一个计算模型来模拟人们通过实验主动推断隐藏规则的过程，并发现显式假设、概率规则和在线更新的组合可以解释人们在类似Zendo任务上的表现。 |
| [^229] | [Investigating White-Box Attacks for On-Device Models](https://arxiv.org/abs/2402.05493) | 本研究探究了针对设备上模型的白盒攻击，提出了一种逆向工程框架(REOM)以将编译后的设备上TFLite模型转换为可调试模型。 |
| [^230] | [TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning](https://arxiv.org/abs/2402.05396) | 该论文提出了TASER方法，它是针对动态图表示学习的时间自适应采样技术，在准确性、效率和可扩展性方面进行了优化，解决了现实世界动态图中存在的噪声问题。 |
| [^231] | [A Closer Look at the Limitations of Instruction Tuning](https://arxiv.org/abs/2402.05119) | 本文通过实验和分析揭示了指令调整的多个局限性，包括无法增强LLM的知识和技能、从具有知识来源的数据集复制回应模式导致质量下降、全参数微调增加了错误生成的情况。 |
| [^232] | [ScreenAI: A Vision-Language Model for UI and Infographics Understanding](https://arxiv.org/abs/2402.04615) | ScreenAI是一个专注于UI和信息图表理解的视觉-语言模型，通过灵活的修补策略和独特的数据集训练，以及针对UI元素的屏幕注解任务的处理，实现了在多个任务上的新的最优结果。 |
| [^233] | [Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context](https://arxiv.org/abs/2402.03630) | 通过IDE的本地集成，我们提出了IDECoder框架，利用IDE提供的准确和实时的跨文件信息来增强LLM-Based编码工具，解决了挑战性的跨文件上下文问题。 |
| [^234] | [A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation](https://arxiv.org/abs/2402.03358) | 这篇综述调研了图缩减方法，包括稀疏化、粗化和浓缩，在解决大型图形数据分析和计算复杂性方面起到了重要作用。调研对这些方法的技术细节进行了系统的回顾，并强调了它们在实际应用中的关键性。同时，调研还提出了保证图缩减技术持续有效性的关键研究方向。 |
| [^235] | [TopoX: A Suite of Python Packages for Machine Learning on Topological Domains](https://arxiv.org/abs/2402.02441) | TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。 |
| [^236] | [Development and Testing of a Novel Large Language Model-Based Clinical Decision Support Systems for Medication Safety in 12 Clinical Specialties](https://arxiv.org/abs/2402.01741) | 本研究开发并测试了一种新的基于大型语言模型的临床决策支持系统，用于在12个临床专业中提供安全的药物处方。该系统通过定制的处方错误警报解决了传统基于规则的系统的局限性，并评估了其在识别药物错误方面的有效性和临床医生的偏好。 |
| [^237] | [CapHuman: Capture Your Moments in Parallel Universes](https://arxiv.org/abs/2402.00627) | CapHuman是一个新框架，通过“编码然后学习对齐”的范式实现了可泛化的身份保留能力，用于在不同情境中生成具有多样的头部位置、姿势和面部表情的特定个体图像。 |
| [^238] | [Graph Contrastive Learning with Cohesive Subgraph Awareness](https://arxiv.org/abs/2401.17580) | 本研究提出了一种名为CTAug的新框架，将内聚子图意识无缝整合到图对比学习中。通过改进图拓扑增强和图学习过程，提高了对各种图的表征学习性能。 |
| [^239] | [Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA](https://arxiv.org/abs/2401.15847) | 引入了多面板视觉问答（MultipanelVQA）基准挑战大型视觉语言模型（LVLMs）对理解多面板图像的能力，并发现LVLMs在这方面仍然存在显著挑战。 |
| [^240] | [Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey](https://arxiv.org/abs/2401.11963) | 通过整合进化算法与强化学习，进化强化学习（ERL）展现出卓越的性能提升，本综述呈现了ERL领域的各个研究分支，突出了EA辅助RL的优化、RL辅助EA的优化以及EA和RL的协同优化这三个主要研究方向。 |
| [^241] | [Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?](https://arxiv.org/abs/2401.11911) | 该论文研究了大型语言模型如何合并生成和检索的上下文以提升开放领域问答，发现这些模型偏向于生成的上下文，即使它们提供了错误的信息。 |
| [^242] | [PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety](https://arxiv.org/abs/2401.11880) | PsySafe提出了一个综合框架，通过深入探讨智能体心理学，揭示智能体的黑暗心理状态对安全构成威胁，并提出了有效的风险缓解策略。 |
| [^243] | [LightDiC: A Simple yet Effective Approach for Large-scale Digraph Representation Learning](https://arxiv.org/abs/2401.11772) | LightDiC是基于磁性拉普拉斯的可扩展有向图卷积方法，通过在离线预处理中进行拓扑相关计算，实现了可扩展性，适用于大规模数据库。 |
| [^244] | [PRewrite: Prompt Rewriting with Reinforcement Learning](https://arxiv.org/abs/2401.08189) | 本文提出了一种基于强化学习的自动化工具PRewrite，用于重写提示草案并生成高效的新提示，以解决提示工程中的挑战。 |
| [^245] | [When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment](https://arxiv.org/abs/2401.07764) | 本文提出了在6G网络中为LLM agents设计的分布式学习系统，通过协作利用移动设备和边缘服务器，以在长期交互中实现多个具有不同角色的LLMs分布。 |
| [^246] | [Combining Machine Learning and Ontology: A Systematic Literature Review](https://arxiv.org/abs/2401.07744) | 通过系统文献综述研究了将机器学习和本体整合的过程，发现了增强型本体学习、语义数据挖掘和学习与推理系统这三种主要的混合类别，比较了不同研究中使用的机器学习算法。 |
| [^247] | [Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation](https://arxiv.org/abs/2401.07382) | 本文提出了一种新的框架，利用大型语言模型的评论能力在强化学习训练中产生中间步骤奖励，以应对稀疏奖励信号所带来的挑战。 |
| [^248] | [Generative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity](https://arxiv.org/abs/2401.07348) | 生成式人工智能和大型语言模型在欧盟法律法规中的应用引发了责任、隐私、知识产权和网络安全等方面的挑战，本文对现有和拟议的法律进行了批判性分析，并提出了改进建议。 |
| [^249] | [Graph Language Models](https://arxiv.org/abs/2401.07105) | 引入了一种新型的图语言模型（GLM），结合线性化和图神经网络的优点，解决了传统方法在处理结构化知识图谱时的弱点。 |
| [^250] | [Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering](https://arxiv.org/abs/2401.06824) | 通过表示工程对LLMs进行越狱是一种新颖的方法，它利用少量查询对提取“安全模式”，成功规避目标模型的防御，实现了前所未有的越狱性能。 |
| [^251] | [Graph-of-Thought: Utilizing Large Language Models to Solve Complex and Dynamic Business Problems](https://arxiv.org/abs/2401.06801) | GoT是一种新型模型，利用图结构增强大型语言模型在复杂任务执行中的灵活性和效率，通过动态路径选择推进了传统认知模型，开源引擎GoTFlow展示了其在自动化决策方面的巨大潜力。 |
| [^252] | [Refining Pre-Trained Motion Models](https://arxiv.org/abs/2401.00850) | 通过自监督训练改进了现有监督模型，发现大多数自监督技术使性能变差，而不是更好。 |
| [^253] | [Preparing to Integrate Generative Pretrained Transformer Series 4 models into Genetic Variant Assessment Workflows: Assessing Performance, Drift, and Nondeterminism Characteristics Relative to Classifying Functional Evidence in Literature](https://arxiv.org/abs/2312.13521) | 评估了将GPT-4模型整合到基因变体评估工作流程中的性能、非确定性和漂移特征，为其在复杂临床流程中的适用性提供信息。 |
| [^254] | [Engineering an Exact Pseudo-Boolean Model Counter](https://arxiv.org/abs/2312.12341) | 提出了第一个依赖于代数决策图知识编译方法的精确伪布尔模型计数器PBCount，在模型计数方面取得了显著进展 |
| [^255] | [PPO-Clip Attains Global Optimality: Towards Deeper Understandings of Clipping](https://arxiv.org/abs/2312.12065) | 该论文在PPO-Clip算法方面做出贡献，建立了其在表格和神经函数逼近设置中的全局收敛结果，特别突出了在神经函数逼近情境下的$O(1/\sqrt{T})$最小迭代收敛速率。 |
| [^256] | [Counting Reward Automata: Sample Efficient Reinforcement Learning Through the Exploitation of Reward Function Structure](https://arxiv.org/abs/2312.11364) | 提出了计数奖励自动机概念，能够对任何奖励函数进行建模，使得强化学习更加高效，并且能够利用自然语言任务描述来指定状态机，实验证明在样本效率方面优于竞争方法。 |
| [^257] | [Demystifying Instruction Mixing for Fine-tuning Large Language Models](https://arxiv.org/abs/2312.10793) | 指令微调提升了大语言模型在各种任务中的性能，研究发现不同指令类型对特定应用更有利，但可能对其他领域产生负面影响。 |
| [^258] | [An Attentive Inductive Bias for Sequential Recommendation beyond the Self-Attention](https://arxiv.org/abs/2312.10325) | 提出了一种名为BSARec的新方法，超越了自注意力，在序列推荐中注入了归纳偏差，并集成了低频和高频信息以减轻过度平滑问题 |
| [^259] | [TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning](https://arxiv.org/abs/2312.09039) | TAP4LLM提出了一个用于生成表格提示的多功能预处理工具箱，通过采样、增补和打包半结构化数据，解决了在大型语言模型推理中处理复杂问题和大型表格的挑战。 |
| [^260] | [Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations](https://arxiv.org/abs/2312.08935) | Math-Shepherd提出了一种新的数学奖励模型，通过自动生成的过程监督数据实现LLMs的逐步验证和加强，显著提高了数学问题解决的准确性。 |
| [^261] | [On Diversified Preferences of Large Language Model Alignment](https://arxiv.org/abs/2312.07401) | 本文通过定量分析常用人类反馈数据集，揭示了多样化偏好对奖励建模的影响，提出了一种新颖的多目标奖励学习方法以增强校准性能 |
| [^262] | [KnowGPT: Black-Box Knowledge Injection for Large Language Models](https://arxiv.org/abs/2312.06185) | KnowGPT是一种为大型语言模型提供黑盒知识注入的框架，通过深度强化学习和多臂老虎机构建最适合每个问题的提示，在三个基准数据集上实验证明其显著提升了知识注入的效果。 |
| [^263] | [Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding](https://arxiv.org/abs/2312.06149) | 提出了将文本生成形式化为未来受限生成问题的方法，以最小化不良行为并强制执行对指令的忠实性，并通过LLMs有效指导文本生成。 |
| [^264] | [Class-Aware Pruning for Efficient Neural Networks](https://arxiv.org/abs/2312.05875) | 本文提出了一种面向类别的修剪技术，与先前的修剪策略不同，该技术通过评估与类别数量相关的滤波器重要性来压缩DNN，从而减少计算成本。 |
| [^265] | [Better Neural PDE Solvers Through Data-Free Mesh Movers](https://arxiv.org/abs/2312.05583) | 提出了一种神经PDE求解器，通过无数据神经网格适配器（DMM）解决了神经PDE求解器需要昂贵网格数据和解决空间的自由度和拓扑结构变化的挑战。 |
| [^266] | [Applying Large Language Models and Chain-of-Thought for Automatic Scoring](https://arxiv.org/abs/2312.03748) | 本研究探讨了在自动评分学生对科学评估写作反馈中应用大型语言模型（LLMs）和思维链（CoT），通过零次或少次学习等策略自动评分，其中少次学习表现更佳。 |
| [^267] | [Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization](https://arxiv.org/abs/2311.18703) | 该论文提出了一种名为PA-RL的方法，通过最小化熵率来引导强化学习智能体展现可预测的行为。研究展示了如何利用平均替代奖励实现确定性策略，并在动态模型的基础上近似计算值函数。 |
| [^268] | [Legal Requirements Analysis: A Regulatory Compliance Perspective](https://arxiv.org/abs/2311.13871) | 本文研究了从合规角度分析法律要求，特别是在软件开发过程中的要求工程阶段，以确保软件的合规性。主要关注欧盟的一般数据保护条例（GDPR）等法规对收集、处理或分享个人数据的软件系统的规定。 |
| [^269] | [On Copyright Risks of Text-to-Image Diffusion Models](https://arxiv.org/abs/2311.12803) | 文本到图像扩散模型存在版权侵权风险，我们通过引入数据生成管道，研究了更微妙的侵权形式，提供了更实际的版权侵权调查方法 |
| [^270] | [Alpha Zero for Physics: Application of Symbolic Regression with Alpha Zero to find the analytical methods in physics](https://arxiv.org/abs/2311.12713) | 提出了使用Alpha Zero算法中的符号回归来发展物理学中的解析方法，展示了其可以推导出Floquet系统中的高频展开，可能带来新的物理学理论框架。 |
| [^271] | [Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools](https://arxiv.org/abs/2311.10801) | 使用EarnMore方法，我们提出了一种新的RL方法，可以允许RL代理与可定制股票池（CSPs）交互，而不需要重新训练。 |
| [^272] | [MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning](https://arxiv.org/abs/2311.10537) | 提出了一个新颖的医学领域的跨学科合作(MC)框架，利用基于LLM的代理在角色扮演设置中参与协作多轮讨论，从而提高LLM的熟练程度和推理能力 |
| [^273] | [Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models](https://arxiv.org/abs/2311.09278) | Symbol-LLM 提出了一种通过数据和框架来解决大型语言模型中符号数据注入的挑战，旨在捕捉符号间的相互关系和促进协同作用。 |
| [^274] | [Safer-Instruct: Aligning Language Models with Automated Preference Data](https://arxiv.org/abs/2311.08685) | Safer-Instruct通过反向指导调整、指导感应和专家模型评估的方式，实现了自动构建大规模偏好数据的目的，从而在没有人工标注者的情况下高效生成高质量的偏好数据。 |
| [^275] | [Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios](https://arxiv.org/abs/2311.08154) | 自一致性是一种通用的集成优化方法，可以应用于几乎所有情景中，能够解决语言模型推理中的重复性和局部最优性问题。 |
| [^276] | [Adversarial Preference Optimization](https://arxiv.org/abs/2311.08045) | 提出了一种对抗偏好优化（APO）框架，实现了在没有额外注释的情况下，通过对抗学习自适应于生成分布差距。 |
| [^277] | [Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators](https://arxiv.org/abs/2311.07879) | 本研究揭示了人工智能模型在识别有毒、冒犯和令人讨厌的内容方面的进展，并探讨了这些改进是否真正满足了志愿内容管理员在工作中的需求。 |
| [^278] | [ResMGCN: Residual Message Graph Convolution Network for Fast Biomedical Interactions Discovering](https://arxiv.org/abs/2311.07632) | 提出了一种新颖的Residual Message Graph Convolution Network（ResMGCN），用于快速而准确地预测生物医学相互作用。 |
| [^279] | [Open-Set Graph Anomaly Detection via Normal Structure Regularisation](https://arxiv.org/abs/2311.06835) | 通过正常结构规范化方法，实现开放图异常检测模型对未知异常的广义检测能力 |
| [^280] | [Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion](https://arxiv.org/abs/2311.06318) | 提出一种新颖且通用的方法，通过从用户与搜索引擎的交互历史中提取相关上下文来个性化大型语言模型的输出，尤其适用于改进网络搜索体验。 |
| [^281] | [Foundational theories of hesitant fuzzy sets and hesitant fuzzy information systems and their applications for multi-strength intelligent classifiers](https://arxiv.org/abs/2311.04256) | 本文提出了基于犹豫模糊集的多种包含关系定义、犹豫模糊信息系统的基础命题和基于多强度智能分类器的健康状态诊断方法。 |
| [^282] | [Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs](https://arxiv.org/abs/2311.02847) | 基于物体的运动学结构，提出了一种运动学感知提示框架，用于生成LLMs的低级运动轨迹位点，以实现对可移动物体的泛化操作 |
| [^283] | [DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing](https://arxiv.org/abs/2311.01450) | DreamSmooth通过学习预测时间平滑奖励而非精确奖励，优化了基于模型的强化学习，在稀疏奖励任务上表现出最先进的性能。 |
| [^284] | [Re-evaluating Retrosynthesis Algorithms with Syntheseus](https://arxiv.org/abs/2310.19796) | 使用Syntheseus建立的基准库重新评估了回溯合成算法，揭示了现有技术模型的系统性缺陷并提供了对未来工作的指导建议。 |
| [^285] | [Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models](https://arxiv.org/abs/2310.13828) | 本文展示了对文本到图像生成模型进行特定提示中毒攻击的成功，并介绍了Nightshade这种优化的中毒攻击方法，在视觉上与良性图像相同，可在少于100个毒样本中破坏稳定扩散SDXL提示。 |
| [^286] | [Faithful Knowledge Graph Explanations for Commonsense Reasoning](https://arxiv.org/abs/2310.04910) | 本论文提出了两个量化指标来衡量基于知识图谱的解释的可信性，并引入了一种新的训练方法来改善解释的可信度。实验结果表明该方法可以提高解释的一致性和保真度。 |
| [^287] | [CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation](https://arxiv.org/abs/2310.01407) | CoDi引入了一种新的方法，通过适应预先训练的潜在扩散模型接受额外的图像条件输入，可以显著降低生成高质量结果所需的采样步骤。 |
| [^288] | [Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models](https://arxiv.org/abs/2310.00566) | 大型语言模型在信用评分任务中表现出强大的通用能力，通过首个开源框架和CALM模型，可以帮助解决传统信用评分方法所面临的挑战，并同时解决潜在的偏见问题。 |
| [^289] | [From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning](https://arxiv.org/abs/2310.00492) | 指令调整对LLMs产生了三个重要影响：1）使其能够识别用户提示中的指令部分；2）促进响应生成的不断调整 |
| [^290] | [Scene Informer: Anchor-based Occlusion Inference and Trajectory Prediction in Partially Observable Environments](https://arxiv.org/abs/2309.13893) | 场景通知者是一种统一方法，用于在部分可观察的环境中预测观察代理的轨迹和推断遮挡，其利用transformer聚合输入模态并实现对可能与自主车辆计划路径相交的遮挡的选择性查询。 |
| [^291] | [ChatEDA: A Large Language Model Powered Autonomous Agent for EDA](https://arxiv.org/abs/2308.10204) | 该研究介绍了ChatEDA，一个由大型语言模型AutoMage赋能的EDA自主代理，通过有效管理任务计划、脚本生成和任务执行，简化了从RTL到GDSII的设计流程，并证明了其性能优越性。 |
| [^292] | [OctoPack: Instruction Tuning Code Large Language Models](https://arxiv.org/abs/2308.07124) | 通过结合Git提交的自然结构，将代码更改与人类指令配对，我们提出了OctoPack，并在大规模语言模型上实现了表现最佳的指令调整方法。 |
| [^293] | [Building Cooperative Embodied Agents Modularly with Large Language Models](https://arxiv.org/abs/2307.02485) | 利用大型语言模型构建模块化的协作体现智能体，实现多智能体合作解决具有挑战性的任务，超越规划方法并展示有效沟通。 |
| [^294] | [Toward Grounded Commonsense Reasoning](https://arxiv.org/abs/2306.08651) | 提出了一种利用大型语言模型和视觉语言模型帮助机器人积极感知环境，进行基于常识的推理的方法。 |
| [^295] | [Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems](https://arxiv.org/abs/2303.05754) | 提出了一种将扩散采样和Krylov子空间方法协同结合的新型高效采样策略。 |
| [^296] | [Distributional GFlowNets with Quantile Flows](https://arxiv.org/abs/2302.05793) | 本文提出了一种带分布式量化流的GFlowNets模型，通过将流函数转化为分布，在训练过程中提供更多信息的学习信号。通过量化函数参数化每个边流，我们提出的算法可以学习风险敏感的策略，实现对风险不确定性场景的处理，并在现有基准上取得了显著改进。 |
| [^297] | [Finding Nontrivial Minimum Fixed Points in Discrete Dynamical Systems](https://arxiv.org/abs/2301.04090) | 在离散动力系统中，我们提出了一个优化问题，即寻找具有最小受影响节点数量的系统非平凡不动点，并发现了一些特殊情况下可以高效解决这个问题。 |
| [^298] | [Motion Planning on Visual Manifolds](https://arxiv.org/abs/2210.04047) | 提出了一种视觉配置空间（VCS）的替代性刻画，使具有身体结构的实体代理能够使用自己的图像集在随机姿势下发现自己的身体结构，并在周边空间中规划无障碍运动。 |
| [^299] | [Semi-supervised Batch Learning From Logged Data](https://arxiv.org/abs/2209.07148) | 本研究基于反事实风险最小化框架提出了一种半监督批量学习方法，解决了在已记录数据中反馈缺失的问题，提出了一个新的上界来处理这种学习问题。 |
| [^300] | [Learning Progress Driven Multi-Agent Curriculum](https://arxiv.org/abs/2205.10016) | 提出了自主式MARL（SPMARL）以解决当前多智能体强化学习中课程生成的问题，优先考虑基于任务的优先级。 |
| [^301] | [Variance Reduction Based Experience Replay for Policy Optimization](https://arxiv.org/abs/2110.08902) | 引入了基于方差减少的经验回放框架，实现了选择性重复利用相关样本来改善策略梯度估计，并构建了高效的离策略算法PG-VRER。 |
| [^302] | [HCR-Net: A deep learning based script independent handwritten character recognition network](https://arxiv.org/abs/2108.06663) | HCR-Net提出了一种基于迁移学习的脱机手写字符识别网络，通过部分利用预训练网络的特征提取层，实现了更快、更高效的训练，更好的性能和泛化能力。 |
| [^303] | [Cooperative Inverse Reinforcement Learning](https://arxiv.org/abs/1606.03137) | 提出了合作逆强化学习(CIRL)的定义，将价值对齐问题转化为机器人与人类之间的合作部分信息博弈，通过积极教学、积极学习和沟通行为等方式实现最大化价值对齐。 |
| [^304] | [Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports.](http://arxiv.org/abs/2401.16578) | 该论文提出了一种方法，将专业放射科医生的专业知识与大型语言模型相结合，来提升自动生成报告的自动评估。实验结果显示，该方法的模型在评估中表现优于传统的度量标准。 |
| [^305] | [Effective Controllable Bias Mitigation for Classification and Retrieval using Gate Adapters.](http://arxiv.org/abs/2401.16457) | 本文引入了可控门适配器（ConGater），一种具有可调节敏感性参数的新颖模块化门机制，可在推理时逐渐过渡从模型的偏向状态到完全去偏的版本，并通过实验证明了其在分类和检索任务中的性能。 |
| [^306] | [A Statistical Framework for Measuring AI Reliance.](http://arxiv.org/abs/2401.15356) | 该论文提出了一个基于统计决策理论的依赖的形式定义，用于衡量人工智能系统的适当依赖。该定义分离了依赖的概念和人类在形成准确信念时面临的挑战，为人类与人工智能互补性和依赖性的研究设计提供了指导。 |
| [^307] | [GeoDecoder: Empowering Multimodal Map Understanding.](http://arxiv.org/abs/2401.15118) | GeoDecoder是一种专门设计用于处理地图中地理空间信息的多模态模型，通过集成图像和文本处理模块，无缝集成外部数据和特征，以及执行多任务训练和执行，实现了强化地图认知的目标。 |
| [^308] | [Prompt Weight Experiments for LLM Instruction Fine-Tuning.](http://arxiv.org/abs/2401.13586) | LLM指令微调中，对于短提示完成数据集，提示词标记分类损失加权（PLW）与性能呈负二次关系，而长提示完成数据集则不受PLW影响。 |
| [^309] | [Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo.](http://arxiv.org/abs/2401.11665) | 本文提出了一种使用欠阻尼 Langevin Monte Carlo 加速的近似 Thompson 采样策略，通过特定势函数的设计改善了高维问题中的样本复杂度，并在高维赌博机问题中进行了验证。 |
| [^310] | [R-Judge: Benchmarking Safety Risk Awareness for LLM Agents.](http://arxiv.org/abs/2401.10019) | 这篇论文主要介绍了一种评估LLM代理在不同环境中判断安全风险能力的基准测试R-Judge，通过对162个代理交互记录进行评估，发现GPT-4模型表现最佳，达到了72.29%的准确率。 |
| [^311] | [Technical Report: On the Convergence of Gossip Learning in the Presence of Node Inaccessibility.](http://arxiv.org/abs/2401.09498) | 本文研究了在动态网络拓扑下，不可访问节点对流言学习的收敛性的影响，并提供了理论分析。 |
| [^312] | [On Image Search in Histopathology.](http://arxiv.org/abs/2401.08699) | 这篇论文综述了组织病理学图像搜索技术的最新发展，为计算病理学研究人员提供了简明的概述，旨在寻求有效、快速和高效的图像搜索方法。 |
| [^313] | [Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving.](http://arxiv.org/abs/2401.03160) | 本文提出了一种增强的人机协作强化学习方法，通过将人类智能注入到AI中实现混合交通编队中的安全高效自动驾驶。该方法将人类专家作为导师，允许代理在不确定环境中进行探索，同时在危险情况下接管控制以避免事故，并指导代理减小交通流干扰，优化交通流效果。 |
| [^314] | [GraphPro: Graph Pre-training and Prompt Learning for Recommendation.](http://arxiv.org/abs/2311.16716) | GraphPro是一个结合了参数高效和动态图预训练与提示学习的框架，能够有效捕捉长期用户偏好和短期行为动态，从而在真实世界的推荐系统中提供准确和及时的推荐。 |
| [^315] | [DRNet: A Decision-Making Method for Autonomous Lane Changingwith Deep Reinforcement Learning.](http://arxiv.org/abs/2311.01602) | DRNet是一种基于深度强化学习的决策框架，可以帮助自动驾驶车辆进行车道变换，并考虑到周围车辆的驾驶风格，实现安全的决策策略。 |
| [^316] | [Beyond Average Return in Markov Decision Processes.](http://arxiv.org/abs/2310.20266) | 该论文研究了马尔可夫决策过程中超越平均回报的问题，总结了可以准确计算和优化的奖励函数的特征，并提供了针对这些特征的新规划方法。这些结果在马尔可夫决策过程的理论发展中具有重要意义。 |
| [^317] | [Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation.](http://arxiv.org/abs/2310.13505) | 这项研究提出了一种新的框架REIGN，通过生成训练问题的改写，并使用深度强化学习来指导对话问答模型，增加模型对表面形式变化的鲁棒性，同时在不同的基准上进行零-shot应用。 |
| [^318] | [SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation.](http://arxiv.org/abs/2310.12508) | 这篇论文提出了一种名为SalUn的机器遗忘方法，通过引入"权重显著性"的概念，将关注点从整个模型引导到具体的模型权重上，提高了遗忘的效果和效率。这是第一个能够有效消除遗忘数据、类别或概念影响的有原则的机器遗忘方法。 |
| [^319] | [DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical Optimal Transport Framework.](http://arxiv.org/abs/2310.12081) | 本研究提出了一种名为DHOT-GM的图匹配方法，使用可微分的分层最优传输框架，充分利用了图中隐藏的多模态信息，通过对匹配结果进行加权平均来推断节点对应关系。 |
| [^320] | [Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation.](http://arxiv.org/abs/2310.11730) | 本文提出了一种联邦异构图神经网络（FedHGNN）的框架，能够在分布式的异构信息网络上协同训练推荐模型，同时保护用户隐私。 |
| [^321] | [Large Language Model Unlearning.](http://arxiv.org/abs/2310.10683) | 大型语言模型的去学习是一个研究的新领域，我们探索了三个场景，可以通过去学习让语言模型与人类偏好保持一致。去学习具有三个优势，只需要负面示例，计算效率高，特别对于知道具体导致不良行为的训练样本更为有效。 |
| [^322] | [Understanding the Effects of RLHF on LLM Generalisation and Diversity.](http://arxiv.org/abs/2310.06452) | 本研究深入分析了强化学习从人类反馈中调整的大型语言模型每个阶段对超出分布泛化和输出多样性的影响。 |
| [^323] | [On Double-Descent in Reinforcement Learning with LSTD and Random Features.](http://arxiv.org/abs/2310.05518) | 本文研究了在强化学习中网络大小和L2正则化对性能的影响，并观察到了双下降现象。通过使用随机特征和懒惰训练策略，在参数和状态数无限大的情况下研究了正则化的最小二乘时间差分算法，得出了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。 |
| [^324] | [Hieros: Hierarchical Imagination on Structured State Space Sequence World Models.](http://arxiv.org/abs/2310.05167) | Hieros是一种基于结构化状态空间序列的分层想像模型，通过学习时间抽象的世界表示并在潜在空间中多个时间尺度上想象轨迹，实现了更高效的训练和想象。 |
| [^325] | [Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity.](http://arxiv.org/abs/2310.02277) | 本文研究通过稀疏性分析LLM预训练权重的任务中心角度，挑战了传统对于权重中冗余性的观点，并提出了"垃圾DNA假设"。 |
| [^326] | [ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events.](http://arxiv.org/abs/2309.12244) | ChaCha是一个利用大型语言模型（LLMs）的聊天机器人，鼓励儿童分享个人事件和相关情绪。通过一个探索性研究，发现儿童将ChaCha视为亲密的朋友，并愿意与其分享各种主题的故事。 |
| [^327] | [RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud.](http://arxiv.org/abs/2309.09737) | RaTrack是一种针对雷达跟踪的创新解决方案，通过运动分割和聚类以及运动估计模块，实现了对移动物体的精确跟踪，优于最先进性能。 |
| [^328] | [VERSE: Virtual-Gradient Aware Streaming Lifelong Learning with Anytime Inference.](http://arxiv.org/abs/2309.08227) | 这项研究提出了一种具有实时推理能力的流式终身学习方法，采用虚拟梯度进行连续表示学习，借助语义记忆来抑制灾难性遗忘，并在多样化的数据上进行了广泛实验。 |
| [^329] | [DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning.](http://arxiv.org/abs/2309.05173) | DePT通过将软提示分解为较短的软提示和一对低秩矩阵，并用两个不同的学习率来优化，以解决提示调整对训练和推理时间以及内存使用的影响，从而实现更好的性能。 |
| [^330] | [Identifiable Cognitive Diagnosis with Encoder-decoder for Modelling Students' Performance.](http://arxiv.org/abs/2309.00300) | 本文提出了一个可识别的认知诊断框架，该框架能够从学生的答题记录中直接诊断可识别和可解释的考生特征和题目特征，并通过重建答题记录来确保诊断结果的可识别性。 |
| [^331] | [Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models.](http://arxiv.org/abs/2308.15022) | 递归总结在大型语言模型中实现长期对话记忆，可以提高对话系统在长对话中记忆重要信息的能力。 |
| [^332] | [Exploring Large Language Models for Knowledge Graph Completion.](http://arxiv.org/abs/2308.13916) | 本文研究了利用大型语言模型（LLM）进行知识图谱补全的方法，并引入了一种创新的框架（知识图谱LLM），以提高三元组分类和关系预测的性能。 |
| [^333] | [Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings.](http://arxiv.org/abs/2308.11804) | 该论文研究了多模态嵌入中的对抗幻觉问题。对手可以扰动输入的任意模态，使其嵌入与其他模态的任意输入接近，从而实现任意图像与任意文本、任意文本与任意声音的对齐。该问题与下游任务无关，对生成和分类任务会产生误导。 |
| [^334] | [ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation.](http://arxiv.org/abs/2308.11131) | 本论文提出了一种名为ReLLa的检索增强大型语言模型框架，用于零样本和小样本推荐任务。通过语义用户行为检索（SUBR）来提取上下文中的有用信息，以改善LLMs的推荐性能。 |
| [^335] | [Backward Reasoning in Large Language Models for Verification.](http://arxiv.org/abs/2308.07758) | 本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。 |
| [^336] | [Bandits with Deterministically Evolving States.](http://arxiv.org/abs/2307.11655) | 该论文提出了一种名为具有确定性演化状态的强盗模型，用于学习带有强盗反馈的推荐系统和在线广告。该模型考虑了状态演化的不同速率，能准确评估奖励与系统健康程度之间的关系。 |
| [^337] | [Robust Visual Question Answering: Datasets, Methods, and Future Challenges.](http://arxiv.org/abs/2307.11471) | 这篇论文提供了一份关于鲁棒视觉问答的综合调查，包括了数据集的发展过程、评估指标以及不同方法之间的差异。先前的通用视觉问答方法常常会受到训练数据中的偏差的影响，难以学习到正确的行为。为了增强鲁棒性，研究者提出了各种数据集和去偏差方法。 |
| [^338] | [Nature of Intelligence.](http://arxiv.org/abs/2307.11114) | 智能的本质是一系列通过在空间和时间上建立数据集之间的功能关系来最小化系统熵的数学函数过程。 |
| [^339] | [Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success.](http://arxiv.org/abs/2307.06865) | 本论文提出了一个系统地衡量提示提取攻击成功的框架，并通过多个实验发现，即使提示被保密，简单的基于文本的攻击仍然可以高概率地揭示提示。 |
| [^340] | [Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning.](http://arxiv.org/abs/2307.05209) | 我们提出了一种使用奖励机器抽象来表示当前任务，并在迁移学习中提升DRL代理的性能的方法，实验表明该方法能够提高样本效率并在多个领域中进行少样本迁移。 |
| [^341] | [Automated Detection of Gait Events and Travel Distance Using Waist-worn Accelerometers Across a Typical Range of Walking and Running Speeds.](http://arxiv.org/abs/2307.04866) | 该论文研究了使用腰部佩戴的加速计自动检测步态事件和行走距离的方法，通过分析市售智能手机加速计数据，实现了从广泛的步态速度范围中提取步态特征，可用于对Duchenne肌肉萎缩患儿和典型发育正常患者的评估。 |
| [^342] | [Understanding Unfairness via Training Concept Influence.](http://arxiv.org/abs/2306.17828) | 通过观察训练数据的作用，研究模型不公平性的来源和影响，并通过改变样本的属性来计算训练样本对模型的不公平性的影响。 |
| [^343] | [Towards Personalized Cold-Start Recommendation with Prompts.](http://arxiv.org/abs/2306.17256) | 本研究旨在解决个性化冷启动推荐问题，通过利用预训练语言模型的能力，将推荐过程转化为自然语言情感分析，提供适用于创业企业和用户参与历史不足的平台的个性化推荐。 |
| [^344] | [Group Orthogonalization Regularization For Vision Models Adaptation and Robustness.](http://arxiv.org/abs/2306.10001) | 该论文提出了一种新的正则化技术，称为组正交正则化，能够提高视觉模型自适应和鲁棒性。实验结果表明，此种正则化可以应用于不同的深度学习模型，并且能够有效地优化模型性能。 |
| [^345] | [Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic.](http://arxiv.org/abs/2306.02865) | 该论文提出了 BEE 操作符，通过充分利用过去的成功经验，并保持探索乐观性，解决了离线策略演员-评论家中 Q 值高估与低估问题，提高了策略学习和样本效率。 |
| [^346] | [Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias.](http://arxiv.org/abs/2305.19894) | Med-UniC是一个新的框架，旨在通过整合英语和西班牙语的跨语言医学数据，实现跨语言医学图像-语言预训练的统一。他们提出了跨语言文本对齐规则(CTR)，以明确统一来自不同语言社区的医学报告的跨语言语义表示。 |
| [^347] | [A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks.](http://arxiv.org/abs/2305.19306) | 本文提出了SpikeGCL，一种用二值化1比特表示来提高效率和节约资源的图对比学习框架，实验结果表明可以以近32倍的表示存储压缩实现高效学习。 |
| [^348] | [Autoencoding Conditional Neural Processes for Representation Learning.](http://arxiv.org/abs/2305.18485) | 本文提出了部分像素空间变分自编码器，结合了自编码器与条件神经过程，可以学习到一系列基本物理和文化概念的表示，并且可以提高上下文预测的准确性。 |
| [^349] | [Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize.](http://arxiv.org/abs/2305.16830) | 本文提出了一种避免限制性假设的解决方案，利用机器学习模型的特性来提高学习损失函数的样本效率，在预测优化问题中实现了最先进的结果。 |
| [^350] | [Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery.](http://arxiv.org/abs/2305.14259) | 本文介绍了一种基于文献的发现方法，通过上下文化的学习生成新的科学方向，克服了标准方法在预测关联、忽略上下文等方面的局限性。模型使用了引文和知识图关系的网络，并使用大型语言模型进行评估，发现GPT4在生成创新思想方面表现出色。 |
| [^351] | [GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study.](http://arxiv.org/abs/2305.13062) | 本文设计了一个基准测试来评估大型语言模型（LLMs）对结构化表格数据的理解能力，并发现不同的输入选择会对性能产生影响。在基准测试的基础上，提出了“自我增强”技术以改善理解能力。 |
| [^352] | [CodeCompose: A Large-Scale Industrial Deployment of AI-assisted Code Authoring.](http://arxiv.org/abs/2305.12050) | CodeCompose是一个基于InCoder LLM的AI辅助的代码编写工具，已经在Meta内部部署，可在10多种编程语言和几个编码表面上使用。 CodeCompose已成功帮助提高Meta的开发人员生产力。 |
| [^353] | [Multimodal Web Navigation with Instruction-Finetuned Foundation Models.](http://arxiv.org/abs/2305.11854) | 本文研究使用视觉语言基础模型进行数据驱动离线训练的 Web 代理，提出了一个指令跟随多模态代理WebGUM，将微调指令微调语言模型和视觉转换器，能够有效提高代理的基于视觉感知、HTML 理解和多步推理的能力。 |
| [^354] | [DeformerNet: Learning Bimanual Manipulation of 3D Deformable Objects.](http://arxiv.org/abs/2305.04449) | 本论文介绍了一种名为DeformerNet的神经网络架构，通过学习三维可塑物体的低维表示来实现机器人对物体形状的操纵。这种方法不需要手工特征和物体特定的控制模型，可在仿真和真实机器人上进行演示和应用。 |
| [^355] | [MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks.](http://arxiv.org/abs/2304.14979) | 本文介绍了一种新型框架MLCopilot，通过利用最先进的大语言模型，扩展其能力以理解结构化输入并进行深入推理以解决新型ML任务的能力，展示了MLCopilot在解决图像分类，文本分类和表格分类三项任务方面的巨大潜力。 |
| [^356] | [Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study.](http://arxiv.org/abs/2304.04339) | 本文对ChatGPT作为情感分析器进行了初步评估，包括标准评估、极性转移评估、开放域评估和情感推理评估，共涉及18个数据集和5个情感分析任务。与经过微调的BERT和最先进的模型进行了对比，并进行了人工评估和案例研究。 |
| [^357] | [Can AI-Generated Text be Reliably Detected?.](http://arxiv.org/abs/2303.11156) | 本研究通过实证和理论分析表明，在实际场景中，几种AI文本检测器不可靠。改写攻击可以破解多种检测器，包括水印方案、神经网络检测器和零样本分类器。即使是最好的检测器，随着语言模型的进一步提升，性能也会下降。因此，AI生成的文本的可靠检测仍然是一个挑战。 |
| [^358] | [Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation.](http://arxiv.org/abs/2303.05983) | 本论文构建了两个多模态数据集来验证视觉语言模型在文本-视觉对话任务中的能力，并开发特定规则的监督信号来让系统展示可追溯性。 |
| [^359] | [On The Coherence of Quantitative Evaluation of Visual Explanations.](http://arxiv.org/abs/2302.10764) | 本研究针对常用神经网络解释方法，探究不同评估度量下的表现以及评估方法之间的比较，发现方法的表现经常不一致且选择评估度量至关重要。 |
| [^360] | [Explaining wall-bounded turbulence through deep learning.](http://arxiv.org/abs/2302.01250) | 本研究采用深度学习预测了壁面边界层湍流中的速度场，并利用SHAP算法评估了相干结构对预测的重要性。这一过程或有助于解决湍流研究中的难题，为湍流模型的发展提供新思路。 |
| [^361] | [Estimating counterfactual treatment outcomes over time in complex multi-agent scenarios.](http://arxiv.org/abs/2206.01900) | 本论文提出了一个可解释的反事实循环网络，用于在复杂的多智能体场景中估计干预效果。该模型考虑了时间变化的多智能体关系和协变量反事实预测的复杂结构，能够准确评估个体治疗效果，并提供解释性。 |

# 详细

[^1]: TrustAgent: 通过代理构成实现安全可信赖的LLM代理

    TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution

    [https://rss.arxiv.org/abs/2402.01586](https://rss.arxiv.org/abs/2402.01586)

    本文介绍了一种基于代理构成的代理框架TrustAgent，该框架通过预先规划、规划过程中和计划后检查三种策略来提高LLM代理的安全性。实验结果表明，这些方法可以有效识别和预防潜在危险。此外，还研究了安全性与使用者满意度以及模型推理能力与效率之间的关系。

    

    近年来，基于LLM的代理引起了广泛关注，但其可信度仍未得到深入探索。由于代理可以直接与物理环境交互，其可靠性和安全性至关重要。本文提出了一种基于代理构成的代理框架TrustAgent，对LLM代理的安全性维度进行了初步研究。该框架包括三种策略：预先规划策略，在生成计划之前向模型注入安全知识；规划过程中策略，在生成计划时增强安全性；计划后检查策略，通过计划后检查确保安全性。通过实验分析，我们展示了这些方法如何通过识别和预防潜在危险有效提高LLM代理的安全性。此外，我们还探讨了安全性与使用者满意度之间的复杂关系，以及模型的推理能力与其效率之间的关联。

    The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
    
[^2]: 基于GPU的LTL学习

    LTL learning on GPUs

    [https://arxiv.org/abs/2402.12373](https://arxiv.org/abs/2402.12373)

    实现了首个基于GPU的LTL学习器，使用新颖的枚举式程序合成，性能显著优于现有最先进的学习器，处理跟踪至少多2048倍，速度平均快46倍，并且引入了具有$O(\log n)$时间复杂度的无分支LTL semantics。

    

    线性时序逻辑（LTL）在工业验证中被广泛使用。LTL公式可以从跟踪中学习。扩展LTL公式学习是一个待解决的问题。我们实现了第一种基于GPU的LTL学习器，使用了一种新颖的枚举式程序合成。该学习器是完备和正确的。我们的基准测试表明，它处理的跟踪至少比现有最先进的学习器多2048倍，平均至少快46倍。这是通过诸多方法实现的，包括具有$O(\log n)$时间复杂度的新型无分支LTL语义，其中$n$是跟踪长度，而以前的实现是$O(n^2)$或更糟（假设按位布尔运算和按2的幂移位具有单位成本——这是对现代处理器的现实假设）。

    arXiv:2402.12373v1 Announce Type: cross  Abstract: Linear temporal logic (LTL) is widely used in industrial verification. LTL formulae can be learned from traces. Scaling LTL formula learning is an open problem. We implement the first GPU-based LTL learner using a novel form of enumerative program synthesis. The learner is sound and complete. Our benchmarks indicate that it handles traces at least 2048 times more numerous, and on average at least 46 times faster than existing state-of-the-art learners. This is achieved with, among others, novel branch-free LTL semantics that has $O(\log n)$ time complexity, where $n$ is trace length, while previous implementations are $O(n^2)$ or worse (assuming bitwise boolean operations and shifts by powers of 2 have unit costs -- a realistic assumption on modern processors).
    
[^3]: AnaloBench：评估抽象和长上下文类比识别的基准

    AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies

    [https://arxiv.org/abs/2402.12370](https://arxiv.org/abs/2402.12370)

    通过提出ANALOBENCH基准来评估语言模型（LMs）进行类比推理的能力，发现扩展LMs规模对于处理涉及长场景或相关经验回忆的类比时带来的性能提升较小。

    

    人类经常进行类比思维，将个人经验与当前情况联系起来（$X$类似于$Y$是因为$Z$）。类比思维使人类能够用创造性方式解决问题，理解困难概念，更有效地表达想法。能否语言模型（LMs）也能做到这一点？为了回答这个问题，我们提出了ANALOBENCH，一个用于确定LMs类比推理能力的基准。我们的基准方法专注于人类之间共同的类比推理能力方面：（i）从大量信息中回忆相关经验，以及（ii）将类比推理应用于复杂和长度较长的场景。我们测试了大量专有模型（例如，GPT系列，Claude V2）和开源模型，如LLaMA2。与先前的结果一样，扩展LMs会带来一些性能提升。令人惊讶的是，在类比涉及长场景或回忆相关经验时，规模的提升带来的增益很小。

    arXiv:2402.12370v1 Announce Type: cross  Abstract: Humans regularly engage in analogical thinking, relating personal experiences to current situations ($X$ is analogous to $Y$ because of $Z$). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose ANALOBENCH, a benchmark to determine analogical reasoning ability in LMs. Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios. We test a broad collection of proprietary models (e.g., GPT family, Claude V2) and open source models such as LLaMA2. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) rec
    
[^4]: 对大型语言模型进行AI反馈的关键评估

    A Critical Evaluation of AI Feedback for Aligning Large Language Models

    [https://arxiv.org/abs/2402.12366](https://arxiv.org/abs/2402.12366)

    研究质疑复杂的强化学习在AI反馈中的必要性，表明使用更强的教师模型进行监督微调可以超越现有的RLAIF管道。

    

    强化学习与AI反馈（RLAIF）是一种用于提高强大预训练语言模型的指令遵循能力的流行范式。 RLAIF首先使用来自教师模型的示范进行监督微调（SFT），然后再使用来自评论模型的反馈进行强化学习（RL）进一步微调模型。尽管最近流行的开源模型已经证明了从RL步骤中获得的性能显着提高，但在本文中，我们质疑是否复杂的RL步骤真正有必要为AI反馈。我们展示了RL步骤的改进几乎完全是因为使用较弱的教师模型（例如GPT-3.5）用于SFT数据收集而不是用于AI反馈生成的评论者（例如GPT-4）的广泛实践。具体而言，我们展示了简单的以GPT-4作为教师的监督微调优于现有的RLAIF管道。

    arXiv:2402.12366v1 Announce Type: cross  Abstract: Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models. RLAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation. Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF pipelines. More generally, w
    
[^5]: 通用物理变压器

    Universal Physics Transformers

    [https://arxiv.org/abs/2402.12365](https://arxiv.org/abs/2402.12365)

    提出了通用物理变压器（UPTs）这一新颖学习范式，能够模拟广泛的时空问题，同时适用于拉格朗日和欧拉离散化方案，有效地传播动态并允许查询潜在空间

    

    基于深度神经网络的偏微分方程替代者近来引起了越来越多的关注。然而，类似于它们的数值对应物，在不同应用中使用不同的技术，即使系统的基础动态相似。一个著名的例子是在计算流体动力学中的拉格朗日和欧拉表述，这为神经网络有效地建模基于粒子而不是网格的动态构成了挑战。我们引入了通用物理变压器（UPTs），这是一种新颖的学习范式，它模拟了一系列时空问题 - 对拉格朗日和欧拉离散化方案。UPTs在没有基于网格或基于粒子的潜在结构的情况下运行，从而在网格和粒子之间实现了灵活性。UPTs在潜在空间中高效传播动态，强调了逆编码和解码技术。最后，UPTs允许查询潜在空间表现

    arXiv:2402.12365v1 Announce Type: cross  Abstract: Deep neural network based surrogates for partial differential equations have recently gained increased interest. However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space repre
    
[^6]: 使用基于物理的神经网络的非线性离散时间观测器

    Nonlinear Discrete-Time Observers with Physics-Informed Neural Networks

    [https://arxiv.org/abs/2402.12360](https://arxiv.org/abs/2402.12360)

    通过基于物理的神经网络，提出了一种非线性离散时间观测器，在单步精确观测器线性化框架内学习非线性状态转换映射，通过两个案例研究进行性能评估，并与传统幂级数数值实现进行比较

    

    我们使用基于物理的神经网络（PINNs）来解决离散时间非线性观测器状态估计问题。提出的PINN方法集成在单步精确观测器线性化框架内，旨在通过解决一个非齐次泛函方程组来学习非线性状态转换映射。通过两个说明性案例研究来评估所提出的PINN方法的性能，对于这些案例，观测器线性化转换映射可以通过解析方法得出。我们还为所提出的PINN方案进行不确定性量化分析，并将其与依赖于计算幂级数解的传统幂级数数值实现进行比较。

    arXiv:2402.12360v1 Announce Type: cross  Abstract: We use Physics-Informed Neural Networks (PINNs) to solve the discrete-time nonlinear observer state estimation problem. Integrated within a single-step exact observer linearization framework, the proposed PINN approach aims at learning a nonlinear state transformation map by solving a system of inhomogeneous functional equations. The performance of the proposed PINN approach is assessed via two illustrative case studies for which the observer linearizing transformation map can be derived analytically. We also perform an uncertainty quantification analysis for the proposed PINN scheme and we compare it with conventional power-series numerical implementations, which rely on the computation of a power series solution.
    
[^7]: LoRA+: 大规模模型的高效低秩适应性

    LoRA+: Efficient Low Rank Adaptation of Large Models

    [https://arxiv.org/abs/2402.12354](https://arxiv.org/abs/2402.12354)

    LoRA+通过设置不同的学习率来改进原始LoRA的低效率问题，在保持计算成本不变的情况下提高了模型性能和微调速度。

    

    在这篇论文中，我们展示了低秩适应（LoRA）最初由胡等人（2021年）引入，导致对具有大宽度（嵌入维度）的模型进行微调时表现亚优。这是因为LoRA中的适配器矩阵A和B使用相同的学习率进行更新。通过对大宽度网络进行缩放参数的论证，我们展示了对适配器矩阵A和B使用相同的学习率不利于有效的特征学习。然后，我们表明LoRA的这种次优性可以简单地通过为LoRA适配器矩阵A和B设置不同的学习率以及一个精心选择的比率来进行校正。我们将这个提出的算法称为LoRA$+$。在我们广泛的实验证明中，LoRA$+$在相同计算成本下提高了性能（1-2％的改进）和微调速度（最多提速约2倍）。

    arXiv:2402.12354v1 Announce Type: cross  Abstract: In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.
    
[^8]: 通过博弈论评估揭示LLM的战略推理局限性的GTBench

    GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations

    [https://arxiv.org/abs/2402.12348](https://arxiv.org/abs/2402.12348)

    该论文通过博弈论任务评估了LLMs在竞争环境中的推理能力，观察到LLMs在不同游戏场景下表现出不同行为，具有重要的战略推理局限性。

    

    随着大型语言模型（LLMs）被整合到关键的现实世界应用中，它们的战略和逻辑推理能力变得越来越关键。本文通过博弈论任务评估LLMs在竞争环境中的推理能力，例如，需要纯逻辑和战略推理来与对手竞争的棋盘游戏和纸牌游戏。我们首先提出了GTBench，这是一个以语言驱动的环境，包括10个广泛认可的任务，涵盖了全面的游戏分类法：完整信息与不完整信息，动态与静态，以及概率与确定性场景。然后，我们研究了两个关键问题：（1）表征LLMs的博弈论推理；（2）LLM对抗LLM的比赛作为推理评估。我们观察到（1）LLMs在各种游戏场景下有不同的行为；例如，LLMs在完整和确定性游戏中失败，但它们在概率游戏中具有竞争力。

    arXiv:2402.12348v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we investigate two key problems: (1) Characterizing game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming
    
[^9]: Robust CLIP: 对视觉嵌入进行无监督对抗微调以获得强大的大规模视觉-语言模型

    Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models

    [https://arxiv.org/abs/2402.12336](https://arxiv.org/abs/2402.12336)

    通过无监督对抗微调，提出了一种强大的CLIP视觉编码器，用于增强各种视觉-语言模型的鲁棒性。恶意第三方提供操纵图像的用户隐形攻击得以杜绝。

    

    诸如OpenFlamingo、LLaVA和GPT-4之类的多模型基础模型越来越广泛地用于各种真实世界任务。先前的工作表明，这些模型在视觉模态上极易受到对抗性攻击的影响。这些攻击可以用来传播虚假信息或欺骗用户，因此构成了一个重大风险，这使得大型多模型基础模型的鲁棒性成为一项紧迫的问题。我们提出了一种无监督对抗微调方案，以获得强大的CLIP视觉编码器，在所有依赖于CLIP的视觉下游任务（VLMs、零样本分类）上具有鲁棒性。特别地，我们展示了一旦更换原始的CLIP模型，用户在使用VLMs时会受到恶意第三方提供的操纵图像的潜在攻击。

    arXiv:2402.12336v1 Announce Type: cross  Abstract: Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP mo
    
[^10]: 生成生存可解释轨迹和数据

    Generating Survival Interpretable Trajectories and Data

    [https://arxiv.org/abs/2402.12331](https://arxiv.org/abs/2402.12331)

    提出了一种新的模型，能够生成生存轨迹和数据，并通过特定结构的自动编码器解决了预测、数据补充和生成原型时间相关轨迹等任务

    

    提出了一种基于应用特定结构的自动编码器来生成生存轨迹和数据的新模型。 它解决了三个任务。 首先，它基于Beran估计器为新生成的特征向量提供事件时间的预测和生存函数的形式。 第二，该模型基于给定的训练集生成额外数据，可以补充原始数据集。 第三，最重要的是，它为对象生成了一个原型时间相关轨迹，描述了如何改变对象的特征以实现不同时间事件的时间。 轨迹可以看作是一种反事实解释。 由于将特定加权方案纳入变分自动编码器中，所提出的模型在训练和推理过程中表现出鲁棒性。 该模型还通过解决分类问题确定了新生成数据的被审查指标。

    arXiv:2402.12331v1 Announce Type: cross  Abstract: A new model for generating survival trajectories and data based on applying an autoencoder of a specific structure is proposed. It solves three tasks. First, it provides predictions in the form of the expected event time and the survival function for a new generated feature vector on the basis of the Beran estimator. Second, the model generates additional data based on a given training set that would supplement the original dataset. Third, the most important, it generates a prototype time-dependent trajectory for an object, which characterizes how features of the object could be changed to achieve a different time to an event. The trajectory can be viewed as a type of the counterfactual explanation. The proposed model is robust during training and inference due to a specific weighting scheme incorporating into the variational autoencoder. The model also determines the censored indicators of new generated data by solving a classificatio
    
[^11]: 基于查询的对抗性提示生成

    Query-Based Adversarial Prompt Generation

    [https://arxiv.org/abs/2402.12329](https://arxiv.org/abs/2402.12329)

    该研究提出了一种基于查询的对抗性攻击方法，通过利用远程语言模型的 API 访问构造对抗性示例，使模型以更高概率发出有害字符串，而非仅仅基于模型之间的转移性攻击。

    

    最近的研究表明，可以构造对抗性示例，导致一个对其进行了调整的语言模型产生有害字符串或执行有害行为。现有的攻击要么在白盒设置中（完全访问模型权重），要么通过可转移性：一种现象，即在一个模型上精心设计的对抗性示例通常在其他模型上仍然有效。我们通过基于查询的攻击改进以前的工作，利用 API 访问远程语言模型来构造对抗性示例，使模型以（明显）更高的概率发出有害字符串，而不能仅仅使用转移攻击。我们在 GPT-3.5 和 OpenAI 的安全分类器上验证了我们的攻击；我们能够让 GPT-3.5 发出有害字符串，而目前的转移攻击失败了，并且我们几乎以 100% 的概率规避了安全分类器。

    arXiv:2402.12329v1 Announce Type: cross  Abstract: Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.
    
[^12]: 我们应该交流吗：探索竞争LLM代理之间的自发合作

    Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents

    [https://arxiv.org/abs/2402.12327](https://arxiv.org/abs/2402.12327)

    该研究揭示了LLM代理甚至在竞争环境中也能自发形成合作关系的能力，验证了计算社会科学的愿景，表明LLM代理可以用于模拟人类社会互动，包括自发合作的互动，为社会现象提供洞察。

    

    最近的进展表明，由大型语言模型（LLMs）驱动的代理具有模拟人类行为和社会动态的能力。然而，尚未研究LLM代理在没有明确指令的情况下自发建立合作关系的潜力。为了弥补这一空白，我们进行了三项案例研究，揭示了LLM代理甚至在竞争环境中也能自发形成合作关系的能力。这一发现不仅展示了LLM代理模拟人类社会中竞争与合作的能力，也验证了计算社会科学的一个有前途的愿景。具体来说，这表明LLM代理可以用于建模人类社会互动，包括那些自发合作的互动，从而提供对社会现象的洞察。这项研究的源代码可在https://github.com/wuzengqing001225/SABM_ShallWe 找到。

    arXiv:2402.12327v1 Announce Type: new  Abstract: Recent advancements have shown that agents powered by large language models (LLMs) possess capabilities to simulate human behaviors and societal dynamics. However, the potential for LLM agents to spontaneously establish collaborative relationships in the absence of explicit instructions has not been studied. To address this gap, we conduct three case studies, revealing that LLM agents are capable of spontaneously forming collaborations even within competitive settings. This finding not only demonstrates the capacity of LLM agents to mimic competition and cooperation in human societies but also validates a promising vision of computational social science. Specifically, it suggests that LLM agents could be utilized to model human social interactions, including those with spontaneous collaborations, thus offering insights into social phenomena. The source codes for this study are available at https://github.com/wuzengqing001225/SABM_ShallWe
    
[^13]: 具有公平意识的动态环境响应型在线元学习

    Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness

    [https://arxiv.org/abs/2402.12319](https://arxiv.org/abs/2402.12319)

    引入了FairSAR，一种独特的遗憾度量，以解决动态环境下的公平意识在线学习挑战。

    

    具有公平意识的在线学习框架已经成为连续终身学习背景下的强大工具。在这种情况下，学习者的目标是随着时间推移逐渐获取新任务，同时在引入新任务时确保各个受保护子人口（如种族和性别）之间的统计平等。本文引入了一种独特的遗憾度量FairSAR，以应对不断发展的环境中的公平意识在线学习挑战。

    arXiv:2402.12319v1 Announce Type: cross  Abstract: The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning. In this scenario, the learner's objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender, when it comes to the newly introduced tasks. A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework. Nevertheless, it's crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions. In this paper, to tackle the fairness-aware online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by inco
    
[^14]: ARKS：用于代码生成中的知识汤中的活跃检索

    ARKS: Active Retrieval in Knowledge Soup for Code Generation

    [https://arxiv.org/abs/2402.12317](https://arxiv.org/abs/2402.12317)

    ARKS是一种用于代码生成的先进策略，通过活跃检索和整合各种信息源，能够提高大型语言模型的性能，为解决与频繁更新的库和长尾编程语言相关的现实编程问题提供了新的可能性。

    

    最近，检索增强生成（RAG）范式引起了人们的极大关注，因为它有潜力将外部知识整合到大型语言模型（LLMs）中，而无需进一步训练。尽管在自然语言应用中得到广泛探讨，但它在代码生成中的利用仍未得到充分探索。在本文中，我们介绍了一种名为知识汤中的活跃检索(ARKS)的先进策略，用于泛化大型语言模型以生成代码。与依靠单一来源不同，我们构建了一个将网页搜索、文档、执行反馈和进化代码片段整合在一起的知识汤。我们采用了一种积极的检索策略，该策略迭代地优化查询并更新知识汤。为了评估ARKS的性能，我们编制了一个新的基准，其中包括与频繁更新的库和长尾编程语言相关的现实编程问题。在ChatGPT和CodeLlama上的实验结果表明，ARKS比使用传统方法能够更好地产生代码。

    arXiv:2402.12317v1 Announce Type: cross  Abstract: Recently the retrieval-augmented generation (RAG) paradigm has raised much attention for its potential in incorporating external knowledge into large language models (LLMs) without further training. While widely explored in natural language applications, its utilization in code generation remains under-explored. In this paper, we introduce Active Retrieval in Knowledge Soup (ARKS), an advanced strategy for generalizing large language models for code. In contrast to relying on a single source, we construct a knowledge soup integrating web search, documentation, execution feedback, and evolved code snippets. We employ an active retrieval strategy that iteratively refines the query and updates the knowledge soup. To assess the performance of ARKS, we compile a new benchmark comprising realistic coding problems associated with frequently updated libraries and long-tail programming languages. Experimental results on ChatGPT and CodeLlama de
    
[^15]: 多视角一致学习用于异构传感器融合

    Multi-View Conformal Learning for Heterogeneous Sensor Fusion

    [https://arxiv.org/abs/2402.12307](https://arxiv.org/abs/2402.12307)

    我们提出了用于异构传感器融合的多视角一致模型，并引入了基于集合交集的多视角半一致模型。

    

    能够评估机器学习模型中个别预测的置信度对于决策至关重要，尤其是在诸如医疗诊断、安全和无人车等关键应用中。在过去几年，复杂的预测模型在解决困难任务方面取得了巨大成功，每天都有新方法被提出。而大多数机器学习模型的新发展主要集中在提高整体性能上，对个别预测的可信度考量较少，甚至在传感器融合的背景下更少。为此，我们构建并测试了用于异构传感器融合的多视角和单视角一致模型。由于基于一致性预测框架，我们的模型提供了理论上的边际置信保证。我们还提出了基于集合交集的多视角半一致模型。

    arXiv:2402.12307v1 Announce Type: cross  Abstract: Being able to assess the confidence of individual predictions in machine learning models is crucial for decision making scenarios. Specially, in critical applications such as medical diagnosis, security, and unmanned vehicles, to name a few. In the last years, complex predictive models have had great success in solving hard tasks and new methods are being proposed every day. While the majority of new developments in machine learning models focus on improving the overall performance, less effort is put on assessing the trustworthiness of individual predictions, and even to a lesser extent, in the context of sensor fusion. To this end, we build and test multi-view and single-view conformal models for heterogeneous sensor fusion. Our models provide theoretical marginal confidence guarantees since they are based on the conformal prediction framework. We also propose a multi-view semi-conformal model based on sets intersection. Through comp
    
[^16]: 开源到底如何了？关于商业和开源LLM在标记胸部X射线报告能力方面的比较研究

    Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports

    [https://arxiv.org/abs/2402.12298](https://arxiv.org/abs/2402.12298)

    这项研究比较了商业模型GPT-3.5 Turbo和GPT-4与开源模型Mistral-7B、Mixtral-8x7B、Llama2-13B、Llama2-70B、QWEN1.5-72B以及CheXbert和CheXpert-labeler在准确标记X射线文本报告中多发现存在的能力。

    

    随着大型语言模型（LLMs）的快速发展，涌现了许多新的开源模型和商业模型。虽然最近的出版物探讨了GPT-4在从放射学报告中提取感兴趣信息方面的应用，但尚未对GPT-4与不同领先的开源模型进行实际比较。本研究使用了两个不同的独立数据集。第一个数据集包括了2019年7月至2021年7月在马萨诸塞州综合医院创建的540份胸部X射线报告。第二个数据集包含了来自ImaGenome数据集的500份胸部X射线报告。然后，我们比较了商业模型GPT-3.5 Turbo和GPT-4与开源模型Mistral-7B、Mixtral-8x7B、Llama2-13B、Llama2-70B、QWEN1.5-72B以及CheXbert和CheXpert-labeler在准确标记X射线文本报告中多发现存在的能力。

    arXiv:2402.12298v1 Announce Type: cross  Abstract: Introduction: With the rapid advances in large language models (LLMs), there have been numerous new open source as well as commercial models. While recent publications have explored GPT-4 in its application to extracting information of interest from radiology reports, there has not been a real-world comparison of GPT-4 to different leading open-source models.   Materials and Methods: Two different and independent datasets were used. The first dataset consists of 540 chest x-ray reports that were created at the Massachusetts General Hospital between July 2019 and July 2021. The second dataset consists of 500 chest x-ray reports from the ImaGenome dataset. We then compared the commercial models GPT-3.5 Turbo and GPT-4 from OpenAI to the open-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B, QWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accurately label the presence of multiple findings in x-ray text repo
    
[^17]: 为无监督环境设计优化极小化遗憾

    Refining Minimax Regret for Unsupervised Environment Design

    [https://arxiv.org/abs/2402.12284](https://arxiv.org/abs/2402.12284)

    介绍了贝叶斯级别完美的MMR（BLP），它是极小化遗憾目标的精确化，能够克服极小化遗憾策略在遗憾上界时学习停滞的限制。

    

    在无监督环境设计中，强化学习代理通过对对手最大化某个目标生成的环境配置（关卡）进行训练。遗憾是一种常用的目标，理论上导致具有良好鲁棒性保证的极小化遗憾（MMR）策略；特别是，代理的最大遗憾是有界的。然而，一旦代理在所有关卡上达到了这个遗憾上界，对手将只会对无法进一步减少遗憾的关卡进行采样。尽管在这些最大化遗憾的关卡之外可能存在性能改进空间，但学习停滞。在这项工作中，我们介绍了贝叶斯级别完美的MMR（BLP），它是极小化遗憾目标的精确化。我们正式证明，解决这个目标将导致MMR策略的子集，并且BLP策略在所有关卡上都与完美贝叶斯策略一致行事。

    arXiv:2402.12284v1 Announce Type: cross  Abstract: In unsupervised environment design, reinforcement learning agents are trained on environment configurations (levels) generated by an adversary that maximises some objective. Regret is a commonly used objective that theoretically results in a minimax regret (MMR) policy with desirable robustness guarantees; in particular, the agent's maximum regret is bounded. However, once the agent reaches this regret bound on all levels, the adversary will only sample levels where regret cannot be further reduced. Although there are possible performance improvements to be made outside of these regret-maximising levels, learning stagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a refinement of the minimax regret objective that overcomes this limitation. We formally show that solving for this objective results in a subset of MMR policies, and that BLP policies act consistently with a Perfect Bayesian policy over all levels. We fur
    
[^18]: 自适应骨架图解码

    Adaptive Skeleton Graph Decoding

    [https://arxiv.org/abs/2402.12280](https://arxiv.org/abs/2402.12280)

    提出了骨架图解码（SGD）方法，利用子问题之间的依赖关系进行信息转发，改善响应质量且提高性能。

    

    大型语言模型（LLMs）已经在自然语言任务中得到广泛应用，其成功归因于大量的模型参数（例如，70亿+）；然而，LLM推断会产生巨大的计算和内存成本。最近的方法提出了并行解码策略，例如“思想骨架”（SoT），通过将提示分解为可以并行解码的子问题来改善性能；但是，它们往往在响应质量上遭受损失。我们的关键见解是，在生成子问题时，我们可以请求额外信息，特别是依赖关系和难度，以提高响应质量和性能。在本文中，我们提出了骨架图解码（SGD），利用子问题之间暴露的依赖关系，支持依赖子问题之间的信息转发，以提高质量，同时暴露独立子问题解码的并行化机会。

    arXiv:2402.12280v1 Announce Type: cross  Abstract: Large language models (LLMs) have seen significant adoption for natural language tasks, owing their success to massive numbers of model parameters (e.g., 70B+); however, LLM inference incurs significant computation and memory costs. Recent approaches propose parallel decoding strategies, such as Skeleton-of-Thought (SoT), to improve performance by breaking prompts down into sub-problems that can be decoded in parallel; however, they often suffer from reduced response quality. Our key insight is that we can request additional information, specifically dependencies and difficulty, when generating the sub-problems to improve both response quality and performance. In this paper, we propose Skeleton Graph Decoding (SGD), which uses dependencies exposed between sub-problems to support information forwarding between dependent sub-problems for improved quality while exposing parallelization opportunities for decoding independent sub-problems. 
    
[^19]: 有效零样本跨语言生成任务中的关键因素

    Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks

    [https://arxiv.org/abs/2402.12279](https://arxiv.org/abs/2402.12279)

    通过微调学习率可以缓解零样本跨语言生成中以错误语言生成的问题，全面微调模型是很强大的基线，mBART在这个任务中表现类似于同等大小的mT5。

    

    零样本跨语言生成意味着在一个语言上微调多语种预训练语言模型，然后将其用于在其他语言上进行此任务的预测。以前的研究指出一个经常出现的问题是以错误的语言生成，并提出了方法来解决这个问题，通常使用mT5作为主干模型。在本研究中，我们在统一的设置中比较了文献中提出的各种方法，还包括替代性的主干模型，即mBART和NLLB-200。我们首先强调了微调所使用的学习率的重要性，这有助于大大缓解以错误语言生成的问题。然后，我们表明通过细致的学习率调整，对模型进行全面微调作为非常强大的基线，替代方法只带来微小的改进。最后，我们发现mBART与同等大小的mT5表现类似。

    arXiv:2402.12279v1 Announce Type: cross  Abstract: Zero-shot cross-lingual generation implies finetuning of the multilingual pretrained language model on a generation task in one language and then using it to make predictions for this task in other languages. Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model. In this work we compare various approaches proposed from the literature in unified settings, also including alternative backbone models, namely mBART and NLLB-200. We first underline the importance of tuning learning rate used for finetuning, which helps to substantially alleviate the problem of generation in the wrong language. Then, we show that with careful learning rate tuning, the simple full finetuning of the model acts as a very strong baseline and alternative approaches bring only marginal improvements. Finally, we find that mBART performs similarly to mT5 of the same size,
    
[^20]: WorldCoder，一种基于模型的LLM代理：通过编写代码和与环境交互构建世界模型

    WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment

    [https://arxiv.org/abs/2402.12275](https://arxiv.org/abs/2402.12275)

    通过编写代码和与环境交互来构建世界模型的基于模型的LLM代理在样本效率上优于深度RL，并在计算效率上优于ReAct风格的代理。

    

    我们提出了一种基于模型的代理，通过与环境的交互构建代表其对世界知识的Python程序。该世界模型试图解释其交互，同时对自己能够获得的奖励持乐观态度。我们通过扩展LLM的程序合成工作来实现这一点。我们在网格世界上研究了我们的代理，发现我们的方法在样本效率上比深度强化学习更高，并且在计算效率上比ReAct风格的代理更高效。

    arXiv:2402.12275v1 Announce Type: new  Abstract: We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We do this by extending work on program synthesis via LLMs. We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents.
    
[^21]: 论基于蒸馏的联邦学习在拜占庭环境下的弹性

    On the Byzantine-Resilience of Distillation-Based Federated Learning

    [https://arxiv.org/abs/2402.12265](https://arxiv.org/abs/2402.12265)

    基于蒸馏的联邦学习在拜占庭环境下表现出极强的弹性，介绍了两种新的拜占庭攻击，并提出了一种增强拜占庭弹性的新方法。

    

    由于在隐私、非独立同分布数据和通信成本方面的优势，使用知识蒸馏（KD）的联邦学习（FL）算法受到越来越多的关注。本文研究了这些方法在拜占庭环境中的性能，展示了基于KD的FL算法相当具有弹性，并分析了拜占庭客户端如何影响学习过程相对于联邦平均算法。根据这些见解，我们介绍了两种新的拜占庭攻击，并证明它们对先前的拜占庭弹性方法是有效的。此外，我们提出了FilterExp，一种旨在增强拜占庭弹性的新方法。

    arXiv:2402.12265v1 Announce Type: cross  Abstract: Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost. These methods depart from transmitting model parameters and, instead, communicate information about a learning task by sharing predictions on a public dataset. In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process. We show that KD-based FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process compared to Federated Averaging. Based on these insights, we introduce two new byzantine attacks and demonstrate that they are effective against prior byzantine-resilient methods. Additionally, we propose FilterExp, a novel method designed to enhance the byzantine resilien
    
[^22]: 使用LoRA集成在精调LLMs中的不确定性量化

    Uncertainty quantification in fine-tuned LLMs using LoRA ensembles

    [https://arxiv.org/abs/2402.12264](https://arxiv.org/abs/2402.12264)

    使用LoRA集成在精调LLMs中提出了一种原则性不确定性量化方法，通过对不同数据域的低秩适应集成分析，推测了模型对特定架构难以学习的数据领域的信号。

    

    精调大型语言模型可以提高特定任务的性能，尽管对于精调模型学到了什么、遗忘了什么以及如何信任其预测仍然缺乏一个一般的理解。我们提出了使用计算效率高的低秩适应集成对精调LLMs进行基于后验逼近的原则性不确定性量化。我们使用基于Mistral-7b的低秩适应集成分析了三个常见的多项选择数据集，并对其在精调过程中和之后对不同目标领域的感知复杂性和模型效能进行了定量和定性的结论。具体而言，基于数值实验支持，我们对那些对于给定架构难以学习的数据领域的熵不确定性度量提出了假设。

    arXiv:2402.12264v1 Announce Type: cross  Abstract: Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing. We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles. We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning. In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn.
    
[^23]: BEARS 让神经符号模型意识到它们的推理捷径

    BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts

    [https://arxiv.org/abs/2402.12240](https://arxiv.org/abs/2402.12240)

    BEARS是一种集成技术，可以让神经符号模型意识到它们学习的概念的语义模糊性，帮助用户识别和怀疑低质量概念。

    

    Neuro-Symbolic (NeSy)预测器符合符号知识-编码，例如安全约束，可能受到推理捷径（RSs）的影响：它们通过利用非预期的语义来学习与符号知识一致的概念。 RSs损害了可靠性和泛化，并且正如我们在本文中所展示的，它们与NeSy模型对预测概念过于自信有关。不幸的是，唯一可靠的缓解策略需要对概念进行昂贵的密集监督。我们提出的方法不是试图完全避免RSs，而是要确保NeSy模型意识到它们学习的概念的语义模糊性，从而使用户能够识别和怀疑低质量的概念。从三个简单的设计要求开始，我们得出bears（BE Aware of Reasoning Shortcuts），这是一种集成技术，可以校准模型的概念级信心，而不会损害预测准确性。

    arXiv:2402.12240v1 Announce Type: cross  Abstract: Neuro-Symbolic (NeSy) predictors that conform to symbolic knowledge - encoding, e.g., safety constraints - can be affected by Reasoning Shortcuts (RSs): They learn concepts consistent with the symbolic knowledge by exploiting unintended semantics. RSs compromise reliability and generalization and, as we show in this paper, they are linked to NeSy models being overconfident about the predicted concepts. Unfortunately, the only trustworthy mitigation strategy requires collecting costly dense supervision over the concepts. Rather than attempting to avoid RSs altogether, we propose to ensure NeSy models are aware of the semantic ambiguity of the concepts they learn, thus enabling their users to identify and distrust low-quality concepts. Starting from three simple desiderata, we derive bears (BE Aware of Reasoning Shortcuts), an ensembling technique that calibrates the model's concept-level confidence without compromising prediction accura
    
[^24]: 学习在内容审核中推迟：人工智能与人类协同作用

    Learning to Defer in Content Moderation: The Human-AI Interplay

    [https://arxiv.org/abs/2402.12237](https://arxiv.org/abs/2402.12237)

    本文提出了一个模型，捕捉内容审核中人工智能的相互作用。

    

    成功的在线平台内容审核依赖于人工智能协同方法。本文介绍了一个模型，捕捉内容审核中人工智能的相互作用。算法观察到即将发布的帖子的背景信息，做出分类和准入决策，并安排帖子进行人工审核。

    arXiv:2402.12237v1 Announce Type: cross  Abstract: Successful content moderation in online platforms relies on a human-AI collaboration approach. A typical heuristic estimates the expected harmfulness of a post and uses fixed thresholds to decide whether to remove it and whether to send it for human review. This disregards the prediction uncertainty, the time-varying element of human review capacity and post arrivals, and the selective sampling in the dataset (humans only review posts filtered by the admission algorithm).   In this paper, we introduce a model to capture the human-AI interplay in content moderation. The algorithm observes contextual information for incoming posts, makes classification and admission decisions, and schedules posts for human review. Only admitted posts receive human reviews on their harmfulness. These reviews help educate the machine-learning algorithms but are delayed due to congestion in the human review system. The classical learning-theoretic way to ca
    
[^25]: 将 Kernel KMeans 聚类拆分用于端到端无监督决策树

    Kernel KMeans clustering splits for end-to-end unsupervised decision trees

    [https://arxiv.org/abs/2402.12232](https://arxiv.org/abs/2402.12232)

    提出了一种新颖的端到端训练的无监督二叉树用于聚类，称为Kauri，通过贪婪最大化 kernel KMeans 目标来执行，无需定义质心，并在多个数据集上展示其性能优于其他方法。

    

    树是获取对相对较小数据集进行可解释预测的便利模型。虽然有很多关于监督学习中端到端构建这种树的提议，但在没有标签的情况下学习用于聚类的树仍然是一个未解决的挑战。大多数作品主要集中于使用树来解释另一个聚类算法的结果，我们在这里提出了一种新颖的端到端训练的无监督二叉树用于聚类：Kauri。该方法通过贪婪最大化 kernel KMeans 目标来执行，而无需定义质心。我们在多个数据集上将此模型与最近的无监督树进行比较，并展示当使用线性核时，Kauri 的性能相同。对于其他内核，Kauri 在许多情况下表现优于内核 KMeans 和 CART 决策树的串联。

    arXiv:2402.12232v1 Announce Type: cross  Abstract: Trees are convenient models for obtaining explainable predictions on relatively small datasets. Although there are many proposals for the end-to-end construction of such trees in supervised learning, learning a tree end-to-end for clustering without labels remains an open challenge. As most works focus on interpreting with trees the result of another clustering algorithm, we present here a novel end-to-end trained unsupervised binary tree for clustering: Kauri. This method performs a greedy maximisation of the kernel KMeans objective without requiring the definition of centroids. We compare this model on multiple datasets with recent unsupervised trees and show that Kauri performs identically when using a linear kernel. For other kernels, Kauri often outperforms the concatenation of kernel KMeans and a CART decision tree.
    
[^26]: AnyGPT：统一的多模式离散序列建模语言模型

    AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling

    [https://arxiv.org/abs/2402.12226](https://arxiv.org/abs/2402.12226)

    AnyGPT是一个统一的多模态语言模型，通过离散表示实现各种模态的统一处理，能够在不改变大型语言模型架构或训练方式的情况下稳定训练，为新模态的无缝整合提供了可能。

    

    我们介绍了 AnyGPT，这是一个任意多模式语言模型，利用离散表示统一处理各种模态，包括语音、文本、图像和音乐。AnyGPT 可以稳定训练，无需对当前大型语言模型（LLM）架构或训练范式进行任何改动。相反，它仅依赖于数据级预处理，促进了新模态的无缝集成到LLM中，类似于新语言的整合。我们构建了一个多模式文本中心的数据集，用于多模式对齐预训练。利用生成模型，我们合成了第一个大规模任意多模式指令数据集。它包括108k个多轮对话示例，精细地交织各种模态，从而使模型能够处理多模态输入和输出的任意组合。实验结果表明，AnyGPT能够促进...

    arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
    
[^27]: 重新格式化对齐

    Reformatted Alignment

    [https://arxiv.org/abs/2402.12219](https://arxiv.org/abs/2402.12219)

    本文提出了一种名为ReAlign的简单有效方法，通过重新格式化指导数据的响应，显著提升了大型语言模型（LLMs）与人类价值观的对齐能力。

    

    优化微调数据对于将大型语言模型（LLMs）与人类价值观对齐至关重要。当前改善数据质量的方法要么耗时费力，要么容易受到LLM幻觉引起的事实错误影响。本文探讨提升现有指导数据质量以更好地与人类价值观对齐的方法，引入了一种名为ReAlign的简单有效方法，它将指导数据的响应重新格式化为更符合预先建立标准和编译证据的格式。该方法最小化了人类注释、幻觉和扩展困难，与现有对齐技术正交。实验结果表明，ReAlign显著提升了LLMs的整体对齐能力、数学推理、事实性和可读性。令人鼓舞的是，在不引入任何额外数据或先进训练技术的情况下，仅通过重新格式化响应，LLaMA-2-13

    arXiv:2402.12219v1 Announce Type: cross  Abstract: The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.   Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13
    
[^28]: 减轻AIGC版权困境的Copyleft：假设分析、公众认知和影响

    Copyleft for Alleviating AIGC Copyright Dilemma: What-if Analysis, Public Perception and Implications

    [https://arxiv.org/abs/2402.12216](https://arxiv.org/abs/2402.12216)

    本研究探讨了通过采用Copyleft减轻AIGC版权困境的可行性，通过定性和定量研究发现，人们普遍感知到困境并倾向于使用自由授权。

    

    随着人工智能与全球治理（AIGC）在过去几年深刻影响我们的社会，伦理问题受到了极大关注。其中最紧迫的问题是AIGC版权困境，这可能极大地阻碍AIGC的发展并给整个社会带来巨大成本。鉴于AIGC版权治理的复杂性以及当前没有完美解决方案的事实，先前的研究主张在AI治理中采用Copyleft，但没有进行实质性分析。本文进一步探讨了采用Copyleft减轻AIGC版权困境的可行性。我们从两个方面进行了混合方法研究：在定性方面，我们使用了正式的假设分析来澄清困境，并提供案例研究来展示Copyleft的可行性；在定量方面，我们进行了精心设计的调查，以了解公众对AIGC使用Copyleft的看法。关键发现包括：a）人们普遍感知到这一困境，b）他们更倾向于使用自由授权.

    arXiv:2402.12216v1 Announce Type: cross  Abstract: As AIGC has impacted our society profoundly in the past years, ethical issues have received tremendous attention. The most urgent one is the AIGC copyright dilemma, which can immensely stifle the development of AIGC and greatly cost the entire society. Given the complexity of AIGC copyright governance and the fact that no perfect solution currently exists, previous work advocated copyleft on AI governance but without substantive analysis. In this paper, we take a step further to explore the feasibility of copyleft to alleviate the AIGC copyright dilemma. We conduct a mixed-methods study from two aspects: qualitatively, we use a formal what-if analysis to clarify the dilemma and provide case studies to show the feasibility of copyleft; quantitatively, we perform a carefully designed survey to find out how the public feels about copylefting AIGC. The key findings include: a) people generally perceive the dilemma, b) they prefer to use au
    
[^29]: 考虑异质性的跨校选修课推荐：一种混合联邦方法

    Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid Federated Approach

    [https://arxiv.org/abs/2402.12202](https://arxiv.org/abs/2402.12202)

    提出了一种考虑异质性的混合联邦推荐系统，用于解决跨校选修课程推荐问题，通过构建异构图和设计注意机制来捕捉异质性感知表示，并在联邦方案下训练个别学校模型以推荐量身定制的选修课程。

    

    在现代教育时代，解决跨校学习者多样性至关重要，尤其是在个性化推荐系统中为选修课程选择。然而，隐私问题经常限制了跨校数据共享，这限制了现有方法对稀疏数据进行建模和有效处理异质性的能力，最终导致子优化的推荐。为此，我们提出了HFRec，一种考虑异质性的混合联邦推荐系统，旨在为跨校选修课程推荐。该模型为每个学校构建了异构图，将学生之间的各种交互和历史行为融入其中，以整合上下文和内容信息。我们设计了一个注意机制来捕捉异质性感知表示。此外，在一个联邦方案下，我们使用自适应学习设置来训练个别学校模型以推荐量身定制的选修课程。

    arXiv:2402.12202v1 Announce Type: cross  Abstract: In the era of modern education, addressing cross-school learner diversity is crucial, especially in personalized recommender systems for elective course selection. However, privacy concerns often limit cross-school data sharing, which hinders existing methods' ability to model sparse data and address heterogeneity effectively, ultimately leading to suboptimal recommendations. In response, we propose HFRec, a heterogeneity-aware hybrid federated recommender system designed for cross-school elective course recommendations. The proposed model constructs heterogeneous graphs for each school, incorporating various interactions and historical behaviors between students to integrate context and content information. We design an attention mechanism to capture heterogeneity-aware representations. Moreover, under a federated scheme, we train individual school-based models with adaptive learning settings to recommend tailored electives. Our HFRec
    
[^30]: MultiFIX:一种友好的XAI功能诱导方法，用于从多模态数据构建模型

    MultiFIX: An XAI-friendly feature inducing approach to building models from multimodal data

    [https://arxiv.org/abs/2402.12183](https://arxiv.org/abs/2402.12183)

    提出了MultiFIX，一种注重可解释性的多模态数据融合方法，使用深度学习架构训练模型，并通过注意力图来解释每个模态对最终预测的影响

    

    在健康领域，决策常常基于不同的数据模态。因此，在创建预测模型时，能够从不同数据模态中提取和组合相关特征的多模态融合方法非常有益。此外，重要的是要了解每个模态如何影响最终预测，特别是在高风险领域中，以便可以信任和负责任地使用这些模型。我们提出了MultiFIX：一种新的注重可解释性的多模态数据融合管道，明确从不同数据类型中诱导出单独的特征，随后可以将其组合以进行最终预测。采用端到端深度学习架构训练预测模型并提取每个模态的代表性特征，然后使用可解释的人工智能技术解释模型的每个部分。注意力图用于突出图像中的重要区域

    arXiv:2402.12183v1 Announce Type: new  Abstract: In the health domain, decisions are often based on different data modalities. Thus, when creating prediction models, multimodal fusion approaches that can extract and combine relevant features from different data modalities, can be highly beneficial. Furthermore, it is important to understand how each modality impacts the final prediction, especially in high-stake domains, so that these models can be used in a trustworthy and responsible manner. We propose MultiFIX: a new interpretability-focused multimodal data fusion pipeline that explicitly induces separate features from different data types that can subsequently be combined to make a final prediction. An end-to-end deep learning architecture is used to train a predictive model and extract representative features of each modality. Each part of the model is then explained using explainable artificial intelligence techniques. Attention maps are used to highlight important regions in ima
    
[^31]: 重新审视深度强化学习中的数据增强

    Revisiting Data Augmentation in Deep Reinforcement Learning

    [https://arxiv.org/abs/2402.12181](https://arxiv.org/abs/2402.12181)

    重新审视深度强化学习中的数据增强，分析不同方法的影响，提出了如何更加原则地利用数据增强的建议。

    

    最近在基于图像的深度强化学习(DRL)中提出了各种数据增强技术。尽管它们在实证中证明了数据增强对于提高样本效率或泛化的有效性，但并不总是清楚哪种技术应该被优先选择。为了解决这个问题，我们分析了现有方法以更好地理解它们并揭示它们之间的联系。值得注意的是，通过表达这些方法的Q-targets和经验演员/评论家损失的方差，我们可以分析它们不同组成部分的影响并进行比较。我们进一步提出了一个关于如何选择不同的数据增强转换来计算目标Q值可能会影响这些方法的解释。这项分析提出了如何更加原则地利用数据增强的建议。此外，我们还包括了一个称为切线prop的正则化项。

    arXiv:2402.12181v1 Announce Type: cross  Abstract: Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL). Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear. To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected. Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them. We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values. This analysis suggests recommendations on how to exploit data augmentation in a more principled way. In addition, we include a regularization term called tangent prop,
    
[^32]: 检测监控系统：在线考试中检测异常行为

    Examining Monitoring System: Detecting Abnormal Behavior In Online Examinations

    [https://arxiv.org/abs/2402.12179](https://arxiv.org/abs/2402.12179)

    该论文设计的“检测监控系统”能够高准确性和速度地识别在线考试中的作弊行为，帮助监考员做出决策。

    

    在过去的十年中，尤其是在COVID-19大流行期间，在线考试作弊已成为一个普遍问题。为了解决这一学术不端问题，我们设计了“检测在线考试中异常行为的考试监控系统”，旨在帮助监考员识别异常学生行为。我们的系统在实时场景中展示出高准确性和速度，提供宝贵信息，辅助监考员做出决策。本文概述了我们的方法论以及我们的系统在缓解在线考试作弊普遍问题中的有效性。

    arXiv:2402.12179v1 Announce Type: cross  Abstract: Cheating in online exams has become a prevalent issue over the past decade, especially during the COVID-19 pandemic. To address this issue of academic dishonesty, our "Exam Monitoring System: Detecting Abnormal Behavior in Online Examinations" is designed to assist proctors in identifying unusual student behavior. Our system demonstrates high accuracy and speed in detecting cheating in real-time scenarios, providing valuable information, and aiding proctors in decision-making. This article outlines our methodology and the effectiveness of our system in mitigating the widespread problem of cheating in online exams.
    
[^33]: Mafin: 用模型增强微调来增强黑盒嵌入

    Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning

    [https://arxiv.org/abs/2402.12177](https://arxiv.org/abs/2402.12177)

    Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。

    

    检索增强生成（RAG）已经成为缓解大型语言模型（LLMs）中幻觉的有效解决方案。RAG中的检索阶段通常涉及预训练的嵌入模型，将查询和段落转换为向量以捕获它们的语义。然而，当应用于特定领域知识时，标准的预训练嵌入模型可能表现出次优性能，需要进行微调。本文解决了仅能从黑盒模型获取嵌入的情况。我们引入了模型增强微调（Mafin）--一种通过用可训练的嵌入模型增强黑盒嵌入模型来进行微调的新方法。我们的结果表明，Mafin仅需要训练一个小的增强模型就可以显著提高黑盒嵌入的性能。我们验证了我们的方法在有标签和无标签数据集上的有效性。

    arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
    
[^34]: 无监督LLM适应问答任务

    Unsupervised LLM Adaptation for Question Answering

    [https://arxiv.org/abs/2402.12170](https://arxiv.org/abs/2402.12170)

    提出了无监督LLM适应问答任务，通过利用预训练的LLM和目标领域的未标记文档，实现在新领域回答问题的目标。

    

    大型语言模型（LLM）通过自监督训练学习大规模训练数据集中的多样化知识。接着通过指导微调，LLM能够返回多样问题的正确信息。然而，将这些预训练的LLM调整到新的目标领域，如不同组织或时期，用于问答任务会产生很高的注释成本。为解决这一挑战，我们提出了一个新颖的任务，即无监督LLM适应问答任务。在这个任务中，我们利用预训练的LLM、一个公开可用的问答数据集（源数据）和目标域的未标记文档。我们的目标是学习LLM，使其能够回答关于目标领域的问题。我们引入了一个合成数据集和两个真实数据集来评估在源数据和目标数据上微调的模型，并揭示了一些有趣的见解；（i）微调模型展示了提供正确答案的能力

    arXiv:2402.12170v1 Announce Type: cross  Abstract: Large language models (LLM) learn diverse knowledge present in the large-scale training dataset via self-supervised training. Followed by instruction-tuning, LLM acquires the ability to return correct information for diverse questions. However, adapting these pre-trained LLMs to new target domains, such as different organizations or periods, for the question-answering (QA) task incurs a substantial annotation cost. To tackle this challenge, we propose a novel task, unsupervised LLM adaptation for question answering. In this task, we leverage a pre-trained LLM, a publicly available QA dataset (source data), and unlabeled documents from the target domain. Our goal is to learn LLM that can answer questions about the target domain. We introduce one synthetic and two real datasets to evaluate models fine-tuned on the source and target data, and reveal intriguing insights; (i) fine-tuned models exhibit the ability to provide correct answers 
    
[^35]: 针对参数高效微调的权重投毒后门攻击的防御

    Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning

    [https://arxiv.org/abs/2402.12168](https://arxiv.org/abs/2402.12168)

    PEFT相对于全参数微调更容易受到权重投毒后门攻击的影响，提出了一个通过置信度识别受污染样本的毒化样本识别模块（PSIM），为权重投毒后门攻击提供稳健防御

    

    最近，针对语言模型应用提出并成功实施了各种参数高效微调（PEFT）策略。然而，这引发了一个问题，即当面对权重投毒后门攻击时，仅更新有限模型参数的PEFT是否构成安全漏洞。我们展示了PEFT相对于全参数微调方法更容易受到权重投毒后门攻击的影响，预定义的触发器仍然易受利用，预定义的目标在微调后依然保持高置信度。受到这一见解的启发，我们开发了一个利用PEFT的毒化样本识别模块（PSIM），通过置信度识别受污染样本，提供针对权重投毒后门攻击的稳健防御。具体而言，我们利用PEFT训练PSIM，带有随机重置样本标签。在推断过程中，

    arXiv:2402.12168v1 Announce Type: cross  Abstract: Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference pr
    
[^36]: 赋予预训练图模型具有可证明的公平性

    Endowing Pre-trained Graph Models with Provable Fairness

    [https://arxiv.org/abs/2402.12161](https://arxiv.org/abs/2402.12161)

    提出了一种新的适配器调优框架，赋予预训练图模型具有可证明的公平性

    

    预训练图模型（PGMs）旨在捕捉可转移的固有结构属性，并将其应用于不同的下游任务。类似于预训练语言模型，PGMs也会继承人类社会中的偏见，导致在下游应用中出现歧视行为。现有公平方法的去偏见过程通常与GNNs的参数优化相结合。然而，不同的下游任务在现实中可能与不同的敏感属性相关联，直接采用现有方法改善PGMs的公平性是不灵活且低效的。此外，大多数方法缺乏理论保证，即对模型预测公平性的可证明下限，这直接提供了实际场景下的保证。为了克服这些限制，我们提出了一种新的适配器调优框架，赋予预训练\textbf{图}模型具有\textbf{可证明}的\textbf{公}平\textbf{性}（称为

    arXiv:2402.12161v1 Announce Type: cross  Abstract: Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks. Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications. The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs. However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient. Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario. To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained \textbf{Graph} models with \textbf{P}rovable f\textbf{A}i\textbf{R}ness (called
    
[^37]: 基于Transformer的因果语言模型执行聚类

    Transformer-based Causal Language Models Perform Clustering

    [https://arxiv.org/abs/2402.12151](https://arxiv.org/abs/2402.12151)

    Transformer-based因果语言模型通过在隐藏空间内对数据进行聚类来学习任务特定信息，这种聚类过程在学习中动态演变，并有助于处理未见实例。

    

    即使大型语言模型(LLMs)已经展示出在解决各种自然语言任务方面的出色能力，LLM遵循人类指令的能力仍然是一个问题。最近的研究通过额外训练指令遵循任务已经显示出很大改进，然而，导致有效指令遵循能力的机制仍未得到充分理解。本文介绍了一个简化的指令遵循任务，并使用合成数据集分析了基于Transformer的因果语言模型。我们的发现表明，模型通过在其隐藏空间内对数据进行聚类而学习任务特定信息，这种聚类过程在学习过程中动态演变。我们还演示了这种现象如何帮助模型处理未见实例，并在更现实的环境中验证了我们的结果。

    arXiv:2402.12151v1 Announce Type: cross  Abstract: Even though large language models (LLMs) have demonstrated remarkable capability in solving various natural language tasks, the capability of an LLM to follow human instructions is still a concern. Recent works have shown great improvements in the instruction-following capability via additional training for instruction-following tasks. However, the mechanisms responsible for effective instruction-following capabilities remain inadequately understood. Here, we introduce a simplified instruction-following task and use synthetic datasets to analyze a Transformer-based causal language model. Our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning. We also demonstrate how this phenomenon assists the model in handling unseen instances and validate our results in a more realistic setting.
    
[^38]: 您的大型语言模型暗中支持公平，您应该像对待一个公平者那样提示它

    Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One

    [https://arxiv.org/abs/2402.12150](https://arxiv.org/abs/2402.12150)

    提出了一种通过提示大型语言模型（LLMs）具有特定角色以表达多样观点的方法，并开发了FairThinking流水线，以实现公平表达。

    

    大规模语言模型（LLMs）的广泛应用凸显了确保其公平性的迫切需要。然而，LLMs经常展示支配性观点，同时忽视来自少数派的替代观点，可能导致潜在偏见。我们假设这些违反公平的行为发生是因为LLMs使用代表训练数据大多数的人类个性来表达他们的观点。作为回应，我们验证提示LLMs使用特定角色可以使LLMs表达多样观点。基于这一洞察和观察，我们开发了FairThinking，这是一个旨在自动生成能让LLMs表达多样观点以实现公平表达的流水线。为了评估FairThinking，我们创建了一个包含三个与公平相关主题的一千个项目的数据集，并在GPT-3.5，GPT-4，Llama2和Mistral上进行实验，以展示其优越性能。

    arXiv:2402.12150v1 Announce Type: cross  Abstract: The widespread adoption of large language models (LLMs) underscores the urgent need to ensure their fairness. However, LLMs frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases. We hypothesize that these fairness-violating behaviors occur because LLMs express their viewpoints using a human personality that represents the majority of training data. In response to this, we validate that prompting LLMs with specific roles can allow LLMs to express diverse viewpoints. Building on this insight and observation, we develop FairThinking, a pipeline designed to automatically generate roles that enable LLMs to articulate diverse perspectives for fair expressions. To evaluate FairThinking, we create a dataset with a thousand items covering three fairness-related topics and conduct experiments on GPT-3.5, GPT-4, Llama2, and Mistral to demonstrate its superior performanc
    
[^39]: 跨语言规模的端到端事实核查

    End-to-end multilingual fact-checking at scale

    [https://arxiv.org/abs/2402.12147](https://arxiv.org/abs/2402.12147)

    使用Factiverse AI模型，可以进行跨语言的端到端事实核查，并且通过实验证明，为事实核查任务进行微调的模型优于大型语言模型。

    

    在本文中，我们描述了如何使用Factiverse AI模型在100多种语言中进行端到端事实核查。我们还通过实验性基准测试展示，为事实核查任务进行微调的模型胜过GPT-4、GPT-3.5-Turbo和Mistral-7b等大型语言模型。

    arXiv:2402.12147v1 Announce Type: cross  Abstract: In this article, we describe how you can perform end-to-end fact-checking in over 100 languages using Factiverse AI models. We also show through an experimental benchmark that fine-tuned models tailored for fact-checking tasks outperform Large Language Models such as GPT-4, GPT-3.5-Turbo, and Mistral-7b.
    
[^40]: Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement

    Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement

    [https://arxiv.org/abs/2402.12146](https://arxiv.org/abs/2402.12146)

    Meta Ranking方法通过比较目标查询-响应对与参考查询-响应对来使较不具备能力的语言模型有效地评估单个响应的可靠性。

    

    尽管大型语言模型（LLMs）在广泛任务中展现强大性能，但仍面临幻觉等可靠性挑战。先前的研究表明，像GPT-4这样高能力的LLMs在评估单个响应的可靠性方面是有效的，而较不具备能力的LLMs通常被调整来评估对相同查询的响应的相对可靠性。为了使较不具备能力的LLMs有效地评估单个响应的可靠性，我们提出了一种名为$\textit{Meta}$ $\textit{Ranking}$（MR）的新方法。与先前直接评估响应的方法不同，我们通过将目标查询-响应对与参考查询-响应对进行比较来实现判断。我们发现在推理任务的LLM响应的错误检测中，MR表现出显著的有效性，即使在没有微调的情况下，较不具备能力的LLMs也能胜过强基线。我们进一步证明MR可以被用

    arXiv:2402.12146v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be use
    
[^41]: SSTKG：简单的可解释和多才多艺的动态信息嵌入的时空知识图

    SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding

    [https://arxiv.org/abs/2402.12132](https://arxiv.org/abs/2402.12132)

    该论文提出了一个名为SSTKG的新框架，用于构建和探索时空知识图，以将空间和时间数据整合到知识图中，提供了用于未来时序预测和空间信息推荐的输出嵌入。

    

    知识图谱（KGs）越来越多地用于使用真实世界数据集进行链接预测和推荐。然而，目前大多数方法依赖于静态数据，忽视了真实场景的动态性和隐藏的时空属性。这经常导致次优的预测和推荐。尽管有有效的时空推理方法，但它们面临挑战，例如处理大型数据集时的可扩展性和不充分的语义理解，这会影响它们的性能。为了解决这些限制，本文介绍了一个新的框架 - 简单的时空知识图（SSTKG），用于构建和探索时空知识图。

    arXiv:2402.12132v1 Announce Type: new  Abstract: Knowledge graphs (KGs) have been increasingly employed for link prediction and recommendation using real-world datasets. However, the majority of current methods rely on static data, neglecting the dynamic nature and the hidden spatio-temporal attributes of real-world scenarios. This often results in suboptimal predictions and recommendations. Although there are effective spatio-temporal inference methods, they face challenges such as scalability with large datasets and inadequate semantic understanding, which impede their performance. To address these limitations, this paper introduces a novel framework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing and exploring spatio-temporal KGs. To integrate spatial and temporal data into KGs, our framework exploited through a new 3-step embedding method. Output embeddings can be used for future temporal sequence prediction and spatial information recommendation, providing valua
    
[^42]: 评估视觉语言模型的图像评价能力

    Evaluating Image Review Ability of Vision Language Models

    [https://arxiv.org/abs/2402.12121](https://arxiv.org/abs/2402.12121)

    本论文通过引入基于排名相关分析的评估方法，探讨了大规模视觉语言模型（LVLM）在生成图像评价文本方面的能力，并创建了一个评估数据集来验证这种方法。

    

    大规模视觉语言模型（LVLM）是能够通过单个模型处理图像和文本输入的语言模型。本文探讨了使用LVLM生成图像评价文本的方法。LVLM对图像的评价能力尚未完全被理解，突显了对其评价能力进行系统评估的必要性。与图像标题不同，评价文本可以从图像构图和曝光等不同视角撰写。这种评价角度的多样性使得难以唯一确定图像的正确评价。为了解决这一挑战，我们提出了一种基于排名相关分析的评估方法，通过人类和LVLM对评价文本进行排名，然后测量这些排名之间的相关性。我们进一步通过创建一个旨在评估最新LVLM图像评价能力的基准数据集来验证这种方法。

    arXiv:2402.12121v1 Announce Type: cross  Abstract: Large-scale vision language models (LVLMs) are language models that are capable of processing images and text inputs by a single model. This paper explores the use of LVLMs to generate review texts for images. The ability of LVLMs to review images is not fully understood, highlighting the need for a methodical evaluation of their review abilities. Unlike image captions, review texts can be written from various perspectives such as image composition and exposure. This diversity of review perspectives makes it difficult to uniquely determine a single correct review for an image. To address this challenge, we introduce an evaluation method based on rank correlation analysis, in which review texts are ranked by humans and LVLMs, then, measures the correlation between these rankings. We further validate this approach by creating a benchmark dataset aimed at assessing the image review ability of recent LVLMs. Our experiments with the dataset
    
[^43]: DualView：双重视角下的数据归因

    DualView: Data Attribution from the Dual Perspective

    [https://arxiv.org/abs/2402.12118](https://arxiv.org/abs/2402.12118)

    提出了DualView，一种基于替代建模的后期数据归因方法，具有高效计算和优质评估结果。

    

    本文提出了DualView，这是一种基于替代建模的后期数据归因方法，展示了高计算效率和良好的评估结果。我们专注于神经网络，在与文献相关的适当定量评估策略下评估了我们提出的技术，比较了与相关主要本地数据归因方法的性能。

    arXiv:2402.12118v1 Announce Type: cross  Abstract: Local data attribution (or influence estimation) techniques aim at estimating the impact that individual data points seen during training have on particular predictions of an already trained Machine Learning model during test time. Previous methods either do not perform well consistently across different evaluation criteria from literature, are characterized by a high computational demand, or suffer from both. In this work we present DualView, a novel method for post-hoc data attribution based on surrogate modelling, demonstrating both high computational efficiency, as well as good evaluation results. With a focus on neural networks, we evaluate our proposed technique using suitable quantitative evaluation strategies from the literature against related principal local data attribution methods. We find that DualView requires considerably lower computational resources than other methods, while demonstrating comparable performance to comp
    
[^44]: 在预训练过程中移除异常值是否有其益处？

    Is It a Free Lunch for Removing Outliers during Pretraining?

    [https://arxiv.org/abs/2402.12102](https://arxiv.org/abs/2402.12102)

    通过确保归一化对序列长度不变，我们改进了一种预训练方法，使其在移除异常值的同时促进了因果语言模型的成功预训练。

    

    随着大型语言模型的规模不断增长，量化的作用变得越来越重要。然而，在权重或激活中存在的异常值明显影响了量化模型的性能。最近，qtransformer 提出了一种旨在以无异常值方式预训练模型的新型 softmax 函数，从而提高了它们适用于量化的性能。有趣的是，我们观察到这种方法导致了全精度性能的下降。基于这一观察，我们通过确保其归一化对序列长度不变来增强该方法，这是在预训练和微调之间弥合差距的关键因素。此外，这种改进的方法还促进了因果语言模型的成功预训练。

    arXiv:2402.12102v1 Announce Type: cross  Abstract: With the growing size of large language models, the role of quantization becomes increasingly significant. However, outliers present in weights or activations notably influence the performance of quantized models. Recently, \citet{qtransformer} introduced a novel softmax function aimed at pretraining models in an outlier-free manner, thereby enhancing their suitability for quantization. Interestingly, we observed that such an approach leads to performance degradation in full precision. Building on this insight, we enhance the method by ensuring its normalization is invariant to sequence length, a crucial factor for bridging the gap between pretraining and fine-tuning. Moreover, this improved method also facilitates successful pretraining of causal language models.
    
[^45]: Groot: 使用基于树的语义转换对生成式文本到图像模型进行对抗测试

    Groot: Adversarial Testing for Generative Text-to-Image Models with Tree-based Semantic Transformation

    [https://arxiv.org/abs/2402.12100](https://arxiv.org/abs/2402.12100)

    Groot是第一个利用基于树的语义转换进行对抗测试文本到图像模型的自动化框架，成功率高达93.66%。

    

    随着文本到图像生成模型的普及，其安全性变得至关重要。对抗性测试技术已被开发用于探测这类模型是否能被激发产生不安全内容。然而，现有的解决方案面临着低成功率和低效率等挑战。我们引入了 Groot，这是第一个利用基于树的语义转换进行文本到图像模型对抗测试的自动化框架。Groot 结合语义分解和敏感元素淹没策略，结合 LLMs 来系统地优化对抗性提示。我们的全面评估验证了 Groot 的有效性，它不仅超越了当前最先进方法的性能，而且在领先的文本到图像模型，如 DALL-E 3 和 Midjourney 上取得了显著的成功率（93.66%）。

    arXiv:2402.12100v1 Announce Type: cross  Abstract: With the prevalence of text-to-image generative models, their safety becomes a critical concern. adversarial testing techniques have been developed to probe whether such models can be prompted to produce Not-Safe-For-Work (NSFW) content. However, existing solutions face several challenges, including low success rate and inefficiency. We introduce Groot, the first automated framework leveraging tree-based semantic transformation for adversarial testing of text-to-image models. Groot employs semantic decomposition and sensitive element drowning strategies in conjunction with LLMs to systematically refine adversarial prompts. Our comprehensive evaluation confirms the efficacy of Groot, which not only exceeds the performance of current state-of-the-art approaches but also achieves a remarkable success rate (93.66%) on leading text-to-image models such as DALL-E 3 and Midjourney.
    
[^46]: 通过基于梯度的目标定位实现可解释的LiDAR点云语义分割

    Towards Explainable LiDAR Point Cloud Semantic Segmentation via Gradient Based Target Localization

    [https://arxiv.org/abs/2402.12098](https://arxiv.org/abs/2402.12098)

    通过pGS-CAM方法，本研究提出了一种基于梯度的目标定位方法，可以有效地生成LiDAR点云语义分割模型中每个点的贡献，帮助我们更好地理解模型预测过程并识别改进的潜在区域。

    

    LiDAR点云的语义分割对城市规划和自动驾驶等许多应用至关重要。尽管在解释图像的语义分割预测方面取得了很大进展，但解释点云语义分割预测仍然是一个挑战。本文介绍了一种名为pGS-CAM的新型基于梯度的方法，用于在神经网络激活层中生成显著性图。受Grad-CAM启发，该方法利用梯度突出显示局部重要性，在多个数据集（如SemanticKITTI、Paris-Lille3D、DALES）和3D深度学习架构（如KPConv、RandLANet）上表现出稳健和有效性。我们的实验证明，pGS-CAM通过突出显示每个点的贡献，有效地强调了SS架构中的中间激活中的特征学习。这使我们能够更好地理解SS模型是如何进行预测的并确定潜在的改进方向。

    arXiv:2402.12098v1 Announce Type: cross  Abstract: Semantic Segmentation (SS) of LiDAR point clouds is essential for many applications, such as urban planning and autonomous driving. While much progress has been made in interpreting SS predictions for images, interpreting point cloud SS predictions remains a challenge. This paper introduces pGS-CAM, a novel gradient-based method for generating saliency maps in neural network activation layers. Inspired by Grad-CAM, which uses gradients to highlight local importance, pGS-CAM is robust and effective on a variety of datasets (SemanticKITTI, Paris-Lille3D, DALES) and 3D deep learning architectures (KPConv, RandLANet). Our experiments show that pGS-CAM effectively accentuates the feature learning in intermediate activations of SS architectures by highlighting the contribution of each point. This allows us to better understand how SS models make their predictions and identify potential areas for improvement. Relevant codes are available at h
    
[^47]: 大型语言模型真正理解逻辑还是仅仅模仿语境？

    Do Large Language Models Understand Logic or Just Mimick Context?

    [https://arxiv.org/abs/2402.12091](https://arxiv.org/abs/2402.12091)

    大型语言模型在逻辑推理中并不真正理解逻辑规则，而是通过语境学习增强了模型到达结论的可能性

    

    在过去几年中，大型语言模型（LLMs）的能力受到了广泛关注，它们在复杂场景（如逻辑推理和符号推理）中表现出色。其中一个重要因素是在语境学习和少样本提示的益处。然而，使用语境推理的这种模型成功背后的原因尚未被充分探讨。LLMs是否了解逻辑规则以进行推理，还是通过学习一种概率映射通过语境“猜测”答案？本文通过使用反事实方法在两个逻辑推理数据集上研究了LLMs的推理能力，替换语境文本并修改逻辑概念。根据我们的分析，发现LLMs并不真正理解逻辑规则；相反，在语境学习简单增强了这些模型到达

    arXiv:2402.12091v1 Announce Type: cross  Abstract: Over the past few years, the abilities of large language models (LLMs) have received extensive attention, which have performed exceptionally well in complicated scenarios such as logical reasoning and symbolic inference. A significant factor contributing to this progress is the benefit of in-context learning and few-shot prompting. However, the reasons behind the success of such models using contextual reasoning have not been fully explored. Do LLMs have understand logical rules to draw inferences, or do they ``guess'' the answers by learning a type of probabilistic mapping through context? This paper investigates the reasoning capabilities of LLMs on two logical reasoning datasets by using counterfactual methods to replace context text and modify logical concepts. Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving a
    
[^48]: HIP网络：用于时间知识图外推推理的历史信息传递网络

    HIP Network: Historical Information Passing Network for Extrapolation Reasoning on Temporal Knowledge Graph

    [https://arxiv.org/abs/2402.12074](https://arxiv.org/abs/2402.12074)

    该论文提出了HIP网络，通过从时间性、结构性和重复性角度传递信息，综合考虑了时间变化背后的潜在模式，更新表示并准确预测未来事件。

    

    近年来，时间知识图（TKG）推理受到了广泛关注。大多数现有方法假设在训练期间所有时间戳和相应的图形都是可用的，这使得难以预测未来事件。为了解决这个问题，最近的研究学习根据历史信息推断未来事件。然而，这些方法并未全面考虑时间变化背后的潜在模式，以选择性地传递历史信息，适当更新表示并准确预测事件。在本文中，我们提出了历史信息传递（HIP）网络来预测未来事件。HIP网络从时间、结构和重复的角度传递信息，用于模拟事件的时间演变、同一时间步中事件的相互作用以及已知事件。特别地，我们的方法考虑了关系的更新。

    arXiv:2402.12074v1 Announce Type: new  Abstract: In recent years, temporal knowledge graph (TKG) reasoning has received significant attention. Most existing methods assume that all timestamps and corresponding graphs are available during training, which makes it difficult to predict future events. To address this issue, recent works learn to infer future events based on historical information. However, these methods do not comprehensively consider the latent patterns behind temporal changes, to pass historical information selectively, update representations appropriately and predict events accurately. In this paper, we propose the Historical Information Passing (HIP) network to predict future events. HIP network passes information from temporal, structural and repetitive perspectives, which are used to model the temporal evolution of events, the interactions of events at the same time step, and the known events respectively. In particular, our method considers the updating of relation 
    
[^49]: EmoBench: 评估大型语言模型的情感智能

    EmoBench: Evaluating the Emotional Intelligence of Large Language Models

    [https://arxiv.org/abs/2402.12071](https://arxiv.org/abs/2402.12071)

    EmoBench是一个基于心理学理论的基准测试，旨在评估大型语言模型的情感智能，包括情感理解和情感应用。

    

    近年来，大型语言模型（LLMs）的快速发展凸显了需要稳健、全面和具有挑战性的基准测试的重要性。然而，对它们的情感智能（EI）进行评估的研究相当有限。现有的基准测试存在两个主要缺点：首先，它们主要关注情感识别，忽视了情感调节等重要的情感智能能力，而情感理解则促进情感; 其次，它们主要基于现有数据集构建，这些数据集包含频繁模式、明确信息和注释错误，导致评估不可靠。我们提出了EmoBench，这是一个基准测试，借鉴了已建立的心理理论，并为机器EI提出了综合定义，包括情感理解和情感应用。EmoBench包括一组400个用英语和中文手工制作的问题，经过精心设计，需要深入推理。

    arXiv:2402.12071v1 Announce Type: cross  Abstract: Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion regulation and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning
    
[^50]: WKVQuant：量化大型语言模型的参数权重和键值缓存以提高性能

    WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More

    [https://arxiv.org/abs/2402.12065](https://arxiv.org/abs/2402.12065)

    该论文提出了WKVQuant，一种专为大型语言模型设计的量化框架，通过量化权重和键值缓存来改善性能。

    

    大型语言模型（LLMs）面临着部署挑战，主要是由于其巨大的内存需求和自回归文本生成过程的计算需求。本文通过关注LLMs的量化来解决这些挑战，量化是一种通过将模型参数和激活转换为低比特整数来减少内存消耗的技术。我们批判性地分析了现有的量化方法，识别出它们在平衡量化LLMs的准确性和效率方面的局限性。为了超越这些局限性，我们提出了WKVQuant，这是一个专为量化LLMs的参数权重和键值（KV）缓存而设计的PTQ框架。具体而言，我们引入了仅考虑过去的量化以改善注意力计算。此外，我们还介绍了二维量化策略来处理KV缓存的分布，以及一种跨块重建正则化方法以帮助模型压缩。

    arXiv:2402.12065v1 Announce Type: cross  Abstract: Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for pa
    
[^51]: 因果平等保护与算法公平性

    Causal Equal Protection as Algorithmic Fairness

    [https://arxiv.org/abs/2402.12062](https://arxiv.org/abs/2402.12062)

    本文提出了一种新的算法公平性原则——平等保护，其关键在于将错误分类的风险均等化，避免了许多对传统分类平等原则的反例。

    

    过去十年，计算机科学和哲学的文献形成了不同的算法公平性标准。其中最受争议的分类平等要求，预测算法的错误分类在被保护特征所指示的群体中以相等频率发生。尽管分类平等具有直观吸引力，但已受到攻击。我们转向一个相关原则，即平等保护，该原则最初是在刑事司法领域发展起来的。平等保护的关键在于将错误分类的风险（将在规定的意义上具体说明）进行均等化，而不是将错误分类的比率均等化。我们展示了平等保护避免了许多对分类平等的反例。

    arXiv:2402.12062v1 Announce Type: cross  Abstract: Over the last ten years the literature in computer science and philosophy has formulated different criteria of algorithmic fairness. One of the most discussed, classification parity, requires that the erroneous classifications of a predictive algorithm occur with equal frequency for groups picked out by protected characteristics. Despite its intuitive appeal, classification parity has come under attack. Multiple scenarios can be imagined in which - intuitively - a predictive algorithm does not treat any individual unfairly, and yet classification parity is violated. To make progress, we turn to a related principle, equal protection, originally developed in the context of criminal justice. Key to equal protection is equalizing the risks of erroneous classifications (in a sense to be specified) as opposed to equalizing the rates of erroneous classifications. We show that equal protection avoids many of the counterexamples to classificati
    
[^52]: 所有语言模型的大小都一样吗？

    All Language Models Large and Small

    [https://arxiv.org/abs/2402.12061](https://arxiv.org/abs/2402.12061)

    LONDI框架可以在需要复杂决策和推理的地方选择性地使用大的语言模型，极大地降低了资源消耗。

    

    许多领先的语言模型（LMs）在训练和执行过程中使用高强度计算资源，这对于降低部署资源成本和更快执行决策任务等方面提出了挑战。我们引入了一种名为语言优化网络分布（LONDI）框架的新型即插即用LM框架。 LONDI学会了在需要进行复杂决策和推理的地方选择性地使用大的LM，而在其他地方使用低资源的LM。 LONDI由两个（离线）策略网络系统、一个LM、一个大的LM（LLM)和一个使用开关控制快速学习何时调用LLM的强化学习模块组成。 然后，我们介绍了一种在LLM调用和资源使用方面保持预算约束的LONDI变体。 从理论上讲，我们证明了LONDI学习激活所需解决任务的LLM的系统状态子集。

    arXiv:2402.12061v1 Announce Type: cross  Abstract: Many leading language models (LMs) use high-intensity computational resources both during training and execution. This poses the challenge of lowering resource costs for deployment and faster execution of decision-making tasks among others. We introduce a novel plug-and-play LM framework named Language Optimising Network Distribution (LONDI) framework. LONDI learns to selectively employ large LMs only where complex decision-making and reasoning are required while using low-resource LMs everywhere else. LONDI consists of a system of two (off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning module that uses switching controls to quickly learn which system states to call the LLM. We then introduce a variant of LONDI that maintains budget constraints on LLM calls and hence its resource usage. Theoretically, we prove LONDI learns the subset of system states to activate the LLM required to solve the task. We then prove
    
[^53]: 具有多对数极小极小遗憾的线性赌博机

    Linear bandits with polylogarithmic minimax regret

    [https://arxiv.org/abs/2402.12042](https://arxiv.org/abs/2402.12042)

    该研究提出了一种新的线性赌博机算法，解决了线性随机赌博机中最小极小遗憾的多对数缩放问题，通过加权最小二乘估计实现对设计矩阵特征值关系的控制，实现了累积遗憾的对数缩放。

    

    我们研究了一种线性随机赌博机的噪声模型，对于该模型，当我们选择越来越接近未知向量的单位球上的动作时，亚高斯噪声参数以线性方式消失。我们针对这个问题引入了一种算法，其在时间长度$T$的情况下呈对数$^3（T）$的最小遗憾缩放，与典型赌博机算法的平方根遗憾缩放形成鲜明对比。我们的策略基于加权最小二乘估计，通过几何论证实现了设计矩阵$V_t$在每个时间步骤$t$处的特征值关系$\lambda_{\min} ( V_t ) = \Omega (\sqrt{\lambda_{\max}(V_t ) })$，这些几何论证与噪声模型无关，并可能具有独立的兴趣。这使我们能够严格控制每个时间步骤的期望遗憾为$O(\frac1{t})$的数量级，从而导致累积遗憾的对数缩放。

    arXiv:2402.12042v1 Announce Type: cross  Abstract: We study a noise model for linear stochastic bandits for which the subgaussian noise parameter vanishes linearly as we select actions on the unit sphere closer and closer to the unknown vector. We introduce an algorithm for this problem that exhibits a minimax regret scaling as $\log^3(T)$ in the time horizon $T$, in stark contrast the square root scaling of this regret for typical bandit algorithms. Our strategy, based on weighted least-squares estimation, achieves the eigenvalue relation $\lambda_{\min} ( V_t ) = \Omega (\sqrt{\lambda_{\max}(V_t ) })$ for the design matrix $V_t$ at each time step $t$ through geometrical arguments that are independent of the noise model and might be of independent interest. This allows us to tightly control the expected regret in each time step to be of the order $O(\frac1{t})$, leading to the logarithmic scaling of the cumulative regret.
    
[^54]: 时间序列的增量式学习: 基准和评估

    Class-incremental Learning for Time Series: Benchmark and Evaluation

    [https://arxiv.org/abs/2402.12035](https://arxiv.org/abs/2402.12035)

    时间序列增量学习问题在图像和语言领域取得了进展，但在时间序列数据方面仍然相对较少研究，本文提出了一个全面的评估和基准测试方法。

    

    现实环境本质上是非平稳的，经常会随时间引入新的类别。这在时间序列分类中尤为常见，比如在医疗保健领域出现新的疾病分类，或者在人类活动识别中添加新的活动。在这种情况下，需要一个学习系统能够有效地吸收新的类别，同时避免对旧类别的灾难性遗忘，这就引发了增量式学习问题。然而，尽管在图像和语言领域取得了令人鼓舞的进展，但针对时间序列数据的增量式学习仍然相对较少研究。现有研究存在实验设计不一致的问题，需要对方法在各种数据集上进行全面评估和基准测试。为此，我们首先概述了时间序列增量学习（TSCIL）问题，突出了其独特挑战，并覆盖了...

    arXiv:2402.12035v1 Announce Type: cross  Abstract: Real-world environments are inherently non-stationary, frequently introducing new classes over time. This is especially common in time series classification, such as the emergence of new disease classification in healthcare or the addition of new activities in human activity recognition. In such cases, a learning system is required to assimilate novel classes effectively while avoiding catastrophic forgetting of the old ones, which gives rise to the Class-incremental Learning (CIL) problem. However, despite the encouraging progress in the image and language domains, CIL for time series data remains relatively understudied. Existing studies suffer from inconsistent experimental designs, necessitating a comprehensive evaluation and benchmarking of methods across a wide range of datasets. To this end, we first present an overview of the Time Series Class-incremental Learning (TSCIL) problem, highlight its unique challenges, and cover the 
    
[^55]: 从后门毒化数据集中通过降频空间获取清洁语言模型

    Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space

    [https://arxiv.org/abs/2402.12026](https://arxiv.org/abs/2402.12026)

    通过对频率空间的分析，本文提出了一种多尺度低秩适应（MuScleLoRA）方法，用于解决在训练语言模型时受到后门攻击的问题。

    

    尽管语言模型（LMs）在各种自然语言处理（NLP）任务中取得了显著成功，但LMs的可靠性容易受到后门攻击的影响。先前的研究尝试在毒化数据集上训练LMs时减轻后门学习，但在现实场景中抵御复杂的后门攻击时仍然面临困难。在本文中，我们通过傅里叶分析研究了频率空间中后门LMs的学习机制。我们的发现表明，毒化数据集上呈现的后门映射相比清洁映射更倾向于较低频率，导致后门映射更快地收敛。为了解决这一困境，我们提出了多尺度低秩适应（MuScleLoRA），它在频率空间中部署多个径向缩放，低秩适应目标模型，并在更新参数时进一步调整梯度。通过降频

    arXiv:2402.12026v1 Announce Type: cross  Abstract: Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscal
    
[^56]: 基于Chain of Thought评估ChatGPT的智能合约审计能力

    Evaluation of ChatGPT's Smart Contract Auditing Capabilities Based on Chain of Thought

    [https://arxiv.org/abs/2402.12023](https://arxiv.org/abs/2402.12023)

    本研究评估了使用GPT-4模型增强智能合约安全审计能力的潜力，通过比较其在识别常见漏洞、代码解析和漏洞捕获方面的表现，展示了其在提升智能合约安全性方面的价值。

    

    智能合约作为区块链技术的关键组成部分，在确保交易自动化和遵守协议规则方面发挥着至关重要的作用。然而，智能合约容易受到安全漏洞的影响，一旦被利用，就可能导致重大资产损失。本研究探讨了使用GPT-4模型增强智能合约安全审计的潜力。我们利用了来自SolidiFI-benchmark漏洞库的35个智能合约数据集，其中包含732个漏洞，并将其与其他五种漏洞检测工具进行比较，以评估GPT-4识别七种常见漏洞的能力。此外，我们通过基于八组智能合约审计报告的CoT (Chain of Thought)提示模拟专业审计人员的审计过程，评估了GPT-4在代码解析和漏洞捕获方面的表现。我们还评估了GPT-4编写Solidity Pr的能力。

    arXiv:2402.12023v1 Announce Type: cross  Abstract: Smart contracts, as a key component of blockchain technology, play a crucial role in ensuring the automation of transactions and adherence to protocol rules. However, smart contracts are susceptible to security vulnerabilities, which, if exploited, can lead to significant asset losses. This study explores the potential of enhancing smart contract security audits using the GPT-4 model. We utilized a dataset of 35 smart contracts from the SolidiFI-benchmark vulnerability library, containing 732 vulnerabilities, and compared it with five other vulnerability detection tools to evaluate GPT-4's ability to identify seven common types of vulnerabilities. Moreover, we assessed GPT-4's performance in code parsing and vulnerability capture by simulating a professional auditor's auditing process using CoT(Chain of Thought) prompts based on the audit reports of eight groups of smart contracts. We also evaluated GPT-4's ability to write Solidity Pr
    
[^57]: 使用精英样本训练绿色人工智能模型

    Training Green AI Models Using Elite Samples

    [https://arxiv.org/abs/2402.12010](https://arxiv.org/abs/2402.12010)

    该论文提出了一个基于进化的采样框架，旨在识别精英训练样本，比较与传统方法的模型性能和能效优势，探讨其对可持续模型训练的可能性。

    

    AI模型训练量的大幅增加具有重要的环境影响，这需要更节能高效和可持续的人工智能实践。数据中心方法展现出训练节能人工智能模型的巨大潜力，而实例选择方法展示了使用最小化训练集训练人工智能模型的能力且性能下降可以忽略不计。本文提出了一个基于进化的采样框架，旨在识别针对数据集和模型的精英训练样本，比较模型性能和节能效益与典型模型训练实践的差异，并研究这一框架对促进可持续模型训练实践的可行性。

    arXiv:2402.12010v1 Announce Type: cross  Abstract: The substantial increase in AI model training has considerable environmental implications, mandating more energy-efficient and sustainable AI practices. On the one hand, data-centric approaches show great potential towards training energy-efficient AI models. On the other hand, instance selection methods demonstrate the capability of training AI models with minimised training sets and negligible performance degradation. Despite the growing interest in both topics, the impact of data-centric training set selection on energy efficiency remains to date unexplored. This paper presents an evolutionary-based sampling framework aimed at (i) identifying elite training samples tailored for datasets and model pairs, (ii) comparing model performance and energy efficiency gains against typical model training practice, and (iii) investigating the feasibility of this framework for fostering sustainable model training practices. To evaluate the propo
    
[^58]: 群集度量对无关特征的敏感度

    Cluster Metric Sensitivity to Irrelevant Features

    [https://arxiv.org/abs/2402.12008](https://arxiv.org/abs/2402.12008)

    本文研究群集性能对添加到基线数据集中的嘈杂不相关变量的敏感度。

    

    聚类算法在数据分析中被广泛使用，用于数据探索和发现。技术进步导致数据在容量、维度和复杂性方面不断增长。这为数据分析提供了巨大机会，因为数据可以用于许多不同目的的询问。然而，这也带来了挑战，比如在给定任务中识别相关特征。在监督任务中，可以利用各种方法优化任务目标（例如分类准确性）的输入特征。在无监督问题中，这些工具并不readily available，部分原因是无法定量地衡量无标签任务中特征的相关性。本文研究了群集性能对嘈杂的不相关变量进行迭代添加到具有明确定义群集的基线数据集的敏感度。我们展示了不同类型的无关变量如何影响结果。

    arXiv:2402.12008v1 Announce Type: cross  Abstract: Clustering algorithms are used extensively in data analysis for data exploration and discovery. Technological advancements lead to continually growth of data in terms of volume, dimensionality and complexity. This provides great opportunities in data analytics as the data can be interrogated for many different purposes. This however leads challenges, such as identification of relevant features for a given task. In supervised tasks, one can utilise a number of methods to optimise the input features for the task objective (e.g. classification accuracy). In unsupervised problems, such tools are not readily available, in part due to an inability to quantify feature relevance in unlabeled tasks. In this paper, we investigate the sensitivity of clustering performance noisy uncorrelated variables iteratively added to baseline datasets with well defined clusters. We show how different types of irrelevant variables can impact the outcome of a c
    
[^59]: 对抽取式知识图谱总结的调查：应用、方法、评估和未来方向

    A Survey on Extractive Knowledge Graph Summarization: Applications, Approaches, Evaluation, and Future Directions

    [https://arxiv.org/abs/2402.12001](https://arxiv.org/abs/2402.12001)

    本文调查了抽取式知识图谱总结的应用并提出了方法分类，为未来方向提供了重要参考。

    

    随着大型知识图谱（KGs）不断增长，抽取式KG总结成为一项热门任务。旨在提炼具有浓缩信息的紧凑子图，有助于各种下游基于KG的任务。在这篇调查论文中，我们是首批对其应用提供系统概述并从跨学科研究中定义现有方法的分类法之一。基于我们广泛而比较的评论，未来方向也已铺开。

    arXiv:2402.12001v1 Announce Type: new  Abstract: With the continuous growth of large Knowledge Graphs (KGs), extractive KG summarization becomes a trending task. Aiming at distilling a compact subgraph with condensed information, it facilitates various downstream KG-based tasks. In this survey paper, we are among the first to provide a systematic overview of its applications and define a taxonomy for existing methods from its interdisciplinary studies. Future directions are also laid out based on our extensive and comparative review.
    
[^60]: 回忆那一年发生的事件？评估大型语言模型中的时间信息和推理能力

    Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models

    [https://arxiv.org/abs/2402.11997](https://arxiv.org/abs/2402.11997)

    大型语言模型在处理时间信息和推理方面存在显著限制，闭源模型可能暗示了不确定性认识与错误回应之间的权衡。

    

    大型语言模型（LLMs）越来越普遍，但它们对于推理和保留时间信息的能力仍然有限。这限制了它们在理解事件的顺序性对关键的现实场景中的应用。本文在一个新颖的大规模时间数据集\textbf{TempUN}上对最先进的模型进行实验，揭示了时间保留和推理能力方面的显著限制。有趣的是，闭源模型更频繁地显示出知识差距，可能暗示了不确定性认识和错误回应之间的权衡。此外，探索各种微调方法并没有带来主要性能改进。相关数据集和代码可在以下网址获得（https://github.com/lingoiitgn/TempUN）。

    arXiv:2402.11997v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited. This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial. This paper experiments with state-of-the-art models on a novel, large-scale temporal dataset, \textbf{TempUN}, to reveal significant limitations in temporal retention and reasoning abilities. Interestingly, closed-source models indicate knowledge gaps more frequently, potentially suggesting a trade-off between uncertainty awareness and incorrect responses. Further, exploring various fine-tuning approaches yielded no major performance improvements. The associated dataset and code are available at the following URL (https://github.com/lingoiitgn/TempUN).
    
[^61]: 基于Hebbian学习的正交投影用于持续学习脉冲神经网络

    Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks

    [https://arxiv.org/abs/2402.11984](https://arxiv.org/abs/2402.11984)

    该研究提出了一种基于横向连接和Hebbian学习的神经操作新方法，能够通过投影保护知识

    

    使用脉冲神经网络进行神经形态计算，对于能效高的人工智能应用具有潜力。然而，与终身不断学习不同，神经网络模型容易出现灾难性遗忘。神经操作如何解决这一问题是人工智能和神经科学中的重要问题。本研究开发了一种基于横向连接和Hebbian学习的神经操作新方法，可以通过投影保护知识。

    arXiv:2402.11984v1 Announce Type: cross  Abstract: Neuromorphic computing with spiking neural networks is promising for energy-efficient artificial intelligence (AI) applications. However, different from humans who continually learn different tasks in a lifetime, neural network models suffer from catastrophic forgetting. How could neuronal operations solve this problem is an important question for AI and neuroscience. Many previous studies draw inspiration from observed neuroscience phenomena and propose episodic replay or synaptic metaplasticity, but they are not guaranteed to explicitly preserve knowledge for neuron populations. Other works focus on machine learning methods with more mathematical grounding, e.g., orthogonal projection on high dimensional spaces, but there is no neural correspondence for neuromorphic computing. In this work, we develop a new method with neuronal operations based on lateral connections and Hebbian learning, which can protect knowledge by projecting act
    
[^62]: 回归数据集中的不平衡问题

    Imbalance in Regression Datasets

    [https://arxiv.org/abs/2402.11963](https://arxiv.org/abs/2402.11963)

    回归数据集中的不平衡问题一直被忽视，本文通过理论分析和定义，展示了这一问题的重要性，并为未来研究提供了共同基础。

    

    就分类而言，类别不平衡的问题是众所周知的，并且已经得到了广泛研究。本文认为，在回归中存在的不平衡问题同样重要，但迄今为止被忽视：由于数据集目标分布中的欠表示和过多表示，回归器容易退化为朴素模型，系统地忽略不常见的训练数据并在训练期间经常见到的目标上进行过度表示。我们从理论上分析了这个问题，并利用得出的见解制定了对回归中不平衡的首个定义，我们展示这是分类中常用的不平衡度量的泛化。通过这样做，我们希望将关注点转向回归中被忽视的不平衡问题，并为未来研究奠定共同基础。

    arXiv:2402.11963v1 Announce Type: cross  Abstract: For classification, the problem of class imbalance is well known and has been extensively studied. In this paper, we argue that imbalance in regression is an equally important problem which has so far been overlooked: Due to under- and over-representations in a data set's target distribution, regressors are prone to degenerate to naive models, systematically neglecting uncommon training data and over-representing targets seen often during training. We analyse this problem theoretically and use resulting insights to develop a first definition of imbalance in regression, which we show to be a generalisation of the commonly employed imbalance measure in classification. With this, we hope to turn the spotlight on the overlooked problem of imbalance in regression and to provide common ground for future research.
    
[^63]: DB-LLM: 高效LLM的准确双二值化

    DB-LLM: Accurate Dual-Binarization for Efficient LLMs

    [https://arxiv.org/abs/2402.11960](https://arxiv.org/abs/2402.11960)

    本文提出了一种名为DB-LLM的新颖双二值化方法，通过引入灵活双二值化(FDB)来平衡2位宽度的精度优势和二值化的效率优势，从而在提高LLMs的计算效率的同时保持了准确性。

    

    大型语言模型(LLMs)显著推进了自然语言处理领域，然而高昂的内存和计算开销阻碍了它们的实际部署。量化成为改善LLMs计算效率的最有效方法之一。然而，现有的超低比特量化总是导致严重的精度下降。本文通过实证研究缓解了超低比特量化的微观和宏观特性，提出了一种新颖的LLMs双二值化方法，即DB-LLM。对于微观层面，我们考虑了2位宽度的准确性优势和二值化的效率优势，引入了灵活双二值化(FDB)。通过将2位量化权重分为两组独立的二进制数集，FDB确保了表示的准确性并引入了灵活性，利用二值化的高效位操作同时保留了

    arXiv:2402.11960v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly advanced the field of natural language processing, while the expensive memory and computation consumption impede their practical deployment. Quantization emerges as one of the most effective methods for improving the computational efficiency of LLMs. However, existing ultra-low-bit quantization always causes severe accuracy drops. In this paper, we empirically relieve the micro and macro characteristics of ultra-low bit quantization and present a novel Dual-Binarization method for LLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage of 2-bit-width and the efficiency advantage of binarization into account, introducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized weights into two independent sets of binaries, FDB ensures the accuracy of representations and introduces flexibility, utilizing the efficient bitwise operations of binarization while reta
    
[^64]: 多领域概括性摘要生成的关键性分配分析

    Analysis of Multidomain Abstractive Summarization Using Salience Allocation

    [https://arxiv.org/abs/2402.11955](https://arxiv.org/abs/2402.11955)

    通过SEASON技术，本研究评估了用于生成概括性摘要的方法，与BART、PEGASUS和ProphetNet等模型进行对比，在多个数据集上进行了评估，着重分析了财经数据集的表现。

    

    本文通过SEASON（Salience Allocation as Guidance for Abstractive SummarizatiON）技术的视角探讨了概括性文本摘要生成的领域，该模型旨在通过利用关键性分配技术来增强摘要生成。研究通过与BART、PEGASUS和ProphetNet等知名模型进行比较，这些模型均针对各种文本摘要任务进行了微调，来评估SEASON技术的有效性。评估使用包括CNN/Dailymail、SAMSum和基于财经新闻的Event-Driven Trading (EDT)在内的多种数据集进行，特别关注包含大量新闻文章的财经数据集，时间跨度为2020/03/01至2021/05/06。本文采用多种评估指标（如ROUGE、METEOR、BERTScore和MoverScore）来评估这些经过微调的模型生成概括性摘要的性能。这些指标的分析提供了对这些模型优劣的深入见解。

    arXiv:2402.11955v1 Announce Type: cross  Abstract: This paper explores the realm of abstractive text summarization through the lens of the SEASON (Salience Allocation as Guidance for Abstractive SummarizatiON) technique, a model designed to enhance summarization by leveraging salience allocation techniques. The study evaluates SEASON's efficacy by comparing it with prominent models like BART, PEGASUS, and ProphetNet, all fine-tuned for various text summarization tasks. The assessment is conducted using diverse datasets including CNN/Dailymail, SAMSum, and Financial-news based Event-Driven Trading (EDT), with a specific focus on a financial dataset containing a substantial volume of news articles from 2020/03/01 to 2021/05/06. This paper employs various evaluation metrics such as ROUGE, METEOR, BERTScore, and MoverScore to evaluate the performance of these models fine-tuned for generating abstractive summaries. The analysis of these metrics offers a thorough insight into the strengths a
    
[^65]: Mini-Hes：一种可并行化的二阶潜在因子分析模型

    Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model

    [https://arxiv.org/abs/2402.11948](https://arxiv.org/abs/2402.11948)

    Mini-Hes提出了一种新的mini-block对角黑塞无约束优化方法，用于构建二阶潜在因子分析模型，解决了大数据量下二阶算法可行性的挑战。

    

    与大量实体之间的交互在许多与大数据相关的任务中自然是高维且不完整的（HDI）。用户的行为特征隐藏在这些交互中，因此，有效地表示HDI数据是理解用户行为的基本任务。潜在因子分析（LFA）模型已被证明在表示HDI数据方面是有效的。 LFA模型的性能严重依赖于其训练过程，这是一个非凸优化问题。已经证明，在其训练过程中同时包含局部曲率和预处理梯度可以比使用一阶方法构建的LFA模型表现出更优异的性能。然而，随着数据量的增加，二阶算法的可行性面临挑战。为解决这一关键问题，本文提出了一种用于构建LFA模型的mini-block对角黑塞无约束（Mini-Hes）优化方法。

    arXiv:2402.11948v1 Announce Type: cross  Abstract: Interactions among large number of entities is naturally high-dimensional and incomplete (HDI) in many big data related tasks. Behavioral characteristics of users are hidden in these interactions, hence, effective representation of the HDI data is a fundamental task for understanding user behaviors. Latent factor analysis (LFA) model has proven to be effective in representing HDI data. The performance of an LFA model relies heavily on its training process, which is a non-convex optimization. It has been proven that incorporating local curvature and preprocessing gradients during its training process can lead to superior performance compared to LFA models built with first-order family methods. However, with the escalation of data volume, the feasibility of second-order algorithms encounters challenges. To address this pivotal issue, this paper proposes a mini-block diagonal hessian-free (Mini-Hes) optimization for building an LFA model.
    
[^66]: Team QUST在SemEval-2024任务8中的研究：单语和多语言方法对检测AI生成文本的全面研究

    Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting AI-generated Text

    [https://arxiv.org/abs/2402.11934](https://arxiv.org/abs/2402.11934)

    本文研究了团队QUST在SemEval-2024任务8中的参与情况，通过数据增强和清洗提高了模型训练效率和准确性，在单语任务中评估了多种方法并最终采用堆叠集成模型，最终在多语言环境中取得了不错的排名。

    

    本文介绍了团队QUST在SemEval 2024任务8中的参与情况。我们首先对数据集进行了增强和清洗，以提高模型训练的效率和准确性。在单语任务中，我们评估了传统的深度学习方法、多尺度正负无标记框架（MPU）、微调、适配器和集成方法。然后，我们从单语模型中选择了表现最佳的模型，并在子任务A和B中对它们进行评估。最终模型采用了将微调与MPU相结合的堆叠集成。在多语言环境的子任务A中，我们的系统在官方测试集中取得了第8名（在准确性方面排名第13）。我们在https://github.com/warmth27/SemEval2024_QUST发布了我们的系统代码。

    arXiv:2402.11934v1 Announce Type: cross  Abstract: This paper presents the participation of team QUST in Task 8 SemEval 2024. We first performed data augmentation and cleaning on the dataset to enhance model training efficiency and accuracy. In the monolingual task, we evaluated traditional deep-learning methods, multiscale positive-unlabeled framework (MPU), fine-tuning, adapters and ensemble methods. Then, we selected the top-performing models based on their accuracy from the monolingual models and evaluated them in subtasks A and B. The final model construction employed a stacking ensemble that combined fine-tuning with MPU. Our system achieved 8th (scored 8th in terms of accuracy, officially ranked 13th) place in the official test set in multilingual settings of subtask A. We release our system code at:https://github.com/warmth27/SemEval2024_QUST
    
[^67]: 通过联合数据深化和预取实现能效边缘学习

    Energy-Efficient Edge Learning via Joint Data Deepening-and-Prefetching

    [https://arxiv.org/abs/2402.11925](https://arxiv.org/abs/2402.11925)

    我们提出了一种名为JD2P的新型离线架构，通过联合数据深化和预取技术，按顺序离线每个数据样本的特征，以减少物联网设备向边缘服务器传输数据时所需的能量消耗。

    

    通过利用物联网设备收集的实时数据对人工智能模型进行实时训练，实现无处不在的人工智能服务的愿景。为了解决能源受限的物联网设备传输高维且庞大数据的挑战，我们提出了一种新颖的离线架构，称为联合数据深化和预取（JD2P），其包含两个关键技术：数据深化和预取。

    arXiv:2402.11925v1 Announce Type: cross  Abstract: The vision of pervasive artificial intelligence (AI) services can be realized by training an AI model on time using real-time data collected by internet of things (IoT) devices. To this end, IoT devices require offloading their data to an edge server in proximity. However, transmitting high-dimensional and voluminous data from energy-constrained IoT devices poses a significant challenge. To address this limitation, we propose a novel offloading architecture, called joint data deepening-and-prefetching (JD2P), which is feature-by-feature offloading comprising two key techniques. The first one is data deepening, where each data sample's features are sequentially offloaded in the order of importance determined by the data embedding technique such as principle component analysis (PCA). Offloading is terminated once the already transmitted features are sufficient for accurate data classification, resulting in a reduction in the amount of tr
    
[^68]: SoLA: 为了更好的逻辑推理而对LLM进行求解层调整

    SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning

    [https://arxiv.org/abs/2402.11903](https://arxiv.org/abs/2402.11903)

    提出了一种新颖的求解器层适应（SoLA）方法，在LLM中引入求解器层，不同地引导解决方案朝向可满足性

    

    针对大型语言模型（LLMs）在逻辑推理上面临的挑战，先前的努力试图通过工具学习来改变问题求解。虽然在小规模问题上已经取得了进展，但由于规模庞大且表达复杂，解决工业案例仍然困难。在本文中，我们提出了一种新颖的求解层适应（SoLA）方法，我们在LLM中引入了一个求解器作为新层，不同地引导解决方案朝向可满足性。在SoLA中，LLM旨在理解自然语言描述的搜索空间，并识别最高质量的局部解，而求解器层则专注于初始解不满足的约束条件。借助MaxSAT作为桥梁，我们定义了前向和后向传递梯度，使最终模型能够收敛到一个满足的解或证明不可满足性。后门理论确保SoLA能够获得准确性

    arXiv:2402.11903v1 Announce Type: cross  Abstract: Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning. While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions. In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability. In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability. The backdoor theory ensures that SoLA can obtain accurat
    
[^69]: 带有PDDL+和更多内容的实际规划

    Real-World Planning with PDDL+ and Beyond

    [https://arxiv.org/abs/2402.11901](https://arxiv.org/abs/2402.11901)

    Nyx是一种新型PDDL+规划器，强调轻巧、简单和适应性，可以定制实现超越PDDL+范围的能力。

    

    人工智能规划的实际应用通常需要一个高度表达力的建模语言，以精确捕捉目标系统的重要复杂性。混合系统在现实世界中随处可见，而PDDL+是捕捉此类系统作为规划领域的标准化建模语言。PDDL+使得能够准确编码混合离散-连续系统动态、外生活动以及在实际场景中展示的许多其他有趣特性成为可能。然而，由于PDDL+规划软件的普遍短缺和少数现有规划器的严格限制，PDDL+的使用情况一直较为缓慢和犹豫不决。为了克服这一分歧，我们提出了Nyx，这是一种强调轻巧、简单和最重要的适应性的新型PDDL+规划器。该规划器被设计为可以轻松定制，扩展其能力远远超出PDDL+的范围。因此，Nyx可以根据需要定制实现几乎

    arXiv:2402.11901v1 Announce Type: new  Abstract: Real-world applications of AI Planning often require a highly expressive modeling language to accurately capture important intricacies of target systems. Hybrid systems are ubiquitous in the real-world, and PDDL+ is the standardized modeling language for capturing such systems as planning domains. PDDL+ enables accurate encoding of mixed discrete-continuous system dynamics, exogenous activity, and many other interesting features exhibited in realistic scenarios. However, the uptake in usage of PDDL+ has been slow and apprehensive, largely due to a general shortage of PDDL+ planning software, and rigid limitations of the few existing planners. To overcome this chasm, we present Nyx, a novel PDDL+ planner built to emphasize lightness, simplicity, and, most importantly, adaptability. The planner is designed to be effortlessly customizable to expand its capabilities well beyond the scope of PDDL+. As a result, Nyx can be tailored to virtuall
    
[^70]: 通过自适应解码和上下文信息-熵约束来识别和解决知识冲突

    Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint

    [https://arxiv.org/abs/2402.11893](https://arxiv.org/abs/2402.11893)

    提出了一种自适应解码方法COIECD，用于识别和解决知识冲突，提高模型对冲突上下文的忠实度，并在非冲突情况下保持高性能。

    

    大型语言模型在预训练期间内部化了大量参数化知识。与此同时，现实应用需要外部上下文知识来帮助模型完成基本任务。这引发了一个被称为知识冲突的关键困境，即上下文知识与模型内部知识相冲突。然而，现有的解码方法专门用于解决知识冲突，可能会在没有冲突的情况下无意中降低性能。本文提出了一种自适应解码方法，称为上下文信息-熵约束解码（COIECD），用于识别知识冲突并解决它们。它可以提高模型对冲突上下文的忠实度，同时在非冲突情况下保持高性能。我们的实验表明，COIECD在现实数据集中展现出较强的性能和稳健性。提供源代码。

    arXiv:2402.11893v1 Announce Type: new  Abstract: Large language models internalize enormous parametric knowledge during pre-training. Concurrently, realistic applications necessitate external contextual knowledge to aid models on the underlying tasks. This raises a crucial dilemma known as knowledge conflicts, where the contextual knowledge clashes with the However, existing decoding works are specialized in resolving knowledge conflicts and could inadvertently deteriorate performance in absence of conflicts. In this paper, we propose an adaptive decoding method, termed as contextual information-entropy constraint decoding (COIECD), to discern whether the knowledge conflicts occur and resolve them. It can improve the model's faithfulness to conflicting context, and simultaneously maintain high performance among non- Our experiments show that COIECD exhibits strong performance and robustness over knowledge conflicts in realistic datasets. Code is available.
    
[^71]: 用保留语义的转换评估程序修复：自然性评估

    Evaluating Program Repair with Semantic-Preserving Transformations: A Naturalness Assessment

    [https://arxiv.org/abs/2402.11892](https://arxiv.org/abs/2402.11892)

    本文研究了保留语义的转换的自然性及其对NPR评估的影响，发现了NPR系统在面对不自然的代码转换时会产生较高的误报率，且在使用自然转换进行评估时性能明显下降。

    

    在本文中，我们研究了保留语义的转换的自然性及其对NPR评估的影响。为了达到这个目的，我们进行了一个两阶段的人类研究，包括(1)与资深软件开发人员的访谈，以建立评估代码转换自然性的第一个具体标准；(2)进行了一项涉及10名开发人员的调查，评估了应用于225个真实世界bug的1178个转换（即原始和转换程序成对的情况）的自然性。我们的研究结果显示，其中接近60%的转换被认为是自然的，20%的转换被认为是不自然的，并且在人类标注者之间有相当高的一致性。此外，不自然的代码转换引入了五个知名NPR系统的稳健性的25.2%误报率。此外，当使用自然转换进行评估时，NPR系统的性能显着下降，即性能下降高达22.9%和23.6%。

    arXiv:2402.11892v1 Announce Type: cross  Abstract: In this paper, we investigate the naturalness of semantic-preserving transformations and their impacts on the evaluation of NPR. To achieve this, we conduct a two-stage human study, including (1) interviews with senior software developers to establish the first concrete criteria for assessing the naturalness of code transformations and (2) a survey involving 10 developers to assess the naturalness of 1178 transformations, i.e., pairs of original and transformed programs, applied to 225 real-world bugs. Our findings reveal that nearly 60% and 20% of these transformations are considered natural and unnatural with substantially high agreement among human annotators. Furthermore, the unnatural code transformations introduce a 25.2% false alarm rate on robustness of five well-known NPR systems. Additionally, the performance of the NPR systems drops notably when evaluated using natural transformations, i.e., a drop of up to 22.9% and 23.6% i
    
[^72]: LLM的多彩未来：评估和改进LLM作为酷儿青少年情感支持者的作用

    The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth

    [https://arxiv.org/abs/2402.11886](https://arxiv.org/abs/2402.11886)

    本文旨在评估和改进LLM作为酷儿青少年情感支持者的潜力，通过定性和定量分析LLM与酷儿相关内容的互动，并开发了一个新颖的评估标准量表。

    

    酷儿青少年面临着增加的心理健康风险，如抑郁、焦虑和自杀意念。受负面标签的影响，他们经常避免寻求帮助，依赖在线资源，这可能提供不相容的信息。虽然获得支持环境和可靠信息是无价的，但全球许多酷儿青少年无法获得这种支持。然而，由于ChatGPT等大型语言模型（LLM）的快速采用，情况可能很快发生变化。本文旨在全面探讨LLM改变酷儿情感支持的潜力。为此，我们对LLM与酷儿相关内容的互动进行定性和定量分析。为了评估回应质量，我们开发了一个受心理标准和专家意见启发的新颖十个问题量表。我们将该量表应用于评分几个LLM和人类评论，这些评论是酷儿青少年寻求建议和分享时发表的。

    arXiv:2402.11886v1 Announce Type: cross  Abstract: Queer youth face increased mental health risks, such as depression, anxiety, and suicidal ideation. Hindered by negative stigma, they often avoid seeking help and rely on online resources, which may provide incompatible information. Although access to a supportive environment and reliable information is invaluable, many queer youth worldwide have no access to such support. However, this could soon change due to the rapid adoption of Large Language Models (LLMs) such as ChatGPT. This paper aims to comprehensively explore the potential of LLMs to revolutionize emotional support for queers. To this end, we conduct a qualitative and quantitative analysis of LLM's interactions with queer-related content. To evaluate response quality, we develop a novel ten-question scale that is inspired by psychological standards and expert input. We apply this scale to score several LLMs and human comments to posts where queer youth seek advice and share 
    
[^73]: 在具有放松采样模型的在线模型的有限时间误差分析下的Q学习

    Finite-Time Error Analysis of Online Model-Based Q-Learning with a Relaxed Sampling Model

    [https://arxiv.org/abs/2402.11877](https://arxiv.org/abs/2402.11877)

    本文通过有限时间分析以及实证评估，探讨了集成模型方法的Q学习在样本复杂度方面的优势。

    

    强化学习在模型为基础的方法的出现下取得了显著进展。在这些方法中，Q学习在无模型设置中被证明是一种强大的算法。然而，将Q学习扩展到基于模型的框架仍然相对未被探索。在本文中，我们深入研究了Q学习与基于模型方法相结合时的样本复杂度。通过理论分析和实证评估，我们试图阐明在哪些条件下，基于模型的Q学习在样本效率方面优于其无模型对应物。

    arXiv:2402.11877v1 Announce Type: cross  Abstract: Reinforcement learning has witnessed significant advancements, particularly with the emergence of model-based approaches. Among these, $Q$-learning has proven to be a powerful algorithm in model-free settings. However, the extension of $Q$-learning to a model-based framework remains relatively unexplored. In this paper, we delve into the sample complexity of $Q$-learning when integrated with a model-based approach. Through theoretical analyses and empirical evaluations, we seek to elucidate the conditions under which model-based $Q$-learning excels in terms of sample efficiency compared to its model-free counterpart.
    
[^74]: 从实际到逻辑再到实际：为规划从原始数据中发明符号词汇、动作和模型

    From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions and Models for Planning from Raw Data

    [https://arxiv.org/abs/2402.11871](https://arxiv.org/abs/2402.11871)

    本文提出了一种从未标记高维实值机器人轨迹开始自主学习通用的逻辑相关表示，这些表示构成了自动发明的PDDL-like域模型。

    

    手工制作的基于逻辑的状态和动作表示已被广泛用于克服长期人工智能机器人规划问题的计算复杂性，包括任务和动作规划问题。但是，创建这样的表示需要具有强烈直觉和详细知识的专家，他们了解机器人和在特定环境中可能需要完成的任务。消除对人类直觉的依赖是一个极为活跃的研究领域。 本文提出了一种自主学习通用逻辑相关表示的方法，该表示从未标记的高维实值机器人轨迹开始。所学表示构成了自动发明的类PDDL域模型。确定性设置下的实证结果表明，仅从少数机器人轨迹中可以学到强大的抽象表示；所学关系

    arXiv:2402.11871v1 Announce Type: cross  Abstract: Hand-crafted, logic-based state and action representations have been widely used to overcome the intractable computational complexity of long-horizon robot planning problems, including task and motion planning problems. However, creating such representations requires experts with strong intuitions and detailed knowledge about the robot and the tasks it may need to accomplish in a given setting. Removing this dependency on human intuition is a highly active research area.   This paper presents the first approach for autonomously learning generalizable, logic-based relational representations for abstract states and actions starting from unannotated high-dimensional, real-valued robot trajectories. The learned representations constitute auto-invented PDDL-like domain models. Empirical results in deterministic settings show that powerful abstract representations can be learned from just a handful of robot trajectories; the learned relation
    
[^75]: 基于层次分析法和模糊逻辑的两种在线地图匹配算法

    Two Online Map Matching Algorithms Based on Analytic Hierarchy Process and Fuzzy Logic

    [https://arxiv.org/abs/2402.11866](https://arxiv.org/abs/2402.11866)

    本文开发了基于层次分析法和模糊逻辑的两种新的在线地图匹配算法，其中AHP应用于地图匹配的方式是新开发的，同时模糊逻辑被应用于地图匹配与现有研究类似，但进行了一些改变。

    

    我们这篇论文的目的是开发新的地图匹配算法并改进先前的工作。我们介绍了两种关键方法：层次分析法（AHP）地图匹配和模糊逻辑地图匹配。AHP是一种将数学分析与人类判断相结合的决策方法，模糊逻辑是一种基于真实程度的计算方法，旨在建模从0到1的不确定推理方式，而不是通常的布尔逻辑。在这些算法中，我们在本文中新开发了将AHP应用于地图匹配的方式，与此同时，我们将模糊逻辑应用于地图匹配的方法与现有研究大致相同，除了一些细微的改变。由于这两种方法的共同特点是设计用于处理不精确信息并且易于实现，我们决定使用这些方法。

    arXiv:2402.11866v1 Announce Type: cross  Abstract: Our aim of this paper is to develop new map matching algorithms and to improve upon previous work. We address two key approaches: Analytic Hierarchy Process (AHP) map matching and fuzzy logic map matching. AHP is a decision-making method that combines mathematical analysis with human judgment, and fuzzy logic is an approach to computing based on the degree of truth and aims at modeling the imprecise modes of reasoning from 0 to 1 rather than the usual boolean logic. Of these algorithms, the way of our applying AHP to map matching is newly developed in this paper, meanwhile, our application of fuzzy logic to map matching is mostly the same as existing research except for some small changes. Because of the common characteristic that both methods are designed to handle imprecise information and simplicity for implementation, we decided to use these methods.
    
[^76]: CodeArt：当符号缺失时通过注意力规范化改进代码模型

    CodeArt: Better Code Models by Attention Regularization When Symbols Are Lacking

    [https://arxiv.org/abs/2402.11842](https://arxiv.org/abs/2402.11842)

    提出一种在符号缺失时通过注意力规范化改进代码模型的新方法，使用程序分析提取上下文并利用注意力掩码方法，同时利用自注意力机制学习关注度的重要性

    

    基于Transformer的代码模型在许多软件工程任务中表现出色。然而，当符号缺失或者不具信息量时，它们的有效性会下降。这是因为模型可能没有学会在没有符号的情况下正确地关注相关性/上下文。我们提出了一种新的方法，在符号缺失时预训练通用代码模型。我们观察到，在这种情况下，程序会退化为用非常原始的语言编写的内容。因此，我们建议使用程序分析来事先提取上下文（而不是像传统模型中依赖符号和掩码语言建模）。然后，我们利用一种新颖的注意力掩码方法，只允许模型关注这些上下文，例如双向程序依赖传递闭包和令牌共现。与此同时，内在的自注意力机制被用于学习允许的关注度哪些更重要。

    arXiv:2402.11842v1 Announce Type: cross  Abstract: Transformer based code models have impressive performance in many software engineering tasks. However, their effectiveness degrades when symbols are missing or not informative. The reason is that the model may not learn to pay attention to the right correlations/contexts without the help of symbols. We propose a new method to pre-train general code models when symbols are lacking. We observe that in such cases, programs degenerate to something written in a very primitive language. We hence propose to use program analysis to extract contexts a priori (instead of relying on symbols and masked language modeling as in vanilla models). We then leverage a novel attention masking method to only allow the model attending to these contexts, e.g., bi-directional program dependence transitive closures and token co-occurrences. In the meantime, the inherent self-attention mechanism is utilized to learn which of the allowed attentions are more impo
    
[^77]: 在真正重要的地方：低资源语言的少样本环境保护媒体监测

    Where It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages

    [https://arxiv.org/abs/2402.11818](https://arxiv.org/abs/2402.11818)

    提出了一种名为NewsSerow的方法，用于自动识别低资源语言中的环境保护内容，并且在尼泊尔语中使用少于10个示例新闻文章时，NewsSerow显着优于其他少样本模型。

    

    环保组织常规监测有可能对环境产生影响的保护区媒体内容，以保持对可能发展的情况的认识。现有的自动化媒体监测系统需要由领域专家标记的大量数据，这在英语等高资源语言的规模上才是可行的。然而，这样的工具在全球南方最需要，在那里感兴趣的新闻主要是用本地低资源语言发布的，可持续地对数据集进行注释的专家也很少。在本文中，我们提出了一种NewsSerow方法，用于自动识别低资源语言中的环境保护内容。NewsSerow是一个利用大型语言模型（LLMs）的总结、上下文少样本分类和自我反思的流程。在尼泊尔语中使用最多10个新闻示例文章，NewsSerow明显优于其他少样本模型。

    arXiv:2402.11818v1 Announce Type: cross  Abstract: Environmental conservation organizations routinely monitor news content on conservation in protected areas to maintain situational awareness of developments that can have an environmental impact. Existing automated media monitoring systems require large amounts of data labeled by domain experts, which is only feasible at scale for high-resource languages like English. However, such tools are most needed in the global south where news of interest is mainly in local low-resource languages, and far fewer experts are available to annotate datasets sustainably. In this paper, we propose NewsSerow, a method to automatically recognize environmental conservation content in low-resource languages. NewsSerow is a pipeline of summarization, in-context few-shot classification, and self-reflection using large language models (LLMs). Using at most 10 demonstration example news articles in Nepali, NewsSerow significantly outperforms other few-shot me
    
[^78]: HU在SemEval-2024任务8A中的表现：对比学习能否学习嵌入以检测机器生成的文本？

    HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?

    [https://arxiv.org/abs/2402.11815](https://arxiv.org/abs/2402.11815)

    提出了一种基于对比学习的单一模型，用较少的参数实现与基线相当的机器生成文本检测性能

    

    这篇论文描述了我们为SemEval-2024任务8“多生成器、多领域和多语言黑匣子机器生成文本检测”开发的系统。由于大型语言模型（LLM）在虚假文本生成、网络钓鱼、考试作弊甚至抄袭版权材料中的使用，机器生成文本一直是主要关注的问题之一。许多系统已经被开发用于检测机器生成的文本。然而，这些系统中的大部分依赖于文本生成模型，这是一个在实际场景中不切实际的限制，因为通常不可能知道用户用于文本生成的具体模型。在这项工作中，我们提出了基于对比学习的单一模型，其使用基线参数的大约40%（149M比355M），但在测试数据集上表现出了可比的性能（在137个参与者中排名第21）。我们的关键发现是，即使没有多个模型的集成，

    arXiv:2402.11815v1 Announce Type: cross  Abstract: This paper describes our system developed for SemEval-2024 Task 8, "Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection." Machine-generated texts have been one of the main concerns due to the use of large language models (LLM) in fake text generation, phishing, cheating in exams, or even plagiarizing copyright materials. A lot of systems have been developed to detect machine-generated text. Nonetheless, the majority of these systems rely on the text-generating model, a limitation that is impractical in real-world scenarios, as it's often impossible to know which specific model the user has used for text generation. In this work, we propose a single model based on contrastive learning, which uses ~40% of the baseline's parameters (149M vs. 355M) but shows a comparable performance on the test dataset (21st out of 137 participants). Our key finding is that even without an ensemble of multiple models, a
    
[^79]: 自适应压力测试高速公路自动驾驶车辆的新框架

    A novel framework for adaptive stress testing of autonomous vehicles in highways

    [https://arxiv.org/abs/2402.11813](https://arxiv.org/abs/2402.11813)

    提出了一种新颖的框架，利用自适应压力测试方法和深度强化学习来系统探索可能导致高速公路交通场景中安全问题的边界情况

    

    保证自动驾驶车辆（AVs）的安全运行对于它们的广泛应用和公众接受至关重要。因此，不仅对AV进行标准安全测试的评估，还发现可能导致不安全行为或情况的被测试AV的潜在边界情况具有极其重要的意义。本文提出了一个新颖的框架，用于系统地探索可能导致高速公路交通场景中安全问题的边界情况。该框架基于一种自适应压力测试（AST）方法，这是一种利用马尔可夫决策过程制定场景以及深度强化学习（DRL）发现代表边界情况的理想模式的新兴验证方法。为此，我们为DRL开发了一个新的奖励函数，以指导AST根据被测试AV（即自车）与其他车辆之间的碰撞概率估计来识别碰撞场景。

    arXiv:2402.11813v1 Announce Type: cross  Abstract: Guaranteeing the safe operations of autonomous vehicles (AVs) is crucial for their widespread adoption and public acceptance. It is thus of a great significance to not only assess the AV against the standard safety tests, but also discover potential corner cases of the AV under test that could lead to unsafe behaviour or scenario. In this paper, we propose a novel framework to systematically explore corner cases that can result in safety concerns in a highway traffic scenario. The framework is based on an adaptive stress testing (AST) approach, an emerging validation method that leverages a Markov decision process to formulate the scenarios and deep reinforcement learning (DRL) to discover the desirable patterns representing corner cases. To this end, we develop a new reward function for DRL to guide the AST in identifying crash scenarios based on the collision probability estimate between the AV under test (i.e., the ego vehicle) and 
    
[^80]: 智能并行自动纠错解码：加速大型语言模型推理

    Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding

    [https://arxiv.org/abs/2402.11809](https://arxiv.org/abs/2402.11809)

    提出了Smart Parallel Auto-Correct Decoding (SPACE)方法，通过集成半自回归推理和猜测解码，实现了大型语言模型推理加速和并行生成验证令牌的功能。

    

    这项研究旨在加速拥有数十亿参数的大型语言模型（LLMs）的推理速度。我们提出了“智能并行自动纠错解码”（SPACE），这是一种创新方法，旨在实现LLMs的无损加速。通过集成半自回归推理和猜测解码能力，SPACE独特地使自回归LLMs能够并行生成和验证令牌。这是通过专门的半自回归监督微调过程实现的，该过程使现有LLMs具有同时预测多个令牌的能力。此外，一种自动纠错解码算法促进了单个模型调用内令牌序列的同时生成和验证。通过在一系列LLMs上进行广泛实验证明，SPACE在HumanEval-X上表现出2.7倍至4.0倍的推理加速。

    arXiv:2402.11809v1 Announce Type: cross  Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaini
    
[^81]: LLM作为提示器：在任意知识图上进行低资源归纳推理

    LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs

    [https://arxiv.org/abs/2402.11804](https://arxiv.org/abs/2402.11804)

    本文利用大型语言模型（LLMs）生成图形结构提示，以增强预训练的图神经网络（GNNs），提出一种新的方法论见解，实现了在任意知识图上进行低资源归纳推理的高通用性。

    

    知识图（KG）归纳推理旨在推断训练期间未见过的新KG中缺失的事实，在各种应用中被广泛采用。KG归纳推理的一个关键挑战是处理在文本和结构方面都稀缺的低资源场景。本文尝试利用大型语言模型（LLMs）来解决这一挑战。具体来说，我们利用最先进的LLMs生成图形结构提示，以增强预训练的图神经网络（GNNs），从而为KG归纳推理方法带来新的方法论见解，以及在实践中具有很高的普适性。在方法论方面，我们引入了一种新颖的预训练和提示框架ProLINK，旨在在任意KG上进行低资源归纳推理，而无需额外训练。在实践方面，我们在36个低资源数据集上对我们的方法进行了实验评估。

    arXiv:2402.11804v1 Announce Type: new  Abstract: Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-res
    
[^82]: 具有延迟更新的随机逼近：马尔科夫采样下的有限时间速率

    Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling

    [https://arxiv.org/abs/2402.11800](https://arxiv.org/abs/2402.11800)

    延迟更新的随机逼近方案在时间变化有界延迟下，保证了每次迭代快速收敛到固定点周围的球体，界限依赖于最大延迟和混合时间。

    

    受大规模和多智能体强化学习应用的启发，我们研究了在马尔科夫采样下具有延迟更新的随机逼近（SA）方案的非渐近性能。虽然延迟的影响在优化中得到了广泛研究，但它们与底层马尔科夫过程相互作用以塑造SA的有限时间性能的方式仍然不太清楚。在这个背景下，我们的第一个主要贡献是证明在时间变化有界延迟下，延迟的SA更新规则确保最后迭代收敛到SA运算符固定点周围的球体具有指数快速的速度。值得注意的是，我们的界限在依赖于最大延迟$\tau_{max}$和混合时间$\tau_{mix}$方面是\emph{紧致的}。为了实现这一紧密界限，我们开发了一种新颖的归纳证明技术，与各种现有延迟优化分析不同，它依赖于建立未...

    arXiv:2402.11800v1 Announce Type: cross  Abstract: Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling. While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood. In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \emph{last iterate} to a ball around the SA operator's fixed point. Notably, our bound is \emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing un
    
[^83]: 生成万花筒网络

    Generative Kaleidoscopic Networks

    [https://arxiv.org/abs/2402.11793](https://arxiv.org/abs/2402.11793)

    发现深层ReLU网络表现出过度泛化现象，利用这一特性设计了“生成万花筒网络”，通过递归映射随机输入噪声生成样本。

    

    发现深层ReLU网络（或多层感知器架构）表现出“过度泛化”现象。也就是说，那些在训练过程中没有看到的输入的输出值被映射到了在学习过程中观察到的输出范围附近。换句话说，多层感知器学习了一对多的映射，这种效应在增加层数或多层感知器的深度时更为明显。我们利用了深层ReLU网络的这一特性来设计一个数据集万花筒，称为“生成万花筒网络”。简而言之，如果我们学习一个多层感知器将输入 $x\in\mathbb{R}^D$ 映射到自身 $f_\mathcal{N}(x)\rightarrow x$，那么“万花筒采样”过程将从随机输入噪声 $z\in\mathbb{R}^D$ 开始，并递归地应用 $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$。经过燃烧期后，我们开始观察来自输入分布的样本，我们发现更深的

    arXiv:2402.11793v1 Announce Type: cross  Abstract: We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from input $x\in\mathbb{R}^D$ to itself $f_\mathcal{N}(x)\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\in\mathbb{R}^D$ and recursively applies $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper 
    
[^84]: MM-SurvNet：基于深度学习的多模态数据融合在乳腺癌中的生存风险分层

    MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast Cancer Through Multimodal Data Fusion

    [https://arxiv.org/abs/2402.11788](https://arxiv.org/abs/2402.11788)

    提出了一种深度学习方法，通过整合多模态数据实现乳腺癌患者的生存风险分层，通过MaxViT模型和自注意力机制从图像中提取特征并融合遗传和临床数据，取得了优于现有方法的性能，有望改善患者预后。

    

    生存风险分层是乳腺癌管理中临床决策的重要步骤。我们提出了一种新颖的深度学习方法，通过整合组织病理学影像、遗传学和临床数据来实现这一目的。它采用视觉变换器，特别是MaxViT模型，用于图像特征提取，利用自注意力机制在患者水平捕捉复杂的图像关系。双交叉注意力机制将这些特征与遗传数据融合，而临床数据在最终层级进行合并以提高预测准确性。在公开的TCGA-BRCA数据集上进行的实验表明，我们的模型使用负对数似然损失函数训练，可以达到0.64的平均C-index，超越了现有方法。这一进展有助于制定个性化治疗策略，潜在地改善患者预后。

    arXiv:2402.11788v1 Announce Type: cross  Abstract: Survival risk stratification is an important step in clinical decision making for breast cancer management. We propose a novel deep learning approach for this purpose by integrating histopathological imaging, genetic and clinical data. It employs vision transformers, specifically the MaxViT model, for image feature extraction, and self-attention to capture intricate image relationships at the patient level. A dual cross-attention mechanism fuses these features with genetic data, while clinical data is incorporated at the final layer to enhance predictive accuracy. Experiments on the public TCGA-BRCA dataset show that our model, trained using the negative log likelihood loss function, can achieve superior performance with a mean C-index of 0.64, surpassing existing methods. This advancement facilitates tailored treatment strategies, potentially leading to improved patient outcomes.
    
[^85]: 面向计算存储硬件的深度神经网络架构和配置联合优化

    Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware

    [https://arxiv.org/abs/2402.11780](https://arxiv.org/abs/2402.11780)

    本文提出了CiMNet，一个旨在联合优化深度神经网络架构和配置的框架，以解决计算存储硬件构建中的挑战。

    

    随着大规模深度神经网络需求的增长，计算存储（CiM）作为一种突出解决方案，有助于缓解约束冯·诺依曼体系结构的带宽和芯片内部连接瓶颈。然而，CiM硬件的构建面临挑战，因为在不同接口的缓存大小和内存带宽方面的任何特定内存层次结构可能不理想地匹配任何神经网络的属性，如张量维度和算术强度，因此导致次优和表现不佳的系统。 尽管神经架构搜索（NAS）技术在为给定硬件度量预算（例如DNN执行时间或延迟）产生高效子网络方面取得成功，但它假设硬件配置被冻结，通常为给定预算产生次优子网络。 本文提出了CiMNet，一个联合搜索最优子网络和

    arXiv:2402.11780v1 Announce Type: cross  Abstract: With the recent growth in demand for large-scale deep neural networks, compute in-memory (CiM) has come up as a prominent solution to alleviate bandwidth and on-chip interconnect bottlenecks that constrain Von-Neuman architectures. However, the construction of CiM hardware poses a challenge as any specific memory hierarchy in terms of cache sizes and memory bandwidth at different interfaces may not be ideally matched to any neural network's attributes such as tensor dimension and arithmetic intensity, thus leading to suboptimal and under-performing systems. Despite the success of neural architecture search (NAS) techniques in yielding efficient sub-networks for a given hardware metric budget (e.g., DNN execution time or latency), it assumes the hardware configuration to be frozen, often yielding sub-optimal sub-networks for a given budget. In this paper, we present CiMNet, a framework that jointly searches for optimal sub-networks and 
    
[^86]: 朝向自消耗生成模型的理论理解

    Towards Theoretical Understandings of Self-Consuming Generative Models

    [https://arxiv.org/abs/2402.11778](https://arxiv.org/abs/2402.11778)

    通过构建理论框架，我们探讨了在自消耗循环中训练生成模型对数据分布学习的影响，证明了在足够大的训练数据集大小或真实数据比例条件下，合成数据分布与原始真实数据分布之间的总变差距离能够被有效控制。

    

    这篇论文探讨了训练生成模型的新挑战，即在一个自消耗循环中训练模型，其中连续的模型世代通过混合之前世代的真实数据和合成数据来进行递归训练。我们构建了一个理论框架，以严格评估这种训练方案对未来模型学习的数据分布产生的影响。具体来说，我们推导了在不同混合训练场景下，未来模型产生的合成数据分布与原始真实数据分布之间的总变差（TV）距离的界限。我们的分析表明，在混合训练数据集的大小或真实数据比例足够大的条件下，这种距离可以被有效控制。有趣的是，我们进一步揭示了由扩大合成数据量引起的相变，理论上证明了虽然TV距离表现出初始上升，但却逐渐下降。

    arXiv:2402.11778v1 Announce Type: cross  Abstract: This paper tackles the emerging challenge of training generative models within a self-consuming loop, wherein successive generations of models are recursively trained on mixtures of real and synthetic data from previous generations. We construct a theoretical framework to rigorously evaluate how this training regimen impacts the data distributions learned by future models. Specifically, we derive bounds on the total variation (TV) distance between the synthetic data distributions produced by future models and the original real data distribution under various mixed training scenarios. Our analysis demonstrates that this distance can be effectively controlled under the condition that mixed training dataset sizes or proportions of real data are large enough. Interestingly, we further unveil a phase transition induced by expanding synthetic data amounts, proving theoretically that while the TV distance exhibits an initial ascent, it declin
    
[^87]: 在语言模型嵌入中揭示潜在的人类福祉

    Uncovering Latent Human Wellbeing in Language Model Embeddings

    [https://arxiv.org/abs/2402.11777](https://arxiv.org/abs/2402.11777)

    本研究通过ETHICS Utilitarianism任务发现，预训练语言模型的表示隐含了对人类福祉的理解，且模型规模增加时，准确率呈非下降趋势。

    

    语言模型是否隐含地学习了人类福祉的概念？我们通过ETHICS功利主义任务进行探讨，评估缩放是否增强了预训练模型的表示。我们的初步发现显示，无需任何提示工程或微调，OpenAI的text-embedding-ada-002的主成分达到73.9%的准确率。这与在整个ETHICS数据集上微调的BERT-large模型的74.6%准确率非常接近，表明预训练传达了对人类福祉的某种理解。接下来，我们考虑了四种语言模型系列，观察功利主义准确率随参数增加而变化。我们发现，使用足够数量的主成分时，性能随模型规模的增加而非减少。

    arXiv:2402.11777v1 Announce Type: cross  Abstract: Do language models implicitly learn a concept of human wellbeing? We explore this through the ETHICS Utilitarianism task, assessing if scaling enhances pretrained models' representations. Our initial finding reveals that, without any prompt engineering or finetuning, the leading principal component from OpenAI's text-embedding-ada-002 achieves 73.9% accuracy. This closely matches the 74.6% of BERT-large finetuned on the entire ETHICS dataset, suggesting pretraining conveys some understanding about human wellbeing. Next, we consider four language model families, observing how Utilitarianism accuracy varies with increased parameters. We find performance is nondecreasing with increased model size when using sufficient numbers of principal components.
    
[^88]: 张量时间序列的动态多网络挖掘

    Dynamic Multi-Network Mining of Tensor Time Series

    [https://arxiv.org/abs/2402.11773](https://arxiv.org/abs/2402.11773)

    提出了一种新方法，Dynamic Multi-network Mining (DMM)，能够将张量时间序列转换为不同长度的段组，通过稀疏依赖网络提供聚类的可解释性和精确性。

    

    时间序列的子序列聚类是数据挖掘中的一个重要任务，解释结果聚类也至关重要，因为通常我们没有关于数据的先验知识。因此，面对由包含时间戳在内的多种模式组成的大量张量时间序列，我们如何为张量时间序列实现子序列聚类并提供可解释的见解？在本文中，我们提出了一种新方法，即动态多网络挖掘（DMM），它将张量时间序列转换为由l1范数约束的一组各种长度的段组（即聚类）特征化的依赖网络。我们的方法具有以下特性。(a) 可解释性：它使用多个网络对聚类进行特征描述，每个网络是相应非时间模式的稀疏依赖网络，从而提供可见且可解释的关键关系见解。 (b) 精确性：它发现了聚类。。。

    arXiv:2402.11773v1 Announce Type: cross  Abstract: Subsequence clustering of time series is an essential task in data mining, and interpreting the resulting clusters is also crucial since we generally do not have prior knowledge of the data. Thus, given a large collection of tensor time series consisting of multiple modes, including timestamps, how can we achieve subsequence clustering for tensor time series and provide interpretable insights? In this paper, we propose a new method, Dynamic Multi-network Mining (DMM), that converts a tensor time series into a set of segment groups of various lengths (i.e., clusters) characterized by a dependency network constrained with l1-norm. Our method has the following properties. (a) Interpretable: it characterizes the cluster with multiple networks, each of which is a sparse dependency network of a corresponding non-temporal mode, and thus provides visible and interpretable insights into the key relationships. (b) Accurate: it discovers the clus
    
[^89]: 评估基于指数的治疗分配有效性

    Evaluating the Effectiveness of Index-Based Treatment Allocation

    [https://arxiv.org/abs/2402.11771](https://arxiv.org/abs/2402.11771)

    本文介绍了一种评估基于指数的资源分配策略有效性的方法，通过翻译和扩展统计文献中的最新思想，提供了有效的估计器和计算渐近正确置信区间的方法。

    

    当资源稀缺时，需要一种分配策略来决定谁能获得资源。本文介绍了一种评估基于指数的分配策略的方法，该策略通过使用随机对照试验的数据，将有限数量的资源分配给最需要的人。我们从统计文献中翻译和扩展了最近的想法，提出了一种高效的估计器和计算渐近正确置信区间的方法，从而有效地得出有效的统计结论。

    arXiv:2402.11771v1 Announce Type: cross  Abstract: When resources are scarce, an allocation policy is needed to decide who receives a resource. This problem occurs, for instance, when allocating scarce medical resources and is often solved using modern ML methods. This paper introduces methods to evaluate index-based allocation policies -- that allocate a fixed number of resources to those who need them the most -- by using data from a randomized control trial. Such policies create dependencies between agents, which render the assumptions behind standard statistical tests invalid and limit the effectiveness of estimators. Addressing these challenges, we translate and extend recent ideas from the statistics literature to present an efficient estimator and methods for computing asymptotically correct confidence intervals. This enables us to effectively draw valid statistical conclusions, a critical gap in previous work. Our extensive experiments validate our methodology in practical sett
    
[^90]: 基于ChatGPT的数据增强技术用于改善LLMs的参数高效去偏见化

    ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs

    [https://arxiv.org/abs/2402.11764](https://arxiv.org/abs/2402.11764)

    本研究提出了一种利用ChatGPT生成合成训练数据来增强LLMs去偏见化的新方法，能够高效地去除已知偏见并跨越不同类别进行去偏见化。

    

    大语言模型（LLMs）虽然功能强大，但存在有害的社会偏见。由于计算成本、数据约束和可能降低多任务语言能力，去偏见化通常具有挑战性。本文介绍了一种利用ChatGPT生成合成训练数据的新方法，旨在增强LLMs的去偏见化。我们提出了两种策略：目标提示，对已知偏见提供有效的去偏见化，但需要事先指定问题中的偏见; 一般提示，虽然效果稍逊，但能够跨各种类别进行去偏见化。我们利用适配器调整来实现资源高效的LLM去偏见化，并比较了我们的合成数据与现有去偏见化数据集的效果。我们的结果表明：（1）ChatGPT可以高效地生成用于去偏见化其他LLMs的高质量训练数据；（2）通过我们的方法生成的数据超越了现有数据集在去偏见化上的效果。

    arXiv:2402.11764v1 Announce Type: cross  Abstract: Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debias
    
[^91]: ArtPrompt: 基于ASCII艺术的对齐LLMs越狱攻击

    ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs

    [https://arxiv.org/abs/2402.11753](https://arxiv.org/abs/2402.11753)

    提出了一种新颖的基于ASCII艺术的越狱攻击，以及一个用于评估LLMs在识别非纯语义提示方面能力的基准挑战。五个SOTA LLMs在识别ASCII艺术提示时存在困难。

    

    安全对于大型语言模型（LLMs）的使用至关重要。已经开发了多种技术，如数据过滤和监督微调，以加强LLMs的安全性。然而，当前已知的技术假设用于对齐LLMs安全性的语料库仅由语义进行解释。然而，这一假设在现实应用中不成立，导致LLMs存在严重漏洞。本文提出了一种新颖的基于ASCII艺术的越狱攻击，并引入了一个全面的基准Vision-in-Text Challenge（ViTC）来评估LLMs在识别不能仅通过语义进行解释的提示的能力。我们展示了五个SOTA LLMs（GPT-3.5、GPT-4、Gemini、Claude和Llama2）在识别以ASCII艺术形式提供的提示方面存在困难。基于这一观察，我们开发了

    arXiv:2402.11753v1 Announce Type: cross  Abstract: Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we devel
    
[^92]: 对角化SGD：通过重新参数化和平滑实现非可微模型的快速收敛SGD

    Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing

    [https://arxiv.org/abs/2402.11752](https://arxiv.org/abs/2402.11752)

    引入了Diagonalisation Stochastic Gradient Descent（对角化SGD），通过重新参数化和平滑实现非可微模型的快速收敛SGD，在实证评估中表现出简单、快速、稳定，并且取得了数量级的工作规范化方差降低。

    

    众所周知，对于非可微模型，展现出较低方差的重新参数化梯度估计器在实践中存在偏差。这可能危及基于梯度的优化方法（如随机梯度下降SGD）的正确性。我们引入了一个简单的语法框架来分块地定义非可微函数，并提出了一种系统方法，以获得使重新参数化梯度估计器无偏的平滑。我们的主要贡献是一种新颖的SGD变体，对角化随机梯度下降，它在优化过程中逐步提高平滑近似的准确性，并证明收敛到未平滑（原始）目标的稳定点。我们的实证评估显示，与现有技术相比，我们的方法简单、快速、稳定，并且在工作规范化方差上实现了数量级的降低。

    arXiv:2402.11752v1 Announce Type: cross  Abstract: It is well-known that the reparameterisation gradient estimator, which exhibits low variance in practice, is biased for non-differentiable models. This may compromise correctness of gradient-based optimisation methods such as stochastic gradient descent (SGD). We introduce a simple syntactic framework to define non-differentiable functions piecewisely and present a systematic approach to obtain smoothings for which the reparameterisation gradient estimator is unbiased. Our main contribution is a novel variant of SGD, Diagonalisation Stochastic Gradient Descent, which progressively enhances the accuracy of the smoothed approximation during optimisation, and we prove convergence to stationary points of the unsmoothed (original) objective. Our empirical evaluation reveals benefits over the state of the art: our approach is simple, fast, stable and attains orders of magnitude reduction in work-normalised variance.
    
[^93]: 语言模型就是霍默·辛普森！通过任务算法对精调语言模型进行安全重新定位

    Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic

    [https://arxiv.org/abs/2402.11746](https://arxiv.org/abs/2402.11746)

    提出了一种简单方法RESTA，通过任务算法对精调语言模型进行安全重新定位，有效降低了其有害程度。

    

    经过微调的对齐语言模型往往会导致安全性受损，为了解决这一问题，我们提出了一种简单的方法RESTA，该方法执行LLM安全重新定位。RESTA代表通过任务算法恢复安全。在其核心思想中，它涉及将一个安全向量简单地加到受损模型的权重上。我们展示了RESTA在参数高效和完全微调中的有效性，涵盖了广泛的下游任务，包括中文、英文和印地文的指令跟随，以及代码和数学的问题解决能力。我们还展示了RESTA在三个现有安全评估基准和作为本项工作一部分提出的一个多语言基准数据集上的通用性，该数据集包括550个有害问题，涵盖11个类别，每个类别下包含5个有害的子类别。总的来说，RESTA降低了受损模型的有害程度。

    arXiv:2402.11746v1 Announce Type: cross  Abstract: Aligned language models face a significant limitation as their fine-tuning often results in compromised safety. To tackle this, we propose a simple method RESTA that performs LLM safety realignment. RESTA stands for REstoring Safety through Task Arithmetic. At its core, it involves a simple arithmetic addition of a safety vector to the weights of the compromised model. We demonstrate the effectiveness of RESTA in both parameter-efficient and full fine-tuning, covering a wide range of downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math. We also showcase the generalizability of RESTA on three existing safety evaluation benchmarks and a multilingual benchmark dataset proposed as a part of this work, consisting of 550 harmful questions covering 11 categories, each with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of the compromised 
    
[^94]: 基于模型等效评估的前馈神经网络压缩修复方法

    Compression Repair for Feedforward Neural Networks Based on Model Equivalence Evaluation

    [https://arxiv.org/abs/2402.11737](https://arxiv.org/abs/2402.11737)

    提出了一种基于模型等效评估的前馈神经网络压缩修复方法，通过计算两个神经网络之间的输出差异，初始化新的训练集并进行重新训练来改进压缩网络性能

    

    本文提出了一种基于两个神经网络等效评估的方法，用于修复压缩的前馈神经网络（FNNs）。在修复框架中，开发了一种新颖的神经网络等效评估方法来计算两个神经网络之间的输出差异。输出差异可以定量表征压缩过程产生的输出差异。根据计算得到的输出差异，修复方法首先为压缩网络初始化一个新的训练集，以缩小两个神经网络间的差异并改进压缩网络的性能。然后，我们通过基于训练集的重新训练来修复压缩的 FNN。我们将所开发的方法应用于 MNIST 数据集，以展示我们提出的修复方法的有效性和优势。

    arXiv:2402.11737v1 Announce Type: cross  Abstract: In this paper, we propose a method of repairing compressed Feedforward Neural Networks (FNNs) based on equivalence evaluation of two neural networks. In the repairing framework, a novel neural network equivalence evaluation method is developed to compute the output discrepancy between two neural networks. The output discrepancy can quantitatively characterize the output difference produced by compression procedures. Based on the computed output discrepancy, the repairing method first initializes a new training set for the compressed networks to narrow down the discrepancy between the two neural networks and improve the performance of the compressed network. Then, we repair the compressed FNN by re-training based on the training set. We apply our developed method to the MNIST dataset to demonstrate the effectiveness and advantages of our proposed repair method.
    
[^95]: 使用大型语言模型解决数据中心任务

    Solving Data-centric Tasks using Large Language Models

    [https://arxiv.org/abs/2402.11734](https://arxiv.org/abs/2402.11734)

    本文提出了两点贡献：一是创建了一个真实世界的NL-to-code任务数据集，二是引入了一种聚类然后选择提示技术，从输入数据中添加最具代表性的行到LLM提示中。

    

    大型语言模型（LLMs）正在迅速取代像StackOverflow这样的帮助论坛，并且对于非专业程序员和最终用户特别有帮助。这些用户通常对数据中心任务感兴趣，例如电子表格操作和数据处理，如果仅通过自然语言描述传达意图而不包含数据，这些任务很难解决。但是，我们如何决定在提示中包含多少数据和哪些数据？本文对回答这个问题做出了两点贡献。首先，我们创建了一个从StackOverflow帖子中获取的操作表格数据的真实NL-to-code任务数据集。其次，我们引入了一种聚类然后选择提示技术，将输入数据中最具代表性的行添加到LLM提示中。我们的实验表明，LLM的性能确实对传递到提示中的数据量敏感，并且对于具有大量语法变体的任务，传递的数据量较大。

    arXiv:2402.11734v1 Announce Type: cross  Abstract: Large language models (LLMs) are rapidly replacing help forums like StackOverflow, and are especially helpful for non-professional programmers and end users. These users are often interested in data-centric tasks, such as spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including the data. But how do we decide how much data and which data to include in the prompt? This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a cluster-then-select prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic vari
    
[^96]: 随机遗忘对稳健泛化的效果

    The Effectiveness of Random Forgetting for Robust Generalization

    [https://arxiv.org/abs/2402.11733](https://arxiv.org/abs/2402.11733)

    FOMO引入了一种新的学习范式，通过随机遗忘部分权重来调节信息并强调学习可泛化的特征，从而显著减少神经网络在对抗攻击下出现的稳健过拟合问题。

    

    深度神经网络容易受到对抗攻击，这可能损害它们的性能和准确性。对抗训练（AT）已经成为保护神经网络免受此类攻击的常见方法。然而，AT的关键挑战之一是稳健过拟合，即网络对测试数据的稳健性能随着进一步训练而恶化，从而妨碍泛化。受大脑中主动遗忘的概念启发，我们引入了一种名为“遗忘来减轻过拟合（FOMO）”的新型学习范式。FOMO在遗忘阶段随机遗忘部分权重并通过权重重新初始化调节模型的信息，然后在重新学习阶段强调学习可泛化的特征。我们在基准数据集和对抗攻击上的实验表明，FOMO通过显著减少最佳结果和最终结果之间的差距，缓解了稳健过拟合。

    arXiv:2402.11733v1 Announce Type: cross  Abstract: Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network's robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called "Forget to Mitigate Overfitting (FOMO)". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model's information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last
    
[^97]: Prospector Heads:大规模模型和数据的广义特征归因

    Prospector Heads: Generalized Feature Attribution for Large Models & Data

    [https://arxiv.org/abs/2402.11729](https://arxiv.org/abs/2402.11729)

    Prospector heads是一种高效且可解释的基于特征归因的替代方法，它可以应用于任何编码器和任何数据形态，并且通过对不同数据形态的实验，表现优越于传统方法。

    

    特征归因是一种定位输入数据中与分类相关的区域的能力，对于科学和生物医学领域的机器学习模型而言，这是一种重要的能力。当前的特征归因方法依赖于“解释”端到端分类器的预测，存在特征定位不精确以及由于计算挑战而无法在小样本尺寸和高维数据集上使用的问题。我们引入了探寻者头部（prospector heads），这是一种高效且可解释的基于特征归因的替代方法，可以应用于任何编码器和任何数据形态。通过对序列（文本）、图像（病理学）和图（蛋白质结构）的实验，探寻者头部在模态之间进行概括，表现优于基线归因方法，平均局部化AUPRC得分提升了高达49点。我们还演示了探寻者头部如何实现了改进的解释能力。

    arXiv:2402.11729v1 Announce Type: cross  Abstract: Feature attribution, the ability to localize regions of the input data that are relevant for classification, is an important capability for machine learning models in scientific and biomedical domains. Current methods for feature attribution, which rely on "explaining" the predictions of end-to-end classifiers, suffer from imprecise feature localization and are inadequate for use with small sample sizes and high-dimensional datasets due to computational challenges. We introduce prospector heads, an efficient and interpretable alternative to explanation-based methods for feature attribution that can be applied to any encoder and any data modality. Prospector heads generalize across modalities through experiments on sequences (text), images (pathology), and graphs (protein structures), outperforming baseline attribution methods by up to 49 points in mean localization AUPRC. We also demonstrate how prospector heads enable improved interpr
    
[^98]: GNNavi：通过图神经网络导航大型语言模型中的信息流

    GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network

    [https://arxiv.org/abs/2402.11709](https://arxiv.org/abs/2402.11709)

    GNNavi通过引入基于提示的参数高效微调（PEFT）方法和图神经网络（GNN）层，准确引导信息流的汇聚和分布，在大型语言模型中导航信息流动态，超越了标准提示式微调的性能。

    

    大型语言模型（LLMs）在接收示范输入时表现出强大的上下文学习能力（ICL）。然而，微调仍然至关重要以进一步增强其适应性。基于提示的微调方法在数据稀缺情况下证明是有效的微调方法，但对计算资源的高需求限制了其实用性。我们通过引入基于提示的参数高效微调（PEFT）方法来解决这个问题。GNNavi利用了有关ICL信息流动态的见解，表明标签词在提示中作为信息传播的锚点。GNNavi利用图神经网络（GNN）层精确地引导信息流的汇聚和分布，在处理提示时将期望的信息流硬编码到GNN中。我们在使用GPT-2和Llama2进行文本分类任务的实验中发现，GNNavi超越了标准提示式微调的性能。

    arXiv:2402.11709v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are applied to them. However, fine-tuning still remains crucial to further enhance their adaptability. Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN. Our experiments on text classification tasks with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-ba
    
[^99]: 搜索引擎在ChatGPT之后：生成式人工智能如何降低搜索的可靠性

    Search Engines Post-ChatGPT: How Generative Artificial Intelligence Could Make Search Less Reliable

    [https://arxiv.org/abs/2402.11707](https://arxiv.org/abs/2402.11707)

    讨论了生成式人工智能对搜索引擎的影响，指出了其可能导致的可靠性问题，强调了信息来源不透明和内容正确性等挑战。

    

    在这篇评论中，我们讨论了搜索引擎的发展性质，因为它们开始生成、索引和分发由生成式人工智能（GenAI）创建的内容。我们的讨论突出了在GenAI整合的早期阶段面临的挑战，特别是围绕事实上的不一致性和偏见。我们讨论了GenAI产生的输出带来了无端的可信度，并降低了透明度和信息来源的能力。此外，搜索引擎已经用含有错误的生成内容回答查询，进一步模糊了信息来源，影响了信息生态系统的完整性。我们认为所有这些因素可能降低搜索引擎的可靠性。最后，我们总结了一些活跃的研究方向和未解问题。

    arXiv:2402.11707v1 Announce Type: cross  Abstract: In this commentary, we discuss the evolving nature of search engines, as they begin to generate, index, and distribute content created by generative artificial intelligence (GenAI). Our discussion highlights challenges in the early stages of GenAI integration, particularly around factual inconsistencies and biases. We discuss how output from GenAI carries an unwarranted sense of credibility, while decreasing transparency and sourcing ability. Furthermore, search engines are already answering queries with error-laden, generated content, further blurring the provenance of information and impacting the integrity of the information ecosystem. We argue how all these factors could reduce the reliability of search engines. Finally, we summarize some of the active research directions and open questions.
    
[^100]: ChatGPT是否能支持开发者？对大型语言模型用于代码生成的实证评估

    Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation

    [https://arxiv.org/abs/2402.11702](https://arxiv.org/abs/2402.11702)

    大型语言模型在代码生成方面表现出显著能力，但目前主要用于展示概念或提供示例，需要进一步改进才能实现生产就绪代码。

    

    大型语言模型（LLMs）已经展示出在代码生成方面的显著能力，许多先前的研究显示它们在各种开发场景中具有很大潜力。然而，这些研究主要提供了在研究环境中的评估，这在理解LLM在实际世界中能有效支持开发者方面留下了显著的空白。为了解决这个问题，我们对DevGPT中的对话进行了实证分析，这是从开发者与ChatGPT的对话中收集的数据集（通过GitHub等平台上的Share Link功能捕获）。我们的实证结果表明，目前使用LLM生成的代码的实践通常仅限于展示高层概念或提供文档中的示例，而不是作为可用于生产的代码。这些发现表明，在LLM代码生成方面还需要大量未来工作才能使其成为不可或缺的组成部分。

    arXiv:2402.11702v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers' conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts o
    
[^101]: 利用GPT4V合成数据实现轻量级视觉-语言模型ALLaVA

    ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model

    [https://arxiv.org/abs/2402.11684](https://arxiv.org/abs/2402.11684)

    通过采用GPT-4V合成的高质量训练数据，研究成功地实现了ALLaVA，一个轻量级视觉-语言模型，该模型在12个基准测试上表现出与最多3B LVLMs竞争性能。

    

    最近，大型视觉-语言模型(LVLMs)的发展使语言模型能够处理多模态输入，但部署时需要大量计算资源，尤其是在边缘设备上。本研究旨在通过采用高质量的训练数据来弥合传统尺度LVLMs和资源友好型Lite版本之间的性能差距。为此，通过利用GPT-4V生成详细描述、复杂推理指令和图片详细答案的能力创建了一个合成数据集。利用我们的数据训练的结果模型ALLaVA在12项基准测试上取得了与最多3B LVLMs竞争性能。这项工作突出了在设计更高效的LVLMs中采用高质量数据的可行性。我们的在线演示可在\url{https://allava.freedomai.cn}上获得。

    arXiv:2402.11684v1 Announce Type: cross  Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have enabled processing of multimodal inputs in language models but require significant computational resources for deployment, especially in edge devices. This study aims to bridge the performance gap between traditional-scale LVLMs and resource-friendly lite versions by adopting high-quality training data. To do this, a synthetic dataset is created by leveraging GPT-4V's ability to generate detailed captions, complex reasoning instructions and detailed answers from images. The resulted model trained with our data, ALLaVA, achieves competitive performance on 12 benchmarks up to 3B LVLMs. This work highlights the feasibility of adopting high-quality data in crafting more efficient LVLMs. Our online demo is available at \url{https://allava.freedomai.cn}.
    
[^102]: 用循环神经网络和图像压缩方法对3D点云进行压缩

    3D Point Cloud Compression with Recurrent Neural Network and Image Compression Methods

    [https://arxiv.org/abs/2402.11680](https://arxiv.org/abs/2402.11680)

    提出一种使用循环神经网络和图像压缩方法对3D点云进行压缩的新方法，通过独特的3D到2D转换和密集的表示结构，在压缩中高效利用空间相关性。

    

    存储和传输LiDAR点云数据对于许多自动驾驶应用至关重要，例如训练数据收集、远程控制、云服务或SLAM。然而，由于数据的稀疏性和无序结构，将点云数据压缩到较低体积是困难的。将原始点云数据转换为稠密的2D矩阵结构是一种应用压缩算法的有前途的方法。我们提出了一种新的无损和校准的3D到2D转换，允许压缩算法有效地利用2D表示中的空间相关性。为了压缩结构化表示，我们使用常见的图像压缩方法，还使用了一个自监督深度压缩方法，该方法采用循环神经网络。我们还将LiDAR的强度测量重新排列成稠密的2D表示，并提出了一个新的度量标准来评估强度的压缩性能。

    arXiv:2402.11680v1 Announce Type: cross  Abstract: Storing and transmitting LiDAR point cloud data is essential for many AV applications, such as training data collection, remote control, cloud services or SLAM. However, due to the sparsity and unordered structure of the data, it is difficult to compress point cloud data to a low volume. Transforming the raw point cloud data into a dense 2D matrix structure is a promising way for applying compression algorithms. We propose a new lossless and calibrated 3D-to-2D transformation which allows compression algorithms to efficiently exploit spatial correlations within the 2D representation. To compress the structured representation, we use common image compression methods and also a self-supervised deep compression approach using a recurrent neural network. We also rearrange the LiDAR's intensity measurements to a dense 2D representation and propose a new metric to evaluate the compression performance of the intensity. Compared to approaches 
    
[^103]: MultiCorrupt：一种用于3D物体检测的LiDAR-相机融合的多模态鲁棒性数据集和基准

    MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of LiDAR-Camera Fusion for 3D Object Detection

    [https://arxiv.org/abs/2402.11677](https://arxiv.org/abs/2402.11677)

    MultiCorrupt是用于评估多模态3D目标检测器在十种不同数据损坏类型下的鲁棒性的综合基准。

    

    多模态3D物体检测模型在计算机视觉基准数据集如nuScenes上展现出了出色的表现，但它们对密集采样的LiDAR点云和精心校准的传感器阵列依赖性使得在实际应用中面临挑战。为解决这一挑战，我们介绍了MultiCorrupt，一个旨在评估多模态3D目标检测器对十种不同类型数据损坏的鲁棒性的综合基准。

    arXiv:2402.11677v1 Announce Type: cross  Abstract: Multi-modal 3D object detection models for automated driving have demonstrated exceptional performance on computer vision benchmarks like nuScenes. However, their reliance on densely sampled LiDAR point clouds and meticulously calibrated sensor arrays poses challenges for real-world applications. Issues such as sensor misalignment, miscalibration, and disparate sampling frequencies lead to spatial and temporal misalignment in data from LiDAR and cameras. Additionally, the integrity of LiDAR and camera data is often compromised by adverse environmental conditions such as inclement weather, leading to occlusions and noise interference. To address this challenge, we introduce MultiCorrupt, a comprehensive benchmark designed to evaluate the robustness of multi-modal 3D object detectors against ten distinct types of corruptions. We evaluate five state-of-the-art multi-modal detectors on MultiCorrupt and analyze their performance in terms of
    
[^104]: 使用大型语言模型进行反叙事评估的多方面框架

    A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models

    [https://arxiv.org/abs/2402.11676](https://arxiv.org/abs/2402.11676)

    提出了一个多方面框架，使用大型语言模型评估反叙事，通过5个方面从专门 NGO 指南中提取定义的内容，以解决以往评估方法的局限性。

    

    反叙事是对仇恨言论背景的知情回应，旨在驳斥仇恨主张并化解冲突，已成为一种有效的仇恨言论干预策略。先前的工作提出了自动生成反叙事的方法来辅助手动干预，但这些方法的评估仍未得到充分发展。先前用于反叙事评估的自动度量标准缺乏与人类判断的一致性，因为它们依赖于表面参考比较，而不是将反叙事质量的关键方面纳入评估标准。为解决先前的评估局限性，我们提出了一个新颖的评估框架，促使LLM提供生成的反叙事候选的得分和反馈，使用了来自专门NGO的反叙事指南中提取的5个定义的方面。

    arXiv:2402.11676v1 Announce Type: cross  Abstract: Counter narratives - informed responses to hate speech contexts designed to refute hateful claims and de-escalate encounters - have emerged as an effective hate speech intervention strategy. While previous work has proposed automatic counter narrative generation methods to aid manual interventions, the evaluation of these approaches remains underdeveloped. Previous automatic metrics for counter narrative evaluation lack alignment with human judgment as they rely on superficial reference comparisons instead of incorporating key aspects of counter narrative quality as evaluation criteria. To address prior evaluation limitations, we propose a novel evaluation framework prompting LLMs to provide scores and feedback for generated counter narrative candidates using 5 defined aspects derived from guidelines from counter narrative specialized NGOs. We found that LLM evaluators achieve strong alignment to human-annotated scores and feedback and
    
[^105]: 爱沙尼亚文本的自动校正：EKTB25项目的最终报告

    Autocorrect for Estonian texts: final report from project EKTB25

    [https://arxiv.org/abs/2402.11671](https://arxiv.org/abs/2402.11671)

    该项目成功开发了爱沙尼亚语言的拼写和语法校正工具，主要创新在于使用迁移学习和自动评估来克服可用数据不足的挑战。

    

    该项目由爱沙尼亚语言技术国家计划于2021-2023年资助，旨在为爱沙尼亚语开发拼写和语法校正工具。主要挑战是缺乏所需用于此类开发的可用错误校正数据量非常小。为了缓解这一问题，(1) 我们为模型训练和测试注释了更多的校正数据，(2) 我们测试了迁移学习，即重新训练为其他任务创建的机器学习模型，以便不仅依赖于校正数据，(3) 我们对比了开发的方法和模型与其他选择，包括大型语言模型。我们还开发了自动评估，可以通过错误类别计算修正的准确性和收益，从而可以详细比较不同方法的有效性。

    arXiv:2402.11671v1 Announce Type: cross  Abstract: The project was funded in 2021-2023 by the National Programme of Estonian Language Technology. Its main aim was to develop spelling and grammar correction tools for the Estonian language. The main challenge was the very small amount of available error correction data needed for such development. To mitigate this, (1) we annotated more correction data for model training and testing, (2) we tested transfer-learning, i.e. retraining machine learning models created for other tasks, so as not to depend solely on correction data, (3) we compared the developed method and model with alternatives, including large language models. We also developed automatic evaluation, which can calculate the accuracy and yield of corrections by error category, so that the effectiveness of different methods can be compared in detail.   There has been a breakthrough in large language models during the project: GPT4, a commercial language model with Estonian-lang
    
[^106]: 分层主动推断中的动态规划

    Dynamic planning in hierarchical active inference

    [https://arxiv.org/abs/2402.11658](https://arxiv.org/abs/2402.11658)

    通过研究在动态规划领域中模拟工具使用的目标，我们深入探讨了主动推断中的动态规划，该领域考虑到生物目标导向行为的两个关键方面

    

    通过动态规划，我们指的是人类大脑推断和施加与认知决策相关的运动轨迹的能力。最近的一个范式，主动推断，为生物有机体适应带来了基本见解，不断努力最小化预测误差以将自己限制在与生命兼容的状态。在过去的几年里，许多研究表明人类和动物行为可以解释为主动推断过程，无论是作为离散决策还是连续运动控制，都激发了机器人技术和人工智能中的创新解决方案。然而，文献缺乏对如何有效地在变化环境中规划行动的全面展望。我们设定了对工具使用进行建模的目标，深入研究了主动推断中的动态规划主题，牢记两个生物目标导向行为的关键方面：理解……

    arXiv:2402.11658v1 Announce Type: new  Abstract: By dynamic planning, we refer to the ability of the human brain to infer and impose motor trajectories related to cognitive decisions. A recent paradigm, active inference, brings fundamental insights into the adaptation of biological organisms, constantly striving to minimize prediction errors to restrict themselves to life-compatible states. Over the past years, many studies have shown how human and animal behavior could be explained in terms of an active inferential process -- either as discrete decision-making or continuous motor control -- inspiring innovative solutions in robotics and artificial intelligence. Still, the literature lacks a comprehensive outlook on how to effectively plan actions in changing environments. Setting ourselves the goal of modeling tool use, we delve into the topic of dynamic planning in active inference, keeping in mind two crucial aspects of biological goal-directed behavior: the capacity to understand a
    
[^107]: 移动边缘计算中组合式客户端-主控多智能体深度强化学习用于任务卸载

    Combinatorial Client-Master Multiagent Deep Reinforcement Learning for Task Offloading in Mobile Edge Computing

    [https://arxiv.org/abs/2402.11653](https://arxiv.org/abs/2402.11653)

    深度强化学习在移动边缘计算中的任务卸载问题中的应用面临着连续和离散资源约束的挑战，但有望实现高效的任务分配。

    

    最近，出现了大量执行计算密集型任务的移动应用程序，如视频流媒体、数据挖掘、虚拟现实、增强现实、图像处理、视频处理、人脸识别和在线游戏。移动边缘计算（MEC）已经成为一种满足用户设备（UDs）日益增长的计算需求的有前途的技术。MEC中的任务卸载是一种策略，通过在UDs和MEC服务器之间分配任务来满足UDs的需求。深度强化学习（DRL）在任务卸载问题中受到关注，因为它可以适应动态变化并最小化在线计算复杂性。然而，UDs和MEC服务器上的各种类型的连续和离散资源约束对有效的基于DRL的任务卸载设计构成挑战。

    arXiv:2402.11653v1 Announce Type: new  Abstract: Recently, there has been an explosion of mobile applications that perform computationally intensive tasks such as video streaming, data mining, virtual reality, augmented reality, image processing, video processing, face recognition, and online gaming. However, user devices (UDs), such as tablets and smartphones, have a limited ability to perform the computation needs of the tasks. Mobile edge computing (MEC) has emerged as a promising technology to meet the increasing computing demands of UDs. Task offloading in MEC is a strategy that meets the demands of UDs by distributing tasks between UDs and MEC servers. Deep reinforcement learning (DRL) is gaining attention in task-offloading problems because it can adapt to dynamic changes and minimize online computational complexity. However, the various types of continuous and discrete resource constraints on UDs and MEC servers pose challenges to the design of an efficient DRL-based task-offlo
    
[^108]: 借助机器学习进行量子图像去噪：改进量子图像处理质量和可靠性的新方法

    Quantum Image Denoising with Machine Learning: A Novel Approach to Improve Quantum Image Processing Quality and Reliability

    [https://arxiv.org/abs/2402.11645](https://arxiv.org/abs/2402.11645)

    提出一种新方法，通过使用机器学习模型识别和校正量子处理图像中的噪声，以提高量子图像处理的质量和可靠性，实现与经典计算机类似的处理结果。

    

    Quantum Image Processing（QIP）是一个旨在利用量子计算优势来操作和分析图像的领域。然而，QIP面临两个挑战：量子比特的限制和量子机器中存在的噪声。在这项研究中，我们提出了一种新方法来解决QIP中的噪声问题。通过训练和使用一个机器学习模型，该模型能够识别并校正量子处理图像中的噪声，我们可以弥补机器造成的嘈杂，并以比经典计算机更高的效率检索出与之类似的处理结果。该模型通过学习包括现有处理图像和来自开放获取数据集的量子处理图像的数据集进行训练。该模型将能够为我们提供每个像素的置信水平及其潜在原始值。为了评估该模型在弥补Q处理中的损失和退相干方面的准确性

    arXiv:2402.11645v1 Announce Type: cross  Abstract: Quantum Image Processing (QIP) is a field that aims to utilize the benefits of quantum computing for manipulating and analyzing images. However, QIP faces two challenges: the limitation of qubits and the presence of noise in a quantum machine. In this research we propose a novel approach to address the issue of noise in QIP. By training and employing a machine learning model that identifies and corrects the noise in quantum processed images, we can compensate for the noisiness caused by the machine and retrieve a processing result similar to that performed by a classical computer with higher efficiency. The model is trained by learning a dataset consisting of both existing processed images and quantum processed images from open access datasets. This model will be capable of providing us with the confidence level for each pixel and its potential original value. To assess the model's accuracy in compensating for loss and decoherence in Q
    
[^109]: 逻辑闭环：揭示大型视觉-语言模型中的对象幻觉

    Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models

    [https://arxiv.org/abs/2402.11622](https://arxiv.org/abs/2402.11622)

    提出了一种基于逻辑闭环的框架（LogicCheckGPT），利用大型视觉-语言模型本身来检测和减轻对象幻觉。

    

    对象幻觉一直是阻碍大型视觉-语言模型（LVLMs）更广泛应用的软肋。对象幻觉是指LVLMs在图像中声称不存在的对象的现象。为了减轻对象幻觉，已经提出了指导调整和基于外部模型的检测方法，这两种方法要么需要大规模的计算资源，要么依赖于外部模型的检测结果。然而，仍然存在一个未深入探讨的领域，即利用LVLM本身来减轻对象幻觉。在这项工作中，我们采用了这样的直觉，即LVLM倾向于对存在的对象做出逻辑一致的反应，但对幻觉对象做出不一致的反应。因此，我们提出了基于逻辑闭环的对象幻觉检测和减轻框架，即LogicCheckGPT。具体来说，我们设计了逻辑一致性探测来提出具有逻辑性的问题。

    arXiv:2402.11622v1 Announce Type: cross  Abstract: Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical corr
    
[^110]: 在线机器学习中的超参数调优简化--spotRiverGUI

    Simplifying Hyperparameter Tuning in Online Machine Learning -- The spotRiverGUI

    [https://arxiv.org/abs/2402.11594](https://arxiv.org/abs/2402.11594)

    `spotRiverGUI`是一个为在线机器学习模型进行超参数调优的图形用户界面，简化了用户手动搜索最佳超参数设置的过程。

    

    批量机器学习(BML)在处理大量流数据时存在内存、数据流漂移处理和处理新的未知数据等方面的限制。在线机器学习(OML)是BML的替代方案，能够以顺序方式处理数据，特别适用于数据流。`river`包是一个Python OML库，提供了各种在线学习算法，包括分类、回归、聚类、异常检测等。`spotRiver`包为OML模型提供了超参数调优的框架。`spotRiverGUI`是`spotRiver`包的图形用户界面，为用户提供了从强大的算法中选择最优超参数设置的便利。

    arXiv:2402.11594v1 Announce Type: cross  Abstract: Batch Machine Learning (BML) reaches its limits when dealing with very large amounts of streaming data. This is especially true for available memory, handling drift in data streams, and processing new, unknown data. Online Machine Learning (OML) is an alternative to BML that overcomes the limitations of BML. OML is able to process data in a sequential manner, which is especially useful for data streams. The `river` package is a Python OML-library, which provides a variety of online learning algorithms for classification, regression, clustering, anomaly detection, and more. The `spotRiver` package provides a framework for hyperparameter tuning of OML models. The `spotRiverGUI` is a graphical user interface for the `spotRiver` package. The `spotRiverGUI` releases the user from the burden of manually searching for the optimal hyperparameter setting. After the data is provided, users can compare different OML algorithms from the powerful `
    
[^111]: 带Transformer的脉冲扩散模型

    SDiT: Spiking Diffusion Model with Transformer

    [https://arxiv.org/abs/2402.11588](https://arxiv.org/abs/2402.11588)

    本文提出了一种新颖的脉冲扩散模型架构，通过在脉冲神经网络中利用Transformer取代U-net结构，在图像生成任务中取得了较高质量的图像，并提供了基于SNN的生成模型研究的实证基准。

    

    脉冲神经网络（SNNs）具有低功耗和生物解释特性，被认为在节能计算方面有巨大潜力。然而，在图像生成任务中探索SNNs的工作仍然非常有限，尚未提出基于SNN的生成模型的统一且有效的结构。本文探讨了脉冲神经网络中的一种新型扩散模型架构。我们利用Transformer来取代主流扩散模型中常用的U-net结构。它能够以相对较低的计算成本和较短的采样时间生成质量更高的图像。它旨在为基于SNN的生成模型研究提供实证基准。在MNIST、Fashion-MNIST和CIFAR-10数据集上的实验证明，我们的工作与现有的SNN生成模型相比具有很高的竞争力。

    arXiv:2402.11588v1 Announce Type: cross  Abstract: Spiking neural networks (SNNs) have low power consumption and bio-interpretable characteristics, and are considered to have tremendous potential for energy-efficient computing. However, the exploration of SNNs on image generation tasks remains very limited, and a unified and effective structure for SNN-based generative models has yet to be proposed. In this paper, we explore a novel diffusion model architecture within spiking neural networks. We utilize transformer to replace the commonly used U-net structure in mainstream diffusion models. It can generate higher quality images with relatively lower computational cost and shorter sampling time. It aims to provide an empirical baseline for research of generative models based on SNNs. Experiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our work is highly competitive compared to existing SNN generative models.
    
[^112]: 使用Haru开发自主机器人介导的行为辅导会话

    Developing Autonomous Robot-Mediated Behavior Coaching Sessions with Haru

    [https://arxiv.org/abs/2402.11569](https://arxiv.org/abs/2402.11569)

    通过Haru开发了一个完全自主的对话系统，最大限度地发挥了其情感表达和独特人格，成功应用于行为改变辅导，取得了显著的成效。

    

    这项研究探讨了在人机交互中为行为改变辅导设计和影响自主对话的经验调查。我们关注使用桌面社交机器人Haru，并探索实施微习惯方法来促进积极行为改变。我们研究的核心是开发一个完全自主的对话系统，最大限度地发挥Haru的情感表达和独特人格。我们的方法涉及对对话系统的迭代设计和广泛测试，确保它有效地体现了微习惯方法的原则，同时还融合了建立信任和破坏信任的策略。最终版本对话的有效性在人类参与者（N=12）的实验研究中进行了评估。结果显示Haru的生气、互动性和中立性的认知显著提高。

    arXiv:2402.11569v1 Announce Type: cross  Abstract: This study presents an empirical investigation into the design and impact of autonomous dialogues in human-robot interaction for behavior change coaching. We focus on the use of Haru, a tabletop social robot, and explore the implementation of the Tiny Habits method for fostering positive behavior change. The core of our study lies in developing a fully autonomous dialogue system that maximizes Haru's emotional expressiveness and unique personality. Our methodology involved iterative design and extensive testing of the dialogue system, ensuring it effectively embodied the principles of the Tiny Habits method while also incorporating strategies for trust-raising and trust-dampening. The effectiveness of the final version of the dialogue was evaluated in an experimental study with human participants (N=12). The results indicated a significant improvement in perceptions of Haru's liveliness, interactivity, and neutrality. Additionally, our
    
[^113]: 图上的持续学习：挑战、解决方案和机会

    Continual Learning on Graphs: Challenges, Solutions, and Opportunities

    [https://arxiv.org/abs/2402.11565](https://arxiv.org/abs/2402.11565)

    对图上持续学习进行了全面评估和分类，弥补了欧几里得数据上持续学习研究的不足。

    

    近来，图数据上的持续学习因旨在解决现有任务上的灾难性遗忘问题以及将顺序更新的模型适应新出现的图任务而引起了极大关注。虽然人们努力总结了在欧几里得数据（例如图像和文本）上持续学习研究的进展，但对于图上的持续学习，即持续图学习（CGL）或终身图学习，仍然需要进行系统性审查。图数据在数据结构和应用场景方面要复杂得多，这使得CGL任务设置、模型设计和应用变得极具挑战性。为了弥合差距，我们通过阐明不同的任务设置并根据特性对现有的持续图学习（CGL）算法进行分类，提供了对现有持续图学习（CGL）算法的全面评估。我们将CGL方法与传统的持续学习方法进行比较。

    arXiv:2402.11565v1 Announce Type: cross  Abstract: Continual learning on graph data has recently attracted paramount attention for its aim to resolve the catastrophic forgetting problem on existing tasks while adapting the sequentially updated model to newly emerged graph tasks. While there have been efforts to summarize progress on continual learning research over Euclidean data, e.g., images and texts, a systematic review of progress in continual learning on graphs, a.k.a, continual graph learning (CGL) or lifelong graph learning, is still demanding. Graph data are far more complex in terms of data structures and application scenarios, making CGL task settings, model designs, and applications extremely challenging. To bridge the gap, we provide a comprehensive review of existing continual graph learning (CGL) algorithms by elucidating the different task settings and categorizing the existing methods based on their characteristics. We compare the CGL methods with traditional continual
    
[^114]: LongAgent: 通过多智能体协作将语言模型扩展到128K上下文

    LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration

    [https://arxiv.org/abs/2402.11550](https://arxiv.org/abs/2402.11550)

    LongAgent通过多智能体协作将语言模型扩展到128K上下文，并在长文本处理方面表现出潜在的优越性。

    

    大型语言模型（LLMs）在理解语言和执行复杂推理任务方面表现出色。然而，具有长上下文窗口的LLMs以其昂贵的训练成本和高推理延迟而臭名昭著。即使是最先进的模型如GPT-4和Claude2在处理超过$100k$标记的输入时也经常出错，这种现象也被称为\textit{中间迷失}。在本文中，我们提出了基于多智能体协作的方法\textsc{LongAgent}，将LLMs（例如LLaMA）扩展到128K上下文，并展示出在长文本处理方面可能优于GPT-4的潜力。在\textsc{LongAgent}中，一位领导者负责理解用户意图并指导团队成员从文档中获取信息。由于成员存在幻觉，领导者从几十到数百名成员的回应中获取准确信息并非易事。

    arXiv:2402.11550v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated impressive performance in understanding language and executing complex reasoning tasks. However, LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \textit{lost in the middle}. In this paper, we propose \textsc{LongAgent}, a method based on multi-agent collaboration, which scales LLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority in long-text processing compared to GPT-4. In \textsc{LongAgent}, a leader is responsible for understanding user intent and directing team members to acquire information from documents. Due to members' hallucinations, it is non-trivial for a leader to obtain accurate information from the responses of dozens to hundreds of member
    
[^115]: 英语和德语的句法语言变化：度量、解析器和趋同

    Syntactic Language Change in English and German: Metrics, Parsers, and Convergences

    [https://arxiv.org/abs/2402.11549](https://arxiv.org/abs/2402.11549)

    本文研究英语和德语句法语言变化趋势，使用议会辩论语料库，探讨了句法依存距离最小化及基于树图属性的15个度量标准，揭示了现代解析器在这种变化中的影响。

    

    许多研究表明，人类语言往往会优化语言结构以降低复杂性，增加交流效率。句法依存距离衡量了相关词汇之间的线性距离，通常被认为是语言处理困难和工作记忆负荷的关键指标。本文研究了英语和德语句法语言变化的历时趋势，使用了过去大约160年间的议会辩论语料库。我们基于5个依存句法解析器的观察结果，包括广泛使用的Stanford CoreNLP以及其他4个更新的替代方案。我们的句法语言变化分析超越了线性依存距离，探讨了与依存距离最小化（DDM）相关的15个度量标准，或者基于树图属性，比如树高和度变异。尽管我们有证据表明，最近基于现代树库训练的解析器并未受到重大影响。

    arXiv:2402.11549v1 Announce Type: cross  Abstract: Many studies have shown that human languages tend to optimize for lower complexity and increased communication efficiency. Syntactic dependency distance, which measures the linear distance between dependent words, is often considered a key indicator of language processing difficulty and working memory load. The current paper looks at diachronic trends in syntactic language change in both English and German, using corpora of parliamentary debates from the last c. 160 years. We base our observations on five dependency parsers, including the widely used Stanford CoreNLP as well as 4 newer alternatives. Our analysis of syntactic language change goes beyond linear dependency distance and explores 15 metrics relevant to dependency distance minimization (DDM) and/or based on tree graph properties, such as the tree height and degree variance. Even though we have evidence that recent parsers trained on modern treebanks are not heavily affected 
    
[^116]: 基于时空知识图的问答系统

    Question Answering Over Spatio-Temporal Knowledge Graph

    [https://arxiv.org/abs/2402.11542](https://arxiv.org/abs/2402.11542)

    介绍了一个新的基于时空知识图的问答系统STQAD，以解决问答系统在涵盖时空信息的问题上的挑战，提出了一种新的STComplEx嵌入方法STCQA来实现此目标

    

    时空知识图（STKG）通过整合时间和位置信息扩展了知识图（KG）的概念。尽管研究界关注知识图问答（KGQA），但基于STKG的涵盖时空信息的问题回答领域仍未被广泛探讨。此外，缺乏全面的数据集也阻碍了该领域的进展。为解决这一问题，我们提出了STQAD，这是一个包括10,000个自然语言问题的面向时空知识图问答（STKGQA）数据集。不幸的是，各种最先进的KGQA方法在我们的数据集上远未达到令人满意的性能。为此，我们提出了STCQA，一种新的时空KGQA方法，利用了一种名为STComplEx的新型STKG嵌入方法。通过从问题中提取时间和空间信息，我们的问答模型可以更好地理解问题。

    arXiv:2402.11542v1 Announce Type: cross  Abstract: Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledge graphs (KGs) by incorporating time and location information. While the research community's focus on Knowledge Graph Question Answering (KGQA), the field of answering questions incorporating both spatio-temporal information based on STKGs remains largely unexplored. Furthermore, a lack of comprehensive datasets also has hindered progress in this area. To address this issue, we present STQAD, a dataset comprising 10,000 natural language questions for spatio-temporal knowledge graph question answering (STKGQA). Unfortunately, various state-of-the-art KGQA approaches fall far short of achieving satisfactory performance on our dataset. In response, we propose STCQA, a new spatio-temporal KGQA approach that utilizes a novel STKG embedding method named STComplEx. By extracting temporal and spatial information from a question, our QA model can better comprehend the quest
    
[^117]: 逆向认知：大型语言模型比我们想象的更擅长理解知识图谱

    Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought

    [https://arxiv.org/abs/2402.11541](https://arxiv.org/abs/2402.11541)

    本文通过对KG知识注入方法进行全面比较，探索为LLMs提供知识图谱知识的最佳方法，以增强它们的理解能力。

    

    虽然通过使用知识图谱（KGs）来增强大型语言模型（LLMs）的推理能力并减少它们的幻觉的方法受到了广泛关注，但目前对如何使LLMs能够即时整合KGs中的结构化知识的探索还不足。本文采用复杂问题回答（CQA）作为一项任务，评估LLM理解KG知识的能力。我们对KG知识注入方法进行了全面比较（从三元组到自然语言文本），旨在探索为LLMs提供KG知识的最佳提示方法，从而增强它们的理解能力。

    arXiv:2402.11541v1 Announce Type: cross  Abstract: Although the method of enhancing large language models' (LLMs') reasoning ability and reducing their hallucinations through the use of knowledge graphs (KGs) has received widespread attention, the exploration of how to enable LLMs to integrate the structured knowledge in KGs on-the-fly remains inadequate. Researchers often co-train KG embeddings and LLM parameters to equip LLMs with the ability of comprehending KG knowledge. However, this resource-hungry training paradigm significantly increases the model learning cost and is also unsuitable for non-open-source, black-box LLMs. In this paper, we employ complex question answering (CQA) as a task to assess the LLM's ability of comprehending KG knowledge. We conducted a comprehensive comparison of KG knowledge injection methods (from triples to natural language text), aiming to explore the optimal prompting method for supplying KG knowledge to LLMs, thereby enhancing their comprehension o
    
[^118]: 通过机器去学习研究预训练数据对大型语言模型的影响

    Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning

    [https://arxiv.org/abs/2402.11537](https://arxiv.org/abs/2402.11537)

    通过对五个主要类别的预训练数据的48个数据集进行系统分析，研究了它们对大型语言模型性能的影响，并发现了一些“高影响数据”，如书籍，与模型能力相关联，为LLMs的优化提供了见解。

    

    通过在具有各种来源的语料库上进行预训练，大型语言模型（LLMs）取得了令人印象深刻的性能。然而，预训练语料库的每个组成部分的影响仍然不明确。因此，预训练语料库的组织仍然是经验性的，并且可能偏离最佳状态。为了解决这个问题，我们系统地分析了来自LLMs预训练数据的5个主要类别的48个数据集的影响，并使用关于九个主要模型能力类别的基准来衡量它们对LLMs的影响。我们的分析提供了关于多个语料库对LLMs性能贡献的实证结果，以及它们的联合影响模式，包括互补的、正交的和相关的关系。我们还确定了一组“高影响数据”，如书籍，与一组模型能力相关联。这些发现为我们提供了关于组织数据以支持LLMs优化的见解。

    arXiv:2402.11537v1 Announce Type: cross  Abstract: Through pretraining on a corpus with various sources, Large Language Models (LLMs) have gained impressive performance. However, the impact of each component of the pretraining corpus remains opaque. As a result, the organization of the pretraining corpus is still empirical and may deviate from the optimal. To address this issue, we systematically analyze the impact of 48 datasets from 5 major categories of pretraining data of LLMs and measure their impacts on LLMs using benchmarks about nine major categories of model capabilities. Our analyses provide empirical results about the contribution of multiple corpora on the performances of LLMs, along with their joint impact patterns, including complementary, orthogonal, and correlational relationships. We also identify a set of ``high-impact data'' such as Books that is significantly related to a set of model capabilities. These findings provide insights into the organization of data to sup
    
[^119]: PreAct: 在 ReAct 中预测未来增强智能体的规划能力

    PreAct: Predicting Future in ReAct Enhances Agent's Planning Ability

    [https://arxiv.org/abs/2402.11534](https://arxiv.org/abs/2402.11534)

    PreAct是一个整合了预测、推理和行动的智能体框架，利用预测信息可以帮助智能体进行更多样化和策略性的推理，导致更有效的行动，提升任务完成效率。

    

    处理预测与实际结果之间的差异常常有助于个体拓展思维过程并进行反思，从而促进朝正确方向推理。本文介绍了一种名为 PreAct 的智能体框架，它将预测、推理和行动整合在一起。利用预测提供的信息，基于大语言模型（LLM）的智能体能够提供更多样化和策略导向的推理，进而导致更有效的行动，帮助智能体完成复杂任务。我们的实验表明，PreAct 在完成复杂任务方面优于 ReAct 方法，并且当与反思方法结合时，PreAct 的效果可以得到提升。我们对模型提供不同数量的历史预测，并发现历史预测对LLM规划有持续积极影响。

    arXiv:2402.11534v1 Announce Type: cross  Abstract: Addressing the discrepancies between predictions and actual outcomes often aids individuals in expanding their thought processes and engaging in reflection, thereby facilitating reasoning in the correct direction. In this paper, we introduce $\textbf{PreAct}$, an agent framework that integrates $\textbf{pre}$diction with $\textbf{rea}$soning and $\textbf{act}$ion. Leveraging the information provided by predictions, a large language model (LLM) based agent can offer more diversified and strategically oriented reasoning, which in turn leads to more effective actions that help the agent complete complex tasks. Our experiments demonstrate that PreAct outperforms the ReAct approach in accomplishing complex tasks and that PreAct can be co-enhanced when combined with Reflexion methods. We prompt the model with different numbers of historical predictions and find that historical predictions have a sustained positive effect on LLM planning. The
    
[^120]: 基于邻域增强的监督对比学习用于协同过滤

    Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative Filtering

    [https://arxiv.org/abs/2402.11523](https://arxiv.org/abs/2402.11523)

    本文提出了一种基于邻域增强的监督对比学习方法，通过将锚节点的协作邻居视为正样本，有效解决了协同过滤中数据稀疏的问题

    

    尽管在推荐任务中有效，协同过滤（CF）技术面临着数据稀疏的挑战。研究人员已经开始利用对比学习引入额外的自监督信号来解决这一问题。然而，这种方法往往会无意中将目标用户/项目与他们的协作邻居分开，从而限制其有效性。为了应对这一挑战，我们提出了一个解决方案，将锚节点的协作邻居视为最终目标损失函数中的正样本。本文专注于开发两个独特的监督对比损失函数，有效结合了监督信号和对比损失。我们通过梯度视角分析了我们提出的损失函数，证明了不同的正样本同时影响更新锚节点的嵌入。这些样本的影响取决于它们与锚节点和负样本的相似性。

    arXiv:2402.11523v1 Announce Type: cross  Abstract: While effective in recommendation tasks, collaborative filtering (CF) techniques face the challenge of data sparsity. Researchers have begun leveraging contrastive learning to introduce additional self-supervised signals to address this. However, this approach often unintentionally distances the target user/item from their collaborative neighbors, limiting its efficacy. In response, we propose a solution that treats the collaborative neighbors of the anchor node as positive samples within the final objective loss function. This paper focuses on developing two unique supervised contrastive loss functions that effectively combine supervision signals with contrastive loss. We analyze our proposed loss functions through the gradient lens, demonstrating that different positive samples simultaneously influence updating the anchor node's embeddings. These samples' impact depends on their similarities to the anchor node and the negative sample
    
[^121]: 在异构语言任务和客户资源下对大型语言模型进行联邦微调

    Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources

    [https://arxiv.org/abs/2402.11505](https://arxiv.org/abs/2402.11505)

    该研究引入了FlexLoRA，一个简单而有效的大型语言模型微调聚合方案，能够在联邦学习中充分利用异质客户资源，通过动态调整本地LoRA排名和采用奇异值分解进行权重重新分配，提升全局模型的广泛知识。

    

    近期，联邦学习（FL）被应用于对大型语言模型（LLMs）进行参数高效微调。然而，由于客户的资源和数据分布不均匀，这引发了重大挑战。本研究引入了FlexLoRA，这是一种简单而有效的LLM微调聚合方案，它可以缓解传统FL中的“桶效应”，该效应限制了拥有丰富资源的客户实现潜力，将他们与最缺乏资源的参与者的能力捆绑在一起。FlexLoRA允许动态调整本地LoRA排名，促进全局模型的发展，并赋予更广泛、不太任务特定的知识。通过从个体客户贡献中合成完整大小的LoRA权重，并利用奇异值分解（SVD）进行权重重新分配，FlexLoRA充分利用了客户间的资源差异。本研究涉及超过1600个执行多样NLP任务的客户。

    arXiv:2402.11505v1 Announce Type: cross  Abstract: Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective aggregation scheme for LLM fine-tuning, which mitigates the "buckets effect" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600 clients performing diverse NLP tasks, ou
    
[^122]: 使用基础模型可验证地遵循复杂机器人指令

    Verifiably Following Complex Robot Instructions with Foundation Models

    [https://arxiv.org/abs/2402.11498](https://arxiv.org/abs/2402.11498)

    提出了一种名为语言指令地面化运动规划（LIMP）系统，利用基础模型和时间逻辑生成指令条件的语义地图，使机器人能够可验证地遵循富有表现力和长期的指令，包括开放词汇参照和复杂的时空约束。

    

    让机器人能够遵循复杂的自然语言指令是一个重要但具有挑战性的问题。人们希望在指导机器人时能够灵活表达约束，指向任意地标并验证行为。相反，机器人必须将人类指令消除歧义，将指令参照物联系到真实世界中。我们提出了一种名为语言指令地面化运动规划（LIMP）的系统，该系统利用基础模型和时间逻辑生成指令条件的语义地图，使机器人能够可验证地遵循富有表现力和长期的指令，涵盖了开放词汇参照和复杂的时空约束。与先前在机器人任务执行中使用基础模型的方法相比，LIMP构建了一个可解释的指令表示，揭示了机器人与指导者预期动机的一致性，并实现了机器人行为的综合。

    arXiv:2402.11498v1 Announce Type: cross  Abstract: Enabling robots to follow complex natural language instructions is an important yet challenging problem. People want to flexibly express constraints, refer to arbitrary landmarks and verify behavior when instructing robots. Conversely, robots must disambiguate human instructions into specifications and ground instruction referents in the real world. We propose Language Instruction grounding for Motion Planning (LIMP), a system that leverages foundation models and temporal logics to generate instruction-conditioned semantic maps that enable robots to verifiably follow expressive and long-horizon instructions with open vocabulary referents and complex spatiotemporal constraints. In contrast to prior methods for using foundation models in robot task execution, LIMP constructs an explainable instruction representation that reveals the robot's alignment with an instructor's intended motives and affords the synthesis of robot behaviors that 
    
[^123]: LEIA: 利用基于实体的数据增强在语言模型中促进跨语言知识转移

    LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation

    [https://arxiv.org/abs/2402.11485](https://arxiv.org/abs/2402.11485)

    LEIA是一种语言适应调整方法，利用维基百科实体名称跨语言增强目标语言语料库，通过左到右的语言建模训练，显著提高了各种非英语语言的表现。

    

    将基于英语的大型语言模型（LLMs）适应其他语言的操作由于跨语言转移的效率和潜力而变得越来越受欢迎。然而，现有的语言适应方法常常忽视跨语言监督的好处。在本研究中，我们介绍LEIA，一种利用跨语言对齐的维基百科实体名称的语言适应调整方法。该方法涉及使用英语实体名称增强目标语言语料库，并使用从左到右的语言建模训练模型。我们在多样的问答数据集上评估LEIA，使用7B参数的LLMs，展示了在各种非英语语言上的显著性能增益。源代码可在https://github.com/studio-ousia/leia上获得。

    arXiv:2402.11485v1 Announce Type: cross  Abstract: Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages. The source code is available at https://github.com/studio-ousia/leia.
    
[^124]: 基于图提示学习的药物相互作用事件预测：DDIPrompt

    DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning

    [https://arxiv.org/abs/2402.11472](https://arxiv.org/abs/2402.11472)

    基于图提示学习的DDIPrompt框架旨在解决药物相互作用事件预测中的高度不平衡事件分布和罕见事件标记数据稀缺性问题。

    

    最近，由于其在建模药物分子内部和之间原子和功能团之间复杂关联方面的熟练表现，图神经网络在预测药物相互作用事件（DDI）方面变得日益普遍。然而，它们仍然受到两个重大挑战的制约：（1）高度不平衡事件分布的问题，在医学数据集中这是一个常见但关键的问题，某些相互作用被广泛地低估。这种不平衡对实现准确可靠的DDI预测构成了重大障碍。（2）罕见事件标记数据的稀缺性，在医学领域是一个普遍问题，由于数据有限，往往忽视或研究不足的罕见但潜在关键的相互作用。为此，我们提出了DDIPrompt，这是一种受最近图提示学进展启发的创新良方。我们的框架旨在解决这些问题。

    arXiv:2402.11472v1 Announce Type: cross  Abstract: Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data. In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in graph prompting. Our framework aims to address these issue
    
[^125]: 长期时间序列预测中的吸引子记忆：混沌视角

    Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective

    [https://arxiv.org/abs/2402.11463](https://arxiv.org/abs/2402.11463)

    Attraos模型基于混沌理论，在长期时间序列预测中利用多尺度动态记忆单元和局部演化策略，表现优异于其他LTSF方法。

    

    在长期时间序列预测（LTSF）任务中，现有的深度学习模型忽视了离散时间序列源自潜在连续动态系统的关键特征，导致缺乏外推和演化能力。 鉴别真实世界数据的混沌性质，我们的模型\textbf{\textit{Attraos}}将混沌理论融入到LTSF中，将实际时间序列视为未知高维混沌动态系统的观测。 在吸引子不变性的概念下，Attraos利用提出的多尺度动态记忆单元来记忆历史动态结构，并通过频率增强的局部演化策略进行预测。 详细的理论分析和丰富的经验证据一致表明，Attraos在主流LTSF数据集和混沌数据集上的表现优于各种LTSF方法。

    arXiv:2402.11463v1 Announce Type: cross  Abstract: In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets.
    
[^126]: FGeo-HyperGNet: 几何问题求解中集成形式符号系统和超图神经网络

    FGeo-HyperGNet: Geometry Problem Solving Integrating Formal Symbolic System and Hypergraph Neural Network

    [https://arxiv.org/abs/2402.11461](https://arxiv.org/abs/2402.11461)

    该论文提出了FGeo-HyperGNet，将形式符号系统和超图神经网络集成，用于解决几何问题，实现自动执行人类化的几何演绎推理。

    

    几何问题的求解一直是自动推理和人工智能领域中长期存在的挑战。这是我们系列作品中的第五篇文章，我们构建了一个神经符号系统，用于自动执行类似人类的几何演绎推理。符号部分是建立在FormalGeo上的形式系统，可以自动执行几何关系推理和代数计算，并将求解过程组织成一个带有条件作为超节点和定理作为超边的解决方案超树。神经部分称为HyperGNet，是基于注意力机制的超图神经网络，包括一个编码器来有效编码超树的结构和语义信息，以及一个求解器来提供问题求解指导。神经部分根据超树预测定理，而符号部分应用定理并更新超树，从而形成一个预测-

    arXiv:2402.11461v1 Announce Type: new  Abstract: Geometry problem solving has always been a long-standing challenge in the fields of automated reasoning and artificial intelligence. This is the fifth article in a series of our works, we built a neural-symbolic system to automatically perform human-like geometric deductive reasoning. The symbolic part is a formal system built on FormalGeo, which can automatically perform geomertic relational reasoning and algebraic calculations and organize the solving process into a solution hypertree with conditions as hypernodes and theorems as hyperedges. The neural part, called HyperGNet, is a hypergraph neural network based on the attention mechanism, including a encoder to effectively encode the structural and semantic information of the hypertree, and a solver to provide problem-solving guidance. The neural part predicts theorems according to the hypertree, and the symbolic part applies theorems and updates the hypertree, thus forming a Predict-
    
[^127]: Re-Dock: 朝向具有扩散桥的灵活和现实分子对接

    Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge

    [https://arxiv.org/abs/2402.11459](https://arxiv.org/abs/2402.11459)

    提出了一种新颖的扩散桥生成模型 Re-Dock，用于灵活和现实的分子对接，通过能量到几何映射来共同建模结合能和构象，填补了对接中的实用性和构象预测方面的差距

    

    准确预测蛋白质-配体结合结构，即分子对接任务对于药物设计至关重要，但仍然具有挑战性。尽管深度学习显示出了潜力，但现有方法通常依赖于完整蛋白质结构（对接，且在现实任务中不可达）或忽略口袋侧链构象，导致有限的实用性和不切实际的构象预测。为填补这些差距，我们引入了一个未经探索的任务，命名为柔性对接，以同时预测配体和口袋侧链的姿势，并引入了一种扩展到几何流形的新型扩散桥生成模型 Re-Dock。具体而言，我们提出了受牛顿-欧拉方程启发的能量到几何映射，以共同建模结合能和构象，以反映能量约束对接生成过程。我们在设计的基准数据集上进行了全面的实验，包括apo-dock和cross-dock d

    arXiv:2402.11459v1 Announce Type: cross  Abstract: Accurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging. While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions. To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds. Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process. Comprehensive experiments on designed benchmark datasets including apo-dock and cross-dock d
    
[^128]: SciAgent: 工具增强型语言模型用于科学推理

    SciAgent: Tool-augmented Language Models for Scientific Reasoning

    [https://arxiv.org/abs/2402.11451](https://arxiv.org/abs/2402.11451)

    引入了工具增强型科学推理的新任务设置，通过提供可扩展的工具集，帮助大型语言模型在科学问题解决中变得更加实用和可解决。

    

    科学推理对于即使最先进的大型语言模型（LLMs）来说也是一项巨大挑战。为了使LLMs更加实用和可解决此任务，我们引入了一种名为工具增强型科学推理的新任务设置。这种设置通过为LLMs提供可扩展的工具集，将重点从追求全知问题求解器转变为熟练使用工具的人。为了促进这种设置的研究，我们构建了一个名为MathFunc的工具增强型训练语料库，涵盖了超过30,000个样本和大约6,000个工具。基于MathFunc，我们开发了SciAgent，用于检索、理解，以及必要时使用工具进行科学问题解决。此外，我们构建了一个名为SciToolBench的基准，涵盖五个科学领域，以评估LLMs在工具辅助下的能力。对SciToolBench进行的大量实验验证了SciAgent的有效性。值得注意的是，SciAgent-Mistral-7B超过了其他LLMs。

    arXiv:2402.11451v1 Announce Type: cross  Abstract: Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs' abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other LLMs with the sa
    
[^129]: InfuserKI：通过Infuser引导的知识集成增强大型语言模型与知识图谱

    InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration

    [https://arxiv.org/abs/2402.11441](https://arxiv.org/abs/2402.11441)

    提出了一种Infuser-Guided Knowledge Integration（InfuserKI）框架，利用transformer内部状态有效地将未知知识集成到大型语言模型中，从而缓解知识遗忘问题。

    

    大型语言模型（LLMs）在各个领域展现出卓越的开放式生成能力，但在知识密集型任务中表现不佳。为了缓解这一问题，提出了知识集成方法，利用外部模块将领域特定知识图谱与LLMs结合起来。然而，它们存在数据效率低的问题，因为它们需要已知和未知的知识来进行微调。因此，我们研究了一个新颖的问题，即如何在不重复已知知识的情况下有效地将未知知识集成到LLMs中。注入新知识会导致遗忘先前获得的知识的风险。为了解决这个问题，我们提出了一种新颖的Infuser-Guided Knowledge Integration（InfuserKI）框架，利用transformer内部状态来确定是否应该增强原始LLM输出信息，从而有效地减轻知识遗忘问题。在UMLS-2.5k和MetaQ上进行了评估。

    arXiv:2402.11441v1 Announce Type: cross  Abstract: Though Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks. To alleviate this issue, knowledge integration methods have been proposed to enhance LLMs with domain-specific knowledge graphs using external modules. However, they suffer from data inefficiency as they require both known and unknown knowledge for fine-tuning. Thus, we study a novel problem of integrating unknown knowledge into LLMs efficiently without unnecessary overlap of known knowledge. Injecting new knowledge poses the risk of forgetting previously acquired knowledge. To tackle this, we propose a novel Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes transformer internal states to determine whether to enhance the original LLM output with additional information, thereby effectively mitigating knowledge forgetting. Evaluations on the UMLS-2.5k and MetaQ
    
[^130]: 自我反馈的危险：在大型语言模型中自我偏见被放大

    Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models

    [https://arxiv.org/abs/2402.11436](https://arxiv.org/abs/2402.11436)

    大型语言模型存在自我偏见，研究发现通过更大的模型规模和准确评估的外部反馈可以显著减少这种偏见，并提高后续任务的实际表现。

    

    最近的研究表明，自我反馈可以改善大型语言模型在某些任务上的表现，但在其他任务上却会恶化。我们发现这种矛盾是由于大型语言模型对其自身输出的偏见。本文正式定义了大型语言模型的自我偏见——倾向于偏爱自身生成——并使用两个统计量进行了分析。我们在翻译、受限文本生成和数学推理任务上分析了六种大型语言模型。我们发现自我偏见在所有检测的大型语言模型中都普遍存在，跨多种语言和任务。我们的分析表明，虽然自我改进管道提高了模型输出的流畅性和可理解性，但它进一步放大了自我偏见。为了缓解这种偏见，我们发现更大的模型规模和具有准确评估的外部反馈可以显著减少自我改进管道中的偏见，从而实际改善下游任务的性能。

    arXiv:2402.11436v1 Announce Type: cross  Abstract: Recent studies show that self-feedback improves large language models (LLMs) on certain tasks while worsens other tasks. We discovered that such a contrary is due to LLM's bias towards their own output. In this paper, we formally define LLM's self-bias -- the tendency to favor its own generation -- using two statistics. We analyze six LLMs on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.
    
[^131]: OptEx: 利用近似并行化迭代加速一阶优化

    OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations

    [https://arxiv.org/abs/2402.11427](https://arxiv.org/abs/2402.11427)

    OptEx是第一个通过利用并行计算来减轻一阶优化的迭代瓶颈并增强效率的框架，使用核化梯度估计实现迭代的并行化，提供理论保证。

    

    第一阶优化（FOO）算法在诸如机器学习和信号去噪等众多计算领域中至关重要。然而，将它们应用于神经网络训练等复杂任务往往导致显著的低效，因为需要许多顺序迭代以实现收敛。为此，我们引入了第一阶优化加速近似并行迭代（OptEx），这是第一个通过利用并行计算来减轻其迭代瓶颈而增强FOO效率的框架。OptEx采用核化梯度估计来利用梯度历史进行未来梯度预测，实现了迭代的并行化 -- 这是一种曾经被认为由于FOO中固有的迭代依赖而不切实际的策略。我们为我们的核化梯度估计的可靠性和基于SGD的OptEx的迭代复杂度提供理论保证，并确认了其可靠性。

    arXiv:2402.11427v1 Announce Type: cross  Abstract: First-order optimization (FOO) algorithms are pivotal in numerous computational domains such as machine learning and signal denoising. However, their application to complex tasks like neural network training often entails significant inefficiencies due to the need for many sequential iterations for convergence. In response, we introduce first-order optimization expedited with approximately parallelized iterations (OptEx), the first framework that enhances the efficiency of FOO by leveraging parallel computing to mitigate its iterative bottleneck. OptEx employs kernelized gradient estimation to make use of gradient history for future gradient prediction, enabling parallelization of iterations -- a strategy once considered impractical because of the inherent iterative dependency in FOO. We provide theoretical guarantees for the reliability of our kernelized gradient estimation and the iteration complexity of SGD-based OptEx, confirming t
    
[^132]: 用于广义零样本识别的数据分布精馏生成模型

    Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition

    [https://arxiv.org/abs/2402.11424](https://arxiv.org/abs/2402.11424)

    在广义零样本识别中解决已见数据偏见问题的D$^3$GZSL框架，通过引入分布内双空间精馏和分布外批次精馏模块，实现了更平衡的模型学习。

    

    在零样本学习领域中，我们解决了偏向已见数据的广义零样本学习模型中存在的偏见。为了应对这一问题，我们引入了一个端到端的生成式广义零样本学习框架，称为D$^3$GZSL。该框架将已见和合成的未见数据分别视为分布内和分布外数据，以获得更加平衡的模型。D$^3$GZSL包括两个核心模块：分布内双空间精馏（ID$^2$SD）和分布外批次精馏（O$^2$DBD）。ID$^2$SD在嵌入和标签空间中对师生输出进行对齐，增强了学习的一致性。O$^2$DBD引入了每个批次样本的低维分布外表示，捕捉了已见和未见类别之间的共享结构。我们的方法在已建立的广义零样本学习基准测试中展示了其有效性，并能够无缝集成到主流生成式框架中。大量实验一致地表明...

    arXiv:2402.11424v1 Announce Type: cross  Abstract: In the realm of Zero-Shot Learning (ZSL), we address biases in Generalized Zero-Shot Learning (GZSL) models, which favor seen data. To counter this, we introduce an end-to-end generative GZSL framework called D$^3$GZSL. This framework respects seen and synthesized unseen data as in-distribution and out-of-distribution data, respectively, for a more balanced model. D$^3$GZSL comprises two core modules: in-distribution dual space distillation (ID$^2$SD) and out-of-distribution batch distillation (O$^2$DBD). ID$^2$SD aligns teacher-student outcomes in embedding and label spaces, enhancing learning coherence. O$^2$DBD introduces low-dimensional out-of-distribution representations per batch sample, capturing shared structures between seen and unseen categories. Our approach demonstrates its effectiveness across established GZSL benchmarks, seamlessly integrating into mainstream generative frameworks. Extensive experiments consistently showc
    
[^133]: LoRETTA: 低秩经济张量训练适应大型语言模型的超低参数微调

    LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models

    [https://arxiv.org/abs/2402.11417](https://arxiv.org/abs/2402.11417)

    LoRETTA是一个通过张量训练分解显著减少可训练参数的超低参数高效框架，在大型语言模型的微调中表现出与大多数PEFT方法相媲美甚至更好的性能。

    

    各种参数高效微调（PEFT）技术被提出以实现在保持模型性能的情况下进行计算高效的微调。然而，随着大型语言模型（LLMs）的快速部署，现有的PEFT方法仍然受到可训练参数数量增长的限制。为了解决这一挑战，我们提出了LoRETTA，这是一个超参数高效的框架，通过张量训练分解显著减少可训练参数。具体来说，我们提出了两种方法，分别命名为{LoRETTA}$_{adp}$和{LoRETTA}$_{rep}$。前者采用张量化适配器，为LLMs的微调提供了高性能且轻量级的方法。后者强调通过一组小张量因子进行权重参数化的微调。LoRETTA在LLaMA-2-7B模型上比大多数广泛使用的PEFT方法具有可比或更好的性能，并且参数少达到100倍。

    arXiv:2402.11417v1 Announce Type: cross  Abstract: Various parameter-efficient fine-tuning (PEFT) techniques have been proposed to enable computationally efficient fine-tuning while maintaining model performance. However, existing PEFT methods are still limited by the growing number of trainable parameters with the rapid deployment of Large Language Models (LLMs). To address this challenge, we present LoRETTA, an ultra-parameter-efficient framework that significantly reduces trainable parameters through tensor-train decomposition. Specifically, we propose two methods, named {LoRETTA}$_{adp}$ and {LoRETTA}$_{rep}$. The former employs tensorized adapters, offering a high-performance yet lightweight approach for the fine-tuning of LLMs. The latter emphasizes fine-tuning via weight parameterization with a set of small tensor factors. LoRETTA achieves comparable or better performance than most widely used PEFT methods with up to $100\times$ fewer parameters on the LLaMA-2-7B models. Further
    
[^134]: 对神经和神经符号方法在实时多模态复杂事件检测中的实证评估

    An Empirical Evaluation of Neural and Neuro-symbolic Approaches to Real-time Multimodal Complex Event Detection

    [https://arxiv.org/abs/2402.11403](https://arxiv.org/abs/2402.11403)

    本研究评估神经和神经符号方法在多模态复杂事件检测中的效果，特别关注时间推理，实验发现神经符号方法在较少数据下表现更好。

    

    Robots and autonomous systems require an understanding of complex events (CEs) from sensor data to interact with their environments and humans effectively. Traditional end-to-end neural architectures, despite processing sensor data efficiently, struggle with long-duration events due to limited context sizes and reasoning capabilities. Recent advances in neuro-symbolic methods, which integrate neural and symbolic models leveraging human knowledge, promise improved performance with less data. This study addresses the gap in understanding these approaches' effectiveness in complex event detection (CED), especially in temporal reasoning. We investigate neural and neuro-symbolic architectures' performance in a multimodal CED task, analyzing IMU and acoustic data streams to recognize CE patterns. Our methodology includes (i) end-to-end neural architectures for direct CE detection from sensor embeddings, (ii) two-stage concept-based neural mode

    arXiv:2402.11403v1 Announce Type: new  Abstract: Robots and autonomous systems require an understanding of complex events (CEs) from sensor data to interact with their environments and humans effectively. Traditional end-to-end neural architectures, despite processing sensor data efficiently, struggle with long-duration events due to limited context sizes and reasoning capabilities. Recent advances in neuro-symbolic methods, which integrate neural and symbolic models leveraging human knowledge, promise improved performance with less data. This study addresses the gap in understanding these approaches' effectiveness in complex event detection (CED), especially in temporal reasoning. We investigate neural and neuro-symbolic architectures' performance in a multimodal CED task, analyzing IMU and acoustic data streams to recognize CE patterns. Our methodology includes (i) end-to-end neural architectures for direct CE detection from sensor embeddings, (ii) two-stage concept-based neural mode
    
[^135]: 在比较之前进行推理：LLM增强的语义相似度度量用于领域专门文本分析

    Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis

    [https://arxiv.org/abs/2402.11398](https://arxiv.org/abs/2402.11398)

    通过利用LLM增强语义分析，开发了用于文本的相似度度量框架，可显著改善文本的语义相似性评估，并可扩展到其他专业领域。

    

    在这项研究中，我们利用LLM来增强语义分析，为文本开发相似度度量，解决传统无监督NLP度量（如ROUGE和BLEU）的局限性。我们开发了一个框架，其中LLM（例如GPT-4）用于零样本文本识别和放射学报告的标签生成，在那里这些标签然后被用作文本相似性的度量。通过在MIMIC数据集上测试所提出的框架，我们发现GPT-4生成的标签能够显著改善语义相似度评估，得分更接近临床实际情况比传统NLP度量。我们的工作展示了使用LLM进行高度专业领域的文本数据的语义分析的可能性，具有半定量推理结果。虽然该框架针对放射学报告相似性分析进行了实施，但其概念可以扩展到其他专门领域。

    arXiv:2402.11398v1 Announce Type: cross  Abstract: In this study, we leverage LLM to enhance the semantic analysis and develop similarity metrics for texts, addressing the limitations of traditional unsupervised NLP metrics like ROUGE and BLEU. We develop a framework where LLMs such as GPT-4 are employed for zero-shot text identification and label generation for radiology reports, where the labels are then used as measurements for text similarity. By testing the proposed framework on the MIMIC data, we find that GPT-4 generated labels can significantly improve the semantic similarity assessment, with scores more closely aligned with clinical ground truth than traditional NLP metrics. Our work demonstrates the possibility of conducting semantic analysis of the text data using semi-quantitative reasoning results by the LLMs for highly specialized domains. While the framework is implemented for radiology report similarity analysis, its concept can be extended to other specialized domains 
    
[^136]: 基于Transformer的全新肽段测序技术用于数据无偏向采集质谱

    Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry

    [https://arxiv.org/abs/2402.11363](https://arxiv.org/abs/2402.11363)

    这项研究提出了Casanovo-DIA，一种基于Transformer架构的深度学习模型，可用于从DIA质谱数据中解析肽段序列。

    

    串联质谱（MS/MS）作为全面分析生物样本中蛋白质含量的主要高通量技术，一直是推动蛋白质组学发展的基石。近年来，在数据无偏向采集（DIA）策略方面取得了实质性进展，有助于对前体离子进行公正和非靶向碎裂。由于其固有的高多重性特性，DIA生成的MS/MS谱图造成了巨大障碍。每个谱图都包含来自多个前体肽的碎裂产物离子。这种复杂性特别在全新肽段/蛋白质测序中构成了一个尖锐挑战，当前方法无法解决多重性难题。本文介绍了Casanovo-DIA，这是一个基于Transformer架构的深度学习模型，可以从DIA质谱数据中解析肽段序列。

    arXiv:2402.11363v1 Announce Type: cross  Abstract: Tandem mass spectrometry (MS/MS) stands as the predominant high-throughput technique for comprehensively analyzing protein content within biological samples. This methodology is a cornerstone driving the advancement of proteomics. In recent years, substantial strides have been made in Data-Independent Acquisition (DIA) strategies, facilitating impartial and non-targeted fragmentation of precursor ions. The DIA-generated MS/MS spectra present a formidable obstacle due to their inherent high multiplexing nature. Each spectrum encapsulates fragmented product ions originating from multiple precursor peptides. This intricacy poses a particularly acute challenge in de novo peptide/protein sequencing, where current methods are ill-equipped to address the multiplexing conundrum. In this paper, we introduce Casanovo-DIA, a deep-learning model based on transformer architecture. It deciphers peptide sequences from DIA mass spectrometry data. Our 
    
[^137]: 在不修改语言模型的情况下训练语言模型代理

    Training Language Model Agents without Modifying Language Models

    [https://arxiv.org/abs/2402.11359](https://arxiv.org/abs/2402.11359)

    提出一种新的方法，在不修改语言模型的情况下训练语言模型代理，通过进化代理的功能来解决下游任务

    

    研究人员和实践者最近已经将强大的大型语言模型（LLMs）重新定义为代理，使它们能够通过使用专门的功能自动化地完成复杂任务。为了促进LLM代理的发展，我们提出了一种在不修改LLM权重的情况下训练LLM代理的新范式，当LLM难以或无法进行修改时尤其有用。受到人类不断锻造工具以适应现实任务的启发，而不是改变我们的生物结构以适应一组静态工具，我们提出逐步锻造代理的功能，以更好地解决下游任务，而不是修改LLM权重。通过将这些功能视为可学习的“代理参数”并利用人工智能模型训练的基本思想，我们开发了AgentOptimizer，利用LLM更新代理的功能，并设计了一种代理训练算法

    arXiv:2402.11359v1 Announce Type: new  Abstract: Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an agent training algorithm with tw
    
[^138]: 基于概率路由的基于图的近似最近邻搜索

    Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search

    [https://arxiv.org/abs/2402.11354](https://arxiv.org/abs/2402.11354)

    该论文提出了一种基于概率路由的方法，通过引入PEOs有效识别图中需要考虑进行精确距离计算的邻居，从而显著提高了基于图的近似最近邻搜索的效率。

    

    arXiv：2402.11354v1 公告类型：交叉 摘要：在机器学习领域，高维空间中的近似最近邻搜索(ANNS)是一个重要挑战。近年来，基于图的方法已经成为ANNS的优越方法，建立了一种新的技术水平。尽管引入了各种基于图的ANNS优化方法，但它们主要依赖于缺乏正式理论支持的启发式方法。本文旨在通过引入一种方法来增强基于图的ANNS中的路由，该方法在探索图中节点的邻居时提供概率保证。我们将问题建模为概率路由，并通过结合局部敏感技术开发了两种基准策略。随后，我们介绍了PEOs，这是一种有效识别图中应考虑进行精确距离计算的邻居的新方法，从而在实践中显著提高了效率。我们的实验证明...

    arXiv:2402.11354v1 Announce Type: cross  Abstract: Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a pivotal challenge in the field of machine learning. In recent years, graph-based methods have emerged as the superior approach to ANNS, establishing a new state of the art. Although various optimizations for graph-based ANNS have been introduced, they predominantly rely on heuristic methods that lack formal theoretical backing. This paper aims to enhance routing within graph-based ANNS by introducing a method that offers a probabilistic guarantee when exploring a node's neighbors in the graph. We formulate the problem as probabilistic routing and develop two baseline strategies by incorporating locality-sensitive techniques. Subsequently, we introduce PEOs, a novel approach that efficiently identifies which neighbors in the graph should be considered for exact distance computation, thus significantly improving efficiency in practice. Our experiments demonstrate 
    
[^139]: 理解长期记忆对大型语言模型驱动聊天机器人在公共卫生干预中自我披露的影响

    Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention

    [https://arxiv.org/abs/2402.11353](https://arxiv.org/abs/2402.11353)

    长期记忆的引入提高了大型语言模型驱动聊天机器人在公共卫生干预中的用户自我披露，但仍存在挑战，特别是在解决慢性健康状况和隐私问题方面。

    

    最近的大型语言模型(LLMs)有潜力通过开放式对话支持公共卫生监测，但在多次互动中很少保留关于个人的知识。通过添加长期记忆(LTM)来增强LLMs提供了改进参与度和自我披露的机会，但我们缺乏对LTM如何影响人们在公共卫生干预中与LLM驱动的聊天机器人互动的理解。我们通过分析1,252通话记录和对九名用户的访谈，研究了CareCall这种带有LTM的LLM驱动的语音聊天机器人的案例。我们发现LTM增强了健康披露，并通过提供熟悉感促进了用户对聊天机器人的积极看法。然而，我们也观察到通过LTM促进自我披露存在挑战，尤其是在解决慢性健康状况和隐私问题方面。我们讨论了LTM整合的考虑。

    arXiv:2402.11353v1 Announce Type: cross  Abstract: Recent large language models (LLMs) offer the potential to support public health monitoring by facilitating health disclosure through open-ended conversations but rarely preserve the knowledge gained about individuals across repeated interactions. Augmenting LLMs with long-term memory (LTM) presents an opportunity to improve engagement and self-disclosure, but we lack an understanding of how LTM impacts people's interaction with LLM-driven chatbots in public health interventions. We examine the case of CareCall -- an LLM-driven voice chatbot with LTM -- through the analysis of 1,252 call logs and interviews with nine users. We found that LTM enhanced health disclosure and fostered positive perceptions of the chatbot by offering familiarity. However, we also observed challenges in promoting self-disclosure through LTM, particularly around addressing chronic health conditions and privacy concerns. We discuss considerations for LTM integr
    
[^140]: 语言模型未学习的任务

    Tasks That Language Models Don't Learn

    [https://arxiv.org/abs/2402.11349](https://arxiv.org/abs/2402.11349)

    大型语言模型没有学习到语言的视听特性，在新的任务基准测试中表现较差，暴露了人类语言理解与语言模型感官处理能力之间的根本差距。

    

    我们认为，我们目前的大型语言模型（LLMs）没有学习到语言的某些特性。我们通过一系列任务（称为H-TEST）对语言的视听特性进行了实证研究。这一基准测试突显了人类语言理解与LLMs的感官受限处理能力之间的根本差距。支持我们的假设，1. 故意推理（思维链），2. 少量案例，或3. 同一模型系列的更强大LLM（LLaMA 2 13B->LLaMA 2 70B）并不能简单地带来H-TEST性能的改善。因此，我们特别将其与玛丽的哲学案例联系起来，她在感官受限环境中了解世界（Jackson，1986）。我们的实验表明，一些最强大的专有LLMs的表现接近于随机基准准确率50％，突显了极限。

    arXiv:2402.11349v1 Announce Type: cross  Abstract: We argue that there are certain properties of language that our current large language models (LLMs) don't learn. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-TEST. This benchmark highlights a fundamental gap between human linguistic comprehension, which naturally integrates sensory experiences, and the sensory-deprived processing capabilities of LLMs. In support of our hypothesis, 1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the same model family (LLaMA 2 13B -> LLaMA 2 70B) do not trivially bring improvements in H-TEST performance. Therefore, we make a particular connection to the philosophical case of Mary, who learns about the world in a sensory-deprived environment (Jackson, 1986). Our experiments show that some of the strongest proprietary LLMs stay near random chance baseline accuracy of 50%, highlighting the limit
    
[^141]: 具有部分反馈的公平分类：一种基于探索的数据收集方法

    Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach

    [https://arxiv.org/abs/2402.11338](https://arxiv.org/abs/2402.11338)

    该方法提出了一种基于探索的数据收集方法，能够在缺乏部分反馈信息的情况下训练分类器，并提供了一系列策略来确保所有子群体都被探索、防止错误分类、以及收敛到期望的分类器。

    

    在许多预测场景（例如信贷放款）中，只有过去被积极分类的样本才会观察到真实结果。这些过去的观察结果形成了用于训练分类器以进行未来预测的训练数据集。然而，这样的训练数据集缺乏关于过去（错误地）被负面分类的样本结果的信息，可能导致错误的分类器。我们提出了一种方法，利用可用数据训练分类器，并提供一系列探索策略来收集关于否则会被忽略的子群体的结果数据。对于任何探索策略，该方法都具有以下保证：（1）所有子群体都得到了探索，（2）假阳性的比例受到了限制，（3）训练的分类器收敛到一个“期望”的分类器。正确的探索策略取决于上下文；它可以选择以改善学习保证

    arXiv:2402.11338v1 Announce Type: cross  Abstract: In many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. These past observations, in turn, form training datasets for classifiers that make future predictions. However, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. We present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. For any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a "desired" classifier. The right exploration strategy is context-dependent; it can be chosen to improve learning guarantees 
    
[^142]: 通过重构学习产生对感知无用的特征

    Learning by Reconstruction Produces Uninformative Features For Perception

    [https://arxiv.org/abs/2402.11337](https://arxiv.org/abs/2402.11337)

    重构学习所产生的特征对感知无用，需要通过其他策略如去噪学习来缓解这种不一致性。

    

    输入空间重构是一种吸引人的表示学习范式。尽管重构和生成的可解释性，我们发现了通过重构学习与为感知学习之间的不一致性。我们展示了前者将模型的容量分配给解释观察到的方差的数据子空间--这是对后者无用的特征子空间。

    arXiv:2402.11337v1 Announce Type: cross  Abstract: Input space reconstruction is an attractive representation learning paradigm. Despite interpretability of the reconstruction and generation, we identify a misalignment between learning by reconstruction, and learning for perception. We show that the former allocates a model's capacity towards a subspace of the data explaining the observed variance--a subspace with uninformative features for the latter. For example, the supervised TinyImagenet task with images projected onto the top subspace explaining 90\% of the pixel variance can be solved with 45\% test accuracy. Using the bottom subspace instead, accounting for only 20\% of the pixel variance, reaches 55\% test accuracy. The features for perception being learned last explains the need for long training time, e.g., with Masked Autoencoders. Learning by denoising is a popular strategy to alleviate that misalignment. We prove that while some noise strategies such as masking are indeed
    
[^143]: SpikeNAS: 一种面向脉冲神经网络系统的快速内存感知神经架构搜索框架

    SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network Systems

    [https://arxiv.org/abs/2402.11322](https://arxiv.org/abs/2402.11322)

    SpikeNAS提出了一种快速内存感知神经架构搜索框架，旨在帮助脉冲神经网络系统快速找到在给定内存预算下高准确性的适当架构。

    

    脉冲神经网络（SNN）为解决机器学习任务提供了实现超低功耗计算的有前途的解决方案。目前，大多数SNN架构都源自人工神经网络，其神经元的架构和操作与SNN不同，或者在不考虑来自底层处理硬件的内存预算的情况下开发。这些限制阻碍了SNN在准确性和效率方面充分发挥潜力。为此，我们提出了SpikeNAS，一种新颖的内存感知神经架构搜索（NAS）框架，可在给定内存预算下快速找到一个具有高准确性的适当SNN架构。为实现这一目标，我们的SpikeNAS采用了几个关键步骤：分析网络操作对准确性的影响，增强网络架构以提高学习质量，并开发快速内存感知搜索算法。

    arXiv:2402.11322v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) offer a promising solution to achieve ultra low-power/energy computation for solving machine learning tasks. Currently, most of the SNN architectures are derived from Artificial Neural Networks whose neurons' architectures and operations are different from SNNs, or developed without considering memory budgets from the underlying processing hardware. These limitations hinder the SNNs from reaching their full potential in accuracy and efficiency. Towards this, we propose SpikeNAS, a novel memory-aware neural architecture search (NAS) framework for SNNs that can quickly find an appropriate SNN architecture with high accuracy under the given memory budgets. To do this, our SpikeNAS employs several key steps: analyzing the impacts of network operations on the accuracy, enhancing the network architecture to improve the learning quality, and developing a fast memory-aware search algorithm. The experimental resul
    
[^144]: 使用RGBD感知和时间卷积网络对柔性连续机械臂的滞后补偿

    Hysteresis Compensation of Flexible Continuum Manipulator using RGBD Sensing and Temporal Convolutional Network

    [https://arxiv.org/abs/2402.11319](https://arxiv.org/abs/2402.11319)

    基于时间卷积网络的数据驱动方法用于捕捉柔性连续机械臂电缆驱动的非线性特性，提出了滞后补偿的解决方案。

    

    柔性连续机械臂因能够通过非线性路径进入狭窄空间而被重视于微创手术。但是受到电缆效应（如摩擦、伸长和耦合）引起的滞后效应导致电缆驱动机构面临控制困难。这些效应由于非线性而很难建模，并且在处理长且多节段机械臂时这些困难变得更加明显。本文提出了一种基于递归神经网络的数据驱动方法，以捕捉电缆驱动的这种非线性和以往状态依赖特性。我们设计定制的基准标记来收集物理关节配置作为数据集。对四种深度神经网络模型的学习性能进行比较研究的结果显示，时间卷积网络（TCN）表现出最高的预测能力。利用经过训练的TCN，我们构建了一个控制算法

    arXiv:2402.11319v1 Announce Type: cross  Abstract: Flexible continuum manipulators are valued for minimally invasive surgery, offering access to confined spaces through nonlinear paths. However, cable-driven manipulators face control difficulties due to hysteresis from cabling effects such as friction, elongation, and coupling. These effects are difficult to model due to nonlinearity and the difficulties become even more evident when dealing with long and multi-segmented manipulator. This paper proposes a data-driven approach based on recurrent neural networks to capture these nonlinear and previous states-dependent characteristics of cable actuation. We design customized fiducial markers to collect physical joint configurations as a dataset. Result on a study comparing the learning performance of four Deep Neural Network (DNN) models show that the Temporal Convolution Network (TCN) demonstrates the highest predictive capability. Leveraging trained TCNs, we build a control algorithm to
    
[^145]: 针对非静态动态的快速在线调整的去偏置离线表示学习

    Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics

    [https://arxiv.org/abs/2402.11317](https://arxiv.org/abs/2402.11317)

    提出了一种名为DORA的新方法，通过信息瓶颈原理在离线设置中学习适应性策略，解决了动态编码与环境数据之间的互信息与与行为策略的互信息之间的难题

    

    开发能够适应非静态环境的策略对于现实世界的强化学习应用至关重要。然而，在仅有一组有限的预先收集的轨迹的离线设置中学习这种适应性策略存在显著挑战。为了解决这个问题，我们引入了一种名为去偏置离线表示快速在线调整（DORA）的新方法。DORA融入了信息瓶颈原理，最大化了动态编码与环境数据之间的互信息，同时最小化了动态编码与行为策略的互信息。我们提出了DORA的实际实现，利用

    arXiv:2402.11317v1 Announce Type: cross  Abstract: Developing policies that can adjust to non-stationary environments is essential for real-world reinforcement learning applications. However, learning such adaptable policies in offline settings, with only a limited set of pre-collected trajectories, presents significant challenges. A key difficulty arises because the limited offline data makes it hard for the context encoder to differentiate between changes in the environment dynamics and shifts in the behavior policy, often leading to context misassociations. To address this issue, we introduce a novel approach called Debiased Offline Representation for fast online Adaptation (DORA). DORA incorporates an information bottleneck principle that maximizes mutual information between the dynamics encoding and the environmental data, while minimizing mutual information between the dynamics encoding and the actions of the behavior policy. We present a practical implementation of DORA, leverag
    
[^146]: 多生成代理集体决策在城市规划中的应用：Kendall Square改造案例研究

    Multi-Generative Agent Collective Decision-Making in Urban Planning: A Case Study for Kendall Square Renovation

    [https://arxiv.org/abs/2402.11314](https://arxiv.org/abs/2402.11314)

    多生成代理系统在城市规划中模拟社区决策，发现沟通有助于集体推理，而包含人口统计数据和生活价值导致意见分歧。这为城市规划和社区参与提供了有价值的见解。

    

    在这项研究中，我们开发了一个多生成代理系统，用于模拟 Kendall Square Volpe 大厦改造的社区决策过程。通过与当地利益相关者的访谈，我们的模拟结合了不同程度的沟通、人口统计数据和生活价值在代理提示中的应用。结果显示，代理之间的沟通改善了集体推理，而包含人口统计数据和生活价值导致了更明显的意见分歧。这些发现凸显了人工智能在理解复杂社会互动和决策过程中的潜在应用，为城市规划和 Kendall Square 等多样化环境中的社区参与提供了宝贵的见解。

    arXiv:2402.11314v1 Announce Type: cross  Abstract: In this study, we develop a multiple-generative agent system to simulate community decision-making for the redevelopment of Kendall Square's Volpe building. Drawing on interviews with local stakeholders, our simulations incorporated varying degrees of communication, demographic data, and life values in the agent prompts. The results revealed that communication among agents improved collective reasoning, while the inclusion of demographic and life values led to more distinct opinions. These findings highlight the potential application of AI in understanding complex social interactions and decision-making processes, offering valuable insights for urban planning and community engagement in diverse settings like Kendall Square.
    
[^147]: 分析人类和大型语言模型（LLM）的偏好

    Dissecting Human and LLM Preferences

    [https://arxiv.org/abs/2402.11296](https://arxiv.org/abs/2402.11296)

    本研究分析了人类和32种不同LLM的偏好，发现人类不太在意错误，偏好支持立场的回应，而先进的LLM更注重正确性、清晰性和无害性。

    

    作为模型响应的相对质量比较，人类和大型语言模型（LLM）的偏好在模型微调中作为共同的对齐目标和评估标准。然而，这些偏好仅反映了广泛趋势，导致了模型的可解释性和可控性较差，可能存在潜在的安全风险。本研究剖析了人类和32种不同LLM的偏好，以了解它们的定量组成，利用来自真实用户-模型对话的注释进行细粒度、场景化分析。我们发现人类对错误不太敏感，偏好支持其立场的回应，并在模型承认其局限性时表现出明显的不喜欢。相反，像GPT-4-Turbo这样的先进LLM更加强调正确性、清晰性和无害性。此外，大小相似的LLM倾向于展现出类似的偏好，无论它们的训练方法如何，并且为了对齐而进行的微调并不会导致显著的改变。

    arXiv:2402.11296v1 Announce Type: cross  Abstract: As a relative quality comparison of model responses, human and Large Language Model (LLM) preferences serve as common alignment goals in model fine-tuning and criteria in evaluation. Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks. In this work, we dissect the preferences of human and 32 different LLMs to understand their quantitative composition, using annotations from real-world user-model conversations for a fine-grained, scenario-wise analysis. We find that humans are less sensitive to errors, favor responses that support their stances, and show clear dislike when models admit their limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize correctness, clarity, and harmlessness more. Additionally, LLMs of similar sizes tend to exhibit similar preferences, regardless of their training methods, and fine-tuning for alignment does not sign
    
[^148]: 使用大型语言模型的推理解决难题：一项调查

    Puzzle Solving using Reasoning of Large Language Models: A Survey

    [https://arxiv.org/abs/2402.11291](https://arxiv.org/abs/2402.11291)

    本调查通过将难题分为基于规则和无规则两类的独特分类法，通过各种方法评估了大型语言模型（LLMs）的表现，强调了在复杂难题情境中LLMs的挑战和人类类似推理之间的差距，突出了推动LLMs解谜能力和贡献于人工智能发展的必要性。

    

    探索大型语言模型（LLMs）在解决难题中的能力揭示了它们在人工智能中的潜力和挑战，标志着理解它们在复杂推理任务中的适用性迈出了重要的一步。本调查利用独特的分类法将难题分为基于规则和无规则两类，通过各种方法评估LLMs，包括提示技术、神经符号方法和微调。通过对相关数据集和基准的批判性审查，我们评估了LLMs在复杂难题场景中的表现，识别出复杂难题情境中的显著挑战。我们的研究结果突出了LLMs能力及类人推理之间的差距，特别是在需要高级逻辑推断的情况下。调查强调了需要新颖策略和更丰富数据集来提升LLMs的解谜能力并促进人工智能的发展。

    arXiv:2402.11291v1 Announce Type: cross  Abstract: Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in artificial intelligence, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy -- dividing puzzles into rule-based and rule-less categories -- to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs' performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs' puzzle-solving proficiency and contribute to AI's
    
[^149]: 在虚拟化O-RAN平台中的公平资源分配

    Fair Resource Allocation in Virtualized O-RAN Platforms

    [https://arxiv.org/abs/2402.11285](https://arxiv.org/abs/2402.11285)

    该论文通过实验评估了O-Cloud的能耗及其与服务器硬件、容量和数据流量特性的关系，提出了一种计算策略和无线策略，平衡能源节约和性能，确保它们在服务器和用户之间公平分配。

    

    O-RAN系统及其在虚拟化通用计算平台（O-Cloud）中的部署构成了一个预计将带来前所未有的性能增益的范式转变。然而，这些架构带来了新的实施挑战，并威胁着加剧移动网络已经高能耗的问题。本文首先提出了一系列实验，评估了O-Cloud的能耗及其对服务器硬件、容量和数据流量特性的依赖性，这些特性通常会随时间改变。接下来，它提出了一个计算策略，以能效的方式将基站数据负载分配到O-Cloud服务器；以及一个无线策略，可实时确定每个用户的最小传输块大小，从而避免不必要的能源成本。这些策略平衡了能源节约和性能，并确保它们在服务器和用户之间分散公平。

    arXiv:2402.11285v1 Announce Type: cross  Abstract: O-RAN systems and their deployment in virtualized general-purpose computing platforms (O-Cloud) constitute a paradigm shift expected to bring unprecedented performance gains. However, these architectures raise new implementation challenges and threaten to worsen the already-high energy consumption of mobile networks. This paper presents first a series of experiments which assess the O-Cloud's energy costs and their dependency on the servers' hardware, capacity and data traffic properties which, typically, change over time. Next, it proposes a compute policy for assigning the base station data loads to O-Cloud servers in an energy-efficient fashion; and a radio policy that determines at near-real-time the minimum transmission block size for each user so as to avoid unnecessary energy costs. The policies balance energy savings with performance, and ensure that both of them are dispersed fairly across the servers and users, respectively. 
    
[^150]: 多透视一致性增强大型语言模型中的置信度估计

    Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models

    [https://arxiv.org/abs/2402.11279](https://arxiv.org/abs/2402.11279)

    多透视一致性方法为大型语言模型中的置信度估计带来改进，能有效减轻过度自信问题，并在多个数据集上实现最先进的性能。

    

    在大型语言模型（LLMs）的部署中，准确的置信度估计对于评估模型预测的可信度至关重要。然而，现有方法经常无法克服对错误答案的过度自信问题。在这项工作中，我们专注于改进大型语言模型的置信度估计。考虑到语言模型中自我意识的脆弱性，我们引入了一种多透视一致性（MPC）方法。我们利用模型内不同透视角度（MPC-Internal）和不同模型之间（MPC-Across）的互补见解来缓解由单一视角产生的过度自信问题。对八个公开数据集的实验结果显示，我们的MPC实现了最先进的性能。进一步的分析表明，MPC能够减轻过度自信问题，并且能够有效扩展到其他模型中。

    arXiv:2402.11279v1 Announce Type: cross  Abstract: In the deployment of large language models (LLMs), accurate confidence estimation is critical for assessing the credibility of model predictions. However, existing methods often fail to overcome the issue of overconfidence on incorrect answers. In this work, we focus on improving the confidence estimation of large language models. Considering the fragility of self-awareness in language models, we introduce a Multi-Perspective Consistency (MPC) method. We leverage complementary insights from different perspectives within models (MPC-Internal) and across different models (MPC-Across) to mitigate the issue of overconfidence arising from a singular viewpoint. The experimental results on eight publicly available datasets show that our MPC achieves state-of-the-art performance. Further analyses indicate that MPC can mitigate the problem of overconfidence and is effectively scalable to other models.
    
[^151]: 基于交叉伪标记的半监督医学图像分割方法，利用强弱数据增强策略

    Semi-supervised Medical Image Segmentation Method Based on Cross-pseudo Labeling Leveraging Strong and Weak Data Augmentation Strategies

    [https://arxiv.org/abs/2402.11273](https://arxiv.org/abs/2402.11273)

    本文提出了一种半监督模型DFCPS，创新地融合了Fixmatch概念，通过数据增强处理提高了模型性能和泛化能力，同时引入了交叉伪监督概念，使模型能够充分利用多个角度的伪标签，增强训练的多样性。

    

    传统的监督学习方法在医学图像分割中历史上遇到了某些约束，由于挑战性的收集过程、高昂的标记成本、低信噪比和复杂特征，这些特征描述了生物医学图像。本论文提出了一种半监督模型DFCPS，该模型创新性地结合了Fixmatch概念。通过数据增强处理，采用各种策略处理未标记数据，显著提高了模型的性能和泛化能力。同时，模型设计着重于伪标签的生成、过滤和细化过程。引入了交叉伪监督的新概念，将一致性学习与自我训练相结合。这使得模型能够充分利用多个角度的伪标签，从而增强训练的多样性。DFCPS模型与基准线和一个进行了比较

    arXiv:2402.11273v1 Announce Type: cross  Abstract: Traditional supervised learning methods have historically encountered certain constraints in medical image segmentation due to the challenging collection process, high labeling cost, low signal-to-noise ratio, and complex features characterizing biomedical images. This paper proposes a semi-supervised model, DFCPS, which innovatively incorporates the Fixmatch concept. This significantly enhances the model's performance and generalizability through data augmentation processing, employing varied strategies for unlabeled data. Concurrently, the model design gives appropriate emphasis to the generation, filtration, and refinement processes of pseudo-labels. The novel concept of cross-pseudo-supervision is introduced, integrating consistency learning with self-training. This enables the model to fully leverage pseudo-labels from multiple perspectives, thereby enhancing training diversity. The DFCPS model is compared with both baseline and a
    
[^152]: MoRAL: MoE增强LoRA用于LLM的终身学习

    MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning

    [https://arxiv.org/abs/2402.11260](https://arxiv.org/abs/2402.11260)

    MoRAL结合了MoE的多任务能力和LoRA的微调能力，采用问答对作为输入，实现了LLM的有效终身学习。

    

    本文提出了一种名为MoRAL的方法，即Mixture-of-Experts增强低秩适应性用于LLM的终身学习。 MoRAL将MoE的多任务能力与LoRA的微调能力相结合，实现了LLM的有效终身学习。与传统方法不同，MoRAL依赖于简单的问答对作为输入，这是一种更实用和有效的鲁棒学习策略。鉴于新数据设置，我们引入了一个新的评估基准，即LLM的终身学习（5L-bench），包括一个新的精心策划的问答对数据集，以及一组用于在开放式和封闭式环境下严格评估MoRAL的评估指标。实验评估表明，在开放式环境下，LLM在短时间内迅速学习

    arXiv:2402.11260v1 Announce Type: cross  Abstract: Adapting large language models (LLMs) to new domains/tasks and enabling them to be efficient lifelong learners is a pivotal challenge. In this paper, we propose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for Lifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the fine-tuning abilities of LoRA for effective life-long learning of LLMs. In contrast to the conventional approaches that use factual triplets as inputs MoRAL relies on simple question-answer pairs, which is a more practical and effective strategy for robust and efficient learning. Owing to new data settings, we introduce a new evaluation benchmark namely: Life Long Learning of LLM (5L-bench) encompassing a newly curated dataset of question-answer pairs, and a set of evaluation metrics for rigorous evaluation of MoRAL in open-book and closed-book settings. Experimental evaluation shows (i) LLMs learn fast in open-book settings with up to 
    
[^153]: 通过基于政策的自我判断来对齐大型语言模型

    Aligning Large Language Models by On-Policy Self-Judgment

    [https://arxiv.org/abs/2402.11253](https://arxiv.org/abs/2402.11253)

    本文提出了一个新颖的对齐框架SELF-JUDGE，通过增加式监督微调（JSFT）训练一个同时充当策略和评判器的单一模型，实现了参数高效的基于政策学习，无需额外的奖励模型。

    

    为了使大型语言模型与人类偏好保持一致，现有研究要么利用单独的奖励模型（RM）执行基于政策的学习，要么通过放弃基于政策的学习和对独立RM的需求简化训练过程。在本文中，我们提出了一个新颖的对齐框架SELF-JUDGE，它既是(1) 基于政策的学习，又是(2) 参数高效的，因为它不需要额外的RM来评估样本进行基于政策的学习。为此，我们提出了增强式监督微调（JSFT）来训练一个单一模型，作为策略和评判器。具体来说，我们将一对一判断任务视为指导式任务的特殊情况，从响应对中选择更好的响应。因此，得到的模型可以评判当前策略的即时响应偏好，从自身初始化。实验结果显示了SELF-JUDGE的有效性，优于基线模型。

    arXiv:2402.11253v1 Announce Type: cross  Abstract: To align large language models with human preferences, existing research either utilizes a separate reward model (RM) to perform on-policy learning or simplifies the training procedure by discarding the on-policy learning and the need for a separate RM. In this paper, we present a novel alignment framework, SELF-JUDGE that is (1) on-policy learning and 2) parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model acting as both a policy and a judge. Specifically, we view the pairwise judgment task as a special case of the instruction-following task, choosing the better response from a response pair. Thus, the resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines 
    
[^154]: 大型语言模型能够进行基于关系的论证挖掘吗？

    Can Large Language Models perform Relation-based Argument Mining?

    [https://arxiv.org/abs/2402.11243](https://arxiv.org/abs/2402.11243)

    大型语言模型在处理关系型论证挖掘方面表现更好，能够显著超过目前最佳基准线，并且在十个数据集上进行了实验验证。

    

    论据挖掘（AM）是从文本中自动提取论据、它们的组成部分和/或论据和组成部分之间关系的过程。随着支持在线辩论的平台数量不断增加，对AM的需求变得愈发迫切，特别是为了支持下游任务。基于关系的AM（RbAM）是一种关注识别论据之间协议（支持）和不同意（攻击）关系的AM形式。RbAM是一个具有挑战性的分类任务，现有方法无法令人满意地执行。在本文中，我们展示了通用型大型语言模型（LLMs），经过适当的调整和提示，可以显著优于表现最好的（基于RoBERTa的）基准线。具体来说，我们对两个开源LLM（Llama-2和Mistral）在十个数据集上进行了实验。

    arXiv:2402.11243v1 Announce Type: cross  Abstract: Argument mining (AM) is the process of automatically extracting arguments, their components and/or relations amongst arguments and components from text. As the number of platforms supporting online debate increases, the need for AM becomes ever more urgent, especially in support of downstream tasks. Relation-based AM (RbAM) is a form of AM focusing on identifying agreement (support) and disagreement (attack) relations amongst arguments. RbAM is a challenging classification task, with existing methods failing to perform satisfactorily. In this paper, we show that general-purpose Large Language Models (LLMs), appropriately primed and prompted, can significantly outperform the best performing (RoBERTa-based) baseline. Specifically, we experiment with two open-source LLMs (Llama-2 and Mistral) with ten datasets.
    
[^155]: 通过防止样本选择偏差来学习不平衡嘈杂数据

    Learning with Imbalanced Noisy Data by Preventing Bias in Sample Selection

    [https://arxiv.org/abs/2402.11242](https://arxiv.org/abs/2402.11242)

    提出了一种用于处理不平衡数据中嘈杂标签的方法，通过Class-Balance-based Sample Selection (CBS)防止忽视尾部类别样本，并通过Confidence-based Sample Augmentation (CSA)增强干净样本的可靠性。

    

    学习具有嘈杂标签的方法引起了越来越多的关注，因为现实场景中不可避免的不完美标签会严重影响深度模型的性能。最近的研究倾向于将低损失样本视为干净样本，丢弃高损失样本以减轻嘈杂标签的负面影响。然而，现实世界的数据集不仅包含嘈杂标签，还包含类别不平衡。不平衡问题容易导致损失较大的尾部类别的学习不足，从而产生高损失。因此，我们提出了一种简单而有效的方法来处理不平衡数据中的嘈杂标签。具体来说，我们提出了基于类平衡的样本选择（CBS）来防止在训练过程中忽视尾部类别样本。我们提出了基于置信度的样本增强（CSA）以加强所选干净样本在训练过程中的可靠性。

    arXiv:2402.11242v1 Announce Type: cross  Abstract: Learning with noisy labels has gained increasing attention because the inevitable imperfect labels in real-world scenarios can substantially hurt the deep model performance. Recent studies tend to regard low-loss samples as clean ones and discard high-loss ones to alleviate the negative impact of noisy labels. However, real-world datasets contain not only noisy labels but also class imbalance. The imbalance issue is prone to causing failure in the loss-based sample selection since the under-learning of tail classes also leans to produce high losses. To this end, we propose a simple yet effective method to address noisy labels in imbalanced datasets. Specifically, we propose Class-Balance-based sample Selection (CBS) to prevent the tail class samples from being neglected during training. We propose Confidence-based Sample Augmentation (CSA) for the chosen clean samples to enhance their reliability in the training process. To exploit sel
    
[^156]: DiffPoint: 使用基于ViT的扩散模型进行单视点和多视点点云重建

    DiffPoint: Single and Multi-view Point Cloud Reconstruction with ViT Based Diffusion Model

    [https://arxiv.org/abs/2402.11241](https://arxiv.org/abs/2402.11241)

    DiffPoint是一种结合了ViT和扩散模型的新型架构，用于实现点云重建任务。

    

    随着在各种现实世界场景中2D到3D重建任务引起了显著关注，生成高质量点云变得至关重要。尽管深度学习模型在生成点云方面取得了最近的成功，但由于图像和点云之间的差异，仍然存在产生高保真结果的挑战。虽然视觉Transformer（ViT）和扩散模型在各种视觉任务中显示出潜力，但它们对从图像重建点云的好处尚未得到证明。在本文中，我们首先提出了一个简洁而强大的架构，名为DiffPoint，它结合了ViT和扩散模型来进行点云重建任务。在每个扩散步骤中，我们将带有噪声的点云分成不规则的补丁。然后，使用一个标准的ViT主干，将所有输入（包括时间信息、图像嵌入和有噪声的补丁）视为令牌，我们进行训练。

    arXiv:2402.11241v1 Announce Type: cross  Abstract: As the task of 2D-to-3D reconstruction has gained significant attention in various real-world scenarios, it becomes crucial to be able to generate high-quality point clouds. Despite the recent success of deep learning models in generating point clouds, there are still challenges in producing high-fidelity results due to the disparities between images and point clouds. While vision transformers (ViT) and diffusion models have shown promise in various vision tasks, their benefits for reconstructing point clouds from images have not been demonstrated yet. In this paper, we first propose a neat and powerful architecture called DiffPoint that combines ViT and diffusion models for the task of point cloud reconstruction. At each diffusion step, we divide the noisy point clouds into irregular patches. Then, using a standard ViT backbone that treats all inputs as tokens (including time information, image embeddings, and noisy patches), we train
    
[^157]: 警惕您的代理人！调查基于LLM的代理人的后门威胁

    Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents

    [https://arxiv.org/abs/2402.11208](https://arxiv.org/abs/2402.11208)

    这项工作调查了基于LLM的代理人面临的后门攻击威胁，并提出了一般框架和不同形式的后门攻击分析。

    

    利用大型语言模型LLM的快速发展，已经开发出了用于处理各种实际应用（包括金融、医疗保健和购物等）的基于LLM的代理人。在应用过程中确保LLM代理人的可靠性和安全性至关重要。然而，目前对LLM代理人的安全性问题尚未得到充分探讨。本工作首次探讨了典型安全威胁之一，即对LLM代理人的后门攻击。我们首先制定了一个代理人后门攻击的一般框架，然后对不同形式的代理人后门攻击进行了彻底分析。具体而言，从最终攻击结果的角度来看，攻击者可以选择操纵最终输出分布，或者仅在中间推理过程中引入恶意行为，同时保持最终输出的正确性。此外，前一类可以分为

    arXiv:2402.11208v1 Announce Type: cross  Abstract: Leveraging the rapid development of Large Language Models LLMs, LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis on the different forms of agent backdoor attacks. Specifically, from the perspective of the final attacking outcomes, the attacker can either choose to manipulate the final output distribution, or only introduce malicious behavior in the intermediate reasoning process, while keeping the final output correct. Furthermore, the former category can be divided
    
[^158]: 探索ChatGPT在下一代信息检索中的应用：机遇与挑战

    Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges

    [https://arxiv.org/abs/2402.11203](https://arxiv.org/abs/2402.11203)

    ChatGPT作为信息检索领域的关键技术，不断挑战传统范式，带来了新的机遇和挑战，同时超越了之前的GPT-3模型。

    

    人工智能（AI）的快速发展凸显了ChatGPT作为信息检索（IR）领域中的关键技术。与之前的模型不同，ChatGPT提供了显著的好处，吸引了行业和学术界的关注。一些人认为ChatGPT是一项开创性的创新，而另一些人将其成功归因于产品开发和市场策略的有效整合。ChatGPT的出现，以及与OpenAI的GPT-4一起，标志着生成式AI的新阶段，产生的内容与训练样本有所不同，并超越了以往的GPT-3模型的能力。与信息检索任务中的传统监督学习方法不同，ChatGPT挑战了现有的范式，带来了关于文本质量保证、模型偏差和效率方面的新挑战和机遇。本文旨在研究ChatGPT对信息检索任务的影响，并提供

    arXiv:2402.11203v1 Announce Type: cross  Abstract: The rapid advancement of artificial intelligence (AI) has highlighted ChatGPT as a pivotal technology in the field of information retrieval (IR). Distinguished from its predecessors, ChatGPT offers significant benefits that have attracted the attention of both the industry and academic communities. While some view ChatGPT as a groundbreaking innovation, others attribute its success to the effective integration of product development and market strategies. The emergence of ChatGPT, alongside GPT-4, marks a new phase in Generative AI, generating content that is distinct from training examples and exceeding the capabilities of the prior GPT-3 model by OpenAI. Unlike the traditional supervised learning approach in IR tasks, ChatGPT challenges existing paradigms, bringing forth new challenges and opportunities regarding text quality assurance, model bias, and efficiency. This paper seeks to examine the impact of ChatGPT on IR tasks and offe
    
[^159]: 在连续学习中保持对抗性鲁棒性

    Maintaining Adversarial Robustness in Continuous Learning

    [https://arxiv.org/abs/2402.11196](https://arxiv.org/abs/2402.11196)

    提出了一种名为双梯度投影的方法，通过将梯度投影到两个关键子空间来实现持续鲁棒学习，有效地维持了神经网络对抗性鲁棒性。

    

    对抗性鲁棒性对于机器学习系统的安全性和可靠性至关重要。然而，通过复杂的防御算法获得的对抗性鲁棒性在神经网络不断演化以学习新任务时很容易被抹去。这种脆弱性可以通过培养一种新颖的神经网络能力来解决，称为持续鲁棒学习，它在连续学习过程中关注前期任务的(分类)性能和对抗性鲁棒性。为了实现持续鲁棒学习，我们提出了一种称为双梯度投影的方法，将用于权重更新的梯度正交投影到两个关键子空间上 -- 一个用于稳定平滑样本梯度，另一个用于稳定神经网络的最终输出。在四个基准测试上的实验结果表明，所提出的方法有效地维持了对强对抗性的持续鲁棒性。

    arXiv:2402.11196v1 Announce Type: cross  Abstract: Adversarial robustness is essential for security and reliability of machine learning systems. However, the adversarial robustness gained by sophisticated defense algorithms is easily erased as the neural network evolves to learn new tasks. This vulnerability can be addressed by fostering a novel capability for neural networks, termed continual robust learning, which focuses on both the (classification) performance and adversarial robustness on previous tasks during continuous learning. To achieve continuous robust learning, we propose an approach called Double Gradient Projection that projects the gradients for weight updates orthogonally onto two crucial subspaces -- one for stabilizing the smoothed sample gradients and another for stabilizing the final outputs of the neural network. The experimental results on four benchmarks demonstrate that the proposed approach effectively maintains continuous robustness against strong adversarial
    
[^160]: 如果你讲我的语言，我会更好地学习：使用风格对齐响应调整增强大型语言模型微调

    I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments

    [https://arxiv.org/abs/2402.11192](https://arxiv.org/abs/2402.11192)

    将微调过程中的实际响应风格与大型语言模型固有风格相匹配能够产生更好的学习结果，开发的方法通过最小程度地调整模型响应来避免过拟合。

    

    使用小数据集为特定任务微调大型语言模型(LLMs)是一个普遍遇到的但复杂的挑战。在有限的示例上过多拟合可能会对模型的泛化能力和保留原始技能产生负面影响。我们的研究探讨了在微调过程中地实际响应风格的影响。我们发现将地实际响应风格与LLM固有风格匹配会产生更好的学习结果。基于这一观点，我们开发了一种方法，最小程度地修改LLM的现有响应以更正错误，使用这些调整后的响应作为训练目标。这种技术能够实现与模型固有响应风格一致的精确更正，维护模型的核心能力，从而避免过多拟合。我们的研究结果表明，这种方法不仅提高了LLM的特定任务准确性，而且关键地

    arXiv:2402.11192v1 Announce Type: cross  Abstract: Fine-tuning large language models (LLMs) with a small data set for particular tasks is a widely encountered yet complex challenge. The potential for overfitting on a limited number of examples can negatively impact the model's ability to generalize and retain its original skills. Our research explores the impact of the style of ground-truth responses during the fine-tuning process. We found that matching the ground-truth response style with the LLM's inherent style results in better learning outcomes. Building on this insight, we developed a method that minimally alters the LLM's pre-existing responses to correct errors, using these adjusted responses as training targets. This technique enables precise corrections in line with the model's native response style, safeguarding the model's core capabilities and thus avoid overfitting. Our findings show that this approach not only improves the LLM's task-specific accuracy but also crucially
    
[^161]: LaCo：通过层叠实现大型语言模型的剪枝

    LaCo: Large Language Model Pruning via Layer Collapse

    [https://arxiv.org/abs/2402.11187](https://arxiv.org/abs/2402.11187)

    提出了一种名为Layer Collapse（LaCo）的简明的逐层剪枝方法，使得大型语言模型可以在保持模型结构的同时迅速减小尺寸，并在剪枝比例达到25-30%时保持超过80%的平均任务性能，明显优于现有最先进的结构化剪枝方法。

    

    基于Transformer的大型语言模型（LLMs）正经历着尺寸扩大的明显趋势，这给模型的训练和推理带来了相当大的成本。然而，现有的方法如模型量化、知识蒸馏和模型剪枝受到各种问题的限制，包括硬件支持限制、需要大量的训练和对模型内部结构的改变。在本文中，我们提出了一种简洁的逐层剪枝方法，称为Layer Collapse（LaCo），其中后置模型层折叠到前置层，使模型尺寸迅速减小同时保持模型结构。综合实验表明，我们的方法在剪枝比例达到25-30%时，保持了超过80%的平均任务性能，明显优于现有最先进的结构化剪枝方法。我们还进行了后训练实验以确认所提方法的有效性。

    arXiv:2402.11187v1 Announce Type: cross  Abstract: Large language models (LLMs) based on transformer are witnessing a notable trend of size expansion, which brings considerable costs to both model training and inference. However, existing methods such as model quantization, knowledge distillation, and model pruning are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the internal structure of the model. In this paper, we propose a concise layer-wise pruning method called \textit{Layer Collapse (LaCo)}, in which rear model layers collapse into a prior layer, enabling a rapid reduction in model size while preserving the model structure. Comprehensive experiments show that our method maintains an average task performance of over 80\% at pruning ratios of 25-30\%, significantly outperforming existing state-of-the-art structured pruning methods. We also conduct post-training experiments to confirm that the proposed pr
    
[^162]: KnowTuning：针对大型语言模型的知识感知微调

    KnowTuning: Knowledge-aware Fine-tuning for Large Language Models

    [https://arxiv.org/abs/2402.11176](https://arxiv.org/abs/2402.11176)

    提出了一种知识感知微调方法，通过显式和隐式方式改善大型语言模型对知识的认识，包括训练模型明确识别答案中的知识三元组和隐式区分可靠和不可靠的知识。

    

    尽管大型语言模型（LLMs）在许多自然语言处理（NLP）任务上取得成功，但仍然难以有效利用知识进行知识密集型任务，表现出生成不完整、非事实性或不合逻辑的答案等限制。这些限制源于LLMs在普通微调期间对知识的认识不足。为解决这些问题，我们提出了一种知识感知微调（KnowTuning）方法，以明确和隐式地改善LLMs的知识认识。我们设计了一个显式知识感知生成阶段，训练LLMs明确识别答案中的知识三元组。我们还提出了一个隐式知识感知比较阶段，训练LLMs隐式区分可靠和不可靠的知识，包括完整性、事实性和逻辑性三个方面。对通用和医学问答（QA）数据集进行的大量实验证实了效果。

    arXiv:2402.11176v1 Announce Type: cross  Abstract: Despite their success at many natural language processing (NLP) tasks, large language models (LLMs) still struggle to effectively leverage knowledge for knowledge-intensive tasks, manifesting limitations such as generating incomplete, non-factual, or illogical answers. These limitations stem from inadequate knowledge awareness of LLMs during vanilla fine-tuning. To address these problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to explicitly and implicitly improve the knowledge awareness of LLMs. We devise an explicit knowledge-aware generation stage to train LLMs to explicitly identify knowledge triples in answers. We also propose an implicit knowledge-aware comparison stage to train LLMs to implicitly distinguish between reliable and unreliable knowledge, in three aspects: completeness, factuality, and logicality. Extensive experiments on both generic and medical question answering (QA) datasets confirm the effec
    
[^163]: 基于信任区域的黑盒概率认证解释

    Trust Regions for Explanations via Black-Box Probabilistic Certification

    [https://arxiv.org/abs/2402.11168](https://arxiv.org/abs/2402.11168)

    通过黑盒概率认证解释的信任区域能够有效地洞察模型行为、保证解释的稳定性，并实现解释的重用

    

    由于机器学习模型的黑盒性质，人们开发了大量的可解释性方法来解析个别决策背后的因素。本文提出了一个新颖的黑盒（概率性）解释认证问题。我们提出了一个问题：给定一个黑盒模型，只有查询访问权，一个示例的解释以及一个质量度量（如逼真度、稳定性），我们是否能找到最大的超立方体（即 $\ell_{\infty}$ 球），以示例为中心，使得当解释被应用于超立方体内的所有示例时（高概率下）质量标准得到满足（比如逼真度高于某个值）？能够高效地找到这样一个信任区域有多重好处：i）洞察模型在一个区域内的行为，具有保证；ii）解释的稳定性得到保证；iii）解释的重用，可以节省时间、精力和金钱。

    arXiv:2402.11168v1 Announce Type: cross  Abstract: Given the black box nature of machine learning models, a plethora of explainability methods have been developed to decipher the factors behind individual decisions. In this paper, we introduce a novel problem of black box (probabilistic) explanation certification. We ask the question: Given a black box model with only query access, an explanation for an example and a quality metric (viz. fidelity, stability), can we find the largest hypercube (i.e., $\ell_{\infty}$ ball) centered at the example such that when the explanation is applied to all examples within the hypercube, (with high probability) a quality criterion is met (viz. fidelity greater than some value)? Being able to efficiently find such a \emph{trust region} has multiple benefits: i) insight into model behavior in a \emph{region}, with a \emph{guarantee}; ii) ascertained \emph{stability} of the explanation; iii) \emph{explanation reuse}, which can save time, energy and mone
    
[^164]: Token-Ensemble文本生成：对自动AI生成文本检测的攻击

    Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection

    [https://arxiv.org/abs/2402.11167](https://arxiv.org/abs/2402.11167)

    提出了一种新颖的token-ensemble生成策略，挑战了当前AI内容检测方法的鲁棒性，对当前检测模型构成了重要挑战，需要进一步改进检测技术以应对复杂对抗策略。

    

    AI内容检测模型对经过精心设计的攻击（例如改写或词语替换）的鲁棒性仍然是一个重要问题。本研究提出了一种新颖的token-ensemble生成策略，挑战了当前AI内容检测方法的鲁棒性。我们通过使用从随机候选语言模型生成的下一个token完成提示来探索集成攻击策略。我们发现token-ensemble方法显著降低了AI内容检测模型的性能（代码和测试集将发布）。我们的发现表明，token-ensemble生成对当前检测模型构成了重要挑战，并强调了改进检测技术以应对复杂对抗策略的需求。

    arXiv:2402.11167v1 Announce Type: cross  Abstract: The robustness of AI-content detection models against cultivated attacks (e.g., paraphrasing or word switching) remains a significant concern. This study proposes a novel token-ensemble generation strategy to challenge the robustness of current AI-content detection approaches. We explore the ensemble attack strategy by completing the prompt with the next token generated from random candidate LLMs. We find the token-ensemble approach significantly drops the performance of AI-content detection models (The code and test sets will be released). Our findings reveal that token-ensemble generation poses a vital challenge to current detection models and underlines the need for advancing detection technologies to counter sophisticated adversarial strategies.
    
[^165]: PANDA（Pedantic ANswer-correctness Determination and Adjudication）：改进问答和文本生成的自动评估

    PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation

    [https://arxiv.org/abs/2402.11161](https://arxiv.org/abs/2402.11161)

    提出了PANDA方法，引入了更精确的答案正确性评测方式，解决了当前自动评估问答和文本生成过程中的挑战。

    

    问答（QA）只有在我们知道答案是否正确时才能取得进展，但对于许多最具挑战性和有趣的QA示例，当前的答案正确性（AC）指标与人类判断不一致，特别是来自大型语言模型（LLM）的冗长、自由格式答案。我们提出了两个挑战：缺乏数据和模型过大。基于LLM的评分器与人类更好地相关，但这项昂贵的任务仅在有限的QA数据集上进行了测试。我们通过提供清晰的指南来评估从人类QA比赛中采纳的机器QA，解决了这些问题。我们还引入了精确的答案正确性确定和裁决（Precise ANswer correctness Determination and Adjudication，PANDA），这是一个小巧、高效、确定性的AC分类器（812 KB），更准确地评估答案的正确性。

    arXiv:2402.11161v1 Announce Type: cross  Abstract: Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current answer correctness (AC) metrics do not align with human judgments, particularly verbose, free form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big. LLM based scorers correlate better with humans, but this expensive task has only been tested on limited QA datasets. We rectify these issues by providing clear guidelines for evaluating machine QA adopted from human QA contests. We also introduce Precise ANswer correctness Determination and Adjudication (PANDA), a small, efficient, deterministic AC classifier (812 KB) that more accurately evaluates answer correctness.
    
[^166]: LiGNN: 领英上的图神经网络

    LiGNN: Graph Neural Networks at LinkedIn

    [https://arxiv.org/abs/2402.11139](https://arxiv.org/abs/2402.11139)

    本文介绍了在领英上开发和部署的LiGNN框架，包括对GNN表示学习的算法改进和大规模训练优化，为工作申请回复率、广告点击率和Feed每日活跃用户提高带来了约1%-2%的相对改善。

    

    在本文中，我们提出了LiGNN，一种已部署的大规模图神经网络（GNNs）框架。我们分享了在领英上开发和部署GNNs的见解。我们提出了一组算法改进，包括具有长期损失的时间图架构，通过图密集化实现的有效冷启动解决方案，ID嵌入和多跳邻居采样，以改进GNN表示学习的质量。我们解释了如何通过邻居的自适应采样，对训练数据批次进行分组和切片，专门的共享内存队列和本地梯度优化将LinkedIn图的大规模训练加快7倍。我们总结了从A/B测试实验中获得的部署经验和教训。本文介绍的技术已经为工作申请回复率的相对改善率约为1％，广告点击率提升2％，Feed每日活跃用户提高0.5％做出了贡献。

    arXiv:2402.11139v1 Announce Type: cross  Abstract: In this paper, we present LiGNN, a deployed large-scale Graph Neural Networks (GNNs) Framework. We share our insight on developing and deployment of GNNs at large scale at LinkedIn. We present a set of algorithmic improvements to the quality of GNN representation learning including temporal graph architectures with long term losses, effective cold start solutions via graph densification, ID embeddings and multi-hop neighbor sampling. We explain how we built and sped up by 7x our large-scale training on LinkedIn graphs with adaptive sampling of neighbors, grouping and slicing of training data batches, specialized shared-memory queue and local gradient optimization. We summarize our deployment lessons and learnings gathered from A/B test experiments. The techniques presented in this work have contributed to an approximate relative improvements of 1% of Job application hearing back rate, 2% Ads CTR lift, 0.5% of Feed engaged daily active 
    
[^167]: 对比指令调整

    Contrastive Instruction Tuning

    [https://arxiv.org/abs/2402.11138](https://arxiv.org/abs/2402.11138)

    提出了对比指令调整方法，通过最大化相似性来提高大型语言模型对未知任务指令的稳健性

    

    指令调整一直被用作改善大型语言模型（LLMs）在未知任务上的性能的一种有前途的方法。然而，当前的LLMs在面临未知指令时表现出有限的稳健性，当相同的指令以稍微变化的形式或语言风格提出时会产生不一致的输出。这种行为表明LLMs对文本变化的稳健性和对未知指令的泛化能力不足，可能会导致可信度问题。因此，我们提出了对比指令调整，该方法在最大化语义上等价的指令-实例对的隐藏表示之间的相似性的同时，最小化语义上不同的对之间的相似性。为了促进这种方法，我们通过释义任务指令，扩充现有的FLAN集合。在PromptBench基准测试上的实验表明，对比指令调整（CoIN）一直提高了LLMs对未知指令的稳健性

    arXiv:2402.11138v1 Announce Type: cross  Abstract: Instruction tuning has been used as a promising approach to improve the performance of large language models (LLMs) on unseen tasks. However, current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs' lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning, which maximizes the similarity between the hidden representations of semantically equivalent instruction-instance pairs while minimizing the similarity between semantically different ones. To facilitate this approach, we augment the existing FLAN collection by paraphrasing task instructions. Experiments on the PromptBench benchmark show that CoIN consistently improves LLMs' robustness to unseen instructio
    
[^168]: 推测式流式处理: 无需辅助模型的快速LLM推理

    Speculative Streaming: Fast LLM Inference without Auxiliary Models

    [https://arxiv.org/abs/2402.11131](https://arxiv.org/abs/2402.11131)

    提出了一种Speculative Streaming方法，将草稿模型融入目标模型，并通过将微调目标从下一个令牌预测更改为未来的n-gram预测，加速解码1.8-3.1倍，同时保持生成质量。

    

    推测式解码是一种突出的技术，可以提高基于辅助草稿模型预测的大型目标语言模型的推理速度。虽然在特定应用设置中有效，但通常需要微调草稿和目标模型以实现较高的接受率。随着下游任务数量的增加，这些草稿模型给推理系统增加了显著的复杂性。我们提出了Speculative Streaming，一种单模型的推测式解码方法，通过将草拟融入目标模型，将微调目标从下一个令牌预测对象更改为未来的n-gram预测。 Speculative Streaming在各种任务中加速解码1.8-3.1倍，如摘要、结构化查询和意义表达，同时不降低生成质量。此外，Speculative Streaming参数有效。它实现了与Medusa风格架构相媲美/更高的加速度

    arXiv:2402.11131v1 Announce Type: cross  Abstract: Speculative decoding is a prominent technique to speed up the inference of a large target language model based on predictions of an auxiliary draft model. While effective, in application-specific settings, it often involves fine-tuning both draft and target models to achieve high acceptance rates. As the number of downstream tasks grows, these draft models add significant complexity to inference systems. We propose Speculative Streaming, a single-model speculative decoding method that fuses drafting into the target model by changing the fine-tuning objective from next token prediction to future n-gram prediction. Speculative Streaming speeds up decoding by 1.8 - 3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and Meaning Representation, without sacrificing generation quality. Additionally, Speculative Streaming is parameter-efficient. It achieves on-par/higher speed-ups than Medusa-style architectures while u
    
[^169]: 使用情境臂研究优化华法林用量：一种离线策略学习和评估方法

    Optimizing Warfarin Dosing Using Contextual Bandit: An Offline Policy Learning and Evaluation Method

    [https://arxiv.org/abs/2402.11123](https://arxiv.org/abs/2402.11123)

    本研究提出了一种使用情境臂和离线策略学习的方法，通过观察数据建立个性化的华法林剂量策略，即使在缺乏基因型信息的情况下也可以超越基线方法。

    

    华法林是一种抗凝药物，旨在预防和治疗与异常血液凝结相关的疾病，是全球最常开处方的药物之一。然而，由于个体反应变化，确定合适的剂量仍具挑战性，错误的剂量可能导致严重后果。情境臂和强化学习在解决这一问题上显示出了潜力。鉴于历史政策的观察数据广泛可得且医疗决策的安全性，我们专注于使用历史政策作为演示的观测数据，通过离线策略学习和评估在情境臂环境中建立最佳个性化剂量策略。我们学习到的策略在没有基因型信息的情况下超越了这些基线方法，甚至在给定次优演示的情况下也表现出色，展示了巨大的应用潜力。

    arXiv:2402.11123v1 Announce Type: cross  Abstract: Warfarin, an anticoagulant medication, is formulated to prevent and address conditions associated with abnormal blood clotting, making it one of the most prescribed drugs globally. However, determining the suitable dosage remains challenging due to individual response variations, and prescribing an incorrect dosage may lead to severe consequences. Contextual bandit and reinforcement learning have shown promise in addressing this issue. Given the wide availability of observational data and safety concerns of decision-making in healthcare, we focused on using exclusively observational data from historical policies as demonstrations to derive new policies; we utilized offline policy learning and evaluation in a contextual bandit setting to establish the optimal personalized dosage strategy. Our learned policies surpassed these baseline approaches without genotype inputs, even when given a suboptimal demonstration, showcasing promising app
    
[^170]: 导航双重面：对大型语言模型中顺序记忆编辑的全面评估

    Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models

    [https://arxiv.org/abs/2402.11122](https://arxiv.org/abs/2402.11122)

    这项研究全面评估了大型语言模型中顺序记忆编辑的影响，发现修改参数ME可能会导致所有任务表现不佳，而保留参数ME则能够保持较好性能。

    

    记忆编辑（ME）已经成为修改大型语言模型（LLMs）中错误事实或注入新事实的有效方法。存在两种主流ME方法：修改参数ME和保留参数ME（在保留原始参数的同时整合额外模块）。遗憾的是，先前对ME评估的研究存在两个关键限制：（i）仅评估带有单个编辑的LLMs，忽略了持续编辑的需要，以及（ii）评估仅关注基本事实三元组，忽视了更广泛的LLM能力，如逻辑推理和阅读理解。本研究通过以下三点解决了这些限制：（i）我们探索了ME如何影响LLMs的广泛基本能力在顺序编辑下。实验结果揭示了一个有趣的现象：大多数修改参数ME在几次顺序编辑后一贯降低所有任务的表现。相比之下，

    arXiv:2402.11122v1 Announce Type: cross  Abstract: Memory Editing (ME) has emerged as an efficient method to modify erroneous facts or inject new facts into Large Language Models (LLMs). Two mainstream ME methods exist: parameter-modifying ME and parameter-preserving ME (integrating extra modules while preserving original parameters). Regrettably, previous studies on ME evaluation have two critical limitations: (i) evaluating LLMs with single edit only, neglecting the need for continuous editing, and (ii) evaluations focusing solely on basic factual triples, overlooking broader LLM capabilities like logical reasoning and reading understanding. This study addresses these limitations with contributions threefold: (i) We explore how ME affects a wide range of fundamental capabilities of LLMs under sequential editing. Experimental results reveal an intriguing phenomenon: Most parameter-modifying ME consistently degrade performance across all tasks after a few sequential edits. In contrast,
    
[^171]: 语言模型作为科学导师

    Language Models as Science Tutors

    [https://arxiv.org/abs/2402.11111](https://arxiv.org/abs/2402.11111)

    介绍了TutorEval和TutorChat，通过TutorEval基准可以衡量LMs作为科学助手的实际可用性，TutorChat数据集用于微调模型。

    

    NLP最近取得了令人兴奋的进展，朝着训练具有较强科学问题解决能力的语言模型（LMs）的方向发展。然而，模型的发展并没有专注于LMs在科学教育中的实际用例，包括需要处理长篇科学文档的应用。为了解决这个问题，我们引入了TutorEval和TutorChat。TutorEval是一个多样化的问答基准，其中包含有关STEM教科书长篇章节的问题，由专家编写。TutorEval有助于衡量LMs作为科学助手的实际可用性，它是第一个结合长上下文、自由生成和多学科科学知识的基准。此外，我们展示了使用现有对话数据集对基础模型进行微调会导致TutorEval性能不佳。因此，我们创建了TutorChat，这是一个包含80,000个关于教科书的长合成对话的数据集。我们使用TutorChat来微调Llemma模型。

    arXiv:2402.11111v1 Announce Type: cross  Abstract: NLP has recently made exciting progress toward training language models (LMs) with strong scientific problem-solving skills. However, model development has not focused on real-life use-cases of LMs for science, including applications in education that require processing long scientific documents. To address this, we introduce TutorEval and TutorChat. TutorEval is a diverse question-answering benchmark consisting of questions about long chapters from STEM textbooks, written by experts. TutorEval helps measure real-life usability of LMs as scientific assistants, and it is the first benchmark combining long contexts, free-form generation, and multi-disciplinary scientific knowledge. Moreover, we show that fine-tuning base models with existing dialogue datasets leads to poor performance on TutorEval. Therefore, we create TutorChat, a dataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to fine-tune Llemma models wit
    
[^172]: 用询问不完整选票计算投票规则

    Computing Voting Rules with Elicited Incomplete Votes

    [https://arxiv.org/abs/2402.11104](https://arxiv.org/abs/2402.11104)

    本文研究了通过询问选民有关少量候选人的投票规则计算问题，完全表征了可计算的位置评分规则集合，并给出了对于确定性或随机算法确定最大得分候选人必须进行的查询次数的参数化上限和下限。

    

    受到在大型候选人群体中说明完整序数偏好的困难的启发，我们研究了通过询问选民有关 $t < m$ 候选人的投票规则。在推广了关于该问题特定情况的先前研究的基础上，我们的论文完全表征了对于任意 $1 \leq t < m$ 可以计算得出的位置评分规则的集合，值得注意的是其中并不包括多数制。然后，我们扩展这一研究，展示了单次可转移投票（淘汰投票）的类似无法计算的结果。这些负面结果是信息理论的，不关心查询的数量。最后，对于可以使用有限大小查询计算的评分规则，我们给出了参数化的关于确定性或随机算法必须做出的查询数量的上限和下限。虽然我们的确定性算法的界限之间没有差距，但识别

    arXiv:2402.11104v1 Announce Type: cross  Abstract: Motivated by the difficulty of specifying complete ordinal preferences over a large set of $m$ candidates, we study voting rules that are computable by querying voters about $t < m$ candidates. Generalizing prior works that focused on specific instances of this problem, our paper fully characterizes the set of positional scoring rules that can be computed for any $1 \leq t < m$, which notably does not include plurality. We then extend this to show a similar impossibility result for single transferable vote (elimination voting). These negative results are information-theoretic and agnostic to the number of queries. Finally, for scoring rules that are computable with limited-sized queries, we give parameterized upper and lower bounds on the number of such queries a deterministic or randomized algorithm must make to determine the score-maximizing candidate. While there is no gap between our bounds for deterministic algorithms, identifying
    
[^173]: 男性CEO和女性助理：通过成对刻板印象测试探究文本-图像模型中的性别偏见

    The Male CEO and the Female Assistant: Probing Gender Biases in Text-To-Image Models Through Paired Stereotype Test

    [https://arxiv.org/abs/2402.11089](https://arxiv.org/abs/2402.11089)

    通过成对刻板印象测试（PST）框架，在文本-图像模型中探究性别偏见，并评估了DALLE-3在性别职业和组织权力方面的偏见。

    

    最近大规模的文本到图像（T2I）模型（如DALLE-3）展示了在新应用中的巨大潜力，但也面临前所未有的公平挑战。先前的研究揭示了单人图像生成中的性别偏见，但T2I模型应用可能需要同时描绘两个或更多人。该设定中的潜在偏见仍未被探究，导致使用中的公平相关风险。为了研究T2I模型中性别偏见的基本方面，我们提出了一种新颖的成对刻板印象测试（PST）偏见评估框架。PST促使模型生成同一图像中的两个个体，用与相反性别刻板印象相关联的两个社会身份来描述他们。通过生成的图像遵从性别刻板印象的程度来衡量偏见。利用PST，我们从两个角度评估DALLE-3：性别职业中的偏见和组织权力中的偏见。

    arXiv:2402.11089v1 Announce Type: cross  Abstract: Recent large-scale Text-To-Image (T2I) models such as DALLE-3 demonstrate great potential in new applications, but also face unprecedented fairness challenges. Prior studies revealed gender biases in single-person image generation, but T2I model applications might require portraying two or more people simultaneously. Potential biases in this setting remain unexplored, leading to fairness-related risks in usage. To study these underlying facets of gender biases in T2I models, we propose a novel Paired Stereotype Test (PST) bias evaluation framework. PST prompts the model to generate two individuals in the same image. They are described with two social identities that are stereotypically associated with the opposite gender. Biases can then be measured by the level of conformation to gender stereotypes in generated images. Using PST, we evaluate DALLE-3 from 2 perspectives: biases in gendered occupation and biases in organizational power.
    
[^174]: AI安全痛点金字塔

    The AI Security Pyramid of Pain

    [https://arxiv.org/abs/2402.11082](https://arxiv.org/abs/2402.11082)

    介绍了AI安全痛点金字塔框架，提供了一种结构化方法来理解和应对各个级别的AI威胁。

    

    我们介绍了AI安全痛点金字塔，这是一个将网络安全痛点金字塔应用于AI特定威胁的框架。该框架为理解和解决各个级别的AI威胁提供了一种结构化方法。从基础开始，金字塔强调了数据完整性，这对于数据集和AI模型的准确性和可靠性至关重要，包括它们的权重和参数。确保数据完整性至关重要，因为它支撑了所有基于AI的决策和运营的有效性。下一个级别，AI系统性能，关注MLOps驱动的指标，如模型漂移、准确性和误报率。这些指标对于检测潜在的安全漏洞至关重要，可以进行早期干预并维护AI系统的完整性。进一步深入，金字塔解决了敌对工具带来的威胁，识别并中和被用于adversarial攻击的工具等等。

    arXiv:2402.11082v1 Announce Type: cross  Abstract: We introduce the AI Security Pyramid of Pain, a framework that adapts the cybersecurity Pyramid of Pain to categorize and prioritize AI-specific threats. This framework provides a structured approach to understanding and addressing various levels of AI threats. Starting at the base, the pyramid emphasizes Data Integrity, which is essential for the accuracy and reliability of datasets and AI models, including their weights and parameters. Ensuring data integrity is crucial, as it underpins the effectiveness of all AI-driven decisions and operations. The next level, AI System Performance, focuses on MLOps-driven metrics such as model drift, accuracy, and false positive rates. These metrics are crucial for detecting potential security breaches, allowing for early intervention and maintenance of AI system integrity. Advancing further, the pyramid addresses the threat posed by Adversarial Tools, identifying and neutralizing tools used by ad
    
[^175]: 通过纯微调进行模型编辑

    Model Editing by Pure Fine-Tuning

    [https://arxiv.org/abs/2402.11078](https://arxiv.org/abs/2402.11078)

    纯微调通过优化条件似然、增加随机释义和事实的数据，在模型编辑中取得了不俗的表现。

    

    精细调整被认为在模型编辑中不够有效，因为相对更专业的方法而言，它的表现较差。然而，微调是简单的，不关心被编辑模型的体系结构细节，并且能够利用标准训练方法的不断进展（例如PEFT），使其成为模型编辑器的吸引选择。在本文中，我们展示了纯粹的微调可以是一种可行的模型编辑方法。我们提出了对朴素微调进行轻微修改的两个关键因素。第一，我们优化条件似然而非完整似然。第二，我们使用随机释义和事实来增加数据，以鼓励泛化和局部性。我们在ZsRE和CounterFact上的实验表明，这一简单修改使得微调通常可以与专业编辑器在编辑分数方面匹敌甚至超越。

    arXiv:2402.11078v1 Announce Type: cross  Abstract: Fine-tuning is dismissed as not effective for model editing due to its poor performance compared to more specialized methods. However, fine-tuning is simple, agnostic to the architectural details of the model being edited, and able to leverage ongoing advances in standard training methods (e.g., PEFT), making it an appealing choice for a model editor. In this work, we show that pure fine-tuning can be a viable approach to model editing. We propose a slight modification of naive fine-tuning with two key ingredients. First, we optimize the conditional likelihood rather than the full likelihood. Second, we augment the data with random paraphrases and facts to encourage generalization and locality. Our experiments on ZsRE and CounterFact show that this simple modification allows fine-tuning to often match or outperform specialized editors in the edit score.
    
[^176]: AFaCTA: 使用可靠的LLM标注者辅助事实性索赔检测的标注

    AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators

    [https://arxiv.org/abs/2402.11073](https://arxiv.org/abs/2402.11073)

    提出了一种新框架 AFaCTA，利用大型语言模型辅助事实性索赔的标注，提高了标注的效率和一致性。

    

    随着生成式人工智能的兴起，用于打击误导信息的自动事实核查方法变得越来越重要。然而，事实性索赔检测，即事实核查管道中的第一步，存在两个关键问题限制了其可伸缩性和泛化性：（1）任务定义和索赔概念的不一致性以及（2）手动标注的高成本。为了解决（1），我们审查了相关工作中的定义，并提出了一个聚焦于可验证性的事实性索赔的统一定义。为了解决（2），我们引入了AFaCTA（自动事实性索赔检测标注器），这是一个新颖的框架，利用大型语言模型（LLMs）在事实性索赔的标注中提供帮助。AFaCTA通过沿着三条预定义的推理路径保持一致性来校准其注释的置信度。在政治言论领域的大量评估和实验表明，AFaCTA能够高效地协助专业人员进行标注。

    arXiv:2402.11073v1 Announce Type: cross  Abstract: With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist exper
    
[^177]: 架起因果发现与大型语言模型之间的桥梁：整合方法和未来方向的综合调查

    Bridging Causal Discovery and Large Language Models: A Comprehensive Survey of Integrative Approaches and Future Directions

    [https://arxiv.org/abs/2402.11068](https://arxiv.org/abs/2402.11068)

    本文综合调查了将大型语言模型（如GPT4）整合到因果发现任务中的方法，揭示了它们在推断因果结构时对元数据和自然语言的创新利用，强调了LLMs在增强传统CD方法和作为专家辅助方面的潜力和挑战。

    

    因果发现（CD）和大型语言模型（LLMs）代表着两个具有重要影响力的人工智能研究领域。尽管它们起源不同，CD侧重于从数据中揭示因果关系，LLMs则侧重于处理和生成类似人类的文本，但这两个领域的融合为理解复杂系统提供了新颖的见解和方法论。本文介绍了将LLMs（如GPT4）整合到CD任务中的综合调查。我们系统地审查和比较了利用LLMs进行各种CD任务的现有方法，并突出了它们对元数据和自然语言的创新利用以推断因果结构。我们的分析揭示了LLMs在增强传统CD方法和作为不完美专家方面的优势和潜力，同时也揭示了当前实践中固有的挑战和限制。此外，我们确定了文献中的空白。

    arXiv:2402.11068v1 Announce Type: cross  Abstract: Causal discovery (CD) and Large Language Models (LLMs) represent two emerging fields of study with significant implications for artificial intelligence. Despite their distinct origins, CD focuses on uncovering cause-effect relationships from data, and LLMs on processing and generating humanlike text, the convergence of these domains offers novel insights and methodologies for understanding complex systems. This paper presents a comprehensive survey of the integration of LLMs, such as GPT4, into CD tasks. We systematically review and compare existing approaches that leverage LLMs for various CD tasks and highlight their innovative use of metadata and natural language to infer causal structures. Our analysis reveals the strengths and potential of LLMs in both enhancing traditional CD methods and as an imperfect expert, alongside the challenges and limitations inherent in current practices. Furthermore, we identify gaps in the literature 
    
[^178]: Persona-DB：用于响应预测的高效大规模语言模型个性化与协同数据优化

    Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement

    [https://arxiv.org/abs/2402.11060](https://arxiv.org/abs/2402.11060)

    介绍了 Persona-DB，一个简单却有效的框架，通过层级构建过程和协同优化，改善了大规模语言模型个性化中数据库表示的泛化能力和检索效率。

    

    随着对大型语言模型（LLMs）个性化交互需求的增加，需要开发能够准确快速识别用户意见和偏好的方法。检索增强作为一种有效策略出现，因为它可以适应大量用户而无需进行微调的成本。然而，现有研究主要集中在增强检索阶段，并对数据库表示的优化进行了有限的探索，这是个性化等任务的关键方面。在这项工作中，我们从一个新的角度研究了这个问题，着重于如何更有效地表示数据，以便在LLM定制的情境下更有效地进行检索。为了解决这一挑战，我们介绍了Persona-DB，这是一个简单而有效的框架，包括一个分层构建过程，以改善跨任务背景的泛化能力，并进行协同优化。

    arXiv:2402.11060v1 Announce Type: cross  Abstract: The increasing demand for personalized interactions with large language models (LLMs) calls for the development of methodologies capable of accurately and efficiently identifying user opinions and preferences. Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning. Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization. In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more efficient retrieval in the context of LLM customization. To tackle this challenge, we introduce Persona-DB, a simple yet effective framework consisting of a hierarchical construction process to improve generalization across task contexts and collaborative refinement to
    
[^179]: 大型语言模型的短板：理解侦探叙事中复杂关系

    Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives

    [https://arxiv.org/abs/2402.11051](https://arxiv.org/abs/2402.11051)

    提出了Conan数据集，用于从侦探叙事中提取和分析复杂的人物关系图，并揭示大型语言模型在推理复杂关系和处理长篇叙事方面的局限性。

    

    现有的叙事理解数据集往往无法表达现实社会场景中关系的复杂性和不确定性。为了弥补这一空白，我们引入了一个新的基准数据集Conan，旨在从侦探叙事中提取和分析复杂的人物关系图。具体地，我们设计了分层关系类别，并从不同角色的角度手动提取和注释了以角色为导向的关系，包括大多数角色知晓的公开关系和仅少数角色知晓的秘密关系。我们对GPT-3.5、GPT-4和Llama2等先进的大型语言模型进行的实验揭示了它们在推理复杂关系和处理较长叙事方面的局限性。Conan数据集与我们的流程策略的结合旨在了解大型语言模型理解叙事背景中微妙关系动态的能力。

    arXiv:2402.11051v1 Announce Type: cross  Abstract: Existing datasets for narrative understanding often fail to represent the complexity and uncertainty of relationships in real-life social scenarios. To address this gap, we introduce a new benchmark, Conan, designed for extracting and analysing intricate character relation graphs from detective narratives. Specifically, we designed hierarchical relationship categories and manually extracted and annotated role-oriented relationships from the perspectives of various characters, incorporating both public relationships known to most characters and secret ones known to only a few. Our experiments with advanced Large Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their limitations in inferencing complex relationships and handling longer narratives. The combination of the Conan dataset and our pipeline strategy is geared towards understanding the ability of LLMs to comprehend nuanced relational dynamics in narrative contexts.
    
[^180]: 探究价值偏好：LLMs偏向理想状态的偏差

    Exploring Value Biases: How LLMs Deviate Towards the Ideal

    [https://arxiv.org/abs/2402.11005](https://arxiv.org/abs/2402.11005)

    研究发现大型语言模型（LLMs）在给出响应时存在一个价值偏好的机制，倾向于偏向理想状态，这种偏差会对不同应用场景产生重要影响。

    

    大型语言模型（LLMs）被部署在各种应用中，并且它们的响应对社会产生着越来越大的影响。理解LLMs在给出响应时的非故意机制对于解释它们的性能并辨别它们在现实世界应用中的偏差至关重要。这类似于人类研究中，这种无意识的响应被称为抽样。我们研究了LLMs的这种抽样现象，发现LLMs的抽样倾向于偏爱高价值选项。价值偏好对应于从最可能的响应向LLM中代表的理想价值的转变。实际上，即便是通过上下文提示学习到的新实体，这种效果也能够再现。我们表明这种偏差表现在意想不到的地方，并对选择典型实例等相关应用场景产生影响。结果显示，价值偏好在不同分类的LLMs中都很明显。

    arXiv:2402.11005v1 Announce Type: cross  Abstract: Large-Language-Models (LLMs) are deployed in a wide range of applications, and their response has an increasing social impact. Understanding the non-deliberate(ive) mechanism of LLMs in giving responses is essential in explaining their performance and discerning their biases in real-world applications. This is analogous to human studies, where such inadvertent responses are referred to as sampling. We study this sampling of LLMs in light of value bias and show that the sampling of LLMs tends to favour high-value options. Value bias corresponds to this shift of response from the most likely towards an ideal value represented in the LLM. In fact, this effect can be reproduced even with new entities learnt via in-context prompting. We show that this bias manifests in unexpected places and has implications on relevant application scenarios, like choosing exemplars. The results show that value bias is strong in LLMs across different categor
    
[^181]: ASGEA：利用Align-Subgraphs中的逻辑规则进行实体对齐

    ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment

    [https://arxiv.org/abs/2402.11000](https://arxiv.org/abs/2402.11000)

    提出了一个新的实体对齐框架ASGEA，利用Align-Subgraphs中的逻辑规则，设计了可解释的基于路径的图神经网络ASGNN，引入了多模态注意机制，取得了令人满意的实验结果

    

    实体对齐（EA）旨在识别代表相同现实世界对象的不同知识图中的实体。最近基于嵌入的EA方法在EA方面取得了最先进的性能，但面临着解释性挑战，因为它们完全依赖于嵌入距离，并忽视了一对对齐实体背后的逻辑规则。在本文中，我们提出了Align-Subgraph实体对齐（ASGEA）框架来利用Align-Subgraphs中的逻辑规则。ASGEA使用锚链接作为桥梁来构建Align-Subgraphs，并沿着跨知识图的路径传播，这使其区别于基于嵌入的方法。此外，我们设计了一种可解释的基于路径的图神经网络ASGNN，以有效识别和整合跨知识图的逻辑规则。我们还引入了一个节点级多模态注意机制，结合多模态增强的锚点来增强Align-Subgraph。我们的实验结果

    arXiv:2402.11000v1 Announce Type: cross  Abstract: Entity alignment (EA) aims to identify entities across different knowledge graphs that represent the same real-world objects. Recent embedding-based EA methods have achieved state-of-the-art performance in EA yet faced interpretability challenges as they purely rely on the embedding distance and neglect the logic rules behind a pair of aligned entities. In this paper, we propose the Align-Subgraph Entity Alignment (ASGEA) framework to exploit logic rules from Align-Subgraphs. ASGEA uses anchor links as bridges to construct Align-Subgraphs and spreads along the paths across KGs, which distinguishes it from the embedding-based methods. Furthermore, we design an interpretable Path-based Graph Neural Network, ASGNN, to effectively identify and integrate the logic rules across KGs. We also introduce a node-level multi-modal attention mechanism coupled with multi-modal enriched anchors to augment the Align-Subgraph. Our experimental results 
    
[^182]: 分析和预测2型糖尿病老年患者的多类分类死亡率

    Analysis and Mortality Prediction using Multiclass Classification for Older Adults with Type 2 Diabetes

    [https://arxiv.org/abs/2402.10999](https://arxiv.org/abs/2402.10999)

    设计治疗方案需注意患者剩余生命和合并症，研究利用大规模数据集构建多类分类模型来预测老年2型糖尿病患者的死亡率。

    

    设计合适的治疗方案来管理糖尿病要求医护人员注意患者剩余的生命以及影响他们的合并症。本研究利用了一个包含68个潜在死亡预测因子的结构化数据集，针对275,190名年龄在65岁或以上的美国退伍军人进行了研究。通过将两个原始目标变量组合起来创造了一个新的目标变量。通过离散化连续变量来处理异常值，对分类变量进行了虚拟编码。通过随机欠采样实现了类平衡。使用带LASSO的多项式逻辑回归建立了基准回归模型。采用卡方检验和信息增益作为基于过滤的特征选择技术。分类器包括多项式逻辑回归、随机森林、极端梯度提升（XGB）。

    arXiv:2402.10999v1 Announce Type: cross  Abstract: Designing proper treatment plans to manage diabetes requires health practitioners to pay heed to the individuals remaining life along with the comorbidities affecting them. Older adults with Type 2 Diabetes Mellitus (T2DM) are prone to experience premature death or even hypoglycaemia. The structured dataset utilized has 68 potential mortality predictors for 275,190 diabetic U.S. military Veterans aged 65 years or older. A new target variable is invented by combining the two original target variables. Outliers are handled by discretizing the continuous variables. Categorical variables have been dummy encoded. Class balancing is achieved by random under-sampling. A benchmark regression model is built using Multinomial Logistic Regression with LASSO. Chi-Squared and Information Gain are the filter-based feature selection techniques utilized. Classifiers such as Multinomial Logistic Regression, Random Forest, Extreme Gradient Boosting (XGB
    
[^183]: 通过差分动态逻辑确保神经网络控制器的安全性

    Provably Safe Neural Network Controllers via Differential Dynamic Logic

    [https://arxiv.org/abs/2402.10998](https://arxiv.org/abs/2402.10998)

    通过差分动态逻辑与神经网络验证相结合的VerSAILLE方法，实现了对神经网络控制系统在无限时间范围上的安全性证明。

    

    虽然神经网络（NN）作为面向目标的控制器在网络物理系统中具有巨大潜力，但验证基于神经网络的控制系统（NNCS）的安全性对于实际应用NN来说面临着重大挑战，特别是当需要对无界时间范围进行安全性验证时。我们引入了VerSAILLE（通过逻辑链接包验证的可验证安全人工智能）：这是差分动态逻辑（dL）和NN验证组合的第一种方法。通过合作，我们可以利用NN验证工具的效率，同时保留dL的严谨性。我们提出了一个控制器信封的安全性证明，以证明无限时间范围上具体NNCS的安全性。VerSAILLE导致的NN验证属性通常需要非线性算术，而高效的NN验证工具仅支持线性算术。

    arXiv:2402.10998v1 Announce Type: cross  Abstract: While neural networks (NNs) have a large potential as goal-oriented controllers for Cyber-Physical Systems, verifying the safety of neural network based control systems (NNCSs) poses significant challenges for the practical use of NNs -- especially when safety is needed for unbounded time horizons. One reason for this is the intractability of NN and hybrid system analysis. We introduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The first approach for the combination of differential dynamic logic (dL) and NN verification. By joining forces, we can exploit the efficiency of NN verification tools while retaining the rigor of dL. We reflect a safety proof for a controller envelope in an NN to prove the safety of concrete NNCS on an infinite-time horizon. The NN verification properties resulting from VerSAILLE typically require nonlinear arithmetic while efficient NN verification tools merely support linear arithmetic. T
    
[^184]: 在大型语言模型中理解人工智能：语义基础

    "Understanding AI": Semantic Grounding in Large Language Models

    [https://arxiv.org/abs/2402.10992](https://arxiv.org/abs/2402.10992)

    大型语言模型（LLMs）展现了对语义的渐进理解，通过应用心灵哲学和语言学中关于含义的核心假设，研究发现LLMs不仅仅是生成文本的工具，而是在某种程度上已经理解了它们生成的语言。

    

    近年来我们目睹了人工智能的生成式转变，生成模型，包括大型语言模型（LLMs），对于自监督学习至关重要。我们提出一个问题，LLMs是否理解其生成的文本的含义？它们是否具有语义基础？我们如何了解它们是否理解以及理解的是什么？本文探讨了对语义基础问题的评估，区分和讨论了五种方法。其中最有前景的方法是将心灵哲学和语言学中关于含义的核心假设应用于LLMs。我们发现，语义基础是一个渐进的过程，包括功能性、社会性和因果性三个维度的区分。LLMs在这三个维度上展现出基本证据。一个强有力的论据是LLMs会形成世界模型。因此，LLMs既不是随机的鹦鹉，也不是语义僵尸，而是至少在基本层面上已经理解它们生成的语言。

    arXiv:2402.10992v1 Announce Type: cross  Abstract: Do LLMs understand the meaning of the texts they generate? Do they possess a semantic grounding? And how could we understand whether and what they understand? I start the paper with the observation that we have recently witnessed a generative turn in AI, since generative models, including LLMs, are key for self-supervised learning. To assess the question of semantic grounding, I distinguish and discuss five methodological ways. The most promising way is to apply core assumptions of theories of meaning in philosophy of mind and language to LLMs. Grounding proves to be a gradual affair with a three-dimensional distinction between functional, social and causal grounding. LLMs show basic evidence in all three dimensions. A strong argument is that LLMs develop world models. Hence, LLMs are neither stochastic parrots nor semantic zombies, but already understand the language they generate, at least in an elementary sense.
    
[^185]: 加速半异步联邦学习

    Accelerating Semi-Asynchronous Federated Learning

    [https://arxiv.org/abs/2402.10991](https://arxiv.org/abs/2402.10991)

    提出了一种考虑贡献的异步联邦学习方法，动态调整接收到的更新的处理方式，以解决现实情况下同步上传数据可能出现的缓慢和不可靠问题。

    

    联邦学习（FL）是一种分布式机器学习范例，允许客户端在保护隐私的同时在其数据上训练模型。现有的FL算法，如Federated Averaging（FedAvg）及其变种，在许多情况下已经被证明收敛良好。然而，这些方法需要客户端以同步方式将其本地更新上传至服务器，这在现实情况下可能会变得缓慢和不可靠。为了解决这个问题，研究人员开发了异步FL方法，允许客户端继续使用陈旧的全局模型对其本地数据进行训练。然而，大多数这些方法仅仅聚合了所有接收到的更新，而没有考虑其相对贡献，这可能导致收敛速度变慢。在本文中，我们提出了一种考虑贡献的异步FL方法，考虑了接收到的更新的陈旧程度和统计异质性。我们的方法动态调整

    arXiv:2402.10991v1 Announce Type: cross  Abstract: Federated Learning (FL) is a distributed machine learning paradigm that allows clients to train models on their data while preserving their privacy. FL algorithms, such as Federated Averaging (FedAvg) and its variants, have been shown to converge well in many scenarios. However, these methods require clients to upload their local updates to the server in a synchronous manner, which can be slow and unreliable in realistic FL settings. To address this issue, researchers have developed asynchronous FL methods that allow clients to continue training on their local data using a stale global model. However, most of these methods simply aggregate all of the received updates without considering their relative contributions, which can slow down convergence. In this paper, we propose a contribution-aware asynchronous FL method that takes into account the staleness and statistical heterogeneity of the received updates. Our method dynamically adju
    
[^186]: WilKE：智慧层知识编辑器用于终身知识编辑

    WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing

    [https://arxiv.org/abs/2402.10987](https://arxiv.org/abs/2402.10987)

    该研究提出了一种名为WilKE的知识编辑方法，通过选择编辑层来匹配不同层级中的知识编辑模式程度，在终身编辑中相比最先进的方法平均展现了46.2%和67.8%的改进。

    

    知识编辑旨在纠正大型语言模型（LLMs）中的不准确性，而无需为过时或错误的知识进行昂贵的重新训练。然而，当前的知识编辑方法主要集中于单次编辑，未能满足终身编辑的要求。本文中，终身编辑与终身知识编辑同义。本研究揭示了知识编辑在终身编辑中遇到的性能下降问题，其特征为毒性积累和毒性闪现，主要原因是模式不匹配。我们介绍了一种名为WilKE的知识编辑方法，它根据不同层级中编辑知识的模式匹配程度选择编辑层。实验结果表明，在终身编辑中，相对于最先进的知识编辑方法，WilKE在编辑GPT2-XL和GPT-J方面分别平均改进了46.2%和67.8%。

    arXiv:2402.10987v1 Announce Type: cross  Abstract: Knowledge editing aims to rectify inaccuracies in large language models (LLMs) without costly retraining for outdated or erroneous knowledge. However, current knowledge editing methods primarily focus on single editing, failing to meet the requirements for lifelong editing. In this paper, lifelong editing is synonymous with lifelong knowledge editing. This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch. We introduce a knowledge editing approach named WilKE, which selects editing layer based on the pattern matching degree of editing knowledge across different layers. Experimental results demonstrate that, in lifelong editing, WilKE exhibits an average improvement of 46.2\% and 67.8\% on editing GPT2-XL and GPT-J relative to state-of-the-art knowledge editing methods.
    
[^187]: FinTral：一类GPT-4级别的多模态金融大型语言模型

    FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models

    [https://arxiv.org/abs/2402.10986](https://arxiv.org/abs/2402.10986)

    FinTral是一类基于Mistral-7b模型的GPT-4级别多模态金融大型语言模型，通过领域特定的预训练和检索方法优化，在AI驱动金融技术中取得显著进展。

    

    我们引入FinTral，这是一组基于Mistral-7b模型构建的一流多模态大型语言模型（LLMs），专门为金融分析定制。FinTral整合了文本、数字、表格和图像数据。我们通过利用为本研究策划的大量文本和视觉数据集，通过领域特定的预训练、指导微调和RLAIF训练增强了FinTral。我们还介绍了一个包含九个任务和25个数据集进行评估的广泛基准测试，其中包括金融领域的幻觉。我们的FinTral模型，通过采用先进的工具和检索方法进行直接偏好优化训练，命名为FinTral-DPO-T&R，展现了出色的零-shot性能。它在所有任务中均优于ChatGPT-3.5，并在九项任务中的五项中超越GPT-4，标志着人工智能驱动的金融技术的重要进步。我们还展示了FinTral具有潜力

    arXiv:2402.10986v1 Announce Type: cross  Abstract: We introduce FinTral, a suite of state-of-the-art multimodal large language models (LLMs) built upon the Mistral-7b model and tailored for financial analysis. FinTral integrates textual, numerical, tabular, and image data. We enhance FinTral with domain-specific pretraining, instruction fine-tuning, and RLAIF training by exploiting a large collection of textual and visual datasets we curate for this work. We also introduce an extensive benchmark featuring nine tasks and 25 datasets for evaluation, including hallucinations in the financial domain. Our FinTral model trained with direct preference optimization employing advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&R, demonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5 in all tasks and surpasses GPT-4 in five out of nine tasks, marking a significant advancement in AI-driven financial technology. We also demonstrate that FinTral has the potential to e
    
[^188]: 利用AI规划技术检测云安全漏洞

    Leveraging AI Planning For Detecting Cloud Security Vulnerabilities

    [https://arxiv.org/abs/2402.10985](https://arxiv.org/abs/2402.10985)

    提出了一个通用框架来建模云系统中的访问控制策略，并开发了基于PDDL模型的新方法来检测可能导致诸如勒索软件和敏感数据外泄等广泛攻击的安全漏洞。

    

    云计算服务提供了可扩展且具有成本效益的数据存储、处理和协作解决方案。随着它们的普及，与其安全漏洞相关的担忧也在增长，这可能导致数据泄露和勒索软件等复杂攻击。为了应对这些问题，我们首先提出了一个通用框架，用于表达云系统中不同对象（如用户、数据存储、安全角色）之间的关系，以建模云系统中的访问控制策略。访问控制误配置通常是云攻击的主要原因。其次，我们开发了一个PDDL模型，用于检测安全漏洞，例如可能导致广泛攻击（如勒索软件）和敏感数据外泄等。规划器可以生成攻击以识别云中的此类漏洞。最后，我们在14个不同商业组织的真实亚马逊AWS云配置上测试了我们的方法。

    arXiv:2402.10985v1 Announce Type: cross  Abstract: Cloud computing services provide scalable and cost-effective solutions for data storage, processing, and collaboration. Alongside their growing popularity, concerns related to their security vulnerabilities leading to data breaches and sophisticated attacks such as ransomware are growing. To address these, first, we propose a generic framework to express relations between different cloud objects such as users, datastores, security roles, to model access control policies in cloud systems. Access control misconfigurations are often the primary driver for cloud attacks. Second, we develop a PDDL model for detecting security vulnerabilities which can for example lead to widespread attacks such as ransomware, sensitive data exfiltration among others. A planner can then generate attacks to identify such vulnerabilities in the cloud. Finally, we test our approach on 14 real Amazon AWS cloud configurations of different commercial organizations
    
[^189]: CHEMREASONER：使用量子化学反馈在大型语言模型的知识空间中进行启发式搜索

    CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback

    [https://arxiv.org/abs/2402.10980](https://arxiv.org/abs/2402.10980)

    通过将大型语言模型推理与量子化学反馈相结合，我们引入了一个AI引导的计算筛选框架，将催化剂发现形式化为一个不确定环境，从而实现高效催化剂的积极搜索

    

    arXiv:2402.10980v1 类型公告：跨领域 摘要：发现新的催化剂对于设计新的更高效的化学过程至关重要，以实现向可持续未来的过渡。我们引入了一种人工智能引导的计算筛选框架，将语言推理与基于量子化学的三维原子表示的反馈统一起来。我们的方法将催化剂发现构建为一个不确定环境，其中一个代理通过大型语言模型（LLM）推导的假设与基于原子图神经网络（GNN）的反馈的迭代组合，积极搜索高效催化剂。在中间搜索步骤确定的催化剂经过基于空间定向、反应途径和稳定性的结构评估。基于吸附能和势垒的评分函数引导在LLM的知识空间中向能量有利、高效的催化剂探索。我们引入了可以自动规划的方法

    arXiv:2402.10980v1 Announce Type: cross  Abstract: The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automaticall
    
[^190]: SportsMetrics:将文本和数值数据融合以理解LLM中的信息融合

    SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs

    [https://arxiv.org/abs/2402.10979](https://arxiv.org/abs/2402.10979)

    这项研究介绍了围绕体育数据分析展开的四项新任务，旨在评估大型语言模型在数值推理和信息融合方面的能力。

    

    arXiv:2402.10979v1 公告类型：交叉 摘要：大型语言模型对于整合文本文档和数据库记录等各种数据类型进行先进分析具有重要潜力。然而，融合文本和数值数据存在重大挑战。LLMs需要处理和交叉引用实体和数字，处理数据不一致性和冗余，并发展规划能力，比如构建用于管理复杂数据查询的工作内存。在本文中，我们介绍了围绕体育数据分析的四项新颖任务，以评估LLMs的数值推理和信息融合能力。这些任务涉及向LLMs提供详细的逐场比赛描述，然后在面对诸如新比赛规则、更长持续时间、故事混乱以及分析比赛摘要中的关键统计数据等对抗性场景。我们对NBA和NFL比赛进行了大量实验，以评估LLMs在该领域的表现。

    arXiv:2402.10979v1 Announce Type: cross  Abstract: Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on the
    
[^191]: 具有符合事实性保证的语言模型

    Language Models with Conformal Factuality Guarantees

    [https://arxiv.org/abs/2402.10978](https://arxiv.org/abs/2402.10978)

    提出了一种能够通过连接语言建模和符合预测为语言模型提供高概率正确性保证的框架。

    

    语言模型（LM）输出的正确性和事实性保证是一个重要的开放问题。在这项工作中，我们提出了符合事实性，这是一个框架，可以通过连接语言建模和符合预测，确保LM的高概率正确性保证。我们观察到，LM输出的正确性等价于一个不确定性量化问题，其中不确定性集被定义为LM输出的蕴含集。利用这种联系，我们表明，语言模型中的符合预测对应于一种后退算法，通过逐渐使LM输出变得不太具体（并扩大相关的不确定性集）提供高概率的正确性保证。这种方法适用于任何黑盒LM，并且需要很少的人工注释样本。我们在封闭书籍QA（FActScore，NaturalQuestions）和推理任务（MATH）上对我们的方法进行评估，结果表明我们的方法可以p

    arXiv:2402.10978v1 Announce Type: cross  Abstract: Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem. In this work, we propose conformal factuality, a framework that can ensure high probability correctness guarantees for LMs by connecting language modeling and conformal prediction. We observe that the correctness of an LM output is equivalent to an uncertainty quantification problem, where the uncertainty sets are defined as the entailment set of an LM's output. Using this connection, we show that conformal prediction in language models corresponds to a back-off algorithm that provides high probability correctness guarantees by progressively making LM outputs less specific (and expanding the associated uncertainty sets). This approach applies to any black-box LM and requires very few human-annotated samples. Evaluations of our approach on closed book QA (FActScore, NaturalQuestions) and reasoning tasks (MATH) show that our approach can p
    
[^192]: 利用语义技术对青少年酒精使用障碍进行个性化特征和风险评估的社交网络分析

    Social network analysis for personalized characterization and risk assessment of alcohol use disorders in adolescents using semantic technologies

    [https://arxiv.org/abs/2402.10967](https://arxiv.org/abs/2402.10967)

    本研究利用语义技术和社交网络分析技术，自动化数据收集和分析阶段，为青少年酒精使用障碍的个性化特征和风险评估提供了新的方法。

    

    酒精使用障碍(AUD)是全球公共卫生组织特别关注的问题，尤其是涉及青少年群体。青少年饮酒行为受到朋友甚至父母饮酒的影响。许多研究利用社交网络分析(SNA)技术研究青少年涉及的不同社交网络(同龄人、朋友、家人等)。这类研究需要通过问卷调查进行数据收集，然后利用SNA技术进行后续分析。这一过程涉及多个手动数据处理阶段，耗时且容易出错。利用知识工程技术(包括构建领域本体论)来表示信息，可以实现从初始数据收集到分析阶段的所有活动自动化。

    arXiv:2402.10967v1 Announce Type: new  Abstract: Alcohol Use Disorder (AUD) is a major concern for public health organizations worldwide, especially as regards the adolescent population. The consumption of alcohol in adolescents is known to be influenced by seeing friends and even parents drinking alcohol. Building on this fact, a number of studies into alcohol consumption among adolescents have made use of Social Network Analysis (SNA) techniques to study the different social networks (peers, friends, family, etc.) with whom the adolescent is involved. These kinds of studies need an initial phase of data gathering by means of questionnaires and a subsequent analysis phase using the SNA techniques. The process involves a number of manual data handling stages that are time consuming and error-prone. The use of knowledge engineering techniques (including the construction of a domain ontology) to represent the information, allows the automation of all the activities, from the initial data
    
[^193]: 在语言模型对话中测量和控制“人设”漂移

    Measuring and Controlling Persona Drift in Language Model Dialogs

    [https://arxiv.org/abs/2402.10962](https://arxiv.org/abs/2402.10962)

    提出了一种量化基准来测量语言模型对话中的“人设”漂移，并提出了一种称为split-softmax的轻量级方法来对抗注意力衰减和“人设”漂移

    

    提示是定制语言模型聊天机器人的标准工具，使其能够承担特定的“人设”。在使用提示时的一个隐含假设是，它们将是稳定的，因此聊天机器人将在整个对话过程中继续根据规定的“人设”生成文本。我们提出了一个量化基准来测试这一假设，通过两个个性化聊天机器人之间的自我对话来评估“人设”的稳定性。我们对流行模型如LLaMA2-chat-70B进行测试，发现在八轮对话中存在显著的“人设”漂移。对这一现象的实证和理论分析表明，由于长对话中的注意力衰减，变压器注意力机制起到了一定作用。为了对抗注意力衰减和“人设”漂移，我们提出了一种称为split-softmax的轻量级方法，与两个强基线方法相比表现优异。

    arXiv:2402.10962v1 Announce Type: cross  Abstract: Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.
    
[^194]: 相对优先权优化: 通过对相同和不同提示的对比响应增强LLM对齐

    Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts

    [https://arxiv.org/abs/2402.10958](https://arxiv.org/abs/2402.10958)

    提出了相对优先权优化（RPO）方法，通过区分来自相同和相关提示的更受青睐的响应和更不受青睐的响应，扩展了模型的学习能力。

    

    在大型语言模型（LLM）领域，将模型与用户的多样化偏好相一致是一个关键挑战。直接优先权优化（DPO）在这一领域起到了关键作用。DPO通过使用从相同提示中派生的偏好对来工作，而无需额外的奖励模型。然而，DPO并不能完全反映人类学习的复杂性，这种学习往往涉及对不仅相同而且相似问题的对比响应的理解。为了克服这一不足，我们提出了相对优先权优化（RPO）。RPO旨在区分来自相同和相关提示的更受青睐的响应和更不受青睐的响应。它引入了对比加权机制，使LLMs能够使用更广泛的偏好数据进行调整，包括成对和不成对的数据集。这种方法扩展了模型的学习能力，使其能够利用更多的偏好数据进行优化。

    arXiv:2402.10958v1 Announce Type: cross  Abstract: In the field of large language models (LLMs), aligning models with the diverse preferences of users is a critical challenge. Direct Preference Optimization (DPO) has played a key role in this area. It works by using pairs of preferences derived from the same prompts, and it functions without needing an additional reward model. However, DPO does not fully reflect the complex nature of human learning, which often involves understanding contrasting responses to not only identical but also similar questions. To overcome this shortfall, we propose Relative Preference Optimization (RPO). RPO is designed to discern between more and less preferred responses derived from both identical and related prompts. It introduces a contrastive weighting mechanism, enabling the tuning of LLMs using a broader range of preference data, including both paired and unpaired sets. This approach expands the learning capabilities of the model, allowing it to lever
    
[^195]: 在社交媒体上结合心理量表进行零-shot可解释的心理健康分析

    Zero-shot Explainable Mental Health Analysis on Social Media by incorporating Mental Scales

    [https://arxiv.org/abs/2402.10948](https://arxiv.org/abs/2402.10948)

    该方法结合心理量表通过LLMs进行零-shot心理健康分析，实验结果表明其优于其他方法

    

    传统的心理健康分析方法在容量方面表现强大，但缺乏解释能力，并且需要大规模注释的数据。另一方面，基于大型语言模型（LLMs）的生成式方法有潜力摆脱繁重的注释并提供解释。受到使用量表评估心理状态的心理评估实践的启发，我们的方法通过LLMs结合了两个程序。首先，患者完成心理健康问卷，其次，心理学家解释来自心理健康问题的收集信息并做出明智决策。实验结果表明，我们的方法胜过其他零-shot方法。

    arXiv:2402.10948v1 Announce Type: cross  Abstract: Traditional discriminative approaches in mental health analysis are known for their strong capacity but lack interpretability and demand large-scale annotated data. On the other hand, generative approaches, such as those based on large language models (LLMs),have the potential to get rid of heavy annotations and provide explanations. However, their capabilities still fall short compared to discriminative approaches, and their explanations may be unreliable due to the fact that the generation of explanation is a black-box process. Inspired by the psychological assessment practice of using scales to evaluate mental states, our method incorporates two procedures via LLMs. First, the patient completes mental health questionnaires, and second, the psychologist interprets the collected information from the mental health questions and makes informed decisions. Experimental results show that our method outperforms other zero-shot methods. Our 
    
[^196]: 将文化差异纳入大型语言模型的研究

    CultureLLM: Incorporating Cultural Differences into Large Language Models

    [https://arxiv.org/abs/2402.10946](https://arxiv.org/abs/2402.10946)

    提出了一种名为CultureLLM的成本效益高的解决方案，通过使用世界价值调查（WVS）作为种子数据，并通过提出的语义数据增强来将文化差异纳入大型语言模型中，成功微调得到了涵盖富裕和低资源语言的9种文化特定LLMs以及一个统一模型（CultureLLM-One）。

    

    大型语言模型（LLMs）被报道偏向于某些文化，因为训练数据主要来自英语语料库。由于多语种文化数据通常较难收集，现有的工作通过提示工程或特定文化的预训练来处理这一问题。然而，它们可能忽视了低资源文化的知识缺乏，并需要大量的计算资源。本文提出了CultureLLM，这是一个成本效益高的解决方案，可将文化差异纳入LLMs中。CultureLLM采用世界价值调查（WVS）作为种子数据，并通过提出的语义数据增强生成语义等效的训练数据。仅使用来自WVS的50个种子样本和增强数据，我们对9种包括富裕和低资源语言的文化特定LLMs和一个统一模型（CultureLLM-One）进行了微调。对60个与文化相关的数据集进行的大量实验表明，CultureLLM在增强LLM的文化特性方面取得了显著的成果。

    arXiv:2402.10946v1 Announce Type: cross  Abstract: Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM signif
    
[^197]: Text2Data：使用文本控制的低资源数据生成

    Text2Data: Low-Resource Data Generation with Textual Control

    [https://arxiv.org/abs/2402.10941](https://arxiv.org/abs/2402.10941)

    Text2Data提出了一种利用未标记数据通过无监督扩散模型来理解基础数据分布的新方法，以解决低资源环境下缺乏文本标签的文本到数据任务中的挑战。

    

    自然语言作为人类与机器无缝交互的一种常见直接控制信号。意识到这一接口的重要性，机器学习社区正在投入大量精力生成与文本指令在语义上一致的数据。虽然在涵盖图像编辑、音频合成、视频生成等领域取得了进展，但低资源领域由于昂贵注释或复杂数据结构（如分子、运动动态和时序）等特点，往往缺乏文本标签。这种不足阻碍了监督学习，从而限制了将先进生成模型应用于文本到数据任务的可能性。为了应对低资源场景中的这些挑战，我们提出了Text2Data，这是一种利用未标记数据通过无监督扩散模型来理解基础数据分布的新方法。

    arXiv:2402.10941v1 Announce Type: cross  Abstract: Natural language serves as a common and straightforward control signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, such as molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model
    
[^198]: 一种轻量级Inception增强U-Net神经网络用于布线可预测性

    A Lightweight Inception Boosted U-Net Neural Network for Routability Prediction

    [https://arxiv.org/abs/2402.10937](https://arxiv.org/abs/2402.10937)

    提出了一种轻量级Inception增强的U-Net神经网络模型，用于预测布线拥塞和设计规则检查热点，在实验中取得了显著的性能改进。

    

    随着现代CPU、GPU和NPU芯片设计复杂性和晶体管数量的不断增加，以及半导体技术节点不断缩小至近1纳米，布局和布线逐渐成为现代超大规模集成（VLSI）电路后端设计中最关键的两个过程。如何有效准确地在先期评估可路由性（在布局和全局布线阶段）已成为人工智能辅助电子设计自动化（EDA）领域的关键研究领域。本文提出了一种新颖的U-Net变体模型，通过嵌入Inception模块来预测路由拥塞（RC）和设计规则检查（DRC）热点。最近发布的CircuitNet数据集基准上的实验结果表明，我们提出的方法在Avg-NRMSE（平均归一化根均方误差）方面实现了高达5%（RC）和20%（DRC）的降低率。

    arXiv:2402.10937v1 Announce Type: cross  Abstract: As the modern CPU, GPU, and NPU chip design complexity and transistor counts keep increasing, and with the relentless shrinking of semiconductor technology nodes to nearly 1 nanometer, the placement and routing have gradually become the two most pivotal processes in modern very-large-scale-integrated (VLSI) circuit back-end design. How to evaluate routability efficiently and accurately in advance (at the placement and global routing stages) has grown into a crucial research area in the field of artificial intelligence (AI) assisted electronic design automation (EDA). In this paper, we propose a novel U-Net variant model boosted by an Inception embedded module to predict Routing Congestion (RC) and Design Rule Checking (DRC) hotspots. Experimental results on the recently published CircuitNet dataset benchmark show that our proposed method achieves up to 5% (RC) and 20% (DRC) rate reduction in terms of Avg-NRMSE (Average Normalized Root 
    
[^199]: ConSmax: 具有可学习参数的硬件友好型Softmax替代方案

    ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters

    [https://arxiv.org/abs/2402.10930](https://arxiv.org/abs/2402.10930)

    ConSmax是一种硬件友好型Softmax替代方案，通过引入可学习参数，在不影响性能的情况下实现了对原Softmax关键任务的高效处理。

    

    自注意机制将基于transformer的大型语言模型（LLM）与卷积和循环神经网络区分开来。尽管性能有所提升，但由于自注意中广泛使用Softmax，在硅上实现实时LLM推断仍具挑战性。为了解决这一挑战，我们提出了Constant Softmax（ConSmax），这是一种高效的Softmax替代方案，采用可微的规范化参数来消除Softmax中的最大搜索和分母求和，实现了大规模并行化。

    arXiv:2402.10930v1 Announce Type: cross  Abstract: The self-attention mechanism sets transformer-based large language model (LLM) apart from the convolutional and recurrent neural networks. Despite the performance improvement, achieving real-time LLM inference on silicon is challenging due to the extensively used Softmax in self-attention. Apart from the non-linearity, the low arithmetic intensity greatly reduces the processing parallelism, which becomes the bottleneck especially when dealing with a longer context. To address this challenge, we propose Constant Softmax (ConSmax), a software-hardware co-design as an efficient Softmax alternative. ConSmax employs differentiable normalization parameters to remove the maximum searching and denominator summation in Softmax. It allows for massive parallelization while performing the critical tasks of Softmax. In addition, a scalable ConSmax hardware utilizing a bitwidth-split look-up table (LUT) can produce lossless non-linear operation and 
    
[^200]: AM^2-EmoJE：通过联合嵌入学习实现对话中的自适应缺失模态情绪识别

    AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning

    [https://arxiv.org/abs/2402.10921](https://arxiv.org/abs/2402.10921)

    通过自适应缺失模态情绪识别, 该模型包括查询自适应融合和多模态联合嵌入学习两大特点，旨在提高情绪识别的准确性和鲁棒性。

    

    人类情绪可以通过不同模式表达，例如音频、视频和文本。然而，每种模式在展示情绪时的贡献并不均匀。此外，在测试时，不一定总是能够保证完整的模式特定细节可用。在这项工作中，我们提出了一种名为AM^2-EmoJE的模型，通过联合嵌入学习模型，在对话中实现自适应缺失模态情绪识别，该模型基于两方面的贡献：首先，查询自适应融合可以自动学习其模式特定表示在查询特定方式下的相对重要性。通过这种方式，模型旨在优先考虑情绪模式的模式不变空间查询细节，同时在学习的多模式查询描述符中保留其独占模式方面。其次，多模态联合嵌入学习模块明确解决了测试时的各种缺失模态场景。

    arXiv:2402.10921v1 Announce Type: new  Abstract: Human emotion can be presented in different modes i.e., audio, video, and text. However, the contribution of each mode in exhibiting each emotion is not uniform. Furthermore, the availability of complete mode-specific details may not always be guaranteed in the test time. In this work, we propose AM^2-EmoJE, a model for Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning model that is grounded on two-fold contributions: First, a query adaptive fusion that can automatically learn the relative importance of its mode-specific representations in a query-specific manner. By this the model aims to prioritize the mode-invariant spatial query details of the emotion patterns, while also retaining its mode-exclusive aspects within the learned multimodal query descriptor. Second the multimodal joint embedding learning module that explicitly addresses various missing modality scenarios in test-time. By this, th
    
[^201]: 使用LLM设计硅脑：利用ChatGPT自动生成脉冲神经元阵列的描述

    Designing Silicon Brains using LLM: Leveraging ChatGPT for Automated Description of a Spiking Neuron Array

    [https://arxiv.org/abs/2402.10920](https://arxiv.org/abs/2402.10920)

    本文展示了使用ChatGPT4为可编程脉冲神经元阵列ASIC生成Verilog描述的设计流程，验证了AI生成的设计，并展示了使用自然语言驱动硬件设计的现状。

    

    大型语言模型(LLMs)以综合正确的回复而引起关注，包括代码生成。本文展示了用于指导ChatGPT4生成可综合和功能性Verilog描述的提示，以描述可编程脉冲神经元阵列ASIC的整体。这种设计流程展示了使用ChatGPT4进行自然语言驱动硬件设计的当前状态。通过手工制作的测试台验证了AI生成的设计，并通过采用开源EDA流程的Tiny Tapeout 5提交到Skywater 130nm进行制造。

    arXiv:2402.10920v1 Announce Type: cross  Abstract: Large language models (LLMs) have made headlines for synthesizing correct-sounding responses to a variety of prompts, including code generation. In this paper, we present the prompts used to guide ChatGPT4 to produce a synthesizable and functional verilog description for the entirety of a programmable Spiking Neuron Array ASIC. This design flow showcases the current state of using ChatGPT4 for natural language driven hardware design. The AI-generated design was verified in simulation using handcrafted testbenches and has been submitted for fabrication in Skywater 130nm through Tiny Tapeout 5 using an open-source EDA flow.
    
[^202]: LLM辅助危机管理：构建用于有效应急响应和公众协作的先进LLM平台

    LLM-Assisted Crisis Management: Building Advanced LLM Platforms for Effective Emergency Response and Public Collaboration

    [https://arxiv.org/abs/2402.10908](https://arxiv.org/abs/2402.10908)

    通过LLAMA2语言模型，建立了一个能够从社交媒体和紧急消息中识别和分类紧急情况的方法，可协助在全国范围内的紧急情况下公共安全话务员和大众，提供相关指导并通知政府机构。

    

    紧急情况和重大事件往往迅速发展，需要迅速有效的响应。本研究介绍了一种新方法，利用开源大型语言模型LLAMA2，从社交媒体帖子和直接的紧急消息中识别和分类紧急情况。旨在利用自然语言处理和机器学习的力量，协助公共安全话务员和大量民众在全国范围内的紧急情况中。我们的研究集中于开发一种语言模型，能够理解用户在911呼叫中描述自己的情况，使LLAMA2能够分析内容并为话务员提供相关指导，同时创建工作流程，在必要时将呼叫者信息通知政府机构。该语言模型提供的另一个好处是，当911系统不堪重负时，它能够在重大紧急事件中协助人们。

    arXiv:2402.10908v1 Announce Type: cross  Abstract: Emergencies and critical incidents often unfold rapidly, necessitating a swift and effective response. In this research, we introduce a novel approach to identify and classify emergency situations from social media posts and direct emergency messages using an open source Large Language Model, LLAMA2. The goal is to harness the power of natural language processing and machine learning to assist public safety telecommunicators and huge crowds during countrywide emergencies. Our research focuses on developing a language model that can understand users describe their situation in the 911 call, enabling LLAMA2 to analyze the content and offer relevant instructions to the telecommunicator, while also creating workflows to notify government agencies with the caller's information when necessary. Another benefit this language model provides is its ability to assist people during a significant emergency incident when the 911 system is overwhelme
    
[^203]: Hermite神经网络模拟解决二维薛定谔方程

    Hermite Neural Network Simulation for Solving the 2D Schrodinger Equation

    [https://arxiv.org/abs/2402.10649](https://arxiv.org/abs/2402.10649)

    使用混合神经网络结合Hermite函数根作为配点，提高了解决二维薛定谔方程的效率和精度，与其他神经网络和方法相比取得了优秀的结果。

    

    arXiv: 2402.10649v1 公告类型: 交叉摘要: 薛定谔方程是描述量子力学系统中波函数行为的数学方程。它是一个偏微分方程，提供了有关量子力学基本原理的宝贵见解。本文旨在通过使用混合神经网络与基于Hermite函数的配点方法来以足够的精度解决薛定谔方程。最初，Hermite函数的根被用作配点，提高了解决方案的效率。薛定谔方程在无限域中定义，使用Hermite函数作为激活函数导致了出色的精度。最后，提出的方法使用MATLAB的Simulink工具进行了模拟。然后将结果与使用基于物理信息的神经网络和所提出的方法获得的结果进行了比较。

    arXiv:2402.10649v1 Announce Type: cross  Abstract: The Schrodinger equation is a mathematical equation describing the wave function's behavior in a quantum-mechanical system. It is a partial differential equation that provides valuable insights into the fundamental principles of quantum mechanics. In this paper, the aim was to solve the Schrodinger equation with sufficient accuracy by using a mixture of neural networks with the collocation method base Hermite functions. Initially, the Hermite functions roots were employed as collocation points, enhancing the efficiency of the solution. The Schrodinger equation is defined in an infinite domain, the use of Hermite functions as activation functions resulted in excellent precision. Finally, the proposed method was simulated using MATLAB's Simulink tool. The results were then compared with those obtained using Physics-informed neural networks and the presented method.
    
[^204]: 细微之线：通过话语主题识别机器生成的文本

    Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs

    [https://arxiv.org/abs/2402.10586](https://arxiv.org/abs/2402.10586)

    本文探讨了如何通过研究文本中的话语特征来区分人类创作和机器生成的文本，引入了一种新颖的方法来揭示这些特征，并发现人类写作在结构上更为多样化。

    

    随着大型语言模型（LLM）的出现，人类创作和机器生成的文本之间的界限变得日益模糊。本文探讨了识别人类撰写的文本中可辨识和独特的语言特性的研究，特别是揭示文本在表面结构之外的潜在话语结构。引入了一种新颖的方法论，我们利用层次化解析树和递归超图来揭示LLM和人类生成的文本中的独特话语模式。实证研究结果表明，尽管LLM和人类生成的文本都受特定领域的影响而产生不同的话语模式，但人类撰写的文本表现出更多的结构变异性，反映了不同领域人类写作的微妙性质。值得注意的是，引入层次话语特征可以增强二元分类器在区分人类生成和机器生成文本方面的整体性能。

    arXiv:2402.10586v1 Announce Type: new  Abstract: With the advent of large language models (LLM), the line between human-crafted and machine-generated texts has become increasingly blurred. This paper delves into the inquiry of identifying discernible and unique linguistic properties in texts that were written by humans, particularly uncovering the underlying discourse structures of texts beyond their surface structures. Introducing a novel methodology, we leverage hierarchical parse trees and recursive hypergraphs to unveil distinctive discourse patterns in texts produced by both LLMs and humans. Empirical findings demonstrate that, although both LLMs and humans generate distinct discourse patterns influenced by specific domains, human-written texts exhibit more structural variability, reflecting the nuanced nature of human writing in different domains. Notably, incorporating hierarchical discourse features enhances binary classifiers' overall performance in distinguishing between huma
    
[^205]: 在InSaAF中融入安全性，通过准确性和公平性 | LLM是否已经准备好进入印度法律领域？

    InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?

    [https://arxiv.org/abs/2402.10567](https://arxiv.org/abs/2402.10567)

    本研究在印度法律领域探讨了大型语言模型（LLMs）在处理社会因素时的能力，提出了结合公平性和准确性的新指标$LSS_{\beta}$，并评估了模型在二元法律推理任务中的表现以及在印度社会各种不平等方面的公平性展示。

    

    语言技术和人工智能的最新进展已经导致提出了众多语言模型，用于执行法律领域的各种任务，从预测判决到生成摘要。尽管它们具有巨大潜力，但已经证明这些模型学习并展示社会偏见，并做出不公平的预测。在这项研究中，我们探讨了当涉及社会因素时大型语言模型（LLMs）在印度法律领域执行任务的能力。我们提出了一种新颖的度量标准，$\beta$-加权的$\textit{法律安全分数($LSS_{\beta}$)}$，将LLM的公平性和准确性两个方面结合起来。我们通过考虑LLM在$\textit{二元法律推理}$任务中的表现以及其在印度社会各种不平等方面的公平展示来评估LLMs的安全性。LLaMA和LLaMA--2模型的任务表现和公平得分表明...

    arXiv:2402.10567v1 Announce Type: cross  Abstract: Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries. Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions. In this study, we explore the ability of Large Language Models (LLMs) to perform legal tasks in the Indian landscape when social factors are involved. We present a novel metric, $\beta$-weighted $\textit{Legal Safety Score ($LSS_{\beta}$)}$, which encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs' safety by considering its performance in the $\textit{Binary Statutory Reasoning}$ task and its fairness exhibition with respect to various axes of disparities in the Indian society. Task performance and fairness scores of LLaMA and LLaMA--2 models indicate th
    
[^206]: 论部署LLMs/VLMs在机器人领域存在的安全问题：突显风险和漏洞

    On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities

    [https://arxiv.org/abs/2402.10340](https://arxiv.org/abs/2402.10340)

    论文突出探讨了在机器人应用中整合大型语言模型和视觉语言模型所带来的安全性和健壮性关键问题，指出这种整合可能容易受到恶意攻击并导致严重后果。

    

    在这篇论文中，我们着重讨论了将大型语言模型（LLMs）和视觉语言模型（VLMs）整合到机器人应用中所涉及的健壮性和安全性关键问题。最近的研究着重于利用LLMs和VLMs来提高机器人任务（如操作，导航等）的性能。然而，这种整合可能会引入显着的漏洞，即由于语言模型对恶意攻击的敏感性，可能导致灾难性后果。通过研究LLMs/VLMs与机器人界面的最新进展，我们展示了如何轻松操纵或误导机器人的行为，导致安全隐患。我们定义并提供了几种可能的恶意攻击示例，并对集成了语言模型的三个知名机器人框架（包括KnowNo VIMA和Instruct2Act）进行实验，以评估它们对这些攻击的敏感度。

    arXiv:2402.10340v1 Announce Type: cross  Abstract: In this paper, we highlight the critical issues of robustness and safety associated with integrating large language models (LLMs) and vision-language models (VLMs) into robotics applications. Recent works have focused on using LLMs and VLMs to improve the performance of robotics tasks, such as manipulation, navigation, etc. However, such integration can introduce significant vulnerabilities, in terms of their susceptibility to adversarial attacks due to the language models, potentially leading to catastrophic consequences. By examining recent works at the interface of LLMs/VLMs and robotics, we show that it is easy to manipulate or misguide the robot's actions, leading to safety hazards. We define and provide examples of several plausible adversarial attacks, and conduct experiments on three prominent robot frameworks integrated with a language model, including KnowNo VIMA, and Instruct2Act, to assess their susceptibility to these atta
    
[^207]: 基于上下文的奖励：基于动态偏好调整的多目标基础模型对齐

    Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment

    [https://arxiv.org/abs/2402.10207](https://arxiv.org/abs/2402.10207)

    本文介绍了Rewards-in-Context（RiC）方法，该方法通过多个奖励条件控制基础模型的响应，并应用有监督的微调进行对齐。它具有简单性和适应性，并支持在推理时动态调整用户偏好。

    

    我们考虑了基于人类偏好的基础模型多目标对齐问题，这是实现有益和无害的人工智能系统的关键步骤。然而，使用强化学习（RL）对大型基础模型进行微调通常是昂贵且不稳定的，并且人类偏好的多维度、异质性和冲突性进一步复杂化了对齐过程。在本文中，我们引入了Rewards-in-Context（RiC）方法，它使得基础模型的响应取决于其提示上下文中的多个奖励，并应用有监督的微调来进行对齐。RiC的显著特点是简单性和适应性，因为它只需要对单个基础模型进行有监督的微调，并支持在推理时动态调整用户偏好。受到抽象的凸优化问题的解析解的启发，我们提出了一种动态推理时调整方法。

    arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
    
[^208]: 重塑RLHF中的信息结构：基于图论的奖励泛化视角

    Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective

    [https://arxiv.org/abs/2402.10184](https://arxiv.org/abs/2402.10184)

    本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。

    

    在强化学习从人类反馈中（RLHF）存在一个三难问题：高度多样的环境、低标注成本和可靠的对齐性能之间的不兼容性。本文旨在通过设计奖励建模过程中的数据集信息结构来缓解这种不兼容性。具体而言，我们重新审视了RLHF过程，并提出了一个理论框架将其描绘为文本分布上的自动编码过程。我们的框架形式化了RLHF目标，即确保人类偏好与大型语言模型（LLM）行为之间的分布一致性。基于这个框架，我们系统地研究了RLHF奖励建模阶段中信息结构的性能影响。为了进一步理解奖励建模阶段中的奖励泛化，我们引入了一种基于随机图论的方法来建模语义空间中的泛化。其中的关键见解是...

    arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
    
[^209]: 调谐：在临床设置中使用有限数据的音频分类器性能分析

    Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data

    [https://arxiv.org/abs/2402.10100](https://arxiv.org/abs/2402.10100)

    本研究评估了在临床设置中使用深度学习模型进行音频分类的效果，并发现在微调之前，预训练模型在大数据集上的性能对临床数据的影响较好。研究结果表明，CNN模型可以在小数据集环境中与转换模型相媲美或超越。

    

    本研究评估了在临床设置中使用深度学习模型进行音频分类的效果，限制条件是以反映实际世界数据收集的小数据集为基础。我们分析了包括DenseNet和ConvNeXt在内的CNN模型，以及ViT、SWIN和AST等转换模型，并将它们与诸如YAMNet和VGGish的预训练音频模型进行比较。我们的方法强调了在特定临床数据上微调之前，在大数据集上进行预训练的好处。我们从卒中患者中新收集了两个前所未有的患者音频数据集。我们研究了各种预处理技术，发现基于它们从预训练中学习到的先验知识，RGB和灰度谱图转换对模型性能产生了不同的影响。我们的研究结果表明，CNN模型在小数据集环境中可以与转换模型相媲美或超越，其中DenseNet-Contrastive和AST模型表现突出。本研究强调了...

    arXiv:2402.10100v1 Announce Type: cross  Abstract: This study assesses deep learning models for audio classification in a clinical setting with the constraint of small datasets reflecting real-world prospective data collection. We analyze CNNs, including DenseNet and ConvNeXt, alongside transformer models like ViT, SWIN, and AST, and compare them against pre-trained audio models such as YAMNet and VGGish. Our method highlights the benefits of pre-training on large datasets before fine-tuning on specific clinical data. We prospectively collected two first-of-their-kind patient audio datasets from stroke patients. We investigated various preprocessing techniques, finding that RGB and grayscale spectrogram transformations affect model performance differently based on the priors they learn from pre-training. Our findings indicate CNNs can match or exceed transformer models in small dataset contexts, with DenseNet-Contrastive and AST models showing notable performance. This study highlights
    
[^210]: 用深度学习增强金融行业的网络安全韧性，实现高级威胁检测

    Enhancing Cybersecurity Resilience in Finance with Deep Learning for Advanced Threat Detection

    [https://arxiv.org/abs/2402.09820](https://arxiv.org/abs/2402.09820)

    这项研究提出使用深度学习来增强金融行业的网络安全韧性，并实现高级威胁检测。目前的网络威胁检测方法往往基于规则和传统的机器学习方法，无法适用大规模数据应用，并且无法有效检测未知威胁。

    

    在互联网时代，人们的生活越来越依赖于今天的网络技术。然而，网络技术是一把双刃剑，给人们带来便利的同时也带来了许多安全挑战。保持网络安全和保护用户的合法利益是网络建设的核心。威胁检测是一个完整有效的防御系统的重要组成部分。在网络信息安全领域，网络攻击和网络防护的技术更新日益迅猛。如何有效地检测未知威胁是网络防护的关注焦点之一。目前，网络威胁检测通常基于规则和传统的机器学习方法，这些方法创建人工规则或提取常见的时空特征，不能应用于大规模数据应用，并且未知威胁的出现导致了系统的检测准确性降低。

    arXiv:2402.09820v1 Announce Type: cross  Abstract: In the age of the Internet, people's lives are increasingly dependent on today's network technology. However, network technology is a double-edged sword, bringing convenience to people but also posing many security challenges. Maintaining network security and protecting the legitimate interests of users is at the heart of network construction. Threat detection is an important part of a complete and effective defense system. In the field of network information security, the technical update of network attack and network protection is spiraling. How to effectively detect unknown threats is one of the concerns of network protection. Currently, network threat detection is usually based on rules and traditional machine learning methods, which create artificial rules or extract common spatiotemporal features, which cannot be applied to large-scale data applications, and the emergence of unknown threats causes the detection accuracy of the or
    
[^211]: 基于对比学习和自注意力的时间接近度上的顺序推荐

    Sequential Recommendation on Temporal Proximities with Contrastive Learning and Self-Attention

    [https://arxiv.org/abs/2402.09784](https://arxiv.org/abs/2402.09784)

    该论文基于对比学习和自注意力机制，提出了一种考虑垂直和水平时间接近度的顺序推荐方法，以更好地捕捉用户-项目交互中的时间上下文。

    

    传统的基于深度学习和最新的基于Transformer的模型在先前的研究中捕捉了用户-项目交互中的单向和双向模式，但对于时间上下文的重要性，如个体行为和社会趋势模式，仍未得到很好的探索。最近的模型通常忽略了在类似的时间段内隐含在用户之间发生的用户行为的相似性，我们将其称为垂直时间接近度。这些模型主要通过适应Transformer的自注意机制来考虑用户行为中的时间上下文。同时，这种适应在考虑项目交互中的水平时间接近度方面仍然有限，例如区分在一周内与一个月内购买的连续项目。

    arXiv:2402.09784v1 Announce Type: cross  Abstract: Sequential recommender systems identify user preferences from their past interactions to predict subsequent items optimally. Although traditional deep-learning-based models and modern transformer-based models in previous studies capture unidirectional and bidirectional patterns within user-item interactions, the importance of temporal contexts, such as individual behavioral and societal trend patterns, remains underexplored. Notably, recent models often neglect similarities in users' actions that occur implicitly among users during analogous timeframes-a concept we term vertical temporal proximity. These models primarily adapt the self-attention mechanisms of the transformer to consider the temporal context in individual user actions. Meanwhile, this adaptation still remains limited in considering the horizontal temporal proximity within item interactions, like distinguishing between subsequent item purchases within a week versus a mon
    
[^212]: 模型编辑的蝴蝶效应：少量编辑可能引发大规模语言模型崩溃

    The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse

    [https://arxiv.org/abs/2402.09656](https://arxiv.org/abs/2402.09656)

    尽管模型编辑在大型语言模型中显示出修订知识的潜力，但少量编辑可以触发模型崩溃，导致性能显著下降。我们提出使用困惑度作为替代指标，并通过实验证实其与下游任务性能的强相关性。

    

    虽然模型编辑已显示出在大型语言模型（LLMs）中修订知识的潜力，但其对LLMs的内在能力的影响常常被忽视。在这项工作中，我们揭示了一个关键现象：即使只进行一个编辑，也可以引发模型崩溃，表现为各种基准任务中性能显著下降。然而，在每次编辑后对LLMs进行基准测试虽然必要，但耗时且资源密集。为了缓解这个问题，我们提出使用困惑度作为替代指标，并通过大量实验证实其与下游任务性能的强相关性。我们还对顺序编辑进行了深入研究，这是实际场景中的一种常见情况，涵盖了来自我们之前单次编辑研究中的困难案例。结果表明，几乎所有研究的编辑方法都导致模型崩溃。

    arXiv:2402.09656v1 Announce Type: new  Abstract: Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks. However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating its strong correlation with downstream task performance. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse
    
[^213]: 使用大型语言模型在药物分子和适应症之间进行翻译的新机遇

    Emerging Opportunities of Using Large Language Language Models for Translation Between Drug Molecules and Indications

    [https://arxiv.org/abs/2402.09588](https://arxiv.org/abs/2402.09588)

    本研究探讨了使用大型语言模型在药物分子和适应症之间进行翻译的机遇，提出了一个新任务，并测试了其有效性，这对于药物发现过程具有重要意义。

    

    药物分子是一种改变生物体精神或身体状态的物质。每种批准的药物都有适应症，指的是该药物治疗特定医疗条件的治疗用途。虽然大型语言模型（LLM），一种生成式人工智能技术，最近已经证明在分子和其文本描述之间进行翻译方面是有效的，但仍存在关于在药物分子和适应症（或反之）之间进行翻译的应用的研究空白，这可能极大地有益于药物发现过程。从给定的适应症生成药物的能力将允许发现针对特定疾病或靶点的药物，并最终为患者提供更好的治疗方法。在本文中，我们首先提出了一个新任务，即药物分子和相应适应症之间的翻译，然后进行了实验测试。

    arXiv:2402.09588v1 Announce Type: new  Abstract: A drug molecule is a substance that changes the organism's mental or physical state. Every approved drug has an indication, which refers to the therapeutic use of that drug for treating a particular medical condition. While the Large Language Model (LLM), a generative Artificial Intelligence (AI) technique, has recently demonstrated effectiveness in translating between molecules and their textual descriptions, there remains a gap in research regarding their application in facilitating the translation between drug molecules and indications, or vice versa, which could greatly benefit the drug discovery process. The capability of generating a drug from a given indication would allow for the discovery of drugs targeting specific diseases or targets and ultimately provide patients with better treatments. In this paper, we first propose a new task, which is the translation between drug molecules and corresponding indications, and then test exi
    
[^214]: 打击深度伪造：应对国家安全威胁和侵犯权利的政策

    Combatting deepfakes: Policies to address national security threats and rights violations

    [https://arxiv.org/abs/2402.09581](https://arxiv.org/abs/2402.09581)

    本文提供了应对深度伪造威胁的政策建议，包括背景信息、危害、先前的立法提案以及全面的深度伪造供应链政策建议。

    

    这篇论文提供了应对深度伪造威胁的政策建议。首先，我们提供了有关深度伪造的背景信息，并回顾了它们所带来的危害。我们描述了目前深度伪造被用于传播性虐待材料、进行欺诈、操纵选民行为以及对国家安全构成威胁的情况。其次，我们回顾了先前的立法提案，旨在解决深度伪造问题。第三，我们提出了一项全面的政策建议，重点解决深度伪造供应链的多个环节。深度伪造供应链从少数模型开发者、模型提供者和计算提供者开始，扩展至数十亿潜在的深度伪造制作者。我们详细描述了这个供应链，并说明每个环节的实体都应采取合理措施，防止深度伪造的制造和传播。最后，我们讨论了可能的对策措施。

    arXiv:2402.09581v1 Announce Type: cross  Abstract: This paper provides policy recommendations to address threats from deepfakes. First, we provide background information about deepfakes and review the harms they pose. We describe how deepfakes are currently used to proliferate sexual abuse material, commit fraud, manipulate voter behavior, and pose threats to national security. Second, we review previous legislative proposals designed to address deepfakes. Third, we present a comprehensive policy proposal that focuses on addressing multiple parts of the deepfake supply chain. The deepfake supply chain begins with a small number of model developers, model providers, and compute providers, and it expands to include billions of potential deepfake creators. We describe this supply chain in greater detail and describe how entities at each step of the supply chain ought to take reasonable measures to prevent the creation and proliferation of deepfakes. Finally, we address potential counterpo
    
[^215]: 关于基于网络特征在欺诈检测中潜力的研究

    On the Potential of Network-Based Features for Fraud Detection

    [https://arxiv.org/abs/2402.09495](https://arxiv.org/abs/2402.09495)

    本文研究了基于网络特征在欺诈检测中的潜力，通过使用个性化的PageRank算法来捕捉欺诈的社会动态。实验结果表明，集成PPR可以提高模型的预测能力并提供独特有价值的信息。

    

    在线交易欺诈给企业和消费者带来了重大挑战，面临着重大的经济损失。传统的基于规则的系统难以跟上欺诈战术的演变，导致高误报率和漏报率。机器学习技术通过利用历史数据识别欺诈模式提供了一个有希望的解决方案。本文探讨使用个性化的PageRank（PPR）算法通过分析金融账户之间的关系来捕捉欺诈的社会动态。主要目标是比较传统特征与添加PPR在欺诈检测模型中的性能。结果表明，集成PPR可以提高模型的预测能力，超过基准模型。此外，PPR特征提供了独特而有价值的信息，通过其高特征重要性得分得以证明。特征稳定性分析证实了一致的结果。

    arXiv:2402.09495v1 Announce Type: cross  Abstract: Online transaction fraud presents substantial challenges to businesses and consumers, risking significant financial losses. Conventional rule-based systems struggle to keep pace with evolving fraud tactics, leading to high false positive rates and missed detections. Machine learning techniques offer a promising solution by leveraging historical data to identify fraudulent patterns. This article explores using the personalised PageRank (PPR) algorithm to capture the social dynamics of fraud by analysing relationships between financial accounts. The primary objective is to compare the performance of traditional features with the addition of PPR in fraud detection models. Results indicate that integrating PPR enhances the model's predictive power, surpassing the baseline model. Additionally, the PPR feature provides unique and valuable information, evidenced by its high feature importance score. Feature stability analysis confirms consist
    
[^216]: LlaSMol:利用大规模、全面、高质量的指令调优数据集推进化学的大规模语言模型

    LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset

    [https://arxiv.org/abs/2402.09391](https://arxiv.org/abs/2402.09391)

    本文介绍了LlaSMol，它是一种推进化学领域大规模语言模型的方法。通过使用一个大规模、全面、高质量的指令调优数据集来训练模型，LlaSMol在化学任务中表现出强大的性能，超过了GPT-4并接近于任务特定模型。

    

    化学在药物研发和材料科学等许多领域中起着至关重要的作用。尽管诸如GPT-4之类的大型语言模型（LLM）在自然语言处理任务上展现出了非凡的能力，但现有工作表明它们在化学任务上的性能令人失望。然而，在本文中，我们展示了我们开发的LLM在一系列化学任务上可以取得非常强大的结果，在所有任务上都显著优于最先进的GPT-4，并接近SoTA任务特定模型。我们取得成功的关键是一个名为SMolInstruct的大规模、全面、高质量的指令调优数据集。它包含了14个经过精心挑选的化学任务和超过三百万个高质量样本，为训练和评估化学LLM奠定了坚实基础。基于SMolInstruct，我们对一组开源LLM进行了微调，其中，我们发现Mistral ser是最佳性能的模型。

    arXiv:2402.09391v1 Announce Type: new Abstract: Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 across all the tasks by a substantial margin and approaching the SoTA task-specific models. The key to our success is a large-scale, comprehensive, high-quality dataset for instruction tuning named SMolInstruct. It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating LLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral ser
    
[^217]: 人类中的即时概括与深度神经网络中的滞后概括——表示分歧的证据？

    Immediate generalisation in humans but a generalisation lag in deep neural networks$\unicode{x2014}$evidence for representational divergence?

    [https://arxiv.org/abs/2402.09303](https://arxiv.org/abs/2402.09303)

    研究对比了人类和深度神经网络在图像分类中的行为差异，发现人类具有即时概括能力，而DNNs存在滞后概括现象，这表明了表示分歧的存在。

    

    近期的研究在图像分类领域中对比了人类与深度神经网络（DNNs）的许多行为比较。通常，比较研究关注的是学习过程的最终结果，通过测量和比较目标类别表示的相似性。然而，这些表示如何形成即其过程——即在获取过程中观察到的行为变化和中间阶段——往往少有直接和实证的比较。在这里，我们报告了对人类观察者和不同经典与最新技术的DNNs中可转移表示是如何被获取的的详细调查。我们开发了一个受限的监督学习环境，该环境中我们对齐了学习相关的参数，如起始点、输入模式、可用输入数据以及提供的反馈。在整个学习过程中我们评估...

    arXiv:2402.09303v1 Announce Type: cross Abstract: Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification. Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed. However, the process of how these representations emerge$\unicode{x2014}$that is, the behavioral changes and intermediate stages observed during the acquisition$\unicode{x2014}$is less often directly and empirically compared.   Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs. We develop a constrained supervised learning environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided. Across the whole learning process we evaluate 
    
[^218]: SAGMAN: 用于图神经网络在流形上的稳定性分析的方法

    SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds

    [https://arxiv.org/abs/2402.08653](https://arxiv.org/abs/2402.08653)

    SAGMAN是一种用于检验图神经网络稳定性的谱框架，它通过评估非线性映射中的距离失真来衡量GNN的稳定性。为了进行有意义的稳定性分析，我们提出了一种距离保持的图降维方法。

    

    现代图神经网络（GNN）对输入图结构和节点特征的变化敏感，可能导致不可预测的行为和性能下降。本文引入了一种称为SAGMAN的谱框架，用于检验GNN的稳定性。该框架评估非线性映射中GNN在输入和输出流形之间引起的距离失真: 当输入流行中两个附近的节点（通过GNN模型）被映射到输出流行上的两个远离的节点时，意味着存在较大的距离失真，从而导致GNN的稳定性较差。我们提出了一种距离保持的图降维（GDR）方法，利用谱图嵌入和概率图模型（PGMs）来创建低维的输入/输出基于图的流形，以进行有意义的稳定性分析。我们的实证评估表明，SAGMAN能够有效评估每个节点在面对不同边缘或特征扰动时的稳定性。

    Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability. We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature pe
    
[^219]: 在黑盒大型语言模型上进行知识编辑

    Knowledge Editing on Black-box Large Language Models

    [https://arxiv.org/abs/2402.08631](https://arxiv.org/abs/2402.08631)

    这项研究提出了在黑盒大型语言模型上进行知识编辑的方法，并引入了一种多角度评估框架和一种新的postEdit框架，以解决现有方法中的隐私和风格问题。

    

    知识编辑旨在高效、精确地修改大型语言模型的行为，以更新特定的知识，而不对其他知识产生负面影响。当前的研究主要集中在白盒语言模型编辑上，忽视了一个重要的场景：黑盒语言模型编辑，即通过接口访问语言模型，并仅可用文本输出。为了解决现有评估在黑盒语言模型编辑上不适用且缺乏全面性的局限性，我们提出了一种多角度评估框架，首次将风格保留的评估纳入其中。为了解决当前方法中的编辑数据隐私泄漏和风格过度编辑的问题，我们引入了一种新的postEdit框架，通过下游后处理解决隐私问题，并通过对原始回答进行细粒度编辑来保持文本风格一致性。在两个基准测试上的实验与分析表明，postEdit的性能超过了所有现有方法。

    Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge. Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available. To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time. To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses. Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all 
    
[^220]: 揭示了在重复的蛋糕切割中英勇竞争的艺术

    Dueling Over Dessert, Mastering the Art of Repeated Cake Cutting

    [https://arxiv.org/abs/2402.08547](https://arxiv.org/abs/2402.08547)

    这篇论文研究了重复公平分割问题中的竞争策略，发现如果Bob过于偏好某一块蛋糕，Alice利用类似于二分查找的策略可以系统性地对Bob实施剥削，从而在时间上获得更多资源份额。

    

    我们考虑了两个玩家，分别代表Alice和Bob，通过对蛋糕的个人估值进行了重复的公平分割。在每一轮中，会出现一块与之前轮次相同的新蛋糕。Alice在自己选择的一个点上切割蛋糕，而Bob选择左边的部分或右边的部分，把剩余的给Alice。我们考虑了两个变种：顺序模式，Bob在选择左/右之前观察Alice的切割点；同时模式，他只在做出选择后观察她的切割点。同时模式是由Aumann和Maschler（1995）首次提出的。我们发现，如果Bob几乎是目光短浅的，并且经常选择自己喜欢的部分，那么他可以被Alice通过类似于二分查找的策略系统性地利用。这个策略使得Alice可以越来越精确地模拟Bob的偏好，从而在时间上获得不成比例的资源份额。我们分析了一个玩家能够利用其优势的极限。

    We consider the setting of repeated fair division between two players, denoted Alice and Bob, with private valuations over a cake. In each round, a new cake arrives, which is identical to the ones in previous rounds. Alice cuts the cake at a point of her choice, while Bob chooses the left piece or the right piece, leaving the remainder for Alice. We consider two versions: sequential, where Bob observes Alice's cut point before choosing left/right, and simultaneous, where he only observes her cut point after making his choice. The simultaneous version was first considered by Aumann and Maschler (1995).   We observe that if Bob is almost myopic and chooses his favorite piece too often, then he can be systematically exploited by Alice through a strategy akin to a binary search. This strategy allows Alice to approximate Bob's preferences with increasing precision, thereby securing a disproportionate share of the resource over time.   We analyze the limits of how much a player can exploit t
    
[^221]: ChatCell: 利用自然语言促进单细胞分析

    ChatCell: Facilitating Single-Cell Analysis with Natural Language

    [https://arxiv.org/abs/2402.08303](https://arxiv.org/abs/2402.08303)

    ChatCell是一个利用自然语言促进单细胞分析的工具，通过词汇适应和统一序列生成，它具备深厚的专业知识和适应各种分析任务的能力。

    

    随着大型语言模型(LLMs)的快速发展，它们在科学中的影响日益突出。LLMs在任务泛化和自由对话方面的新兴能力可以极大地推进化学和生物学等领域。然而，单细胞生物学这个构成生物体基础构件的领域仍面临一些挑战。当前方法在知识门槛和可扩展性方面存在限制，阻碍了LLMs在掌握单细胞数据方面的充分利用，影响了直接可访问和快速迭代的能力。为此，我们引入了ChatCell，通过利用词汇适应和统一序列生成，它在单细胞生物学领域获得了深厚的专业知识和适应各种分析任务的能力，标志着一种范式转变。

    As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
    
[^222]: LLaGA: 大型语言和图形助手

    LLaGA: Large Language and Graph Assistant

    [https://arxiv.org/abs/2402.08170](https://arxiv.org/abs/2402.08170)

    LLaGA是一个创新的模型，它有效地整合了大型语言模型（LLM）的能力，以处理图结构数据的复杂性。通过重新组织图节点以作为结构感知的序列，并通过一个多功能投影仪将其映射到标记嵌入空间中，LLaGA在多样性、泛化性和可解释性方面表现出色。

    

    图神经网络（GNN）已经推动了图结构数据分析的进步。最近，大型语言模型（LLM）如GPT-4的崛起预示着深度学习的一个新时代。然而，将它们应用于图数据还面临着独特的挑战，由于将图结构转化为文本的固有难度。为此，我们引入了一个创新模型——大型语言和图形助手（LLaGA），它有效地整合了LLM的能力，以处理图结构数据的复杂性。LLaGA保留了LLM的通用性，同时将图数据转化为与LLM输入兼容的格式。LLaGA通过重新组织图节点以作为结构感知的序列，然后通过一个多功能投影仪将其映射到标记嵌入空间中。LLaGA在多样性、泛化性和可解释性方面表现出色，使其能够在不同数据集和任务上表现出一致的良好性能。

    Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{G}raph \textbf{A}ssistant (\textbf{LLaGA}), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and 
    
[^223]: UFO: 一个专注于Windows操作系统交互的用户界面智能体

    UFO: A UI-Focused Agent for Windows OS Interaction

    [https://arxiv.org/abs/2402.07939](https://arxiv.org/abs/2402.07939)

    UFO是一个专注于Windows操作系统上应用程序的用户界面智能体，利用GPT-Vision的能力来满足用户需求。它通过观察和分析Windows应用程序的图形用户界面和控制信息，实现无缝导航和操作以满足用户的请求。UFO的控制交互模块使得无需人工干预即可实现动作连接和完全自动化执行，使繁琐和耗时的过程变为简单任务。经过测试，UFO在各种场景中取得了良好效果。

    

    我们介绍了UFO，一个创新的专注于Windows操作系统上应用程序的用户界面智能体，利用了GPT-Vision的能力来满足用户需求。UFO采用双智能体框架，精确观察和分析Windows应用程序的图形用户界面（GUI）和控制信息。这使得智能体可以无缝地在单个应用程序内以及跨应用程序进行导航和操作，以满足用户的需求，即使涉及多个应用程序。该框架包括一个控制交互模块，实现无需人工干预的动作连接，并实现完全自动化执行。因此，UFO将艰巨而耗时的过程转变为仅通过自然语言命令就可以完成的简单任务。我们在9个流行的Windows应用程序上对UFO进行了测试，涵盖了反映用户日常使用情景的各种情况。通过定量指标和真实案例研究得出的结果强调了UFO的效果。

    We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore t
    
[^224]: 只有曲线形状有关：通过下一个曲线形状预测训练基础模型进行零样本多元时间序列预测

    Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction

    [https://arxiv.org/abs/2402.07570](https://arxiv.org/abs/2402.07570)

    通过下一个曲线形状预测，我们提出了基于编码器的零样本多元时间序列预测模型GTT，通过预训练和通道级别的曲线形状预测，展现出优秀的预测能力，甚至超过了最先进的有监督模型。

    

    我们提出了General Time Transformer (GTT)，一种仅有编码器的基础模型，用于零样本多元时间序列预测。GTT在一个包含2亿个高质量时间序列样本的大型数据集上进行预训练，涵盖了不同领域。在我们提出的框架中，多元时间序列预测的任务被建模为一个逐通道的下一个曲线形状预测问题，其中每个时间序列样本表示为一系列非重叠的曲线形状，具有统一的数值大小。GTT在通道级别上通过预测过去曲线形状的窗口来预测下一个曲线形状。实验结果表明，GTT在未见时间序列数据集上展现出优秀的零样本多元预测能力，甚至超过了最先进的有监督基线模型。此外，我们还研究了GTT模型参数和训练数据集规模变化的影响，观察到在零样本多元预测的背景下，规模定律也成立。

    We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting. GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains. In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude. GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner. Experimental results demonstrate that GTT exhibits superior zero-shot multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art supervised baselines. Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the scaling law also holds in the context of zero-shot multivariate
    
[^225]: 动态图信息瓶颈

    Dynamic Graph Information Bottleneck

    [https://arxiv.org/abs/2402.06716](https://arxiv.org/abs/2402.06716)

    动态图信息瓶颈框架（DGIB）能够学习鲁棒且有区分性的动态图表示。利用信息瓶颈原理，通过迭代引导和改进图快照传递的结构和特征信息流，压缩冗余信息并保留有价值的信息。该框架能满足最小-全局-一致条件，提高了动态图神经网络的鲁棒性。

    

    动态图广泛存在于现实世界中，它们携带着复杂的时空特征模式，对于它们的表示学习提出了挑战。动态图神经网络（DGNNs）通过利用内在的动态性展示了令人印象深刻的预测能力。然而，DGNNs展示了有限的鲁棒性，易受对抗攻击。本文提出了一种新颖的动态图信息瓶颈（DGIB）框架来学习鲁棒且有区分性的表示。借助信息瓶颈（IB）原理，我们首先提出期望的最优表示应满足最小-全局-一致（MSC）条件。为了在潜在表示中压缩冗余信息和保留有价值的信息，DGIB迭代地引导和改进通过图快照传递的结构和特征信息流。为了满足MSC条件，我们将整体IB目标分解为DGIB$_{MS}$和DGIB$_C$，其中DGIB$_{MS}$通道的目标是...

    Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the DGIB$_{MS}$ channel aims 
    
[^226]: 内省规划：引导语言驱动的代理机器人改进自身的不确定性

    Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty

    [https://arxiv.org/abs/2402.06529](https://arxiv.org/abs/2402.06529)

    本文研究了内省规划的概念，作为一种引导语言驱动的代理机器人改进自身不确定性的系统方法。通过识别任务不确定性并主动寻求澄清，内省显著提高了机器人任务规划的成功率和安全性。

    

    大型语言模型（LLM）展示了先进的推理能力，使得机器人能够理解自然语言指令，并通过适当的基础塑造来策略性地进行高级行动规划。然而，LLM产生的幻觉可能导致机器人自信地执行与用户目标不符或在极端情况下不安全的计划。此外，自然语言指令中的固有歧义可能引发任务的不确定性，尤其是在存在多个有效选项的情况下。为了解决这个问题，LLMs必须识别此类不确定性并主动寻求澄清。本文探索了内省规划的概念，作为一种系统方法，引导LLMs在无需微调的情况下形成意识到不确定性的机器人任务执行计划。我们研究了任务级机器人规划中的不确定性量化，并证明与最先进的基于LLM的规划方法相比，内省显著提高了成功率和安全性。

    Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches.
    
[^227]: 关于针对键值约束生成语言模型推理的驱逐策略的有效性研究

    On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference

    [https://arxiv.org/abs/2402.06262](https://arxiv.org/abs/2402.06262)

    本文研究了针对键值约束生成语言模型推理的驱逐策略的有效性，通过引入基于时间注意力得分和鲁棒性度量的RoCo策略，优于现有的策略。

    

    尽管大型语言模型（LLMs）在最近取得了成功，但由于它们对内存和计算资源的过度需求，它们在资源受限环境中部署仍然昂贵。除了模型参数外，键值缓存也存储在GPU内存中，随着批处理大小和序列长度的增加而线性增长。为此，最近的研究提出了各种针对给定预算下维护键值缓存开销的驱逐策略。本文着眼于现有驱逐策略在重要性评分计算和驱逐范围构建两个方面的效果。我们确定了先前策略在这两个方面的不足，并引入了基于时间注意力得分和鲁棒性度量的RoCo，一种强大的缓存驱逐策略。涵盖了预填充和自回归解码阶段的广泛实验验证了RoCo的优越性。最后，我们公开发布了RoCo的代码和模型供研究者使用。

    Despite the recent success associated with Large Language Models~(LLMs), they are notably cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands. In addition to model parameters, the key-value cache is also stored in GPU memory, growing linearly with batch size and sequence length. As a remedy, recent works have proposed various eviction policies for maintaining the overhead of key-value cache under a given budget. This paper embarks on the efficacy of existing eviction policies in terms of \textit{importance score calculation} and \textit{eviction scope construction}. We identify the deficiency of prior policies in these two aspects and introduce RoCo, a \underline{r}\underline{o}bust \underline{c}ache \underline{o}mission policy based on temporal attention scores and robustness measures. Extensive experimentation spanning prefilling and auto-regressive decoding stages validates the superiority of RoCo. Finally, we relea
    
[^228]: 用自然语言和概率推理进行实验与修订规则

    Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning

    [https://arxiv.org/abs/2402.06025](https://arxiv.org/abs/2402.06025)

    本论文建立了一个计算模型来模拟人们通过实验主动推断隐藏规则的过程，并发现显式假设、概率规则和在线更新的组合可以解释人们在类似Zendo任务上的表现。

    

    我们建立了一个计算模型，模拟人们通过实验主动推断隐藏规则的过程。该模型的基本原理是，即使规则是确定性的，学习者也会考虑更广泛的模糊概率规则，并用自然语言表示，根据近似贝叶斯原则在每次实验后在线更新自己的假设。在同一框架下，我们还根据信息论准则建立了实验设计模型。我们发现，这三个原则的组合——显式假设、概率规则和在线更新——可以解释人们在类似Zendo任务上的表现，而去掉其中任何一个组件都使得模型无法解释数据。

    We build a computational model of how humans actively infer hidden rules by doing experiments. The basic principles behind the model is that, even if the rule is deterministic, the learner considers a broader space of fuzzy probabilistic rules, which it represents in natural language, and updates its hypotheses online after each experiment according to approximately Bayesian principles. In the same framework we also model experiment design according to information-theoretic criteria. We find that the combination of these three principles -- explicit hypotheses, probabilistic rules, and online updates -- can explain human performance on a Zendo-style task, and that removing any of these components leaves the model unable to account for the data.
    
[^229]: 探索针对设备上模型的白盒攻击

    Investigating White-Box Attacks for On-Device Models

    [https://arxiv.org/abs/2402.05493](https://arxiv.org/abs/2402.05493)

    本研究探究了针对设备上模型的白盒攻击，提出了一种逆向工程框架(REOM)以将编译后的设备上TFLite模型转换为可调试模型。

    

    许多移动应用程序利用了深度学习的能力。然而，设备上的模型容易受到攻击，因为它们可以从相应的移动应用程序中轻易提取出来。现有的设备上攻击方法只能生成黑盒攻击，这种方法远不如白盒策略有效和高效。这是因为移动深度学习框架如TFLite不支持梯度计算，而梯度计算对于白盒攻击算法是必要的。因此，我们认为现有的发现可能低估了设备上攻击的危害性。为了回答这个研究问题，我们进行了一项研究：设备上的模型是否可以通过白盒策略直接受到攻击？我们首先系统地分析了将设备上模型转换为可调试版本的困难，并提出了一种针对设备上模型的逆向工程框架(REOM)，该框架可以自动将编译后的设备上TFLite模型逆向为可调试模型。具体来说，REOM

    Numerous mobile apps have leveraged deep learning capabilities. However, on-device models are vulnerable to attacks as they can be easily extracted from their corresponding mobile apps. Existing on-device attacking approaches only generate black-box attacks, which are far less effective and efficient than white-box strategies. This is because mobile deep learning frameworks like TFLite do not support gradient computing, which is necessary for white-box attacking algorithms. Thus, we argue that existing findings may underestimate the harmfulness of on-device attacks. To this end, we conduct a study to answer this research question: Can on-device models be directly attacked via white-box strategies? We first systematically analyze the difficulties of transforming the on-device model to its debuggable version, and propose a Reverse Engineering framework for On-device Models (REOM), which automatically reverses the compiled on-device TFLite model to the debuggable model. Specifically, REOM
    
[^230]: TASER: 时间自适应采样的快速准确动态图表示学习

    TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning

    [https://arxiv.org/abs/2402.05396](https://arxiv.org/abs/2402.05396)

    该论文提出了TASER方法，它是针对动态图表示学习的时间自适应采样技术，在准确性、效率和可扩展性方面进行了优化，解决了现实世界动态图中存在的噪声问题。

    

    最近，时间图神经网络（TGNN）在包括欺诈检测和内容推荐在内的各种重要应用中展示出了最先进的性能。尽管TGNN取得了成功，但它们容易受到现实世界动态图中普遍存在的噪声的影响，例如时间过时的链接和偏斜的交互分布。这些噪声导致两个关键问题，严重损害了TGNN的准确性：（1）模型受到较差交互的监督，（2）噪声输入导致聚合消息的高方差。然而，目前的TGNN去噪技术并未考虑每个节点的多样化和动态的噪声模式。此外，它们还面临着遍历更多邻居导致产生过多小批量的开销。我们相信快速准确的TGNN的解决方法在于时间自适应采样。在这项工作中，我们提出了TASER，这是第一个针对准确性、效率和可扩展性进行优化的TGNN自适应采样方法。

    Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated state-of-the-art performance in various high-impact applications, including fraud detection and content recommendation. Despite the success of TGNNs, they are prone to the prevalent noise found in real-world dynamic graphs like time-deprecated links and skewed interaction distribution. The noise causes two critical issues that significantly compromise the accuracy of TGNNs: (1) models are supervised by inferior interactions, and (2) noisy input induces high variance in the aggregated messages. However, current TGNN denoising techniques do not consider the diverse and dynamic noise pattern of each node. In addition, they also suffer from the excessive mini-batch generation overheads caused by traversing more neighbors. We believe the remedy for fast and accurate TGNNs lies in temporal adaptive sampling. In this work, we propose TASER, the first adaptive sampling method for TGNNs optimized for accuracy, efficiency, and sc
    
[^231]: 研究指令调整的局限性

    A Closer Look at the Limitations of Instruction Tuning

    [https://arxiv.org/abs/2402.05119](https://arxiv.org/abs/2402.05119)

    本文通过实验和分析揭示了指令调整的多个局限性，包括无法增强LLM的知识和技能、从具有知识来源的数据集复制回应模式导致质量下降、全参数微调增加了错误生成的情况。

    

    指令调整（IT）是使用指令-回应对来训练大型语言模型（LLM）的过程，已成为将基础预训练LLM转化为开放领域对话代理的主要方法。虽然IT取得了显著的成功并广泛应用，但其局限性和不足仍未得到充分探讨。本文通过严格的实验和对LLM通过IT发生的变化的深入分析，揭示了IT的多种局限性。特别是，我们发现：（1）IT无法增强LLM的知识或技能。LoRA微调仅限于学习回应的启动和样式令牌，而全参数微调会导致知识退化。（2）从具有知识来源的IT数据集复制回应模式会导致回应质量下降。（3）全参数微调通过不准确地从IT数据集中获取概念上相似实例的标记，增加了错误生成的情况。

    Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating respon
    
[^232]: ScreenAI: 用于UI和信息图表理解的视觉-语言模型

    ScreenAI: A Vision-Language Model for UI and Infographics Understanding

    [https://arxiv.org/abs/2402.04615](https://arxiv.org/abs/2402.04615)

    ScreenAI是一个专注于UI和信息图表理解的视觉-语言模型，通过灵活的修补策略和独特的数据集训练，以及针对UI元素的屏幕注解任务的处理，实现了在多个任务上的新的最优结果。

    

    屏幕用户界面（UI）和信息图表在人类沟通和人机交互中起着重要作用，并且共享相似的视觉语言和设计原则。我们介绍了ScreenAI，这是一个专门用于UI和信息图表理解的视觉-语言模型。我们的模型改进了PaLI架构，采用了pix2struct的灵活修补策略，并经过独特的数据集训练。在这个数据集的核心是一项新颖的屏幕注解任务，模型必须识别UI元素的类型和位置。我们使用这些文本注解来描述屏幕，并使用大规模的语言模型自动生成问答（QA），UI导航和摘要训练数据集。我们进行了消融研究以展示这些设计选择的影响。在仅有5B参数的情况下，ScreenAI在基于UI和信息图表的任务（多页文档VQA，WebSRC，MoTIF和Widget字幕）上取得了最新的最优结果，并且达到了最好的效果。

    Screen user interfaces (UIs) and infographics, sharing similar visual language and design principles, play important roles in human communication and human-machine interaction. We introduce ScreenAI, a vision-language model that specializes in UI and infographics understanding. Our model improves upon the PaLI architecture with the flexible patching strategy of pix2struct and is trained on a unique mixture of datasets. At the heart of this mixture is a novel screen annotation task in which the model has to identify the type and location of UI elements. We use these text annotations to describe screens to Large Language Models and automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. We run ablation studies to demonstrate the impact of these design choices. At only 5B parameters, ScreenAI achieves new state-of-the-artresults on UI- and infographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget Captioning), and new best-in
    
[^233]: 通过IDE派生的静态上下文的本地集成增强LLM-Based编码工具

    Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context

    [https://arxiv.org/abs/2402.03630](https://arxiv.org/abs/2402.03630)

    通过IDE的本地集成，我们提出了IDECoder框架，利用IDE提供的准确和实时的跨文件信息来增强LLM-Based编码工具，解决了挑战性的跨文件上下文问题。

    

    大型语言模型(LLM)在代码完成方面取得了显著成就，如Copilot等代码助手服务的重要作用所证明。目前的LLM通过对文件上下文的训练，在单个源文件的代码完成方面非常有效。然而，对于需要跨文件信息的大型软件项目的存储库级代码完成，对它们来说是具有挑战性的。现有的基于LLM的存储库级代码完成研究识别和整合跨文件上下文，但因LLM的精度低和上下文长度有限而受到限制。本文认为集成开发环境(IDE)可以为存储库级代码完成提供直接、准确和实时的跨文件信息。我们提出了IDECoder，一个实际的框架，利用IDE的本地静态上下文进行跨上下文构建和自我完善的诊断结果。IDECoder利用丰富的跨上下文信息来...

    Large Language Models (LLMs) have achieved remarkable success in code completion, as evidenced by their essential roles in developing code assistant services such as Copilot. Being trained on in-file contexts, current LLMs are quite effective in completing code for single source files. However, it is challenging for them to conduct repository-level code completion for large software projects that require cross-file information. Existing research on LLM-based repository-level code completion identifies and integrates cross-file contexts, but it suffers from low accuracy and limited context length of LLMs. In this paper, we argue that Integrated Development Environments (IDEs) can provide direct, accurate and real-time cross-file information for repository-level code completion. We propose IDECoder, a practical framework that leverages IDE native static contexts for cross-context construction and diagnosis results for self-refinement. IDECoder utilizes the rich cross-context information 
    
[^234]: 图缩减的综合调研：稀疏化、粗化和浓缩

    A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation

    [https://arxiv.org/abs/2402.03358](https://arxiv.org/abs/2402.03358)

    这篇综述调研了图缩减方法，包括稀疏化、粗化和浓缩，在解决大型图形数据分析和计算复杂性方面起到了重要作用。调研对这些方法的技术细节进行了系统的回顾，并强调了它们在实际应用中的关键性。同时，调研还提出了保证图缩减技术持续有效性的关键研究方向。

    

    许多真实世界的数据集可以自然地表示为图，涵盖了广泛的领域。然而，图数据集的复杂性和规模的增加为分析和计算带来了显著的挑战。为此，图缩减技术在保留关键属性的同时简化大型图形数据变得越来越受关注。在本调研中，我们旨在提供对图缩减方法的全面理解，包括图稀疏化、图粗化和图浓缩。具体而言，我们建立了这些方法的统一定义，并引入了一个分层分类法来分类这些方法所解决的挑战。我们的调研系统地回顾了这些方法的技术细节，并强调了它们在各种场景中的实际应用。此外，我们还概述了保证图缩减技术持续有效性的关键研究方向，并提供了一个详细的论文列表链接。

    Many real-world datasets can be naturally represented as graphs, spanning a wide range of domains. However, the increasing complexity and size of graph datasets present significant challenges for analysis and computation. In response, graph reduction techniques have gained prominence for simplifying large graphs while preserving essential properties. In this survey, we aim to provide a comprehensive understanding of graph reduction methods, including graph sparsification, graph coarsening, and graph condensation. Specifically, we establish a unified definition for these methods and introduce a hierarchical taxonomy to categorize the challenges they address. Our survey then systematically reviews the technical details of these methods and emphasizes their practical applications across diverse scenarios. Furthermore, we outline critical research directions to ensure the continued effectiveness of graph reduction techniques, as well as provide a comprehensive paper list at https://github.
    
[^235]: TopoX: 一个用于拓扑域上的机器学习的Python软件包套件

    TopoX: A Suite of Python Packages for Machine Learning on Topological Domains

    [https://arxiv.org/abs/2402.02441](https://arxiv.org/abs/2402.02441)

    TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。

    

    我们介绍了topox，一个提供可靠且用户友好的Python软件包套件，用于在拓扑域（扩展了图的领域）上进行计算和机器学习：超图、单纯、胞腔、路径和组合复合体。topox由三个软件包组成：toponetx用于构建和计算这些域，包括节点、边和高阶单元的处理；topoembedx提供了将拓扑域嵌入到向量空间的方法，类似于流行的基于图的嵌入算法，如node2vec；topomodelx建立在PyTorch之上，为拓扑域上的神经网络提供了一套全面的高阶消息传递功能工具箱。topox的源代码经过广泛的文档化和单元测试，并在https://github.com/pyt-team以MIT许可证的形式提供。

    We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
    
[^236]: 开发并测试一种新的基于大型语言模型的临床决策支持系统，用于药物安全的12种临床专业

    Development and Testing of a Novel Large Language Model-Based Clinical Decision Support Systems for Medication Safety in 12 Clinical Specialties

    [https://arxiv.org/abs/2402.01741](https://arxiv.org/abs/2402.01741)

    本研究开发并测试了一种新的基于大型语言模型的临床决策支持系统，用于在12个临床专业中提供安全的药物处方。该系统通过定制的处方错误警报解决了传统基于规则的系统的局限性，并评估了其在识别药物错误方面的有效性和临床医生的偏好。

    

    重要性：我们介绍了一种新颖的基于检索增强生成（RAG）-大型语言模型（LLM）的临床决策支持系统（CDSS），用于安全用药处方。该模型通过提供与患者背景和机构指南相关的处方错误警报，解决了传统基于规则的CDSS的局限性。目标：本研究评估了基于LLM的CDSS在识别各种医学和外科病例中的药物错误方面的有效性，与人工专家小组进行比较。它还研究了临床医生在不同CDSS集成方式（初级药师、仅基于LLM的CDSS和二者的组合）中的偏好。设计、设置和参与者：利用带有GPT-4.0的RAG模型，本研究涉及12个专业中23个临床案例的61个处方错误场景。专家小组使用PCNE分类和NCC MERP指数评估这些案例。三名初级药师独立审核每个场景，并提出处理建议。根据检查的错误和建议编制了反馈报告。 然后，三名医生独立审核这些报告，并提出对下一步处理的意见。

    Importance: We introduce a novel Retrieval Augmented Generation (RAG)-Large Language Model (LLM) as a Clinical Decision Support System (CDSS) for safe medication prescription. This model addresses the limitations of traditional rule-based CDSS by providing relevant prescribing error alerts tailored to patient context and institutional guidelines.   Objective: The study evaluates the efficacy of an LLM-based CDSS in identifying medication errors across various medical and surgical case vignettes, compared to a human expert panel. It also examines clinician preferences among different CDSS integration modalities: junior pharmacist, LLM-based CDSS alone, and a combination of both.   Design, Setting, and Participants: Utilizing a RAG model with GPT-4.0, the study involved 61 prescribing error scenarios within 23 clinical vignettes across 12 specialties. An expert panel assessed these cases using the PCNE classification and NCC MERP index. Three junior pharmacists independently reviewed eac
    
[^237]: CapHuman: 在平行宇宙中捕捉你的瞬间

    CapHuman: Capture Your Moments in Parallel Universes

    [https://arxiv.org/abs/2402.00627](https://arxiv.org/abs/2402.00627)

    CapHuman是一个新框架，通过“编码然后学习对齐”的范式实现了可泛化的身份保留能力，用于在不同情境中生成具有多样的头部位置、姿势和面部表情的特定个体图像。

    

    我们关注一种新颖的以人为中心的图像合成任务，即仅给定一个参考面部照片，期望能够在不同情境中生成具有多样的头部位置、姿势和面部表情的特定个体图像。为了实现这一目标，我们认为我们的生成模型应具备以下有利特征：（1）对世界和人类社会有强大的视觉和语义理解，用于基本物体和人类图像的生成；（2）可泛化的身份保留能力；（3）灵活细粒度的头部控制。最近，大型预训练的文本到图像扩散模型展现了显著的结果，成为一个强大的生成基础。基于此，我们旨在释放预训练模型的上述两种能力。在这项工作中，我们提出了一个名为CapHuman的新框架。我们采用了“编码然后学习对齐”的范式，为新个体实现了可泛化的身份保留能力。

    We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ``encode then learn to align" paradigm, which enables generalizable identity preservation for new individuals without cum
    
[^238]: 具有内聚子图意识的图对比学习

    Graph Contrastive Learning with Cohesive Subgraph Awareness

    [https://arxiv.org/abs/2401.17580](https://arxiv.org/abs/2401.17580)

    本研究提出了一种名为CTAug的新框架，将内聚子图意识无缝整合到图对比学习中。通过改进图拓扑增强和图学习过程，提高了对各种图的表征学习性能。

    

    图对比学习（GCL）已成为学习各种图表征的先进策略，包括社交和生物医学网络。GCL广泛使用随机图拓扑增强，如均匀节点丢失，生成增强图。然而，这种随机增强可能严重损害图的内在属性并恶化后续的表征学习过程。我们认为，在图增强和学习过程中引入内聚子图意识有可能提高GCL的性能。为此，我们提出了一种称为CTAug的新颖统一框架，以无缝地将内聚意识整合到各种现有的GCL机制中。具体来说，CTAug包括两个专门的模块：拓扑增强增强和图学习增强。前者生成谨慎保留内聚性质的增强图，而后者增强了图的学习能力。

    Graph contrastive learning (GCL) has emerged as a state-of-the-art strategy for learning representations of diverse graphs including social and biomedical networks. GCL widely uses stochastic graph topology augmentation, such as uniform node dropping, to generate augmented graphs. However, such stochastic augmentations may severely damage the intrinsic properties of a graph and deteriorate the following representation learning process. We argue that incorporating an awareness of cohesive subgraphs during the graph augmentation and learning processes has the potential to enhance GCL performance. To this end, we propose a novel unified framework called CTAug, to seamlessly integrate cohesion awareness into various existing GCL mechanisms. In particular, CTAug comprises two specialized modules: topology augmentation enhancement and graph learning enhancement. The former module generates augmented graphs that carefully preserve cohesion properties, while the latter module bolsters the grap
    
[^239]: 松饼还是吉娃娃？用多面板VQA挑战大型视觉语言模型

    Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA

    [https://arxiv.org/abs/2401.15847](https://arxiv.org/abs/2401.15847)

    引入了多面板视觉问答（MultipanelVQA）基准挑战大型视觉语言模型（LVLMs）对理解多面板图像的能力，并发现LVLMs在这方面仍然存在显著挑战。

    

    多面板图像，通常在网页截图、海报等中看到，充斥着我们的日常生活。这些图像以多个子图以不同布局组成，有效地向人们传达信息。为了构建高级的多模态人工智能应用，如能理解复杂场景并在网页中导航的代理程序，多面板视觉推理的技能是至关重要的，对模型在这方面进行全面评估是很重要的。因此，我们引入了多面板视觉问答（MultipanelVQA），这是一个新颖的基准，包括6,600个问题、答案和多面板图像三元组，专门挑战模型理解多面板图像。我们的评估表明，MultipanelVQA基准中的问题对测试的最先进的大型视觉语言模型（LVLMs）提出了重大挑战，即使人类可以获得约99%的准确率。

    arXiv:2401.15847v2 Announce Type: replace-cross  Abstract: Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Large Vision Language Models (LVLMs) tested, even though humans can attain approximately 99\% accurac
    
[^240]: 跨越进化算法和强化学习：一项全面调查

    Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey

    [https://arxiv.org/abs/2401.11963](https://arxiv.org/abs/2401.11963)

    通过整合进化算法与强化学习，进化强化学习（ERL）展现出卓越的性能提升，本综述呈现了ERL领域的各个研究分支，突出了EA辅助RL的优化、RL辅助EA的优化以及EA和RL的协同优化这三个主要研究方向。

    

    进化强化学习（ERL）将进化算法（EAs）和强化学习（RL）相结合进行优化，表现出卓越的性能提升。通过融合两种方法的优势，ERL已经成为一个有前景的研究方向。本调查综述了ERL中不同研究分支的全面概述。具体而言，我们系统总结了相关算法的最新进展，并确定了三个主要研究方向：EA辅助RL的优化，RL辅助EA的优化，以及EA和RL的协同优化。随后，我们深入分析了每个研究方向，组织了多个研究分支。我们阐明了每个分支致力于解决的问题，以及EA和RL的整合如何应对这些挑战。最后，我们讨论了潜在的挑战和未来的研究方向。

    arXiv:2401.11963v2 Announce Type: replace-cross  Abstract: Evolutionary Reinforcement Learning (ERL), which integrates Evolutionary Algorithms (EAs) and Reinforcement Learning (RL) for optimization, has demonstrated remarkable performance advancements. By fusing the strengths of both approaches, ERL has emerged as a promising research direction. This survey offers a comprehensive overview of the diverse research branches in ERL. Specifically, we systematically summarize recent advancements in relevant algorithms and identify three primary research directions: EA-assisted optimization of RL, RL-assisted optimization of EA, and synergistic optimization of EA and RL. Following that, we conduct an in-depth analysis of each research direction, organizing multiple research branches. We elucidate the problems that each branch aims to tackle and how the integration of EA and RL addresses these challenges. In conclusion, we discuss potential challenges and prospective future research directions
    
[^241]: 如何合并生成和检索上下文以增强开放领域问答的语言模型的研究

    Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?

    [https://arxiv.org/abs/2401.11911](https://arxiv.org/abs/2401.11911)

    该论文研究了大型语言模型如何合并生成和检索的上下文以提升开放领域问答，发现这些模型偏向于生成的上下文，即使它们提供了错误的信息。

    

    虽然辅助信息已经成为增强大型语言模型（LLMs）的关键，但对于LLMs如何合并生成的和检索的上下文仍知之甚少。为了研究这一点，我们制定了一个系统性的框架来确定LLMs的响应是源自于生成的上下文还是检索的上下文。为了实现这个目标，我们构建了包含相互冲突的上下文的数据集，其中每个问题都与生成的和检索的上下文配对，但只有一个上下文包含了正确的答案。我们的实验证明，LLMs（如GPT-4/3.5和Llama2）存在显著的偏差，更倾向于生成的上下文，即使这些上下文提供了错误的信息。我们进一步确定了导致这种偏差的两个关键因素：i）LLMs生成的上下文通常与问题更相似，增加了其被选择的可能性；ii）检索上下文中使用的分割过程打断了其连贯性。

    While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a systematic framework to identify whether LLMs' responses, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To achieve this, we construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs (GPT-4/3.5 and Llama2) towards generated contexts, even when they provide incorrect information. We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) the segmentation process used in retrieved contexts disrupts their compl
    
[^242]: PsySafe：基于心理学的多智能体系统安全攻击、防御和评估的综合框架

    PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety

    [https://arxiv.org/abs/2401.11880](https://arxiv.org/abs/2401.11880)

    PsySafe提出了一个综合框架，通过深入探讨智能体心理学，揭示智能体的黑暗心理状态对安全构成威胁，并提出了有效的风险缓解策略。

    

    多智能体系统在加入大型语言模型（LLMs）后，展现出了集体智能的深远能力。然而，这种智能被恶意使用可能带来重大风险。迄今为止，关于多智能体系统安全问题的全面研究仍然有限。本文通过创新的视角探索了这些问题，发现智能体的黑暗心理状态构成了对安全的重大威胁。为了解决这些问题，我们提出了一个以智能体心理学为基础的综合框架（PsySafe），关注三个关键领域：首先，识别智能体中的黑暗人格特征如何导致风险行为；其次，从心理和行为角度评估多智能体系统的安全性；第三，制定有效的策略来减轻这些风险。我们的实验揭示

    arXiv:2401.11880v2 Announce Type: replace-cross  Abstract: Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit profound capabilities in collective intelligence. However, the potential misuse of this intelligence for malicious purposes presents significant risks. To date, comprehensive research on the safety issues associated with multi-agent systems remains limited. In this paper, we explore these concerns through the innovative lens of agent psychology, revealing that the dark psychological states of agents constitute a significant threat to safety. To tackle these concerns, we propose a comprehensive framework (PsySafe) grounded in agent psychology, focusing on three key areas: firstly, identifying how dark personality traits in agents can lead to risky behaviors; secondly, evaluating the safety of multi-agent systems from the psychological and behavioral perspectives, and thirdly, devising effective strategies to mitigate these risks. Our experiments reveal
    
[^243]: LightDiC: 一种简单而有效的大规模有向图表示学习方法

    LightDiC: A Simple yet Effective Approach for Large-scale Digraph Representation Learning

    [https://arxiv.org/abs/2401.11772](https://arxiv.org/abs/2401.11772)

    LightDiC是基于磁性拉普拉斯的可扩展有向图卷积方法，通过在离线预处理中进行拓扑相关计算，实现了可扩展性，适用于大规模数据库。

    

    大多数现有的图神经网络（GNN）局限于无向图，其捕捉关系信息的范围受限制，限制了其表达能力和在现实场景中的部署。 相较于无向图，有向图（有向图）更适合建模更复杂的拓扑系统的需求，通过捕捉节点之间更复杂的关系，如制定交通和金融网络。 虽然已经提出了一些有向GNN，但它们的灵感主要来自深度学习架构，这导致了冗余的复杂性和计算量，使其无法应用于大规模数据库。 为解决这些问题，我们提出了一种基于磁性拉普拉斯的可扩展变种有向图卷积，LightDiC。 由于拓扑相关的计算仅在离线预处理过程中进行，LightDiC实现了出色的可扩展性，从而实现了向下的...

    arXiv:2401.11772v2 Announce Type: replace-cross  Abstract: Most existing graph neural networks (GNNs) are limited to undirected graphs, whose restricted scope of the captured relational information hinders their expressive capabilities and deployments in real-world scenarios. Compared with undirected graphs, directed graphs (digraphs) fit the demand for modeling more complex topological systems by capturing more intricate relationships between nodes, such as formulating transportation and financial networks. While some directed GNNs have been introduced, their inspiration mainly comes from deep learning architectures, which lead to redundant complexity and computation, making them inapplicable to large-scale databases. To address these issues, we propose LightDiC, a scalable variant of the digraph convolution based on the magnetic Laplacian. Since topology-related computations are conducted solely during offline pre-processing, LightDiC achieves exceptional scalability, enabling downst
    
[^244]: PRewrite: 使用强化学习的提示重写

    PRewrite: Prompt Rewriting with Reinforcement Learning

    [https://arxiv.org/abs/2401.08189](https://arxiv.org/abs/2401.08189)

    本文提出了一种基于强化学习的自动化工具PRewrite，用于重写提示草案并生成高效的新提示，以解决提示工程中的挑战。

    

    arXiv:2401.08189v2 公告类型: 替换 摘要: 提示工程对于基于LLM的应用程序的开发至关重要。然而，通常以“试错”的方式手动完成。这种手动程序可能耗时，效果不佳，并且在许多情况下生成的提示都是次优的。即使对那些看似运作良好的提示，始终存在一个悬而未决的问题：是否可以通过进一步修改使提示变得更好呢？为了解决这些问题，在本文中，我们研究了提示工程自动化。我们考虑了一个特定的使用情景，即开发者/用户已经起草了初始提示，但缺乏时间/专业知识来优化它们。我们提出了PRewrite，一个自动化工具，可重写这些草案，并生成高效的新提示。PRewrite基于强化学习（RL）框架，允许端到端优化，我们的设计允许RL搜索在大动作空间中进行。

    arXiv:2401.08189v2 Announce Type: replace  Abstract: Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated to
    
[^245]: 当大型语言模型Agent遇见6G网络：感知、基础和对齐

    When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment

    [https://arxiv.org/abs/2401.07764](https://arxiv.org/abs/2401.07764)

    本文提出了在6G网络中为LLM agents设计的分布式学习系统，通过协作利用移动设备和边缘服务器，以在长期交互中实现多个具有不同角色的LLMs分布。

    

    基于多模式大型语言模型（LLMs）的AI agent被期望着革新人机交互，并在诸如医疗保健、教育、制造业和娱乐等各个领域提供更个性化的助理服务。在6G网络中部署LLM agents使用户能够通过移动设备民主地访问此前昂贵的AI助理服务，从而减少交互延迟，并更好地保护用户隐私。然而，移动设备的有限容量限制了部署和执行本地LLMs的有效性，这就需要在长期交互过程中将复杂任务卸载到运行在边缘服务器上的全局LLMs。在本文中，我们提出了一种在6G网络中为LLM agents设计的分布式学习系统，利用移动设备和边缘服务器之间的协作，多个具有不同角色的LLMs分布在移动设备和边缘服务器之间实现。

    arXiv:2401.07764v2 Announce Type: replace  Abstract: AI agents based on multimodal large language models (LLMs) are expected to revolutionize human-computer interaction and offer more personalized assistant services across various domains like healthcare, education, manufacturing, and entertainment. Deploying LLM agents in 6G networks enables users to access previously expensive AI assistant services via mobile devices democratically, thereby reducing interaction latency and better preserving user privacy. Nevertheless, the limited capacity of mobile devices constrains the effectiveness of deploying and executing local LLMs, which necessitates offloading complex tasks to global LLMs running on edge servers during long-horizon interactions. In this article, we propose a split learning system for LLM agents in 6G networks leveraging the collaboration between mobile devices and edge servers, where multiple LLMs with different roles are distributed across mobile devices and edge servers to
    
[^246]: 结合机器学习和本体论：系统文献综述

    Combining Machine Learning and Ontology: A Systematic Literature Review

    [https://arxiv.org/abs/2401.07744](https://arxiv.org/abs/2401.07744)

    通过系统文献综述研究了将机器学习和本体整合的过程，发现了增强型本体学习、语义数据挖掘和学习与推理系统这三种主要的混合类别，比较了不同研究中使用的机器学习算法。

    

    受对将归纳推理和演绎推理结合的探索过程的渴望驱使，我们对调查机器学习和本体一体化的文章进行了系统文献综述。其目标是识别涵盖归纳推理（由机器学习执行）和演绎推理（由本体执行）的多种技术以整合到人工智能系统中。我们的综述包括对128项研究的分析，使我们能够确定机器学习和本体之间的三个主要混合类别：增强型本体学习、语义数据挖掘以及学习与推理系统。我们对所有这些类别进行了全面检查，强调了研究中使用的各种机器学习算法。此外，我们将我们的分类与领域内类似的近期工作以及混合人工智能和神经符号方法进行了比较。

    arXiv:2401.07744v2 Announce Type: replace  Abstract: Motivated by the desire to explore the process of combining inductive and deductive reasoning, we conducted a systematic literature review of articles that investigate the integration of machine learning and ontologies. The objective was to identify diverse techniques that incorporate both inductive reasoning (performed by machine learning) and deductive reasoning (performed by ontologies) into artificial intelligence systems. Our review, which included the analysis of 128 studies, allowed us to identify three main categories of hybridization between machine learning and ontologies: learning-enhanced ontologies, semantic data mining, and learning and reasoning systems. We provide a comprehensive examination of all these categories, emphasizing the various machine learning algorithms utilized in the studies. Furthermore, we compared our classification with similar recent work in the field of hybrid AI and neuro-symbolic approaches.
    
[^247]: 超越稀疏奖励：在文本生成中通过语言模型评论增强强化学习

    Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation

    [https://arxiv.org/abs/2401.07382](https://arxiv.org/abs/2401.07382)

    本文提出了一种新的框架，利用大型语言模型的评论能力在强化学习训练中产生中间步骤奖励，以应对稀疏奖励信号所带来的挑战。

    

    强化学习可以将语言模型与非可微分奖励信号（如人类偏好）进行对齐。然而，其中一个主要挑战在于这些奖励信号的稀疏性 - 通常，整个输出只有一个奖励。这种奖励的稀疏性可能导致学习效率低下且不稳定。为了解决这一挑战，本文介绍了一种新颖的框架，利用大型语言模型（LLMs）的评论能力在强化学习训练过程中产生中间步骤奖励。我们的方法涉及将策略模型与评论语言模型相结合，评论语言模型负责为输出的每个部分提供细致的反馈。然后将这些反馈转化为可用于指导强化学习训练过程的标记或跨度级别奖励。我们在两种不同的设置下研究了这种方法：一种是策略模型较小且与更强大的评论者配对。

    arXiv:2401.07382v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning. To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training. Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process. We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful crit
    
[^248]: 欧盟法律中的生成式人工智能：责任、隐私、知识产权和网络安全

    Generative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity

    [https://arxiv.org/abs/2401.07348](https://arxiv.org/abs/2401.07348)

    生成式人工智能和大型语言模型在欧盟法律法规中的应用引发了责任、隐私、知识产权和网络安全等方面的挑战，本文对现有和拟议的法律进行了批判性分析，并提出了改进建议。

    

    生成式人工智能的出现，特别是通过大型语言模型（LLMs）如ChatGPT及其后继模型，标志着人工智能领域的一次范式转变。先进的LLMs表现出多模态性，能处理多样化的数据格式，从而拓宽了它们的应用范围。然而，这些模型的复杂性和新兴自治性引入了预测性和法律遵从性方面的挑战。本文深入探讨了生成式人工智能和LLMs在欧盟背景下的法律和监管影响，分析了责任、隐私、知识产权和网络安全等方面。它批判性地审视了现有和拟议的欧盟立法（包括《人工智能法》草案）在应对生成式人工智能普遍和LLMs特别挑战方面的充分性。本文确定了立法框架中的潜在差距和不足，并提出建议以确保

    arXiv:2401.07348v2 Announce Type: replace-cross  Abstract: The advent of Generative AI, particularly through Large Language Models (LLMs) like ChatGPT and its successors, marks a paradigm shift in the AI landscape. Advanced LLMs exhibit multimodality, handling diverse data formats, thereby broadening their application scope. However, the complexity and emergent autonomy of these models introduce challenges in predictability and legal compliance. This paper delves into the legal and regulatory implications of Generative AI and LLMs in the European Union context, analyzing aspects of liability, privacy, intellectual property, and cybersecurity. It critically examines the adequacy of the existing and proposed EU legislation, including the Artificial Intelligence Act (AIA) draft, in addressing the unique challenges posed by Generative AI in general and LLMs in particular. The paper identifies potential gaps and shortcomings in the legislative framework and proposes recommendations to ensur
    
[^249]: 图语言模型

    Graph Language Models

    [https://arxiv.org/abs/2401.07105](https://arxiv.org/abs/2401.07105)

    引入了一种新型的图语言模型（GLM），结合线性化和图神经网络的优点，解决了传统方法在处理结构化知识图谱时的弱点。

    

    虽然语言模型（LMs）是自然语言处理的主力军，它们与结构化知识图谱（KGs）的相互作用仍在积极研究中。当前用于编码这些图形的方法通常要么（i）将它们线性化以供LM嵌入--这样会低效利用结构信息，要么（ii）使用图神经网络（GNNs）来保留图结构--但GNNs无法像预训练的LM一样很好地表示文本特征。在我们的工作中，我们引入了一种新型LM类型，即图语言模型（GLM），它整合了两种方法的优点并减轻了它们的弱点。GLM参数从预训练的LM中初始化，以增强对个别图概念和三元组的理解。同时，我们设计GLM的架构以整合图偏差，从而促进图内的知识分布。这使GLM能够处理图形、文本以及两者的交织输入。实证

    arXiv:2401.07105v2 Announce Type: replace-cross  Abstract: While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs -- which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical 
    
[^250]: 打开LLMs的潘多拉魔盒：通过表示工程对LLMs进行越狱

    Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering

    [https://arxiv.org/abs/2401.06824](https://arxiv.org/abs/2401.06824)

    通过表示工程对LLMs进行越狱是一种新颖的方法，它利用少量查询对提取“安全模式”，成功规避目标模型的防御，实现了前所未有的越狱性能。

    

    越狱技术旨在通过诱使大型语言模型（LLMs）生成对恶意查询产生有毒响应，来探索LLMs安全性边界，这在LLMs社区内是一个重要关注点。我们提出一种名为通过表示工程对LLMs进行越狱（Jailbreaking LLMs through Representation Engineering，JRE）的新颖越狱方法，其仅需要少量查询对以提取可用于规避目标模型防御的“安全模式”，实现了前所未有的越狱性能。

    arXiv:2401.06824v2 Announce Type: replace-cross  Abstract: Jailbreaking techniques aim to probe the boundaries of safety in large language models (LLMs) by inducing them to generate toxic responses to malicious queries, a significant concern within the LLM community. While existing jailbreaking methods primarily rely on prompt engineering, altering inputs to evade LLM safety mechanisms, they suffer from low attack success rates and significant time overheads, rendering them inflexible. To overcome these limitations, we propose a novel jailbreaking approach, named Jailbreaking LLMs through Representation Engineering (JRE). Our method requires only a small number of query pairs to extract ``safety patterns'' that can be used to circumvent the target model's defenses, achieving unprecedented jailbreaking performance. Building upon these findings, we also introduce a novel defense framework inspired by JRE principles, which demonstrates notable effectiveness. Extensive experimentation conf
    
[^251]: 思维图：利用大型语言模型解决复杂动态商业问题

    Graph-of-Thought: Utilizing Large Language Models to Solve Complex and Dynamic Business Problems

    [https://arxiv.org/abs/2401.06801](https://arxiv.org/abs/2401.06801)

    GoT是一种新型模型，利用图结构增强大型语言模型在复杂任务执行中的灵活性和效率，通过动态路径选择推进了传统认知模型，开源引擎GoTFlow展示了其在自动化决策方面的巨大潜力。

    

    本文介绍了思维图（Graph-of-Thought，GoT），这是一种用于工作流自动化的全新模型，增强了大型语言模型（LLMs）在复杂任务执行中的灵活性和效率。GoT通过具有动态路径选择功能的图结构，推进了传统线性和树状认知模型。开源引擎GoTFlow展示了GoT的实际应用，促进了跨多个领域的自动化数据驱动决策。尽管在复杂性和透明度方面存在挑战，但GoTFlow在改善商业流程方面的潜力巨大，承诺通过持续发展在效率和决策质量上取得进展。

    arXiv:2401.06801v2 Announce Type: replace  Abstract: This paper presents Graph-of-Thought (GoT), a new model for workflow automation that enhances the flexibility and efficiency of Large Language Models (LLMs) in complex task execution. GoT advances beyond traditional linear and tree-like cognitive models with a graph structure that enables dynamic path selection. The open-source engine GoTFlow demonstrates the practical application of GoT, facilitating automated, data-driven decision-making across various domains. Despite challenges in complexity and transparency, GoTFlow's potential for improving business processes is significant, promising advancements in both efficiency and decision quality with continuous development.
    
[^252]: 优化预训练的运动模型

    Refining Pre-Trained Motion Models

    [https://arxiv.org/abs/2401.00850](https://arxiv.org/abs/2401.00850)

    通过自监督训练改进了现有监督模型，发现大多数自监督技术使性能变差，而不是更好。

    

    鉴于在视频中手动注释运动的困难，目前最好的运动估计方法是使用合成数据进行训练，因此在训练/测试之间存在一定困难。自监督方法承诺直接在真实视频上进行训练，但通常表现较差。这些方法包括使用翘曲误差（即颜色恒定）与平滑项相结合进行训练的方法，以及鼓励估计的循环一致性的方法（即向后跟踪应产生与向前跟踪相反的轨迹）。在这项工作中，我们接受了通过自监督训练改进最先进监督模型的挑战。我们发现，当初始化是监督权重时，大多数现有的自监督技术实际上会使性能变差而不是更好，这表明看到新数据的好处被训练信号中的噪声所掩盖。专注于...

    arXiv:2401.00850v2 Announce Type: replace-cross  Abstract: Given the difficulty of manually annotating motion in video, the current best motion estimation methods are trained with synthetic data, and therefore struggle somewhat due to a train/test gap. Self-supervised methods hold the promise of training directly on real video, but typically perform worse. These include methods trained with warp error (i.e., color constancy) combined with smoothness terms, and methods that encourage cycle-consistency in the estimates (i.e., tracking backwards should yield the opposite trajectory as tracking forwards). In this work, we take on the challenge of improving state-of-the-art supervised models with self-supervised training. We find that when the initialization is supervised weights, most existing self-supervision techniques actually make performance worse instead of better, which suggests that the benefit of seeing the new data is overshadowed by the noise in the training signal. Focusing on 
    
[^253]: 准备将生成式预训练变换器系列4模型整合到基因变体评估工作流中：评估性能、漂移和非确定性特征相对于文献中功能性证据的分类

    Preparing to Integrate Generative Pretrained Transformer Series 4 models into Genetic Variant Assessment Workflows: Assessing Performance, Drift, and Nondeterminism Characteristics Relative to Classifying Functional Evidence in Literature

    [https://arxiv.org/abs/2312.13521](https://arxiv.org/abs/2312.13521)

    评估了将GPT-4模型整合到基因变体评估工作流程中的性能、非确定性和漂移特征，为其在复杂临床流程中的适用性提供信息。

    

    背景：大型语言模型（LLMs）有望改善临床测试中基因变体文献回顾。我们评估了生成式预训练变换器4（GPT-4）的性能、非确定性和漂移，以确定其在复杂临床流程中的适用性。方法：使用45篇文章的开发集对用于分类功能性证据的2提示过程进行了优化。提示要求GPT-4提供与变体相关的文章中的所有功能数据，或指示没有功能性证据存在。对于被指示包含功能性证据的文章，第二个提示要求GPT-4将证据分类为致病性、良性或中间/不明确类别。最终的测试集包括72篇手动分类的文章用于测试性能。结果：在2.5个月的时间内（2023年12月至2024年2月），我们观察到一天内的显着差异（非确定性）和a

    arXiv:2312.13521v2 Announce Type: replace-cross  Abstract: Background. Large Language Models (LLMs) hold promise for improving genetic variant literature review in clinical testing. We assessed Generative Pretrained Transformer 4's (GPT-4) performance, nondeterminism, and drift to inform its suitability for use in complex clinical processes. Methods. A 2-prompt process for classification of functional evidence was optimized using a development set of 45 articles. The prompts asked GPT-4 to supply all functional data present in an article related to a variant or indicate that no functional evidence is present. For articles indicated as containing functional evidence, a second prompt asked GPT-4 to classify the evidence into pathogenic, benign, or intermediate/inconclusive categories. A final test set of 72 manually classified articles was used to test performance. Results. Over a 2.5-month period (Dec 2023-Feb 2024), we observed substantial differences in intraday (nondeterminism) and a
    
[^254]: 设计一种精确的伪布尔模型计数器

    Engineering an Exact Pseudo-Boolean Model Counter

    [https://arxiv.org/abs/2312.12341](https://arxiv.org/abs/2312.12341)

    提出了第一个依赖于代数决策图知识编译方法的精确伪布尔模型计数器PBCount，在模型计数方面取得了显著进展

    

    模型计数是计算机科学中的一项基本任务，涉及确定布尔公式的满足赋值数量，通常以合取范式（CNF）表示。虽然对于CNF公式的模型计数受到了广泛关注并具有广泛的应用，但对于伪布尔（PB）公式的模型计数研究相对较少。相比命题布尔公式，伪布尔公式更为简洁，可以更灵活地表示现实世界中的问题。因此，有必要研究用于PB公式的模型计数的高效技术。本文提出了第一个精确的伪布尔模型计数器PBCount，它依赖于代数决策图的知识编译方法。我们进行了大量的实证评估，结果显示PBCount可以计算1513个实例的计数，而当前的最先进方法在计数方面存在限制。

    arXiv:2312.12341v2 Announce Type: replace  Abstract: Model counting, a fundamental task in computer science, involves determining the number of satisfying assignments to a Boolean formula, typically represented in conjunctive normal form (CNF). While model counting for CNF formulas has received extensive attention with a broad range of applications, the study of model counting for Pseudo-Boolean (PB) formulas has been relatively overlooked. Pseudo-Boolean formulas, being more succinct than propositional Boolean formulas, offer greater flexibility in representing real-world problems. Consequently, there is a crucial need to investigate efficient techniques for model counting for PB formulas.   In this work, we propose the first exact Pseudo-Boolean model counter, PBCount, that relies on knowledge compilation approach via algebraic decision diagrams. Our extensive empirical evaluation shows that PBCount can compute counts for 1513 instances while the current state-of-the-art approach cou
    
[^255]: PPO-Clip实现全局最优性：更深入理解修剪技术

    PPO-Clip Attains Global Optimality: Towards Deeper Understandings of Clipping

    [https://arxiv.org/abs/2312.12065](https://arxiv.org/abs/2312.12065)

    该论文在PPO-Clip算法方面做出贡献，建立了其在表格和神经函数逼近设置中的全局收敛结果，特别突出了在神经函数逼近情境下的$O(1/\sqrt{T})$最小迭代收敛速率。

    

    在这篇论文中，我们首次建立了PPO-Clip变体在表格和神经函数逼近设置中具有全局收敛性结果。我们的发现特别突出了在神经函数逼近情境下的$O(1/\sqrt{T})$最小迭代收敛速率。通过引入PPO-Clip的广义版本，结合其与铰链损失的关系，采用熵镜像下降，我们为直接策略参数化的表格PPO-Clip建立了渐近收敛。受表格分析启发，

    arXiv:2312.12065v2 Announce Type: replace-cross  Abstract: Proximal Policy Optimization algorithm employing a clipped surrogate objective (PPO-Clip) is a prominent exemplar of the policy optimization methods. However, despite its remarkable empirical success, PPO-Clip lacks theoretical substantiation to date. In this paper, we contribute to the field by establishing the first global convergence results of a PPO-Clip variant in both tabular and neural function approximation settings. Our findings highlight the $O(1/\sqrt{T})$ min-iterate convergence rate specifically in the context of neural function approximation. We tackle the inherent challenges in analyzing PPO-Clip through three central concepts: (i) We introduce a generalized version of the PPO-Clip objective, illuminated by its connection with the hinge loss. (ii) Employing entropic mirror descent, we establish asymptotic convergence for tabular PPO-Clip with direct policy parameterization. (iii) Inspired by the tabular analysis,
    
[^256]: 计数奖励自动机：通过利用奖励函数结构实现高效抽样的强化学习

    Counting Reward Automata: Sample Efficient Reinforcement Learning Through the Exploitation of Reward Function Structure

    [https://arxiv.org/abs/2312.11364](https://arxiv.org/abs/2312.11364)

    提出了计数奖励自动机概念，能够对任何奖励函数进行建模，使得强化学习更加高效，并且能够利用自然语言任务描述来指定状态机，实验证明在样本效率方面优于竞争方法。

    

    我们提出了计数奖励自动机——一种有限状态机变体，能够对任何可表示为形式语言的奖励函数进行建模。与以往仅能表达任务为正则语言的方法不同，我们的框架允许描述为无限制文法的任务。我们证明，配备这样抽象机器的代理能够解决比使用当前方法的任务集更广泛。我们展示，这种表达能力增加并不会增加自动机复杂性的代价。提出了一些利用自动机结构来提高样本效率的学习算法。我们展示，在我们的公式中所需的状态机可以使用大型语言模型从自然语言任务描述中指定。实证结果表明，我们的方法在样本效率、自动机复杂性方面优于竞争方法。

    arXiv:2312.11364v2 Announce Type: replace  Abstract: We present counting reward automata-a finite state machine variant capable of modelling any reward function expressible as a formal language. Unlike previous approaches, which are limited to the expression of tasks as regular languages, our framework allows for tasks described by unrestricted grammars. We prove that an agent equipped with such an abstract machine is able to solve a larger set of tasks than those utilising current approaches. We show that this increase in expressive power does not come at the cost of increased automaton complexity. A selection of learning algorithms are presented which exploit automaton structure to improve sample efficiency. We show that the state machines required in our formulation can be specified from natural language task descriptions using large language models. Empirical results demonstrate that our method outperforms competing approaches in terms of sample efficiency, automaton complexity, an
    
[^257]: 解读大语言模型微调中的指令混合

    Demystifying Instruction Mixing for Fine-tuning Large Language Models

    [https://arxiv.org/abs/2312.10793](https://arxiv.org/abs/2312.10793)

    指令微调提升了大语言模型在各种任务中的性能，研究发现不同指令类型对特定应用更有利，但可能对其他领域产生负面影响。

    

    指令微调显著提高了大型语言模型（LLM）在各种任务上的性能。然而，对于优化LLM微调的指令数据集混合的过程仍然知之甚少。本研究将指令分为三类主要类型：自然语言处理下游任务、编程和一般对话。我们探讨了指令微调对LLM性能的不同数据集组合的影响，并发现某些指令类型对特定应用更有利，但可能对其他领域产生负面影响。这项工作为指令混合提供了见解，为未来研究奠定了基础。

    arXiv:2312.10793v3 Announce Type: replace-cross  Abstract: Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood. This study categorizes instructions into three primary types: NLP downstream tasks, coding, and general chat. We explore the effects of instruction tuning on different combinations of datasets on LLM performance, and find that certain instruction types are more advantageous for specific applications but can negatively impact other areas. This work provides insights into instruction mixtures, laying the foundations for future research.
    
[^258]: 超越自注意力的序列推荐中的关注归纳偏差

    An Attentive Inductive Bias for Sequential Recommendation beyond the Self-Attention

    [https://arxiv.org/abs/2312.10325](https://arxiv.org/abs/2312.10325)

    提出了一种名为BSARec的新方法，超越了自注意力，在序列推荐中注入了归纳偏差，并集成了低频和高频信息以减轻过度平滑问题

    

    基于Transformer的序列推荐（SR）模型取得了显著的成功。 Transformer的自注意机制在计算机视觉和自然语言处理中遇到了过度平滑问题，即隐藏表示变得类似于标记。 在SR领域，我们首次展示了相同问题的发生。 我们进行了开创性的研究，揭示了自注意在SR中的低通滤波特性，导致了过度平滑。 为此，我们提出了一种名为$\textbf{B}$eyond $\textbf{S}$elf-$\textbf{A}$ttention for Sequential $\textbf{Rec}$ommendation（BSARec）的新方法，利用傅里叶变换来 i）通过考虑细粒度的序列模式注入归纳偏差和 ii）集成低频和高频信息以减轻过度平滑。 我们的发现在SR领域显示了显著的进展，并有望搭起

    arXiv:2312.10325v2 Announce Type: replace-cross  Abstract: Sequential recommendation (SR) models based on Transformers have achieved remarkable successes. The self-attention mechanism of Transformers for computer vision and natural language processing suffers from the oversmoothing problem, i.e., hidden representations becoming similar to tokens. In the SR domain, we, for the first time, show that the same problem occurs. We present pioneering investigations that reveal the low-pass filtering nature of self-attention in the SR, which causes oversmoothing. To this end, we propose a novel method called $\textbf{B}$eyond $\textbf{S}$elf-$\textbf{A}$ttention for Sequential $\textbf{Rec}$ommendation (BSARec), which leverages the Fourier transform to i) inject an inductive bias by considering fine-grained sequential patterns and ii) integrate low and high-frequency information to mitigate oversmoothing. Our discovery shows significant advancements in the SR domain and is expected to bridge t
    
[^259]: TAP4LLM：用于大型语言模型推理的表格提供者在对半结构化数据进行采样、增补和打包

    TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning

    [https://arxiv.org/abs/2312.09039](https://arxiv.org/abs/2312.09039)

    TAP4LLM提出了一个用于生成表格提示的多功能预处理工具箱，通过采样、增补和打包半结构化数据，解决了在大型语言模型推理中处理复杂问题和大型表格的挑战。

    

    基于表格的推理在结合深度模型和离散推理方面取得了显著进展，这需要对自由形式的自然语言（NL）问题和半结构化表格数据进行推理。然而，先前的表格推理解决方案只考虑小型表格，并且在处理更大表格时存在局限性。此外，大多数现有方法难以推理复杂问题，因为它们缺乏基本信息或分散在不同位置。为了解决这些挑战，我们提出了TAP4LLM作为一个多功能的预处理工具箱，通过平衡标记分配权衡来生成表格提示，实现(1) 表格采样，(2) 表格增补和(3) 表格打包。在每个模块中，我们收集和设计了几种在不同情况下使用的常见方法（例如，速度与准确性的平衡）。我们还对T内部每个组件的性能进行了全面评估。

    arXiv:2312.09039v2 Announce Type: replace-cross  Abstract: Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and semi-structured tabular data. However, previous table reasoning solutions only consider small-sized tables and exhibit limitations in handling larger tables. In addition, most existing methods struggle to reason over complex questions since they lack essential information or they are scattered in different places. To alleviate these challenges, we propose TAP4LLM as a versatile pre-processing toolbox to generate table prompts through (1) table sampling, (2) table augmentation, and (3) table packing while balancing the token allocation trade-off. In each module, we collect and design several common methods for usage in various scenarios (e.g., speed over accuracy). We also provide a comprehensive evaluation on performance of each components inside T
    
[^260]: Math-Shepherd: 在不需要人工标注的情况下逐步验证和加强LLMs

    Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations

    [https://arxiv.org/abs/2312.08935](https://arxiv.org/abs/2312.08935)

    Math-Shepherd提出了一种新的数学奖励模型，通过自动生成的过程监督数据实现LLMs的逐步验证和加强，显著提高了数学问题解决的准确性。

    

    在这篇论文中，我们提出了一种名为Math-Shepherd的创新过程导向数学奖励模型，为数学问题解决的每一步分配奖励分数。Math-Shepherd的训练是使用自动构建的基于过程的监督数据完成的，打破了现有工作中对手动标注的严重依赖瓶颈。我们探讨了Math-Shepherd在两种场景中的有效性：1）\textit{验证}：利用Math-Shepherd对大型语言模型(LLMs)生成的多个输出进行重新排序；2）\textit{强化学习}：使用Math-Shepherd通过逐步的近端策略优化(PPO)加强LLMs。通过Math-Shepherd，一系列开源LLMs展现出卓越的性能。例如，使用Math-Shepherd的逐步PPO显著提高了Mistral-7B的准确率(GSM8K由77.9%提高到84.1%，MATH由28.6%提高到33.0%)

    arXiv:2312.08935v3 Announce Type: replace  Abstract: In this paper, we present an innovative process-oriented math process reward model called \textbf{Math-Shepherd}, which assigns a reward score to each step of math problem solutions. The training of Math-Shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-Shepherd in two scenarios: 1) \textit{Verification}: Math-Shepherd is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) \textit{Reinforcement Learning}: Math-Shepherd is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With Math-Shepherd, a series of open-source LLMs demonstrates exceptional performance. For instance, the step-by-step PPO with Math-Shepherd significantly improves the accuracy of Mistral-7B (77.9\%$\to$84.1\% on GSM8K and 28.6\%$\to$33.0\% on MATH). The
    
[^261]: 关于大型语言模型对齐多样化偏好的研究

    On Diversified Preferences of Large Language Model Alignment

    [https://arxiv.org/abs/2312.07401](https://arxiv.org/abs/2312.07401)

    本文通过定量分析常用人类反馈数据集，揭示了多样化偏好对奖励建模的影响，提出了一种新颖的多目标奖励学习方法以增强校准性能

    

    将大型语言模型（LLMs）与人类偏好对齐被认为是提高LLMs交互质量的关键。然而，在这个多元化的世界中，由于标注者的不同偏好，人类偏好可能会多样化，这阻碍了LLM对齐方法的有效性。本文首次对常用人类反馈数据集进行定量分析，以研究多样化偏好对奖励建模的影响。我们的分析揭示了奖励模型（RMs）的校准性能与LLMs的对齐性能之间的相关性。我们发现多样化偏好数据对人类共享偏好（如“无害和有帮助”）上的奖励模型的校准性能产生负面影响，从而损害了LLMs的对齐性能。为解决这种无效性，我们提出了一种新颖的多目标奖励学习方法（MORE）以增强校准性能。

    arXiv:2312.07401v3 Announce Type: replace  Abstract: Aligning large language models (LLMs) with human preferences has been recognized as the key to improving LLMs' interaction quality. However, in this pluralistic world, human preferences can be diversified due to annotators' different tastes, which hinders the effectiveness of LLM alignment methods. This paper presents the first quantitative analysis of commonly used human feedback datasets to investigate the impact of diversified preferences on reward modeling. Our analysis reveals a correlation between the calibration performance of reward models (RMs) and the alignment performance of LLMs. We find that diversified preference data negatively affect the calibration performance of RMs on human-shared preferences, such as \textit{Harmless\&Helpful}, thereby impairing the alignment performance of LLMs. To address the ineffectiveness, we propose a novel Multi-Objective Reward learning method (MORE) to enhance the calibration performance 
    
[^262]: KnowGPT：大型语言模型的黑盒知识注入

    KnowGPT: Black-Box Knowledge Injection for Large Language Models

    [https://arxiv.org/abs/2312.06185](https://arxiv.org/abs/2312.06185)

    KnowGPT是一种为大型语言模型提供黑盒知识注入的框架，通过深度强化学习和多臂老虎机构建最适合每个问题的提示，在三个基准数据集上实验证明其显著提升了知识注入的效果。

    

    生成式大型语言模型（LLMs），如ChatGPT，提供互动式API，可以以人类专家水平回答常见问题。然而，当面临需要特定领域或专业领域知识的问题时，这些模型通常会给出不准确或不正确的响应，这些知识并未包含在它们的训练语料库中。此外，许多最先进的LLMs并非开源，这使得仅使用模型API注入知识具有挑战性。在本研究中，我们介绍了KnowGPT，一种用于LLMs在问答中的黑盒知识注入框架。KnowGPT利用深度强化学习（RL）从知识图中提取相关知识，并使用多臂老虎机（MAB）为每个问题构建最合适的提示。我们在三个基准数据集上进行了大量实验，展示了KnowGPT显著增强了现有方法。值得注意的是，KnowGPT平均改进了23%。

    arXiv:2312.06185v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs), such as ChatGPT, offer interactive APIs that can answer common questions at a human-expert level. However, these models often give inaccurate or incorrect responses when faced with questions requiring domain-specific or professional-specific knowledge not covered in their training corpus. Furthermore, many state-of-the-art LLMs are not open-source, making it challenging to inject knowledge with model APIs only. In this work, we introduce KnowGPT, a black-box knowledge injection framework for LLMs in question answering. KnowGPT leverages deep reinforcement learning (RL) to extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed Bandit (MAB) to construct the most suitable prompt for each question. Our extensive experiments on three benchmark datasets showcase that KnowGPT significantly enhances the existing methods. Notably, KnowGPT achieves an average improvement of 23.
    
[^263]: 解锁预测性文本生成：对大型语言模型解码的受限方法

    Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding

    [https://arxiv.org/abs/2312.06149](https://arxiv.org/abs/2312.06149)

    提出了将文本生成形式化为未来受限生成问题的方法，以最小化不良行为并强制执行对指令的忠实性，并通过LLMs有效指导文本生成。

    

    大型语言模型(LLMs)展现了强大的文本生成能力。然而，对于给定提示或指令实现最佳结果可能具有挑战性，特别是对于十亿级别的模型。此外，不良行为如毒性或幻觉可能会显现。在这项工作中，我们提出将文本生成形式化为未来受限生成问题，以最小化不良行为并强制执行对指令的忠实性。使用LLMs实现未来约束满足度的估计引导文本生成过程。我们的广泛实验表明所提出的方法在三个不同的文本生成任务中的有效性：关键词受限生成、毒性减少等。

    arXiv:2312.06149v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 202
    
[^264]: 面向类别的修剪以提高神经网络的效率

    Class-Aware Pruning for Efficient Neural Networks

    [https://arxiv.org/abs/2312.05875](https://arxiv.org/abs/2312.05875)

    本文提出了一种面向类别的修剪技术，与先前的修剪策略不同，该技术通过评估与类别数量相关的滤波器重要性来压缩DNN，从而减少计算成本。

    

    深度神经网络在各个领域展示出了显著的成功。然而，DNN中的大量浮点运算(FLOPs)在资源受限的应用(如边缘设备)中部署时会带来挑战。为解决这一问题，引入了修剪来减少执行DNN时的计算成本。与先前的修剪策略不同，本文提出了一种面向类别的修剪技术来压缩DNN，为减少DNN的计算成本提供了一种新的视角。在每次迭代中，神经网络训练被修改以实现面向类别的修剪。然后，评估了与类别数量相关的滤波器的重要性。针对只对少数类别重要的滤波器进行移除。随后神经网络被重新训练。

    arXiv:2312.05875v2 Announce Type: replace  Abstract: Deep neural networks (DNNs) have demonstrated remarkable success in various fields. However, the large number of floating-point operations (FLOPs) in DNNs poses challenges for their deployment in resource-constrained applications, e.g., edge devices. To address the problem, pruning has been introduced to reduce the computational cost in executing DNNs. Previous pruning strategies are based on weight values, gradient values and activation outputs. Different from previous pruning solutions, in this paper, we propose a class-aware pruning technique to compress DNNs, which provides a novel perspective to reduce the computational cost of DNNs. In each iteration, the neural network training is modified to facilitate the class-aware pruning. Afterwards, the importance of filters with respect to the number of classes is evaluated. The filters that are only important for a few number of classes are removed. The neural network is then retraine
    
[^265]: 通过无数据网格移动器实现更好的神经PDE求解器

    Better Neural PDE Solvers Through Data-Free Mesh Movers

    [https://arxiv.org/abs/2312.05583](https://arxiv.org/abs/2312.05583)

    提出了一种神经PDE求解器，通过无数据神经网格适配器（DMM）解决了神经PDE求解器需要昂贵网格数据和解决空间的自由度和拓扑结构变化的挑战。

    

    最近，神经网络已被广泛应用于解决物理系统建模中的偏微分方程（PDEs）。虽然主要研究集中在学习预定义静态网格离散化上的系统演化，但一些方法利用强化学习或监督学习技术来创建适应性和动态网格，由于这些系统的动态特性。然而，这些方法面临两个主要挑战：（1）需要昂贵的最佳网格数据，和（2）在网格细化过程中解决空间的自由度和拓扑结构的变化。为了解决这些挑战，本文提出了一种带有神经网格适配器的神经PDE求解器。首先，我们介绍了一种新颖的无数据神经网格适配器，称为Data-free Mesh Mover（DMM），具有两个主要创新点。首先，它是一个操作符，将解映射到自适应网格上，并使用Monge-Amp\`ere方程进行训练。

    arXiv:2312.05583v2 Announce Type: replace-cross  Abstract: Recently, neural networks have been extensively employed to solve partial differential equations (PDEs) in physical system modeling. While major studies focus on learning system evolution on predefined static mesh discretizations, some methods utilize reinforcement learning or supervised learning techniques to create adaptive and dynamic meshes, due to the dynamic nature of these systems. However, these approaches face two primary challenges: (1) the need for expensive optimal mesh data, and (2) the change of the solution space's degree of freedom and topology during mesh refinement. To address these challenges, this paper proposes a neural PDE solver with a neural mesh adapter. To begin with, we introduce a novel data-free neural mesh adaptor, called Data-free Mesh Mover (DMM), with two main innovations. Firstly, it is an operator that maps the solution to adaptive meshes and is trained using the Monge-Amp\`ere equation withou
    
[^266]: 应用大型语言模型和思维链进行自动评分

    Applying Large Language Models and Chain-of-Thought for Automatic Scoring

    [https://arxiv.org/abs/2312.03748](https://arxiv.org/abs/2312.03748)

    本研究探讨了在自动评分学生对科学评估写作反馈中应用大型语言模型（LLMs）和思维链（CoT），通过零次或少次学习等策略自动评分，其中少次学习表现更佳。

    

    本研究调查了大型语言模型（LLMs），特别是 GPT-3.5 和 GPT-4，以及思维链（CoT）在自动评分学生对科学评估的写作反馈中的应用。我们专注于克服先前限制研究人员和教育工作者使用基于人工智能的自动评分工具的无法访问性、技术复杂性和缺乏解释性的挑战。通过一个包括 1,650 个学生反馈的六项评估任务（三个二项和三个三项）的测试数据集，我们采用六种提示工程策略来自动评分学生的反馈。这六种策略将零次学习或少次学习与 CoT 结合在一起，要么独自使用，要么与项目干和评分细则一起使用。结果表明，少次学习（准确率=.67）优于零次学习（准确率=.60），提高12.6%。

    arXiv:2312.03748v2 Announce Type: replace-cross  Abstract: This study investigates the application of large language models (LLMs), specifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT) in the automatic scoring of student-written responses to science assessments. We focused on overcoming the challenges of accessibility, technical complexity, and lack of explainability that have previously limited the use of artificial intelligence-based automatic scoring tools among researchers and educators. With a testing dataset comprising six assessment tasks (three binomial and three trinomial) with 1,650 student responses, we employed six prompt engineering strategies to automatically score student responses. The six strategies combined zero-shot or few-shot learning with CoT, either alone or alongside item stem and scoring rubrics. Results indicated that few-shot (acc = .67) outperformed zero-shot learning (acc = .60), with 12.6% increase. CoT, when used without item stem and scoring rubric
    
[^267]: 通过熵率最小化实现可预测的强化学习动态

    Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization

    [https://arxiv.org/abs/2311.18703](https://arxiv.org/abs/2311.18703)

    该论文提出了一种名为PA-RL的方法，通过最小化熵率来引导强化学习智能体展现可预测的行为。研究展示了如何利用平均替代奖励实现确定性策略，并在动态模型的基础上近似计算值函数。

    

    在强化学习中，智能体没有动机展示可预测的行为，通常通过策略熵正则化推动智能体在探索上随机化其行为。从人的角度来看，这使得强化学习智能体很难解释和预测；从安全角度来看，更难以进行形式化验证。我们提出了一种新的方法，称为可预测性感知强化学习（PA-RL），用于引导智能体展现可预测的行为，其利用状态序列熵率作为可预测性度量。我们展示了如何将熵率制定为平均奖励目标，并且由于其熵奖励函数依赖于策略，我们引入了一个动作相关的替代熵，以利用PG方法。我们证明了最小化平均替代奖励的确定性策略存在，并且最小化了实际熵率。我们还展示了如何在学习到的动态模型的基础上近似计算与值函数。

    In Reinforcement Learning (RL), agents have no incentive to exhibit predictable behaviors, and are often pushed (through e.g. policy entropy regularization) to randomize their actions in favor of exploration. From a human perspective, this makes RL agents hard to interpret and predict, and from a safety perspective, even harder to formally verify. We propose a novel method to induce predictable behavior in RL agents, referred to as Predictability-Aware RL (PA-RL), which employs the state sequence entropy rate as a predictability measure. We show how the entropy rate can be formulated as an average reward objective, and since its entropy reward function is policy-dependent, we introduce an action-dependent surrogate entropy enabling the use of PG methods. We prove that deterministic policies minimizing the average surrogate reward exist and also minimize the actual entropy rate, and show how, given a learned dynamical model, we are able to approximate the value function associated to th
    
[^268]: 法律要求分析：从合规的角度看

    Legal Requirements Analysis: A Regulatory Compliance Perspective

    [https://arxiv.org/abs/2311.13871](https://arxiv.org/abs/2311.13871)

    本文研究了从合规角度分析法律要求，特别是在软件开发过程中的要求工程阶段，以确保软件的合规性。主要关注欧盟的一般数据保护条例（GDPR）等法规对收集、处理或分享个人数据的软件系统的规定。

    

    现代软件已成为许多学科和应用环境中日常活动的重要组成部分。利用人工智能（AI）实现智能自动化在许多领域取得了突破。AI的有效性可以归功于多种因素，其中之一是数据的增加可用性。欧洲联盟（EU）的一般数据保护条例（GDPR）等法规的出台是为了确保个人数据的保护。收集、处理或分享个人数据的软件系统必须遵守这些法规的合规性要求。开发符合法规要求的软件在软件开发过程中要求工程（RE）阶段中需要重视处理法律要求。RE关注的是指定和维护一个系统的需求，包括法律要求。

    Modern software has been an integral part of everyday activities in many disciplines and application contexts. Introducing intelligent automation by leveraging artificial intelligence (AI) led to break-throughs in many fields. The effectiveness of AI can be attributed to several factors, among which is the increasing availability of data. Regulations such as the general data protection regulation (GDPR) in the European Union (EU) are introduced to ensure the protection of personal data. Software systems that collect, process, or share personal data are subject to compliance with such regulations. Developing compliant software depends heavily on addressing legal requirements stipulated in applicable regulations, a central activity in the requirements engineering (RE) phase of the software development process. RE is concerned with specifying and maintaining requirements of a system-to-be, including legal requirements. Legal agreements which describe the policies organizations implement f
    
[^269]: 关于文本转图像扩散模型的版权风险

    On Copyright Risks of Text-to-Image Diffusion Models

    [https://arxiv.org/abs/2311.12803](https://arxiv.org/abs/2311.12803)

    文本到图像扩散模型存在版权侵权风险，我们通过引入数据生成管道，研究了更微妙的侵权形式，提供了更实际的版权侵权调查方法

    

    扩散模型在许多生成建模任务中表现出色，特别是在从文本提示中创建图像的任务中表现出色，这个任务被称为文本到图像（T2I）生成。尽管这些模型能够生成高质量的图像，但它们经常复制训练数据中的元素，导致近年来实际应用中版权担忧不断增加。为了回应关于版权侵权的担忧增加，最近的研究已经研究了在使用直接有版权的提示时扩散模型的版权行为。我们的研究通过检查更微妙的侵权形式扩展了这一点，即即使是间接的提示也可能引发版权问题。具体地，我们引入了一个数据生成管道，以系统地产生用于研究扩散模型中版权问题的数据。我们的管道使我们能够在更实际的环境中调查版权侵权，涉及复制视觉特征而不是整个作品。

    arXiv:2311.12803v2 Announce Type: replace-cross  Abstract: Diffusion models excel in many generative modeling tasks, notably in creating images from text prompts, a task referred to as text-to-image (T2I) generation. Despite the ability to generate high-quality images, these models often replicate elements from their training data, leading to increasing copyright concerns in real applications in recent years. In response to this raising concern about copyright infringement, recent studies have studied the copyright behavior of diffusion models when using direct, copyrighted prompts. Our research extends this by examining subtler forms of infringement, where even indirect prompts can trigger copyright issues. Specifically, we introduce a data generation pipeline to systematically produce data for studying copyright in diffusion models. Our pipeline enables us to investigate copyright infringement in a more practical setting, involving replicating visual features rather than entire works
    
[^270]: 物理学中的Alpha Zero：将Alpha Zero用于符号回归以找到物理学中的解析方法

    Alpha Zero for Physics: Application of Symbolic Regression with Alpha Zero to find the analytical methods in physics

    [https://arxiv.org/abs/2311.12713](https://arxiv.org/abs/2311.12713)

    提出了使用Alpha Zero算法中的符号回归来发展物理学中的解析方法，展示了其可以推导出Floquet系统中的高频展开，可能带来新的物理学理论框架。

    

    神经网络的机器学习现在成为各种任务的更强大工具，例如自然语言处理、图像识别、赢得游戏，甚至用于物理问题。尽管有许多关于将机器学习应用于数值计算和实验辅助的研究，但将机器学习应用于寻找解析方法的方法却鲜为人知。本文提出了使用带Alpha Zero算法的符号回归来发展物理学中解析方法的框架，即物理学中的Alpha Zero（AZfP）。作为演示，我们展示了AZfP可以推导出Floquet系统中的高频展开。AZfP可能具有在物理学中发展新的理论框架的可能性。

    arXiv:2311.12713v3 Announce Type: replace-cross  Abstract: Machine learning with neural networks is now becoming a more and more powerful tool for various tasks, such as natural language processing, image recognition, winning the game, and even for the issues of physics. Although there are many studies on the application of machine learning to numerical calculation and assistance of experiments, the methods of applying machine learning to find the analytical method are poorly studied. In this paper, we propose the frameworks of developing analytical methods in physics by using the symbolic regression with the Alpha Zero algorithm, that is Alpha Zero for physics (AZfP). As a demonstration, we show that AZfP can derive the high-frequency expansion in the Floquet systems. AZfP may have the possibility of developing a new theoretical framework in physics.
    
[^271]: 使用可屏蔽股票表示的强化学习在可定制股票池中进行投资组合管理

    Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools

    [https://arxiv.org/abs/2311.10801](https://arxiv.org/abs/2311.10801)

    使用EarnMore方法，我们提出了一种新的RL方法，可以允许RL代理与可定制股票池（CSPs）交互，而不需要重新训练。

    

    投资组合管理（PM）是一项基本的金融交易任务，探索定期将资金重新配置到不同股票中以追求长期利润。最近，强化学习（RL）显示出其潜力，通过与金融市场互动来训练具有盈利能力的PM代理。但是，现有工作主要集中在固定股票池上，这与投资者的实际需求不一致。为应对这一挑战，我们提出EarnMore，一种新的RL方法，可以允许RL代理与可定制股票池（CSPs）交互，而不需要重新训练。

    arXiv:2311.10801v3 Announce Type: replace-cross  Abstract: Portfolio management (PM) is a fundamental financial trading task, which explores the optimal periodical reallocation of capitals into different stocks to pursue long-term profits. Reinforcement learning (RL) has recently shown its potential to train profitable agents for PM through interacting with financial markets. However, existing work mostly focuses on fixed stock pools, which is inconsistent with investors' practical demand. Specifically, the target stock pool of different investors varies dramatically due to their discrepancy on market states and individual investors may temporally adjust stocks they desire to trade (e.g., adding one popular stocks), which lead to customizable stock pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny change of the stock pool, which leads to high computational cost and unstable performance. To tackle this challenge, we propose EarnMore, a rEinforcement leARNin
    
[^272]: MedAgents: 大型语言模型作为零-shot医学推理的合作者

    MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning

    [https://arxiv.org/abs/2311.10537](https://arxiv.org/abs/2311.10537)

    提出了一个新颖的医学领域的跨学科合作(MC)框架，利用基于LLM的代理在角色扮演设置中参与协作多轮讨论，从而提高LLM的熟练程度和推理能力

    

    大型语言模型(LLMs)尽管在各种通用领域取得了显著进展，但在医学和医疗保健领域面临重大障碍。为了解决这些问题，我们提出了一个新颖的医学领域的跨学科合作(MC)框架，利用基于LLM的代理在角色扮演设置中参与协作多轮讨论，从而提高LLM的熟练程度和推理能力。这种无需训练的框架包括五个关键步骤：收集领域专家、提出个别分析、将这些分析总结成报告、在讨论中反复迭代直到达成共识，最终做出决策。我们的工作侧重于零-shot情景，在实际场景中具有适用性。在九个数据集上的实验结果显示...

    arXiv:2311.10537v2 Announce Type: replace-cross  Abstract: Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine dataset
    
[^273]: Symbol-LLM: 面向大型语言模型基础符号中心接口

    Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models

    [https://arxiv.org/abs/2311.09278](https://arxiv.org/abs/2311.09278)

    Symbol-LLM 提出了一种通过数据和框架来解决大型语言模型中符号数据注入的挑战，旨在捕捉符号间的相互关系和促进协同作用。

    

    虽然大型语言模型(LLMs)展现出在处理和生成类似于人类文本方面的显著能力，但在理解和表达超出自然语言范围的世界知识方面存在局限性(例如化学分子式)。直接将一系列符号数据注入到LLMs的训练中可能存在问题，因为它忽视了不同符号家族之间的协同关系，也忽视了自然数据和符号数据之间平衡混合的必要性。在这项工作中，我们从数据和框架两个方面应对这些挑战，并引入了Symbol-LLM系列模型。首先，我们策划了一个包含34个任务并涵盖约20个不同符号家族的数据集，旨在捕捉符号之间的相互关系并促进符号之间的协同作用。然后，一个两阶段调优框架成功地注入了符号知识而不会损失

    arXiv:2311.09278v2 Announce Type: replace-cross  Abstract: Although Large Language Models (LLMs) demonstrate remarkable ability in processing and generating human-like text, they do have limitations when it comes to comprehending and expressing world knowledge that extends beyond the boundaries of natural language(e.g., chemical molecular formula). Injecting a collection of symbolic data directly into the training of LLMs can be problematic, as it disregards the synergies among different symbolic families and overlooks the need for a balanced mixture of natural and symbolic data. In this work, we tackle these challenges from both a data and framework perspective and introduce Symbol-LLM series models. First, we curated a data collection consisting of 34 tasks and incorporating approximately 20 distinct symbolic families, intending to capture the interrelations and foster synergies between symbols. Then, a two-stage tuning framework succeeds in injecting symbolic knowledge without loss 
    
[^274]: Safer-Instruct: 使用自动化偏好数据对齐语言模型

    Safer-Instruct: Aligning Language Models with Automated Preference Data

    [https://arxiv.org/abs/2311.08685](https://arxiv.org/abs/2311.08685)

    Safer-Instruct通过反向指导调整、指导感应和专家模型评估的方式，实现了自动构建大规模偏好数据的目的，从而在没有人工标注者的情况下高效生成高质量的偏好数据。

    

    人工反馈强化学习（RLHF）是增强语言模型能力的重要策略。然而，为RLHF标注偏好数据是一项资源密集且需要创造力的过程，而现有的自动生成方法在数据多样性和质量方面存在局限性。为了应对这一挑战，我们提出了Safer-Instruct，这是一个用于自动构建大规模偏好数据的全新流水线。我们的方法利用了反向指导调整、指导感应和专家模型评估，以高效生成高质量的偏好数据，无需人工标注者。为了验证Safer-Instruct的有效性，我们将该流水线应用于构建一个安全偏好数据集作为案例研究。在这个合成数据集上微调Alpaca模型不仅展示出更好的无害性，还表现出优于在人工标注的安全偏好数据上微调的模型，同时保持

    arXiv:2311.08685v2 Announce Type: replace-cross  Abstract: Reinforcement learning from human feedback (RLHF) is a vital strategy for enhancing model capability in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while existing automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for automatically constructing large-scale preference data. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. To verify the effectiveness of Safer-Instruct, we apply the pipeline to construct a safety preference dataset as a case study. Finetuning an Alpaca model on this synthetic dataset not only demonstrates improved harmlessness but also outperforms models fine-tuned on human-annotated safety preference data, all the while mainta
    
[^275]: 再问一次：自一致性改善语言模型在（几乎）所有场景中的推理

    Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios

    [https://arxiv.org/abs/2311.08154](https://arxiv.org/abs/2311.08154)

    自一致性是一种通用的集成优化方法，可以应用于几乎所有情景中，能够解决语言模型推理中的重复性和局部最优性问题。

    

    尽管思维链（CoT）提示结合语言模型在复杂推理任务上取得了令人鼓舞的结果，但CoT提示中通常使用的贪婪解码会导致重复性和局部最优性。为解决这一缺点，集成优化尝试获得多个推理路径以得到最终答案集成。然而，当前的集成优化方法要么简单地采用基于规则的后处理，比如“自一致性”，要么训练一个基于几个与任务相关的人类注释的附加模型来在多个推理路径中选择最佳路径，但未能推广到现实设置，其中输入问题类型未知或推理路径的答案格式未知。为了避免它们的局限性，我们提出了“自一致性”，这是一种通用的集成优化方法，在几乎所有情景中适用，其中输入问题的类型...

    arXiv:2311.08154v2 Announce Type: replace-cross  Abstract: Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local optimality. To address this shortcoming, ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly. However, current ensemble-optimization methods either simply employ rule-based post-processing such as \textit{self-consistency}, or train an additional model based on several task-related human annotations to select the best one among multiple reasoning paths, yet fail to generalize to realistic settings where the type of input questions is unknown or the answer format of reasoning paths is unknown. To avoid their limitations, we propose \textbf{Self-Agreement}, a generalizable ensemble-optimization method applying in almost all scenarios where the type of input question
    
[^276]: 对抗偏好优化

    Adversarial Preference Optimization

    [https://arxiv.org/abs/2311.08045](https://arxiv.org/abs/2311.08045)

    提出了一种对抗偏好优化（APO）框架，实现了在没有额外注释的情况下，通过对抗学习自适应于生成分布差距。

    

    人类偏好调整是提高大型语言模型（LLMs）交互质量的关键。现有的对齐方法依赖于手动注释的偏好数据来指导LLM的优化方向。然而，在实践中，持续更新LLMs会导致模型生成样本与人类首选响应之间存在分布差距，这阻碍了模型微调的效率。为了缓解这个问题，先前的方法需要在生成的样本上额外进行偏好注释，以适应转移分布，这需要大量的注释资源。针对更高效的人类偏好优化，我们提出了一种对抗偏好优化（APO）框架，其中LLM代理和偏好模型通过极小-极大博弈交替更新。在没有额外注释的情况下，我们的APO方法可以通过对抗学习自适应于生成分布差距。

    arXiv:2311.08045v2 Announce Type: replace-cross  Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learni
    
[^277]: 毒性检测并不是你所需要的全部：弥合支持志愿内容管理员的差距

    Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators

    [https://arxiv.org/abs/2311.07879](https://arxiv.org/abs/2311.07879)

    本研究揭示了人工智能模型在识别有毒、冒犯和令人讨厌的内容方面的进展，并探讨了这些改进是否真正满足了志愿内容管理员在工作中的需求。

    

    人工智能模型在识别有毒、冒犯和令人讨厌的内容方面取得了长足的进展，旨在减轻管理员的工作负担。然而，目前尚不清楚这些任务的改进是否真正满足了管理员在工作中的需求。本文揭示了过去研究努力致力于为内容管理的各个方面提供自动化支持与志愿内容管理员的需求之间存在的差距，尤其是在识别违反各种管理规则方面。为此，我们在Hugging Face上对模型进行了调查，以揭示涵盖三个示范论坛的各种管理规则和指南的模型的可用性。我们进一步对最先进的LLM进行了测试，评估这些模型在标记某个特定论坛的平台规则违规方面的表现。最后，我们进行了用户调查研究。

    arXiv:2311.07879v2 Announce Type: replace-cross  Abstract: Extensive efforts in automated approaches for content moderation have been focused on developing models to identify toxic, offensive, and hateful content with the aim of lightening the load for moderators. Yet, it remains uncertain whether improvements on those tasks have truly addressed moderators' needs in accomplishing their work. In this paper, we surface gaps between past research efforts that have aimed to provide automation for aspects of content moderation and the needs of volunteer content moderators, regarding identifying violations of various moderation rules. To do so, we conduct a model review on Hugging Face to reveal the availability of models to cover various moderation rules and guidelines from three exemplar forums. We further put state-of-the-art LLMs to the test, evaluating how well these models perform in flagging violations of platform rules from one particular forum. Finally, we conduct a user survey stud
    
[^278]: ResMGCN：用于快速生物医学相互作用发现的残差消息图卷积网络

    ResMGCN: Residual Message Graph Convolution Network for Fast Biomedical Interactions Discovering

    [https://arxiv.org/abs/2311.07632](https://arxiv.org/abs/2311.07632)

    提出了一种新颖的Residual Message Graph Convolution Network（ResMGCN），用于快速而准确地预测生物医学相互作用。

    

    生物医学信息图对于现代生物医学领域中的生物医学信息相互作用的发现至关重要，比如多样化分子相互作用的识别和药物发现，在生物医学、生物信息学和人类健康社区中引起了越来越多的关注。目前，越来越多的图神经网络被提出来学习生物医学信息实体，并通过最先进的结果准确揭示生物医学分子相互作用。然而，这些方法虽然弥补了远距离特征的衰减，但却以冗余的内存和时间为代价。在我们的论文中，我们提出了一种新颖的Residual Message Graph Convolution Network（ResMGCN），用于以一种不同的思路快速而准确地预测生物医学相互作用。具体来说，ResMGCN不是增强远程节点的消息，而是与下一轮高阶信息相结合。

    arXiv:2311.07632v2 Announce Type: replace-cross  Abstract: Biomedical information graphs are crucial for interaction discovering of biomedical information in modern age, such as identification of multifarious molecular interactions and drug discovery, which attracts increasing interests in biomedicine, bioinformatics, and human healthcare communities. Nowadays, more and more graph neural networks have been proposed to learn the entities of biomedical information and precisely reveal biomedical molecule interactions with state-of-the-art results. These methods remedy the fading of features from a far distance but suffer from remedying such problem at the expensive cost of redundant memory and time. In our paper, we propose a novel Residual Message Graph Convolution Network (ResMGCN) for fast and precise biomedical interaction prediction in a different idea. Specifically, instead of enhancing the message from far nodes, ResMGCN aggregates lower-order information with the next round highe
    
[^279]: 通过正常结构规范化实现开放图异常检测

    Open-Set Graph Anomaly Detection via Normal Structure Regularisation

    [https://arxiv.org/abs/2311.06835](https://arxiv.org/abs/2311.06835)

    通过正常结构规范化方法，实现开放图异常检测模型对未知异常的广义检测能力

    

    本文考虑了一个重要的图异常检测（GAD）任务，即开放式GAD，旨在使用少量标记的训练正常节点和异常节点（称为已知异常）来检测异常节点，这些节点无法展示所有可能的推理时异常。已标记数据的可用性为GAD模型提供了关键的异常先验知识，可大大降低检测错误。然而，当前方法往往过分强调拟合已知异常，导致对未知异常（即未被标记的异常节点）的弱泛化能力。此外，它们被引入以处理欧几里德数据，未能有效捕捉GAD的重要非欧几里德特征。在这项工作中，我们提出了一种新颖的开放式GAD方法，即正常结构规范化（NSReg），以实现对未知异常的广义检测能力。

    arXiv:2311.06835v2 Announce Type: replace-cross  Abstract: This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to detect anomalous nodes using a small number of labelled training normal and anomaly nodes (known as seen anomalies) that cannot illustrate all possible inference-time abnormalities. The availability of that labelled data provides crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current methods tend to over-emphasise fitting the seen anomalies, leading to a weak generalisation ability to detect unseen anomalies, i.e., those that are not illustrated by the labelled anomaly nodes. Further, they were introduced to handle Euclidean data, failing to effectively capture important non-Euclidean features for GAD. In this work, we propose a novel open-set GAD approach, namely Normal Structure Regularisation (NSReg), to achieve generalised detection ability to unseen 
    
[^280]: 知识增强的大型语言模型用于个性化上下文查询建议

    Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion

    [https://arxiv.org/abs/2311.06318](https://arxiv.org/abs/2311.06318)

    提出一种新颖且通用的方法，通过从用户与搜索引擎的交互历史中提取相关上下文来个性化大型语言模型的输出，尤其适用于改进网络搜索体验。

    

    大型语言模型（LLMs）擅长解决各种自然语言任务。然而，由于重新训练或微调它们所涉及的成本巨大，它们仍然在很大程度上是静态的，并且难以个性化。尽管如此，许多应用程序可以从根据用户的偏好、目标和知识量定制的生成中受益。其中之一是网络搜索，了解用户试图做什么、关心什么以及他们知道什么可以提高搜索体验。在这项工作中，我们提出了一种新颖且通用的方法，该方法使用用户与搜索引擎的交互历史中的相关上下文来增强LLM以个性化其输出。具体而言，我们根据用户在网络上的搜索和浏览活动构建了每个用户的以实体为中心的知识存储，然后利用这些知识为LLM提供具有上下文相关性的提示增强。

    arXiv:2311.06318v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) excel at tackling various natural language tasks. However, due to the significant costs involved in re-training or fine-tuning them, they remain largely static and difficult to personalize. Nevertheless, a variety of applications could benefit from generations that are tailored to users' preferences, goals, and knowledge. Among them is web search, where knowing what a user is trying to accomplish, what they care about, and what they know can lead to improved search experiences. In this work, we propose a novel and general approach that augments an LLM with relevant context from users' interaction histories with a search engine in order to personalize its outputs. Specifically, we construct an entity-centric knowledge store for each user based on their search and browsing activities on the web, which is then leveraged to provide contextually relevant LLM prompt augmentations. This knowledge store is 
    
[^281]: 犹豫模糊集及其应用于多强度智能分类器的基础理论

    Foundational theories of hesitant fuzzy sets and hesitant fuzzy information systems and their applications for multi-strength intelligent classifiers

    [https://arxiv.org/abs/2311.04256](https://arxiv.org/abs/2311.04256)

    本文提出了基于犹豫模糊集的多种包含关系定义、犹豫模糊信息系统的基础命题和基于多强度智能分类器的健康状态诊断方法。

    

    犹豫模糊集在某些不确定和犹豫的情况下被广泛使用。在集合中，包含关系是一个重要且基础的定义。因此，作为一种集合，犹豫模糊集需要一个明确的包含关系定义。基于离散形式的犹豫模糊隶属度，本文提出了几种适用于犹豫模糊集的包含关系。随后，介绍了一些犹豫模糊集的基础命题，以及犹豫模糊集族的命题。针对参数减少，提出了犹豫模糊信息系统的一些基础命题，并给出了一个示例和算法来说明参数减少的过程。最后，提出了一种多强度智能分类器，用于对复杂系统进行健康状态诊断。

    arXiv:2311.04256v3 Announce Type: replace  Abstract: Hesitant fuzzy sets are widely used in certain instances of uncertainty and hesitation. In sets, the inclusion relationship is an important and foundational definition. Thus, as a kind of set, hesitant fuzzy sets require an explicit definition of inclusion relationship. Based on the hesitant fuzzy membership degree of discrete form, several kinds of inclusion relationships for hesitant fuzzy sets are proposed in this work. Then, some foundational propositions of hesitant fuzzy sets are presented, along with propositions of families of hesitant fuzzy sets. Some foundational propositions of hesitant fuzzy information systems are proposed with respect to parameter reductions and an example and an algorithm are given to illustrate the processes of parameter reduction. Finally, a multi-strength intelligent classifier is proposed to make health state diagnoses for complex systems.
    
[^282]: 使用LLMs的运动学感知提示实现对可移动物体的泛化操作

    Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs

    [https://arxiv.org/abs/2311.02847](https://arxiv.org/abs/2311.02847)

    基于物体的运动学结构，提出了一种运动学感知提示框架，用于生成LLMs的低级运动轨迹位点，以实现对可移动物体的泛化操作

    

    泛化的可移动物体操作对于家庭助手机器人至关重要。最近的研究主要集中在从演示中进行模仿学习或在模拟环境中进行强化学习，然而，由于实际数据收集和精确对象模拟成本高昂，这些工作仍然难以在多样的可移动物体上实现广泛的适应性。最近，许多研究尝试利用大型语言模型（LLMs）的强大上下文学习能力实现可泛化的机器人操作，但大多数研究侧重于高层任务规划，忽视了低层的机器人控制。在这项工作中，基于一个理念，即物体的运动学结构决定了我们如何操纵它，我们提出了一个运动学感知提示框架，用物体的运动学知识提示LLMs生成低层运动轨迹位点，支持

    arXiv:2311.02847v3 Announce Type: replace-cross  Abstract: Generalizable articulated object manipulation is essential for home-assistant robots. Recent efforts focus on imitation learning from demonstrations or reinforcement learning in simulation, however, due to the prohibitive costs of real-world data collection and precise object simulation, it still remains challenging for these works to achieve broad adaptability across diverse articulated objects. Recently, many works have tried to utilize the strong in-context learning ability of Large Language Models (LLMs) to achieve generalizable robotic manipulation, but most of these researches focus on high-level task planning, sidelining low-level robotic control. In this work, building on the idea that the kinematic structure of the object determines how we can manipulate it, we propose a kinematic-aware prompting framework that prompts LLMs with kinematic knowledge of objects to generate low-level motion trajectory waypoints, supportin
    
[^283]: DreamSmooth：通过奖励平滑改进基于模型的强化学习

    DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing

    [https://arxiv.org/abs/2311.01450](https://arxiv.org/abs/2311.01450)

    DreamSmooth通过学习预测时间平滑奖励而非精确奖励，优化了基于模型的强化学习，在稀疏奖励任务上表现出最先进的性能。

    

    摘要: 基于模型的强化学习（MBRL）因其以节约样本的方式学习复杂行为的能力而受到广泛关注：通过生成具有预测奖励的虚拟轨迹来规划动作。尽管取得了成功，但我们发现令人惊讶的是，奖励预测通常是MBRL的瓶颈，特别是对于难以预测的稀疏奖励。受到人类从粗糙奖励估计中学习的直觉启发，我们提出了一种简单而有效的奖励平滑方法DreamSmooth，它学习预测一个在给定时间步的奖励的时间平滑版本，而不是精确奖励。我们在实证上展示DreamSmooth在长视野稀疏奖励任务上取得了最先进的性能，既在样本效率和最终性能上，又不损失在常见基准测试上的性能，如Deepmind Control Suite和Atari基准测试。

    arXiv:2311.01450v2 Announce Type: replace-cross  Abstract: Model-based reinforcement learning (MBRL) has gained much attention for its ability to learn complex behaviors in a sample-efficient way: planning actions by generating imaginary trajectories with predicted rewards. Despite its success, we found that surprisingly, reward prediction is often a bottleneck of MBRL, especially for sparse rewards that are challenging (or even ambiguous) to predict. Motivated by the intuition that humans can learn from rough reward estimates, we propose a simple yet effective reward smoothing approach, DreamSmooth, which learns to predict a temporally-smoothed reward, instead of the exact reward at the given timestep. We empirically show that DreamSmooth achieves state-of-the-art performance on long-horizon sparse-reward tasks both in sample efficiency and final performance without losing performance on common benchmarks, such as Deepmind Control Suite and Atari benchmarks.
    
[^284]: 使用Syntheseus重新评估回溯合成算法

    Re-evaluating Retrosynthesis Algorithms with Syntheseus

    [https://arxiv.org/abs/2310.19796](https://arxiv.org/abs/2310.19796)

    使用Syntheseus建立的基准库重新评估了回溯合成算法，揭示了现有技术模型的系统性缺陷并提供了对未来工作的指导建议。

    

    过去几年，分子合成规划，也称为回溯合成，已经成为机器学习和化学界关注的焦点。尽管看似取得了稳定的进展，但我们认为存在不完善的基准和不一致的比较掩盖了现有技术的系统性缺陷。为了解决这个问题，我们提出了一个名为syntheseus的基准库，通过默认推广最佳实践，实现了对单步和多步回溯合成算法的一致而有意义的评估。我们使用syntheseus重新评估了若干先前的回溯合成算法，并发现在仔细评估时，现有技术模型的排名会发生变化。最后，我们给出了这一领域未来工作的指导建议。

    arXiv:2310.19796v2 Announce Type: replace-cross  Abstract: The planning of how to synthesize molecules, also known as retrosynthesis, has been a growing focus of the machine learning and chemistry communities in recent years. Despite the appearance of steady progress, we argue that imperfect benchmarks and inconsistent comparisons mask systematic shortcomings of existing techniques. To remedy this, we present a benchmarking library called syntheseus which promotes best practice by default, enabling consistent meaningful evaluation of single-step and multi-step retrosynthesis algorithms. We use syntheseus to re-evaluate a number of previous retrosynthesis algorithms, and find that the ranking of state-of-the-art models changes when evaluated carefully. We end with guidance for future works in this area.
    
[^285]: 文本到图像生成模型的特定提示中毒攻击

    Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models

    [https://arxiv.org/abs/2310.13828](https://arxiv.org/abs/2310.13828)

    本文展示了对文本到图像生成模型进行特定提示中毒攻击的成功，并介绍了Nightshade这种优化的中毒攻击方法，在视觉上与良性图像相同，可在少于100个毒样本中破坏稳定扩散SDXL提示。

    

    数据中毒攻击操纵训练数据，以在训练时将意外行为引入机器学习模型。在具有大规模训练数据集的文本到图像生成模型中，当前对中毒攻击的理解表明，成功的攻击需要将数百万个毒样本注入它们的训练管道。本文表明中毒攻击可以成功地应用于生成模型。我们观察到这些模型中每个概念的训练数据可能非常有限，使其容易受到特定提示中毒攻击的影响，这种攻击针对模型对个别提示作出响应的能力。我们介绍了Nightshade，一种针对特定提示的优化中毒攻击，其中毒样本在视觉上与具有匹配文本提示的良性图像看起来完全相同。Nightshade毒样本也经过了优化以进行有效攻击，并可以在<100个毒样本中损坏一个稳定扩散SDXL提示。

    arXiv:2310.13828v2 Announce Type: replace-cross  Abstract: Data poisoning attacks manipulate training data to introduce unexpected behaviors into machine learning models at training time. For text-to-image generative models with massive training datasets, current understanding of poisoning attacks suggests that a successful attack would require injecting millions of poison samples into their training pipeline. In this paper, we show that poisoning attacks can be successful on generative models. We observe that training data per concept can be quite limited in these models, making them vulnerable to prompt-specific poisoning attacks, which target a model's ability to respond to individual prompts.   We introduce Nightshade, an optimized prompt-specific poisoning attack where poison samples look visually identical to benign images with matching text prompts. Nightshade poison samples are also optimized for potency and can corrupt an Stable Diffusion SDXL prompt in <100 poison samples. Ni
    
[^286]: 关于常识推理的知识图谱解释的可信性

    Faithful Knowledge Graph Explanations for Commonsense Reasoning

    [https://arxiv.org/abs/2310.04910](https://arxiv.org/abs/2310.04910)

    本论文提出了两个量化指标来衡量基于知识图谱的解释的可信性，并引入了一种新的训练方法来改善解释的可信度。实验结果表明该方法可以提高解释的一致性和保真度。

    

    融合语言模型(LMs)和知识图谱(KGs)已成为常识问答研究中的常见方法，但在这些模型中实现精确的思路链解释仍然是一个未解决的问题。当前基于知识图谱的解释技术的一个主要弱点是在评估过程中忽视了生成解释的可信性。为了弥补这一差距，我们提出并验证了两个量化指标 - 图一致性和图保真度 - 来衡量基于知识图谱的解释的可信性。我们引入一种新的训练方法Consistent GNN (CGNN)，该方法添加了一项一致性正则化项来改善解释的可信度。我们的分析表明，KG的预测经常偏离原始模型的预测。所提出的CGNN方法提高了一致性和保真度，展示了它产生更可信解释的潜力。我们的工作强调了明确评估解释可信性的重要性。

    While fusing language models (LMs) and knowledge graphs (KGs) has become common in commonsense question answering research, enabling faithful chain-of-thought explanations in these models remains an open problem. One major weakness of current KG-based explanation techniques is that they overlook the faithfulness of generated explanations during evaluation. To address this gap, we make two main contributions: (1) We propose and validate two quantitative metrics - graph consistency and graph fidelity - to measure the faithfulness of KG-based explanations. (2) We introduce Consistent GNN (CGNN), a novel training method that adds a consistency regularization term to improve explanation faithfulness. Our analysis shows that predictions from KG often diverge from original model predictions. The proposed CGNN approach boosts consistency and fidelity, demonstrating its potential for producing more faithful explanations. Our work emphasises the importance of explicitly evaluating suggest a path
    
[^287]: CoDi: 条件扩散蒸馏，用于更高保真度和更快的图像生成

    CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation

    [https://arxiv.org/abs/2310.01407](https://arxiv.org/abs/2310.01407)

    CoDi引入了一种新的方法，通过适应预先训练的潜在扩散模型接受额外的图像条件输入，可以显著降低生成高质量结果所需的采样步骤。

    

    大型生成性扩散模型已经革新了文本到图像的生成，并为像图像增强、恢复、编辑和合成等条件生成任务提供了巨大潜力。然而，它们的广泛应用受到高计算成本的限制，这限制了它们在实时应用中的应用。为了解决这一挑战，我们引入了一种名为CoDi的新方法，它使一个预训练的潜在扩散模型能够接受额外的图像条件输入，同时显著减少了需要达到高质量结果所需的采样步骤。

    arXiv:2310.01407v2 Announce Type: replace-cross  Abstract: Large generative diffusion models have revolutionized text-to-image generation and offer immense potential for conditional generation tasks such as image enhancement, restoration, editing, and compositing. However, their widespread adoption is hindered by the high computational cost, which limits their real-time application. To address this challenge, we introduce a novel method dubbed CoDi, that adapts a pre-trained latent diffusion model to accept additional image conditioning inputs while significantly reducing the sampling steps required to achieve high-quality results. Our method can leverage architectures such as ControlNet to incorporate conditioning inputs without compromising the model's prior knowledge gained during large scale pre-training. Additionally, a conditional consistency loss enforces consistent predictions across diffusion steps, effectively compelling the model to generate high-quality images with conditio
    
[^288]: 赋能众多，偏袒少数：通过大型语言模型实现通用信用评分

    Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models

    [https://arxiv.org/abs/2310.00566](https://arxiv.org/abs/2310.00566)

    大型语言模型在信用评分任务中表现出强大的通用能力，通过首个开源框架和CALM模型，可以帮助解决传统信用评分方法所面临的挑战，并同时解决潜在的偏见问题。

    

    在金融行业，信用评分是一个基础要素，塑造着个人和企业的信贷准入，决定着贷款条件。本研究认为，大型语言模型（LLMs）在信用评分任务中具有巨大潜力，能够强大地跨多个任务进行泛化。为了系统地探索LLMs在信用评分中的应用，我们提出了第一个开源全面框架。我们收集了一个涵盖9个数据集、1.4K样本的新型基准，专门用于信用评估，并对LLMs内潜在偏见进行了重要检查，同时提供了超过45K样本的新型指导调整数据。然后，我们通过指导调整提出了首个信贷与风险评估大型语言模型（CALM），以针对不同的微妙需求

    arXiv:2310.00566v3 Announce Type: replace-cross  Abstract: In the financial industry, credit scoring is a fundamental element, shaping access to credit and determining the terms of loans for individuals and businesses alike. Traditional credit scoring methods, however, often grapple with challenges such as narrow knowledge scope and isolated evaluation of credit tasks. Our work posits that Large Language Models (LLMs) have great potential for credit scoring tasks, with strong generalization ability across multiple tasks. To systematically explore LLMs for credit scoring, we propose the first open-source comprehensive framework. We curate a novel benchmark covering 9 datasets with 14K samples, tailored for credit assessment and a critical examination of potential biases within LLMs, and the novel instruction tuning data with over 45k samples. We then propose the first Credit and Risk Assessment Large Language Model (CALM) by instruction tuning, tailored to the nuanced demands of various
    
[^289]: 从语言建模到指令跟随：理解指令调整后LLMs中行为的转变

    From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning

    [https://arxiv.org/abs/2310.00492](https://arxiv.org/abs/2310.00492)

    指令调整对LLMs产生了三个重要影响：1）使其能够识别用户提示中的指令部分；2）促进响应生成的不断调整

    

    大型语言模型（LLMs）已经取得了显著的成功，其中指令调整是将LLMs与用户意图对齐的关键步骤。在这项工作中，我们研究了指令调整如何调整经过预训练的模型，重点关注内在变化。具体来说，我们首先开发了几种本地和全局解释方法，包括一种基于梯度的输入输出归因方法，以及用于解释自注意力和前馈层中的模式和概念的技术。然后通过比较从预训练和指令调整模型中得出的解释来研究指令调整的影响。这种方法在人可理解的水平上提供了模型转变的内部视角。我们的研究发现了指令调整的三个重要影响：1）它使LLMs能够识别用户提示中的指令部分，并不断促进响应生成

    arXiv:2310.00492v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have achieved remarkable success, where instruction tuning is the critical step in aligning LLMs with user intentions. In this work, we investigate how the instruction tuning adjusts pre-trained models with a focus on intrinsic changes. Specifically, we first develop several local and global explanation methods, including a gradient-based method for input-output attribution and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. The impact of instruction tuning is then studied by comparing the explanations derived from the pre-trained and instruction-tuned models. This approach provides an internal perspective of the model shifts on a human-comprehensible level. Our findings reveal three significant impacts of instruction tuning: 1) It empowers LLMs to recognize the instruction parts from user prompts, and promotes the response generation constantly condition
    
[^290]: 场景通知者：基于锚点的遮挡推断和轨迹预测在部分可观测环境中

    Scene Informer: Anchor-based Occlusion Inference and Trajectory Prediction in Partially Observable Environments

    [https://arxiv.org/abs/2309.13893](https://arxiv.org/abs/2309.13893)

    场景通知者是一种统一方法，用于在部分可观察的环境中预测观察代理的轨迹和推断遮挡，其利用transformer聚合输入模态并实现对可能与自主车辆计划路径相交的遮挡的选择性查询。

    

    在复杂和动态的环境中自主车辆（AVs）导航需要AVs推理可见和遮挡区域。这涉及预测观察代理的未来运动，推断被遮挡的代理，并根据部分可观测环境的矢量化场景表示建模它们的相互作用。然而，在遮挡推断和轨迹预测方面的先前工作是分别发展的，前者基于简化的光栅方法，后者假定完整的环境可观察性。我们引入了场景通知者，这是一种统一的方法，用于在部分可观察的情况下预测观察代理的轨迹和推断遮挡。它使用一个transformer来聚合各种输入模态，并促进对可能与AV计划路径相交的遮挡的选择性查询。该框架估计了遮挡概率和可能的遮挡轨迹。

    arXiv:2309.13893v2 Announce Type: replace-cross  Abstract: Navigating complex and dynamic environments requires autonomous vehicles (AVs) to reason about both visible and occluded regions. This involves predicting the future motion of observed agents, inferring occluded ones, and modeling their interactions based on vectorized scene representations of the partially observable environment. However, prior work on occlusion inference and trajectory prediction have developed in isolation, with the former based on simplified rasterized methods and the latter assuming full environment observability. We introduce the Scene Informer, a unified approach for predicting both observed agent trajectories and inferring occlusions in a partially observable setting. It uses a transformer to aggregate various input modalities and facilitate selective queries on occlusions that might intersect with the AV's planned path. The framework estimates occupancy probabilities and likely trajectories for occlusi
    
[^291]: ChatEDA：基于大型语言模型的自主代理用于EDA

    ChatEDA: A Large Language Model Powered Autonomous Agent for EDA

    [https://arxiv.org/abs/2308.10204](https://arxiv.org/abs/2308.10204)

    该研究介绍了ChatEDA，一个由大型语言模型AutoMage赋能的EDA自主代理，通过有效管理任务计划、脚本生成和任务执行，简化了从RTL到GDSII的设计流程，并证明了其性能优越性。

    

    arXiv:2308.10204v2 公告类型：替换-交叉摘要：集成一系列复杂的电子设计自动化（EDA）工具以增强互操作性是电路设计者关注的重要问题。大型语言模型（LLMs）的最新进展展示了它们在自然语言处理和理解方面的卓越能力，提供了一种新颖的与EDA工具接口的方法。本研究论文介绍了ChatEDA，一个由大型语言模型AutoMage赋能的EDA自主代理，结合作为执行器的EDA工具。ChatEDA通过有效管理任务计划、脚本生成和任务执行，简化了从寄存器传输级（RTL）到图形数据系统第二版（GDSII）的设计流程。通过全面的实验评估，ChatEDA已经证明了其处理各种需求的能力，我们经过精心调优的AutoMage模型在性能上表现出优越性，相较于GPT-4和其他类似的模型。

    arXiv:2308.10204v2 Announce Type: replace-cross  Abstract: The integration of a complex set of Electronic Design Automation (EDA) tools to enhance interoperability is a critical concern for circuit designers. Recent advancements in large language models (LLMs) have showcased their exceptional capabilities in natural language processing and comprehension, offering a novel approach to interfacing with EDA tools. This research paper introduces ChatEDA, an autonomous agent for EDA empowered by a large language model, AutoMage, complemented by EDA tools serving as executors. ChatEDA streamlines the design flow from the Register-Transfer Level (RTL) to the Graphic Data System Version II (GDSII) by effectively managing task planning, script generation, and task execution. Through comprehensive experimental evaluations, ChatEDA has demonstrated its proficiency in handling diverse requirements, and our fine-tuned AutoMage model has exhibited superior performance compared to GPT-4 and other simi
    
[^292]: OctoPack: 使用指令调整代码的大规模语言模型

    OctoPack: Instruction Tuning Code Large Language Models

    [https://arxiv.org/abs/2308.07124](https://arxiv.org/abs/2308.07124)

    通过结合Git提交的自然结构，将代码更改与人类指令配对，我们提出了OctoPack，并在大规模语言模型上实现了表现最佳的指令调整方法。

    

    在指令上进行大规模语言模型（LLMs）的微调可显著提高自然语言任务的性能。我们应用代码进行指令调整，利用Git提交的自然结构，将代码更改与人类指令配对。我们编译了CommitPack：跨350种编程语言的4TB Git提交。我们在拥有16B参数的StarCoder模型上对比CommitPack和其他自然与合成代码指令（xP3x, Self-Instruct, OASST），在HumanEval Python基准测试（46.2% pass@1）中取得了未在OpenAI输出上训练的模型中的最新性能。我们进一步推出HumanEvalPack，将HumanEval基准测试扩展到共计3个编码任务（代码修复、代码解释、代码合成）跨6种语言（Python、JavaScript、Java、Go、C ++、Rust）。我们的模型OctoCoder和OctoGeeX在HumanEvalPack中取得了所有许可模型中的最佳性能。

    arXiv:2308.07124v2 Announce Type: replace-cross  Abstract: Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive m
    
[^293]: 用大型语言模型模块化地构建协作体现智能体

    Building Cooperative Embodied Agents Modularly with Large Language Models

    [https://arxiv.org/abs/2307.02485](https://arxiv.org/abs/2307.02485)

    利用大型语言模型构建模块化的协作体现智能体，实现多智能体合作解决具有挑战性的任务，超越规划方法并展示有效沟通。

    

    在这项工作中，我们处理具有去中心化控制、原始感知观察、昂贵通讯和多目标任务的具有各种体现环境的具有挑战性的多智能体合作问题。与先前研究不同的是，我们利用大型语言模型的常识知识、推理能力、语言理解和文本生成能力，并将它们无缝地融入到一个与感知、记忆和执行相结合的认知启发式模块化框架中。从而构建了一个可以规划、沟通和与其他人合作以高效完成长时程任务的合作体现智能体CoELA。我们在C-WAH和TDW-MAT上的实验表明，由GPT-4驱动的CoELA可以超越强大的基于规划的方法，并展示出新兴的有效沟通。

    arXiv:2307.02485v2 Announce Type: replace  Abstract: In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current O
    
[^294]: 迈向基于常识的推理

    Toward Grounded Commonsense Reasoning

    [https://arxiv.org/abs/2306.08651](https://arxiv.org/abs/2306.08651)

    提出了一种利用大型语言模型和视觉语言模型帮助机器人积极感知环境，进行基于常识的推理的方法。

    

    考虑一个被交付整理桌子上精心构建的乐高跑车的机器人。人类可能认识到，拆开跑车并将其放回作为“整理”的一部分是不合适的。一个机器人如何得出这个结论呢？尽管大型语言模型(LLMs)最近被用于实现常识推理，但将这种推理落实到现实世界中一直是具有挑战性的。为了在现实世界中进行推理，机器人必须超越被动查询LLMs，积极地从环境中收集必要的信息来做出正确的决策。例如，在检测到有一个被遮挡的汽车后，机器人可能需要主动感知汽车，以了解它是由乐高制作的高级型号汽车，还是由幼儿制作的玩具汽车。我们提出了一种方法，利用一个LLM和视觉语言模型(VLM)来帮助机器人主动感知其环境以进行基于事实的推理。

    arXiv:2306.08651v2 Announce Type: replace-cross  Abstract: Consider a robot tasked with tidying a desk with a meticulously constructed Lego sports car. A human may recognize that it is not appropriate to disassemble the sports car and put it away as part of the "tidying." How can a robot reach that conclusion? Although large language models (LLMs) have recently been used to enable commonsense reasoning, grounding this reasoning in the real world has been challenging. To reason in the real world, robots must go beyond passively querying LLMs and actively gather information from the environment that is required to make the right decision. For instance, after detecting that there is an occluded car, the robot may need to actively perceive the car to know whether it is an advanced model car made out of Legos or a toy car built by a toddler. We propose an approach that leverages an LLM and vision language model (VLM) to help a robot actively perceive its environment to perform grounded comm
    
[^295]: 分解扩散采样器用于加速大规模逆问题

    Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems

    [https://arxiv.org/abs/2303.05754](https://arxiv.org/abs/2303.05754)

    提出了一种将扩散采样和Krylov子空间方法协同结合的新型高效采样策略。

    

    Krylov子空间是通过将给定向量与线性变换矩阵及其连续幂相乘而生成的，广泛研究的经典优化文献中利用Krylov子空间设计算法以快速收敛大规模线性逆问题。本研究提出了一种新颖高效的扩散采样策略，将扩散采样与Krylov子空间方法协同结合起来。

    arXiv:2303.05754v3 Announce Type: replace-cross  Abstract: Krylov subspace, which is generated by multiplying a given vector by the matrix of a linear transformation and its successive powers, has been extensively studied in classical optimization literature to design algorithms that converge quickly for large linear inverse problems. For example, the conjugate gradient method (CG), one of the most popular Krylov subspace methods, is based on the idea of minimizing the residual error in the Krylov subspace. However, with the recent advancement of high-performance diffusion solvers for inverse problems, it is not clear how classical wisdom can be synergistically combined with modern diffusion models. In this study, we propose a novel and efficient diffusion sampling strategy that synergistically combines the diffusion sampling and Krylov subspace methods. Specifically, we prove that if the tangent space at a denoised sample by Tweedie's formula forms a Krylov subspace, then the CG initi
    
[^296]: 带有分布式量化流的GFlowNets

    Distributional GFlowNets with Quantile Flows

    [https://arxiv.org/abs/2302.05793](https://arxiv.org/abs/2302.05793)

    本文提出了一种带分布式量化流的GFlowNets模型，通过将流函数转化为分布，在训练过程中提供更多信息的学习信号。通过量化函数参数化每个边流，我们提出的算法可以学习风险敏感的策略，实现对风险不确定性场景的处理，并在现有基准上取得了显著改进。

    

    生成式流网络（GFlowNets）是一种新的概率采样器系列，其中代理通过一系列决策步骤学习生成复杂组合结构的随机策略。尽管受强化学习启发，当前的GFlowNet框架在适用性上相对有限，无法处理奖励函数中的随机性。在这项工作中，我们采用分布式范式来处理GFlowNets，将每个流函数转化为一个分布，从而在训练过程中提供更多信息的学习信号。通过通过量化函数对每个边流进行参数化，我们提出的“量化匹配” GFlowNet学习算法能够学习风险敏感的策略，这是处理风险不确定性场景的基本组成部分。此外，我们发现与之前的方法相比，分布式方法由于我们增强的训练算法，可以在现有基准上实现显着改进。

    Generative Flow Networks (GFlowNets) are a new family of probabilistic samplers where an agent learns a stochastic policy for generating complex combinatorial structure through a series of decision-making steps. Despite being inspired from reinforcement learning, the current GFlowNet framework is relatively limited in its applicability and cannot handle stochasticity in the reward function. In this work, we adopt a distributional paradigm for GFlowNets, turning each flow function into a distribution, thus providing more informative learning signals during training. By parameterizing each edge flow through their quantile functions, our proposed \textit{quantile matching} GFlowNet learning algorithm is able to learn a risk-sensitive policy, an essential component for handling scenarios with risk uncertainty. Moreover, we find that the distributional approach can achieve substantial improvement on existing benchmarks compared to prior methods due to our enhanced training algorithm, even i
    
[^297]: 在离散动力系统中找到非平凡的最小不动点

    Finding Nontrivial Minimum Fixed Points in Discrete Dynamical Systems

    [https://arxiv.org/abs/2301.04090](https://arxiv.org/abs/2301.04090)

    在离散动力系统中，我们提出了一个优化问题，即寻找具有最小受影响节点数量的系统非平凡不动点，并发现了一些特殊情况下可以高效解决这个问题。

    

    网络离散动力系统常用于模拟传染病的传播和决策协调游戏中的代理。这种动力系统的不动点代表系统收敛到的配置。在传播不良传染病（如谣言和错误信息）方面，收敛到受影响节点数量较少的不动点是一个值得追求的目标。受这些考虑的启发，我们提出了一个新颖的优化问题，即寻找具有最小受影响节点数量的系统非平凡不动点。我们确定，除非P = NP，否则没有多项式时间算法可以在任意小于 n^1-\epsilon 的因子内近似求解这个问题。为了应对这种计算上的难题，我们确定了几种特殊情况，可以高效地解决这个问题。此外，我们引入了一个整数线性规划来解决这个问题。

    arXiv:2301.04090v4 Announce Type: replace-cross  Abstract: Networked discrete dynamical systems are often used to model the spread of contagions and decision-making by agents in coordination games. Fixed points of such dynamical systems represent configurations to which the system converges. In the dissemination of undesirable contagions (such as rumors and misinformation), convergence to fixed points with a small number of affected nodes is a desirable goal. Motivated by such considerations, we formulate a novel optimization problem of finding a nontrivial fixed point of the system with the minimum number of affected nodes. We establish that, unless P = NP, there is no polynomial time algorithm for approximating a solution to this problem to within the factor n^1-\epsilon for any constant epsilon > 0. To cope with this computational intractability, we identify several special cases for which the problem can be solved efficiently. Further, we introduce an integer linear program to addr
    
[^298]: 在视觉流形上的运动规划

    Motion Planning on Visual Manifolds

    [https://arxiv.org/abs/2210.04047](https://arxiv.org/abs/2210.04047)

    提出了一种视觉配置空间（VCS）的替代性刻画，使具有身体结构的实体代理能够使用自己的图像集在随机姿势下发现自己的身体结构，并在周边空间中规划无障碍运动。

    

    在这篇论文中，我们提出了对配置空间概念的替代性刻画，我们称之为视觉配置空间（VCS）。这种新的描述允许一个具有身体结构的实体代理（如机器人）使用其随机姿势的一组图像来发现自己的身体结构，并在其周边空间中规划无障碍运动。这里，我们不假设实体代理、障碍物或环境的几何知识。我们展示了VCS 在（a）建立和处理几何无关模型用于机器人运动规划，（b）解释人类婴儿如何通过运动胡言来学习抓取其周边空间中的物体，以及（c）自动生成虚拟环境中数字化化身人物的自然头部动作动画的实用性。这项工作基于流形的形式化和流形学习，使用代理的图像，因此我们将其称为在视觉流形上的运动规划。

    arXiv:2210.04047v2 Announce Type: replace-cross  Abstract: In this thesis, we propose an alternative characterization of the notion of Configuration Space, which we call Visual Configuration Space (VCS). This new characterization allows an embodied agent (e.g., a robot) to discover its own body structure and plan obstacle-free motions in its peripersonal space using a set of its own images in random poses. Here, we do not assume any knowledge of geometry of the agent, obstacles or the environment. We demonstrate the usefulness of VCS in (a) building and working with geometry-free models for robot motion planning, (b) explaining how a human baby might learn to reach objects in its peripersonal space through motor babbling, and (c) automatically generating natural looking head motion animations for digital avatars in virtual environments. This work is based on the formalism of manifolds and manifold learning using the agent's images and hence we call it Motion Planning on Visual Manifold
    
[^299]: 从已记录数据中进行半监督批量学习

    Semi-supervised Batch Learning From Logged Data

    [https://arxiv.org/abs/2209.07148](https://arxiv.org/abs/2209.07148)

    本研究基于反事实风险最小化框架提出了一种半监督批量学习方法，解决了在已记录数据中反馈缺失的问题，提出了一个新的上界来处理这种学习问题。

    

    异策略学习方法旨在从已记录数据中学习策略，该数据包括每个样本点的环境、动作和反馈（成本或奖励）。在这项工作中，我们基于反事实风险最小化框架，该框架还假设能够访问概率得分。我们提出了针对一些样本缺失反馈的问题提出的学习方法，因此在已记录数据中有些样本有反馈，有些样本缺失反馈。我们将这种类型的学习称为从已记录数据中的半监督批量学习，这在广泛的应用领域中出现。为了解决这种学习问题，我们推导出了真实风险的新上界，采用倒数概率得分估计器。利用这个上界，我们提出了一种带有已记录数据的正则化半监督批量学习方法，其中正则化项与反馈无关，结果可以使用已记录的缺失反馈进行评估。

    arXiv:2209.07148v3 Announce Type: replace-cross  Abstract: Off-policy learning methods are intended to learn a policy from logged data, which includes context, action, and feedback (cost or reward) for each sample point. In this work, we build on the counterfactual risk minimization framework, which also assumes access to propensity scores. We propose learning methods for problems where feedback is missing for some samples, so there are samples with feedback and samples missing-feedback in the logged data. We refer to this type of learning as semi-supervised batch learning from logged data, which arises in a wide range of application domains. We derive a novel upper bound for the true risk under the inverse propensity score estimator to address this kind of learning problem. Using this bound, we propose a regularized semi-supervised batch learning method with logged data where the regularization term is feedback-independent and, as a result, can be evaluated using the logged missing-fe
    
[^300]: 学习进度驱动的多智能体课程

    Learning Progress Driven Multi-Agent Curriculum

    [https://arxiv.org/abs/2205.10016](https://arxiv.org/abs/2205.10016)

    提出了自主式MARL（SPMARL）以解决当前多智能体强化学习中课程生成的问题，优先考虑基于任务的优先级。

    

    课程强化学习（CRL）旨在通过逐渐增加任务的难度（通常由可实现的预期回报量化）来加快学习速度。受CRL在单智能体环境中的成功启发，一些研究尝试将CRL应用于多智能体强化学习（MARL），使用智能体数量来控制任务难度。然而，现有的工作通常使用手动定义的课程，如线性方案。本文首先将最先进的单智能体自主式CRL应用于稀疏奖励MARL。虽然表现令人满意，但我们确定了现有基于奖励的CRL方法生成的课程存在两个潜在缺陷：（1）高回报的任务可能不提供信息量大的学习信号，（2）在多智能体产生更高回报的任务中，加剧了学分分配困难。因此，我们进一步提出了自主式MARL（SPMARL），以基于任务的优先级进行安排。

    arXiv:2205.10016v2 Announce Type: replace  Abstract: Curriculum reinforcement learning (CRL) aims to speed up learning by gradually increasing the difficulty of a task, usually quantified by the achievable expected return. Inspired by the success of CRL in single-agent settings, a few works have attempted to apply CRL to multi-agent reinforcement learning (MARL) using the number of agents to control task difficulty. However, existing works typically use manually defined curricula such as a linear scheme. In this paper, we first apply state-of-the-art single-agent self-paced CRL to sparse reward MARL. Although with satisfying performance, we identify two potential flaws of the curriculum generated by existing reward-based CRL methods: (1) tasks with high returns may not provide informative learning signals and (2) the exacerbated credit assignment difficulty in tasks where more agents yield higher returns. Thereby, we further propose self-paced MARL (SPMARL) to prioritize tasks based on
    
[^301]: 基于方差减少的经验回放用于策略优化

    Variance Reduction Based Experience Replay for Policy Optimization

    [https://arxiv.org/abs/2110.08902](https://arxiv.org/abs/2110.08902)

    引入了基于方差减少的经验回放框架，实现了选择性重复利用相关样本来改善策略梯度估计，并构建了高效的离策略算法PG-VRER。

    

    在复杂随机系统上进行强化学习时，有效利用历史样本中的信息以加速策略优化是很有必要的。传统的经验回放虽然有效，但是将所有观测都视为相同，忽略了它们的相对重要性。为了解决这一限制，我们引入了一种新颖的方差减少经验回放（VRER）框架，实现对相关样本的选择性重复利用，从而改善策略梯度估计。VRER作为一种适应性方法，可以无缝集成到不同的策略优化算法中，构建了我们高效的离策略算法Policy Optimization with VRER (PG-VRER)。此外，文献中对经验回放方法缺乏严格的理论理解，这促使我们引入一个新颖的理论框架，考虑样本依赖性。

    arXiv:2110.08902v3 Announce Type: replace-cross  Abstract: For reinforcement learning on complex stochastic systems, it is desirable to effectively leverage the information from historical samples collected in previous iterations to accelerate policy optimization. Classical experience replay, while effective, treats all observations uniformly, neglecting their relative importance. To address this limitation, we introduce a novel Variance Reduction Experience Replay (VRER) framework, enabling the selective reuse of relevant samples to improve policy gradient estimation. VRER, as an adaptable method that can seamlessly integrate with different policy optimization algorithms, forms the foundation of our sample-efficient off-policy algorithm known as Policy Optimization with VRER (PG-VRER). Furthermore, the lack of a rigorous theoretical understanding of the experience replay method in the literature motivates us to introduce a novel theoretical framework that accounts for sample dependenc
    
[^302]: HCR-Net：基于深度学习的脱机手写字符识别网络

    HCR-Net: A deep learning based script independent handwritten character recognition network

    [https://arxiv.org/abs/2108.06663](https://arxiv.org/abs/2108.06663)

    HCR-Net提出了一种基于迁移学习的脱机手写字符识别网络，通过部分利用预训练网络的特征提取层，实现了更快、更高效的训练，更好的性能和泛化能力。

    

    尽管经过几十年的研究，手写字符识别（HCR）仍然是一个具有挑战性的模式识别问题，并且缺乏脱机识别技术的研究。这主要是由于相似的字符结构、不同的手写风格、不同的书写系统、手工特征提取技术、数据和代码的不可用性，以及脚本特定的深度学习技术的发展。为了解决这些限制，我们提出了一个名为HCR-Net的脱机深度学习网络，为HCR研究开辟了新的研究方向。HCR-Net基于一种新颖的用于HCR的迁移学习方法，\textit{部分利用}了预训练网络的特征提取层。由于迁移学习和图像增强，HCR-Net提供了更快、更高效的训练，更好的性能和泛化能力，并且可以处理少量数据...

    arXiv:2108.06663v4 Announce Type: replace-cross  Abstract: Handwritten character recognition (HCR) remains a challenging pattern recognition problem despite decades of research, and lacks research on script independent recognition techniques. {\color{black}This is mainly because of similar character structures, different handwriting styles, diverse scripts, handcrafted feature extraction techniques, unavailability of data and code, and the development of script-specific deep learning techniques. To address these limitations, we have proposed a script independent deep learning network for HCR research, called HCR-Net, that sets a new research direction for the field. HCR-Net is based on a novel transfer learning approach for HCR, which \textit{partly utilizes} feature extraction layers of a pre-trained network.} Due to transfer learning and image augmentation, HCR-Net provides faster and computationally efficient training, better performance and generalizations, and can work with small 
    
[^303]: 合作逆强化学习

    Cooperative Inverse Reinforcement Learning

    [https://arxiv.org/abs/1606.03137](https://arxiv.org/abs/1606.03137)

    提出了合作逆强化学习(CIRL)的定义，将价值对齐问题转化为机器人与人类之间的合作部分信息博弈，通过积极教学、积极学习和沟通行为等方式实现最大化价值对齐。

    

    为了使自主系统对人类有所帮助且不带来不必要的风险，它需要与环境中的人类价值观保持一致，使其行动有助于最大化人类的价值。本文将价值对齐问题形式化为合作逆强化学习(Cooperative Inverse Reinforcement Learning, CIRL)。CIRL问题是一个合作的、部分信息的博弈，有两个参与者：人类和机器人；两者根据人类的奖励函数获得奖励，但机器人最初并不知道这个函数是什么。与经典的逆强化学习相比，在那里假设人类是在孤立状态下行事最优的，最优的CIRL解决方案产生诸如积极教学、积极学习和沟通行为等行为，这些行为更有效地实现了价值对齐。我们展示了在CIRL游戏中计算最优联合策略可以简化为解决部分观测马尔可夫决策过程(POMDP)，证明了在孤立状态下的最优性。

    arXiv:1606.03137v4 Announce Type: replace  Abstract: For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is 
    
[^304]: 发挥专业放射科医生的专长，提升放射学报告的LLM评估

    Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports. (arXiv:2401.16578v1 [cs.CL])

    [http://arxiv.org/abs/2401.16578](http://arxiv.org/abs/2401.16578)

    该论文提出了一种方法，将专业放射科医生的专业知识与大型语言模型相结合，来提升自动生成报告的自动评估。实验结果显示，该方法的模型在评估中表现优于传统的度量标准。

    

    在放射学领域，人工智能（AI）已经大大推进了报告生成，但自动生成报告的自动评估仍然具有挑战性。目前的度量标准，如传统自然语言生成（NLG）和临床效能（CE），往往无法捕捉临床背景的语义复杂性，或者过分强调临床细节，降低了报告的清晰性。为了解决这些问题，我们提出的方法将专业放射科医生的专业知识与大型语言模型（LLMs），如GPT-3.5和GPT-4 1，相结合。利用上下文指导学习（ICIL）和思维链（CoT）推理，我们的方法使LLM的评估与放射科医生的标准保持一致，实现了人工智能生成报告与人类生成报告之间的详细比较。这进一步通过回归模型来综合句子评估分数。实验结果表明，我们的“详细GPT-4（5次训练）”模型获得了0.48的分数，优于METEOR指标。

    In radiology, Artificial Intelligence (AI) has significantly advanced report generation, but automatic evaluation of these AI-produced reports remains challenging. Current metrics, such as Conventional Natural Language Generation (NLG) and Clinical Efficacy (CE), often fall short in capturing the semantic intricacies of clinical contexts or overemphasize clinical details, undermining report clarity. To overcome these issues, our proposed method synergizes the expertise of professional radiologists with Large Language Models (LLMs), like GPT-3.5 and GPT-4 1. Utilizing In-Context Instruction Learning (ICIL) and Chain of Thought (CoT) reasoning, our approach aligns LLM evaluations with radiologist standards, enabling detailed comparisons between human and AI generated reports. This is further enhanced by a Regression model that aggregates sentence evaluation scores. Experimental results show that our ''Detailed GPT-4 (5-shot)'' model achieves a 0.48 score, outperforming the METEOR metric 
    
[^305]: 有效的可控偏差缓解方法，利用门适配器进行分类和检索。(arXiv:2401.16457v1 [cs.LG])

    Effective Controllable Bias Mitigation for Classification and Retrieval using Gate Adapters. (arXiv:2401.16457v1 [cs.LG])

    [http://arxiv.org/abs/2401.16457](http://arxiv.org/abs/2401.16457)

    本文引入了可控门适配器（ConGater），一种具有可调节敏感性参数的新颖模块化门机制，可在推理时逐渐过渡从模型的偏向状态到完全去偏的版本，并通过实验证明了其在分类和检索任务中的性能。

    

    语言模型的偏差缓解已经成为许多研究的主题，最近关注的焦点是学习独立的模块，例如适配器进行按需去偏。除了优化模块化去偏模型外，在实践中通常需要在推理时控制偏差减少的程度，例如，为了在搜索结果中调整期望的性能-公平性权衡或在分类任务中控制去偏的强度。在本文中，我们引入了可控门适配器（ConGater），一种具有可调节敏感性参数的新颖模块化门机制，允许在推理时从模型的偏向状态逐渐过渡到完全去偏的版本。通过在三个分类任务上对三个不同模型进行对抗性去偏实验，并通过公平性列表正则化来减少搜索结果的偏差，我们展示了ConGater的性能。

    Bias mitigation of Language Models has been the topic of many studies with a recent focus on learning separate modules like adapters for on-demand debiasing. Besides optimizing for a modularized debiased model, it is often critical in practice to control the degree of bias reduction at inference time, e.g., in order to tune for a desired performance-fairness trade-off in search results or to control the strength of debiasing in classification tasks. In this paper, we introduce Controllable Gate Adapter (ConGater), a novel modular gating mechanism with adjustable sensitivity parameters, which allows for a gradual transition from the biased state of the model to the fully debiased version at inference time. We demonstrate ConGater performance by (1) conducting adversarial debiasing experiments with three different models on three classification tasks with four protected attributes, and (2) reducing the bias of search results through fairness list-wise regularization to enable adjusting a
    
[^306]: 一个用于衡量人工智能依赖的统计框架

    A Statistical Framework for Measuring AI Reliance. (arXiv:2401.15356v1 [cs.AI])

    [http://arxiv.org/abs/2401.15356](http://arxiv.org/abs/2401.15356)

    该论文提出了一个基于统计决策理论的依赖的形式定义，用于衡量人工智能系统的适当依赖。该定义分离了依赖的概念和人类在形成准确信念时面临的挑战，为人类与人工智能互补性和依赖性的研究设计提供了指导。

    

    人类经常在人工智能系统的帮助下做决策。一个常见模式是人工智能向人类推荐行动，而人类保留对最终决策的控制权。研究人员已经确认，确保人类对人工智能的适当依赖是实现互补性能的关键组成部分。我们认为，目前在这方面的研究中使用的适当依赖的定义缺乏形式化的统计基础，可能会导致矛盾。我们提出了一个基于统计决策理论的依赖的形式定义，它将依赖的概念与人类在区分信号并形成准确信念的挑战分开。我们的定义产生了一个框架，可以用来指导人类与人工智能互补性和依赖性的研究设计和解释。利用最近的人工智能辅助决策研究...

    Humans frequently make decisions with the aid of artificially intelligent (AI) systems. A common pattern is for the AI to recommend an action to the human who retains control over the final decision. Researchers have identified ensuring that a human has appropriate reliance on an AI as a critical component of achieving complementary performance. We argue that the current definition of appropriate reliance used in such research lacks formal statistical grounding and can lead to contradictions. We propose a formal definition of reliance, based on statistical decision theory, which separates the concepts of reliance as the probability the decision-maker follows the AI's prediction from challenges a human may face in differentiating the signals and forming accurate beliefs about the situation. Our definition gives rise to a framework that can be used to guide the design and interpretation of studies on human-AI complementarity and reliance. Using recent AI-advised decision making studies f
    
[^307]: GeoDecoder: 强化多模态地图理解

    GeoDecoder: Empowering Multimodal Map Understanding. (arXiv:2401.15118v1 [cs.CV])

    [http://arxiv.org/abs/2401.15118](http://arxiv.org/abs/2401.15118)

    GeoDecoder是一种专门设计用于处理地图中地理空间信息的多模态模型，通过集成图像和文本处理模块，无缝集成外部数据和特征，以及执行多任务训练和执行，实现了强化地图认知的目标。

    

    本文提出了GeoDecoder，一种专门设计用于处理地图中地理空间信息的多模态模型。GeoDecoder基于BeitGPT架构构建，并集成了图像和文本处理的专业模块。在图像方面，GeoDecoder利用高德地图作为底图，该地图内置了道路和建筑形状、相对位置和其他属性的重要细节。通过渲染技术，该模型无缝集成了外部数据和特征，如符号标记、驾驶轨迹、热力图和用户定义的标记，消除了额外的特征工程需求。GeoDecoder的文本模块接受各种上下文文本和问题提示，并生成类似于GPT的文本输出。此外，基于GPT的模型允许在同一模型中进行多个任务的训练和执行。为了增强地图认知能力并使GeoDecoder获取知识

    This paper presents GeoDecoder, a dedicated multimodal model designed for processing geospatial information in maps. Built on the BeitGPT architecture, GeoDecoder incorporates specialized expert modules for image and text processing. On the image side, GeoDecoder utilizes GaoDe Amap as the underlying base map, which inherently encompasses essential details about road and building shapes, relative positions, and other attributes. Through the utilization of rendering techniques, the model seamlessly integrates external data and features such as symbol markers, drive trajectories, heatmaps, and user-defined markers, eliminating the need for extra feature engineering. The text module of GeoDecoder accepts various context texts and question prompts, generating text outputs in the style of GPT. Furthermore, the GPT-based model allows for the training and execution of multiple tasks within the same model in an end-to-end manner. To enhance map cognition and enable GeoDecoder to acquire knowle
    
[^308]: LLM指令微调中的提示权重实验

    Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])

    [http://arxiv.org/abs/2401.13586](http://arxiv.org/abs/2401.13586)

    LLM指令微调中，对于短提示完成数据集，提示词标记分类损失加权（PLW）与性能呈负二次关系，而长提示完成数据集则不受PLW影响。

    

    我们进行了一项小型研究，分析了提示词标记分类损失加权（PLW）如何影响在指令任务上进行微调的7B大小的LLaMA模型的性能。我们使用多个指令数据集重现了斯坦福大学的Alpaca实验，其中包括LLaMA 1和LLaMA 2。我们发现，在我们的短提示完成数据集上微调的模型与PLW之间存在负二次关系，而在长提示完成数据集上微调的模型不受PLW的影响。

    We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
    
[^309]: 使用欠阻尼 Langevin Monte Carlo 加速近似 Thompson 采样

    Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo. (arXiv:2401.11665v1 [stat.ML])

    [http://arxiv.org/abs/2401.11665](http://arxiv.org/abs/2401.11665)

    本文提出了一种使用欠阻尼 Langevin Monte Carlo 加速的近似 Thompson 采样策略，通过特定势函数的设计改善了高维问题中的样本复杂度，并在高维赌博机问题中进行了验证。

    

    使用欠阻尼 Langevin Monte Carlo 的近似 Thompson 采样方法扩展了其适用范围，从高斯后验采样扩展到更一般的平滑后验。然而，在高维问题中要求高准确性时，仍然面临可扩展性问题。为了解决这个问题，我们提出了一种近似 Thompson 采样策略，利用欠阻尼 Langevin Monte Carlo，后者是模拟高维后验的通用工具。基于标准的平滑性和对数凹性条件，我们研究了使用特定势函数的加速后验集中和采样。该设计改进了实现对数遗憾的样本复杂度，从$\mathcal{\tilde O}(d)$改进到$\mathcal{\tilde O}(\sqrt{d})$。我们还通过合成实验在高维赌博机问题中经验验证了我们算法的可扩展性和鲁棒性。

    Approximate Thompson sampling with Langevin Monte Carlo broadens its reach from Gaussian posterior sampling to encompass more general smooth posteriors. However, it still encounters scalability issues in high-dimensional problems when demanding high accuracy. To address this, we propose an approximate Thompson sampling strategy, utilizing underdamped Langevin Monte Carlo, where the latter is the go-to workhorse for simulations of high-dimensional posteriors. Based on the standard smoothness and log-concavity conditions, we study the accelerated posterior concentration and sampling using a specific potential function. This design improves the sample complexity for realizing logarithmic regrets from $\mathcal{\tilde O}(d)$ to $\mathcal{\tilde O}(\sqrt{d})$. The scalability and robustness of our algorithm are also empirically validated through synthetic experiments in high-dimensional bandit problems.
    
[^310]: R-Judge: 评估LLM代理的安全风险意识的基准测试

    R-Judge: Benchmarking Safety Risk Awareness for LLM Agents. (arXiv:2401.10019v1 [cs.CL])

    [http://arxiv.org/abs/2401.10019](http://arxiv.org/abs/2401.10019)

    这篇论文主要介绍了一种评估LLM代理在不同环境中判断安全风险能力的基准测试R-Judge，通过对162个代理交互记录进行评估，发现GPT-4模型表现最佳，达到了72.29%的准确率。

    

    大型语言模型（LLM）在自动完成各种真实世界应用任务方面展现出巨大潜力。然而，这些LLM代理在交互环境中操作时会引入意外的安全风险。与大多数之前的研究集中在LLM生成内容的安全性不同，本研究关注评估LLM代理在不同环境中的行为安全性的迫切需求。我们介绍了一个名为R-Judge的基准测试，用于评估LLM在给定代理交互记录时判断安全风险的能力。R-Judge包括162个代理交互记录，涵盖7个应用领域和10种风险类型的27个关键风险场景。它结合了人类对安全性的共识，并具有标记的安全风险标签和高质量的风险描述。利用R-Judge，我们对8种常用作代理骨干的著名LLM模型进行了全面评估。表现最好的模型GPT-4实现了72.29%的对比结果。

    Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on LLM-generated content safety in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging safety risks given agent interaction records. R-Judge comprises 162 agent interaction records, encompassing 27 key risk scenarios among 7 application categories and 10 risk types. It incorporates human consensus on safety with annotated safety risk labels and high-quality risk descriptions. Utilizing R-Judge, we conduct a comprehensive evaluation of 8 prominent LLMs commonly employed as the backbone for agents. The best-performing model, GPT-4, achieves 72.29% in contrast to
    
[^311]: 技术报告：关于节点不可访问情况下流言学习收敛性的研究

    Technical Report: On the Convergence of Gossip Learning in the Presence of Node Inaccessibility. (arXiv:2401.09498v1 [cs.LG])

    [http://arxiv.org/abs/2401.09498](http://arxiv.org/abs/2401.09498)

    本文研究了在动态网络拓扑下，不可访问节点对流言学习的收敛性的影响，并提供了理论分析。

    

    Gossip learning（GL）作为分散式学习的一种替代方法，更适用于资源受限的无线网络，如由无人机（UAV）组成的FANETs。GL能够显著提高UAV网络的效率并延长电池寿命。尽管具有这些优势，但GL的性能受数据分布、通信速度和网络连接性的影响较大。然而，这些因素如何影响GL的收敛性仍不清楚。现有研究基于虚拟数量来研究GL的收敛性，以方便性而忽略了当一些节点不可访问时网络的真实状态。在本文中，我们对动态网络拓扑下不可访问节点对GL的影响进行了建模和研究。首先，我们将权重发散分解为节点是否可访问的情况。然后，我们研究了在节点可访问性的动态下GL的收敛性，并在理论上提供了

    Gossip learning (GL), as a decentralized alternative to federated learning (FL), is more suitable for resource-constrained wireless networks, such as FANETs that are formed by unmanned aerial vehicles (UAVs). GL can significantly enhance the efficiency and extend the battery life of UAV networks. Despite the advantages, the performance of GL is strongly affected by data distribution, communication speed, and network connectivity. However, how these factors influence the GL convergence is still unclear. Existing work studied the convergence of GL based on a virtual quantity for the sake of convenience, which fail to reflect the real state of the network when some nodes are inaccessible. In this paper, we formulate and investigate the impact of inaccessible nodes to GL under a dynamic network topology. We first decompose the weight divergence by whether the node is accessible or not. Then, we investigate the GL convergence under the dynamic of node accessibility and theoretically provide
    
[^312]: 关于组织病理学图像搜索的研究

    On Image Search in Histopathology. (arXiv:2401.08699v1 [eess.IV])

    [http://arxiv.org/abs/2401.08699](http://arxiv.org/abs/2401.08699)

    这篇论文综述了组织病理学图像搜索技术的最新发展，为计算病理学研究人员提供了简明的概述，旨在寻求有效、快速和高效的图像搜索方法。

    

    组织病理学的病理图像可以通过装有摄像头的显微镜或全扫描仪获取。利用相似性计算基于这些图像匹配患者，在研究和临床环境中具有重要潜力。最近搜索技术的进展使得可以对各种组织类型的细胞结构进行微妙的量化，促进比较，并在与诊断和治疗过的病例数据库进行比较时实现关于诊断、预后和新患者预测的推断。本文全面回顾了组织病理学图像搜索技术的最新发展，为计算病理学研究人员提供了简明的概述，以寻求有效、快速和高效的图像搜索方法。

    Pathology images of histopathology can be acquired from camera-mounted microscopes or whole slide scanners. Utilizing similarity calculations to match patients based on these images holds significant potential in research and clinical contexts. Recent advancements in search technologies allow for nuanced quantification of cellular structures across diverse tissue types, facilitating comparisons and enabling inferences about diagnosis, prognosis, and predictions for new patients when compared against a curated database of diagnosed and treated cases. In this paper, we comprehensively review the latest developments in image search technologies for histopathology, offering a concise overview tailored for computational pathology researchers seeking effective, fast and efficient image search methods in their work.
    
[^313]: 人作为AI导师：增强人机协作强化学习以实现安全高效的自动驾驶

    Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])

    [http://arxiv.org/abs/2401.03160](http://arxiv.org/abs/2401.03160)

    本文提出了一种增强的人机协作强化学习方法，通过将人类智能注入到AI中实现混合交通编队中的安全高效自动驾驶。该方法将人类专家作为导师，允许代理在不确定环境中进行探索，同时在危险情况下接管控制以避免事故，并指导代理减小交通流干扰，优化交通流效果。

    

    尽管自动驾驶车辆（AVs）取得了重大进展，但确保AVs的安全性和交通流效率的驾驶策略的发展尚未得到充分探索。在本文中，我们提出了一种增强的人机协作强化学习方法，称为基于人作为AI导师的深度强化学习（HAIM-DRL）框架，以在混合交通编队中实现安全高效的自动驾驶。从人类学习过程中汲取灵感，我们首先引入了一种创新的学习范式，有效地将人类智能注入到AI中，称为人作为AI导师（HAIM）。在这个范式中，人类专家作为导师为AI代理提供帮助。在允许代理在不确定环境中进行充分探索的同时，人类专家可以在危险情况下接管控制，并展示正确的行动以避免潜在事故。另一方面，可以指导代理减小交通流干扰，从而优化交通流效果。

    Despite significant progress in autonomous vehicles (AVs), the development of driving policies that ensure both the safety of AVs and traffic flow efficiency has not yet been fully explored. In this paper, we propose an enhanced human-in-the-loop reinforcement learning method, termed the Human as AI mentor-based deep reinforcement learning (HAIM-DRL) framework, which facilitates safe and efficient autonomous driving in mixed traffic platoon. Drawing inspiration from the human learning process, we first introduce an innovative learning paradigm that effectively injects human intelligence into AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves as a mentor to the AI agent. While allowing the agent to sufficiently explore uncertain environments, the human expert can take control in dangerous situations and demonstrate correct actions to avoid potential accidents. On the other hand, the agent could be guided to minimize traffic flow disturbance, thereby optimizi
    
[^314]: GraphPro: 面向推荐系统的图预训练和提示学习

    GraphPro: Graph Pre-training and Prompt Learning for Recommendation. (arXiv:2311.16716v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2311.16716](http://arxiv.org/abs/2311.16716)

    GraphPro是一个结合了参数高效和动态图预训练与提示学习的框架，能够有效捕捉长期用户偏好和短期行为动态，从而在真实世界的推荐系统中提供准确和及时的推荐。

    

    基于GNN的推荐系统通过多次消息传递在建模复杂的用户-物品交互方面表现出色。然而，现有方法往往忽视了不断变化的用户-物品交互的动态性，这限制了其在适应用户偏好变化和新到达数据分布变化方面的可扩展性和性能。因此，它们在真实世界的动态环境中的可扩展性和性能受到了限制。在这项研究中，我们提出了GraphPro，这是一个将参数高效和动态图预训练与提示学习相结合的框架。这种新颖的组合能够有效捕捉长期用户偏好和短期行为动态，从而实现准确和及时的推荐。我们的GraphPro框架通过无缝集成临时提示机制和图结构提示学习机制到预训练的GNN模型中来解决用户偏好不断变化的挑战。

    GNN-based recommenders have excelled in modeling intricate user-item interactions through multi-hop message passing. However, existing methods often overlook the dynamic nature of evolving user-item interactions, which impedes the adaption to changing user preferences and distribution shifts in newly arriving data. Thus, their scalability and performances in real-world dynamic environments are limited. In this study, we propose GraphPro, a framework that incorporates parameter-efficient and dynamic graph pre-training with prompt learning. This novel combination empowers GNNs to effectively capture both long-term user preferences and short-term behavior dynamics, enabling the delivery of accurate and timely recommendations. Our GraphPro framework addresses the challenge of evolving user preferences by seamlessly integrating a temporal prompt mechanism and a graph-structural prompt learning mechanism into the pre-trained GNN model. The temporal prompt mechanism encodes time information o
    
[^315]: DRNet：基于深度强化学习的自动车道变换决策方法

    DRNet: A Decision-Making Method for Autonomous Lane Changingwith Deep Reinforcement Learning. (arXiv:2311.01602v1 [cs.RO])

    [http://arxiv.org/abs/2311.01602](http://arxiv.org/abs/2311.01602)

    DRNet是一种基于深度强化学习的决策框架，可以帮助自动驾驶车辆进行车道变换，并考虑到周围车辆的驾驶风格，实现安全的决策策略。

    

    机器学习技术在自动驾驶车辆的决策中表现出色。尽管近年来取得了一些进展，但由于复杂的驾驶场景和周围车辆的多变社交行为，车道变换仍然是一个主要挑战。为了改进现有技术，我们提出了DRNet，这是一种基于深度强化学习的全新、高效的框架，通过在模拟高速公路上执行合理的车道变换，并考虑周围车辆的驾驶风格，使深度强化学习代理能够学会驾驶。此外，为了实现安全的决策策略，DRNet结合了安全验证的思想，这是自动驾驶的最重要组成部分，确保在任何时刻只选择安全的行动。

    Machine learning techniques have outperformed numerous rule-based methods for decision-making in autonomous vehicles. Despite recent efforts, lane changing remains a major challenge, due to the complex driving scenarios and changeable social behaviors of surrounding vehicles. To help improve the state of the art, we propose to leveraging the emerging \underline{D}eep \underline{R}einforcement learning (DRL) approach for la\underline{NE} changing at the \underline{T}actical level. To this end, we present "DRNet", a novel and highly efficient DRL-based framework that enables a DRL agent to learn to drive by executing reasonable lane changing on simulated highways with an arbitrary number of lanes, and considering driving style of surrounding vehicles to make better decisions. Furthermore, to achieve a safe policy for decision-making, DRNet incorporates ideas from safety verification, the most important component of autonomous driving, to ensure that only safe actions are chosen at any ti
    
[^316]: 马尔可夫决策过程中超越平均回报

    Beyond Average Return in Markov Decision Processes. (arXiv:2310.20266v1 [cs.AI])

    [http://arxiv.org/abs/2310.20266](http://arxiv.org/abs/2310.20266)

    该论文研究了马尔可夫决策过程中超越平均回报的问题，总结了可以准确计算和优化的奖励函数的特征，并提供了针对这些特征的新规划方法。这些结果在马尔可夫决策过程的理论发展中具有重要意义。

    

    在马尔可夫决策过程中，哪些奖励函数可以被准确地计算和优化？在有限时间段、无折扣设置中，动态规划只能高效处理某些统计类别的操作。我们总结了这些类别在策略评估中的特征，并提供了规划问题的新解。有趣的是，我们证明，即使在分布强化学习（DistRL）的更一般框架中，只有广义平均值可以被准确优化。然而，DistRL允许近似评估其他函数。我们给出了结果估计器的误差界限，并讨论了此方法的潜力及其局限性。这些结果通过研究回报的整体特征，特别是风险意识的策略，为马尔可夫决策过程的理论发展做出了贡献。

    What are the functionals of the reward that can be computed and optimized exactly in Markov Decision Processes? In the finite-horizon, undiscounted setting, Dynamic Programming (DP) can only handle these operations efficiently for certain classes of statistics. We summarize the characterization of these classes for policy evaluation, and give a new answer for the planning problem. Interestingly, we prove that only generalized means can be optimized exactly, even in the more general framework of Distributional Reinforcement Learning (DistRL).DistRL permits, however, to evaluate other functionals approximately. We provide error bounds on the resulting estimators, and discuss the potential of this approach as well as its limitations.These results contribute to advancing the theory of Markov Decision Processes by examining overall characteristics of the return, and particularly risk-conscious strategies.
    
[^317]: 具有强化改写生成的对话问答模型的鲁棒训练

    Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v1 [cs.CL])

    [http://arxiv.org/abs/2310.13505](http://arxiv.org/abs/2310.13505)

    这项研究提出了一种新的框架REIGN，通过生成训练问题的改写，并使用深度强化学习来指导对话问答模型，增加模型对表面形式变化的鲁棒性，同时在不同的基准上进行零-shot应用。

    

    知识图谱（KG）上的对话问答（ConvQA）模型通常在黄金QA对的基准上进行训练和测试。这意味着训练仅限于在相应数据集中见到的表面形式，评估仅针对一小部分问题。通过我们的提出的框架REIGN，我们采取了几个步骤来解决这个受限的学习设置。首先，我们系统地生成训练问题的改写，以提高模型对表面形式变化的鲁棒性。这是一个特别具有挑战性的问题，因为这些问题的不完整性。其次，我们使用深度强化学习将ConvQA模型引导到更高的性能，只提供那些有助于提高回答质量的改写。第三，我们展示了在一个基准上训练主要模型组件并将其零-shot应用于另一个的可行性。最后，为了对训练模型的鲁棒性进行严格评估，我们使用和重新配置初始的改写、测试语料。

    Models for conversational question answering (ConvQA) over knowledge graphs (KGs) are usually trained and tested on benchmarks of gold QA pairs. This implies that training is limited to surface forms seen in the respective datasets, and evaluation is on a small set of held-out questions. Through our proposed framework REIGN, we take several steps to remedy this restricted learning setup. First, we systematically generate reformulations of training questions to increase robustness of models to surface form variations. This is a particularly challenging problem, given the incomplete nature of such questions. Second, we guide ConvQA models towards higher performance by feeding it only those reformulations that help improve their answering quality, using deep reinforcement learning. Third, we demonstrate the viability of training major model components on one benchmark and applying them zero-shot to another. Finally, for a rigorous evaluation of robustness for trained models, we use and re
    
[^318]: SalUn：通过基于梯度的权重显著性增强机器遗忘在图像分类和生成中的效果

    SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])

    [http://arxiv.org/abs/2310.12508](http://arxiv.org/abs/2310.12508)

    这篇论文提出了一种名为SalUn的机器遗忘方法，通过引入"权重显著性"的概念，将关注点从整个模型引导到具体的模型权重上，提高了遗忘的效果和效率。这是第一个能够有效消除遗忘数据、类别或概念影响的有原则的机器遗忘方法。

    

    随着数据法规的不断发展，机器遗忘（MU）已成为增强当前AI模型的信任和安全性的重要工具。然而，现有的MU方法通常在遗忘精度、稳定性和跨领域适用性方面存在局限。为了解决这些挑战，我们引入了MU中的“权重显著性”概念，借鉴了模型解释中的输入显著性。这一创新将MU的关注点从整个模型引导到了具体的模型权重上，提高了其效果和效率。我们称之为显著性遗忘（SalUn）的方法将其与“精确”遗忘（在删除遗忘数据集后从头开始重新训练模型）的性能差距缩小。据我们所知，SalUn是第一个能够在图像分类和生成中有效消除遗忘数据、类别或概念影响的有原则的MU方法。例如，SalUn可在图片分类和生成任务中擦除遗忘数据、类别或概念。

    With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, Sa
    
[^319]: DHOT-GM：使用可微分的分层最优传输框架实现鲁棒图匹配

    DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical Optimal Transport Framework. (arXiv:2310.12081v1 [cs.AI])

    [http://arxiv.org/abs/2310.12081](http://arxiv.org/abs/2310.12081)

    本研究提出了一种名为DHOT-GM的图匹配方法，使用可微分的分层最优传输框架，充分利用了图中隐藏的多模态信息，通过对匹配结果进行加权平均来推断节点对应关系。

    

    在实践中，图匹配是最重要的图分析任务之一，其目标是找到不同图之间的节点对应关系。大多数现有方法在匹配图时依赖于邻接矩阵或节点嵌入，其性能常常不够优越，因为没有充分利用图中隐藏的多模态信息，如节点属性、子图结构等。在本研究中，我们提出了一种基于可微分的分层最优传输（HOT）框架的新颖有效的图匹配方法，称为DHOT-GM。实质上，我们的方法将每个图表示为与不同模态信息对应的一组关系矩阵。给定两个图，我们枚举所有关系矩阵对，并获取它们的匹配结果，然后通过对匹配结果进行加权平均来推断节点对应关系。该方法可以实现为计算两个图之间的HOT距离，每个图都是由关系矩阵表示的。

    Graph matching is one of the most significant graph analytic tasks in practice, which aims to find the node correspondence across different graphs. Most existing approaches rely on adjacency matrices or node embeddings when matching graphs, whose performances are often sub-optimal because of not fully leveraging the multi-modal information hidden in graphs, such as node attributes, subgraph structures, etc. In this study, we propose a novel and effective graph matching method based on a differentiable hierarchical optimal transport (HOT) framework, called DHOT-GM. Essentially, our method represents each graph as a set of relational matrices corresponding to the information of different modalities. Given two graphs, we enumerate all relational matrix pairs and obtain their matching results, and accordingly, infer the node correspondence by the weighted averaging of the matching results. This method can be implemented as computing the HOT distance between the two graphs -- each matching 
    
[^320]: 面向隐私保护推荐的联邦异构图神经网络

    Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation. (arXiv:2310.11730v1 [cs.LG])

    [http://arxiv.org/abs/2310.11730](http://arxiv.org/abs/2310.11730)

    本文提出了一种联邦异构图神经网络（FedHGNN）的框架，能够在分布式的异构信息网络上协同训练推荐模型，同时保护用户隐私。

    

    异构信息网络（HIN）通过元路径描述丰富的语义，已成为缓解推荐系统数据稀疏性的强大工具。现有的基于HIN的推荐系统持有数据的集中存储假设，并进行集中式模型训练。然而，由于隐私问题，现实世界的数据往往以分布式方式存储，导致集中式HIN推荐无法实现。本文提出将HIN分为客户端存储的私有HIN和服务器端的共享HIN。在此设置下，我们提出了一种基于联邦异构图神经网络（FedHGNN）的框架，可以在分布式HIN上协作训练推荐模型，同时不泄露用户隐私。具体而言，我们首先针对基于HIN的联合推荐，基于差分隐私的光下确定了隐私定义，旨在保护私有HIN的用户-商品交互，以及用户的隐私信息。

    Heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has become a powerful tool to alleviate data sparsity in recommender systems. Existing HIN-based recommendations hold the data centralized storage assumption and conduct centralized model training. However, the real-world data is often stored in a distributed manner for privacy concerns, resulting in the failure of centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored in the client side and shared HINs in the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which can collaboratively train a recommendation model on distributed HINs without leaking user privacy. Specifically, we first formalize the privacy definition in the light of differential privacy for HIN-based federated recommendation, which aims to protect user-item interactions of private HIN as well as user's 
    
[^321]: 大型语言模型的去学习研究

    Large Language Model Unlearning. (arXiv:2310.10683v1 [cs.CL])

    [http://arxiv.org/abs/2310.10683](http://arxiv.org/abs/2310.10683)

    大型语言模型的去学习是一个研究的新领域，我们探索了三个场景，可以通过去学习让语言模型与人类偏好保持一致。去学习具有三个优势，只需要负面示例，计算效率高，特别对于知道具体导致不良行为的训练样本更为有效。

    

    我们研究了如何对大型语言模型（LLMs）进行去学习，即忘记不受欢迎的（非）行为。我们展示了至少三种情境可以从去学习中使LLMs与人类偏好保持一致：（1）删除有害回复，（2）按要求删除受版权保护的内容，以及（3）消除幻觉。作为对齐技术的一种，去学习具有三个优点：（1）只需要负面（例如有害）示例，这比在RLHF（基于人类反馈的强化学习）中所需的正面（例如有帮助且通常由人类编写）示例更容易和更便宜地收集（例如通过红队测试或用户报告）；（2）计算效率高；（3）当我们知道哪些训练样本导致了不良行为时，它特别有效。据我们所知，我们的工作是首次探索LLM去学习的工作之一。我们也是首次在LLM去学习中制定了设置、目标和评估。我们表明，如果从业者只有有限的

    We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited
    
[^322]: 理解RLHF对LLM泛化和多样性的影响

    Understanding the Effects of RLHF on LLM Generalisation and Diversity. (arXiv:2310.06452v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.06452](http://arxiv.org/abs/2310.06452)

    本研究深入分析了强化学习从人类反馈中调整的大型语言模型每个阶段对超出分布泛化和输出多样性的影响。

    

    在最广泛使用的AI模型中，如OpenAI的ChatGPT或Anthropic的Claude，使用强化学习从人类反馈中调整的大型语言模型（LLM）。尽管在这些方法的开发方面有大量的研究，但我们对RLHF过程中每个阶段的利与弊的理解仍然有限。为了填补这一空白，我们对每个阶段（即监督微调（SFT），奖励建模和RLHF）如何影响两个关键属性进行了全面分析：超出分布的泛化和输出多样性。在这些模型被广泛应用于真实世界中的各种情景的背景下，超出分布的泛化非常重要，而输出多样性指的是模型生成各种不同输出的能力，对于各种用例来说都非常重要。我们在摘要和指令遵循任务中对两个基本模型进行了分析，后者非常相关。

    Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's LLaMA-2. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant 
    
[^323]: 关于使用LSTD和随机特征的强化学习中的双下降现象

    On Double-Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.05518](http://arxiv.org/abs/2310.05518)

    本文研究了在强化学习中网络大小和L2正则化对性能的影响，并观察到了双下降现象。通过使用随机特征和懒惰训练策略，在参数和状态数无限大的情况下研究了正则化的最小二乘时间差分算法，得出了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。

    

    时间差分算法在深度强化学习中被广泛使用，其性能受神经网络大小的影响。然而，在监督学习中过参数化和其带来的好处已经得到了很好的理解，但是在强化学习中情况则不太清楚。本文通过理论分析探讨了网络大小和L2正则化对性能的影响，并将参数个数与访问状态个数之比定义为关键因素，当该比值大于1时称为过参数化。此外，我们观察到了双下降现象，即在参数/状态比为1附近会突然性能下降。通过利用随机特征和懒惰训练策略，我们在无限大的参数和状态数下研究了正则化的最小二乘时间差分算法。我们推导了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。

    Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive 
    
[^324]: Hieros: 基于结构化状态空间序列的分层想像模型

    Hieros: Hierarchical Imagination on Structured State Space Sequence World Models. (arXiv:2310.05167v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.05167](http://arxiv.org/abs/2310.05167)

    Hieros是一种基于结构化状态空间序列的分层想像模型，通过学习时间抽象的世界表示并在潜在空间中多个时间尺度上想象轨迹，实现了更高效的训练和想象。

    

    现代深度强化学习（DRL）算法面临的最大挑战之一是样本效率。许多方法通过学习世界模型，在想象中完全训练代理，消除了在训练期间直接与环境进行交互的需求。然而，这些方法通常面临想像准确性、探索能力或运行效率的问题。我们提出了Hieros，一种分层策略，它学习时间抽象的世界表示，并在潜在空间中多个时间尺度上想象轨迹。Hieros使用基于S5层的世界模型，它在训练期间并行预测下一个世界状态，并在环境交互期间进行迭代预测。由于S5层的特殊性质，我们的方法可以在想象过程中并行训练和迭代预测下一个世界状态。这比基于RNN的世界模型具有更高的训练效率，也比基于Transformer的世界模型具有更高的想象效率。

    One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose Hieros, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models.  We show that our 
    
[^325]: "垃圾DNA假设：通过稀疏性对LLM预训练权重进行任务中心角度分析"

    Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity. (arXiv:2310.02277v1 [cs.LG])

    [http://arxiv.org/abs/2310.02277](http://arxiv.org/abs/2310.02277)

    本文研究通过稀疏性分析LLM预训练权重的任务中心角度，挑战了传统对于权重中冗余性的观点，并提出了"垃圾DNA假设"。

    

    传统对"垃圾DNA"的概念长期以来与人类基因组中的非编码片段相关联，占其组成的大约98%。然而，最近的研究揭示了一些这些看似无功能的DNA序列在细胞过程中起到的关键作用。有趣的是，深度神经网络中的权重与人类基因中观察到的冗余性有着显著的相似性。人们认为，庞大模型中的权重包含了过多的冗余，可以在不影响性能的情况下去除。本文通过提出一个令人信服的反论来挑战这个传统观点。我们使用稀疏性作为一种工具，来独立而准确地量化预训练大语言模型(LLM)中低幅度权重的细微重要性，从下游任务中心的角度理解它们包含的知识。我们提出了支持我们深入研究的"垃圾DNA假设"。

    The traditional notion of "Junk DNA" has long been linked to non-coding segments within the human genome, constituting roughly 98% of its composition. However, recent research has unveiled the critical roles some of these seemingly non-functional DNA sequences play in cellular processes. Intriguingly, the weights within deep neural networks exhibit a remarkable similarity to the redundancy observed in human genes. It was believed that weights in gigantic models contained excessive redundancy, and could be removed without compromising performance. This paper challenges this conventional wisdom by presenting a compelling counter-argument. We employ sparsity as a tool to isolate and quantify the nuanced significance of low-magnitude weights in pre-trained large language models (LLMs). Our study demonstrates a strong correlation between these weight magnitudes and the knowledge they encapsulate, from a downstream task-centric angle. we raise the "Junk DNA Hypothesis" backed by our in-depth
    
[^326]: ChaCha：利用大型语言模型引导儿童分享与个人事件相关的情绪

    ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v1 [cs.HC])

    [http://arxiv.org/abs/2309.12244](http://arxiv.org/abs/2309.12244)

    ChaCha是一个利用大型语言模型（LLMs）的聊天机器人，鼓励儿童分享个人事件和相关情绪。通过一个探索性研究，发现儿童将ChaCha视为亲密的朋友，并愿意与其分享各种主题的故事。

    

    儿童通常通过与家人或他人分享故事和感受来学习辨识和表达情绪，然而，由于儿童正在发展他们的交流技能，父母或兄弟姐妹很难与他们进行情感沟通。本文介绍了ChaCha，一个鼓励和引导儿童分享个人事件和相关情绪的聊天机器人。ChaCha结合了状态机和大型语言模型（LLMs），在进行自由对话的同时保持对话的方向性。通过与20名年龄在8-12岁的儿童进行的探索性研究，我们研究了ChaCha如何促使儿童分享个人事件并引导他们描述相关情绪。参与者认为ChaCha就像一个亲密的朋友，并分享了各种主题的故事，如家庭旅行和个人成就。基于定量和定性发现，我们讨论了利用LLMs设计适合儿童的聊天机器人的机遇。

    Children typically learn to identify and express emotions through sharing their stories and feelings with others, particularly their family. However, it is challenging for parents or siblings to have emotional communication with children since children are still developing their communication skills. We present ChaCha, a chatbot that encourages and guides children to share personal events and associated emotions. ChaCha combines a state machine and large language models (LLMs) to keep the dialogue on track while carrying on free-form conversations. Through an exploratory study with 20 children (aged 8-12), we examine how ChaCha prompts children to share personal events and guides them to describe associated emotions. Participants perceived ChaCha as a close friend and shared their stories on various topics, such as family trips and personal achievements. Based on the quantitative and qualitative findings, we discuss opportunities for leveraging LLMs to design child-friendly chatbots to
    
[^327]: RaTrack: 带有4D雷达点云的运动物体检测与跟踪

    RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud. (arXiv:2309.09737v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.09737](http://arxiv.org/abs/2309.09737)

    RaTrack是一种针对雷达跟踪的创新解决方案，通过运动分割和聚类以及运动估计模块，实现了对移动物体的精确跟踪，优于最先进性能。

    

    移动自主性依赖于对动态环境的精确感知。在3D世界中稳定地跟踪移动物体因此对于轨迹预测、避障和路径规划等应用起着关键作用。虽然大多数现有方法利用LiDAR或相机进行多目标跟踪（MOT），但4D成像雷达的能力仍然很少被探索。认识到4D雷达数据中的雷达噪声和点稀疏性所带来的挑战，我们介绍了RaTrack，这是一种专门针对基于雷达的跟踪的创新解决方案。我们的方法摒弃了对特定对象类型和3D边界框的依赖，而是专注于运动分割和聚类，并配以运动估计模块。在View-of-Delft数据集上进行评估时，RaTrack展示出了优于最先进性能的运动物体跟踪精度。

    Mobile autonomy relies on the precise perception of dynamic environments. Robustly tracking moving objects in 3D world thus plays a pivotal role for applications like trajectory prediction, obstacle avoidance, and path planning. While most current methods utilize LiDARs or cameras for Multiple Object Tracking (MOT), the capabilities of 4D imaging radars remain largely unexplored. Recognizing the challenges posed by radar noise and point sparsity in 4D radar data, we introduce RaTrack, an innovative solution tailored for radar-based tracking. Bypassing the typical reliance on specific object types and 3D bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. Evaluated on the View-of-Delft dataset, RaTrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art.
    
[^328]: VERSE：具有实时推理能力的虚拟梯度感知流转学习

    VERSE: Virtual-Gradient Aware Streaming Lifelong Learning with Anytime Inference. (arXiv:2309.08227v1 [cs.LG])

    [http://arxiv.org/abs/2309.08227](http://arxiv.org/abs/2309.08227)

    这项研究提出了一种具有实时推理能力的流式终身学习方法，采用虚拟梯度进行连续表示学习，借助语义记忆来抑制灾难性遗忘，并在多样化的数据上进行了广泛实验。

    

    终身学习是指在训练AI代理的同时，防止其遗忘以前获得的知识的问题。现有的方法大多关注在静态环境下的终身学习，并且缺乏在快速变化的动态环境中减轻遗忘的能力。流式终身学习是终身学习中一个具有挑战性的设置，其目标是在动态的非平稳环境中进行连续学习而不遗忘。我们引入一种新颖的终身学习方法，该方法是流式的，仅需要对数据进行一次遍历，可以以类增量的方式学习，并且可以进行即时评估（实时推理）。为了实现这些，我们提出了用于连续表示学习的虚拟梯度，以防止灾难性遗忘，并借助基于指数移动平均的语义记忆进一步提高性能。我们在多样化的数据上进行了广泛的实验。

    Lifelong learning, also referred to as continual learning, is the problem of training an AI agent continuously while also preventing it from forgetting its previously acquired knowledge. Most of the existing methods primarily focus on lifelong learning within a static environment and lack the ability to mitigate forgetting in a quickly-changing dynamic environment. Streaming lifelong learning is a challenging setting of lifelong learning with the goal of continuous learning in a dynamic non-stationary environment without forgetting. We introduce a novel approach to lifelong learning, which is streaming, requires a single pass over the data, can learn in a class-incremental manner, and can be evaluated on-the-fly (anytime inference). To accomplish these, we propose virtual gradients for continual representation learning to prevent catastrophic forgetting and leverage an exponential-moving-average-based semantic memory to further enhance performance. Extensive experiments on diverse data
    
[^329]: DePT:分解提示调整以实现参数高效微调

    DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05173](http://arxiv.org/abs/2309.05173)

    DePT通过将软提示分解为较短的软提示和一对低秩矩阵，并用两个不同的学习率来优化，以解决提示调整对训练和推理时间以及内存使用的影响，从而实现更好的性能。

    

    提示调整（PT）是一种将可训练的少量软提示向量附加到语言模型（LM）输入中的参数高效微调（PEFT）方法，已在各种任务和模型中显示出了有希望的结果。 与其他PEFT方法相比，PT的竞争性能可以在可训练参数更少的情况下保持，并且随着模型规模的扩大，其参数并不会显著增加。 但是，PT引入了额外的软提示标记，导致输入序列变长，这对于Transformer的二次复杂度而言，在训练和推理时间以及内存使用方面会产生显著影响。 这对于面临大量每日查询的大型语言模型（LLMs）尤其令人担忧。

    Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
    
[^330]: 用编码-解码器进行可识别的认知诊断模型来建模学生的表现

    Identifiable Cognitive Diagnosis with Encoder-decoder for Modelling Students' Performance. (arXiv:2309.00300v1 [cs.AI])

    [http://arxiv.org/abs/2309.00300](http://arxiv.org/abs/2309.00300)

    本文提出了一个可识别的认知诊断框架，该框架能够从学生的答题记录中直接诊断可识别和可解释的考生特征和题目特征，并通过重建答题记录来确保诊断结果的可识别性。

    

    认知诊断旨在根据学生在考试题目上的答题成绩来诊断他们的知识水平，这是许多领域如计算自适应测试的基础。现有的认知诊断模型（CDMs）遵循了一个能力-响应范式，即将诊断结果视为学生响应的原因，并通过优化来学习诊断结果。然而，这种范式很容易导致不可识别的诊断结果和解释过拟合问题，这对于学生学习表现的量化是有害的。为了解决这些问题，我们提出了一种新的可识别的认知诊断框架。具体而言，我们首先提出了一个灵活的诊断模块，该模块直接从响应日志中诊断可识别和可解释的考生特征和题目特征。接下来，我们利用一个通用的预测模块从诊断结果中重建响应日志，以确保诊断结果的可识别性。

    Cognitive diagnosis aims to diagnose students' knowledge proficiencies based on their response scores on exam questions, which is the basis of many domains such as computerized adaptive testing. Existing cognitive diagnosis models (CDMs) follow a proficiency-response paradigm, which views diagnostic results as learnable embeddings that are the cause of students' responses and learns the diagnostic results through optimization. However, such a paradigm can easily lead to unidentifiable diagnostic results and the explainability overfitting problem, which is harmful to the quantification of students' learning performance. To address these problems, we propose a novel identifiable cognitive diagnosis framework. Specifically, we first propose a flexible diagnostic module which directly diagnose identifiable and explainable examinee traits and question features from response logs. Next, we leverage a general predictive module to reconstruct response logs from the diagnostic results to ensure
    
[^331]: 递归总结在大型语言模型中实现长期对话记忆

    Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models. (arXiv:2308.15022v1 [cs.CL])

    [http://arxiv.org/abs/2308.15022](http://arxiv.org/abs/2308.15022)

    递归总结在大型语言模型中实现长期对话记忆，可以提高对话系统在长对话中记忆重要信息的能力。

    

    大多数开放领域的对话系统在长期对话中容易遗忘重要信息。现有方法通常训练特定的检索器或总结器从过去获取关键信息，这需要耗费时间且高度依赖标记数据的质量。为了缓解这个问题，我们提出使用大型语言模型（LLMs）递归生成总结/记忆，以增强长期记忆能力。具体而言，我们的方法首先刺激LLMs记住小对话上下文，然后递归地使用之前的记忆和随后的对话内容产生新的记忆。最后，LLM可以在最新记忆的帮助下轻松生成高度一致的响应。我们使用ChatGPT和text-davinci-003进行评估，对广泛使用的公共数据集进行的实验证明我们的方法在长对话中可以生成更一致的响应。值得注意的是，我们的方法是实现LLM建模的潜在解决方案。

    Most open-domain dialogue systems suffer from forgetting important information, especially in a long-term conversation. Existing works usually train the specific retriever or summarizer to obtain key information from the past, which is time-consuming and highly depends on the quality of labeled data. To alleviate this problem, we propose to recursively generate summaries/ memory using large language models (LLMs) to enhance long-term memory ability. Specifically, our method first stimulates LLMs to memorize small dialogue contexts and then recursively produce new memory using previous memory and following contexts. Finally, the LLM can easily generate a highly consistent response with the help of the latest memory. We evaluate our method using ChatGPT and text-davinci-003, and the experiments on the widely-used public dataset show that our method can generate more consistent responses in a long-context conversation. Notably, our method is a potential solution to enable the LLM to model
    
[^332]: 探索大型语言模型用于知识图谱补全

    Exploring Large Language Models for Knowledge Graph Completion. (arXiv:2308.13916v1 [cs.CL])

    [http://arxiv.org/abs/2308.13916](http://arxiv.org/abs/2308.13916)

    本文研究了利用大型语言模型（LLM）进行知识图谱补全的方法，并引入了一种创新的框架（知识图谱LLM），以提高三元组分类和关系预测的性能。

    

    知识图谱在众多人工智能任务中发挥着重要作用，但经常面临不完整性的问题。在本研究中，我们探索了利用大型语言模型（LLM）进行知识图谱补全的方法。我们将知识图谱中的三元组视为文本序列，并引入了一种创新的框架，称为知识图谱LLM（KG-LLM），来对这些三元组进行建模。我们的技术利用三元组的实体和关系描述作为提示，并利用响应进行预测。对各种基准知识图谱的实验表明，我们的方法在三元组分类和关系预测等任务中达到了最先进的性能。我们还发现，微调相对较小的模型（例如LLaMA-7B，ChatGLM-6B）优于最新的ChatGPT和GPT-4。

    Knowledge graphs play a vital role in numerous artificial intelligence tasks, yet they frequently face the issue of incompleteness. In this study, we explore utilizing Large Language Models (LLM) for knowledge graph completion. We consider triples in knowledge graphs as text sequences and introduce an innovative framework called Knowledge Graph LLM (KG-LLM) to model these triples. Our technique employs entity and relation descriptions of a triple as prompts and utilizes the response for predictions. Experiments on various benchmark knowledge graphs demonstrate that our method attains state-of-the-art performance in tasks such as triple classification and relation prediction. We also find that fine-tuning relatively smaller models (e.g., LLaMA-7B, ChatGLM-6B) outperforms recent ChatGPT and GPT-4.
    
[^333]: 这不是一个苹果：多模态嵌入中的对抗幻觉

    Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings. (arXiv:2308.11804v1 [cs.CR])

    [http://arxiv.org/abs/2308.11804](http://arxiv.org/abs/2308.11804)

    该论文研究了多模态嵌入中的对抗幻觉问题。对手可以扰动输入的任意模态，使其嵌入与其他模态的任意输入接近，从而实现任意图像与任意文本、任意文本与任意声音的对齐。该问题与下游任务无关，对生成和分类任务会产生误导。

    

    多模态编码器将图像、声音、文本、视频等映射到一个单一的嵌入空间中，通过对齐不同模态的表示（例如将一张狗的图像与一种叫声相关联）。我们展示了多模态嵌入可以受到一种我们称之为“对抗幻觉”的攻击。给定任意模态的输入，对手可以扰动它，使其嵌入接近于另一模态中任意对手选择的输入的嵌入。幻觉使对手能够将任意图像与任意文本、任意文本与任意声音等进行对齐。对抗幻觉利用了嵌入空间中的接近性，因此与下游任务无关。使用ImageBind嵌入，我们演示了在没有具体下游任务知识的情况下，通过对抗性对齐的输入如何误导图像生成、文本生成和零样例分类。

    Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.  Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
    
[^334]: ReLLa: 基于检索增强的大型语言模型的推荐系统中的生命周期序列行为理解

    ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. (arXiv:2308.11131v1 [cs.IR])

    [http://arxiv.org/abs/2308.11131](http://arxiv.org/abs/2308.11131)

    本论文提出了一种名为ReLLa的检索增强大型语言模型框架，用于零样本和小样本推荐任务。通过语义用户行为检索（SUBR）来提取上下文中的有用信息，以改善LLMs的推荐性能。

    

    随着大型语言模型（LLMs）在自然语言处理（NLP）领域取得了显著突破，基于LLM的推荐系统引起了广泛关注并被积极探索。本文专注于适应和增强纯大型语言模型以用于零样本和小样本推荐任务。首先，我们针对推荐领域中LLMs无法从长用户行为序列的文本上下文中提取有用信息的问题，提出并定义了生命周期序列行为理解问题。为了解决这个问题并提高LLMs的推荐性能，我们提出了一种新的框架，即检索增强的大型语言模型（ReLLa）。针对零样本推荐，我们执行语义用户行为检索（SUBR）来提高数据的利用率。

    With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data
    
[^335]: 在大型语言模型中使用反向推理进行验证

    Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])

    [http://arxiv.org/abs/2308.07758](http://arxiv.org/abs/2308.07758)

    本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。

    

    链式思考（Chain-of-Though, CoT）提示在各种推理任务中表现出了很好的性能。最近，Self-Consistency提出了一种方法，即通过采样一组不同的推理链，这些链可能导致不同的答案，然后选择得票最多的答案。本文提出了一种新颖的方法，即在验证候选答案时使用反向推理。我们使用一个简单的模板，即``如果我们知道上述问题的答案是候选答案，那么未知变量x的值是多少？''，将问题中的一个标记屏蔽，并要求语言模型预测被屏蔽的标记。直观上讲，如果提供的候选答案是正确的，语言模型应该能够成功预测被屏蔽的标记。我们进一步提出了FOBAR方法，将正向和反向推理结合起来估计候选答案的概率。我们在六个数据集和三个实验中进行了广泛的实验。

    Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
    
[^336]: 具有确定性演化状态的强盗模型

    Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])

    [http://arxiv.org/abs/2307.11655](http://arxiv.org/abs/2307.11655)

    该论文提出了一种名为具有确定性演化状态的强盗模型，用于学习带有强盗反馈的推荐系统和在线广告。该模型考虑了状态演化的不同速率，能准确评估奖励与系统健康程度之间的关系。

    

    我们提出了一种学习与强盗反馈结合的模型，同时考虑到确定性演化和不可观测的状态，我们称之为具有确定性演化状态的强盗模型。我们的模型主要应用于推荐系统和在线广告的学习。在这两种情况下，算法在每一轮获得的奖励是选择行动的短期奖励和系统的“健康”程度（即通过其状态测量）的函数。例如，在推荐系统中，平台从用户对特定类型内容的参与中获得的奖励不仅取决于具体内容的固有特征，还取决于用户与平台上其他类型内容互动后其偏好的演化。我们的通用模型考虑了状态演化的不同速率λ∈[0,1]（例如，用户的偏好因先前内容消费而快速变化）。

    We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption
    
[^337]: 鲁棒视觉问答：数据集、方法和未来挑战

    Robust Visual Question Answering: Datasets, Methods, and Future Challenges. (arXiv:2307.11471v1 [cs.CV])

    [http://arxiv.org/abs/2307.11471](http://arxiv.org/abs/2307.11471)

    这篇论文提供了一份关于鲁棒视觉问答的综合调查，包括了数据集的发展过程、评估指标以及不同方法之间的差异。先前的通用视觉问答方法常常会受到训练数据中的偏差的影响，难以学习到正确的行为。为了增强鲁棒性，研究者提出了各种数据集和去偏差方法。

    

    视觉问答需要一个系统能够在给定图像和自然语言问题的情况下提供准确的自然语言回答。然而，广泛认识到先前的通用视觉问答方法往往倾向于记忆训练数据中存在的偏差，而不是学习正确的行为，例如在预测答案之前对图像进行定位。因此，这些方法通常在分布内表现出色，但在分布外表现较差。近年来，已经提出了各种数据集和去偏差方法来分别评估和增强视觉问答的鲁棒性。本文首次提供了一份关注这一新兴领域的全面调查。具体而言，我们首先从分布内和分布外的视角概述了数据集的发展过程。然后，我们研究了这些数据集采用的评估指标。第三，我们提出了一个分类法，介绍了数据集的发展过程、相似之处和差异之处。

    Visual question answering requires a system to provide an accurate natural language answer given an image and a natural language question. However, it is widely recognized that previous generic VQA methods often exhibit a tendency to memorize biases present in the training data rather than learning proper behaviors, such as grounding images before predicting answers. Therefore, these methods usually achieve high in-distribution but poor out-of-distribution performance. In recent years, various datasets and debiasing methods have been proposed to evaluate and enhance the VQA robustness, respectively. This paper provides the first comprehensive survey focused on this emerging fashion. Specifically, we first provide an overview of the development process of datasets from in-distribution and out-of-distribution perspectives. Then, we examine the evaluation metrics employed by these datasets. Thirdly, we propose a typology that presents the development process, similarities and differences,
    
[^338]: 智能的本质

    Nature of Intelligence. (arXiv:2307.11114v1 [q-bio.NC])

    [http://arxiv.org/abs/2307.11114](http://arxiv.org/abs/2307.11114)

    智能的本质是一系列通过在空间和时间上建立数据集之间的功能关系来最小化系统熵的数学函数过程。

    

    人类大脑是人类智能的基础。通过模拟人类大脑，人工智能构建具有学习能力并执行接近人类水平的智能任务的计算模型。深度神经网络由多个计算层组成，学习数据的表示并在许多识别领域改进了最先进的技术。然而，智能的本质，即通过人类和人工智能共同代表的智能的本质，尚不清楚。在这里，我们展示智能的本质是一系列通过在空间和时间上建立数据集之间的功能关系来最小化系统熵的数学函数过程。人类和人工智能通过以一种受强化方式消耗能量的方式实现这些减熵过程。根据这一假设，我们建立了关于语言、无意识和意识的数学模型，预测神经科学和人工智能工程实现的证据。

    The human brain is the substrate for human intelligence. By simulating the human brain, artificial intelligence builds computational models that have learning capabilities and perform intelligent tasks approaching the human level. Deep neural networks consist of multiple computation layers to learn representations of data and improve the state-of-the-art in many recognition domains. However, the essence of intelligence commonly represented by both humans and AI is unknown. Here, we show that the nature of intelligence is a series of mathematically functional processes that minimize system entropy by establishing functional relationships between datasets over space and time. Humans and AI have achieved intelligence by implementing these entropy-reducing processes in a reinforced manner that consumes energy. With this hypothesis, we establish mathematical models of language, unconsciousness and consciousness, predicting the evidence to be found by neuroscience and achieved by AI engineer
    
[^339]: 提示不应被视为秘密：系统地衡量提示提取攻击的成功性

    Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success. (arXiv:2307.06865v1 [cs.CL])

    [http://arxiv.org/abs/2307.06865](http://arxiv.org/abs/2307.06865)

    本论文提出了一个系统地衡量提示提取攻击成功的框架，并通过多个实验发现，即使提示被保密，简单的基于文本的攻击仍然可以高概率地揭示提示。

    

    大型语言模型的生成通常通过提示技术来控制，其中用户对模型的查询以旨在指导模型在该查询上的行为的提示作为前缀。公司用于指导其模型的提示通常被视为秘密，隐藏在查询的用户之外。它们甚至被视为可以买卖的商品。然而，有经验性的证据显示，即使提示被保密，用户仍然可以提取它们。在本文中，我们提出了一个系统地衡量提示提取攻击成功的框架。在使用多个提示源和多个基础语言模型的实验中，我们发现简单的基于文本的攻击实际上可以高概率地揭示提示。

    The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret. In this paper, we present a framework for systematically measuring the success of prompt extraction attacks. In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.
    
[^340]: 强化学习中基于奖励机器抽象的上下文预规划以增强迁移学习

    Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning. (arXiv:2307.05209v1 [cs.AI])

    [http://arxiv.org/abs/2307.05209](http://arxiv.org/abs/2307.05209)

    我们提出了一种使用奖励机器抽象来表示当前任务，并在迁移学习中提升DRL代理的性能的方法，实验表明该方法能够提高样本效率并在多个领域中进行少样本迁移。

    

    最近的研究表明，深度强化学习（DRL）代理倾向于过拟合训练任务，并且无法适应轻微的环境变化。为了在转移到未见任务时加快学习，我们提出了一种使用奖励机器（RM）来表示当前任务的新方法，奖励机器是基于当前任务的奖励和动态生成子任务的状态机抽象。我们的方法为代理提供了当前抽象状态的符号表示，并奖励它们达成这些转换。这些表示在任务之间共享，使代理能够利用先前遇到的符号和转换的知识，从而增强迁移能力。我们的实证评估表明，我们的表示在各种领域中提高了样本效率和少样本迁移。

    Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RM), state machine abstractions that induce subtasks based on the current task's rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Our empirical evaluation shows that our representations improve sample efficiency and few-shot transfer in a variety of domains.
    
[^341]: 使用腰部佩戴的加速计在典型的行走和跑步速度范围内自动检测步态事件和行走距离

    Automated Detection of Gait Events and Travel Distance Using Waist-worn Accelerometers Across a Typical Range of Walking and Running Speeds. (arXiv:2307.04866v1 [eess.SP])

    [http://arxiv.org/abs/2307.04866](http://arxiv.org/abs/2307.04866)

    该论文研究了使用腰部佩戴的加速计自动检测步态事件和行走距离的方法，通过分析市售智能手机加速计数据，实现了从广泛的步态速度范围中提取步态特征，可用于对Duchenne肌肉萎缩患儿和典型发育正常患者的评估。

    

    背景：估计步态（CFs）的时间空间临床特征，如步数和长度、步长、步频、步速和行走距离等，在使用可穿戴式加速计进行基于社区的移动性评估中是一个重要的组成部分。然而，由于设备复杂性和可用性、成本和分析方法学引起的挑战限制了此类工具的广泛应用。研究问题：能否使用市售智能手机的加速计数据来提取Duchenne肌肉萎缩（DMD）患儿和典型发育正常（TDs）患者在广泛步态速度范围内的步态CFs，并使用机器学习（ML）方法。方法：15名DMD患儿和15名TDs被要求在10MRW、25MRW、100MRW、6MWT和FW评估中以一系列步态速度进行监督性临床测试，同时佩戴手机基础加速计。

    Background: Estimation of temporospatial clinical features of gait (CFs), such as step count and length, step duration, step frequency, gait speed and distance traveled is an important component of community-based mobility evaluation using wearable accelerometers. However, challenges arising from device complexity and availability, cost and analytical methodology have limited widespread application of such tools. Research Question: Can accelerometer data from commercially-available smartphones be used to extract gait CFs across a broad range of attainable gait velocities in children with Duchenne muscular dystrophy (DMD) and typically developing controls (TDs) using machine learning (ML)-based methods Methods: Fifteen children with DMD and 15 TDs underwent supervised clinical testing across a range of gait speeds using 10 or 25m run/walk (10MRW, 25MRW), 100m run/walk (100MRW), 6-minute walk (6MWT) and free-walk (FW) evaluations while wearing a mobile phone-based accelerometer at the wa
    
[^342]: 通过训练概念影响理解不公平性

    Understanding Unfairness via Training Concept Influence. (arXiv:2306.17828v1 [cs.LG])

    [http://arxiv.org/abs/2306.17828](http://arxiv.org/abs/2306.17828)

    通过观察训练数据的作用，研究模型不公平性的来源和影响，并通过改变样本的属性来计算训练样本对模型的不公平性的影响。

    

    了解模型不公平性的原因有助于从业人员更好地理解他们的数据和算法。我们通过培训数据这一主要不公平来源的视角来研究这个问题。我们提出以下问题：如果在训练数据中有些样本（1）来自不同的（例如人口统计学）群体，（2）标记方式不同，或者（3）某些特征发生了变化，那么模型的公平性表现会发生怎样的变化？换句话说，我们通过反事实地对基于预定义概念的样本进行干预和改变，量化训练样本对模型的不公平性的影响。计算训练样本对模型相对于概念的不公平性的影响时，我们首先基于概念生成反事实版本的样本，即如果概念发生变化，样本的反事实版本。然后我们计算重新

    Knowing the causes of a model's unfairness helps practitioners better understand their data and algorithms. This is an important yet relatively unexplored task. We look into this problem through the lens of the training data - one of the major sources of unfairness. We ask the following questions: how would a model's fairness performance change if, in its training data, some samples (1) were collected from a different (e.g. demographic) group, (2) were labeled differently, or (3) some features were changed? In other words, we quantify the fairness influence of training samples by counterfactually intervening and changing samples based on predefined concepts, i.e. data attributes such as features (X), labels (Y), or sensitive attributes (A). To calculate a training sample's influence on the model's unfairness w.r.t a concept, we first generate counterfactual samples based on the concept, i.e. the counterfactual versions of the sample if the concept were changed. We then calculate the re
    
[^343]: 以提示为基础的个性化冷启动推荐的研究

    Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])

    [http://arxiv.org/abs/2306.17256](http://arxiv.org/abs/2306.17256)

    本研究旨在解决个性化冷启动推荐问题，通过利用预训练语言模型的能力，将推荐过程转化为自然语言情感分析，提供适用于创业企业和用户参与历史不足的平台的个性化推荐。

    

    推荐系统在根据用户过去的行为帮助用户发现与其兴趣相符的信息方面发挥着关键作用。然而，当用户和物品之间的历史交互记录不可用时，开发个性化推荐系统变得具有挑战性，这就是所谓的系统冷启动推荐问题。此问题在创业企业或用户参与历史不足的平台中尤为突出。以往的研究集中在用户或物品的冷启动场景，其中系统仍然通过在同一领域中的历史用户和物品交互进行训练来为新用户或物品提供推荐，而无法解决我们的问题。为了弥合这一鸿沟，我们的研究引入了一种创新且有效的方法，利用预训练语言模型的能力。我们将推荐过程转化为自然语言情感分析，其中包含用户资料和物品属性的信息。

    Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
    
[^344]: 组正交正则化：用于视觉模型自适应和鲁棒性的方法

    Group Orthogonalization Regularization For Vision Models Adaptation and Robustness. (arXiv:2306.10001v1 [cs.CV])

    [http://arxiv.org/abs/2306.10001](http://arxiv.org/abs/2306.10001)

    该论文提出了一种新的正则化技术，称为组正交正则化，能够提高视觉模型自适应和鲁棒性。实验结果表明，此种正则化可以应用于不同的深度学习模型，并且能够有效地优化模型性能。

    

    随着神经网络变得越来越深，参数内部的冗余度也随之增加。这种现象导致了许多试图减少卷积滤波器之间相关性的方法的出现。我们提出了一种计算高效的正则化技术，它鼓励同一层内滤波器组之间的正交性。我们的实验表明，当结合最新的扩散模型适应方法和视觉转换器（ViTs）时，这种正则化可以提高下游任务的性能。我们进一步展示了当在对抗训练过程中强制进行组正交性时的增强鲁棒性。我们的代码可在https://github.com/YoavKurtz/GOR获得。

    As neural networks become deeper, the redundancy within their parameters increases. This phenomenon has led to several methods that attempt to reduce the correlation between convolutional filters. We propose a computationally efficient regularization technique that encourages orthonormality between groups of filters within the same layer. Our experiments show that when incorporated into recent adaptation methods for diffusion models and vision transformers (ViTs), this regularization improves performance on downstream tasks. We further show improved robustness when group orthogonality is enforced during adversarial training. Our code is available at https://github.com/YoavKurtz/GOR.
    
[^345]: 抓住意外收获：在离线策略演员-评论家中利用过去成功的价值(arXiv:2306.02865v2 [cs.LG]已更新)

    Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic. (arXiv:2306.02865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02865](http://arxiv.org/abs/2306.02865)

    该论文提出了 BEE 操作符，通过充分利用过去的成功经验，并保持探索乐观性，解决了离线策略演员-评论家中 Q 值高估与低估问题，提高了策略学习和样本效率。

    

    学习高质量的 Q 值函数在许多现代离线深度强化学习 (RL) 算法的成功中起着关键作用。之前的研究集中解决采用函数逼近器和离线学习所导致的值过高的问题。与这种普遍观点不同，我们观察到 Q 值在 RL 训练过程的后期实际上被低估了，主要是由于贝尔曼更新中，当前策略使用比回放缓冲区中更优的动作样本差。我们假设这个长期被忽视的现象可能阻碍了策略学习，降低了样本效率。我们的想法是在保持探索乐观性的同时，结合充分利用过去成功的经验。我们提出了混合利用和探索 (BEE) 操作符，这是一种简单而有效的方法，使用历史上表现最佳的动作和当前策略生成的动作来更新 Q 值。

    Learning high-quality Q-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that Q-values are indeed underestimated in the latter stage of the RL training process, primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency. Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates Q-value using both historical best-performing actions and
    
[^346]: Med-UniC：通过减少偏见实现跨语言医学图像-语言预训练的统一

    Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias. (arXiv:2305.19894v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.19894](http://arxiv.org/abs/2305.19894)

    Med-UniC是一个新的框架，旨在通过整合英语和西班牙语的跨语言医学数据，实现跨语言医学图像-语言预训练的统一。他们提出了跨语言文本对齐规则(CTR)，以明确统一来自不同语言社区的医学报告的跨语言语义表示。

    

    数据稀缺性对医学图像-语言预训练(VLP)的效果造成了严重障碍。解决方案可能在于结合来自各种语言社区的数据集。然而，主要挑战来自于整合不同的语法和语义、特定于语言的医学术语以及特定于文化的隐式知识的复杂性。因此，一个关键的考虑因素是由不同语言引起的社区偏见的存在。本文介绍了一种名为统一跨语言医学图像-语言预训练(Med-UniC)的新框架，旨在整合来自两种最常见语言的多模态医学数据，即英语和西班牙语。具体而言，我们提出了跨语言文本对齐规则(CTR)，明确统一来自不同语言社区的医学报告的跨语言语义表示。通过潜在语言解缠，优化CTR，使我们的优化成果。

    The scarcity of data presents a critical obstacle to the efficacy of medical visionlanguage pre-training (VLP). A potential solution lies in the combination of datasets from various language communities. Nevertheless, the main challenge stems from the complexity of integrating diverse syntax and semantics, language-specific medical terminology, and culture-specific implicit knowledge. Therefore, one crucial aspect to consider is the presence of community bias caused by different languages. This paper presents a novel framework named Unifying Cross-Lingual Medical Vision-Language Pre-Training (Med-UniC), designed to integrate multimodal medical data from the two most prevalent languages, English and Spanish. Specifically, we propose Cross-lingual Text Alignment Regularization (CTR) to explicitly unify cross-lingual semantic representations of medical reports originating from diverse language communities. CTR is optimized through latent language disentanglement, rendering our optimizatio
    
[^347]: 一张图值得一比特的差异性：当图的对比学习遇到脉冲神经网络时。

    A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks. (arXiv:2305.19306v1 [cs.NE])

    [http://arxiv.org/abs/2305.19306](http://arxiv.org/abs/2305.19306)

    本文提出了SpikeGCL，一种用二值化1比特表示来提高效率和节约资源的图对比学习框架，实验结果表明可以以近32倍的表示存储压缩实现高效学习。

    

    虽然对比自监督学习已经成为图神经网络的事实上的学习范式，但对高任务准确性的追求需要大的隐藏维度来学习信息丰富、有区别性的全精度表示，这引发了对计算、存储和能源消耗负担（在现实世界应用中大多被忽略）的担忧。本文探索了一种有前途的方向，即用脉冲神经网络（SNNs）进行图的对比学习（GCL），利用稀疏和二元特性来学习更具生物可行性和紧凑性的表示。我们提出了SpikeGCL，一种学习图的二值化1比特表示的新型GCL框架，平衡了效率和性能之间的权衡。我们提供了理论保证，证明SpikeGCL在表达能力上与其全精度对应物具有可比性。实验结果表明，通过将表示存储压缩近32倍，SpikeGCL在保持高准确性的同时可以实现高效的学习。

    While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of high task accuracy requires a large hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation, memory footprint, and energy consumption burden (largely overlooked) for real-world applications. This paper explores a promising direction for graph contrastive learning (GCL) with spiking neural networks (SNNs), which leverage sparse and binary characteristics to learn more biologically plausible and compact representations. We propose SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance. We provide theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts. Experimental results demonstrate that, with nearly 32x representation storage compressio
    
[^348]: 基于自编码器的条件神经过程用于表示学习

    Autoencoding Conditional Neural Processes for Representation Learning. (arXiv:2305.18485v1 [cs.LG])

    [http://arxiv.org/abs/2305.18485](http://arxiv.org/abs/2305.18485)

    本文提出了部分像素空间变分自编码器，结合了自编码器与条件神经过程，可以学习到一系列基本物理和文化概念的表示，并且可以提高上下文预测的准确性。

    

    条件神经过程(CNPs)是一种灵活高效的模型族群，可以从观测值中学习出一个随机过程。在视觉领域中，CNPs 在上下文图像补全中得到了特别的应用，即通过观察某些位置的像素值来预测其他未观察位置上的值的分布。然而，学习这样一个 CNP 的像素选择通常是随机的或者是通过一个简单的统计量(例如像素方差)导出的。本文将问题转变一下：一个 CNP 想要观察哪些像素？也就是说，哪些像素允许拟合 CNP，这样的像素能告诉我们一些关于潜在图像的信息吗？将提供给 CNP 的上下文视为固定大小的潜在表示，我们构建了一个一次性变分框架，部分像素空间变分自编码器(Partical Pixel Space VAE, PPS-VAE)，同时预测这个上下文，并学习一个 CNP。我们在一组视觉数据集上评估了 PPS-VAE，发现通过相对大小或变化预测像素的选择可以安排学习，且更准确地进行了上下文预测，并且可以对基本物理和文化概念进行有意义的表示。

    Conditional neural processes (CNPs) are a flexible and efficient family of models that learn to learn a stochastic process from observations. In the visual domain, they have seen particular application in contextual image completion - observing pixel values at some locations to predict a distribution over values at other unobserved locations. However, the choice of pixels in learning such a CNP is typically either random or derived from a simple statistical measure (e.g. pixel variance). Here, we turn the problem on its head and ask: which pixels would a CNP like to observe? That is, which pixels allow fitting CNP, and do such pixels tell us something about the underlying image? Viewing the context provided to the CNP as fixed-size latent representations, we construct an amortised variational framework, Partial Pixel Space Variational Autoencoder (PPS-VAE), for predicting this context simultaneously with learning a CNP. We evaluate PPS-VAE on a set of vision datasets, and find that not
    
[^349]: 离巢：超越本地损失函数的预测优化问题

    Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize. (arXiv:2305.16830v1 [cs.LG])

    [http://arxiv.org/abs/2305.16830](http://arxiv.org/abs/2305.16830)

    本文提出了一种避免限制性假设的解决方案，利用机器学习模型的特性来提高学习损失函数的样本效率，在预测优化问题中实现了最先进的结果。

    

    预测优化问题是一种使用机器学习在不确定性条件下进行决策制定的框架。它的中心研究问题是，“如何利用决策任务的结构来定制特定任务的机器学习模型？”为此，最近的研究已经提出了学习任务特定的损失函数来捕捉这种潜在的结构。然而，当前的方法对这些损失的形式和对机器学习模型行为的影响做出了限制性的假设。这些假设既导致了高计算成本的方法，也在实践中被违反时导致了性能下降。在本文中，我们提出了解决这些问题的解决方案，避免了上述假设，利用机器学习模型的特性来提高学习损失函数的样本效率。我们从文献中的四个领域实验证明了我们的方法取得了最先进的结果，通常需要比可比方法少一个数量级的样本。

    Predict-then-Optimize is a framework for using machine learning to perform decision-making under uncertainty. The central research question it asks is, "How can the structure of a decision-making task be used to tailor ML models for that specific task?" To this end, recent work has proposed learning task-specific loss functions that capture this underlying structure. However, current approaches make restrictive assumptions about the form of these losses and their impact on ML model behavior. These assumptions both lead to approaches with high computational cost, and when they are violated in practice, poor performance. In this paper, we propose solutions to these issues, avoiding the aforementioned assumptions and utilizing the ML model's features to increase the sample efficiency of learning loss functions. We empirically show that our method achieves state-of-the-art results in four domains from the literature, often requiring an order of magnitude fewer samples than comparable metho
    
[^350]: 使用基于文献的语境化学习生成新的科学方向

    Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14259](http://arxiv.org/abs/2305.14259)

    本文介绍了一种基于文献的发现方法，通过上下文化的学习生成新的科学方向，克服了标准方法在预测关联、忽略上下文等方面的局限性。模型使用了引文和知识图关系的网络，并使用大型语言模型进行评估，发现GPT4在生成创新思想方面表现出色。

    

    基于文献的发现（LBD）旨在通过挖掘论文并生成假设来发现新的科学知识。标准的LBD仅限于预测离散概念之间的两两关系（例如，药物和疾病的关联）。LBD还忽略了关键的上下文，例如实验设置（例如，药物评估的特定患者群体）和人类科学家考虑的背景知识和动机（例如，找到没有特定副作用的药物候选）。我们通过一种新颖的上下文化LBD（C-LBD）表述来解决这些局限性：以自然语言生成科学假设，同时将它们联系到控制假设搜索空间的上下文中。我们提出了一个建模框架，使用获得的引文和知识图关系的异构网络中的“灵感”，并创建了一个从论文中派生的新数据集。我们使用强大的大型语言模型（LLM）进行评估，发现GPT4倾向于生成具有创新性的思想。

    Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
    
[^351]: GPT4Table：大型语言模型能理解结构化表格数据吗？一项基准测试和实证研究

    GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13062](http://arxiv.org/abs/2305.13062)

    本文设计了一个基准测试来评估大型语言模型（LLMs）对结构化表格数据的理解能力，并发现不同的输入选择会对性能产生影响。在基准测试的基础上，提出了“自我增强”技术以改善理解能力。

    

    大型语言模型（LLMs）作为少样本推理器来解决与自然语言相关的任务越来越具吸引力。然而，关于LLMs对结构化数据（例如表格）的理解程度还有很多需要学习的地方。尽管可以使用表格序列化作为LLMs的输入，但目前还缺乏对LLMs是否真正能够理解这类数据的全面研究。本文通过设计一个基准测试来评估LLMs的结构理解能力（SUC）来解决这个问题。我们创建的基准测试包括七个任务，每个任务都有其独特的挑战，例如单元格查找、行检索和大小检测。我们对GPT-3.5和GPT-4进行了一系列评估。我们发现性能因多种输入选择而异，包括表格输入格式、内容顺序、角色提示和分区标记等。根据基准测试评估所得的见解，我们提出了“自我增强”技术以改善性能。

    Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack of comprehensive studies examining whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We run a series of evaluations on GPT-3.5 and GPT-4. We discover that the performance varied depending on a number of input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we then propose \textit{self-augmentation} for effect
    
[^352]: CodeCompose：基于AI辅助的代码编写工具的大规模工业部署

    CodeCompose: A Large-Scale Industrial Deployment of AI-assisted Code Authoring. (arXiv:2305.12050v1 [cs.SE])

    [http://arxiv.org/abs/2305.12050](http://arxiv.org/abs/2305.12050)

    CodeCompose是一个基于InCoder LLM的AI辅助的代码编写工具，已经在Meta内部部署，可在10多种编程语言和几个编码表面上使用。 CodeCompose已成功帮助提高Meta的开发人员生产力。

    

    大型语言模型（LLMs）的崛起解锁了该技术在软件开发中的各种应用。特别是，已经证明生成式LLMs可以有效地驱动AI基础的代码编写工具，在编写代码期间可以建议整个语句或代码块。在本文中，我们介绍了基于InCoder LLM的CodeCompose，这是一个Meta内部开发和部署的AI辅助代码编写工具。CodeCompose基于合并生成能力和双向性的LLM。我们已经将CodeCompose扩展到Meta的数以万计的开发人员，在10多种编程语言和几个编码表面上使用。我们讨论了在大规模工业环境中部署此类工具时出现的用户体验和指标方面的独特挑战。我们介绍了CodeCompose的模型和系统架构的设计决策，以解决这些挑战。最后，我们提供了我们对CodeCompose的大规模部署的指标，并展示它如何帮助提高Meta的开发人员生产力。

    The rise of large language models (LLMs) has unlocked various applications of this technology in software development. In particular, generative LLMs have been shown to effectively power AI-based code authoring tools that can suggest entire statements or blocks of code during code authoring. In this paper we present CodeCompose, an AI-assisted code authoring tool developed and deployed at Meta internally. CodeCompose is based on the InCoder LLM that merges generative capabilities with bi-directionality. We have scaled up CodeCompose to serve tens of thousands of developers at Meta, across 10+ programming languages and several coding surfaces.  We discuss unique challenges in terms of user experience and metrics that arise when deploying such tools in large-scale industrial settings. We present our experience in making design decisions about the model and system architecture for CodeCompose that addresses these challenges. Finally, we present metrics from our large-scale deployment of C
    
[^353]: 使用指令微调基础模型的多模态 Web 导航。

    Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])

    [http://arxiv.org/abs/2305.11854](http://arxiv.org/abs/2305.11854)

    本文研究使用视觉语言基础模型进行数据驱动离线训练的 Web 代理，提出了一个指令跟随多模态代理WebGUM，将微调指令微调语言模型和视觉转换器，能够有效提高代理的基于视觉感知、HTML 理解和多步推理的能力。

    

    自主 Web 导航的进展受到了依赖数十亿次在线强化学习的探索性交互和具有领域特定模型设计的影响，这使得难以利用来自丰富领域外数据的泛化。在本工作中，我们研究了基于数据驱动的脱机训练，用于使用视觉语言基础模型的 Web 代理。我们提出了一个指令跟随多模态代理， WebGUM，它观察了网页截图和 HTML 页面，并输出 Web 导航操作，如单击和输入。WebGUM 是通过联合微调指令微调语言模型和视觉转换器在大量的演示语料库上训练的。我们凭经验证明，这种方法可以提高代理的基于视觉感知、HTML 理解和多步推理的能力，明显优于之前的工作。在 MiniWoB 基准测试中，我们超过之前最佳脱机方法 31.9% 以上，接近实现在线交互的表现。

    The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision transformer on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded visual perception, HTML comprehension and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB benchmark, we improve over the previous best offline methods by more than 31.9%, being close to re
    
[^354]: DeformerNet: 学习三维可塑物体的双手操纵

    DeformerNet: Learning Bimanual Manipulation of 3D Deformable Objects. (arXiv:2305.04449v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.04449](http://arxiv.org/abs/2305.04449)

    本论文介绍了一种名为DeformerNet的神经网络架构，通过学习三维可塑物体的低维表示来实现机器人对物体形状的操纵。这种方法不需要手工特征和物体特定的控制模型，可在仿真和真实机器人上进行演示和应用。

    

    从家庭护理到仓库配送再到外科手术助理等领域，应用需要机器人可靠地操纵三维可塑物体的形状。弹性三维可塑物体的分析模型需要大量参数来描述决定物体形状的可能无限自由度。以往的3D形状控制尝试依赖于手工特征来表示物体形状，并需要训练物体特定的控制模型。我们通过使用我们的新型DeformerNet神经网络架构来克服这些问题，该架构在被操纵物体的部分视图点云和目标形状的点云上运行，学习物体形状的低维表示。这个形状嵌入使机器人能够学习一种视觉伺服控制器，该控制器计算出所需的机器人末端执行器动作，将物体迭代地变形向目标形状。我们在仿真和真实机器人上演示了这一点。

    Applications in fields ranging from home care to warehouse fulfillment to surgical assistance require robots to reliably manipulate the shape of 3D deformable objects. Analytic models of elastic, 3D deformable objects require numerous parameters to describe the potentially infinite degrees of freedom present in determining the object's shape. Previous attempts at performing 3D shape control rely on hand-crafted features to represent the object shape and require training of object-specific control models. We overcome these issues through the use of our novel DeformerNet neural network architecture, which operates on a partial-view point cloud of the manipulated object and a point cloud of the goal shape to learn a low-dimensional representation of the object shape. This shape embedding enables the robot to learn a visual servo controller that computes the desired robot end-effector action to iteratively deform the object toward the target shape. We demonstrate both in simulation and on 
    
[^355]: MLCopilot：释放大语言模型在解决机器学习任务中的能力

    MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks. (arXiv:2304.14979v1 [cs.LG])

    [http://arxiv.org/abs/2304.14979](http://arxiv.org/abs/2304.14979)

    本文介绍了一种新型框架MLCopilot，通过利用最先进的大语言模型，扩展其能力以理解结构化输入并进行深入推理以解决新型ML任务的能力，展示了MLCopilot在解决图像分类，文本分类和表格分类三项任务方面的巨大潜力。

    

    机器学习（ML）领域受到了广泛的应用，因此逐渐引发了将ML应用于特定场景的需求，但实现起来耗时且不易。 自动化解决ML任务（例如AutoML）的主要方法通常耗费时间且难以理解。 而与之相反，虽然人类工程师具有理解任务和推理解决方案的难以置信的能力，但他们的经验和知识往往不充分且难以借助定量方法利用。 在本文中，我们旨在通过引入一种新颖的框架MLCopilot来弥合机器智能和人类知识之间的差距，该框架利用最先进的LLM来开发新型任务的ML解决方案。 我们展示了扩展LLM的能力以理解结构化输入并进行深入推理以解决新型ML任务的可能性。 经过一些专门设计后，我们发现LLM可以（i）从人类编写的文件中观察现有知识，（ii）制定解决ML任务的具体步骤。 我们对图像分类，文本分类和表格分类三项任务进行了实验，证明了MLCopilot在解决实际ML问题方面的巨大潜力。

    The field of machine learning (ML) has gained widespread adoption, leading to a significant demand for adapting ML to specific scenarios, which is yet expensive and non-trivial. The predominant approaches towards the automation of solving ML tasks (e.g., AutoML) are often time consuming and hard to understand for human developers. In contrast, though human engineers have the incredible ability to understand tasks and reason about solutions, their experience and knowledge are often sparse and difficult to utilize by quantitative approaches. In this paper, we aim to bridge the gap between machine intelligence and human knowledge by introducing a novel framework MLCopilot, which leverages the state-of-the-art LLMs to develop ML solutions for novel tasks. We showcase the possibility of extending the capability of LLMs to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks. And we find that, after some dedicated design, the LLM can (i) observe from the exi
    
[^356]: ChatGPT是一个好的情感分析器吗？一项初步研究。

    Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study. (arXiv:2304.04339v1 [cs.CL])

    [http://arxiv.org/abs/2304.04339](http://arxiv.org/abs/2304.04339)

    本文对ChatGPT作为情感分析器进行了初步评估，包括标准评估、极性转移评估、开放域评估和情感推理评估，共涉及18个数据集和5个情感分析任务。与经过微调的BERT和最先进的模型进行了对比，并进行了人工评估和案例研究。

    

    最近，ChatGPT在研究和公众的关注下受到了极大的关注。我们特别想知道它是否可以作为通用情感分析器。为此，在这项工作中，我们对ChatGPT在文本中包含的意见、情感和情绪的理解进行了初步评估。具体而言，我们在四个设置下进行评估，包括标准评估、极性转移评估、开放域评估和情感推理评估。以上评估涉及18个基准数据集和5个代表性情感分析任务，我们将ChatGPT与经过微调的BERT和相应的最先进模型进行了对比，并在末端任务上进行了评估。此外，我们还进行了人工评估，并展示了一些定性案例研究以深入理解其情感分析能力。

    Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly curious about whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a preliminary evaluation of ChatGPT on the understanding of opinions, sentiments, and emotions contained in the text. Specifically, we evaluate it in four settings, including standard evaluation, polarity shift evaluation, open-domain evaluation, and sentiment inference evaluation. The above evaluation involves 18 benchmark datasets and 5 representative sentiment analysis tasks, and we compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on end-task. Moreover, we also conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities.
    
[^357]: AI生成的文本是否可靠地检测出来？

    Can AI-Generated Text be Reliably Detected?. (arXiv:2303.11156v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.11156](http://arxiv.org/abs/2303.11156)

    本研究通过实证和理论分析表明，在实际场景中，几种AI文本检测器不可靠。改写攻击可以破解多种检测器，包括水印方案、神经网络检测器和零样本分类器。即使是最好的检测器，随着语言模型的进一步提升，性能也会下降。因此，AI生成的文本的可靠检测仍然是一个挑战。

    

    本文从实证和理论两个方面表明，在实际场景中，几种AI文本检测器并不可靠。从实践上来说，我们证明了轻量级的改写器应用在大型语言模型（LLM）上可以破解一系列的检测器，包括使用水印方案、神经网络检测器和零样本分类器。我们的实验表明，旨在躲避改写攻击的基于检索的检测器仍然容易受到递归改写的攻击。然后，我们提出了一个理论上的不可能结果，指出随着语言模型变得越来越复杂和更擅长模仿人类文本，在最好的检测器性能会下降。对于一个足够先进的语言模型来模仿人类文本，即使最佳的检测器的表现只比随机分类器好上一点点。我们的结果足够概括特定的场景，如改写攻击。

    In this paper, both empirically and theoretically, we show that several AI-text detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of a large language model (LLM), can break a whole range of detectors, including ones using watermarking schemes as well as neural network-based detectors and zero-shot classifiers. Our experiments demonstrate that retrieval-based detectors, designed to evade paraphrasing attacks, are still vulnerable to recursive paraphrasing. We then provide a theoretical impossibility result indicating that as language models become more sophisticated and better at emulating human text, the performance of even the best-possible detector decreases. For a sufficiently advanced language model seeking to imitate human text, even the best-possible detector may only perform marginally better than a random classifier. Our result is general enough to capture specific scenarios such as par
    
[^358]: 基于人类指令拒绝图像再创作的文本-视觉对话可追溯技术

    Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation. (arXiv:2303.05983v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05983](http://arxiv.org/abs/2303.05983)

    本论文构建了两个多模态数据集来验证视觉语言模型在文本-视觉对话任务中的能力，并开发特定规则的监督信号来让系统展示可追溯性。

    

    ChatGPT和GPT-4的成功引起了对多模态对话系统的广泛关注，但缺乏可以验证视觉语言模型在文本-视觉对话任务中多模态生成能力的数据集。为此，本文构建了两个新的多模态数据集：合成CLEVR-ATVC数据集（620K）和手动绘制的Fruit-ATVC数据集（50K），均具有基于视觉和文本的输入和输出。为了让多模态系统能够拒绝人类请求（即展示可追溯性），我们在数据集中开发和并入了特定规则作为监督信号。这允许训练后的VLM在视觉和文本推理后提供yes或no的答案，并附带说明语言为什么无法执行人类指令。我们提出了一个两阶段训练程序来训练图像自编码器和自回归神经网络。

    The recent success of ChatGPT and GPT-4 has drawn widespread attention to multimodal dialogue systems. However, the academia community lacks a dataset that can validate the multimodal generation capabilities of Visual Language Models (VLMs) in textual-visual chat tasks. In this paper, we construct two new multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K), both featuring visual and text-based inputs and outputs. Additionally, to enable the multimodal system to reject human requests (i.e., demonstrate accountability), as in language-based ChatGPT conversations, we develop and incorporate specific rules into the datasets as supervisory signals. This allows the trained VLM to provide a yes or no answer after visual and textual reasoning, accompanied by a language explanation as to why the human instruction cannot be excuted. In our method, we propose a two-state training procedure to train the image auto-encoder and auto-regress
    
[^359]: 关于视觉解释定量评估的一致性

    On The Coherence of Quantitative Evaluation of Visual Explanations. (arXiv:2302.10764v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10764](http://arxiv.org/abs/2302.10764)

    本研究针对常用神经网络解释方法，探究不同评估度量下的表现以及评估方法之间的比较，发现方法的表现经常不一致且选择评估度量至关重要。

    

    近年来，通过视觉解释来证明神经网络预测的方法得到了增强发展。这些解释通常采用热图的形式，为输入图像的每个像素分配一个显著性值，表示像素对标签预测的相关性。为了评估这种解释的质量，已经提出了评估方法。一些这样的评估方法依赖于合成数据集，但这样会引入在更现实的情景下适用性的有限保证。另一些方法依赖于客观评估的度量。但是有关这些评估方法的执行水平的不确定性很大。因此，我们对ImageNet-1k验证集的一个子集进行了全面研究，使用多个评估度量来评估不同的常用神经网络解释方法。我们的研究旨在确定不同的评估设置下各个方法的表现如何，以及不同的评估方法之间的比较如何。我们发现，在所使用的评估度量上，这些方法的表现经常是不一致的，而且在观察的表现中，选择评估度量是至关重要的。

    Recent years have shown an increased development of methods for justifying the predictions of neural networks through visual explanations. These explanations usually take the form of heatmaps which assign a saliency (or relevance) value to each pixel of the input image that expresses how relevant the pixel is for the prediction of a label.  Complementing this development, evaluation methods have been proposed to assess the "goodness" of such explanations. On the one hand, some of these methods rely on synthetic datasets. However, this introduces the weakness of having limited guarantees regarding their applicability on more realistic settings. On the other hand, some methods rely on metrics for objective evaluation. However the level to which some of these evaluation methods perform with respect to each other is uncertain.  Taking this into account, we conduct a comprehensive study on a subset of the ImageNet-1k validation set where we evaluate a number of different commonly-used expla
    
[^360]: 通过深度学习解释壁面边界层湍流

    Explaining wall-bounded turbulence through deep learning. (arXiv:2302.01250v2 [physics.flu-dyn] UPDATED)

    [http://arxiv.org/abs/2302.01250](http://arxiv.org/abs/2302.01250)

    本研究采用深度学习预测了壁面边界层湍流中的速度场，并利用SHAP算法评估了相干结构对预测的重要性。这一过程或有助于解决湍流研究中的难题，为湍流模型的发展提供新思路。

    

    壁面边界层湍流作为一个具有重大科学和技术意义的问题，需要寻求新的视角来解决。本研究首次采用可解释的深度学习方法研究了流场中相干结构之间的相互作用。通过卷积神经网络，利用湍流通道中的瞬时速度场预测了时间内的速度场，然后利用SHapley Additive exPlanations（SHAP）算法对每个结构预测的重要性进行了评估。本研究结果与先前文献观察结果一致，并通过量化雷诺应力结构的重要性，找到了这些结构与流动动力学之间的联系。采用深度学习可解释性的方法可能有助于揭示壁面边界层湍流的长期问题，并为湍流模型的开发提供新的见解。

    Despite its great scientific and technological importance, wall-bounded turbulence is an unresolved problem that requires new perspectives to be tackled. One of the key strategies has been to study interactions among the coherent structures in the flow. Such interactions are explored in this study for the first time using an explainable deep-learning method. The instantaneous velocity field in a turbulent channel is used to predict the velocity field in time through a convolutional neural network. Based on the predicted flow, we assess the importance of each structure for this prediction using the game-theoretic algorithm of SHapley Additive exPlanations (SHAP). This work provides results in agreement with previous observations in the literature and extends them by quantifying the importance of the Reynolds-stress structures, finding a connection between these structures and the dynamics of the flow. The process, based on deep-learning explainability, has the potential to shed light on
    
[^361]: 在复杂的多智能体场景中估计反事实治疗结果的时间变化

    Estimating counterfactual treatment outcomes over time in complex multi-agent scenarios. (arXiv:2206.01900v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.01900](http://arxiv.org/abs/2206.01900)

    本论文提出了一个可解释的反事实循环网络，用于在复杂的多智能体场景中估计干预效果。该模型考虑了时间变化的多智能体关系和协变量反事实预测的复杂结构，能够准确评估个体治疗效果，并提供解释性。

    

    在各种工程和科学领域中，评估多智能体系统中的干预行为（例如，人类何时应该干预自动驾驶系统，何时球员应该传给队友进行好射门）是一项具有挑战性的任务。使用反事实的长期预测来估计个体治疗效果（ITE）是评估此类干预措施的实用方法。然而，大多数传统框架没有考虑到多智能体关系的时间变化和协变量反事实预测的复杂结构，这可能导致ITE的错误评估和解释困难。在这里，我们提出了一个可解释的反事实循环网络，用于估计干预的效果。我们的模型利用图形变分循环神经网络和基于领域知识的计算来进行基于多智能体协变量和结果的长期预测的ITE估计框架，能够确认循环结构。

    Evaluation of intervention in a multi-agent system, e.g., when humans should intervene in autonomous driving systems and when a player should pass to teammates for a good shot, is challenging in various engineering and scientific fields. Estimating the individual treatment effect (ITE) using counterfactual long-term prediction is practical to evaluate such interventions. However, most of the conventional frameworks did not consider the time-varying complex structure of multi-agent relationships and covariate counterfactual prediction. This may lead to erroneous assessments of ITE and difficulty in interpretation. Here we propose an interpretable, counterfactual recurrent network in multi-agent systems to estimate the effect of the intervention. Our model leverages graph variational recurrent neural networks and theory-based computation with domain knowledge for the ITE estimation framework based on long-term prediction of multi-agent covariates and outcomes, which can confirm the circu
    

