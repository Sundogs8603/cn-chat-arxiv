# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Quantum circuit synthesis with diffusion models.](http://arxiv.org/abs/2311.02041) | 该论文提出了一种利用扩散模型进行量子电路合成的方法，通过使用生成式机器学习模型，可以在基于门的量子电路中产生所需的量子操作，而且能够绕过经典模拟量子动力学的指数级开销。实验证明该模型在纠缠生成和酉编译等任务中表现优秀，并支持扩展功能以适应不同的量子设备约束条件。 |
| [^2] | [APRICOT: Acuity Prediction in Intensive Care Unit (ICU): Predicting Stability, Transitions, and Life-Sustaining Therapies.](http://arxiv.org/abs/2311.02026) | APRICOT是一种基于Transformer的神经网络，用于在ICU患者中实时预测敏感度状态，并在多个数据集上进行了广泛验证。 |
| [^3] | [Active Reasoning in an Open-World Environment.](http://arxiv.org/abs/2311.02018) | 该论文提出了一个交互式的开放世界环境$Conan$，用于评估在不完全信息的情况下的主动推理能力。通过主动探索和多轮预测推理，代理可以利用新发现和现有信息解决问题。 |
| [^4] | [DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network for Food Deliveries.](http://arxiv.org/abs/2311.02017) | DeliverAI是一个基于强化学习的分布式路径共享网络，用于优化食品配送的多目标优化问题，以减少配送成本并提高消费者满意度。 |
| [^5] | [Obtaining Explainable Classification Models using Distributionally Robust Optimization.](http://arxiv.org/abs/2311.01994) | 本论文介绍了一种利用分布鲁棒优化获取可解释的分类模型的方法，通过构建稀疏的规则集合来同时解决规则集的稀疏性和预测准确性之间的权衡，从而保证泛化性能并降低计算成本。 |
| [^6] | [RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches.](http://arxiv.org/abs/2311.01977) | 通过使用粗糙的轨迹草图，我们提出了一种名为RT-Trajectory的策略条件方法，在泛化到新任务方面取得了成功。 |
| [^7] | [The language of prompting: What linguistic properties make a prompt successful?.](http://arxiv.org/abs/2311.01967) | 本文研究了不同大小的预训练和指导调优的语言模型在语言结构上有所不同的提示上的表现，结果显示语言模型在性能上对提示的语言属性有较高的敏感性。 |
| [^8] | [Don't Make Your LLM an Evaluation Benchmark Cheater.](http://arxiv.org/abs/2311.01964) | 本文讨论了不恰当使用评估基准和误导性解释评估结果的潜在风险和影响，特别关注了基准泄漏现象。 |
| [^9] | [Assessing Fidelity in XAI post-hoc techniques: A Comparative Study with Ground Truth Explanations Datasets.](http://arxiv.org/abs/2311.01961) | 本研究通过引入具有可靠的解释真相的图像数据集，对当前最先进的XAI方法进行了公平客观的比较，结果显示基于反向传播的XAI方法相对于其他方法具有更高的准确性和可靠性，但会生成更多的噪声唤醒图。 |
| [^10] | [A Quantitative Autonomy Quantification Framework for Fully Autonomous Robotic Systems.](http://arxiv.org/abs/2311.01939) | 本文提出了一个基于任务需求的自主度评估框架，主要关注完全自主模式，通过确定机器人任务特性，推导出自主度的三个度量标准，并将自主度分为自主度水平和自主度程度两部分。 |
| [^11] | [Supermind Ideator: Exploring generative AI to support creative problem-solving.](http://arxiv.org/abs/2311.01937) | Supermind Ideator是一个使用生成式人工智能技术的系统，旨在支持创造性问题解决。该系统使用大型语言模型和专门设计的用户界面，能够提供新的创意想法，并帮助用户使用创造性问题解决技术。该系统还可以应用于各种问题，并特别适用于生成关于设计人组或人机组合的创新想法。 |
| [^12] | [GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling.](http://arxiv.org/abs/2311.01927) | GateLoop是一种完全数据控制的线性递归序列模型，优于现有模型，可以提供数据控制的相对位置信息给Attention。 |
| [^13] | [Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review.](http://arxiv.org/abs/2311.01918) | 本综述文章探讨了大型语言模型（LLMs）在医学中的应用和意义。LLMs在知识检索、研究支持、临床工作流自动化和诊断辅助方面具有巨大潜力，尤其是多模态LLMs可以处理医学影像和电子健康记录等多样化数据类型以增强诊断能力。 |
| [^14] | [Enhancing Functional Data Analysis with Sequential Neural Networks: Advantages and Comparative Study.](http://arxiv.org/abs/2311.01875) | 本论文研究了如何利用顺序神经网络（SNNs）增强功能数据分析（FDA），并通过与常见FDA回归模型的比较分析和实际数据分析证明了SNNs的有效性。 |
| [^15] | [Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval.](http://arxiv.org/abs/2311.01870) | Multi-EuP是一个新的多语言基准数据集，涵盖了来自欧洲议会的22K个多语言文档，旨在研究信息检索中的公平性，包括语言和人口偏见。它提供了真实的多语言语料库和跨语言相关性评判，并提供了与文档相关的丰富人口统计信息，可用于评估单语和多语信息检索。 |
| [^16] | [Towards Concept-Aware Large Language Models.](http://arxiv.org/abs/2311.01866) | 本文研究了概念在语言模型中的作用，并探讨了开发概念感知语言模型的方法。通过预训练LLMs或使用现有LLMs的输出，我们证明了这种方法更好地符合人类直觉并改善了预测的鲁棒性。 |
| [^17] | [SortNet: Learning To Rank By a Neural-Based Sorting Algorithm.](http://arxiv.org/abs/2311.01864) | SortNet是一种使用神经网络作为比较器来进行自适应排序的算法，通过迭代过程构建训练集，根据成对项目之间的排序示例来训练神经网络。 |
| [^18] | [FAME: Flexible, Scalable Analogy Mappings Engine.](http://arxiv.org/abs/2311.01860) | 这项工作提出了一个灵活可扩展的类比映射引擎，通过自动提取常识表示，并使用这些表示来确定实体之间的映射关系。与以往方法不同的是，该引擎可以处理部分类比，并提供新实体的建议。实验证明其在经典2x2类比问题上的准确率为81.2％，在更大的问题上为77.8％，此外，该引擎还优于人类表现。 |
| [^19] | [A Neural Radiance Field-Based Architecture for Intelligent Multilayered View Synthesis.](http://arxiv.org/abs/2311.01842) | 该论文提出了基于神经辐射场的智能多层视图合成架构，为移动自组网中的数据包路由问题提供了红火蚁优化路由选择策略。 |
| [^20] | [AFPQ: Asymmetric Floating Point Quantization for LLMs.](http://arxiv.org/abs/2311.01792) | AFPQ提出了一种面向LLMs的非对称浮点量化方法，通过为正值和负值设置不同的比例尺，显著提高了准确性，并且可以与其他量化方法相结合，无需额外存储空间。 |
| [^21] | [TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine.](http://arxiv.org/abs/2311.01786) | TCM-GPT是一种用于传统中医领域适应的大型语言模型的高效预训练方法，通过构建一个传统中医专用语料库进行预训练，取得了良好的效果。 |
| [^22] | [Modeling the Uncertainty with Maximum Discrepant Students for Semi-supervised 2D Pose Estimation.](http://arxiv.org/abs/2311.01770) | 本文提出了一个用于半监督姿态估计的简单而高效的框架，从建模伪标签的不确定性的角度来评估伪标签的质量，并通过构建两个最大差异学生来推动老师生成不同的决策边界。实验结果表明了该方法的有效性。 |
| [^23] | [Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for Indonesian Language.](http://arxiv.org/abs/2311.01757) | 本研究旨在针对印度尼西亚语，利用生成式预训练语言模型实现多任务生成式基于方面的情感分析方法，并开发了Indo LEGO-ABSA模型。 |
| [^24] | [RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization.](http://arxiv.org/abs/2311.01753) | RiskQ是一种解决多智能体强化学习中风险敏感协调要求的方法，通过引入风险敏感的个体-全局最大（RIGM）原则和建模联合回报分布实现价值因子分解。 |
| [^25] | [Energy Efficiency Optimization for Subterranean LoRaWAN Using A Reinforcement Learning Approach: A Direct-to-Satellite Scenario.](http://arxiv.org/abs/2311.01743) | 本文介绍了一种地下LoRaWAN网络能效优化的强化学习方法，该方法采用多智能体深度Q网络和多智能体优势演员-评论家算法来分配扩频因子，以最小化共享扩频因子干扰并优化系统的能效。 |
| [^26] | [Flexible Error Mitigation of Quantum Processes with Data Augmentation Empowered Neural Model.](http://arxiv.org/abs/2311.01727) | 提出了一种数据增强强化的神经模型，该模型可以灵活地缓解量子过程中的各种噪声，并展示了在不同类型量子过程中与先前方法相比的优越性能。 |
| [^27] | [Towards Calibrated Robust Fine-Tuning of Vision-Language Models.](http://arxiv.org/abs/2311.01723) | 本文提出了一个名为校准鲁棒微调（CaRot）的方法，针对视觉语言模型在分布变化下的校准问题。通过该方法，作者成功提高了预训练模型的校准性能和鲁棒性能。 |
| [^28] | [An Empirical Study of Benchmarking Chinese Aspect Sentiment Quad Prediction.](http://arxiv.org/abs/2311.01713) | 本研究构建了两个大规模的中国ASQP数据集，对生成式预训练变压器（GPT）系列模型在ASQP上的性能进行了评估，并展示了改进ASQP技术和提高GPT性能的重要性。 |
| [^29] | [Data-Free Distillation of Language Model by Text-to-Text Transfer.](http://arxiv.org/abs/2311.01689) | 本文提出了一种无数据知识蒸馏（DFKD）框架，即DFKD-T$^{3}$，利用预训练的生成式语言模型作为数据生成器，将通用领域语料库转化为压缩友好的任务数据。实验证明该方法能够提升各种下游任务的蒸馏性能。 |
| [^30] | [The R.O.A.D. to precision medicine.](http://arxiv.org/abs/2311.01681) | 我们提出了一种预后分层匹配框架，将观察数据转化为类似于随机试验，为精准医学铺平了道路。通过纠正患者特征的影响，我们创建了临床直观的治疗建议。通过应用于胃肠间质瘤（GIST）的观察数据，我们证明了这些建议的优于专家建议。 |
| [^31] | [DialogBench: Evaluating LLMs as Human-like Dialogue Systems.](http://arxiv.org/abs/2311.01677) | 本文提出了DialogBench，一个对话评估基准，用于评估LLMs作为人类对话系统的能力。通过对28个LLMs的广泛测试，发现指导微调对提升性能效果显著。 |
| [^32] | [MineSegSAT: An automated system to evaluate mining disturbed area extents from Sentinel-2 imagery.](http://arxiv.org/abs/2311.01676) | 本文介绍了MineSegSAT，一个使用Sentinel-2数据和SegFormer深度学习分割架构的模型，用于预测矿产开采现场受到影响的区域。该模型利用先进的空间理解能力进行准确的土地覆盖分类。通过研究不同损失函数的效果，验证了该模型的可靠性和有效性。 |
| [^33] | [Deep Learning-driven Community Resilience Rating based on Intertwined Socio-Technical Systems Features.](http://arxiv.org/abs/2311.01661) | 这项研究提出了一个基于三层深度学习模型的社区韧性评估方法，通过捕捉社区社会技术系统的异质特征和非线性相互作用，实现对社区韧性的全面评估。 |
| [^34] | [MARRS: Multimodal Reference Resolution System.](http://arxiv.org/abs/2311.01650) | MARRS是一个在设备上运行的多模态参考解析系统，能够处理对话式、视觉和背景上下文，并通过不同的机器学习模型实现上下文查询的处理。这个系统能够在保护用户隐私的同时理解上下文。 |
| [^35] | [RTP: Rethinking Tensor Parallelism with Memory Deduplication.](http://arxiv.org/abs/2311.01635) | RTP是一种以内存去重为重点的创新方法，能够在分布式训练环境中优化内存消耗并实现与分布式数据并行性相当的性能。 |
| [^36] | ["Close...but not as good as an educator." -- Using ChatGPT to provide formative feedback in large-class collaborative learning.](http://arxiv.org/abs/2311.01634) | 在大型协作学习中，使用ChatGPT为学习者提供形成性反馈，受访者对反馈持积极评价，但只有少数小组使用反馈来改进评估计划。 |
| [^37] | [VQPy: An Object-Oriented Approach to Modern Video Analytics.](http://arxiv.org/abs/2311.01623) | VQPy是一种面向对象的视频分析方法，它使用Python变体作为前端，并具有可扩展的后端，可以自动构建和优化基于视频对象的处理流程。 |
| [^38] | [Look-Ahead Selective Plasticity for Continual Learning of Visual Tasks.](http://arxiv.org/abs/2311.01617) | 提出了一种新的持续学习机制，利用对比表示学习来减少灾难性遗忘，通过观察冗余诱导能力，识别并保留对神经网络转移能力最有贡献的参数，以实现在任务边界时的选择性可塑性。 |
| [^39] | [Responsible Emergent Multi-Agent Behavior.](http://arxiv.org/abs/2311.01609) | 这篇论文研究了负责任的新兴多智能体行为，探讨了在可解释性和公正性的框架下如何理解和塑造多智能体学习，并强调了人类问题解决的多智能体的本质特征。 |
| [^40] | [DRNet: A Decision-Making Method for Autonomous Lane Changingwith Deep Reinforcement Learning.](http://arxiv.org/abs/2311.01602) | DRNet是一种基于深度强化学习的决策框架，可以帮助自动驾驶车辆进行车道变换，并考虑到周围车辆的驾驶风格，实现安全的决策策略。 |
| [^41] | [Domain Adaptive Graph Neural Networks for Constraining Cosmological Parameters Across Multiple Data Sets.](http://arxiv.org/abs/2311.01588) | 该论文研究了通过领域适应图神经网络对宇宙学参数进行约束的方法。通过利用GNNs捕捉宇宙学信息和使用最大均值差异进行领域适应，该方法在不同数据集上具有较好的泛化能力。 |
| [^42] | [MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition.](http://arxiv.org/abs/2311.01580) | MetaReVision是一种检索增强的元学习模型，通过使用检索到的基本概念作为支持集合来快速学习和识别新的图像基础组合概念。 |
| [^43] | [Improving Fairness using Vision-Language Driven Image Augmentation.](http://arxiv.org/abs/2311.01573) | 本文提出了一种基于视觉语言驱动的图像增强方法，以改善训练深度学习模型中的公平性问题。通过学习和应用可解释的路径来编辑受保护特征，该方法成功减轻了数据中的相关性，提高了数据集的公平性。 |
| [^44] | [Market Concentration Implications of Foundation Models.](http://arxiv.org/abs/2311.01550) | 对基础模型市场的结构进行分析，发现最强大的模型趋向自然垄断，需要双管齐下的监管响应来确保市场竞争和模型质量标准，以最大程度地对社会福利做出贡献。 |
| [^45] | [Open-Set Object Recognition Using Mechanical Properties During Interaction.](http://arxiv.org/abs/2311.01540) | 提出了一种利用机械性能进行开放式对象识别的框架，通过利用已知对象的知识来估计聚类中心和大小的聚类算法，通过实验证明该框架能够更好地识别对象，并且我们的聚类算法表现更好。 |
| [^46] | [NOD-TAMP: Multi-Step Manipulation Planning with Neural Object Descriptors.](http://arxiv.org/abs/2311.01530) | NOD-TAMP是一个基于TAMP的框架，利用神经物体描述符来解决复杂操纵任务中的泛化问题，通过从少量人类演示中提取轨迹并进行调整，有效解决了长时程任务的挑战，并在模拟环境中优于现有方法。 |
| [^47] | [The Behavior of Large Language Models When Prompted to Generate Code Explanations.](http://arxiv.org/abs/2311.01490) | 本论文研究了大型语言模型在生成代码解释时的行为。Java和Python的解释在可读性和词汇密度方面表现一致，但在完整性、简洁性和上下文性方面得分较低。 |
| [^48] | [FedSN: A General Federated Learning Framework over LEO Satellite Networks.](http://arxiv.org/abs/2311.01483) | FedSN是一个通用的联邦学习框架，用于解决在LEO卫星网络中的异构计算和存储能力、有限的上行速率以及模型陈旧等关键挑战。 |
| [^49] | [Relation Extraction from News Articles (RENA): A Tool for Epidemic Surveillance.](http://arxiv.org/abs/2311.01472) | RENA是一种基于浏览器的关系提取工具，用于从英语新闻文章中提取与传染病相关的关键实体和语义关系，为流行病监测提供实时解析和关键信息提取的能力。 |
| [^50] | [Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI.](http://arxiv.org/abs/2311.01463) | 这篇论文描述了在医疗人工智能中创建可靠、可信和无偏置的LLM模型的关键要素，着重于量化、验证和缓解幻觉问题，并讨论了LLM在医疗领域的未来发展。 |
| [^51] | [FacadeNet: Conditional Facade Synthesis via Selective Editing.](http://arxiv.org/abs/2311.01240) | FacadeNet是一种深度学习方法，通过条件生成对抗网络实现了从不同视角合成建筑立面图像，并通过引入选择性编辑模块，实现了对视角相关元素进行精确修改。实验结果表明，该方法在建筑立面生成方面具有领先的性能。 |
| [^52] | [Ultra-Efficient On-Device Object Detection on AI-Integrated Smart Glasses with TinyissimoYOLO.](http://arxiv.org/abs/2311.01057) | 本文介绍了在智能眼镜上实现超高效设备内目标检测的设计和实施，利用新型低功耗处理器实现小型机器学习算法，以便在具有小尺寸和有限电池容量的智能眼镜上实现长时间连续运行。 |
| [^53] | [Improving Interpersonal Communication by Simulating Audiences with Language Models.](http://arxiv.org/abs/2311.00687) | 本论文提出了一个基于大型语言模型（LLM）模拟的框架，通过探索解决方案空间、生成沟通候选以及模拟受众反应，来改善人际沟通。通过评估八个涵盖人际沟通基本过程的场景，展示了该框架的有效性。 |
| [^54] | [CapsFusion: Rethinking Image-Text Data at Scale.](http://arxiv.org/abs/2310.20550) | CapsFusion是一个先进的框架，通过利用大型语言模型整合和细化来自网络图像-文本对和合成字幕的信息，提供了更高质量、更可扩展的多模态预训练数据。 |
| [^55] | [Meta Learning for Multi-View Visuomotor Systems.](http://arxiv.org/abs/2310.20414) | 该论文提出了一种新的方法，使用元学习来快速适应多视图运动系统，实验结果表明可以显著减少所需的新训练周期数量。 |
| [^56] | [A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging.](http://arxiv.org/abs/2310.20381) | 本文对GPT-4V在医学影像中的多模态能力进行了全面研究和评估，发现其在生成描述性报告和医学VQA方面有潜力，但在某些评估指标上仍需改进。 |
| [^57] | [Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization.](http://arxiv.org/abs/2310.20033) | 本文提出了一种使用ChatGPT来生成高质量反馈数据以改善临床笔记总结的事实一致性的新方法。 |
| [^58] | [JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music Generation.](http://arxiv.org/abs/2310.19180) | JEN-1 Composer是一个统一的框架，能够以高保真、灵活的方式生成多音轨音乐。 |
| [^59] | [Improving Intrinsic Exploration by Creating Stationary Objectives.](http://arxiv.org/abs/2310.18144) | 该论文提出了一个新的方法：通过创建固定目标，将原始的非固定奖励转化为固定奖励，从而改善了强化学习中的内在探索。 |
| [^60] | [Managing AI Risks in an Era of Rapid Progress.](http://arxiv.org/abs/2310.17688) | 在人工智能快速进展的时代，我们提出了管理即将到来的先进人工智能系统所带来的风险的优先事项。 |
| [^61] | ["Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters.](http://arxiv.org/abs/2310.09219) | 本文对LLM生成的推荐信中的性别偏见进行了细致的研究，并设计了评估方法来展现通过语言风格和词汇内容来体现的性别偏见。 |
| [^62] | [Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods.](http://arxiv.org/abs/2310.05619) | 本文提出了一种通过分析归因分数的连续属性来确定应显示的最佳 k 个标记的动态 Top-k 估计方法，用于整合特征归因方法之间的分歧。实验证明，动态 k 主要改进了集成梯度和 GradientXInput 的表现，为人类解释提供了具有信息价值的归因信号。 |
| [^63] | [Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems.](http://arxiv.org/abs/2310.05280) | 这项研究评估了对话系统中的人格偏见对社交偏见的影响，并建立了一个综合评估框架来衡量不同人格采用下的偏见程度。 |
| [^64] | [Learning Separable Hidden Unit Contributions for Speaker-Adaptive Lip-Reading.](http://arxiv.org/abs/2310.05058) | 本文提出了一种用于适应个体的唇读方法，通过学习可分离的隐藏单元贡献，实现了对浅层和深层的不同处理，并利用个体的特征来自适应地增强或抑制唇读特征，提高唇读的性能和鲁棒性。 |
| [^65] | [Hard View Selection for Contrastive Learning.](http://arxiv.org/abs/2310.03940) | 本文提出了一种Easy、无需学习但强大的Hard View Selection策略，通过选择更难的样本，提高了对比学习模型的性能。 |
| [^66] | [EGOFALLS: A visual-audio dataset and benchmark for fall detection using egocentric cameras.](http://arxiv.org/abs/2309.04579) | 这项研究提出了一种使用自我中心摄像头进行摔倒检测的方法，并构建了一个新的视听数据集。通过迟决策融合将音频和视觉信息相结合可以提高检测性能。 |
| [^67] | [Automating Behavioral Testing in Machine Translation.](http://arxiv.org/abs/2309.02553) | 本文提出了一种利用大型语言模型自动生成源句子的方法，以测试机器翻译模型在多种情况下的行为。通过对多个机器翻译系统应用该方法，发现在测试结果与传统准确率度量存在差异的情况下，仍可观察到一致的趋势。 |
| [^68] | [BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior.](http://arxiv.org/abs/2309.01865) | BigFUSE是一种全局上下文感知的图像融合方法，通过考虑光子传播的全局影响和局部图像质量，稳定了双视图光片荧光显微镜中的图像融合。 |
| [^69] | [From SMOTE to Mixup for Deep Imbalanced Classification.](http://arxiv.org/abs/2308.15457) | 本研究提出了一种从SMOTE到Mixup的方法，用于深度不平衡分类。通过对SMOTE进行改进，并结合Mixup技术，我们构建了一个统一的数据增强框架。研究表明，Mixup技术通过实现多数类和少数类之间的不平衡间隙来改善泛化能力。我们还提出了一种新颖的基于边界的Mixup技术，更明确地实现了不平衡间隙。实验结果表明我们的方法在多个数据集上取得了最好的性能。 |
| [^70] | [ChatGPT for GTFS: From Words to Information.](http://arxiv.org/abs/2308.02618) | 本研究探索了使用ChatGPT语言模型从GTFS数据中检索信息的可行性，验证了ChatGPT（GPT-3.5）在GTFS规范理解和信息提取方面的能力。程序合成方法在信息检索任务中表现出更高的准确率，为解决GTFS数据信息获取问题提供了一种有效的方法。 |
| [^71] | [An Empirical Study on Fairness Improvement with Multiple Protected Attributes.](http://arxiv.org/abs/2308.01923) | 本文通过广泛研究，发现对于单个保护属性的公平性改善会大大降低对未考虑保护属性的公平性，但在多属性模式下可以保持准确性。 |
| [^72] | [General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications.](http://arxiv.org/abs/2307.14283) | 这里是中文总结出的一句话要点：本论文讨论了通用目的人工智能系统（GPAIS）的性质、定义、分类和开放挑战，并提出了一种新的定义，允许根据其性质和限制逐步区分GPAIS的类型。 |
| [^73] | [Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information.](http://arxiv.org/abs/2307.08964) | 本论文提出了一种使用景观替代品的学习方法，旨在解决部分信息下数学优化问题中的挑战。这种方法可以通过学习优化器来加速优化过程，并且能够处理问题的不确定性。 |
| [^74] | [An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient.](http://arxiv.org/abs/2307.08873) | 本研究提出了一种风险厌恶策略梯度的替代方法，通过使用基尼离差来替代方差，缓解了方差方法的局限性，并在实证评估中取得了高回报和低风险的成果。 |
| [^75] | [LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning.](http://arxiv.org/abs/2307.02345) | 本研究通过研究在线和离线增强学习中 Bellman 近似误差的分布发现，Bellman 误差符合逻辑分布。基于这一发现，本研究提出了一种使用 Logistic 最大似然函数作为替代方法的方案，并通过实验证明了其有效性。 |
| [^76] | [What Truly Matters in Trajectory Prediction for Autonomous Driving?.](http://arxiv.org/abs/2306.15136) | 在自动驾驶系统中，轨迹预测的准确性在固定数据集上表现很好，但在实际驾驶场景中却存在显著差异。现有的评估方法忽视了动力学差距和计算效率对预测结果的影响。 |
| [^77] | [Guiding Language Models of Code with Global Context using Monitors.](http://arxiv.org/abs/2306.10763) | 本文提出了一种使用监视器引导全局上下文的方法来指导代码语言模型，在处理类型、功能或API等全局上下文时，能够提高代码语言模型的性能和准确性。 |
| [^78] | [Fine-Tuning Language Models with Advantage-Induced Policy Alignment.](http://arxiv.org/abs/2306.02231) | 本论文提出了一种新算法APA，其采用优势诱导策略对齐用于强化学习语言模型。相对于传统方法（PPO），APA在语言任务中表现更好，避免了模型的崩溃与不稳定性。 |
| [^79] | [Doubly Robust Self-Training.](http://arxiv.org/abs/2306.00265) | 本文提出了一种双重稳健自我训练算法，可以在伪标签不准确和完全准确时分别采取不同的训练策略，实现有效的半监督学习。实验结果表明，该算法在ImageNet和nuScenes数据集上均比标准自我训练总结更好。 |
| [^80] | [Grammar Prompting for Domain-Specific Language Generation with Large Language Models.](http://arxiv.org/abs/2305.19234) | 本文提出了一种基于语法提示的方法，使用专用的语法来增强示例，为大型语言模型（LLM）在特定领域的语言生成任务中使用外部知识和特定约束条件进行上下文学习。 |
| [^81] | [Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection.](http://arxiv.org/abs/2305.18381) | 研究提出了一种基于选择最有价值的样本的方法，以扩展现有的蒸馏算法，从而更好地利用训练样本，显著降低训练成本，拓展对更大更多元化数据集的数据集蒸馏，并持续提高性能。 |
| [^82] | [A Unified Approach for Maximizing Continuous DR-submodular Functions.](http://arxiv.org/abs/2305.16671) | 本文提出了一种适用于一系列设置和 Oracle 访问类型的统一方法，用于最大化连续 DR-submodular 函数，为 16 种情况中的 9 种提供了新的/改进的结果，并且针对基于随机函数值的 Oracle 取得了第一个适用于随机 DR-submodular 函数的后悔界限。 |
| [^83] | [Are Diffusion Models Vision-And-Language Reasoners?.](http://arxiv.org/abs/2305.16397) | 本文针对扩散-语言图像生成模型进行转换和评估，介绍了生成-鉴别评估基准(GDBench)基于7个视觉语言复杂任务，并发现转换后的模型在组合性任务方面的表现优于CLIP，通过微调可提高其组合性能。 |
| [^84] | [Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment.](http://arxiv.org/abs/2305.13669) | 本文提出了MixAlign框架，通过与用户和知识库交互，实现自动的问题-知识对齐，从而解决了语言模型因无法正确理解问题和知识而导致的幻觉问题。 |
| [^85] | [Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference.](http://arxiv.org/abs/2305.13484) | Flover是一种用于自回归模型并行推断的时间融合框架，解决了并行性不足和灵活性差的问题，可以实现更加高效的推断性能。 |
| [^86] | [Causal Discovery with Missing Data in a Multicentric Clinical Study.](http://arxiv.org/abs/2305.10050) | 本文扩展了最新的因果发现算法，利用专家知识从多中心临床研究的缺失数据中分析了不同缺失机制对恢复的因果图的影响，验证了所恢复因果图的临床相关性，并用图形分离来验证因果通路，讨论了因果图的拟合度和从临床决策角度的一致性。 |
| [^87] | [Learning Decision Trees with Gradient Descent.](http://arxiv.org/abs/2305.03515) | 本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。 |
| [^88] | [DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction.](http://arxiv.org/abs/2304.11015) | DIN-SQL通过将复杂的文本到SQL任务分解为子问题，并将这些子问题的解决方案馈入到大型语言模型中，显著提高了它们的表现，使准确性超过了当前最先进的技术。 |
| [^89] | [OpenAGI: When LLM Meets Domain Experts.](http://arxiv.org/abs/2304.04370) | 基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。 |
| [^90] | [Why think step-by-step? Reasoning emerges from the locality of experience.](http://arxiv.org/abs/2304.03843) | 本文通过语言模型研究何时以及为什么推理是有帮助的，测试推理在训练数据由相互影响强烈的局部变量集群组成时是否有效。通过一步步的推理，能够将准确的局部推理链接在一起，以估算在训练中没有同时观察到的变量之间的关系。 |
| [^91] | [Roots and Requirements for Collaborative AI.](http://arxiv.org/abs/2303.12040) | 论文探讨了AI协同合作的历史和要求，是协同AI研究的动机和背景。 |
| [^92] | [xASTNN: Improved Code Representations for Industrial Practice.](http://arxiv.org/abs/2303.07104) | xASTNN是一种基于极端抽象语法树（AST）的神经网络，旨在将深度学习技术推广到工业实践中。它的优点包括适用于不同编程语言和实际场景，不需要复杂的数据预处理，以及提供了三个设计来保证其有效性。 |
| [^93] | [SEGA: Instructing Text-to-Image Models using Semantic Guidance.](http://arxiv.org/abs/2301.12247) | 本论文介绍了一种称为SEGA的语义引导方法，用于指导文本到图像模型的生成过程。通过与扩散过程的互动，SEGA可以灵活地在语义方向上引导模型的生成，实现细微和广泛的编辑以及优化整体艺术构思。实验证明SEGA在多种任务和生成架构上都表现出了出色的多功能性、灵活性和改进。 |
| [^94] | [Tracr: Compiled Transformers as a Laboratory for Interpretability.](http://arxiv.org/abs/2301.05062) | Tracr是一个编译器，将可读性强的程序编译成标准的仅解码变压器模型，该编译模型的已知结构可以用于设计实验和评估可解释方法。 |
| [^95] | [Multimodal Prototype-Enhanced Network for Few-Shot Action Recognition.](http://arxiv.org/abs/2212.04873) | 该论文提出了一种多模态原型增强网络(MORN)用于少样本动作识别，通过利用标签文本的语义信息来增强原型，具有较好的性能表现。 |
| [^96] | [From Understanding Genetic Drift to a Smart-Restart Mechanism for Estimation-of-Distribution Algorithms.](http://arxiv.org/abs/2206.09090) | 这篇论文介绍了一种基于智能重启机制的分布估计算法，该算法可以在基因漂变风险高的情况下停止运行，并寻找良好的参数范围以运行EDA，从而提高性能。 |
| [^97] | [Stabilizing the LIF Neuron Training.](http://arxiv.org/abs/2202.00282) | 该论文研究了稳定LIF神经元训练的方法，通过实验和理论分析，确定了在不同任务和网络中选择最佳替代梯度的稳定性与效果的关系，减少了对超参数搜索的需求。 |
| [^98] | [Graph Neural Diffusion Networks for Semi-supervised Learning.](http://arxiv.org/abs/2201.09698) | 提出了一种名为 GND-Nets 的图神经网络，利用浅层网络和局部、全局邻域信息来解决图半监督学习中的过度平滑和欠平滑问题。 |
| [^99] | [Feature-Attending Recurrent Modules for Generalization in Reinforcement Learning.](http://arxiv.org/abs/2112.08369) | "Feature-Attending Recurrent Modules" (FARM)是一种学习状态表示的体系结构，通过特征注意机制来捕捉空间和时间规律性，从而改善强化学习代理的泛化能力。 |
| [^100] | [On minimizers and convolutional filters: theoretical connections and applications to genome analysis.](http://arxiv.org/abs/2111.08452) | 该论文通过对哈希函数属性进行数学分析，发现在分类字母表上的序列分析中，使用随机高斯初始化的卷积滤波器和最大池化等价于选择一种最小化器排序，能够有效提取与其他最小化器距离较近但与序列中的k-mer相距较远的重要特征。 |
| [^101] | [Numerical influence of ReLU'(0) on backpropagation.](http://arxiv.org/abs/2106.12915) | 本研究研究了ReLU'(0)值对深度学习中反向传播的数值影响，发现在32位精度下会出现显著的变化，而在16位精度下是系统性的。在普通的SGD训练中，选择ReLU'(0) = 0似乎是最有效的。此外，重新调整方法 tend to buffer ReLU'(0)值的影响。 |

# 详细

[^1]: 用扩散模型进行量子电路合成

    Quantum circuit synthesis with diffusion models. (arXiv:2311.02041v1 [quant-ph])

    [http://arxiv.org/abs/2311.02041](http://arxiv.org/abs/2311.02041)

    该论文提出了一种利用扩散模型进行量子电路合成的方法，通过使用生成式机器学习模型，可以在基于门的量子电路中产生所需的量子操作，而且能够绕过经典模拟量子动力学的指数级开销。实验证明该模型在纠缠生成和酉编译等任务中表现优秀，并支持扩展功能以适应不同的量子设备约束条件。

    

    量子计算最近成为一项具有变革性的技术。然而，它所承诺的优势依赖于将量子操作有效地转化为可行的物理实现。在此工作中，我们使用生成式机器学习模型，具体而言是去噪扩散模型（DMs），以促进这种转化。通过文本条件，我们引导模型在基于门的量子电路中产生所需的量子操作。值得注意的是，DMs允许在训练过程中避免经典模拟量子动力学中固有的指数级开销，这是先前机器学习技术中一直存在的瓶颈。我们在两个任务上展示了该模型的能力：纠缠生成和酉编译。该模型在生成新电路方面表现出色，并支持典型的DM扩展，例如掩码和编辑，以使电路生成符合目标量子设备的约束条件。由于其灵活性和泛化能力，我们的方法可以应用于各种量子任务。

    Quantum computing has recently emerged as a transformative technology. Yet, its promised advantages rely on efficiently translating quantum operations into viable physical realizations. In this work, we use generative machine learning models, specifically denoising diffusion models (DMs), to facilitate this transformation. Leveraging text-conditioning, we steer the model to produce desired quantum operations within gate-based quantum circuits. Notably, DMs allow to sidestep during training the exponential overhead inherent in the classical simulation of quantum dynamics -- a consistent bottleneck in preceding ML techniques. We demonstrate the model's capabilities across two tasks: entanglement generation and unitary compilation. The model excels at generating new circuits and supports typical DM extensions such as masking and editing to, for instance, align the circuit generation to the constraints of the targeted quantum device. Given their flexibility and generalization abilities, we
    
[^2]: APRICOT: 重症监护病房(ICU)中的敏感度预测：预测稳定性、转变和维持生命的治疗

    APRICOT: Acuity Prediction in Intensive Care Unit (ICU): Predicting Stability, Transitions, and Life-Sustaining Therapies. (arXiv:2311.02026v1 [cs.AI])

    [http://arxiv.org/abs/2311.02026](http://arxiv.org/abs/2311.02026)

    APRICOT是一种基于Transformer的神经网络，用于在ICU患者中实时预测敏感度状态，并在多个数据集上进行了广泛验证。

    

    ICU中的患者严重程度状态可能会在稳定和不稳定之间迅速变化，有时会导致危及生命的情况。早期检测到恶化可能会导致更及时的干预和更好的生存率。目前的方法依赖于手动的每日评估。已经开发了一些数据驱动的方法，使用死亡率作为ICU中敏感度的代理。然而，这些方法并未整合敏感度状态以确定患者的稳定性或对维持生命治疗的需求。在本研究中，我们提出了APRICOT（重症监护病房中的敏感度预测），一种基于Transformer的神经网络，用于实时预测ICU患者的敏感度状态。我们在三个大型数据集上外部、时间上和前瞻性地开发和广泛验证了APRICOT模型：佛罗里达大学健康中心（UFH）、eICU合作研究数据库（eICU）和重症监护医疗信息市场（MIMIC）-IV。

    The acuity state of patients in the intensive care unit (ICU) can quickly change from stable to unstable, sometimes leading to life-threatening conditions. Early detection of deteriorating conditions can result in providing more timely interventions and improved survival rates. Current approaches rely on manual daily assessments. Some data-driven approaches have been developed, that use mortality as a proxy of acuity in the ICU. However, these methods do not integrate acuity states to determine the stability of a patient or the need for life-sustaining therapies. In this study, we propose APRICOT (Acuity Prediction in Intensive Care Unit), a Transformer-based neural network to predict acuity state in real-time in ICU patients. We develop and extensively validate externally, temporally, and prospectively the APRICOT model on three large datasets: University of Florida Health (UFH), eICU Collaborative Research Database (eICU), and Medical Information Mart for Intensive Care (MIMIC)-IV. T
    
[^3]: 在开放世界环境中的主动推理

    Active Reasoning in an Open-World Environment. (arXiv:2311.02018v1 [cs.AI])

    [http://arxiv.org/abs/2311.02018](http://arxiv.org/abs/2311.02018)

    该论文提出了一个交互式的开放世界环境$Conan$，用于评估在不完全信息的情况下的主动推理能力。通过主动探索和多轮预测推理，代理可以利用新发现和现有信息解决问题。

    

    近年来，视觉语言学习的最新进展通过整合丰富的世界知识，在完整信息的问答数据集上取得了显著的成功。然而，大部分模型是被动的，根据预存的知识回答问题。相比之下，人类具有主动探索、积累和推理的能力，利用新发现和现有信息解决不完全信息的问题。针对这一差距，我们引入了$Conan$，一个交互式的开放世界环境，用于评估主动推理。$Conan$促进主动探索，并促进多轮预测推理，类似于富有挑战性的开放世界环境如Minecraft。与主要依赖单轮推断的指令跟随的之前工作不同，$Conan$迫使代理与周围环境进行主动互动，将新的证据与先前的知识融合，从不完全的观察中阐明事件。

    Recent advances in vision-language learning have achieved notable success on complete-information question-answering datasets through the integration of extensive world knowledge. Yet, most models operate passively, responding to questions based on pre-stored knowledge. In stark contrast, humans possess the ability to actively explore, accumulate, and reason using both newfound and existing information to tackle incomplete-information questions. In response to this gap, we introduce $Conan$, an interactive open-world environment devised for the assessment of active reasoning. $Conan$ facilitates active exploration and promotes multi-round abductive inference, reminiscent of rich, open-world settings like Minecraft. Diverging from previous works that lean primarily on single-round deduction via instruction following, $Conan$ compels agents to actively interact with their surroundings, amalgamating new evidence with prior knowledge to elucidate events from incomplete observations. Our an
    
[^4]: DeliverAI: 强化学习为基础的分布式路径共享网络用于食品配送

    DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network for Food Deliveries. (arXiv:2311.02017v1 [cs.LG])

    [http://arxiv.org/abs/2311.02017](http://arxiv.org/abs/2311.02017)

    DeliverAI是一个基于强化学习的分布式路径共享网络，用于优化食品配送的多目标优化问题，以减少配送成本并提高消费者满意度。

    

    在过去十年中，从生产者到消费者的物品配送经历了显著的增长，并且最近的流行病进一步推动了这一增长。亚马逊生鲜、Shopify、UberEats、InstaCart和DoorDash正在迅速发展，并共享相同的消费品或食品配送业务模式。现有的食品配送方法存在缺陷，因为每次配送都是在最短时间路径上从生产者直接到消费者进行优化。我们观察到，在当前模型下，有很大的减少配送成本的空间。我们将我们的食品配送问题建模为一个多目标优化问题，消费者满意度和配送成本都需要进行优化。受出租车行业中拼车成功的启发，我们提出了DeliverAI - 一种基于强化学习的路径共享算法。与以前的路径共享尝试不同，DeliverAI可以提供实时、时间高效的决策。

    Delivery of items from the producer to the consumer has experienced significant growth over the past decade and has been greatly fueled by the recent pandemic. Amazon Fresh, Shopify, UberEats, InstaCart, and DoorDash are rapidly growing and are sharing the same business model of consumer items or food delivery. Existing food delivery methods are sub-optimal because each delivery is individually optimized to go directly from the producer to the consumer via the shortest time path. We observe a significant scope for reducing the costs associated with completing deliveries under the current model. We model our food delivery problem as a multi-objective optimization, where consumer satisfaction and delivery costs, both, need to be optimized. Taking inspiration from the success of ride-sharing in the taxi industry, we propose DeliverAI - a reinforcement learning-based path-sharing algorithm. Unlike previous attempts for path-sharing, DeliverAI can provide real-time, time-efficient decision-
    
[^5]: 利用分布鲁棒优化获取可解释的分类模型

    Obtaining Explainable Classification Models using Distributionally Robust Optimization. (arXiv:2311.01994v1 [stat.ML])

    [http://arxiv.org/abs/2311.01994](http://arxiv.org/abs/2311.01994)

    本论文介绍了一种利用分布鲁棒优化获取可解释的分类模型的方法，通过构建稀疏的规则集合来同时解决规则集的稀疏性和预测准确性之间的权衡，从而保证泛化性能并降低计算成本。

    

    对于人类用户来说，模型的可解释性对于理解提议分类器如何根据特征值给数据分配标签至关重要。我们研究使用特征值规则集构建的广义线性模型，该模型可以捕捉非线性依赖和交互作用。规则集的稀疏性和预测准确性之间存在固有的权衡。使用现有方法来找到合适的稀疏度选择（例如通过交叉验证）计算成本很高。我们提出了一种新的公式来学习同时解决这些竞争因素的规则集合。通过利用分布鲁棒优化来确保良好的泛化性能，同时保持低计算成本。该公式利用列生成有效地搜索规则集合的空间并构建稀疏的规则集合，与随机森林或Boosting及其变体等技术相比。我们提出了理论结果来推动这一公式的发展。

    Model explainability is crucial for human users to be able to interpret how a proposed classifier assigns labels to data based on its feature values. We study generalized linear models constructed using sets of feature value rules, which can capture nonlinear dependencies and interactions. An inherent trade-off exists between rule set sparsity and its prediction accuracy. It is computationally expensive to find the right choice of sparsity -- e.g., via cross-validation -- with existing methods. We propose a new formulation to learn an ensemble of rule sets that simultaneously addresses these competing factors. Good generalization is ensured while keeping computational costs low by utilizing distributionally robust optimization. The formulation utilizes column generation to efficiently search the space of rule sets and constructs a sparse ensemble of rule sets, in contrast with techniques like random forests or boosting and their variants. We present theoretical results that motivate an
    
[^6]: RT-Trajectory: 通过回顾轨迹草图实现机器人任务的泛化

    RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches. (arXiv:2311.01977v1 [cs.RO])

    [http://arxiv.org/abs/2311.01977](http://arxiv.org/abs/2311.01977)

    通过使用粗糙的轨迹草图，我们提出了一种名为RT-Trajectory的策略条件方法，在泛化到新任务方面取得了成功。

    

    泛化仍然是强大的机器人学习系统的最重要愿景之一。虽然最近提出的方法在泛化到新的物体、语义概念或视觉分布转移方面显示出了潜力，但泛化到新任务仍然具有挑战性。例如，一个在拾取和放置任务上训练的语言条件策略将无法泛化到折叠任务，即使折叠的臂部轨迹与拾取和放置类似。我们的关键观点是，如果我们通过粗糙的轨迹草图来表示任务，这种泛化将变得可行。我们提出了一种使用粗糙轨迹草图的策略条件方法，称为RT-Trajectory，它是实用的、易于指定的，可以使策略有效地执行原本具有挑战性的新任务。

    Generalization remains one of the most important desiderata for robust robot learning systems. While recently proposed approaches show promise in generalization to novel objects, semantic concepts, or visual distribution shifts, generalization to new tasks remains challenging. For example, a language-conditioned policy trained on pick-and-place tasks will not be able to generalize to a folding task, even if the arm trajectory of folding is similar to pick-and-place. Our key insight is that this kind of generalization becomes feasible if we represent the task through rough trajectory sketches. We propose a policy conditioning method using such rough trajectory sketches, which we call RT-Trajectory, that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform. We find that trajectory sketches strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to 
    
[^7]: 提示的语言：什么语言属性使得提示成功？

    The language of prompting: What linguistic properties make a prompt successful?. (arXiv:2311.01967v1 [cs.CL])

    [http://arxiv.org/abs/2311.01967](http://arxiv.org/abs/2311.01967)

    本文研究了不同大小的预训练和指导调优的语言模型在语言结构上有所不同的提示上的表现，结果显示语言模型在性能上对提示的语言属性有较高的敏感性。

    

    最新一代的语言模型可以通过提示来在许多自然语言处理任务中实现令人印象深刻的零样本或少样本性能。然而，由于性能对提示的选择非常敏感，人们付出了相当大的努力来进行众包提示或设计用于优化提示的方法。然而，我们仍然缺乏对提示的语言属性与任务性能之间的系统理解。在这项工作中，我们研究了不同大小的预训练和指导调优的语言模型在语义上等效但在语言结构上有所不同的提示上的表现。我们通过调查语法属性（如情态、时态、语态和语气）以及通过使用同义词引入词汇-语义变化。我们的发现与常见假设相矛盾，即语言模型在低困惑度的提示上达到最佳性能，这些提示反映了预训练或指导调优数据中的语言使用。提示在数据集或模型之间转移效果不佳，性能有所下降。

    The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we still lack a systematic understanding of how linguistic properties of prompts correlate with task performance. In this work, we investigate how LLMs of different sizes, pre-trained and instruction-tuned, perform on prompts that are semantically equivalent, but vary in linguistic structure. We investigate both grammatical properties such as mood, tense, aspect and modality, as well as lexico-semantic variation through the use of synonyms. Our findings contradict the common assumption that LLMs achieve optimal performance on lower perplexity prompts that reflect language use in pretraining or instruction-tuning data. Prompts transfer poorly between datasets or models, and performanc
    
[^8]: 不要让你的LLM成为一个评估基准欺骗者

    Don't Make Your LLM an Evaluation Benchmark Cheater. (arXiv:2311.01964v1 [cs.CL])

    [http://arxiv.org/abs/2311.01964](http://arxiv.org/abs/2311.01964)

    本文讨论了不恰当使用评估基准和误导性解释评估结果的潜在风险和影响，特别关注了基准泄漏现象。

    

    大型语言模型（LLMs）已经极大地推动了人工智能的前沿，实现了模型能力的显著提升。为了评估模型性能，通常的做法是构建评估基准，以测量LLMs在不同方面的能力水平。尽管已经发布了许多高质量的基准，但对于这些基准的合理使用和不同模型的公平比较的关注越来越多。鉴于这些关注，本文讨论了不恰当使用评估基准和误导性解释评估结果的潜在风险和影响。特别地，我们关注了一个特殊问题，即导致不恰当评估的\emph{基准泄漏}，即评估集相关的数据偶尔被用于模型训练。由于预训练数据通常是在模型测试之前准备的，因此这种现象变得更加普遍。

    Large language models~(LLMs) have greatly advanced the frontiers of artificial intelligence, attaining remarkable improvement in model capacity. To assess the model performance, a typical approach is to construct evaluation benchmarks for measuring the ability level of LLMs in different aspects. Despite that a number of high-quality benchmarks have been released, the concerns about the appropriate use of these benchmarks and the fair comparison of different models are increasingly growing. Considering these concerns, in this paper, we discuss the potential risk and impact of inappropriately using evaluation benchmarks and misleadingly interpreting the evaluation results. Specially, we focus on a special issue that would lead to inappropriate evaluation, \ie \emph{benchmark leakage}, referring that the data related to evaluation sets is occasionally used for model training. This phenomenon now becomes more common since pre-training data is often prepared ahead of model test. We conduct 
    
[^9]: 对XAI后置技术的忠实度评估：与Ground Truth解释数据集的比较研究

    Assessing Fidelity in XAI post-hoc techniques: A Comparative Study with Ground Truth Explanations Datasets. (arXiv:2311.01961v1 [cs.CV])

    [http://arxiv.org/abs/2311.01961](http://arxiv.org/abs/2311.01961)

    本研究通过引入具有可靠的解释真相的图像数据集，对当前最先进的XAI方法进行了公平客观的比较，结果显示基于反向传播的XAI方法相对于其他方法具有更高的准确性和可靠性，但会生成更多的噪声唤醒图。

    

    评估可解释人工智能（XAI）方法与其基础模型的忠实度是一项具有挑战性的任务，主要是因为缺乏解释的真实性。然而，评估忠实度是确保正确的XAI方法的必要步骤。在本研究中，我们通过引入三个具有可靠的解释真相的新颖图像数据集，对当前最先进的XAI方法进行了公平客观的比较。本比较的主要目标是鉴定低忠实度的方法，并将其排除在进一步研究之外，从而促进更可信和有效的XAI技术的发展。我们的结果表明，基于输出信息向输入的反向传播的XAI方法相对于依赖灵敏度分析或类激活图（CAM）的方法具有更高的准确性和可靠性。然而，反向传播方法往往会产生更多的噪声唤醒图。

    The evaluation of the fidelity of eXplainable Artificial Intelligence (XAI) methods to their underlying models is a challenging task, primarily due to the absence of a ground truth for explanations. However, assessing fidelity is a necessary step for ensuring a correct XAI methodology. In this study, we conduct a fair and objective comparison of the current state-of-the-art XAI methods by introducing three novel image datasets with reliable ground truth for explanations. The primary objective of this comparison is to identify methods with low fidelity and eliminate them from further research, thereby promoting the development of more trustworthy and effective XAI techniques. Our results demonstrate that XAI methods based on the backpropagation of output information to input yield higher accuracy and reliability compared to methods relying on sensitivity analysis or Class Activation Maps (CAM). However, the backpropagation method tends to generate more noisy saliency maps. These finding
    
[^10]: 自主机器人系统的量化自主度评价框架

    A Quantitative Autonomy Quantification Framework for Fully Autonomous Robotic Systems. (arXiv:2311.01939v1 [cs.RO])

    [http://arxiv.org/abs/2311.01939](http://arxiv.org/abs/2311.01939)

    本文提出了一个基于任务需求的自主度评估框架，主要关注完全自主模式，通过确定机器人任务特性，推导出自主度的三个度量标准，并将自主度分为自主度水平和自主度程度两部分。

    

    尽管自主功能促进了机器人系统在有限人类监督的领域及更远处的部署，但在任务需求和自主能力之间找到对应关系仍然是一个开放的挑战。因此，在过去三十年中提出了许多量化自主度的方法，但据我们所知，所有这些方法都没有辨别自主度变化的子模式特征，并且一些方法基于违反Goodhart定律的度量标准。本文侧重于完全自主模式，并提出了一个基于任务需求的自主度评估框架。该框架通过确定机器人任务特性，推导出三个自主度度量标准，即所需能力、可靠性和响应性，并通过确定自主度水平和自主度程度的函数，将自主度作为两部分度量来衡量。这些特性基于机器人最终取代人类的观念。

    Although autonomous functioning facilitates deployment of robotic systems in domains that admit limited human oversight on our planet and beyond, finding correspondence between task requirements and autonomous capability is still an open challenge. Consequently, a number of methods for quantifying autonomy have been proposed over the last three decades, but to our knowledge all these have no discernment of sub-mode features of variation of autonomy and some are based on metrics that violet the Goodhart's law. This paper focuses on the full autonomous mode and proposes a task-requirements based autonomy assessment framework. The framework starts by establishing robot task characteristics from which three autonomy metrics, namely requisite capability, reliability and responsiveness, and functions for determining autonomy as a two-part measure, namely of level of autonomy and degree of autonomy are derived. These characteristics are founded on the realization that robots ultimately replac
    
[^11]: 超级心智理念者：探索生成式人工智能以支持创造性问题解决

    Supermind Ideator: Exploring generative AI to support creative problem-solving. (arXiv:2311.01937v1 [cs.AI])

    [http://arxiv.org/abs/2311.01937](http://arxiv.org/abs/2311.01937)

    Supermind Ideator是一个使用生成式人工智能技术的系统，旨在支持创造性问题解决。该系统使用大型语言模型和专门设计的用户界面，能够提供新的创意想法，并帮助用户使用创造性问题解决技术。该系统还可以应用于各种问题，并特别适用于生成关于设计人组或人机组合的创新想法。

    

    以往支持创造性问题解决的努力包括刺激创意思维的技术（如头脑风暴和设计思维）以及记录和分享这些想法的软件工具。现在，生成式人工智能技术可以提供用户从未想到的新想法，用户可以从这些想法中选择，或者用它们来刺激更多的想法。在这里，我们描述了这样一个系统，Supermind Ideator。该系统使用了一个大型语言模型（GPT 3.5），并添加了提示、微调和专门设计的用户界面，以帮助人们使用创造性问题解决技术。其中一些技术可应用于任何问题；另一些则专门用于帮助生成关于如何设计人组或人机组合（“超级心智”）的创新想法。我们还描述了我们使用这个系统的初步经验，并提出了扩展这个系统以支持其他特定问题解决领域技术的方法。

    Previous efforts to support creative problem-solving have included (a) techniques (such as brainstorming and design thinking) to stimulate creative ideas, and (b) software tools to record and share these ideas. Now, generative AI technologies can suggest new ideas that might never have occurred to the users, and users can then select from these ideas or use them to stimulate even more ideas. Here, we describe such a system, Supermind Ideator. The system uses a large language model (GPT 3.5) and adds prompting, fine tuning, and a user interface specifically designed to help people use creative problem-solving techniques. Some of these techniques can be applied to any problem; others are specifically intended to help generate innovative ideas about how to design groups of people and/or computers ("superminds"). We also describe our early experiences with using this system and suggest ways it could be extended to support additional techniques for other specific problem-solving domains.
    
[^12]: GateLoop: 完全数据控制的线性递归用于序列建模

    GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v1 [cs.LG])

    [http://arxiv.org/abs/2311.01927](http://arxiv.org/abs/2311.01927)

    GateLoop是一种完全数据控制的线性递归序列模型，优于现有模型，可以提供数据控制的相对位置信息给Attention。

    

    线性递归已被证明是一种有效建模长序列的强大工具。在这项工作中，我们表明现有模型未能充分利用其潜力。在这一发现的基础上，我们开发了GateLoop，这是一种基础性的序列模型，通过使用数据控制的状态转换来推广线性递归模型，如S4、S5、LRU和RetNet。利用这一理论进步，GateLoop在自回归语言建模方面在实证上优于现有模型。我们的方法具有低成本的$O(l)$递归模式和高度优化的关联扫描实现的高效$O(l \log_{2} l)$并行模式。此外，我们还推导出了一个$O(l^2)$的代理注意力模式，揭示了对Transformer和最近提出的架构的显著影响。具体而言，我们证明了我们的方法可以被解释为向Attention提供数据控制的相对位置信息。而许多现有模型仅依赖于数据无关的位置信息。

    Linear Recurrence has proven to be a powerful tool for modeling long sequences efficiently. In this work, we show that existing models fail to take full advantage of its potential. Motivated by this finding, we develop GateLoop, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet, by employing data-controlled state transitions. Utilizing this theoretical advance, GateLoop empirically outperforms existing models for auto-regressive language modeling. Our method comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$ parallel mode making use of highly optimized associative scan implementations. Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing remarkable implications for Transformer and recently proposed architectures. Specifically, we prove that our approach can be interpreted as providing data-controlled relative-positional information to Attention. While many existing models solely rely on da
    
[^13]: 大型语言模型阐明了人工医疗助手的进展路径：一项综述

    Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review. (arXiv:2311.01918v1 [cs.CL])

    [http://arxiv.org/abs/2311.01918](http://arxiv.org/abs/2311.01918)

    本综述文章探讨了大型语言模型（LLMs）在医学中的应用和意义。LLMs在知识检索、研究支持、临床工作流自动化和诊断辅助方面具有巨大潜力，尤其是多模态LLMs可以处理医学影像和电子健康记录等多样化数据类型以增强诊断能力。

    

    随着人工智能的快速发展，大型语言模型（LLMs）展现了模拟人类级别语言理解和推理的潜力。这引发了将LLMs应用于增强医疗各个方面的重要兴趣，范围从医学教育到临床决策支持。然而，医学涉及多方面的数据模态和微妙的推理技能，这给LLMs的整合带来了挑战。本文综述了LLMs在医学中的应用和影响。首先，考察了通用型和专门化LLMs的基本应用，展示了它们在知识检索、研究支持、临床工作流自动化和诊断辅助方面的作用。鉴于医学的固有多模态特性，该综述进一步关注多模态LLMs，研究其处理医学影像和电子健康记录等多样化数据类型以增强诊断能力的能力。

    With the rapid development of artificial intelligence, large language models (LLMs) have shown promising capabilities in mimicking human-level language comprehension and reasoning. This has sparked significant interest in applying LLMs to enhance various aspects of healthcare, ranging from medical education to clinical decision support. However, medicine involves multifaceted data modalities and nuanced reasoning skills, presenting challenges for integrating LLMs. This paper provides a comprehensive review on the applications and implications of LLMs in medicine. It begins by examining the fundamental applications of general-purpose and specialized LLMs, demonstrating their utilities in knowledge retrieval, research support, clinical workflow automation, and diagnostic assistance. Recognizing the inherent multimodality of medicine, the review then focuses on multimodal LLMs, investigating their ability to process diverse data types like medical imaging and EHRs to augment diagnostic ac
    
[^14]: 用顺序神经网络增强功能数据分析：优势和比较研究

    Enhancing Functional Data Analysis with Sequential Neural Networks: Advantages and Comparative Study. (arXiv:2311.01875v1 [cs.LG])

    [http://arxiv.org/abs/2311.01875](http://arxiv.org/abs/2311.01875)

    本论文研究了如何利用顺序神经网络（SNNs）增强功能数据分析（FDA），并通过与常见FDA回归模型的比较分析和实际数据分析证明了SNNs的有效性。

    

    功能数据分析（FDA）是一种处理高维度和复杂数据结构特征的统计领域。顺序神经网络（SNNs）是一种特殊的神经网络，能够处理序列数据，这是功能数据的一个基本特征。尽管SNNs在建模功能数据方面非常灵活，但在FDA社区中使用不多。SNNs的一个显著优势是易于实施，可以让广大非学术界的人群能够使用。相反，基于FDA的方法在实践中存在挑战，尤其是对于非专业人士来说，因为它们过于复杂。基于这一点，我们提议在FDA应用中利用SNNs，并通过与常见FDA回归模型的比较分析和实际数据分析来证明其有效性。SNNs的架构使我们能够克服传统FDA方法的局限性，提供了更好的功能数据分析。

    Functional Data Analysis (FDA) is a statistical domain developed to handle functional data characterized by high dimensionality and complex data structures. Sequential Neural Networks (SNNs) are specialized neural networks capable of processing sequence data, a fundamental aspect of functional data. Despite their great flexibility in modeling functional data, SNNs have been inadequately employed in the FDA community. One notable advantage of SNNs is the ease of implementation, making them accessible to a broad audience beyond academia. Conversely, FDA-based methodologies present challenges, particularly for practitioners outside the field, due to their intricate complexity. In light of this, we propose utilizing SNNs in FDA applications and demonstrate their effectiveness through comparative analyses against popular FDA regression models based on numerical experiments and real-world data analysis. SNN architectures allow us to surpass the limitations of traditional FDA methods, offerin
    
[^15]: 多语言欧洲议会数据集用于分析信息检索中的偏见

    Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval. (arXiv:2311.01870v1 [cs.CL])

    [http://arxiv.org/abs/2311.01870](http://arxiv.org/abs/2311.01870)

    Multi-EuP是一个新的多语言基准数据集，涵盖了来自欧洲议会的22K个多语言文档，旨在研究信息检索中的公平性，包括语言和人口偏见。它提供了真实的多语言语料库和跨语言相关性评判，并提供了与文档相关的丰富人口统计信息，可用于评估单语和多语信息检索。

    

    我们介绍了Multi-EuP，这是一个新的多语言基准数据集，包括来自欧洲议会的22K个多语言文档，涵盖了24种语言。该数据集旨在研究多语言信息检索（IR）环境下的公平性，以分析在排名上的语言和人口偏见。它拥有一个真实的多语言语料库，其中的主题被翻译成了所有24种语言，并提供跨语言相关性评判。此外，它还提供了与文档相关的丰富人口统计信息，便于研究人口偏见。我们报告了Multi-EuP在单语和多语信息检索基准测试中的有效性。我们还进行了一项关于标记化策略选择引起的语言偏见的初步实验。

    We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K multi-lingual documents collected from the European Parliament, spanning 24 languages. This dataset is designed to investigate fairness in a multilingual information retrieval (IR) context to analyze both language and demographic bias in a ranking context. It boasts an authentic multilingual corpus, featuring topics translated into all 24 languages, as well as cross-lingual relevance judgments. Furthermore, it offers rich demographic information associated with its documents, facilitating the study of demographic bias. We report the effectiveness of Multi-EuP for benchmarking both monolingual and multilingual IR. We also conduct a preliminary experiment on language bias caused by the choice of tokenization strategy.
    
[^16]: 朝着概念感知的大型语言模型

    Towards Concept-Aware Large Language Models. (arXiv:2311.01866v1 [cs.CL])

    [http://arxiv.org/abs/2311.01866](http://arxiv.org/abs/2311.01866)

    本文研究了概念在语言模型中的作用，并探讨了开发概念感知语言模型的方法。通过预训练LLMs或使用现有LLMs的输出，我们证明了这种方法更好地符合人类直觉并改善了预测的鲁棒性。

    

    概念在各种人类认知功能中起着关键作用，包括学习、推理和交流。然而，目前对于赋予机器形成和推理概念的能力的研究非常有限。尤其是，目前的大型语言模型（LLMs）主要在词元级别上操作，而不是概念级别。本文分析了当代LLMs对人类概念及其结构的捕捉能力，并讨论了在不同阶段中开发概念感知LLMs的方法。我们提出了一种使用概念进行预训练的LLMs方法，并探讨了使用现有LLMs输出的更简单方法。尽管简单，我们的概念验证证明了更好地匹配人类直觉，并提升了预测的鲁棒性。这些初步结果显示了概念感知LLMs的潜力。

    Concepts play a pivotal role in various human cognitive functions, including learning, reasoning and communication. However, there is very little work on endowing machines with the ability to form and reason with concepts. In particular, state-of-the-art large language models (LLMs) work at the level of tokens, not concepts.  In this work, we analyze how well contemporary LLMs capture human concepts and their structure. We then discuss ways to develop concept-aware LLMs, taking place at different stages of the pipeline. We sketch a method for pretraining LLMs using concepts, and also explore the simpler approach that uses the output of existing LLMs. Despite its simplicity, our proof-of-concept is shown to better match human intuition, as well as improve the robustness of predictions. These preliminary results underscore the promise of concept-aware LLMs.
    
[^17]: SortNet: 通过神经网络排序算法进行学习排序

    SortNet: Learning To Rank By a Neural-Based Sorting Algorithm. (arXiv:2311.01864v1 [cs.LG])

    [http://arxiv.org/abs/2311.01864](http://arxiv.org/abs/2311.01864)

    SortNet是一种使用神经网络作为比较器来进行自适应排序的算法，通过迭代过程构建训练集，根据成对项目之间的排序示例来训练神经网络。

    

    关于相关性排名的问题，即根据给定的标准对一组对象进行排序。由于用户可能偏好不同的相关性标准，因此排序算法应该能够根据用户需求进行调整。学习排序的任务在文献中存在两种主要方法：1）通过示例学习的得分函数，评估每个对象的属性，生成可用于对对象进行排序的绝对相关性值；2）一种成对方法，通过使用对象对来学习“偏好函数”，定义哪一个对象应该首先排名。在本文中，我们提出了SortNet，一种使用神经网络作为比较器来对对象进行自适应排序的算法。神经网络的训练集提供了对于成对项目之间所需排序的示例，并且通过迭代过程构建，每次迭代都会添加最具信息性的训练示例。此外，比较器采用了连接主义体系结构。

    The problem of relevance ranking consists of sorting a set of objects with respect to a given criterion. Since users may prefer different relevance criteria, the ranking algorithms should be adaptable to the user needs. Two main approaches exist in literature for the task of learning to rank: 1) a score function, learned by examples, which evaluates the properties of each object yielding an absolute relevance value that can be used to order the objects or 2) a pairwise approach, where a "preference function" is learned using pairs of objects to define which one has to be ranked first. In this paper, we present SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. The neural network training set provides examples of the desired ordering between pairs of items and it is constructed by an iterative procedure which, at each iteration, adds the most informative training examples. Moreover, the comparator adopts a connectionist architecture that 
    
[^18]: FAME：灵活可扩展的类比映射引擎

    FAME: Flexible, Scalable Analogy Mappings Engine. (arXiv:2311.01860v1 [cs.CL])

    [http://arxiv.org/abs/2311.01860](http://arxiv.org/abs/2311.01860)

    这项工作提出了一个灵活可扩展的类比映射引擎，通过自动提取常识表示，并使用这些表示来确定实体之间的映射关系。与以往方法不同的是，该引擎可以处理部分类比，并提供新实体的建议。实验证明其在经典2x2类比问题上的准确率为81.2％，在更大的问题上为77.8％，此外，该引擎还优于人类表现。

    

    类比是人类认知的核心能力之一；在面对新情境时，我们经常从其他领域中转移先前的经验。大多数关于计算类比的工作都严重依赖复杂的手工制作输入。在本研究中，我们放松了输入要求，只需对实体进行映射。我们自动提取常识表示，并使用它们来确定实体之间的映射关系。与以前的方法不同，我们的框架可以处理部分类比，并建议添加新的实体。此外，我们的方法的输出易于解释，用户可以理解为什么选择了特定的映射。实验证明，我们的模型正确地映射了81.2％的经典2x2类比问题（猜测水平=50％）。在更大的问题上，它的准确率达到77.8％（平均猜测水平=13.1％）。在另一个实验中，我们展示了我们的算法胜过了人类的表现，并且自动建议的新实体类似于人类建议的实体。

    Analogy is one of the core capacities of human cognition; when faced with new situations, we often transfer prior experience from other domains. Most work on computational analogy relies heavily on complex, manually crafted input. In this work, we relax the input requirements, requiring only names of entities to be mapped. We automatically extract commonsense representations and use them to identify a mapping between the entities. Unlike previous works, our framework can handle partial analogies and suggest new entities to be added. Moreover, our method's output is easily interpretable, allowing for users to understand why a specific mapping was chosen.  Experiments show that our model correctly maps 81.2% of classical 2x2 analogy problems (guess level=50%). On larger problems, it achieves 77.8% accuracy (mean guess level=13.1%). In another experiment, we show our algorithm outperforms human performance, and the automatic suggestions of new entities resemble those suggested by humans. 
    
[^19]: 基于神经辐射场的智能多层视图合成架构

    A Neural Radiance Field-Based Architecture for Intelligent Multilayered View Synthesis. (arXiv:2311.01842v1 [cs.NI])

    [http://arxiv.org/abs/2311.01842](http://arxiv.org/abs/2311.01842)

    该论文提出了基于神经辐射场的智能多层视图合成架构，为移动自组网中的数据包路由问题提供了红火蚁优化路由选择策略。

    

    移动自组网由一些无线便携节点组成，它们在途中自发聚集，建立一个临时网络，无需任何中央管理。移动自组网由一群数量可观且密集的移动节点组成，它们穿越各种地形，仅依靠无线接口进行通信，无需任何集中式管理。此外，路由应提供一种在任意两个节点之间即时传输数据的方法。然而，寻找最佳的数据包路由是主要问题。所提议的协议的主要目标是找到费用最低的最小容量获取方法，以确保在任一节点故障的情况下也能保证可靠的传输。本研究提出了通过红火蚁优化路由选择策略来改进按需源路由系统。

    A mobile ad hoc network is made up of a number of wireless portable nodes that spontaneously come together en route for establish a transitory network with no need for any central management. A mobile ad hoc network (MANET) is made up of a sizable and reasonably dense community of mobile nodes that travel across any terrain and rely solely on wireless interfaces for communication, not on any well before centralized management. Furthermore, routing be supposed to offer a method for instantly delivering data across a network between any two nodes. Finding the best packet routing from across infrastructure is the major issue, though. The proposed protocol's major goal is to identify the least-expensive nominal capacity acquisition that assures the transportation of realistic transport that ensures its durability in the event of any node failure. This study suggests the Optimized Route Selection via Red Imported Fire Ants (RIFA) Strategy as a way to improve on-demand source routing systems
    
[^20]: AFPQ：面向LLMs的非对称浮点量化

    AFPQ: Asymmetric Floating Point Quantization for LLMs. (arXiv:2311.01792v1 [cs.CL])

    [http://arxiv.org/abs/2311.01792](http://arxiv.org/abs/2311.01792)

    AFPQ提出了一种面向LLMs的非对称浮点量化方法，通过为正值和负值设置不同的比例尺，显著提高了准确性，并且可以与其他量化方法相结合，无需额外存储空间。

    

    大型语言模型（LLMs）在各种任务中表现出色，但面临有限的内存容量和带宽的部署挑战。低位权重量化可以节省内存并加速推断。尽管浮点（FP）格式在LLM量化中表现出良好性能，但它们在小组大小或子4位时往往表现不佳。我们发现，之前的FP量化缺乏不对称性，不适合处理LLM权重张量的不对称值分布。在这项工作中，我们提出了非对称FP量化（AFPQ），为正值和负值设置了分别的比例尺。我们的方法显著提高了准确性，并可以轻松地插入其他量化方法，包括GPTQ和AWQ，以获得更好的性能。此外，与非对称整数（INT）量化相比，不需要额外的存储空间。代码可在https://github.com/zhangsichengsjtu/AFPQ获得。

    Large language models (LLMs) show great performance in various tasks, but face deployment challenges from limited memory capacity and bandwidth. Low-bit weight quantization can save memory and accelerate inference. Although floating-point (FP) formats show good performance in LLM quantization, they tend to perform poorly with small group sizes or sub-4 bits. We find the reason is that the absence of asymmetry in previous FP quantization makes it unsuitable for handling asymmetric value distribution of LLM weight tensors. In this work, we propose asymmetric FP quantization (AFPQ), which sets separate scales for positive and negative values. Our method leads to large accuracy improvements and can be easily plugged into other quantization methods, including GPTQ and AWQ, for better performance. Besides, no additional storage is needed compared with asymmetric integer (INT) quantization. The code is available at https://github.com/zhangsichengsjtu/AFPQ.
    
[^21]: TCM-GPT:用于传统中医领域适应的大型语言模型的高效预训练

    TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine. (arXiv:2311.01786v1 [cs.CL])

    [http://arxiv.org/abs/2311.01786](http://arxiv.org/abs/2311.01786)

    TCM-GPT是一种用于传统中医领域适应的大型语言模型的高效预训练方法，通过构建一个传统中医专用语料库进行预训练，取得了良好的效果。

    

    预训练和微调已成为各种自然语言处理任务中的一种有前途的范式。预训练的大型语言模型（LLM）的有效性得到了进一步提升，并具有在医学领域，特别是传统中医领域应用的潜力。然而，将这些通用模型应用于特定领域往往产生次优结果，主要是由于缺乏领域知识、独特目标和计算效率等挑战。此外，它们在专业领域（如传统中医）的有效性需要进行全面评估。为了解决上述问题，我们提出了一种新颖的领域特定的TCMDA（传统中医领域适应）方法，即利用领域特定语料库进行高效预训练。具体而言，我们首先通过识别领域关键词并从通用语料库中检索，构建了一个大型的传统中医特定语料库TCM-Corpus-1B。然后，我们的TCMDA方法使用这个语料库进行预训练。通过实验验证了我们方法的有效性。

    Pre-training and fine-tuning have emerged as a promising paradigm across various natural language processing (NLP) tasks. The effectiveness of pretrained large language models (LLM) has witnessed further enhancement, holding potential for applications in the field of medicine, particularly in the context of Traditional Chinese Medicine (TCM). However, the application of these general models to specific domains often yields suboptimal results, primarily due to challenges like lack of domain knowledge, unique objectives, and computational efficiency. Furthermore, their effectiveness in specialized domains, such as Traditional Chinese Medicine, requires comprehensive evaluation. To address the above issues, we propose a novel domain specific TCMDA (TCM Domain Adaptation) approach, efficient pre-training with domain-specific corpus. Specifically, we first construct a large TCM-specific corpus, TCM-Corpus-1B, by identifying domain keywords and retreving from general corpus. Then, our TCMDA 
    
[^22]: 用最大差异模型来建模半监督二维姿态估计中的不确定性

    Modeling the Uncertainty with Maximum Discrepant Students for Semi-supervised 2D Pose Estimation. (arXiv:2311.01770v1 [cs.CV])

    [http://arxiv.org/abs/2311.01770](http://arxiv.org/abs/2311.01770)

    本文提出了一个用于半监督姿态估计的简单而高效的框架，从建模伪标签的不确定性的角度来评估伪标签的质量，并通过构建两个最大差异学生来推动老师生成不同的决策边界。实验结果表明了该方法的有效性。

    

    半监督姿态估计是计算机视觉中具有实际挑战的任务。虽然出现了许多优秀的半监督分类方法，但这些方法通常使用置信度来评估伪标签的质量，在姿态估计任务中很难实现。例如，在姿态估计中，置信度仅表示热图位置是关键点的可能性，而不是预测的质量。在本文中，我们提出了一个简单而高效的框架，从伪标签的不确定性建模的角度来估计半监督姿态估计任务中的伪标签质量。具体而言，在双平均师傅框架下，我们构建了两个最大差异学生，以有效地推动两个老师为同一样本生成不同的决策边界。此外，我们创建了多个不确定性来评估伪标签的质量。实验结果证明了我们的方法的有效性。

    Semi-supervised pose estimation is a practically challenging task for computer vision. Although numerous excellent semi-supervised classification methods have emerged, these methods typically use confidence to evaluate the quality of pseudo-labels, which is difficult to achieve in pose estimation tasks. For example, in pose estimation, confidence represents only the possibility that a position of the heatmap is a keypoint, not the quality of that prediction. In this paper, we propose a simple yet efficient framework to estimate the quality of pseudo-labels in semi-supervised pose estimation tasks from the perspective of modeling the uncertainty of the pseudo-labels. Concretely, under the dual mean-teacher framework, we construct the two maximum discrepant students (MDSs) to effectively push two teachers to generate different decision boundaries for the same sample. Moreover, we create multiple uncertainties to assess the quality of the pseudo-labels. Experimental results demonstrate th
    
[^23]: Indo LEGO-ABSA：一种针对印度尼西亚语的多任务生成式基于方面的情感分析方法

    Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for Indonesian Language. (arXiv:2311.01757v1 [cs.CL])

    [http://arxiv.org/abs/2311.01757](http://arxiv.org/abs/2311.01757)

    本研究旨在针对印度尼西亚语，利用生成式预训练语言模型实现多任务生成式基于方面的情感分析方法，并开发了Indo LEGO-ABSA模型。

    

    方面级情感分析是一种自然语言处理方法，旨在识别和理解与实体的特定方面相关的情感。前期研究已经利用生成式预训练语言模型进行方面级情感分析。LEGO-ABSA是一个成功利用生成式预训练语言模型进行方面级情感分析的框架，特别在英文中。LEGO-ABSA利用多任务学习和提示方法来提高模型性能。然而，在印度尼西亚语环境中尚未应用该方法。因此，本研究旨在利用生成式预训练语言模型在印度尼西亚语的方面级情感分析中实现多任务学习和提示方法。在本研究中，开发了Indo LEGO-ABSA模型，该模型是一个基于方面的情感分析模型。

    Aspect-based sentiment analysis is a method in natural language processing aimed at identifying and understanding sentiments related to specific aspects of an entity. Aspects are words or phrases that represent an aspect or attribute of a particular entity. Previous research has utilized generative pre-trained language models to perform aspect-based sentiment analysis. LEGO-ABSA is one framework that has successfully employed generative pre-trained language models in aspect-based sentiment analysis, particularly in English. LEGO-ABSA uses a multitask learning and prompting approach to enhance model performance. However, the application of this approach has not been done in the context of Bahasa Indonesia. Therefore, this research aims to implement the multitask learning and prompting approach in aspect-based sentiment analysis for Bahasa Indonesia using generative pre-trained language models. In this study, the Indo LEGO-ABSA model is developed, which is an aspect-based sentiment analy
    
[^24]: RiskQ: 风险敏感的多智能体强化学习价值因子分解

    RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization. (arXiv:2311.01753v1 [cs.MA])

    [http://arxiv.org/abs/2311.01753](http://arxiv.org/abs/2311.01753)

    RiskQ是一种解决多智能体强化学习中风险敏感协调要求的方法，通过引入风险敏感的个体-全局最大（RIGM）原则和建模联合回报分布实现价值因子分解。

    

    多智能体系统特点是环境不确定性、智能体的策略多样性和部分可观测性，这导致了显著的风险。在多智能体强化学习（MARL）的背景下，学习对风险敏感的协调和分散策略是具有挑战性的。为了在风险敏感的MARL中制定协调要求，我们介绍了风险敏感的个体-全局最大（RIGM）原理，作为个体-全局最大（IGM）和分布式IGM（DIGM）原理的一种推广。该原理要求每个智能体的风险敏感动作选择集合应与中央策略的风险敏感动作选择等价。当前的MARL价值因子分解方法对于常见的风险度量（例如风险价值（VaR）度量或扭曲的风险度量）不满足RIGM原则。因此，我们提出了RiskQ来解决这个限制，通过建模联合回报分布来实现价值因子分解。

    Multi-agent systems are characterized by environmental uncertainty, varying policies of agents, and partial observability, which result in significant risks. In the context of Multi-Agent Reinforcement Learning (MARL), learning coordinated and decentralized policies that are sensitive to risk is challenging. To formulate the coordination requirements in risk-sensitive MARL, we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM) principles. This principle requires that the collection of risk-sensitive action selections of each agent should be equivalent to the risk-sensitive action selection of the central policy. Current MARL value factorization methods do not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements. Therefore, we propose RiskQ to address this limitation, which models the joint return distribution by modeli
    
[^25]: 地下LoRaWAN网络能效优化的强化学习方法：直接卫星连接场景下的应用

    Energy Efficiency Optimization for Subterranean LoRaWAN Using A Reinforcement Learning Approach: A Direct-to-Satellite Scenario. (arXiv:2311.01743v1 [cs.IT])

    [http://arxiv.org/abs/2311.01743](http://arxiv.org/abs/2311.01743)

    本文介绍了一种地下LoRaWAN网络能效优化的强化学习方法，该方法采用多智能体深度Q网络和多智能体优势演员-评论家算法来分配扩频因子，以最小化共享扩频因子干扰并优化系统的能效。

    

    在偏远农业和灾害救援行动中，地下LoRaWAN与非地球网络（NTN）的集成为经济和社会带来了重大好处。LoRa调制利用准正交扩频因子（SFs）来优化数据速率、空闲时间、覆盖范围和能量消耗。然而，在大规模地下LoRaWAN NTN中，有效地为终端设备分配SFs以最小化共享扩频因子干扰仍然具有挑战性。为了解决这个问题，我们研究了基于强化学习的SFs分配方案来优化系统的能效。为了有效捕捉密集网络中设备与环境之间的互动，我们提出了一种使用基于分析奖励机制的多智能体dueling double deep Q-network（MAD3QN）和多智能体优势演员-评论家（MAA2C）算法的SFs分配技术。与四个基准相比，我们提出的基于强化学习的SFs分配方法表现更好。

    The integration of subterranean LoRaWAN and non-terrestrial networks (NTN) delivers substantial economic and societal benefits in remote agriculture and disaster rescue operations. The LoRa modulation leverages quasi-orthogonal spreading factors (SFs) to optimize data rates, airtime, coverage and energy consumption. However, it is still challenging to effectively assign SFs to end devices for minimizing co-SF interference in massive subterranean LoRaWAN NTN. To address this, we investigate a reinforcement learning (RL)-based SFs allocation scheme to optimize the system's energy efficiency (EE). To efficiently capture the device-to-environment interactions in dense networks, we proposed an SFs allocation technique using the multi-agent dueling double deep Q-network (MAD3QN) and the multi-agent advantage actor-critic (MAA2C) algorithms based on an analytical reward mechanism. Our proposed RL-based SFs allocation approach evinces better performance compared to four benchmarks in the extre
    
[^26]: 使用数据增强强化的神经模型对量子过程进行灵活的误差缓解

    Flexible Error Mitigation of Quantum Processes with Data Augmentation Empowered Neural Model. (arXiv:2311.01727v1 [quant-ph])

    [http://arxiv.org/abs/2311.01727](http://arxiv.org/abs/2311.01727)

    提出了一种数据增强强化的神经模型，该模型可以灵活地缓解量子过程中的各种噪声，并展示了在不同类型量子过程中与先前方法相比的优越性能。

    

    神经网络在量子计算的各种任务中显示出了其有效性。然而，在量子误差缓解中的应用受到对无噪声统计的依赖限制，这是实现实际量子进展的关键步骤。为了解决这一关键挑战，我们提出了一种数据增强强化的神经模型用于误差缓解（DAEM）。我们的模型不需要任何关于特定噪声类型和测量设置的先验知识，并且可以仅根据目标量子过程的噪声测量结果估计无噪声统计值，使其非常适合实际实施。在数值实验中，我们展示了该模型在缓解各种类型的噪声（包括马尔可夫噪声和非马尔可夫噪声）方面与先前的误差缓解方法相比的优越性能。我们进一步通过利用该模型来缓解多种类型的量子过程中的错误来展示其多功能性。

    Neural networks have shown their effectiveness in various tasks in the realm of quantum computing. However, their application in quantum error mitigation, a crucial step towards realizing practical quantum advancements, has been restricted by reliance on noise-free statistics. To tackle this critical challenge, we propose a data augmentation empowered neural model for error mitigation (DAEM). Our model does not require any prior knowledge about the specific noise type and measurement settings and can estimate noise-free statistics solely from the noisy measurement results of the target quantum process, rendering it highly suitable for practical implementation. In numerical experiments, we show the model's superior performance in mitigating various types of noise, including Markovian noise and Non-Markovian noise, compared with previous error mitigation methods. We further demonstrate its versatility by employing the model to mitigate errors in diverse types of quantum processes, includ
    
[^27]: 实现对视觉语言模型的校准鲁棒微调

    Towards Calibrated Robust Fine-Tuning of Vision-Language Models. (arXiv:2311.01723v1 [cs.CV])

    [http://arxiv.org/abs/2311.01723](http://arxiv.org/abs/2311.01723)

    本文提出了一个名为校准鲁棒微调（CaRot）的方法，针对视觉语言模型在分布变化下的校准问题。通过该方法，作者成功提高了预训练模型的校准性能和鲁棒性能。

    

    微调可以释放预训练模型在特定任务上的潜力，但会影响模型对于非分布数据集的泛化能力。为了缓解这个问题，鲁棒微调旨在确保模型在非分布数据集以及微调的分布数据集上都有良好的性能。然而，在可靠的机器学习中，置信度校准这一标准却经常被忽视，尽管在现实世界中高风险的机器学习应用中（如自动驾驶和医学诊断）需求日益增加。我们首次提出了对细调的视觉语言模型在分布变化下校准的担忧，并通过显示普通微调甚至最先进的鲁棒微调方法对预训练的视觉语言模型的校准造成了损害，尤其是在非分布数据集上。为了解决这个问题，我们提出了一种简单的方法，称为校准鲁棒微调（CaRot），它在校准和鲁棒性上提供了奖励。

    While fine-tuning unleashes the potential of a pre-trained model to a specific task, it trades off the model's generalization capability on out-of-distribution (OOD) datasets. To mitigate this, robust fine-tuning aims to ensure performance on OOD datasets as well as an in-distribution (ID) dataset for which the model is being tuned. However, another criterion for reliable machine learning (ML), confidence calibration, has been overlooked despite its increasing demand for real-world high-stakes ML applications (e.g., autonomous driving and medical diagnosis). For the first time, we raise concerns about the calibration of fine-tuned vision-language models (VLMs) under distribution shift by showing that naive fine-tuning and even state-of-the-art robust fine-tuning methods hurt the calibration of pre-trained VLMs, especially on OOD datasets. To address this, we provide a simple approach, called a calibrated robust fine-tuning (CaRot) that incentivizes the calibration and robustness on bot
    
[^28]: 对中国方面情感四元预测进行基准测试的实证研究

    An Empirical Study of Benchmarking Chinese Aspect Sentiment Quad Prediction. (arXiv:2311.01713v1 [cs.CL])

    [http://arxiv.org/abs/2311.01713](http://arxiv.org/abs/2311.01713)

    本研究构建了两个大规模的中国ASQP数据集，对生成式预训练变压器（GPT）系列模型在ASQP上的性能进行了评估，并展示了改进ASQP技术和提高GPT性能的重要性。

    

    方面情感四元预测（ASQP）是方面级情感分析的一个关键子任务。目前的ASQP数据集特点是规模小且四元组密度低，这阻碍了技术的发展。为了扩大容量，我们构建了两个大规模的中国ASQP数据集，从多个在线平台收集。这些数据集具有几个显著的特点：更大的规模（每个数据集都有10,000+个样本），丰富的方面类别，每个句子更多的词数以及比现有ASQP数据集更高的密度。此外，我们首次评估了生成式预训练变压器（GPT）系列模型在ASQP上的性能，并展示了潜在的问题。与最先进的ASQP基准线实验强调了需要探索额外技术来解决ASQP的需求，以及进一步研究改进GPT性能的方法的重要性。

    Aspect sentiment quad prediction (ASQP) is a critical subtask of aspect-level sentiment analysis. Current ASQP datasets are characterized by their small size and low quadruple density, which hinders technical development. To expand capacity, we construct two large Chinese ASQP datasets crawled from multiple online platforms. The datasets hold several significant characteristics: larger size (each with 10,000+ samples) and rich aspect categories, more words per sentence, and higher density than existing ASQP datasets. Moreover, we are the first to evaluate the performance of Generative Pre-trained Transformer (GPT) series models on ASQP and exhibit potential issues. The experiments with state-of-the-art ASQP baselines underscore the need to explore additional techniques to address ASQP, as well as the importance of further investigation into methods to improve the performance of GPTs.
    
[^29]: 无数据的语言模型蒸馏：通过文本到文本转换实现

    Data-Free Distillation of Language Model by Text-to-Text Transfer. (arXiv:2311.01689v1 [cs.CL])

    [http://arxiv.org/abs/2311.01689](http://arxiv.org/abs/2311.01689)

    本文提出了一种无数据知识蒸馏（DFKD）框架，即DFKD-T$^{3}$，利用预训练的生成式语言模型作为数据生成器，将通用领域语料库转化为压缩友好的任务数据。实验证明该方法能够提升各种下游任务的蒸馏性能。

    

    当原始训练数据不可用时，无数据知识蒸馏（DFKD）在压缩模型方面起着至关重要的作用。先前在自然语言处理领域对DFKD的研究主要集中在对类别任务进行蒸馏的仅编码器结构，忽视了生成式语言建模的重要进展。在本研究中，我们提出了一种新颖的DFKD框架，名为DFKD-T$^{3}$，预训练的生成式语言模型也可以作为一个可控的数据生成器，用于模型压缩。这个新颖的DFKD-T$^{3}$框架导致了一个端到端可学习的文本到文本框架，将通用领域语料库转化为压缩友好的任务数据，旨在提高模型的特异性和多样性。大量实验证明我们的方法可以提升在情感分析、语言可接受性和信息提取等各种下游任务中的蒸馏性能。此外，我们还展示了生成的文本可以直接应用于...

    Data-Free Knowledge Distillation (DFKD) plays a vital role in compressing the model when original training data is unavailable. Previous works for DFKD in NLP mainly focus on distilling encoder-only structures like BERT on classification tasks, which overlook the notable progress of generative language modeling. In this work, we propose a novel DFKD framework, namely DFKD-T$^{3}$, where the pretrained generative language model can also serve as a controllable data generator for model compression. This novel framework DFKD-T$^{3}$ leads to an end-to-end learnable text-to-text framework to transform the general domain corpus to compression-friendly task data, targeting to improve both the \textit{specificity} and \textit{diversity}. Extensive experiments show that our method can boost the distillation performance in various downstream tasks such as sentiment analysis, linguistic acceptability, and information extraction. Furthermore, we show that the generated texts can be directly used 
    
[^30]: 通向精准医学的R.O.A.D.

    The R.O.A.D. to precision medicine. (arXiv:2311.01681v1 [stat.AP])

    [http://arxiv.org/abs/2311.01681](http://arxiv.org/abs/2311.01681)

    我们提出了一种预后分层匹配框架，将观察数据转化为类似于随机试验，为精准医学铺平了道路。通过纠正患者特征的影响，我们创建了临床直观的治疗建议。通过应用于胃肠间质瘤（GIST）的观察数据，我们证明了这些建议的优于专家建议。

    

    我们提出了一种预后分层匹配框架，解决了随机试验数据亚组分析的不足，并将观察数据转化为类似于随机试验，为精准医学铺平了道路。我们的方法通过一种新的两步过程纠正了治疗下预测结果的潜在混淆的影响，这些预测结果使用来训练最优策略树（OPTs），OPTs是决策树，根据患者特征将治疗最优地分配给亚组。这有助于创造临床直观的治疗建议。我们将这个框架应用到了具有胃肠间质瘤（GIST）的患者的观察数据，并使用敏感性和特异性指标在外部队列中验证了OPTs。我们证明，这些建议优于GIST领域的专家建议。我们进一步应用了相同的方法

    We propose a prognostic stratum matching framework that addresses the deficiencies of Randomized trial data subgroup analysis and transforms ObservAtional Data to be used as if they were randomized, thus paving the road for precision medicine. Our approach counters the effects of unobserved confounding in observational data by correcting the estimated probabilities of the outcome under a treatment through a novel two-step process. These probabilities are then used to train Optimal Policy Trees (OPTs), which are decision trees that optimally assign treatments to subgroups of patients based on their characteristics. This facilitates the creation of clinically intuitive treatment recommendations. We applied our framework to observational data of patients with gastrointestinal stromal tumors (GIST) and validated the OPTs in an external cohort using the sensitivity and specificity metrics. We show that these recommendations outperformed those of experts in GIST. We further applied the same 
    
[^31]: DialogBench: 将LLMs作为人类对话系统进行评估

    DialogBench: Evaluating LLMs as Human-like Dialogue Systems. (arXiv:2311.01677v1 [cs.CL])

    [http://arxiv.org/abs/2311.01677](http://arxiv.org/abs/2311.01677)

    本文提出了DialogBench，一个对话评估基准，用于评估LLMs作为人类对话系统的能力。通过对28个LLMs的广泛测试，发现指导微调对提升性能效果显著。

    

    大型语言模型(LLMs)在新的对话能力方面取得了显著突破，刷新了人们对对话系统的印象。对话系统长期以来的目标是足够像人类，以便通过满足交流、情感和社交归属的需要与用户建立长期联系。因此，迫切需要评估LLMs作为人类对话系统的能力。本文提出了DialogBench，一个对话评估基准，目前包含12个对话任务，评估LLMs作为人类对话系统应具备的能力。具体来说，我们使用GPT-4生成每个任务的评估实例。我们首先根据广泛使用的设计原则设计基本提示，并进一步减轻现有的偏见，生成更高质量的评估实例。我们对28个LLMs进行了广泛的测试（包括预训练和监督指导调优），结果显示指导微调效益显著。

    Large language models (LLMs) have achieved remarkable breakthroughs in new dialogue capabilities, refreshing human's impressions on dialogue systems. The long-standing goal of dialogue systems is to be human-like enough to establish long-term connections with users by satisfying the need for communication, affection and social belonging. Therefore, there has been an urgent need to evaluate LLMs as human-like dialogue systems. In this paper, we propose DialogBench, a dialogue evaluation benchmark that currently contains $12$ dialogue tasks to assess the capabilities of LLMs as human-like dialogue systems should have. Specifically, we prompt GPT-4 to generate evaluation instances for each task. We first design the basic prompt based on widely-used design principles and further mitigate the existing biases to generate higher-quality evaluation instances. Our extensive test over $28$ LLMs (including pre-trained and supervised instruction-tuning) shows that instruction fine-tuning benefits 
    
[^32]: MineSegSAT：一个自动化系统，用于评估Sentinel-2影像中的采矿干扰区域范围

    MineSegSAT: An automated system to evaluate mining disturbed area extents from Sentinel-2 imagery. (arXiv:2311.01676v1 [cs.CV])

    [http://arxiv.org/abs/2311.01676](http://arxiv.org/abs/2311.01676)

    本文介绍了MineSegSAT，一个使用Sentinel-2数据和SegFormer深度学习分割架构的模型，用于预测矿产开采现场受到影响的区域。该模型利用先进的空间理解能力进行准确的土地覆盖分类。通过研究不同损失函数的效果，验证了该模型的可靠性和有效性。

    

    评估矿产开采行业的环境影响在理解和减轻开采活动的生态后果方面起着关键作用。本文介绍了MineSegSAT，一个利用在Sentinel-2数据上训练的SegFormer深度学习分割架构来预测矿产开采现场环境受到影响的区域的新方法。所使用的数据是从2021年加拿大西部的非重叠地区收集的，其中包含了通过2021年高分辨率卫星图像确定的受矿业活动环境影响的土地区域。采用SegFormer架构，这是一种先进的语义分割框架，利用其先进的空间理解能力进行准确的土地覆盖分类。我们研究了包括Dice、Tversky和Lovasz损失在内的损失函数的效果。训练好的模型用于对测试区域进行推断。

    Assessing the environmental impact of the mineral extraction industry plays a critical role in understanding and mitigating the ecological consequences of extractive activities. This paper presents MineSegSAT, a model that presents a novel approach to predicting environmentally impacted areas of mineral extraction sites using the SegFormer deep learning segmentation architecture trained on Sentinel-2 data. The data was collected from non-overlapping regions over Western Canada in 2021 containing areas of land that have been environmentally impacted by mining activities that were identified from high-resolution satellite imagery in 2021. The SegFormer architecture, a state-of-the-art semantic segmentation framework, is employed to leverage its advanced spatial understanding capabilities for accurate land cover classification. We investigate the efficacy of loss functions including Dice, Tversky, and Lovasz loss respectively. The trained model was utilized for inference over the test reg
    
[^33]: 基于深度学习的社区韧性评估：基于相互关联的社会技术系统特征

    Deep Learning-driven Community Resilience Rating based on Intertwined Socio-Technical Systems Features. (arXiv:2311.01661v1 [cs.SI])

    [http://arxiv.org/abs/2311.01661](http://arxiv.org/abs/2311.01661)

    这项研究提出了一个基于三层深度学习模型的社区韧性评估方法，通过捕捉社区社会技术系统的异质特征和非线性相互作用，实现对社区韧性的全面评估。

    

    社区韧性是一个复杂且多维的现象，其源于不同社会技术系统之间的复杂非线性相互作用和其韧性特性。然而，目前关于社区韧性的研究主要集中在脆弱性评估，并采用基于指标的方法，其对于捕捉社区社会技术系统内部的异质特征及其非线性相互作用以塑造韧性的稳健性、冗余性和资源性等组成部分的能力有限。为了弥补这一差距，本文提出了一个基于三层深度学习模型的社区韧性评估方法（称为Resili-Net）。在社区社会技术系统（即设施、基础设施和社会）中，确定和计算了具体的12个可测量的韧性特征，与韧性的稳健性、冗余性和资源性三个组成部分相关。通过使用多个美国大都市统计区的公开可访问数据进行实证研究，证明了该方法的有效性。

    Community resilience is a complex and muti-faceted phenomenon that emerges from complex and nonlinear interactions among different socio-technical systems and their resilience properties. However, present studies on community resilience focus primarily on vulnerability assessment and utilize index-based approaches, with limited ability to capture heterogeneous features within community socio-technical systems and their nonlinear interactions in shaping robustness, redundancy, and resourcefulness components of resilience. To address this gap, this paper presents an integrated three-layer deep learning model for community resilience rating (called Resili-Net). Twelve measurable resilience features are specified and computed within community socio-technical systems (i.e., facilities, infrastructures, and society) related to three resilience components of robustness, redundancy, and resourcefulness. Using publicly accessible data from multiple metropolitan statistical areas in the United S
    
[^34]: MARRS: 多模态参考解析系统

    MARRS: Multimodal Reference Resolution System. (arXiv:2311.01650v1 [cs.CL])

    [http://arxiv.org/abs/2311.01650](http://arxiv.org/abs/2311.01650)

    MARRS是一个在设备上运行的多模态参考解析系统，能够处理对话式、视觉和背景上下文，并通过不同的机器学习模型实现上下文查询的处理。这个系统能够在保护用户隐私的同时理解上下文。

    

    成功处理上下文对于任何对话理解任务都是至关重要的。这个上下文可能是对话式的（依赖于之前的用户查询或系统回答），也可能是视觉的（依赖于用户看到的东西，例如他们的屏幕上），或者是背景的（基于一些信号，比如响起的闹钟或者正在播放的音乐）。在这项工作中，我们介绍了MARRS（多模态参考解析系统），它是一个在设备上运行的自然语言理解系统的框架，在处理对话式、视觉和背景上下文方面负责。特别是，我们提出了不同的机器学习模型来实现上下文查询的处理；具体而言，一个用于实现参考解析，一个用于通过查询重写处理上下文。我们还描述了这些模型如何相互补充，形成一个统一、连贯、轻量级的系统，可以在保护用户隐私的同时理解上下文。

    Successfully handling context is essential for any dialog understanding task. This context maybe be conversational (relying on previous user queries or system responses), visual (relying on what the user sees, for example, on their screen), or background (based on signals such as a ringing alarm or playing music). In this work, we present an overview of MARRS, or Multimodal Reference Resolution System, an on-device framework within a Natural Language Understanding system, responsible for handling conversational, visual and background context. In particular, we present different machine learning models to enable handing contextual queries; specifically, one to enable reference resolution, and one to handle context via query rewriting. We also describe how these models complement each other to form a unified, coherent, lightweight system that can understand context while preserving user privacy.
    
[^35]: RTP: 用内存去重思考张量并行性

    RTP: Rethinking Tensor Parallelism with Memory Deduplication. (arXiv:2311.01635v1 [cs.DC])

    [http://arxiv.org/abs/2311.01635](http://arxiv.org/abs/2311.01635)

    RTP是一种以内存去重为重点的创新方法，能够在分布式训练环境中优化内存消耗并实现与分布式数据并行性相当的性能。

    

    在神经网络模型的不断发展中，一个突出的挑战是与训练庞大模型相关的显著内存开销。本研究针对这一挑战深入研究了旋转张量并行性（RTP）。RTP是一种创新的方法，重点关注分布式训练环境中的内存去重。它具有定制的通信原语和Flyweight Pattern初始化等独特功能。此外，RTP确保了分区计算和分区权重通信之间的无缝重叠，优化了训练过程。我们的实证评估强调了RTP的效率，揭示了在分布式系统训练过程中，其内存消耗与最佳值非常接近 - 在多台机器之间公平地分配单台机器的内存开销。实验结果表明，RTP能够实现与分布式数据并行性相当的性能。

    In the evolving landscape of neural network models, one prominent challenge stand out: the significant memory overheads associated with training expansive models. Addressing this challenge, this study delves deep into the Rotated Tensor Parallelism (RTP). RTP is an innovative approach that strategically focuses on memory deduplication in distributed training environments. It boasts of unique features like a customized communication primitive and the Flyweight Pattern initialization. Furthermore, RTP ensures a seamless overlap between partition computation and partition weight communication, optimizing the training process. Our empirical evaluations underscore RTP's efficiency, revealing that its memory consumption during distributed system training is remarkably close to the optimal - distributing the memory overhead of a single machine equitably among multiple machines. The experimental results demonstrate that RTP is capable of achieving comparable performance to Distributed Data Par
    
[^36]: “接近……但不及教育者”--使用ChatGPT为大型协作学习中提供形成性反馈

    "Close...but not as good as an educator." -- Using ChatGPT to provide formative feedback in large-class collaborative learning. (arXiv:2311.01634v1 [cs.HC])

    [http://arxiv.org/abs/2311.01634](http://arxiv.org/abs/2311.01634)

    在大型协作学习中，使用ChatGPT为学习者提供形成性反馈，受访者对反馈持积极评价，但只有少数小组使用反馈来改进评估计划。

    

    在短时间内向多个问题导向学习小组提供个性化的形成性反馈几乎是不可能的。我们使用ChatGPT来提供个性化的形成性反馈，在一个小时的Zoom分组活动中教授从业卫生专业人员如何为数字卫生计划制定评估计划。学习者完成了包括Likert量表和开放式问题的评估调查，并进行了分析。44名调查受访者中有一半从未使用过ChatGPT。总体而言，受访者对反馈持积极评价，描述了各种不同的小组动态，并对反馈做出了适应性回应，但只有三个小组使用反馈循环来改进他们的评估计划。未来的教育者可以从我们的经验中学习，包括设计提示、提供如何使用ChatGPT的说明，以及为ChatGPT的最佳小组互动提供支持。未来的研究者应该探索ChatGPT对小组动态的影响。

    Delivering personalised, formative feedback to multiple problem-based learning groups in a short time period can be almost impossible. We employed ChatGPT to provide personalised formative feedback in a one-hour Zoom break-out room activity that taught practicing health professionals how to formulate evaluation plans for digital health initiatives. Learners completed an evaluation survey that included Likert scales and open-ended questions that were analysed. Half of the 44 survey respondents had never used ChatGPT before. Overall, respondents found the feedback favourable, described a wide range of group dynamics, and had adaptive responses to the feedback, yet only three groups used the feedback loop to improve their evaluation plans. Future educators can learn from our experience including engineering prompts, providing instructions on how to use ChatGPT, and scaffolding optimal group interactions with ChatGPT. Future researchers should explore the influence of ChatGPT on group dyna
    
[^37]: VQPy：一种面向现代视频分析的面向对象方法。

    VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])

    [http://arxiv.org/abs/2311.01623](http://arxiv.org/abs/2311.01623)

    VQPy是一种面向对象的视频分析方法，它使用Python变体作为前端，并具有可扩展的后端，可以自动构建和优化基于视频对象的处理流程。

    

    视频分析广泛应用于当今系统和服务中。在视频分析的前沿是用户开发的视频查询，以找到特定感兴趣的对象。基于视频对象（例如人，动物，汽车等）与传统面向对象语言建模的对象相似的洞察力，我们提出了一种面向视频分析的面向对象方法。这种方法名为VQPy，包括一个前端（一种Python变体，其中包含用户可以表达视频对象及其交互的结构）和一个可扩展的后端，可以基于视频对象自动生成和优化管道。我们已经实施和开源了VQPy，它已经作为Cisco DeepVision框架的一部分产品化。

    Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
    
[^38]: 提前选择性可塑性用于视觉任务的持续学习

    Look-Ahead Selective Plasticity for Continual Learning of Visual Tasks. (arXiv:2311.01617v1 [cs.CV])

    [http://arxiv.org/abs/2311.01617](http://arxiv.org/abs/2311.01617)

    提出了一种新的持续学习机制，利用对比表示学习来减少灾难性遗忘，通过观察冗余诱导能力，识别并保留对神经网络转移能力最有贡献的参数，以实现在任务边界时的选择性可塑性。

    

    对比表示学习已经成为一种有前途的持续学习技术，因为它可以学习到对灾难性遗忘具有鲁棒性并且对未来的任务有很好泛化能力的表示。以大脑中创建和更新的事件模型为启发，我们提出了一种新的机制，该机制发生在任务边界，即一个任务结束并另一个任务开始时。通过观察对比损失对神经网络输出的冗余诱导能力，我们的方法利用新任务的前几个样本，识别和保留对神经网络的转移能力最有贡献的参数，从而释放网络的其余部分来学习新的特征。我们在诸如CIFAR10和TinyImagenet等基准计算机视觉数据集上评估了所提出的方法，并展示了在任务增量方面的最先进性能。

    Contrastive representation learning has emerged as a promising technique for continual learning as it can learn representations that are robust to catastrophic forgetting and generalize well to unseen future tasks. Previous work in continual learning has addressed forgetting by using previous task data and trained models. Inspired by event models created and updated in the brain, we propose a new mechanism that takes place during task boundaries, i.e., when one task finishes and another starts. By observing the redundancy-inducing ability of contrastive loss on the output of a neural network, our method leverages the first few samples of the new task to identify and retain parameters contributing most to the transfer ability of the neural network, freeing up the remaining parts of the network to learn new features. We evaluate the proposed methods on benchmark computer vision datasets including CIFAR10 and TinyImagenet and demonstrate state-of-the-art performance in the task-incrementa
    
[^39]: 负责任的新兴多智能体行为

    Responsible Emergent Multi-Agent Behavior. (arXiv:2311.01609v1 [cs.AI])

    [http://arxiv.org/abs/2311.01609](http://arxiv.org/abs/2311.01609)

    这篇论文研究了负责任的新兴多智能体行为，探讨了在可解释性和公正性的框架下如何理解和塑造多智能体学习，并强调了人类问题解决的多智能体的本质特征。

    

    负责任的人工智能已经成为人工智能研究社区的焦点。随着基于神经网络的学习算法在实际应用中的普及，负责任人工智能领域在确保这些系统保持高度人类兼容性方面起到了重要作用。尽管取得了进展，负责任人工智能的现有技术却忽视了一个关键点：人类问题是多智能体问题。主流方法主要考虑单个人工智能系统在孤立环境中的性能，但人类问题本质上是多智能体问题。从驾车到谈判经济政策，人类问题解决涉及多个个体的相互作用和行为动机的相互作用。本论文开展了负责任的新兴多智能体行为的研究，展示了研究人员和从业者如何更好地理解和塑造多智能体学习与负责任人工智能的三个支柱：可解释性、公正性、代理回应。

    Responsible AI has risen to the forefront of the AI research community. As neural network-based learning algorithms continue to permeate real-world applications, the field of Responsible AI has played a large role in ensuring that such systems maintain a high-level of human-compatibility. Despite this progress, the state of the art in Responsible AI has ignored one crucial point: human problems are multi-agent problems. Predominant approaches largely consider the performance of a single AI system in isolation, but human problems are, by their very nature, multi-agent. From driving in traffic to negotiating economic policy, human problem-solving involves interaction and the interplay of the actions and motives of multiple individuals.  This dissertation develops the study of responsible emergent multi-agent behavior, illustrating how researchers and practitioners can better understand and shape multi-agent learning with respect to three pillars of Responsible AI: interpretability, fairn
    
[^40]: DRNet：基于深度强化学习的自动车道变换决策方法

    DRNet: A Decision-Making Method for Autonomous Lane Changingwith Deep Reinforcement Learning. (arXiv:2311.01602v1 [cs.RO])

    [http://arxiv.org/abs/2311.01602](http://arxiv.org/abs/2311.01602)

    DRNet是一种基于深度强化学习的决策框架，可以帮助自动驾驶车辆进行车道变换，并考虑到周围车辆的驾驶风格，实现安全的决策策略。

    

    机器学习技术在自动驾驶车辆的决策中表现出色。尽管近年来取得了一些进展，但由于复杂的驾驶场景和周围车辆的多变社交行为，车道变换仍然是一个主要挑战。为了改进现有技术，我们提出了DRNet，这是一种基于深度强化学习的全新、高效的框架，通过在模拟高速公路上执行合理的车道变换，并考虑周围车辆的驾驶风格，使深度强化学习代理能够学会驾驶。此外，为了实现安全的决策策略，DRNet结合了安全验证的思想，这是自动驾驶的最重要组成部分，确保在任何时刻只选择安全的行动。

    Machine learning techniques have outperformed numerous rule-based methods for decision-making in autonomous vehicles. Despite recent efforts, lane changing remains a major challenge, due to the complex driving scenarios and changeable social behaviors of surrounding vehicles. To help improve the state of the art, we propose to leveraging the emerging \underline{D}eep \underline{R}einforcement learning (DRL) approach for la\underline{NE} changing at the \underline{T}actical level. To this end, we present "DRNet", a novel and highly efficient DRL-based framework that enables a DRL agent to learn to drive by executing reasonable lane changing on simulated highways with an arbitrary number of lanes, and considering driving style of surrounding vehicles to make better decisions. Furthermore, to achieve a safe policy for decision-making, DRNet incorporates ideas from safety verification, the most important component of autonomous driving, to ensure that only safe actions are chosen at any ti
    
[^41]: 针对多个数据集约束宇宙学参数的领域适应图神经网络

    Domain Adaptive Graph Neural Networks for Constraining Cosmological Parameters Across Multiple Data Sets. (arXiv:2311.01588v1 [astro-ph.CO])

    [http://arxiv.org/abs/2311.01588](http://arxiv.org/abs/2311.01588)

    该论文研究了通过领域适应图神经网络对宇宙学参数进行约束的方法。通过利用GNNs捕捉宇宙学信息和使用最大均值差异进行领域适应，该方法在不同数据集上具有较好的泛化能力。

    

    研究表明，与依赖于摘要统计量（如功率谱）的方法相比，深度学习模型在从复杂宇宙学数据集中提取信息方面表现更好。然而，由于不同模拟套件中的子网格物理实现和数值逼近的差异，模型在一个宇宙学模拟的数据上训练后，在另一个模拟数据上的表现会下降。同样，对于任何模拟数据训练的模型，在应用于观测数据时也可能出现性能下降。通过在两个不同套件的CAMELS水动力宇宙学模拟数据上进行训练，我们研究了领域适应图神经网络（DA-GNNs）的泛化能力。通过利用GNNs，我们可以利用它们捕捉来自星系分布的结构无标度宇宙学信息的能力。此外，通过包括无监督的领域适配最大均值差异（MMD），我们使模型能够自适应地学习两个模拟数据之间的差异。

    Deep learning models have been shown to outperform methods that rely on summary statistics, like the power spectrum, in extracting information from complex cosmological data sets. However, due to differences in the subgrid physics implementation and numerical approximations across different simulation suites, models trained on data from one cosmological simulation show a drop in performance when tested on another. Similarly, models trained on any of the simulations would also likely experience a drop in performance when applied to observational data. Training on data from two different suites of the CAMELS hydrodynamic cosmological simulations, we examine the generalization capabilities of Domain Adaptive Graph Neural Networks (DA-GNNs). By utilizing GNNs, we capitalize on their capacity to capture structured scale-free cosmological information from galaxy distributions. Moreover, by including unsupervised domain adaptation via Maximum Mean Discrepancy (MMD), we enable our models to ex
    
[^42]: MetaReVision: 使用检索进行元学习的视觉基础组合概念获取

    MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition. (arXiv:2311.01580v1 [cs.CL])

    [http://arxiv.org/abs/2311.01580](http://arxiv.org/abs/2311.01580)

    MetaReVision是一种检索增强的元学习模型，通过使用检索到的基本概念作为支持集合来快速学习和识别新的图像基础组合概念。

    

    人类能够通过回忆和推广从过去的经验中获取的基本概念来学习新的组合概念。受到这一观察的启发，本文提出了一种检索增强的元学习模型 - MetaReVision，以解决视觉基础组合概念学习问题。MetaReVision由一个检索模块和一个元学习模块组成，旨在将检索到的基本概念作为支持集合并用于元训练视觉-语言模型来识别基于图像的组合概念。通过从检索器构建的episode进行元学习，MetaReVision学习到了一个通用的组合表示，可以快速更新以识别新的组合概念。我们创建了CompCOCO和CompFlickr来评估基于图像的组合概念学习。实验结果表明，MetaReVision优于其他竞争基线，并且检索模块起着重要作用。

    Humans have the ability to learn novel compositional concepts by recalling and generalizing primitive concepts acquired from past experiences. Inspired by this observation, in this paper, we propose MetaReVision, a retrieval-enhanced meta-learning model to address the visually grounded compositional concept learning problem. The proposed MetaReVision consists of a retrieval module and a meta-learning module which are designed to incorporate retrieved primitive concepts as a supporting set to meta-train vision-anguage models for grounded compositional concept recognition. Through meta-learning from episodes constructed by the retriever, MetaReVision learns a generic compositional representation that can be fast updated to recognize novel compositional concepts. We create CompCOCO and CompFlickr to benchmark the grounded compositional concept learning. Our experimental results show that MetaReVision outperforms other competitive baselines and the retrieval module plays an important role 
    
[^43]: 使用视觉语言驱动的图像增强改善公平性

    Improving Fairness using Vision-Language Driven Image Augmentation. (arXiv:2311.01573v1 [cs.CV])

    [http://arxiv.org/abs/2311.01573](http://arxiv.org/abs/2311.01573)

    本文提出了一种基于视觉语言驱动的图像增强方法，以改善训练深度学习模型中的公平性问题。通过学习和应用可解释的路径来编辑受保护特征，该方法成功减轻了数据中的相关性，提高了数据集的公平性。

    

    在训练深度学习鉴别模型时，公平性是至关重要的，特别是在面部领域。模型往往将特定特征（如年龄和肤色）与无关属性（下游任务）相关联，导致偏见与现实不符。众所周知，这些相关性存在于数据中，然后在训练过程中传递给模型。本文提出了一种方法来减轻这些相关性，以提高公平性。为此，我们学习了在预训练扩散模型（DiffAE）的语义空间中位于可解释和有意义的路径，这些路径由对比性文本二极体进行监督。也就是说，我们学习了编辑受保护特征（年龄和肤色）。然后将这些路径应用于增强图像，以提高给定数据集的公平性。我们在CelebA-HQ和UTKFace上对年龄和肤色作为受保护特征的几个下游任务进行了该方法的测试。

    Fairness is crucial when training a deep-learning discriminative model, especially in the facial domain. Models tend to correlate specific characteristics (such as age and skin color) with unrelated attributes (downstream tasks), resulting in biases which do not correspond to reality. It is common knowledge that these correlations are present in the data and are then transferred to the models during training. This paper proposes a method to mitigate these correlations to improve fairness. To do so, we learn interpretable and meaningful paths lying in the semantic space of a pre-trained diffusion model (DiffAE) -- such paths being supervised by contrastive text dipoles. That is, we learn to edit protected characteristics (age and skin color). These paths are then applied to augment images to improve the fairness of a given dataset. We test the proposed method on CelebA-HQ and UTKFace on several downstream tasks with age and skin color as protected characteristics. As a proxy for fairnes
    
[^44]: 基础模型的市场集中度影响

    Market Concentration Implications of Foundation Models. (arXiv:2311.01550v1 [cs.AI])

    [http://arxiv.org/abs/2311.01550](http://arxiv.org/abs/2311.01550)

    对基础模型市场的结构进行分析，发现最强大的模型趋向自然垄断，需要双管齐下的监管响应来确保市场竞争和模型质量标准，以最大程度地对社会福利做出贡献。

    

    我们分析了基础模型市场的结构，即那些驱动ChatGPT等应用并适用于下游用途的大型AI模型，并研究了其对竞争政策和监管的影响。我们观察到，能力最强的模型有一种向自然垄断的趋势，并且可能面对广阔的市场。这需要双管齐下的监管响应：（一）反垄断机构需要通过解决战略行为，特别是确保垄断不会垂直传播到下游用途，来确保市场的可竞争性；（二）考虑到市场约束的降低，监管机构应确保最具能力的模型符合足够的质量标准（包括安全、隐私、非歧视、可靠性和互操作性标准），以最大程度地对社会福利做出贡献。监管机构还应确保AI和非AI应用之间有一个公平的监管环境。

    We analyze the structure of the market for foundation models, i.e., large AI models such as those that power ChatGPT and that are adaptable to downstream uses, and we examine the implications for competition policy and regulation. We observe that the most capable models will have a tendency towards natural monopoly and may have potentially vast markets. This calls for a two-pronged regulatory response: (i) Antitrust authorities need to ensure the contestability of the market by tackling strategic behavior, in particular by ensuring that monopolies do not propagate vertically to downstream uses, and (ii) given the diminished potential for market discipline, there is a role for regulators to ensure that the most capable models meet sufficient quality standards (including safety, privacy, non-discrimination, reliability and interoperability standards) to maximally contribute to social welfare. Regulators should also ensure a level regulatory playing field between AI and non-AI application
    
[^45]: 使用机械性能进行开放式对象识别

    Open-Set Object Recognition Using Mechanical Properties During Interaction. (arXiv:2311.01540v1 [cs.RO])

    [http://arxiv.org/abs/2311.01540](http://arxiv.org/abs/2311.01540)

    提出了一种利用机械性能进行开放式对象识别的框架，通过利用已知对象的知识来估计聚类中心和大小的聚类算法，通过实验证明该框架能够更好地识别对象，并且我们的聚类算法表现更好。

    

    尽管大多数触觉机器人在封闭集条件下运行，但它们在超出机器人知识范围的开放条件下操作是具有挑战性的。我们提出了一个使用机械性能进行开放式识别的框架，以识别已知对象并逐步标记新对象。其主要贡献是利用已知对象的知识来估计聚类中心和大小的聚类算法，而不是像传统算法那样随机选择。该框架通过与实际对象的交互中估计的机械性能进行验证。结果显示，与由新颖性检测器贡献的替代方法相比，该框架能更好地识别对象。重要的是，我们的聚类算法比其他方法具有更好的聚类性能。此外，超参数研究显示，聚类结果与聚类大小密切相关，需要适当进行调整。

    while most of the tactile robots are operated in close-set conditions, it is challenging for them to operate in open-set conditions where test objects are beyond the robots' knowledge. We proposed an open-set recognition framework using mechanical properties to recongise known objects and incrementally label novel objects. The main contribution is a clustering algorithm that exploits knowledge of known objects to estimate cluster centre and sizes, unlike a typical algorithm that randomly selects them. The framework is validated with the mechanical properties estimated from a real object during interaction. The results show that the framework could recognise objects better than alternative methods contributed by the novelty detector. Importantly, our clustering algorithm yields better clustering performance than other methods. Furthermore, the hyperparameters studies show that cluster size is important to clustering results and needed to be tuned properly.
    
[^46]: NOD-TAMP:多步骤操纵规划中的神经物体描述符

    NOD-TAMP: Multi-Step Manipulation Planning with Neural Object Descriptors. (arXiv:2311.01530v1 [cs.RO])

    [http://arxiv.org/abs/2311.01530](http://arxiv.org/abs/2311.01530)

    NOD-TAMP是一个基于TAMP的框架，利用神经物体描述符来解决复杂操纵任务中的泛化问题，通过从少量人类演示中提取轨迹并进行调整，有效解决了长时程任务的挑战，并在模拟环境中优于现有方法。

    

    在家居和工厂环境中开发复杂操纵任务的智能机器人仍然具有挑战性，因为长时程任务、接触丰富的操纵以及需要在各种物体形状和场景布局之间进行泛化。虽然任务和运动规划（TAMP）提供了一个有希望的解决方案，但是它的假设，如动力学模型，限制了它在新颖背景中的适应性。神经物体描述符（NODs）在物体和场景泛化方面显示出了潜力，但在处理更广泛任务方面存在局限性。我们提出的基于TAMP的框架NOD-TAMP从少数人类演示中提取短的操纵轨迹，使用NOD特征来调整这些轨迹，并组合它们来解决广泛的长时程任务。在模拟环境中验证后，NOD-TAMP有效应对各种挑战，优于现有方法，建立了一个强有力的操纵规划框架。

    Developing intelligent robots for complex manipulation tasks in household and factory settings remains challenging due to long-horizon tasks, contact-rich manipulation, and the need to generalize across a wide variety of object shapes and scene layouts. While Task and Motion Planning (TAMP) offers a promising solution, its assumptions such as kinodynamic models limit applicability in novel contexts. Neural object descriptors (NODs) have shown promise in object and scene generalization but face limitations in addressing broader tasks. Our proposed TAMP-based framework, NOD-TAMP, extracts short manipulation trajectories from a handful of human demonstrations, adapts these trajectories using NOD features, and composes them to solve broad long-horizon tasks. Validated in a simulation environment, NOD-TAMP effectively tackles varied challenges and outperforms existing methods, establishing a cohesive framework for manipulation planning. For videos and other supplemental material, see the pr
    
[^47]: 当被要求生成代码解释时，大型语言模型的行为研究

    The Behavior of Large Language Models When Prompted to Generate Code Explanations. (arXiv:2311.01490v1 [cs.SE])

    [http://arxiv.org/abs/2311.01490](http://arxiv.org/abs/2311.01490)

    本论文研究了大型语言模型在生成代码解释时的行为。Java和Python的解释在可读性和词汇密度方面表现一致，但在完整性、简洁性和上下文性方面得分较低。

    

    本论文系统地探究了大型语言模型（LLMs）在生成介绍编程课程中使用的代码示例解释时的行为。正如我们所展示的，LLMs生成的代码解释的性质在很大程度上取决于提示的措辞、被解释的目标代码示例、编程语言、温度参数和LLM的版本。然而，对于Java和Python而言，它们在两个主要方面保持一致：可读性水平大约在7-8年级，以及词汇密度，即与总解释大小相对的有意义的单词的相对大小。此外，这些解释在正确性方面得分很高，但在完整性、简洁性和上下文性方面得分较低。

    This paper systematically explores how Large Language Models (LLMs) generate explanations of code examples of the type used in intro-to-programming courses. As we show, the nature of code explanations generated by LLMs varies considerably based on the wording of the prompt, the target code examples being explained, the programming language, the temperature parameter, and the version of the LLM. Nevertheless, they are consistent in two major respects for Java and Python: the readability level, which hovers around 7-8 grade, and lexical density, i.e., the relative size of the meaningful words with respect to the total explanation size. Furthermore, the explanations score very high in correctness but less on three other metrics: completeness, conciseness, and contextualization.
    
[^48]: FedSN：一个适用于LEO卫星网络的通用联邦学习框架

    FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v1 [cs.LG])

    [http://arxiv.org/abs/2311.01483](http://arxiv.org/abs/2311.01483)

    FedSN是一个通用的联邦学习框架，用于解决在LEO卫星网络中的异构计算和存储能力、有限的上行速率以及模型陈旧等关键挑战。

    

    最近，许多低地球轨道（LEO）卫星已经由商业公司成功地发射和部署到太空中，如SpaceX。由于LEO卫星配备了多模传感器，它们不仅用于通信，还用于各种机器学习应用，如空间调制识别、遥感图像分类等。然而，由于与LEO卫星的有限接触时间（例如5分钟），地面站（GS）可能无法下载如此大量的原始感测数据进行集中模型训练。因此，联邦学习（FL）已经成为解决这个问题的有希望的解决方案，通过在设备上进行训练。不幸的是，要在LEO卫星上使用FL，我们仍然面临三个关键挑战，即i）异构计算和存储能力，ii）有限的上行速率，以及iii）模型陈旧问题。为此，我们提出了一种名为FedSN的通用FL框架来解决上述挑战，一

    Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, an
    
[^49]: 从新闻文章中提取关系（RENA）：用于流行病监测的工具

    Relation Extraction from News Articles (RENA): A Tool for Epidemic Surveillance. (arXiv:2311.01472v1 [cs.CL])

    [http://arxiv.org/abs/2311.01472](http://arxiv.org/abs/2311.01472)

    RENA是一种基于浏览器的关系提取工具，用于从英语新闻文章中提取与传染病相关的关键实体和语义关系，为流行病监测提供实时解析和关键信息提取的能力。

    

    关系提取从新闻文章（RENA）是一个基于浏览器的工具，旨在提取与传染病相关的英语新闻文章中的关键实体及其语义关系。该系统采用React框架构建，为用户提供了一个优雅且用户友好的界面。它允许用户输入新闻文章并从两个模型中选择，以生成所提供文本中的关系的全面列表。因此，RENA允许实时解析新闻文章提取流行病监测的关键信息，为开源情报驱动的流行病预警系统EPIWATCH做出贡献。

    Relation Extraction from News Articles (RENA) is a browser-based tool designed to extract key entities and their semantic relationships in English language news articles related to infectious diseases. Constructed using the React framework, this system presents users with an elegant and user-friendly interface. It enables users to input a news article and select from a choice of two models to generate a comprehensive list of relations within the provided text. As a result, RENA allows real-time parsing of news articles to extract key information for epidemic surveillance, contributing to EPIWATCH, an open-source intelligence-based epidemic warning system.
    
[^50]: 支持可信度的LLM创建过程：处理医疗AI中的幻觉

    Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI. (arXiv:2311.01463v1 [cs.CL])

    [http://arxiv.org/abs/2311.01463](http://arxiv.org/abs/2311.01463)

    这篇论文描述了在医疗人工智能中创建可靠、可信和无偏置的LLM模型的关键要素，着重于量化、验证和缓解幻觉问题，并讨论了LLM在医疗领域的未来发展。

    

    在短时间内，大型语言模型在多个领域中迅速增多。然而，由于准确性、连贯性和幻觉等问题，医疗领域对其采用存在犹豫。鉴于医疗事关重大，许多研究人员甚至提出在解决这些问题之前不应使用这些模型。在本文中，我们描述了创建可靠、可信和无偏置模型的关键要素，这是其在医疗领域应用的必要条件。具体而言，我们着重于在医疗背景下对幻觉进行量化、验证和缓解。最后，我们讨论了LLM在医疗领域未来的可能发展方向。

    Large language models have proliferated across multiple domains in as short period of time. There is however hesitation in the medical and healthcare domain towards their adoption because of issues like factuality, coherence, and hallucinations. Give the high stakes nature of healthcare, many researchers have even cautioned against its usage until these issues are resolved. The key to the implementation and deployment of LLMs in healthcare is to make these models trustworthy, transparent (as much possible) and explainable. In this paper we describe the key elements in creating reliable, trustworthy, and unbiased models as a necessary condition for their adoption in healthcare. Specifically we focus on the quantification, validation, and mitigation of hallucinations in the context in healthcare. Lastly, we discuss how the future of LLMs in healthcare may look like.
    
[^51]: FacadeNet: 通过选择性编辑进行条件立面综合的深度学习方法

    FacadeNet: Conditional Facade Synthesis via Selective Editing. (arXiv:2311.01240v1 [cs.CV])

    [http://arxiv.org/abs/2311.01240](http://arxiv.org/abs/2311.01240)

    FacadeNet是一种深度学习方法，通过条件生成对抗网络实现了从不同视角合成建筑立面图像，并通过引入选择性编辑模块，实现了对视角相关元素进行精确修改。实验结果表明，该方法在建筑立面生成方面具有领先的性能。

    

    我们介绍了FacadeNet，一种基于深度学习的方法，可以从不同的视角合成建筑立面图像。我们的方法使用条件生成对抗网络，将一个立面的单一视图和所需的视角信息作为输入，生成一个从不同视角看到的立面图像。为了精确修改与视角相关的元素（如窗户和门），同时保留与视角无关的结构（如墙壁），我们引入了选择性编辑模块。该模块利用从预训练的视觉转换器中提取的图像嵌入。我们的实验表明，在建筑立面生成方面，我们的方法表现出了最先进的性能，超过了其他方法。

    We introduce FacadeNet, a deep learning approach for synthesizing building facade images from diverse viewpoints. Our method employs a conditional GAN, taking a single view of a facade along with the desired viewpoint information and generates an image of the facade from the distinct viewpoint. To precisely modify view-dependent elements like windows and doors while preserving the structure of view-independent components such as walls, we introduce a selective editing module. This module leverages image embeddings extracted from a pre-trained vision transformer. Our experiments demonstrated state-of-the-art performance on building facade generation, surpassing alternative methods.
    
[^52]: 带有TinyissimoYOLO的AI集成智能眼镜上的超高效设备内目标检测

    Ultra-Efficient On-Device Object Detection on AI-Integrated Smart Glasses with TinyissimoYOLO. (arXiv:2311.01057v1 [cs.CV])

    [http://arxiv.org/abs/2311.01057](http://arxiv.org/abs/2311.01057)

    本文介绍了在智能眼镜上实现超高效设备内目标检测的设计和实施，利用新型低功耗处理器实现小型机器学习算法，以便在具有小尺寸和有限电池容量的智能眼镜上实现长时间连续运行。

    

    智能眼镜借助尖端计算技术、加速硬件架构和小型AI算法，正迅速获得先进功能。在面向全天使用以实现满意用户体验时，将AI集成到具有小尺寸和有限电池容量的智能眼镜仍然具有挑战性。本文阐述了利用新型低功耗处理器实现小型机器学习算法设计和实现，以在智能眼镜中实现长时间连续运行。我们探索了智能眼镜在实时目标检测情况下的能量和时延效率。为此，我们设计了一个智能眼镜原型作为研究平台，其中包括两个微控制器，包括一个具有视觉AI硬件加速器的新型毫瓦级功率RISC-V并行处理器，以及一个用于通信的低功耗蓝牙模块。智能眼镜集成了图像和音频感应接口等电源循环机制。

    Smart glasses are rapidly gaining advanced functionality thanks to cutting-edge computing technologies, accelerated hardware architectures, and tiny AI algorithms. Integrating AI into smart glasses featuring a small form factor and limited battery capacity is still challenging when targeting full-day usage for a satisfactory user experience. This paper illustrates the design and implementation of tiny machine-learning algorithms exploiting novel low-power processors to enable prolonged continuous operation in smart glasses. We explore the energy- and latency-efficient of smart glasses in the case of real-time object detection. To this goal, we designed a smart glasses prototype as a research platform featuring two microcontrollers, including a novel milliwatt-power RISC-V parallel processor with a hardware accelerator for visual AI, and a Bluetooth low-power module for communication. The smart glasses integrate power cycling mechanisms, including image and audio sensing interfaces. Fur
    
[^53]: 通过使用语言模型模拟受众群体，改善人际沟通

    Improving Interpersonal Communication by Simulating Audiences with Language Models. (arXiv:2311.00687v1 [cs.AI])

    [http://arxiv.org/abs/2311.00687](http://arxiv.org/abs/2311.00687)

    本论文提出了一个基于大型语言模型（LLM）模拟的框架，通过探索解决方案空间、生成沟通候选以及模拟受众反应，来改善人际沟通。通过评估八个涵盖人际沟通基本过程的场景，展示了该框架的有效性。

    

    我们如何与他人进行沟通以实现自己的目标？我们利用先前的经验或他人的建议，或者通过预测对方的反应来构造候选表达。然而，我们的经验是有限和有偏见的，而且对潜在结果进行推理可能是困难且认知上具有挑战性的。本文中，我们探讨了如何利用大型语言模型（LLM）模拟来帮助我们更好地沟通。我们提出了探索-生成-模拟（EGS）框架，该框架接受任何一个个体与一个目标受众进行沟通的场景作为输入。EGS（1）通过生成与场景相关的多样化建议来探索解决方案空间，（2）生成以部分建议为条件的沟通候选，（3）模拟不同受众的反应，以确定最佳候选和建议的使用。我们在涵盖人际沟通十个基本过程的八个场景上评估了该框架。

    How do we communicate with others to achieve our goals? We use our prior experience or advice from others, or construct a candidate utterance by predicting how it will be received. However, our experiences are limited and biased, and reasoning about potential outcomes can be difficult and cognitively challenging. In this paper, we explore how we can leverage Large Language Model (LLM) simulations to help us communicate better. We propose the Explore-Generate-Simulate (EGS) framework, which takes as input any scenario where an individual is communicating to an audience with a goal they want to achieve. EGS (1) explores the solution space by producing a diverse set of advice relevant to the scenario, (2) generates communication candidates conditioned on subsets of the advice, and (3) simulates the reactions from various audiences to determine both the best candidate and advice to use. We evaluate the framework on eight scenarios spanning the ten fundamental processes of interpersonal com
    
[^54]: CapsFusion: 重新思考大规模图像-文本数据

    CapsFusion: Rethinking Image-Text Data at Scale. (arXiv:2310.20550v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.20550](http://arxiv.org/abs/2310.20550)

    CapsFusion是一个先进的框架，通过利用大型语言模型整合和细化来自网络图像-文本对和合成字幕的信息，提供了更高质量、更可扩展的多模态预训练数据。

    

    大规模多模态模型展示了在零样本情况下执行多样化多模态任务的显著泛化能力。大规模基于网络的图像-文本对在这一成功中起着根本性的贡献，但存在着过多的噪声。最近的研究使用由生成式字幕模型合成的替代字幕，并取得了显著的基准性能。然而，我们的实验证明，在使用合成字幕训练的模型中存在着显著的可扩展性不足和世界知识丧失问题，这些问题在其初始基准成功中大部分被掩盖了。经过进一步的研究，我们确定根本原因是现有合成字幕中过于简化的语言结构和缺乏知识细节。为了提供更高质量、更可扩展的多模态预训练数据，我们提出了CapsFusion，这是一个先进的框架，利用大型语言模型来整合和细化来自网络图像-文本对和合成字幕的信息。

    Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experi
    
[^55]: 多视觉运动系统的元学习

    Meta Learning for Multi-View Visuomotor Systems. (arXiv:2310.20414v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2310.20414](http://arxiv.org/abs/2310.20414)

    该论文提出了一种新的方法，使用元学习来快速适应多视图运动系统，实验结果表明可以显著减少所需的新训练周期数量。

    

    本文介绍了一种新的方法，用于快速适应机器人的多视觉运动系统，以适应不同的相机配置。它利用元学习来微调感知网络，同时保持策略网络不变。实验结果表明，所需的新训练周期数量显著减少，以实现基准性能。

    This paper introduces a new approach for quickly adapting a multi-view visuomotor system for robots to varying camera configurations from the baseline setup. It utilises meta-learning to fine-tune the perceptual network while keeping the policy network fixed. Experimental results demonstrate a significant reduction in the number of new training episodes needed to attain baseline performance.
    
[^56]: GPT-4V在医学影像中的多模态能力的全面研究

    A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging. (arXiv:2310.20381v1 [cs.CV])

    [http://arxiv.org/abs/2310.20381](http://arxiv.org/abs/2310.20381)

    本文对GPT-4V在医学影像中的多模态能力进行了全面研究和评估，发现其在生成描述性报告和医学VQA方面有潜力，但在某些评估指标上仍需改进。

    

    本文对GPT-4V在不同医学影像任务中的能力进行了全面评估，包括放射学报告生成、医学视觉问答(VQA)和视觉定位。尽管先前的研究探索了GPT-4V在医学影像中的性能，但据我们所知，我们的研究是首个基于公开可用基准的定量评估。我们的研究发现，当给出结构良好的提示时，GPT-4V在胸部X射线图像的生成描述性报告方面具有潜力。然而，在MIMIC-CXR数据集基准上的表现揭示了某些评估指标(如CIDEr)的改进空间。在医学VQA领域，GPT-4V在区分问题类型方面表现出熟练，但在准确度方面不及现有基准。此外，我们的分析发现常规评估指标如BLEU分数的局限性，呼吁开发更好的评价指标。

    This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding. While prior efforts have explored GPT-4V's performance in medical imaging, to the best of our knowledge, our study represents the first quantitative evaluation on publicly available benchmarks. Our findings highlight GPT-4V's potential in generating descriptive reports for chest X-ray images, particularly when guided by well-structured prompts. However, its performance on the MIMIC-CXR dataset benchmark reveals areas for improvement in certain evaluation metrics, such as CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in distinguishing between question types but falls short of prevailing benchmarks in terms of accuracy. Furthermore, our analysis finds the limitations of conventional evaluation metrics like the BLEU score, advocating for the development of m
    
[^57]: 用于临床总结中事实对齐的合成模仿编辑反馈

    Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization. (arXiv:2310.20033v1 [cs.CL])

    [http://arxiv.org/abs/2310.20033](http://arxiv.org/abs/2310.20033)

    本文提出了一种使用ChatGPT来生成高质量反馈数据以改善临床笔记总结的事实一致性的新方法。

    

    大型语言模型（LLMs）如GPT和LLaMA系列在捕捉和浓缩关键上下文信息及在总结任务中实现最先进的性能方面表现出了异常能力。然而，社区对这些模型的虚构问题的担忧仍在不断上升。LLMs有时会生成虚构的摘要，这在临床领域的NLP任务（例如临床笔记总结）中可能会导致严重错误的诊断。使用人类反馈对LLMs进行微调已经显示出在生成过程中实现事实一致性的承诺，但这种训练过程需要高质量的人工注释数据，而在临床领域获取这样的数据可能非常昂贵。在这项工作中，我们提出了一种新的管道，使用ChatGPT代替人类专家生成高质量的反馈数据，以改善临床笔记总结的事实一致性。

    Large Language Models (LLMs) like the GPT and LLaMA families have demonstrated exceptional capabilities in capturing and condensing critical contextual information and achieving state-of-the-art performance in the summarization task. However, community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using human feedback has shown the promise of aligning LLMs to be factually consistent during generation, but such training procedure requires high-quality human-annotated data, which can be extremely expensive to get in the clinical domain. In this work, we propose a new pipeline using ChatGPT instead of human experts to generate high-quality feedback data for improving factual consistency in the clinical note summari
    
[^58]: JEN-1 Composer: 一个用于高保真多音轨音乐生成的统一框架

    JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music Generation. (arXiv:2310.19180v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2310.19180](http://arxiv.org/abs/2310.19180)

    JEN-1 Composer是一个统一的框架，能够以高保真、灵活的方式生成多音轨音乐。

    

    随着生成式人工智能的快速发展，从零开始生成音乐的文本到音乐合成任务已成为一个有前景的方向。然而，对于多音轨生成的更细粒度控制仍然是一个挑战。现有模型具有较强的原始生成能力，但缺乏以可控的方式单独组成和组合多音轨的灵活性，这与人类作曲家的典型工作流程不同。为了解决这个问题，我们提出了JEN-1 Composer，一个统一的框架，通过一个模型高效地建模多音轨音乐的边缘、条件和联合分布。JEN-1 Composer框架能够无缝地整合任何基于扩散的音乐生成系统，例如Jen-1，增强其多功能多音轨音乐生成能力。我们引入了一种课程训练策略，以逐步指导模型从单音轨生成到灵活的生成过程。

    With rapid advances in generative artificial intelligence, the text-to-music synthesis task has emerged as a promising direction for music generation from scratch. However, finer-grained control over multi-track generation remains an open challenge. Existing models exhibit strong raw generation capability but lack the flexibility to compose separate tracks and combine them in a controllable manner, differing from typical workflows of human composers. To address this issue, we propose JEN-1 Composer, a unified framework to efficiently model marginal, conditional, and joint distributions over multi-track music via a single model. JEN-1 Composer framework exhibits the capacity to seamlessly incorporate any diffusion-based music generation system, \textit{e.g.} Jen-1, enhancing its capacity for versatile multi-track music generation. We introduce a curriculum training strategy aimed at incrementally instructing the model in the transition from single-track generation to the flexible genera
    
[^59]: 通过创建固定目标来改进内在探索

    Improving Intrinsic Exploration by Creating Stationary Objectives. (arXiv:2310.18144v1 [cs.LG])

    [http://arxiv.org/abs/2310.18144](http://arxiv.org/abs/2310.18144)

    该论文提出了一个新的方法：通过创建固定目标，将原始的非固定奖励转化为固定奖励，从而改善了强化学习中的内在探索。

    

    强化学习中的探索奖励通过定义自定义的内在目标来引导长期探索。基于计数的方法使用状态访问频率来获得探索奖励。本文发现，任何从基于计数的方法导出的内在奖励函数都是非固定的，因此为代理人构建了一个难以优化的目标。我们工作的关键贡献在于通过增强状态表示将原始的非固定奖励转化为固定奖励。为此，我们引入了用于探索的固定目标（SOFE）框架。SOFE需要识别不同探索奖励的足够统计量，并找到一种将这些统计量高效编码作为深度网络输入的方法。SOFE基于提出扩展状态空间的状态增强，但有希望简化代理目标的优化。我们的实验结果表明，SOFE改善了探索效果。

    Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Count-based methods use the frequency of state visits to derive an exploration bonus. In this paper, we identify that any intrinsic reward function derived from count-based methods is non-stationary and hence induces a difficult objective to optimize for the agent. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework. SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. Our experiments show that SOFE improves the
    
[^60]: 在快速发展时代管理人工智能风险

    Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)

    [http://arxiv.org/abs/2310.17688](http://arxiv.org/abs/2310.17688)

    在人工智能快速进展的时代，我们提出了管理即将到来的先进人工智能系统所带来的风险的优先事项。

    

    在这篇简短的共识文中，我们概述了即将到来的先进人工智能系统所带来的风险。我们审查了大规模的社会危害和恶意使用，以及人类对自主人工智能系统失去控制的不可逆转的损失。鉴于人工智能的快速和持续进展，我们提出了人工智能研发和治理的优先事项。

    In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose priorities for AI R&D and governance.
    
[^61]: "凯利是一个温暖的人，约瑟夫是一个榜样": LLM生成的推荐信中的性别偏见

    "Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v1 [cs.CL])

    [http://arxiv.org/abs/2310.09219](http://arxiv.org/abs/2310.09219)

    本文对LLM生成的推荐信中的性别偏见进行了细致的研究，并设计了评估方法来展现通过语言风格和词汇内容来体现的性别偏见。

    

    随着生成语言模型的进步，用户已经开始使用大型语言模型（LLM）来协助撰写各种类型的内容，包括推荐信等职业文件。尽管它们的方便性，但这些应用引入了前所未有的公平问题。由于生成的推荐信可能被用户直接在职业或学术场景中使用，它们有可能造成直接的社会伤害，如降低女性申请者的成功率。因此，对于未来的缓解和监控，全面研究此类实际应用情况中的公平问题和相关伤害势在必行。在本文中，我们对LLM生成的推荐信中的性别偏见进行了批判性的研究。受社会科学研究结果的启发，我们设计了评估方法，通过两个维度来展现LLM生成的信件中的性别偏见：语言风格的偏见和词汇内容的偏见。此外，我们还研究了推荐信中性别偏见的程度。

    As generative language models advance, users have started to utilize Large Language Models (LLMs) to assist in writing various types of content, including professional documents such as recommendation letters. Despite their convenience, these applications introduce unprecedented fairness concerns. As generated reference letters might be directly utilized by users in professional or academic scenarios, they have the potential to cause direct social harms, such as lowering success rates for female applicants. Therefore, it is imminent and necessary to comprehensively study fairness issues and associated harms in such real-world use cases for future mitigation and monitoring. In this paper, we critically examine gender bias in LLM-generated reference letters. Inspired by findings in social science, we design evaluation methods to manifest gender biases in LLM-generated letters through 2 dimensions: biases in language style and biases in lexical content. Furthermore, we investigate the ext
    
[^62]: 动态 Top-k 估计方法用于整合特征归因方法之间的分歧

    Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods. (arXiv:2310.05619v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05619](http://arxiv.org/abs/2310.05619)

    本文提出了一种通过分析归因分数的连续属性来确定应显示的最佳 k 个标记的动态 Top-k 估计方法，用于整合特征归因方法之间的分歧。实验证明，动态 k 主要改进了集成梯度和 GradientXInput 的表现，为人类解释提供了具有信息价值的归因信号。

    

    特征归因分数用于通过突出显示 k 个标记来向用户解释文本分类器的预测。本文提出了一种通过分析归因分数的连续属性来确定应显示的最佳 k 个标记的方法。我们的方法在句子之间是动态的，不依赖于具体的方法，并且可以处理句子长度的偏差。我们通过在 NLI 任务中比较多种方法和人类之间的一致性，使用固定的 k 和动态的 k。我们发现，在使用静态的 k 时，基于扰动的方法和 Vanilla Gradient 在大多数方法之间和方法与人类之间的一致性指标上表现得最好。然而，它们在使用动态 k 时的优势消失了，而动态 k 主要改进了集成梯度和 GradientXInput 的表现。据我们所知，这是首次证明通过分析归因分数的连续属性对于整合人类解释的归因信号是具有信息价值的。

    Feature attribution scores are used for explaining the prediction of a text classifier to users by highlighting a k number of tokens. In this work, we propose a way to determine the number of optimal k tokens that should be displayed from sequential properties of the attribution scores. Our approach is dynamic across sentences, method-agnostic, and deals with sentence length bias. We compare agreement between multiple methods and humans on an NLI task, using fixed k and dynamic k. We find that perturbation-based methods and Vanilla Gradient exhibit highest agreement on most method--method and method--human agreement metrics with a static k. Their advantage over other methods disappears with dynamic ks which mainly improve Integrated Gradient and GradientXInput. To our knowledge, this is the first evidence that sequential properties of attribution scores are informative for consolidating attribution signals for human interpretation.
    
[^63]: 个性化随机鹦鹉更危险吗？评估对话系统中的人格偏见

    Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05280](http://arxiv.org/abs/2310.05280)

    这项研究评估了对话系统中的人格偏见对社交偏见的影响，并建立了一个综合评估框架来衡量不同人格采用下的偏见程度。

    

    最近大型语言模型的发展使其能够按照自由形式的指令进行操作，包括在对话中模仿通用或特定人口群体的人格。通用人格指的是来自某一人口群体的个体（例如亚洲人），而特定人格可以是历史人物的实际姓名。虽然采用人格使对话系统更具吸引力和亲和力，但也存在潜在风险，可能通过与用户的交互而加剧社会偏见，进一步造成社会伤害。在本文中，我们系统地研究“人格偏见”，我们将其定义为有害对话模型行为对不同人格采用的敏感性。我们将人格偏见分为有害表达和有害认同两类，同时建立了一个全面的评估框架，以衡量五个方面的人格偏见：冒犯性、有毒延续、关怀、刻板印象的认同以及

    Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. Generic personas refer to an individual from a demographic group (e.g. an Asian person), whereas specific personas can be actual names of historical figures. While the adoption of personas allows dialogue systems to be more engaging and approachable to users, it also carries the potential risk of exacerbating social biases in model responses, further causing societal harms through interactions with users. In this paper, we systematically study "persona biases", which we define to be the sensitivity of harmful dialogue model behaviors to different persona adoptions. We categorize persona biases into biases in harmful expression and harmful agreement, as well as establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and To
    
[^64]: 学习可分离的隐藏单元贡献用于适应个体的唇读技术

    Learning Separable Hidden Unit Contributions for Speaker-Adaptive Lip-Reading. (arXiv:2310.05058v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.05058](http://arxiv.org/abs/2310.05058)

    本文提出了一种用于适应个体的唇读方法，通过学习可分离的隐藏单元贡献，实现了对浅层和深层的不同处理，并利用个体的特征来自适应地增强或抑制唇读特征，提高唇读的性能和鲁棒性。

    

    本文提出了一种新颖的适应个体的唇读方法。首先，一个个体的特征可以通过他/她的几张面部图像，甚至是一张浅层网络的单一图像来准确描绘，而与讲话面部相关的细粒度动态特征则需要深层序列网络来准确表示。因此，我们将浅层和深层分别处理以适应不同的唇读情境。其次，我们观察到一个个体独特的特征（例如突出的口腔和下颌）对于不同的单词和发音的唇读表现具有不同的影响，需要自适应地增强或抑制特征以实现稳健的唇读。基于这两点观察，我们提出利用个体的特征来自动学习具有不同目标的可分离隐藏单元贡献。

    In this paper, we propose a novel method for speaker adaptation in lip reading, motivated by two observations. Firstly, a speaker's own characteristics can always be portrayed well by his/her few facial images or even a single image with shallow networks, while the fine-grained dynamic features associated with speech content expressed by the talking face always need deep sequential networks to represent accurately. Therefore, we treat the shallow and deep layers differently for speaker adaptive lip reading. Secondly, we observe that a speaker's unique characteristics ( e.g. prominent oral cavity and mandible) have varied effects on lip reading performance for different words and pronunciations, necessitating adaptive enhancement or suppression of the features for robust lip reading. Based on these two observations, we propose to take advantage of the speaker's own characteristics to automatically learn separable hidden unit contributions with different targets for shallow layers and de
    
[^65]: 对比学习的难视图选择

    Hard View Selection for Contrastive Learning. (arXiv:2310.03940v1 [cs.CV])

    [http://arxiv.org/abs/2310.03940](http://arxiv.org/abs/2310.03940)

    本文提出了一种Easy、无需学习但强大的Hard View Selection策略，通过选择更难的样本，提高了对比学习模型的性能。

    

    许多对比学习方法训练模型对图像输入的不同“视图”具有不变性，而一个好的数据增强流程对此至关重要。然而，大多数方法仍然依赖于对图像增强流程中的操作进行随机抽样，如随机裁剪或颜色扭曲操作。本文认为视图生成及其对性能的影响在目前研究中尚未得到足够的关注。为了解决这个问题，我们提出了一种易于实施但强大的“难视图选择”策略，该策略通过将训练过程中的随机视图生成扩展到更难的样本，提高了模型的性能。策略包括以下迭代步骤：1）随机选择多个视图并创建两个视图的配对，2）进行向前传递...

    Many Contrastive Learning (CL) methods train their models to be invariant to different "views" of an image input for which a good data augmentation pipeline is crucial. While considerable efforts were directed towards improving pre-text tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax centering), the majority of these methods remain strongly reliant on the random sampling of operations within the image augmentation pipeline, such as the random resized crop or color distortion operation. In this paper, we argue that the role of the view generation and its effect on performance has so far received insufficient attention. To address this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS) strategy designed to extend the random view generation to expose the pretrained model to harder samples during CL training. It encompasses the following iterative steps: 1) randomly sample multiple views and create pairs of two views, 2) run forward pa
    
[^66]: EGOFALLS:一种使用自我中心摄像头进行摔倒检测的视听数据集和基准（arXiv:2309.04579v1 [cs.CV]）

    EGOFALLS: A visual-audio dataset and benchmark for fall detection using egocentric cameras. (arXiv:2309.04579v1 [cs.CV])

    [http://arxiv.org/abs/2309.04579](http://arxiv.org/abs/2309.04579)

    这项研究提出了一种使用自我中心摄像头进行摔倒检测的方法，并构建了一个新的视听数据集。通过迟决策融合将音频和视觉信息相结合可以提高检测性能。

    

    对于脆弱人群，如老年人，摔倒往往是严重且常导致死亡的。以往的研究通过依赖单个传感器（图像或加速度计）捕捉数据来解决摔倒的检测问题。在本研究中，我们依赖于从自我中心摄像头捕捉的视频中提取的多模态描述符。我们提出的方法包括一个在提取的描述符之上构建的迟决策融合层。此外，我们还收集了一个新的数据集来评估我们提出的方法。这是我们认为的第一个公共同类数据集。该数据集包含14个受试者的10,948个视频样本。我们进行了消融实验以评估单个特征提取器的性能，视觉信息融合以及视觉和音频信息的融合。此外，我们还进行了内部和外部交叉验证的实验。我们的结果表明，通过迟决策融合将音频和视觉信息相结合可以提高检测性能。

    Falls are significant and often fatal for vulnerable populations such as the elderly. Previous works have addressed the detection of falls by relying on data capture by a single sensor, images or accelerometers. In this work, we rely on multimodal descriptors extracted from videos captured by egocentric cameras. Our proposed method includes a late decision fusion layer that builds on top of the extracted descriptors. Furthermore, we collect a new dataset on which we assess our proposed approach. We believe this is the first public dataset of its kind. The dataset comprises 10,948 video samples by 14 subjects. We conducted ablation experiments to assess the performance of individual feature extractors, fusion of visual information, and fusion of both visual and audio information. Moreover, we experimented with internal and external cross-validation. Our results demonstrate that the fusion of audio and visual information through late decision fusion improves detection performance, making
    
[^67]: 自动化机器翻译的行为测试

    Automating Behavioral Testing in Machine Translation. (arXiv:2309.02553v1 [cs.CL])

    [http://arxiv.org/abs/2309.02553](http://arxiv.org/abs/2309.02553)

    本文提出了一种利用大型语言模型自动生成源句子的方法，以测试机器翻译模型在多种情况下的行为。通过对多个机器翻译系统应用该方法，发现在测试结果与传统准确率度量存在差异的情况下，仍可观察到一致的趋势。

    

    NLP中的行为测试通过分析输入-输出行为来细粒度评估系统的语言能力。然而，目前关于机器翻译中行为测试的研究仅限于手工设计的测试范围有限、涵盖的语言种类也有限。为了解决这一限制，我们提出利用大型语言模型生成多样化的源句子，以测试机器翻译模型在不同情况下的行为。然后，我们可以使用相同的语言模型生成备选集，以验证机器翻译模型是否表现出预期的行为。我们的方法旨在使机器翻译系统的行为测试实际可行，同时只需要最少的人力投入。在实验中，我们将提出的评估框架应用于多个可用的机器翻译系统，结果显示，尽管总体上通过率与传统准确率度量可观察到的趋势相符，但仍存在差异。

    Behavioral testing in NLP allows fine-grained evaluation of systems by examining their linguistic capabilities through the analysis of input-output behavior. Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages. To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations. We can then verify whether the MT model exhibits the expected behavior through matching candidate sets that are also generated using LLMs. Our approach aims to make behavioral testing of MT systems practical while requiring only minimal human effort. In our experiments, we apply our proposed evaluation framework to assess multiple available MT systems, revealing that while in general pass-rates follow the trends observable from traditional accuracy-based metri
    
[^68]: BigFUSE: 在具有图像形成先验的双视图光片荧光显微镜中进行全局上下文感知图像融合

    BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior. (arXiv:2309.01865v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2309.01865](http://arxiv.org/abs/2309.01865)

    BigFUSE是一种全局上下文感知的图像融合方法，通过考虑光子传播的全局影响和局部图像质量，稳定了双视图光片荧光显微镜中的图像融合。

    

    光片荧光显微镜（LSFM）是一种平面照明技术，可以对样本进行高分辨率成像，但在光子通过厚组织时会出现光散射引起的图像模糊。为了解决这个问题，双视图成像是有帮助的。通过从相对方向观察样本，可以理想地扫描样本的各个部分。然而，最近的图像融合方法在比较两个视图的图像质量来确定焦点时，会产生空间不一致的焦点度量，因为它们的视野有限。在这里，我们提出了BigFUSE，一种全局上下文感知的图像融合器，通过考虑组织中光子传播的全局影响，并根据局部图像质量来确定焦点和模糊，从而稳定了LSFM中的图像融合。受到双视图LSFM中图像形成先验的启发，图像融合被视为使用贝叶斯定理估计焦点和模糊边界。

    Light-sheet fluorescence microscopy (LSFM), a planar illumination technique that enables high-resolution imaging of samples, experiences defocused image quality caused by light scattering when photons propagate through thick tissues. To circumvent this issue, dualview imaging is helpful. It allows various sections of the specimen to be scanned ideally by viewing the sample from opposing orientations. Recent image fusion approaches can then be applied to determine in-focus pixels by comparing image qualities of two views locally and thus yield spatially inconsistent focus measures due to their limited field-of-view. Here, we propose BigFUSE, a global context-aware image fuser that stabilizes image fusion in LSFM by considering the global impact of photon propagation in the specimen while determining focus-defocus based on local image qualities. Inspired by the image formation prior in dual-view LSFM, image fusion is considered as estimating a focus-defocus boundary using Bayes Theorem, 
    
[^69]: 从SMOTE到Mixup用于深度不平衡分类

    From SMOTE to Mixup for Deep Imbalanced Classification. (arXiv:2308.15457v1 [cs.LG])

    [http://arxiv.org/abs/2308.15457](http://arxiv.org/abs/2308.15457)

    本研究提出了一种从SMOTE到Mixup的方法，用于深度不平衡分类。通过对SMOTE进行改进，并结合Mixup技术，我们构建了一个统一的数据增强框架。研究表明，Mixup技术通过实现多数类和少数类之间的不平衡间隙来改善泛化能力。我们还提出了一种新颖的基于边界的Mixup技术，更明确地实现了不平衡间隙。实验结果表明我们的方法在多个数据集上取得了最好的性能。

    

    鉴于不平衡的数据，使用深度学习训练好的分类器因为少数类的泛化能力差而困难重重。传统上，用于数据增强的知名少数类合成过采样技术（SMOTE），作为一种面向不平衡学习的数据挖掘方法，被用来改善这种泛化。然而，SMOTE在深度学习中是否也有益处仍不清楚。在这项工作中，我们研究了为什么原始的SMOTE对深度学习来说是不足的，并使用软标签增强了SMOTE。将得到的软SMOTE与Mixup，一种现代数据增强技术，连接在一起，形成了一个统一的框架，将传统和现代的数据增强技术纳入同一个范畴。在这个框架中进行系统研究表明，Mixup通过隐式地实现多数类和少数类之间的不平衡间隙来改善泛化能力。然后，我们提出了一种新颖的基于边界的Mixup技术，更明确地实现了不平衡间隙。大量的实验表明，我们的方法在多个数据集上相对于最先进的算法都取得了最好的性能。

    Given imbalanced data, it is hard to train a good classifier using deep learning because of the poor generalization of minority classes. Traditionally, the well-known synthetic minority oversampling technique (SMOTE) for data augmentation, a data mining approach for imbalanced learning, has been used to improve this generalization. However, it is unclear whether SMOTE also benefits deep learning. In this work, we study why the original SMOTE is insufficient for deep learning, and enhance SMOTE using soft labels. Connecting the resulting soft SMOTE with Mixup, a modern data augmentation technique, leads to a unified framework that puts traditional and modern data augmentation techniques under the same umbrella. A careful study within this framework shows that Mixup improves generalization by implicitly achieving uneven margins between majority and minority classes. We then propose a novel margin-aware Mixup technique that more explicitly achieves uneven margins. Extensive experimental r
    
[^70]: ChatGPT用于GTFS: 从文字到信息

    ChatGPT for GTFS: From Words to Information. (arXiv:2308.02618v1 [cs.IR])

    [http://arxiv.org/abs/2308.02618](http://arxiv.org/abs/2308.02618)

    本研究探索了使用ChatGPT语言模型从GTFS数据中检索信息的可行性，验证了ChatGPT（GPT-3.5）在GTFS规范理解和信息提取方面的能力。程序合成方法在信息检索任务中表现出更高的准确率，为解决GTFS数据信息获取问题提供了一种有效的方法。

    

    广泛使用的公交通行数据发布标准General Transit Feed Specification（GTFS）是表格数据，信息分散在不同的文件中，需要专门的工具或包来检索信息。与此同时，使用大型语言模型进行文本和信息检索的趋势也在增长。本研究的想法是看看当前广泛采用的LLMs（ChatGPT）是否能够使用自然语言指令从GTFS中检索信息。我们首先测试ChatGPT（GPT-3.5）是否理解GTFS规范。GPT-3.5在我们的多项选择问题（MCQ）中正确回答了77%。接下来，我们利用过滤的GTFS数据集对LLM进行信息提取任务。对于信息检索，我们比较了零-shot和程序合成。程序合成的效果更好，在简单问题上达到了约90%的准确率，在复杂问题上达到了约40%的准确率。

    The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.
    
[^71]: 多重保护属性的公平性改善的实证研究

    An Empirical Study on Fairness Improvement with Multiple Protected Attributes. (arXiv:2308.01923v1 [cs.LG])

    [http://arxiv.org/abs/2308.01923](http://arxiv.org/abs/2308.01923)

    本文通过广泛研究，发现对于单个保护属性的公平性改善会大大降低对未考虑保护属性的公平性，但在多属性模式下可以保持准确性。

    

    现有研究主要关注单个保护属性的机器学习（ML）软件的公平性改善，但考虑到许多用户具有多个保护属性，这是不现实的。本文对多个保护属性的公平性改善进行了广泛研究，涵盖了11种最先进的公平性改善方法。我们分析了在考虑多个保护属性时，这些方法在不同数据集、评估指标和ML模型上的有效性。结果显示，改善单个保护属性的公平性大大降低了未考虑的保护属性的公平性。在88.3％的情况下观察到这种降低（平均为57.5％）。更令人惊讶的是，在考虑单个和多个保护属性时，准确率损失方面几乎没有差异，这表明在多属性模式下可以保持准确性。然而，在处理多个保护属性时，精确度和召回率的影响较大。

    Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on precision and recall when handling
    
[^72]: 通用目的人工智能系统（GPAIS）：性质、定义、分类、开放挑战和影响

    General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications. (arXiv:2307.14283v1 [cs.AI])

    [http://arxiv.org/abs/2307.14283](http://arxiv.org/abs/2307.14283)

    这里是中文总结出的一句话要点：本论文讨论了通用目的人工智能系统（GPAIS）的性质、定义、分类和开放挑战，并提出了一种新的定义，允许根据其性质和限制逐步区分GPAIS的类型。

    

    大部分人工智能（AI）应用都设计用于特定和有限的任务。然而，有许多场景需要更通用的AI，能够解决各种任务而不需要专门为它们设计。通用目的人工智能系统（GPAIS）这个术语被定义为指代这些AI系统。尽管迄今为止，实现人工通用智能的可能性，即足够强大以模拟人类并改进各种智力任务，一直是一个愿望、虚构的概念，并被认为对我们社会构成风险。虽然我们离实现这一目标可能还很遥远，但GPAIS是现实存在并位居人工智能研究的前沿。本文讨论了现有GPAIS定义，并提出了一种新的定义，允许根据其性质和限制逐步区分GPAIS的类型。我们区分了封闭世界和开放世界的GPAIS，描述其自主程度和...

    Most applications of Artificial Intelligence (AI) are designed for a confined and specific task. However, there are many scenarios that call for a more general AI, capable of solving a wide array of tasks without being specifically designed for them. The term General-Purpose Artificial Intelligence Systems (GPAIS) has been defined to refer to these AI systems. To date, the possibility of an Artificial General Intelligence, powerful enough to perform any intellectual task as if it were human, or even improve it, has remained an aspiration, fiction, and considered a risk for our society. Whilst we might still be far from achieving that, GPAIS is a reality and sitting at the forefront of AI research.  This work discusses existing definitions for GPAIS and proposes a new definition that allows for a gradual differentiation among types of GPAIS according to their properties and limitations. We distinguish between closed-world and open-world GPAIS, characterising their degree of autonomy and
    
[^73]: 景观替代品：在部分信息下学习数学优化的决策损失

    Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information. (arXiv:2307.08964v1 [cs.LG])

    [http://arxiv.org/abs/2307.08964](http://arxiv.org/abs/2307.08964)

    本论文提出了一种使用景观替代品的学习方法，旨在解决部分信息下数学优化问题中的挑战。这种方法可以通过学习优化器来加速优化过程，并且能够处理问题的不确定性。

    

    最近的学习集成优化工作在优化问题只有部分可观测或通用优化器在无专家调优的情况下表现不佳的情况下显示出了希望。通过学习一个优化器$ \mathbf{g} $来解决这些具有挑战性的问题，通过利用过去的经验，可以显著加速优化过程。优化器可以通过已知最优解的监督或通过优化复合函数$ f\circ \mathbf{g} $的隐式方式进行训练。隐式方法可能不需要最优解作为标签，并且能够处理问题的不确定性；然而，由于在训练和测试过程中频繁调用优化器$ \mathbf{g} $，因此训练和部署缓慢。对于组合求解器，由于$ \mathbf{g} $的稀疏梯度，训练进一步受到挑战。为了解决这些问题，我们提出使用平滑可学习的景观替代品$ M $作为一种替代方法。

    Recent works in learning-integrated optimization have shown promise in settings where the optimization problem is only partially observed or where general-purpose optimizers perform poorly without expert tuning. By learning an optimizer $\mathbf{g}$ to tackle these challenging problems with $f$ as the objective, the optimization process can be substantially accelerated by leveraging past experience. The optimizer can be trained with supervision from known optimal solutions or implicitly by optimizing the compound function $f\circ \mathbf{g}$. The implicit approach may not require optimal solutions as labels and is capable of handling problem uncertainty; however, it is slow to train and deploy due to frequent calls to optimizer $\mathbf{g}$ during both training and testing. The training is further challenged by sparse gradients of $\mathbf{g}$, especially for combinatorial solvers. To address these challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as a replace
    
[^74]: 一种风险厌恶策略梯度的方差替代：基尼离差

    An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient. (arXiv:2307.08873v1 [cs.LG])

    [http://arxiv.org/abs/2307.08873](http://arxiv.org/abs/2307.08873)

    本研究提出了一种风险厌恶策略梯度的替代方法，通过使用基尼离差来替代方差，缓解了方差方法的局限性，并在实证评估中取得了高回报和低风险的成果。

    

    在风险厌恶的强化学习中，限制策略回报的方差是一种常见选择，因为它具有明确的数学定义和易于解释。传统方法直接限制总回报方差，而最近的方法通过限制每步奖励方差作为代理。本文彻底研究了这些基于方差的方法的局限性，如数字尺度的敏感性和阻碍策略学习，并提出使用替代风险衡量标准——基尼离差。我们研究了这种新风险衡量标准的各种属性，并导出了一种用于最小化基尼离差的策略梯度算法。在风险厌恶可以明确定义的领域进行实证评估时，我们的算法可以缓解基于方差的风险衡量标准的局限性，并在其他策略无法学到合理策略时实现高回报和低风险，以方差和基尼离差度量。

    Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.
    
[^75]: LLQL: 逻辑似然 Q-Learning 用于增强学习

    LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning. (arXiv:2307.02345v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02345](http://arxiv.org/abs/2307.02345)

    本研究通过研究在线和离线增强学习中 Bellman 近似误差的分布发现，Bellman 误差符合逻辑分布。基于这一发现，本研究提出了一种使用 Logistic 最大似然函数作为替代方法的方案，并通过实验证明了其有效性。

    

    现代增强学习（RL）可以分为在线和离线两种变体。作为在线和离线 RL 的关键方面，当前对 Bellman 方程的研究主要集中在优化技术和性能增强上，而不是探索 Bellman 误差的固有结构特性，如其分布特征。本研究通过对 Bellman 方程进行迭代探索，研究了在线 RL 和离线 RL 中 Bellman 近似误差的分布情况。我们观察到无论是在线 RL 还是离线 RL，Bellman 误差都符合逻辑分布。基于这一发现，本研究采用 Logistic 最大似然函数（LLoss）作为常用的 MSE Loss 的替代方法，假设 Bellman 误差服从正态分布。通过广泛的数值实验验证了我们的假设，在不同的在线和离线环境中得到了验证。

    Modern reinforcement learning (RL) can be categorized into online and offline variants. As a pivotal aspect of both online and offline RL, current research on the Bellman equation revolves primarily around optimization techniques and performance enhancement rather than exploring the inherent structural properties of the Bellman error, such as its distribution characteristics. This study investigates the distribution of the Bellman approximation error in both online and offline settings through iterative exploration of the Bellman equation. We observed that both in online RL and offline RL, the Bellman error conforms to a Logistic distribution. Building upon this discovery, this study employed the Logistics maximum likelihood function (LLoss) as an alternative to the commonly used MSE Loss, assuming that Bellman errors adhere to a normal distribution. We validated our hypotheses through extensive numerical experiments across diverse online and offline environments. In particular, we app
    
[^76]: 自动驾驶轨迹预测中真正重要的是什么？

    What Truly Matters in Trajectory Prediction for Autonomous Driving?. (arXiv:2306.15136v1 [cs.RO])

    [http://arxiv.org/abs/2306.15136](http://arxiv.org/abs/2306.15136)

    在自动驾驶系统中，轨迹预测的准确性在固定数据集上表现很好，但在实际驾驶场景中却存在显著差异。现有的评估方法忽视了动力学差距和计算效率对预测结果的影响。

    

    在自动驾驶系统中，轨迹预测在确保安全和促进平稳导航方面起着至关重要的作用。然而，我们观察到在固定数据集上的预测器准确性与在下游任务中的驾驶性能之间存在显著差异。这种差异源于当前轨迹预测评估协议中忽视了两个因素：1）数据集与实际驾驶场景之间的动力学差距；2）预测器的计算效率。在实际场景中，预测算法影响自动驾驶车辆的行为，进而改变道路上其他参与者的行为。这种互动产生了针对预测器的特定动力学，直接影响预测结果。由于其他参与者的反应在数据集上是预先确定的，因此在固定数据集和实际驾驶场景中进行的评估之间存在显著的动力学差距。此外，仅关注准确性无法满足对预测器动态行为和计算效率的需求。

    In the autonomous driving system, trajectory prediction plays a vital role in ensuring safety and facilitating smooth navigation. However, we observe a substantial discrepancy between the accuracy of predictors on fixed datasets and their driving performance when used in downstream tasks. This discrepancy arises from two overlooked factors in the current evaluation protocols of trajectory prediction: 1) the dynamics gap between the dataset and real driving scenario; and 2) the computational efficiency of predictors. In real-world scenarios, prediction algorithms influence the behavior of autonomous vehicles, which, in turn, alter the behaviors of other agents on the road. This interaction results in predictor-specific dynamics that directly impact prediction results. As other agents' responses are predetermined on datasets, a significant dynamics gap arises between evaluations conducted on fixed datasets and actual driving scenarios. Furthermore, focusing solely on accuracy fails to ad
    
[^77]: 使用监视器引导全局上下文指导代码语言模型

    Guiding Language Models of Code with Global Context using Monitors. (arXiv:2306.10763v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.10763](http://arxiv.org/abs/2306.10763)

    本文提出了一种使用监视器引导全局上下文的方法来指导代码语言模型，在处理类型、功能或API等全局上下文时，能够提高代码语言模型的性能和准确性。

    

    代码语言模型（LMs）在周围代码提供足够上下文时效果很好。但当需要在存储库或链接库中使用在训练过程中未见过的类型、功能或API时，这种情况就不再成立。LMs在对这种全局上下文的意识有限时会出现错误预测的情况。集成开发环境（IDEs）通过静态分析帮助开发人员了解存储库上下文。我们将开发人员享受到的这种帮助扩展到了LMs。我们提出了监视器引导解码（MGD）的方法，其中监视器使用静态分析来引导解码过程。我们构建了一个用于Java方法补全的存储库级数据集PragmaticCode，并在其上评估了MGD。在不同参数规模的模型上，通过监视类型一致的对象解引用，MGD能够持续提高编译率并与真实结果达成一致。此外，具有更少参数的LMs，在与MGD相结合时能够超越更大的LMs的性能。

    Language models of code (LMs) work well when the surrounding code provides sufficient context. This is not true when it becomes necessary to use types, functionality or APIs defined elsewhere in the repository or a linked library, especially those not seen during training. LMs suffer from limited awareness of such global context and end up hallucinating.  Integrated development environments (IDEs) assist developers in understanding repository context using static analysis. We extend this assistance, enjoyed by developers, to LMs. We propose monitor-guided decoding (MGD) where a monitor uses static analysis to guide the decoding. We construct a repository-level dataset PragmaticCode for method-completion in Java and evaluate MGD on it. On models of varying parameter scale, by monitoring for type-consistent object dereferences, MGD consistently improves compilation rates and agreement with ground truth. Further, LMs with fewer parameters, when augmented with MGD, can outperform larger LM
    
[^78]: 采用优势诱导策略对齐的Fine-Tuning语言模型

    Fine-Tuning Language Models with Advantage-Induced Policy Alignment. (arXiv:2306.02231v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02231](http://arxiv.org/abs/2306.02231)

    本论文提出了一种新算法APA，其采用优势诱导策略对齐用于强化学习语言模型。相对于传统方法（PPO），APA在语言任务中表现更好，避免了模型的崩溃与不稳定性。

    

    人类反馈强化学习（RLHF）已经成为将大型语言模型（LLMs）与人类偏好对齐的可靠方法。在众多RLHF技术中，接近策略优化（PPO）是最常用的方法之一。然而，尽管PPO很流行，但它可能会遭受模式崩溃、不稳定和效率低下的问题。我们展示了一种新颖的算法--基于估计优势的平方误差损失函数的优势诱导策略对齐（APA），可以减轻这些问题。我们通过实验证明，当使用单独的奖励模型作为评估器时，APA在语言任务中始终比PPO表现出更好的性能。此外，与PPO相比，APA可以更稳定地控制模型与初始策略的偏差，确保模型提高性能而不会崩溃为确定性输出。除了经验结果之外，我们还提供了APA的理论分析。

    Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment (APA), which leverages a squared error loss function based on the estimated advantages. We demonstrate empirically that APA consistently outperforms PPO in language tasks by a large margin, when a separate reward model is employed as the evaluator. In addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. In addition to empirical results, we also prov
    
[^79]: 双重稳健自我训练

    Doubly Robust Self-Training. (arXiv:2306.00265v1 [cs.LG])

    [http://arxiv.org/abs/2306.00265](http://arxiv.org/abs/2306.00265)

    本文提出了一种双重稳健自我训练算法，可以在伪标签不准确和完全准确时分别采取不同的训练策略，实现有效的半监督学习。实验结果表明，该算法在ImageNet和nuScenes数据集上均比标准自我训练总结更好。

    

    自我训练是解决半监督学习问题的一种重要技术。它通过生成伪标签并将其与有限的标记数据集结合使用进行训练，从而利用无标签数据。自我训练的有效性在很大程度上依赖于这些伪标签的准确性。本文引入了双重稳健自我训练，这是一种新颖的半监督算法，可以保证在两个极端之间平衡。当伪标签完全不正确时，我们的方法将被减少到仅使用标记数据进行训练。相反，当伪标签完全准确时，我们的方法将变成利用所有伪标签数据和标记数据进行训练的过程，从而增加有效的样本量。通过在ImageNet图像分类和nuScenes自主驾驶数据集上的实证评估，我们证明了双重稳健损失优于标准自我训练基线的优越性。

    Self-training is an important technique for solving semi-supervised learning problems. It leverages unlabeled data by generating pseudo-labels and combining them with a limited labeled dataset for training. The effectiveness of self-training heavily relies on the accuracy of these pseudo-labels. In this paper, we introduce doubly robust self-training, a novel semi-supervised algorithm that provably balances between two extremes. When the pseudo-labels are entirely incorrect, our method reduces to a training process solely using labeled data. Conversely, when the pseudo-labels are completely accurate, our method transforms into a training process utilizing all pseudo-labeled data and labeled data, thus increasing the effective sample size. Through empirical evaluations on both the ImageNet dataset for image classification and the nuScenes autonomous driving dataset for 3D object detection, we demonstrate the superiority of the doubly robust loss over the standard self-training baseline.
    
[^80]: 基于大语言模型的特定领域语言生成中的语法提示

    Grammar Prompting for Domain-Specific Language Generation with Large Language Models. (arXiv:2305.19234v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.19234](http://arxiv.org/abs/2305.19234)

    本文提出了一种基于语法提示的方法，使用专用的语法来增强示例，为大型语言模型（LLM）在特定领域的语言生成任务中使用外部知识和特定约束条件进行上下文学习。

    

    大型语言模型（LLM）可以从仅有几个上下文示例中学习执行各种自然语言任务。然而，对于从高度结构化的语言（例如，从语义解析到复杂的特定领域语言）生成字符串，LLM只从少量示例中进行泛化是具有挑战性的。我们探讨了$\textbf{语法提示}$作为一种简单的方法，通过在背科斯-诺尔范式（BNF）中表达的语法来启用LLM使用外部知识和特定领域的约束条件来进行上下文学习。语法提示使用一个专门的语法来增强每个演示示例，该语法足以生成特定的输出示例，其中该专门的语法是全DSL语法的子集。对于推理，LLM首先预测一个给定测试输入的BNF语法，然后根据语法规则生成输出。实验表明，语法提示可以使LLM在特定领域的语言生成任务中表现出色。

    Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We explore $\textbf{grammar prompting}$ as a simple approach for enabling LLMs to use external knowledge and domain-specific constraints, expressed through a grammar expressed in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perfor
    
[^81]: 从大型矿石中提炼黄金: 基于关键样本选择的高效数据集蒸馏

    Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v1 [cs.LG])

    [http://arxiv.org/abs/2305.18381](http://arxiv.org/abs/2305.18381)

    研究提出了一种基于选择最有价值的样本的方法，以扩展现有的蒸馏算法，从而更好地利用训练样本，显著降低训练成本，拓展对更大更多元化数据集的数据集蒸馏，并持续提高性能。

    

    数据效率学习近年来备受关注，特别是对于拥有大量多模型的现在，数据集蒸馏可以成为有效的解决方案。然而，数据集蒸馏过程本身仍然非常低效。在本文中，我们首先引用信息理论来建模蒸馏问题，观察到数据集蒸馏中存在严重的数据冗余，我们提出了一种方法来扩展现有的蒸馏算法，以便通过选择最有价值的样本来更好地利用这些训练样本。我们进一步对样本选择进行全面分析，并验证了其优化过程。这种新策略能够显著减少训练成本，扩大现有算法范围以对更庞大和多元化的数据集进行数据集蒸馏，例如，在某些情况下，只需要0.04％的训练数据就足以保持可比的蒸馏效果。此外，我们的策略能够持续提高性能，其贡献可能为蒸馏过程的动力学开辟新的分析方法。

    Data-efficient learning has drawn significant attention, especially given the current trend of large multi-modal models, where dataset distillation can be an effective solution. However, the dataset distillation process itself is still very inefficient. In this work, we model the distillation problem with reference to information theory. Observing that severe data redundancy exists in dataset distillation, we argue to put more emphasis on the utility of the training samples. We propose a family of methods to exploit the most valuable samples, which is validated by our comprehensive analysis of the optimal data selection. The new strategy significantly reduces the training cost and extends a variety of existing distillation algorithms to larger and more diversified datasets, e.g. in some cases only 0.04% training data is sufficient for comparable distillation performance. Moreover, our strategy consistently enhances the performance, which may open up new analyses on the dynamics of dist
    
[^82]: 一种统一的方法用于最大化连续 DR-submodular 函数

    A Unified Approach for Maximizing Continuous DR-submodular Functions. (arXiv:2305.16671v1 [cs.LG])

    [http://arxiv.org/abs/2305.16671](http://arxiv.org/abs/2305.16671)

    本文提出了一种适用于一系列设置和 Oracle 访问类型的统一方法，用于最大化连续 DR-submodular 函数，为 16 种情况中的 9 种提供了新的/改进的结果，并且针对基于随机函数值的 Oracle 取得了第一个适用于随机 DR-submodular 函数的后悔界限。

    

    本文提出了一种统一的方法，用于最大化连续的 DR-submodular 函数，涵盖了一系列设置和 Oracle 访问类型。我们的方法包括针对单调和非单调函数的 Frank-Wolfe 类型离线算法，具有不同的一般凸集限制。我们考虑了 Oracle 提供函数梯度或仅函数值的访问以及确定性或随机性访问的设置。我们在所有情况下确定了所需的 Oracle 访问数量。我们的方法为 16 个考虑的情况中的 9 个提供了新的/改进的结果，在两个情况下避免了计算上昂贵的投影，而所提出的框架在其余五个情况下与最先进的方法相匹配。值得注意的是，我们针对基于随机函数值的 Oracle 的方法，为随机 DR-submodular 函数提供了第一个带有探险反馈的后悔界限。

    This paper presents a unified approach for maximizing continuous DR-submodular functions that encompasses a range of settings and oracle access types. Our approach includes a Frank-Wolfe type offline algorithm for both monotone and non-monotone functions, with different restrictions on the general convex set. We consider settings where the oracle provides access to either the gradient of the function or only the function value, and where the oracle access is either deterministic or stochastic. We determine the number of required oracle accesses in all cases. Our approach gives new/improved results for nine out of the sixteen considered cases, avoids computationally expensive projections in two cases, with the proposed framework matching performance of state-of-the-art approaches in the remaining five cases. Notably, our approach for the stochastic function value-based oracle enables the first regret bounds with bandit feedback for stochastic DR-submodular functions.
    
[^83]: 扩散模型是否是视觉语言推理器？

    Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v1 [cs.CV])

    [http://arxiv.org/abs/2305.16397](http://arxiv.org/abs/2305.16397)

    本文针对扩散-语言图像生成模型进行转换和评估，介绍了生成-鉴别评估基准(GDBench)基于7个视觉语言复杂任务，并发现转换后的模型在组合性任务方面的表现优于CLIP，通过微调可提高其组合性能。

    

    近期，使用去噪扩散过程的文本-图像生成模型已取得了巨大的定性成功。然而，与鉴别式视觉-语言模型不同，将基于扩散的生成模型置于自动细粒度定量评估高级现象（如组合性）的任务中是一项非常棘手的任务。为此，我们开展了两项创新。首先，我们使用一种称为DiffusionITM的新方法将基于扩散的模型（在我们的情况下，是稳定扩散）转换为任何图像文本匹配(ITM)任务。其次，我们引入了7个复杂的视觉语言任务、偏差评估和详细分析的生成-鉴别评估基准(GDBench)。我们发现，Stable Diffusion + DiffusionITM在许多任务上具有竞争力，并在组合性任务（如CLEVR和Winoground等）上优于CLIP。我们通过在MS-COCO上微调保持图像特征的转移设置进一步提高其组合性能。

    Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality. Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM. Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis. We find that Stable Diffusion + DiffusionITM is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground. We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining ge
    
[^84]: 通过交互式问题-知识对齐解决语言模型幻觉问题

    Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment. (arXiv:2305.13669v1 [cs.CL])

    [http://arxiv.org/abs/2305.13669](http://arxiv.org/abs/2305.13669)

    本文提出了MixAlign框架，通过与用户和知识库交互，实现自动的问题-知识对齐，从而解决了语言模型因无法正确理解问题和知识而导致的幻觉问题。

    

    尽管语言模型近期进展显著，但仍面临幻觉问题，可能会生成误导性和不支持的回答。一种缓解幻觉问题的常见方法是从知识库中检索和整合支持证据。然而，用户的问题通常与存储的知识不太对齐，因为他们在提问前不知道可用的信息。这种不对齐可能限制语言模型定位和利用知识的能力，可能迫使其通过忽略或覆盖检索到的证据而产生幻觉。为了解决这个问题，我们介绍了 MixAlign，一个框架，它与用户和知识库交互以获得并整合关于用户问题与存储信息相关性的澄清信息。 MixAlign 采用语言模型实现自动问题-知识对齐，并在需要时通过人工用户澄清进一步增强这种对齐。

    Despite the remarkable recent advances in language models, they still struggle with the hallucination problem and can generate misleading and unsupported responses. A common approach to mitigate the hallucination issue is retrieving and incorporating supporting evidence from a knowledge base. However, user questions usually do not align well with the stored knowledge, as they are unaware of the information available before asking questions. This misalignment can limit the language model's ability to locate and utilize the knowledge, potentially forcing it to hallucinate by ignoring or overriding the retrieved evidence. To address this issue, we introduce MixAlign, a framework that interacts with both the user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. MixAlign employs a language model to achieve automatic question-knowledge alignment and, if necessary, further enhances this alignment through human user clari
    
[^85]: Flover：一种用于高效自回归模型并行推断的时间融合框架

    Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v1 [cs.DC])

    [http://arxiv.org/abs/2305.13484](http://arxiv.org/abs/2305.13484)

    Flover是一种用于自回归模型并行推断的时间融合框架，解决了并行性不足和灵活性差的问题，可以实现更加高效的推断性能。

    

    在深度学习领域快速发展的背景下，模型推断性能成为一个关键因素，尤其是在模型变得更加复杂并被部署在多个应用场景中的情况下。自回归模型由于在众多生成任务中表现优异，因此备受关注。这些模型设计上采用了一种时间依赖结构，其中当前token的概率分布受到前面token的影响。然而，这种本质上的序列特性遵循马尔可夫链假设，缺乏时间并行性，因此存在独特的挑战。特别是在工业背景下，推断请求遵循泊松时间分布，需要不同的响应长度，这种并行性的缺失更加明显。现有的解决方案如动态批处理和并发模型实例，然而，这些粗粒度的方法存在严重的开销和缺乏灵活性，无法实现最优化。

    In the rapidly evolving field of deep learning, the performance of model inference has become a pivotal aspect as models become more complex and are deployed in diverse applications. Among these, autoregressive models stand out due to their state-of-the-art performance in numerous generative tasks. These models, by design, harness a temporal dependency structure, where the current token's probability distribution is conditioned on preceding tokens. This inherently sequential characteristic, however, adheres to the Markov Chain assumption and lacks temporal parallelism, which poses unique challenges. Particularly in industrial contexts where inference requests, following a Poisson time distribution, necessitate diverse response lengths, this absence of parallelism is more profound. Existing solutions, such as dynamic batching and concurrent model instances, nevertheless, come with severe overheads and a lack of flexibility, these coarse-grained methods fall short of achieving optimal la
    
[^86]: 在多中心临床研究中处理缺失数据的因果发现

    Causal Discovery with Missing Data in a Multicentric Clinical Study. (arXiv:2305.10050v1 [stat.ME])

    [http://arxiv.org/abs/2305.10050](http://arxiv.org/abs/2305.10050)

    本文扩展了最新的因果发现算法，利用专家知识从多中心临床研究的缺失数据中分析了不同缺失机制对恢复的因果图的影响，验证了所恢复因果图的临床相关性，并用图形分离来验证因果通路，讨论了因果图的拟合度和从临床决策角度的一致性。

    

    由于观测数据的数据生成模型和相关联的因果图通常不存在，因此从观测数据中检验临床假设的因果推断面临许多困难。此外，观测数据可能包含缺失的值，这会影响因果发现算法恢复因果图的效果，这是临床研究中经常被忽略的关键问题。针对这些问题，我们使用子宫内膜癌的多中心研究数据，分析不同缺失机制对恢复的因果图的影响。我们扩展了最先进的因果发现算法，利用专家知识而不损失理论的严谨性，验证了所恢复因果图的临床相关性。最后，我们使用图形分离来验证因果通路，讨论了因果图的拟合度和从临床决策角度的一致性。

    Causal inference for testing clinical hypotheses from observational data presents many difficulties because the underlying data-generating model and the associated causal graph are not usually available. Furthermore, observational data may contain missing values, which impact the recovery of the causal graph by causal discovery algorithms: a crucial issue often ignored in clinical studies. In this work, we use data from a multi-centric study on endometrial cancer to analyze the impact of different missingness mechanisms on the recovered causal graph. This is achieved by extending state-of-the-art causal discovery algorithms to exploit expert knowledge without sacrificing theoretical soundness. We validate the recovered graph with expert physicians, showing that our approach finds clinically-relevant solutions. Finally, we discuss the goodness of fit of our graph and its consistency from a clinical decision-making perspective using graphical separation to validate causal pathways.
    
[^87]: 使用梯度下降学习决策树

    Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])

    [http://arxiv.org/abs/2305.03515](http://arxiv.org/abs/2305.03515)

    本文提出了一种使用梯度下降学习决策树的新方法，可以联合优化所有树的参数，从而避免了贪心算法造成次优解的问题。该方法在二分类任务上表现优异，并在多类任务中达到有竞争力的结果。

    

    决策树是用于许多机器学习任务的常见工具，因为它们具有高度的解释性。然而，从数据中学习决策树是一个困难的优化问题，因为它是非凸和非可微的。因此，通常的方法是使用一种贪婪生长算法来学习决策树，在每个内部节点上局部最小化不纯度。不幸的是，这种贪心过程可能会导致次优的决策树。在本文中，我们提出了一种使用梯度下降学习难以处理的轴对齐决策树的新方法。所提出的方法使用反向传播和直通算子在密集的决策树表示上联合优化所有树的参数。我们的方法在二分类基准测试上优于现有方法，并在多类任务中实现了有竞争力的结果。

    Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
    
[^88]: DIN-SQL: 自纠正的文本到SQL分解式上下文学习

    DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction. (arXiv:2304.11015v1 [cs.CL])

    [http://arxiv.org/abs/2304.11015](http://arxiv.org/abs/2304.11015)

    DIN-SQL通过将复杂的文本到SQL任务分解为子问题，并将这些子问题的解决方案馈入到大型语言模型中，显著提高了它们的表现，使准确性超过了当前最先进的技术。

    

    本文研究了将复杂的文本到SQL任务分解为较小的子任务，并且这种分解如何显著提高大型语言模型在推理过程中的表现。我们展示了尽管SQL查询具有声明式结构，但可以将其分解为子问题，并将这些子问题的解决方案馈入到大型语言模型中，从而显著提高它们的表现。我们的实验表明，这种方法能够稳定提高三种大型语言模型的表现，大约提高了10％，将大型语言模型的准确性推向最新水平，并在Holdout Spider数据集上甚至超过了经过精调的大型模型。

    We study the problem of decomposing a complex text-to-sql task into smaller sub-tasks and how such a decomposition can significantly improve the performance of Large Language Models (LLMs) in the reasoning process. There is currently a significant gap between the performance of fine-tuned models and prompting approaches using LLMs on challenging text-to-sql datasets such as Spider. We show that SQL queries, despite their declarative structure, can be broken down into sub-problems and the solutions of those sub-problems can be fed into LLMs to significantly improve their performance. Our experiments with three LLMs show that this approach consistently improves their performance by roughly 10%, pushing the accuracy of LLMs towards state-of-the-art, and even beating large fine-tuned models on the holdout Spider dataset.
    
[^89]: OpenAGI：当LLM遇到领域专家

    OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])

    [http://arxiv.org/abs/2304.04370](http://arxiv.org/abs/2304.04370)

    基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。

    

    人类具有将基本技能组合成复杂技能以解决复杂任务的显著能力。这种能力对于人工智能同样重要，因此，我们断言，除了开发大型综合智能模型外，将不同领域专家模型应用于复杂任务解决能力同样关键，以在人工智能通用智能的追求中使其具备这种能力。最近的大型语言模型（LLM）的发展证明其具有出色的学习和推理能力，使它们成为选择、综合和执行外部模型以解决复杂任务的控制器的有前途的选择。在这个项目中，我们开发了一个名为OpenAGI的开源AGI研究平台，专门设计为提供复杂的多步骤任务，并配有任务特定的数据集、评估指标和各种可扩展模型。OpenAGI将复杂任务阐释为自然语言问答，旨在促进领域专家和语言模型之间的协同作用。

    Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
    
[^90]: 为什么要逐步思考？推理源于经验的局部性。

    Why think step-by-step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v1 [cs.AI])

    [http://arxiv.org/abs/2304.03843](http://arxiv.org/abs/2304.03843)

    本文通过语言模型研究何时以及为什么推理是有帮助的，测试推理在训练数据由相互影响强烈的局部变量集群组成时是否有效。通过一步步的推理，能够将准确的局部推理链接在一起，以估算在训练中没有同时观察到的变量之间的关系。

    

    人类有着强大而神秘的推理能力。通过一系列纯粹的思维步骤，我们可以推理出我们无法直接得出的推论 - 尽管我们从世界上没有得到任何额外数据。同样地，大型语言模型可以通过一步步的推理，在回答问题之前生成中间步骤，从而更好地完成复杂的任务。我们使用语言模型研究何时以及为什么推理是有帮助的，测试推理在训练数据由相互影响强烈的局部变量集群组成时是否有效。这些训练条件能够将准确的局部推理链接在一起，以估算在训练中没有同时观察到的变量之间的关系。我们使用贝叶斯网络定义的联合分布的样品对自回归变压器进行训练，但每个样品只包括其中的一部分变量。我们比较使用推理生成的变量子集与使用完整集合进行训练的方案的性能。

    Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite that fact that we get no additional data from the world. Similarly, large language models can perform better at complex tasks through chain-of-thought reasoning, where they generate intermediate steps before answering a question. We use language models to investigate the questions of when and why reasoning is helpful, testing the hypothesis that reasoning is effective when training data consisting of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We train an autoregressive transformer on samples from joint distributions defined by Bayes nets, but only include a subset of all the variables in each sample. We compare lang
    
[^91]: 协同人工智能的根源和要求

    Roots and Requirements for Collaborative AI. (arXiv:2303.12040v1 [cs.AI])

    [http://arxiv.org/abs/2303.12040](http://arxiv.org/abs/2303.12040)

    论文探讨了AI协同合作的历史和要求，是协同AI研究的动机和背景。

    

    AI协作者的愿景长期以来一直是科幻小说的经典素材，其中人工智能代理理解协作和人类沟通的微妙差别。它们通过贡献特殊的才能给他们的人类合作者和团队带来优势。多年来，政府咨询团体和人工智能领域的领袖一直倡导AIs应该具有人类兼容性和有效协作的能力。然而，具备像才华横溢的人那样协作能力的强大的AI仍然遥不可及。这篇论文依据对人工智能和人类代理有效和强大协作所需认知的分析，概述了公众和AI愿景中关于人工协作者的历史，开始于早期智能增强(IA)和人工智能(AI)的愿景。这篇论文旨在成为协同AI的第二个立场文件(Stefik & Price, 2023)的动机和背景。第二篇论文回顾了多学科的现状，并提出了一个AI协作研究的路线图。

    The vision of AI collaborators has long been a staple of science fiction, where artificial agents understand nuances of collaboration and human communication. They bring advantages to their human collaborators and teams by contributing their special talents. Government advisory groups and leaders in AI have advocated for years that AIs should be human compatible and be capable of effective collaboration. Nonetheless, robust AIs that can collaborate like talented people remain out of reach. This position paper draws on a cognitive analysis of what effective and robust collaboration requires of human and artificial agents. It sketches a history of public and AI visions for artificial collaborators, starting with early visions of intelligence augmentation (IA) and artificial intelligence (AI). It is intended as motivation and context for a second position paper on collaborative AI (Stefik & Price, 2023). The second paper reviews the multi-disciplinary state-of-the-art and proposes a roadm
    
[^92]: xASTNN：用于工业实践的改进代码表示方法

    xASTNN: Improved Code Representations for Industrial Practice. (arXiv:2303.07104v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2303.07104](http://arxiv.org/abs/2303.07104)

    xASTNN是一种基于极端抽象语法树（AST）的神经网络，旨在将深度学习技术推广到工业实践中。它的优点包括适用于不同编程语言和实际场景，不需要复杂的数据预处理，以及提供了三个设计来保证其有效性。

    

    深度学习技术在软件工程中的应用越来越普遍。其中一个关键问题是为代码相关任务开发高质量且易于使用的源代码表示方法。最近几年研究界已经取得了令人瞩目的成果。然而，由于部署困难和性能瓶颈，这些方法很少被应用于工业领域。在本文中，我们提出了xASTNN，一种基于极端抽象语法树（AST）的神经网络，旨在将这种技术推广到工业实践中。xASTNN的三个优点：首先，xASTNN完全基于广泛使用的AST，不需要复杂的数据预处理，适用于不同的编程语言和实际场景。其次，提出了三个密切相关的设计来保证xASTNN的有效性，包括用于代码自然性的语句子树序列、用于句法建模的门控递归单元。

    The application of deep learning techniques in software engineering becomes increasingly popular. One key problem is developing high-quality and easy-to-use source code representations for code-related tasks. The research community has acquired impressive results in recent years. However, due to the deployment difficulties and performance bottlenecks, seldom these approaches are applied to the industry. In this paper, we present xASTNN, an eXtreme Abstract Syntax Tree (AST)-based Neural Network for source code representation, aiming to push this technique to industrial practice. The proposed xASTNN has three advantages. First, xASTNN is completely based on widely-used ASTs and does not require complicated data pre-processing, making it applicable to various programming languages and practical scenarios. Second, three closely-related designs are proposed to guarantee the effectiveness of xASTNN, including statement subtree sequence for code naturalness, gated recursive unit for syntacti
    
[^93]: SEGA：使用语义引导指导文本到图像模型的指导

    SEGA: Instructing Text-to-Image Models using Semantic Guidance. (arXiv:2301.12247v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.12247](http://arxiv.org/abs/2301.12247)

    本论文介绍了一种称为SEGA的语义引导方法，用于指导文本到图像模型的生成过程。通过与扩散过程的互动，SEGA可以灵活地在语义方向上引导模型的生成，实现细微和广泛的编辑以及优化整体艺术构思。实验证明SEGA在多种任务和生成架构上都表现出了出色的多功能性、灵活性和改进。

    

    最近，文本到图像扩散模型因其令人惊讶的能力可以仅通过文本生成高保真度的图像而受到了广泛的关注。然而，实现与用户意图对齐的单次生成几乎是不可能的，而输入提示的微小改变往往会导致非常不同的图像。这使得用户在语义控制方面有限。为了使用户实现控制，我们展示了如何通过与扩散过程互动来灵活地引导语义方向。这种语义引导(SEGA)可以推广到任何使用无分类器引导的生成架构。更重要的是，它允许进行细微和广泛的编辑，组成和风格的变化，以及优化整体艺术构思。我们使用各种任务展示了SEGA在潜在和像素级扩散模型（如Stable Diffusion，Paella和DeepFloyd-IF）上的有效性，从而为其多功能性，灵活性和改进提供了有力的证据。

    Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions. This semantic guidance (SEGA) generalizes to any generative architecture using classifier-free guidance. More importantly, it allows for subtle and extensive edits, changes in composition and style, as well as optimizing the overall artistic conception. We demonstrate SEGA's effectiveness on both latent and pixel-based diffusion models such as Stable Diffusion, Paella, and DeepFloyd-IF using a variety of tasks, thus providing strong evidence for its versatility, flexibility, and improvements ov
    
[^94]: Tracr: 编译变压器模型作为可解释性实验室

    Tracr: Compiled Transformers as a Laboratory for Interpretability. (arXiv:2301.05062v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05062](http://arxiv.org/abs/2301.05062)

    Tracr是一个编译器，将可读性强的程序编译成标准的仅解码变压器模型，该编译模型的已知结构可以用于设计实验和评估可解释方法。

    

    我们展示了如何将可读性强的程序编译成标准的仅解码变压器模型。我们的编译器Tracr生成具有已知结构的模型，可以用于设计实验。例如，我们使用它来研究执行多步算法的变压器中的“叠加”。此外，Tracr编译模型的已知结构可以作为评估可解释方法的真实基准。通常，由于变压器学习的“程序”是未知的，因此不清楚解释是否成功。我们通过实现和检查包括计算令牌频率、排序和括号检查在内的程序来演示我们的方法。我们在https://github.com/deepmind/tracr提供了Tracr的开源实现。

    We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.
    
[^95]: 多模态原型增强网络用于少样本动作识别

    Multimodal Prototype-Enhanced Network for Few-Shot Action Recognition. (arXiv:2212.04873v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.04873](http://arxiv.org/abs/2212.04873)

    该论文提出了一种多模态原型增强网络(MORN)用于少样本动作识别，通过利用标签文本的语义信息来增强原型，具有较好的性能表现。

    

    当前的少样本动作识别方法主要采用原型学习框架，遵循ProtoNet的方法，显示了原型的重要性。虽然它们取得了相对较好的性能，但多模态信息的效果被忽略，例如标签文本。在这项工作中，我们提出了一种新颖的多模态原型增强网络（MORN），它利用标签文本的语义信息作为多模态信息来增强原型。我们引入了一个CLIP视觉编码器和一个冻结的CLIP文本编码器，以获得具有良好多模态初始化的特征。然后，在视觉流程中，通过一个时间关系交叉变换器(TRX)模块计算视觉原型。在文本流程中，使用一个语义增强(SE)模块和一个扩张操作来获取文本原型。最后，通过一个多模态原型增强(MPE)模块计算最终的多模态原型。此外，我们定义了一个原型相似性差异(PRIDE)来评估质量。

    Current methods for few-shot action recognition mainly fall into the metric learning framework following ProtoNet, which demonstrates the importance of prototypes. Although they achieve relatively good performance, the effect of multimodal information is ignored, e.g. label texts. In this work, we propose a novel MultimOdal PRototype-ENhanced Network (MORN), which uses the semantic information of label texts as multimodal information to enhance prototypes. A CLIP visual encoder and a frozen CLIP text encoder are introduced to obtain features with good multimodal initialization. Then in the visual flow, visual prototypes are computed by a Temporal-Relational CrossTransformer (TRX) module for example. In the text flow, a semantic-enhanced (SE) module and an inflating operation are used to obtain text prototypes. The final multimodal prototypes are then computed by a multimodal prototype-enhanced (MPE) module. Besides, we define a PRototype SImilarity DiffErence (PRIDE) to evaluate the qu
    
[^96]: 从理解基因漂变到基于智能重启机制的分布估计算法

    From Understanding Genetic Drift to a Smart-Restart Mechanism for Estimation-of-Distribution Algorithms. (arXiv:2206.09090v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2206.09090](http://arxiv.org/abs/2206.09090)

    这篇论文介绍了一种基于智能重启机制的分布估计算法，该算法可以在基因漂变风险高的情况下停止运行，并寻找良好的参数范围以运行EDA，从而提高性能。

    

    估计分布算法（EDAs）是一种优化算法，它从搜索空间中学习一个分布，从中可以轻松地采样出好的解决方案。大多数EDAs的关键参数是样本大小（种群大小）。如果种群大小太小，概率模型更新仅基于少量样本，导致不希望出现的基因漂变效应。种群大小过大会避免遗传漂变，但会减缓进程。基于最近量化分析的种群大小如何导致基因漂变，我们设计了EDAs的智能重启机制。当基因漂变风险很高时停止运行，它会自动在良好的参数范围内运行EDA。通过数学运行时间分析，我们为这种智能重启方案证明了一个通用的性能保证。特别地，这表明在许多情况下，如果已知最佳的（问题特定的）参数值，重启方案会自动发现这些值，从而导致更好的性能。

    Estimation-of-distribution algorithms (EDAs) are optimization algorithms that learn a distribution on the search space from which good solutions can be sampled easily. A key parameter of most EDAs is the sample size (population size). If the population size is too small, the update of the probabilistic model builds on few samples, leading to the undesired effect of genetic drift. Too large population sizes avoid genetic drift, but slow down the process.  Building on a recent quantitative analysis of how the population size leads to genetic drift, we design a smart-restart mechanism for EDAs. By stopping runs when the risk for genetic drift is high, it automatically runs the EDA in good parameter regimes.  Via a mathematical runtime analysis, we prove a general performance guarantee for this smart-restart scheme. This in particular shows that in many situations where the optimal (problem-specific) parameter values are known, the restart scheme automatically finds these, leading to the a
    
[^97]: 稳定LIF神经元训练

    Stabilizing the LIF Neuron Training. (arXiv:2202.00282v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2202.00282](http://arxiv.org/abs/2202.00282)

    该论文研究了稳定LIF神经元训练的方法，通过实验和理论分析，确定了在不同任务和网络中选择最佳替代梯度的稳定性与效果的关系，减少了对超参数搜索的需求。

    

    脉冲神经形态计算利用二进制活动来提高人工智能的能源效率。然而，二进制活动的非平滑性要求使用近似梯度，也称为替代梯度（SG），以弥合与深度学习的性能差距。文献中已提出了几种SG，但目前尚不清楚如何确定适合特定任务和网络的最佳SG。在昂贵的超参数搜索后，大多数SG形状都可以实现良好的性能。因此，我们旨在在不同的压力测试中实验证明最佳SG，并在实验和理论上减少未来对网格搜索的需求。为了理解该领域的差距，我们展示了更复杂的任务和网络需要更慎重地选择SG，即使整体上，快速Sigmoid函数的导数在各种学习率下表现优于其他SG。因此，在训练之前，我们设计了一种基于稳定性的理论方法来选择初始化和SG形状。

    Spiking Neuromorphic Computing uses binary activity to improve Artificial Intelligence energy efficiency. However, the non-smoothness of binary activity requires approximate gradients, known as Surrogate Gradients (SG), to close the performance gap with Deep Learning. Several SG have been proposed in the literature, but it remains unclear how to determine the best SG for a given task and network. Good performance can be achieved with most SG shapes, after a costly search of hyper-parameters. Thus, we aim at experimentally and theoretically define the best SG across different stress tests, to reduce future need of grid search. To understand the gap for this line of work, we show that more complex tasks and networks need more careful choice of SG, even if overall the derivative of the fast sigmoid outperforms other SG across tasks and networks, for a wide range of learning rates. We therefore design a stability based theoretical method to choose initialization and SG shape before trainin
    
[^98]: 图神经扩散网络用于半监督学习

    Graph Neural Diffusion Networks for Semi-supervised Learning. (arXiv:2201.09698v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.09698](http://arxiv.org/abs/2201.09698)

    提出了一种名为 GND-Nets 的图神经网络，利用浅层网络和局部、全局邻域信息来解决图半监督学习中的过度平滑和欠平滑问题。

    

    图卷积网络 (GCN) 是用于基于图的半监督学习的先驱模型。然而，GCN 在标记稀疏的图上表现不佳。其两层版本不能有效地将标签信息传播到整个图结构（即欠平滑问题），而其深层版本则过度平滑且难以训练（即过度平滑问题）。为了解决这两个问题，我们提出了一种新的图神经网络，称为 GND-Nets（图神经扩散网络），它在单层中利用了顶点的局部和全局邻域信息。利用浅层网络可以缓解过度平滑问题，而利用局部和全局邻域信息可以缓解欠平滑问题。顶点的局部和全局邻域信息的利用是通过一种称为神经扩散的新图扩散方法实现的，该方法将神经网络融入传统的线性和非线性图扩散中。

    Graph Convolutional Networks (GCN) is a pioneering model for graph-based semi-supervised learning. However, GCN does not perform well on sparsely-labeled graphs. Its two-layer version cannot effectively propagate the label information to the whole graph structure (i.e., the under-smoothing problem) while its deep version over-smoothens and is hard to train (i.e., the over-smoothing problem). To solve these two issues, we propose a new graph neural network called GND-Nets (for Graph Neural Diffusion Networks) that exploits the local and global neighborhood information of a vertex in a single layer. Exploiting the shallow network mitigates the over-smoothing problem while exploiting the local and global neighborhood information mitigates the under-smoothing problem. The utilization of the local and global neighborhood information of a vertex is achieved by a new graph diffusion method called neural diffusions, which integrate neural networks into the conventional linear and nonlinear gra
    
[^99]: 特征注意的递归模块在强化学习中的泛化能力

    Feature-Attending Recurrent Modules for Generalization in Reinforcement Learning. (arXiv:2112.08369v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.08369](http://arxiv.org/abs/2112.08369)

    "Feature-Attending Recurrent Modules" (FARM)是一种学习状态表示的体系结构，通过特征注意机制来捕捉空间和时间规律性，从而改善强化学习代理的泛化能力。

    

    许多重要的任务都是以物体为基础定义的。为了在这些任务上实现泛化，强化学习（RL）代理需要利用物体所引发的结构。先前的工作要么硬编码对象中心的特征，要么使用复杂的对象中心生成模型，要么使用局部空间特征更新状态。然而，这些方法在实现泛化RL代理方面的成功有限。受此启发，我们引入了"特征注意的递归模块"（FARM），这是一种学习状态表示的体系结构，依赖于简单、广泛适用的归纳偏置来捕捉空间和时间规律性。FARM学习了一种分布于多个模块之间的状态表示，每个模块都使用具有表现力的特征注意机制关注时空特征。我们展示了这种方法改善了RL代理在对象中心任务上的泛化能力。我们在2D和3D环境中研究了任务套件，并发现FARM在泛化能力方面表现更好。

    Many important tasks are defined in terms of object. To generalize across these tasks, a reinforcement learning (RL) agent needs to exploit the structure that the objects induce. Prior work has either hard-coded object-centric features, used complex object-centric generative models, or updated state using local spatial features. However, these approaches have had limited success in enabling general RL agents. Motivated by this, we introduce "Feature-Attending Recurrent Modules" (FARM), an architecture for learning state representations that relies on simple, broadly applicable inductive biases for capturing spatial and temporal regularities. FARM learns a state representation that is distributed across multiple modules that each attend to spatiotemporal features with an expressive feature attention mechanism. We show that this improves an RL agent's ability to generalize across object-centric tasks. We study task suites in both 2D and 3D environments and find that FARM better generaliz
    
[^100]: 关于最小化器和卷积滤波器的理论连接及其在基因组分析中的应用

    On minimizers and convolutional filters: theoretical connections and applications to genome analysis. (arXiv:2111.08452v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08452](http://arxiv.org/abs/2111.08452)

    该论文通过对哈希函数属性进行数学分析，发现在分类字母表上的序列分析中，使用随机高斯初始化的卷积滤波器和最大池化等价于选择一种最小化器排序，能够有效提取与其他最小化器距离较近但与序列中的k-mer相距较远的重要特征。

    

    最小化器和卷积神经网络(CNN)是两种完全不同的流行技术，均被用于分析生物序列。从表面上看，这些方法似乎完全不同。最小化器使用滚动窗口的最小哈希方法提取每个窗口中的一个重要k-mer特征。CNN则以随机初始化的卷积滤波器和池化操作为基础，通过多个神经层来学习滤波器本身及其用于分类序列的方法。本文主要结果是对哈希函数属性进行了仔细的数学分析，显示对于分类字母表上的序列，使用随机高斯初始化的卷积滤波器和最大池化等价于选择一个最小化器排序，使得选择的k-mer与序列中的k-mer（按汉明距离）相距较远，但与其他最小化器相距较近。在实证实验中，我们发现这种方法能够有效降低计算复杂度并与传统方法具有相当的性能。

    Minimizers and convolutional neural networks (CNNs) are two quite distinct popular techniques that have both been employed to analyze categorical biological sequences. At face value, the methods seem entirely dissimilar. Minimizers use min-wise hashing on a rolling window to extract a single important k-mer feature per window. CNNs start with a wide array of randomly initialized convolutional filters, paired with a pooling operation, and then multiple additional neural layers to learn both the filters themselves and how they can be used to classify the sequence.  Here, our main result is a careful mathematical analysis of hash function properties showing that for sequences over a categorical alphabet, random Gaussian initialization of convolutional filters with max-pooling is equivalent to choosing a minimizer ordering such that selected k-mers are (in Hamming distance) far from the k-mers within the sequence but close to other minimizers. In empirical experiments, we find that this pr
    
[^101]: ReLU'(0)对反向传播的数值影响研究

    Numerical influence of ReLU'(0) on backpropagation. (arXiv:2106.12915v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.12915](http://arxiv.org/abs/2106.12915)

    本研究研究了ReLU'(0)值对深度学习中反向传播的数值影响，发现在32位精度下会出现显著的变化，而在16位精度下是系统性的。在普通的SGD训练中，选择ReLU'(0) = 0似乎是最有效的。此外，重新调整方法 tend to buffer ReLU'(0)值的影响。

    

    在理论上，神经网络中ReLU'(0)在[0, 1]范围内的选择对反向传播和训练几乎没有影响。然而，在现实世界中，32位默认精度结合深度学习问题的规模，使其成为训练方法的超参数。我们研究了ReLU'(0)值对几种精度水平（16位，32位，64位）、各种网络（全连接、VGG、ResNet）和数据集（MNIST、CIFAR10、SVHN）的重要性。我们观察到在32位精度下，反向传播输出出现了显著的变化，这种情况大约出现了一半的时间。这种影响在双精度下消失，而在16位精度下是系统性的。对于普通的SGD训练而言，选择ReLU'(0) = 0似乎是最有效的。我们还发现，批归一化或ADAM等重新调整方法 tend to buffer ReLU'(0)值的影响。总体而言，我们想要传达的信息是，非光滑问题的算法微分可能隐藏了一些参数。

    In theory, the choice of ReLU'(0) in [0, 1] for a neural network has a negligible influence both on backpropagation and training. Yet, in the real world, 32 bits default precision combined with the size of deep learning problems makes it a hyperparameter of training methods. We investigate the importance of the value of ReLU'(0) for several precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST, CIFAR10, SVHN). We observe considerable variations of backpropagation outputs which occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU'(0) = 0 seems to be the most efficient. We also evidence that reconditioning approaches as batch-norm or ADAM tend to buffer the influence of ReLU'(0)'s value. Overall, the message we want to convey is that algorithmic differentiation of nonsmooth problems potentially hides parameters that 
    

