# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense.](http://arxiv.org/abs/2307.11730) | 本文针对分散式联邦学习中的通信安全挑战，引入了一个安全模块，通过结合加密技术和移动目标防御技术来对抗通信攻击。 |
| [^2] | [Benchmark datasets for biomedical knowledge graphs with negative statements.](http://arxiv.org/abs/2307.11719) | 这篇论文提出了一种具有负面陈述的生物医学知识图谱基准数据集，旨在解决评估考虑负面陈述的方法的困难。该数据集包含三个关系预测任务的数据，并利用两种基于路径的方法生成了知识图谱嵌入。 |
| [^3] | [Statement-based Memory for Neural Source Code Summarization.](http://arxiv.org/abs/2307.11709) | 该论文研究了基于语句的记忆神经源代码摘要化。当前的方法往往将子程序作为单个单位处理，但事实上代码的行为取决于语句之间的流动。 |
| [^4] | [SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design.](http://arxiv.org/abs/2307.11694) | 本文提出了一种通过上下文学习个性化药物协同作用并进行药物设计的方法，该方法利用小型的个性化数据集，不依赖于文本语料库、分子指纹或蛋白质相互作用的领域特定知识，取得了竞争性的结果。 |
| [^5] | [Interpretable Graph Networks Formulate Universal Algebra Conjectures.](http://arxiv.org/abs/2307.11688) | 提出了首次使用人工智能来研究通用代数的猜想，并引入了可解释的图网络以增强可解释性。 |
| [^6] | [Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts.](http://arxiv.org/abs/2307.11661) | 本文展示了如何利用GPT-4生成具有视觉描述性的文本，并将其用于适应CLIP到下游任务。与CLIP的默认提示相比，在专门细粒度数据集上显示了较大的0-shot迁移准确性改进。 |
| [^7] | [Bandits with Deterministically Evolving States.](http://arxiv.org/abs/2307.11655) | 该论文提出了一种名为具有确定性演化状态的强盗模型，用于学习带有强盗反馈的推荐系统和在线广告。该模型考虑了状态演化的不同速率，能准确评估奖励与系统健康程度之间的关系。 |
| [^8] | [Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models.](http://arxiv.org/abs/2307.11643) | 本文提出了一个名为AI推理器的解释性模型，能够从图像中提取缺陷的形态学特征，并利用决策树进行推理。它通过可视化和文字说明解释基于掩模的缺陷检测和分类模型的输出，并提供有效的缓解策略以提升整体模型性能。 |
| [^9] | [Integration of Domain Expert-Centric Ontology Design into the CRISP-DM for Cyber-Physical Production Systems.](http://arxiv.org/abs/2307.11637) | 本文提出了一种将领域专家中心本体设计集成到CRISP-DM中的方法，以解决公司在应用数据驱动项目时所遇到的挑战。 |
| [^10] | [On the Complexity of the Bipartite Polarization Problem: from Neutral to Highly Polarized Discussions.](http://arxiv.org/abs/2307.11621) | 本论文研究了双分区极化问题的复杂性，并提出了一个实例生成模型，通过控制实例的极化程度与解决这些实例的复杂度之间的关系。研究表明，实例的极化程度越高，解决问题越容易。 |
| [^11] | [CausE: Towards Causal Knowledge Graph Embedding.](http://arxiv.org/abs/2307.11610) | CausE是一个采用因果知识图谱嵌入和嵌入解缠的框架，利用因果干预进行稳定预测，并在知识图谱完整性任务上取得了最先进的性能。 |
| [^12] | [Feature Map Testing for Deep Neural Networks.](http://arxiv.org/abs/2307.11563) | 本研究提出了DeepFeature方法，从特征图层面对深度神经网络进行测试，解决了当前测试方法主要关注神经元而忽略特征图的问题。 |
| [^13] | [CycleIK: Neuro-inspired Inverse Kinematics.](http://arxiv.org/abs/2307.11554) | 本文介绍了CycleIK，一种神经机器人逆运动学方法，它包括生成对抗网络（GAN）和多层感知器（MLP）架构。通过嵌入混合神经遗传逆运动学流水线并使用稠密数据集进行训练和测试，我们展示了该方法在逆运动学上的优越性能。这使得CycleIK可以直接部署到机器人硬件上。 |
| [^14] | [Identifying Relevant Features of CSE-CIC-IDS2018 Dataset for the Development of an Intrusion Detection System.](http://arxiv.org/abs/2307.11544) | 本文研究了CSE-CIC-IDS2018数据集中相关特征的识别，重点关注入侵检测系统的开发。通过应用特征选择方法和分类算法，确定了最终的特征子集，并评估了其效果。 |
| [^15] | [Model Reporting for Certifiable AI: A Proposal from Merging EU Regulation into AI Development.](http://arxiv.org/abs/2307.11525) | 提出了结合欧盟法规与最新研究趋势的可证明人工智能模型报告的建议，通过使用标准化的卡片记录AI应用的开发过程，并引入用例和操作卡以满足监管要求，旨在开发安全可信的AI系统并支持第三方审计。 |
| [^16] | [Multi-modal Hate Speech Detection using Machine Learning.](http://arxiv.org/abs/2307.11519) | 本研究提出了一种使用多模态系统的方法，通过提取图像特征、音频特征值和文本，结合机器学习和自然语言处理技术，来检测视频内容中的仇恨言论。 |
| [^17] | [IndigoVX: Where Human Intelligence Meets AI for Optimal Decision Making.](http://arxiv.org/abs/2307.11516) | IndigoVX是一种将人类智慧与人工智能相结合的最优决策方法，通过迭代反馈循环，利用人类专家的背景知识和人工智能的数据驱动洞见，制定和优化朝着明确定义的目标的策略，并使用定量化的三分数模式进行评估和改进。 |
| [^18] | [General regularization in covariate shift adaptation.](http://arxiv.org/abs/2307.11503) | 本文研究了协变量偏移自适应中的一般正则化方法，并通过组合已有结果得到了新的结果。在弱平滑条件下证明了实现与标准监督学习中相同精度所需的样本量要比现有分析证明的少。 |
| [^19] | [Adaptive ResNet Architecture for Distributed Inference in Resource-Constrained IoT Systems.](http://arxiv.org/abs/2307.11499) | 这项研究提出了一种自适应ResNet架构，用于在资源受限的物联网系统中进行分布式推理。通过识别可删除但不会显著影响性能的连接，实现对模型的适应性缩减。该架构能够降低延迟和能耗，并提高系统的鲁棒性。 |
| [^20] | [Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting.](http://arxiv.org/abs/2307.11494) | 本研究提出了一种面向概率时间序列预测的自引导扩散模型，称为TSDiff。该模型不需要辅助网络或训练过程的改变，在预测、改进和合成数据生成等时间序列任务上展现出了竞争力。 |
| [^21] | [Distribution Shift Matters for Knowledge Distillation with Webly Collected Images.](http://arxiv.org/abs/2307.11469) | 本研究提出了一种解决知识蒸馏中分布偏移问题的新方法，通过动态选择有用的训练样本并对特征和分类进行对齐，提高了训练出的学生网络的可靠性。 |
| [^22] | [Zero-touch realization of Pervasive Artificial Intelligence-as-a-service in 6G networks.](http://arxiv.org/abs/2307.11468) | 在6G网络中，本文介绍了一种新颖的平台架构，通过使用基于区块链的智能系统支持部署零接触的PAI即服务，实现了普适人工智能服务的系统化分发。 |
| [^23] | [Improve Long-term Memory Learning Through Rescaling the Error Temporally.](http://arxiv.org/abs/2307.11462) | 本研究通过时间上重新缩放错误度量，改善了长期记忆学习的偏向于短期记忆的问题，并缓解了梯度消失的困扰。数值实验验证了适当的时间上重新缩放错误在有效的长期记忆学习中的重要性。 |
| [^24] | [Incorporating Human Translator Style into English-Turkish Literary Machine Translation.](http://arxiv.org/abs/2307.11457) | 本论文研究了英土文学翻译，在机器翻译模型中采用翻译家的风格特征，并通过微调预训练模型获得了高度还原人类译者风格的机器翻译效果。 |
| [^25] | [Providing personalized Explanations: a Conversational Approach.](http://arxiv.org/abs/2307.11452) | 本文提出了一种通过连续对话的方式，将个性化解释传达给被解释者的方法，并证明了只要存在一个被解释者理解并且解释者知晓的对于初始主张的解释，对话将因为被解释者对初始主张的证明而终止。 |
| [^26] | [AIGC Empowering Telecom Sector White Paper.](http://arxiv.org/abs/2307.11449) | 本文探讨了AIGC（GPT）在电信行业中的应用，提出了一个电信增强认知能力系统，为电信服务的构建提供了解决方案。 |
| [^27] | [Batching for Green AI -- An Exploratory Study on Inference.](http://arxiv.org/abs/2307.11434) | 这项研究探讨了在深度学习模型推理阶段引入批处理对能源消耗和响应时间的影响，并发现批处理对这两个指标都有显著影响。 |
| [^28] | [A Video-based Detector for Suspicious Activity in Examination with OpenPose.](http://arxiv.org/abs/2307.11413) | 本研究提出了一种利用OpenPose框架和卷积神经网络(CNN)分析考试视频并高效有效地检测可疑活动的框架。 |
| [^29] | [Deep Directly-Trained Spiking Neural Networks for Object Detection.](http://arxiv.org/abs/2307.11411) | EMS-YOLO是一种直接训练的脉冲神经网络框架，通过使用替代梯度而不是ANN-SNN转换策略，成功解决了深度SNN的目标检测问题。 |
| [^30] | [Probabilistic Modeling of Inter- and Intra-observer Variability in Medical Image Segmentation.](http://arxiv.org/abs/2307.11397) | 本文提出了一种新颖的概率模型Pionono，用于医学图像分割中的观察者间和观察者内变异。该模型通过捕捉每个标记者的标记行为并将其与图像特征集成，产生概率分割预测。实验证明Pionono在准确性和效率上优于现有模型，并且可以预测多个一致的分割图，为诊断过程提供了宝贵的信息。 |
| [^31] | [Large Language Model-based System to Provide Immediate Feedback to Students in Flipped Classroom Preparation Learning.](http://arxiv.org/abs/2307.11388) | 本文提出了基于大型语言模型的系统，旨在为翻转课堂准备学习中的学生提供即时反馈。该系统不仅解决了学生在观看课堂视频时遇到的问题，还通过对回答与上下文的对齐和教师答案的收集，提供了额外的学习指导。 |
| [^32] | [Diverse Offline Imitation via Fenchel Duality.](http://arxiv.org/abs/2307.11373) | 本文提出了一个离线技能发现算法，通过Fenchel对偶方法将强化学习和无监督技能发现结合起来，实现学习与专家相一致的多样的技能。 |
| [^33] | [CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study.](http://arxiv.org/abs/2307.11346) | CohortGPT是一种增强型GPT，用于解决临床研究中的参与者招募任务。此研究发现传统的大型语言模型在医疗文本分类中的性能一般，这可能是由于它们忽略了语言中的上下文信息。 |
| [^34] | [A Two-stage Fine-tuning Strategy for Generalizable Manipulation Skill of Embodied AI.](http://arxiv.org/abs/2307.11343) | 本文提出了一种两阶段微调策略，旨在基于Maniskill2基准进一步提升具有身体的人工智能模型的泛化能力。通过在ManiSkill2挑战赛中取得第一名，我们的研究结果突出了该方法提高泛化能力的潜力。 |
| [^35] | [OpenGDA: Graph Domain Adaptation Benchmark for Cross-network Learning.](http://arxiv.org/abs/2307.11341) | OpenGDA是一个用于跨网络学习的图领域自适应基准，提供了丰富的数据集和评估方法以全面评估模型在不同任务和场景中的性能。 |
| [^36] | [Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields.](http://arxiv.org/abs/2307.11335) | Tri-MipRF 提出了一种新颖的三向 Mip 编码方法，通过将预滤波的三维特征空间分解为正交的 mipmaps，实现了神经辐射场的即时重建和抗锯齿高保真渲染。 |
| [^37] | [Analysis of Elephant Movement in Sub-Saharan Africa: Ecological, Climatic, and Conservation Perspectives.](http://arxiv.org/abs/2307.11325) | 本研究分析了萨赫勒以南非洲象移动的模式，重点关注了季节变化和降雨模式等动态驱动因素。研究结果有助于预测生态因素对象迁徙的潜在影响，并为制定保护策略提供了综合的视角。 |
| [^38] | [HVDetFusion: A Simple and Robust Camera-Radar Fusion Framework.](http://arxiv.org/abs/2307.11323) | HVDetFusion是一种简单而强大的相机-雷达融合框架，支持纯相机数据和相机+雷达数据的多模态检测，通过修改框架和利用先验信息实现更好的感知和过滤误报信息。 |
| [^39] | [How to Tidy Up a Table: Fusing Visual and Semantic Commonsense Reasoning for Robotic Tasks with Vague Objectives.](http://arxiv.org/abs/2307.11319) | 这项工作提出了一种融合视觉和语义常识推理的方法来解决具有模糊目标的机器人任务中的整理桌子问题。通过利用大规模语言模型的学习，可以推理出人类行为的常识。尽管语言模型的能力受限，但通过考虑感知和低级控制因素，可以解决整理桌子的任务。 |
| [^40] | [XLDA: Linear Discriminant Analysis for Scaling Continual Learning to Extreme Classification at the Edge.](http://arxiv.org/abs/2307.11317) | 本文提出了一种名为 XLDA 的框架，可在边缘上进行极端分类，其中使用的线性判别分析（LDA）分类器等效于全连接（FC）层，通过优化实现了加速训练和推理的方法，并在极端数据集上进行了验证。 |
| [^41] | [DPM-OT: A New Diffusion Probabilistic Model Based on Optimal Transport.](http://arxiv.org/abs/2307.11308) | DPM-OT是一种基于最优传输的新型扩散概率模型，通过在不同阶段的潜变量之间进行最优传输，提出了一个快速DPMs的统一学习框架，可以在约10个函数评估内生成高质量的样本。 |
| [^42] | [Kernelized Offline Contextual Dueling Bandits.](http://arxiv.org/abs/2307.11288) | 本论文提出了基于内核的离线背景双向竞标者算法，用于解决基于偏好反馈的问题，并证明了算法的遗憾界。通过实验证明该方法优于使用均匀采样上下文的相似策略。 |
| [^43] | [Eliminating Unintended Stable Fixpoints for Hybrid Reasoning Systems.](http://arxiv.org/abs/2307.11286) | 这项研究介绍了一种类似AFT的方法论，能够利用先前计算的上界更精确地捕捉混合MKNF知识库的语义。 |
| [^44] | [Joint one-sided synthetic unpaired image translation and segmentation for colorectal cancer prevention.](http://arxiv.org/abs/2307.11253) | 通过联合训练分割模型和生成模型，使用3D技术和生成对抗网络合成逼真的图像，从而解决医学图像分析中的数据集获取困难问题，并在结肠息肉分割任务上取得有希望的结果。 |
| [^45] | [On-Sensor Data Filtering using Neuromorphic Computing for High Energy Physics Experiments.](http://arxiv.org/abs/2307.11242) | 本文研究了在高能物理实验中利用神经形态计算的尖峰神经网络（SNN）模型对传感器数据进行滤波的方法。通过将传入的电荷波形转换为二值事件流，并优化SNN的系统设计和超参数，我们得到了信号有效率约为91%的SNN模型，参数数量几乎是深度神经网络的一半。 |
| [^46] | [Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models.](http://arxiv.org/abs/2307.11224) | Jina Embeddings是一组高性能的句子嵌入模型，能够捕捉文本的语义本质。该论文详细介绍了Jina Embeddings的开发过程，并通过性能评估验证了其优越性能。 |
| [^47] | [Towards Ontologically Grounded and Language-Agnostic Knowledge Graphs.](http://arxiv.org/abs/2307.11206) | 本论文提出通过抽象对象的具象化和承认概念和类型之间的本体区别，建立起一个基于本体和语言无关的知识图谱表示，以缓解知识图谱整合中的困难。 |
| [^48] | [Exploring reinforcement learning techniques for discrete and continuous control tasks in the MuJoCo environment.](http://arxiv.org/abs/2307.11166) | 该论文研究了在MuJoCo环境中离散和连续控制任务的强化学习方法，通过比较不同算法，发现DDPG在少量episode中表现优于其他方法。 |
| [^49] | [Of Models and Tin Men -- a behavioural economics study of principal-agent problems in AI alignment using large-language models.](http://arxiv.org/abs/2307.11137) | 本研究基于行为经济学角度，对使用大语言模型进行AI对齐中的委托-代理问题进行研究，发现现实世界中的AI安全问题不仅涉及设计者与代理之间的冲突，还涉及到多个代理之间的信息不对称与效用函数之间的错位。 |
| [^50] | [Contrastive Graph Pooling for Explainable Classification of Brain Networks.](http://arxiv.org/abs/2307.11133) | 本论文提出了一种针对脑网络的对比图池化方法，以实现对脑网络的可解释分类。通过定制化的图神经网络和特殊设计的可解释特征提取方法，在5个静息态fMRI脑网络数据集上取得了优于最先进基准线的结果。 |
| [^51] | [Approximate Computing Survey, Part II: Application-Specific & Architectural Approximation Techniques and Applications.](http://arxiv.org/abs/2307.11128) | 近似计算是一种能够调整系统设计结果质量以提高能源效率和/或性能的新兴解决方案，已吸引学术界和工业界的广泛关注。这篇论文是一个关于应用特定和架构近似技术的调查的第二部分。 |
| [^52] | [Nature of Intelligence.](http://arxiv.org/abs/2307.11114) | 智能的本质是一系列通过在空间和时间上建立数据集之间的功能关系来最小化系统熵的数学函数过程。 |
| [^53] | [Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games.](http://arxiv.org/abs/2307.11105) | 本文介绍了在AAA游戏测试中部署强化学习代理所面临的技术挑战，并提出了一些帮助游戏行业采用这项技术的研究方向。 |
| [^54] | [Solving multiphysics-based inverse problems with learned surrogates and constraints.](http://arxiv.org/abs/2307.11099) | 本论文将学习代理和学习约束相结合用于解决基于多物理的反问题，通过该方法不仅改善了对流体流动性质的反演精度，而且为反演多模态数据提供了一个有效的解决方案。 |
| [^55] | [Dense Sample Deep Learning.](http://arxiv.org/abs/2307.10991) | 密集样本深度学习是一种针对深度学习网络的研究方法，旨在揭示学习机制和表示的未知特性，并解决大规模数据和隐藏单元存在的问题。 |
| [^56] | [AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models.](http://arxiv.org/abs/2307.10711) | AdjointDPM是一种新的伴随灵敏度方法，用于扩散概率模型的梯度反向传播，解决了DPM定制化中内存消耗高的问题，并通过解决增强的ODE将损失的梯度反向传播到模型的参数。 |
| [^57] | [Detecting deceptive reviews using text classification.](http://arxiv.org/abs/2307.10617) | 这篇论文提出了一种使用机器学习模型的方法来识别虚假评论，并通过在餐馆评论的数据集上进行实验验证了其性能。 |
| [^58] | [Ethosight: A Joint-Embedding Based System for Nuanced Perception Using Contextual Label Affinity Metric and Reasoning Based Iterative Learning.](http://arxiv.org/abs/2307.10577) | Ethosight是一种零样本计算机视觉算法，通过联合嵌入、上下文标签关联度计算和基于推理的迭代学习，实现对细微行为和场景细节的准确感知，同时消除了对预先存在符号知识的需求。 |
| [^59] | [(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs.](http://arxiv.org/abs/2307.10490) | 本论文展示了如何利用图像和声音在多模态LLMs中进行间接指令注入，攻击者通过生成对抗扰动并将其融入图像或音频录音中，以操纵模型输出特定文本和指导对话的行为。 |
| [^60] | [ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats.](http://arxiv.org/abs/2307.09782) | ZeroQuant-FP通过使用浮点格式进行LLMs训练后量化，解决了在大型语言模型中平衡计算效率和保持模型质量的挑战，并发现FP8激活优于INT8，并且FP4权重表现与INT4相当甚至更优。 |
| [^61] | [Ord2Seq: Regard Ordinal Regression as Label Sequence Prediction.](http://arxiv.org/abs/2307.09004) | Ord2Seq是一种将序回归问题转化为标签序列预测的方法，通过递归的二元分类步骤微妙地区分相邻类别，在不同场景中达到优于现有方法的性能表现。 |
| [^62] | [Computing the gradients with respect to all parameters of a quantum neural network using a single circuit.](http://arxiv.org/abs/2307.08167) | 该论文提出了一种使用单个电路计算量子神经网络所有参数梯度的方法，相比传统方法，它具有较低的电路深度和较少的编译时间，从而加速了总体运行时间。 |
| [^63] | [Quantitative CLTs in Deep Neural Networks.](http://arxiv.org/abs/2307.06092) | 本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。 |
| [^64] | [Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis.](http://arxiv.org/abs/2307.04541) | 本研究提出了一种用于开放式医学诊断的大边缘稀疏嵌入方法，通过引入角度边缘和自适应尺度，使算法能够正确分类已知类别并识别未知类别，以进一步支持医学诊断。 |
| [^65] | [Tranfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data.](http://arxiv.org/abs/2307.03512) | 本文研究了利用传输学习方法在LiDAR数据上识别埋藏的考古结构的语义分割。实验结果表明，传输学习的应用可以提高性能，为未来工作提供基准。 |
| [^66] | [Hybrid Knowledge-Data Driven Channel Semantic Acquisition and Beamforming for Cell-Free Massive MIMO.](http://arxiv.org/abs/2307.03070) | 本文提出了一种混合知识数据驱动的方法，用于无小区大规模MIMO系统中的通道语义获取和多用户波束赋形，可提高室外无线系统的性能，支持扩展现实应用。 |
| [^67] | [From $O(\sqrt n)$ to $O(\log n)$ in Quadratic Programming.](http://arxiv.org/abs/2306.15079) | 这篇论文提出了一种迭代复杂度为$O(\log(n))$的二次规划优化算法，并通过严格的理论证明验证了该算法的可行性。这一重大突破使得我们从$O(\sqrt{n})$的优化算法过渡到$O(\log(n))$的优化算法，其在大数据和人工智能时代具有重要应用价值。 |
| [^68] | [Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models.](http://arxiv.org/abs/2306.14096) | 本文提出了一个用于企业预警的新型、广泛的中文细粒度金融情感分析数据集FinChina SA，并使用现有开源大语言模型对其进行评估和实验。该数据集将成为推进真实金融情感分析任务探索的宝贵资源。 |
| [^69] | [Adaptive interventions for both accuracy and time in AI-assisted human decision making.](http://arxiv.org/abs/2306.07458) | 本研究探索适用于人工决策辅助的智能干预方案，在同时考虑准确性和时间性的前提下，根据问题和用户的属性自适应地展示AI辅助具有良好的效果。 |
| [^70] | [High-dimensional and Permutation Invariant Anomaly Detection.](http://arxiv.org/abs/2306.03933) | 该研究引入了一种置换不变的高维密度估计方法，通过学习后将其用于高能物理数据中的异常检测，能够有效地识别出在仅具备背景假设下排除异常的喷注。 |
| [^71] | [Shift-Robust Molecular Relational Learning with Causal Substructure.](http://arxiv.org/abs/2305.18451) | 本文提出了一种鲁棒性强的分子关系学习模型CMRL，通过检测与化学反应因果相关的核心亚结构来应对分子关系学习中的数据分布变化。 |
| [^72] | [Enhancing Coherence of Extractive Summarization with Multitask Learning.](http://arxiv.org/abs/2305.12851) | 本研究提出了一种使用多任务学习架构来增强抽取式摘要的连贯性的方法。实验证明，该方法显著提高了抽取式摘要的连贯性，并在其他评价指标方面也表现出良好的性能。 |
| [^73] | [Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness.](http://arxiv.org/abs/2303.17555) | 本文通过批判性回顾AI公平性文献中30篇交织性讨论，揭示研究人员普遍缺乏对交织性的整体理解，其一方面将其缩小为在群体子组上进行公平度量的优化，另一方面则在社会背景和权力结构的讨论方面存在欠缺。 |
| [^74] | [BoxSnake: Polygonal Instance Segmentation with Box Supervision.](http://arxiv.org/abs/2303.11630) | BoxSnake是一种新的端到端训练技术，可以仅使用框注释实现有效的多边形实例分割，相较于基于掩膜的弱监督方法，BoxSnake显示出显着的优越性。 |
| [^75] | [Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames.](http://arxiv.org/abs/2302.04973) | 本文介绍了一种通过以槽为中心的参考框架来改进对象发现的方法，通过在Slot Attention中融入空间对称性，可以大幅提高数据效率和整体对象发现效果。 |
| [^76] | [SpArX: Sparse Argumentative Explanations for Neural Networks.](http://arxiv.org/abs/2301.09559) | 该论文提出了一种稀疏的神经网络论证解释方法SpArX，通过利用多层感知器和定量论证框架之间的关系，可以为神经网络的决策过程提供更忠实和深入的解释。 |
| [^77] | [NusaCrowd: Open Source Initiative for Indonesian NLP Resources.](http://arxiv.org/abs/2212.09648) | NusaCrowd是一个印尼自然语言处理资源的开源倡议，已汇集137个数据集和118个数据加载程序，为印尼语和印度尼西亚本地语言的自然语言处理研究提供了多种实验手段。 |
| [^78] | [Evidence of Vocal Tract Articulation in Self-Supervised Learning of Speech.](http://arxiv.org/abs/2210.11723) | 这项研究分析了一系列自监督学习模型，通过线性探测方法将语音表示与发音轨迹相联系。结果显示声道发音在自监督学习中发挥了重要作用。 |
| [^79] | [A simple declarative model of the Federal Disaster Assistance Policy -- modelling and measuring transparency.](http://arxiv.org/abs/2207.07392) | 本研究量化分析了联邦灾难援助政策的一个简单模型，并从三个不同利益相关者的角度进行了评估，进而考虑了3种政策修改及其影响，旨在为各利益相关者的偏好排序提供参考。 |
| [^80] | [The activity-weight duality in feed forward neural networks: The geometric determinants of generalization.](http://arxiv.org/abs/2203.10736) | 这项研究发现了在前馈神经网络中，神经元活动的变化与连接到下一层神经元的权重变化之间的准确对偶关系。通过这种对偶性，我们能够将输入数据的变化映射到对应的权重变化，并发现泛化损失可以通过解的损失函数的Hessian矩阵的特征方向的几何因子的乘积来表示。 |
| [^81] | [Learning Multi-agent Skills for Tabular Reinforcement Learning using Factor Graphs.](http://arxiv.org/abs/2201.08227) | 本论文提出了一个方法可以直接在多智能体场景中计算多智能体技能，通过智能体之间的合作性探索行为来改善联合状态空间的连通性。 |

# 详细

[^1]: 通过移动目标防御减轻分散式联邦学习中的通信威胁

    Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense. (arXiv:2307.11730v1 [cs.CR])

    [http://arxiv.org/abs/2307.11730](http://arxiv.org/abs/2307.11730)

    本文针对分散式联邦学习中的通信安全挑战，引入了一个安全模块，通过结合加密技术和移动目标防御技术来对抗通信攻击。

    

    分散式联邦学习（DFL）的兴起使得机器学习模型可以在联邦参与方之间进行训练，促进了分散式模型聚合并减少对服务器的依赖。然而，这种方法引入了独特的通信安全挑战，尚未在文献中得到充分解决。这些挑战主要源于聚合过程的分散性质、参与者的多样化角色和责任以及缺乏监管和缓解威胁的中央机构。本文针对这些挑战，首先界定了一个全面的威胁模型，突出了DFL通信的潜在风险。针对这些确定的风险，本文引入了一个专为DFL平台设计的安全模块来对抗基于通信的攻击。该模块结合了对称和非对称加密等安全技术与移动目标防御（MTD）技术。

    The rise of Decentralized Federated Learning (DFL) has enabled the training of machine learning models across federated participants, fostering decentralized model aggregation and reducing dependence on a server. However, this approach introduces unique communication security challenges that have yet to be thoroughly addressed in the literature. These challenges primarily originate from the decentralized nature of the aggregation process, the varied roles and responsibilities of the participants, and the absence of a central authority to oversee and mitigate threats. Addressing these challenges, this paper first delineates a comprehensive threat model, highlighting the potential risks of DFL communications. In response to these identified risks, this work introduces a security module designed for DFL platforms to counter communication-based attacks. The module combines security techniques such as symmetric and asymmetric encryption with Moving Target Defense (MTD) techniques, including
    
[^2]: 具有负面陈述的生物医学知识图谱基准数据集

    Benchmark datasets for biomedical knowledge graphs with negative statements. (arXiv:2307.11719v1 [cs.AI])

    [http://arxiv.org/abs/2307.11719](http://arxiv.org/abs/2307.11719)

    这篇论文提出了一种具有负面陈述的生物医学知识图谱基准数据集，旨在解决评估考虑负面陈述的方法的困难。该数据集包含三个关系预测任务的数据，并利用两种基于路径的方法生成了知识图谱嵌入。

    

    知识图谱表示关于现实世界实体的事实，其中大多数事实被定义为正面陈述。负面陈述在开放世界假设下非常稀缺但高度相关。此外，已经证明它们可以改善多个应用程序的性能，特别是在生物医学领域。然而，目前没有支持考虑这些负面陈述的方法评估的基准数据集存在。我们提供了三个关系预测任务的数据集，包括蛋白质-蛋白质相互作用预测、基因-疾病关联预测和疾病预测，旨在解决构建具有负面陈述的知识图谱基准的困难。这些数据集包括来自两个成功的生物医学本体，基因本体和人类表型本体的数据，并添加了负面陈述。我们还使用两种流行的基于路径的方法为每个数据集生成了知识图谱嵌入，并评估了其性能。

    Knowledge graphs represent facts about real-world entities. Most of these facts are defined as positive statements. The negative statements are scarce but highly relevant under the open-world assumption. Furthermore, they have been demonstrated to improve the performance of several applications, namely in the biomedical domain. However, no benchmark dataset supports the evaluation of the methods that consider these negative statements.  We present a collection of datasets for three relation prediction tasks protein-protein interaction prediction, gene-disease association prediction and disease prediction - that aim at circumventing the difficulties in building benchmarks for knowledge graphs with negative statements. These datasets include data from two successful biomedical ontologies, Gene Ontology and Human Phenotype Ontology, enriched with negative statements.  We also generate knowledge graph embeddings for each dataset with two popular path-based methods and evaluate the perfor
    
[^3]: 基于语句的记忆神经源代码摘要化

    Statement-based Memory for Neural Source Code Summarization. (arXiv:2307.11709v1 [cs.AI])

    [http://arxiv.org/abs/2307.11709](http://arxiv.org/abs/2307.11709)

    该论文研究了基于语句的记忆神经源代码摘要化。当前的方法往往将子程序作为单个单位处理，但事实上代码的行为取决于语句之间的流动。

    

    源代码摘要化是将源代码行为写成自然语言描述的任务。代码摘要化为程序员提供了软件文档的基础。代码的简短描述可以帮助程序员快速理解程序，而不必阅读代码本身。近年来，神经源代码摘要化已经成为自动化代码摘要化技术研究的前沿。到目前为止，最受欢迎的摘要化目标是程序子程序。简而言之，就是使用从代码仓库中提取的大量子程序示例来训练编码器-解码器神经架构。编码器表示代码，解码器表示摘要。然而，大多数当前方法尝试将子程序视为单个单位。例如，通过将整个子程序作为输入传递给Transformer或基于RNN的编码器。但代码行为往往取决于语句之间的流动。通常可以通过动态分析来进行评估

    Source code summarization is the task of writing natural language descriptions of source code behavior. Code summarization underpins software documentation for programmers. Short descriptions of code help programmers understand the program quickly without having to read the code itself. Lately, neural source code summarization has emerged as the frontier of research into automated code summarization techniques. By far the most popular targets for summarization are program subroutines. The idea, in a nutshell, is to train an encoder-decoder neural architecture using large sets of examples of subroutines extracted from code repositories. The encoder represents the code and the decoder represents the summary. However, most current approaches attempt to treat the subroutine as a single unit. For example, by taking the entire subroutine as input to a Transformer or RNN-based encoder. But code behavior tends to depend on the flow from statement to statement. Normally dynamic analysis may she
    
[^4]: SynerGPT:上下文学习用于个性化药物协同作用预测和药物设计

    SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design. (arXiv:2307.11694v1 [cs.AI])

    [http://arxiv.org/abs/2307.11694](http://arxiv.org/abs/2307.11694)

    本文提出了一种通过上下文学习个性化药物协同作用并进行药物设计的方法，该方法利用小型的个性化数据集，不依赖于文本语料库、分子指纹或蛋白质相互作用的领域特定知识，取得了竞争性的结果。

    

    预测药物的协同组合可以加速癌症治疗的发现，特别是通过活检细胞个性化的治疗。在本文中，我们提出了一种新的设置和模型用于上下文中的药物协同学习。我们给出了一个小的“个性化数据集”，其中包含特定癌症靶细胞上下文中的10-20个药物协同关系。我们的目标是预测该上下文中的额外药物协同关系。受最近工作的启发，该工作通过预训练GPT语言模型（LM）来“上下文学习”常见的功能类。我们设计了一种 新的预训练方案，使GPT模型能够上下文学习“药物协同功能”。我们的模型 - 不使用任何文本语料库，分子指纹，蛋白质相互作用或任何其他领域特定的知识 - 能够取得竞争性的结果。我们进一步将我们的上下文方法与遗传算法结合起来，以优化模型提示并选择协同候选项。

    Predicting synergistic drug combinations can help accelerate discovery of cancer treatments, particularly therapies personalized to a patient's specific tumor via biopsied cells. In this paper, we propose a novel setting and models for in-context drug synergy learning. We are given a small "personalized dataset" of 10-20 drug synergy relationships in the context of specific cancer cell targets. Our goal is to predict additional drug synergy relationships in that context. Inspired by recent work that pre-trains a GPT language model (LM) to "in-context learn" common function classes, we devise novel pre-training schemes that enable a GPT model to in-context learn "drug synergy functions". Our model -- which does not use any textual corpora, molecular fingerprints, protein interaction or any other domain-specific knowledge -- is able to achieve competitive results. We further integrate our in-context approach with a genetic algorithm to optimize model prompts and select synergy candidates
    
[^5]: 可解释的图网络用于形成通用代数猜想

    Interpretable Graph Networks Formulate Universal Algebra Conjectures. (arXiv:2307.11688v1 [cs.LG])

    [http://arxiv.org/abs/2307.11688](http://arxiv.org/abs/2307.11688)

    提出了首次使用人工智能来研究通用代数的猜想，并引入了可解释的图网络以增强可解释性。

    

    最近人工智能的崛起使得研究人员能够探究几十年来传统方法无法解决的困难数学问题。然而，人工智能在通用代数领域中的应用仍然完全未被探索。本研究提出了首次使用人工智能来研究通用代数的猜想，结合等式和拓扑特征进行建模。虽然拓扑表示法可以使用图神经网络来分析这些属性，但这些模型的透明性和可解释性有限，限制了它们直接用于验证现有猜想或提出新猜想的能力。为了弥补这些差距，我们提出了一个通用算法，基于通用代数的猜想生成可用于人工智能的数据集，并引入了一种新颖的神经层来构建完全可解释的图网络。我们的实验结果表明，可解释的图网络能够增强...

    The rise of Artificial Intelligence (AI) recently empowered researchers to investigate hard mathematical problems which eluded traditional approaches for decades. Yet, the use of AI in Universal Algebra (UA) -- one of the fields laying the foundations of modern mathematics -- is still completely unexplored. This work proposes the first use of AI to investigate UA's conjectures with an equivalent equational and topological characterization. While topological representations would enable the analysis of such properties using graph neural networks, the limited transparency and brittle explainability of these models hinder their straightforward use to empirically validate existing conjectures or to formulate new ones. To bridge these gaps, we propose a general algorithm generating AI-ready datasets based on UA's conjectures, and introduce a novel neural layer to build fully interpretable graph networks. The results of our experiments demonstrate that interpretable graph networks: (i) enhan
    
[^6]: 用GPT-4增强CLIP：利用视觉描述作为提示

    Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. (arXiv:2307.11661v1 [cs.CV])

    [http://arxiv.org/abs/2307.11661](http://arxiv.org/abs/2307.11661)

    本文展示了如何利用GPT-4生成具有视觉描述性的文本，并将其用于适应CLIP到下游任务。与CLIP的默认提示相比，在专门细粒度数据集上显示了较大的0-shot迁移准确性改进。

    

    对比预训练的大型视觉-语言模型（VLMs）如CLIP在下游数据集上提供了良好性能，从而革新了视觉表示学习。VLMs通过设计与数据集相关的提示来0-shot适应下游数据集。这种提示工程利用了领域专业知识和验证数据集。同时，像GPT-4这样的生成预训练模型的最新发展意味着它们可以用作先进的互联网搜索工具。它们还可以被操作以提供任何结构化的视觉信息。在这项工作中，我们展示了如何利用GPT-4生成具有视觉描述性的文本，并将其用于适应CLIP到下游任务。与CLIP的默认提示相比，我们在专门细粒度数据集（如EuroSAT（~7％）、DTD（~7％）、SUN397（~4.6％）和CUB（~3.3％））上显示出了较大的0-shot迁移准确性改进。我们还设计了一个简单的少量样本适配器，它可以学习选择最佳的s

    Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt. We also design a simple few-shot adapter that learns to choose the best possible s
    
[^7]: 具有确定性演化状态的强盗模型

    Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])

    [http://arxiv.org/abs/2307.11655](http://arxiv.org/abs/2307.11655)

    该论文提出了一种名为具有确定性演化状态的强盗模型，用于学习带有强盗反馈的推荐系统和在线广告。该模型考虑了状态演化的不同速率，能准确评估奖励与系统健康程度之间的关系。

    

    我们提出了一种学习与强盗反馈结合的模型，同时考虑到确定性演化和不可观测的状态，我们称之为具有确定性演化状态的强盗模型。我们的模型主要应用于推荐系统和在线广告的学习。在这两种情况下，算法在每一轮获得的奖励是选择行动的短期奖励和系统的“健康”程度（即通过其状态测量）的函数。例如，在推荐系统中，平台从用户对特定类型内容的参与中获得的奖励不仅取决于具体内容的固有特征，还取决于用户与平台上其他类型内容互动后其偏好的演化。我们的通用模型考虑了状态演化的不同速率λ∈[0,1]（例如，用户的偏好因先前内容消费而快速变化）。

    We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption
    
[^8]: 基于形态学图像分析和特征提取的AI缺陷检测和分类模型推理

    Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models. (arXiv:2307.11643v1 [cs.CV])

    [http://arxiv.org/abs/2307.11643](http://arxiv.org/abs/2307.11643)

    本文提出了一个名为AI推理器的解释性模型，能够从图像中提取缺陷的形态学特征，并利用决策树进行推理。它通过可视化和文字说明解释基于掩模的缺陷检测和分类模型的输出，并提供有效的缓解策略以提升整体模型性能。

    

    随着人工智能模型在工程和制造等行业的使用越来越普遍，这些模型提供透明的推理以解释其预测结果变得至关重要。本文提出了AI推理器，该推理器从图像中提取缺陷的形态学特征，并利用决策树对缺陷特征进行推理。然后，AI推理器通过可视化图表和文字说明提供对基于掩模的缺陷检测和分类模型的输出进行解释。它还提供了有效的缓解策略，以提升数据预处理和整体模型性能。AI推理器在使用366张含有缺陷的图像集合上测试了解释IE Mask R-CNN模型输出的能力。结果表明，AI推理器在解释IE Mask R-CNN模型的预测方面具有很高的效果。总而言之，所提出的AI推理器为改善模型性能提供了一个解决方案。

    As the use of artificial intelligent (AI) models becomes more prevalent in industries such as engineering and manufacturing, it is essential that these models provide transparent reasoning behind their predictions. This paper proposes the AI-Reasoner, which extracts the morphological characteristics of defects (DefChars) from images and utilises decision trees to reason with the DefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e. charts) and textual explanations to provide insights into outputs made by masked-based defect detection and classification models. It also provides effective mitigation strategies to enhance data pre-processing and overall model performance. The AI-Reasoner was tested on explaining the outputs of an IE Mask R-CNN model using a set of 366 images containing defects. The results demonstrated its effectiveness in explaining the IE Mask R-CNN model's predictions. Overall, the proposed AI-Reasoner provides a solution for improving the performanc
    
[^9]: 将领域专家中心本体设计集成到面向物理生产系统的CRISP-DM中

    Integration of Domain Expert-Centric Ontology Design into the CRISP-DM for Cyber-Physical Production Systems. (arXiv:2307.11637v1 [cs.AI])

    [http://arxiv.org/abs/2307.11637](http://arxiv.org/abs/2307.11637)

    本文提出了一种将领域专家中心本体设计集成到CRISP-DM中的方法，以解决公司在应用数据驱动项目时所遇到的挑战。

    

    在工业4.0和物理生产系统领域，大量有潜在价值的数据被生成。机器学习和数据挖掘方法已经被证明在从收集的数据中提取复杂和隐含的模式方面是有希望的。所得到的知识可以用于改进诸如诊断或维护计划等任务。然而，这样的数据驱动项目通常需要大量的时间来理解和准备数据，因此常常导致失败。在处理工业4.0环境中的挑战时，领域特定本体的应用已经证明了其优越性。然而，针对物理生产系统的本体设计工作流程和工件尚未系统地集成到CRISP-DM中。因此，本文旨在提出一种综合方法来充分利用领域专家中心本体设计在CRISP-DM中的优势。

    In the age of Industry 4.0 and Cyber-Physical Production Systems (CPPSs) vast amounts of potentially valuable data are being generated. Methods from Machine Learning (ML) and Data Mining (DM) have proven to be promising in extracting complex and hidden patterns from the data collected. The knowledge obtained can in turn be used to improve tasks like diagnostics or maintenance planning. However, such data-driven projects, usually performed with the Cross-Industry Standard Process for Data Mining (CRISP-DM), often fail due to the disproportionate amount of time needed for understanding and preparing the data. The application of domain-specific ontologies has demonstrated its advantageousness in a wide variety of Industry 4.0 application scenarios regarding the aforementioned challenges. However, workflows and artifacts from ontology design for CPPSs have not yet been systematically integrated into the CRISP-DM. Accordingly, this contribution intends to present an integrated approach so t
    
[^10]: 关于双分区极化问题复杂性的研究：从中立到高度极化的讨论

    On the Complexity of the Bipartite Polarization Problem: from Neutral to Highly Polarized Discussions. (arXiv:2307.11621v1 [cs.AI])

    [http://arxiv.org/abs/2307.11621](http://arxiv.org/abs/2307.11621)

    本论文研究了双分区极化问题的复杂性，并提出了一个实例生成模型，通过控制实例的极化程度与解决这些实例的复杂度之间的关系。研究表明，实例的极化程度越高，解决问题越容易。

    

    双分区极化问题是一个优化问题，目标是在表示通过社交网络发展起来的辩论的带权重和标签的图上找到最高极化的双分区。图中的节点代表用户的观点，边代表用户之间的一致或不一致。这个问题可以看作是最大割问题的一种推广，在之前的工作中已经找到了一些近似解和精确解，这些解是从Reddit的讨论中得到的，这表明这些实例似乎很容易解决。在本文中，我们通过引入一个实例生成模型来进一步研究该问题的复杂性，该模型的一个参数控制了实例的极化程度，而这与解决这些实例的平均复杂度有关。我们得到的平均复杂度的结果与我们的假设一致：实例的极化程度越高，找到对应的解越容易。

    The Bipartite Polarization Problem is an optimization problem where the goal is to find the highest polarized bipartition on a weighted and labelled graph that represents a debate developed through some social network, where nodes represent user's opinions and edges agreement or disagreement between users. This problem can be seen as a generalization of the maxcut problem, and in previous work approximate solutions and exact solutions have been obtained for real instances obtained from Reddit discussions, showing that such real instances seem to be very easy to solve. In this paper, we investigate further the complexity of this problem, by introducing an instance generation model where a single parameter controls the polarization of the instances in such a way that this correlates with the average complexity to solve those instances. The average complexity results we obtain are consistent with our hypothesis: the higher the polarization of the instance, the easier is to find the corres
    
[^11]: CausE: 朝向因果知识图谱嵌入的方向

    CausE: Towards Causal Knowledge Graph Embedding. (arXiv:2307.11610v1 [cs.CL])

    [http://arxiv.org/abs/2307.11610](http://arxiv.org/abs/2307.11610)

    CausE是一个采用因果知识图谱嵌入和嵌入解缠的框架，利用因果干预进行稳定预测，并在知识图谱完整性任务上取得了最先进的性能。

    

    知识图谱嵌入（KGE）的重点是将知识图谱（KG）中的实体和关系表示为连续的向量空间，这可以用于预测缺失的三元组以实现知识图谱完整性（KGC）。然而，KGE模型通常只是简单地学习三元组数据的结构关联，并且在现实世界的KG中，嵌入可能会被微不足道的模式和噪声链接所误导。为了解决这个问题，我们在因果性和嵌入解缠方面建立了KGE的新模式。我们进一步提出了Causality-enhanced knowledge graph Embedding（CausE）框架。CausE使用因果干预来估计混杂嵌入的因果效应，并设计新的训练目标来进行稳定预测。实验结果表明，CausE可以优于基线模型，并实现最先进的KGC性能。我们在https://github.com/zjukg/CausE上发布了我们的代码。

    Knowledge graph embedding (KGE) focuses on representing the entities and relations of a knowledge graph (KG) into the continuous vector spaces, which can be employed to predict the missing triples to achieve knowledge graph completion (KGC). However, KGE models often only briefly learn structural correlations of triple data and embeddings would be misled by the trivial patterns and noisy links in real-world KGs. To address this issue, we build the new paradigm of KGE in the context of causality and embedding disentanglement. We further propose a Causality-enhanced knowledge graph Embedding (CausE) framework. CausE employs causal intervention to estimate the causal effect of the confounder embeddings and design new training objectives to make stable predictions. Experimental results demonstrate that CausE could outperform the baseline models and achieve state-of-the-art KGC performance. We release our code in https://github.com/zjukg/CausE.
    
[^12]: 深度神经网络的特征图测试

    Feature Map Testing for Deep Neural Networks. (arXiv:2307.11563v1 [cs.SE])

    [http://arxiv.org/abs/2307.11563](http://arxiv.org/abs/2307.11563)

    本研究提出了DeepFeature方法，从特征图层面对深度神经网络进行测试，解决了当前测试方法主要关注神经元而忽略特征图的问题。

    

    由于深度神经网络在安全关键任务中的广泛应用，深度学习测试引起了越来越多的关注。在测试过程中，通过模糊测试或使用测试指标选择的测试用例被输入模型中，以找到导致故障的测试单元（如神经元和特征图，激活它们几乎肯定会导致模型错误），并将其报告给DNN开发者，随后开发者重新修复它们（例如，使用测试用例重新训练模型）。然而，目前的测试指标主要关注神经元，这意味着通过引导模糊测试或使用这些指标选择发现的测试用例主要集中于检测导致故障的神经元，而忽略了导致故障的特征图。在这项工作中，我们提出了DeepFeature，它从特征图层面对DNN进行测试。当进行测试时，DeepFeature将仔细检查模型中的每个内部特征图，并识别可能存在的漏洞。

    Due to the widespread application of deep neural networks~(DNNs) in safety-critical tasks, deep learning testing has drawn increasing attention. During the testing process, test cases that have been fuzzed or selected using test metrics are fed into the model to find fault-inducing test units (e.g., neurons and feature maps, activating which will almost certainly result in a model error) and report them to the DNN developer, who subsequently repair them~(e.g., retraining the model with test cases). Current test metrics, however, are primarily concerned with the neurons, which means that test cases that are discovered either by guided fuzzing or selection with these metrics focus on detecting fault-inducing neurons while failing to detect fault-inducing feature maps.  In this work, we propose DeepFeature, which tests DNNs from the feature map level. When testing is conducted, DeepFeature will scrutinize every internal feature map in the model and identify vulnerabilities that can be enh
    
[^13]: CycleIK：脑神经启发的逆运动学

    CycleIK: Neuro-inspired Inverse Kinematics. (arXiv:2307.11554v1 [cs.RO])

    [http://arxiv.org/abs/2307.11554](http://arxiv.org/abs/2307.11554)

    本文介绍了CycleIK，一种神经机器人逆运动学方法，它包括生成对抗网络（GAN）和多层感知器（MLP）架构。通过嵌入混合神经遗传逆运动学流水线并使用稠密数据集进行训练和测试，我们展示了该方法在逆运动学上的优越性能。这使得CycleIK可以直接部署到机器人硬件上。

    

    本文介绍了CycleIK，一种神经机器人逆运动学方法，该方法包括生成对抗网络（GAN）和多层感知器（MLP）架构两种新颖的神经启发方法。这些方法可以单独应用，但我们还展示了如何将它们嵌入混合神经遗传逆运动学流水线，通过顺序最小二乘规划（SLSQP）或遗传算法（GA）进行进一步优化。我们使用从新型脑神经启发协作机器人（NICOL）的随机机器人配置收集的稠密数据集来训练和测试模型，该机器人具有两个冗余的8自由度操作器。我们利用最先进的BioIK方法中的加权多目标函数来支持训练过程和混合神经遗传架构。我们验证了神经模型可以与最先进的逆运动学方法竞争，从而可以直接部署到机器人硬件上。

    The paper introduces CycleIK, a neuro-robotic approach that wraps two novel neuro-inspired methods for the inverse kinematics (IK) task, a Generative Adversarial Network (GAN), and a Multi-Layer Perceptron architecture. These methods can be used in a standalone fashion, but we also show how embedding these into a hybrid neuro-genetic IK pipeline allows for further optimization via sequential least-squares programming (SLSQP) or a genetic algorithm (GA). The models are trained and tested on dense datasets that were collected from random robot configurations of the new Neuro-Inspired COLlaborator (NICOL), a semi-humanoid robot with two redundant 8-DoF manipulators. We utilize the weighted multi-objective function from the state-of-the-art BioIK method to support the training process and our hybrid neuro-genetic architecture. We show that the neural models can compete with state-of-the-art IK approaches, which allows for deployment directly to robotic hardware. Additionally, it is shown t
    
[^14]: CSE-CIC-IDS2018数据集中相关特征的识别用于入侵检测系统的开发

    Identifying Relevant Features of CSE-CIC-IDS2018 Dataset for the Development of an Intrusion Detection System. (arXiv:2307.11544v1 [cs.AI])

    [http://arxiv.org/abs/2307.11544](http://arxiv.org/abs/2307.11544)

    本文研究了CSE-CIC-IDS2018数据集中相关特征的识别，重点关注入侵检测系统的开发。通过应用特征选择方法和分类算法，确定了最终的特征子集，并评估了其效果。

    

    入侵检测系统（IDS）是IT系统的必要组成部分。其关键组件是一个分类模块，不断评估网络流量的特征并识别可能的威胁。其效率很大程度上取决于正确选择要监控的特征。因此，在开发IDS的过程中，确定一个最小的特征集合来安全区分恶意流量和良性流量是必不可少的。本文介绍了在CSE-CIC-IDS2018 on AWS数据集上的预处理和特征选择工作流程及其结果，重点关注五种攻击类型。为了识别相关特征，应用了六种特征选择方法，并基于它们的平均得分制定了最终的特征排序。接下来，基于不同的排序阈值值，形成了几个特征子集，并使用五种分类算法进行尝试。

    Intrusion detection systems (IDSs) are essential elements of IT systems. Their key component is a classification module that continuously evaluates some features of the network traffic and identifies possible threats. Its efficiency is greatly affected by the right selection of the features to be monitored. Therefore, the identification of a minimal set of features that are necessary to safely distinguish malicious traffic from benign traffic is indispensable in the course of the development of an IDS. This paper presents the preprocessing and feature selection workflow as well as its results in the case of the CSE-CIC-IDS2018 on AWS dataset, focusing on five attack types. To identify the relevant features, six feature selection methods were applied, and the final ranking of the features was elaborated based on their average score. Next, several subsets of the features were formed based on different ranking threshold values, and each subset was tried with five classification algorithms
    
[^15]: 可证明人工智能模型报告：将欧盟法规融入人工智能开发的建议

    Model Reporting for Certifiable AI: A Proposal from Merging EU Regulation into AI Development. (arXiv:2307.11525v1 [cs.AI])

    [http://arxiv.org/abs/2307.11525](http://arxiv.org/abs/2307.11525)

    提出了结合欧盟法规与最新研究趋势的可证明人工智能模型报告的建议，通过使用标准化的卡片记录AI应用的开发过程，并引入用例和操作卡以满足监管要求，旨在开发安全可信的AI系统并支持第三方审计。

    

    尽管在可解释和安全的人工智能方面取得了很大进展，从业者仍然面临缺乏人工智能安全法规和标准的问题。在这项工作中，我们将欧盟最新的法规努力与最新的人工智能指南提出的建议与研究的最新趋势相结合：数据卡和模型卡。我们建议使用标准化的卡片来记录人工智能应用程序的开发过程。我们的主要贡献是引入了用例和操作卡，以及对数据和模型卡的更新以满足监管要求。我们在卡片中引用了最新的研究以及法规的来源，并尽可能提供了额外支持材料和工具箱的参考。我们的目标是设计出能够帮助从业者在开发过程中开发安全的人工智能系统的卡片，同时使第三方审计人工智能应用程序更加高效，易于理解，并建立对系统的信任。我们的工作结合了来自…（部分内容省略）

    Despite large progress in Explainable and Safe AI, practitioners suffer from a lack of regulation and standards for AI safety. In this work we merge recent regulation efforts by the European Union and first proposals for AI guidelines with recent trends in research: data and model cards. We propose the use of standardized cards to document AI applications throughout the development process. Our main contribution is the introduction of use-case and operation cards, along with updates for data and model cards to cope with regulatory requirements. We reference both recent research as well as the source of the regulation in our cards and provide references to additional support material and toolboxes whenever possible. The goal is to design cards that help practitioners develop safe AI systems throughout the development process, while enabling efficient third-party auditing of AI applications, being easy to understand, and building trust in the system. Our work incorporates insights from i
    
[^16]: 使用机器学习进行多模态仇恨言论检测

    Multi-modal Hate Speech Detection using Machine Learning. (arXiv:2307.11519v1 [cs.AI])

    [http://arxiv.org/abs/2307.11519](http://arxiv.org/abs/2307.11519)

    本研究提出了一种使用多模态系统的方法，通过提取图像特征、音频特征值和文本，结合机器学习和自然语言处理技术，来检测视频内容中的仇恨言论。

    

    随着互联网用户和媒体内容的不断增长，追踪音频和视频中的仇恨言论变得非常困难。将视频或音频转换为文本并不能准确检测到仇恨言论，因为人们有时会将仇恨词汇作为幽默或愉快的意味使用，并在视频中使用不同的声调或展示不同的动作。当前最先进的仇恨言论检测模型大多是基于单一模态的。本研究提出了一种多模态系统的综合方法，通过提取图像特征、音频中的特征值、文本和使用机器学习和自然语言处理来检测视频内容中的仇恨言论。

    With the continuous growth of internet users and media content, it is very hard to track down hateful speech in audio and video. Converting video or audio into text does not detect hate speech accurately as human sometimes uses hateful words as humorous or pleasant in sense and also uses different voice tones or show different action in the video. The state-ofthe-art hate speech detection models were mostly developed on a single modality. In this research, a combined approach of multimodal system has been proposed to detect hate speech from video contents by extracting feature images, feature values extracted from the audio, text and used machine learning and Natural language processing.
    
[^17]: IndigoVX: 人类智慧与人工智能相结合的最优决策方法

    IndigoVX: Where Human Intelligence Meets AI for Optimal Decision Making. (arXiv:2307.11516v1 [cs.AI])

    [http://arxiv.org/abs/2307.11516](http://arxiv.org/abs/2307.11516)

    IndigoVX是一种将人类智慧与人工智能相结合的最优决策方法，通过迭代反馈循环，利用人类专家的背景知识和人工智能的数据驱动洞见，制定和优化朝着明确定义的目标的策略，并使用定量化的三分数模式进行评估和改进。

    

    本文定义了一种新的方法，通过将人类智慧与人工智能相结合，实现最优目标解决。我们提出的人工智能系统，Indigo，是指通过迭代目标导向优化进行知情数值决策。当与人类协作者结合时，我们将这个联合系统命名为IndigoVX，即虚拟专家。该系统概念简单。我们设想该方法可以应用于游戏或商业策略，人类提供战略背景，而人工智能提供最佳的数据驱动移动。Indigo通过迭代反馈循环运作，利用人类专家的背景知识和人工智能的数据驱动洞见，制定和优化朝着明确定义的目标的策略。使用定量化的三分数模式，这种混合化允许联合团队评估策略并改进计划，同时实时适应挑战和变化。

    This paper defines a new approach for augmenting human intelligence with AI for optimal goal solving. Our proposed AI, Indigo, is an acronym for Informed Numerical Decision-making through Iterative Goal-Oriented optimization. When combined with a human collaborator, we term the joint system IndigoVX, for Virtual eXpert. The system is conceptually simple. We envisage this method being applied to games or business strategies, with the human providing strategic context and the AI offering optimal, data-driven moves. Indigo operates through an iterative feedback loop, harnessing the human expert's contextual knowledge and the AI's data-driven insights to craft and refine strategies towards a well-defined goal. Using a quantified three-score schema, this hybridization allows the combined team to evaluate strategies and refine their plan, while adapting to challenges and changes in real-time.
    
[^18]: 在协变量偏移自适应中的一般正则化方法

    General regularization in covariate shift adaptation. (arXiv:2307.11503v1 [cs.LG])

    [http://arxiv.org/abs/2307.11503](http://arxiv.org/abs/2307.11503)

    本文研究了协变量偏移自适应中的一般正则化方法，并通过组合已有结果得到了新的结果。在弱平滑条件下证明了实现与标准监督学习中相同精度所需的样本量要比现有分析证明的少。

    

    样本重加权是纠正在再生核希尔伯特空间(RKHS)中由未来数据分布与训练数据分布不同引起的最小二乘学习算法错误的最常用方法之一。在实际情况中，样本权重是由未来数据分布对训练数据分布的估计Radon-Nikod\'ym导数的值确定的。本研究回顾了在RKHS中重新加权核回归的已知误差界限，并通过组合得到新的结果。我们在弱平滑条件下表明，为了实现与标准监督学习中数据分布差异相同精度的样本数目要比现有的分析证明的少。

    Sample reweighting is one of the most widely used methods for correcting the error of least squares learning algorithms in reproducing kernel Hilbert spaces (RKHS), that is caused by future data distributions that are different from the training data distribution. In practical situations, the sample weights are determined by values of the estimated Radon-Nikod\'ym derivative, of the future data distribution w.r.t.~the training data distribution. In this work, we review known error bounds for reweighted kernel regression in RKHS and obtain, by combination, novel results. We show under weak smoothness conditions, that the amount of samples, needed to achieve the same order of accuracy as in the standard supervised learning without differences in data distributions, is smaller than proven by state-of-the-art analyses.
    
[^19]: 适应资源受限的物联网系统中的分布式推理的自适应ResNet架构

    Adaptive ResNet Architecture for Distributed Inference in Resource-Constrained IoT Systems. (arXiv:2307.11499v1 [cs.AI])

    [http://arxiv.org/abs/2307.11499](http://arxiv.org/abs/2307.11499)

    这项研究提出了一种自适应ResNet架构，用于在资源受限的物联网系统中进行分布式推理。通过识别可删除但不会显著影响性能的连接，实现对模型的适应性缩减。该架构能够降低延迟和能耗，并提高系统的鲁棒性。

    

    随着深度神经网络的不断扩展和复杂化，大多数边缘设备无法处理其庞大的处理需求。因此，分布式推理的概念对于在节点集群中分配神经网络是必要的。然而，分布可能导致额外的能耗和设备之间的依赖，这些设备往往受到不稳定的传输速率的影响。不稳定的传输速率会影响物联网设备的实时性能，导致低延迟、高能耗和潜在的故障。因此，对于动态系统来说，必须有一个具有自适应架构的弹性DNN，可以根据可用资源调整大小。本文提出了一项实证研究，识别了可以在ResNet中删除而不会显著影响模型性能的连接，以在资源短缺的情况下实现分布。基于结果，提出了一个多目标优化问题，旨在最小化延迟和...

    As deep neural networks continue to expand and become more complex, most edge devices are unable to handle their extensive processing requirements. Therefore, the concept of distributed inference is essential to distribute the neural network among a cluster of nodes. However, distribution may lead to additional energy consumption and dependency among devices that suffer from unstable transmission rates. Unstable transmission rates harm real-time performance of IoT devices causing low latency, high energy usage, and potential failures. Hence, for dynamic systems, it is necessary to have a resilient DNN with an adaptive architecture that can downsize as per the available resources. This paper presents an empirical study that identifies the connections in ResNet that can be dropped without significantly impacting the model's performance to enable distribution in case of resource shortage. Based on the results, a multi-objective optimization problem is formulated to minimize latency and ma
    
[^20]: 预测、改进、合成：面向概率时间序列预测的自引导扩散模型

    Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. (arXiv:2307.11494v1 [cs.LG])

    [http://arxiv.org/abs/2307.11494](http://arxiv.org/abs/2307.11494)

    本研究提出了一种面向概率时间序列预测的自引导扩散模型，称为TSDiff。该模型不需要辅助网络或训练过程的改变，在预测、改进和合成数据生成等时间序列任务上展现出了竞争力。

    

    扩散模型在各个领域的生成建模任务中取得了最先进的性能。之前关于时间序列扩散模型的研究主要集中在开发针对特定预测或填补任务的条件模型。在这项工作中，我们探索了面向多种时间序列应用的任务不可知条件下的扩散模型的潜力。我们提出了TSDiff，一种面向时间序列的无条件训练的扩散模型。我们的自引导机制在推理过程中使得TSDiff能够为下游任务进行条件设置，而无需辅助网络或改变训练过程。我们在三个不同的时间序列任务上展示了我们方法的有效性：预测、改进和合成数据生成。首先，我们表明TSDiff与几种任务特定的条件预测方法相竞争（预测）。其次，我们利用TSDiff学到的隐性概率密度来迭代地改进p

    Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the p
    
[^21]: 使用网络收集的图像进行知识蒸馏中的分布偏移问题

    Distribution Shift Matters for Knowledge Distillation with Webly Collected Images. (arXiv:2307.11469v1 [cs.CV])

    [http://arxiv.org/abs/2307.11469](http://arxiv.org/abs/2307.11469)

    本研究提出了一种解决知识蒸馏中分布偏移问题的新方法，通过动态选择有用的训练样本并对特征和分类进行对齐，提高了训练出的学生网络的可靠性。

    

    知识蒸馏旨在从预先训练好的教师网络学习一个轻量级的学生网络。在实践中，现有的知识蒸馏方法通常在原始训练数据由于隐私问题和数据管理考虑不可用时变得不可行。因此，无数据知识蒸馏方法通过从互联网收集训练样本。然而，其中大部分忽视了原始训练数据和网络收集数据之间的常见分布偏移，影响了训练好的学生网络的可靠性。为了解决这个问题，我们提出了一种名为“不同分布下的知识蒸馏”（KD$^{3}$）的新方法，它由三个组成部分组成。具体而言，我们首先根据教师网络和学生网络的综合预测动态选择有用的训练样本。随后，我们对加权特征和分类进行对齐。

    Knowledge distillation aims to learn a lightweight student network from a pre-trained teacher network. In practice, existing knowledge distillation methods are usually infeasible when the original training data is unavailable due to some privacy issues and data management considerations. Therefore, data-free knowledge distillation approaches proposed to collect training instances from the Internet. However, most of them have ignored the common distribution shift between the instances from original training data and webly collected data, affecting the reliability of the trained student network. To solve this problem, we propose a novel method dubbed ``Knowledge Distillation between Different Distributions" (KD$^{3}$), which consists of three components. Specifically, we first dynamically select useful training instances from the webly collected data according to the combined predictions of teacher network and student network. Subsequently, we align both the weighted features and classif
    
[^22]: 在6G网络中实现零接触的普适人工智能即服务

    Zero-touch realization of Pervasive Artificial Intelligence-as-a-service in 6G networks. (arXiv:2307.11468v1 [cs.AI])

    [http://arxiv.org/abs/2307.11468](http://arxiv.org/abs/2307.11468)

    在6G网络中，本文介绍了一种新颖的平台架构，通过使用基于区块链的智能系统支持部署零接触的PAI即服务，实现了普适人工智能服务的系统化分发。

    

    未来6G技术的愿景是通过超密集网络、低延迟和快速数据传输来支持普适人工智能（PAI），并使用零接触解决方案实现自我X（例如，自配置、自监测和自愈）服务。然而，对于6G的研究仍处于初级阶段，目前只完成了对其设计的概念化、实施的调查和用例的规划的第一步。为此，学术界和工业界逐渐从AI分发的理论研究转向实际部署和标准化。然而，设计一种端到端的框架来通过允许第三方应用程序辅助的零接触服务配置更容易访问AI服务的分发尚未得到很好的研究。在这种背景下，我们引入了一种新颖的平台架构，在6G网络中使用基于区块链的智能系统支持部署零接触的PAI即服务。

    The vision of the upcoming 6G technologies, characterized by ultra-dense network, low latency, and fast data rate is to support Pervasive AI (PAI) using zero-touch solutions enabling self-X (e.g., self-configuration, self-monitoring, and self-healing) services. However, the research on 6G is still in its infancy, and only the first steps have been taken to conceptualize its design, investigate its implementation, and plan for use cases. Toward this end, academia and industry communities have gradually shifted from theoretical studies of AI distribution to real-world deployment and standardization. Still, designing an end-to-end framework that systematizes the AI distribution by allowing easier access to the service using a third-party application assisted by a zero-touch service provisioning has not been well explored. In this context, we introduce a novel platform architecture to deploy a zero-touch PAI-as-a-Service (PAIaaS) in 6G networks supported by a blockchain-based smart system.
    
[^23]: 通过时间重新缩放错误以改善长期记忆学习

    Improve Long-term Memory Learning Through Rescaling the Error Temporally. (arXiv:2307.11462v1 [cs.LG])

    [http://arxiv.org/abs/2307.11462](http://arxiv.org/abs/2307.11462)

    本研究通过时间上重新缩放错误度量，改善了长期记忆学习的偏向于短期记忆的问题，并缓解了梯度消失的困扰。数值实验验证了适当的时间上重新缩放错误在有效的长期记忆学习中的重要性。

    

    本文研究了序列建模中长期记忆学习的错误度量选择。我们检查了常用错误度量（包括平均绝对/平方误差）对短期记忆的偏向。我们的发现表明，在学习线性函数时，所有时间上有正权重的错误都偏向于短期记忆。为了减少这种偏差并改善长期记忆学习，我们提出了使用时间上重新缩放的错误。除了减少对短期记忆的偏向外，这种方法还可以缓解梯度消失问题。我们在不同的长期记忆任务和序列模型上进行数值实验，以验证我们的假设。数值结果确认了适当的时间上重新缩放误差对于有效的长期记忆学习的重要性。据我们所知，这是第一篇定量分析序列建模中不同错误对短期记忆偏向的工作。

    This paper studies the error metric selection for long-term memory learning in sequence modelling. We examine the bias towards short-term memory in commonly used errors, including mean absolute/squared error. Our findings show that all temporally positive-weighted errors are biased towards short-term memory in learning linear functionals. To reduce this bias and improve long-term memory learning, we propose the use of a temporally rescaled error. In addition to reducing the bias towards short-term memory, this approach can also alleviate the vanishing gradient issue. We conduct numerical experiments on different long-memory tasks and sequence models to validate our claims. Numerical results confirm the importance of appropriate temporally rescaled error for effective long-term memory learning. To the best of our knowledge, this is the first work that quantitatively analyzes different errors' memory bias towards short-term memory in sequence modelling.
    
[^24]: 将人类翻译风格融入英土文学机器翻译

    Incorporating Human Translator Style into English-Turkish Literary Machine Translation. (arXiv:2307.11457v1 [cs.CL])

    [http://arxiv.org/abs/2307.11457](http://arxiv.org/abs/2307.11457)

    本论文研究了英土文学翻译，在机器翻译模型中采用翻译家的风格特征，并通过微调预训练模型获得了高度还原人类译者风格的机器翻译效果。

    

    尽管机器翻译系统主要设计用于一般领域，但存在将这些系统适应其他领域（如文学翻译）的趋势。本文着重研究英土文学翻译，并开发考虑翻译家风格特征的机器翻译模型。我们通过人工对齐的特定译者作品来微调预训练的机器翻译模型。我们对手动对齐、自动对齐、数据增强方法和语料库大小对翻译效果进行了详细分析。我们提出了一种基于风格特征的方法来评估译者在输出翻译中的风格。我们展示了通过将模型适应到译者的风格，可以高度重现人类译者的风格在目标机器翻译中的效果。

    Although machine translation systems are mostly designed to serve in the general domain, there is a growing tendency to adapt these systems to other domains like literary translation. In this paper, we focus on English-Turkish literary translation and develop machine translation models that take into account the stylistic features of translators. We fine-tune a pre-trained machine translation model by the manually-aligned works of a particular translator. We make a detailed analysis of the effects of manual and automatic alignments, data augmentation methods, and corpus size on the translations. We propose an approach based on stylistic features to evaluate the style of a translator in the output translations. We show that the human translator style can be highly recreated in the target machine translations by adapting the models to the style of the translator.
    
[^25]: 提供个性化解释：一种对话式方法

    Providing personalized Explanations: a Conversational Approach. (arXiv:2307.11452v1 [cs.MA])

    [http://arxiv.org/abs/2307.11452](http://arxiv.org/abs/2307.11452)

    本文提出了一种通过连续对话的方式，将个性化解释传达给被解释者的方法，并证明了只要存在一个被解释者理解并且解释者知晓的对于初始主张的解释，对话将因为被解释者对初始主张的证明而终止。

    

    由于利益相关者可能具有各种知识和背景，人工智能系统的应用越来越需要对其行为进行个性化解释。一般来说，解释者和被解释者之间的对话不仅能帮助解释者了解被解释者的背景，还能让被解释者更好地理解解释。在本文中，我们提出了一种通过与被解释者进行连续对话来向其传达个性化解释的方法。我们证明了只要存在一个被解释者理解并且解释者知晓的对于初始主张的解释，那么对话将因为被解释者对初始主张的证明而终止。

    The increasing applications of AI systems require personalized explanations for their behaviors to various stakeholders since the stakeholders may have various knowledge and backgrounds. In general, a conversation between explainers and explainees not only allows explainers to obtain the explainees' background, but also allows explainees to better understand the explanations. In this paper, we propose an approach for an explainer to communicate personalized explanations to an explainee through having consecutive conversations with the explainee. We prove that the conversation terminates due to the explainee's justification of the initial claim as long as there exists an explanation for the initial claim that the explainee understands and the explainer is aware of.
    
[^26]: AIGC赋能电信行业白皮书

    AIGC Empowering Telecom Sector White Paper. (arXiv:2307.11449v1 [cs.AI])

    [http://arxiv.org/abs/2307.11449](http://arxiv.org/abs/2307.11449)

    本文探讨了AIGC（GPT）在电信行业中的应用，提出了一个电信增强认知能力系统，为电信服务的构建提供了解决方案。

    

    在全球GPT热潮中，人们深切意识到作为一项具有变革性的技术和经济社会发展的关键力量的人工智能将给全球产业带来巨大的飞跃和突破，并深刻影响未来的竞争格局。作为信息通信基础设施的建设者和运营商，电信行业为人工智能的发展提供基础支持，甚至在人工智能应用方面处于领先地位。如何实现AIGC（GPT）应用并在电信行业中实施AIGC是电信从业者必须思考和回答的问题。通过研究GPT作为AIGC的典型代表，作者分析了GPT如何通过场景赋能电信行业，讨论了当前GPT通用模型与电信服务之间的差距，并首次提出了一个电信增强认知能力系统，回答了如何构建一个电信服务的问题。

    In the global craze of GPT, people have deeply realized that AI, as a transformative technology and key force in economic and social development, will bring great leaps and breakthroughs to the global industry and profoundly influence the future world competition pattern. As the builder and operator of information and communication infrastructure, the telecom sector provides infrastructure support for the development of AI, and even takes the lead in the implementation of AI applications. How to enable the application of AIGC (GPT) and implement AIGC in the telecom sector are questions that telecom practitioners must ponder and answer. Through the study of GPT, a typical representative of AIGC, the authors have analyzed how GPT empowers the telecom sector in the form of scenarios, discussed the gap between the current GPT general model and telecom services, proposed for the first time a Telco Augmented Cognition capability system, provided answers to how to construct a telecom service 
    
[^27]: 为环保人工智能而批处理 - 探索推理过程的研究

    Batching for Green AI -- An Exploratory Study on Inference. (arXiv:2307.11434v1 [cs.LG])

    [http://arxiv.org/abs/2307.11434](http://arxiv.org/abs/2307.11434)

    这项研究探讨了在深度学习模型推理阶段引入批处理对能源消耗和响应时间的影响，并发现批处理对这两个指标都有显著影响。

    

    在开发新的神经网络时，批大小是一个需要调整的重要参数。除了其他质量指标外，它对模型的准确性、泛化能力、训练时间和并行性具有很大的影响。这个事实是众所周知并被广泛研究的。然而，在深度学习模型应用阶段，当模型被最终用户用于推理时，我们发现对引入批大小的潜在好处存在忽视。在这项研究中，我们考察了输入批处理对于五个在计算机视觉领域被认为是最先进的完全训练的神经网络的能源消耗和响应时间的影响。结果表明，批处理对这两个指标都有显著影响。此外，我们还呈现了过去十年神经网络的能源效率和准确性的时间线。我们发现，总体上，能源消耗在这段时间内明显上升。

    The batch size is an essential parameter to tune during the development of new neural networks. Amongst other quality indicators, it has a large degree of influence on the model's accuracy, generalisability, training times and parallelisability. This fact is generally known and commonly studied. However, during the application phase of a deep learning model, when the model is utilised by an end-user for inference, we find that there is a disregard for the potential benefits of introducing a batch size. In this study, we examine the effect of input batching on the energy consumption and response times of five fully-trained neural networks for computer vision that were considered state-of-the-art at the time of their publication. The results suggest that batching has a significant effect on both of these metrics. Furthermore, we present a timeline of the energy efficiency and accuracy of neural networks over the past decade. We find that in general, energy consumption rises at a much ste
    
[^28]: 采用OpenPose的基于视频的考试可疑活动检测器

    A Video-based Detector for Suspicious Activity in Examination with OpenPose. (arXiv:2307.11413v1 [cs.CV])

    [http://arxiv.org/abs/2307.11413](http://arxiv.org/abs/2307.11413)

    本研究提出了一种利用OpenPose框架和卷积神经网络(CNN)分析考试视频并高效有效地检测可疑活动的框架。

    

    考试是学习过程中至关重要的一部分，学术机构投入大量资源维护其完整性，防止学生或监考员作弊。然而，作弊在考试中变得猖獗，这损害了考试的完整性。依靠监考员监视每个学生的传统方法不切实际且无效。为解决这个问题，需要连续记录考试过程以监控学生的可疑活动。然而，这些录像通常过长以至于监考员无法有效分析，疲劳可能导致他们错过重要细节。为扩大监控范围，监考员可以使用固定架设在高处或佩戴的摄像头。本文介绍了一个利用自动化分析视频并有效高效地检测考试中可疑活动的框架。我们利用OpenPose框架和卷积神经网络(CNN)识别学生交换的

    Examinations are a crucial part of the learning process, and academic institutions invest significant resources into maintaining their integrity by preventing cheating from students or facilitators. However, cheating has become rampant in examination setups, compromising their integrity. The traditional method of relying on invigilators to monitor every student is impractical and ineffective. To address this issue, there is a need to continuously record exam sessions to monitor students for suspicious activities. However, these recordings are often too lengthy for invigilators to analyze effectively, and fatigue may cause them to miss significant details. To widen the coverage, invigilators could use fixed overhead or wearable cameras. This paper introduces a framework that uses automation to analyze videos and detect suspicious activities during examinations efficiently and effectively. We utilized the OpenPose framework and Convolutional Neural Network (CNN) to identify students exch
    
[^29]: 深度直接训练的脉冲神经网络用于目标检测

    Deep Directly-Trained Spiking Neural Networks for Object Detection. (arXiv:2307.11411v1 [cs.CV])

    [http://arxiv.org/abs/2307.11411](http://arxiv.org/abs/2307.11411)

    EMS-YOLO是一种直接训练的脉冲神经网络框架，通过使用替代梯度而不是ANN-SNN转换策略，成功解决了深度SNN的目标检测问题。

    

    脉冲神经网络（SNN）是一种受大脑启发的高能效模型，它通过时空动态来编码信息。最近，直接训练的深度SNN在少数时间步骤上的分类任务中表现出了很高的性能。然而，如何设计一个直接训练的SNN来解决目标检测回归任务仍然是一个具有挑战性的问题。为了解决这个问题，我们提出了EMS-YOLO，一种用于目标检测的新型直接训练的SNN框架，它是第一个使用替代梯度而不是ANN-SNN转换策略来训练深度SNN的尝试。具体地，我们设计了一个全脉冲残差块EMS-ResNet，它可以有效地扩展直接训练的SNN的深度，并且能够具有低功耗。此外，我们在理论上分析并证明了EMS-ResNet可以避免梯度消失或梯度爆炸的问题。实验结果表明，我们的方法优于最先进的ANN-SNN转换方法。

    Spiking neural networks (SNNs) are brain-inspired energy-efficient models that encode information in spatiotemporal dynamics. Recently, deep SNNs trained directly have shown great success in achieving high performance on classification tasks with very few time steps. However, how to design a directly-trained SNN for the regression task of object detection still remains a challenging problem. To address this problem, we propose EMS-YOLO, a novel directly-trained SNN framework for object detection, which is the first trial to train a deep SNN with surrogate gradients for object detection rather than ANN-SNN conversion strategies. Specifically, we design a full-spike residual block, EMS-ResNet, which can effectively extend the depth of the directly-trained SNN with low power consumption. Furthermore, we theoretically analyze and prove the EMS-ResNet could avoid gradient vanishing or exploding. The results demonstrate that our approach outperforms the state-of-the-art ANN-SNN conversion me
    
[^30]: 医学图像分割中的观察者间和观察者内变异的概率建模

    Probabilistic Modeling of Inter- and Intra-observer Variability in Medical Image Segmentation. (arXiv:2307.11397v1 [eess.IV])

    [http://arxiv.org/abs/2307.11397](http://arxiv.org/abs/2307.11397)

    本文提出了一种新颖的概率模型Pionono，用于医学图像分割中的观察者间和观察者内变异。该模型通过捕捉每个标记者的标记行为并将其与图像特征集成，产生概率分割预测。实验证明Pionono在准确性和效率上优于现有模型，并且可以预测多个一致的分割图，为诊断过程提供了宝贵的信息。

    

    医学图像分割是一项具有挑战性的任务，特别是由于观察者间和观察者内的变异性，即使是在医学专家之间也存在。在本文中，我们提出了一种新颖的模型，称为概率观察者间和观察者内变异网络（Pionono）。它通过多维概率分布捕捉每个标记者的标记行为，并将此信息与图像的特征图集成起来，产生概率分割预测。该模型通过变分推断进行优化，并可以端到端地进行训练。它在STAPLE、概率U-Net和基于混淆矩阵的模型等最先进的模型上表现出色。此外，Pionono预测多个一致的分割图，模拟了评分者的专业意见，为诊断过程提供了额外的有价值信息。在真实的癌症分割数据集上的实验证明了Pionono的高准确性和效率，使其成为一种强大的医学工具。

    Medical image segmentation is a challenging task, particularly due to interand intra-observer variability, even between medical experts. In this paper, we propose a novel model, called Probabilistic Inter-Observer and iNtra-Observer variation NetwOrk (Pionono). It captures the labeling behavior of each rater with a multidimensional probability distribution and integrates this information with the feature maps of the image to produce probabilistic segmentation predictions. The model is optimized by variational inference and can be trained end-to-end. It outperforms state-of-the-art models such as STAPLE, Probabilistic U-Net, and models based on confusion matrices. Additionally, Pionono predicts multiple coherent segmentation maps that mimic the rater's expert opinion, which provides additional valuable information for the diagnostic process. Experiments on real-world cancer segmentation datasets demonstrate the high accuracy and efficiency of Pionono, making it a powerful tool for med
    
[^31]: 基于大型语言模型的系统为翻转课堂准备学习提供即时反馈

    Large Language Model-based System to Provide Immediate Feedback to Students in Flipped Classroom Preparation Learning. (arXiv:2307.11388v1 [cs.HC])

    [http://arxiv.org/abs/2307.11388](http://arxiv.org/abs/2307.11388)

    本文提出了基于大型语言模型的系统，旨在为翻转课堂准备学习中的学生提供即时反馈。该系统不仅解决了学生在观看课堂视频时遇到的问题，还通过对回答与上下文的对齐和教师答案的收集，提供了额外的学习指导。

    

    本文提出了一种使用大型语言模型为翻转课堂准备学习中的学生提供即时反馈的系统。该研究旨在解决翻转课堂模式中的挑战，例如确保学生在情感上参与并保持学习积极性。在翻转课堂准备中，学生经常对课堂视频的内容有疑问，但教师很难立即回答这些问题。所提出的系统使用了ChatGPT API，在实际实践中用于准备学习的视频观看支持系统上进行开发。ChatGPT的回答往往与学生问题的上下文不一致。因此，本文还提出了一种将回答与上下文对齐的方法。本文还提出了一种方法来收集教师对学生问题的回答，并将其作为学生的额外指导。本文讨论了所提出系统的设计和实现。

    This paper proposes a system that uses large language models to provide immediate feedback to students in flipped classroom preparation learning. This study aimed to solve challenges in the flipped classroom model, such as ensuring that students are emotionally engaged and motivated to learn. Students often have questions about the content of lecture videos in the preparation of flipped classrooms, but it is difficult for teachers to answer them immediately. The proposed system was developed using the ChatGPT API on a video-watching support system for preparation learning that is being used in real practice. Answers from ChatGPT often do not align with the context of the student's question. Therefore, this paper also proposes a method to align the answer with the context. This paper also proposes a method to collect the teacher's answers to the students' questions and use them as additional guides for the students. This paper discusses the design and implementation of the proposed syst
    
[^32]: 通过Fenchel对偶实现多样的离线模仿

    Diverse Offline Imitation via Fenchel Duality. (arXiv:2307.11373v1 [cs.LG])

    [http://arxiv.org/abs/2307.11373](http://arxiv.org/abs/2307.11373)

    本文提出了一个离线技能发现算法，通过Fenchel对偶方法将强化学习和无监督技能发现结合起来，实现学习与专家相一致的多样的技能。

    

    在无监督技能发现领域，最近取得了显著进展，各种工作提出了以互信息为基础的目标，作为内在驱动。先前的工作主要集中在设计需要在线环境访问的算法。相比之下，我们开发了一个\textit{离线}技能发现算法。我们的问题形式化考虑了在KL-散度约束下最大化互信息目标。更确切地说，约束确保每个技能的状态占用保持在一个具有良好状态操作覆盖率的离线数据集的支持范围内与专家的状态占用逼近。我们的主要贡献是连接Fenchel对偶、强化学习和无监督技能发现，并给出一个简单的离线算法，用于学习与专家相一致的多样的技能。

    There has been significant recent progress in the area of unsupervised skill discovery, with various works proposing mutual information based objectives, as a source of intrinsic motivation. Prior works predominantly focused on designing algorithms that require online access to the environment. In contrast, we develop an \textit{offline} skill discovery algorithm. Our problem formulation considers the maximization of a mutual information objective constrained by a KL-divergence. More precisely, the constraints ensure that the state occupancy of each skill remains close to the state occupancy of an expert, within the support of an offline dataset with good state-action coverage. Our main contribution is to connect Fenchel duality, reinforcement learning and unsupervised skill discovery, and to give a simple offline algorithm for learning diverse skills that are aligned with an expert.
    
[^33]: CohortGPT：一种用于临床研究参与者招募的增强型GPT

    CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study. (arXiv:2307.11346v1 [cs.CL])

    [http://arxiv.org/abs/2307.11346](http://arxiv.org/abs/2307.11346)

    CohortGPT是一种增强型GPT，用于解决临床研究中的参与者招募任务。此研究发现传统的大型语言模型在医疗文本分类中的性能一般，这可能是由于它们忽略了语言中的上下文信息。

    

    基于非结构化医疗文本（如临床记录和放射学报告）进行参与者招募是临床研究中具有挑战性但重要的任务。最近，大型语言模型（LLMs）如ChatGPT在语言理解、推理和生成方面取得了巨大成功。因此，有必要测试它们在解决临床研究中的参与者招募任务中的可行性。然而，当应用于知识密集型问题设置（如医疗文本分类）时，LLMs的性能一般。可能的解释是LLMs仅使用医疗文本，忽略了语言中丰富的上下文信息。

    Participant recruitment based on unstructured medical texts such as clinical notes and radiology reports has been a challenging yet important task for the cohort establishment in clinical research. Recently, Large Language Models (LLMs) such as ChatGPT have achieved tremendous success in various downstream tasks thanks to their promising performance in language understanding, inference, and generation. It is then natural to test their feasibility in solving the cohort recruitment task, which involves the classification of a given paragraph of medical text into disease label(s). However, when applied to knowledge-intensive problem settings such as medical text classification, where the LLMs are expected to understand the decision made by human experts and accurately identify the implied disease labels, the LLMs show a mediocre performance. A possible explanation is that, by only using the medical text, the LLMs neglect to use the rich context of additional information that languages aff
    
[^34]: 对于具有身体的人工智能的通用操作技能的两阶段微调策略

    A Two-stage Fine-tuning Strategy for Generalizable Manipulation Skill of Embodied AI. (arXiv:2307.11343v1 [cs.AI])

    [http://arxiv.org/abs/2307.11343](http://arxiv.org/abs/2307.11343)

    本文提出了一种两阶段微调策略，旨在基于Maniskill2基准进一步提升具有身体的人工智能模型的泛化能力。通过在ManiSkill2挑战赛中取得第一名，我们的研究结果突出了该方法提高泛化能力的潜力。

    

    Chat-GPT的出现引发了对于具有身体的人工智能的兴趣激增。然而，许多现有的具有身体的人工智能模型严重依赖于与训练环境的大量交互，这在实际环境中可能不可行。为此，Maniskill2引入了一个完整的物理模拟基准，用于操作各种3D对象。该基准使得代理可以使用不同的演示数据集进行训练，并评估他们在测试环境中适应未见场景的能力。本文提出了一种新颖的两阶段微调策略，旨在基于Maniskill2基准进一步提升我们模型的泛化能力。通过大量实验证明了我们方法的有效性，并在ManiSkill2挑战赛的三个赛道中获得第一名。我们的研究结果突出了我们方法提高具有身体的人工智能模型泛化能力的潜力，并为它们的实际应用铺平了道路。

    The advent of Chat-GPT has led to a surge of interest in Embodied AI. However, many existing Embodied AI models heavily rely on massive interactions with training environments, which may not be practical in real-world situations. To this end, the Maniskill2 has introduced a full-physics simulation benchmark for manipulating various 3D objects. This benchmark enables agents to be trained using diverse datasets of demonstrations and evaluates their ability to generalize to unseen scenarios in testing environments. In this paper, we propose a novel two-stage fine-tuning strategy that aims to further enhance the generalization capability of our model based on the Maniskill2 benchmark. Through extensive experiments, we demonstrate the effectiveness of our approach by achieving the 1st prize in all three tracks of the ManiSkill2 Challenge. Our findings highlight the potential of our method to improve the generalization abilities of Embodied AI models and pave the way for their ractical appli
    
[^35]: OpenGDA:用于跨网络学习的图领域自适应基准

    OpenGDA: Graph Domain Adaptation Benchmark for Cross-network Learning. (arXiv:2307.11341v1 [cs.AI])

    [http://arxiv.org/abs/2307.11341](http://arxiv.org/abs/2307.11341)

    OpenGDA是一个用于跨网络学习的图领域自适应基准，提供了丰富的数据集和评估方法以全面评估模型在不同任务和场景中的性能。

    

    图领域自适应模型在跨网络学习任务中被广泛采用，旨在传递标签或结构知识。目前，在评估图领域自适应模型时存在两个主要限制。一方面，它们主要用于特定的跨网络节点分类任务，对于边级和图级任务的讨论相对较少。另一方面，它们主要在有限的场景中进行测试，如社交网络或引用网络，缺乏在更丰富场景中测试模型能力的验证。为了全面评估模型，提高模型在实际应用中的实用性，我们提出了一个名为OpenGDA的基准。它提供了丰富的预处理和统一的数据集，用于不同类型的任务（节点，边，图）。这些数据集来自于不同的场景，涵盖了网络信息系统，城市系统和自然系统。此外，它还集成了最先进的模型，并提供了标准化的和最终的模型评估方法。

    Graph domain adaptation models are widely adopted in cross-network learning tasks, with the aim of transferring labeling or structural knowledge. Currently, there mainly exist two limitations in evaluating graph domain adaptation models. On one side, they are primarily tested for the specific cross-network node classification task, leaving tasks at edge-level and graph-level largely under-explored. Moreover, they are primarily tested in limited scenarios, such as social networks or citation networks, lacking validation of model's capability in richer scenarios. As comprehensively assessing models could enhance model practicality in real-world applications, we propose a benchmark, known as OpenGDA. It provides abundant pre-processed and unified datasets for different types of tasks (node, edge, graph). They originate from diverse scenarios, covering web information systems, urban systems and natural systems. Furthermore, it integrates state-of-the-art models with standardized and end-to
    
[^36]: Tri-MipRF: 高效抗锯齿神经辐射场的三向 Mip 表示方法

    Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields. (arXiv:2307.11335v1 [cs.CV])

    [http://arxiv.org/abs/2307.11335](http://arxiv.org/abs/2307.11335)

    Tri-MipRF 提出了一种新颖的三向 Mip 编码方法，通过将预滤波的三维特征空间分解为正交的 mipmaps，实现了神经辐射场的即时重建和抗锯齿高保真渲染。

    

    尽管神经辐射场（NeRF）取得了巨大的进展，但我们仍然面临质量和效率之间的抉择，例如，MipNeRF能够呈现精细详细和抗锯齿的渲染效果，但需要数天的训练时间，而Instant-ngp可以在几分钟内完成重建，但在渲染不同距离或分辨率时会出现模糊或锯齿化的问题，因为忽略了采样区域。为此，我们提出了一种新颖的三向 Mip 编码，实现了神经辐射场的即时重建和抗锯齿高保真渲染。关键在于将预滤波的三维特征空间分解为三个正交的 mipmaps。通过利用二维预滤波特征图，我们可以高效地进行三维区域采样，从而显著提高渲染质量而不损失效率。为应对新的三向 Mip 表示方法，我们提出了一种锥投射渲染技术，以高效采样抗锯齿的三维特征。

    Despite the tremendous progress in neural radiance fields (NeRF), we still face a dilemma of the trade-off between quality and efficiency, e.g., MipNeRF presents fine-detailed and anti-aliased renderings but takes days for training, while Instant-ngp can accomplish the reconstruction in a few minutes but suffers from blurring or aliasing when rendering at various distances or resolutions due to ignoring the sampling area. To this end, we propose a novel Tri-Mip encoding that enables both instant reconstruction and anti-aliased high-fidelity rendering for neural radiance fields. The key is to factorize the pre-filtered 3D feature spaces in three orthogonal mipmaps. In this way, we can efficiently perform 3D area sampling by taking advantage of 2D pre-filtered feature maps, which significantly elevates the rendering quality without sacrificing efficiency. To cope with the novel Tri-Mip representation, we propose a cone-casting rendering technique to efficiently sample anti-aliased 3D fea
    
[^37]: 萨赫勒以南非洲象移动的分析：生态学、气候和保护观点

    Analysis of Elephant Movement in Sub-Saharan Africa: Ecological, Climatic, and Conservation Perspectives. (arXiv:2307.11325v1 [q-bio.PE])

    [http://arxiv.org/abs/2307.11325](http://arxiv.org/abs/2307.11325)

    本研究分析了萨赫勒以南非洲象移动的模式，重点关注了季节变化和降雨模式等动态驱动因素。研究结果有助于预测生态因素对象迁徙的潜在影响，并为制定保护策略提供了综合的视角。

    

    象与环境的相互作用对生态学和保护策略都有深远的影响。本研究提出了一种分析方法来解读萨赫勒以南非洲象移动的复杂模式，重点关注季节变化和降雨模式等关键生态驱动因素。尽管围绕这些具有影响力的因素存在复杂性，我们的分析提供了对非洲动态景观背景下象迁徙行为的全面视角。我们综合的方法使我们能够预测这些生态决定因素对象迁徙的潜在影响，这是建立知情的保护策略的关键一步。考虑到全球气候变化对季节和降雨模式的影响，这种预测尤为重要，因为它未来可能会对象的行动产生显著影响。我们的工作成果旨在不仅推进对移动生态学的理解，同时也为保护实践提供参考。

    The interaction between elephants and their environment has profound implications for both ecology and conservation strategies. This study presents an analytical approach to decipher the intricate patterns of elephant movement in Sub-Saharan Africa, concentrating on key ecological drivers such as seasonal variations and rainfall patterns. Despite the complexities surrounding these influential factors, our analysis provides a holistic view of elephant migratory behavior in the context of the dynamic African landscape. Our comprehensive approach enables us to predict the potential impact of these ecological determinants on elephant migration, a critical step in establishing informed conservation strategies. This projection is particularly crucial given the impacts of global climate change on seasonal and rainfall patterns, which could substantially influence elephant movements in the future. The findings of our work aim to not only advance the understanding of movement ecology but also f
    
[^38]: HVDetFusion：一种简单而强大的相机-雷达融合框架

    HVDetFusion: A Simple and Robust Camera-Radar Fusion Framework. (arXiv:2307.11323v1 [cs.CV])

    [http://arxiv.org/abs/2307.11323](http://arxiv.org/abs/2307.11323)

    HVDetFusion是一种简单而强大的相机-雷达融合框架，支持纯相机数据和相机+雷达数据的多模态检测，通过修改框架和利用先验信息实现更好的感知和过滤误报信息。

    

    在自动驾驶领域，3D目标检测是非常重要的感知模块。尽管当前的SOTA算法结合了相机和激光雷达传感器，但受到昂贵的激光雷达价格的限制，当前主流的方案是纯相机传感器或相机+雷达传感器。本研究提出了一种新的检测算法HVDetFusion，它是一种多模态检测算法，不仅支持纯相机数据作为检测输入，还可以将雷达数据和相机数据进行融合输入。相机数据流不依赖于雷达数据的输入，从而解决了之前方法的缺点。在纯相机数据流中，我们修改了Bevdet4D框架以提高感知效果和更高效的推断，并且该数据流有完整的3D检测输出。此外，为了融合雷达信号的优势，我们利用不同物体位置的先验信息来过滤原始雷达数据的误报信息。

    In the field of autonomous driving, 3D object detection is a very important perception module. Although the current SOTA algorithm combines Camera and Lidar sensors, limited by the high price of Lidar, the current mainstream landing schemes are pure Camera sensors or Camera+Radar sensors. In this study, we propose a new detection algorithm called HVDetFusion, which is a multi-modal detection algorithm that not only supports pure camera data as input for detection, but also can perform fusion input of radar data and camera data. The camera stream does not depend on the input of Radar data, thus addressing the downside of previous methods. In the pure camera stream, we modify the framework of Bevdet4D for better perception and more efficient inference, and this stream has the whole 3D detection output. Further, to incorporate the benefits of Radar signals, we use the prior information of different object positions to filter the false positive information of the original radar data, accor
    
[^39]: 如何整理一张桌子：融合视觉和语义常识推理解决具有模糊目标的机器人任务

    How to Tidy Up a Table: Fusing Visual and Semantic Commonsense Reasoning for Robotic Tasks with Vague Objectives. (arXiv:2307.11319v1 [cs.RO])

    [http://arxiv.org/abs/2307.11319](http://arxiv.org/abs/2307.11319)

    这项工作提出了一种融合视觉和语义常识推理的方法来解决具有模糊目标的机器人任务中的整理桌子问题。通过利用大规模语言模型的学习，可以推理出人类行为的常识。尽管语言模型的能力受限，但通过考虑感知和低级控制因素，可以解决整理桌子的任务。

    

    在许多现实场景中，模糊的目标给机器人技术带来了长期的挑战，因为很难定义规则、奖励或约束以进行优化。像整理一张凌乱的桌子这样的任务对人类来说可能很简单，但由于常识推理的歧义和灵活性，表达整洁的标准却很复杂。近年来，在大规模语言模型（LLM）的进一步发展中，我们有了通过这些模糊目标进行推理的机会：LLM通过学习大量人类数据来捕捉有关人类行为的有意义的常识。然而，由于LLM仅训练于语言输入，它们在感知和低级控制方面的能力有限，因此可能在机器人任务中遇到困难。在这项工作中，我们提出了一种简单的方法来解决整理桌子的任务，这是一个具有模糊目标的机器人任务的示例。具体而言，整理一张桌子的任务不仅涉及按照类型和功能对物体进行聚类以实现语义整洁，还需要考虑感知和低级控制方面的因素。

    Vague objectives in many real-life scenarios pose long-standing challenges for robotics, as defining rules, rewards, or constraints for optimization is difficult. Tasks like tidying a messy table may appear simple for humans, but articulating the criteria for tidiness is complex due to the ambiguity and flexibility in commonsense reasoning. Recent advancement in Large Language Models (LLMs) offers us an opportunity to reason over these vague objectives: learned from extensive human data, LLMs capture meaningful common sense about human behavior. However, as LLMs are trained solely on language input, they may struggle with robotic tasks due to their limited capacity to account for perception and low-level controls. In this work, we propose a simple approach to solve the task of table tidying, an example of robotic tasks with vague objectives. Specifically, the task of tidying a table involves not just clustering objects by type and functionality for semantic tidiness but also considerin
    
[^40]: XLDA: 线性判别分析用于在边缘上进行极端分类的扩展

    XLDA: Linear Discriminant Analysis for Scaling Continual Learning to Extreme Classification at the Edge. (arXiv:2307.11317v1 [cs.LG])

    [http://arxiv.org/abs/2307.11317](http://arxiv.org/abs/2307.11317)

    本文提出了一种名为 XLDA 的框架，可在边缘上进行极端分类，其中使用的线性判别分析（LDA）分类器等效于全连接（FC）层，通过优化实现了加速训练和推理的方法，并在极端数据集上进行了验证。

    

    流式线性判别分析（LDA）在边缘上部署受限类别（最多1000个）的增量学习中已经被证明有效，但在极度分类场景中尚未得到证明。本文提出了：（a）XLDA，一种用于边缘部署中类增量学习的框架，其中LDA分类器被证明与FC层等效，包括在极度分类场景中；（b）优化以实现基于XLDA的训练和推理，其中存在可用计算资源的限制。我们展示了批处理训练方法下的42倍加速和最近邻搜索在AliProducts（50k类别）和Google Landmarks V2（81k类别）等极端数据集上的5倍推理加速。

    Streaming Linear Discriminant Analysis (LDA) while proven in Class-incremental Learning deployments at the edge with limited classes (upto 1000), has not been proven for deployment in extreme classification scenarios. In this paper, we present: (a) XLDA, a framework for Class-IL in edge deployment where LDA classifier is proven to be equivalent to FC layer including in extreme classification scenarios, and (b) optimizations to enable XLDA-based training and inference for edge deployment where there is a constraint on available compute resources. We show up to 42x speed up using a batched training approach and up to 5x inference speedup with nearest neighbor search on extreme datasets like AliProducts (50k classes) and Google Landmarks V2 (81k classes)
    
[^41]: DPM-OT:一种基于最优传输的新型扩散概率模型

    DPM-OT: A New Diffusion Probabilistic Model Based on Optimal Transport. (arXiv:2307.11308v1 [cs.CV])

    [http://arxiv.org/abs/2307.11308](http://arxiv.org/abs/2307.11308)

    DPM-OT是一种基于最优传输的新型扩散概率模型，通过在不同阶段的潜变量之间进行最优传输，提出了一个快速DPMs的统一学习框架，可以在约10个函数评估内生成高质量的样本。

    

    从扩散概率模型(DPMs)中采样可以看作是一个分段分布变换，通常需要几百或几千个逆扩散轨迹步骤，才能得到高质量的图像。最近在设计DPMs的快速取样器方面取得了一些进展，通过知识蒸馏或调整方差进化曲线或降噪方程，实现了在取样速度和样本质量之间的平衡。然而，这种方法在两个方面都不能达到最佳效果，并且在短距离上经常遇到模式混合问题。为了解决这个问题，我们创新地将逆扩散视为在不同阶段的潜变量之间的最优传输问题，并提出了DPM-OT，这是一种用OT映射表示的高速DPMs的统一学习框架，可以在约10个函数评估内生成高质量的样本。通过计算数据潜变量和白噪声之间的半离散最优传输映射，我们获得了一条从前至后的快速捷径。

    Sampling from diffusion probabilistic models (DPMs) can be viewed as a piecewise distribution transformation, which generally requires hundreds or thousands of steps of the inverse diffusion trajectory to get a high-quality image. Recent progress in designing fast samplers for DPMs achieves a trade-off between sampling speed and sample quality by knowledge distillation or adjusting the variance schedule or the denoising equation. However, it can't be optimal in both aspects and often suffer from mode mixture in short steps. To tackle this problem, we innovatively regard inverse diffusion as an optimal transport (OT) problem between latents at different stages and propose the DPM-OT, a unified learning framework for fast DPMs with a direct expressway represented by OT map, which can generate high-quality samples within around 10 function evaluations. By calculating the semi-discrete optimal transport map between the data latents and the white noise, we obtain an expressway from the prio
    
[^42]: 基于内核的离线背景双向竞标者算法

    Kernelized Offline Contextual Dueling Bandits. (arXiv:2307.11288v1 [cs.LG])

    [http://arxiv.org/abs/2307.11288](http://arxiv.org/abs/2307.11288)

    本论文提出了基于内核的离线背景双向竞标者算法，用于解决基于偏好反馈的问题，并证明了算法的遗憾界。通过实验证明该方法优于使用均匀采样上下文的相似策略。

    

    基于偏好的反馈在许多应用中非常重要，这些应用中无法直接评估奖励函数。在人们对大型语言模型的强化学习中，这是一个显著的最新实例。对于许多这些应用，获取人类反馈的成本可能相当高甚至不可行。在这项工作中，我们利用一个事实，即代理通常可以选择获得人类反馈的上下文，以最高效地确定一个良好策略，并引入了离线背景双向竞标者设置。我们为这个设置提供了一个上界置信区间样式的算法，并证明了一个遗憾上界。我们还通过经验证实这种方法胜过使用均匀采样上下文的类似策略。

    Preference-based feedback is important for many applications where direct evaluation of a reward function is not feasible. A notable recent example arises in reinforcement learning from human feedback on large language models. For many of these applications, the cost of acquiring the human feedback can be substantial or even prohibitive. In this work, we take advantage of the fact that often the agent can choose contexts at which to obtain human feedback in order to most efficiently identify a good policy, and introduce the offline contextual dueling bandit setting. We give an upper-confidence-bound style algorithm for this setting and prove a regret bound. We also give empirical confirmation that this method outperforms a similar strategy that uses uniformly sampled contexts.
    
[^43]: 消除混合推理系统中意外的稳定固定点

    Eliminating Unintended Stable Fixpoints for Hybrid Reasoning Systems. (arXiv:2307.11286v1 [cs.AI])

    [http://arxiv.org/abs/2307.11286](http://arxiv.org/abs/2307.11286)

    这项研究介绍了一种类似AFT的方法论，能够利用先前计算的上界更精确地捕捉混合MKNF知识库的语义。

    

    多种非单调语义可以被表达为在AFT（Approximation Fixpoint Theory）下定义的近似计算器。使用传统的AFT理论，无法定义依赖于先前迭代计算的稳定修订信息的近似计算器。然而，对于将经典否定纳入非单调推理的语义来说，这些信息很重要。在这项工作中，我们引入了一种类似AFT的方法论，可以利用先前计算的上界更精确地捕捉语义。我们通过扩展最新的近似计算器，演示了我们的框架适用于混合MKNF（最小知识和否定为失败）知识库。

    A wide variety of nonmonotonic semantics can be expressed as approximators defined under AFT (Approximation Fixpoint Theory). Using traditional AFT theory, it is not possible to define approximators that rely on information computed in previous iterations of stable revision. However, this information is rich for semantics that incorporate classical negation into nonmonotonic reasoning. In this work, we introduce a methodology resembling AFT that can utilize priorly computed upper bounds to more precisely capture semantics. We demonstrate our framework's applicability to hybrid MKNF (minimal knowledge and negation as failure) knowledge bases by extending the state-of-the-art approximator.
    
[^44]: 联合单侧合成无配对图像转换和结肠癌预防的分割

    Joint one-sided synthetic unpaired image translation and segmentation for colorectal cancer prevention. (arXiv:2307.11253v1 [cs.CV])

    [http://arxiv.org/abs/2307.11253](http://arxiv.org/abs/2307.11253)

    通过联合训练分割模型和生成模型，使用3D技术和生成对抗网络合成逼真的图像，从而解决医学图像分析中的数据集获取困难问题，并在结肠息肉分割任务上取得有希望的结果。

    

    深度学习在分析医学图像方面表现出色。然而，由于隐私问题、标准化问题和缺乏注释，数据集很难获得。我们通过使用3D技术和生成对抗网络结合产生逼真的合成图像来解决这些问题。我们提出了CUT-seg，一种联合训练方法，其中分割模型和生成模型一起训练产生逼真的图像，并学习分割息肉。我们利用最近的单侧转换模型，因为它们使用的内存显著较少，可以在训练循环中添加一个分割模型。相比其他内存密集型的图像转换方法需要两阶段训练，CUT-seg表现更好，计算成本更低，而且只需要一个真实图像和零真实注释就可以在五个真实息肉分割数据集上取得有希望的结果。作为本研究的一部分，我们发布了...

    Deep learning has shown excellent performance in analysing medical images. However, datasets are difficult to obtain due privacy issues, standardization problems, and lack of annotations. We address these problems by producing realistic synthetic images using a combination of 3D technologies and generative adversarial networks. We propose CUT-seg, a joint training where a segmentation model and a generative model are jointly trained to produce realistic images while learning to segment polyps. We take advantage of recent one-sided translation models because they use significantly less memory, allowing us to add a segmentation model in the training loop. CUT-seg performs better, is computationally less expensive, and requires less real images than other memory-intensive image translation approaches that require two stage training. Promising results are achieved on five real polyp segmentation datasets using only one real image and zero real annotations. As a part of this study we releas
    
[^45]: 利用神经形态计算的传感器数据滤波在高能物理实验中的应用

    On-Sensor Data Filtering using Neuromorphic Computing for High Energy Physics Experiments. (arXiv:2307.11242v1 [cs.NE])

    [http://arxiv.org/abs/2307.11242](http://arxiv.org/abs/2307.11242)

    本文研究了在高能物理实验中利用神经形态计算的尖峰神经网络（SNN）模型对传感器数据进行滤波的方法。通过将传入的电荷波形转换为二值事件流，并优化SNN的系统设计和超参数，我们得到了信号有效率约为91%的SNN模型，参数数量几乎是深度神经网络的一半。

    

    本文描述了利用基于神经形态计算的尖峰神经网络（SNN）模型对在高亮度大型强子对撞机进行的高能物理实验中的传感器数据进行滤波的研究。我们提出了一种开发紧凑型神经形态模型的方法，该模型基于粒子的横向动量来滤除传感器数据，目标是减少传输到下游电子设备的数据量。传入的电荷波形被转换为二值事件流，然后由SNN进行处理。我们介绍了针对硬件部署进行优化的准确且紧凑的SNN的各种系统设计选择，从数据编码到训练算法的最佳超参数。我们的结果表明，利用进化算法和优化的超参数训练的SNN在信号有效率方面大约达到91%，参数数量几乎是深度神经网络的一半。

    This work describes the investigation of neuromorphic computing-based spiking neural network (SNN) models used to filter data from sensor electronics in high energy physics experiments conducted at the High Luminosity Large Hadron Collider. We present our approach for developing a compact neuromorphic model that filters out the sensor data based on the particle's transverse momentum with the goal of reducing the amount of data being sent to the downstream electronics. The incoming charge waveforms are converted to streams of binary-valued events, which are then processed by the SNN. We present our insights on the various system design choices - from data encoding to optimal hyperparameters of the training algorithm - for an accurate and compact SNN optimized for hardware deployment. Our results show that an SNN trained with an evolutionary algorithm and an optimized set of hyperparameters obtains a signal efficiency of about 91% with nearly half as many parameters as a deep neural netw
    
[^46]: Jina Embeddings:一种新颖的高性能句子嵌入模型

    Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])

    [http://arxiv.org/abs/2307.11224](http://arxiv.org/abs/2307.11224)

    Jina Embeddings是一组高性能的句子嵌入模型，能够捕捉文本的语义本质。该论文详细介绍了Jina Embeddings的开发过程，并通过性能评估验证了其优越性能。

    

    Jina Embeddings由一组高性能的句子嵌入模型组成，能够将各种文本输入转化为数值表示，从而捕捉文本的语义本质。虽然这些模型并非专门设计用于文本生成，但在密集检索和语义文本相似性等应用中表现出色。本文详细介绍了Jina Embeddings的开发过程，从创建高质量的成对和三元数据集开始。它强调了数据清理在数据集准备中的关键作用，并对模型训练过程进行了深入探讨，最后利用Massive Textual Embedding Benchmark（MTEB）进行了全面的性能评估。

    Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB).
    
[^47]: 迈向本体基础和语言无关的知识图谱

    Towards Ontologically Grounded and Language-Agnostic Knowledge Graphs. (arXiv:2307.11206v1 [cs.AI])

    [http://arxiv.org/abs/2307.11206](http://arxiv.org/abs/2307.11206)

    本论文提出通过抽象对象的具象化和承认概念和类型之间的本体区别，建立起一个基于本体和语言无关的知识图谱表示，以缓解知识图谱整合中的困难。

    

    知识图谱（KGs）已成为应用中表示事实信息的标准技术，如推荐引擎、搜索和问答系统。然而，持续更新KGs以及整合来自不同领域和不同语言的KGs仍然是一个主要挑战。我们在此建议通过抽象对象的具象化和承认概念和类型之间的本体区别，我们可以得到一个基于本体和语言无关的表示，从而可以缓解KGs整合中的困难。

    Knowledge graphs (KGs) have become the standard technology for the representation of factual information in applications such as recommendation engines, search, and question-answering systems. However, the continual updating of KGs, as well as the integration of KGs from different domains and KGs in different languages, remains to be a major challenge. What we suggest here is that by a reification of abstract objects and by acknowledging the ontological distinction between concepts and types, we arrive at an ontologically grounded and language-agnostic representation that can alleviate the difficulties in KG integration.
    
[^48]: 在MuJoCo环境中探索离散和连续控制任务的强化学习方法

    Exploring reinforcement learning techniques for discrete and continuous control tasks in the MuJoCo environment. (arXiv:2307.11166v1 [cs.LG])

    [http://arxiv.org/abs/2307.11166](http://arxiv.org/abs/2307.11166)

    该论文研究了在MuJoCo环境中离散和连续控制任务的强化学习方法，通过比较不同算法，发现DDPG在少量episode中表现优于其他方法。

    

    我们利用快速的物理模拟器MuJoCo在连续控制环境中运行任务，并揭示每个任务的观测空间、动作空间、奖励等详细信息。通过比较离散化方法中的Q学习和SARSA，将值基方法应用于连续控制任务，并使用它们作为基准，逐步向最先进的深度策略梯度方法DDPG过渡。在大量的episode中，Q学习的得分超过了SARSA，但DDPG在少量的episode中表现优于两者。最后，我们还通过调整模型超参数，期望在更少的时间和资源消耗下获得更好的性能。我们预期新设计的DDPG将大幅提高性能，然而在只有少数episode之后，我们已经能够达到不错的平均奖励。我们期望在足够的时间和计算资源下进一步提高性能。

    We leverage the fast physics simulator, MuJoCo to run tasks in a continuous control environment and reveal details like the observation space, action space, rewards, etc. for each task. We benchmark value-based methods for continuous control by comparing Q-learning and SARSA through a discretization approach, and using them as baselines, progressively moving into one of the state-of-the-art deep policy gradient method DDPG. Over a large number of episodes, Qlearning outscored SARSA, but DDPG outperformed both in a small number of episodes. Lastly, we also fine-tuned the model hyper-parameters expecting to squeeze more performance but using lesser time and resources. We anticipated that the new design for DDPG would vastly improve performance, yet after only a few episodes, we were able to achieve decent average rewards. We expect to improve the performance provided adequate time and computational resources.
    
[^49]: 模型与锡人之间——使用大语言模型研究AI对齐中的委托-代理问题的行为经济学研究

    Of Models and Tin Men -- a behavioural economics study of principal-agent problems in AI alignment using large-language models. (arXiv:2307.11137v1 [cs.AI])

    [http://arxiv.org/abs/2307.11137](http://arxiv.org/abs/2307.11137)

    本研究基于行为经济学角度，对使用大语言模型进行AI对齐中的委托-代理问题进行研究，发现现实世界中的AI安全问题不仅涉及设计者与代理之间的冲突，还涉及到多个代理之间的信息不对称与效用函数之间的错位。

    

    AI对齐通常被描述为一个设计者与人工智能代理之间的相互作用，设计者试图确保代理的行为与其目的一致，并且风险仅仅是由于设计者意图中的效用函数与代理的内部效用函数之间的意外错位而导致的冲突。然而，随着使用大语言模型（LLM）实例化的代理的出现，这种描述不能捕捉到AI安全的核心方面，因为现实世界中设计者与代理之间并没有一对一的对应关系，而且许多代理，无论是人工智能还是人类，都具有多样的价值观。因此，AI安全具有经济方面的问题，委托-代理问题可能会出现。

    AI Alignment is often presented as an interaction between a single designer and an artificial agent in which the designer attempts to ensure the agent's behavior is consistent with its purpose, and risks arise solely because of conflicts caused by inadvertent misalignment between the utility function intended by the designer and the resulting internal utility function of the agent. With the advent of agents instantiated with large-language models (LLMs), which are typically pre-trained, we argue this does not capture the essential aspects of AI safety because in the real world there is not a one-to-one correspondence between designer and agent, and the many agents, both artificial and human, have heterogeneous values. Therefore, there is an economic aspect to AI safety and the principal-agent problem is likely to arise. In a principal-agent problem conflict arises because of information asymmetry together with inherent misalignment between the utility of the agent and its principal, an
    
[^50]: 对脑网络的可解释分类进行对比图池化。

    Contrastive Graph Pooling for Explainable Classification of Brain Networks. (arXiv:2307.11133v1 [q-bio.NC])

    [http://arxiv.org/abs/2307.11133](http://arxiv.org/abs/2307.11133)

    本论文提出了一种针对脑网络的对比图池化方法，以实现对脑网络的可解释分类。通过定制化的图神经网络和特殊设计的可解释特征提取方法，在5个静息态fMRI脑网络数据集上取得了优于最先进基准线的结果。

    

    功能性磁共振成像(fMRI)是一种常用的测量神经活动的技术。其应用在识别帕金森病、阿尔茨海默病和自闭症等神经退行性疾病方面尤为重要。最近的fMRI数据分析将大脑建模为图，并通过图神经网络(GNN)提取特征。然而，fMRI数据的独特特征要求对GNN进行特殊设计。定制GNN以生成有效且可解释的特征仍然具有挑战性。在本文中，我们提出了对比双注意块和可微分图池化方法ContrastPool，以更好地利用GNN分析脑网络，满足fMRI的特殊要求。我们将我们的方法应用于5个静息态fMRI脑网络数据集的3种疾病，并证明其优于最先进的基准线。我们的案例研究证实，我们的方法提取的模式与神经科学文献中的领域知识相匹配。

    Functional magnetic resonance imaging (fMRI) is a commonly used technique to measure neural activation. Its application has been particularly important in identifying underlying neurodegenerative conditions such as Parkinson's, Alzheimer's, and Autism. Recent analysis of fMRI data models the brain as a graph and extracts features by graph neural networks (GNNs). However, the unique characteristics of fMRI data require a special design of GNN. Tailoring GNN to generate effective and domain-explainable features remains challenging. In this paper, we propose a contrastive dual-attention block and a differentiable graph pooling method called ContrastPool to better utilize GNN for brain networks, meeting fMRI-specific requirements. We apply our method to 5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its superiority over state-of-the-art baselines. Our case study confirms that the patterns extracted by our method match the domain knowledge in neuroscience literatu
    
[^51]: 近似计算调查，第二部分：应用特定和架构近似技术及应用

    Approximate Computing Survey, Part II: Application-Specific & Architectural Approximation Techniques and Applications. (arXiv:2307.11128v1 [cs.AR])

    [http://arxiv.org/abs/2307.11128](http://arxiv.org/abs/2307.11128)

    近似计算是一种能够调整系统设计结果质量以提高能源效率和/或性能的新兴解决方案，已吸引学术界和工业界的广泛关注。这篇论文是一个关于应用特定和架构近似技术的调查的第二部分。

    

    计算密集型应用的挑战性部署，如人工智能（AI）和数字信号处理（DSP），迫使计算系统界探索新的设计方法。近似计算成为一种新兴解决方案，允许在系统设计中调整结果的质量，以提高能源效率和/或性能。近年来，这种根本性的范式转变吸引了学术界和工业界的兴趣，并在不同设计层面（从系统到集成电路）上进行了重要的近似技术和方法的研究。受近似计算在过去10年的广泛吸引力的驱使，我们进行了一个两部分的调查，涵盖了关键方面（如术语和应用）并回顾了传统计算堆栈的各个层面的最新近似技术。在我们的调查第二部分中，我们对应用特定和架构近似技术的技术细节进行分类和介绍。

    The challenging deployment of compute-intensive applications from domains such Artificial Intelligence (AI) and Digital Signal Processing (DSP), forces the community of computing systems to explore new design approaches. Approximate Computing appears as an emerging solution, allowing to tune the quality of results in the design of a system in order to improve the energy efficiency and/or performance. This radical paradigm shift has attracted interest from both academia and industry, resulting in significant research on approximation techniques and methodologies at different design layers (from system down to integrated circuits). Motivated by the wide appeal of Approximate Computing over the last 10 years, we conduct a two-part survey to cover key aspects (e.g., terminology and applications) and review the state-of-the art approximation techniques from all layers of the traditional computing stack. In Part II of our survey, we classify and present the technical details of application-s
    
[^52]: 智能的本质

    Nature of Intelligence. (arXiv:2307.11114v1 [q-bio.NC])

    [http://arxiv.org/abs/2307.11114](http://arxiv.org/abs/2307.11114)

    智能的本质是一系列通过在空间和时间上建立数据集之间的功能关系来最小化系统熵的数学函数过程。

    

    人类大脑是人类智能的基础。通过模拟人类大脑，人工智能构建具有学习能力并执行接近人类水平的智能任务的计算模型。深度神经网络由多个计算层组成，学习数据的表示并在许多识别领域改进了最先进的技术。然而，智能的本质，即通过人类和人工智能共同代表的智能的本质，尚不清楚。在这里，我们展示智能的本质是一系列通过在空间和时间上建立数据集之间的功能关系来最小化系统熵的数学函数过程。人类和人工智能通过以一种受强化方式消耗能量的方式实现这些减熵过程。根据这一假设，我们建立了关于语言、无意识和意识的数学模型，预测神经科学和人工智能工程实现的证据。

    The human brain is the substrate for human intelligence. By simulating the human brain, artificial intelligence builds computational models that have learning capabilities and perform intelligent tasks approaching the human level. Deep neural networks consist of multiple computation layers to learn representations of data and improve the state-of-the-art in many recognition domains. However, the essence of intelligence commonly represented by both humans and AI is unknown. Here, we show that the nature of intelligence is a series of mathematically functional processes that minimize system entropy by establishing functional relationships between datasets over space and time. Humans and AI have achieved intelligence by implementing these entropy-reducing processes in a reinforced manner that consumes energy. With this hypothesis, we establish mathematical models of language, unconsciousness and consciousness, predicting the evidence to be found by neuroscience and achieved by AI engineer
    
[^53]: 部署强化学习代理进行AAA游戏测试的技术挑战

    Technical Challenges of Deploying Reinforcement Learning Agents for Game Testing in AAA Games. (arXiv:2307.11105v1 [cs.SE])

    [http://arxiv.org/abs/2307.11105](http://arxiv.org/abs/2307.11105)

    本文介绍了在AAA游戏测试中部署强化学习代理所面临的技术挑战，并提出了一些帮助游戏行业采用这项技术的研究方向。

    

    从研究到实际应用，特别是对于大型复杂软件系统来说，是一个困难的问题。在大规模游戏制作中，主要原因之一是开发环境可能与最终产品存在很大差异。本技术论文描述了一个在现有基于脚本机器人的自动化游戏测试解决方案中添加实验性强化学习系统的努力，以增加其能力。我们介绍了如何集成这个强化学习系统，旨在增加类似[1]的AAA游戏（包括Battlefield 2042和Dead Space（2023））的测试覆盖率。本技术论文旨在展示在游戏制作中利用强化学习的案例，并介绍了希望在游戏中进行相同探索的人可能会遇到的最大时间消耗。此外，为了帮助游戏行业更快地采用这项技术，我们提出了一些我们认为的研究方向。

    Going from research to production, especially for large and complex software systems, is fundamentally a hard problem. In large-scale game production, one of the main reasons is that the development environment can be very different from the final product. In this technical paper we describe an effort to add an experimental reinforcement learning system to an existing automated game testing solution based on scripted bots in order to increase its capacity. We report on how this reinforcement learning system was integrated with the aim to increase test coverage similar to [1] in a set of AAA games including Battlefield 2042 and Dead Space (2023). The aim of this technical paper is to show a use-case of leveraging reinforcement learning in game production and cover some of the largest time sinks anyone who wants to make the same journey for their game may encounter. Furthermore, to help the game industry to adopt this technology faster, we propose a few research directions that we believ
    
[^54]: 用学习的代理和约束解决基于多物理的反问题

    Solving multiphysics-based inverse problems with learned surrogates and constraints. (arXiv:2307.11099v1 [physics.geo-ph])

    [http://arxiv.org/abs/2307.11099](http://arxiv.org/abs/2307.11099)

    本论文将学习代理和学习约束相结合用于解决基于多物理的反问题，通过该方法不仅改善了对流体流动性质的反演精度，而且为反演多模态数据提供了一个有效的解决方案。

    

    在地质碳封存监测中，当多模态时变数据昂贵且数值模拟成本高昂时，解决基于多物理的反问题可能具有挑战性。我们通过将计算成本低廉的学习代理与学习约束相结合来克服这些挑战。这种组合不仅能够大大改善对重要流体流动性质（渗透率）的反演，还能为反演多模态数据（包括井测量和主动源时变地震数据）提供一个自然的平台。通过添加学习约束，我们得到了一个计算可行的反演方法，其精度仍然准确。这通过包含一个经过训练的深度神经网络（称为归一化流），使模型迭代保持在分布内，从而保证了作为代理的经过训练的傅里叶神经算子的准确性，这些算子用于代替涉及部分计算昂贵的多相流模拟。

    Solving multiphysics-based inverse problems for geological carbon storage monitoring can be challenging when multimodal time-lapse data are expensive to collect and costly to simulate numerically. We overcome these challenges by combining computationally cheap learned surrogates with learned constraints. Not only does this combination lead to vastly improved inversions for the important fluid-flow property, permeability, it also provides a natural platform for inverting multimodal data including well measurements and active-source time-lapse seismic data. By adding a learned constraint, we arrive at a computationally feasible inversion approach that remains accurate. This is accomplished by including a trained deep neural network, known as a normalizing flow, which forces the model iterates to remain in-distribution, thereby safeguarding the accuracy of trained Fourier neural operators that act as surrogates for the computationally expensive multiphase flow simulations involving partia
    
[^55]: 密集样本深度学习

    Dense Sample Deep Learning. (arXiv:2307.10991v1 [cs.AI])

    [http://arxiv.org/abs/2307.10991](http://arxiv.org/abs/2307.10991)

    密集样本深度学习是一种针对深度学习网络的研究方法，旨在揭示学习机制和表示的未知特性，并解决大规模数据和隐藏单元存在的问题。

    

    深度学习（DL）是20世纪80年代提出的一种神经网络算法的变体，在人工智能（AI）领域取得了令人惊讶的进展，包括语言翻译、蛋白质折叠、自动驾驶汽车，以及最近的类人语言模型（CHATbots）。尽管深度学习（DL）网络的使用越来越广泛，但对于使这些网络在如此广泛的应用中有效的学习机制和表示仍知之甚少。部分原因可能是其大规模架构和大规模数据的使用，但深度学习表示的本质仍然大部分未知。不幸的是，具有数百万或数十亿个标记的训练集存在未知的组合方式，同时数百万或数十亿个隐藏单元的网络难以可视化，其机制也难以揭示。在本文中，我们提出了一种密集样本深度学习的方法。

    Deep Learning (DL) , a variant of the neural network algorithms originally proposed in the 1980s, has made surprising progress in Artificial Intelligence (AI), ranging from language translation, protein folding, autonomous cars, and more recently human-like language models (CHATbots), all that seemed intractable until very recently. Despite the growing use of Deep Learning (DL) networks, little is actually understood about the learning mechanisms and representations that makes these networks effective across such a diverse range of applications. Part of the answer must be the huge scale of the architecture and of course the large scale of the data, since not much has changed since 1987. But the nature of deep learned representations remain largely unknown. Unfortunately training sets with millions or billions of tokens have unknown combinatorics and Networks with millions or billions of hidden units cannot easily be visualized and their mechanisms cannot be easily revealed. In this pap
    
[^56]: AdjointDPM: 扩散概率模型梯度反向传播的伴随灵敏度方法

    AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models. (arXiv:2307.10711v1 [cs.CV])

    [http://arxiv.org/abs/2307.10711](http://arxiv.org/abs/2307.10711)

    AdjointDPM是一种新的伴随灵敏度方法，用于扩散概率模型的梯度反向传播，解决了DPM定制化中内存消耗高的问题，并通过解决增强的ODE将损失的梯度反向传播到模型的参数。

    

    现有的定制化方法需要多个参考样例来将预训练的扩散概率模型(DPMs)与用户提供的概念对齐。本文旨在解决当唯一可用的监督是定义在生成内容上的可微度量时的DPM定制化挑战。由于DPM的采样过程涉及对去噪UNet的递归调用，朴素的梯度反向传播需要存储所有迭代的中间状态，导致内存消耗极高。为了解决这个问题，我们提出了一种新的方法AdjointDPM，首先通过求解相应的概率流ODE从扩散模型中生成新样本。然后使用伴随灵敏度方法通过求解另一个增强的ODE将损失的梯度反向传播到模型的参数(包括调制信号、网络权重和初始噪声)。为了减少正向生成和反向传播中的数值误差

    Existing customization methods require access to multiple reference examples to align pre-trained diffusion probabilistic models (DPMs) with user-provided concepts. This paper aims to address the challenge of DPM customization when the only available supervision is a differentiable metric defined on the generated contents. Since the sampling procedure of DPMs involves recursive calls to the denoising UNet, na\"ive gradient backpropagation requires storing the intermediate states of all iterations, resulting in extremely high memory consumption. To overcome this issue, we propose a novel method AdjointDPM, which first generates new samples from diffusion models by solving the corresponding probability-flow ODEs. It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models' parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE. To reduce numerical errors in both the forward generation and 
    
[^57]: 使用文本分类检测虚假评论

    Detecting deceptive reviews using text classification. (arXiv:2307.10617v1 [cs.IR])

    [http://arxiv.org/abs/2307.10617](http://arxiv.org/abs/2307.10617)

    这篇论文提出了一种使用机器学习模型的方法来识别虚假评论，并通过在餐馆评论的数据集上进行实验验证了其性能。

    

    近年来，在线评论在推广任何产品或服务方面发挥着重要作用。企业可能会嵌入虚假评论以吸引客户购买他们的产品。他们甚至可能突出强调自己产品的优点或批评竞争对手的产品。市场营销人员、广告商和其他在线商业用户有动机为他们想要推广的产品编写虚假的正面评论，或者为他们真正不喜欢的产品提供虚假的负面评论。因此，识别虚假评论是一个紧迫且持续的研究领域。本研究论文提出了一种机器学习模型方法来识别虚假评论。论文调查了在一个餐馆评论的虚假意见垃圾语料库数据集上进行的多次实验的性能。我们采用了n-gram模型和最大特征来识别虚假评论。

    In recent years, online reviews play a vital role for promoting any kind of product or services. Businesses may embed fake reviews in order to attract customers to purchase their products. They may even highlight the benefits of their own product or criticize the competition's product. Marketers, advertisers, and other online business users have incentive to create fake positive reviews for products which they want to promote or give fake negative reviews for products which they really don't like. So now-a-days writing a deceptive review is inevitable thing for promoting their own business or degrading competitor's reputation. Thus, identifying deceptive reviews is an intense and on-going research area. This research paper proposes machine learning model approach to identify deceptive reviews. The paper investigates the performance of the several experiments done on a Deceptive Opinion Spam Corpus dataset of restaurants reviews. We developed a n-gram model and max features to identify 
    
[^58]: Ethosight: 一种基于联合嵌入的系统，利用上下文标签关联度度量和基于推理的迭代学习进行细致感知

    Ethosight: A Joint-Embedding Based System for Nuanced Perception Using Contextual Label Affinity Metric and Reasoning Based Iterative Learning. (arXiv:2307.10577v1 [cs.CV])

    [http://arxiv.org/abs/2307.10577](http://arxiv.org/abs/2307.10577)

    Ethosight是一种零样本计算机视觉算法，通过联合嵌入、上下文标签关联度计算和基于推理的迭代学习，实现对细微行为和场景细节的准确感知，同时消除了对预先存在符号知识的需求。

    

    传统的计算机视觉模型通常需要大量的人工努力来进行数据获取和验证，特别是在检测细微的行为细节或事件时。在实际应用中，区分常规行为和潜在风险的困难，如区分常规购物和潜在扒窃，进一步复杂化了这一过程。我们提出了Ethosight，一种新颖的零样本计算机视觉算法。Ethosight消除了对预先存在的符号知识的需求，从用户需求和感兴趣的语义知识出发进行自主学习。通过使用局部标签关联度计算和基于推理的迭代学习循环，Ethosight推断场景细节并迭代地优化标签集。推理机制可以来自大型语言模型如GPT4、符号推理器如OpenNARS或混合系统。Ethosight还充分利用了预训练的多模态模型ImageBind的能力。

    Traditional computer vision models often require extensive manual effort for data acquisition and validation, particularly when detecting subtle behavioral nuances or events. The difficulty in distinguishing routine behaviors from potential risks in real-world applications, like differentiating routine shopping from potential shoplifting, further complicates the process.  We present Ethosight, a novel zero-shot computer vision algorithm. Ethosight eradicates the need for pre-existing symbolic knowledge, initiating from a clean slate based on user requirements and semantic knowledge of interest. Using localized label affinity calculations and a reasoning-guided iterative learning loop, Ethosight infers scene details and iteratively refines the label set. Reasoning mechanisms can be derived from large language models like GPT4, symbolic reasoners like OpenNARS, or hybrid systems.  Ethosight further capitalizes on the capabilities of a pre-trained multi-modal model, ImageBind, generating 
    
[^59]: 图像和声音的滥用用于在多模态LLMs中进行间接指令注入

    (Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])

    [http://arxiv.org/abs/2307.10490](http://arxiv.org/abs/2307.10490)

    本论文展示了如何利用图像和声音在多模态LLMs中进行间接指令注入，攻击者通过生成对抗扰动并将其融入图像或音频录音中，以操纵模型输出特定文本和指导对话的行为。

    

    我们展示了如何利用图像和声音在多模态LLMs中进行间接提示和指令注入。攻击者生成与提示相对应的对抗扰动，并将其融入图像或音频录音中。当用户向（未修改的良性）模型询问被扰动的图像或音频时，扰动会引导模型输出攻击者选择的文本和/或使后续对话遵循攻击者的指令。我们用几个概念验证示例针对LLaVa和PandaGPT来说明这种攻击。

    We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
    
[^60]: ZeroQuant-FP: 使用浮点格式进行LLMs训练后量化的一项飞跃

    ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats. (arXiv:2307.09782v1 [cs.LG])

    [http://arxiv.org/abs/2307.09782](http://arxiv.org/abs/2307.09782)

    ZeroQuant-FP通过使用浮点格式进行LLMs训练后量化，解决了在大型语言模型中平衡计算效率和保持模型质量的挑战，并发现FP8激活优于INT8，并且FP4权重表现与INT4相当甚至更优。

    

    在大型语言模型（LLMs）的复杂领域中，平衡计算效率和保持模型质量是一个巨大的挑战。本研究通过探讨浮点（FP）量化的可行性，特别关注FP8和FP4，以应对均匀量化的固有限制，尤其是处理离群值，并受到NVIDIA H100硬件的启发。我们的全面调查发现，在LLMs中，FP8激活始终优于其整数（INT8）等效，性能优势在包含超过十亿参数的模型中更为明显。对于权重量化，我们的研究结果表明，FP4的性能与INT4相当，甚至更优，简化了在像H100这样支持FP的硬件上的部署。为了减少由权重和激活之间差异引起的精度对齐开销，我们提出了两个缩放约束。

    In the complex domain of large language models (LLMs), striking a balance between computational efficiency and maintaining model quality is a formidable challenge. Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers, and motivated by the launch of NVIDIA's H100 hardware, this study delves into the viability of floating-point (FP) quantization, particularly focusing on FP8 and FP4, as a potential solution. Our comprehensive investigation reveals that for LLMs, FP8 activation consistently outshines its integer (INT8) equivalent, with the performance edge becoming more noticeable in models possessing parameters beyond one billion. For weight quantization, our findings indicate that FP4 exhibits comparable, if not superior, performance to INT4, simplifying deployment on FP-supported hardware like H100. To mitigate the overhead from precision alignment caused by the disparity between weights and activations, we propose two scaling constraints
    
[^61]: Ord2Seq: 将序回归视为标签序列预测

    Ord2Seq: Regard Ordinal Regression as Label Sequence Prediction. (arXiv:2307.09004v1 [cs.AI])

    [http://arxiv.org/abs/2307.09004](http://arxiv.org/abs/2307.09004)

    Ord2Seq是一种将序回归问题转化为标签序列预测的方法，通过递归的二元分类步骤微妙地区分相邻类别，在不同场景中达到优于现有方法的性能表现。

    

    序回归是将对象实例分为序列类别的分类问题。在许多情况下，如医疗疾病分级、电影评分等方面已得到广泛研究。现有方法仅关注学习类间序关系，但在区分相邻类别方面仍存在局限性。本文中，我们提出了一种简单的序列预测框架，称为Ord2Seq，它首次将每个序类别标签转化为特殊的标签序列，从而将序回归任务视为序列预测过程。通过这种方式，我们将序回归任务分解为一系列递归的二元分类步骤，以微妙地区分相邻类别。全面的实验证明了区分相邻类别对性能改进的有效性，并且我们的新方法在四种不同的场景中超过了现有最先进性能。代码将在完成后提供。

    Ordinal regression refers to classifying object instances into ordinal categories. It has been widely studied in many scenarios, such as medical disease grading, movie rating, etc. Known methods focused only on learning inter-class ordinal relationships, but still incur limitations in distinguishing adjacent categories thus far. In this paper, we propose a simple sequence prediction framework for ordinal regression called Ord2Seq, which, for the first time, transforms each ordinal category label into a special label sequence and thus regards an ordinal regression task as a sequence prediction process. In this way, we decompose an ordinal regression task into a series of recursive binary classification steps, so as to subtly distinguish adjacent categories. Comprehensive experiments show the effectiveness of distinguishing adjacent categories for performance improvement and our new approach exceeds state-of-the-art performances in four different scenarios. Codes will be available upon a
    
[^62]: 使用单个电路计算量子神经网络所有参数的梯度

    Computing the gradients with respect to all parameters of a quantum neural network using a single circuit. (arXiv:2307.08167v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2307.08167](http://arxiv.org/abs/2307.08167)

    该论文提出了一种使用单个电路计算量子神经网络所有参数梯度的方法，相比传统方法，它具有较低的电路深度和较少的编译时间，从而加速了总体运行时间。

    

    在使用参数平移规则计算量子神经网络的梯度时，需要对网络的单个可调参数计算两次代价函数。当参数总数较高时，需要调整和运行多次用于计算的量子电路。在这里，我们提出了一种仅使用一个电路计算所有梯度的方法，它具有较低的电路深度和较少的经典寄存器。我们还在真实量子硬件和模拟器上进行了实验证明，我们的方法具有电路编译时间明显缩短的优势，从而加速了总体运行时间。

    When computing the gradients of a quantum neural network using the parameter-shift rule, the cost function needs to be calculated twice for the gradient with respect to a single adjustable parameter of the network. When the total number of parameters is high, the quantum circuit for the computation has to be adjusted and run for many times. Here we propose an approach to compute all the gradients using a single circuit only, with a much reduced circuit depth and less classical registers. We also demonstrate experimentally, on both real quantum hardware and simulator, that our approach has the advantages that the circuit takes a significantly shorter time to compile than the conventional approach, resulting in a speedup on the total runtime.
    
[^63]: 深度神经网络中的定量中心极限定理

    Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])

    [http://arxiv.org/abs/2307.06092](http://arxiv.org/abs/2307.06092)

    本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。

    

    我们研究了具有随机高斯权重和偏置的全连接神经网络的分布，其中隐藏层宽度与大常数 $n$ 成比例。在非线性的温和假设下，我们得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限。我们的定理表明，无论是对于有限维分布还是整个过程，随机全连接网络（及其导数）与相应的无限宽高斯过程之间的距离都会按照 $n^{-\gamma}$ 缩放，其中 $\gamma>0$，指数取决于用于度量差异的度量方式。我们的界限在网络宽度的依赖性方面比文献中以前提供的任何界限都要强。

    We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma>0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
    
[^64]: 学习用于开放式医学诊断的大边缘稀疏嵌入

    Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis. (arXiv:2307.04541v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.04541](http://arxiv.org/abs/2307.04541)

    本研究提出了一种用于开放式医学诊断的大边缘稀疏嵌入方法，通过引入角度边缘和自适应尺度，使算法能够正确分类已知类别并识别未知类别，以进一步支持医学诊断。

    

    在深度学习的推动下，计算机辅助诊断取得了巨大的进展。然而，在非受控的实验室环境中，算法可能面临多种挑战。开放式识别(OSR)作为一个重要问题，表示在训练中未见过的类别可能出现在测试中。在医学领域，这可能是由于训练数据集的不完全收集以及不断出现的新的或罕见的疾病。OSR需要一个算法不仅能正确分类已知类别，还能识别未知类别，并将其转发给专家进行进一步的诊断。为了解决OSR问题，我们假设已知类别可能密集地占据了嵌入空间的小部分，而其余的稀疏区域可能被识别为未知类别。基于此，我们提出了统一两种机制的开放边缘余弦损失(OMCL)。前者被称为具有自适应尺度的边缘损失(MLAS)，引入了角度边缘以增强类内紧密度和类间可分离性，并结合了自适应尺度。

    Fueled by deep learning, computer-aided diagnosis achieves huge advances. However, out of controlled lab environments, algorithms could face multiple challenges. Open set recognition (OSR), as an important one, states that categories unseen in training could appear in testing. In medical fields, it could derive from incompletely collected training datasets and the constantly emerging new or rare diseases. OSR requires an algorithm to not only correctly classify known classes, but also recognize unknown classes and forward them to experts for further diagnosis. To tackle OSR, we assume that known classes could densely occupy small parts of the embedding space and the remaining sparse regions could be recognized as unknowns. Following it, we propose Open Margin Cosine Loss (OMCL) unifying two mechanisms. The former, called Margin Loss with Adaptive Scale (MLAS), introduces angular margin for reinforcing intra-class compactness and inter-class separability, together with an adaptive scali
    
[^65]: 利用传输学习方法在LiDAR数据上识别埋藏的考古结构的语义分割研究

    Tranfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data. (arXiv:2307.03512v1 [cs.CV])

    [http://arxiv.org/abs/2307.03512](http://arxiv.org/abs/2307.03512)

    本文研究了利用传输学习方法在LiDAR数据上识别埋藏的考古结构的语义分割。实验结果表明，传输学习的应用可以提高性能，为未来工作提供基准。

    

    当将深度学习应用于考古研究中的遥感数据时，一个显著的障碍是适用于模型训练的合适数据集的有限可用性。传输学习的应用经常被用来减轻这个缺点。然而，仍有必要探索在不同考古数据集上应用传输学习的有效性。本文比较了使用两个语义分割深度神经网络在两个LiDAR数据集上的各种传输学习配置的性能。实验结果表明，基于传输学习的方法在考古学中可以提高性能，尽管尚未观察到系统性的改进。我们提供了关于此类技术有效性的具体见解，可作为未来工作的基准。

    When applying deep learning to remote sensing data in archaeological research, a notable obstacle is the limited availability of suitable datasets for training models. The application of transfer learning is frequently employed to mitigate this drawback. However, there is still a need to explore its effectiveness when applied across different archaeological datasets. This paper compares the performance of various transfer learning configurations using two semantic segmentation deep neural networks on two LiDAR datasets. The experimental results indicate that transfer learning-based approaches in archaeology can lead to performance improvements, although a systematic enhancement has not yet been observed. We provide specific insights about the validity of such techniques that can serve as a baseline for future works.
    
[^66]: 混合知识数据驱动的通道语义获取和波束赋形用于无小区大规模MIMO系统

    Hybrid Knowledge-Data Driven Channel Semantic Acquisition and Beamforming for Cell-Free Massive MIMO. (arXiv:2307.03070v1 [eess.SP] CROSS LISTED)

    [http://arxiv.org/abs/2307.03070](http://arxiv.org/abs/2307.03070)

    本文提出了一种混合知识数据驱动的方法，用于无小区大规模MIMO系统中的通道语义获取和多用户波束赋形，可提高室外无线系统的性能，支持扩展现实应用。

    

    本文旨在推动室外无线系统的发展，以更好地支持普遍存在的扩展现实（XR）应用，并缩小与当前室内无线传输能力之间的差距。我们提出了一种混合知识数据驱动的方法，用于无小区大规模多输入多输出（MIMO）系统中的通道语义获取和多用户波束赋形。具体而言，我们首先提出了一种基于数据驱动的多层感知机（MLP）-Mixer自编码器，用于通道语义获取，在此过程中联合优化了导频信号、信道语义嵌入的CSI量化器以及信道语义提取的CSI重构。此外，基于获取的通道语义，我们进一步提出了一种知识驱动的深度展开多用户波束赋形器，在室外XR场景中能够实现良好的频谱效率，并对不完美的CSI具有鲁棒性。通过展开传统的迭代超松弛（SOR）线性波束赋形

    This paper focuses on advancing outdoor wireless systems to better support ubiquitous extended reality (XR) applications, and close the gap with current indoor wireless transmission capabilities. We propose a hybrid knowledge-data driven method for channel semantic acquisition and multi-user beamforming in cell-free massive multiple-input multiple-output (MIMO) systems. Specifically, we firstly propose a data-driven multiple layer perceptron (MLP)-Mixer-based auto-encoder for channel semantic acquisition, where the pilot signals, CSI quantizer for channel semantic embedding, and CSI reconstruction for channel semantic extraction are jointly optimized in an end-to-end manner. Moreover, based on the acquired channel semantic, we further propose a knowledge-driven deep-unfolding multi-user beamformer, which is capable of achieving good spectral efficiency with robustness to imperfect CSI in outdoor XR scenarios. By unfolding conventional successive over-relaxation (SOR)-based linear beamf
    
[^67]: 从$O(\sqrt{n})$到$O(\log(n))$的二次规划问题

    From $O(\sqrt n)$ to $O(\log n)$ in Quadratic Programming. (arXiv:2306.15079v1 [math.OC])

    [http://arxiv.org/abs/2306.15079](http://arxiv.org/abs/2306.15079)

    这篇论文提出了一种迭代复杂度为$O(\log(n))$的二次规划优化算法，并通过严格的理论证明验证了该算法的可行性。这一重大突破使得我们从$O(\sqrt{n})$的优化算法过渡到$O(\log(n))$的优化算法，其在大数据和人工智能时代具有重要应用价值。

    

    多年来，数值优化理论一直存在一个困扰，即是否存在一个迭代复杂度为$O(\log(n))$的优化算法。本文通过引入一种全新的优化算法和严格的理论证明来回答这个问题。该算法以有界盒二次规划问题（Box-QP）为起点，许多实际优化问题可以通过对偶理论转化为Box-QP问题。本文首次提出了一个迭代复杂度为$O(\log(n))$的QP算法，尤其是其表现类似于“直接”方法：所需迭代次数是确定性的，精确值为$\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$。这一重大突破使得我们能够从$O(\sqrt{n})$的优化算法过渡到$O(\log(n))$的优化算法，其出色的可扩展性在当今的大数据和人工智能时代尤为重要。

    A "dark cloud" hangs over numerical optimization theory for decades, namely, whether an optimization algorithm $O(\log(n))$ iteration complexity exists. "Yes", this paper answers, with a new optimization algorithm and strict theory proof. It starts with box-constrained quadratic programming (Box-QP), and many practical optimization problems fall into Box-QP. Smooth quadratic programming (QP) and nonsmooth Lasso can be reformulated as Box-QP via duality theory. It is the first time to present an $O(\log(n))$ iteration complexity QP algorithm, in particular, which behaves like a "direct" method: the required number of iterations is deterministic with exact value $\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$. This significant breakthrough enables us to transition from the $O(\sqrt{n})$ to the $O(\log(n))$ optimization algorithm, whose amazing scalability is particularly relevant in today's era of big data and artificial intelligence.
    
[^68]: 基于大语言模型的中文细粒度金融情感分析

    Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v1 [cs.CL])

    [http://arxiv.org/abs/2306.14096](http://arxiv.org/abs/2306.14096)

    本文提出了一个用于企业预警的新型、广泛的中文细粒度金融情感分析数据集FinChina SA，并使用现有开源大语言模型对其进行评估和实验。该数据集将成为推进真实金融情感分析任务探索的宝贵资源。

    

    金融领域实体级别的细粒度情感分析是情感分析的重要子任务，目前面临着众多挑战。其中主要挑战之一来自于缺乏专门设计用于金融文本情感分析的高质量大规模标注语料库，这限制了开发有效文本处理技术所需的数据的可用性。大语言模型（LLMs）的最新进展在自然语言处理任务中取得了显著的性能，主要集中在语言模式匹配方面。在本文中，我们提出了一个新颖的、广泛的中文细粒度金融情感分析数据集FinChina SA，用于企业预警。我们对流行的现有开源LLMs使用我们的数据集进行了全面的评估和实验。我们坚信，我们的数据集将成为推动真实世界金融情感分析任务探索的宝贵资源。

    Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which shoul
    
[^69]: 适用于人工决策辅助的智能干预方案：既考虑准确性又兼顾时间性

    Adaptive interventions for both accuracy and time in AI-assisted human decision making. (arXiv:2306.07458v1 [cs.HC])

    [http://arxiv.org/abs/2306.07458](http://arxiv.org/abs/2306.07458)

    本研究探索适用于人工决策辅助的智能干预方案，在同时考虑准确性和时间性的前提下，根据问题和用户的属性自适应地展示AI辅助具有良好的效果。

    

    在需要高准确性但同时时间又紧迫的环境下，例如在急诊室工作的医生，我们希望提供人工智能辅助，既能提高准确性又能减少时间。但是，不同类型的人工智能功能带来的好处是不同的：一些能够减少时间，但会增加对人工智能的过度依赖，而其他一些则相反。因此，我们希望根据问题和用户的各种属性（如知识水平）来自适应地展示人工智能辅助，以便在准确性和时间性之间做出最佳权衡。我们通过一个用户需要为外星人开药方的研究来探索自适应AI辅助的潜力。我们发现根据问题自适应AI辅助是有益的，可以达到时间和准确性的良好平衡。未来的研究将考虑使用机器学习算法（如强化学习）来实现快速自适应。

    In settings where users are both time-pressured and need high accuracy, such as doctors working in Emergency Rooms, we want to provide AI assistance that both increases accuracy and reduces time. However, different types of AI assistance have different benefits: some reduce time taken while increasing overreliance on AI, while others do the opposite. We therefore want to adapt what AI assistance we show depending on various properties (of the question and of the user) in order to best tradeoff our two objectives. We introduce a study where users have to prescribe medicines to aliens, and use it to explore the potential for adapting AI assistance. We find evidence that it is beneficial to adapt our AI assistance depending on the question, leading to good tradeoffs between time taken and accuracy. Future work would consider machine-learning algorithms (such as reinforcement learning) to automatically adapt quickly.
    
[^70]: 高维和置换不变异常检测。

    High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])

    [http://arxiv.org/abs/2306.03933](http://arxiv.org/abs/2306.03933)

    该研究引入了一种置换不变的高维密度估计方法，通过学习后将其用于高能物理数据中的异常检测，能够有效地识别出在仅具备背景假设下排除异常的喷注。

    

    由于学习高维概率密度的困难，新物理过程的异常检测方法通常局限于低维空间。特别是在成分级别上，将置换不变性和可变长度的输入等良好性质合并到流行的密度估计方法中变得更加困难。在本研究中，我们引入了一种基于扩散模型的粒子物理数据置换不变密度估计器，专门设计用于处理可变长度的输入。我们通过将学习到的密度用作置换不变的异常检测评分来展示我们方法的功效，有效地识别出在仅具备背景假设下的可能性较低的喷注。为了验证我们的密度估计方法，我们研究了学习到的密度比与被监督分类算法获得的密度之间的比较。

    Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
    
[^71]: 具有因果亚结构的分子关系学习模型在数据分布变化时的鲁棒性

    Shift-Robust Molecular Relational Learning with Causal Substructure. (arXiv:2305.18451v1 [cs.LG])

    [http://arxiv.org/abs/2305.18451](http://arxiv.org/abs/2305.18451)

    本文提出了一种鲁棒性强的分子关系学习模型CMRL，通过检测与化学反应因果相关的核心亚结构来应对分子关系学习中的数据分布变化。

    

    最近，分子关系学习引起了分子科学领域的广泛关注，其目标是预测分子对之间的相互作用行为。本文提出了一种鲁棒性强的分子关系学习模型CMRL，它通过检测与化学反应因果相关的核心亚结构来应对分子关系学习中的数据分布变化。为此，我们首先假定基于分子科学领域知识的因果关系，并构建结构因果模型（SCM）来揭示变量之间的关系。基于SCM，我们引入了一个新的条件干预框架，其干预是基于成对分子条件的。使用条件干预框架，我们的模型成功地从因果亚结构中学习，并减轻了与化学反应虚假相关的快捷亚结构的混淆效应。本文在各种任务和真实和合成数据集上进行了广泛实验。

    Recently, molecular relational learning, whose goal is to predict the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. In this work, we propose CMRL that is robust to the distributional shift in molecular relational learning by detecting the core substructure that is causally related to chemical reactions. To do so, we first assume a causal relationship based on the domain knowledge of molecular sciences and construct a structural causal model (SCM) that reveals the relationship between variables. Based on the SCM, we introduce a novel conditional intervention framework whose intervention is conditioned on the paired molecule. With the conditional intervention framework, our model successfully learns from the causal substructure and alleviates the confounding effect of shortcut substructures that are spuriously correlated to chemical reactions. Extensive experiments on various tasks with real-world and sy
    
[^72]: 使用多任务学习增强抽取式摘要的连贯性

    Enhancing Coherence of Extractive Summarization with Multitask Learning. (arXiv:2305.12851v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12851](http://arxiv.org/abs/2305.12851)

    本研究提出了一种使用多任务学习架构来增强抽取式摘要的连贯性的方法。实验证明，该方法显著提高了抽取式摘要的连贯性，并在其他评价指标方面也表现出良好的性能。

    

    本研究提出了一种使用多任务学习架构来增强抽取式摘要的连贯性的方法。该架构包含一个抽取式摘要生成器和一个连贯性判别器模块。连贯性判别器通过在线训练增强了对输入句子连贯性的判断能力。同时，我们通过更新摘要生成器的参数来最大化连贯性判别器的连贯性分数。为了能够以可微分的方式训练抽取式摘要，我们引入了两种策略，包括预训练转换模型和转换矩阵方法，用于合并句子表示。实验证明，我们提出的方法显著提高了抽取式摘要中连贯句子在原始文章中按位置的比例（即自动句子级连贯度指标），而在其他自动评价指标方面也表现出良好的性能。

    This study proposes a multitask learning architecture for extractive summarization with coherence boosting. The architecture contains an extractive summarizer and coherent discriminator module. The coherent discriminator is trained online on the sentence vectors of the augmented textual input, thus improving its general ability of judging whether the input sentences are coherent. Meanwhile, we maximize the coherent scores from the coherent discriminator by updating the parameters of the summarizer. To make the extractive sentences trainable in a differentiable manner, we introduce two strategies, including pre-trained converting model (model-based) and converting matrix (MAT-based) that merge sentence representations. Experiments show that our proposed method significantly improves the proportion of consecutive sentences in the extracted summaries based on their positions in the original article (i.e., automatic sentence-level coherence metric), while the goodness in terms of other aut
    
[^73]: 对压迫矩阵的分解:揭示交织性在AI公平性中的作用的批判性回顾与再想象

    Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness. (arXiv:2303.17555v1 [cs.CY])

    [http://arxiv.org/abs/2303.17555](http://arxiv.org/abs/2303.17555)

    本文通过批判性回顾AI公平性文献中30篇交织性讨论，揭示研究人员普遍缺乏对交织性的整体理解，其一方面将其缩小为在群体子组上进行公平度量的优化，另一方面则在社会背景和权力结构的讨论方面存在欠缺。

    

    交织性是一个关键框架，通过调查和实践，它使我们能够检查社会不平等如何通过结构和纪律领域持续存在。在AI公平的理念中，“公平性”是至关重要的，我们认为采用交织性作为分析框架对于有效地实现公平至关重要。通过对AI公平文献中30篇关于交织性的讨论进行批判性回顾，我们归纳和演绎出:1)交织性指导如何在AI公平范例中操作，2)揭示交织性的概念化和实现之间的差距。我们发现，研究人员普遍将交织性缩减为针对人口亚组的公平指标进行优化。他们也未能讨论它们的社会背景，当提到权力时，他们主要将其置于AI流程中。我们将进一步阐述并评估这些差距对于临床研究和实践的影响。

    Intersectionality is a critical framework that, through inquiry and praxis, allows us to examine how social inequalities persist through domains of structure and discipline. Given AI fairness' raison d'\^etre of ``fairness,'' we argue that adopting intersectionality as an analytical framework is pivotal to effectively operationalizing fairness. Through a critical review of how intersectionality is discussed in 30 papers from the AI fairness literature, we deductively and inductively: 1) map how intersectionality tenets operate within the AI fairness paradigm and 2) uncover gaps between the conceptualization and operationalization of intersectionality. We find that researchers overwhelmingly reduce intersectionality to optimizing for fairness metrics over demographic subgroups. They also fail to discuss their social context and when mentioning power, they mostly situate it only within the AI pipeline. We: 3) outline and assess the implications of these gaps for critical inquiry and prax
    
[^74]: BoxSnake：使用框注释的多边形实例分割

    BoxSnake: Polygonal Instance Segmentation with Box Supervision. (arXiv:2303.11630v1 [cs.CV])

    [http://arxiv.org/abs/2303.11630](http://arxiv.org/abs/2303.11630)

    BoxSnake是一种新的端到端训练技术，可以仅使用框注释实现有效的多边形实例分割，相较于基于掩膜的弱监督方法，BoxSnake显示出显着的优越性。

    

    带框注释的实例分割因只需要简单的框标注而非昂贵的掩膜或多边形标注而引起了广泛关注。然而，现有的带框实例分割模型主要集中在基于掩膜的框架上。我们提出了一种新的端到端训练技术BoxSnake，首次仅使用框注释实现有效的多边形实例分割。我们的方法包括两个损失函数：（1）基于点的单元损失，约束预测多边形的边界框以实现粗略分割；（2）距离感知的成对损失，促使预测的多边形贴合物体边界。与基于掩膜的弱监督方法相比，BoxSnake进一步降低了预测分割与边界框之间的性能差距，并在Cityscapes数据集上表现出显着的优越性。

    Box-supervised instance segmentation has gained much attention as it requires only simple box annotations instead of costly mask or polygon annotations. However, existing box-supervised instance segmentation models mainly focus on mask-based frameworks. We propose a new end-to-end training technique, termed BoxSnake, to achieve effective polygonal instance segmentation using only box annotations for the first time. Our method consists of two loss functions: (1) a point-based unary loss that constrains the bounding box of predicted polygons to achieve coarse-grained segmentation; and (2) a distance-aware pairwise loss that encourages the predicted polygons to fit the object boundaries. Compared with the mask-based weakly-supervised methods, BoxSnake further reduces the performance gap between the predicted segmentation and the bounding box, and shows significant superiority on the Cityscapes dataset.
    
[^75]: 不变的槽注意力: 通过以槽为中心的参考框架进行对象发现

    Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames. (arXiv:2302.04973v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.04973](http://arxiv.org/abs/2302.04973)

    本文介绍了一种通过以槽为中心的参考框架来改进对象发现的方法，通过在Slot Attention中融入空间对称性，可以大幅提高数据效率和整体对象发现效果。

    

    从原始感知数据中自动发现可组合的抽象是机器学习中长期存在的挑战。最近，基于槽的神经网络以自我监督的方式学习对象在这个方向上取得了令人兴奋的进展。然而，它们通常在充分捕捉视觉世界中的空间对称性方面表现不佳，导致样本效率低下，比如在纠结对象外观和姿态时。在本文中，我们提出了一种简单但极其有效的方法，通过以槽为中心的参考框架来融入空间对称性。我们将等变性引入到Slot Attention的注意力和生成机制中，通过平移、缩放和旋转位置编码来实现对每个对象姿态变换的等变性。这些改变几乎没有额外的计算开销，易于实现，并可以在数据效率和整体对象发现方面取得巨大的改进。我们在广泛的语法范围内对我们的方法进行了评估。

    Automatically discovering composable abstractions from raw perceptual data is a long-standing challenge in machine learning. Recent slot-based neural networks that learn about objects in a self-supervised manner have made exciting progress in this direction. However, they typically fall short at adequately capturing spatial symmetries present in the visual world, which leads to sample inefficiency, such as when entangling object appearance and pose. In this paper, we present a simple yet highly effective method for incorporating spatial symmetries via slot-centric reference frames. We incorporate equivariance to per-object pose transformations into the attention and generation mechanism of Slot Attention by translating, scaling, and rotating position encodings. These changes result in little computational overhead, are easy to implement, and can result in large gains in terms of data efficiency and overall improvements to object discovery. We evaluate our method on a wide range of synt
    
[^76]: SpArX: 稀疏的神经网络论证解释

    SpArX: Sparse Argumentative Explanations for Neural Networks. (arXiv:2301.09559v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.09559](http://arxiv.org/abs/2301.09559)

    该论文提出了一种稀疏的神经网络论证解释方法SpArX，通过利用多层感知器和定量论证框架之间的关系，可以为神经网络的决策过程提供更忠实和深入的解释。

    

    神经网络在人工智能中有各种应用，但解释它们的决策仍然具有挑战性。现有方法通常关注解释改变单个输入如何影响神经网络的输出。然而，一个与神经网络的输入输出行为一致的解释未必忠实于其实际机制。在本文中，我们利用多层感知器和定量论证框架之间的关系，为多层感知器的机制创建了论证性解释。我们的SpArX方法首先将多层感知器稀疏化，同时保持尽可能多的原始结构。然后将稀疏的多层感知器转化为等效的定量论证框架，以揭示多层感知器的潜在决策过程，产生全局和/或局部解释。我们通过实验证明，SpArX比现有方法可以给出更忠实的解释，同时提供更深入的洞察实际推理过程。

    Neural networks (NNs) have various applications in AI, but explaining their decisions remains challenging. Existing approaches often focus on explaining how changing individual inputs affects NNs' outputs. However, an explanation that is consistent with the input-output behaviour of an NN is not necessarily faithful to the actual mechanics thereof. In this paper, we exploit relationships between multi-layer perceptrons (MLPs) and quantitative argumentation frameworks (QAFs) to create argumentative explanations for the mechanics of MLPs. Our SpArX method first sparsifies the MLP while maintaining as much of the original structure as possible. It then translates the sparse MLP into an equivalent QAF to shed light on the underlying decision process of the MLP, producing global and/or local explanations. We demonstrate experimentally that SpArX can give more faithful explanations than existing approaches, while simultaneously providing deeper insights into the actual reasoning process of M
    
[^77]: NusaCrowd：印尼自然语言处理资源的开源倡议

    NusaCrowd: Open Source Initiative for Indonesian NLP Resources. (arXiv:2212.09648v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09648](http://arxiv.org/abs/2212.09648)

    NusaCrowd是一个印尼自然语言处理资源的开源倡议，已汇集137个数据集和118个数据加载程序，为印尼语和印度尼西亚本地语言的自然语言处理研究提供了多种实验手段。

    

    我们提出了NusaCrowd，这是一个协作倡议，旨在收集和统一印尼语言的现有资源，包括开放以前非公开的资源。通过该倡议，我们汇集了137个数据集和118个标准化数据加载程序。数据集的质量已经经过手动和自动评估，它们的价值通过多个实验得到了证明。NusaCrowd的数据收集使得可以创建印尼语和印度尼西亚本地语言的零样本自然语言理解和生成基准，进一步推动了印尼语和印度尼西亚本地语言的多语言自动语音识别基准的创建。我们的工作致力于推进对在使用广泛的语言的自然语言处理（NLP）研究的发展，从而使之受到更多关注。

    We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the datasets has been assessed manually and automatically, and their value is demonstrated through multiple experiments. NusaCrowd's data collection enables the creation of the first zero-shot benchmarks for natural language understanding and generation in Indonesian and the local languages of Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual automatic speech recognition benchmark in Indonesian and the local languages of Indonesia. Our work strives to advance natural language processing (NLP) research for languages that are under-represented despite being widely spoken.
    
[^78]: 证据显示自监督学习中的声道发音

    Evidence of Vocal Tract Articulation in Self-Supervised Learning of Speech. (arXiv:2210.11723v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2210.11723](http://arxiv.org/abs/2210.11723)

    这项研究分析了一系列自监督学习模型，通过线性探测方法将语音表示与发音轨迹相联系。结果显示声道发音在自监督学习中发挥了重要作用。

    

    最近的自监督学习模型已经证明能够学习到丰富的语音表示，这些表示可以方便地被各种下游任务利用。为了理解这种效用，对语音自监督学习模型进行了各种分析，以揭示学习表示中的信息是哪些以及如何编码的。尽管先前的分析范围涵盖了声学、音位和语义等方面，但对语音产生的物理基础的关注还不够。为了弥补这一差距，我们进行了一项全面的分析，将语音表示与电磁装置记录的发音轨迹相联系。我们的分析基于线性探测方法，其中我们以EMA线性映射的平均相关性测量发音得分。我们分析了SUPERB基准测试排行榜中选择的一组自监督学习模型，并对最成功的两个模型Wav2Vec 2.0和HuBERT进行了更进一步的逐层分析。

    Recent self-supervised learning (SSL) models have proven to learn rich representations of speech, which can readily be utilized by diverse downstream tasks. To understand such utilities, various analyses have been done for speech SSL models to reveal which and how information is encoded in the learned representations. Although the scope of previous analyses is extensive in acoustic, phonetic, and semantic perspectives, the physical grounding by speech production has not yet received full attention. To bridge this gap, we conduct a comprehensive analysis to link speech representations to articulatory trajectories measured by electromagnetic articulography (EMA). Our analysis is based on a linear probing approach where we measure articulatory score as an average correlation of linear mapping to EMA. We analyze a set of SSL models selected from the leaderboard of the SUPERB benchmark and perform further layer-wise analyses on two most successful models, Wav2Vec 2.0 and HuBERT. Surprisingl
    
[^79]: 一种简单的联邦灾难援助政策声明性模型——透明度的建模和测量。

    A simple declarative model of the Federal Disaster Assistance Policy -- modelling and measuring transparency. (arXiv:2207.07392v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2207.07392](http://arxiv.org/abs/2207.07392)

    本研究量化分析了联邦灾难援助政策的一个简单模型，并从三个不同利益相关者的角度进行了评估，进而考虑了3种政策修改及其影响，旨在为各利益相关者的偏好排序提供参考。

    

    本文从三个不同利益相关者的角度，对联邦灾难援助政策的一个简单模型进行了数量分析。这种数量分析方法是新的，在其他领域如业务和医疗流程中也有应用。利益相关者对过程透明度很感兴趣，但每个人对透明度的具体定义都有不同的看法。我们还考虑了三种联邦灾难援助政策的修改，并分析了从利益相关者的角度看，每种政策下利益相关者满意度的变化情况。这种分析被用来对四种政策的偏好进行排序，以便考虑到所有集体利益相关者的偏好。

    In this paper we will provide a quantitative analysis of a simple model of the Federal Disaster Assistance policy from the viewpoint of three different stakeholders. This quantitative methodology is new and has applications to other areas such as business and healthcare processes. The stakeholders are interested in process transparency but each has a different opinion on precisely what constitutes transparency. We will also consider three modifications to the Federal Disaster Assistance policy and analyse, from a stakeholder viewpoint, how stakeholder satisfaction changes from process to process. This analysis is used to rank the favourability of four policies with respect to all collective stakeholder preferences.
    
[^80]: 前馈神经网络中的活动-权重对偶性：泛化性的几何决定因素

    The activity-weight duality in feed forward neural networks: The geometric determinants of generalization. (arXiv:2203.10736v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.10736](http://arxiv.org/abs/2203.10736)

    这项研究发现了在前馈神经网络中，神经元活动的变化与连接到下一层神经元的权重变化之间的准确对偶关系。通过这种对偶性，我们能够将输入数据的变化映射到对应的权重变化，并发现泛化损失可以通过解的损失函数的Hessian矩阵的特征方向的几何因子的乘积来表示。

    

    机器学习中一个基本的问题是泛化性。在具有大量权重（参数）的神经网络模型中，可以找到很多解来很好地拟合训练数据。关键问题是哪个解能够描述不在训练集中的测试数据。在这里，我们报告了在任何前馈神经网络的密集连接层中，给定层神经元活动的变化与连接到下一层神经元的权重变化之间的确切对偶（等价）关系的发现。活动-权重（A-W）对偶性使我们能够将输入（数据）的变化映射到相应的对偶权重的变化。通过使用这种映射，我们表明泛化损失可以分解为在权重空间中的解的损失函数的Hessian矩阵的不同特征方向的贡献之和。给定特征方向的贡献是两个几何因子（行列式）的乘积：尖锐度

    One of the fundamental problems in machine learning is generalization. In neural network models with a large number of weights (parameters), many solutions can be found to fit the training data equally well. The key question is which solution can describe testing data not in the training set. Here, we report the discovery of an exact duality (equivalence) between changes in activities in a given layer of neurons and changes in weights that connect to the next layer of neurons in a densely connected layer in any feed forward neural network. The activity-weight (A-W) duality allows us to map variations in inputs (data) to variations of the corresponding dual weights. By using this mapping, we show that the generalization loss can be decomposed into a sum of contributions from different eigen-directions of the Hessian matrix of the loss function at the solution in weight space. The contribution from a given eigen-direction is the product of two geometric factors (determinants): the sharpn
    
[^81]: 使用因子图学习表格强化学习的多智能体技能

    Learning Multi-agent Skills for Tabular Reinforcement Learning using Factor Graphs. (arXiv:2201.08227v3 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2201.08227](http://arxiv.org/abs/2201.08227)

    本论文提出了一个方法可以直接在多智能体场景中计算多智能体技能，通过智能体之间的合作性探索行为来改善联合状态空间的连通性。

    

    技能发现被开发用于改善单智能体情景中稀疏奖励信号的强化学习探索能力，通过连接状态转移图的Fiedler向量提供的嵌入空间中最远的状态。然而，这些技能发现方法无法直接推广到多智能体场景，因为系统中的智能体数量增加，联合状态空间呈指数增长。因此，现有研究在多智能体场景中采用技能仍然依赖于单智能体技能发现，并未直接发现能够改善智能体联合状态空间连通性的联合技能。在本文中，我们展示了使用合作性探索行为在智能体之间直接计算多智能体技能的可行性，同时仍然享受分解的便利性。我们的关键思想是将联合状态空间逼近为Kronecker图——Kronecker

    Covering skill (a.k.a., option) discovery has been developed to improve the exploration of reinforcement learning in single-agent scenarios with sparse reward signals, through connecting the most distant states in the embedding space provided by the Fiedler vector of the state transition graph. However, these option discovery methods cannot be directly extended to multi-agent scenarios, since the joint state space grows exponentially with the number of agents in the system. Thus, existing researches on adopting options in multi-agent scenarios still rely on single-agent option discovery and fail to directly discover the joint options that can improve the connectivity of the joint state space of agents. In this paper, we show that it is indeed possible to directly compute multi-agent options with collaborative exploratory behaviors among the agents, while still enjoying the ease of decomposition. Our key idea is to approximate the joint state space as a Kronecker graph -- the Kronecker 
    

