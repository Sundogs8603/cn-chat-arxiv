# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models](https://arxiv.org/abs/2403.15388) | PruMerge提出了一种自适应的视觉令牌减少方法，可以有效减少大型多模态模型中的视觉令牌数量，同时保持模型性能。 |
| [^2] | [LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis](https://arxiv.org/abs/2403.15385) | LATTE3D通过构建可扩展的架构、利用3D数据并采用摊还方法，在显著更大的提示集上实现快速、高质量的文本增强3D合成。 |
| [^3] | [Can large language models explore in-context?](https://arxiv.org/abs/2403.15371) | 研究发现，大型语言模型在没有实质干预的情况下很难有效进行探索，除了特定配置下的GPT-4具有满意的探索行为外，其他模型表现不稳定。 |
| [^4] | [CoLLEGe: Concept Embedding Generation for Large Language Models](https://arxiv.org/abs/2403.15362) | CoLLEGe是一个元学习框架，能够为大型语言模型生成灵活的新概念嵌入，用于现代化少样本概念学习。 |
| [^5] | [Collaborative AI Teaming in Unknown Environments via Active Goal Deduction](https://arxiv.org/abs/2403.15341) | 通过积极目标推断，利用预训练的策略，实现零射击策略适应，对未知代理人进行最佳团队合作。 |
| [^6] | [A Technological Perspective on Misuse of Available AI](https://arxiv.org/abs/2403.15325) | 潜在的对民用人工智能的恶意滥用对国家和国际安全构成严重威胁，研究展示了免费可用的AI如何被结合成自主武器系统，并提出控制点和进一步措施以防止潜在的威胁。 |
| [^7] | [Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection](https://arxiv.org/abs/2403.15317) | 提出了Point-DETR3D，一个师生框架用于弱监督半监督3D检测，充分利用点级监督优势，克服了将弱监督3D先验信息编码到模型中的挑战。 |
| [^8] | [CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking](https://arxiv.org/abs/2403.15313) | CR3DT是一个相机与雷达融合模型，结合了雷达在3D检测和跟踪中的潜力，通过在State-of-the-Art相机架构基础上实现了显著的改进。 |
| [^9] | [KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing](https://arxiv.org/abs/2403.15304) | KTbench提出了一种无数据泄漏的知识追踪框架，解决了KT模型中KC之间的相关性学习可能导致的性能下降问题。 |
| [^10] | [Planning with a Learned Policy Basis to Optimally Solve Complex Tasks](https://arxiv.org/abs/2403.15301) | 使用继承特征学习策略基础，使每个（子）策略解决一个子问题，在FSA描述的任务中，组合这些（子）策略可用于无需额外学习生成最优解决方案，方法能够渐近达到全局最优性，即使在随机环境中也如此。 |
| [^11] | [Sphere Neural-Networks for Rational Reasoning](https://arxiv.org/abs/2403.15297) | 该论文提出了一种球形神经网络（SphNNs）来进行理性推理，通过将计算构建块从向量推广到球体，实现了人类类似的推理能力，并开发了用于三段论推理的SphNN。 |
| [^12] | [Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review](https://arxiv.org/abs/2403.15274) | 该研究调查了在生物信息学和生物医学信息学领域应用ChatGPT的情况，总结了其当前优势和局限性，并为未来发展提供了一些见解。 |
| [^13] | [Hierarchical Information Enhancement Network for Cascade Prediction in Social Networks](https://arxiv.org/abs/2403.15257) | 提出了面向社交网络级联预测的分层信息增强网络（HIENet），将基本级联序列、用户社交图和子级联图集成到一个统一的框架中，有效地挖掘了不同模态之间的层级语义关联。 |
| [^14] | [Safe Learning of PDDL Domains with Conditional Effects -- Extended Version](https://arxiv.org/abs/2403.15251) | 学习具有条件效果的非平凡安全行动模型可能需要指数数量的样本，我们提出了SAM Learning of Conditional Effects (Conditional-SAM)算法来解决这一问题。 |
| [^15] | [Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach](https://arxiv.org/abs/2403.15250) | 评估大规模LLM中因素对性能的影响通过全面的统计分析，有助于更好地理解和推动这些模型的发展 |
| [^16] | [Spectral Motion Alignment for Video Motion Transfer using Diffusion Models](https://arxiv.org/abs/2403.15249) | 提出了一种名为Spectral Motion Alignment（SMA）的新框架，通过傅立叶和小波变换来优化和对齐运动向量，学习整帧全局运动动态，减轻空间伪影，有效改善运动转移。 |
| [^17] | [Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks](https://arxiv.org/abs/2403.15248) | 通过自监督学习，该研究利用SimCLR对农业图像进行预训练，成功应用于多样化的农业视觉任务，消除了大规模带标注数据集的需求。 |
| [^18] | [Reasoning-Enhanced Object-Centric Learning for Videos](https://arxiv.org/abs/2403.15245) | 设计了一种新颖的推理模块STATM，利用记忆缓冲区增强模型在复杂场景中的感知能力。 |
| [^19] | [Multi-perspective Memory Enhanced Network for Identifying Key Nodes in Social Networks](https://arxiv.org/abs/2403.15235) | 提出了一个多角度记忆增强网络（MMEN），用于在社交网络中识别关键节点，通过多角度挖掘关键节点并利用记忆网络存储历史信息，以增强模型在未知情境中的泛化能力。 |
| [^20] | [Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations](https://arxiv.org/abs/2403.15218) | 该研究探讨了使用部分任意模型（SAM）在众包环境中从非专家处策划医学图像标注的可行性，以生成用于训练3D DL分割模型的"密集"分割掩模。 |
| [^21] | [(Un)making AI Magic: a Design Taxonomy](https://arxiv.org/abs/2403.15216) | 通过构建设计方法的分类学，研究了AI事物设计中魔法的影响，提出了七大设计原则，并探讨了这些原则对魅力和喜忧无常的影响。 |
| [^22] | [FSD-Inference: Fully Serverless Distributed Inference with Scalable Cloud Communication](https://arxiv.org/abs/2403.15195) | FSD-Inference是第一个完全无服务器且高度可扩展的分布式ML推断系统，引入了全新的无服务器通信方案。 |
| [^23] | [SFOD: Spiking Fusion Object Detector](https://arxiv.org/abs/2403.15192) | SFOD是一种简单而高效的基于脉冲神经网络的目标检测方法，通过设计脉冲融合模块，实现了首次在事件相机中对不同尺度的特征图进行融合。 |
| [^24] | [Brain-grounding of semantic vectors improves neural decoding of visual stimuli](https://arxiv.org/abs/2403.15176) | 提出了一种表示学习框架，称为语义向量的脑接地，通过微调预训练的特征向量，使其更好地与人类大脑中视觉刺激的神经表示对齐。 |
| [^25] | [Exploring the Task-agnostic Trait of Self-supervised Learning in the Context of Detecting Mental Disorders](https://arxiv.org/abs/2403.15170) | 本研究探索了在检测主要抑郁症和创伤后应激障碍时，在交互会话期间收集的音频和视频数据上使用自监督学习的任务无关表示。 |
| [^26] | [Transition Graph Properties of Target Class Classification](https://arxiv.org/abs/2403.15167) | 目标类分类的关键在于转换图的属性，研究表明理想的转换图结构是朝根顶点方向取向的有根树。 |
| [^27] | [Modular Deep Active Learning Framework for Image Annotation: A Technical Report for the Ophthalmo-AI Project](https://arxiv.org/abs/2403.15143) | 提出了MedDeepCyleAL，一个实现完整主动学习周期的端到端框架，为研究人员提供了灵活选择深度学习模型类型的可能性。 |
| [^28] | [CACA Agent: Capability Collaboration based AI Agent](https://arxiv.org/abs/2403.15137) | 提出了CACA代理，采用基于能力协作的开放架构，整合了一组协作能力来实现AI代理，增强了AI代理的规划能力和可扩展性。 |
| [^29] | [Language Models in Dialogue: Conversational Maxims for Human-AI Interactions](https://arxiv.org/abs/2403.15115) | 提出了一组最大化准则，用于描述有效的人机对话，包括传统的 Grice 四个最大化准则以及两个新准则，对于解决现代人机互动中的特殊行为问题。 |
| [^30] | [Solving a Real-World Package Delivery Routing Problem Using Quantum Annealers](https://arxiv.org/abs/2403.15114) | 这项研究开发了一种量子-经典混合求解器 Q4RPD，旨在解决现实世界的包裹投递路径问题，避免了问题简化和技术捷径，针对包裹重量和尺寸的真实约束条件。 |
| [^31] | [Text clustering with LLM embeddings](https://arxiv.org/abs/2403.15112) | 研究表明，LLM嵌入能够捕捉结构化语言的细微差别，BERT在性能上领先于轻量级选项，增加嵌入维度和摘要技术并不一致地提高聚类效率 |
| [^32] | [Subequivariant Reinforcement Learning Framework for Coordinated Motion Control](https://arxiv.org/abs/2403.15100) | 使用子等变原理的CoordiGraph框架在强化学习中增强协调运动控制，改善了关节之间的关系建模，提高了泛化能力和样本效率。 |
| [^33] | [Argument-Aware Approach To Event Linking](https://arxiv.org/abs/2403.15097) | 引入论据感知方法改进事件链接模型，能更好地识别和分类不在知识库中的事件提及，弥补了这一领域的研究空白。 |
| [^34] | [Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning](https://arxiv.org/abs/2403.15091) | 改进长短期记忆污水处理模拟器，解决DRL优化工业过程中的挑战，通过减少复合误差提高模型的准确性 |
| [^35] | [Comprehensive Lipidomic Automation Workflow using Large Language Models](https://arxiv.org/abs/2403.15076) | 该论文提出了使用大型语言模型的全面脂质组学自动化工作流程，该工作流程包括自动生成工作流程、详细统计分析和脂质标注，以及在气泡电喷雾电离-多反应监测方法中识别不饱和脂质碳碳双键位置的模块化方法。 |
| [^36] | [Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation](https://arxiv.org/abs/2403.15075) | 提出了一种名为双侧不对称图对比学习（BusGCL）的框架，通过考虑用户-项目节点关系密度的双侧不对称性，实现了更好的推理用户和项目图。 |
| [^37] | [MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition Integration](https://arxiv.org/abs/2403.15059) | 提出了一种名为MM-Diff的统一且免调优的图像个性化框架，能够在几秒内生成高保真的单个和多个主体图像，并利用视觉编码器同时增强文本一致性和主题保真度。 |
| [^38] | [Continual Vision-and-Language Navigation](https://arxiv.org/abs/2403.15049) | 该论文提出了持续视觉和语言导航（CVLN）范式，旨在解决现有训练VLN代理方法固有的固定数据集的重大限制，使代理能够在不断变化的真实世界中进行导航。 |
| [^39] | [Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning](https://arxiv.org/abs/2403.15048) | 该研究提出了一种用于检测由TTI模型生成的卡通角色图像中视觉幻觉的系统，通过结合姿势感知上下文视觉学习和视觉语言模型，利用RGB图像和姿势信息，实现了更准确的决策，显著提高了视觉幻觉的识别能力，推动了TTI模型在非照片真实领域的发展。 |
| [^40] | [Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour Analysis In-the-wild](https://arxiv.org/abs/2403.15044) | 结合多模态融合方法和预训练模型特征，在野外情感行为分析中取得出色性能 |
| [^41] | [Grey-informed neural network for time-series forecasting](https://arxiv.org/abs/2403.15027) | 本研究提出了灰色信息神经网络（GINN），通过遵循灰色系统的微分方程模型，提高了神经网络输出的可解释性，使其能够有效处理小数据样本，产生可靠的预测。 |
| [^42] | [Magic for the Age of Quantized DNNs](https://arxiv.org/abs/2403.14999) | 提出了一种量化感知训练方法，引入新型标准化方法并使用缩放量化加权，实现了在最小精度降级的情况下有效的量化深度神经网络 |
| [^43] | [Piecewise-Linear Manifolds for Deep Metric Learning](https://arxiv.org/abs/2403.14977) | 提出了一种在深度度量学习中使用分段线性流形的方法，并通过模拟高维数据流形来改善相似性估计，从而提高了无监督度量学习的性能。 |
| [^44] | [A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning](https://arxiv.org/abs/2403.14972) | 提出了一种演绎式的图谱辩论方法（BDoG），在多模态推理中防止意见陈腐化和减少由图像引入的分心概念，实验证明其在科学问答和MMBench上取得了最先进的结果。 |
| [^45] | [Comprehensive Evaluation and Insights into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation](https://arxiv.org/abs/2403.14965) | 本文提出了一种利用大型语言模型自动化生成BDD验收测试的新方法，并通过使用GPT-3.5和GPT-4等模型，展示了其在提高准确性和增强BDD实践方面的有效性。 |
| [^46] | [Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation](https://arxiv.org/abs/2403.14952) | 提出了一种基于检索增强的响应生成方法(RARG)，通过收集科学来源的证据来生成反虚假信息的响应，相比现有方法，可以提高文本质量和避免过度重复。 |
| [^47] | [Simple Graph Condensation](https://arxiv.org/abs/2403.14951) | 提出了一种简化的图压缩方法，旨在减少图神经网络所带来的不必要复杂性。 |
| [^48] | [A Single Linear Layer Yields Task-Adapted Low-Rank Matrices](https://arxiv.org/abs/2403.14946) | 通过研究转换矩阵将$ W_0 $转换为低秩矩阵的关系信息，我们提出单一线性层可以生成任务自适应的低秩矩阵。 |
| [^49] | [Unifying Lane-Level Traffic Prediction from a Graph Structural Perspective: Benchmark and Baseline](https://arxiv.org/abs/2403.14941) | 本文提出了一个简单的基线模型GraphMLP，基于图结构和MLP网络，在车道级交通预测中建立了统一的空间拓扑结构和预测任务，帮助突破了现有评估标准和数据公开性的限制。 |
| [^50] | [Attention-Driven Reasoning: Unlocking the Potential of Large Language Models](https://arxiv.org/abs/2403.14932) | 通过注意力机制优化，可以显著提高大型语言模型的推理能力，尤其对于非STEM问题。 |
| [^51] | [Hierarchical Skip Decoding for Efficient Autoregressive Text Generation](https://arxiv.org/abs/2403.14919) | 提出了一种名为Hierarchical Skip Decoding（HSD）的新型解码策略，用于高效的自回归文本生成，通过分层地自适应跳过解码层来减少计算负载和分配计算资源。 |
| [^52] | [Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning](https://arxiv.org/abs/2403.14895) | Stance Reasoner是一种利用显式推理和世界知识进行零-shot社交媒体立场检测的方法，优于当前领先模型，并能更好地横跨目标进行泛化。 |
| [^53] | [AutoRE: Document-Level Relation Extraction with Large Language Models](https://arxiv.org/abs/2403.14888) | AutoRE 是一种端到端的文档级关系抽取模型，采用了一种名为RHF的新颖关系抽取范式，可有效处理分布在文档中的多个关系和三元组事实。 |
| [^54] | [Establishing a leader in a pairwise comparisons method](https://arxiv.org/abs/2403.14885) | 该论文展示了两种算法，可以在成对比较方法中实现操纵攻击，进而选择领导者，并通过模拟分析展示了PC矩阵大小、不一致程度和操纵轻松程度之间的关系。 |
| [^55] | [Learning Quadruped Locomotion Using Differentiable Simulation](https://arxiv.org/abs/2403.14864) | 本文提出了一种新的可微分仿真框架，通过将复杂的全身仿真解耦为两个单独的连续域，并与更精确的模型对齐，来克服四足动作中的不连续性挑战。 |
| [^56] | [Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models](https://arxiv.org/abs/2403.14859) | 通过比较基础和指令调优的大型语言模型在英语句子可信度任务中的表现，发现对数似然（LL）分数是最可靠的句子可信度指标，但仍低于人类表现。 |
| [^57] | [Local Causal Discovery with Linear non-Gaussian Cyclic Models](https://arxiv.org/abs/2403.14843) | 提出一种通用的、统一的局部因果发现方法，使用线性非高斯模型，实现了从目标变量的马尔可夫毯中精确识别等效的局部有向结构和因果强度 |
| [^58] | [Crowdsourced Multilingual Speech Intelligibility Testing](https://arxiv.org/abs/2403.14817) | 本研究提出了一种众包多语言语音可懂度评估方法，通过收集和公开发布多语言语音数据，解决了现有实验室测量昂贵且不易扩展的问题。 |
| [^59] | [The opportunities and risks of large language models in mental health](https://arxiv.org/abs/2403.14814) | 大型语言模型在心理健康领域有望提供新颖的解决方案，但应注意其应用可能带来的风险，并积极采取策略减轻这些风险。 |
| [^60] | [Deep Active Learning: A Reality Check](https://arxiv.org/abs/2403.14800) | 深度主动学习方法的全面评估发现在一般情况下，没有单一模型方法能明显优于基于熵的主动学习，同时揭示了起始预算、预算步长和预训练等因素对取得优越结果的重要性，并拓展了在其他任务中的应用。 |
| [^61] | [Planning and Acting While the Clock Ticks](https://arxiv.org/abs/2403.14796) | 提出了并发规划和执行的新问题设置，允许在规划终止之前派发动作，适用于一些时间压力较大的情况。 |
| [^62] | [Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits](https://arxiv.org/abs/2403.14791) | Particip-AI 是一个框架，旨在通过从非专业公众那里收集当前和未来的人工智能使用情况、危害和益处，引领人工智能的民主发展。 |
| [^63] | [Latent Diffusion Models for Attribute-Preserving Image Anonymization](https://arxiv.org/abs/2403.14790) | 这项研究提出了基于潜在扩散模型的图像匿名化方法，通过保留场景中的每个元素传达相同意义，但使得重新识别变得困难。 |
| [^64] | [Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering](https://arxiv.org/abs/2403.14783) | 本研究提出了一种自适应多智体系统，名为多智体VQA，通过使用专门的智体工具，克服了基础模型在目标检测和计数中的局限性，在零样本情况下实现了良好的性能，为未来研究提供了新的方向。 |
| [^65] | [StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text](https://arxiv.org/abs/2403.14773) | StreamingT2V是一种自回归方法，用于生成长视频，可以产生80、240、600、1200帧甚至更多帧的视频，并具有平滑的过渡。 |
| [^66] | [Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures](https://arxiv.org/abs/2403.14772) | 通过稀疏编码层设计新网络架构以提高对模型逆推攻击的鲁棒性。 |
| [^67] | [NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein Classification in Graph Neural Networks](https://arxiv.org/abs/2403.14736) | 提出了NaNa和MiGu两种语义数据增强方法，结合了蛋白质的主链化学和侧链生物物理信息，用于增强图神经网络中的蛋白质分类任务。 |
| [^68] | [A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond](https://arxiv.org/abs/2403.14734) | 神经代码智能领域的调查系统回顾了50多种代表性模型和超过680项相关作品，突出了不同研究阶段的范式和技术转变。 |
| [^69] | [Open Knowledge Base Canonicalization with Multi-task Learning](https://arxiv.org/abs/2403.14733) | 提出了一个多任务学习框架MulCanon来处理开放知识库（OKB）规范化问题，并通过在软聚类过程中使用扩散模型来改进名词短语的表示。 |
| [^70] | [Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It](https://arxiv.org/abs/2403.14715) | LS方法在深度神经网络分类器训练中的标签平滑效果被发现会负面影响选择性分类，通过影响模型预测不确定性，此研究阐明了这一现象。 |
| [^71] | [AI for bureaucratic productivity: Measuring the potential of AI to help automate 143 million UK government transactions](https://arxiv.org/abs/2403.14712) | 人工智能在自动化英国政府1.43亿笔交易中展现巨大潜力，节省每笔复杂交易一分钟的时间即可节约大量工作时间。 |
| [^72] | [Human-in-the-Loop AI for Cheating Ring Detection](https://arxiv.org/abs/2403.14711) | 本文介绍了一种人在环AI作弊环检测系统，通过设计原则、评估方法及符合负责任AI标准，实现了检测作弊者的目标。 |
| [^73] | [Use of recommendation models to provide support to dyslexic students](https://arxiv.org/abs/2403.14710) | 使用AI推荐模型为诵读障碍学生提供高度个性化的支持工具，以提供有针对性的实用帮助。 |
| [^74] | [Safeguarding Marketing Research: The Generation, Identification, and Mitigation of AI-Fabricated Disinformation](https://arxiv.org/abs/2403.14706) | 人工智能的生成能力带来了对营销研究的威胁，本研究展示了其在误导性用户生成内容方面的熟练程度，量化了其对营销研究的扰乱影响，并提出了高级检测框架。 |
| [^75] | [Concept-Best-Matching: Evaluating Compositionality in Emergent Communication](https://arxiv.org/abs/2403.14705) | 提出了一种评估新兴通信组合性的方法，通过找到 emerged words 与 natural language concepts 之间的最佳匹配，实现了直接而可解释的映射。 |
| [^76] | [A minimal coalition logic](https://arxiv.org/abs/2403.14704) | 提出了一个基于一般并发博弈模型的最小联盟逻辑，不具备传统模型中的独立性、序列性和确定性假设，展示了其完备性并与传统模型进行了比较 |
| [^77] | [An AIC-based approach for articulating unpredictable problems in open complex environments](https://arxiv.org/abs/2403.14697) | 该研究论文提出了一种基于人工智能的方法，旨在通过采用系统方法来提高建筑师在设计可靠系统方面的预测能力，重点关注动态和不可预测环境下系统的操作。 |
| [^78] | [Application of GPT Language Models for Innovation in Activities in University Teaching](https://arxiv.org/abs/2403.14694) | GPT语言模型在大学教学活动创新中的应用不仅可以支持理解和生成内容、问题解决，还可以在个性化和测试纠错等方面提供帮助，但是在国际化方面需注意避免其误用导致的全球问题。 |
| [^79] | [A2CI: A Cloud-based, Service-oriented Geospatial Cyberinfrastructure to Support Atmospheric Research](https://arxiv.org/abs/2403.14693) | 本论文介绍了一个基于云的、面向服务的地理空间下层结构A2CI，旨在支持大气研究，能有效应对收集和整理的大量地球科学数据所带来的挑战。 |
| [^80] | [The AI Assessment Scale (AIAS) in action: A pilot implementation of GenAI supported assessment](https://arxiv.org/abs/2403.14692) | 该论文介绍了AI评估量表（AIAS）的实践应用，通过灵活框架将GenAI技术纳入教育评估中，显著降低了与GenAI相关的学术不端案件，提高了学生的学业成绩。 |
| [^81] | [Large Language Models and User Trust: Focus on Healthcare](https://arxiv.org/abs/2403.14691) | 本文探讨了临床医生对LLMs的信任、数据来源从主要是人类生成到人工智能生成内容的转变，以及随之而来对LLMs精确性和临床医师能力的影响之间的不断发展的关系，强调了LLMs在医疗保健中的整合加深可能带来的挑战和风险。 |
| [^82] | [Incorporating Graph Attention Mechanism into Geometric Problem Solving Based on Deep Reinforcement Learning](https://arxiv.org/abs/2403.14690) | 提出了基于深度强化学习的图注意机制，用于自动且高效地添加几何问题中的辅助组件 |
| [^83] | [Developing and Deploying Industry Standards for Artificial Intelligence in Education (AIED): Challenges, Strategies, and Future Directions](https://arxiv.org/abs/2403.14689) | 教育领域人工智能的发展需要制定和实施产业标准，以解决互操作性、可扩展性和道德治理等挑战。 |
| [^84] | [On the Performance of Imputation Techniques for Missing Values on Healthcare Datasets](https://arxiv.org/abs/2403.14687) | 本研究比较了七种填补技术在健康数据集上的性能，结果显示... |
| [^85] | [A Moral Imperative: The Need for Continual Superalignment of Large Language Models](https://arxiv.org/abs/2403.14683) | 实现终身超对齐需要对当前大型语言模型架构进行重大变革，以解决其在理解和适应动态人类道德和不断发展的全球情景方面的局限性。 |
| [^86] | [Deep Generative Domain Adaptation with Temporal Relation Knowledge for Cross-User Activity Recognition](https://arxiv.org/abs/2403.14682) | 该研究引入了一种CVAE-USM方法，通过放松独立同分布假设和利用时间关系，有效地在不同用户之间对齐数据分布，从而改进活动识别。 |
| [^87] | [AI Ethics: A Bibliometric Analysis, Critical Issues, and Key Gaps](https://arxiv.org/abs/2403.14681) | 通过对过去二十年的AI伦理学文献进行全面计量分析，揭示了AI伦理研究的发展三部曲，并提出了七个关键AI伦理问题，同时指出了关于大伦理模型（LEM）和AI识别与扩展的两个研究空白。 |
| [^88] | [Trust in AI: Progress, Challenges, and Future Directions](https://arxiv.org/abs/2403.14680) | 人工智能中的信任是控制其传播程度的调节器，通过增加信任和减少不信任，可以显著影响人工智能的采用速度。 |
| [^89] | [Unified Uncertainty Estimation for Cognitive Diagnosis Models](https://arxiv.org/abs/2403.14676) | 提出了一种统一不确定性评估方法，适用于各种认知诊断模型，填补了对于具有交互功能参数的复杂模型的学术空白 |
| [^90] | [Predicting Learning Performance with Large Language Models: A Study in Adult Literacy](https://arxiv.org/abs/2403.14668) | 该研究使用大型语言模型GPT-4探讨了在ITS中预测成人识字计划学习表现的应用，并发现GPT-4在此方面具有竞争力的预测能力。 |
| [^91] | [Case Studies of AI Policy Development in Africa](https://arxiv.org/abs/2403.14662) | 非洲国家在AI准备方面取得的进展没有完全被全球准备情况评估捕捉到，通过对四个非洲国家进行案例研究，提出了如何改善国家的AI准备标准以及如何使社会能够获益于AI的高层政策考虑。 |
| [^92] | [Machina Economicus: A New Paradigm for Prosumers in the Energy Internet of Smart Cities](https://arxiv.org/abs/2403.14660) | 在智能城市能源互联网中，提出了一个新的模式——经济机器人，旨在研究在资源优化、信息交换和交互协议等方面，实现prosumers的经济合理性，以建立一个高效、经济、社会最优的能源分享平台。 |
| [^93] | [Social Intelligence Data Infrastructure: Structuring the Present and Navigating the Future](https://arxiv.org/abs/2403.14659) | 本研究构建了一个名为Social AI Data Infrastructure的社会智能数据基础设施，包括一个全面的社交AI分类系统和一个480个NLP数据集的数据库，通过分析现有数据集工作以及评估语言模型在不同社会智能方面的表现，帮助研究者深入了解当前数据格局并提供未来数据集发展方向的整体观点。 |
| [^94] | [Identifying Potential Inlets of Man in the Artificial Intelligence Development Process](https://arxiv.org/abs/2403.14658) | 该论文旨在探讨典型人工智能开发过程如何引导创造出种族化技术，以及如何更好理解这些技术如何强化对黑人的排斥。 |
| [^95] | [MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation](https://arxiv.org/abs/2403.14652) | MemeCraft是一款创新的模因生成器，利用大型语言模型和视觉语言模型生成支持特定社会运动的模因，提供端到端的流程，无需人工干预，带有内在安全机制。 |
| [^96] | [Harnessing the Computing Continuum across Personalized Healthcare, Maintenance and Inspection, and Farming 4.0](https://arxiv.org/abs/2403.14650) | AI-SPRINT项目专注于开发和实施横跨计算连续性的人工智能应用，在个性化医疗、维护和检验以及农业4.0领域取得了重大的科学进展。 |
| [^97] | [Designing Multi-Step Action Models for Enterprise AI Adoption](https://arxiv.org/abs/2403.14645) | 本文介绍了Empsing设计的多步骤行动模型(MSAM)，旨在解决企业采用AI所面临的挑战，通过全面探讨设计原则、架构和未来发展，以及评估其性能并展望其潜在影响。 |
| [^98] | [Exploring ChatGPT and its Impact on Society](https://arxiv.org/abs/2403.14643) | ChatGPT是一种基于Transformer架构的大型语言模型，能够生成人类化的对话回复，可革新各行业并改变技术互动方式。 |
| [^99] | [Revolutionising Distance Learning: A Comparative Study of Learning Progress with AI-Driven Tutoring](https://arxiv.org/abs/2403.14642) | 本研究首次证明生成式AI能显著提高大学生的学习速度，使用AI助手Syntea在远程学习学生中平均减少了27%的学习时间，表明生成式AI可以通过个性化显著改进和加快学习。 |
| [^100] | [Testing autonomous vehicles and AI: perspectives and challenges from cybersecurity, transparency, robustness and fairness](https://arxiv.org/abs/2403.14641) | 探讨了将人工智能整合到自动驾驶车辆中所涉及的挑战，重点关注了网络安全审计、决策过程的可解释性以及评估预测系统的稳健性和道德行为等方面的重要性 |
| [^101] | [On Defining Smart Cities using Transformer Neural Networks](https://arxiv.org/abs/2403.14639) | 使用Transformer神经网络和语义文本分析，本论文尝试创建一个新的智慧城市定义“妥协”版本，并提出语义相似度度量作为评估技术。 |
| [^102] | [AI Fairness in Practice](https://arxiv.org/abs/2403.14636) | 探讨了如何通过基于上下文和以社会为中心的方法来理解AI公平性，从而帮助项目团队更好地识别、减轻和管理不公平偏见和歧视可能在整个AI项目工作流程中出现的多种方式。 |
| [^103] | [AI Sustainability in Practice Part One: Foundations for Sustainable AI Projects](https://arxiv.org/abs/2403.14635) | 该论文介绍了AI可持续性概念及工具，旨在指导AI项目团队评估社会影响和伦理可容忍性，提出了促进利益相关者参与的流程。 |
| [^104] | [Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models](https://arxiv.org/abs/2403.14633) | 本文调查了大型语言模型中是否存在社会经济偏见，引入了一个新的数据集SilverSpoon，并评估了这种偏见的程度以及随着模型大小的变化。 |
| [^105] | [Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion](https://arxiv.org/abs/2403.14617) | Videoshop是一个无需训练的视频编辑算法，通过图像为基础的方法实现了本地化语义编辑，从而允许用户对视频进行精细控制，取得了更高质量的编辑效果。 |
| [^106] | [AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks](https://arxiv.org/abs/2403.14468) | AnyV2V是一种适用于任何视频到视频编辑任务的即插即用框架，通过两个主要步骤简化视频编辑，支持广泛的视频编辑任务，并能够处理传统和新颖的编辑需求。 |
| [^107] | [Analysing Diffusion Segmentation for Medical Images](https://arxiv.org/abs/2403.14440) | 本研究批判性地分析和讨论了医学图像的扩散分割与扩散图像生成之间的差异，强调了针对扩散分割的架构改进带来的益处。 |
| [^108] | ["This is not a data problem": Algorithms and Power in Public Higher Education in Canada](https://arxiv.org/abs/2403.13969) | 研究揭示了公立高等教育中算法决策的影响，包括学生监视增加、不平等加剧和教师-学生关系的自动化。 |
| [^109] | [Accurately Predicting Probabilities of Safety-Critical Rare Events for Intelligent Systems](https://arxiv.org/abs/2403.13869) | 该研究致力于发展一个在评估安全关键自主安全性的重要性方面，在精度和召回率方面都表现出色的关键性预测模型。 |
| [^110] | [The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI](https://arxiv.org/abs/2403.13784) | 提出了模型开放框架（MOF），它是一个排名分类系统，根据完整性和开放性评估机器学习模型，旨在促进完整性、开放性以及遵循开放科学原则，可以帮助准确识别模型的透明性和可重现性。 |
| [^111] | [S2DM: Sector-Shaped Diffusion Models for Video Generation](https://arxiv.org/abs/2403.13408) | S2DM提出了一种新颖的Sector-Shaped Diffusion Model，能够生成具有一致语义和随机特征的一组相关数据，同时在时间特征上变化，在视频生成任务上表现优异。 |
| [^112] | [Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts](https://arxiv.org/abs/2403.13362) | 通过创建使用 GPT-2 的机器人账户，在社交媒体平台上回复用户的推文，鼓励用户接触和关注验证的、意识形态平衡的新闻，以增加用户接触这些新闻并提高参与度。 |
| [^113] | [A Unified Model for Longitudinal Multi-Modal Multi-View Prediction with Missingness](https://arxiv.org/abs/2403.12211) | 该研究提出了一种用于处理纵向多模态多视图数据缺失的统一模型，能够利用所有可用数据进行预测。 |
| [^114] | [Tur[k]ingBench: A Challenge Benchmark for Web Agents](https://arxiv.org/abs/2403.11905) | Tur[k]ingBench是一个挑战性的网络代理基准测试，用于评估最先进的多模态模型在处理包含文本指示和多模态上下文的复杂任务时的泛化能力。 |
| [^115] | [Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton](https://arxiv.org/abs/2403.11879) | 通过融合技术整合全局背景信息和采用LSTM架构进行时间分析，我们的方法在情感模仿强度预测任务上取得了显着的改进。 |
| [^116] | [CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations](https://arxiv.org/abs/2403.11220) | 提出了一种用于未知退化下目标检测的链式思维驱动自适应增强器CPA-Enhancer，并将其集成到通用检测器中，有效提升受损图像的检测性能 |
| [^117] | [Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction](https://arxiv.org/abs/2403.10581) | 提出了一种大型语言模型指导的双注意力ECG网络，用于心力衰竭风险预测，能够捕捉复杂的心电图特征，有效应对低风险和高风险组之间的不平衡。 |
| [^118] | [Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution Analyst?](https://arxiv.org/abs/2403.10482) | 大型语言模型和AI代理的整合在绩效归因分析领域标志着一项开创性发展，能自动化和增强投资组合绩效归因分析。 |
| [^119] | [Predicting Generalization of AI Colonoscopy Models to Unseen Data](https://arxiv.org/abs/2403.09920) | 使用“Masked Siamese Network”（MSN）在未标记数据上识别新现象，并预测结肠镜模型对未知技术和不同国家数据的性能。 |
| [^120] | [Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation](https://arxiv.org/abs/2403.09738) | 大型语言模型作为生成式用户模拟器在对话推荐中展现出潜力，新的协议通过五个任务评估了语言模型模拟人类行为的准确程度，揭示了模型与人类行为的偏差，并提出了如何通过模型选择和提示策略减少这些偏差。 |
| [^121] | [VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding](https://arxiv.org/abs/2403.09530) | 提出了一个统一的VisionGPT-3D框架，整合了最先进的视觉模型，有助于提升计算机视觉对于3D视觉理解的能力 |
| [^122] | [Simple and Scalable Strategies to Continually Pre-train Large Language Models](https://arxiv.org/abs/2403.08763) | 通过简单和可扩展的学习率调整、重放数据的方法，可以在不重新训练的情况下，持续预训练大型语言模型以匹配完全重新训练时的性能。 |
| [^123] | [TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation](https://arxiv.org/abs/2403.07869) | TeleMoMa 是一种面向移动操作的模块化多功能远程操作系统，通过整合多种人机接口、降低门槛且具有通用性，为移动操作器提供了全身远程操作的解决方案。 |
| [^124] | [Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling](https://arxiv.org/abs/2403.05752) | 本文提出了一种自动化TOSG提取的方法KG-TOSA，用于在大型知识图上进行面向任务的图神经网络训练，以减轻对大型KG的过多计算负担。 |
| [^125] | [HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy](https://arxiv.org/abs/2403.05574) | 这一创新心理治疗模型HealMe通过基于心理治疗框架的共情对话，有效解决了根深蒂固的负面思维，并促进了理性、平衡的观点。 |
| [^126] | [Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level](https://arxiv.org/abs/2403.04690) | 该研究提出了一种更快的邻域注意力机制，通过将注意力限制在最近的邻居之间来降低自注意力的计算复杂度，实现了显著的性能提升。 |
| [^127] | [Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning](https://arxiv.org/abs/2403.02893) | 提出了一种使用异构图对比迁移学习的方法，实现了零样本跨语言文档级事件因果识别，并在实验证明在F1得分上优于之前的最先进模型。 |
| [^128] | [Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation](https://arxiv.org/abs/2403.01479) | "本文提出了“Align-to-Distill”（A2D）策略，通过在训练过程中自适应地对齐学生注意力头与其教师对应物，转化了组合映射启发式方法为学习问题，实验结果显示A2D的有效性，对WMT-2022 De->Dsb和WMT-2014 En->De的BLEU分数分别获得高达+3.61和+0.63的提升。" |
| [^129] | [KoCoSa: Korean Context-aware Sarcasm Detection Dataset](https://arxiv.org/abs/2402.14428) | 该论文介绍了一个新的针对韩文对话讽刺检测任务的数据集KoCoSa，提出了一种高效的讽刺检测数据集生成流程，并提供了针对该任务的简单但有效的基线模型。 |
| [^130] | [ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer](https://arxiv.org/abs/2402.02733) | 本研究提出了一种新颖的一阶段方法，结合肖像风格转换实现人脸逆龄化，解决了NPR图像上编辑年龄的问题，并在单个生成步骤中执行。该方法利用了现有的人脸逆龄化和风格转换网络，并且独特地融合了不同的潜在向量，从而保留了面部属性。 |
| [^131] | [Promoting Segment Anything Model towards Highly Accurate Dichotomous Image Segmentation](https://arxiv.org/abs/2401.00248) | 将段分离任意模型推进至高度准确的二元图像分割，通过提出DIS-SAM框架，成功改进SAM模型在细节方面的表现，实现了显著增强的分割精度。 |
| [^132] | [Building Efficient Universal Classifiers with Natural Language Inference](https://arxiv.org/abs/2312.17543) | 本文探讨了如何利用自然语言推理作为通用分类任务，提供了构建通用分类器的详细步骤，并分享了该通用分类器在33个数据集上的训练结果 |
| [^133] | [Learning to Embed Time Series Patches Independently](https://arxiv.org/abs/2312.16427) | 学习独立嵌入时间序列片段可以产生更好的时间序列表示，通过简单的块重构任务和独立嵌入每个块的MLP模型以及互补对比学习来实现。 |
| [^134] | [Soft Contrastive Learning for Time Series](https://arxiv.org/abs/2312.16424) | 提出了一种名为SoftCLT的方法，通过引入实例级和时间级软对比损失，解决了在时间序列中忽略固有相关性所导致的学习表示质量下降的问题。 |
| [^135] | [VideoPoet: A Large Language Model for Zero-Shot Video Generation](https://arxiv.org/abs/2312.14125) | VideoPoet是一种大型语言模型，能够从多种条件信号中生成高质量视频及匹配音频，并且在零样本视频生成领域展示了最先进的能力。 |
| [^136] | [PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models](https://arxiv.org/abs/2312.13964) | PIA通过插拔式模块在文本到图像模型中实现个性化图像动画，并解决了保留独特风格、高保真细节和动作可控性的挑战 |
| [^137] | [FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction](https://arxiv.org/abs/2312.03187) | 开发了一种从用户自发面部表情反应中自动注释用户对生成图像偏好的方法，发现多个面部动作单元与用户对生成图像的评估高度相关，可用于通过这些面部动作单元区分图像对并自动标注用户偏好。 |
| [^138] | [Hulk: A Universal Knowledge Translator for Human-Centric Tasks](https://arxiv.org/abs/2312.01697) | Hulk是第一个多模态人类中心通用模型，能够处理2D视觉、3D视觉、基于骨架和视觉语言任务，无需任务特定微调 |
| [^139] | [Empowering Autonomous Driving with Large Language Models: A Safety Perspective](https://arxiv.org/abs/2312.00812) | 本文通过整合大型语言模型（LLMs）到自动驾驶系统中，利用其常识知识和推理能力，作为智能决策者来增强驾驶性能和安全性。 |
| [^140] | [Fast ODE-based Sampling for Diffusion Models in Around 5 Steps](https://arxiv.org/abs/2312.00094) | 提出了一种基于几何观察的Approximate MEan-Direction Solver（AMED-Solver），能够通过直接学习均方向来消除截断误差，从而实现快速扩散抽样。 |
| [^141] | [Generalisable Agents for Neural Network Optimisation](https://arxiv.org/abs/2311.18598) | 通用智能体用于神经网络优化是一种多智能体强化学习方法，通过动态调度超参数来优化神经网络训练，可以有效改善全局性能并与手工设计启发式方法相竞争。 |
| [^142] | [ID-like Prompt Learning for Few-Shot Out-of-Distribution Detection](https://arxiv.org/abs/2311.15243) | 提出了一种利用CLIP识别ID样式异常值并通过提示学习进行OOD检测的新框架，能够有效提高识别最具挑战性OOD样本的能力。 |
| [^143] | [Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint](https://arxiv.org/abs/2311.10278) | 本文提出了一种利用多保真神经网络(MFNN)解决光学图像与实际机械性能之间映射的方法。 |
| [^144] | [MacGyver: Are Large Language Models Creative Problem Solvers?](https://arxiv.org/abs/2311.09682) | 通过创建MACGYVER数据集并与人类比较，研究发现大型语言模型在创意问题解决方面独具挑战性，在知识广度和可行性方面与人类存在独特差异，同时还展示了通过新的提示技术提升大型语言模型的问题解决能力潜力。 |
| [^145] | [E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity](https://arxiv.org/abs/2310.15929) | 首次将信息熵引入剪枝度量设计，提高在大型语言模型中 N:M 稀疏性的准确性。 |
| [^146] | [Stabilizing reinforcement learning control: A modular framework for optimizing over all stable behavior](https://arxiv.org/abs/2310.14098) | 提出了一个框架，结合了深度强化学习的优化驱动和无模型的优势，以及使用Youla-Kucera参数化提供的稳定性保证，为设计反馈控制器提供了一种优化过程。 |
| [^147] | [AI-Dentify: Deep learning for proximal caries detection on bitewing x-ray -- HUNT4 Oral Health Study](https://arxiv.org/abs/2310.00354) | 通过深度学习模型，研究展示了对HUNT4口腔健康研究中全景X光图像进行快速准确齿龈龋齿检测的潜力 |
| [^148] | [LogPr\'ecis: Unleashing Language Models for Automated Malicious Log Analysis](https://arxiv.org/abs/2307.08309) | 本文研究如何利用最先进的语言模型自动分析类文本的Unix shell攻击日志，提出了LogPr\'ecis方法，可以自动识别攻击者策略并揭示攻击者的目标顺序 |
| [^149] | [FunQA: Towards Surprising Video Comprehension](https://arxiv.org/abs/2306.14899) | FunQA是一个旨在评估和提高基于反直觉和有趣视频的视频推理深度的数据集，涵盖了HumorQA、CreativeQA和MagicQA三种以前未被探索的惊喜视频类型。 |
| [^150] | [Finding the right XAI method -- A Guide for the Evaluation and Ranking of Explainable AI Methods in Climate Science](https://arxiv.org/abs/2303.00652) | 这项工作介绍了气候背景下的XAI评估，并讨论了不同的期望解释特性，为了评估和排序可解释人工智能方法在气候科学中的应用。 |
| [^151] | [Cross-domain Random Pre-training with Prototypes for Reinforcement Learning](https://arxiv.org/abs/2302.05614) | 提出了CRPTpro框架，利用原型进行跨领域自监督随机预训练，提高预训练效率，并实现在不同领域中定义的视觉控制RL任务。 |
| [^152] | [Decision-making with Speculative Opponent Models](https://arxiv.org/abs/2211.11940) | 提出了一种使用纯粹局部信息实现推测对手建模的多智能体分布式演员-评论家算法，能够帮助受控代理做出决策。 |
| [^153] | [Similarity-based Label Inference Attack against Training and Inference of Split Learning](https://arxiv.org/abs/2203.05222) | 本文研究了在Split learning的训练和推理过程中基于相似性的标签推断攻击，提出了相似性度量并设计了三种标签推断攻击。 |
| [^154] | [On Image Search in Histopathology.](http://arxiv.org/abs/2401.08699) | 这篇论文综述了组织病理学图像搜索技术的最新发展，为计算病理学研究人员提供了简明的概述，旨在寻求有效、快速和高效的图像搜索方法。 |
| [^155] | [A Simple Way to Incorporate Novelty Detection in World Models.](http://arxiv.org/abs/2310.08731) | 本文提出了一个简单的方法，通过利用世界模型幻觉状态和真实观察状态的不匹配性作为异常分数，将新颖性检测纳入世界模型强化学习代理中。在新环境中与传统方法相比，我们的工作具有优势。 |
| [^156] | [From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity.](http://arxiv.org/abs/2309.16512) | 本文通过Clifford的几何代数和凸优化，提出了一种分析深度神经网络的新方法。我们展示了深度ReLU神经网络的最优权重可以通过训练样本的楔积来获得，并且训练问题可以简化为对楔积特征进行凸优化，从而揭示了神经网络内部的几何结构。 |
| [^157] | [LLMR: Real-time Prompting of Interactive Worlds using Large Language Models.](http://arxiv.org/abs/2309.12276) | LLMR是一个用于实时创建和修改交互式混合现实体验的框架，通过利用大型语言模型和新颖的策略，它能够解决训练数据稀缺和设计目标复杂的问题，并在性能上超过标准的GPT-4。我们展示了LLMR的跨平台互操作性，并通过评估和用户研究证明了其对于生成和编辑各种对象、工具和场景的能力。 |
| [^158] | [MSAC: Multiple Speech Attribute Control Method for Speech Emotion Recognition.](http://arxiv.org/abs/2308.04025) | 本研究针对语音情感识别(SER)提出了MSAC方法，通过构建新颖的CNN-based SER模型和多语音属性控制方法MSAC，实现了对情感的更精细控制和捕捉，从而提升了SER的可靠性和效果。 |
| [^159] | [Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing Based on GCN Trustworthiness Prediction.](http://arxiv.org/abs/2306.04366) | 本文提出了一种基于GCN可信度预测的协同移动群感知的高效招募策略，通过捕获工人之间的非对称信任关系和工人能力来实现有效的任务分配，优于现有方法。 |
| [^160] | [Earth Movers in The Big Data Era: A Review of Optimal Transport in Machine Learning.](http://arxiv.org/abs/2305.05080) | 本文回顾了最优输运在机器学习中的应用，并探讨了如何将其扩展以适应大数据和高维数据的需求。 |
| [^161] | [Formal Interpretability with Merlin-Arthur Classifiers.](http://arxiv.org/abs/2206.00759) | 该论文提出了一种新型的多智能体交互分类器，利用“Merlin-Arthur”协议的启发，在不假设最优智能体或特征独立分布的情况下，通过相对强度和“非对称特征相关性”概念捕捉特征之间精确的相关性，提供可证明的可解释性保证。 |

# 详细

[^1]: LLaVA-PruMerge: 自适应令牌减少用于高效大型多模态模型

    LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models

    [https://arxiv.org/abs/2403.15388](https://arxiv.org/abs/2403.15388)

    PruMerge提出了一种自适应的视觉令牌减少方法，可以有效减少大型多模态模型中的视觉令牌数量，同时保持模型性能。

    

    大型多模态模型(LMMs)通过连接视觉编码器和大型语言模型展现了显著的推理能力。最近的LMMs包括了更复杂的视觉输入，如高分辨率图像和视频，这显著增加了视觉令牌的数量。为了解决这个问题，我们探索了一种令牌减少机制，并发现类似于先前的工作，许多视觉令牌在空间上是冗余的。基于此，我们提出了PruMerge，一种新颖的自适应视觉令牌减少方法，大大减少了视觉令牌的数量，同时保持了可比的模型性能。

    arXiv:2403.15388v1 Announce Type: cross  Abstract: Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select 
    
[^2]: LATTE3D: 大规模摊还式文本增强3D合成

    LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis

    [https://arxiv.org/abs/2403.15385](https://arxiv.org/abs/2403.15385)

    LATTE3D通过构建可扩展的架构、利用3D数据并采用摊还方法，在显著更大的提示集上实现快速、高质量的文本增强3D合成。

    

    最近的文本到3D生成方法产生了令人印象深刻的3D结果，但需要耗时的优化，每个提示可能需要长达一小时。像ATT3D这样的摊还方法同时优化多个提示，以提高效率，实现快速的文本到3D合成。然而，它们无法捕捉高频几何和纹理细节，并且很难扩展到大型提示集，因此泛化能力较差。我们引入了LATTE3D，解决了这些限制，实现在显著更大的提示集上进行快速、高质量的生成。我们的方法的关键之处在于 1)构建可扩展的架构和 2)利用3D数据在优化过程中通过3D感知扩散先验、形状正则化和模型初始化，以实现对不同和复杂训练提示的稳健性。LATTE3D摊还了神经场和纹理表面的生成，能在单次前向传递中产生高度详细的纹理网格。

    arXiv:2403.15385v1 Announce Type: cross  Abstract: Recent text-to-3D generation approaches produce impressive 3D results but require time-consuming optimization that can take up to an hour per prompt. Amortized methods like ATT3D optimize multiple prompts simultaneously to improve efficiency, enabling fast text-to-3D synthesis. However, they cannot capture high-frequency geometry and texture details and struggle to scale to large prompt sets, so they generalize poorly. We introduce LATTE3D, addressing these limitations to achieve fast, high-quality generation on a significantly larger prompt set. Key to our method is 1) building a scalable architecture and 2) leveraging 3D data during optimization through 3D-aware diffusion priors, shape regularization, and model initialization to achieve robustness to diverse and complex training prompts. LATTE3D amortizes both neural field and textured surface generation to produce highly detailed textured meshes in a single forward pass. LATTE3D gen
    
[^3]: 大型语言模型能够进行上下文中的探索吗？

    Can large language models explore in-context?

    [https://arxiv.org/abs/2403.15371](https://arxiv.org/abs/2403.15371)

    研究发现，大型语言模型在没有实质干预的情况下很难有效进行探索，除了特定配置下的GPT-4具有满意的探索行为外，其他模型表现不稳定。

    

    我们研究现代大型语言模型（LLMs）在进行探索方面的能力，这是强化学习和决策制定中的核心能力。我们关注现有LLMs的原生性能，没有进行训练干预。我们将LLMs部署为简单多臂老虎机环境中的代理，并完全在上下文中指定环境描述和交互历史，即在LLM提示内部进行。我们使用各种提示设计对GPT-3.5、GPT-4和Llama2进行实验，发现这些模型在没有实质干预的情况下并没有稳健地进行探索：i）在我们的所有实验中，只有一个配置导致了令人满意的探索行为：具有思维链推理和外部总结的交互历史的GPT-4，这些被呈现为充分统计的情况；ii）所有其他配置都没有产生稳健的探索行为，包括具有思维链推理的其他配置。

    arXiv:2403.15371v1 Announce Type: cross  Abstract: We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration, a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5, GPT-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally summarized interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thou
    
[^4]: CoLLEGe: 大型语言模型的概念嵌入生成

    CoLLEGe: Concept Embedding Generation for Large Language Models

    [https://arxiv.org/abs/2403.15362](https://arxiv.org/abs/2403.15362)

    CoLLEGe是一个元学习框架，能够为大型语言模型生成灵活的新概念嵌入，用于现代化少样本概念学习。

    

    当前语言模型无法快速学习新概念，通常需要更复杂的微调过程才能学习得更稳健。本文引入了一种名为CoLLEGe（Concept Learning with Language Embedding Generation）的新方法，用于现代化的少样本概念学习。CoLLEGe是一个元学习框架，能够使用少量示例句子或定义生成新概念的灵活嵌入。我们的主要元学习目标只是促进语言模型在随后的句子中进行下一个词预测，使其与语言模型的预训练兼容。

    arXiv:2403.15362v1 Announce Type: cross  Abstract: Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts. Classic methods for few-shot word learning in NLP, relying on global word vectors, are less applicable to large language models. In this paper, we introduce a novel approach named CoLLEGe (Concept Learning with Language Embedding Generation) to modernize few-shot concept learning. CoLLEGe is a meta-learning framework capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions. Our primary meta-learning objective is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining. We design a series of tasks to test new concept lear
    
[^5]: 通过积极目标演绎在未知环境中进行协作人工智能团队

    Collaborative AI Teaming in Unknown Environments via Active Goal Deduction

    [https://arxiv.org/abs/2403.15341](https://arxiv.org/abs/2403.15341)

    通过积极目标推断，利用预训练的策略，实现零射击策略适应，对未知代理人进行最佳团队合作。

    

    随着人工智能（AI）的进步，我们看到越来越多需要AI与其他代理人密切合作的情境，这些代理人的目标和策略可能事先未知。然而，现有的训练协作代理的方法通常需要预先定义和已知的奖励信号，并且无法解决与经常具有潜在目标/奖励的未知代理人进行团队合作的问题。为了应对这一挑战，我们提出了与未知代理人合作的框架，该框架利用核密度贝叶斯逆学习方法进行积极目标推断，并利用预训练的、以目标为条件的策略实现零射击策略适应。我们证明了在我们的框架中无偏奖励估计对于与未知代理人进行最佳团队合作是足够的。我们进一步评估了在重新设计的多智能体粒子和星际争霸II微管理环境中与不同行为/奖励的多样未知代理人进行团队合作的框架

    arXiv:2403.15341v1 Announce Type: new  Abstract: With the advancements of artificial intelligence (AI), we're seeing more scenarios that require AI to work closely with other agents, whose goals and strategies might not be known beforehand. However, existing approaches for training collaborative agents often require defined and known reward signals and cannot address the problem of teaming with unknown agents that often have latent objectives/rewards. In response to this challenge, we propose teaming with unknown agents framework, which leverages kernel density Bayesian inverse learning method for active goal deduction and utilizes pre-trained, goal-conditioned policies to enable zero-shot policy adaptation. We prove that unbiased reward estimates in our framework are sufficient for optimal teaming with unknown agents. We further evaluate the framework of redesigned multi-agent particle and StarCraft II micromanagement environments with diverse unknown agents of different behaviors/rew
    
[^6]: 对可用人工智能滥用的技术视角

    A Technological Perspective on Misuse of Available AI

    [https://arxiv.org/abs/2403.15325](https://arxiv.org/abs/2403.15325)

    潜在的对民用人工智能的恶意滥用对国家和国际安全构成严重威胁，研究展示了免费可用的AI如何被结合成自主武器系统，并提出控制点和进一步措施以防止潜在的威胁。

    

    潜在的对民用人工智能（AI）的恶意滥用对国家和国际安全构成严重威胁。本文从技术角度定义自主系统，解释AI发展的特点，并展示已经存在并公开可用的AI技术如何被滥用。为了强调这一点，我们开发了三个潜在被滥用的AI的示例用例，这些用例威胁到政治、数字和身体安全。这些用例可以基于现有的AI技术和学术界、私营部门和开发者社区的组件构建，展示了如何将免费可用的AI组合成自主武器系统。基于这些用例，我们推断控制点和进一步措施，以防止通过滥用AI造成的潜在威胁。此外，我们倡导在讨论自主武器系统时考虑对民用AI系统的恶意滥用。

    arXiv:2403.15325v1 Announce Type: cross  Abstract: Potential malicious misuse of civilian artificial intelligence (AI) poses serious threats to security on a national and international level. Besides defining autonomous systems from a technological viewpoint and explaining how AI development is characterized, we show how already existing and openly available AI technology could be misused. To underline this, we developed three exemplary use cases of potentially misused AI that threaten political, digital and physical security. The use cases can be built from existing AI technologies and components from academia, the private sector and the developer-community. This shows how freely available AI can be combined into autonomous weapon systems. Based on the use cases, we deduce points of control and further measures to prevent the potential threat through misused AI. Further, we promote the consideration of malicious misuse of civilian AI systems in the discussion on autonomous weapon syst
    
[^7]: Point-DETR3D：利用空间点先验信息增强图像数据的弱监督半监督3D目标检测

    Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection

    [https://arxiv.org/abs/2403.15317](https://arxiv.org/abs/2403.15317)

    提出了Point-DETR3D，一个师生框架用于弱监督半监督3D检测，充分利用点级监督优势，克服了将弱监督3D先验信息编码到模型中的挑战。

    

    训练高精度的3D检测器需要大量带有7个自由度的标记3D注释，这是费时费力的。因此，提出了点注释的形式，为3D检测在实际应用中提供了重要前景，不仅更易获得且成本更低廉，而且为目标定位提供了强大的空间信息。在本文中，我们经验性地发现，仅仅将Point-DETR改编为其3D形式并不简单，遇到了两个主要瓶颈：1）无法将强大的3D先验信息编码到模型中，2）由于激光雷达点的极度稀疏性，在远距离区域生成质量低下的伪标签。为了克服这些挑战，我们引入了Point-DETR3D，一个用于弱监督半监督3D检测的师生框架，旨在充分利用在受限的实例级注释预算内的点级监督。与P不同

    arXiv:2403.15317v1 Announce Type: cross  Abstract: Training high-accuracy 3D detectors necessitates massive labeled 3D annotations with 7 degree-of-freedom, which is laborious and time-consuming. Therefore, the form of point annotations is proposed to offer significant prospects for practical applications in 3D detection, which is not only more accessible and less expensive but also provides strong spatial information for object localization.In this paper, we empirically discover that it is non-trivial to merely adapt Point-DETR to its 3D form, encountering two main bottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it generates low-quality pseudo labels in distant regions due to the extreme sparsity of LiDAR points. To overcome these challenges, we introduce Point-DETR3D, a teacher-student framework for weakly semi-supervised 3D detection, designed to fully capitalize on point-wise supervision within a constrained instance-wise annotation budget.Different from P
    
[^8]: CR3DT：相机与雷达融合用于3D检测和跟踪

    CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking

    [https://arxiv.org/abs/2403.15313](https://arxiv.org/abs/2403.15313)

    CR3DT是一个相机与雷达融合模型，结合了雷达在3D检测和跟踪中的潜力，通过在State-of-the-Art相机架构基础上实现了显著的改进。

    

    精确检测和跟踪周围物体对于实现自动驾驶车辆至关重要。虽然光探测与测距（LiDAR）传感器已经成为高性能的基准，但仅使用相机的解决方案的吸引力在于其成本效益。尽管无线电探测与测距（RADAR）传感器在汽车系统中被广泛使用，由于数据稀疏和测量噪声的原因，它们在3D检测和跟踪中的潜力长期被忽视。作为一个最新的发展，相机与雷达的结合正成为一种有前途的解决方案。本文提出了Camera-RADAR 3D Detection and Tracking (CR3DT)，这是一个用于3D物体检测和多物体跟踪的相机-雷达融合模型。在基于最先进的只有相机的BEVDet架构的基础上，CR3DT在检测和跟踪能力方面取得了显著的改进，通过整合雷达数据

    arXiv:2403.15313v1 Announce Type: cross  Abstract: Accurate detection and tracking of surrounding objects is essential to enable self-driving vehicles. While Light Detection and Ranging (LiDAR) sensors have set the benchmark for high performance, the appeal of camera-only solutions lies in their cost-effectiveness. Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise. As a recent development, the combination of RADARs and cameras is emerging as a promising solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object detection, and Multi-Object Tracking (MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporating the sp
    
[^9]: KTbench：一种全新的无数据泄漏的知识追踪框架

    KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing

    [https://arxiv.org/abs/2403.15304](https://arxiv.org/abs/2403.15304)

    KTbench提出了一种无数据泄漏的知识追踪框架，解决了KT模型中KC之间的相关性学习可能导致的性能下降问题。

    

    知识追踪（KT）涉及在智能辅导系统中预测学生对学习项目的未来表现。学习项目被标记为称为知识概念（KCs）的技能标签。许多KT模型通过用构成KC的学习项目取代学习项目来将学习项目-学生交互序列扩展为KC-学生交互序列，从而解决了稀疏的学习项目-学生交互问题并最小化了模型参数。然而，这种方法存在两个问题。第一个问题是模型学习同一项目内的KC之间的相关性的能力，这可能导致基本事实标签的泄漏并阻碍模型性能。第二个问题是现有的基准实现忽略了计数问题

    arXiv:2403.15304v1 Announce Type: cross  Abstract: Knowledge Tracing (KT) is concerned with predicting students' future performance on learning items in intelligent tutoring systems. Learning items are tagged with skill labels called knowledge concepts (KCs). Many KT models expand the sequence of item-student interactions into KC-student interactions by replacing learning items with their constituting KCs. This often results in a longer sequence length. This approach addresses the issue of sparse item-student interactions and minimises model parameters. However, two problems have been identified with such models.   The first problem is the model's ability to learn correlations between KCs belonging to the same item, which can result in the leakage of ground truth labels and hinder performance. This problem can lead to a significant decrease in performance on datasets with a higher number of KCs per item. The second problem is that the available benchmark implementations ignore accounti
    
[^10]: 使用学习的策略基础进行规划以最优地解决复杂任务

    Planning with a Learned Policy Basis to Optimally Solve Complex Tasks

    [https://arxiv.org/abs/2403.15301](https://arxiv.org/abs/2403.15301)

    使用继承特征学习策略基础，使每个（子）策略解决一个子问题，在FSA描述的任务中，组合这些（子）策略可用于无需额外学习生成最优解决方案，方法能够渐近达到全局最优性，即使在随机环境中也如此。

    

    传统的强化学习方法可以成功解决各种顺序决策问题。然而，在具有非马尔可夫奖励规范的情景中学习能够可靠泛化于多个任务的策略是一个具有挑战性的问题。我们提出使用继承特征来学习一个策略基础，使得其中的每一个（子）策略解决一个明确定义的子问题。在由有限状态自动机（FSA）描述的任务中涉及相同一组子问题时，这些（子）策略的组合可以被用来生成一个最优解决方案而无需额外的学习。与其他通过规划组合（子）策略的方法相比，我们的方法在渐近上达到全局最优性，即使在随机环境中也是如此。

    arXiv:2403.15301v1 Announce Type: cross  Abstract: Conventional reinforcement learning (RL) methods can successfully solve a wide range of sequential decision problems. However, learning policies that can generalize predictably across multiple tasks in a setting with non-Markovian reward specifications is a challenging problem. We propose to use successor features to learn a policy basis so that each (sub)policy in it solves a well-defined subproblem. In a task described by a finite state automaton (FSA) that involves the same set of subproblems, the combination of these (sub)policies can then be used to generate an optimal solution without additional learning. In contrast to other methods that combine (sub)policies via planning, our method asymptotically attains global optimality, even in stochastic environments.
    
[^11]: 用于理性推理的球形神经网络

    Sphere Neural-Networks for Rational Reasoning

    [https://arxiv.org/abs/2403.15297](https://arxiv.org/abs/2403.15297)

    该论文提出了一种球形神经网络（SphNNs）来进行理性推理，通过将计算构建块从向量推广到球体，实现了人类类似的推理能力，并开发了用于三段论推理的SphNN。

    

    大型语言模型（LLMs）如ChatGPT的成功得到了广泛的认可，其类人问题回答的能力以及不断提升的推理性能都证明了这一点。然而，LLMs是否会进行推理仍然不清楚。传统神经网络如何在定性上扩展以超越统计范式并实现高级认知是一个未解之谜。在这里，我们通过将计算构建块从向量推广到球体的方式提出了一种极简的定性扩展。我们提出了球形神经网络（SphNNs）用于通过模型构建和检查进行类人推理，并为三段论推理开发了SphNN，这是人类理性的缩影。SphNN不使用训练数据，而是使用邻域空间关系的神经符号转换映射来指导从当前球形配置向目标的转换。

    arXiv:2403.15297v1 Announce Type: new  Abstract: The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by their planetary popularity, their capability of human-like question-answering, and also by their steadily improved reasoning performance. However, it remains unclear whether LLMs reason. It is an open problem how traditional neural networks can be qualitatively extended to go beyond the statistic paradigm and achieve high-level cognition. Here, we present a minimalist qualitative extension by generalising computational building blocks from vectors to spheres. We propose Sphere Neural Networks (SphNNs) for human-like reasoning through model construction and inspection, and develop SphNN for syllogistic reasoning, a microcosm of human rationality. Instead of training data, SphNN uses a neuro-symbolic transition map of neighbourhood spatial relations to guide transformations from the current sphere configuration towards the target. SphNN is the first neural model th
    
[^12]: 使用ChatGPT进行生物信息学和生物医学信息学：第一年回顾

    Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review

    [https://arxiv.org/abs/2403.15274](https://arxiv.org/abs/2403.15274)

    该研究调查了在生物信息学和生物医学信息学领域应用ChatGPT的情况，总结了其当前优势和局限性，并为未来发展提供了一些见解。

    

    2023年标志着在各个学科领域应用大型语言模型（LLM）聊天机器人ChatGPT的探索取得了显著进展。我们调查了ChatGPT在生物信息学和生物医学信息学各个领域的应用情况，涵盖组学、遗传学、生物医学文本挖掘、药物发现、生物医学图像理解、生物信息学编程和生物信息学教育。我们的调查描绘了该聊天机器人在生物信息学领域的当前优势和局限性，并提供了未来发展的潜在方向。

    arXiv:2403.15274v1 Announce Type: cross  Abstract: The year 2023 marked a significant surge in the exploration of applying large language model (LLM) chatbots, notably ChatGPT, across various disciplines. We surveyed the applications of ChatGPT in various sectors of bioinformatics and biomedical informatics throughout the year, covering omics, genetics, biomedical text mining, drug discovery, biomedical image understanding, bioinformatics programming, and bioinformatics education. Our survey delineates the current strengths and limitations of this chatbot in bioinformatics and offers insights into potential avenues for future development.
    
[^13]: 面向社交网络级联预测的分层信息增强网络

    Hierarchical Information Enhancement Network for Cascade Prediction in Social Networks

    [https://arxiv.org/abs/2403.15257](https://arxiv.org/abs/2403.15257)

    提出了面向社交网络级联预测的分层信息增强网络（HIENet），将基本级联序列、用户社交图和子级联图集成到一个统一的框架中，有效地挖掘了不同模态之间的层级语义关联。

    

    在网络中理解信息级联是许多应用中的一个基础问题。本研究提出了一种新颖的分层信息增强网络（HIENet）用于级联预测。我们的方法将基本级联序列、用户社交图和子级联图集成到一个统一的框架中。具体来说，HIENet利用DeepWalk将级联信息采样为一系列序列，然后收集用户之间的路径信息以提取传播者之间的社交关系，此外，我们采用了时间标记的图卷积网络来有效地聚合子级联图信息。

    arXiv:2403.15257v1 Announce Type: cross  Abstract: Understanding information cascades in networks is a fundamental issue in numerous applications. Current researches often sample cascade information into several independent paths or subgraphs to learn a simple cascade representation. However, these approaches fail to exploit the hierarchical semantic associations between different modalities, limiting their predictive performance. In this work, we propose a novel Hierarchical Information Enhancement Network (HIENet) for cascade prediction. Our approach integrates fundamental cascade sequence, user social graphs, and sub-cascade graph into a unified framework. Specifically, HIENet utilizes DeepWalk to sample cascades information into a series of sequences. It then gathers path information between users to extract the social relationships of propagators. Additionally, we employ a time-stamped graph convolutional network to aggregate sub-cascade graph information effectively. Ultimately, 
    
[^14]: 使用有条件效果的PDDL领域的安全学习--扩展版本

    Safe Learning of PDDL Domains with Conditional Effects -- Extended Version

    [https://arxiv.org/abs/2403.15251](https://arxiv.org/abs/2403.15251)

    学习具有条件效果的非平凡安全行动模型可能需要指数数量的样本，我们提出了SAM Learning of Conditional Effects (Conditional-SAM)算法来解决这一问题。

    

    强大的领域独立规划器已经被开发用来解决各种类型的规划问题。这些规划器通常需要一个代理行动模型，该模型以某种规划领域描述语言给出。手动设计这样一个行动模型是一个众所周知的具有挑战性的任务。另一种方法是从观测中自动学习行动模型。如果使用这样的行动模型创建的每个计划都与真实未知的行动模型保持一致，则这样的行动模型被称为安全的。存在用于学习这种安全行动模型的算法，但它们无法处理具有条件或通用效果的领域，而这些是许多规划问题中常见的构造。我们证明了学习具有条件效果的非平凡安全行动模型可能需要指数数量的样本。然后，我们确定了合理的假设条件，在这些条件下这样的学习是可行的，并提出了学习条件效果的SAM Learning（Conditional-SAM），这是第一个al

    arXiv:2403.15251v1 Announce Type: new  Abstract: Powerful domain-independent planners have been developed to solve various types of planning problems. These planners often require a model of the acting agent's actions, given in some planning domain description language. Manually designing such an action model is a notoriously challenging task. An alternative is to automatically learn action models from observation. Such an action model is called safe if every plan created with it is consistent with the real, unknown action model. Algorithms for learning such safe action models exist, yet they cannot handle domains with conditional or universal effects, which are common constructs in many planning problems. We prove that learning non-trivial safe action models with conditional effects may require an exponential number of samples. Then, we identify reasonable assumptions under which such learning is tractable and propose SAM Learning of Conditional Effects (Conditional-SAM), the first al
    
[^15]: 大规模评估结果在LLM中的全面重新评估：一种多方位统计方法

    Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach

    [https://arxiv.org/abs/2403.15250](https://arxiv.org/abs/2403.15250)

    评估大规模LLM中因素对性能的影响通过全面的统计分析，有助于更好地理解和推动这些模型的发展

    

    在LLM快速发展的背景下，评估在理解和推动这些模型前进中的重要性日益凸显。评估揭示了缩放、训练类型、架构等因素深刻影响LLM的性能。然而，这些因素对性能评分的影响程度和性质仍然存在争议，因为大多数评估局限于有限数量的模型和数据点。通过统计视角更有效地澄清这些因素对性能得分的影响可以更有效地实现。我们的研究对这些LLM进行了彻底的重新检查，针对当前评估方法的不足之处。随着一个统一的评估框架的出现，我们的研究利用了广泛的评估结果数据集，引入了一种全面的统计方法论。其中包括ANOVA、Tukey HSD检验、GAMM的应用

    arXiv:2403.15250v1 Announce Type: cross  Abstract: Amidst the rapid evolution of LLMs, the significance of evaluation in comprehending and propelling these models forward is increasingly paramount. Evaluations have revealed that factors such as scaling, training types, architectures and other factors profoundly impact the performance of LLMs. However, the extent and nature of these impacts continue to be subjects of debate because most assessments have been restricted to a limited number of models and data points. Clarifying the effects of these factors on performance scores can be more effectively achieved through a statistical lens. Our study embarks on a thorough re-examination of these LLMs, targeting the inadequacies in current evaluation methods. With the advent of a uniform evaluation framework, our research leverages an expansive dataset of evaluation results, introducing a comprehensive statistical methodology. This includes the application of ANOVA, Tukey HSD tests, GAMM, and
    
[^16]: 使用扩散模型进行视频运动转移的光谱运动对齐

    Spectral Motion Alignment for Video Motion Transfer using Diffusion Models

    [https://arxiv.org/abs/2403.15249](https://arxiv.org/abs/2403.15249)

    提出了一种名为Spectral Motion Alignment（SMA）的新框架，通过傅立叶和小波变换来优化和对齐运动向量，学习整帧全局运动动态，减轻空间伪影，有效改善运动转移。

    

    扩散模型的发展在视频生成和理解方面产生了巨大影响。特别是，文本到视频扩散模型（VDMs）显著促进了将输入视频定制为目标外观、运动等。尽管取得了这些进展，但准确提取视频帧的运动信息仍然存在挑战。现有作品利用连续帧残差作为目标运动向量，但它们固有地缺乏全局运动背景，并容易受到逐帧失真的影响。为了解决这个问题，我们提出了光谱运动对齐（SMA），这是一个通过傅立叶和小波变换来优化和对齐运动向量的新框架。SMA通过整合频域正则化来学习运动模式，促进整帧全局运动动态的学习，并减轻空间伪影。大量实验证明了SMA在改善运动转移方面的有效性。

    arXiv:2403.15249v1 Announce Type: cross  Abstract: The evolution of diffusion models has greatly impacted video generation and understanding. Particularly, text-to-video diffusion models (VDMs) have significantly facilitated the customization of input video with target appearance, motion, etc. Despite these advances, challenges persist in accurately distilling motion information from video frames. While existing works leverage the consecutive frame residual as the target motion vector, they inherently lack global motion context and are vulnerable to frame-wise distortions. To address this, we present Spectral Motion Alignment (SMA), a novel framework that refines and aligns motion vectors using Fourier and wavelet transforms. SMA learns motion patterns by incorporating frequency-domain regularization, facilitating the learning of whole-frame global motion dynamics, and mitigating spatial artifacts. Extensive experiments demonstrate SMA's efficacy in improving motion transfer while main
    
[^17]: 自监督骨干框架用于多样化农业视觉任务

    Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks

    [https://arxiv.org/abs/2403.15248](https://arxiv.org/abs/2403.15248)

    通过自监督学习，该研究利用SimCLR对农业图像进行预训练，成功应用于多样化的农业视觉任务，消除了大规模带标注数据集的需求。

    

    农业中的计算机视觉具有改变游戏规则的能力，可以将农业转变为数据驱动、精确和可持续的行业。深度学习赋予了农业视觉分析大量复杂的视觉数据的能力，但严重依赖于大规模带标注数据集的可用性。这仍然是一个瓶颈，因为手动标注容易出错、耗时且昂贵。缺乏有效的标注方法激发了我们考虑自监督学习作为一种范式转变，从原始农业图像数据中学习有意义的特征表示。在这项工作中，我们探讨了自监督表示学习如何通过消除对大规模带标注数据集的需求，解锁了对多样化农业视觉任务的潜在适用性。我们提出了一个利用SimCLR，一种对比学习方法，对大量未标注的真实世界农业数据集上预训练ResNet-50骨干的轻量级框架。

    arXiv:2403.15248v1 Announce Type: cross  Abstract: Computer vision in agriculture is game-changing with its ability to transform farming into a data-driven, precise, and sustainable industry. Deep learning has empowered agriculture vision to analyze vast, complex visual data, but heavily rely on the availability of large annotated datasets. This remains a bottleneck as manual labeling is error-prone, time-consuming, and expensive. The lack of efficient labeling approaches inspired us to consider self-supervised learning as a paradigm shift, learning meaningful feature representations from raw agricultural image data. In this work, we explore how self-supervised representation learning unlocks the potential applicability to diverse agriculture vision tasks by eliminating the need for large-scale annotated datasets. We propose a lightweight framework utilizing SimCLR, a contrastive learning approach, to pre-train a ResNet-50 backbone on a large, unannotated dataset of real-world agricult
    
[^18]: 视频的增强推理对象中心学习

    Reasoning-Enhanced Object-Centric Learning for Videos

    [https://arxiv.org/abs/2403.15245](https://arxiv.org/abs/2403.15245)

    设计了一种新颖的推理模块STATM，利用记忆缓冲区增强模型在复杂场景中的感知能力。

    

    物体中心学习旨在将复杂的视觉场景分解为更易处理的物体表示，提升机器学习系统对物理世界的理解和推理能力。最近，基于槽位的视频模型展现出在分割和跟踪物体方面出色的能力，但忽视了有效推理模块的重要性。为了增强模型在复杂场景中的感知能力，我们设计了一种名为具有记忆缓冲区的基于槽位的时空变换器（STATM）的新型推理模块。记忆缓冲区主要用于存储来自上游模块的槽位信息，基于槽位的时空变换器通过槽位为基础进行预测。

    arXiv:2403.15245v1 Announce Type: cross  Abstract: Object-centric learning aims to break down complex visual scenes into more manageable object representations, enhancing the understanding and reasoning abilities of machine learning systems toward the physical world. Recently, slot-based video models have demonstrated remarkable proficiency in segmenting and tracking objects, but they overlook the importance of the effective reasoning module. In the real world, reasoning and predictive abilities play a crucial role in human perception and object tracking; in particular, these abilities are closely related to human intuitive physics. Inspired by this, we designed a novel reasoning module called the Slot-based Time-Space Transformer with Memory buffer (STATM) to enhance the model's perception ability in complex scenes. The memory buffer primarily serves as storage for slot information from upstream modules, the Slot-based Time-Space Transformer makes predictions through slot-based spatio
    
[^19]: 多角度记忆增强网络用于在社交网络中识别关键节点

    Multi-perspective Memory Enhanced Network for Identifying Key Nodes in Social Networks

    [https://arxiv.org/abs/2403.15235](https://arxiv.org/abs/2403.15235)

    提出了一个多角度记忆增强网络（MMEN），用于在社交网络中识别关键节点，通过多角度挖掘关键节点并利用记忆网络存储历史信息，以增强模型在未知情境中的泛化能力。

    

    在社交网络中识别关键节点在及时阻止虚假信息方面起着至关重要的作用。现有的关键节点识别方法通常仅从传播结构的角度考虑节点影响，并且对未知情境的泛化能力不足。本文提出了一种新颖的多角度记忆增强网络（MMEN），用于在社交网络中识别关键节点，该网络从多个角度挖掘关键节点并利用记忆网络存储历史信息。具体来说，MMEN首先从用户属性和传播结构的角度构建两个传播网络，并使用图注意力网络更新节点特征表示。同时，记忆网络用于存储类似子图的信息，增强模型在未知情境中的泛化性能。最后，MMEN应用自适应权重来结合节点的影响力。

    arXiv:2403.15235v1 Announce Type: cross  Abstract: Identifying key nodes in social networks plays a crucial role in timely blocking false information. Existing key node identification methods usually consider node influence only from the propagation structure perspective and have insufficient generalization ability to unknown scenarios. In this paper, we propose a novel Multi-perspective Memory Enhanced Network (MMEN) for identifying key nodes in social networks, which mines key nodes from multiple perspectives and utilizes memory networks to store historical information. Specifically, MMEN first constructs two propagation networks from the perspectives of user attributes and propagation structure and updates node feature representations using graph attention networks. Meanwhile, the memory network is employed to store information of similar subgraphs, enhancing the model's generalization performance in unknown scenarios. Finally, MMEN applies adaptive weights to combine the node influ
    
[^20]: 无论何时、何地、谁人：研究用于众包医学图像标注的部分任意模型的可行性

    Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations

    [https://arxiv.org/abs/2403.15218](https://arxiv.org/abs/2403.15218)

    该研究探讨了使用部分任意模型（SAM）在众包环境中从非专家处策划医学图像标注的可行性，以生成用于训练3D DL分割模型的"密集"分割掩模。

    

    医学图像分割注释的策划是一项耗时且劳动密集的任务，需要领域专业知识，导致"狭窄"专注的深度学习（DL）模型具有有限的转化效用。最近，像部分任意模型（SAM）这样的基础模型通过出色的零样本泛化能力彻底改变了语义分割，跨各个领域包括医学成像，对于简化注释过程有很大希望。然而，SAM尚未在众包环境中进行评估，以策划注释来训练3D DL分割模型。在这项工作中，我们探索了SAM用于从非专业人员中众包"稀疏"注释，产生用于训练3D nnU-Net模型（一种最先进的DL分割模型）的"密集"分割掩模的潜力。我们的结果表明，与地面真实度相比，SAM生成的注释展现出高平均Dice分数。

    arXiv:2403.15218v1 Announce Type: cross  Abstract: Curating annotations for medical image segmentation is a labor-intensive and time-consuming task that requires domain expertise, resulting in "narrowly" focused deep learning (DL) models with limited translational utility. Recently, foundation models like the Segment Anything Model (SAM) have revolutionized semantic segmentation with exceptional zero-shot generalizability across various domains, including medical imaging, and hold a lot of promise for streamlining the annotation process. However, SAM has yet to be evaluated in a crowd-sourced setting to curate annotations for training 3D DL segmentation models. In this work, we explore the potential of SAM for crowd-sourcing "sparse" annotations from non-experts to generate "dense" segmentation masks for training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our results indicate that while SAM-generated annotations exhibit high mean Dice scores compared to ground-truth a
    
[^21]: AI魔术的制造与解构：一个设计分类学

    (Un)making AI Magic: a Design Taxonomy

    [https://arxiv.org/abs/2403.15216](https://arxiv.org/abs/2403.15216)

    通过构建设计方法的分类学，研究了AI事物设计中魔法的影响，提出了七大设计原则，并探讨了这些原则对魅力和喜忧无常的影响。

    

    本文通过构建一个设计方法的分类学，研究了魔术在AI事物设计中所起的作用，这些设计方法可以增加或减少魔法和魅力的感知。我们从围绕AI技术近期发展的设计话语开始，强调特定的交互特性，比如算法不确定性和错误，并阐明与魔法和超自然思维的关系。通过分析和反思来自两届设计与AI硕士课程中52名学生的设计项目，我们确定了七大设计原则，并解开了每个原则在魅力和喜忧无常方面的影响。最后，我们阐述了设计/人机交互从业者可以如何接纳和利用这种分类学，特别是以支持探索与反思。

    arXiv:2403.15216v1 Announce Type: cross  Abstract: This paper examines the role that enchantment plays in the design of AI things by constructing a taxonomy of design approaches that increase or decrease the perception of magic and enchantment. We start from the design discourse surrounding recent developments in AI technologies, highlighting specific interaction qualities such as algorithmic uncertainties and errors and articulating relations to the rhetoric of magic and supernatural thinking. Through analyzing and reflecting upon 52 students' design projects from two editions of a Master course in design and AI, we identify seven design principles and unpack the effects of each in terms of enchantment and disenchantment. We conclude by articulating ways in which this taxonomy can be approached and appropriated by design/HCI practitioners, especially to support exploration and reflexivity.
    
[^22]: FSD-Inference: 具有可扩展云通信的完全无服务器分布式推断

    FSD-Inference: Fully Serverless Distributed Inference with Scalable Cloud Communication

    [https://arxiv.org/abs/2403.15195](https://arxiv.org/abs/2403.15195)

    FSD-Inference是第一个完全无服务器且高度可扩展的分布式ML推断系统，引入了全新的无服务器通信方案。

    

    arXiv:2403.15195v1 公告类型:跨领域 抽象: 无服务器计算提供了具有吸引力的可伸缩性、弹性和成本效益。然而，对内存、CPU和函数运行时间的限制阻碍了其在数据密集型应用和机器学习（ML）工作负载中的应用。传统的“全服务器”平台通过快速网络和已建立良好的进程间通信（IPC）机制（如MPI和共享内存）实现了分布式计算。在无服务器领域缺乏此类解决方案的情况下，具有重要IPC要求的并行计算具有挑战性。我们提出FSD-Inference，这是第一个完全无服务器且高度可扩展的分布式ML推断系统。我们探讨潜在的通信渠道，与函数即服务（FaaS）计算相结合，为无服务器数据密集型计算环境中的分布式ML设计了一流的解决方案。我们引入了用于ML推断的全新无服务器通信方案。

    arXiv:2403.15195v1 Announce Type: cross  Abstract: Serverless computing offers attractive scalability, elasticity and cost-effectiveness. However, constraints on memory, CPU and function runtime have hindered its adoption for data-intensive applications and machine learning (ML) workloads. Traditional 'server-ful' platforms enable distributed computation via fast networks and well-established inter-process communication (IPC) mechanisms such as MPI and shared memory. In the absence of such solutions in the serverless domain, parallel computation with significant IPC requirements is challenging. We present FSD-Inference, the first fully serverless and highly scalable system for distributed ML inference. We explore potential communication channels, in conjunction with Function-as-a-Service (FaaS) compute, to design a state-of-the-art solution for distributed ML within the context of serverless data-intensive computing. We introduce novel fully serverless communication schemes for ML infe
    
[^23]: SFOD：脉冲融合目标检测器

    SFOD: Spiking Fusion Object Detector

    [https://arxiv.org/abs/2403.15192](https://arxiv.org/abs/2403.15192)

    SFOD是一种简单而高效的基于脉冲神经网络的目标检测方法，通过设计脉冲融合模块，实现了首次在事件相机中对不同尺度的特征图进行融合。

    

    事件相机以高时间分辨率、高动态范围、低功耗和高像素带宽为特征，为特定背景下的目标检测提供了独特的能力。然而，事件数据固有的稀疏性和异步性给现有的目标检测算法带来挑战。脉冲神经网络（SNNs）受人脑编码和处理信息的方式启发，为这些困难提供了潜在解决方案。然而，当前对事件相机使用SNN进行目标检测的性能受到限制。在本文中，我们提出了脉冲融合目标检测器（SFOD），这是一种简单高效的基于SNN的目标检测方法。具体来说，我们设计了一个脉冲融合模块，实现了首次在应用于事件相机的SNN中从不同尺度融合特征图。此外，通过整合我们的分析和实验进行。

    arXiv:2403.15192v1 Announce Type: cross  Abstract: Event cameras, characterized by high temporal resolution, high dynamic range, low power consumption, and high pixel bandwidth, offer unique capabilities for object detection in specialized contexts. Despite these advantages, the inherent sparsity and asynchrony of event data pose challenges to existing object detection algorithms. Spiking Neural Networks (SNNs), inspired by the way the human brain codes and processes information, offer a potential solution to these difficulties. However, their performance in object detection using event cameras is limited in current implementations. In this paper, we propose the Spiking Fusion Object Detector (SFOD), a simple and efficient approach to SNN-based object detection. Specifically, we design a Spiking Fusion Module, achieving the first-time fusion of feature maps from different scales in SNNs applied to event cameras. Additionally, through integrating our analysis and experiments conducted d
    
[^24]: 语义向量的脑接地改善了神经解码视觉刺激

    Brain-grounding of semantic vectors improves neural decoding of visual stimuli

    [https://arxiv.org/abs/2403.15176](https://arxiv.org/abs/2403.15176)

    提出了一种表示学习框架，称为语义向量的脑接地，通过微调预训练的特征向量，使其更好地与人类大脑中视觉刺激的神经表示对齐。

    

    发展准确全面的算法来解码大脑内容是神经科学和脑机接口领域的一个长期目标。之前的研究已经证明了通过训练机器学习模型将大脑活动模式映射到一个语义向量表示的神经解码的可行性。为了解决这个问题，我们提出了一个表示学习框架，称为语义向量的脑接地，它对预训练的特征向量进行微调，以更好地与人类大脑中视觉刺激的神经表示对齐。

    arXiv:2403.15176v1 Announce Type: cross  Abstract: Developing algorithms for accurate and comprehensive neural decoding of mental contents is one of the long-cherished goals in the field of neuroscience and brain-machine interfaces. Previous studies have demonstrated the feasibility of neural decoding by training machine learning models to map brain activity patterns into a semantic vector representation of stimuli. These vectors, hereafter referred as pretrained feature vectors, are usually derived from semantic spaces based solely on image and/or text features and therefore they might have a totally different characteristics than how visual stimuli is represented in the human brain, resulting in limiting the capability of brain decoders to learn this mapping. To address this issue, we propose a representation learning framework, termed brain-grounding of semantic vectors, which fine-tunes pretrained feature vectors to better align with the neural representation of visual stimuli in t
    
[^25]: 在检测心理疾病的情境中探索自监督学习的任务无关特性

    Exploring the Task-agnostic Trait of Self-supervised Learning in the Context of Detecting Mental Disorders

    [https://arxiv.org/abs/2403.15170](https://arxiv.org/abs/2403.15170)

    本研究探索了在检测主要抑郁症和创伤后应激障碍时，在交互会话期间收集的音频和视频数据上使用自监督学习的任务无关表示。

    

    自监督学习（SSL）已经被研究用于在各个领域生成任务无关的表示。然而，到目前为止，尚未有人探索用于检测多种心理障碍的这种方法。存在任务无关表示的理由在于多种心理障碍之间的症状重叠。因此，收集用于心理健康评估的行为数据可能包含与多种障碍相关的属性。受此启发，本研究探讨了通过SSL推导出的任务无关表示，用于使用在互动会话期间收集的音频和视频数据检测重度抑郁障碍（MDD）和创伤后应激障碍（PTSD）的情境。本研究采用了通过预测多个固定目标或掩膜帧训练的SSL模型。我们提出了一系列固定目标，以使生成的表示对MDD的检测更加高效。

    arXiv:2403.15170v1 Announce Type: cross  Abstract: Self-supervised learning (SSL) has been investigated to generate task-agnostic representations across various domains. However, such investigation has not been conducted for detecting multiple mental disorders. The rationale behind the existence of a task-agnostic representation lies in the overlapping symptoms among multiple mental disorders. Consequently, the behavioural data collected for mental health assessment may carry a mixed bag of attributes related to multiple disorders. Motivated by that, in this study, we explore a task-agnostic representation derived through SSL in the context of detecting major depressive disorder (MDD) and post-traumatic stress disorder (PTSD) using audio and video data collected during interactive sessions. This study employs SSL models trained by predicting multiple fixed targets or masked frames. We propose a list of fixed targets to make the generated representation more efficient for detecting MDD 
    
[^26]: 目标类分类的转换图属性

    Transition Graph Properties of Target Class Classification

    [https://arxiv.org/abs/2403.15167](https://arxiv.org/abs/2403.15167)

    目标类分类的关键在于转换图的属性，研究表明理想的转换图结构是朝根顶点方向取向的有根树。

    

    目标类分类是一个混合分类和转换模型，其综合目标是将对象分配到某个所谓的目标或正常类。分类过程是迭代的，在每一步中，某个类中的对象经历与该类相对应的动作，引发对象向其中一个类的转变。我们称之为类转换的转换序列必须设计得能为对象最终分配到目标类提供支持。转换过程可以用有向图的形式描述，最终分类的成功主要取决于这个图的属性。在我们之前的研究中，我们表明了转换图的理想结构是一个朝根顶点方向取向的有根树，该顶点对应于正常类。很明显，任意算法（策略）的转换图可能没有

    arXiv:2403.15167v1 Announce Type: cross  Abstract: Target class classification is a mixed classification and transition model whose integrated goal is to assign objects to a certain, so called target or normal class. The classification process is iterative, and in each step an object in a certain class undergoes an action attached to that class, initiating the transition of the object to one of the classes. The sequence of transitions, which we call class transitions, must be designed to provide the final assignment of objects to the target class. The transition process can be described in the form of a directed graph, and the success of the final classification is mainly due to the properties of this graph. In our previous research we showed that the desirable structure of the transition graph is an oriented rooted tree with orientation towards the root vertex, which corresponds to the normal class. It is clear that the transition graph of an arbitrary algorithm (policy) may not have 
    
[^27]: 图像标注的模块化深度主动学习框架：眼科-AI项目的技术报告

    Modular Deep Active Learning Framework for Image Annotation: A Technical Report for the Ophthalmo-AI Project

    [https://arxiv.org/abs/2403.15143](https://arxiv.org/abs/2403.15143)

    提出了MedDeepCyleAL，一个实现完整主动学习周期的端到端框架，为研究人员提供了灵活选择深度学习模型类型的可能性。

    

    图像标注是医学成像和疾病诊断领域中确保患者获得适当治疗并追踪治疗过程中进展的最关键任务之一。然而，手动标注大量的2D和3D成像数据可能极为繁琐。基于深度学习的分割算法完全改变了这一过程，使得自动化图像分割成为可能。通过准确分割医学图像，这些算法可以大大减少需要手动标注的时间和工作量。此外，通过融合主动学习方法，这些分割算法可以在较少的标注数据基础上发挥更高的效能。我们引入了MedDeepCycleAL，一个实现完整AL循环的端到端框架。它为研究人员提供了选择他们希望采用的深度学习模型类型的灵活性，并包括了一个注释

    arXiv:2403.15143v1 Announce Type: cross  Abstract: Image annotation is one of the most essential tasks for guaranteeing proper treatment for patients and tracking progress over the course of therapy in the field of medical imaging and disease diagnosis. However, manually annotating a lot of 2D and 3D imaging data can be extremely tedious. Deep Learning (DL) based segmentation algorithms have completely transformed this process and made it possible to automate image segmentation. By accurately segmenting medical images, these algorithms can greatly minimize the time and effort necessary for manual annotation. Additionally, by incorporating Active Learning (AL) methods, these segmentation algorithms can perform far more effectively with a smaller amount of ground truth data. We introduce MedDeepCyleAL, an end-to-end framework implementing the complete AL cycle. It provides researchers with the flexibility to choose the type of deep learning model they wish to employ and includes an annot
    
[^28]: CACA Agent：基于能力协作的AI代理

    CACA Agent: Capability Collaboration based AI Agent

    [https://arxiv.org/abs/2403.15137](https://arxiv.org/abs/2403.15137)

    提出了CACA代理，采用基于能力协作的开放架构，整合了一组协作能力来实现AI代理，增强了AI代理的规划能力和可扩展性。

    

    随着基于大型语言模型（LLMs）的AI代理在各个领域的实际应用中展现出潜力，如何快速部署AI代理以及如何方便地扩展AI代理的应用场景已成为一个挑战。本文提出了CACA代理（基于能力协作的AI代理），采用了受服务计算启发的开放架构。CACA代理整合了一组协作能力来实现AI代理，不仅减少了对单个LLM的依赖，还增强了AI代理的规划能力和可用工具的可扩展性。利用所提出的系统，我们展示了一个演示来说明操作和应用场景的扩展。

    arXiv:2403.15137v1 Announce Type: new  Abstract: As AI Agents based on Large Language Models (LLMs) have shown potential in practical applications across various fields, how to quickly deploy an AI agent and how to conveniently expand the application scenario of AI agents has become a challenge. Previous studies mainly focused on implementing all the reasoning capabilities of AI agents within a single LLM, which often makes the model more complex and also reduces the extensibility of AI agent functionality. In this paper, we propose CACA Agent (Capability Collaboration based AI Agent), using an open architecture inspired by service computing. CACA Agent integrates a set of collaborative capabilities to implement AI Agents, not only reducing the dependence on a single LLM, but also enhancing the extensibility of both the planning abilities and the tools available to AI agents. Utilizing the proposed system, we present a demo to illustrate the operation and the application scenario exten
    
[^29]: 对话中的语言模型：人机交互的会话最大化准则

    Language Models in Dialogue: Conversational Maxims for Human-AI Interactions

    [https://arxiv.org/abs/2403.15115](https://arxiv.org/abs/2403.15115)

    提出了一组最大化准则，用于描述有效的人机对话，包括传统的 Grice 四个最大化准则以及两个新准则，对于解决现代人机互动中的特殊行为问题。

    

    现代语言模型虽然复杂，但在对话环境中存在一些固有缺陷。我们认为观察到的许多缺陷可以归因于违反一个或多个对话原则。通过借鉴社会科学和人工智能领域的广泛研究，我们提出了一组最大化准则 - 包括数量、质量、相关性、方式、仁慈以及透明度 - 来描述有效的人机对话。我们首先证明了在人机互动背景下 Grice 的前四个最大化准则的适用性。然后，我们认为两个新的准则，仁慈（涉及生成和参与有害内容）和透明度（涉及识别自己的知识边界、操作约束和意图），对于解决现代人机互动中独特行为是必要的。提出的准则为如何提供具体指导提供了指导。

    arXiv:2403.15115v1 Announce Type: cross  Abstract: Modern language models, while sophisticated, exhibit some inherent shortcomings, particularly in conversational settings. We claim that many of the observed shortcomings can be attributed to violation of one or more conversational principles. By drawing upon extensive research from both the social science and AI communities, we propose a set of maxims -- quantity, quality, relevance, manner, benevolence, and transparency -- for describing effective human-AI conversation. We first justify the applicability of the first four maxims (from Grice) in the context of human-AI interactions. We then argue that two new maxims, benevolence (concerning the generation of, and engagement with, harmful content) and transparency (concerning recognition of one's knowledge boundaries, operational constraints, and intents), are necessary for addressing behavior unique to modern human-AI interactions. The proposed maxims offer prescriptive guidance on how
    
[^30]: 使用量子退火器解决现实世界的包裹投递路径问题

    Solving a Real-World Package Delivery Routing Problem Using Quantum Annealers

    [https://arxiv.org/abs/2403.15114](https://arxiv.org/abs/2403.15114)

    这项研究开发了一种量子-经典混合求解器 Q4RPD，旨在解决现实世界的包裹投递路径问题，避免了问题简化和技术捷径，针对包裹重量和尺寸的真实约束条件。

    

    最近几年关于量子计算和路径问题之间的研究非常多产。大部分作品围绕着经典问题，如旅行推销员问题或车辆路径问题。尽管处理这些问题具有价值，但不可否认的是它们以学术为导向的特点无法满足现实世界的要求。本研究的主要目标是提出一种解决现实情况的方法，避免问题简化或技术捷径。相反，开发了一种量子-经典混合求解器，命名为Q4RPD，考虑到一系列真实约束条件，如异构车队、优先投递和根据包裹重量和尺寸确定的容量。Q4RPD采用了D-Wave的Leap受限二次模型混合求解器。为了证明Q4RPD的应用，设计了一个实验组

    arXiv:2403.15114v1 Announce Type: cross  Abstract: Research focused on the conjunction between quantum computing and routing problems has been very prolific in recent years. Most of the works revolve around classical problems such as the Traveling Salesman Problem or the Vehicle Routing Problem. Even though working on these problems is valuable, it is also undeniable that their academic-oriented nature falls short of real-world requirements. The main objective of this research is to present a solving method for realistic instances, avoiding problem relaxations or technical shortcuts. Instead, a quantum-classical hybrid solver has been developed, coined Q4RPD, that considers a set of real constraints such as a heterogeneous fleet of vehicles, priority deliveries, and capacities characterized by two values: weight and dimensions of the packages. Q4RPD resorts to the Leap Constrained Quadratic Model Hybrid Solver of D-Wave. To demonstrate the application of Q4RPD, an experimentation compo
    
[^31]: 使用LLM嵌入进行文本聚类

    Text clustering with LLM embeddings

    [https://arxiv.org/abs/2403.15112](https://arxiv.org/abs/2403.15112)

    研究表明，LLM嵌入能够捕捉结构化语言的细微差别，BERT在性能上领先于轻量级选项，增加嵌入维度和摘要技术并不一致地提高聚类效率

    

    文本聚类是组织不断增长的数字内容的重要方法，有助于结构化和发现未分类数据中的隐藏模式。在这项研究中，我们调查了不同文本嵌入（特别是大型语言模型LLMs中使用的）和聚类算法如何影响文本数据集的聚类方式。进行了一系列实验以评估嵌入是如何影响聚类结果的，以及通过摘要进行降维和嵌入大小调整的作用。结果显示，LLM嵌入在捕获结构化语言的细微差别方面表现出色，而BERT在性能上领先于轻量级选项。此外，我们发现增加嵌入维度和摘要技术并不一致地提高聚类效率，这表明这些策略需要仔细分析才能在实际模型中使用。这些结果突出了一种

    arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
    
[^32]: 用于协调运动控制的子等变强化学习框架

    Subequivariant Reinforcement Learning Framework for Coordinated Motion Control

    [https://arxiv.org/abs/2403.15100](https://arxiv.org/abs/2403.15100)

    使用子等变原理的CoordiGraph框架在强化学习中增强协调运动控制，改善了关节之间的关系建模，提高了泛化能力和样本效率。

    

    有效的协调对于基于强化学习的运动控制至关重要，尤其是当代理体和它们的运动复杂性增加时。然而，许多现有方法难以考虑到关节之间的复杂依赖关系。我们引入了CoordiGraph，这是一种利用物理学中的子等变原理来增强强化学习协调运动控制的新型架构。该方法将等变性原理嵌入到学习过程中，以在重力影响下模拟关节之间重要的微妙关系，有助于运动控制。通过在不同环境中对复杂代理体进行广泛实验，我们突出了我们方法的优点。与当前主流方法相比，CoordiGraph显著提高了泛化能力和样本效率。

    arXiv:2403.15100v1 Announce Type: cross  Abstract: Effective coordination is crucial for motion control with reinforcement learning, especially as the complexity of agents and their motions increases. However, many existing methods struggle to account for the intricate dependencies between joints. We introduce CoordiGraph, a novel architecture that leverages subequivariant principles from physics to enhance coordination of motion control with reinforcement learning. This method embeds the principles of equivariance as inherent patterns in the learning process under gravity influence, which aids in modeling the nuanced relationships between joints vital for motion control. Through extensive experimentation with sophisticated agents in diverse environments, we highlight the merits of our approach. Compared to current leading methods, CoordiGraph notably enhances generalization and sample efficiency.
    
[^33]: 论据感知事件链接方法

    Argument-Aware Approach To Event Linking

    [https://arxiv.org/abs/2403.15097](https://arxiv.org/abs/2403.15097)

    引入论据感知方法改进事件链接模型，能更好地识别和分类不在知识库中的事件提及，弥补了这一领域的研究空白。

    

    arXiv:2403.15097v1 公告类型: 跨领域 摘要: 事件链接将文本中的事件提及与知识库（KB）中相关节点连接起来。先前在事件链接方面的研究主要借鉴了实体链接的方法，忽略了事件的独特特征。与广泛探讨的实体链接任务相比，事件具有更加复杂的结构，可以通过检查其关联的论据更有效地加以区分。此外，事件的信息丰富性导致事件知识库的稀缺性。这强调了事件链接模型需要识别和分类不在知识库中的事件提及作为“超出知识库”的重要性，而这一领域受到了有限关注。在这项工作中，我们通过引入一个论据感知方法来应对这些挑战。首先，我们通过标记事件论据信息来改进事件链接模型，有助于识别有关事件提及的关键信息。随后，为了帮助模型处理“超出知识库”

    arXiv:2403.15097v1 Announce Type: cross  Abstract: Event linking connects event mentions in text with relevant nodes in a knowledge base (KB). Prior research in event linking has mainly borrowed methods from entity linking, overlooking the distinct features of events. Compared to the extensively explored entity linking task, events have more complex structures and can be more effectively distinguished by examining their associated arguments. Moreover, the information-rich nature of events leads to the scarcity of event KBs. This emphasizes the need for event linking models to identify and classify event mentions not in the KB as ``out-of-KB,'' an area that has received limited attention. In this work, we tackle these challenges by introducing an argument-aware approach. First, we improve event linking models by augmenting input text with tagged event argument information, facilitating the recognition of key information about event mentions. Subsequently, to help the model handle ``out-
    
[^34]: 针对深度强化学习的改进长短期记忆污水处理模拟器

    Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning

    [https://arxiv.org/abs/2403.15091](https://arxiv.org/abs/2403.15091)

    改进长短期记忆污水处理模拟器，解决DRL优化工业过程中的挑战，通过减少复合误差提高模型的准确性

    

    虽然深度强化学习(DRL)在机器人技术和游戏领域取得了杰出成果，但在污水处理等工业过程的优化中实施DRL仍然具有挑战性。其中一个挑战是缺乏能够尽可能准确地代表实际工厂的模拟环境，以训练DRL策略。污水处理数据的随机性和非线性导致模型在长时间范围内产生不稳定和不正确的预测。改进的模型中可能出现模拟行为不正确的一个可能原因是复合误差问题，即在模拟过程中误差的累积。复合误差是因为模型在每个时间步中使用其预测作为输入，实际数据与预测之间的误差随着模拟的进行而累积。我们实施了两种方法来改善污水处理模型的训练结果。

    arXiv:2403.15091v1 Announce Type: cross  Abstract: Even though Deep Reinforcement Learning (DRL) showed outstanding results in the fields of Robotics and Games, it is still challenging to implement it in the optimization of industrial processes like wastewater treatment. One of the challenges is the lack of a simulation environment that will represent the actual plant as accurately as possible to train DRL policies. Stochasticity and non-linearity of wastewater treatment data lead to unstable and incorrect predictions of models over long time horizons. One possible reason for the models' incorrect simulation behavior can be related to the issue of compounding error, which is the accumulation of errors throughout the simulation. The compounding error occurs because the model utilizes its predictions as inputs at each time step. The error between the actual data and the prediction accumulates as the simulation continues. We implemented two methods to improve the trained models for wastew
    
[^35]: 使用大型语言模型的全面脂质组学自动化工作流程

    Comprehensive Lipidomic Automation Workflow using Large Language Models

    [https://arxiv.org/abs/2403.15076](https://arxiv.org/abs/2403.15076)

    该论文提出了使用大型语言模型的全面脂质组学自动化工作流程，该工作流程包括自动生成工作流程、详细统计分析和脂质标注，以及在气泡电喷雾电离-多反应监测方法中识别不饱和脂质碳碳双键位置的模块化方法。

    

    脂质组学生成大量数据，使得手动标注和解释变得具有挑战性。脂质的化学和结构多样性以及结构异构体进一步复杂了标注过程。尽管存在多种用于目标脂质鉴定的商业和开源软件，但缺乏自动生成工作流程以及与统计和生物信息学工具集成的自动化方法。我们开发了全面脂质组学自动化工作流程（CLAW）平台，带有基于定制多反应监测（MRM）前体和产物离子对转换的解析、详细的统计分析和脂质标注集成工作流程。CLAW包含多个模块，包括在与臭氧电喷雾电离（OzESI）-MRM方法相结合时识别不饱和脂质的碳碳双键位置。为展示CLAW中自动化工作流程的实用性，进行了大规模脂质组学数据收集。

    arXiv:2403.15076v1 Announce Type: cross  Abstract: Lipidomics generates large data that makes manual annotation and interpretation challenging. Lipid chemical and structural diversity with structural isomers further complicates annotation. Although, several commercial and open-source software for targeted lipid identification exists, it lacks automated method generation workflows and integration with statistical and bioinformatics tools. We have developed the Comprehensive Lipidomic Automated Workflow (CLAW) platform with integrated workflow for parsing, detailed statistical analysis and lipid annotations based on custom multiple reaction monitoring (MRM) precursor and product ion pair transitions. CLAW contains several modules including identification of carbon-carbon double bond position(s) in unsaturated lipids when combined with ozone electrospray ionization (OzESI)-MRM methodology. To demonstrate the utility of the automated workflow in CLAW, large-scale lipidomics data was collec
    
[^36]: 双侧不对称图对比学习用于推荐

    Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation

    [https://arxiv.org/abs/2403.15075](https://arxiv.org/abs/2403.15075)

    提出了一种名为双侧不对称图对比学习（BusGCL）的框架，通过考虑用户-项目节点关系密度的双侧不对称性，实现了更好的推理用户和项目图。

    

    最近的方法利用图对比学习在图结构的用户-项目交互数据中进行协同过滤，展示了其在推荐任务中的有效性。然而，它们忽略了用户-项目节点之间的差异关系密度导致多跳图交互计算后双向节点的图适应性不同，这限制了现有模型实现理想结果的能力。为了解决这一问题，我们提出了一个新的推荐任务框架，称为双侧不对称图对比学习（BusGCL），考虑了用户-项目节点关系密度的双侧不对称性，通过双侧切片对比训练更好地推理用户和项目图。特别地，考虑基于超图的图卷积网络（GCN）在挖掘隐式相似性方面的聚合能力更适合用户节点

    arXiv:2403.15075v1 Announce Type: cross  Abstract: Recent methods utilize graph contrastive Learning within graph-structured user-item interaction data for collaborative filtering and have demonstrated their efficacy in recommendation tasks. However, they ignore that the difference relation density of nodes between the user- and item-side causes the adaptability of graphs on bilateral nodes to be different after multi-hop graph interaction calculation, which limits existing models to achieve ideal results. To solve this issue, we propose a novel framework for recommendation tasks called Bilateral Unsymmetrical Graph Contrastive Learning (BusGCL) that consider the bilateral unsymmetry on user-item node relation density for sliced user and item graph reasoning better with bilateral slicing contrastive training. Especially, taking into account the aggregation ability of hypergraph-based graph convolutional network (GCN) in digging implicit similarities is more suitable for user nodes, emb
    
[^37]: MM-Diff：通过多模态条件融合实现高保真图像个性化

    MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition Integration

    [https://arxiv.org/abs/2403.15059](https://arxiv.org/abs/2403.15059)

    提出了一种名为MM-Diff的统一且免调优的图像个性化框架，能够在几秒内生成高保真的单个和多个主体图像，并利用视觉编码器同时增强文本一致性和主题保真度。

    

    最近，基于扩散模型的免调优个性化图像生成的进展令人印象深刻。然而，为了提高主题保真度，现有方法要么重新训练扩散模型，要么将其融入密集的视觉嵌入，这两者都存在泛化和效率低的问题。此外，由于无约束的跨注意机制，这些方法在多主体图像生成中出现问题。在本文中，我们提出了MM-Diff，一个统一且免调优的图像个性化框架，能够在几秒内生成单个和多个主题的高保真图像。具体地，为了同时增强文本一致性和主题保真度，MM-Diff利用视觉编码器将输入图像转换为CLS和补丁嵌入。CLS嵌入一方面用于增强文本嵌入，另一方面与补丁嵌入一起得出少量富含细节的主题嵌入。

    arXiv:2403.15059v1 Announce Type: cross  Abstract: Recent advances in tuning-free personalized image generation based on diffusion models are impressive. However, to improve subject fidelity, existing methods either retrain the diffusion model or infuse it with dense visual embeddings, both of which suffer from poor generalization and efficiency. Also, these methods falter in multi-subject image generation due to the unconstrained cross-attention mechanism. In this paper, we propose MM-Diff, a unified and tuning-free image personalization framework capable of generating high-fidelity images of both single and multiple subjects in seconds. Specifically, to simultaneously enhance text consistency and subject fidelity, MM-Diff employs a vision encoder to transform the input image into CLS and patch embeddings. CLS embeddings are used on the one hand to augment the text embeddings, and on the other hand together with patch embeddings to derive a small number of detail-rich subject embeddin
    
[^38]: Continual Vision-and-Language Navigation

    Continual Vision-and-Language Navigation

    [https://arxiv.org/abs/2403.15049](https://arxiv.org/abs/2403.15049)

    该论文提出了持续视觉和语言导航（CVLN）范式，旨在解决现有训练VLN代理方法固有的固定数据集的重大限制，使代理能够在不断变化的真实世界中进行导航。

    

    视觉和语言导航（VLN）代理根据自然语言指令和观察到的视觉信息导航到目的地。现有的VLN代理训练方法预设固定数据集，导致一个重大限制：引入新环境需要重新训练以保留已经遇到的环境的知识。这使得在不断变化的真实世界中训练VLN代理变得困难。为了解决这一限制，我们提出了持续视觉和语言导航（CVLN）范式，旨在通过一个持续学习过程评估代理。

    arXiv:2403.15049v1 Announce Type: cross  Abstract: Vision-and-Language Navigation (VLN) agents navigate to a destination using natural language instructions and the visual information they observe. Existing methods for training VLN agents presuppose fixed datasets, leading to a significant limitation: the introduction of new environments necessitates retraining with previously encountered environments to preserve their knowledge. This makes it difficult to train VLN agents that operate in the ever-changing real world. To address this limitation, we present the Continual Vision-and-Language Navigation (CVLN) paradigm, designed to evaluate agents trained through a continual learning process. For the training and evaluation of CVLN agents, we re-arrange existing VLN datasets to propose two datasets: CVLN-I, focused on navigation via initial-instruction interpretation, and CVLN-D, aimed at navigation through dialogue with other agents. Furthermore, we propose two novel rehearsal-based meth
    
[^39]: 卡通幻觉检测: 姿势感知上下文视觉学习

    Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning

    [https://arxiv.org/abs/2403.15048](https://arxiv.org/abs/2403.15048)

    该研究提出了一种用于检测由TTI模型生成的卡通角色图像中视觉幻觉的系统，通过结合姿势感知上下文视觉学习和视觉语言模型，利用RGB图像和姿势信息，实现了更准确的决策，显著提高了视觉幻觉的识别能力，推动了TTI模型在非照片真实领域的发展。

    

    大规模文本到图像（TTI）模型已经成为各种生成领域中生成训练数据的常见方法。然而，视觉幻觉，尤其是在非照片真实风格如卡通人物中包含了感知上关键的缺陷，依然是一个令人担忧的问题。我们提出了一种新颖的用于检测TTI模型生成的卡通角色图像的视觉幻觉检测系统。我们的方法利用了姿势感知上下文视觉学习（PA-ICVL）与视觉语言模型（VLMs），同时利用RGB图像和姿势信息。通过从一个经过微调的姿势估计器中获得姿势指导，我们使VLM能够做出更准确的决策。实验结果表明，在识别视觉幻觉方面，与仅依赖于RGB图像的基线方法相比，取得了显著的改进。这项研究通过减轻视觉幻觉，推动了TTI模型在非照片真实领域的潜力。

    arXiv:2403.15048v1 Announce Type: cross  Abstract: Large-scale Text-to-Image (TTI) models have become a common approach for generating training data in various generative fields. However, visual hallucinations, which contain perceptually critical defects, remain a concern, especially in non-photorealistic styles like cartoon characters. We propose a novel visual hallucination detection system for cartoon character images generated by TTI models. Our approach leverages pose-aware in-context visual learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB images and pose information. By incorporating pose guidance from a fine-tuned pose estimator, we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual hallucinations compared to baseline methods relying solely on RGB images. This research advances TTI models by mitigating visual hallucinations, expanding their potential in non-photorealistic domains.
    
[^40]: 在野外情感行为分析中使用预训练模型特征的多模态融合

    Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour Analysis In-the-wild

    [https://arxiv.org/abs/2403.15044](https://arxiv.org/abs/2403.15044)

    结合多模态融合方法和预训练模型特征，在野外情感行为分析中取得出色性能

    

    多模态融合是大多数多模态任务中的重要方法。随着大型预训练模型数量的激增，结合多模态融合方法和预训练模型特征可以在许多多模态任务中取得出色的性能。本文介绍了我们的方法，利用这两种优势来解决表达（Expr）识别和价位-唤醒（VA）估计任务。我们使用预训练模型评估Aff-Wild2数据库，然后提取模型的最终隐藏层作为特征。在进行预处理、插值或卷积以对齐提取的特征后，采用不同模型进行模态融合。我们的代码可以在GitHub上找到- FulgenceWen/ABAW6th。

    arXiv:2403.15044v1 Announce Type: cross  Abstract: Multimodal fusion is a significant method for most multimodal tasks. With the recent surge in the number of large pre-trained models, combining both multimodal fusion methods and pre-trained model features can achieve outstanding performance in many multimodal tasks. In this paper, we present our approach, which leverages both advantages for addressing the task of Expression (Expr) Recognition and Valence-Arousal (VA) Estimation. We evaluate the Aff-Wild2 database using pre-trained models, then extract the final hidden layers of the models as features. Following preprocessing and interpolation or convolution to align the extracted features, different models are employed for modal fusion. Our code is available at GitHub - FulgenceWen/ABAW6th.
    
[^41]: 灰色信息神经网络用于时间序列预测

    Grey-informed neural network for time-series forecasting

    [https://arxiv.org/abs/2403.15027](https://arxiv.org/abs/2403.15027)

    本研究提出了灰色信息神经网络（GINN），通过遵循灰色系统的微分方程模型，提高了神经网络输出的可解释性，使其能够有效处理小数据样本，产生可靠的预测。

    

    神经网络模型在各个领域展现出了出色的性能，成功解决了复杂问题。然而，大多数模型被视为黑盒，需要大量数据进行开发。因此，在数据有限的情况下，由于缺乏透明度和数据稀缺性，构建适当的模型变得具有挑战性。为了解决这些挑战，本研究建议实施灰色信息神经网络（GINN）。GINN 确保神经网络的输出遵循灰色系统的微分方程模型，提高了可解释性。此外，结合灰色系统理论中的先验知识使传统神经网络能够有效处理小数据样本。我们提出的模型已被观察到能够揭示现实世界中的潜在模式，并基于经验数据产生可靠的预测。

    arXiv:2403.15027v1 Announce Type: cross  Abstract: Neural network models have shown outstanding performance and successful resolutions to complex problems in various fields. However, the majority of these models are viewed as black-box, requiring a significant amount of data for development. Consequently, in situations with limited data, constructing appropriate models becomes challenging due to the lack of transparency and scarcity of data. To tackle these challenges, this study suggests the implementation of a grey-informed neural network (GINN). The GINN ensures that the output of the neural network follows the differential equation model of the grey system, improving interpretability. Moreover, incorporating prior knowledge from grey system theory enables traditional neural networks to effectively handle small data samples. Our proposed model has been observed to uncover underlying patterns in the real world and produce reliable forecasts based on empirical data.
    
[^42]: 魔法与量子化深度神经网络时代

    Magic for the Age of Quantized DNNs

    [https://arxiv.org/abs/2403.14999](https://arxiv.org/abs/2403.14999)

    提出了一种量化感知训练方法，引入新型标准化方法并使用缩放量化加权，实现了在最小精度降级的情况下有效的量化深度神经网络

    

    最近，深度神经网络中的参数数量急剧增加，如大型语言模型（LLMs）所示，使得在小规模计算机上进行推理变得更加困难。因此，模型压缩技术对产品整合至关重要。在本文中，我们提出了一种量化感知训练方法。我们引入了一种新颖的标准化方法（层批标准化），其独立于小批量大小，并且在推理期间不需要额外的计算成本。然后，我们通过带权标准化的缩放量化加权。我们还使用相同的函数量化激活函数，并应用代理梯度来训练既具有量化权重又具有量化激活函数的模型。我们将这种方法称为魔法与量子化DNN时代（MaQD）。实验结果表明，我们的量化方法可以在最小精度降级的情况下实现。

    arXiv:2403.14999v1 Announce Type: cross  Abstract: Recently, the number of parameters in DNNs has explosively increased, as exemplified by LLMs (Large Language Models), making inference on small-scale computers more difficult. Model compression technology is, therefore, essential for integration into products. In this paper, we propose a method of quantization-aware training. We introduce a novel normalization (Layer-Batch Normalization) that is independent of the mini-batch size and does not require any additional computation cost during inference. Then, we quantize the weights by the scaled round-clip function with the weight standardization. We also quantize activation functions using the same function and apply surrogate gradients to train the model with both quantized weights and the quantized activation functions. We call this method Magic for the age of Quantised DNNs (MaQD). Experimental results show that our quantization method can be achieved with minimal accuracy degradation
    
[^43]: 分段线性流形在深度度量学习中的应用

    Piecewise-Linear Manifolds for Deep Metric Learning

    [https://arxiv.org/abs/2403.14977](https://arxiv.org/abs/2403.14977)

    提出了一种在深度度量学习中使用分段线性流形的方法，并通过模拟高维数据流形来改善相似性估计，从而提高了无监督度量学习的性能。

    

    无监督深度度量学习（UDML）致力于仅使用无标签数据学习语义表示空间。我们提出使用分段线性逼近模型高维数据流形，其中每个低维线性片段近似于点的小邻域内的数据流形。这些邻域用于估计数据点之间的相似性。我们在实验中表明，这种相似性估计与基准相比更好地反映了地面真相。我们还展示了在无监督设置中，可以使用在监督度量学习中常用的代理来模拟分段线性流形，从而有助于提高性能。

    arXiv:2403.14977v1 Announce Type: cross  Abstract: Unsupervised deep metric learning (UDML) focuses on learning a semantic representation space using only unlabeled data. This challenging problem requires accurately estimating the similarity between data points, which is used to supervise a deep network. For this purpose, we propose to model the high-dimensional data manifold using a piecewise-linear approximation, with each low-dimensional linear piece approximating the data manifold in a small neighborhood of a point. These neighborhoods are used to estimate similarity between data points. We empirically show that this similarity estimate correlates better with the ground truth than the similarity estimates of current state-of-the-art techniques. We also show that proxies, commonly used in supervised metric learning, can be used to model the piecewise-linear manifold in an unsupervised setting, helping improve performance. Our method outperforms existing unsupervised metric learning 
    
[^44]: 一图胜千言：多模态推理中的图谱辩论

    A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning

    [https://arxiv.org/abs/2403.14972](https://arxiv.org/abs/2403.14972)

    提出了一种演绎式的图谱辩论方法（BDoG），在多模态推理中防止意见陈腐化和减少由图像引入的分心概念，实验证明其在科学问答和MMBench上取得了最先进的结果。

    

    本文介绍了一项旨在将多智能体辩论引入多模态推理的试点研究。该研究解决了两个关键挑战：由于过度总结而导致意见陈腐化，以及由于图像引入转移性概念而导致注意力分散的问题。这些挑战源自现有辩论方案的归纳（自下而上）性质。为解决这一问题，我们提出了一种演绎（自上而下）的辩论方法，称为图谱辩论（BDoG）。在BDoG中，辩论仅限于蓝图图中，以防止通过世界级摘要而导致意见陈腐化。此外，通过在图中的分支中存储证据，BDoG缓解了频繁但无关的概念带来的分散注意力现象。大量实验验证了BDoG，在科学问答和MMBench中取得了最新成果，并相较于先前的方法具有显著改进。

    arXiv:2403.14972v1 Announce Type: new  Abstract: This paper presents a pilot study aimed at introducing multi-agent debate into multimodal reasoning. The study addresses two key challenges: the trivialization of opinions resulting from excessive summarization and the diversion of focus caused by distractor concepts introduced from images. These challenges stem from the inductive (bottom-up) nature of existing debating schemes. To address the issue, we propose a deductive (top-down) debating approach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are confined to a blueprint graph to prevent opinion trivialization through world-level summarization. Moreover, by storing evidence in branches within the graph, BDoG mitigates distractions caused by frequent but irrelevant concepts. Extensive experiments validate BDoG, achieving state-of-the-art results in Science QA and MMBench with significant improvements over previous methods.
    
[^45]: 大型语言模型在自动化行为驱动开发验收测试生成中的全面评估和见解

    Comprehensive Evaluation and Insights into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation

    [https://arxiv.org/abs/2403.14965](https://arxiv.org/abs/2403.14965)

    本文提出了一种利用大型语言模型自动化生成BDD验收测试的新方法，并通过使用GPT-3.5和GPT-4等模型，展示了其在提高准确性和增强BDD实践方面的有效性。

    

    行为驱动开发（BDD）是一种促进开发人员、QA分析员和利益相关者合作的敏捷测试方法论。在本文中，我们提出了一种新颖的方法来利用大型语言模型（LLMs）来增强BDD实践，从而自动化验收测试生成。我们的研究使用零和少量提示来评估诸如GPT-3.5、GPT-4、Llama-2-13B和PaLM-2等LLMs。该论文提出了一个包括数据集、提示技术、LLMs和评估过程的详细方法论。结果表明，GPT-3.5和GPT-4生成无错误的BDD验收测试，表现更佳。少量提示技术突显了通过纳入示例进行上下文学习来提供更高准确性的能力。此外，该研究还检查了语法错误、验证准确性以及LLMs的比较分析，揭示了它们在增强BDD实践中的有效性。

    arXiv:2403.14965v1 Announce Type: cross  Abstract: Behavior-driven development (BDD) is an Agile testing methodology fostering collaboration among developers, QA analysts, and stakeholders. In this manuscript, we propose a novel approach to enhance BDD practices using large language models (LLMs) to automate acceptance test generation. Our study uses zero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B, and PaLM-2. The paper presents a detailed methodology that includes the dataset, prompt techniques, LLMs, and the evaluation process. The results demonstrate that GPT-3.5 and GPT-4 generate error-free BDD acceptance tests with better performance. The few-shot prompt technique highlights its ability to provide higher accuracy by incorporating examples for in-context learning. Furthermore, the study examines syntax errors, validation accuracy, and comparative analysis of LLMs, revealing their effectiveness in enhancing BDD practices. However, our study acknowledg
    
[^46]: 基于证据驱动的在线虚假信息检索增强响应生成

    Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation

    [https://arxiv.org/abs/2403.14952](https://arxiv.org/abs/2403.14952)

    提出了一种基于检索增强的响应生成方法(RARG)，通过收集科学来源的证据来生成反虚假信息的响应，相比现有方法，可以提高文本质量和避免过度重复。

    

    纸：arXiv:2403.14952v1   类型：交叉   摘要：在线虚假信息的泛滥对公共利益构成了重要威胁。虽然许多在线用户积极参与对抗虚假信息，但很多响应缺乏礼貌和支持事实。作为解决方案，提出了文本生成方法，可以自动生成反虚假信息的响应。然而，现有方法通常在没有利用外部知识的情况下进行端到端训练，导致文本质量不佳和响应过于重复。在本文中，我们提出了用于在线虚假信息的检索增强响应生成（RARG），该方法从科学来源收集支持证据，并基于这些证据生成反虚假信息的响应。特别是，我们的RARG包括两个阶段：（1）证据收集，我们设计了一个检索流程来检索和重新排列证据文档，使用一个dat

    arXiv:2403.14952v1 Announce Type: cross  Abstract: The proliferation of online misinformation has posed significant threats to public interest. While numerous online users actively participate in the combat against misinformation, many of such responses can be characterized by the lack of politeness and supporting facts. As a solution, text generation approaches are proposed to automatically produce counter-misinformation responses. Nevertheless, existing methods are often trained end-to-end without leveraging external knowledge, resulting in subpar text quality and excessively repetitive responses. In this paper, we propose retrieval augmented response generation for online misinformation (RARG), which collects supporting evidence from scientific sources and generates counter-misinformation responses based on the evidences. In particular, our RARG consists of two stages: (1) evidence collection, where we design a retrieval pipeline to retrieve and rerank evidence documents using a dat
    
[^47]: 简单图压缩

    Simple Graph Condensation

    [https://arxiv.org/abs/2403.14951](https://arxiv.org/abs/2403.14951)

    提出了一种简化的图压缩方法，旨在减少图神经网络所带来的不必要复杂性。

    

    大规模图上繁重的训练成本已经引起了对图压缩的极大兴趣，涉及调整图神经网络（GNNs）在小尺度压缩图上的训练以在大规模原始图上使用。现有方法主要集中在调整压缩图和原始图之间的关键指标，如梯度、GNNs的分布和轨迹，从而在下游任务上实现了令人满意的性能。然而，这些复杂指标需要复杂的计算，可能会干扰压缩图的优化过程，使得压缩过程非常繁重和不稳定。在各个领域简化模型取得成功的背景下，我们提出了一种简化的图压缩中的指标对准方法，旨在减少从GNNs继承的不必要复杂性。在我们的方法中，我们消除外部参数，仅保留目标的压缩

    arXiv:2403.14951v1 Announce Type: cross  Abstract: The burdensome training costs on large-scale graphs have aroused significant interest in graph condensation, which involves tuning Graph Neural Networks (GNNs) on a small condensed graph for use on the large-scale original graph. Existing methods primarily focus on aligning key metrics between the condensed and original graphs, such as gradients, distribution and trajectory of GNNs, yielding satisfactory performance on downstream tasks. However, these complex metrics necessitate intricate computations and can potentially disrupt the optimization process of the condensation graph, making the condensation process highly demanding and unstable. Motivated by the recent success of simplified models in various fields, we propose a simplified approach to metric alignment in graph condensation, aiming to reduce unnecessary complexity inherited from GNNs. In our approach, we eliminate external parameters and exclusively retain the target conden
    
[^48]: 一个线性层生成任务自适应低秩矩阵

    A Single Linear Layer Yields Task-Adapted Low-Rank Matrices

    [https://arxiv.org/abs/2403.14946](https://arxiv.org/abs/2403.14946)

    通过研究转换矩阵将$ W_0 $转换为低秩矩阵的关系信息，我们提出单一线性层可以生成任务自适应的低秩矩阵。

    

    低秩适应（LoRA）是一种广泛使用的参数高效调整（PEFT）方法，它通过由两个低秩矩阵$ A $和$ B $组成的增量矩阵$ \Delta W $更新初始权重矩阵$ W_0 $。先前的研究表明$ W_0 $和$ \Delta W $之间存在关联。在这项研究中，我们旨在深入探讨$ W_0 $与低秩矩阵$ A $和$ B $之间的关系，以进一步理解LoRA的行为。特别地，我们分析了一个将$ W_0 $转换为低秩矩阵的转换矩阵，其中蕴含了关系的信息。我们的分析表明转换矩阵在每一层之间是相似的。受到这些发现的启发，我们假设一个单一线性层，将每一层的$ W_0 $作为输入，可以生成任务自适应的低秩矩阵。为了验证这一假设，我们设计了一种名为有条件参数化的LoRA (CondLoRA) 方法，来更新初始权重...

    arXiv:2403.14946v1 Announce Type: cross  Abstract: Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning (PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix $\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study suggested that there is correlation between $W_0$ and $\Delta W$. In this study, we aim to delve deeper into relationships between $W_0$ and low-rank matrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular, we analyze a conversion matrix that transform $W_0$ into low-rank matrices, which encapsulates information about the relationships. Our analysis reveals that the conversion matrices are similar across each layer. Inspired by these findings, we hypothesize that a single linear layer, which takes each layer's $W_0$ as input, can yield task-adapted low-rank matrices. To confirm this hypothesis, we devise a method named Conditionally Parameterized LoRA (CondLoRA) that updates initial weig
    
[^49]: 从图结构角度统一车道级交通预测：基准和基线

    Unifying Lane-Level Traffic Prediction from a Graph Structural Perspective: Benchmark and Baseline

    [https://arxiv.org/abs/2403.14941](https://arxiv.org/abs/2403.14941)

    本文提出了一个简单的基线模型GraphMLP，基于图结构和MLP网络，在车道级交通预测中建立了统一的空间拓扑结构和预测任务，帮助突破了现有评估标准和数据公开性的限制。

    

    交通预测长期以来一直是研究中的一个焦点和关键领域，在过去几年里，既见证了从城市级到道路级预测取得的重大进展。随着车辆对一切（V2X）技术、自动驾驶和交通领域的大规模模型的进步，道路级交通预测已经成为一个不可或缺的方向。然而，这一领域的进一步进展受到了全面和统一的评估标准的缺乏以及有限的公开数据和代码的阻碍。本文对车道级交通预测中现有研究进行了广泛的分析和分类，建立了统一的空间拓扑结构和预测任务，并介绍了一个基于图结构和MLP网络的简单基线模型GraphMLP。我们复制了现有研究中尚不公开的代码，并基于此充分而公正地评估了各种模型。

    arXiv:2403.14941v1 Announce Type: cross  Abstract: Traffic prediction has long been a focal and pivotal area in research, witnessing both significant strides from city-level to road-level predictions in recent years. With the advancement of Vehicle-to-Everything (V2X) technologies, autonomous driving, and large-scale models in the traffic domain, lane-level traffic prediction has emerged as an indispensable direction. However, further progress in this field is hindered by the absence of comprehensive and unified evaluation standards, coupled with limited public availability of data and code. This paper extensively analyzes and categorizes existing research in lane-level traffic prediction, establishes a unified spatial topology structure and prediction tasks, and introduces a simple baseline model, GraphMLP, based on graph structure and MLP networks. We have replicated codes not publicly available in existing studies and, based on this, thoroughly and fairly assessed various models in 
    
[^50]: 专注驱动的推理:释放大型语言模型的潜力

    Attention-Driven Reasoning: Unlocking the Potential of Large Language Models

    [https://arxiv.org/abs/2403.14932](https://arxiv.org/abs/2403.14932)

    通过注意力机制优化，可以显著提高大型语言模型的推理能力，尤其对于非STEM问题。

    

    大型语言模型（LLMs）展示了卓越的能力，但它们的推理能力和基础机制仍不为人所了解。我们提出了一种通过注意力机制优化来增强LLMs推理能力的新方法，而无需额外的训练数据。我们确定了由非语义标记导致的注意力分布的低效率，并提出了一种算法来重新平衡偏斜分布，使模型能够抽象更加微妙的知识。我们的实验表明，推理能力得到了显着改进，特别是对于非STEM问题。我们深入探讨了注意力模式在LLMs推理中的作用，并提出了一种增强这些能力的方法，为更强大和多功能的语言模型铺平了道路。

    arXiv:2403.14932v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown remarkable capabilities, but their reasoning abilities and underlying mechanisms remain poorly understood. We present a novel approach to enhance LLMs' reasoning through attention mechanism optimization, without additional training data. We identify inefficiencies in the attention distribution caused by non-semantic tokens and propose an algorithm to re-balance the skewed distribution, enabling the model to abstract more nuanced knowledge. Our experiments demonstrate significantly improved reasoning capabilities, particularly for non-STEM questions. We provide insights into the role of attention patterns in LLMs' reasoning and propose a method to enhance these abilities, paving the way for more powerful and versatile language models.
    
[^51]: 用于高效自回归文本生成的分层跳跃解码

    Hierarchical Skip Decoding for Efficient Autoregressive Text Generation

    [https://arxiv.org/abs/2403.14919](https://arxiv.org/abs/2403.14919)

    提出了一种名为Hierarchical Skip Decoding（HSD）的新型解码策略，用于高效的自回归文本生成，通过分层地自适应跳过解码层来减少计算负载和分配计算资源。

    

    自回归解码策略是一种常用的文本生成任务方法，适用于预训练语言模型，而提前结束是一种有效的加速推断阶段的方法。在本研究中，我们提出了一种名为Hierarchical Skip Decoding（HSD）的新型解码策略，用于高效的自回归文本生成。与需要额外可训练组件的现有方法不同，HSD是一种即插即用的方法，适用于自回归文本生成模型，它根据当前序列长度以分层的方式自适应地跳过解码层，从而减少计算负载并分配计算资源。在五个带有预先训练语言模型的文本生成数据集上进行的全面实验显示，HSD在平衡效率和文本质量方面具有优势。几乎跳过一半的层，HSD可以与原始自回归d模型相比保持90%的文本质量。

    arXiv:2403.14919v1 Announce Type: cross  Abstract: Autoregressive decoding strategy is a commonly used method for text generation tasks with pre-trained language models, while early-exiting is an effective approach to speedup the inference stage. In this work, we propose a novel decoding strategy named Hierarchical Skip Decoding (HSD) for efficient autoregressive text generation. Different from existing methods that require additional trainable components, HSD is a plug-and-play method applicable to autoregressive text generation models, it adaptively skips decoding layers in a hierarchical manner based on the current sequence length, thereby reducing computational workload and allocating computation resources. Comprehensive experiments on five text generation datasets with pre-trained language models demonstrate HSD's advantages in balancing efficiency and text quality. With almost half of the layers skipped, HSD can sustain 90% of the text quality compared to vanilla autoregressive d
    
[^52]: 方位推理器：利用显式推理在社交媒体上进行零-shot立场检测

    Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning

    [https://arxiv.org/abs/2403.14895](https://arxiv.org/abs/2403.14895)

    Stance Reasoner是一种利用显式推理和世界知识进行零-shot社交媒体立场检测的方法，优于当前领先模型，并能更好地横跨目标进行泛化。

    

    社交媒体平台是丰富的观点内容来源。立场检测允许自动从这些内容中提取用户对各种话题的意见。我们关注零-shot立场检测，即模型的成功依赖于（a）对目标话题的知识；以及（b）学习可以用于新话题的通用推理策略。我们提出了Stance Reasoner，一种利用背景知识上的显式推理来引导模型推断有关文档在目标上的立场的零-shot立场检测方法。具体而言，我们的方法使用预训练语言模型作为世界知识的来源，采用上下文学习方法生成中间推理步骤。Stance Reasoner在3个Twitter数据集上表现优异，包括完全监督的模型。它可以更好地横跨目标进行泛化。

    arXiv:2403.14895v1 Announce Type: cross  Abstract: Social media platforms are rich sources of opinionated content. Stance detection allows the automatic extraction of users' opinions on various topics from such content. We focus on zero-shot stance detection, where the model's success relies on (a) having knowledge about the target topic; and (b) learning general reasoning strategies that can be employed for new topics. We present Stance Reasoner, an approach to zero-shot stance detection on social media that leverages explicit reasoning over background knowledge to guide the model's inference about the document's stance on a target. Specifically, our method uses a pre-trained language model as a source of world knowledge, with the chain-of-thought in-context learning approach to generate intermediate reasoning steps. Stance Reasoner outperforms the current state-of-the-art models on 3 Twitter datasets, including fully supervised models. It can better generalize across targets, while a
    
[^53]: AutoRE：使用大型语言模型进行文档级关系抽取

    AutoRE: Document-Level Relation Extraction with Large Language Models

    [https://arxiv.org/abs/2403.14888](https://arxiv.org/abs/2403.14888)

    AutoRE 是一种端到端的文档级关系抽取模型，采用了一种名为RHF的新颖关系抽取范式，可有效处理分布在文档中的多个关系和三元组事实。

    

    大型语言模型(LLMs)展示了在理解和生成文本方面的异常能力，这激励着许多研究人员利用它们进行信息抽取(IE)任务，包括关系抽取(RE)。然而，大多数现有方法主要设计用于句子级关系抽取(SentRE)任务，这通常涵盖了单个句子内的一组关系和三元组事实。此外，一些方法采用将关系作为候选选择集成到提示模板中的方式，导致在处理分布在给定文档中的多个关系和三元组事实时效率低下，性能亚优，并在处理文档级关系抽取(DocRE)任务时面临独特挑战。为了克服这些限制，我们介绍了AutoRE，这是一个端到端的DocRE模型，采用了一种名为RHF(Re

    arXiv:2403.14888v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional abilities in comprehending and generating text, motivating numerous researchers to utilize them for Information Extraction (IE) purposes, including Relation Extraction (RE). Nonetheless, most existing methods are predominantly designed for Sentence-level Relation Extraction (SentRE) tasks, which typically encompass a restricted set of relations and triplet facts within a single sentence. Furthermore, certain approaches resort to treating relations as candidate choices integrated into prompt templates, leading to inefficient processing and suboptimal performance when tackling Document-Level Relation Extraction (DocRE) tasks, which entail handling multiple relations and triplet facts distributed across a given document, posing distinct challenges. To overcome these limitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel RE extraction paradigm named RHF (Re
    
[^54]: 在一种成对比较方法中建立领导者

    Establishing a leader in a pairwise comparisons method

    [https://arxiv.org/abs/2403.14885](https://arxiv.org/abs/2403.14885)

    该论文展示了两种算法，可以在成对比较方法中实现操纵攻击，进而选择领导者，并通过模拟分析展示了PC矩阵大小、不一致程度和操纵轻松程度之间的关系。

    

    像选举制度一样，决策方法也容易受到决策者的操纵。有效防御这些威胁的能力只能来自对操纵机制的深入了解。在本文中，我们展示了两种可以用来发动操纵攻击的算法。它们可以在成对比较方法中使两个选择的替代方案的权重相等，并因此选择一个领导者。理论考虑与蒙特卡洛模拟相伴随，展示了PC矩阵的大小、不一致程度和操纵的轻松程度之间的关系。这项工作是我们之前研究的延续，发表在(Szybowski et al., 2023)的论文中。

    arXiv:2403.14885v1 Announce Type: new  Abstract: Abstract Like electoral systems, decision-making methods are also vulnerable to manipulation by decision-makers. The ability to effectively defend against such threats can only come from thoroughly understanding the manipulation mechanisms. In the presented article, we show two algorithms that can be used to launch a manipulation attack. They allow for equating the weights of two selected alternatives in the pairwise comparison method and, consequently, choosing a leader. The theoretical considerations are accompanied by a Monte Carlo simulation showing the relationship between the size of the PC matrix, the degree of inconsistency, and the ease of manipulation. This work is a continuation of our previous research published in the paper (Szybowski et al., 2023)
    
[^55]: 使用可微分仿真学习四足动作

    Learning Quadruped Locomotion Using Differentiable Simulation

    [https://arxiv.org/abs/2403.14864](https://arxiv.org/abs/2403.14864)

    本文提出了一种新的可微分仿真框架，通过将复杂的全身仿真解耦为两个单独的连续域，并与更精确的模型对齐，来克服四足动作中的不连续性挑战。

    

    最近大部分机器人运动控制的进展都是由无模型强化学习驱动的，本文探讨了可微分仿真的潜力。可微分仿真通过使用机器人模型计算低变异一阶梯度，承诺了更快的收敛速度和更稳定的训练，但到目前为止，其在四足机器人控制方面的应用仍然有限。可微分仿真面临的主要挑战在于由于接触丰富环境（如四足动作）中的不连续性，导致机器人任务的复杂优化景观。本文提出了一个新的可微分仿真框架以克服这些挑战。关键想法包括将可能由于接触而出现不连续性的复杂全身仿真解耦为两个单独的连续域。随后，我们将简化模型产生的机器人状态与更精确的不可微分模型对齐。

    arXiv:2403.14864v1 Announce Type: cross  Abstract: While most recent advancements in legged robot control have been driven by model-free reinforcement learning, we explore the potential of differentiable simulation. Differentiable simulation promises faster convergence and more stable training by computing low-variant first-order gradients using the robot model, but so far, its use for legged robot control has remained limited to simulation. The main challenge with differentiable simulation lies in the complex optimization landscape of robotic tasks due to discontinuities in contact-rich environments, e.g., quadruped locomotion. This work proposes a new, differentiable simulation framework to overcome these challenges. The key idea involves decoupling the complex whole-body simulation, which may exhibit discontinuities due to contact, into two separate continuous domains. Subsequently, we align the robot state resulting from the simplified model with a more precise, non-differentiable 
    
[^56]: 在基础模型和指令调优的大型语言模型中比较可信度估计

    Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models

    [https://arxiv.org/abs/2403.14859](https://arxiv.org/abs/2403.14859)

    通过比较基础和指令调优的大型语言模型在英语句子可信度任务中的表现，发现对数似然（LL）分数是最可靠的句子可信度指标，但仍低于人类表现。

    

    指令调优的LLM可以响应明确制定为提示的查询，这极大地促进了与人类用户的交互。然而，基于提示的方法可能并不总是能够利用LLM在预训练期间获得的隐式知识。本文对评估LLM中语义可信度的方法进行了全面研究。我们通过（a）明确提示和（b）直接读取模型分配给字符串的概率的隐式估计，在英语句子可信度任务中比较了基础和指令调优LLM的性能。实验1表明，跨模型架构和可信度数据集，（i）对数似然（LL）分数是句子可信度最可靠的指标，零照射提示产生不一致且通常效果不佳的结果；（ii）基于LL的性能仍低于人类表现；（iii）指令调优模型有

    arXiv:2403.14859v1 Announce Type: cross  Abstract: Instruction-tuned LLMs can respond to explicit queries formulated as prompts, which greatly facilitates interaction with human users. However, prompt-based approaches might not always be able to tap into the wealth of implicit knowledge acquired by LLMs during pre-training. This paper presents a comprehensive study of ways to evaluate semantic plausibility in LLMs. We compare base and instruction-tuned LLM performance on an English sentence plausibility task via (a) explicit prompting and (b) implicit estimation via direct readout of the probabilities models assign to strings. Experiment 1 shows that, across model architectures and plausibility datasets, (i) log likelihood ($\textit{LL}$) scores are the most reliable indicator of sentence plausibility, with zero-shot prompting yielding inconsistent and typically poor results; (ii) $\textit{LL}$-based performance is still inferior to human performance; (iii) instruction-tuned models hav
    
[^57]: 具有线性非高斯循环模型的局部因果发现

    Local Causal Discovery with Linear non-Gaussian Cyclic Models

    [https://arxiv.org/abs/2403.14843](https://arxiv.org/abs/2403.14843)

    提出一种通用的、统一的局部因果发现方法，使用线性非高斯模型，实现了从目标变量的马尔可夫毯中精确识别等效的局部有向结构和因果强度

    

    局部因果发现具有重要的实际意义，因为经常会出现发现全局因果结构并非必要的情况，兴趣仅仅在于单个目标变量。本工作提出了一种通用的、统一的局部因果发现方法，使用线性非高斯模型，无论其是否是循环的或非循环的。我们将独立成分分析的应用从全局上下文扩展到独立子空间分析，从目标变量的马尔可夫毯中精确识别等效的局部有向结构和因果强度。

    arXiv:2403.14843v1 Announce Type: cross  Abstract: Local causal discovery is of great practical significance, as there are often situations where the discovery of the global causal structure is unnecessary, and the interest lies solely on a single target variable. Most existing local methods utilize conditional independence relations, providing only a partially directed graph, and assume acyclicity for the ground-truth structure, even though real-world scenarios often involve cycles like feedback mechanisms. In this work, we present a general, unified local causal discovery method with linear non-Gaussian models, whether they are cyclic or acyclic. We extend the application of independent component analysis from the global context to independent subspace analysis, enabling the exact identification of the equivalent local directed structures and causal strengths from the Markov blanket of the target variable. We also propose an alternative regression-based method in the particular acycl
    
[^58]: 众包多语言语音可懂度测试

    Crowdsourced Multilingual Speech Intelligibility Testing

    [https://arxiv.org/abs/2403.14817](https://arxiv.org/abs/2403.14817)

    本研究提出了一种众包多语言语音可懂度评估方法，通过收集和公开发布多语言语音数据，解决了现有实验室测量昂贵且不易扩展的问题。

    

    随着生成音频特性的出现，对其对语音可懂度的影响进行快速评估的需求日益增加。除了现有的昂贵且不易扩展的实验室测量外，对于众包评估语言可懂度的工作相对较少。相关标准和建议尚未明确定义，公开的多语言测试材料也尚未充足。为了应对这一挑战，我们提出了一种众包可懂度评估方法。我们详细介绍了测试设计、多语言语音数据的收集和公开发布，以及我们早期实验的结果。

    arXiv:2403.14817v1 Announce Type: cross  Abstract: With the advent of generative audio features, there is an increasing need for rapid evaluation of their impact on speech intelligibility. Beyond the existing laboratory measures, which are expensive and do not scale well, there has been comparatively little work on crowdsourced assessment of intelligibility. Standards and recommendations are yet to be defined, and publicly available multilingual test materials are lacking. In response to this challenge, we propose an approach for a crowdsourced intelligibility assessment. We detail the test design, the collection and public release of the multilingual speech data, and the results of our early experiments.
    
[^59]: 大型语言模型在心理健康领域的机会和风险

    The opportunities and risks of large language models in mental health

    [https://arxiv.org/abs/2403.14814](https://arxiv.org/abs/2403.14814)

    大型语言模型在心理健康领域有望提供新颖的解决方案，但应注意其应用可能带来的风险，并积极采取策略减轻这些风险。

    

    全球心理健康问题的发生率正在上升，人们越来越意识到现有的心理保健模式无法充分扩展以满足需求。随着大型语言模型（LLMs）的出现，人们对它们具有创造新颖、大规模解决方案以支持心理健康的承诺感到乐观。尽管它们还处于初期阶段，LLMs已被应用于与心理健康相关的任务。本综述总结了已有文献中关于利用LLMs提供心理健康教育、评估和干预的努力，并突出了每个领域中产生积极影响的关键机会。然后，我们强调了将LLMs应用于心理健康领域所伴随的风险，并鼓励采用策略来减轻这些风险。对于心理健康支持的迫切需求必须与负责任的心理健康LLMs的开发、测试和部署相平衡。特别关键的是确保心理健康...

    arXiv:2403.14814v1 Announce Type: cross  Abstract: Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this review, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. Especially critical is ensuring that mental he
    
[^60]: 深度主动学习：现实检验

    Deep Active Learning: A Reality Check

    [https://arxiv.org/abs/2403.14800](https://arxiv.org/abs/2403.14800)

    深度主动学习方法的全面评估发现在一般情况下，没有单一模型方法能明显优于基于熵的主动学习，同时揭示了起始预算、预算步长和预训练等因素对取得优越结果的重要性，并拓展了在其他任务中的应用。

    

    我们对最先进的深度主动学习方法进行了全面评估。令人惊讶的是，在一般情况下，没有单一模型方法能明显优于基于熵的主动学习，甚至有些方法表现不如随机抽样。我们深入探讨了一些被忽视的方面，如起始预算、预算步长和预训练的影响，揭示了它们在取得卓越结果中的重要性。此外，我们将评估扩展到其他任务，探索主动学习与半监督学习、目标检测相结合的有效性。我们的实验提供了有价值的见解和具体建议，为未来的主动学习研究提供了指导。通过揭示当前方法的局限性，了解不同实验设置的影响，我们旨在激发在具有有限注释预算的真实场景中更有效地训练深度学习模型。这项工作有助于推动

    arXiv:2403.14800v1 Announce Type: cross  Abstract: We conduct a comprehensive evaluation of state-of-the-art deep active learning methods. Surprisingly, under general settings, no single-model method decisively outperforms entropy-based active learning, and some even fall short of random sampling. We delve into overlooked aspects like starting budget, budget step, and pretraining's impact, revealing their significance in achieving superior results. Additionally, we extend our evaluation to other tasks, exploring the active learning effectiveness in combination with semi-supervised learning, and object detection. Our experiments provide valuable insights and concrete recommendations for future active learning studies. By uncovering the limitations of current methods and understanding the impact of different experimental settings, we aim to inspire more efficient training of deep learning models in real-world scenarios with limited annotation budgets. This work contributes to advancing a
    
[^61]: 在时钟滴答声中规划和行动

    Planning and Acting While the Clock Ticks

    [https://arxiv.org/abs/2403.14796](https://arxiv.org/abs/2403.14796)

    提出了并发规划和执行的新问题设置，允许在规划终止之前派发动作，适用于一些时间压力较大的情况。

    

    标准的时间规划假设规划是离线进行的，然后执行从时间0开始。最近，引入了情境化时间规划，其中规划从时间0开始，然后在规划终止后执行。情境化时间规划反映了更加真实的情景，即在规划过程中时间是流逝的。然而，在情境化时间规划中，必须在执行任何动作之前生成完整的计划。在一些时间压力较大的问题中，时间太紧迫，无法在必须执行第一个动作之前完成规划。例如，一辆自动驾驶汽车在一辆卡车朝它倒车时，可能现在应该躲开，然后再计划如何到达目的地。在本文中，我们提出了一个新的问题设置：并发规划和执行，即在规划终止之前可以派发（执行）动作。与以前关于规划和执行的工作不同，我们必须处理实时时钟的截止时间。

    arXiv:2403.14796v1 Announce Type: new  Abstract: Standard temporal planning assumes that planning takes place offline and then execution starts at time 0. Recently, situated temporal planning was introduced, where planning starts at time 0 and execution occurs after planning terminates. Situated temporal planning reflects a more realistic scenario where time passes during planning. However, in situated temporal planning a complete plan must be generated before any action is executed. In some problems with time pressure, timing is too tight to complete planning before the first action must be executed. For example, an autonomous car that has a truck backing towards it should probably move out of the way now and plan how to get to its destination later. In this paper, we propose a new problem setting: concurrent planning and execution, in which actions can be dispatched (executed) before planning terminates. Unlike previous work on planning and execution, we must handle wall clock deadli
    
[^62]: Particip-AI: 一种民主调查框架，用于预测未来人工智能的使用情况、危害和益处

    Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits

    [https://arxiv.org/abs/2403.14791](https://arxiv.org/abs/2403.14791)

    Particip-AI 是一个框架，旨在通过从非专业公众那里收集当前和未来的人工智能使用情况、危害和益处，引领人工智能的民主发展。

    

    通用人工智能，如ChatGPT，似乎降低了公众使用人工智能及利用其力量的门槛。然而，人工智能的治理和发展仍掌握在少数人手中，发展速度加快且缺乏风险评估。作为迈向人工智能民主治理和风险评估的第一步，我们介绍了Particip-AI，一个框架用于从非专业公众那里收集当前和将来的人工智能使用情况及其危害和益处。我们的框架允许通过收集使用情况更加细致和详细地研究公众对人工智能的意见，通过在备选方案下（即开发和不开发一种使用情况）进行风险评估呈现出多样化的危害，并通过做出对其发展的结论性选择阐明人工智能发展的紧张关系。为展示我们的框架对指导民主人工智能的承诺，我们收集了来自295个人口多样化的回应。

    arXiv:2403.14791v1 Announce Type: cross  Abstract: General purpose AI, such as ChatGPT, seems to have lowered the barriers for the public to use AI and harness its power. However, the governance and development of AI still remain in the hands of a few, and the pace of development is accelerating without proper assessment of risks. As a first step towards democratic governance and risk assessment of AI, we introduce Particip-AI, a framework to gather current and future AI use cases and their harms and benefits from non-expert public. Our framework allows us to study more nuanced and detailed public opinions on AI through collecting use cases, surfacing diverse harms through risk assessment under alternate scenarios (i.e., developing and not developing a use case), and illuminating tensions over AI development through making a concluding choice on its development. To showcase the promise of our framework towards guiding democratic AI, we gather responses from 295 demographically diverse 
    
[^63]: 基于潜在扩散模型进行属性保留图像匿名化

    Latent Diffusion Models for Attribute-Preserving Image Anonymization

    [https://arxiv.org/abs/2403.14790](https://arxiv.org/abs/2403.14790)

    这项研究提出了基于潜在扩散模型的图像匿名化方法，通过保留场景中的每个元素传达相同意义，但使得重新识别变得困难。

    

    图像匿名化的生成技术对于生成能够保护图像中被描述个体隐私的数据集具有巨大潜力，同时实现高数据保真度和实用性。现有方法在保留面部属性方面进行了大量研究，但未能采纳更全面的视角，考虑在匿名化过程中将场景和背景纳入考虑。本文提出了一种基于潜在扩散模型（LDMs）的图像匿名化方法，据我们所知，这是首个采用这种方法的研究。场景的每一个元素都被保留以传达相同的意义，但以一种使重新识别变得困难的方式进行操纵。我们针对此目的提出了两种LDMs：CAMOUFLaGE-Base利用预训练的ControlNets的组合，以及一种旨在增加实际图像和匿名化图像之间距离的新控制机制。CAMOFULaGE-Light基于Adapter技术，并配备了一种编码

    arXiv:2403.14790v1 Announce Type: cross  Abstract: Generative techniques for image anonymization have great potential to generate datasets that protect the privacy of those depicted in the images, while achieving high data fidelity and utility. Existing methods have focused extensively on preserving facial attributes, but failed to embrace a more comprehensive perspective that considers the scene and background into the anonymization process. This paper presents, to the best of our knowledge, the first approach to image anonymization based on Latent Diffusion Models (LDMs). Every element of a scene is maintained to convey the same meaning, yet manipulated in a way that makes re-identification difficult. We propose two LDMs for this purpose: CAMOUFLaGE-Base exploits a combination of pre-trained ControlNets, and a new controlling mechanism designed to increase the distance between the real and anonymized images. CAMOFULaGE-Light is based on the Adapter technique, coupled with an encoding
    
[^64]: 多智体VQA：探索零样本视觉问答中的多智体基础模型

    Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering

    [https://arxiv.org/abs/2403.14783](https://arxiv.org/abs/2403.14783)

    本研究提出了一种自适应多智体系统，名为多智体VQA，通过使用专门的智体工具，克服了基础模型在目标检测和计数中的局限性，在零样本情况下实现了良好的性能，为未来研究提供了新的方向。

    

    这项工作探讨了基础模型在视觉问答（VQA）任务中的零样本能力。我们提出了一种自适应多智体系统，命名为多智体VQA，通过使用专门的智体作为工具，以克服基础模型在目标检测和计数中的局限性。与现有方法不同，我们的研究侧重于在不对其进行特定VQA数据集微调的情况下系统的性能，使其在开放世界中更加实用和稳健。我们在零样本场景下提出了初步实验结果，并突出了一些失败案例，为未来研究提供了新的方向。

    arXiv:2403.14783v1 Announce Type: cross  Abstract: This work explores the zero-shot capabilities of foundation models in Visual Question Answering (VQA) tasks. We propose an adaptive multi-agent system, named Multi-Agent VQA, to overcome the limitations of foundation models in object detection and counting by using specialized agents as tools. Unlike existing approaches, our study focuses on the system's performance without fine-tuning it on specific VQA datasets, making it more practical and robust in the open world. We present preliminary experimental results under zero-shot scenarios and highlight some failure cases, offering new directions for future research.
    
[^65]: StreamingT2V: 一种一致、动态和可扩展的基于文本的长视频生成方法

    StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text

    [https://arxiv.org/abs/2403.14773](https://arxiv.org/abs/2403.14773)

    StreamingT2V是一种自回归方法，用于生成长视频，可以产生80、240、600、1200帧甚至更多帧的视频，并具有平滑的过渡。

    

    arXiv:2403.14773v1 公告类型: 交叉 摘要: 文本到视频的扩散模型可以生成遵循文本指令的高质量视频，使得创建多样化和个性化内容变得更加容易。然而，现有方法大多集中在生成高质量的短视频（通常为16或24帧），当天真地扩展到长视频合成的情况时，通常会出现硬裁剪。为了克服这些限制，我们引入了StreamingT2V，这是一种自回归方法，用于生成80、240、600、1200或更多帧的长视频，具有平滑的过渡。主要组件包括：（i）一种名为条件注意力模块（CAM）的短期记忆块，通过注意机制将当前生成条件设置为先前块提取的特征，实现一致的块过渡，（ii）一种名为外观保存模块的长期记忆块，从第一个视频块中提取高级场景和对象特征，以防止th

    arXiv:2403.14773v1 Announce Type: cross  Abstract: Text-to-video diffusion models enable the generation of high-quality videos that follow text instructions, making it easy to create diverse and individual content. However, existing approaches mostly focus on high-quality short video generation (typically 16 or 24 frames), ending up with hard-cuts when naively extended to the case of long video synthesis. To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions. The key components are:(i) a short-term memory block called conditional attention module (CAM), which conditions the current generation on the features extracted from the previous chunk via an attentional mechanism, leading to consistent chunk transitions, (ii) a long-term memory block called appearance preservation module, which extracts high-level scene and object features from the first video chunk to prevent th
    
[^66]: 通过稀疏编码架构提高模型逆推攻击的鲁棒性

    Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures

    [https://arxiv.org/abs/2403.14772](https://arxiv.org/abs/2403.14772)

    通过稀疏编码层设计新网络架构以提高对模型逆推攻击的鲁棒性。

    

    最近的模型逆推攻击算法允许对手通过反复查询神经网络并检查其输出来重建网络的私有训练数据。 在这项工作中，我们开发了一种新颖的网络架构，利用稀疏编码层来获得对这类攻击的卓越鲁棒性。 三十年来，计算机科学研究已经研究了稀疏编码在图像去噪，目标识别和对抗性误分设置中的作用，但据我们所知，其与最先进的隐私漏洞之间的联系尚未被研究。然而，稀疏编码架构提供了一种有利的手段来抵御模型逆推攻击，因为它们允许我们控制编码在网络的中间表示中的无关私人信息的数量，而这种方式可以在训练过程中高效计算，并且众所周知只有较小的影响。

    arXiv:2403.14772v1 Announce Type: cross  Abstract: Recent model inversion attack algorithms permit adversaries to reconstruct a neural network's private training data just by repeatedly querying the network and inspecting its outputs. In this work, we develop a novel network architecture that leverages sparse-coding layers to obtain superior robustness to this class of attacks. Three decades of computer science research has studied sparse coding in the context of image denoising, object recognition, and adversarial misclassification settings, but to the best of our knowledge, its connection to state-of-the-art privacy vulnerabilities remains unstudied. However, sparse coding architectures suggest an advantageous means to defend against model inversion attacks because they allow us to control the amount of irrelevant private information encoded in a network's intermediate representations in a manner that can be computed efficiently during training and that is known to have little effect
    
[^67]: NaNa和MiGu：语义数据增强技术在图神经网络中增强蛋白质分类

    NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein Classification in Graph Neural Networks

    [https://arxiv.org/abs/2403.14736](https://arxiv.org/abs/2403.14736)

    提出了NaNa和MiGu两种语义数据增强方法，结合了蛋白质的主链化学和侧链生物物理信息，用于增强图神经网络中的蛋白质分类任务。

    

    蛋白质分类任务在药物发现中至关重要。现实世界中的蛋白质结构是动态变化的，这将决定蛋白质的性质。然而，现有的机器学习方法，如ProNet，仅访问有限的构象特征和蛋白质侧链特征，导致预测中蛋白质结构的不切实际和蛋白质类别的不准确性。在本文中，我们提出了新颖的语义数据增强方法NaNa和MiGu，将蛋白质主链化学和侧链生物物理信息纳入蛋白质分类任务和共嵌残差学习框架。具体来说，我们利用了蛋白质的分子生物物理、二级结构、化学键和离子特征来促进蛋白质分类任务。

    arXiv:2403.14736v1 Announce Type: cross  Abstract: Protein classification tasks are essential in drug discovery. Real-world protein structures are dynamic, which will determine the properties of proteins. However, the existing machine learning methods, like ProNet (Wang et al., 2022a), only access limited conformational characteristics and protein side-chain features, leading to impractical protein structure and inaccuracy of protein classes in their predictions. In this paper, we propose novel semantic data augmentation methods, Novel Augmentation of New Node Attributes (NaNa), and Molecular Interactions and Geometric Upgrading (MiGu) to incorporate backbone chemical and side-chain biophysical information into protein classification tasks and a co-embedding residual learning framework. Specifically, we leverage molecular biophysical, secondary structure, chemical bonds, and ionic features of proteins to facilitate protein classification tasks. Furthermore, our semantic augmentation me
    
[^68]: 一项神经代码智能的调查：范式、进展与未来

    A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond

    [https://arxiv.org/abs/2403.14734](https://arxiv.org/abs/2403.14734)

    神经代码智能领域的调查系统回顾了50多种代表性模型和超过680项相关作品，突出了不同研究阶段的范式和技术转变。

    

    arXiv:2403.14734v1 公告类型: 跨领域 摘要: 神经代码智能--利用深度学习理解、生成和优化代码--在整个社会上具有巨大的潜力，可产生深远影响。作为自然语言和编程语言之间的桥梁，这一领域在过去几年引起了两个研究社区研究人员的极大关注。本调查系统地和按时间顺序回顾了代码智能方面的进展，包括50多种代表性模型及其变体、20多种任务类别以及超过680项相关作品。我们遵循历史进展，跟踪不同研究阶段的范式转变（例如，从使用循环神经网络对代码建模到大型语言模型时代）。同时，我们重点介绍了不同阶段涵盖的模型、任务和评估的主要技术转变。对于应用，我们

    arXiv:2403.14734v1 Announce Type: cross  Abstract: Neural Code Intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. Bridging the gap between Natural Language and Programming Language, this domain has drawn significant attention from researchers in both research communities over the past few years. This survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over 50 representative models and their variants, more than 20 categories of tasks, and an extensive coverage of over 680 related works. We follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of Large Language Models). Concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. For applications, we 
    
[^69]: 用多任务学习进行开放知识库规范化

    Open Knowledge Base Canonicalization with Multi-task Learning

    [https://arxiv.org/abs/2403.14733](https://arxiv.org/abs/2403.14733)

    提出了一个多任务学习框架MulCanon来处理开放知识库（OKB）规范化问题，并通过在软聚类过程中使用扩散模型来改进名词短语的表示。

    

    大型开放知识库（OKB）的构建对于诸多基于知识的网络应用如网络搜索至关重要。然而，OKB中的名词短语和关系短语往往存在冗余和歧义，这需要对OKB进行规范化的研究。当前的解决方案通过设计先进的聚类算法，并使用知识图嵌入（KGE）进一步促进规范化过程。然而，这些工作未能充分利用聚类和KGE学习之间的协同作用，并且为这些子任务设计的方法是次优的。因此，我们提出了一个名为MulCanon的多任务学习框架来解决OKB的规范化问题。此外，在软聚类过程中使用扩散模型来改进名词短语的表示，带来更准确的表征。

    arXiv:2403.14733v1 Announce Type: new  Abstract: The construction of large open knowledge bases (OKBs) is integral to many knowledge-driven applications on the world wide web such as web search. However, noun phrases and relational phrases in OKBs often suffer from redundancy and ambiguity, which calls for the investigation on OKB canonicalization. Current solutions address OKB canonicalization by devising advanced clustering algorithms and using knowledge graph embedding (KGE) to further facilitate the canonicalization process. Nevertheless, these works fail to fully exploit the synergy between clustering and KGE learning, and the methods designed for these subtasks are sub-optimal. To this end, we put forward a multi-task learning framework, namely MulCanon, to tackle OKB canonicalization. In addition, diffusion model is used in the soft clustering process to improve the noun phrase representations with neighboring information, which can lead to more accurate representations. MulCano
    
[^70]: 理解为何标签平滑会降低选择性分类的效果以及如何解决这个问题

    Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It

    [https://arxiv.org/abs/2403.14715](https://arxiv.org/abs/2403.14715)

    LS方法在深度神经网络分类器训练中的标签平滑效果被发现会负面影响选择性分类，通过影响模型预测不确定性，此研究阐明了这一现象。

    

    标签平滑（LS）是一种流行的深度神经网络分类器训练的正则化方法，因为它在提高测试准确性方面效果显著，并且实现简单。"硬"的one-hot标签通过将概率质量均匀分配给其他类别来进行"平滑化"，从而减少过度拟合。在这项工作中，我们揭示了LS如何负面影响选择性分类（SC）- 其目标是利用模型的预测不确定性来拒绝错误分类。我们首先在一系列任务和架构中从经验上证明LS会导致SC的一致性降级。然后，我们通过分析logit级别的梯度来解释这一点，表明LS通过在错误概率低时更加正则化最大logit，而在错误概率高时更少正则化，加剧了过度自信和低自信。这阐明了以前报道的强分类器在SC中性能不佳的实验结果。

    arXiv:2403.14715v1 Announce Type: cross  Abstract: Label smoothing (LS) is a popular regularisation method for training deep neural network classifiers due to its effectiveness in improving test accuracy and its simplicity in implementation. "Hard" one-hot labels are "smoothed" by uniformly distributing probability mass to other classes, reducing overfitting. In this work, we reveal that LS negatively affects selective classification (SC) - where the aim is to reject misclassifications using a model's predictive uncertainty. We first demonstrate empirically across a range of tasks and architectures that LS leads to a consistent degradation in SC. We then explain this by analysing logit-level gradients, showing that LS exacerbates overconfidence and underconfidence by regularising the max logit more when the probability of error is low, and less when the probability of error is high. This elucidates previously reported experimental results where strong classifiers underperform in SC. We
    
[^71]: 用于官僚生产力的人工智能：衡量AI帮助自动化英国政府1.43亿笔交易的潜力

    AI for bureaucratic productivity: Measuring the potential of AI to help automate 143 million UK government transactions

    [https://arxiv.org/abs/2403.14712](https://arxiv.org/abs/2403.14712)

    人工智能在自动化英国政府1.43亿笔交易中展现巨大潜力，节省每笔复杂交易一分钟的时间即可节约大量工作时间。

    

    目前政府对人工智能潜力在改善公共服务生产力方面充满期待，通过自动化复杂但重复的官僚任务，从而释放熟练员工的时间。本文通过对英国中央政府面向公民的官僚决策流程进行规模化探索，衡量其AI驱动自动化的潜力。我们估计英国中央政府每年进行大约10亿笔面向公众的交易，提供约400种服务，其中大约有1.43亿笔是复杂重复交易。我们估计这些复杂交易中有84%是高度可自动化的，代表着一个巨大的潜在机遇：仅节省一个复杂交易的一分钟平均时间，就相当于节省约1200人年的工作量。

    arXiv:2403.14712v1 Announce Type: cross  Abstract: There is currently considerable excitement within government about the potential of artificial intelligence to improve public service productivity through the automation of complex but repetitive bureaucratic tasks, freeing up the time of skilled staff. Here, we explore the size of this opportunity, by mapping out the scale of citizen-facing bureaucratic decision-making procedures within UK central government, and measuring their potential for AI-driven automation. We estimate that UK central government conducts approximately one billion citizen-facing transactions per year in the provision of around 400 services, of which approximately 143 million are complex repetitive transactions. We estimate that 84% of these complex transactions are highly automatable, representing a huge potential opportunity: saving even an average of just one minute per complex transaction would save the equivalent of approximately 1,200 person-years of work e
    
[^72]: 人在环AI用于作弊环检测

    Human-in-the-Loop AI for Cheating Ring Detection

    [https://arxiv.org/abs/2403.14711](https://arxiv.org/abs/2403.14711)

    本文介绍了一种人在环AI作弊环检测系统，通过设计原则、评估方法及符合负责任AI标准，实现了检测作弊者的目标。

    

    近年来，由于在线考试的易获取性，在线考试变得越来越受欢迎。然而，人们对在线考试的安全性提出了一些担忧，特别是在专业作弊服务帮助恶意考生通过考试的背景下，形成了所谓的“作弊环”。本文介绍了一种人在环AI作弊环检测系统，旨在检测和阻止这些作弊环。我们概述了这种人在环AI系统的基本逻辑，探讨了其设计原则，旨在实现检测作弊者的目标。此外，我们阐述了用于评估其性能和公平性的方法，旨在减轻与AI系统相关的意外风险。系统的设计和开发遵循了负责任的AI（RAI）标准，确保在整个开发过程中整合了伦理考虑。

    arXiv:2403.14711v1 Announce Type: cross  Abstract: Online exams have become popular in recent years due to their accessibility. However, some concerns have been raised about the security of the online exams, particularly in the context of professional cheating services aiding malicious test takers in passing exams, forming so-called "cheating rings". In this paper, we introduce a human-in-the-loop AI cheating ring detection system designed to detect and deter these cheating rings. We outline the underlying logic of this human-in-the-loop AI system, exploring its design principles tailored to achieve its objectives of detecting cheaters. Moreover, we illustrate the methodologies used to evaluate its performance and fairness, aiming to mitigate the unintended risks associated with the AI system. The design and development of the system adhere to Responsible AI (RAI) standards, ensuring that ethical considerations are integrated throughout the entire development process.
    
[^73]: 使用推荐模型为诵读障碍学生提供支持

    Use of recommendation models to provide support to dyslexic students

    [https://arxiv.org/abs/2403.14710](https://arxiv.org/abs/2403.14710)

    使用AI推荐模型为诵读障碍学生提供高度个性化的支持工具，以提供有针对性的实用帮助。

    

    阅读障碍是最普遍的特定学习障碍，严重影响不同认知领域。这反过来在学习过程中严重影响辨读障碍学生。因此，这些学生必须得到特定的支持。此外，这种支持必须高度个性化，因为障碍产生的问题可能因人而异。在这项工作中，我们探讨了利用人工智能为辨读障碍学生建议最合适的支持工具的可能性，以提供一种真正有用的有针对性帮助。为此，我们依赖于推荐算法，这是机器学习的一个分支，旨在检测个人偏好并提供最合适的建议。我们实施和训练了三个协同过滤推荐模型，分别是基于项目的模型、基于用户的模型和加权混合模型，并研究了它们在一个较大数据集上的性能。

    arXiv:2403.14710v1 Announce Type: cross  Abstract: Dyslexia is the most widespread specific learning disorder and significantly impair different cognitive domains. This, in turn, negatively affects dyslexic students during their learning path. Therefore, specific support must be given to these students. In addition, such a support must be highly personalized, since the problems generated by the disorder can be very different from one to another. In this work, we explored the possibility of using AI to suggest the most suitable supporting tools for dyslexic students, so as to provide a targeted help that can be of real utility. To do this, we relied on recommendation algorithms, which are a branch of machine learning, that aim to detect personal preferences and provide the most suitable suggestions. We hence implemented and trained three collaborative-filtering recommendation models, namely an item-based, a user-based and a weighted-hybrid model, and studied their performance on a large
    
[^74]: 保护营销研究：人工智能制造的虚假信息的生成、识别和缓解

    Safeguarding Marketing Research: The Generation, Identification, and Mitigation of AI-Fabricated Disinformation

    [https://arxiv.org/abs/2403.14706](https://arxiv.org/abs/2403.14706)

    人工智能的生成能力带来了对营销研究的威胁，本研究展示了其在误导性用户生成内容方面的熟练程度，量化了其对营销研究的扰乱影响，并提出了高级检测框架。

    

    生成式人工智能带来了生成内容几乎能够模仿人类贡献的能力，引入了一种前所未有的威胁：这些模型的大规模部署可以用来操纵公众舆论和扭曲认知，导致对数字平台的信任下降。本研究在营销文献和实践方面做出了三方面贡献。首先，它展示了人工智能在制造模仿真实内容形式的误导性用户生成内容（UGC）方面的熟练程度。其次，它量化了这种UGC对营销研究的扰乱影响，突显了即使是最低水平的虚假信息也会使分析框架受到影响的脆弱性。第三，它提出并评估了高级检测框架，揭示了标准技术无法过滤掉人工智能生成的虚假信息。我们主张采取综合性方法来保护营销研究，整合

    arXiv:2403.14706v1 Announce Type: cross  Abstract: Generative AI has ushered in the ability to generate content that closely mimics human contributions, introducing an unprecedented threat: Deployed en masse, these models can be used to manipulate public opinion and distort perceptions, resulting in a decline in trust towards digital platforms. This study contributes to marketing literature and practice in three ways. First, it demonstrates the proficiency of AI in fabricating disinformative user-generated content (UGC) that mimics the form of authentic content. Second, it quantifies the disruptive impact of such UGC on marketing research, highlighting the susceptibility of analytics frameworks to even minimal levels of disinformation. Third, it proposes and evaluates advanced detection frameworks, revealing that standard techniques are insufficient for filtering out AI-generated disinformation. We advocate for a comprehensive approach to safeguarding marketing research that integrates
    
[^75]: 概念最佳匹配：评估新兴通信中的组合性

    Concept-Best-Matching: Evaluating Compositionality in Emergent Communication

    [https://arxiv.org/abs/2403.14705](https://arxiv.org/abs/2403.14705)

    提出了一种评估新兴通信组合性的方法，通过找到 emerged words 与 natural language concepts 之间的最佳匹配，实现了直接而可解释的映射。

    

    学习沟通以完成给定任务的人工智能代理获取的通信协议通常对人类来说是不透明的。大量研究尝试通过各种评估方法评估新兴通信，其中\emph{组合性}是一个突出的期望特征。然而，当前的评估程序并未直接暴露新兴通信的组合性。我们提出了一种评估新兴通信组合性的方法，即找到新兴词汇与自然语言概念之间的最佳匹配。最佳匹配算法提供了全局分数和从新兴词汇到自然语言概念的翻译映射。据我们所知，这是首次提供新兴词汇与人类概念之间直接而可解释的映射。

    arXiv:2403.14705v1 Announce Type: new  Abstract: Artificial agents that learn to communicate in order to accomplish a given task acquire communication protocols that are typically opaque to a human. A large body of work has attempted to evaluate the emergent communication via various evaluation measures, with \emph{compositionality} featuring as a prominent desired trait. However, current evaluation procedures do not directly expose the compositionality of the emergent communication. We propose a procedure to assess the compositionality of emergent communication by finding the best-match between emerged words and natural language concepts. The best-match algorithm provides both a global score and a translation-map from emergent words to natural language concepts. To the best of our knowledge, it is the first time that such direct and interpretable mapping between emergent words and human concepts is provided.
    
[^76]: 一个最小联盟逻辑

    A minimal coalition logic

    [https://arxiv.org/abs/2403.14704](https://arxiv.org/abs/2403.14704)

    提出了一个基于一般并发博弈模型的最小联盟逻辑，不具备传统模型中的独立性、序列性和确定性假设，展示了其完备性并与传统模型进行了比较

    

    联盟逻辑是战略推理研究中的一个中心逻辑。本文首先指出联盟逻辑模型，即并发博弈模型，存在三个过于强的假设。其一是代理的独立性；即，两个不同联盟的两个可用联合动作的合并总是可以合并成两个联盟的联盟。其二是序列性；即，联盟总是有可用的联合动作。其三是确定性，即，大联盟的合作动作总是有唯一结果。其次，我们提出了一个基于一般并发博弈模型的联盟逻辑，该模型不具备这三个假设。我们展示了这一逻辑的完备性，并与联盟逻辑进行了详细比较。在战略推理的背景下，这一逻辑似乎是最小的。

    arXiv:2403.14704v1 Announce Type: cross  Abstract: Coalition logic is a central logic in strategic reasoning studies. In this paper, we first argue that Coalition Logic models, concurrent game models, have three too-strong assumptions. The first one is the independence of agents; that is, the merge of two available joint actions of two disjoint coalitions is always available for the union of the two coalitions. The second one is seriality; that is, coalitions always have available joint actions. The third one is determinism, that is, the grand coalition's joint actions always have a unique outcome. Second, we present a coalition logic based on general concurrent game models, which do not have the three assumptions. We show the completeness of this logic and compare it with Coalition Logic in detail. This logic seems minimal in the context of strategic reasoning.
    
[^77]: 基于人工智能的方法来解决开放复杂环境中的不可预测问题

    An AIC-based approach for articulating unpredictable problems in open complex environments

    [https://arxiv.org/abs/2403.14697](https://arxiv.org/abs/2403.14697)

    该研究论文提出了一种基于人工智能的方法，旨在通过采用系统方法来提高建筑师在设计可靠系统方面的预测能力，重点关注动态和不可预测环境下系统的操作。

    

    这篇研究论文提出了一种方法，旨在增强建筑师在设计和保证系统方面的预测能力，重点关注在动态和不可预测环境中运行的系统。通过采用系统方法，我们旨在改善建筑师在设计可靠系统（例如基于机器学习的系统）时的预测能力。利用航空航天案例研究来说明这种方法。识别了影响飞机检测的多个因素（挑战），展示了我们方法在复杂操作环境中的有效性。我们的方法主要旨在增强建筑师的预测能力。

    arXiv:2403.14697v1 Announce Type: cross  Abstract: This research paper presents an approach to enhancing the predictive capability of architects in the design and assurance of systems, focusing on systems operating in dynamic and unpredictable environments. By adopting a systems approach, we aim to improve architects' predictive capabilities in designing dependable systems (for example, ML-based systems). An aerospace case study is used to illustrate the approach. Multiple factors (challenges) influencing aircraft detection are identified, demonstrating the effectiveness of our approach in a complex operational setting. Our approach primarily aimed to enhance the architect's predictive capability.
    
[^78]: GPT语言模型在大学教学活动创新中的应用

    Application of GPT Language Models for Innovation in Activities in University Teaching

    [https://arxiv.org/abs/2403.14694](https://arxiv.org/abs/2403.14694)

    GPT语言模型在大学教学活动创新中的应用不仅可以支持理解和生成内容、问题解决，还可以在个性化和测试纠错等方面提供帮助，但是在国际化方面需注意避免其误用导致的全球问题。

    

    GPT（生成式预训练变换器）语言模型是一种人工智能和自然语言处理技术，可以实现自动文本生成。在将GPT语言模型应用于大学教学的各个维度中存在着日益增长的兴趣。从学生和教师活动创新的角度来看，它们可以在理解和生成内容、问题解决以及个性化和测试纠错等方面提供支持。从国际化的角度来看，误用这些模型代表着一个需要采取一系列共同措施的全球问题，这些措施需要各地区的大学共同参与。在一些国家，已经对评估工具进行了审查，以确保工作是由学生完成的，而不是由人工智能完成的。为此，我们在计算机科学这种代表性学科的一个具体课题，比如软件工程中进行了详细的实验。

    arXiv:2403.14694v1 Announce Type: cross  Abstract: The GPT (Generative Pre-trained Transformer) language models are an artificial intelligence and natural language processing technology that enables automatic text generation. There is a growing interest in applying GPT language models to university teaching in various dimensions. From the perspective of innovation in student and teacher activities, they can provide support in understanding and generating content, problem-solving, as well as personalization and test correction, among others. From the dimension of internationalization, the misuse of these models represents a global problem that requires taking a series of common measures in universities from different geographical areas. In several countries, there has been a review of assessment tools to ensure that work is done by students and not by AI. To this end, we have conducted a detailed experiment in a representative subject of Computer Science such as Software Engineering, wh
    
[^79]: A2CI：基于云的面向服务的地理空间下层结构，支持大气研究

    A2CI: A Cloud-based, Service-oriented Geospatial Cyberinfrastructure to Support Atmospheric Research

    [https://arxiv.org/abs/2403.14693](https://arxiv.org/abs/2403.14693)

    本论文介绍了一个基于云的、面向服务的地理空间下层结构A2CI，旨在支持大气研究，能有效应对收集和整理的大量地球科学数据所带来的挑战。

    

    大地科学数据为科学界提供了巨大的机遇。利用遥感卫星、地面传感器网络甚至社交媒体输入收集到的丰富信息，现在可以进行更多大规模、长期和高分辨率的研究。然而，NASA和其他政府机构每小时收集和整理的数百TB信息对于希望改善对地球大气系统的理解的大气科学家来说构成了重大挑战。这些挑战包括大量数据的有效发现、组织、分析和可视化。本文报告了一个由NSF资助的项目的成果，该项目开发了一个地理空间下层结构——A2CI（大气分析下层结构），以支持大气研究。首先我们介绍了基于服务的系统框架，然后详细描述了...

    arXiv:2403.14693v1 Announce Type: cross  Abstract: Big earth science data offers the scientific community great opportunities. Many more studies at large-scales, over long-terms and at high resolution can now be conducted using the rich information collected by remote sensing satellites, ground-based sensor networks, and even social media input. However, the hundreds of terabytes of information collected and compiled on an hourly basis by NASA and other government agencies present a significant challenge for atmospheric scientists seeking to improve the understanding of the Earth atmospheric system. These challenges include effective discovery, organization, analysis and visualization of large amounts of data. This paper reports the outcomes of an NSF-funded project that developed a geospatial cyberinfrastructure -- the A2CI (Atmospheric Analysis Cyberinfrastructure) -- to support atmospheric research. We first introduce the service-oriented system framework then describe in detail the
    
[^80]: AI 评估量表（AIAS）的实践：GenAI 支持评估的试点实施

    The AI Assessment Scale (AIAS) in action: A pilot implementation of GenAI supported assessment

    [https://arxiv.org/abs/2403.14692](https://arxiv.org/abs/2403.14692)

    该论文介绍了AI评估量表（AIAS）的实践应用，通过灵活框架将GenAI技术纳入教育评估中，显著降低了与GenAI相关的学术不端案件，提高了学生的学业成绩。

    

    在高等教育中快速采用生成人工智能（GenAI）技术引发了对学术诚信、评估实践和学生学习的关注。禁止或阻止GenAI工具已被证明是无效的，惩罚性方法忽略了这些技术的潜在好处。本文介绍了在英国越南大学（BUV）进行的试点研究的结果，探讨了人工智能评估量表（AIAS）的实施，这是一个灵活的框架，用于将GenAI纳入教育评估中。AIAS由五个级别组成，从“无AI”到“完全AI”，使教育工作者能够设计侧重于需要人类输入和批判性思维的评估。在实施AIAS后，试点研究结果表明与GenAI相关的学术不端案件显着减少，学生成绩提高了5.9%。

    arXiv:2403.14692v1 Announce Type: cross  Abstract: The rapid adoption of Generative Artificial Intelligence (GenAI) technologies in higher education has raised concerns about academic integrity, assessment practices, and student learning. Banning or blocking GenAI tools has proven ineffective, and punitive approaches ignore the potential benefits of these technologies. This paper presents the findings of a pilot study conducted at British University Vietnam (BUV) exploring the implementation of the Artificial Intelligence Assessment Scale (AIAS), a flexible framework for incorporating GenAI into educational assessments. The AIAS consists of five levels, ranging from 'No AI' to 'Full AI', enabling educators to design assessments that focus on areas requiring human input and critical thinking.   Following the implementation of the AIAS, the pilot study results indicate a significant reduction in academic misconduct cases related to GenAI, a 5.9% increase in student attainment across the 
    
[^81]: 大型语言模型和用户信任：以医疗为重点

    Large Language Models and User Trust: Focus on Healthcare

    [https://arxiv.org/abs/2403.14691](https://arxiv.org/abs/2403.14691)

    本文探讨了临床医生对LLMs的信任、数据来源从主要是人类生成到人工智能生成内容的转变，以及随之而来对LLMs精确性和临床医师能力的影响之间的不断发展的关系，强调了LLMs在医疗保健中的整合加深可能带来的挑战和风险。

    

    本文探讨了临床医生对LLMs的信任、数据来源从主要是人类生成到人工智能生成内容的转变，以及随之而来对LLMs精确性和临床医师能力的影响之间的不断发展的关系。其中一个主要问题是随着LLMs在学习过程中越来越依赖它们的输出，可能会导致输出质量下降，并导致临床医师技能减少，因为他们与基本诊断过程的参与减少。尽管目前还处于理论阶段，但这种反馈循环构成了一个重大挑战，因为LLMs在医疗保健中的整合加深，强调了需要积极对话和战略措施，以确保LLM技术的安全有效使用。此外，我们深入探讨了与LLMs自我参考学习循环以及医疗保健专业人员技能下降相关的潜在风险。

    arXiv:2403.14691v1 Announce Type: cross  Abstract: This paper explores the evolving relationship between clinician trust in LLMs, the transformation of data sources from predominantly human-generated to AI-generated content, and the subsequent impact on the precision of LLMs and clinician competence. One of the primary concerns identified is the potential feedback loop that arises as LLMs become more reliant on their outputs for learning, which may lead to a degradation in output quality and a reduction in clinician skills due to decreased engagement with fundamental diagnostic processes. While theoretical at this stage, this feedback loop poses a significant challenge as the integration of LLMs in healthcare deepens, emphasizing the need for proactive dialogue and strategic measures to ensure the safe and effective use of LLM technology. Moreover, we delve into the potential risks associated with LLMs' self-referential learning loops and the deskilling of healthcare professionals. The
    
[^82]: 将图注意机制融入基于深度强化学习的几何问题求解

    Incorporating Graph Attention Mechanism into Geometric Problem Solving Based on Deep Reinforcement Learning

    [https://arxiv.org/abs/2403.14690](https://arxiv.org/abs/2403.14690)

    提出了基于深度强化学习的图注意机制，用于自动且高效地添加几何问题中的辅助组件

    

    在在线教育背景下，设计一个自动求解几何问题的求解器被认为是迈向通用数学人工智能的关键一步，其依托自然语言理解和传统逻辑推理。在大多数情况下，问题的解决是通过添加辅助组件如线条或点来进行的。然而，由于在需要做出关键决策时选择合适的辅助组件的复杂性，自动添加辅助组件具有挑战性。目前的最新性能是通过从类别库中穷举所有可能的策略，以识别具有最大可能性的策略来实现的。然而，为了在效率方面做出妥协，必须采用广泛的策略搜索。为了自动且高效地添加辅助组件，我们提出了基于语言模型（如BERT）的深度强化学习框架。

    arXiv:2403.14690v1 Announce Type: cross  Abstract: In the context of online education, designing an automatic solver for geometric problems has been considered a crucial step towards general math Artificial Intelligence (AI), empowered by natural language understanding and traditional logical inference. In most instances, problems are addressed by adding auxiliary components such as lines or points. However, adding auxiliary components automatically is challenging due to the complexity in selecting suitable auxiliary components especially when pivotal decisions have to be made. The state-of-the-art performance has been achieved by exhausting all possible strategies from the category library to identify the one with the maximum likelihood. However, an extensive strategy search have to be applied to trade accuracy for ef-ficiency. To add auxiliary components automatically and efficiently, we present deep reinforcement learning framework based on the language model, such as BERT. We first
    
[^83]: 发展和部署教育领域人工智能产业标准：挑战、策略和未来方向

    Developing and Deploying Industry Standards for Artificial Intelligence in Education (AIED): Challenges, Strategies, and Future Directions

    [https://arxiv.org/abs/2403.14689](https://arxiv.org/abs/2403.14689)

    教育领域人工智能的发展需要制定和实施产业标准，以解决互操作性、可扩展性和道德治理等挑战。

    

    人工智能在教育领域的应用承诺通过提供个性化学习体验、自动化行政和教学任务以及降低内容创建成本来革新教育实践。然而，在开发和部署教育领域人工智能解决方案方面缺乏标准化实践导致生态系统分散，给互操作性、可扩展性和道德治理带来挑战。本文旨在解决在教育领域人工智能发展和实施产业标准的紧迫需求，提供对当前局势、挑战和克服这些障碍的策略方法的全面分析。我们开始通过研究AIED在不同教育环境中的各种应用，并确定缺乏标准化的关键领域，包括系统互操作性、本体映射、数据集成、评估和道德治理。

    arXiv:2403.14689v1 Announce Type: cross  Abstract: The adoption of Artificial Intelligence in Education (AIED) holds the promise of revolutionizing educational practices by offering personalized learning experiences, automating administrative and pedagogical tasks, and reducing the cost of content creation. However, the lack of standardized practices in the development and deployment of AIED solutions has led to fragmented ecosystems, which presents challenges in interoperability, scalability, and ethical governance. This article aims to address the critical need to develop and implement industry standards in AIED, offering a comprehensive analysis of the current landscape, challenges, and strategic approaches to overcome these obstacles. We begin by examining the various applications of AIED in various educational settings and identify key areas lacking in standardization, including system interoperability, ontology mapping, data integration, evaluation, and ethical governance. Then, 
    
[^84]: 在健康数据集上缺失值填补技术的性能

    On the Performance of Imputation Techniques for Missing Values on Healthcare Datasets

    [https://arxiv.org/abs/2403.14687](https://arxiv.org/abs/2403.14687)

    本研究比较了七种填补技术在健康数据集上的性能，结果显示...

    

    缺失值是真实世界数据集的一种常见特征，尤其是在健康数据中。本研究旨在比较七种填补技术（均值填补、中位数填补、最近观察值填补、K-最近邻填补、插值填补、Missforest填补和链式方程多重填补）在三个健康数据集上的性能。将数据集引入了不同百分比的缺失值（10\%、15\%、20\%和25\%），并使用填补技术对这些缺失值进行填补。通过均方根误差（RMSE）和平均绝对误差（MAE）对其性能进行评估。结果表明Mi

    arXiv:2403.14687v1 Announce Type: cross  Abstract: Missing values or data is one popular characteristic of real-world datasets, especially healthcare data. This could be frustrating when using machine learning algorithms on such datasets, simply because most machine learning models perform poorly in the presence of missing values. The aim of this study is to compare the performance of seven imputation techniques, namely Mean imputation, Median Imputation, Last Observation carried Forward (LOCF) imputation, K-Nearest Neighbor (KNN) imputation, Interpolation imputation, Missforest imputation, and Multiple imputation by Chained Equations (MICE), on three healthcare datasets. Some percentage of missing values - 10\%, 15\%, 20\% and 25\% - were introduced into the dataset, and the imputation techniques were employed to impute these missing values. The comparison of their performance was evaluated by using root mean squared error (RMSE) and mean absolute error (MAE). The results show that Mi
    
[^85]: 一项道义使命：对大型语言模型持续超对齐的需求

    A Moral Imperative: The Need for Continual Superalignment of Large Language Models

    [https://arxiv.org/abs/2403.14683](https://arxiv.org/abs/2403.14683)

    实现终身超对齐需要对当前大型语言模型架构进行重大变革，以解决其在理解和适应动态人类道德和不断发展的全球情景方面的局限性。

    

    这篇论文探讨了在人工智能系统中实现终身超对齐的挑战，尤其是在大型语言模型（LLMs）中。超对齐是一个理论框架，旨在确保超智能人工智能系统符合人类的价值观和目标。尽管其展望令人振奋，我们认为实现超对齐需要对当前LLM架构进行重大变革，因为它们在理解和适应人类道德的动态性和不断发展的全球情景方面固有的局限性。我们剖析了将不断变化的人类价值观谱系编码到LLMs中的挑战，突出了静态人工智能模型与人类社会动态性之间的差异。为了说明这些挑战，我们分析了两个不同的示例：一个展示了人类价值观的定性转变，另一个呈现了可量化的变化。通过这些示例，我们说明…

    arXiv:2403.14683v1 Announce Type: cross  Abstract: This paper examines the challenges associated with achieving life-long superalignment in AI systems, particularly large language models (LLMs). Superalignment is a theoretical framework that aspires to ensure that superintelligent AI systems act in accordance with human values and goals. Despite its promising vision, we argue that achieving superalignment requires substantial changes in the current LLM architectures due to their inherent limitations in comprehending and adapting to the dynamic nature of these human ethics and evolving global scenarios. We dissect the challenges of encoding an ever-changing spectrum of human values into LLMs, highlighting the discrepancies between static AI models and the dynamic nature of human societies. To illustrate these challenges, we analyze two distinct examples: one demonstrates a qualitative shift in human values, while the other presents a quantifiable change. Through these examples, we illus
    
[^86]: 使用时间关系知识的深度生成领域自适应方法用于跨用户活动识别

    Deep Generative Domain Adaptation with Temporal Relation Knowledge for Cross-User Activity Recognition

    [https://arxiv.org/abs/2403.14682](https://arxiv.org/abs/2403.14682)

    该研究引入了一种CVAE-USM方法，通过放松独立同分布假设和利用时间关系，有效地在不同用户之间对齐数据分布，从而改进活动识别。

    

    在人类活动识别（HAR）中，训练和测试数据是独立同分布（i.i.d.）的假设通常失败，特别是在跨用户场景中，其中数据分布存在显著差异。我们的研究引入了一个条件变分自动编码器与通用序列映射（CVAE-USM）方法，通过放松 i.i.d. 假设并利用时间关系来有效地对齐不同用户之间的数据分布，从而解决时间序列领域自适应在HAR中的独特挑战。

    arXiv:2403.14682v1 Announce Type: cross  Abstract: In human activity recognition (HAR), the assumption that training and testing data are independent and identically distributed (i.i.d.) often fails, particularly in cross-user scenarios where data distributions vary significantly. This discrepancy highlights the limitations of conventional domain adaptation methods in HAR, which typically overlook the inherent temporal relations in time-series data. To bridge this gap, our study introduces a Conditional Variational Autoencoder with Universal Sequence Mapping (CVAE-USM) approach, which addresses the unique challenges of time-series domain adaptation in HAR by relaxing the i.i.d. assumption and leveraging temporal relations to align data distributions effectively across different users. This method combines the strengths of Variational Autoencoder (VAE) and Universal Sequence Mapping (USM) to capture and utilize common temporal patterns between users for improved activity recognition. Ou
    
[^87]: AI伦理学：文献计量分析、关键问题和主要研究空白

    AI Ethics: A Bibliometric Analysis, Critical Issues, and Key Gaps

    [https://arxiv.org/abs/2403.14681](https://arxiv.org/abs/2403.14681)

    通过对过去二十年的AI伦理学文献进行全面计量分析，揭示了AI伦理研究的发展三部曲，并提出了七个关键AI伦理问题，同时指出了关于大伦理模型（LEM）和AI识别与扩展的两个研究空白。

    

    人工智能（AI）伦理学已经成为一个日益重要的学术研究领域。本研究对过去二十年的AI伦理学文献进行了全面的计量分析。分析揭示了一个明显的三部曲发展过程，包括一个孵化阶段，随后是专注于赋予AI人类属性的阶段，最终是强调发展以人为中心的AI系统的第三阶段。在此基础上，作者提出了七个关键AI伦理问题，涵盖了Collingridge困境、AI地位辩论、与AI透明度和可解释性相关的挑战、隐私保护复杂性、对公正和公平的考虑、对算法统治和人类衰弱的担忧，以及超智能的问题。最后，他们确定了AI伦理学中两个值得关注的研究空白，即大伦理模型（LEM）和AI识别与扩展。

    arXiv:2403.14681v1 Announce Type: cross  Abstract: Artificial intelligence (AI) ethics has emerged as a burgeoning yet pivotal area of scholarly research. This study conducts a comprehensive bibliometric analysis of the AI ethics literature over the past two decades. The analysis reveals a discernible tripartite progression, characterized by an incubation phase, followed by a subsequent phase focused on imbuing AI with human-like attributes, culminating in a third phase emphasizing the development of human-centric AI systems. After that, they present seven key AI ethics issues, encompassing the Collingridge dilemma, the AI status debate, challenges associated with AI transparency and explainability, privacy protection complications, considerations of justice and fairness, concerns about algocracy and human enfeeblement, and the issue of superintelligence. Finally, they identify two notable research gaps in AI ethics regarding the large ethics model (LEM) and AI identification and exten
    
[^88]: 人工智能中的信任: 进展、挑战和未来方向

    Trust in AI: Progress, Challenges, and Future Directions

    [https://arxiv.org/abs/2403.14680](https://arxiv.org/abs/2403.14680)

    人工智能中的信任是控制其传播程度的调节器，通过增加信任和减少不信任，可以显著影响人工智能的采用速度。

    

    人工智能系统在我们日常生活中的广泛应用通过各种应用、服务和产品，说明了来自用户角度对人工智能的信任/不信任的重要性。与其他技术相比，由人工智能驱动的系统不仅作为一些有益工具广泛渗透到我们的生活中，而且还会成为代表我们的替代性代理人，或者会影响人类思维、决策和行动的操纵性心智。近来，各种研究已经关注了人工智能中信任/不信任的不同维度及其相关考虑因素。在这篇系统性文献综述中，在对当前人工智能文献中对信任的概念化之后，我们将调查

    arXiv:2403.14680v1 Announce Type: cross  Abstract: The increasing use of artificial intelligence (AI) systems in our daily life through various applications, services, and products explains the significance of trust/distrust in AI from a user perspective. AI-driven systems (as opposed to other technologies) have ubiquitously diffused in our life not only as some beneficial tools to be used by human agents but also are going to be substitutive agents on our behalf, or manipulative minds that would influence human thought, decision, and agency. Trust/distrust in AI plays the role of a regulator and could significantly control the level of this diffusion, as trust can increase, and distrust may reduce the rate of adoption of AI. Recently, varieties of studies have paid attention to the variant dimension of trust/distrust in AI, and its relevant considerations. In this systematic literature review, after conceptualization of trust in the current AI literature review, we will investigate tr
    
[^89]: 统一不确定性评估方法用于认知诊断模型

    Unified Uncertainty Estimation for Cognitive Diagnosis Models

    [https://arxiv.org/abs/2403.14676](https://arxiv.org/abs/2403.14676)

    提出了一种统一不确定性评估方法，适用于各种认知诊断模型，填补了对于具有交互功能参数的复杂模型的学术空白

    

    认知诊断模型已被广泛应用于不同领域，尤其是智能教育，用于测量用户对知识概念的熟练程度，基于此，用户可以获得个性化的指导。然而，由于模型和数据的联系薄弱，测量并不总是可靠，测量的不确定性也为决策提供了重要信息。然而，对于认知诊断中不确定性评估的研究落后于对认知诊断高级模型结构的研究。现有方法效率有限，并为具有交互功能参数（如基于深度学习的模型）的精密模型留下了一个学术空白。为解决这些问题，我们提出了一种适用于广泛认知诊断模型的统一不确定性评估方法。具体而言，基于估计认知诊断模型参数的后验分布的思想，我们首先提供一种u

    arXiv:2403.14676v1 Announce Type: cross  Abstract: Cognitive diagnosis models have been widely used in different areas, especially intelligent education, to measure users' proficiency levels on knowledge concepts, based on which users can get personalized instructions. As the measurement is not always reliable due to the weak links of the models and data, the uncertainty of measurement also offers important information for decisions. However, the research on the uncertainty estimation lags behind that on advanced model structures for cognitive diagnosis. Existing approaches have limited efficiency and leave an academic blank for sophisticated models which have interaction function parameters (e.g., deep learning-based models). To address these problems, we propose a unified uncertainty estimation approach for a wide range of cognitive diagnosis models. Specifically, based on the idea of estimating the posterior distributions of cognitive diagnosis model parameters, we first provide a u
    
[^90]: 用大型语言模型预测学习表现：成人识字研究

    Predicting Learning Performance with Large Language Models: A Study in Adult Literacy

    [https://arxiv.org/abs/2403.14668](https://arxiv.org/abs/2403.14668)

    该研究使用大型语言模型GPT-4探讨了在ITS中预测成人识字计划学习表现的应用，并发现GPT-4在此方面具有竞争力的预测能力。

    

    arXiv:2403.14668v1 公告类别：跨领域 智能辅导系统（ITS）显著增强了成人识字培训，这是社会参与、就业机会和终身学习的关键因素。我们的研究探讨了高级AI模型（包括GPT-4等大型语言模型）在ITS中预测成人识字计划学习表现的应用。这项研究受到了LLMs基于其内在推理和计算能力预测学习表现的潜力的启发。通过使用ITS AutoTutor的阅读理解数据集，我们通过五折交叉验证技术评估了GPT-4与传统机器学习方法在预测学习表现方面的预测能力。我们的研究结果显示，GPT-4展现出与传统的机器学习方法（如贝叶斯知识跟踪、表现因素分析、稀疏因素分析）具有竞争力的预测能力。

    arXiv:2403.14668v1 Announce Type: cross  Abstract: Intelligent Tutoring Systems (ITSs) have significantly enhanced adult literacy training, a key factor for societal participation, employment opportunities, and lifelong learning. Our study investigates the application of advanced AI models, including Large Language Models (LLMs) like GPT-4, for predicting learning performance in adult literacy programs in ITSs. This research is motivated by the potential of LLMs to predict learning performance based on its inherent reasoning and computational capabilities. By using reading comprehension datasets from the ITS, AutoTutor, we evaluate the predictive capabilities of GPT-4 versus traditional machine learning methods in predicting learning performance through five-fold cross-validation techniques. Our findings show that the GPT-4 presents the competitive predictive abilities with traditional machine learning methods such as Bayesian Knowledge Tracing, Performance Factor Analysis, Sparse Fact
    
[^91]: 非洲人工智能政策发展案例研究

    Case Studies of AI Policy Development in Africa

    [https://arxiv.org/abs/2403.14662](https://arxiv.org/abs/2403.14662)

    非洲国家在AI准备方面取得的进展没有完全被全球准备情况评估捕捉到，通过对四个非洲国家进行案例研究，提出了如何改善国家的AI准备标准以及如何使社会能够获益于AI的高层政策考虑。

    

    arXiv:2403.14662v1 公告类型：跨领域 摘要：人工智能（AI）需要新的评估方式，以评估非洲国家在国家技术使用和战略方面的准备情况。我们对现有的针对一般数字采纳和特定AI政策的“准备情况”评估进行了调查。我们得出结论，现有的全球准备情况评估并没有完全捕捉到非洲国家在AI准备方面的进步，并为如何更好地利用这些评估来适应非洲背景奠定了基础。我们考虑这些指标在多大程度上与非洲背景相吻合，以及这些指标在捕捉非洲国家在实现AI能力方面的实际工作方面所遗漏的内容。通过对非洲四个地理和经济尺度不同的国家进行案例研究，我们提出了全球评估遗漏的细微差别，并提供了如何改善国家的AI准备标准以及如何使社会能够获益于AI的高层政策考虑。

    arXiv:2403.14662v1 Announce Type: cross  Abstract: Artificial Intelligence (AI) requires new ways of evaluating national technology use and strategy for African nations. We conduct a survey of existing 'readiness' assessments both for general digital adoption and for AI policy in particular. We conclude that existing global readiness assessments do not fully capture African states' progress in AI readiness and lay the groundwork for how assessments can be better used for the African context. We consider the extent to which these indicators map to the African context and what these indicators miss in capturing African states' on-the-ground work in meeting AI capability. Through case studies of four African nations of diverse geographic and economic dimensions, we identify nuances missed by global assessments and offer high-level policy considerations for how states can best improve their AI readiness standards and prepare their societies to capture the benefits of AI.
    
[^92]: 经济机器人：智能城市能源互联网中的新兴模式

    Machina Economicus: A New Paradigm for Prosumers in the Energy Internet of Smart Cities

    [https://arxiv.org/abs/2403.14660](https://arxiv.org/abs/2403.14660)

    在智能城市能源互联网中，提出了一个新的模式——经济机器人，旨在研究在资源优化、信息交换和交互协议等方面，实现prosumers的经济合理性，以建立一个高效、经济、社会最优的能源分享平台。

    

    能源互联网（EI）作为新兴的共享经济平台，利用物联网（IoT）和人工智能（AI）为智能城市提供灵活的本地能源供应。EI旨在解锁在智能城市中基于屋顶光伏板、车联网技术、分组能源管理等的本地能源市场中，prosumers之间的点对点能源交易和分享，prosumers可以在此之间灵活切换提供者和消费者的角色。然而，将prosumers整合到EI中将面临许多挑战，需要在资源优化、信息交换和交互协议等方面提供基于AI/IoT的高级解决方案，以建立一个高效、经济、社会最优的能源分享平台。

    arXiv:2403.14660v1 Announce Type: cross  Abstract: Energy Internet (EI) is emerging as new share economy platform for flexible local energy supplies in smart cities. Empowered by the Internet-of-Things (IoT) and Artificial Intelligence (AI), EI aims to unlock peer-to-peer energy trading and sharing among prosumers, who can adeptly switch roles between providers and consumers in localized energy markets with rooftop photovoltaic panels, vehicle-to-everything technologies, packetized energy management, etc. The integration of prosumers in EI, however, will encounter many challenges in modelling, analyzing, and designing an efficient, economic, and social-optimal platform for energy sharing, calling for advanced AI/IoT-based solutions to resource optimization, information exchange, and interaction protocols in the context of the share economy. In this study, we aim to introduce a recently emerged paradigm, Machina Economicus, to investigate the economic rationality in modelling, analysis,
    
[^93]: 社会智能数据基础设施：构建现状和引领未来

    Social Intelligence Data Infrastructure: Structuring the Present and Navigating the Future

    [https://arxiv.org/abs/2403.14659](https://arxiv.org/abs/2403.14659)

    本研究构建了一个名为Social AI Data Infrastructure的社会智能数据基础设施，包括一个全面的社交AI分类系统和一个480个NLP数据集的数据库，通过分析现有数据集工作以及评估语言模型在不同社会智能方面的表现，帮助研究者深入了解当前数据格局并提供未来数据集发展方向的整体观点。

    

    随着自然语言处理（NLP）系统越来越多地整合到人类社会生活中，这些技术将需要越来越多地依赖社会智能。尽管存在许多有价值的数据集可以衡量社会智能的孤立维度，但目前尚不存在任何作品来将这些线索结合在一起，形成一个研究者可以快速识别研究空白和未来方向的凝聚子领域。为实现这一目标，我们构建了一个社交AI数据基础设施，其中包括一个全面的社交AI分类系统和一个包含480个NLP数据集的数据库。我们的基础设施使我们能够分析现有的数据集工作，同时评估语言模型在不同社会智能方面的性能。我们的分析表明，它在帮助深入了解当前数据格局并提供对未来数据集发展潜在方向的整体观点方面具有实用性。

    arXiv:2403.14659v1 Announce Type: cross  Abstract: As Natural Language Processing (NLP) systems become increasingly integrated into human social life, these technologies will need to increasingly rely on social intelligence. Although there are many valuable datasets that benchmark isolated dimensions of social intelligence, there does not yet exist any body of work to join these threads into a cohesive subfield in which researchers can quickly identify research gaps and future directions. Towards this goal, we build a Social AI Data Infrastructure, which consists of a comprehensive social AI taxonomy and a data library of 480 NLP datasets. Our infrastructure allows us to analyze existing dataset efforts, and also evaluate language models' performance in different social intelligence aspects. Our analyses demonstrate its utility in enabling a thorough understanding of current data landscape and providing a holistic perspective on potential directions for future dataset development. We s
    
[^94]: 识别人工智能开发过程中的人的潜在进路

    Identifying Potential Inlets of Man in the Artificial Intelligence Development Process

    [https://arxiv.org/abs/2403.14658](https://arxiv.org/abs/2403.14658)

    该论文旨在探讨典型人工智能开发过程如何引导创造出种族化技术，以及如何更好理解这些技术如何强化对黑人的排斥。

    

    在本文中，我们希望确定典型或标准人工智能开发过程如何鼓励或促成种族化技术的创建。我们首先要理解西尔维娅·温特（Sylvia Wynter）对生物中心人类类型及其将黑人排除在人类之外的定义。然后我们概述了我们认为用于开发基于人工智能的技术的典型步骤，将其分为6个阶段：确定问题、开发过程和管理工具选择、数据集开发和数据处理、模型开发、部署和风险评估、以及集成和监控。本文的目标是更好地理解温特的生物中心人是如何被我们在人工智能生命周期中生产的技术及生命周期本身所代表和强化的；我们希望确定通过区分黑人与“理想”人类导致永续性的方式。

    arXiv:2403.14658v1 Announce Type: cross  Abstract: In this paper we hope to identify how the typical or standard artificial intelligence development process encourages or facilitates the creation of racialized technologies. We begin by understanding Sylvia Wynter's definition of the biocentric Man genre and its exclusion of Blackness from humanness. We follow this with outlining what we consider to be the typical steps for developing an AI-based technology, which we have broken down into 6 stages: identifying a problem, development process and management tool selection, dataset development and data processing, model development, deployment and risk assessment, and integration and monitoring. The goal of this paper is to better understand how Wynter's biocentric Man is being represented and reinforced by the technologies we are producing in the AI lifecycle and by the lifecycle itself; we hope to identify ways in which the distinction of Blackness from the "ideal" human leads to perpetu
    
[^95]: MemeCraft：情境和立场驱动的多模态模因生成

    MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation

    [https://arxiv.org/abs/2403.14652](https://arxiv.org/abs/2403.14652)

    MemeCraft是一款创新的模因生成器，利用大型语言模型和视觉语言模型生成支持特定社会运动的模因，提供端到端的流程，无需人工干预，带有内在安全机制。

    

    arXiv:2403.14652v1 公告类型：跨领域 摘要：在线模因在社交媒体时代作为强大的数字文化产物崭露头角，它们不仅提供了幽默，还为政治话语、社会批判和信息传播提供了平台。它们在塑造在线社区情绪方面的广泛影响力使其成为竞选和推动意识形态的宝贵工具。尽管已经开发了几种模因生成工具，但它们在系统性评估方面仍存在差距，以及在有效传达意识形态方面的能力有限。为解决这一问题，我们引入了MemeCraft，一款创新的模因生成器，利用大型语言模型（LLM）和视觉语言模型（VLM）生成支持特定社会运动的模因。MemeCraft提供了一个端到端的流程，将用户提示转化为引人入胜的多模态模因，无需人工干预。基于对创造有争议内容的潜在滥用的认识，具有内在安全机制

    arXiv:2403.14652v1 Announce Type: cross  Abstract: Online memes have emerged as powerful digital cultural artifacts in the age of social media, offering not only humor but also platforms for political discourse, social critique, and information dissemination. Their extensive reach and influence in shaping online communities' sentiments make them invaluable tools for campaigning and promoting ideologies. Despite the development of several meme-generation tools, there remains a gap in their systematic evaluation and their ability to effectively communicate ideologies. Addressing this, we introduce MemeCraft, an innovative meme generator that leverages large language models (LLMs) and visual language models (VLMs) to produce memes advocating specific social movements. MemeCraft presents an end-to-end pipeline, transforming user prompts into compelling multimodal memes without manual intervention. Conscious of the misuse potential in creating divisive content, an intrinsic safety mechanism
    
[^96]: 在个性化医疗、维护和检验以及农业4.0领域利用计算连续性

    Harnessing the Computing Continuum across Personalized Healthcare, Maintenance and Inspection, and Farming 4.0

    [https://arxiv.org/abs/2403.14650](https://arxiv.org/abs/2403.14650)

    AI-SPRINT项目专注于开发和实施横跨计算连续性的人工智能应用，在个性化医疗、维护和检验以及农业4.0领域取得了重大的科学进展。

    

    arXiv:2403.14650v1 公告类型: 跨领域 摘要: AI-SPRINT项目于2021年启动，由欧洲委员会资助，专注于开发和实施横跨计算连续性的人工智能应用。这种连续性确保了来自集中式数据中心到边缘设备的计算资源和服务的协同集成，促进了高效和适应性的计算和应用交付。AI-SPRINT取得了重大的科学进展，包括流程简化、效率提高以及在实时操作方面的能力，正如三个实际用例所证明的。本文深入探讨了个性化医疗、维护和检验以及农业4.0这些应用，突出它们的实际实施和通过集成AI-SPRINT技术实现的目标。我们分析了所提出的工具链如何有效地解决各种挑战并优化流程。

    arXiv:2403.14650v1 Announce Type: cross  Abstract: The AI-SPRINT project, launched in 2021 and funded by the European Commission, focuses on the development and implementation of AI applications across the computing continuum. This continuum ensures the coherent integration of computational resources and services from centralized data centers to edge devices, facilitating efficient and adaptive computation and application delivery. AI-SPRINT has achieved significant scientific advances, including streamlined processes, improved efficiency, and the ability to operate in real time, as evidenced by three practical use cases. This paper provides an in-depth examination of these applications -- Personalized Healthcare, Maintenance and Inspection, and Farming 4.0 -- highlighting their practical implementation and the objectives achieved with the integration of AI-SPRINT technologies. We analyze how the proposed toolchain effectively addresses a range of challenges and refines processes, disc
    
[^97]: 为企业AI采用设计多步骤行动模型

    Designing Multi-Step Action Models for Enterprise AI Adoption

    [https://arxiv.org/abs/2403.14645](https://arxiv.org/abs/2403.14645)

    本文介绍了Empsing设计的多步骤行动模型(MSAM)，旨在解决企业采用AI所面临的挑战，通过全面探讨设计原则、架构和未来发展，以及评估其性能并展望其潜在影响。

    

    本文介绍了Empsing设计的闭源AI模型Multi-Step Action Model (MSAM)，旨在解决阻碍企业采用AI的挑战。通过全面的研究，本文探讨了MSAM的基本原则、设计架构和未来发展轨迹。它通过严格的测试方法评估了MSAM的性能，并设想了其对推动组织内AI采用的潜在影响。

    arXiv:2403.14645v1 Announce Type: cross  Abstract: This paper introduces the Multi-Step Action Model (MSAM), a closed-source AI model designed by Empsing to address challenges hindering AI adoption in enterprises. Through a holistic examination, this paper explores MSAM's foundational principles, design architecture, and future trajectory. It evaluates MSAM's performance via rigorous testing methodologies and envisions its potential impact on advancing AI adoption within organizations.
    
[^98]: 探究ChatGPT及其对社会的影响

    Exploring ChatGPT and its Impact on Society

    [https://arxiv.org/abs/2403.14643](https://arxiv.org/abs/2403.14643)

    ChatGPT是一种基于Transformer架构的大型语言模型，能够生成人类化的对话回复，可革新各行业并改变技术互动方式。

    

    人工智能已经存在一段时间了，但突然间比以往任何时候都受到了更多的关注。感谢谷歌、微软、元宇宙等科技界主要品牌的创新。然而，OpenAI通过其开创性发明ChatGPT触发了按钮。ChatGPT是一种基于Transformer架构的大型语言模型（LLM），能够在对话背景中生成类似人类的回复。它使用深度学习算法来生成对输入文本的自然语言回复。其庞大的参数数量、上下文生成和面向开放域的训练使其成为一种多功能且有效的工具，可应用于从聊天机器人到客户服务再到语言翻译等广泛领域。它具有彻底改变各行业并转变我们与技术互动方式的潜力。然而，使用ChatGPT也引发了一些担忧，包括道德方面的。

    arXiv:2403.14643v1 Announce Type: cross  Abstract: Artificial intelligence has been around for a while, but suddenly it has received more attention than ever before. Thanks to innovations from companies like Google, Microsoft, Meta, and other major brands in technology. OpenAI, though, has triggered the button with its ground-breaking invention ChatGPT. ChatGPT is a Large Language Model (LLM) based on Transformer architecture that has the ability to generate human-like responses in a conversational context. It uses deep learning algorithms to generate natural language responses to input text. Its large number of parameters, contextual generation, and open-domain training make it a versatile and effective tool for a wide range of applications, from chatbots to customer service to language translation. It has the potential to revolutionize various industries and transform the way we interact with technology. However, the use of ChatGPT has also raised several concerns, including ethical,
    
[^99]: 革新远程学习：基于人工智能辅导的学习进展的比较研究

    Revolutionising Distance Learning: A Comparative Study of Learning Progress with AI-Driven Tutoring

    [https://arxiv.org/abs/2403.14642](https://arxiv.org/abs/2403.14642)

    本研究首次证明生成式AI能显著提高大学生的学习速度，使用AI助手Syntea在远程学习学生中平均减少了27%的学习时间，表明生成式AI可以通过个性化显著改进和加快学习。

    

    Generative AI预计将对教育产生巨大积极影响; 但是，目前尚未在大学层面展示这种潜力。在这项研究中，我们首次提出证据表明，生成式AI能够显著提高大学生的学习速度。我们测试了是否使用AI动力学辅导助手Syntea影响了IU国际应用科学大学40多个课程中数百名远程学习学生的学习速度。我们的分析表明，使用Syntea在Syntea发布后第三个月显著减少了他们的学习时间--平均约减少了27\%。总的来说，该效应的幅度和方法的可扩展性证明了生成式AI作为显著改进和加快学习的关键杠杆。

    arXiv:2403.14642v1 Announce Type: cross  Abstract: Generative AI is expected to have a vast, positive impact on education; however, at present, this potential has not yet been demonstrated at scale at university level. In this study, we present first evidence that generative AI can increase the speed of learning substantially in university students. We tested whether using the AI-powered teaching assistant Syntea affected the speed of learning of hundreds of distance learning students across more than 40 courses at the IU International University of Applied Sciences. Our analysis suggests that using Syntea reduced their study time substantially--by about 27\% on average--in the third month after the release of Syntea. Taken together, the magnitude of the effect and the scalability of the approach implicate generative AI as a key lever to significantly improve and accelerate learning by personalisation.
    
[^100]: 测试自动驾驶车辆和人工智能：从网络安全、透明度、稳健性和公平性的视角探讨挑战与前景

    Testing autonomous vehicles and AI: perspectives and challenges from cybersecurity, transparency, robustness and fairness

    [https://arxiv.org/abs/2403.14641](https://arxiv.org/abs/2403.14641)

    探讨了将人工智能整合到自动驾驶车辆中所涉及的挑战，重点关注了网络安全审计、决策过程的可解释性以及评估预测系统的稳健性和道德行为等方面的重要性

    

    本研究探讨了将人工智能（AI）整合到自动驾驶车辆（AVs）中所涉及的复杂性，研究了AI组件引入的挑战以及对测试程序的影响，着重关注可信赖AI的一些基本要求。讨论的主题包括AI在AVs的各个操作层中的作用、欧盟AI法案对AVs的影响，以及对高级驾驶辅助系统（ADAS）和自动驾驶系统（ADS）的新测试方法的需求。研究还就网络安全审计的重要性、AI决策过程中的可解释性需求以及评估AVs中预测系统稳健性和道德行为的协议提供了详细分析。该论文指出了一些重要挑战，并提出了AI在AV技术研究和开发中未来方向的建议，强调了跨学科专业知识的需求。

    arXiv:2403.14641v1 Announce Type: cross  Abstract: This study explores the complexities of integrating Artificial Intelligence (AI) into Autonomous Vehicles (AVs), examining the challenges introduced by AI components and the impact on testing procedures, focusing on some of the essential requirements for trustworthy AI. Topics addressed include the role of AI at various operational layers of AVs, the implications of the EU's AI Act on AVs, and the need for new testing methodologies for Advanced Driver Assistance Systems (ADAS) and Automated Driving Systems (ADS). The study also provides a detailed analysis on the importance of cybersecurity audits, the need for explainability in AI decision-making processes and protocols for assessing the robustness and ethical behaviour of predictive systems in AVs. The paper identifies significant challenges and suggests future directions for research and development of AI in AV technology, highlighting the need for multidisciplinary expertise.
    
[^101]: 使用Transformer神经网络来定义智慧城市

    On Defining Smart Cities using Transformer Neural Networks

    [https://arxiv.org/abs/2403.14639](https://arxiv.org/abs/2403.14639)

    使用Transformer神经网络和语义文本分析，本论文尝试创建一个新的智慧城市定义“妥协”版本，并提出语义相似度度量作为评估技术。

    

    全球各地的城市正在迅速采用智能技术，改变城市生活。尽管存在这一趋势，但有关“智慧城市”的普遍接受的定义仍然难以界定。本文旨在创建一个应与先前参与定义这一概念的大多数专家 resonating 的新“妥协”定义，并旨在验证现有定义之一。我们从行业、学术界和各种相关组织中审查了60个智慧城市的定义，采用基于Transformer架构的生成AI和语义文本分析以达成这一妥协。我们提出了一种语义相似度度量作为评估技术，通常可用于比较不同智慧城市定义，评估其独特性或相似性。我们的方法利用生成AI来分析各种现有定义。

    arXiv:2403.14639v1 Announce Type: cross  Abstract: Cities worldwide are rapidly adopting smart technologies, transforming urban life. Despite this trend, a universally accepted definition of 'smart city' remains elusive. Past efforts to define it have not yielded a consensus, as evidenced by the numerous definitions in use. In this paper, we endeavored to create a new 'compromise' definition that should resonate with most experts previously involved in defining this concept and aimed to validate one of the existing definitions. We reviewed 60 definitions of smart cities from industry, academia, and various relevant organizations, employing transformer architecture-based generative AI and semantic text analysis to reach this compromise. We proposed a semantic similarity measure as an evaluation technique, which could generally be used to compare different smart city definitions, assessing their uniqueness or resemblance. Our methodology employed generative AI to analyze various existing
    
[^102]: 实践中的AI公平性

    AI Fairness in Practice

    [https://arxiv.org/abs/2403.14636](https://arxiv.org/abs/2403.14636)

    探讨了如何通过基于上下文和以社会为中心的方法来理解AI公平性，从而帮助项目团队更好地识别、减轻和管理不公平偏见和歧视可能在整个AI项目工作流程中出现的多种方式。

    

    达成对于AI公平性普遍接受的定义长期以来一直是人工智能伦理和治理中的一个核心挑战。在社会中，关于公平概念意味着什么以及如何最好实践的观点各不相同。本文探讨了如何通过探索基于上下文和以社会为中心的方法来理解AI公平性，从而帮助项目团队更好地识别、减轻和管理不公平偏见和歧视可能在整个AI项目工作流程中出现的多种方式。

    arXiv:2403.14636v1 Announce Type: cross  Abstract: Reaching consensus on a commonly accepted definition of AI Fairness has long been a central challenge in AI ethics and governance. There is a broad spectrum of views across society on what the concept of fairness means and how it should best be put to practice. In this workbook, we tackle this challenge by exploring how a context-based and society-centred approach to understanding AI Fairness can help project teams better identify, mitigate, and manage the many ways that unfair bias and discrimination can crop up across the AI project workflow.   We begin by exploring how, despite the plurality of understandings about the meaning of fairness, priorities of equality and non-discrimination have come to constitute the broadly accepted core of its application as a practical principle. We focus on how these priorities manifest in the form of equal protection from direct and indirect discrimination and from discriminatory harassment. These e
    
[^103]: AI可持续性在实践中的应用 第一部分：可持续AI项目的基础

    AI Sustainability in Practice Part One: Foundations for Sustainable AI Projects

    [https://arxiv.org/abs/2403.14635](https://arxiv.org/abs/2403.14635)

    该论文介绍了AI可持续性概念及工具，旨在指导AI项目团队评估社会影响和伦理可容忍性，提出了促进利益相关者参与的流程。

    

    可持续的AI项目持续关注设计、开发和部署AI技术可能对个人和社会产生的转变效应以及短期、中期和长期影响。着重于AI可持续性的项目确保价值导向、协作和预见性反思引导评估潜在社会和伦理影响，并引导负责任的创新实践。该工作手册是成对提供的第一部分，提供了将AI可持续性付诸实践所需的概念和工具。它介绍了SUM价值，帮助AI项目团队评估其项目的潜在社会影响和伦理可容忍性。然后介绍了利益相关者参与流程(SEP)，提供工具促进利益相关者的比例参与和意见，强调平等和有意义的参与和定位。

    arXiv:2403.14635v1 Announce Type: cross  Abstract: Sustainable AI projects are continuously responsive to the transformative effects as well as short-, medium-, and long-term impacts on individuals and society that the design, development, and deployment of AI technologies may have. Projects, which centre AI Sustainability, ensure that values-led, collaborative, and anticipatory reflection both guides the assessment of potential social and ethical impacts and steers responsible innovation practices.   This workbook is the first part of a pair that provides the concepts and tools needed to put AI Sustainability into practice. It introduces the SUM Values, which help AI project teams to assess the potential societal impacts and ethical permissibility of their projects. It then presents a Stakeholder Engagement Process (SEP), which provides tools to facilitate proportionate engagement of and input from stakeholders with an emphasis on equitable and meaningful participation and positionali
    
[^104]: 出身富贵？探讨大型语言模型中的社会经济偏见

    Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models

    [https://arxiv.org/abs/2403.14633](https://arxiv.org/abs/2403.14633)

    本文调查了大型语言模型中是否存在社会经济偏见，引入了一个新的数据集SilverSpoon，并评估了这种偏见的程度以及随着模型大小的变化。

    

    社会经济偏见在社会中加剧了不公平现象，根据个人经济和社会背景影响获取机会和资源的机会。这一普遍问题持续地延续了系统性的不平等，阻碍了作为一个社会追求包容性进步。在本文中，我们调查了大型语言模型中是否存在社会经济偏见。为此，我们引入了一个新的数据集（SilverSpoon），包含3000个样本，展示了牵涉到弱势群体由于他们的处境而实施道德模糊行为的假设情景，并问这种行为是否在道德上成立。此外，这个数据集具有双重标记方案，并由属于社会经济两端的人进行了注释。使用SilverSpoon，我们评估了大型语言模型中表现出的社会经济偏见程度以及该程度如何随模型大小变化。

    arXiv:2403.14633v1 Announce Type: cross  Abstract: Socioeconomic bias in society exacerbates disparities, influencing access to opportunities and resources based on individuals' economic and social backgrounds. This pervasive issue perpetuates systemic inequalities, hindering the pursuit of inclusive progress as a society. In this paper, we investigate the presence of socioeconomic bias, if any, in large language models. To this end, we introduce a novel dataset (SilverSpoon), consisting of 3000 samples that illustrate hypothetical scenarios that involve underprivileged people performing ethically ambiguous actions due to their circumstances, and ask whether the action is ethically justified. Further, this dataset has a dual-labeling scheme and has been annotated by people belonging to both ends of the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of socioeconomic bias expressed in large language models and the variation of this degree as a function of model size. W
    
[^105]: Videoshop：具有噪声外推扩散反演的本地化语义视频编辑

    Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion

    [https://arxiv.org/abs/2403.14617](https://arxiv.org/abs/2403.14617)

    Videoshop是一个无需训练的视频编辑算法，通过图像为基础的方法实现了本地化语义编辑，从而允许用户对视频进行精细控制，取得了更高质量的编辑效果。

    

    我们介绍了Videoshop，这是一个无需训练的用于本地化语义编辑的视频编辑算法。Videoshop允许用户使用任何编辑软件，包括Photoshop和生成填充，修改第一帧；它会自动将这些更改传播到其余帧，保持语义、空间和时间上的一致运动。与现有方法只能通过不精确的文本指令进行编辑不同，Videoshop允许用户添加或删除对象，语义上更改对象，将素材照片插入视频等，并对位置和外观进行细粒度控制。我们通过对潜在值进行噪声外推反演的图像为基础的视频编辑来实现这一目标，从中我们生成根据编辑图像调整的视频。Videoshop在2个编辑基准测试中使用10个评估指标对6个基线取得了更高质量的编辑效果。

    arXiv:2403.14617v1 Announce Type: cross  Abstract: We introduce Videoshop, a training-free video editing algorithm for localized semantic edits. Videoshop allows users to use any editing software, including Photoshop and generative inpainting, to modify the first frame; it automatically propagates those changes, with semantic, spatial, and temporally consistent motion, to the remaining frames. Unlike existing methods that enable edits only through imprecise textual instructions, Videoshop allows users to add or remove objects, semantically change objects, insert stock photos into videos, etc. with fine-grained control over locations and appearance. We achieve this through image-based video editing by inverting latents with noise extrapolation, from which we generate videos conditioned on the edited image. Videoshop produces higher quality edits against 6 baselines on 2 editing benchmarks using 10 evaluation metrics.
    
[^106]: AnyV2V：一种适用于任何视频到视频编辑任务的即插即用框架

    AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks

    [https://arxiv.org/abs/2403.14468](https://arxiv.org/abs/2403.14468)

    AnyV2V是一种适用于任何视频到视频编辑任务的即插即用框架，通过两个主要步骤简化视频编辑，支持广泛的视频编辑任务，并能够处理传统和新颖的编辑需求。

    

    arXiv:2403.14468v1 公告类型: 跨越 摘要: 视频到视频编辑涉及编辑源视频以及额外的控制（例如文本提示、主题或风格），以生成与源视频和提供的控制相匹配的新视频。传统方法受限于特定的编辑类型，限制了它们满足广泛用户需求的能力。在本文中，我们介绍了AnyV2V，这是一种新颖的免训练框架，旨在将视频编辑简化为两个主要步骤：（1）利用现成的图像编辑模型（例如InstructPix2Pix、InstantID等）修改第一帧，（2）利用现有的图像到视频生成模型（例如I2VGen-XL）进行DDIM逆转和特征注入。在第一阶段，AnyV2V可以插入任何现有的图像编辑工具，以支持广泛的视频编辑任务。除了传统的基于提示的编辑方法，AnyV2V还可以支持新颖的视频编辑任务，包括参考

    arXiv:2403.14468v1 Announce Type: cross  Abstract: Video-to-video editing involves editing a source video along with additional control (such as text prompts, subjects, or styles) to generate a new video that aligns with the source video and the provided control. Traditional methods have been constrained to certain editing types, limiting their ability to meet the wide range of user demands. In this paper, we introduce AnyV2V, a novel training-free framework designed to simplify video editing into two primary steps: (1) employing an off-the-shelf image editing model (e.g. InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion and feature injection. In the first stage, AnyV2V can plug in any existing image editing tools to support an extensive array of video editing tasks. Beyond the traditional prompt-based editing methods, AnyV2V also can support novel video editing tasks, including refe
    
[^107]: 分析医学图像的扩散分割

    Analysing Diffusion Segmentation for Medical Images

    [https://arxiv.org/abs/2403.14440](https://arxiv.org/abs/2403.14440)

    本研究批判性地分析和讨论了医学图像的扩散分割与扩散图像生成之间的差异，强调了针对扩散分割的架构改进带来的益处。

    

    随着其能够提供概率建模和生成多样化输出的能力，去噪扩散概率模型变得越来越受欢迎。这种多功能性启发了它们被用于图像分割，模型的多次预测可以产生分割结果，不仅质量高，而且能够捕捉模型本质上的不确定性。本文提出了用于改进扩散分割性能的强大架构。然而，对扩散分割和图像生成之间的差异缺乏分析和讨论，并且缺乏对这些架构提供的改进在分割领域与在特定于扩散分割的益处之间进行区分的深入评估。

    arXiv:2403.14440v1 Announce Type: cross  Abstract: Denoising Diffusion Probabilistic models have become increasingly popular due to their ability to offer probabilistic modeling and generate diverse outputs. This versatility inspired their adaptation for image segmentation, where multiple predictions of the model can produce segmentation results that not only achieve high quality but also capture the uncertainty inherent in the model. Here, powerful architectures were proposed for improving diffusion segmentation performance. However, there is a notable lack of analysis and discussions on the differences between diffusion segmentation and image generation, and thorough evaluations are missing that distinguish the improvements these architectures provide for segmentation in general from their benefit for diffusion segmentation specifically. In this work, we critically analyse and discuss how diffusion segmentation for medical images differs from diffusion image generation, with a partic
    
[^108]: 这并非一个数据问题：算法与权力在加拿大公立高等教育中的应用

    "This is not a data problem": Algorithms and Power in Public Higher Education in Canada

    [https://arxiv.org/abs/2403.13969](https://arxiv.org/abs/2403.13969)

    研究揭示了公立高等教育中算法决策的影响，包括学生监视增加、不平等加剧和教师-学生关系的自动化。

    

    算法决策在公立高等教育中日益被采用。高等教育机构的数据驱动实践扩展与新公共管理方法在新自由主义政府的推动下同步进行。本研究对加拿大安大略省一个公立学院数据和算法的深度民族志案例进行了定性分析。我们确定了学院使用的数据、算法和结果。我们评估了学院的流程和关系如何支持这些结果，以及不同利益相关者对学院数据驱动系统的看法。此外，我们发现日益依赖算法决策导致学生监视增加，现有不平等加剧，并导致教师-学生关系的自动化。最后，我们确定了由算法决策延续的增加制度权力的循环。

    arXiv:2403.13969v1 Announce Type: cross  Abstract: Algorithmic decision-making is increasingly being adopted across public higher education. The expansion of data-driven practices by post-secondary institutions has occurred in parallel with the adoption of New Public Management approaches by neoliberal administrations. In this study, we conduct a qualitative analysis of an in-depth ethnographic case study of data and algorithms in use at a public college in Ontario, Canada. We identify the data, algorithms, and outcomes in use at the college. We assess how the college's processes and relationships support those outcomes and the different stakeholders' perceptions of the college's data-driven systems. In addition, we find that the growing reliance on algorithmic decisions leads to increased student surveillance, exacerbation of existing inequities, and the automation of the faculty-student relationship. Finally, we identify a cycle of increased institutional power perpetuated by algorit
    
[^109]: 准确预测智能系统的安全关键稀有事件概率

    Accurately Predicting Probabilities of Safety-Critical Rare Events for Intelligent Systems

    [https://arxiv.org/abs/2403.13869](https://arxiv.org/abs/2403.13869)

    该研究致力于发展一个在评估安全关键自主安全性的重要性方面，在精度和召回率方面都表现出色的关键性预测模型。

    

    智能系统越来越成为我们日常生活中的重要组成部分，然而罕见的安全关键事件对它们的实际部署构成了重大潜在威胁。应对这一挑战的关键在于准确预测在给定时间步长内从当前状态发生安全关键事件的概率，一个我们定义为“重要性”的指标。预测重要性的复杂性源自于极端数据不平衡，这是由高维变量中与罕见事件相关联引起的一个挑战，我们称之为罕见性诅咒。现有方法往往要么过于保守，要么容易忽视安全关键事件，因此很难同时实现高精度和召回率，这严重限制了它们的适用性。本研究旨在开发一个重要性预测模型，在评估安全关键自主安全性的重要性方面，在精度和召回率方面都表现出色。

    arXiv:2403.13869v1 Announce Type: cross  Abstract: Intelligent systems are increasingly integral to our daily lives, yet rare safety-critical events present significant latent threats to their practical deployment. Addressing this challenge hinges on accurately predicting the probability of safety-critical events occurring within a given time step from the current state, a metric we define as 'criticality'. The complexity of predicting criticality arises from the extreme data imbalance caused by rare events in high dimensional variables associated with the rare events, a challenge we refer to as the curse of rarity. Existing methods tend to be either overly conservative or prone to overlooking safety-critical events, thus struggling to achieve both high precision and recall rates, which severely limits their applicability. This study endeavors to develop a criticality prediction model that excels in both precision and recall rates for evaluating the criticality of safety-critical auton
    
[^110]: 模型开放框架: 促进人工智能中的可重现性、透明度和可用性的完整性和开放性

    The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI

    [https://arxiv.org/abs/2403.13784](https://arxiv.org/abs/2403.13784)

    提出了模型开放框架（MOF），它是一个排名分类系统，根据完整性和开放性评估机器学习模型，旨在促进完整性、开放性以及遵循开放科学原则，可以帮助准确识别模型的透明性和可重现性。

    

    生成式人工智能（GAI）提供了前所未有的可能性，但其商业化引发了关于透明度、可重现性、偏见和安全性的担忧。许多"开源"的GAI模型缺乏完整理解和再现所必需的组件，一些采用限制性许可证，这种行为被称为"开源洗白"。我们提出了模型开放框架（MOF），这是一个根据完整性和开放性对机器学习模型进行排名分类的系统，遵循开放科学、开源、开放数据和开放获取的原则。MOF要求模型开发生命周期的特定组件被包含并根据适当的开放许可证发布。该框架旨在防止宣称自己是开放的模型被误解，指导研究人员和开发者以宽松的许可证发布所有模型组件，并帮助公司、学术界和爱好者识别可以安全采用的模型。

    arXiv:2403.13784v1 Announce Type: new  Abstract: Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety. Many "open-source" GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as "openwashing." We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses. This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adop
    
[^111]: S2DM：面向视频生成的扇形扩散模型

    S2DM: Sector-Shaped Diffusion Models for Video Generation

    [https://arxiv.org/abs/2403.13408](https://arxiv.org/abs/2403.13408)

    S2DM提出了一种新颖的Sector-Shaped Diffusion Model，能够生成具有一致语义和随机特征的一组相关数据，同时在时间特征上变化，在视频生成任务上表现优异。

    

    扩散模型在图像生成方面取得了巨大成功。然而，当将这一思想应用于视频生成时，我们面临着保持视频帧一致性和连续性的重大挑战。这主要是由于缺乏一个有效的框架来将视频帧与期望的时间特征对齐，同时保持一致的语义和随机特征所致。在本工作中，我们提出了一种新颖的Sector-Shaped Diffusion Model（S2DM），其扇形扩散区域由一组以相同噪声点为起点的射线状反向扩散过程形成。S2DM能够生成一组在语义和随机特征上共享相同特征的内在相关数据，同时在适当的引导条件下在时间特征上变化。我们将S2DM应用于视频生成任务，并探讨了光流作为时间条件的使用。我们的实验结果表明，S2DM优于许多已存在的...

    arXiv:2403.13408v2 Announce Type: replace-cross  Abstract: Diffusion models have achieved great success in image generation. However, when leveraging this idea for video generation, we face significant challenges in maintaining the consistency and continuity across video frames. This is mainly caused by the lack of an effective framework to align frames of videos with desired temporal features while preserving consistent semantic and stochastic features. In this work, we propose a novel Sector-Shaped Diffusion Model (S2DM) whose sector-shaped diffusion region is formed by a set of ray-shaped reverse diffusion processes starting at the same noise point. S2DM can generate a group of intrinsically related data sharing the same semantic and stochastic features while varying on temporal features with appropriate guided conditions. We apply S2DM to video generation tasks, and explore the use of optical flow as temporal conditions. Our experimental results show that S2DM outperforms many exis
    
[^112]: 利用大型语言模型和真实机器人账户激励社交媒体平台上的新闻消费

    Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts

    [https://arxiv.org/abs/2403.13362](https://arxiv.org/abs/2403.13362)

    通过创建使用 GPT-2 的机器人账户，在社交媒体平台上回复用户的推文，鼓励用户接触和关注验证的、意识形态平衡的新闻，以增加用户接触这些新闻并提高参与度。

    

    极化、信任下降以及对民主规范支持动摇是美国民主面临的紧迫威胁。接触验证和优质新闻可能降低个人对这些威胁的易感性，并使公民更具抗击错误信息、民粹主义和极端党派言论的能力。该项目探讨了如何在一个生态有效的环境中增强用户接触和参与验证的、意识形态平衡的新闻。我们依赖于对 28,457 个 Twitter 用户进行的大规模为期两周的田野实验（从 2023 年 1 月 19 日到 2 月 3 日）。我们创建了 28 个利用 GPT-2 的机器人，在用户发表有关体育、娱乐或生活方式的推文时回复一个内容相关的回复，其中包含两个硬代码元素：一个指向优质新闻机构相关主题部分的 URL 和鼓励关注其 Twitter 账户。为进一步测试机器人对性别的差异影响，被试用户被随机分配以接受...

    arXiv:2403.13362v1 Announce Type: cross  Abstract: Polarization, declining trust, and wavering support for democratic norms are pressing threats to U.S. democracy. Exposure to verified and quality news may lower individual susceptibility to these threats and make citizens more resilient to misinformation, populism, and hyperpartisan rhetoric. This project examines how to enhance users' exposure to and engagement with verified and ideologically balanced news in an ecologically valid setting. We rely on a large-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on 28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users tweeting about sports, entertainment, or lifestyle with a contextual reply containing two hardcoded elements: a URL to the topic-relevant section of quality news organization and an encouragement to follow its Twitter account. To further test differential effects by gender of the bots, treated users were randomly assigned to receive re
    
[^113]: 一种用于纵向多模态多视图缺失预测的统一模型

    A Unified Model for Longitudinal Multi-Modal Multi-View Prediction with Missingness

    [https://arxiv.org/abs/2403.12211](https://arxiv.org/abs/2403.12211)

    该研究提出了一种用于处理纵向多模态多视图数据缺失的统一模型，能够利用所有可用数据进行预测。

    

    医疗记录通常由不同的模态组成，如图片、文本和表格信息。整合所有模态可以提供患者状况的全面视图，而纵向分析能更好地理解疾病进展。本文介绍了一种用于处理纵向多模态多视图（MMMV）数据缺失的统一模型。我们的方法可以使用任意数量的时间点作为输入，旨在利用所有可用数据，无论其是否完整。我们在骨关节炎倡议（OAI）的膝关节骨关节炎数据集上进行了对疼痛和Kellgren-Lawrence分级（KLG）在未来时间点的预测的广泛实验。

    arXiv:2403.12211v1 Announce Type: cross  Abstract: Medical records often consist of different modalities, such as images, text, and tabular information. Integrating all modalities offers a holistic view of a patient's condition, while analyzing them longitudinally provides a better understanding of disease progression. However, real-world longitudinal medical records present challenges: 1) patients may lack some or all of the data for a specific timepoint, and 2) certain modalities or views might be absent for all patients during a particular period. In this work, we introduce a unified model for longitudinal multi-modal multi-view (MMMV) prediction with missingness. Our method allows as many timepoints as desired for input, and aims to leverage all available data, regardless of their availability. We conduct extensive experiments on the knee osteoarthritis dataset from the Osteoarthritis Initiative (OAI) for pain and Kellgren-Lawrence grade (KLG) prediction at a future timepoint. We d
    
[^114]: Tur[k]ingBench：用于网络代理的挑战基准测试

    Tur[k]ingBench: A Challenge Benchmark for Web Agents

    [https://arxiv.org/abs/2403.11905](https://arxiv.org/abs/2403.11905)

    Tur[k]ingBench是一个挑战性的网络代理基准测试，用于评估最先进的多模态模型在处理包含文本指示和多模态上下文的复杂任务时的泛化能力。

    

    最近的聊天机器人展示了在原始文本形式下理解和交流的令人印象深刻的能力。然而，世界上不仅仅是原始文本。例如，人们在网页上花费大量时间，在这些网页上，文本与其他形式交织在一起，并以各种复杂互动的形式完成任务。最先进的多模型是否能够推广到这种复杂的领域呢？为了回答这个问题，我们介绍了TurkingBench，一个由包含多模态背景的文本说明制定的任务基准。与现有的使用人工合成的网页的工作不同，这里我们使用最初设计用于各种注释目的的自然HTML页面。每个任务的HTML说明也被实例化为各种值（从众包任务获得）以形成任务的新实例。这个基准包含32.2K个实例。

    arXiv:2403.11905v1 Announce Type: new  Abstract: Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains?   To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instanc
    
[^115]: 单模态多任务融合用于情感模仿预测

    Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton

    [https://arxiv.org/abs/2403.11879](https://arxiv.org/abs/2403.11879)

    通过融合技术整合全局背景信息和采用LSTM架构进行时间分析，我们的方法在情感模仿强度预测任务上取得了显着的改进。

    

    在这项研究中，我们提出了一种方法，用于在第六届户外情感行为分析研讨会和竞赛中进行情感模仿强度（EMI）估计任务。我们的方法利用了Wav2Vec 2.0框架，在一个全面的播客数据集上进行了预训练，以提取涵盖语言和语外元素的广泛音频特征。我们通过一种融合技术增强了特征表示，该技术将个体特征与全局均值向量相结合，引入全局背景信息到我们的分析中。此外，我们从Wav2Vec 2.0模型中引入了一个预训练的valence-arousal-dominance（VAD）模块。我们的融合采用了一种长短期记忆（LSTM）架构，用于对音频数据进行高效的时间分析。仅利用所提供的音频数据，我们的方法在已建立的基准线上表现出显著的改进。

    arXiv:2403.11879v1 Announce Type: cross  Abstract: In this study, we propose a methodology for the Emotional Mimicry Intensity (EMI) Estimation task within the context of the 6th Workshop and Competition on Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0 framework, pre-trained on a comprehensive podcast dataset, to extract a broad range of audio features encompassing both linguistic and paralinguistic elements. We enhance feature representation through a fusion technique that integrates individual features with a global mean vector, introducing global contextual insights into our analysis. Additionally, we incorporate a pre-trained valence- arousal-dominance (VAD) module from the Wav2Vec 2.0 model. Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient temporal analysis of audio data. Utilizing only the provided audio data, our approach demonstrates significant improvements over the established baseline.
    
[^116]: CPA-Enhancer：链式思维驱动自适应增强器用于未知退化下的目标检测

    CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations

    [https://arxiv.org/abs/2403.11220](https://arxiv.org/abs/2403.11220)

    提出了一种用于未知退化下目标检测的链式思维驱动自适应增强器CPA-Enhancer，并将其集成到通用检测器中，有效提升受损图像的检测性能

    

    目前，已经广泛研究了在已知单一退化情况下的目标检测方法。然而，现有方法需要先验知识来确定退化类型，并为每种类型训练一个单独的模型，从而限制了它们在不可预测环境中的实际应用。为了解决这一挑战，我们提出了一种链式思维（CoT）驱动的自适应增强器CPA-Enhancer，用于未知退化情况下的目标检测。具体而言，CPA-Enhancer在CoT提示的逐步指导下逐步调整其增强策略，这些提示编码了与退化相关的信息。据我们所知，这是首个利用CoT提示进行目标检测任务的工作。总的来说，CPA-Enhancer是一个即插即用的增强模型，可以集成到任何通用检测器中，在不事先知道退化类型的情况下，在受损图像上实现显著提升。实验结果表明，CPA-E

    arXiv:2403.11220v1 Announce Type: cross  Abstract: Object detection methods under known single degradations have been extensively investigated. However, existing approaches require prior knowledge of the degradation type and train a separate model for each, limiting their practical applications in unpredictable environments. To address this challenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer, CPA-Enhancer, for object detection under unknown degradations. Specifically, CPA-Enhancer progressively adapts its enhancement strategy under the step-by-step guidance of CoT prompts, that encode degradation-related information. To the best of our knowledge, it's the first work that exploits CoT prompting for object detection tasks. Overall, CPA-Enhancer is a plug-and-play enhancement model that can be integrated into any generic detectors to achieve substantial gains on degraded images, without knowing the degradation type priorly. Experimental results demonstrate that CPA-E
    
[^117]: 大型语言模型指导的心力衰竭风险预测的ECG双注意力网络

    Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction

    [https://arxiv.org/abs/2403.10581](https://arxiv.org/abs/2403.10581)

    提出了一种大型语言模型指导的双注意力ECG网络，用于心力衰竭风险预测，能够捕捉复杂的心电图特征，有效应对低风险和高风险组之间的不平衡。

    

    心力衰竭（HF）由于全球死亡率不断上升而构成重大公共卫生挑战。通过早期诊断和预防来解决这一问题可显著减少疾病对社会的影响。本文引入了一种使用临床获取的12导联心电图（ECG）进行HF风险预测的方法。我们提出了一种新颖的、轻量级的双注意力ECG网络，旨在捕捉对早期HF预测至关重要的复杂心电图特征，尽管低风险和高风险组之间存在明显的不平衡。该网络具有一个跨导注意力模块和12个导联特定的时间注意力模块，以捕捉交叉导联交互作用和每个导联内的局部时间动态。为了防止模型过拟合于有限的训练数据，我们利用一个大型语言模型（LLM）与公共ECG-Report数据集进行预训练，用于进行ECG-报告对齐任务。然后对网络进行fine-tune以用于HF风险预测

    arXiv:2403.10581v1 Announce Type: cross  Abstract: Heart failure (HF) poses a significant public health challenge due to its rising global mortality rate. Addressing this issue through early diagnosis and prevention could significantly reduce the disease's impact. This work introduces a methodology for HF risk prediction using clinically acquired 12-lead electrocardiograms (ECGs). We present a novel, lightweight dual-attention ECG network designed to capture complex ECG features essential for early HF prediction, despite the notable imbalance between low and high-risk groups. The network features a cross-lead attention module and twelve lead-specific temporal attention modules to capture cross-lead interactions and local temporal dynamics within each lead. To prevent model overfitting from limited training data, we leverage a large language model (LLM) with a public ECG-Report dataset for pretraining on an ECG-report alignment task. The network is then fine-tuned for HF risk prediction
    
[^118]: GPT4动力AI代理能成为足够优秀的绩效归因分析师吗？

    Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution Analyst?

    [https://arxiv.org/abs/2403.10482](https://arxiv.org/abs/2403.10482)

    大型语言模型和AI代理的整合在绩效归因分析领域标志着一项开创性发展，能自动化和增强投资组合绩效归因分析。

    

    绩效归因分析被定义为解释投资组合相对于基准的超额绩效驱动因素的过程，在投资组合管理中占据重要地位，在投资决策过程中起着至关重要的作用，特别是在基金管理行业。根植于牢固的金融和数学框架中，这种分析技术的重要性和方法学已在众多学术研究论文和著作中得到广泛记录。大型语言模型（LLMs）和AI代理的整合标志着该领域的开创性发展。这些代理旨在通过准确计算和分析投资组合表现与基准之间的差异，自动化和增强绩效归因分析。本研究介绍了将AI代理应用于各种重要的绩效归因任务，包括分析

    arXiv:2403.10482v1 Announce Type: cross  Abstract: Performance attribution analysis, defined as the process of explaining the drivers of the excess performance of an investment portfolio against a benchmark, stands as a significant aspect of portfolio management and plays a crucial role in the investment decision-making process, particularly within the fund management industry. Rooted in a solid financial and mathematical framework, the importance and methodologies of this analytical technique are extensively documented across numerous academic research papers and books. The integration of large language models (LLMs) and AI agents marks a groundbreaking development in this field. These agents are designed to automate and enhance the performance attribution analysis by accurately calculating and analyzing portfolio performances against benchmarks. In this study, we introduce the application of an AI Agent for a variety of essential performance attribution tasks, including the analysis 
    
[^119]: 预测AI结肠镜模型对未知数据的泛化能力

    Predicting Generalization of AI Colonoscopy Models to Unseen Data

    [https://arxiv.org/abs/2403.09920](https://arxiv.org/abs/2403.09920)

    使用“Masked Siamese Network”（MSN）在未标记数据上识别新现象，并预测结肠镜模型对未知技术和不同国家数据的性能。

    

    背景和目标 AI结肠镜算法的泛化能力对于在临床实践中更广泛的应用至关重要。然而，目前评估在未知数据上的性能的技术需要昂贵且耗时的标签。我们使用“Masked Siamese Network”（MSN）在未知数据中识别新现象并预测息肉检测器的性能。MSN被训练来预测息肉图像中被屏蔽的区域，而无需任何标签。我们测试了MSN仅在以色列数据上进行训练的能力，以及在日本结肠镜（354个视频，128小时）上检测未知技术：窄带成像（NBI）和色细胞内镜（CE）。我们还测试了MSN预测跨国结肠镜视频上的息肉计算机辅助检测（CADe）的性能，尽管MSN未接受过来自日本的数据的训练。

    arXiv:2403.09920v1 Announce Type: cross  Abstract: Background and aims Generalizability of AI colonoscopy algorithms is important for wider adoption in clinical practice. However, current techniques for evaluating performance on unseen data require expensive and time-intensive labels.   Methods We use a "Masked Siamese Network" (MSN) to identify novel phenomena in unseen data and predict polyp detector performance. MSN is trained to predict masked out regions of polyp images, without any labels. We test MSN's ability to be trained on data only from Israel and detect unseen techniques, narrow-band imaging (NBI) and chromendoscoy (CE), on colonoscopes from Japan (354 videos, 128 hours). We also test MSN's ability to predict performance of Computer Aided Detection (CADe) of polyps on colonoscopies from both countries, even though MSN is not trained on data from Japan.   Results MSN correctly identifies NBI and CE as less similar to Israel whitelight than Japan whitelight (bootstrapped z-t
    
[^120]: 评估大语言模型作为对话推荐中生成用户模拟器

    Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation

    [https://arxiv.org/abs/2403.09738](https://arxiv.org/abs/2403.09738)

    大型语言模型作为生成式用户模拟器在对话推荐中展现出潜力，新的协议通过五个任务评估了语言模型模拟人类行为的准确程度，揭示了模型与人类行为的偏差，并提出了如何通过模型选择和提示策略减少这些偏差。

    

    合成用户是对话推荐系统评估中成本效益较高的真实用户代理。大型语言模型表现出在模拟类似人类行为方面的潜力，这引发了它们能否代表多样化用户群体的问题。我们引入了一个新的协议，用于衡量语言模型能够准确模拟对话推荐中人类行为的程度。该协议由五个任务组成，每个任务旨在评估合成用户应该表现出的关键特性：选择要谈论的物品，表达二进制偏好，表达开放式偏好，请求推荐以及提供反馈。通过对基准模拟器的评估，我们展示了这些任务有效地揭示了语言模型与人类行为的偏差，并提供了关于如何通过模型选择和提示策略减少这些偏差的见解。

    arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
    
[^121]: VisionGPT-3D:一种用于增强3D视觉理解的通用多模态代理

    VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding

    [https://arxiv.org/abs/2403.09530](https://arxiv.org/abs/2403.09530)

    提出了一个统一的VisionGPT-3D框架，整合了最先进的视觉模型，有助于提升计算机视觉对于3D视觉理解的能力

    

    文本向视觉组件的演进促进了人们日常生活的便利，例如从文本生成图像、视频并识别图像中所需的元素。以前的计算机视觉模型专注于基于明确定义对象的图像检测、分类。大型语言模型(LLMs)将自然语言转换为视觉对象，为文本背景提供了视觉布局。OpenAI GPT-4已成为LLMs的顶峰，而计算机视觉(CV)领域拥有大量最先进的模型和算法，可将2D图像转换为它们的3D表示。然而，算法与问题之间的不匹配可能导致不良结果。针对这一挑战，我们提出了一个统一的VisionGPT-3D框架， conslidate了最先进的视觉模型，从而促进了发展。

    arXiv:2403.09530v1 Announce Type: cross  Abstract: The evolution of text to visual components facilitates people's daily lives, such as generating image, videos from text and identifying the desired elements within the images. Computer vision models involving the multimodal abilities in the previous days are focused on image detection, classification based on well-defined objects. Large language models (LLMs) introduces the transformation from nature language to visual objects, which present the visual layout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs, while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models and algorithms to convert 2D images to their 3D representations. However, the mismatching between the algorithms with the problem could lead to undesired results. In response to this challenge, we propose an unified VisionGPT-3D framework to consolidate the state-of-the-art vision models, thereby facilitating the development
    
[^122]: 持续预训练大型语言模型的简单可扩展策略

    Simple and Scalable Strategies to Continually Pre-train Large Language Models

    [https://arxiv.org/abs/2403.08763](https://arxiv.org/abs/2403.08763)

    通过简单和可扩展的学习率调整、重放数据的方法，可以在不重新训练的情况下，持续预训练大型语言模型以匹配完全重新训练时的性能。

    

    大型语言模型（LLMs）通常在数十亿的标记上进行常规预训练，一旦有新数据可用就重新开始该过程。一个更有效率的解决方案是持续预训练这些模型，与重新训练相比能节省大量计算资源。然而，新数据引起的分布转移通常会导致在以前数据上降低性能或无法适应新数据。在本工作中，我们展示了一种简单且可扩展的学习率（LR）重新升温、LR重新衰减和重放上一数据的组合足以与完全从头开始重新训练在所有可用数据上的性能相匹配，从最终损失和语言模型（LM）评估基准的角度衡量。具体而言，我们展示了在两个常用的LLM预训练数据集（英语→英语）之间的弱但现实的分布转移以及更强烈的分布转移（英语→德语）下的情况。

    arXiv:2403.08763v1 Announce Type: cross  Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at th
    
[^123]: TeleMoMa：一种用于移动操作的模块化多功能远程操作系统

    TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation

    [https://arxiv.org/abs/2403.07869](https://arxiv.org/abs/2403.07869)

    TeleMoMa 是一种面向移动操作的模块化多功能远程操作系统，通过整合多种人机接口、降低门槛且具有通用性，为移动操作器提供了全身远程操作的解决方案。

    

    机器人学中限制模仿学习的关键瓶颈是数据的匮乏。这个问题在移动操作中更为严重，因为与静止操作相比，由于缺乏可用且易于使用的远程操作界面，收集演示更加困难。在这项工作中，我们展示了TeleMoMa，这是一种用于全身远程操作移动操作器的通用和模块化界面。TeleMoMa将包括RGB和深度摄像头、虚拟现实控制器、键盘、操纵杆等多个人机接口整合在一起，以及这些接口的任何组合。在其更易访问的版本中， TeleMoMa可以仅使用视觉（如RGB-D相机）即可工作，降低了人类提供移动操作演示的门槛。我们通过在模拟环境和现实世界中远程操作几个现有的移动操作器——PAL Tiago++, Toyota HSR和Fetch来展现TeleMoMa的多功能性。

    arXiv:2403.07869v1 Announce Type: cross  Abstract: A critical bottleneck limiting imitation learning in robotics is the lack of data. This problem is more severe in mobile manipulation, where collecting demonstrations is harder than in stationary manipulation due to the lack of available and easy-to-use teleoperation interfaces. In this work, we demonstrate TeleMoMa, a general and modular interface for whole-body teleoperation of mobile manipulators. TeleMoMa unifies multiple human interfaces including RGB and depth cameras, virtual reality controllers, keyboard, joysticks, etc., and any combination thereof. In its more accessible version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering the entry bar for humans to provide mobile manipulation demonstrations. We demonstrate the versatility of TeleMoMa by teleoperating several existing mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and the real world. We demonstrate the quality of the demonst
    
[^124]: 在大型知识图上训练面向任务的图神经网络，实现准确高效建模

    Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling

    [https://arxiv.org/abs/2403.05752](https://arxiv.org/abs/2403.05752)

    本文提出了一种自动化TOSG提取的方法KG-TOSA，用于在大型知识图上进行面向任务的图神经网络训练，以减轻对大型KG的过多计算负担。

    

    知识图（KG）是一种包含各种节点和边类型的异构图。异构图神经网络（HGNNs）通常用于在KG上训练节点分类和链接预测等机器学习任务。然而，HGNN方法受KG的大小、密度以及节点和边类型数量的影响，表现出过多的复杂性。AI从业者手工设计出一个与特定任务相关的KG G的子图，我们称之为面向任务的子图（TOSG），其中包含G中与任务相关的节点和边类型的子集。使用TOSG而不是G来训练任务可以减轻对大型KG所需的过多计算。设计TOSG需要深入了解KG的结构和任务的目标，因此具有挑战性且耗时。本文提出了KG-TOSA，一种自动化TOSG提取的方法，用于在大型KG上进行面向任务的HGNN训练。

    arXiv:2403.05752v1 Announce Type: cross  Abstract: A Knowledge Graph (KG) is a heterogeneous graph encompassing a diverse range of node and edge types. Heterogeneous Graph Neural Networks (HGNNs) are popular for training machine learning tasks like node classification and link prediction on KGs. However, HGNN methods exhibit excessive complexity influenced by the KG's size, density, and the number of node and edge types. AI practitioners handcraft a subgraph of a KG G relevant to a specific task. We refer to this subgraph as a task-oriented subgraph (TOSG), which contains a subset of task-related node and edge types in G. Training the task using TOSG instead of G alleviates the excessive computation required for a large KG. Crafting the TOSG demands a deep understanding of the KG's structure and the task's objectives. Hence, it is challenging and time-consuming. This paper proposes KG-TOSA, an approach to automate the TOSG extraction for task-oriented HGNN training on a large KG. In KG
    
[^125]: 利用大型语言模型在心理治疗中进行认知重构

    HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy

    [https://arxiv.org/abs/2403.05574](https://arxiv.org/abs/2403.05574)

    这一创新心理治疗模型HealMe通过基于心理治疗框架的共情对话，有效解决了根深蒂固的负面思维，并促进了理性、平衡的观点。

    

    大型语言模型（LLMs）在心理治疗中可以发挥重要作用，熟练处理认知重构等关键任务，克服羞耻、不信任、治疗师技能差异和资源稀缺等挑战。在先前的认知重构中，主要将负面情绪转化为积极的，但这些方法效果有限，经常不能促进客户自我发现替代视角。在本文中，我们揭示了帮助和赋能通过自适应语言在心理增强（HealMe）模型。这种新颖的认知重构疗法方法有效地解决了根深蒂固的负面想法，并促进理性、平衡的视角。HealMe与传统LLM方法不同，采用基于心理治疗框架的共情对话。它通过系统指导客户区分情境和感受，集思广益寻找替代视角，并制定...

    arXiv:2403.05574v1 Announce Type: cross  Abstract: Large Language Models (LLMs) can play a vital role in psychotherapy by adeptly handling the crucial task of cognitive reframing and overcoming challenges such as shame, distrust, therapist skill variability, and resource scarcity. Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients' self-discovery of alternative perspectives. In this paper, we unveil the Helping and Empowering through Adaptive Language in Mental Enhancement (HealMe) model. This novel cognitive reframing therapy method effectively addresses deep-rooted negative thoughts and fosters rational, balanced perspectives. Diverging from traditional LLM methods, HealMe employs empathetic dialogue based on psychotherapeutic frameworks. It systematically guides clients through distinguishing circumstances from feelings, brainstorming alternative viewpoints, and developing 
    
[^126]: 更快的邻域注意力: 在线程块级别减少自注意力的O(n^2)成本

    Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level

    [https://arxiv.org/abs/2403.04690](https://arxiv.org/abs/2403.04690)

    该研究提出了一种更快的邻域注意力机制，通过将注意力限制在最近的邻居之间来降低自注意力的计算复杂度，实现了显著的性能提升。

    

    邻域注意力通过限制每个标记的注意力范围为其最近的邻居来降低自注意力的成本。该限制由窗口大小和扩张因子参数化，介于线性投影和自注意力之间绘制了可能的注意力模式谱。邻域注意力，以及更一般地滑动窗口注意力模式，在基础设施方面长期受到限制，特别是在更高秩的空间（2-D和3-D），促使开发定制内核的发展，这些内核在功能或性能方面受限，如果不是两者都有。在这项工作中，我们首先展示邻域注意力可以表示为批量化的GEMM问题，类似于标准注意力，并为1-D和2-D邻域注意力实现它。与现有的简单内核相比，这些内核平均提供了分别是1-D和2-D邻域注意力的全精度延迟改进分别为895%和272%。

    arXiv:2403.04690v1 Announce Type: cross  Abstract: Neighborhood attention reduces the cost of self attention by restricting each token's attention span to its nearest neighbors. This restriction, parameterized by a window size and dilation factor, draws a spectrum of possible attention patterns between linear projection and self attention. Neighborhood attention, and more generally sliding window attention patterns, have long been bounded by infrastructure, particularly in higher-rank spaces (2-D and 3-D), calling for the development of custom kernels, which have been limited in either functionality, or performance, if not both. In this work, we first show that neighborhood attention can be represented as a batched GEMM problem, similar to standard attention, and implement it for 1-D and 2-D neighborhood attention. These kernels on average provide 895% and 272% improvement in full precision latency compared to existing naive kernels for 1-D and 2-D neighborhood attention respectively. 
    
[^127]: 使用异构图对比迁移学习实现零样本跨语言文档级事件因果识别

    Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning

    [https://arxiv.org/abs/2403.02893](https://arxiv.org/abs/2403.02893)

    提出了一种使用异构图对比迁移学习的方法，实现了零样本跨语言文档级事件因果识别，并在实验证明在F1得分上优于之前的最先进模型。

    

    事件因果识别（ECI）指的是在文本中检测事件之间的因果关系。然而，大多数现有研究都集中在高资源语言下的句子级ECI，而对于低资源语言下更具挑战性的文档级ECI（DECI）却尚未得到充分探索。在本文中，我们提出了一种带有多粒度对比传递学习（GIMC）的异构图交互模型，用于实现零样本跨语言文档级ECI。具体来说，我们引入了一个异构图交互网络来建模文档中分散事件之间的远距离依赖关系。然后，为了提高从源语言学习到的因果知识的跨语言可转移性，我们提出了一个多粒度对比传递学习模块，以调整跨语言间的因果表示。大量实验证明，我们的框架在平均F1得分上优于之前的最先进模型约9.4%和8.2%。

    arXiv:2403.02893v1 Announce Type: cross  Abstract: Event Causality Identification (ECI) refers to detect causal relations between events in texts. However, most existing studies focus on sentence-level ECI with high-resource language, leaving more challenging document-level ECI (DECI) with low-resource languages under-explored. In this paper, we propose a Heterogeneous Graph Interaction Model with Multi-granularity Contrastive Transfer Learning (GIMC) for zero-shot cross-lingual document-level ECI. Specifically, we introduce a heterogeneous graph interaction network to model the long-distance dependencies between events that are scattered over document. Then, to improve cross-lingual transferability of causal knowledge learned from source language, we propose a multi-granularity contrastive transfer learning module to align the causal representations across languages. Extensive experiments show our framework outperforms previous state-of-the-art model by 9.4% and 8.2% of average F1 sco
    
[^128]: Align-to-Distill: 可训练的注意力对齐在神经机器翻译中的知识蒸馏

    Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation

    [https://arxiv.org/abs/2403.01479](https://arxiv.org/abs/2403.01479)

    "本文提出了“Align-to-Distill”（A2D）策略，通过在训练过程中自适应地对齐学生注意力头与其教师对应物，转化了组合映射启发式方法为学习问题，实验结果显示A2D的有效性，对WMT-2022 De->Dsb和WMT-2014 En->De的BLEU分数分别获得高达+3.61和+0.63的提升。"

    

    可扩展的深度模型和大规模数据集的出现提高了神经机器翻译的性能。知识蒸馏（KD）通过将知识从教师模型传输到更紧凑的学生模型来提高效率。然而，针对Transformer架构的KD方法通常依赖于启发式方法，特别是在决定要从哪些教师层中蒸馏知识时。本文介绍了“Align-to-Distill”（A2D）策略，旨在通过在训练过程中自适应地对齐学生注意力头与其教师对应物来解决特征映射问题。A2D中的注意力对齐模块执行学生和教师注意力头之间的密集逐头比较，将组合映射启发式方法转化为学习问题。我们的实验展示了A2D的有效性，对WMT-2022 De->Dsb和WMT-2014 En->De的BLEU分数分别获得高达+3.61和+0.63的提升。

    arXiv:2403.01479v1 Announce Type: cross  Abstract: The advent of scalable deep models and large datasets has improved the performance of Neural Machine Translation. Knowledge Distillation (KD) enhances efficiency by transferring knowledge from a teacher model to a more compact student model. However, KD approaches to Transformer architecture often rely on heuristics, particularly when deciding which teacher layers to distill from. In this paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to address the feature mapping problem by adaptively aligning student attention heads with their teacher counterparts during training. The Attention Alignment Module in A2D performs a dense head-by-head comparison between student and teacher attention heads across layers, turning the combinatorial mapping heuristics into a learning problem. Our experiments show the efficacy of A2D, demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb and WMT-2014 En->De, respe
    
[^129]: KoCoSa: 韩文上下文感知讽刺检测数据集

    KoCoSa: Korean Context-aware Sarcasm Detection Dataset

    [https://arxiv.org/abs/2402.14428](https://arxiv.org/abs/2402.14428)

    该论文介绍了一个新的针对韩文对话讽刺检测任务的数据集KoCoSa，提出了一种高效的讽刺检测数据集生成流程，并提供了针对该任务的简单但有效的基线模型。

    

    讽刺是一种言语讽刺的方式，指的是有人说了和他们的本意相反的话，通常是为了嘲笑一个人、情况或想法。检测对话中的讽刺通常是困难的，因为检测讽刺应该反映上下文（即对话历史）。本文介绍了一个针对韩文对话讽刺检测任务的新数据集KoCoSa（韩文上下文感知讽刺检测数据集），包括12.8K个日常韩文对话以及该任务在最后一次回复上的标签。为了构建该数据集，我们提出了一种高效的讽刺检测数据集生成流程：1）使用大型语言模型从源对话中生成新的讽刺对话，2）自动和手动过滤异常和有毒对话，3）为讽刺检测任务进行人工注释。我们还提供了一个简单但有效的针对韩文讽刺检测任务的基线，该基线是在我们的数据集上训练的。

    arXiv:2402.14428v1 Announce Type: cross  Abstract: Sarcasm is a way of verbal irony where someone says the opposite of what they mean, often to ridicule a person, situation, or idea. It is often difficult to detect sarcasm in the dialogue since detecting sarcasm should reflect the context (i.e., dialogue history). In this paper, we introduce a new dataset for the Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware Sarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and the labels for this task on the last response. To build the dataset, we propose an efficient sarcasm detection dataset generation pipeline: 1) generating new sarcastic dialogues from source dialogues with large language models, 2) automatic and manual filtering of abnormal and toxic dialogues, and 3) human annotation for the sarcasm detection task. We also provide a simple but effective baseline for the Korean sarcasm detection task trained on our dataset. Experimental results on t
    
[^130]: ToonAging: 艺术肖像风格转换下的人脸逆龄化

    ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer

    [https://arxiv.org/abs/2402.02733](https://arxiv.org/abs/2402.02733)

    本研究提出了一种新颖的一阶段方法，结合肖像风格转换实现人脸逆龄化，解决了NPR图像上编辑年龄的问题，并在单个生成步骤中执行。该方法利用了现有的人脸逆龄化和风格转换网络，并且独特地融合了不同的潜在向量，从而保留了面部属性。

    

    人脸逆龄化是计算机视觉和图形学中的一个重要领域，在电影、广告和直播等逼真领域中具有重要应用。最近，将人脸逆龄化应用于非逼真图像，如漫画、插图和动画，在各种娱乐行业中成为一个新的需求。然而，缺乏一个能够无缝编辑NPR图像上显现年龄的网络意味着这些任务一直局限于一个简单的顺序方法，这往往会导致不愉快的伪影和由于域差异而丢失面部属性。在本文中，我们引入了一种新颖的单阶段人脸逆龄化方法，结合了肖像风格转换，在一个生成步骤中完成。我们利用现有的人脸逆龄化和风格转换网络，两者都在相同的PR领域进行训练。我们的方法独特地融合了不同的潜在向量，每个向量负责管理与衰老相关的属性。

    Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attribu
    
[^131]: 将“段分离任意模型”推进至高度准确的二元图像分割

    Promoting Segment Anything Model towards Highly Accurate Dichotomous Image Segmentation

    [https://arxiv.org/abs/2401.00248](https://arxiv.org/abs/2401.00248)

    将段分离任意模型推进至高度准确的二元图像分割，通过提出DIS-SAM框架，成功改进SAM模型在细节方面的表现，实现了显著增强的分割精度。

    

    Segment Anything Model (SAM)代表了计算机视觉基础模型的重大突破，提供了大规模图像分割模型。然而，尽管SAM的零-shot表现，其分割蒙版缺乏细粒度细节，特别是在准确描绘对象边界方面。我们对SAM是否可以作为基础模型进一步改进以实现高度精确的对象分割（即称为二元图像分割DIS）抱有很高期望。为解决这一问题，我们提出了DIS-SAM，将SAM推进至DIS，具有极高的精确细节。DIS-SAM是一个专门为高度准确分割而设计的框架，保持了SAM的可促进设计。DIS-SAM采用了两阶段方法，将SAM与专门用于DIS的修改后的IS-Net集成在一起。尽管简单，DIS-SAM相比SAM和HQ-SA表现出显着增强的分割精度。

    arXiv:2401.00248v2 Announce Type: replace-cross  Abstract: The Segment Anything Model (SAM) represents a significant breakthrough into foundation models for computer vision, providing a large-scale image segmentation model. However, despite SAM's zero-shot performance, its segmentation masks lack fine-grained details, particularly in accurately delineating object boundaries. We have high expectations regarding whether SAM, as a foundation model, can be improved towards highly accurate object segmentation, which is known as dichotomous image segmentation (DIS). To address this issue, we propose DIS-SAM, which advances SAM towards DIS with extremely accurate details. DIS-SAM is a framework specifically tailored for highly accurate segmentation, maintaining SAM's promptable design. DIS-SAM employs a two-stage approach, integrating SAM with a modified IS-Net dedicated to DIS. Despite its simplicity, DIS-SAM demonstrates significantly enhanced segmentation accuracy compared to SAM and HQ-SA
    
[^132]: 使用自然语言推理构建高效的通用分类器

    Building Efficient Universal Classifiers with Natural Language Inference

    [https://arxiv.org/abs/2312.17543](https://arxiv.org/abs/2312.17543)

    本文探讨了如何利用自然语言推理作为通用分类任务，提供了构建通用分类器的详细步骤，并分享了该通用分类器在33个数据集上的训练结果

    

    arXiv:2312.17543v2 公告类型：替换交叉。生成型大型语言模型(LLMs)已经成为零样本学习和零样本学习的主流选择，这要归功于文本生成的通用性。然而，许多用户在只想自动化一个分类任务时，并不需要生成型LLMs的广泛能力。较小的类似BERT的模型也可以学习通用任务，这使它们可以在不需要微调（零样本分类）的情况下执行任何文本分类任务，或者只用少量样本学习新任务（少样本），同时比生成型LLMs高效得多。本文(1) 解释了如何将自然语言推理（NLI）作为通用分类任务，其原理类似于生成型LLMs的指导微调，(2) 提供了用于构建通用分类器的可重用Jupyter笔记本的逐步指南，(3) 共享了经过训练的通用分类器，在33个数据集上训练

    arXiv:2312.17543v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs) have become the mainstream choice for fewshot and zeroshot learning thanks to the universality of text generation. Many users, however, do not need the broad capabilities of generative LLMs when they only want to automate a classification task. Smaller BERT-like models can also learn universal tasks, which allow them to do any text classification task without requiring fine-tuning (zeroshot classification) or to learn new tasks with only a few examples (fewshot), while being significantly more efficient than generative LLMs. This paper (1) explains how Natural Language Inference (NLI) can be used as a universal classification task that follows similar principles as instruction fine-tuning of generative LLMs, (2) provides a step-by-step guide with reusable Jupyter notebooks for building a universal classifier, and (3) shares the resulting universal classifier that is trained on 33 datasets
    
[^133]: 独立学习将时间序列片段嵌入

    Learning to Embed Time Series Patches Independently

    [https://arxiv.org/abs/2312.16427](https://arxiv.org/abs/2312.16427)

    学习独立嵌入时间序列片段可以产生更好的时间序列表示，通过简单的块重构任务和独立嵌入每个块的MLP模型以及互补对比学习来实现。

    

    最近，掩码时间序列建模作为一种自监督表示学习策略引起了广泛关注。受计算机视觉中的掩码图像建模启发，最近的研究首先将时间序列进行分块处理并部分掩盖，然后训练Transformer模型通过从未掩盖的块预测被掩盖块来捕捉块之间的依赖关系。然而，我们认为捕捉这种块之间的依赖关系可能不是时间序列表示学习的最佳策略；相反，独立学习嵌入片段会产生更好的时间序列表示。具体而言，我们建议使用1）简单的块重构任务，自动将每个块进行编码而不查看其他块，以及2）独自嵌入每个块的简单块式MLP。此外，我们引入互补对比学习来有效地分层捕获相邻时间序列信息。

    arXiv:2312.16427v2 Announce Type: replace-cross  Abstract: Masked time series modeling has recently gained much attention as a self-supervised representation learning strategy for time series. Inspired by masked image modeling in computer vision, recent works first patchify and partially mask out time series, and then train Transformers to capture the dependencies between patches by predicting masked patches from unmasked patches. However, we argue that capturing such patch dependencies might not be an optimal strategy for time series representation learning; rather, learning to embed patches independently results in better time series representations. Specifically, we propose to use 1) the simple patch reconstruction task, which autoencode each patch without looking at other patches, and 2) the simple patch-wise MLP that embeds each patch independently. In addition, we introduce complementary contrastive learning to hierarchically capture adjacent time series information efficiently. 
    
[^134]: 时间序列的软对比学习

    Soft Contrastive Learning for Time Series

    [https://arxiv.org/abs/2312.16424](https://arxiv.org/abs/2312.16424)

    提出了一种名为SoftCLT的方法，通过引入实例级和时间级软对比损失，解决了在时间序列中忽略固有相关性所导致的学习表示质量下降的问题。

    

    对比学习已经被证明在自监督学习中对于从时间序列中学习表示是有效的。然而，将时间序列中相似的实例或相邻时间戳的值进行对比会忽略它们固有的相关性，从而导致学习表示的质量下降。为了解决这个问题，我们提出了SoftCLT，一种简单而有效的时间序列软对比学习策略。这是通过引入从零到一的软赋值的实例级和时间级对比损失来实现的。具体来说，我们为1)基于数据空间上的时间序列之间的距离定义了实例级对比损失的软赋值，并为2)基于时间戳之间的差异定义了时间级对比损失。SoftCLT是一种即插即用的时间序列对比学习方法，可以提高学习表示的质量，没有过多复杂的设计。

    arXiv:2312.16424v2 Announce Type: replace-cross  Abstract: Contrastive learning has shown to be effective to learn representations from time series in a self-supervised way. However, contrasting similar time series instances or values from adjacent timestamps within a time series leads to ignore their inherent correlations, which results in deteriorating the quality of learned representations. To address this issue, we propose SoftCLT, a simple yet effective soft contrastive learning strategy for time series. This is achieved by introducing instance-wise and temporal contrastive loss with soft assignments ranging from zero to one. Specifically, we define soft assignments for 1) instance-wise contrastive loss by the distance between time series on the data space, and 2) temporal contrastive loss by the difference of timestamps. SoftCLT is a plug-and-play method for time series contrastive learning that improves the quality of learned representations without bells and whistles. In experi
    
[^135]: VideoPoet：用于零样本视频生成的大型语言模型

    VideoPoet: A Large Language Model for Zero-Shot Video Generation

    [https://arxiv.org/abs/2312.14125](https://arxiv.org/abs/2312.14125)

    VideoPoet是一种大型语言模型，能够从多种条件信号中生成高质量视频及匹配音频，并且在零样本视频生成领域展示了最先进的能力。

    

    我们提出了VideoPoet，这是一种能够从各种不同的条件信号中合成高质量视频及匹配音频的语言模型。VideoPoet采用解码器-仅Transformer架构，可以处理多模态输入，包括图像、视频、文本和音频。训练协议遵循大型语言模型（LLMs）的方式，包括两个阶段：预训练和特定任务的适应。在预训练阶段，VideoPoet在自回归Transformer框架中结合了多模态生成目标的混合。预训练的LLM作为一个基础，可以为各种视频生成任务进行调整。我们展示了实证结果，展示了该模型在零样本视频生成方面的最新能力，特别突出了VideoPoet生成高保真运动的能力。

    arXiv:2312.14125v2 Announce Type: replace-cross  Abstract: We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/
    
[^136]: PIA:通过插拔式模块在文本到图像模型中实现个性化图像动画

    PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models

    [https://arxiv.org/abs/2312.13964](https://arxiv.org/abs/2312.13964)

    PIA通过插拔式模块在文本到图像模型中实现个性化图像动画，并解决了保留独特风格、高保真细节和动作可控性的挑战

    

    最近个性化文本到图像（T2I）模型的进展已经彻底改变了内容创作，使非专家能够生成具有独特风格的惊人图像。然而，通过文本为这些个性化图像增加逼真的动作在保留独特风格、高保真细节和通过文本实现动作可控性方面面临着重大挑战。在本文中，我们提出了PIA，一种个性化图像动画生成器，其在与条件图像对齐、通过文本实现动作可控性以及与各种个性化T2I模型的兼容性上表现出色，无需特定调整。为了实现这些目标，PIA在基础T2I模型的基础上构建了经过良好训练的时间对齐层，从而实现了任何个性化T2I模型向图像动画模型的无缝转换。PIA的一个关键组件是引入条件模块，利用条件帧和帧间

    arXiv:2312.13964v2 Announce Type: replace-cross  Abstract: Recent advancements in personalized text-to-image (T2I) models have revolutionized content creation, empowering non-experts to generate stunning images with unique styles. While promising, adding realistic motions into these personalized images by text poses significant challenges in preserving distinct styles, high-fidelity details, and achieving motion controllability by text. In this paper, we present PIA, a Personalized Image Animator that excels in aligning with condition images, achieving motion controllability by text, and the compatibility with various personalized T2I models without specific tuning. To achieve these goals, PIA builds upon a base T2I model with well-trained temporal alignment layers, allowing for the seamless transformation of any personalized T2I model into an image animation model. A key component of PIA is the introduction of the condition module, which utilizes the condition frame and inter-frame af
    
[^137]: FERGI：来自自发面部表情反应的文本到图像生成用户偏好的自动注释

    FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction

    [https://arxiv.org/abs/2312.03187](https://arxiv.org/abs/2312.03187)

    开发了一种从用户自发面部表情反应中自动注释用户对生成图像偏好的方法，发现多个面部动作单元与用户对生成图像的评估高度相关，可用于通过这些面部动作单元区分图像对并自动标注用户偏好。

    

    研究人员提出使用人类偏好反馈数据来微调文本到图像生成模型。然而，由于其依赖于手动注释，人类反馈收集的可扩展性受到限制。因此，我们开发并测试了一种方法，从用户的自发面部表情反应中自动注释其对生成图像的偏好。我们收集了一个面部表情反应到生成图像（FERGI）的数据集，并展示了多个面部运动单元（AUs）的激活与用户对生成图像的评估高度相关。具体来说，AU4（眉毛下垂者）反映了对生成图像的负面评价，而AU12（嘴角拉动者）反映了正面评价。这两者在两个方面都很有用。首先，我们可以准确地使用这些AU响应存在实质差异的图像对之间自动注释用户偏好。

    arXiv:2312.03187v2 Announce Type: replace-cross  Abstract: Researchers have proposed to use data of human preference feedback to fine-tune text-to-image generative models. However, the scalability of human feedback collection has been limited by its reliance on manual annotation. Therefore, we develop and test a method to automatically annotate user preferences from their spontaneous facial expression reaction to the generated images. We collect a dataset of Facial Expression Reaction to Generated Images (FERGI) and show that the activations of multiple facial action units (AUs) are highly correlated with user evaluations of the generated images. Specifically, AU4 (brow lowerer) is reflective of negative evaluations of the generated image whereas AU12 (lip corner puller) is reflective of positive evaluations. These can be useful in two ways. Firstly, we can automatically annotate user preferences between image pairs with substantial difference in these AU responses with an accuracy sig
    
[^138]: Hulk: 一种面向人类中心任务的通用知识翻译器

    Hulk: A Universal Knowledge Translator for Human-Centric Tasks

    [https://arxiv.org/abs/2312.01697](https://arxiv.org/abs/2312.01697)

    Hulk是第一个多模态人类中心通用模型，能够处理2D视觉、3D视觉、基于骨架和视觉语言任务，无需任务特定微调

    

    人类中心感知任务，例如行人检测、基于骨架的动作识别和姿态估计，在诸如元宇宙和体育分析等广泛的工业应用中具有重要意义。近来，出现了发展旨在受益于广泛人类中心感知任务的人类中心基础模型的激增。虽然许多人类中心基础模型取得了成功，但它们没有探索用于人类中心及需要任务特定微调的3D和视觉语言任务。这些限制限制了它们在更多下游任务和情境中的应用。为了解决这些问题，我们提出了Hulk，第一个能够在无需任务特定微调的情况下处理2D视觉、3D视觉、基于骨架和视觉语言任务的多模态人类中心通用模型。实现这一目标的关键在于将各种任务特定头部压缩成两个通用头部，一个用于离散表示，如语言，

    arXiv:2312.01697v4 Announce Type: replace-cross  Abstract: Human-centric perception tasks, e.g., pedestrian detection, skeleton-based action recognition, and pose estimation, have wide industrial applications, such as metaverse and sports analysis. There is a recent surge to develop human-centric foundation models that can benefit a broad range of human-centric perception tasks. While many human-centric foundation models have achieved success, they did not explore 3D and vision-language tasks for human-centric and required task-specific finetuning. These limitations restrict their application to more downstream tasks and situations. To tackle these problems, we present Hulk, the first multimodal human-centric generalist model, capable of addressing 2D vision, 3D vision, skeleton-based, and vision-language tasks without task-specific finetuning. The key to achieving this is condensing various task-specific heads into two general heads, one for discrete representations, e.g., languages, 
    
[^139]: 用大型语言模型赋能自动驾驶：一个安全的视角

    Empowering Autonomous Driving with Large Language Models: A Safety Perspective

    [https://arxiv.org/abs/2312.00812](https://arxiv.org/abs/2312.00812)

    本文通过整合大型语言模型（LLMs）到自动驾驶系统中，利用其常识知识和推理能力，作为智能决策者来增强驾驶性能和安全性。

    

    自动驾驶（AD）在长尾未知驾驶场景中遇到了重大的安全障碍，主要源自AD系统内部深度神经网络的不可解释性和泛化能力差，特别是在分布外和不确定数据方面。为此，本文探讨了将大型语言模型（LLM）整合到AD系统中，利用它们强大的常识知识和推理能力。所提出的方法将LLM用作行为规划中的智能决策者，配备一个安全验证器护盾进行上下文安全学习，以增强驾驶性能和安全性。我们在模拟环境中展示了两个关键研究：一种自适应LLM调节的模型预测控制（MPC）和一种带有状态机的LLM启用交互式行为规划方案。表现出比现有技术方法更优越的性能和安全度量指标。

    arXiv:2312.00812v4 Announce Type: replace  Abstract: Autonomous Driving (AD) encounters significant safety hurdles in long-tail unforeseen driving scenarios, largely stemming from the non-interpretability and poor generalization of the deep neural networks within the AD system, particularly in out-of-distribution and uncertain data. To this end, this paper explores the integration of Large Language Models (LLMs) into AD systems, leveraging their robust common-sense knowledge and reasoning abilities. The proposed methodologies employ LLMs as intelligent decision-makers in behavioral planning, augmented with a safety verifier shield for contextual safety learning, for enhancing driving performance and safety. We present two key studies in a simulated environment: an adaptive LLM-conditioned Model Predictive Control (MPC) and an LLM-enabled interactive behavior planning scheme with a state machine. Demonstrating superior performance and safety metrics compared to state-of-the-art approach
    
[^140]: 在大约5个步骤中，用于扩散模型的快速基于ODE的抽样

    Fast ODE-based Sampling for Diffusion Models in Around 5 Steps

    [https://arxiv.org/abs/2312.00094](https://arxiv.org/abs/2312.00094)

    提出了一种基于几何观察的Approximate MEan-Direction Solver（AMED-Solver），能够通过直接学习均方向来消除截断误差，从而实现快速扩散抽样。

    

    从扩散模型中进行抽样可以被视为解决相应的常微分方程（ODE），旨在以尽可能少的函数评估次数（NFE）获得准确解。最近，出现了利用高阶ODE求解器的各种快速抽样器，并且比最初的一阶求解器表现更好。然而，这些数值方法固有地导致某些近似误差，极大地降低了具有极小NFE（例如，约为5）的样本质量。相反，基于几何观察，每个抽样轨迹几乎位于嵌入在环境空间中的二维子空间中，我们提出了用于快速扩散抽样的AME近似均方向求解器（AMED-Solver），通过直接学习均方向来消除截断误差。此外，我们的方法可以轻松作为插件使用，以进一步改进现有的基于ODE的方法。

    arXiv:2312.00094v2 Announce Type: replace-cross  Abstract: Sampling from diffusion models can be treated as solving the corresponding ordinary differential equations (ODEs), with the aim of obtaining an accurate solution with as few number of function evaluations (NFE) as possible. Recently, various fast samplers utilizing higher-order ODE solvers have emerged and achieved better performance than the initial first-order one. However, these numerical methods inherently result in certain approximation errors, which significantly degrades sample quality with extremely small NFE (e.g., around 5). In contrast, based on the geometric observation that each sampling trajectory almost lies in a two-dimensional subspace embedded in the ambient space, we propose Approximate MEan-Direction Solver (AMED-Solver) that eliminates truncation errors by directly learning the mean direction for fast diffusion sampling. Besides, our method can be easily used as a plugin to further improve existing ODE-base
    
[^141]: 通用智能体用于神经网络优化

    Generalisable Agents for Neural Network Optimisation

    [https://arxiv.org/abs/2311.18598](https://arxiv.org/abs/2311.18598)

    通用智能体用于神经网络优化是一种多智能体强化学习方法，通过动态调度超参数来优化神经网络训练，可以有效改善全局性能并与手工设计启发式方法相竞争。

    

    深度神经网络的优化是一项具有挑战性的任务，原因在于复杂的训练动态、高计算要求和长时间训练。为了解决这一困难，我们提出了通用智能体用于神经网络优化（GANNO）的框架--一种多智能体强化学习（MARL）方法，通过动态和响应式地调度超参数来优化神经网络训练。GANNO利用每层一个智能体观察局部化的网络动态，并相应地采取行动来调整这些动态，从而在层级上集体改善全局性能。本文中，我们使用GANNO来控制层级学习率，并展示该框架可以产生有用且响应灵活的调度，与手工设计的启发式方法相竞争。此外，显示GANNO在各种看不见的初始条件下表现出稳健性，并且能够成功...

    arXiv:2311.18598v2 Announce Type: replace-cross  Abstract: Optimising deep neural networks is a challenging task due to complex training dynamics, high computational requirements, and long training times. To address this difficulty, we propose the framework of Generalisable Agents for Neural Network Optimisation (GANNO) -- a multi-agent reinforcement learning (MARL) approach that learns to improve neural network optimisation by dynamically and responsively scheduling hyperparameters during training. GANNO utilises an agent per layer that observes localised network dynamics and accordingly takes actions to adjust these dynamics at a layerwise level to collectively improve global performance. In this paper, we use GANNO to control the layerwise learning rate and show that the framework can yield useful and responsive schedules that are competitive with handcrafted heuristics. Furthermore, GANNO is shown to perform robustly across a wide variety of unseen initial conditions, and can succe
    
[^142]: ID样式提示学习用于少样本异常检测

    ID-like Prompt Learning for Few-Shot Out-of-Distribution Detection

    [https://arxiv.org/abs/2311.15243](https://arxiv.org/abs/2311.15243)

    提出了一种利用CLIP识别ID样式异常值并通过提示学习进行OOD检测的新框架，能够有效提高识别最具挑战性OOD样本的能力。

    

    异常检测方法通常利用辅助异常值来训练模型识别异常样本，尤其是从辅助异常值数据集中发现具有挑战性的异常值以改善异常检测。然而，它们可能仍面临有效区分与ID数据非常相似的最具挑战性的OOD样本的局限性，即ID样式样本。为此，我们提出了一个新颖的OOD检测框架，利用CLIP从ID样本的邻近空间中发现ID样式的异常值，从而帮助识别这些最具挑战性的OOD样本。然后提出了一个提示学习框架，利用识别的ID样式异常值进一步利用CLIP的能力进行OOD检测。受益于强大的CLIP，我们只需要少量ID样本即可学习模型的提示，而无需暴露其他辅助信息。

    arXiv:2311.15243v3 Announce Type: replace-cross  Abstract: Out-of-distribution (OOD) detection methods often exploit auxiliary outliers to train model identifying OOD samples, especially discovering challenging outliers from auxiliary outliers dataset to improve OOD detection. However, they may still face limitations in effectively distinguishing between the most challenging OOD samples that are much like in-distribution (ID) data, i.e., \idlike samples. To this end, we propose a novel OOD detection framework that discovers \idlike outliers using CLIP \cite{DBLP:conf/icml/RadfordKHRGASAM21} from the vicinity space of the ID samples, thus helping to identify these most challenging OOD samples. Then a prompt learning framework is proposed that utilizes the identified \idlike outliers to further leverage the capabilities of CLIP for OOD detection. Benefiting from the powerful CLIP, we only need a small number of ID samples to learn the prompts of the model without exposing other auxiliary
    
[^143]: 物理增强的光学表面印迹多保真学习

    Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint

    [https://arxiv.org/abs/2311.10278](https://arxiv.org/abs/2311.10278)

    本文提出了一种利用多保真神经网络(MFNN)解决光学图像与实际机械性能之间映射的方法。

    

    人类指纹作为每个人的独特而强大的特征，可以帮助警察识别身份。类似地，许多自然体和内在机械特性也可以通过表面特征得到唯一识别。本文提出了一种新颖的方法，使用多保真神经网络(MFNN)解决这个反问题。

    arXiv:2311.10278v2 Announce Type: replace-cross  Abstract: Human fingerprints serve as one unique and powerful characteristic for each person, from which policemen can recognize the identity. Similar to humans, many natural bodies and intrinsic mechanical qualities can also be uniquely identified from surface characteristics. To measure the elasto-plastic properties of one material, one formally sharp indenter is pushed into the measured body under constant force and retracted, leaving a unique residual imprint of the minute size from several micrometers to nanometers. However, one great challenge is how to map the optical image of this residual imprint into the real wanted mechanical properties, \ie, the tensile force curve. In this paper, we propose a novel method to use multi-fidelity neural networks (MFNN) to solve this inverse problem. We first build up the NN model via pure simulation data, and then bridge the sim-to-real gap via transfer learning. Considering the difficulty of c
    
[^144]: MacGyver：大型语言模型是否是创意问题解决者？

    MacGyver: Are Large Language Models Creative Problem Solvers?

    [https://arxiv.org/abs/2311.09682](https://arxiv.org/abs/2311.09682)

    通过创建MACGYVER数据集并与人类比较，研究发现大型语言模型在创意问题解决方面独具挑战性，在知识广度和可行性方面与人类存在独特差异，同时还展示了通过新的提示技术提升大型语言模型的问题解决能力潜力。

    

    我们在一个全新的约束设置中探究了现代大型语言模型的创意问题解决能力。为此，我们创建了MACGYVER，这是一个自动生成的数据集，包含超过1600个特意设计的现实世界问题，旨在引发物体的创新使用，并需要超越常规思维。我们随后向大型语言模型和人类展示我们的数据集，以比较和对比它们的问题解决能力。MACGYVER对这两个群体都具有挑战性，但以独特和互补的方式呈现。例如，人类擅长熟悉的任务，但在特定领域知识上有困难，导致更高的差异。相比之下，大型语言模型暴露于各种专业知识，尝试更广泛的问题，但在提出物理上不可行的行动时失败。最后，我们对大型语言模型进行了详细的错误分析，并展示了通过新的提示技术提高它们的问题解决能力的潜力。

    arXiv:2311.09682v2 Announce Type: replace-cross  Abstract: We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniqu
    
[^145]: E-Sparse: 通过基于信息熵的 N:M 稀疏性提升大型语言模型推理能力

    E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity

    [https://arxiv.org/abs/2310.15929](https://arxiv.org/abs/2310.15929)

    首次将信息熵引入剪枝度量设计，提高在大型语言模型中 N:M 稀疏性的准确性。

    

    传统的剪枝方法在大型语言模型（LLMs）中很难实现，因为它们训练过程昂贵，计算需求大。本文首次将隐藏状态特征的信息熵引入到剪枝度量设计中，即 E-Sparse，以提高LLM中 N:M 稀疏性的准确性。E-Sparse利用信息丰富性来提升通道的重要性，并进一步结合几种新颖技术来实现：(1)引入信息熵来增强参数权重和输入特征范数的重要性作为一种新颖的剪枝度量，并在不修改剩余权重的情况下执行N:M稀疏性。(2)设计全局朴素洗牌和局部块洗牌，快速优化信息分布，充分应对 N:M 稀疏性对LLMs准确性的影响。E-Sparse 被实现为一种 Spars

    arXiv:2310.15929v2 Announce Type: replace-cross  Abstract: Traditional pruning methods are known to be challenging to work in Large Language Models (LLMs) for Generative AI because of their unaffordable training process and large computational demands. For the first time, we introduce the information entropy of hidden state features into a pruning metric design, namely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse employs the information richness to leverage the channel importance, and further incorporates several novel techniques to put it into effect: (1) it introduces information entropy to enhance the significance of parameter weights and input feature norms as a novel pruning metric, and performs N:M sparsity without modifying the remaining weights. (2) it designs global naive shuffle and local block shuffle to quickly optimize the information distribution and adequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is implemented as a Spars
    
[^146]: 稳定强化学习控制：用于优化所有稳定行为的模块化框架

    Stabilizing reinforcement learning control: A modular framework for optimizing over all stable behavior

    [https://arxiv.org/abs/2310.14098](https://arxiv.org/abs/2310.14098)

    提出了一个框架，结合了深度强化学习的优化驱动和无模型的优势，以及使用Youla-Kucera参数化提供的稳定性保证，为设计反馈控制器提供了一种优化过程。

    

    我们提出了一个用于设计反馈控制器的框架，结合了深度强化学习的优化驱动和无模型的优势，以及使用Youla-Kucera参数化提供的稳定性保证来定义搜索域。最近在行为系统方面的进展使我们能够构建一个数据驱动的内部模型；这使得在完全基于输入-输出探索数据的基础上构建Youla-Kucera参数化的替代性实现成为可能。或许更值得关注的是，我们在存在噪声的情况下制定和分析了这类数据驱动模型的稳定性。Youla-Kucera方法对于控制器设计需要一个稳定的“参数”。为了训练强化学习代理，所有稳定线性运算符的集合通过矩阵因子化方法被明确给出。此外，我们还通过神经网络进行了非线性扩展，以表达参数化的一组稳定运算符。

    arXiv:2310.14098v2 Announce Type: replace-cross  Abstract: We propose a framework for the design of feedback controllers that combines the optimization-driven and model-free advantages of deep reinforcement learning with the stability guarantees provided by using the Youla-Kucera parameterization to define the search domain. Recent advances in behavioral systems allow us to construct a data-driven internal model; this enables an alternative realization of the Youla-Kucera parameterization based entirely on input-output exploration data. Perhaps of independent interest, we formulate and analyze the stability of such data-driven models in the presence of noise. The Youla-Kucera approach requires a stable "parameter" for controller design. For the training of reinforcement learning agents, the set of all stable linear operators is given explicitly through a matrix factorization approach. Moreover, a nonlinear extension is given using a neural network to express a parameterized set of stab
    
[^147]: AI-Dentify: 深度学习用于近邻牙龈龋齿在全景X光上的检测--HUNT4口腔健康研究

    AI-Dentify: Deep learning for proximal caries detection on bitewing x-ray -- HUNT4 Oral Health Study

    [https://arxiv.org/abs/2310.00354](https://arxiv.org/abs/2310.00354)

    通过深度学习模型，研究展示了对HUNT4口腔健康研究中全景X光图像进行快速准确齿龈龋齿检测的潜力

    

    背景：牙齿龋坏的诊断需要对患者的诊断性全景X光图像进行手动检查，然后通过视觉检查和触诊识别具有潜在病变的牙齿。然而，人工智能的使用，特别是深度学习，有望通过提供快速和信息丰富的全景X光图像分析来帮助诊断。

    arXiv:2310.00354v2 Announce Type: replace-cross  Abstract: Background: Dental caries diagnosis requires the manual inspection of diagnostic bitewing images of the patient, followed by a visual inspection and probing of the identified dental pieces with potential lesions. Yet the use of artificial intelligence, and in particular deep-learning, has the potential to aid in the diagnosis by providing a quick and informative analysis of the bitewing images.   Methods: A dataset of 13,887 bitewings from the HUNT4 Oral Health Study were annotated individually by six different experts, and used to train three different object detection deep-learning architectures: RetinaNet (ResNet50), YOLOv5 (M size), and EfficientDet (D0 and D1 sizes). A consensus dataset of 197 images, annotated jointly by the same six dentist, was used for evaluation. A five-fold cross validation scheme was used to evaluate the performance of the AI models.   Results: he trained models show an increase in average precision
    
[^148]: LogPr\'ecis：释放语言模型用于自动恶意日志分析

    LogPr\'ecis: Unleashing Language Models for Automated Malicious Log Analysis

    [https://arxiv.org/abs/2307.08309](https://arxiv.org/abs/2307.08309)

    本文研究如何利用最先进的语言模型自动分析类文本的Unix shell攻击日志，提出了LogPr\'ecis方法，可以自动识别攻击者策略并揭示攻击者的目标顺序

    

    arXiv:2307.08309v3 公告类型：替换-交叉 摘要：安全相关日志的收集是理解攻击行为和诊断漏洞的关键。然而，它们的分析仍然是一个艰巨的挑战。最近，语言模型（LMs）在理解自然语言和编程语言方面表现出无与伦比的潜力。一个问题是LMs是否以及如何对安全专家有用，因为他们的日志包含内在混乱和混淆信息。本文系统研究了如何从LM的最新技术中受益，以自动分析类文本的Unix shell攻击日志。我们提出了一种彻底的设计方法论，导致LogPr\'ecis。它以原始的shell会话作为输入，并自动识别和分配攻击者策略给会话的每个部分，即揭示攻击者目标的顺序。我们展示了LogPr\'ecis支持分析两个大数据集的能力

    arXiv:2307.08309v3 Announce Type: replace-cross  Abstract: The collection of security-related logs holds the key to understanding attack behaviors and diagnosing vulnerabilities. Still, their analysis remains a daunting challenge. Recently, Language Models (LMs) have demonstrated unmatched potential in understanding natural and programming languages. The question arises whether and how LMs could be also useful for security experts since their logs contain intrinsically confused and obfuscated information. In this paper, we systematically study how to benefit from the state-of-the-art in LM to automatically analyze text-like Unix shell attack logs. We present a thorough design methodology that leads to LogPr\'ecis. It receives as input raw shell sessions and automatically identifies and assigns the attacker tactic to each portion of the session, i.e., unveiling the sequence of the attacker's goals. We demonstrate LogPr\'ecis capability to support the analysis of two large datasets conta
    
[^149]: FunQA：迈向令人惊讶的视频理解

    FunQA: Towards Surprising Video Comprehension

    [https://arxiv.org/abs/2306.14899](https://arxiv.org/abs/2306.14899)

    FunQA是一个旨在评估和提高基于反直觉和有趣视频的视频推理深度的数据集，涵盖了HumorQA、CreativeQA和MagicQA三种以前未被探索的惊喜视频类型。

    

    令人惊讶的视频，比如有趣的片段、创意演出或视觉幻象，吸引了相当多的关注。对这些视频的欣赏不仅仅是对视觉刺激的反应；相反，它取决于人类理解（以及欣赏）这些视频中所描绘的常识违反的能力。我们引入了FunQA，这是一个具有挑战性的视频问答（QA）数据集，专门设计用来评估和提高基于反直觉和有趣视频的视频推理深度。与大多数侧重于不太惊讶的背景（例如烹饪或说明视频）的视频QA基准不同，FunQA涵盖了三种以前未被探索的类型的惊喜视频：1）HumorQA，2）CreativeQA和3）MagicQA。对于每个子集，我们建立了严格的QA任务，旨在评估模型在反直觉时间戳定位、详细视频描述以及围绕反直觉进行推理的能力。

    arXiv:2306.14899v2 Announce Type: replace-cross  Abstract: Surprising videos, such as funny clips, creative performances, or visual illusions, attract significant attention. Enjoyment of these videos is not simply a response to visual stimuli; rather, it hinges on the human capacity to understand (and appreciate) commonsense violations depicted in these videos. We introduce FunQA, a challenging video question-answering (QA) dataset specifically designed to evaluate and enhance the depth of video reasoning based on counter-intuitive and fun videos. Unlike most video QA benchmarks which focus on less surprising contexts, e.g., cooking or instructional videos, FunQA covers three previously unexplored types of surprising videos: 1) HumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous QA tasks designed to assess the model's capability in counter-intuitive timestamp localization, detailed video description, and reasoning around counter-intuitiveness. We also pose hi
    
[^150]: 确定正确的XAI方法--气候科学中可解释人工智能方法的评估和排序指南

    Finding the right XAI method -- A Guide for the Evaluation and Ranking of Explainable AI Methods in Climate Science

    [https://arxiv.org/abs/2303.00652](https://arxiv.org/abs/2303.00652)

    这项工作介绍了气候背景下的XAI评估，并讨论了不同的期望解释特性，为了评估和排序可解释人工智能方法在气候科学中的应用。

    

    可解释人工智能（XAI）方法揭示了机器学习算法的预测。存在几种不同的方法，已经应用于气候科学中。然而，通常缺少地面真实解释使他们的评估和比较变得复杂，进而阻碍了XAI方法的选择。因此，在这项工作中，我们介绍了气候背景下的XAI评估，并讨论了不同的期望解释特性，即稳健性、忠实性、随机性、复杂性和定位性。为此，我们选择以某一案例研究先前的工作，预测了十年的年均温度图。在训练了多层感知器（MLP）和卷积神经网络（CNN）之后，应用多种XAI方法，并计算它们在每个属性上与随机均匀解释的技能分数。独立于网络，我们发现XAI m

    arXiv:2303.00652v2 Announce Type: replace-cross  Abstract: Explainable artificial intelligence (XAI) methods shed light on the predictions of machine learning algorithms. Several different approaches exist and have already been applied in climate science. However, usually missing ground truth explanations complicate their evaluation and comparison, subsequently impeding the choice of the XAI method. Therefore, in this work, we introduce XAI evaluation in the climate context and discuss different desired explanation properties, namely robustness, faithfulness, randomization, complexity, and localization. To this end, we chose previous work as a case study where the decade of annual-mean temperature maps is predicted. After training both a multi-layer perceptron (MLP) and a convolutional neural network (CNN), multiple XAI methods are applied and their skill scores in reference to a random uniform explanation are calculated for each property. Independent of the network, we find that XAI m
    
[^151]: 具有原型的跨领域随机预训练用于强化学习

    Cross-domain Random Pre-training with Prototypes for Reinforcement Learning

    [https://arxiv.org/abs/2302.05614](https://arxiv.org/abs/2302.05614)

    提出了CRPTpro框架，利用原型进行跨领域自监督随机预训练，提高预训练效率，并实现在不同领域中定义的视觉控制RL任务。

    

    此工作已提交给IEEE进行可能的出版。 CRPTpro提出了一种用于基于图像的RL的跨领域自监督随机预训练框架，利用原型。 CRPTpro采用了跨领域随机策略，可以轻松快速地从多个领域中抽样多样化数据，以提高预训练效率。此外，通过提出一种新颖的内在损失进行原型表示学习，以在不同领域中预训练有效且通用的编码器。在没有微调的情况下，跨领域编码器可以高效地应用于不同领域中定义的具有挑战性的下游视觉控制RL任务。 与以前的方法如APT和Proto-RL相比，CRP

    arXiv:2302.05614v2 Announce Type: replace-cross  Abstract: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Task-agnostic cross-domain pre-training shows great potential in image-based Reinforcement Learning (RL) but poses a big challenge. In this paper, we propose CRPTpro, a Cross-domain self-supervised Random Pre-Training framework with prototypes for image-based RL. CRPTpro employs cross-domain random policy to easily and quickly sample diverse data from multiple domains, to improve pre-training efficiency. Moreover, prototypical representation learning with a novel intrinsic loss is proposed to pre-train an effective and generic encoder across different domains. Without finetuning, the cross-domain encoder can be implemented for challenging downstream visual-control RL tasks defined in different domains efficiently. Compared with prior arts like APT and Proto-RL, CRP
    
[^152]: 用推测对手模型进行决策

    Decision-making with Speculative Opponent Models

    [https://arxiv.org/abs/2211.11940](https://arxiv.org/abs/2211.11940)

    提出了一种使用纯粹局部信息实现推测对手建模的多智能体分布式演员-评论家算法，能够帮助受控代理做出决策。

    

    对手建模通过构建其他代理的模型，使受控代理的决策受益。现有方法通常假设可以访问对手的观察和行为，但当对手的行为不可观察或难以获得时，这是不可行的。我们提出了一种新颖的多智能体分布式演员-评论家算法，通过纯粹的局部信息（即受控代理的观察、行为和奖励）实现推测对手建模。具体而言，演员维持对对手的推测信念，我们称之为推测对手模型，以使用局部观察来预测对手的动作，并相应地做出决策。此外，分布式评论家模型政策的回报分布。它反映了演员的质量，因此可以指导演员所依赖的推测对手模型的训练。大量实验证实了我们的方法成功地...

    arXiv:2211.11940v2 Announce Type: replace  Abstract: Opponent modeling has benefited a controlled agent's decision-making by constructing models of other agents. Existing methods commonly assume access to opponents' observations and actions, which is infeasible when opponents' behaviors are unobservable or hard to obtain. We propose a novel multi-agent distributional actor-critic algorithm to achieve speculative opponent modeling with purely local information (i.e., the controlled agent's observations, actions, and rewards). Specifically, the actor maintains a speculated belief of the opponents, which we call the speculative opponent models, to predict opponent actions using local observations and makes decisions accordingly. Further, the distributional critic models the return distribution of the policy. It reflects the quality of the actor and thus can guide the training of the speculative opponent model that the actor relies on. Extensive experiments confirm that our method successf
    
[^153]: 基于相似性的标签推断攻击：针对分布式学习的训练和推理

    Similarity-based Label Inference Attack against Training and Inference of Split Learning

    [https://arxiv.org/abs/2203.05222](https://arxiv.org/abs/2203.05222)

    本文研究了在Split learning的训练和推理过程中基于相似性的标签推断攻击，提出了相似性度量并设计了三种标签推断攻击。

    

    Split learning是一种有望实现隐私保护的分布式学习范式。学习模型可以被切割成多个部分，在参与者之间进行协作训练，只交换切割层的中间结果。了解Split learning的安全性能对许多隐私敏感应用至关重要。本文表明，在Split learning的训练和推理过程中，交换的中间结果，包括破碎的数据（即从原始数据提取的特征）和梯度，已经可以透露出私密标签。我们对潜在的标签泄漏进行了数学分析，并针对梯度和破碎的数据提出了余弦相似度和欧氏相似度测量。然后，这两种相似度测量显示可以在欧氏空间中统一。基于相似性度量，我们设计了三种标签推断攻击，可以高效地恢复私密

    arXiv:2203.05222v2 Announce Type: replace-cross  Abstract: Split learning is a promising paradigm for privacy-preserving distributed learning. The learning model can be cut into multiple portions to be collaboratively trained at the participants by exchanging only the intermediate results at the cut layer. Understanding the security performance of split learning is critical for many privacy-sensitive applications. This paper shows that the exchanged intermediate results, including the smashed data (i.e., extracted features from the raw data) and gradients during training and inference of split learning, can already reveal the private labels. We mathematically analyze the potential label leakages and propose the cosine and Euclidean similarity measurements for gradients and smashed data, respectively. Then, the two similarity measurements are shown to be unified in Euclidean space. Based on the similarity metric, we design three label inference attacks to efficiently recover the private
    
[^154]: 关于组织病理学图像搜索的研究

    On Image Search in Histopathology. (arXiv:2401.08699v1 [eess.IV])

    [http://arxiv.org/abs/2401.08699](http://arxiv.org/abs/2401.08699)

    这篇论文综述了组织病理学图像搜索技术的最新发展，为计算病理学研究人员提供了简明的概述，旨在寻求有效、快速和高效的图像搜索方法。

    

    组织病理学的病理图像可以通过装有摄像头的显微镜或全扫描仪获取。利用相似性计算基于这些图像匹配患者，在研究和临床环境中具有重要潜力。最近搜索技术的进展使得可以对各种组织类型的细胞结构进行微妙的量化，促进比较，并在与诊断和治疗过的病例数据库进行比较时实现关于诊断、预后和新患者预测的推断。本文全面回顾了组织病理学图像搜索技术的最新发展，为计算病理学研究人员提供了简明的概述，以寻求有效、快速和高效的图像搜索方法。

    Pathology images of histopathology can be acquired from camera-mounted microscopes or whole slide scanners. Utilizing similarity calculations to match patients based on these images holds significant potential in research and clinical contexts. Recent advancements in search technologies allow for nuanced quantification of cellular structures across diverse tissue types, facilitating comparisons and enabling inferences about diagnosis, prognosis, and predictions for new patients when compared against a curated database of diagnosed and treated cases. In this paper, we comprehensively review the latest developments in image search technologies for histopathology, offering a concise overview tailored for computational pathology researchers seeking effective, fast and efficient image search methods in their work.
    
[^155]: 一个简单的方法在世界模型中实现新颖性检测

    A Simple Way to Incorporate Novelty Detection in World Models. (arXiv:2310.08731v1 [cs.AI])

    [http://arxiv.org/abs/2310.08731](http://arxiv.org/abs/2310.08731)

    本文提出了一个简单的方法，通过利用世界模型幻觉状态和真实观察状态的不匹配性作为异常分数，将新颖性检测纳入世界模型强化学习代理中。在新环境中与传统方法相比，我们的工作具有优势。

    

    使用世界模型进行强化学习已经取得了显著的成功。然而，当世界机制或属性发生突然变化时，代理的性能和可靠性可能会显著下降。我们将视觉属性或状态转换的突变称为“新颖性”。在生成的世界模型框架中实施新颖性检测是保护部署时代理的关键任务。在本文中，我们提出了一种简单的边界方法，用于将新颖性检测纳入世界模型强化学习代理中，通过利用世界模型幻觉状态和真实观察状态的不匹配性作为异常分数。首先，我们提供了与序列决策相关的新颖性检测本体论，然后我们提供了在代理在世界模型中学习的转换分布中检测新颖性的有效方法。最后，我们展示了我们的工作在新环境中与传统方法相比的优势。

    Reinforcement learning (RL) using world models has found significant recent successes. However, when a sudden change to world mechanics or properties occurs then agent performance and reliability can dramatically decline. We refer to the sudden change in visual properties or state transitions as {\em novelties}. Implementing novelty detection within generated world model frameworks is a crucial task for protecting the agent when deployed. In this paper, we propose straightforward bounding approaches to incorporate novelty detection into world model RL agents, by utilizing the misalignment of the world model's hallucinated states and the true observed states as an anomaly score. We first provide an ontology of novelty detection relevant to sequential decision making, then we provide effective approaches to detecting novelties in a distribution of transitions learned by an agent in a world model. Finally, we show the advantage of our work in a novel environment compared to traditional ma
    
[^156]: 从复杂到清晰：通过Clifford的几何代数和凸优化的分析表达深度神经网络的权重

    From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity. (arXiv:2309.16512v1 [cs.LG])

    [http://arxiv.org/abs/2309.16512](http://arxiv.org/abs/2309.16512)

    本文通过Clifford的几何代数和凸优化，提出了一种分析深度神经网络的新方法。我们展示了深度ReLU神经网络的最优权重可以通过训练样本的楔积来获得，并且训练问题可以简化为对楔积特征进行凸优化，从而揭示了神经网络内部的几何结构。

    

    本文介绍了一种基于几何（Clifford）代数和凸优化的神经网络分析方法。我们展示了当使用标准正则化损失进行训练时，深度ReLU神经网络的最优权重由训练样本的楔积给出。此外，训练问题可简化为对楔积特征进行凸优化，在其中编码训练数据集的几何结构。该结构以数据向量生成的三角形和平行体的有符号体积表示。凸问题通过$\ell_1$正则化找到样本的一个小子集，以发现仅相关的楔积特征。我们的分析提供了对深度神经网络内部工作机制的新视角，并揭示了隐藏层的作用。

    In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford) algebra and convex optimization. We show that optimal weights of deep ReLU neural networks are given by the wedge product of training samples when trained with standard regularized loss. Furthermore, the training problem reduces to convex optimization over wedge product features, which encode the geometric structure of the training dataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by data vectors. The convex problem finds a small subset of samples via $\ell_1$ regularization to discover only relevant wedge product features. Our analysis provides a novel perspective on the inner workings of deep neural networks and sheds light on the role of the hidden layers.
    
[^157]: LLMR：使用大型语言模型实时提示交互式世界的框架

    LLMR: Real-time Prompting of Interactive Worlds using Large Language Models. (arXiv:2309.12276v1 [cs.HC])

    [http://arxiv.org/abs/2309.12276](http://arxiv.org/abs/2309.12276)

    LLMR是一个用于实时创建和修改交互式混合现实体验的框架，通过利用大型语言模型和新颖的策略，它能够解决训练数据稀缺和设计目标复杂的问题，并在性能上超过标准的GPT-4。我们展示了LLMR的跨平台互操作性，并通过评估和用户研究证明了其对于生成和编辑各种对象、工具和场景的能力。

    

    我们提出了用于混合现实场景的大型语言模型(LLMR)，这是一个框架，用于实时创建和修改交互式混合现实体验。LLMR利用了新颖的策略来解决训练数据稀缺或设计目标需要合成内部动态、直观分析或高级交互的困难情况。我们的框架依赖于文本交互和Unity游戏引擎。通过融合场景理解、任务规划、自我调试和内存管理技术，LLMR在平均错误率上比标准的GPT-4提高了4倍。我们展示了LLMR与几个示例世界的跨平台互操作性，并通过多个创建和修改任务对其进行了评估，以展示它能够生成和编辑各种对象、工具和场景。最后，我们进行了一个有多样性的可用性研究（N=11），揭示了参与者对该系统有积极的体验，并愿意再次使用它。

    We present Large Language Model for Mixed Reality (LLMR), a framework for the real-time creation and modification of interactive Mixed Reality experiences using LLMs. LLMR leverages novel strategies to tackle difficult cases where ideal training data is scarce, or where the design goal requires the synthesis of internal dynamics, intuitive analysis, or advanced interactivity. Our framework relies on text interaction and the Unity game engine. By incorporating techniques for scene understanding, task planning, self-debugging, and memory management, LLMR outperforms the standard GPT-4 by 4x in average error rate. We demonstrate LLMR's cross-platform interoperability with several example worlds, and evaluate it on a variety of creation and modification tasks to show that it can produce and edit diverse objects, tools, and scenes. Finally, we conducted a usability study (N=11) with a diverse set that revealed participants had positive experiences with the system and would use it again.
    
[^158]: MSAC：用于语音情感识别的多语音属性控制方法

    MSAC: Multiple Speech Attribute Control Method for Speech Emotion Recognition. (arXiv:2308.04025v1 [cs.SD])

    [http://arxiv.org/abs/2308.04025](http://arxiv.org/abs/2308.04025)

    本研究针对语音情感识别(SER)提出了MSAC方法，通过构建新颖的CNN-based SER模型和多语音属性控制方法MSAC，实现了对情感的更精细控制和捕捉，从而提升了SER的可靠性和效果。

    

    尽管取得了显著进展，但由于情感属性的复杂性和歧义性，尤其是在自然环境下，语音情感识别（SER）仍然具有挑战性。而当前的研究主要关注识别和泛化能力，本文首次探索了SER方法的可靠性，并研究了如何通过各种语音属性的数据分布来建模语音情感。具体来说，我们首先构建了一种新颖的基于CNN的SER模型，采用了加性边界最大化软件最大化损失函数，扩大了不同类别特征之间的距离，从而增强了它们的区分能力。其次，我们提出了一种新颖的多语音属性控制方法MSAC，以明确控制语音属性，使模型受情感无关属性的影响较小，并捕捉到更细粒度的情感相关特征。第三，我们首次尝试测试和分析了所提出的SER工作流程的可靠性。

    Despite significant progress, speech emotion recognition (SER) remains challenging due to inherent complexity and ambiguity of the emotion attribute, particularly in wild world. Whereas current studies primarily focus on recognition and generalization capabilities, this work pioneers an exploration into the reliability of SER methods and investigates how to model the speech emotion from the aspect of data distribution across various speech attributes. Specifically, we first build a novel CNN-based SER model which adopts additive margin softmax loss to expand the distance between features of different classes, thereby enhancing their discrimination. Second, a novel multiple speech attribute control method MSAC is proposed to explicitly control speech attributes, enabling the model to be less affected by emotion-agnostic attributes and capture more fine-grained emotion-related features. Third, we make a first attempt to test and analyze the reliability of the proposed SER workflow using 
    
[^159]: 基于GCN可信度预测的协同移动群感知的高效招募策略

    Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing Based on GCN Trustworthiness Prediction. (arXiv:2306.04366v1 [cs.SI])

    [http://arxiv.org/abs/2306.04366](http://arxiv.org/abs/2306.04366)

    本文提出了一种基于GCN可信度预测的协同移动群感知的高效招募策略，通过捕获工人之间的非对称信任关系和工人能力来实现有效的任务分配，优于现有方法。

    

    协同移动群感知可以通过促进任务感知的团队合作来提高数据质量和覆盖范围，而工人招募则代表着一个复杂的多目标优化问题。现有策略主要关注工人本身的特征，忽略了工人之间的非对称信任关系，从而影响了任务效用评估的合理性。为解决这个问题，本文首先使用Mini-Batch K-Means聚类算法和边缘服务器来实现高效的分布式工人招募。利用历史数据和任务要求获得工人的能力类型和距离。使用工人社交网络中的信任导向图输入至图卷积网络（GCN）框架进行训练，捕获工人之间的非对称信任关系。通过工人之间的高信任值，防止CMCS场景下的隐私泄露。最终，利用预测的信任和工人能力构建了一个无向招募图，以实现有效的任务分配。实验结果表明，与现有方法相比，这种招募方法在招募准确度、任务完成时间和能量消耗方面表现优异。

    Collaborative Mobile Crowd Sensing (CMCS) enhances data quality and coverage by promoting teamwork in task sensing, with worker recruitment representing a complex multi-objective optimization problem. Existing strategies mainly focus on the characteristics of workers themselves, neglecting the asymmetric trust relationships between them, which affects the rationality of task utility evaluation. To address this, this paper first employs the Mini-Batch K-Means clustering algorithm and deploys edge servers to enable efficient distributed worker recruitment. Historical data and task requirements are utilized to obtain workers' ability types and distances. A trust-directed graph in the worker's social network is input into the Graph Convolutional Network (GCN) framework for training, capturing asymmetric trustworthiness between worker pairs. Privacy leakage is prevented in CMCS scenarios through high trust values between workers. Ultimately, an undirected recruitment graph is constructed us
    
[^160]: 大数据时代的地球移动者: 最优输运在机器学习中的回顾

    Earth Movers in The Big Data Era: A Review of Optimal Transport in Machine Learning. (arXiv:2305.05080v1 [cs.LG])

    [http://arxiv.org/abs/2305.05080](http://arxiv.org/abs/2305.05080)

    本文回顾了最优输运在机器学习中的应用，并探讨了如何将其扩展以适应大数据和高维数据的需求。

    

    最优输运(OT)是一个数学框架,首次出现于18世纪,并引发出大量方法来回答许多理论和应用问题。过去的十年见证了这个经典优化问题对机器学习的显着贡献。本文探讨了最优输运在机器学习中的使用方式及其扩展的问题。在专题与背景的允许下,我们提供了关于最优输运的全面调查,并确保其呈现具有可访问性。首先,我们解释了最优输运的背景,并介绍了不同的类型、特性和显著应用。然后,我们着重探讨了如何将最优输运扩展以应对当前大数据和高维数据的需求。我们对用于扩展OT的文献方法进行了系统分析,并以结构化的方式呈现结果以促进理解。最后,我们探讨了可扩展最优输运在机器学习中未来研究的一些最有前途的方向。

    Optimal Transport (OT) is a mathematical framework that first emerged in the eighteenth century and has led to a plethora of methods for answering many theoretical and applied questions. The last decade is a witness of the remarkable contributions of this classical optimization problem to machine learning. This paper is about where and how optimal transport is used in machine learning with a focus on the question of salable optimal transport. We provide a comprehensive survey of optimal transport while ensuring an accessible presentation as permitted by the nature of the topic and the context. First, we explain optimal transport background and introduce different flavors (i.e. mathematical formulations), properties, and notable applications. We then address the fundamental question of how to scale optimal transport to cope with the current demands of big and high dimensional data. We conduct a systematic analysis of the methods used in the literature for scaling OT and present the find
    
[^161]: Merlin-Arthur分类器的形式可解释性

    Formal Interpretability with Merlin-Arthur Classifiers. (arXiv:2206.00759v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00759](http://arxiv.org/abs/2206.00759)

    该论文提出了一种新型的多智能体交互分类器，利用“Merlin-Arthur”协议的启发，在不假设最优智能体或特征独立分布的情况下，通过相对强度和“非对称特征相关性”概念捕捉特征之间精确的相关性，提供可证明的可解释性保证。

    

    我们提出了一种新类型的多智能体交互分类器，即使是像神经网络这样的复杂智能体也能提供可证明的可解释性保证。这些保证包括对此分类器选择的特征之间互信息的上下界约束。我们的结果受交互式证明系统中 Merlin-Arthur 协议的启发，并以可测量的指标（如声音和完整性）表达了这些约束。与现有的交互式设置相比，我们不依赖于最优智能体或特征独立分布的假设。相反，我们利用智能体的相对强度以及新的“非对称特征相关性”概念来捕捉使可解释性保证困难的精确相关性类型。 我们通过两个小规模数据集的数值实验来测试我们的结果，这些实验可验证高互信息性。

    We propose a new type of multi-agent interactive classifier that provides provable interpretability guarantees even for complex agents such as neural networks. These guarantees consist of bounds on the mutual information of the features selected by this classifier. Our results are inspired by the Merlin-Arthur protocol from Interactive Proof Systems and express these bounds in terms of measurable metrics such as soundness and completeness. Compared to existing interactive setups we do not rely on optimal agents or on the assumption that features are distributed independently. Instead, we use the relative strength of the agents as well as the new concept of Asymmetric Feature Correlation which captures the precise kind of correlations that make interpretability guarantees difficult. %relates the information carried by sets of features to one of the individual features. We test our results through numerical experiments on two small-scale datasets where high mutual information can be veri
    

