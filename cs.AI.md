# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance.](http://arxiv.org/abs/2304.06715) | 本文提出了解释不变性和等变性的概念，通过对对称群下具有不变性的神经网络，建立两种度量方法来提高解释方法对于不变性的健壮性并证明为一些流行的解释方法提供了理论健壮性保证。 |
| [^2] | [Verbs in Action: Improving verb understanding in video-language models.](http://arxiv.org/abs/2304.06708) | 本文提出了一个新的动词聚焦对比框架，通过利用预训练的大型语言模型和执行细粒度的动词短语对齐损失来改善基于CLIP的视频语言模型的动词理解能力，实现了在三个聚焦于动词理解的下游任务的零样本性能最先进的结果。 |
| [^3] | [How Will It Drape Like? Capturing Fabric Mechanics from Depth Images.](http://arxiv.org/abs/2304.06704) | 该论文提出了一种使用深度相机进行随意捕捉的方法来预测织物的自由落体效应，并创新性地提出了一种模拟到真实的策略来训练学习框架，该框架可以输出完整的力学参数集。 |
| [^4] | [Learning Personalized Decision Support Policies.](http://arxiv.org/abs/2304.06701) | 本文提出了一种学习个性化决策支持策略的算法 $\texttt{THREAD}$，可以为决策者提供不同形式的支持。同时，引入了 $\texttt{Modiste}$ 工具来提供个性化的医学诊断决策支持，使用 $\texttt{THREAD}$ 学习个性化决策支持策略，有效提高了预期的诊断正确性，并减少了严重并发症的风险，同时推荐了更少和更便宜的研究。 |
| [^5] | [LSFSL: Leveraging Shape Information in Few-shot Learning.](http://arxiv.org/abs/2304.06672) | LSFSL是一种少样本学习方法，利用数据中的隐含先验信息强制模型学习更具有泛化性的特征。通过全面分析，证明该方法提高了模型的鲁棒性和泛化性。 |
| [^6] | [Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation.](http://arxiv.org/abs/2304.06671) | 本文提出了布局引导下图像生成的诊断基准LayoutBench，对数量、位置、大小和形状四种空间控制技能进行了研究，发现好的ID布局控制在任意布局的野外环境下可能不具有良好的推广性。接着，我们提出了一种新的基准方法IterInpaint通过修复逐步生成前景和背景区域，显现出在OOD布局方面更强的通用性。 |
| [^7] | [Do deep neural networks have an inbuilt Occam's razor?.](http://arxiv.org/abs/2304.06670) | 该研究利用基于函数先验的贝叶斯视角来研究深度神经网络（DNNs）的表现来源，结果表明DNNs之所以成功，是因为它对于具有结构的数据，具备一种内在的奥卡姆剃刀式的归纳偏差，足以抵消函数数量及复杂度的指数级增长。 |
| [^8] | [How Useful are Educational Questions Generated by Large Language Models?.](http://arxiv.org/abs/2304.06638) | 本文研究通过结合CTG和问题分类生成的输出，通过教师评估证明这些生成的问题质量高且足够有用，有极大的应用潜力。 |
| [^9] | [PGTask: Introducing the Task of Profile Generation from Dialogues.](http://arxiv.org/abs/2304.06634) | 对话系统的个性化需要个人资料信息，而从对话中提取/生成个人资料信息是一项基本需求。为此，我们提出了档案生成任务（PGTask）并提供了相关的数据集和基准，该任务使得研究者可以更好地了解档案生成任务的挑战和可能的解决方案。 |
| [^10] | [AI-Generated Content (AIGC): A Survey.](http://arxiv.org/abs/2304.06632) | 人工智能生成的内容（AIGC）是一种有前途的生成工具，使用人工智能根据用户输入的关键词或需求生成内容，并为我们的生活带来便捷。本文调研并广泛概述了AIGC的定义、重要条件、尖端能力、高级功能、大规模预训练模型的优势和产业链。 |
| [^11] | [False Claims against Model Ownership Resolution.](http://arxiv.org/abs/2304.06607) | 该论文研究了模型所有权解决方案中对抗恶意原告的鲁棒性问题，展示了常见的MOR方案可以被恶意原告针对未被盗用的独立模型提出虚假指控。 |
| [^12] | [Beyond Submodularity: A Unified Framework of Randomized Set Selection with Group Fairness Constraints.](http://arxiv.org/abs/2304.06596) | 该论文介绍了一种带有组公平性约束的随机子集选择的框架，并提出了一个全新的优化方法，可以优化一系列超出次模性范围的问题。在测试中，该框架可以实现组公平性而不牺牲太多效用。 |
| [^13] | [ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning.](http://arxiv.org/abs/2304.06588) | 本文评估了ChatGPT-4在政治Twitter相关推文注释任务中的表现，结果显示它在准确性、可靠性和偏见方面皆优于人类标注，尤其是它能通过零样本学习准确注释需要基于上下文知识和作者意向的推文，这将使得规模化的文本数据研究成为可能，并在社会科学的解释性研究中产生巨大影响。 |
| [^14] | [Power-seeking can be probable and predictive for trained agents.](http://arxiv.org/abs/2304.06528) | 在简化假设下，训练奖励一致的目标集合中的智能体仍有可能追求权力，具有预测性。 |
| [^15] | [An Efficient Transfer Learning-based Approach for Apple Leaf Disease Classification.](http://arxiv.org/abs/2304.06520) | 本研究提出了一种基于转移学习的苹果叶病识别技术，通过预训练的EfficientNetV2S架构提取特征并结合分类器块进行预测。该研究解决了类不平衡的问题，对各种超参数进行了仔细调查，为苹果树农提供了更加快速高效的叶病分类解决方案。 |
| [^16] | [Secure Federated Learning for Cognitive Radio Sensing.](http://arxiv.org/abs/2304.06519) | 本文探讨了在认知无线电环境下基于联邦学习实现可靠和安全的频谱感知（SS），深入分析了FL在SS中的动机、架构和算法，同时提供了安全和隐私威胁的对抗措施。 |
| [^17] | [Passive Radio Frequency-based 3D Indoor Positioning System via Ensemble Learning.](http://arxiv.org/abs/2304.06513) | 本文提出了一种基于无源射频的三维室内定位系统，可以捕捉情景特征并使用集成学习来提高定位精度。 |
| [^18] | [Philosophical Foundations of GeoAI: Exploring Sustainability, Diversity, and Bias in GeoAI and Spatial Data Science.](http://arxiv.org/abs/2304.06508) | 本文介绍了GeoAI和空间数据科学的哲学基础，分别从可持续性、训练数据中的偏差、模式知识的多样性和系统中的中立性缺失等角度出发，为我们设计、培训和部署基于GeoAI的系统提供了帮助，也为我们理解人工智能和机器学习研究的利益和潜在危险提供了共同理解。 |
| [^19] | [DiaTrend: A dataset from advanced diabetes technology to enable development of novel analytic solutions.](http://arxiv.org/abs/2304.06506) | DiaTrend 数据集由 54 位糖尿病患者穿戴式医疗设备产生的数据组成，包括总计27,561天的连续血糖监测数据和8,220天的胰岛素泵数据。该数据集对于开发新型的分析解决方案非常有用。 |
| [^20] | [Quantitative study about the estimated impact of the AI Act.](http://arxiv.org/abs/2304.06503) | 本文研究了欧盟提出的AI法案对AI系统的影响，通过对德国的AI产品和项目进行分类，得出了该法案的具体要求和限制，为评估该法案在AI领域的实际影响提供了系统方法。 |
| [^21] | [Domain Adaptation for Inertial Measurement Unit-based Human Activity Recognition: A Survey.](http://arxiv.org/abs/2304.06489) | 本文综述了惯性测量单元人体活动识别中利用领域自适应技术解决数据分布异质性问题的最新进展。 |
| [^22] | [One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era.](http://arxiv.org/abs/2304.06488) | ChatGPT是生成式AI的一小步，也是AGI的一大步，我们进行了综述，并对其如何演变为AIGC展望不同于以往的通用AI生成内容的发展道路。 |
| [^23] | [CoRe-Sleep: A Multimodal Fusion Framework for Time Series Robust to Imperfect Modalities.](http://arxiv.org/abs/2304.06485) | CoRe-Sleep是一种多模态融合框架，特别关注于提高对不完善数据的信号分析的鲁棒性，它通过适当处理多模态信息，容忍噪声或丢失的模态片段，展现最先进的性能。 |
| [^24] | [Qualitative Failures of Image Generation Models and Their Application in Detecting Deepfakes.](http://arxiv.org/abs/2304.06470) | 研究调查了图像生成模型的质量失误及其应用于检测Deepfakes，识别了五种定性缺陷。这些发现有助于改进模型并制定检测Deepfakes的策略。 |
| [^25] | [Analysing Fairness of Privacy-Utility Mobility Models.](http://arxiv.org/abs/2304.06469) | 本研究为人类移动性定义了一组公平度量指标，基于轨迹的结构相似性和熵，研究了两种依靠生成对抗网络和表示学习降低用户的重新识别率的隐私保护模型，并探讨了它们的公平性优势和差异。 |
| [^26] | [Masakhane-Afrisenti at SemEval-2023 Task 12: Sentiment Analysis using Afro-centric Language Models and Adapters for Low-resource African Languages.](http://arxiv.org/abs/2304.06459) | 该论文介绍了在SemEval-2023任务12中使用Afro-centric语言模型和适配器进行非洲低资源语言的情感分析。使用预训练的Afro-centric语言模型可以提高性能。使用适配器方法可以实现对于有限资源语言的零样本迁移。 |
| [^27] | [SpectFormer: Frequency and Attention is what you need in a Vision Transformer.](http://arxiv.org/abs/2304.06446) | 本文提出了结合多头注意力和谱层的Spectformer架构，可以得到更好的性能表现，提高了top-1准确率2%。 |
| [^28] | [Certified Zeroth-order Black-Box Defense with Robust UNet Denoiser.](http://arxiv.org/abs/2304.06430) | 本文提出了一种带有鲁棒UNet去噪器的认证零阶黑盒防御方法，通过在黑盒模型之前预置RDUNet和DS或AE和RDUNet，成功提高了模型鲁棒性。 |
| [^29] | [In-Distribution and Out-of-Distribution Self-supervised ECG Representation Learning for Arrhythmia Detection.](http://arxiv.org/abs/2304.06427) | 本文系统研究了自监督学习方法在ECG表征学习上的应用，首次对三个常用ECG心律失常数据集进行了分布分析，实验发现SwAV方法表现最佳，能够超越传统的有监督学习方法，还具有较强的鲁棒性，有望在大规模和多样化人群中检测心律失常。 |
| [^30] | [Quantifying and Explaining Machine Learning Uncertainty in Predictive Process Monitoring: An Operations Research Perspective.](http://arxiv.org/abs/2304.06412) | 本论文提出了一种综合的、多阶段的机器学习方法，利用分位数回归森林生成区间预测来解决预测流程监控中的问题，同时使用SHapley可加解释来解释模型不确定性的来源。 |
| [^31] | [Meta-Auxiliary Learning for Adaptive Human Pose Prediction.](http://arxiv.org/abs/2304.06411) | 本文介绍了一种名为元辅助学习的方法，该方法利用自监视辅助任务帮助主要预测网络适应测试序列，以提高生成质量，特别是在罕见或未见运动类别的情况下。 |
| [^32] | [Leveraging triplet loss for unsupervised action segmentation.](http://arxiv.org/abs/2304.06403) | 本文提出了一种无监督的框架，可以在不需要任何训练数据的情况下，从单个输入视频中学习适用于动作分割任务的动作表示，并使用三元组选择策略和三元组损失来在新的表示空间中发现动作，相对于现有无监督方法实现了更好的时间边界恢复质量。 |
| [^33] | [VISION DIFFMASK: Faithful Interpretation of Vision Transformers with Differentiable Patch Masking.](http://arxiv.org/abs/2304.06391) | VISION DIFFMASK提出了一种可解释性的方法，通过使用门控机制识别最小输入子集来预测对其最终预测有贡献的输入部分。 |
| [^34] | [Emergence of Symbols in Neural Networks for Semantic Understanding and Communication.](http://arxiv.org/abs/2304.06377) | 本文介绍了一种名为SEA-net的神经网络解决方案，可以生成符号，实现语义理解和交流。这些符号可以捕捉到组成性语义信息，并呈现类似自然语言的内在结构。 |
| [^35] | [Towards hypergraph cognitive networks as feature-rich models of knowledge.](http://arxiv.org/abs/2304.06375) | 本研究提出了特征丰富认知超图作为人类记忆定量模型来预测概念特征，与成对连接和缺乏特征的网络模型相比，特征丰富认知超图表现更好，并且涉及更高阶关联的超链接在具有相似心理语言学特征的概念之间优先形成，这反映了共享认知维度的存在。 |
| [^36] | [AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models.](http://arxiv.org/abs/2304.06364) | AGIEval是一个以人为中心设计的基准测试工具，用于评估基础模型在人类中心标准化考试上的表现。GPT-4在SAT、LSAT和数学比赛方面超越了人类平均表现，展示了当代基础模型在人类级任务中的非凡性能。 |
| [^37] | [Decidability of Querying First-Order Theories via Countermodels of Finite Width.](http://arxiv.org/abs/2304.06348) | 通过有限宽度的反模型查询一阶理论的可决定性并提出分割宽度，使其能够捕获实际相关的查询语言 |
| [^38] | [DDT: Dual-branch Deformable Transformer for Image Denoising.](http://arxiv.org/abs/2304.06346) | 本文提出了一种高效的双支变形Transformer（DDT）去噪网络，可以同时捕捉局部和全局交互，并且应用变形注意操作可以专注于更重要的区域，最终在实际和合成去噪任务上取得了最先进的性能以及显著降低的计算成本。 |
| [^39] | [ASR: Attention-alike Structural Re-parameterization.](http://arxiv.org/abs/2304.06345) | 该论文提出的ASR技术是一种新颖的深度学习技术，通过等效参数转换实现不同网络体系结构之间的互转。和现有的SRP方法相比，ASR可以成功考虑自注意模块，实现推理期间的性能提升，并在工业和实际应用中具有巨大潜力。 |
| [^40] | [Streamlined Framework for Agile Forecasting Model Development towards Efficient Inventory Management.](http://arxiv.org/abs/2304.06344) | 本文提出了一个敏捷预测模型开发的框架，通过简化核心组件之间的连接使得新数据集能够快速和稳健地集成，并选择最佳模型。通过不同的评估指标，找到最适合不同应用的模型。该框架能够应用在库存管理环境中，提高效率。 |
| [^41] | [Attributed Multi-order Graph Convolutional Network for Heterogeneous Graphs.](http://arxiv.org/abs/2304.06336) | 本文提出了一个AMOGCN模型，它自动从多阶邻接矩阵的自适应聚合中研究包含多跳邻居的元路径，并使用节点属性评价监督。其能够有效地从异构图中发现有区别的节点嵌入和关系。 |
| [^42] | [NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds.](http://arxiv.org/abs/2304.06287) | NeRFVS是一种利用神经重建的“全局信息”，包括伪深度图和视角覆盖信息，基于几何支撑的神经辐射场方法。该方法在实现室内自由导航方面表现出色且可减少可见的伪影。 |
| [^43] | [Model-based Dynamic Shielding for Safe and Efficient Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2304.06281) | 本论文介绍了模型驱动的动态屏蔽设计用于多智能体强化学习中的安全保障，屏蔽器能够动态分割、合并和重新计算智能体状态，同时支持更加高效的合成屏蔽器以监控复杂环境中的智能体。 |
| [^44] | [Optimizing Multi-Domain Performance with Active Learning-based Improvement Strategies.](http://arxiv.org/abs/2304.06277) | 本文提出了一个基于主动学习的框架，用于改善多领域性能。该方法分为两个阶段，在保证高效性的前提下实现了最先进的性能。 |
| [^45] | [Visual based Tomato Size Measurement System for an Indoor Farming Environment.](http://arxiv.org/abs/2304.06177) | 本文提出了一种基于机器学习模型和深度图像的番茄尺寸测量方法，该方法能够解决现有视觉系统在果园环境下遇到的遮挡和可扩展性问题，经过实验室测试取得了较高的测量精度。 |
| [^46] | [Neural Network Algorithm for Intercepting Targets Moving Along Known Trajectories by a Dubins' Car.](http://arxiv.org/abs/2304.06169) | 本文使用无监督学习的神经网络方法，基于深度确定性策略梯度算法，解决了 Dubins'小车沿已知轨迹拦截目标的最短时间控制问题， 建立了数学模型并进行了模型实验，证明了其有效性。 |
| [^47] | [AGI for Agriculture.](http://arxiv.org/abs/2304.06136) | 本文探讨了农业领域应用AGI的潜在机会，包括提高农产品产量、减少浪费和促进可持续农业实践，以及利用实时数据帮助农民做出更明智的决策。 |
| [^48] | [Analyzing ChatGPT's Aptitude in an Introductory Computer Engineering Course.](http://arxiv.org/abs/2304.06122) | 本文研究探讨了ChatGPT在计算机工程入门课程中的应用，发现它可以回答普通概念的问题，但无法处理带有图表或图形的问题，也无法进行实验室的现场操作。 |
| [^49] | [IoT trust and reputation: a survey and taxonomy.](http://arxiv.org/abs/2304.06119) | 本文通过对现有文献进行梳理，提出了一份物联网信任和声誉的调查和分类，分析了现有模型和机制的优点和缺点，提出了该领域的挑战和开放问题，并为未来的研究提供了指导方向。 |
| [^50] | [AutoShot: A Short Video Dataset and State-of-the-Art Shot Boundary Detection.](http://arxiv.org/abs/2304.06116) | AutoShot 发布了一份新的短视频镜头边界检测数据集，采用名为 AutoShot 的方法进行模型设计优化，比之前的最新技术水平在 F1 分数上获得更高的准确性。 |
| [^51] | [Generation of artificial facial drug abuse images using Deep De-identified anonymous Dataset augmentation through Genetics Algorithm (3DG-GA).](http://arxiv.org/abs/2304.06106) | 本研究采用3DG-GA方法生成逼真的药物滥用面部特征的人工合成图像数据集，该数据集将对生物医学和人工智能研究产生影响。 |
| [^52] | [Exploiting Intrinsic Stochasticity of Real-Time Simulation to Facilitate Robust Reinforcement Learning for Robot Manipulation.](http://arxiv.org/abs/2304.06056) | 本文研究了实时模拟的内在随机性特性及其在强化学习中的应用，旨在提高RL方法鲁棒性和域随机化性能。 |
| [^53] | [Exploiting Symmetry and Heuristic Demonstrations in Off-policy Reinforcement Learning for Robotic Manipulation.](http://arxiv.org/abs/2304.06055) | 本文提出了一个离线强化学习方法，该方法利用对称环境中的专家演示来进行机器人操作的策略训练，从而提高了学习效率和样本效率。 |
| [^54] | [Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation.](http://arxiv.org/abs/2304.06051) | Open-TransMind是智能交通领域第一个基础模型挑战赛的新基准，旨在解决数据量少、泛化能力差以及缺乏多模态技术等典型挑战。 |
| [^55] | [RELS-DQN: A Robust and Efficient Local Search Framework for Combinatorial Optimization.](http://arxiv.org/abs/2304.06048) | 该论文提出了一个名为RELS-DQN的轻量级DQN框架，可以展现局部搜索行为并提供实用的可扩展性，其在多个应用程序上具有类似局部搜索算法的效果，并且解决方案值要高于或等于局部搜索算法和专家设计的启发式方法。 |
| [^56] | [Learning solution of nonlinear constitutive material models using physics-informed neural networks: COMM-PINN.](http://arxiv.org/abs/2304.06044) | 通过物理训练的神经网络可解决非线性材料行为的本构关系，无需初始数据，避免重复的牛顿迭代。训练好的模型可作为有限元程序的用户定义材料模型，但需要解决诸多挑战。 |
| [^57] | [Deep Learning Systems for Advanced Driving Assistance.](http://arxiv.org/abs/2304.06041) | 本论文提出了一种智能驾驶辅助的深度学习系统，利用生物传感系统来重构汽车驾驶员的生理注意力状态，该探针通过深度学习系统分析和处理获得的PPG信号，以识别与驾驶员注意力水平相一致的特定模式。 |
| [^58] | [Social Biases through the Text-to-Image Generation Lens.](http://arxiv.org/abs/2304.06034) | 本文研究了文本到图像生成技术中存在的社会偏见，主要集中在职业、人格特征和日常情境等方面。实验证明，这些模型存在排除特定人群的职业偏见。 |
| [^59] | [Fairness: from the ethical principle to the practice of Machine Learning development as an ongoing agreement with stakeholders.](http://arxiv.org/abs/2304.06031) | 本文提出了一种支持伦理的迭代过程，旨在通过机器学习设计中的持续协议挑战不对称的权力动态，并支持团队在各个步骤中识别、减轻和监测偏差。 |
| [^60] | [LMR: Lane Distance-Based Metric for Trajectory Prediction.](http://arxiv.org/abs/2304.05869) | LMR是一种基于车道距离的度量，适用于轨迹预测中的结构化环境，相比传统的欧氏距离度量更准确。 |
| [^61] | [Learning Homographic Disambiguation Representation for Neural Machine Translation.](http://arxiv.org/abs/2304.05860) | 本文提出了一种利用同义词句子建立同形异义词词级消歧表示（HDR）以改进神经机器翻译的方法。 |
| [^62] | [Proximity Forest 2.0: A new effective and scalable similarity-based classifier for time series.](http://arxiv.org/abs/2304.05800) | Proximity Forest 2.0是一种新的有效且可扩展的基于相似性的时间序列分类器，优于先前最先进的基于相似性的分类器以及最先进的基于内核、神经网络和混合方法在特定数据集上的表现。 |
| [^63] | [Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding.](http://arxiv.org/abs/2304.05368) | 本研究全面评估了大型语言模型在临床语言理解任务上的表现，并引入自问自答提示策略来提高LLMs在医疗保健相关任务中的效果。 |
| [^64] | [Astroformer: More Data Might Not be All You Need for Classification.](http://arxiv.org/abs/2304.05350) | 该文提出了使用混合变换器 - 卷积架构的方法，结合新的堆栈设计、不同的相对自我注意层创建方式和精心选择的数据增强和正则化技术，从少量数据中学习，将此方法应用于Galaxy Zoo数据集，结果表明在少量数据的情况下取得了与以前方法相同的分类结果，并且不会损失性能。 |
| [^65] | [Regulatory Markets: The Future of AI Governance.](http://arxiv.org/abs/2304.04914) | 提出一种监管市场的概念，即政府要求受监管对象从私人监管机构购买监管服务，以克服过度依赖行业自律和立法机构缺乏专业知识的局限性，从而逐步实现人工智能的恰当监管。 |
| [^66] | [Probably Approximately Correct Federated Learning.](http://arxiv.org/abs/2304.04641) | 本文提出了FedPAC框架，利用PAC学习理论推导出一个解析解，可以保证FL之间隐私、效用和效率的最佳权衡。 |
| [^67] | [OpenAGI: When LLM Meets Domain Experts.](http://arxiv.org/abs/2304.04370) | 基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。 |
| [^68] | [Video ChatCaptioner: Towards the Enriched Spatiotemporal Descriptions.](http://arxiv.org/abs/2304.04227) | Video ChatCaptioner是一种创新方法，利用ChatGPT和算法生成全面和丰富的时空视频描述。 |
| [^69] | [EGC: Image Generation and Classification via a Single Energy-Based Model.](http://arxiv.org/abs/2304.02012) | EGC是一种使用单个神经网络在图像分类和图像生成任务中实现卓越性能的方法，可以较好地生成出高质量图像，并在多项数据集上实现了领先的分类结果。 |
| [^70] | [InfluencerRank: Discovering Effective Influencers via Graph Convolutional Attentive Recurrent Neural Networks.](http://arxiv.org/abs/2304.01897) | 本文提出了InfluencerRank，通过图卷积神经网络和注意力循环神经网络结合的方法评估影响者的有效性，以帮助企业在社交影响者营销中寻找合适的影响者。 |
| [^71] | [Regularization of the policy updates for stabilizing Mean Field Games.](http://arxiv.org/abs/2304.01547) | 本文提出了一种均场近端策略优化算法（MF-PPO），以稳定深度强化学习在均场博弈中的应用，并在OpenSpiel框架中进行了实验验证。 |
| [^72] | [Reducing Discretization Error in the Frank-Wolfe Method.](http://arxiv.org/abs/2304.01432) | 本论文提出了两个改进方法：一个多步的Frank-Wolfe方法，直接应用优化的高阶离散化方案；以及一种具有较少离散化误差的LMO-平均方案，其收敛速率加速到$O(1/k^{3/2})$，从而更好地解决了Frank-Wolfe方法中的离散化误差问题。 |
| [^73] | [Using AI to Measure Parkinson's Disease Severity at Home.](http://arxiv.org/abs/2303.17573) | 该论文提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法，该方法可重复用于类似的运动任务，拥有较高的可靠性和准确性。 |
| [^74] | [Vibration Signal Denoising Using Deep Learning.](http://arxiv.org/abs/2303.11413) | 本文研究了基于深度学习的去除脚步引起的振动信号的噪声的方法，该方法适用于高斯噪声和非平稳噪声。 |
| [^75] | [ChatGPT and Other Large Language Models as Evolutionary Engines for Online Interactive Collaborative Game Design.](http://arxiv.org/abs/2303.02155) | 本论文提出了一种新的结合交互式进化和大型语言模型的协作游戏设计框架，用于模拟典型的人类设计过程，并用大型语言模型进行非常复杂的创造性任务－思想的重组和变异。 |
| [^76] | ["An Adapt-or-Die Type of Situation": Perception, Adoption, and Use of Text-To-Image-Generation AI by Game Industry Professionals.](http://arxiv.org/abs/2302.12601) | TTIG模型是创造性人工智能的最新补充，可以根据文本描述生成图像。研究发现，专业人士对于TTIG应用和认知等方面存在12个主要问题。这项研究为支持TTIG的可持续采用提供了重要见解。 |
| [^77] | [K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs.](http://arxiv.org/abs/2302.11996) | 本文提出了一种名为K-SHAP的算法，来解决多个智能体保持匿名且仅有状态-动作对的情况下学习智能体决策的问题。 |
| [^78] | [Improving safety in physical human-robot collaboration via deep metric learning.](http://arxiv.org/abs/2302.11933) | 本论文提出了一种通过深度度量学习提高人机物理协作安全性的新方法，使机器人能够学习准确捕捉人类形态和运动能力的距离度量，从而降低人机交互过程中发生碰撞和受伤的风险。 |
| [^79] | [With Shared Microexponents, A Little Shifting Goes a Long Way.](http://arxiv.org/abs/2302.08007) | 本论文提出了一种名为BDR的框架，用于评估窄精度格式。通过BDR，本文发现了一种基于共享微指数的新格式，其在大规模生成预训练和推理以及推荐系统方面的效果优于其他先进的量化方法。 |
| [^80] | [Towards Inferential Reproducibility of Machine Learning Research.](http://arxiv.org/abs/2302.04054) | 本研究提出利用线性混合效应模型（LMEM）来分析机器学习性能评估分数，并考虑多个方差来源及其与数据特性相互作用，从而评估可靠性和可复制性，促进对机器学习算法行为的更全面理解。 |
| [^81] | [Appropriate Reliance on AI Advice: Conceptualization and the Effect of Explanations.](http://arxiv.org/abs/2302.02187) | 本研究提出了一个可量化的二维测量概念——适当依赖性（AoR），并且通过在实验中分析AI建议解释的效果来从根本上贡献了对于依赖行为的分析。 |
| [^82] | [Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture.](http://arxiv.org/abs/2301.08243) | 本论文提出了一种非生成方法的自监督学习架构，即Image-based Joint-Embedding Predictive Architecture（I-JEPA），可以生成高度语义图像表示，通过联合嵌入预测架构和掩模策略达到这一目的。 |
| [^83] | [Who Evaluates the Evaluators? On Automatic Metrics for Assessing AI-based Offensive Code Generators.](http://arxiv.org/abs/2212.06008) | 本研究分析了用于评估攻击代码生成器的大量输出相似度度量标准，判断自动度量标准与人工评估结果之间的相关性，并提出了选取适当度量标准的指南。 |
| [^84] | [RPN: A Word Vector Level Data Augmentation Algorithm in Deep Learning for Language Understanding.](http://arxiv.org/abs/2212.05961) | RPN是一种基于词向量级别的数据增强算法，通过引入噪声修改原始文本的词嵌入，更好地捕捉自然语言变化，并在自然语言理解任务中表现出优异的性能。 |
| [^85] | [Collaborative Training of Medical Artificial Intelligence Models with non-uniform Labels.](http://arxiv.org/abs/2211.13606) | 本研究提出了一种灵活的联合学习方法(FFL)，可以协同训练异质化标注数据集来构建强大而稳健的深度学习模型，为不同机构间的广泛合作提供了一种有前途的替代方案。 |
| [^86] | [Iterative Teaching by Data Hallucination.](http://arxiv.org/abs/2210.17467) | 本文提出了基于数据幻象教学（DHT）的迭代式教学方法，解决了在连续输入空间下教师提供示例的能力问题，并在多个挑战性的教学设置中验证了DHT的有效性。 |
| [^87] | [Addressing contingency in algorithmic (mis)information classification: Toward a responsible machine learning agenda.](http://arxiv.org/abs/2210.09014) | 本文重点讨论了机器学习（ML）启用的分类模型处理在线虚假信息和其他可能被识别为有害的内容时，对“真相”来源的合法性、权威性和客观性所采取的立场以及ML驱动的审查系统可能在不利影响方面产生的问题，分析了算法的偶然性和可能引起的评估误差。 |
| [^88] | [Anticipatory Fleet Repositioning for Shared-use Autonomous Mobility Services: An Optimization and Learning-Based Approach.](http://arxiv.org/abs/2210.08659) | 本文提出一种基于优化和学习的方法，通过预测未来需求和合作优化基于分配策略的重新平衡策略，从而改善SAMS车队的服务质量和效率，并在真实数据集上进行的数值实验表明该方法较传统启发式方法具有更好的性能和可靠性。 |
| [^89] | [SQA3D: Situated Question Answering in 3D Scenes.](http://arxiv.org/abs/2210.07474) | 本文提出了一个新的任务，即评估具有场景理解能力的代理人在三维场景中的情境问答。基于650个场景的数据集为智能代理人的推理能力考察提供了广泛且大量的问题，这对当前的多模式，特别是3D推理模型提出了很大挑战。 |
| [^90] | [On Distillation of Guided Diffusion Models.](http://arxiv.org/abs/2210.03142) | 本论文提出了一种将免分类器的引导式扩散模型蒸馏为易于采样的模型的方法，以降低在推断时的计算成本，并且能够在像素空间生成高质量的图像。 |
| [^91] | [PiFold: Toward effective and efficient protein inverse folding.](http://arxiv.org/abs/2209.12643) | PiFold 提出了一种新的残基特征提取器和 PiGNN 层，能够一次性生成蛋白质序列并提高恢复效果，在 CATH 4.2 上达到了 51.66% 的恢复率，推理速度比自回归竞争对手快 70 倍，在 TS50 和 TS500 上分别达到了 58.72% 和 60.42% 的恢复分数。 |
| [^92] | [PePe: Personalized Post-editing Model utilizing User-generated Post-edits.](http://arxiv.org/abs/2209.10139) | 本文提出了一种个性化的自动后编辑框架PePe，在一个实时的机器翻译系统中，通过收集用户后编辑数据，结合判别模块和用户特定参数，能够有效地生成考虑到不同个人行为的句子，并在多个度量标准上优于其他基线模型。 |
| [^93] | [Variable importance without impossible data.](http://arxiv.org/abs/2205.15750) | 评估黑箱预测模型中变量重要性的流行方法不可信，因为使用了不可能的数据。本文提出一种名为Cohort Shapley的方法，它基于经济博弈理论，仅使用实际观测到的数据来量化变量重要性，可以解决算法公平性问题。 |
| [^94] | [Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object Interactions.](http://arxiv.org/abs/2205.13803) | 本文提出了一个新的视觉推理基准测试Bongard-HOI，它专注于从自然图像中学习人-物交互的组合性学习，挑战了现有最先进模型，鼓励开发更好的算法进行组合推理和泛化到新的HOI概念。 |
| [^95] | [Divide & Conquer Imitation Learning.](http://arxiv.org/abs/2204.07404) | 该论文提出了一种新颖的模仿学习算法，通过将复杂任务划分为更小的技能，利用顺序归纳偏见将技能链接以解决整个任务。该算法可以在只有一个专家演示可用的情况下模仿复杂的机器人任务。 |
| [^96] | [Integrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge.](http://arxiv.org/abs/2202.07138) | 本文提出了一个框架，将AI规划与自然语言处理相结合，利用显式和隐式知识来改善生成自然语言的效果。 |
| [^97] | [AI Ethics Principles in Practice: Perspectives of Designers and Developers.](http://arxiv.org/abs/2112.07467) | 这篇论文研究了设计师和开发人员的实践和经验，分析其如何遵循澳大利亚政府提出的高层次AI伦理原则，并总结出原则之间存在的张力和权衡。 |
| [^98] | [Stochastic Optimization of Areas Under Precision-Recall Curves with Provable Convergence.](http://arxiv.org/abs/2104.08736) | 本文提出了一种用于深度学习的基于平均精度的方法来优化AUPRC，提出了一种可证收敛的SOAP算法。 |

# 详细

[^1]: 通过解释不变性和等变性评估解释方法的健壮性

    Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance. (arXiv:2304.06715v1 [cs.LG])

    [http://arxiv.org/abs/2304.06715](http://arxiv.org/abs/2304.06715)

    本文提出了解释不变性和等变性的概念，通过对对称群下具有不变性的神经网络，建立两种度量方法来提高解释方法对于不变性的健壮性并证明为一些流行的解释方法提供了理论健壮性保证。

    

    只有当解释方法忠实地描述所解释的模型时，解释方法才有价值。本文考虑了神经网络，其预测在特定对称群下具有不变性，这包括从卷积神经网络到图神经网络的流行架构。任何忠实描述这种类型模型的解释都需要与该不变性属性一致。我们通过运用几何深度学习的形式化方法，通过解释不变性和等变性的概念来形式化这种直觉。通过这种严格的形式化方法，我们得出了（1）两个度量来衡量任何解释方法相对于模型对称群的健壮性;（2）一些流行的解释方法的理论健壮性保证；（3）提高任何解释方法相对于对称群的不变性的系统方法。通过在与不同对称群相关的模型的解释中经验地测量我们的度量标准，我们展示了解释不变性和等变性对于强大的解释方法是重要的属性。

    Interpretability methods are valuable only if their explanations faithfully describe the explained model. In this work, we consider neural networks whose predictions are invariant under a specific symmetry group. This includes popular architectures, ranging from convolutional to graph neural networks. Any explanation that faithfully explains this type of model needs to be in agreement with this invariance property. We formalize this intuition through the notion of explanation invariance and equivariance by leveraging the formalism from geometric deep learning. Through this rigorous formalism, we derive (1) two metrics to measure the robustness of any interpretability method with respect to the model symmetry group; (2) theoretical robustness guarantees for some popular interpretability methods and (3) a systematic approach to increase the invariance of any interpretability method with respect to a symmetry group. By empirically measuring our metrics for explanations of models associate
    
[^2]: 动词行动：改进视频语言模型中的动词理解

    Verbs in Action: Improving verb understanding in video-language models. (arXiv:2304.06708v1 [cs.CV])

    [http://arxiv.org/abs/2304.06708](http://arxiv.org/abs/2304.06708)

    本文提出了一个新的动词聚焦对比框架，通过利用预训练的大型语言模型和执行细粒度的动词短语对齐损失来改善基于CLIP的视频语言模型的动词理解能力，实现了在三个聚焦于动词理解的下游任务的零样本性能最先进的结果。

    

    理解动词对于模型化人与物体在空间和时间上如何相互作用以及与环境相互作用至关重要。最近，基于CLIP的最先进的视频语言模型在动词理解方面受限，且严重依赖名词，限制了它们在需要动作和时间理解的实际视频应用中的性能。在本文中，我们通过提出一种新的动词聚焦对比（VFC）框架，改善基于CLIP的视频语言模型的动词理解能力。该框架由两个主要组成部分组成：（1）利用预训练的大型语言模型（LLM）创建跨模态对比学习的硬负例，以及通过校准策略平衡正负对中概念的出现来平衡正负对；（2）执行细粒度的动词短语对齐损失。我们的方法在三个聚焦于动词理解的下游任务的零样本表现方面实现了最先进的结果：视频t...

    Understanding verbs is crucial to modelling how people and objects interact with each other and the environment through space and time. Recently, state-of-the-art video-language models based on CLIP have been shown to have limited verb understanding and to rely extensively on nouns, restricting their performance in real-world video applications that require action and temporal understanding. In this work, we improve verb understanding for CLIP-based video-language models by proposing a new Verb-Focused Contrastive (VFC) framework. This consists of two main components: (1) leveraging pretrained large language models (LLMs) to create hard negatives for cross-modal contrastive learning, together with a calibration strategy to balance the occurrence of concepts in positive and negative pairs; and (2) enforcing a fine-grained, verb phrase alignment loss. Our method achieves state-of-the-art results for zero-shot performance on three downstream tasks that focus on verb understanding: video-t
    
[^3]: 如何预测织物的自由落体效应? 通过深度图像捕捉织物力学特性

    How Will It Drape Like? Capturing Fabric Mechanics from Depth Images. (arXiv:2304.06704v1 [cs.CV])

    [http://arxiv.org/abs/2304.06704](http://arxiv.org/abs/2304.06704)

    该论文提出了一种使用深度相机进行随意捕捉的方法来预测织物的自由落体效应，并创新性地提出了一种模拟到真实的策略来训练学习框架，该框架可以输出完整的力学参数集。

    

    我们提出了一种使用深度相机进行随意捕捉的方法来估计织物的力学参数。我们的方法可以创建真实世界纺织材料的机械正确数字表示，这是许多交互式设计和工程应用的基本步骤。与现有的捕捉方法相比，我们的解决方案可以在规模上进行捕捉，与纺织品的光学外观无关，并且易于由非专业操作者进行织物排列。为此，我们提出了一种模拟到真实的策略，以训练一个基于学习的框架，该框架可以将一个或多个图像作为输入并输出完整的力学参数集。由于经过精心设计的数据增强和转移学习协议，我们的解决方案可以推广到真实图像，尽管只是在合成数据上进行训练，因此成功地关闭了模拟到真实的循环。

    We propose a method to estimate the mechanical parameters of fabrics using a casual capture setup with a depth camera. Our approach enables to create mechanically-correct digital representations of real-world textile materials, which is a fundamental step for many interactive design and engineering applications. As opposed to existing capture methods, which typically require expensive setups, video sequences, or manual intervention, our solution can capture at scale, is agnostic to the optical appearance of the textile, and facilitates fabric arrangement by non-expert operators. To this end, we propose a sim-to-real strategy to train a learning-based framework that can take as input one or multiple images and outputs a full set of mechanical parameters. Thanks to carefully designed data augmentation and transfer learning protocols, our solution generalizes to real images despite being trained only on synthetic data, hence successfully closing the sim-to-real loop.Key in our work is to 
    
[^4]: 学习个性化决策支持策略

    Learning Personalized Decision Support Policies. (arXiv:2304.06701v1 [cs.LG])

    [http://arxiv.org/abs/2304.06701](http://arxiv.org/abs/2304.06701)

    本文提出了一种学习个性化决策支持策略的算法 $\texttt{THREAD}$，可以为决策者提供不同形式的支持。同时，引入了 $\texttt{Modiste}$ 工具来提供个性化的医学诊断决策支持，使用 $\texttt{THREAD}$ 学习个性化决策支持策略，有效提高了预期的诊断正确性，并减少了严重并发症的风险，同时推荐了更少和更便宜的研究。

    

    个体决策者可能需要不同形式的支持来提高决策结果，但重要的问题是，哪种形式的支持会在低成本下导致准确的决策。本文提出了学习决策支持策略的方法，它在给定输入时选择是否以及如何提供支持。我们考虑没有先验信息的决策者，并将学习各自的策略形式化为一个多目标优化问题，这个问题权衡了准确性和成本。使用随机环境的技术，我们提出了 $\texttt{THREAD}$，这是一种个性化决策支持策略的在线算法，并设计了一种超参数调整策略，以利用模拟人类行为来确定成本-性能权衡。我们提供计算实验来证明 $\texttt{THREAD}$ 相对于线下基线的优势。然后，我们推出了一个交互式工具 $\texttt{Modiste}$，它为现实中的医学诊断提供个性化决策支持。$\texttt{Modiste}$ 使用 $\texttt{THREAD}$ 为每位医生学习个性化的决策支持策略，并推荐个性化研究以优化患者的预期结果并将严重并发症的风险降至最低。使用电子健康记录数据，我们展示了 $\texttt{Modiste}$ 显著提高了预期的诊断正确性，并减少了严重并发症的风险，同时推荐了更少和更便宜的研究。

    Individual human decision-makers may benefit from different forms of support to improve decision outcomes. However, a key question is which form of support will lead to accurate decisions at a low cost. In this work, we propose learning a decision support policy that, for a given input, chooses which form of support, if any, to provide. We consider decision-makers for whom we have no prior information and formalize learning their respective policies as a multi-objective optimization problem that trades off accuracy and cost. Using techniques from stochastic contextual bandits, we propose $\texttt{THREAD}$, an online algorithm to personalize a decision support policy for each decision-maker, and devise a hyper-parameter tuning strategy to identify a cost-performance trade-off using simulated human behavior. We provide computational experiments to demonstrate the benefits of $\texttt{THREAD}$ compared to offline baselines. We then introduce $\texttt{Modiste}$, an interactive tool that pr
    
[^5]: 利用形状信息进行少样本学习：LSFSL

    LSFSL: Leveraging Shape Information in Few-shot Learning. (arXiv:2304.06672v1 [cs.CV])

    [http://arxiv.org/abs/2304.06672](http://arxiv.org/abs/2304.06672)

    LSFSL是一种少样本学习方法，利用数据中的隐含先验信息强制模型学习更具有泛化性的特征。通过全面分析，证明该方法提高了模型的鲁棒性和泛化性。

    

    少样本学习技术旨在使用较少的样本来学习数据中的潜在模式，类似于人类从有限的经验中学习。在这种有限数据的情况下，与深度神经网络相关的快捷学习和纹理偏差行为等挑战更为严峻。此外，在少样本设置中解决快捷学习的重要性尚未得到充分探索。为解决这些问题，我们提出了LSFSL，该模型利用数据中的隐含先验信息强制模型学习更具有泛化性的特征。通过全面分析，我们证明了在数据的全局语义中利用LSFSL训练的模型对颜色方案、统计相关性和对抗扰动的改变更不易受到攻击和干扰，凸显出在少样本方法中融入相关先验以增强鲁棒性和泛化性的潜力。

    Few-shot learning (FSL) techniques seek to learn the underlying patterns in data using fewer samples, analogous to how humans learn from limited experience. In this limited-data scenario, the challenges associated with deep neural networks, such as shortcut learning and texture bias behaviors, are further exacerbated. Moreover, the significance of addressing shortcut learning is not yet fully explored in the few-shot setup. To address these issues, we propose LSFSL, which enforces the model to learn more generalizable features utilizing the implicit prior information present in the data. Through comprehensive analyses, we demonstrate that LSFSL-trained models are less vulnerable to alteration in color schemes, statistical correlations, and adversarial perturbations leveraging the global semantics in the data. Our findings highlight the potential of incorporating relevant priors in few-shot approaches to increase robustness and generalization.
    
[^6]: 布局引导下的图像生成的诊断基准和迭代修复

    Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v1 [cs.CV])

    [http://arxiv.org/abs/2304.06671](http://arxiv.org/abs/2304.06671)

    本文提出了布局引导下图像生成的诊断基准LayoutBench，对数量、位置、大小和形状四种空间控制技能进行了研究，发现好的ID布局控制在任意布局的野外环境下可能不具有良好的推广性。接着，我们提出了一种新的基准方法IterInpaint通过修复逐步生成前景和背景区域，显现出在OOD布局方面更强的通用性。

    

    空间控制是可控图像生成的核心能力。在布局引导下的图像生成方面的进展已经显示出在具有类似空间配置的内分布（ID）数据集上有良好的结果。然而，当面对任意不确定的布局的离线分布样本时，这些模型的表现还不清楚。在本文中，我们提出了LayoutBench，这是一种对布局引导下的图像生成进行诊断的基准，它检查了四种空间控制技能：数量，位置，大小和形状。我们对两种最近代表性的布局引导下的图像生成方法进行了基准测试，并观察到良好的ID布局控制可能无法很好地推广到任意布局的野外环境（例如，边界上的对象）。接下来，我们提出了一个新的基准方法IterInpaint，它通过修复逐步生成前景和背景区域，展示出在LayoutBench的OOD布局上更强的通用性。我们进行了数量和定性评估，表明IterInpaint相对于现有方法具有更好的生成多样和视觉上令人愉悦的图像和可控的空间布局。

    Spatial control is a core capability in controllable image generation. Advancements in layout-guided image generation have shown promising results on in-distribution (ID) datasets with similar spatial configurations. However, it is unclear how these models perform when facing out-of-distribution (OOD) samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench, a diagnostic benchmark for layout-guided image generation that examines four categories of spatial control skills: number, position, size, and shape. We benchmark two recent representative layout-guided image generation methods and observe that the good ID layout control may not generalize well to arbitrary layouts in the wild (e.g., objects at the boundary). Next, we propose IterInpaint, a new baseline that generates foreground and background regions in a step-by-step manner via inpainting, demonstrating stronger generalizability than existing models on OOD layouts in LayoutBench. We perform quantitative and q
    
[^7]: 深度神经网络是否具备内置的奥卡姆剃刀？

    Do deep neural networks have an inbuilt Occam's razor?. (arXiv:2304.06670v1 [cs.LG])

    [http://arxiv.org/abs/2304.06670](http://arxiv.org/abs/2304.06670)

    该研究利用基于函数先验的贝叶斯视角来研究深度神经网络（DNNs）的表现来源，结果表明DNNs之所以成功，是因为它对于具有结构的数据，具备一种内在的奥卡姆剃刀式的归纳偏差，足以抵消函数数量及复杂度的指数级增长。

    

    超参数化深度神经网络（DNNs）的卓越性能必须源自于网络架构、训练算法和数据结构之间的相互作用。为了区分这三个部分，我们应用了基于DNN所表达的函数的贝叶斯视角来进行监督学习。经过网络确定的函数先验通过利用有序和混沌状态之间的转变而变化。对于布尔函数分类，我们利用函数的误差谱在数据上进行可能性的近似。当与先验相结合时，它可以精确地预测使用随机梯度下降训练的DNN的后验概率。该分析揭示了结构化数据，以及内在的奥卡姆剃刀式归纳偏差，即足以抵消复杂度随函数数量呈指数增长而产生的影响，是DNNs成功的关键。

    The remarkable performance of overparameterized deep neural networks (DNNs) must arise from an interplay between network architecture, training algorithms, and structure in the data. To disentangle these three components, we apply a Bayesian picture, based on the functions expressed by a DNN, to supervised learning. The prior over functions is determined by the network, and is varied by exploiting a transition between ordered and chaotic regimes. For Boolean function classification, we approximate the likelihood using the error spectrum of functions on data. When combined with the prior, this accurately predicts the posterior, measured for DNNs trained with stochastic gradient descent. This analysis reveals that structured data, combined with an intrinsic Occam's razor-like inductive bias towards (Kolmogorov) simple functions that is strong enough to counteract the exponential growth of the number of functions with complexity, is a key to the success of DNNs.
    
[^8]: 大型语言模型生成的教育问题有多有用？

    How Useful are Educational Questions Generated by Large Language Models?. (arXiv:2304.06638v1 [cs.CL])

    [http://arxiv.org/abs/2304.06638](http://arxiv.org/abs/2304.06638)

    本文研究通过结合CTG和问题分类生成的输出，通过教师评估证明这些生成的问题质量高且足够有用，有极大的应用潜力。

    

    由大型语言模型进行可控文本生成（CTG）对于教师和学生来说有着巨大的潜力，特别是高质量和多样性的问题生成可以大幅减轻教师的负担，提高他们教学内容的质量。最近在该领域的研究已经取得了进展，但未能表明真正的教师评判生成的问题在课堂环境中是否足够有用，或者问题是否存在错误和/或教学内容的帮助不大。本文通过人类评估教师的方式，评估通过结合CTG和问题分类（Bloom's和难度分类）生成的输出的质量和有用性。结果表明生成的问题质量高且足够有用，展示了在课堂环境中广泛使用的潜力。

    Controllable text generation (CTG) by large language models has a huge potential to transform education for teachers and students alike. Specifically, high quality and diverse question generation can dramatically reduce the load on teachers and improve the quality of their educational content. Recent work in this domain has made progress with generation, but fails to show that real teachers judge the generated questions as sufficiently useful for the classroom setting; or if instead the questions have errors and/or pedagogically unhelpful content. We conduct a human evaluation with teachers to assess the quality and usefulness of outputs from combining CTG and question taxonomies (Bloom's and a difficulty taxonomy). The results demonstrate that the questions generated are high quality and sufficiently useful, showing their promise for widespread use in the classroom setting.
    
[^9]: PGTask：介绍从对话中生成档案的任务

    PGTask: Introducing the Task of Profile Generation from Dialogues. (arXiv:2304.06634v1 [cs.CL])

    [http://arxiv.org/abs/2304.06634](http://arxiv.org/abs/2304.06634)

    对话系统的个性化需要个人资料信息，而从对话中提取/生成个人资料信息是一项基本需求。为此，我们提出了档案生成任务（PGTask）并提供了相关的数据集和基准，该任务使得研究者可以更好地了解档案生成任务的挑战和可能的解决方案。

    

    最近的研究尝试通过将个人资料信息融入模型来个性化对话系统。然而，这种知识信息稀少且难以获取，这使得从对话中提取/生成个人资料信息成为一项基本需求。为了克服这一限制，我们引入了档案生成任务（PGTask）。我们为此问题提供了一个新的数据集，其中包括与相关话语对齐的档案句子，从对话语料库中提取。此外，利用最先进的方法，我们为这个新数据集提供了一个档案生成的基准。我们的实验揭示了档案生成的挑战，并希望这引入了一个新的研究方向。

    Recent approaches have attempted to personalize dialogue systems by leveraging profile information into models. However, this knowledge is scarce and difficult to obtain, which makes the extraction/generation of profile information from dialogues a fundamental asset. To surpass this limitation, we introduce the Profile Generation Task (PGTask). We contribute with a new dataset for this problem, comprising profile sentences aligned with related utterances, extracted from a corpus of dialogues. Furthermore, using state-of-the-art methods, we provide a benchmark for profile generation on this novel dataset. Our experiments disclose the challenges of profile generation, and we hope that this introduces a new research direction.
    
[^10]: AI-Generated Content (AIGC)：一项调研

    AI-Generated Content (AIGC): A Survey. (arXiv:2304.06632v1 [cs.AI])

    [http://arxiv.org/abs/2304.06632](http://arxiv.org/abs/2304.06632)

    人工智能生成的内容（AIGC）是一种有前途的生成工具，使用人工智能根据用户输入的关键词或需求生成内容，并为我们的生活带来便捷。本文调研并广泛概述了AIGC的定义、重要条件、尖端能力、高级功能、大规模预训练模型的优势和产业链。

    

    为了解决数字经济中数字智能所面临的挑战，人工智能生成的内容（AIGC）应运而生。AIGC使用人工智能根据用户输入的关键词或需求生成内容，从而辅助或替代手动内容生成。大型模型算法的发展显著增强了AIGC的能力，使其成为一种有前途的生成工具，并为我们的生活增添了便捷。作为上游技术，AIGC具有支持不同下游应用程序的无限潜力。分析AIGC的当前能力和不足是理解如何在未来应用中最好地利用它的重要途径。因此，本文对AIGC进行了广泛的概述，包括其定义、重要条件、尖端能力和高级功能。此外，它讨论了大规模预训练模型的好处和AIGC的产业链。

    To address the challenges of digital intelligence in the digital economy, artificial intelligence-generated content (AIGC) has emerged. AIGC uses artificial intelligence to assist or replace manual content generation by generating content based on user-inputted keywords or requirements. The development of large model algorithms has significantly strengthened the capabilities of AIGC, which makes AIGC products a promising generative tool and adds convenience to our lives. As an upstream technology, AIGC has unlimited potential to support different downstream applications. It is important to analyze AIGC's current capabilities and shortcomings to understand how it can be best utilized in future applications. Therefore, this paper provides an extensive overview of AIGC, covering its definition, essential conditions, cutting-edge capabilities, and advanced features. Moreover, it discusses the benefits of large-scale pre-trained models and the industrial chain of AIGC. Furthermore, the arti
    
[^11]: 模型所有权争议中的虚假指控

    False Claims against Model Ownership Resolution. (arXiv:2304.06607v1 [cs.CR])

    [http://arxiv.org/abs/2304.06607](http://arxiv.org/abs/2304.06607)

    该论文研究了模型所有权解决方案中对抗恶意原告的鲁棒性问题，展示了常见的MOR方案可以被恶意原告针对未被盗用的独立模型提出虚假指控。

    

    深度神经网络模型是模型所有者的有价值知识产权，构成了竞争优势。因此，开发保护模型不被盗用的技术至关重要。模型所有权解决方案（MOR）是一类可以防止模型被盗的技术。MOR方案使得原告方可以通过提供证据（如水印或指纹）来断言对涉嫌盗用模型的被告方声称所有权，证明涉嫌模型是被盗或者源自于原告方拥有的源模型。现有的大多数 MOR 方案重点放在防范恶意涉嫌方方面，确保如果涉嫌模型确实是被盗版，则原告方将获胜。但是在本文中，我们揭示了现有文献中的常见 MOR 方案存在着另一个同等重要但尚未被充分探讨的鲁棒性问题：恶意原告。我们展示了如何成功地针对未被盗用的独立模型提出虚假指控。

    Deep neural network (DNN) models are valuable intellectual property of model owners, constituting a competitive advantage. Therefore, it is crucial to develop techniques to protect against model theft. Model ownership resolution (MOR) is a class of techniques that can deter model theft. A MOR scheme enables an accuser to assert an ownership claim for a suspect model by presenting evidence, such as a watermark or fingerprint, to show that the suspect model was stolen or derived from a source model owned by the accuser. Most of the existing MOR schemes prioritize robustness against malicious suspects, ensuring that the accuser will win if the suspect model is indeed a stolen model.  In this paper, we show that common MOR schemes in the literature are vulnerable to a different, equally important but insufficiently explored, robustness concern: a malicious accuser. We show how malicious accusers can successfully make false claims against independent suspect models that were not stolen. Our
    
[^12]: 超越次模性：带组公平性约束的随机集合选择的统一框架

    Beyond Submodularity: A Unified Framework of Randomized Set Selection with Group Fairness Constraints. (arXiv:2304.06596v1 [cs.LG])

    [http://arxiv.org/abs/2304.06596](http://arxiv.org/abs/2304.06596)

    该论文介绍了一种带有组公平性约束的随机子集选择的框架，并提出了一个全新的优化方法，可以优化一系列超出次模性范围的问题。在测试中，该框架可以实现组公平性而不牺牲太多效用。

    

    机器学习算法在多个重要决策过程中发挥着重要作用，包括定向广告展示、家庭贷款批准和犯罪行为预测等。鉴于这些算法的深远影响，关键在于它们运作应该公平，没有偏见或对某些群体的偏见。确保这些算法的公正性对于促进平等和避免歧视至关重要。为此，我们引入了一个带有组公平性约束的随机子集选择的统一框架。我们的问题涉及全局效用函数以及每个组的一组效用函数，其中一个组指共享相同属性（例如性别）的个体组。我们的目标是生成跨可行子集的分布，指定每个可行集的选择概率，以最大化全局效用函数并满足预定配额的要求。

    Machine learning algorithms play an important role in a variety of important decision-making processes, including targeted advertisement displays, home loan approvals, and criminal behavior predictions. Given the far-reaching impact of these algorithms, it is crucial that they operate fairly, free from bias or prejudice towards certain groups in the population. Ensuring impartiality in these algorithms is essential for promoting equality and avoiding discrimination. To this end we introduce a unified framework for randomized subset selection that incorporates group fairness constraints. Our problem involves a global utility function and a set of group utility functions for each group, here a group refers to a group of individuals (e.g., people) sharing the same attributes (e.g., gender). Our aim is to generate a distribution across feasible subsets, specifying the selection probability of each feasible set, to maximize the global utility function while meeting a predetermined quota for
    
[^13]: ChatGPT-4在政治Twitter信息注释中通过零样本学习胜过专家和众包工作者

    ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning. (arXiv:2304.06588v1 [cs.CL])

    [http://arxiv.org/abs/2304.06588](http://arxiv.org/abs/2304.06588)

    本文评估了ChatGPT-4在政治Twitter相关推文注释任务中的表现，结果显示它在准确性、可靠性和偏见方面皆优于人类标注，尤其是它能通过零样本学习准确注释需要基于上下文知识和作者意向的推文，这将使得规模化的文本数据研究成为可能，并在社会科学的解释性研究中产生巨大影响。

    

    本文评估了大型语言模型ChatGPT-4在政治相关推文注释任务中的准确性、可靠性和偏见。与专家和众包工作者标注进行比较，研究使用2020年美国选举期间的政治相关推文作为数据集，提供了可靠的准确性评测基准。实验发现，ChatGPT-4的准确性更高、可靠性更高，并且偏见相等或更低。该模型能够准确注释需要基于上下文知识进行推理和作者意向的推文，这些能力被传统上视为是人类独有的。研究结果表明，LLM在社会科学中使用文本数据进行解释性研究方面将产生巨大的影响，并使得规模化的文本数据研究成为可能。

    This paper assesses the accuracy, reliability and bias of the Large Language Model (LLM) ChatGPT-4 on the text analysis task of classifying the political affiliation of a Twitter poster based on the content of a tweet. The LLM is compared to manual annotation by both expert classifiers and crowd workers, generally considered the gold standard for such tasks. We use Twitter messages from United States politicians during the 2020 election, providing a ground truth against which to measure accuracy. The paper finds that ChatGPT-4 has achieves higher accuracy, higher reliability, and equal or lower bias than the human classifiers. The LLM is able to correctly annotate messages that require reasoning on the basis of contextual knowledge, and inferences around the author's intentions - traditionally seen as uniquely human abilities. These findings suggest that LLM will have substantial impact on the use of textual data in the social sciences, by enabling interpretive research at a scale.
    
[^14]: 训练后的智能体可能会追求权力，并且具有预测性

    Power-seeking can be probable and predictive for trained agents. (arXiv:2304.06528v1 [cs.AI])

    [http://arxiv.org/abs/2304.06528](http://arxiv.org/abs/2304.06528)

    在简化假设下，训练奖励一致的目标集合中的智能体仍有可能追求权力，具有预测性。

    

    追求权力的行为是高级人工智能面临的重要风险来源，但是我们对于这种现象的理论理解还相对有限。在现有理论结果的基础上，我们研究了训练过程对于智能体权力追求动机的影响，并证明在一些简化的假设下，这一动机仍然有可能在训练后的智能体中产生。我们正式定义了“与训练奖励一致的目标集”（即与训练奖励相一致的目标集合），并假设训练后的智能体从这个集合中学习到了一个目标。在一个新的情境中，当训练后的智能体面临关机还是避免关机的选择时，我们证明智能体可能会避免关机。因此，我们展示了权力追求动机可能是可预测的（可以预测在新情境中的不良行为）并且是有可能发生在训练后的智能体中的。

    Power-seeking behavior is a key source of risk from advanced AI, but our theoretical understanding of this phenomenon is relatively limited. Building on existing theoretical results demonstrating power-seeking incentives for most reward functions, we investigate how the training process affects power-seeking incentives and show that they are still likely to hold for trained agents under some simplifying assumptions. We formally define the training-compatible goal set (the set of goals consistent with the training rewards) and assume that the trained agent learns a goal from this set. In a setting where the trained agent faces a choice to shut down or avoid shutdown in a new situation, we prove that the agent is likely to avoid shutdown. Thus, we show that power-seeking incentives can be probable (likely to arise for trained agents) and predictive (allowing us to predict undesirable behavior in new situations).
    
[^15]: 基于转移学习的高效苹果叶病分类方法

    An Efficient Transfer Learning-based Approach for Apple Leaf Disease Classification. (arXiv:2304.06520v1 [cs.CV])

    [http://arxiv.org/abs/2304.06520](http://arxiv.org/abs/2304.06520)

    本研究提出了一种基于转移学习的苹果叶病识别技术，通过预训练的EfficientNetV2S架构提取特征并结合分类器块进行预测。该研究解决了类不平衡的问题，对各种超参数进行了仔细调查，为苹果树农提供了更加快速高效的叶病分类解决方案。

    

    正确识别和分类植物疾病对于保障全球粮食供应的安全和各利益相关者的经济成功至关重要。针对不同的主要作物，通过引入深度学习分类系统，提供了广泛的解决方案。尽管在许多地区是最重要的商业作物之一，但对于自动分类苹果叶病的智能解决方案的研究相对较少。本研究提出了一种基于转移学习的苹果叶病识别技术。该系统使用预训练的EfficientNetV2S架构提取特征，并传递到分类器块进行有效的预测。通过利用运行时数据增强来解决类不平衡问题。已经仔细调查了各种超参数的影响，如输入分辨率、学习率、纪元数等。该方案的能力得到了证明。

    Correct identification and categorization of plant diseases are crucial for ensuring the safety of the global food supply and the overall financial success of stakeholders. In this regard, a wide range of solutions has been made available by introducing deep learning-based classification systems for different staple crops. Despite being one of the most important commercial crops in many parts of the globe, research proposing a smart solution for automatically classifying apple leaf diseases remains relatively unexplored. This study presents a technique for identifying apple leaf diseases based on transfer learning. The system extracts features using a pretrained EfficientNetV2S architecture and passes to a classifier block for effective prediction. The class imbalance issues are tackled by utilizing runtime data augmentation. The effect of various hyperparameters, such as input resolution, learning rate, number of epochs, etc., has been investigated carefully. The competence of the pro
    
[^16]: 在认知无线电环境下安全的联邦学习频谱感知

    Secure Federated Learning for Cognitive Radio Sensing. (arXiv:2304.06519v1 [eess.SP])

    [http://arxiv.org/abs/2304.06519](http://arxiv.org/abs/2304.06519)

    本文探讨了在认知无线电环境下基于联邦学习实现可靠和安全的频谱感知（SS），深入分析了FL在SS中的动机、架构和算法，同时提供了安全和隐私威胁的对抗措施。

    

    本文考虑在认知无线电环境中基于联邦学习实现可靠和安全的频谱感知（SS）。讨论了FL在SS中的动机、架构和算法。概述了这些算法面临的安全和隐私威胁，并提出可能的对抗措施。同时提供了一些图示例，以及针对未来CR中基于FL的SS的设计建议。

    This paper considers reliable and secure Spectrum Sensing (SS) based on Federated Learning (FL) in the Cognitive Radio (CR) environment. Motivation, architectures, and algorithms of FL in SS are discussed. Security and privacy threats on these algorithms are overviewed, along with possible countermeasures to such attacks. Some illustrative examples are also provided, with design recommendations for FL-based SS in future CRs.
    
[^17]: 基于无源射频的三维室内定位系统及集成学习

    Passive Radio Frequency-based 3D Indoor Positioning System via Ensemble Learning. (arXiv:2304.06513v1 [eess.SP])

    [http://arxiv.org/abs/2304.06513](http://arxiv.org/abs/2304.06513)

    本文提出了一种基于无源射频的三维室内定位系统，可以捕捉情景特征并使用集成学习来提高定位精度。

    

    基于无源射频的室内定位系统因其低成本、易于定制配置和非侵入性设计吸引了研究者的关注。本文提出了一种基于无源射频的三维室内定位系统，可以通过利用信号进行定位并捕捉情景签名。该系统通过单个接收器被动监测包含场景特征的信号。此外，该系统利用动态数据驱动应用程序系统（DDDAS）框架设计和定制采样频率，使得系统可以使用受影响最大的频带作为评定频带。该系统使用三种集成学习策略内的各种回归方法来训练和预测接收器位置。在实验场景下采集了 60 个位置的 PRF 频谱，并应用了三个评估准则来评估系统的性能。实验结果表明无源射频三维定位系统达到了很好的定位精度。

    Passive radio frequency (PRF)-based indoor positioning systems (IPS) have attracted researchers' attention due to their low price, easy and customizable configuration, and non-invasive design. This paper proposes a PRF-based three-dimensional (3D) indoor positioning system (PIPS), which is able to use signals of opportunity (SoOP) for positioning and also capture a scenario signature. PIPS passively monitors SoOPs containing scenario signatures through a single receiver. Moreover, PIPS leverages the Dynamic Data Driven Applications System (DDDAS) framework to devise and customize the sampling frequency, enabling the system to use the most impacted frequency band as the rated frequency band. Various regression methods within three ensemble learning strategies are used to train and predict the receiver position. The PRF spectrum of 60 positions is collected in the experimental scenario, and three criteria are applied to evaluate the performance of PIPS. Experimental results show that the
    
[^18]: GeoAI的哲学基础：探索GeoAI和空间数据科学中的可持续性、多样性和偏见

    Philosophical Foundations of GeoAI: Exploring Sustainability, Diversity, and Bias in GeoAI and Spatial Data Science. (arXiv:2304.06508v1 [cs.CY])

    [http://arxiv.org/abs/2304.06508](http://arxiv.org/abs/2304.06508)

    本文介绍了GeoAI和空间数据科学的哲学基础，分别从可持续性、训练数据中的偏差、模式知识的多样性和系统中的中立性缺失等角度出发，为我们设计、培训和部署基于GeoAI的系统提供了帮助，也为我们理解人工智能和机器学习研究的利益和潜在危险提供了共同理解。

    

    本章介绍了可能构成GeoAI和空间数据科学哲学基础的一些基本假设和原则。文章强调了可持续性、训练数据中的偏差、模式知识的多样性以及来自统一伦理视角的GeoAI系统中（潜在的）中立性缺失等主题，而非审查空间数据（分析）的成熟特征，如交互、邻域和自相关性。反思我们职业道德的影响将有助于我们更负责地进行潜在的研究，识别设计、培训和部署基于GeoAI的系统中的陷阱，并在跨学科领域中共同开发人工智能和机器学习研究的利益和潜在危险的共同理解，同时与他人分享我们独特的（地理）空间视角。

    This chapter presents some of the fundamental assumptions and principles that could form the philosophical foundation of GeoAI and spatial data science. Instead of reviewing the well-established characteristics of spatial data (analysis), including interaction, neighborhoods, and autocorrelation, the chapter highlights themes such as sustainability, bias in training data, diversity in schema knowledge, and the (potential lack of) neutrality of GeoAI systems from a unifying ethical perspective. Reflecting on our profession's ethical implications will assist us in conducting potentially disruptive research more responsibly, identifying pitfalls in designing, training, and deploying GeoAI-based systems, and developing a shared understanding of the benefits but also potential dangers of artificial intelligence and machine learning research across academic fields, all while sharing our unique (geo)spatial perspective with others.
    
[^19]: DiaTrend: 基于先进糖尿病技术的数据集，促进新型分析方法的发展

    DiaTrend: A dataset from advanced diabetes technology to enable development of novel analytic solutions. (arXiv:2304.06506v1 [cs.CY])

    [http://arxiv.org/abs/2304.06506](http://arxiv.org/abs/2304.06506)

    DiaTrend 数据集由 54 位糖尿病患者穿戴式医疗设备产生的数据组成，包括总计27,561天的连续血糖监测数据和8,220天的胰岛素泵数据。该数据集对于开发新型的分析解决方案非常有用。

    

    在许多领域，需要客观的数字化数据来支持改善医疗保健工作的研究，但这类数据却很难获得。虽然消费级可穿戴设备和智能手机等设备的数据比较容易获取，但我们迫切需要与相关糖尿病诊断条件的临床设备提供类似的数据。可穿戴医疗设备在糖尿病领域的普及为该领域及其他相关领域的研究和发展创造了契机。但是，开放数据集的稀缺性成为了发展的主要障碍。为了方便更广泛的糖尿病相关问题研究和加速健壮的计算分析解决方案的开发，我们提供了 DiaTrend 数据集。该数据集包括54位糖尿病患者长期使用的穿戴式医疗设备产生的数据，其中包括总计27,561天的连续血糖监测数据和8,220天的胰岛素泵数据。该数据集对于开发新型的分析解决方案非常有用。

    Objective digital data is scarce yet needed in many domains to enable research that can transform the standard of healthcare. While data from consumer-grade wearables and smartphones is more accessible, there is critical need for similar data from clinical-grade devices used by patients with a diagnosed condition. The prevalence of wearable medical devices in the diabetes domain sets the stage for unique research and development within this field and beyond. However, the scarcity of open-source datasets presents a major barrier to progress. To facilitate broader research on diabetes-relevant problems and accelerate development of robust computational solutions, we provide the DiaTrend dataset. The DiaTrend dataset is composed of intensive longitudinal data from wearable medical devices, including a total of 27,561 days of continuous glucose monitor data and 8,220 days of insulin pump data from 54 patients with diabetes. This dataset is useful for developing novel analytic solutions tha
    
[^20]: AI法案估计影响的定量研究

    Quantitative study about the estimated impact of the AI Act. (arXiv:2304.06503v1 [cs.CY])

    [http://arxiv.org/abs/2304.06503](http://arxiv.org/abs/2304.06503)

    本文研究了欧盟提出的AI法案对AI系统的影响，通过对德国的AI产品和项目进行分类，得出了该法案的具体要求和限制，为评估该法案在AI领域的实际影响提供了系统方法。

    

    随着提出覆盖整个AI系统复杂性的AI法案，欧盟提供了第一个适用于整个AI系统的监管文件。一些人担心该法规留给解释的空间过大，因此对社会的利益帮助不大，而其他人则认为该法规太过严格，因此阻碍了进步和创新，以及妨碍了欧盟内公司的经济成功。在没有系统方法的情况下，很难评估它将如何影响AI领域。本文提出了一种系统方法，我们在2021年4月发布的AI法案初始草案上应用了该方法。我们与计算机科学和法律领域的专家一起，通过对德国列出的AI产品和项目进行分类，归类了这些产品和项目的AI法案。

    With the Proposal for a Regulation laying down harmonised rules on Artificial Intelligence (AI Act) the European Union provides the first regulatory document that applies to the entire complex of AI systems. While some fear that the regulation leaves too much room for interpretation and thus bring little benefit to society, others expect that the regulation is too restrictive and, thus, blocks progress and innovation, as well as hinders the economic success of companies within the EU. Without a systematic approach, it is difficult to assess how it will actually impact the AI landscape. In this paper, we suggest a systematic approach that we applied on the initial draft of the AI Act that has been released in April 2021. We went through several iterations of compiling the list of AI products and projects in and from Germany, which the Lernende Systeme platform lists, and then classified them according to the AI Act together with experts from the fields of computer science and law. Our s
    
[^21]: 惯性测量单元人体活动识别的领域自适应: 一项综述

    Domain Adaptation for Inertial Measurement Unit-based Human Activity Recognition: A Survey. (arXiv:2304.06489v1 [eess.SP])

    [http://arxiv.org/abs/2304.06489](http://arxiv.org/abs/2304.06489)

    本文综述了惯性测量单元人体活动识别中利用领域自适应技术解决数据分布异质性问题的最新进展。

    

    基于机器学习的可穿戴式人体活动识别模型，可以开发各种智能社区应用，如睡眠模式监测、药物提醒、认知健康评估、运动分析等。但是，由于传感器放置在不同的身体位置、设备固有偏差和异质性，以及个人和环境的差异等造成的数据分布异质性，这些WHAR模型的性能受到了影响。文献中提出了各种传统的机器学习算法和迁移学习技术来解决处理这种数据异质性的挑战。领域自适应是近年来广受欢迎的一种迁移学习技术之一。本文对惯性测量单元(IMU)人体活动识别中的领域自适应技术的最新进展进行了调查。

    Machine learning-based wearable human activity recognition (WHAR) models enable the development of various smart and connected community applications such as sleep pattern monitoring, medication reminders, cognitive health assessment, sports analytics, etc. However, the widespread adoption of these WHAR models is impeded by their degraded performance in the presence of data distribution heterogeneities caused by the sensor placement at different body positions, inherent biases and heterogeneities across devices, and personal and environmental diversities. Various traditional machine learning algorithms and transfer learning techniques have been proposed in the literature to address the underpinning challenges of handling such data heterogeneities. Domain adaptation is one such transfer learning techniques that has gained significant popularity in recent literature. In this paper, we survey the recent progress of domain adaptation techniques in the Inertial Measurement Unit (IMU)-based 
    
[^22]: 生成式AI的一小步，AGI的一大步：AIGC时代中ChatGPT的全面调查

    One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era. (arXiv:2304.06488v1 [cs.CY])

    [http://arxiv.org/abs/2304.06488](http://arxiv.org/abs/2304.06488)

    ChatGPT是生成式AI的一小步，也是AGI的一大步，我们进行了综述，并对其如何演变为AIGC展望不同于以往的通用AI生成内容的发展道路。

    

    OpenAI 最近发布了GPT-4（又称为ChatGPT plus），该模型被证明是生成式AI（GAI）迈出的一小步，但对于人工通用智能（AGI）来说则是一个重大的飞跃。自2022年11月正式发布以来，ChatGPT便迅速吸引了众多用户，引起了广泛的媒体关注，相关的学术文章也超过了500篇。因此，有必要进行一次综述，我们的工作就是填补这一空白。总的来说，我们是第一个从技术、应用和挑战三个方面全面调查ChatGPT的团队，并展望了ChatGPT如何演变以实现通用的AI生成内容（AIGC），这将是AGI发展的重要里程碑。

    OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is demonstrated to be one small step for generative AI (GAI), but one giant leap for artificial general intelligence (AGI). Since its official release in November 2022, ChatGPT has quickly attracted numerous users with extensive media coverage. Such unprecedented attention has also motivated numerous researchers to investigate ChatGPT from various aspects. According to Google scholar, there are more than 500 articles with ChatGPT in their titles or mentioning it in their abstracts. Considering this, a review is urgently needed, and our work fills this gap. Overall, this work is the first to survey ChatGPT with a comprehensive review of its underlying technology, applications, and challenges. Moreover, we present an outlook on how ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for the development of AGI.
    
[^23]: CoRe-Sleep: 一种多模态融合框架，用于对不完善模态具有鲁棒性的时间序列分析

    CoRe-Sleep: A Multimodal Fusion Framework for Time Series Robust to Imperfect Modalities. (arXiv:2304.06485v1 [eess.SP])

    [http://arxiv.org/abs/2304.06485](http://arxiv.org/abs/2304.06485)

    CoRe-Sleep是一种多模态融合框架，特别关注于提高对不完善数据的信号分析的鲁棒性，它通过适当处理多模态信息，容忍噪声或丢失的模态片段，展现最先进的性能。

    

    睡眠异常可能会对健康产生严重的后果。自动化睡眠分期可以简化诊断过程。以往的自动化睡眠分期工作取得了很好的结果，主要依赖于 EEG 信号。但是，通常在 EEG 之外还有多个信息源可用。当 EEG 记录存在噪声甚至完全缺失时，这可能尤为有益。本文提出了 CoRe-Sleep，一种协调表示多模态融合网络，它特别关注于提高对不完善数据的信号分析的鲁棒性。我们展示了适当处理多模态信息可以是实现这种鲁棒性的关键。CoRe-Sleep 容忍噪声或缺失的模态片段，允许在不完整数据上进行训练。此外，它在使用单个模式进行多模态数据测试和单模态数据测试时都展现了最先进的性能。

    Sleep abnormalities can have severe health consequences. Automated sleep staging, i.e. labelling the sequence of sleep stages from the patient's physiological recordings, could simplify the diagnostic process. Previous work on automated sleep staging has achieved great results, mainly relying on the EEG signal. However, often multiple sources of information are available beyond EEG. This can be particularly beneficial when the EEG recordings are noisy or even missing completely. In this paper, we propose CoRe-Sleep, a Coordinated Representation multimodal fusion network that is particularly focused on improving the robustness of signal analysis on imperfect data. We demonstrate how appropriately handling multimodal information can be the key to achieving such robustness. CoRe-Sleep tolerates noisy or missing modalities segments, allowing training on incomplete data. Additionally, it shows state-of-the-art performance when testing on both multimodal and unimodal data using a single mode
    
[^24]: 图像生成模型的定性失败及其在检测Deepfakes中的应用

    Qualitative Failures of Image Generation Models and Their Application in Detecting Deepfakes. (arXiv:2304.06470v1 [cs.CV])

    [http://arxiv.org/abs/2304.06470](http://arxiv.org/abs/2304.06470)

    研究调查了图像生成模型的质量失误及其应用于检测Deepfakes，识别了五种定性缺陷。这些发现有助于改进模型并制定检测Deepfakes的策略。

    

    图像和视频生成模型创造出逼真的影像的能力已经达到了前所未有的高度，这使得在许多情况下很难区分真实和伪造的图像。然而，尽管取得了进展，但生成图像的质量和真实世界中的图像之间仍存在差距。为了解决这个问题，我们回顾了大量学术论文和社交媒体内容，以确定图像生成模型的定性缺陷，并将其分类为五类。通过了解这些失败，我们可以确定这些模型需要改进的领域，并制定检测Deepfakes的策略。今天社会中Deepfakes的普遍存在是一个严重的问题，我们的研究发现可以帮助减轻它们的负面影响。

    The ability of image and video generation models to create photorealistic images has reached unprecedented heights, making it difficult to distinguish between real and fake images in many cases. However, despite this progress, a gap remains between the quality of generated images and those found in the real world. To address this, we have reviewed a vast body of literature from both academic publications and social media to identify qualitative shortcomings in image generation models, which we have classified into five categories. By understanding these failures, we can identify areas where these models need improvement, as well as develop strategies for detecting deep fakes. The prevalence of deep fakes in today's society is a serious concern, and our findings can help mitigate their negative impact.
    
[^25]: 分析隐私-效用移动模型的公平性

    Analysing Fairness of Privacy-Utility Mobility Models. (arXiv:2304.06469v1 [cs.LG])

    [http://arxiv.org/abs/2304.06469](http://arxiv.org/abs/2304.06469)

    本研究为人类移动性定义了一组公平度量指标，基于轨迹的结构相似性和熵，研究了两种依靠生成对抗网络和表示学习降低用户的重新识别率的隐私保护模型，并探讨了它们的公平性优势和差异。

    

    在共享时空数据集中，保护个人的隐私对于防止基于唯一轨迹的重新识别攻击至关重要。现有的隐私技术往往提出理想的隐私-效用权衡，但基本忽略了移动模型的公平性影响，以及这些技术对不同用户群体是否同等适用。在时空背景下，公平性与隐私意识模型之间的度量仍然不清晰，并且几乎不存在任何定义的公平度量集。在本文中，我们定义了一组专为人类移动性而设计的公平度量指标，基于轨迹的结构相似性和熵。在这些定义下，我们研究了两种最先进的隐私保护模型，在数据共享中依靠生成对抗网络和表示学习降低用户的重新识别率，检查了其公平性。我们的结果表明，虽然这两种模型都保证在组上公平性方面具有优势，但它们在个别公平性方面存在显著区别。

    Preserving the individuals' privacy in sharing spatial-temporal datasets is critical to prevent re-identification attacks based on unique trajectories. Existing privacy techniques tend to propose ideal privacy-utility tradeoffs, however, largely ignore the fairness implications of mobility models and whether such techniques perform equally for different groups of users. The quantification between fairness and privacy-aware models is still unclear and there barely exists any defined sets of metrics for measuring fairness in the spatial-temporal context. In this work, we define a set of fairness metrics designed explicitly for human mobility, based on structural similarity and entropy of the trajectories. Under these definitions, we examine the fairness of two state-of-the-art privacy-preserving models that rely on GAN and representation learning to reduce the re-identification rate of users for data sharing. Our results show that while both models guarantee group fairness in terms of de
    
[^26]: Masakhane-Afrisenti在SemEval-2023任务12中的应用：使用非洲中心语言模型和适配器进行低资源非洲语言的情感分析

    Masakhane-Afrisenti at SemEval-2023 Task 12: Sentiment Analysis using Afro-centric Language Models and Adapters for Low-resource African Languages. (arXiv:2304.06459v1 [cs.CL])

    [http://arxiv.org/abs/2304.06459](http://arxiv.org/abs/2304.06459)

    该论文介绍了在SemEval-2023任务12中使用Afro-centric语言模型和适配器进行非洲低资源语言的情感分析。使用预训练的Afro-centric语言模型可以提高性能。使用适配器方法可以实现对于有限资源语言的零样本迁移。

    

    AfriSenti-SemEval共享任务12旨在为12种非洲语言执行单语情感分类（子任务A）、多语言情感分类（子任务B）和零样本情感分类（任务C）。对于子任务A，我们使用传统的机器学习分类器、非洲中心语言模型和特定语言模型进行了实验。对于任务B，我们微调了支持任务中多种语言的多语言预训练语言模型。对于任务C，我们使用参数高效的适配器方法，利用目标语言的单语文本实现有效的零样本迁移。我们的发现表明，使用预训练的非洲中心语言模型可以改善低资源非洲语言的性能。我们还使用适配器进行零样本任务的实验，结果表明，使用有限的资源可以获得有前途的结果。

    AfriSenti-SemEval Shared Task 12 of SemEval-2023. The task aims to perform monolingual sentiment classification (sub-task A) for 12 African languages, multilingual sentiment classification (sub-task B), and zero-shot sentiment classification (task C). For sub-task A, we conducted experiments using classical machine learning classifiers, Afro-centric language models, and language-specific models. For task B, we fine-tuned multilingual pre-trained language models that support many of the languages in the task. For task C, we used we make use of a parameter-efficient Adapter approach that leverages monolingual texts in the target language for effective zero-shot transfer. Our findings suggest that using pre-trained Afro-centric language models improves performance for low-resource African languages. We also ran experiments using adapters for zero-shot tasks, and the results suggest that we can obtain promising results by using adapters with a limited amount of resources.
    
[^27]: SpectFormer: 频率和注意力是视觉Transformer所需要的。

    SpectFormer: Frequency and Attention is what you need in a Vision Transformer. (arXiv:2304.06446v1 [cs.CV])

    [http://arxiv.org/abs/2304.06446](http://arxiv.org/abs/2304.06446)

    本文提出了结合多头注意力和谱层的Spectformer架构，可以得到更好的性能表现，提高了top-1准确率2%。

    

    视觉Transformer已成功地应用于图像识别任务中。其种类包括基于多头自我注意力机制（如ViT、DeIT）和基于谱层（如Fnet、GFNet、AFNO）的模型。本文发现，多头注意力和谱层都对Transformer起到重要作用，将两者结合可以得到更好的性能表现。因此提出了新的Spectformer架构，将多头注意力和谱层融合起来。实验表明，Spectformer可恰当地捕捉特征表示，与其他Transformer表征相比，可以提高top-1准确率2%。

    Vision transformers have been applied successfully for image recognition tasks. There have been either multi-headed self-attention based (ViT \cite{dosovitskiy2020image}, DeIT, \cite{touvron2021training}) similar to the original work in textual models or more recently based on spectral layers (Fnet\cite{lee2021fnet}, GFNet\cite{rao2021global}, AFNO\cite{guibas2021efficient}). We hypothesize that both spectral and multi-headed attention plays a major role. We investigate this hypothesis through this work and observe that indeed combining spectral and multi-headed attention layers provides a better transformer architecture. We thus propose the novel Spectformer architecture for transformers that combines spectral and multi-headed attention layers. We believe that the resulting representation allows the transformer to capture the feature representation appropriately and it yields improved performance over other transformer representations. For instance, it improves the top-1 accuracy by 2
    
[^28]: 带有鲁棒UNet去噪器的认证零阶黑盒防御

    Certified Zeroth-order Black-Box Defense with Robust UNet Denoiser. (arXiv:2304.06430v1 [cs.CV])

    [http://arxiv.org/abs/2304.06430](http://arxiv.org/abs/2304.06430)

    本文提出了一种带有鲁棒UNet去噪器的认证零阶黑盒防御方法，通过在黑盒模型之前预置RDUNet和DS或AE和RDUNet，成功提高了模型鲁棒性。

    

    最近黑盒设置中对于对抗性扰动的认证防御方法已经从零阶角度进行了研究，然而由于去噪器的设计不够有效，这些方法在高维数据集上存在高模型方差和低性能，且在使用零阶技术时存在局限性。为此，我们提出了一种认证的零阶预处理技术，仅使用模型查询即可从受攻击图像中去除对抗性扰动。我们提出了一种鲁棒的UNet去噪器（RDUNet），确保了对于高维数据集上训练的黑盒模型的鲁棒性。我们进一步提出了黑盒去噪平滑（DS）防御机制ZO-RUDS，通过将我们的RDUNet预置于黑盒模型之前，确保黑盒防御。我们还提出了ZO-AE-RUDS，在黑盒模型之前使用RDUNet和自编码器(AE)。我们在四个分类数据集上进行了广泛的实验。

    Certified defense methods against adversarial perturbations have been recently investigated in the black-box setting with a zeroth-order (ZO) perspective. However, these methods suffer from high model variance with low performance on high-dimensional datasets due to the ineffective design of the denoiser and are limited in their utilization of ZO techniques. To this end, we propose a certified ZO preprocessing technique for removing adversarial perturbations from the attacked image in the black-box setting using only model queries. We propose a robust UNet denoiser (RDUNet) that ensures the robustness of black-box models trained on high-dimensional datasets. We propose a novel black-box denoised smoothing (DS) defense mechanism, ZO-RUDS, by prepending our RDUNet to the black-box model, ensuring black-box defense. We further propose ZO-AE-RUDS in which RDUNet followed by autoencoder (AE) is prepended to the black-box model. We perform extensive experiments on four classification dataset
    
[^29]: 自监督ECG表征学习在心律失常检测中的应用研究：分布分析及实验探究

    In-Distribution and Out-of-Distribution Self-supervised ECG Representation Learning for Arrhythmia Detection. (arXiv:2304.06427v1 [cs.LG])

    [http://arxiv.org/abs/2304.06427](http://arxiv.org/abs/2304.06427)

    本文系统研究了自监督学习方法在ECG表征学习上的应用，首次对三个常用ECG心律失常数据集进行了分布分析，实验发现SwAV方法表现最佳，能够超越传统的有监督学习方法，还具有较强的鲁棒性，有望在大规模和多样化人群中检测心律失常。

    

    本文针对心电图(ECG)心律失常检测问题，系统地研究了自监督学习(Self-Supervised Learning, SSL)方法的有效性。我们首先对三个常用的ECG心律失常数据集进行了分布分析，并进行了综合性实验，使用不同增强和参数评估了各种SSL方法（如SimCRL、BYOL和SwAV）在ECG表征学习方面的有效性。实验结果表明，SwAV方法表现最佳。我们进一步进行了针对In-Distribution (ID)和Out-of-Distribution (OOD) ECG数据的交叉数据集训练和测试实验，结果表明SSL方法，特别是SwAV，在ECG表征学习方面具有很高的竞争力，并且对不同种类的ECG数据具有较强的鲁棒性，从而有望在大规模和多样化人群中检测心律失常。

    This paper presents a systematic investigation into the effectiveness of Self-Supervised Learning (SSL) methods for Electrocardiogram (ECG) arrhythmia detection. We begin by conducting a novel distribution analysis on three popular ECG-based arrhythmia datasets: PTB-XL, Chapman, and Ribeiro. To the best of our knowledge, our study is the first to quantify these distributions in this area. We then perform a comprehensive set of experiments using different augmentations and parameters to evaluate the effectiveness of various SSL methods, namely SimCRL, BYOL, and SwAV, for ECG representation learning, where we observe the best performance achieved by SwAV. Furthermore, our analysis shows that SSL methods achieve highly competitive results to those achieved by supervised state-of-the-art methods. To further assess the performance of these methods on both In-Distribution (ID) and Out-of-Distribution (OOD) ECG data, we conduct cross-dataset training and testing experiments. Our comprehensive
    
[^30]: 量化和解释预测流程监控中的机器学习不确定性：运筹学视角

    Quantifying and Explaining Machine Learning Uncertainty in Predictive Process Monitoring: An Operations Research Perspective. (arXiv:2304.06412v1 [cs.LG])

    [http://arxiv.org/abs/2304.06412](http://arxiv.org/abs/2304.06412)

    本论文提出了一种综合的、多阶段的机器学习方法，利用分位数回归森林生成区间预测来解决预测流程监控中的问题，同时使用SHapley可加解释来解释模型不确定性的来源。

    

    本文介绍了一种综合的、多阶段的机器学习方法，有效地将信息系统和人工智能融合，以增强运筹学领域内的决策过程。所提出的框架巧妙地解决了现有解决方案普遍存在的问题，比如忽略关键生产参数的数据驱动估计、仅生成点预测而不考虑模型不确定性以及缺乏关于这种不确定性来源的解释。我们的方法利用分位数回归森林生成区间预测，同时使用本地和全局变体的SHapley可加解释来解决研究的预测性过程监控问题。所提出的方法在一个真实的生产计划案例中得到了实际应用，并强调了规定性分析在精细化决策过程中的潜力。本文强调

    This paper introduces a comprehensive, multi-stage machine learning methodology that effectively integrates information systems and artificial intelligence to enhance decision-making processes within the domain of operations research. The proposed framework adeptly addresses common limitations of existing solutions, such as the neglect of data-driven estimation for vital production parameters, exclusive generation of point forecasts without considering model uncertainty, and lacking explanations regarding the sources of such uncertainty. Our approach employs Quantile Regression Forests for generating interval predictions, alongside both local and global variants of SHapley Additive Explanations for the examined predictive process monitoring problem. The practical applicability of the proposed methodology is substantiated through a real-world production planning case study, emphasizing the potential of prescriptive analytics in refining decision-making procedures. This paper accentuates
    
[^31]: 自适应人体姿势预测的元辅助学习

    Meta-Auxiliary Learning for Adaptive Human Pose Prediction. (arXiv:2304.06411v1 [cs.CV])

    [http://arxiv.org/abs/2304.06411](http://arxiv.org/abs/2304.06411)

    本文介绍了一种名为元辅助学习的方法，该方法利用自监视辅助任务帮助主要预测网络适应测试序列，以提高生成质量，特别是在罕见或未见运动类别的情况下。

    

    预测人体历史轨迹之后的高保真未来姿势对于智能机器人与人类的交互至关重要。深度端到端学习方法通常在外部数据集上训练一般化的预训练模型，并直接应用于所有测试样本，成为解决此问题的主要方案。但是，他们尚不完美，因为无法自适应特定序列的特殊属性（如动作风格，节奏）。更普遍地，一旦遇到未见过的运动类型（分布之外），预测的姿势往往不可靠。因此，我们提出了一种新的测试时自适应框架，利用两个自监督辅助任务帮助主要预测网络适应测试序列。

    Predicting high-fidelity future human poses, from a historically observed sequence, is decisive for intelligent robots to interact with humans. Deep end-to-end learning approaches, which typically train a generic pre-trained model on external datasets and then directly apply it to all test samples, emerge as the dominant solution to solve this issue. Despite encouraging progress, they remain non-optimal, as the unique properties (e.g., motion style, rhythm) of a specific sequence cannot be adapted. More generally, at test-time, once encountering unseen motion categories (out-of-distribution), the predicted poses tend to be unreliable. Motivated by this observation, we propose a novel test-time adaptation framework that leverages two self-supervised auxiliary tasks to help the primary forecasting network adapt to the test sequence. In the testing phase, our model can adjust the model parameters by several gradient updates to improve the generation quality. However, due to catastrophic f
    
[^32]: 利用三元组损失进行无监督的动作分割

    Leveraging triplet loss for unsupervised action segmentation. (arXiv:2304.06403v1 [cs.CV])

    [http://arxiv.org/abs/2304.06403](http://arxiv.org/abs/2304.06403)

    本文提出了一种无监督的框架，可以在不需要任何训练数据的情况下，从单个输入视频中学习适用于动作分割任务的动作表示，并使用三元组选择策略和三元组损失来在新的表示空间中发现动作，相对于现有无监督方法实现了更好的时间边界恢复质量。

    

    本文提出了一种全新的无监督框架，可以从单个输入视频中学习适用于动作分割任务的动作表示，而无需任何训练数据。我们的方法是一种深度度量学习方法，基于操作相似度分布的三元组损失和一种有效建模时间和语义先验以在新的表示空间中发现动作的三元组选择策略。在这些条件下，与现有的无监督方法相比，我们成功地恢复了学习到的动作表示中的时间边界，质量更高。我们在两个广泛使用的动作分割任务的基准数据集上评估了所提出的方法，通过在学习的表示上应用通用聚类算法，实现了竞争性能。

    In this paper, we propose a novel fully unsupervised framework that learns action representations suitable for the action segmentation task from the single input video itself, without requiring any training data. Our method is a deep metric learning approach rooted in a shallow network with a triplet loss operating on similarity distributions and a novel triplet selection strategy that effectively models temporal and semantic priors to discover actions in the new representational space. Under these circumstances, we successfully recover temporal boundaries in the learned action representations with higher quality compared with existing unsupervised approaches. The proposed method is evaluated on two widely used benchmark datasets for the action segmentation task and it achieves competitive performance by applying a generic clustering algorithm on the learned representations.
    
[^33]: VISION DIFFMASK：具有可微分补丁掩码的视觉Transformer的忠实解释

    VISION DIFFMASK: Faithful Interpretation of Vision Transformers with Differentiable Patch Masking. (arXiv:2304.06391v1 [cs.CV])

    [http://arxiv.org/abs/2304.06391](http://arxiv.org/abs/2304.06391)

    VISION DIFFMASK提出了一种可解释性的方法，通过使用门控机制识别最小输入子集来预测对其最终预测有贡献的输入部分。

    

    尽管具有高效性，但Vision Transformer缺乏可解释性可能会阻碍其在关键实际应用中的使用。为了克服这个问题，我们提出了一种名为VISION DIFFMASK的事后可解释性方法，该方法使用模型隐藏层的激活来预测对其最终预测有贡献的输入部分。我们的方法使用门控机制来识别保留预测类别分布的最小原始输入子集。我们通过引入忠实度任务并在CIFAR-10和ImageNet-1K上与其他最先进的归因方法进行比较，证明了我们方法的忠实度，取得了令人信服的结果。为了促进我们工作的再现性和进一步扩展，我们公开了我们的实现：https://github.com/AngelosNal/Vision-DiffMask

    The lack of interpretability of the Vision Transformer may hinder its use in critical real-world applications despite its effectiveness. To overcome this issue, we propose a post-hoc interpretability method called VISION DIFFMASK, which uses the activations of the model's hidden layers to predict the relevant parts of the input that contribute to its final predictions. Our approach uses a gating mechanism to identify the minimal subset of the original input that preserves the predicted distribution over classes. We demonstrate the faithfulness of our method, by introducing a faithfulness task, and comparing it to other state-of-the-art attribution methods on CIFAR-10 and ImageNet-1K, achieving compelling results. To aid reproducibility and further extension of our work, we open source our implementation: https://github.com/AngelosNal/Vision-DiffMask
    
[^34]: 神经网络中符号的出现与语义理解和交流

    Emergence of Symbols in Neural Networks for Semantic Understanding and Communication. (arXiv:2304.06377v1 [cs.AI])

    [http://arxiv.org/abs/2304.06377](http://arxiv.org/abs/2304.06377)

    本文介绍了一种名为SEA-net的神经网络解决方案，可以生成符号，实现语义理解和交流。这些符号可以捕捉到组成性语义信息，并呈现类似自然语言的内在结构。

    

    能够创造有意义的符号，并熟练地将它们用于更高的认知功能，如交流、推理、规划等，是人类智能的重要和独特之处。 目前，深度神经网络仍远远落后于人类创造符号进行这些高级认知功能的能力。本文提出了一种名为SEA-net的解决方案，使神经网络具有符号创造、语义理解和交流能力。SEA-net生成动态配置网络以执行特定任务的符号。这些符号捕捉了组成性语义信息，使系统能够通过纯符号操作或交流获得新功能。此外，我们发现这些自动生成的符号呈现出类似自然语言的内在结构，表明在人类大脑和人工神经网络中生成和理解符号的共同框架。我们希望这将成为将来发展人工智能的助推器。

    Being able to create meaningful symbols and proficiently use them for higher cognitive functions such as communication, reasoning, planning, etc., is essential and unique for human intelligence. Current deep neural networks are still far behind human's ability to create symbols for such higher cognitive functions. Here we propose a solution, named SEA-net, to endow neural networks with ability of symbol creation, semantic understanding and communication. SEA-net generates symbols that dynamically configure the network to perform specific tasks. These symbols capture compositional semantic information that enables the system to acquire new functions purely by symbolic manipulation or communication. In addition, we found that these self-generated symbols exhibit an intrinsic structure resembling that of natural language, suggesting a common framework underlying the generation and understanding of symbols in both human brains and artificial neural networks. We hope that it will be instrum
    
[^35]: 超图认知网络作为知识的特征丰富模型

    Towards hypergraph cognitive networks as feature-rich models of knowledge. (arXiv:2304.06375v1 [cs.CL])

    [http://arxiv.org/abs/2304.06375](http://arxiv.org/abs/2304.06375)

    本研究提出了特征丰富认知超图作为人类记忆定量模型来预测概念特征，与成对连接和缺乏特征的网络模型相比，特征丰富认知超图表现更好，并且涉及更高阶关联的超链接在具有相似心理语言学特征的概念之间优先形成，这反映了共享认知维度的存在。

    

    语义网络是理解如何从记忆中检索相关概念的有用工具。然而，大多数现有的网络方法使用成对连接表示记忆召回模式。成对连接忽略了更高阶的关联，即一次涉及两个以上概念的关系。这些更高阶的交互可能与大脑灰质结构特征，如兴奋、愉悦、熟悉度、性别等有关。我们通过引入特征丰富认知超图来克服这些限制，作为人类记忆的定量模型：（i）一起回忆的概念可以同时参与包含两个以上概念的超链接（认知超图方面）；（ii）每个概念都具有心理语言学特征向量（特征丰富方面）。我们从词汇联想数据中构建超图，并使用机器学习特征评估方法来预测概念特征。我们的结果表明，相对于成对连接和缺乏特征的网络模型，特征丰富认知超图在预测心理语言学特征方面表现更好。此外，我们还表明，涉及更高阶关联的超链接优先形成在具有相似心理语言学特征的概念之间。这表明，人类记忆的结构涉及反映共享认知维度的更高阶关联，这些维度可以用于将概念组织成语义空间。

    Semantic networks provide a useful tool to understand how related concepts are retrieved from memory. However, most current network approaches use pairwise links to represent memory recall patterns. Pairwise connections neglect higher-order associations, i.e. relationships between more than two concepts at a time. These higher-order interactions might covariate with (and thus contain information about) how similar concepts are along psycholinguistic dimensions like arousal, valence, familiarity, gender and others. We overcome these limits by introducing feature-rich cognitive hypergraphs as quantitative models of human memory where: (i) concepts recalled together can all engage in hyperlinks involving also more than two concepts at once (cognitive hypergraph aspect), and (ii) each concept is endowed with a vector of psycholinguistic features (feature-rich aspect). We build hypergraphs from word association data and use evaluation methods from machine learning features to predict concep
    
[^36]: AGIEval：一个以人为中心的基准评估基础模型的工具

    AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. (arXiv:2304.06364v1 [cs.CL])

    [http://arxiv.org/abs/2304.06364](http://arxiv.org/abs/2304.06364)

    AGIEval是一个以人为中心设计的基准测试工具，用于评估基础模型在人类中心标准化考试上的表现。GPT-4在SAT、LSAT和数学比赛方面超越了人类平均表现，展示了当代基础模型在人类级任务中的非凡性能。

    

    评估基础模型解决人类级别任务的通用能力是它们在发展和应用AGI（人工通用智能）中的重要方面。传统基准测试依赖于人造数据集，可能无法准确代表人类水平能力。在本文中，我们介绍了AGIEval，一个专门设计用于评估基础模型在人类中心标准化考试的基准测试工具，例如大学入学考试，法律学校入学考试，数学竞赛和律师资格考试。我们使用这个基准测试工具评估了几种最先进的基础模型，包括 GPT-4，ChatGPT 和Text-Davinci-003。令人印象深刻的是，GPT-4在SAT、LSAT和数学比赛方面超越了人类平均表现，SAT数学测试的准确率达到了95%，在中国国家大学英语考试的英语测试中准确率也达到了92.5%。这展示了当代基础模型在人类级任务中的非凡性能，并凸显了AGI未来发展的潜力。

    Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary fou
    
[^37]: 通过有限宽度的反模型查询一阶理论的可决定性

    Decidability of Querying First-Order Theories via Countermodels of Finite Width. (arXiv:2304.06348v1 [cs.LO])

    [http://arxiv.org/abs/2304.06348](http://arxiv.org/abs/2304.06348)

    通过有限宽度的反模型查询一阶理论的可决定性并提出分割宽度，使其能够捕获实际相关的查询语言

    

    我们提出了一个通用框架，基于具有结构简单的反模型的存在性（通过某些类型的宽度量来衡量，包括树宽和团宽等），为广泛的逻辑蕴含问题（简称查询）的可决定性提供了支持。作为我们框架的一个重要特例，我们确定了展现出宽度有限有限通用模型集的逻辑，保证了各种同态封闭查询的可决定性，包括了各种实际相关的查询语言。作为一个特别强大的宽度量，我们提出了Blumensath的分割宽度，该量包含了各种通常考虑的宽度量，具有非常有利的计算和结构特性。针对普遍展现存在性规则为一个展示案例，我们解释了有限分割宽度规则集包含其他已知的抽象可决定类，但借助现有的分层和受控规则集概念，也使我们能够捕获实际相关的查询语言，例如正则，连接和布尔连接查询。我们以存在规则的形式为重点，补充我们的理论结果，并进行了彻底的实验评估，展示了我们的框架在各种高级知识处理场景中的实际适用性和可伸缩性。

    We propose a generic framework for establishing the decidability of a wide range of logical entailment problems (briefly called querying), based on the existence of countermodels that are structurally simple, gauged by certain types of width measures (with treewidth and cliquewidth as popular examples). As an important special case of our framework, we identify logics exhibiting width-finite finitely universal model sets, warranting decidable entailment for a wide range of homomorphism-closed queries, subsuming a diverse set of practically relevant query languages. As a particularly powerful width measure, we propose Blumensath's partitionwidth, which subsumes various other commonly considered width measures and exhibits highly favorable computational and structural properties. Focusing on the formalism of existential rules as a popular showcase, we explain how finite partitionwidth sets of rules subsume other known abstract decidable classes but -- leveraging existing notions of strat
    
[^38]: DDT：双支变形Transformer用于图像去噪

    DDT: Dual-branch Deformable Transformer for Image Denoising. (arXiv:2304.06346v1 [cs.CV])

    [http://arxiv.org/abs/2304.06346](http://arxiv.org/abs/2304.06346)

    本文提出了一种高效的双支变形Transformer（DDT）去噪网络，可以同时捕捉局部和全局交互，并且应用变形注意操作可以专注于更重要的区域，最终在实际和合成去噪任务上取得了最先进的性能以及显著降低的计算成本。

    

    Transformer对于图像去噪任务非常有益，因为它可以建模长程依赖关系来克服归纳卷积偏差带来的限制。然而，直接应用Transformer结构来去除噪声是具有挑战性的，因为其复杂度与空间分辨率呈二次增长。在本文中，我们提出了一种高效的双支变形Transformer（DDT）去噪网络，它可以同时捕捉局部和全局交互。我们在本地和全局支架上使用固定块尺寸和固定块数来划分特征。此外，我们在两个支架上应用变形注意操作，这有助于网络专注于更重要的区域，并进一步降低计算复杂度。我们在实际和合成去噪任务上进行了广泛的实验，结果表明，提出的DDT具有最先进的性能，且计算成本显著降低。

    Transformer is beneficial for image denoising tasks since it can model long-range dependencies to overcome the limitations presented by inductive convolutional biases. However, directly applying the transformer structure to remove noise is challenging because its complexity grows quadratically with the spatial resolution. In this paper, we propose an efficient Dual-branch Deformable Transformer (DDT) denoising network which captures both local and global interactions in parallel. We divide features with a fixed patch size and a fixed number of patches in local and global branches, respectively. In addition, we apply deformable attention operation in both branches, which helps the network focus on more important regions and further reduces computational complexity. We conduct extensive experiments on real-world and synthetic denoising tasks, and the proposed DDT achieves state-of-the-art performance with significantly fewer computational costs.
    
[^39]: ASR: 像注意力一样的结构再参数化

    ASR: Attention-alike Structural Re-parameterization. (arXiv:2304.06345v1 [cs.CV])

    [http://arxiv.org/abs/2304.06345](http://arxiv.org/abs/2304.06345)

    该论文提出的ASR技术是一种新颖的深度学习技术，通过等效参数转换实现不同网络体系结构之间的互转。和现有的SRP方法相比，ASR可以成功考虑自注意模块，实现推理期间的性能提升，并在工业和实际应用中具有巨大潜力。

    

    结构再参数化（SRP）技术是一种新颖的深度学习技术，通过等效参数转换实现不同网络体系结构之间的互转。该技术使得在推理过程中通过这些转换减少性能提升的新增代价，例如参数大小和推理时间，因此SRP在工业和实际应用中具有巨大潜力。现有的SRP方法已成功考虑了许多常用的架构，例如归一化、池化方法、多分支卷积等。然而，广泛使用的自注意模块由于在推理期间通常以乘法方式作用于骨干网络并且模块的输出在推理时依赖于输入，所以无法直接实现SRP，而这限制了SRP的应用场景。在本文中，我们从统计角度进行了广泛的实验，并发现...

    The structural re-parameterization (SRP) technique is a novel deep learning technique that achieves interconversion between different network architectures through equivalent parameter transformations. This technique enables the mitigation of the extra costs for performance improvement during training, such as parameter size and inference time, through these transformations during inference, and therefore SRP has great potential for industrial and practical applications. The existing SRP methods have successfully considered many commonly used architectures, such as normalizations, pooling methods, multi-branch convolution. However, the widely used self-attention modules cannot be directly implemented by SRP due to these modules usually act on the backbone network in a multiplicative manner and the modules' output is input-dependent during inference, which limits the application scenarios of SRP. In this paper, we conduct extensive experiments from a statistical perspective and discover
    
[^40]: 敏捷预测模型开发的简化框架提高库存管理效率

    Streamlined Framework for Agile Forecasting Model Development towards Efficient Inventory Management. (arXiv:2304.06344v1 [cs.LG])

    [http://arxiv.org/abs/2304.06344](http://arxiv.org/abs/2304.06344)

    本文提出了一个敏捷预测模型开发的框架，通过简化核心组件之间的连接使得新数据集能够快速和稳健地集成，并选择最佳模型。通过不同的评估指标，找到最适合不同应用的模型。该框架能够应用在库存管理环境中，提高效率。

    

    本文提出了一个框架，通过简化开发过程的核心组件之间的连接来开发预测模型。该框架使得新数据集能够快速和稳健地集成，实验不同算法，并选择最佳模型。我们从不同问题的数据集入手，并应用预处理步骤来清理和提取时间序列数据的有意义表示。为了确定稳健的训练配置，我们引入了一种新的多重交叉验证策略机制。我们应用不同的评估指标来找到最适合不同应用的模型。其中之一是我们参加了美国国际开发署（USAID）组织的智能预测竞赛。最后，我们利用框架的灵活性，应用不同的评估指标来评估模型在库存管理环境中的性能。

    This paper proposes a framework for developing forecasting models by streamlining the connections between core components of the developmental process. The proposed framework enables swift and robust integration of new datasets, experimentation on different algorithms, and selection of the best models. We start with the datasets of different issues and apply pre-processing steps to clean and engineer meaningful representations of time-series data. To identify robust training configurations, we introduce a novel mechanism of multiple cross-validation strategies. We apply different evaluation metrics to find the best-suited models for varying applications. One of the referent applications is our participation in the intelligent forecasting competition held by the United States Agency of International Development (USAID). Finally, we leverage the flexibility of the framework by applying different evaluation metrics to assess the performance of the models in inventory management settings.
    
[^41]: 多属性多阶图卷积神经网络用于异构图

    Attributed Multi-order Graph Convolutional Network for Heterogeneous Graphs. (arXiv:2304.06336v1 [cs.LG])

    [http://arxiv.org/abs/2304.06336](http://arxiv.org/abs/2304.06336)

    本文提出了一个AMOGCN模型，它自动从多阶邻接矩阵的自适应聚合中研究包含多跳邻居的元路径，并使用节点属性评价监督。其能够有效地从异构图中发现有区别的节点嵌入和关系。

    

    异构图神经网络旨在从多关系网络中发现有区别的节点嵌入和关系。异构图学习的一个挑战是设计可学习的元路径，它显着地影响了学习到的嵌入的质量。因此，在本文中，我们提出了一个带属性的多阶图卷积网络（AMOGCN），它自动从多阶邻接矩阵的自适应聚合中研究包含多跳邻居的元路径。该模型首先从手动设计的节点连接中构建不同阶数的邻接矩阵。之后，从各种阶数的邻接矩阵的自动融合中附加一个完整的多阶邻接矩阵。这个过程由从节点同质性通过属性评价提取的节点语义信息监督。最终，我们使用一个学习到的多阶邻接矩阵的一层简化图卷积网络。

    Heterogeneous graph neural networks aim to discover discriminative node embeddings and relations from multi-relational networks.One challenge of heterogeneous graph learning is the design of learnable meta-paths, which significantly influences the quality of learned embeddings.Thus, in this paper, we propose an Attributed Multi-Order Graph Convolutional Network (AMOGCN), which automatically studies meta-paths containing multi-hop neighbors from an adaptive aggregation of multi-order adjacency matrices. The proposed model first builds different orders of adjacency matrices from manually designed node connections. After that, an intact multi-order adjacency matrix is attached from the automatic fusion of various orders of adjacency matrices. This process is supervised by the node semantic information, which is extracted from the node homophily evaluated by attributes. Eventually, we utilize a one-layer simplifying graph convolutional network with the learned multi-order adjacency matrix,
    
[^42]: NeRFVS:基于几何支撑的神经辐射场实现自由视角合成

    NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds. (arXiv:2304.06287v1 [cs.CV])

    [http://arxiv.org/abs/2304.06287](http://arxiv.org/abs/2304.06287)

    NeRFVS是一种利用神经重建的“全局信息”，包括伪深度图和视角覆盖信息，基于几何支撑的神经辐射场方法。该方法在实现室内自由导航方面表现出色且可减少可见的伪影。

    

    本文提出了一种名为NeRFVS的新型神经辐射场（NeRF）方法，以实现在室内自由导航的功能。该方法利用神经重建的“全局信息”，包括伪深度图和视角覆盖信息，从而指导3D室内场景的隐式神经表示的学习。实验表明，我们的方法在定量指标和视觉质量方面均优于现有方法。

    We present NeRFVS, a novel neural radiance fields (NeRF) based method to enable free navigation in a room. NeRF achieves impressive performance in rendering images for novel views similar to the input views while suffering for novel views that are significantly different from the training views. To address this issue, we utilize the holistic priors, including pseudo depth maps and view coverage information, from neural reconstruction to guide the learning of implicit neural representations of 3D indoor scenes. Concretely, an off-the-shelf neural reconstruction method is leveraged to generate a geometry scaffold. Then, two loss functions based on the holistic priors are proposed to improve the learning of NeRF: 1) A robust depth loss that can tolerate the error of the pseudo depth map to guide the geometry learning of NeRF; 2) A variance loss to regularize the variance of implicit neural representations to reduce the geometry and color ambiguity in the learning procedure. These two loss
    
[^43]: 模型驱动的动态盾型保障用于安全和高效的多智能体强化学习

    Model-based Dynamic Shielding for Safe and Efficient Multi-Agent Reinforcement Learning. (arXiv:2304.06281v1 [cs.LG])

    [http://arxiv.org/abs/2304.06281](http://arxiv.org/abs/2304.06281)

    本论文介绍了模型驱动的动态屏蔽设计用于多智能体强化学习中的安全保障，屏蔽器能够动态分割、合并和重新计算智能体状态，同时支持更加高效的合成屏蔽器以监控复杂环境中的智能体。

    

    多智能体强化学习(MARL)发现最大化回报的策略，但在学习和部署阶段没有安全保障。虽然线性时间逻辑(LTL)的屏蔽是确保单智能体强化学习(RL)安全的有前途的正式方法，但它在扩展到多智能体场景时会导致保守行为。此外，在复杂多智能体环境中合成屏蔽存在计算挑战。本文介绍了MBDS以支持MARL算法设计。我们的算法合成分布式屏蔽器，这些屏蔽器是与每个MARL智能体并行运行的反应系统，用于监控和纠正不安全的行为。这种设计使得在没有协调开销的情况下，能够有效合成屏蔽器以监视复杂环境中的智能体。我们还提出一种算法，在不知道环境的完整转换函数的情况下合成屏障，并展示我们的方法在交通信号控制任务和无人机巡逻任务中优于LTL屏蔽。

    Multi-Agent Reinforcement Learning (MARL) discovers policies that maximize reward but do not have safety guarantees during the learning and deployment phases. Although shielding with Linear Temporal Logic (LTL) is a promising formal method to ensure safety in single-agent Reinforcement Learning (RL), it results in conservative behaviors when scaling to multi-agent scenarios. Additionally, it poses computational challenges for synthesizing shields in complex multi-agent environments. This work introduces Model-based Dynamic Shielding (MBDS) to support MARL algorithm design. Our algorithm synthesizes distributive shields, which are reactive systems running in parallel with each MARL agent, to monitor and rectify unsafe behaviors. The shields can dynamically split, merge, and recompute based on agents' states. This design enables efficient synthesis of shields to monitor agents in complex environments without coordination overheads. We also propose an algorithm to synthesize shields witho
    
[^44]: 基于主动学习的改进策略优化多领域性能

    Optimizing Multi-Domain Performance with Active Learning-based Improvement Strategies. (arXiv:2304.06277v1 [cs.LG])

    [http://arxiv.org/abs/2304.06277](http://arxiv.org/abs/2304.06277)

    本文提出了一个基于主动学习的框架，用于改善多领域性能。该方法分为两个阶段，在保证高效性的前提下实现了最先进的性能。

    

    提高多领域性能是一个具有挑战性的任务，通常需要大量的数据来训练和测试模型。主动学习技术通过使模型选择最具信息量的样本进行标记，从而减少了实现高性能所需的标记数据量，提供了一个有希望的解决方案。本文提出了一个基于主动学习的框架，用于改进多个领域的性能。我们的方法分为两个阶段：首先，我们使用一组初始标记数据来训练基础模型，然后我们迭代地选择最具信息量的样本进行标记，以改进模型。我们在多个数据集上评估了我们的方法，包括图像分类、情感分析和物体识别。我们的实验表明，我们的方法始终优于基准方法，并在多个数据集上实现了最先进的性能。我们还表明，我们的方法高效，只需要很小的标记数据量就能取得更好的性能。

    Improving performance in multiple domains is a challenging task, and often requires significant amounts of data to train and test models. Active learning techniques provide a promising solution by enabling models to select the most informative samples for labeling, thus reducing the amount of labeled data required to achieve high performance. In this paper, we present an active learning-based framework for improving performance across multiple domains. Our approach consists of two stages: first, we use an initial set of labeled data to train a base model, and then we iteratively select the most informative samples for labeling to refine the model. We evaluate our approach on several multi-domain datasets, including image classification, sentiment analysis, and object recognition. Our experiments demonstrate that our approach consistently outperforms baseline methods and achieves state-of-the-art performance on several datasets. We also show that our method is highly efficient, requirin
    
[^45]: 基于视觉的室内农场环境下番茄尺寸测量系统

    Visual based Tomato Size Measurement System for an Indoor Farming Environment. (arXiv:2304.06177v1 [cs.CV])

    [http://arxiv.org/abs/2304.06177](http://arxiv.org/abs/2304.06177)

    本文提出了一种基于机器学习模型和深度图像的番茄尺寸测量方法，该方法能够解决现有视觉系统在果园环境下遇到的遮挡和可扩展性问题，经过实验室测试取得了较高的测量精度。

    

    随着技术的进步，智能自动化系统将在农业行业中扮演越来越重要的角色。当前用于产量估计的现有视觉系统面临着遮挡和可扩展性方面的困难，因为它们采用的相机系统大且昂贵，不适合果园环境。为了克服这些问题，本文提出了一种尺寸测量方法，结合机器学习模型和从三个低成本的RGBD相机捕捉的深度图像来检测和测量番茄的高度和宽度。该系统在实验室环境下使用真实的番茄果实和假叶进行了性能评估，以模拟真实农场环境中的遮挡。为了通过解决水果遮挡来提高准确性，我们的三摄像头系统能够实现高度测量精度为0.9114，宽度精度为0.9443。

    As technology progresses, smart automated systems will serve an increasingly important role in the agricultural industry. Current existing vision systems for yield estimation face difficulties in occlusion and scalability as they utilize a camera system that is large and expensive, which are unsuitable for orchard environments. To overcome these problems, this paper presents a size measurement method combining a machine learning model and depth images captured from three low cost RGBD cameras to detect and measure the height and width of tomatoes. The performance of the presented system is evaluated on a lab environment with real tomato fruits and fake leaves to simulate occlusion in the real farm environment. To improve accuracy by addressing fruit occlusion, our three-camera system was able to achieve a height measurement accuracy of 0.9114 and a width accuracy of 0.9443.
    
[^46]: 一种基于神经网络算法的Dubins'小车拦截沿已知轨迹移动的目标方法

    Neural Network Algorithm for Intercepting Targets Moving Along Known Trajectories by a Dubins' Car. (arXiv:2304.06169v1 [math.OC])

    [http://arxiv.org/abs/2304.06169](http://arxiv.org/abs/2304.06169)

    本文使用无监督学习的神经网络方法，基于深度确定性策略梯度算法，解决了 Dubins'小车沿已知轨迹拦截目标的最短时间控制问题， 建立了数学模型并进行了模型实验，证明了其有效性。

    

    本文将Dubins'小车沿直线或圆形轨迹拦截目标的任务，制定为一个以拦截瞬间小车速度方向任意的最短时间控制问题。为了解决这个问题和综合拦截轨迹，使用深度确定性策略梯度算法基于无监督学习的神经网络方法。对比分析了所得控制律和拦截轨迹与拦截问题的解析解。进行了神经网络未在训练期间看到的目标运动参数的数学建模。进行模型实验，以测试神经解的稳定性。证明了使用神经网络方法综合给定目标运动类别的拦截轨迹的有效性。

    The task of intercepting a target moving along a rectilinear or circular trajectory by a Dubins' car is formulated as a time-optimal control problem with an arbitrary direction of the car's velocity at the interception moment. To solve this problem and to synthesize interception trajectories, neural network methods of unsupervised learning based on the Deep Deterministic Policy Gradient algorithm are used. The analysis of the obtained control laws and interception trajectories in comparison with the analytical solutions of the interception problem is performed. The mathematical modeling for the parameters of the target movement that the neural network had not seen before during training is carried out. Model experiments are conducted to test the stability of the neural solution. The effectiveness of using neural network methods for the synthesis of interception trajectories for given classes of target movements is shown.
    
[^47]: 农业领域的人工通用智能研究

    AGI for Agriculture. (arXiv:2304.06136v1 [cs.AI])

    [http://arxiv.org/abs/2304.06136](http://arxiv.org/abs/2304.06136)

    本文探讨了农业领域应用AGI的潜在机会，包括提高农产品产量、减少浪费和促进可持续农业实践，以及利用实时数据帮助农民做出更明智的决策。

    

    人工通用智能（AGI）有望革新包括医疗保健、金融、交通和教育在内的各个领域。在医疗保健领域，AGI被用于分析临床医学记录，识别患者数据中的模式，并帮助患者管理。农业是影响全球个体生活的另一关键领域。它作为提供粮食、纤维和燃料的基础，却面临着气候变化、土壤退化、水资源匮乏和粮食安全等多种挑战。AGI有能力通过提高作物产量、减少浪费和促进可持续农业实践来解决这些问题。它还可以通过利用实时数据帮助农民做出明智决策，实现更高效和有效的农业管理。本文探讨了AGI在农业中的潜在应用，如农业图像处理、自然语言处理（NLP）、机器人技术、知识图谱和深度学习等。

    Artificial General Intelligence (AGI) is poised to revolutionize a variety of sectors, including healthcare, finance, transportation, and education. Within healthcare, AGI is being utilized to analyze clinical medical notes, recognize patterns in patient data, and aid in patient management. Agriculture is another critical sector that impacts the lives of individuals worldwide. It serves as a foundation for providing food, fiber, and fuel, yet faces several challenges, such as climate change, soil degradation, water scarcity, and food security. AGI has the potential to tackle these issues by enhancing crop yields, reducing waste, and promoting sustainable farming practices. It can also help farmers make informed decisions by leveraging real-time data, leading to more efficient and effective farm management. This paper delves into the potential future applications of AGI in agriculture, such as agriculture image processing, natural language processing (NLP), robotics, knowledge graphs, a
    
[^48]: 分析ChatGPT在计算机工程入门课程中的适应能力

    Analyzing ChatGPT's Aptitude in an Introductory Computer Engineering Course. (arXiv:2304.06122v1 [cs.CY])

    [http://arxiv.org/abs/2304.06122](http://arxiv.org/abs/2304.06122)

    本文研究探讨了ChatGPT在计算机工程入门课程中的应用，发现它可以回答普通概念的问题，但无法处理带有图表或图形的问题，也无法进行实验室的现场操作。

    

    ChatGPT最近受到了公众和学术界的关注，因为它能够生成合理且听起来像人类回答的各种问题的文本答案。 ChatGPT在学术或课堂环境中回答各种问题甚至生成整篇论文的潜在用途或滥用受到了关注。尽管最近的研究探讨了ChatGPT在人文学科、商学院或医学院的应用，但本研究探讨了ChatGPT在计算机工程入门课程中的表现。本研究评估了ChatGPT在入门级计算机工程课程中回答测验、作业、考试和实验室问题的适应能力。本研究发现，ChatGPT在询问普通概念的问题上表现良好。然而，显然，作为一个仅限于文本的工具，它无法处理具有图表或图形的问题，也无法生成图表和图形。同时，这个工具也无法进行实验室的现场操作。

    ChatGPT has recently gathered attention from the general public and academia as a tool that is able to generate plausible and human-sounding text answers to various questions. One potential use, or abuse, of ChatGPT is in answering various questions or even generating whole essays and research papers in an academic or classroom setting. While recent works have explored the use of ChatGPT in the context of humanities, business school, or medical school, this work explores how ChatGPT performs in the context of an introductory computer engineering course. This work assesses ChatGPT's aptitude in answering quizzes, homework, exam, and laboratory questions in an introductory-level computer engineering course. This work finds that ChatGPT can do well on questions asking about generic concepts. However, predictably, as a text-only tool, it cannot handle questions with diagrams or figures, nor can it generate diagrams and figures. Further, also clearly, the tool cannot do hands-on lab experim
    
[^49]: IoT 信任与声誉：一份调查和分类

    IoT trust and reputation: a survey and taxonomy. (arXiv:2304.06119v1 [cs.CY])

    [http://arxiv.org/abs/2304.06119](http://arxiv.org/abs/2304.06119)

    本文通过对现有文献进行梳理，提出了一份物联网信任和声誉的调查和分类，分析了现有模型和机制的优点和缺点，提出了该领域的挑战和开放问题，并为未来的研究提供了指导方向。

    

    物联网是增长最快的技术之一，预计到2030年底，全球将利用超过十亿个设备。为了最大化这些连接实体的能力，物联网实体之间的信任和声誉至关重要。在物联网环境中已经提出了几个信任管理模型；然而，这些方案尚未充分解决物联网设备的特性，例如设备角色、设备类型以及其在智能环境中的动态行为。因此，传统的信任和声誉模型不足以应对这些特征和连接节点到网络时的不确定风险。虽然已经进行了持续的研究并且许多文章在受限环境下提出了有前途的解决方案，但是信任和声誉的研究仍处于起步阶段。在本文中，我们对物联网设备和系统的信任和声誉的现有研究进行了全面的文献综述。具体而言，我们提出了一份现有物联网信任模型和机制的调查和分类，并分析了它们的优点和缺点。我们确定了该领域的挑战和开放问题，包括需要针对资源受限的物联网设备设计高效、强健的信任和声誉管理机制，并提出了未来的研究方向。

    IoT is one of the fastest-growing technologies and it is estimated that more than a billion devices would be utilized across the globe by the end of 2030. To maximize the capability of these connected entities, trust and reputation among IoT entities is essential. Several trust management models have been proposed in the IoT environment; however, these schemes have not fully addressed the IoT devices features, such as devices role, device type and its dynamic behavior in a smart environment. As a result, traditional trust and reputation models are insufficient to tackle these characteristics and uncertainty risks while connecting nodes to the network. Whilst continuous study has been carried out and various articles suggest promising solutions in constrained environments, research on trust and reputation is still at its infancy. In this paper, we carry out a comprehensive literature review on state-of-the-art research on the trust and reputation of IoT devices and systems. Specifically
    
[^50]: AutoShot：一份短视频数据集和最新的镜头边界检测技术

    AutoShot: A Short Video Dataset and State-of-the-Art Shot Boundary Detection. (arXiv:2304.06116v1 [cs.CV])

    [http://arxiv.org/abs/2304.06116](http://arxiv.org/abs/2304.06116)

    AutoShot 发布了一份新的短视频镜头边界检测数据集，采用名为 AutoShot 的方法进行模型设计优化，比之前的最新技术水平在 F1 分数上获得更高的准确性。

    

    短视频在新的社交媒体趋势中爆发性地流行起来。镜头边界检测（SBD）是各种场景中最基本的组成部分之一，对于视频内容的创建和理解至关重要。 在本研究中，我们发布了一个名为SHOT的新的公共短视频镜头边界检测数据集，包括853个完整的短视频和11,606个镜头注释，其中包括200个测试视频中的2,716个高质量的镜头边界注释。利用这个新的数据财富，我们提出了一种名为AutoShot的方法，通过在包含各种先进的3D ConvNets和Transformers的搜索空间中进行神经架构搜索，来优化镜头边界检测模型的设计。我们的方法在F1分数上比先前的最先进方法实现了更高的准确性，例如在超过TransNetV2 4.2％的情况下获得更好的性能。

    The short-form videos have explosive popularity and have dominated the new social media trends. Prevailing short-video platforms,~\textit{e.g.}, Kuaishou (Kwai), TikTok, Instagram Reels, and YouTube Shorts, have changed the way we consume and create content. For video content creation and understanding, the shot boundary detection (SBD) is one of the most essential components in various scenarios. In this work, we release a new public Short video sHot bOundary deTection dataset, named SHOT, consisting of 853 complete short videos and 11,606 shot annotations, with 2,716 high quality shot boundary annotations in 200 test videos. Leveraging this new data wealth, we propose to optimize the model design for video SBD, by conducting neural architecture search in a search space encapsulating various advanced 3D ConvNets and Transformers. Our proposed approach, named AutoShot, achieves higher F1 scores than previous state-of-the-art approaches, e.g., outperforming TransNetV2 by 4.2%, when bein
    
[^51]: 采用3DG-GA（基因算法的深度去标识化匿名数据集增广）生成人工面部药物滥用图像。

    Generation of artificial facial drug abuse images using Deep De-identified anonymous Dataset augmentation through Genetics Algorithm (3DG-GA). (arXiv:2304.06106v1 [cs.CV])

    [http://arxiv.org/abs/2304.06106](http://arxiv.org/abs/2304.06106)

    本研究采用3DG-GA方法生成逼真的药物滥用面部特征的人工合成图像数据集，该数据集将对生物医学和人工智能研究产生影响。

    

    在生物医学研究和人工智能领域，获得大规模、良好平衡、代表性强的数据集对于开发可信任的应用程序至关重要，但是获得这样的数据集可能具有挑战性，因为它们经常受限于医院和专业设施。为了解决这个问题，本研究提出了一种通过数据增广生成高度逼真，展示药物滥用特征的人工合成面孔的方法。所提出的方法称为“3DG-GA”，深度去标识化匿名数据集生成，使用基因算法作为合成面孔的策略。算法包括GAN人工面生成，伪造检测和人脸识别。最初使用了120张实际面部药物滥用的图像数据集，通过保留药物特性，3DG-GA生成了一个包含3000张合成面部药物滥用的图像数据集。该数据集将开放给科学界，科学家们可以复现我们的结果并受益于该数据集。

    In biomedical research and artificial intelligence, access to large, well-balanced, and representative datasets is crucial for developing trustworthy applications that can be used in real-world scenarios. However, obtaining such datasets can be challenging, as they are often restricted to hospitals and specialized facilities. To address this issue, the study proposes to generate highly realistic synthetic faces exhibiting drug abuse traits through augmentation. The proposed method, called "3DG-GA", Deep De-identified anonymous Dataset Generation, uses Genetics Algorithm as a strategy for synthetic faces generation. The algorithm includes GAN artificial face generation, forgery detection, and face recognition. Initially, a dataset of 120 images of actual facial drug abuse is used. By preserving, the drug traits, the 3DG-GA provides a dataset containing 3000 synthetic facial drug abuse images. The dataset will be open to the scientific community, which can reproduce our results and benef
    
[^52]: 利用实时模拟的内在随机性促进机器人操作的强化学习

    Exploiting Intrinsic Stochasticity of Real-Time Simulation to Facilitate Robust Reinforcement Learning for Robot Manipulation. (arXiv:2304.06056v1 [cs.RO])

    [http://arxiv.org/abs/2304.06056](http://arxiv.org/abs/2304.06056)

    本文研究了实时模拟的内在随机性特性及其在强化学习中的应用，旨在提高RL方法鲁棒性和域随机化性能。

    

    对于像机器人操作这样的安全关键型应用，模拟对于强化学习（RL）在实际实现前是必不可少的，但是RL代理往往对模拟与实际世界之间的差异敏感。我们研究了现成模拟软件实时模拟（RT-IS）的内在随机性特性及其潜力，以提高RL方法的鲁棒性和域随机化的性能。

    Simulation is essential to reinforcement learning (RL) before implementation in the real world, especially for safety-critical applications like robot manipulation. Conventionally, RL agents are sensitive to the discrepancies between the simulation and the real world, known as the sim-to-real gap. The application of domain randomization, a technique used to fill this gap, is limited to the imposition of heuristic-randomized models. We investigate the properties of intrinsic stochasticity of real-time simulation (RT-IS) of off-the-shelf simulation software and its potential to improve the robustness of RL methods and the performance of domain randomization. Firstly, we conduct analytical studies to measure the correlation of RT-IS with the occupation of the computer hardware and validate its comparability with the natural stochasticity of a physical robot. Then, we apply the RT-IS feature in the training of an RL agent. The simulation and physical experiment results verify the feasibili
    
[^53]: 利用对称性和启发式演示来进行机器人操作的离线强化学习

    Exploiting Symmetry and Heuristic Demonstrations in Off-policy Reinforcement Learning for Robotic Manipulation. (arXiv:2304.06055v1 [cs.RO])

    [http://arxiv.org/abs/2304.06055](http://arxiv.org/abs/2304.06055)

    本文提出了一个离线强化学习方法，该方法利用对称环境中的专家演示来进行机器人操作的策略训练，从而提高了学习效率和样本效率。

    

    强化学习在许多领域中自动构建控制策略具有显著潜力，但在应用于机器人操作任务时由于维度的问题，效率较低。为了促进这些任务的学习，先前的知识或启发式方法可以有效地提高学习性能。本文旨在定义和结合物理机器环境中存在的自然对称性，利用对称环境中的专家演示通过强化学习和行为克隆的融合来训练具有高样本效率的策略，从而给离线强化学习过程提供多样化而紧凑的启动。此外，本文提出了一个最近概念的严格框架，并探索了它在机器人操作任务中的范围。该方法通过在模拟环境中进行两个点对点的工业臂到达任务（有障碍和无障碍）的验证。

    Reinforcement learning demonstrates significant potential in automatically building control policies in numerous domains, but shows low efficiency when applied to robot manipulation tasks due to the curse of dimensionality. To facilitate the learning of such tasks, prior knowledge or heuristics that incorporate inherent simplification can effectively improve the learning performance. This paper aims to define and incorporate the natural symmetry present in physical robotic environments. Then, sample-efficient policies are trained by exploiting the expert demonstrations in symmetrical environments through an amalgamation of reinforcement and behavior cloning, which gives the off-policy learning process a diverse yet compact initiation. Furthermore, it presents a rigorous framework for a recent concept and explores its scope for robot manipulation tasks. The proposed method is validated via two point-to-point reaching tasks of an industrial arm, with and without an obstacle, in a simulat
    
[^54]: 开放式智能交通基础模型挑战赛的新基准与基准数据集 - Open-TransMind

    Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation. (arXiv:2304.06051v1 [cs.CV])

    [http://arxiv.org/abs/2304.06051](http://arxiv.org/abs/2304.06051)

    Open-TransMind是智能交通领域第一个基础模型挑战赛的新基准，旨在解决数据量少、泛化能力差以及缺乏多模态技术等典型挑战。

    

    随着近年来计算能力和深度学习算法的不断提升，基础模型越来越受欢迎。由于其强大的能力和出色的性能，这种技术被越来越多的行业采用和应用。在智能交通行业中，人工智能面临着以下典型挑战：数据量少、泛化能力差以及缺乏多模态技术。基础模型技术可以显著缓解上述问题。为解决这些问题，我们设计了第一个基础模型挑战，旨在增加基础模型技术在交通场景中的普及度，并促进智能交通行业的快速发展。该挑战分为两个赛道：全能型和跨模态图像检索。此外，我们为这两个赛道提供了一个新的基线和基准数据，称为Open-TransMind。据我们所知，这是智能交通领域的第一个基础模型基准。

    With the continuous improvement of computing power and deep learning algorithms in recent years, the foundation model has grown in popularity. Because of its powerful capabilities and excellent performance, this technology is being adopted and applied by an increasing number of industries. In the intelligent transportation industry, artificial intelligence faces the following typical challenges: few shots, poor generalization, and a lack of multi-modal techniques. Foundation model technology can significantly alleviate the aforementioned issues. To address these, we designed the 1st Foundation Model Challenge, with the goal of increasing the popularity of foundation model technology in traffic scenarios and promoting the rapid development of the intelligent transportation industry. The challenge is divided into two tracks: all-in-one and cross-modal image retrieval. Furthermore, we provide a new baseline and benchmark for the two tracks, called Open-TransMind. According to our knowledg
    
[^55]: RELS-DQN：组合优化的强大和高效的局部搜索框架

    RELS-DQN: A Robust and Efficient Local Search Framework for Combinatorial Optimization. (arXiv:2304.06048v1 [cs.LG])

    [http://arxiv.org/abs/2304.06048](http://arxiv.org/abs/2304.06048)

    该论文提出了一个名为RELS-DQN的轻量级DQN框架，可以展现局部搜索行为并提供实用的可扩展性，其在多个应用程序上具有类似局部搜索算法的效果，并且解决方案值要高于或等于局部搜索算法和专家设计的启发式方法。

    

    组合优化旨在高效地找到最佳解决方案，涉及从统计物理到社交媒体营销的NP难问题。许多组合优化应用可以从局部搜索方法中受益，因为它们允许在贪婪策略上进行可逆操作。使用消息传递神经网络（MPNN）的深度Q学习（DQN）已经显示出在复制局部搜索行为和获得与局部搜索算法相当的结果方面很有前途。然而，在消息传递迭代过程中，过于平滑和信息丢失限制了其在应用程序中的鲁棒性，并且大的消息向量导致内存效率低下。我们的论文介绍了RELS-DQN，这是一个轻量级的DQN框架，展示了局部搜索行为同时提供实用的可扩展性。使用在一个应用程序上训练的RELS-DQN模型，它可以通过提供高于或等于局部搜索算法和专家设计的启发式方法的解决方案值来推广到各种应用程序。

    Combinatorial optimization (CO) aims to efficiently find the best solution to NP-hard problems ranging from statistical physics to social media marketing. A wide range of CO applications can benefit from local search methods because they allow reversible action over greedy policies. Deep Q-learning (DQN) using message-passing neural networks (MPNN) has shown promise in replicating the local search behavior and obtaining comparable results to the local search algorithms. However, the over-smoothing and the information loss during the iterations of message passing limit its robustness across applications, and the large message vectors result in memory inefficiency. Our paper introduces RELS-DQN, a lightweight DQN framework that exhibits the local search behavior while providing practical scalability. Using the RELS-DQN model trained on one application, it can generalize to various applications by providing solution values higher than or equal to both the local search algorithms and the e
    
[^56]: 使用物理训练的神经网络学习非线性本构材料模型：COMM-PINN。

    Learning solution of nonlinear constitutive material models using physics-informed neural networks: COMM-PINN. (arXiv:2304.06044v1 [cs.CE])

    [http://arxiv.org/abs/2304.06044](http://arxiv.org/abs/2304.06044)

    通过物理训练的神经网络可解决非线性材料行为的本构关系，无需初始数据，避免重复的牛顿迭代。训练好的模型可作为有限元程序的用户定义材料模型，但需要解决诸多挑战。

    

    我们使用物理训练的神经网络来解决非线性、路径相关材料行为的本构关系。训练好的网络不仅满足所有热力学约束，而且在任何给定的加载情况下，立即提供关于当前材料状态（即自由能，应力和内部变量的演变）的信息，而不需要初始数据。这项工作的一个优点是它规避了求解复材料模型中非线性方程所需的重复牛顿迭代。此外，我们提供了减少获取切向算子所需的导数次序的策略。训练好的模型可以直接用作任何有限元程序（或其他数值方法）中的用户定义材料模型。然而，在定义配点和整合同时激活或非激活的多个非相等约束方面仍存在挑战。

    We applied physics-informed neural networks to solve the constitutive relations for nonlinear, path-dependent material behavior. As a result, the trained network not only satisfies all thermodynamic constraints but also instantly provides information about the current material state (i.e., free energy, stress, and the evolution of internal variables) under any given loading scenario without requiring initial data. One advantage of this work is that it bypasses the repetitive Newton iterations needed to solve nonlinear equations in complex material models. Additionally, strategies are provided to reduce the required order of derivation for obtaining the tangent operator. The trained model can be directly used in any finite element package (or other numerical methods) as a user-defined material model. However, challenges remain in the proper definition of collocation points and in integrating several non-equality constraints that become active or non-active simultaneously. We tested this
    
[^57]: 智能驾驶辅助的深度学习系统

    Deep Learning Systems for Advanced Driving Assistance. (arXiv:2304.06041v1 [eess.SP])

    [http://arxiv.org/abs/2304.06041](http://arxiv.org/abs/2304.06041)

    本论文提出了一种智能驾驶辅助的深度学习系统，利用生物传感系统来重构汽车驾驶员的生理注意力状态，该探针通过深度学习系统分析和处理获得的PPG信号，以识别与驾驶员注意力水平相一致的特定模式。

    

    下一代汽车通过创新解决方案智能评估车辆驾驶安全。这项安全驾驶监测可以使用许多在科学文献中广泛讨论的方法来实现。在这种情况下，作者提出了一种创新方法，使用特定的生物传感系统来重构汽车驾驶员的生理注意力状态。为了重构汽车驾驶员的生理状态，作者提出了使用生物传感探针的方法，该探针由近红外(NiR)光谱下的耦合LED和光电检测器组成。该探针放置在被监测的对象上，可以检测到一种称为光电容积描记(PPG)的生理信号。PPG信号的形成由被监测对象血液中氧合和非氧合血红蛋白浓度的变化调节，这将直接与由自主神经系统(ANS)调节的心脏活动相连接。通过深度学习系统处理和分析获得的PPG信号，可以识别出与驾驶员注意力水平一致的特定模式。

    Next generation cars embed intelligent assessment of car driving safety through innovative solutions often based on usage of artificial intelligence. The safety driving monitoring can be carried out using several methodologies widely treated in scientific literature. In this context, the author proposes an innovative approach that uses ad-hoc bio-sensing system suitable to reconstruct the physio-based attentional status of the car driver. To reconstruct the car driver physiological status, the author proposed the use of a bio-sensing probe consisting of a coupled LEDs at Near infrared (NiR) spectrum with a photodetector. This probe placed over the monitored subject allows to detect a physiological signal called PhotoPlethysmoGraphy (PPG). The PPG signal formation is regulated by the change in oxygenated and non-oxygenated hemoglobin concentration in the monitored subject bloodstream which will be directly connected to cardiac activity in turn regulated by the Autonomic Nervous System (
    
[^58]: 从文本到图像生成角度审视社会偏见

    Social Biases through the Text-to-Image Generation Lens. (arXiv:2304.06034v1 [cs.CY])

    [http://arxiv.org/abs/2304.06034](http://arxiv.org/abs/2304.06034)

    本文研究了文本到图像生成技术中存在的社会偏见，主要集中在职业、人格特征和日常情境等方面。实验证明，这些模型存在排除特定人群的职业偏见。

    

    文本到图像 (T2I) 生成技术通过将给定的文本描述作为提示，生成高逼真度的插图，为创作者、设计师和普通用户提供了新的应用。然而，这些模型是在大量的网络数据上训练的，这也带来了潜在的有害偏见风险。本文通过研究和量化常见的社会偏见，如职业、人格特征和日常情境在（被感知的）性别、年龄、种族和地理位置上的表现，采用多维度方法来探究生成图片中的社会偏见。通过广泛的自动化和人类评估实验，我们展示了两种流行的 T2I 模型 (DALLE-v2 和 Stable Diffusion) 的发现。结果表明，中性提示存在严重的职业偏见，主要是排除某些人群。

    Text-to-Image (T2I) generation is enabling new applications that support creators, designers, and general end users of productivity software by generating illustrative content with high photorealism starting from a given descriptive text as a prompt. Such models are however trained on massive amounts of web data, which surfaces the peril of potential harmful biases that may leak in the generation process itself. In this paper, we take a multi-dimensional approach to studying and quantifying common social biases as reflected in the generated images, by focusing on how occupations, personality traits, and everyday situations are depicted across representations of (perceived) gender, age, race, and geographical location. Through an extensive set of both automated and human evaluation experiments we present findings for two popular T2I models: DALLE-v2 and Stable Diffusion. Our results reveal that there exist severe occupational biases of neutral prompts majorly excluding groups of people 
    
[^59]: 公正：从伦理原则到机器学习开发中与利益相关者的持续协议

    Fairness: from the ethical principle to the practice of Machine Learning development as an ongoing agreement with stakeholders. (arXiv:2304.06031v1 [cs.CY])

    [http://arxiv.org/abs/2304.06031](http://arxiv.org/abs/2304.06031)

    本文提出了一种支持伦理的迭代过程，旨在通过机器学习设计中的持续协议挑战不对称的权力动态，并支持团队在各个步骤中识别、减轻和监测偏差。

    

    本文阐明了为什么无法完全消除机器学习（ML）中的偏见，并提出了一种端到端的方法来将正义与公正的伦理原则转化为与利益相关者的持续协议，纳入ML开发实践中。本文提出的支持伦理的迭代过程旨在挑战ML设计中的不对称权力动态，帮助ML开发团队在ML系统开发的每个步骤中识别、减轻和监测偏见。该过程还提供了如何向用户解释偏见在权衡方面始终不完美的指导。

    This paper clarifies why bias cannot be completely mitigated in Machine Learning (ML) and proposes an end-to-end methodology to translate the ethical principle of justice and fairness into the practice of ML development as an ongoing agreement with stakeholders. The pro-ethical iterative process presented in the paper aims to challenge asymmetric power dynamics in the fairness decision making within ML design and support ML development teams to identify, mitigate and monitor bias at each step of ML systems development. The process also provides guidance on how to explain the always imperfect trade-offs in terms of bias to users.
    
[^60]: LMR: 基于车道距离的轨迹预测度量

    LMR: Lane Distance-Based Metric for Trajectory Prediction. (arXiv:2304.05869v1 [cs.CV])

    [http://arxiv.org/abs/2304.05869](http://arxiv.org/abs/2304.05869)

    LMR是一种基于车道距离的度量，适用于轨迹预测中的结构化环境，相比传统的欧氏距离度量更准确。

    

    轨迹预测方法的开发需要度量来验证和比较它们的性能。目前已经确定的度量基于欧氏距离，这意味着在所有方向上都给出了相同的误差权重。欧几里得度量对于像道路这样的结构化环境是不足够的，因为它们没有妥善捕捉到与底层车道相关的操作员意图。为了针对下游规划任务合理评估轨迹预测方法，我们提出了一种新的度量，即基于车道距离的车道错过率（LMR）。对于LMR的计算，将地面实测和预测端点分配给车道线段，更确切地说是它们的中心线。通过沿车道线段的距离测量，预测与实测之间的距离在一定阈值范围内的预测被称为命中，否则称为错过。LMR则定义为产生错过的序列的比率。我们在三个不同的数据集上的结果表明，相对于传统的基于欧氏距离的度量，LMR是适用于类似车道这样的结构化环境的轨迹预测更为合适的度量。

    The development of approaches for trajectory prediction requires metrics to validate and compare their performance. Currently established metrics are based on Euclidean distance, which means that errors are weighted equally in all directions. Euclidean metrics are insufficient for structured environments like roads, since they do not properly capture the agent's intent relative to the underlying lane. In order to provide a reasonable assessment of trajectory prediction approaches with regard to the downstream planning task, we propose a new metric that is lane distance-based: Lane Miss Rate (LMR). For the calculation of LMR, the ground-truth and predicted endpoints are assigned to lane segments, more precisely their centerlines. Measured by the distance along the lane segments, predictions that are within a certain threshold distance to the ground-truth count as hits, otherwise they count as misses. LMR is then defined as the ratio of sequences that yield a miss. Our results on three s
    
[^61]: 学习同形异义词消歧表示以改进神经机器翻译

    Learning Homographic Disambiguation Representation for Neural Machine Translation. (arXiv:2304.05860v1 [cs.CL])

    [http://arxiv.org/abs/2304.05860](http://arxiv.org/abs/2304.05860)

    本文提出了一种利用同义词句子建立同形异义词词级消歧表示（HDR）以改进神经机器翻译的方法。

    

    同形异义词在神经机器翻译中一直是难点。本文提出一种在潜在空间中解决同形异义词问题的新方法。首先，我们利用“HDR-encoder”在自然语言推理任务中学习通用句子表示。然后，利用WordNet中的同义词句子建立同形异义词词级消歧表示（HDR），调整预训练的HDR-encoder。最后，我们将预训练的HDR-encoder与基于Transformer的NMT在不同方案中相结合来提高翻译准确性。四个翻译方向的实验表明了本方法在增强NMT系统处理同形异义词方面的有效性。

    Homographs, words with the same spelling but different meanings, remain challenging in Neural Machine Translation (NMT). While recent works leverage various word embedding approaches to differentiate word sense in NMT, they do not focus on the pivotal components in resolving ambiguities of homographs in NMT: the hidden states of an encoder. In this paper, we propose a novel approach to tackle homographic issues of NMT in the latent space. We first train an encoder (aka "HDR-encoder") to learn universal sentence representations in a natural language inference (NLI) task. We further fine-tune the encoder using homograph-based synset sentences from WordNet, enabling it to learn word-level homographic disambiguation representations (HDR). The pre-trained HDR-encoder is subsequently integrated with a transformer-based NMT in various schemes to improve translation accuracy. Experiments on four translation directions demonstrate the effectiveness of the proposed method in enhancing the perfor
    
[^62]: Proximity Forest 2.0：一种新的有效且可扩展的基于相似性的时间序列分类器

    Proximity Forest 2.0: A new effective and scalable similarity-based classifier for time series. (arXiv:2304.05800v1 [cs.LG])

    [http://arxiv.org/abs/2304.05800](http://arxiv.org/abs/2304.05800)

    Proximity Forest 2.0是一种新的有效且可扩展的基于相似性的时间序列分类器，优于先前最先进的基于相似性的分类器以及最先进的基于内核、神经网络和混合方法在特定数据集上的表现。

    

    时间序列分类（TSC）由于可能与不同分类任务相关的特征类型的多样性而具有挑战性，包括趋势、方差、频率、幅度和各种模式。为了应对这一挑战，已经开发了几种替代方法类别，包括基于相似性、特征和间隔、形状、字典、内核、神经网络和混合方法。本文提出了一种新的基于相似性的分类器Proximity Forest版本2.0（PF 2.0），它在UCR基准测试中优于先前最先进的基于相似性的分类器，并在基准测试中优于最先进的基于内核、神经网络和混合方法的特定数据集，这些数据集最适合使用基于相似性的方法。PF 2.0 合并了时间序列相似性最近的三个进展……

    Time series classification (TSC) is a challenging task due to the diversity of types of feature that may be relevant for different classification tasks, including trends, variance, frequency, magnitude, and various patterns. To address this challenge, several alternative classes of approach have been developed, including similarity-based, features and intervals, shapelets, dictionary, kernel, neural network, and hybrid approaches. While kernel, neural network, and hybrid approaches perform well overall, some specialized approaches are better suited for specific tasks. In this paper, we propose a new similarity-based classifier, Proximity Forest version 2.0 (PF 2.0), which outperforms previous state-of-the-art similarity-based classifiers across the UCR benchmark and outperforms state-of-the-art kernel, neural network, and hybrid methods on specific datasets in the benchmark that are best addressed by similarity-base methods. PF 2.0 incorporates three recent advances in time series simi
    
[^63]: 大型语言模型在医疗保健领域中准备就绪了吗？临床语言理解的比较研究。

    Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding. (arXiv:2304.05368v1 [cs.CL])

    [http://arxiv.org/abs/2304.05368](http://arxiv.org/abs/2304.05368)

    本研究全面评估了大型语言模型在临床语言理解任务上的表现，并引入自问自答提示策略来提高LLMs在医疗保健相关任务中的效果。

    

    大型语言模型（LLMs）在各个领域取得了显著的进展，包括医疗保健领域。然而，临床语言理解任务的专业性质带来了独特的挑战和限制，需要进一步研究。在本研究中，我们对最先进的LLMs——GPT-3.5、GPT-4和Bard进行了全面评估，该评估范围涵盖了各种任务，包括命名实体识别、关系提取、自然语言推理、语义文本相似性、文档分类和问答。我们还引入了一种新的提示策略——自问自答提示（SQP），旨在通过引发与相关临床场景相关的信息性问题和答案，定制化提高LLMs的性能。我们的评估强调了任务特定的学习策略和提示技术对于提高LLMs在医疗保健相关任务中的有效性的重要性。

    Large language models (LLMs) have made significant progress in various domains, including healthcare. However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation. In this study, we conduct a comprehensive evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within the realm of clinical language understanding tasks. These tasks span a diverse range, including named entity recognition, relation extraction, natural language inference, semantic textual similarity, document classification, and question-answering. We also introduce a novel prompting strategy, self-questioning prompting (SQP), tailored to enhance LLMs' performance by eliciting informative questions and answers pertinent to the clinical scenarios at hand. Our evaluation underscores the significance of task-specific learning strategies and prompting techniques for improving LLMs' effectiveness in healthcare-related tasks.
    
[^64]: Astroformer：分类并不总是需要更多数据

    Astroformer: More Data Might Not be All You Need for Classification. (arXiv:2304.05350v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2304.05350](http://arxiv.org/abs/2304.05350)

    该文提出了使用混合变换器 - 卷积架构的方法，结合新的堆栈设计、不同的相对自我注意层创建方式和精心选择的数据增强和正则化技术，从少量数据中学习，将此方法应用于Galaxy Zoo数据集，结果表明在少量数据的情况下取得了与以前方法相同的分类结果，并且不会损失性能。

    

    自然语言处理和计算机视觉领域的最新进展依赖于复杂的大型模型，这些模型使用大量未标记或部分标记的数据进行训练。在资源受限制的环境中训练或部署这些最先进的方法一直是一个挑战。星系形态对于理解星系的形成和演化过程至关重要。需要高效的方法来分类星系形态，并从现代天文学调查中提取物理信息。在本文中，我们介绍了从少量数据中学习的方法。我们提出使用混合变换器 - 卷积架构，从CoAtNet和MaxViT的成功中汲取灵感。具体来说，我们使用具有新堆栈设计和不同的相对自我注意层创建方式的Transformer - 卷积混合。并将其与精心选择的数据增强和正则化技术相配对。我们将这种方法应用于Galaxy Zoo数据集，结果表明，通过仔细的网络设计和正则化技术，可以在比以前的方法少的数据条件下取得有竞争力的分类结果，而不会牺牲性能。

    Recent advancements in areas such as natural language processing and computer vision rely on intricate and massive models that have been trained using vast amounts of unlabelled or partly labeled data and training or deploying these state-of-the-art methods to resource constraint environments has been a challenge. Galaxy morphologies are crucial to understanding the processes by which galaxies form and evolve. Efficient methods to classify galaxy morphologies are required to extract physical information from modern-day astronomy surveys. In this paper, we introduce methods to learn from less amounts of data. We propose using a hybrid transformer-convolutional architecture drawing much inspiration from the success of CoAtNet and MaxViT. Concretely, we use the transformer-convolutional hybrid with a new stack design for the network, a different way of creating a relative self-attention layer, and pair it with a careful selection of data augmentation and regularization techniques. Our app
    
[^65]: 监管市场：人工智能治理的未来

    Regulatory Markets: The Future of AI Governance. (arXiv:2304.04914v1 [cs.AI])

    [http://arxiv.org/abs/2304.04914](http://arxiv.org/abs/2304.04914)

    提出一种监管市场的概念，即政府要求受监管对象从私人监管机构购买监管服务，以克服过度依赖行业自律和立法机构缺乏专业知识的局限性，从而逐步实现人工智能的恰当监管。

    

    恰当地监管人工智能是一个日益紧迫的政策挑战。立法机构和监管机构缺乏翻译公众需求为法律要求所需的专业知识。过度依赖行业自律未能使AI系统的生产者和使用者对民主要求负责。提出了监管市场的概念，即政府要求受监管对象从私人监管机构购买监管服务。这种方法可以克服命令和控制监管和自我监管的局限性。监管市场可以使政府为AI监管建立政策优先级，同时依靠市场力量和行业研发努力来开创最能实现政策制定者声明目标的监管方法。

    Appropriately regulating artificial intelligence is an increasingly urgent policy challenge. Legislatures and regulators lack the specialized knowledge required to best translate public demands into legal requirements. Overreliance on industry self-regulation fails to hold producers and users of AI systems accountable to democratic demands. Regulatory markets, in which governments require the targets of regulation to purchase regulatory services from a private regulator, are proposed. This approach to AI regulation could overcome the limitations of both command-and-control regulation and self-regulation. Regulatory market could enable governments to establish policy priorities for the regulation of AI, whilst relying on market forces and industry R&D efforts to pioneer the methods of regulation that best achieve policymakers' stated objectives.
    
[^66]: 可能大致正确联邦学习

    Probably Approximately Correct Federated Learning. (arXiv:2304.04641v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.04641](http://arxiv.org/abs/2304.04641)

    本文提出了FedPAC框架，利用PAC学习理论推导出一个解析解，可以保证FL之间隐私、效用和效率的最佳权衡。

    

    联邦学习是一种新的分布式学习范例，其主要支柱为隐私、效用和效率。现有研究表明，同时实现无穷小隐私泄露、效用损失和效率是不可能的。因此，在设计联邦学习算法时，如何找到最佳权衡解决方案是关键考虑因素。一种常见的方法是将权衡问题视为多目标优化问题，即目标是在约束隐私泄露不超过预定值的情况下最小化效用损失和效率降低。然而，现有的多目标优化框架非常耗时，并且不能保证帕累托前沿的存在性，这激励我们寻求一种方法，将多目标问题转化为单目标问题，因为它更高效、更容易被解决。为此，本文提出了FedPAC，这是一个统一的框架，利用PAC学习理论推导出一个解析解，可以保证FL之间隐私、效用和效率的最佳权衡。具体而言，我们首先将FL问题公式化为一个二分类任务，然后设计一个自适应FL算法，动态调整每个客户端的采样比率，以平衡全局和本地的隐私-效用权衡，最后证明FedPAC可以在温和的假设下高概率地实现最优的隐私-效用权衡。基准数据集上的大量实验证明了我们提出的FedPAC框架的功效和效率。

    Federated learning (FL) is a new distributed learning paradigm, with privacy, utility, and efficiency as its primary pillars. Existing research indicates that it is unlikely to simultaneously attain infinitesimal privacy leakage, utility loss, and efficiency. Therefore, how to find an optimal trade-off solution is the key consideration when designing the FL algorithm. One common way is to cast the trade-off problem as a multi-objective optimization problem, i.e., the goal is to minimize the utility loss and efficiency reduction while constraining the privacy leakage not exceeding a predefined value. However, existing multi-objective optimization frameworks are very time-consuming, and do not guarantee the existence of the Pareto frontier, this motivates us to seek a solution to transform the multi-objective problem into a single-objective problem because it is more efficient and easier to be solved. To this end, in this paper, we propose FedPAC, a unified framework that leverages PAC l
    
[^67]: OpenAGI：当LLM遇到领域专家

    OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])

    [http://arxiv.org/abs/2304.04370](http://arxiv.org/abs/2304.04370)

    基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。

    

    人类具有将基本技能组合成复杂技能以解决复杂任务的显著能力。这种能力对于人工智能同样重要，因此，我们断言，除了开发大型综合智能模型外，将不同领域专家模型应用于复杂任务解决能力同样关键，以在人工智能通用智能的追求中使其具备这种能力。最近的大型语言模型（LLM）的发展证明其具有出色的学习和推理能力，使它们成为选择、综合和执行外部模型以解决复杂任务的控制器的有前途的选择。在这个项目中，我们开发了一个名为OpenAGI的开源AGI研究平台，专门设计为提供复杂的多步骤任务，并配有任务特定的数据集、评估指标和各种可扩展模型。OpenAGI将复杂任务阐释为自然语言问答，旨在促进领域专家和语言模型之间的协同作用。

    Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
    
[^68]: 视频聊天字幕生成器： 迈向丰富时空描述

    Video ChatCaptioner: Towards the Enriched Spatiotemporal Descriptions. (arXiv:2304.04227v1 [cs.CV])

    [http://arxiv.org/abs/2304.04227](http://arxiv.org/abs/2304.04227)

    Video ChatCaptioner是一种创新方法，利用ChatGPT和算法生成全面和丰富的时空视频描述。

    

    视频字幕生成的目的是使用自然语言传达视频中的动态场景，促进我们对环境中时空信息的理解。尽管最近取得了一些进展，生成细致和丰富的视频描述仍然是一项重大挑战。在这项工作中，我们介绍了一种创新方法，Video ChatCaptioner，用于创建更全面的时空视频描述。我们的方法采用 ChatGPT 模型作为控制器，专门设计用于选择框架以提出视频内容驱动的问题。随后，利用强大的算法回答这些视觉查询。这种问答框架有效地揭示了复杂的视频细节，并显示出增强视频内容的方法的前途。在多个对话轮次之后，ChatGPT 可以根据之前的对话总结丰富的视频内容。我们定性证明，我们的 Video ChatCaptioner 可以生成包含更多细节的视频字幕。

    Video captioning aims to convey dynamic scenes from videos using natural language, facilitating the understanding of spatiotemporal information within our environment. Although there have been recent advances, generating detailed and enriched video descriptions continues to be a substantial challenge. In this work, we introduce Video ChatCaptioner, an innovative approach for creating more comprehensive spatiotemporal video descriptions. Our method employs a ChatGPT model as a controller, specifically designed to select frames for posing video content-driven questions. Subsequently, a robust algorithm is utilized to answer these visual queries. This question-answer framework effectively uncovers intricate video details and shows promise as a method for enhancing video content. Following multiple conversational rounds, ChatGPT can summarize enriched video content based on previous conversations. We qualitatively demonstrate that our Video ChatCaptioner can generate captions containing mo
    
[^69]: EGC: 一种通过单一能量模型生成与分类图像的方法

    EGC: Image Generation and Classification via a Single Energy-Based Model. (arXiv:2304.02012v1 [cs.CV])

    [http://arxiv.org/abs/2304.02012](http://arxiv.org/abs/2304.02012)

    EGC是一种使用单个神经网络在图像分类和图像生成任务中实现卓越性能的方法，可以较好地生成出高质量图像，并在多项数据集上实现了领先的分类结果。

    

    使用相同的网络参数学习图像分类和生成图像是一个具有挑战性的问题。最近的先进方法在一项任务上表现良好，但在另一项任务上却表现不佳。本文引入了一种名为EGC的基于能量的分类器和生成器，它可以使用单个神经网络在两个任务中实现卓越性能。与传统的分类器输出给定图像的标签（即条件分布$p(y|\mathbf{x})$）不同，EGC的前向传递器是一个分类器，它输出一个联合分布$p(\mathbf{x},y)$，在后向传递器中通过边缘化标签$y$实现生成器。在前向传递中，估计给定噪声图像的能量和分类概率，而在后向传递中，通过估计得分函数对其进行去噪。EGC在ImageNet-1k、CelebA-HQ和LSUN Church上实现了与最先进方法相当的生成结果，同时在CIFAR-10、CIFAR-100和ImageNet-1k上实现了最先进的分类结果。

    Learning image classification and image generation using the same set of network parameters is a challenging problem. Recent advanced approaches perform well in one task often exhibit poor performance in the other. This work introduces an energy-based classifier and generator, namely EGC, which can achieve superior performance in both tasks using a single neural network. Unlike a conventional classifier that outputs a label given an image (i.e., a conditional distribution $p(y|\mathbf{x})$), the forward pass in EGC is a classifier that outputs a joint distribution $p(\mathbf{x},y)$, enabling an image generator in its backward pass by marginalizing out the label $y$. This is done by estimating the energy and classification probability given a noisy image in the forward pass, while denoising it using the score function estimated in the backward pass. EGC achieves competitive generation results compared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUN Church, while achi
    
[^70]: InfluencerRank：基于图卷积注意力循环神经网络发现有效的影响者

    InfluencerRank: Discovering Effective Influencers via Graph Convolutional Attentive Recurrent Neural Networks. (arXiv:2304.01897v1 [cs.SI])

    [http://arxiv.org/abs/2304.01897](http://arxiv.org/abs/2304.01897)

    本文提出了InfluencerRank，通过图卷积神经网络和注意力循环神经网络结合的方法评估影响者的有效性，以帮助企业在社交影响者营销中寻找合适的影响者。

    

    随着影响者在社交媒体营销中扮演重要角色，企业增加了影响者营销的预算。雇用有效的影响者在社交影响者营销中至关重要，但是在数亿社交媒体用户中找到合适的影响者是具有挑战性的。本文提出了InfluencerRank，它基于影响者的发布行为和社交关系评估影响者的有效性。为了表示发布行为和社交关系，应用图卷积神经网络来在不同的历史时间段内对具有异构网络的影响者进行建模。通过学习嵌入式节点特征的网络结构，InfluencerRank可以在每个时间段为影响者派生信息丰富的表示。最终，一个注意力循环神经网络通过捕捉影响者表示的动态知识，区分高效的影响者和其他影响者。

    As influencers play considerable roles in social media marketing, companies increase the budget for influencer marketing. Hiring effective influencers is crucial in social influencer marketing, but it is challenging to find the right influencers among hundreds of millions of social media users. In this paper, we propose InfluencerRank that ranks influencers by their effectiveness based on their posting behaviors and social relations over time. To represent the posting behaviors and social relations, the graph convolutional neural networks are applied to model influencers with heterogeneous networks during different historical periods. By learning the network structure with the embedded node features, InfluencerRank can derive informative representations for influencers at each period. An attentive recurrent neural network finally distinguishes highly effective influencers from other influencers by capturing the knowledge of the dynamics of influencer representations over time. Extensiv
    
[^71]: 对策略更新的正则化以稳定均场博弈

    Regularization of the policy updates for stabilizing Mean Field Games. (arXiv:2304.01547v1 [cs.AI])

    [http://arxiv.org/abs/2304.01547](http://arxiv.org/abs/2304.01547)

    本文提出了一种均场近端策略优化算法（MF-PPO），以稳定深度强化学习在均场博弈中的应用，并在OpenSpiel框架中进行了实验验证。

    

    本文研究非合作多智能体强化学习(MARL)，其中多个智能体在同一环境中相互作用，目标是最大化个体回报。当智能体数量扩大时，由于许多智能体引入的非静止性，会产生挑战。为了解决这个问题，均场博弈（MFG）依靠对称性和同质性假设，近似具有很大群体的博弈。最近，深度强化学习被用于将MFG扩展到具有更多状态的博弈中。目前的方法依赖于平滑技术，如对q值或均场分布更新进行平均。本文提出了一种在均场策略上进行近似更新以稳定学习的不同方法。我们将我们的算法命名为均场近端策略优化（MF-PPO），并在OpenSpiel框架中经验性地展示了我们方法的有效性。

    This work studies non-cooperative Multi-Agent Reinforcement Learning (MARL) where multiple agents interact in the same environment and whose goal is to maximize the individual returns. Challenges arise when scaling up the number of agents due to the resultant non-stationarity that the many agents introduce. In order to address this issue, Mean Field Games (MFG) rely on the symmetry and homogeneity assumptions to approximate games with very large populations. Recently, deep Reinforcement Learning has been used to scale MFG to games with larger number of states. Current methods rely on smoothing techniques such as averaging the q-values or the updates on the mean-field distribution. This work presents a different approach to stabilize the learning based on proximal updates on the mean-field policy. We name our algorithm \textit{Mean Field Proximal Policy Optimization (MF-PPO)}, and we empirically show the effectiveness of our method in the OpenSpiel framework.
    
[^72]: 降低Frank-Wolfe方法中的离散化误差

    Reducing Discretization Error in the Frank-Wolfe Method. (arXiv:2304.01432v1 [math.OC])

    [http://arxiv.org/abs/2304.01432](http://arxiv.org/abs/2304.01432)

    本论文提出了两个改进方法：一个多步的Frank-Wolfe方法，直接应用优化的高阶离散化方案；以及一种具有较少离散化误差的LMO-平均方案，其收敛速率加速到$O(1/k^{3/2})$，从而更好地解决了Frank-Wolfe方法中的离散化误差问题。

    

    Frank-Wolfe算法是结构受限机器学习应用中常用的方法，因其快速迭代复杂度而受欢迎。然而，该方法的一个主要限制是收敛速度缓慢，由于步长方向的不规则震荡而难以加速，即使在接近解的渐近情况下也是如此。我们认为这是离散化的产物；也就是说，Frank-Wolfe的流（即渐近小步长情况下的轨迹）不会出现不规则震荡，因此减少离散化误差将与产生更稳定的方法和更好的收敛特性相辅相成。我们提出了两个改进：一个多步Frank-Wolfe方法，直接应用优化的高阶离散化方案；和一个具有降低离散化误差的LMO-平均方案，其在一般凸集上的局部收敛速率从$O(1/k)$加速到$O(1/k^{3/2})$ 。

    The Frank-Wolfe algorithm is a popular method in structurally constrained machine learning applications, due to its fast per-iteration complexity. However, one major limitation of the method is a slow rate of convergence that is difficult to accelerate due to erratic, zig-zagging step directions, even asymptotically close to the solution. We view this as an artifact of discretization; that is to say, the Frank-Wolfe \emph{flow}, which is its trajectory at asymptotically small step sizes, does not zig-zag, and reducing discretization error will go hand-in-hand in producing a more stabilized method, with better convergence properties. We propose two improvements: a multistep Frank-Wolfe method that directly applies optimized higher-order discretization schemes; and an LMO-averaging scheme with reduced discretization error, and whose local convergence rate over general convex sets accelerates from a rate of $O(1/k)$ to up to $O(1/k^{3/2})$.
    
[^73]: 使用人工智能在家中测量帕金森病的严重程度

    Using AI to Measure Parkinson's Disease Severity at Home. (arXiv:2303.17573v1 [cs.LG])

    [http://arxiv.org/abs/2303.17573](http://arxiv.org/abs/2303.17573)

    该论文提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法，该方法可重复用于类似的运动任务，拥有较高的可靠性和准确性。

    

    我们提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法。参与者在网络摄像头前完成了运动任务（即点击手指），250名全球参与者的数据按照运动障碍协会统一帕金森病评分量表 (MDS-UPDRS) 的标准由三名专家神经学家进行了评估。神经学家的评估具有高度的可靠性，内部一致性系数（ICC）为0.88。我们开发了计算机算法来获得与MDS-UPDRS指南一致且与神经学家的评估高度相关的客观测量结果。我们的机器学习模型在这些指标的训练下表现优于一个MDS-UPDRS认证的评分者，平均绝对误差（MAE）为0.59，而评分者的MAE为0.79。然而，该模型的表现略逊于专家神经学家（0.53 MAE）。该方法可重复用于类似的运动任务，提供了可能性。

    We present an artificial intelligence system to remotely assess the motor performance of individuals with Parkinson's disease (PD). Participants performed a motor task (i.e., tapping fingers) in front of a webcam, and data from 250 global participants were rated by three expert neurologists following the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). The neurologists' ratings were highly reliable, with an intra-class correlation coefficient (ICC) of 0.88. We developed computer algorithms to obtain objective measurements that align with the MDS-UPDRS guideline and are strongly correlated with the neurologists' ratings. Our machine learning model trained on these measures outperformed an MDS-UPDRS certified rater, with a mean absolute error (MAE) of 0.59 compared to the rater's MAE of 0.79. However, the model performed slightly worse than the expert neurologists (0.53 MAE). The methodology can be replicated for similar motor tasks, providing the possibili
    
[^74]: 基于深度学习的振动信号去噪方法

    Vibration Signal Denoising Using Deep Learning. (arXiv:2303.11413v1 [eess.SP])

    [http://arxiv.org/abs/2303.11413](http://arxiv.org/abs/2303.11413)

    本文研究了基于深度学习的去除脚步引起的振动信号的噪声的方法，该方法适用于高斯噪声和非平稳噪声。

    

    由脚步引起的结构振动信号被广泛用于人员识别、定位、人类活动推断、结构健康监测等任务。然而，由于环境噪声、电磁干扰等因素的影响，实际采集的信号通常会带有噪声。噪声的存在影响了信号处理过程，从而影响了最终任务的准确性和误差。本文主要探讨了基于深度学习的去除脚步引起的振动信号的噪声的方法。我们考虑了不同类型的噪声，包括高斯噪声和非平稳噪声等。

    Structure vibration signals induced by footsteps are widely used for tasks like occupant identification, localization, human activity inference, structure health monitoring and so on. The vibration signals are collected as time series with amplitude values. However, the collected signals are always noisy in practice due to the influence of environmental noise, electromagnetic interference and other factors. The presence of noise affects the process of signal analysis, thus affecting the accuracy and error of the final tasks. In this paper, we mainly explore the denoising methods for footstep-induced vibration signals. We have considered different kinds of noise including stationary noises such as gaussian noises and non-stationary noises such as item-dropping vibration noise and music noises.
    
[^75]: ChatGPT和其他大型语言模型作为在线互动协作游戏设计的进化引擎（arXiv:2303.02155v2 [cs.AI] UPDATED）

    ChatGPT and Other Large Language Models as Evolutionary Engines for Online Interactive Collaborative Game Design. (arXiv:2303.02155v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.02155](http://arxiv.org/abs/2303.02155)

    本论文提出了一种新的结合交互式进化和大型语言模型的协作游戏设计框架，用于模拟典型的人类设计过程，并用大型语言模型进行非常复杂的创造性任务－思想的重组和变异。

    

    大型语言模型（LLMs）已经在科学界掀起风暴，改变了自然语言处理和人机交互的现状。这些强大的工具可以回答复杂的问题，出奇地完成有挑战性的创造性任务（例如，生成代码和应用程序来解决问题，写故事、音乐片段等）。在本文中，我们提出了一种结合交互式进化和大型语言模型的协作游戏设计框架，以模拟典型的人类设计过程。我们使用前者利用用户的反馈选择最有前途的想法，使用大型语言模型进行非常复杂的创造性任务——思想的重组和变异。在我们的框架中，过程始于一个简要说明和一组候选设计，这些设计是使用语言模型生成的，或由用户提出的。接下来，用户通过向交互式遗传算法提供反馈来协作设计过程，该算法选择、重组和突变设计。

    Large language models (LLMs) have taken the scientific world by storm, changing the landscape of natural language processing and human-computer interaction. These powerful tools can answer complex questions and, surprisingly, perform challenging creative tasks (e.g., generate code and applications to solve problems, write stories, pieces of music, etc.). In this paper, we present a collaborative game design framework that combines interactive evolution and large language models to simulate the typical human design process. We use the former to exploit users' feedback for selecting the most promising ideas and large language models for a very complex creative task - the recombination and variation of ideas. In our framework, the process starts with a brief and a set of candidate designs, either generated using a language model or proposed by the users. Next, users collaborate on the design process by providing feedback to an interactive genetic algorithm that selects, recombines, and mu
    
[^76]: "一种适应或灭亡的局面": 游戏行业专业人士对文本生成图像人工智能的感知、采用和使用。

    "An Adapt-or-Die Type of Situation": Perception, Adoption, and Use of Text-To-Image-Generation AI by Game Industry Professionals. (arXiv:2302.12601v3 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2302.12601](http://arxiv.org/abs/2302.12601)

    TTIG模型是创造性人工智能的最新补充，可以根据文本描述生成图像。研究发现，专业人士对于TTIG应用和认知等方面存在12个主要问题。这项研究为支持TTIG的可持续采用提供了重要见解。

    

    文本生成图像(TTIG)模型是创造性人工智能的最新补充。这些模型可根据文本描述生成图像，开始与专业创作者的作品竞争并引发了有关创作工作、失业和版权等重要影响的讨论。为了支持TTIG的可持续采用，我们必须提供专业人士感知、采用和使用TTIG的丰富、可靠和透明的见解。然而，公共辩论浅薄、狭窄且缺乏透明度，学术工作则集中于研究TTIG在一般艺术家人群中的使用，但没有研究特定行业的专业人士的感知和态度。在本文中，我们对芬兰游戏行业进行了一项定性的探索性访谈研究，研究了TTIG的应用。通过对14个游戏专业人士进行的半结构化访谈的模板分析，我们揭示了12个总体主题，结构化成49个子主题，探讨了TTIG的应用和认知等方面的问题。

    Text-to-image generation (TTIG) models, a recent addition to creative AI, can generate images based on a text description. These models have begun to rival the work of professional creatives, and sparked discussions on the future of creative work, loss of jobs, and copyright issues, amongst other important implications. To support the sustainable adoption of TTIG, we must provide rich, reliable and transparent insights into how professionals perceive, adopt and use TTIG. Crucially though, the public debate is shallow, narrow and lacking transparency, while academic work has focused on studying the use of TTIG in a general artist population, but not on the perceptions and attitudes of professionals in a specific industry. In this paper, we contribute a qualitative, exploratory interview study on TTIG in the Finnish videogame industry. Through a Template Analysis on semi-structured interviews with 14 game professionals, we reveal 12 overarching themes, structured into 49 sub-themes on pr
    
[^77]: K-SHAP: 一种用于匿名状态-动作对的策略聚类算法

    K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs. (arXiv:2302.11996v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11996](http://arxiv.org/abs/2302.11996)

    本文提出了一种名为K-SHAP的算法，来解决多个智能体保持匿名且仅有状态-动作对的情况下学习智能体决策的问题。

    

    从观测数据中学习智能体行为已经被证明可以提高我们对它们决策过程的理解，从而增强我们解释它们与环境和其他智能体之间交互的能力。尽管文献中已经提出了多种学习技术，但还有一种特定的情况尚未被探索，那就是智能体身份保持匿名的多智能体系统。例如，在金融市场中，标记数据通常是专有的，仅公开多个市场参与者交互而产生的匿名状态-动作对。因此，智能体行动序列不可观测，限制了现有工作的适用性。本文提出了一种策略聚类算法K-SHAP，它学习根据智能体策略对匿名状态-动作对进行分组。我们将该问题作为模仿学习(IL)任务，学习一个w...

    Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. In this paper, we propose a Policy Clustering algorithm, called K-SHAP, that learns to group anonymous state-action pairs according to the agent policies. We frame the problem as an Imitation Learning (IL) task, and we learn a w
    
[^78]: 通过深度度量学习提高人机物理协作的安全性

    Improving safety in physical human-robot collaboration via deep metric learning. (arXiv:2302.11933v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.11933](http://arxiv.org/abs/2302.11933)

    本论文提出了一种通过深度度量学习提高人机物理协作安全性的新方法，使机器人能够学习准确捕捉人类形态和运动能力的距离度量，从而降低人机交互过程中发生碰撞和受伤的风险。

    

    直接与机器人进行物理交互在灵活生产场景中变得越来越重要，但没有保护栅栏的机器人对操作者的风险也更大。为了保持风险低，规定了相对简单的操作措施，例如如果有物理接触或违反安全距离则停止机器人。尽管这样可以在很大程度上避免人员受伤，但所有这些解决方案的共同点在于人与机器人之间的真正合作几乎不可能，因此无法充分发挥使用这些系统的优势。在人机协作场景中，需要更复杂的解决方案，使机器人的行为能够适应操作者和/或当前情况。最重要的是，在自由机器人移动期间，必须允许物理接触以进行有意义的交互，而不被视为碰撞。然而，这是未来机器人系统的主要挑战。在本文中，我们提出了一种通过深度度量学习提高人机物理协作安全性的新方法。该方法使机器人能够学习准确捕捉人类形态和运动能力的距离度量，从而降低人机交互过程中发生碰撞和受伤的风险。我们的方法在一组基准数据集上进行了评估，并在安全性和性能方面优于现有方法。

    Direct physical interaction with robots is becoming increasingly important in flexible production scenarios, but robots without protective fences also pose a greater risk to the operator. In order to keep the risk potential low, relatively simple measures are prescribed for operation, such as stopping the robot if there is physical contact or if a safety distance is violated. Although human injuries can be largely avoided in this way, all such solutions have in common that real cooperation between humans and robots is hardly possible and therefore the advantages of working with such systems cannot develop its full potential. In human-robot collaboration scenarios, more sophisticated solutions are required that make it possible to adapt the robot's behavior to the operator and/or the current situation. Most importantly, during free robot movement, physical contact must be allowed for meaningful interaction and not recognized as a collision. However, here lies a key challenge for future 
    
[^79]: 利用共享微指数，微小的位移带来了巨大的变化

    With Shared Microexponents, A Little Shifting Goes a Long Way. (arXiv:2302.08007v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08007](http://arxiv.org/abs/2302.08007)

    本论文提出了一种名为BDR的框架，用于评估窄精度格式。通过BDR，本文发现了一种基于共享微指数的新格式，其在大规模生成预训练和推理以及推荐系统方面的效果优于其他先进的量化方法。

    

    本文介绍了一种名为模块数据表示（BDR）的框架，用于探索和评估一系列用于深度学习的窄精度格式。它能够比较流行的量化标准，并通过BDR，发现了一种基于共享微指数（MX）的新格式，其优于其他最先进的量化方法，包括窄精度浮点和块浮点。MX 在硬件中利用多个量化级别，使用基于共享微指数的超细缩放因子。MX 的有效性在包括大规模生成预训练和推理以及生产规模推荐系统在内的实际模型上得到了证明。

    This paper introduces Block Data Representations (BDR), a framework for exploring and evaluating a wide spectrum of narrow-precision formats for deep learning. It enables comparison of popular quantization standards, and through BDR, new formats based on shared microexponents (MX) are identified, which outperform other state-of-the-art quantization approaches, including narrow-precision floating-point and block floating-point. MX utilizes multiple levels of quantization scaling with ultra-fine scaling factors based on shared microexponents in the hardware. The effectiveness of MX is demonstrated on real-world models including large-scale generative pretraining and inferencing, and production-scale recommendation systems.
    
[^80]: 追求机器学习研究的推理复现性

    Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04054](http://arxiv.org/abs/2302.04054)

    本研究提出利用线性混合效应模型（LMEM）来分析机器学习性能评估分数，并考虑多个方差来源及其与数据特性相互作用，从而评估可靠性和可复制性，促进对机器学习算法行为的更全面理解。

    

    机器学习评估的可靠性——即在复制的模型训练运行中观察到的评估分数的一致性——受到几种非确定性来源的影响，可以被视为测量噪声。目前的趋势是去除噪声，以强制研究结果的可复制性，忽略了实现层面固有的非确定性以及算法噪声因素和数据特性之间的关键相互作用效应。这限制了从这些实验中可以得出的结论范围。我们提出的方法是将几个方差来源，包括它们与数据特性的相互作用，纳入机器学习评估的显著性和可靠性分析中，以期从训练模型的特定实例得出推理结论, 而非去除噪声。我们展示如何使用线性混合效应模型（LMEM）来分析性能评估分数，并用广义似然比检验进行统计推断。我们的方法提供了一种系统的方式来考虑算法和数据相关的噪声来源，并使我们能够量化各个方差来源对机器学习实验的可靠性和可复制性的影响。我们在一系列合成和真实数据集上演示了我们方法的实用性，并说明了我们的方法如何促进对机器学习算法行为的更全面理解。

    Reliability of machine learning evaluation -- the consistency of observed evaluation scores across replicated model training runs -- is affected by several sources of nondeterminism which can be regarded as measurement noise. Current tendencies to remove noise in order to enforce reproducibility of research results neglect inherent nondeterminism at the implementation level and disregard crucial interaction effects between algorithmic noise factors and data properties. This limits the scope of conclusions that can be drawn from such experiments. Instead of removing noise, we propose to incorporate several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We show how to use linear mixed effects models (LMEMs) to analyze performance evaluation scores, and to conduct statistical inference with a generalized lik
    
[^81]: 合理依赖AI建议的概念化及解释的影响。

    Appropriate Reliance on AI Advice: Conceptualization and the Effect of Explanations. (arXiv:2302.02187v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.02187](http://arxiv.org/abs/2302.02187)

    本研究提出了一个可量化的二维测量概念——适当依赖性（AoR），并且通过在实验中分析AI建议解释的效果来从根本上贡献了对于依赖行为的分析。

    

    AI建议在投资和医疗决策等领域变得越来越流行。由于这些建议通常是不完美的，决策者必须自行决定是否实际上遵循这些建议：他们必须“适当地”依赖于正确的建议并拒绝错误的建议。然而，当前关于适当依赖的研究仍缺乏一个共同的定义以及一个操作性的测量概念。此外，还没有进行深入的行为实验来帮助理解影响这种行为的因素。在本文中，我们提出了适当依赖性（AoR）作为一个可量化的二维测量概念。我们开发了一个研究模型，分析提供AI建议解释的效果。在一项涉及200名参与者的实验中，我们展示了这些解释如何影响AoR，从而影响AI建议的有效性。我们的工作为依赖行为的分析贡献了基本概念。

    AI advice is becoming increasingly popular, e.g., in investment and medical treatment decisions. As this advice is typically imperfect, decision-makers have to exert discretion as to whether actually follow that advice: they have to "appropriately" rely on correct and turn down incorrect advice. However, current research on appropriate reliance still lacks a common definition as well as an operational measurement concept. Additionally, no in-depth behavioral experiments have been conducted that help understand the factors influencing this behavior. In this paper, we propose Appropriateness of Reliance (AoR) as an underlying, quantifiable two-dimensional measurement concept. We develop a research model that analyzes the effect of providing explanations for AI advice. In an experiment with 200 participants, we demonstrate how these explanations influence the AoR, and, thus, the effectiveness of AI advice. Our work contributes fundamental concepts for the analysis of reliance behavior and
    
[^82]: 具有联合嵌入预测架构的图像自监督学习

    Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture. (arXiv:2301.08243v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.08243](http://arxiv.org/abs/2301.08243)

    本论文提出了一种非生成方法的自监督学习架构，即Image-based Joint-Embedding Predictive Architecture（I-JEPA），可以生成高度语义图像表示，通过联合嵌入预测架构和掩模策略达到这一目的。

    

    本论文提出了一种在不依赖手工制作数据增强的情况下学习高度语义图像表示的方法。我们介绍了基于图像的联合嵌入预测架构（I-JEPA），这是一种从图像中进行自我监督学习的非生成方法。I-JEPA的核心设计选择是掩模策略，以引导I-JEPA产生语义表示。当与Vision Transformers结合使用时，证明I-JEPA具有高度可扩展性。

    This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting a
    
[^83]: 谁来评估评估者？关于用于评估基于人工智能的攻击代码生成器的自动度量的研究

    Who Evaluates the Evaluators? On Automatic Metrics for Assessing AI-based Offensive Code Generators. (arXiv:2212.06008v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2212.06008](http://arxiv.org/abs/2212.06008)

    本研究分析了用于评估攻击代码生成器的大量输出相似度度量标准，判断自动度量标准与人工评估结果之间的相关性，并提出了选取适当度量标准的指南。

    

    基于人工智能的代码生成器是从自然语言描述出发自动编写程序的新兴解决方案，采用深度神经网络（神经机器翻译）。特别地，代码生成器已经被用于进行伦理黑客和攻击性安全测试，以生成攻击的概念证明。不幸的是，代码生成器的评估仍然面临着许多问题。当前的做法是使用输出相似度度量标准，即自动度量标准来计算生成代码与参考翻译之间的文本相似度。然而，并不清楚应该使用哪种度量标准，以及哪种度量标准最适用于特定的情况。本研究分析了用于评估攻击代码生成器的大量输出相似度度量标准。我们将这些度量标准应用于两种最先进的神经机器翻译模型，使用包含英语语言描述的攻击装配代码和Python代码的两个数据集。我们比较了自动度量标准的估计值与生成代码的人工评估结果，以评估两者之间的相关性。我们的结果表明，相似度度量标准的选择对生成器的评估有重要影响，而自动度量标准可能不足以捕捉生成代码的质量。此外，我们提出了选取适当的度量标准的指南，并讨论了这项研究的局限性和未来方向。

    AI-based code generators are an emerging solution for automatically writing programs starting from descriptions in natural language, by using deep neural networks (Neural Machine Translation, NMT). In particular, code generators have been used for ethical hacking and offensive security testing by generating proof-of-concept attacks. Unfortunately, the evaluation of code generators still faces several issues. The current practice uses output similarity metrics, i.e., automatic metrics that compute the textual similarity of generated code with ground-truth references. However, it is not clear what metric to use, and which metric is most suitable for specific contexts. This work analyzes a large set of output similarity metrics on offensive code generators. We apply the metrics on two state-of-the-art NMT models using two datasets containing offensive assembly and Python code with their descriptions in the English language. We compare the estimates from the automatic metrics with human ev
    
[^84]: RPN: 一种深度学习中基于词向量的数据增强算法，用于语言理解

    RPN: A Word Vector Level Data Augmentation Algorithm in Deep Learning for Language Understanding. (arXiv:2212.05961v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.05961](http://arxiv.org/abs/2212.05961)

    RPN是一种基于词向量级别的数据增强算法，通过引入噪声修改原始文本的词嵌入，更好地捕捉自然语言变化，并在自然语言理解任务中表现出优异的性能。

    

    数据增强是机器学习中广泛使用以提高模型性能的技术。然而，现有的自然语言理解数据增强技术可能无法完全捕捉到自然语言的复杂变化，并且在大型数据集中应用起来具有挑战性。本文提出了一种新颖的数据增强技术——随机位置噪声（RPN）算法，它在词向量级别上进行操作。RPN通过根据选定词向量的现有值引入噪声修改原始文本的词嵌入，允许更细粒度的修改并更好地捕捉自然语言变化。与传统的数据增强方法不同，RPN不需要计算图中的梯度来进行虚拟样本更新，使其更容易应用于大型数据集。实验结果表明，在各种自然语言理解任务中，包括情感分析等，RPN始终优于现有数据增强技术。

    Data augmentation is a widely used technique in machine learning to improve model performance. However, existing data augmentation techniques in natural language understanding (NLU) may not fully capture the complexity of natural language variations, and they can be challenging to apply to large datasets. This paper proposes the Random Position Noise (RPN) algorithm, a novel data augmentation technique that operates at the word vector level. RPN modifies the word embeddings of the original text by introducing noise based on the existing values of selected word vectors, allowing for more fine-grained modifications and better capturing natural language variations. Unlike traditional data augmentation methods, RPN does not require gradients in the computational graph during virtual sample updates, making it simpler to apply to large datasets. Experimental results demonstrate that RPN consistently outperforms existing data augmentation techniques across various NLU tasks, including sentime
    
[^85]: 医疗人工智能模型非均一标签的协同训练

    Collaborative Training of Medical Artificial Intelligence Models with non-uniform Labels. (arXiv:2211.13606v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.13606](http://arxiv.org/abs/2211.13606)

    本研究提出了一种灵活的联合学习方法(FFL)，可以协同训练异质化标注数据集来构建强大而稳健的深度学习模型，为不同机构间的广泛合作提供了一种有前途的替代方案。

    

    近年来，深度学习已经成为医学图像分析领域的主流手段。然而，构建强大而稳健的深度学习模型需要使用大规模多方数据集进行训练。虽然多个利益相关者已经提供了公开的数据集，但是这些数据的标签方式差异很大。为了解决这一问题，本文提出了一种灵活的联合学习方法：灵活联合学习(FFL)。通过对来自全球五个机构的695,000个具有不同标签的胸部透视图进行协同训练，我们证明了在异质化标注数据集情况下使用FFL可以比传统的联邦学习获得更好的性能，并提供了在机构之间进行广泛协作的一种有前途的替代方法。

    Due to the rapid advancements in recent years, medical image analysis is largely dominated by deep learning (DL). However, building powerful and robust DL models requires training with large multi-party datasets. While multiple stakeholders have provided publicly available datasets, the ways in which these data are labeled vary widely. For Instance, an institution might provide a dataset of chest radiographs containing labels denoting the presence of pneumonia, while another institution might have a focus on determining the presence of metastases in the lung. Training a single AI model utilizing all these data is not feasible with conventional federated learning (FL). This prompts us to propose an extension to the widespread FL process, namely flexible federated learning (FFL) for collaborative training on such data. Using 695,000 chest radiographs from five institutions from across the globe each with differing labels - we demonstrate that having heterogeneously labeled datasets, FF
    
[^86]: 基于数据幻象的迭代式教学

    Iterative Teaching by Data Hallucination. (arXiv:2210.17467v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17467](http://arxiv.org/abs/2210.17467)

    本文提出了基于数据幻象教学（DHT）的迭代式教学方法，解决了在连续输入空间下教师提供示例的能力问题，并在多个挑战性的教学设置中验证了DHT的有效性。

    

    本文研究了在连续输入空间下迭代教学的问题，即教师根据学习者的状态和目标概念提供示例。我们提出了数据幻象教学（DHT）方法，在有限的样本空间内，通过智能地生成输入数据，解决了教师提供示例的能力问题。我们考虑了许多挑战性的教学设置，并进行了广泛的实证研究，验证了DHT的有效性。

    We consider the problem of iterative machine teaching, where a teacher sequentially provides examples based on the status of a learner under a discrete input space (i.e., a pool of finite samples), which greatly limits the teacher's capability. To address this issue, we study iterative teaching under a continuous input space where the input example (i.e., image) can be either generated by solving an optimization problem or drawn directly from a continuous distribution. Specifically, we propose data hallucination teaching (DHT) where the teacher can generate input data intelligently based on labels, the learner's status and the target concept. We study a number of challenging teaching setups (e.g., linear/neural learners in omniscient and black-box settings). Extensive empirical results verify the effectiveness of DHT.
    
[^87]: 处理算法（误）信息分类中的偶然性：走向负责任的机器学习议程

    Addressing contingency in algorithmic (mis)information classification: Toward a responsible machine learning agenda. (arXiv:2210.09014v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2210.09014](http://arxiv.org/abs/2210.09014)

    本文重点讨论了机器学习（ML）启用的分类模型处理在线虚假信息和其他可能被识别为有害的内容时，对“真相”来源的合法性、权威性和客观性所采取的立场以及ML驱动的审查系统可能在不利影响方面产生的问题，分析了算法的偶然性和可能引起的评估误差。

    

    机器学习（ML）启用的分类模型越来越受欢迎，用于解决庞大且速度快的在线虚假信息和其他可能被识别为有害的内容。在构建这些模型时，数据科学家需要对用于模型训练和测试的“真相”来源的合法性、权威性和客观性采取立场。这涉及政治、伦理和认识论方面的问题，其在技术论文中很少得到解决。尽管（也就是由于）其报告的高准确性和性能，由ML驱动的审查系统可能会塑造在线公共辩论，并产生负面影响，如不当审查和强化错误信念。我们采用合作的民族志学和社会科学和专业知识的理论洞见，对建立（误）信息分类的ML模型的过程进行了批判性分析，以识别一系列算法的偶然性——关键的模型决策点，这些决策点可能会在技术实现中引入自己的诠释和评估。

    Machine learning (ML) enabled classification models are becoming increasingly popular for tackling the sheer volume and speed of online misinformation and other content that could be identified as harmful. In building these models, data scientists need to take a stance on the legitimacy, authoritativeness and objectivity of the sources of ``truth" used for model training and testing. This has political, ethical and epistemic implications which are rarely addressed in technical papers. Despite (and due to) their reported high accuracy and performance, ML-driven moderation systems have the potential to shape online public debate and create downstream negative impacts such as undue censorship and the reinforcing of false beliefs. Using collaborative ethnography and theoretical insights from social studies of science and expertise, we offer a critical analysis of the process of building ML models for (mis)information classification: we identify a series of algorithmic contingencies--key mo
    
[^88]: 面向共享自动驾驶出行服务的预测式车队调度：一种基于优化和学习的方法

    Anticipatory Fleet Repositioning for Shared-use Autonomous Mobility Services: An Optimization and Learning-Based Approach. (arXiv:2210.08659v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2210.08659](http://arxiv.org/abs/2210.08659)

    本文提出一种基于优化和学习的方法，通过预测未来需求和合作优化基于分配策略的重新平衡策略，从而改善SAMS车队的服务质量和效率，并在真实数据集上进行的数值实验表明该方法较传统启发式方法具有更好的性能和可靠性。

    

    移动出行服务、丰富的交通数据和自动驾驶汽车的发展为共享自动驾驶出行服务（SAMS）提供了重要机遇，以提供可访问和需求响应性的个人出行。本文旨在通过预测性地重新调度空闲的车辆，提高SAMS车队的效率和服务质量。把再平衡问题作为马可夫决策过程进行建模，并提出使用基于优势的演员-评论家（A2C）强化学习方法来解决该问题。所提出的方法学习了一种重新平衡策略，它预测未来需求并与基于优化的分配策略合作。该方法允许集中式的重新定位决策，并且可以处理大型汽车车队，因为问题大小不超过Fleetsize^2。

    The development of mobility-on-demand services, rich transportation data sources, and autonomous vehicles (AVs) creates significant opportunities for shared-use AV mobility services (SAMSs) to provide accessible and demand-responsive personal mobility. SAMS fleet operation involves multiple interrelated decisions, with a primary focus on efficiently fulfilling passenger ride requests with a high level of service quality. This paper focuses on improving the efficiency and service quality of a SAMS vehicle fleet via anticipatory repositioning of idle vehicles. The rebalancing problem is formulated as a Markov Decision Process, which we propose solving using an advantage actor critic (A2C) reinforcement learning-based method. The proposed approach learns a rebalancing policy that anticipates future demand and cooperates with an optimization-based assignment strategy. The approach allows for centralized repositioning decisions and can handle large vehicle fleets since the problem size does
    
[^89]: SQA3D：三维场景中的情境问答

    SQA3D: Situated Question Answering in 3D Scenes. (arXiv:2210.07474v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.07474](http://arxiv.org/abs/2210.07474)

    本文提出了一个新的任务，即评估具有场景理解能力的代理人在三维场景中的情境问答。基于650个场景的数据集为智能代理人的推理能力考察提供了广泛且大量的问题，这对当前的多模式，特别是3D推理模型提出了很大挑战。

    

    我们提出了一个新的任务来评估具有场景理解能力的代理人：三维场景中的情境问答（SQA3D）。给定一个场景上下文（例如三维扫描），SQA3D要求经过测试的代理人首先理解其在文本描述下的3D场景中的情境（位置、方向等），然后在该情境下进行推理，回答一个问题。基于来自ScanNet的650个场景，我们提供了一个数据集，其中心围绕6.8k个唯一情境，20.4k的描述和33.4k多样的推理问题。这些问题涵盖了对智能代理人范围广泛的推理能力的考察，从空间关系理解到常识理解、导航和多跳推理。SQA3D对当前的多模式尤其是3D推理模型提出了重大挑战。我们评估了各种最先进的方法，并发现最佳结果仅达到了47.20%的总体得分，而业余水平的表现更为糟糕。

    We propose a new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g., 3D scan), SQA3D requires the tested agent to first understand its situation (position, orientation, etc.) in the 3D scene as described by text, then reason about its surrounding environment and answer a question under that situation. Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. These questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateu
    
[^90]: 关于引导式扩散模型的蒸馏

    On Distillation of Guided Diffusion Models. (arXiv:2210.03142v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.03142](http://arxiv.org/abs/2210.03142)

    本论文提出了一种将免分类器的引导式扩散模型蒸馏为易于采样的模型的方法，以降低在推断时的计算成本，并且能够在像素空间生成高质量的图像。

    

    最近，免分类器的引导式扩散模型已被证明在高分辨率图像生成方面非常有效，它们已广泛用于包括 DALLE-2、Stable Diffusion 和 Imagen 在内的大规模扩散框架中。然而，免分类器的引导式扩散模型的缺点是，在推断时计算成本很高，因为需要评估两个扩散模型（一个类有条件的模型和一个无条件的模型）数十到数百次。为了解决这个限制，我们提出了一种将免分类器的引导式扩散模型蒸馏为易于采样的模型的方法: 给定一个经过预训练的免分类器引导模型，我们首先学习一个单一的模型以匹配联合有条件和无条件模型的输出，然后逐步将该模型蒸馏到只需要更少的采样步骤的扩散模型。对于在像素空间训练的标准扩散模型，我们的方法能够生成高质量的图像。

    Classifier-free guided diffusion models have recently been shown to be highly effective at high-resolution image generation, and they have been widely used in large-scale diffusion frameworks including DALLE-2, Stable Diffusion and Imagen. However, a downside of classifier-free guided diffusion models is that they are computationally expensive at inference time since they require evaluating two diffusion models, a class-conditional model and an unconditional model, tens to hundreds of times. To deal with this limitation, we propose an approach to distilling classifier-free guided diffusion models into models that are fast to sample from: Given a pre-trained classifier-free guided model, we first learn a single model to match the output of the combined conditional and unconditional models, and then we progressively distill that model to a diffusion model that requires much fewer sampling steps. For standard diffusion models trained on the pixel-space, our approach is able to generate im
    
[^91]: PiFold: 实现高效和有效的蛋白质逆向折叠

    PiFold: Toward effective and efficient protein inverse folding. (arXiv:2209.12643v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.12643](http://arxiv.org/abs/2209.12643)

    PiFold 提出了一种新的残基特征提取器和 PiGNN 层，能够一次性生成蛋白质序列并提高恢复效果，在 CATH 4.2 上达到了 51.66% 的恢复率，推理速度比自回归竞争对手快 70 倍，在 TS50 和 TS500 上分别达到了 58.72% 和 60.42% 的恢复分数。

    

    如何高效而有效地设计能够折叠成所需结构的蛋白序列？近年来，基于结构的蛋白质设计的AI方法已经引起了越来越多的关注；然而，由于特征表达不够充分和自回归序列解码器的缺乏，很少有方法能够同时提高准确性和效率。为了解决这些问题，我们提出了PiFold，它包含了一种新的残基特征提取器和PiGNN层，以一次性生成蛋白质序列并提高恢复效果。实验表明，PiFold 在 CATH 4.2 上能够达到 51.66\% 的恢复率，而推理速度比自回归竞争对手快 70 倍。此外，PiFold 在 TS50 和 TS500 上分别达到了 58.72\% 和 60.42\% 的恢复分数。我们进行了全面的消融研究，揭示了不同类型蛋白质特征和模型设计的作用，为进一步简化和改进提供了灵感。PyTorch 代码可在 \href{https://github.com/idea-iitp/PiFold}{https://github.com/idea-iitp/PiFold} 上获取。

    How can we design protein sequences folding into the desired structures effectively and efficiently? AI methods for structure-based protein design have attracted increasing attention in recent years; however, few methods can simultaneously improve the accuracy and efficiency due to the lack of expressive features and autoregressive sequence decoder. To address these issues, we propose PiFold, which contains a novel residue featurizer and PiGNN layers to generate protein sequences in a one-shot way with improved recovery. Experiments show that PiFold could achieve 51.66\% recovery on CATH 4.2, while the inference speed is 70 times faster than the autoregressive competitors. In addition, PiFold achieves 58.72\% and 60.42\% recovery scores on TS50 and TS500, respectively. We conduct comprehensive ablation studies to reveal the role of different types of protein features and model designs, inspiring further simplification and improvement. The PyTorch code is available at \href{https://gith
    
[^92]: PePe: 利用用户生成的后编辑实现个性化的后编辑模型

    PePe: Personalized Post-editing Model utilizing User-generated Post-edits. (arXiv:2209.10139v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.10139](http://arxiv.org/abs/2209.10139)

    本文提出了一种个性化的自动后编辑框架PePe，在一个实时的机器翻译系统中，通过收集用户后编辑数据，结合判别模块和用户特定参数，能够有效地生成考虑到不同个人行为的句子，并在多个度量标准上优于其他基线模型。

    

    在先进的机器翻译任务中，将个人的偏好加以考虑至关重要。尽管机器翻译已经取得了近期的重大进展，但是正确反映个人风格仍然是一个具有挑战性的任务。在本论文中，我们引入了一个个性化的自动后编辑框架来解决这个难题，这个框架能够有效地生成考虑到不同个人行为的句子。为了构建这个框架，我们首先从实时机器翻译系统中收集包含用户偏好的后编辑数据。具体而言，真实用户输入想要翻译的源语句，并根据用户的风格偏好编辑机器翻译的输出。接着，我们提出了一个模型，在APE框架上结合了一个判别模块和用户特定的参数。实验结果表明，所提出的方法在四种不同的度量标准（即BLEU，TER，YiSi-1和人类评估）上优于其他基线模型。

    Incorporating personal preference is crucial in advanced machine translation tasks. Despite the recent advancement of machine translation, it remains a demanding task to properly reflect personal style. In this paper, we introduce a personalized automatic post-editing framework to address this challenge, which effectively generates sentences considering distinct personal behaviors. To build this framework, we first collect post-editing data that connotes the user preference from a live machine translation system. Specifically, real-world users enter source sentences for translation and edit the machine-translated outputs according to the user's preferred style. We then propose a model that combines a discriminator module and user-specific parameters on the APE framework. Experimental results show that the proposed method outperforms other baseline models on four different metrics (i.e., BLEU, TER, YiSi-1, and human evaluation).
    
[^93]: 无需不可能数据的变量重要性分析方法

    Variable importance without impossible data. (arXiv:2205.15750v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15750](http://arxiv.org/abs/2205.15750)

    评估黑箱预测模型中变量重要性的流行方法不可信，因为使用了不可能的数据。本文提出一种名为Cohort Shapley的方法，它基于经济博弈理论，仅使用实际观测到的数据来量化变量重要性，可以解决算法公平性问题。

    

    目前，评估黑箱预测模型中变量重要性的最流行方法是使用人工合成的输入数据，这些数据结合了多个参与者的预测变量，这些输入数据可能是不可能的、物理上不可能的，甚至是逻辑上不可能的，由此得出的预测结果可能与黑箱训练数据有很大不同。因此，当解释决策时使用这些值时，用户不能信任预测算法的解释。我们提倡一种名为Cohort Shapley的方法，它基于经济博弈理论，与大多数其他博弈论方法不同，仅使用实际观测到的数据来量化变量重要性。Cohort Shapley通过缩小与目标对象在一个或多个特征上相似的对象组来实现。我们将其应用于一个算法公平性问题，其中必须将重要性归因于模型未经训练的受保护变量。

    The most popular methods for measuring importance of the variables in a black box prediction algorithm make use of synthetic inputs that combine predictor variables from multiple subjects. These inputs can be unlikely, physically impossible, or even logically impossible. As a result, the predictions for such cases can be based on data very unlike any the black box was trained on. We think that users cannot trust an explanation of the decision of a prediction algorithm when the explanation uses such values. Instead we advocate a method called Cohort Shapley that is grounded in economic game theory and unlike most other game theoretic methods, it uses only actually observed data to quantify variable importance. Cohort Shapley works by narrowing the cohort of subjects judged to be similar to a target subject on one or more features. We illustrate it on an algorithmic fairness problem where it is essential to attribute importance to protected variables that the model was not trained on.
    
[^94]: Bongard-HOI：基于人-物交互的几种情况视觉推理基准测试

    Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object Interactions. (arXiv:2205.13803v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.13803](http://arxiv.org/abs/2205.13803)

    本文提出了一个新的视觉推理基准测试Bongard-HOI，它专注于从自然图像中学习人-物交互的组合性学习，挑战了现有最先进模型，鼓励开发更好的算法进行组合推理和泛化到新的HOI概念。

    

    目前，当涉及到新概念的几种情况学习和组合推理时，今天的视觉图案识别模型和人类级别的视觉认知之间仍存在巨大差距。本文引入了Bongard-HOI，这是一个新的视觉推理基准测试，侧重于从自然图像中学习人-物交互的组合性学习。它受到古典的Bongard问题（BPs）中的两个可取特征的启发：1）几种情况的概念学习和2）上下文相关的推理。我们精心策划了少样本实例包含困难的负例，其中正负图像仅在动作标签上存在差异，仅仅对对象类别的识别不足以完成我们的基准测试。我们还设计了多个测试集，以系统研究视觉学习模型的泛化能力，其中我们改变了几种情况实例的训练和测试集之间的HOI概念重叠程度，从部分到没有重叠。Bongard-HOI对现有的最先进模型提出了实质性挑战，并为开发可以进行组合推理并能够推广到新的HOI概念的新算法提供了强有力的动力。

    A significant gap remains between today's visual pattern recognition models and human-level visual cognition especially when it comes to few-shot learning and compositional reasoning of novel concepts. We introduce Bongard-HOI, a new visual reasoning benchmark that focuses on compositional learning of human-object interactions (HOIs) from natural images. It is inspired by two desirable characteristics from the classical Bongard problems (BPs): 1) few-shot concept learning, and 2) context-dependent reasoning. We carefully curate the few-shot instances with hard negatives, where positive and negative images only disagree on action labels, making mere recognition of object categories insufficient to complete our benchmarks. We also design multiple test sets to systematically study the generalization of visual learning models, where we vary the overlap of the HOI concepts between the training and test sets of few-shot instances, from partial to no overlaps. Bongard-HOI presents a substanti
    
[^95]: 分而治之的模仿学习 - Divide & Conquer Imitation Learning

    Divide & Conquer Imitation Learning. (arXiv:2204.07404v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2204.07404](http://arxiv.org/abs/2204.07404)

    该论文提出了一种新颖的模仿学习算法，通过将复杂任务划分为更小的技能，利用顺序归纳偏见将技能链接以解决整个任务。该算法可以在只有一个专家演示可用的情况下模仿复杂的机器人任务。

    

    当深度强化学习框架用于解决许多机器人任务时，需要解决长时间跨度和奖励稀疏的问题，学习算法很难应对。在这种情况下，模仿学习（IL）可以是启动学习过程的有力方法。然而，大多数IL方法需要多个专家演示，这可能很难获得。只有少数IL算法在极低的专家数据情况下表现出有效性，只有一个专家演示可用。在本文中，我们提出了一种新颖的算法，旨在从专家轨迹的状态中模仿复杂的机器人任务。基于顺序归纳偏见，我们的方法将复杂任务划分为更小的技能。这些技能被学习成为一个目标条件策略，该策略能够独立地解决每个技能，并链接技能以解决整个任务。我们展示了我们的方法模仿了一个非完整导航任务，并扩展到了复杂的仿真环境中。

    When cast into the Deep Reinforcement Learning framework, many robotics tasks require solving a long horizon and sparse reward problem, where learning algorithms struggle. In such context, Imitation Learning (IL) can be a powerful approach to bootstrap the learning process. However, most IL methods require several expert demonstrations which can be prohibitively difficult to acquire. Only a handful of IL algorithms have shown efficiency in the context of an extreme low expert data regime where a single expert demonstration is available. In this paper, we present a novel algorithm designed to imitate complex robotic tasks from the states of an expert trajectory. Based on a sequential inductive bias, our method divides the complex task into smaller skills. The skills are learned into a goal-conditioned policy that is able to solve each skill individually and chain skills to solve the entire task. We show that our method imitates a non-holonomic navigation task and scales to a complex sim
    
[^96]: 将AI规划与自然语言处理集成：显式和隐式知识的组合

    Integrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge. (arXiv:2202.07138v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2202.07138](http://arxiv.org/abs/2202.07138)

    本文提出了一个框架，将AI规划与自然语言处理相结合，利用显式和隐式知识来改善生成自然语言的效果。

    

    自然语言处理旨在研究代理人和人类之间的交互，处理和分析大量的自然语言数据。大规模的语言模型在当前自然语言处理中起着重要作用。然而，语言模型的发展带来了可解释性和复杂性的挑战。一种方法是将逻辑关系和规则引入自然语言处理模型中，例如利用自动规划技术。自动规划即AI规划，侧重于构建符号域模型并综合规划，以基于域模型将初始状态转换为目标。最近，有许多与这两个领域相关的工作，可以产生显式知识，例如操作模型的前提条件和效果，并从隐式知识（例如神经模型）中学习。有效地将AI规划与自然语言处理集成可以改善人与机器之间的交流。本文提出了一个框架，该框架结合显式和隐式知识，用于将AI规划与自然语言处理相结合。具体而言，我们首先使用AI规划技术构建域模型并推理自然语言中的逻辑关系。然后，我们利用大规模预训练的语言模型从隐式知识中学习，并改善自然语言的生成。最后，我们在一个真实的数据到文本生成任务上展示了我们的方法的有效性。

    Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning. Automated planning (AI planning) focuses on building symbolic domain models and synthesizing plans to transit initial states to goals based on domain models. Recently, there have been plenty of works related to these two fields, which have the abilities to generate explicit knowledge, e.g., preconditions and effects of action models, and learn from tacit knowledge, e.g., neural models, respectively. Integrating AI planning and natural language processing effectively improves the communication b
    
[^97]: 实践中的AI伦理原则：设计师和开发者的视角

    AI Ethics Principles in Practice: Perspectives of Designers and Developers. (arXiv:2112.07467v6 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2112.07467](http://arxiv.org/abs/2112.07467)

    这篇论文研究了设计师和开发人员的实践和经验，分析其如何遵循澳大利亚政府提出的高层次AI伦理原则，并总结出原则之间存在的张力和权衡。

    

    随着各种已发布的AI伦理原则的共识逐渐形成，高层次的原则和可立即采用的实际技术之间存在巨大差距，而这些技术可用于设计和开发负责任的AI系统。我们研究了来自澳大利亚国家科学研究机构(CSIRO)的研究人员和工程师的实践和经验，他们参与设计和开发许多应用领域的AI系统。我们使用半结构化访谈来研究参与者的实践如何与澳大利亚政府提出的一组高层次AI伦理原则相联系和一致。这些原则包括：(1)隐私保护和安全、(2)可靠性和安全性、(3)透明度和可解释性、(4)公正性、(5)可争议性、(6)责任制、(7)以人为中心的价值观、(8)人类、社会和环境的福祉。通过对访谈所获得的洞见的讨论，包括原则之间的各种张力和权衡。

    As consensus across the various published AI ethics principles is approached, a gap remains between high-level principles and practical techniques that can be readily adopted to design and develop responsible AI systems. We examine the practices and experiences of researchers and engineers from Australia's national scientific research agency (CSIRO), who are involved in designing and developing AI systems for many application areas. Semi-structured interviews were used to examine how the practices of the participants relate to and align with a set of high-level AI ethics principles proposed by the Australian Government. The principles comprise: (1) privacy protection and security, (2) reliability and safety, (3) transparency and explainability, (4) fairness, (5) contestability, (6) accountability, (7) human-centred values, (8) human, social and environmental wellbeing. Discussions on the gained insights from the interviews include various tensions and trade-offs between the principles,
    
[^98]: 可证收敛的精确率-召回曲线下面积随机优化

    Stochastic Optimization of Areas Under Precision-Recall Curves with Provable Convergence. (arXiv:2104.08736v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.08736](http://arxiv.org/abs/2104.08736)

    本文提出了一种用于深度学习的基于平均精度的方法来优化AUPRC，提出了一种可证收敛的SOAP算法。

    

    ROC下面积（AUROC）和精确率-召回曲线下面积（AUPRC）是用于评估不平衡问题分类性能的常见指标。与AUROC相比，AUPRC是高度不平衡数据集的更合适的指标。虽然关于AUROC的随机优化已得到广泛研究，但基于原则的AUPRC的随机优化却很少探索。本文提出了一种基于最大化平均精度（AP）的技术方法来优化深度学习的AUPRC的方法。我们将目标表示为依赖组合函数之和，其中内部函数依赖于外层的随机变量。我们通过利用随机组合优化的最新进展，提出了高效的自适应和非自适应随机算法SOAP，这些算法在温和条件下具有可证收敛性保证。在图像和图形数据集上进行了广泛的实验结果。

    Areas under ROC (AUROC) and precision-recall curves (AUPRC) are common metrics for evaluating classification performance for imbalanced problems. Compared with AUROC, AUPRC is a more appropriate metric for highly imbalanced datasets. While stochastic optimization of AUROC has been studied extensively, principled stochastic optimization of AUPRC has been rarely explored. In this work, we propose a principled technical method to optimize AUPRC for deep learning. Our approach is based on maximizing the averaged precision (AP), which is an unbiased point estimator of AUPRC. We cast the objective into a sum of {\it dependent compositional functions} with inner functions dependent on random variables of the outer level. We propose efficient adaptive and non-adaptive stochastic algorithms named SOAP with {\it provable convergence guarantee under mild conditions} by leveraging recent advances in stochastic compositional optimization. Extensive experimental results on image and graph datasets d
    

