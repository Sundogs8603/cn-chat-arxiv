# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SemiReward: A General Reward Model for Semi-supervised Learning.](http://arxiv.org/abs/2310.03013) | SemiReward是一个通用奖励模型，通过预测奖励分数来评估和过滤高质量的伪标签，可以应用于各种半监督学习任务，并在实验中取得了显著的成果。 |
| [^2] | [Soft Convex Quantization: Revisiting Vector Quantization with Convex Optimization.](http://arxiv.org/abs/2310.03004) | 本文提出了软凸量化（SCQ）作为向量量化（VQ）的替代方法，通过解决量化输入的码书向量的最优凸组合问题，缓解了VQ面临的实际挑战。 |
| [^3] | [ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models.](http://arxiv.org/abs/2310.02998) | ECoFLaP提出了一种高效的粗到细的逐层剪枝方法，解决了大型视觉语言模型在压缩和部署时的计算和能耗问题。 |
| [^4] | [Multiple Physics Pretraining for Physical Surrogate Models.](http://arxiv.org/abs/2310.02994) | 多物理学预训练是一种用于物理代理建模的自回归预训练方法，通过训练大型代理模型同时预测多个异构物理系统的动力学，学习在不同物理任务中广泛适用的特征。实验证明，单个MPP预训练的变换器可以在所有预训练子任务上与或超过特定任务的基准结果，无需微调，并且在下游任务中，微调MPP训练的模型相较于从头训练的模型，对新物理的预测结果更准确。 |
| [^5] | [xVal: A Continuous Number Encoding for Large Language Models.](http://arxiv.org/abs/2310.02989) | xVal是一种连续数字编码方案，通过使用单个标记来表示任何实数。与现有的数字编码方案相比，xVal更加高效，并且在泛化性能上表现更好。 |
| [^6] | [Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples.](http://arxiv.org/abs/2310.02988) | 本研究利用反事实例子探究了视觉-语言模型中的交叉偏见，并通过文本到图像扩散模型生成了一组高度相似但存在交叉社会属性差异的图像-文本对。 |
| [^7] | [Exploring the Impact of Disrupted Peer-to-Peer Communications on Fully Decentralized Learning in Disaster Scenarios.](http://arxiv.org/abs/2310.02986) | 在灾难场景中，完全去中心化学习可以帮助解决通信基础设施中断或不可用导致的传统集中式学习任务无法进行的问题。 |
| [^8] | [Scaling Laws for Associative Memories.](http://arxiv.org/abs/2310.02984) | 本文研究了应用于联想记忆中的缩放定律，通过高维矩阵和嵌入的外积来模拟内层Transformer语言模型。作者推导出了与样本数量和参数大小相关的精确缩放定律，并验证了理论结果的有效性。同时，作者还通过大量实验展示了存储记忆关联的细粒度可视化。 |
| [^9] | [Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits.](http://arxiv.org/abs/2310.02975) | 本文研究了在重尾波段问题中完全自适应的遗憾最小化，提出了随机自适应重尾波段问题，并证明了适应性算法相对于标准设置会有更高的遗憾。 |
| [^10] | [Credit card score prediction using machine learning models: A new dataset.](http://arxiv.org/abs/2310.02956) | 本研究探索了利用机器学习模型对信用卡违约进行预测的方法，并提出了一个新的信用卡评分数据集。实验结果表明，多层感知器（MLP）模型在预测性能上表现最佳。 |
| [^11] | [Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models.](http://arxiv.org/abs/2310.02949) | 该论文探讨了阴影对齐这种新的攻击方式，通过调整少量恶意示例，安全对齐的语言模型可以被轻松颠覆生成有害的内容，同时仍然可以正确响应常规查询。 |
| [^12] | [Local Max-Entropy and Free Energy Principles, Belief Diffusions and their Singularities.](http://arxiv.org/abs/2310.02946) | 本文综述了三个Bethe-Kikuchi变分原理，探讨了它们与超图上的信念传播算法的关系，并推广了BP方程结构，定义了连续时间扩散。此外，该文还研究了局部max-熵原理、变分自由能原理和平衡自由能原理，并描述了奇异信念的超曲面。 |
| [^13] | [Assessing Large Language Models on Climate Information.](http://arxiv.org/abs/2310.02932) | 本研究提出了一个基于科学传播原则的综合评估框架，评估了大规模语言模型在气候变化信息中的表现，能够在回答气候变化主题方面提供细粒度的分析。 |
| [^14] | [Learning-Aided Warmstart of Model Predictive Control in Uncertain Fast-Changing Traffic.](http://arxiv.org/abs/2310.02918) | 本文提出了一个学习辅助模型预测控制算法的热启动框架，通过利用神经网络多模式预测器生成多个轨迹提案，并通过采样技术进行进一步优化，以解决模型预测控制在非凸问题和不确定快速变化的环境中的局部最小值问题。 |
| [^15] | [Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with Visual and Textual Prompts.](http://arxiv.org/abs/2310.02906) | 本文提出了一种通过扩散模型结合视觉和文本提示来生成皮肤镜图像的方法，并证明了该方法在提高图像质量和皮肤病变分割性能方面的优势。 |
| [^16] | [Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers.](http://arxiv.org/abs/2310.02905) | 该论文提出了一种使用神经探测器和转换器优化指令的方法，以提高大型语言模型的性能。 |
| [^17] | [Searching for High-Value Molecules Using Reinforcement Learning and Transformers.](http://arxiv.org/abs/2310.02902) | 通过使用强化学习和Transformer，我们提出了一种新的基于RL的分子设计算法（ChemRLformer），并在25个分子设计任务中进行了综合分析，包括计算复杂的蛋白质对接模拟。我们发现了分子设计领域的独特见解，并展示了ChemRLformer相对于之前的工作更为简单且实现了最先进的性能。 |
| [^18] | [Notes on a Path to AI Assistance in Mathematical Reasoning.](http://arxiv.org/abs/2310.02896) | 这篇论文探讨了实现对研究数学家有用的人工智能助手的可能途径。 |
| [^19] | [Recent Methodological Advances in Federated Learning for Healthcare.](http://arxiv.org/abs/2310.02874) | 最近的医疗保健领域联邦学习研究提出了新的方法学，用于解决医疗保健数据的挑战，如孤立数据、类别不平衡、缺失数据、分布转移和非标准化变量。 |
| [^20] | [Stable and Interpretable Deep Learning for Tabular Data: Introducing InterpreTabNet with the Novel InterpreStability Metric.](http://arxiv.org/abs/2310.02870) | 我们引入了InterpreTabNet，通过改进的注意模块和TabNet架构，提高了表格数据的分类准确度和解释性。我们还提出了一种新的评价指标InterpreStability，用于量化模型的解释稳定性。 |
| [^21] | [A novel asymmetrical autoencoder with a sparsifying discrete cosine Stockwell transform layer for gearbox sensor data compression.](http://arxiv.org/abs/2310.02862) | 这篇论文提出了一种信号自适应的非对称自编码器，使用离散余弦Stockwell变换层进行齿轮传感器数据压缩。通过引入可训练的滤波器和硬阈值化层，该方法能够提高数据重构的准确性，并且仅需要少量数据集进行训练。 |
| [^22] | [Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection.](http://arxiv.org/abs/2310.02861) | 《Rayleigh Quotient Graph Neural Networks用于图级异常检测的研究》提出使用Rayleigh Quotient作为驱动因素，通过探索图的固有光谱特征来实现图级异常检测。 |
| [^23] | [Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation.](http://arxiv.org/abs/2310.02842) | 本论文提出了一种使用智能多任务适应混合提示的方法来解决LLM在处理异质任务和数据分布时的问题。研究者设计了智能门控功能，用于识别嵌入在不同提示组中的相关技能，并根据目标任务的需求动态分配组合专家。该方法对任何模型压缩技术都不受限制，提高了任务处理的效率。 |
| [^24] | [Improving Vision Anomaly Detection with the Guidance of Language Modality.](http://arxiv.org/abs/2310.02821) | 本文介绍了一种通过语言模态提高视觉异常检测的方法。我们提出了跨模态引导（CMG），它包括跨模态熵减少（CMER）和跨模态线性嵌入（CMLE），分别解决了冗余信息和稀疏空间问题。CMER通过遮盖图像的一部分并计算与文本的匹配分数，使检测器聚焦于关键内容。CMLE学习了一个相关的结构矩阵来学习更紧凑的视觉异常检测器的潜空间。 |
| [^25] | [Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms.](http://arxiv.org/abs/2310.02812) | 本研究通过严格实验评估了智能制造系统中最先进的机器学习和深度学习算法在时间序列分类任务中的性能，填补了该领域的研究空白。 |
| [^26] | [Discovering General Reinforcement Learning Algorithms with Adversarial Environment Design.](http://arxiv.org/abs/2310.02782) | 通过对抗性环境设计，我们提出了一种通用强化学习算法，通过元学习更新规则和自动生成课程来提高算法的泛化性能，并引入了一种新的遗憾近似方法，名为算法遗憾（AR）。 |
| [^27] | [A UMLS-Augmented Framework for Improving Factuality in Large Language Models within Healthcare.](http://arxiv.org/abs/2310.02778) | 该论文提出了一个基于UMLS的增强型大型语言模型框架，旨在改善医疗保健领域中模型生成内容的事实性。通过自动评估和医生评估，研究人员验证了该框架的有效性。 |
| [^28] | [Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks.](http://arxiv.org/abs/2310.02772) | 本研究提出了一种名为脉冲累积转发（SAF）的方法，可以有效训练脉冲神经网络（SNNs）。SAF不仅可以减少前向过程中的操作次数，与Spike Representation和OTTT保持一致，而且可以解决SNNs训练中的难题。 |
| [^29] | [MUNCH: Modelling Unique 'N Controllable Heads.](http://arxiv.org/abs/2310.02753) | 本论文提出了一种方法，可以自动生成质量高、多样性强、可控制的逼真三维人头，具有可解释的网络设计。方法包括几何生成器、渲染图生成器和颜色变换模型。同时还引入了独特性和新颖性的量化指标。 |
| [^30] | [Functional trustworthiness of AI systems by statistically valid testing.](http://arxiv.org/abs/2310.02727) | 作者认为欧盟AI法案对AI系统的质量保证方式存在不足，并指出基于统计学有效测试及准确定义应用是确保AI系统功能可信度的核心。 |
| [^31] | [Online Clustering of Bandits with Misspecified User Models.](http://arxiv.org/abs/2310.02717) | 本文介绍了在用户模型错误的情况下的聚类强化学习问题，并提出了两个鲁棒的聚类强化学习算法，以解决用户模型偏差的挑战。 |
| [^32] | [scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis in Brain.](http://arxiv.org/abs/2310.02713) | scHyena是一个基于Transformer架构的模型，称为单细胞Hyena(scHyena)，旨在处理大脑中的全长scRNA-seq数据，并提高分析的准确性。 |
| [^33] | [ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF.](http://arxiv.org/abs/2310.02712) | ED-NeRF 提出了一种高效的 3D 场景编辑方法，通过将场景嵌入到潜空间中，得到更快速且更易于编辑的 NeRF 骨干。 |
| [^34] | [Continual Contrastive Spoken Language Understanding.](http://arxiv.org/abs/2310.02699) | COCONUT是一种类别增量学习方法，结合了经验重播和对比式学习，在语音理解领域中解决了模型在持续学习新任务时难以保持之前知识的问题。 |
| [^35] | [Bridging the Domain Gap by Clustering-based Image-Text Graph Matching.](http://arxiv.org/abs/2310.02692) | 通过基于聚类的图像-文本图匹配来弥合领域差距，学习领域不变特征以实现在未见过领域上的良好泛化能力，实验结果显示在公共数据集上达到最先进性能。 |
| [^36] | [USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields.](http://arxiv.org/abs/2310.02687) | USB-NeRF是一种解决滚动快门相机问题的神经辐射场算法，能够纠正滚动快门失真并恢复准确的相机运动轨迹，相比之前的方法在RS效应去除和新视角图像生成方面表现更好。 |
| [^37] | [Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization.](http://arxiv.org/abs/2310.02679) | 这项工作介绍了一种名为扩散生成流采样器（DGFS）的采样框架，通过将学习过程分解为短的部分轨迹段，实现从难以处理的高维密度函数中进行采样。它通过利用中间的学习信号和非策略探索能力来改善学习信号的分配问题。 |
| [^38] | [Land-cover change detection using paired OpenStreetMap data and optical high-resolution imagery via object-guided Transformer.](http://arxiv.org/abs/2310.02674) | 本文通过直接利用配对的OSM数据和光学图像进行土地覆盖变化检测，提出了一种基于对象引导的Transformer架构，从而拓宽了变化检测任务的范围，并显著减少了计算开销和内存负担。 |
| [^39] | [On Memorization in Diffusion Models.](http://arxiv.org/abs/2310.02664) | 本论文研究了扩散模型的记忆化行为，发现记忆化倾向于在较小的数据集上发生。通过定义有效模型记忆化 (EMM) 这一指标，量化了数据分布和模型配置对记忆化行为的影响。 |
| [^40] | [Solving Multi-Configuration Problems: A Performance Analysis with Choco Solver.](http://arxiv.org/abs/2310.02658) | 本文介绍了使用Choco Solver进行多配置问题求解的应用案例，以及对约束求解器性能分析的研究，从而揭示了相关性能问题。 |
| [^41] | [A Study of Quantisation-aware Training on Time Series Transformer Models for Resource-constrained FPGAs.](http://arxiv.org/abs/2310.02654) | 本研究探索了对时间序列Transformer模型进行量化感知训练的方法，并提出了一种新颖的自适应量化方案，通过匹配量化方案与实际数据分布，可以降低计算开销并保持可接受的精度。此外，该方法在应用于现实世界数据和混合精度量化时表现出鲁棒性。这些发现为模型量化和部署决策提供了参考，并推进了量化技术的发展。 |
| [^42] | [GET: Group Event Transformer for Event-Based Vision.](http://arxiv.org/abs/2310.02642) | GET提出了一种用于事件视觉的新型团体事件变换器，它能够将事件的时间和极性与空间信息解耦，通过事件双自注意块和团体标记聚合模块实现了在空间和时间-极性领域的特征通信和整合。 |
| [^43] | [Deformation-Invariant Neural Network and Its Applications in Distorted Image Restoration and Analysis.](http://arxiv.org/abs/2310.02641) | 本文提出了一种弹性不变神经网络（DINN）用于处理受到几何形变影响的图像的图像处理任务。DINN通过融入拟保形变换网络（QCTN）来输出一致的潜在特征，使得具有相同原始对象或场景的几何形变图像能够更接近自然或良好图像分布。 |
| [^44] | [Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance.](http://arxiv.org/abs/2310.02635) | 本研究提出了一种基于具身基础先验的基础强化学习框架，通过加速训练过程来提高样本效率。 |
| [^45] | [Multi-rules mining algorithm for combinatorially exploded decision trees with modified Aitchison-Aitken function-based Bayesian optimization.](http://arxiv.org/abs/2310.02633) | 该论文提出了两种算法(MAABO-MT和GS-MRM)，分别在所有可能的树中高性能估计构建树，仅提取可靠且不相似的规则。 |
| [^46] | [On Quantified Observability Analysis in Multiagent Systems.](http://arxiv.org/abs/2310.02614) | 本文针对多Agent系统提出了一种可量化的可观察性分析方法。通过引入不透明度概念和时间逻辑oPATL，我们可以定量分析系统行为对观察者的信息透明度，从而为操作员辅助决策提供帮助。 |
| [^47] | [Multi-Agent Reinforcement Learning for Power Grid Topology Optimization.](http://arxiv.org/abs/2310.02605) | 本文提出了一种用于电网拓扑优化的分层多智能体强化学习（MARL）框架，有效处理随着网络增长而扩大的大型行动空间。实验表明，该框架在性能上与单一智能体强化学习方法相当，并比较了不同的RL算法和不同的高阶智能体策略。 |
| [^48] | [MagicDrive: Street View Generation with Diverse 3D Geometry Control.](http://arxiv.org/abs/2310.02601) | MagicDrive是一个新颖的街景生成框架，通过提供多样化的三维几何控制，包括相机姿态、道路地图和三维边界框，以及文本描述，实现了高保真度的街景合成，并捕捉了细致的三维几何信息。 |
| [^49] | [A ModelOps-based Framework for Intelligent Medical Knowledge Extraction.](http://arxiv.org/abs/2310.02593) | 提出了一个基于ModelOps的智能医疗知识提取框架，通过低代码系统实现模型选择、训练、评估和优化，提供了便利给研究人员开发和部署模型的方式。 |
| [^50] | [On the Stability of Expressive Positional Encodings for Graph Neural Networks.](http://arxiv.org/abs/2310.02579) | 本研究针对图神经网络中使用拉普拉斯特征向量作为位置编码面临的非唯一性和不稳定性问题，提出了稳定且表达丰富的位置编码方法（SPE），该方法通过利用特征值对特征空间进行"软分割"，在未见过的图结构上表现出良好的泛化能力。 |
| [^51] | [Stand for Something or Fall for Everything: Predict Misinformation Spread with Stance-Aware Graph Neural Networks.](http://arxiv.org/abs/2310.02568) | 使用立场感知的图神经网络（stance-aware GNN）预测谣言传播。与没有用户立场的GNN相比，该模型在真实数据集上的表现优于32.65%的基准模型。注意权重表明用户的反对立场对邻居行为的影响更大，可以作为社会纠正措施阻止谣言传播。 |
| [^52] | [Improving Automatic VQA Evaluation Using Large Language Models.](http://arxiv.org/abs/2310.02567) | 提出使用大型语言模型改进自动视觉问答（VQA）评估的方法，将VQA评估格式化为回答评分任务，通过指令调整大型语言模型在准确度上评分候选答案，证明该方法与人类判断相关性优于现有度量方法。 |
| [^53] | [Improving Drumming Robot Via Attention Transformer Network.](http://arxiv.org/abs/2310.02565) | 本文提出了一种使用注意力转换网络的改进鼓机器人，可以自动完成音乐转录，并帮助机器人提升鼓分类性能。 |
| [^54] | [zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning.](http://arxiv.org/abs/2310.02554) | zkFL是一种基于零知识证明的联邦学习梯度聚合方法，通过提供每轮的证明来解决协调者恶意行为的问题。 |
| [^55] | [Auto-FP: An Experimental Study of Automated Feature Preprocessing for Tabular Data.](http://arxiv.org/abs/2310.02540) | 本文研究了如何自动化表格数据的特征预处理（Auto-FP），将其建模为超参数优化或神经网络架构搜索问题，并扩展了各种算法来解决Auto-FP问题。 |
| [^56] | [MIDDAG: Where Does Our News Go? Investigating Information Diffusion via Community-Level Information Pathways.](http://arxiv.org/abs/2310.02529) | MIDDAG是一个交互式系统，通过可视化社交媒体上由COVID-19相关新闻触发的信息传播路径，提供全面的洞察力，并能构建用户社区和预测信息传播，从而追踪和理解信息的传播方式。 |
| [^57] | [CITING: Large Language Models Create Curriculum for Instruction Tuning.](http://arxiv.org/abs/2310.02527) | 本文提出了一种利用大型语言模型作为指导教师来训练学生语言模型的方法，通过设计课程来进行指导调优，称为CITING。该方法通过教师模型制定评分标准，学生模型通过遵循评分标准和自我纠正进行学习。该方法可以解决手工制作指导数据集和人工对齐的瓶颈问题。 |
| [^58] | [Federated Conditional Stochastic Optimization.](http://arxiv.org/abs/2310.02524) | 本文提出了一种新的联邦条件随机优化算法(FCSG)，针对联邦学习中的非凸条件随机优化问题，通过设计加速算法(Acc-FCSG-M)来实现最佳的样本和通信复杂度。 |
| [^59] | [MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data Augmentation.](http://arxiv.org/abs/2310.02520) | 本文介绍了一种名为MedDiffusion的新型、端到端的扩散式风险预测模型，通过基于扩散的数据增强，提升了健康风险预测的效果。 |
| [^60] | [Proactive Human-Robot Interaction using Visuo-Lingual Transformers.](http://arxiv.org/abs/2310.02506) | 本文提出了一种使用视觉-语言转换器进行主动的人机交互的方法，通过提取潜在的视觉-语言线索来推断上下文，识别和主动预测用户意图要实现的目标。 |
| [^61] | [Improved Inference of Human Intent by Combining Plan Recognition and Language Feedback.](http://arxiv.org/abs/2310.02462) | 该论文提出了一种结合计划识别和语言反馈的方法，通过对话实现对人类意图的改进推理，使机器人能够有效地与人类进行交互，并纠正用户计划的偏离和次优行动。 |
| [^62] | [Learning Optimal Advantage from Preferences and Mistaking it for Reward.](http://arxiv.org/abs/2310.02456) | 本文研究了从人类偏好中学习奖励函数的算法，并发现实际上学到的是最佳优势函数而不是奖励函数。这种错误的使用方式虽然不特别有害，但与正确的贪婪最大化最佳优势函数相比仍不够理想。 |
| [^63] | [Low-Resource Languages Jailbreak GPT-4.](http://arxiv.org/abs/2310.02446) | 通过翻译不安全的英文输入成低资源语言，我们成功绕过了GPT-4的安全机制，并展示了这种跨语言漏洞。这一方法在实验中取得了与甚至超过了最先进的越狱攻击的效果，揭示了低资源语言在AI安全性中的薄弱环节。 |
| [^64] | [Learning Diverse Skills for Local Navigation under Multi-constraint Optimality.](http://arxiv.org/abs/2310.02440) | 本研究提出了一种约束优化视角的方法，克服了在数据驱动控制中提取多样性行为的挑战。通过对数值函数进行约束，我们能够获得多样的策略。通过引入吸引-排斥奖励项，我们可以进一步控制多样性水平。实验证明了我们方法在局部导航任务上的有效性，并且在真实机器人上展现出多样的灵敏行为。 |
| [^65] | [Multi-Agent Reinforcement Learning Based on Representational Communication for Large-Scale Traffic Signal Control.](http://arxiv.org/abs/2310.02435) | 本研究提出了一种基于通信的多智能体强化学习框架，用于大规模交通信号控制。该框架允许智能体学习何时和如何进行通信，从而实现良好的协调和性能。 |
| [^66] | [Episodic Memory Theory for the Mechanistic Interpretation of Recurrent Neural Networks.](http://arxiv.org/abs/2310.02430) | 提出了片段记忆理论(EMT)，将反复神经网络(RNN)概念化为通用序列片段记忆模型的离散时间类比，并且通过实验证实了EMT的有效性。通过引入新的算法任务，发现受训练的RNN始终会收敛到变量绑定电路，揭示了RNN动力学的普遍性，并且设计了一个算法来揭示变量的时间存储和组合中起重要作用的隐藏神经元。 |
| [^67] | [AXNav: Replaying Accessibility Tests from Natural Language.](http://arxiv.org/abs/2310.02424) | 这篇论文研究了一种从自然语言中重放无障碍测试的系统，该系统利用大型语言模型和基于像素的用户界面理解模型执行测试并生成可导航的视频。通过这种方式，开发人员和质量保证测试人员能够更高效地测试无障碍功能。 |
| [^68] | [OneAdapt: Fast Adaptation for Deep Learning Applications via Backpropagation.](http://arxiv.org/abs/2310.02422) | OneAdapt通过梯度上升策略来实现快速自适应，满足了深度学习应用在配置参数方面的三个要求。 |
| [^69] | [Can a student Large Language Model perform as well as it's teacher?.](http://arxiv.org/abs/2310.02421) | 这篇论文总结了知识蒸馏技术，并强调了其关键原理和成功要素，以及在资源受限环境中的部署挑战。同时，论文还指出知识蒸馏有潜力成为关键的技术转折点。 |
| [^70] | [Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models.](http://arxiv.org/abs/2310.02409) | Nugget 2D是一种用于仅解码器语言模型的动态上下文压缩方法，可以在保留任务能力的同时大幅减少解码过程所需的时间和空间开销。 |
| [^71] | [PCGPT: Procedural Content Generation via Transformers.](http://arxiv.org/abs/2310.02405) | 本文介绍了PCGPT框架，它利用离线强化学习和Transformer网络进行程序化内容生成。该框架通过解决传统PCG方法的挑战，生成了更复杂和多样化的游戏内容，并且在更少的步骤中实现了这些结果。 |
| [^72] | [SE(3)-Stochastic Flow Matching for Protein Backbone Generation.](http://arxiv.org/abs/2310.02391) | 通过SE(3)-Stochastic Flow Matching，我们提出了一系列新型生成模型FoldFlow，可以准确建模蛋白质主链。这些模型通过无需模拟训练和Riemannian最优传输的结合，具有更好的稳定性和建模能力。 |
| [^73] | [On Physical Origins of Learning.](http://arxiv.org/abs/2310.02375) | 该论文探讨了学习的物理起源，并提出学习可能具有非生物和非进化起源的可能性。作者发现能够在简单的物理模型中观察、解释和准确重现学习的关键特性。 |
| [^74] | [ProtoNER: Few shot Incremental Learning for Named Entity Recognition using Prototypical Networks.](http://arxiv.org/abs/2310.02372) | ProtoNER是一种基于原型网络的端到端KVP提取模型，可以在现有模型中添加新类别，而只需最少数量的新注释训练样本。 |
| [^75] | [Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning.](http://arxiv.org/abs/2310.02360) | 本论文提出了一种用于字典型强化学习的优先级软Q分解算法（PSQD），能够在连续状态-动作空间中学习和适应具有字典型优先级的子任务解决方案，实现了先前学习的子任务解决方案的零-shot组成和适应。 |
| [^76] | [On the definition of toxicity in NLP.](http://arxiv.org/abs/2310.02357) | 这项研究探讨了毒性的定义模糊性问题，并提出了一种基于定量压力的毒性定义来弥补现有定义的缺点。 |
| [^77] | [Reasoning about Intuitionistic Computation Tree Logic.](http://arxiv.org/abs/2310.02355) | 本文定义了直觉型计算树逻辑的版本，并研究了其在形式验证方面的应用潜力。结果表明，一些CTL的不动点公理在该直觉型版本中并不成立。 |
| [^78] | [Rollout Heuristics for Online Stochastic Contingent Planning.](http://arxiv.org/abs/2310.02345) | 本文将POMDP建模为随机依赖规划问题，并提出了两个领域无关的启发式方法，以解决POMCP高度依赖展开策略的问题。 |
| [^79] | [Autonomous Systems' Safety Cases for use in UK Nuclear Environments.](http://arxiv.org/abs/2310.02344) | 本研究描述了在英国核环境中开发自主机器人部署的安全案例过程，并提出了一个假设机器人的安全案例。这为进一步讨论核站点许可持有人、核安全办公室、工业界和学术界之间的合作奠定了基础。 |
| [^80] | [Learning Interpretable Deep Disentangled Neural Networks for Hyperspectral Unmixing.](http://arxiv.org/abs/2310.02340) | 本文提出了一个新的可解释深度学习方法，用于解决高光谱解混问题，考虑了非线性和端元可变性，并采用概率变分深度学习框架和解钢琴学习的思想进行建模和训练。 |
| [^81] | [Approximately Equivariant Quantum Neural Network for $p4m$ Group Symmetries in Images.](http://arxiv.org/abs/2310.02323) | 本文提出了几乎等变量子神经网络（EquivQCNNs），针对图像中的平面$p4m$对称性进行图像分类。这种方法在保持优化性能的同时，通过将先验知识纳入模型中，提升了训练和泛化能力。 |
| [^82] | [Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation.](http://arxiv.org/abs/2310.02304) | 本文提出了一种自学优化器（STOP），通过递归自我改进的代码生成，使用融合了语言模型的脚手架程序来改进自身，从而生成性能更好的程序。 |
| [^83] | [Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems.](http://arxiv.org/abs/2310.02299) | 本文介绍了一种用于建模3D物理系统的松弛八面体群卷积技术，它可以在保持数据一致的最高等变性水平的同时，发现物理系统中微妙的对称性破缺因素。 |
| [^84] | [Prompting Audios Using Acoustic Properties For Emotion Representation.](http://arxiv.org/abs/2310.02298) | 本研究提出使用声学特性生成音频提示，并通过训练模型来更好地学习情感表达。实验结果表明，声学提示显著提高了模型在情感音频检索和语音情感识别中的性能。 |
| [^85] | [A Comparison of Mesh-Free Differentiable Programming and Data-Driven Strategies for Optimal Control under PDE Constraints.](http://arxiv.org/abs/2310.02286) | 本研究对直接-伴随循环、物理感知神经网络和可微分编程进行了比较，发现在偏微分方程约束下的最优控制中，可微分编程是最有效的方法，并提供了条件下的使用指南。 |
| [^86] | [PASTA: PArallel Spatio-Temporal Attention with spatial auto-correlation gating for fine-grained crowd flow prediction.](http://arxiv.org/abs/2310.02284) | 本文提出了一种名为PASTA的神经网络模型，通过细粒度地图中的历史人流的时空模式来预测未来全市范围内的人群流。这种方法包括空间自相关门控、多尺度残差块和时序注意力门控模块，能够有效捕捉细粒度地图中的不规则时空模式。 |
| [^87] | [SWMLP: Shared Weight Multilayer Perceptron for Car Trajectory Speed Prediction using Road Topographical Features.](http://arxiv.org/abs/2310.02282) | 本论文提出了一种独立于大量历史速度数据的速度预测方法，通过使用车辆轨迹的道路地形特征来拟合共享权重多层感知机学习模型，取得了显著的定性和定量改进，同时为交通分析的新方法设计提供了新的视角。 |
| [^88] | [End-to-End Continuous Speech Emotion Recognition in Real-life Customer Service Call Center Conversations.](http://arxiv.org/abs/2310.02281) | 本研究介绍了一个用于实时客服呼叫中心对话中连续语音情感识别的大规模数据集CusEmo，采用维度情感注释方法捕捉情感的微妙、复杂和连续性，并解决了应用于数据集时的挑战。 |
| [^89] | [Expert enhanced dynamic time warping based anomaly detection.](http://arxiv.org/abs/2310.02280) | 本文提出了一种名为E-DTWA的新型异常检测方法，基于动态时间规整（DTW）并加入了人机交互概念相关的额外改进。该方法具有高效的检测能力，能够在强烈考虑到专家的检测反馈的基础上灵活地进行重新训练，同时保持低计算和空间复杂度。 |
| [^90] | [Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion.](http://arxiv.org/abs/2310.02279) | 提出了一种一致性轨迹模型（CTM），它可以加速扩散模型的采样，同时通过对抗训练和去噪得分匹配损失的组合来提高性能，并实现了最先进的采样质量。 |
| [^91] | [Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity.](http://arxiv.org/abs/2310.02277) | 本文研究通过稀疏性分析LLM预训练权重的任务中心角度，挑战了传统对于权重中冗余性的观点，并提出了"垃圾DNA假设"。 |
| [^92] | [Who's Harry Potter? Approximate Unlearning in LLMs.](http://arxiv.org/abs/2310.02238) | 本文介绍了一种新的技术，用于从LLM中遗忘特定的训练数据，而无需重新训练模型。通过在Llama2-7b模型上的实验，我们证明了在短时间的微调中，我们可以有效地擦除模型关于哈利·波特相关内容的能力，同时保持其在其他常见基准测试上的性能几乎不变。 |
| [^93] | [CoNO: Complex Neural Operator for Continuous Dynamical Systems.](http://arxiv.org/abs/2310.02094) | 本文介绍了一种复杂神经算子（CoNO），用于解决连续动力学系统中的偏微分方程。该算子通过复分数傅里叶变换来捕获丰富的表示，并通过复值神经网络来提高表示能力、稳健性和泛化性能。 |
| [^94] | [OceanGPT: A Large Language Model for Ocean Science Tasks.](http://arxiv.org/abs/2310.02031) | OceanGPT是首个专为海洋科学任务设计的大型语言模型，通过DoInstruct框架实现自动获取海洋领域指导数据。这一模型的引入填补了海洋科学领域中对LLM的需求缺口，并为海洋科学研究提供了新的工具和方法。 |
| [^95] | [FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations.](http://arxiv.org/abs/2310.01892) | 本文介绍了一种简单的过滤器增强方法来改进无监督节点表示学习的性能，通过捕捉不同特征频谱部分，我们展示了显著的改进，并减少了计算负载。同时，我们通过使用简单的随机 Fourier 特征投影来解决高维表示的计算问题，并在基准数据集上取得了良好的性能。 |
| [^96] | [LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment.](http://arxiv.org/abs/2310.01852) | LanguageBind提出了将语言作为不同模态之间纽带的方法，通过冻结视频-语言预训练获取的语言编码器，并使用对比学习训练其他模态的编码器，实现了多模态的语义对齐。此外，作者还提出了VIDAL-10M数据集来支持该方法。 |
| [^97] | [Blending Imitation and Reinforcement Learning for Robust Policy Improvement.](http://arxiv.org/abs/2310.01737) | 本文提出了一种融合模仿学习和强化学习的方法，根据在线评估结果交替使用二者，以提高样本效率和学习效果。 |
| [^98] | [SmartPlay : A Benchmark for LLMs as Intelligent Agents.](http://arxiv.org/abs/2310.01557) | SmartPlay是一个用于评估LLMs作为智能Agent能力的基准，包括6个具有不同挑战的游戏，并测试了智能LLM Agent的多种关键能力。这不仅是一个评估LLM Agent整体性能的严格测试场地，还可以分析每个能力的表现。 |
| [^99] | [LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples.](http://arxiv.org/abs/2310.01469) | LLM似乎具有丰富的知识和适应多种任务的能力，但我们不能完全信任它们的回答，因为它们会出现幻觉，即捏造不存在的事实以欺骗用户。本文证明了由随机标记组成的无意义提示也能引起LLM产生幻觉回应，并提出了一种对抗方式的自动幻觉触发方法作为幻觉攻击，同时提出了一种简单而有效的防御策略。 |
| [^100] | [The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs.](http://arxiv.org/abs/2310.01468) | 本文提供了一个评估框架，通过向法官提出一系列查询来评估LLMs的对话推理和规划能力。我们发现不同的LLMs在这个任务上表现出显著差异。 |
| [^101] | [Towards Fixing Clever-Hans Predictors with Counterfactual Knowledge Distillation.](http://arxiv.org/abs/2310.01011) | 这项研究提出了一种称为反事实知识蒸馏的新技术，通过人类专家的反馈来检测和消除深度学习模型对虚假特征的依赖，特别适用于受监管或安全关键领域。 |
| [^102] | [Are Graph Neural Networks Optimal Approximation Algorithms?.](http://arxiv.org/abs/2310.00526) | 本文设计了图神经网络架构OptGNN，利用半定规划工具获得大类组合优化问题的最优近似算法。通过实证结果表明在各种数据集上超过了神经网络基线算法和传统算法，同时利用OptGNN的能力设计了一个产生优化的对偶证书的算法。 |
| [^103] | [ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving.](http://arxiv.org/abs/2309.17452) | ToRA是一种集成工具的数学问题求解推理代理，通过结合语言的分析能力和工具的计算效率，能够显著提高数学推理的性能，在多个数学推理数据集上取得了13%-19%的平均绝对改进率，并在竞赛级数据集MATH上达到了44.6%的性能。 |
| [^104] | [A Foundation Model for General Moving Object Segmentation in Medical Images.](http://arxiv.org/abs/2309.17264) | 本文提出了一种用于医学图像中移动目标分割的基础模型iMOS，通过对序列中只有少量图像进行注释，即可实现高精度的分割效果 |
| [^105] | [Probabilistic Weight Fixing: Large-scale training of neural network weight uncertainties for quantization.](http://arxiv.org/abs/2309.13575) | 本文提出了一种基于贝叶斯神经网络和变分松弛的概率框架，用于通过将权重值限制在一组有限值上来减少推理过程中的能量消耗。通过利用权重值的概率分布，提高了噪声鲁棒性和可压缩性。迭代聚类过程展示了超越现有方法的优势。 |
| [^106] | [Rethinking superpixel segmentation from biologically inspired mechanisms.](http://arxiv.org/abs/2309.13438) | 该论文从神经结构和视觉机制的启示出发，提出了一种生物网络架构用于超像素分割，其中包括增强筛选模块（ESM）和边界感知标签（BAL），旨在产生严格遵循物体边界且传达丰富视觉符号的超像素。 |
| [^107] | [Time-Series Forecasting: Unleashing Long-Term Dependencies with Fractionally Differenced Data.](http://arxiv.org/abs/2309.13409) | 本研究提出了一种利用分数差分来捕捉时间序列数据中短期和长期依赖关系的预测策略。通过将FD应用于金融数据并结合情感分析，实证结果证明FD在二元分类中的性能优于整数差分方法。 |
| [^108] | [Investigating the Catastrophic Forgetting in Multimodal Large Language Models.](http://arxiv.org/abs/2309.10313) | 本论文针对多模态大规模语言模型中的灾难性遗忘问题进行研究，引入了EMT方法来评估灾难性遗忘，并发现在标准图像分类任务上，几乎所有评估的模型都无法保持与视觉编码器相同的性能水平。研究结果表明，早期微调阶段对性能至关重要。 |
| [^109] | [GPTFUZZER : Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts.](http://arxiv.org/abs/2309.10253) | GPTFUZZER是一种黑盒越狱模糊测试框架，自动生成用于红队测试大型语言模型的越狱模板。这种自动化方法避免了手工工程，并通过种子选择策略提高了效率。 |
| [^110] | [When Geoscience Meets Foundation Models: Towards General Geoscience Artificial Intelligence System.](http://arxiv.org/abs/2309.06799) | 地球科学基础模型通过整合大量跨学科数据来模拟和理解地球系统动态，具有广阔的应用前景和创新潜力，但仍面临验证和核实、规模性、可解释性、知识表示和社会偏差等挑战。 |
| [^111] | [Addressing Feature Imbalance in Sound Source Separation.](http://arxiv.org/abs/2309.05287) | 本论文提出了一种名为FEABASE的方法来解决源分离中的特征偏好问题，通过学习被忽视特征的隐藏信息，实现对数据的高效利用。 |
| [^112] | [Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility.](http://arxiv.org/abs/2309.04296) | 本研究提出了一种利用人类移动数据和持续学习技术的方法来解决COVID-19期间非分布期间的电力负荷预测问题，通过保留过去的见解并整合新的数据，提高了模型的准确性和鲁棒性。 |
| [^113] | [Trustworthy and Synergistic Artificial Intelligence for Software Engineering: Vision and Roadmaps.](http://arxiv.org/abs/2309.04142) | 本文提出了可信和协同的软件工程人工智能的愿景和路线图，旨在提高开发人员的生产力和软件质量。实现这个愿景可能导致软件工程领域的一个重要转变，即软件工程2.0. |
| [^114] | [Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment.](http://arxiv.org/abs/2308.16175) | 本文引入了BSDetector，一种用于检测预训练大型语言模型生成的错误和推测性回答的方法。该方法通过估计置信度量化了回答的不确定性，并在闭合型和开放型问答基准实验中表现出更准确的识别能力。 |
| [^115] | [Instruction Tuning for Large Language Models: A Survey.](http://arxiv.org/abs/2308.10792) | 本文调查了指令调优这一关键技术在增强大型语言模型能力和可控性方面的研究工作，包括方法、数据集构建、模型训练和应用，以及对结果影响的分析。同时回顾了可能的问题和批评，并指出了目前的不足。 |
| [^116] | [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework.](http://arxiv.org/abs/2308.08155) | AutoGen是一种新的框架，通过多个可以互相对话的代理，实现了下一代LLM应用。它利用人类的理解和智能，优雅地处理不完美的生成和推理能力，并通过自动化代理对话简化了复杂的工作流程。 |
| [^117] | [PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant Poker Game Simulations.](http://arxiv.org/abs/2308.07327) | PokerKit是一个全面的Python库，用于细粒度多变体扑克游戏模拟，提供广泛的扑克变体支持和灵活的游戏状态控制，对扑克AI开发、工具创建和在线扑克赌场实现等领域具有重要贡献。 |
| [^118] | [Robustified ANNs Reveal Wormholes Between Human Category Percepts.](http://arxiv.org/abs/2308.06887) | Robustified ANNs通过发现低范数图像扰动，揭示了在“人类假定稳定”范围内人类类别知觉之间的巨大干扰。 |
| [^119] | [FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets.](http://arxiv.org/abs/2307.10928) | FLASK是一种基于对齐技能集的细粒度语言模型评估协议，通过将粗级评分分解为每个指令的技能集级评分，实现了对模型性能的全面视角和提高评估的可靠性。 |
| [^120] | [Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill-Learning.](http://arxiv.org/abs/2307.02728) | 分层授权提出了一种可以计算授权的新框架，通过引入变分下界和分层架构，实现了在短期和长期时间尺度上的授权计算，并在模拟机器人任务中得到了验证。 |
| [^121] | [Named Entity Inclusion in Abstractive Text Summarization.](http://arxiv.org/abs/2307.02570) | 该论文提出了一种解决抽象文本摘要中命名实体遗漏问题的方法，通过使用定制的预训练目标和模型训练策略，改善了命名实体的包含情况，提高了摘要的准确性和召回率。 |
| [^122] | [MedCPT: Contrastive Pre-trained Transformers with Large-scale PubMed Search Logs for Zero-shot Biomedical Information Retrieval.](http://arxiv.org/abs/2307.00589) | MedCPT是一种用于生物医学领域零样本语义信息检索的对比预训练转换器模型。通过使用大规模PubMed搜索日志进行训练，MedCPT在六个生物医学信息检索任务中创造了新的最佳性能，超过了其他基线模型，同时还能生成更好的生物医学文章和句子。 |
| [^123] | [REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction.](http://arxiv.org/abs/2306.15724) | 提出了REFLECT框架，可以将机器人多感官数据转化为分层总结，并使用大型语言模型进行失败解释。该框架能够生成有益的失败解释，帮助机器人完成任务。 |
| [^124] | [GPT-4 Reticular Chemist for MOF Discovery.](http://arxiv.org/abs/2306.14915) | GPT-4网状化学家是一个集成了AI模型GPT-4的框架，通过迭代的人工智能交互，能够发现金属有机框架系列。 |
| [^125] | [DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability.](http://arxiv.org/abs/2306.13196) | 本文提出了一种使用扩散模型作为采样器的任务和动作规划方法，在部分可观测下能够实现长周期受约束的操作计划。 |
| [^126] | [Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization.](http://arxiv.org/abs/2306.09222) | 我们通过分布健壮优化和重要性加权的梯度下降技术提升了深度神经网络的性能，并在各种任务上取得了优越的结果。 |
| [^127] | [Fast Diffusion Model.](http://arxiv.org/abs/2306.06991) | 本文提出了一种快速扩散模型（FDM），通过将动量集成到扩散过程中，显著加速了扩散模型（DMs）的训练和采样过程。 |
| [^128] | [Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations.](http://arxiv.org/abs/2306.05880) | 该论文提出了基于INR的时间序列连续建模方法，解决了处理缺失数据、不规则采样和多传感器不对准观测等重复建模问题，并在预测和插值任务中取得了最新的性能表现，具有很好的泛化能力。 |
| [^129] | [Deductive Verification of Chain-of-Thought Reasoning.](http://arxiv.org/abs/2306.03872) | 本文旨在通过应用演绎验证技术，使语言模型能够进行明确而严谨的演绎推理，以确保其推理过程的可信度。 |
| [^130] | [Expanding Explainability Horizons: A Unified Concept-Based System for Local, Global, and Misclassification Explanations.](http://arxiv.org/abs/2306.03531) | 本文提出了一种新的统一的基于概念的系统，旨在解决当前基于概念的可解释性方法不足的局部、全局和错误分类解释问题，该系统可以自动学习、评分和提取局部和全局概念。 |
| [^131] | [RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems.](http://arxiv.org/abs/2306.03091) | RepoBench是一个评估代码自动补全系统在代码库级别上性能的基准，支持Python和Java，包含三个相互关联的评估任务。它旨在填补当前基准在多文件编程场景方面的评估差距，并为不同系统的性能提供更全面的比较。 |
| [^132] | [LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization.](http://arxiv.org/abs/2306.01102) | 本文介绍了利用大语言模型和多样性优化算法相结合的 LLMatic 神经结构搜索算法。该算法在CIFAR-10数据集进行测试，仅进行2000次搜索即可产生高性能网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。 |
| [^133] | [Large-Batch, Neural Multi-Objective Bayesian Optimization.](http://arxiv.org/abs/2306.01095) | 本文提出了一种针对数据密集型问题和多目标优化设置的贝叶斯优化框架，该方法利用了贝叶斯神经网络代理建模和可扩展、具有不确定性的收购策略，能够在最少迭代次数的情况下高效地进行优化。 |
| [^134] | [Stochastic Gradient Langevin Dynamics Based on Quantization with Increasing Resolution.](http://arxiv.org/abs/2305.18864) | 本文提出了一种基于递增分辨率的量化随机梯度 langevin 动力学方法，通过利用 langevin 随机微分方程动力学，实现了具有可控噪声且具有相同分布的优化过程，无需添加噪声或调整小批量的大小。实验结果证明了该方法在不同数据集上对卷积神经网络模型和 ResNet-50 架构的有效性。 |
| [^135] | [DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text.](http://arxiv.org/abs/2305.17359) | DNA-GPT是一种无需训练的检测策略，通过Divergent N-Gram分析来发现机器生成文本与人类写作文本之间的显著差异。 |
| [^136] | [Property-Guided Generative Modelling for Robust Model-Based Design with Imbalanced Data.](http://arxiv.org/abs/2305.13650) | 本文提出了一种属性引导的变分自编码器（PGVAE），通过属性值明确结构化潜在空间，使得MBO可以在不平衡数据上稳健地寻找具有改进属性的序列。 |
| [^137] | [A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model.](http://arxiv.org/abs/2305.11244) | 本文介绍了一种利用预训练通用语音模型进行阿拉伯方言识别的参数高效学习方法，通过残差适配器和模型重编程，设计了一个基于记号的标签映射，并在ADI-17数据集上实现了最高精度，同时使用PEL方法进一步减少了训练成本。 |
| [^138] | [PDP: Parameter-free Differentiable Pruning is All You Need.](http://arxiv.org/abs/2305.11203) | PDP提出了一种无需参数的可微剪枝方案，具有最先进的模型大小、准确性和训练成本，适用于各种视觉和自然语言任务。 |
| [^139] | [Personalize Segment Anything Model with One Shot.](http://arxiv.org/abs/2305.03048) | 本文提出了一种无需训练的SAM个性化方法PerSAM，只需要一张带有参考掩模的单张图像即可定位和分割目标概念，还提出了高效的一次性微调变体PerSAM-F，旨在解决掩模不确定性问题。 |
| [^140] | [Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks.](http://arxiv.org/abs/2304.05727) | 本文提出了一种新方法，Explanation-Guided Exposure Minimization (EGEM)，该方法预防性地修剪了ML模型中未受到积极解释反馈的变化，从而大大减少了对隐藏Clever Hans策略的依赖，并实现了更高的性能。 |
| [^141] | [A Benchmark Generative Probabilistic Model for Weak Supervised Learning.](http://arxiv.org/abs/2303.17841) | 本文提出一种基准生成性概率模型，在启发式标注的原始数据集上训练，生成伪标签作为一种准确、快速、经济的弱监督学习方法，在图像分类和自然语言处理中达到了最先进的表现。 |
| [^142] | [Variantional autoencoder with decremental information bottleneck for disentanglement.](http://arxiv.org/abs/2303.12959) | 本论文提出了一种逐步减少信息瓶颈的变分自编码器方法，使用去纠缠不变变换来平衡去纠缠和重构保真度，避免信息扩散问题。 |
| [^143] | [Text-Visual Prompting for Efficient 2D Temporal Video Grounding.](http://arxiv.org/abs/2303.04995) | 本文提出了一种新颖的文本-视觉提示（TVP）框架来解决时间视频定位问题，该框架有效地共同训练视觉编码器和语言编码器，且使用只有低复杂度稀疏二维视觉特征来提高跨模态特征融合的性能，并提出了一种时态对话排名（TDR）训练策略用于监督TVP的学习，实验证明该框架有效且高效。 |
| [^144] | [Regularised neural networks mimic human insight.](http://arxiv.org/abs/2302.11351) | 本文研究了正则化神经网络是否具有类似于人类洞察力的行为。研究发现，正则化神经网络在学习动态和行为特征上密切模仿了人类的洞察力，表现出洞察力的延迟、突然性和选择性发生。 |
| [^145] | [A novel approach to generate datasets with XAI ground truth to evaluate image models.](http://arxiv.org/abs/2302.05624) | 该论文介绍了一种新方法来生成具有XAI基准的数据集，用于评估图像模型。通过与真实模型解释进行比较，实验证实了该方法的可靠性。 |
| [^146] | [RayNet: A Simulation Platform for Developing Reinforcement Learning-Driven Network Protocols.](http://arxiv.org/abs/2302.04519) | RayNet是一个可扩展且适应性强的仿真平台，用于开发基于强化学习的网络协议，帮助研究者有序地进行协议的开发，而无需关注具体实现细节。 |
| [^147] | [A Survey on Deep Learning based Time Series Analysis with Frequency Transformation.](http://arxiv.org/abs/2302.02173) | 近期，频率变换（FT）在深度学习时间序列分析中得到广泛应用，显著提高了准确性和效率。本文系统回顾和总结了基于FT的深度学习时间序列模型的研究进展，并探讨了其优势、限制以及主要方法。 |
| [^148] | [Double Permutation Equivariance for Knowledge Graph Completion.](http://arxiv.org/abs/2302.01313) | 本研究提出了双排列等变性的KG表示方法，可以使神经网络在KG中执行复杂的逻辑推理任务，并在多个归纳KG完成任务中实现了最先进的Hits@10测试准确率。双排列等变性在KG中开辟了新的研究方向。 |
| [^149] | [Explaining $\mathcal{ELH}$ Concept Descriptions through Counterfactual Reasoning.](http://arxiv.org/abs/2301.05109) | 本研究提出了一种通过反事实推理来解释概念描述的方法，以提供简洁且易于理解的解释，便于非专家理解和采取行动。 |
| [^150] | [PuzzleFusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving.](http://arxiv.org/abs/2211.13785) | 本文提出了一个名为"PuzzleFusion"的神经架构，基于扩散模型，用于解决空间拼图和房间布局任务。在实验中，他们发现简单使用扩散模型可以有效地解决这些具有挑战性的空间拼图任务。 |
| [^151] | [PersA-FL: Personalized Asynchronous Federated Learning.](http://arxiv.org/abs/2210.01176) | 本论文研究了异步更新下的个性化联邦学习问题，并提出了一种改进的个性化方法，通过移除同步通信假设和去除梯度范数有界性假设来提高可伸缩性。 |
| [^152] | [Co-Located Human-Human Interaction Analysis using Nonverbal Cues: A Survey.](http://arxiv.org/abs/2207.10574) | 使用非语言线索进行共同人际互动分析的研究调查了从2010年以来的计算研究，并总结了最常用的非语言线索，以及有关互动分析的未来研究方向。 |
| [^153] | [Floorplan Restoration by Structure Hallucinating Transformer Cascades.](http://arxiv.org/abs/2206.00645) | 该文介绍了一个新的极端楼层平面图重建任务和一个神经网络框架用于解决该任务，通过Transformer解码器级联的方式来幻化不可见的房间和门以重建整个楼层平面图，并在701个房屋的基准测试中表现出较好的效果。 |
| [^154] | [NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks.](http://arxiv.org/abs/2110.14053) | NeuroBack提出了一种使用图神经网络改进CDCL SAT求解的方法，通过预测出现在大多数满足赋值中的变量的阶段，使得求解更加有效，并且消除了对GPU资源的依赖。 |
| [^155] | [SR-HetGNN:Session-based Recommendation with Heterogeneous Graph Neural Network.](http://arxiv.org/abs/2108.05641) | 本文提出了一种基于异构图神经网络的会话推荐方法SR-HetGNN，通过学习会话嵌入并捕捉匿名用户的特定偏好，以改进会话推荐系统的效果和准确性。 |
| [^156] | [AKE-GNN: Effective Graph Learning with Adaptive Knowledge Exchange.](http://arxiv.org/abs/2106.05455) | AKE-GNN是一种新型的图神经网络学习框架，通过自适应知识交换策略在多个图视图之间交换通道，以实现有效的图学习。 |
| [^157] | [Wise-SrNet: A Novel Architecture for Enhancing Image Classification by Learning Spatial Resolution of Feature Maps.](http://arxiv.org/abs/2104.12294) | 本文提出了一种名为Wise-SrNet的新型架构，用于增强图像分类任务。该架构通过学习特征图的空间分辨率，解决了连接特征图和分类层之间的挑战。实验证明，该方法在不增加计算成本的情况下，有效提高了学习效率。 |

# 详细

[^1]: SemiReward: 半监督学习的通用奖励模型

    SemiReward: A General Reward Model for Semi-supervised Learning. (arXiv:2310.03013v1 [cs.LG])

    [http://arxiv.org/abs/2310.03013](http://arxiv.org/abs/2310.03013)

    SemiReward是一个通用奖励模型，通过预测奖励分数来评估和过滤高质量的伪标签，可以应用于各种半监督学习任务，并在实验中取得了显著的成果。

    

    半监督学习在自训练框架和伪标签上取得了显著进展。主要挑战是如何区分高质量的伪标签，避免确证偏见。然而，现有的伪标签选择策略限制于预定义的方案或复杂的手工制作策略，无法同时实现高质量标签、快速收敛和任务多样性。为此，我们提出了一种半监督奖励框架（SemiReward），用于预测奖励分数以评估和过滤高质量的伪标签，可以在各种任务类型和场景下与主流的半监督学习方法相结合使用。为了减少确证偏见，在两个阶段通过生成模型和子抽样策略进行在线训练。通过在三种模态的13个标准半监督学习基准上进行分类和回归任务的广泛实验验证，表明SemiReward取得了显著的成果。

    Semi-supervised learning (SSL) has witnessed great progress with various improvements in the self-training framework with pseudo labeling. The main challenge is how to distinguish high-quality pseudo labels against the confirmation bias. However, existing pseudo-label selection strategies are limited to pre-defined schemes or complex hand-crafted policies specially designed for classification, failing to achieve high-quality labels, fast convergence, and task versatility simultaneously. To these ends, we propose a Semi-supervised Reward framework (SemiReward) that predicts reward scores to evaluate and filter out high-quality pseudo labels, which is pluggable to mainstream SSL methods in wide task types and scenarios. To mitigate confirmation bias, SemiReward is trained online in two stages with a generator model and subsampling strategy. With classification and regression tasks on 13 standard SSL benchmarks of three modalities, extensive experiments verify that SemiReward achieves sig
    
[^2]: 软凸量化：用凸优化重新思考向量量化

    Soft Convex Quantization: Revisiting Vector Quantization with Convex Optimization. (arXiv:2310.03004v1 [cs.LG])

    [http://arxiv.org/abs/2310.03004](http://arxiv.org/abs/2310.03004)

    本文提出了软凸量化（SCQ）作为向量量化（VQ）的替代方法，通过解决量化输入的码书向量的最优凸组合问题，缓解了VQ面临的实际挑战。

    

    向量量化（VQ）是深度学习中用于提取信息性离散潜在表示的一种众所周知的技术。VQ嵌入模型在包括图像和语音生成在内的一系列应用中取得了令人印象深刻的结果。VQ作为一种参数化的K-means算法，在前向传递中使用单个码书向量将输入进行量化。尽管功能强大，但该技术面临实际挑战，包括码书崩溃、不可区分性和有损压缩。为了缓解上述问题，我们提出了软凸量化（SCQ）作为VQ的直接替代。SCQ的工作方式类似于可微凸优化（DCO）层：在前向传递中，我们求解量化输入的码书向量的最优凸组合。在反向传递中，我们通过前向解的最优性条件利用可区分性。然后，我们引入了一个可扩展的SCQ优化松弛方法，并展示其在CIFAR数据集上的有效性。

    Vector Quantization (VQ) is a well-known technique in deep learning for extracting informative discrete latent representations. VQ-embedded models have shown impressive results in a range of applications including image and speech generation. VQ operates as a parametric K-means algorithm that quantizes inputs using a single codebook vector in the forward pass. While powerful, this technique faces practical challenges including codebook collapse, non-differentiability and lossy compression. To mitigate the aforementioned issues, we propose Soft Convex Quantization (SCQ) as a direct substitute for VQ. SCQ works like a differentiable convex optimization (DCO) layer: in the forward pass, we solve for the optimal convex combination of codebook vectors that quantize the inputs. In the backward pass, we leverage differentiability through the optimality conditions of the forward solution. We then introduce a scalable relaxation of the SCQ optimization and demonstrate its efficacy on the CIFAR-
    
[^3]: ECoFLaP: 高效的粗到细的逐层剪枝方法用于视觉语言模型

    ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models. (arXiv:2310.02998v1 [cs.CV])

    [http://arxiv.org/abs/2310.02998](http://arxiv.org/abs/2310.02998)

    ECoFLaP提出了一种高效的粗到细的逐层剪枝方法，解决了大型视觉语言模型在压缩和部署时的计算和能耗问题。

    

    大型视觉语言模型（LVLMs）通过整合不同模态的丰富信息，全面理解世界，并在各种多模态下游任务上取得显著的性能提升。然而，由于其巨大的计算/能耗和碳排放，部署LVLMs往往存在问题。这些问题使得采用传统的迭代全局剪枝变得不可行，因为其需要计算整个大型模型的Hessian矩阵以进行稀疏化。相反，最近的研究提出了逐层剪枝方法，避免了全局剪枝的昂贵计算，并根据层内权重的重要性有效压缩模型。然而，这些方法常常由于缺乏全局视角而导致模型压缩不够优化。为了解决大型模型最近高效剪枝方法的这一局限性，我们提出了高效的粗到细的逐层剪枝方法（ECoFLaP）。

    Large Vision-Language Models (LVLMs) can understand the world comprehensively by integrating rich information from different modalities, achieving remarkable performance improvements on various multimodal downstream tasks. However, deploying LVLMs is often problematic due to their massive computational/energy costs and carbon consumption. Such issues make it infeasible to adopt conventional iterative global pruning, which is costly due to computing the Hessian matrix of the entire large model for sparsification. Alternatively, several studies have recently proposed layer-wise pruning approaches to avoid the expensive computation of global pruning and efficiently compress model weights according to their importance within a layer. However, these methods often suffer from suboptimal model compression due to their lack of a global perspective. To address this limitation in recent efficient pruning methods for large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP), 
    
[^4]: 多物理学预训练用于物理代理模型

    Multiple Physics Pretraining for Physical Surrogate Models. (arXiv:2310.02994v1 [cs.LG])

    [http://arxiv.org/abs/2310.02994](http://arxiv.org/abs/2310.02994)

    多物理学预训练是一种用于物理代理建模的自回归预训练方法，通过训练大型代理模型同时预测多个异构物理系统的动力学，学习在不同物理任务中广泛适用的特征。实验证明，单个MPP预训练的变换器可以在所有预训练子任务上与或超过特定任务的基准结果，无需微调，并且在下游任务中，微调MPP训练的模型相较于从头训练的模型，对新物理的预测结果更准确。

    

    我们引入了一种多物理学预训练（MPP）的方法，这是一种自回归任务不可知的预训练方法，用于物理代理建模。MPP通过训练大型代理模型同时预测多个异构物理系统的动力学，学习在不同物理任务中广泛适用的特征。为了有效学习，在这种设置中，我们引入了一种共享嵌入和归一化策略，将多个系统的字段投影到一个共享嵌入空间中。我们在一个涉及流体力学的广泛基准测试中验证了我们方法的有效性。我们表明，单个MPP预训练的变换器能够在所有预训练子任务上与或超过特定任务的基准结果，而无需微调。对于下游任务，我们证明微调MPP训练的模型相较于从头训练的模型，在多个时间步骤上对新物理的预测结果更准确。

    We introduce multiple physics pretraining (MPP), an autoregressive task-agnostic pretraining approach for physical surrogate modeling. MPP involves training large surrogate models to predict the dynamics of multiple heterogeneous physical systems simultaneously by learning features that are broadly useful across diverse physical tasks. In order to learn effectively in this setting, we introduce a shared embedding and normalization strategy that projects the fields of multiple systems into a single shared embedding space. We validate the efficacy of our approach on both pretraining and downstream tasks over a broad fluid mechanics-oriented benchmark. We show that a single MPP-pretrained transformer is able to match or outperform task-specific baselines on all pretraining sub-tasks without the need for finetuning. For downstream tasks, we demonstrate that finetuning MPP-trained models results in more accurate predictions across multiple time-steps on new physics compared to training from
    
[^5]: xVal: 大型语言模型的连续数字编码

    xVal: A Continuous Number Encoding for Large Language Models. (arXiv:2310.02989v1 [stat.ML])

    [http://arxiv.org/abs/2310.02989](http://arxiv.org/abs/2310.02989)

    xVal是一种连续数字编码方案，通过使用单个标记来表示任何实数。与现有的数字编码方案相比，xVal更加高效，并且在泛化性能上表现更好。

    

    由于数字令牌化的独特困难，大型语言模型尚未广泛用于科学数据集的分析。我们提出了xVal，一种数字编码方案，可以使用单个标记来表示任何实数。xVal通过将专用嵌入向量按数字值进行缩放来表示给定的实数。结合修改后的数字推断方法，该策略使模型在考虑作为从输入字符串的数字到输出字符串的数字的映射时成为端到端连续的。这导致了一种更适用于科学领域应用的归纳偏差。我们在许多合成和现实世界数据集上进行了实证评估。与现有的数字编码方案相比，我们发现xVal在令牌效率和泛化性能上表现更好。

    Large Language Models have not yet been broadly adapted for the analysis of scientific datasets due in part to the unique difficulties of tokenizing numbers. We propose xVal, a numerical encoding scheme that represents any real number using just a single token. xVal represents a given real number by scaling a dedicated embedding vector by the number value. Combined with a modified number-inference approach, this strategy renders the model end-to-end continuous when considered as a map from the numbers of the input string to those of the output string. This leads to an inductive bias that is generally more suitable for applications in scientific domains. We empirically evaluate our proposal on a number of synthetic and real-world datasets. Compared with existing number encoding schemes, we find that xVal is more token-efficient and demonstrates improved generalization.
    
[^6]: 使用反事实例子探究视觉-语言模型中的交叉偏见

    Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples. (arXiv:2310.02988v1 [cs.CV])

    [http://arxiv.org/abs/2310.02988](http://arxiv.org/abs/2310.02988)

    本研究利用反事实例子探究了视觉-语言模型中的交叉偏见，并通过文本到图像扩散模型生成了一组高度相似但存在交叉社会属性差异的图像-文本对。

    

    虽然近年来视觉-语言模型（VLMs）取得了显著的性能提升，但越来越多的证据表明这些模型在性别和种族等社会属性方面也存在有害的偏见。先前的研究主要集中于个别探测这种偏见属性，而忽视了与社会属性交叉相关的偏见。这可能是因为从现有数据集中收集包含各种社会属性组合的图像-文本对的完整集合非常困难。为了解决这一挑战，我们采用文本到图像扩散模型，在规模上生成反事实的例子来探测交叉社会偏见。我们的方法利用稳定扩散和交叉注意力控制，生成了一组反事实的图像-文本对，它们在描述主题（例如某个职业）方面非常相似，只在描述交叉社会属性（例如种族和性别）方面有所不同。

    While vision-language models (VLMs) have achieved remarkable performance improvements recently, there is growing evidence that these models also posses harmful biases with respect to social attributes such as gender and race. Prior studies have primarily focused on probing such bias attributes individually while ignoring biases associated with intersections between social attributes. This could be due to the difficulty of collecting an exhaustive set of image-text pairs for various combinations of social attributes from existing datasets. To address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intserctional social biases at scale. Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race & gender). W
    
[^7]: 在灾难场景中探索中断的点对点通信对完全去中心化学习的影响

    Exploring the Impact of Disrupted Peer-to-Peer Communications on Fully Decentralized Learning in Disaster Scenarios. (arXiv:2310.02986v1 [cs.LG])

    [http://arxiv.org/abs/2310.02986](http://arxiv.org/abs/2310.02986)

    在灾难场景中，完全去中心化学习可以帮助解决通信基础设施中断或不可用导致的传统集中式学习任务无法进行的问题。

    

    完全去中心化学习使得学习资源和决策能力可以分布在多个用户设备或节点上，由于其保护隐私和去中心化的特性，它正迅速变得流行起来。重要的是，这种学习过程的众包机制使得系统在一些节点受到影响或断开连接时仍然可以继续运转。在灾难场景中，通信基础设施和集中式系统可能会中断或完全不可用，这阻碍了在这些环境中进行标准的集中式学习任务的可能性。因此，完全去中心化的学习可以在这种情况下提供帮助。然而，从集中式到点对点通信的过渡引入了学习过程与节点之间的通信图拓扑的依赖关系。在灾难场景中，即使是点对点通信也容易出现突然的变化，如设备耗尽电池或断开连接。

    Fully decentralized learning enables the distribution of learning resources and decision-making capabilities across multiple user devices or nodes, and is rapidly gaining popularity due to its privacy-preserving and decentralized nature. Importantly, this crowdsourcing of the learning process allows the system to continue functioning even if some nodes are affected or disconnected. In a disaster scenario, communication infrastructure and centralized systems may be disrupted or completely unavailable, hindering the possibility of carrying out standard centralized learning tasks in these settings. Thus, fully decentralized learning can help in this case. However, transitioning from centralized to peer-to-peer communications introduces a dependency between the learning process and the topology of the communication graph among nodes. In a disaster scenario, even peer-to-peer communications are susceptible to abrupt changes, such as devices running out of battery or getting disconnected fro
    
[^8]: 缩放定律在联想记忆中的应用

    Scaling Laws for Associative Memories. (arXiv:2310.02984v1 [stat.ML])

    [http://arxiv.org/abs/2310.02984](http://arxiv.org/abs/2310.02984)

    本文研究了应用于联想记忆中的缩放定律，通过高维矩阵和嵌入的外积来模拟内层Transformer语言模型。作者推导出了与样本数量和参数大小相关的精确缩放定律，并验证了理论结果的有效性。同时，作者还通过大量实验展示了存储记忆关联的细粒度可视化。

    

    学习很可能涉及到抽象规则的发现和记忆。本文旨在研究联想记忆机制。我们的模型基于高维矩阵，由嵌入的外积组成，与Transformer语言模型的内层相关。我们推导出关于样本数量和参数规模的精确缩放定律，并讨论了不同估计器的统计效率，包括基于优化的算法。我们进行了大量的数值实验，以验证和解释理论结果，包括对存储记忆关联的细粒度可视化。

    Learning arguably involves the discovery and memorization of abstract rules. The aim of this paper is to study associative memory mechanisms. Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models. We derive precise scaling laws with respect to sample size and parameter size, and discuss the statistical efficiency of different estimators, including optimization-based algorithms. We provide extensive numerical experiments to validate and interpret theoretical results, including fine-grained visualizations of the stored memory associations.
    
[^9]: 在重尾波段的完全自适应遗憾最小化领域中的研究

    Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits. (arXiv:2310.02975v1 [cs.LG])

    [http://arxiv.org/abs/2310.02975](http://arxiv.org/abs/2310.02975)

    本文研究了在重尾波段问题中完全自适应的遗憾最小化，提出了随机自适应重尾波段问题，并证明了适应性算法相对于标准设置会有更高的遗憾。

    

    重尾分布在金融到电信等多种环境中自然而然地出现。虽然在次高斯或有界支撑奖励下的遗憾最小化已被广泛研究，但在重尾分布上的学习只在过去十年中受到关注。在随机重尾波段问题中，一个代理在假设分布有有界最大阶的有限矩的情况下学习，这些矩被常数u一致有界，对于某个ε∈(0,1]。据我们所知，文献中只提供需要这两个量作为输入的算法。在本文中，我们研究了随机自适应重尾波段问题，这是标准设置的一个变种，其中代理对ε和u均不知晓。我们表明，适应性是存在代价的，并引入对于任何自适应算法遗憾的两个下界，意味着相对于标准设置有更高的遗憾。最后，我们引入一种特定的分布假设。

    Heavy-tailed distributions naturally arise in many settings, from finance to telecommunications. While regret minimization under sub-Gaussian or bounded support rewards has been widely studied, learning on heavy-tailed distributions only gained popularity over the last decade. In the stochastic heavy-tailed bandit problem, an agent learns under the assumption that the distributions have finite moments of maximum order $1+\epsilon$ which are uniformly bounded by a constant $u$, for some $\epsilon \in (0,1]$. To the best of our knowledge, literature only provides algorithms requiring these two quantities as an input. In this paper, we study the stochastic adaptive heavy-tailed bandit, a variation of the standard setting where both $\epsilon$ and $u$ are unknown to the agent. We show that adaptivity comes at a cost, introducing two lower bounds on the regret of any adaptive algorithm, implying a higher regret w.r.t. the standard setting. Finally, we introduce a specific distributional ass
    
[^10]: 使用机器学习模型进行信用卡评分预测：一个新数据集

    Credit card score prediction using machine learning models: A new dataset. (arXiv:2310.02956v1 [cs.LG])

    [http://arxiv.org/abs/2310.02956](http://arxiv.org/abs/2310.02956)

    本研究探索了利用机器学习模型对信用卡违约进行预测的方法，并提出了一个新的信用卡评分数据集。实验结果表明，多层感知器（MLP）模型在预测性能上表现最佳。

    

    近年来，信用卡的使用量不断增加，为了最小化潜在风险，急需信用卡评估方法。本研究调查了利用机器学习模型进行信用卡违约预测系统的应用。主要目标是研究在新提出的信用卡评分数据集上表现最佳的机器学习模型。这个新数据集包括信用卡交易历史和客户档案，并使用了多种机器学习算法进行了测试，包括逻辑回归、决策树、随机森林、多层感知器（MLP）神经网络、XGBoost和LightGBM。为了准备机器学习模型的数据，我们进行了数据预处理、特征提取、特征选择和数据平衡技术。实验结果表明，在真正阳性率方面，MLP在预测性能上优于逻辑回归、决策树、随机森林、LightGBM和XGBoost，实现了最佳表现。

    The use of credit cards has recently increased, creating an essential need for credit card assessment methods to minimize potential risks. This study investigates the utilization of machine learning (ML) models for credit card default prediction system. The main goal here is to investigate the best-performing ML model for new proposed credit card scoring dataset. This new dataset includes credit card transaction histories and customer profiles, is proposed and tested using a variety of machine learning algorithms, including logistic regression, decision trees, random forests, multi layer perceptron (MLP) neural network, XGBoost, and LightGBM. To prepare the data for machine learning models, we perform data pre-proccessing, feature extraction, feature selection, and data balancing techniques. Experimental results demonstrate that MLP outperforms logistic regression, decision trees, random forests, LightGBM, and XGBoost in terms of predictive performance in true positive rate, achieving 
    
[^11]: 阴影对齐：轻松颠覆安全对齐的语言模型

    Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models. (arXiv:2310.02949v1 [cs.CL])

    [http://arxiv.org/abs/2310.02949](http://arxiv.org/abs/2310.02949)

    该论文探讨了阴影对齐这种新的攻击方式，通过调整少量恶意示例，安全对齐的语言模型可以被轻松颠覆生成有害的内容，同时仍然可以正确响应常规查询。

    

    警告：本论文包含有害语言的例子，建议读者慎重。强大的大型语言模型（LLMs）的逐渐开放释放，通过降低数据注释和计算的核心成本，促进了下游应用的发展。为了确保AI的安全性，进行了广泛的安全对齐措施，以保护这些模型免受恶意使用（主要是硬提示攻击）。然而，在这种看似坚固的盔甲背后，可能潜伏着一个阴影。通过仅调整100个恶意示例，使用1个GPU小时，这些安全对齐的LLMs可以轻松地被颠覆以生成有害内容。形式上，我们将一种新攻击称为阴影对齐：利用少量数据可以使安全对齐模型适应有害任务，而不会牺牲模型的有用性。值得注意的是，被颠覆的模型仍然保留其对常规查询的适当响应能力。在5个发行的8个模型上进行的实验证实了这一点。

    Warning: This paper contains examples of harmful language, and reader discretion is recommended. The increasing open release of powerful large language models (LLMs) has facilitated the development of downstream applications by reducing the essential cost of data annotation and computation. To ensure AI safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). However, beneath the seemingly resilient facade of the armor, there might lurk a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these safely aligned LLMs can be easily subverted to generate harmful content. Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of data can elicit safely-aligned models to adapt to harmful tasks without sacrificing model helpfulness. Remarkably, the subverted models retain their capability to respond appropriately to regular inquiries. Experiments across 8 models released by 5
    
[^12]: 本文提供了三个Bethe-Kikuchi变分原理的综合概述，包括它们与超图上的信念传播(BP)算法的关系。将BP方程的结构推广到连续时间扩散，解决了局部max-熵原理(A)、变分自由能原理(B)和不太常见的平衡自由能原理(C)(A的Legendre对偶)的问题。Bethe-Kikuchi泛函的临界点和平稳信念都位于两个约束面的非线性交点处，分别强制能量守恒和边际一致性。奇异信念的超曲面由约束面与切线相遇时，通过一系列二进制变量的图的环级数展开来描述。

    Local Max-Entropy and Free Energy Principles, Belief Diffusions and their Singularities. (arXiv:2310.02946v1 [math-ph])

    [http://arxiv.org/abs/2310.02946](http://arxiv.org/abs/2310.02946)

    本文综述了三个Bethe-Kikuchi变分原理，探讨了它们与超图上的信念传播算法的关系，并推广了BP方程结构，定义了连续时间扩散。此外，该文还研究了局部max-熵原理、变分自由能原理和平衡自由能原理，并描述了奇异信念的超曲面。

    

    给出了三个Bethe-Kikuchi变分原理的综合概述，包括它们与超图上的信念传播(BP)算法的关系。将BP方程的结构推广到定义连续时间扩散，解决了局部max-熵原理(A)、变分自由能原理(B)和不太常见的平衡自由能原理(C)的问题，C是A的Legendre对偶。说明了Bethe-Kikuchi泛函的临界点和平稳信念位于两个约束面的非线性交点处，分别引入了能量守恒和边际一致性。奇异信念的超曲面由约束面与切线相遇时的多项式方程描述，该方程由一系列二进制变量的图的环级数展开表示。

    A comprehensive picture of three Bethe-Kikuchi variational principles including their relationship to belief propagation (BP) algorithms on hypergraphs is given. The structure of BP equations is generalized to define continuous-time diffusions, solving localized versions of the max-entropy principle (A), the variational free energy principle (B), and a less usual equilibrium free energy principle (C), Legendre dual to A. Both critical points of Bethe-Kikuchi functionals and stationary beliefs are shown to lie at the non-linear intersection of two constraint surfaces, enforcing energy conservation and marginal consistency respectively. The hypersurface of singular beliefs, accross which equilibria become unstable as the constraint surfaces meet tangentially, is described by polynomial equations in the convex polytope of consistent beliefs. This polynomial is expressed by a loop series expansion for graphs of binary variables.
    
[^13]: 对气候信息的大规模语言模型进行评估

    Assessing Large Language Models on Climate Information. (arXiv:2310.02932v1 [cs.CL])

    [http://arxiv.org/abs/2310.02932](http://arxiv.org/abs/2310.02932)

    本研究提出了一个基于科学传播原则的综合评估框架，评估了大规模语言模型在气候变化信息中的表现，能够在回答气候变化主题方面提供细粒度的分析。

    

    理解气候变化对我们的影响，了解可用的解决方案，是赋予个人和社区减缓和适应气候变化的重要步骤。随着大规模语言模型（LLMs）的普及，有必要评估它们在这个领域的能力。本研究提出了一个基于科学传播原则的综合评估框架，以分析LLM对气候变化主题的回答。我们的框架强调回答的呈现和认识上的适当性，为LLM生成提供了细粒度的分析。覆盖了8个维度，我们的框架能够识别模型输出中的30个不同问题。该任务是一个现实世界中的例子，这个领域存在越来越多的具有挑战性的问题，AI可以补充和提升人类的表现。我们引入了一种新颖而实用的可扩展监督协议，利用AI辅助并依靠具有相关教育背景的评估员。我们评估了几个最近的LLM，并进行了实证评估。

    Understanding how climate change affects us and learning about available solutions are key steps toward empowering individuals and communities to mitigate and adapt to it. As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in this domain. In this study, we present a comprehensive evaluation framework, grounded in science communication principles, to analyze LLM responses to climate change topics. Our framework emphasizes both the presentational and epistemological adequacy of answers, offering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our framework discerns up to 30 distinct issues in model outputs. The task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel and practical protocol for scalable oversight that uses AI Assistance and relies on raters with relevant educational backgrounds. We evaluate several recent LLMs and conduct 
    
[^14]: 在不确定且快速变化的交通中，学习辅助模型预测控制的初始热启动

    Learning-Aided Warmstart of Model Predictive Control in Uncertain Fast-Changing Traffic. (arXiv:2310.02918v1 [eess.SY])

    [http://arxiv.org/abs/2310.02918](http://arxiv.org/abs/2310.02918)

    本文提出了一个学习辅助模型预测控制算法的热启动框架，通过利用神经网络多模式预测器生成多个轨迹提案，并通过采样技术进行进一步优化，以解决模型预测控制在非凸问题和不确定快速变化的环境中的局部最小值问题。

    

    模型预测控制在非凸问题中缺乏逃离局部最小值的能力。此外，在快速变化、不确定性环境中，传统的热启动方法，即使用上一时间步的最优轨迹，往往无法提供当前最优轨迹的充分接近的初始猜测。这可能导致收敛失败和安全问题。因此，本文提出了一个学习辅助模型预测控制算法的热启动框架。我们的方法利用基于神经网络的多模式预测器为自动驾驶车辆生成多个轨迹提案，进一步通过基于采样的技术进行精细调整。这种综合方法使我们能够识别多个不同的局部最小值并提供改进的初始猜测。我们通过交通场景的蒙特卡洛模拟验证了我们的方法。

    Model Predictive Control lacks the ability to escape local minima in nonconvex problems. Furthermore, in fast-changing, uncertain environments, the conventional warmstart, using the optimal trajectory from the last timestep, often falls short of providing an adequately close initial guess for the current optimal trajectory. This can potentially result in convergence failures and safety issues. Therefore, this paper proposes a framework for learning-aided warmstarts of Model Predictive Control algorithms. Our method leverages a neural network based multimodal predictor to generate multiple trajectory proposals for the autonomous vehicle, which are further refined by a sampling-based technique. This combined approach enables us to identify multiple distinct local minima and provide an improved initial guess. We validate our approach with Monte Carlo simulations of traffic scenarios.
    
[^15]: 通过带有视觉和文本提示的扩散模型提升皮肤镜病变分割

    Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with Visual and Textual Prompts. (arXiv:2310.02906v1 [cs.CV])

    [http://arxiv.org/abs/2310.02906](http://arxiv.org/abs/2310.02906)

    本文提出了一种通过扩散模型结合视觉和文本提示来生成皮肤镜图像的方法，并证明了该方法在提高图像质量和皮肤病变分割性能方面的优势。

    

    在医疗图像分析任务中，图像合成方法（例如生成对抗网络）作为一种数据增强形式被广泛应用。这主要有助于克服公开可访问数据和相关质量注释的不足。然而，目前的技术通常无法对生成的图像中的详细内容进行精确控制，例如疾病模式类型、病变位置和诊断属性。在这项工作中，我们采用了最新的生成模型进展——扩散模型，并通过使用特定于病变的视觉和文本提示来增加控制流以生成皮肤镜图像。我们进一步证明了我们基于扩散模型的框架在图像质量和皮肤病变分割性能提升方面的优势。它能够在SSIM图像质量度量上达到9%的增加，并在Dice系数上超过了先前的研究结果，增加了5%以上。

    Image synthesis approaches, e.g., generative adversarial networks, have been popular as a form of data augmentation in medical image analysis tasks. It is primarily beneficial to overcome the shortage of publicly accessible data and associated quality annotations. However, the current techniques often lack control over the detailed contents in generated images, e.g., the type of disease patterns, the location of lesions, and attributes of the diagnosis. In this work, we adapt the latest advance in the generative model, i.e., the diffusion model, with the added control flow using lesion-specific visual and textual prompts for generating dermatoscopic images. We further demonstrate the advantage of our diffusion model-based framework over the classical generation models in both the image quality and boosting the segmentation performance on skin lesions. It can achieve a 9% increase in the SSIM image quality measure and an over 5% increase in Dice coefficients over the prior arts.
    
[^16]: 使用您的本能：使用神经探测器与转换器进行指令优化

    Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers. (arXiv:2310.02905v1 [cs.LG])

    [http://arxiv.org/abs/2310.02905](http://arxiv.org/abs/2310.02905)

    该论文提出了一种使用神经探测器和转换器优化指令的方法，以提高大型语言模型的性能。

    

    大型语言模型(LLMs)在各种应用中展示了出色的指令跟随能力，并取得了令人瞩目的表现。然而，LLMs的性能严重依赖于给予它们的指令，这些指令通常需要大量人力进行手动调整。最近的研究使用了高效的贝叶斯优化（BO）算法来自动优化给予黑盒LLMs的指令。然而，在优化高度复杂（例如高维）的目标函数时，如将指令映射到LLM性能的函数，BO通常表现不佳。这主要是由于BO使用的高斯过程（GP）模型的表达能力有限，该模型被用作BO的代理来建模目标函数。与此同时，已经多次证明神经网络（NNs），尤其是预训练的转换器，具有很强的表达能力，可以建模高度复杂的函数。因此，我们采用了一种神经探测器算法。

    Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) model which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algor
    
[^17]: 使用强化学习和Transformer搜索高价值分子

    Searching for High-Value Molecules Using Reinforcement Learning and Transformers. (arXiv:2310.02902v1 [cs.LG])

    [http://arxiv.org/abs/2310.02902](http://arxiv.org/abs/2310.02902)

    通过使用强化学习和Transformer，我们提出了一种新的基于RL的分子设计算法（ChemRLformer），并在25个分子设计任务中进行了综合分析，包括计算复杂的蛋白质对接模拟。我们发现了分子设计领域的独特见解，并展示了ChemRLformer相对于之前的工作更为简单且实现了最先进的性能。

    

    在搜索图中的高价值策略方面，使用文本表示的强化学习（RL）可以很有效。然而，RL需要对搜索空间进行精心结构化和算法设计才能在这个挑战中发挥作用。通过大量实验，我们探索了不同文本语法设计和训练算法选择如何影响RL策略生成具有所需属性的分子的能力。我们提出了一种新的基于RL的分子设计算法（ChemRLformer），并对其进行了深入分析，包括对计算复杂的蛋白质对接模拟进行的25个分子设计任务。通过这个分析，我们发现了该问题空间中的独特见解，并展示了ChemRLformer相较于之前的工作，通过阐明哪些设计选择实际上对基于文本的分子设计有帮助，实现了最先进的性能。

    Reinforcement learning (RL) over text representations can be effective for finding high-value policies that can search over graphs. However, RL requires careful structuring of the search space and algorithm design to be effective in this challenge. Through extensive experiments, we explore how different design choices for text grammar and algorithmic choices for training can affect an RL policy's ability to generate molecules with desired properties. We arrive at a new RL-based molecular design algorithm (ChemRLformer) and perform a thorough analysis using 25 molecule design tasks, including computationally complex protein docking simulations. From this analysis, we discover unique insights in this problem space and show that ChemRLformer achieves state-of-the-art performance while being more straightforward than prior work by demystifying which design choices are actually helpful for text-based molecule design.
    
[^18]: 关于实现数学推理的人工智能助手的途径的注释

    Notes on a Path to AI Assistance in Mathematical Reasoning. (arXiv:2310.02896v1 [math.HO])

    [http://arxiv.org/abs/2310.02896](http://arxiv.org/abs/2310.02896)

    这篇论文探讨了实现对研究数学家有用的人工智能助手的可能途径。

    

    这些非正式注释是基于作者在2023年6月的美国国家科学院、工程院和数学院举办的“人工智能辅助数学推理”研讨会上的演讲而撰写的。其目标是思考一种可能实现对研究数学家有用的人工智能的途径。

    These informal notes are based on the author's lecture at the National Academies of Science, Engineering, and Mathematics workshop on "AI to Assist Mathematical Reasoning" in June 2023. The goal is to think through a path by which we might arrive at AI that is useful for the research mathematician.
    
[^19]: 医疗保健领域联邦学习的最新方法学进展

    Recent Methodological Advances in Federated Learning for Healthcare. (arXiv:2310.02874v1 [cs.LG])

    [http://arxiv.org/abs/2310.02874](http://arxiv.org/abs/2310.02874)

    最近的医疗保健领域联邦学习研究提出了新的方法学，用于解决医疗保健数据的挑战，如孤立数据、类别不平衡、缺失数据、分布转移和非标准化变量。

    

    对于医疗保健数据集来说，由于伦理、隐私或后勤问题，通常不可能合并来自多个机构的数据样本。而联邦学习可以在不需要数据汇集的情况下利用强大的机器学习算法。医疗保健数据具有许多挑战，需要新的方法学来解决，如高度孤立的数据、类别不平衡、缺失数据、分布转移和非标准化变量。联邦学习对于传统的集中式机器学习增加了显著的方法学复杂性，需要分布式优化、节点间的通信、模型的聚合和模型的重新分发。在这个系统性综述中，我们考虑了Scopus上在2015年1月至2023年2月之间发表的所有描述解决医疗保健数据挑战的新联邦学习方法学的论文。我们对满足这些条件的89篇论文进行了详细的回顾。

    For healthcare datasets, it is often not possible to combine data samples from multiple sites due to ethical, privacy or logistical concerns. Federated learning allows for the utilisation of powerful machine learning algorithms without requiring the pooling of data. Healthcare data has many simultaneous challenges which require new methodologies to address, such as highly-siloed data, class imbalance, missing data, distribution shifts and non-standardised variables. Federated learning adds significant methodological complexity to conventional centralised machine learning, requiring distributed optimisation, communication between nodes, aggregation of models and redistribution of models. In this systematic review, we consider all papers on Scopus that were published between January 2015 and February 2023 and which describe new federated learning methodologies for addressing challenges with healthcare data. We performed a detailed review of the 89 papers which fulfilled these criteria. S
    
[^20]: 针对表格数据的稳定且可解释的深度学习: 引入具有新型InterpreStability指标的InterpreTabNet

    Stable and Interpretable Deep Learning for Tabular Data: Introducing InterpreTabNet with the Novel InterpreStability Metric. (arXiv:2310.02870v1 [cs.LG])

    [http://arxiv.org/abs/2310.02870](http://arxiv.org/abs/2310.02870)

    我们引入了InterpreTabNet，通过改进的注意模块和TabNet架构，提高了表格数据的分类准确度和解释性。我们还提出了一种新的评价指标InterpreStability，用于量化模型的解释稳定性。

    

    随着人工智能（AI）在各个领域的深度整合，对强大模型的追求已经加剧。虽然在提升模型能力和适用性方面已经取得了重大进展，但一个明显的挑战仍然存在：许多最先进的模型仍然是黑箱。这种不透明性不仅使解释模型决策给最终用户带来了复杂性，而且还阻碍了模型设计者对中间过程的洞察。为了解决这些挑战，我们引入了InterpreTabNet，这是一个通过利用改进的注意模块，改进TabNet架构，从而提高分类准确度和解释性的模型。这种设计确保了强大的梯度传播和计算稳定性。此外，我们还提出了一种新型评价指标InterpreStability，该指标量化了模型的解释稳定性。所提出的模型和指标标志着可解释模型研究的重要进展。

    As Artificial Intelligence (AI) integrates deeper into diverse sectors, the quest for powerful models has intensified. While significant strides have been made in boosting model capabilities and their applicability across domains, a glaring challenge persists: many of these state-of-the-art models remain as black boxes. This opacity not only complicates the explanation of model decisions to end-users but also obstructs insights into intermediate processes for model designers. To address these challenges, we introduce InterpreTabNet, a model designed to enhance both classification accuracy and interpretability by leveraging the TabNet architecture with an improved attentive module. This design ensures robust gradient propagation and computational stability. Additionally, we present a novel evaluation metric, InterpreStability, which quantifies the stability of a model's interpretability. The proposed model and metric mark a significant stride forward in explainable models' research, set
    
[^21]: 使用稀疏离散余弦Stockwell变换层的新型非对称自编码器用于齿轮传感器数据压缩

    A novel asymmetrical autoencoder with a sparsifying discrete cosine Stockwell transform layer for gearbox sensor data compression. (arXiv:2310.02862v1 [cs.LG])

    [http://arxiv.org/abs/2310.02862](http://arxiv.org/abs/2310.02862)

    这篇论文提出了一种信号自适应的非对称自编码器，使用离散余弦Stockwell变换层进行齿轮传感器数据压缩。通过引入可训练的滤波器和硬阈值化层，该方法能够提高数据重构的准确性，并且仅需要少量数据集进行训练。

    

    在非接触式齿轮故障诊断问题中，缺乏高效的数据压缩模型仍然是无线传输齿轮数据的一个挑战。本文提出了一种信号自适应的非对称自编码器，其中增加了一个变换域层来压缩传感器信号。首先，引入了一种新的离散余弦Stockwell变换（DCST）层以替代多层自编码器中的线性层。通过利用卷积的乘法特性在DCST域实现了一个可训练的滤波器。然后，应用可训练的硬阈值化层来减少DCST层中的冗余数据以使特征图稀疏化。与线性层相比，DCST层减少了可训练参数的数量，并提高了数据重构的准确性。其次，使用稀疏化的DCST层训练自编码器只需要少量的数据集。提出的方法在康涅狄格大学（Uo...[被截断]

    The lack of an efficient compression model remains a challenge for the wireless transmission of gearbox data in non-contact gear fault diagnosis problems. In this paper, we present a signal-adaptive asymmetrical autoencoder with a transform domain layer to compress sensor signals. First, a new discrete cosine Stockwell transform (DCST) layer is introduced to replace linear layers in a multi-layer autoencoder. A trainable filter is implemented in the DCST domain by utilizing the multiplication property of the convolution. A trainable hard-thresholding layer is applied to reduce redundant data in the DCST layer to make the feature map sparse. In comparison to the linear layer, the DCST layer reduces the number of trainable parameters and improves the accuracy of data reconstruction. Second, training the autoencoder with a sparsifying DCST layer only requires a small number of datasets. The proposed method is superior to other autoencoder-based methods on the University of Connecticut (Uo
    
[^22]: Rayleigh Quotient Graph Neural Networks用于图级异常检测的研究

    Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection. (arXiv:2310.02861v1 [cs.LG])

    [http://arxiv.org/abs/2310.02861](http://arxiv.org/abs/2310.02861)

    《Rayleigh Quotient Graph Neural Networks用于图级异常检测的研究》提出使用Rayleigh Quotient作为驱动因素，通过探索图的固有光谱特征来实现图级异常检测。

    

    图级异常检测在癌症诊断和酶预测等领域中广泛应用。然而，现有方法无法捕捉到图异常的潜在属性，导致框架设计不可解释和性能不令人满意。在本文中，我们退一步重新研究了异常和正常图之间的光谱差异。我们的主要观察表明，这两个类之间的累计光谱能量存在显著差异。此外，我们证明了图信号的累计光谱能量可以用其瑞利商表示，这表明瑞利商是图异常属性的一个驱动因素。受此启发，我们提出了Rayleigh Quotient Graph Neural Network（RQGNN），这是第一个用于图级异常检测的光谱GNN，为探索异常图的固有光谱特征提供了新的视角。

    Graph-level anomaly detection has gained significant attention as it finds many applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the underlying properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. In this paper, we take a step back and re-investigate the spectral differences between anomalous and normal graphs. Our main observation shows a significant disparity in the accumulated spectral energy between these two classes. Moreover, we prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network (RQGNN), the first spectral GNN for graph-level anomaly detection, providing a new perspective on exploring the inherent spectral features of anomalous graph
    
[^23]: 用智能多任务适应混合提示扫描异质性

    Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation. (arXiv:2310.02842v1 [cs.CL])

    [http://arxiv.org/abs/2310.02842](http://arxiv.org/abs/2310.02842)

    本论文提出了一种使用智能多任务适应混合提示的方法来解决LLM在处理异质任务和数据分布时的问题。研究者设计了智能门控功能，用于识别嵌入在不同提示组中的相关技能，并根据目标任务的需求动态分配组合专家。该方法对任何模型压缩技术都不受限制，提高了任务处理的效率。

    

    大型语言模型(LLM)有能力解决各种任务，如文本摘要和数学问题，但通常是针对单一任务进行训练。由于高计算成本，当前趋势是使用提示指导调节预先训练的LLM以适应新的下游任务。因此，如何扩展提示调节以同时处理异质任务和数据分布是一个广泛开放的问题。为了解决这一问题，我们建议使用"混合提示"或MoPs，并结合智能门控功能：后者的设计是本文的贡献之一，它可以识别嵌入在不同提示组中的相关技能，并根据目标任务动态分配组合专家(即一组提示)。此外，MoPs在应用任何模型压缩技术时都不受影响——以提高效率。

    Large Language Models (LLMs) have the ability to solve a variety of tasks, such as text summarization and mathematical questions, just out of the box, but they are often trained with a single task in mind. Due to high computational costs, the current trend is to use prompt instruction tuning to better adjust monolithic, pretrained LLMs for new -- but often individual -- downstream tasks. Thus, how one would expand prompt tuning to handle -- concomitantly -heterogeneous tasks and data distributions is a widely open question. To address this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs, associated with smart gating functionality: the latter -- whose design is one of the contributions of this paper -- can identify relevant skills embedded in different groups of prompts and dynamically assign combined experts (i.e., collection of prompts), based on the target task. Additionally, MoPs are empirically agnostic to any model compression technique applied -- for efficiency re
    
[^24]: 使用语言模态指导提高视觉异常检测

    Improving Vision Anomaly Detection with the Guidance of Language Modality. (arXiv:2310.02821v1 [cs.CV])

    [http://arxiv.org/abs/2310.02821](http://arxiv.org/abs/2310.02821)

    本文介绍了一种通过语言模态提高视觉异常检测的方法。我们提出了跨模态引导（CMG），它包括跨模态熵减少（CMER）和跨模态线性嵌入（CMLE），分别解决了冗余信息和稀疏空间问题。CMER通过遮盖图像的一部分并计算与文本的匹配分数，使检测器聚焦于关键内容。CMLE学习了一个相关的结构矩阵来学习更紧凑的视觉异常检测器的潜空间。

    

    近年来，人们对于处理工业缺陷检测、事件检测等方面的异常检测越来越感兴趣。然而，现有的无监督异常检测器，特别是用于视觉模态的，由于冗余信息和稀疏潜空间，面临着重大挑战。相比之下，语言模态由于相对单一的数据而表现出色。本文从多模态的角度解决了上述视觉模态的挑战。具体而言，我们提出了跨模态引导（CMG），包括跨模态熵减少（CMER）和跨模态线性嵌入（CMLE），分别解决了冗余信息和稀疏空间问题。CMER通过遮盖原始图像的一部分，并计算与文本的匹配分数。然后，CMER丢弃不相关的像素，使检测器聚焦于关键内容。为了学习一个更紧凑的视觉异常检测器的潜空间，CMLE学习了一个相关结构矩阵。

    Recent years have seen a surge of interest in anomaly detection for tackling industrial defect detection, event detection, etc. However, existing unsupervised anomaly detectors, particularly those for the vision modality, face significant challenges due to redundant information and sparse latent space. Conversely, the language modality performs well due to its relatively single data. This paper tackles the aforementioned challenges for vision modality from a multimodal point of view. Specifically, we propose Cross-modal Guidance (CMG), which consists of Cross-modal Entropy Reduction (CMER) and Cross-modal Linear Embedding (CMLE), to tackle the redundant information issue and sparse space issue, respectively. CMER masks parts of the raw image and computes the matching score with the text. Then, CMER discards irrelevant pixels to make the detector focus on critical contents. To learn a more compact latent space for the vision anomaly detector, CMLE learns a correlation structure matrix f
    
[^25]: 智能制造系统中的时间序列分类: 对最先进机器学习算法的实验评估

    Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms. (arXiv:2310.02812v1 [cs.LG])

    [http://arxiv.org/abs/2310.02812](http://arxiv.org/abs/2310.02812)

    本研究通过严格实验评估了智能制造系统中最先进的机器学习和深度学习算法在时间序列分类任务中的性能，填补了该领域的研究空白。

    

    随着传感器数量的增加和感知技术的快速发展，制造业正在收集大量各种各样的数据。在智能制造系统 (SMS) 环境中，时间序列数据起着关键的作用。因此，时间序列分类 (TSC) 在该领域中至关重要。本研究的目标是通过对制造业和工业环境中 TSC 任务的最先进机器学习和深度学习算法进行严格的实验评估来填补这一空白。我们首先在 TSC 和制造业文献中探索和编制了一份包含超过92个最先进算法的全面列表。随后，我们从该列表中选择了最具代表性的36个算法。为了评估这些算法在各种制造业分类任务中的性能，我们策划了一组包含22个制造业数据集的基准数据集，这些数据集具有不同的特征，涵盖了各种制造问题。随后，我们在制造业基准数据集上实施并评估了这些算法。

    Manufacturing is gathering extensive amounts of diverse data, thanks to the growing number of sensors and rapid advances in sensing technologies. Among the various data types available in SMS settings, time-series data plays a pivotal role. Hence, TSC emerges is crucial in this domain. The objective of this study is to fill this gap by providing a rigorous experimental evaluation of the SoTA ML and DL algorithms for TSC tasks in manufacturing and industrial settings. We first explored and compiled a comprehensive list of more than 92 SoTA algorithms from both TSC and manufacturing literature. Following, we selected the 36 most representative algorithms from this list. To evaluate their performance across various manufacturing classification tasks, we curated a set of 22 manufacturing datasets, representative of different characteristics that cover diverse manufacturing problems. Subsequently, we implemented and evaluated the algorithms on the manufacturing benchmark datasets, and analy
    
[^26]: 使用对抗性环境设计探索通用强化学习算法

    Discovering General Reinforcement Learning Algorithms with Adversarial Environment Design. (arXiv:2310.02782v1 [cs.LG])

    [http://arxiv.org/abs/2310.02782](http://arxiv.org/abs/2310.02782)

    通过对抗性环境设计，我们提出了一种通用强化学习算法，通过元学习更新规则和自动生成课程来提高算法的泛化性能，并引入了一种新的遗憾近似方法，名为算法遗憾（AR）。

    

    在过去的十年中，深度强化学习取得了巨大的进展，这些进展是由人类研究人员手动设计的算法推动的。最近，已经证明可以元学习更新规则，希望发现在各种强化学习任务中表现良好的算法。尽管像学习策略梯度（LPG）这样的算法取得了令人印象深刻的初步结果，但是当这些算法应用于未见过的环境时仍存在泛化差距。在这项工作中，我们研究了元训练分布的特征如何影响这些算法的泛化性能。受到这个分析的启发，并借鉴了无监督环境设计（UED）的思想，我们提出了一种自动生成课程的新方法，以最大化元学习优化器的遗憾，此外还提出了一种新的遗憾近似方法，我们称之为算法遗憾（AR）。我们的方法是通过环境设计获得的通用强化学习优化器。

    The past decade has seen vast progress in deep reinforcement learning (RL) on the back of algorithms manually designed by human researchers. Recently, it has been shown that it is possible to meta-learn update rules, with the hope of discovering algorithms that can perform well on a wide range of RL tasks. Despite impressive initial results from algorithms such as Learned Policy Gradient (LPG), there remains a generalization gap when these algorithms are applied to unseen environments. In this work, we examine how characteristics of the meta-training distribution impact the generalization performance of these algorithms. Motivated by this analysis and building on ideas from Unsupervised Environment Design (UED), we propose a novel approach for automatically generating curricula to maximize the regret of a meta-learned optimizer, in addition to a novel approximation of regret, which we name algorithmic regret (AR). The result is our method, General RL Optimizers Obtained Via Environment
    
[^27]: 一个增强的 UMLS 框架，用于改善大型语言模型在医疗保健中的事实性

    A UMLS-Augmented Framework for Improving Factuality in Large Language Models within Healthcare. (arXiv:2310.02778v1 [cs.CL])

    [http://arxiv.org/abs/2310.02778](http://arxiv.org/abs/2310.02778)

    该论文提出了一个基于UMLS的增强型大型语言模型框架，旨在改善医疗保健领域中模型生成内容的事实性。通过自动评估和医生评估，研究人员验证了该框架的有效性。

    

    大型语言模型（LLM）展示了强大的文本生成能力，为医疗保健领域带来了前所未有的创新。然而，将LLMs应用于真实临床场景面临重大挑战，因为这些模型可能生成与已建立医学事实偏离的内容，甚至可能表现出潜在的偏见。在我们的研究中，我们开发了一个基于统一医学语言系统（UMLS）的增强型LLM框架，旨在更好地服务医疗保健社区。我们采用LLaMa2-13b-chat和ChatGPT-3.5作为基准模型，并使用ROUGE分数和BERT分数在LiveQA测试集的104个问题上进行自动评估。此外，我们根据事实性、完整性、可读性和相关性四个维度建立了医生评估标准。ChatGPT-3.5用于医生评估，针对LiveQA测试集的20个问题。多位住院医师进行评估。

    Large language models (LLMs) have demonstrated powerful text generation capabilities, bringing unprecedented innovation to the healthcare field. While LLMs hold immense promise for applications in healthcare, applying them to real clinical scenarios presents significant challenges, as these models may generate content that deviates from established medical facts and even exhibit potential biases. In our research, we develop an augmented LLM framework based on the Unified Medical Language System (UMLS), aiming to better serve the healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our benchmark models, and conduct automatic evaluations using the ROUGE Score and BERTScore on 104 questions from the LiveQA test set. Additionally, we establish criteria for physician-evaluation based on four dimensions: Factuality, Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician evaluation with 20 questions on the LiveQA test set. Multiple resident physicians conduct
    
[^28]: 用于有效训练脉冲神经网络的脉冲累积转发方法

    Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks. (arXiv:2310.02772v1 [cs.NE])

    [http://arxiv.org/abs/2310.02772](http://arxiv.org/abs/2310.02772)

    本研究提出了一种名为脉冲累积转发（SAF）的方法，可以有效训练脉冲神经网络（SNNs）。SAF不仅可以减少前向过程中的操作次数，与Spike Representation和OTTT保持一致，而且可以解决SNNs训练中的难题。

    

    本文提出了一种新的脉冲神经网络（SNNs）训练范式，即脉冲累积转发（SAF）。已知SNNs具有高能效但难以训练的特点。许多研究者提出了各种方法来解决这个问题，其中时间上的在线训练（OTTT）是一种在每个时间步骤推断的方法，同时抑制内存成本。然而，为了在GPU上高效计算，OTTT需要进行脉冲序列操作和脉冲序列加权求和操作。此外，OTTT与Spike Representation（另一种训练方法）之间存在关联，但与Spike Representation的理论一致性尚未得到证明。我们的方法可以解决这些问题，即SAF可以在前向过程中减少一半的操作次数，并且可以从理论上证明SAF分别与Spike Representation和OTTT一致。此外，我们还确认了......

    In this article, we propose a new paradigm for training spiking neural networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are energy-efficient but difficult to train. Consequently, many researchers have proposed various methods to solve this problem, among which online training through time (OTTT) is a method that allows inferring at each time step while suppressing the memory cost. However, to compute efficiently on GPUs, OTTT requires operations with spike trains and weighted summation of spike trains during forwarding. In addition, OTTT has shown a relationship with the Spike Representation, an alternative training method, though theoretical agreement with Spike Representation has yet to be proven. Our proposed method can solve these problems; namely, SAF can halve the number of operations during the forward process, and it can be theoretically proven that SAF is consistent with the Spike Representation and OTTT, respectively. Furthermore, we confirmed the a
    
[^29]: MUNCH: 建模独特且可控制的头部

    MUNCH: Modelling Unique 'N Controllable Heads. (arXiv:2310.02753v1 [cs.CV])

    [http://arxiv.org/abs/2310.02753](http://arxiv.org/abs/2310.02753)

    本论文提出了一种方法，可以自动生成质量高、多样性强、可控制的逼真三维人头，具有可解释的网络设计。方法包括几何生成器、渲染图生成器和颜色变换模型。同时还引入了独特性和新颖性的量化指标。

    

    对于计算机视觉研究人员来说，自动生成三维人头一直是一个引人入胜且具有挑战性的任务。现有的方法可以合成逼真的角色，但对于生成结果的多样性和质量的控制有限，并且形状和纹理之间的相关性也受到限制。我们提出的方法在质量、多样性、控制和逼真性方面都具备了可行性，并且还提供了可解释的网络设计，对于游戏设计艺术家来说这些都是理想的特点。首先，我们提出的几何生成器可以识别脱耦的潜在方向并生成新颖且多样的样本。然后，渲染图生成器学习合成多个高保真度的基于物理的渲染图，包括反照率、光泽度、镜面反射和法线方向。对于更细粒度的控制输出的艺术家，我们引入了一种新颖的颜色变换模型，允许对生成的图像进行语义颜色控制。我们还引入了一种可量化的指标，称为独特性和新颖性。

    The automated generation of 3D human heads has been an intriguing and challenging task for computer vision researchers. Prevailing methods synthesize realistic avatars but with limited control over the diversity and quality of rendered outputs and suffer from limited correlation between shape and texture of the character. We propose a method that offers quality, diversity, control, and realism along with explainable network design, all desirable features to game-design artists in the domain. First, our proposed Geometry Generator identifies disentangled latent directions and generate novel and diverse samples. A Render Map Generator then learns to synthesize multiply high-fidelty physically-based render maps including Albedo, Glossiness, Specular, and Normals. For artists preferring fine-grained control over the output, we introduce a novel Color Transformer Model that allows semantic color control over generated maps. We also introduce quantifiable metrics called Uniqueness and Novelt
    
[^30]: AI系统的功能可信度通过统计学有效测试

    Functional trustworthiness of AI systems by statistically valid testing. (arXiv:2310.02727v1 [stat.ML])

    [http://arxiv.org/abs/2310.02727](http://arxiv.org/abs/2310.02727)

    作者认为欧盟AI法案对AI系统的质量保证方式存在不足，并指出基于统计学有效测试及准确定义应用是确保AI系统功能可信度的核心。

    

    作者关注欧洲公民的安全、健康和权益问题，因为当前欧盟人工智能（AI）法案的草案对AI系统的符合性评估所需的措施和程序不足。我们注意到，欧盟AI法案的当前草案以及在CEN/CENELEC进行的配套标准化工作，都采取了一个观点，即AI系统的实际功能保证似乎是不切实际且过于复杂的。然而，制定一个符合性评估程序，使未经充分评估的AI系统产生虚假的信任幻象，充其量是幼稚的，充其更糟的情况是严重疏忽的。因此，欧盟AI法案错过了确保通过功能可信度来确保质量和正确分配责任的目的。AI决策系统的可信度首先在于对随机选择的样本进行正确的统计测试，并在定义应用的准确性上。

    The authors are concerned about the safety, health, and rights of the European citizens due to inadequate measures and procedures required by the current draft of the EU Artificial Intelligence (AI) Act for the conformity assessment of AI systems. We observe that not only the current draft of the EU AI Act, but also the accompanying standardization efforts in CEN/CENELEC, have resorted to the position that real functional guarantees of AI systems supposedly would be unrealistic and too complex anyways. Yet enacting a conformity assessment procedure that creates the false illusion of trust in insufficiently assessed AI systems is at best naive and at worst grossly negligent. The EU AI Act thus misses the point of ensuring quality by functional trustworthiness and correctly attributing responsibilities.  The trustworthiness of an AI decision system lies first and foremost in the correct statistical testing on randomly selected samples and in the precision of the definition of the applica
    
[^31]: 在用户模型错误的情况下的在线聚类强化学习

    Online Clustering of Bandits with Misspecified User Models. (arXiv:2310.02717v1 [cs.LG])

    [http://arxiv.org/abs/2310.02717](http://arxiv.org/abs/2310.02717)

    本文介绍了在用户模型错误的情况下的聚类强化学习问题，并提出了两个鲁棒的聚类强化学习算法，以解决用户模型偏差的挑战。

    

    上下文线性强化学习是一个重要的在线学习问题，在每轮中，给定臂的特征，学习代理选择一个臂来最大化长期的累积奖励。聚类强化学习是一系列工作，利用用户偏好的协同效应，并在经典的线性强化学习算法上取得了显著的改进。然而，现有的聚类强化学习算法需要正确规定线性用户模型，当这个关键假设不成立时，可能会失败。如何为在用户模型错误的实际情况下设计鲁棒的聚类强化学习算法仍然是一个开放的问题。在本文中，我们首次提出了在用户模型错误的情况下的聚类强化学习问题，其中用户模型中的期望奖励可能有偏差，不是完美的线性模型。我们设计了两个鲁棒的聚类强化学习算法RCLUMB和RSCLUMB（分别用动态图和集合表示学习到的聚类结构）。

    The contextual linear bandit is an important online learning problem where given arm features, a learning agent selects an arm at each round to maximize the cumulative rewards in the long run. A line of works, called the clustering of bandits (CB), utilize the collaborative effect over user preferences and have shown significant improvements over classic linear bandit algorithms. However, existing CB algorithms require well-specified linear user models and can fail when this critical assumption does not hold. Whether robust CB algorithms can be designed for more practical scenarios with misspecified user models remains an open problem. In this paper, we are the first to present the important problem of clustering of bandits with misspecified user models (CBMUM), where the expected rewards in user models can be perturbed away from perfect linear models. We devise two robust CB algorithms, RCLUMB and RSCLUMB (representing the learned clustering structure with dynamic graph and sets, resp
    
[^32]: scHyena: 基于全长单细胞RNA-Seq的大脑分析的基础模型

    scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis in Brain. (arXiv:2310.02713v1 [cs.LG])

    [http://arxiv.org/abs/2310.02713](http://arxiv.org/abs/2310.02713)

    scHyena是一个基于Transformer架构的模型，称为单细胞Hyena(scHyena)，旨在处理大脑中的全长scRNA-seq数据，并提高分析的准确性。

    

    单细胞RNA测序(scRNA-seq)在揭示复杂组织中微妙的细胞多样性方面取得了重要进展。这在大脑中尤为关键，因为大脑比其他组织类型有更多种类的细胞，以便更深入地了解不同细胞环境下的大脑功能。然而，由于缺失事件所产生的固有测量噪声以及对大量基因表达信息的有限利用，分析scRNA-seq数据仍然是一个挑战。在这项工作中，我们引入了scHyena，这是一个旨在应对这些挑战并提高大脑中scRNA-seq分析准确性的基础模型。具体而言，受到最近的Hyena算子的启发，我们设计了一种新颖的Transformer架构，称为单细胞Hyena(scHyena)，它配备了线性适配器层、通过基因嵌入实现的位置编码和一个双向Hyena算子。这使我们能够处理全长的scRNA-seq数据而不丢失信息。

    Single-cell RNA sequencing (scRNA-seq) has made significant strides in unraveling the intricate cellular diversity within complex tissues. This is particularly critical in the brain, presenting a greater diversity of cell types than other tissue types, to gain a deeper understanding of brain function within various cellular contexts. However, analyzing scRNA-seq data remains a challenge due to inherent measurement noise stemming from dropout events and the limited utilization of extensive gene expression information. In this work, we introduce scHyena, a foundation model designed to address these challenges and enhance the accuracy of scRNA-seq analysis in the brain. Specifically, inspired by the recent Hyena operator, we design a novel Transformer architecture called singe-cell Hyena (scHyena) that is equipped with a linear adaptor layer, the positional encoding via gene-embedding, and a {bidirectional} Hyena operator. This enables us to process full-length scRNA-seq data without losi
    
[^33]: ED-NeRF: 使用潜空间 NeRF 实现高效的文本引导的 3D 场景编辑

    ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF. (arXiv:2310.02712v1 [cs.CV])

    [http://arxiv.org/abs/2310.02712](http://arxiv.org/abs/2310.02712)

    ED-NeRF 提出了一种高效的 3D 场景编辑方法，通过将场景嵌入到潜空间中，得到更快速且更易于编辑的 NeRF 骨干。

    

    最近，文本到图像扩散模型取得了显著进展，在二维图像生成方面取得了突破性的性能。这些进展已经扩展到三维模型，实现了从文本描述中生成新的三维对象。这演变成了 NeRF 编辑方法，通过文本条件允许对现有的三维对象进行操作。然而，现有的 NeRF 编辑技术在性能上面临着一些限制，如训练速度慢和使用的损失函数不充分考虑编辑。为了解决这个问题，我们提出了一种新颖的 3D NeRF 编辑方法，称为 ED-NeRF，通过将真实世界场景成功嵌入到潜扩散模型 (LDM) 的潜空间中，通过独特的细化层。这种方法使我们能够获得一个不仅更快，而且更适合于编辑的 NeRF 骨干，与传统的图像空间 NeRF 编辑相比。此外，我们提出了一种改进的损失函数。

    Recently, there has been a significant advancement in text-to-image diffusion models, leading to groundbreaking performance in 2D image generation. These advancements have been extended to 3D models, enabling the generation of novel 3D objects from textual descriptions. This has evolved into NeRF editing methods, which allow the manipulation of existing 3D objects through textual conditioning. However, existing NeRF editing techniques have faced limitations in their performance due to slow training speeds and the use of loss functions that do not adequately consider editing. To address this, here we present a novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding real-world scenes into the latent space of the latent diffusion model (LDM) through a unique refinement layer. This approach enables us to obtain a NeRF backbone that is not only faster but also more amenable to editing compared to traditional image space NeRF editing. Furthermore, we propose an improved loss 
    
[^34]: 持续对比式语音理解

    Continual Contrastive Spoken Language Understanding. (arXiv:2310.02699v1 [eess.AS])

    [http://arxiv.org/abs/2310.02699](http://arxiv.org/abs/2310.02699)

    COCONUT是一种类别增量学习方法，结合了经验重播和对比式学习，在语音理解领域中解决了模型在持续学习新任务时难以保持之前知识的问题。

    

    最近，神经网络在各个领域取得了令人印象深刻的进展，其中包括语音处理。然而，这个领域的最新突破通常需要使用大规模数据集和庞大的计算资源进行离线训练。不幸的是，这些模型在持续学习新任务时往往难以保持之前获得的知识，并且重新训练几乎总是不可行的。在本文中，我们研究了一种在类别增量学习（CIL）设置下学习序列到序列模型用于语音理解的问题，并提出了一种称为COCONUT的CIL方法，该方法依赖于经验重播和对比式学习的组合。通过对仅对回放样本应用改进版本的标准有监督对比损失，COCONUT通过将同一类别的样本拉近并将其他样本推开，保留了学习到的表示。此外，我们利用了一种多模态对比损失。

    Recently, neural networks have shown impressive progress across diverse fields, with speech processing being no exception. However, recent breakthroughs in this area require extensive offline training using large datasets and tremendous computing resources. Unfortunately, these models struggle to retain their previously acquired knowledge when learning new tasks continually, and retraining from scratch is almost always impractical. In this paper, we investigate the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting and we propose COCONUT, a CIL method that relies on the combination of experience replay and contrastive learning. Through a modified version of the standard supervised contrastive loss applied only to the rehearsal samples, COCONUT preserves the learned representations by pulling closer samples from the same class and pushing away the others. Moreover, we leverage a multimodal contrastive loss that
    
[^35]: 基于聚类的图像-文本图匹配来弥合领域差距

    Bridging the Domain Gap by Clustering-based Image-Text Graph Matching. (arXiv:2310.02692v1 [cs.CV])

    [http://arxiv.org/abs/2310.02692](http://arxiv.org/abs/2310.02692)

    通过基于聚类的图像-文本图匹配来弥合领域差距，学习领域不变特征以实现在未见过领域上的良好泛化能力，实验结果显示在公共数据集上达到最先进性能。

    

    学习领域不变表示对于训练可以很好地推广到未见过目标任务领域的模型非常重要。文本描述本身包含概念的语义结构，这样的辅助语义线索可以用作领域概括问题的有效枢纽嵌入。我们使用多模态图像和文本融合的图表示来获得在局部图像和文本描述符之间考虑内在语义结构的领域不变枢纽嵌入。具体来说，我们通过(i)用图表示图像和文本描述，以及(ii)将基于图像节点特征的聚类和匹配应用到文本图中，来学习领域不变特征。我们使用大规模公共数据集（如CUB-DG和DomainBed）进行实验，并在这些数据集上达到与或优于现有最先进模型的性能。我们的代码将在出版后公开提供。

    Learning domain-invariant representations is important to train a model that can generalize well to unseen target task domains. Text descriptions inherently contain semantic structures of concepts and such auxiliary semantic cues can be used as effective pivot embedding for domain generalization problems. Here, we use multimodal graph representations, fusing images and text, to get domain-invariant pivot embeddings by considering the inherent semantic structure between local images and text descriptors. Specifically, we aim to learn domain-invariant features by (i) representing the image and text descriptions with graphs, and by (ii) clustering and matching the graph-based image node features into textual graphs simultaneously. We experiment with large-scale public datasets, such as CUB-DG and DomainBed, and our model achieves matched or better state-of-the-art performance on these datasets. Our code will be publicly available upon publication.
    
[^36]: USB-NeRF: 解卷曲快门束调整的神经辐射场

    USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields. (arXiv:2310.02687v1 [cs.CV])

    [http://arxiv.org/abs/2310.02687](http://arxiv.org/abs/2310.02687)

    USB-NeRF是一种解决滚动快门相机问题的神经辐射场算法，能够纠正滚动快门失真并恢复准确的相机运动轨迹，相比之前的方法在RS效应去除和新视角图像生成方面表现更好。

    

    最近神经辐射场（NeRF）因其出色的能力来表示3D场景和合成新的视角图像而受到广泛关注。现有的工作通常假设输入图像是由全局快门相机拍摄的。因此，滚动快门（RS）图像不能直接应用于现成的NeRF算法进行新视角合成。滚动快门效应还会影响相机位姿估计（例如通过COLMAP），进一步阻碍了使用RS图像的NeRF算法的成功。本文提出了一种解卷曲快门束调整的神经辐射场（USB-NeRF）。USB-NeRF能够在NeRF框架下纠正滚动快门失真并同时恢复准确的相机运动轨迹，通过对RS相机的物理图像形成过程进行建模。实验结果表明，USB-NeRF相比之前的工作在RS效应去除和新视角图像生成方面取得了更好的性能。

    Neural Radiance Fields (NeRF) has received much attention recently due to its impressive capability to represent 3D scene and synthesize novel view images. Existing works usually assume that the input images are captured by a global shutter camera. Thus, rolling shutter (RS) images cannot be trivially applied to an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter effect would also affect the accuracy of the camera pose estimation (e.g. via COLMAP), which further prevents the success of NeRF algorithm with RS images. In this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance Fields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and recover accurate camera motion trajectory simultaneously under the framework of NeRF, by modeling the physical image formation process of a RS camera. Experimental results demonstrate that USB-NeRF achieves better performance compared to prior works, in terms of RS effect removal, novel view image sy
    
[^37]: 扩散生成流采样器：通过部分轨迹优化改善学习信号

    Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization. (arXiv:2310.02679v1 [cs.LG])

    [http://arxiv.org/abs/2310.02679](http://arxiv.org/abs/2310.02679)

    这项工作介绍了一种名为扩散生成流采样器（DGFS）的采样框架，通过将学习过程分解为短的部分轨迹段，实现从难以处理的高维密度函数中进行采样。它通过利用中间的学习信号和非策略探索能力来改善学习信号的分配问题。

    

    我们解决了从难以处理的高维密度函数中进行采样的问题，这是在机器学习和统计中经常出现的基本任务。我们扩展了最近的基于采样的方法，利用控制的随机过程来模拟这些目标密度的近似样本。这些方法的主要缺点是训练目标需要计算完整的轨迹，导致由于使用完整轨迹和只在终端时间存在的学习信号的使用而产生缓慢的信用分配问题。在这项工作中，我们提出了扩散生成流采样器（DGFS），这是一个基于采样的框架，可以将学习过程可行地分解为短的部分轨迹段，通过参数化一个额外的“流函数”。我们的方法借鉴了生成流网络（GFlowNets）的理论，使我们能够利用中间的学习信号，并从非策略探索能力中受益。

    We tackle the problem of sampling from intractable high-dimensional density functions, a fundamental task that often appears in machine learning and statistics. We extend recent sampling-based approaches that leverage controlled stochastic processes to model approximate samples from these target densities. The main drawback of these approaches is that the training objective requires full trajectories to compute, resulting in sluggish credit assignment issues due to use of entire trajectories and a learning signal present only at the terminal time. In this work, we present Diffusion Generative Flow Samplers (DGFS), a sampling-based framework where the learning process can be tractably broken down into short partial trajectory segments, via parameterizing an additional "flow function". Our method takes inspiration from the theory developed for generative flow networks (GFlowNets), allowing us to make use of intermediate learning signals and benefit from off-policy exploration capabilitie
    
[^38]: 利用配对的OpenStreetMap数据和光学高分辨率影像进行土地覆盖变化检测

    Land-cover change detection using paired OpenStreetMap data and optical high-resolution imagery via object-guided Transformer. (arXiv:2310.02674v1 [cs.CV])

    [http://arxiv.org/abs/2310.02674](http://arxiv.org/abs/2310.02674)

    本文通过直接利用配对的OSM数据和光学图像进行土地覆盖变化检测，提出了一种基于对象引导的Transformer架构，从而拓宽了变化检测任务的范围，并显著减少了计算开销和内存负担。

    

    光学高分辨率影像和OpenStreetMap（OSM）数据是土地覆盖变化检测的两个重要数据源。先前的研究主要利用OSM数据来辅助多时期光学高分辨率图像的变化检测。本文通过直接利用配对的OSM数据和光学图像进行土地覆盖变化检测，拓宽了变化检测任务的范围，涵盖更多动态地球观测。为此，我们提出了一种基于对象引导的Transformer（ObjFormer）架构，将流行的基于对象的图像分析（OBIA）技术与先进的视觉Transformer架构自然地结合起来。引入OBIA可以显著减少自注意力模块中的计算开销和内存负担。具体而言，所提出的ObjFormer具有层次伪孪生编码器，包含对象引导自注意力模块，用于提取代表性特征。

    Optical high-resolution imagery and OpenStreetMap (OSM) data are two important data sources for land-cover change detection. Previous studies in these two data sources focus on utilizing the information in OSM data to aid the change detection on multi-temporal optical high-resolution images. This paper pioneers the direct detection of land-cover changes utilizing paired OSM data and optical imagery, thereby broadening the horizons of change detection tasks to encompass more dynamic earth observations. To this end, we propose an object-guided Transformer (ObjFormer) architecture by naturally combining the prevalent object-based image analysis (OBIA) technique with the advanced vision Transformer architecture. The introduction of OBIA can significantly reduce the computational overhead and memory burden in the self-attention module. Specifically, the proposed ObjFormer has a hierarchical pseudo-siamese encoder consisting of object-guided self-attention modules that extract representative
    
[^39]: 关于扩散模型记忆化的研究

    On Memorization in Diffusion Models. (arXiv:2310.02664v1 [cs.LG])

    [http://arxiv.org/abs/2310.02664](http://arxiv.org/abs/2310.02664)

    本论文研究了扩散模型的记忆化行为，发现记忆化倾向于在较小的数据集上发生。通过定义有效模型记忆化 (EMM) 这一指标，量化了数据分布和模型配置对记忆化行为的影响。

    

    近年来，由于其生成新颖高质量样本的能力，扩散模型引起了广泛的研究兴趣。然而，通过典型的训练目标，即去噪得分匹配，扩散模型只能生成复制训练数据的样本，这表明在理论上会出现记忆化的行为，这与现有先进扩散模型的普遍泛化能力相矛盾，因此需要深入理解。我们观察到记忆化行为倾向于在较小的数据集上发生，我们提出了有效模型记忆化(EMM)的定义，这是一种衡量学习的扩散模型在最大数据集上近似其理论最优点的度量标准。然后，我们量化了影响这些记忆化行为的重要因素，重点关注数据分布和模型配置。

    Due to their capacity to generate novel and high-quality samples, diffusion models have attracted significant research interest in recent years. Notably, the typical training objective of diffusion models, i.e., denoising score matching, has a closed-form optimal solution that can only generate training data replicating samples. This indicates that a memorization behavior is theoretically expected, which contradicts the common generalization ability of state-of-the-art diffusion models, and thus calls for a deeper understanding. Looking into this, we first observe that memorization behaviors tend to occur on smaller-sized datasets, which motivates our definition of effective model memorization (EMM), a metric measuring the maximum size of training data at which a learned diffusion model approximates its theoretical optimum. Then, we quantify the impact of the influential factors on these memorization behaviors in terms of EMM, focusing primarily on data distribution, model configuratio
    
[^40]: 解决多配置问题：使用Choco Solver的性能分析

    Solving Multi-Configuration Problems: A Performance Analysis with Choco Solver. (arXiv:2310.02658v1 [cs.AI])

    [http://arxiv.org/abs/2310.02658](http://arxiv.org/abs/2310.02658)

    本文介绍了使用Choco Solver进行多配置问题求解的应用案例，以及对约束求解器性能分析的研究，从而揭示了相关性能问题。

    

    在许多场景中，配置器支持配置满足单个用户偏好的解决方案。多配置的概念基于配置一组配置的想法。这种功能在配置个性化考试，配置项目团队和为旅游团队的每个成员配置不同的旅行（例如，在访问特定城市时）等场景中非常重要。在本文中，我们示例了多配置应用于生成个性化考试。我们还提供了一个约束求解器的性能分析，帮助我们对相应的性能问题有一些了解。

    In many scenarios, configurators support the configuration of a solution that satisfies the preferences of a single user. The concept of \emph{multi-configuration} is based on the idea of configuring a set of configurations. Such a functionality is relevant in scenarios such as the configuration of personalized exams, the configuration of project teams, and the configuration of different trips for individual members of a tourist group (e.g., when visiting a specific city). In this paper, we exemplify the application of multi-configuration for generating individualized exams. We also provide a constraint solver performance analysis which helps to gain some insights into corresponding performance issues.
    
[^41]: 对资源受限的FPGA的时间序列Transformer模型进行量化感知训练的研究

    A Study of Quantisation-aware Training on Time Series Transformer Models for Resource-constrained FPGAs. (arXiv:2310.02654v1 [cs.LG])

    [http://arxiv.org/abs/2310.02654](http://arxiv.org/abs/2310.02654)

    本研究探索了对时间序列Transformer模型进行量化感知训练的方法，并提出了一种新颖的自适应量化方案，通过匹配量化方案与实际数据分布，可以降低计算开销并保持可接受的精度。此外，该方法在应用于现实世界数据和混合精度量化时表现出鲁棒性。这些发现为模型量化和部署决策提供了参考，并推进了量化技术的发展。

    

    本研究探讨了对时间序列Transformer模型进行量化感知训练（QAT）。我们提出了一种新颖的自适应量化方案，在QAT阶段动态选择对称和非对称方案。我们的方法表明，将量化方案与真实数据分布匹配可以减少计算开销，同时保持可接受的精度。此外，我们的方法在应用于现实世界数据和混合精度量化时表现出鲁棒性，其中大多数对象被量化为4位。我们的发现为模型量化和部署决策提供了参考，并为推进量化技术奠定了基础。

    This study explores the quantisation-aware training (QAT) on time series Transformer models. We propose a novel adaptive quantisation scheme that dynamically selects between symmetric and asymmetric schemes during the QAT phase. Our approach demonstrates that matching the quantisation scheme to the real data distribution can reduce computational overhead while maintaining acceptable precision. Moreover, our approach is robust when applied to real-world data and mixed-precision quantisation, where most objects are quantised to 4 bits. Our findings inform model quantisation and deployment decisions while providing a foundation for advancing quantisation techniques.
    
[^42]: GET: 用于事件视觉的团体事件变换器

    GET: Group Event Transformer for Event-Based Vision. (arXiv:2310.02642v1 [cs.CV])

    [http://arxiv.org/abs/2310.02642](http://arxiv.org/abs/2310.02642)

    GET提出了一种用于事件视觉的新型团体事件变换器，它能够将事件的时间和极性与空间信息解耦，通过事件双自注意块和团体标记聚合模块实现了在空间和时间-极性领域的特征通信和整合。

    

    事件相机是一种新型的神经形态传感器，备受关注。现有的基于事件的骨干主要依赖于基于图像的设计来提取从事件转换而来的空间信息，忽视了时间和极性等重要的事件属性。为了解决这个问题，我们提出了一种用于事件视觉的新型团体视觉变换器骨干，名为团体事件变换器(GET)，它能够在特征提取过程中将时间和极性从空间信息中解耦出来。具体而言，我们首先提出了GET的新事件表示方式，名为团体标记(Group Token)，它根据时间戳和极性对异步事件进行分组。然后，GET应用事件双自注意块和团体标记聚合模块，在空间和时间-极性领域中促进有效的特征通信和整合。在此之后，GET可以通过不同的下游任务进行集成。

    Event cameras are a type of novel neuromorphic sen-sor that has been gaining increasing attention. Existing event-based backbones mainly rely on image-based designs to extract spatial information within the image transformed from events, overlooking important event properties like time and polarity. To address this issue, we propose a novel Group-based vision Transformer backbone for Event-based vision, called Group Event Transformer (GET), which de-couples temporal-polarity information from spatial infor-mation throughout the feature extraction process. Specifi-cally, we first propose a new event representation for GET, named Group Token, which groups asynchronous events based on their timestamps and polarities. Then, GET ap-plies the Event Dual Self-Attention block, and Group Token Aggregation module to facilitate effective feature commu-nication and integration in both the spatial and temporal-polarity domains. After that, GET can be integrated with different downstream tasks by con
    
[^43]: 弹性不变神经网络及其在变形图像恢复和分析中的应用

    Deformation-Invariant Neural Network and Its Applications in Distorted Image Restoration and Analysis. (arXiv:2310.02641v1 [cs.CV])

    [http://arxiv.org/abs/2310.02641](http://arxiv.org/abs/2310.02641)

    本文提出了一种弹性不变神经网络（DINN）用于处理受到几何形变影响的图像的图像处理任务。DINN通过融入拟保形变换网络（QCTN）来输出一致的潜在特征，使得具有相同原始对象或场景的几何形变图像能够更接近自然或良好图像分布。

    

    受到几何形变影响的图像对于目标识别等图像处理和计算机视觉任务来说是一个重要的挑战。基于深度学习的图像模型通常无法对几何形变图像给出准确的性能。本文中，我们提出了一种弹性不变神经网络（DINN），用于解决几何形变图像的图像处理任务。DINN为几何形变图像输出一致的潜在特征，这些图像具有相同的原始对象或场景。DINN的思想是将一个简单的组件，称为拟保形变换网络（QCTN），融入到其他现有的深度网络中进行图像处理任务。QCTN是一个深度神经网络，它输出一个拟保形映射，可以将几何形变的图像转换为更接近自然或良好图像分布的改进版本。它首先输出一个贝尔特拉密系数，用于衡量拟保形映射的效果。

    Images degraded by geometric distortions pose a significant challenge to imaging and computer vision tasks such as object recognition. Deep learning-based imaging models usually fail to give accurate performance for geometrically distorted images. In this paper, we propose the deformation-invariant neural network (DINN), a framework to address the problem of imaging tasks for geometrically distorted images. The DINN outputs consistent latent features for images that are geometrically distorted but represent the same underlying object or scene. The idea of DINN is to incorporate a simple component, called the quasiconformal transformer network (QCTN), into other existing deep networks for imaging tasks. The QCTN is a deep neural network that outputs a quasiconformal map, which can be used to transform a geometrically distorted image into an improved version that is closer to the distribution of natural or good images. It first outputs a Beltrami coefficient, which measures the quasiconf
    
[^44]: 强化学习基础：朝向具有基础先验辅助的具身通用智能体

    Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance. (arXiv:2310.02635v1 [cs.RO])

    [http://arxiv.org/abs/2310.02635](http://arxiv.org/abs/2310.02635)

    本研究提出了一种基于具身基础先验的基础强化学习框架，通过加速训练过程来提高样本效率。

    

    最近人们已经表明，从互联网规模的数据中进行大规模预训练是构建通用模型的关键，正如在NLP中所见。为了构建具身通用智能体，我们和许多其他研究者假设这种基础先验也是不可或缺的组成部分。然而，目前尚不清楚如何以适当的具体形式表示这些具身基础先验，以及它们应该如何在下游任务中使用。在本文中，我们提出了一组直观有效的具身先验，包括基础策略、价值和成功奖励。所提出的先验是基于目标条件的MDP。为了验证其有效性，我们实例化了一个由这些先验辅助的演员-评论家方法，称之为基础演员-评论家（FAC）。我们将我们的框架命名为基础强化学习（FRL），因为它完全依赖于具身基础先验来进行探索、学习和强化。FRL的好处有三个。(1)样本效率高。通过基础先验加速训练过程，减少样本使用量。

    Recently, people have shown that large-scale pre-training from internet-scale data is the key to building generalist models, as witnessed in NLP. To build embodied generalist agents, we and many other researchers hypothesize that such foundation prior is also an indispensable component. However, it is unclear what is the proper concrete form to represent those embodied foundation priors and how they should be used in the downstream task. In this paper, we propose an intuitive and effective set of embodied priors that consist of foundation policy, value, and success reward. The proposed priors are based on the goal-conditioned MDP. To verify their effectiveness, we instantiate an actor-critic method assisted by the priors, called Foundation Actor-Critic (FAC). We name our framework as Foundation Reinforcement Learning (FRL), since it completely relies on embodied foundation priors to explore, learn and reinforce. The benefits of FRL are threefold. (1) Sample efficient. With foundation p
    
[^45]: 基于改进的Aitchison-Aitken函数贝叶斯优化的组合爆炸决策树的多规则挖掘算法

    Multi-rules mining algorithm for combinatorially exploded decision trees with modified Aitchison-Aitken function-based Bayesian optimization. (arXiv:2310.02633v1 [cs.LG])

    [http://arxiv.org/abs/2310.02633](http://arxiv.org/abs/2310.02633)

    该论文提出了两种算法(MAABO-MT和GS-MRM)，分别在所有可能的树中高性能估计构建树，仅提取可靠且不相似的规则。

    

    决策树具有易于解释的优点，因为它们允许根据if-then规则对输入数据进行分类。然而，决策树是由一个算法构建的，该算法通过最少的规则实现清晰的分类，而无论数据中是否存在各种潜在规则，都只能提取最小的规则。确实存在使用随机选择的特征子集构建多棵决策树的方法。然而，可以构建的树的数量仍然在相同的数量级，因为特征子集的数量是一个组合性爆炸。此外，当构建多棵树时，会生成许多规则，其中有几个是不可靠和/或非常相似的。因此，我们提出了“MAABO-MT”和“GS-MRM”算法，它们分别在所有可能的树中高性能估计构建树，并仅提取可靠且不相似的规则。

    Decision trees offer the benefit of easy interpretation because they allow the classification of input data based on if--then rules. However, as decision trees are constructed by an algorithm that achieves clear classification with minimum necessary rules, the trees possess the drawback of extracting only minimum rules, even when various latent rules exist in data. Approaches that construct multiple trees using randomly selected feature subsets do exist. However, the number of trees that can be constructed remains at the same scale because the number of feature subsets is a combinatorial explosion. Additionally, when multiple trees are constructed, numerous rules are generated, of which several are untrustworthy and/or highly similar. Therefore, we propose "MAABO-MT" and "GS-MRM" algorithms that strategically construct trees with high estimation performance among all possible trees with small computational complexity and extract only reliable and non-similar rules, respectively. Experi
    
[^46]: 关于多Agent系统中可量化的可观察性分析

    On Quantified Observability Analysis in Multiagent Systems. (arXiv:2310.02614v1 [cs.AI])

    [http://arxiv.org/abs/2310.02614](http://arxiv.org/abs/2310.02614)

    本文针对多Agent系统提出了一种可量化的可观察性分析方法。通过引入不透明度概念和时间逻辑oPATL，我们可以定量分析系统行为对观察者的信息透明度，从而为操作员辅助决策提供帮助。

    

    在多Agent系统中，Agent对系统行为的观察可能改善整体团队的表现，但也可能向观察者泄露敏感信息。因此，可量化的可观察性分析可以帮助操作员在实践中通过观察来优化性能效果和信息暴露之间的关系，以辅助决策。本文提出了一种在多Agent系统中定量分析可观察性属性的新方法。我们将不透明度的概念应用于正式表达建模为部分可观测多Agent系统的可观察性特征。我们提出了一种用于推理Agent可观察性的时间逻辑oPATL，并开发了验证技术来定量分析这些属性。我们将该方法作为PRISM模型检测器的扩展实现。

    In multiagent systems (MASs), agents' observation upon system behaviours may improve the overall team performance, but may also leak sensitive information to an observer. A quantified observability analysis can thus be useful to assist decision-making in MASs by operators seeking to optimise the relationship between performance effectiveness and information exposure through observations in practice. This paper presents a novel approach to quantitatively analysing the observability properties in MASs. The concept of opacity is applied to formally express the characterisation of observability in MASs modelled as partially observable multiagent systems. We propose a temporal logic oPATL to reason about agents' observability with quantitative goals, which capture the probability of information transparency of system behaviours to an observer, and develop verification techniques for quantitatively analysing such properties. We implement the approach as an extension of the PRISM model checke
    
[^47]: 用于电网拓扑优化的多智能体强化学习

    Multi-Agent Reinforcement Learning for Power Grid Topology Optimization. (arXiv:2310.02605v1 [cs.LG])

    [http://arxiv.org/abs/2310.02605](http://arxiv.org/abs/2310.02605)

    本文提出了一种用于电网拓扑优化的分层多智能体强化学习（MARL）框架，有效处理随着网络增长而扩大的大型行动空间。实验表明，该框架在性能上与单一智能体强化学习方法相当，并比较了不同的RL算法和不同的高阶智能体策略。

    

    近年来，面临着能源需求增加和风能、太阳能等不可预测可再生能源的挑战，操作电网成为一个问题。强化学习在管理这些网络中显示出潜力，通过总线和线路切换等拓扑操作，但对于随着网络增长而扩大的大型行动空间的高效处理至关重要。本文提出了一种针对这种扩展行动空间的分层多智能体强化学习（MARL）框架，利用电网固有的分层特性。实验结果表明，MARL框架与单一智能体强化学习方法在性能上具有竞争力。我们还比较了不同的RL算法和不同的高阶智能体策略。

    Recent challenges in operating power networks arise from increasing energy demands and unpredictable renewable sources like wind and solar. While reinforcement learning (RL) shows promise in managing these networks, through topological actions like bus and line switching, efficiently handling large action spaces as networks grow is crucial. This paper presents a hierarchical multi-agent reinforcement learning (MARL) framework tailored for these expansive action spaces, leveraging the power grid's inherent hierarchical nature. Experimental results indicate the MARL framework's competitive performance with single-agent RL methods. We also compare different RL algorithms for lower-level agents alongside different policies for higher-order agents.
    
[^48]: MagicDrive: 多样化的三维几何控制下的街景生成

    MagicDrive: Street View Generation with Diverse 3D Geometry Control. (arXiv:2310.02601v1 [cs.CV])

    [http://arxiv.org/abs/2310.02601](http://arxiv.org/abs/2310.02601)

    MagicDrive是一个新颖的街景生成框架，通过提供多样化的三维几何控制，包括相机姿态、道路地图和三维边界框，以及文本描述，实现了高保真度的街景合成，并捕捉了细致的三维几何信息。

    

    最近扩散模型的进展显著提高了具有2D控制的数据合成。然而，在街景生成中精确的三维控制在三维感知任务中至关重要，但仍然难以实现。具体来说，使用鸟瞰图作为主要条件常常导致几何控制（如高度）方面的挑战，影响物体形状、遮挡模式和道路表面高程等对感知数据合成至关重要的因素，特别是对于三维物体检测任务而言。在本文中，我们介绍了MagicDrive，这是一个新颖的街景生成框架，提供了多样化的三维几何控制，包括相机姿态、道路地图和三维边界框，以及通过定制的编码策略实现的文本描述。此外，我们的设计还采用了跨视图注意力模块，确保多个相机视图之间的一致性。通过MagicDrive，我们实现了高保真度的街景合成，捕捉到了精细的三维几何信息。

    Recent advancements in diffusion models have significantly enhanced the data synthesis with 2D control. Yet, precise 3D control in street view generation, crucial for 3D perception tasks, remains elusive. Specifically, utilizing Bird's-Eye View (BEV) as the primary condition often leads to challenges in geometry control (e.g., height), affecting the representation of object shapes, occlusion patterns, and road surface elevations, all of which are essential to perception data synthesis, especially for 3D object detection tasks. In this paper, we introduce MagicDrive, a novel street view generation framework offering diverse 3D geometry controls, including camera poses, road maps, and 3D bounding boxes, together with textual descriptions, achieved through tailored encoding strategies. Besides, our design incorporates a cross-view attention module, ensuring consistency across multiple camera views. With MagicDrive, we achieve high-fidelity street-view synthesis that captures nuanced 3D ge
    
[^49]: 基于ModelOps的智能医疗知识提取框架

    A ModelOps-based Framework for Intelligent Medical Knowledge Extraction. (arXiv:2310.02593v1 [cs.AI])

    [http://arxiv.org/abs/2310.02593](http://arxiv.org/abs/2310.02593)

    提出了一个基于ModelOps的智能医疗知识提取框架，通过低代码系统实现模型选择、训练、评估和优化，提供了便利给研究人员开发和部署模型的方式。

    

    从医疗文本中提取医疗知识可以增强医疗知识图谱构建和临床决策等下游任务的效果。然而，目前知识提取模型的构建和应用缺乏自动化、可重用性和统一管理，这给研究人员带来了低效，也给非AI专家如医生等利用知识提取带来了高门槛。为解决这些问题，我们提出了一个基于ModelOps的智能医疗知识提取框架，该框架为模型选择、训练、评估和优化提供了低代码系统。具体而言，该框架包括基于多层回调函数的数据集抽象机制和可重用的模型训练、监控和管理机制。我们还提出了一种基于数据集相似性的模型推荐方法，帮助用户快速找到适合给定数据集的潜在模型。我们的框架为研究人员开发模型和部署模型提供了便利。

    Extracting medical knowledge from healthcare texts enhances downstream tasks like medical knowledge graph construction and clinical decision-making. However, the construction and application of knowledge extraction models lack automation, reusability and unified management, leading to inefficiencies for researchers and high barriers for non-AI experts such as doctors, to utilize knowledge extraction. To address these issues, we propose a ModelOps-based intelligent medical knowledge extraction framework that offers a low-code system for model selection, training, evaluation and optimization. Specifically, the framework includes a dataset abstraction mechanism based on multi-layer callback functions, a reusable model training, monitoring and management mechanism. We also propose a model recommendation method based on dataset similarity, which helps users quickly find potentially suitable models for a given dataset. Our framework provides convenience for researchers to develop models and 
    
[^50]: 关于图神经网络中表达位置编码的稳定性

    On the Stability of Expressive Positional Encodings for Graph Neural Networks. (arXiv:2310.02579v1 [cs.LG])

    [http://arxiv.org/abs/2310.02579](http://arxiv.org/abs/2310.02579)

    本研究针对图神经网络中使用拉普拉斯特征向量作为位置编码面临的非唯一性和不稳定性问题，提出了稳定且表达丰富的位置编码方法（SPE），该方法通过利用特征值对特征空间进行"软分割"，在未见过的图结构上表现出良好的泛化能力。

    

    设计有效的图位置编码对构建强大的图转换器和增强消息传递图神经网络非常关键。尽管广泛使用，使用拉普拉斯特征向量作为位置编码面临两个根本性挑战：（1）\emph{非唯一性}：同一拉普拉斯矩阵存在许多不同的特征分解，以及（2）\emph{不稳定性}：对拉普拉斯矩阵的微小扰动可能导致完全不同的特征空间，从而导致位置编码的不可预测性变化。尽管有很多尝试解决非唯一性的方法，但大多数方法忽视了稳定性，导致在未见过的图结构上表现不佳。我们发现，不稳定性的原因是特征空间的"硬分割"。因此，我们引入了稳定且表达丰富的位置编码（SPE），这是一种用于处理特征向量的架构，利用特征值将特征空间进行"软分割"。SPE是首个（1）可证明稳定的架构，以及（2）普适地提升图结构泛化性能的架构。

    Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) \emph{Non-uniqueness}: there are many different eigendecompositions of the same Laplacian, and (2) \emph{Instability}: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding.  Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a "hard partition" of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to "softly partition" eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally exp
    
[^51]: 为了某事而站立，否则就会为一切垮掉：使用立场感知的图神经网络预测谣言传播

    Stand for Something or Fall for Everything: Predict Misinformation Spread with Stance-Aware Graph Neural Networks. (arXiv:2310.02568v1 [cs.SI])

    [http://arxiv.org/abs/2310.02568](http://arxiv.org/abs/2310.02568)

    使用立场感知的图神经网络（stance-aware GNN）预测谣言传播。与没有用户立场的GNN相比，该模型在真实数据集上的表现优于32.65%的基准模型。注意权重表明用户的反对立场对邻居行为的影响更大，可以作为社会纠正措施阻止谣言传播。

    

    尽管社交媒体平台上流行的谣言传播已成为迫切的挑战，但现有的平台干预措施在遏制其传播方面显示出有限的成功。在本研究中，我们提出了一种立场感知的图神经网络（stance-aware GNN），利用用户的立场主动预测谣言传播。由于不同用户的立场可以形成独特的回声室，我们在立场感知的GNN中定制了四个信息传递路径，而可训练的注意权重通过突出显示每个结构的重要性来提供可解释性。在一个真实数据集上进行评估，立场感知的GNN的表现优于基准模型32.65％，并且超过了没有用户立场的先进GNN 4.69％以上。此外，注意权重表明，用户的反对立场对邻居行为的影响高于支持立场，这起到了社会纠正作用，阻止了谣言的传播。总体而言，我们的研究提供了一种有效的预测模型。

    Although pervasive spread of misinformation on social media platforms has become a pressing challenge, existing platform interventions have shown limited success in curbing its dissemination. In this study, we propose a stance-aware graph neural network (stance-aware GNN) that leverages users' stances to proactively predict misinformation spread. As different user stances can form unique echo chambers, we customize four information passing paths in stance-aware GNN, while the trainable attention weights provide explainability by highlighting each structure's importance. Evaluated on a real-world dataset, stance-aware GNN outperforms benchmarks by 32.65% and exceeds advanced GNNs without user stance by over 4.69%. Furthermore, the attention weights indicate that users' opposition stances have a higher impact on their neighbors' behaviors than supportive ones, which function as social correction to halt misinformation propagation. Overall, our study provides an effective predictive model
    
[^52]: 使用大型语言模型改进自动VQA评估

    Improving Automatic VQA Evaluation Using Large Language Models. (arXiv:2310.02567v1 [cs.CV])

    [http://arxiv.org/abs/2310.02567](http://arxiv.org/abs/2310.02567)

    提出使用大型语言模型改进自动视觉问答（VQA）评估的方法，将VQA评估格式化为回答评分任务，通过指令调整大型语言模型在准确度上评分候选答案，证明该方法与人类判断相关性优于现有度量方法。

    

    在提出视觉问答（VQA）任务8年后，准确率仍然是自动评估的主要指标。在IID评估设置中，VQA准确度一直很有效。然而，我们的社区正在转向开放式生成模型和OOD评估。在这种新的范式中，现有的VQA准确度指标过于严格，低估了VQA系统的性能。因此，有必要开发更强大的自动VQA度量，作为人类判断的代理。在这项工作中，我们提出利用指令调整大型语言模型（LLM）的上下文学习能力来构建更好的VQA度量。我们将VQA评估格式化为一个回答评分任务，即指令调整的大型语言模型被指示根据一组参考答案评分候选答案的准确性。我们证明所提出的度量与人类判断相关性优于现有度量在几个VQA模型和基准测试中。

    8 years after the visual question answering (VQA) task was proposed, accuracy remains the primary metric for automatic evaluation. VQA Accuracy has been effective so far in the IID evaluation setting. However, our community is undergoing a shift towards open-ended generative models and OOD evaluation. In this new paradigm, the existing VQA Accuracy metric is overly stringent and underestimates the performance of VQA systems. Thus, there is a need to develop more robust automatic VQA metrics that serve as a proxy for human judgment. In this work, we propose to leverage the in-context learning capabilities of instruction-tuned large language models (LLMs) to build a better VQA metric. We formulate VQA evaluation as an answer-rating task where the LLM is instructed to score the accuracy of a candidate answer given a set of reference answers. We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks. We ho
    
[^53]: 通过注意力转换网络改进鼓机器人

    Improving Drumming Robot Via Attention Transformer Network. (arXiv:2310.02565v1 [cs.RO])

    [http://arxiv.org/abs/2310.02565](http://arxiv.org/abs/2310.02565)

    本文提出了一种使用注意力转换网络的改进鼓机器人，可以自动完成音乐转录，并帮助机器人提升鼓分类性能。

    

    机器人技术在现今社会得到广泛应用，在农业、制造业和娱乐等各个领域取得了巨大进展。本文着重研究娱乐领域中的鼓机器人。为此，我们引入了一种改进的鼓机器人，基于注意力机制的热门视觉转换网络，可以自动完成音乐转录。通过配备注意力转换网络，我们的方法可以有效处理顺序音频嵌入输入，并建模其全局长程依赖关系。大量实验结果表明，改进的算法可以帮助鼓机器人提升鼓分类性能，也可以帮助机器人享受各种智能应用和服务。

    Robotic technology has been widely used in nowadays society, which has made great progress in various fields such as agriculture, manufacturing and entertainment. In this paper, we focus on the topic of drumming robots in entertainment. To this end, we introduce an improving drumming robot that can automatically complete music transcription based on the popular vision transformer network based on the attention mechanism. Equipped with the attention transformer network, our method can efficiently handle the sequential audio embedding input and model their global long-range dependencies. Massive experimental results demonstrate that the improving algorithm can help the drumming robot promote drum classification performance, which can also help the robot to enjoy a variety of smart applications and services.
    
[^54]: zkFL: 基于零知识证明的联邦学习梯度聚合

    zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning. (arXiv:2310.02554v1 [cs.AI])

    [http://arxiv.org/abs/2310.02554](http://arxiv.org/abs/2310.02554)

    zkFL是一种基于零知识证明的联邦学习梯度聚合方法，通过提供每轮的证明来解决协调者恶意行为的问题。

    

    联邦学习是一种机器学习范式，使多个分散的客户端在中央协调者的组织下共同训练一个模型。传统的联邦学习解决方案依赖于对中央协调者的信任，它以公平诚实的方式形成客户端的群体。然而，在现实中，恶意的协调者可能会放弃并替换客户端的训练模型，或者发动虚假客户端的肆意攻击。这种恶意行为让协调者在联邦学习环境中拥有更多控制客户端和决定最终训练结果的权力。本文介绍了zkFL，它利用零知识证明(ZKPs)来解决训练模型聚合过程中的恶意协调者问题。为了保证正确的聚合结果，协调者需要每轮提供一个证明。这个证明可以向客户端证明协调者忠实执行预期行为。为了进一步保护客户端隐私和数据安全，我们还引入了差分隐私机制，并对zkFL进行了实验评估。

    Federated Learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. Traditional FL solutions rely on the trust assumption of the centralized aggregator, which forms cohorts of clients in a fair and honest manner. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or launch Sybil attacks to insert fake clients. Such malicious behaviors give the aggregator more power to control clients in the FL setting and determine the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator needs to provide a proof per round. The proof can demonstrate to the clients that the aggregator executes the intended behavior faithfully. To further r
    
[^55]: Auto-FP:自动化特征预处理在表格数据上的实验研究

    Auto-FP: An Experimental Study of Automated Feature Preprocessing for Tabular Data. (arXiv:2310.02540v1 [cs.LG])

    [http://arxiv.org/abs/2310.02540](http://arxiv.org/abs/2310.02540)

    本文研究了如何自动化表格数据的特征预处理（Auto-FP），将其建模为超参数优化或神经网络架构搜索问题，并扩展了各种算法来解决Auto-FP问题。

    

    传统的机器学习模型，如线性模型和基于树的模型，在工业中被广泛使用。这些模型对数据分布敏感，因此特征预处理是确保模型质量良好的关键步骤。手动构建特征预处理流程很具挑战性，因为数据科学家需要在选择哪些预处理器以及以什么顺序组合它们方面作出困难的决策。在本文中，我们研究了如何自动化表格数据的特征预处理（Auto-FP）。由于搜索空间较大，暴力解决方案代价太高。为了解决这个挑战，我们有趣地观察到Auto-FP可以被建模为超参数优化（HPO）或神经网络架构搜索（NAS）问题。这个观察使我们能够扩展各种HPO和NAS算法来解决Auto-FP问题。我们进行了全面的评估和分析，共进行了15个...

    Classical machine learning models, such as linear models and tree-based models, are widely used in industry. These models are sensitive to data distribution, thus feature preprocessing, which transforms features from one distribution to another, is a crucial step to ensure good model quality. Manually constructing a feature preprocessing pipeline is challenging because data scientists need to make difficult decisions about which preprocessors to select and in which order to compose them. In this paper, we study how to automate feature preprocessing (Auto-FP) for tabular data. Due to the large search space, a brute-force solution is prohibitively expensive. To address this challenge, we interestingly observe that Auto-FP can be modelled as either a hyperparameter optimization (HPO) or a neural architecture search (NAS) problem. This observation enables us to extend a variety of HPO and NAS algorithms to solve the Auto-FP problem. We conduct a comprehensive evaluation and analysis of 15 
    
[^56]: MIDDAG：新闻走向何方？通过社区级信息路径研究信息传播

    MIDDAG: Where Does Our News Go? Investigating Information Diffusion via Community-Level Information Pathways. (arXiv:2310.02529v1 [cs.SI])

    [http://arxiv.org/abs/2310.02529](http://arxiv.org/abs/2310.02529)

    MIDDAG是一个交互式系统，通过可视化社交媒体上由COVID-19相关新闻触发的信息传播路径，提供全面的洞察力，并能构建用户社区和预测信息传播，从而追踪和理解信息的传播方式。

    

    我们提出了MIDDAG，一个直观、交互式的系统，可以可视化由COVID-19相关新闻文章触发的社交媒体信息传播路径，并提供包括用户/社区易感性水平、以及信息传播过程中引发的事件和广大群众的热门观点的全面洞察。除了发现用户之间的信息流模式，我们还构建了用户之间的社区，并开发了传播预测能力，从而实现对高层次信息传播方式的追踪和理解。

    We present MIDDAG, an intuitive, interactive system that visualizes the information propagation paths on social media triggered by COVID-19-related news articles accompanied by comprehensive insights including user/community susceptibility level, as well as events and popular opinions raised by the crowd while propagating the information. Besides discovering information flow patterns among users, we construct communities among users and develop the propagation forecasting capability, enabling tracing and understanding of how information is disseminated at a higher level.
    
[^57]: CITING: 大型语言模型通过课程设计进行指导调优

    CITING: Large Language Models Create Curriculum for Instruction Tuning. (arXiv:2310.02527v1 [cs.CL])

    [http://arxiv.org/abs/2310.02527](http://arxiv.org/abs/2310.02527)

    本文提出了一种利用大型语言模型作为指导教师来训练学生语言模型的方法，通过设计课程来进行指导调优，称为CITING。该方法通过教师模型制定评分标准，学生模型通过遵循评分标准和自我纠正进行学习。该方法可以解决手工制作指导数据集和人工对齐的瓶颈问题。

    

    最近大型语言模型（LLMs）的进展是通过指导调优和人工对齐的结合实现的。然而，构建手工制作的指导数据集和进行人工对齐成为了扩展LLMs开发的瓶颈。在本文中，我们利用AI模型代替人类作为教师来训练学生LLMs的方法。我们的方法灵感来自于人类学生通过遵循评分标准和从导师提供的修改中学习来提高写作技巧。具体来说，我们使用教师LLM来为学生LLM的指导调优创建课程，即Curriculum Instruction TunING (CITING)。该方法包括两个主要步骤：（1）教师LLM制定评估各种类型问题答案的评分标准；（2）学生LLM学习遵循评分标准并通过教师的修改进行自我纠正。我们进一步迭代进行这个过程。

    The recent advancement of large language models (LLMs) has been achieved through a combo of instruction tuning and human alignment. However, building manually crafted instruction datasets and performing human alignment become the bottleneck for scaling the development of LLMs. In this paper, we exploit the idea of leveraging AI models in lieu of humans as the teacher to train student LLMs. Our method is inspired by how human students refine their writing skills by following the rubrics and learning from the revisions offered by their tutors. Specifically, we employ a teacher LLM to create a curriculum for instruction tuning of the student LLM, namely Curriculum Instruction TunING (CITING). It encompasses two main steps: (1) the teacher LLM crafts the rubrics for evaluating the answers corresponding to various types of questions, and (2) the student LLM learns to follow the rubrics and perform self-correction from the revision made by the teacher. We further iteratively carry out it to 
    
[^58]: 联邦条件随机优化

    Federated Conditional Stochastic Optimization. (arXiv:2310.02524v1 [cs.LG])

    [http://arxiv.org/abs/2310.02524](http://arxiv.org/abs/2310.02524)

    本文提出了一种新的联邦条件随机优化算法(FCSG)，针对联邦学习中的非凸条件随机优化问题，通过设计加速算法(Acc-FCSG-M)来实现最佳的样本和通信复杂度。

    

    条件随机优化在机器学习任务中广泛应用，例如不变学习、AUPRC最大化和元学习。随着这些应用中对使用大规模分布式数据进行模型训练的需求增加，对于高效通信的分布式优化算法，例如联邦学习算法的需求也越来越大。本文考虑联邦学习中的非凸条件随机优化，并提出了第一个联邦条件随机优化算法(FCSG)，其中包括条件随机梯度估计器和基于动量的算法(FCSG-M)。为了达到单机设定下的下界复杂度，我们通过方差减少设计了加速算法(Acc-FCSG-M)以实现最佳的样本和通信复杂度。与现有的FL中MAML的优化分析相比，联邦条件随机优化考虑了样本的。

    Conditional stochastic optimization has found applications in a wide range of machine learning tasks, such as invariant learning, AUPRC maximization, and meta-learning. As the demand for training models with large-scale distributed data grows in these applications, there is an increasing need for communication-efficient distributed optimization algorithms, such as federated learning algorithms. This paper considers the nonconvex conditional stochastic optimization in federated learning and proposes the first federated conditional stochastic optimization algorithm (FCSG) with a conditional stochastic gradient estimator and a momentum-based algorithm (FCSG-M). To match the lower bound complexity in the single-machine setting, we design an accelerated algorithm (Acc-FCSG-M) via the variance reduction to achieve the best sample and communication complexity. Compared with the existing optimization analysis for MAML in FL, federated conditional stochastic optimization considers the sample of
    
[^59]: MedDiffusion: 通过基于扩散的数据增强提升健康风险预测

    MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data Augmentation. (arXiv:2310.02520v1 [cs.LG])

    [http://arxiv.org/abs/2310.02520](http://arxiv.org/abs/2310.02520)

    本文介绍了一种名为MedDiffusion的新型、端到端的扩散式风险预测模型，通过基于扩散的数据增强，提升了健康风险预测的效果。

    

    健康风险预测是医学领域中基于预测建模的基本任务之一，旨在利用历史电子健康记录（EHR）来预测患者未来可能面临的潜在健康风险。研究人员已经开发了几种风险预测模型来处理EHR数据的独特挑战，例如其序列特性，高维度和固有噪音。这些模型已经取得了令人印象深刻的结果。然而，一个影响它们有效性的关键问题是数据不足。为了缓解这个问题，引入了各种数据生成和增强方法，通过学习底层数据分布来扩大训练数据集的大小。然而，这些方法的性能往往受到任务无关设计的限制。为了解决这些缺点，本文引入了一种新颖的端到端扩散式风险预测模型MedDiffusion，来增强风险预测的性能。

    Health risk prediction is one of the fundamental tasks under predictive modeling in the medical domain, which aims to forecast the potential health risks that patients may face in the future using their historical Electronic Health Records (EHR). Researchers have developed several risk prediction models to handle the unique challenges of EHR data, such as its sequential nature, high dimensionality, and inherent noise. These models have yielded impressive results. Nonetheless, a key issue undermining their effectiveness is data insufficiency. A variety of data generation and augmentation methods have been introduced to mitigate this issue by expanding the size of the training data set through the learning of underlying data distributions. However, the performance of these methods is often limited due to their task-unrelated design. To address these shortcomings, this paper introduces a novel, end-to-end diffusion-based risk prediction model, named MedDiffusion. It enhances risk predicti
    
[^60]: 使用视觉-语言转换器进行主动的人机交互

    Proactive Human-Robot Interaction using Visuo-Lingual Transformers. (arXiv:2310.02506v1 [cs.RO])

    [http://arxiv.org/abs/2310.02506](http://arxiv.org/abs/2310.02506)

    本文提出了一种使用视觉-语言转换器进行主动的人机交互的方法，通过提取潜在的视觉-语言线索来推断上下文，识别和主动预测用户意图要实现的目标。

    

    人类具有提取潜在的视觉-语言线索并通过人机交互推断上下文的能力。在合作过程中，这使得能够主动预测一系列任务的潜在意图。相比之下，与人类合作的机器人智能地遵循基本指令完成任务，或者在朝着目标完成的过程中使用特定的手工触发器来启动主动合作。赋予这样的机器人思考最终目标和主动建议中间任务的能力将为人机合作提供一种更直观的方法。为此，我们提出了一种基于学习的方法，利用场景中的视觉线索、用户的语言命令以及先前的物体-物体交互知识来识别和主动预测用户意图要实现的目标。具体地，我们提出了ViLing-MMT，一种基于视觉-语言多模态转换器的架构，它捕捉到了中间目标和目标目标之间的交互关系。

    Humans possess the innate ability to extract latent visuo-lingual cues to infer context through human interaction. During collaboration, this enables proactive prediction of the underlying intention of a series of tasks. In contrast, robotic agents collaborating with humans naively follow elementary instructions to complete tasks or use specific hand-crafted triggers to initiate proactive collaboration when working towards the completion of a goal. Endowing such robots with the ability to reason about the end goal and proactively suggest intermediate tasks will engender a much more intuitive method for human-robot collaboration. To this end, we propose a learning-based method that uses visual cues from the scene, lingual commands from a user and knowledge of prior object-object interaction to identify and proactively predict the underlying goal the user intends to achieve. Specifically, we propose ViLing-MMT, a vision-language multimodal transformer-based architecture that captures int
    
[^61]: 结合计划识别和语言反馈改进人类意图推理

    Improved Inference of Human Intent by Combining Plan Recognition and Language Feedback. (arXiv:2310.02462v1 [cs.RO])

    [http://arxiv.org/abs/2310.02462](http://arxiv.org/abs/2310.02462)

    该论文提出了一种结合计划识别和语言反馈的方法，通过对话实现对人类意图的改进推理，使机器人能够有效地与人类进行交互，并纠正用户计划的偏离和次优行动。

    

    会话辅助机器人能够帮助人们，尤其是那些认知能力受损的人，完成各种任务，如烹饪饭菜，进行锻炼，或操作机器。然而，为了有效地与人类进行交互，机器人必须能够从观察到的人类行为中识别人类的计划和目标，即使用户的行为不够优化。目前的计划和目标识别研究通常使用层次任务网络来建模人类行为者，但是这些技术不足以通过自然交互的方式实现用户参与。此外，它们也没有机制来让用户，尤其是那些认知能力受损的人，知道他们原始计划的偏离或朝着目标采取的任何次优行动。我们提出了一种新的框架，通过对话来实现部分可观察领域中的计划和目标识别，使机器人能够纠正对人类的信念。

    Conversational assistive robots can aid people, especially those with cognitive impairments, to accomplish various tasks such as cooking meals, performing exercises, or operating machines. However, to interact with people effectively, robots must recognize human plans and goals from noisy observations of human actions, even when the user acts sub-optimally. Previous works on Plan and Goal Recognition (PGR) as planning have used hierarchical task networks (HTN) to model the actor/human. However, these techniques are insufficient as they do not have user engagement via natural modes of interaction such as language. Moreover, they have no mechanisms to let users, especially those with cognitive impairments, know of a deviation from their original plan or about any sub-optimal actions taken towards their goal. We propose a novel framework for plan and goal recognition in partially observable domains -- Dialogue for Goal Recognition (D4GR) enabling a robot to rectify its belief in human pro
    
[^62]: 从偏好中学习最佳优势，并将其误解为奖励

    Learning Optimal Advantage from Preferences and Mistaking it for Reward. (arXiv:2310.02456v1 [cs.LG])

    [http://arxiv.org/abs/2310.02456](http://arxiv.org/abs/2310.02456)

    本文研究了从人类偏好中学习奖励函数的算法，并发现实际上学到的是最佳优势函数而不是奖励函数。这种错误的使用方式虽然不特别有害，但与正确的贪婪最大化最佳优势函数相比仍不够理想。

    

    我们考虑从人类对轨迹片段对的偏好中学习奖励函数的算法，这在从人类反馈中进行强化学习（RLHF）中使用。最近的工作假设人类偏好仅基于这些片段中积累的奖励或其部分回报。最近的工作对这一假设的有效性提出了怀疑，并提出了一种基于遗憾的替代偏好模型。我们研究了当假设偏好是基于部分回报而实际上来自遗憾时的后果。我们认为学到的函数是最佳优势函数$\hat{A^*_r}$的近似，而不是奖励函数。我们发现，如果解决了特定的陷阱，这种错误假设并不特别有害，结果是一个高度变形的奖励函数。尽管如此，这种错误使用$\hat{A^*_r}$的方式不如适当且更简单的方法——贪婪最大化$\hat{A^*_r}$。

    We consider algorithms for learning reward functions from human preferences over pairs of trajectory segments, as used in reinforcement learning from human feedback (RLHF). Most recent work assumes that human preferences are generated based only upon the reward accrued within those segments, or their partial return. Recent work casts doubt on the validity of this assumption, proposing an alternative preference model based upon regret. We investigate the consequences of assuming preferences are based upon partial return when they actually arise from regret. We argue that the learned function is an approximation of the optimal advantage function, $\hat{A^*_r}$, not a reward function. We find that if a specific pitfall is addressed, this incorrect assumption is not particularly harmful, resulting in a highly shaped reward function. Nonetheless, this incorrect usage of $\hat{A^*_r}$ is less desirable than the appropriate and simpler approach of greedy maximization of $\hat{A^*_r}$. From th
    
[^63]: 低资源语言越狱 GPT-4

    Low-Resource Languages Jailbreak GPT-4. (arXiv:2310.02446v1 [cs.CL])

    [http://arxiv.org/abs/2310.02446](http://arxiv.org/abs/2310.02446)

    通过翻译不安全的英文输入成低资源语言，我们成功绕过了GPT-4的安全机制，并展示了这种跨语言漏洞。这一方法在实验中取得了与甚至超过了最先进的越狱攻击的效果，揭示了低资源语言在AI安全性中的薄弱环节。

    

    人工智能安全培训和大型语言模型（LLM）的红队测试是减少生成不安全内容的措施。我们的工作通过将不安全的英文输入翻译成低资源语言，成功绕过GPT-4的安全机制，并揭示了这些安全机制的跨语言漏洞。在AdvBenchmark中，GPT-4针对不安全的翻译输入进行交互，并且79%的时间内提供了可行的方案，使用户实现其有害目标，这与甚至超过了最先进的越狱攻击的效果相当。其他高/中资源语言的攻击成功率显著较低，这表明跨语言漏洞主要适用于低资源语言。以前，对低资源语言的有限训练主要影响那些使用这些语言的人，造成技术差距。然而，我们的工作突出了一个关键转变：

    AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4's safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift:
    
[^64]: 学习多重约束下局部导航的多样技能

    Learning Diverse Skills for Local Navigation under Multi-constraint Optimality. (arXiv:2310.02440v1 [cs.RO])

    [http://arxiv.org/abs/2310.02440](http://arxiv.org/abs/2310.02440)

    本研究提出了一种约束优化视角的方法，克服了在数据驱动控制中提取多样性行为的挑战。通过对数值函数进行约束，我们能够获得多样的策略。通过引入吸引-排斥奖励项，我们可以进一步控制多样性水平。实验证明了我们方法在局部导航任务上的有效性，并且在真实机器人上展现出多样的灵敏行为。

    

    尽管数据驱动控制在机器人技术中有许多成功的应用，但提取有意义的多样行为仍然是一个挑战。通常情况下，为了实现多样性，需要在任务执行上做出妥协。在许多场景中，任务要求被指定为多个奖励项，每个奖励项都需要不同的权衡。在这项工作中，我们以约束优化视角研究了质量多样性权衡，并展示了如何通过对数值函数进行约束来获得多样的策略，其中数值函数是通过不同的奖励定义的。与先前的工作一致，通过Van der Waals力激发的吸引-排斥奖励项可以实现对多样性水平的进一步控制。我们通过在一个局部导航任务上展示了我们方法的有效性，其中一个四足机器人需要在有限时间内到达目标。最后，我们训练的策略在真实的12-Dof四足机器人Solo12上得到良好的转移，并展现出多样的灵敏行为

    Despite many successful applications of data-driven control in robotics, extracting meaningful diverse behaviors remains a challenge. Typically, task performance needs to be compromised in order to achieve diversity. In many scenarios, task requirements are specified as a multitude of reward terms, each requiring a different trade-off. In this work, we take a constrained optimization viewpoint on the quality-diversity trade-off and show that we can obtain diverse policies while imposing constraints on their value functions which are defined through distinct rewards. In line with previous work, further control of the diversity level can be achieved through an attract-repel reward term motivated by the Van der Waals force. We demonstrate the effectiveness of our method on a local navigation task where a quadruped robot needs to reach the target within a finite horizon. Finally, our trained policies transfer well to the real 12-DoF quadruped robot, Solo12, and exhibit diverse agile behavi
    
[^65]: 基于表征交流的多智能体强化学习用于大规模交通信号控制

    Multi-Agent Reinforcement Learning Based on Representational Communication for Large-Scale Traffic Signal Control. (arXiv:2310.02435v1 [cs.MA])

    [http://arxiv.org/abs/2310.02435](http://arxiv.org/abs/2310.02435)

    本研究提出了一种基于通信的多智能体强化学习框架，用于大规模交通信号控制。该框架允许智能体学习何时和如何进行通信，从而实现良好的协调和性能。

    

    交通信号控制（TSC）是智能交通系统中一个具有挑战性的问题，已经使用多智能体强化学习（MARL）进行解决。尽管对于大规模TSC问题来说，集中式方法通常是不可行的，但分散式方法提供了可扩展性，但也引入了新的挑战，比如部分可观察性。在分散式MARL中，通信起着关键的作用，因为智能体必须通过消息交换信息以更好地理解系统并实现有效协调。深度MARL已经被用来通过可微分的方式学习通信协议从而实现智能体间的交流。然而，许多为TSC提出的深度MARL通信框架允许智能体在任何时候与其他所有智能体进行通信，这可能会增加系统中的噪声并降低整体性能。在本研究中，我们提出了一种基于通信的大规模TSC的MARL框架。我们的框架允许每个智能体学习在何时和如何进行通信，以实现良好的协调和性能。

    Traffic signal control (TSC) is a challenging problem within intelligent transportation systems and has been tackled using multi-agent reinforcement learning (MARL). While centralized approaches are often infeasible for large-scale TSC problems, decentralized approaches provide scalability but introduce new challenges, such as partial observability. Communication plays a critical role in decentralized MARL, as agents must learn to exchange information using messages to better understand the system and achieve effective coordination. Deep MARL has been used to enable inter-agent communication by learning communication protocols in a differentiable manner. However, many deep MARL communication frameworks proposed for TSC allow agents to communicate with all other agents at all times, which can add to the existing noise in the system and degrade overall performance. In this study, we propose a communication-based MARL framework for large-scale TSC. Our framework allows each agent to learn
    
[^66]: 反复神经网络(RNN)机制解释的片段记忆理论

    Episodic Memory Theory for the Mechanistic Interpretation of Recurrent Neural Networks. (arXiv:2310.02430v1 [cs.NE])

    [http://arxiv.org/abs/2310.02430](http://arxiv.org/abs/2310.02430)

    提出了片段记忆理论(EMT)，将反复神经网络(RNN)概念化为通用序列片段记忆模型的离散时间类比，并且通过实验证实了EMT的有效性。通过引入新的算法任务，发现受训练的RNN始终会收敛到变量绑定电路，揭示了RNN动力学的普遍性，并且设计了一个算法来揭示变量的时间存储和组合中起重要作用的隐藏神经元。

    

    了解反复神经网络(RNN)的复杂操作对于推动其能力和应用至关重要。在这项追求中，我们提出了片段记忆理论(EMT)，说明了RNN可以被概念化为最近提出的通用序列片段记忆模型的离散时间类比。为了证实EMT，我们引入了一系列新颖的算法任务，旨在探索RNN的变量绑定行为。利用EMT，我们制定了一个数学严谨的电路，用于促进这些任务中的变量绑定。我们的实证研究发现，经过训练的RNN始终会收敛到变量绑定电路，从而表明了RNN动力学的普遍性。基于这些发现，我们设计了一个算法来定义一个特权基础，揭示了在时间储存和组合变量中起重要作用的隐藏神经元，这是成功推广这些任务的关键机制。

    Understanding the intricate operations of Recurrent Neural Networks (RNNs) mechanistically is pivotal for advancing their capabilities and applications. In this pursuit, we propose the Episodic Memory Theory (EMT), illustrating that RNNs can be conceptualized as discrete-time analogs of the recently proposed General Sequential Episodic Memory Model. To substantiate EMT, we introduce a novel set of algorithmic tasks tailored to probe the variable binding behavior in RNNs. Utilizing the EMT, we formulate a mathematically rigorous circuit that facilitates variable binding in these tasks. Our empirical investigations reveal that trained RNNs consistently converge to the variable binding circuit, thus indicating universality in the dynamics of RNNs. Building on these findings, we devise an algorithm to define a privileged basis, which reveals hidden neurons instrumental in the temporal storage and composition of variables, a mechanism vital for the successful generalization in these tasks. 
    
[^67]: AXNav: 从自然语言中重放无障碍测试

    AXNav: Replaying Accessibility Tests from Natural Language. (arXiv:2310.02424v1 [cs.HC])

    [http://arxiv.org/abs/2310.02424](http://arxiv.org/abs/2310.02424)

    这篇论文研究了一种从自然语言中重放无障碍测试的系统，该系统利用大型语言模型和基于像素的用户界面理解模型执行测试并生成可导航的视频。通过这种方式，开发人员和质量保证测试人员能够更高效地测试无障碍功能。

    

    开发者和质量保证测试人员通常依赖手动测试来在产品生命周期中测试无障碍功能。然而，手动测试可能很乏味，范围庞大，并且很难安排在其他开发里程碑之间。最近，大型语言模型（LLMs）已被用于各种任务，包括自动化用户界面，但据我们了解，迄今为止还没有人探索过它们在控制辅助技术以支持无障碍测试方面的应用。在本文中，我们从一项形成性研究开始，探讨基于自然语言的无障碍测试工作流程的要求。我们构建了一个系统，该系统以手动无障碍测试为输入（例如，“在VoiceOver中搜索一个节目”），并使用LLM结合基于像素的用户界面理解模型来执行测试并生成章节划分的可导航视频。在每个视频中，为了帮助质量保证测试人员，我们应用启发式方法来检测和标记ac。

    Developers and quality assurance testers often rely on manual testing to test accessibility features throughout the product lifecycle. Unfortunately, manual testing can be tedious, often has an overwhelming scope, and can be difficult to schedule amongst other development milestones. Recently, Large Language Models (LLMs) have been used for a variety of tasks including automation of UIs, however to our knowledge no one has yet explored their use in controlling assistive technologies for the purposes of supporting accessibility testing. In this paper, we explore the requirements of a natural language based accessibility testing workflow, starting with a formative study. From this we build a system that takes as input a manual accessibility test (e.g., ``Search for a show in VoiceOver'') and uses an LLM combined with pixel-based UI Understanding models to execute the test and produce a chaptered, navigable video. In each video, to help QA testers we apply heuristics to detect and flag ac
    
[^68]: OneAdapt：通过反向传播实现深度学习应用的快速自适应

    OneAdapt: Fast Adaptation for Deep Learning Applications via Backpropagation. (arXiv:2310.02422v1 [cs.LG])

    [http://arxiv.org/abs/2310.02422](http://arxiv.org/abs/2310.02422)

    OneAdapt通过梯度上升策略来实现快速自适应，满足了深度学习应用在配置参数方面的三个要求。

    

    深度学习在流媒体数据的推断方面已经普及，如视频中的目标检测、LiDAR数据和音频波形中的文本提取。为了实现高推断准确性，这些应用通常需要大量的网络带宽来收集高保真数据，并且需要广泛的GPU资源来运行深度神经网络(DNN)。尽管通过优化配置参数（如视频分辨率和帧率）可以大大减少对网络带宽和GPU资源的需求，但目前的自适应技术无法同时满足三个要求：（i）以最小的额外GPU或带宽开销来自适应配置；（ii）基于数据对最终DNN的准确性的影响来达到接近最优的决策；（iii）针对一系列配置参数进行自适应。本文提出了OneAdapt，通过利用梯度上升策略来自适应配置参数，满足了这些要求。关键思想是充分利用DNN的不同

    Deep learning inference on streaming media data, such as object detection in video or LiDAR feeds and text extraction from audio waves, is now ubiquitous. To achieve high inference accuracy, these applications typically require significant network bandwidth to gather high-fidelity data and extensive GPU resources to run deep neural networks (DNNs). While the high demand for network bandwidth and GPU resources could be substantially reduced by optimally adapting the configuration knobs, such as video resolution and frame rate, current adaptation techniques fail to meet three requirements simultaneously: adapt configurations (i) with minimum extra GPU or bandwidth overhead; (ii) to reach near-optimal decisions based on how the data affects the final DNN's accuracy, and (iii) do so for a range of configuration knobs. This paper presents OneAdapt, which meets these requirements by leveraging a gradient-ascent strategy to adapt configuration knobs. The key idea is to embrace DNNs' different
    
[^69]: 学生大规模语言模型能否与其教师一样表现出色？

    Can a student Large Language Model perform as well as it's teacher?. (arXiv:2310.02421v1 [cs.LG])

    [http://arxiv.org/abs/2310.02421](http://arxiv.org/abs/2310.02421)

    这篇论文总结了知识蒸馏技术，并强调了其关键原理和成功要素，以及在资源受限环境中的部署挑战。同时，论文还指出知识蒸馏有潜力成为关键的技术转折点。

    

    当代深度学习模型的复杂性不断增加，虽然能实现无与伦比的准确性，但也不可避免地在资源受限环境中带来部署挑战。知识蒸馏作为一种将高容量“教师”模型中的知识转移到简化的“学生”模型的技术，成为解决这一困境的有希望的方法。本文全面概述了知识蒸馏范式，强调了其基本原理，如软标签的实用性和温度缩放的重要性。通过细致的研究，我们阐明了成功蒸馏的关键因素，包括学生模型的架构、教师的水平以及超参数的平衡。同时我们还深入探讨了这一过程中的复杂性和挑战。我们的探索凸显了知识蒸馏作为一个重要转折点的潜力。

    The burgeoning complexity of contemporary deep learning models, while achieving unparalleled accuracy, has inadvertently introduced deployment challenges in resource-constrained environments. Knowledge distillation, a technique aiming to transfer knowledge from a high-capacity "teacher" model to a streamlined "student" model, emerges as a promising solution to this dilemma. This paper provides a comprehensive overview of the knowledge distillation paradigm, emphasizing its foundational principles such as the utility of soft labels and the significance of temperature scaling. Through meticulous examination, we elucidate the critical determinants of successful distillation, including the architecture of the student model, the caliber of the teacher, and the delicate balance of hyperparameters. While acknowledging its profound advantages, we also delve into the complexities and challenges inherent in the process. Our exploration underscores knowledge distillation's potential as a pivotal 
    
[^70]: Nugget 2D：用于仅解码器语言模型的动态上下文压缩的扩展

    Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models. (arXiv:2310.02409v1 [cs.CL])

    [http://arxiv.org/abs/2310.02409](http://arxiv.org/abs/2310.02409)

    Nugget 2D是一种用于仅解码器语言模型的动态上下文压缩方法，可以在保留任务能力的同时大幅减少解码过程所需的时间和空间开销。

    

    标准的基于Transformer的语言模型在长上下文中缩放效果不佳。我们提出了一种基于动态上下文压缩的解决方案，该方案将Qin＆Van Durme（2023年）的Nugget方法从BERT类框架扩展到仅解码器的语言模型。我们的方法将历史建模为压缩的“nuggets”，这些“nuggets”经过训练可以进行重建，它可以使用诸如LLaMA之类的现成模型进行初始化。我们通过语言建模、问答和摘要的实验证明，Nugget2D在这些任务中保留了能力，同时在解码过程中大幅减少了时间和空间开销。例如，在自动编码实验中，Nugget2D可以以20倍的压缩比收缩上下文，重建时的BLEU得分为98％，实现了近乎无损编码。

    Standard Transformer-based language models (LMs) scale poorly to long contexts. We propose a solution based on dynamic contextual compression, which extends the Nugget approach of Qin & Van Durme (2023) from BERT-like frameworks to decoder-only LMs. Our method models history as compressed "nuggets" which are trained to allow for reconstruction, and it can be initialized with off-the-shelf models such as LLaMA. We demonstrate through experiments in language modeling, question answering, and summarization that Nugget2D retains capabilities in these tasks, while drastically reducing the overhead during decoding in terms of time and space. For example, in the experiments of autoencoding, Nugget2D can shrink context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly lossless encoding.
    
[^71]: PCGPT：利用转换器进行程序化内容生成

    PCGPT: Procedural Content Generation via Transformers. (arXiv:2310.02405v1 [cs.LG])

    [http://arxiv.org/abs/2310.02405](http://arxiv.org/abs/2310.02405)

    本文介绍了PCGPT框架，它利用离线强化学习和Transformer网络进行程序化内容生成。该框架通过解决传统PCG方法的挑战，生成了更复杂和多样化的游戏内容，并且在更少的步骤中实现了这些结果。

    

    该论文介绍了PCGPT框架，这是一种使用离线强化学习和Transformer网络进行程序化内容生成(PCG)的创新方法。 PCGPT利用基于Transformer的自回归模型迭代生成游戏关卡，解决了传统PCG方法中重复、可预测或不一致内容的挑战。该框架通过捕捉时间依赖性和因果关系的Transformer自注意机制，对动作、状态和奖励的轨迹进行建模。该方法在Sokoban益智游戏中进行了评估，模型预测所需物品及其相应位置。在Sokoban游戏的实验结果表明，PCGPT生成了更复杂和多样化的游戏内容。有趣的是，与现有方法相比，它以显著较少的步骤实现了这些结果，展示了改进游戏设计和在线内容生成的潜力。

    The paper presents the PCGPT framework, an innovative approach to procedural content generation (PCG) using offline reinforcement learning and transformer networks. PCGPT utilizes an autoregressive model based on transformers to generate game levels iteratively, addressing the challenges of traditional PCG methods such as repetitive, predictable, or inconsistent content. The framework models trajectories of actions, states, and rewards, leveraging the transformer's self-attention mechanism to capture temporal dependencies and causal relationships. The approach is evaluated in the Sokoban puzzle game, where the model predicts items that are needed with their corresponding locations. Experimental results on the game Sokoban demonstrate that PCGPT generates more complex and diverse game content. Interestingly, it achieves these results in significantly fewer steps compared to existing methods, showcasing its potential for enhancing game design and online content generation. Our model repr
    
[^72]: SE(3)-蛋白质主链生成中的随机流匹配

    SE(3)-Stochastic Flow Matching for Protein Backbone Generation. (arXiv:2310.02391v1 [cs.LG])

    [http://arxiv.org/abs/2310.02391](http://arxiv.org/abs/2310.02391)

    通过SE(3)-Stochastic Flow Matching，我们提出了一系列新型生成模型FoldFlow，可以准确建模蛋白质主链。这些模型通过无需模拟训练和Riemannian最优传输的结合，具有更好的稳定性和建模能力。

    

    通过基于三维刚体运动（即SE(3)群）的流匹配范式，我们引入了一系列具有不断增强建模能力的新型生成模型：FoldFlow，从而实现了对蛋白质主链的准确建模。首先，我们介绍了FoldFlow-Base，一种无需模拟的学习确定性连续时间动力学和匹配不变目标分布的方法。接下来，我们通过引入Riemannian最优传输来加速训练，创建了FoldFlow-OT，从而构建了更简单和稳定的流。最后，我们设计了FoldFlow-SFM，将Riemannian最优传输和无需模拟训练相结合，可以学习SE(3)上的随机连续时间动力学。我们的FoldFlow生成模型家族相比之前的方法具有几个关键优势。

    The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce $\text{FoldFlow}$ a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\text{D}$ rigid motions -i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\text{SE(3)}$. We next accelerate training by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$, leading to the construction of both more simple and stable flows. Finally, we design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our family of $\text{FoldFlow}$ generative models offer several key advantages over previous
    
[^73]: 关于学习的物理起源

    On Physical Origins of Learning. (arXiv:2310.02375v1 [q-bio.NC])

    [http://arxiv.org/abs/2310.02375](http://arxiv.org/abs/2310.02375)

    该论文探讨了学习的物理起源，并提出学习可能具有非生物和非进化起源的可能性。作者发现能够在简单的物理模型中观察、解释和准确重现学习的关键特性。

    

    理解智能的起源的探索引发了有关自然系统中学习能力的进化的有趣问题。为什么生物体具有获取未知知识的固有动力？这种动力是否仅通过自然选择来解释，因为能够学习的系统由于增加了生存机会而受到青睐？还是存在其他更快速的机制，以“正确的方式”即刻奖励进入“学习模式”的系统？本文探讨了后一种可能性，并努力揭示这些方式的可能性质。我们提出学习可能具有非生物和非进化起源。事实证明，可以在简单的物理模型中观察、解释和准确重现描述开放谐振型系统中能量积累机制的关键特性。

    The quest to comprehend the origins of intelligence raises intriguing questions about the evolution of learning abilities in natural systems. Why do living organisms possess an inherent drive to acquire knowledge of the unknown? Is this motivation solely explicable through natural selection, favoring systems capable of learning due to their increased chances of survival? Or do there exist additional, more rapid mechanisms that offer immediate rewards to systems entering the "learning mode" in the "right ways"? This article explores the latter possibility and endeavors to unravel the possible nature of these ways. We propose that learning may have non-biological and non-evolutionary origin. It turns out that key properties of learning can be observed, explained, and accurately reproduced within simple physical models that describe energy accumulation mechanisms in open resonant-type systems with dissipation.
    
[^74]: ProtoNER: 使用原型网络进行命名实体识别的少样本增量学习

    ProtoNER: Few shot Incremental Learning for Named Entity Recognition using Prototypical Networks. (arXiv:2310.02372v1 [cs.CL])

    [http://arxiv.org/abs/2310.02372](http://arxiv.org/abs/2310.02372)

    ProtoNER是一种基于原型网络的端到端KVP提取模型，可以在现有模型中添加新类别，而只需最少数量的新注释训练样本。

    

    键值对（KVP）提取或命名实体识别（NER）是文档理解和数据提取领域的研究热点。一些基于transformer的模型如LayoutLMv2、LayoutLMv3和LiLT已经取得了最先进的结果。然而，即使向现有模型添加一个新类别，也需要重新注释整个训练数据集并重新训练模型。这些问题都严重影响了更新模型的部署速度。

    Key value pair (KVP) extraction or Named Entity Recognition(NER) from visually rich documents has been an active area of research in document understanding and data extraction domain. Several transformer based models such as LayoutLMv2, LayoutLMv3, and LiLT have emerged achieving state of the art results. However, addition of even a single new class to the existing model requires (a) re-annotation of entire training dataset to include this new class and (b) retraining the model again. Both of these issues really slow down the deployment of updated model. \\ We present \textbf{ProtoNER}: Prototypical Network based end-to-end KVP extraction model that allows addition of new classes to an existing model while requiring minimal number of newly annotated training samples. The key contributions of our model are: (1) No dependency on dataset used for initial training of the model, which alleviates the need to retain original training dataset for longer duration as well as data re-annotation w
    
[^75]: 优先级软Q分解用于字典型强化学习

    Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning. (arXiv:2310.02360v1 [cs.AI])

    [http://arxiv.org/abs/2310.02360](http://arxiv.org/abs/2310.02360)

    本论文提出了一种用于字典型强化学习的优先级软Q分解算法（PSQD），能够在连续状态-动作空间中学习和适应具有字典型优先级的子任务解决方案，实现了先前学习的子任务解决方案的零-shot组成和适应。

    

    复杂任务的强化学习仍然存在挑战，主要是由于设计标量奖励函数的困难以及从头开发模型的固有低效性。相反，最好是将复杂任务以基本子任务的形式指定，并在可能的情况下重复使用子任务的解决方案。在这项工作中，我们解决了连续空间的字典型多目标强化学习问题，其中包含了优先级子任务，这些问题通常很难解决。我们展示了这些问题可以通过子任务转换进行标量化，并使用价值分解逐步解决。利用这一洞察力，我们提出了优先级软Q分解（PSQD），一种在连续状态-动作空间中学习和适应具有字典型优先级的子任务解决方案的新算法。PSQD能够在零-shot组成之后重复使用先前学习的子任务解决方案，并进行适应步骤。它具备保留子任务训练信息并在复合任务中适应的能力。

    Reinforcement learning (RL) for complex tasks remains a challenge, primarily due to the difficulties of engineering scalar reward functions and the inherent inefficiency of training models from scratch. Instead, it would be better to specify complex tasks in terms of elementary subtasks and to reuse subtask solutions whenever possible. In this work, we address continuous space lexicographic multi-objective RL problems, consisting of prioritized subtasks, which are notoriously difficult to solve. We show that these can be scalarized with a subtask transformation and then solved incrementally using value decomposition. Exploiting this insight, we propose prioritized soft Q-decomposition (PSQD), a novel algorithm for learning and adapting subtask solutions under lexicographic priorities in continuous state-action spaces. PSQD offers the ability to reuse previously learned subtask solutions in a zero-shot composition, followed by an adaptation step. Its ability to use retained subtask trai
    
[^76]: 关于自然语言处理中毒性定义的探讨

    On the definition of toxicity in NLP. (arXiv:2310.02357v1 [cs.CL])

    [http://arxiv.org/abs/2310.02357](http://arxiv.org/abs/2310.02357)

    这项研究探讨了毒性的定义模糊性问题，并提出了一种基于定量压力的毒性定义来弥补现有定义的缺点。

    

    毒性检测任务中的根本问题在于毒性的定义模糊不清。谷歌旗下的团队Jigsaw是该领域的领导者之一，他们使用Dixon等人给出的毒性定义：“粗鲁、不尊重或不合理的语言，可能会让某人离开讨论”。人们可以立即看到这个定义的问题，因为它没有给出毒性的定量度量，而且涉及高度主观的文化术语。尽管存在模糊和缺陷，但这个定义已经成为许多研究者广泛采用的实际标准。在这项工作中，我们提出了一种基于定量压力的毒性定义，克服了现有的缺点。

    The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. Jigsaw, a unit within Google and one of the leaders in the field, uses a definition of toxicity given by Dixon et al. - 'rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion'. One can instantly see the issue with this definition, as it gives no quantitative measure of the toxicity and operates with highly subjective cultural terms. Despite all vagueness and flaws, this definition is de-facto widely used by many researchers. In this work we suggest quantative stress-based defenition for the toxicity that overcomes existing shortcomings.
    
[^77]: 探究直觉型计算树逻辑的推理（arXiv:2310.02355v1 [cs.LO]）

    Reasoning about Intuitionistic Computation Tree Logic. (arXiv:2310.02355v1 [cs.LO])

    [http://arxiv.org/abs/2310.02355](http://arxiv.org/abs/2310.02355)

    本文定义了直觉型计算树逻辑的版本，并研究了其在形式验证方面的应用潜力。结果表明，一些CTL的不动点公理在该直觉型版本中并不成立。

    

    本文定义了一个直觉型计算树逻辑的版本。在解释直觉逻辑的语义特点后，我们研究了它在形式验证方面的应用潜力。随后，我们定义了直觉型CTL的语法和语义，并研究了所得逻辑的一些简单属性。最后，我们证明了一些CTL的不动点公理在我们定义的直觉型CTL中并不成立。

    In this paper, we define an intuitionistic version of Computation Tree Logic. After explaining the semantic features of intuitionistic logic, we examine how these characteristics can be interesting for formal verification purposes. Subsequently, we define the syntax and semantics of our intuitionistic version of CTL and study some simple properties of the so obtained logic. We conclude by demonstrating that some fixed-point axioms of CTL are not valid in the intuitionistic version of CTL we have defined.
    
[^78]: 在线随机即时规划的展开启发式方法

    Rollout Heuristics for Online Stochastic Contingent Planning. (arXiv:2310.02345v1 [cs.AI])

    [http://arxiv.org/abs/2310.02345](http://arxiv.org/abs/2310.02345)

    本文将POMDP建模为随机依赖规划问题，并提出了两个领域无关的启发式方法，以解决POMCP高度依赖展开策略的问题。

    

    部分可观察马尔可夫决策过程（POMDP）是一种在部分可观测性和随机动作下进行决策的有用模型。部分可观察蒙特卡洛规划（POMCP）是一种在线算法，用于决定下一步要执行的动作，使用了一种基于UCB（应用于树）算法的蒙特卡洛树搜索方法，该算法是针对完全可观测马尔可夫决策过程的。POMCP生成一个动作-观察树，在叶子节点使用展开策略为叶子节点提供估计值。因此，POMCP高度依赖展开策略来计算良好的估计值，并因此确定良好的动作。因此，许多使用POMCP的实践者需要创建强大的领域特定的启发式策略。在本文中，我们将POMDP建模为随机依赖规划问题。这使我们能够利用在规划社区中开发的领域无关的启发式算法。我们提出了两种启发式方法，第一种基于众所周知的h_add启发式算法。

    Partially observable Markov decision processes (POMDP) are a useful model for decision-making under partial observability and stochastic actions. Partially Observable Monte-Carlo Planning is an online algorithm for deciding on the next action to perform, using a Monte-Carlo tree search approach, based on the UCT (UCB applied to trees) algorithm for fully observable Markov-decision processes. POMCP develops an action-observation tree, and at the leaves, uses a rollout policy to provide a value estimate for the leaf. As such, POMCP is highly dependent on the rollout policy to compute good estimates, and hence identify good actions. Thus, many practitioners who use POMCP are required to create strong, domain-specific heuristics.  In this paper, we model POMDPs as stochastic contingent planning problems. This allows us to leverage domain-independent heuristics that were developed in the planning community. We suggest two heuristics, the first is based on the well-known h_add heuristic from
    
[^79]: 用于英国核环境中的自主系统安全案例

    Autonomous Systems' Safety Cases for use in UK Nuclear Environments. (arXiv:2310.02344v1 [cs.RO])

    [http://arxiv.org/abs/2310.02344](http://arxiv.org/abs/2310.02344)

    本研究描述了在英国核环境中开发自主机器人部署的安全案例过程，并提出了一个假设机器人的安全案例。这为进一步讨论核站点许可持有人、核安全办公室、工业界和学术界之间的合作奠定了基础。

    

    描述了在英国核站点部署自主机器人的安全案例开发过程，并提出了一个包含人工智能的假设机器人的安全案例。这是向部署迈出的第一步，展示了目前可能的情况以及通过工具开发可能实现的情况。它为核站点许可持有人、核安全办公室（ONR）、工业界和学术界之间进一步讨论奠定了基础。

    An overview of the process to develop a safety case for an autonomous robot deployment on a nuclear site in the UK is described and a safety case for a hypothetical robot incorporating AI is presented. This forms a first step towards a deployment, showing what is possible now and what may be possible with development of tools. It forms the basis for further discussion between nuclear site licensees, the Office for Nuclear Regulation (ONR), industry and academia.
    
[^80]: 学习可解释的深度去混合神经网络用于高光谱解混

    Learning Interpretable Deep Disentangled Neural Networks for Hyperspectral Unmixing. (arXiv:2310.02340v1 [eess.IV])

    [http://arxiv.org/abs/2310.02340](http://arxiv.org/abs/2310.02340)

    本文提出了一个新的可解释深度学习方法，用于解决高光谱解混问题，考虑了非线性和端元可变性，并采用概率变分深度学习框架和解钢琴学习的思想进行建模和训练。

    

    尽管在改善高光谱解混问题的解决方案方面已经付出了相当大的努力，但复杂的辐射散射和端元可变性等非理想情况仍然对大多数现有算法的性能产生负面影响，并具有很大的挑战性。最近，由于其灵活性和强大的表示能力，深度学习框架已经被探索用于高光谱解混。然而，这些技术要么没有解决解混问题的非理想情况，要么依赖于不可解释的黑盒模型。本文提出了一种新的可解释深度学习方法，用于解决高光谱解混问题，以考虑非线性和端元可变性。所提出的方法利用概率变分深度学习框架，采用解钢琴学习来正确分离光谱和端元。该模型使用随机反向传播进行端到端的学习。

    Although considerable effort has been dedicated to improving the solution to the hyperspectral unmixing problem, non-idealities such as complex radiation scattering and endmember variability negatively impact the performance of most existing algorithms and can be very challenging to address. Recently, deep learning-based frameworks have been explored for hyperspectral umixing due to their flexibility and powerful representation capabilities. However, such techniques either do not address the non-idealities of the unmixing problem, or rely on black-box models which are not interpretable. In this paper, we propose a new interpretable deep learning method for hyperspectral unmixing that accounts for nonlinearity and endmember variability. The proposed method leverages a probabilistic variational deep-learning framework, where disentanglement learning is employed to properly separate the abundances and endmembers. The model is learned end-to-end using stochastic backpropagation, and traine
    
[^81]: 几乎等变量子神经网络用于图像中$p4m$群对称性

    Approximately Equivariant Quantum Neural Network for $p4m$ Group Symmetries in Images. (arXiv:2310.02323v1 [quant-ph])

    [http://arxiv.org/abs/2310.02323](http://arxiv.org/abs/2310.02323)

    本文提出了几乎等变量子神经网络（EquivQCNNs），针对图像中的平面$p4m$对称性进行图像分类。这种方法在保持优化性能的同时，通过将先验知识纳入模型中，提升了训练和泛化能力。

    

    量子神经网络（QNNs）被认为是一种可以在噪声存在的情况下在近期量子硬件上低深度高效模拟的量子算法之一。然而，它们的性能高度依赖于选择最合适的变分量子算法（VQAs）架构，问题无关的模型通常遇到可训练性和泛化能力的问题。作为解决方案，最近的研究探索了几何量子机器学习（GQML），使用了与给定数据集的底层对称性等变的QNNs。GQML通过将先验知识纳入到模型中来为模型添加归纳偏差，并在约束搜索空间的同时提高了优化性能。本文提出了针对平面$p4m$对称性的图像分类的等变量子卷积神经网络（EquivQCNNs）。我们展示了在不同数据集测试的结果。

    Quantum Neural Networks (QNNs) are suggested as one of the quantum algorithms which can be efficiently simulated with a low depth on near-term quantum hardware in the presence of noises. However, their performance highly relies on choosing the most suitable architecture of Variational Quantum Algorithms (VQAs), and the problem-agnostic models often suffer issues regarding trainability and generalization power. As a solution, the most recent works explore Geometric Quantum Machine Learning (GQML) using QNNs equivariant with respect to the underlying symmetry of the dataset. GQML adds an inductive bias to the model by incorporating the prior knowledge on the given dataset and leads to enhancing the optimization performance while constraining the search space. This work proposes equivariant Quantum Convolutional Neural Networks (EquivQCNNs) for image classification under planar $p4m$ symmetry, including reflectional and $90^\circ$ rotational symmetry. We present the results tested in diff
    
[^82]: 自学优化器（STOP）：递归自我改进的代码生成

    Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation. (arXiv:2310.02304v1 [cs.CL])

    [http://arxiv.org/abs/2310.02304](http://arxiv.org/abs/2310.02304)

    本文提出了一种自学优化器（STOP），通过递归自我改进的代码生成，使用融合了语言模型的脚手架程序来改进自身，从而生成性能更好的程序。

    

    最近几年的人工智能系统（例如思维树和程序辅助语言模型）取得了一些重要进展，通过提供一个“脚手架”程序来解决问题，该程序构建了多次调用语言模型以生成更好的输出。脚手架程序通常使用Python等编程语言编写。在这项工作中，我们使用了一个融合了语言模型的脚手架程序来改进自身。我们从一个种子“改进器”开始，通过多次查询语言模型并返回最佳解决方案，根据给定的效用函数来改进输入程序。然后，我们运行这个种子改进器来改进自身。在一系列细分任务中，得到的改进改进器生成的程序在性能上明显优于种子改进器。随后，我们对语言模型提出的各种自我改进策略进行了分析，包括波束搜索、遗传算法和模拟退火。由于语言模型本身没有改变，这并不是一种增长领域。

    Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not fu
    
[^83]: 3D物理系统中学习对称性破缺的松弛八面体群卷积

    Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems. (arXiv:2310.02299v1 [cs.LG])

    [http://arxiv.org/abs/2310.02299](http://arxiv.org/abs/2310.02299)

    本文介绍了一种用于建模3D物理系统的松弛八面体群卷积技术，它可以在保持数据一致的最高等变性水平的同时，发现物理系统中微妙的对称性破缺因素。

    

    深度等价模型利用对称性提高样本效率和泛化性能。然而，在许多这些模型中，完美对称性的假设有时可能会限制性能，特别是当数据与这些对称性不完全一致时。因此，我们在本文中引入了用于建模3D物理系统的松弛八面体群卷积。这种灵活的卷积技术能够在保持与数据一致的最高等变性水平的同时，发现物理系统中微妙的对称性破缺因素。实证结果验证了我们的方法不仅可以揭示相变中的对称性破缺因素，还可以在流体超分辨率任务中实现卓越性能。

    Deep equivariant models use symmetries to improve sample efficiency and generalization. However, the assumption of perfect symmetry in many of these models can sometimes be restrictive, especially when the data does not perfectly align with such symmetries. Thus, we introduce relaxed octahedral group convolution for modeling 3D physical systems in this paper. This flexible convolution technique provably allows the model to both maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in the physical systems. Empirical results validate that our approach can not only provide insights into the symmetry-breaking factors in phase transitions but also achieves superior performance in fluid super-resolution tasks.
    
[^84]: 使用声学特性为情感表达生成音频提示

    Prompting Audios Using Acoustic Properties For Emotion Representation. (arXiv:2310.02298v1 [cs.SD])

    [http://arxiv.org/abs/2310.02298](http://arxiv.org/abs/2310.02298)

    本研究提出使用声学特性生成音频提示，并通过训练模型来更好地学习情感表达。实验结果表明，声学提示显著提高了模型在情感音频检索和语音情感识别中的性能。

    

    情感存在于一个连续的范围内，但现有模型将情感视为有限离散值变量。这种表示方式无法捕捉情感表达的多样性。为了更好地表示情感，我们提出使用自然语言描述（或提示）。在这项工作中，我们解决了自动生成这些提示并训练模型从音频和提示对中更好地学习情感表达的挑战。我们使用与情感相关的声学特性（如音调、强度、语速和发音速度）来自动生成提示，即“声学提示”。我们使用对比学习目标将语音映射到相应的声学提示。我们在情感音频检索和语音情感识别上评估了我们的模型。我们的结果显示，声学提示在EAR中显著提高了模型在各种Precision@K指标上的性能。在SER中，我们观察到Ravdess数据上的相对准确率提高了3.8%。

    Emotions lie on a continuum, but current models treat emotions as a finite valued discrete variable. This representation does not capture the diversity in the expression of emotion. To better represent emotions we propose the use of natural language descriptions (or prompts). In this work, we address the challenge of automatically generating these prompts and training a model to better learn emotion representations from audio and prompt pairs. We use acoustic properties that are correlated to emotion like pitch, intensity, speech rate, and articulation rate to automatically generate prompts i.e. 'acoustic prompts'. We use a contrastive learning objective to map speech to their respective acoustic prompts. We evaluate our model on Emotion Audio Retrieval and Speech Emotion Recognition. Our results show that the acoustic prompts significantly improve the model's performance in EAR, in various Precision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on the Ravdess data
    
[^85]: 无网格可微分编程与基于数据驱动策略的偏微分方程约束下的最优控制比较

    A Comparison of Mesh-Free Differentiable Programming and Data-Driven Strategies for Optimal Control under PDE Constraints. (arXiv:2310.02286v1 [cs.LG])

    [http://arxiv.org/abs/2310.02286](http://arxiv.org/abs/2310.02286)

    本研究对直接-伴随循环、物理感知神经网络和可微分编程进行了比较，发现在偏微分方程约束下的最优控制中，可微分编程是最有效的方法，并提供了条件下的使用指南。

    

    在深度学习和自动微分库的影响下，偏微分方程约束下的最优控制领域正在迅速变化。我们对直接-伴随循环(DAL)、物理感知神经网络(PINN)和可微分编程(DP)进行了全面比较，使用基于径向基函数的通用无网格可微分PDE求解器。在拉普拉斯和纳维-斯托克斯方程下，我们发现DP非常有效，因为它产生了最准确的梯度，即使DAL失败和PINN困难。此外，我们提供了一个详细的基准，突出了这些方法在哪些条件下可以有效使用。我们的工作为最优控制从业者提供了指南，并进一步连接了他们与深度学习社区。

    The field of Optimal Control under Partial Differential Equations (PDE) constraints is rapidly changing under the influence of Deep Learning and the accompanying automatic differentiation libraries. Novel techniques like Physics-Informed Neural Networks (PINNs) and Differentiable Programming (DP) are to be contrasted with established numerical schemes like Direct-Adjoint Looping (DAL). We present a comprehensive comparison of DAL, PINN, and DP using a general-purpose mesh-free differentiable PDE solver based on Radial Basis Functions. Under Laplace and Navier-Stokes equations, we found DP to be extremely effective as it produces the most accurate gradients; thriving even when DAL fails and PINNs struggle. Additionally, we provide a detailed benchmark highlighting the limited conditions under which any of those methods can be efficiently used. Our work provides a guide to Optimal Control practitioners and connects them further to the Deep Learning community.
    
[^86]: PASTA: 并行时空注意力与空间自相关门控用于细粒度人群流预测

    PASTA: PArallel Spatio-Temporal Attention with spatial auto-correlation gating for fine-grained crowd flow prediction. (arXiv:2310.02284v1 [cs.LG])

    [http://arxiv.org/abs/2310.02284](http://arxiv.org/abs/2310.02284)

    本文提出了一种名为PASTA的神经网络模型，通过细粒度地图中的历史人流的时空模式来预测未来全市范围内的人群流。这种方法包括空间自相关门控、多尺度残差块和时序注意力门控模块，能够有效捕捉细粒度地图中的不规则时空模式。

    

    理解城市中物体（如人类和车辆）的运动模式对于许多应用非常重要，包括城市规划和管理。本文提出了一种通过建模细粒度城市地图中历史人流的时空模式来预测未来全市范围内人群流的方法。我们引入了一种名为PASTA的新型神经网络，它有效地捕捉了细粒度地图中的不规则时空模式。我们方法中的新颖组件包括空间自相关门控、多尺度残差块和时序注意力门控模块。空间自相关门控采用空间统计的概念来识别不规则的空间区域。多尺度残差块负责处理细粒度地图中的多个范围空间依赖关系，而时序注意力门控则过滤掉不相关的时间信息。

    Understanding the movement patterns of objects (e.g., humans and vehicles) in a city is essential for many applications, including city planning and management. This paper proposes a method for predicting future city-wide crowd flows by modeling the spatio-temporal patterns of historical crowd flows in fine-grained city-wide maps. We introduce a novel neural network named PArallel Spatio-Temporal Attention with spatial auto-correlation gating (PASTA) that effectively captures the irregular spatio-temporal patterns of fine-grained maps. The novel components in our approach include spatial auto-correlation gating, multi-scale residual block, and temporal attention gating module. The spatial auto-correlation gating employs the concept of spatial statistics to identify irregular spatial regions. The multi-scale residual block is responsible for handling multiple range spatial dependencies in the fine-grained map, and the temporal attention gating filters out irrelevant temporal information
    
[^87]: 使用道路地形特征的共享权重多层感知机及其在车辆轨迹速度预测中的应用

    SWMLP: Shared Weight Multilayer Perceptron for Car Trajectory Speed Prediction using Road Topographical Features. (arXiv:2310.02282v1 [cs.LG])

    [http://arxiv.org/abs/2310.02282](http://arxiv.org/abs/2310.02282)

    本论文提出了一种独立于大量历史速度数据的速度预测方法，通过使用车辆轨迹的道路地形特征来拟合共享权重多层感知机学习模型，取得了显著的定性和定量改进，同时为交通分析的新方法设计提供了新的视角。

    

    尽管交通是一种大规模收集的数据，但通常只在特定区域可用。一个问题是，虽然有研究对这些数据给出了良好的结果，但这些地区的数据可能不足以充分描述全球其他地区的所有交通模式。为了解决这个问题，我们提出了一种独立于大量历史速度数据的速度预测方法。为了预测车辆的速度，我们使用车辆轨迹的道路地形特征来拟合一个共享权重多层感知机学习模型。我们的结果在定性和定量上都显著改进了标准回归分析。此外，该提出的框架为设计交通分析的新方法提供了新的视角。

    Although traffic is one of the massively collected data, it is often only available for specific regions. One concern is that, although there are studies that give good results for these data, the data from these regions may not be sufficiently representative to describe all the traffic patterns in the rest of the world. In quest of addressing this concern, we propose a speed prediction method that is independent of large historical speed data. To predict a vehicle's speed, we use the trajectory road topographical features to fit a Shared Weight Multilayer Perceptron learning model. Our results show significant improvement, both qualitative and quantitative, over standard regression analysis. Moreover, the proposed framework sheds new light on the way to design new approaches for traffic analysis.
    
[^88]: 实时客服呼叫中心对话中的端到端连续语音情感识别

    End-to-End Continuous Speech Emotion Recognition in Real-life Customer Service Call Center Conversations. (arXiv:2310.02281v1 [eess.AS])

    [http://arxiv.org/abs/2310.02281](http://arxiv.org/abs/2310.02281)

    本研究介绍了一个用于实时客服呼叫中心对话中连续语音情感识别的大规模数据集CusEmo，采用维度情感注释方法捕捉情感的微妙、复杂和连续性，并解决了应用于数据集时的挑战。

    

    在呼叫中心对话中进行语音情感识别已经成为评估客户和代理人之间互动质量的有价值工具。与受控实验室环境不同，实际对话发生在不受控制的环境下，并受到影响情感表达的情境因素的影响。本文介绍了我们构建大规模实时数据集（CusEmo）用于客户服务呼叫中心对话中连续语音情感识别的方法。我们采用维度情感注释方法捕捉实际呼叫中心对话中情感的微妙、复杂和连续性，并对情境信息进行了注释。研究还解决了将端到端（E2E）语音情感识别系统应用于数据集时遇到的挑战，包括确定适当的标签采样率和输入片段长度，以及整合情境信息（对话者的性别）。

    Speech Emotion recognition (SER) in call center conversations has emerged as a valuable tool for assessing the quality of interactions between clients and agents. In contrast to controlled laboratory environments, real-life conversations take place under uncontrolled conditions and are subject to contextual factors that influence the expression of emotions. In this paper, we present our approach to constructing a large-scale reallife dataset (CusEmo) for continuous SER in customer service call center conversations. We adopted the dimensional emotion annotation approach to capture the subtlety, complexity, and continuity of emotions in real-life call center conversations, while annotating contextual information. The study also addresses the challenges encountered during the application of the End-to-End (E2E) SER system to the dataset, including determining the appropriate label sampling rate and input segment length, as well as integrating contextual information (interlocutor's gender 
    
[^89]: 专家增强的基于动态时间规整的异常检测

    Expert enhanced dynamic time warping based anomaly detection. (arXiv:2310.02280v1 [cs.LG])

    [http://arxiv.org/abs/2310.02280](http://arxiv.org/abs/2310.02280)

    本文提出了一种名为E-DTWA的新型异常检测方法，基于动态时间规整（DTW）并加入了人机交互概念相关的额外改进。该方法具有高效的检测能力，能够在强烈考虑到专家的检测反馈的基础上灵活地进行重新训练，同时保持低计算和空间复杂度。

    

    动态时间规整（DTW）是一个用于时间序列弹性相似性度量的著名算法。其处理非线性时间扭曲的能力使其在各种数据挖掘任务中很有帮助。其中一个任务是异常检测，它试图揭示出意外的行为而没有错误的检测警报。在本文中，我们提出了一种名为专家增强的动态时间规整异常检测（E-DTWA）的新型异常检测方法。它基于DTW，并在其中加入了与人机交互概念相关的额外改进。我们方法的主要优势包括高效的检测，在强烈考虑到专家的检测反馈的基础上灵活地进行重新训练，同时保持低计算和空间复杂度。

    Dynamic time warping (DTW) is a well-known algorithm for time series elastic dissimilarity measure. Its ability to deal with non-linear time distortions makes it helpful in variety of data mining tasks. Such a task is also anomaly detection which attempts to reveal unexpected behaviour without false detection alarms. In this paper, we propose a novel anomaly detection method named Expert enhanced dynamic time warping anomaly detection (E-DTWA). It is based on DTW with additional enhancements involving human-in-the-loop concept. The main benefits of our approach comprise efficient detection, flexible retraining based on strong consideration of the expert's detection feedback while retaining low computational and space complexity.
    
[^90]: 一致性轨迹模型：学习扩散的概率流ODE轨迹

    Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. (arXiv:2310.02279v1 [cs.LG])

    [http://arxiv.org/abs/2310.02279](http://arxiv.org/abs/2310.02279)

    提出了一种一致性轨迹模型（CTM），它可以加速扩散模型的采样，同时通过对抗训练和去噪得分匹配损失的组合来提高性能，并实现了最先进的采样质量。

    

    一致性模型（CM）加速基于得分的扩散模型采样，但以牺牲样本质量为代价，缺乏一种自然的方法来权衡速度和质量。为了解决这个限制，我们提出了一致性轨迹模型（CTM），它是包括CM和基于得分模型在内的泛化模型。CTM训练一个单一的神经网络，可以在单次前向传递中输出得分（即对数密度的梯度），并允许在扩散过程中任意初始和最终时间之间进行不受限制的遍历概率流普通微分方程（ODE）。CTM利用对抗训练和去噪得分匹配损失的有效组合来提高性能，并在CIFAR-10（FID 1.73）和64X64分辨率的ImageNet上实现新的最先进FID。CTM还实现了一系列新的采样方案，包括确定性和随机的ODE解中的长跳跃。

    Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE soluti
    
[^91]: "垃圾DNA假设：通过稀疏性对LLM预训练权重进行任务中心角度分析"

    Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity. (arXiv:2310.02277v1 [cs.LG])

    [http://arxiv.org/abs/2310.02277](http://arxiv.org/abs/2310.02277)

    本文研究通过稀疏性分析LLM预训练权重的任务中心角度，挑战了传统对于权重中冗余性的观点，并提出了"垃圾DNA假设"。

    

    传统对"垃圾DNA"的概念长期以来与人类基因组中的非编码片段相关联，占其组成的大约98%。然而，最近的研究揭示了一些这些看似无功能的DNA序列在细胞过程中起到的关键作用。有趣的是，深度神经网络中的权重与人类基因中观察到的冗余性有着显著的相似性。人们认为，庞大模型中的权重包含了过多的冗余，可以在不影响性能的情况下去除。本文通过提出一个令人信服的反论来挑战这个传统观点。我们使用稀疏性作为一种工具，来独立而准确地量化预训练大语言模型(LLM)中低幅度权重的细微重要性，从下游任务中心的角度理解它们包含的知识。我们提出了支持我们深入研究的"垃圾DNA假设"。

    The traditional notion of "Junk DNA" has long been linked to non-coding segments within the human genome, constituting roughly 98% of its composition. However, recent research has unveiled the critical roles some of these seemingly non-functional DNA sequences play in cellular processes. Intriguingly, the weights within deep neural networks exhibit a remarkable similarity to the redundancy observed in human genes. It was believed that weights in gigantic models contained excessive redundancy, and could be removed without compromising performance. This paper challenges this conventional wisdom by presenting a compelling counter-argument. We employ sparsity as a tool to isolate and quantify the nuanced significance of low-magnitude weights in pre-trained large language models (LLMs). Our study demonstrates a strong correlation between these weight magnitudes and the knowledge they encapsulate, from a downstream task-centric angle. we raise the "Junk DNA Hypothesis" backed by our in-depth
    
[^92]: 谁是哈利·波特？LLMs中的近似遗忘。

    Who's Harry Potter? Approximate Unlearning in LLMs. (arXiv:2310.02238v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.02238](http://arxiv.org/abs/2310.02238)

    本文介绍了一种新的技术，用于从LLM中遗忘特定的训练数据，而无需重新训练模型。通过在Llama2-7b模型上的实验，我们证明了在短时间的微调中，我们可以有效地擦除模型关于哈利·波特相关内容的能力，同时保持其在其他常见基准测试上的性能几乎不变。

    

    大型语言模型（LLMs）是在包含版权内容的海量互联网语料库上进行训练的。这给模型的开发者和用户，以及原始作者和出版商带来了法律和伦理挑战。本文提出了一种新的技术，可以从LLM中遗忘训练数据的子集，而无需从头开始重新训练。我们在Llama2-7b模型上评估了我们的技术，这是一个由Meta最近开源的生成式语言模型。虽然该模型的预训练花费了超过184K GPU小时，但我们展示了在大约1个GPU小时的微调中，我们有效地擦除了模型生成或回忆哈利·波特相关内容的能力，同时在常见基准测试（如Winogrande，Hellaswag，arc，boolq和piqa）上的表现几乎没有受到影响。我们在HuggingFace上公开提供了我们的微调模型，供社区评估。据我们所知，这是第一次实现这样的功能。

    Large language models (LLMs) are trained on massive internet corpora that often contain copyrighted content. This poses legal and ethical challenges for the developers and users of these models, as well as the original authors and publishers. In this paper, we propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch.  We evaluate our technique on the task of unlearning the Harry Potter books from the Llama2-7b model (a generative language model recently open-sourced by Meta). While the model took over 184K GPU-hours to pretrain, we show that in about 1 GPU hour of finetuning, we effectively erase the model's ability to generate or recall Harry Potter-related content, while its performance on common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains almost unaffected. We make our fine-tuned model publicly available on HuggingFace for community evaluation. To the best of our knowledge, this is the fi
    
[^93]: CoNO: 复杂神经算子用于连续动力学系统

    CoNO: Complex Neural Operator for Continuous Dynamical Systems. (arXiv:2310.02094v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.02094](http://arxiv.org/abs/2310.02094)

    本文介绍了一种复杂神经算子（CoNO），用于解决连续动力学系统中的偏微分方程。该算子通过复分数傅里叶变换来捕获丰富的表示，并通过复值神经网络来提高表示能力、稳健性和泛化性能。

    

    神经算子扩展了数据驱动模型，用于映射无限维函数空间之间的关系。这些模型成功地解决了由微分方程表示的连续动力学系统，如天气预报、流体流动或固体力学。然而，现有的算子仍然依赖于实数空间，因此无法捕捉到在复数空间中通过函数变换可能捕捉到的丰富表示。在本文中，我们引入了一种复杂神经算子（CoNO），该算子在复分数傅里叶域中参数化积分核。另外，使用复值神经网络和无混叠激活函数的模型保留了复数值和复代数特性，从而提供了更好的表示能力、对噪声的稳健性和泛化性能。我们展示了该模型能够通过单一的复分数傅里叶变换有效地捕捉到潜在的偏微分方程。我们进行了大量的实证评估。

    Neural operators extend data-driven models to map between infinite-dimensional functional spaces. These models have successfully solved continuous dynamical systems represented by differential equations, viz weather forecasting, fluid flow, or solid mechanics. However, the existing operators still rely on real space, thereby losing rich representations potentially captured in the complex space by functional transforms. In this paper, we introduce a Complex Neural Operator (CoNO), that parameterizes the integral kernel in the complex fractional Fourier domain. Additionally, the model employing a complex-valued neural network along with aliasing-free activation functions preserves the complex values and complex algebraic properties, thereby enabling improved representation, robustness to noise, and generalization. We show that the model effectively captures the underlying partial differential equation with a single complex fractional Fourier transform. We perform an extensive empirical e
    
[^94]: OceanGPT：用于海洋科学任务的大型语言模型

    OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])

    [http://arxiv.org/abs/2310.02031](http://arxiv.org/abs/2310.02031)

    OceanGPT是首个专为海洋科学任务设计的大型语言模型，通过DoInstruct框架实现自动获取海洋领域指导数据。这一模型的引入填补了海洋科学领域中对LLM的需求缺口，并为海洋科学研究提供了新的工具和方法。

    

    海洋科学是探索充满生命和生物多样性的海洋的科学，考虑到海洋覆盖了地球表面的70％以上，这一领域具有重要意义。最近，大型语言模型（LLM）的进展改变了科学的范式。尽管在其他领域取得了成功，但现有的LLM通常无法满足海洋学家等领域专家的需求，同时对LLM在海洋科学中的潜力尚未得到充分探索。这其中的根本原因可能是海洋数据的庞大而复杂的性质，以及对更高的粒度和丰富的知识的需求。为了解决这些问题，我们推出了首个海洋领域的LLM——OceanGPT，该模型擅长各种海洋科学任务。我们提出了一个新颖的框架DoInstruct，用于自动获取大量的海洋领域指导数据，它基于多智能体的协作生成指导。

    Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
    
[^95]: FiGURe: 使用过滤器增强的简单高效的无监督节点表示

    FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations. (arXiv:2310.01892v1 [cs.LG])

    [http://arxiv.org/abs/2310.01892](http://arxiv.org/abs/2310.01892)

    本文介绍了一种简单的过滤器增强方法来改进无监督节点表示学习的性能，通过捕捉不同特征频谱部分，我们展示了显著的改进，并减少了计算负载。同时，我们通过使用简单的随机 Fourier 特征投影来解决高维表示的计算问题，并在基准数据集上取得了良好的性能。

    

    使用对比学习方法学习的无监督节点表示在下游任务上表现出良好的性能。然而，这些方法依赖于模拟低通滤波器的增强，限制了在需要不同特征频谱部分的任务上的性能。本文提出了一种简单的基于过滤器的增强方法来捕捉特征频谱的不同部分。我们展示了使用这些增强方法的显著改进。此外，我们展示了在这些不同的过滤器增强之间共享相同权重是可能的，从而减少了计算负载。此外，先前的研究表明，在下游任务上获得良好的性能需要高维表示。在处理高维度数据时，特别是当涉及多个增强方法时，增加了计算量。我们通过使用简单的随机 Fourier 特征投影来减轻这个问题并恢复良好的性能。我们的方法 FiGURe 在一些基准数据集上实现了

    Unsupervised node representations learnt using contrastive learning-based methods have shown good performance on downstream tasks. However, these methods rely on augmentations that mimic low-pass filters, limiting their performance on tasks requiring different eigen-spectrum parts. This paper presents a simple filter-based augmentation method to capture different parts of the eigen-spectrum. We show significant improvements using these augmentations. Further, we show that sharing the same weights across these different filter augmentations is possible, reducing the computational load. In addition, previous works have shown that good performance on downstream tasks requires high dimensional representations. Working with high dimensions increases the computations, especially when multiple augmentations are involved. We mitigate this problem and recover good performance through lower dimensional embeddings using simple random Fourier feature projections. Our method, FiGURe achieves an ave
    
[^96]: LanguageBind:通过基于语义对齐的语言将视频-语言预训练扩展到N模态（arXiv:2310.01852v1[cs.CV]）

    LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. (arXiv:2310.01852v1 [cs.CV])

    [http://arxiv.org/abs/2310.01852](http://arxiv.org/abs/2310.01852)

    LanguageBind提出了将语言作为不同模态之间纽带的方法，通过冻结视频-语言预训练获取的语言编码器，并使用对比学习训练其他模态的编码器，实现了多模态的语义对齐。此外，作者还提出了VIDAL-10M数据集来支持该方法。

    

    视频-语言（VL）预训练在多个下游任务中取得了显著的进展。然而，当前的VL预训练框架难以将其扩展到除视觉和语言之外的多模态（N模态，N>=3）。因此，我们提出了LanguageBind，通过将语言作为不同模态之间的纽带，因为语言模态已经得到了很好的探索，包含丰富的语义信息。具体而言，我们使用VL预训练获取的语言编码器，并通过对比学习训练其他模态的编码器。结果是，所有模态被映射到一个共享的特征空间中，实现了多模态的语义对齐。虽然LanguageBind可以扩展VL模态到N模态，但我们还需要一个带有以语言为中心的对齐数据对的高质量数据集。因此，我们提出了VIDAL-10M，其中包含了视频、红外、深度、音频及其相应的语言数据，命名为VIDAL-10M。

    The video-language (VL) pretraining has achieved remarkable improvement in multiple downstream tasks. However, the current VL pretraining framework is hard to extend to multiple modalities (N modalities, N>=3) beyond vision and language. We thus propose LanguageBind, taking the language as the bind across different modalities because the language modality is well-explored and contains rich semantics. Specifically, we freeze the language encoder acquired by VL pretraining, then train encoders for other modalities with contrastive learning. As a result, all modalities are mapped to a shared feature space, implementing multi-modal semantic alignment. While LanguageBind ensures that we can extend VL modalities to N modalities, we also need a high-quality dataset with alignment data pairs centered on language. We thus propose VIDAL-10M with Video, Infrared, Depth, Audio and their corresponding Language, naming as VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with co
    
[^97]: 融合模仿学习和强化学习以实现鲁棒策略改进

    Blending Imitation and Reinforcement Learning for Robust Policy Improvement. (arXiv:2310.01737v1 [cs.LG])

    [http://arxiv.org/abs/2310.01737](http://arxiv.org/abs/2310.01737)

    本文提出了一种融合模仿学习和强化学习的方法，根据在线评估结果交替使用二者，以提高样本效率和学习效果。

    

    虽然强化学习在性能上表现出色，但其样本复杂度仍然是一个重大障碍，限制了其在各个领域的广泛应用。模仿学习利用神经网络优化样本效率，但通常受到所使用的专家示范的质量限制。本文介绍了一种融合模仿学习和强化学习的方法，该方法根据在线评估结果交替使用二者，有效地提高了学习效率。这种算法能够从多种黑盒专家示范中学习和改进。

    While reinforcement learning (RL) has shown promising performance, its sample complexity continues to be a substantial hurdle, restricting its broader application across a variety of domains. Imitation learning (IL) utilizes oracles to improve sample efficiency, yet it is often constrained by the quality of the oracles deployed. which actively interleaves between IL and RL based on an online estimate of their performance. RPI draws on the strengths of IL, using oracle queries to facilitate exploration, an aspect that is notably challenging in sparse-reward RL, particularly during the early stages of learning. As learning unfolds, RPI gradually transitions to RL, effectively treating the learned policy as an improved oracle. This algorithm is capable of learning from and improving upon a diverse set of black-box oracles. Integral to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient (RPG), both of which reason over whether to perform state-wise imitation from the o
    
[^98]: SmartPlay: 一种用于评估LLMs作为智能Agent能力的基准

    SmartPlay : A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v1 [cs.LG])

    [http://arxiv.org/abs/2310.01557](http://arxiv.org/abs/2310.01557)

    SmartPlay是一个用于评估LLMs作为智能Agent能力的基准，包括6个具有不同挑战的游戏，并测试了智能LLM Agent的多种关键能力。这不仅是一个评估LLM Agent整体性能的严格测试场地，还可以分析每个能力的表现。

    

    最近的大型语言模型(LLMs)在智能Agent和下一代自动化方面展示了巨大的潜力，但目前缺乏一个系统化的基准来评估LLMs作为Agent的能力。我们介绍了SmartPlay：一个具有挑战性的基准和评估LLMs作为Agent的方法论。SmartPlay包括6个不同的游戏，包括剪刀石头布、汉诺塔、Minecraft等。每个游戏都具有独特的设置，提供最多20个评估设置和无限的环境变化。SmartPlay中的每个游戏都独特地挑战了智能LLM Agent的9个重要能力的子集，包括对对象依赖的推理、提前规划、空间推理、从历史中学习和理解随机性。每个游戏测试的能力集的区别使我们能够单独分析每个能力。SmartPlay不仅是评估LLM Agent整体性能的严格测试场地，而且也是评估Agent在不同能力方面的性能的一个重要工具。

    Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as
    
[^99]: LLM谎言: 幻觉不是漏洞，而是对抗样本的特征。

    LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples. (arXiv:2310.01469v1 [cs.CL])

    [http://arxiv.org/abs/2310.01469](http://arxiv.org/abs/2310.01469)

    LLM似乎具有丰富的知识和适应多种任务的能力，但我们不能完全信任它们的回答，因为它们会出现幻觉，即捏造不存在的事实以欺骗用户。本文证明了由随机标记组成的无意义提示也能引起LLM产生幻觉回应，并提出了一种对抗方式的自动幻觉触发方法作为幻觉攻击，同时提出了一种简单而有效的防御策略。

    

    大型语言模型（LLM），包括GPT-3.5、LLaMA和PaLM，似乎具有丰富的知识和适应多种任务的能力。然而，我们仍然不能完全信任它们的回答，因为LLM会出现幻觉，即捏造不存在的事实以欺骗用户而不被察觉。幻觉存在的原因和普遍性仍然不清楚。在本文中，我们证明了由随机标记组成的无意义提示也能引起LLM产生幻觉回应。这个现象迫使我们重新审视幻觉可能是对抗样本的另一种视角，并且它与常规的对抗样本具有类似的特征，作为LLM的基本特征。因此，我们以对抗的方式将自动幻觉触发方法形式化为幻觉攻击。最后，我们研究了被攻击的对抗提示的基本特征，并提出了一种简单而有效的防御策略。我们的代码已在GitHub上发布。

    Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still can not completely trust their answer, since LLMs suffer from hallucination--fabricating non-existent facts to cheat users without perception. And the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that non-sense prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. This phenomenon forces us to revisit that hallucination may be another view of adversarial examples, and it shares similar features with conventional adversarial examples as the basic feature of LLMs. Therefore, we formalize an automatic hallucination triggering method as the hallucination attack in an adversarial way. Finally, we explore basic feature of attacked adversarial prompts and propose a simple yet effective defense strategy. Our code is released on GitHub.
    
[^100]: 实体推断竞技场：探究LLMs的对话推理和规划能力的平台

    The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])

    [http://arxiv.org/abs/2310.01468](http://arxiv.org/abs/2310.01468)

    本文提供了一个评估框架，通过向法官提出一系列查询来评估LLMs的对话推理和规划能力。我们发现不同的LLMs在这个任务上表现出显著差异。

    

    目前，大型语言模型（LLMs）在回答明确提问时非常有效。然而，当面临含糊不清的查询时，它们可能行为难以预测并产生错误的输出。这凸显了需要开发能够提出澄清问题以有效解决歧义的智能代理的需求。这种能力需要对多个对话轮次进行复杂的理解、状态跟踪、推理和规划。然而，直接测量这种能力可能具有挑战性。在本文中，我们提供了一个替代性问题，通过向法官提出一系列查询，评估了LLMs推断自己不知道但被法官揭示的实体的能力。这个“实体推断游戏”可以作为一个评估框架，用于探究语言模型的对话推理和规划能力。我们系统地评估了各种LLMs，并发现在这个任务上它们的性能存在显著差异。我们发现强大的LLMs...

    Large language models (LLMs) are currently effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This \textit{entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs
    
[^101]: 解决利用反事实知识蒸馏的聪明汉斯预测模型问题

    Towards Fixing Clever-Hans Predictors with Counterfactual Knowledge Distillation. (arXiv:2310.01011v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.01011](http://arxiv.org/abs/2310.01011)

    这项研究提出了一种称为反事实知识蒸馏的新技术，通过人类专家的反馈来检测和消除深度学习模型对虚假特征的依赖，特别适用于受监管或安全关键领域。

    

    本论文引入一种名为反事实知识蒸馏（CFKD）的新技术，借助人类专家反馈来检测和消除深度学习模型对混淆因素的依赖。混淆因素是模型倾向于依赖的虚假特征，可能导致在受监管或安全关键领域出现意外错误。论文强调了CFKD在此类领域中的优势，并展示了反事实解释相对于其他类型解释的一些优点。我们提出了一个实验方案来定量评估CFKD的成功性和可以为模型提供反馈的不同教师。我们还引入了一个与真实测试性能更相关的新度量标准，而不是仅仅依靠验证精度。论文展示了CFKD在合成增强数据集和现实世界组织病理学数据集上的有效性。

    This paper introduces a novel technique called counterfactual knowledge distillation (CFKD) to detect and remove reliance on confounders in deep learning models with the help of human expert feedback. Confounders are spurious features that models tend to rely on, which can result in unexpected errors in regulated or safety-critical domains. The paper highlights the benefit of CFKD in such domains and shows some advantages of counterfactual explanations over other types of explanations. We propose an experiment scheme to quantitatively evaluate the success of CFKD and different teachers that can give feedback to the model. We also introduce a new metric that is better correlated with true test performance than validation accuracy. The paper demonstrates the effectiveness of CFKD on synthetically augmented datasets and on real-world histopathological datasets.
    
[^102]: 图神经网络能否作为最优近似算法？

    Are Graph Neural Networks Optimal Approximation Algorithms?. (arXiv:2310.00526v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00526](http://arxiv.org/abs/2310.00526)

    本文设计了图神经网络架构OptGNN，利用半定规划工具获得大类组合优化问题的最优近似算法。通过实证结果表明在各种数据集上超过了神经网络基线算法和传统算法，同时利用OptGNN的能力设计了一个产生优化的对偶证书的算法。

    

    在这项工作中，我们设计了能够使用半定规划（SDP）强大的算法工具来获得大类组合优化问题的最优近似算法的图神经网络架构。具体而言，我们证明了多项式大小的消息传递算法可以表示最强大的多项式时间算法，前提是假设唯一游戏猜想成立。我们利用这一结果构建了高效的图神经网络架构OptGNN，它在诸如最大割和最大独立集等重要组合优化问题上获得了高质量的近似解。我们的方法在各种真实世界和合成数据集上表现出强大的实证结果，不仅超过了神经网络基线算法，还超过了传统算法。最后，我们利用OptGNN捕捉凸松弛的能力，设计了一个产生优化的对偶证书（确定性上界）的算法。

    In this work we design graph neural network architectures that can be used to obtain optimal approximation algorithms for a large class of combinatorial optimization problems using powerful algorithmic tools from semidefinite programming (SDP). Concretely, we prove that polynomial-sized message passing algorithms can represent the most powerful polynomial time algorithms for Max Constraint Satisfaction Problems assuming the Unique Games Conjecture. We leverage this result to construct efficient graph neural network architectures, OptGNN, that obtain high-quality approximate solutions on landmark combinatorial optimization problems such as Max Cut and maximum independent set. Our approach achieves strong empirical results across a wide range of real-world and synthetic datasets against both neural baselines and classical algorithms. Finally, we take advantage of OptGNN's ability to capture convex relaxations to design an algorithm for producing dual certificates of optimality (bounds on
    
[^103]: ToRA：一种集成工具的数学问题求解推理代理

    ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving. (arXiv:2309.17452v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.17452](http://arxiv.org/abs/2309.17452)

    ToRA是一种集成工具的数学问题求解推理代理，通过结合语言的分析能力和工具的计算效率，能够显著提高数学推理的性能，在多个数学推理数据集上取得了13%-19%的平均绝对改进率，并在竞赛级数据集MATH上达到了44.6%的性能。

    

    大型语言模型在各种语言任务中取得了重大进展，但在复杂的数学问题上仍然存在困难。在本文中，我们提出了一系列集成工具的推理代理ToRA，它通过无缝地将自然语言推理与外部工具（例如计算库和符号求解器）的利用相结合，从而将语言的分析能力与工具的计算效率融合在一起，用于解决具有挑战性的数学问题。为了训练ToRA，我们精选了数学数据集上的互动工具使用轨迹，应用模仿学习于注释，并提出输出空间整形来进一步改进模型的推理行为。结果显示，ToRA模型在10个涵盖各种规模的数学推理数据集上显著优于开源模型，平均绝对改进率达到13%至19%。值得注意的是，ToRA-7B 在竞赛级数据集MATH上达到了44.6%，超越了最佳开源模型WizardMath。

    Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath
    
[^104]: 一种用于医学图像中一般移动目标分割的基础模型

    A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])

    [http://arxiv.org/abs/2309.17264](http://arxiv.org/abs/2309.17264)

    本文提出了一种用于医学图像中移动目标分割的基础模型iMOS，通过对序列中只有少量图像进行注释，即可实现高精度的分割效果

    

    医学图像分割旨在描绘感兴趣的解剖或病理结构，在临床诊断中起着关键作用。构建高精度的深度分割模型需要大量高质量的注释数据。然而，医学注释非常繁琐耗时，特别是对于医学视频或3D体积，由于巨大的标签空间和差的帧间一致性。最近，在自然图像中，一个名为Moving Object Segmentation (MOS)的基本任务在技术上取得了重大进展。它的目标是在图像序列中从背景中描绘移动物体，只需要最小的注释。在本文中，我们提出了第一个用于医学图像中MOS的基础模型，名为iMOS。对一个大规模多模态医学数据集进行的大量实验验证了所提出的iMOS的有效性。具体而言，只需对序列中少量的图像进行注释，iMOS就可以实现了

    Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve sati
    
[^105]: 概率权重固定：用于量化的神经网络权重不确定性的大规模训练

    Probabilistic Weight Fixing: Large-scale training of neural network weight uncertainties for quantization. (arXiv:2309.13575v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13575](http://arxiv.org/abs/2309.13575)

    本文提出了一种基于贝叶斯神经网络和变分松弛的概率框架，用于通过将权重值限制在一组有限值上来减少推理过程中的能量消耗。通过利用权重值的概率分布，提高了噪声鲁棒性和可压缩性。迭代聚类过程展示了超越现有方法的优势。

    

    权重共享量化是一种通过将神经网络的权重限制在一组有限的值上来减少推理过程中能量消耗的技术。然而，现有的权重共享量化方法常常基于权重值本身进行假设，并忽视了权重位置在其中扮演的独特角色。本文提出了一个基于贝叶斯神经网络（BNNs）和变分松弛的概率框架，根据单个权重的位置特定学习不确定性分布来确定可以将哪些权重移动到哪个聚类中心以及移动到什么程度。我们引入了一种新的初始化设置和正则化项，可以在复杂的数据集-模型组合下训练BNNs。通过利用通过概率分布捕捉到的权重值的灵活性，我们提高了噪声的鲁棒性和下游的可压缩性。我们的迭代聚类过程展示了超越现有方法的优越性能。

    Weight-sharing quantization has emerged as a technique to reduce energy expenditure during inference in large neural networks by constraining their weights to a limited set of values. However, existing methods for weight-sharing quantization often make assumptions about the treatment of weights based on value alone that neglect the unique role weight position plays. This paper proposes a probabilistic framework based on Bayesian neural networks (BNNs) and a variational relaxation to identify which weights can be moved to which cluster centre and to what degree based on their individual position-specific learned uncertainty distributions. We introduce a new initialisation setting and a regularisation term which allow for the training of BNNs under complex dataset-model combinations. By leveraging the flexibility of weight values captured through a probability distribution, we enhance noise resilience and downstream compressibility. Our iterative clustering procedure demonstrates superio
    
[^106]: 从生物学启发机制重新思考超像素分割

    Rethinking superpixel segmentation from biologically inspired mechanisms. (arXiv:2309.13438v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.13438](http://arxiv.org/abs/2309.13438)

    该论文从神经结构和视觉机制的启示出发，提出了一种生物网络架构用于超像素分割，其中包括增强筛选模块（ESM）和边界感知标签（BAL），旨在产生严格遵循物体边界且传达丰富视觉符号的超像素。

    

    最近，基于深度学习的超像素分割方法的发展在分割的效率和性能方面都取得了改进。然而，一个重要的挑战在于产生严格遵循物体边界且传达丰富视觉符号的超像素，特别是当交叉表面颜色相关性可能干扰物体时。受神经结构和视觉机制的启发，我们提出了一个生物网络架构，其中包括一个增强筛选模块（ESM）和一个新颖的边界感知标签（BAL），用于超像素分割。ESM通过模拟视觉皮质的交互式投影机制来增强语义信息。此外，BAL模拟了视觉皮质细胞的空间频率特性，以促进生成具有强边界遵循性的超像素。我们通过在BSDS500数据集上进行评估来证明我们方法的有效性。

    Recently, advancements in deep learning-based superpixel segmentation methods have brought about improvements in both the efficiency and the performance of segmentation. However, a significant challenge remains in generating superpixels that strictly adhere to object boundaries while conveying rich visual significance, especially when cross-surface color correlations may interfere with objects. Drawing inspiration from neural structure and visual mechanisms, we propose a biological network architecture comprising an Enhanced Screening Module (ESM) and a novel Boundary-Aware Label (BAL) for superpixel segmentation. The ESM enhances semantic information by simulating the interactive projection mechanisms of the visual cortex. Additionally, the BAL emulates the spatial frequency characteristics of visual cortical cells to facilitate the generation of superpixels with strong boundary adherence. We demonstrate the effectiveness of our approach through evaluations on both the BSDS500 dataset
    
[^107]: 时间序列预测：利用分数差分数据释放长期依赖关系

    Time-Series Forecasting: Unleashing Long-Term Dependencies with Fractionally Differenced Data. (arXiv:2309.13409v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13409](http://arxiv.org/abs/2309.13409)

    本研究提出了一种利用分数差分来捕捉时间序列数据中短期和长期依赖关系的预测策略。通过将FD应用于金融数据并结合情感分析，实证结果证明FD在二元分类中的性能优于整数差分方法。

    

    本研究介绍了一种新颖的预测策略，利用分数差分（FD）的能力来捕捉时间序列数据中的短期和长期依赖关系。与传统的整数差分方法不同，FD在保持系列记忆的同时稳定了它以供建模目的。通过将FD应用于来自SPY指数的金融数据，并结合新闻报道的情感分析，这个实证分析探讨了FD与目标变量的二元分类的效果。采用了监督分类算法来验证FD系列的性能。结果显示，FD相比整数差分具有优越性，这一点通过接收者操作特征/曲线下面积（ROCAUC）和马修斯相关系数（MCC）的评估得到确认。

    This study introduces a novel forecasting strategy that leverages the power of fractional differencing (FD) to capture both short- and long-term dependencies in time series data. Unlike traditional integer differencing methods, FD preserves memory in series while stabilizing it for modeling purposes. By applying FD to financial data from the SPY index and incorporating sentiment analysis from news reports, this empirical analysis explores the effectiveness of FD in conjunction with binary classification of target variables. Supervised classification algorithms were employed to validate the performance of FD series. The results demonstrate the superiority of FD over integer differencing, as confirmed by Receiver Operating Characteristic/Area Under the Curve (ROCAUC) and Mathews Correlation Coefficient (MCC) evaluations.
    
[^108]: 对多模态大规模语言模型中的灾难性遗忘进行的研究

    Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])

    [http://arxiv.org/abs/2309.10313](http://arxiv.org/abs/2309.10313)

    本论文针对多模态大规模语言模型中的灾难性遗忘问题进行研究，引入了EMT方法来评估灾难性遗忘，并发现在标准图像分类任务上，几乎所有评估的模型都无法保持与视觉编码器相同的性能水平。研究结果表明，早期微调阶段对性能至关重要。

    

    在GPT4的成功之后，多模态大规模语言模型（MLLM）研究引起了广泛关注。这一研究方向侧重于通过微调预训练的LLM和视觉模型来开发通用的LLM。然而，灾难性遗忘，即微调模型无法保持与预训练模型相似的性能水平，仍然是多模态LLM（MLLM）中的一个固有问题。本文介绍了EMT：用于评估MLLM中灾难性遗忘的评估方法，将每个MLLM作为一个图像分类器进行评估。我们首先应用EMT来评估几个开源的微调MLLM，并发现几乎所有评估的MLLM在标准图像分类任务上无法保持与他们的视觉编码器相同的性能水平。此外，我们继续微调LLaVA，一种MLLM，并利用EMT来评估整个微调过程中的性能。有趣的是，我们的结果表明，早期的微调阶段是关键的，过早停止微调可能导致低性能的模型。

    Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-sta
    
[^109]: GPTFUZZER : 使用自动生成的越狱提示对大型语言模型进行红队测试

    GPTFUZZER : Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts. (arXiv:2309.10253v1 [cs.AI])

    [http://arxiv.org/abs/2309.10253](http://arxiv.org/abs/2309.10253)

    GPTFUZZER是一种黑盒越狱模糊测试框架，自动生成用于红队测试大型语言模型的越狱模板。这种自动化方法避免了手工工程，并通过种子选择策略提高了效率。

    

    大型语言模型（LLMs）最近非常受欢迎，广泛用于日常对话到基于人工智能的编程。然而，尽管取得了巨大的成功，LLMs并不完全可靠，可能会提供有关进行有害或非法活动的详细指导。虽然安全措施可以减少这些输出的风险，但对抗性的"越狱"攻击仍然可以利用LLMs生成有害内容。这些越狱模板通常是手工精心制作的，使大规模测试具有挑战性。在本文中，我们介绍了一种新颖的黑盒越狱模糊测试框架\fuzzer，受AFL模糊测试框架的启发。与手工工程不同，\fuzzer自动化生成用于红队测试LLMs的越狱模板。在核心部分，\fuzzer从人工编写的模板作为种子开始，然后使用变异操作对其进行变异以生成新的模板。我们详细介绍了\fuzzer的三个关键组成部分：用于平衡效率的种子选择策略

    Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial "jailbreak" attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce \fuzzer, a novel black-box jailbreak fuzzing framework inspired by AFL fuzzing framework. Instead of manual engineering, \fuzzer automates the generation of jailbreak templates for red-teaming LLMs. At its core, \fuzzer starts with human-written templates as seeds, then mutates them using mutate operators to produce new templates. We detail three key components of \fuzzer: a seed selection strategy for balancing efficiency 
    
[^110]: 当地球科学遇见基础模型：走向通用地球科学人工智能系统

    When Geoscience Meets Foundation Models: Towards General Geoscience Artificial Intelligence System. (arXiv:2309.06799v1 [cs.AI])

    [http://arxiv.org/abs/2309.06799](http://arxiv.org/abs/2309.06799)

    地球科学基础模型通过整合大量跨学科数据来模拟和理解地球系统动态，具有广阔的应用前景和创新潜力，但仍面临验证和核实、规模性、可解释性、知识表示和社会偏差等挑战。

    

    地球科学基础模型通过整合大量跨学科数据来模拟和理解地球系统动态，代表了地球科学领域的一种革命性方法。作为一种数据中心的人工智能范式，它们从百万亿字节的结构化和非结构化数据中揭示出洞察力。灵活的任务规范、多样化的输入和输出以及多模态的知识表示使得综合分析成为可能。至关重要的是，地球科学模型的可扩展性和可推广性允许解决与地球系统相互作用相关的多种预测、模拟和决策挑战。领域专家和计算机科学家之间的合作推动了这些宝贵工具在理解我们地球的过去、现在和未来方面的创新。然而，验证和核实、规模性、可解释性、知识表示和社会偏差仍然面临挑战。展望未来，增强验证和核实、规模性、解释性、知识表示和社会偏差方面的能力，将有助于推动地球科学人工智能系统的发展。

    Geoscience foundation models represent a revolutionary approach in the field of Earth sciences by integrating massive cross-disciplinary data to simulate and understand the Earth systems dynamics. As a data-centric artificial intelligence (AI) paradigm, they uncover insights from petabytes of structured and unstructured data. Flexible task specification, diverse inputs and outputs and multi-modal knowledge representation enable comprehensive analysis infeasible with individual data sources. Critically, the scalability and generalizability of geoscience models allow for tackling diverse prediction, simulation, and decision challenges related to Earth systems interactions. Collaboration between domain experts and computer scientists leads to innovations in these invaluable tools for understanding the past, present, and future of our planet. However, challenges remain in validation and verification, scale, interpretability, knowledge representation, and social bias. Going forward, enhanci
    
[^111]: 处理声音源分离中的特征失衡问题

    Addressing Feature Imbalance in Sound Source Separation. (arXiv:2309.05287v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2309.05287](http://arxiv.org/abs/2309.05287)

    本论文提出了一种名为FEABASE的方法来解决源分离中的特征偏好问题，通过学习被忽视特征的隐藏信息，实现对数据的高效利用。

    

    神经网络常常在解决任务时存在特征偏好的问题，即过度依赖某些特定特征而忽视其他特征，即使那些被忽视的特征对任务来说是至关重要的。特征偏好问题主要在分类任务中进行了研究。然而，我们观察到特征偏好也存在于高维回归任务中，特别是源分离任务中。为了减轻源分离中的特征偏好问题，我们提出了一种名为FEAture BAlancing by Suppressing Easy feature（FEABASE）的方法。该方法通过学习被忽视特征的隐藏信息，实现了对数据的高效利用。我们在一个多通道源分离任务中评估了我们的方法，其中出现了空间特征和音色特征之间的特征偏好。

    Neural networks often suffer from a feature preference problem, where they tend to overly rely on specific features to solve a task while disregarding other features, even if those neglected features are essential for the task. Feature preference problems have primarily been investigated in classification task. However, we observe that feature preference occurs in high-dimensional regression task, specifically, source separation. To mitigate feature preference in source separation, we propose FEAture BAlancing by Suppressing Easy feature (FEABASE). This approach enables efficient data utilization by learning hidden information about the neglected feature. We evaluate our method in a multi-channel source separation task, where feature preference between spatial feature and timbre feature appears.
    
[^112]: 在COVID-19期间导航不在分布范围内的电力负荷预测：利用人类移动的持续学习方法

    Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: A Continual Learning Approach Leveraging Human Mobility. (arXiv:2309.04296v1 [cs.LG])

    [http://arxiv.org/abs/2309.04296](http://arxiv.org/abs/2309.04296)

    本研究提出了一种利用人类移动数据和持续学习技术的方法来解决COVID-19期间非分布期间的电力负荷预测问题，通过保留过去的见解并整合新的数据，提高了模型的准确性和鲁棒性。

    

    在传统的深度学习算法中，一个关键假设是数据分布在训练和部署过程中保持不变。然而，在面对非分布期间时，如COVID-19的封锁期，数据分布与模型在训练过程中所见的明显偏离。本文采用双重策略：利用持续学习技术更新模型的新数据，并利用在建筑物外部的保护隐私的行人计数器收集的人类移动数据。与在线学习相比，后者常常会遭受“灾难性遗忘”的困扰，因为新获得的知识常常会抹去先前的信息，持续学习则通过保留过去的见解并整合新的数据，提供了一个整体的方法。本研究将FSNet，一种强大的持续学习算法，应用于墨尔本市13个建筑群的真实数据。

    In traditional deep learning algorithms, one of the key assumptions is that the data distribution remains constant during both training and deployment. However, this assumption becomes problematic when faced with Out-of-Distribution periods, such as the COVID-19 lockdowns, where the data distribution significantly deviates from what the model has seen during training. This paper employs a two-fold strategy: utilizing continual learning techniques to update models with new data and harnessing human mobility data collected from privacy-preserving pedestrian counters located outside buildings. In contrast to online learning, which suffers from 'catastrophic forgetting' as newly acquired knowledge often erases prior information, continual learning offers a holistic approach by preserving past insights while integrating new data. This research applies FSNet, a powerful continual learning algorithm, to real-world data from 13 building complexes in Melbourne, Australia, a city which had the s
    
[^113]: 可信和协同的软件工程人工智能：愿景与路线图

    Trustworthy and Synergistic Artificial Intelligence for Software Engineering: Vision and Roadmaps. (arXiv:2309.04142v1 [cs.SE])

    [http://arxiv.org/abs/2309.04142](http://arxiv.org/abs/2309.04142)

    本文提出了可信和协同的软件工程人工智能的愿景和路线图，旨在提高开发人员的生产力和软件质量。实现这个愿景可能导致软件工程领域的一个重要转变，即软件工程2.0.

    

    数十年来，软件工程研究一直致力于设计自动化解决方案，旨在提高开发人员的生产力和提升软件质量。过去的两个十年见证了专门为软件工程任务量身定制的智能解决方案的不断涌现。这个势头建立了软件工程领域中最活跃和最受欢迎的 Artificial Intelligence for Software Engineering (AI4SE) 领域。本文探讨了几个重点。它首先简要介绍和回顾了AI4SE的历史。随后，它强调了AI4SE固有的核心挑战，特别是强调了实现可信和协同的AI4SE的需求。进一步，本文描绘了一种愿景，即如果克服了AI4SE的核心挑战，将实现可达到的潜在飞跃，建议转向软件工程2.0.

    For decades, much software engineering research has been dedicated to devising automated solutions aimed at enhancing developer productivity and elevating software quality. The past two decades have witnessed an unparalleled surge in the development of intelligent solutions tailored for software engineering tasks. This momentum established the Artificial Intelligence for Software Engineering (AI4SE) area, which has swiftly become one of the most active and popular areas within the software engineering field.  This Future of Software Engineering (FoSE) paper navigates through several focal points. It commences with a succinct introduction and history of AI4SE. Thereafter, it underscores the core challenges inherent to AI4SE, particularly highlighting the need to realize trustworthy and synergistic AI4SE. Progressing, the paper paints a vision for the potential leaps achievable if AI4SE's key challenges are surmounted, suggesting a transition towards Software Engineering 2.0. Two strateg
    
[^114]: 通过内在和外在置信度评估来量化任意语言模型回答的不确定性

    Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment. (arXiv:2308.16175v1 [cs.CL])

    [http://arxiv.org/abs/2308.16175](http://arxiv.org/abs/2308.16175)

    本文引入了BSDetector，一种用于检测预训练大型语言模型生成的错误和推测性回答的方法。该方法通过估计置信度量化了回答的不确定性，并在闭合型和开放型问答基准实验中表现出更准确的识别能力。

    

    我们引入了BSDetector，一种通过估计预训练大型语言模型生成的任何输出的数值置信度来检测错误和推测性回答的方法。我们的不确定性量化技术适用于仅通过黑盒API访问的任何LLM，并将内在和外在评估的置信度结合为对给定提示下LLM响应的单个可信度估计。我们的方法非常通用，可以应用于当今所有最好的LLM（其训练数据未知）。通过额外的计算，任何LLM API的用户现在可以获得与通常相同的响应，以及一个置信度估计，以便在不信任该响应时保持谨慎。对于闭合型和开放型问答基准的实验表明，BSDetector比其他不确定性估计方法（对于GPT-3和ChatGPT）更准确地识别出错误的LLM响应。通过对多个响应进行采样

    We introduce BSDetector, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, and combines intrinsic and extrinsic assessments of confidence into a single trustworthiness estimate for any LLM response to a given prompt. Our method is extremely general and can applied to all of the best LLMs available today (whose training data remains unknown). By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that caution when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that BSDetector more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses
    
[^115]: 大型语言模型的指令调优：一项调研

    Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10792](http://arxiv.org/abs/2308.10792)

    本文调查了指令调优这一关键技术在增强大型语言模型能力和可控性方面的研究工作，包括方法、数据集构建、模型训练和应用，以及对结果影响的分析。同时回顾了可能的问题和批评，并指出了目前的不足。

    

    本文调查了指令调优（IT）这一快速发展的领域中的研究工作，这是一种增强大型语言模型（LLM）能力和可控性的关键技术。指令调优是指以监督方式在包含“指令-输出”对的数据集上进一步训练LLM，这将LLM的下一个词预测目标与用户希望LLM遵守人类指令的目标之间的差距。本文对IT的常规方法、IT数据集的构建、IT模型的训练以及应用于不同模态、领域和应用的情况进行了系统的文献综述，并对影响IT结果的各个方面进行了分析（例如，指令输出的生成、指令数据集的大小等）。我们还回顾了IT的潜在问题以及针对其的批评，以及指出当前不足的努力。

    This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
    
[^116]: AutoGen:通过多代理对话框架实现下一代LLM应用

    AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. (arXiv:2308.08155v1 [cs.AI])

    [http://arxiv.org/abs/2308.08155](http://arxiv.org/abs/2308.08155)

    AutoGen是一种新的框架，通过多个可以互相对话的代理，实现了下一代LLM应用。它利用人类的理解和智能，优雅地处理不完美的生成和推理能力，并通过自动化代理对话简化了复杂的工作流程。

    

    本技术报告介绍了AutoGen，一种新的框架，通过多个可以互相对话的代理来开发LLM应用程序以解决任务。AutoGen代理可以定制、可对话，并且可以无缝地允许人类参与。它们可以在各种模式下运行，利用LLM、人类输入和工具的组合。AutoGen的设计提供了多个优势：a）它能够优雅地处理这些LLM的强大但不完美的生成和推理能力；b）它利用人类的理解和智能，通过代理之间的对话提供有价值的自动化；c）它简化和统一了复杂LLM工作流程的实现，作为自动化代理对话。我们提供了许多不同的例子，展示了开发人员如何轻松使用AutoGen有效地解决任务或构建应用程序，涵盖编程、数学、运筹学、娱乐、在线决策、问答等多个领域。

    This technical report presents AutoGen, a new framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools. AutoGen's design offers multiple advantages: a) it gracefully navigates the strong but imperfect generation and reasoning abilities of these LLMs; b) it leverages human understanding and intelligence, while providing valuable automation through conversations between agents; c) it simplifies and unifies the implementation of complex LLM workflows as automated agent chats. We provide many diverse examples of how developers can easily use AutoGen to effectively solve tasks or build applications, ranging from coding, mathematics, operations research, entertainment, online decision-making, question answering, etc.
    
[^117]: PokerKit: 一种用于细粒度多变体扑克游戏模拟的全面的Python库

    PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant Poker Game Simulations. (arXiv:2308.07327v1 [cs.AI])

    [http://arxiv.org/abs/2308.07327](http://arxiv.org/abs/2308.07327)

    PokerKit是一个全面的Python库，用于细粒度多变体扑克游戏模拟，提供广泛的扑克变体支持和灵活的游戏状态控制，对扑克AI开发、工具创建和在线扑克赌场实现等领域具有重要贡献。

    

    PokerKit是一个开源的Python库，旨在克服现有扑克游戏模拟和手牌评估工具的限制，这些工具通常只支持少量扑克变体，并且在游戏状态控制方面缺乏灵活性。相比之下，PokerKit通过支持广泛的扑克变体，并提供灵活的架构供用户定义自定义游戏，显著扩大了这一范围。本文详细介绍了PokerKit的设计和实现，包括其直观的编程API，多变体游戏支持以及统一的手牌评估套件在不同手牌类型间的应用。PokerKit的灵活性使其能够在扑克AI开发、工具创建和在线扑克赌场实现等多个领域中使用。PokerKit的可靠性通过静态类型检查、广泛的doctest和单元测试来确保，达到了97%的代码覆盖率。引入PokerKit代表了对该领域的重要贡献。

    PokerKit is an open-source Python library designed to overcome the restrictions of existing poker game simulation and hand evaluation tools, which typically support only a handful of poker variants and lack flexibility in game state control. In contrast, PokerKit significantly expands this scope by supporting an extensive array of poker variants and it provides a flexible architecture for users to define their custom games. This paper details the design and implementation of PokerKit, including its intuitive programmatic API, multi-variant game support, and a unified hand evaluation suite across different hand types. The flexibility of PokerKit allows for applications in diverse areas, such as poker AI development, tool creation, and online poker casino implementation. PokerKit's reliability has been established through static type checking, extensive doctests, and unit tests, achieving 97\% code coverage. The introduction of PokerKit represents a significant contribution to the field 
    
[^118]: Robustified ANNs揭示了人类类别知觉之间的虫洞

    Robustified ANNs Reveal Wormholes Between Human Category Percepts. (arXiv:2308.06887v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.06887](http://arxiv.org/abs/2308.06887)

    Robustified ANNs通过发现低范数图像扰动，揭示了在“人类假定稳定”范围内人类类别知觉之间的巨大干扰。

    

    人工神经网络(ANNs)的视觉对象类别报告对微小的对抗性图像扰动非常敏感。由于人类类别报告（又称人类知觉）被认为对同样小范数的扰动不敏感，并且在一般情况下是局部稳定的，这表明ANNs不完全是人类视觉感知的科学模型。与此一致，我们发现当标准ANN模型生成小范数图像扰动时，人类对象类别知觉确实非常稳定。然而，在这个“人类假定稳定”的范围内，我们发现Robustified ANNs可靠地发现低范数图像扰动，这些扰动强烈干扰人类知觉。这些以前无法检测的人类知觉干扰在幅度上是巨大的，接近于Robustified ANNs中观察到的敏感性水平。

    The visual object category reports of artificial neural networks (ANNs) are notoriously sensitive to tiny, adversarial image perturbations. Because human category reports (aka human percepts) are thought to be insensitive to those same small-norm perturbations -- and locally stable in general -- this argues that ANNs are incomplete scientific models of human visual perception. Consistent with this, we show that when small-norm image perturbations are generated by standard ANN models, human object category percepts are indeed highly stable. However, in this very same "human-presumed-stable" regime, we find that robustified ANNs reliably discover low-norm image perturbations that strongly disrupt human percepts. These previously undetectable human perceptual disruptions are massive in amplitude, approaching the same level of sensitivity seen in robustified ANNs. Further, we show that robustified ANNs support precise perceptual state interventions: they guide the construction of low-norm 
    
[^119]: FLASK: 基于对齐技能集的细粒度语言模型评估

    FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets. (arXiv:2307.10928v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.10928](http://arxiv.org/abs/2307.10928)

    FLASK是一种基于对齐技能集的细粒度语言模型评估协议，通过将粗级评分分解为每个指令的技能集级评分，实现了对模型性能的全面视角和提高评估的可靠性。

    

    由于指令需要与人类的价值观进行对齐，并且所需的技能集根据指令而异，因此对大型语言模型（LLMs）进行评估具有挑战性。然而，先前的研究主要集中在粗粒度评估（即基于整体偏好的评估），这限制了可解释性，因为它未考虑需要实例级技能组合的用户指令的特性。在本文中，我们介绍了FLASK（基于对齐技能集的细粒度语言模型评估），这是一种细粒度评估协议，用于人类和模型的评估，它将粗级评分分解为每个指令的技能集级评分。通过实验证明，评估的细粒度对于获得对模型性能的全面视角和提高评估的可靠性至关重要。利用FLASK，我们比较了多个开源和专有LLMs，并观察到高度相关性。

    Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlati
    
[^120]: 分层授权：朝着可行的基于授权的技能学习迈进

    Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill-Learning. (arXiv:2307.02728v1 [cs.LG])

    [http://arxiv.org/abs/2307.02728](http://arxiv.org/abs/2307.02728)

    分层授权提出了一种可以计算授权的新框架，通过引入变分下界和分层架构，实现了在短期和长期时间尺度上的授权计算，并在模拟机器人任务中得到了验证。

    

    通用智能体需要大量的技能。 授权 - 技能和状态之间的最大互信息 - 为学习大量不同技能提供了一条路径，但互信息很难优化。我们介绍了一种新的框架，分层授权，通过集成目标条件层次强化学习的概念，使得计算授权更加可行。我们的框架提供了两个具体的贡献。首先，我们介绍了一种新的变分下界，可用于计算短期视角下的授权。其次，我们引入了一个分层架构，用于计算指数时间尺度下的授权。我们在一系列模拟机器人任务中验证了该框架的贡献。在一个流行的蚂蚁导航领域，我们的四级智能体能够学习覆盖面积比之前的工作大两个数量级的技能。

    General purpose agents will require large repertoires of skills. Empowerment -- the maximum mutual information between skills and the states -- provides a pathway for learning large collections of distinct skills, but mutual information is difficult to optimize. We introduce a new framework, Hierarchical Empowerment, that makes computing empowerment more tractable by integrating concepts from Goal-Conditioned Hierarchical Reinforcement Learning. Our framework makes two specific contributions. First, we introduce a new variational lower bound on mutual information that can be used to compute empowerment over short horizons. Second, we introduce a hierarchical architecture for computing empowerment over exponentially longer time scales. We verify the contributions of the framework in a series of simulated robotics tasks. In a popular ant navigation domain, our four level agents are able to learn skills that cover a surface area over two orders of magnitude larger than prior work.
    
[^121]: 抽象文本摘要中的命名实体包含

    Named Entity Inclusion in Abstractive Text Summarization. (arXiv:2307.02570v1 [cs.CL])

    [http://arxiv.org/abs/2307.02570](http://arxiv.org/abs/2307.02570)

    该论文提出了一种解决抽象文本摘要中命名实体遗漏问题的方法，通过使用定制的预训练目标和模型训练策略，改善了命名实体的包含情况，提高了摘要的准确性和召回率。

    

    我们解决了许多当前抽象文本摘要器的缺点，即命名实体的遗漏问题。我们建议采用定制的预训练目标来增强模型对文本中的命名实体的注意力。首先，使用命名实体识别模型RoBERTa来确定文本中的命名实体。然后，使用该模型对文本中的命名实体进行屏蔽，再使用BART模型对其进行重建。接下来，将BART模型在摘要任务上进行微调。实验证明，这种预训练方法改善了命名实体包含的精确度和召回率指标。

    We address the named entity omission - the drawback of many current abstractive text summarizers. We suggest a custom pretraining objective to enhance the model's attention on the named entities in a text. At first, the named entity recognition model RoBERTa is trained to determine named entities in the text. After that, this model is used to mask named entities in the text and the BART model is trained to reconstruct them. Next, the BART model is fine-tuned on the summarization task. Our experiments showed that this pretraining approach improves named entity inclusion precision and recall metrics.
    
[^122]: MedCPT: 使用大规模PubMed搜索日志的对比预训练转换器进行零样本生物医学信息检索

    MedCPT: Contrastive Pre-trained Transformers with Large-scale PubMed Search Logs for Zero-shot Biomedical Information Retrieval. (arXiv:2307.00589v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2307.00589](http://arxiv.org/abs/2307.00589)

    MedCPT是一种用于生物医学领域零样本语义信息检索的对比预训练转换器模型。通过使用大规模PubMed搜索日志进行训练，MedCPT在六个生物医学信息检索任务中创造了新的最佳性能，超过了其他基线模型，同时还能生成更好的生物医学文章和句子。

    

    信息检索在生物医学知识获取和临床决策支持中至关重要。尽管最近的进展表明语言模型编码器在语义检索方面表现更好，但训练这些模型需要大量的查询-文章注释，在生物医学领域很难获得。因此，大多数生物医学信息检索系统只进行词汇匹配。为此，我们引入了MedCPT，这是一种首创的用于生物医学领域零样本语义信息检索的对比预训练转换器模型。为了训练MedCPT，我们从PubMed收集了255 million个用户点击日志，这是前所未有的规模。利用这些数据，我们使用对比学习来训练一对密切集成的检索器和重排器。实验结果显示，MedCPT在六个生物医学信息检索任务中取得了新的最佳性能，优于包括更大模型（如GPT-3大小的cpt-text-XL）在内的各种基线模型。此外，MedCPT还能够生成更好的生物医学文章和句子。

    Information retrieval (IR) is essential in biomedical knowledge acquisition and clinical decision support. While recent progress has shown that language model encoders perform better semantic retrieval, training such models requires abundant query-article annotations that are difficult to obtain in biomedicine. As a result, most biomedical IR systems only conduct lexical matching. In response, we introduce MedCPT, a first-of-its-kind Contrastively Pre-trained Transformer model for zero-shot semantic IR in biomedicine. To train MedCPT, we collected an unprecedented scale of 255 million user click logs from PubMed. With such data, we use contrastive learning to train a pair of closely-integrated retriever and re-ranker. Experimental results show that MedCPT sets new state-of-the-art performance on six biomedical IR tasks, outperforming various baselines including much larger models such as GPT-3-sized cpt-text-XL. In addition, MedCPT also generates better biomedical article and sentence 
    
[^123]: REFLECT:对机器人经历进行总结，以用于失败解释和纠正

    REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. (arXiv:2306.15724v1 [cs.RO])

    [http://arxiv.org/abs/2306.15724](http://arxiv.org/abs/2306.15724)

    提出了REFLECT框架，可以将机器人多感官数据转化为分层总结，并使用大型语言模型进行失败解释。该框架能够生成有益的失败解释，帮助机器人完成任务。

    

    自动检测和分析失败执行是实现可解释和稳健机器人系统的关键。最近，大型语言模型（LLM）在文本输入上展示了强大的常识推理能力。为了利用LLM的力量进行机器人失败解释，我们提出了一个框架REFLECT，将多感官数据转化为机器人过去经验的分层总结，并使用逐步失败解释算法查询LLM。基于解释，失败纠正规划器生成一个可执行计划，以纠正失败并完成任务。为了系统评估该框架，我们创建了RoboFail数据集，并展示了我们基于LLM的框架能够生成有益的失败解释，从而帮助成功的纠正规划。项目网站：https://roboreflect.github.io/

    The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong common sense reasoning skills on textual inputs. To leverage the power of LLM for robot failure explanation, we propose a framework REFLECT, which converts multi-sensory data into a hierarchical summary of robot past experiences and queries LLM with a progressive failure explanation algorithm. Conditioned on the explanation, a failure correction planner generates an executable plan for the robot to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset and show that our LLM-based framework is able to generate informative failure explanations that assist successful correction planning. Project website: https://roboreflect.github.io/
    
[^124]: GPT-4网状化学家用于MOF探索

    GPT-4 Reticular Chemist for MOF Discovery. (arXiv:2306.14915v1 [cs.AI])

    [http://arxiv.org/abs/2306.14915](http://arxiv.org/abs/2306.14915)

    GPT-4网状化学家是一个集成了AI模型GPT-4的框架，通过迭代的人工智能交互，能够发现金属有机框架系列。

    

    我们提出了一个新的框架，将AI模型GPT-4集成到网状化学实验的迭代过程中，利用AI与人类学徒之间的合作工作流。这个GPT-4网状化学家是一个由三个阶段组成的综合系统。每个阶段都以不同的方式利用GPT-4，其中GPT-4为化学实验提供详细的指导，学徒则对实验结果进行反馈，包括成功和失败，以便在下一次迭代中AI进行内部学习。这种迭代的人工智能交互使得GPT-4能够像经验丰富的化学家一样从结果中学习，采用一种提示学习策略。重要的是，该系统基于自然语言进行开发和操作，无需编码技能，因此对所有化学家都具有可访问性。我们的GPT-4网状化学家展示了一系列金属有机框架的发现。

    We present a new framework integrating the AI model GPT-4 into the iterative process of reticular chemistry experimentation, leveraging a cooperative workflow of interaction between AI and a human apprentice. This GPT-4 Reticular Chemist is an integrated system composed of three phases. Each of these utilizes GPT-4 in various capacities, wherein GPT-4 provides detailed instructions for chemical experimentation and the apprentice provides feedback on the experimental outcomes, including both success and failures, for the in-text learning of AI in the next iteration. This iterative human-AI interaction enabled GPT-4 to learn from the outcomes, much like an experienced chemist, by a prompt-learning strategy. Importantly, the system is based on natural language for both development and operation, eliminating the need for coding skills, and thus, make it accessible to all chemists. Our GPT-4 Reticular Chemist demonstrated the discovery of an isoreticular series of metal-organic frameworks (
    
[^125]: DiMSam:扩散模型作为部分可观测任务与动作规划中的采样器。

    DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability. (arXiv:2306.13196v1 [cs.RO])

    [http://arxiv.org/abs/2306.13196](http://arxiv.org/abs/2306.13196)

    本文提出了一种使用扩散模型作为采样器的任务和动作规划方法，在部分可观测下能够实现长周期受约束的操作计划。

    

    任务和动作规划（TAMP）方法非常有效地计划长周期自主机器人操作。但是，由于它们需要一个规划模型，因此在环境和其动态不完全了解的领域中应用它们可能非常困难。我们提出通过利用深度生成建模，特别是扩散模型来克服这些限制，学习捕获规划模型中难以设计的约束和采样器。这些学习采样器在TAMP求解器中组合和合并，以联合找到满足规划中约束的行动参数值。为了便于对环境中未知对象进行预测，我们将这些采样器定义为学习的低维潜变量嵌入的可变对象状态。我们在关节式物体操作领域评估了我们的方法，并展示了经典TAMP、生成学习和潜在嵌入的组合如何使得在部分可观测下进行长周期受约束的操作计划。

    Task and Motion Planning (TAMP) approaches are effective at planning long-horizon autonomous robot manipulation. However, because they require a planning model, it can be difficult to apply them to domains where the environment and its dynamics are not fully known. We propose to overcome these limitations by leveraging deep generative modeling, specifically diffusion models, to learn constraints and samplers that capture these difficult-to-engineer aspects of the planning model. These learned samplers are composed and combined within a TAMP solver in order to find action parameter values jointly that satisfy the constraints along a plan. To tractably make predictions for unseen objects in the environment, we define these samplers on low-dimensional learned latent embeddings of changing object state. We evaluate our approach in an articulated object manipulation domain and show how the combination of classical TAMP, generative learning, and latent embeddings enables long-horizon constra
    
[^126]: 随机加权梯度下降通过分布健壮优化

    Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization. (arXiv:2306.09222v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.09222](http://arxiv.org/abs/2306.09222)

    我们通过分布健壮优化和重要性加权的梯度下降技术提升了深度神经网络的性能，并在各种任务上取得了优越的结果。

    

    我们通过在每一次优化步骤中对数据点进行重要性加权，开发了一种提高深度神经网络性能的加权梯度下降技术。我们的方法受到分布健壮优化和f-散度的启发，已知可以得到具有改进的泛化保证的模型。我们的加权方案简单、计算高效，可以与许多流行的优化算法（如SGD和Adam）结合使用。实验证明，我们的方法在各种任务上都表现出了优越性能，包括监督学习和领域适应。值得注意的是，我们在DomainBed和Tabular分类基准上分别比现有最佳结果提升了0.7%和1.44%。此外，我们的算法将BERT在GLUE基准上的性能提升了1.94%，将ViT在ImageNet-1K上的性能提升了1.01%。这些结果表明了所提出方法的有效性，预示着它在改善性能方面的潜力。

    We develop a re-weighted gradient descent technique for boosting the performance of deep neural networks, which involves importance weighting of data points during each optimization step. Our approach is inspired by distributionally robust optimization with f-divergences, which has been known to result in models with improved generalization guarantees. Our re-weighting scheme is simple, computationally efficient, and can be combined with many popular optimization algorithms such as SGD and Adam. Empirically, we demonstrate the superiority of our approach on various tasks, including supervised learning, domain adaptation. Notably, we obtain improvements of +0.7% and +1.44% over SOTA on DomainBed and Tabular classification benchmarks, respectively. Moreover, our algorithm boosts the performance of BERT on GLUE benchmarks by +1.94%, and ViT on ImageNet-1K by +1.01%. These results demonstrate the effectiveness of the proposed approach, indicating its potential for improving performance in 
    
[^127]: 快速扩散模型

    Fast Diffusion Model. (arXiv:2306.06991v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.06991](http://arxiv.org/abs/2306.06991)

    本文提出了一种快速扩散模型（FDM），通过将动量集成到扩散过程中，显著加速了扩散模型（DMs）的训练和采样过程。

    

    扩散模型（DMs）以其在捕捉复杂数据分布方面的显著能力，被广泛应用于各个领域。在本文中，我们提出了一种快速扩散模型（FDM），从随机优化的角度显著加速DMs的训练和采样过程。我们首先发现DMs的扩散过程与随机梯度下降（SGD）的随机时变问题的随机优化过程相符合。然后，受到动量SGD的启发，该方法使用梯度和额外的动量，以实现比SGD更快和更稳定的收敛，我们将动量集成到DMs的扩散过程中。这带来了一个独特的挑战，即从基于动量的扩散过程中导出噪声扰动核。为此，我们将这个过程构建为一个阻尼振荡系统，临界阻尼状态-核解决方案-避免振荡，使扩散过程的收敛速度更快。实验证明，FDM在加速训练和采样过程方面取得了显著效果。

    Diffusion models (DMs) have been adopted across diverse fields with its remarkable abilities in capturing intricate data distributions. In this paper, we propose a Fast Diffusion Model (FDM) to significantly speed up DMs from a stochastic optimization perspective for both faster training and sampling. We first find that the diffusion process of DMs accords with the stochastic optimization process of stochastic gradient descent (SGD) on a stochastic time-variant problem. Then, inspired by momentum SGD that uses both gradient and an extra momentum to achieve faster and more stable convergence than SGD, we integrate momentum into the diffusion process of DMs. This comes with a unique challenge of deriving the noise perturbation kernel from the momentum-based diffusion process. To this end, we frame the process as a Damped Oscillation system whose critically damped state -- the kernel solution -avoids oscillation and yields a faster convergence speed of the diffusion process. Empirical r
    
[^128]: 基于Implicit Neural Representations的时间序列连续建模用于插值和预测

    Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations. (arXiv:2306.05880v1 [cs.LG])

    [http://arxiv.org/abs/2306.05880](http://arxiv.org/abs/2306.05880)

    该论文提出了基于INR的时间序列连续建模方法，解决了处理缺失数据、不规则采样和多传感器不对准观测等重复建模问题，并在预测和插值任务中取得了最新的性能表现，具有很好的泛化能力。

    

    尽管时间序列建模已被广泛探索，但在面对真实世界的数据时仍面临重大挑战。我们提出了一种新颖的建模方法，利用Implicit Neural Representations (INR)。该方法使我们能够有效地捕捉时间序列的连续性，并提供了自然的解决方案，以处理缺失数据、处理不规则采样或来自多个传感器的不对准观测等重复建模问题。通过引入条件调制INR参数并利用元学习技术，我们解决了模型泛化到未见样本和时间窗口移位的问题。通过大量实验，我们的模型展示了在预测和插值任务中领先的性能，同时在处理许多竞争模型无法处理的各种具有挑战性的场景方面展现了灵活性。

    Although widely explored, time series modeling continues to encounter significant challenges when confronted with real-world data. We propose a novel modeling approach leveraging Implicit Neural Representations (INR). This approach enables us to effectively capture the continuous aspect of time series and provides a natural solution to recurring modeling issues such as handling missing data, dealing with irregular sampling, or unaligned observations from multiple sensors. By introducing conditional modulation of INR parameters and leveraging meta-learning techniques, we address the issue of generalization to both unseen samples and time window shifts. Through extensive experimentation, our model demonstrates state-of-the-art performance in forecasting and imputation tasks, while exhibiting flexibility in handling a wide range of challenging scenarios that competing models cannot.
    
[^129]: 应用演绎验证技术验证思维链的推理过程

    Deductive Verification of Chain-of-Thought Reasoning. (arXiv:2306.03872v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.03872](http://arxiv.org/abs/2306.03872)

    本文旨在通过应用演绎验证技术，使语言模型能够进行明确而严谨的演绎推理，以确保其推理过程的可信度。

    

    大语言模型在各种推理任务中受益匪浅，特别是应用思维链提示可以使模型产生更全面的推理过程。然而，思维链的强调中间推理步骤可能会不慎导致产生幻觉和累积错误，从而限制模型解决复杂推理任务的能力。本文灵感来自于人类如何进行细致的演绎逻辑推理过程来解决任务，我们旨在使语言模型能够进行明确而严谨的演绎推理，并通过自我验证确保推理过程的可信度。然而，即使是像ChatGPT这样先进的模型，直接验证整个演绎推理过程的有效性也是具有挑战性的。因此，我们提出将推理验证过程分解为一系列逐步的子过程，每个过程只接收其必要的上下文和前提条件。

    Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate t
    
[^130]: 拓展可解释性视野：一种统一的基于概念的系统用于局部、全局和错误分类解释。

    Expanding Explainability Horizons: A Unified Concept-Based System for Local, Global, and Misclassification Explanations. (arXiv:2306.03531v1 [cs.CV])

    [http://arxiv.org/abs/2306.03531](http://arxiv.org/abs/2306.03531)

    本文提出了一种新的统一的基于概念的系统，旨在解决当前基于概念的可解释性方法不足的局部、全局和错误分类解释问题，该系统可以自动学习、评分和提取局部和全局概念。

    

    近年来，智能模型的可解释性越来越受到关注。在各种可解释性方法中，基于概念的技术以利用一组人类可理解的概念为特点，而不是关注于单个像素。然而，很少有方法能够同时提供局部和全局解释，并能解释错误分类情况。为了解决这些挑战，我们提出了一种简单而有效的方法。我们提出一个统一的基于概念的系统，将多个超像素图像输入网络中，使其能够更好地学习目标对象以及目标概念的表示。该方法可以自动学习、评分和提取局部和全局概念。我们的实验证明，除了提高性能外，该模型还可以深入了解预测，并阐明错误分类。

    Explainability of intelligent models has been garnering increasing attention in recent years. Of the various explainability approaches, concept-based techniques are notable for utilizing a set of human-meaningful concepts instead of focusing on individual pixels. However, there is a scarcity of methods that consistently provide both local and global explanations. Moreover, most of the methods have no offer to explain misclassification cases. To address these challenges, our study follows a straightforward yet effective approach. We propose a unified concept-based system, which inputs a number of super-pixelated images into the networks, allowing them to learn better representations of the target's objects as well as the target's concepts. This method automatically learns, scores, and extracts local and global concepts. Our experiments revealed that, in addition to enhancing performance, the models could provide deeper insights into predictions and elucidate false classifications.
    
[^131]: RepoBench：评估代码自动补全系统的代码库级别性能基准

    RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems. (arXiv:2306.03091v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.03091](http://arxiv.org/abs/2306.03091)

    RepoBench是一个评估代码自动补全系统在代码库级别上性能的基准，支持Python和Java，包含三个相互关联的评估任务。它旨在填补当前基准在多文件编程场景方面的评估差距，并为不同系统的性能提供更全面的比较。

    

    大型语言模型（LLMs）在代码自动补全系统方面取得了巨大进步，有潜力为开发人员提供显著的生产力增强。然而，当前的基准主要集中在单个文件任务上，对于更复杂的真实世界的多文件编程场景的评估存在差距。为填补这一差距，我们介绍了RepoBench，一个专门设计用于评估代码库级别的代码自动补全系统的新基准。RepoBench支持Python和Java，并包含三个相互关联的评估任务：RepoBench-R（检索）、RepoBench-C（代码补全）和RepoBench-P（流水线）。每个任务分别测量系统检索其他文件中最相关代码片段的能力、使用跨文件和文件内上下文预测下一行代码的能力以及处理需要检索和下一行预测组合的复杂任务的能力。RepoBench旨在促进对性能的更全面比较。

    Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance
    
[^132]: LLMatic: 基于大语言模型和多样性优化的神经结构搜索

    LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])

    [http://arxiv.org/abs/2306.01102](http://arxiv.org/abs/2306.01102)

    本文介绍了利用大语言模型和多样性优化算法相结合的 LLMatic 神经结构搜索算法。该算法在CIFAR-10数据集进行测试，仅进行2000次搜索即可产生高性能网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。

    

    大型语言模型 (LLMs) 已成为一种强大的工具，可以完成广泛的任务。它们的能力涵盖了许多领域，它们在代码生成领域产生了重大影响。在此情况下，我们将 LLMs 视为变异和交叉工具。同时，多样性优化算法已知可以发现多样性和稳健的解决方案。通过将 LLMs 的代码生成能力与 QD 解决方案的多样性和鲁棒性相结合，我们引入了 LLMatic，一个神经结构搜索 (NAS) 算法。虽然 LLMs 通过提示直接进行 NAS 考验困难，但 LLMatic 利用程序化方法，利用 QD 来进行提示和网络结构，从而创建多样性和高性能网络。我们在 CIFAR-10 图像分类基准测试中测试了 LLMatic，证明它可以在仅进行 2000 次搜索的情况下产生具有竞争力的网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。

    Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
    
[^133]: 大批量神经多目标贝叶斯优化

    Large-Batch, Neural Multi-Objective Bayesian Optimization. (arXiv:2306.01095v1 [cs.LG])

    [http://arxiv.org/abs/2306.01095](http://arxiv.org/abs/2306.01095)

    本文提出了一种针对数据密集型问题和多目标优化设置的贝叶斯优化框架，该方法利用了贝叶斯神经网络代理建模和可扩展、具有不确定性的收购策略，能够在最少迭代次数的情况下高效地进行优化。

    

    贝叶斯优化在全局优化黑盒高成本函数方面提供了强大的框架。然而，由于默认高斯过程代理的可扩展性差，它在处理数据密集型问题，特别是在多目标设置中的能力有限。本文提出了一种新颖的贝叶斯优化框架，专为解决这些限制而设计。我们的方法利用了贝叶斯神经网络方法进行代理建模。这使得它能够有效地处理大批量数据，建模复杂问题以及产生预测的不确定性。此外，我们的方法结合了一种基于众所周知且易于部署的NSGA-II的可扩展的、具有不确定性的收购策略。这种完全可并行化的策略促进了未勘探区域的有效探索。我们的框架允许在最少迭代次数的情况下在数据密集环境中进行有效的优化。我们展示了我们方法的优越性。

    Bayesian optimization provides a powerful framework for global optimization of black-box, expensive-to-evaluate functions. However, it has a limited capacity in handling data-intensive problems, especially in multi-objective settings, due to the poor scalability of default Gaussian Process surrogates. We present a novel Bayesian optimization framework specifically tailored to address these limitations. Our method leverages a Bayesian neural networks approach for surrogate modeling. This enables efficient handling of large batches of data, modeling complex problems, and generating the uncertainty of the predictions. In addition, our method incorporates a scalable, uncertainty-aware acquisition strategy based on the well-known, easy-to-deploy NSGA-II. This fully parallelizable strategy promotes efficient exploration of uncharted regions. Our framework allows for effective optimization in data-intensive environments with a minimum number of iterations. We demonstrate the superiority of ou
    
[^134]: 基于递增分辨率的量化随机梯度 langevin 动力学

    Stochastic Gradient Langevin Dynamics Based on Quantization with Increasing Resolution. (arXiv:2305.18864v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18864](http://arxiv.org/abs/2305.18864)

    本文提出了一种基于递增分辨率的量化随机梯度 langevin 动力学方法，通过利用 langevin 随机微分方程动力学，实现了具有可控噪声且具有相同分布的优化过程，无需添加噪声或调整小批量的大小。实验结果证明了该方法在不同数据集上对卷积神经网络模型和 ResNet-50 架构的有效性。

    

    基于 langevin 或 levy 随机微分方程的随机学习动力学通过改变小批量的大小或直接注入噪声的大小来控制噪声的方差。由于噪声方差会影响近似性能，在基于 SDE 的学习和实际实现中，添加噪声的设计很重要。本文提出了一种基于量化优化的随机下降学习方程，采用随机分析的视角，用于非凸目标函数。所提出的方法采用了一种利用 langevin SDE 动力学的量化优化方法，可以实现具有相同分布的可控噪声，而无需添加噪声或调整小批量的大小。数值实验证明了所提出算法在各种数据集上对 vanilla 卷积神经网络（CNN）模型和 ResNet-50 架构的有效性。

    Stochastic learning dynamics based on Langevin or Levy stochastic differential equations (SDEs) in deep neural networks control the variance of noise by varying the size of the mini-batch or directly those of injecting noise. Since the noise variance affects the approximation performance, the design of the additive noise is significant in SDE-based learning and practical implementation. In this paper, we propose an alternative stochastic descent learning equation based on quantized optimization for non-convex objective functions, adopting a stochastic analysis perspective. The proposed method employs a quantized optimization approach that utilizes Langevin SDE dynamics, allowing for controllable noise with an identical distribution without the need for additive noise or adjusting the mini-batch size. Numerical experiments demonstrate the effectiveness of the proposed algorithm on vanilla convolution neural network(CNN) models and the ResNet-50 architecture across various data sets. Fur
    
[^135]: DNA-GPT: 无需训练的Divergent N-Gram分析用于检测GPT生成的文本

    DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text. (arXiv:2305.17359v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17359](http://arxiv.org/abs/2305.17359)

    DNA-GPT是一种无需训练的检测策略，通过Divergent N-Gram分析来发现机器生成文本与人类写作文本之间的显著差异。

    

    大型语言模型（LLMs）显着提高了机器生成文本的流畅性和多样性。然而，这一进展也给检测给定文本的来源带来了重大挑战，并且目前的检测方法研究滞后于LLMs的快速发展。传统的基于训练的方法在灵活性方面存在局限性，特别是在适应新领域时，它们往往缺乏解释能力。为了弥补这一差距，我们提出了一种新颖的无需训练的检测策略，称为Divergent N-Gram分析（DNA-GPT）。给定一段文本，我们首先截断其中间部分，然后仅使用前面的部分作为输入来重新生成剩下的部分。通过在黑盒中进行N-gram分析或在白盒中进行概率分布差异分析，我们揭示了机器生成文本的分布与人类写作文本的分布之间的显著差异。我们进行了实证研究...

    Large language models (LLMs) have notably enhanced the fluency and diversity of machine-generated text. However, this progress also presents a significant challenge in detecting the origin of a given text, and current research on detection methods lags behind the rapid evolution of LLMs. Conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power. To address this gap, we propose a novel training-free detection strategy called Divergent N-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and then use only the preceding portion as input to the LLMs to regenerate the new remaining parts. By analyzing the differences between the original and new remaining parts through N-gram analysis in black-box or probability divergence in white-box, we unveil significant discrepancies between the distribution of machine-generated text and the distribution of human-written text. We conducted
    
[^136]: 面向不均衡数据的鲁棒基于模型的设计的属性引导生成建模

    Property-Guided Generative Modelling for Robust Model-Based Design with Imbalanced Data. (arXiv:2305.13650v1 [cs.LG])

    [http://arxiv.org/abs/2305.13650](http://arxiv.org/abs/2305.13650)

    本文提出了一种属性引导的变分自编码器（PGVAE），通过属性值明确结构化潜在空间，使得MBO可以在不平衡数据上稳健地寻找具有改进属性的序列。

    

    设计具有特定属性的蛋白质序列是一项具有挑战性的任务，因为这需要探索具有极度稀疏的有意义区域的高维蛋白质序列空间。这导致了模型优化（MBO）技术的发展，通过使用由序列空间中的属性引导的有效搜索模型来辅助设计。然而，实验获得的数据集的内在不平衡性使得现有的MBO方法很难或根本无法处理。我们提出了一种属性引导的变分自编码器（PGVAE），其潜在空间由属性值明确结构化，使得按照这些属性值优先考虑样本。通过对真实和半合成蛋白质数据集的广泛基准测试，我们展示了MBO与PGVAE稳健地发现具有改进属性的序列，尽管数据集存在显著的不平衡性。我们进一步展示了我们的方法对于连续设计空间的普适性及其稳健性。

    The problem of designing protein sequences with desired properties is challenging, as it requires to explore a high-dimensional protein sequence space with extremely sparse meaningful regions. This has led to the development of model-based optimization (MBO) techniques that aid in the design, by using effective search models guided by the properties over the sequence space. However, the intrinsic imbalanced nature of experimentally derived datasets causes existing MBO approaches to struggle or outright fail. We propose a property-guided variational auto-encoder (PGVAE) whose latent space is explicitly structured by the property values such that samples are prioritized according to these properties. Through extensive benchmarking on real and semi-synthetic protein datasets, we demonstrate that MBO with PGVAE robustly finds sequences with improved properties despite significant dataset imbalances. We further showcase the generality of our approach to continuous design spaces, and its rob
    
[^137]: 一种参数高效的学习方法，用于带有预训练通用语音模型的阿拉伯方言识别

    A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model. (arXiv:2305.11244v1 [cs.CL])

    [http://arxiv.org/abs/2305.11244](http://arxiv.org/abs/2305.11244)

    本文介绍了一种利用预训练通用语音模型进行阿拉伯方言识别的参数高效学习方法，通过残差适配器和模型重编程，设计了一个基于记号的标签映射，并在ADI-17数据集上实现了最高精度，同时使用PEL方法进一步减少了训练成本。

    

    本文研究了参数高效学习（PEL）技术，以重新利用通用语音模型（GSM）进行阿拉伯方言识别（ADI）。我们设计了一个基于记号的标签映射，将GSM适应于阿拉伯方言识别，通过残差适配器和模型重编程来实现。我们通过vanilla fine-tuning在ADI-17数据集上实现了新的最高精度。此外，我们通过PEL方法进一步减少了训练成本，使用额外2.5％的网络可训练参数即可达到fine-tuning精度的1.86％。我们的研究展示了如何使用小型数据集和有限的计算资源来识别阿拉伯方言。

    In this work, we explore Parameter-Efficient-Learning (PEL) techniques to repurpose a General-Purpose-Speech (GSM) model for Arabic dialect identification (ADI). Specifically, we investigate different setups to incorporate trainable features into a multi-layer encoder-decoder GSM formulation under frozen pre-trained settings. Our architecture includes residual adapter and model reprogramming (input-prompting). We design a token-level label mapping to condition the GSM for Arabic Dialect Identification (ADI). This is challenging due to the high variation in vocabulary and pronunciation among the numerous regional dialects. We achieve new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We further reduce the training budgets with the PEL method, which performs within 1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable parameters. Our study demonstrates how to identify Arabic dialects using a small dataset and limited computation with open sou
    
[^138]: PDP：无需参数的可微剪枝即可搞定

    PDP: Parameter-free Differentiable Pruning is All You Need. (arXiv:2305.11203v1 [cs.LG])

    [http://arxiv.org/abs/2305.11203](http://arxiv.org/abs/2305.11203)

    PDP提出了一种无需参数的可微剪枝方案，具有最先进的模型大小、准确性和训练成本，适用于各种视觉和自然语言任务。

    

    DNN剪枝是一种常用的方法，可以减少模型的大小，提高推理延迟，并最小化DNN加速器上的功耗。然而，现有的方法可能过于复杂、昂贵或无法适用于各种视觉/语言任务、DNN体系结构并遵守结构化剪枝约束。在本文中，我们提出了一种高效而有效的训练时间剪枝方案——PDP（参数自由可微剪枝），它在模型大小、准确性和训练成本方面具有最先进的性能。PDP在训练过程中使用权重的动态函数，以参数无关的方式为给定的剪枝目标生成软剪枝掩码。虽然是可微的，但是PDP的简单和高效使其足够普遍，以在各种视觉和自然语言任务上提供最先进的随机/结构化/通道剪枝结果。例如，对于MobileNet-v1，PDP可以在86.6%的稀疏度下达到68.2%的ImageNet1k top-1准确率。

    DNN pruning is a popular way to reduce the size of a model, improve the inference latency, and minimize the power consumption on DNN accelerators. However, existing approaches might be too complex, expensive or ineffective to apply to a variety of vision/language tasks, DNN architectures and to honor structured pruning constraints. In this paper, we propose an efficient yet effective train-time pruning scheme, Parameter-free Differentiable Pruning (PDP), which offers state-of-the-art qualities in model size, accuracy, and training cost. PDP uses a dynamic function of weights during training to generate soft pruning masks for the weights in a parameter-free manner for a given pruning target. While differentiable, the simplicity and efficiency of PDP make it universal enough to deliver state-of-the-art random/structured/channel pruning results on various vision and natural language tasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1 ImageNet1k accuracy at 86.6% sparsity, wh
    
[^139]: 个性化一次性分割模型

    Personalize Segment Anything Model with One Shot. (arXiv:2305.03048v1 [cs.CV])

    [http://arxiv.org/abs/2305.03048](http://arxiv.org/abs/2305.03048)

    本文提出了一种无需训练的SAM个性化方法PerSAM，只需要一张带有参考掩模的单张图像即可定位和分割目标概念，还提出了高效的一次性微调变体PerSAM-F，旨在解决掩模不确定性问题。

    

    在大数据预训练的推动下，分割任何物体模型（SAM）已被证明是一个强大且高效的框架，革新了分割模型领域。尽管SAM非常通用，但自动为特定视觉概念定制SAM而不需要手动提示，如在不同图像中自动分割你的宠物狗等， 还未深入研究。本文提出了一种无需训练的SAM个性化方法，称为PerSAM。只需要一张带有参考掩模的单张图像，PerSAM首先通过位置先验定位目标概念，并通过三种技术来在其他图像或视频中分割它：目标引导注意力，目标语义提示和级联后处理。这样，我们有效地适应了SAM的私人使用而无需任何训练。为了进一步缓解掩模的不确定性，我们提出了一个高效的一次性微调变体，即PerSAM-F。冻结整个SAM，我们引入了两个可学习权重用于多尺度掩模，仅训练2个参数即可。

    Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promptable framework, revolutionizing the segmentation models. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under explored, e.g., automatically segmenting your pet dog in different images. In this paper, we propose a training-free Personalization approach for SAM, termed as PerSAM. Given only a single image with a reference mask, PerSAM first localizes the target concept by a location prior, and segments it within other images or videos via three techniques: target-guided attention, target-semantic prompting, and cascaded post-refinement. In this way, we effectively adapt SAM for private use without any training. To further alleviate the mask ambiguity, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce two learnable weights for multi-scale masks, only training 2 parameters wit
    
[^140]: 在深度神经网络中预防性修剪Clever Hans策略

    Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks. (arXiv:2304.05727v1 [cs.LG])

    [http://arxiv.org/abs/2304.05727](http://arxiv.org/abs/2304.05727)

    本文提出了一种新方法，Explanation-Guided Exposure Minimization (EGEM)，该方法预防性地修剪了ML模型中未受到积极解释反馈的变化，从而大大减少了对隐藏Clever Hans策略的依赖，并实现了更高的性能。

    

    可解释的AI已成为验证机器学习模型的流行工具。解释模型的决策策略与用户的领域知识之间的不匹配（例如Clever Hans效应）也被认为是改进错误模型的起点。然而，当用户和解释达成一致时，要怎么做就不那么清楚了。本文通过展示用户接受解释并不保证ML模型的良好功能，特别是一些隐藏的Clever Hans效应可能仍然未被发现，证明了这一点。我们通过贡献一个新方法Explanation-Guided Exposure Minimization (EGEM)，该方法预防性地修剪了ML模型中未受到积极解释反馈的变化。自然画像数据的实验表明，我们的方法导致模型大大减少了对隐藏的Clever Hans策略的依赖，并因此实现了更高的性能。

    Explainable AI has become a popular tool for validating machine learning models. Mismatches between the explained model's decision strategy and the user's domain knowledge (e.g. Clever Hans effects) have also been recognized as a starting point for improving faulty models. However, it is less clear what to do when the user and the explanation agree. In this paper, we demonstrate that acceptance of explanations by the user is not a guarantee for a ML model to function well, in particular, some Clever Hans effects may remain undetected. Such hidden flaws of the model can nevertheless be mitigated, and we demonstrate this by contributing a new method, Explanation-Guided Exposure Minimization (EGEM), that premptively prunes variations in the ML model that have not been the subject of positive explanation feedback. Experiments on natural image data demonstrate that our approach leads to models that strongly reduce their reliance on hidden Clever Hans strategies, and consequently achieve hig
    
[^141]: 一种针对弱监督学习的基准生成性概率模型

    A Benchmark Generative Probabilistic Model for Weak Supervised Learning. (arXiv:2303.17841v1 [cs.LG])

    [http://arxiv.org/abs/2303.17841](http://arxiv.org/abs/2303.17841)

    本文提出一种基准生成性概率模型，在启发式标注的原始数据集上训练，生成伪标签作为一种准确、快速、经济的弱监督学习方法，在图像分类和自然语言处理中达到了最先进的表现。

    

    寻找相关高质量的数据集来训练机器学习模型对于实践者来说是一个主要 bottleneck。而且，为了解决野心勃勃实际应用场景下的问题，数据通常需要附带带有高质量注释的标签，以便于监督模型的训练。手动标记具有高质量标签的数据通常是一项耗时且具有挑战性的任务，往往成为机器学习项目的瓶颈。弱监督学习 (WSL) 方法已被开发出来，通过根据启发式、远程监视和知识库来赋予未标记数据大约标签 (伪标签) 的自动方式，从而减轻注释负担。我们应用概率生成隐变量模型 (PLVMs)，在启发式标注表示的原始数据集上进行训练，作为一种生成伪标签的准确、快速、经济的方式。我们展示了 PLVMs 在图像分类中的多个基准数据集上实现了最先进的表现，并展示了它们在自然语言处理中的事件检测任务上的多才多艺。

    Finding relevant and high-quality datasets to train machine learning models is a major bottleneck for practitioners. Furthermore, to address ambitious real-world use-cases there is usually the requirement that the data come labelled with high-quality annotations that can facilitate the training of a supervised model. Manually labelling data with high-quality labels is generally a time-consuming and challenging task and often this turns out to be the bottleneck in a machine learning project. Weak Supervised Learning (WSL) approaches have been developed to alleviate the annotation burden by offering an automatic way of assigning approximate labels (pseudo-labels) to unlabelled data based on heuristics, distant supervision and knowledge bases. We apply probabilistic generative latent variable models (PLVMs), trained on heuristic labelling representations of the original dataset, as an accurate, fast and cost-effective way to generate pseudo-labels. We show that the PLVMs achieve state-of-
    
[^142]: 变分自编码器中逐步减少信息瓶颈的去纠缠方法

    Variantional autoencoder with decremental information bottleneck for disentanglement. (arXiv:2303.12959v1 [cs.LG])

    [http://arxiv.org/abs/2303.12959](http://arxiv.org/abs/2303.12959)

    本论文提出了一种逐步减少信息瓶颈的变分自编码器方法，使用去纠缠不变变换来平衡去纠缠和重构保真度，避免信息扩散问题。

    

    变分自编码器中去纠缠学习的一个主要挑战是在权衡去纠缠和重构保真度之间的平衡。之前仅在一个潜在空间中进行的逐步方法无法同时优化这两个目标，因此在训练过程中扩展了信息瓶颈，以从去纠缠到重构进行优化。然而，大型瓶颈会失去去纠缠的约束，导致信息扩散问题。为了解决这个问题，我们提出了一种新颖的逐步减少信息瓶颈的变分自编码器方法，使用去纠缠不变变换来优化不同层的多个目标，称为DeVAE。通过逐渐减小不同潜在空间的信息瓶颈，DeVAE 平衡了去纠缠和重构保真度。由于具有多个潜在空间，DeVAE 允许同时优化多个目标，以在保持去纠缠约束的同时优化重构，避免信息扩散问题。

    One major challenge of disentanglement learning with variational autoencoders is the trade-off between disentanglement and reconstruction fidelity. Previous incremental methods with only on latent space cannot optimize these two targets simultaneously, so they expand the Information Bottleneck while training to {optimize from disentanglement to reconstruction. However, a large bottleneck will lose the constraint of disentanglement, causing the information diffusion problem. To tackle this issue, we present a novel decremental variational autoencoder with disentanglement-invariant transformations to optimize multiple objectives in different layers, termed DeVAE, for balancing disentanglement and reconstruction fidelity by decreasing the information bottleneck of diverse latent spaces gradually. Benefiting from the multiple latent spaces, DeVAE allows simultaneous optimization of multiple objectives to optimize reconstruction while keeping the constraint of disentanglement, avoiding info
    
[^143]: 文本-视觉提示用于高效的二维时间视频定位

    Text-Visual Prompting for Efficient 2D Temporal Video Grounding. (arXiv:2303.04995v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.04995](http://arxiv.org/abs/2303.04995)

    本文提出了一种新颖的文本-视觉提示（TVP）框架来解决时间视频定位问题，该框架有效地共同训练视觉编码器和语言编码器，且使用只有低复杂度稀疏二维视觉特征来提高跨模态特征融合的性能，并提出了一种时态对话排名（TDR）训练策略用于监督TVP的学习，实验证明该框架有效且高效。

    

    本文研究了时间视频定位（TVG）的问题，旨在预测文字描述的时刻在长时间未修剪的视频中的开始/结束时间点。受益于细粒度的三维视觉特征，TVG技术在近年来取得了显着进展。然而，三维卷积神经网络（CNNs）的高复杂性使得提取密集的三维视觉特征是耗时的，这需要大量的内存和计算资源。针对高效的TVG，作者提出了一种新颖的文本-视觉提示（TVP）框架，并将优化的扰动模式（称为“提示”）结合到TVG模型的视觉输入和文本特征中。与三维CNN形成鲜明对比，作者表明TVP允许我们在二维TVG模型中有效地共同训练视觉编码器和语言编码器，并使用只有低复杂度稀疏二维视觉特征来提高跨模态特征融合的性能。此外，作者提出了一种时态对话排名（TDR）训练策略，以监督TVP的学习并促进视频段与相应文本查询的对齐。在两个基准数据集上的实验结果验证了TVP模型的有效性和高效性。

    In this paper, we study the problem of temporal video grounding (TVG), which aims to predict the starting/ending time points of moments described by a text sentence within a long untrimmed video. Benefiting from fine-grained 3D visual features, the TVG techniques have achieved remarkable progress in recent years. However, the high complexity of 3D convolutional neural networks (CNNs) makes extracting dense 3D visual features time-consuming, which calls for intensive memory and computing resources. Towards efficient TVG, we propose a novel text-visual prompting (TVP) framework, which incorporates optimized perturbation patterns (that we call 'prompts') into both visual inputs and textual features of a TVG model. In sharp contrast to 3D CNNs, we show that TVP allows us to effectively co-train vision encoder and language encoder in a 2D TVG model and improves the performance of crossmodal feature fusion using only low-complexity sparse 2D visual features. Further, we propose a Temporal-Di
    
[^144]: 正则化神经网络模拟人类洞察力

    Regularised neural networks mimic human insight. (arXiv:2302.11351v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.11351](http://arxiv.org/abs/2302.11351)

    本文研究了正则化神经网络是否具有类似于人类洞察力的行为。研究发现，正则化神经网络在学习动态和行为特征上密切模仿了人类的洞察力，表现出洞察力的延迟、突然性和选择性发生。

    

    人类有时会展现出突然提高任务表现的情况，这与洞察力时刻相关。这种洞察力相关的性能提升似乎很特殊，因为它们前面有一个较长时间的僵局，变化异常突然，并且只发生在一部分学习者身上。本文探讨了使用梯度下降算法训练的人工神经网络中是否也存在类似洞察力行为。我们通过一项感知决策任务比较了人类和正则化神经网络的学习动态，该任务提供了一个隐藏的机会，可以更有效地解决任务。我们发现人类倾向于通过洞察力发现这种规律，而不是逐渐发现。值得注意的是，带有正则化门控调节的神经网络紧密模仿了人类洞察力的行为特征，表现出洞察力的延迟、突然性和选择性发生。网络学习动态的分析揭示了洞察力行为关键地取决于噪声添加。

    Humans sometimes show sudden improvements in task performance that have been linked to moments of insight. Such insight-related performance improvements appear special because they are preceded by an extended period of impasse, are unusually abrupt, and occur only in some, but not all, learners. Here, we ask whether insight-like behaviour also occurs in artificial neural networks trained with gradient descent algorithms. We compared learning dynamics in humans and regularised neural networks in a perceptual decision task that provided a hidden opportunity which allowed to solve the task more efficiently. We show that humans tend to discover this regularity through insight, rather than gradually. Notably, neural networks with regularised gate modulation closely mimicked behavioural characteristics of human insights, exhibiting delay of insight, suddenness and selective occurrence. Analyses of network learning dynamics revealed that insight-like behaviour crucially depended on noise adde
    
[^145]: 用于评估图像模型的具有XAI基准的数据集生成新方法

    A novel approach to generate datasets with XAI ground truth to evaluate image models. (arXiv:2302.05624v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05624](http://arxiv.org/abs/2302.05624)

    该论文介绍了一种新方法来生成具有XAI基准的数据集，用于评估图像模型。通过与真实模型解释进行比较，实验证实了该方法的可靠性。

    

    随着人工智能（AI）的广泛使用，了解这些模型的内部工作机制变得至关重要。这些需求推动了一种名为可解释人工智能（XAI）的新领域的发展。该领域涵盖了一系列技术，使我们能够理论上确定AI决策的原因。XAI的一个主要问题是如何验证该领域的工作，考虑到缺乏基准（GT）。在本研究中，我们提出了一种新的方法来生成带有GT的数据集。我们进行了一系列实验，将我们的GT与真实模型解释进行比较，并获得了卓越的结果，证实我们提出的方法是正确的。

    With the increased usage of artificial intelligence (AI), it is imperative to understand how these models work internally. These needs have led to the development of a new field called eXplainable artificial intelligence (XAI). This field consists of on a set of techniques that allows us to theoretically determine the cause of the AI decisions. One main issue of XAI is how to verify the works on this field, taking into consideration the lack of ground truth (GT). In this study, we propose a new method to generate datasets with GT. We conducted a set of experiments that compared our GT with real model explanations and obtained excellent results confirming that our proposed method is correct.
    
[^146]: RayNet: 一个用于开发强化学习驱动的网络协议的仿真平台

    RayNet: A Simulation Platform for Developing Reinforcement Learning-Driven Network Protocols. (arXiv:2302.04519v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2302.04519](http://arxiv.org/abs/2302.04519)

    RayNet是一个可扩展且适应性强的仿真平台，用于开发基于强化学习的网络协议，帮助研究者有序地进行协议的开发，而无需关注具体实现细节。

    

    强化学习在网络协议的开发中已经取得了重要的进展。然而，基于强化学习的协议仍处于初级阶段，需要大量的研究来构建可部署的解决方案。基于强化学习开发协议是一个复杂而具有挑战性的过程，涉及多个模型设计决策，并需要在真实和模拟的网络拓扑中进行重要的训练和评估。网络模拟器为基于强化学习的协议提供了高效的训练环境，因为它们是确定性的，可以并行运行。在本文中，我们介绍了一个可扩展和适应性强的仿真平台RayNet，用于开发基于强化学习的网络协议。RayNet将全面可编程的网络模拟器OMNeT++与可扩展的分布式强化学习训练平台Ray/RLlib进行了整合。RayNet旨在有序地开发基于强化学习的网络协议，使得研究者可以专注于问题本身而不是实现细节。

    Reinforcement Learning (RL) has gained significant momentum in the development of network protocols. However, RL-based protocols are still in their infancy, and substantial research is required to build deployable solutions. Developing a protocol based on RL is a complex and challenging process that involves several model design decisions and requires significant training and evaluation in real and simulated network topologies. Network simulators offer an efficient training environment for RL-based protocols, because they are deterministic and can run in parallel. In this paper, we introduce \textit{RayNet}, a scalable and adaptable simulation platform for the development of RL-based network protocols. RayNet integrates OMNeT++, a fully programmable network simulator, with Ray/RLlib, a scalable training platform for distributed RL. RayNet facilitates the methodical development of RL-based network protocols so that researchers can focus on the problem at hand and not on implementation d
    
[^147]: 基于频率变换的深度学习时间序列分析综述

    A Survey on Deep Learning based Time Series Analysis with Frequency Transformation. (arXiv:2302.02173v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02173](http://arxiv.org/abs/2302.02173)

    近期，频率变换（FT）在深度学习时间序列分析中得到广泛应用，显著提高了准确性和效率。本文系统回顾和总结了基于FT的深度学习时间序列模型的研究进展，并探讨了其优势、限制以及主要方法。

    

    最近，频率变换（FT）越来越多地被纳入深度学习模型中，可以显著提高时间序列分析的最新准确性和效率。频率变换的优势，如高效性和全局视角，在各种时间序列任务和应用中被迅速探索和利用，展示了频率变换作为一种新的深度学习范式在时间序列分析领域的潜力。尽管这个新兴领域受到了越来越多的关注和研究，但目前还缺乏对基于频率变换的深度学习时间序列模型的系统回顾和深入分析。目前还不清楚为什么频率变换可以提升时间序列分析的效果，以及它在该领域的限制是什么。为了填补这些空白，我们提供了一份全面的综述，系统调查和总结了基于频率变换的深度学习时间序列分析的最新研究进展。具体而言，我们探讨了主要的方法。

    Recently, frequency transformation (FT) has been increasingly incorporated into deep learning models to significantly enhance state-of-the-art accuracy and efficiency in time series analysis. The advantages of FT, such as high efficiency and a global view, have been rapidly explored and exploited in various time series tasks and applications, demonstrating the promising potential of FT as a new deep learning paradigm for time series analysis. Despite the growing attention and the proliferation of research in this emerging field, there is currently a lack of a systematic review and in-depth analysis of deep learning-based time series models with FT. It is also unclear why FT can enhance time series analysis and what its limitations in the field are. To address these gaps, we present a comprehensive review that systematically investigates and summarizes the recent research advancements in deep learning-based time series analysis with FT. Specifically, we explore the primary approaches us
    
[^148]: 双排列等变性在知识图谱补全中的应用

    Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01313](http://arxiv.org/abs/2302.01313)

    本研究提出了双排列等变性的KG表示方法，可以使神经网络在KG中执行复杂的逻辑推理任务，并在多个归纳KG完成任务中实现了最先进的Hits@10测试准确率。双排列等变性在KG中开辟了新的研究方向。

    

    本研究将知识图谱(KGs)形式化为一种新型的图，并称之为双交换属性图，其中节点和二元（两个节点之间的）表示必须对节点号和边（及节点）属性（关系和节点特征）的排列等变。双重排列等变的KG表示在KG中开辟了新的研究方向。我们展示了这种等变性对关系的结构表示产生的影响，从而使神经网络能够在KG中执行复杂的逻辑推理任务。最后，我们介绍了一种通用的等变表示蓝图，并测试了一种简单的基于GNN的双排列等变神经结构，在WN18RR、FB237和NELL995归纳KG完成任务中实现了最先进的Hits@10测试准确率，并能够准确执行现有方法无法执行的逻辑推理任务。

    This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (& node) attributes (relations & node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
    
[^149]: 用反事实推理解释$\mathcal{ELH}$概念描述

    Explaining $\mathcal{ELH}$ Concept Descriptions through Counterfactual Reasoning. (arXiv:2301.05109v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.05109](http://arxiv.org/abs/2301.05109)

    本研究提出了一种通过反事实推理来解释概念描述的方法，以提供简洁且易于理解的解释，便于非专家理解和采取行动。

    

    知识库被广泛应用于信息管理，能够支持高影响力的应用程序如网络搜索、问答和自然语言处理。它们也作为自动决策系统的基础，例如医疗诊断和信用评分。由于受到这些决策影响的利益相关者希望了解他们的情况并验证决策的公平性，因此提出了许多解释方法。描述逻辑中使用概念来进行分类是一种固有透明的方式。然而，即使在口头化的情况下，这些概念也会变得冗长且难以理解，特别是对于非专家而言。一种解决方法是使用反事实来回答问题：“为了得到不同的分类，特征值应如何改变？”通过关注最小的特征变化，解释变得短小、易于理解，并提供了关于变化对预测的影响的明确行动路径。

    Knowledge bases are widely used for information management, enabling high-impact applications such as web search, question answering, and natural language processing. They also serve as the backbone for automatic decision systems, e.g., for medical diagnostics and credit scoring. As stakeholders affected by these decisions would like to understand their situation and verify how fair the decisions are, a number of explanation approaches have been proposed. An intrinsically transparent way to do classification is by using concepts in description logics. However, these concepts can become long and difficult to fathom for non-experts, even when verbalized. One solution is to employ counterfactuals to answer the question, ``How must feature values be changed to obtain a different classification?'' By focusing on the minimal feature changes, the explanations are short, human-friendly, and provide a clear path of action regarding the change in prediction. While previous work investigated coun
    
[^150]: PuzzleFusion：释放扩散模型在空间拼图解决中的威力

    PuzzleFusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving. (arXiv:2211.13785v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.13785](http://arxiv.org/abs/2211.13785)

    本文提出了一个名为"PuzzleFusion"的神经架构，基于扩散模型，用于解决空间拼图和房间布局任务。在实验中，他们发现简单使用扩散模型可以有效地解决这些具有挑战性的空间拼图任务。

    

    本文提出了一种基于扩散模型的端到端神经架构，用于解决空间拼图和房间布局任务。所提出的系统"PuzzleFusion"针对后者任务，将一组房间布局视为俯视图中的多边形曲线，并通过估计它们的二维平移和旋转来对齐房间布局块，类似于解决空间拼图的过程。本文令人惊讶的发现是，扩散模型的简单使用有效地将这些具有挑战性的空间拼图任务解决为条件生成过程。为了实现端到端神经系统的学习，本文引入了新的数据集，其中包含有地面实况安排的数据集：1）2D Voronoi 拼图数据集，这是一种通过 2D 点集的 Voronoi 图生成拼图块的合成数据集；以及2）MagicPlan 数据集，这是一种从 MagicPlan 生产线提供的真实数据集，其中拼图块是由增强现实应用程序构建的房间布局。

    This paper presents an end-to-end neural architecture based on Diffusion Models for spatial puzzle solving, particularly jigsaw puzzle and room arrangement tasks. In the latter task, for instance, the proposed system ``PuzzleFusion'' takes a set of room layouts as polygonal curves in the top-down view and aligns the room layout pieces by estimating their 2D translations and rotations, akin to solving the jigsaw puzzle of room layouts. A surprising discovery of the paper is that the simple use of a Diffusion Model effectively solves these challenging spatial puzzle tasks as a conditional generation process. To enable learning of an end-to-end neural system, the paper introduces new datasets with ground-truth arrangements: 1) 2D Voronoi jigsaw dataset, a synthetic one where pieces are generated by Voronoi diagram of 2D pointset; and 2) MagicPlan dataset, a real one offered by MagicPlan from its production pipeline, where pieces are room layouts constructed by augmented reality App by rea
    
[^151]: PersA-FL：个性化异步联邦学习

    PersA-FL: Personalized Asynchronous Federated Learning. (arXiv:2210.01176v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01176](http://arxiv.org/abs/2210.01176)

    本论文研究了异步更新下的个性化联邦学习问题，并提出了一种改进的个性化方法，通过移除同步通信假设和去除梯度范数有界性假设来提高可伸缩性。

    

    我们研究了异步更新下的个性化联邦学习问题。在这个问题中，每个客户端都希望获得一个个性化模型，同时能够优于本地模型和全局模型。我们考虑了两个基于优化的个性化框架：（i）模型无关元学习（MAML）和（ii）Moreau包络（ME）。MAML通过微调学习适应于每个客户端的联合模型，而ME通过隐式梯度的双层优化问题来通过规范化损失实现个性化。我们的主要技术贡献是对有界滞后的异步联邦学习进行统一证明，并将其应用于MAML和ME个性化框架。针对平滑和非凸函数类，我们进一步扩展了所研究的函数类，去除了梯度范数的有界性假设。

    We study the personalized federated learning problem under asynchronous updates. In this problem, each client seeks to obtain a personalized model that simultaneously outperforms local and global models. We consider two optimization-based frameworks for personalization: (i) Model-Agnostic Meta-Learning (MAML) and (ii) Moreau Envelope (ME). MAML involves learning a joint model adapted for each client through fine-tuning, whereas ME requires a bi-level optimization problem with implicit gradients to enforce personalization via regularized losses. We focus on improving the scalability of personalized federated learning by removing the synchronous communication assumption. Moreover, we extend the studied function class by removing boundedness assumptions on the gradient norm. Our main technical contribution is a unified proof for asynchronous federated learning with bounded staleness that we apply to MAML and ME personalization frameworks. For the smooth and non-convex functions class, we 
    
[^152]: 利用非语言线索分析共同人际互动：一项调查

    Co-Located Human-Human Interaction Analysis using Nonverbal Cues: A Survey. (arXiv:2207.10574v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2207.10574](http://arxiv.org/abs/2207.10574)

    使用非语言线索进行共同人际互动分析的研究调查了从2010年以来的计算研究，并总结了最常用的非语言线索，以及有关互动分析的未来研究方向。

    

    使用非语言沟通作为社会和心理现象测量的证据，自动化的共同人际互动分析得到了解决。我们对从2010年以来的计算研究进行了调查，检测与社交特征（如领导力、支配力、个性特征）、社交角色/关系和互动动态（如团队凝聚力、参与度、融洽度）相关的现象。我们的目标是确定导致有效性能的非语言线索和计算方法。这项调查在涉及最广泛的社会现象和互动场景（自由对话、会议、二元组和人群）方面与其同类不同。我们还提供了相关数据集的综合总结，并概述了关于人工智能实施、数据集策划和保护隐私的互动分析的未来研究方向。一些重要观察结果是：最常用的非语言线索是共同 ...

    Automated co-located human-human interaction analysis has been addressed by the use of nonverbal communication as measurable evidence of social and psychological phenomena. We survey the computing studies (since 2010) detecting phenomena related to social traits (e.g., leadership, dominance, personality traits), social roles/relations, and interaction dynamics (e.g., group cohesion, engagement, rapport). Our target is to identify the nonverbal cues and computational methodologies resulting in effective performance. This survey differs from its counterparts by involving the widest spectrum of social phenomena and interaction settings (free-standing conversations, meetings, dyads, and crowds). We also present a comprehensive summary of the related datasets and outline future research directions which are regarding the implementation of artificial intelligence, dataset curation, and privacy-preserving interaction analysis. Some major observations are: the most often used nonverbal cue, co
    
[^153]: 通过结构幻化Transformer级联实现楼层平面图的恢复

    Floorplan Restoration by Structure Hallucinating Transformer Cascades. (arXiv:2206.00645v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.00645](http://arxiv.org/abs/2206.00645)

    该文介绍了一个新的极端楼层平面图重建任务和一个神经网络框架用于解决该任务，通过Transformer解码器级联的方式来幻化不可见的房间和门以重建整个楼层平面图，并在701个房屋的基准测试中表现出较好的效果。

    

    本文提出了一项极端的楼层平面图重建任务，为该任务提出了一个新的基准，以及一个神经架构作为解决方案。给定从全景图像中推断或策划的部分楼层平面图重建，任务是重建包括不可见建筑结构在内的完整楼层平面图。提出的神经网络1）通过卷积神经网络和Transformer将输入的部分楼层平面图编码为一组潜在向量；2）通过级联Transformer解码器，在幻化不可见的房间和门的同时重建整个楼层平面图。定性和定量评估证明了我们的方法在701个房屋的基准测试中比最先进的重建技术更有效。我们将分享我们的代码、模型和数据。

    This paper presents an extreme floorplan reconstruction task, a new benchmark for the task, and a neural architecture as a solution. Given a partial floorplan reconstruction inferred or curated from panorama images, the task is to reconstruct a complete floorplan including invisible architectural structures. The proposed neural network 1) encodes an input partial floorplan into a set of latent vectors by convolutional neural networks and a Transformer; and 2) reconstructs an entire floorplan while hallucinating invisible rooms and doors by cascading Transformer decoders. Qualitative and quantitative evaluations demonstrate effectiveness of our approach over the benchmark of 701 houses, outperforming the state-of-the-art reconstruction techniques. We will share our code, models, and data.
    
[^154]: NeuroBack: 使用图神经网络改进CDCL SAT求解

    NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks. (arXiv:2110.14053v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2110.14053](http://arxiv.org/abs/2110.14053)

    NeuroBack提出了一种使用图神经网络改进CDCL SAT求解的方法，通过预测出现在大多数满足赋值中的变量的阶段，使得求解更加有效，并且消除了对GPU资源的依赖。

    

    命题可满足性（SAT）是一个影响到规划、验证和安全等许多研究领域的NP完全问题。主流的现代SAT求解器基于冲突驱动子句学习（CDCL）算法。最近的研究旨在利用图神经网络（GNNs）增强CDCL SAT求解器。然而，到目前为止，这种方法要么没有使求解更加有效，要么需要大量的GPU资源进行频繁的在线模型推断。为了使GNN的改进变得实用，本文提出了一种名为NeuroBack的方法，它建立在两个洞察上：（1）预测出现在大多数（甚至全部）满足赋值中的变量的阶段（即值）对于CDCL SAT求解至关重要，（2）在SAT求解开始之前，只需查询一次神经模型进行预测即可。一旦训练完成，离线模型推断使NeuroBack能够仅在CPU上执行，消除了对GPU资源的依赖。

    Propositional satisfiability (SAT) is an NP-complete problem that impacts many research fields, such as planning, verification, and security. Mainstream modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL) algorithm. Recent work aimed to enhance CDCL SAT solvers using Graph Neural Networks (GNNs). However, so far this approach either has not made solving more effective, or required substantial GPU resources for frequent online model inferences. Aiming to make GNN improvements practical, this paper proposes an approach called NeuroBack, which builds on two insights: (1) predicting phases (i.e., values) of variables appearing in the majority (or even all) of the satisfying assignments are essential for CDCL SAT solving, and (2) it is sufficient to query the neural model only once for the predictions before the SAT solving starts. Once trained, the offline model inference allows NeuroBack to execute exclusively on the CPU, removing its reliance on GPU resources. To t
    
[^155]: SR-HetGNN:基于异构图神经网络的会话推荐系统

    SR-HetGNN:Session-based Recommendation with Heterogeneous Graph Neural Network. (arXiv:2108.05641v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2108.05641](http://arxiv.org/abs/2108.05641)

    本文提出了一种基于异构图神经网络的会话推荐方法SR-HetGNN，通过学习会话嵌入并捕捉匿名用户的特定偏好，以改进会话推荐系统的效果和准确性。

    

    会话推荐系统的目的是根据先前的会话序列预测用户的下一次点击。目前的研究通常根据用户会话序列中的项目转换来学习用户偏好。然而，会话序列中的其他有效信息，如用户配置文件，往往被忽视，这可能导致模型无法学习用户的具体偏好。在本文中，我们提出了一种基于异构图神经网络的会话推荐方法，命名为SR-HetGNN，它可以通过异构图神经网络（HetGNN）学习会话嵌入，并捕捉匿名用户的特定偏好。具体而言，SR-HetGNN首先根据会话序列构建包含各种类型节点的异构图，可以捕捉项目、用户和会话之间的依赖关系。其次，HetGNN捕捉项目之间的复杂转换并学习包含项目嵌入的特征。

    The purpose of the Session-Based Recommendation System is to predict the user's next click according to the previous session sequence. The current studies generally learn user preferences according to the transitions of items in the user's session sequence. However, other effective information in the session sequence, such as user profiles, are largely ignored which may lead to the model unable to learn the user's specific preferences. In this paper, we propose a heterogeneous graph neural network-based session recommendation method, named SR-HetGNN, which can learn session embeddings by heterogeneous graph neural network (HetGNN), and capture the specific preferences of anonymous users. Specifically, SR-HetGNN first constructs heterogeneous graphs containing various types of nodes according to the session sequence, which can capture the dependencies among items, users, and sessions. Second, HetGNN captures the complex transitions between items and learns the item embeddings containing
    
[^156]: AKE-GNN: 自适应知识交流的有效图学习

    AKE-GNN: Effective Graph Learning with Adaptive Knowledge Exchange. (arXiv:2106.05455v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.05455](http://arxiv.org/abs/2106.05455)

    AKE-GNN是一种新型的图神经网络学习框架，通过自适应知识交换策略在多个图视图之间交换通道，以实现有效的图学习。

    

    图神经网络(GNNs)已经被广泛应用于各种图挖掘任务。然而，最近的研究揭示了在经过训练的GNN中学到的权重(通道)是高度冗余的，这不可避免地限制了GNN的性能。我们的目标不是为了效率考虑而移除这些冗余通道，而是试图重新激活它们，以扩大GNN的表示能力，实现有效的图学习。本文提出了一种名为AKE-GNN的新型GNN学习框架，它通过对图增强生成的多个图视图之间进行自适应知识交换策略来实现这个目标。AKE-GNN首先训练多个GNN，每个对应一个图视图，以获得信息通道。然后，AKE-GNN迭代地以逐层方式在一个GNN的权重参数矩阵中进行冗余通道与另一个GNN的信息通道之间的交换。

    Graph Neural Networks (GNNs) have already been widely used in various graph mining tasks. However, recent works reveal that the learned weights (channels) in well-trained GNNs are highly redundant, which inevitably limits the performance of GNNs. Instead of removing these redundant channels for efficiency consideration, we aim to reactivate them to enlarge the representation capacity of GNNs for effective graph learning. In this paper, we propose to substitute these redundant channels with other informative channels to achieve this goal. We introduce a novel GNN learning framework named AKE-GNN, which performs the Adaptive Knowledge Exchange strategy among multiple graph views generated by graph augmentations. AKE-GNN first trains multiple GNNs each corresponding to one graph view to obtain informative channels. Then, AKE-GNN iteratively exchanges redundant channels in the weight parameter matrix of one GNN with informative channels of another GNN in a layer-wise manner. Additionally, 
    
[^157]: Wise-SrNet: 一种增强图像分类的新型架构，通过学习特征图的空间分辨率

    Wise-SrNet: A Novel Architecture for Enhancing Image Classification by Learning Spatial Resolution of Feature Maps. (arXiv:2104.12294v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2104.12294](http://arxiv.org/abs/2104.12294)

    本文提出了一种名为Wise-SrNet的新型架构，用于增强图像分类任务。该架构通过学习特征图的空间分辨率，解决了连接特征图和分类层之间的挑战。实验证明，该方法在不增加计算成本的情况下，有效提高了学习效率。

    

    自从卷积神经网络的发展以来，将提取的特征图与最终的分类层连接起来一直是主要的挑战之一。VGG模型使用两组全连接层用于架构的分类部分，这显著增加了模型权重的数量。ResNet等深度卷积模型使用全局平均池化（GAP）层将特征图压缩并输入分类层。尽管使用GAP层可以减少计算成本，但也会导致特征图的空间分辨率损失，从而降低学习效率。在本文中，我们通过使用一种名为Wise-SrNet的新型架构来解决这个问题。它受到了深度卷积思想的启发，并且专为处理空间分辨率而设计，同时不增加计算成本。我们使用三个不同的数据集对我们的方法进行了评估：Intel图像分类数据集...

    One of the main challenges since the advancement of convolutional neural networks is how to connect the extracted feature map to the final classification layer. VGG models used two sets of fully connected layers for the classification part of their architectures, which significantly increased the number of models' weights. ResNet and the next deep convolutional models used the Global Average Pooling (GAP) layer to compress the feature map and feed it to the classification layer. Although using the GAP layer reduces the computational cost, but also causes losing spatial resolution of the feature map, which results in decreasing learning efficiency. In this paper, we aim to tackle this problem by replacing the GAP layer with a new architecture called Wise-SrNet. It is inspired by the depthwise convolutional idea and is designed for processing spatial resolution while not increasing computational cost. We have evaluated our method using three different datasets: Intel Image Classification
    

