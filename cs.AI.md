# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [An Investigation of Large Language Models for Real-World Hate Speech Detection.](http://arxiv.org/abs/2401.03346) | 本研究调查了大规模语言模型在现实世界中对恶意言论检测的有效性，并发现现有方法在上下文感知方面存在显著限制。而大规模语言模型具有潜力作为上下文感知恶意言论检测的知识库，但目前缺乏有效提示这些模型的方法。 |
| [^2] | [Rediscovering Ranganathan: A Prismatic View of His Life through the Knowledge Graph Spectrum.](http://arxiv.org/abs/2401.03343) | 本研究通过一个新颖的生平知识图谱（KG）提供了库图与信息科学领域先驱人物S.R. Ranganathan的360度视角，这种专门的表示在范围和覆盖范围上无可比拟，并呼吁整个社区共同努力。 |
| [^3] | [MTAC: Hierarchical Reinforcement Learning-based Multi-gait Terrain-adaptive Quadruped Controller.](http://arxiv.org/abs/2401.03337) | 本研究提出了一种基于层次化强化学习的多步态适应地形四足控制器（MTAC），能够在动态和粗糙的地形环境中实现多种自适应步态，并且具有高效解决任务的能力。 |
| [^4] | [Attention and Autoencoder Hybrid Model for Unsupervised Online Anomaly Detection.](http://arxiv.org/abs/2401.03322) | 本文介绍了一种基于注意力和自编码器的混合模型，用于无监督的在线时间序列异常检测，通过结合注意力和自编码器，并在自编码器的潜在空间中预测下一个时间步骤窗口，提高了模型的准确性。 |
| [^5] | [Comparison of Microservice Call Rate Predictions for Replication in the Cloud.](http://arxiv.org/abs/2401.03319) | 本文通过比较线性回归、多层感知机和梯度提升回归三种机器学习模型在微服务调用率预测方面的性能，发现梯度提升回归模型在减小误差方面表现出色，并且能够准确预测微服务所需的副本数量。 |
| [^6] | [Malla: Demystifying Real-world Large Language Model Integrated Malicious Services.](http://arxiv.org/abs/2401.03315) | 本研究对212个真实的恶意服务（Malla）进行了系统研究，揭示了它们在地下市场的扩散和对公共LLM服务的影响，以及其使用的策略和技术。 |
| [^7] | [Enhancing Context Through Contrast.](http://arxiv.org/abs/2401.03314) | 本研究提出了一种通过对比学习来提高神经机器翻译性能的新方法，利用巴洛双胞胎损失最大化互信息。与其他方法不同的是，该方法通过上下文增强来提升性能，而不需要明确地增加数据或从头开始学习嵌入。 |
| [^8] | [Exploiting Data Hierarchy as a New Modality for Contrastive Learning.](http://arxiv.org/abs/2401.03312) | 本研究提出了一种利用具有层次结构的数据作为对比学习新模态的方法，实现了对教堂的概念表示的学习。通过利用数据的空间层次结构，该方法在自监督学习中取得了良好的效果。实验证明数据集结构是一种有价值的弱监督学习模态。 |
| [^9] | [CAVIAR: Co-simulation of 6G Communications, 3D Scenarios and AI for Digital Twins.](http://arxiv.org/abs/2401.03310) | CAVIAR是一种用于数字孪生的6G通信，3D场景和AI的协同仿真方法，其主要贡献包括不同架构的详细描述，应用于6G中的无人机搜索和救援任务，并生成有关计算资源使用情况的基准数据。 |
| [^10] | [MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning.](http://arxiv.org/abs/2401.03306) | 本研究提出了一种基于模型的方法，通过模型值扩展和策略正则化，可以在高维领域中有效地进行离线预训练和在线微调的强化学习，解决了离线到在线微调中的分布偏移、离散动力学数据和非稳态奖励问题。 |
| [^11] | [Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT.](http://arxiv.org/abs/2401.03302) | 本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤，并解决了在罕见情况下的肿瘤检测问题。研究使用了来自国家脑映射实验室的数据集，通过修改样本数量和患者分布，使模型能够应对真实世界场景中的异常情况。 |
| [^12] | [Real Time Human Detection by Unmanned Aerial Vehicles.](http://arxiv.org/abs/2401.03275) | 本研究提出了一种无人机热红外物体检测框架，利用FLIR相机和YOLO模型实现了对图片和视频中人体的实时检测，平均精度为72.5％。 |
| [^13] | [Autonomous Navigation in Complex Environments.](http://arxiv.org/abs/2401.03267) | 本文研究了在复杂环境中应用CNN-DNN网络融合进行自主导航的方法，通过模仿学习使用LiDAR和摄像头数据对机器人进行训练，并通过Monte-Carlo测试了其鲁棒性。 |
| [^14] | [SeqNAS: Neural Architecture Search for Event Sequence Classification.](http://arxiv.org/abs/2401.03246) | SeqNAS是一种针对事件序列分类的神经架构搜索算法，通过引入一种简单而表达力强的搜索空间，利用常用的构建块进行搜索，提供高质量的任务特定解决方案。 |
| [^15] | [Artificial Intelligence for Operations Research: Revolutionizing the Operations Research Process.](http://arxiv.org/abs/2401.03244) | 本研究回顾了人工智能技术在运筹学中的应用，旨在通过改进运筹学过程的不同阶段来提高其效果和效率。人工智能与运筹学的协同作用将推动领域内的创新和决策制定的改进。 |
| [^16] | [Using Large Language Models to Assess Tutors' Performance in Reacting to Students Making Math Errors.](http://arxiv.org/abs/2401.03238) | 本研究探究了使用生成式人工智能评估教师在学生数学错误反应方面的表现。研究结果发现，GPT-3.5-Turbo和GPT-4在评估教师的表现方面显示出了熟练度，但在识别学生需要额外帮助的情况上仍存在局限性。 |
| [^17] | [Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices.](http://arxiv.org/abs/2401.03233) | 本文介绍了一种用于最大化模型收敛速率的Split Learning肌电假肢控制中的切层选择算法，通过加速收敛过程提高了假肢控制的性能。 |
| [^18] | [MirrorDiffusion: Stabilizing Diffusion Process in Zero-shot Image Translation by Prompts Redescription and Beyond.](http://arxiv.org/abs/2401.03221) | 本论文提出了一种通过提示重新描述的策略来稳定零样本图像翻译中的扩散过程，解决了去噪扩散概率模型在反演过程中的不稳定性问题。 |
| [^19] | [Understanding Representation Learnability of Nonlinear Self-Supervised Learning.](http://arxiv.org/abs/2401.03214) | 这篇论文是第一次准确分析非线性自监督学习模型的学习结果，通过使用梯度下降算法训练模型并证明其收敛到局部最小值，在理解数据表示可学习性方面做出了重要贡献。 |
| [^20] | [Decision Making in Non-Stationary Environments with Policy-Augmented Search.](http://arxiv.org/abs/2401.03197) | 在非稳定环境下的决策制定是一个具有挑战性的问题，本文介绍了一种新的算法--策略增强蒙特卡洛树搜索（PA-MCTS），它将在线搜索与策略学习相结合。 |
| [^21] | [SecureReg: A Combined Framework for Proactively Exposing Malicious Domain Name Registrations.](http://arxiv.org/abs/2401.03196) | SecureReg是一个结合了自然语言处理和多层感知器模型的方法，用于在域名注册过程中主动暴露恶意域名注册，提供了早期威胁检测的解决方案，显著减少了漏洞窗口，并为主动预防性操作做出了贡献。 |
| [^22] | [Learning Persistent Community Structures in Dynamic Networks via Topological Data Analysis.](http://arxiv.org/abs/2401.03194) | 本论文提出了一种通过拓扑数据分析学习动态网络中的持久社区结构的方法。通过引入最小化拓扑变化的概念，提出了一种深度图聚类框架，并引入了神经网络正则化方法，用于确保时间一致性和聚类准确性。 |
| [^23] | [MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model Editing.](http://arxiv.org/abs/2401.03190) | 本论文提出了一种简单而有效的方法，利用多语言补丁神经元来解决多语言模型的跨语言知识同步问题，并取得了良好的实验结果。 |
| [^24] | [Part-of-Speech Tagger for Bodo Language using Deep Learning approach.](http://arxiv.org/abs/2401.03175) | 本文介绍了使用深度学习方法的Bodo语词性标注器。首先，我们提出了Bodo语言模型BodoBERT，这项工作是第一个为Bodo开发的语言模型。其次，我们提出了基于深度学习的Bodo词性标注模型，该模型利用了BiLSTM、CRF和BodoBERT与BytePairEmbeddings的组合。尽管研究已经在资源丰富的语言中进行了大量的语言模型和词性标注模型的研究，但对于低资源语言如Bodo，仍然缺乏相关研究。 |
| [^25] | [Exploration of Adolescent Depression Risk Prediction Based on Census Surveys and General Life Issues.](http://arxiv.org/abs/2401.03171) | 本研究使用青少年人口普查数据来预测抑郁风险，关注儿童对抑郁的经历和日常生活情况。 |
| [^26] | [PosDiffNet: Positional Neural Diffusion for Point Cloud Registration in a Large Field of View with Perturbations.](http://arxiv.org/abs/2401.03167) | PosDiffNet是一种针对大视角扰动下的点云配准的模型，使用基于Beltrami流和神经常微分方程的图神经PDE以及位置嵌入来实现点云的高维特征表示和补丁间对齐，进而进行配准。 |
| [^27] | [Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving.](http://arxiv.org/abs/2401.03160) | 本文提出了一种增强的人机协作强化学习方法，通过将人类智能注入到AI中实现混合交通编队中的安全高效自动驾驶。该方法将人类专家作为导师，允许代理在不确定环境中进行探索，同时在危险情况下接管控制以避免事故，并指导代理减小交通流干扰，优化交通流效果。 |
| [^28] | [Quartet Logic: A Four-Step Reasoning (QLFR) framework for advancing Short Text Classification.](http://arxiv.org/abs/2401.03158) | 四步推理（QLFR）框架是一种通过引入句法和语义丰富的CoT来提升短文本分类任务中大型语言模型（LLMs）性能的方法。 |
| [^29] | [Decentralized Multi-Agent Active Search and Tracking when Targets Outnumber Agents.](http://arxiv.org/abs/2401.03154) | 该论文介绍了一种处理多目标跟踪的分散式多智能体算法，该算法能在智能体数量少于目标数量时实现主动搜索和跟踪，并使用异步智能体通信来协调动作。 |
| [^30] | [TelTrans: Applying Multi-Type Telecom Data to Transportation Evaluation and Prediction via Multifaceted Graph Modeling.](http://arxiv.org/abs/2401.03138) | 这篇论文提出了使用地理单元交通流数据来改进交通评估和预测的方法。通过结合多变量、时间和空间方面的图神经网络，该方法在长期预测中取得了优越的准确率，并展示了将地理单元交通流整合到交通系统中的潜力。 |
| [^31] | [SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning.](http://arxiv.org/abs/2401.03137) | SPQR论文介绍了一种使用尖峰随机模型来控制强化学习中Q-集合的独立性的方法，通过引入基于随机矩阵理论的正则化损失来克服过高估计偏差。 |
| [^32] | [TimeGraphs: Graph-based Temporal Reasoning.](http://arxiv.org/abs/2401.03134) | TimeGraphs是一种基于图的时间推理方法，通过将动态交互建模为分层时间图，实现了对不同时间尺度的自适应推理。 |
| [^33] | [A Physics-guided Generative AI Toolkit for Geophysical Monitoring.](http://arxiv.org/abs/2401.03131) | EdGeo工具包利用物理原理指导的扩散模型生成高保真度的地下速度图，并通过使用声波方程生成地震波形数据来改善模型修剪后的ML模型性能。 |
| [^34] | [Manifold-based Shapley for SAR Recognization Network Explanation.](http://arxiv.org/abs/2401.03128) | 本研究提出了一种基于流形的Shapley方法，通过将高维特征投影到低维流形特征中，解决了传统Shapley在高维模型以及复杂场景下的解释问题和可解释性挑战。 |
| [^35] | [A white box solution to the black box problem of AI.](http://arxiv.org/abs/2401.03093) | 一种解决人工智能黑盒问题的白盒解决方案是使用基于相关领域一般理论的确定性逻辑细胞自动机的规则，该细胞自动机实现自动并行逻辑推理。 |
| [^36] | [UMIE: Unified Multimodal Information Extraction with Instruction Tuning.](http://arxiv.org/abs/2401.03082) | UMIE是一种统一的多模态信息提取器，使用指导调节的方法能够跨任务进行信息提取，并且具有强大的泛化能力和可解释性。 |
| [^37] | [CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution.](http://arxiv.org/abs/2401.03065) | CRUXEval是一个包含800个Python函数的代码推理、理解和执行的基准测试。通过评估二十个代码模型，发现许多在HumanEval上得分高的模型在该基准测试上没有相同的改进。使用CoT的GPT-4展现了最佳性能，但仍未解决问题。与开源模型相比，闭源模型的性能差距更大。 |
| [^38] | [Reliability-Optimized User Admission Control for URLLC Traffic: A Neural Contextual Bandit Approach.](http://arxiv.org/abs/2401.03059) | 本研究提出了一种基于神经上下文强化学习方法的QoS-aware UE接入控制策略，旨在在关联URRLC UE与小区之前，准确估计QoS并避免小区过载。 |
| [^39] | [AccidentGPT: Large Multi-Modal Foundation Model for Traffic Accident Analysis.](http://arxiv.org/abs/2401.03040) | 本文介绍了AccidentGPT，这是一个用于交通事故分析的大型多模态基础模型。它利用多模态输入数据自动重建事故过程视频，并提供多模态输出的多任务分析。此外，AccidentGPT还采用了多模态提示与反馈、混合训练模式和边缘-云分割配置以增强性能，并提出了一些研究机会。 |
| [^40] | [The Rise of Diffusion Models in Time-Series Forecasting.](http://arxiv.org/abs/2401.03006) | 本文调查了扩散模型在时间序列预测中的应用，提供了对这些模型的全面背景信息和详细说明，同时也对它们在不同数据集上的有效性和彼此之间的比较进行了分析。其贡献包括对扩散模型在时间序列预测中应用的彻底探索和按时间顺序排序的模型概述。这是一份对人工智能和时间序列分析领域的研究人员来说具有价值的资源。 |
| [^41] | [Bridging Modalities: Knowledge Distillation and Masked Training for Translating Multi-Modal Emotion Recognition to Uni-Modal, Speech-Only Emotion Recognition.](http://arxiv.org/abs/2401.03000) | 本文介绍了一种创新方法，用于将多模态情感识别转化为更实用且资源高效的单模态、仅语音的情感识别。使用知识蒸馏和屏蔽训练技术来解决现有模型所依赖的多模态输入在实际应用中可能不可行的问题。 |
| [^42] | [Blar-SQL: Faster, Stronger, Smaller NL2SQL.](http://arxiv.org/abs/2401.02997) | 该研究通过任务分解，提出了Blar-SQL框架，将两个不同的模型组合，分别专注于不同的任务，从而极大地提高了NL2SQL任务的准确性。该模型比GPT-4更小、更快、更便宜。 |
| [^43] | [CANAMRF: An Attention-Based Model for Multimodal Depression Detection.](http://arxiv.org/abs/2401.02995) | CANAMRF 是一种基于注意力机制的多模态抑郁检测模型，通过考虑不同模态之间的相对重要性，有效地进行多模态表示，并在两个基准数据集上取得了最先进的性能。 |
| [^44] | [Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM.](http://arxiv.org/abs/2401.02994) | 本研究介绍了一种名为“混合”的方法，通过组合多个适度规模的聊天AI模型，可以达到或超越比它们更大的模型的性能表现。 |
| [^45] | [Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion.](http://arxiv.org/abs/2401.02993) | 本文提出了一种计算高效的检索表示融合方法ReFusion，通过将检索表示直接融合到语言模型中，解决了将外部数据库的知识融入非知识密集型任务中的挑战。 |
| [^46] | [Advanced Unstructured Data Processing for ESG Reports: A Methodology for Structured Transformation and Enhanced Analysis.](http://arxiv.org/abs/2401.02992) | 本研究引入了一种创新的方法论，利用"非结构化核心库"，将ESG报告转换为结构化、可分析的格式，并在文本清洗、从图像中敏锐识别和提取文本以及标准化报告中的表格等方面取得了显著进展，为工业生态学和企业可持续性评估领域的发展奠定了基础。 |
| [^47] | [Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach.](http://arxiv.org/abs/2401.02987) | 本研究提出一种基于多头后验的方法，通过利用实体的元特征和模型的表示之间的一致性作为度量标准，有效评估预训练模型在各个领域的表现。 |
| [^48] | [Identification of Regulatory Requirements Relevant to Business Processes: A Comparative Study on Generative AI, Embedding-based Ranking, Crowd and Expert-driven Methods.](http://arxiv.org/abs/2401.02986) | 本研究比较了基于嵌入的自然语言处理排序方法，生成AI模型以及众包和专家驱动方法，旨在研究如何辅助法律和领域专家识别与业务流程相关的监管要求。 |
| [^49] | [Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education.](http://arxiv.org/abs/2401.02985) | 本研究评估了七个主要的大型语言模型在GMAT考试上的表现，并发现大多数模型优于人类考生，其中GPT-4 Turbo不仅在其他模型之上，而且超过了顶级商学院研究生的平均分数。此外，通过案例研究，本研究还考察了GPT-4 Turbo在解释答案、评估回答、识别错误、调整指导和生成替代场景方面的能力。 |
| [^50] | [Large Language Models in Mental Health Care: a Scoping Review.](http://arxiv.org/abs/2401.02984) | 本综述研究对大型语言模型在心理健康护理中的应用和结果进行了综合分析，发现其在诊断、治疗和患者参与增强等方面具有多样化的应用。同时，该研究还识别和讨论了在这些专业领域中所面临的挑战和限制。 |
| [^51] | [BIBench: Benchmarking Data Analysis Knowledge of Large Language Models.](http://arxiv.org/abs/2401.02982) | BIBench是一个旨在评估大型语言模型（LLMs）在商业智能（BI）数据分析领域中能力的综合基准测试，其通过测试模型在BI基础知识、应用知识和技术技能三个维度上的表现来进行评估。 |
| [^52] | [Fine-tuning and Utilization Methods of Domain-specific LLMs.](http://arxiv.org/abs/2401.02981) | 本研究调查了领域特定LLM的微调和利用方法，以金融领域为例，详细介绍了数据集选择、预处理、模型选择以及在金融领域LLM微调中的关键因素。研究探讨了领域特定词汇的构建和安全合规性的考虑因素，并提供了在金融领域生成领域特定LLM的过程和实施方法。多种金融案例被涵盖在内，包括股票价格预测、金融新闻情绪分析、自动文档处理、研究和信息提取等。 |
| [^53] | [Are we describing the same sound? An analysis of word embedding spaces of expressive piano performance.](http://arxiv.org/abs/2401.02979) | 本文探讨了对表现力钢琴演奏特征的不确定性，测试了五个嵌入模型及其相似性结构与真值的对应关系，并进行了进一步评估。嵌入模型的质量在这方面显示出很大的差异性。 |
| [^54] | [Learning from a Generative AI Predecessor -- The Many Motivations for Interacting with Conversational Agents.](http://arxiv.org/abs/2401.02978) | 这项研究通过分析与虚拟伙伴Zo互动的动机，总结出了多种增加互动性的方法，为生成式AI的发展提供了借鉴。 |
| [^55] | [Trace and Edit Relation Associations in GPT.](http://arxiv.org/abs/2401.02976) | 本研究介绍了一种在GPT模型中分析和修改实体关系的新方法，通过关系追踪技术，我们识别了MLP模块和注意机制在处理关系信息方面的关键作用，实验表明该方法在特异性和泛化性方面取得了平衡的改善。 |
| [^56] | [Efficacy of Utilizing Large Language Models to Detect Public Threat Posted Online.](http://arxiv.org/abs/2401.02974) | 本文研究了利用大型语言模型(LLMs)检测在线公开威胁的效力。通过实验发现，不同的LLMs在威胁和非威胁识别方面表现出较高的准确性，其中GPT-4的表现最佳。研究还发现PaLM API的定价非常具有成本效益。研究结果表明，LLMs可以有效地增强人工内容审查，帮助减轻新兴的在线风险。 |
| [^57] | [Deep Anomaly Detection in Text.](http://arxiv.org/abs/2401.02971) | 本研究旨在开发一种基于自我监督学习的方法，用于文本中的异常检测，通过利用预处理任务来提高最新技术，并在半监督和无监督的情况下对两个数据集进行了显著改进。 |
| [^58] | [Automated Localization of Blood Vessels in Retinal Images.](http://arxiv.org/abs/2401.02962) | 本论文研究了处理视网膜图像中血管定位的两种自动方法，通过减少明亮病变的影响并使用多尺度线算子定位血管结构，以此提高医学图像分析的效果。 |
| [^59] | [Fairness-Aware Job Scheduling for Multi-Job Federated Learning.](http://arxiv.org/abs/2401.02740) | 本文提出了一种公平感知联邦作业调度（FairFedJS）方法，以确保将高需求的FL客户端数据集公平分配给需要它们的FL作业，并在实验证明其优势。 |
| [^60] | [Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks.](http://arxiv.org/abs/2401.02731) | 本文提出了一种参数高效稀疏制作的方法，它使用专家混合式架构将密集模型转换为稀疏模型，以实现在模型容量有限的情况下进行指令调整和泛化能力增强。 |
| [^61] | [The Compute Divide in Machine Learning: A Threat to Academic Contribution and Scrutiny?.](http://arxiv.org/abs/2401.02452) | 本论文调查了机器学习研究中计算分歧对学术贡献和审查的影响，发现计算分歧导致学术界在计算密集型研究主题中的影响力降低，并且学术研究趋向于使用工业界开发的开源、预训练模型。为解决这一问题，建议通过国家支持的计算基础设施与开放科学倡议的结合来拓展学术见解。 |
| [^62] | [Generalist embedding models are better at short-context clinical semantic search than specialized embedding models.](http://arxiv.org/abs/2401.01943) | 本研究发现，在临床语义搜索方面，通用嵌入模型比专业嵌入模型表现更好，这表明现有的临床专业化模型对输入的微小变化更敏感。 |
| [^63] | [AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI.](http://arxiv.org/abs/2401.01651) | 本论文介绍了AIGCBench，一个全面评估AI生成的图像到视频内容的基准。通过引入多样化且开放领域的图像-文本数据集，AIGCBench解决了现有基准的局限性。为了建立统一的评估框架，该基准包括11个度量指标，涵盖控制视频对齐、动态效果、时间一致性和视频质量等方面。 |
| [^64] | [GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse.](http://arxiv.org/abs/2401.01523) | 通过基于迷因的社交虐待研究对大型多模态模型的安全洞察，我们引入了综合的迷因基准测试集GOAT-Bench，评估各种LMMs在识别和回应迷因中体现的微妙社交虐待方面的能力。 |
| [^65] | [Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review.](http://arxiv.org/abs/2401.01519) | 本文综述了大型语言模型（LLMs）在心理学应用中的前沿，包括如何模拟人类认知和行为、提供创新工具进行文献回顾、假设生成、实验设计等。 |
| [^66] | [Token Propagation Controller for Efficient Vision Transformer.](http://arxiv.org/abs/2401.01470) | 本文提出一种新颖的令牌传播控制器（TPC），通过结合暂停概率和重新开始概率，实现了对令牌的减少和重复利用的控制，从而提高了视觉Transformer的效率和令牌利用率。 |
| [^67] | [Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data.](http://arxiv.org/abs/2401.01383) | 我们提出了一种使用联邦多轨迹GNN的方法，通过稀缺数据预测婴儿脑连接性。通过联邦学习，我们通过聚合多个医院的本地学习结果来提高模型性能，同时保护数据隐私。 |
| [^68] | [A Comprehensive Study of Knowledge Editing for Large Language Models.](http://arxiv.org/abs/2401.01286) | 本研究全面研究了大型语言模型的知识编辑，旨在有效修改模型的行为，同时保持整体性能。 |
| [^69] | [PPBFL: A Privacy Protected Blockchain-based Federated Learning Model.](http://arxiv.org/abs/2401.01204) | PPBFL是一种隐私保护的基于区块链的联邦学习模型，通过应用区块链和自适应差分隐私添加算法，增强了联邦学习的安全性和节点的积极参与。同时引入了交易混合机制，更好地保护本地训练的身份隐私。 |
| [^70] | [Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases.](http://arxiv.org/abs/2401.00926) | 本文提出了一种创新的白细胞检测方法，使用多级特征融合和变形自注意DETR，通过解决白细胞尺度差异问题和提高检测精度，以改善传统血液检测的效率和准确性。 |
| [^71] | [Bringing Back the Context: Camera Trap Species Identification as Link Prediction on Multimodal Knowledge Graphs.](http://arxiv.org/abs/2401.00608) | 本研究利用相机陷阱图像的结构化上下文，提高其在物种识别任务中的泛化能力，并解决了数据稀缺和泛化能力增强的问题。 |
| [^72] | [Bidirectional Temporal Plan Graph: Enabling Switchable Passing Orders for More Efficient Multi-Agent Path Finding Plan Execution.](http://arxiv.org/abs/2401.00315) | 引入了一种新的图形表示，双向时间计划图（BTPG），允许在执行过程中切换通行顺序，避免不必要的等待时间，解决了多智能体路径规划中的执行效率问题。 |
| [^73] | [Turing's Test, a Beautiful Thought Experiment.](http://arxiv.org/abs/2401.00009) | 本文对图灵的美丽的思维实验进行了历史重建，提供了大量证据和一些原创答案，同时回答了图灵测试的核心问题。 |
| [^74] | [Action-Item-Driven Summarization of Long Meeting Transcripts.](http://arxiv.org/abs/2312.17581) | 本文提出了一种新方法来自动化生成行动项驱动的会议摘要，通过递归生成分段摘要并使用行动项提取算法。同时，本文还引入了三种用于将长记录分割成主题部分的新方法，以提高算法的效率和解决问题。 |
| [^75] | [Preference as Reward, Maximum Preference Optimization with Importance Sampling.](http://arxiv.org/abs/2312.16430) | 本文提出了一种使用重要性抽样进行最大偏好优化的算法，该算法通过直接优化生成策略来消除对奖励模型的需求，提高了数据利用率和稳定性，并通过解决KL正则化问题来改善偏好学习效果。 |
| [^76] | [YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal Information Extraction.](http://arxiv.org/abs/2312.15548) | 本文提出了一个端到端的增强对话指导的通用信息抽取调优框架（YAYI-UIE），利用对话数据和信息抽取数据共同增强信息抽取性能，在中文数据集上达到了业界领先的性能，在英文数据集上也达到了可比较的性能。 |
| [^77] | [Mutual Information as Intrinsic Reward of Reinforcement Learning Agents for On-demand Ride Pooling.](http://arxiv.org/abs/2312.15195) | 本文提出了一个用于按需共乘车辆派遣的框架，利用互信息作为强化学习智能体的内在奖励，以解决现有算法中只考虑收入最大化而无法满足异常分布请求的问题。 |
| [^78] | [Efficient Asynchronous Federated Learning with Sparsification and Quantization.](http://arxiv.org/abs/2312.15186) | 本论文提出了一种高效异步稀疏化量化联邦学习方法（TEASQ-Fed），利用边缘设备的并行参与，解决了传统方法中设备拖慢训练和通信瓶颈的问题。 |
| [^79] | [Training Convolutional Neural Networks with the Forward-Forward algorithm.](http://arxiv.org/abs/2312.14924) | 本文基于Forward-Forward算法将其应用于卷积神经网络（CNN）的训练，采用了新颖的空间扩展标签技术，在MNIST手写数字数据集上达到了99.16%的分类准确率。 |
| [^80] | [Gerrymandering Planar Graphs.](http://arxiv.org/abs/2312.14721) | 本研究探讨了选区规划问题在平面图中的计算复杂性。在$\lambda$-外平面图中，问题可以在多项式时间内解决，但在一般的平面图中是NP-complete。我们还讨论了在候选人数较大时的近似算法。 |
| [^81] | [Unlocking Pre-trained Image Backbones for Semantic Image Synthesis.](http://arxiv.org/abs/2312.13314) | 本文提出了一种新的GAN鉴别器类别，利用预训练的图像分类任务的特征骨干网络生成高度逼真的图像，并引入了更好的上下文建模和交叉注意力技术，生成更多样化的图像。 |
| [^82] | [Continual Learning: Forget-free Winning Subnetworks for Video Representations.](http://arxiv.org/abs/2312.11973) | 本研究基于"彩票票据假设"，提出了一种连续学习方法，通过利用稀疏子网络和FSO进行任务增量学习、少样本类增量学习和视频增量学习，实现高效学习和有效的权重重用。 |
| [^83] | ["Paraphrasing The Original Text" Makes High Accuracy Long-Context QA.](http://arxiv.org/abs/2312.11193) | 本论文提出了一种名为"原文改写"的任务来处理长文本问答，通过低成本高效的方法成功扩展了现有模型的上下文窗口至32k，并在多文档问答中达到了最先进的准确性。 |
| [^84] | [SAME: Sample Reconstruction against Model Extraction Attacks.](http://arxiv.org/abs/2312.10578) | SAME是一种防御模型提取攻击的新方法，基于样本重建的概念，无需额外的数据集和模型访问，并且具有更实用的保护能力。 |
| [^85] | [Symbolic Numeric Planning with Patterns.](http://arxiv.org/abs/2312.09963) | 本文提出了一种符号模式规划的新方法，通过编码问题为一个公式可以比现有方法更有效地寻找解决方案。在实验中，我们的规划器Patty在今年的IPC问题上表现出优异的性能。 |
| [^86] | [LLMind: Orchestrating AI and IoT with LLMs for Complex Task Execution.](http://arxiv.org/abs/2312.09007) | LLMind是一个利用大型语言模型（LLMs）作为中央协调器的AI框架，将LLMs与领域特定的AI模块整合，使得物联网设备能够有效协同执行复杂任务。 |
| [^87] | [Mixture-of-Linear-Experts for Long-term Time Series Forecasting.](http://arxiv.org/abs/2312.06786) | MoLE是一种混合线性专家模型，通过训练多个线性中心模型和一个路由模型，能够适应时间序列模式的周期性变化，并显著降低了预测误差。 |
| [^88] | [Calibration-free online test-time adaptation for electroencephalography motor imagery decoding.](http://arxiv.org/abs/2311.18520) | 本研究探索了在线测试时间自适应（OTTA）的概念，无需校准且保护隐私，以实现无监督的连续自适应模型运行。研究使用了脑电运动想象解码任务，并通过轻量级架构和不同的OTTA技术来提高准确率。 |
| [^89] | [Moving Sampling Physics-informed Neural Networks induced by Moving Mesh PDE.](http://arxiv.org/abs/2311.16167) | 这项工作提出了一种基于移动网格PDE的移动采样物理信息神经网络(MMPDE-Net)，通过解决移动网格PDE来自适应生成新的采样点，并且结合物理信息神经网络（PINN）提出了移动采样PINN（MS-PINN）的框架。数值实验验证了MS-PINN相对于PINN的性能改善。 |
| [^90] | [How to ensure a safe control strategy? Towards a SRL for urban transit autonomous operation.](http://arxiv.org/abs/2311.14457) | 本文提出了一个用于城市轨道交通自主运营列车的安全智能控制的SSA-DRL框架，在深度强化学习的基础上结合了线性时态逻辑、强化学习和蒙特卡洛树搜索。该框架能够生成满足速度约束和进度约束的安全控制命令序列。 |
| [^91] | [Open Set Dandelion Network for IoT Intrusion Detection.](http://arxiv.org/abs/2311.11249) | 本文提出了一种基于无监督异构领域适应的开放集dandelion网络（OSDN）用于物联网入侵检测，该模型通过从知识丰富的源网络入侵领域进行入侵知识传输，以实现更准确的入侵检测。在开放集设置下，它能够检测到在源领域中未观测到的新兴目标领域入侵。 |
| [^92] | [Retrieval-Augmented Generative Agent for Reaction Condition Recommendation in Chemical Synthesis.](http://arxiv.org/abs/2311.10776) | 本研究提出了一种转变性的人工智能代理，利用检索增强生成（RAG）技术自动化化学中的反应条件推荐（RCR）任务，通过模拟专家化学家的策略，使用大型语言模型（LLM）和新反应指纹，显著优于传统人工智能。此系统可以减轻化学家的工作负担，使他们能够更专注于更基础和创造性的科学问题。 |
| [^93] | [Explore Spurious Correlations at the Concept Level in Language Models for Text Classification.](http://arxiv.org/abs/2311.08648) | 本文研究了语言模型在文本分类中概念级别的误相关性问题，并通过使用ChatGPT分配概念标签和引入数据再平衡技术来解决这一问题。 |
| [^94] | [The Impact of Adversarial Node Placement in Decentralized Federated Learning Networks.](http://arxiv.org/abs/2311.07946) | 本文研究了分布式联邦学习网络中对抗节点部署的影响，提出了两种基线策略，并提出了一种新颖的攻击算法，优先考虑对抗的分散性而不是中心性。 |
| [^95] | [GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation.](http://arxiv.org/abs/2311.03035) | GTP-ViT是一种基于图传播的高效视觉Transformer模型，通过将不太重要的令牌信息传播给更重要的令牌，实现了模型的高效和信息保留。 |
| [^96] | [Levels of AGI: Operationalizing Progress on the Path to AGI.](http://arxiv.org/abs/2311.02462) | 本研究提出了一个框架来对人工通用智能（AGI）模型及其前驱进行分类。该框架引入了不同层次的AGI性能、广泛性和自主性，并提供了一个共同的语言用于比较模型、评估风险，并衡量在AGI路径上的进展。 |
| [^97] | [Pre-trained Recommender Systems: A Causal Debiasing Perspective.](http://arxiv.org/abs/2310.19251) | 本文探讨了将预训练模型的范式应用于推荐系统的可能性和挑战，提出开发一种通用推荐系统，可以用于少样本学习，并在未知新领域中快速适应，以提高性能。 |
| [^98] | [Federated Multi-Objective Learning.](http://arxiv.org/abs/2310.09866) | 本研究提出了一种新的联邦多目标学习（FMOL）框架，在满足多代理多任务学习应用的分布式性质和数据隐私需求的同时，支持不同客户端上的不同目标函数集合。通过引入联邦学习的范式，将多目标优化（MOO）推广到联邦学习领域。 |
| [^99] | [Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift.](http://arxiv.org/abs/2310.07535) | 在协变量转移下，我们提出了一种新的损失函数和表示匹配损失来优化模型的准确性和公平性，通过实验证明在公平性和准确性权衡方面优于其他基线算法，并且提出了一种未经研究的非对称协变量转移设置。 |
| [^100] | [Stepwise functional refoundation of relational concept analysis.](http://arxiv.org/abs/2310.06441) | 逐步功能重构的关系概念分析（RCA）是形式概念分析的扩展，通过定义良构解决方案的空间和相关函数，解决了RCA在循环依赖数据上返回单一概念格家族的问题。 |
| [^101] | [Higher-Order DeepTrails: Unified Approach to *Trails.](http://arxiv.org/abs/2310.04477) | 本论文提出了一种用于分析人类行为的统一方法，通过使用自回归语言模型来捕捉序列中的高阶依赖关系，以改进Web浏览或交通导航等应用的底层基础设施或用户界面。 |
| [^102] | [STAMP: Differentiable Task and Motion Planning via Stein Variational Gradient Descent.](http://arxiv.org/abs/2310.01775) | STAMP是一种基于Stein变分梯度下降的算法，通过并行化和可微仿真高效地搜索多个多样化的任务和运动规划解决方案。 |
| [^103] | [Deep Neighbor Layer Aggregation for Lightweight Self-Supervised Monocular Depth Estimation.](http://arxiv.org/abs/2309.09272) | 该论文提出了一种用于轻量级自监督单眼深度估计的深度邻居层聚合方法，通过使用上下文特征融合和轻量级通道注意力，在减少参数数量的同时保持准确性，在KITTI基准测试中取得了更好的结果。 |
| [^104] | [Continuous Modeling of the Denoising Process for Speech Enhancement Based on Deep Learning.](http://arxiv.org/abs/2309.09270) | 本文介绍了一种基于深度学习的连续建模方法，用于语音增强去噪过程。通过训练神经网络学习表示不同状态变量，并使用控制因子实现可控的降噪水平。实验结果表明，在清晰目标中保留少量噪声可以改善语音增强效果。 |
| [^105] | [Can Large Language Models Understand Real-World Complex Instructions?.](http://arxiv.org/abs/2309.09150) | 本论文提出了一个用于评估大规模语言模型理解复杂指令能力的基准——CELLO。通过设计复杂指令的八个特征并构建全面的评估数据集，可以解决现有基准的不足。 |
| [^106] | [TextBind: Multi-turn Interleaved Multimodal Instruction-following.](http://arxiv.org/abs/2309.08637) | TextBind是一个注释极少的框架，用于将较大规模的语言模型赋予多轮交错多模态指令跟随能力，并通过图像-标题对生成多轮多模态指令-回应对话。这个框架对于解决实际任务具有重要意义，并为未来的研究提供了数据集、模型和演示。 |
| [^107] | [Online Submodular Maximization via Online Convex Optimization.](http://arxiv.org/abs/2309.04339) | 本论文研究了在线设置下的一般性子模最大化问题，并将一类大型子模函数归约到在线凸优化问题中。这种归约方式可在组合优化中实现次线性遗憾，并且适用于许多不同版本的在线学习问题。 |
| [^108] | [Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies.](http://arxiv.org/abs/2309.02045) | 通过提示策略，本论文探究了如何通过应用角色扮演和思维链提示策略来增强大型语言模型（LLMs）在情感分析中的性能，并在三个不同领域的数据集上进行了评估。 |
| [^109] | [Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation.](http://arxiv.org/abs/2309.00096) | 本研究提出了一种通过属性分解聚合的框架来解决开放词汇语义分割中存在的问题，该框架受到人类认知解释新概念的启发。 |
| [^110] | [LLM Powered Sim-to-real Transfer for Traffic Signal Control.](http://arxiv.org/abs/2308.14284) | 本研究利用大型语言模型（LLMs）通过基于提示的行动转换，解决了交通信号控制任务中从仿真到真实的迁移问题。 |
| [^111] | [Exploring Format Consistency for Instruction Tuning.](http://arxiv.org/abs/2307.15504) | 本研究探究了指令调整的格式一致性，并提出了统一指令调整（UIT）框架，通过自动格式转换来提高泛化性能。该研究强调了格式一致性的重要性。 |
| [^112] | [RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$.](http://arxiv.org/abs/2306.15909) | RL$^3$是一种原则性混合方法，通过将传统强化学习学到的任务特定动作值作为元强化学习神经网络的输入，提高了元强化学习的性能。 |
| [^113] | [From $O(\sqrt n)$ to $O(\log n)$ in Quadratic Programming.](http://arxiv.org/abs/2306.15079) | 这篇论文提出了一种迭代复杂度为$O(\log(n))$的二次规划优化算法，并通过严格的理论证明验证了该算法的可行性。这一重大突破使得我们从$O(\sqrt{n})$的优化算法过渡到$O(\log(n))$的优化算法，其在大数据和人工智能时代具有重要应用价值。 |
| [^114] | [RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks.](http://arxiv.org/abs/2306.11335) | 该论文介绍了一个基于逼真机器人操作模拟器的基准评估RM-PRT，并提出了通用的评估流程，旨在通过大规模自然语言指令和多模态提示对机器人操作进行详细评估。 |
| [^115] | [Tackling Cooperative Incompatibility for Zero-Shot Human-AI Coordination.](http://arxiv.org/abs/2306.03034) | 本文提出了解决零样本人工智能协同中协作不兼容性的COLE框架，利用图形理论的视角，通过制定开放式目标，在涉及不熟悉的人类情境中确保人工智能代理和队友之间的协调。 |
| [^116] | [Cross-Lingual Transfer Learning for Low-Resource Speech Translation.](http://arxiv.org/abs/2306.00789) | 提出了一种三步跨语言迁移学习框架，通过在现有框架中增加一步语义知识蒸馏，该方法有效地增强了自动语音翻译中从高资源语言到低资源语言的跨语言迁移能力，显著改善了翻译性能，特别是对于低资源语言，并减少了跨语言迁移间隙(TRFGap)。 |
| [^117] | [A novel application for real-time arrhythmia detection using YOLOv8.](http://arxiv.org/abs/2305.16727) | 本文提出了一种使用YOLOv8算法进行心律失常检测的新应用程序，其模型能够实现持续监测，并以高准确性进行实时心律失常检测。 |
| [^118] | [AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback.](http://arxiv.org/abs/2305.14387) | 该论文提出了一种名为AlpacaFarm的低成本模拟器，该模拟器为从人类反馈中学习的研究和开发提供了一种解决方案，通过设计LLM提示来模拟人类反馈，提出自动评估并提供参考实现，克服了数据收集的高昂成本、缺乏可信的评估和缺乏参考方法实现的挑战。 |
| [^119] | [Non-flat ABA is an Instance of Bipolar Argumentation.](http://arxiv.org/abs/2305.12453) | 本文研究了基于假设的论证（ABA）的一个限制，即只能假设而不能推导的问题，提出了一种新颖的双极论证框架（BAFs），可以实例化一般的ABA框架，并证明了它们之间的关系。 |
| [^120] | [Can Language Models Solve Graph Problems in Natural Language?.](http://arxiv.org/abs/2305.10037) | 本论文提出了自然语言图形(NLGraph)，这是一个全面的基于图形问题解决测试，旨在评估LLM在文本描述的图形结构和图形解决方案方面的处理能力。实验结果表明，LLM(GPT-3/4)具有相应的图形推理能力。 |
| [^121] | [Answering Complex Questions over Text by Hybrid Question Parsing and Execution.](http://arxiv.org/abs/2305.07789) | 提出了一种混合问题解析和执行框架，在文字问答系统中实现回答复杂问题，通过解析问题为H表达式并设计混合执行器实现。在基准数据集中实现了最先进的准确率和效率表现。 |
| [^122] | [Comparing Foundation Models using Data Kernels.](http://arxiv.org/abs/2305.05126) | 本文采用基于数据内核的方法比较基础模型，不受度量指标的约束，通过嵌入空间几何实现点对点和多模型比较，并成功诱导了一组与下游指标强相关的模型距离函数流形。 |
| [^123] | [DroidBot-GPT: GPT-powered UI Automation for Android.](http://arxiv.org/abs/2304.07061) | DroidBot-GPT是一款利用GPT模型自动化Android应用程序的工具，可以根据任务的自然语言描述自动生成并执行操作，有望提高移动应用程序的测试和开发效率。 |
| [^124] | [A Contrastive Learning Scheme with Transformer Innate Patches.](http://arxiv.org/abs/2303.14806) | 本文提出了一种使用Transformer内在补丁的对比学习方案，可在密集下游预测任务中受益。该方案适用于所有视觉Transformer架构，易于实现，并且无需大规模批处理。对比Transformer方案在航空图像分割中有效。 |
| [^125] | [Learning Causal Attributions in Neural Networks: Beyond Direct Effects.](http://arxiv.org/abs/2303.13850) | 本文提出了一种方法来估计和维护神经网络模型中输入-输出属性的因果关系，不仅考虑直接影响，还能考虑间接影响。此方法能够在训练神经网络模型的同时有效地量化因果归因，实验结果表明该方法能够有效地学习因果归因。 |
| [^126] | [Heckerthoughts.](http://arxiv.org/abs/2302.05449) | 这本论文是作者在斯坦福大学和微软研究院工作经验的技术回忆录，涉及了机器学习和人工智能的基本概念、应用以及创造过程中的故事。 |
| [^127] | [Evaluating Self-Supervised Learning via Risk Decomposition.](http://arxiv.org/abs/2302.03068) | 通过风险分解，提出四个误差部分评估自监督学习对169个视觉模型的影响，为SSL的设计和使用提供宝贵的见解。 |
| [^128] | [LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain.](http://arxiv.org/abs/2301.13126) | LEXTREME是一个多语言和多任务的法律领域基准，该基准提供了11个数据集涵盖24种语言的测评，最佳模型（XLM-R large）在数据集和语言综合评分上均达到了61.3。这使得LEXTREME仍然具有挑战性并且有改进空间。 |
| [^129] | [Compression, Generalization and Learning.](http://arxiv.org/abs/2301.12767) | 本文提出了一种新的理论，允许在压缩的改变概率上保持控制，并获得了紧密的有限样本边界来评估压缩的改变概率。这对学习应用中的错误分类和错误预测具有重要意义。 |
| [^130] | [Impossibility Theorems for Feature Attribution.](http://arxiv.org/abs/2212.11870) | 本文阐述了对于中等规模的模型类，任何完整的线性特征归因方法都无法胜任推断模型行为的改进，启示我们在具体定义最终任务的重要性方面要汲取经验，采用重复模型评估的简单直接方法可以胜过许多其他复杂的特征归因方法。 |
| [^131] | [Revisiting Color-Event based Tracking: A Unified Network, Dataset, and Metric.](http://arxiv.org/abs/2211.11010) | 本文提出了一个用于颜色-事件统一跟踪的单阶段骨干网络（CEUTrack），通过将事件点和RGB帧转换为体素，将裁剪后的模板和搜索区域投影到令牌中，并通过统一的Transformer骨干网络实现了目标对象的定位。这一方法简单、有效且高效。 |
| [^132] | [NESTER: An Adaptive Neurosymbolic Method for Treatment Effect Estimation.](http://arxiv.org/abs/2211.04370) | NESTER是一种自适应的神经符号化方法进行治疗效果评估，将治疗效果估计的所有要求集成到一个框架中，该方法比现有最先进的方法在多个基准数据集上性能更好。 |
| [^133] | [DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization.](http://arxiv.org/abs/2207.05631) | 这篇论文提出了一个名为DGPO的算法，可以在解决任务时发现多种策略，从而提高策略鲁棒性和与用户交互的乐趣。 |
| [^134] | [From Attribution Maps to Human-Understandable Explanations through Concept Relevance Propagation.](http://arxiv.org/abs/2206.03208) | 本论文介绍了一种概念相关传播（CRP）方法，将局部和全局观点结合起来，为个别预测提供了“何地”和“何物”两个问题的解答。该方法提供了更人类可解释的解释，并通过概念图谱深入了解模型的表示和推理能力。 |
| [^135] | [Open Vocabulary Electroencephalography-To-Text Decoding and Zero-shot Sentiment Classification.](http://arxiv.org/abs/2112.02690) | 本文拓展了脑电图-文本解码的问题，实现了开放词汇的解码和零-shot情感分类。通过利用预训练的语言模型，我们的模型在解码和情感分类任务上明显优于现有方法。 |
| [^136] | [Logical Assessment Formula and Its Principles for Evaluations with Inaccurate Ground-Truth Labels.](http://arxiv.org/abs/2110.11567) | 我们提出了逻辑评估公式（LAF），通过逻辑推理揭示了其在具有不准确实际标签的评估中的原则。LAF可以在困难的任务中应用于具有不准确实际标签的评估，并合理工作；也可以从逻辑角度应用于简单的任务中，但无法像常规策略一样自信工作。 |
| [^137] | [INVIGORATE: Interactive Visual Grounding and Grasping in Clutter.](http://arxiv.org/abs/2108.11092) | INVIGORATE是一个通过自然语言与人类进行交互并在杂乱环境中抓取指定物体的机器人系统。该系统能够推断目标物体、推断物体阻挡关系，并生成多步计划来消除歧义并成功抓取目标物体。 |

# 详细

[^1]: 大规模语言模型在现实世界中对恶意言论检测的调查

    An Investigation of Large Language Models for Real-World Hate Speech Detection. (arXiv:2401.03346v1 [cs.CY])

    [http://arxiv.org/abs/2401.03346](http://arxiv.org/abs/2401.03346)

    本研究调查了大规模语言模型在现实世界中对恶意言论检测的有效性，并发现现有方法在上下文感知方面存在显著限制。而大规模语言模型具有潜力作为上下文感知恶意言论检测的知识库，但目前缺乏有效提示这些模型的方法。

    

    恶意言论已经成为困扰我们社交空间的一个主要问题。虽然在解决这个问题上已经做出了一些显著的努力，但现有方法在有效检测在线恶意言论方面仍然存在显著限制。现有方法的一个主要限制是恶意言论检测是一个高度上下文相关的问题，而这些方法无法完全捕捉恶意言论的上下文以进行准确的预测。最近，大规模语言模型（LLMs）在几个自然语言任务中展示了最先进的性能。LLMs经过大量的自然语言数据进行了广泛的训练，使其能够掌握复杂的上下文细节。因此，它们可以用作上下文感知的恶意言论检测的知识库。然而，使用LLMs检测恶意言论的一个基本问题是没有关于有效提示LLMs进行上下文感知的恶意言论检测的研究。在本研究中，我们进行了一个大规模的研究，调查恶意言论检测方面的问题。

    Hate speech has emerged as a major problem plaguing our social spaces today. While there have been significant efforts to address this problem, existing methods are still significantly limited in effectively detecting hate speech online. A major limitation of existing methods is that hate speech detection is a highly contextual problem, and these methods cannot fully capture the context of hate speech to make accurate predictions. Recently, large language models (LLMs) have demonstrated state-of-the-art performance in several natural language tasks. LLMs have undergone extensive training using vast amounts of natural language data, enabling them to grasp intricate contextual details. Hence, they could be used as knowledge bases for context-aware hate speech detection. However, a fundamental problem with using LLMs to detect hate speech is that there are no studies on effectively prompting LLMs for context-aware hate speech detection. In this study, we conduct a large-scale study of hat
    
[^2]: 重新发现Ranganathan：通过知识图谱的光谱视角了解他的生平

    Rediscovering Ranganathan: A Prismatic View of His Life through the Knowledge Graph Spectrum. (arXiv:2401.03343v1 [cs.DL])

    [http://arxiv.org/abs/2401.03343](http://arxiv.org/abs/2401.03343)

    本研究通过一个新颖的生平知识图谱（KG）提供了库图与信息科学领域先驱人物S.R. Ranganathan的360度视角，这种专门的表示在范围和覆盖范围上无可比拟，并呼吁整个社区共同努力。

    

    本研究提出了一个关于库图与信息科学领域的先驱人物S.R. Ranganathan教授的新颖生平知识图谱（KG）。发现关于Ranganathan的相关事实存在于各种资源中，以碎片化和零散的方式提供信息。通过这个专门的KG，我们希望为他的生平和成就提供一个360度的视角。据我们所知，这种专门的表示在其范围和覆盖范围上是无可比拟的：使用最先进的技术供任何人公开访问、使用/再使用和贡献。受Ranganathan的理论和思想的启发，KG使用“基于特征的方法论”在两个层面上进行了发展：在关键的生平方面的标识和本体模型的开发。最后，通过这项研究，我们呼吁整个社区共同努力。

    The present study puts forward a novel biographical knowledge graph (KG) on Prof. S. R. Ranganathan, one of the pioneering figures in the Library and Information Science (LIS) domain. It has been found that most of the relevant facts about Ranganathan exist in a variety of resources (e.g., books, essays, journal articles, websites, blogs, etc.), offering information in a fragmented and piecemeal way. With this dedicated KG (henceforth known as RKG), we hope to furnish a 360-degree view of his life and achievements. To the best of our knowledge, such a dedicated representation is unparalleled in its scope and coverage: using state-of-the-art technology for anyone to openly access, use/re-use, and contribute. Inspired by Ranganathan's theories and ideas, the KG was developed using a "facet-based methodology" at two levels: in the identification of the vital biographical aspects and the development of the ontological model. Finally, with this study, we call for a community-driven effort t
    
[^3]: MTAC: 基于层次化强化学习的多步态适应地形四足控制器

    MTAC: Hierarchical Reinforcement Learning-based Multi-gait Terrain-adaptive Quadruped Controller. (arXiv:2401.03337v1 [cs.RO])

    [http://arxiv.org/abs/2401.03337](http://arxiv.org/abs/2401.03337)

    本研究提出了一种基于层次化强化学习的多步态适应地形四足控制器（MTAC），能够在动态和粗糙的地形环境中实现多种自适应步态，并且具有高效解决任务的能力。

    

    城市搜救任务需要快速响应以最小化人员伤亡和损失。通常，这些努力会由人道主义机器人协助，这些机器人需要处理动态操作条件，如不平坦和粗糙的地形，尤其是在类似地震等大规模伤亡事件中。四足机器人由于其多功能设计，具备在这种情况下协助的潜力。然而，由于这些机器人具有多自由度，控制其在动态和粗糙的地形环境中的运动是一个具有挑战性的问题。当前针对四足机器人的运动控制器在产生多种自适应步态、在时间和资源上高效解决任务以及需要繁琐的训练和手动调试过程方面存在局限性。为解决这些挑战，我们提出了MTAC：一种多步态适应地形控制器，该控制器利用了层次化强化学习方法，在时间和内存效率上进行了优化。我们展示了我们的提议的成果。

    Urban search and rescue missions require rapid first response to minimize loss of life and damage. Often, such efforts are assisted by humanitarian robots which need to handle dynamic operational conditions such as uneven and rough terrains, especially during mass casualty incidents like an earthquake. Quadruped robots, owing to their versatile design, have the potential to assist in such scenarios. However, control of quadruped robots in dynamic and rough terrain environments is a challenging problem due to the many degrees of freedom of these robots. Current locomotion controllers for quadrupeds are limited in their ability to produce multiple adaptive gaits, solve tasks in a time and resource-efficient manner, and require tedious training and manual tuning procedures. To address these challenges, we propose MTAC: a multi-gait terrain-adaptive controller, which utilizes a Hierarchical reinforcement learning (HRL) approach while being time and memory-efficient. We show that our propos
    
[^4]: 基于注意力和自编码器的混合模型用于无监督在线异常检测

    Attention and Autoencoder Hybrid Model for Unsupervised Online Anomaly Detection. (arXiv:2401.03322v1 [cs.LG])

    [http://arxiv.org/abs/2401.03322](http://arxiv.org/abs/2401.03322)

    本文介绍了一种基于注意力和自编码器的混合模型，用于无监督的在线时间序列异常检测，通过结合注意力和自编码器，并在自编码器的潜在空间中预测下一个时间步骤窗口，提高了模型的准确性。

    

    本论文介绍了一种基于注意力和自编码器的混合模型，用于时间序列的无监督在线异常检测。自编码器捕捉短嵌入中的局部结构模式，而注意力模型学习长期特征，通过位置编码实现并行计算。在方法上独特的是，我们提出的混合模型首次在时间序列异常检测中结合了注意力和自编码器。它采用了类似深度变换器模型的注意力机制，并对自编码器的潜在空间中预测下一个时间步骤窗口进行了关键架构修改。该模型利用验证数据集中的阈值进行异常检测，并引入了一种基于分析误差的第一统计矩方法的替代方法，提高了准确性而不依赖于验证数据集。通过在多样的实际基准数据集上进行评估，并与其他广为认可的模型进行比较，证实了模型的有效性。

    This paper introduces a hybrid attention and autoencoder (AE) model for unsupervised online anomaly detection in time series. The autoencoder captures local structural patterns in short embeddings, while the attention model learns long-term features, facilitating parallel computing with positional encoding. Unique in its approach, our proposed hybrid model combines attention and autoencoder for the first time in time series anomaly detection. It employs an attention-based mechanism, akin to the deep transformer model, with key architectural modifications for predicting the next time step window in the autoencoder's latent space. The model utilizes a threshold from the validation dataset for anomaly detection and introduces an alternative method based on analyzing the first statistical moment of error, improving accuracy without dependence on a validation dataset. Evaluation on diverse real-world benchmark datasets and comparing with other well-established models, confirms the effective
    
[^5]: 微服务复制在云中的调用率预测比较

    Comparison of Microservice Call Rate Predictions for Replication in the Cloud. (arXiv:2401.03319v1 [cs.DC])

    [http://arxiv.org/abs/2401.03319](http://arxiv.org/abs/2401.03319)

    本文通过比较线性回归、多层感知机和梯度提升回归三种机器学习模型在微服务调用率预测方面的性能，发现梯度提升回归模型在减小误差方面表现出色，并且能够准确预测微服务所需的副本数量。

    

    如今，许多用户在一群云机器上部署基于微服务的应用程序，这些应用程序具有各种相互连接，并受到动态用户需求的随机变化的影响。为了解决这个问题，我们比较了三种机器学习（ML）模型，用于基于微服务时间预测微服务调用率，并旨在估计可扩展性要求。我们在阿里巴巴的微服务跟踪数据上应用了线性回归（LR）、多层感知机（MLP）和梯度提升回归（GBR）模型。预测结果显示，与GBR和MLP模型相比，LR模型的训练时间更短。然而，与LR和MLP模型相比，GBR模型降低了均方差和平均绝对百分比误差。此外，预测结果显示，梯度提升模型对每个微服务所需的副本数量与实际测试数据非常接近，而无需进行任何预测。

    Today, many users deploy their microservice-based applications with various interconnections on a cluster of Cloud machines, subject to stochastic changes due to dynamic user requirements. To address this problem, we compare three machine learning (ML) models for predicting the microservice call rates based on the microservice times and aiming at estimating the scalability requirements. We apply the linear regression (LR), multilayer perception (MLP), and gradient boosting regression (GBR) models on the Alibaba microservice traces. The prediction results reveal that the LR model reaches a lower training time than the GBR and MLP models. However, the GBR reduces the mean absolute error and the mean absolute percentage error compared to LR and MLP models. Moreover, the prediction results show that the required number of replicas for each microservice by the gradient boosting model is close to the actual test data without any prediction.
    
[^6]: Malla: 揭秘现实世界中大规模语言模型整合恶意服务

    Malla: Demystifying Real-world Large Language Model Integrated Malicious Services. (arXiv:2401.03315v1 [cs.CR])

    [http://arxiv.org/abs/2401.03315](http://arxiv.org/abs/2401.03315)

    本研究对212个真实的恶意服务（Malla）进行了系统研究，揭示了它们在地下市场的扩散和对公共LLM服务的影响，以及其使用的策略和技术。

    

    大规模语言模型（LLMs）的地下利用，也称为Malla，正在增加，加剧了网络安全威胁，并对LLMs技术的可信度提出了疑问。然而，迄今为止，很少有工作努力去了解这种新型网络犯罪的规模、影响和技术。本文是第一次对212个真实的Malla进行系统研究，揭示了它们在地下市场的扩散，并揭示了它们的操作模式。我们的研究揭开了Malla生态系统，揭示了其显著的增长对当今公共LLM服务的影响。通过对212个Mallas进行研究，我们发现了8个后端LLMs，以及182个绕过公共LLM API保护措施的提示。我们进一步揭示了Mallas使用的策略，包括滥用未经审查的LLMs和通过越狱提示利用公共LLM API。我们的发现有助于更好地理解Malla犯罪行为的实质。

    The underground exploitation of large language models (LLMs) for malicious services (i.e., Malla) is witnessing an uptick, amplifying the cyber threat landscape and posing questions about the trustworthiness of LLM technologies. However, there has been little effort to understand this new cybercrime, in terms of its magnitude, impact, and techniques. In this paper, we conduct the first systematic study on 212 real-world Mallas, uncovering their proliferation in underground marketplaces and exposing their operational modalities. Our study discloses the Malla ecosystem, revealing its significant growth and impact on today's public LLM services. Through examining 212 Mallas, we uncovered eight backend LLMs used by Mallas, along with 182 prompts that circumvent the protective measures of public LLM APIs. We further demystify the tactics employed by Mallas, including the abuse of uncensored LLMs and the exploitation of public LLM APIs through jailbreak prompts. Our findings enable a better 
    
[^7]: 提升对比度的上下文增强

    Enhancing Context Through Contrast. (arXiv:2401.03314v1 [cs.CL])

    [http://arxiv.org/abs/2401.03314](http://arxiv.org/abs/2401.03314)

    本研究提出了一种通过对比学习来提高神经机器翻译性能的新方法，利用巴洛双胞胎损失最大化互信息。与其他方法不同的是，该方法通过上下文增强来提升性能，而不需要明确地增加数据或从头开始学习嵌入。

    

    神经机器翻译受益于语义丰富的表示。通过语言建模和对比学习使用互信息最大化目标，已经实现了对这种表示的大幅进展。语言建模的依赖性使得在学习表示的通用性和模型在语言建模任务上的性能之间存在权衡。虽然对比学习改进了性能，但其成功不能仅归因于互信息。我们提出了一种新的上下文增强步骤，通过使用巴洛双胞胎损失最大化互信息来提高神经机器翻译的性能。与其他方法不同的是，我们不是显式地增加数据，而是将语言视为隐含的增强，消除了破坏语义信息的风险。此外，我们的方法不会从头开始学习嵌入，并且可以推广到任何一组预训练的嵌入。

    Neural machine translation benefits from semantically rich representations. Considerable progress in learning such representations has been achieved by language modelling and mutual information maximization objectives using contrastive learning. The language-dependent nature of language modelling introduces a trade-off between the universality of the learned representations and the model's performance on the language modelling tasks. Although contrastive learning improves performance, its success cannot be attributed to mutual information alone. We propose a novel Context Enhancement step to improve performance on neural machine translation by maximizing mutual information using the Barlow Twins loss. Unlike other approaches, we do not explicitly augment the data but view languages as implicit augmentations, eradicating the risk of disrupting semantic information. Further, our method does not learn embeddings from scratch and can be generalised to any set of pre-trained embeddings. Fin
    
[^8]: 利用数据层次结构作为对比学习的新模态

    Exploiting Data Hierarchy as a New Modality for Contrastive Learning. (arXiv:2401.03312v1 [cs.CV])

    [http://arxiv.org/abs/2401.03312](http://arxiv.org/abs/2401.03312)

    本研究提出了一种利用具有层次结构的数据作为对比学习新模态的方法，实现了对教堂的概念表示的学习。通过利用数据的空间层次结构，该方法在自监督学习中取得了良好的效果。实验证明数据集结构是一种有价值的弱监督学习模态。

    

    本研究探讨了如何利用具有层次结构的数据帮助神经网络学习教堂的概念表示。基于WikiScenes数据集，该数据集提供了教堂组件的空间有序的层次结构。我们提出了一种新颖的层次对比训练方法，利用三元组边界损失在编码器的潜在空间中表示数据的空间层次结构。因此，该方法探究了数据集结构是否提供了有价值的自监督学习信息。我们使用t-SNE对得到的潜在空间进行可视化，并通过将其与其他特定数据集的对比学习方法在公共下游分类任务中进行比较来评估提出的方法。该方法在可弱监督和基准方法上表现出色。我们的研究结果表明数据集结构是一种有价值的弱监督学习模态。

    This work investigates how hierarchically structured data can help neural networks learn conceptual representations of cathedrals. The underlying WikiScenes dataset provides a spatially organized hierarchical structure of cathedral components. We propose a novel hierarchical contrastive training approach that leverages a triplet margin loss to represent the data's spatial hierarchy in the encoder's latent space. As such, the proposed approach investigates if the dataset structure provides valuable information for self-supervised learning. We apply t-SNE to visualize the resultant latent space and evaluate the proposed approach by comparing it with other dataset-specific contrastive learning methods using a common downstream classification task. The proposed method outperforms the comparable weakly-supervised and baseline methods. Our findings suggest that dataset structure is a valuable modality for weakly-supervised learning.
    
[^9]: CAVIAR: 用于数字孪生的6G通信，3D场景和人工智能的协同仿真

    CAVIAR: Co-simulation of 6G Communications, 3D Scenarios and AI for Digital Twins. (arXiv:2401.03310v1 [cs.NI])

    [http://arxiv.org/abs/2401.03310](http://arxiv.org/abs/2401.03310)

    CAVIAR是一种用于数字孪生的6G通信，3D场景和AI的协同仿真方法，其主要贡献包括不同架构的详细描述，应用于6G中的无人机搜索和救援任务，并生成有关计算资源使用情况的基准数据。

    

    数字孪生是推进移动通信技术的重要技术，特别是在需要同时模拟无线信道，3D场景和机器学习的应用场景中。为了解决这一需求，本文介绍了一种模块化协同仿真方法，称为CAVIAR。在这里，CAVIAR被升级以支持消息传递库，并利用不同的6G相关模拟器实现数字孪生系统的虚拟对应物。本文的主要贡献包括对不同CAVIAR架构的详细描述，将该方法应用于评估基于无人机的搜索和救援任务（SAR）的6G用例，并生成有关计算资源使用情况的基准数据。为了执行SAR协同仿真，我们采用了五个开源解决方案：物理和链路级网络模拟器Sionna，自主车辆模拟器AirSim，scikit-learn用于训练用于MIMO波束选择的决策树。

    Digital twins are an important technology for advancing mobile communications, specially in use cases that require simultaneously simulating the wireless channel, 3D scenes and machine learning. Aiming at providing a solution to this demand, this work describes a modular co-simulation methodology called CAVIAR. Here, CAVIAR is upgraded to support a message passing library and enable the virtual counterpart of a digital twin system using different 6G-related simulators. The main contributions of this work are the detailed description of different CAVIAR architectures, the implementation of this methodology to assess a 6G use case of UAV-based search and rescue mission (SAR), and the generation of benchmarking data about the computational resource usage. For executing the SAR co-simulation we adopt five open-source solutions: the physical and link level network simulator Sionna, the simulator for autonomous vehicles AirSim, scikit-learn for training a decision tree for MIMO beam selectio
    
[^10]: MOTO: 离线预训练与在线微调用于基于模型的机器人学习

    MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning. (arXiv:2401.03306v1 [cs.LG])

    [http://arxiv.org/abs/2401.03306](http://arxiv.org/abs/2401.03306)

    本研究提出了一种基于模型的方法，通过模型值扩展和策略正则化，可以在高维领域中有效地进行离线预训练和在线微调的强化学习，解决了离线到在线微调中的分布偏移、离散动力学数据和非稳态奖励问题。

    

    我们研究了在现实机器人任务中，从高维观测中进行离线预训练和在线微调的强化学习问题。最近的离线无模型方法成功地使用在线微调来提高智能体在数据收集策略上的性能，或者适应新的任务。与此同时，基于模型的强化学习算法在样本效率和解决的任务复杂性方面取得了显著进展，但在微调方面仍然被低估了。在这项工作中，我们认为现有的基于模型的离线强化学习方法在高维领域中不适用于离线到在线微调，原因是分布偏移、离散动力学数据和非稳态奖励问题。我们提出了一种基于模型的方法，通过模型值扩展和策略正则化，可以高效地重用先前的数据，同时通过控制认知不确定性来防止模型的利用。

    We study the problem of offline pre-training and online fine-tuning for reinforcement learning from high-dimensional observations in the context of realistic robot tasks. Recent offline model-free approaches successfully use online fine-tuning to either improve the performance of the agent over the data collection policy or adapt to novel tasks. At the same time, model-based RL algorithms have achieved significant progress in sample efficiency and the complexity of the tasks they can solve, yet remain under-utilized in the fine-tuning setting. In this work, we argue that existing model-based offline RL methods are not suitable for offline-to-online fine-tuning in high-dimensional domains due to issues with distribution shifts, off-dynamics data, and non-stationary rewards. We propose an on-policy model-based method that can efficiently reuse prior data through model-based value expansion and policy regularization, while preventing model exploitation by controlling epistemic uncertainty
    
[^11]: 行动中的现实主义：使用YOLOv8和DeiT从医学图像中诊断脑肿瘤的异常感知

    Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])

    [http://arxiv.org/abs/2401.03302](http://arxiv.org/abs/2401.03302)

    本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤，并解决了在罕见情况下的肿瘤检测问题。研究使用了来自国家脑映射实验室的数据集，通过修改样本数量和患者分布，使模型能够应对真实世界场景中的异常情况。

    

    在医学科学领域，由于脑肿瘤在患者中的罕见程度，可靠地检测和分类脑肿瘤仍然是一个艰巨的挑战。因此，在异常情况下检测肿瘤的能力对于确保及时干预和改善患者结果至关重要。本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤。来自国家脑映射实验室（NBML）的精选数据集包括81名患者，其中包括30例肿瘤病例和51例正常病例。检测和分类流程被分为两个连续的任务。检测阶段包括全面的数据分析和预处理，以修改图像样本和每个类别的患者数量，以符合真实世界场景中的异常分布（9个正常样本对应1个肿瘤样本）。此外，在测试中除了常见的评估指标外，我们还采用了... [摘要长度已达到上限]

    In the field of medical sciences, reliable detection and classification of brain tumors from images remains a formidable challenge due to the rarity of tumors within the population of patients. Therefore, the ability to detect tumors in anomaly scenarios is paramount for ensuring timely interventions and improved patient outcomes. This study addresses the issue by leveraging deep learning (DL) techniques to detect and classify brain tumors in challenging situations. The curated data set from the National Brain Mapping Lab (NBML) comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The detection and classification pipelines are separated into two consecutive tasks. The detection phase involved comprehensive data analysis and pre-processing to modify the number of image samples and the number of patients of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with real world scenarios. Next, in addition to common evaluation metrics for the testing, we emplo
    
[^12]: 无人机实时人体检测

    Real Time Human Detection by Unmanned Aerial Vehicles. (arXiv:2401.03275v1 [cs.CV])

    [http://arxiv.org/abs/2401.03275](http://arxiv.org/abs/2401.03275)

    本研究提出了一种无人机热红外物体检测框架，利用FLIR相机和YOLO模型实现了对图片和视频中人体的实时检测，平均精度为72.5％。

    

    计算机视觉和遥感中的一个最重要问题是物体检测，即在图像中识别特定类别的多种事物。无人机（UAV）产生的热红外（TIR）遥感多场景照片和视频是公共安全的两个重要数据来源。由于目标尺度小、场景信息复杂、相对可视视频分辨率低和缺乏公开可用的标记数据集和训练模型，它们的物体检测过程仍然困难。本研究提出了一种用于图片和视频的无人机TIR物体检测框架。使用用于收集地面TIR照片和视频的前视红外（FLIR）相机，创建了基于CNN架构的“You Only Look Once”（YOLO）模型。结果表明，在验证任务中，人体物体的IOU（交并比）= 0.5时的平均精度为72.5％。

    One of the most important problems in computer vision and remote sensing is object detection, which identifies particular categories of diverse things in pictures. Two crucial data sources for public security are the thermal infrared (TIR) remote sensing multi-scenario photos and videos produced by unmanned aerial vehicles (UAVs). Due to the small scale of the target, complex scene information, low resolution relative to the viewable videos, and dearth of publicly available labeled datasets and training models, their object detection procedure is still difficult. A UAV TIR object detection framework for pictures and videos is suggested in this study. The Forward-looking Infrared (FLIR) cameras used to gather ground-based TIR photos and videos are used to create the ``You Only Look Once'' (YOLO) model, which is based on CNN architecture. Results indicated that in the validating task, detecting human object had an average precision at IOU (Intersection over Union) = 0.5, which was 72.5\%
    
[^13]: 复杂环境中的自主导航

    Autonomous Navigation in Complex Environments. (arXiv:2401.03267v1 [cs.RO])

    [http://arxiv.org/abs/2401.03267](http://arxiv.org/abs/2401.03267)

    本文研究了在复杂环境中应用CNN-DNN网络融合进行自主导航的方法，通过模仿学习使用LiDAR和摄像头数据对机器人进行训练，并通过Monte-Carlo测试了其鲁棒性。

    

    本文探讨了在模拟环境中应用CNN-DNN网络融合来构建机器人导航控制器的方法。模拟环境用于模拟地下救援情景，将自主代理任务设定为在未知的洞穴系统中寻找目标。采用模仿学习的方法，使用LiDAR和摄像头数据来训练控制算法以进行空间导航并找到目标。训练好的模型通过Monte-Carlo进行鲁棒性测试。

    This paper explores the application of CNN-DNN network fusion to construct a robot navigation controller within a simulated environment. The simulated environment is constructed to model a subterranean rescue situation, such that an autonomous agent is tasked with finding a goal within an unknown cavernous system. Imitation learning is used to train the control algorithm to use LiDAR and camera data to navigate the space and find the goal. The trained model is then tested for robustness using Monte-Carlo.
    
[^14]: SeqNAS: 事件序列分类的神经架构搜索

    SeqNAS: Neural Architecture Search for Event Sequence Classification. (arXiv:2401.03246v1 [cs.LG])

    [http://arxiv.org/abs/2401.03246](http://arxiv.org/abs/2401.03246)

    SeqNAS是一种针对事件序列分类的神经架构搜索算法，通过引入一种简单而表达力强的搜索空间，利用常用的构建块进行搜索，提供高质量的任务特定解决方案。

    

    神经架构搜索(NAS)方法广泛应用于各个行业，以获得高质量的任务特定解决方案，最大限度地减少人为干预。事件序列在各种工业应用中得到广泛使用，包括流失预测、客户分割、欺诈检测和故障诊断等。这些数据包括具有不规则时间戳的分类和实数值组件。尽管NAS方法的有用性，但之前的方法仅应用于其他领域的图像、文本或时间序列。我们的工作通过引入一种新颖的NAS算法SeqNAS来解决这个限制，该算法专门用于事件序列分类。我们开发了一个简单但表达力强的搜索空间，利用常用的事件序列分类构建块，包括多头自注意力、卷积和循环单元。为了进行搜索，我们采用了顺序贝叶斯优化，并利用先前训练过的模型作为教师模型的集成。

    Neural Architecture Search (NAS) methods are widely used in various industries to obtain high quality taskspecific solutions with minimal human intervention. Event Sequences find widespread use in various industrial applications including churn prediction customer segmentation fraud detection and fault diagnosis among others. Such data consist of categorical and real-valued components with irregular timestamps. Despite the usefulness of NAS methods previous approaches only have been applied to other domains images texts or time series. Our work addresses this limitation by introducing a novel NAS algorithm SeqNAS specifically designed for event sequence classification. We develop a simple yet expressive search space that leverages commonly used building blocks for event sequence classification including multihead self attention convolutions and recurrent cells. To perform the search we adopt sequential Bayesian Optimization and utilize previously trained models as an ensemble of teache
    
[^15]: 运筹学的人工智能：改变运筹学过程的革命性技术

    Artificial Intelligence for Operations Research: Revolutionizing the Operations Research Process. (arXiv:2401.03244v1 [math.OC])

    [http://arxiv.org/abs/2401.03244](http://arxiv.org/abs/2401.03244)

    本研究回顾了人工智能技术在运筹学中的应用，旨在通过改进运筹学过程的不同阶段来提高其效果和效率。人工智能与运筹学的协同作用将推动领域内的创新和决策制定的改进。

    

    人工智能技术的迅速发展为各个领域，包括运筹学，开辟了新的机遇。本调查论文探讨了在运筹学过程中的人工智能整合（AI4OR），以提高其在参数生成、模型构建和模型优化等多个阶段的效果和效率。通过全面概述现有技术的同时，研究人工智能改变运筹学的潜力，本文旨在激发进一步研究和创新，开发人工智能增强的运筹学方法和工具。人工智能和运筹学的协同作用将推动各个领域的重大进展和新型解决方案，最终实现更加有效和高效的决策制定。

    The rapid advancement of artificial intelligence (AI) techniques has opened up new opportunities to revolutionize various fields, including operations research (OR). This survey paper explores the integration of AI within the OR process (AI4OR) to enhance its effectiveness and efficiency across multiple stages, such as parameter generation, model formulation, and model optimization. By providing a comprehensive overview of the state-of-the-art and examining the potential of AI to transform OR, this paper aims to inspire further research and innovation in the development of AI-enhanced OR methods and tools. The synergy between AI and OR is poised to drive significant advancements and novel solutions in a multitude of domains, ultimately leading to more effective and efficient decision-making.
    
[^16]: 使用大型语言模型评估教师对学生数学错误反应的表现

    Using Large Language Models to Assess Tutors' Performance in Reacting to Students Making Math Errors. (arXiv:2401.03238v1 [cs.HC])

    [http://arxiv.org/abs/2401.03238](http://arxiv.org/abs/2401.03238)

    本研究探究了使用生成式人工智能评估教师在学生数学错误反应方面的表现。研究结果发现，GPT-3.5-Turbo和GPT-4在评估教师的表现方面显示出了熟练度，但在识别学生需要额外帮助的情况上仍存在局限性。

    

    研究表明，教师在应对低效学生的数学错误时应采取战略性的方法。教师不应直接指出错误，而应引导学生自己发现和纠正错误。虽然教师课程已经引入了这种教学技能，但人工评价教师在实际应用该策略时是困难且耗时的。大型语言模型（LLM）在实际辅导过程中向教师提供实时评估的潜力显示出来，但在这种情境下，对其准确性了解甚少。在这项研究中，我们调查了生成式人工智能在评估实际教师对学生数学错误反应表现中的能力。通过分析50个真实辅导对话，我们发现GPT-3.5-Turbo和GPT-4在评估与学生犯错反应相关的标准方面表现出了熟练度。然而，这两个模型在识别学生需要额外帮助的情况上存在局限性。

    Research suggests that tutors should adopt a strategic approach when addressing math errors made by low-efficacy students. Rather than drawing direct attention to the error, tutors should guide the students to identify and correct their mistakes on their own. While tutor lessons have introduced this pedagogical skill, human evaluation of tutors applying this strategy is arduous and time-consuming. Large language models (LLMs) show promise in providing real-time assessment to tutors during their actual tutoring sessions, yet little is known regarding their accuracy in this context. In this study, we investigate the capacity of generative AI to evaluate real-life tutors' performance in responding to students making math errors. By analyzing 50 real-life tutoring dialogues, we find both GPT-3.5-Turbo and GPT-4 demonstrate proficiency in assessing the criteria related to reacting to students making errors. However, both models exhibit limitations in recognizing instances where the student 
    
[^17]: 基于Split Learning的肌电假肢控制中的收敛速率最大化

    Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices. (arXiv:2401.03233v1 [cs.LG])

    [http://arxiv.org/abs/2401.03233](http://arxiv.org/abs/2401.03233)

    本文介绍了一种用于最大化模型收敛速率的Split Learning肌电假肢控制中的切层选择算法，通过加速收敛过程提高了假肢控制的性能。

    

    Split Learning (SL)是一种有前途的分布式学习方法，可以在资源有限的环境中应用于基于肌电的假肢控制。与深度学习和联邦学习等其他学习方法相比，SL能够提供更优的解决方案，因为假肢设备在处理能力和电池寿命方面非常有限。在这些情况下，实现SL的可行性源于其固有的模型分割，其中客户端执行较小的模型部分。然而，选择不恰当的切层会阻碍SL系统的训练过程。本文提出了一种用于最大化模型收敛速率的切层选择算法。性能评估表明，所提出的算法在改善假肢控制的肌电模式识别任务中显著加速了收敛过程。

    Split Learning (SL) is a promising Distributed Learning approach in electromyography (EMG) based prosthetic control, due to its applicability within resource-constrained environments. Other learning approaches, such as Deep Learning and Federated Learning (FL), provide suboptimal solutions, since prosthetic devices are extremely limited in terms of processing power and battery life. The viability of implementing SL in such scenarios is caused by its inherent model partitioning, with clients executing the smaller model segment. However, selecting an inadequate cut layer hinders the training process in SL systems. This paper presents an algorithm for optimal cut layer selection in terms of maximizing the convergence rate of the model. The performance evaluation demonstrates that the proposed algorithm substantially accelerates the convergence in an EMG pattern recognition task for improving prosthetic device control.
    
[^18]: MirrorDiffusion: 通过提示重新描述来稳定零样本图像翻译中的扩散过程及其后续工作

    MirrorDiffusion: Stabilizing Diffusion Process in Zero-shot Image Translation by Prompts Redescription and Beyond. (arXiv:2401.03221v1 [cs.CV])

    [http://arxiv.org/abs/2401.03221](http://arxiv.org/abs/2401.03221)

    本论文提出了一种通过提示重新描述的策略来稳定零样本图像翻译中的扩散过程，解决了去噪扩散概率模型在反演过程中的不稳定性问题。

    

    最近，文本到图像扩散模型成为图像处理领域的新范例，包括内容生成、图像恢复和图像到图像的翻译。给定一个目标提示，去噪扩散概率模型（DDPM）能够生成逼真而合适的图像。凭借这个吸引人的特性，图像翻译任务有潜力在没有目标图像样本进行监督的情况下进行。通过使用目标文本提示进行领域自适应，扩散模型能够优势地实现零样本图像到图像的翻译。然而，DDPM的采样和反演过程是随机的，因此反演过程常常无法完全重构输入内容。具体而言，在扩散和反演过程中，位移效应会逐渐积累，导致重构结果偏离源域。为了使重构更加明确，我们提出了一种提示重新描述策略来实现镜像效应。

    Recently, text-to-image diffusion models become a new paradigm in image processing fields, including content generation, image restoration and image-to-image translation. Given a target prompt, Denoising Diffusion Probabilistic Models (DDPM) are able to generate realistic yet eligible images. With this appealing property, the image translation task has the potential to be free from target image samples for supervision. By using a target text prompt for domain adaption, the diffusion model is able to implement zero-shot image-to-image translation advantageously. However, the sampling and inversion processes of DDPM are stochastic, and thus the inversion process often fail to reconstruct the input content. Specifically, the displacement effect will gradually accumulated during the diffusion and inversion processes, which led to the reconstructed results deviating from the source domain. To make reconstruction explicit, we propose a prompt redescription strategy to realize a mirror effect
    
[^19]: 理解非线性自监督学习的表示可学习性

    Understanding Representation Learnability of Nonlinear Self-Supervised Learning. (arXiv:2401.03214v1 [cs.LG])

    [http://arxiv.org/abs/2401.03214](http://arxiv.org/abs/2401.03214)

    这篇论文是第一次准确分析非线性自监督学习模型的学习结果，通过使用梯度下降算法训练模型并证明其收敛到局部最小值，在理解数据表示可学习性方面做出了重要贡献。

    

    自监督学习（SSL）在许多下游任务中经验证明其数据表示可学习性。关于数据表示可学习性的理论研究很少，其中许多将非线性神经网络视为“黑箱”，仅关注最终的数据表示。然而，神经网络的准确学习结果对于描述SSL模型学到的数据分布特征至关重要。我们的论文是首次准确分析非线性SSL模型的学习结果。我们考虑了一个包含两个特征的玩具数据分布：与标签相关的特征和隐藏特征。与以往的依赖于闭式解的线性设置工作不同，我们使用梯度下降算法在特定的初始化区域下训练一个1层非线性SSL模型，并证明模型收敛到一个局部最小值。此外，与复杂的迭代分析不同，我们提出了一个新的分析过程，使用t

    Self-supervised learning (SSL) has empirically shown its data representation learnability in many downstream tasks. There are only a few theoretical works on data representation learnability, and many of those focus on final data representation, treating the nonlinear neural network as a ``black box". However, the accurate learning results of neural networks are crucial for describing the data distribution features learned by SSL models. Our paper is the first to analyze the learning results of the nonlinear SSL model accurately. We consider a toy data distribution that contains two features: the label-related feature and the hidden feature. Unlike previous linear setting work that depends on closed-form solutions, we use the gradient descent algorithm to train a 1-layer nonlinear SSL model with a certain initialization region and prove that the model converges to a local minimum. Furthermore, different from the complex iterative analysis, we propose a new analysis process which uses t
    
[^20]: 在非稳定环境下的决策制定与策略增强搜索

    Decision Making in Non-Stationary Environments with Policy-Augmented Search. (arXiv:2401.03197v1 [cs.AI])

    [http://arxiv.org/abs/2401.03197](http://arxiv.org/abs/2401.03197)

    在非稳定环境下的决策制定是一个具有挑战性的问题，本文介绍了一种新的算法--策略增强蒙特卡洛树搜索（PA-MCTS），它将在线搜索与策略学习相结合。

    

    在许多重要问题中，存在着不确定性下的连续决策制定。针对这类问题，传统的方法包括强化学习和在线搜索（如蒙特卡洛树搜索）。前者通过与环境的交互来学习策略（通常在执行之前完成），而后者在决策时使用环境的生成模型来采样有前景的行动轨迹。在非稳定环境下的决策制定尤为具有挑战性，因为代理操作的环境可能随时间变化。两种方法在这种情况下都存在缺陷--一方面，执行之前学习的策略在环境改变时变得陈旧，重新学习需要时间和计算资源。另一方面，在线搜索在允许的运行时间有限时可能会返回次优行动。本文介绍了一种新的算法--策略增强蒙特卡洛树搜索（PA-MCTS），它将在线搜索与策略学习相结合。

    Sequential decision-making under uncertainty is present in many important problems. Two popular approaches for tackling such problems are reinforcement learning and online search (e.g., Monte Carlo tree search). While the former learns a policy by interacting with the environment (typically done before execution), the latter uses a generative model of the environment to sample promising action trajectories at decision time. Decision-making is particularly challenging in non-stationary environments, where the environment in which an agent operates can change over time. Both approaches have shortcomings in such settings -- on the one hand, policies learned before execution become stale when the environment changes and relearning takes both time and computational effort. Online search, on the other hand, can return sub-optimal actions when there are limitations on allowed runtime. In this paper, we introduce \textit{Policy-Augmented Monte Carlo tree search} (PA-MCTS), which combines actio
    
[^21]: SecureReg:一个结合方法用于主动暴露恶意域名注册

    SecureReg: A Combined Framework for Proactively Exposing Malicious Domain Name Registrations. (arXiv:2401.03196v1 [cs.CR])

    [http://arxiv.org/abs/2401.03196](http://arxiv.org/abs/2401.03196)

    SecureReg是一个结合了自然语言处理和多层感知器模型的方法，用于在域名注册过程中主动暴露恶意域名注册，提供了早期威胁检测的解决方案，显著减少了漏洞窗口，并为主动预防性操作做出了贡献。

    

    随着网络安全威胁不断增加，不法分子每天注册数千个新域名进行垃圾邮件、网络钓鱼和驱动下载等互联网攻击，强调了创新检测方法的需求。本文介绍了一种先进的方法，用于在注册过程开始时识别可疑域名。附带的数据流程通过比较新域名与注册域名产生关键特征，强调了关键相似度得分。利用自然语言处理（NLP）技术的新颖组合，包括预训练的Canine模型和多层感知器（MLP）模型，我们的系统分析语义和数值属性，为早期威胁检测提供了强大的解决方案。该综合方法显著减少了漏洞窗口，加强了对潜在威胁的防御。研究结果证明了该综合方法的有效性，并为开发主动预防性操作的努力做出了贡献。

    Rising cyber threats, with miscreants registering thousands of new domains daily for Internet-scale attacks like spam, phishing, and drive-by downloads, emphasize the need for innovative detection methods. This paper introduces a cutting-edge approach for identifying suspicious domains at the onset of the registration process. The accompanying data pipeline generates crucial features by comparing new domains to registered domains,emphasizing the crucial similarity score. Leveraging a novel combination of Natural Language Processing (NLP) techniques, including a pretrained Canine model, and Multilayer Perceptron (MLP) models, our system analyzes semantic and numerical attributes, providing a robust solution for early threat detection. This integrated approach significantly reduces the window of vulnerability, fortifying defenses against potential threats. The findings demonstrate the effectiveness of the integrated approach and contribute to the ongoing efforts in developing proactive s
    
[^22]: 通过拓扑数据分析学习动态网络中的持久社区结构

    Learning Persistent Community Structures in Dynamic Networks via Topological Data Analysis. (arXiv:2401.03194v1 [cs.AI])

    [http://arxiv.org/abs/2401.03194](http://arxiv.org/abs/2401.03194)

    本论文提出了一种通过拓扑数据分析学习动态网络中的持久社区结构的方法。通过引入最小化拓扑变化的概念，提出了一种深度图聚类框架，并引入了神经网络正则化方法，用于确保时间一致性和聚类准确性。

    

    动态社区检测方法通常缺乏有效的机制来确保时间一致性，从而阻碍了对网络演化的分析。本文提出了一种新颖的深度图聚类框架，通过对小时间间隔内的社区间结构进行拓扑变化的最小化，来保证时间的一致性。具体来说，为了解决表示塌缩问题，我们首先引入了基于矩阵分解的深度图聚类算法MFC，保留了节点嵌入。基于静态聚类结果，我们构建概率社区网络，并计算它们的持久同调，这是一种强大的拓扑度量，用于评估它们之间的结构相似性。此外，我们还引入了一种新颖的神经网络正则化方法TopoReg，用于确保社区间结构的拓扑相似性随时间间隔的保持。我们的方法提高了真实网络上的时间一致性和聚类准确性。

    Dynamic community detection methods often lack effective mechanisms to ensure temporal consistency, hindering the analysis of network evolution. In this paper, we propose a novel deep graph clustering framework with temporal consistency regularization on inter-community structures, inspired by the concept of minimal network topological changes within short intervals. Specifically, to address the representation collapse problem, we first introduce MFC, a matrix factorization-based deep graph clustering algorithm that preserves node embedding. Based on static clustering results, we construct probabilistic community networks and compute their persistence homology, a robust topological measure, to assess structural similarity between them. Moreover, a novel neural network regularization TopoReg is introduced to ensure the preservation of topological similarity between inter-community structures over time intervals. Our approach enhances temporal consistency and clustering accuracy on real-
    
[^23]: MPN: 利用多语言补丁神经元进行跨语言模型编辑

    MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model Editing. (arXiv:2401.03190v1 [cs.CL])

    [http://arxiv.org/abs/2401.03190](http://arxiv.org/abs/2401.03190)

    本论文提出了一种简单而有效的方法，利用多语言补丁神经元来解决多语言模型的跨语言知识同步问题，并取得了良好的实验结果。

    

    大型语言模型通常编码了大量的事实知识，但由于外部信息的不断变化，它们经常过时。解决这个挑战的一个有前途的方法是利用模型编辑方法以高效的方式更新知识。然而，大部分现有的模型编辑技术局限于单语框架，因此无法解决多语言模型的跨语言知识同步的关键问题。为了解决这个问题，我们提出了一种简单而有效的方法，即训练多语言补丁神经元来存储跨语言知识。它可以轻松适应现有方法，增强它们的跨语言编辑能力。为了评估我们的方法，我们使用XNLI数据集和自建的XFEVER数据集进行了实验。实验结果表明，我们提出的方法在跨语言编辑任务中实现了改进的性能，而不需要额外的工作。

    Large language models are known for encoding a vast amount of factual knowledge, but they often becomes outdated due to the ever-changing nature of external information. A promising solution to this challenge is the utilization of model editing methods to update the knowledge in an efficient manner. However, the majority of existing model editing techniques are limited to monolingual frameworks, thus failing to address the crucial issue of cross-lingual knowledge synchronization for multilingual models. To tackle this problem, we propose a simple yet effective method that trains multilingual patch neuron to store cross-lingual knowledge. It can be easily adapted to existing approaches to enhance their cross-lingual editing capabilities. To evaluate our method, we conduct experiments using both the XNLI dataset and a self-constructed XFEVER dataset. Experimental results demonstrate that our proposed method achieves improved performance in cross-lingual editing tasks without requiring ex
    
[^24]: 使用深度学习方法的Bodo语词性标注器

    Part-of-Speech Tagger for Bodo Language using Deep Learning approach. (arXiv:2401.03175v1 [cs.CL])

    [http://arxiv.org/abs/2401.03175](http://arxiv.org/abs/2401.03175)

    本文介绍了使用深度学习方法的Bodo语词性标注器。首先，我们提出了Bodo语言模型BodoBERT，这项工作是第一个为Bodo开发的语言模型。其次，我们提出了基于深度学习的Bodo词性标注模型，该模型利用了BiLSTM、CRF和BodoBERT与BytePairEmbeddings的组合。尽管研究已经在资源丰富的语言中进行了大量的语言模型和词性标注模型的研究，但对于低资源语言如Bodo，仍然缺乏相关研究。

    

    语言处理系统如词性标注、命名实体识别、机器翻译、语音识别和语言建模在资源丰富的语言中已经得到了广泛研究。然而，对于一些低资源语言，如Bodo、Mizo、Nagamese等，这些系统的研究要么尚未开始，要么处于起步阶段。语言模型在现代自然语言处理的下游任务中起着重要作用。对于资源丰富的语言已经进行了大量的语言模型研究。然而，像Bodo、Rabha和Mising这样的语言仍然缺乏相关研究。本研究首先提出了BodoBERT，一个用于Bodo语的语言模型。据我们所知，这项工作是第一个为Bodo开发语言模型的努力。其次，我们提出了一个基于集成深度学习的Bodo词性标注模型。该词性标注模型基于BiLSTM和CRF的组合，以及BodoBERT与BytePairEmbeddings的堆叠嵌入。我们涵盖了多种语言...（摘要被截断）

    Language Processing systems such as Part-of-speech tagging, Named entity recognition, Machine translation, Speech recognition, and Language modeling (LM) are well-studied in high-resource languages. Nevertheless, research on these systems for several low-resource languages, including Bodo, Mizo, Nagamese, and others, is either yet to commence or is in its nascent stages. Language model plays a vital role in the downstream tasks of modern NLP. Extensive studies are carried out on LMs for high-resource languages. Nevertheless, languages such as Bodo, Rabha, and Mising continue to lack coverage. In this study, we first present BodoBERT, a language model for the Bodo language. To the best of our knowledge, this work is the first such effort to develop a language model for Bodo. Secondly, we present an ensemble DL-based POS tagging model for Bodo. The POS tagging model is based on combinations of BiLSTM with CRF and stacked embedding of BodoBERT with BytePairEmbeddings. We cover several lan
    
[^25]: 基于人口普查调查和一般生活问题的青少年抑郁风险预测探索

    Exploration of Adolescent Depression Risk Prediction Based on Census Surveys and General Life Issues. (arXiv:2401.03171v1 [cs.LG])

    [http://arxiv.org/abs/2401.03171](http://arxiv.org/abs/2401.03171)

    本研究使用青少年人口普查数据来预测抑郁风险，关注儿童对抑郁的经历和日常生活情况。

    

    在当代社会，生活和工作的压力不断增加，心理疾病已成为现代健康关注的重要问题，而COVID-19疫情进一步凸显了这个问题。青少年抑郁的患病率正在稳步上升，而传统的诊断方法，如量表或面试，对于检测青少年抑郁显得尤为不足。面对这些挑战，出现了许多基于人工智能的方法来辅助诊断心理健康问题。然而，大多数这些方法都集中在量表存在的基本问题上，或者采用多模态方法，如面部表情识别。根据日常习惯和行为来诊断抑郁风险的研究局限于小规模的定性研究。我们的研究利用青少年人口普查数据来预测抑郁风险，重点关注儿童对抑郁的经历和他们的日常生活情况。我们引入了一种方法

    In contemporary society, the escalating pressures of life and work have propelled psychological disorders to the forefront of modern health concerns, an issue that has been further accentuated by the COVID-19 pandemic. The prevalence of depression among adolescents is steadily increasing, and traditional diagnostic methods, which rely on scales or interviews, prove particularly inadequate for detecting depression in young people. Addressing these challenges, numerous AI-based methods for assisting in the diagnosis of mental health issues have emerged. However, most of these methods center around fundamental issues with scales or use multimodal approaches like facial expression recognition. Diagnosis of depression risk based on everyday habits and behaviors has been limited to small-scale qualitative studies. Our research leverages adolescent census data to predict depression risk, focusing on children's experiences with depression and their daily life situations. We introduced a method
    
[^26]: PosDiffNet:具有扰动的大视角点云配准的位置神经扩散

    PosDiffNet: Positional Neural Diffusion for Point Cloud Registration in a Large Field of View with Perturbations. (arXiv:2401.03167v1 [cs.CV])

    [http://arxiv.org/abs/2401.03167](http://arxiv.org/abs/2401.03167)

    PosDiffNet是一种针对大视角扰动下的点云配准的模型，使用基于Beltrami流和神经常微分方程的图神经PDE以及位置嵌入来实现点云的高维特征表示和补丁间对齐，进而进行配准。

    

    点云配准是三维计算机视觉中一项关键技术，具有广泛的应用范围。然而，这个任务在具有动态对象、环境噪声或其他扰动的大视角中可能具有挑战性。为了应对这一挑战，我们提出了一种名为PosDiffNet的模型。我们的方法基于窗口级、补丁级和点级对应进行分层配准。我们利用基于Beltrami流的图神经偏微分方程(PDE)获得点云的高维特征和位置嵌入。我们将位置嵌入引入基于神经常微分方程(ODE)的Transformer模块中，以高效表示点内的补丁。我们利用高特征相似性分数导出的多级对应来促进点云之间的对齐。随后，我们使用基于SVD的算法等配准方法来预测变换。

    Point cloud registration is a crucial technique in 3D computer vision with a wide range of applications. However, this task can be challenging, particularly in large fields of view with dynamic objects, environmental noise, or other perturbations. To address this challenge, we propose a model called PosDiffNet. Our approach performs hierarchical registration based on window-level, patch-level, and point-level correspondence. We leverage a graph neural partial differential equation (PDE) based on Beltrami flow to obtain high-dimensional features and position embeddings for point clouds. We incorporate position embeddings into a Transformer module based on a neural ordinary differential equation (ODE) to efficiently represent patches within points. We employ the multi-level correspondence derived from the high feature similarity scores to facilitate alignment between point clouds. Subsequently, we use registration methods such as SVD-based algorithms to predict the transformation using c
    
[^27]: 人作为AI导师：增强人机协作强化学习以实现安全高效的自动驾驶

    Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])

    [http://arxiv.org/abs/2401.03160](http://arxiv.org/abs/2401.03160)

    本文提出了一种增强的人机协作强化学习方法，通过将人类智能注入到AI中实现混合交通编队中的安全高效自动驾驶。该方法将人类专家作为导师，允许代理在不确定环境中进行探索，同时在危险情况下接管控制以避免事故，并指导代理减小交通流干扰，优化交通流效果。

    

    尽管自动驾驶车辆（AVs）取得了重大进展，但确保AVs的安全性和交通流效率的驾驶策略的发展尚未得到充分探索。在本文中，我们提出了一种增强的人机协作强化学习方法，称为基于人作为AI导师的深度强化学习（HAIM-DRL）框架，以在混合交通编队中实现安全高效的自动驾驶。从人类学习过程中汲取灵感，我们首先引入了一种创新的学习范式，有效地将人类智能注入到AI中，称为人作为AI导师（HAIM）。在这个范式中，人类专家作为导师为AI代理提供帮助。在允许代理在不确定环境中进行充分探索的同时，人类专家可以在危险情况下接管控制，并展示正确的行动以避免潜在事故。另一方面，可以指导代理减小交通流干扰，从而优化交通流效果。

    Despite significant progress in autonomous vehicles (AVs), the development of driving policies that ensure both the safety of AVs and traffic flow efficiency has not yet been fully explored. In this paper, we propose an enhanced human-in-the-loop reinforcement learning method, termed the Human as AI mentor-based deep reinforcement learning (HAIM-DRL) framework, which facilitates safe and efficient autonomous driving in mixed traffic platoon. Drawing inspiration from the human learning process, we first introduce an innovative learning paradigm that effectively injects human intelligence into AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves as a mentor to the AI agent. While allowing the agent to sufficiently explore uncertain environments, the human expert can take control in dangerous situations and demonstrate correct actions to avoid potential accidents. On the other hand, the agent could be guided to minimize traffic flow disturbance, thereby optimizi
    
[^28]: 四步推理（QLFR）框架：推进短文本分类的四步推理

    Quartet Logic: A Four-Step Reasoning (QLFR) framework for advancing Short Text Classification. (arXiv:2401.03158v1 [cs.CL])

    [http://arxiv.org/abs/2401.03158](http://arxiv.org/abs/2401.03158)

    四步推理（QLFR）框架是一种通过引入句法和语义丰富的CoT来提升短文本分类任务中大型语言模型（LLMs）性能的方法。

    

    短文本分类（STC）对于处理和理解当代数字平台上流行的简洁而重要的内容至关重要。STC在抓住语义和句法复杂性方面遇到困难，这个问题在传统的预训练语言模型中很明显。尽管图卷积网络通过整合外部知识库提高了性能，但这些方法受到应用知识质量和范围的限制。最近，大型语言模型（LLMs）和思维链（CoT）的出现显著提高了复杂推理任务的性能。然而，一些研究指出了它们在基础NLP任务中的应用限制。因此，本研究旨在运用CoT来研究LLMs在STC任务中的能力。本研究引入了四步推理（QLFR）框架。这个框架主要包括句法和语义丰富的CoT，有效利用了LLMs的能力。

    Short Text Classification (STC) is crucial for processing and comprehending the brief but substantial content prevalent on contemporary digital platforms. The STC encounters difficulties in grasping semantic and syntactic intricacies, an issue that is apparent in traditional pre-trained language models. Although Graph Convolutional Networks enhance performance by integrating external knowledge bases, these methods are limited by the quality and extent of the knowledge applied. Recently, the emergence of Large Language Models (LLMs) and Chain-of-Thought (CoT) has significantly improved the performance of complex reasoning tasks. However, some studies have highlighted the limitations of their application in fundamental NLP tasks. Consequently, this study sought to employ CoT to investigate the capabilities of LLMs in STC tasks. This study introduces Quartet Logic: A Four-Step Reasoning (QLFR) framework. This framework primarily incorporates Syntactic and Semantic Enrichment CoT, effectiv
    
[^29]: 分散式多智能体主动搜索和跟踪当目标超过智能体数量时

    Decentralized Multi-Agent Active Search and Tracking when Targets Outnumber Agents. (arXiv:2401.03154v1 [cs.RO])

    [http://arxiv.org/abs/2401.03154](http://arxiv.org/abs/2401.03154)

    该论文介绍了一种处理多目标跟踪的分散式多智能体算法，该算法能在智能体数量少于目标数量时实现主动搜索和跟踪，并使用异步智能体通信来协调动作。

    

    多智能体多目标跟踪在野生动物巡逻、安全监控或环境监测等领域有广泛应用。现有算法常常做出一些限制性假设：目标数量和初始位置已知，或者智能体已被预分配到监控环境的不重叠分区，减轻了探索的负担。然而，当智能体数量少于目标数量时，这种假设会限制算法的适用性，因为智能体无法持续跟踪其视野中的目标。此外，多智能体跟踪算法还假设智能体间观测的同步，或者需要一个中央控制器来协调联合动作。相反，我们关注于分散式多智能体、多目标、同时主动搜索和跟踪的设置，其中智能体间通信是异步的。我们提出的算法DecSTER使用了一种基于概率假设密度滤波器的顺序蒙特卡洛实现。

    Multi-agent multi-target tracking has a wide range of applications, including wildlife patrolling, security surveillance or environment monitoring. Such algorithms often make restrictive assumptions: the number of targets and/or their initial locations may be assumed known, or agents may be pre-assigned to monitor disjoint partitions of the environment, reducing the burden of exploration. This also limits applicability when there are fewer agents than targets, since agents are unable to continuously follow the targets in their fields of view. Multi-agent tracking algorithms additionally assume inter-agent synchronization of observations, or the presence of a central controller to coordinate joint actions. Instead, we focus on the setting of decentralized multi-agent, multi-target, simultaneous active search-and-tracking with asynchronous inter-agent communication. Our proposed algorithm DecSTER uses a sequential monte carlo implementation of the probability hypothesis density filter fo
    
[^30]: TelTrans：通过多方面的图模型将多种类型的电信数据应用于交通评估和预测

    TelTrans: Applying Multi-Type Telecom Data to Transportation Evaluation and Prediction via Multifaceted Graph Modeling. (arXiv:2401.03138v1 [cs.LG])

    [http://arxiv.org/abs/2401.03138](http://arxiv.org/abs/2401.03138)

    这篇论文提出了使用地理单元交通流数据来改进交通评估和预测的方法。通过结合多变量、时间和空间方面的图神经网络，该方法在长期预测中取得了优越的准确率，并展示了将地理单元交通流整合到交通系统中的潜力。

    

    为了解决基于位置边界检测器的交通预测的局限性，我们提出了地理单元交通流（GCT）流，这是一种利用广泛的蜂窝交通覆盖来捕捉移动模式的新型数据来源。我们广泛的分析验证了它在交通领域的潜力。针对与车辆相关的GCT流预测，我们提出了一种图神经网络，结合了多变量、时间和空间方面，以提高准确率。实验证明我们的模型在长期预测中的优越性，我们还强调了将GCT流整合到交通系统中的潜力。

    To address the limitations of traffic prediction from location-bound detectors, we present Geographical Cellular Traffic (GCT) flow, a novel data source that leverages the extensive coverage of cellular traffic to capture mobility patterns. Our extensive analysis validates its potential for transportation. Focusing on vehicle-related GCT flow prediction, we propose a graph neural network that integrates multivariate, temporal, and spatial facets for improved accuracy. Experiments reveal our model's superiority over baselines, especially in long-term predictions. We also highlight the potential for GCT flow integration into transportation systems.
    
[^31]: SPQR:使用尖峰随机模型控制Q-集合的独立性，用于强化学习

    SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning. (arXiv:2401.03137v1 [cs.LG])

    [http://arxiv.org/abs/2401.03137](http://arxiv.org/abs/2401.03137)

    SPQR论文介绍了一种使用尖峰随机模型来控制强化学习中Q-集合的独立性的方法，通过引入基于随机矩阵理论的正则化损失来克服过高估计偏差。

    

    缓解过高估计偏差是深度强化学习在更复杂任务或包含超出分布数据的离线数据集上获得成功表现的关键挑战。为了克服过高估计偏差，研究了Q-learning的集成方法来利用多个Q函数的多样性。由于网络初始化一直是促进Q函数多样性的主要方法，因此在文献中研究了启发式设计的多样性注入方法。然而，先前的研究并未尝试从理论角度保证集成的独立性。通过引入基于随机矩阵理论的Q-集合独立性的新型正则化损失，我们提出了一种用于强化学习的尖峰Wishart Q-集合独立性正则化方法（SPQR）。

    Alleviating overestimation bias is a critical challenge for deep reinforcement learning to achieve successful performance on more complex tasks or offline datasets containing out-of-distribution data. In order to overcome overestimation bias, ensemble methods for Q-learning have been investigated to exploit the diversity of multiple Q-functions. Since network initialization has been the predominant approach to promote diversity in Q-functions, heuristically designed diversity injection methods have been studied in the literature. However, previous studies have not attempted to approach guaranteed independence over an ensemble from a theoretical perspective. By introducing a novel regularization loss for Q-ensemble independence based on random matrix theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR) for reinforcement learning. Specifically, we modify the intractable hypothesis testing criterion for the Q-ensemble independence into a tractable KL divergence 
    
[^32]: TimeGraphs: 基于图的时间推理

    TimeGraphs: Graph-based Temporal Reasoning. (arXiv:2401.03134v1 [cs.LG])

    [http://arxiv.org/abs/2401.03134](http://arxiv.org/abs/2401.03134)

    TimeGraphs是一种基于图的时间推理方法，通过将动态交互建模为分层时间图，实现了对不同时间尺度的自适应推理。

    

    许多现实世界的系统展示了时间上的动态行为，这些行为可以被捕捉为复杂代理交互的时间序列。为了进行时间推理，目前的方法主要通过简单的序列模型来编码时间动态。然而，一般来说，这些模型在有效捕捉输入中丰富动态的全谱时往往失败，因为动态并不均匀分布。特别是，有关信息可能更难提取，并且即使它们不包含重大变化或新信息，也会浪费计算能力来处理所有单独的时间步长。在这里，我们提出了TimeGraphs，一种将动态交互作为分层时间图来表征的新方法，与传统的顺序表示不同。我们的方法使用紧凑的基于图的表示来建模交互，实现了对不同时间尺度的自适应推理。采用自监督方法，TimeGraphs构建了一个多层级事件层次结构

    Many real-world systems exhibit temporal, dynamic behaviors, which are captured as time series of complex agent interactions. To perform temporal reasoning, current methods primarily encode temporal dynamics through simple sequence-based models. However, in general these models fail to efficiently capture the full spectrum of rich dynamics in the input, since the dynamics is not uniformly distributed. In particular, relevant information might be harder to extract and computing power is wasted for processing all individual timesteps, even if they contain no significant changes or no new information. Here we propose TimeGraphs, a novel approach that characterizes dynamic interactions as a hierarchical temporal graph, diverging from traditional sequential representations. Our approach models the interactions using a compact graph-based representation, enabling adaptive reasoning across diverse time scales. Adopting a self-supervised method, TimeGraphs constructs a multi-level event hierar
    
[^33]: 用物理指导的生成型人工智能工具包进行地球物理监测

    A Physics-guided Generative AI Toolkit for Geophysical Monitoring. (arXiv:2401.03131v1 [cs.LG])

    [http://arxiv.org/abs/2401.03131](http://arxiv.org/abs/2401.03131)

    EdGeo工具包利用物理原理指导的扩散模型生成高保真度的地下速度图，并通过使用声波方程生成地震波形数据来改善模型修剪后的ML模型性能。

    

    全波形反演(FWI)在地球科学中起着重要作用，用于探索地下。它利用地震波来成像地下速度图。随着机器学习(ML)技术的发展，使用ML的数据驱动方法已经出现，用于FWI任务，相比传统基于物理的方法，提供了更高的准确性和更低的计算成本。然而，地球科学中一个常见的挑战是没有数据，严重限制了ML的有效性。这个问题在模型修剪过程中变得更加严重，这是地球科学中必不可少的一步，因为环境复杂性。为了解决这个问题，我们介绍了EdGeo工具包，它使用物理原理引导的扩散模型来生成高保真度的速度图。该工具包使用声波方程来生成相应的地震波形数据，便于修剪后的ML模型进行微调。我们的结果表明，在各种情况下，SSIM分数都有显著提高，同时减少了MAE和MSE。

    Full-waveform inversion (FWI) plays a vital role in geoscience to explore the subsurface. It utilizes the seismic wave to image the subsurface velocity map. As the machine learning (ML) technique evolves, the data-driven approaches using ML for FWI tasks have emerged, offering enhanced accuracy and reduced computational cost compared to traditional physics-based methods. However, a common challenge in geoscience, the unprivileged data, severely limits ML effectiveness. The issue becomes even worse during model pruning, a step essential in geoscience due to environmental complexities. To tackle this, we introduce the EdGeo toolkit, which employs a diffusion-based model guided by physics principles to generate high-fidelity velocity maps. The toolkit uses the acoustic wave equation to generate corresponding seismic waveform data, facilitating the fine-tuning of pruned ML models. Our results demonstrate significant improvements in SSIM scores and reduction in both MAE and MSE across vario
    
[^34]: 基于流形的Shapley在合成孔径雷达（SAR）识别网络解释中的应用

    Manifold-based Shapley for SAR Recognization Network Explanation. (arXiv:2401.03128v1 [cs.AI])

    [http://arxiv.org/abs/2401.03128](http://arxiv.org/abs/2401.03128)

    本研究提出了一种基于流形的Shapley方法，通过将高维特征投影到低维流形特征中，解决了传统Shapley在高维模型以及复杂场景下的解释问题和可解释性挑战。

    

    可解释的人工智能（XAI）在增强深度神经网络的透明度和可信度方面具有重要意义，特别是在一些风险高和成本高的场景中，如合成孔径雷达（SAR）。Shapley是一种具有坚实数学基础的基于游戏的解释技术。然而，Shapley假设模型的特征是独立的，这使得对于高维模型，Shapley的解释无效。本研究引入了一种基于流形的Shapley方法，通过将高维特征投影到低维流形特征中，并随后获得Fusion-Shap，旨在解决传统Shap遇到的错误解释问题和复杂场景下的可解释性挑战。

    Explainable artificial intelligence (XAI) holds immense significance in enhancing the deep neural network's transparency and credibility, particularly in some risky and high-cost scenarios, like synthetic aperture radar (SAR). Shapley is a game-based explanation technique with robust mathematical foundations. However, Shapley assumes that model's features are independent, rendering Shapley explanation invalid for high dimensional models. This study introduces a manifold-based Shapley method by projecting high-dimensional features into low-dimensional manifold features and subsequently obtaining Fusion-Shap, which aims at (1) addressing the issue of erroneous explanations encountered by traditional Shap; (2) resolving the challenge of interpretability that traditional Shap faces in complex scenarios.
    
[^35]: 一种解决人工智能黑盒问题的白盒解决方案

    A white box solution to the black box problem of AI. (arXiv:2401.03093v1 [cs.AI])

    [http://arxiv.org/abs/2401.03093](http://arxiv.org/abs/2401.03093)

    一种解决人工智能黑盒问题的白盒解决方案是使用基于相关领域一般理论的确定性逻辑细胞自动机的规则，该细胞自动机实现自动并行逻辑推理。

    

    基于神经网络的人工智能取得了重大进展。然而，由于缺乏透明性，对其可靠性和安全性存在担忧。这就是人工智能的黑盒问题。在这里，我们展示了如何使用符号 AI 来解决这个问题，符号 AI 具有透明的白盒性质。符号 AI 的广泛应用受到数学模型和自然语言术语的不透明性、缺乏统一本体论以及搜索选项的组合爆炸的阻碍。为了解决人工智能的黑盒问题并实现通用的符号 AI，我们提议使用基于相关领域一般理论的确定性逻辑细胞自动机的规则。在这种情况下，相关领域的一般理论起到了细胞自动机推理的知识库的作用。细胞自动机在复杂系统的三个层次上实现自动并行逻辑推理。

    Artificial intelligence based on neural networks has made significant progress. However, there are concerns about the reliability and security of this approach due to its lack of transparency. This is the black box problem of AI. Here we show how this problem can be solved using symbolic AI, which has a transparent white box nature. The widespread use of symbolic AI is hindered by the opacity of mathematical models and natural language terms, the lack of a unified ontology, and the combinatorial explosion of search options. To solve the AI black box problem and to implement general-purpose symbolic AI, we propose to use deterministic logic cellular automata with rules based on first principles of the general theory of the relevant domain. In this case, the general theory of the relevant domain plays the role of a knowledge base for the cellular automaton inference. A cellular automaton implements automatic parallel logical inference at three levels of organization of a complex system. 
    
[^36]: UMIE：带有指导调节的统一多模态信息提取

    UMIE: Unified Multimodal Information Extraction with Instruction Tuning. (arXiv:2401.03082v1 [cs.AI])

    [http://arxiv.org/abs/2401.03082](http://arxiv.org/abs/2401.03082)

    UMIE是一种统一的多模态信息提取器，使用指导调节的方法能够跨任务进行信息提取，并且具有强大的泛化能力和可解释性。

    

    随着多媒体内容的普及，多模态信息提取（MIE）越来越受关注。然而，当前的MIE方法通常采用特定任务的模型结构，导致跨任务的泛化能力有限，并且未充分利用共享的MIE任务知识。为了解决这些问题，我们提出了UMIE，一种统一的多模态信息提取器，将三个MIE任务统一为一个生成问题，使用指导调节能够有效提取文本和视觉提及。大量实验证明，我们的单一UMIE在三个任务的六个MIE数据集上优于各种最先进（SoTA）方法。此外，深入分析证明UMIE在零-shot设置下具有很强的泛化能力，对指导变体具有鲁棒性，并能提供可解释性。我们的研究是朝着统一的MIE模型迈出的第一步，同时也开始探索指导调节和大型语言模型在MI领域的应用。

    Multimodal information extraction (MIE) gains significant attention as the popularity of multimedia content increases. However, current MIE methods often resort to using task-specific model structures, which results in limited generalizability across tasks and underutilizes shared knowledge across MIE tasks. To address these issues, we propose UMIE, a unified multimodal information extractor to unify three MIE tasks as a generation problem using instruction tuning, being able to effectively extract both textual and visual mentions. Extensive experiments show that our single UMIE outperforms various state-of-the-art (SoTA) methods across six MIE datasets on three tasks. Furthermore, in-depth analysis demonstrates UMIE's strong generalization in the zero-shot setting, robustness to instruction variants, and interpretability. Our research serves as an initial step towards a unified MIE model and initiates the exploration into both instruction tuning and large language models within the MI
    
[^37]: CRUXEval: 一个代码推理、理解和执行的基准测试

    CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution. (arXiv:2401.03065v1 [cs.SE])

    [http://arxiv.org/abs/2401.03065](http://arxiv.org/abs/2401.03065)

    CRUXEval是一个包含800个Python函数的代码推理、理解和执行的基准测试。通过评估二十个代码模型，发现许多在HumanEval上得分高的模型在该基准测试上没有相同的改进。使用CoT的GPT-4展现了最佳性能，但仍未解决问题。与开源模型相比，闭源模型的性能差距更大。

    

    我们提出了CRUXEval（代码推理、理解和执行评估），一个包含800个Python函数（3-13行）的基准测试。每个函数都有一个输入输出对，可以进行两个自然任务：输入预测和输出预测。首先，我们提出了一个通用的生成执行基准测试的方法，可以用来创建未来变种的基准测试。其次，我们在我们的基准测试中评估了二十个代码模型，并发现许多最近在HumanEval上得分高的模型在我们的基准测试上没有取得相同的改进。第三，我们展示了简单的CoT和微调方案可以改善在我们的基准测试上的性能，但仍然远未解决问题。最佳设置，使用CoT的GPT-4，在输入和输出预测上的pass@1分别达到了75%和81%。相比之下，Code Llama 34B在输入和输出预测上的pass@1分别为50%和46%，突显了开源模型和闭源模型之间的差距。

    We present CRUXEval (Code Reasoning, Understanding, and eXecution Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each function comes with an input-output pair, leading to two natural tasks: input prediction and output prediction. First, we propose a generic recipe for generating our execution benchmark which can be used to create future variation of the benchmark. Second, we evaluate twenty code models on our benchmark and discover that many recent high-scoring models on HumanEval do not show the same improvements on our benchmark. Third, we show that simple CoT and fine-tuning schemes can improve performance on our benchmark but remain far from solving it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75% and 81% on input and output prediction, respectively. In contrast, Code Llama 34B achieves a pass@1 of 50% and 46% on input and output prediction, highlighting the gap between open and closed source models. As no model is close to 
    
[^38]: 为URLLC流量优化可靠性的用户接入控制：一种神经上下文强化学习方法

    Reliability-Optimized User Admission Control for URLLC Traffic: A Neural Contextual Bandit Approach. (arXiv:2401.03059v1 [cs.LG])

    [http://arxiv.org/abs/2401.03059](http://arxiv.org/abs/2401.03059)

    本研究提出了一种基于神经上下文强化学习方法的QoS-aware UE接入控制策略，旨在在关联URRLC UE与小区之前，准确估计QoS并避免小区过载。

    

    超可靠低延迟通信（URLLC）是下一代无线网络中广泛范围的新兴服务的基石。URLLC基本上依赖于网络能够主动确定是否有足够的资源来支持URLLC流量，并因此防止所谓的小区过载。然而，为URLLC用户设备（UE）实现准确的服务质量（QoS）预测并防止小区超载是非常具有挑战性的任务。这是由于QoS指标（延迟和可靠性）依赖于流量和信道统计数据、用户的移动性和UE之间的相互依赖性。在本文中，开发了一种新的QoS感知UE接入控制方法，用于在将其与小区关联之前，主动估计URLLC UE的QoS，并相应地仅接纳不会导致小区过载的UE子集。为此，制定了一个优化问题来找到一种有效的UE接入控制策略，

    Ultra-reliable low-latency communication (URLLC) is the cornerstone for a broad range of emerging services in next-generation wireless networks. URLLC fundamentally relies on the network's ability to proactively determine whether sufficient resources are available to support the URLLC traffic, and thus, prevent so-called cell overloads. Nonetheless, achieving accurate quality-of-service (QoS) predictions for URLLC user equipment (UEs) and preventing cell overloads are very challenging tasks. This is due to dependency of the QoS metrics (latency and reliability) on traffic and channel statistics, users' mobility, and interdependent performance across UEs. In this paper, a new QoS-aware UE admission control approach is developed to proactively estimate QoS for URLLC UEs, prior to associating them with a cell, and accordingly, admit only a subset of UEs that do not lead to a cell overload. To this end, an optimization problem is formulated to find an efficient UE admission control policy,
    
[^39]: AccidentGPT:用于交通事故分析的大型多模态基础模型

    AccidentGPT: Large Multi-Modal Foundation Model for Traffic Accident Analysis. (arXiv:2401.03040v1 [cs.LG])

    [http://arxiv.org/abs/2401.03040](http://arxiv.org/abs/2401.03040)

    本文介绍了AccidentGPT，这是一个用于交通事故分析的大型多模态基础模型。它利用多模态输入数据自动重建事故过程视频，并提供多模态输出的多任务分析。此外，AccidentGPT还采用了多模态提示与反馈、混合训练模式和边缘-云分割配置以增强性能，并提出了一些研究机会。

    

    交通事故分析对于提高公共安全和制定道路规章制度至关重要。传统方法虽然被广泛使用，但常常受限于手动分析过程、主观决策、单模态输出以及与敏感数据相关的隐私问题。本文介绍了AccidentGPT的概念，这是一个交通事故分析的基础模型，它将多模态输入数据用于自动重建事故过程视频并提供多模态输出的多任务分析。AccidentGPT的设计采用了多模态提示与反馈，用于任务导向的适应性，混合训练模式以利用标记和未标记数据，以及边缘-云分割配置以保护数据隐私。为了充分发挥该模型的功能，我们提出了一些研究机会。本文将填补传统方法中的空白。

    Traffic accident analysis is pivotal for enhancing public safety and developing road regulations. Traditional approaches, although widely used, are often constrained by manual analysis processes, subjective decisions, uni-modal outputs, as well as privacy issues related to sensitive data. This paper introduces the idea of AccidentGPT, a foundation model of traffic accident analysis, which incorporates multi-modal input data to automatically reconstruct the accident process video with dynamics details, and furthermore provide multi-task analysis with multi-modal outputs. The design of the AccidentGPT is empowered with a multi-modality prompt with feedback for task-oriented adaptability, a hybrid training schema to leverage labelled and unlabelled data, and a edge-cloud split configuration for data privacy. To fully realize the functionalities of this model, we proposes several research opportunities. This paper serves as the stepping stone to fill the gaps in traditional approaches of t
    
[^40]: 时间序列预测中扩散模型的兴起

    The Rise of Diffusion Models in Time-Series Forecasting. (arXiv:2401.03006v1 [cs.LG])

    [http://arxiv.org/abs/2401.03006](http://arxiv.org/abs/2401.03006)

    本文调查了扩散模型在时间序列预测中的应用，提供了对这些模型的全面背景信息和详细说明，同时也对它们在不同数据集上的有效性和彼此之间的比较进行了分析。其贡献包括对扩散模型在时间序列预测中应用的彻底探索和按时间顺序排序的模型概述。这是一份对人工智能和时间序列分析领域的研究人员来说具有价值的资源。

    

    本调查探讨了扩散模型在时间序列预测中的应用。扩散模型在生成型人工智能的各个领域中展示出最先进的结果。本文包括对扩散模型的全面背景信息，详细介绍其条件方法，并审查了其在时间序列预测中的应用。分析涵盖了11个具体的时间序列实现，它们的直觉和理论基础，不同数据集上的有效性以及彼此之间的比较。该工作的关键贡献是对扩散模型在时间序列预测中应用的彻底探索，并提供了一个按时间顺序排序的模型概述。此外，本文对该领域的最新技术水平进行了深入讨论，并概述了潜在的未来研究方向。这对于人工智能和时间序列分析领域的研究人员来说是一份宝贵的资源，提供了对最新进展和未来发展的清晰视图。

    This survey delves into the application of diffusion models in time-series forecasting. Diffusion models are demonstrating state-of-the-art results in various fields of generative AI. The paper includes comprehensive background information on diffusion models, detailing their conditioning methods and reviewing their use in time-series forecasting. The analysis covers 11 specific time-series implementations, the intuition and theory behind them, the effectiveness on different datasets, and a comparison among each other. Key contributions of this work are the thorough exploration of diffusion models' applications in time-series forecasting and a chronologically ordered overview of these models. Additionally, the paper offers an insightful discussion on the current state-of-the-art in this domain and outlines potential future research directions. This serves as a valuable resource for researchers in AI and time-series analysis, offering a clear view of the latest advancements and future p
    
[^41]: 跨模态连接：知识蒸馏和屏蔽训练用于将多模态情感识别转化为单模态、仅语音的情感识别

    Bridging Modalities: Knowledge Distillation and Masked Training for Translating Multi-Modal Emotion Recognition to Uni-Modal, Speech-Only Emotion Recognition. (arXiv:2401.03000v1 [cs.SD])

    [http://arxiv.org/abs/2401.03000](http://arxiv.org/abs/2401.03000)

    本文介绍了一种创新方法，用于将多模态情感识别转化为更实用且资源高效的单模态、仅语音的情感识别。使用知识蒸馏和屏蔽训练技术来解决现有模型所依赖的多模态输入在实际应用中可能不可行的问题。

    

    本文提出了一种创新方法，以解决将多模态情感识别模型转化为更实用且资源高效的单模态情感识别模型的挑战，具体关注语音情感识别。从语音信号中识别情感是一项关键任务，应用于人机交互、情感计算和心理健康评估。然而，现有的最先进模型通常依赖于多模态输入，包括来自多个来源的信息，如面部表情和手势，在实际场景中可能不易获得或不可行。为了解决这个问题，我们提出了一种利用知识蒸馏和屏蔽训练技术的新框架。

    This paper presents an innovative approach to address the challenges of translating multi-modal emotion recognition models to a more practical and resource-efficient uni-modal counterpart, specifically focusing on speech-only emotion recognition. Recognizing emotions from speech signals is a critical task with applications in human-computer interaction, affective computing, and mental health assessment. However, existing state-of-the-art models often rely on multi-modal inputs, incorporating information from multiple sources such as facial expressions and gestures, which may not be readily available or feasible in real-world scenarios. To tackle this issue, we propose a novel framework that leverages knowledge distillation and masked training techniques.
    
[^42]: Blar-SQL: 更快、更强、更小的NL2SQL

    Blar-SQL: Faster, Stronger, Smaller NL2SQL. (arXiv:2401.02997v1 [cs.CL])

    [http://arxiv.org/abs/2401.02997](http://arxiv.org/abs/2401.02997)

    该研究通过任务分解，提出了Blar-SQL框架，将两个不同的模型组合，分别专注于不同的任务，从而极大地提高了NL2SQL任务的准确性。该模型比GPT-4更小、更快、更便宜。

    

    大型语言模型（LLMs）在自然语言到SQL任务（NL2SQL）领域取得了可观的声誉。在本研究中，我们展示了如何通过任务分解来极大地改善LLMs在数据库理解和查询生成方面的能力，以便使用一个SQL查询来回答人类的问题。我们通过组合两个不同模型，分别专注于两个任务，对开源模型（特别是Llama-2和Code Llama）进行了精调，以利用每个模型的核心竞争力，进一步提高最终SQL查询的准确性。我们提出了一个新的框架来将模式划分为块，以将更多信息适应有限的上下文中。我们的结果与GPT-4获得的结果相当，同时比GPT-4更小135倍、更快90倍，并且比GPT-4便宜100倍以上。

    Large Language Models (LLMs) have gained considerable notoriety in the field of natural language to SQL tasks (NL2SQL). In this study, we show how task decomposition can greatly benefit LLMs in database understanding and query generation in order to answer human questions with an SQL query.  We fined-tuned open source models, specifically Llama-2 and Code Llama, by combining 2 different models each designated to focus on one of two tasks in order to leverage each model's core competency to further increase the accuracy of the final SQL query.  We propose a new framework to divide the schema into chunks in order to fit more information into a limited context. Our results are comparable with those obtained by GPT-4 at the same time being 135 times smaller, 90 times faster and more than 100 times cheaper than GPT-4.
    
[^43]: CANAMRF：一种基于注意力的多模态抑郁检测模型

    CANAMRF: An Attention-Based Model for Multimodal Depression Detection. (arXiv:2401.02995v1 [cs.CL])

    [http://arxiv.org/abs/2401.02995](http://arxiv.org/abs/2401.02995)

    CANAMRF 是一种基于注意力机制的多模态抑郁检测模型，通过考虑不同模态之间的相对重要性，有效地进行多模态表示，并在两个基准数据集上取得了最先进的性能。

    

    多模态抑郁检测是一个重要的研究课题，旨在利用多模态数据预测人类的心理状态。之前的方法平等对待不同的模态，并通过简单的数学运算融合每个模态，没有对它们之间的相对重要性进行衡量，这不能获得适用于下游抑郁任务的良好多模态表示。为了解决上述问题，我们提出了一种带有自适应多模态循环融合的跨模态注意力网络（CANAMRF）用于多模态抑郁检测。CANAMRF由多模态特征提取器、自适应多模态循环融合模块和混合注意力模块构成。通过在两个基准数据集上进行实验，CANAMRF展示出了最先进的性能，突显了我们提出方法的有效性。

    Multimodal depression detection is an important research topic that aims to predict human mental states using multimodal data. Previous methods treat different modalities equally and fuse each modality by na\"ive mathematical operations without measuring the relative importance between them, which cannot obtain well-performed multimodal representations for downstream depression tasks. In order to tackle the aforementioned concern, we present a Cross-modal Attention Network with Adaptive Multi-modal Recurrent Fusion (CANAMRF) for multimodal depression detection. CANAMRF is constructed by a multimodal feature extractor, an Adaptive Multimodal Recurrent Fusion module, and a Hybrid Attention Module. Through experimentation on two benchmark datasets, CANAMRF demonstrates state-of-the-art performance, underscoring the effectiveness of our proposed approach.
    
[^44]: 基于混合方法的聊天AI模型：相对于万亿级参数模型的更廉价、更好的替代方案

    Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM. (arXiv:2401.02994v1 [cs.CL])

    [http://arxiv.org/abs/2401.02994](http://arxiv.org/abs/2401.02994)

    本研究介绍了一种名为“混合”的方法，通过组合多个适度规模的聊天AI模型，可以达到或超越比它们更大的模型的性能表现。

    

    在会话型AI研究中，越来越多的模型采用了更多的参数，如ChatGPT等模型。虽然这些庞大的模型往往能生成更好的聊天回复，但它们需要大量的计算资源和内存。本研究探讨了一个重要问题：能否通过组合较小的模型来达到与单个大模型相当或更好的性能？我们提出了一种称为“混合”的方法，它是一种简单但有效的将多个聊天AI集成在一起的方法。我们的实证证据表明，当特定较小的模型协同混合时，它们可以潜在地超越或匹敌大型模型的性能。例如，仅集成三个适度规模的模型（6B/13B参数）就可以达到或甚至超越ChatGPT（175B+参数）等大型模型的性能指标。这个假设经过了严格的测试。

    In conversational AI research, there's a noticeable trend towards developing models with a larger number of parameters, exemplified by models like ChatGPT. While these expansive models tend to generate increasingly better chat responses, they demand significant computational resources and memory. This study explores a pertinent question: Can a combination of smaller models collaboratively achieve comparable or enhanced performance relative to a singular large model? We introduce an approach termed "blending", a straightforward yet effective method of integrating multiple chat AIs. Our empirical evidence suggests that when specific smaller models are synergistically blended, they can potentially outperform or match the capabilities of much larger counterparts. For instance, integrating just three models of moderate size (6B/13B paramaeters) can rival or even surpass the performance metrics of a substantially larger model like ChatGPT (175B+ paramaters). This hypothesis is rigorously tes
    
[^45]: 使用计算高效的检索表示融合提高自然语言理解能力

    Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion. (arXiv:2401.02993v1 [cs.CL])

    [http://arxiv.org/abs/2401.02993](http://arxiv.org/abs/2401.02993)

    本文提出了一种计算高效的检索表示融合方法ReFusion，通过将检索表示直接融合到语言模型中，解决了将外部数据库的知识融入非知识密集型任务中的挑战。

    

    从外部数据库中获取知识并融入语言模型的检索增强方法在各种知识密集型任务（例如问答和文本生成）中取得了巨大成功。然而，在非知识密集型任务（例如文本分类）中集成检索仍然具有挑战性。现有的研究集中在将检索内容拼接到输入中形成提示性的输入。然而，这种方法要求语言模型具备处理长文本的能力。此外，推断这种拼接数据也会消耗大量的计算资源。为了解决这些问题，本文提出了一种计算高效的检索表示融合方法ReFusion，采用神经架构搜索。主要思想是直接将检索表示与语言模型进行融合。具体地，我们首先提出了一种在线检索模块，重新检索...

    Retrieval-based augmentations that aim to incorporate knowledge from an external database into language models have achieved great success in various knowledge-intensive (KI) tasks, such as question-answering and text generation. However, integrating retrievals in non-knowledge-intensive (NKI) tasks, such as text classification, is still challenging. Existing works focus on concatenating retrievals to inputs as context to form the prompt-based inputs. Unfortunately, such methods require language models to have the capability to handle long texts. Besides, inferring such concatenated data would also consume a significant amount of computational resources.  To solve these challenges, we propose \textbf{ReFusion} in this paper, a computation-efficient \textbf{Re}trieval representation \textbf{Fusion} with neural architecture search. The main idea is to directly fuse the retrieval representations into the language models. Specifically, we first propose an online retrieval module that retri
    
[^46]: ESG报告的高级非结构化数据处理：结构化转换和增强分析的方法论

    Advanced Unstructured Data Processing for ESG Reports: A Methodology for Structured Transformation and Enhanced Analysis. (arXiv:2401.02992v1 [cs.CL])

    [http://arxiv.org/abs/2401.02992](http://arxiv.org/abs/2401.02992)

    本研究引入了一种创新的方法论，利用"非结构化核心库"，将ESG报告转换为结构化、可分析的格式，并在文本清洗、从图像中敏锐识别和提取文本以及标准化报告中的表格等方面取得了显著进展，为工业生态学和企业可持续性评估领域的发展奠定了基础。

    

    在企业可持续性发展领域，分析非结构化的环境、社会和治理（ESG）报告是一个复杂的挑战，因为它们具有各种格式和复杂的内容。本研究引入了一种创新的方法论，利用"非结构化核心库"，专门针对这些挑战将ESG报告转换为结构化、可分析的格式。我们的方法在文本清洗、从图像中敏锐地识别和提取文本以及标准化这些报告中的表格等方面显著推进了现有研究。强调其处理不同行业的不同页面布局和报告样式方面的能力，该方法熟练地管理各种数据类型，包括文本、图像和表格。这项研究对于工业生态学和企业可持续性评估领域具有重大意义，为应用先进的自然语言处理技术奠定了基础。

    In the evolving field of corporate sustainability, analyzing unstructured Environmental, Social, and Governance (ESG) reports is a complex challenge due to their varied formats and intricate content. This study introduces an innovative methodology utilizing the "Unstructured Core Library", specifically tailored to address these challenges by transforming ESG reports into structured, analyzable formats. Our approach significantly advances the existing research by offering high-precision text cleaning, adept identification and extraction of text from images, and standardization of tables within these reports. Emphasizing its capability to handle diverse data types, including text, images, and tables, the method adeptly manages the nuances of differing page layouts and report styles across industries. This research marks a substantial contribution to the fields of industrial ecology and corporate sustainability assessment, paving the way for the application of advanced NLP technologies an
    
[^47]: 你的预训练模型有改进吗？一种基于多头后验的方法

    Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach. (arXiv:2401.02987v1 [cs.CL])

    [http://arxiv.org/abs/2401.02987](http://arxiv.org/abs/2401.02987)

    本研究提出一种基于多头后验的方法，通过利用实体的元特征和模型的表示之间的一致性作为度量标准，有效评估预训练模型在各个领域的表现。

    

    预训练模型的出现对自然语言处理（NLP）、计算机视觉和关系型数据集等领域产生了显著影响。传统上，这些模型通过下游任务进行评估。然而，这引发了如何更高效、更有效地评估这些模型的问题。在本研究中，我们探索了一种新颖的方法，即利用与每个实体相关的元特征作为世界知识的来源，并利用模型的实体表示。我们提出使用这些表示和元特征之间的一致性作为评估预训练模型的度量标准。我们的方法在各个领域表现出了有效性，包括具有关系型数据集、大型语言模型和图像模型的模型。

    The emergence of pretrained models has significantly impacted from Natural Language Processing (NLP) and Computer Vision to relational datasets. Traditionally, these models are assessed through fine-tuned downstream tasks. However, this raises the question of how to evaluate these models more efficiently and more effectively. In this study, we explore a novel approach where we leverage the meta features associated with each entity as a source of worldly knowledge and employ entity representations from the models. We propose using the consistency between these representations and the meta features as a metric for evaluating pretrained models. Our method's effectiveness is demonstrated across various domains, including models with relational datasets, large language models and images models.
    
[^48]: 识别与业务流程相关的监管要求：一项关于生成AI、基于嵌入的排序、众包和专家驱动方法的比较研究

    Identification of Regulatory Requirements Relevant to Business Processes: A Comparative Study on Generative AI, Embedding-based Ranking, Crowd and Expert-driven Methods. (arXiv:2401.02986v1 [cs.CL])

    [http://arxiv.org/abs/2401.02986](http://arxiv.org/abs/2401.02986)

    本研究比较了基于嵌入的自然语言处理排序方法，生成AI模型以及众包和专家驱动方法，旨在研究如何辅助法律和领域专家识别与业务流程相关的监管要求。

    

    组织面临着确保遵守各种监管文件中越来越多要求的挑战。哪些要求是相关的取决于如组织的地理位置、领域、规模和业务流程等方面的因素。考虑到这些情境因素，首先需要识别相关文件（例如法律、规则、指令、政策），然后详细分析识别文件的哪些部分与给定业务流程的哪个步骤相关。目前，识别与业务流程相关的监管要求主要由领域和法律专家手动完成，对他们来说是一项巨大的工作量，特别是对于可能经常变化的大量监管文件。因此，本研究探讨了如何辅助法律和领域专家评估相关要求。为此，我们比较了基于嵌入的自然语言处理排序方法，生成AI模型及众包和专家驱动方法。

    Organizations face the challenge of ensuring compliance with an increasing amount of requirements from various regulatory documents. Which requirements are relevant depends on aspects such as the geographic location of the organization, its domain, size, and business processes. Considering these contextual factors, as a first step, relevant documents (e.g., laws, regulations, directives, policies) are identified, followed by a more detailed analysis of which parts of the identified documents are relevant for which step of a given business process. Nowadays the identification of regulatory requirements relevant to business processes is mostly done manually by domain and legal experts, posing a tremendous effort on them, especially for a large number of regulatory documents which might frequently change. Hence, this work examines how legal and domain experts can be assisted in the assessment of relevant requirements. For this, we compare an embedding-based NLP ranking method, a generativ
    
[^49]: 在GMAT上评估大型语言模型：对未来商业教育的影响

    Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education. (arXiv:2401.02985v1 [cs.CL])

    [http://arxiv.org/abs/2401.02985](http://arxiv.org/abs/2401.02985)

    本研究评估了七个主要的大型语言模型在GMAT考试上的表现，并发现大多数模型优于人类考生，其中GPT-4 Turbo不仅在其他模型之上，而且超过了顶级商学院研究生的平均分数。此外，通过案例研究，本研究还考察了GPT-4 Turbo在解释答案、评估回答、识别错误、调整指导和生成替代场景方面的能力。

    

    人工智能尤其是大型语言模型和生成式人工智能的迅速发展，为在各个领域中的应用开辟了新的途径，然而其在商业教育中的作用仍未得到充分探索。本研究引入了首个基准来评估七个重要的大型语言模型，包括OpenAI的模型（GPT-3.5 Turbo、GPT-4和GPT-4 Turbo）、Google的模型（PaLM 2、Gemini 1.0 Pro）和Anthropic的模型（Claude 2和Claude 2.1），在GMAT考试上的表现。我们的分析显示，大多数语言模型优于人类考生，其中GPT-4 Turbo不仅在其他模型之上，而且超过了顶级商学院研究生的平均分数。通过一个案例研究，本研究考察了GPT-4 Turbo在解释答案、评估回答、识别错误、调整指导和生成替代场景方面的能力。

    The rapid evolution of artificial intelligence (AI), especially in the domain of Large Language Models (LLMs) and generative AI, has opened new avenues for application across various fields, yet its role in business education remains underexplored. This study introduces the first benchmark to assess the performance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and GPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models (Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission process for graduate business programs. Our analysis shows that most LLMs outperform human candidates, with GPT-4 Turbo not only outperforming the other models but also surpassing the average scores of graduate students at top business schools. Through a case study, this research examines GPT-4 Turbo's ability to explain answers, evaluate responses, identify errors, tailor instructions, and generate alternative scenarios. The latest LLM versions, GPT-4 Turbo,
    
[^50]: 大型语言模型在心理健康护理中的应用：一项综述研究

    Large Language Models in Mental Health Care: a Scoping Review. (arXiv:2401.02984v1 [cs.CL])

    [http://arxiv.org/abs/2401.02984](http://arxiv.org/abs/2401.02984)

    本综述研究对大型语言模型在心理健康护理中的应用和结果进行了综合分析，发现其在诊断、治疗和患者参与增强等方面具有多样化的应用。同时，该研究还识别和讨论了在这些专业领域中所面临的挑战和限制。

    

    目的：大型语言模型（LLM）的使用越来越广泛，需要对它们在心理健康护理领域的应用和结果进行全面的综述。本综述研究旨在对LLMs在心理健康护理中的现有发展和应用进行批判性分析，突出它们的成功，并识别这些专业领域中的挑战和限制。材料和方法：2023年11月，在PubMed、Web of Science、Google Scholar、arXiv、medRxiv和PsyArXiv六个数据库中进行了广泛的文献搜索，遵循2020年版的“系统评价和Meta分析的首选报告项目”（PRISMA）指南。最初识别了313篇出版物，按照研究纳入标准，最终选择了34篇出版物进行综述。结果：我们发现了LLMs在心理健康护理中的多种应用，包括诊断、治疗、患者参与增强等。关键挑战和限制方面的发现将被总结和讨论。

    Objective: The growing use of large language models (LLMs) stimulates a need for a comprehensive review of their applications and outcomes in mental health care contexts. This scoping review aims to critically analyze the existing development and applications of LLMs in mental health care, highlighting their successes and identifying their challenges and limitations in these specialized fields. Materials and Methods: A broad literature search was conducted in November 2023 using six databases (PubMed, Web of Science, Google Scholar, arXiv, medRxiv, and PsyArXiv) following the 2020 version of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. A total of 313 publications were initially identified, and after applying the study inclusion criteria, 34 publications were selected for the final review. Results: We identified diverse applications of LLMs in mental health care, including diagnosis, therapy, patient engagement enhancement, etc. Key challen
    
[^51]: BIBench: 大型语言模型数据分析知识基准测试

    BIBench: Benchmarking Data Analysis Knowledge of Large Language Models. (arXiv:2401.02982v1 [cs.CL])

    [http://arxiv.org/abs/2401.02982](http://arxiv.org/abs/2401.02982)

    BIBench是一个旨在评估大型语言模型（LLMs）在商业智能（BI）数据分析领域中能力的综合基准测试，其通过测试模型在BI基础知识、应用知识和技术技能三个维度上的表现来进行评估。

    

    大型语言模型（LLMs）在各种任务中展示了令人印象深刻的能力。然而，它们在数据分析的专业领域中的熟练度和可靠性，特别是在以数据驱动思维为重点的领域中，仍然存在不确定性。为了填补这一差距，我们介绍了BIBench，这是一个全面的基准测试，旨在评估LLMs在商业智能（BI）的背景下的数据分析能力。BIBench通过三个维度评估LLMs：1）BI基础知识，评估模型的数值推理能力和对金融概念的熟悉程度；2）BI知识应用，确定模型快速理解文本信息并从多个视角生成分析问题的能力；3）BI技术技能，检查模型使用技术知识解决现实数据分析挑战的能力。BIBench包括11个子任务，涵盖分类、提取和生成三种任务类型。

    Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. However, their proficiency and reliability in the specialized domain of Data Analysis, particularly with a focus on data-driven thinking, remain uncertain. To bridge this gap, we introduce BIBench, a comprehensive benchmark designed to evaluate the data analysis capabilities of LLMs within the context of Business Intelligence (BI). BIBench assesses LLMs across three dimensions: 1) BI foundational knowledge, evaluating the models' numerical reasoning and familiarity with financial concepts; 2) BI knowledge application, determining the models' ability to quickly comprehend textual information and generate analysis questions from multiple views; and 3) BI technical skills, examining the models' use of technical knowledge to address real-world data analysis challenges. BIBench comprises 11 sub-tasks, spanning three categories of task types: classification, extraction, and generation. Additi
    
[^52]: 领域特定LLM的微调和利用方法

    Fine-tuning and Utilization Methods of Domain-specific LLMs. (arXiv:2401.02981v1 [cs.CL])

    [http://arxiv.org/abs/2401.02981](http://arxiv.org/abs/2401.02981)

    本研究调查了领域特定LLM的微调和利用方法，以金融领域为例，详细介绍了数据集选择、预处理、模型选择以及在金融领域LLM微调中的关键因素。研究探讨了领域特定词汇的构建和安全合规性的考虑因素，并提供了在金融领域生成领域特定LLM的过程和实施方法。多种金融案例被涵盖在内，包括股票价格预测、金融新闻情绪分析、自动文档处理、研究和信息提取等。

    

    最近发布的预训练的大型语言模型（LLM）引起了相当大的关注，但关于领域特定LLM的微调和应用的研究仍然很少。本研究调查了领域特定LLM的微调和利用方法，重点关注LLM的趋势、基础模型和用于领域特定预训练的方法。以金融行业为例，详细介绍了数据集选择、预处理、模型选择以及在金融领域LLM微调中关键的考虑因素。针对金融数据的独特特点，本研究探讨了领域特定词汇的构建，以及安全性和合规性的考虑因素。在LLM微调的实际应用中，本研究概述了在金融领域生成领域特定LLM的过程和实施方法。包括股票价格预测、金融新闻情绪分析、自动文档处理、研究和信息提取等多种金融案例被涵盖在内。

    Recent releases of pre-trained Large Language Models (LLMs) have gained considerable traction, yet research on fine-tuning and employing domain-specific LLMs remains scarce. This study investigates approaches for fine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs, foundational models, and methods for domain-specific pre-training. Focusing on the financial sector, it details dataset selection, preprocessing, model choice, and considerations crucial for LLM fine-tuning in finance. Addressing the unique characteristics of financial data, the study explores the construction of domain-specific vocabularies and considerations for security and regulatory compliance. In the practical application of LLM fine-tuning, the study outlines the procedure and implementation for generating domain-specific LLMs in finance. Various financial cases, including stock price prediction, sentiment analysis of financial news, automated document processing, research, information extract
    
[^53]: 我们在描述同样的声音吗？对表现力钢琴演奏词嵌入空间的分析

    Are we describing the same sound? An analysis of word embedding spaces of expressive piano performance. (arXiv:2401.02979v1 [cs.CL])

    [http://arxiv.org/abs/2401.02979](http://arxiv.org/abs/2401.02979)

    本文探讨了对表现力钢琴演奏特征的不确定性，测试了五个嵌入模型及其相似性结构与真值的对应关系，并进行了进一步评估。嵌入模型的质量在这方面显示出很大的差异性。

    

    语义嵌入在基于自然语言的信息检索中起到至关重要的作用。嵌入模型将单词和上下文表示为向量，其空间配置是根据大型文本语料库中单词的分布导出的。尽管这些表示一般非常强大，但它们可能未能考虑到细粒度的领域特定细微差别。在本文中，我们探讨了这种对表现力钢琴演奏特征的不确定性。我们使用一个音乐研究数据集，其中包含自由文本演奏特征的注释，并进行了一个后续研究，将注释分类成不同的聚类。我们得出了一个特定领域语义相似性结构的真值。我们测试了五个嵌入模型及其相似性结构与真值的对应关系。我们进一步评估了上下文提示、中心度降低、跨模态相似性和k-means聚类的影响。嵌入模型的质量在这方面显示出很大的差异性。

    Semantic embeddings play a crucial role in natural language-based information retrieval. Embedding models represent words and contexts as vectors whose spatial configuration is derived from the distribution of words in large text corpora. While such representations are generally very powerful, they might fail to account for fine-grained domain-specific nuances. In this article, we investigate this uncertainty for the domain of characterizations of expressive piano performance. Using a music research dataset of free text performance characterizations and a follow-up study sorting the annotations into clusters, we derive a ground truth for a domain-specific semantic similarity structure. We test five embedding models and their similarity structure for correspondence with the ground truth. We further assess the effects of contextualizing prompts, hubness reduction, cross-modal similarity, and k-means clustering. The quality of embedding models shows great variability with respect to this 
    
[^54]: 从生成式AI前辈中学习——与对话代理互动的许多动机

    Learning from a Generative AI Predecessor -- The Many Motivations for Interacting with Conversational Agents. (arXiv:2401.02978v1 [cs.CL])

    [http://arxiv.org/abs/2401.02978](http://arxiv.org/abs/2401.02978)

    这项研究通过分析与虚拟伙伴Zo互动的动机，总结出了多种增加互动性的方法，为生成式AI的发展提供了借鉴。

    

    要使生成式AI成功，它必须具备多么引人入胜的对话能力？近60年来，一些对话代理会回应任何问题或评论以保持对话的进行。近年来，许多人开始利用机器学习或复杂的语言处理技术，例如Tay、Xiaoice、Zo、Hugging Face、Kuki和Replika。与生成式AI不同的是，它们关注的是互动性，而不是专业知识。数百万人被激发起与它们进行互动。那么这些吸引力是什么呢？如果生成式AI同样引人入胜，它会做得更好吗，还是应该减少互动性？在生成式AI出现之前，我们进行了大规模定量和定性分析，以了解数百万人与微软伙伴Zo进行互动的动机。我们研究了2000个匿名用户的完整聊天记录，并确定了数十种人们与该软件进行互动的动机。设计师们学会了不同的方法来提高互动性。

    For generative AI to succeed, how engaging a conversationalist must it be? For almost sixty years, some conversational agents have responded to any question or comment to keep a conversation going. In recent years, several utilized machine learning or sophisticated language processing, such as Tay, Xiaoice, Zo, Hugging Face, Kuki, and Replika. Unlike generative AI, they focused on engagement, not expertise. Millions of people were motivated to engage with them. What were the attractions? Will generative AI do better if it is equally engaging, or should it be less engaging? Prior to the emergence of generative AI, we conducted a large-scale quantitative and qualitative analysis to learn what motivated millions of people to engage with one such 'virtual companion,' Microsoft's Zo. We examined the complete chat logs of 2000 anonymized people. We identified over a dozen motivations that people had for interacting with this software. Designers learned different ways to increase engagement. 
    
[^55]: 在GPT模型中追踪和编辑关系关联

    Trace and Edit Relation Associations in GPT. (arXiv:2401.02976v1 [cs.CL])

    [http://arxiv.org/abs/2401.02976](http://arxiv.org/abs/2401.02976)

    本研究介绍了一种在GPT模型中分析和修改实体关系的新方法，通过关系追踪技术，我们识别了MLP模块和注意机制在处理关系信息方面的关键作用，实验表明该方法在特异性和泛化性方面取得了平衡的改善。

    

    本研究介绍了一种新颖的方法，用于分析和修改GPT模型中的实体关系，与ROME的基于实体的方法不同。我们开发了一种关系追踪技术，以了解语言模型计算对关系判断的影响。使用FewRel数据集，我们识别了MLP模块和注意机制在处理关系信息方面的关键作用。我们的方法在一个新的数据集上与ROME进行了测试，显示出在特异性和泛化性方面的平衡改善，突显了操纵早期层模块以提高模型理解和准确性的潜力。

    This study introduces a novel approach for analyzing and modifying entity relationships in GPT models, diverging from ROME's entity-focused methods. We develop a relation tracing technique to understand the influence of language model computations on relationship judgments. Using the FewRel dataset, we identify key roles of MLP modules and attention mechanisms in processing relationship information. Our method, tested against ROME on a new dataset, shows improved balance in specificity and generalization, underscoring the potential of manipulating early-layer modules for enhanced model understanding and accuracy.
    
[^56]: 利用大型语言模型检测在线公开威胁的效力

    Efficacy of Utilizing Large Language Models to Detect Public Threat Posted Online. (arXiv:2401.02974v1 [cs.CL])

    [http://arxiv.org/abs/2401.02974](http://arxiv.org/abs/2401.02974)

    本文研究了利用大型语言模型(LLMs)检测在线公开威胁的效力。通过实验发现，不同的LLMs在威胁和非威胁识别方面表现出较高的准确性，其中GPT-4的表现最佳。研究还发现PaLM API的定价非常具有成本效益。研究结果表明，LLMs可以有效地增强人工内容审查，帮助减轻新兴的在线风险。

    

    本文研究了利用大型语言模型(LLMs)检测在线公开威胁的效力。在对威胁 retoric 的传播和暴力预告的增长越来越担忧的背景下，自动内容分析技术可以帮助早期发现和处理。我们开发了自定义的数据收集工具，从一个热门的韩国在线社区收集了500个非威胁示例和20个威胁示例的帖子标题。各种LLMs (GPT-3.5, GPT-4, PaLM) 被提示将单个帖子分类为"威胁"或"安全"。统计分析发现所有模型在威胁和非威胁识别方面表现出较高的准确性，通过卡方拟合度检验也得到了验证。GPT-4 的整体表现最好，非威胁精度达到了97.9%，威胁精度达到了100%。可行性分析还显示PaLM API的定价非常具有成本效益。研究结果显示，LLMs 在规模化环境中可以有效地增强人工内容审查，以帮助减轻新兴的在线风险。

    This paper examines the efficacy of utilizing large language models (LLMs) to detect public threats posted online. Amid rising concerns over the spread of threatening rhetoric and advance notices of violence, automated content analysis techniques may aid in early identification and moderation. Custom data collection tools were developed to amass post titles from a popular Korean online community, comprising 500 non-threat examples and 20 threats. Various LLMs (GPT-3.5, GPT-4, PaLM) were prompted to classify individual posts as either "threat" or "safe." Statistical analysis found all models demonstrated strong accuracy, passing chi-square goodness of fit tests for both threat and non-threat identification. GPT-4 performed best overall with 97.9% non-threat and 100% threat accuracy. Affordability analysis also showed PaLM API pricing as highly cost-efficient. The findings indicate LLMs can effectively augment human content moderation at scale to help mitigate emerging online risks. Howe
    
[^57]: 文本中的深度异常检测

    Deep Anomaly Detection in Text. (arXiv:2401.02971v1 [cs.CL])

    [http://arxiv.org/abs/2401.02971](http://arxiv.org/abs/2401.02971)

    本研究旨在开发一种基于自我监督学习的方法，用于文本中的异常检测，通过利用预处理任务来提高最新技术，并在半监督和无监督的情况下对两个数据集进行了显著改进。

    

    近年来，深度异常检测方法变得越来越受欢迎，诸如堆叠自动编码器、变分自动编码器和生成对抗网络等方法大大改进了最新技术。其他方法通过使用神经网络学习适当的核函数来增强经典模型（如单类支持向量机）。自我监督学习在异常检测领域中的代表线学习方面的最新发展证明在这一背景下非常有益。受计算机视觉领域中使用自我监督学习进行异常检测的进展启发，本论文旨在通过利用为文本语料库量身定制的预处理任务来开发一种检测异常的方法。这种方法在20Newsgroups和AG News两个数据集上大大改进了半监督和无监督的异常检测最新技术，从而证明了自我监督异常检测器在自然领域的潜力。

    Deep anomaly detection methods have become increasingly popular in recent years, with methods like Stacked Autoencoders, Variational Autoencoders, and Generative Adversarial Networks greatly improving the state-of-the-art. Other methods rely on augmenting classical models (such as the One-Class Support Vector Machine), by learning an appropriate kernel function using Neural Networks. Recent developments in representation learning by self-supervision are proving to be very beneficial in the context of anomaly detection. Inspired by the advancements in anomaly detection using self-supervised learning in the field of computer vision, this thesis aims to develop a method for detecting anomalies by exploiting pretext tasks tailored for text corpora. This approach greatly improves the state-of-the-art on two datasets, 20Newsgroups, and AG News, for both semi-supervised and unsupervised anomaly detection, thus proving the potential for self-supervised anomaly detectors in the field of natural
    
[^58]: 视网膜图像中血管的自动定位

    Automated Localization of Blood Vessels in Retinal Images. (arXiv:2401.02962v1 [eess.IV])

    [http://arxiv.org/abs/2401.02962](http://arxiv.org/abs/2401.02962)

    本论文研究了处理视网膜图像中血管定位的两种自动方法，通过减少明亮病变的影响并使用多尺度线算子定位血管结构，以此提高医学图像分析的效果。

    

    血管结构是视网膜中最重要的部分之一，医生可以通过分析其特征来检测许多疾病。视网膜图像中血管的定位是医学图像分析中的一个重要过程。存在明亮病变和黑暗病变的情况下，这个过程也更具挑战性。本论文分析了两种自动定位血管的方法，可以处理健康和不健康（病理性）的视网膜图像。每种方法包括两个主要步骤，其中第二个步骤在两种方法中是相同的。在第一步中，使用算法减少明亮病变的影响。在方法1中，该算法基于K-均值分割，在方法2中，它基于正则化过程。在两种方法的第二步中，使用多尺度线算子定位线状血管结构，并忽略通常被认为具有不规则模式的黑暗病变。介绍了方法后，根据方法的介绍，一种一种详细的评估方法indicating其在血管定位上的表现好坏.

    Vessel structure is one of the most important parts of the retina which physicians can detect many diseases by analysing its features. Localization of blood vessels in retina images is an important process in medical image analysis. This process is also more challenging with the presence of bright and dark lesions. In this thesis, two automated vessel localization methods to handle both healthy and unhealthy (pathological) retina images are analyzed. Each method consists of two major steps and the second step is the same in the two methods. In the first step, an algorithm is used to decrease the effect of bright lesions. In Method 1, this algorithm is based on K- Means segmentation, and in Method 2, it is based on a regularization procedure. In the second step of both methods, a multi-scale line operator is used to localize the line-shaped vascular structures and ignore the dark lesions which are generally assumed to have irregular patterns. After the introduction of the methods, a det
    
[^59]: 为多任务联邦学习提供公平性感知的作业调度

    Fairness-Aware Job Scheduling for Multi-Job Federated Learning. (arXiv:2401.02740v1 [cs.LG])

    [http://arxiv.org/abs/2401.02740](http://arxiv.org/abs/2401.02740)

    本文提出了一种公平感知联邦作业调度（FairFedJS）方法，以确保将高需求的FL客户端数据集公平分配给需要它们的FL作业，并在实验证明其优势。

    

    联邦学习（FL）使多个数据所有者（即FL客户端）能够在不泄露敏感私人数据的情况下共同训练机器学习模型。现有的FL研究主要关注垄断场景，在该场景中，单个FL服务器在每轮训练中选择一部分FL客户端来更新其本地模型。实际上，可能会有多个FL服务器同时尝试从同一个池中选择客户端。本文提出了一种首创的公平感知联邦作业调度（FairFedJS）方法来弥合这一差距。基于Lyapunov优化，它通过同时考虑当前需求和作业付款出价，确保将高需求的FL客户端数据集公平分配给需要它们的FL作业，以防止等待时间过长。基于两个数据集对FairFedJS与四种最先进的方法进行了大量实验证明了其显著优势。它在平均上击败了最佳基准线31.9%和1.0%。

    Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to collaboratively train machine learning models without disclosing sensitive private data. Existing FL research mostly focuses on the monopoly scenario in which a single FL server selects a subset of FL clients to update their local models in each round of training. In practice, there can be multiple FL servers simultaneously trying to select clients from the same pool. In this paper, we propose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS) approach to bridge this gap. Based on Lyapunov optimization, it ensures fair allocation of high-demand FL client datasets to FL jobs in need of them, by jointly considering the current demand and the job payment bids, in order to prevent prolonged waiting. Extensive experiments comparing FairFedJS against four state-of-the-art approaches on two datasets demonstrate its significant advantages. It outperforms the best baseline by 31.9% and 1.0% on avera
    
[^60]: 参数高效稀疏制作：从密集型到专家混合式用于通用任务的指令调整

    Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks. (arXiv:2401.02731v1 [cs.AI])

    [http://arxiv.org/abs/2401.02731](http://arxiv.org/abs/2401.02731)

    本文提出了一种参数高效稀疏制作的方法，它使用专家混合式架构将密集模型转换为稀疏模型，以实现在模型容量有限的情况下进行指令调整和泛化能力增强。

    

    大型语言模型(LLMs)在通用自然语言处理(NLP)任务中展示出相当的熟练程度。指令调整作为一种成功的范例，增强了LLMs遵循自然语言指令并在各种任务中展现出强大的泛化能力。然而，由于模型容量限制，这些模型在多个任务中经常遇到性能限制。在指令调整阶段扩展模型容量面临着巨大的挑战。为了解决这个问题，我们引入了一种新颖的方法，参数高效稀疏制作(PESC)，它使用专家混合式(MoE)架构将密集模型转换为稀疏模型。PESC将适配器集成到稀疏模型的MoE层中，区分不同的专家而不改变这些层中的个体权重。这种方法显著降低了计算成本和GPU内存需求，通过最小的增加实现了模型容量的扩展。

    Large Language Models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across a wide range of tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce a novel approach, Parameter-Efficient Sparsity Crafting (PESC), which transitions dense models to sparse models using a Mixture of Experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal increase 
    
[^61]: 机器学习中的计算分歧：对学术贡献和审查的威胁？

    The Compute Divide in Machine Learning: A Threat to Academic Contribution and Scrutiny?. (arXiv:2401.02452v1 [cs.CY])

    [http://arxiv.org/abs/2401.02452](http://arxiv.org/abs/2401.02452)

    本论文调查了机器学习研究中计算分歧对学术贡献和审查的影响，发现计算分歧导致学术界在计算密集型研究主题中的影响力降低，并且学术研究趋向于使用工业界开发的开源、预训练模型。为解决这一问题，建议通过国家支持的计算基础设施与开放科学倡议的结合来拓展学术见解。

    

    工业和学术人工智能实验室在使用计算资源方面存在明显差异。我们提供了一项数据驱动的调查，以了解计算分歧对机器学习研究的影响。我们表明，计算分歧与计算密集型研究主题中学术独立研究团队的表示减少有关，特别是基础模型方面。我们认为，学术界在推进相关技术、提供关键的评估和审查以及在这些模型的传播方面可能发挥的作用将较小。与研究重心的这种变化同时，学术研究在倡导工业界开发的开源、预训练模型方面出现了明显转变。为了应对这一趋势所带来的挑战，尤其是对影响力模型的审查减少，我们建议采取一些方法来有针对性地拓展学术见解。这些方法包括国家支持的计算基础设施与开放科学倡议的结合。

    There are pronounced differences in the extent to which industrial and academic AI labs use computing resources. We provide a data-driven survey of the role of the compute divide in shaping machine learning research. We show that a compute divide has coincided with a reduced representation of academic-only research teams in compute intensive research topics, especially foundation models. We argue that, academia will likely play a smaller role in advancing the associated techniques, providing critical evaluation and scrutiny, and in the diffusion of such models. Concurrent with this change in research focus, there is a noticeable shift in academic research towards embracing open source, pre-trained models developed within the industry. To address the challenges arising from this trend, especially reduced scrutiny of influential models, we recommend approaches aimed at thoughtfully expanding academic insights. Nationally-sponsored computing infrastructure coupled with open science initia
    
[^62]: 通用嵌入模型在短语境临床语义搜索方面表现比专业嵌入模型更好

    Generalist embedding models are better at short-context clinical semantic search than specialized embedding models. (arXiv:2401.01943v1 [cs.CL])

    [http://arxiv.org/abs/2401.01943](http://arxiv.org/abs/2401.01943)

    本研究发现，在临床语义搜索方面，通用嵌入模型比专业嵌入模型表现更好，这表明现有的临床专业化模型对输入的微小变化更敏感。

    

    基于大型语言模型（LLM）的工具和解决方案在医疗领域的应用日益增多，这已成为一个重要趋势。然而，在这个高度关键和敏感的领域中使用它们对其稳健性产生了重要的问题，特别是对输入变化和生成的输出的可靠性。本研究通过构建基于ICD-10-CM代码描述的文本数据集来解决这些问题，该数据集广泛应用于美国医院，包含许多临床术语及其易于复制的改写。然后，我们在语义搜索任务中对现有的通用或临床专业化的嵌入模型进行了基准测试，目标是正确匹配改写的文本与原始描述。我们的结果表明，通用模型比临床模型表现更好，这表明现有的临床专业化模型对输入的微小变化更敏感，从而使其困惑。

    The increasing use of tools and solutions based on Large Language Models (LLMs) for various tasks in the medical domain has become a prominent trend. Their use in this highly critical and sensitive domain has thus raised important questions about their robustness, especially in response to variations in input, and the reliability of the generated outputs. This study addresses these questions by constructing a textual dataset based on the ICD-10-CM code descriptions, widely used in US hospitals and containing many clinical terms, and their easily reproducible rephrasing. We then benchmarked existing embedding models, either generalist or specialized in the clinical domain, in a semantic search task where the goal was to correctly match the rephrased text to the original description. Our results showed that generalist models performed better than clinical models, suggesting that existing clinical specialized models are more sensitive to small changes in input that confuse them. The highl
    
[^63]: AIGCBench：AI生成的图像到视频内容的全面评估

    AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI. (arXiv:2401.01651v1 [cs.CV])

    [http://arxiv.org/abs/2401.01651](http://arxiv.org/abs/2401.01651)

    本论文介绍了AIGCBench，一个全面评估AI生成的图像到视频内容的基准。通过引入多样化且开放领域的图像-文本数据集，AIGCBench解决了现有基准的局限性。为了建立统一的评估框架，该基准包括11个度量指标，涵盖控制视频对齐、动态效果、时间一致性和视频质量等方面。

    

    人工智能生成内容（AIGC）领域正在迅速发展，尤其是视频生成。本论文介绍了AIGCBench，这是一种开创性的综合性和可扩展性基准，旨在评估各种视频生成任务，主要关注图像到视频（I2V）生成。AIGCBench解决了现有基准的局限性，这些基准缺乏多样化的数据集，通过包括一个多样化且开放领域的图像-文本数据集来评估不同的最先进算法在相等条件下的性能。我们采用了一种新颖的文本组合器和GPT-4来创建丰富的文本提示，然后使用先进的文本到图像模型生成图像。为了建立一个统一的视频生成任务评估框架，我们的基准包括11个度量指标，涵盖四个维度，以评估算法性能。这些维度是控制视频对齐，动态效果，时间一致性和视频质量。

    The burgeoning field of Artificial Intelligence Generated Content (AIGC) is witnessing rapid advancements, particularly in video generation. This paper introduces AIGCBench, a pioneering comprehensive and scalable benchmark designed to evaluate a variety of video generation tasks, with a primary focus on Image-to-Video (I2V) generation. AIGCBench tackles the limitations of existing benchmarks, which suffer from a lack of diverse datasets, by including a varied and open-domain image-text dataset that evaluates different state-of-the-art algorithms under equivalent conditions. We employ a novel text combiner and GPT-4 to create rich text prompts, which are then used to generate images via advanced Text-to-Image models. To establish a unified evaluation framework for video generation tasks, our benchmark includes 11 metrics spanning four dimensions to assess algorithm performance. These dimensions are control-video alignment, motion effects, temporal consistency, and video quality. These 
    
[^64]: GOAT-Bench: 通过基于迷因的社交虐待研究对大型多模态模型的安全洞察

    GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse. (arXiv:2401.01523v1 [cs.CL])

    [http://arxiv.org/abs/2401.01523](http://arxiv.org/abs/2401.01523)

    通过基于迷因的社交虐待研究对大型多模态模型的安全洞察，我们引入了综合的迷因基准测试集GOAT-Bench，评估各种LMMs在识别和回应迷因中体现的微妙社交虐待方面的能力。

    

    社交媒体的指数级增长深刻改变了信息的创造、传播和吸收方式，在数字时代产生了前所未有的影响。遗憾的是，这个爆炸也导致了网络迷因的滥用数量显著增加。评估迷因的负面影响是相当具有挑战性的，因为它们通常具有微妙和隐晦的含义，这些含义不能直接通过显性的文本和图像传达出来。鉴于此，大型多模态模型(LMMs)作为处理多样化多模态任务的卓越能力的焦点引起了人们的兴趣。针对这一发展，我们的论文旨在深入研究各种LMMs(如GPT-4V)识别和回应迷因中体现的微妙社交虐待方面的能力。我们引入了综合的迷因基准测试集GOAT-Bench，其中包含超过6K个多样的迷因，涵盖的主题包括隐性仇恨言论、性别歧视和网络欺凌等。利用GOAT-Be

    The exponential growth of social media has profoundly transformed how information is created, disseminated, and absorbed, exceeding any precedent in the digital age. Regrettably, this explosion has also spawned a significant increase in the online abuse of memes. Evaluating the negative impact of memes is notably challenging, owing to their often subtle and implicit meanings, which are not directly conveyed through the overt text and imagery. In light of this, large multimodal models (LMMs) have emerged as a focal point of interest due to their remarkable capabilities in handling diverse multimodal tasks. In response to this development, our paper aims to thoroughly examine the capacity of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of social abuse manifested in memes. We introduce the comprehensive meme benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing GOAT-Be
    
[^65]: 探索LLMs在心理学应用中的前沿：一份综述

    Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. (arXiv:2401.01519v1 [cs.LG])

    [http://arxiv.org/abs/2401.01519](http://arxiv.org/abs/2401.01519)

    本文综述了大型语言模型（LLMs）在心理学应用中的前沿，包括如何模拟人类认知和行为、提供创新工具进行文献回顾、假设生成、实验设计等。

    

    本文探索了大型语言模型（LLMs）在心理学应用中的前沿。心理学经历了几次理论变革，当前人工智能（AI）和机器学习，特别是LLMs的使用有望开启新的研究方向。我们详细探讨了LLMs如ChatGPT在心理学研究中的转变。文章讨论了LLMs在认知与行为心理学、临床与咨询心理学、教育与发展心理学以及社会与文化心理学等心理学分支中的影响，强调了它们模拟人类认知和行为方面的潜力。本文深入探讨了这些模型模拟人类文本生成的能力，为心理学中的文献回顾、假设生成、实验设计、实验对象、数据分析、学术写作和同行评审等提供创新工具。虽然LLMs在推动研究方法学方面起着重要作用，

    This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologie
    
[^66]: 高效视觉Transformer的令牌传播控制器

    Token Propagation Controller for Efficient Vision Transformer. (arXiv:2401.01470v1 [cs.CV])

    [http://arxiv.org/abs/2401.01470](http://arxiv.org/abs/2401.01470)

    本文提出一种新颖的令牌传播控制器（TPC），通过结合暂停概率和重新开始概率，实现了对令牌的减少和重复利用的控制，从而提高了视觉Transformer的效率和令牌利用率。

    

    视觉Transformer（ViTs）在各种计算机视觉任务上取得了有希望的结果，然而它们在输入令牌数量上的二次复杂度限制了它们在资源受限环境下的应用。以前的方法采用逐渐减少令牌来解决这个挑战，假设一个层中的令牌冗余意味着所有后续层中也有冗余。我们经验证明这个假设通常是不正确的，即一个层中多余的令牌在后面的层中可以是有用的。基于这个关键洞察力，我们提出了一种新颖的令牌传播控制器（TPC），它结合了两种不同的令牌分布，即暂停概率和重新开始概率，用来控制令牌的减少和重复利用，从而实现更高效的令牌利用。为了改善令牌分布的估计，我们提出了一种平滑机制，作为正则化器，有助于去除噪声异常值。此外

    Vision transformers (ViTs) have achieved promising results on a variety of Computer Vision tasks, however their quadratic complexity in the number of input tokens has limited their application specially in resource-constrained settings. Previous approaches that employ gradual token reduction to address this challenge assume that token redundancy in one layer implies redundancy in all the following layers. We empirically demonstrate that this assumption is often not correct, i.e., tokens that are redundant in one layer can be useful in later layers. We employ this key insight to propose a novel token propagation controller (TPC) that incorporates two different token-distributions, i.e., pause probability and restart probability to control the reduction and reuse of tokens respectively, which results in more efficient token utilization. To improve the estimates of token distributions, we propose a smoothing mechanism that acts as a regularizer and helps remove noisy outliers. Furthermore
    
[^67]: 使用稀缺数据和联邦多轨迹GNN预测婴儿脑连接性

    Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data. (arXiv:2401.01383v1 [q-bio.NC])

    [http://arxiv.org/abs/2401.01383](http://arxiv.org/abs/2401.01383)

    我们提出了一种使用联邦多轨迹GNN的方法，通过稀缺数据预测婴儿脑连接性。通过联邦学习，我们通过聚合多个医院的本地学习结果来提高模型性能，同时保护数据隐私。

    

    对于识别早期脑连接性发展的动态过程，了解婴儿脑网络在出生后的第一年中的复杂演化至关重要。现有的深度学习解决方案存在三个主要局限性。首先，它们不能泛化到多轨迹预测任务，其中每个图轨迹对应于特定的成像模态或连接类型（例如T1-w MRI）。其次，现有模型需要大量的训练数据集才能达到令人满意的性能，而这往往很难获取。第三，它们不能有效利用不完整的时间序列数据。为了解决这些限制，我们引入了FedGmTE-Net++，一种联邦图形多轨迹演化网络。通过联邦学习的力量，我们在有限的医院数据集中聚合了不同医院的本地学习结果。结果即可提高每个医院本地生成模型的性能，同时保护数据隐私。

    The understanding of the convoluted evolution of infant brain networks during the first postnatal year is pivotal for identifying the dynamics of early brain connectivity development. Existing deep learning solutions suffer from three major limitations. First, they cannot generalize to multi-trajectory prediction tasks, where each graph trajectory corresponds to a particular imaging modality or connectivity type (e.g., T1-w MRI). Second, existing models require extensive training datasets to achieve satisfactory performance which are often challenging to obtain. Third, they do not efficiently utilize incomplete time series data. To address these limitations, we introduce FedGmTE-Net++, a federated graph-based multi-trajectory evolution network. Using the power of federation, we aggregate local learnings among diverse hospitals with limited datasets. As a result, we enhance the performance of each hospital's local generative model, while preserving data privacy. The three key innovation
    
[^68]: 大型语言模型的知识编辑全面研究

    A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])

    [http://arxiv.org/abs/2401.01286](http://arxiv.org/abs/2401.01286)

    本研究全面研究了大型语言模型的知识编辑，旨在有效修改模型的行为，同时保持整体性能。

    

    大型语言模型(LLM)在理解和生成与人类交流紧密相似的文本方面展现出了非凡的能力。然而，其主要限制在于训练过程中的显著计算需求，这是由于其广泛的参数化造成的。这一挑战在于世界的动态性，需要频繁更新LLM以修正过时的信息或集成新知识，从而确保其持续的相关性。许多应用需要在训练后进行持续的模型调整，以解决缺陷或不良行为。近年来，对于LLM的知识编辑技术的兴趣越来越高，在特定领域内有效地修改LLM的行为，同时保持整体性能在各种输入中的表现。本文首先定义了知识编辑的目标和挑战，然后综述了现有的知识编辑方法和技术，并讨论了其应用和未来发展的方向。

    Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
    
[^69]: PPBFL: 一种隐私保护的基于区块链的联邦学习模型

    PPBFL: A Privacy Protected Blockchain-based Federated Learning Model. (arXiv:2401.01204v1 [cs.CR])

    [http://arxiv.org/abs/2401.01204](http://arxiv.org/abs/2401.01204)

    PPBFL是一种隐私保护的基于区块链的联邦学习模型，通过应用区块链和自适应差分隐私添加算法，增强了联邦学习的安全性和节点的积极参与。同时引入了交易混合机制，更好地保护本地训练的身份隐私。

    

    随着机器学习的快速发展和对数据隐私的日益关注，联邦学习成为一个越来越突出的焦点。然而，模型参数攻击和缺乏激励机制等挑战阻碍了联邦学习的有效性。因此，我们提出了一种隐私保护的基于区块链的联邦学习模型（PPBFL），以增强联邦学习的安全性并促进节点在模型训练中的积极参与。区块链确保了存储在星际文件系统（IPFS）中的模型参数不被篡改。同时，我们采用了一种新颖的自适应差分隐私添加算法，同时应用于本地模型和全局模型，保护本地模型的隐私同时防止因联邦学习中存在大量本地模型而降低全局模型的安全性。此外，我们引入了一个新的交易混合机制，以更好地保护本地训练的身份隐私。

    With the rapid development of machine learning and growing concerns about data privacy, federated learning has become an increasingly prominent focus. However, challenges such as attacks on model parameters and the lack of incentive mechanisms hinder the effectiveness of federated learning. Therefore, we propose a Privacy Protected Blockchain-based Federated Learning Model (PPBFL) to enhance the security of federated learning and promote the active participation of nodes in model training. Blockchain ensures that model parameters stored in the InterPlanetary File System (IPFS) remain unaltered. A novel adaptive differential privacy addition algorithm is simultaneously applied to local and global models, preserving the privacy of local models and preventing a decrease in the security of the global model due to the presence of numerous local models in federated learning. Additionally, we introduce a new mix transactions mechanism to better protect the identity privacy of local training c
    
[^70]: 准确的变形DETR和多级特征融合用于辅助血液疾病诊断的白细胞检测

    Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases. (arXiv:2401.00926v1 [cs.CV])

    [http://arxiv.org/abs/2401.00926](http://arxiv.org/abs/2401.00926)

    本文提出了一种创新的白细胞检测方法，使用多级特征融合和变形自注意DETR，通过解决白细胞尺度差异问题和提高检测精度，以改善传统血液检测的效率和准确性。

    

    在标准医院血液检测中，传统的方法需要医生使用显微镜从患者的血液显微图像中手动分离白细胞。然后通过自动白细胞分类器对这些分离的白细胞进行分类，以确定血样中不同类型白细胞的比例和体积，从而协助疾病诊断。这种方法不仅耗时、耗力，而且容易出现错误，因为图像质量和环境条件等因素，可能导致后续分类错误和误诊。为了解决这些问题，本文提出了一种创新的白细胞检测方法：多级特征融合和变形自注意DETR（MFDS-DETR）。为了解决白细胞尺度差异的问题，我们设计了高级筛选特征融合金字塔（HS-FPN），实现了多级融合。该模型使用高级特征作为特征融合的输入，同时采用变形自注意DETR实现精确的白细胞检测。

    In standard hospital blood tests, the traditional process requires doctors to manually isolate leukocytes from microscopic images of patients' blood using microscopes. These isolated leukocytes are then categorized via automatic leukocyte classifiers to determine the proportion and volume of different types of leukocytes present in the blood samples, aiding disease diagnosis. This methodology is not only time-consuming and labor-intensive, but it also has a high propensity for errors due to factors such as image quality and environmental conditions, which could potentially lead to incorrect subsequent classifications and misdiagnosis. To address these issues, this paper proposes an innovative method of leukocyte detection: the Multi-level Feature Fusion and Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte scale disparity, we designed the High-level Screening-feature Fusion Pyramid (HS-FPN), enabling multi-level fusion. This model uses high-level features as 
    
[^71]: 将上下文带回来：多模态知识图谱上的相机陷阱物种识别作为链接预测

    Bringing Back the Context: Camera Trap Species Identification as Link Prediction on Multimodal Knowledge Graphs. (arXiv:2401.00608v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.00608](http://arxiv.org/abs/2401.00608)

    本研究利用相机陷阱图像的结构化上下文，提高其在物种识别任务中的泛化能力，并解决了数据稀缺和泛化能力增强的问题。

    

    相机陷阱在动物生态学中是宝贵的工具，用于生物多样性监测和保护。然而，挑战如在新的未知位置部署时的糟糕泛化限制了它们的实际应用。图像自然与可能在不同模态下的异质上下文相关联。在这项工作中，我们利用与相机陷阱图像相关联的结构化上下文，改善在相机陷阱中物种识别这个任务的超出分布的泛化能力。例如，一张野生动物的照片可能与拍摄地点和时间以及关于动物物种的结构化生物学知识相关联。虽然现有的工作通常忽视这一点，但将这样的上下文带回来可以带来一些潜在的好处，如解决数据稀缺和增强泛化能力。然而，有效地将这样的异质上下文整合到视觉领域是一个具有挑战性的问题。

    Camera traps are valuable tools in animal ecology for biodiversity monitoring and conservation. However, challenges like poor generalization to deployment at new unseen locations limit their practical application. Images are naturally associated with heterogeneous forms of context possibly in different modalities. In this work, we leverage the structured context associated with the camera trap images to improve out-of-distribution generalization for the task of species identification in camera traps. For example, a photo of a wild animal may be associated with information about where and when it was taken, as well as structured biology knowledge about the animal species. While typically overlooked by existing work, bringing back such context offers several potential benefits for better image understanding, such as addressing data scarcity and enhancing generalization. However, effectively integrating such heterogeneous context into the visual domain is a challenging problem. To address
    
[^72]: 双向时间计划图：为更高效的多智能体路径规划执行提供可切换的通行顺序

    Bidirectional Temporal Plan Graph: Enabling Switchable Passing Orders for More Efficient Multi-Agent Path Finding Plan Execution. (arXiv:2401.00315v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2401.00315](http://arxiv.org/abs/2401.00315)

    引入了一种新的图形表示，双向时间计划图（BTPG），允许在执行过程中切换通行顺序，避免不必要的等待时间，解决了多智能体路径规划中的执行效率问题。

    

    多智能体路径规划（MAPF）问题涉及为共享环境中的多个智能体规划无碰撞路径。大多数MAPF解算器依赖于智能体能够在特定时间步到达特定位置的假设。然而，现实世界的执行不确定性可能导致智能体偏离此假设，从而导致碰撞和死锁。先前的研究通过让智能体遵循一个时间计划图（TPG），在每个位置强制执行MAPF计划中定义的一致通行顺序来解决这个问题。然而，我们发现TPG过于严格，因为在某些情况下，满足通行顺序需要智能体无必要地等待，导致执行时间变长。为了克服这个问题，我们引入了一种新的图形表示，称为双向时间计划图（BTPG），允许在执行过程中切换通行顺序，避免不必要的等待时间。我们设计了两种构建BTPG的任意时间算法：

    The Multi-Agent Path Finding (MAPF) problem involves planning collision-free paths for multiple agents in a shared environment. The majority of MAPF solvers rely on the assumption that an agent can arrive at a specific location at a specific timestep. However, real-world execution uncertainties can cause agents to deviate from this assumption, leading to collisions and deadlocks. Prior research solves this problem by having agents follow a Temporal Plan Graph (TPG), enforcing a consistent passing order at every location as defined in the MAPF plan. However, we show that TPGs are overly strict because, in some circumstances, satisfying the passing order requires agents to wait unnecessarily, leading to longer execution time. To overcome this issue, we introduce a new graphical representation called a Bidirectional Temporal Plan Graph (BTPG), which allows switching passing orders during execution to avoid unnecessary waiting time. We design two anytime algorithms for constructing a BTPG:
    
[^73]: 图灵测试，一个美丽的思维实验

    Turing's Test, a Beautiful Thought Experiment. (arXiv:2401.00009v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2401.00009](http://arxiv.org/abs/2401.00009)

    本文对图灵的美丽的思维实验进行了历史重建，提供了大量证据和一些原创答案，同时回答了图灵测试的核心问题。

    

    在大规模语言模型的浪潮中，关于图灵测试及其对人工智能的价值的争论和问题重新兴起，并引发了数十年来实际的“图灵”测试。如果人工智能是量子物理学，现在可能已有几只“薛定谔的”猫被杀死了。迟到总比不到好，现在是对图灵美丽的思维实验进行历史重建的时候了。在本文中，我提供了大量证据，包括新的档案来源，针对图灵1950年论文的几个未解决问题给出了原创答案，并回答了图灵测试的核心问题。

    In the wake of large language models, there has been a resurgence of claims and questions about the Turing test and its value for AI, which are reminiscent of decades of practical "Turing" tests. If AI were quantum physics, by now several "Schr\"odinger's" cats could have been killed. Better late than never, it is time for a historical reconstruction of Turing's beautiful thought experiment. In this paper I present a wealth of evidence, including new archival sources, give original answers to several open questions about Turing's 1950 paper, and address the core question of the value of Turing's test.
    
[^74]: 长时间会议记录的行动项驱动摘要生成

    Action-Item-Driven Summarization of Long Meeting Transcripts. (arXiv:2312.17581v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.17581](http://arxiv.org/abs/2312.17581)

    本文提出了一种新方法来自动化生成行动项驱动的会议摘要，通过递归生成分段摘要并使用行动项提取算法。同时，本文还引入了三种用于将长记录分割成主题部分的新方法，以提高算法的效率和解决问题。

    

    在在线会议的流行下，自动生成会议摘要的模型变得更加实用。本文介绍了一种创新而有效的方法来自动化生成会议摘要。当前解决这个问题的方法只生成一般而基本的摘要，将会议简单地视为一个长对话。然而，我们的新算法可以根据会议记录中的行动项生成抽象的会议摘要。这是通过递归生成摘要并并行运行我们的行动项提取算法来实现的。所有这些章节摘要然后合并并总结在一起，以创建一个连贯且以行动项为导向的摘要。此外，本文还介绍了三种将长记录分割成基于主题的部分的新方法，以提高算法的时间效率，并解决问题。

    The increased prevalence of online meetings has significantly enhanced the practicality of a model that can automatically generate the summary of a given meeting. This paper introduces a novel and effective approach to automate the generation of meeting summaries. Current approaches to this problem generate general and basic summaries, considering the meeting simply as a long dialogue. However, our novel algorithms can generate abstractive meeting summaries that are driven by the action items contained in the meeting transcript. This is done by recursively generating summaries and employing our action-item extraction algorithm for each section of the meeting in parallel. All of these sectional summaries are then combined and summarized together to create a coherent and action-item-driven summary. In addition, this paper introduces three novel methods for dividing up long transcripts into topic-based sections to improve the time efficiency of our algorithm, as well as to resolve the iss
    
[^75]: 偏好作为奖励，使用重要性抽样进行最大偏好优化

    Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16430](http://arxiv.org/abs/2312.16430)

    本文提出了一种使用重要性抽样进行最大偏好优化的算法，该算法通过直接优化生成策略来消除对奖励模型的需求，提高了数据利用率和稳定性，并通过解决KL正则化问题来改善偏好学习效果。

    

    偏好学习是将语言模型与人类价值观对齐的关键技术。从人类反馈强化学习（RLHF）是一种基于模型的算法，用于优化偏好学习，首先拟合偏好分数的奖励模型，然后使用基于策略梯度算法进行优化，以最大化奖励。RLHF的处理过程复杂、耗时且不稳定。直接偏好优化（DPO）算法使用离策略算法直接优化生成策略，消除了对奖励模型的需求，具有高效和稳定的数据利用率。DPO使用布拉德利-特里模型和对数损失，导致在偏好接近确定性时忽略了KL正则化项而过度拟合偏好数据。IPO使用一种基于根查找的成对均方误差损失来解决忽略KL正则化问题，并学习到最优策略。但是IPO的成对损失仍然无法使KL正则化生效。本文设计了一种新的算法，使用重要性抽样技术来解决偏好学习中的优化问题。

    Preference learning is a key technology for aligning language models with human values. Reinforcement Learning from Human Feedback (RLHF) is a model based algorithm to optimize preference learning, which first fitting a reward model for preference score, and then optimizing generating policy with on-policy PPO algorithm to maximize the reward. The processing of RLHF is complex, time-consuming and unstable. Direct Preference Optimization (DPO) algorithm using off-policy algorithm to direct optimize generating policy and eliminating the need for reward model, which is data efficient and stable. DPO use Bradley-Terry model and log-loss which leads to over-fitting to the preference data at the expense of ignoring KL-regularization term when preference near deterministic. IPO uses a root-finding pairwise MSE loss to solve the ignoring KL-regularization problem, and learning an optimal policy. But IPO's pairwise loss still can't s make the KL-regularization to work. In this paper, we design 
    
[^76]: YAYI-UIE: 一个增强对话指导的通用信息抽取调优框架

    YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal Information Extraction. (arXiv:2312.15548v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.15548](http://arxiv.org/abs/2312.15548)

    本文提出了一个端到端的增强对话指导的通用信息抽取调优框架（YAYI-UIE），利用对话数据和信息抽取数据共同增强信息抽取性能，在中文数据集上达到了业界领先的性能，在英文数据集上也达到了可比较的性能。

    

    信息抽取任务的难点在于处理特定任务的标签模式和异构数据结构。最近的工作提出了基于大型语言模型的方法来统一建模不同的信息抽取任务。然而，这些现有方法在除了英语以外的中文语言的信息提取能力上存在不足。在本文中，我们提出了一个端到端的增强对话指导的通用信息抽取调优框架（YAYI-UIE），支持中文和英文。具体而言，我们利用对话数据和信息抽取数据共同增强信息抽取性能。实验结果表明，我们提出的框架在中文数据集上达到了业界领先的性能，同时在有监督和零样本设置下在英文数据集上也达到了可比较的性能。

    The difficulty of the information extraction task lies in dealing with the task-specific label schemas and heterogeneous data structures. Recent work has proposed methods based on large language models to uniformly model different information extraction tasks. However, these existing methods are deficient in their information extraction capabilities for Chinese languages other than English. In this paper, we propose an end-to-end chat-enhanced instruction tuning framework for universal information extraction (YAYI-UIE), which supports both Chinese and English. Specifically, we utilize dialogue data and information extraction data to enhance the information extraction performance jointly. Experimental results show that our proposed framework achieves state-of-the-art performance on Chinese datasets while also achieving comparable performance on English datasets under both supervised settings and zero-shot settings.
    
[^77]: 互信息作为强化学习智能体的内在奖励，用于按需共乘的车辆派遣

    Mutual Information as Intrinsic Reward of Reinforcement Learning Agents for On-demand Ride Pooling. (arXiv:2312.15195v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.15195](http://arxiv.org/abs/2312.15195)

    本文提出了一个用于按需共乘车辆派遣的框架，利用互信息作为强化学习智能体的内在奖励，以解决现有算法中只考虑收入最大化而无法满足异常分布请求的问题。

    

    按需共乘服务的出现允许每辆车同时为多名乘客提供服务，从而增加了司机的收入，并使乘客能以较低的价格旅行，而不像UberX和Lyft等出租车/按需服务只能为一名乘客分配一辆车。尽管按需共乘服务可以带来如此多的好处，但共乘服务需要一个明确定义的匹配策略，以最大程度地为所有各方（乘客，司机，聚合公司和环境）提供利益，其中区域调度车辆对匹配和收入有重要影响。现有算法通常仅考虑收入最大化，这使得请求具有异常分布的乘客难以获得乘车。如何在确保合理请求分配的同时增加收入，对共乘服务公司（聚合公司）提出了挑战。在本文中，我们提出了一个车辆派遣的框架，用于共乘服务。

    The emergence of on-demand ride pooling services allows each vehicle to serve multiple passengers at a time, thus increasing drivers' income and enabling passengers to travel at lower prices than taxi/car on-demand services (only one passenger can be assigned to a car at a time like UberX and Lyft). Although on-demand ride pooling services can bring so many benefits, ride pooling services need a well-defined matching strategy to maximize the benefits for all parties (passengers, drivers, aggregation companies and environment), in which the regional dispatching of vehicles has a significant impact on the matching and revenue. Existing algorithms often only consider revenue maximization, which makes it difficult for requests with unusual distribution to get a ride. How to increase revenue while ensuring a reasonable assignment of requests brings a challenge to ride pooling service companies (aggregation companies). In this paper, we propose a framework for vehicle dispatching for ride po
    
[^78]: 高效异步稀疏化量化联邦学习

    Efficient Asynchronous Federated Learning with Sparsification and Quantization. (arXiv:2312.15186v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2312.15186](http://arxiv.org/abs/2312.15186)

    本论文提出了一种高效异步稀疏化量化联邦学习方法（TEASQ-Fed），利用边缘设备的并行参与，解决了传统方法中设备拖慢训练和通信瓶颈的问题。

    

    在数据分布在多个边缘设备上的情况下，联邦学习越来越受到关注，它可以在不传输原始数据的情况下合作训练机器学习模型。传统的联邦学习使用参数服务器和大量边缘设备来进行模型训练，每轮选择几个设备参与。然而，有些设备可能会拖慢训练过程甚至导致系统崩溃，而其他空闲设备则闲置不用。由于设备和服务器之间的带宽相对较低，中间数据的通信成为瓶颈。本文提出了一种带有稀疏化和量化的时间高效异步联邦学习方法（TEASQ-Fed），它可以充分利用边缘设备主动参与训练过程。我们利用控制参数来选择适当数量的并行边缘设备。

    While data is distributed in multiple edge devices, Federated Learning (FL) is attracting more and more attention to collaboratively train a machine learning model without transferring raw data. FL generally exploits a parameter server and a large number of edge devices during the whole process of the model training, while several devices are selected in each round. However, straggler devices may slow down the training process or even make the system crash during training. Meanwhile, other idle edge devices remain unused. As the bandwidth between the devices and the server is relatively low, the communication of intermediate data becomes a bottleneck. In this paper, we propose Time-Efficient Asynchronous federated learning with Sparsification and Quantization, i.e., TEASQ-Fed. TEASQ-Fed can fully exploit edge devices to asynchronously participate in the training process by actively applying for tasks. We utilize control parameters to choose an appropriate number of parallel edge device
    
[^79]: 使用Forward-Forward算法训练卷积神经网络

    Training Convolutional Neural Networks with the Forward-Forward algorithm. (arXiv:2312.14924v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.14924](http://arxiv.org/abs/2312.14924)

    本文基于Forward-Forward算法将其应用于卷积神经网络（CNN）的训练，采用了新颖的空间扩展标签技术，在MNIST手写数字数据集上达到了99.16%的分类准确率。

    

    近年来，使用深度神经网络对图像进行分析的最新成功几乎全部实现于卷积神经网络（CNN）。这些CNN以及所有深度神经网络架构的训练都使用了反向传播算法，将网络的输出与期望结果进行比较，利用差异来调整网络权重以达到期望的输出。在2022年的一篇预印本中，Geoffrey Hinton提出了一种替代的训练方式，即在网络的输入中同时传递期望的结果和图像。这种称为Forward Forward（FF）算法到目前为止仅在全连接网络中使用过。在本文中，我们展示了如何将FF范式扩展到CNN中。我们的FF训练的CNN采用了一种新颖的空间扩展标签技术，在MNIST手写数字数据集上实现了99.16%的分类准确率。我们展示了不同超参数对所提出的模型性能的影响。

    The recent successes in analyzing images with deep neural networks are almost exclusively achieved with Convolutional Neural Networks (CNNs). The training of these CNNs, and in fact of all deep neural network architectures, uses the backpropagation algorithm where the output of the network is compared with the desired result and the difference is then used to tune the weights of the network towards the desired outcome. In a 2022 preprint, Geoffrey Hinton suggested an alternative way of training which passes the desired results together with the images at the input of the network. This so called Forward Forward (FF) algorithm has up to now only been used in fully connected networks. In this paper, we show how the FF paradigm can be extended to CNNs. Our FF-trained CNN, featuring a novel spatially-extended labeling technique, achieves a classification accuracy of 99.16% on the MNIST hand-written digits dataset. We show how different hyperparameters affect the performance of the proposed 
    
[^80]: 选区（gerrymandering）平面图的计算复杂性研究

    Gerrymandering Planar Graphs. (arXiv:2312.14721v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2312.14721](http://arxiv.org/abs/2312.14721)

    本研究探讨了选区规划问题在平面图中的计算复杂性。在$\lambda$-外平面图中，问题可以在多项式时间内解决，但在一般的平面图中是NP-complete。我们还讨论了在候选人数较大时的近似算法。

    

    我们研究了选区规划问题（gerrymandering）的计算复杂性。数学上，选区设计者（gerrymanderer）试图将一个加权图划分为$k$个连通分量（选区），以使其候选人（政党）赢得尽可能多的选区。之前的工作主要关注图是路径或树的特殊情况。我们的研究重点是图是平面图的实际情况。我们证明了在$\lambda$-外平面图中，当候选人数和$\lambda$是常数，顶点权重（投票权重）是多项式界限时，选区规划问题可以在多项式时间内解决。相反，在一般的平面图中，即使只有两个候选人，该问题也是NP完全的。这促使我们研究选区规划平面图的近似算法。然而，当候选人数较大时，我们证明很难区分选区设计者可以从这些项中脱颖而出的情况。

    We study the computational complexity of the map redistricting problem (gerrymandering). Mathematically, the electoral district designer (gerrymanderer) attempts to partition a weighted graph into $k$ connected components (districts) such that its candidate (party) wins as many districts as possible. Prior work has principally concerned the special cases where the graph is a path or a tree. Our focus concerns the realistic case where the graph is planar. We prove that the gerrymandering problem is solvable in polynomial time in $\lambda$-outerplanar graphs, when the number of candidates and $\lambda$ are constants and the vertex weights (voting weights) are polynomially bounded. In contrast, the problem is NP-complete in general planar graphs even with just two candidates. This motivates the study of approximation algorithms for gerrymandering planar graphs. However, when the number of candidates is large, we prove it is hard to distinguish between instances where the gerrymanderer can
    
[^81]: 解锁预训练的图像骨干用于语义图像合成

    Unlocking Pre-trained Image Backbones for Semantic Image Synthesis. (arXiv:2312.13314v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.13314](http://arxiv.org/abs/2312.13314)

    本文提出了一种新的GAN鉴别器类别，利用预训练的图像分类任务的特征骨干网络生成高度逼真的图像，并引入了更好的上下文建模和交叉注意力技术，生成更多样化的图像。

    

    语义图像合成是一项重要的条件性图像生成任务，它可以通过用户提供的语义标签图生成图像，同时控制生成图像的内容和空间布局。本文提出了一种新的GAN鉴别器类别，用于语义图像合成，通过利用预训练的图像分类任务的特征骨干网络生成高度逼真的图像。同时，我们还引入了一种新的生成器架构，具有更好的上下文建模能力，并使用交叉注意力将噪声注入潜变量，从而生成更多样化的图像。

    Semantic image synthesis, i.e., generating images from user-provided semantic label maps, is an important conditional image generation task as it allows to control both the content as well as the spatial layout of generated images. Although diffusion models have pushed the state of the art in generative image modeling, the iterative nature of their inference process makes them computationally demanding. Other approaches such as GANs are more efficient as they only need a single feed-forward pass for generation, but the image quality tends to suffer on large and diverse datasets. In this work, we propose a new class of GAN discriminators for semantic image synthesis that generates highly realistic images by exploiting feature backbone networks pre-trained for tasks such as image classification. We also introduce a new generator architecture with better context modeling and using cross-attention to inject noise into latent variables, leading to more diverse generated images. Our model, w
    
[^82]: 连续学习: 面向视频表示的免遗忘优胜子网络

    Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.11973](http://arxiv.org/abs/2312.11973)

    本研究基于"彩票票据假设"，提出了一种连续学习方法，通过利用稀疏子网络和FSO进行任务增量学习、少样本类增量学习和视频增量学习，实现高效学习和有效的权重重用。

    

    受到"彩票票据假设"（LTH）的启发，该假设强调在较大的密集网络中存在高效子网络，研究了在适当的稀疏条件下表现优秀的优胜子网络（WSN）在各种连续学习任务中的应用。它利用来自密集网络的预先存在的权重，在任务增量学习（TIL）场景中实现高效学习。在少样本类增量学习（FSCIL）中，设计了一种称为软子网络（SoftNet）的WSN变体，以防止数据样本稀缺时的过拟合。此外，考虑了WSN权重的稀疏重用，用于视频增量学习（VIL）。考虑了在WSN中使用傅立叶子神经运算器（FSO），它能够对视频进行紧凑编码，并在不同带宽下识别可重用的子网络。我们将FSO集成到不同的连续学习架构中，包括VIL、TIL和FSCIL。

    Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the existence of efficient subnetworks within larger, dense networks, a high-performing Winning Subnetwork (WSN) in terms of task performance under appropriate sparsity conditions is considered for various continual learning tasks. It leverages pre-existing weights from dense networks to achieve efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is designed to prevent overfitting when the data samples are scarce. Furthermore, the sparse reuse of WSN weights is considered for Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It enables compact encoding of videos and identifies reusable subnetworks across varying bandwidths. We have integrated FSO into different architectural frameworks for continual learning, including VIL, TIL, and FSCIL. Our c
    
[^83]: "原文改写"提高了高精度长文本问答的效果

    "Paraphrasing The Original Text" Makes High Accuracy Long-Context QA. (arXiv:2312.11193v6 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.11193](http://arxiv.org/abs/2312.11193)

    本论文提出了一种名为"原文改写"的任务来处理长文本问答，通过低成本高效的方法成功扩展了现有模型的上下文窗口至32k，并在多文档问答中达到了最先进的准确性。

    

    当面对长文本时，大多数开源生成式语言模型的上下文窗口限制在4k以内，这限制了它们的能力。即使是具有更长上下文窗口的模型也无法在长上下文问题上保证令人满意的准确性。为了解决这个问题，我们从训练数据的角度出发，从理论上证明了提高处理长上下文能力需要的是"有效"而不仅仅是"长"的数据。基于这个洞见，我们提出了使用"原文改写"任务，并通过一种低成本高效的方法，成功将现有模型的上下文窗口扩展到32k。我们的微调模型在具有相近规模的模型中在多文档问答方面达到了最先进的准确性。模型和训练数据已经在HuggingFace（https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k）和WiseModel（https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k）上提供。

    Most open-source generative language models currently have a context window of no more than 4k, limiting their ability when facing long text. Even models with longer context windows cannot guarantee satisfactory accuracy on long-context problems. To tackle this issue, we explore from the perspective of training data and theoretically demonstrate that improving the capability to handle long contexts requires "effective" rather than simply "long" data. Based on this insight, we propose using the "original text paraphrasing" task and successfully extend the context window of existing models to 32k through a low-cost and effective method. Our fine-tuned model achieves state-of-the-art accuracy in multi-document-QA among models of comparable scale. The model and training data have been made available on HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and WiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).
    
[^84]: SAME: 防范模型提取攻击的样本重建方法

    SAME: Sample Reconstruction against Model Extraction Attacks. (arXiv:2312.10578v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2312.10578](http://arxiv.org/abs/2312.10578)

    SAME是一种防御模型提取攻击的新方法，基于样本重建的概念，无需额外的数据集和模型访问，并且具有更实用的保护能力。

    

    尽管深度学习模型在各个领域中表现出显著的性能，但它们的部署需要大量的资源和先进的计算基础设施。作为解决方案，机器学习即服务（MLaaS）应运而生，降低了用户发布或产品化他们的深度学习模型的门槛。然而，之前的研究已经强调了与MLaaS相关的潜在隐私和安全风险，其中一个主要威胁是模型提取攻击。为了解决这个问题，有许多防御解决方案，但它们都存在不切实际的假设和泛化问题，使它们对可靠的保护不够实用。受到这些限制的驱动，我们引入了一种基于样本重建概念的新型防御机制SAME。该策略对防御者的能力要求最小，消除了对辅助的离群分布（OOD）数据集、用户查询历史、白盒模型访问和额外干预的需求。

    While deep learning models have shown significant performance across various domains, their deployment needs extensive resources and advanced computing infrastructure. As a solution, Machine Learning as a Service (MLaaS) has emerged, lowering the barriers for users to release or productize their deep learning models. However, previous studies have highlighted potential privacy and security concerns associated with MLaaS, and one primary threat is model extraction attacks. To address this, there are many defense solutions but they suffer from unrealistic assumptions and generalization issues, making them less practical for reliable protection. Driven by these limitations, we introduce a novel defense mechanism, SAME, based on the concept of sample reconstruction. This strategy imposes minimal prerequisites on the defender's capabilities, eliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user query history, white-box model access, and additional intervention during m
    
[^85]: 具有模式的符号数值规划

    Symbolic Numeric Planning with Patterns. (arXiv:2312.09963v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.09963](http://arxiv.org/abs/2312.09963)

    本文提出了一种符号模式规划的新方法，通过编码问题为一个公式可以比现有方法更有效地寻找解决方案。在实验中，我们的规划器Patty在今年的IPC问题上表现出优异的性能。

    

    本文提出了一种解决线性数值规划问题的新方法，称为符号模式规划。给定一个规划问题Π，一个界限n和一个模式 - 定义为任意动作的序列 - 我们将寻找一个针对Π和界限n的计划问题编码为一个公式，该公式的变量和/或子句比最先进的rolled-up和松弛-松弛-$\exists$编码更少。更重要的是，我们证明了对于任何给定的界限，后两种编码都不可能找到一个有效的计划，而我们的编码可以。在实验方面，我们考虑了其他6个规划系统 - 包括参加今年国际规划竞赛（IPC）的系统 - 并展示了我们的规划器Patty在今年IPC问题上的非常好的比较性能。

    In this paper, we propose a novel approach for solving linear numeric planning problems, called Symbolic Pattern Planning. Given a planning problem $\Pi$, a bound $n$ and a pattern -- defined as an arbitrary sequence of actions -- we encode the problem of finding a plan for $\Pi$ with bound $n$ as a formula with fewer variables and/or clauses than the state-of-the-art rolled-up and relaxed-relaxed-$\exists$ encodings. More importantly, we prove that for any given bound, it is never the case that the latter two encodings allow finding a valid plan while ours does not. On the experimental side, we consider 6 other planning systems -- including the ones which participated in this year's International Planning Competition (IPC) -- and we show that our planner Patty has remarkably good comparative performances on this year's IPC problems.
    
[^86]: LLMind: 为复杂任务执行与AI和物联网进行协调的LLM框架

    LLMind: Orchestrating AI and IoT with LLMs for Complex Task Execution. (arXiv:2312.09007v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2312.09007](http://arxiv.org/abs/2312.09007)

    LLMind是一个利用大型语言模型（LLMs）作为中央协调器的AI框架，将LLMs与领域特定的AI模块整合，使得物联网设备能够有效协同执行复杂任务。

    

    本文介绍了LLMind，这是一个利用大型语言模型（LLMs）作为中央协调器的AI框架。该框架将LLMs与领域特定的AI模块整合，使得物联网设备能够有效协同执行复杂任务。LLMs通过用户友好的社交媒体平台与用户进行自然对话，提出执行复杂任务的计划。具体而言，复杂任务的执行是通过控制脚本实现的，这可能涉及多个领域特定的AI模块和物联网设备的协作。LLMs使用基于有限状态机（FSMs）的语言编码转换方法生成控制脚本。该框架还结合了语义分析和响应优化技术，以提高速度和效果。最终，该框架的设计不仅旨在创新物联网设备控制和丰富用户体验，还促进智能和集成的物联网设备。

    In this paper, we introduce LLMind, an AI framework that utilizes large language models (LLMs) as a central orchestrator. The framework integrates LLMs with domain-specific AI modules, enabling IoT devices to collaborate effectively in executing complex tasks. The LLM engages in natural conversations with human users via a user-friendly social media platform to come up with a plan to execute complex tasks. In particular, the execution of a complex task, which may involve the collaborations of multiple domain-specific AI modules and IoT devices, is realized through a control script. The LLM generates the control script using a Language-Code transformation approach based on finite-state machines (FSMs). The framework also incorporates semantic analysis and response optimization techniques to enhance speed and effectiveness. Ultimately, this framework is designed not only to innovate IoT device control and enrich user experiences but also to foster an intelligent and integrated IoT device
    
[^87]: 混合线性专家用于长期时间序列预测

    Mixture-of-Linear-Experts for Long-term Time Series Forecasting. (arXiv:2312.06786v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.06786](http://arxiv.org/abs/2312.06786)

    MoLE是一种混合线性专家模型，通过训练多个线性中心模型和一个路由模型，能够适应时间序列模式的周期性变化，并显著降低了预测误差。

    

    长期时间序列预测(LTSF)旨在预测给定过去值的时间序列的未来值。当前在这个问题上的最先进技术(SOTA)在某些情况下是由以线性为中心的模型实现的，这些模型主要具有线性映射层。然而，由于其固有的简单性，它们不能够适应时间序列模式的周期性变化。为了解决这个挑战，我们提出了一种混合专家风格的增强线性模型的方法，并提出了混合线性专家(MoLE)。MoLE不是训练单个模型，而是训练多个以线性为中心的模型(即专家)和一个权衡和混合其输出的路由模型。虽然整个框架是端到端训练的，但每个专家都学会专门处理特定的时间模式，而路由模型则学会自适应地组合专家们的输出。实验证明，MoLE降低了线性中心模型(DLinear，RLinear和RMLP)的预测误差。

    Long-term time series forecasting (LTSF) aims to predict future values of a time series given the past values. The current state-of-the-art (SOTA) on this problem is attained in some cases by linear-centric models, which primarily feature a linear mapping layer. However, due to their inherent simplicity, they are not able to adapt their prediction rules to periodic changes in time series patterns. To address this challenge, we propose a Mixture-of-Experts-style augmentation for linear-centric models and propose Mixture-of-Linear-Experts (MoLE). Instead of training a single model, MoLE trains multiple linear-centric models (i.e., experts) and a router model that weighs and mixes their outputs. While the entire framework is trained end-to-end, each expert learns to specialize in a specific temporal pattern, and the router model learns to compose the experts adaptively. Experiments show that MoLE reduces forecasting error of linear-centric models, including DLinear, RLinear, and RMLP, in 
    
[^88]: 无需校准的在线测试时间自适应应用于脑电运动想象解码

    Calibration-free online test-time adaptation for electroencephalography motor imagery decoding. (arXiv:2311.18520v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2311.18520](http://arxiv.org/abs/2311.18520)

    本研究探索了在线测试时间自适应（OTTA）的概念，无需校准且保护隐私，以实现无监督的连续自适应模型运行。研究使用了脑电运动想象解码任务，并通过轻量级架构和不同的OTTA技术来提高准确率。

    

    脑-计算机接口（BCI）在解码能力方面取得了显著进展，这为将人类大脑与外部设备相连提供了一条有希望的途径，这主要得益于愈发复杂的技术，特别是深度学习。然而，在现实世界的场景中实现高准确率仍然是一个挑战，因为不同测试和受试者之间存在分布差异。在本文中，我们将探讨在线测试时间自适应（OTTA）的概念，以在推断过程中以无监督的方式持续自适应模型。我们的方法通过在自适应过程中不需要访问源数据来保证隐私的保护。此外，OTTA通过不需要任何特定于会话或受试者的数据来实现无需校准运行。我们将使用轻量级架构以及不同的OTTA技术（如对齐，自适应批量归一化）来研究脑电运动想象解码的任务。

    Providing a promising pathway to link the human brain with external devices, Brain-Computer Interfaces (BCIs) have seen notable advancements in decoding capabilities, primarily driven by increasingly sophisticated techniques, especially deep learning. However, achieving high accuracy in real-world scenarios remains a challenge due to the distribution shift between sessions and subjects. In this paper we will explore the concept of online test-time adaptation (OTTA) to continuously adapt the model in an unsupervised fashion during inference time. Our approach guarantees the preservation of privacy by eliminating the requirement to access the source data during the adaptation process. Additionally, OTTA achieves calibration-free operation by not requiring any session- or subject-specific data. We will investigate the task of electroencephalography (EEG) motor imagery decoding using a lightweight architecture together with different OTTA techniques like alignment, adaptive batch normaliza
    
[^89]: 基于移动网格PDE的移动采样物理信息神经网络

    Moving Sampling Physics-informed Neural Networks induced by Moving Mesh PDE. (arXiv:2311.16167v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2311.16167](http://arxiv.org/abs/2311.16167)

    这项工作提出了一种基于移动网格PDE的移动采样物理信息神经网络(MMPDE-Net)，通过解决移动网格PDE来自适应生成新的采样点，并且结合物理信息神经网络（PINN）提出了移动采样PINN（MS-PINN）的框架。数值实验验证了MS-PINN相对于PINN的性能改善。

    

    在这项工作中，我们提出了一种基于移动网格方法的端到端自适应采样神经网络（MMPDE-Net），通过求解移动网格PDE，可以自适应生成新的采样点。该模型旨在改善采样点生成的质量。此外，我们基于MMPDE-Net开发了一种迭代算法，使得采样点更加精确和可控。由于MMPDE-Net是独立于深度学习求解器的框架，我们将其与物理信息神经网络（PINN）相结合，提出了移动采样PINN（MS-PINN），并在一些假设下通过误差分析验证了其有效性。最后，我们通过四个典型实例的数值实验验证了MS-PINN相对于PINN的性能改善，从而数值上证明了我们方法的有效性。

    In this work, we propose an end-to-end adaptive sampling neural network (MMPDE-Net) based on the moving mesh method, which can adaptively generate new sampling points by solving the moving mesh PDE. This model focuses on improving the quality of sampling points generation. Moreover, we develop an iterative algorithm based on MMPDE-Net, which makes the sampling points more precise and controllable. Since MMPDE-Net is a framework independent of the deep learning solver, we combine it with physics-informed neural networks (PINN) to propose moving sampling PINN (MS-PINN) and demonstrate its effectiveness by error analysis under some assumptions. Finally, we demonstrate the performance improvement of MS-PINN compared to PINN through numerical experiments of four typical examples, which numerically verify the effectiveness of our method.
    
[^90]: 如何确保安全的控制策略？迈向城市交通自主运营的SRL

    How to ensure a safe control strategy? Towards a SRL for urban transit autonomous operation. (arXiv:2311.14457v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.14457](http://arxiv.org/abs/2311.14457)

    本文提出了一个用于城市轨道交通自主运营列车的安全智能控制的SSA-DRL框架，在深度强化学习的基础上结合了线性时态逻辑、强化学习和蒙特卡洛树搜索。该框架能够生成满足速度约束和进度约束的安全控制命令序列。

    

    深度强化学习逐渐展示出其在城市轨道交通自主运营中潜在的决策能力。然而，由于强化学习无法在学习和执行过程中保证安全性，这仍然是实际应用强化学习面临的主要障碍之一。鉴于这个缺点，在安全关键的自主运营领域应用强化学习仍然具有挑战性，无法生成避免超速操作的安全控制命令序列。因此，本文提出了一个用于城市轨道交通自主运营列车的安全智能控制的SSA-DRL框架。所提出的框架结合了线性时态逻辑、强化学习和蒙特卡洛树搜索，并包括四个主要模块：后置屏蔽、搜索树模块、DRL框架和额外的行动者。此外，框架的输出可以满足速度约束和进度约束。

    Deep reinforcement learning has gradually shown its latent decision-making ability in urban rail transit autonomous operation. However, since reinforcement learning can not neither guarantee safety during learning nor execution, this is still one of the major obstacles to the practical application of reinforcement learning. Given this drawback, reinforcement learning applied in the safety-critical autonomous operation domain remains challenging without generating a safe control command sequence that avoids overspeed operations. Therefore, a SSA-DRL framework is proposed in this paper for safe intelligent control of urban rail transit autonomous operation trains. The proposed framework is combined with linear temporal logic, reinforcement learning and Monte Carlo tree search and consists of four mainly module: a post-posed shielding, a searching tree module, a DRL framework and an additional actor. Furthermore, the output of the framework can meet speed constraint, schedule constraint a
    
[^91]: 开放集dandelion网络用于物联网入侵检测

    Open Set Dandelion Network for IoT Intrusion Detection. (arXiv:2311.11249v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.11249](http://arxiv.org/abs/2311.11249)

    本文提出了一种基于无监督异构领域适应的开放集dandelion网络（OSDN）用于物联网入侵检测，该模型通过从知识丰富的源网络入侵领域进行入侵知识传输，以实现更准确的入侵检测。在开放集设置下，它能够检测到在源领域中未观测到的新兴目标领域入侵。

    

    随着物联网设备的广泛应用，保护它们免受恶意入侵变得至关重要。然而，物联网数据的稀缺性限制了传统入侵检测方法的适用性，这些方法高度依赖于数据。为了解决这个问题，我们在本文中提出了基于无监督异构领域适应的开放集dandelion网络（OSDN）。OSDN模型通过从知识丰富的源网络入侵领域进行入侵知识传输，从而实现对数据稀缺的目标物联网入侵领域的更准确的入侵检测。在开放集设置下，它还能检测到在源领域中未观测到的新兴目标领域入侵。为了实现这一点，OSDN模型将源领域形成一个类似蒲公英的特征空间，其中每个入侵类别被紧密分组并且不同的入侵类别被分隔开，即同时强调了类别间的可分性和类别内的紧凑性。

    As IoT devices become widely, it is crucial to protect them from malicious intrusions. However, the data scarcity of IoT limits the applicability of traditional intrusion detection methods, which are highly data-dependent. To address this, in this paper we propose the Open-Set Dandelion Network (OSDN) based on unsupervised heterogeneous domain adaptation in an open-set manner. The OSDN model performs intrusion knowledge transfer from the knowledge-rich source network intrusion domain to facilitate more accurate intrusion detection for the data-scarce target IoT intrusion domain. Under the open-set setting, it can also detect newly-emerged target domain intrusions that are not observed in the source domain. To achieve this, the OSDN model forms the source domain into a dandelion-like feature space in which each intrusion category is compactly grouped and different intrusion categories are separated, i.e., simultaneously emphasising inter-category separability and intra-category compactn
    
[^92]: 在化学合成中的反应条件推荐中，检索增强生成代理

    Retrieval-Augmented Generative Agent for Reaction Condition Recommendation in Chemical Synthesis. (arXiv:2311.10776v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2311.10776](http://arxiv.org/abs/2311.10776)

    本研究提出了一种转变性的人工智能代理，利用检索增强生成（RAG）技术自动化化学中的反应条件推荐（RCR）任务，通过模拟专家化学家的策略，使用大型语言模型（LLM）和新反应指纹，显著优于传统人工智能。此系统可以减轻化学家的工作负担，使他们能够更专注于更基础和创造性的科学问题。

    

    最近的人工智能研究为化学社会中的自动化化学反应铺平了一个有前途的未来。本研究提出了一种转变性的人工智能代理，利用检索增强生成（RAG）技术自动化化学中的反应条件推荐（RCR）任务。通过模拟专家化学家的搜索和分析策略，该代理使用大型语言模型（LLM）来查询分子数据库，并从在线文献中提取关键数据。此外，该人工智能代理还配备了我们为RCR任务开发的新反应指纹。由于RAG技术的使用，我们的代理使用更新的在线数据库作为知识源，显著优于仅受其训练数据固定知识限制的传统人工智能。由此产生的系统可以显著减轻化学家的工作负担，使他们能够更专注于更基础和创造性的科学问题。这一重大进展将计算技术与化学社会更紧密联系起来。

    Recent artificial intelligence (AI) research plots a promising future of automatic chemical reactions within the chemistry society. This study presents a transformative AI agent that automates the reaction condition recommendation (RCR) task in chemistry using retrieval-augmented generation (RAG) technology. By emulating expert chemists search and analysis strategies, the agent employs large language models (LLMs) to interrogate molecular databases and distill critical data from online literature. Further, the AI agent is equipped with our novel reaction fingerprint developed for the RCR task. Thanks to the RAG technology, our agent uses updated online databases as knowledge sources, significantly outperforming conventional AIs confined to the fixed knowledge within its training data. The resulting system can significantly reduce chemists workload, allowing them to focus on more fundamental and creative scientific problems. This significant advancement brings closer computational techn
    
[^93]: 在文本分类中探索语言模型中的概念级别的误相关性

    Explore Spurious Correlations at the Concept Level in Language Models for Text Classification. (arXiv:2311.08648v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.08648](http://arxiv.org/abs/2311.08648)

    本文研究了语言模型在文本分类中概念级别的误相关性问题，并通过使用ChatGPT分配概念标签和引入数据再平衡技术来解决这一问题。

    

    语言模型在众多自然语言处理任务中取得了显著的成功，采用了微调和上下文学习方法。虽然语言模型表现出卓越的性能，但由于训练数据中标签分布不平衡或上下文学习实例产生的误相关性，它们面临着鲁棒性挑战。以往的研究主要集中在词语、短语和句法特征上，忽视了概念级别的研究，这往往是由于缺乏概念标签和难以确定输入文本中的概念内容。本文提出了两个主要贡献。首先，我们使用ChatGPT为文本分配概念标签，评估模型在微调或上下文学习测试数据中的概念偏差。我们发现，当语言模型在训练或提示中遇到概念和标签之间的误相关性时，会采取预测的捷径。其次，我们引入了一种数据再平衡技术，将ChatGPT生成的反事实数据纳入其中。

    Language models (LMs) have achieved notable success in numerous NLP tasks, employing both fine-tuning and in-context learning (ICL) methods. While language models demonstrate exceptional performance, they face robustness challenges due to spurious correlations arising from imbalanced label distributions in training data or ICL exemplars. Previous research has primarily concentrated on word, phrase, and syntax features, neglecting the concept level, often due to the absence of concept labels and difficulty in identifying conceptual content in input texts. This paper introduces two main contributions. First, we employ ChatGPT to assign concept labels to texts, assessing concept bias in models during fine-tuning or ICL on test data. We find that LMs, when encountering spurious correlations between a concept and a label in training or prompts, resort to shortcuts for predictions. Second, we introduce a data rebalancing technique that incorporates ChatGPT-generated counterfactual data, ther
    
[^94]: 分布式联邦学习网络中对抗节点部署的影响

    The Impact of Adversarial Node Placement in Decentralized Federated Learning Networks. (arXiv:2311.07946v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2311.07946](http://arxiv.org/abs/2311.07946)

    本文研究了分布式联邦学习网络中对抗节点部署的影响，提出了两种基线策略，并提出了一种新颖的攻击算法，优先考虑对抗的分散性而不是中心性。

    

    随着联邦学习（FL）越来越受欢迎，新的分布式框架也越来越普及。这些框架利用分布式环境的优势，实现了快速和节能的设备间通信。然而，这种增长也加大了对强大安全措施的需求。尽管现有研究已经探讨了FL安全的各个方面，但是对于分布式网络中对抗节点部署的作用仍然较少研究。本文通过分析不同对抗部署策略下分布式FL的性能，来填补这一空白。我们建立了两种基线策略来部署对抗节点：随机部署和基于网络中心性的部署。在此基础上，我们提出了一种新颖的攻击算法，通过最大化对抗节点之间的平均网络距离，优先考虑对抗的分散性而不是中心性。

    As Federated Learning (FL) grows in popularity, new decentralized frameworks are becoming widespread. These frameworks leverage the benefits of decentralized environments to enable fast and energy-efficient inter-device communication. However, this growing popularity also intensifies the need for robust security measures. While existing research has explored various aspects of FL security, the role of adversarial node placement in decentralized networks remains largely unexplored. This paper addresses this gap by analyzing the performance of decentralized FL for various adversarial placement strategies when adversaries can jointly coordinate their placement within a network. We establish two baseline strategies for placing adversarial node: random placement and network centrality-based placement. Building on this foundation, we propose a novel attack algorithm that prioritizes adversarial spread over adversarial centrality by maximizing the average network distance between adversaries.
    
[^95]: GTP-ViT: 基于图传播的高效视觉Transformer模型

    GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation. (arXiv:2311.03035v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2311.03035](http://arxiv.org/abs/2311.03035)

    GTP-ViT是一种基于图传播的高效视觉Transformer模型，通过将不太重要的令牌信息传播给更重要的令牌，实现了模型的高效和信息保留。

    

    视觉Transformer模型（ViTs）在计算机视觉领域引起了革命，但在资源受限设备上部署仍然具有挑战性，因为计算需求较高。为了加快预训练的ViTs，研究人员开发了令牌修剪和令牌合并方法，旨在减少参与计算的令牌数量。然而，这些方法仍然有一些局限性，如修剪令牌导致的图像信息丢失和令牌匹配过程的低效性。本文介绍了一种新颖的基于图传播的令牌传播（GTP）方法，以解决高效ViTs中平衡模型效率和信息保留的挑战。受到图摘要算法的启发，GTP将不太重要的令牌信息细致地传播给空间和语义上相关的更重要的令牌。因此，剩下的少数令牌作为整个令牌图的摘要，使得该方法能够减少计算量。

    Vision Transformers (ViTs) have revolutionized the field of computer vision, yet their deployments on resource-constrained devices remain challenging due to high computational demands. To expedite pre-trained ViTs, token pruning and token merging approaches have been developed, which aim at reducing the number of tokens involved in the computation. However, these methods still have some limitations, such as image information loss from pruned tokens and inefficiency in the token-matching process. In this paper, we introduce a novel Graph-based Token Propagation (GTP) method to resolve the challenge of balancing model efficiency and information preservation for efficient ViTs. Inspired by graph summarization algorithms, GTP meticulously propagates less significant tokens' information to spatially and semantically connected tokens that are of greater importance. Consequently, the remaining few tokens serve as a summarization of the entire token graph, allowing the method to reduce computa
    
[^96]: AGI的层次：将AGI路径上的进展可操作化

    Levels of AGI: Operationalizing Progress on the Path to AGI. (arXiv:2311.02462v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.02462](http://arxiv.org/abs/2311.02462)

    本研究提出了一个框架来对人工通用智能（AGI）模型及其前驱进行分类。该框架引入了不同层次的AGI性能、广泛性和自主性，并提供了一个共同的语言用于比较模型、评估风险，并衡量在AGI路径上的进展。

    

    我们提出了一个框架，用于对人工通用智能（AGI）模型及其前驱的能力和行为进行分类。该框架引入了AGI性能、广泛性和自主性的层次。我们希望这个框架能够像自动驾驶的层次一样，通过提供一个共同的语言来比较模型、评估风险，并衡量在AGI路径上的进展。为了开发我们的框架，我们分析了现有的AGI定义，并提取出了一个有用的AGI本体论应满足的六个原则。这些原则包括关注能力而不是机制；分别评估广泛性和性能；定义AGI路径上的阶段，而不是专注于终点。基于这些原则，我们提出了“AGI的层次”，根据能力的深度（性能）和广度（广泛性），并思考当前系统如何符合这个本体论。我们讨论了对实现AGI所提出的具有挑战性的要求。

    We propose a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors. This framework introduces levels of AGI performance, generality, and autonomy. It is our hope that this framework will be useful in an analogous way to the levels of autonomous driving, by providing a common language to compare models, assess risks, and measure progress along the path to AGI. To develop our framework, we analyze existing definitions of AGI, and distill six principles that a useful ontology for AGI should satisfy. These principles include focusing on capabilities rather than mechanisms; separately evaluating generality and performance; and defining stages along the path toward AGI, rather than focusing on the endpoint. With these principles in mind, we propose 'Levels of AGI' based on depth (performance) and breadth (generality) of capabilities, and reflect on how current systems fit into this ontology. We discuss the challenging req
    
[^97]: 预训练推荐系统：一种因果去偏见的视角

    Pre-trained Recommender Systems: A Causal Debiasing Perspective. (arXiv:2310.19251v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2310.19251](http://arxiv.org/abs/2310.19251)

    本文探讨了将预训练模型的范式应用于推荐系统的可能性和挑战，提出开发一种通用推荐系统，可以用于少样本学习，并在未知新领域中快速适应，以提高性能。

    

    最近对于预训练的视觉/语言模型的研究表明了一种新的、有前景的解决方案建立范式在人工智能领域，其中模型可以在广泛描述通用任务空间的数据上进行预训练，然后成功地适应解决各种下游任务，即使训练数据非常有限（如在零样本学习或少样本学习场景中）。受到这样的进展的启发，我们在本文中研究了将这种范式调整到推荐系统领域的可能性和挑战，这一领域在预训练模型的视角下较少被调查。特别是，我们提出开发一种通用推荐系统，通过对从不同领域中提取的通用用户-物品交互数据进行训练，捕捉到通用的交互模式，然后可以快速适应提升少样本学习性能，在未知新领域（数据有限）中发挥作用。

    Recent studies on pre-trained vision/language models have demonstrated the practical benefit of a new, promising solution-building paradigm in AI where models can be pre-trained on broad data describing a generic task space and then adapted successfully to solve a wide range of downstream tasks, even when training data is severely limited (e.g., in zero- or few-shot learning scenarios). Inspired by such progress, we investigate in this paper the possibilities and challenges of adapting such a paradigm to the context of recommender systems, which is less investigated from the perspective of pre-trained model. In particular, we propose to develop a generic recommender that captures universal interaction patterns by training on generic user-item interaction data extracted from different domains, which can then be fast adapted to improve few-shot learning performance in unseen new domains (with limited data).  However, unlike vision/language data which share strong conformity in the semant
    
[^98]: 联邦多目标学习

    Federated Multi-Objective Learning. (arXiv:2310.09866v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.09866](http://arxiv.org/abs/2310.09866)

    本研究提出了一种新的联邦多目标学习（FMOL）框架，在满足多代理多任务学习应用的分布式性质和数据隐私需求的同时，支持不同客户端上的不同目标函数集合。通过引入联邦学习的范式，将多目标优化（MOO）推广到联邦学习领域。

    

    在最近几年中，多目标优化（MOO）作为许多多代理多任务学习应用的基础问题出现。然而，现有的MOO算法仍局限于集中式学习环境，无法满足这些多代理多任务学习应用的分布式性质和数据隐私需求。这激发了我们提出一种新的联邦多目标学习（FMOL）框架，其中多个客户端在保持他们的训练数据私密的同时，分布式协作解决一个MOO问题。值得注意的是，我们的FMOL框架允许不同客户端上的不同目标函数集合，以支持广泛的应用，这首次将MOO形式化推广到联邦学习范式中。对于这个FMOL框架，我们提出了两种新的联邦多目标优化（FMOO）算法，称为联邦多梯度下降平均（FMGDA）和联邦随机梯度下降（Federated SGD）。

    In recent years, multi-objective optimization (MOO) emerges as a foundational problem underpinning many multi-agent multi-task learning applications. However, existing algorithms in MOO literature remain limited to centralized learning settings, which do not satisfy the distributed nature and data privacy needs of such multi-agent multi-task learning applications. This motivates us to propose a new federated multi-objective learning (FMOL) framework with multiple clients distributively and collaboratively solving an MOO problem while keeping their training data private. Notably, our FMOL framework allows a different set of objective functions across different clients to support a wide range of applications, which advances and generalizes the MOO formulation to the federated learning paradigm for the first time. For this FMOL framework, we propose two new federated multi-objective optimization (FMOO) algorithms called federated multi-gradient descent averaging (FMGDA) and federated stoc
    
[^99]: 在协变量转移下，仅凭少量测试样本改善公平性和准确性的权衡

    Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift. (arXiv:2310.07535v1 [cs.LG])

    [http://arxiv.org/abs/2310.07535](http://arxiv.org/abs/2310.07535)

    在协变量转移下，我们提出了一种新的损失函数和表示匹配损失来优化模型的准确性和公平性，通过实验证明在公平性和准确性权衡方面优于其他基线算法，并且提出了一种未经研究的非对称协变量转移设置。

    

    测试数据中的协变量转移会显著降低模型的准确性和公平性表现。在这种情况下，确保不同敏感群体之间的公平性非常重要，因为这涉及到诸如刑事司法等社会影响。我们在无监督的情况下进行操作，只有一小组无标签的测试样本和一个带标签的训练集可用。为了解决这个问题，我们提出了三个贡献。第一个贡献是基于新型复合加权熵的目标函数，用于预测准确性，并通过表示匹配损失来优化公平性。我们通过实验证明，使用我们的损失函数进行优化，在几个标准数据集上在公平性-准确性权衡方面优于许多最先进的基线算法。我们的第二个贡献是一个新的设置，我们称之为非对称协变量转移，在我们所知的范围内尚未研究过非对称协变量转移。

    Covariate shift in the test data can significantly downgrade both the accuracy and the fairness performance of the model. Ensuring fairness across different sensitive groups in such settings is of paramount importance due to societal implications like criminal justice. We operate under the unsupervised regime where only a small set of unlabeled test samples along with a labeled training set is available. Towards this problem, we make three contributions. First is a novel composite weighted entropy based objective for prediction accuracy which is optimized along with a representation matching loss for fairness. We experimentally verify that optimizing with our loss formulation outperforms a number of state-of-the-art baselines in the pareto sense with respect to the fairness-accuracy tradeoff on several standard datasets. Our second contribution is a new setting we term Asymmetric Covariate Shift that, to the best of our knowledge, has not been studied before. Asymmetric covariate shift
    
[^100]: 逐步功能重构的关系概念分析

    Stepwise functional refoundation of relational concept analysis. (arXiv:2310.06441v1 [cs.AI])

    [http://arxiv.org/abs/2310.06441](http://arxiv.org/abs/2310.06441)

    逐步功能重构的关系概念分析（RCA）是形式概念分析的扩展，通过定义良构解决方案的空间和相关函数，解决了RCA在循环依赖数据上返回单一概念格家族的问题。

    

    关系概念分析（RCA）是形式概念分析的扩展，允许同时处理多个相关的语境。它被设计用于从数据中学习描述逻辑理论，并在各种应用中使用。关于RCA的一个令人困惑的观察是，尽管数据存在循环依赖关系，它返回一个单一的概念格家族，其他解决方案可能被认为是可接受的。RCA的语义以操作方式提供，对此问题并没有提供明确的解释。在本报告中，我们将这些可接受的解决方案定义为属于初始语境确定的空间的概念格家族（良构），不能扩展新属性（饱和），并且仅涉及该家族的概念（自支持）。我们通过定义良构解决方案的空间以及该空间上的两个函数（一个扩张函数和一个收缩函数），采用功能视图来描述RCA过程。我们展示了可接受的解决方案…

    Relational concept analysis (RCA) is an extension of formal concept analysis allowing to deal with several related contexts simultaneously. It has been designed for learning description logic theories from data and used within various applications. A puzzling observation about RCA is that it returns a single family of concept lattices although, when the data feature circular dependencies, other solutions may be considered acceptable. The semantics of RCA, provided in an operational way, does not shed light on this issue. In this report, we define these acceptable solutions as those families of concept lattices which belong to the space determined by the initial contexts (well-formed), cannot scale new attributes (saturated), and refer only to concepts of the family (self-supported). We adopt a functional view on the RCA process by defining the space of well-formed solutions and two functions on that space: one expansive and the other contractive. We show that the acceptable solutions a
    
[^101]: 高阶DeepTrails：对*Trails的统一方法

    Higher-Order DeepTrails: Unified Approach to *Trails. (arXiv:2310.04477v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04477](http://arxiv.org/abs/2310.04477)

    本论文提出了一种用于分析人类行为的统一方法，通过使用自回归语言模型来捕捉序列中的高阶依赖关系，以改进Web浏览或交通导航等应用的底层基础设施或用户界面。

    

    分析、理解和描述人类行为在不同的环境中具有优势，如网络浏览或交通导航。理解人类行为自然有助于改进和优化底层基础设施或用户界面。通常，人类导航是由状态间转换的序列表示。先前的工作建议使用假设来分析这些转换，代表不同的导航直观。为了在数学上抓住这个设置，使用一阶马尔可夫链来捕捉行为，因此可以应用不同类型的图比较，但是会有损失序列中的高阶依赖信息的固有缺点。为此，我们提出使用自回归语言模型来分析整个序列，因为它们通常用于建模序列中的高阶依赖关系。我们展示了我们的方法可以轻松地适应不同的设置。

    Analyzing, understanding, and describing human behavior is advantageous in different settings, such as web browsing or traffic navigation. Understanding human behavior naturally helps to improve and optimize the underlying infrastructure or user interfaces. Typically, human navigation is represented by sequences of transitions between states. Previous work suggests to use hypotheses, representing different intuitions about the navigation to analyze these transitions. To mathematically grasp this setting, first-order Markov chains are used to capture the behavior, consequently allowing to apply different kinds of graph comparisons, but comes with the inherent drawback of losing information about higher-order dependencies within the sequences. To this end, we propose to analyze entire sequences using autoregressive language models, as they are traditionally used to model higher-order dependencies in sequences. We show that our approach can be easily adapted to model different settings in
    
[^102]: STAMP：通过Stein变分梯度下降实现可微的任务和运动规划

    STAMP: Differentiable Task and Motion Planning via Stein Variational Gradient Descent. (arXiv:2310.01775v1 [cs.RO])

    [http://arxiv.org/abs/2310.01775](http://arxiv.org/abs/2310.01775)

    STAMP是一种基于Stein变分梯度下降的算法，通过并行化和可微仿真高效地搜索多个多样化的任务和运动规划解决方案。

    

    许多操作任务，如使用工具或装配零件，往往需要符号和几何推理。任务和运动规划（TAMP）算法通常通过对高级任务序列进行树搜索并检查运动学和动力学可行性来解决这些问题。虽然性能良好，但大多数现有算法的效率非常低，因为其时间复杂性随可能动作和物体数量的增加呈指数增长。此外，它们只能找到单个解决方案，而可能存在许多可行的计划。为了解决这些限制，我们提出了一种名为Stein任务和运动规划（STAMP）的新算法，它利用并行化和可微仿真来高效地搜索多个多样化的计划。STAMP将离散和连续的TAMP问题转化为可以使用变分推断解决的连续优化问题。我们的算法基于Stein变分梯度下降，一种概率推断方法。

    Planning for many manipulation tasks, such as using tools or assembling parts, often requires both symbolic and geometric reasoning. Task and Motion Planning (TAMP) algorithms typically solve these problems by conducting a tree search over high-level task sequences while checking for kinematic and dynamic feasibility. While performant, most existing algorithms are highly inefficient as their time complexity grows exponentially with the number of possible actions and objects. Additionally, they only find a single solution to problems in which many feasible plans may exist. To address these limitations, we propose a novel algorithm called Stein Task and Motion Planning (STAMP) that leverages parallelization and differentiable simulation to efficiently search for multiple diverse plans. STAMP relaxes discrete-and-continuous TAMP problems into continuous optimization problems that can be solved using variational inference. Our algorithm builds upon Stein Variational Gradient Descent, a gra
    
[^103]: 深度邻居层聚合用于轻量级自监督单眼深度估计

    Deep Neighbor Layer Aggregation for Lightweight Self-Supervised Monocular Depth Estimation. (arXiv:2309.09272v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.09272](http://arxiv.org/abs/2309.09272)

    该论文提出了一种用于轻量级自监督单眼深度估计的深度邻居层聚合方法，通过使用上下文特征融合和轻量级通道注意力，在减少参数数量的同时保持准确性，在KITTI基准测试中取得了更好的结果。

    

    随着自监督单眼深度估计在机器人和自动驾驶领域的频繁使用，模型的效率变得越来越重要。目前大多数方法采用更大、更复杂的网络来提高深度估计的准确性。一些研究者将Transformer引入到自监督单眼深度估计中以获得更好的性能，但这种方法导致参数和计算量较高。我们提出了一种使用上下文特征融合的完全卷积深度估计网络。与UNet++和HRNet不同，我们使用高分辨率和低分辨率特征来保留小目标和快速移动物体的信息，而不是进行远程融合。在解码器阶段，我们还采用基于卷积的轻量级通道注意力来提升深度估计结果。我们的方法减少了参数数量而不损失准确性。在KITTI基准测试中的实验证明了我们的方法可以获得更好的结果。

    With the frequent use of self-supervised monocular depth estimation in robotics and autonomous driving, the model's efficiency is becoming increasingly important. Most current approaches apply much larger and more complex networks to improve the precision of depth estimation. Some researchers incorporated Transformer into self-supervised monocular depth estimation to achieve better performance. However, this method leads to high parameters and high computation. We present a fully convolutional depth estimation network using contextual feature fusion. Compared to UNet++ and HRNet, we use high-resolution and low-resolution features to reserve information on small targets and fast-moving objects instead of long-range fusion. We further promote depth estimation results employing lightweight channel attention based on convolution in the decoder stage. Our method reduces the parameters without sacrificing accuracy. Experiments on the KITTI benchmark show that our method can get better result
    
[^104]: 基于深度学习的语音增强去噪过程的连续建模

    Continuous Modeling of the Denoising Process for Speech Enhancement Based on Deep Learning. (arXiv:2309.09270v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2309.09270](http://arxiv.org/abs/2309.09270)

    本文介绍了一种基于深度学习的连续建模方法，用于语音增强去噪过程。通过训练神经网络学习表示不同状态变量，并使用控制因子实现可控的降噪水平。实验结果表明，在清晰目标中保留少量噪声可以改善语音增强效果。

    

    本文提出了一种基于连续建模的深度学习语音增强去噪方法，重点关注去噪过程。我们使用一个状态变量来表示去噪过程，起始状态为噪声语音，结束状态为清晰语音。状态变量中的噪声成分随着状态指数的变化而逐渐减少，直到噪声成分为0。在训练过程中，我们使用类似UNet的神经网络学习估计从连续去噪过程中采样得到的每个状态变量。在测试中，我们引入一个控制因子作为嵌入向量，范围从零到一，用于控制降噪水平。该方法能够实现可控的语音增强，并可以适应各种应用场景。实验结果表明，在清晰目标中保留少量噪声有助于语音增强，证明了客观语音指标和自动语音识别的改进。

    In this paper, we explore a continuous modeling approach for deep-learning-based speech enhancement, focusing on the denoising process. We use a state variable to indicate the denoising process. The starting state is noisy speech and the ending state is clean speech. The noise component in the state variable decreases with the change of the state index until the noise component is 0. During training, a UNet-like neural network learns to estimate every state variable sampled from the continuous denoising process. In testing, we introduce a controlling factor as an embedding, ranging from zero to one, to the neural network, allowing us to control the level of noise reduction. This approach enables controllable speech enhancement and is adaptable to various application scenarios. Experimental results indicate that preserving a small amount of noise in the clean target benefits speech enhancement, as evidenced by improvements in both objective speech measures and automatic speech recogniti
    
[^105]: 大规模语言模型能够理解真实世界复杂指令吗？

    Can Large Language Models Understand Real-World Complex Instructions?. (arXiv:2309.09150v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.09150](http://arxiv.org/abs/2309.09150)

    本论文提出了一个用于评估大规模语言模型理解复杂指令能力的基准——CELLO。通过设计复杂指令的八个特征并构建全面的评估数据集，可以解决现有基准的不足。

    

    大规模语言模型（LLMs）能够理解人类指令，展示了它们在传统NLP任务之外的实用应用潜力。然而，它们仍然在复杂指令上存在困难，这些指令可以是需要多个任务和约束的复杂任务描述，或者包含长篇背景、噪声、异构信息和多轮格式的复杂输入。由于这些特点，LLMs常常忽略任务描述中的语义约束，产生错误的格式，违反长度或样本计数的约束，对输入文本不忠实。现有的基准不足以评估LLMs理解复杂指令的能力，因为它们是封闭式和简单的。为了弥补这一间隙，我们提出了CELLO，一个用于系统评估LLMs遵循复杂指令能力的基准。我们为复杂指令设计了八个特征，并从现实场景中构建了一个全面的评估数据集。我们还建立了四个评价标准。

    Large language models (LLMs) can understand human instructions, showing their potential for pragmatic applications beyond traditional NLP tasks. However, they still struggle with complex instructions, which can be either complex task descriptions that require multiple tasks and constraints, or complex input that contains long context, noise, heterogeneous information and multi-turn format. Due to these features, LLMs often ignore semantic constraints from task descriptions, generate incorrect formats, violate length or sample count constraints, and be unfaithful to the input text. Existing benchmarks are insufficient to assess LLMs' ability to understand complex instructions, as they are close-ended and simple. To bridge this gap, we propose CELLO, a benchmark for evaluating LLMs' ability to follow complex instructions systematically. We design eight features for complex instructions and construct a comprehensive evaluation dataset from real-world scenarios. We also establish four crit
    
[^106]: TextBind: 多轮交错多模态指令跟随

    TextBind: Multi-turn Interleaved Multimodal Instruction-following. (arXiv:2309.08637v1 [cs.CL])

    [http://arxiv.org/abs/2309.08637](http://arxiv.org/abs/2309.08637)

    TextBind是一个注释极少的框架，用于将较大规模的语言模型赋予多轮交错多模态指令跟随能力，并通过图像-标题对生成多轮多模态指令-回应对话。这个框架对于解决实际任务具有重要意义，并为未来的研究提供了数据集、模型和演示。

    

    具有指令跟随能力的大型语言模型已经在人工智能领域产生了革命性的影响。这些模型通过其自然语言界面展示了卓越的泛化能力，可以解决各种实际任务。然而，它们的性能在很大程度上依赖于高质量的示例数据，而这往往很难获得。当涉及到多模态指令跟随时，这个挑战变得更加严峻。我们引入了TextBind，这是一个几乎不需要注释的框架，用于赋予较大规模的语言模型多轮交错多模态指令跟随能力。我们的方法仅需要图像-标题对，并从语言模型生成多轮多模态指令-回应对话。我们发布了我们的数据集、模型和演示，以促进未来在多模态指令跟随领域的研究。

    Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. We release our dataset, model, and demo to foster future research in the area of multimodal instruction following.
    
[^107]: 通过在线凸优化实现在线子模最大化

    Online Submodular Maximization via Online Convex Optimization. (arXiv:2309.04339v1 [cs.LG])

    [http://arxiv.org/abs/2309.04339](http://arxiv.org/abs/2309.04339)

    本论文研究了在线设置下的一般性子模最大化问题，并将一类大型子模函数归约到在线凸优化问题中。这种归约方式可在组合优化中实现次线性遗憾，并且适用于许多不同版本的在线学习问题。

    

    我们研究了在线设置下的一般性子模最大化问题在一般性模性约束下。我们证明了在线优化一类大型子模函数，即加权阈值势函数，可以归约到在线凸优化(OCO)问题。这是因为这个类别的函数可以进行凹松弛;因此，结合适当的舍入方案，OCO策略可以在组合设置中实现次线性遗憾。我们还展示了我们的简化方式可以应用在许多不同版本的在线学习问题中，包括动态遗憾、强盗和乐观学习等设置。

    We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, weighted threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings.
    
[^108]: 通过提示策略增强评论文本的多领域情感分析

    Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies. (arXiv:2309.02045v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.02045](http://arxiv.org/abs/2309.02045)

    通过提示策略，本论文探究了如何通过应用角色扮演和思维链提示策略来增强大型语言模型（LLMs）在情感分析中的性能，并在三个不同领域的数据集上进行了评估。

    

    大型语言模型（LLMs）在科学研究和实际应用中取得了重大进展。现有研究已证明了LLMs在各种自然语言处理任务中的最新性能。然而，如何通过提示策略进一步增强LLMs在特定任务中的性能仍然是一个重要问题。本文探讨了通过应用提示策略来提高LLMs在情感分析中的性能。我们对情感分析任务的提示过程进行了建模，并介绍了两种针对情感分析的新颖策略：角色扮演（RP）提示和思维链（CoT）提示。具体地，我们还提出了RP-CoT提示策略，它是RP提示和CoT提示的结合。我们在三个不同领域的数据集上进行了比较实验，以评估所提出的情感分析策略的有效性。结果表明...

    Large Language Models (LLMs) have made significant strides in both scientific research and practical applications. Existing studies have demonstrated the state-of-the-art (SOTA) performance of LLMs in various natural language processing tasks. However, the question of how to further enhance LLMs' performance in specific task using prompting strategies remains a pivotal concern. This paper explores the enhancement of LLMs' performance in sentiment analysis through the application of prompting strategies. We formulate the process of prompting for sentiment analysis tasks and introduce two novel strategies tailored for sentiment analysis: RolePlaying (RP) prompting and Chain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT prompting strategy which is a combination of RP prompting and CoT prompting. We conduct comparative experiments on three distinct domain datasets to evaluate the effectiveness of the proposed sentiment analysis strategies. The results demonstrate tha
    
[^109]: 开放词汇语义分割通过属性分解聚合

    Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation. (arXiv:2309.00096v1 [cs.CV])

    [http://arxiv.org/abs/2309.00096](http://arxiv.org/abs/2309.00096)

    本研究提出了一种通过属性分解聚合的框架来解决开放词汇语义分割中存在的问题，该框架受到人类认知解释新概念的启发。

    

    开放词汇语义分割是一项具有挑战性的任务，需要在推理时对新的对象类别进行分割。最近的研究探索了视觉-语言预训练来处理这个任务，但在实际场景中存在不切实际的假设，即低质量的文本类别名称。例如，这种范式假设新的文本类别将被准确完整地提供，并且存在于预训练期间的词典中。然而，当遇到简短或不完整的名称、在预训练的词典中不存在的新词以及难以描述的类别时，异常情况经常发生。为了解决这些问题，本文提出了一种新的分解-聚合框架，受人类认知在理解新概念方面的启发。具体地，在分解阶段，我们将类别名称分解为多样的属性描述，以丰富语义上下文。设计了两种属性构建策略：使用大型语言

    Open-vocabulary semantic segmentation is a challenging task that requires segmenting novel object categories at inference time. Recent works explore vision-language pre-training to handle this task, but suffer from unrealistic assumptions in practical scenarios, i.e., low-quality textual category names. For example, this paradigm assumes that new textual categories will be accurately and completely provided, and exist in lexicons during pre-training. However, exceptions often happen when meet with ambiguity for brief or incomplete names, new words that are not present in the pre-trained lexicons, and difficult-to-describe categories for users. To address these issues, this work proposes a novel decomposition-aggregation framework, inspired by human cognition in understanding new concepts. Specifically, in the decomposition stage, we decouple class names into diverse attribute descriptions to enrich semantic contexts. Two attribute construction strategies are designed: using large langu
    
[^110]: LLM强化了交通信号控制的从仿真到真实的迁移

    LLM Powered Sim-to-real Transfer for Traffic Signal Control. (arXiv:2308.14284v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.14284](http://arxiv.org/abs/2308.14284)

    本研究利用大型语言模型（LLMs）通过基于提示的行动转换，解决了交通信号控制任务中从仿真到真实的迁移问题。

    

    尽管已经有很多解决方案用于交通信号控制（TSC）任务，旨在提供高效的交通和减轻拥堵浪费，但仍然存在性能差距，当在仿真器中训练的策略部署到现实世界时。本研究利用大型语言模型（LLMs）通过基于提示的扎根行动转换，来理解和描述系统动态。通过接受填空提示模板，并根据可以访问的上下文填写答案，利用预训练的LLM的推理能力，应用于对系统动态的理解。

    Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks aiming to provide efficient transportation and mitigate congestion waste. In recent, promising results have been attained by Reinforcement Learning (RL) methods through trial and error in simulators, bringing confidence in solving cities' congestion headaches. However, there still exist performance gaps when simulator-trained policies are deployed to the real world. This issue is mainly introduced by the system dynamic difference between the training simulator and the real-world environments. The Large Language Models (LLMs) are trained on mass knowledge and proved to be equipped with astonishing inference abilities. In this work, we leverage LLMs to understand and profile the system dynamics by a prompt-based grounded action transformation. Accepting the cloze prompt template, and then filling in the answer based on accessible context, the pre-trained LLM's inference ability is exploited and applied to understa
    
[^111]: 探索指令调整的格式一致性

    Exploring Format Consistency for Instruction Tuning. (arXiv:2307.15504v1 [cs.CL])

    [http://arxiv.org/abs/2307.15504](http://arxiv.org/abs/2307.15504)

    本研究探究了指令调整的格式一致性，并提出了统一指令调整（UIT）框架，通过自动格式转换来提高泛化性能。该研究强调了格式一致性的重要性。

    

    指令调整已经成为一种提升大型语言模型遵循人类指令能力的有前途的方法。研究表明，增加训练数据中指令的多样性和数量可以持续提升泛化性能，从而促进了最近的一项努力，即收集各种指令并将现有的指令调整数据集整合到更大的集合中。然而，不同用户有其独特的表达指令的方式，不同数据集之间通常存在指令风格和格式的变化，即格式不一致性。在这项工作中，我们研究了格式不一致性如何影响指令调整的性能。我们提出了一个名为“统一指令调整”（UIT）的框架，通过调用OpenAI的API实现在不同的指令调整数据集之间的自动格式转换。我们展示了UIT成功提高了在未见指令上的泛化性能，并强调了格式一致性的重要性。

    Instruction tuning has emerged as a promising approach to enhancing large language models in following human instructions. It is shown that increasing the diversity and number of instructions in the training data can consistently enhance generalization performance, which facilitates a recent endeavor to collect various instructions and integrate existing instruction tuning datasets into larger collections. However, different users have their unique ways of expressing instructions, and there often exist variations across different datasets in the instruction styles and formats, i.e., format inconsistency. In this work, we study how format inconsistency may impact the performance of instruction tuning. We propose a framework called "Unified Instruction Tuning" (UIT), which calls OpenAI APIs for automatic format transfer among different instruction tuning datasets. We show that UIT successfully improves the generalization performance on unseen instructions, which highlights the importance
    
[^112]: RL$^3$:通过RL内部的RL$^2$提升元强化学习方法

    RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])

    [http://arxiv.org/abs/2306.15909](http://arxiv.org/abs/2306.15909)

    RL$^3$是一种原则性混合方法，通过将传统强化学习学到的任务特定动作值作为元强化学习神经网络的输入，提高了元强化学习的性能。

    

    元强化学习（meta-RL）方法，如RL$^2$，已经成为学习针对给定任务分布的数据高效的强化学习算法的有希望的方法。然而，这些强化学习算法在长期任务和超出分布任务方面存在困难，因为它们依赖于递归神经网络来处理经验序列，而不是将它们总结为一般的强化学习组件，例如价值函数。此外，即使是transformers在训练和推理成本变得禁止之前也对它们可以有效推理的历史长度有实际限制。相比之下，传统的强化学习算法在数据效率方面不足，因为它们没有利用领域知识，但随着更多数据的可用性，它们会收敛到最优策略。在本文中，我们提出了RL$^3$，一种组合了传统强化学习和元强化学习的原则性混合方法，通过将通过传统强化学习学习到的特定任务动作值作为元强化学习神经网络的一个输入。

    Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
    
[^113]: 从$O(\sqrt{n})$到$O(\log(n))$的二次规划问题

    From $O(\sqrt n)$ to $O(\log n)$ in Quadratic Programming. (arXiv:2306.15079v1 [math.OC])

    [http://arxiv.org/abs/2306.15079](http://arxiv.org/abs/2306.15079)

    这篇论文提出了一种迭代复杂度为$O(\log(n))$的二次规划优化算法，并通过严格的理论证明验证了该算法的可行性。这一重大突破使得我们从$O(\sqrt{n})$的优化算法过渡到$O(\log(n))$的优化算法，其在大数据和人工智能时代具有重要应用价值。

    

    多年来，数值优化理论一直存在一个困扰，即是否存在一个迭代复杂度为$O(\log(n))$的优化算法。本文通过引入一种全新的优化算法和严格的理论证明来回答这个问题。该算法以有界盒二次规划问题（Box-QP）为起点，许多实际优化问题可以通过对偶理论转化为Box-QP问题。本文首次提出了一个迭代复杂度为$O(\log(n))$的QP算法，尤其是其表现类似于“直接”方法：所需迭代次数是确定性的，精确值为$\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$。这一重大突破使得我们能够从$O(\sqrt{n})$的优化算法过渡到$O(\log(n))$的优化算法，其出色的可扩展性在当今的大数据和人工智能时代尤为重要。

    A "dark cloud" hangs over numerical optimization theory for decades, namely, whether an optimization algorithm $O(\log(n))$ iteration complexity exists. "Yes", this paper answers, with a new optimization algorithm and strict theory proof. It starts with box-constrained quadratic programming (Box-QP), and many practical optimization problems fall into Box-QP. Smooth quadratic programming (QP) and nonsmooth Lasso can be reformulated as Box-QP via duality theory. It is the first time to present an $O(\log(n))$ iteration complexity QP algorithm, in particular, which behaves like a "direct" method: the required number of iterations is deterministic with exact value $\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$. This significant breakthrough enables us to transition from the $O(\sqrt{n})$ to the $O(\log(n))$ optimization algorithm, whose amazing scalability is particularly relevant in today's era of big data and artificial intelligence.
    
[^114]: RM-PRT: 真实的机器人操作模拟器和基于渐进推理任务的基准评估

    RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks. (arXiv:2306.11335v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.11335](http://arxiv.org/abs/2306.11335)

    该论文介绍了一个基于逼真机器人操作模拟器的基准评估RM-PRT，并提出了通用的评估流程，旨在通过大规模自然语言指令和多模态提示对机器人操作进行详细评估。

    

    最近，预训练的大规模语言模型（LLM），如ChatGPT和GPT-4的出现，显着推进了机器的自然语言理解能力。这一突破使我们能够将这些开源LLM无缝地集成到统一的机器人模拟器环境中，以帮助机器人准确理解和执行人类自然语言指令。为此，我们引入了一个逼真的机器人操作模拟器，并在此基础上构建了一个基于渐进推理任务的机器人操作基准（RM-PRT）。具体而言，RM-PRT基准评估基于Unreal Engine 5构建了一个新的高保真数字双胞胎场景，其中包括782个类别，2023个物体，并使用ChatGPT生成了15,000个自然语言指令，以详细评估机器人操作。我们提出了一个通用的RM-PRT基准评估流程，该流程接受包含自然语言指令的多模态提示作为输入，并自动输出行动。

    Recently, the advent of pre-trained large-scale language models (LLMs) like ChatGPT and GPT-4 have significantly advanced the machine's natural language understanding capabilities. This breakthrough has allowed us to seamlessly integrate these open-source LLMs into a unified robot simulator environment to help robots accurately understand and execute human natural language instructions. To this end, in this work, we introduce a realistic robotic manipulation simulator and build a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRT benchmark builds a new high-fidelity digital twin scene based on Unreal Engine 5, which includes 782 categories, 2023 objects, and 15K natural language instructions generated by ChatGPT for a detailed evaluation of robot manipulation. We propose a general pipeline for the RM-PRT benchmark that takes as input multimodal prompts containing natural language instructions and automatically outputs action
    
[^115]: 解决零样本人工智能协同中的协作不兼容性

    Tackling Cooperative Incompatibility for Zero-Shot Human-AI Coordination. (arXiv:2306.03034v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.03034](http://arxiv.org/abs/2306.03034)

    本文提出了解决零样本人工智能协同中协作不兼容性的COLE框架，利用图形理论的视角，通过制定开放式目标，在涉及不熟悉的人类情境中确保人工智能代理和队友之间的协调。

    

    在涉及不熟悉的人类的情境中，确保人工智能代理和队友（人类玩家或者人工智能代理）之间的协调仍然是零样本协同中的重大挑战。当人工智能代理无法与某些先前未知的合作伙伴进行同步时，协作不兼容性问题特别突出。传统算法通过优化固定的目标在人口中合作，促进策略和行为的多样性来与合作伙伴合作。然而，这些技术可能导致学习损失，并且无法与人口中的特定策略合作，这种现象被称为学习中的协作不兼容性。为了解决学习中的协作不兼容性并有效地解决ZSC中的问题，我们引入了 Cooperative Open-ended LEarning（COLE）框架，该框架利用图形理论的视角，对两个玩家的合作游戏制定开放式目标。

    Securing coordination between AI agent and teammates (human players or AI agents) in contexts involving unfamiliar humans continues to pose a significant challenge in Zero-Shot Coordination. The issue of cooperative incompatibility becomes particularly prominent when an AI agent is unsuccessful in synchronizing with certain previously unknown partners. Traditional algorithms have aimed to collaborate with partners by optimizing fixed objectives within a population, fostering diversity in strategies and behaviors. However, these techniques may lead to learning loss and an inability to cooperate with specific strategies within the population, a phenomenon named cooperative incompatibility in learning. In order to solve cooperative incompatibility in learning and effectively address the problem in the context of ZSC, we introduce the Cooperative Open-ended LEarning (COLE) framework, which formulates open-ended objectives in cooperative games with two players using perspectives of graph th
    
[^116]: 低资源语音翻译的跨语言迁移学习

    Cross-Lingual Transfer Learning for Low-Resource Speech Translation. (arXiv:2306.00789v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.00789](http://arxiv.org/abs/2306.00789)

    提出了一种三步跨语言迁移学习框架，通过在现有框架中增加一步语义知识蒸馏，该方法有效地增强了自动语音翻译中从高资源语言到低资源语言的跨语言迁移能力，显著改善了翻译性能，特别是对于低资源语言，并减少了跨语言迁移间隙(TRFGap)。

    

    本文提出了一种新颖的三步跨语言迁移学习框架，用于增强自动语音翻译中从高资源语言到低资源语言的跨语言迁移能力。该方法将语义知识蒸馏步骤集成到现有的两步跨语言迁移学习框架XLS-R中。这一额外的步骤旨在通过使用无标签语音进行自监督学习来对多语言语音编码器进行预训练以编码语义知识。我们提出的三步跨语言迁移学习框架解决了XLS-R框架中高资源语言和低资源语言之间存在的大的跨语言迁移差距。我们通过在CoVoST-2基准测试上进行广泛实验和比较来验证我们的提议，结果显示在翻译性能方面取得了显著改进，特别是对于低资源语言，并且跨语言迁移间隙(TRFGap)有明显减少。

    The paper presents a novel three-step transfer learning framework for enhancing cross-lingual transfer from high- to low-resource languages in the downstream application of Automatic Speech Translation. The approach integrates a semantic knowledge-distillation step into the existing two-step cross-lingual transfer learning framework XLS-R. This extra step aims to encode semantic knowledge in the multilingual speech encoder pre-trained via Self-Supervised Learning using unlabeled speech. Our proposed three-step cross-lingual transfer learning framework addresses the large cross-lingual transfer gap (TRFGap) observed in the XLS-R framework between high-resource and low-resource languages. We validate our proposal through extensive experiments and comparisons on the CoVoST-2 benchmark, showing significant improvements in translation performance, especially for low-resource languages, and a notable reduction in the TRFGap.
    
[^117]: YOLOv8实时心律失常检测的新应用

    A novel application for real-time arrhythmia detection using YOLOv8. (arXiv:2305.16727v1 [cs.CV])

    [http://arxiv.org/abs/2305.16727](http://arxiv.org/abs/2305.16727)

    本文提出了一种使用YOLOv8算法进行心律失常检测的新应用程序，其模型能够实现持续监测，并以高准确性进行实时心律失常检测。

    

    近年来，降低远程心血管健康监护的医疗费用需求越来越高。检测和分类心脏心律失常对于诊断心脏异常患者至关重要。本文提出了一种新的应用程序，利用最先进的You-Only-Look-Once（YOLO）v8算法对单导联心电图信号进行分类，进行心律失常检测。通过对MIT-BIH数据集进行微调后，建立起一个定制的YOLOv8模型，能够实时检测心律失常，以实现持续监测。结果表明，在NVIDIA Tesla V100上，我们的模型能够以0.002秒的检测时间和0.961的mAP@50检测心跳。研究证明了实时心律失常检测的潜力，模型输出可以被视觉解释，适用于家庭用户。此外，该研究可以延伸到开发实时的可解释人工智能模型，应用于心血管健康监护。

    In recent years, there has been an increasing need to reduce healthcare costs in remote monitoring of cardiovascular health. Detecting and classifying cardiac arrhythmia is critical to diagnosing patients with cardiac abnormalities. This paper shows that complex systems such as electrocardiograms (ECG) can be applicable for at-home monitoring. This paper proposes a novel application for arrhythmia detection using the state-of-the-art You-Only-Look-Once (YOLO)v8 algorithm to classify single-lead ECG signals. A custom YOLOv8 model was fine-tuned on the MIT-BIH dataset to detect arrhythmia in real-time to allow continuous monitoring. Results show that our model can detect heartbeats with a mAP@50 of 0.961 with a detection time of 0.002s on an NVIDIA Tesla V100. Our study demonstrated the potential of real-time arrhythmia detection, where the model output can be visually interpreted for at-home users. Furthermore, this study could be extended into a real-time XAI model, deployed in the hea
    
[^118]: AlpacaFarm: 一种从人类反馈中学习的方法模拟框架

    AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])

    [http://arxiv.org/abs/2305.14387](http://arxiv.org/abs/2305.14387)

    该论文提出了一种名为AlpacaFarm的低成本模拟器，该模拟器为从人类反馈中学习的研究和开发提供了一种解决方案，通过设计LLM提示来模拟人类反馈，提出自动评估并提供参考实现，克服了数据收集的高昂成本、缺乏可信的评估和缺乏参考方法实现的挑战。

    

    大型语言模型（LLMs）如ChatGPT因其良好的指令跟随能力而得到了广泛应用。开发这些LLMs需要使用人类反馈进行训练的复杂且尚不明确的工作流程。将此指令跟随过程复制和理解面临三大挑战： 数据收集的高昂成本，缺乏可信的评估和缺乏参考方法实现。我们通过AlpacaFarm解决了这些挑战，这是一个低成本的模拟器，可用于从反馈中学习的研究和开发。第一，我们设计了LLM提示来模拟人类反馈，其成本比众包工作者便宜45倍，并且与人类反馈具有高度一致性。第二，我们提出了一种自动评估方法，并将其与真实世界交互中获得的人类指令进行验证。第三，我们为几种从配对反馈中学习的方法（PPO，best-of-n，expert iteration等）提供了参考实现。

    Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback
    
[^119]: 非平坦ABA是双极论证的实例

    Non-flat ABA is an Instance of Bipolar Argumentation. (arXiv:2305.12453v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.12453](http://arxiv.org/abs/2305.12453)

    本文研究了基于假设的论证（ABA）的一个限制，即只能假设而不能推导的问题，提出了一种新颖的双极论证框架（BAFs），可以实例化一般的ABA框架，并证明了它们之间的关系。

    

    基于假设的论证（ABA）是一个广为人知的结构化论证形式，其中论证和它们之间的攻击是基于规则、可废弃的假设及其相反的观点。对ABA框架（ABAFs）施加的一个常见限制是它们是平坦的，即每个可废弃的假设只能被假设，而不能被推导出来。尽管已知平坦的ABAFs可以被转化为Dung提出的抽象论证框架（AFs），但不存在从一般的、可能非平坦的ABAFs到任何类型的抽象论证形式的转换。在本文中，我们填补了这个空白，并展示了双极AFs（BAFs）可以实例化一般的ABAFs。为此，我们开发了合适的、新颖的BAF语义，借鉴了推导支持的概念。我们研究了我们的BAFs的基本属性，包括计算复杂性，并在几种语义下证明了与ABAFs的期望关系。最后，为了支持计算和可解释性，我们提出了一种模型转换方法。

    Assumption-based Argumentation (ABA) is a well-known structured argumentation formalism, whereby arguments and attacks between them are drawn from rules, defeasible assumptions and their contraries. A common restriction imposed on ABA frameworks (ABAFs) is that they are flat, i.e., each of the defeasible assumptions can only be assumed, but not derived. While it is known that flat ABAFs can be translated into abstract argumentation frameworks (AFs) as proposed by Dung, no translation exists from general, possibly non-flat ABAFs into any kind of abstract argumentation formalism. In this paper, we close this gap and show that bipolar AFs (BAFs) can instantiate general ABAFs. To this end we develop suitable, novel BAF semantics which borrow from the notion of deductive support. We investigate basic properties of our BAFs, including computational complexity, and prove the desired relation to ABAFs under several semantics. Finally, in order to support computation and explainability, we prop
    
[^120]: 语言模型能否用自然语言解决图问题？

    Can Language Models Solve Graph Problems in Natural Language?. (arXiv:2305.10037v1 [cs.CL])

    [http://arxiv.org/abs/2305.10037](http://arxiv.org/abs/2305.10037)

    本论文提出了自然语言图形(NLGraph)，这是一个全面的基于图形问题解决测试，旨在评估LLM在文本描述的图形结构和图形解决方案方面的处理能力。实验结果表明，LLM(GPT-3/4)具有相应的图形推理能力。

    

    大型语言模型越来越多地应用于一些具有隐式图形结构的任务，例如机器人规划、多跳问题回答或知识探索、结构化常识推理等等。虽然LLM在这些任务中已经取得了一定的进展，但是LLM是否能够显式处理图形的文本描述，将它们映射到基于概念的空间中，并执行结构化操作仍然尚未得到足够的研究。为此，我们提出了自然语言图形(NLGraph)，它是一个设计用于自然语言的基于图形问题解决全面测试。NLGraph包含29,370个问题，涵盖了八个图形推理任务，从简单的连接和最短路径到复杂的最大流和模拟图神经网络等任务不等。我们在NLGraph基准测试上评估了LLM(GPT-3/4)，并发现1)语言模型具有相应的图形推理能力；

    Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language
    
[^121]: 混合问题解析与执行答案复杂问题的文字问答系统

    Answering Complex Questions over Text by Hybrid Question Parsing and Execution. (arXiv:2305.07789v1 [cs.CL])

    [http://arxiv.org/abs/2305.07789](http://arxiv.org/abs/2305.07789)

    提出了一种混合问题解析和执行框架，在文字问答系统中实现回答复杂问题，通过解析问题为H表达式并设计混合执行器实现。在基准数据集中实现了最先进的准确率和效率表现。

    

    文本问答系统的主导模式是基于端到端的神经网络，其在回答自然语言问题方面表现突出，但在回答复杂问题方面表现不足。这与基于语义解析的方法在结构化数据源（如关系数据库、知识图谱）上广泛适应形成对比，后者将自然语言问题转换为逻辑形式，并利用查询引擎进行执行。为了结合神经和符号方法的优势，我们提出了一种在文本问答系统中进行解析和执行问题的框架。它包括两个中心支柱：（1）我们将各种复杂问题解析成中间表示，称为H表达式，它由简单问题组成原语和表示它们之间关系的符号操作组成；（2）为了执行产生的H表达式，我们设计了一个混合执行器，它集成了确定规则来翻译符号操作，与处理原始问题的插入神经模块。我们在包含复杂问题的大规模基准数据集上评估了我们的方法，并在准确性和效率指标方面取得了最先进的表现。

    The dominant paradigm of textual question answering systems is based on end-to-end neural networks, which excels at answering natural language questions but falls short on complex ones. This stands in contrast to the broad adaptation of semantic parsing approaches over structured data sources (e.g., relational database, knowledge graphs), that convert natural language questions to logical forms and execute them with query engines. Towards combining the strengths of neural and symbolic methods, we propose a framework of question parsing and execution on textual QA. It comprises two central pillars: (1) We parse the question of varying complexity into an intermediate representation, named H-expression, which is composed of simple questions as the primitives and symbolic operations representing the relationships among them; (2) To execute the resulting H-expressions, we design a hybrid executor, which integrates the deterministic rules to translate the symbolic operations with a drop-in n
    
[^122]: 使用数据内核比较基础模型

    Comparing Foundation Models using Data Kernels. (arXiv:2305.05126v1 [cs.LG])

    [http://arxiv.org/abs/2305.05126](http://arxiv.org/abs/2305.05126)

    本文采用基于数据内核的方法比较基础模型，不受度量指标的约束，通过嵌入空间几何实现点对点和多模型比较，并成功诱导了一组与下游指标强相关的模型距离函数流形。

    

    最近自主学习和神经网络扩展的进展使得可以创建大型基础模型，这些模型可以轻松地适应各种下游任务。目前比较基础模型的范式涉及在各种策划数据集上使用聚合指标进行基准测试。不幸的是，这种模型比较方法严重依赖于度量指标的选择，这使得它在理想度量不明显或不可用的情况下不适用。在这项工作中，我们提出了一种没有度量指标的基础模型比较方法，通过它们的嵌入空间几何来实现。我们的方法基于随机图理论，并促进点对点和多模型比较。此外，我们展示了如何使用我们的框架诱导一组配备有与一些下游指标强相关的距离函数的模型流形。

    Recent advances in self-supervised learning and neural network scaling have enabled the creation of large models -- known as foundation models -- which can be easily adapted to a wide range of downstream tasks. The current paradigm for comparing foundation models involves benchmarking them with aggregate metrics on various curated datasets. Unfortunately, this method of model comparison is heavily dependent on the choice of metric, which makes it unsuitable for situations where the ideal metric is either not obvious or unavailable. In this work, we present a metric-free methodology for comparing foundation models via their embedding space geometry. Our methodology is grounded in random graph theory, and facilitates both pointwise and multi-model comparison. Further, we demonstrate how our framework can be used to induce a manifold of models equipped with a distance function that correlates strongly with several downstream metrics.
    
[^123]: DroidBot-GPT：基于GPT的Android UI自动化

    DroidBot-GPT: GPT-powered UI Automation for Android. (arXiv:2304.07061v1 [cs.SE])

    [http://arxiv.org/abs/2304.07061](http://arxiv.org/abs/2304.07061)

    DroidBot-GPT是一款利用GPT模型自动化Android应用程序的工具，可以根据任务的自然语言描述自动生成并执行操作，有望提高移动应用程序的测试和开发效率。

    

    本文介绍了DroidBot-GPT，这是一种利用类似GPT的大型语言模型（LLM）自动化与Android移动应用程序交互的工具。给定所需任务的自然语言描述，DroidBot-GPT可以自动生成并执行操作，导航应用程序以完成任务。它通过将应用程序GUI状态信息和智能手机屏幕上可用的操作转换为自然语言提示，并要求LLM选择动作来实现。由于LLM通常受过大量数据的训练，包括各种软件应用程序的操作指南，因此它具有根据提供的信息作出合理动作选择的能力。我们使用了一个自创建的数据集对DroidBot-GPT进行评估，该数据集包含来自10个类别的17个Android应用程序的33个任务。它可以成功完成39.39%的任务，并且平均部分完成进度约为66.76%。鉴于我们的方法是完全自动的，并且用于训练LLM的数据是广泛可用的，我们认为DroidBot-GPT在改善移动应用程序的测试和开发效率方面具有巨大潜力。

    This paper introduces DroidBot-GPT, a tool that utilizes GPT-like large language models (LLMs) to automate the interactions with Android mobile applications. Given a natural language description of a desired task, DroidBot-GPT can automatically generate and execute actions that navigate the app to complete the task. It works by translating the app GUI state information and the available actions on the smartphone screen to natural language prompts and asking the LLM to make a choice of actions. Since the LLM is typically trained on a large amount of data including the how-to manuals of diverse software applications, it has the ability to make reasonable choices of actions based on the provided information. We evaluate DroidBot-GPT with a self-created dataset that contains 33 tasks collected from 17 Android applications spanning 10 categories. It can successfully complete 39.39% of the tasks, and the average partial completion progress is about 66.76%. Given the fact that our method is f
    
[^124]: 基于Transformer内在补丁的对比学习方案

    A Contrastive Learning Scheme with Transformer Innate Patches. (arXiv:2303.14806v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.14806](http://arxiv.org/abs/2303.14806)

    本文提出了一种使用Transformer内在补丁的对比学习方案，可在密集下游预测任务中受益。该方案适用于所有视觉Transformer架构，易于实现，并且无需大规模批处理。对比Transformer方案在航空图像分割中有效。

    

    本文提出了一种使用Transformer内在补丁的对比学习方案，称为对比Transformer。对比Transformer能够使得现有的对比学习技术在密集下游预测任务（如语义分割）中受益。该方案通过监督补丁级对比学习，在选择补丁时基于地面真值掩膜，后续用于难负样本和难正样本采样。该方案适用于所有视觉Transformer架构，易于实现，并且引入的额外内存开销很小。此外，该方案无需大规模批处理，因为每个补丁被视为一张图像。我们将对比Transformer应用于航空图像分割的案例中，这种情况下存在低分辨率数据、大类别不平衡和相似语义类别。我们进行了大量实验证明了对比Transformer方案的有效性。

    This paper presents Contrastive Transformer, a contrastive learning scheme using the Transformer innate patches. Contrastive Transformer enables existing contrastive learning techniques, often used for image classification, to benefit dense downstream prediction tasks such as semantic segmentation. The scheme performs supervised patch-level contrastive learning, selecting the patches based on the ground truth mask, subsequently used for hard-negative and hard-positive sampling. The scheme applies to all vision-transformer architectures, is easy to implement, and introduces minimal additional memory footprint. Additionally, the scheme removes the need for huge batch sizes, as each patch is treated as an image.  We apply and test Contrastive Transformer for the case of aerial image segmentation, known for low-resolution data, large class imbalance, and similar semantic classes. We perform extensive experiments to show the efficacy of the Contrastive Transformer scheme on the ISPRS Potsda
    
[^125]: 神经网络中的因果归因学习：超越直接影响

    Learning Causal Attributions in Neural Networks: Beyond Direct Effects. (arXiv:2303.13850v1 [cs.LG])

    [http://arxiv.org/abs/2303.13850](http://arxiv.org/abs/2303.13850)

    本文提出了一种方法来估计和维护神经网络模型中输入-输出属性的因果关系，不仅考虑直接影响，还能考虑间接影响。此方法能够在训练神经网络模型的同时有效地量化因果归因，实验结果表明该方法能够有效地学习因果归因。

    

    近年来，捕捉和维护神经网络模型中的因果关系引起了越来越多的关注。本文研究了用因果方法估计和维护神经网络模型中的输入-输出属性。特别地，现有的研究仅假设输入变量独立（由于神经网络架构），因此仅研究直接影响。我们将神经网络视为结构性因果模型，并提出在输入特征中引入边缘以捕捉和维护直接和间接因果效应的简单而有效的方法，同时训练神经网络模型。我们还提出有效的近似策略来量化高维数据的因果归因。我们在合成和真实数据集上进行了各种实验，结果表明，所提出的方法学习了接近基本事实效果的直接和间接因果归因。

    There has been a growing interest in capturing and maintaining causal relationships in Neural Network (NN) models in recent years. We study causal approaches to estimate and maintain input-output attributions in NN models in this work. In particular, existing efforts in this direction assume independence among input variables (by virtue of the NN architecture), and hence study only direct causal effects. Viewing an NN as a structural causal model (SCM), we instead focus on going beyond direct effects, introduce edges among input features, and provide a simple yet effective methodology to capture and maintain direct and indirect causal effects while training an NN model. We also propose effective approximation strategies to quantify causal attributions in high dimensional data. Our wide range of experiments on synthetic and real-world datasets show that the proposed ante-hoc method learns causal attributions for both direct and indirect causal effects close to the ground truth effects.
    
[^126]: Heckerthoughts.（arXiv:2302.05449v4 [cs.AI] UPDATED）

    Heckerthoughts. (arXiv:2302.05449v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.05449](http://arxiv.org/abs/2302.05449)

    这本论文是作者在斯坦福大学和微软研究院工作经验的技术回忆录，涉及了机器学习和人工智能的基本概念、应用以及创造过程中的故事。

    

    本文是关于我在斯坦福大学和微软研究院工作的技术回忆录。包括了与机器学习和人工智能相关的基本概念，这些概念的应用以及其创造背后的故事。

    This manuscript is technical memoir about my work at Stanford and Microsoft Research. Included are fundamental concepts central to machine learning and artificial intelligence, applications of these concepts, and stories behind their creation.
    
[^127]: 通过风险分解评估自监督学习

    Evaluating Self-Supervised Learning via Risk Decomposition. (arXiv:2302.03068v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03068](http://arxiv.org/abs/2302.03068)

    通过风险分解，提出四个误差部分评估自监督学习对169个视觉模型的影响，为SSL的设计和使用提供宝贵的见解。

    

    自监督学习（SSL）的流程设计涉及架构、增强和预训练数据等诸多选择。然而，SSL通常使用单一度量来评估，这并不能提供深入的洞察和改进方案。为解决这些问题，我们提出了一个SSL风险分解，从逼近、表示可用性、探针泛化和编码器泛化等角度对错误进行分解。我们分析了30个设计选择对169个在ImageNet上评估的SSL视觉模型的影响，并为每个组件提供了高效的估计器，为SSL模型的设计和使用提供宝贵的见解。

    Self-supervised learning (SSL) pipelines differ in many design choices such as the architecture, augmentations, or pretraining data. Yet SSL is typically evaluated using a single metric: linear probing on ImageNet. This does not provide much insight into why or when a model is better, now how to improve it. To address this, we propose an SSL risk decomposition, which generalizes the classical supervised approximation-estimation decomposition by considering errors arising from the representation learning step. Our decomposition consists of four error components: approximation, representation usability, probe generalization, and encoder generalization. We provide efficient estimators for each component and use them to analyze the effect of 30 design choices on 169 SSL vision models evaluated on ImageNet. Our analysis gives valuable insights for designing and using SSL models. For example, it highlights the main sources of error and shows how to improve SSL in specific settings (full- vs 
    
[^128]: LEXTREME：多语言和多任务的法律领域基准

    LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain. (arXiv:2301.13126v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.13126](http://arxiv.org/abs/2301.13126)

    LEXTREME是一个多语言和多任务的法律领域基准，该基准提供了11个数据集涵盖24种语言的测评，最佳模型（XLM-R large）在数据集和语言综合评分上均达到了61.3。这使得LEXTREME仍然具有挑战性并且有改进空间。

    

    最近，在transformer架构的显著进展推动下，法律自然语言处理领域取得了惊人的增长。为了衡量进展，精心策划和具有挑战性的基准是至关重要的。然而，大多数基准只能处理英文，而在法律自然语言处理方面尚未有多语言基准可用。此外，许多基准已经饱和，最佳模型明显优于最佳人类，并达到近乎完美的分数。我们调查了法律自然语言处理文献，并选择了11个涵盖24种语言的数据集，创建了LEXTREME。为了进行公平比较，我们提出了两种综合评分，一种基于数据集，一种基于语言。最佳基线模型（XLM-R large）在数据集综合评分和语言综合评分上均达到了61.3。这表明LEXTREME仍然非常具有挑战性，并且为改进留下了充足空间。为了方便研究人员和实践者使用，我们将LEXTREME与所有数据一起发布在huggingface上。

    Lately, propelled by the phenomenal advances around the transformer architecture, the legal NLP field has enjoyed spectacular growth. To measure progress, well curated and challenging benchmarks are crucial. However, most benchmarks are English only and in legal NLP specifically there is no multilingual benchmark available yet. Additionally, many benchmarks are saturated, with the best models clearly outperforming the best humans and achieving near perfect scores. We survey the legal NLP literature and select 11 datasets covering 24 languages, creating LEXTREME. To provide a fair comparison, we propose two aggregate scores, one based on the datasets and one on the languages. The best baseline (XLM-R large) achieves both a dataset aggregate score a language aggregate score of 61.3. This indicates that LEXTREME is still very challenging and leaves ample room for improvement. To make it easy for researchers and practitioners to use, we release LEXTREME on huggingface together with all the
    
[^129]: 压缩、泛化和学习

    Compression, Generalization and Learning. (arXiv:2301.12767v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12767](http://arxiv.org/abs/2301.12767)

    本文提出了一种新的理论，允许在压缩的改变概率上保持控制，并获得了紧密的有限样本边界来评估压缩的改变概率。这对学习应用中的错误分类和错误预测具有重要意义。

    

    压缩函数是一种将观测集缩小为尺寸减小的子集的映射，同时保留其信息内容。在多个应用中，新观测使压缩集发生变化的条件被解释为新观测带来了额外的信息，在学习理论中，这对应于错误分类或错误预测。本文建立了一个新理论的基础，允许在压缩的改变概率上保持控制（与学习应用中的统计“风险”相对应）。在适当的条件下，压缩集的基数被证明是压缩的改变概率的一致估计量（不对压缩集的尺寸设置上限）；此外，在普遍适用的偏好条件下获得了前所未有的紧密的有限样本边界来评估压缩的改变概率。所有结果都可以在完全应用中使用。

    A compression function is a map that slims down an observational set into a subset of reduced size, while preserving its informational content. In multiple applications, the condition that one new observation makes the compressed set change is interpreted that this observation brings in extra information and, in learning theory, this corresponds to misclassification, or misprediction. In this paper, we lay the foundations of a new theory that allows one to keep control on the probability of change of compression (which maps into the statistical "risk" in learning applications). Under suitable conditions, the cardinality of the compressed set is shown to be a consistent estimator of the probability of change of compression (without any upper limit on the size of the compressed set); moreover, unprecedentedly tight finite-sample bounds to evaluate the probability of change of compression are obtained under a generally applicable condition of preference. All results are usable in a fully 
    
[^130]: 特征归因的不可能定理

    Impossibility Theorems for Feature Attribution. (arXiv:2212.11870v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11870](http://arxiv.org/abs/2212.11870)

    本文阐述了对于中等规模的模型类，任何完整的线性特征归因方法都无法胜任推断模型行为的改进，启示我们在具体定义最终任务的重要性方面要汲取经验，采用重复模型评估的简单直接方法可以胜过许多其他复杂的特征归因方法。

    

    尽管有许多可产生合理解释的可解释性方法，但该领域也经验性地看到了许多失败案例。鉴于这些结果，对于实践者如何以原则性方式使用这些方法并在它们之间进行选择仍不清楚。在本文中，我们展示了对于中等规模的模型类（神经网络容易满足），任何完整的线性特征归因方法（例如Integrated Gradients和SHAP）可以被证明对于推断模型行为的改进都无法胜任。我们的结果适用于常见的最终任务，如描述局部模型行为、识别虚假特征和算法回溯。我们工作的一个重要启示是具体定义最终任务的重要性：一旦这样的最终任务被定义，一个简单和直接的方法——重复模型评估——可以胜过许多其他复杂的特征归因方法。

    Despite a sea of interpretability methods that can produce plausible explanations, the field has also empirically seen many failure cases of such methods. In light of these results, it remains unclear for practitioners how to use these methods and choose between them in a principled way. In this paper, we show that for moderately rich model classes (easily satisfied by neural networks), any feature attribution method that is complete and linear -- for example, Integrated Gradients and SHAP -- can provably fail to improve on random guessing for inferring model behaviour. Our results apply to common end-tasks such as characterizing local model behaviour, identifying spurious features, and algorithmic recourse. One takeaway from our work is the importance of concretely defining end-tasks: once such an end-task is defined, a simple and direct approach of repeated model evaluations can outperform many other complex feature attribution methods.
    
[^131]: 重新审视基于颜色事件的跟踪：一个统一的网络、数据集和度量

    Revisiting Color-Event based Tracking: A Unified Network, Dataset, and Metric. (arXiv:2211.11010v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.11010](http://arxiv.org/abs/2211.11010)

    本文提出了一个用于颜色-事件统一跟踪的单阶段骨干网络（CEUTrack），通过将事件点和RGB帧转换为体素，将裁剪后的模板和搜索区域投影到令牌中，并通过统一的Transformer骨干网络实现了目标对象的定位。这一方法简单、有效且高效。

    

    近年来，将颜色和事件相机（也称为动态视觉传感器，DVS）结合起来进行鲁棒的目标跟踪是一个新兴的研究课题。现有的颜色-事件跟踪框架通常包含多个离散的模块，可能导致低效率和高计算复杂度，包括特征提取、融合、匹配、交互学习等。本文提出了一个用于颜色-事件统一跟踪（CEUTrack）的单阶段骨干网络，该网络同时实现了上述功能。给定事件点和RGB帧，我们首先将点转换为体素，并分别裁剪模板和搜索区域的两种模态。然后，这些区域被投影到令牌中，并并行输入到统一的Transformer骨干网络。输出特征将被输入到跟踪头部进行目标对象定位。我们提出的CEUTrack简单、有效、高效，达到了超过75 FPS和新的SOTA性能。

    Combining the Color and Event cameras (also called Dynamic Vision Sensors, DVS) for robust object tracking is a newly emerging research topic in recent years. Existing color-event tracking framework usually contains multiple scattered modules which may lead to low efficiency and high computational complexity, including feature extraction, fusion, matching, interactive learning, etc. In this paper, we propose a single-stage backbone network for Color-Event Unified Tracking (CEUTrack), which achieves the above functions simultaneously. Given the event points and RGB frames, we first transform the points into voxels and crop the template and search regions for both modalities, respectively. Then, these regions are projected into tokens and parallelly fed into the unified Transformer backbone network. The output features will be fed into a tracking head for target object localization. Our proposed CEUTrack is simple, effective, and efficient, which achieves over 75 FPS and new SOTA perform
    
[^132]: NESTER：一种自适应的神经符号化方法进行治疗效果评估

    NESTER: An Adaptive Neurosymbolic Method for Treatment Effect Estimation. (arXiv:2211.04370v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.04370](http://arxiv.org/abs/2211.04370)

    NESTER是一种自适应的神经符号化方法进行治疗效果评估，将治疗效果估计的所有要求集成到一个框架中，该方法比现有最先进的方法在多个基准数据集上性能更好。

    

    从观测数据中进行治疗效果评估是因果推断中的一个核心问题。基于潜在结果框架的方法通过利用因果推断中的归纳偏置和启发式方法来解决这个问题。每种现有的技术都通过设计神经网络架构和正则化器来解决治疗效果评估的特定方面，例如控制倾向得分、强制随机化等。在本文中，我们提出了一种自适应方法，称为神经符号治疗效果估计器（NESTER），它是一种治疗效果评估的通用方法。NESTER将治疗效果估计的所有要求集成到一个框架中。为此，我们设计了一个基于文献中使用的归纳偏置的治疗效果估计的领域特定语言（DSL）。我们还在理论上研究了NESTER在治疗效果估计任务中的能力。我们全面的实证结果表明，与现有的最先进方法相比，NESTER在多个基准数据集上的效果更好。

    Treatment effect estimation from observational data is a central problem in causal inference. Methods based on potential outcomes framework solve this problem by exploiting inductive biases and heuristics from causal inference. Each existing technique addresses a specific aspect of treatment effect estimation, such as controlling propensity score, enforcing randomization, etc., by designing neural network architectures and regularizers. In this paper, we propose an adaptive method called Neurosymbolic Treatment Effect Estimator (NESTER), a generalized method for treatment effect estimation. NESTER brings together all the desiderata for treatment effect estimation into one framework. For this purpose, we design a Domain Specific Language (DSL) for the treatment effect estimation based on inductive biases used in literature. We also theoretically study NESTER's capability for the treatment effect estimation task. Our comprehensive empirical results show that NESTER performs better on ben
    
[^133]: DGPO: 使用多样化策略优化发现多种解决方案

    DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization. (arXiv:2207.05631v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.05631](http://arxiv.org/abs/2207.05631)

    这篇论文提出了一个名为DGPO的算法，可以在解决任务时发现多种策略，从而提高策略鲁棒性和与用户交互的乐趣。

    

    大多数强化学习算法都试图寻找解决给定任务的单个最佳策略。然而，学习多种解决方案通常是有价值的，例如，使智能体与用户的交互更加有趣，或者提高策略对意外干扰的鲁棒性。我们提出了一种名为多样化策略优化（DGPO）的在线算法，用于发现解决给定任务的多种策略。与现有工作不同的是，它通过在单次运行中训练共享策略网络实现此目的。具体而言，我们设计了一种基于信息理论多样性目标的内在奖励。我们的最终目标交替约束策略多样性和外在奖励。我们将约束优化问题转化为概率推断任务，并使用策略迭代来最大化得到的下界。实验结果表明，我们的方法能够在各种环境中有效地发现多样化的策略，包括 Atari 游戏和 Mujoco 模拟器，并且能够提供一系列性能和多样性之间的权衡。

    Most reinforcement learning algorithms seek a single optimal strategy that solves a given task. However, it can often be valuable to learn a diverse set of solutions, for instance, to make an agent's interaction with users more engaging, or improve the robustness of a policy to an unexpected perturbance. We propose Diversity-Guided Policy Optimization (DGPO), an on-policy algorithm that discovers multiple strategies for solving a given task. Unlike prior work, it achieves this with a shared policy network trained over a single run. Specifically, we design an intrinsic reward based on an information-theoretic diversity objective. Our final objective alternately constraints on the diversity of the strategies and on the extrinsic reward. We solve the constrained optimization problem by casting it as a probabilistic inference task and use policy iteration to maximize the derived lower bound. Experimental results show that our method efficiently discovers diverse strategies in a wide variet
    
[^134]: 从归因图到可理解的人类解释：通过概念相关传播

    From Attribution Maps to Human-Understandable Explanations through Concept Relevance Propagation. (arXiv:2206.03208v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03208](http://arxiv.org/abs/2206.03208)

    本论文介绍了一种概念相关传播（CRP）方法，将局部和全局观点结合起来，为个别预测提供了“何地”和“何物”两个问题的解答。该方法提供了更人类可解释的解释，并通过概念图谱深入了解模型的表示和推理能力。

    

    可解释的人工智能（XAI）领域旨在使当今强大但不透明的深度学习模型变得透明。而局部的XAI方法通过归因图解释个别预测，从而确定重要特征出现的位置（但不提供有关它们代表什么的信息），而全局解释技术则可视化模型通常学习编码的概念。因此，这两种方法只提供了部分洞察力，并将解释模型的负担留给用户。在这项工作中，我们介绍了概念相关传播（CRP）方法，它结合了局部和全局的观点，从而能够回答个别预测的“何地”和“何物”的问题。我们展示了我们方法在各种设置中的能力，展示了CRP如何提供更具人类解释性的解释，并通过概念图谱深入了解模型的表示和推理能力。

    The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to today's powerful but opaque deep learning models. While local XAI methods explain individual predictions in form of attribution maps, thereby identifying where important features occur (but not providing information about what they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of methods thus only provide partial insights and leave the burden of interpreting the model's reasoning to the user. In this work we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives and thus allows answering both the "where" and "what" questions for individual predictions. We demonstrate the capability of our method in various settings, showcasing that CRP leads to more human interpretable explanations and provides deep insights into the model's representation and reasoning through concept atlases, 
    
[^135]: 开放词汇的脑电图-文本解码与零-shot情感分类

    Open Vocabulary Electroencephalography-To-Text Decoding and Zero-shot Sentiment Classification. (arXiv:2112.02690v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2112.02690](http://arxiv.org/abs/2112.02690)

    本文拓展了脑电图-文本解码的问题，实现了开放词汇的解码和零-shot情感分类。通过利用预训练的语言模型，我们的模型在解码和情感分类任务上明显优于现有方法。

    

    最新的脑电图-文本系统使用神经网络从脑信号中直接解码语言取得了巨大成功。然而，当前的方法仅限于小型封闭的词汇表，这对于自然交流来说远远不够。此外，大多数高性能方法需要来自侵入性设备（如ECoG）的数据。本文将问题扩展到开放词汇的脑电图-文本序列到序列解码和自然阅读任务的零-shot句子情感分类。我们假设人脑可以作为一个特殊的文本编码器，并提出了一个新颖的框架，利用预训练的语言模型（如BART）。我们的模型在脑电图-文本解码方面取得了40.1%的BLEU-1得分，在基于脑电图的零-shot三元情感分类方面取得了55.6%的F1得分，显著优于有监督的基线模型。此外，我们展示了我们提出的模型可以处理来自不同被试和来源的数据。

    State-of-the-art brain-to-text systems have achieved great success in decoding language directly from brain signals using neural networks. However, current approaches are limited to small closed vocabularies which are far from enough for natural communication. In addition, most of the high-performing approaches require data from invasive devices (e.g., ECoG). In this paper, we extend the problem to open vocabulary Electroencephalography(EEG)-To-Text Sequence-To-Sequence decoding and zero-shot sentence sentiment classification on natural reading tasks. We hypothesis that the human brain functions as a special text encoder and propose a novel framework leveraging pre-trained language models (e.g., BART). Our model achieves a 40.1% BLEU-1 score on EEG-To-Text decoding and a 55.6% F1 score on zero-shot EEG-based ternary sentiment classification, which significantly outperforms supervised baselines. Furthermore, we show that our proposed model can handle data from various subjects and sourc
    
[^136]: 评估具有不准确地实际标签的方法的逻辑评估公式及其原则

    Logical Assessment Formula and Its Principles for Evaluations with Inaccurate Ground-Truth Labels. (arXiv:2110.11567v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2110.11567](http://arxiv.org/abs/2110.11567)

    我们提出了逻辑评估公式（LAF），通过逻辑推理揭示了其在具有不准确实际标签的评估中的原则。LAF可以在困难的任务中应用于具有不准确实际标签的评估，并合理工作；也可以从逻辑角度应用于简单的任务中，但无法像常规策略一样自信工作。

    

    在人工智能应用的预测模型评估中，常常使用具有准确实际标签的评估来进行。然而，在某些特定领域，例如医学组织病理学全切片图像分析中，准确实际标签的定义往往困难，甚至不存在。为了缓解这种情况，我们提出了逻辑评估公式（LAF），并通过不确定状态下的逻辑推理揭示了其在具有不准确实际标签的评估中的原则。根据LAF的原则，我们总结了LAF的实用性：1）LAF可应用于更困难的、具有不准确实际标签的评估任务中，并能像常规策略一样合理地工作；2）LAF可从逻辑角度应用于更简单的、具有不准确实际标签的评估任务中，但无法像常规策略一样自信地工作。

    Evaluations with accurate ground-truth labels (AGTLs) have been widely employed to assess predictive models for artificial intelligence applications. However, in some specific fields, such as medical histopathology whole slide image analysis, it is quite usual the situation that AGTLs are difficult to be precisely defined or even do not exist. To alleviate this situation, we propose logical assessment formula (LAF) and reveal its principles for evaluations with inaccurate ground-truth labels (IAGTLs) via logical reasoning under uncertainty. From the revealed principles of LAF, we summarize the practicability of LAF: 1) LAF can be applied for evaluations with IAGTLs on a more difficult task, able to act like usual strategies for evaluations with AGTLs reasonably; 2) LAF can be applied for evaluations with IAGTLs from the logical perspective on an easier task, unable to act like usual strategies for evaluations with AGTLs confidently.
    
[^137]: INVIGORATE: 互动视觉场景理解和抓取

    INVIGORATE: Interactive Visual Grounding and Grasping in Clutter. (arXiv:2108.11092v1 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2108.11092](http://arxiv.org/abs/2108.11092)

    INVIGORATE是一个通过自然语言与人类进行交互并在杂乱环境中抓取指定物体的机器人系统。该系统能够推断目标物体、推断物体阻挡关系，并生成多步计划来消除歧义并成功抓取目标物体。

    

    本文介绍了INVIGORATE，一个通过自然语言与人类进行交互并在杂乱环境中抓取指定物体的机器人系统。在这种杂乱环境中，物体可能会相互遮挡、阻挡甚至叠放在一起。INVIGORATE面临着几个挑战：（i）从输入的语言表达和RGB图像中推断出目标物体，而忽略其他遮挡物体；（ii）从图像中推断出物体的阻挡关系（OBR）；（iii）生成一个多步计划，通过提问消除目标物体的歧义并成功抓取它。我们为目标检测、视觉场景理解、问题生成和OBR检测以及抓取训练了独立的神经网络。它们可以处理任意的物体类别和语言表达，只要有相应的训练数据集。然而，视觉感知中的误差以及人类语言中的歧义不可避免地会对机器人的性能产生负面影响。为了克服这些不确定性，我们建立了一个部分可观察的马尔可夫决策模型。

    This paper presents INVIGORATE, a robot system that interacts with human through natural language and grasps a specified object in clutter. The objects may occlude, obstruct, or even stack on top of one another. INVIGORATE embodies several challenges: (i) infer the target object among other occluding objects, from input language expressions and RGB images, (ii) infer object blocking relationships (OBRs) from the images, and (iii) synthesize a multi-step plan to ask questions that disambiguate the target object and to grasp it successfully. We train separate neural networks for object detection, for visual grounding, for question generation, and for OBR detection and grasping. They allow for unrestricted object categories and language expressions, subject to the training datasets. However, errors in visual perception and ambiguity in human languages are inevitable and negatively impact the robot's performance. To overcome these uncertainties, we build a partially observable Markov decis
    

