# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [DeepAAT: Deep Automated Aerial Triangulation for Fast UAV-based Mapping](https://rss.arxiv.org/abs/2402.01134) | DeepAAT是一个专门为无人机影像AAT设计的深度学习网络，通过考虑影像的空间和光谱特征，提高了AAT的效率和精度。 |
| [^2] | [An Optimization Framework to Personalize Passive Cardiac Mechanics](https://arxiv.org/abs/2404.02807) | 这个研究提出了一个逆有限元分析框架来个性化估计心脏组织的被动力学特性，通过嵌套优化方案的使用，可以更好地逼近匹配图像数据的材料参数。 |
| [^3] | [Measuring Social Norms of Large Language Models](https://arxiv.org/abs/2404.02491) | 论文提出了一个挑战，测试大型语言模型对社会规范的理解，构建了一个数据集涵盖广泛的社会规范问题，通过多代理框架基于大型语言模型来提高模型对社会规范的理解能力。 |
| [^4] | [Techniques for Measuring the Inferential Strength of Forgetting Policies](https://arxiv.org/abs/2404.02454) | 本文提出了一种衡量原理论推理强度变化的损失函数，并使用Problog工具计算损失度量，最终得出了关于不同遗忘策略强度的研究方法和实际应用示例。 |
| [^5] | [Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief](https://arxiv.org/abs/2404.02448) | 提出一种新型的基于电动车辆的路径问题，通过组合基于规则的车辆选择器和基于强化学习的节点选择器解决电动车辆路径问题，以最小化总行驶距离和故障基站数量。 |
| [^6] | [Already Moderate Population Sizes Provably Yield Strong Robustness to Noise](https://arxiv.org/abs/2404.02090) | 适中的种群规模可以在先验位噪声存在时保持强鲁棒性，而不会增加在OneMax基准上的渐近运行时间 |
| [^7] | [Gen4DS: Workshop on Data Storytelling in an Era of Generative AI](https://arxiv.org/abs/2404.01622) | 论文讨论了数据叙事在生成AI时代的重要性和挑战，并邀请学术界和工业界一起探讨生成AI对数据叙事的影响。 |
| [^8] | [360+x: A Panoptic Multi-modal Scene Understanding Dataset](https://arxiv.org/abs/2404.00989) | 该数据集是第一个融合多个视角和多个数据模态以模拟现实世界中日常信息获取方式的数据库。 |
| [^9] | [DRCT: Saving Image Super-resolution away from Information Bottleneck](https://arxiv.org/abs/2404.00722) | 基于Vision Transformer的DRCT方法采用创新的机制解决了图像超分辨率中空间信息衰减的问题，提升了模型性能。 |
| [^10] | [Cross-lingual Named Entity Corpus for Slavic Languages](https://arxiv.org/abs/2404.00482) | 介绍了一个手动标注的用于六种斯拉夫语言的命名实体语料库，提供了两个训练数据集划分，并使用预训练的多语言模型进行命名实体的识别、分类、引用词化和链接。 |
| [^11] | [A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks](https://arxiv.org/abs/2404.00076) | 提出了一种后门攻击方法，名为“DirtyFlipping”，利用脏标签技术在选定的数据模式中输入触发器，从而实现隐蔽的后门。 |
| [^12] | [FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation](https://arxiv.org/abs/2403.20261) | FABind+通过改进口袋预测和姿态生成，提升分子对接表现 |
| [^13] | [Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science](https://arxiv.org/abs/2403.20208) | 本研究旨在利用大型语言模型解决数据科学中表格数据预测任务，通过在丰富的数据集上训练Llama-2模型并进行实际应用，取得显著的改进。 |
| [^14] | [NeuraLunaDTNet: Feedforward Neural Network-Based Routing Protocol for Delay-Tolerant Lunar Communication Networks](https://arxiv.org/abs/2403.20199) | 提出了NeuraLunaDTNet协议，利用前馈神经网络增强了PRoPHET路由协议，通过学习动态变化的时空图中的联系计划来优化月球通信效率。 |
| [^15] | [TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods](https://arxiv.org/abs/2403.20150) | TFB通过解决数据领域覆盖不足、对传统方法的刻板印象以及不一致、不灵活的流程等问题，推动了时间序列预测方法基准比较的最新技术发展。 |
| [^16] | [Segmentation Re-thinking Uncertainty Estimation Metrics for Semantic Segmentation](https://arxiv.org/abs/2403.19826) | 重新思考了语义分割中的不确定性估计度量，发现并改进了PAvPU框架中的核心缺陷。 |
| [^17] | [Understanding the Learning Dynamics of Alignment with Human Feedback](https://arxiv.org/abs/2403.18742) | 本研究对人类偏好对齐的学习动态进行了理论分析，显示了偏好数据集的分布如何影响模型更新速度，并提供了对训练准确度的严格保证，同时揭示了优化易于优先考虑高偏好可区分性行为的复杂现象。 |
| [^18] | [LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation](https://arxiv.org/abs/2403.17601) | 提出了一种学习者感知的监督模仿学习方法，通过变分自动编码器增强专家状态，以解决多智体模仿学习中的协变量偏移问题 |
| [^19] | [Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography](https://arxiv.org/abs/2403.16687) | 探究将ChatGPT应用于对话教学在脑电图学中的有效性，研究发现在对话式教学场景中，ChatGPT能够有效履行教学角色，促进学生学习。 |
| [^20] | [Understanding Domain-Size Generalization in Markov Logic Networks](https://arxiv.org/abs/2403.15933) | 本文量化了马尔科夫逻辑网络在不同大小领域间内部一致性缺失的问题，并提出最大化数据对数似然同时最小化参数方差的方式来优化领域大小泛化。 |
| [^21] | [The Journey to Trustworthy AI- Part 1: Pursuit of Pragmatic Frameworks](https://arxiv.org/abs/2403.15457) | 本文回顾了值得信赖的人工智能（TAI）和其各种定义，主张不应将“负责任的”或“道德的”人工智能等术语视为TAI的替代，而是提倡以公平性、偏见、风险、安全性、可解释性和可靠性等关键属性为中心的方法，认识到地缘政治和地理原因导致的人工智能监管差异对跨国公司构成挑战。 |
| [^22] | [On Pretraining Data Diversity for Self-Supervised Learning](https://arxiv.org/abs/2403.13808) | 增加预训练数据多样性可以提高自监督学习性能，但仅在与下游数据的分布距离较小时有效。 |
| [^23] | [Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs](https://arxiv.org/abs/2403.13801) | 用自然语言推理进行坐标级控制，能够显著提高机器人行动规划的成功率，并且具有将机器人技能迁移到新任务的潜力。 |
| [^24] | [Caching-Augmented Lifelong Multi-Agent Path Finding](https://arxiv.org/abs/2403.13421) | CAL-MAPF引入了缓存机制来增强终身多智能体路径规划的性能，通过合适的输入任务分布、高缓存命中率和流畅的交通这三个因素显著提高了解决方案的稳定性。 |
| [^25] | [Federated reinforcement learning for robot motion planning with zero-shot generalization](https://arxiv.org/abs/2403.13245) | 该论文提出了一个联邦强化学习框架，实现了机器人运动规划中的零次通用化，通过协作学习多个学习者和中央服务器，在不共享原始数据的情况下达到全局最优解。 |
| [^26] | [Provable Privacy with Non-Private Pre-Processing](https://arxiv.org/abs/2403.13041) | 提出了一个框架，能够评估非私密数据相关预处理算法引起的额外隐私成本，并利用平滑DP和预处理算法的有界敏感性建立整体隐私保证的上限 |
| [^27] | [DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM](https://arxiv.org/abs/2403.12488) | DetToolChain提出了一种新的提示范式，可以释放MLLM的零拍摄物体检测能力，并通过检测链式思维自动化任务分解和逐步框细化规划。 |
| [^28] | [Correcting misinformation on social media with a large language model](https://arxiv.org/abs/2403.11169) | 提出了一种名为MUSE的大型语言模型，通过访问最新信息并评估可信度，以解决社交媒体上误信息纠正的难题。 |
| [^29] | [Prediction of readmission of patients by extracting biomedical concepts from clinical texts](https://arxiv.org/abs/2403.09722) | 从临床文本中提取生物医学概念预测患者的再入院情况，可以帮助医生选择适当的治疗方法，从而减少患者再次入院的比率，实现有效的治疗成本降低。 |
| [^30] | [Pre-Sorted Tsetlin Machine (The Genetic K-Medoid Method)](https://arxiv.org/abs/2403.09680) | 该论文提出了一种利用Tsetlin Machines进行传统监督学习的机器学习预排序阶段方法，在MNIST级别的分类问题上取得了显著的精度提升，以及训练时间和推理时间大幅度减少。 |
| [^31] | [Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference](https://arxiv.org/abs/2403.09054) | 本文提出了一种名为“Keyformer”的创新推断时间方法，旨在通过选择关键标记来减少KV缓存的挑战，提高内存带宽利用率。 |
| [^32] | [Exploring Safety Generalization Challenges of Large Language Models via Code](https://arxiv.org/abs/2403.07865) | 本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。 |
| [^33] | [PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck](https://arxiv.org/abs/2403.05297) | PEEB是一种基于部分的图像分类器，通过将类别名称转换为描述视觉部分的文本描述符，并将检测到的部分的嵌入与文本描述符匹配，从而在零样本设置中表现出色，并且不仅在监督学习中表现出色，而且还首次实现用户编辑类定义形成新分类器无需重新训练。 |
| [^34] | [Restricted Bayesian Neural Network](https://arxiv.org/abs/2403.04810) | 本研究提出了限制贝叶斯神经网络的新架构，显著减少了网络存储空间复杂性，并引入了一种能够有效处理不确定性的算法，确保在目标函数缺乏完美凸性时稳健地收敛至全局最优解。 |
| [^35] | [Ever-Evolving Memory by Blending and Refining the Past](https://arxiv.org/abs/2403.04787) | 提出了一种新颖的长期对话记忆方案CREEM，通过混合过去记忆并引入完善过程来实现聊天机器人回应的整体改进和连贯性。 |
| [^36] | [IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators](https://arxiv.org/abs/2403.03894) | 通过利用编译器中间表示来改进代码-LMs的多语言能力和促进跨语言转移。 |
| [^37] | [General2Specialized LLMs Translation for E-commerce](https://arxiv.org/abs/2403.03689) | 提出了一个名为G2ST的两步微调范式，通过自对比语义增强将通用NMT模型转换为专门用于电子商务的NMT模型，以提高翻译质量。 |
| [^38] | [Polynormer: Polynomial-Expressive Graph Transformer in Linear Time](https://arxiv.org/abs/2403.01232) | Polynormer提出了一种多项式表达GT模型，具有线性复杂度，结合本地和全局等变注意力模型，平衡了表现力和可扩展性。 |
| [^39] | [A Survey of Route Recommendations: Methods, Applications, and Opportunities](https://arxiv.org/abs/2403.00284) | 基于城市计算的路线推荐综述对路线推荐研究中的传统机器学习和现代深度学习方法进行了分类，展示了与城市计算场景相关的新应用，并揭示了最新进展。 |
| [^40] | [Cross-domain Chinese Sentence Pattern Parsing](https://arxiv.org/abs/2402.16311) | 本文提出了一种利用大型语言模型进行自我训练的创新方法，通过动态生成训练数据将源领域句法规则与目标领域句子相结合，增强句式结构解析器对各种领域的适应能力，实验证明其在教科书和新闻领域的效果优于基于规则的基准模型1.68个百分点。 |
| [^41] | [Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding](https://arxiv.org/abs/2402.15102) | 自动广告竞标中使用了一种新的迭代离线强化学习框架，有效缓解了传统RL算法在在线环境下性能下降的问题。 |
| [^42] | [IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus](https://arxiv.org/abs/2402.14710) | 发布了IEPile，一个包含约0.32B个标记的综合双语IE指令语料库，通过收集和清理33个现有IE数据集并引入基于模式的指令生成，可以提高大型语言模型在信息抽取领域的性能，尤其是零样本泛化。 |
| [^43] | [Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models](https://arxiv.org/abs/2402.14207) | 提出了一种名为STORM的写作系统，用于通过检索和多视角提问合成主题概要，以辅助从头开始写类似维基百科的文章。 |
| [^44] | [Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective](https://arxiv.org/abs/2402.10184) | 本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。 |
| [^45] | [Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data](https://arxiv.org/abs/2402.10100) | 本研究评估了在临床设置中使用深度学习模型进行音频分类的效果，并发现在微调之前，预训练模型在大数据集上的性能对临床数据的影响较好。研究结果表明，CNN模型可以在小数据集环境中与转换模型相媲美或超越。 |
| [^46] | [OrderBkd: Textual backdoor attack through repositioning](https://arxiv.org/abs/2402.07689) | 本论文提出了一种通过重新定位句子中的两个单词实施文本后门攻击的方法，与已有的攻击方式相比，在攻击成功率、困惑度和与干净样本的语义相似性方面表现更好，并且对ONION防御方法具有鲁棒性。 |
| [^47] | [Dynamic Graph Information Bottleneck](https://arxiv.org/abs/2402.06716) | 动态图信息瓶颈框架（DGIB）能够学习鲁棒且有区分性的动态图表示。利用信息瓶颈原理，通过迭代引导和改进图快照传递的结构和特征信息流，压缩冗余信息并保留有价值的信息。该框架能满足最小-全局-一致条件，提高了动态图神经网络的鲁棒性。 |
| [^48] | [Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations](https://arxiv.org/abs/2402.05713) | 该研究发现在医学影像中，可以通过针对特定人群的标签污染攻击来破坏深度学习模型的性能，并引入对抗性的诊断不足偏见。研究结果还表明，人群在训练数据中的表示对于不可检测的对抗性偏见攻击的脆弱性直接相关。 |
| [^49] | [Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving](https://arxiv.org/abs/2402.05359) | 该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。 |
| [^50] | [Zero-Shot Clinical Trial Patient Matching with LLMs](https://arxiv.org/abs/2402.05125) | 本研究基于LLMs开发了一个零样本临床试验患者匹配系统，可以高效评估患者是否符合入选标准，并通过优化提示策略和检索流程提高了数据和成本效率。 |
| [^51] | [Online Transfer Learning for RSV Case Detection](https://arxiv.org/abs/2402.01987) | 这项研究介绍了一种名为PVAW的在线多源转移学习方法，通过动态加权机制实现了对序列流行病学数据的自适应调整，并在分析RSV数据的应用中取得了显著的模型性能改进。 |
| [^52] | [Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators](https://arxiv.org/abs/2401.17548) | 本文提出了一种新方法，LIFT，通过利用通道相关性和领先指标，为多元时间序列预测提供准确的预测。LIFT方法可以无缝与任意时间序列预测方法协作，大量实验证明了其有效性。 |
| [^53] | [Customizing Language Model Responses with Contrastive In-Context Learning](https://arxiv.org/abs/2401.17390) | 本论文提出了一种使用对比示例来定制语言模型回复的方法，通过提供正面示例和负面示例，使模型学会如何回避负面特征，从而更好地满足用户需求。 |
| [^54] | [MLCA-AVSR: Multi-Layer Cross Attention Fusion based Audio-Visual Speech Recognition](https://arxiv.org/abs/2401.03424) | 提出了基于多层交叉注意力融合的音频-视觉语音识别（MLCA-AVSR）方法，通过在不同级别的音频/视觉编码器上融合模态特征，有效提高了系统的稳健性。 |
| [^55] | [Towards an end-to-end artificial intelligence driven global weather forecasting system](https://arxiv.org/abs/2312.12462) | 提出了一种端到端基于人工智能的全球天气预报系统，通过将AI技术应用于数据同化和天气预报模型，实现了从数据处理到预测全过程的自动化。 |
| [^56] | [Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?](https://arxiv.org/abs/2311.09702) | 本研究探讨了大型语言模型存在的幻觉和不忠实推理问题，提出一种新的探测方法和基准测试以研究LLMs在推理过程中是否会采取欺骗性语义快捷方式。 |
| [^57] | [To Tell The Truth: Language of Deception and Language Models](https://arxiv.org/abs/2311.07092) | 在高风险环境中，研究人员通过分析电视游戏节目数据发现，即使只使用语言线索，基于大型语言模型构建的模型可以与人类主体具有类似的真相检测性能。 |
| [^58] | [Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes](https://arxiv.org/abs/2310.10648) | 通过使用决策模型Bridge，结合专家的认知任务分析，成功利用大型语言模型（LLMs）来弥补新手和专家在纠正数学错误中的知识差距。 |
| [^59] | [ARS-DETR: Aspect Ratio Sensitive Oriented Object Detection with Transformer](https://arxiv.org/abs/2303.04989) | 提出了一种面向长宽比敏感的定向目标检测器ARS-DETR，采用高精度度量AP$_{75}$来衡量模型性能，并通过新的角度分类方法和旋转可变形注意力模块实现了竞争性能。 |
| [^60] | [Locality Sensitive Sparse Encoding for Learning World Models Online.](http://arxiv.org/abs/2401.13034) | 本文提出了一种基于局部敏感稀疏编码的线性回归模型，通过非线性随机特征实现对复杂环境的拟合。这种模型能够高效地进行稀疏更新，实现了优化拟合先前经验的Follow-The-Leader（FTL）世界模型。 |
| [^61] | [Tiny Multi-Agent DRL for Twins Migration in UAV Metaverses: A Multi-Leader Multi-Follower Stackelberg Game Approach.](http://arxiv.org/abs/2401.09680) | 本论文提出了一种基于小型机器学习的Stackelberg博弈框架，在无人机Metaverse中实现高效的双胞胎迁移，以提供无缝沉浸式体验。 |
| [^62] | [DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation.](http://arxiv.org/abs/2401.04747) | DiffSHEG是一种基于扩散的实时语音驱动的整体三维表情和手势生成方法，通过联合生成同步表情和手势，并引入基于扩散模型的任意长序列生成策略，实现了高质量的同步表情和手势生成。 |
| [^63] | [Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series.](http://arxiv.org/abs/2401.03955) | 本论文介绍了一种名为微小时间混合器 (TTMs) 的预训练模型，该模型针对多变量时间序列的零/少样本预测进行了优化。与大型预训练模型相比，TTMs模型更小、更快，并考虑了跨通道相关性，能够在短时间内进行有效的预测。 |
| [^64] | [Rethinking Dimensional Rationale in Graph Contrastive Learning from Causal Perspective.](http://arxiv.org/abs/2312.10401) | 本论文从因果角度重新思考图对比学习中的维度理论，并提出在图中捕捉维度理论的方法，以改善性能，并解决图模型学习中的问题。以上方法在实验中得到验证。 |
| [^65] | [QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models.](http://arxiv.org/abs/2310.08041) | QLLM是一种为大规模语言模型设计的准确高效的低位宽后训练量化方法，通过引入自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。 |
| [^66] | [How does prompt engineering affect ChatGPT performance on unsupervised entity resolution?.](http://arxiv.org/abs/2310.06174) | 本研究对提示工程对ChatGPT在无监督实体解析中的影响进行了初步实验研究，结果显示提示可以显著影响实体解析的质量。 |
| [^67] | [Positive and Risky Message Assessment for Music Products.](http://arxiv.org/abs/2309.10182) | 这项研究提出了一个新的问题：如何评估音乐产品中的正面和风险信息。研究者提出了一个多任务预测模型，通过序数约束解决这个问题，并且取得了显著优于其他方法的结果。 |
| [^68] | [Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree Image Generation.](http://arxiv.org/abs/2309.03467) | 该论文提出了一种自回归全方位感知生成网络（AOG-Net）用于生成360度图像，通过渐进地外扩不完整的360度图像，并与窄视场（NFoV）和文本引导相结合或单独使用。这种方法可以生成更细致和与文本一致的模式，并为用户在生成过程中灵活编辑条件。 |
| [^69] | [Exploring the Integration Strategies of Retriever and Large Language Models.](http://arxiv.org/abs/2308.12574) | 本文通过探索不同的检索器和大型语言模型整合方法来增强答案生成，并发现常用的连接方法存在局限性。为了解决这个问题，本文提出了四种替代策略，包括两种单轮方法和两种多轮策略。 |
| [^70] | [A Theory of Intelligences: Concepts, Models, Implications.](http://arxiv.org/abs/2308.12411) | 这篇论文提出了一种智能的理论，讨论了智能的核心要素和挑战，并提出了基于第一原理的理论。研究重点是人类智能，并与机器进行比较，目的是为更广泛的生命、集合体和非设计的物理化学系统提供描述。 |
| [^71] | [Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links.](http://arxiv.org/abs/2308.03929) | 本研究通过构建基于本体的知识图谱，利用疾病本体和症状本体构建数学模型，利用事实核查算法和网络中心度指标分析ChatGPT生成的文本与真实医学文献之间的准确性，以验证疾病-症状关系。 |
| [^72] | [Exploring Non-Regular Extensions of Propositional Dynamic Logic with Description-Logics Features.](http://arxiv.org/abs/2307.09913) | 研究了非正则路径表达式对于扩展ALC描述逻辑中可满足性检查和查询的可决定性的影响，并提供了一系列不可决定性结果。 |
| [^73] | [Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects.](http://arxiv.org/abs/2306.10125) | 自监督学习（SSL）在时间序列分析中的应用取得了显著性能，通过减少对标注数据的依赖，即使只有少量标注数据，也能实现高性能。 |
| [^74] | [DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting.](http://arxiv.org/abs/2306.09862) | DoubleAdapt是一个增量学习的方法，用于股票趋势预测。它利用元学习技术自动学习如何将股票数据适应到本地平稳分布空间中，从而有效地适应数据和模型，减轻分布漂移的影响。 |
| [^75] | [Fedstellar: A Platform for Decentralized Federated Learning.](http://arxiv.org/abs/2306.09750) | Fedstellar是一个联邦学习平台，支持物理或虚拟设备的去中心化、半去中心化和中心化的方式训练模型，旨在解决现有平台在处理异构联盟网络拓扑等问题时的挑战。 |
| [^76] | [Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers.](http://arxiv.org/abs/2305.17328) | 《Zero-TPrune》是一个考虑到令牌的重要性和相似性的零射击方法，它利用预训练Transformer模型的注意图来进行令牌剪枝，以求解在边缘设备上Transformer模型即插即用的难题。 |
| [^77] | [AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys.](http://arxiv.org/abs/2305.09620) | 本论文研究了利用经过全国代表性调查微调的大语言模型（LLMs）来增强调查的观点预测，取得了在遗漏数据插值和回溯推理方面优秀的成果，在零次预测方面仍需进一步研究。 |
| [^78] | [Impact of Position Bias on Language Models in Token Classification.](http://arxiv.org/abs/2304.13567) | 研究了语言模型在token分类任务中的位置偏差问题，通过实验表明在具体任务中，BERT、ERNIE、ELECTRA等编码器以及GPT2和BLOOM等解码器的平均性能下降了3%和9%。 |
| [^79] | [A Recommender System Approach for Very Large-scale Multiobjective Optimization.](http://arxiv.org/abs/2304.04067) | 本论文提出了一种基于推荐系统的大规模多目标优化方法，将解决方案视为用户，通过汤普森抽样和高斯过程逼近 Pareto-最优前沿上的后验分布，成功解决了处理大规模问题的难题。 |

# 详细

[^1]: DeepAAT: 快速无人机地图制作的深度自动航空三角测量

    DeepAAT: Deep Automated Aerial Triangulation for Fast UAV-based Mapping

    [https://rss.arxiv.org/abs/2402.01134](https://rss.arxiv.org/abs/2402.01134)

    DeepAAT是一个专门为无人机影像AAT设计的深度学习网络，通过考虑影像的空间和光谱特征，提高了AAT的效率和精度。

    

    自动航空三角测量（AAT）旨在同时恢复图像姿态和重建稀疏点，对地球观测至关重要。凭借其在摄影测量领域数十年的研究积淀，AAT已经发展成为大规模无人机地图制作中广泛应用的基本过程。然而，传统的AAT方法仍面临效率低和稳健性有限等挑战。本文介绍了DeepAAT，这是一个专门为无人机影像AAT设计的深度学习网络。DeepAAT考虑了影像的空间和光谱特征，增强了解决错误匹配对和准确预测图像姿态的能力。DeepAAT在AAT的效率方面取得了重大突破，确保了场景的全面覆盖和精度。其处理速度比增量AAT方法快几百倍，比全局AAT方法快几十倍，同时保持可比的重建水平。

    Automated Aerial Triangulation (AAT), aiming to restore image pose and reconstruct sparse points simultaneously, plays a pivotal role in earth observation. With its rich research heritage spanning several decades in photogrammetry, AAT has evolved into a fundamental process widely applied in large-scale Unmanned Aerial Vehicle (UAV) based mapping. Despite its advancements, classic AAT methods still face challenges like low efficiency and limited robustness. This paper introduces DeepAAT, a deep learning network designed specifically for AAT of UAV imagery. DeepAAT considers both spatial and spectral characteristics of imagery, enhancing its capability to resolve erroneous matching pairs and accurately predict image poses. DeepAAT marks a significant leap in AAT's efficiency, ensuring thorough scene coverage and precision. Its processing speed outpaces incremental AAT methods by hundreds of times and global AAT methods by tens of times while maintaining a comparable level of reconstruct
    
[^2]: 一个个性化被动心脏力学的优化框架

    An Optimization Framework to Personalize Passive Cardiac Mechanics

    [https://arxiv.org/abs/2404.02807](https://arxiv.org/abs/2404.02807)

    这个研究提出了一个逆有限元分析框架来个性化估计心脏组织的被动力学特性，通过嵌套优化方案的使用，可以更好地逼近匹配图像数据的材料参数。

    

    个性化的心脏力学建模是一种强大的工具，用于理解心脏功能在健康和疾病中的生物力学并帮助治疗计划。然而，当前的模型仅限于使用在单一心脏相位获取的医学图像，通常限制了它们用于处理动态图像获取的适用性。本研究介绍了一种逆有限元分析（iFEA）框架，使用时间相关的医学图像数据来估计心脏组织的被动机械特性。该iFEA框架依赖于一种新颖的嵌套优化方案，其中外部迭代利用传统优化方法来最佳逼近匹配图像数据的材料参数，而内部迭代采用增广Sellier算法来估计无应力参考构型。重点放在表征被动机械行为上，该框架采用基于结构的方法

    arXiv:2404.02807v1 Announce Type: cross  Abstract: Personalized cardiac mechanics modeling is a powerful tool for understanding the biomechanics of cardiac function in health and disease and assisting in treatment planning. However, current models are limited to using medical images acquired at a single cardiac phase, often limiting their applicability for processing dynamic image acquisitions. This study introduces an inverse finite element analysis (iFEA) framework to estimate the passive mechanical properties of cardiac tissue using time-dependent medical image data. The iFEA framework relies on a novel nested optimization scheme, in which the outer iterations utilize a traditional optimization method to best approximate material parameters that fit image data, while the inner iterations employ an augmented Sellier's algorithm to estimate the stress-free reference configuration. With a focus on characterizing the passive mechanical behavior, the framework employs structurally based 
    
[^3]: 测量大型语言模型的社会规范

    Measuring Social Norms of Large Language Models

    [https://arxiv.org/abs/2404.02491](https://arxiv.org/abs/2404.02491)

    论文提出了一个挑战，测试大型语言模型对社会规范的理解，构建了一个数据集涵盖广泛的社会规范问题，通过多代理框架基于大型语言模型来提高模型对社会规范的理解能力。

    

    我们提出了一个新的挑战，以检验大型语言模型是否理解社会规范。与现有数据集不同，我们的数据集要求具有解决社会规范的基本理解。我们的数据集包含最大的社会规范技能集，包括402项技能和12,383个问题，涵盖了从观点和论点到文化和法律等广泛的社会规范。我们根据K-12课程设计了我们的数据集。这使得可以直接将大型语言模型的社会理解能力与人类进行比较，更具体地说是与小学生进行比较。尽管先前的工作在我们的基准测试中产生几乎随机的准确度，但最近的大型语言模型（如GPT3.5-Turbo和LLaMA2-Chat）能够显著提高性能，仅略低于人类性能。然后，我们提出了一个基于大型语言模型的多代理框架，以提高模型理解社会规范的能力。

    arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
    
[^4]: 衡量遗忘策略的推理强度技术

    Techniques for Measuring the Inferential Strength of Forgetting Policies

    [https://arxiv.org/abs/2404.02454](https://arxiv.org/abs/2404.02454)

    本文提出了一种衡量原理论推理强度变化的损失函数，并使用Problog工具计算损失度量，最终得出了关于不同遗忘策略强度的研究方法和实际应用示例。

    

    知识表示中的遗忘技术被证明是一种强大且有广泛应用的知识工程工具。然而，关于不同的遗忘策略或不同遗忘操作符的使用如何影响原理论的推理强度几乎没有研究。本文旨在根据模型计数和概率理论的直觉定义用于衡量推理强度变化的损失函数。研究了此类损失度量的性质，并提出了一种实用的知识工程工具，用于使用Problog计算损失度量。论文包括一个用于研究和确定不同遗忘策略强度的工作方法，以及展示如何利用Problog应用理论结果的具体示例。虽然重点是遗忘，但结果更为普遍，并且应具有更广泛的应用。

    arXiv:2404.02454v1 Announce Type: new  Abstract: The technique of forgetting in knowledge representation has been shown to be a powerful and useful knowledge engineering tool with widespread application. Yet, very little research has been done on how different policies of forgetting, or use of different forgetting operators, affects the inferential strength of the original theory. The goal of this paper is to define loss functions for measuring changes in inferential strength based on intuitions from model counting and probability theory. Properties of such loss measures are studied and a pragmatic knowledge engineering tool is proposed for computing loss measures using Problog. The paper includes a working methodology for studying and determining the strength of different forgetting policies, in addition to concrete examples showing how to apply the theoretical results using Problog. Although the focus is on forgetting, the results are much more general and should have wider applicati
    
[^5]: 电动车辆路径问题用于应急供电：面向电信基站救助

    Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief

    [https://arxiv.org/abs/2404.02448](https://arxiv.org/abs/2404.02448)

    提出一种新型的基于电动车辆的路径问题，通过组合基于规则的车辆选择器和基于强化学习的节点选择器解决电动车辆路径问题，以最小化总行驶距离和故障基站数量。

    

    作为一家电信提供商，我们公司有一个关键使命，即在停电期间保持电信服务。为了实现这一使命，至关重要的是维持电信基站的电力。本文考虑一种解决方案，即电动车辆 (EVs) 直接前往其位置为基站提供电力。我们的目标是找到最小化所有电动车辆的总行驶距离和故障基站数量的EV路线。在本文中，我们将这一路径问题形式化为新型的电动车辆路径问题 (EVRP) 变体，并提出了将基于规则的车辆选择器和基于强化学习（RL）的节点选择器相结合的求解器。车辆选择器的规则确保了所选EV开始移动时的确切环境状态。此外，RL模型的节点选择实现了快速路径生成，在紧急情况下尤为重要。我们在机器人上对我们的求解器进行评估

    arXiv:2404.02448v1 Announce Type: cross  Abstract: As a telecom provider, our company has a critical mission to maintain telecom services even during power outages. To accomplish the mission, it is essential to maintain the power of the telecom base stations. Here we consider a solution where electric vehicles (EVs) directly supply power to base stations by traveling to their locations. The goal is to find EV routes that minimize both the total travel distance of all EVs and the number of downed base stations. In this paper, we formulate this routing problem as a new variant of the Electric Vehicle Routing Problem (EVRP) and propose a solver that combines a rule-based vehicle selector and a reinforcement learning (RL)-based node selector. The rule of the vehicle selector ensures the exact environmental states when the selected EV starts to move. In addition, the node selection by the RL model enables fast route generation, which is critical in emergencies. We evaluate our solver on bot
    
[^6]: 已经适中的种群规模可证明对噪声具有强大的鲁棒性

    Already Moderate Population Sizes Provably Yield Strong Robustness to Noise

    [https://arxiv.org/abs/2404.02090](https://arxiv.org/abs/2404.02090)

    适中的种群规模可以在先验位噪声存在时保持强鲁棒性，而不会增加在OneMax基准上的渐近运行时间

    

    经验表明，典型的进化算法可以很好地应对诸如嘈杂的函数评估等随机干扰。在第一次针对$(1+\lambda)$和$(1,\lambda)$进化算法在先验位噪声存在时的数学运行时间分析中，我们表明两种算法都能容忍恒定的噪声概率，而不会增加在OneMax基准上的渐近运行时间。为此，种群规模$\lambda$应至少为问题规模$n$的对数。在这方向上的唯一先前结果涉及不太现实的一位噪声模型，需要超线性的问题规模种群大小，并且对于OneMax基准证明了大致是无噪声运行时间的三次方的运行时间保证。我们的显着更强结果基于一种新颖的证明方法，即无噪声后代可以看作是父代和有噪声的后代之间的有偏统一交叉。

    arXiv:2404.02090v1 Announce Type: cross  Abstract: Experience shows that typical evolutionary algorithms can cope well with stochastic disturbances such as noisy function evaluations.   In this first mathematical runtime analysis of the $(1+\lambda)$ and $(1,\lambda)$ evolutionary algorithms in the presence of prior bit-wise noise, we show that both algorithms can tolerate constant noise probabilities without increasing the asymptotic runtime on the OneMax benchmark. For this, a population size $\lambda$ suffices that is at least logarithmic in the problem size $n$. The only previous result in this direction regarded the less realistic one-bit noise model, required a population size super-linear in the problem size, and proved a runtime guarantee roughly cubic in the noiseless runtime for the OneMax benchmark. Our significantly stronger results are based on the novel proof argument that the noiseless offspring can be seen as a biased uniform crossover between the parent and the noisy o
    
[^7]: Gen4DS：生成AI时代的数据叙事研讨会

    Gen4DS: Workshop on Data Storytelling in an Era of Generative AI

    [https://arxiv.org/abs/2404.01622](https://arxiv.org/abs/2404.01622)

    论文讨论了数据叙事在生成AI时代的重要性和挑战，并邀请学术界和工业界一起探讨生成AI对数据叙事的影响。

    

    叙事是一种古老而珍贵的人类能力，在数字时代得到了新生。在过去的十年里，学术界和工业界对数据叙事的认可和应用有了显著增长。最近，生成AI的快速发展为这一领域带来了新的机遇和挑战，引发了许多新问题。我们邀请大家参加我们的研讨会（Gen4DS），讨论生成AI如何促进数据叙事的创作？生成AI如何改变数据叙事者的工作流程？在叙事中加入AI的风险和隐患是什么？

    arXiv:2404.01622v1 Announce Type: cross  Abstract: Storytelling is an ancient and precious human ability that has been rejuvenated in the digital age. Over the last decade, there has been a notable surge in the recognition and application of data storytelling, both in academia and industry. Recently, the rapid development of generative AI has brought new opportunities and challenges to this field, sparking numerous new questions. These questions may not necessarily be quickly transformed into papers, but we believe it is necessary to promptly discuss them to help the community better clarify important issues and research agendas for the future. We thus invite you to join our workshop (Gen4DS) to discuss questions such as: How can generative AI facilitate the creation of data stories? How might generative AI alter the workflow of data storytellers? What are the pitfalls and risks of incorporating AI in storytelling? We have designed both paper presentations and interactive activities (i
    
[^8]: 360+x：一个全景多模态场景理解数据集

    360+x: A Panoptic Multi-modal Scene Understanding Dataset

    [https://arxiv.org/abs/2404.00989](https://arxiv.org/abs/2404.00989)

    该数据集是第一个融合多个视角和多个数据模态以模拟现实世界中日常信息获取方式的数据库。

    

    人类对世界的感知受到多种视角和模态的影响。虽然许多现有数据集侧重于从某种视角（例如自我中心或第三人称视角）理解场景，但我们的数据集提供了一个全景视角（即多视角和多数据模态）。具体而言，我们捕捉了第三人称全景和前视图，以及具有视频、多通道音频、定向双耳延迟、位置数据和文本场景描述等丰富模态的自我的单眼/双眼视图，呈现了对世界的全面观察。图1展示了我们的360+x数据集的所有28个场景类别。据我们所知，这是第一个涵盖多个视角和多个数据模态的数据库，模拟了现实世界中日常信息的获取方式。通过我们的基准分析，我们提出了5种不同

    arXiv:2404.00989v1 Announce Type: cross  Abstract: Human perception of the world is shaped by a multitude of viewpoints and modalities. While many existing datasets focus on scene understanding from a certain perspective (e.g. egocentric or third-person views), our dataset offers a panoptic perspective (i.e. multiple viewpoints with multiple data modalities). Specifically, we encapsulate third-person panoramic and front views, as well as egocentric monocular/binocular views with rich modalities including video, multi-channel audio, directional binaural delay, location data and textual scene descriptions within each scene captured, presenting comprehensive observation of the world. Figure 1 offers a glimpse of all 28 scene categories of our 360+x dataset. To the best of our knowledge, this is the first database that covers multiple viewpoints with multiple data modalities to mimic how daily information is accessed in the real world. Through our benchmark analysis, we presented 5 differe
    
[^9]: DRCT：将图像超分辨率保存在信息瓶颈之外

    DRCT: Saving Image Super-resolution away from Information Bottleneck

    [https://arxiv.org/abs/2404.00722](https://arxiv.org/abs/2404.00722)

    基于Vision Transformer的DRCT方法采用创新的机制解决了图像超分辨率中空间信息衰减的问题，提升了模型性能。

    

    近年来，基于Vision Transformer的低层视觉任务应用取得了广泛的成功。与基于CNN的模型不同，Transformer更擅长捕捉长距离依赖关系，可以利用非局部区域的信息重建图像。在超分辨率领域，基于Swin Transformer的方法已经成为主流，因为它们能够捕捉全局空间信息，并且具有旋转窗口注意机制，有助于在不同窗口之间交换信息。许多研究人员通过扩大感知野或设计复杂网络来提高图像质量和网络效率，取得了令人称赞的结果。然而，我们观察到在前向传播过程中，由于深度增加，空间信息往往会减少，从而导致空间信息的丢失，并最终限制了模型的潜力。

    arXiv:2404.00722v1 Announce Type: cross  Abstract: In recent years, Vision Transformer-based applications to low-level vision tasks have achieved widespread success. Unlike CNN-based models, Transformers are more adept at capturing long-range dependencies, enabling the reconstruction of images utilizing information from non-local areas. In the domain of super-resolution, Swin-transformer-based approaches have become mainstream due to their capacity to capture global spatial information and their shifting-window attention mechanism that facilitates the interchange of information between different windows. Many researchers have enhanced image quality and network efficiency by expanding the receptive field or designing complex networks, yielding commendable results. However, we observed that spatial information tends to diminish during the forward propagation process due to increased depth, leading to a loss of spatial information and, consequently, limiting the model's potential. To addr
    
[^10]: 用于斯拉夫语的跨语言命名实体语料库

    Cross-lingual Named Entity Corpus for Slavic Languages

    [https://arxiv.org/abs/2404.00482](https://arxiv.org/abs/2404.00482)

    介绍了一个手动标注的用于六种斯拉夫语言的命名实体语料库，提供了两个训练数据集划分，并使用预训练的多语言模型进行命名实体的识别、分类、引用词化和链接。

    

    本文介绍了一个手动注释的包含六种斯拉夫语言（保加利亚语、捷克语、波兰语、斯洛文尼亚语、俄语和乌克兰语）命名实体的语料库。这项工作是2017-2023年间斯拉夫自然语言处理研讨会的一系列共享任务的结果。该语料库包含了5017份涵盖七个主题的文档，文档标有五类命名实体，每个实体由类别、引用词和唯一跨语言标识符描述。我们提供了两个训练调整的数据集划分 - 单个主题划分和跨主题划分。对于每个划分，我们使用基于transformer的神经网络架构设置了基准，使用预训练的多语言模型XLM-RoBERTa-large进行命名实体提及识别和分类，以及mT5-large进行命名实体引用词化和链接。

    arXiv:2404.00482v1 Announce Type: cross  Abstract: This paper presents a corpus manually annotated with named entities for six Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017-2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5 017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits - single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models - XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking.
    
[^11]: 使用倒置标签的后门方法：脏标签翻转攻击

    A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks

    [https://arxiv.org/abs/2404.00076](https://arxiv.org/abs/2404.00076)

    提出了一种后门攻击方法，名为“DirtyFlipping”，利用脏标签技术在选定的数据模式中输入触发器，从而实现隐蔽的后门。

    

    基于声音的机器学习系统经常使用公共或第三方数据，这可能是不准确的。这使得训练在这些数据上的深度神经网络（DNN）模型容易受到潜在的数据毒化攻击。在这种攻击类型中，攻击者可以使用毒化数据来训练DNN模型，可能会降低其性能。另一种对我们的研究非常相关的数据毒化攻击类型是标签翻转，攻击者在其中操纵数据子集的标签。已经证明，即使是能力有限的攻击者，这些攻击也可能极大地降低系统性能。在本研究中，我们提出了一种名为“DirtyFlipping”的后门攻击，使用脏标签技术，“标签对标签”，在与目标类别相关的选定数据模式中输入触发器（拍手），从而实现了隐蔽的后门。

    arXiv:2404.00076v1 Announce Type: cross  Abstract: Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named 'DirtyFlipping', which uses dirty label techniques, "label-on-label", to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.
    
[^12]: FABind+: 通过改进口袋预测和姿态生成增强分子对接

    FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation

    [https://arxiv.org/abs/2403.20261](https://arxiv.org/abs/2403.20261)

    FABind+通过改进口袋预测和姿态生成，提升分子对接表现

    

    分子对接是药物发现中至关重要的过程。传统技术依赖于受物理原理支配的广泛采样和模拟，但这些方法往往速度慢且昂贵。基于深度学习的方法的出现显示出显著的前景，提供了精确性和效率的增长。建立在FABind的基础工作之上，这是一个专注于速度和准确性的模型，我们提出了FABind+，这是一个大大提升其前身性能的增强版。我们确定口袋预测是分子对接中的一个关键瓶颈，并提出了一种显著改进口袋预测的新方法，从而简化了对接过程。此外，我们对对接模块进行了修改，以增强其姿态生成能力。为了缩小与传统采样/生成方法之间的差距，我们结合了一个简单而有效的s

    arXiv:2403.20261v1 Announce Type: cross  Abstract: Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective s
    
[^13]: 发挥大型语言模型在数据科学中预测表格任务的潜力

    Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science

    [https://arxiv.org/abs/2403.20208](https://arxiv.org/abs/2403.20208)

    本研究旨在利用大型语言模型解决数据科学中表格数据预测任务，通过在丰富的数据集上训练Llama-2模型并进行实际应用，取得显著的改进。

    

    在数据科学领域，分类、回归和缺失值填充等预测任务是与表格数据相关的常见挑战。这项研究旨在应用大型语言模型(LLMs)来解决这些预测任务。尽管LLMs擅长理解自然语言，但在处理结构化表格数据方面表现不佳。我们的研究旨在通过收集带有指令注释的表格语料库，并在这一丰富的数据集上对Llama-2进行大规模训练，以弥合这一差距。此外，我们研究了将训练模型应用于零-shot预测、少-shot预测和上下文学习场景的实际应用。通过广泛实验，我们的方法论显示了显著的改进。

    arXiv:2403.20208v1 Announce Type: new  Abstract: In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improv
    
[^14]: NeuraLunaDTNet：基于前馈神经网络的延迟容忍月球通信网络路由协议

    NeuraLunaDTNet: Feedforward Neural Network-Based Routing Protocol for Delay-Tolerant Lunar Communication Networks

    [https://arxiv.org/abs/2403.20199](https://arxiv.org/abs/2403.20199)

    提出了NeuraLunaDTNet协议，利用前馈神经网络增强了PRoPHET路由协议，通过学习动态变化的时空图中的联系计划来优化月球通信效率。

    

    空间通信面临严重延迟、难以预测的路径和通信中断等挑战。延迟容忍网络架构是为了解决这些情况而专门设计的，适用于应对一些挑战。传统的DTN路由协议在优化性能方面存在不足，由于空间通信的固有复杂性。研究人员致力于利用人工智能的最新进展来缓解一些路由挑战。我们提出利用前馈神经网络开发一种新颖的协议NeuraLunaDTNet，通过学习动态变化的时空图中的联系计划来提高PRoPHET路由协议在月球通信中的效率。

    arXiv:2403.20199v1 Announce Type: cross  Abstract: Space Communication poses challenges such as severe delays, hard-to-predict routes and communication disruptions. The Delay Tolerant Network architecture, having been specifically designed keeping such scenarios in mind, is suitable to address some challenges. The traditional DTN routing protocols fall short of delivering optimal performance, due to the inherent complexities of space communication. Researchers have aimed at using recent advancements in AI to mitigate some routing challenges [9]. We propose utilising a feedforward neural network to develop a novel protocol NeuraLunaDTNet, which enhances the efficiency of the PRoPHET routing protocol for lunar communication, by learning contact plans in dynamically changing spatio-temporal graph.
    
[^15]: TFB：面向时间序列预测方法全面且公平的基准比较

    TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods

    [https://arxiv.org/abs/2403.20150](https://arxiv.org/abs/2403.20150)

    TFB通过解决数据领域覆盖不足、对传统方法的刻板印象以及不一致、不灵活的流程等问题，推动了时间序列预测方法基准比较的最新技术发展。

    

    时间序列会在经济、交通、健康和能源等不同领域中产生，对未来数值的预测在许多重要应用中起着关键作用。不出所料，许多预测方法被提出。为了确保进展，有必要能够以全面且可靠的方式经验性地研究和比较这些方法。为了实现这一目标，我们提出了TFB，一个自动化的时间序列预测（TSF）方法基准测试。TFB通过解决与数据集、比较方法和评估管道相关的缺点，推动了最新技术的发展：1）数据领域覆盖不足，2）对传统方法的刻板印象，3）不一致和不灵活的流程。为了获得更好的领域覆盖率，我们包括了来自10个不同领域的数据集：交通、电力、能源、环境、自然、经济、股票市场、银行、健康和网络。我们还提供了一个时间序列特性

    arXiv:2403.20150v1 Announce Type: cross  Abstract: Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series char
    
[^16]: 重新思考语义分割中的不确定性估计度量

    Segmentation Re-thinking Uncertainty Estimation Metrics for Semantic Segmentation

    [https://arxiv.org/abs/2403.19826](https://arxiv.org/abs/2403.19826)

    重新思考了语义分割中的不确定性估计度量，发现并改进了PAvPU框架中的核心缺陷。

    

    在计算机视觉领域中，语义分割作为机器学习中的一个基本应用，将图像的每个像素分类为不同的语义类别。这项任务通过包含不确定性量化来超越传统的准确度度量，不确定性量化是评估每个分割预测可靠性的关键指标。我们的研究识别出PAvPU（Patch Accuracy versus Patch Uncertainty）框架中的三个核心缺陷，并提出旨在改进该度量的强大解决方案。通过解决这些问题，我们旨在增强可靠性和适用性。

    arXiv:2403.19826v1 Announce Type: new  Abstract: In the domain of computer vision, semantic segmentation emerges as a fundamental application within machine learning, wherein individual pixels of an image are classified into distinct semantic categories. This task transcends traditional accuracy metrics by incorporating uncertainty quantification, a critical measure for assessing the reliability of each segmentation prediction. Such quantification is instrumental in facilitating informed decision-making, particularly in applications where precision is paramount. Within this nuanced framework, the metric known as PAvPU (Patch Accuracy versus Patch Uncertainty) has been developed as a specialized tool for evaluating entropy-based uncertainty in image segmentation tasks. However, our investigation identifies three core deficiencies within the PAvPU framework and proposes robust solutions aimed at refining the metric. By addressing these issues, we aim to enhance the reliability and applic
    
[^17]: 理解人类反馈对齐学习动态的研究

    Understanding the Learning Dynamics of Alignment with Human Feedback

    [https://arxiv.org/abs/2403.18742](https://arxiv.org/abs/2403.18742)

    本研究对人类偏好对齐的学习动态进行了理论分析，显示了偏好数据集的分布如何影响模型更新速度，并提供了对训练准确度的严格保证，同时揭示了优化易于优先考虑高偏好可区分性行为的复杂现象。

    

    大型语言模型（LLMs）与人类意图对齐已成为安全部署模型在实际系统中的关键任务。现有的对齐方法虽然在经验上取得了成功，但理论上了解这些方法如何影响模型行为仍然是一个悬而未决的问题。我们的工作首次尝试在理论上分析人类偏好对齐的学习动态。我们正式展示了偏好数据集的分布如何影响模型更新速度，并对训练准确度提供了严格的保证。我们的理论还揭示了一个复杂现象，即优化易于优先考虑具有更高偏好可区分性的行为。我们在当代LLMs和对齐任务上在实证上验证了我们的发现，强化了我们的理论见解，并为未来的对齐方法提供了启示。免责声明：本文包含有效

    arXiv:2403.18742v1 Announce Type: cross  Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potent
    
[^18]: LASIL：学习者感知的长期微观交通仿真监督模仿学习

    LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation

    [https://arxiv.org/abs/2403.17601](https://arxiv.org/abs/2403.17601)

    提出了一种学习者感知的监督模仿学习方法，通过变分自动编码器增强专家状态，以解决多智体模仿学习中的协变量偏移问题

    

    微观交通仿真在交通工程中起着至关重要的作用，通过提供关于单个车辆行为和整体交通流的见解。然而，创建一个真实的模拟器，精确复制各种交通条件下的人类驾驶行为，面临着重大挑战。传统的依赖启发式模型的模拟器往往由于现实世界交通环境的复杂性而无法提供准确的模拟。由于协变量偏移问题，现有的基于模仿学习的模拟器经常无法生成稳定的长期模拟。在本文中，我们提出了一种称为学习者感知的监督模仿学习的新方法，以解决多智体模仿学习中的协变量偏移问题。通过利用变分自动编码器同时建模专家和学习者状态分布，我们的方法增强了专家状态，从而使增强状态意识到

    arXiv:2403.17601v1 Announce Type: new  Abstract: Microscopic traffic simulation plays a crucial role in transportation engineering by providing insights into individual vehicle behavior and overall traffic flow. However, creating a realistic simulator that accurately replicates human driving behaviors in various traffic conditions presents significant challenges. Traditional simulators relying on heuristic models often fail to deliver accurate simulations due to the complexity of real-world traffic environments. Due to the covariate shift issue, existing imitation learning-based simulators often fail to generate stable long-term simulations. In this paper, we propose a novel approach called learner-aware supervised imitation learning to address the covariate shift problem in multi-agent imitation learning. By leveraging a variational autoencoder simultaneously modeling the expert and learner state distribution, our approach augments expert states such that the augmented state is aware 
    
[^19]: 探究将ChatGPT应用于对话教学在脑电图学中的有效性

    Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography

    [https://arxiv.org/abs/2403.16687](https://arxiv.org/abs/2403.16687)

    探究将ChatGPT应用于对话教学在脑电图学中的有效性，研究发现在对话式教学场景中，ChatGPT能够有效履行教学角色，促进学生学习。

    

    近年来，人工智能技术的快速发展，尤其是大型语言模型（LLMs）如ChatGPT的出现，为教育领域的应用带来了显著的前景。 LLM具有解释知识、回答问题和考虑上下文的能力，因此为学生提供对话式教学支持。因此，检验LLM有效履行教学角色的能力，从而在对话教学场景中促进学生学习的能力，类似于人类教育者，是一个非常有价值的研究课题。 本研究招募了34名本科生作为参与者，随机分为两组。 实验组使用ChatGPT进行对话式教学，而控制组与人类教师互动。 两组都学习了信息相关课程“数字图像”的直方图均衡单元。

    arXiv:2403.16687v1 Announce Type: cross  Abstract: In recent years, the rapid development of artificial intelligence technology, especially the emergence of large language models (LLMs) such as ChatGPT, has presented significant prospects for application in the field of education. LLMs possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students. Therefore, an examination of the capacity of LLMs to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic. This research recruited 34 undergraduate students as participants, who were randomly divided into two groups. The experimental group engaged in dialogic teaching using ChatGPT, while the control group interacted with human teachers. Both groups learned the histogram equalization unit in the information-related course "Digital Ima
    
[^20]: 理解马尔科夫逻辑网络中的域大小泛化

    Understanding Domain-Size Generalization in Markov Logic Networks

    [https://arxiv.org/abs/2403.15933](https://arxiv.org/abs/2403.15933)

    本文量化了马尔科夫逻辑网络在不同大小领域间内部一致性缺失的问题，并提出最大化数据对数似然同时最小化参数方差的方式来优化领域大小泛化。

    

    我们研究了马尔科夫逻辑网络（MLNs）在不同大小的关系结构之间的泛化行为。多个研究注意到，在给定域上学习的MLNs在不同大小的域上泛化很差。这种行为源于MLN在不同域大小上使用时的内部一致性缺失。在本文中，我们量化了这种不一致性，并将其限制在MLN参数的方差范围内。参数方差还限制了从不同域大小中取出的MLN边缘分布之间的KL散度。我们利用这些界限展示，最大化数据对数似然同时最小化参数方差，对应于域大小泛化的两个自然概念。我们的理论结果适用于指数随机图和其他基于马尔科夫网络的关系模型。最后，我们观察到已知的解决方案会减少方差

    arXiv:2403.15933v1 Announce Type: new  Abstract: We study the generalization behavior of Markov Logic Networks (MLNs) across relational structures of different sizes. Multiple works have noticed that MLNs learned on a given domain generalize poorly across domains of different sizes. This behavior emerges from a lack of internal consistency within an MLN when used across different domain sizes. In this paper, we quantify this inconsistency and bound it in terms of the variance of the MLN parameters. The parameter variance also bounds the KL divergence between an MLN's marginal distributions taken from different domain sizes. We use these bounds to show that maximizing the data log-likelihood while simultaneously minimizing the parameter variance corresponds to two natural notions of generalization across domain sizes. Our theoretical results apply to Exponential Random Graphs and other Markov network based relational models. Finally, we observe that solutions known to decrease the varia
    
[^21]: 走向值得信赖的人工智能之旅-第一部分：追求务实框架

    The Journey to Trustworthy AI- Part 1: Pursuit of Pragmatic Frameworks

    [https://arxiv.org/abs/2403.15457](https://arxiv.org/abs/2403.15457)

    本文回顾了值得信赖的人工智能（TAI）和其各种定义，主张不应将“负责任的”或“道德的”人工智能等术语视为TAI的替代，而是提倡以公平性、偏见、风险、安全性、可解释性和可靠性等关键属性为中心的方法，认识到地缘政治和地理原因导致的人工智能监管差异对跨国公司构成挑战。

    

    本文回顾了值得信赖的人工智能（TAI）及其各种定义。考虑到任何社会中尊重的原则，TAI通常被一些属性所特征，其中一些属性已导致监管或工程背景下的混淆。我们反对使用诸如“负责任的”或“道德的”人工智能等术语来替代TAI。为了帮助澄清任何混乱，我们建议将它们抛在脑后。鉴于TAI固有的主观性和复杂性，开发一个通用框架被认为是不可行的。相反，我们主张采取以公平性、偏见、风险、安全性、可解释性和可靠性等关键属性为中心的方法。我们审视了正在进行的监管环境，重点关注欧盟、中国和美国的倡议。我们认识到，基于地缘政治和地理原因而不同的人工智能监管对跨国公司构成额外挑战。

    arXiv:2403.15457v1 Announce Type: cross  Abstract: This paper reviews Trustworthy Artificial Intelligence (TAI) and its various definitions. Considering the principles respected in any society, TAI is often characterized by a few attributes, some of which have led to confusion in regulatory or engineering contexts. We argue against using terms such as Responsible or Ethical AI as substitutes for TAI. And to help clarify any confusion, we suggest leaving them behind. Given the subjectivity and complexity inherent in TAI, developing a universal framework is deemed infeasible. Instead, we advocate for approaches centered on addressing key attributes and properties such as fairness, bias, risk, security, explainability, and reliability. We examine the ongoing regulatory landscape, with a focus on initiatives in the EU, China, and the USA. We recognize that differences in AI regulations based on geopolitical and geographical reasons pose an additional challenge for multinational companies. 
    
[^22]: 关于自监督学习的预训练数据多样性

    On Pretraining Data Diversity for Self-Supervised Learning

    [https://arxiv.org/abs/2403.13808](https://arxiv.org/abs/2403.13808)

    增加预训练数据多样性可以提高自监督学习性能，但仅在与下游数据的分布距离较小时有效。

    

    我们探讨了使用更多样化数据集对自监督学习(SSL)性能的影响，这些数据集的特征是唯一样本数量，在固定的计算预算下。我们的研究结果一致表明，增加预训练数据的多样性可以提高SSL性能，尽管只有当与下游数据的分布距离很小的时候才是如此。值得注意的是，即使通过网络爬虫或扩散生成的数据等方式实现了异常大的预训练数据多样性，分布转移仍然是一个挑战。我们的实验涵盖了七种SSL方法，使用了诸如ImageNet和YFCC100M等大规模数据集，总计超过200个GPU天。代码和训练模型将在https://github.com/hammoudhasan/DiversitySSL 上提供。

    arXiv:2403.13808v1 Announce Type: cross  Abstract: We explore the impact of training with more diverse datasets, characterized by the number of unique samples, on the performance of self-supervised learning (SSL) under a fixed computational budget. Our findings consistently demonstrate that increasing pretraining data diversity enhances SSL performance, albeit only when the distribution distance to the downstream data is minimal. Notably, even with an exceptionally large pretraining data diversity achieved through methods like web crawling or diffusion-generated data, among other ways, the distribution shift remains a challenge. Our experiments are comprehensive with seven SSL methods using large-scale datasets such as ImageNet and YFCC100M amounting to over 200 GPU days. Code and trained models will be available at https://github.com/hammoudhasan/DiversitySSL .
    
[^23]: 自然语言作为政策：与LLMs一起进行坐标级体态控制的推理

    Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs

    [https://arxiv.org/abs/2403.13801](https://arxiv.org/abs/2403.13801)

    用自然语言推理进行坐标级控制，能够显著提高机器人行动规划的成功率，并且具有将机器人技能迁移到新任务的潜力。

    

    我们展示了与LLMs一起解决机器人行动规划问题的实验结果。最近，LLMs已经应用于机器人行动规划，特别是使用代码生成方法将复杂的高级指令转换为中级策略代码。相比之下，我们的方法通过获取任务和场景对象的文本描述，通过自然语言推理制定行动规划，并输出坐标级控制命令，从而减少了作为政策的中间表示代码的必要性。我们的方法在多模态提示仿真基准上进行了评估，表明我们的自然语言推理实验显著提高了成功率，与缺席相比。此外，我们的方法展示了自然语言描述有潜力将机器人技能从已知任务转移到以前未见任务。

    arXiv:2403.13801v1 Announce Type: cross  Abstract: We demonstrate experimental results with LLMs that address robotics action planning problems. Recently, LLMs have been applied in robotics action planning, particularly using a code generation approach that converts complex high-level instructions into mid-level policy codes. In contrast, our approach acquires text descriptions of the task and scene objects, then formulates action planning through natural language reasoning, and outputs coordinate level control commands, thus reducing the necessity for intermediate representation code as policies. Our approach is evaluated on a multi-modal prompt simulation benchmark, demonstrating that our prompt engineering experiments with natural language reasoning significantly enhance success rates compared to its absence. Furthermore, our approach illustrates the potential for natural language descriptions to transfer robotics skills from known tasks to previously unseen tasks.
    
[^24]: 基于缓存增强的终身多智能体路径规划

    Caching-Augmented Lifelong Multi-Agent Path Finding

    [https://arxiv.org/abs/2403.13421](https://arxiv.org/abs/2403.13421)

    CAL-MAPF引入了缓存机制来增强终身多智能体路径规划的性能，通过合适的输入任务分布、高缓存命中率和流畅的交通这三个因素显著提高了解决方案的稳定性。

    

    多智能体路径规划 (Multi-Agent Path Finding，MAPF) 在各种应用中至关重要，涉及为多个机器人找到无碰撞路径。终身MAPF会在代理完成其初始目标后立即将目标重新分配给代理，提供对实际仓库规划的更准确逼近。本文提出了一种名为缓存增强终身MAPF (CAL-MAPF)的新机制，旨在提高终身MAPF的性能。我们开发了一种称为缓存的临时物品存储和替换的新类型地图网格，并为其设计了一种锁定机制，以提高规划解决方案的稳定性。该缓存机制经过各种缓存替换策略和一系列输入任务分布的评估。我们通过实验确定了三个显著影响CAL-MAPF性能的主要因素：合适的输入任务分布、高缓存命中率和流畅的交通。

    arXiv:2403.13421v2 Announce Type: replace-cross  Abstract: Multi-Agent Path Finding (MAPF), which involves finding collision-free paths for multiple robots, is crucial in various applications. Lifelong MAPF, where targets are reassigned to agents as soon as they complete their initial targets, offers a more accurate approximation of real-world warehouse planning. In this paper, we present a novel mechanism named Caching-Augmented Lifelong MAPF (CAL-MAPF), designed to improve the performance of Lifelong MAPF. We have developed a new type of map grid called cache for temporary item storage and replacement, and designed a locking mechanism for it to improve the stability of the planning solution. This cache mechanism was evaluated using various cache replacement policies and a spectrum of input task distributions. We identified three main factors significantly impacting CAL-MAPF performance through experimentation: suitable input task distribution, high cache hit rate, and smooth traffic.
    
[^25]: 机器人运动规划的联邦强化学习与零次通用化

    Federated reinforcement learning for robot motion planning with zero-shot generalization

    [https://arxiv.org/abs/2403.13245](https://arxiv.org/abs/2403.13245)

    该论文提出了一个联邦强化学习框架，实现了机器人运动规划中的零次通用化，通过协作学习多个学习者和中央服务器，在不共享原始数据的情况下达到全局最优解。

    

    本文考虑了使用零次通用化学习控制策略进行机器人运动规划的问题，即在部署学习策略到新环境时不需要数据收集和策略调整。我们开发了一个联邦强化学习框架，实现了多个学习者和中央服务器（云端）的协作学习，而不分享原始数据。在每次迭代中，每个学习者将其本地控制策略和相应的估计归一化到达时间上传至云端，然后云端在学习者间计算全局最优并将最优策略广播给学习者。每个学习者然后在下一次迭代中从其本地控制策略和云端中选择。提出的框架利用了到达时间和安全性的零次通用化保证。对于几乎必然收敛，几乎一致性，Pare的理论保证//}

    arXiv:2403.13245v1 Announce Type: cross  Abstract: This paper considers the problem of learning a control policy for robot motion planning with zero-shot generalization, i.e., no data collection and policy adaptation is needed when the learned policy is deployed in new environments. We develop a federated reinforcement learning framework that enables collaborative learning of multiple learners and a central server, i.e., the Cloud, without sharing their raw data. In each iteration, each learner uploads its local control policy and the corresponding estimated normalized arrival time to the Cloud, which then computes the global optimum among the learners and broadcasts the optimal policy to the learners. Each learner then selects between its local control policy and that from the Cloud for next iteration. The proposed framework leverages on the derived zero-shot generalization guarantees on arrival time and safety. Theoretical guarantees on almost-sure convergence, almost consensus, Pare
    
[^26]: 具有非私密预处理的可证明隐私

    Provable Privacy with Non-Private Pre-Processing

    [https://arxiv.org/abs/2403.13041](https://arxiv.org/abs/2403.13041)

    提出了一个框架，能够评估非私密数据相关预处理算法引起的额外隐私成本，并利用平滑DP和预处理算法的有界敏感性建立整体隐私保证的上限

    

    当分析差分私密（DP）机器学习管道时，通常会忽略数据相关的预处理的潜在隐私成本。在这项工作中，我们提出了一个通用框架，用于评估由非私密数据相关预处理算法引起的额外隐私成本。我们的框架通过利用两个新的技术概念建立了整体隐私保证的上限：一种称为平滑DP的DP变体以及预处理算法的有界敏感性。

    arXiv:2403.13041v1 Announce Type: cross  Abstract: When analysing Differentially Private (DP) machine learning pipelines, the potential privacy cost of data-dependent pre-processing is frequently overlooked in privacy accounting. In this work, we propose a general framework to evaluate the additional privacy cost incurred by non-private data-dependent pre-processing algorithms. Our framework establishes upper bounds on the overall privacy guarantees by utilising two new technical notions: a variant of DP termed Smooth DP and the bounded sensitivity of the pre-processing algorithms. In addition to the generic framework, we provide explicit overall privacy guarantees for multiple data-dependent pre-processing algorithms, such as data imputation, quantization, deduplication and PCA, when used in combination with several DP algorithms. Notably, this framework is also simple to implement, allowing direct integration into existing DP pipelines.
    
[^27]: DetToolChain：一种释放MLLM检测能力的新提示范式

    DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM

    [https://arxiv.org/abs/2403.12488](https://arxiv.org/abs/2403.12488)

    DetToolChain提出了一种新的提示范式，可以释放MLLM的零拍摄物体检测能力，并通过检测链式思维自动化任务分解和逐步框细化规划。

    

    我们提出DetToolChain，一种新颖的提示范式，用于释放多模态大语言模型（MLLMs）的零拍摄物体检测能力，如GPT-4V和Gemini。我们的方法包括一个受高精度检测先验启发的检测提示工具包和一个实现这些提示的新Chain-of-Thought。具体来说，工具包中的提示旨在引导MLLM集中在区域信息上（例如，放大），按照测量标准阅读坐标（例如，叠加标尺和指南针），并从上下文信息中推断（例如，叠加场景图）。基于这些工具，新的检测Chain-of-Thought可以自动将任务分解为简单的子任务，诊断预测，并规划逐步框细化。我们的框架的有效性在一系列检测任务中得到了证实，特别是在困难情况下。与现有的最先进技术相比

    arXiv:2403.12488v1 Announce Type: cross  Abstract: We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot object detection ability of multimodal large language models (MLLMs), such as GPT-4V and Gemini. Our approach consists of a detection prompting toolkit inspired by high-precision detection priors and a new Chain-of-Thought to implement these prompts. Specifically, the prompts in the toolkit are designed to guide the MLLM to focus on regional information (e.g., zooming in), read coordinates according to measure standards (e.g., overlaying rulers and compasses), and infer from the contextual information (e.g., overlaying scene graphs). Building upon these tools, the new detection chain-of-thought can automatically decompose the task into simple subtasks, diagnose the predictions, and plan for progressive box refinements. The effectiveness of our framework is demonstrated across a spectrum of detection tasks, especially hard cases. Compared to existing state-of-
    
[^28]: 使用大型语言模型纠正社交媒体上的错误信息

    Correcting misinformation on social media with a large language model

    [https://arxiv.org/abs/2403.11169](https://arxiv.org/abs/2403.11169)

    提出了一种名为MUSE的大型语言模型，通过访问最新信息并评估可信度，以解决社交媒体上误信息纠正的难题。

    

    误信息会破坏公众对科学和民主的信任，特别是在社交媒体上，不准确信息会迅速传播。专家和普通人通过手动识别和解释不准确信息已经被证明是有效的纠正误信息的方法。然而，这种方法很难扩展，这是一个担忧，因为大型语言模型（LLMs）等技术使误信息更容易生成。LLMs还具有多功能能力，可以加速纠正误信息；然而，它们由于缺乏最新信息、倾向于生成似是而非的内容和引用以及无法处理多模态信息而面临困难。为了解决这些问题，我们提出了MUSE，这是一个带有最新信息访问和可信度评估的LLM。通过检索上下文证据和反驳，MUSE可以提供准确可信的解释和参考。它还描述

    arXiv:2403.11169v1 Announce Type: cross  Abstract: Misinformation undermines public trust in science and democracy, particularly on social media where inaccuracies can spread rapidly. Experts and laypeople have shown to be effective in correcting misinformation by manually identifying and explaining inaccuracies. Nevertheless, this approach is difficult to scale, a concern as technologies like large language models (LLMs) make misinformation easier to produce. LLMs also have versatile capabilities that could accelerate misinformation correction; however, they struggle due to a lack of recent information, a tendency to produce plausible but false content and references, and limitations in addressing multimodal information. To address these issues, we propose MUSE, an LLM augmented with access to and credibility evaluation of up-to-date information. By retrieving contextual evidence and refutations, MUSE can provide accurate and trustworthy explanations and references. It also describes 
    
[^29]: 从临床文本中提取生物医学概念预测患者的再入院情况

    Prediction of readmission of patients by extracting biomedical concepts from clinical texts

    [https://arxiv.org/abs/2403.09722](https://arxiv.org/abs/2403.09722)

    从临床文本中提取生物医学概念预测患者的再入院情况，可以帮助医生选择适当的治疗方法，从而减少患者再次入院的比率，实现有效的治疗成本降低。

    

    如今，存在大量的电子健康数据为进行旨在改善为患者提供的医疗服务并降低医疗系统成本的研究创造了潜在能力。近年来医学领域备受关注的一个话题是识别出刚从医院出院后可能很快再次入院的患者。这种识别可以帮助医生选择适当的治疗方法，从而减少患者再次入院的比率，实现有效的治疗成本降低。本研究讨论了利用文本挖掘方法和对患者电子文件中的出院报告文本进行处理来预测患者再次入院情况。为此，使用两种方法评估了各种机器学习模型的性能：词袋模型和概念袋模型。

    arXiv:2403.09722v1 Announce Type: cross  Abstract: Today, the existence of a vast amount of electronic health data has created potential capacities for conducting studies aiming to improve the medical services provided to patients and reduce the costs of the healthcare system. One of the topics that has been receiving attention in the field of medicine in recent years is the identification of patients who are likely to be re-hospitalized shortly after being discharged from the hospital. This identification can help doctors choose appropriate treatment methods, thereby reducing the rate of patient re-hospitalization and resulting in effective treatment cost reduction. In this study, the prediction of patient re-hospitalization using text mining approaches and the processing of discharge report texts in the patient's electronic file has been discussed. To this end, the performance of various machine learning models has been evaluated using two approaches: bag of word and bag of concept, 
    
[^30]: 预排序Tsetlin机器（基因K-Medoid方法）

    Pre-Sorted Tsetlin Machine (The Genetic K-Medoid Method)

    [https://arxiv.org/abs/2403.09680](https://arxiv.org/abs/2403.09680)

    该论文提出了一种利用Tsetlin Machines进行传统监督学习的机器学习预排序阶段方法，在MNIST级别的分类问题上取得了显著的精度提升，以及训练时间和推理时间大幅度减少。

    

    本文提出了一种利用Tsetlin Machines进行传统监督学习的机器学习预排序阶段。首先，利用快速遗传算法从数据集中确定N个数据点，以解决最大离散化问题。然后，这些被用作运行K-Medoid聚类算法的初始放置。最后，利用快速遗传算法通过最大化汉明距离来对齐N个独立的Tsetlin Machines。对于MNIST级别的分类问题，结果显示准确度提高了高达10％，训练时间减少了约383倍，推理时间减少了约86倍。

    arXiv:2403.09680v1 Announce Type: cross  Abstract: This paper proposes a machine learning pre-sort stage to traditional supervised learning using Tsetlin Machines. Initially, N data-points are identified from the dataset using an expedited genetic algorithm to solve the maximum dispersion problem. These are then used as the initial placement to run the K-Medoid clustering algorithm. Finally, an expedited genetic algorithm is used to align N independent Tsetlin Machines by maximising hamming distance. For MNIST level classification problems, results demonstrate up to 10% improvement in accuracy, approx. 383X reduction in training time and approx. 86X reduction in inference time.
    
[^31]: Keyformer：通过关键标记选择减少KV缓存以实现高效的生成推断

    Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference

    [https://arxiv.org/abs/2403.09054](https://arxiv.org/abs/2403.09054)

    本文提出了一种名为“Keyformer”的创新推断时间方法，旨在通过选择关键标记来减少KV缓存的挑战，提高内存带宽利用率。

    

    Transformer已经成为大型语言模型(LLMs)的基础架构。在生成语言模型中，推断过程涉及两个主要阶段：提示处理和标记生成。标记生成，构成了大部分计算工作量，主要涉及向量-矩阵乘法和与键-值(KV)缓存交互。由于从存储系统传输权重和KV缓存值到计算单元的开销，这一阶段受到内存带宽的限制。这种内存瓶颈在需要长上下文和大量文本生成的应用中尤为突出，这两者对LLMs越来越重要。  本文介绍了一种创新的推断时间方法“Keyformer”，以缓解与KV缓存大小和内存带宽利用相关的挑战。Keyformer利用了这样的观察结果，大约90

    arXiv:2403.09054v1 Announce Type: cross  Abstract: Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.   This paper introduces "Keyformer", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90
    
[^32]: 通过代码探索大型语言模型的安全泛化挑战

    Exploring Safety Generalization Challenges of Large Language Models via Code

    [https://arxiv.org/abs/2403.07865](https://arxiv.org/abs/2403.07865)

    本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。

    

    大型语言模型（LLMs）的快速发展带来了自然语言处理方面的显著能力，但也引发了人们对它们潜在误用的担忧。本文引入了CodeAttack，一个将自然语言输入转换为代码输入的框架，为测试LLMs的安全泛化提供了一个新颖的环境。我们对包括GPT-4、Claude-2和Llama-2系列在内的最新LLMs进行了全面研究，发现这些模型对于代码输入存在共同的安全漏洞：CodeAttack在超过80%的时间内始终绕过所有模型的安全保护。

    arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
    
[^33]: PEEB：具有可解释和可编辑语言瓶颈的基于部分的图像分类器

    PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck

    [https://arxiv.org/abs/2403.05297](https://arxiv.org/abs/2403.05297)

    PEEB是一种基于部分的图像分类器，通过将类别名称转换为描述视觉部分的文本描述符，并将检测到的部分的嵌入与文本描述符匹配，从而在零样本设置中表现出色，并且不仅在监督学习中表现出色，而且还首次实现用户编辑类定义形成新分类器无需重新训练。

    

    基于CLIP的分类器依赖于包含{text encoder已知的类名称}的提示。也就是说，CLIP在新类别或其名称很少在互联网上出现的类别（例如鸟类的学名）上表现不佳。针对细粒度分类，我们提出了PEEB - 一种可解释和可编辑的分类器，用于（1）将类别名称表达为一组预定义的描述视觉部分的文本描述符；和（2）将检测到的部分的嵌入与每个类别中的文本描述符进行匹配，以计算用于分类的逻辑分数。在一个零样本设置中，其中类别名称是未知的，PEEB在准确性上大幅优于CLIP（约为10倍）。与基于部分的分类器相比，PEEB不仅在监督学习设置上是最先进的（88.80%准确率），而且还是第一个能够让用户编辑类定义以形成新的分类器而无需重新训练的分类器。

    arXiv:2403.05297v1 Announce Type: cross  Abstract: CLIP-based classifiers rely on the prompt containing a {class name} that is known to the text encoder. That is, CLIP performs poorly on new classes or the classes whose names rarely appear on the Internet (e.g., scientific names of birds). For fine-grained classification, we propose PEEB - an explainable and editable classifier to (1) express the class name into a set of pre-defined text descriptors that describe the visual parts of that class; and (2) match the embeddings of the detected parts to their textual descriptors in each class to compute a logit score for classification. In a zero-shot setting where the class names are unknown, PEEB outperforms CLIP by a large margin (~10x in accuracy). Compared to part-based classifiers, PEEB is not only the state-of-the-art on the supervised-learning setting (88.80% accuracy) but also the first to enable users to edit the class definitions to form a new classifier without retraining. Compar
    
[^34]: 限制贝叶斯神经网络

    Restricted Bayesian Neural Network

    [https://arxiv.org/abs/2403.04810](https://arxiv.org/abs/2403.04810)

    本研究提出了限制贝叶斯神经网络的新架构，显著减少了网络存储空间复杂性，并引入了一种能够有效处理不确定性的算法，确保在目标函数缺乏完美凸性时稳健地收敛至全局最优解。

    

    现代深度学习工具在解决复杂问题方面非常有效。然而，它们作为黑盒模型的运行方式增加了预测的不确定性。此外，它们面临着各种挑战，包括在大型网络中需要大量存储空间、过拟合、欠拟合、梯度消失等问题。本研究探讨了贝叶斯神经网络的概念，提出了一种能够显著减少网络存储空间复杂性的新型架构。此外，我们介绍了一种能够有效处理不确定性的算法，确保稳健的收敛值，避免陷入局部最优解，尤其是当目标函数缺乏完美的凸性时。

    arXiv:2403.04810v1 Announce Type: cross  Abstract: Modern deep learning tools are remarkably effective in addressing intricate problems. However, their operation as black-box models introduces increased uncertainty in predictions. Additionally, they contend with various challenges, including the need for substantial storage space in large networks, issues of overfitting, underfitting, vanishing gradients, and more. This study explores the concept of Bayesian Neural Networks, presenting a novel architecture designed to significantly alleviate the storage space complexity of a network. Furthermore, we introduce an algorithm adept at efficiently handling uncertainties, ensuring robust convergence values without becoming trapped in local optima, particularly when the objective function lacks perfect convexity.
    
[^35]: 通过混合和完善过去来不断演进记忆

    Ever-Evolving Memory by Blending and Refining the Past

    [https://arxiv.org/abs/2403.04787](https://arxiv.org/abs/2403.04787)

    提出了一种新颖的长期对话记忆方案CREEM，通过混合过去记忆并引入完善过程来实现聊天机器人回应的整体改进和连贯性。

    

    对于类似人类的聊天机器人，构建长期记忆至关重要。构建记忆的一个天真方法可能只是列出总结的对话。然而，当说话者的状态随时间变化时，这样做可能会导致问题，并积累矛盾信息。记忆保持有组织对于降低回应生成器的混乱很重要。在本文中，我们提出了一种新颖的长期对话记忆方案，CREEM。与仅基于当前对话构建记忆的现有方法不同，我们提出的模型在记忆形成过程中混合过去的记忆。此外，我们引入了完善过程来处理多余或过时信息。这种创新性方法通过确保一个更加知情和动态演变的长期记忆，旨在提高聊天机器人回应的整体改进和连贯性。

    arXiv:2403.04787v1 Announce Type: cross  Abstract: For a human-like chatbot, constructing a long-term memory is crucial. A naive approach for making a memory could be simply listing the summarized dialogue. However, this can lead to problems when the speaker's status change over time and contradictory information gets accumulated. It is important that the memory stays organized to lower the confusion for the response generator. In this paper, we propose a novel memory scheme for long-term conversation, CREEM. Unlike existing approaches that construct memory based solely on current sessions, our proposed model blending past memories during memory formation. Additionally, we introduce refining process to handle redundant or outdated information. This innovative approach seeks for overall improvement and coherence of chatbot responses by ensuring a more informed and dynamically evolving long-term memory.
    
[^36]: IRCoder: 中间表示使语言模型成为稳健的多语言代码生成器

    IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators

    [https://arxiv.org/abs/2403.03894](https://arxiv.org/abs/2403.03894)

    通过利用编译器中间表示来改进代码-LMs的多语言能力和促进跨语言转移。

    

    arXiv:2403.03894v1 公告类型: 新的 摘要: 代码理解和生成已迅速成为语言模型（LMs）最受欢迎的应用之一。然而，与自然语言LM的研究相比，对代码-LMs（即用于代码生成的LMs）的多语言方面的研究，如不同编程语言之间的跨语言转移，特定于语言的数据增强以及事后LM调整，以及利用原始文本内容之外的数据源，要稀少得多。特别是，大多数主流代码-LMs仅在源代码文件上进行了预训练。在这项工作中，我们研究了利用现成的编译器中间表示（跨编程语言共享）来改进代码-LMs的多语言能力并促进跨语言转移的前景。为此，我们首先编制了SLTrans，一个由近400万个自包含源代码文件组成的并行数据集。

    arXiv:2403.03894v1 Announce Type: new  Abstract: Code understanding and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.   To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled wi
    
[^37]: 通用到专业的电子商务LLMs翻译

    General2Specialized LLMs Translation for E-commerce

    [https://arxiv.org/abs/2403.03689](https://arxiv.org/abs/2403.03689)

    提出了一个名为G2ST的两步微调范式，通过自对比语义增强将通用NMT模型转换为专门用于电子商务的NMT模型，以提高翻译质量。

    

    现有的神经机器翻译（NMT）模型主要处理通用领域的翻译，忽略了具有特殊写作公式的领域，比如电子商务和法律文件。以电子商务为例，文本通常包含大量领域相关词汇，并且存在更多的语法问题，这导致当前NMT方法的性能较差。为解决这些问题，我们收集了两个与领域相关的资源，包括一组术语对（对齐的中英双语术语）和一个针对电子商务领域进行注释的平行语料库。此外，我们提出了一个两步微调范式（名为G2ST），其中包括自对比语义增强，以将一个通用NMT模型转换为专门用于电子商务的NMT模型。该范式适用于基于大型语言模型（LLMs）的NMT模型。对真实电子商务标题的广泛评估表明了卓越的翻译质量。

    arXiv:2403.03689v1 Announce Type: cross  Abstract: Existing Neural Machine Translation (NMT) models mainly handle translation in the general domain, while overlooking domains with special writing formulas, such as e-commerce and legal documents. Taking e-commerce as an example, the texts usually include amounts of domain-related words and have more grammar problems, which leads to inferior performances of current NMT methods. To address these problems, we collect two domain-related resources, including a set of term pairs (aligned Chinese-English bilingual terms) and a parallel corpus annotated for the e-commerce domain. Furthermore, we propose a two-step fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general NMT model to the specialized NMT model for e-commerce. The paradigm can be used for the NMT models based on Large language models (LLMs). Extensive evaluations on real e-commerce titles demonstrate the superior translation quality and 
    
[^38]: Polynormer: 多项式表达的线性时间图转换器

    Polynormer: Polynomial-Expressive Graph Transformer in Linear Time

    [https://arxiv.org/abs/2403.01232](https://arxiv.org/abs/2403.01232)

    Polynormer提出了一种多项式表达GT模型，具有线性复杂度，结合本地和全局等变注意力模型，平衡了表现力和可扩展性。

    

    图转换器（GTs）已经成为一种有前途的架构，理论上它比消息传递图神经网络（GNNs）更具表现力。然而，典型的GT模型至少具有二次复杂度，因此无法扩展到大型图。虽然最近提出了几种线性GTs，但它们在几个热门图数据集上仍落后于GNN对应模型，这对于它们的实际表现力构成了一个重要关注点。为了平衡GTs的表现力和可扩展性之间的权衡，我们提出了Polynormer，一个具有线性复杂度的多项式表达GT模型。Polynormer构建在一个新颖的基础模型上，该模型在输入特征上学习高次多项式。为了使基础模型具有置换等变性，我们将其与图拓扑和节点特征分开集成，从而产生本地和全局等变关注模型。因此，Polynormer采用了线性的局部到全局关注方案。

    arXiv:2403.01232v1 Announce Type: cross  Abstract: Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently proposed, they still lag behind GNN counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomial-expressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme t
    
[^39]: 路线推荐综述：方法、应用和机会

    A Survey of Route Recommendations: Methods, Applications, and Opportunities

    [https://arxiv.org/abs/2403.00284](https://arxiv.org/abs/2403.00284)

    基于城市计算的路线推荐综述对路线推荐研究中的传统机器学习和现代深度学习方法进行了分类，展示了与城市计算场景相关的新应用，并揭示了最新进展。

    

    现今，随着先进的信息技术部署在整个城市，大量数据和强大的计算资源正在使现代城市发展智能化。作为智能交通的重要组成部分，路线推荐及其应用被广泛使用，直接影响市民的出行习惯。基于大数据（可能是多模式）开发智能高效的出行路线已成为路线推荐研究的核心挑战。我们的综述对基于城市计算的路线推荐工作进行了全面回顾。它分为以下三个部分：1）方法论。我们对大量传统机器学习和现代深度学习方法进行分类。同时，我们讨论它们的历史关系并揭示最新进展。2）应用方面。我们展示了大量与城市计算场景中路线推荐相关的新应用。3）我们迪

    arXiv:2403.00284v1 Announce Type: new  Abstract: Nowadays, with advanced information technologies deployed citywide, large data volumes and powerful computational resources are intelligentizing modern city development. As an important part of intelligent transportation, route recommendation and its applications are widely used, directly influencing citizens` travel habits. Developing smart and efficient travel routes based on big data (possibly multi-modal) has become a central challenge in route recommendation research. Our survey offers a comprehensive review of route recommendation work based on urban computing. It is organized by the following three parts: 1) Methodology-wise. We categorize a large volume of traditional machine learning and modern deep learning methods. Also, we discuss their historical relations and reveal the edge-cutting progress. 2) Application\-wise. We present numerous novel applications related to route commendation within urban computing scenarios. 3) We di
    
[^40]: 跨领域的中文句式结构解析

    Cross-domain Chinese Sentence Pattern Parsing

    [https://arxiv.org/abs/2402.16311](https://arxiv.org/abs/2402.16311)

    本文提出了一种利用大型语言模型进行自我训练的创新方法，通过动态生成训练数据将源领域句法规则与目标领域句子相结合，增强句式结构解析器对各种领域的适应能力，实验证明其在教科书和新闻领域的效果优于基于规则的基准模型1.68个百分点。

    

    arXiv:2402.16311v1 公告类型: 跨领域 句式结构（SPS）解析是一种主要用于语言教学的句法分析方法。现有的SPS解析器主要依赖于教科书语料库进行训练，缺乏跨领域能力。为了克服这一限制，本文提出了一种创新方法，利用大型语言模型（LLMs）在自我训练框架内。从源领域中提取部分句法规则，与目标领域句子结合动态生成训练数据，增强了解析器对不同领域的适应能力。在教科书和新闻领域进行的实验表明，所提出的方法效果显著，F1指标比基于规则的基准模型高出1.68个百分点。

    arXiv:2402.16311v1 Announce Type: cross  Abstract: Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.To overcome this constraint, this paper proposes an innovative approach leveraging large language models (LLMs) within a self-training framework. Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics.
    
[^41]: 轨迹式迭代强化学习框架用于自动竞标

    Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding

    [https://arxiv.org/abs/2402.15102](https://arxiv.org/abs/2402.15102)

    自动广告竞标中使用了一种新的迭代离线强化学习框架，有效缓解了传统RL算法在在线环境下性能下降的问题。

    

    在在线广告中，广告主参与广告竞拍以获取广告机会，通常是通过需求方平台(DSPs)提供的自动竞标工具。目前的自动竞标算法通常采用强化学习（RL）。然而，由于安全性问题，大多数基于RL的自动竞标策略是在模拟环境中进行训练的，在在线环境中部署会导致性能下降。为了缩小这一差距，我们可以并行部署多个自动竞标代理以收集大量交互数据集。然后，可以利用离线RL算法训练新策略。训练后的策略随后可以部署以进行进一步的数据收集，从而形成一个迭代训练框架，我们将其称为迭代离线RL。在这项工作中，我们确定了这种迭代离线RL框架的性能瓶颈，其根源在于由于内在原因而导致的探索和利用的低效问题。

    arXiv:2402.15102v1 Announce Type: cross  Abstract: In online advertising, advertisers participate in ad auctions to acquire ad opportunities, often by utilizing auto-bidding tools provided by demand-side platforms (DSPs). The current auto-bidding algorithms typically employ reinforcement learning (RL). However, due to safety concerns, most RL-based auto-bidding policies are trained in simulation, leading to a performance degradation when deployed in online environments. To narrow this gap, we can deploy multiple auto-bidding agents in parallel to collect a large interaction dataset. Offline RL algorithms can then be utilized to train a new policy. The trained policy can subsequently be deployed for further data collection, resulting in an iterative training framework, which we refer to as iterative offline RL. In this work, we identify the performance bottleneck of this iterative offline RL framework, which originates from the ineffective exploration and exploitation caused by the inhe
    
[^42]: IEPile: 挖掘大规模基于模式的信息抽取语料库

    IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus

    [https://arxiv.org/abs/2402.14710](https://arxiv.org/abs/2402.14710)

    发布了IEPile，一个包含约0.32B个标记的综合双语IE指令语料库，通过收集和清理33个现有IE数据集并引入基于模式的指令生成，可以提高大型语言模型在信息抽取领域的性能，尤其是零样本泛化。

    

    大型语言模型（LLMs）在各个领域展现出了显著的潜力；然而，在信息抽取（IE）方面表现出了显著的性能差距。高质量的指令数据是提升LLMs特定能力的关键，而当前的IE数据集往往规模较小、分散且缺乏标准化的模式。因此，我们介绍了IEPile，一个综合的双语（英文和中文）IE指令语料库，包含约0.32B个标记。我们通过收集和清理33个现有IE数据集构建IEPile，并引入基于模式的指令生成来挖掘大规模语料库。在LLaMA和Baichuan上的实验结果表明，使用IEPile可以提高LLMs在IE方面的性能，尤其是零样本泛化。我们开源了资源和预训练模型，希望为自然语言处理社区提供有价值的支持。

    arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
    
[^43]: 用大型语言模型从头开始辅助撰写类似维基百科文章

    Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models

    [https://arxiv.org/abs/2402.14207](https://arxiv.org/abs/2402.14207)

    提出了一种名为STORM的写作系统，用于通过检索和多视角提问合成主题概要，以辅助从头开始写类似维基百科的文章。

    

    我们研究如何应用大型语言模型从头开始撰写基于事实和有条理的长篇文章，使其在广度和深度上与维基百科页面可媲美。这一尚未深入研究的问题在撰写前阶段提出了新的挑战，包括如何研究主题并准备大纲以便撰写。我们提出了STORM，一个用于通过检索和多视角提问进行主题概要合成的写作系统。STORM模拟了撰写前阶段，其中（1）发现研究给定主题的多样化观点，（2）模拟会话，撰写持有不同观点的作者向基于可信互联网来源的主题专家提问，（3）整理收集到的信息以创建大纲。为了评估，我们整理了FreshWiki，一个包含最新高质量维基百科文章的数据集，并制定了大纲评估指标以评估撰写前阶段。

    arXiv:2402.14207v1 Announce Type: cross  Abstract: We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.   For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from 
    
[^44]: 重塑RLHF中的信息结构：基于图论的奖励泛化视角

    Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective

    [https://arxiv.org/abs/2402.10184](https://arxiv.org/abs/2402.10184)

    本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。

    

    在强化学习从人类反馈中（RLHF）存在一个三难问题：高度多样的环境、低标注成本和可靠的对齐性能之间的不兼容性。本文旨在通过设计奖励建模过程中的数据集信息结构来缓解这种不兼容性。具体而言，我们重新审视了RLHF过程，并提出了一个理论框架将其描绘为文本分布上的自动编码过程。我们的框架形式化了RLHF目标，即确保人类偏好与大型语言模型（LLM）行为之间的分布一致性。基于这个框架，我们系统地研究了RLHF奖励建模阶段中信息结构的性能影响。为了进一步理解奖励建模阶段中的奖励泛化，我们引入了一种基于随机图论的方法来建模语义空间中的泛化。其中的关键见解是...

    arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
    
[^45]: 调谐：在临床设置中使用有限数据的音频分类器性能分析

    Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data

    [https://arxiv.org/abs/2402.10100](https://arxiv.org/abs/2402.10100)

    本研究评估了在临床设置中使用深度学习模型进行音频分类的效果，并发现在微调之前，预训练模型在大数据集上的性能对临床数据的影响较好。研究结果表明，CNN模型可以在小数据集环境中与转换模型相媲美或超越。

    

    本研究评估了在临床设置中使用深度学习模型进行音频分类的效果，限制条件是以反映实际世界数据收集的小数据集为基础。我们分析了包括DenseNet和ConvNeXt在内的CNN模型，以及ViT、SWIN和AST等转换模型，并将它们与诸如YAMNet和VGGish的预训练音频模型进行比较。我们的方法强调了在特定临床数据上微调之前，在大数据集上进行预训练的好处。我们从卒中患者中新收集了两个前所未有的患者音频数据集。我们研究了各种预处理技术，发现基于它们从预训练中学习到的先验知识，RGB和灰度谱图转换对模型性能产生了不同的影响。我们的研究结果表明，CNN模型在小数据集环境中可以与转换模型相媲美或超越，其中DenseNet-Contrastive和AST模型表现突出。本研究强调了...

    arXiv:2402.10100v1 Announce Type: cross  Abstract: This study assesses deep learning models for audio classification in a clinical setting with the constraint of small datasets reflecting real-world prospective data collection. We analyze CNNs, including DenseNet and ConvNeXt, alongside transformer models like ViT, SWIN, and AST, and compare them against pre-trained audio models such as YAMNet and VGGish. Our method highlights the benefits of pre-training on large datasets before fine-tuning on specific clinical data. We prospectively collected two first-of-their-kind patient audio datasets from stroke patients. We investigated various preprocessing techniques, finding that RGB and grayscale spectrogram transformations affect model performance differently based on the priors they learn from pre-training. Our findings indicate CNNs can match or exceed transformer models in small dataset contexts, with DenseNet-Contrastive and AST models showing notable performance. This study highlights
    
[^46]: OrderBkd: 通过重新定位进行的文本后门攻击

    OrderBkd: Textual backdoor attack through repositioning

    [https://arxiv.org/abs/2402.07689](https://arxiv.org/abs/2402.07689)

    本论文提出了一种通过重新定位句子中的两个单词实施文本后门攻击的方法，与已有的攻击方式相比，在攻击成功率、困惑度和与干净样本的语义相似性方面表现更好，并且对ONION防御方法具有鲁棒性。

    

    使用第三方数据集和预训练的机器学习模型对NLP系统构成威胁，可能隐藏后门攻击。现有的攻击方式包括插入标记或句子重述等污染数据样本，这要么改变了原始文本的语义，要么可以被检测出来。我们与以往工作的主要区别在于，我们使用重新定位句子中的两个单词作为触发器。通过设计并应用基于词性的规则来选择这些标记，我们在SST-2和AG分类数据集上保持了高攻击成功率，同时在困惑度和与干净样本的语义相似性方面优于现有攻击方法。此外，我们展示了我们的攻击对ONION防御方法的鲁棒性。论文中的所有代码和数据可在https://github.com/alekseevskaia/OrderBkd获取。

    The use of third-party datasets and pre-trained machine learning models poses a threat to NLP systems due to possibility of hidden backdoor attacks. Existing attacks involve poisoning the data samples such as insertion of tokens or sentence paraphrasing, which either alter the semantics of the original texts or can be detected. Our main difference from the previous work is that we use the reposition of a two words in a sentence as a trigger. By designing and applying specific part-of-speech (POS) based rules for selecting these tokens, we maintain high attack success rate on SST-2 and AG classification datasets while outperforming existing attacks in terms of perplexity and semantic similarity to the clean samples. In addition, we show the robustness of our attack to the ONION defense method. All the code and data for the paper can be obtained at https://github.com/alekseevskaia/OrderBkd.
    
[^47]: 动态图信息瓶颈

    Dynamic Graph Information Bottleneck

    [https://arxiv.org/abs/2402.06716](https://arxiv.org/abs/2402.06716)

    动态图信息瓶颈框架（DGIB）能够学习鲁棒且有区分性的动态图表示。利用信息瓶颈原理，通过迭代引导和改进图快照传递的结构和特征信息流，压缩冗余信息并保留有价值的信息。该框架能满足最小-全局-一致条件，提高了动态图神经网络的鲁棒性。

    

    动态图广泛存在于现实世界中，它们携带着复杂的时空特征模式，对于它们的表示学习提出了挑战。动态图神经网络（DGNNs）通过利用内在的动态性展示了令人印象深刻的预测能力。然而，DGNNs展示了有限的鲁棒性，易受对抗攻击。本文提出了一种新颖的动态图信息瓶颈（DGIB）框架来学习鲁棒且有区分性的表示。借助信息瓶颈（IB）原理，我们首先提出期望的最优表示应满足最小-全局-一致（MSC）条件。为了在潜在表示中压缩冗余信息和保留有价值的信息，DGIB迭代地引导和改进通过图快照传递的结构和特征信息流。为了满足MSC条件，我们将整体IB目标分解为DGIB$_{MS}$和DGIB$_C$，其中DGIB$_{MS}$通道的目标是...

    Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the DGIB$_{MS}$ channel aims 
    
[^48]: 明明就在眼前：对弱势患者群体进行不可检测的对抗性偏见攻击

    Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations

    [https://arxiv.org/abs/2402.05713](https://arxiv.org/abs/2402.05713)

    该研究发现在医学影像中，可以通过针对特定人群的标签污染攻击来破坏深度学习模型的性能，并引入对抗性的诊断不足偏见。研究结果还表明，人群在训练数据中的表示对于不可检测的对抗性偏见攻击的脆弱性直接相关。

    

    人工智能在放射学中的广泛应用揭示了深度学习模型加剧对弱势患者群体的临床偏见的风险。虽然先前的文献主要关注训练的深度学习模型所展示的偏见的量化，但针对特定人口群体的对抗性偏见攻击以及其在临床环境中的影响仍然是一个未被充分研究的医学影像领域。在这项工作中，我们证明了针对人口统计学标签的毒化攻击可以向深度学习模型引入对抗性的诊断不足偏见，并在不影响整体模型性能的情况下降低对被低估群体的性能。此外，我们的结果在多个性能指标和人口群体（如性别、年龄以及其交叉子群）上表明，群体对于不可检测的对抗性偏见攻击的脆弱性与其在模型的训练数据中的表征直接相关。

    The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance. Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data.
    
[^49]: 利用分治程序指导大型语言模型对问题求解进行引导

    Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving

    [https://arxiv.org/abs/2402.05359](https://arxiv.org/abs/2402.05359)

    该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。

    

    基础模型，如大型语言模型（LLMs），因其广泛的应用而引起了广泛的关注。现有的研究表明，适当的提示设计，如思维链，可以释放LLM在不同领域的强大能力。然而，对于处理涉及重复子任务和/或具有欺骗性内容的任务（如算术计算和文章级虚假新闻检测），现有的提示策略要么表现出表达能力不足，要么由幻觉引发中间错误。为了使LLM对这些中间错误更具辨别力，我们提出了一种以分治程序引导LLM的方法，同时确保优越的表达能力和任务分解、子任务解决和解决组装过程的分离。理论分析表明，我们的策略可以引导LLM扩展固定深度Transformer的表达能力。实验表明，我们提出的方法可以实现

    Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
    
[^50]: 零样本临床试验患者匹配与LLMs

    Zero-Shot Clinical Trial Patient Matching with LLMs

    [https://arxiv.org/abs/2402.05125](https://arxiv.org/abs/2402.05125)

    本研究基于LLMs开发了一个零样本临床试验患者匹配系统，可以高效评估患者是否符合入选标准，并通过优化提示策略和检索流程提高了数据和成本效率。

    

    将患者与临床试验匹配是推出新药的关键难题。目前，识别符合试验入选标准的患者是高度手动的，每位患者需花费长达1小时。然而，自动筛选具有挑战性，因为它需要理解非结构化的临床文本。大型语言模型（LLMs）提供了一个有望的解决方案。在这项工作中，我们探索了它们在试验匹配中的应用。首先，我们设计了一个基于LLM的系统，可以在给定一个患者的病史作为非结构化的临床文本时，评估该患者是否符合一组包含标准（也以自由文本形式指定）。我们的零样本系统在n2c2 2018队列选择基准测试中取得了最先进的得分。其次，我们通过识别一种提示策略，改善了我们方法的数据和成本效率，该策略与现状相比可以将患者匹配时间和成本降低一个数量级，并且开发了一个两阶段的检索流程，减少了匹配消除的次数。

    Matching patients to clinical trials is a key unsolved challenge in bringing new drugs to market. Today, identifying patients who meet a trial's eligibility criteria is highly manual, taking up to 1 hour per patient. Automated screening is challenging, however, as it requires understanding unstructured clinical text. Large language models (LLMs) offer a promising solution. In this work, we explore their application to trial matching. First, we design an LLM-based system which, given a patient's medical history as unstructured clinical text, evaluates whether that patient meets a set of inclusion criteria (also specified as free text). Our zero-shot system achieves state-of-the-art scores on the n2c2 2018 cohort selection benchmark. Second, we improve the data and cost efficiency of our method by identifying a prompting strategy which matches patients an order of magnitude faster and more cheaply than the status quo, and develop a two-stage retrieval pipeline that reduces the number of 
    
[^51]: 在线转移学习用于RSV病例检测

    Online Transfer Learning for RSV Case Detection

    [https://arxiv.org/abs/2402.01987](https://arxiv.org/abs/2402.01987)

    这项研究介绍了一种名为PVAW的在线多源转移学习方法，通过动态加权机制实现了对序列流行病学数据的自适应调整，并在分析RSV数据的应用中取得了显著的模型性能改进。

    

    转移学习已经成为机器学习中的一个关键技术，在各种真实世界应用中表现出高效性。然而，当将这种方法应用于序列流行病学数据时，会面临一个重要挑战，即标注信息匮乏。为了解决这个挑战，我们引入了一种新颖的在线多源转移学习方法，称为预测体积自适应加权（PVAW）。PVAW在整合模型中创造性地实现了动态加权机制，可以根据每个源模型和目标模型的相关性和贡献度自动调整权重。我们通过在匹兹堡大学医学中心收集的多个季节的呼吸道合胞病毒（RSV）数据上应用PVAW，证明了其有效性。我们的方法在模型性能上显著优于现有基线，突出了在线转移学习在处理这一问题上的潜力。

    Transfer learning has become a pivotal technique in machine learning, renowned for its effectiveness in various real-world applications. However, a significant challenge arises when applying this approach to sequential epidemiological data, often characterized by a scarcity of labeled information. To address this challenge, we introduce Predictive Volume-Adaptive Weighting (PVAW), a novel online multi-source transfer learning method. PVAW innovatively implements a dynamic weighting mechanism within an ensemble model, allowing for the automatic adjustment of weights based on the relevance and contribution of each source and target model. We demonstrate the effectiveness of PVAW through its application in analyzing Respiratory Syncytial Virus (RSV) data, collected over multiple seasons at the University of Pittsburgh Medical Center. Our method showcases significant improvements in model performance over existing baselines, highlighting the potential of online transfer learning in handlin
    
[^52]: 重新思考多元时间序列预测的通道相关性：从领先指标中学习

    Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators

    [https://arxiv.org/abs/2401.17548](https://arxiv.org/abs/2401.17548)

    本文提出了一种新方法，LIFT，通过利用通道相关性和领先指标，为多元时间序列预测提供准确的预测。LIFT方法可以无缝与任意时间序列预测方法协作，大量实验证明了其有效性。

    

    最近，独立于通道的方法在多元时间序列（MTS）预测中取得了最先进的性能。尽管这些方法减少了过拟合的风险，但它们错过了利用通道相关性进行准确预测的潜在机会。我们认为，在变量之间存在局部平稳的领先-滞后关系，即一些滞后变量在短时间内可能遵循领先指标。利用这种通道相关性是有益的，因为领先指标提供了先进信息，可以用来减少滞后变量的预测难度。在本文中，我们提出了一种名为LIFT的新方法，该方法首先在每个时间步骤高效地估计领先指标及其领先步骤，然后巧妙地允许滞后变量利用来自领先指标的先进信息。LIFT作为一个插件，可以与任意时间序列预测方法无缝协作。进行了大量实验证明了LIFT方法的有效性。

    Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments o
    
[^53]: 使用对比式上下文学习定制语言模型的回复

    Customizing Language Model Responses with Contrastive In-Context Learning

    [https://arxiv.org/abs/2401.17390](https://arxiv.org/abs/2401.17390)

    本论文提出了一种使用对比示例来定制语言模型回复的方法，通过提供正面示例和负面示例，使模型学会如何回避负面特征，从而更好地满足用户需求。

    

    大型语言模型 (LLMs) 对于机器学习应用变得越来越重要。然而，将LLMs与我们的意图对齐可能会具有挑战性，特别是当我们希望生成优于其他内容的内容，或者当我们希望LLMs以一种难以描述的风格或语气进行回应时。为了解决这个问题，我们提出了一种使用对比示例来更好地描述我们的意图的方法。这涉及提供正面示例来说明真实的意图，以及负面示例来展示我们希望LLMs避免的特征。负面示例可以从标记数据中检索，由人工编写，或由LLMs自动生成。在生成答案之前，我们要求模型分析这些示例，以教会自己避免什么。这个推理步骤为模型提供了与用户需求相关的适当表达，并引导其生成更好的答案。我们在合成和真实数据上测试了我们的方法。

    Large language models (LLMs) are becoming increasingly important for machine learning applications. However, it can be challenging to align LLMs with our intent, particularly when we want to generate content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. To address this challenge, we propose an approach that uses contrastive examples to better describe our intent. This involves providing positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid. The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself. Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid. This reasoning step provides the model with the appropriate articulation of the user's need and guides it towards generting a better answer. We tested our approach on both synthesized and real
    
[^54]: 基于多层交叉注意力融合的音频-视觉语音识别（MLCA-AVSR）

    MLCA-AVSR: Multi-Layer Cross Attention Fusion based Audio-Visual Speech Recognition

    [https://arxiv.org/abs/2401.03424](https://arxiv.org/abs/2401.03424)

    提出了基于多层交叉注意力融合的音频-视觉语音识别（MLCA-AVSR）方法，通过在不同级别的音频/视觉编码器上融合模态特征，有效提高了系统的稳健性。

    

    自动语音识别（ASR）系统在嘈杂环境中明显退化，而音频-视觉语音识别（AVSR）系统旨在用抗噪音的视觉线索补充音频流，并提高系统的稳健性。然而，当前研究主要集中在融合好学习的模态特征，如模态特定编码器的输出，而没有考虑模态特征学习期间的上下文关系。在本研究中，我们提出了一种基于多层交叉注意力融合的AVSR（MLCA-AVSR）方法，通过在不同级别的音频/视觉编码器上融合它们来促进每个模态的表示学习。对MISP2022-AVSR挑战数据集上的实验结果显示了我们提出的系统的有效性，在Eval集上实现了30.57%的拼接最小置换字符误差率（cpCER），相对于我们的p取得了高达3.17%的相对改善。

    arXiv:2401.03424v2 Announce Type: replace-cross  Abstract: While automatic speech recognition (ASR) systems degrade significantly in noisy environments, audio-visual speech recognition (AVSR) systems aim to complement the audio stream with noise-invariant visual cues and improve the system's robustness. However, current studies mainly focus on fusing the well-learned modality features, like the output of modality-specific encoders, without considering the contextual relationship during the modality feature learning. In this study, we propose a multi-layer cross-attention fusion based AVSR (MLCA-AVSR) approach that promotes representation learning of each modality by fusing them at different levels of audio/visual encoders. Experimental results on the MISP2022-AVSR Challenge dataset show the efficacy of our proposed system, achieving a concatenated minimum permutation character error rate (cpCER) of 30.57% on the Eval set and yielding up to 3.17% relative improvement compared with our p
    
[^55]: 实现端到端人工智能驱动的全球天气预报系统

    Towards an end-to-end artificial intelligence driven global weather forecasting system

    [https://arxiv.org/abs/2312.12462](https://arxiv.org/abs/2312.12462)

    提出了一种端到端基于人工智能的全球天气预报系统，通过将AI技术应用于数据同化和天气预报模型，实现了从数据处理到预测全过程的自动化。

    

    天气预报系统对科学和社会至关重要，在将人工智能（AI）应用于中期天气预报方面取得了重大成就。然而，现有的基于AI的天气预报模型依赖于传统数值天气预报（NWP）系统的分析或再分析产品作为预测的初始条件。初始状态通常由传统数据同化组件生成，这是计算昂贵且耗时。本文提出了一种基于AI的数据同化模型，即Adas，用于全球天气变量。我们将Adas与先进的基于AI的天气预报模型（即FengWu）结合起来，构建了第一个端到端基于AI的全球天气预报系统：FengWu-Adas。我们证明了Adas能够同化稀疏的全球观测数据，产生高质量的分析结果，使系统能够稳定运行。

    arXiv:2312.12462v2 Announce Type: replace-cross  Abstract: The weather forecasting system is important for science and society, and significant achievements have been made in applying artificial intelligence (AI) to medium-range weather forecasting. However, existing AI-based weather forecasting models rely on analysis or reanalysis products from the traditional numerical weather prediction (NWP) systems as initial conditions for making predictions. Initial states are typically generated by traditional data assimilation component, which is computational expensive and time-consuming. Here we present an AI-based data assimilation model, i.e., Adas, for global weather variables. And we combine Adas with the advanced AI-based weather forecasting model (i.e., FengWu) to construct the first end-to-end AI-based global weather forecasting system: FengWu-Adas. We demonstrate that Adas can assimilate sparse global observations to produce high-quality analysis, enabling the system operate stably 
    
[^56]: 推理链上的欺骗性语义快捷方式：模型在没有幻觉的情况下能走多远？

    Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?

    [https://arxiv.org/abs/2311.09702](https://arxiv.org/abs/2311.09702)

    本研究探讨了大型语言模型存在的幻觉和不忠实推理问题，提出一种新的探测方法和基准测试以研究LLMs在推理过程中是否会采取欺骗性语义快捷方式。

    

    尽管大型语言模型（LLMs）近期取得了显著进展，并在众多基准测试中表现出色，但最近的研究揭示了LLMs存在幻觉和不忠实推理的问题。本研究探讨了一种特定类型由语义关联引起的幻觉。具体来说，我们调查了LLMs在提示中是否会因为某些关键字/实体偏见而采取捷径，而不是遵循正确的推理路径。为了量化这一现象，我们提出了一种名为EureQA的新型探测方法和基准测试。我们从LLMs会以绝对确定性正确回答的问题开始，然后递归地用证据句子遮蔽重要实体，要求模型在回答问题之前找到根据证据链条遮蔽的实体。

    arXiv:2311.09702v2 Announce Type: replace-cross  Abstract: Despite the recent advancement in large language models (LLMs) and their high performances across numerous benchmarks, recent research has unveiled that LLMs suffer from hallucinations and unfaithful reasoning. This work studies a specific type of hallucination induced by semantic associations. Specifically, we investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following the correct reasoning path. To quantify this phenomenon, we propose a novel probing method and benchmark called EureQA. We start from questions that LLMs will answer correctly with utmost certainty, and mask the important entity with evidence sentence recursively, asking models to find masked entities according to a chain of evidence before answering the question.   During the construction of the evidence, we purposefully replace semantic clues (entities) that may lead to the correct answer with distractor
    
[^57]: 揭示真相：欺骗语言和语言模型

    To Tell The Truth: Language of Deception and Language Models

    [https://arxiv.org/abs/2311.07092](https://arxiv.org/abs/2311.07092)

    在高风险环境中，研究人员通过分析电视游戏节目数据发现，即使只使用语言线索，基于大型语言模型构建的模型可以与人类主体具有类似的真相检测性能。

    

    arXiv:2311.07092v2 公告类型：替换-cross 摘要：基于文本的错误信息渗透到在线讨论中，然而人们能够从这种欺骗性文本内容中辨别真相的证据却很少。我们分析了一档新颖的电视游戏节目数据，其中高风险环境中相互之间存在冲突目标的个体之间的对话导致谎言。我们调查了欺骗语言潜在可验证语言线索在客观真相存在的情况下的表现，这是以往基于文本的欺骗数据集中缺少的一个显著特征。我们展示了存在一类探测器（算法），其真相检测性能与人类主体相似，即使前者只使用语言线索，而后者则通过完全访问所有潜在线索源（语言和视听）进行对话。我们的模型，建立在大型语言模型之上，采用瓶颈框架来学习可辨别的线索，以确定真相的行为

    arXiv:2311.07092v2 Announce Type: replace-cross  Abstract: Text-based misinformation permeates online discourses, yet evidence of people's ability to discern truth from such deceptive textual content is scarce. We analyze a novel TV game show data where conversations in a high-stake environment between individuals with conflicting objectives result in lies. We investigate the manifestation of potentially verifiable language cues of deception in the presence of objective truth, a distinguishing feature absent in previous text-based deception datasets. We show that there exists a class of detectors (algorithms) that have similar truth detection performance compared to human subjects, even when the former accesses only the language cues while the latter engages in conversations with complete access to all potential sources of cues (language and audio-visual). Our model, built on a large language model, employs a bottleneck framework to learn discernible cues to determine truth, an act of 
    
[^58]: 通过决策模型弥补新手与专家之间的差距：以纠正数学错误为案例研究

    Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes

    [https://arxiv.org/abs/2310.10648](https://arxiv.org/abs/2310.10648)

    通过使用决策模型Bridge，结合专家的认知任务分析，成功利用大型语言模型（LLMs）来弥补新手和专家在纠正数学错误中的知识差距。

    

    高质量辅导规模化仍然是教育中的一项主要挑战。由于需求增长，许多平台聘用新手导师，他们与经验丰富的教育工作者不同，难以解决学生的错误，因此无法抓住主要的学习机会。我们的工作探讨了大型语言模型（LLMs）在纠正数学错误中弥补新手和专家之间知识差距的潜力。我们提出Bridge，这是一种利用认知任务分析将专家的潜在思维过程转化为纠正模型的方法。这涉及专家识别(A)学生的错误、(B)纠正策略和(C)生成回应之前的意图。我们构建了一个包含700个真实辅导对话的数据集，由专家标注了他们的决策。我们在我们的数据集上评估了最先进的LLMs，并发现专家的决策模型对LLMs来说是至关重要的，以弥补这一差距：回应f

    arXiv:2310.10648v2 Announce Type: replace-cross  Abstract: Scaling high-quality tutoring remains a major challenge in education. Due to growing demand, many platforms employ novice tutors who, unlike experienced educators, struggle to address student mistakes and thus fail to seize prime learning opportunities. Our work explores the potential of large language models (LLMs) to close the novice-expert knowledge gap in remediating math mistakes. We contribute Bridge, a method that uses cognitive task analysis to translate an expert's latent thought process into a decision-making model for remediation. This involves an expert identifying (A) the student's error, (B) a remediation strategy, and (C) their intention before generating a response. We construct a dataset of 700 real tutoring conversations, annotated by experts with their decisions. We evaluate state-of-the-art LLMs on our dataset and find that the expert's decision-making model is critical for LLMs to close the gap: responses f
    
[^59]: ARS-DETR: 面向长宽比敏感的定向目标检测与Transformer

    ARS-DETR: Aspect Ratio Sensitive Oriented Object Detection with Transformer

    [https://arxiv.org/abs/2303.04989](https://arxiv.org/abs/2303.04989)

    提出了一种面向长宽比敏感的定向目标检测器ARS-DETR，采用高精度度量AP$_{75}$来衡量模型性能，并通过新的角度分类方法和旋转可变形注意力模块实现了竞争性能。

    

    现有的定向目标检测方法通常使用度量AP$_{50}$来衡量模型的性能。我们认为AP$_{50}$在定向目标检测中本质上不适用，因为它对角度偏差具有较大的容忍度。因此，我们主张使用高精度度量，例如AP$_{75}$，来衡量模型的性能。在本文中，我们提出了一种带有Transformer的面向长宽比敏感的定向目标检测器，称为ARS-DETR，在高精度定向目标检测中表现出竞争力。具体而言，我们提出了一种新的角度分类方法，称为面向长宽比的圆滑标签（AR-CSL），以更合理地平滑角度标签，并丢弃先前工作引入的超参数（例如CSL）。然后，设计了一个旋转可变形注意力模块，用于根据相应角度旋转采样点，并消除之前工作引入的样本点与特征点之间的错位。

    arXiv:2303.04989v2 Announce Type: replace-cross  Abstract: Existing oriented object detection methods commonly use metric AP$_{50}$ to measure the performance of the model. We argue that AP$_{50}$ is inherently unsuitable for oriented object detection due to its large tolerance in angle deviation. Therefore, we advocate using high-precision metric, e.g. AP$_{75}$, to measure the performance of models. In this paper, we propose an Aspect Ratio Sensitive Oriented Object Detector with Transformer, termed ARS-DETR, which exhibits a competitive performance in high-precision oriented object detection. Specifically, a new angle classification method, calling Aspect Ratio aware Circle Smooth Label (AR-CSL), is proposed to smooth the angle label in a more reasonable way and discard the hyperparameter that introduced by previous work (e.g. CSL). Then, a rotated deformable attention module is designed to rotate the sampling points with the corresponding angles and eliminate the misalignment betwe
    
[^60]: 在线学习世界模型的局部敏感稀疏编码

    Locality Sensitive Sparse Encoding for Learning World Models Online. (arXiv:2401.13034v1 [cs.LG])

    [http://arxiv.org/abs/2401.13034](http://arxiv.org/abs/2401.13034)

    本文提出了一种基于局部敏感稀疏编码的线性回归模型，通过非线性随机特征实现对复杂环境的拟合。这种模型能够高效地进行稀疏更新，实现了优化拟合先前经验的Follow-The-Leader（FTL）世界模型。

    

    为了解决神经网络在在线学习中遇到的数据非平稳性问题，本文提出了一种基于局部敏感稀疏编码的线性回归模型，该模型通过非线性随机特征实现了对复杂环境的拟合。通过引入局部敏感稀疏编码，我们能够进行高效的稀疏更新，在平衡模型容量和计算效率的同时实现优化拟合所有先前经验的Follow-The-Leader（FTL）世界模型。

    Acquiring an accurate world model online for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even 
    
[^61]: 小型多智能体深度强化学习在无人机Metaverse中的双胞胎迁移：一种多领导者多从属者Stackelberg博弈方法

    Tiny Multi-Agent DRL for Twins Migration in UAV Metaverses: A Multi-Leader Multi-Follower Stackelberg Game Approach. (arXiv:2401.09680v1 [cs.AI])

    [http://arxiv.org/abs/2401.09680](http://arxiv.org/abs/2401.09680)

    本论文提出了一种基于小型机器学习的Stackelberg博弈框架，在无人机Metaverse中实现高效的双胞胎迁移，以提供无缝沉浸式体验。

    

    无人机与Metaverse的协同作用正在催生一种新兴范式，称为无人机Metaverse，它创建了一个统一的生态系统，融合了物理和虚拟空间，改变了无人机的交互和虚拟探索。无人机双胞胎（UTs）作为无人机的数字孪生品，通过使其更具沉浸感、真实感和信息丰富性，革新无人机应用。UTs部署在地面基站（例如道路边缘单元（RSUs））上，并通过为无人机Metaverse用户（UMUs）提供Metaverse服务。由于无人机的动态移动性和RSUs的有限通信覆盖范围，进行实时的UT迁移至关重要，以确保UMUs的无缝沉浸式体验。然而，选择合适的RSUs并优化所需带宽对于实现可靠高效的UT迁移是具有挑战性的。为了解决这些挑战，我们提出了一种基于修剪技术的小型机器学习Stackelberg博弈框架，以实现高效的UT迁移。

    The synergy between Unmanned Aerial Vehicles (UAVs) and metaverses is giving rise to an emerging paradigm named UAV metaverses, which create a unified ecosystem that blends physical and virtual spaces, transforming drone interaction and virtual exploration. UAV Twins (UTs), as the digital twins of UAVs that revolutionize UAV applications by making them more immersive, realistic, and informative, are deployed and updated on ground base stations, e.g., RoadSide Units (RSUs), to offer metaverse services for UAV Metaverse Users (UMUs). Due to the dynamic mobility of UAVs and limited communication coverages of RSUs, it is essential to perform real-time UT migration to ensure seamless immersive experiences for UMUs. However, selecting appropriate RSUs and optimizing the required bandwidth is challenging for achieving reliable and efficient UT migration. To address the challenges, we propose a tiny machine learning-based Stackelberg game framework based on pruning techniques for efficient UT 
    
[^62]: DiffSHEG:一种基于扩散的实时语音驱动的整体三维表情和手势生成方法

    DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation. (arXiv:2401.04747v1 [cs.SD])

    [http://arxiv.org/abs/2401.04747](http://arxiv.org/abs/2401.04747)

    DiffSHEG是一种基于扩散的实时语音驱动的整体三维表情和手势生成方法，通过联合生成同步表情和手势，并引入基于扩散模型的任意长序列生成策略，实现了高质量的同步表情和手势生成。

    

    我们提出了DiffSHEG，一种基于扩散的方法，用于实时语音驱动的整体三维表情和手势生成，适用于任意长度的语音输入。与以往的研究不同，我们专注于联合生成同步表情和手势，而不是分别生成。为了解决这个问题，我们提出了基于扩散的联动生成变换器，使得从表情到手势的单向信息流更加顺畅，有利于匹配联合的表情和手势分布。此外，我们引入了一种基于扩散模型的任意长序列生成策略，提供了灵活性和计算效率。我们的方法能够实现由语音驱动的高质量同步表情和手势生成的实际解决方案。通过在两个公共数据集上进行评估，我们的方法在定量和定性方面均取得了最先进的性能。此外，用户研究证实了我们方法的有效性。

    We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D Expression and Gesture generation with arbitrary length. While previous works focused on co-speech gesture or expression generation individually, the joint generation of synchronized expressions and gestures remains barely explored. To address this, our diffusion-based co-speech motion generation transformer enables uni-directional information flow from expression to gesture, facilitating improved matching of joint expression-gesture distributions. Furthermore, we introduce an outpainting-based sampling strategy for arbitrary long sequence generation in diffusion models, offering flexibility and computational efficiency. Our method provides a practical solution that produces high-quality synchronized expression and gesture generation driven by speech. Evaluated on two public datasets, our approach achieves state-of-the-art performance both quantitatively and qualitatively. Additionally, a user study confirms 
    
[^63]: 微小时间混合器 (TTMs): 针对多变量时间序列的增强零/少样本预测的快速预训练模型

    Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. (arXiv:2401.03955v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.03955](http://arxiv.org/abs/2401.03955)

    本论文介绍了一种名为微小时间混合器 (TTMs) 的预训练模型，该模型针对多变量时间序列的零/少样本预测进行了优化。与大型预训练模型相比，TTMs模型更小、更快，并考虑了跨通道相关性，能够在短时间内进行有效的预测。

    

    零/少样本学习的大型预训练模型在语言和视觉领域表现出色，但在多变量时间序列 (TS) 中面临着多样性和公开预训练数据稀缺的挑战。因此，最近在时间序列预测中使用预训练的大型语言模型 (LLMs) 进行各种适应的趋势逐渐增加。这些方法利用跨领域迁移学习，出奇地取得了令人印象深刻的结果。然而，这些模型通常非常缓慢且庞大（大约十亿个参数），并且不考虑跨通道相关性。为了解决这个问题，我们提出了多层微小时间混合器 (TTM)，这是一种基于轻量级 TSMixer 结构的显著小型模型。TTM 是首个成功开发的微型通用预训练模型（≤100万个参数），专门在公开TS数据集上进行快速训练（仅需4-8小时），具有有效的迁移学习能力进行预测。

    Large Pretrained models for zero/few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pretraining data. Consequently, there has been a recent surge in utilizing pretrained large language models (LLMs) with various adaptations for time series forecasting. These approaches employ cross-domain transfer learning and surprisingly yield impressive results. However, these models are typically very slow and large ($\sim$billion parameters) and do not consider cross-channel correlations. To address this, we present Multi-level Tiny Time Mixers (TTM), a significantly small model based on the lightweight TSMixer architecture. TTM marks the first success in developing tiny general-pretrained models ($\le$1 million parameters), exclusively trained on public TS datasets in a flash of just 4-8 hrs with effective transfer learning capabilities for forecasting. To tackle the complexi
    
[^64]: 从因果角度重新思考图对比学习中的维度理论

    Rethinking Dimensional Rationale in Graph Contrastive Learning from Causal Perspective. (arXiv:2312.10401v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.10401](http://arxiv.org/abs/2312.10401)

    本论文从因果角度重新思考图对比学习中的维度理论，并提出在图中捕捉维度理论的方法，以改善性能，并解决图模型学习中的问题。以上方法在实验中得到验证。

    

    图对比学习是一种在图中捕捉不变信息的学习范式。最近的研究专注于探索图的结构理论，从而增加不变信息的可区分性。然而，这些方法可能导致图模型朝向解释图的可解释性进行错误学习，因此学习到的噪声和与任务无关的信息干扰了图的预测。为了探索图的内在理论，我们提出了从图中捕捉维度理论的方法，但这在文献中并没有得到足够的关注。通过实验验证了上述路径的可行性。为了阐明维度理论对性能改进的内在机制，我们从因果角度重新思考图对比学习中的维度理论。

    Graph contrastive learning is a general learning paradigm excelling at capturing invariant information from diverse perturbations in graphs. Recent works focus on exploring the structural rationale from graphs, thereby increasing the discriminability of the invariant information. However, such methods may incur in the mis-learning of graph models towards the interpretability of graphs, and thus the learned noisy and task-agnostic information interferes with the prediction of graphs. To this end, with the purpose of exploring the intrinsic rationale of graphs, we accordingly propose to capture the dimensional rationale from graphs, which has not received sufficient attention in the literature. The conducted exploratory experiments attest to the feasibility of the aforementioned roadmap. To elucidate the innate mechanism behind the performance improvement arising from the dimensional rationale, we rethink the dimensional rationale in graph contrastive learning from a causal perspective a
    
[^65]: QLLM: 大规模语言模型的准确高效低位宽量化

    QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])

    [http://arxiv.org/abs/2310.08041](http://arxiv.org/abs/2310.08041)

    QLLM是一种为大规模语言模型设计的准确高效的低位宽后训练量化方法，通过引入自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。

    

    大规模语言模型在自然语言处理领域表现出色，但由于其所需资源过大，限制了其广泛应用。虽然量化感知训练（Quantization-Aware Training，QAT）提供了一种解决方案，但它的训练成本过高，因此后训练量化（Post-Training Quantization，PTQ）成为大规模语言模型更实际的方法。在现有研究中，特定通道中的激活离群值被认为是导致后训练量化准确性下降的瓶颈。本文提出了QLLM，一种为大规模语言模型设计的准确高效的低位宽后训练量化方法。QLLM引入了一种自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。具体来说，通过通道拆分和通道组装，在保证低位宽的情况下将离群通道分解成多个子通道。

    Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a 
    
[^66]: 提示工程对ChatGPT在无监督实体解析中的性能影响是如何的？

    How does prompt engineering affect ChatGPT performance on unsupervised entity resolution?. (arXiv:2310.06174v1 [cs.AI])

    [http://arxiv.org/abs/2310.06174](http://arxiv.org/abs/2310.06174)

    本研究对提示工程对ChatGPT在无监督实体解析中的影响进行了初步实验研究，结果显示提示可以显著影响实体解析的质量。

    

    实体解析（ER）是一种半自动确定两个实体是否指向相同基础实体的问题，应用范围从医疗保健到电子商务。传统的ER解决方案需要相当多的手动专业知识，包括特征工程以及训练数据的识别和策划。在许多情况下，这些技术高度依赖于领域。随着大型语言模型（LLMs）的最新发展，有机会使ER更加无缝和领域无关。然而，众所周知，LLMs可能存在风险，其输出质量可能取决于所谓的提示工程。不幸的是，迄今为止，关于使用像ChatGPT这样的LLMs解决ER的不同提示方法的影响的系统实验研究还缺乏。本文旨在填补这一空白，通过进行这样一项研究。尽管这只是初步性质的研究，我们的结果表明，提示可以显著影响实体解析的质量。

    Entity Resolution (ER) is the problem of semi-automatically determining when two entities refer to the same underlying entity, with applications ranging from healthcare to e-commerce. Traditional ER solutions required considerable manual expertise, including feature engineering, as well as identification and curation of training data. In many instances, such techniques are highly dependent on the domain. With recent advent in large language models (LLMs), there is an opportunity to make ER much more seamless and domain-independent. However, it is also well known that LLMs can pose risks, and that the quality of their outputs can depend on so-called prompt engineering. Unfortunately, a systematic experimental study on the effects of different prompting methods for addressing ER, using LLMs like ChatGPT, has been lacking thus far. This paper aims to address this gap by conducting such a study. Although preliminary in nature, our results show that prompting can significantly affect the qu
    
[^67]: 音乐产品的正面和风险信息评估

    Positive and Risky Message Assessment for Music Products. (arXiv:2309.10182v1 [cs.CL])

    [http://arxiv.org/abs/2309.10182](http://arxiv.org/abs/2309.10182)

    这项研究提出了一个新的问题：如何评估音乐产品中的正面和风险信息。研究者提出了一个多任务预测模型，通过序数约束解决这个问题，并且取得了显著优于其他方法的结果。

    

    在这项工作中，我们提出了一个新颖的研究问题：评估音乐产品中的正面和风险信息。我们首先建立了一个多角度多级音乐内容评估的基准，然后提出了一种有效的多任务预测模型，并通过序数约束来解决这个问题。我们的结果显示，所提出的方法不仅明显优于强大的针对特定任务的对应方法，而且可以同时评估多个方面。

    In this work, we propose a novel research problem: assessing positive and risky messages from music products. We first establish a benchmark for multi-angle multi-level music content assessment and then present an effective multi-task prediction model with ordinality-enforcement to solve this problem. Our result shows the proposed method not only significantly outperforms strong task-specific counterparts but can concurrently evaluate multiple aspects.
    
[^68]: Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree Image Generation. (arXiv:2309.03467v1 [cs.CV])

    Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree Image Generation. (arXiv:2309.03467v1 [cs.CV])

    [http://arxiv.org/abs/2309.03467](http://arxiv.org/abs/2309.03467)

    该论文提出了一种自回归全方位感知生成网络（AOG-Net）用于生成360度图像，通过渐进地外扩不完整的360度图像，并与窄视场（NFoV）和文本引导相结合或单独使用。这种方法可以生成更细致和与文本一致的模式，并为用户在生成过程中灵活编辑条件。

    

    360度（全方位）图像提供了一个场景的全景球状视图。最近，在从由数字相机和智能手机捕获的传统窄视场（NFoV）图像合成360度图像方面引起了越来越多的关注，以在各种场景中提供身临其境的体验，如虚拟现实。然而，现有的方法通常无法合成复杂的视觉细节或确保生成的图像与用户提供的提示一致。在这项研究中，提出了一种自回归全方位感知生成网络（AOG-Net），通过使用NFoV和文本引导相结合或单独进行渐进地外扩不完整的360度图像来进行360度图像生成。这种自回归方案不仅允许通过动态生成和调整过程来获取更精细和与文本一致的模式，还为用户在生成过程中编辑条件提供了更大的灵活性。

    A 360-degree (omni-directional) image provides an all-encompassing spherical view of a scene. Recently, there has been an increasing interest in synthesising 360-degree images from conventional narrow field of view (NFoV) images captured by digital cameras and smartphones, for providing immersive experiences in various scenarios such as virtual reality. Yet, existing methods typically fall short in synthesizing intricate visual details or ensure the generated images align consistently with user-provided prompts. In this study, autoregressive omni-aware generative network (AOG-Net) is proposed for 360-degree image generation by out-painting an incomplete 360-degree image progressively with NFoV and text guidances joinly or individually. This autoregressive scheme not only allows for deriving finer-grained and text-consistent patterns by dynamically generating and adjusting the process but also offers users greater flexibility to edit their conditions throughout the generation process. A
    
[^69]: 探索检索器和大型语言模型的整合策略

    Exploring the Integration Strategies of Retriever and Large Language Models. (arXiv:2308.12574v1 [cs.IR])

    [http://arxiv.org/abs/2308.12574](http://arxiv.org/abs/2308.12574)

    本文通过探索不同的检索器和大型语言模型整合方法来增强答案生成，并发现常用的连接方法存在局限性。为了解决这个问题，本文提出了四种替代策略，包括两种单轮方法和两种多轮策略。

    

    检索到的段落和大型语言模型（如ChatGPT）的整合为提高开放领域问答作出了显著贡献。然而，如何将检索到的段落融入答案生成过程中的最佳方法仍然缺乏探索。本文旨在通过研究不同的方法来结合检索到的段落和大型语言模型以增强答案生成。我们首先研究了常用的连接方法的局限性。令人惊讶的是，即使正确的文档在前k个检索到的段落中，这种方法经常会生成“未知”输出。为了解决这个问题，我们探索了四种将检索到的段落与大型语言模型整合的替代策略。这些策略包括两种利用思维链推理的单轮方法和两种利用反馈循环的多轮策略。通过全面的分析和实验，我们发现...

    The integration of retrieved passages and large language models (LLMs), such as ChatGPTs, has significantly contributed to improving open-domain question answering. However, there is still a lack of exploration regarding the optimal approach for incorporating retrieved passages into the answer generation process. This paper aims to fill this gap by investigating different methods of combining retrieved passages with LLMs to enhance answer generation. We begin by examining the limitations of a commonly-used concatenation approach. Surprisingly, this approach often results in generating "unknown" outputs, even when the correct document is among the top-k retrieved passages. To address this issue, we explore four alternative strategies for integrating the retrieved passages with the LLMs. These strategies include two single-round methods that utilize chain-of-thought reasoning and two multi-round strategies that incorporate feedback loops. Through comprehensive analyses and experiments, w
    
[^70]: 智能的一种理论：概念、模型和意义

    A Theory of Intelligences: Concepts, Models, Implications. (arXiv:2308.12411v1 [cs.AI])

    [http://arxiv.org/abs/2308.12411](http://arxiv.org/abs/2308.12411)

    这篇论文提出了一种智能的理论，讨论了智能的核心要素和挑战，并提出了基于第一原理的理论。研究重点是人类智能，并与机器进行比较，目的是为更广泛的生命、集合体和非设计的物理化学系统提供描述。

    

    智能是人类用来表示实现目标能力的概念。给予这个广泛的范畴，智能已经被无数次定义，以各种方式进行研究，并使用多种测量方法进行量化。理解智能最终需要理论和量化，但这两者都很难捉摸。我的主要目标是确定智能的一些核心要素，讨论其中的一些挑战，并提出一种基于第一原理的理论。我主要关注以人类为定义和参照对象的智能，常常与机器进行比较，意图为生命、集合体、人工智能等非设计的物理和化学系统提供更一般化的描述。我讨论了智能的关键特征，包括路径效率和目标准确性、智能作为黑盒、环境影响、处理意外情况的灵活性、智能的回归和相对论性质。

    Intelligence is a human construct to represent the ability to achieve goals. Given this wide berth, intelligence has been defined countless times, studied in a variety of ways and quantified using numerous measures. Understanding intelligence ultimately requires theory and quantification, both of which are elusive. My main objectives are to identify some of the central elements in and surrounding intelligence, discuss some of its challenges and propose a theory based on first principles. I focus on intelligence as defined by and for humans, frequently in comparison to machines, with the intention of setting the stage for more general characterizations in life, collectives, human designs such as AI and in non-designed physical and chemical systems. I discuss key features of intelligence, including path efficiency and goal accuracy, intelligence as a Black Box, environmental influences, flexibility to deal with surprisal, the regress of intelligence, the relativistic nature of intelligen
    
[^71]: ChatGPT生物医学生成文本中建立信任的方法：基于本体的知识图谱用于验证疾病-症状关系

    Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links. (arXiv:2308.03929v1 [cs.AI])

    [http://arxiv.org/abs/2308.03929](http://arxiv.org/abs/2308.03929)

    本研究通过构建基于本体的知识图谱，利用疾病本体和症状本体构建数学模型，利用事实核查算法和网络中心度指标分析ChatGPT生成的文本与真实医学文献之间的准确性，以验证疾病-症状关系。

    

    方法：通过创新的方法，我们从真实的医学文献和人工智能生成的内容构建了基于本体的知识图谱。我们的目标是区分事实信息和未经验证的数据。我们收集了两个数据集：一个是使用“人类疾病和症状”查询从生物医学文献中编译的，另一个是由ChatGPT生成的模拟文章。利用这些数据集（PubMed和ChatGPT），我们随机选择了10组每组250个摘要，并使用特定的种子。我们的方法主要是利用疾病本体（DOID）和症状本体（SYMP）构建知识图谱，这是一种强大的数学模型，可以进行无偏差的比较。通过使用我们的事实核查算法和网络中心度指标，我们进行了GPT疾病-症状链接分析，以量化在噪声、假设和重要发现中的事实知识的准确性。结果：通过比较不同ChatGPT知识图谱及其PubMed计数获得的结果，我们发现...

    Methods: Through an innovative approach, we construct ontology-based knowledge graphs from authentic medical literature and AI-generated content. Our goal is to distinguish factual information from unverified data. We compiled two datasets: one from biomedical literature using a "human disease and symptoms" query, and another generated by ChatGPT, simulating articles. With these datasets (PubMed and ChatGPT), we curated 10 sets of 250 abstracts each, selected randomly with a specific seed. Our method focuses on utilizing disease ontology (DOID) and symptom ontology (SYMP) to build knowledge graphs, robust mathematical models that facilitate unbiased comparisons. By employing our fact-checking algorithms and network centrality metrics, we conducted GPT disease-symptoms link analysis to quantify the accuracy of factual knowledge amid noise, hypotheses, and significant findings.  Results: The findings obtained from the comparison of diverse ChatGPT knowledge graphs with their PubMed count
    
[^72]: 探索具有描述逻辑特征的命题动态逻辑的非正则扩展

    Exploring Non-Regular Extensions of Propositional Dynamic Logic with Description-Logics Features. (arXiv:2307.09913v1 [cs.LO])

    [http://arxiv.org/abs/2307.09913](http://arxiv.org/abs/2307.09913)

    研究了非正则路径表达式对于扩展ALC描述逻辑中可满足性检查和查询的可决定性的影响，并提供了一系列不可决定性结果。

    

    我们研究了非正则路径表达式对于扩展ALC描述逻辑中可满足性检查和查询的可决定性的影响。我们主要关注的对象是ALCreg和ALCvpl，分别是使用正则和可见推下语言的路径表达式的扩展。第一个ALCreg是Fischer和Ladner所熟知的命题动态逻辑的一种变种。第二个ALCvpl是由Loding和Serre在2007年引入和研究的。ALCvpl逻辑广义上推广了许多已知的可决定性非正则扩展的ALCreg。我们提供了一系列不可决定性结果。首先，我们展示了在添加看似无害的Self操作符后，对于ALCvpl中的概念可满足性问题的可决定性丧失。其次，我们建立了对于在ALCvpl中添加个体词的概念可满足性问题的不可决定性。有趣的是，我们的不可决定性证明只依赖于一个单一的非正则（可见推下）语言。

    We investigate the impact of non-regular path expressions on the decidability of satisfiability checking and querying in description logics extending ALC. Our primary objects of interest are ALCreg and ALCvpl, the extensions of with path expressions employing, respectively, regular and visibly-pushdown languages. The first one, ALCreg, is a notational variant of the well-known Propositional Dynamic Logic of Fischer and Ladner. The second one, ALCvpl, was introduced and investigated by Loding and Serre in 2007. The logic ALCvpl generalises many known decidable non-regular extensions of ALCreg.  We provide a series of undecidability results. First, we show that decidability of the concept satisfiability problem for ALCvpl is lost upon adding the seemingly innocent Self operator. Second, we establish undecidability for the concept satisfiability problem for ALCvpl extended with nominals. Interestingly, our undecidability proof relies only on one single non-regular (visibly-pushdown) langu
    
[^73]: 自监督学习在时间序列分析中的应用：分类、进展和前景

    Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects. (arXiv:2306.10125v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10125](http://arxiv.org/abs/2306.10125)

    自监督学习（SSL）在时间序列分析中的应用取得了显著性能，通过减少对标注数据的依赖，即使只有少量标注数据，也能实现高性能。

    

    自监督学习（SSL）最近在各种时间序列任务上取得了令人瞩目的性能。SSL最突出的优势是减少对标注数据的依赖。基于预训练和微调策略，即使只有少量标注数据，也可以实现高性能。与许多关于计算机视觉和自然语言处理的自监督学习综述相比，目前还缺乏针对时间序列SSL的综述。为了填补这一空白，本文回顾了当前时间序列数据中的自监督学习（SSL）方法的最新研究进展。为此，我们首先全面回顾了与自监督学习（SSL）和时间序列相关的现有综述，然后通过总结从生成型、对比型和对抗型三个角度对现有时间序列自监督学习方法进行了新的分类。这些方法进一步细分为十个子类，详细回顾和讨论了它们的关键直觉、主要框架、优势和限制。

    Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages an
    
[^74]: DoubleAdapt：一种用于股票趋势预测的增量学习元学习方法

    DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting. (arXiv:2306.09862v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.09862](http://arxiv.org/abs/2306.09862)

    DoubleAdapt是一个增量学习的方法，用于股票趋势预测。它利用元学习技术自动学习如何将股票数据适应到本地平稳分布空间中，从而有效地适应数据和模型，减轻分布漂移的影响。

    

    股票趋势预测是量化投资的基本任务之一，准确预测价格趋势是不可或缺的。作为一项在线服务，股票数据随时随地持续到达。使用最新数据对预测模型进行增量更新是实用而高效的，因为这些新数据可能揭示了未来股票市场中会重复出现的一些新模式。然而，由于分布漂移（即概念漂移）的挑战，股票趋势预测的增量学习仍然没有得到充分探索。随着股票市场动态演变，未来数据的分布可能会与增量数据稍微或显着地不同，从而阻碍增量更新的有效性。为了解决这一挑战，我们提出了一个利用两个适配器的端到端框架——DoubleAdapt，可以有效地适应数据和模型，以减轻分布漂移的影响。我们的关键洞察力是利用元学习技术自动学习如何将股票数据适应到本地平稳分布空间中。

    Stock trend forecasting is a fundamental task of quantitative investment where precise predictions of price trends are indispensable. As an online service, stock data continuously arrive over time. It is practical and efficient to incrementally update the forecast model with the latest data which may reveal some new patterns recurring in the future stock market. However, incremental learning for stock trend forecasting still remains under-explored due to the challenge of distribution shifts (a.k.a. concept drifts). With the stock market dynamically evolving, the distribution of future data can slightly or significantly differ from incremental data, hindering the effectiveness of incremental updates. To address this challenge, we propose DoubleAdapt, an end-to-end framework with two adapters, which can effectively adapt the data and the model to mitigate the effects of distribution shifts. Our key insight is to automatically learn how to adapt stock data into a locally stationary distri
    
[^75]: Fedstellar：一个去中心化联邦学习平台

    Fedstellar: A Platform for Decentralized Federated Learning. (arXiv:2306.09750v1 [cs.LG])

    [http://arxiv.org/abs/2306.09750](http://arxiv.org/abs/2306.09750)

    Fedstellar是一个联邦学习平台，支持物理或虚拟设备的去中心化、半去中心化和中心化的方式训练模型，旨在解决现有平台在处理异构联盟网络拓扑等问题时的挑战。

    

    2016年，谷歌提出了联邦学习（FL）作为一种新的范式，可以在保护数据隐私的同时跨联盟参与者训练机器学习（ML）模型。虽然中心化联邦学习（CFL）是最常用的方法，但它存在通信瓶颈、单点故障和对中央服务器的依赖等局限。去中心化联邦学习（DFL）通过实现去中心化模型聚合和最小化对中央实体的依赖，来解决这些问题。然而，目前训练DFL模型的平台在处理异构联盟网络拓扑等关键问题方面存在困难。为了克服这些挑战，本文提出了Fedstellar，这是一个新型的平台，旨在在物理或虚拟设备的不同联盟中以去中心化、半去中心化和中心化的方式训练FL模型。

    In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train Machine Learning (ML) models across the participants of a federation while preserving data privacy. Since its birth, Centralized FL (CFL) has been the most used approach, where a central entity aggregates participants' models to create a global one. However, CFL presents limitations such as communication bottlenecks, single point of failure, and reliance on a central server. Decentralized Federated Learning (DFL) addresses these issues by enabling decentralized model aggregation and minimizing dependency on a central entity. Despite these advances, current platforms training DFL models struggle with key issues such as managing heterogeneous federation network topologies. To overcome these challenges, this paper presents Fedstellar, a novel platform designed to train FL models in a decentralized, semi-decentralized, and centralized fashion across diverse federations of physical or virtualized devices. The Feds
    
[^76]: 《Zero-TPrune: 基于预训练Transformers关注图的零射击令牌剪枝方法》

    Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers. (arXiv:2305.17328v1 [cs.CV])

    [http://arxiv.org/abs/2305.17328](http://arxiv.org/abs/2305.17328)

    《Zero-TPrune》是一个考虑到令牌的重要性和相似性的零射击方法，它利用预训练Transformer模型的注意图来进行令牌剪枝，以求解在边缘设备上Transformer模型即插即用的难题。

    

    最近在边缘设备上部署Transformer模型变得越来越具有挑战性，原因是模型的体积呈指数级增长，而推理成本则随输入序列中令牌数量的平方提高。令牌剪枝是解决这一挑战的新兴解决方法之一，由于其易于在各种Transformer支持的模型上部署。然而，大多数令牌剪枝方法需要在剪枝后或期间进行计算密集型的微调过程，在许多情况下这是不可取的。最近的一些研究探讨了没有微调的即插即用的预训练Transformer的剪枝方法。但是，它们只考虑了令牌的重要性。在这项工作中，我们提出了Zero-TPrune，这是一种零射击方法，它既考虑令牌的重要性又考虑相似性来执行令牌剪枝。Zero-TPrune利用预训练Transformer模型的注意图来为令牌生成一个重要性排名并移除信息较少的令牌。注意矩阵可用于推断即插即用的模型。

    Deployment of Transformer models on the edge is increasingly challenging due to the exponentially growing model size and inference cost that scales quadratically with the number of tokens in the input sequence. Token pruning is an emerging solution to address this challenge due to its ease of deployment on various Transformer backbones. However, most token pruning methods require a computationally-expensive fine-tuning process after or during pruning, which is not desirable in many cases. Some recent works explore pruning of off-the-shelf pre-trained Transformers without fine-tuning. However, they only take the importance of tokens into consideration. In this work, we propose Zero-TPrune, the first zero-shot method that considers both the importance and similarity of tokens in performing token pruning. Zero-TPrune leverages the attention graph of pre-trained Transformer models to produce an importance rank for tokens and removes the less informative tokens. The attention matrix can be 
    
[^77]: AI增强的调查：利用大语言模型进行全国代表性调查的观点预测

    AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. (arXiv:2305.09620v1 [cs.CL])

    [http://arxiv.org/abs/2305.09620](http://arxiv.org/abs/2305.09620)

    本论文研究了利用经过全国代表性调查微调的大语言模型（LLMs）来增强调查的观点预测，取得了在遗漏数据插值和回溯推理方面优秀的成果，在零次预测方面仍需进一步研究。

    

    本论文研究了如何使用经过全国代表性调查微调的大语言模型（LLMs）来增强调查。本文探讨了LLMs在观点预测中，遗漏数据插值，回溯推理和零次预测三个不同应用。我们提出了一种新的方法论框架，将调查问题、个人信念和时间背景的神经嵌入引入到观点预测的个性化LLMs中。在1972年到2021年的“常规社会调查”中，我们从68,846名美国人中获得了3,110个二进制观点，在Alpaca-7b模型的基础上取得了最好的成果，在缺失数据插值（AUC=0.87，公开观点预测为$\rho$=0.99）和回溯推理（AUC=0.86，$\rho$=0.98）方面表现出色。这些显著的预测能力能够以高置信度填补缺失的趋势，并标明公众态度何时发生变化，如同性婚姻的获取支持。然而，在零次预测的情况下，模型的表现受到限制，需要进一步研究。

    How can we use large language models (LLMs) to augment surveys? This paper investigates three distinct applications of LLMs fine-tuned by nationally representative surveys for opinion prediction -- missing data imputation, retrodiction, and zero-shot prediction. We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our best models based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and $\rho$ = 0.99 for public opinion prediction) and retrodiction (AUC = 0.86, $\rho$ = 0.98). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zer
    
[^78]: 位置偏差对token分类中的语言模型的影响

    Impact of Position Bias on Language Models in Token Classification. (arXiv:2304.13567v1 [cs.CL])

    [http://arxiv.org/abs/2304.13567](http://arxiv.org/abs/2304.13567)

    研究了语言模型在token分类任务中的位置偏差问题，通过实验表明在具体任务中，BERT、ERNIE、ELECTRA等编码器以及GPT2和BLOOM等解码器的平均性能下降了3%和9%。

    

    语言模型在自然语言处理任务中表现出了最先进的性能。命名实体识别(NER)或词性标注等下游任务已知存在数据不平衡问题，特别是在正负示例的比例和类不平衡方面。本文研究了语言模型的另一个特定问题，即token分类任务中正示例的位置偏差。因此，我们对基于Token分类基准测试的语言模型的性能进行了深入的位置偏差评估。我们的研究包括CoNLL03和OntoNote5.0用于NER，English Tree Bank UD_en和TweeBank用于POS标记。我们提出了一种评估方法，以研究Transformer模型中的位置偏差。我们发现像BERT、ERNIE、ELECTRA这样的编码器和像GPT2 和BLOOM这样的解码器平均性能下降了3%和9%。

    Language Models (LMs) have shown state-of-the-art performance in Natural Language Processing (NLP) tasks. Downstream tasks such as Named Entity Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data imbalance issues, specifically in terms of the ratio of positive to negative examples, and class imbalance. In this paper, we investigate an additional specific issue for language models, namely the position bias of positive examples in token classification tasks. Therefore, we conduct an in-depth evaluation of the impact of position bias on the performance of LMs when fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We propose an evaluation approach to investigate position bias in Transformer models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in their performan
    
[^79]: 基于推荐系统的大规模多目标优化方法研究

    A Recommender System Approach for Very Large-scale Multiobjective Optimization. (arXiv:2304.04067v1 [cs.NE])

    [http://arxiv.org/abs/2304.04067](http://arxiv.org/abs/2304.04067)

    本论文提出了一种基于推荐系统的大规模多目标优化方法，将解决方案视为用户，通过汤普森抽样和高斯过程逼近 Pareto-最优前沿上的后验分布，成功解决了处理大规模问题的难题。

    

    本论文定义了决策变量数量超过100,000个纬度的问题为大规模多目标优化问题。由于许多实际问题需要优化十万级别的变量，这是一个重要的问题类别。现有的进化优化方法在处理这种规模非常大的问题时存在不足。受到现有推荐系统成功处理历史交互有限的大规模物品的启发，在本文中，我们提出了一种称为“基于推荐系统的大规模多目标优化”（VMORS）的方法。该方法的思想是将这类问题转化为可以由推荐系统解决的问题。在该框架下，解决方案被视为用户，不同的进化方向是等待推荐的项目。我们使用汤普森抽样通过高斯过程逼近 Pareto-最优前沿上的后验分布，以推荐进化方向。对基准问题的实验结果证明了该方法在解决大规模多目标优化问题方面的有效性。

    We define very large multi-objective optimization problems to be multiobjective optimization problems in which the number of decision variables is greater than 100,000 dimensions. This is an important class of problems as many real-world problems require optimizing hundreds of thousands of variables. Existing evolutionary optimization methods fall short of such requirements when dealing with problems at this very large scale. Inspired by the success of existing recommender systems to handle very large-scale items with limited historical interactions, in this paper we propose a method termed Very large-scale Multiobjective Optimization through Recommender Systems (VMORS). The idea of the proposed method is to transform the defined such very large-scale problems into a problem that can be tackled by a recommender system. In the framework, the solutions are regarded as users, and the different evolution directions are items waiting for the recommendation. We use Thompson sampling to recom
    

