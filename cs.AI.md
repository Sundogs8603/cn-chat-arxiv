# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Already Moderate Population Sizes Provably Yield Strong Robustness to Noise](https://arxiv.org/abs/2404.02090) | 适中的种群规模可以在先验位噪声存在时保持强鲁棒性，而不会增加在OneMax基准上的渐近运行时间 |
| [^2] | [Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and Mitigation](https://arxiv.org/abs/2404.01030) | 该调查综述了文本到图像生成中的偏见问题，重点讨论了性别、肤色和地域文化这些方面，旨在帮助理解当前进展和研究空白。 |
| [^3] | [HARMamba: Efficient Wearable Sensor Human Activity Recognition Based on Bidirectional Selective SSM](https://arxiv.org/abs/2403.20183) | HARMamba利用更轻量级的选择性SSM作为基础模型架构，以解决计算资源挑战 |
| [^4] | [Self-Improved Learning for Scalable Neural Combinatorial Optimization](https://arxiv.org/abs/2403.19561) | 提出一种新颖的自我改进学习(SIL)方法，实现神经组合优化的更好可扩展性，通过自身生成解决方案作为伪标签，设计线性复杂度的注意机制来处理大规模组合优化问题实例。 |
| [^5] | [Detecting Generative Parroting through Overfitting Masked Autoencoders](https://arxiv.org/abs/2403.19050) | 本研究提出利用过拟合的遮蔽自编码器(MAE)来检测生成模型中的生成性模仿，建立了基于训练数据集损失的检测阈值，为了确保生成模型的合法使用和提升其法律合规性提供了一种新方法。 |
| [^6] | [An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing](https://arxiv.org/abs/2403.16854) | 通过专家代币路由将多个专家LLM协同作为通用型，可以实现多个专家LLMs的无缝集成，支持隐式专业知识的学习和动态扩展新的专家LLMs，同时更好地隐藏协作细节，展现出比现有多LLM协作范式更好的效果和稳健性。 |
| [^7] | [Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization](https://arxiv.org/abs/2403.16071) | 通过利用唇部地标引导的细粒度视觉线索，提出了一种适应说话人的唇读模型，有效降低说话人之间的视觉变化。 |
| [^8] | [Knowledge-guided Machine Learning: Current Trends and Future Prospects](https://arxiv.org/abs/2403.15989) | 知识引导的机器学习（KGML）结合科学知识和数据在机器学习框架中，以实现更好的泛化能力、科学一致性和结果可解释性。 |
| [^9] | [Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments](https://arxiv.org/abs/2403.12237) | 本文提出了一种通过将Transformer架构和演员-评论家强化学习模型相结合的新方法TRL-HPO，在资源受限的IoT环境中实现了高效的超参数优化，该方法在MNIST数据集上表现优良。 |
| [^10] | [Chronos: Learning the Language of Time Series](https://arxiv.org/abs/2403.07815) | Chronos框架通过在固定词汇上训练预训练的概率时间序列模型，在大量数据集上进行了全面基准测试，表现出在训练语料库中的数据集上明显优于其他方法，并且在新数据集上的零样本性能表现可比甚至优于其他方法。 |
| [^11] | [Uncertainty Quantification with Deep Ensembles for 6D Object Pose Estimation](https://arxiv.org/abs/2403.07741) | 本研究提出了一种使用深度集成来量化多阶段6D物体姿态估计方法不确定性的方法。 |
| [^12] | [Privacy-Preserving Diffusion Model Using Homomorphic Encryption](https://arxiv.org/abs/2403.05794) | 本文介绍了一种使用同态加密的隐私保护扩散模型HE-Diffusion，通过最小失真方法和稀疏张量表示来提高效率，实现了500倍的加速。 |
| [^13] | [Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling](https://arxiv.org/abs/2402.19471) | 研究利用大型语言模型提出信息量丰富的问题，在Battleship游戏中展示出与人类表现相匹配的效果，并揭示了贝叶斯模型如何指导问问题行为。 |
| [^14] | [Behavioral Refinement via Interpolant-based Policy Diffusion](https://arxiv.org/abs/2402.16075) | 使用比高斯更具信息量的源头启动扩散方法有助于克服模仿学习任务中的限制。 |
| [^15] | [Zero-shot generalization across architectures for visual classification](https://arxiv.org/abs/2402.14095) | 不同神经网络在跨架构和层间泛化到未知类别的能力存在差异，准确性并不是泛化能力的良好预测因子，泛化能力随着层深度呈非单调变化。 |
| [^16] | [Large Language Models as Zero-shot Dialogue State Tracker through Function Calling](https://arxiv.org/abs/2402.10466) | 本研究提出了一种通过函数调用将大型语言模型用于零-shot对话状态追踪的新方法，能够在任务导向对话中取得出色的性能，适应不同领域而无需大量数据收集或模型调整。 |
| [^17] | [The Effect of Data Poisoning on Counterfactual Explanations](https://arxiv.org/abs/2402.08290) | 本研究研究了反事实解释在数据污染方面的脆弱性，发现最先进的反事实生成方法和工具包容易受到数据污染的影响。 |
| [^18] | [Instance-Level Safety-Aware Fidelity of Synthetic Data and Its Calibration](https://arxiv.org/abs/2402.07031) | 本论文研究了实例级别的合成数据质量与安全感知，引入了四种超越纯视觉特征的合成数据质量，并提出了优化方法来减少合成和真实图像之间的质量差距。 |
| [^19] | [CIC: A framework for Culturally-aware Image Captioning](https://arxiv.org/abs/2402.05374) | CIC是一种面向文化感知图像字幕的框架，通过结合视觉问答和大型语言模型，它能够生成能描述图像中文化元素的详细字幕。 |
| [^20] | [BetterV: Controlled Verilog Generation with Discriminative Guidance](https://arxiv.org/abs/2402.03375) | 本文提出了一种Verilog生成框架BetterV，通过微调大型语言模型和生成判别器的使用，实现了可控的Verilog生成，能够生成语法和功能正确的Verilog实现。 |
| [^21] | [Learning to Embed Time Series Patches Independently](https://arxiv.org/abs/2312.16427) | 学习独立嵌入时间序列片段可以产生更好的时间序列表示，通过简单的块重构任务和独立嵌入每个块的MLP模型以及互补对比学习来实现。 |
| [^22] | [Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting](https://arxiv.org/abs/2310.18948) | 通过利用AIS数据预测船舶轨迹，本研究旨在通过减少船舶与鲸鱼碰撞来建立更安全的海洋环境。 |
| [^23] | [Mixture-of-Linear-Experts for Long-term Time Series Forecasting.](http://arxiv.org/abs/2312.06786) | MoLE是一种混合线性专家模型，通过训练多个线性中心模型和一个路由模型，能够适应时间序列模式的周期性变化，并显著降低了预测误差。 |
| [^24] | [Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning.](http://arxiv.org/abs/2310.02360) | 本论文提出了一种用于字典型强化学习的优先级软Q分解算法（PSQD），能够在连续状态-动作空间中学习和适应具有字典型优先级的子任务解决方案，实现了先前学习的子任务解决方案的零-shot组成和适应。 |
| [^25] | [Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances.](http://arxiv.org/abs/2310.02246) | 本文提出了一种解决一系列线性系统实例中设置求解器参数的方法，通过使用在线学习算法选择参数，可以接近最佳总迭代次数的性能，而无需进行额外的矩阵计算。 |
| [^26] | [Content Bias in Deep Learning Age Approximation: A new Approach Towards more Explainability.](http://arxiv.org/abs/2310.02067) | 本文提出了一种新的方法来评估深度学习年龄估计中的内容偏差，并验证了训练的神经网络依赖于图像内容。通过使用两种不同的技术减轻图像内容的影响，提出的方法具有潜在的对策效果。 |
| [^27] | [Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks.](http://arxiv.org/abs/2309.16391) | 本文介绍了一种通过Sobolev训练的2-Cats网络，它能够非参数地逼近任何二维Copula，并且在估计输出方面优于现有技术。 |
| [^28] | [HANS, are you clever? Clever Hans Effect Analysis of Neural Systems.](http://arxiv.org/abs/2309.12481) | 这项研究调查了指导调整的大型语言模型（It-LLMs）对多项选择题（MCQ）的鲁棒性能力，在选择顺序变动时揭示了选择偏见和推理能力的问题。 |
| [^29] | [CPLLM: Clinical Prediction with Large Language Models.](http://arxiv.org/abs/2309.11295) | CPLLM是一种使用大规模语言模型进行临床疾病预测的方法。通过量化和提示来微调语言模型，利用患者的历史诊断记录来预测目标疾病的诊断结果。实验证明，CPLLM在各项指标上均超越了其他基线模型，显示出显著的改进。 |
| [^30] | [Large language models can accurately predict searcher preferences.](http://arxiv.org/abs/2309.10621) | 大型语言模型可以通过从真实用户那里获取高质量的第一方数据来准确预测搜索者的偏好。 |
| [^31] | [Fin-Fact: A Benchmark Dataset for Multimodal Financial Fact Checking and Explanation Generation.](http://arxiv.org/abs/2309.08793) | Fin-Fact是一个用于多模态金融事实核查和解释生成的基准数据集，通过提供专业的注释和证据，以及多模态信息源来增强事实性分析，从而打击金融领域的错误信息，促进透明度，并建立信任。 |
| [^32] | [APLA: Additional Perturbation for Latent Noise with Adversarial Training Enables Consistency.](http://arxiv.org/abs/2308.12605) | APLA是一种基于扩散模型的文本到视频生成网络结构，通过引入附加摄动的层噪声与对抗训练，解决了视频生成中一致性细节丢失的问题。 |
| [^33] | [LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked.](http://arxiv.org/abs/2308.07308) | 本文提出了一种通过自检来防御大型语言模型(LLMs)对抗性攻击的简单方法，即让模型自行过滤回应。实验结果表明，即使模型未对齐人类价值观，通过使用语言模型验证内容，仍然可以防止模型向用户呈现有害内容。 |
| [^34] | [ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications.](http://arxiv.org/abs/2307.12488) | 本文通过对ChatGPT在安全导向的程序分析中的表现进行研究，旨在了解其优势和局限性。研究结果可以帮助我们更好地理解ChatGPT在安全领域的应用潜力。 |
| [^35] | [Towards Environmentally Equitable AI via Geographical Load Balancing.](http://arxiv.org/abs/2307.05494) | 本研究通过地理负载平衡的方式解决了人工智能在不同地区对环境的不平等影响，从而推进环境公平的人工智能发展。 |
| [^36] | [Temporalising Unique Characterisability and Learnability of Ontology-Mediated Queries.](http://arxiv.org/abs/2306.07662) | 本文研究了在时间化本体中介查询中唯一可特征性和可学习性的问题，并提出了相应的传递结果。 |
| [^37] | [MULTIGAIN 2.0: MDP controller synthesis for multiple mean-payoff, LTL and steady-state constraints.](http://arxiv.org/abs/2305.16752) | MULTIGAIN 2.0是一个基于PRISM的控制器综合工具，扩展了MultiGain的多维能力，可对具有多维约束的概率系统进行控制器的形式验证和综合，并提供了二维和三维 Pareto 曲线的可视化能力。 |
| [^38] | [Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA.](http://arxiv.org/abs/2304.06027) | 本文提出了一种新方法C-LoRA，用于持续自定制文本到图像扩散模型，并避免了新概念加入后过去相似概念的图像质量下降的问题。 |
| [^39] | [Disentangled Representation Learning.](http://arxiv.org/abs/2211.11695) | 解缠表示学习旨在学习一个模型，能够识别和解缠观测数据中隐藏的因素，从而产生可解释的数据表示。它在提高模型可解释性、可控性、鲁棒性和泛化能力方面具有广泛的应用潜力。 |
| [^40] | [TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation.](http://arxiv.org/abs/2211.09325) | 本论文提出了一种名为TAX-Pose的系统，在机器人操作中实现了任务特定跨姿势的估计。通过学习对象之间的对应关系，这种系统能够在给定操作任务的情况下准确估计两个对象之间的跨姿势，并利用估计结果指导下游的运动规划。 |

# 详细

[^1]: 已经适中的种群规模可证明对噪声具有强大的鲁棒性

    Already Moderate Population Sizes Provably Yield Strong Robustness to Noise

    [https://arxiv.org/abs/2404.02090](https://arxiv.org/abs/2404.02090)

    适中的种群规模可以在先验位噪声存在时保持强鲁棒性，而不会增加在OneMax基准上的渐近运行时间

    

    经验表明，典型的进化算法可以很好地应对诸如嘈杂的函数评估等随机干扰。在第一次针对$(1+\lambda)$和$(1,\lambda)$进化算法在先验位噪声存在时的数学运行时间分析中，我们表明两种算法都能容忍恒定的噪声概率，而不会增加在OneMax基准上的渐近运行时间。为此，种群规模$\lambda$应至少为问题规模$n$的对数。在这方向上的唯一先前结果涉及不太现实的一位噪声模型，需要超线性的问题规模种群大小，并且对于OneMax基准证明了大致是无噪声运行时间的三次方的运行时间保证。我们的显着更强结果基于一种新颖的证明方法，即无噪声后代可以看作是父代和有噪声的后代之间的有偏统一交叉。

    arXiv:2404.02090v1 Announce Type: cross  Abstract: Experience shows that typical evolutionary algorithms can cope well with stochastic disturbances such as noisy function evaluations.   In this first mathematical runtime analysis of the $(1+\lambda)$ and $(1,\lambda)$ evolutionary algorithms in the presence of prior bit-wise noise, we show that both algorithms can tolerate constant noise probabilities without increasing the asymptotic runtime on the OneMax benchmark. For this, a population size $\lambda$ suffices that is at least logarithmic in the problem size $n$. The only previous result in this direction regarded the less realistic one-bit noise model, required a population size super-linear in the problem size, and proved a runtime guarantee roughly cubic in the noiseless runtime for the OneMax benchmark. Our significantly stronger results are based on the novel proof argument that the noiseless offspring can be seen as a biased uniform crossover between the parent and the noisy o
    
[^2]: 文本到图像生成中的偏见调查：定义、评估和缓解

    Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and Mitigation

    [https://arxiv.org/abs/2404.01030](https://arxiv.org/abs/2404.01030)

    该调查综述了文本到图像生成中的偏见问题，重点讨论了性别、肤色和地域文化这些方面，旨在帮助理解当前进展和研究空白。

    

    最近强大的大型模型，如OpenAI的DALLE-3和Google的Gemini，使得用户能够从文本提示生成高质量图像。然而，即使是简单的提示也可能导致文本到图像模型在生成的图像中展现明显的社会偏见。这种偏见可能会导致社会中的分配和代表性伤害，进一步边缘化少数群体。鉴于这一问题，最近有大量研究致力于调查文本到图像系统中偏见的不同维度。然而，对这些研究的全面回顾仍然缺乏，阻碍了对当前进展和研究空白的系统性理解。我们提出了关于文本到图像生成模型中偏见的第一次广泛调查。在这项调查中，我们回顾了先前关于偏见维度的研究：性别、肤色和地域文化。

    arXiv:2404.01030v1 Announce Type: cross  Abstract: The recent advancement of large and powerful models with Text-to-Image (T2I) generation abilities -- such as OpenAI's DALLE-3 and Google's Gemini -- enables users to generate high-quality images from textual prompts. However, it has become increasingly evident that even simple prompts could cause T2I models to exhibit conspicuous social bias in generated images. Such bias might lead to both allocational and representational harms in society, further marginalizing minority groups. Noting this problem, a large body of recent works has been dedicated to investigating different dimensions of bias in T2I systems. However, an extensive review of these studies is lacking, hindering a systematic understanding of current progress and research gaps. We present the first extensive survey on bias in T2I generative models. In this survey, we review prior studies on dimensions of bias: Gender, Skintone, and Geo-Culture. Specifically, we discuss how 
    
[^3]: HARMamba: 基于双向选择性SSM的高效可穿戴传感器人体活动识别

    HARMamba: Efficient Wearable Sensor Human Activity Recognition Based on Bidirectional Selective SSM

    [https://arxiv.org/abs/2403.20183](https://arxiv.org/abs/2403.20183)

    HARMamba利用更轻量级的选择性SSM作为基础模型架构，以解决计算资源挑战

    

    可穿戴传感器的人体活动识别（HAR）是活动感知领域的重要研究领域。最近，一种高效的硬件感知状态空间模型（SSM）Mamba作为一种有前途的替代方案出现。HARMamba引入了更轻量级的选择性SSM作为活动识别的基本模型架构，以解决系统计算负载和内存使用的挑战。

    arXiv:2403.20183v1 Announce Type: cross  Abstract: Wearable sensor human activity recognition (HAR) is a crucial area of research in activity sensing. While transformer-based temporal deep learning models have been extensively studied and implemented, their large number of parameters present significant challenges in terms of system computing load and memory usage, rendering them unsuitable for real-time mobile activity recognition applications. Recently, an efficient hardware-aware state space model (SSM) called Mamba has emerged as a promising alternative. Mamba demonstrates strong potential in long sequence modeling, boasts a simpler network architecture, and offers an efficient hardware-aware design. Leveraging SSM for activity recognition represents an appealing avenue for exploration. In this study, we introduce HARMamba, which employs a more lightweight selective SSM as the foundational model architecture for activity recognition. The goal is to address the computational resourc
    
[^4]: 自我改进学习用于可扩展神经组合优化

    Self-Improved Learning for Scalable Neural Combinatorial Optimization

    [https://arxiv.org/abs/2403.19561](https://arxiv.org/abs/2403.19561)

    提出一种新颖的自我改进学习(SIL)方法，实现神经组合优化的更好可扩展性，通过自身生成解决方案作为伪标签，设计线性复杂度的注意机制来处理大规模组合优化问题实例。

    

    end-to-end神经组合优化(NCO)方法在解决复杂组合优化问题方面表现出有希望的性能，而不需要专家设计。然而，现有方法在处理大规模问题时存在困难，限制了它们的实际适用性。为了克服这一限制，本研究提出了一种新颖的自我改进学习(SIL)方法，以实现神经组合优化的更好可扩展性。具体来说，我们开发了一种高效的自我改进机制，使模型能够在没有标记数据的情况下直接在大规模问题实例上进行训练。通过一种创新的局部重构方法，该方法可以通过自身迭代生成更好的解决方案作为伪标签，以指导有效的模型训练。此外，我们设计了一种线性复杂度的注意机制，使模型能够有效处理低计算开销的大规模组合优化问题实例。

    arXiv:2403.19561v1 Announce Type: cross  Abstract: The end-to-end neural combinatorial optimization (NCO) method shows promising performance in solving complex combinatorial optimization problems without the need for expert design. However, existing methods struggle with large-scale problems, hindering their practical applicability. To overcome this limitation, this work proposes a novel Self-Improved Learning (SIL) method for better scalability of neural combinatorial optimization. Specifically, we develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without any labeled data. Powered by an innovative local reconstruction approach, this method can iteratively generate better solutions by itself as pseudo-labels to guide efficient model training. In addition, we design a linear complexity attention mechanism for the model to efficiently handle large-scale combinatorial problem instances with low computation overhead. Comprehens
    
[^5]: 通过过拟合的遮蔽自编码器检测生成性模仿

    Detecting Generative Parroting through Overfitting Masked Autoencoders

    [https://arxiv.org/abs/2403.19050](https://arxiv.org/abs/2403.19050)

    本研究提出利用过拟合的遮蔽自编码器(MAE)来检测生成模型中的生成性模仿，建立了基于训练数据集损失的检测阈值，为了确保生成模型的合法使用和提升其法律合规性提供了一种新方法。

    

    生成式人工智能模型的出现彻底改变了数字内容创建的方式，然而由于生成性模仿问题，模型过于模仿其训练数据而给版权完整性带来挑战。本研究提出了一种新方法来解决这个问题，即利用一个过拟合的遮蔽自编码器(MAE)来有效地检测这种模仿样本。我们基于训练数据集上的平均损失建立一个检测阈值，从而精确定位修改后数据集中的模仿内容。初步评估表明了有希望的结果，显示了我们方法确保生成模型的合法使用并加强法律合规性方面的潜力。

    arXiv:2403.19050v1 Announce Type: cross  Abstract: The advent of generative AI models has revolutionized digital content creation, yet it introduces challenges in maintaining copyright integrity due to generative parroting, where models mimic their training data too closely. Our research presents a novel approach to tackle this issue by employing an overfitted Masked Autoencoder (MAE) to detect such parroted samples effectively. We establish a detection threshold based on the mean loss across the training dataset, allowing for the precise identification of parroted content in modified datasets. Preliminary evaluations demonstrate promising results, suggesting our method's potential to ensure ethical use and enhance the legal compliance of generative models.
    
[^6]: 一个专家价值一个代币：通过专家代币路由将多个专家LLM协同作为通用型

    An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing

    [https://arxiv.org/abs/2403.16854](https://arxiv.org/abs/2403.16854)

    通过专家代币路由将多个专家LLM协同作为通用型，可以实现多个专家LLMs的无缝集成，支持隐式专业知识的学习和动态扩展新的专家LLMs，同时更好地隐藏协作细节，展现出比现有多LLM协作范式更好的效果和稳健性。

    

    我们提出了专家代币路由（Expert-Token-Routing），这是一个统一的通用型框架，可以实现多个专家LLM的无缝集成。我们的框架将专家LLMs表示为元LLM词汇中的特殊专家代币。元LLM可以路由到专家LLM，就像生成新代币一样。专家代币路由不仅可以从现有的指导数据集中学习专家LLMs的隐式专业知识，还可以以即插即用的方式动态扩展新的专家LLMs。它还可以隐藏用户视角中的详细协作过程，促进交互就像是一个单一的LLM一样。我们的框架在涵盖六个不同专家领域的基准测试中胜过了各种现有的多LLM协作范式，展现了通过协同多个专家LLM来构建通用型LLM系统的效果和稳健性。

    arXiv:2403.16854v1 Announce Type: cross  Abstract: We present Expert-Token-Routing, a unified generalist framework that facilitates seamless integration of multiple expert LLMs. Our framework represents expert LLMs as special expert tokens within the vocabulary of a meta LLM. The meta LLM can route to an expert LLM like generating new tokens. Expert-Token-Routing not only supports learning the implicit expertise of expert LLMs from existing instruction dataset but also allows for dynamic extension of new expert LLMs in a plug-and-play manner. It also conceals the detailed collaboration process from the user's perspective, facilitating interaction as though it were a singular LLM. Our framework outperforms various existing multi-LLM collaboration paradigms across benchmarks that incorporate six diverse expert domains, demonstrating effectiveness and robustness in building generalist LLM system via synergizing multiple expert LLMs.
    
[^7]: 通过互信息正则化进行基准引导的跨说话人唇读

    Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization

    [https://arxiv.org/abs/2403.16071](https://arxiv.org/abs/2403.16071)

    通过利用唇部地标引导的细粒度视觉线索，提出了一种适应说话人的唇读模型，有效降低说话人之间的视觉变化。

    

    Lip reading，即通过视觉唇部运动解释无声语音的过程，由于其广泛的实际应用而引起人们的广泛关注。深度学习方法极大地改进了当前的唇读系统。然而，在说话人变化的交叉说话人场景中进行唇读，由于说话人之间的变异性，存在挑战性问题。一个训练良好的唇读系统在处理全新的说话人时可能表现不佳。为了学习一个适应说话人的唇读模型，一个关键的见解是减少说话人之间的视觉变化，避免模型过度拟合特定说话人。本研究针对基于混合CTC/attention架构的输入视觉线索和基于隐变量表示，提出利用唇部地标引导的细粒度视觉线索，而不是频繁使用的裁剪嘴巴图片作为输入特征，减少说话人特定的外观特征。

    arXiv:2403.16071v1 Announce Type: new  Abstract: Lip reading, the process of interpreting silent speech from visual lip movements, has gained rising attention for its wide range of realistic applications. Deep learning approaches greatly improve current lip reading systems. However, lip reading in cross-speaker scenarios where the speaker identity changes, poses a challenging problem due to inter-speaker variability. A well-trained lip reading system may perform poorly when handling a brand new speaker. To learn a speaker-robust lip reading model, a key insight is to reduce visual variations across speakers, avoiding the model overfitting to specific speakers. In this work, in view of both input visual clues and latent representations based on a hybrid CTC/attention architecture, we propose to exploit the lip landmark-guided fine-grained visual clues instead of frequently-used mouth-cropped images as input features, diminishing speaker-specific appearance characteristics. Furthermore, 
    
[^8]: 知识引导的机器学习：当前趋势与未来展望

    Knowledge-guided Machine Learning: Current Trends and Future Prospects

    [https://arxiv.org/abs/2403.15989](https://arxiv.org/abs/2403.15989)

    知识引导的机器学习（KGML）结合科学知识和数据在机器学习框架中，以实现更好的泛化能力、科学一致性和结果可解释性。

    

    本文概述了科学建模，并讨论了与基于过程的模型相比，机器学习方法在科学建模中的互补优势和劣势。文章还介绍了新兴领域科学知识引导的机器学习（KGML）研究的当前状态，旨在利用科学知识和数据在机器学习框架中实现更好的泛化能力、科学一致性和结果可解释性。我们从使用的科学知识类型、探讨的知识-ML集成形式以及在机器学习中整合科学知识的方法等方面讨论了KGML研究的不同方面。我们还讨论了在环境科学中发展的KGML方法的一些常见用例类别，以每个类别中的实例为例。

    arXiv:2403.15989v1 Announce Type: cross  Abstract: This paper presents an overview of scientific modeling and discusses the complementary strengths and weaknesses of ML methods for scientific modeling in comparison to process-based models. It also provides an introduction to the current state of research in the emerging field of scientific knowledge-guided machine learning (KGML) that aims to use both scientific knowledge and data in ML frameworks to achieve better generalizability, scientific consistency, and explainability of results. We discuss different facets of KGML research in terms of the type of scientific knowledge used, the form of knowledge-ML integration explored, and the method for incorporating scientific knowledge in ML. We also discuss some of the common categories of use cases in environmental sciences where KGML methods are being developed, using illustrative examples in each category.
    
[^9]: 面向资源受限的IoT环境的高效基于Transformer的超参数优化

    Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments

    [https://arxiv.org/abs/2403.12237](https://arxiv.org/abs/2403.12237)

    本文提出了一种通过将Transformer架构和演员-评论家强化学习模型相结合的新方法TRL-HPO，在资源受限的IoT环境中实现了高效的超参数优化，该方法在MNIST数据集上表现优良。

    

    超参数优化（HPO）过程对于找到表现最佳的卷积神经网络（CNNs）至关重要。HPO的自动化过程以其可观的计算占用和缺乏透明度而闻名；这两个因素在资源受限的物联网（IoT）环境中至关重要。本文通过提出一种结合Transformer架构和演员-评论家强化学习（RL）模型的新方法TRL-HPO，旨在解决这些问题，TRL-HPO配备了多头注意力，实现了并行化和渐进生成层。我们通过在MNIST数据集上评估TRL-HPO，并将其与从头开始构建CNN模型的最新方法进行比较，从而从经验上验证了这些假设。结果显示，在相同时间范围内，TRL-HPO的分类结果优于这些方法的结果6.8%，证明了TRL-HPO的高效性。

    arXiv:2403.12237v1 Announce Type: cross  Abstract: The hyper-parameter optimization (HPO) process is imperative for finding the best-performing Convolutional Neural Networks (CNNs). The automation process of HPO is characterized by its sizable computational footprint and its lack of transparency; both important factors in a resource-constrained Internet of Things (IoT) environment. In this paper, we address these problems by proposing a novel approach that combines transformer architecture and actor-critic Reinforcement Learning (RL) model, TRL-HPO, equipped with multi-headed attention that enables parallelization and progressive generation of layers. These assumptions are founded empirically by evaluating TRL-HPO on the MNIST dataset and comparing it with state-of-the-art approaches that build CNN models from scratch. The results show that TRL-HPO outperforms the classification results of these approaches by 6.8% within the same time frame, demonstrating the efficiency of TRL-HPO for 
    
[^10]: Chronos: 学习时间序列的语言

    Chronos: Learning the Language of Time Series

    [https://arxiv.org/abs/2403.07815](https://arxiv.org/abs/2403.07815)

    Chronos框架通过在固定词汇上训练预训练的概率时间序列模型，在大量数据集上进行了全面基准测试，表现出在训练语料库中的数据集上明显优于其他方法，并且在新数据集上的零样本性能表现可比甚至优于其他方法。

    

    我们介绍了Chronos，一个简单但有效的预训练概率时间序列模型框架。Chronos使用缩放和量化将时间序列值标记化为固定词汇，并通过交叉熵损失在这些标记化的时间序列上训练现有的基于Transformer的语言模型架构。我们在大量公开可用数据集上基于T5系列（参数范围从20M到710M）对Chronos模型进行了预训练，同时通过高斯过程生成了一个合成数据集以提高泛化能力。在包含42个数据集的全面基准测试中，涵盖了传统的本地模型和深度学习方法，我们展示了Chronos模型：（a）在训练语料库中的数据集上明显优于其他方法；（b）相对于专门训练的方法，在新数据集上的零样本性能可比甚至优于其他方法。

    arXiv:2403.07815v1 Announce Type: cross  Abstract: We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained spe
    
[^11]: 使用深度集成进行6D物体姿态估计的不确定性量化

    Uncertainty Quantification with Deep Ensembles for 6D Object Pose Estimation

    [https://arxiv.org/abs/2403.07741](https://arxiv.org/abs/2403.07741)

    本研究提出了一种使用深度集成来量化多阶段6D物体姿态估计方法不确定性的方法。

    

    6D物体姿态的估计是许多计算机视觉应用中的基本任务。特别是在人机交互、工业检验和自动化等高风险场景中，可靠的姿态估计至关重要。近年来，提出了越来越精确和鲁棒的基于深度学习的6D物体姿态估计方法。许多表现最佳的方法并非端到端可训练，而是由多个阶段组成。在深度不确定性量化的背景下，深度集成被认为是最先进的，因为它们已被证明能够产生良好校准和鲁棒的不确定性估计。然而，深度集成只能应用于可以端到端训练的方法。在这项工作中，我们提出了一种使用深度集成来量化多阶段6D物体姿态估计方法的不确定性的方法。对于实现，我们选择SurfEmb作为代表，因为它是一种

    arXiv:2403.07741v1 Announce Type: cross  Abstract: The estimation of 6D object poses is a fundamental task in many computer vision applications. Particularly, in high risk scenarios such as human-robot interaction, industrial inspection, and automation, reliable pose estimates are crucial. In the last years, increasingly accurate and robust deep-learning-based approaches for 6D object pose estimation have been proposed. Many top-performing methods are not end-to-end trainable but consist of multiple stages. In the context of deep uncertainty quantification, deep ensembles are considered as state of the art since they have been proven to produce well-calibrated and robust uncertainty estimates. However, deep ensembles can only be applied to methods that can be trained end-to-end. In this work, we propose a method to quantify the uncertainty of multi-stage 6D object pose estimation approaches with deep ensembles. For the implementation, we choose SurfEmb as representative, since it is on
    
[^12]: 使用同态加密的隐私保护扩散模型

    Privacy-Preserving Diffusion Model Using Homomorphic Encryption

    [https://arxiv.org/abs/2403.05794](https://arxiv.org/abs/2403.05794)

    本文介绍了一种使用同态加密的隐私保护扩散模型HE-Diffusion，通过最小失真方法和稀疏张量表示来提高效率，实现了500倍的加速。

    

    在本文中，我们介绍了一种利用同态加密的隐私保护稳定扩散框架，称为HE-Diffusion，主要关注于保护扩散过程中的去噪阶段。HE-Diffusion是一个量身定制的加密框架，专门设计以与稳定扩散的独特架构相匹配，确保隐私和功能性。为了解决固有的计算挑战，我们提出了一种新颖的最小失真方法，使得部分图像加密更加高效，显著降低了开销而不损害模型的输出质量。此外，我们采用了稀疏张量表示来加速计算操作，提高了隐私保护扩散过程的整体效率。我们成功实现了基于HE的隐私保护稳定扩散推理。实验结果表明，HE-Diffusion相比之下实现了500倍的加速。

    arXiv:2403.05794v1 Announce Type: cross  Abstract: In this paper, we introduce a privacy-preserving stable diffusion framework leveraging homomorphic encryption, called HE-Diffusion, which primarily focuses on protecting the denoising phase of the diffusion process. HE-Diffusion is a tailored encryption framework specifically designed to align with the unique architecture of stable diffusion, ensuring both privacy and functionality. To address the inherent computational challenges, we propose a novel min-distortion method that enables efficient partial image encryption, significantly reducing the overhead without compromising the model's output quality. Furthermore, we adopt a sparse tensor representation to expedite computational operations, enhancing the overall efficiency of the privacy-preserving diffusion process. We successfully implement HE-based privacy-preserving stable diffusion inference. The experimental results show that HE-Diffusion achieves 500 times speedup compared wit
    
[^13]: 严格的LIPS沉没舰船：在Battleship中使用语言信息程序抽样提出问题

    Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling

    [https://arxiv.org/abs/2402.19471](https://arxiv.org/abs/2402.19471)

    研究利用大型语言模型提出信息量丰富的问题，在Battleship游戏中展示出与人类表现相匹配的效果，并揭示了贝叶斯模型如何指导问问题行为。

    

    问题结合了我们对语言的掌握和我们对于在有限认知资源情况下推断不确定性的出色能力。人们如何在巨大假设空间中提出信息量丰富的问题？我们研究了这些在基于战舰游戏Battleship的经典提问任务中的权衡。我们的语言信息程序抽样（LIPS）模型利用大型语言模型（LLMs）生成自然语言问题，将其转化为符号程序，并评估其预期信息增益。我们发现，即使在一个令人惊讶的资源预算下，这种简单的蒙特卡罗优化策略也能产生反映人类在各种Battleship棋盘场景中表现的丰富问题。相比之下，仅使用LLM的基线在将问题与棋盘状态联系起来方面存在困难；值得注意的是，GPT-4V并没有比无视觉基线提供改进。我们的结果展示了贝叶斯提问模型如何可能模拟和指导人类的问问题行为。

    arXiv:2402.19471v1 Announce Type: cross  Abstract: Questions combine our mastery of language with our remarkable facility for reasoning about uncertainty. How do people navigate vast hypothesis spaces to pose informative questions given limited cognitive resources? We study these tradeoffs in a classic grounded question-asking task based on the board game Battleship. Our language-informed program sampling (LIPS) model uses large language models (LLMs) to generate natural language questions, translate them into symbolic programs, and evaluate their expected information gain. We find that with a surprisingly modest resource budget, this simple Monte Carlo optimization strategy yields informative questions that mirror human performance across varied Battleship board scenarios. In contrast, LLM-only baselines struggle to ground questions in the board state; notably, GPT-4V provides no improvement over non-visual baselines. Our results illustrate how Bayesian models of question-asking can l
    
[^14]: 基于插值的策略扩散的行为细化

    Behavioral Refinement via Interpolant-based Policy Diffusion

    [https://arxiv.org/abs/2402.16075](https://arxiv.org/abs/2402.16075)

    使用比高斯更具信息量的源头启动扩散方法有助于克服模仿学习任务中的限制。

    

    模仿学习使人工智能代理通过从演示中学习来模仿行为。最近，拥有建模高维度和多模态分布能力的扩散模型在模仿学习任务上表现出色。这些模型通过将动作（或状态）从标准高斯噪声中扩散来塑造策略。然而，要学习的目标策略通常与高斯分布显著不同，这种不匹配可能导致在使用少量扩散步骤（以提高推理速度）和有限数据下性能不佳。这项工作的关键思想是，从比高斯更具信息量的源头开始，可以使扩散方法克服上述限制。我们提供了理论结果、一种新方法和实证发现，展示了使用信息量丰富的源策略的好处。我们的方法，称为BRIDGER，利用了随机性。

    arXiv:2402.16075v1 Announce Type: cross  Abstract: Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to overcome the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGER, leverages the stochast
    
[^15]: 跨架构零样本泛化的视觉分类

    Zero-shot generalization across architectures for visual classification

    [https://arxiv.org/abs/2402.14095](https://arxiv.org/abs/2402.14095)

    不同神经网络在跨架构和层间泛化到未知类别的能力存在差异，准确性并不是泛化能力的良好预测因子，泛化能力随着层深度呈非单调变化。

    

    深度网络的一个关键优势是对未见数据的泛化能力，但其与分类准确性的关系尚不清楚。我们利用一种极简的视觉数据集和一种泛化度量，展示了从深度卷积网络（CNNs）到transformers的流行网络在通过层和架构泛化到未见类别方面的能力存在差异。准确性并不是泛化能力的良好预测因子，并且泛化能力随着层深度呈非单调变化。代码可在https://github.com/dyballa/zero-shot-generalization 找到。

    arXiv:2402.14095v1 Announce Type: cross  Abstract: Generalization to unseen data is a key desideratum for deep networks, but its relation to classification accuracy is unclear. Using a minimalist vision dataset and a measure of generalizability, we show that popular networks, from deep convolutional networks (CNNs) to transformers, vary in their power to extrapolate to unseen classes both across layers and across architectures. Accuracy is not a good predictor of generalizability, and generalization varies non-monotonically with layer depth. Code is available at https://github.com/dyballa/zero-shot-generalization.
    
[^16]: 将大型语言模型作为零-shot对话状态追踪器通过函数调用

    Large Language Models as Zero-shot Dialogue State Tracker through Function Calling

    [https://arxiv.org/abs/2402.10466](https://arxiv.org/abs/2402.10466)

    本研究提出了一种通过函数调用将大型语言模型用于零-shot对话状态追踪的新方法，能够在任务导向对话中取得出色的性能，适应不同领域而无需大量数据收集或模型调整。

    

    大型语言模型（LLMs）在会话系统中日益普遍，这是因为它们在一般情境中具有先进的理解和生成能力。然而，在需要不仅进行响应生成还需要在特定任务和领域内进行有效对话状态追踪（DST）的任务导向对话（TOD）中，它们的有效性仍不尽人意。在这项工作中，我们提出了一种通过函数调用解决LLMs中的DST的新方法FnCTOD。这种方法改进了零-shot DST，使其能够适应各种领域，而无需进行大量数据收集或模型调整。我们的实验结果表明，我们的方法在使用开源或专有LLMs时都取得了出色的性能：通过上下文提示，使得各种7B或13B参数模型超越了之前由ChatGPT实现的最新技术成果（SOTA）的水平，并提高了ChatGPT的性能，击败了

    arXiv:2402.10466v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the
    
[^17]: 数据污染对反事实解释的影响

    The Effect of Data Poisoning on Counterfactual Explanations

    [https://arxiv.org/abs/2402.08290](https://arxiv.org/abs/2402.08290)

    本研究研究了反事实解释在数据污染方面的脆弱性，发现最先进的反事实生成方法和工具包容易受到数据污染的影响。

    

    反事实解释是分析黑盒系统预测的一种流行方法，它们提供了根据不同情况建议改变输入以获得不同（更有利）系统输出的计算补救机会。然而，最近的研究突显了它们对不同类型操纵的脆弱性。本研究研究了反事实解释对数据污染的脆弱性。我们在增加三个不同层次的补救成本方面，形式化地研究了反事实解释在单个实例、某个子组或所有实例上的数据污染。我们证明了最先进的反事实生成方法和工具包对此类数据污染是脆弱的。

    Counterfactual explanations provide a popular method for analyzing the predictions of black-box systems, and they can offer the opportunity for computational recourse by suggesting actionable changes on how to change the input to obtain a different (i.e. more favorable) system output. However, recent work highlighted their vulnerability to different types of manipulations. This work studies the vulnerability of counterfactual explanations to data poisoning. We formalize data poisoning in the context of counterfactual explanations for increasing the cost of recourse on three different levels: locally for a single instance, or a sub-group of instances, or globally for all instances. We demonstrate that state-of-the-art counterfactual generation methods \& toolboxes are vulnerable to such data poisoning.
    
[^18]: 实例级别的安全感知与合成数据质量及其校准

    Instance-Level Safety-Aware Fidelity of Synthetic Data and Its Calibration

    [https://arxiv.org/abs/2402.07031](https://arxiv.org/abs/2402.07031)

    本论文研究了实例级别的合成数据质量与安全感知，引入了四种超越纯视觉特征的合成数据质量，并提出了优化方法来减少合成和真实图像之间的质量差距。

    

    建模和校准合成数据的质量对塑造未来安全可靠的自动驾驶技术至关重要，它提供了一种成本效益高且可扩展的替代方案，可以取代真实世界的数据收集。我们关注其在安全关键应用中的作用，引入了超越纯视觉输入特征的四种实例级别质量，旨在使合成数据与现实世界的安全问题相一致。我们提出了一种优化方法来改进合成数据生成器，减少由基于DNN的组件识别出的质量差距。我们的研究结果表明，这种调优可以增强合成和真实图像中安全关键错误之间的相关性。

    Modeling and calibrating the fidelity of synthetic data is paramount in shaping the future of safe and reliable self-driving technology by offering a cost-effective and scalable alternative to real-world data collection. We focus on its role in safety-critical applications, introducing four types of instance-level fidelity that go beyond mere visual input characteristics. The aim is to align synthetic data with real-world safety issues. We suggest an optimization method to refine the synthetic data generator, reducing fidelity gaps identified by the DNN-based component. Our findings show this tuning enhances the correlation between safety-critical errors in synthetic and real images.
    
[^19]: CIC：一种面向文化感知图像字幕的框架

    CIC: A framework for Culturally-aware Image Captioning

    [https://arxiv.org/abs/2402.05374](https://arxiv.org/abs/2402.05374)

    CIC是一种面向文化感知图像字幕的框架，通过结合视觉问答和大型语言模型，它能够生成能描述图像中文化元素的详细字幕。

    

    图像字幕通过使用视觉-语言预训练模型（VLPs）如BLIP从图像生成描述性句子，这种方法已经取得了很大的改进。然而，当前的方法缺乏对图像中所描绘的文化元素（例如亚洲文化群体的传统服装）生成详细描述性字幕的能力。在本文中，我们提出了一种新的框架，\textbf{面向文化感知图像字幕（CIC）}，该框架能够从代表不同文化的图像中生成字幕并描述文化元素。受到将视觉模态和大型语言模型（LLMs）通过适当的提示进行组合的方法的启发，我们的框架（1）根据图像中的文化类别生成问题，（2）利用生成的问题从视觉问答（VQA）中提取文化视觉元素，（3）使用带有提示的LLMs生成文化感知字幕。我们在4个不同大学的45名参与者上进行了人工评估。

    Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures. Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts. Our human evaluation conducted on 45 participants from 4 dif
    
[^20]: BetterV: 通过有区分度的引导实现可控的Verilog生成

    BetterV: Controlled Verilog Generation with Discriminative Guidance

    [https://arxiv.org/abs/2402.03375](https://arxiv.org/abs/2402.03375)

    本文提出了一种Verilog生成框架BetterV，通过微调大型语言模型和生成判别器的使用，实现了可控的Verilog生成，能够生成语法和功能正确的Verilog实现。

    

    由于现代集成电路（IC）日益复杂，需要自动化的电路设计方法。近年来，硬件设计语言生成方面的研究不断增多，以便促进设计过程。在本文中，我们提出了一个Verilog生成框架BetterV，该框架在处理特定领域数据集的基础上对大型语言模型（LLMs）进行微调，并结合生成判别器以指导特定设计需求。Verilog模块是从互联网中收集、过滤和处理，形成了一个干净而丰富的数据集。通过Instruct-tuning方法，对LLMs进行精细调整，以了解关于Verilog的知识。此外，通过数据增强丰富训练集，并用于训练特定下游任务的生成判别器，为LLMs优化Verilog实现提供指导。BetterV能够生成语法和功能上正确的Verilog实现。

    Due to the growing complexity of modern Integrated Circuits (ICs), there is a need for automated circuit design methods. Recent years have seen rising research in hardware design language generation to facilitate the design process. In this work, we propose a Verilog generation framework, BetterV, which fine-tunes the large language models (LLMs) on processed domain-specific datasets and incorporates generative discriminators for guidance on particular design demands. The Verilog modules are collected, filtered and processed from internet to form a clean and abundant dataset. Instruct-tuning methods are specially designed to fine-tuned the LLMs to understand the knowledge about Verilog. Furthermore, data are augmented to enrich the training set and also used to train a generative discriminator on particular downstream task, which leads a guidance for the LLMs to optimize the Verilog implementation. BetterV has the ability to generate syntactically and functionally correct Verilog, whic
    
[^21]: 独立学习将时间序列片段嵌入

    Learning to Embed Time Series Patches Independently

    [https://arxiv.org/abs/2312.16427](https://arxiv.org/abs/2312.16427)

    学习独立嵌入时间序列片段可以产生更好的时间序列表示，通过简单的块重构任务和独立嵌入每个块的MLP模型以及互补对比学习来实现。

    

    最近，掩码时间序列建模作为一种自监督表示学习策略引起了广泛关注。受计算机视觉中的掩码图像建模启发，最近的研究首先将时间序列进行分块处理并部分掩盖，然后训练Transformer模型通过从未掩盖的块预测被掩盖块来捕捉块之间的依赖关系。然而，我们认为捕捉这种块之间的依赖关系可能不是时间序列表示学习的最佳策略；相反，独立学习嵌入片段会产生更好的时间序列表示。具体而言，我们建议使用1）简单的块重构任务，自动将每个块进行编码而不查看其他块，以及2）独自嵌入每个块的简单块式MLP。此外，我们引入互补对比学习来有效地分层捕获相邻时间序列信息。

    arXiv:2312.16427v2 Announce Type: replace-cross  Abstract: Masked time series modeling has recently gained much attention as a self-supervised representation learning strategy for time series. Inspired by masked image modeling in computer vision, recent works first patchify and partially mask out time series, and then train Transformers to capture the dependencies between patches by predicting masked patches from unmasked patches. However, we argue that capturing such patch dependencies might not be an optimal strategy for time series representation learning; rather, learning to embed patches independently results in better time series representations. Specifically, we propose to use 1) the simple patch reconstruction task, which autoencode each patch without looking at other patches, and 2) the simple patch-wise MLP that embeds each patch independently. In addition, we introduce complementary contrastive learning to hierarchically capture adjacent time series information efficiently. 
    
[^22]: 通过多路径长期船舶轨迹预测建立更安全的海洋环境

    Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting

    [https://arxiv.org/abs/2310.18948](https://arxiv.org/abs/2310.18948)

    通过利用AIS数据预测船舶轨迹，本研究旨在通过减少船舶与鲸鱼碰撞来建立更安全的海洋环境。

    

    海上交通对于实现全球经济增长至关重要，同时也需要在可持续性和保护濒危海洋物种方面履行生态义务，尤其是保护大型鲸类种群。在这方面，自动识别系统(AIS)数据通过提供船舶运动的实时流数据，可以实现强化的交通监控，从而避免船舶与鲸鱼碰撞。本研究探讨利用AIS数据预测长期船舶轨迹，从而预防船舶与鲸鱼的碰撞。为此，我们采用双向长短期记忆网络(Bi-LSTM)构建了一种编码器-解码器模型架构，通过将1到3小时的AIS数据作为输入，预测接下来12小时的船舶轨迹。我们从历史AIS数据中提取潜在路线和目的地的概率特征，并将其作为模型的输入。模型随后预测船舶的轨迹，考虑到潜在路线和目的地的影响。

    Maritime transportation is paramount in achieving global economic growth, entailing concurrent ecological obligations in sustainability and safeguarding endangered marine species, most notably preserving large whale populations. In this regard, the Automatic Identification System (AIS) data plays a significant role by offering real-time streaming data on vessel movement, allowing enhanced traffic monitoring. This study explores using AIS data to prevent vessel-to-whale collisions by forecasting long-term vessel trajectories from engineered AIS data sequences. For such a task, we have developed an encoder-decoder model architecture using Bidirectional Long Short-Term Memory Networks (Bi-LSTM) to predict the next 12 hours of vessel trajectories using 1 to 3 hours of AIS data as input. We feed the model with probabilistic features engineered from historical AIS data that refer to each trajectory's potential route and destination. The model then predicts the vessel's trajectory, considerin
    
[^23]: 混合线性专家用于长期时间序列预测

    Mixture-of-Linear-Experts for Long-term Time Series Forecasting. (arXiv:2312.06786v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.06786](http://arxiv.org/abs/2312.06786)

    MoLE是一种混合线性专家模型，通过训练多个线性中心模型和一个路由模型，能够适应时间序列模式的周期性变化，并显著降低了预测误差。

    

    长期时间序列预测(LTSF)旨在预测给定过去值的时间序列的未来值。当前在这个问题上的最先进技术(SOTA)在某些情况下是由以线性为中心的模型实现的，这些模型主要具有线性映射层。然而，由于其固有的简单性，它们不能够适应时间序列模式的周期性变化。为了解决这个挑战，我们提出了一种混合专家风格的增强线性模型的方法，并提出了混合线性专家(MoLE)。MoLE不是训练单个模型，而是训练多个以线性为中心的模型(即专家)和一个权衡和混合其输出的路由模型。虽然整个框架是端到端训练的，但每个专家都学会专门处理特定的时间模式，而路由模型则学会自适应地组合专家们的输出。实验证明，MoLE降低了线性中心模型(DLinear，RLinear和RMLP)的预测误差。

    Long-term time series forecasting (LTSF) aims to predict future values of a time series given the past values. The current state-of-the-art (SOTA) on this problem is attained in some cases by linear-centric models, which primarily feature a linear mapping layer. However, due to their inherent simplicity, they are not able to adapt their prediction rules to periodic changes in time series patterns. To address this challenge, we propose a Mixture-of-Experts-style augmentation for linear-centric models and propose Mixture-of-Linear-Experts (MoLE). Instead of training a single model, MoLE trains multiple linear-centric models (i.e., experts) and a router model that weighs and mixes their outputs. While the entire framework is trained end-to-end, each expert learns to specialize in a specific temporal pattern, and the router model learns to compose the experts adaptively. Experiments show that MoLE reduces forecasting error of linear-centric models, including DLinear, RLinear, and RMLP, in 
    
[^24]: 优先级软Q分解用于字典型强化学习

    Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning. (arXiv:2310.02360v1 [cs.AI])

    [http://arxiv.org/abs/2310.02360](http://arxiv.org/abs/2310.02360)

    本论文提出了一种用于字典型强化学习的优先级软Q分解算法（PSQD），能够在连续状态-动作空间中学习和适应具有字典型优先级的子任务解决方案，实现了先前学习的子任务解决方案的零-shot组成和适应。

    

    复杂任务的强化学习仍然存在挑战，主要是由于设计标量奖励函数的困难以及从头开发模型的固有低效性。相反，最好是将复杂任务以基本子任务的形式指定，并在可能的情况下重复使用子任务的解决方案。在这项工作中，我们解决了连续空间的字典型多目标强化学习问题，其中包含了优先级子任务，这些问题通常很难解决。我们展示了这些问题可以通过子任务转换进行标量化，并使用价值分解逐步解决。利用这一洞察力，我们提出了优先级软Q分解（PSQD），一种在连续状态-动作空间中学习和适应具有字典型优先级的子任务解决方案的新算法。PSQD能够在零-shot组成之后重复使用先前学习的子任务解决方案，并进行适应步骤。它具备保留子任务训练信息并在复合任务中适应的能力。

    Reinforcement learning (RL) for complex tasks remains a challenge, primarily due to the difficulties of engineering scalar reward functions and the inherent inefficiency of training models from scratch. Instead, it would be better to specify complex tasks in terms of elementary subtasks and to reuse subtask solutions whenever possible. In this work, we address continuous space lexicographic multi-objective RL problems, consisting of prioritized subtasks, which are notoriously difficult to solve. We show that these can be scalarized with a subtask transformation and then solved incrementally using value decomposition. Exploiting this insight, we propose prioritized soft Q-decomposition (PSQD), a novel algorithm for learning and adapting subtask solutions under lexicographic priorities in continuous state-action spaces. PSQD offers the ability to reuse previously learned subtask solutions in a zero-shot composition, followed by an adaptation step. Its ability to use retained subtask trai
    
[^25]: 学习放松：在一系列线性系统实例中设置求解器参数

    Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances. (arXiv:2310.02246v1 [cs.LG])

    [http://arxiv.org/abs/2310.02246](http://arxiv.org/abs/2310.02246)

    本文提出了一种解决一系列线性系统实例中设置求解器参数的方法，通过使用在线学习算法选择参数，可以接近最佳总迭代次数的性能，而无需进行额外的矩阵计算。

    

    解决线性系统$Ax=b$是一种基本的科学计算原理，已经开发了许多求解器和预处理器。它们带有参数，其最佳值取决于要解决的系统，并且通常无法或成本过高以确定；因此在实践中使用次优启发式。我们考虑在需要解决许多相关线性系统的常见情况下，例如在单个数值模拟期间。在这种情况下，我们是否可以顺序选择参数，以获得接近最佳总迭代次数的性能，而无需进行额外的矩阵计算？对于过度轻松（SOR）这种标准求解器，我们回答肯定的。这种方法能够使用仅迭代次数作为反馈的赌徒在线学习算法，选择序列实例的参数，使得总成本接近最佳固定的ω值。

    Solving a linear system $Ax=b$ is a fundamental scientific computing primitive for which numerous solvers and preconditioners have been developed. These come with parameters whose optimal values depend on the system being solved and are often impossible or too expensive to identify; thus in practice sub-optimal heuristics are used. We consider the common setting in which many related linear systems need to be solved, e.g. during a single numerical simulation. In this scenario, can we sequentially choose parameters that attain a near-optimal overall number of iterations, without extra matrix computations? We answer in the affirmative for Successive Over-Relaxation (SOR), a standard solver whose parameter $\omega$ has a strong impact on its runtime. For this method, we prove that a bandit online learning algorithm -- using only the number of iterations as feedback -- can select parameters for a sequence of instances such that the overall cost approaches that of the best fixed $\omega$ as
    
[^26]: 深度学习年龄估计中的内容偏差：朝着更可解释性的新方法

    Content Bias in Deep Learning Age Approximation: A new Approach Towards more Explainability. (arXiv:2310.02067v1 [cs.CV])

    [http://arxiv.org/abs/2310.02067](http://arxiv.org/abs/2310.02067)

    本文提出了一种新的方法来评估深度学习年龄估计中的内容偏差，并验证了训练的神经网络依赖于图像内容。通过使用两种不同的技术减轻图像内容的影响，提出的方法具有潜在的对策效果。

    

    在时间图像取证的背景下，很难确定一个神经网络训练仅仅利用与年龄相关的特征。通常，时间相近的图像（例如属于同一年龄类别的）具有一些共同的内容属性。这种内容偏差可以被神经网络利用。本文提出了一种评估图像内容影响的新方法。该方法使用带有嵌入式年龄信号的合成图像进行验证，通过该方法表明，在年龄分类的上下文中训练的“标准”神经网络在很大程度上依赖于图像内容。作为潜在的对策，本文应用了两种不同的技术来减轻训练过程中图像内容的影响，并且通过所提出的方法进行了评估。

    In the context of temporal image forensics, it is not evident that a neural network, trained on images from different time-slots (classes), exploit solely age related features. Usually, images taken in close temporal proximity (e.g., belonging to the same age class) share some common content properties. Such content bias can be exploited by a neural network. In this work, a novel approach that evaluates the influence of image content is proposed. This approach is verified using synthetic images (where content bias can be ruled out) with an age signal embedded. Based on the proposed approach, it is shown that a `standard' neural network trained in the context of age classification is strongly dependent on image content. As a potential countermeasure, two different techniques are applied to mitigate the influence of the image content during training, and they are also evaluated by the proposed method.
    
[^27]: 通过Sobolev训练的二维Copula逼近变换：2-Cats网络

    Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks. (arXiv:2309.16391v1 [cs.LG])

    [http://arxiv.org/abs/2309.16391](http://arxiv.org/abs/2309.16391)

    本文介绍了一种通过Sobolev训练的2-Cats网络，它能够非参数地逼近任何二维Copula，并且在估计输出方面优于现有技术。

    

    Copula是一种强大的统计工具，用于捕捉数据维度之间的依赖关系。在应用Copula时，我们可以通过首先估计独立的边际分布（一个简单任务），然后估计连接边际的单个Copula函数C（一个困难任务）来估计多元分布函数。对于二维数据，Copula是一个形如C：(u，v)∈\mathbf{I}^2\rightarrow \mathbf{I}的二次增函数，其中\mathbf{I}=[0，1]。在本文中，我们展示了神经网络（NNs）如何能够非参数地逼近任何二维Copula。我们的方法被称为2-Cats，受到物理启发的神经网络和Sobolev训练文献的启发。我们不仅证明了我们能够比现有技术更好地估计2D Copula的输出，而且我们的方法是非参数的，并且符合Copula C的数学性质。

    Copulas are a powerful statistical tool that captures dependencies across data dimensions. When applying Copulas, we can estimate multivariate distribution functions by initially estimating independent marginals, an easy task, and then a single copulating function, $C$, to connect the marginals, a hard task. For two-dimensional data, a copula is a two-increasing function of the form $C: (u,v)\in \mathbf{I}^2 \rightarrow \mathbf{I}$, where $\mathbf{I} = [0, 1]$. In this paper, we show how Neural Networks (NNs) can approximate any two-dimensional copula non-parametrically. Our approach, denoted as 2-Cats, is inspired by the Physics-Informed Neural Networks and Sobolev Training literature. Not only do we show that we can estimate the output of a 2d Copula better than the state-of-the-art, our approach is non-parametric and respects the mathematical properties of a Copula $C$.
    
[^28]: HANS，你聪明吗？神经系统的Clever Hans效应分析

    HANS, are you clever? Clever Hans Effect Analysis of Neural Systems. (arXiv:2309.12481v1 [cs.CL])

    [http://arxiv.org/abs/2309.12481](http://arxiv.org/abs/2309.12481)

    这项研究调查了指导调整的大型语言模型（It-LLMs）对多项选择题（MCQ）的鲁棒性能力，在选择顺序变动时揭示了选择偏见和推理能力的问题。

    

    指导调整的大型语言模型(It-LLMs)展示出了在认知状态、意图和反应方面推理的出色能力，可以让人们有效地引导和理解日常社交互动。事实上，已经提出了几个多项选择题(MCQ)基准来构建对模型能力的确切评估。然而，早期的研究表明It-LLMs中存在固有的“顺序偏见”，给适当的评估带来了挑战。本文通过使用四个MCQ基准对It-LLMs的抵抗能力进行了研究。通过引入对抗性示例，我们展示了显著的性能差距，特别是在选择顺序变动时，揭示了选择偏见并引发了对推理能力的讨论。通过第一位置和模型选择之间的相关性，我们假设在模型中存在结构启发式方法。

    Instruction-tuned Large Language Models (It-LLMs) have been exhibiting outstanding abilities to reason around cognitive states, intentions, and reactions of all people involved, letting humans guide and comprehend day-to-day social interactions effectively. In fact, several multiple-choice questions (MCQ) benchmarks have been proposed to construct solid assessments of the models' abilities. However, earlier works are demonstrating the presence of inherent "order bias" in It-LLMs, posing challenges to the appropriate evaluation. In this paper, we investigate It-LLMs' resilience abilities towards a series of probing tests using four MCQ benchmarks. Introducing adversarial examples, we show a significant performance gap, mainly when varying the order of the choices, which reveals a selection bias and brings into discussion reasoning abilities. Following a correlation between first positions and model choices due to positional bias, we hypothesized the presence of structural heuristics in 
    
[^29]: CPLLM: 基于大规模语言模型的临床预测

    CPLLM: Clinical Prediction with Large Language Models. (arXiv:2309.11295v1 [cs.CL])

    [http://arxiv.org/abs/2309.11295](http://arxiv.org/abs/2309.11295)

    CPLLM是一种使用大规模语言模型进行临床疾病预测的方法。通过量化和提示来微调语言模型，利用患者的历史诊断记录来预测目标疾病的诊断结果。实验证明，CPLLM在各项指标上均超越了其他基线模型，显示出显著的改进。

    

    我们提出了一种使用大规模语言模型 (LLM) 进行临床疾病预测的方法，该方法包括对预训练的语言模型进行微调。我们利用量化和提示来微调LLM，任务是预测患者在下一次就诊或随后的诊断中是否会被诊断为目标疾病，并利用他们的历史诊断记录。我们将结果与多个基线模型进行了比较，包括逻辑回归、RETAIN和Med-BERT，后者是使用结构化电子病历数据进行疾病预测的当前最先进模型。实验结果显示，CPLLM在PR-AUC和ROC-AUC指标上均超过了所有测试模型，相比基线模型显示出显著的改进。

    We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical disease prediction. We utilized quantization and fine-tuned the LLM using prompts, with the task of predicting whether patients will be diagnosed with a target disease during their next visit or in the subsequent diagnosis, leveraging their historical diagnosis records. We compared our results versus various baselines, including Logistic Regression, RETAIN, and Med-BERT, which is the current state-of-the-art model for disease prediction using structured EHR data. Our experiments have shown that CPLLM surpasses all the tested models in terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements compared to the baseline models.
    
[^30]: 大型语言模型能够准确预测搜索者的偏好

    Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])

    [http://arxiv.org/abs/2309.10621](http://arxiv.org/abs/2309.10621)

    大型语言模型可以通过从真实用户那里获取高质量的第一方数据来准确预测搜索者的偏好。

    

    相关性标签是评估和优化搜索系统的关键。获取大量相关性标签通常需要第三方标注人员，但存在低质量数据的风险。本论文介绍了一种改进标签质量的替代方法，通过从真实用户那里获得仔细反馈来获取高质量的第一方数据。

    Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
    
[^31]: Fin-Fact:一种面向多模态金融事实核查和解释生成的基准数据集

    Fin-Fact: A Benchmark Dataset for Multimodal Financial Fact Checking and Explanation Generation. (arXiv:2309.08793v1 [cs.AI])

    [http://arxiv.org/abs/2309.08793](http://arxiv.org/abs/2309.08793)

    Fin-Fact是一个用于多模态金融事实核查和解释生成的基准数据集，通过提供专业的注释和证据，以及多模态信息源来增强事实性分析，从而打击金融领域的错误信息，促进透明度，并建立信任。

    

    金融领域的事实核查尚未充分探索，该领域缺乏高质量的数据集。本文提出了Fin-Fact，一种用于金融领域多模态事实核查的基准数据集。值得注意的是，它包括专业事实核查人员的注释和证据，提供专业知识和可信度。由于其多模态性质涵盖了文本和视觉内容，Fin-Fact提供了补充信息源，以增强事实性分析。其主要目标是在金融领域打击错误信息，促进透明度，并在财务报告和新闻传播中建立信任。通过提供有深度的解释，Fin-Fact使用户，包括领域专家和终端用户，能够理解事实核查决策的推理过程，验证声明的可信度，并促进对事实核查流程的信任。Fin-Fact数据集以及我们的实验代码可在https://github.com/IIT-DM/Fin-Fact/ 上找到。

    Fact-checking in financial domain is under explored, and there is a shortage of quality dataset in this domain. In this paper, we propose Fin-Fact, a benchmark dataset for multimodal fact-checking within the financial domain. Notably, it includes professional fact-checker annotations and justifications, providing expertise and credibility. With its multimodal nature encompassing both textual and visual content, Fin-Fact provides complementary information sources to enhance factuality analysis. Its primary objective is combating misinformation in finance, fostering transparency, and building trust in financial reporting and news dissemination. By offering insightful explanations, Fin-Fact empowers users, including domain experts and end-users, to understand the reasoning behind fact-checking decisions, validating claim credibility, and fostering trust in the fact-checking process. The Fin-Fact dataset, along with our experimental codes is available at https://github.com/IIT-DM/Fin-Fact/
    
[^32]: APLA: 附加摄动的层噪声与对抗训练使一致性成为可能

    APLA: Additional Perturbation for Latent Noise with Adversarial Training Enables Consistency. (arXiv:2308.12605v1 [cs.CV])

    [http://arxiv.org/abs/2308.12605](http://arxiv.org/abs/2308.12605)

    APLA是一种基于扩散模型的文本到视频生成网络结构，通过引入附加摄动的层噪声与对抗训练，解决了视频生成中一致性细节丢失的问题。

    

    扩散模型在视频生成方面取得了令人期待的进展。然而，它们经常难以在帧之间保留局部区域的一致细节。其中一个潜在原因是传统扩散模型在逼近高斯噪声分布时利用了预测噪声，没有充分考虑输入本身的内在信息的影响。此外，这些模型强调预测和参考之间的区别，忽视了视频本身固有的信息。为了解决这个限制，受到自注意机制的启发，我们提出了一种基于扩散模型的文本到视频（T2V）生成网络结构，名为附加摄动的层噪声与对抗训练（APLA）。我们的方法只需要一个视频作为输入，并建立在预训练稳定的扩散网络上。值得注意的是，我们引入了一个额外的紧凑网络，称为视频生成变换器（VGT）。

    Diffusion models have exhibited promising progress in video generation. However, they often struggle to retain consistent details within local regions across frames. One underlying cause is that traditional diffusion models approximate Gaussian noise distribution by utilizing predictive noise, without fully accounting for the impact of inherent information within the input itself. Additionally, these models emphasize the distinction between predictions and references, neglecting information intrinsic to the videos. To address this limitation, inspired by the self-attention mechanism, we propose a novel text-to-video (T2V) generation network structure based on diffusion models, dubbed Additional Perturbation for Latent noise with Adversarial training (APLA). Our approach only necessitates a single video as input and builds upon pre-trained stable diffusion networks. Notably, we introduce an additional compact network, known as the Video Generation Transformer (VGT). This auxiliary compo
    
[^33]: LLM自卫：通过自检，LLMs意识到它们被愚弄了。

    LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked. (arXiv:2308.07308v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.07308](http://arxiv.org/abs/2308.07308)

    本文提出了一种通过自检来防御大型语言模型(LLMs)对抗性攻击的简单方法，即让模型自行过滤回应。实验结果表明，即使模型未对齐人类价值观，通过使用语言模型验证内容，仍然可以防止模型向用户呈现有害内容。

    

    近年来，大型语言模型（LLMs）由于其能够对人类提示做出高质量文本回应而变得非常受欢迎。然而，研究表明，这些模型在回应用户提示时可能生成有害内容（例如，给用户提供犯罪指导）。文献中已经着重研究如何通过方法（例如通过强化学习将模型与人类价值观对齐）来减轻这些风险。然而，研究发现，即使对齐的语言模型也容易受到绕过生成有害文本限制的对抗性攻击。我们提出了一种简单的方法来防御这些攻击，即大型语言模型对自己的回应进行过滤。我们目前的研究结果表明，即使模型没有被微调以与人类价值观对齐，也可以通过使用语言模型验证内容来防止其向用户呈现有害内容。

    Large language models (LLMs) have skyrocketed in popularity in recent years due to their ability to generate high-quality text in response to human prompting. However, these models have been shown to have the potential to generate harmful content in response to user prompting (e.g., giving users instructions on how to commit crimes). There has been a focus in the literature on mitigating these risks, through methods like aligning models with human values through reinforcement learning. However, it has been shown that even aligned language models are susceptible to adversarial attacks that bypass their restrictions on generating harmful text. We propose a simple approach to defending against these attacks by having a large language model filter its own responses. Our current results show that even if a model is not fine-tuned to be aligned with human values, it is possible to stop it from presenting harmful content to users by validating the content using a language model.
    
[^34]: ChatGPT用于软件安全：探索ChatGPT在安全应用中的优点和局限性

    ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications. (arXiv:2307.12488v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2307.12488](http://arxiv.org/abs/2307.12488)

    本文通过对ChatGPT在安全导向的程序分析中的表现进行研究，旨在了解其优势和局限性。研究结果可以帮助我们更好地理解ChatGPT在安全领域的应用潜力。

    

    作为一个多才多艺的大型语言模型，ChatGPT在各个领域应对问题的潜力得到了显著的展示。它能够分析、理解和综合来自在线资源和用户输入的信息，引起了广泛的关注。先前的研究已经探索了ChatGPT在代码生成和代码审查方面的能力。在本文中，我们深入研究了ChatGPT在面向安全的程序分析中的能力，从攻击者和安全分析师的角度进行了探讨。我们通过一个案例研究来评估ChatGPT在几个安全导向的程序分析任务中的回答质量，并有意地引入挑战来评估其响应能力。通过对ChatGPT提供的答案质量的考察，我们对其在安全导向的程序分析领域的优点和局限性有了更清晰的认识。

    ChatGPT, as a versatile large language model, has demonstrated remarkable potential in addressing inquiries across various domains. Its ability to analyze, comprehend, and synthesize information from both online sources and user inputs has garnered significant attention. Previous research has explored ChatGPT's competence in code generation and code reviews. In this paper, we delve into ChatGPT's capabilities in security-oriented program analysis, focusing on perspectives from both attackers and security analysts. We present a case study involving several security-oriented program analysis tasks while deliberately introducing challenges to assess ChatGPT's responses. Through an examination of the quality of answers provided by ChatGPT, we gain a clearer understanding of its strengths and limitations in the realm of security-oriented program analysis.
    
[^35]: 通过地理负载平衡实现环境公平的人工智能

    Towards Environmentally Equitable AI via Geographical Load Balancing. (arXiv:2307.05494v1 [cs.AI])

    [http://arxiv.org/abs/2307.05494](http://arxiv.org/abs/2307.05494)

    本研究通过地理负载平衡的方式解决了人工智能在不同地区对环境的不平等影响，从而推进环境公平的人工智能发展。

    

    受到大型语言和基础模型的飙升人气推动，人工智能（AI）模型的巨大环境足迹的快速增长受到了越来越多的关注。虽然提出了许多方法使AI更节能环保，但环境不平等——即AI的环境足迹在某些地区可能不成比例地更高——已经出现，并引发了社会生态正义的关切。本文通过平衡AI的区域负面环境影响来首次解决AI的环境不平等问题。具体而言，我们专注于AI模型推理的碳足迹和水足迹，并提出了注重公平的地理负载平衡（GLB）来明确解决AI对最弱势地区的环境影响。我们通过考虑一组分布在地理上的10个数据中心来运行基于跟踪的仿真，这些数据中心为大型L

    Fueled by the soaring popularity of large language and foundation models, the accelerated growth of artificial intelligence (AI) models' enormous environmental footprint has come under increased scrutiny. While many approaches have been proposed to make AI more energy-efficient and environmentally friendly, environmental inequity -- the fact that AI's environmental footprint can be disproportionately higher in certain regions than in others -- has emerged, raising social-ecological justice concerns. This paper takes a first step toward addressing AI's environmental inequity by balancing its regional negative environmental impact. Concretely, we focus on the carbon and water footprints of AI model inference and propose equity-aware geographical load balancing (GLB) to explicitly address AI's environmental impacts on the most disadvantaged regions. We run trace-based simulations by considering a set of 10 geographically-distributed data centers that serve inference requests for a large l
    
[^36]: 时间化本体中介查询的唯一可特征性和可学习性

    Temporalising Unique Characterisability and Learnability of Ontology-Mediated Queries. (arXiv:2306.07662v1 [cs.AI])

    [http://arxiv.org/abs/2306.07662](http://arxiv.org/abs/2306.07662)

    本文研究了在时间化本体中介查询中唯一可特征性和可学习性的问题，并提出了相应的传递结果。

    

    最近，通过示例来研究数据库查询的唯一可特征性和可学习性已经扩展到了本体中介查询。在这里，我们研究了获得的结果在多大程度上可以提升到时间化的本体中介查询。我们系统地介绍了非时间化情况下相关方法，然后展示了通用的传递结果，可以确定现有结果在何种条件下可以推广到时间化查询。

    Recently, the study of the unique characterisability and learnability of database queries by means of examples has been extended to ontology-mediated queries. Here, we study in how far the obtained results can be lifted to temporalised ontology-mediated queries. We provide a systematic introduction to the relevant approaches in the non-temporal case and then show general transfer results pinpointing under which conditions existing results can be lifted to temporalised queries.
    
[^37]: MULTIGAIN 2.0：多个平均回报、LTL和稳态约束的MDP控制器综合

    MULTIGAIN 2.0: MDP controller synthesis for multiple mean-payoff, LTL and steady-state constraints. (arXiv:2305.16752v1 [cs.AI])

    [http://arxiv.org/abs/2305.16752](http://arxiv.org/abs/2305.16752)

    MULTIGAIN 2.0是一个基于PRISM的控制器综合工具，扩展了MultiGain的多维能力，可对具有多维约束的概率系统进行控制器的形式验证和综合，并提供了二维和三维 Pareto 曲线的可视化能力。

    

    我们介绍了MULTIGAIN 2.0，这是一个基于概率模型检查器PRISM构建的控制器综合工具MultiGain的一个重大扩展。这个新版本扩展了MultiGain的多目标能力，允许对具有多维长期平均回报结构、稳态约束和线性时态逻辑属性的概率系统进行控制器的形式验证和综合。此外，MULTIGAIN 2.0提供了一种寻找有限内存解决方案的方法，并具有二维和三维 Pareto 曲线的可视化能力，以便在多目标情况下进行权衡分析。

    We present MULTIGAIN 2.0, a major extension to the controller synthesis tool MultiGain, built on top of the probabilistic model checker PRISM. This new version extends MultiGain's multi-objective capabilities, by allowing for the formal verification and synthesis of controllers for probabilistic systems with multi-dimensional long-run average reward structures, steady-state constraints, and linear temporal logic properties. Additionally, MULTIGAIN 2.0 provides an approach for finding finite memory solutions and the capability for two- and three-dimensional visualization of Pareto curves to facilitate trade-off analysis in multi-objective scenarios
    
[^38]: 持续扩散：使用C-LoRA进行文本到图像扩散的持续定制

    Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA. (arXiv:2304.06027v1 [cs.CV])

    [http://arxiv.org/abs/2304.06027](http://arxiv.org/abs/2304.06027)

    本文提出了一种新方法C-LoRA，用于持续自定制文本到图像扩散模型，并避免了新概念加入后过去相似概念的图像质量下降的问题。

    

    最近的研究表明，在只提供少量示例图像的情况下，自定义文本到图像扩散模型具有显着的能力。我们的研究表明，当使用多个细粒度概念以连续方式（即持续性地）自定义这样的模型时，最近的文本到图像定制技术会遭受灾难性的遗忘。为了解决这个问题，我们提出了一种新方法，C-LoRA，采用流行的稳定扩散模型中的跨注意力层中的连续自我正则化低秩适应。此外，我们使用不包括自定义对象的单词（即“人”用于人脸数据集）并初始化为完全随机嵌入的定制提示。重要的是，我们的方法只引入了微小的额外参数成本。

    Recent works demonstrate a remarkable ability to customize text-to-image diffusion models while only providing a few example images. What happens if you try to customize such models using multiple, fine-grained concepts in a sequential (i.e., continual) manner? In our work, we show that recent state-of-the-art customization of text-to-image models suffer from catastrophic forgetting when new concepts arrive sequentially. Specifically, when adding a new concept, the ability to generate high quality images of past, similar concepts degrade. To circumvent this forgetting, we propose a new method, C-LoRA, composed of a continually self-regularized low-rank adaptation in cross attention layers of the popular Stable Diffusion model. Furthermore, we use customization prompts which do not include the word of the customized object (i.e., "person" for a human face dataset) and are initialized as completely random embeddings. Importantly, our method induces only marginal additional parameter cost
    
[^39]: 解缠表示学习

    Disentangled Representation Learning. (arXiv:2211.11695v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11695](http://arxiv.org/abs/2211.11695)

    解缠表示学习旨在学习一个模型，能够识别和解缠观测数据中隐藏的因素，从而产生可解释的数据表示。它在提高模型可解释性、可控性、鲁棒性和泛化能力方面具有广泛的应用潜力。

    

    解缠表示学习（DRL）旨在学习一个能够识别和解缠可观测数据中隐藏因素的模型。将变化的潜在要素分离成具有语义意义的变量的过程有助于学习可解释的数据表示，模仿人类观察对象或关系时的有意义理解过程。作为一种通用的学习策略，DRL在多个领域中展示了提高模型可解释性、可控性、鲁棒性以及泛化能力的优势，如计算机视觉、自然语言处理、数据挖掘等。本文综合评述了DRL的各个方面，包括动机、定义、方法论、评估、应用和模型设计。我们讨论了基于两个公认定义（直观定义和群论定义）的DRL方法。我们进一步分析了DRL的开展。

    Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controlability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, data mining etc. In this article, we comprehensively review DRL from various aspects including motivations, definitions, methodologies, evaluations, applications and model designs. We discuss works on DRL based on two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition. We further ca
    
[^40]: TAX-Pose：机器人操作的任务特定跨姿势估计

    TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation. (arXiv:2211.09325v2 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2211.09325](http://arxiv.org/abs/2211.09325)

    本论文提出了一种名为TAX-Pose的系统，在机器人操作中实现了任务特定跨姿势的估计。通过学习对象之间的对应关系，这种系统能够在给定操作任务的情况下准确估计两个对象之间的跨姿势，并利用估计结果指导下游的运动规划。

    

    我们如何赋予机器人有效地操作未知物体的能力，并基于示范转移相关技能？端到端学习方法通常无法泛化到新的物体或未见过的配置。相反，我们关注交互对象相关部分的任务特定姿势关系。我们推测这种关系是一种可以转移到同一类别新物体的操作任务的可泛化概念；例如，平底锅相对于烤箱的姿势关系或者杯子相对于杯架的姿势关系。我们称这种任务特定姿势关系为“跨姿势”，并提供了该概念的数学定义。我们提出了一个基于视觉的系统，使用学习的对象间对应关系来学习估计给定操作任务的两个对象之间的跨姿势。然后，估计的跨姿势用于引导下游的运动规划器将对象操纵到所需的姿势。

    How do we imbue robots with the ability to efficiently manipulate unseen objects and transfer relevant skills based on demonstrations? End-to-end learning methods often fail to generalize to novel objects or unseen configurations. Instead, we focus on the task-specific pose relationship between relevant parts of interacting objects. We conjecture that this relationship is a generalizable notion of a manipulation task that can transfer to new objects in the same category; examples include the relationship between the pose of a pan relative to an oven or the pose of a mug relative to a mug rack. We call this task-specific pose relationship "cross-pose" and provide a mathematical definition of this concept. We propose a vision-based system that learns to estimate the cross-pose between two objects for a given manipulation task using learned cross-object correspondences. The estimated cross-pose is then used to guide a downstream motion planner to manipulate the objects into the desired po
    

