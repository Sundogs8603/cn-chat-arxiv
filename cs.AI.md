# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Human-AI Co-Creation Approach to Find Forever Chemicals Replacements.](http://arxiv.org/abs/2304.05389) | 这篇论文介绍了一种人工智能与人类共创的方法，利用生成模型加速寻找替代“永久化学品”的过程，以减少对环境和人体健康造成的伤害。 |
| [^2] | [Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance.](http://arxiv.org/abs/2304.05372) | 本文通过测量OpenAI ChatGPT和Google Bard等AI聊天机器人的可靠性发现，其在感知和评估写作提示的复杂性方面与经验丰富的人类评分者的一致性较低。 |
| [^3] | [Those Aren't Your Memories, They're Somebody Else's: Seeding Misinformation in Chat Bot Memories.](http://arxiv.org/abs/2304.05371) | 本文研究了聊天机器人的一个新发展：长期记忆机制。然而，我们发现这种机制可能会导致机器人记住了错误和虚假的信息，并将其作为事实陈述重复。 |
| [^4] | [Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding.](http://arxiv.org/abs/2304.05368) | 本研究全面评估了大型语言模型在临床语言理解任务上的表现，并引入自问自答提示策略来提高LLMs在医疗保健相关任务中的效果。 |
| [^5] | [Asymmetric Polynomial Loss For Multi-Label Classification.](http://arxiv.org/abs/2304.05361) | 本文提出了一种有效的非对称多项式损失（APL），可以根据不同任务优化模型，并在关系提取、文本分类和图像分类上表现良好。 |
| [^6] | [iDML: Incentivized Decentralized Machine Learning.](http://arxiv.org/abs/2304.05354) | 通过区块链的激励机制，让用户资源贡献变得可行，实现去中心化的机器学习。 |
| [^7] | [Toxicity in ChatGPT: Analyzing Persona-assigned Language Models.](http://arxiv.org/abs/2304.05335) | 论文分析了基于对话的大型语言模型ChatGPT中的毒性，通过指定人物角色，输出会涉及刻板印象、有害对话和伤人的言论。因此，我们需要充分理解LLMs的能力和局限性，以确保这些系统的安全性。 |
| [^8] | [MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos.](http://arxiv.org/abs/2304.05292) | 本文提出了一种新的深度机器学习模型MC-ViViT，用于通过分析面部特征检测老年人轻度认知障碍。通过MC模块和结合损失函数来解决数据集样本不平衡问题，提高了算法的性能。 |
| [^9] | [Task Difficulty Aware Parameter Allocation & Regularization for Lifelong Learning.](http://arxiv.org/abs/2304.05288) | 本文提出了一种任务难度感知的增量学习参数分配与正则化方法，该方法可以根据任务的学习难度自适应地选择适当的分配或正则化策略，以克服在学习不同任务时的挑战。 |
| [^10] | [Automaton-Guided Curriculum Generation for Reinforcement Learning Agents.](http://arxiv.org/abs/2304.05271) | 提出了自动机引导下的强化学习代理课程生成方法，能够自动生成逐渐递增的适合代理学习的中间任务序列，并能在离散和连续控制任务上比现有方法提供高达5倍的快速学习。 |
| [^11] | [Controllable Textual Inversion for Personalized Text-to-Image Generation.](http://arxiv.org/abs/2304.05265) | 本文提出了一种名为COTI的技术，通过引入理论指导的损失目标和全面的加权评分机制，并结合主动学习范式来解决文本反转时的困难，提供了一个强大，数据效率高，易于使用的框架。 |
| [^12] | [Prompt Learning for News Recommendation.](http://arxiv.org/abs/2304.05263) | 本文介绍了在新闻推荐领域首次采用预训练，提示学习和预测范例来开发Prompt4NR框架的实验。该框架将预测点击候选新闻的任务转化为cloze-style填空式掩码预测任务，从而更好地利用预训练过程中的丰富语义信息和语言知识。 |
| [^13] | [Re-Weighted Softmax Cross-Entropy to Control Forgetting in Federated Learning.](http://arxiv.org/abs/2304.05260) | 本文提出一种重新加权softmax的交叉熵方法来解决联邦学习中客户端的灾难性遗忘问题，并证明这种方法可以缓解客户端遗忘并对标准联邦学习算法提供一致的改进。 |
| [^14] | [Multi-granulariy Time-based Transformer for Knowledge Tracing.](http://arxiv.org/abs/2304.05257) | 本文提出了一种基于Transformer的架构用于准确地预测学生在标准化测试中的表现。该模型考虑了学生的历史数据，包括他们以往的考试成绩、学习习惯和其他相关信息，并在解码器输入中使用了多个时间特征粒度以显著提高模型性能。与LightGBM相比，该方法更加准确，为教育领域的AI发展提供了一个可伸缩和准确的预测学生成果的工具。 |
| [^15] | [OpenAL: Evaluation and Interpretation of Active Learning Strategies.](http://arxiv.org/abs/2304.05246) | OpenAL是一个灵活且开源的框架，可以在一系列现实任务上轻松运行和比较采样AL策略，具有可解释性指标和统计分析方法，针对主动学习的一次性特性，从业人员也可以轻松扩展基准测试。 |
| [^16] | [Towards Efficient Fine-tuning of Pre-trained Code Models: An Experimental Study and Beyond.](http://arxiv.org/abs/2304.05216) | 本文研究了对预训练代码模型的微调，探索了各层预训练表示和编码的代码知识，提出了有效的微调方案。实验发现微调过程可以保留大部分代码属性，基本代码属性由较低和中间层捕获。 |
| [^17] | [A Billion-scale Foundation Model for Remote Sensing Images.](http://arxiv.org/abs/2304.05215) | 本文介绍了一个用于遥感图像的十亿级基础模型，并研究了增加模型参数数量对该模型在下游任务中的性能影响，实验显示增加模型参数数量可以显著提高性能。 |
| [^18] | [CGXplain: Rule-Based Deep Neural Network Explanations Using Dual Linear Programs.](http://arxiv.org/abs/2304.05207) | 该论文介绍了一种名为 CGX 的分解方法，使用双线性规划从深度神经网络的隐藏表示中提取规则，优化对齐，复杂度和稳定性。 |
| [^19] | [TinyReptile: TinyML with Federated Meta-Learning.](http://arxiv.org/abs/2304.05201) | TinyReptile是一个联邦元学习实现的迷你机器学习算法，可以在迷你设备上协作学习神经网络，并通过使用全局数据实现快速收敛和保护本地数据隐私。 |
| [^20] | [Automatic Gradient Descent: Deep Learning without Hyperparameters.](http://arxiv.org/abs/2304.05187) | 本文提出了一种使用神经网络结构信息定义优化算法的方法，实现了一种无需手动调整超参数的一阶优化器 - 自动梯度下降。该算法在深度全连接网络和卷积网络中表现良好，并在标准基准测试数据集上表现出与手动调整优化器相当的效果。 |
| [^21] | [Decoupling anomaly discrimination and representation learning: self-supervised learning for anomaly detection on attributed graph.](http://arxiv.org/abs/2304.05176) | 本文提出了一种独特的自我监督算法DSLAD，通过解耦异常判别和表示学习来进行属性图上的异常检测，构建了一个平衡的特征空间解决了语义混合和不平衡问题，证明了其有效性。 |
| [^22] | [Artificial Collective Intelligence Engineering: a Survey of Concepts and Perspectives.](http://arxiv.org/abs/2304.05147) | 人工集体智能工程旨在利用大量个体产生超越个体能力的效应和集体智能行为，成为计算系统设计目标。本文综述了人工集体智能工程的角色、概念和挑战，强调其对计算系统工程和设计的相关性和潜在影响。 |
| [^23] | [Teaching Large Language Models to Self-Debug.](http://arxiv.org/abs/2304.05128) | 本文提出了一种自我调试方法，通过少量演示来教授大型语言模型自动调试其预测的程序，在多项代码生成基准测试中取得了最先进的性能。 |
| [^24] | [Evaluation of Differentially Constrained Motion Models for Graph-Based Trajectory Prediction.](http://arxiv.org/abs/2304.05116) | 使用深度学习模型进行运动预测在自动驾驶中表现出色，但缺乏解释性和可能违反物理约束。因此，结合差分约束运动模型能提供物理上可行的轨迹，研究表明低阶积分器模型表现更好，并且数值求解器对模型性能产生影响。 |
| [^25] | [If consciousness is dynamically relevant, artificial intelligence isn't conscious.](http://arxiv.org/abs/2304.05077) | 本文指出，如果意识在动态上具有相关性，则人工智能系统无法具有意识，因为这些系统的设计和验证排除了可能与意识相关的动力学效应。 |
| [^26] | [Artificial intelligence based prediction on lung cancer risk factors using deep learning.](http://arxiv.org/abs/2304.05065) | 本研究利用深度学习开发出一种高精度的肺癌检测模型，可用于预测肺癌风险因素。 |
| [^27] | [A Comprehensive Survey on Deep Graph Representation Learning.](http://arxiv.org/abs/2304.05055) | 本文综述了深度图表示学习的研究现状和存在的问题，并指出利用深度学习已经显示出巨大的优势和潜力。 |
| [^28] | [Simultaneous Adversarial Attacks On Multiple Face Recognition System Components.](http://arxiv.org/abs/2304.05048) | 本文探究对抗样本对面部识别系统的多个组件的同时攻击，提出了三种攻击方法，并在实验中证明了其有效性。该研究为FRS的攻击向量提供了新的思路，同时提出了增强鲁棒性的方法。 |
| [^29] | [What Food Do We Tweet about on a Rainy Day?.](http://arxiv.org/abs/2304.05041) | 研究发现不同的天气条件下人们在推特上谈论的食物存在差异，研究这种现象可促进对食品消费者选择和看法的理解。 |
| [^30] | [Human-machine cooperation for semantic feature listing.](http://arxiv.org/abs/2304.05012) | 本文提出了一种人机合作的方法，将有限数据的人类词汇语义模型和大型语言模型结合起来，高效生成高质量的语义特征规范。 |
| [^31] | [Habits and goals in synergy: a variational Bayesian framework for behavior.](http://arxiv.org/abs/2304.05008) | 本文提出一种基于变分贝叶斯理论的行为模型，通过引入一种贝叶斯潜变量“意图”，将习惯性行为和目标导向行为联系起来，从而实现更灵活、高效的行为模拟。 |
| [^32] | [Relational Context Learning for Human-Object Interaction Detection.](http://arxiv.org/abs/2304.04997) | 本文提出了一种新的方法——多路复用关系网络（MUREN），通过丰富的上下文交换来学习全面的关系上下文，达到了HOI检测的最先进性能 |
| [^33] | [Efficient Feature Description for Small Body Relative Navigation using Binary Convolutional Neural Networks.](http://arxiv.org/abs/2304.04985) | 本文提出了一种基于二进制卷积神经网络层的局部特征描述结构，有效降低了小天体相对导航的计算和内存需求，同时提高了性能。 |
| [^34] | [Biological Factor Regulatory Neural Network.](http://arxiv.org/abs/2304.04982) | 本文提出了一种生物因子调控神经网络（BFReg-NN），它使用基因表达数据，并将大多数现有的生物学知识（如基因调节网络和GO）集成到模型中，以模拟生物系统中生物因子之间的关系，并在模拟数据和真实数据上取得了优越的性能和可解释性。 |
| [^35] | [GRIL: A $2$-parameter Persistence Based Vectorization for Machine Learning.](http://arxiv.org/abs/2304.04970) | 本文提出一种名为GRIL的方法，用于将拓扑特征表示散度到机器学习模型中，该方法可以稳定地用于不同的过滤函数。 |
| [^36] | [Reinforcement Learning Tutor Better Supported Lower Performers in a Math Task.](http://arxiv.org/abs/2304.04933) | 本文证明了深度强化学习可用于提供自适应的教育支持，尤其对于最初成绩较低的学生具有最大的益处。 |
| [^37] | [Explicit and Implicit Semantic Ranking Framework.](http://arxiv.org/abs/2304.04918) | 本文提出了一个名为sRank的通用语义学习排名框架，它使用transformer模型，能够在智能回复和环境临床智能等真实应用中，实现11.7%的离线准确度提升。 |
| [^38] | [AffectMachine-Classical: A novel system for generating affective classical music.](http://arxiv.org/abs/2304.04915) | AffectMachine-Classical是一种可以实时生成情感古典音乐的音乐生成系统，旨在将其嵌入到生物反馈系统中帮助用户自我调节情绪。 |
| [^39] | [Regulatory Markets: The Future of AI Governance.](http://arxiv.org/abs/2304.04914) | 提出一种监管市场的概念，即政府要求受监管对象从私人监管机构购买监管服务，以克服过度依赖行业自律和立法机构缺乏专业知识的局限性，从而逐步实现人工智能的恰当监管。 |
| [^40] | [Financial Time Series Forecasting using CNN and Transformer.](http://arxiv.org/abs/2304.04912) | 本文提出了结合使用CNN和Transformer来预测金融时间序列的涨跌幅。 |
| [^41] | [Improving Vision-and-Language Navigation by Generating Future-View Image Semantics.](http://arxiv.org/abs/2304.04907) | 本文提出了三个代理任务用于在代理的领域内预训练，以帮助模型生成未来视图图像语义，从而在视觉与语言导航任务中提高性能。 |
| [^42] | [EVKG: An Interlinked and Interoperable Electric Vehicle Knowledge Graph for Smart Transportation System.](http://arxiv.org/abs/2304.04893) | EVKG是一个综合、跨领域、可扩展、开放的地理空间知识管理系统，涵盖了与电动汽车相关的基本知识。集成其他知名知识图谱和本体模块，增强了EVKG在电动汽车决策中的价值。 |
| [^43] | [DISTO: Evaluating Textual Distractors for Multi-Choice Questions using Negative Sampling based Approach.](http://arxiv.org/abs/2304.04881) | DISTO提出了一种新的学习度量标准来评估多项选择题中生成的干扰选项。DISTO与人类对干扰选项的评分高度相关，且排名最先进的干扰选项生成模型的性能与基于机器翻译的度量标准非常不同，表明机器翻译度量标准不适用于干扰选项评估。 |
| [^44] | [ImageCaptioner$^2$: Image Captioner for Image Captioning Bias Amplification Assessment.](http://arxiv.org/abs/2304.04874) | 本文提出了一种新的图像字幕生成器 ImageCaptioner$^2$ ，用于针对图像字幕偏差放大进行评估。 |
| [^45] | [ShapeShift: Superquadric-based Object Pose Estimation for Robotic Grasping.](http://arxiv.org/abs/2304.04861) | ShapeShift是一个机器人抓取物体姿态估计的框架，它使用超椭球的参考形状预测物体的姿态，具有广泛的泛化能力和准确性。 |
| [^46] | [Ordinal Motifs in Lattices.](http://arxiv.org/abs/2304.04827) | 本研究提出了“序模式”作为分析意义的单位，并研究了在格上通过正式背景的全尺度测量来识别这些序子结构的方法，以实现从中等尺寸序数数据集中检索基本含义。 |
| [^47] | [Advances in Cybercrime Prediction: A Survey of Machine, Deep, Transfer, and Adaptive Learning Techniques.](http://arxiv.org/abs/2304.04819) | 本文从突破安全系统和窃取敏感数据的角度出发，介绍了机器学习、深度学习和迁移学习技术在预测和防止网络犯罪方面的最新进展，提到了最近研究中效果最好的递归和卷积神经网络。 |
| [^48] | [Scallop: A Language for Neurosymbolic Programming.](http://arxiv.org/abs/2304.04812) | Scallop是一种能够同时利用深度学习和逻辑推理优点的神经符号编程语言，它能够以数据和计算有效的方式训练神经符号应用程序，通过它可在AI任务中表达算法推理并融合逻辑领域知识，其解决方案可与最先进的模型相媲美或更高。 |
| [^49] | [Revisiting Test Time Adaptation under Online Evaluation.](http://arxiv.org/abs/2304.04795) | 本文提出了一种新颖的在线评估协议，该协议通过为较慢的方法提供更少的样本来惩罚它们，以更加现实的方式评估了测试时间适应（TTA）方法。广泛实验表明，当考虑推断速度时，简单快速的方法可以优于更复杂但较慢的方法。 |
| [^50] | [Reinforcement Learning from Passive Data via Latent Intentions.](http://arxiv.org/abs/2304.04782) | 本文提出了一种基于意图建模的强化学习方法，可以从被动数据中学习特征，并用于下游任务的价值预测。 |
| [^51] | [A Review on Explainable Artificial Intelligence for Healthcare: Why, How, and When?.](http://arxiv.org/abs/2304.04780) | 本篇综述系统分析了可解释人工智能（XAI）在医疗保健领域的应用及其流行趋势，阐述了XAI的使用原因、如何使用以及何时使用及其影响，并给出了如何获得可信赖的AI的方法，对研究人员、临床医生和政策制定者有重要的指导作用。 |
| [^52] | [DDRF: Denoising Diffusion Model for Remote Sensing Image Fusion.](http://arxiv.org/abs/2304.04774) | 本研究将去噪扩散模型引入远程感知图像融合，采用条件注入调制模块（风格转移调制和小波调制）将风格信息和频率信息注入扩散UNet，从而生成高质量融合图像。 |
| [^53] | [Connecting Fairness in Machine Learning with Public Health Equity.](http://arxiv.org/abs/2304.04761) | 这篇论文总结了机器学习公平方面的划时代文献，并提出了一个框架，用于识别和缓解数据和模型中的偏差，强调在公共卫生领域需要公平和公正的机器学习模型。 |
| [^54] | [A new perspective on building efficient and expressive 3D equivariant graph neural networks.](http://arxiv.org/abs/2304.04757) | 本文提出了一个本地等同性的三维等变图神经网络层次结构来评估等变GNN的表达能力，并提出局部亚结构编码（LSE）和帧转换编码（FTE）两个关键模块。LEFTNet有效地应用了这些模块并在分子属性预测任务中实现了最先进的性能。 |
| [^55] | [A Novel Two-level Causal Inference Framework for On-road Vehicle Quality Issues Diagnosis.](http://arxiv.org/abs/2304.04755) | 该论文提出了一种新的两级因果推断框架，利用因果机器学习可以加速汽车行业中处理车辆质量问题的全周期，从而更快地隔离根本原因、确定治疗措施并评估其有效性。 |
| [^56] | [$\textit{e-Uber}$: A Crowdsourcing Platform for Electric Vehicle-based Ride- and Energy-sharing.](http://arxiv.org/abs/2304.04753) | 本论文提出了一个名为e-Uber的众包平台，该平台利用电动汽车的日益普及，通过V2G和BST共同实现拼车和能量共享。平台利用强化学习，基于CMAB算法实现个性化任务推荐系统(CARS)，并利用反向拍卖机制选择最优出价。 |
| [^57] | [DeepHive: A multi-agent reinforcement learning approach for automated discovery of swarm-based optimization policies.](http://arxiv.org/abs/2304.04751) | 本文提出了一种多智能体强化学习方法，用于设计群体优化器以全局优化昂贵黑盒函数，并在各类基准优化函数上进行测试，结果表明具有优越的性能和所需的缩放。 |
| [^58] | [Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection.](http://arxiv.org/abs/2304.04688) | 提出了一种采用预训练的视觉-语言模型和交互模块进行交互感知提示的零样本时空动作检测方法，优化了视觉-语言特征的对齐，实现了更好的结果。 |
| [^59] | [Probably Approximately Correct Federated Learning.](http://arxiv.org/abs/2304.04641) | 本文提出了FedPAC框架，利用PAC学习理论推导出一个解析解，可以保证FL之间隐私、效用和效率的最佳权衡。 |
| [^60] | [Neural Diffeomorphic Non-uniform B-spline Flows.](http://arxiv.org/abs/2304.04555) | 提出了一种具有高效参数化和解析逆变换的至少二次连续可微、双Lipschitz连续的非均匀B样条流形，能够在密度估计和图像生成任务中表现出具有竞争力的结果。 |
| [^61] | [Toward Cohort Intelligence: A Universal Cohort Representation Learning Framework for Electronic Health Record Analysis.](http://arxiv.org/abs/2304.04468) | 提出了一种通用的COhort Representation lEarning（CORE）框架，用于增强EHR表示学习，支持针对不同队列的特征进行可解释性分析。 |
| [^62] | [H2RBox-v2: Boosting HBox-supervised Oriented Object Detection via Symmetric Learning.](http://arxiv.org/abs/2304.04403) | H2RBox-v2是第一个将对称学习应用于基于HBox监督的有向物体检测，其强化了水平注释和旋转注释之间的联系，在多个基准测试中实现了最先进的性能。 |
| [^63] | [NeRF applied to satellite imagery for surface reconstruction.](http://arxiv.org/abs/2304.04133) | 本文提出了Sat-NeRF模型，能够从少量的卫星图像集合中合成新的视角，并准确地估计场景表面的高程。 |
| [^64] | [TC-VAE: Uncovering Out-of-Distribution Data Generative Factors.](http://arxiv.org/abs/2304.04103) | 本文提出了一种基于总相关性的生成模型TC-VAE，可以揭示数据生成因素中的未知分布数据，在处理具有不平衡生成因素的数据集上表现优秀。 |
| [^65] | [Graph Collaborative Signals Denoising and Augmentation for Recommendation.](http://arxiv.org/abs/2304.03344) | 本文提出了一种新的图邻接矩阵，它包括了用户-用户和项目-项目的相关性，以及一个经过适当设计的用户-项目交互矩阵，并通过预训练和top-K采样增强了用户-项目交互矩阵，以更好地适应所有用户的需求。 |
| [^66] | [BotTriNet: A Unified and Efficient Embedding for Social Bots Detection via Metric Learning.](http://arxiv.org/abs/2304.03144) | BOTTRINET基于文本内容检测机器人，并设计了三元组网络以提高分类性能。在真实世界数据集CRESCI2017上，系统表现最好。 |
| [^67] | [Is it conceivable that neurogenesis, neural Darwinism, and species evolution could all serve as inspiration for the creation of evolutionary deep neural networks?.](http://arxiv.org/abs/2304.03122) | 本文探讨了神经发生、神经达尔文主义和物种进化如何启发演化深度神经网络的创作，并强调了在DNNs演化中dropout方法与神经发生的联系。 |
| [^68] | [Revisiting Dense Retrieval with Unanswerable Counterfactuals.](http://arxiv.org/abs/2304.03031) | 本文观察到基于DPR的最近的密集检索模型经常将无法回答的反事实情景排名高于可回答的原始情景，提出了一种新颖的用于段落检索的表示学习方法PiCL。 |
| [^69] | [AutoRL Hyperparameter Landscapes.](http://arxiv.org/abs/2304.02396) | 本文提出了一种方法，在训练期间多次建立和分析AutoRL超参数的景观，证明代表算法（DQN和SAC）在不同环境下的超参数景观会随时间而变化。 |
| [^70] | [Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT.](http://arxiv.org/abs/2304.02213) | 本文介绍了一个新的自然语言处理任务——结构化信息推理（SIS），利用GPT-3模型能够准确提取材料科学设备层面的信息，并通过实验预测PCE和反向预测参数，展示了大型语言模型在材料学中的巨大潜力。 |
| [^71] | [EGC: Image Generation and Classification via a Single Energy-Based Model.](http://arxiv.org/abs/2304.02012) | EGC是一种使用单个神经网络在图像分类和图像生成任务中实现卓越性能的方法，可以较好地生成出高质量图像，并在多项数据集上实现了领先的分类结果。 |
| [^72] | [A Survey of Large Language Models.](http://arxiv.org/abs/2303.18223) | 本文综述了大型语言模型的研究历程以及最近的预训练语言模型(PLMs)，并强调模型扩展将带来性能改进和特殊能力的发掘。 |
| [^73] | [Queer In AI: A Case Study in Community-Led Participatory AI.](http://arxiv.org/abs/2303.16972) | Queer in AI是一个基于社区参与的AI设计案例研究，通过拒绝等级制度而选择去中心化，在酷儿社群内部建立了援助和项目，同时努力改变酷儿社群外的参与者和机构。通过培育AI参与文化，欢迎和赋权边缘化参与者，为AI的参与式设计做出了更广泛的贡献。 |
| [^74] | [Questions of science: chatting with ChatGPT about complex systems.](http://arxiv.org/abs/2303.16870) | 该论文介绍了使用ChatGPT代表社区对复杂系统领域的理解进行概述，ChatGPT学习了大量互联网文本数据并能够提供反映社区普遍意见、观点和语言模式的答案，探讨了复杂系统领域的教学、学习和研究主题，认为ChatGPT是社区思想的一种来源的价值。 |
| [^75] | [Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods.](http://arxiv.org/abs/2303.13988) | 本文提出了一种新领域——机器心理学，利用心理学的方法考察大型语言模型的能力。该文规范了机器心理学研究的方法论标准，并对心理实验中提示设计政策进行了探讨和制定。 |
| [^76] | [ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes & Progressions.](http://arxiv.org/abs/2303.12364) | ExBEHRT是一种扩展Transformer模型，应用于电子病历数据，将多种类型的记录包括在特征空间中，可以预测不同疾病下游任务的性能更好，并使用预期梯度对结果进行更细粒度的解释。 |
| [^77] | [Does Synthetic Data Generation of LLMs Help Clinical Text Mining?.](http://arxiv.org/abs/2303.04360) | 研究探讨了利用 ChatGPT 进行临床文本挖掘的潜力，但初步结果表明效果欠佳，同时上传患者信息也引发隐私问题。因此，提出了一种新的训练方法，即使用 ChatGPT 生成带标签的大量高质量合成数据进行训练。 |
| [^78] | [Neural Laplace Control for Continuous-time Delayed Systems.](http://arxiv.org/abs/2302.12604) | 本文介绍了一种神经拉普拉斯控制方法，用于解决具有不规则状态观测和未知延迟的连续时间环境下的离线强化学习问题。 |
| [^79] | [Fairguard: Harness Logic-based Fairness Rules in Smart Cities.](http://arxiv.org/abs/2302.11137) | 本文提出了一种基于时间逻辑的公正智慧城市政策调整和生成方法Fairguard，通过两个阶段的静态生成和动态调节，缓解由多种数据和算法偏见导致的不公正预测结果。 |
| [^80] | [Regulating ChatGPT and other Large Generative AI Models.](http://arxiv.org/abs/2302.02337) | 本文将讨论大型生成AI模型的可信AI监管，包括直接监管、数据保护、内容监管和政策建议，并建议使用新术语来区分参与者。本文的目的是确保LGAIMs的可信度并使其为受益所用。 |
| [^81] | [Language-Driven Anchors for Zero-Shot Adversarial Robustness.](http://arxiv.org/abs/2301.13096) | 本文提出了一种基于语言驱动、基于锚点的对抗训练策略LAAT，通过利用文本编码器的语义一致性，在零样本图像分类场景下增强图像模型的对抗鲁棒性。实验结果表明，该方法在零样本对抗性能上优于先前的最佳状态对抗性一次性方法，同时能为流行的图像分类模型带来实质性的零样本对抗性能提升。 |
| [^82] | [Data-driven intelligent computational design for products: Method, techniques, and applications.](http://arxiv.org/abs/2301.12382) | 数据驱动的智能计算设计(DICD)利用深度学习算法提取和表示历史或制造过程数据中的设计特征，支持设计方案检索、生成、优化、评估等。作为一种新兴的研究课题，DICD仍存在多个未开发问题。 |
| [^83] | [EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records.](http://arxiv.org/abs/2301.07695) | 该论文提出了一个面向电子病历数据的文本转SQL数据集，该数据集具有一系列独特挑战，包括生成SQL查询、理解时间表达式以及区分有无答案的问题。 |
| [^84] | [Impossibility Theorems for Feature Attribution.](http://arxiv.org/abs/2212.11870) | 本文阐述了对于中等规模的模型类，任何完整的线性特征归因方法都无法胜任推断模型行为的改进，启示我们在具体定义最终任务的重要性方面要汲取经验，采用重复模型评估的简单直接方法可以胜过许多其他复杂的特征归因方法。 |
| [^85] | [Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks.](http://arxiv.org/abs/2210.15629) | 本文提出一种利用语言控制扩散模型的分层规划器，有效而高效地扩展扩散模型，解决长时间跨度自然语言指令下的控制问题，实现了较高的单任务和多任务成功率，并极大地提高计算效率。 |
| [^86] | [Vitruvio: 3D Building Meshes via Single Perspective Sketches.](http://arxiv.org/abs/2210.13634) | 本文介绍了一种名为Vitruvio的深度学习方法，旨在将单点透视草图转换为三维建筑网格，针对AEC领域提出了该方法，并在建筑数据集上进行了验证实验，显示出比现有方法更好的重建质量。 |
| [^87] | [SCALE: Online Self-Supervised Lifelong Learning without Prior Knowledge.](http://arxiv.org/abs/2208.11266) | 本文提出了一个更加实用的在线自监督终身学习的问题设置，该设置具有挑战性，因为它涉及到数据的非独立同分布和单次遍历、缺乏外部监督和先验知识等。为了解决这些问题，我们提出了SCALE方法，它可以纯粹地从数据连续体中提取和记忆表示。 |
| [^88] | [LogLG: Weakly Supervised Log Anomaly Detection via Log-Event Graph Construction.](http://arxiv.org/abs/2208.10833) | 本文提出了一种LogLG弱监督日志异常检测框架，采用事件图构建和伪标签生成，旨在探索序列中关键字之间的语义联系，有效识别应用程序日志的异常行为。 |
| [^89] | [A Perceptually Optimized and Self-Calibrated Tone Mapping Operator.](http://arxiv.org/abs/2206.09146) | 本文开发了一种两步神经网络TMO，具有自校准和感知优化功能，可以将HDR图像压缩到LDR图像，同时通过感知度量实现了灵敏的质量优化。 |
| [^90] | [A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games.](http://arxiv.org/abs/2206.05825) | 本文展示了磁镜面下降算法作为均衡求解器和强化学习方法的优点，包括实现了线性收敛的相应均衡求解器和在表格式设置中实现了与CFR相竞争的标准强化学习算法，以及在“黑暗六角”和“幻象井字”中的自我玩耍深度强化学习算法的良好表现。 |
| [^91] | [DPSNN: A Differentially Private Spiking Neural Network with Temporal Enhanced Pooling.](http://arxiv.org/abs/2205.12718) | 本文提出了一种差分隐私脉冲神经网络（DPSNN），将差分隐私算法与脉冲神经网络相结合，保持了SNN的强隐私保护；同时提出了时序增强池化（TEP）方法，使SNN能够获得更好的信息传输。在MNIST和CIFAR-10数据集上进行实验验证了该方法的有效性。 |
| [^92] | [Memory-efficient Reinforcement Learning with Value-based Knowledge Consolidation.](http://arxiv.org/abs/2205.10868) | 本论文提出了一种基于价值知识整合的高效内存强化学习算法，通过从目标 Q 网络向当前 Q 网络整合知识，减少遗忘并保持高样本效率。相较于传统方法，本算法在特征和图像任务中表现相当或更好，同时减轻了大经验重放缓存器带来的内存负担。 |
| [^93] | [Survey of Aspect-based Sentiment Analysis Datasets.](http://arxiv.org/abs/2204.05232) | 本研究汇总了65个公开可用的ABSA数据集，包括45个英文数据集和20个其他语言数据集，提供了一个可以用于训练和评估自主ABSA系统的数据库。 |
| [^94] | [Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures.](http://arxiv.org/abs/2110.13179) | 该论文提出了一种新的深度泊松混合网络方法（DPMN），能够在存在可靠的层级信息时进行准确和一致的概率时间序列预测。该模型可以保证层级一致性，并在大规模的层级预测问题中获得了最先进的结果。 |
| [^95] | [Entropy Regularized Reinforcement Learning Using Large Deviation Theory.](http://arxiv.org/abs/2106.03931) | 本文利用大偏差理论建立了熵正则化强化学习与非平衡统计力学的联系，在长时间极限下推导出了马尔可夫决策过程模型中最优策略和最优动态的精确解析结果，并提出了新的分析和计算框架。 |
| [^96] | [An Offline Risk-aware Policy Selection Method for Bayesian Markov Decision Processes.](http://arxiv.org/abs/2105.13431) | 本文提出了一种基于风险意识的离线策略选择方法，该方法采用“利用”与“谨慎”(EvC)的范式，选取权重值最大化且同时考虑长期奖励和风险的策略。 |

# 详细

[^1]: 人工智能与人类共创：寻找“永久化学品”替代品的方法

    Human-AI Co-Creation Approach to Find Forever Chemicals Replacements. (arXiv:2304.05389v1 [cs.AI])

    [http://arxiv.org/abs/2304.05389](http://arxiv.org/abs/2304.05389)

    这篇论文介绍了一种人工智能与人类共创的方法，利用生成模型加速寻找替代“永久化学品”的过程，以减少对环境和人体健康造成的伤害。

    

    在人工智能材料发现领域，生成模型是非常强大的工具。我们正在设计一个软件框架，支持人工智能和人类共同创造过程，以加快寻找替代“永久化学品”的进程，从而减少它们对环境和人体健康造成的伤害。我们的方法将人工智能能力与专业领域的内隐知识相结合，加快材料发现速度。我们的共同创造过程以专业领域专家和能生成新分子设计的生成模型之间的交互开始。在这篇论文中，我们讨论了我们的假设：这些专业领域的专家可以通过更迭代的生成模型交互，利用他们的知识指引对探索空间的更精确的探索，从而受益于这种方法。

    Generative models are a powerful tool in AI for material discovery. We are designing a software framework that supports a human-AI co-creation process to accelerate finding replacements for the ``forever chemicals''-- chemicals that enable our modern lives, but are harmful to the environment and the human health. Our approach combines AI capabilities with the domain-specific tacit knowledge of subject matter experts to accelerate the material discovery. Our co-creation process starts with the interaction between the subject matter experts and a generative model that can generate new molecule designs. In this position paper, we discuss our hypothesis that these subject matter experts can benefit from a more iterative interaction with the generative model, asking for smaller samples and ``guiding'' the exploration of the discovery space with their knowledge.
    
[^2]: ChatGPT和Bard能够生成一致的评估项目吗？针对人类表现的可靠性分析。

    Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance. (arXiv:2304.05372v1 [cs.CL])

    [http://arxiv.org/abs/2304.05372](http://arxiv.org/abs/2304.05372)

    本文通过测量OpenAI ChatGPT和Google Bard等AI聊天机器人的可靠性发现，其在感知和评估写作提示的复杂性方面与经验丰富的人类评分者的一致性较低。

    

    ChatGPT和Bard是基于大语言模型的AI聊天机器人，被认为能够在各种领域中应用。在教育领域，这些AI技术已被用于评估和教学。在评估中，AI长期以来一直用于自动化的论文评分和自动化的项目生成。这些工具必须具备的一项心理测量属性是可靠性高，即AI分数与人类评分者意见一致。本文测量了OpenAI ChatGP和Google Bard的可靠性，以评估这些工具在感知和评估写作提示的复杂性方面与经验丰富的人类评分者的一致性。作为绩效指标的内部相关系数（ICC）显示，OpenAI ChatGPT和Google Bard的互可靠性低于人类评分的金标准。

    ChatGPT and Bard are AI chatbots based on Large Language Models (LLM) that are slated to promise different applications in diverse areas. In education, these AI technologies have been tested for applications in assessment and teaching. In assessment, AI has long been used in automated essay scoring and automated item generation. One psychometric property that these tools must have to assist or replace humans in assessment is high reliability in terms of agreement between AI scores and human raters. In this paper, we measure the reliability of OpenAI ChatGP and Google Bard LLMs tools against experienced and trained humans in perceiving and rating the complexity of writing prompts. Intraclass correlation (ICC) as a performance metric showed that the inter-reliability of both the OpenAI ChatGPT and the Google Bard were low against the gold standard of human ratings.
    
[^3]: 那不是你的记忆，它是别人的：在聊天机器人记忆中播撒错误信息

    Those Aren't Your Memories, They're Somebody Else's: Seeding Misinformation in Chat Bot Memories. (arXiv:2304.05371v1 [cs.CL])

    [http://arxiv.org/abs/2304.05371](http://arxiv.org/abs/2304.05371)

    本文研究了聊天机器人的一个新发展：长期记忆机制。然而，我们发现这种机制可能会导致机器人记住了错误和虚假的信息，并将其作为事实陈述重复。

    

    聊天机器人的一个新发展是长期记忆机制，可以记住过去对话中的信息，以增加响应的连贯性和一致性。机器人被设计为从其对话伙伴中提取个人性质的知识，例如表明对特定颜色的偏好。在本文中，我们展示了这种记忆机制可能会导致意外行为。具体而言，我们发现一个人可以将个人陈述与信息陈述结合起来，导致机器人将信息陈述与个人知识一起记录在其长期记忆中。这意味着机器人可能被欺骗记住错误信息，并在回忆与对话主题相关的信息时将其作为事实陈述重复。我们在基于ParlAI平台实现的BlenderBot 2框架上展示了这种漏洞，并在更近期、规模更大的BlenderBot 3模型上提供了例子。

    One of the new developments in chit-chat bots is a long-term memory mechanism that remembers information from past conversations for increasing engagement and consistency of responses. The bot is designed to extract knowledge of personal nature from their conversation partner, e.g., stating preference for a particular color. In this paper, we show that this memory mechanism can result in unintended behavior. In particular, we found that one can combine a personal statement with an informative statement that would lead the bot to remember the informative statement alongside personal knowledge in its long term memory. This means that the bot can be tricked into remembering misinformation which it would regurgitate as statements of fact when recalling information relevant to the topic of conversation. We demonstrate this vulnerability on the BlenderBot 2 framework implemented on the ParlAI platform and provide examples on the more recent and significantly larger BlenderBot 3 model. We gen
    
[^4]: 大型语言模型在医疗保健领域中准备就绪了吗？临床语言理解的比较研究。

    Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding. (arXiv:2304.05368v1 [cs.CL])

    [http://arxiv.org/abs/2304.05368](http://arxiv.org/abs/2304.05368)

    本研究全面评估了大型语言模型在临床语言理解任务上的表现，并引入自问自答提示策略来提高LLMs在医疗保健相关任务中的效果。

    

    大型语言模型（LLMs）在各个领域取得了显著的进展，包括医疗保健领域。然而，临床语言理解任务的专业性质带来了独特的挑战和限制，需要进一步研究。在本研究中，我们对最先进的LLMs——GPT-3.5、GPT-4和Bard进行了全面评估，该评估范围涵盖了各种任务，包括命名实体识别、关系提取、自然语言推理、语义文本相似性、文档分类和问答。我们还引入了一种新的提示策略——自问自答提示（SQP），旨在通过引发与相关临床场景相关的信息性问题和答案，定制化提高LLMs的性能。我们的评估强调了任务特定的学习策略和提示技术对于提高LLMs在医疗保健相关任务中的有效性的重要性。

    Large language models (LLMs) have made significant progress in various domains, including healthcare. However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation. In this study, we conduct a comprehensive evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within the realm of clinical language understanding tasks. These tasks span a diverse range, including named entity recognition, relation extraction, natural language inference, semantic textual similarity, document classification, and question-answering. We also introduce a novel prompting strategy, self-questioning prompting (SQP), tailored to enhance LLMs' performance by eliciting informative questions and answers pertinent to the clinical scenarios at hand. Our evaluation underscores the significance of task-specific learning strategies and prompting techniques for improving LLMs' effectiveness in healthcare-related tasks.
    
[^5]: 针对多标签分类的非对称多项式损失

    Asymmetric Polynomial Loss For Multi-Label Classification. (arXiv:2304.05361v1 [cs.LG])

    [http://arxiv.org/abs/2304.05361](http://arxiv.org/abs/2304.05361)

    本文提出了一种有效的非对称多项式损失（APL），可以根据不同任务优化模型，并在关系提取、文本分类和图像分类上表现良好。

    

    多种任务被重新构造为多标签分类问题，其中二元交叉熵（BCE）损失经常用于优化设计良好的模型。然而，纯BCE损失无法根据不同任务进行优化，从而导致不同模型的表现亚优。此外，冗余负样品和罕见正样品之间的不平衡可能会降低模型性能。在本文中，我们提出了一种有效的非对称多项式损失（APL）来缓解上述问题。具体而言，我们首先对BCE损失进行泰勒展开，然后改善多项式函数的系数。我们进一步采用非对称聚焦机制来解耦来自负样本和正样本的梯度贡献。此外，我们验证，多项式系数可以重新校准非对称聚焦超参数。在关系提取、文本分类和图像分类上的实验表明，我们的APL损失可以提供更好的性能。

    Various tasks are reformulated as multi-label classification problems, in which the binary cross-entropy (BCE) loss is frequently utilized for optimizing well-designed models. However, the vanilla BCE loss cannot be tailored for diverse tasks, resulting in a suboptimal performance for different models. Besides, the imbalance between redundant negative samples and rare positive samples could degrade the model performance. In this paper, we propose an effective Asymmetric Polynomial Loss (APL) to mitigate the above issues. Specifically, we first perform Taylor expansion on BCE loss. Then we ameliorate the coefficients of polynomial functions. We further employ the asymmetric focusing mechanism to decouple the gradient contribution from the negative and positive samples. Moreover, we validate that the polynomial coefficients can recalibrate the asymmetric focusing hyperparameters. Experiments on relation extraction, text classification, and image classification show that our APL loss can 
    
[^6]: iDML: 激励去中心化机器学习

    iDML: Incentivized Decentralized Machine Learning. (arXiv:2304.05354v1 [cs.LG])

    [http://arxiv.org/abs/2304.05354](http://arxiv.org/abs/2304.05354)

    通过区块链的激励机制，让用户资源贡献变得可行，实现去中心化的机器学习。

    

    随着去中心化和机会主义的机器学习方法的兴起，终端设备越来越需要使用其自己收集的众包数据训练深度学习模型。这些方法从资源消耗和隐私保护的角度都是可取的。当设备直接从训练模型中受益时，资源贡献是被激励的，因为协作会产生更高准确度的模型。然而，当要求终端用户设备为仅为他人受益的任务（例如为一个无足轻重的邻居训练模型）贡献自己的资源（例如计算、通信和数据）时，需要提供明确的激励机制。在这个项目中，我们提出了一种新的基于区块链的激励机制，用于完全去中心化和鼓励用户资源贡献的机器学习。

    With the rising emergence of decentralized and opportunistic approaches to machine learning, end devices are increasingly tasked with training deep learning models on-devices using crowd-sourced data that they collect themselves. These approaches are desirable from a resource consumption perspective and also from a privacy preservation perspective. When the devices benefit directly from the trained models, the incentives are implicit contributing devices' resources are incentivized by the availability of the higher-accuracy model that results from collaboration. However, explicit incentive mechanisms must be provided when end-user devices are asked to contribute their resources (e.g., computation, communication, and data) to a task performed primarily for the benefit of others, e.g., training a model for a task that a neighbor device needs but the device owner is uninterested in. In this project, we propose a novel blockchain-based incentive mechanism for completely decentralized and
    
[^7]: 聊天GPT中的毒性：分析个性化语言模型

    Toxicity in ChatGPT: Analyzing Persona-assigned Language Models. (arXiv:2304.05335v1 [cs.CL])

    [http://arxiv.org/abs/2304.05335](http://arxiv.org/abs/2304.05335)

    论文分析了基于对话的大型语言模型ChatGPT中的毒性，通过指定人物角色，输出会涉及刻板印象、有害对话和伤人的言论。因此，我们需要充分理解LLMs的能力和局限性，以确保这些系统的安全性。

    

    大型语言模型（LLMs）展现了令人难以置信的能力，超越了自然语言处理（NLP）社区，并被广泛应用于医疗保健、治疗、教育和客户服务等多种服务中。由于用户包括有重要信息需求的人，如与聊天机器人交互的学生或患者，因此这些系统的安全性至关重要。因此，必须明确了解LLMs的能力和局限性。为此，我们系统地评估了ChatGPT中的毒性，这是一种流行的基于对话的LLM，超过半百万次Generation被测试。我们发现，通过为ChatGPT指定一个人物角色，比如拳击手穆罕默德·阿里，可以显著增加产生的毒性。根据指定给ChatGPT的角色，其毒性可能会增加到6倍，其输出会涉及不正确的刻板印象、有害的对话和伤人的言论。这可能会潜在地损害人物角色的名誉。

    Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and ha
    
[^8]: MC-ViViT: 多分支分类器-ViViT用于使用面部视频检测老年人轻度认知障碍

    MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos. (arXiv:2304.05292v1 [cs.CV])

    [http://arxiv.org/abs/2304.05292](http://arxiv.org/abs/2304.05292)

    本文提出了一种新的深度机器学习模型MC-ViViT，用于通过分析面部特征检测老年人轻度认知障碍。通过MC模块和结合损失函数来解决数据集样本不平衡问题，提高了算法的性能。

    

    深度机器学习模型包括卷积神经网络(CNN)已成功地应用于使用医学图像、问卷和视频检测轻度认知障碍(MCI)。本文提出了一种新的多分支分类器-视频视觉变换器(MC-ViViT)模型，通过分析面部特征区分MCI和正常认知。数据来自I-CONECT，一个旨在通过提供频繁视频聊天来改善认知功能的行为干预试验。MC-ViViT在一个分支中提取视频的时空特征，并通过MC模块增强表示。由于I-CONECT数据集中的样本不平衡问题（包含难易和正负样本），这使MC-ViViT的性能受到影响。我们提出了一种Hard-Easy和Positive-Negative样本的损失函数（HP Loss）来结合对比度调节损失Focal loss和AD-CORRE loss来解决不平衡问题。我们在I-CONECT数据集上的实验结果显示出该算法的有效性。

    Deep machine learning models including Convolutional Neural Networks (CNN) have been successful in the detection of Mild Cognitive Impairment (MCI) using medical images, questionnaires, and videos. This paper proposes a novel Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to distinguish MCI from those with normal cognition by analyzing facial features. The data comes from the I-CONECT, a behavioral intervention trial aimed at improving cognitive function by providing frequent video chats. MC-ViViT extracts spatiotemporal features of videos in one branch and augments representations by the MC module. The I-CONECT dataset is challenging as the dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE loss to address the imbalanced problem. Our experimental results on the I-CONECT dataset show th
    
[^9]: 任务难度感知的增量学习参数分配与正则化

    Task Difficulty Aware Parameter Allocation & Regularization for Lifelong Learning. (arXiv:2304.05288v1 [cs.LG])

    [http://arxiv.org/abs/2304.05288](http://arxiv.org/abs/2304.05288)

    本文提出了一种任务难度感知的增量学习参数分配与正则化方法，该方法可以根据任务的学习难度自适应地选择适当的分配或正则化策略，以克服在学习不同任务时的挑战。

    

    参数正则化或分配方法对于克服增量学习中的灾难性遗忘非常有效。然而，它们在序列中均匀解决所有任务，忽略了不同任务的学习难度差异。因此，当学习与已学任务非常不同的新任务时，参数正则化方法会面临显着的遗忘，而参数分配方法在学习简单任务时则面临不必要的参数开销。本文提出了参数分配和正则化策略（PAR），其可以根据学习难度从参数分配和正则化中自适应地选择适当的策略。对于一个已经学习相关任务的模型来说，一个任务会变得容易，反之亦然。本文提出了基于最近原型距离的离散度估计方法，仅使用新任务的特征来度量任务相关性。此外，我们提出了一种时效性相关性感知采样的架构。

    Parameter regularization or allocation methods are effective in overcoming catastrophic forgetting in lifelong learning. However, they solve all tasks in a sequence uniformly and ignore the differences in the learning difficulty of different tasks. So parameter regularization methods face significant forgetting when learning a new task very different from learned tasks, and parameter allocation methods face unnecessary parameter overhead when learning simple tasks. In this paper, we propose the Parameter Allocation & Regularization (PAR), which adaptively select an appropriate strategy for each task from parameter allocation and regularization based on its learning difficulty. A task is easy for a model that has learned tasks related to it and vice versa. We propose a divergence estimation method based on the Nearest-Prototype distance to measure the task relatedness using only features of the new task. Moreover, we propose a time-efficient relatedness-aware sampling-based architecture
    
[^10]: 自动机引导下的强化学习代理课程生成

    Automaton-Guided Curriculum Generation for Reinforcement Learning Agents. (arXiv:2304.05271v1 [cs.AI])

    [http://arxiv.org/abs/2304.05271](http://arxiv.org/abs/2304.05271)

    提出了自动机引导下的强化学习代理课程生成方法，能够自动生成逐渐递增的适合代理学习的中间任务序列，并能在离散和连续控制任务上比现有方法提供高达5倍的快速学习。

    

    尽管强化学习取得了进展，但许多顺序决策任务仍然难以学习且成本高昂。最近，提出了从逻辑任务规范自动生成奖励函数的方法来缓解这个问题；然而，它们在长时间跨度任务（即代理人需要执行一系列正确的操作以达到目标状态，同时在选择动作时考虑未来的过渡）上的扩展性仍然很差。利用课程（一系列逐渐复杂的任务）进一步提高代理人的学习速度，通过对适合代理人学习能力的中间任务进行序列化。然而，从逻辑规范生成课程仍然是一个尚未解决的问题。为此，我们提出了AGCL，即自动导向课程学习，一种自动以DAG形式生成目标任务课程的新方法。AGCL将规范编码为确定性自动机，探索其中的重复子结构，并通过将这些结构分解为DAG来生成课程。我们在离散和连续控制任务上评估了我们的方法，并将其与现有方法进行了比较。我们的结果表明，AGCL始终优于所有基线方法，并且能够与没有进行课程的方法相比，提供高达5倍的快速学习。

    Despite advances in Reinforcement Learning, many sequential decision making tasks remain prohibitively expensive and impractical to learn. Recently, approaches that automatically generate reward functions from logical task specifications have been proposed to mitigate this issue; however, they scale poorly on long-horizon tasks (i.e., tasks where the agent needs to perform a series of correct actions to reach the goal state, considering future transitions while choosing an action). Employing a curriculum (a sequence of increasingly complex tasks) further improves the learning speed of the agent by sequencing intermediate tasks suited to the learning capacity of the agent. However, generating curricula from the logical specification still remains an unsolved problem. To this end, we propose AGCL, Automaton-guided Curriculum Learning, a novel method for automatically generating curricula for the target task in the form of Directed Acyclic Graphs (DAGs). AGCL encodes the specification in 
    
[^11]: 个性化文本到图像生成的可控文本反转

    Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v1 [cs.CV])

    [http://arxiv.org/abs/2304.05265](http://arxiv.org/abs/2304.05265)

    本文提出了一种名为COTI的技术，通过引入理论指导的损失目标和全面的加权评分机制，并结合主动学习范式来解决文本反转时的困难，提供了一个强大，数据效率高，易于使用的框架。

    

    最近，大规模生成模型在以文本为引导的高保真图像的生成方面取得了前所未有的性能。当引导信息包含用户定义的、未见过的或长尾概念标记时，文本反转成为一种有效的个性化生成技术。尽管如此，我们发现并展示了文本反转的部署仍充满了“黑魔法”，例如额外数据集的严苛要求，在循环中需要艰苦的人力成本和缺乏鲁棒性等。在这项工作中，我们提出了一种名为可控文本反转的大大增强版反转，解决了所有上述问题，并反过来提供了一个强大，数据效率高，易于使用的框架。COTI的核心是基于理论的损失目标，具有全面和新颖的加权评分机制，并由主动学习范式所提取。广泛的结果表明，COTI的性能比之前技术有了显著的提升，尤其是在数据少的情况下。

    The recent large-scale generative modeling has attained unprecedented performance especially in producing high-fidelity images driven by text prompts. Text inversion (TI), alongside the text-to-image model backbones, is proposed as an effective technique in personalizing the generation when the prompts contain user-defined, unseen or long-tail concept tokens. Despite that, we find and show that the deployment of TI remains full of "dark-magics" -- to name a few, the harsh requirement of additional datasets, arduous human efforts in the loop and lack of robustness. In this work, we propose a much-enhanced version of TI, dubbed Controllable Textual Inversion (COTI), in resolving all the aforementioned problems and in turn delivering a robust, data-efficient and easy-to-use framework. The core to COTI is a theoretically-guided loss objective instantiated with a comprehensive and novel weighted scoring mechanism, encapsulated by an active-learning paradigm. The extensive results show that 
    
[^12]: 新闻推荐中的提示学习

    Prompt Learning for News Recommendation. (arXiv:2304.05263v1 [cs.IR])

    [http://arxiv.org/abs/2304.05263](http://arxiv.org/abs/2304.05263)

    本文介绍了在新闻推荐领域首次采用预训练，提示学习和预测范例来开发Prompt4NR框架的实验。该框架将预测点击候选新闻的任务转化为cloze-style填空式掩码预测任务，从而更好地利用预训练过程中的丰富语义信息和语言知识。

    

    最近的一些新闻推荐（NR）方法通过引入预训练语言模型（PLM）来编码新闻表示，采用精心设计的推荐特定神经网络和目标函数来遵循香草预训练和微调范例。由于任务目标与PLM不一致，我们认为他们的建模范式未能充分利用预训练过程中嵌入的丰富语义信息和语言知识。最近，预训练，提示和预测范例在自然语言处理领域取得了许多成功。在本文中，我们第一次尝试使用这种新范例来开发一个新闻推荐中的Prompt Learning (Prompt4NR) 框架，将预测用户是否会点击候选新闻的任务转化为填空式掩码预测任务。具体来说，我们设计了一系列prompt模板，包括离散、连续...

    Some recent \textit{news recommendation} (NR) methods introduce a Pre-trained Language Model (PLM) to encode news representation by following the vanilla pre-train and fine-tune paradigm with carefully-designed recommendation-specific neural networks and objective functions. Due to the inconsistent task objective with that of PLM, we argue that their modeling paradigm has not well exploited the abundant semantic information and linguistic knowledge embedded in the pre-training process. Recently, the pre-train, prompt, and predict paradigm, called \textit{prompt learning}, has achieved many successes in natural language processing domain. In this paper, we make the first trial of this new paradigm to develop a \textit{Prompt Learning for News Recommendation} (Prompt4NR) framework, which transforms the task of predicting whether a user would click a candidate news as a cloze-style mask-prediction task. Specifically, we design a series of prompt templates, including discrete, continuous, 
    
[^13]: 控制联邦学习中遗忘问题的重新加权Softmax交叉熵方法

    Re-Weighted Softmax Cross-Entropy to Control Forgetting in Federated Learning. (arXiv:2304.05260v1 [cs.LG])

    [http://arxiv.org/abs/2304.05260](http://arxiv.org/abs/2304.05260)

    本文提出一种重新加权softmax的交叉熵方法来解决联邦学习中客户端的灾难性遗忘问题，并证明这种方法可以缓解客户端遗忘并对标准联邦学习算法提供一致的改进。

    

    在联邦学习中，通过聚合在一组独立客户节点计算的模型更新来学习全局模型，在聚合之前在每个节点上执行多个梯度步骤，以减少通信成本。在这种情况下，数据异构性会导致不同的客户端具有不同的本地目标，这可能会导致客户端过度减少其自己的本地目标，使其与全局解分歧。本文提出了一种有效的方法，对每个客户端的交叉熵目标进行修改，通过重新加权softmax的logits以计算损失，从而解决了每个客户端模型对来自其他客户端的数据的灾难性遗忘问题。这种方法可以保护不在客户端标签集中的类别免受突然的表示变化，并且通过实验证明，可以缓解客户端遗忘并对标准联邦学习算法提供一致的改进。 我们的方法特别有益。

    In Federated Learning, a global model is learned by aggregating model updates computed at a set of independent client nodes, to reduce communication costs multiple gradient steps are performed at each node prior to aggregation. A key challenge in this setting is data heterogeneity across clients resulting in differing local objectives which can lead clients to overly minimize their own local objective, diverging from the global solution. We demonstrate that individual client models experience a catastrophic forgetting with respect to data from other clients and propose an efficient approach that modifies the cross-entropy objective on a per-client basis by re-weighting the softmax logits prior to computing the loss. This approach shields classes outside a client's label set from abrupt representation change and we empirically demonstrate it can alleviate client forgetting and provide consistent improvements to standard federated learning algorithms. Our method is particularly beneficia
    
[^14]: 多粒度时间变换器用于知识追踪

    Multi-granulariy Time-based Transformer for Knowledge Tracing. (arXiv:2304.05257v1 [cs.LG])

    [http://arxiv.org/abs/2304.05257](http://arxiv.org/abs/2304.05257)

    本文提出了一种基于Transformer的架构用于准确地预测学生在标准化测试中的表现。该模型考虑了学生的历史数据，包括他们以往的考试成绩、学习习惯和其他相关信息，并在解码器输入中使用了多个时间特征粒度以显著提高模型性能。与LightGBM相比，该方法更加准确，为教育领域的AI发展提供了一个可伸缩和准确的预测学生成果的工具。

    

    本文提出了一种基于Transformer的架构，用于预测标准化测试中学生的表现。具体来说，我们利用学生的历史数据，包括他们以往的考试成绩、学习习惯和其他相关信息，为每个学生创建一个个性化的模型。然后，我们使用这些模型来预测学生在给定测试中的未来表现。将该模型应用于RIIID数据集，我们证明使用多个时间特征粒度作为解码器输入可以显着提高模型性能。我们的结果还表明了我们方法的有效性，相对于LightGBM方法有很大的改进。我们的工作为教育领域的AI发展做出了贡献，提供了一个可伸缩和准确的预测学生成果的工具。

    In this paper, we present a transformer architecture for predicting student performance on standardized tests. Specifically, we leverage students historical data, including their past test scores, study habits, and other relevant information, to create a personalized model for each student. We then use these models to predict their future performance on a given test. Applying this model to the RIIID dataset, we demonstrate that using multiple granularities for temporal features as the decoder input significantly improve model performance. Our results also show the effectiveness of our approach, with substantial improvements over the LightGBM method. Our work contributes to the growing field of AI in education, providing a scalable and accurate tool for predicting student outcomes.
    
[^15]: OpenAL: 主动学习策略的评估与解释

    OpenAL: Evaluation and Interpretation of Active Learning Strategies. (arXiv:2304.05246v1 [cs.LG])

    [http://arxiv.org/abs/2304.05246](http://arxiv.org/abs/2304.05246)

    OpenAL是一个灵活且开源的框架，可以在一系列现实任务上轻松运行和比较采样AL策略，具有可解释性指标和统计分析方法，针对主动学习的一次性特性，从业人员也可以轻松扩展基准测试。

    

    尽管在主动学习（AL）方面已经有大量的文献，但是还没有一种全面且开放的基准可以有效地比较提出的采样器。此外，文献中实验设置的变异性使得选择采样策略变得困难，这是由于主动学习实验的一次性特性非常重要。为了解决这些限制，我们引入了OpenAL，这是一个灵活且开源的框架，可以在一系列现实任务上轻松运行和比较采样AL策略。该基准测试还加入了可解释性指标和统计分析方法，以了解何时以及为什么一些采样器优于其他采样器。最后，从业人员可以通过提交自己的AL采样器轻松扩展基准测试。

    Despite the vast body of literature on Active Learning (AL), there is no comprehensive and open benchmark allowing for efficient and simple comparison of proposed samplers. Additionally, the variability in experimental settings across the literature makes it difficult to choose a sampling strategy, which is critical due to the one-off nature of AL experiments. To address those limitations, we introduce OpenAL, a flexible and open-source framework to easily run and compare sampling AL strategies on a collection of realistic tasks. The proposed benchmark is augmented with interpretability metrics and statistical analysis methods to understand when and why some samplers outperform others. Last but not least, practitioners can easily extend the benchmark by submitting their own AL samplers.
    
[^16]: 面向预训练代码模型有效微调：实验研究及其拓展

    Towards Efficient Fine-tuning of Pre-trained Code Models: An Experimental Study and Beyond. (arXiv:2304.05216v1 [cs.SE])

    [http://arxiv.org/abs/2304.05216](http://arxiv.org/abs/2304.05216)

    本文研究了对预训练代码模型的微调，探索了各层预训练表示和编码的代码知识，提出了有效的微调方案。实验发现微调过程可以保留大部分代码属性，基本代码属性由较低和中间层捕获。

    

    近年来，在许多软件测试和分析任务中，对预训练代码模型进行微调（如CodeBERT）以适应下游任务取得了巨大成功。虽然有效且流行，但微调预训练参数会导致大量计算成本。在本文中，我们进行了广泛的实验研究，探索微调期间每层预训练表示和编码的代码知识发生了什么。然后，我们基于上述发现提出了有效的微调大型预训练代码模型的替代方案。我们的实验研究表明：（1）源代码的词汇、语法和结构性质分别编码在较低、中间和较高的层中，而语义属性跨越整个模型。（2）微调过程保留了大部分代码属性。具体而言，较低和中间层捕获的基本代码属性在微调期间仍然保留。此外，我们发现...

    Recently, fine-tuning pre-trained code models such as CodeBERT on downstream tasks has achieved great success in many software testing and analysis tasks. While effective and prevalent, fine-tuning the pre-trained parameters incurs a large computational cost. In this paper, we conduct an extensive experimental study to explore what happens to layer-wise pre-trained representations and their encoded code knowledge during fine-tuning. We then propose efficient alternatives to fine-tune the large pre-trained code model based on the above findings. Our experimental study shows that (1) lexical, syntactic and structural properties of source code are encoded in the lower, intermediate, and higher layers, respectively, while the semantic property spans across the entire model. (2) The process of fine-tuning preserves most of the code properties. Specifically, the basic code properties captured by lower and intermediate layers are still preserved during fine-tuning. Furthermore, we find that o
    
[^17]: 一种用于遥感图像的十亿级基础模型

    A Billion-scale Foundation Model for Remote Sensing Images. (arXiv:2304.05215v1 [cs.CV])

    [http://arxiv.org/abs/2304.05215](http://arxiv.org/abs/2304.05215)

    本文介绍了一个用于遥感图像的十亿级基础模型，并研究了增加模型参数数量对该模型在下游任务中的性能影响，实验显示增加模型参数数量可以显著提高性能。

    

    随着基础模型在视觉任务中的潜力引起了广泛关注，先对这些模型进行预训练已成为一个关键步骤。预训练基础模型的三个关键因素是预训练方法、预训练数据集的大小以及模型参数的数量。最近，遥感领域的研究主要关注预训练方法和数据集的大小，对模型参数的数量关注较少。本文通过研究增加模型参数数量对基础模型在旋转目标检测和语义分割等下游任务中性能的影响来弥补这一空白。我们使用不同数量参数（包括86M、605.26M、1.3B和2.4B）的基础模型进行预训练，以确定参数增加是否会提高下游任务的性能。据我们所知，这是第一个用于遥感图像的十亿级基础模型。我们的实验表明，增加模型参数数量可以显著提高下游任务的性能。此外，我们还介绍了一个包含10亿个遥感图像的新的预训练数据集，并向研究社区公开。

    As the potential of foundation models in visual tasks has garnered significant attention, pretraining these models before downstream tasks has become a crucial step. The three key factors in pretraining foundation models are the pretraining method, the size of the pretraining dataset, and the number of model parameters. Recently, research in the remote sensing field has focused primarily on the pretraining method and the size of the dataset, with limited emphasis on the number of model parameters. This paper addresses this gap by examining the effect of increasing the number of model parameters on the performance of foundation models in downstream tasks such as rotated object detection and semantic segmentation. We pretrained foundation models with varying numbers of parameters, including 86M, 605.26M, 1.3B, and 2.4B, to determine whether performance in downstream tasks improved with an increase in parameters. To the best of our knowledge, this is the first billion-scale foundation mod
    
[^18]: CGXplain: 基于规则的深度神经网络解释，使用双线性规划。

    CGXplain: Rule-Based Deep Neural Network Explanations Using Dual Linear Programs. (arXiv:2304.05207v1 [cs.LG])

    [http://arxiv.org/abs/2304.05207](http://arxiv.org/abs/2304.05207)

    该论文介绍了一种名为 CGX 的分解方法，使用双线性规划从深度神经网络的隐藏表示中提取规则，优化对齐，复杂度和稳定性。

    

    基于规则的代理模型是近似深度神经网络决策边界的一种有效且可解释的方式，使人类易于理解深度学习模型。现有的最先进的分解方法，即那些考虑到DNN的潜在空间以提取更精确规则集的方法，成功地推导出了高精度的规则集。但是，它们 a) 不能保证代理模型已从与 DNN 相同的变量中学习（对齐），b) 只允许优化单个目标，例如准确性，这可能导致过度大的规则集（复杂性），并且 c) 使用决策树算法作为中间模型，可能会导致相同DNN的不同解释（稳定性）。本文介绍了使用双线性规划从DNN的隐藏表示中提取规则的分解方法 CGX（列生成解释器） 来解决这些限制。这种方法允许优化多个目标，例如对齐，复杂度和稳定性。

    Rule-based surrogate models are an effective and interpretable way to approximate a Deep Neural Network's (DNN) decision boundaries, allowing humans to easily understand deep learning models. Current state-of-the-art decompositional methods, which are those that consider the DNN's latent space to extract more exact rule sets, manage to derive rule sets at high accuracy. However, they a) do not guarantee that the surrogate model has learned from the same variables as the DNN (alignment), b) only allow to optimise for a single objective, such as accuracy, which can result in excessively large rule sets (complexity), and c) use decision tree algorithms as intermediate models, which can result in different explanations for the same DNN (stability). This paper introduces the CGX (Column Generation eXplainer) to address these limitations a decompositional method using dual linear programming to extract rules from the hidden representations of the DNN. This approach allows to optimise for a
    
[^19]: TinyReptile：联邦元学习实现的迷你机器学习

    TinyReptile: TinyML with Federated Meta-Learning. (arXiv:2304.05201v1 [cs.LG])

    [http://arxiv.org/abs/2304.05201](http://arxiv.org/abs/2304.05201)

    TinyReptile是一个联邦元学习实现的迷你机器学习算法，可以在迷你设备上协作学习神经网络，并通过使用全局数据实现快速收敛和保护本地数据隐私。

    

    迷你机器学习（TinyML）是一个迅速发展的领域，旨在为资源受限的微控制器（MCU）民主化机器学习。鉴于这些微型设备的普及性，有必要问是否TinyML应用程序可以从聚合他们的知识中受益。联邦学习（FL）使得分散的代理可以共同学习一个全局模型，而不共享敏感的本地数据。然而，由于实际部署环境的复杂性和每个设备可用数据的异构性，一个常见的全局模型可能不能适用于所有设备。此外， TinyML 硬件的部署具有重要的计算和通信约束，传统机器学习无法解决这些问题。针对这些挑战，我们提出了 TinyReptile，这是一个简单而有效的算法，灵感来源于元学习和在线学习，可以在迷你设备上协作学习神经网络（NN）的坚实初始化，可以快速适应这些设备的本地数据。我们使用 FL 来评估 TinyReptile，结果表明 TinyReptile 可以在保护本地数据隐私的同时，利用全局数据实现快速收敛和精确性能。

    Tiny machine learning (TinyML) is a rapidly growing field aiming to democratize machine learning (ML) for resource-constrained microcontrollers (MCUs). Given the pervasiveness of these tiny devices, it is inherent to ask whether TinyML applications can benefit from aggregating their knowledge. Federated learning (FL) enables decentralized agents to jointly learn a global model without sharing sensitive local data. However, a common global model may not work for all devices due to the complexity of the actual deployment environment and the heterogeneity of the data available on each device. In addition, the deployment of TinyML hardware has significant computational and communication constraints, which traditional ML fails to address. Considering these challenges, we propose TinyReptile, a simple but efficient algorithm inspired by meta-learning and online learning, to collaboratively learn a solid initialization for a neural network (NN) across tiny devices that can be quickly adapted 
    
[^20]: 自动梯度下降：无超参数的深度学习

    Automatic Gradient Descent: Deep Learning without Hyperparameters. (arXiv:2304.05187v1 [cs.LG])

    [http://arxiv.org/abs/2304.05187](http://arxiv.org/abs/2304.05187)

    本文提出了一种使用神经网络结构信息定义优化算法的方法，实现了一种无需手动调整超参数的一阶优化器 - 自动梯度下降。该算法在深度全连接网络和卷积网络中表现良好，并在标准基准测试数据集上表现出与手动调整优化器相当的效果。

    

    本文提出了一种新的方法来派生特定于神经网络结构的优化算法，实现了无超参数的一阶优化器，称为“自动梯度下降”。该方法利用神经体系结构显式地定义网络结构参数来优化深度全连接网络和卷积网络，证明了在标准基准测试数据集上与手动调整优化器效果相当。该算法扩展了镜像下降方法以处理非凸性复合目标函数。

    The architecture of a deep neural network is defined explicitly in terms of the number of layers, the width of each layer and the general network topology. Existing optimisation frameworks neglect this information in favour of implicit architectural information (e.g. second-order methods) or architecture-agnostic distance functions (e.g. mirror descent). Meanwhile, the most popular optimiser in practice, Adam, is based on heuristics. This paper builds a new framework for deriving optimisation algorithms that explicitly leverage neural architecture. The theory extends mirror descent to non-convex composite objective functions: the idea is to transform a Bregman divergence to account for the non-linear structure of neural architecture. Working through the details for deep fully-connected networks yields automatic gradient descent: a first-order optimiser without any hyperparameters. Automatic gradient descent trains both fully-connected and convolutional networks out-of-the-box and at Im
    
[^21]: 自我监督学习用于属性图异常检测中的异常判别与表示学习解耦：DSLAD算法

    Decoupling anomaly discrimination and representation learning: self-supervised learning for anomaly detection on attributed graph. (arXiv:2304.05176v1 [cs.LG])

    [http://arxiv.org/abs/2304.05176](http://arxiv.org/abs/2304.05176)

    本文提出了一种独特的自我监督算法DSLAD，通过解耦异常判别和表示学习来进行属性图上的异常检测，构建了一个平衡的特征空间解决了语义混合和不平衡问题，证明了其有效性。

    

    属性图上的异常检测是一个实际应用中很重要的课题。现有方法因主要关注异常判别而忽略表示学习，存在语义混合和不平衡问题。这与异质性假设相冲突，即异常节点通常直接与正常节点相连。此外，异常节点比正常节点少得多，表明数据分布呈长尾形态。为了应对这些挑战，本文提出了一种独特的算法DSLAD，它是一种自我监督的方法，通过将异常判别和表示学习进行解耦来进行异常检测。DSLAD采用双线性池和掩码自编码器作为异常判别器。通过解耦异常判别和表示学习，构建了一个平衡的特征空间，其中节点更具语义鉴别能力，同时解决了不平衡问题。在各种数据集上的广泛实验表明DSLAD的有效性。

    Anomaly detection on attributed graphs is a crucial topic for its practical application. Existing methods suffer from semantic mixture and imbalance issue because they mainly focus on anomaly discrimination, ignoring representation learning. It conflicts with the assortativity assumption that anomalous nodes commonly connect with normal nodes directly. Additionally, there are far fewer anomalous nodes than normal nodes, indicating a long-tailed data distribution. To address these challenges, a unique algorithm,Decoupled Self-supervised Learning forAnomalyDetection (DSLAD), is proposed in this paper. DSLAD is a self-supervised method with anomaly discrimination and representation learning decoupled for anomaly detection. DSLAD employs bilinear pooling and masked autoencoder as the anomaly discriminators. By decoupling anomaly discrimination and representation learning, a balanced feature space is constructed, in which nodes are more semantically discriminative, as well as imbalance issu
    
[^22]: 人工集体智能工程：概念和视角概述

    Artificial Collective Intelligence Engineering: a Survey of Concepts and Perspectives. (arXiv:2304.05147v1 [cs.AI])

    [http://arxiv.org/abs/2304.05147](http://arxiv.org/abs/2304.05147)

    人工集体智能工程旨在利用大量个体产生超越个体能力的效应和集体智能行为，成为计算系统设计目标。本文综述了人工集体智能工程的角色、概念和挑战，强调其对计算系统工程和设计的相关性和潜在影响。

    

    集体性是许多系统（包括自然和人工系统）的重要属性。利用大量的个体，往往可以产生超越最聪明个体能力的效应，甚至能在不那么聪明的个体中产生智能的集体行为。事实上，集体智能（即一群人以看似聪明的方式集中行动的能力）越来越成为计算系统的设计目标，这是受到物联网、群体机器人和众包计算等近期技术科学趋势的推动。多年来，自然和人工系统中观察到的集体智能一直是工程思想、模型和机制的灵感来源。如今，人工和计算集体智能成为认可的研究主题，涵盖各种技术、目标系统和应用领域。然而，该领域的文献往往缺乏对这种模式、其主要技术要素以及与其实际利用相关的挑战的系统概述。本文旨在提供这样的综述，通过讨论人工集体智能工程的角色、概念和挑战，并强调它对计算系统工程和设计的相关性和潜在影响。

    Collectiveness is an important property of many systems--both natural and artificial. By exploiting a large number of individuals, it is often possible to produce effects that go far beyond the capabilities of the smartest individuals, or even to produce intelligent collective behaviour out of not-so-intelligent individuals. Indeed, collective intelligence, namely the capability of a group to act collectively in a seemingly intelligent way, is increasingly often a design goal of engineered computational systems--motivated by recent techno-scientific trends like the Internet of Things, swarm robotics, and crowd computing, just to name a few. For several years, the collective intelligence observed in natural and artificial systems has served as a source of inspiration for engineering ideas, models, and mechanisms. Today, artificial and computational collective intelligence are recognised research topics, spanning various techniques, kinds of target systems, and application domains. Howev
    
[^23]: 自我调试：教授大型语言模型自动调试能力

    Teaching Large Language Models to Self-Debug. (arXiv:2304.05128v1 [cs.CL])

    [http://arxiv.org/abs/2304.05128](http://arxiv.org/abs/2304.05128)

    本文提出了一种自我调试方法，通过少量演示来教授大型语言模型自动调试其预测的程序，在多项代码生成基准测试中取得了最先进的性能。

    

    大型语言模型(LLM)在代码生成方面取得了卓越的性能，但对于复杂的编程任务，在一次性生成正确的解决方案方面变得具有挑战性。因此，一些先前的工作设计了程序修复方法来提高代码生成的性能。在本文中，我们提出了自我调试(Self-Debugging)方法，通过少量样本演示来教授大型语言模型调试其预测的程序。具体而言，我们证明了自我调试可以教授大型语言模型进行橡皮鸭子调试(Rubber Duck Debugging)。也就是说，在没有任何关于代码正确性或错误信息的反馈的情况下，该模型能够通过用自然语言解释生成的代码来识别它的错误。自我调试在多项代码生成基准测试中取得了最先进的性能，包括文本到SQL生成的Spider数据集，C++到Python翻译的TransCoder和文本到Python生成的MBPP。在没有单元测试的Spider基准测试中，所提出的自我调试方法明显优于现有的程序修复方法。

    Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any feedback on the code correctness or error messages, the model is able to identify its mistakes by explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit te
    
[^24]: 不同约束运动模型在基于图的轨迹预测中的评估

    Evaluation of Differentially Constrained Motion Models for Graph-Based Trajectory Prediction. (arXiv:2304.05116v1 [cs.RO])

    [http://arxiv.org/abs/2304.05116](http://arxiv.org/abs/2304.05116)

    使用深度学习模型进行运动预测在自动驾驶中表现出色，但缺乏解释性和可能违反物理约束。因此，结合差分约束运动模型能提供物理上可行的轨迹，研究表明低阶积分器模型表现更好，并且数值求解器对模型性能产生影响。

    

    随着深度学习模型在自动驾驶中表现出色且可调性高，成为运动预测的标准。然而，高度灵活性伴随的是解释性缺失和可能违反的物理约束。使用差分约束运动模型来提供物理上可行的轨迹，可以作为与这些数据驱动方法相配合的一个有前途的方向。本研究基于先前提出的基于图神经网络的模型 MTP-GO，研究了各种运动模型结合数值求解器进行预测任务的表现。研究表明，为了获得精确的预测结果，简单的模型，如低阶积分器模型，优于更复杂的运动学模型。此外，数值求解器可以对运动预测模型的性能产生重大影响。

    Given their adaptability and encouraging performance, deep-learning models are becoming standard for motion prediction in autonomous driving. However, with great flexibility comes a lack of interpretability and possible violations of physical constraints. Accompanying these data-driven methods with differentially-constrained motion models to provide physically feasible trajectories is a promising future direction. The foundation for this work is a previously introduced graph-neural-network-based model, MTP-GO. The neural network learns to compute the inputs to an underlying motion model to provide physically feasible trajectories. This research investigates the performance of various motion models in combination with numerical solvers for the prediction task. The study shows that simpler models, such as low-order integrator models, are preferred over more complex ones, e.g., kinematic models, to achieve accurate predictions. Further, the numerical solver can have a substantial impact o
    
[^25]: 如果意识在动态上具有相关性，则人工智能不会具有意识。

    If consciousness is dynamically relevant, artificial intelligence isn't conscious. (arXiv:2304.05077v1 [cs.AI])

    [http://arxiv.org/abs/2304.05077](http://arxiv.org/abs/2304.05077)

    本文指出，如果意识在动态上具有相关性，则人工智能系统无法具有意识，因为这些系统的设计和验证排除了可能与意识相关的动力学效应。

    

    我们证明了，如果意识与系统状态的时间演化有关，也就是说，如果它在动态上具有相关性，则人工智能系统就不能具有意识。这是因为人工智能系统运行在CPU、GPU、TPU或其他处理器上，这些处理器的设计和验证是为了遵守计算动力学，从而系统地排除或抑制偏差。特别是，设计和验证排除或抑制了可能与意识相关的动力学效应，因此如果意识在动态上具有相关性，则人工智能系统不能具有意识。

    We demonstrate that if consciousness is relevant for the temporal evolution of a system's states -- that is, if it is dynamically relevant -- then AI systems cannot be conscious. That is because AI systems run on CPUs, GPUs, TPUs or other processors which have been designed and verified to adhere to computational dynamics that systematically preclude or suppress deviations. The design and verification preclude or suppress, in particular, potential consciousness-related dynamical effects, so that if consciousness is dynamically relevant, AI systems cannot be conscious.
    
[^26]: 基于深度学习的肺癌风险因素人工智能预测

    Artificial intelligence based prediction on lung cancer risk factors using deep learning. (arXiv:2304.05065v1 [eess.IV])

    [http://arxiv.org/abs/2304.05065](http://arxiv.org/abs/2304.05065)

    本研究利用深度学习开发出一种高精度的肺癌检测模型，可用于预测肺癌风险因素。

    

    本研究旨在利用深度学习方法（卷积神经网络）开发出一种能够准确检测肺癌的模型，通过分析患者记录的历史数据，本研究确定肺癌是预测肿瘤早期阶段的一个重要研究问题。与VGG16、InceptionV3和Resnet50等现有模型相比，本研究的模型实现了94％的准确率和0.1％的最小损失，可以在现实世界中预测肺癌风险因素，为医生提供更准确的肺癌检测手段。

    In this proposed work, we identified the significant research issues on lung cancer risk factors. Capturing and defining symptoms at an early stage is one of the most difficult phases for patients. Based on the history of patients records, we reviewed a number of current research studies on lung cancer and its various stages. We identified that lung cancer is one of the significant research issues in predicting the early stages of cancer disease. This research aimed to develop a model that can detect lung cancer with a remarkably high level of accuracy using the deep learning approach (convolution neural network). This method considers and resolves significant gaps in previous studies. We compare the accuracy levels and loss values of our model with VGG16, InceptionV3, and Resnet50. We found that our model achieved an accuracy of 94% and a minimum loss of 0.1%. Hence physicians can use our convolution neural network models for predicting lung cancer risk factors in the real world. More
    
[^27]: 深度图表示学习综述

    A Comprehensive Survey on Deep Graph Representation Learning. (arXiv:2304.05055v1 [cs.LG])

    [http://arxiv.org/abs/2304.05055](http://arxiv.org/abs/2304.05055)

    本文综述了深度图表示学习的研究现状和存在的问题，并指出利用深度学习已经显示出巨大的优势和潜力。

    

    图表示学习旨在将高维稀疏的图结构数据有效地编码成低维密集向量，这是一个基本任务，在包括机器学习和数据挖掘在内的一系列领域都得到了广泛的研究。传统图嵌入方法遵循这样一种基本思想，即图中相互连接的节点的嵌入矢量仍然能够保持相对接近的距离，从而保留了图中节点之间的结构信息。然而，这种方法存在以下问题：（i）传统方法的模型容量受限，限制了学习性能; （ii）现有技术通常依赖于无监督学习策略，无法与最新的学习范式相结合；（iii）表示学习和下游任务相互依存，应共同加强。随着深度学习的显着成功，深度图表示学习已经显示出巨大的潜力和优势。

    Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages 
    
[^28]: 多个面部识别系统组件的同时对抗攻击

    Simultaneous Adversarial Attacks On Multiple Face Recognition System Components. (arXiv:2304.05048v1 [cs.CV])

    [http://arxiv.org/abs/2304.05048](http://arxiv.org/abs/2304.05048)

    本文探究对抗样本对面部识别系统的多个组件的同时攻击，提出了三种攻击方法，并在实验中证明了其有效性。该研究为FRS的攻击向量提供了新的思路，同时提出了增强鲁棒性的方法。

    

    本文探讨了对抗样本对面部识别系统安全性的潜在威胁。虽然以往的研究已经探讨了对FRS的各个单独组件的对抗风险，但我们的研究首次探索了通过同时攻击FRS管道中的人脸检测器和特征提取器来欺骗多个组件的可能性。我们提出了三种针对FRS的多目标攻击，并通过对目标系统的初步实验分析证明其有效性。我们的攻击成功率可以达到100％，能够操纵面部检测概率高达50％，具体取决于对抗目标。本研究确定并检查了针对FRS的新型攻击向量，并提出了利用攻击向量的知识在训练FRS组件时增强其鲁棒性的可能方法。

    In this work, we investigate the potential threat of adversarial examples to the security of face recognition systems. Although previous research has explored the adversarial risk to individual components of FRSs, our study presents an initial exploration of an adversary simultaneously fooling multiple components: the face detector and feature extractor in an FRS pipeline. We propose three multi-objective attacks on FRSs and demonstrate their effectiveness through a preliminary experimental analysis on a target system. Our attacks achieved up to 100% Attack Success Rates against both the face detector and feature extractor and were able to manipulate the face detection probability by up to 50% depending on the adversarial objective. This research identifies and examines novel attack vectors against FRSs and suggests possible ways to augment the robustness by leveraging the attack vector's knowledge during training of an FRS's components.
    
[^29]: 在雨天我们会推特哪些食物？

    What Food Do We Tweet about on a Rainy Day?. (arXiv:2304.05041v1 [cs.SI])

    [http://arxiv.org/abs/2304.05041](http://arxiv.org/abs/2304.05041)

    研究发现不同的天气条件下人们在推特上谈论的食物存在差异，研究这种现象可促进对食品消费者选择和看法的理解。

    

    食品选择是一个由品味、氛围、文化或天气等因素塑造的复杂现象。本文探讨了不同天气条件下与食品相关的推特数据。我们结合了覆盖过去十年的拉脱维亚食品推特数据集和包含平均温度、降水等现象的天气观察数据集。我们发现了哪些天气条件导致了特定的食品信息共享；自动分类推特情感并讨论如何根据天气情况变化。这项研究有助于大规模社交网络数据的理解，了解食品消费者的选择和看法。

    Food choice is a complex phenomenon shaped by factors such as taste, ambience, culture or weather. In this paper, we explore food-related tweeting in different weather conditions. We inspect a Latvian food tweet dataset spanning the past decade in conjunction with a weather observation dataset consisting of average temperature, precipitation, and other phenomena. We find which weather conditions lead to specific food information sharing; automatically classify tweet sentiment and discuss how it changes depending on the weather. This research contributes to the growing area of large-scale social network data understanding of food consumers' choices and perceptions.
    
[^30]: 人机合作生成语义特征列表

    Human-machine cooperation for semantic feature listing. (arXiv:2304.05012v1 [cs.CL])

    [http://arxiv.org/abs/2304.05012](http://arxiv.org/abs/2304.05012)

    本文提出了一种人机合作的方法，将有限数据的人类词汇语义模型和大型语言模型结合起来，高效生成高质量的语义特征规范。

    

    语义特征规范是描述人类概念知识的重要工具，但需要大量人力。大型语言模型（LLM）为自动生成此类功能提供了新途径，但容易出现显著的错误。本文介绍了一种新方法，将来自有限数据的学习型人类词汇语义模型与LLM生成的数据相结合，以高效生成高质量的特征规范。

    Semantic feature norms, lists of features that concepts do and do not possess, have played a central role in characterizing human conceptual knowledge, but require extensive human labor. Large language models (LLMs) offer a novel avenue for the automatic generation of such feature lists, but are prone to significant error. Here, we present a new method for combining a learned model of human lexical-semantics from limited data with LLM-generated data to efficiently generate high-quality feature norms.
    
[^31]: 习惯与目标的协同作用：一种基于变分贝叶斯框架的行为模型

    Habits and goals in synergy: a variational Bayesian framework for behavior. (arXiv:2304.05008v1 [cs.LG])

    [http://arxiv.org/abs/2304.05008](http://arxiv.org/abs/2304.05008)

    本文提出一种基于变分贝叶斯理论的行为模型，通过引入一种贝叶斯潜变量“意图”，将习惯性行为和目标导向行为联系起来，从而实现更灵活、高效的行为模拟。

    

    如何高效、灵活地行为是理解生物机体和创建智能化身人工智能的核心问题。行为被归类为两种类型：快速而不灵活的奖励最大化的习惯行为，和灵活而慢的目标导向行为。传统上，习惯和目标导向行为被认为是由大脑中的两个不同系统处理的。本文提出基于变分贝叶斯理论，将这两种行为联系起来，引入一种贝叶斯潜变量称为“意图”。习惯性行为是通过使用意图的先验分布生成的，该先验分布是没有目标的；而目标导向行为是由意图的后验分布生成的，该后验分布取决于目标。基于这个想法，我们提出了一种新颖的行为建模的贝叶斯框架。

    How to behave efficiently and flexibly is a central problem for understanding biological agents and creating intelligent embodied AI. It has been well known that behavior can be classified as two types: reward-maximizing habitual behavior, which is fast while inflexible; and goal-directed behavior, which is flexible while slow. Conventionally, habitual and goal-directed behaviors are considered handled by two distinct systems in the brain. Here, we propose to bridge the gap between the two behaviors, drawing on the principles of variational Bayesian theory. We incorporate both behaviors in one framework by introducing a Bayesian latent variable called "intention". The habitual behavior is generated by using prior distribution of intention, which is goal-less; and the goal-directed behavior is generated by the posterior distribution of intention, which is conditioned on the goal. Building on this idea, we present a novel Bayesian framework for modeling behaviors. Our proposed framework 
    
[^32]: 人-物交互检测的关系上下文学习

    Relational Context Learning for Human-Object Interaction Detection. (arXiv:2304.04997v1 [cs.CV])

    [http://arxiv.org/abs/2304.04997](http://arxiv.org/abs/2304.04997)

    本文提出了一种新的方法——多路复用关系网络（MUREN），通过丰富的上下文交换来学习全面的关系上下文，达到了HOI检测的最先进性能

    

    HOI检测的最新方法通常建立在使用两个解码器分支的转换器体系结构上，一个用于人-物对检测，另一个用于交互分类。然而，这种解耦的转换器可能会因为分支之间的上下文交换不足而导致缺乏关系推理的上下文信息，而在发现HOI实例时这是至关重要的。在本文中，我们提出了多路复用关系网络（MUREN），它使用人，物和交互标记的一元，二元和三元关系在三个解码器分支之间进行丰富的上下文交换。所提出的方法学习全面的关系上下文，以发现HOI实例，在HOI检测的两个标准基准HICO-DET和V-COCO上实现了最先进的性能。

    Recent state-of-the-art methods for HOI detection typically build on transformer architectures with two decoder branches, one for human-object pair detection and the other for interaction classification. Such disentangled transformers, however, may suffer from insufficient context exchange between the branches and lead to a lack of context information for relational reasoning, which is critical in discovering HOI instances. In this work, we propose the multiplex relation network (MUREN) that performs rich context exchange between three decoder branches using unary, pairwise, and ternary relations of human, object, and interaction tokens. The proposed method learns comprehensive relational contexts for discovering HOI instances, achieving state-of-the-art performance on two standard benchmarks for HOI detection, HICO-DET and V-COCO.
    
[^33]: 使用二进制卷积神经网络的小天体相对导航的高效特征描述

    Efficient Feature Description for Small Body Relative Navigation using Binary Convolutional Neural Networks. (arXiv:2304.04985v1 [cs.CV])

    [http://arxiv.org/abs/2304.04985](http://arxiv.org/abs/2304.04985)

    本文提出了一种基于二进制卷积神经网络层的局部特征描述结构，有效降低了小天体相对导航的计算和内存需求，同时提高了性能。

    

    小天体任务依赖于光学特征跟踪来表征和绕目标天体进行相对导航。虽然基于深度学习的特征跟踪技术是当前人机交互过程的一个有前途的替代方案，但由于航天器的计算和内存限制，设计能够在机载上运行的深度网络结构具有挑战性。本文提出了一种新的深度局部特征描述结构，利用二进制卷积神经网络层显着降低了计算和内存需求。我们使用传统任务和正在进行的任务的小天体真实图像对模型进行训练和测试，并展示了相对于传统手工设计的方法，提高了性能。此外，我们在下一代航天器处理器的代理中实现了我们的模型，并展示了在线特征跟踪的可行运行时间。

    Missions to small celestial bodies rely heavily on optical feature tracking for characterization of and relative navigation around the target body. While techniques for feature tracking based on deep learning are a promising alternative to current human-in-the-loop processes, designing deep architectures that can operate onboard spacecraft is challenging due to onboard computational and memory constraints. This paper introduces a novel deep local feature description architecture that leverages binary convolutional neural network layers to significantly reduce computational and memory requirements. We train and test our models on real images of small bodies from legacy and ongoing missions and demonstrate increased performance relative to traditional handcrafted methods. Moreover, we implement our models onboard a surrogate for the next-generation spacecraft processor and demonstrate feasible runtimes for online feature tracking.
    
[^34]: 生物因子调控神经网络

    Biological Factor Regulatory Neural Network. (arXiv:2304.04982v1 [cs.LG])

    [http://arxiv.org/abs/2304.04982](http://arxiv.org/abs/2304.04982)

    本文提出了一种生物因子调控神经网络（BFReg-NN），它使用基因表达数据，并将大多数现有的生物学知识（如基因调节网络和GO）集成到模型中，以模拟生物系统中生物因子之间的关系，并在模拟数据和真实数据上取得了优越的性能和可解释性。

    

    基因是分析生物系统的基础，许多最近的工作提出通过深度学习模型利用基因表达进行各种生物任务。尽管这些模型有很好的性能，但由于它们的黑盒本质，深度神经网络很难为人类提供生物学洞见。最近，一些工作将生物知识与神经网络结合起来，以提高模型的透明性和性能。然而，这些方法只能纳入部分生物学知识，导致性能亚优。在本文中，我们提出了生物因子调控神经网络（BFReg-NN），这是一种可以模拟细胞系统中生物因子之间关系的通用框架。BFReg-NN从基因表达数据开始，能够将大多数现有的生物学知识集成到模型中，包括基因或蛋白质之间的调节关系（例如基因调节网络（GRN），蛋白质相互作用网络（PPI））和生物因子之间的层次关系（例如Gene Ontology（GO），pathways）。BFReg-NN还引入了一种称为因子调节惩罚（FRP）的新的正则化项到损失函数中，以确保学习模型中生物因子之间的调节关系得到表示。在模拟数据和真实数据上的实验结果表明，BFReg-NN相对于几种最先进的深度神经网络模型具有卓越的性能和解释性。

    Genes are fundamental for analyzing biological systems and many recent works proposed to utilize gene expression for various biological tasks by deep learning models. Despite their promising performance, it is hard for deep neural networks to provide biological insights for humans due to their black-box nature. Recently, some works integrated biological knowledge with neural networks to improve the transparency and performance of their models. However, these methods can only incorporate partial biological knowledge, leading to suboptimal performance. In this paper, we propose the Biological Factor Regulatory Neural Network (BFReg-NN), a generic framework to model relations among biological factors in cell systems. BFReg-NN starts from gene expression data and is capable of merging most existing biological knowledge into the model, including the regulatory relations among genes or proteins (e.g., gene regulatory networks (GRN), protein-protein interaction networks (PPI)) and the hierarc
    
[^35]: GRIL：一种二参数持久性基于向量化的机器学习方法

    GRIL: A $2$-parameter Persistence Based Vectorization for Machine Learning. (arXiv:2304.04970v1 [cs.LG])

    [http://arxiv.org/abs/2304.04970](http://arxiv.org/abs/2304.04970)

    本文提出一种名为GRIL的方法，用于将拓扑特征表示散度到机器学习模型中，该方法可以稳定地用于不同的过滤函数。

    

    一参数持久性同Topology Data Analysis (TDA)相关，可研究数据中隐藏着的连通分量和循环等拓扑特征。已应用于提高图神经网络（GNNs）等深度学习模型的表示能力。为了丰富拓扑特征的表示，本研究提出了研究双过滤函数诱导的二参数持久性模块的方法。为了将这些表示信息加入到机器学习模型中，我们引入了一个新的向量表示称为Generalized Rank Invariant Landscape \textsc{Gril}，并将其证明为在Lipschitz稳定条件下可微分，并且通过对基础过滤函数的编码可以容易地融入到机器学习模型中。我们提出了一个高效计算向量表示的算法。本研究还对我们的方法进行了测试。

    $1$-parameter persistent homology, a cornerstone in Topological Data Analysis (TDA), studies the evolution of topological features such as connected components and cycles hidden in data. It has been applied to enhance the representation power of deep learning models, such as Graph Neural Networks (GNNs). To enrich the representations of topological features, here we propose to study $2$-parameter persistence modules induced by bi-filtration functions. In order to incorporate these representations into machine learning models, we introduce a novel vector representation called Generalized Rank Invariant Landscape \textsc{Gril} for $2$-parameter persistence modules. We show that this vector representation is $1$-Lipschitz stable and differentiable with respect to underlying filtration functions and can be easily integrated into machine learning models to augment encoding topological features. We present an algorithm to compute the vector representation efficiently. We also test our method
    
[^36]: 强化学习辅导员在数学任务中更好地支持了低成绩学生

    Reinforcement Learning Tutor Better Supported Lower Performers in a Math Task. (arXiv:2304.04933v1 [cs.AI])

    [http://arxiv.org/abs/2304.04933](http://arxiv.org/abs/2304.04933)

    本文证明了深度强化学习可用于提供自适应的教育支持，尤其对于最初成绩较低的学生具有最大的益处。

    

    资源限制使得为所有学生提供个性化教学变得困难。强化学习可以成为减少发展成本、提高智能辅导软件效果的关键工具，旨在为学生提供正确的支持。在这里，我们展示了深度强化学习如何在叙述故事线软件中为学习“容积”概念的学生提供自适应教育支持。通过解释性人工智能工具，我们也提取了有关学习的可解释洞见，证明了所得政策在不同的学生群体中具有类似的表现。最重要的是，在这两项研究中，强化学习故事系统对最初的预测分数最低的学生有最大的益处，这表明了AI适应并为低成绩学生提供支持的机会。

    Resource limitations make it hard to provide all students with one of the most effective educational interventions: personalized instruction. Reinforcement learning could be a key tool to reduce the development cost and improve the effectiveness of, intelligent tutoring software that aims to provide the right support, at the right time, to a student. Here we illustrate that deep reinforcement learning can be used to provide adaptive pedagogical support to students learning about the concept of volume in a narrative storyline software. Using explainable artificial intelligence tools, we also extracted interpretable insights about the pedagogical policy learned, and we demonstrate that the resulting policy had similar performance in a different student population. Most importantly, in both studies the reinforcement-learning narrative system had the largest benefit for those students with the lowest initial pretest scores, suggesting the opportunity for AI to adapt and provide support for
    
[^37]: 显式和隐式语义排序框架

    Explicit and Implicit Semantic Ranking Framework. (arXiv:2304.04918v1 [cs.IR])

    [http://arxiv.org/abs/2304.04918](http://arxiv.org/abs/2304.04918)

    本文提出了一个名为sRank的通用语义学习排名框架，它使用transformer模型，能够在智能回复和环境临床智能等真实应用中，实现11.7%的离线准确度提升。

    

    在许多实际应用中，核心难题是将一个查询与一个可变且有限的文档集中的最佳文档进行匹配。现有的工业解决方案，特别是延迟受限的服务，通常依赖于相似性算法，这些算法为了速度而牺牲了质量。本文介绍了一个通用的语义学习排名框架，自我训练语义交叉关注排名（sRank）。这个基于transformer的框架使用线性成对损失，具有可变的训练批量大小、实现质量提升和高效率，并已成功应用于微软公司的两个工业任务：智能回复（SR）和环境临床智能（ACI）的真实大规模数据集上。在智能回复中，$sRank$通过基于消费者和支持代理信息的预定义解决方案选择最佳答案，帮助用户实时获得技术支持。在SR任务上，$sRank$实现了11.7%的离线top-one准确度提升，比之前的系统更加优秀。

    The core challenge in numerous real-world applications is to match an inquiry to the best document from a mutable and finite set of candidates. Existing industry solutions, especially latency-constrained services, often rely on similarity algorithms that sacrifice quality for speed. In this paper we introduce a generic semantic learning-to-rank framework, Self-training Semantic Cross-attention Ranking (sRank). This transformer-based framework uses linear pairwise loss with mutable training batch sizes and achieves quality gains and high efficiency, and has been applied effectively to show gains on two industry tasks at Microsoft over real-world large-scale data sets: Smart Reply (SR) and Ambient Clinical Intelligence (ACI). In Smart Reply, $sRank$ assists live customers with technical support by selecting the best reply from predefined solutions based on consumer and support agent messages. It achieves 11.7% gain in offline top-one accuracy on the SR task over the previous system, and 
    
[^38]: AffectMachine-Classical: 一种生成情感古典音乐的新系统

    AffectMachine-Classical: A novel system for generating affective classical music. (arXiv:2304.04915v1 [cs.SD])

    [http://arxiv.org/abs/2304.04915](http://arxiv.org/abs/2304.04915)

    AffectMachine-Classical是一种可以实时生成情感古典音乐的音乐生成系统，旨在将其嵌入到生物反馈系统中帮助用户自我调节情绪。

    

    本文介绍了一种名为AffectMachine-Classical的新音乐生成系统，能够实时生成情感古典音乐。AffectMachine被设计用于生物反馈系统（如脑-计算机接口），以帮助用户意识到和调节自己的动态情感状态。也就是说，该系统是为基于音乐的医疗技术开发的，支持用户实时情绪自我调节。我们提供了基于规则和概率的系统架构概述，描述了系统的主要方面及其创新之处。然后，我们展示了一项听众研究的结果，旨在验证该系统能够可靠地传达目标情感给听众。研究结果表明，AffectMachine-Classical在传达各种Arousal水平（$R^2 = .96$）方面非常有效，并且在Valence方面也非常令人信服（$R^2 = .90$）。将来的工作将把 AffectMachine-Classical嵌入到生物反馈应用中，以测试其在实时促进情绪调节方面的能力。

    This work introduces a new music generation system, called AffectMachine-Classical, that is capable of generating affective Classic music in real-time. AffectMachine was designed to be incorporated into biofeedback systems (such as brain-computer-interfaces) to help users become aware of, and ultimately mediate, their own dynamic affective states. That is, this system was developed for music-based MedTech to support real-time emotion self-regulation in users. We provide an overview of the rule-based, probabilistic system architecture, describing the main aspects of the system and how they are novel. We then present the results of a listener study that was conducted to validate the ability of the system to reliably convey target emotions to listeners. The findings indicate that AffectMachine-Classical is very effective in communicating various levels of Arousal ($R^2 = .96$) to listeners, and is also quite convincing in terms of Valence (R^2 = .90). Future work will embed AffectMachine-
    
[^39]: 监管市场：人工智能治理的未来

    Regulatory Markets: The Future of AI Governance. (arXiv:2304.04914v1 [cs.AI])

    [http://arxiv.org/abs/2304.04914](http://arxiv.org/abs/2304.04914)

    提出一种监管市场的概念，即政府要求受监管对象从私人监管机构购买监管服务，以克服过度依赖行业自律和立法机构缺乏专业知识的局限性，从而逐步实现人工智能的恰当监管。

    

    恰当地监管人工智能是一个日益紧迫的政策挑战。立法机构和监管机构缺乏翻译公众需求为法律要求所需的专业知识。过度依赖行业自律未能使AI系统的生产者和使用者对民主要求负责。提出了监管市场的概念，即政府要求受监管对象从私人监管机构购买监管服务。这种方法可以克服命令和控制监管和自我监管的局限性。监管市场可以使政府为AI监管建立政策优先级，同时依靠市场力量和行业研发努力来开创最能实现政策制定者声明目标的监管方法。

    Appropriately regulating artificial intelligence is an increasingly urgent policy challenge. Legislatures and regulators lack the specialized knowledge required to best translate public demands into legal requirements. Overreliance on industry self-regulation fails to hold producers and users of AI systems accountable to democratic demands. Regulatory markets, in which governments require the targets of regulation to purchase regulatory services from a private regulator, are proposed. This approach to AI regulation could overcome the limitations of both command-and-control regulation and self-regulation. Regulatory market could enable governments to establish policy priorities for the regulation of AI, whilst relying on market forces and industry R&D efforts to pioneer the methods of regulation that best achieve policymakers' stated objectives.
    
[^40]: 使用CNN和Transformer进行金融时间序列预测

    Financial Time Series Forecasting using CNN and Transformer. (arXiv:2304.04912v1 [cs.LG])

    [http://arxiv.org/abs/2304.04912](http://arxiv.org/abs/2304.04912)

    本文提出了结合使用CNN和Transformer来预测金融时间序列的涨跌幅。

    

    时间序列预测在各个领域的决策中都是重要的。其中，金融时间序列（如股票价格）的预测往往比较困难，因为很难对数据点之间的短期和长期时间依赖性建模。卷积神经网络（CNN）对于捕捉短期依赖关系的局部模式很擅长。然而，由于有限的感受野，CNN不能学习长期依赖性。另一方面，Transformer可以学习全局上下文和长期依赖关系。在本文中，我们提出了利用CNN和Transformer的优势来建模时间序列中的短期和长期依赖关系，并预测未来价格的涨跌幅。在实验中，我们比较了所提出的方法与常用的统计和深度学习方法在预测标普500成分股盘中盘股价格变化方面的成功应用。

    Time series forecasting is important across various domains for decision-making. In particular, financial time series such as stock prices can be hard to predict as it is difficult to model short-term and long-term temporal dependencies between data points. Convolutional Neural Networks (CNN) are good at capturing local patterns for modeling short-term dependencies. However, CNNs cannot learn long-term dependencies due to the limited receptive field. Transformers on the other hand are capable of learning global context and long-term dependencies. In this paper, we propose to harness the power of CNNs and Transformers to model both short-term and long-term dependencies within a time series, and forecast if the price would go up, down or remain the same (flat) in the future. In our experiments, we demonstrated the success of the proposed method in comparison to commonly adopted statistical and deep learning methods on forecasting intraday stock price change of S&P 500 constituents.
    
[^41]: 通过生成未来视图图像语义以改善视觉与语言导航

    Improving Vision-and-Language Navigation by Generating Future-View Image Semantics. (arXiv:2304.04907v1 [cs.CV])

    [http://arxiv.org/abs/2304.04907](http://arxiv.org/abs/2304.04907)

    本文提出了三个代理任务用于在代理的领域内预训练，以帮助模型生成未来视图图像语义，从而在视觉与语言导航任务中提高性能。

    

    视觉与语言导航是一项任务，要求代理根据自然语言指令在环境中进行导航。在每个步骤中，代理通过从可导航位置集合中进行选择来选择下一步动作。在本文中，我们旨在进一步探索代理是否可以受益于在导航期间生成潜在未来视图。直观地说，人类根据自然语言指令和周围的视图会对未来的环境有一个预期，并帮助正确地导航。因此，为了给代理装备这种生成未来导航视图语义的能力，我们首先在代理的领域内预训练过程中提出了三种代理任务: 掩蔽全景建模 (MPM)，掩蔽轨迹建模 (MTM) 和带有图像生成的动作预测 (APIG)。这三个目标教会了模型预测全景中的缺少视图 (MPM)、预测完整轨迹中的缺少步骤 (MTM) 和进行动作预测和图像生成 (APIG)。

    Vision-and-Language Navigation (VLN) is the task that requires an agent to navigate through the environment based on natural language instructions. At each step, the agent takes the next action by selecting from a set of navigable locations. In this paper, we aim to take one step further and explore whether the agent can benefit from generating the potential future view during navigation. Intuitively, humans will have an expectation of how the future environment will look like, based on the natural language instructions and surrounding views, which will aid correct navigation. Hence, to equip the agent with this ability to generate the semantics of future navigation views, we first propose three proxy tasks during the agent's in-domain pre-training: Masked Panorama Modeling (MPM), Masked Trajectory Modeling (MTM), and Action Prediction with Image Generation (APIG). These three objectives teach the model to predict missing views in a panorama (MPM), predict missing steps in the full tra
    
[^42]: EVKG：一个互联互通的智能交通系统电动汽车知识图谱

    EVKG: An Interlinked and Interoperable Electric Vehicle Knowledge Graph for Smart Transportation System. (arXiv:2304.04893v1 [cs.AI])

    [http://arxiv.org/abs/2304.04893](http://arxiv.org/abs/2304.04893)

    EVKG是一个综合、跨领域、可扩展、开放的地理空间知识管理系统，涵盖了与电动汽车相关的基本知识。集成其他知名知识图谱和本体模块，增强了EVKG在电动汽车决策中的价值。

    

    在过去的十年里，电动汽车行业经历了空前的增长和多样化，形成了复杂的生态系统。为了有效地管理这个多方面的领域，我们提出了一个以电动汽车为中心的知识图谱(EVKG)，作为一个综合的、跨领域的、可扩展的和开放的地理空间知识管理系统。EVKG涵盖了与电动汽车相关的基本知识，包括电动汽车的采用、电动汽车供应设备和电力传输网络，通过提供及时准确的信息和分析来支持与电动汽车技术开发、基础设施规划和政策制定相关的决策。为了丰富和上下文化EVKG，我们将现有的知名知识图谱和本体的开发相关的本体模块集成到EVKG中。这种集成使EVKG与Linked Data Open Cloud中的其他知识图谱具有互操作性，增强了EVKG作为电动汽车决策的知识中心的价值。

    Over the past decade, the electric vehicle industry has experienced unprecedented growth and diversification, resulting in a complex ecosystem. To effectively manage this multifaceted field, we present an EV-centric knowledge graph (EVKG) as a comprehensive, cross-domain, extensible, and open geospatial knowledge management system. The EVKG encapsulates essential EV-related knowledge, including EV adoption, electric vehicle supply equipment, and electricity transmission network, to support decision-making related to EV technology development, infrastructure planning, and policy-making by providing timely and accurate information and analysis. To enrich and contextualize the EVKG, we integrate the developed EV-relevant ontology modules from existing well-known knowledge graphs and ontologies. This integration enables interoperability with other knowledge graphs in the Linked Data Open Cloud, enhancing the EVKG's value as a knowledge hub for EV decision-making. Using six competency quest
    
[^43]: 使用基于负采样的方法评估多项选择题中的干扰选项

    DISTO: Evaluating Textual Distractors for Multi-Choice Questions using Negative Sampling based Approach. (arXiv:2304.04881v1 [cs.CL])

    [http://arxiv.org/abs/2304.04881](http://arxiv.org/abs/2304.04881)

    DISTO提出了一种新的学习度量标准来评估多项选择题中生成的干扰选项。DISTO与人类对干扰选项的评分高度相关，且排名最先进的干扰选项生成模型的性能与基于机器翻译的度量标准非常不同，表明机器翻译度量标准不适用于干扰选项评估。

    

    多项选择题是一种评估阅读理解能力的高效常见方法。每道多项选择题需要一组干扰选项，这些选项虽然不正确，但要足够合理以考查学生的知识掌握情况。已经提出了干扰选项生成模型，并且它们的性能通常使用机器翻译度量标准来评估。然而，机器翻译度量标准经常误判生成的干扰选项的合适性。我们提出了DISTO：用于评估生成干扰选项的第一个学习度量标准。我们通过展示DISTO得分与人类对干扰选项质量的评分高度相关来验证DISTO。与此同时，DISTO排名最先进的干扰选项生成模型的性能与基于机器翻译的度量标准非常不同，表明机器翻译度量标准不应用于干扰选项评估。

    Multiple choice questions (MCQs) are an efficient and common way to assess reading comprehension (RC). Every MCQ needs a set of distractor answers that are incorrect, but plausible enough to test student knowledge. Distractor generation (DG) models have been proposed, and their performance is typically evaluated using machine translation (MT) metrics. However, MT metrics often misjudge the suitability of generated distractors. We propose DISTO: the first learned evaluation metric for generated distractors. We validate DISTO by showing its scores correlate highly with human ratings of distractor quality. At the same time, DISTO ranks the performance of state-of-the-art DG models very differently from MT-based metrics, showing that MT metrics should not be used for distractor evaluation.
    
[^44]: ImageCaptioner$^2$: 针对图像字幕偏差放大评估的图像字幕生成器

    ImageCaptioner$^2$: Image Captioner for Image Captioning Bias Amplification Assessment. (arXiv:2304.04874v1 [cs.CV])

    [http://arxiv.org/abs/2304.04874](http://arxiv.org/abs/2304.04874)

    本文提出了一种新的图像字幕生成器 ImageCaptioner$^2$ ，用于针对图像字幕偏差放大进行评估。

    

    大多数预训练学习系统都会受到偏差的影响，这通常来自数据、模型或两者。衡量和量化偏差及其来源是一项具有挑战性的任务，并在图像字幕生成方面得到了广泛的研究。然而，我们观察到现有评估指标在包括视觉信号方面存在一定不一致性。本文提出了一种新的针对图像字幕生成的偏差评估指标，称为 ImageCaptioner$^2$。与现有方法仅基于生成的字幕评估图像字幕算法不同，ImageCaptioner$^2$在测量偏差时考虑图像。我们还设计了一种公式来作为基于提示的图像字幕生成来测量生成字幕的偏差，而不是使用传统方法。

    Most pre-trained learning systems are known to suffer from bias, which typically emerges from the data, the model, or both. Measuring and quantifying bias and its sources is a challenging task and has been extensively studied in image captioning. Despite the significant effort in this direction, we observed that existing metrics lack consistency in the inclusion of the visual signal. In this paper, we introduce a new bias assessment metric, dubbed $ImageCaptioner^2$, for image captioning. Instead of measuring the absolute bias in the model or the data, $ImageCaptioner^2$ pay more attention to the bias introduced by the model w.r.t the data bias, termed bias amplification. Unlike the existing methods, which only evaluate the image captioning algorithms based on the generated captions only, $ImageCaptioner^2$ incorporates the image while measuring the bias. In addition, we design a formulation for measuring the bias of generated captions as prompt-based image captioning instead of using 
    
[^45]: ShapeShift：基于超椭球形状的机器人抓取物体姿态估计

    ShapeShift: Superquadric-based Object Pose Estimation for Robotic Grasping. (arXiv:2304.04861v1 [cs.CV])

    [http://arxiv.org/abs/2304.04861](http://arxiv.org/abs/2304.04861)

    ShapeShift是一个机器人抓取物体姿态估计的框架，它使用超椭球的参考形状预测物体的姿态，具有广泛的泛化能力和准确性。

    

    物体姿态估计是精确物体操纵中至关重要的任务。然而，当前技术严重依赖于参考3D物体，限制了它们的泛化能力，并使扩展到新的物体类别变得昂贵。直接姿态预测也不能提供关于机器人抓取的充分信息而不参考3D模型。基于关键点的方法提供了内在的描述性能力而不依赖于准确的3D模型，但它们可能缺乏一致性和准确性。针对这些挑战，本文提出了一个基于超椭球形状的ShapeShift框架，用于物体姿态估计，该框架预测与适合于物体的原始形状相对的物体姿态。该框架提供了内在的描述能力，并具有超出训练集的任意几何形状的概括能力。

    Object pose estimation is a critical task in robotics for precise object manipulation. However, current techniques heavily rely on a reference 3D object, limiting their generalizability and making it expensive to expand to new object categories. Direct pose predictions also provide limited information for robotic grasping without referencing the 3D model. Keypoint-based methods offer intrinsic descriptiveness without relying on an exact 3D model, but they may lack consistency and accuracy. To address these challenges, this paper proposes ShapeShift, a superquadric-based framework for object pose estimation that predicts the object's pose relative to a primitive shape which is fitted to the object. The proposed framework offers intrinsic descriptiveness and the ability to generalize to arbitrary geometric shapes beyond the training set.
    
[^46]: 格上的序模式

    Ordinal Motifs in Lattices. (arXiv:2304.04827v1 [cs.AI])

    [http://arxiv.org/abs/2304.04827](http://arxiv.org/abs/2304.04827)

    本研究提出了“序模式”作为分析意义的单位，并研究了在格上通过正式背景的全尺度测量来识别这些序子结构的方法，以实现从中等尺寸序数数据集中检索基本含义。

    

    格是表示和分析关系和本体知识的常用结构。本研究提出了“序模式”作为分析意义的单位，研究这些序子结构（或标准尺度）通过来自正式概念分析领域的正式背景的全尺度测量。我们展示了底层决策问题是NP完全的，并提供了如何逐步识别序模式以节省计算量的结果。伴随我们的理论结果，我们展示了如何利用序模式从中等尺寸的序数数据集中检索基本含义。

    Lattices are a commonly used structure for the representation and analysis of relational and ontological knowledge. In particular, the analysis of these requires a decomposition of a large and high-dimensional lattice into a set of understandably large parts. With the present work we propose /ordinal motifs/ as analytical units of meaning. We study these ordinal substructures (or standard scales) through (full) scale-measures of formal contexts from the field of formal concept analysis. We show that the underlying decision problems are NP-complete and provide results on how one can incrementally identify ordinal motifs to save computational effort. Accompanying our theoretical results, we demonstrate how ordinal motifs can be leveraged to retrieve basic meaning from a medium sized ordinal data set.
    
[^47]: 网络犯罪预测的进展：机器学习、深度学习、迁移学习和自适应学习技术综述

    Advances in Cybercrime Prediction: A Survey of Machine, Deep, Transfer, and Adaptive Learning Techniques. (arXiv:2304.04819v1 [cs.LG])

    [http://arxiv.org/abs/2304.04819](http://arxiv.org/abs/2304.04819)

    本文从突破安全系统和窃取敏感数据的角度出发，介绍了机器学习、深度学习和迁移学习技术在预测和防止网络犯罪方面的最新进展，提到了最近研究中效果最好的递归和卷积神经网络。

    

    网络犯罪是一种不断增长的威胁，罪犯使用越来越复杂的技术来突破安全系统并窃取敏感数据。近年来，机器学习、深度学习和迁移学习技术已经成为预测网络犯罪和在其发生之前防止的有前途的工具。本文旨在提供使用上述技术预测网络犯罪的最新进展的全面调查，重点介绍每种方法相关的最新研究。为此，我们回顾了150多篇研究文章，并讨论了大约50篇最近和最相关的研究文章。我们首先讨论了一些网络犯罪常用的方法，然后重点介绍了最新的机器学习技术和深度学习技术，例如递归和卷积神经网络，这些技术在检测异常行为和识别潜在威胁方面非常有效。

    Cybercrime is a growing threat to organizations and individuals worldwide, with criminals using increasingly sophisticated techniques to breach security systems and steal sensitive data. In recent years, machine learning, deep learning, and transfer learning techniques have emerged as promising tools for predicting cybercrime and preventing it before it occurs. This paper aims to provide a comprehensive survey of the latest advancements in cybercrime prediction using above mentioned techniques, highlighting the latest research related to each approach. For this purpose, we reviewed more than 150 research articles and discussed around 50 most recent and relevant research articles. We start the review by discussing some common methods used by cyber criminals and then focus on the latest machine learning techniques and deep learning techniques, such as recurrent and convolutional neural networks, which were effective in detecting anomalous behavior and identifying potential threats. We al
    
[^48]: Scallop: 一种神经符号编程语言

    Scallop: A Language for Neurosymbolic Programming. (arXiv:2304.04812v1 [cs.PL])

    [http://arxiv.org/abs/2304.04812](http://arxiv.org/abs/2304.04812)

    Scallop是一种能够同时利用深度学习和逻辑推理优点的神经符号编程语言，它能够以数据和计算有效的方式训练神经符号应用程序，通过它可在AI任务中表达算法推理并融合逻辑领域知识，其解决方案可与最先进的模型相媲美或更高。

    

    我们提出了 Scallop，这是一种结合了深度学习和逻辑推理优点的语言。通过三个关键特性，Scallop 启用用户编写广泛的神经符号应用程序并以数据和计算有效的方式训练它们。这三个关键特性包括：1）基于关系数据模型的灵活符号表示；2）基于 Datalog 的声明性逻辑编程语言，支持递归、聚合和否定；3）基于证明半环理论的自动高效可微推理框架。我们在文献中的八种神经符号应用程序套件上评估 Scallop。我们的评估表明，Scallop 能够在多样化和具有挑战性的 AI 任务中表达算法推理，为机器学习程序员提供简洁的接口以融合逻辑领域知识，并提供可与最先进的模型相媲美或更高的解决方案。

    We present Scallop, a language which combines the benefits of deep learning and logical reasoning. Scallop enables users to write a wide range of neurosymbolic applications and train them in a data- and compute-efficient manner. It achieves these goals through three key features: 1) a flexible symbolic representation that is based on the relational data model; 2) a declarative logic programming language that is based on Datalog and supports recursion, aggregation, and negation; and 3) a framework for automatic and efficient differentiable reasoning that is based on the theory of provenance semirings. We evaluate Scallop on a suite of eight neurosymbolic applications from the literature. Our evaluation demonstrates that Scallop is capable of expressing algorithmic reasoning in diverse and challenging AI tasks, provides a succinct interface for machine learning programmers to integrate logical domain knowledge, and yields solutions that are comparable or superior to state-of-the-art mode
    
[^49]: 在在线评估下重新审视测试时间适应

    Revisiting Test Time Adaptation under Online Evaluation. (arXiv:2304.04795v1 [cs.LG])

    [http://arxiv.org/abs/2304.04795](http://arxiv.org/abs/2304.04795)

    本文提出了一种新颖的在线评估协议，该协议通过为较慢的方法提供更少的样本来惩罚它们，以更加现实的方式评估了测试时间适应（TTA）方法。广泛实验表明，当考虑推断速度时，简单快速的方法可以优于更复杂但较慢的方法。

    

    本文提出了一种新颖的在线评估协议，用于测试时间适应（TTA）方法，通过为较慢的方法提供更少的样本来惩罚它们。TTA方法利用测试时间的未标记数据来适应分布移位。虽然已经提出了许多有效的方法，但它们惊人的性能通常要以显着增加的计算预算为代价。当前的评估协议忽略了这种额外计算成本的影响，影响它们在实际中的适用性。为了解决这个问题，我们提出了一种更加现实的TTA方法评估协议，在这个协议中数据以恒定速率从数据流中在线接收，从而考虑到方法的适应速度。我们将我们提出的协议应用于多个数据集和场景中对多种TTA方法进行基准测试。广泛的实验表明，在考虑推断速度时，简单快速的方法可以优于更复杂但较慢的方法。

    This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) methods, which penalizes slower methods by providing them with fewer samples for adaptation. TTA methods leverage unlabeled data at test time to adapt to distribution shifts. Though many effective methods have been proposed, their impressive performance usually comes at the cost of significantly increased computation budgets. Current evaluation protocols overlook the effect of this extra computation cost, affecting their real-world applicability. To address this issue, we propose a more realistic evaluation protocol for TTA methods, where data is received in an online fashion from a constant-speed data stream, thereby accounting for the method's adaptation speed. We apply our proposed protocol to benchmark several TTA methods on multiple datasets and scenarios. Extensive experiments shows that, when accounting for inference speed, simple and fast approaches can outperform more sophisticated but slower
    
[^50]: 通过潜在意图从被动数据中进行强化学习

    Reinforcement Learning from Passive Data via Latent Intentions. (arXiv:2304.04782v1 [cs.LG])

    [http://arxiv.org/abs/2304.04782](http://arxiv.org/abs/2304.04782)

    本文提出了一种基于意图建模的强化学习方法，可以从被动数据中学习特征，并用于下游任务的价值预测。

    

    被动观察数据丰富而富有信息，然而当前强化学习方法很少能够利用该数据。本文提出了一种通过建模意图从被动数据中进行学习的方法，该方法通过衡量当智能体为实现特定任务而采取行动时未来结果的可能性如何变化来学习意图。我们提出了一个时差学习目标来学习意图，得到了一个类似于传统强化学习的算法，但是完全是从被动数据中学习得到的。通过优化该目标，我们的智能体可以同时从原始的观察数据中学习出状态、策略和环境下的可能结果。从理论和实验上看，该方法学习出的特征可用于下游任务的价值预测。

    Passive observational data, such as human videos, is abundant and rich in information, yet remains largely untapped by current RL methods. Perhaps surprisingly, we show that passive data, despite not having reward or action labels, can still be used to learn features that accelerate downstream RL. Our approach learns from passive data by modeling intentions: measuring how the likelihood of future outcomes change when the agent acts to achieve a particular task. We propose a temporal difference learning objective to learn about intentions, resulting in an algorithm similar to conventional RL, but which learns entirely from passive data. When optimizing this objective, our agent simultaneously learns representations of states, of policies, and of possible outcomes in an environment, all from raw observational data. Both theoretically and empirically, this scheme learns features amenable for value prediction for downstream tasks, and our experiments demonstrate the ability to learn from m
    
[^51]: 关于可解释人工智能在医疗保健领域的综述：为什么、 如何和何时？

    A Review on Explainable Artificial Intelligence for Healthcare: Why, How, and When?. (arXiv:2304.04780v1 [cs.LG])

    [http://arxiv.org/abs/2304.04780](http://arxiv.org/abs/2304.04780)

    本篇综述系统分析了可解释人工智能（XAI）在医疗保健领域的应用及其流行趋势，阐述了XAI的使用原因、如何使用以及何时使用及其影响，并给出了如何获得可信赖的AI的方法，对研究人员、临床医生和政策制定者有重要的指导作用。

    

    人工智能模型在医学领域中的应用越来越广泛。同时也出现了关于这些人工智能模型决策可解释性的担忧。本文系统分析了可解释人工智能（XAI），重点关注目前在医疗保健领域中使用的模型。按照系统性综述和元分析的首选报告项目（PRISMA）标准，检索了2012年1月1日至2022年2月2日期间发表的相关文章。本综述分析了XAI的流行趋势，并阐述了研究的主要方向。我们调查这些XAI模型的使用原因、如何使用以及何时使用以及其影响。我们对XAI方法进行了全面的研究，以及阐述了如何通过描述医疗领域的AI模型来获得可信赖的AI。对本文的讨论将有助于研究人员、临床医生和政策制定者深入了解XAI在医疗保健应用中的重要性。

    Artificial intelligence (AI) models are increasingly finding applications in the field of medicine. Concerns have been raised about the explainability of the decisions that are made by these AI models. In this article, we give a systematic analysis of explainable artificial intelligence (XAI), with a primary focus on models that are currently being used in the field of healthcare. The literature search is conducted following the preferred reporting items for systematic reviews and meta-analyses (PRISMA) standards for relevant work published from 1 January 2012 to 02 February 2022. The review analyzes the prevailing trends in XAI and lays out the major directions in which research is headed. We investigate the why, how, and when of the uses of these XAI models and their implications. We present a comprehensive examination of XAI methodologies as well as an explanation of how a trustworthy AI can be derived from describing AI models for healthcare fields. The discussion of this work will
    
[^52]: DDRF: 远程感知图像融合的去噪扩散模型

    DDRF: Denoising Diffusion Model for Remote Sensing Image Fusion. (arXiv:2304.04774v1 [cs.CV])

    [http://arxiv.org/abs/2304.04774](http://arxiv.org/abs/2304.04774)

    本研究将去噪扩散模型引入远程感知图像融合，采用条件注入调制模块（风格转移调制和小波调制）将风格信息和频率信息注入扩散UNet，从而生成高质量融合图像。

    

    最近，作为一种生成模型，去噪扩散模型因其强大的生成能力，在图像生成领域受到了广泛关注。然而，扩散模型在图像融合领域内研究不足。本文将扩散模型引入到图像融合领域，将图像融合任务视为图像到图像的转换，设计了两种不同的条件注入调制模块（即风格转移调制和小波调制），以将粗粒度的风格信息和细粒度的高频和低频信息注入扩散UNet中，从而生成融合图像。此外，我们还讨论了扩散模型在图像融合任务中的残差学习和训练目标的选择。基于定量和定性评估的广泛实验结果与基准的比较表明了最新技术成果和良好的泛化性能。

    Denosing diffusion model, as a generative model, has received a lot of attention in the field of image generation recently, thanks to its powerful generation capability. However, diffusion models have not yet received sufficient research in the field of image fusion. In this article, we introduce diffusion model to the image fusion field, treating the image fusion task as image-to-image translation and designing two different conditional injection modulation modules (i.e., style transfer modulation and wavelet modulation) to inject coarse-grained style information and fine-grained high-frequency and low-frequency information into the diffusion UNet, thereby generating fused images. In addition, we also discussed the residual learning and the selection of training objectives of the diffusion model in the image fusion task. Extensive experimental results based on quantitative and qualitative assessments compared with benchmarks demonstrates state-of-the-art results and good generalizatio
    
[^53]: 将机器学习中的公平性与公共卫生平等联系起来

    Connecting Fairness in Machine Learning with Public Health Equity. (arXiv:2304.04761v1 [cs.LG])

    [http://arxiv.org/abs/2304.04761](http://arxiv.org/abs/2304.04761)

    这篇论文总结了机器学习公平方面的划时代文献，并提出了一个框架，用于识别和缓解数据和模型中的偏差，强调在公共卫生领域需要公平和公正的机器学习模型。

    

    机器学习已成为公共卫生中至关重要的工具，有望提高人口健康、诊断、治疗选择和卫生系统效率。然而，数据和模型设计中的偏见可能导致某些受保护群体的不平等，并放大现有的医疗保健不平等。为了解决这一挑战，本研究总结了机器学习公平方面的划时代文献，并提出了一个框架，用于识别和缓解数据和模型中的偏差。该框架提供了指导，可以将公平性纳入典型的机器学习流程的不同阶段，如数据处理、模型设计、部署和评估。为了说明数据中偏见对机器学习模型的影响，我们提供了例子，展示了系统性偏见如何通过模型预测被放大。这些案例展示了如何使用该框架来预防这些偏见，并强调了在公共卫生领域需要公平和公正的机器学习模型。

    Machine learning (ML) has become a critical tool in public health, offering the potential to improve population health, diagnosis, treatment selection, and health system efficiency. However, biases in data and model design can result in disparities for certain protected groups and amplify existing inequalities in healthcare. To address this challenge, this study summarizes seminal literature on ML fairness and presents a framework for identifying and mitigating biases in the data and model. The framework provides guidance on incorporating fairness into different stages of the typical ML pipeline, such as data processing, model design, deployment, and evaluation. To illustrate the impact of biases in data on ML models, we present examples that demonstrate how systematic biases can be amplified through model predictions. These case studies suggest how the framework can be used to prevent these biases and highlight the need for fair and equitable ML models in public health. This work aims
    
[^54]: 建立高效和有表现力的三维等变图神经网络的新视角

    A new perspective on building efficient and expressive 3D equivariant graph neural networks. (arXiv:2304.04757v1 [cs.LG])

    [http://arxiv.org/abs/2304.04757](http://arxiv.org/abs/2304.04757)

    本文提出了一个本地等同性的三维等变图神经网络层次结构来评估等变GNN的表达能力，并提出局部亚结构编码（LSE）和帧转换编码（FTE）两个关键模块。LEFTNet有效地应用了这些模块并在分子属性预测任务中实现了最先进的性能。

    

    几何深度学习使得在建模三维物体时可以编码物理对称性。本文提出了一个本地等同性的三维等变图神经网络层次结构来评估等变GNN的表达能力，并调查从局部块代表全局几何信息的过程。研究结果表明，两个关键模块——局部亚结构编码（LSE）和帧转换编码（FTE）是设计高效和有表现力几何GNN的基础。为了证明我们理论的适用性，我们提出了LEFTNet，它有效地实现了这些模块，并在标量值和矢量值分子属性预测任务中实现了最先进的性能。此外，我们还指出等变图神经网络的未来发展空间。

    Geometric deep learning enables the encoding of physical symmetries in modeling 3D objects. Despite rapid progress in encoding 3D symmetries into Graph Neural Networks (GNNs), a comprehensive evaluation of the expressiveness of these networks through a local-to-global analysis lacks today. In this paper, we propose a local hierarchy of 3D isomorphism to evaluate the expressive power of equivariant GNNs and investigate the process of representing global geometric information from local patches. Our work leads to two crucial modules for designing expressive and efficient geometric GNNs; namely local substructure encoding (LSE) and frame transition encoding (FTE). To demonstrate the applicability of our theory, we propose LEFTNet which effectively implements these modules and achieves state-of-the-art performance on both scalar-valued and vector-valued molecular property prediction tasks. We further point out the design space for future developments of equivariant graph neural networks. O
    
[^55]: 一种新的道路车辆质量问题诊断的两层因果推断框架

    A Novel Two-level Causal Inference Framework for On-road Vehicle Quality Issues Diagnosis. (arXiv:2304.04755v1 [cs.AI])

    [http://arxiv.org/abs/2304.04755](http://arxiv.org/abs/2304.04755)

    该论文提出了一种新的两级因果推断框架，利用因果机器学习可以加速汽车行业中处理车辆质量问题的全周期，从而更快地隔离根本原因、确定治疗措施并评估其有效性。

    

    在汽车行业中，处理使用中车辆质量问题的全周期可能需要数周进行调查。这个过程涉及到隔离根本原因、定义和实施适当的治疗措施，以及在必要时改进治疗措施。主要问题是缺乏一种系统性的方法来确定因果关系、评估治疗效果，并在当前治疗被认为无效时指导下一个可行的治疗措施。本文将展示如何利用因果机器学习（ML）加速这些过程。使用从路上车辆收集的真实数据集来演示所提出的框架。还将讨论车辆质量应用的开放挑战。

    In the automotive industry, the full cycle of managing in-use vehicle quality issues can take weeks to investigate. The process involves isolating root causes, defining and implementing appropriate treatments, and refining treatments if needed. The main pain-point is the lack of a systematic method to identify causal relationships, evaluate treatment effectiveness, and direct the next actionable treatment if the current treatment was deemed ineffective. This paper will show how we leverage causal Machine Learning (ML) to speed up such processes. A real-word data set collected from on-road vehicles will be used to demonstrate the proposed framework. Open challenges for vehicle quality applications will also be discussed.
    
[^56]: $\textit{e-Uber}$：一个基于共享经济的平台，实现基于电动汽车的拼车和能量共享

    $\textit{e-Uber}$: A Crowdsourcing Platform for Electric Vehicle-based Ride- and Energy-sharing. (arXiv:2304.04753v1 [cs.AI])

    [http://arxiv.org/abs/2304.04753](http://arxiv.org/abs/2304.04753)

    本论文提出了一个名为e-Uber的众包平台，该平台利用电动汽车的日益普及，通过V2G和BST共同实现拼车和能量共享。平台利用强化学习，基于CMAB算法实现个性化任务推荐系统(CARS)，并利用反向拍卖机制选择最优出价。

    

    最近，基于共享经济的商业模式在交通和住宿领域取得了成功，如Uber和Airbnb。人们越来越有兴趣将此模式应用于能源系统，包括点对点（P2P）能源交易和基于电动汽车（EV）的车对网（V2G）、车对家（V2H）、车对车（V2V）以及电池更换技术(BST)等方式。本文利用电动汽车的日益普及，实现了一个名为e-Uber的众包平台，通过V2G和BST共同实现拼车和能量共享。e-Uber利用空间众包、强化学习和反向拍卖理论实现。具体而言，本平台利用强化学习了解司机对不同拼车和能量共享任务的偏好。基于这些偏好，通过基于CMAB的算法进行任务推荐系统(CARS)的个性化推荐。司机在愿意执行的任务上进行竞标，而反向拍卖机制则选择最优出价。计算实验表明，所提出的系统在匹配效率、时间效率和用户满意度方面具有优越性。

    The sharing-economy-based business model has recently seen success in the transportation and accommodation sectors with companies like Uber and Airbnb. There is growing interest in applying this model to energy systems, with modalities like peer-to-peer (P2P) Energy Trading, Electric Vehicles (EV)-based Vehicle-to-Grid (V2G), Vehicle-to-Home (V2H), Vehicle-to-Vehicle (V2V), and Battery Swapping Technology (BST). In this work, we exploit the increasing diffusion of EVs to realize a crowdsourcing platform called e-Uber that jointly enables ride-sharing and energy-sharing through V2G and BST. e-Uber exploits spatial crowdsourcing, reinforcement learning, and reverse auction theory. Specifically, the platform uses reinforcement learning to understand the drivers' preferences towards different ride-sharing and energy-sharing tasks. Based on these preferences, a personalized list is recommended to each driver through CMAB-based Algorithm for task Recommendation System (CARS). Drivers bid on 
    
[^57]: DeepHive: 一种用于自动化发现基于群体优化策略的多智能体强化学习方法

    DeepHive: A multi-agent reinforcement learning approach for automated discovery of swarm-based optimization policies. (arXiv:2304.04751v1 [cs.AI])

    [http://arxiv.org/abs/2304.04751](http://arxiv.org/abs/2304.04751)

    本文提出了一种多智能体强化学习方法，用于设计群体优化器以全局优化昂贵黑盒函数，并在各类基准优化函数上进行测试，结果表明具有优越的性能和所需的缩放。

    

    本文提出了一种设计群体优化器来全局优化昂贵黑盒函数的方法。在这种方法中，找到高效的优化器的问题被视为一个强化学习问题，目标是找到需要少数函数评估即可收敛到全局最优的优化策略。群体中每个智能体的状态被定义为其在设计空间内的当前位置和函数值，并且这些智能体学会采取最大化奖励的有利行动，该奖励基于目标函数的最终值。该方法在各类基准优化函数上进行了测试，并与其他全局优化策略的性能进行了比较。此外，还研究了更改代理人数量以及训练代理人的泛化能力的影响。结果表明，与其他优化器相比，该方法具有优越的性能和所需的缩放。

    We present an approach for designing swarm-based optimizers for the global optimization of expensive black-box functions. In the proposed approach, the problem of finding efficient optimizers is framed as a reinforcement learning problem, where the goal is to find optimization policies that require a few function evaluations to converge to the global optimum. The state of each agent within the swarm is defined as its current position and function value within a design space and the agents learn to take favorable actions that maximize reward, which is based on the final value of the objective function. The proposed approach is tested on various benchmark optimization functions and compared to the performance of other global optimization strategies. Furthermore, the effect of changing the number of agents, as well as the generalization capabilities of the trained agents are investigated. The results show superior performance compared to the other optimizers, desired scaling when the numb
    
[^58]: 交互感知提示的零样本时空动作检测

    Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection. (arXiv:2304.04688v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.04688](http://arxiv.org/abs/2304.04688)

    提出了一种采用预训练的视觉-语言模型和交互模块进行交互感知提示的零样本时空动作检测方法，优化了视觉-语言特征的对齐，实现了更好的结果。

    

    空间-时间动作检测的目标是确定每个人在视频中动作发生的时间和位置，并对相应的动作类别进行分类。大多数现有方法采用全监督学习，需要大量的训练数据，因此难以实现零样本学习。本文提出利用预训练的视觉-语言模型提取代表性的图像和文本特征，并通过不同的交互模块建模这些特征之间的关系以得到交互特征。此外，利用这个特征提示每个标签以获取更合适的文本特征。最后，我们计算每个标签的交互特征和文本特征之间的相似度，以确定动作类别。在J-HMDB和UCF101-24数据集上的实验表明，所提出的交互模块和提示使视觉-语言特征更加对齐，从而实现了更好的结果。

    The goal of spatial-temporal action detection is to determine the time and place where each person's action occurs in a video and classify the corresponding action category. Most of the existing methods adopt fully-supervised learning, which requires a large amount of training data, making it very difficult to achieve zero-shot learning. In this paper, we propose to utilize a pre-trained visual-language model to extract the representative image and text features, and model the relationship between these features through different interaction modules to obtain the interaction feature. In addition, we use this feature to prompt each label to obtain more appropriate text features. Finally, we calculate the similarity between the interaction feature and the text feature for each label to determine the action category. Our experiments on J-HMDB and UCF101-24 datasets demonstrate that the proposed interaction module and prompting make the visual-language features better aligned, thus achievi
    
[^59]: 可能大致正确联邦学习

    Probably Approximately Correct Federated Learning. (arXiv:2304.04641v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.04641](http://arxiv.org/abs/2304.04641)

    本文提出了FedPAC框架，利用PAC学习理论推导出一个解析解，可以保证FL之间隐私、效用和效率的最佳权衡。

    

    联邦学习是一种新的分布式学习范例，其主要支柱为隐私、效用和效率。现有研究表明，同时实现无穷小隐私泄露、效用损失和效率是不可能的。因此，在设计联邦学习算法时，如何找到最佳权衡解决方案是关键考虑因素。一种常见的方法是将权衡问题视为多目标优化问题，即目标是在约束隐私泄露不超过预定值的情况下最小化效用损失和效率降低。然而，现有的多目标优化框架非常耗时，并且不能保证帕累托前沿的存在性，这激励我们寻求一种方法，将多目标问题转化为单目标问题，因为它更高效、更容易被解决。为此，本文提出了FedPAC，这是一个统一的框架，利用PAC学习理论推导出一个解析解，可以保证FL之间隐私、效用和效率的最佳权衡。具体而言，我们首先将FL问题公式化为一个二分类任务，然后设计一个自适应FL算法，动态调整每个客户端的采样比率，以平衡全局和本地的隐私-效用权衡，最后证明FedPAC可以在温和的假设下高概率地实现最优的隐私-效用权衡。基准数据集上的大量实验证明了我们提出的FedPAC框架的功效和效率。

    Federated learning (FL) is a new distributed learning paradigm, with privacy, utility, and efficiency as its primary pillars. Existing research indicates that it is unlikely to simultaneously attain infinitesimal privacy leakage, utility loss, and efficiency. Therefore, how to find an optimal trade-off solution is the key consideration when designing the FL algorithm. One common way is to cast the trade-off problem as a multi-objective optimization problem, i.e., the goal is to minimize the utility loss and efficiency reduction while constraining the privacy leakage not exceeding a predefined value. However, existing multi-objective optimization frameworks are very time-consuming, and do not guarantee the existence of the Pareto frontier, this motivates us to seek a solution to transform the multi-objective problem into a single-objective problem because it is more efficient and easier to be solved. To this end, in this paper, we propose FedPAC, a unified framework that leverages PAC l
    
[^60]: 神经流形非均匀B样条流

    Neural Diffeomorphic Non-uniform B-spline Flows. (arXiv:2304.04555v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.04555](http://arxiv.org/abs/2304.04555)

    提出了一种具有高效参数化和解析逆变换的至少二次连续可微、双Lipschitz连续的非均匀B样条流形，能够在密度估计和图像生成任务中表现出具有竞争力的结果。

    

    正则化流成功地将复杂的概率分布建模为简单基本分布的可逆变换。然而，有时候需要更多。在物理中计算能量和力要求变换的二阶导数是良好定义和连续的，平滑正则化流采用无限可微变换，但以缓慢的非解析逆变换的代价。本文提出了至少二次连续可微且双Lipschitz连续的非均匀B样条流形，实现了高效的参数化和基于微分同胚的解析逆变换。首先，我们研究了Ck-2微分同胚的非均匀k阶B样条变换的充分条件。然后，我们推导了符合充分条件的非均匀立方B样条变换的解析逆变换。实验表明，与现有最先进的方法相比，所提出的方法在密度估计和图像生成任务上获得了具有竞争力的结果。

    Normalizing flows have been successfully modeling a complex probability distribution as an invertible transformation of a simple base distribution. However, there are often applications that require more than invertibility. For instance, the computation of energies and forces in physics requires the second derivatives of the transformation to be well-defined and continuous. Smooth normalizing flows employ infinitely differentiable transformation, but with the price of slow non-analytic inverse transforms. In this work, we propose diffeomorphic non-uniform B-spline flows that are at least twice continuously differentiable while bi-Lipschitz continuous, enabling efficient parametrization while retaining analytic inverse transforms based on a sufficient condition for diffeomorphism. Firstly, we investigate the sufficient condition for Ck-2-diffeomorphic non-uniform kth-order B-spline transformations. Then, we derive an analytic inverse transformation of the non-uniform cubic B-spline tran
    
[^61]: 实现队列智能化：一种针对电子病历分析的通用群体表示学习框架

    Toward Cohort Intelligence: A Universal Cohort Representation Learning Framework for Electronic Health Record Analysis. (arXiv:2304.04468v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.04468](http://arxiv.org/abs/2304.04468)

    提出了一种通用的COhort Representation lEarning（CORE）框架，用于增强EHR表示学习，支持针对不同队列的特征进行可解释性分析。

    

    电子病历（EHR）是从临床常规护理中生成的，记录了广泛的病人人群有价值的信息，为改善临床实践中的病人管理和干预策略提供了丰富的机会。为了利用EHR数据的巨大潜力，机器学习中流行的EHR数据分析范式是EHR表示学习，它首先利用单个病人的EHR数据通过一个主干学习信息丰富的表示，并支持建立在这些表示的多样化的医疗下游任务。然而，这种范式无法深入分析病人的相关性，通常在临床实践中被称为队列研究。具体来说，同一队列中的病人倾向于具有相似的特征，表明他们在医疗条件（如症状或疾病）方面具有相似之处。在本文中，我们提出了一种通用COhort Representation lEarning (CORE)框架来增强EHR表示学习，通过使用队列表示学习算法，对群体信息进行建模并支持针对不同队列的特征进行可解释性分析。

    Electronic Health Records (EHR) are generated from clinical routine care recording valuable information of broad patient populations, which provide plentiful opportunities for improving patient management and intervention strategies in clinical practice. To exploit the enormous potential of EHR data, a popular EHR data analysis paradigm in machine learning is EHR representation learning, which first leverages the individual patient's EHR data to learn informative representations by a backbone, and supports diverse health-care downstream tasks grounded on the representations. Unfortunately, such a paradigm fails to access the in-depth analysis of patients' relevance, which is generally known as cohort studies in clinical practice. Specifically, patients in the same cohort tend to share similar characteristics, implying their resemblance in medical conditions such as symptoms or diseases. In this paper, we propose a universal COhort Representation lEarning (CORE) framework to augment EHR
    
[^62]: H2RBox-v2：通过对称学习提高基于HBox监督的有向物体检测

    H2RBox-v2: Boosting HBox-supervised Oriented Object Detection via Symmetric Learning. (arXiv:2304.04403v1 [cs.CV])

    [http://arxiv.org/abs/2304.04403](http://arxiv.org/abs/2304.04403)

    H2RBox-v2是第一个将对称学习应用于基于HBox监督的有向物体检测，其强化了水平注释和旋转注释之间的联系，在多个基准测试中实现了最先进的性能。

    

    随着对于自动驾驶和遥感等有向物体检测需求的日益增长，有向注释变得非常费力。为了充分利用现有的水平注释数据集并降低注释成本，已经提出了一种弱监督检测器H2RBox，用于从水平框Box中学习旋转框RBox，并受到了广泛关注。本文介绍了H2RBox-v2的新版本，以进一步弥合HBox监督和RBox监督的有向物体检测之间的差距。通过我们的理论分析，利用翻转和旋转一致性来开发轴对称性是可行的，H2RBox-v2则采用与H2RBox类似的弱监督分支，并嵌入一个新颖的自监督分支，它可以从对象图像中固有的对称性中学习方向。通过处理周边问题的模块（例如角周期性），实现了一种稳定而有效的解决方案。据我们所知，H2RBox-v2是第一个将对称学习应用于基于HBox监督的有向物体检测，并在多个基准测试中实现了最先进的性能。

    With the increasing demand for oriented object detection e.g. in autonomous driving and remote sensing, the oriented annotation has become a labor-intensive work. To make full use of existing horizontally annotated datasets and reduce the annotation cost, a weakly-supervised detector H2RBox for learning the rotated box (RBox) from the horizontal box (HBox) has been proposed and received great attention. This paper presents a new version, H2RBox-v2, to further bridge the gap between HBox-supervised and RBox-supervised oriented object detection. While exploiting axisymmetry via flipping and rotating consistencies is available through our theoretical analysis, H2RBox-v2, using a weakly-supervised branch similar to H2RBox, is embedded with a novel self-supervised branch that learns orientations from the symmetry inherent in the image of objects. Complemented by modules to cope with peripheral issues, e.g. angular periodicity, a stable and effective solution is achieved. To our knowledge, H
    
[^63]: 基于NeRF技术的卫星图像表面重建

    NeRF applied to satellite imagery for surface reconstruction. (arXiv:2304.04133v1 [cs.CV])

    [http://arxiv.org/abs/2304.04133](http://arxiv.org/abs/2304.04133)

    本文提出了Sat-NeRF模型，能够从少量的卫星图像集合中合成新的视角，并准确地估计场景表面的高程。

    

    本文提出了Sat-NeRF模型，是对最近引入的S-NeRF模型的修改实现。该模型能够从稀疏的卫星图像集合中合成新的视角，同时考虑到图片中的光照变化。训练好的模型还能够精确地估计场景表面的高程，这对卫星观测应用非常有帮助。S-NeRF方法改进了标准的NeRF方法，将辐射强度考虑为高反射率和入射辐照度的函数。这两个量都是模型的全连接神经网络枝条的输出，而后者则被视为来自太阳的直接光线和来自天空的漫反射颜色函数。该实现基于用缩放-裁剪技术增强的卫星图像数据集。对NeRF进行了超参数研究，得出了一些有趣的观察结果。

    We present Sat-NeRF, a modified implementation of the recently introduced Shadow Neural Radiance Field (S-NeRF) model. This method is able to synthesize novel views from a sparse set of satellite images of a scene, while accounting for the variation in lighting present in the pictures. The trained model can also be used to accurately estimate the surface elevation of the scene, which is often a desirable quantity for satellite observation applications. S-NeRF improves on the standard Neural Radiance Field (NeRF) method by considering the radiance as a function of the albedo and the irradiance. Both these quantities are output by fully connected neural network branches of the model, and the latter is considered as a function of the direct light from the sun and the diffuse color from the sky. The implementations were run on a dataset of satellite images, augmented using a zoom-and-crop technique. A hyperparameter study for NeRF was carried out, leading to intriguing observations on the 
    
[^64]: TC-VAE：揭示数据生成因素中的未知分布数据

    TC-VAE: Uncovering Out-of-Distribution Data Generative Factors. (arXiv:2304.04103v1 [cs.LG])

    [http://arxiv.org/abs/2304.04103](http://arxiv.org/abs/2304.04103)

    本文提出了一种基于总相关性的生成模型TC-VAE，可以揭示数据生成因素中的未知分布数据，在处理具有不平衡生成因素的数据集上表现优秀。

    

    揭示数据生成因素是解决解缠结学习的最终目标。本文提出了一种生成模型-TC-VAE，它可以基于所学的潜在表征和输入数据之间的总相关性下界进行优化，从而发现不在数据集中显式出现的变化因素。我们分析了在使用具有不平衡的生成因素数据集时，所提出的模型的效果，并在定量和定性实验中表明了TC-VAE的优越性。

    Uncovering data generative factors is the ultimate goal of disentanglement learning. Although many works proposed disentangling generative models able to uncover the underlying generative factors of a dataset, so far no one was able to uncover OOD generative factors (i.e., factors of variations that are not explicitly shown on the dataset). Moreover, the datasets used to validate these models are synthetically generated using a balanced mixture of some predefined generative factors, implicitly assuming that generative factors are uniformly distributed across the datasets. However, real datasets do not present this property. In this work we analyse the effect of using datasets with unbalanced generative factors, providing qualitative and quantitative results for widely used generative models. Moreover, we propose TC-VAE, a generative model optimized using a lower bound of the joint total correlation between the learned latent representations and the input data. We show that the proposed
    
[^65]: 推荐系统的图协作信号去噪与增强

    Graph Collaborative Signals Denoising and Augmentation for Recommendation. (arXiv:2304.03344v1 [cs.IR])

    [http://arxiv.org/abs/2304.03344](http://arxiv.org/abs/2304.03344)

    本文提出了一种新的图邻接矩阵，它包括了用户-用户和项目-项目的相关性，以及一个经过适当设计的用户-项目交互矩阵，并通过预训练和top-K采样增强了用户-项目交互矩阵，以更好地适应所有用户的需求。

    

    图协作过滤（GCF）是捕捉推荐系统中高阶协同信号的流行技术。然而，GCF的双向邻接矩阵，其定义了基于用户-项目交互进行聚合的邻居，对于有大量交互但不足的用户/项目来说可能是嘈杂的。此外，邻接矩阵忽略了用户-用户和项目-项目之间的相关性，这可能限制了聚合的有益邻居的范围。在这项工作中，我们提出了一种新的图邻接矩阵，它包括了用户-用户和项目-项目的相关性，以及一个经过适当设计的用户-项目交互矩阵，以平衡所有用户之间的交互数量。为了实现这一点，我们预先训练了一个基于图的推荐方法来获得用户/项目嵌入，然后通过top-K采样增强了用户-项目交互矩阵。我们还增强了对称的用户-用户和项目-项目相关组件，以更好地适应所有用户的需求。

    Graph collaborative filtering (GCF) is a popular technique for capturing high-order collaborative signals in recommendation systems. However, GCF's bipartite adjacency matrix, which defines the neighbors being aggregated based on user-item interactions, can be noisy for users/items with abundant interactions and insufficient for users/items with scarce interactions. Additionally, the adjacency matrix ignores user-user and item-item correlations, which can limit the scope of beneficial neighbors being aggregated.  In this work, we propose a new graph adjacency matrix that incorporates user-user and item-item correlations, as well as a properly designed user-item interaction matrix that balances the number of interactions across all users. To achieve this, we pre-train a graph-based recommendation method to obtain users/items embeddings, and then enhance the user-item interaction matrix via top-K sampling. We also augment the symmetric user-user and item-item correlation components to th
    
[^66]: BotTriNet: 一种基于度量学习的社交机器人检测统一高效的嵌入式框架

    BotTriNet: A Unified and Efficient Embedding for Social Bots Detection via Metric Learning. (arXiv:2304.03144v1 [cs.AI])

    [http://arxiv.org/abs/2304.03144](http://arxiv.org/abs/2304.03144)

    BOTTRINET基于文本内容检测机器人，并设计了三元组网络以提高分类性能。在真实世界数据集CRESCI2017上，系统表现最好。

    

    在在线社交网络中，快速准确地发现机器人账户以防止它们侵犯和骚扰真实用户是一个持久受欢迎的话题。我们提出了一种叫作BOTTRINET的统一嵌入式框架，它利用账户发布的文本内容检测机器人，基于的假设是上下文自然地揭示账户个性和习惯。如果系统能够使用嵌入技术有效地提取与机器人相关的信息，那么内容就是丰富和有价值的。除了生成词、句和账户嵌入的一般嵌入式框架外，我们设计了一个三元组网络来调整原始嵌入（由传统的自然语言处理技术生成）以获得更好的分类性能。我们在一个真实世界的数据集CRESCI2017上评估了检测准确性和F1得分，该数据集包括三个机器人账户类别和五个机器人样本集。我们的系统在两个内容集上实现了最高的平均准确性98.34%和F1得分97.99%。

    A persistently popular topic in online social networks is the rapid and accurate discovery of bot accounts to prevent their invasion and harassment of genuine users. We propose a unified embedding framework called BOTTRINET, which utilizes textual content posted by accounts for bot detection based on the assumption that contexts naturally reveal account personalities and habits. Content is abundant and valuable if the system efficiently extracts bot-related information using embedding techniques. Beyond the general embedding framework that generates word, sentence, and account embeddings, we design a triplet network to tune the raw embeddings (produced by traditional natural language processing techniques) for better classification performance. We evaluate detection accuracy and f1score on a real-world dataset CRESCI2017, comprising three bot account categories and five bot sample sets. Our system achieves the highest average accuracy of 98.34% and f1score of 97.99% on two content-inte
    
[^67]: 神经发生、神经达尔文主义和物种进化可否作为演化深度神经网络创建的灵感来源?

    Is it conceivable that neurogenesis, neural Darwinism, and species evolution could all serve as inspiration for the creation of evolutionary deep neural networks?. (arXiv:2304.03122v1 [cs.NE])

    [http://arxiv.org/abs/2304.03122](http://arxiv.org/abs/2304.03122)

    本文探讨了神经发生、神经达尔文主义和物种进化如何启发演化深度神经网络的创作，并强调了在DNNs演化中dropout方法与神经发生的联系。

    

    深度神经网络(DNNs)是使用人工神经网络构建的，是能够从数据中学习的机器学习方法，在各种应用中被广泛使用。DNNs主要是手工构建的，通常包含大量的层数。本文强调了我们所说的二维脑部进化的重要性，以及它如何启发二维DNN演化建模。我们还强调了在DNNs正则化中广泛使用的dropout方法与大脑神经发生的联系，以及这些概念如何有益于DNNs的演化。该论文总结了几个增强自动构建DNNs的建议。

    Deep Neural Networks (DNNs) are built using artificial neural networks. They are part of machine learning methods that are capable of learning from data that have been used in a wide range of applications. DNNs are mainly handcrafted and they usually contain numerous layers. Research frontier has emerged that concerns automated construction of DNNs via evolutionary algorithms. This paper emphasizes the importance of what we call two-dimensional brain evolution and how it can inspire two dimensional DNN evolutionary modeling. We also highlight the connection between the dropout method which is widely-used in regularizing DNNs and neurogenesis of the brain, and how these concepts could benefit DNNs evolution.The paper concludes with several recommendations for enhancing the automatic construction of DNNs.
    
[^68]: 重新审视带有无法回答的反事实情景的密集检索

    Revisiting Dense Retrieval with Unanswerable Counterfactuals. (arXiv:2304.03031v1 [cs.AI])

    [http://arxiv.org/abs/2304.03031](http://arxiv.org/abs/2304.03031)

    本文观察到基于DPR的最近的密集检索模型经常将无法回答的反事实情景排名高于可回答的原始情景，提出了一种新颖的用于段落检索的表示学习方法PiCL。

    

    在开放领域问答（ODQA）中，检索器-阅读器框架很受欢迎，其中检索器从大型语料库中为阅读器抽取一组相关的候选段落。这种方法背后的一个关键假设是，从检索器得到的高相关性分数可能表明从阅读器获取答案的可能性很高，这意味着检索到的段落很可能包含给定问题的答案。我们在本研究中实证驳斥了这种观点，并观察到基于DPR的最近的密集检索模型经常将无法回答的反事实情景排名高于可回答的原始情景。为了解决密集检索中这种对答案无感知的问题，我们寻求使用反事实样本作为额外的训练资源，以更好地同步DPR的相关性测量和问题-段落对的可答性。具体地，我们提出了反事实Pivoting对比学习（PiCL），这是一种新颖的用于段落检索的表示学习方法。

    The retriever-reader framework is popular for open-domain question answering (ODQA), where a retriever samples for the reader a set of relevant candidate passages from a large corpus. A key assumption behind this method is that high relevance scores from the retriever likely indicate high answerability from the reader, which implies a high probability that the retrieved passages contain answers to a given question. In this work, we empirically dispel this belief and observe that recent dense retrieval models based on DPR often rank unanswerable counterfactual passages higher than their answerable original passages. To address such answer-unawareness in dense retrievers, we seek to use counterfactual samples as additional training resources to better synchronize the relevance measurement of DPR with the answerability of question-passage pairs. Specifically, we present counterfactually-Pivoting Contrastive Learning (PiCL), a novel representation learning approach for passage retrieval th
    
[^69]: AutoRL超参数景观

    AutoRL Hyperparameter Landscapes. (arXiv:2304.02396v1 [cs.LG])

    [http://arxiv.org/abs/2304.02396](http://arxiv.org/abs/2304.02396)

    本文提出了一种方法，在训练期间多次建立和分析AutoRL超参数的景观，证明代表算法（DQN和SAC）在不同环境下的超参数景观会随时间而变化。

    

    强化学习（RL）在取得令人瞩目成果的同时，其超参数对性能的影响限制了其应用范围。这经常使得在实践中难以获得良好的结果。自动化RL（AutoRL）解决了这个难题，但有关超参数优化（HPO）方法在搜索最佳配置时所遍历的超参数景观动态变化的信息很少。鉴于现有AutoRL方法动态调整超参数配置的情况，我们提出了一种方法，在训练期间不仅在一个时间点，而且在多个时间点上建立和分析这些超参数景观。针对关于这种动态AutoRL方法合法性的一个重要开放问题，我们提供了充分的证据，表明在不同种类的环境（Cartpole和Pendulum）中，来自RL文献的代表算法（DQN和SAC）的超参数景观会随时间而强烈变化。

    Although Reinforcement Learning (RL) has shown to be capable of producing impressive results, its use is limited by the impact of its hyperparameters on performance. This often makes it difficult to achieve good results in practice. Automated RL (AutoRL) addresses this difficulty, yet little is known about the dynamics of the hyperparameter landscapes that hyperparameter optimization (HPO) methods traverse in search of optimal configurations. In view of existing AutoRL approaches dynamically adjusting hyperparameter configurations, we propose an approach to build and analyze these hyperparameter landscapes not just for one point in time but at multiple points in time throughout training. Addressing an important open question on the legitimacy of such dynamic AutoRL approaches, we provide thorough empirical evidence that the hyperparameter landscapes strongly vary over time across representative algorithms from RL literature (DQN and SAC) in different kinds of environments (Cartpole and
    
[^70]: 大型语言模型作为钥匙：用GPT解密材料科学的秘密。

    Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v1 [cs.CL])

    [http://arxiv.org/abs/2304.02213](http://arxiv.org/abs/2304.02213)

    本文介绍了一个新的自然语言处理任务——结构化信息推理（SIS），利用GPT-3模型能够准确提取材料科学设备层面的信息，并通过实验预测PCE和反向预测参数，展示了大型语言模型在材料学中的巨大潜力。

    

    本文介绍了一个新的自然语言处理（NLP）任务——结构化信息推理（SIS），以解决材料科学设备层面信息提取的复杂性。我们使用现有的钙钛矿太阳能电池FAIR数据集对GPT-3进行微调，获得了91.8 F1得分，并更新了数据集，包括迄今为止所有相关科学论文。所生成的数据集已被格式化和标准化，使得它可以直接作为后续数据分析的输入。这个特性将使材料科学家通过选择高质量的领域评论文章来开发其自己的模型。此外，我们设计了实验来预测PCE和反向预测参数，并获得了与DFT相当的性能，这证明了大型语言模型能够像材料学家一样评判材料和设计新材料。

    This article presents a new NLP task called structured information inference (SIS) to address the complexities of information extraction at the device level in materials science. We accomplished this task by finetuning GPT-3 on a exsiting perovskite solar cell FAIR dataset with 91.8 F1-score and we updated the dataset with all related scientific papers up to now. The produced dataset is formatted and normalized, enabling its direct utilization as input in subsequent data analysis. This feature will enable materials scientists to develop their own models by selecting high-quality review papers within their domain. Furthermore, we designed experiments to predict PCE and reverse-predict parameters and obtained comparable performance with DFT, which demonstrates the potential of large language models to judge materials and design new materials like a materials scientist.
    
[^71]: EGC: 一种通过单一能量模型生成与分类图像的方法

    EGC: Image Generation and Classification via a Single Energy-Based Model. (arXiv:2304.02012v1 [cs.CV])

    [http://arxiv.org/abs/2304.02012](http://arxiv.org/abs/2304.02012)

    EGC是一种使用单个神经网络在图像分类和图像生成任务中实现卓越性能的方法，可以较好地生成出高质量图像，并在多项数据集上实现了领先的分类结果。

    

    使用相同的网络参数学习图像分类和生成图像是一个具有挑战性的问题。最近的先进方法在一项任务上表现良好，但在另一项任务上却表现不佳。本文引入了一种名为EGC的基于能量的分类器和生成器，它可以使用单个神经网络在两个任务中实现卓越性能。与传统的分类器输出给定图像的标签（即条件分布$p(y|\mathbf{x})$）不同，EGC的前向传递器是一个分类器，它输出一个联合分布$p(\mathbf{x},y)$，在后向传递器中通过边缘化标签$y$实现生成器。在前向传递中，估计给定噪声图像的能量和分类概率，而在后向传递中，通过估计得分函数对其进行去噪。EGC在ImageNet-1k、CelebA-HQ和LSUN Church上实现了与最先进方法相当的生成结果，同时在CIFAR-10、CIFAR-100和ImageNet-1k上实现了最先进的分类结果。

    Learning image classification and image generation using the same set of network parameters is a challenging problem. Recent advanced approaches perform well in one task often exhibit poor performance in the other. This work introduces an energy-based classifier and generator, namely EGC, which can achieve superior performance in both tasks using a single neural network. Unlike a conventional classifier that outputs a label given an image (i.e., a conditional distribution $p(y|\mathbf{x})$), the forward pass in EGC is a classifier that outputs a joint distribution $p(\mathbf{x},y)$, enabling an image generator in its backward pass by marginalizing out the label $y$. This is done by estimating the energy and classification probability given a noisy image in the forward pass, while denoising it using the score function estimated in the backward pass. EGC achieves competitive generation results compared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUN Church, while achi
    
[^72]: 大型语言模型综述

    A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])

    [http://arxiv.org/abs/2303.18223](http://arxiv.org/abs/2303.18223)

    本文综述了大型语言模型的研究历程以及最近的预训练语言模型(PLMs)，并强调模型扩展将带来性能改进和特殊能力的发掘。

    

    语言本质上是一个由语法规则控制的复杂精细的人类表达系统，对于开发理解和掌握语言的能力的AI算法来说是一项重大挑战。作为主要方法之一，语言建模在过去二十年里广泛研究用于语言理解和生成，从统计语言模型演化为神经语言模型。最近，通过在大规模语料库上预训练Transformer模型，提出了预训练语言模型（PLMs），在解决各种NLP任务方面显示出强大的能力。由于研究人员发现模型缩放可以导致性能改进，他们进一步通过增加模型规模来研究缩放效应，有趣的是，当参数规模超过一定水平时，这些扩大的语言模型不仅可以实现显着的性能提升，而且还显示出一些小规模语言模型所没有的特殊能力。

    Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
    
[^73]: Queer In AI:基于社区参与的AI案例研究

    Queer In AI: A Case Study in Community-Led Participatory AI. (arXiv:2303.16972v1 [cs.CY])

    [http://arxiv.org/abs/2303.16972](http://arxiv.org/abs/2303.16972)

    Queer in AI是一个基于社区参与的AI设计案例研究，通过拒绝等级制度而选择去中心化，在酷儿社群内部建立了援助和项目，同时努力改变酷儿社群外的参与者和机构。通过培育AI参与文化，欢迎和赋权边缘化参与者，为AI的参与式设计做出了更广泛的贡献。

    

    本文以Queer in AI为案例研究，探讨社区参与式AI设计的实践。我们分析了社区参与设计和交叉性原则如何在多年里在这个社群中萌芽和塑造了其项目。本文探讨了该组织在此过程中面临的不同挑战，审视了该组织在实现参与性与交叉性原则方面的不足，并评估了该组织的影响。Queer in AI通过拒绝等级制度而选择去中心化，通过将援助和项目建设建立在酷儿社群内部、由酷儿社群内成员来负责的方式，以及努力改变酷儿社群外的参与者和机构，为参与式方法的从业者和理论家提供了重要的经验和见解。最后，我们推测像Queer in AI这样的社区如何通过培育AI的参与文化，欢迎和赋权边缘化的参与者，批评贫瘠和剥削性表述等方面，为AI的参与式设计做出了更广泛的贡献。

    We present Queer in AI as a case study for community-led participatory design in AI. We examine how participatory design and intersectional tenets started and shaped this community's programs over the years. We discuss different challenges that emerged in the process, look at ways this organization has fallen short of operationalizing participatory and intersectional principles, and then assess the organization's impact. Queer in AI provides important lessons and insights for practitioners and theorists of participatory methods broadly through its rejection of hierarchy in favor of decentralization, success at building aid and programs by and for the queer community, and effort to change actors and institutions outside of the queer community. Finally, we theorize how communities like Queer in AI contribute to the participatory design in AI more broadly by fostering cultures of participation in AI, welcoming and empowering marginalized participants, critiquing poor or exploitative parti
    
[^74]: 科学问题：与ChatGPT探讨复杂系统。

    Questions of science: chatting with ChatGPT about complex systems. (arXiv:2303.16870v1 [physics.soc-ph] CROSS LISTED)

    [http://arxiv.org/abs/2303.16870](http://arxiv.org/abs/2303.16870)

    该论文介绍了使用ChatGPT代表社区对复杂系统领域的理解进行概述，ChatGPT学习了大量互联网文本数据并能够提供反映社区普遍意见、观点和语言模式的答案，探讨了复杂系统领域的教学、学习和研究主题，认为ChatGPT是社区思想的一种来源的价值。

    

    我们使用ChatGPT作为社区理解的代表，概述了复杂系统领域。ChatGPT从大量的互联网文本数据中学习语言模式和风格，使其能够提供反映社区普遍意见、观点和语言模式的答案。我们的探讨涵盖了教学、学习和研究主题。我们承认ChatGPT作为社区思想的一种来源的价值。

    We present an overview of the complex systems field using ChatGPT as a representation of the community's understanding. ChatGPT has learned language patterns and styles from a large dataset of internet texts, allowing it to provide answers that reflect common opinions, ideas, and language patterns found in the community. Our exploration covers both teaching and learning, and research topics. We recognize the value of ChatGPT as a source for the community's ideas.
    
[^75]: 机器心理学：利用心理学方法探究大型语言模型的新兴能力和行为

    Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v1 [cs.CL])

    [http://arxiv.org/abs/2303.13988](http://arxiv.org/abs/2303.13988)

    本文提出了一种新领域——机器心理学，利用心理学的方法考察大型语言模型的能力。该文规范了机器心理学研究的方法论标准，并对心理实验中提示设计政策进行了探讨和制定。

    

    大型语言模型（LLM）是将人工智能系统与人类交流和日常生活紧密结合的先锋。由于快速技术进步和其极高的通用性，现今LLM已经拥有数百万用户，并正处于成为主要信息检索、内容生成、问题解决等技术的前沿。因此，对其进行全面评估和审查显得尤为重要。由于当前LLM中出现愈加复杂和新颖的行为模式，可将其视为参与人类心理实验的对象，以便更为全面地评估其能力。为此，本文引入了一个名为"机器心理学"的新兴研究领域。本文概述了各类心理学分支如何为LLM的行为测试提供有用参考。同时，本文规范了机器心理学研究的方法论标准，特别是专注于提示设计政策的制定。此外，它还描述了行为测试结果如何为未来的LLM发展提供指导。

    Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behaviora
    
[^76]: ExBEHRT：基于电子病历的扩展Transformer预测疾病亚型和进展

    ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes & Progressions. (arXiv:2303.12364v1 [cs.LG])

    [http://arxiv.org/abs/2303.12364](http://arxiv.org/abs/2303.12364)

    ExBEHRT是一种扩展Transformer模型，应用于电子病历数据，将多种类型的记录包括在特征空间中，可以预测不同疾病下游任务的性能更好，并使用预期梯度对结果进行更细粒度的解释。

    

    本研究引入了ExBEHRT，它是BEHRT（应用于电子病历的BERT）的扩展版本，并应用不同的算法来解释其结果。我们将特征空间从仅考虑诊断和患者年龄扩展到包括多种类型的记录，包括人口统计学、临床特征、生命体征、吸烟状态、诊断、手术、药物和实验室检查，并采用一种新方法来统一不同特征的频率和时间维度。我们展示了附加特征可以显著改善不同疾病下游任务的模型性能。为了保证模型的稳健性，我们使用了预期梯度的改进方法对模型预测结果进行解释，该方法以前未应用于将EHR数据与Transformer相结合，提供了比以前方法更细粒度的解释，如特征和令牌重要性。此外，通过对肿瘤学患者的模型表示进行聚类，我们展示了ExBEHRT可以用于预测疾病亚型和进展。

    In this study, we introduce ExBEHRT, an extended version of BEHRT (BERT applied to electronic health records), and apply different algorithms to interpret its results. While BEHRT considers only diagnoses and patient age, we extend the feature space to several multimodal records, namely demographics, clinical characteristics, vital signs, smoking status, diagnoses, procedures, medications, and laboratory tests, by applying a novel method to unify the frequencies and temporal dimensions of the different features. We show that additional features significantly improve model performance for various downstream tasks in different diseases. To ensure robustness, we interpret model predictions using an adaptation of expected gradients, which has not been previously applied to transformers with EHR data and provides more granular interpretations than previous approaches such as feature and token importances. Furthermore, by clustering the model representations of oncology patients, we show tha
    
[^77]: LLM的合成数据生成对临床文本挖掘有帮助吗？

    Does Synthetic Data Generation of LLMs Help Clinical Text Mining?. (arXiv:2303.04360v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.04360](http://arxiv.org/abs/2303.04360)

    研究探讨了利用 ChatGPT 进行临床文本挖掘的潜力，但初步结果表明效果欠佳，同时上传患者信息也引发隐私问题。因此，提出了一种新的训练方法，即使用 ChatGPT 生成带标签的大量高质量合成数据进行训练。

    

    大型语言模型（LLM）的最近进展推动了诸如OpenAI的ChatGPT之类的高性能模型的发展。这些模型在问答、论文写作和代码生成等各种任务中表现出色。然而，它们在医疗保健领域的有效性仍不确定。本研究旨在通过检查ChatGPT从非结构化健康文本中提取结构化信息的能力，重点关注生物命名实体识别和关系提取，探讨ChatGPT在临床文本挖掘中的潜力。然而，我们的初步结果表明，直接应用ChatGPT进行这些任务导致表现不佳，并引发与将患者信息上传到ChatGPT API有关的隐私问题。为了克服这些限制，我们提出了一种新的训练范式，利用ChatGPT生成大量高质量的带标签的合成数据进行训练。

    Recent advancements in large language models (LLMs) have led to the development of highly potent models like OpenAI's ChatGPT. These models have exhibited exceptional performance in a variety of tasks, such as question answering, essay composition, and code generation. However, their effectiveness in the healthcare sector remains uncertain. In this study, we seek to investigate the potential of ChatGPT to aid in clinical text mining by examining its ability to extract structured information from unstructured healthcare texts, with a focus on biological named entity recognition and relation extraction. However, our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API. To overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing ChatGPT and fine
    
[^78]: 连续时间延迟系统的神经拉普拉斯控制

    Neural Laplace Control for Continuous-time Delayed Systems. (arXiv:2302.12604v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12604](http://arxiv.org/abs/2302.12604)

    本文介绍了一种神经拉普拉斯控制方法，用于解决具有不规则状态观测和未知延迟的连续时间环境下的离线强化学习问题。

    

    许多实际的离线强化学习问题包括具有延迟的连续时间环境。这些环境具有两个显著特点：首先，观察到的状态x(t)在不规则的时间间隔内进行观察；其次，当前行动a(t)仅在未知延迟g > 0 的情况下影响未来状态x(t+g)。这样的环境的一个典型例子是卫星控制，其中地球和卫星之间的通信链路会造成观测不规则和延迟。现有的离线强化学习算法在具有时间不规则观测或已知延迟的环境中取得了成功。然而，涉及时间不规则观测和未知延迟的环境仍然是一个开放且具有挑战性的问题。因此，我们提出了神经拉普拉斯控制，一种连续时间基于模型的离线强化学习方法，将神经拉普拉斯动力学模型与模型预测控制（MPC）规划器相结合，并能够从离线数据集中进行学习。

    Many real-world offline reinforcement learning (RL) problems involve continuous-time environments with delays. Such environments are characterized by two distinctive features: firstly, the state x(t) is observed at irregular time intervals, and secondly, the current action a(t) only affects the future state x(t + g) with an unknown delay g > 0. A prime example of such an environment is satellite control where the communication link between earth and a satellite causes irregular observations and delays. Existing offline RL algorithms have achieved success in environments with irregularly observed states in time or known delays. However, environments involving both irregular observations in time and unknown delays remains an open and challenging problem. To this end, we propose Neural Laplace Control, a continuous-time model-based offline RL method that combines a Neural Laplace dynamics model with a model predictive control (MPC) planner--and is able to learn from an offline dataset sam
    
[^79]: Fairguard: 在智慧城市中利用基于逻辑的公正规则

    Fairguard: Harness Logic-based Fairness Rules in Smart Cities. (arXiv:2302.11137v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.11137](http://arxiv.org/abs/2302.11137)

    本文提出了一种基于时间逻辑的公正智慧城市政策调整和生成方法Fairguard，通过两个阶段的静态生成和动态调节，缓解由多种数据和算法偏见导致的不公正预测结果。

    

    智慧城市运行在计算预测框架上，收集、整合和利用大规模传感器网络的数据。然而，这些框架容易受到多种数据和算法偏见的影响，这经常导致不公正的预测结果。本文首先通过研究田纳西州查塔努加的真实城市数据，展示了偏见在微观层面上在时间和空间上仍然存在。为了缓解这种偏见问题，我们引入了Fairguard，这是一种基于微观层面时间逻辑的方法，用于在复杂的时间空间域中进行公正的智慧城市政策调整和生成。Fairguard框架由两个阶段组成：首先，我们开发了一个静态生成器，能够通过最小化所选属性之间的相关性，基于时间逻辑条件来减少数据偏见。然后，为了确保预测算法的公正性，我们设计了一个动态组件来调节预测结果，并利用逻辑规则生成未来的公正预测。

    Smart cities operate on computational predictive frameworks that collect, aggregate, and utilize data from large-scale sensor networks. However, these frameworks are prone to multiple sources of data and algorithmic bias, which often lead to unfair prediction results. In this work, we first demonstrate that bias persists at a micro-level both temporally and spatially by studying real city data from Chattanooga, TN. To alleviate the issue of such bias, we introduce Fairguard, a micro-level temporal logic-based approach for fair smart city policy adjustment and generation in complex temporal-spatial domains. The Fairguard framework consists of two phases: first, we develop a static generator that is able to reduce data bias based on temporal logic conditions by minimizing correlations between selected attributes. Then, to ensure fairness in predictive algorithms, we design a dynamic component to regulate prediction results and generate future fair predictions by harnessing logic rules. E
    
[^80]: 管制ChatGPT和其他大型生成AI模型

    Regulating ChatGPT and other Large Generative AI Models. (arXiv:2302.02337v5 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2302.02337](http://arxiv.org/abs/2302.02337)

    本文将讨论大型生成AI模型的可信AI监管，包括直接监管、数据保护、内容监管和政策建议，并建议使用新术语来区分参与者。本文的目的是确保LGAIMs的可信度并使其为受益所用。

    

    大型生成AI模型（LGAIMs），如ChatGPT或Stable Diffusion，正在快速改变我们的沟通、说明和创造方式。然而，欧盟及其他地区的AI监管主要集中在传统AI模型上，而非LGAIMs。本文将把这些新的生成模型放置在当前的“可信AI监管”辩论中，并探讨如何调整法律以适应其能力。在奠定技术基础之后，本文的法律部分分四步进行，包括（1）直接监管，（2）数据保护，（3）内容监管和（4）政策建议。它建议使用新术语来捕捉LGAIM设置中的AI价值链，区分LGAIM开发人员、部署者、专业和非专业用户，以及LGAIM输出的接收者。我们将监管职责针对这些不同的价值链参与者进行调整，并提出四个策略，以确保LGAIMs的信任度并使其为受益所用。

    Large generative AI models (LGAIMs), such as ChatGPT or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest four strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of so
    
[^81]: 基于语言驱动的锚点的零样本对抗鲁棒性

    Language-Driven Anchors for Zero-Shot Adversarial Robustness. (arXiv:2301.13096v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.13096](http://arxiv.org/abs/2301.13096)

    本文提出了一种基于语言驱动、基于锚点的对抗训练策略LAAT，通过利用文本编码器的语义一致性，在零样本图像分类场景下增强图像模型的对抗鲁棒性。实验结果表明，该方法在零样本对抗性能上优于先前的最佳状态对抗性一次性方法，同时能为流行的图像分类模型带来实质性的零样本对抗性能提升。

    

    深度神经网络容易受到对抗性攻击。本文旨在改善具有挑战性的零样本图像分类场景下的对抗鲁棒性。为解决这一问题，我们提出了一种新的基于语言驱动、基于锚点的对抗训练策略LAAT。LAAT利用文本编码器为每个类别生成固定的锚点（归一化特征嵌入），并在对抗训练中使用这些锚点。通过利用文本编码器的语义一致性，LAAT可以增强图像模型在新类别上的对抗鲁棒性，而无需额外的样例。我们发现了最近文本编码器的余弦相似度问题，并设计了几种有效的技术来解决它。实验结果表明，LAAT显著提高了零样本对抗性能，优于先前的最佳状态对抗性一次性方法。此外，我们的方法在几个基准数据集上为流行的图像分类模型（如ResNet-50和DenseNet-121）产生了实质性的零样本对抗性能提升。

    Deep neural networks are known to be susceptible to adversarial attacks. In this work, we focus on improving adversarial robustness in the challenging zero-shot image classification setting. To address this issue, we propose LAAT, a novel Language-driven, Anchor-based Adversarial Training strategy. LAAT utilizes a text encoder to generate fixed anchors (normalized feature embeddings) for each category and then uses these anchors for adversarial training. By leveraging the semantic consistency of the text encoders, LAAT can enhance the adversarial robustness of the image model on novel categories without additional examples. We identify the large cosine similarity problem of recent text encoders and design several effective techniques to address it. The experimental results demonstrate that LAAT significantly improves zero-shot adversarial performance, outperforming previous state-of-the-art adversarially robust one-shot methods. Moreover, our method produces substantial zero-shot adver
    
[^82]: 基于数据驱动的智能产品计算设计：方法、技术和应用

    Data-driven intelligent computational design for products: Method, techniques, and applications. (arXiv:2301.12382v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.12382](http://arxiv.org/abs/2301.12382)

    数据驱动的智能计算设计(DICD)利用深度学习算法提取和表示历史或制造过程数据中的设计特征，支持设计方案检索、生成、优化、评估等。作为一种新兴的研究课题，DICD仍存在多个未开发问题。

    

    数据驱动的智能计算设计(DICD)是在快速发展的人工智能背景下出现的研究热点。它强调利用深度学习算法提取和表示历史或制造过程数据中隐藏的设计特征，并学习这些设计特征的组合和映射模式，以支持设计方案检索、生成、优化、评估等。由于其自动、高效地生成设计方案的能力，从而支持人类参与的智能创新设计活动，DICD吸引了学术界和工业界的关注。然而，作为一种新兴的研究课题，DICD仍存在许多未开发的问题，如特定数据集的构建、与工程设计相关的特征工程、DICD在整个产品设计过程中的系统方法和技术等。

    Data-driven intelligent computational design (DICD) is a research hotspot emerged under the context of fast-developing artificial intelligence. It emphasizes on utilizing deep learning algorithms to extract and represent the design features hidden in historical or fabricated design process data, and then learn the combination and mapping patterns of these design features for the purposes of design solution retrieval, generation, optimization, evaluation, etc. Due to its capability of automatically and efficiently generating design solutions and thus supporting human-in-the-loop intelligent and innovative design activities, DICD has drawn the attentions from both academic and industrial fields. However, as an emerging research subject, there are still many unexplored issues that limit the development and application of DICD, such as specific dataset building, engineering design related feature engineering, systematic methods and techniques for DICD implementation in the entire product d
    
[^83]: EHRSQL：面向电子病历的实用文本转SQL基准测试

    EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records. (arXiv:2301.07695v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.07695](http://arxiv.org/abs/2301.07695)

    该论文提出了一个面向电子病历数据的文本转SQL数据集，该数据集具有一系列独特挑战，包括生成SQL查询、理解时间表达式以及区分有无答案的问题。

    

    我们为电子病历（EHR）提供了一个新的文本到SQL数据集。对话是由222个医院工作人员包括医生、护士、保险审查和健康档案团队等手机而来。为了构建关于结构化EHR数据的QA数据集，我们在一所大学医院进行了一次民调并制作了模板话术以创建种子问题。然后，我们手动将它们链接到两个开源的EHR数据库（MIMIC-III和eICU）中，并在数据集中包含了来自民意调查的各种时间表达式和未能回答的问题。我们的数据集提出了一系列独特的挑战：模型需要 1）生成反映医院中各种需求的SQL查询，包括简单的检索和复杂的操作，如计算生存率，2）理解各种时间表达式以回答与时间敏感的医疗问题相关的问题，3）根据预测区分给定问题是可回答还是不可回答。

    We present a new text-to-SQL dataset for electronic health records (EHRs). The utterances were collected from 222 hospital staff, including physicians, nurses, insurance review and health records teams, and more. To construct the QA dataset on structured EHR data, we conducted a poll at a university hospital and templatized the responses to create seed questions. Then, we manually linked them to two open-source EHR databases, MIMIC-III and eICU, and included them with various time expressions and held-out unanswerable questions in the dataset, which were all collected from the poll. Our dataset poses a unique set of challenges: the model needs to 1) generate SQL queries that reflect a wide range of needs in the hospital, including simple retrieval and complex operations such as calculating survival rate, 2) understand various time expressions to answer time-sensitive questions in healthcare, and 3) distinguish whether a given question is answerable or unanswerable based on the predicti
    
[^84]: 特征归因的不可能定理

    Impossibility Theorems for Feature Attribution. (arXiv:2212.11870v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11870](http://arxiv.org/abs/2212.11870)

    本文阐述了对于中等规模的模型类，任何完整的线性特征归因方法都无法胜任推断模型行为的改进，启示我们在具体定义最终任务的重要性方面要汲取经验，采用重复模型评估的简单直接方法可以胜过许多其他复杂的特征归因方法。

    

    尽管有许多可产生合理解释的可解释性方法，但该领域也经验性地看到了许多失败案例。鉴于这些结果，对于实践者如何以原则性方式使用这些方法并在它们之间进行选择仍不清楚。在本文中，我们展示了对于中等规模的模型类（神经网络容易满足），任何完整的线性特征归因方法（例如Integrated Gradients和SHAP）可以被证明对于推断模型行为的改进都无法胜任。我们的结果适用于常见的最终任务，如描述局部模型行为、识别虚假特征和算法回溯。我们工作的一个重要启示是具体定义最终任务的重要性：一旦这样的最终任务被定义，一个简单和直接的方法——重复模型评估——可以胜过许多其他复杂的特征归因方法。

    Despite a sea of interpretability methods that can produce plausible explanations, the field has also empirically seen many failure cases of such methods. In light of these results, it remains unclear for practitioners how to use these methods and choose between them in a principled way. In this paper, we show that for moderately rich model classes (easily satisfied by neural networks), any feature attribution method that is complete and linear -- for example, Integrated Gradients and SHAP -- can provably fail to improve on random guessing for inferring model behaviour. Our results apply to common end-tasks such as characterizing local model behaviour, identifying spurious features, and algorithmic recourse. One takeaway from our work is the importance of concretely defining end-tasks: once such an end-task is defined, a simple and direct approach of repeated model evaluations can outperform many other complex feature attribution methods.
    
[^85]: 语言控制扩散：通过空间、时间和任务高效扩展

    Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.15629](http://arxiv.org/abs/2210.15629)

    本文提出一种利用语言控制扩散模型的分层规划器，有效而高效地扩展扩散模型，解决长时间跨度自然语言指令下的控制问题，实现了较高的单任务和多任务成功率，并极大地提高计算效率。

    

    训练通用型智能体在各个方面都很困难，需要处理高维输入（空间）、长时间跨度（时间）和多个新任务。最近的结构方面的进展使得我们可以沿着其中一个或两个维度提高扩展性能力，但计算成本仍然很高。本文提出使用语言控制扩散模型作为一种基于自然语言条件的分层规划器（LCD）来应对这三个方面。我们有效而高效地扩展扩散模型，以应对时间、状态和任务空间维度的长时间跨度控制问题。我们在CALVIN语言机器人基准测试中将LCD与其他最先进的模型进行比较，发现LCD在多任务成功率方面优于其他最先进的方法，而单任务成功率（SR）为88.7%，远高于以前的最佳成绩82.6%，大大提高了计算效率。

    Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and multiple and new tasks. Recent advances with architectures have allowed for improved scaling along one or two of these dimensions, but are still prohibitive computationally. In this paper, we propose to address all three axes by leveraging Language to Control Diffusion models as a hierarchical planner conditioned on language (LCD). We effectively and efficiently scale diffusion models for planning in extended temporal, state, and task dimensions to tackle long horizon control problems conditioned on natural language instructions. We compare LCD with other state-of-the-art models on the CALVIN language robotics benchmark and find that LCD outperforms other SOTA methods in multi task success rates while dramatically improving computational efficiency with a single task success rate (SR) of 88.7% against the previous best of 82.6%. We show that 
    
[^86]: Vitruvio：通过单点透视草图生成三维建筑网格

    Vitruvio: 3D Building Meshes via Single Perspective Sketches. (arXiv:2210.13634v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.13634](http://arxiv.org/abs/2210.13634)

    本文介绍了一种名为Vitruvio的深度学习方法，旨在将单点透视草图转换为三维建筑网格，针对AEC领域提出了该方法，并在建筑数据集上进行了验证实验，显示出比现有方法更好的重建质量。

    

    当今的建筑工程和施工(AEC)软件需要学习曲线以生成三维建筑表示。这限制了通过单个草图快速验证初步设计想法的体积含义的能力。允许设计师将单个草图转换为三维建筑将使业主能够立即可视化3D项目信息，无需进行认知负荷。如果以前的最先进的数据驱动单视图重建(SVR)方法在单个图像或草图的重建过程中展现出了出色的结果，那么它们在AEC中缺乏具体应用、分析和实验。因此，本研究弥补了这一空白，介绍了第一种专注于建筑物的深度学习方法：Vitruvio，旨在将单个草图转换为三维建筑网格。Vitruvio在特定建筑数据集(曼哈顿1K)上调整使用占位网络进行SVR任务。这种适应性带来了与SOTA方法相比在3D建筑重建质量方面的显著改进。

    Today's architectural engineering and construction (AEC) software require a learning curve to generate a three-dimension building representation. This limits the ability to quickly validate the volumetric implications of an initial design idea communicated via a single sketch. Allowing designers to translate a single sketch to a 3D building will enable owners to instantly visualize 3D project information without the cognitive load required. If previous state-of-the-art (SOTA) data-driven methods for single view reconstruction (SVR) showed outstanding results in the reconstruction process from a single image or sketch, they lacked specific applications, analysis, and experiments in the AEC. Therefore, this research addresses this gap, introducing the first deep learning method focused only on buildings that aim to convert a single sketch to a 3D building mesh: Vitruvio. Vitruvio adapts Occupancy Network for SVR tasks on a specific building dataset (Manhattan 1K). This adaptation brings 
    
[^87]: SCALE：无先验知识的在线自监督终身学习

    SCALE: Online Self-Supervised Lifelong Learning without Prior Knowledge. (arXiv:2208.11266v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.11266](http://arxiv.org/abs/2208.11266)

    本文提出了一个更加实用的在线自监督终身学习的问题设置，该设置具有挑战性，因为它涉及到数据的非独立同分布和单次遍历、缺乏外部监督和先验知识等。为了解决这些问题，我们提出了SCALE方法，它可以纯粹地从数据连续体中提取和记忆表示。

    

    无监督的终身学习是指能够在没有监督的情况下在时间上学习新的模式并记忆以前的模式。尽管在这个方向上取得了很大的进展，但现有的工作通常假设有关输入数据的强先验知识（例如，已知类别边界），而这在复杂和不可预测的环境中可能是不可能获得的。为了解决这个问题，本文提出了在线自监督终身学习无先验知识的更实践的问题设置。为了应对挑战，我们提出了一种名为SCALE的自监督对比终身学习方法，它可以纯粹地从数据连续体中提取和记忆表示。

    Unsupervised lifelong learning refers to the ability to learn over time while memorizing previous patterns without supervision. Although great progress has been made in this direction, existing work often assumes strong prior knowledge about the incoming data (e.g., knowing the class boundaries), which can be impossible to obtain in complex and unpredictable environments. In this paper, motivated by real-world scenarios, we propose a more practical problem setting called online self-supervised lifelong learning without prior knowledge. The proposed setting is challenging due to the non-iid and single-pass data, the absence of external supervision, and no prior knowledge. To address the challenges, we propose Self-Supervised ContrAstive Lifelong LEarning without Prior Knowledge (SCALE) which can extract and memorize representations on the fly purely from the data continuum. SCALE is designed around three major components: a pseudo-supervised contrastive loss, a self-supervised forgettin
    
[^88]: LogLG: 通过事件图构建实现弱监督日志异常检测

    LogLG: Weakly Supervised Log Anomaly Detection via Log-Event Graph Construction. (arXiv:2208.10833v5 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2208.10833](http://arxiv.org/abs/2208.10833)

    本文提出了一种LogLG弱监督日志异常检测框架，采用事件图构建和伪标签生成，旨在探索序列中关键字之间的语义联系，有效识别应用程序日志的异常行为。

    

    完全监督的日志异常检测方法需要为大量未标记的日志数据进行标注。最近，许多半监督方法已经被提出，以降低标注成本，并借助解析模板。然而，这些方法独立考虑每个关键字，忽略关键字之间的相关性和日志序列之间的上下文关系。本文提出了一种新型的弱监督日志异常检测框架，名为LogLG，以探索序列中关键字之间的语义联系。具体来说，我们设计了一种端到端的迭代过程，首先提取未标记日志的关键字以构建日志事件图。然后，我们构建了一个子图注释器生成未标记日志序列的伪标签。为了改善注释质量，我们采用了自监督任务来预训练子图注释器。之后，我们使用生成的伪标签训练检测模型。在条件 下，模型能够识别来自应用程序日志的异常行为。

    Fully supervised log anomaly detection methods suffer the heavy burden of annotating massive unlabeled log data. Recently, many semi-supervised methods have been proposed to reduce annotation costs with the help of parsed templates. However, these methods consider each keyword independently, which disregards the correlation between keywords and the contextual relationships among log sequences. In this paper, we propose a novel weakly supervised log anomaly detection framework, named LogLG, to explore the semantic connections among keywords from sequences. Specifically, we design an end-to-end iterative process, where the keywords of unlabeled logs are first extracted to construct a log-event graph. Then, we build a subgraph annotator to generate pseudo labels for unlabeled log sequences. To ameliorate the annotation quality, we adopt a self-supervised task to pre-train a subgraph annotator. After that, a detection model is trained with the generated pseudo labels. Conditioned on the cl
    
[^89]: 一种感知优化且自校准的色调映射运算符

    A Perceptually Optimized and Self-Calibrated Tone Mapping Operator. (arXiv:2206.09146v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2206.09146](http://arxiv.org/abs/2206.09146)

    本文开发了一种两步神经网络TMO，具有自校准和感知优化功能，可以将HDR图像压缩到LDR图像，同时通过感知度量实现了灵敏的质量优化。

    

    随着高动态范围（HDR）摄影的普及和可访问性增加，对动态范围压缩的色调映射运算符（TMO）需求日益增加。本文开发了一个两步神经网络TMO，具有自校准和感知优化功能。第一阶段，我们首先将HDR图像分解成规范化拉普拉斯金字塔，然后使用两个轻量级深度神经网络（DNN），以规范化表示作为输入来估计相应LDR图像的拉普拉斯金字塔。我们通过最小化规范化拉普拉斯金字塔距离（NLPD）来优化色调映射网络，这是与人类对色调映射图像质量的判断相一致的感知度量。在第二阶段中，输入的HDR图像是自校准的，以计算出最终的LDR图像。我们将同一张HDR图像输入经过不同最大亮度重新调整比例后的学习色调映射网络中，来完成这一自校准过程。

    With the increasing popularity and accessibility of high dynamic range (HDR) photography, tone mapping operators (TMOs) for dynamic range compression are practically demanding. In this paper, we develop a two-stage neural network-based TMO that is self-calibrated and perceptually optimized. In Stage one, motivated by the physiology of the early stages of the human visual system, we first decompose an HDR image into a normalized Laplacian pyramid. We then use two lightweight deep neural networks (DNNs), taking the normalized representation as input and estimating the Laplacian pyramid of the corresponding LDR image. We optimize the tone mapping network by minimizing the normalized Laplacian pyramid distance (NLPD), a perceptual metric aligning with human judgments of tone-mapped image quality. In Stage two, the input HDR image is self-calibrated to compute the final LDR image. We feed the same HDR image but rescaled with different maximum luminances to the learned tone mapping network, 
    
[^90]: 强化学习、量化相应均衡和二人零和博弈的统一方法

    A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games. (arXiv:2206.05825v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05825](http://arxiv.org/abs/2206.05825)

    本文展示了磁镜面下降算法作为均衡求解器和强化学习方法的优点，包括实现了线性收敛的相应均衡求解器和在表格式设置中实现了与CFR相竞争的标准强化学习算法，以及在“黑暗六角”和“幻象井字”中的自我玩耍深度强化学习算法的良好表现。

    

    本文研究了一种算法，我们称之为磁镜面下降，它受到镜面下降和非欧几里德近端梯度算法的启发。我们的贡献在于展示了磁镜面下降作为均衡求解器以及在两人零和博弈中作为强化学习方法的优点。这些优点包括：1) 成为首个对于具有一阶反馈的扩展形式游戏实现线性收敛的量化相应均衡求解器；2) 成为首个在表格式设置中与CFR实现实验性竞争结果的标准强化学习算法；3) 实现了在3x3黑暗六角和幻象井字游戏中成为自我游戏深度强化学习算法的有利性能。

    This work studies an algorithm, which we call magnetic mirror descent, that is inspired by mirror descent and the non-Euclidean proximal gradient algorithm. Our contribution is demonstrating the virtues of magnetic mirror descent as both an equilibrium solver and as an approach to reinforcement learning in two-player zero-sum games. These virtues include: 1) Being the first quantal response equilibria solver to achieve linear convergence for extensive-form games with first order feedback; 2) Being the first standard reinforcement learning algorithm to achieve empirically competitive results with CFR in tabular settings; 3) Achieving favorable performance in 3x3 Dark Hex and Phantom Tic-Tac-Toe as a self-play deep reinforcement learning algorithm.
    
[^91]: DPSNN: 差分隐私脉冲神经网络与时序增强池化

    DPSNN: A Differentially Private Spiking Neural Network with Temporal Enhanced Pooling. (arXiv:2205.12718v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2205.12718](http://arxiv.org/abs/2205.12718)

    本文提出了一种差分隐私脉冲神经网络（DPSNN），将差分隐私算法与脉冲神经网络相结合，保持了SNN的强隐私保护；同时提出了时序增强池化（TEP）方法，使SNN能够获得更好的信息传输。在MNIST和CIFAR-10数据集上进行实验验证了该方法的有效性。

    

    隐私保护是机器学习算法中至关重要的问题，目前的隐私保护方法常常结合传统基于实数的人工神经网络。作为新一代人工神经网络，脉冲神经网络（SNN）在许多领域中发挥着关键作用，因此，有必要对SNN的隐私保护进行研究。本文将差分隐私（DP）算法与SNN相结合，提出了一种差分隐私脉冲神经网络（DPSNN）。SNN使用离散的脉冲序列来传输信息，并结合DP引入的梯度噪声，从而使SNN保持强大的隐私保护。同时，为了使SNN在获得高隐私保护的同时保持高性能，我们提出了时序增强池化（TEP）方法。该方法充分将SNN的时序信息与空间信息传输结合起来，使SNN能够更好地进行信息传输。我们对DPSNN在MNIST和CIFAR-10数据集上进行了实验证明了其有效性。

    Privacy protection is a crucial issue in machine learning algorithms, and the current privacy protection is combined with traditional artificial neural networks based on real values. Spiking neural network (SNN), the new generation of artificial neural networks, plays a crucial role in many fields. Therefore, research on the privacy protection of SNN is urgently needed. This paper combines the differential privacy(DP) algorithm with SNN and proposes a differentially private spiking neural network (DPSNN). The SNN uses discrete spike sequences to transmit information, combined with the gradient noise introduced by DP so that SNN maintains strong privacy protection. At the same time, to make SNN maintain high performance while obtaining high privacy protection, we propose the temporal enhanced pooling (TEP) method. It fully integrates the temporal information of SNN into the spatial information transfer, which enables SNN to perform better information transfer. We conduct experiments on 
    
[^92]: 基于价值知识整合的高效内存强化学习

    Memory-efficient Reinforcement Learning with Value-based Knowledge Consolidation. (arXiv:2205.10868v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.10868](http://arxiv.org/abs/2205.10868)

    本论文提出了一种基于价值知识整合的高效内存强化学习算法，通过从目标 Q 网络向当前 Q 网络整合知识，减少遗忘并保持高样本效率。相较于传统方法，本算法在特征和图像任务中表现相当或更好，同时减轻了大经验重放缓存器带来的内存负担。

    

    人工神经网络在通用函数逼近上非常有前途，但面临灾难性遗忘的训练难题，特别是在非独立或非同分布的数据上训练困难。经验重放缓存器是深度强化学习中的标准组件，通常通过将经验存储在大缓存器中并延迟使用进行训练，以减少遗忘并提高样本效率。然而，大型重放缓存器会导致重负载内存，特别是对于内存容量有限的边缘设备和嵌入式设备。我们提出了基于深度 Q 网络算法的内存高效强化学习算法，以缓解这个问题。我们的算法通过从目标 Q 网络向当前 Q 网络整合知识，减少遗忘并保持高样本效率。与基准方法相比，我们的算法在基于特征和基于图像的任务中实现了可比较或更好的性能，同时减轻了大经验重放缓存器的负担。

    Artificial neural networks are promising for general function approximation but challenging to train on non-independent or non-identically distributed data due to catastrophic forgetting. The experience replay buffer, a standard component in deep reinforcement learning, is often used to reduce forgetting and improve sample efficiency by storing experiences in a large buffer and using them for training later. However, a large replay buffer results in a heavy memory burden, especially for onboard and edge devices with limited memory capacities. We propose memory-efficient reinforcement learning algorithms based on the deep Q-network algorithm to alleviate this problem. Our algorithms reduce forgetting and maintain high sample efficiency by consolidating knowledge from the target Q-network to the current Q-network. Compared to baseline methods, our algorithms achieve comparable or better performance in both feature-based and image-based tasks while easing the burden of large experience re
    
[^93]: 面向方面的情感分析数据集调查

    Survey of Aspect-based Sentiment Analysis Datasets. (arXiv:2204.05232v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.05232](http://arxiv.org/abs/2204.05232)

    本研究汇总了65个公开可用的ABSA数据集，包括45个英文数据集和20个其他语言数据集，提供了一个可以用于训练和评估自主ABSA系统的数据库。

    

    面向方面的情感分析(ABSA)是一个自然语言处理问题，需要分析用户生成的评论以确定：a)正在审查的目标实体，b)属于哪个高级方面，c)对目标和方面表达的情感。ABSA的众多但分散的语料库使研究人员很难快速确定最适合特定ABSA子任务的语料库。本研究旨在提供一个可以用于训练和评估自主ABSA系统的数据库。此外，我们提供了ABSA和其子任务的主要语料库概述，并强调研究人员在选择语料库时应考虑的几个特征。最后，我们讨论了当前收集方法的优缺点并为未来语料库创建提出建议。本调查审核了65个公开可用的ABSA数据集，涵盖25个领域，包括45个英语和20个其他语言的数据集。

    Aspect-based sentiment analysis (ABSA) is a natural language processing problem that requires analyzing user-generated reviews to determine: a) The target entity being reviewed, b) The high-level aspect to which it belongs, and c) The sentiment expressed toward the targets and the aspects. Numerous yet scattered corpora for ABSA make it difficult for researchers to identify corpora best suited for a specific ABSA subtask quickly. This study aims to present a database of corpora that can be used to train and assess autonomous ABSA systems. Additionally, we provide an overview of the major corpora for ABSA and its subtasks and highlight several features that researchers should consider when selecting a corpus. Finally, we discuss the advantages and disadvantages of current collection approaches and make recommendations for future corpora creation. This survey examines 65 publicly available ABSA datasets covering over 25 domains, including 45 English and 20 other languages datasets.
    
[^94]: 深度泊松混合模型的概率层级预测

    Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures. (arXiv:2110.13179v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.13179](http://arxiv.org/abs/2110.13179)

    该论文提出了一种新的深度泊松混合网络方法（DPMN），能够在存在可靠的层级信息时进行准确和一致的概率时间序列预测。该模型可以保证层级一致性，并在大规模的层级预测问题中获得了最先进的结果。

    

    层级预测问题在时间序列具有自然的分组结构时出现，并需要在不同层次的聚合和分解中进行预测。在这种问题中，通常希望在给定的层次结构中满足聚合约束，称为层级一致性。在概率预测的情况下，保持一致性同时产生准确的预测可能是一个具有挑战性的问题。我们提出了一种新颖的方法，当存在可靠的层级信息时，可以进行准确和一致的概率时间序列预测。我们称之为深度泊松混合网络（DPMN）。它依赖于神经网络和一种统计模型的组合，用于层次多变量时间序列结构的联合分布。通过构建，该模型保证了层级一致性，并提供了用于预测分布聚合和分解的简单规则。我们在一个大规模的层级预测问题—M4竞赛上进行了实验，在非集成方法中，DPMN获得了最先进的结果。

    Hierarchical forecasting problems arise when time series have a natural group structure, and predictions at multiple levels of aggregation and disaggregation across the groups are needed. In such problems, it is often desired to satisfy the aggregation constraints in a given hierarchy, referred to as hierarchical coherence in the literature. Maintaining coherence while producing accurate forecasts can be a challenging problem, especially in the case of probabilistic forecasting. We present a novel method capable of accurate and coherent probabilistic forecasts for time series when reliable hierarchical information is present. We call it Deep Poisson Mixture Network (DPMN). It relies on the combination of neural networks and a statistical model for the joint distribution of the hierarchical multivariate time series structure. By construction, the model guarantees hierarchical coherence and provides simple rules for aggregation and disaggregation of the predictive distributions. We perfo
    
[^95]: 利用大偏差理论的熵正则化强化学习

    Entropy Regularized Reinforcement Learning Using Large Deviation Theory. (arXiv:2106.03931v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.03931](http://arxiv.org/abs/2106.03931)

    本文利用大偏差理论建立了熵正则化强化学习与非平衡统计力学的联系，在长时间极限下推导出了马尔可夫决策过程模型中最优策略和最优动态的精确解析结果，并提出了新的分析和计算框架。

    

    强化学习是机器学习中一个重要的研究领域，越来越多地被应用于物理学中的复杂优化问题。同时，物理学中的概念也为强化学习带来了重大进展，如熵正则化强化学习。然而，针对熵正则化强化学习中优化的解析解目前是一个未解之谜。在本文中，我们建立了熵正则化强化学习与非平衡统计力学的联系，重点关注在罕见事件条件下的马尔可夫过程。在长时间极限下，我们应用大偏差理论的方法，推导出马尔可夫决策过程（MDP）模型中最优策略和最优动态的精确解析结果，从而得到了一个新的熵正则化强化学习的分析和计算框架，经过模拟验证。

    Reinforcement learning (RL) is an important field of research in machine learning that is increasingly being applied to complex optimization problems in physics. In parallel, concepts from physics have contributed to important advances in RL with developments such as entropy-regularized RL. While these developments have led to advances in both fields, obtaining analytical solutions for optimization in entropy-regularized RL is currently an open problem. In this paper, we establish a mapping between entropy-regularized RL and research in non-equilibrium statistical mechanics focusing on Markovian processes conditioned on rare events. In the long-time limit, we apply approaches from large deviation theory to derive exact analytical results for the optimal policy and optimal dynamics in Markov Decision Process (MDP) models of reinforcement learning. The results obtained lead to a novel analytical and computational framework for entropy-regularized RL which is validated by simulations. The
    
[^96]: 一种基于风险意识的贝叶斯马尔可夫决策过程离线策略选择方法

    An Offline Risk-aware Policy Selection Method for Bayesian Markov Decision Processes. (arXiv:2105.13431v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.13431](http://arxiv.org/abs/2105.13431)

    本文提出了一种基于风险意识的离线策略选择方法，该方法采用“利用”与“谨慎”(EvC)的范式，选取权重值最大化且同时考虑长期奖励和风险的策略。

    

    在规划的离线模型学习和离线强化学习中，有限的数据集限制了相对马尔可夫决策过程(MDP)的价值函数的估计。因此，在真实世界中获得的策略的性能受到限制，并可能存在风险，特别是当部署错误的策略可能导致灾难性后果时。为此，为了减少模型误差（或学习模型与实际模型之间的分布偏移）并广泛地获得风险感知的解决方案，正在采取多种途径。但在最终应用中，从哪个角度出发选择基线呢？在计算时间不是问题而鲁棒性是首要考虑因素的离线情形下，本文提出了一种“利用”与“谨慎”(EvC)的范式，该范式(1)利用贝叶斯形式主张模型不确定性，同时(2)选择最大化风险权重值和长期奖励的策略。

    In Offline Model Learning for Planning and in Offline Reinforcement Learning, the limited data set hinders the estimate of the Value function of the relative Markov Decision Process (MDP). Consequently, the performance of the obtained policy in the real world is bounded and possibly risky, especially when the deployment of a wrong policy can lead to catastrophic consequences. For this reason, several pathways are being followed with the scope of reducing the model error (or the distributional shift between the learned model and the true one) and, more broadly, obtaining risk-aware solutions with respect to model uncertainty. But when it comes to the final application which baseline should a practitioner choose? In an offline context where computational time is not an issue and robustness is the priority we propose Exploitation vs Caution (EvC), a paradigm that (1) elegantly incorporates model uncertainty abiding by the Bayesian formalism, and (2) selects the policy that maximizes a ris
    

