# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Learning to Generalize for Cross-domain QA.](http://arxiv.org/abs/2305.08208) | 提出了一种不增加训练成本的跨域问答泛化学习方法，通过结合提示方法和线性探测再微调策略，有效提高了产生式和判别式模型的泛化能力，取得了优于基准方法4.5%-7.9%的结果。 |
| [^2] | [A Dataset Fusion Algorithm for Generalised Anomaly Detection in Homogeneous Periodic Time Series Datasets.](http://arxiv.org/abs/2305.08197) | 该论文介绍了一种名为“数据集融合”的新型数据集合成算法，可将来自多个同质数据集的周期信号融合为单个数据集，同时保留了通用异常检测的独特特征。在两个同质感应电动机（IM）故障数据集的3相电流数据的案例研究中表现出显著优势，有潜力在跨多个来源利用可用数据方面发挥作用。 |
| [^3] | [A Comprehensive Survey on Segment Anything Model for Vision and Beyond.](http://arxiv.org/abs/2305.08196) | 本文综述了最近提出的“任意分割模型”（SAM）的进展，该模型打破了分割界限，提高了计算机视觉基础模型的发展。 |
| [^4] | [Path Planning for Air-Ground Robot Considering Modal Switching Point Optimization.](http://arxiv.org/abs/2305.08178) | 本文提出一种考虑模式切换点优化的空地机器人路径规划方法，通过轻量级全局空间规划技术和可互换的搜索方法来提高能源效率、搜索速度和任务执行的可靠性。 |
| [^5] | [ParaLS: Lexical Substitution via Pretrained Paraphraser.](http://arxiv.org/abs/2305.08146) | 本研究提出了一种词汇替换方法，使用释义生成器生成替代词候选项，并提出两种解码策略，实验结果表明优于基于预训练语言模型的最新方法。 |
| [^6] | [Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering.](http://arxiv.org/abs/2305.08135) | 提出了一种基于概念中心提示的对比解释生成模型CPACE，旨在将获得的符号知识转化为对比解释，用于更好地区分常识问答中的差异。 |
| [^7] | [Theta sequences as eligibility traces: a biological solution to credit assignment.](http://arxiv.org/abs/2305.08124) | 本论文提出了Theta序列作为生物神经网络中资格追踪的一种解决方案，通过模拟证明了Theta序列可以在不需要维护长时间记忆痕迹的情况下，实现对于预测误差的引导。 |
| [^8] | [The Structure and Dynamics of Knowledge Graphs, with Superficiality.](http://arxiv.org/abs/2305.08116) | 该论文提出了第一个知识图谱的结构和动态模型，并引入表层性来简单建模复杂特征，从而掌握全局知识分布的平衡。 |
| [^9] | [Quantum Operation of Affective Artificial Intelligence.](http://arxiv.org/abs/2305.08112) | 该论文分析了体验情感人类做出决策的基本原理，比较了量子理论和经典术语的两种方法。论文阐明了内在噪声下量子测量和情感决策制定之间的类比关系，并表明认知过程在形式上与量子测量具有许多相似特征。该论文的最终目的是使得人工智能能够通过只采用经典概念的公理方法进行运算。 |
| [^10] | [Federated TD Learning over Finite-Rate Erasure Channels: Linear Speedup under Markovian Sampling.](http://arxiv.org/abs/2305.08104) | 该论文提出了一种适用于联邦学习的算法 QFedTD，在有限速抹通道下使用线性函数逼近以达到线性加速的效果，在强化学习方面具有实际应用价值。 |
| [^11] | [Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation.](http://arxiv.org/abs/2305.08096) | 本文揭示了神经机器翻译中知识蒸馏的本质，即来自于教师模型的top-1预测。同时，指出了当前基于词级别的知识蒸馏存在的问题，并提出了一种新方法——Top-1 Information。 |
| [^12] | [AI for Agile development: a Meta-Analysis.](http://arxiv.org/abs/2305.08093) | 本研究从系统性文献综述和纵向元分析出发，探讨了敏捷软件开发中集成人工智能的益处和挑战，需要进一步研究以全面了解其影响和应对相关挑战。 |
| [^13] | [Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives.](http://arxiv.org/abs/2305.08088) | 本文提出了BBT-RGB，一套用于增强黑盒优化效率和性能的直接且互补技术套件，包括两阶段无导数优化策略、自动语言转化器构建及其在少样本设置中的新用法以及更好的提示初始化。 |
| [^14] | [Off-Policy Evaluation for Large Action Spaces via Conjunct Effect Modeling.](http://arxiv.org/abs/2305.08062) | 本研究提出了一个称为OffCEM的估计器，用于对大离散动作空间下上下文匹配策略进行离线策略评估。该估计器通过基于模型的奖励估计来处理残余因果效应，并在新的本地正确性条件下保持无偏性。结果表明，OffCEM在合成和实际大动作空间数据集上优于现有方法。 |
| [^15] | [Two is Better Than One: Digital Siblings to Improve Autonomous Driving Testing.](http://arxiv.org/abs/2305.08060) | 本文提出了数字孪生的概念，使用不同技术构建多个通用仿真器，强化了自动驾驶软件的基于仿真的测试，提高了测试结果的普适性和可靠性。 |
| [^16] | [Semantic-aware Dynamic Retrospective-Prospective Reasoning for Event-level Video Question Answering.](http://arxiv.org/abs/2305.08059) | 本文提出了一种用于视频问答的语义感知动态回顾-前瞻推理方法，通过明确使用问题的语义角色标注（SRL）结构，根据问题SRL结构的哪一部分被关注来促进跨视频帧的复杂推理，实验结果显示性能优越。 |
| [^17] | [A Surprisingly Simple Continuous-Action POMDP Solver: Lazy Cross-Entropy Search Over Policy Trees.](http://arxiv.org/abs/2305.08049) | 提出了一种称为LCEOPT的简单在线连续动作POMDP求解器，利用基于策略树的懒惰交叉熵搜索实现了高效解决具有连续动作空间的挑战性POMDP问题，相较于以往最先进的POMDP求解器可实现高达两个数量级的计算节省。 |
| [^18] | [Towards Understanding the Generalization of Graph Neural Networks.](http://arxiv.org/abs/2305.08048) | 本文探索了图神经网络的泛化；通过理论分析和实验结果发现了影响泛化间隙的体系结构因素。 |
| [^19] | [Provable Multi-instance Deep AUC Maximization with Stochastic Pooling.](http://arxiv.org/abs/2305.08040) | 本文提出了在多实例学习中使用深度AUC最大化（DAM）的方法，并根据包含大量实例的情况下训练的计算挑战，提出了一种基于方差减少的随机池化方法，使得只需对每个包进行少量采样即可计算MIDAM模型，提高了效率和准确性。 |
| [^20] | [DNN-Defender: An in-DRAM Deep Neural Network Defense Mechanism for Adversarial Weight Attack.](http://arxiv.org/abs/2305.08034) | DNN-Defender是一种基于DRAM的受害者重点防御机制，适用于量化DNN，利用内存中交换的潜力以抵御有针对性的位翻转攻击，可以提供高水平的保护。 |
| [^21] | [On enhancing the robustness of Vision Transformers: Defensive Diffusion.](http://arxiv.org/abs/2305.08031) | 本文提出了一种防御性扩散技术作为对抗性净化器，以消除攻击者在原始图像中引入的对抗性噪声，从而提高了ViT模型在医疗应用中的鲁棒性和可靠性。 |
| [^22] | [SongDriver2: Real-time Emotion-based Music Arrangement with Soft Transition.](http://arxiv.org/abs/2305.08029) | SongDriver2实现了基于情绪的实时音乐编排，并提出了柔和过渡机制，使音乐具有高度真实性和平滑过渡。 |
| [^23] | [DRew: Dynamically Rewired Message Passing with Delay.](http://arxiv.org/abs/2305.08018) | 本文提出了一种能够应用于任何MPNN结构的框架，执行基于层的动态重连来确保逐渐密集化的图形。同时引入了一种延迟机制，允许跨层节点之间的跳跃连接。 |
| [^24] | [Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning.](http://arxiv.org/abs/2305.08014) | 本论文提出了一种轻量级全卷积神经网络+迁移学习方法，能够在跨场景手势识别任务中有效解决数据变异性问题。 |
| [^25] | [Beyond the Safeguards: Exploring the Security Risks of ChatGPT.](http://arxiv.org/abs/2305.08005) | 本文探讨了ChatGPT的多种安全风险和绕过保护措施的潜在方法，发现即使保护措施存在，LLMs仍存在伦理和安全风险。本研究为减轻这些风险提供了潜在策略。 |
| [^26] | [On the Computational Cost of Stochastic Security.](http://arxiv.org/abs/2305.07973) | 本文探究了使用长期持续蒙特卡罗模拟是否能提高能量模型的质量，并通过增加计算预算改进了模型的校准性和对抗鲁棒性。 |
| [^27] | [Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics.](http://arxiv.org/abs/2305.07970) | 本研究探讨了大型语言模型的能力，发现其可以将自然语言描述转化为适当的行为，但在区分细微的合作和竞争水平方面的能力受到限制，为使用LLMs在人类决策制定背景下的伦理意义和局限性做出了贡献。 |
| [^28] | [More for Less: Safe Policy Improvement With Stronger Performance Guarantees.](http://arxiv.org/abs/2305.07958) | 本论文提出了一种新的方法，可以用较少的数据保证安全策略优化(SPI)的性能，并降低了SPIBB算法的样本复杂度。 |
| [^29] | [AMTSS: An Adaptive Multi-Teacher Single-Student Knowledge Distillation Framework For Multilingual Language Inference.](http://arxiv.org/abs/2305.07928) | AMTSS是一种自适应多教师单学生知识蒸馏框架，可以在多语言设置下支持成本效益的语言推理，并取得了在XNLI数据集和AliExpress中竞争性的结果。 |
| [^30] | [Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion.](http://arxiv.org/abs/2305.07912) | 这篇论文提出了一种基于提示的预训练语言模型（PPT），用于时间知识图谱补全。通过遮盖策略，将TKGC任务转换为遮盖词预测任务，可以利用预训练语言模型中的语义信息。 |
| [^31] | [Translating SUMO-K to Higher-Order Set Theory.](http://arxiv.org/abs/2305.07903) | 本文介绍了将SUMO-K翻译成高阶集合论的方法，并为超越一阶逻辑的SUMO部分提供了形式化语义。同时也嵌入了一个大型的常识本体论到一个安全的互动定理证明系统中，这可用于创建一些高阶常识推理问题。 |
| [^32] | [PESTS: Persian_English Cross Lingual Corpus for Semantic Textual Similarity.](http://arxiv.org/abs/2305.07893) | 本研究提出了跨语言的语义相似性模型PESTS，并通过波斯语-英语的跨语言语料库来验证模型的准确性。 |
| [^33] | [DAC-MR: Data Augmentation Consistency Based Meta-Regularization for Meta-Learning.](http://arxiv.org/abs/2305.07892) | 为了提高元学习的性能，我们提出了一个基于元知识信息增强的元学习框架。我们通过使用适当的MR目标将元知识初步集成到元目标中，来规范元模型函数类的容量复杂度，有助于在未知任务上实现更好的泛化性能。 |
| [^34] | [Neural operator for structural simulation and bridge health monitoring.](http://arxiv.org/abs/2305.07889) | 本论文提出了结构模拟和桥梁健康监测的神经运算器VINO，通过学习结构响应场和损伤场之间的映射，在前向预测和反向确定损伤区域和程度方面可以比传统有限元模型更准确地预测和判断。 |
| [^35] | [Scalable Educational Question Generation with Pre-trained Language Models.](http://arxiv.org/abs/2305.07871) | 这项研究开发了一种新的教育问题生成模型，能够通过在科学文本和科学问题数据上进行预训练和微调预训练语言模型，实现优秀的教育问题自动生成。 |
| [^36] | [Bridging History with AI A Comparative Evaluation of GPT 3.5, GPT4, and GoogleBARD in Predictive Accuracy and Fact Checking.](http://arxiv.org/abs/2305.07868) | 本研究比较评估了三种大型语言模型的性能，结果表明AI技术在历史事实核查和填补空白方面有着巨大的潜力，其中GPT 4表现最为优异，这说明需要进一步探索AI技术在历史研究中的应用，以便丰富我们对过去的理解和弥合历史知识差距。 |
| [^37] | [Stackelberg Decision Transformer for Asynchronous Action Coordination in Multi-Agent Systems.](http://arxiv.org/abs/2305.07856) | STEER是一种可以解决多智能体系统中异步动作协调问题的启发式方法，通过结合层次决策结构、自回归序列模型的建模能力和探索性学习方法高效地管理空间和时间上的决策过程。 |
| [^38] | [A Federated Learning-based Industrial Health Prognostics for Heterogeneous Edge Devices using Matched Feature Extraction.](http://arxiv.org/abs/2305.07854) | 提出了一种基于联邦学习的健康预测模型，该模型具有特征相似性匹配算法来区分学习来自异构边缘设备的数据，以便开发出更准确的预测模型。 |
| [^39] | [Understanding Model Averaging in Federated Learning on Heterogeneous Data.](http://arxiv.org/abs/2305.07845) | 本文研究了异构数据联邦学习中的模型平均技术，通过可视化损失/错误景观揭示了客户端模型环绕全局模型在一个共同的盆地内，并且发现全局模型在早期训练后的误差主要来自客户端数据集和全局数据集之间非重叠的数据及全局模型与客户端模型之间的最大距离两个因素。 |
| [^40] | [A Simple and Plug-and-play Method for Unsupervised Sentence Representation Enhancement.](http://arxiv.org/abs/2305.07824) | 本文提出了一种名为 RepAL 的简单易用的无监督方法，用于增强句子表示。通过减弱句子嵌入中的冗余信息，可以提高语义匹配和检索的效果。该方法无须训练，可与大多数现有的无监督句子学习模型结合使用。 |
| [^41] | [Cloud-RAIN: Point Cloud Analysis with Reflectional Invariance.](http://arxiv.org/abs/2305.07814) | 论文提出了一种具有反射不变性的点云分析框架Cloud-RAIN，使用二次神经元和PCA规范表示实现。 |
| [^42] | [Using Deepfake Technologies for Word Emphasis Detection.](http://arxiv.org/abs/2305.07791) | 本研究利用Deepfake技术产生没有重音的语音来解决自动语音重音检测任务，通过比较生成的语音和口述语音，能够分离出相对容易检测到的重音模式。 |
| [^43] | [Answering Complex Questions over Text by Hybrid Question Parsing and Execution.](http://arxiv.org/abs/2305.07789) | 提出了一种混合问题解析和执行框架，在文字问答系统中实现回答复杂问题，通过解析问题为H表达式并设计混合执行器实现。在基准数据集中实现了最先进的准确率和效率表现。 |
| [^44] | [Knowledge Authoring for Rules and Actions.](http://arxiv.org/abs/2305.07763) | KALMRA是一个工具，它扩展了英语命令的功能，以允许在KALM中进行规则和操作的创作，并通过规则和操作的创作任务数据集的评估显示出较高的准确性和表现。 |
| [^45] | [In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making.](http://arxiv.org/abs/2305.07722) | AI解释只有在允许人类决策者验证AI预测的正确性时才有用，而大多数决策环境无法进行验证。 |
| [^46] | [AWFSD: Accelerated Wirtinger Flow with Score-based Diffusion Image Prior for Poisson-Gaussian Holographic Phase Retrieval.](http://arxiv.org/abs/2305.07712) | 本文提出了一种名为AWFSD的算法，它基于评分扩散模型作为生成先验，用于解决在实际场景中，由光学成像系统引起的测量数据被混合的泊松和高斯噪声（PG噪声）所破坏的全息相位恢复问题，并且比现有方法具有更优的性能。 |
| [^47] | [Davinci the Dualist: the mind-body divide in large language models and in human learners.](http://arxiv.org/abs/2305.07667) | 本研究探究了大语言模型Davinci中的心身分离，发现其具有弱化的二元论倾向。 |
| [^48] | [Imitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?.](http://arxiv.org/abs/2305.07666) | 本研究探讨了大型语言和语言视觉模型的局限性，以及机器与人类儿童在模仿和创新方面的不同表现。结果表明，机器需要更多的信息才能达到孩子所能做到的水平。 |
| [^49] | [A Comprehensive Survey on Affective Computing; Challenges, Trends, Applications, and Future Directions.](http://arxiv.org/abs/2305.07665) | 本综述介绍了情感计算的重要性、思想、方法和结果，调查了最新的机器学习和混合现实相结合的情感计算方法，并讨论了各种应用程序。 |
| [^50] | [Quantified Semantic Comparison of Convolutional Neural Networks.](http://arxiv.org/abs/2305.07663) | 本研究提出了两种方法来量化卷积神经网络潜在空间中语义信息之间的相似性，从而揭示CNN层内语义信息的流动和相似性，以及不同网络之间的相似度程度。 |
| [^51] | [Generative AI: Implications and Applications for Education.](http://arxiv.org/abs/2305.07605) | 本文研究了一种生成式人工智能技术——基于大型语言模型的聊天机器人（C-LLM），以及它在复杂学生作品的人工智能评估和审查方面的应用，同时探讨了这种技术在教育中的应用范围。 |
| [^52] | [PillarAcc: Sparse PointPillars Accelerator for Real-Time Point Cloud 3D Object Detection on Edge Devices.](http://arxiv.org/abs/2305.07522) | PillarAcc提出了一种稀疏算法-硬件协同设计，有效增强基于Pillar的三维物体检测网络，实现高效率的计算减少。 |
| [^53] | [Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation.](http://arxiv.org/abs/2305.07375) | 本文对ChatGPT的因果推理能力进行了首次全面评估，实验证明ChatGPT是一个好的因果解释者，但不是一个好的因果推理者，存在严重的因果幻觉问题，对于明确的因果关系表现良好。 |
| [^54] | [A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects.](http://arxiv.org/abs/2305.07348) | 本文评估了基于深度学习的单目航天器姿态估计的现有方法，总结了部署该方法在现实任务中仍需克服的挑战。 |
| [^55] | [Enhancing Datalog Reasoning with Hypertree Decompositions.](http://arxiv.org/abs/2305.06854) | 本文提出了一种利用超树分解来增强Datalog推理效率的算法，并将其与标准Datalog算法相结合以减少额外开销。实证评估结果表明，该算法可以提高推理效率和减少内存消耗。 |
| [^56] | [Utility-Maximizing Bidding Strategy for Data Consumers in Auction-based Federated Learning.](http://arxiv.org/abs/2305.06784) | 本文提出了一种首个效用最大化竞标策略（Fed-Bidder），使多个FL数据消费者可以通过AFL有效而高效地竞争数据拥有者。 |
| [^57] | [On practical robust reinforcement learning: adjacent uncertainty set and double-agent algorithm.](http://arxiv.org/abs/2305.06657) | 本文提出了一个名为ARQ-Learning的鲁棒性强化学习算法，采用了一个更加实际的不确定性集，并提出了一种称为“悲观代理”的方法。该算法在表格化的情况下获得了与现有算法相同的快速收敛速度，并为实际应用提供了更好的鲁棒性。而且，本文还首次提出了用于深度Q网络和深度确定策略梯度的鲁棒RL算法PR-DQN和PR-DDPG。 |
| [^58] | [HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level.](http://arxiv.org/abs/2305.06588) | 提出了HAHE模型，使用全局和局部水平的注意力学习了超关系知识图谱的图形结构和顺序结构，并在多个基准数据集上实现了最先进的表现。 |
| [^59] | [How to Index Item IDs for Recommendation Foundation Models.](http://arxiv.org/abs/2305.06569) | 本研究对推荐基础模型的项目索引问题进行了系统检查，提出了一种新的上下文感知索引方法，该方法在项目推荐准确性和文本生成质量方面具有优势。 |
| [^60] | [Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy.](http://arxiv.org/abs/2305.06360) | 本文综述了机器遗忘的现状和技术应用，包括数据删除、扰动和模型更新，讨论了MU在隐私、安全和公正性等领域的潜在益处，以及它在自然语言处理、计算机视觉和推荐系统中的未来发展方向。 |
| [^61] | [Patchwork Learning: A Paradigm Towards Integrative Analysis across Diverse Biomedical Data Sources.](http://arxiv.org/abs/2305.06217) | 补丁学习是一种新的范式，通过整合来自不同数据源的信息，解决了数据隐私、异构数据来源和无法充分利用多个数据模态的挑战。它可以同时利用互补的数据来源，同时保护数据隐私，从而实现开发更全面和具有普适性的ML模型。 |
| [^62] | [VCSUM: A Versatile Chinese Meeting Summarization Dataset.](http://arxiv.org/abs/2305.05280) | 介绍了一个多功能的中文会议摘要数据集VCSum，包括239个现实生活中的会议，总时长超过230小时。该数据集可以适应各种摘要任务或方法，包括基于分割的摘要、多粒度摘要和检索-生成摘要。数据集和代码将在GitHub上发布。 |
| [^63] | [ANALOGICAL - A New Benchmark for Analogy of Long Text for Large Language Models.](http://arxiv.org/abs/2305.05050) | 本文介绍了一种名为“ANALOGICAL”的新型基准，用以内在评估LLMs在长文本类比中的能力，包括六个复杂级别的长文本类比分类，并使用13个数据集和三种距离度量方法来评估8个LLMs在语义向量空间中识别类比对的能力。 |
| [^64] | [A transformer-based method for zero and few-shot biomedical named entity recognition.](http://arxiv.org/abs/2305.04928) | 本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。此方法利用预训练学习给定和潜在类别之间的语义关系，将多类标记分类任务转换为二元标记分类，能够在不同数量的样本情况下达到良好的识别效果。 |
| [^65] | [Human-centered trust framework: An HCI perspective.](http://arxiv.org/abs/2305.03306) | 本论文提出了以人为中心的信任框架，帮助非专家实现在人工智能设计中充分利用用户信任的潜力；相关研究发现了某些计算机科学和人工智能话语中的用户信任误解，并进行了有效性评估，该研究的主要贡献是为抵制设计以技术为中心的易受攻击的交互方式的趋势提供了指导。 |
| [^66] | [CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds.](http://arxiv.org/abs/2305.00969) | CryCeleb是一个基于婴儿哭声的说话人认证数据集，包括超过6小时的手动分割哭声，可用于研究婴儿哭声分析。 |
| [^67] | [Constructing a Knowledge Graph from Textual Descriptions of Software Vulnerabilities in the National Vulnerability Database.](http://arxiv.org/abs/2305.00382) | 本文提出了一种从国家漏洞数据库信息中构建漏洞知识图谱的新方法，结合了命名实体识别、关系提取和实体预测。该方法有助于解决网络安全知识图谱中缺失实体的问题。 |
| [^68] | [Improve Video Representation with Temporal Adversarial Augmentation.](http://arxiv.org/abs/2304.14601) | 本文提出了Temporal Adversarial Augmentation（TA），一种利用时间注意力的视频增强技术，可以通过最大化时间相关的损失函数来改变神经网络对视频片段的注意分布。利用TA，我们提出了Temporal Video Adversarial Fine-tuning（TAF）框架，可以有效地改善视频表示并提高神经网络的泛化能力。 |
| [^69] | [Segment anything, from space?.](http://arxiv.org/abs/2304.13000) | 最近开发的Segment Anything Model（SAM）模型可以基于简单的输入提示（如一个或多个点、边界框或掩码）有效分割自然图像中的对象，对视觉研究人员具有重要意义。此项研究探讨SAM在空中图像问题上的卓越性能，并在多项基准任务上进行了验证，表现良好。 |
| [^70] | [Measuring Massive Multitask Chinese Understanding.](http://arxiv.org/abs/2304.12986) | 本研究提出了一项测试，以衡量大型中文语言模型的多任务准确性，测试涵盖医学、法律、心理学和教育四个主要领域，结果表明所有模型在法律领域中表现都很差，建议研究人员应该开发更加多样化和均衡的多任务中文理解模型。 |
| [^71] | [Artificial General Intelligence (AGI) for Education.](http://arxiv.org/abs/2304.12479) | AGI技术具有革命教育领域潜力，可以建立e-learning平台、教育协作工具等，弥补传统AI模型因受限于数据和人际交互限制而无法满足教育需求的不足。 |
| [^72] | [Sequential Recommendation with Probabilistic Logical Reasoning.](http://arxiv.org/abs/2304.11383) | 本文提出了一种结合了概率逻辑推理的序列推荐框架，以解决神经-符号SR存在的问题。通过将特征嵌入和逻辑嵌入分离，SR-PLR结合相似性匹配和逻辑推理，能够更好地捕捉用户口味的不确定性和演变。 |
| [^73] | [How Well Does the Metropolis Algorithm Cope With Local Optima?.](http://arxiv.org/abs/2304.10848) | Metropolis算法的局部搜索策略并不总是优于进化算法，还需要进一步改进和完善。 |
| [^74] | [Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression?.](http://arxiv.org/abs/2304.09868) | 本文介绍了一种通过保留谱的数据压缩来加速支持向量聚类的方法，可以显著提高聚类速度而不牺牲聚类质量。 |
| [^75] | [Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets.](http://arxiv.org/abs/2304.08742) | 本论文提出了一种简单的方法，利用少量下游专家数据从离线未标记数据集中选择性地查询相关行为（包括许多次优行为），实现了行为检索，进而实现了少样本学习。 |
| [^76] | [Learning Empirical Bregman Divergence for Uncertain Distance Representation.](http://arxiv.org/abs/2304.07689) | 本文介绍了一种新的基于Deep Metric Learning的方法，通过学习经验Bregman散度直接从数据中进行不确定距离表示，能够有效的在模式识别和聚类任务上提高准确性。 |
| [^77] | [Learning Reward Machines in Cooperative Multi-Agent Tasks.](http://arxiv.org/abs/2303.14061) | 本文提出了一种新的多智能体强化学习方法，将合作任务分解与学习奖励机制相结合。该方法有助于解决部分可观察环境中奖励的非马尔可夫性质，并提高了学习策略的可解释性，同时也降低了合作任务的复杂性。 |
| [^78] | [Deep RL with Hierarchical Action Exploration for Dialogue Generation.](http://arxiv.org/abs/2303.13465) | 本篇论文提出了一种新的方法，通过分层行为探索，从多个奖励函数中进行离线学习，并成功地解决了在对话生成中行为采样效率低下的问题，可以更好地识别人类情感细节。 |
| [^79] | [Logic of Differentiable Logics: Towards a Uniform Semantics of DL.](http://arxiv.org/abs/2303.10650) | 该论文介绍了一个元语言——LDL，用于定义DL，该元语言从语法和语义两方面上提高DL的形式化程度，使得对DL的性质和实现进行系统比较研究成为了可能。 |
| [^80] | [Welfare Maximization Algorithm for Solving Budget-Constrained Multi-Component POMDPs.](http://arxiv.org/abs/2303.10302) | 该论文提出了一种福利最大化算法，用于解决预算约束下多组件POMDP问题，它通过最优预算分配来计算最优策略。 |
| [^81] | [A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training.](http://arxiv.org/abs/2303.06318) | 本文提出了一种新的混合并行算法，结合了张量、专家和数据并行，以实现MoE模型的训练，其基本模型比当前最先进的DeepSpeed-MoE大4-8倍。 |
| [^82] | [BrainCLIP: Bridging Brain and Visual-Linguistic Representation via CLIP for Generic Natural Visual Stimulus Decoding from fMRI.](http://arxiv.org/abs/2302.12971) | BrainCLIP是一种从fMRI中获取自然图像信息的解码方法，它利用了CLIP跨模态泛化能力并在语义空间中统一视觉刺激分类和重构任务，同时文本监督也能提高解码模型的性能。 |
| [^83] | [Agile Modeling: From Concept to Classifier in Minutes.](http://arxiv.org/abs/2302.12948) | 本文介绍了敏捷建模的概念，即将任何主观视觉概念转化为计算机视觉模型的过程，并通过用户研究表明，用户可以在30分钟内轻松创建分类器。 |
| [^84] | [Sanity checks and improvements for patch visualisation in prototype-based image classification.](http://arxiv.org/abs/2302.08508) | 本文通过精细的数据集，发现了基于原型的视觉分类中可视化方法的局限性，并提出了使用更忠实方法的必要性。 |
| [^85] | [Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training.](http://arxiv.org/abs/2302.05045) | 本文提出了一种新的方法，利用稀疏子网络来优化两种流行的深度学习并行算法 - 数据并行和层间并行的内存利用和通信。在512个NVIDIA V100 GPU上，我们的优化将27亿参数模型的内存消耗减少了74％，总通信时间减少了40％。 |
| [^86] | [State-wise Safe Reinforcement Learning: A Survey.](http://arxiv.org/abs/2302.03122) | 本文综合回顾了强化学习中解决基于状态约束的方法，讨论了它们在安全保障和可扩展性、安全性和奖励表现、收敛后和训练过程中的安全性等方面的联系、差异和权衡，并讨论了未来发展方向。 |
| [^87] | [Evaluating Self-Supervised Learning via Risk Decomposition.](http://arxiv.org/abs/2302.03068) | 通过风险分解，提出四个误差部分评估自监督学习对169个视觉模型的影响，为SSL的设计和使用提供宝贵的见解。 |
| [^88] | [Double Permutation Equivariance for Knowledge Graph Completion.](http://arxiv.org/abs/2302.01313) | 本研究提出了双排列等变性的KG表示方法，可以使神经网络在KG中执行复杂的逻辑推理任务，并在多个归纳KG完成任务中实现了最先进的Hits@10测试准确率。双排列等变性在KG中开辟了新的研究方向。 |
| [^89] | [Dual Personalization on Federated Recommendation.](http://arxiv.org/abs/2301.08143) | 本研究提出了一种新的个性化联邦推荐框架，可以学习轻量级模型并在智能设备上部署，同时实现对用户和物品的精细个性化。 |
| [^90] | [Masked Autoencoding Does Not Help Natural Language Supervision at Scale.](http://arxiv.org/abs/2301.07836) | 本研究旨在探讨大规模训练下掩模自编码和对比语言图像预训练的有效性。研究结果表明，在小规模训练中这两种方法可以有效地结合使用，但在大规模训练中没有明显的优势。 |
| [^91] | [Measuring a Priori Voting Power -- Taking Delegations Seriously.](http://arxiv.org/abs/2301.02462) | 该研究提出了新的权力指数来测量液态民主选举中选民的先验投票权，特别是在底层网络限制委托的情况下。这些指数的计算并不容易，但是对于特定的设置可以在伪多项式时间内计算。 |
| [^92] | [gRoMA: a Tool for Measuring Deep Neural Networks Global Robustness.](http://arxiv.org/abs/2301.02288) | gRoMA是一种衡量DNN全局鲁棒性的创新工具，采用概率验证方法评估特定输出类别遭受到对抗性输入的概率。该工具可运行于预训练的黑盒分类模型上，并对整个模型和每个输入样本产生鲁棒性测量结果。 |
| [^93] | [Causes and Cures for Interference in Multilingual Translation.](http://arxiv.org/abs/2212.07530) | 研究探究了多语言机器翻译中干扰的主要因素，通过系统化试验发现使用不到10亿参数的标准Transformer配置可以在很大程度上缓解干扰并促进协同，同时发现调整采样温度以控制数据中每个语言对所占比例的方法是平衡语言对之间关系的关键。 |
| [^94] | [AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security.](http://arxiv.org/abs/2212.06951) | 本研究分析了Twitter上与MEV相关的话题，结果表明推文讨论了深刻的伦理问题，包括安全、公平、情感情绪和对MEV解决方案的渴望。 |
| [^95] | [Curriculum Learning for Relative Overgeneralization.](http://arxiv.org/abs/2212.02733) | 本论文提出了一种名为相对过度泛化的课程学习（CURO）的新算法来解决多智能体强化学习中存在的相对过度泛化 (RO) 问题，该方法在解决展示强RO的合作任务方面具有很好的表现。 |
| [^96] | [Logic and Commonsense-Guided Temporal Knowledge Graph Completion.](http://arxiv.org/abs/2211.16865) | 本文提出一种"逻辑和常识引导的嵌入模型"（LCGE），通过共同学习时间敏感性表示（涉及时态和因果性的事件表示）与常识角度上的时间无关表示，解决了时间知识图谱补全中表示事件的实时性和因果性的挑战。 |
| [^97] | [RITA: Boost Autonomous Driving Simulators with Realistic Interactive Traffic Flow.](http://arxiv.org/abs/2211.03408) | RITA是一个集成组件，可以提供高质量的交通流，用于测试和优化自动驾驶策略。它由两个核心模块组成，支持真实交互式交通流和易于使用的控制交通流接口。 |
| [^98] | [data2vec-aqc: Search for the right Teaching Assistant in the Teacher-Student training setup.](http://arxiv.org/abs/2211.01246) | 本文提出了一种新的自我监督学习算法data2vec-aqc，用于从未标记的语音数据中学习语音表示，该算法在语音识别领域取得了显著的性能提高。 |
| [^99] | [Online Control of Adaptive Large Neighborhood Search using Deep Reinforcement Learning.](http://arxiv.org/abs/2211.00759) | 本研究提出一种基于深度强化学习的方法来在线控制自适应大邻域搜索算法，该方法能够自适应选择启发式策略、调整参数和控制接受标准，以获得优化问题的良好解，对应用组合优化问题中的实际问题具有重要意义。 |
| [^100] | [A Late Multi-Modal Fusion Model for Detecting Hybrid Spam E-mail.](http://arxiv.org/abs/2210.14616) | 本研究旨在设计一种有效的方法来过滤混合垃圾邮件，提出了一种多模态融合模型，解决了传统基于文本或基于图像的过滤器无法检测到混合垃圾邮件的问题。 |
| [^101] | [What Makes Pre-trained Language Models Better Zero-shot Learners?.](http://arxiv.org/abs/2209.15206) | 本文提出一个理论框架来解释prompt learning在零样本/少样本场景下的有效性，并基于此提出了一个注释无关的模板选择方法。 |
| [^102] | [Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law.](http://arxiv.org/abs/2209.06049) | 本研究针对印度法律文本，重新训练和从零开始训练了两个PLMs，即LegalBERT和CaseLawBERT，并采用基于印度法律文本的词汇表训练了一个模型。我们在几项基准法律NLP任务中，对印度和非印度的法律文本进行了应用。 |
| [^103] | [A Secure and Efficient Multi-Object Grasping Detection Approach for Robotic Arms.](http://arxiv.org/abs/2209.03511) | 本文提出了一种基于深度学习和边缘云协作的安全高效的机械臂抓取方法，通过GAN训练的编码器和解码器实现了图像加密压缩，同时在OCID数据集上获得了92%的准确率，图像压缩比率达到了0.03%。 |
| [^104] | [Integrating Diverse Knowledge Sources for Online One-shot Learning of Novel Tasks.](http://arxiv.org/abs/2208.09554) | 本文研究了利用多元知识源在线一-shot学习新任务的方法，提出了一种代理，通过整合与环境的交互、任务执行和搜索知识、人类自然语言指令以及大型语言模型的响应来学习正确任务知识。结果表明，代理对多元知识源进行的在线整合总体上提高了一-shot任务学习的水平，减少了人类反馈，使任务学习更快速且可靠。 |
| [^105] | [CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation.](http://arxiv.org/abs/2208.08845) | 本研究提出了用于共情对话生成的CASE模型，通过调整用户的粗细粒度的认知和情感的匹配，可以生成更具共情和信息性的响应。 |
| [^106] | [Error in the Euclidean Preference Model.](http://arxiv.org/abs/2208.08160) | 欧几里得偏好模型可能无法接近任意、真实、人类的偏好，甚至有一些情况下几乎所有偏好配置都不能用欧几里得模型表示。 |
| [^107] | [Unveiling the Latent Space Geometry of Push-Forward Generative Models.](http://arxiv.org/abs/2207.10541) | 本文研究了深度生成模型的潜在空间及其与模型性能之间的关系。借助几何测量理论，我们发现了优化的充分条件。我们提出了一种截断方法，可以在潜在空间中强制实行一个简单的簇结构，并提高了模型的性能。 |
| [^108] | [Scalable Polar Code Construction for Successive Cancellation List Decoding: A Graph Neural Network-Based Approach.](http://arxiv.org/abs/2207.01105) | 本文提出了一种基于图神经网络的可扩展极化码构造方法，该方法的优势在于模型复杂度与块长度和码率无关，数值实验表明该方法优于基于排序的方法。 |
| [^109] | [Artificial Intelligence Techniques for Next-Generation Mega Satellite Networks.](http://arxiv.org/abs/2207.00414) | 本文介绍了如何使用人工智能技术来解决超级卫星网络通信中的挑战，包括卫星间链路、短暂时间过程和卫星覆盖范围等问题。 |
| [^110] | [Predicting Hate Intensity of Twitter Conversation Threads.](http://arxiv.org/abs/2206.08406) | 本文提出了DRAGNET++, 通过考虑对话线程的语义、传播结构和用户交互，以预测推文的回复链中可能存在的仇恨程度，并在两个公开数据集上的实验中表现出优越性能。该模型可为社交媒体平台提供在恶意对话升级之前识别和管理的工具。 |
| [^111] | [A Ihara-Bass Formula for Non-Boolean Matrices and Strong Refutations of Random CSPs.](http://arxiv.org/abs/2204.10881) | 本文提出了一种新概念的“非回溯”矩阵并证明了相应的Ihara-Bass型公式，利用该理论证明了随机k-CSP实例的多项式时间强证明的新结果。 |
| [^112] | [COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks.](http://arxiv.org/abs/2204.09593) | COOL是一种上下文观察器，用于编码局部句法上下文，能够改进基于Transformer的模型在自然语言处理任务中的性能，包括问答。 |
| [^113] | [Pyramid Fusion Transformer for Semantic Segmentation.](http://arxiv.org/abs/2201.04019) | 本文提出了一种基于变换器的金字塔融合方法用于语义分割，来挖掘跨特征金字塔的丰富语义信息，取得了有竞争力的表现。 |
| [^114] | [PARIS: Personalized Activity Recommendation for Improving Sleep Quality.](http://arxiv.org/abs/2110.13745) | 该论文利用机器学习技术，结合可穿戴设备监测的数据，通过时间序列聚类找到与指定主题相关的行为模型并生成相应的睡眠质量活动建议，为提高睡眠质量提供了一种个性化解决方案。 |
| [^115] | [AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning.](http://arxiv.org/abs/2110.13005) | AxoNN是一种利用异步性和消息驱动执行调度每个GPU上神经网络操作的并行深度学习框架，将CPU内存作为冗余空间，降低GPU内存消耗，同时将每个GPU的参数数量增加4倍，从而减少了必需的GPU数量和训练时间。 |
| [^116] | [Label-Assemble: Leveraging Multiple Datasets with Partial Labels.](http://arxiv.org/abs/2109.12265) | "Label-Assemble"是一种整合带有Partial Labels的公共数据集来释放其全部潜力的方法。在新型疾病诊断中，从负样本中学习对计算机辅助诊断和检测都非常有帮助。具体而言，将NIH ChestX-ray14中的现有标签组装起来，可以将COVID-19的准确诊断率从96.3％提高到99.3％。此外，整合标签还可以提高疾病的检测能力。 |
| [^117] | [TrAISformer-A generative transformer for AIS trajectory prediction.](http://arxiv.org/abs/2109.03958) | 本文提出了一种新颖的离散、高维表示的AIS数据和一种新的损失函数，用于显式考虑异质性和多样性，并通过修改变压器网络来预测船舶位置。实验证明，该模型明显优于现有的最新方法。 |
| [^118] | [Trade When Opportunity Comes: Price Movement Forecasting via Locality-Aware Attention and Iterative Refinement Labeling.](http://arxiv.org/abs/2107.11972) | 本论文提出了一种名为LARA的新型价格变动预测框架，包括两个主要部分：局部感知注意力和迭代细化标注。在真实世界的金融数据集上的实验结果表明，LARA在准确性和盈利能力方面优于现有的最先进方法。 |
| [^119] | [Bottom-up and top-down approaches for the design of neuromorphic processing systems: Tradeoffs and synergies between natural and artificial intelligence.](http://arxiv.org/abs/2106.01288) | 神经形态工程代表了一种基于脉冲神经网络体系结构实现的计算的范式转变，本文分别比较了自下而上以及自上而下的设计方法，并介绍和讨论了神经形态器件的物理设计和工艺制造、应用等方面。 |
| [^120] | [Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems.](http://arxiv.org/abs/2106.01263) | 论文提出一种新的响应选择范例Uni-Encoder，解决了Cross-Encoder多次编码相同上下文计算成本高和Poly-Encoder性能下降的问题。该范例在一次前向传递中对所有候选与上下文进行编码。 |
| [^121] | [Multi-Task Attentive Residual Networks for Argument Mining.](http://arxiv.org/abs/2102.12227) | 本文提出了一种多任务注意力残差网络架构，通过利用集成方法、注意力机制和多任务学习，无需假设文档或论据结构，成功应用于多个论述挖掘任务中，成为了一种既通用又高性能的架构。 |

# 详细

[^1]: 跨域问答泛化学习

    Learning to Generalize for Cross-domain QA. (arXiv:2305.08208v1 [cs.CL])

    [http://arxiv.org/abs/2305.08208](http://arxiv.org/abs/2305.08208)

    提出了一种不增加训练成本的跨域问答泛化学习方法，通过结合提示方法和线性探测再微调策略，有效提高了产生式和判别式模型的泛化能力，取得了优于基准方法4.5%-7.9%的结果。

    

    自然语言处理模型的跨域泛化能力，尤其是在问答任务中，一直存在着越来越大的担忧。当前的合成数据增强方法受到了增加训练成本的限制。为了解决这个问题，我们提出了一种结合提示方法和线性探测再微调策略的新方法，该方法不需要额外的成本。我们的方法在理论上和实证上都被证明有效，可以增强产生式和判别式模型的泛化能力。我们的方法优于现有的基准方法，F1得分平均提高了4.5%-7.9%。同时，我们的方法可以轻松地集成到任何预训练模型中，并为未充分开发的跨域问答任务提供了有前途的解决方案。我们在GitHub上公开了我们的源代码*。

    There have been growing concerns regarding the out-of-domain generalization ability of natural language processing (NLP) models, particularly in question-answering (QA) tasks. Current synthesized data augmentation methods for QA are hampered by increased training costs. To address this issue, we propose a novel approach that combines prompting methods and linear probing then fine-tuning strategy, which does not entail additional cost. Our method has been theoretically and empirically shown to be effective in enhancing the generalization ability of both generative and discriminative models. Our approach outperforms state-of-the-art baselines, with an average increase in F1 score of 4.5%-7.9%. Furthermore, our method can be easily integrated into any pre-trained models and offers a promising solution to the under-explored cross-domain QA task. We release our source code at GitHub*.
    
[^2]: 一种用于同质周期时间序列数据的异常检测的数据集融合算法

    A Dataset Fusion Algorithm for Generalised Anomaly Detection in Homogeneous Periodic Time Series Datasets. (arXiv:2305.08197v1 [cs.LG])

    [http://arxiv.org/abs/2305.08197](http://arxiv.org/abs/2305.08197)

    该论文介绍了一种名为“数据集融合”的新型数据集合成算法，可将来自多个同质数据集的周期信号融合为单个数据集，同时保留了通用异常检测的独特特征。在两个同质感应电动机（IM）故障数据集的3相电流数据的案例研究中表现出显著优势，有潜力在跨多个来源利用可用数据方面发挥作用。

    

    神经网络在多个数据集上的推广往往被文献忽略，因为NN通常针对特定的数据源进行优化。在基于时间序列的多数据集模型中，由于来自不同传感器和采集规范的连续数据的融合困难，这变得尤其具有挑战性。然而，在商业环境中，通用性可以有效利用可用数据和计算能力，在AI模型的可持续发展之中非常重要。本文介绍了“数据集融合”算法，这是一种将来自多个同质数据集的周期信号融合为单个数据集的新型数据集合成算法，同时保留了通用异常检测的独特特征。经过实验，在使用无监督LSTMCaps NN对来自2个不同同质感应电动机（IM）故障数据集的3相电流数据进行案例研究时，所提出的方法显著优于常规训练方法，显示出在跨多个来源利用可用数据上的潜力。

    The generalisation of Neural Networks (NN) to multiple datasets is often overlooked in literature due to NNs typically being optimised for specific data sources. This becomes especially challenging in time-series-based multi-dataset models due to difficulties in fusing sequential data from different sensors and collection specifications. In a commercial environment, however, generalisation can effectively utilise available data and computational power, which is essential in the context of Green AI, the sustainable development of AI models. This paper introduces "Dataset Fusion," a novel dataset composition algorithm for fusing periodic signals from multiple homogeneous datasets into a single dataset while retaining unique features for generalised anomaly detection. The proposed approach, tested on a case study of 3-phase current data from 2 different homogeneous Induction Motor (IM) fault datasets using an unsupervised LSTMCaps NN, significantly outperforms conventional training approa
    
[^3]: “视觉和更多领域的任意分割模型综合调查”

    A Comprehensive Survey on Segment Anything Model for Vision and Beyond. (arXiv:2305.08196v1 [cs.CV])

    [http://arxiv.org/abs/2305.08196](http://arxiv.org/abs/2305.08196)

    本文综述了最近提出的“任意分割模型”（SAM）的进展，该模型打破了分割界限，提高了计算机视觉基础模型的发展。

    

    人工智能正向人工通用智能演进，即AI系统具备像人一样的广泛任务和智能水平。这与窄化或专用AI相对应，后者旨在以高效的方式执行特定任务。因此，需要设计广泛数据训练的、适用于各种下游任务的基础模型。最近提出的“任意分割模型“（SAM）在打破分割界限方面取得了重大进展，极大地促进了计算机视觉基础模型的发展。为了充分了解SAM，我们进行了一项综合调查研究。作为第一篇基于SAM基础模型全面回顾视觉及其他领域任意分割任务进展的文章，本文着重讨论了它在各种任务和数据类型中的应用。

    Artificial intelligence (AI) is evolving towards artificial general intelligence, which refers to the ability of an AI system to perform a wide range of tasks and exhibit a level of intelligence similar to that of a human being. This is in contrast to narrow or specialized AI, which is designed to perform specific tasks with a high degree of efficiency. Therefore, it is urgent to design a general class of models, which we term foundation models, trained on broad data that can be adapted to various downstream tasks. The recently proposed segment anything model (SAM) has made significant progress in breaking the boundaries of segmentation, greatly promoting the development of foundation models for computer vision. To fully comprehend SAM, we conduct a survey study. As the first to comprehensively review the progress of segmenting anything task for vision and beyond based on the foundation model of SAM, this work focuses on its applications to various tasks and data types by discussing it
    
[^4]: 考虑模式切换点优化的空地机器人路径规划

    Path Planning for Air-Ground Robot Considering Modal Switching Point Optimization. (arXiv:2305.08178v1 [cs.RO])

    [http://arxiv.org/abs/2305.08178](http://arxiv.org/abs/2305.08178)

    本文提出一种考虑模式切换点优化的空地机器人路径规划方法，通过轻量级全局空间规划技术和可互换的搜索方法来提高能源效率、搜索速度和任务执行的可靠性。

    

    空地机器人是一种创新性的移动平台，可以行驶和飞行。传统的空地机器人路径规划技术无法满足灵活的飞行需求。以往研究主要集中在提高路径的能源效率上，很少考虑搜索速度和起降点优化。本文提出了一个面向现场应用环境的机器人，并提出了一种轻量级全局空间规划技术，基于图搜索算法考虑模式切换点优化，强调能源效率、搜索速度和实际部署的可行性。基本思想是通过采用可互换的搜索方法结合平面和空间搜索来降低计算负担。此外，还提供了一种陷阱避免方法来保护电池的健康和任务执行的完整性。通过仿真实验，证明了该方法在减少能量消耗、提高搜索速度和保证安全可靠执行方面的有效性。

    An innovative sort of mobility platform that can both drive and fly is the air-ground robot. The need for an agile flight cannot be satisfied by traditional path planning techniques for air-ground robots. Prior studies had mostly focused on improving the energy efficiency of paths, seldom taking the seeking speed and optimizing take-off and landing places into account. A robot for the field application environment was proposed, and a lightweight global spatial planning technique for the robot based on the graph-search algorithm taking mode switching point optimization into account, with an emphasis on energy efficiency, searching speed, and the viability of real deployment. The fundamental concept is to lower the computational burden by employing an interchangeable search approach that combines planar and spatial search. Furthermore, to safeguard the health of the power battery and the integrity of the mission execution, a trap escape approach was also provided. Simulations are run to 
    
[^5]: ParaLS：基于预训练释义生成器的词汇替换

    ParaLS: Lexical Substitution via Pretrained Paraphraser. (arXiv:2305.08146v1 [cs.CL])

    [http://arxiv.org/abs/2305.08146](http://arxiv.org/abs/2305.08146)

    本研究提出了一种词汇替换方法，使用释义生成器生成替代词候选项，并提出两种解码策略，实验结果表明优于基于预训练语言模型的最新方法。

    

    词汇替换 (LS) 旨在找到句子中目标词的适当替代词。最近，基于预训练语言模型的 LS 方法取得了显著进展，通过分析目标词周围的语境生成潜在的替代词。然而，这些方法在生成替代词时往往忽视了句子含义的保留。本研究探讨了如何从释义生成器中生成替代词候选项，因为释义生成器生成的释义包含了词汇选择的变化并保留了句子的含义。由于我们无法直接通过常用的解码策略生成替代词，因此提出了两种简单的解码策略，专注于解码过程中目标词的变化。实验结果表明，我们的方法在三个基准测试中优于基于预训练语言模型的 LS 最新方法。

    Lexical substitution (LS) aims at finding appropriate substitutes for a target word in a sentence. Recently, LS methods based on pretrained language models have made remarkable progress, generating potential substitutes for a target word through analysis of its contextual surroundings. However, these methods tend to overlook the preservation of the sentence's meaning when generating the substitutes. This study explores how to generate the substitute candidates from a paraphraser, as the generated paraphrases from a paraphraser contain variations in word choice and preserve the sentence's meaning. Since we cannot directly generate the substitutes via commonly used decoding strategies, we propose two simple decoding strategies that focus on the variations of the target word during decoding. Experimental results show that our methods outperform state-of-the-art LS methods based on pre-trained language models on three benchmarks.
    
[^6]: 区分先于回答：生成对比解释作为常识问答的知识

    Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering. (arXiv:2305.08135v1 [cs.CL])

    [http://arxiv.org/abs/2305.08135](http://arxiv.org/abs/2305.08135)

    提出了一种基于概念中心提示的对比解释生成模型CPACE，旨在将获得的符号知识转化为对比解释，用于更好地区分常识问答中的差异。

    

    现有的知识增强方法通过从不同的知识库获取不同的知识，在某些问答任务中取得了显著的成果。然而，受到检索知识的特性限制，它们仍然难以同时从知识相关性和区分性方面受益。为了解决这个挑战，我们提出了CPACE，一种基于概念中心提示的对比解释生成模型，旨在将获得的符号知识转化为对比解释，用于更好地区分给定候选者之间的差异。

    Existing knowledge-enhanced methods have achieved remarkable results in certain QA tasks via obtaining diverse knowledge from different knowledge bases. However, limited by the properties of retrieved knowledge, they still have trouble benefiting from both the knowledge relevance and distinguishment simultaneously. To address the challenge, we propose CPACE, a Concept-centric Prompt-bAsed Contrastive Explanation Generation model, which aims to convert obtained symbolic knowledge into a contrastive explanation for better distinguishing the differences among given candidates. Firstly, following previous works, we retrieve different types of symbolic knowledge with a concept-centric knowledge extraction module. After that, we generate corresponding contrastive explanations using acquired symbolic knowledge and explanation prompts as guidance for better modeling the knowledge distinguishment and interpretability. Finally, we regard the generated contrastive explanation as external knowledg
    
[^7]: Theta序列作为资格追踪：生物学中的信贷分配解决方案

    Theta sequences as eligibility traces: a biological solution to credit assignment. (arXiv:2305.08124v1 [q-bio.NC])

    [http://arxiv.org/abs/2305.08124](http://arxiv.org/abs/2305.08124)

    本论文提出了Theta序列作为生物神经网络中资格追踪的一种解决方案，通过模拟证明了Theta序列可以在不需要维护长时间记忆痕迹的情况下，实现对于预测误差的引导。

    

    信贷分配问题，例如RL中的策略评估，通常需要通过先前的状态或维护时间上延伸的记忆痕迹来引导预测误差;这些解决方案对于神经元的生物网络来说是不利或不可信的。我们提出Theta序列-海马中theta振荡期间的神经活动链，被认为代表清醒行为的快速播放-through，作为解决方案。通过分析和模拟Theta序列的模型，我们证明它们压缩行为，以使现有但短暂的O（10）ms神经记忆痕迹得以有效延伸，从而实现无需长期记忆痕迹的基于资格的追踪，相当于在TD（λ）中使用资格追踪。

    Credit assignment problems, for example policy evaluation in RL, often require bootstrapping prediction errors through preceding states \textit{or} maintaining temporally extended memory traces; solutions which are unfavourable or implausible for biological networks of neurons. We propose theta sequences -- chains of neural activity during theta oscillations in the hippocampus, thought to represent rapid playthroughs of awake behaviour -- as a solution. By analysing and simulating a model for theta sequences we show they compress behaviour such that existing but short $\mathsf{O}(10)$ ms neuronal memory traces are effectively extended allowing for bootstrap-free credit assignment without long memory traces, equivalent to the use of eligibility traces in TD($\lambda$).
    
[^8]: 知识图谱的结构和动态，以及其表层性质

    The Structure and Dynamics of Knowledge Graphs, with Superficiality. (arXiv:2305.08116v1 [cs.AI])

    [http://arxiv.org/abs/2305.08116](http://arxiv.org/abs/2305.08116)

    该论文提出了第一个知识图谱的结构和动态模型，并引入表层性来简单建模复杂特征，从而掌握全局知识分布的平衡。

    

    大型知识图谱综合了从学术机构和企业到大众集资等项目中获得的人类知识，每两个节点之间的关系代表这两个实体之间的基本事实。关系语义的多样性组成了知识图谱的丰富性，导致出现有时混乱的奇异拓扑结构。然而，这种复杂特征可以通过引入表层性的概念来简单建模，表层性控制着独立生成事实的关系之间的重叠情况，也通过确定错误描述实体的比例来控制全局知识分布的平衡。这是知识图谱结构和动态方面的首个模型，有助于更好地理解正式知识获取和组织。

    Large knowledge graphs combine human knowledge garnered from projects ranging from academia and institutions to enterprises and crowdsourcing. Within such graphs, each relationship between two nodes represents a basic fact involving these two entities. The diversity of the semantics of relationships constitutes the richness of knowledge graphs, leading to the emergence of singular topologies, sometimes chaotic in appearance. However, this complex characteristic can be modeled in a simple way by introducing the concept of superficiality, which controls the overlap between relationships whose facts are generated independently. Superficiality also regulates the balance of the global distribution of knowledge by determining the proportion of misdescribed entities. This is the first model for the structure and dynamics of knowledge graphs. It leads to a better understanding of formal knowledge acquisition and organization.
    
[^9]: 《情感人工智能的量子操作》

    Quantum Operation of Affective Artificial Intelligence. (arXiv:2305.08112v1 [cs.AI])

    [http://arxiv.org/abs/2305.08112](http://arxiv.org/abs/2305.08112)

    该论文分析了体验情感人类做出决策的基本原理，比较了量子理论和经典术语的两种方法。论文阐明了内在噪声下量子测量和情感决策制定之间的类比关系，并表明认知过程在形式上与量子测量具有许多相似特征。该论文的最终目的是使得人工智能能够通过只采用经典概念的公理方法进行运算。

    

    该论文分析了人工智能基本原理，以模仿体验情感人类做出决策过程。该论文比较了基于量子理论和基于经典术语的两种方法。这两种方法都具有很多相似之处，主要是概率性的。论文阐明了内在噪声下量子测量和情感决策制定之间的类比关系。论文还表明，认知过程在形式上与量子测量具有许多相似特征。然而，这并不意味着为了模仿人类决策，情感人工智能必须依赖于量子系统的运作。从量子测量和决策制定的共同特征出发，有助于制定只采用经典概念的公理方法，使得人工智能能够运行。

    The review analyzes the fundamental principles which Artificial Intelligence should be based on in order to imitate the realistic process of taking decisions by humans experiencing emotions. Two approaches are compared, one based on quantum theory and the other employing classical terms. Both these approaches have a number of similarities, being principally probabilistic. The analogies between quantum measurements under intrinsic noise and affective decision making are elucidated. It is shown that cognitive processes have many features that are formally similar to quantum measurements. This, however, in no way means that for the imitation of human decision making Affective Artificial Intelligence has necessarily to rely on the functioning of quantum systems. Appreciating the common features between quantum measurements and decision making helps for the formulation of an axiomatic approach employing only classical notions. Artificial Intelligence, following this approach, operates simil
    
[^10]: 基于有限速抹通道的联邦 TD 学习：马尔可夫采样下的线性加速

    Federated TD Learning over Finite-Rate Erasure Channels: Linear Speedup under Markovian Sampling. (arXiv:2305.08104v1 [cs.LG])

    [http://arxiv.org/abs/2305.08104](http://arxiv.org/abs/2305.08104)

    该论文提出了一种适用于联邦学习的算法 QFedTD，在有限速抹通道下使用线性函数逼近以达到线性加速的效果，在强化学习方面具有实际应用价值。

    

    近年来，由于其在通信和隐私约束下加速监督学习任务的有效性，联邦学习 (FL)引起了广泛关注。然而，关于强化学习是否可以实现类似的加速，在理论上仍然不太清楚。针对这个方向，我们研究了一种联邦策略评估问题，在其中代理通过中央聚合器进行通信，以加快共同策略的评估。为了捕捉 FL 中的典型通信约束，我们考虑可以根据伯努利擦拭模型丢弃数据包的有限容量上行链路通道。在这种情况下，我们提出并分析了 QFedTD-一种带有线性函数逼近的量化联邦时序差分学习算法。我们的主要技术贡献是提供 QFedTD 的有限样本分析，该分析突出了量化和抹除对收敛速率的影响；并建立了与代理数量成线性的加速度。(翻译仅供参考，不代表达意完全正确)

    Federated learning (FL) has recently gained much attention due to its effectiveness in speeding up supervised learning tasks under communication and privacy constraints. However, whether similar speedups can be established for reinforcement learning remains much less understood theoretically. Towards this direction, we study a federated policy evaluation problem where agents communicate via a central aggregator to expedite the evaluation of a common policy. To capture typical communication constraints in FL, we consider finite capacity up-link channels that can drop packets based on a Bernoulli erasure model. Given this setting, we propose and analyze QFedTD - a quantized federated temporal difference learning algorithm with linear function approximation. Our main technical contribution is to provide a finite-sample analysis of QFedTD that (i) highlights the effect of quantization and erasures on the convergence rate; and (ii) establishes a linear speedup w.r.t. the number of agents un
    
[^11]: 探究和改进神经机器翻译中知识蒸馏

    Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation. (arXiv:2305.08096v1 [cs.CL])

    [http://arxiv.org/abs/2305.08096](http://arxiv.org/abs/2305.08096)

    本文揭示了神经机器翻译中知识蒸馏的本质，即来自于教师模型的top-1预测。同时，指出了当前基于词级别的知识蒸馏存在的问题，并提出了一种新方法——Top-1 Information。

    

    知识蒸馏在神经机器翻译领域中是一种有前途的模型压缩技术。然而，知识在哪里隐藏的问题仍不清楚，这可能会阻碍知识蒸馏的发展。在本研究中，我们首先从实证角度揭开了这个谜团，并展示了知识来自教师的top-1预测，这也帮助我们建立了词级和序列级蒸馏之间的潜在连接。此外，我们基于这一发现指出了基础词级蒸馏中存在的两个问题。首先，知识的当前目标是将注意力扩散到整个分布上学习知识，但缺乏对最关键的top-1信息的特殊处理。其次，由于大多数教师的top-1预测与地面实况标记重叠，因此知识被黄金信息所占据，进一步限制了知识蒸馏的潜力。为解决这些问题，我们提出了一种名为\textbf{T}op-1 \textbf{I}nformation的新方法。

    Knowledge distillation (KD) is a promising technique for model compression in neural machine translation. However, where the knowledge hides in KD is still not clear, which may hinder the development of KD. In this work, we first unravel this mystery from an empirical perspective and show that the knowledge comes from the top-1 predictions of teachers, which also helps us build a potential connection between word- and sequence-level KD. Further, we point out two inherent issues in vanilla word-level KD based on this finding. Firstly, the current objective of KD spreads its focus to whole distributions to learn the knowledge, yet lacks special treatment on the most crucial top-1 information. Secondly, the knowledge is largely covered by the golden information due to the fact that most top-1 predictions of teachers overlap with ground-truth tokens, which further restricts the potential of KD. To address these issues, we propose a novel method named \textbf{T}op-1 \textbf{I}nformation \te
    
[^12]: 敏捷开发中的人工智能：一项元分析研究

    AI for Agile development: a Meta-Analysis. (arXiv:2305.08093v1 [cs.SE])

    [http://arxiv.org/abs/2305.08093](http://arxiv.org/abs/2305.08093)

    本研究从系统性文献综述和纵向元分析出发，探讨了敏捷软件开发中集成人工智能的益处和挑战，需要进一步研究以全面了解其影响和应对相关挑战。

    

    本研究探讨了在敏捷软件开发方法中集成人工智能以改善持续集成和交付的益处和挑战。通过系统文献综述和纵向元分析来分析人工智能的作用及其在敏捷软件开发中的未来应用。综述发现了许多关键的挑战，例如需要专业的社会技术专业知识。虽然人工智能为软件开发实践带来了希望，但还需要进一步研究来更好地了解其对过程和从业人员的影响，并解决与其实施相关的间接挑战。

    This study explores the benefits and challenges of integrating Artificial Intelligence with Agile software development methodologies, focusing on improving continuous integration and delivery. A systematic literature review and longitudinal meta-analysis of the retrieved studies was conducted to analyse the role of Artificial Intelligence and it's future applications within Agile software development. The review helped identify critical challenges, such as the need for specialised socio-technical expertise. While Artificial Intelligence holds promise for improved software development practices, further research is needed to better understand its impact on processes and practitioners, and to address the indirect challenges associated with its implementation.
    
[^13]: 使基于提示的黑盒调优更加丰富多彩：从三个正交角度提高模型泛化能力

    Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives. (arXiv:2305.08088v1 [cs.CL])

    [http://arxiv.org/abs/2305.08088](http://arxiv.org/abs/2305.08088)

    本文提出了BBT-RGB，一套用于增强黑盒优化效率和性能的直接且互补技术套件，包括两阶段无导数优化策略、自动语言转化器构建及其在少样本设置中的新用法以及更好的提示初始化。

    

    大型语言模型在各种自然语言处理任务中已经展现出越来越强大的能力。然而，调整这些模型以用于下游任务通常需要巨额的代价或由于商业考虑而不可用。最近，提出了黑盒调优来解决这个问题，通过优化任务特定的提示而不访问梯度和隐藏表示。然而，大多数现有的作品还没有充分利用少样本学习场景下无梯度优化的潜力。在本文中，我们描述了BBT-RGB，这是一个用于增强黑盒优化效率和性能的直接且互补技术套件。具体来说，我们的方法包括三个即插即用的组件：（1）两阶段无导数优化策略，有助于快速收敛并缓解过拟合；（2）自动语言转化器构建及其在少样本设置中的新用法；（3）更好的提示初始化，基于未标记数据的语言学动机句法模式。

    Large language models (LLMs) have shown increasing power on various natural language processing (NLP) tasks. However, tuning these models for downstream tasks usually needs exorbitant costs or is unavailable due to commercial considerations. Recently, black-box tuning has been proposed to address this problem by optimizing task-specific prompts without accessing the gradients and hidden representations. However, most existing works have yet fully exploited the potential of gradient-free optimization under the scenario of few-shot learning. In this paper, we describe BBT-RGB, a suite of straightforward and complementary techniques for enhancing the efficiency and performance of black-box optimization. Specifically, our method includes three plug-and-play components: (1) Two-stage derivative-free optimization strategy that facilitates fast convergence and mitigates overfitting; (2) Automatic verbalizer construction with its novel usage under few-shot settings; (3) Better prompt initializ
    
[^14]: 基于连词效应建模的大动作空间离线策略评估

    Off-Policy Evaluation for Large Action Spaces via Conjunct Effect Modeling. (arXiv:2305.08062v1 [stat.ML])

    [http://arxiv.org/abs/2305.08062](http://arxiv.org/abs/2305.08062)

    本研究提出了一个称为OffCEM的估计器，用于对大离散动作空间下上下文匹配策略进行离线策略评估。该估计器通过基于模型的奖励估计来处理残余因果效应，并在新的本地正确性条件下保持无偏性。结果表明，OffCEM在合成和实际大动作空间数据集上优于现有方法。

    

    本文讨论了对于传统重要性加权方法方巨的大离散动作空间下的上下文匹配策略的离线策略评估（OPE）问题。为了解决方巨问题，我们提出了一个新的估计器OffCEM，该方法基于连词效应模型（CEM），这是一种新的因果效应分解方法，可以将效应分为群集效应和残差效应。OffCEM仅对行动群集应用重要性加权，通过基于模型的奖励估计来处理残余因果效应。我们表明，在新的本地正确性条件下，该估计器是无偏的，该条件仅要求残差效应模型保留每个群集中行动的相对期望奖励差异。为了充分利用CEM和本地正确性，我们还提出了一种新的两步过程，用于执行基于模型的估计，第一步最小化偏差，第二步最小化方差。我们发现，所得到的OPE估计器OffCEM在合成和实际大动作空间数据集上都明显优于现有的最先进方法。

    We study off-policy evaluation (OPE) of contextual bandit policies for large discrete action spaces where conventional importance-weighting approaches suffer from excessive variance. To circumvent this variance issue, we propose a new estimator, called OffCEM, that is based on the conjunct effect model (CEM), a novel decomposition of the causal effect into a cluster effect and a residual effect. OffCEM applies importance weighting only to action clusters and addresses the residual causal effect through model-based reward estimation. We show that the proposed estimator is unbiased under a new condition, called local correctness, which only requires that the residual-effect model preserves the relative expected reward differences of the actions within each cluster. To best leverage the CEM and local correctness, we also propose a new two-step procedure for performing model-based estimation that minimizes bias in the first step and variance in the second step. We find that the resulting O
    
[^15]: 两个优于一个：数字孪生以提高自动驾驶测试

    Two is Better Than One: Digital Siblings to Improve Autonomous Driving Testing. (arXiv:2305.08060v1 [cs.SE])

    [http://arxiv.org/abs/2305.08060](http://arxiv.org/abs/2305.08060)

    本文提出了数字孪生的概念，使用不同技术构建多个通用仿真器，强化了自动驾驶软件的基于仿真的测试，提高了测试结果的普适性和可靠性。

    

    基于仿真的测试是确保自动驾驶软件可靠性的重要一步。实际中，当企业依赖第三方通用仿真器进行内部或外包测试时，测试结果的普适性受到威胁。在本文中，我们通过引入“数字孪生”的概念加强了基于仿真的测试，这是一个新颖的框架，在其中AV在多个使用不同技术构建的通用仿真器上进行测试。首先，针对每个单独的仿真器自动生成测试用例。然后，使用特征映射将测试迁移至各个仿真器之间，以表征所进行的行驶条件。最后，计算联合预测失效概率，并仅在孪生之间达成一致的情况下报告故障。我们使用两个开源仿真器实现了该框架，并在数字孪生的物理比例模型上进行了经验比较。

    Simulation-based testing represents an important step to ensure the reliability of autonomous driving software. In practice, when companies rely on third-party general-purpose simulators, either for in-house or outsourced testing, the generalizability of testing results to real autonomous vehicles is at stake.  In this paper, we strengthen simulation-based testing by introducing the notion of digital siblings, a novel framework in which the AV is tested on multiple general-purpose simulators, built with different technologies. First, test cases are automatically generated for each individual simulator. Then, tests are migrated between simulators, using feature maps to characterize of the exercised driving conditions. Finally, the joint predicted failure probability is computed and a failure is reported only in cases of agreement among the siblings.  We implemented our framework using two open-source simulators and we empirically compared it against a digital twin of a physical scaled a
    
[^16]: 事件级视频问答的语义感知动态回顾-前瞻推理

    Semantic-aware Dynamic Retrospective-Prospective Reasoning for Event-level Video Question Answering. (arXiv:2305.08059v1 [cs.CV])

    [http://arxiv.org/abs/2305.08059](http://arxiv.org/abs/2305.08059)

    本文提出了一种用于视频问答的语义感知动态回顾-前瞻推理方法，通过明确使用问题的语义角色标注（SRL）结构，根据问题SRL结构的哪一部分被关注来促进跨视频帧的复杂推理，实验结果显示性能优越。

    

    事件级视频问答需要跨越视频事件进行复杂推理，以获取提供最佳答案所需的视觉信息。然而，尽管模型性能取得了显著进展，但很少有研究关注在事件层面上使用问题和视觉信息之间的显式语义联系。有必要利用这种语义联系，促进跨视频帧的复杂推理。因此，我们提出了一种面向视频问答的语义感知动态回顾-前瞻推理方法。具体来说，我们在动态推理过程中明确使用问题的语义角色标注（SRL）结构，根据问题SRL结构的哪一部分（agent、verb、patient等）被关注来决定是否移动到下一帧。我们在基准EVQA数据集——TrafficQA上进行了实验。结果表明，我们提出的方法取得了卓越的性能。

    Event-Level Video Question Answering (EVQA) requires complex reasoning across video events to obtain the visual information needed to provide optimal answers. However, despite significant progress in model performance, few studies have focused on using the explicit semantic connections between the question and visual information especially at the event level. There is need for using such semantic connections to facilitate complex reasoning across video frames. Therefore, we propose a semantic-aware dynamic retrospective-prospective reasoning approach for video-based question answering. Specifically, we explicitly use the Semantic Role Labeling (SRL) structure of the question in the dynamic reasoning process where we decide to move to the next frame based on which part of the SRL structure (agent, verb, patient, etc.) of the question is being focused on. We conduct experiments on a benchmark EVQA dataset - TrafficQA. Results show that our proposed approach achieves superior performance 
    
[^17]: 令人惊讶的简单连续行动POMDP求解器：基于策略树的懒惰交叉熵搜索

    A Surprisingly Simple Continuous-Action POMDP Solver: Lazy Cross-Entropy Search Over Policy Trees. (arXiv:2305.08049v1 [cs.AI])

    [http://arxiv.org/abs/2305.08049](http://arxiv.org/abs/2305.08049)

    提出了一种称为LCEOPT的简单在线连续动作POMDP求解器，利用基于策略树的懒惰交叉熵搜索实现了高效解决具有连续动作空间的挑战性POMDP问题，相较于以往最先进的POMDP求解器可实现高达两个数量级的计算节省。

    

    部分可观察马尔可夫决策过程（POMDP）提供了在随机部分可观察环境中进行决策的原则性框架。但对于具有连续动作空间的问题提供良好解决方案仍然具有挑战性。为了简化这个挑战，我们提出了一种称为Lazy Cross-Entropy Search Over Policy Trees (LCEOPT) 的简单在线POMDP求解器。我们的方法在每个计划步骤中使用懒惰交叉熵方法来搜索策略树空间，该树提供了一种简单的策略表示。具体而言，我们维护一个分布在有前途的有限时间策略树上的分布。通过抽样策略、通过蒙特卡罗模拟评估它们并将它们重新拟合到表现最佳的策略上，迭代更新此分布。我们的方法是懒惰的，因为它利用策略树表示来避免策略抽样、评估和分布更新中的冗余计算。与以前针对连续行动空间的最先进的POMDP求解器相比，这导致可节省高达两个数量级的计算。我们的实验表明，LCEOPT可以高精度和高效地解决具有挑战性的连续行动POMDP。

    The Partially Observable Markov Decision Process (POMDP) provides a principled framework for decision making in stochastic partially observable environments. However, computing good solutions for problems with continuous action spaces remains challenging. To ease this challenge, we propose a simple online POMDP solver, called Lazy Cross-Entropy Search Over Policy Trees (LCEOPT). At each planning step, our method uses a lazy Cross-Entropy method to search the space of policy trees, which provide a simple policy representation. Specifically, we maintain a distribution on promising finite-horizon policy trees. The distribution is iteratively updated by sampling policies, evaluating them via Monte Carlo simulation, and refitting them to the top-performing ones. Our method is lazy in the sense that it exploits the policy tree representation to avoid redundant computations in policy sampling, evaluation, and distribution update. This leads to computational savings of up to two orders of magn
    
[^18]: 探索图神经网络泛化的理解

    Towards Understanding the Generalization of Graph Neural Networks. (arXiv:2305.08048v1 [cs.LG])

    [http://arxiv.org/abs/2305.08048](http://arxiv.org/abs/2305.08048)

    本文探索了图神经网络的泛化；通过理论分析和实验结果发现了影响泛化间隙的体系结构因素。

    

    图神经网络是应用于图结构数据的学习和表示中最广泛使用的模型。尽管它们在实际应用中取得了非凡的成功，但从理论上理解它们的工作机制仍处于初级阶段。本文从泛化的角度出发，首先考虑了随机优化的情况下传递学习的泛化间隙和梯度的高概率界限。其次，为流行的GNN提供了泛化间隙的高概率上限。理论结果揭示了影响泛化间隙的体系结构特定因素。基准数据集上的实验结果显示了理论结果和经验证据之间的一致性。我们的结果提供了理解GNN泛化的新思路。

    Graph neural networks (GNNs) are the most widely adopted model in graph-structured data oriented learning and representation. Despite their extraordinary success in real-world applications, understanding their working mechanism by theory is still on primary stage. In this paper, we move towards this goal from the perspective of generalization. To be specific, we first establish high probability bounds of generalization gap and gradients in transductive learning with consideration of stochastic optimization. After that, we provide high probability bounds of generalization gap for popular GNNs. The theoretical results reveal the architecture specific factors affecting the generalization gap. Experimental results on benchmark datasets show the consistency between theoretical results and empirical evidence. Our results provide new insights in understanding the generalization of GNNs.
    
[^19]: 基于随机池化的可证明多实例深度AUC最大化方法

    Provable Multi-instance Deep AUC Maximization with Stochastic Pooling. (arXiv:2305.08040v1 [cs.LG])

    [http://arxiv.org/abs/2305.08040](http://arxiv.org/abs/2305.08040)

    本文提出了在多实例学习中使用深度AUC最大化（DAM）的方法，并根据包含大量实例的情况下训练的计算挑战，提出了一种基于方差减少的随机池化方法，使得只需对每个包进行少量采样即可计算MIDAM模型，提高了效率和准确性。

    

    本文提出了一种深度AUC最大化（DAM）的新型应用，用于多实例学习（MIL），其中将单个类标签分配给一组实例（例如，患者的多个CT扫描的多个2D切片）。我们在DAM的背景下解决了MIL中被忽略但非常重要的计算挑战，即包大小过大，无法在反向传播时加载到GPU内存中，这是MIL标准池化方法所必需的。为了解决这个问题，我们提出了一种基于方差减少的随机池化方法，这种方法可以将关于汇聚预测的损失函数构造为多级组合函数。通过综合随机组合优化和非凸极小最大优化技术，我们提出了一种统一且可证明的多实例DAM（MIDAM）算法，其使用随机平滑最大池化或随机注意力池化，仅对每个包对应的实例进行少量采样来计算 sto。

    This paper considers a novel application of deep AUC maximization (DAM) for multi-instance learning (MIL), in which a single class label is assigned to a bag of instances (e.g., multiple 2D slices of a CT scan for a patient). We address a neglected yet non-negligible computational challenge of MIL in the context of DAM, i.e., bag size is too large to be loaded into {GPU} memory for backpropagation, which is required by the standard pooling methods of MIL. To tackle this challenge, we propose variance-reduced stochastic pooling methods in the spirit of stochastic optimization by formulating the loss function over the pooled prediction as a multi-level compositional function. By synthesizing techniques from stochastic compositional optimization and non-convex min-max optimization, we propose a unified and provable muli-instance DAM (MIDAM) algorithm with stochastic smoothed-max pooling or stochastic attention-based pooling, which only samples a few instances for each bag to compute a sto
    
[^20]: DNN-Defender: 一种用于对抗 Adversarial Weight Attack 的内存中深度神经网络防御机制

    DNN-Defender: An in-DRAM Deep Neural Network Defense Mechanism for Adversarial Weight Attack. (arXiv:2305.08034v1 [cs.CR])

    [http://arxiv.org/abs/2305.08034](http://arxiv.org/abs/2305.08034)

    DNN-Defender是一种基于DRAM的受害者重点防御机制，适用于量化DNN，利用内存中交换的潜力以抵御有针对性的位翻转攻击，可以提供高水平的保护。

    

    随着深度学习在许多安全敏感领域中的部署，机器学习安全变得越来越重要。最近的研究表明，攻击者可以利用DRAM的RowHammer漏洞，以确定性和精确性地翻转深度神经网络(DNN)模型权重的位，从而影响推断准确性。现有的防御机制是基于软件的，例如重构权重需要昂贵的训练开销或性能下降。另一方面，基于通用硬件的受害者/攻击者重点防御机制会导致昂贵的硬件开销，并保留受害者和攻击者行之间的空间连接。在本文中，我们提出了第一种针对量化DNN量身定制的基于DRAM的受害者重点防御机制，称为DNN-Defender，利用了内存中交换的潜力以抵御有针对性的位翻转攻击。我们的结果表明，DNN-Defender可以提供高水平的保护。

    With deep learning deployed in many security-sensitive areas, machine learning security is becoming progressively important. Recent studies demonstrate attackers can exploit system-level techniques exploiting the RowHammer vulnerability of DRAM to deterministically and precisely flip bits in Deep Neural Networks (DNN) model weights to affect inference accuracy. The existing defense mechanisms are software-based, such as weight reconstruction requiring expensive training overhead or performance degradation. On the other hand, generic hardware-based victim-/aggressor-focused mechanisms impose expensive hardware overheads and preserve the spatial connection between victim and aggressor rows. In this paper, we present the first DRAM-based victim-focused defense mechanism tailored for quantized DNNs, named DNN-Defender that leverages the potential of in-DRAM swapping to withstand the targeted bit-flip attacks. Our results indicate that DNN-Defender can deliver a high level of protection dow
    
[^21]: 提高视觉Transformer的鲁棒性：防御性扩散

    On enhancing the robustness of Vision Transformers: Defensive Diffusion. (arXiv:2305.08031v1 [cs.CV])

    [http://arxiv.org/abs/2305.08031](http://arxiv.org/abs/2305.08031)

    本文提出了一种防御性扩散技术作为对抗性净化器，以消除攻击者在原始图像中引入的对抗性噪声，从而提高了ViT模型在医疗应用中的鲁棒性和可靠性。

    

    医疗数据的隐私和保密性在医疗保健环境中至关重要。ViTs是目前最先进的视觉模型，依赖于大量的患者数据进行训练，这引发了有关数据安全和潜在未经授权的访问的担忧。攻击者可能利用ViTs中的漏洞提取敏感患者信息，从而危及患者隐私。本文针对这些漏洞，以确保ViTs在医疗应用中的可信性和可靠性，引入了一种防御性扩散技术作为对抗性净化器，以消除攻击者在原始图像中引入的对抗性噪声。通过利用扩散模型的去噪能力，我们采用反向扩散过程有效地消除攻击示例中的对抗性噪声，从而得到一个更干净的图像，然后将其输入ViT模块。我们的研究结果表明，扩散模型在消除对抗性攻击扰动方面非常有效，从而提高了医疗应用中ViT模型的鲁棒性和可靠性。

    Privacy and confidentiality of medical data are of utmost importance in healthcare settings. ViTs, the SOTA vision model, rely on large amounts of patient data for training, which raises concerns about data security and the potential for unauthorized access. Adversaries may exploit vulnerabilities in ViTs to extract sensitive patient information and compromising patient privacy. This work address these vulnerabilities to ensure the trustworthiness and reliability of ViTs in medical applications. In this work, we introduced a defensive diffusion technique as an adversarial purifier to eliminate adversarial noise introduced by attackers in the original image. By utilizing the denoising capabilities of the diffusion model, we employ a reverse diffusion process to effectively eliminate the adversarial noise from the attack sample, resulting in a cleaner image that is then fed into the ViT blocks. Our findings demonstrate the effectiveness of the diffusion model in eliminating attack-agnost
    
[^22]: SongDriver2：基于情绪的实时音乐编排与柔和过渡

    SongDriver2: Real-time Emotion-based Music Arrangement with Soft Transition. (arXiv:2305.08029v1 [cs.SD])

    [http://arxiv.org/abs/2305.08029](http://arxiv.org/abs/2305.08029)

    SongDriver2实现了基于情绪的实时音乐编排，并提出了柔和过渡机制，使音乐具有高度真实性和平滑过渡。

    

    基于情绪的实时音乐编排旨在将给定的音乐转化为另一个能够实时引起用户特定情感共鸣的音乐，在音乐疗法、游戏配乐和电影配乐等各种场景中具有重要应用价值。然而，由于目标情感的细粒度和可变性，平衡情感实时匹配和柔和情感转换是一项挑战。现有的研究主要集中在实现情感实时匹配，而柔和过渡的问题仍未得到充分研究，影响了音乐的整体情感一致性。本文提出了SongDriver2来解决这个问题。具体地，我们首先识别最后一个时间步的音乐情绪，然后将其与当前时间步的目标输入情绪融合。融合的情感随后作为SongDriver2根据输入旋律数据生成即将到来的音乐的指导。为了调整音乐相似性和情感实时匹配，以实现两种不同情感之间的过渡，我们设计了一种软过渡机制，将插值和平滑滤波器相结合。我们证明，所提出的SongDriver2可以生成具有高度真实性和平滑过渡的情感音乐，这表明其在基于情绪的实时音乐编排应用中具有潜在价值。

    Real-time emotion-based music arrangement, which aims to transform a given music piece into another one that evokes specific emotional resonance with the user in real-time, holds significant application value in various scenarios, e.g., music therapy, video game soundtracks, and movie scores. However, balancing emotion real-time fit with soft emotion transition is a challenge due to the fine-grained and mutable nature of the target emotion. Existing studies mainly focus on achieving emotion real-time fit, while the issue of soft transition remains understudied, affecting the overall emotional coherence of the music. In this paper, we propose SongDriver2 to address this balance. Specifically, we first recognize the last timestep's music emotion and then fuse it with the current timestep's target input emotion. The fused emotion then serves as the guidance for SongDriver2 to generate the upcoming music based on the input melody data. To adjust music similarity and emotion real-time fit f
    
[^23]: DRew：带延迟的动态重连消息传递

    DRew: Dynamically Rewired Message Passing with Delay. (arXiv:2305.08018v1 [cs.LG])

    [http://arxiv.org/abs/2305.08018](http://arxiv.org/abs/2305.08018)

    本文提出了一种能够应用于任何MPNN结构的框架，执行基于层的动态重连来确保逐渐密集化的图形。同时引入了一种延迟机制，允许跨层节点之间的跳跃连接。

    

    已经证明，消息传递神经网络（MPNN）存在过度压缩现象，导致长程相互作用任务表现不佳。这主要归因于只在节点的相邻居之间进行局部消息传递。试图使图形“更连通”并且更适合长程任务的重连方法通常会失去基于图形距离提供的归纳偏差，因为它们会使远程节点在每一层中立即通信。在本文中，我们提出了一个框架，可应用于任何MPNN架构，以执行基于层的重连，以确保逐渐加密图形。我们还提出了一种延迟机制，它允许根据层和它们的相互距离在节点之间进行跳跃连接。我们在几个长程任务上验证了我们的方法，并表明其优于图形变换器和多跳MPNN。

    Message passing neural networks (MPNNs) have been shown to suffer from the phenomenon of over-squashing that causes poor performance for tasks relying on long-range interactions. This can be largely attributed to message passing only occurring locally, over a node's immediate neighbours. Rewiring approaches attempting to make graphs `more connected', and supposedly better suited to long-range tasks, often lose the inductive bias provided by distance on the graph since they make distant nodes communicate instantly at every layer. In this paper we propose a framework, applicable to any MPNN architecture, that performs a layer-dependent rewiring to ensure gradual densification of the graph. We also propose a delay mechanism that permits skip connections between nodes depending on the layer and their mutual distance. We validate our approach on several long-range tasks and show that it outperforms graph Transformers and multi-hop MPNNs.
    
[^24]: 基于表面肌电图像的轻量级全卷积神经网络和迁移学习的跨场景手势识别

    Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning. (arXiv:2305.08014v1 [cs.CV])

    [http://arxiv.org/abs/2305.08014](http://arxiv.org/abs/2305.08014)

    本论文提出了一种轻量级全卷积神经网络+迁移学习方法，能够在跨场景手势识别任务中有效解决数据变异性问题。

    

    利用低分辨率瞬时高清肌电图像进行手势识别可以开辟发展更流畅、更自然的肌肉-计算机界面的新途径。然而，跨场景数据的变异性存在极大的挑战。现有的方法采用非常大且复杂的深度卷积神经网络或基于2SRNN的领域适应方法，来逼近由这些跨场景数据变异性引起的分布偏移。因此，这些方法也需要在预训练和适应阶段中在数百万个训练参数和大规模预训练数据集上进行学习。结果，这使得在实时应用中进行高端资源约束和计算非常昂贵的部署。为了解决这个问题，我们提出了一种轻量级的全卷积神经网络+迁移学习模型，利用轻量级全卷积神经网络和迁移学习(TL)来增强跨场景手势识别。

    Gesture recognition using low-resolution instantaneous HD-sEMG images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the data variability between inter-session and inter-subject scenarios presents a great challenge. The existing approaches employed very large and complex deep ConvNet or 2SRNN-based domain adaptation methods to approximate the distribution shift caused by these inter-session and inter-subject data variability. Hence, these methods also require learning over millions of training parameters and a large pre-trained and target domain dataset in both the pre-training and adaptation stages. As a result, it makes high-end resource-bounded and computationally very expensive for deployment in real-time applications. To overcome this problem, we propose a lightweight All-ConvNet+TL model that leverages lightweight All-ConvNet and transfer learning (TL) for the enhancement of inter-session and inter-subject gesture recogniti
    
[^25]: 超越保障：探索ChatGPT的安全风险

    Beyond the Safeguards: Exploring the Security Risks of ChatGPT. (arXiv:2305.08005v1 [cs.CR])

    [http://arxiv.org/abs/2305.08005](http://arxiv.org/abs/2305.08005)

    本文探讨了ChatGPT的多种安全风险和绕过保护措施的潜在方法，发现即使保护措施存在，LLMs仍存在伦理和安全风险。本研究为减轻这些风险提供了潜在策略。

    

    越来越多的人开始关注大型语言模型（LLMs）如ChatGPT的安全性、安全风险和伦理影响。本文旨在提供有关ChatGPT相关的不同类型安全风险的概述，包括恶意文本和代码生成、私人数据披露、欺诈性服务、信息搜集和生成不道德内容等。我们进行了实证研究，检查了ChatGPT内容过滤器的有效性，并探讨了绕过这些保护的潜在方法，展示了当保护措施存在时LLMs中仍存在的伦理和安全风险。根据安全风险的定性分析，我们讨论了减轻这些风险的潜在策略，并向研究人员、政策制定者和行业专业人员介绍了像ChatGPT这样的LLMs所面临的复杂安全挑战。本研究有助于对LLMs带来的伦理和安全问题进行持续的讨论。

    The increasing popularity of large language models (LLMs) such as ChatGPT has led to growing concerns about their safety, security risks, and ethical implications. This paper aims to provide an overview of the different types of security risks associated with ChatGPT, including malicious text and code generation, private data disclosure, fraudulent services, information gathering, and producing unethical content. We present an empirical study examining the effectiveness of ChatGPT's content filters and explore potential ways to bypass these safeguards, demonstrating the ethical implications and security risks that persist in LLMs even when protections are in place. Based on a qualitative analysis of the security implications, we discuss potential strategies to mitigate these risks and inform researchers, policymakers, and industry professionals about the complex security challenges posed by LLMs like ChatGPT. This study contributes to the ongoing discussion on the ethical and security 
    
[^26]: 论随机安全性的计算成本

    On the Computational Cost of Stochastic Security. (arXiv:2305.07973v1 [cs.LG])

    [http://arxiv.org/abs/2305.07973](http://arxiv.org/abs/2305.07973)

    本文探究了使用长期持续蒙特卡罗模拟是否能提高能量模型的质量，并通过增加计算预算改进了模型的校准性和对抗鲁棒性。

    

    我们探讨了使用朗之万动力学的长期持续蒙特卡罗模拟是否会提高基于能量的模型（EBM）所达到的表征质量。我们考虑一种方案，其中使用训练过的EBM的扩散过程的蒙特卡罗模拟，用于提高独立分类器网络的对抗鲁棒性和校准分数。我们的结果表明，在持续对比散度的计算预算增加吉布斯采样的情况下，改进了模型的校准性和对抗鲁棒性，澄清了实现有效从连续能量势中进行吉布斯采样的新量子和经典硬件和软件的实际价值。

    We investigate whether long-run persistent chain Monte Carlo simulation of Langevin dynamics improves the quality of the representations achieved by energy-based models (EBM). We consider a scheme wherein Monte Carlo simulation of a diffusion process using a trained EBM is used to improve the adversarial robustness and the calibration score of an independent classifier network. Our results show that increasing the computational budget of Gibbs sampling in persistent contrastive divergence improves the calibration and adversarial robustness of the model, elucidating the practical merit of realizing new quantum and classical hardware and software for efficient Gibbs sampling from continuous energy potentials.
    
[^27]: 利用实验经济学研究大型语言模型中出现的类似目标行为

    Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics. (arXiv:2305.07970v1 [cs.GT])

    [http://arxiv.org/abs/2305.07970](http://arxiv.org/abs/2305.07970)

    本研究探讨了大型语言模型的能力，发现其可以将自然语言描述转化为适当的行为，但在区分细微的合作和竞争水平方面的能力受到限制，为使用LLMs在人类决策制定背景下的伦理意义和局限性做出了贡献。

    

    本研究探讨了大型语言模型（LLMs），特别是GPT-3.5，实现合作、竞争、利他和自私行为的自然语言描述在社会困境下的能力。我们聚焦于迭代囚徒困境，这是一个非零和互动的经典例子，但我们的更广泛研究计划包括一系列实验经济学场景，包括最后通牒博弈、独裁者博弈和公共物品游戏。使用被试内实验设计，我们运用不同的提示信息实例化由LLM生成的智能体，表达不同的合作和竞争立场。我们评估了智能体在迭代囚徒困境中的合作水平，同时考虑到它们对合作或出尔反尔的伙伴行动的响应。我们的结果表明，LLMs在某种程度上可以将利他和自私的自然语言描述转化为适当的行为，但展示出区分合作和竞争水平的能力有限。总体而言，我们的研究为在人类决策制定的背景下使用LLMs的伦理意义和局限性提供了证据。

    In this study, we investigate the capacity of large language models (LLMs), specifically GPT-3.5, to operationalise natural language descriptions of cooperative, competitive, altruistic, and self-interested behavior in social dilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of a non-zero-sum interaction, but our broader research program encompasses a range of experimental economics scenarios, including the ultimatum game, dictator game, and public goods game. Using a within-subject experimental design, we instantiated LLM-generated agents with various prompts that conveyed different cooperative and competitive stances. We then assessed the agents' level of cooperation in the iterated Prisoner's Dilemma, taking into account their responsiveness to the cooperative or defection actions of their partners. Our results provide evidence that LLMs can translate natural language descriptions of altruism and selfishness into appropriate behaviour to some extent, but e
    
[^28]: 更少的数据更强的安全策略优化性能保证

    More for Less: Safe Policy Improvement With Stronger Performance Guarantees. (arXiv:2305.07958v1 [cs.LG])

    [http://arxiv.org/abs/2305.07958](http://arxiv.org/abs/2305.07958)

    本论文提出了一种新的方法，可以用较少的数据保证安全策略优化(SPI)的性能，并降低了SPIBB算法的样本复杂度。

    

    在离线强化学习环境中，安全策略优化(SPI)问题旨在根据生成样本数据的行为策略，提高其性能。现有的解决SPI问题的方法需要较高数量的样本，以提供对改进策略性能的实际概率保证。我们提出了一种新的方法来解决SPI问题，用较少的数据即可获得这样的保证。具体来说，为了证明这些保证的正确性，我们设计了数据集和基础环境模型上的隐式转换，这些转换作为推导更紧密的SPI改进界限的理论基础。我们使用专业的SPI与基线引导(SPIBB)算法在标准基准测试中进行实证评估，结果表明我们的方法确实显著降低了SPIBB算法的样本复杂度。

    In an offline reinforcement learning setting, the safe policy improvement (SPI) problem aims to improve the performance of a behavior policy according to which sample data has been generated. State-of-the-art approaches to SPI require a high number of samples to provide practical probabilistic guarantees on the improved policy's performance. We present a novel approach to the SPI problem that provides the means to require less data for such guarantees. Specifically, to prove the correctness of these guarantees, we devise implicit transformations on the data set and the underlying environment model that serve as theoretical foundations to derive tighter improvement bounds for SPI. Our empirical evaluation, using the well-established SPI with baseline bootstrapping (SPIBB) algorithm, on standard benchmarks shows that our method indeed significantly reduces the sample complexity of the SPIBB algorithm.
    
[^29]: AMTSS：一种适用于多语言语言推理的自适应多教师单学生知识蒸馏框架

    AMTSS: An Adaptive Multi-Teacher Single-Student Knowledge Distillation Framework For Multilingual Language Inference. (arXiv:2305.07928v1 [cs.CL])

    [http://arxiv.org/abs/2305.07928](http://arxiv.org/abs/2305.07928)

    AMTSS是一种自适应多教师单学生知识蒸馏框架，可以在多语言设置下支持成本效益的语言推理，并取得了在XNLI数据集和AliExpress中竞争性的结果。

    

    知识蒸馏对于推出多语言预训练语言模型的实际应用至关重要。为支持多语言设置下的成本效益的语言推理，我们提出了AMTSS，一种自适应多教师单学生蒸馏框架，它允许从多个老师中提炼知识到一个学生中。我们首先引入一种自适应学习策略和教师重要性权重，使学生能够有效地从最大边缘老师中学习，并轻松适应新语言。此外，我们提出了一个共享的学生编码器，以不同的投影层支持多种语言，这有助于大大降低开发和机器成本。实验结果显示，AMTSS在公共XNLI数据集和真实的电子商务行业数据集AliExpress（AE）中获得了竞争性的结果。

    Knowledge distillation is of key importance to launching multilingual pre-trained language models for real applications. To support cost-effective language inference in multilingual settings, we propose AMTSS, an adaptive multi-teacher single-student distillation framework, which allows distilling knowledge from multiple teachers to a single student. We first introduce an adaptive learning strategy and teacher importance weight, which enables a student to effectively learn from max-margin teachers and easily adapt to new languages. Moreover, we present a shared student encoder with different projection layers in support of multiple languages, which contributes to largely reducing development and machine cost. Experimental results show that AMTSS gains competitive results on the public XNLI dataset and the realistic industrial dataset AliExpress (AE) in the E-commerce scenario.
    
[^30]: 基于提示的预训练语言模型用于时间知识图谱补全

    Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion. (arXiv:2305.07912v1 [cs.CL])

    [http://arxiv.org/abs/2305.07912](http://arxiv.org/abs/2305.07912)

    这篇论文提出了一种基于提示的预训练语言模型（PPT），用于时间知识图谱补全。通过遮盖策略，将TKGC任务转换为遮盖词预测任务，可以利用预训练语言模型中的语义信息。

    

    时间知识图谱补全（TKGC）是一项重要的任务，它涉及在已知的时间戳上进行推理，以完成缺失部分的事实，并在近年来越来越受到关注。大多数现有方法都集中于基于图神经网络的学习表示，同时粗略地提取时间戳中的信息，并不充分利用关系中隐含的信息。为了解决这些问题，我们提出了一种新的TKGC模型，即基于提示的预训练语言模型（PPT）。我们将一系列采样的四元组转换为预训练语言模型的输入，并将时间戳之间的间隔转换为不同的提示，以形成带有隐含语义信息的连贯句子。我们使用遮盖策略训练我们的模型，将TKGC任务转换为遮盖词预测任务，从而可以利用预训练语言模型中的语义信息。实验结果和广泛的分析表明，

    Temporal Knowledge graph completion (TKGC) is a crucial task that involves reasoning at known timestamps to complete the missing part of facts and has attracted more and more attention in recent years. Most existing methods focus on learning representations based on graph neural networks while inaccurately extracting information from timestamps and insufficiently utilizing the implied information in relations. To address these problems, we propose a novel TKGC model, namely Pre-trained Language Model with Prompts for TKGC (PPT). We convert a series of sampled quadruples into pre-trained language model inputs and convert intervals between timestamps into different prompts to make coherent sentences with implicit semantic information. We train our model with a masking strategy to convert TKGC task into a masked token prediction task, which can leverage the semantic information in pre-trained language models. Experiments on three benchmark datasets and extensive analysis demonstrate that 
    
[^31]: 将SUMO-K翻译至高阶集合论

    Translating SUMO-K to Higher-Order Set Theory. (arXiv:2305.07903v1 [cs.AI])

    [http://arxiv.org/abs/2305.07903](http://arxiv.org/abs/2305.07903)

    本文介绍了将SUMO-K翻译成高阶集合论的方法，并为超越一阶逻辑的SUMO部分提供了形式化语义。同时也嵌入了一个大型的常识本体论到一个安全的互动定理证明系统中，这可用于创建一些高阶常识推理问题。

    

    我们将SUMO的一个子集（SUMO-K）翻译成高阶集合论。该翻译为超越一阶逻辑的SUMO部分提供了形式语义，这些部分以前只有非正式的解释。同时，它还首次将一个大型的常识本体论嵌入一个非常安全的互动定理证明系统中。我们还扩展了之前的工作，从一阶结构中找到SUMO中的矛盾，将其包括高阶结构的一部分。最后，利用这个翻译，我们可以创建一些可使用高阶互动式和自动定理证明器证明的问题。这在几个系统中得到测试，并可用于形成一个高阶常识推理问题语料库。

    We describe a translation from a fragment of SUMO (SUMO-K) into higher-order set theory. The translation provides a formal semantics for portions of SUMO which are beyond first-order and which have previously only had an informal interpretation. It also for the first time embeds a large common-sense ontology into a very secure interactive theorem proving system. We further extend our previous work in finding contradictions in SUMO from first order constructs to include a portion of SUMO's higher order constructs. Finally, using the translation, we can create problems that can be proven using higher-order interactive and automated theorem provers. This is tested in several systems and can be used to form a corpus of higher-order common-sense reasoning problems.
    
[^32]: PESTS: 波斯语-英语跨语言语料库用于语义文本相似度

    PESTS: Persian_English Cross Lingual Corpus for Semantic Textual Similarity. (arXiv:2305.07893v1 [cs.CL])

    [http://arxiv.org/abs/2305.07893](http://arxiv.org/abs/2305.07893)

    本研究提出了跨语言的语义相似性模型PESTS，并通过波斯语-英语的跨语言语料库来验证模型的准确性。

    

    近来，语义文本相似度成为自然语言处理中备受关注的组件。在计算语言学和自然语言处理中，评估单词、短语、段落和文本之间的语义相似性很重要。同时，语义相似性度量要求在源和目标语言中提供具有一定语义相似性的句子对。许多跨语言的语义相似度模型使用机器翻译来弥补跨语言语料库不可用的不足，但机器翻译的误差会降低模型的准确性。然而，在使用语义相似度特征实现机器翻译时，用相同的机器翻译模型可以提高结果的准确性。

    One of the components of natural language processing that has received a lot of investigation recently is semantic textual similarity. In computational linguistics and natural language processing, assessing the semantic similarity of words, phrases, paragraphs, and texts is crucial. Calculating the degree of semantic resemblance between two textual pieces, paragraphs, or phrases provided in both monolingual and cross-lingual versions is known as semantic similarity. Cross lingual semantic similarity requires corpora in which there are sentence pairs in both the source and target languages with a degree of semantic similarity between them. Many existing cross lingual semantic similarity models use a machine translation due to the unavailability of cross lingual semantic similarity dataset, which the propagation of the machine translation error reduces the accuracy of the model. On the other hand, when we want to use semantic similarity features for machine translation the same machine t
    
[^33]: DAC-MR: 基于数据增强一致性的元学习元正则化

    DAC-MR: Data Augmentation Consistency Based Meta-Regularization for Meta-Learning. (arXiv:2305.07892v1 [cs.LG])

    [http://arxiv.org/abs/2305.07892](http://arxiv.org/abs/2305.07892)

    为了提高元学习的性能，我们提出了一个基于元知识信息增强的元学习框架。我们通过使用适当的MR目标将元知识初步集成到元目标中，来规范元模型函数类的容量复杂度，有助于在未知任务上实现更好的泛化性能。

    

    近年来，元学习在机器学习领域内备受关注并推动了现代机器学习的发展。然而，要实现表现良好的元学习模型需要大量具有高质量元数据的训练任务，以表示底层任务泛化目标，有时对于真实应用而言难以获得。当前基于元数据的元学习方法，难以使用不完美的训练任务训练令人满意的元模型。为了解决这个问题，我们提出了一个元知识信息增强的元学习框架（MKIML），通过将补偿的元知识集成到元学习过程中，全面提高元学习的性能。我们通过使用适当的元正则化（MR）目标将元知识初步集成到元目标中，来规范元模型函数类的容量复杂度，有助于在未知任务上实现更好的泛化性能。作为一种实用化实现，我们提出数据增强。

    Meta learning recently has been heavily researched and helped advance the contemporary machine learning. However, achieving well-performing meta-learning model requires a large amount of training tasks with high-quality meta-data representing the underlying task generalization goal, which is sometimes difficult and expensive to obtain for real applications. Current meta-data-driven meta-learning approaches, however, are fairly hard to train satisfactory meta-models with imperfect training tasks. To address this issue, we suggest a meta-knowledge informed meta-learning (MKIML) framework to improve meta-learning by additionally integrating compensated meta-knowledge into meta-learning process. We preliminarily integrate meta-knowledge into meta-objective via using an appropriate meta-regularization (MR) objective to regularize capacity complexity of the meta-model function class to facilitate better generalization on unseen tasks. As a practical implementation, we introduce data augmenta
    
[^34]: 结构模拟和桥梁健康监测的神经运算器

    Neural operator for structural simulation and bridge health monitoring. (arXiv:2305.07889v1 [cs.LG])

    [http://arxiv.org/abs/2305.07889](http://arxiv.org/abs/2305.07889)

    本论文提出了结构模拟和桥梁健康监测的神经运算器VINO，通过学习结构响应场和损伤场之间的映射，在前向预测和反向确定损伤区域和程度方面可以比传统有限元模型更准确地预测和判断。

    

    将深度学习与结构工程相结合已经受到了广泛关注，用于前向问题（结构模拟）和反向问题（结构健康监测）。本研究基于傅里叶神经运算器，提出了VINO（车辆-桥梁相互作用神经运算器），作为桥梁结构的数字孪生。VINO学习结构响应场和损伤场之间的映射。本研究通过运行参数有限元（FE）模拟，考虑结构初始损伤场的随机分布，建立了VBI-FE数据集。随后，在四种损伤情况下进行了实验研究，产生了VBI-EXP数据集。在VINO通过VBI-FE预训练并在健康状态下通过VBI-EXP微调后，模型实现了以下两个改进。首先，前向的VINO比FE模型更准确地从损伤场输入预测结构响应。其次，反向的VINO可以确定损伤区域和程度。

    Infusing deep learning with structural engineering has received widespread attention for both forward problems (structural simulation) and inverse problems (structural health monitoring). Based on Fourier Neural Operator, this study proposes VINO (Vehicle-bridge Interaction Neural Operator) to serve as the digital twin of bridge structures. VINO learns mappings between structural response fields and damage fields. In this study, VBI-FE dataset was established by running parametric finite element (FE) simulations considering a random distribution of structural initial damage field. Subsequently, VBI-EXP dataset was produced by conducting an experimental study under four damage scenarios. After VINO was pre-trained by VBI-FE and fine-tuned by VBI-EXP from the bridge at the healthy state, the model achieved the following two improvements. First, forward VINO can predict structural responses from damage field inputs more accurately than the FE model. Second, inverse VINO can determine, loc
    
[^35]: 基于预训练语言模型的可扩展教学题生成

    Scalable Educational Question Generation with Pre-trained Language Models. (arXiv:2305.07871v1 [cs.AI])

    [http://arxiv.org/abs/2305.07871](http://arxiv.org/abs/2305.07871)

    这项研究开发了一种新的教育问题生成模型，能够通过在科学文本和科学问题数据上进行预训练和微调预训练语言模型，实现优秀的教育问题自动生成。

    

    在全球人口在探索个性化学习之旅时，教育问题的自动生成将在在线教育的扩展中发挥关键作用，实现大规模的自我评估。我们开发了一种新的教育问题生成模型EduQG，通过调整大型语言模型进行构建。我们广泛的实验表明，EduQG能够通过在科学文本和科学问题数据上进一步进行预训练和微调预训练语言模型，生成出更优秀的教育问题。

    The automatic generation of educational questions will play a key role in scaling online education, enabling self-assessment at scale when a global population is manoeuvring their personalised learning journeys. We develop \textit{EduQG}, a novel educational question generation model built by adapting a large language model. Our extensive experiments demonstrate that \textit{EduQG} can produce superior educational questions by further pre-training and fine-tuning a pre-trained language model on the scientific text and science question data.
    
[^36]: AI技术在历史事实核查中的应用——GPT 3.5、GPT4和GoogleBARD的比较评估

    Bridging History with AI A Comparative Evaluation of GPT 3.5, GPT4, and GoogleBARD in Predictive Accuracy and Fact Checking. (arXiv:2305.07868v1 [cs.CL])

    [http://arxiv.org/abs/2305.07868](http://arxiv.org/abs/2305.07868)

    本研究比较评估了三种大型语言模型的性能，结果表明AI技术在历史事实核查和填补空白方面有着巨大的潜力，其中GPT 4表现最为优异，这说明需要进一步探索AI技术在历史研究中的应用，以便丰富我们对过去的理解和弥合历史知识差距。

    

    数字时代信息的迅速扩散凸显了准确的历史记录和解释的重要性。虽然人工智能已在各个领域展示出了潜力，但其在历史事实核查和填补空白方面的潜力仍然不为人知。本研究评估了三种大型语言模型LLMs，即 GPT 3.5、GPT 4和GoogleBARD，它们能够根据给定数据预测和验证历史事件的表现。引入了一种新的评估指标：与现实的距离（DTR），用于评估模型的输出结果与已有的历史事实是否一致。结果显示，AI在历史研究中具有巨大的潜力，其中GPT 4表现最为优异。本文强调了需要进一步研究AI在丰富我们对过去的理解和弥合历史知识差距方面的作用。

    The rapid proliferation of information in the digital era underscores the importance of accurate historical representation and interpretation. While artificial intelligence has shown promise in various fields, its potential for historical fact-checking and gap-filling remains largely untapped. This study evaluates the performance of three large language models LLMs GPT 3.5, GPT 4, and GoogleBARD in the context of predicting and verifying historical events based on given data. A novel metric, Distance to Reality (DTR), is introduced to assess the models' outputs against established historical facts. The results reveal a substantial potential for AI in historical studies, with GPT 4 demonstrating superior performance. This paper underscores the need for further research into AI's role in enriching our understanding of the past and bridging historical knowledge gaps.
    
[^37]: Stackelberg决策变换器用于多智能体系统中的异步动作协调

    Stackelberg Decision Transformer for Asynchronous Action Coordination in Multi-Agent Systems. (arXiv:2305.07856v1 [cs.MA])

    [http://arxiv.org/abs/2305.07856](http://arxiv.org/abs/2305.07856)

    STEER是一种可以解决多智能体系统中异步动作协调问题的启发式方法，通过结合层次决策结构、自回归序列模型的建模能力和探索性学习方法高效地管理空间和时间上的决策过程。

    

    异步动作协调在多智能体系统（MAS）中面临着普遍的挑战，可以被表示为一个Stackelberg游戏（SG）。然而，基于SG的现有多智能体强化学习（MARL）方法的可扩展性受到网络结构或环境限制的严重制约。为了解决这个问题，我们提出了Stackelberg决策变换器（STEER），这是一种启发式方法，它解决了代理之间层次协调的困难。STEER通过将SG的层次决策结构、自回归序列模型的建模能力和MARL的探索性学习方法结合起来，高效地管理空间和时间上的决策过程。我们的研究有助于开发一种有效和适应性强的异步动作协调方法，可广泛应用于MAS中的各种任务类型和环境配置。实验结果表明，我们的方法在不同环境下具有很好的性能表现。

    Asynchronous action coordination presents a pervasive challenge in Multi-Agent Systems (MAS), which can be represented as a Stackelberg game (SG). However, the scalability of existing Multi-Agent Reinforcement Learning (MARL) methods based on SG is severely constrained by network structures or environmental limitations. To address this issue, we propose the Stackelberg Decision Transformer (STEER), a heuristic approach that resolves the difficulties of hierarchical coordination among agents. STEER efficiently manages decision-making processes in both spatial and temporal contexts by incorporating the hierarchical decision structure of SG, the modeling capability of autoregressive sequence models, and the exploratory learning methodology of MARL. Our research contributes to the development of an effective and adaptable asynchronous action coordination method that can be widely applied to various task types and environmental configurations in MAS. Experimental results demonstrate that ou
    
[^38]: 基于匹配特征提取的异构边缘设备工业健康预测的联邦学习

    A Federated Learning-based Industrial Health Prognostics for Heterogeneous Edge Devices using Matched Feature Extraction. (arXiv:2305.07854v1 [cs.LG])

    [http://arxiv.org/abs/2305.07854](http://arxiv.org/abs/2305.07854)

    提出了一种基于联邦学习的健康预测模型，该模型具有特征相似性匹配算法来区分学习来自异构边缘设备的数据，以便开发出更准确的预测模型。

    

    数据驱动的工业健康预测需要丰富的训练数据才能开发准确可靠的预测模型。然而，严格的数据隐私法律和丰富的边缘工业数据需要分散式数据利用。因此，联邦学习（FL）是一个分散式和隐私保护的学习技术，非常适用于工业健康预测领域。然而，由于异构数据的数据异质性，以及由于不同的退化机制和不平等的数据集大小所导致的数据异构性，在联邦学习的基础上开发精度高的训练模型是一个关键的统计挑战。因此，FL在健康预测任务中的应用尚未充分研究。本文提出了一种具有特征相似性匹配的参数聚合算法的 FL 健康预测模型。

    Data-driven industrial health prognostics require rich training data to develop accurate and reliable predictive models. However, stringent data privacy laws and the abundance of edge industrial data necessitate decentralized data utilization. Thus, the industrial health prognostics field is well suited to significantly benefit from federated learning (FL), a decentralized and privacy-preserving learning technique. However, FL-based health prognostics tasks have hardly been investigated due to the complexities of meaningfully aggregating model parameters trained from heterogeneous data to form a high performing federated model. Specifically, data heterogeneity among edge devices, stemming from dissimilar degradation mechanisms and unequal dataset sizes, poses a critical statistical challenge for developing accurate federated models. We propose a pioneering FL-based health prognostic model with a feature similarity-matched parameter aggregation algorithm to discriminatingly learn from h
    
[^39]: 理解异构数据联邦学习中的模型平均

    Understanding Model Averaging in Federated Learning on Heterogeneous Data. (arXiv:2305.07845v1 [cs.LG])

    [http://arxiv.org/abs/2305.07845](http://arxiv.org/abs/2305.07845)

    本文研究了异构数据联邦学习中的模型平均技术，通过可视化损失/错误景观揭示了客户端模型环绕全局模型在一个共同的盆地内，并且发现全局模型在早期训练后的误差主要来自客户端数据集和全局数据集之间非重叠的数据及全局模型与客户端模型之间的最大距离两个因素。

    

    模型平均是联邦学习中广泛采用的一种技术，它会聚集训练于异构数据上的多个客户端模型以获得表现良好的全局模型。然而，其成功背后的原理尚不是很清楚。本文通过可视化损失/错误景观来研究模型平均的几何特性，揭示了客户端模型环绕全局模型在一个共同的盆地内，并且即使全局模型表现优异，也可能偏离盆地底部。进一步的分析表明，全局模型在早期训练后的误差主要来自客户端数据集和全局数据集之间非重叠的数据及全局模型与客户端模型之间的最大距离两个因素。

    Model averaging, a widely adopted technique in federated learning (FL), aggregates multiple client models trained on heterogeneous data to obtain a well-performed global model. However, the rationale behind its success is not well understood. To shed light on this issue, we investigate the geometric properties of model averaging by visualizing the loss/error landscape. The geometrical visualization shows that the client models surround the global model within a common basin, and the global model may deviate from the bottom of the basin even though it performs better than the client models. To further understand this phenomenon, we decompose the expected prediction error of the global model into five factors related to client models. Specifically, we find that the global-model error after early training mainly comes from i) the client-model error on non-overlapping data between client datasets and the global dataset and ii) the maximal distance between the global and client models. Insp
    
[^40]: 一种简单易用的无监督方法用于增强句子表示

    A Simple and Plug-and-play Method for Unsupervised Sentence Representation Enhancement. (arXiv:2305.07824v1 [cs.CL])

    [http://arxiv.org/abs/2305.07824](http://arxiv.org/abs/2305.07824)

    本文提出了一种名为 RepAL 的简单易用的无监督方法，用于增强句子表示。通过减弱句子嵌入中的冗余信息，可以提高语义匹配和检索的效果。该方法无须训练，可与大多数现有的无监督句子学习模型结合使用。

    

    通过无监督的方式生成句子的嵌入对于实际应用中的语义匹配和检索问题是有益的。本文提出了 Representation ALchemy (RepAL)，这是一种非常简单的用于增强句子表示的后处理方法。RepAL 的基本思想是减弱预先训练模型生成的句子嵌入中冗余信息的重要性。通过全面的实验，我们展示了 RepAL 是一种不需要训练且可以与大多数现有的无监督句子学习模型结合使用的即插即用方法。我们还进行了深入的分析以理解 RepAL。

    Generating proper embedding of sentences through an unsupervised way is beneficial to semantic matching and retrieval problems in real-world scenarios. This paper presents Representation ALchemy (RepAL), an extremely simple post-processing method that enhances sentence representations. The basic idea in RepAL is to de-emphasize redundant information of sentence embedding generated by pre-trained models. Through comprehensive experiments, we show that RepAL is free of training and is a plug-and-play method that can be combined with most existing unsupervised sentence learning models. We also conducted in-depth analysis to understand RepAL.
    
[^41]: Cloud-RAIN: 具有反射不变性的点云分析

    Cloud-RAIN: Point Cloud Analysis with Reflectional Invariance. (arXiv:2305.07814v1 [cs.CV])

    [http://arxiv.org/abs/2305.07814](http://arxiv.org/abs/2305.07814)

    论文提出了一种具有反射不变性的点云分析框架Cloud-RAIN，使用二次神经元和PCA规范表示实现。

    

    点云任务的网络被期望在点云被仿射变换（如旋转和反射）时保持不变。迄今为止，与过去几年中受到主要研究关注的旋转不变性相比，反射不变性很少被讨论。然而，反射对称性可以出现在非常常见和重要的情况下，例如：结构化街道的静态反射对称性，来自移动物体（如行人）的双向运动的动态反射对称性，以及不同国家的左右交通惯例。可以惜的是，到目前为止，没有关于点云分析中具有反射不变性的网络的报道。为了填补这个空白，我们提出了一种使用二次神经元和PCA规范表示的框架，称为Cloud-RAIN，来赋予点云模型反射不变性。我们证明了一个定理。

    The networks for point cloud tasks are expected to be invariant when the point clouds are affinely transformed such as rotation and reflection. So far, relative to the rotational invariance that has been attracting major research attention in the past years, the reflection invariance is little addressed. Notwithstanding, reflection symmetry can find itself in very common and important scenarios, e.g., static reflection symmetry of structured streets, dynamic reflection symmetry from bidirectional motion of moving objects (such as pedestrians), and left- and right-hand traffic practices in different countries. To the best of our knowledge, unfortunately, no reflection-invariant network has been reported in point cloud analysis till now. To fill this gap, we propose a framework by using quadratic neurons and PCA canonical representation, referred to as Cloud-RAIN, to endow point \underline{Cloud} models with \underline{R}eflection\underline{A}l \underline{IN}variance. We prove a theorem 
    
[^42]: 使用Deepfake技术实现语音重音检测

    Using Deepfake Technologies for Word Emphasis Detection. (arXiv:2305.07791v1 [cs.LG])

    [http://arxiv.org/abs/2305.07791](http://arxiv.org/abs/2305.07791)

    本研究利用Deepfake技术产生没有重音的语音来解决自动语音重音检测任务，通过比较生成的语音和口述语音，能够分离出相对容易检测到的重音模式。

    

    本研究考虑了自动语音重音检测任务。这个任务很具挑战性，因为重音受到讲话者语音特点的影响，比如口音、方言或声音。为了解决这个问题，我们提出利用Deepfake技术为这个说话者产生一个没有重音的语音。这需要提取口述语音的文本，然后使用来自同一演讲者的语音样本来为这个任务产生没有重音的语音。通过比较生成的语音和口述语音，我们能够分离出相对容易检测到的重音模式。

    In this work, we consider the task of automated emphasis detection for spoken language. This problem is challenging in that emphasis is affected by the particularities of speech of the subject, for example the subject accent, dialect or voice. To address this task, we propose to utilize deep fake technology to produce an emphasis devoid speech for this speaker. This requires extracting the text of the spoken voice, and then using a voice sample from the same speaker to produce emphasis devoid speech for this task. By comparing the generated speech with the spoken voice, we are able to isolate patterns of emphasis which are relatively easy to detect.
    
[^43]: 混合问题解析与执行答案复杂问题的文字问答系统

    Answering Complex Questions over Text by Hybrid Question Parsing and Execution. (arXiv:2305.07789v1 [cs.CL])

    [http://arxiv.org/abs/2305.07789](http://arxiv.org/abs/2305.07789)

    提出了一种混合问题解析和执行框架，在文字问答系统中实现回答复杂问题，通过解析问题为H表达式并设计混合执行器实现。在基准数据集中实现了最先进的准确率和效率表现。

    

    文本问答系统的主导模式是基于端到端的神经网络，其在回答自然语言问题方面表现突出，但在回答复杂问题方面表现不足。这与基于语义解析的方法在结构化数据源（如关系数据库、知识图谱）上广泛适应形成对比，后者将自然语言问题转换为逻辑形式，并利用查询引擎进行执行。为了结合神经和符号方法的优势，我们提出了一种在文本问答系统中进行解析和执行问题的框架。它包括两个中心支柱：（1）我们将各种复杂问题解析成中间表示，称为H表达式，它由简单问题组成原语和表示它们之间关系的符号操作组成；（2）为了执行产生的H表达式，我们设计了一个混合执行器，它集成了确定规则来翻译符号操作，与处理原始问题的插入神经模块。我们在包含复杂问题的大规模基准数据集上评估了我们的方法，并在准确性和效率指标方面取得了最先进的表现。

    The dominant paradigm of textual question answering systems is based on end-to-end neural networks, which excels at answering natural language questions but falls short on complex ones. This stands in contrast to the broad adaptation of semantic parsing approaches over structured data sources (e.g., relational database, knowledge graphs), that convert natural language questions to logical forms and execute them with query engines. Towards combining the strengths of neural and symbolic methods, we propose a framework of question parsing and execution on textual QA. It comprises two central pillars: (1) We parse the question of varying complexity into an intermediate representation, named H-expression, which is composed of simple questions as the primitives and symbolic operations representing the relationships among them; (2) To execute the resulting H-expressions, we design a hybrid executor, which integrates the deterministic rules to translate the symbolic operations with a drop-in n
    
[^44]: 规则和操作的知识创作

    Knowledge Authoring for Rules and Actions. (arXiv:2305.07763v1 [cs.CL])

    [http://arxiv.org/abs/2305.07763](http://arxiv.org/abs/2305.07763)

    KALMRA是一个工具，它扩展了英语命令的功能，以允许在KALM中进行规则和操作的创作，并通过规则和操作的创作任务数据集的评估显示出较高的准确性和表现。

    

    知识表示和推理系统以事实和规则的形式描述和推理复杂的概念和关系。然而，广泛部署知识表示和推理系统时，领域专家往往很难构建正确的逻辑表示。我们提出了KALMRA来解决这些限制，以实现在KALM中进行规则和操作的创作。

    Knowledge representation and reasoning (KRR) systems describe and reason with complex concepts and relations in the form of facts and rules. Unfortunately, wide deployment of KRR systems runs into the problem that domain experts have great difficulty constructing correct logical representations of their domain knowledge. Knowledge engineers can help with this construction process, but there is a deficit of such specialists. The earlier Knowledge Authoring Logic Machine (KALM) based on Controlled Natural Language (CNL) was shown to have very high accuracy for authoring facts and questions. More recently, KALMFL, a successor of KALM, replaced CNL with factual English, which is much less restrictive and requires very little training from users. However, KALMFL has limitations in representing certain types of knowledge, such as authoring rules for multi-step reasoning or understanding actions with timestamps. To address these limitations, we propose KALMRA to enable authoring of rules and 
    
[^45]: 寻求可验证性: 解释很少能够在AI辅助决策中提高决策性能

    In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making. (arXiv:2305.07722v1 [cs.AI])

    [http://arxiv.org/abs/2305.07722](http://arxiv.org/abs/2305.07722)

    AI解释只有在允许人类决策者验证AI预测的正确性时才有用，而大多数决策环境无法进行验证。

    

    目前关于AI辅助决策的文献，涉及可解释的AI系统为人类决策者提供建议，并呈现出一系列不确定和令人困惑的结果。为了综合这些发现，我们提出了一个简单的理论，阐明了AI解释经常无法促使适当的依赖和互补决策表现的失败。我们认为解释只有在允许人类决策者验证AI预测的正确性时才有用，而不是其他期望，例如可解释性或清晰阐述AI的推理过程。先前的研究发现，在许多决策环境中，AI解释并未促进这种验证。此外，无论解释方法如何，大多数环境基本上都无法进行验证。我们最后讨论了更有效的可解释AI辅助决策和人工智能协作的潜在方法。

    The current literature on AI-advised decision making -- involving explainable AI systems advising human decision makers -- presents a series of inconclusive and confounding results. To synthesize these findings, we propose a simple theory that elucidates the frequent failure of AI explanations to engender appropriate reliance and complementary decision making performance. We argue explanations are only useful to the extent that they allow a human decision maker to verify the correctness of an AI's prediction, in contrast to other desiderata, e.g., interpretability or spelling out the AI's reasoning process. Prior studies find in many decision making contexts AI explanations do not facilitate such verification. Moreover, most contexts fundamentally do not allow verification, regardless of explanation method. We conclude with a discussion of potential approaches for more effective explainable AI-advised decision making and human-AI collaboration.
    
[^46]: AWFSD:基于评分扩散图像先验的加速Wirtinger流用于泊松高斯全息相位恢复问题

    AWFSD: Accelerated Wirtinger Flow with Score-based Diffusion Image Prior for Poisson-Gaussian Holographic Phase Retrieval. (arXiv:2305.07712v1 [eess.SP])

    [http://arxiv.org/abs/2305.07712](http://arxiv.org/abs/2305.07712)

    本文提出了一种名为AWFSD的算法，它基于评分扩散模型作为生成先验，用于解决在实际场景中，由光学成像系统引起的测量数据被混合的泊松和高斯噪声（PG噪声）所破坏的全息相位恢复问题，并且比现有方法具有更优的性能。

    

    相位恢复是许多相干成像系统中的一个重要问题。本文旨在解决在实际场景中，由光学成像系统引起的测量数据被混合的泊松和高斯噪声（PG噪声）所破坏的全息相位恢复问题。为了解决这个问题，我们开发了一种基于加速Wirtinger流的新算法，使用评分扩散模型作为生成先验（AWFSD）。特别地，我们将相位恢复问题构建为一个包含数据适配项和正则化项的优化任务。我们导出了PG对数似然函数的梯度及其相应的Lipschitz常数，从而保证实际测量的更准确的数据一致性项。我们通过使用评分扩散模型来捕获（梯度）图像先验分布，将生成先验作为我们正则化方法的一部分引入。我们提供了理论分析，建立了所提出的生成先验与图像相位信息之间的关键联系，并展示了它相对于现有方法的卓越性能。

    Phase retrieval (PR) is an essential problem in a number of coherent imaging systems. This work aims at resolving the holographic phase retrieval problem in real world scenarios where the measurements are corrupted by a mixture of Poisson and Gaussian (PG) noise that stems from optical imaging systems. To solve this problem, we develop a novel algorithm based on Accelerated Wirtinger Flow that uses Score-based Diffusion models as the generative prior (AWFSD). In particular, we frame the PR problem as an optimization task that involves both a data fidelity term and a regularization term. We derive the gradient of the PG log-likelihood function along with its corresponding Lipschitz constant, ensuring a more accurate data consistency term for practical measurements. We introduce a generative prior as part of our regularization approach by using a score-based diffusion model to capture (the gradient of) the image prior distribution. We provide theoretical analysis that establishes a criti
    
[^47]: 达芬奇二重论者：大语言模型中的心身分离和人类学习者中的心身二元论

    Davinci the Dualist: the mind-body divide in large language models and in human learners. (arXiv:2305.07667v1 [cs.AI])

    [http://arxiv.org/abs/2305.07667](http://arxiv.org/abs/2305.07667)

    本研究探究了大语言模型Davinci中的心身分离，发现其具有弱化的二元论倾向。

    

    大量文献表明，人类具有直观的二元论思想，认为精神和身体是不同的存在。过去的研究也表明，二元论是通过学习而出现的（例如Barlev＆Shtulman，2021）。但学习是否足以导致二元论尚不清楚。通过探究Davinci（一种不具有任何内在核心知识的大语言模型）中的心身分离，我们评估了学习的作用。我们发现，Davinci仍然倾向于二元论，并且这种偏见随着学习者的归纳潜力逐渐增加。

    A large literature suggests that people are intuitive Dualists--they consider the mind ethereal, distinct from the body. Past research also shows that Dualism emerges, in part, via learning (e.g., Barlev & Shtulman, 2021). But whether learning is sufficient to give rise to Dualism is unknown.The evidence from human learners does address this question because humans are endowed not only with general learning capacities but also with core knowledge capacities. And recent results suggest that core knowledge begets Dualism (Berent, Theodore & Valencia, 2021; Berent, 2023). To evaluate the role of learning, here, we probe for a mind-body divide in Davinci--a large language model (LLM) that is devoid of any innate core knowledge. We show that Davinci still leans towards Dualism, and that this bias increases systematically with the learner's inductive potential. Thus, davinci (a GPT-3 model) exhibits mild Dualist tendencies, whereas its descendent, text-davinci-003 (a GPT-3.5 model), shows a 
    
[^48]: 模仿与创新：孩子们能做到的，大型语言模型和语言视觉模型尚不能做到的是什么？

    Imitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?. (arXiv:2305.07666v1 [cs.AI])

    [http://arxiv.org/abs/2305.07666](http://arxiv.org/abs/2305.07666)

    本研究探讨了大型语言和语言视觉模型的局限性，以及机器与人类儿童在模仿和创新方面的不同表现。结果表明，机器需要更多的信息才能达到孩子所能做到的水平。

    

    大型语言模型和语言视觉模型是否是智能体一直备受关注。本研究提供了一种替代视角，认为这些人工智能模型是在现代世界中增强文化传播的文化技术，是高效的模仿引擎。通过评估AI模型设计新工具和发现新的因果结构的能力，我们探讨了AI模型对模仿和创新的影响，并将其与人类儿童的反应进行对比。我们的工作是确定从特定的学习技术和数据中可推导出哪些特定表示和能力，以及哪些知识或技能的第一步。关键在于，我们的研究结果表明，机器可能需要更多的大规模语言和图像，才能达到一个孩子所能做到的水平。

    Much discussion about large language models and language-and-vision models has focused on whether these models are intelligent agents. We present an alternative perspective. We argue that these artificial intelligence models are cultural technologies that enhance cultural transmission in the modern world, and are efficient imitation engines. We explore what AI models can tell us about imitation and innovation by evaluating their capacity to design new tools and discover novel causal structures, and contrast their responses with those of human children. Our work serves as a first step in determining which particular representations and competences, as well as which kinds of knowledge or skill, can be derived from particular learning techniques and data. Critically, our findings suggest that machines may need more than large scale language and images to achieve what a child can do.
    
[^49]: 情感计算综述：挑战、趋势、应用和未来方向调查

    A Comprehensive Survey on Affective Computing; Challenges, Trends, Applications, and Future Directions. (arXiv:2305.07665v1 [cs.AI])

    [http://arxiv.org/abs/2305.07665](http://arxiv.org/abs/2305.07665)

    本综述介绍了情感计算的重要性、思想、方法和结果，调查了最新的机器学习和混合现实相结合的情感计算方法，并讨论了各种应用程序。

    

    如其名，情感计算旨在识别人类的情绪、情感和感受。包括语言学、社会学、心理学、计算机科学和生理学等广泛领域研究情感计算。然而，没有研究确定机器学习（ML）和混合现实（XR）如何相互作用。本文讨论了情感计算的重要性、其思想、概念、方法和结果。通过使用ML和XR的方法，我们调查和讨论了情感计算的最新方法。我们调查了最先进的方法以及当前情感数据资源。此外，我们讨论了各种应用程序，其中情感计算具有重大影响，这将帮助未来学者更好地了解其重要性和实际相关性。

    As the name suggests, affective computing aims to recognize human emotions, sentiments, and feelings. There is a wide range of fields that study affective computing, including languages, sociology, psychology, computer science, and physiology. However, no research has ever been done to determine how machine learning (ML) and mixed reality (XR) interact together. This paper discusses the significance of affective computing, as well as its ideas, conceptions, methods, and outcomes. By using approaches of ML and XR, we survey and discuss recent methodologies in affective computing. We survey the state-of-the-art approaches along with current affective data resources. Further, we discuss various applications where affective computing has a significant impact, which will aid future scholars in gaining a better understanding of its significance and practical relevance.
    
[^50]: 卷积神经网络的定量语义比较

    Quantified Semantic Comparison of Convolutional Neural Networks. (arXiv:2305.07663v1 [cs.CV])

    [http://arxiv.org/abs/2305.07663](http://arxiv.org/abs/2305.07663)

    本研究提出了两种方法来量化卷积神经网络潜在空间中语义信息之间的相似性，从而揭示CNN层内语义信息的流动和相似性，以及不同网络之间的相似度程度。

    

    卷积神经网络（CNN）在计算机视觉领域的应用处于领先地位，具有出色的性能，然而它们的工作原理却很难阐明。但是，对于自动驾驶这类安全关键应用，模型选择还应考虑候选模型在模型透明性方面如何表示语义信息。为了解决这一尚未解决的问题，我们的工作提出了两种方法来量化CNN潜在空间中语义信息之间的相似性，旨在揭示CNN层内语义信息的流动和相似性，以及不同网络之间的相似度程度。我们使用了可解释人工智能（XAI）领域的著名技术作为基础，这些技术用于获得每个潜在空间中语义概念的全局向量表示，并基于它们在测试输入上的激活进行比较。本工作在三个不同的目标检测器和两个不同范围的图像数据集上进行了评估。

    The state-of-the-art in convolutional neural networks (CNNs) for computer vision excels in performance, while remaining opaque. But due to safety regulations for safety-critical applications, like perception for automated driving, the choice of model should also take into account how candidate models represent semantic information for model transparency reasons. To tackle this yet unsolved problem, our work proposes two methods for quantifying the similarity between semantic information in CNN latent spaces. These allow insights into both the flow and similarity of semantic information within CNN layers, and into the degree of their similitude between different networks. As a basis, we use renown techniques from the field of explainable artificial intelligence (XAI), which are used to obtain global vector representations of semantic concepts in each latent space. These are compared with respect to their activation on test inputs. When applied to three diverse object detectors and two d
    
[^51]: 生成式人工智能：教育的影响和应用

    Generative AI: Implications and Applications for Education. (arXiv:2305.07605v1 [cs.CY])

    [http://arxiv.org/abs/2305.07605](http://arxiv.org/abs/2305.07605)

    本文研究了一种生成式人工智能技术——基于大型语言模型的聊天机器人（C-LLM），以及它在复杂学生作品的人工智能评估和审查方面的应用，同时探讨了这种技术在教育中的应用范围。

    

    2022年11月ChatGPT的推出引发了一些教育工作者的恐慌，同时也引发了其他人的热情。在生成式人工智能这个大伞下，ChatGPT是一系列技术的例子，用于提供计算机生成的文本、图像和其他数字化媒体。本文研究了生成式人工智能技术之一——来自大型语言模型的聊天机器人(C-LLM)，以及其在复杂学生作品的人工智能评估和审查方面的应用。在讨论中，本文探讨了生成式人工智能的内在限制，即绑定于语料库及其通过二进制表示的文本表示。在这些限制之内，我们提出了生成式人工智能在教育中新兴和潜在的应用范围。

    The launch of ChatGPT in November 2022 precipitated a panic among some educators while prompting qualified enthusiasm from others. Under the umbrella term Generative AI, ChatGPT is an example of a range of technologies for the delivery of computer-generated text, image, and other digitized media. This paper examines the implications for education of one generative AI technology, chatbots responding from large language models, or C-LLM. It reports on an application of a C-LLM to AI review and assessment of complex student work. In a concluding discussion, the paper explores the intrinsic limits of generative AI, bound as it is to language corpora and their textual representation through binary notation. Within these limits, we suggest the range of emerging and potential applications of Generative AI in education.
    
[^52]: PillarAcc: 稀疏点云Pillars加速器——边缘设备上实时三维物体检测

    PillarAcc: Sparse PointPillars Accelerator for Real-Time Point Cloud 3D Object Detection on Edge Devices. (arXiv:2305.07522v1 [cs.AR])

    [http://arxiv.org/abs/2305.07522](http://arxiv.org/abs/2305.07522)

    PillarAcc提出了一种稀疏算法-硬件协同设计，有效增强基于Pillar的三维物体检测网络，实现高效率的计算减少。

    

    使用点云(PC)数据进行三维物体检测对于自动驾驶感知管道至关重要，其中高效的编码是满足严格资源和延迟要求的关键。PointPillars是一种广泛采用的鸟瞰图(BEV)编码方法，将三维点云数据聚合到二维pillar中，实现高精度的三维物体检测。然而，大多数采用PointPillar的最先进方法都忽视了pillar编码的固有稀疏性，错失了大量计算减少的机会。在本研究中，我们提出了一种开创性的算法-硬件协同设计，加速稀疏卷积处理，并最大限度地利用pillar的稀疏性来进行基于pillar的三维物体检测网络。我们使用先进的pillar pruning方法对稀疏化机会进行了研究，并实现了精确性和稀疏性之间的最优平衡。我们引入了PillarAcc，这是一种最先进的稀疏支持机制，通过输入具有线性复杂度的方法增强稀疏pillar卷积。

    3D object detection using point cloud (PC) data is vital for autonomous driving perception pipelines, where efficient encoding is key to meeting stringent resource and latency requirements. PointPillars, a widely adopted bird's-eye view (BEV) encoding, aggregates 3D point cloud data into 2D pillars for high-accuracy 3D object detection. However, most state-of-the-art methods employing PointPillar overlook the inherent sparsity of pillar encoding, missing opportunities for significant computational reduction. In this study, we propose a groundbreaking algorithm-hardware co-design that accelerates sparse convolution processing and maximizes sparsity utilization in pillar-based 3D object detection networks. We investigate sparsification opportunities using an advanced pillar-pruning method, achieving an optimal balance between accuracy and sparsity. We introduce PillarAcc, a state-of-the-art sparsity support mechanism that enhances sparse pillar convolution through linear complexity input
    
[^53]: ChatGPT是一个好的因果推断器吗？全面评估

    Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v1 [cs.CL])

    [http://arxiv.org/abs/2305.07375](http://arxiv.org/abs/2305.07375)

    本文对ChatGPT的因果推理能力进行了首次全面评估，实验证明ChatGPT是一个好的因果解释者，但不是一个好的因果推理者，存在严重的因果幻觉问题，对于明确的因果关系表现良好。

    

    因果推理能力对于众多NLP应用至关重要。尽管ChatGPT在各种NLP任务中表现出令人印象深刻的新兴能力，但ChatGPT在因果推理方面的表现如何仍不清楚。本文对ChatGPT的因果推理能力进行了首次全面评估。实验证明，ChatGPT不是一个好的因果推理者，但是是一个好的因果解释者。此外，ChatGPT在因果推理方面存在严重的幻觉，可能是由于自然语言中因果关系和非因果关系的报告偏见，以及ChatGPT的升级过程，如RLHF。在上下文学习（ICL）和思维链（COT）技术方面，可能会进一步加剧这种因果幻觉。此外，ChatGPT的因果推理能力对于在提示中表达因果概念的词语非常敏感，并且封闭提示比开放提示表现更好。对于句子中的事件，ChatGPT擅长捕捉明确的因果关系。

    Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT's causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit caus
    
[^54]: 基于深度学习的单目航天器姿态估计综述：当前状态、限制和前景。

    A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects. (arXiv:2305.07348v1 [cs.CV])

    [http://arxiv.org/abs/2305.07348](http://arxiv.org/abs/2305.07348)

    本文评估了基于深度学习的单目航天器姿态估计的现有方法，总结了部署该方法在现实任务中仍需克服的挑战。

    

    估算不配合的航天器姿态是一项重要的计算机视觉问题，可以实现在轨自动视觉系统的部署，其应用范围从轨道维修到太空碎片清除。随着计算机视觉的趋势，越来越多的工作在利用深度学习（DL）方法来解决此问题。但是尽管在研究阶段获得了有希望的结果，仍存在阻止这种方法在现实任务中使用的主要挑战。特别是，部署这种计算密集型算法仍然受到少量研究，而在合成图像上进行训练和在真实图像上进行测试时性能下降仍然需要减轻。本文主要目的是全面描述当前基于DL的航天器姿态估计方法，辅助确定有效部署DL的航天器姿态估计在现实任务中的限制。

    Estimating the pose of an uncooperative spacecraft is an important computer vision problem for enabling the deployment of automatic vision-based systems in orbit, with applications ranging from on-orbit servicing to space debris removal. Following the general trend in computer vision, more and more works have been focusing on leveraging Deep Learning (DL) methods to address this problem. However and despite promising research-stage results, major challenges preventing the use of such methods in real-life missions still stand in the way. In particular, the deployment of such computation-intensive algorithms is still under-investigated, while the performance drop when training on synthetic and testing on real images remains to mitigate. The primary goal of this survey is to describe the current DL-based methods for spacecraft pose estimation in a comprehensive manner. The secondary goal is to help define the limitations towards the effective deployment of DL-based spacecraft pose estimat
    
[^55]: 利用超树分解增强Datalog推理

    Enhancing Datalog Reasoning with Hypertree Decompositions. (arXiv:2305.06854v1 [cs.DB])

    [http://arxiv.org/abs/2305.06854](http://arxiv.org/abs/2305.06854)

    本文提出了一种利用超树分解来增强Datalog推理效率的算法，并将其与标准Datalog算法相结合以减少额外开销。实证评估结果表明，该算法可以提高推理效率和减少内存消耗。

    

    基于半朴素评估策略的Datalog推理使用传统的连接计划评估规则，在实践中经常导致冗余和低效，尤其是当规则很复杂时。超树分解有助于确定有效的查询计划并减少查询响应中类似的冗余。然而，如何将其应用于具有递归Datalog程序的物化和增量推理尚不清楚。此外，超树分解需要额外的数据结构，因此在运行时间和内存消耗方面引入了不可忽略的开销。在本文中，我们提供了利用超树分解进行Datalog程序的物化和增量评估的算法。此外，我们以模块化的方式将这种方法与标准Datalog推理算法相结合，以减少分解引起的开销。我们的实证评估表明，在程序包含复杂规则时，这种组合方法可以提高推理效率和减少内存消耗。

    Datalog reasoning based on the semina\"ive evaluation strategy evaluates rules using traditional join plans, which often leads to redundancy and inefficiency in practice, especially when the rules are complex. Hypertree decompositions help identify efficient query plans and reduce similar redundancy in query answering. However, it is unclear how this can be applied to materialisation and incremental reasoning with recursive Datalog programs. Moreover, hypertree decompositions require additional data structures and thus introduce nonnegligible overhead in both runtime and memory consumption. In this paper, we provide algorithms that exploit hypertree decompositions for the materialisation and incremental evaluation of Datalog programs. Furthermore, we combine this approach with standard Datalog reasoning algorithms in a modular fashion so that the overhead caused by the decompositions is reduced. Our empirical evaluation shows that, when the program contains complex rules, the combined 
    
[^56]: 基于拍卖的联邦学习中数据消费者的效用最大化竞标策略

    Utility-Maximizing Bidding Strategy for Data Consumers in Auction-based Federated Learning. (arXiv:2305.06784v1 [cs.LG])

    [http://arxiv.org/abs/2305.06784](http://arxiv.org/abs/2305.06784)

    本文提出了一种首个效用最大化竞标策略（Fed-Bidder），使多个FL数据消费者可以通过AFL有效而高效地竞争数据拥有者。

    

    基于拍卖的联邦学习（AFL）因通过经济手段激励数据拥有者加入FL而受到广泛的研究兴趣。现有工作假设在AFL市场上仅存在一个数据消费者和多个数据拥有者（即垄断市场）。因此，数据拥有者竞标加入数据消费者进行FL。但是，在实际的AFL市场中，多个数据消费者可能会竞争以吸引数据拥有者加入他们各自的FL任务，这种假设是不现实的。本文提出了一种首个效用最大化竞标策略（Fed-Bidder），使多个FL数据消费者可以通过AFL有效而高效地竞争数据拥有者，并提供了能够容纳不同市场动态的各种获胜函数的效用估计能力。基于六个常用基准数据集的广泛实验表明了策略的有效性和可扩展性。

    Auction-based Federated Learning (AFL) has attracted extensive research interest due to its ability to motivate data owners to join FL through economic means. Existing works assume that only one data consumer and multiple data owners exist in an AFL marketplace (i.e., a monopoly market). Therefore, data owners bid to join the data consumer for FL. However, this assumption is not realistic in practical AFL marketplaces in which multiple data consumers can compete to attract data owners to join their respective FL tasks. In this paper, we bridge this gap by proposing a first-of-its-kind utility-maximizing bidding strategy for data consumers in federated learning (Fed-Bidder). It enables multiple FL data consumers to compete for data owners via AFL effectively and efficiently by providing with utility estimation capabilities which can accommodate diverse forms of winning functions, each reflecting different market dynamics. Extensive experiments based on six commonly adopted benchmark dat
    
[^57]: 实用的鲁棒性强化学习：相邻不确定性集和双代理算法

    On practical robust reinforcement learning: adjacent uncertainty set and double-agent algorithm. (arXiv:2305.06657v1 [cs.LG])

    [http://arxiv.org/abs/2305.06657](http://arxiv.org/abs/2305.06657)

    本文提出了一个名为ARQ-Learning的鲁棒性强化学习算法，采用了一个更加实际的不确定性集，并提出了一种称为“悲观代理”的方法。该算法在表格化的情况下获得了与现有算法相同的快速收敛速度，并为实际应用提供了更好的鲁棒性。而且，本文还首次提出了用于深度Q网络和深度确定策略梯度的鲁棒RL算法PR-DQN和PR-DDPG。

    

    鲁棒性强化学习（RL）旨在学习一个策略，该策略在一个不确定性集上优化最差性能。给定一个产生训练样本的标准马尔可夫决策过程（N-MDP），该集合包含通过对N-MDP进行某些扰动而获得的MDP。本文引入了一个新的不确定性集，其中包含比现有集合更实际的MDP。使用这个不确定性集，我们提出了一个鲁棒RL算法，名为ARQ-Learning，用于表格化的情况。此外，我们表征了有限时间的误差界并证明它与Q-Learning和鲁棒Q-Learning（即现有的鲁棒RL方法）一样快地收敛，同时为实际应用提供更好的鲁棒性。我们提出了一种称为“悲观代理”的方法，有效地解决了将ARQ-Learning扩展到大型或连续状态空间的关键瓶颈。利用这一技术，我们首先提出了PRQ-Learning。接着，将其与DQN和DDPG相结合，我们分别开发了PR-DQN和PR-DDPG，这是首个用于深度Q网络和深度确定策略梯度的鲁棒RL算法。我们在基准领域上的实验验证了我们所提出算法的有效性。

    Robust reinforcement learning (RL) aims at learning a policy that optimizes the worst-case performance over an uncertainty set. Given nominal Markov decision process (N-MDP) that generates samples for training, the set contains MDPs obtained by some perturbations from N-MDP. In this paper, we introduce a new uncertainty set containing more realistic MDPs in practice than the existing sets. Using this uncertainty set, we present a robust RL, named ARQ-Learning, for tabular cases. Also, we characterize the finite-time error bounds and prove that it converges as fast as Q-Learning and robust Q-Learning (i.e., the state-of-the-art robust RL method) while providing better robustness for real applications. We propose {\em pessimistic agent} that efficiently tackles the key bottleneck for the extension of ARQ-Learning into large or continuous state spaces. Using this technique, we first propose PRQ-Learning. To the next, combining this with DQN and DDPG, we develop PR-DQN and PR-DDPG, respect
    
[^58]: HAHE: 基于全局和局部水平的分层注意力模型用于超关系知识图谱

    HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level. (arXiv:2305.06588v1 [cs.AI])

    [http://arxiv.org/abs/2305.06588](http://arxiv.org/abs/2305.06588)

    提出了HAHE模型，使用全局和局部水平的注意力学习了超关系知识图谱的图形结构和顺序结构，并在多个基准数据集上实现了最先进的表现。

    

    在超关系知识图谱上进行链接预测是值得尝试的。该论文提出了一种新颖的基于分层注意力的模型——HAHE，包括全局和局部水平的注意力机制来表示超关系知识图谱中的结构。通过采用超图双重注意力层，全局级别的注意力可以建模超关系知识图谱的图形结构；而采用异质性自注意层，局部级别的注意力则可以学习H-Facts内部的顺序结构。实验结果表明，HAHE在多个基准数据集上的链接预测方面取得了最先进的性能。

    Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting HKGs' representation. To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention. The global-level attention can model the graphical structure of HKG using hypergraph dual-attention layers, while the local-level attention can learn the sequential structure inside H-Facts via heterogeneous self-attention layers. Experiment results indicate that HAHE achieves state
    
[^59]: 如何为推荐基础模型索引项目ID

    How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])

    [http://arxiv.org/abs/2305.06569](http://arxiv.org/abs/2305.06569)

    本研究对推荐基础模型的项目索引问题进行了系统检查，提出了一种新的上下文感知索引方法，该方法在项目推荐准确性和文本生成质量方面具有优势。

    

    推荐基础模型将推荐任务转换为自然语言任务，利用大型语言模型（LLM）进行推荐。它通过直接生成建议的项目而不是计算传统推荐模型中每个候选项目的排名得分，简化了推荐管道，避免了多段过滤的问题。为了避免在决定要推荐哪些项目时生成过长的文本，为推荐基础模型创建LLM兼容的项目ID是必要的。本研究系统地研究了推荐基础模型的项目索引问题，以P5为代表的主干模型，并使用各种索引方法复制其结果。我们首先讨论了几种微不足道的项目索引方法（如独立索引、标题索引和随机索引）的问题，并表明它们不适用于推荐基础模型，然后提出了一种新的索引方法，称为上下文感知索引。我们表明，这种索引方法在项目推荐准确性和文本生成质量方面优于其他索引方法。

    Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
    
[^60]: 探索机器遗忘的领域：一篇综述与分类

    Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])

    [http://arxiv.org/abs/2305.06360](http://arxiv.org/abs/2305.06360)

    本文综述了机器遗忘的现状和技术应用，包括数据删除、扰动和模型更新，讨论了MU在隐私、安全和公正性等领域的潜在益处，以及它在自然语言处理、计算机视觉和推荐系统中的未来发展方向。

    

    机器遗忘是一个越来越受关注的领域，因为需要删除或修改机器学习模型所做出的预测。虽然训练模型变得更加有效和准确，但在某些领域（如隐私、安全和公正性），遗忘先前学到的信息的重要性变得越来越显著。本文介绍了机器遗忘的综述，涵盖了当前最先进的技术和方法，包括数据删除、扰动和模型更新。此外，文中还介绍了常用的度量标准和数据集。文章还强调了需要解决的挑战，包括攻击复杂性、标准化、可转移性、可解释性、训练数据和资源限制。本文的贡献包括讨论MU的潜在益处以及它在自然语言处理、计算机视觉和推荐系统中的未来方向。

    Machine unlearning (MU) is a field that is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions in Natural Language Processing, Computer vision, and Recommender Systems. Additionally, 
    
[^61]: 补丁学习：实现跨不同生物医学数据源的综合分析的范式。

    Patchwork Learning: A Paradigm Towards Integrative Analysis across Diverse Biomedical Data Sources. (arXiv:2305.06217v1 [cs.LG])

    [http://arxiv.org/abs/2305.06217](http://arxiv.org/abs/2305.06217)

    补丁学习是一种新的范式，通过整合来自不同数据源的信息，解决了数据隐私、异构数据来源和无法充分利用多个数据模态的挑战。它可以同时利用互补的数据来源，同时保护数据隐私，从而实现开发更全面和具有普适性的ML模型。

    

    在医疗保健领域中，机器学习（ML）提供了许多增强患者护理、人口健康和医疗保健提供者工作流程的机会。然而，由于数据隐私、异构数据来源和无法充分利用多个数据模态的挑战，现实中的临床和成本效益仍然有限。在这篇观点论文中，我们介绍了“补丁学习”（PL），这是一种新的范式，通过集成来自不同数据来源（例如，临床免费文本、医学图像、组学）和分布在不同安全站点上的不同数据模态的不同数据集的信息来解决这些限制。PL允许同时利用互补的数据来源，同时保护数据隐私，从而实现开发更全面和具有普适性的ML模型。我们介绍了补丁学习的概念以及其在医疗保健领域的当前实现，探讨了解决各种问题的潜在机会和适用数据来源。

    Machine learning (ML) in healthcare presents numerous opportunities for enhancing patient care, population health, and healthcare providers' workflows. However, the real-world clinical and cost benefits remain limited due to challenges in data privacy, heterogeneous data sources, and the inability to fully leverage multiple data modalities. In this perspective paper, we introduce "patchwork learning" (PL), a novel paradigm that addresses these limitations by integrating information from disparate datasets composed of different data modalities (e.g., clinical free-text, medical images, omics) and distributed across separate and secure sites. PL allows the simultaneous utilization of complementary data sources while preserving data privacy, enabling the development of more holistic and generalizable ML models. We present the concept of patchwork learning and its current implementations in healthcare, exploring the potential opportunities and applicable data sources for addressing various
    
[^62]: VCSUM：一个多功能的中文会议摘要数据集

    VCSUM: A Versatile Chinese Meeting Summarization Dataset. (arXiv:2305.05280v1 [cs.CL])

    [http://arxiv.org/abs/2305.05280](http://arxiv.org/abs/2305.05280)

    介绍了一个多功能的中文会议摘要数据集VCSum，包括239个现实生活中的会议，总时长超过230小时。该数据集可以适应各种摘要任务或方法，包括基于分割的摘要、多粒度摘要和检索-生成摘要。数据集和代码将在GitHub上发布。

    

    与新闻和聊天摘要相比，由于数据受限，会议摘要的发展受到极大的减速。为此，我们介绍了一个多功能的中文会议摘要数据集VCSum，包括239个现实生活中的会议，总时长超过230小时。我们声称我们的数据集是多功能的，因为我们为每个会议的文本提供了主题划分、头条、分段摘要、整个会议摘要和显要句子等注释。因此，该数据集可以适应各种摘要任务或方法，包括基于分割的摘要、多粒度摘要和检索-生成摘要。我们的分析证实了VCSum的有效性和稳健性。我们还提供了一组关于不同下游摘要任务的基准模型，以便进一步研究VCSum。数据集和代码将在 \url{https://github.com/hahahawu/VCSum} 上发布。

    Compared to news and chat summarization, the development of meeting summarization is hugely decelerated by the limited data. To this end, we introduce a versatile Chinese meeting summarization dataset, dubbed VCSum, consisting of 239 real-life meetings, with a total duration of over 230 hours. We claim our dataset is versatile because we provide the annotations of topic segmentation, headlines, segmentation summaries, overall meeting summaries, and salient sentences for each meeting transcript. As such, the dataset can adapt to various summarization tasks or methods, including segmentation-based summarization, multi-granularity summarization and retrieval-then-generate summarization. Our analysis confirms the effectiveness and robustness of VCSum. We also provide a set of benchmark models regarding different downstream summarization tasks on VCSum to facilitate further research. The dataset and code will be released at \url{https://github.com/hahahawu/VCSum}.
    
[^63]: ANALOGICAL- 一种新的大语言模型文本类比评测基准

    ANALOGICAL - A New Benchmark for Analogy of Long Text for Large Language Models. (arXiv:2305.05050v1 [cs.CL])

    [http://arxiv.org/abs/2305.05050](http://arxiv.org/abs/2305.05050)

    本文介绍了一种名为“ANALOGICAL”的新型基准，用以内在评估LLMs在长文本类比中的能力，包括六个复杂级别的长文本类比分类，并使用13个数据集和三种距离度量方法来评估8个LLMs在语义向量空间中识别类比对的能力。

    

    在过去的十年中，以词级别的类比为形式的类比在衡量诸如word2vec之类的词嵌入方法的质量方面发挥了重要作用。然而，现代的大型语言模型(LLMs)主要根据GLUE和SuperGLUE等基准的外在量度进行评估，而在LLMs是否能够在长文本中绘制类比的方面，只有少数几项研究。本文介绍了一种名为“ANALOGICAL”的新型基准，以六个复杂级别的长文本类比分类对LLMs进行内在评估，分别为 (i)单词、(ii)单词vs句子、(iii)语法、(iv)否定、(v)蕴含和(vi)隐喻。利用13个数据集和三种不同的距离度量方法，我们评估了8个LLMs在语义向量空间中识别类比对的能力(例如，“我能说两种语言”应该更接近“我是双语的”，而“我喜欢巧克力”和“我不喜欢巧克力”应该是正交的)。

    Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity -- (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space (e.g., "I can speak two languages" should be closer to "I am bilingual" while "I like chocolate" and "I do not like chocolate" should be orthog
    
[^64]: 基于Transformer的零样本和少样本生物医学命名实体识别方法

    A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])

    [http://arxiv.org/abs/2305.04928](http://arxiv.org/abs/2305.04928)

    本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。此方法利用预训练学习给定和潜在类别之间的语义关系，将多类标记分类任务转换为二元标记分类，能够在不同数量的样本情况下达到良好的识别效果。

    

    在生物医学领域中，有监督的命名实体识别（NER）依赖于具有给定命名实体的大量注释文本，其创建可能耗时且昂贵。此外，提取新实体通常需要进行额外的注释任务和重新训练模型。为解决这些挑战，本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。该方法基于将多类标记分类任务转换为二元标记分类（标记包含搜索的实体或不包含搜索的实体），并在更多的数据集和生物医学实体上进行预训练，从而可学习到给定和潜在类别之间的语义关系。在9种不同的生物医学实体上，我们在零样本NER、一次样本NER、10次样本NER和100次样本NER上实现了平均F1得分分别为35.44％、50.10％、69.94％和79.51％。

    Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
    
[^65]: 以人为中心的信任框架：一个人机交互的视角

    Human-centered trust framework: An HCI perspective. (arXiv:2305.03306v1 [cs.HC])

    [http://arxiv.org/abs/2305.03306](http://arxiv.org/abs/2305.03306)

    本论文提出了以人为中心的信任框架，帮助非专家实现在人工智能设计中充分利用用户信任的潜力；相关研究发现了某些计算机科学和人工智能话语中的用户信任误解，并进行了有效性评估，该研究的主要贡献是为抵制设计以技术为中心的易受攻击的交互方式的趋势提供了指导。

    

    本文的出发点基于当前人工智能用户信任话题的讨论，旨在提出新颖的以信任为辅助的人机交互方法，从而促进当前技术的采用和应用。我们提出了一个框架（HCTFrame），以指导非专家在人工智能设计中充分利用用户信任的潜力。通过对三个文献综述的发现进行数据三角化，可消除计算机科学和人工智能话语中关于用户信任的一些误解，并进行三个案例研究，以评估心理测量量表在映射潜在用户信任破坏和担忧方面的有效性。本文主要贡献于抵制设计以技术为中心的易受攻击的交互方式的趋势，这可能最终导致更多实际和感知上的信任破裂。我们提出的框架可用于指导系统设计人员如何映射和定义用户信任以及社会伦理和组织需求。

    The rationale of this work is based on the current user trust discourse of Artificial Intelligence (AI). We aim to produce novel HCI approaches that use trust as a facilitator for the uptake (or appropriation) of current technologies. We propose a framework (HCTFrame) to guide non-experts to unlock the full potential of user trust in AI design. Results derived from a data triangulation of findings from three literature reviews demystify some misconceptions of user trust in computer science and AI discourse, and three case studies are conducted to assess the effectiveness of a psychometric scale in mapping potential users' trust breakdowns and concerns. This work primarily contributes to the fight against the tendency to design technical-centered vulnerable interactions, which can eventually lead to additional real and perceived breaches of trust. The proposed framework can be used to guide system designers on how to map and define user trust and the socioethical and organisational need
    
[^66]: CryCeleb: 基于婴儿哭声的说话人认证数据集

    CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2305.00969](http://arxiv.org/abs/2305.00969)

    CryCeleb是一个基于婴儿哭声的说话人认证数据集，包括超过6小时的手动分割哭声，可用于研究婴儿哭声分析。

    

    本文描述了Ubenwa CryCeleb数据集——一个标记的婴儿哭声收集，以及附带的CryCeleb 2023任务——一个基于婴儿哭声的公共说话人验证挑战。我们释放出786名新生儿超过6小时的手动分割哭声，以鼓励婴儿哭声分析方面的研究。

    This paper describes the Ubenwa CryCeleb dataset - a labeled collection of infant cries, and the accompanying CryCeleb 2023 task - a public speaker verification challenge based on infant cry sounds. We release for academic usage more than 6 hours of manually segmented cry sounds from 786 newborns to encourage research in infant cry analysis.
    
[^67]: 从国家漏洞数据库的文本描述中构建知识图谱

    Constructing a Knowledge Graph from Textual Descriptions of Software Vulnerabilities in the National Vulnerability Database. (arXiv:2305.00382v1 [cs.CR])

    [http://arxiv.org/abs/2305.00382](http://arxiv.org/abs/2305.00382)

    本文提出了一种从国家漏洞数据库信息中构建漏洞知识图谱的新方法，结合了命名实体识别、关系提取和实体预测。该方法有助于解决网络安全知识图谱中缺失实体的问题。

    

    知识图谱已经显示出了在多个网络安全领域，例如漏洞评估和威胁分析方面的潜力。在本文中，我们提出了一种从国家漏洞数据库的信息中构建漏洞知识图谱的新方法。我们的方法结合了命名实体识别（NER）、关系提取（RE）、以及使用神经模型、启发式规则和知识图谱嵌入的实体预测。我们演示了我们的方法如何有助于解决网络安全知识图谱中缺失实体的问题，并对其性能进行了评估。

    Knowledge graphs have shown promise for several cybersecurity tasks, such as vulnerability assessment and threat analysis. In this work, we present a new method for constructing a vulnerability knowledge graph from information in the National Vulnerability Database (NVD). Our approach combines named entity recognition (NER), relation extraction (RE), and entity prediction using a combination of neural models, heuristic rules, and knowledge graph embeddings. We demonstrate how our method helps to fix missing entities in knowledge graphs used for cybersecurity and evaluate the performance.
    
[^68]: 用时序对抗增强技术改进视频表示

    Improve Video Representation with Temporal Adversarial Augmentation. (arXiv:2304.14601v1 [cs.CV])

    [http://arxiv.org/abs/2304.14601](http://arxiv.org/abs/2304.14601)

    本文提出了Temporal Adversarial Augmentation（TA），一种利用时间注意力的视频增强技术，可以通过最大化时间相关的损失函数来改变神经网络对视频片段的注意分布。利用TA，我们提出了Temporal Video Adversarial Fine-tuning（TAF）框架，可以有效地改善视频表示并提高神经网络的泛化能力。

    

    最近的研究表明，如果以适当的方式使用，对抗增强有助于神经网络的泛化。本文提出了一种使用时间注意力的新型视频增强技术——Temporal Adversarial Augmentation (TA)。与传统的对抗增强不同，TA专为通过最大化时间相关的损失函数来改变神经网络对视频片段的注意分布而设计。我们证明，TA将获得多样化的时间视角，这显著影响神经网络的焦点。使用这些示例进行训练修复了不平衡的时间信息感知缺陷，并增强了抵御时间偏移的能力，最终导致更好的泛化性能。为了利用TA，我们提出了Temporal Video Adversarial Fine-tuning (TAF)框架来改进视频表示。TAF是一种通用的模型无关、可解释性友好的训练策略。

    Recent works reveal that adversarial augmentation benefits the generalization of neural networks (NNs) if used in an appropriate manner. In this paper, we introduce Temporal Adversarial Augmentation (TA), a novel video augmentation technique that utilizes temporal attention. Unlike conventional adversarial augmentation, TA is specifically designed to shift the attention distributions of neural networks with respect to video clips by maximizing a temporal-related loss function. We demonstrate that TA will obtain diverse temporal views, which significantly affect the focus of neural networks. Training with these examples remedies the flaw of unbalanced temporal information perception and enhances the ability to defend against temporal shifts, ultimately leading to better generalization. To leverage TA, we propose Temporal Video Adversarial Fine-tuning (TAF) framework for improving video representations. TAF is a model-agnostic, generic, and interpretability-friendly training strategy. We
    
[^69]: 从空间中分割任何物体吗？

    Segment anything, from space?. (arXiv:2304.13000v1 [cs.CV])

    [http://arxiv.org/abs/2304.13000](http://arxiv.org/abs/2304.13000)

    最近开发的Segment Anything Model（SAM）模型可以基于简单的输入提示（如一个或多个点、边界框或掩码）有效分割自然图像中的对象，对视觉研究人员具有重要意义。此项研究探讨SAM在空中图像问题上的卓越性能，并在多项基准任务上进行了验证，表现良好。

    

    最近，为视觉任务专门开发的第一个基础模型被开发出来，被称为“Segment Anything Model”（SAM）。SAM可以根据简单的输入提示（如一个或多个点、边界框或掩码）分割输入图像中的对象。作者们在大量的视觉基准任务上研究了SAM的零样本图像分割精度，并发现SAM通常达到了与目标任务训练的视觉模型相似或有时甚至超越其识别精度。SAM在分割方面的卓越泛化能力对于从事自然图像研究的视觉研究人员具有重要意义。在这项工作中，我们研究了SAM的卓越性能是否扩展到空中图像问题，并帮助指导社区对其发展的回应。我们在一组多样化和广泛研究过的基准任务上研究SAM的表现。我们发现，SAM通常在空中图像上有良好的泛化表现，尽管在某些情况下会失败。

    Recently, the first foundation model developed specifically for vision tasks was developed, termed the "Segment Anything Model" (SAM). SAM can segment objects in input imagery based upon cheap input prompts, such as one (or more) points, a bounding box, or a mask. The authors examined the zero-shot image segmentation accuracy of SAM on a large number of vision benchmark tasks and found that SAM usually achieved recognition accuracy similar to, or sometimes exceeding, vision models that had been trained on the target tasks. The impressive generalization of SAM for segmentation has major implications for vision researchers working on natural imagery. In this work, we examine whether SAM's impressive performance extends to overhead imagery problems, and help guide the community's response to its development. We examine SAM's performance on a set of diverse and widely-studied benchmark tasks. We find that SAM does often generalize well to overhead imagery, although it fails in some cases d
    
[^70]: 测量大规模多任务中文理解能力

    Measuring Massive Multitask Chinese Understanding. (arXiv:2304.12986v1 [cs.CL])

    [http://arxiv.org/abs/2304.12986](http://arxiv.org/abs/2304.12986)

    本研究提出了一项测试，以衡量大型中文语言模型的多任务准确性，测试涵盖医学、法律、心理学和教育四个主要领域，结果表明所有模型在法律领域中表现都很差，建议研究人员应该开发更加多样化和均衡的多任务中文理解模型。

    

    大规模中文语言模型的研发正蓬勃发展，但缺乏相应的能力评估。因此，我们提出了一个测试，以衡量大型中文语言模型的多任务准确性。该测试涵盖了医学、法律、心理学和教育四个主要领域，在医学领域有15个子任务，在教育领域有8个子任务。我们发现，在零样本设置下表现最佳的模型平均比表现最差的模型高出近22个百分点。在四个主要领域中，所有模型的平均零样本准确度均未超过0.5。在子领域中，只有GPT-3.5-turbo模型在临床医学中实现了0.703的零样本准确度，这是所有模型在所有子任务中最高的准确度。所有模型在法律领域中表现都很差，最高的零样本准确度仅达到0.259。通过全面评估多个学科的广度和深度的知识，我们建议研究人员应该开发更加多样化和均衡的多任务中文理解模型。

    The development of large-scale Chinese language models is flourishing, yet there is a lack of corresponding capability assessments. Therefore, we propose a test to measure the multitask accuracy of large Chinese language models. This test encompasses four major domains, including medicine, law, psychology, and education, with 15 subtasks in medicine and 8 subtasks in education. We found that the best-performing models in the zero-shot setting outperformed the worst-performing models by nearly 22 percentage points on average. Across the four major domains, the average zero-shot accuracy of all models did not exceed 0.5. In the subdomains, only the GPT-3.5-turbo model achieved a zero-shot accuracy of 0.703 in clinical medicine, which was the highest accuracy among all models across all subtasks. All models performed poorly in the legal domain, with the highest zero-shot accuracy reaching only 0.259. By comprehensively evaluating the breadth and depth of knowledge across multiple discipli
    
[^71]: 用于教育的通用人工智能（AGI）

    Artificial General Intelligence (AGI) for Education. (arXiv:2304.12479v1 [cs.AI])

    [http://arxiv.org/abs/2304.12479](http://arxiv.org/abs/2304.12479)

    AGI技术具有革命教育领域潜力，可以建立e-learning平台、教育协作工具等，弥补传统AI模型因受限于数据和人际交互限制而无法满足教育需求的不足。

    

    由于最新的大型语言模型和聊天机器人（如GPT-4和ChatGPT）的出现，通用人工智能（AGI）作为未来技术已经得到全球认可。AGI旨在通过计算机系统复制人类智能，是具有革命教育领域潜力的关键技术之一。与传统的人工智能模型相比，这些模型通常只针对有限范围的任务进行设计，需要大量特定领域的数据进行训练，可能无法考虑教育中复杂的人际动态。受最近的大规模预训练模型驱动，AGI代表了机器在执行需要人类水平智能的任务方面的重大飞跃，例如推理、解决问题、做出决策，甚至理解人类情感和社交互动。本研究回顾了AGI的关键概念、能力、范围和在未来教育中的潜力，包括建立e-learning平台和教育协作工具等。

    Artificial general intelligence (AGI) has gained global recognition as a future technology due to the emergence of breakthrough large language models and chatbots such as GPT-4 and ChatGPT, respectively. AGI aims to replicate human intelligence through computer systems, which is one of the critical technologies having the potential to revolutionize the field of education. Compared to conventional AI models, typically designed for a limited range of tasks, demand significant amounts of domain-specific data for training and may not always consider intricate interpersonal dynamics in education. AGI, driven by the recent large pre-trained models, represents a significant leap in the capability of machines to perform tasks that require human-level intelligence, such as reasoning, problem-solving, decision-making, and even understanding human emotions and social interactions. This work reviews AGI's key concepts, capabilities, scope, and potential within future education, including setting e
    
[^72]: 带有概率逻辑推理的序列推荐

    Sequential Recommendation with Probabilistic Logical Reasoning. (arXiv:2304.11383v1 [cs.AI])

    [http://arxiv.org/abs/2304.11383](http://arxiv.org/abs/2304.11383)

    本文提出了一种结合了概率逻辑推理的序列推荐框架，以解决神经-符号SR存在的问题。通过将特征嵌入和逻辑嵌入分离，SR-PLR结合相似性匹配和逻辑推理，能够更好地捕捉用户口味的不确定性和演变。

    

    深度学习和符号学习是序列推荐（SR）中经常使用的两种方法。最近的神经-符号SR模型展示了它们使SR具备并发感知和认知能力的潜力。然而，神经-符号SR仍然存在问题，如在逻辑推理中表示用户和物品。在本文中，我们将Deep Neural Network（DNN）SR模型与逻辑推理相结合，提出了一个通用框架，称为带有概率逻辑推理的序列推荐（简称SR-PLR）。该框架通过在DNN和概率逻辑网络中解开特征嵌入和逻辑嵌入，允许SR-PLR从相似性匹配和逻辑推理中受益。为了更好地捕捉用户口味的不确定性和演变，SR-PLR使用概率方法嵌入用户和物品，并对用户的交互模式进行概率逻辑推理。

    Deep learning and symbolic learning are two frequently employed methods in Sequential Recommendation (SR). Recent neural-symbolic SR models demonstrate their potential to enable SR to be equipped with concurrent perception and cognition capacities. However, neural-symbolic SR remains a challenging problem due to open issues like representing users and items in logical reasoning. In this paper, we combine the Deep Neural Network (DNN) SR models with logical reasoning and propose a general framework named Sequential Recommendation with Probabilistic Logical Reasoning (short for SR-PLR). This framework allows SR-PLR to benefit from both similarity matching and logical reasoning by disentangling feature embedding and logic embedding in the DNN and probabilistic logic network. To better capture the uncertainty and evolution of user tastes, SR-PLR embeds users and items with a probabilistic method and conducts probabilistic logical reasoning on users' interaction patterns. Then the feature a
    
[^73]: Metropolis算法在处理局部最优时的效果如何？

    How Well Does the Metropolis Algorithm Cope With Local Optima?. (arXiv:2304.10848v1 [cs.NE])

    [http://arxiv.org/abs/2304.10848](http://arxiv.org/abs/2304.10848)

    Metropolis算法的局部搜索策略并不总是优于进化算法，还需要进一步改进和完善。

    

    Metropolis算法（MA）是一种经典的随机局部搜索启发式算法。它通过偶尔接受次优解避免陷入局部最优。为了更好地并以严格的方式理解这种能力，我们对CLIFF基准测试中的MA进行了数学运行时间分析。除了一个局部最优解外，cliff函数向全局最优解单调递增。因此，为了优化cliff函数，MA只需要一次接受一个劣质解。尽管看起来这是MA从其主要工作原理中获利的理想基准测试，但我们的数学运行时间分析表明这一希望并没有实现。即使在最优温度下（MA的唯一参数），MA优化大多数cliff函数的效率也不如简单的精英进化算法（EAs），后者只能通过生成可能相距很远的优秀解来离开局部最优解。这个结果表明，我们对MA为什么有效的理解需要进一步完善。

    The Metropolis algorithm (MA) is a classic stochastic local search heuristic. It avoids getting stuck in local optima by occasionally accepting inferior solutions. To better and in a rigorous manner understand this ability, we conduct a mathematical runtime analysis of the MA on the CLIFF benchmark. Apart from one local optimum, cliff functions are monotonically increasing towards the global optimum. Consequently, to optimize a cliff function, the MA only once needs to accept an inferior solution. Despite seemingly being an ideal benchmark for the MA to profit from its main working principle, our mathematical runtime analysis shows that this hope does not come true. Even with the optimal temperature (the only parameter of the MA), the MA optimizes most cliff functions less efficiently than simple elitist evolutionary algorithms (EAs), which can only leave the local optimum by generating a superior solution possibly far away. This result suggests that our understanding of why the MA is 
    
[^74]: 通过保留谱的数据压缩加速支持向量聚类

    Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression?. (arXiv:2304.09868v1 [cs.LG])

    [http://arxiv.org/abs/2304.09868](http://arxiv.org/abs/2304.09868)

    本文介绍了一种通过保留谱的数据压缩来加速支持向量聚类的方法，可以显著提高聚类速度而不牺牲聚类质量。

    

    支持向量聚类是一种重要的聚类方法，但是由于其计算昂贵的簇分配步骤，它面临着可伸缩性问题。在本文中，我们通过保留谱的数据压缩来加速支持向量聚类。具体而言，我们将原始数据集压缩成少量谱表示的聚合数据点，然后在压缩后的数据集上执行标准的支持向量聚类，最后将压缩数据集的聚类结果映射回原始数据集以发现簇。我们在真实数据集上的大量实验结果表明，相较于标准支持向量聚类，我们的方法大大提高了速度，而不会损失聚类质量。

    Support vector clustering is an important clustering method. However, it suffers from a scalability issue due to its computational expensive cluster assignment step. In this paper we accelertate the support vector clustering via spectrum-preserving data compression. Specifically, we first compress the original data set into a small amount of spectrally representative aggregated data points. Then, we perform standard support vector clustering on the compressed data set. Finally, we map the clustering results of the compressed data set back to discover the clusters in the original data set. Our extensive experimental results on real-world data set demonstrate dramatically speedups over standard support vector clustering without sacrificing clustering quality.
    
[^75]: 行为检索：通过查询未标记数据集实现少样本模仿学习

    Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets. (arXiv:2304.08742v1 [cs.RO])

    [http://arxiv.org/abs/2304.08742](http://arxiv.org/abs/2304.08742)

    本论文提出了一种简单的方法，利用少量下游专家数据从离线未标记数据集中选择性地查询相关行为（包括许多次优行为），实现了行为检索，进而实现了少样本学习。

    

    在数据效率方面使机器人学习新的视觉动作技能仍然是一个难题，有许多挑战。解决这个问题的一种流行范式是利用大型未标记的数据集，其中包含许多行为，然后使用少量任务特定的人类监督（即介入或演示）来适应特定任务的策略。但是，如何最好地利用狭窄的任务特定监督并将其与离线数据平衡仍然是一个待解决的问题。我们的关键洞察力在于任务特定数据不仅为代理提供了新的训练数据，还可以为代理的学习提供有关先前数据类型的信息。具体来说，我们提出了一种简单的方法，利用少量下游专家数据从离线未标记数据集中选择性地查询相关行为（包括许多次优行为）。然后代理被联合训练在专家和查询数据上。我们观察到，我们的

    Enabling robots to learn novel visuomotor skills in a data-efficient manner remains an unsolved problem with myriad challenges. A popular paradigm for tackling this problem is through leveraging large unlabeled datasets that have many behaviors in them and then adapting a policy to a specific task using a small amount of task-specific human supervision (i.e. interventions or demonstrations). However, how best to leverage the narrow task-specific supervision and balance it with offline data remains an open question. Our key insight in this work is that task-specific data not only provides new data for an agent to train on but can also inform the type of prior data the agent should use for learning. Concretely, we propose a simple approach that uses a small amount of downstream expert data to selectively query relevant behaviors from an offline, unlabeled dataset (including many sub-optimal behaviors). The agent is then jointly trained on the expert and queried data. We observe that our 
    
[^76]: 学习经验Bregman散度用于不确定距离表示

    Learning Empirical Bregman Divergence for Uncertain Distance Representation. (arXiv:2304.07689v1 [cs.CV])

    [http://arxiv.org/abs/2304.07689](http://arxiv.org/abs/2304.07689)

    本文介绍了一种新的基于Deep Metric Learning的方法，通过学习经验Bregman散度直接从数据中进行不确定距离表示，能够有效的在模式识别和聚类任务上提高准确性。

    

    深度度量学习技术已应用于各种监督和无监督学习任务，通过深度网络学习样本嵌入来进行视觉表示。然而，经典方法采用固定距离度量作为两个嵌入之间的相似性函数，可能导致捕捉复杂数据分布的亚最优性能。Bregman散度概括了各种距离度量的度量，并在许多深度度量学习领域中产生。本文首先展示了如何从Bregman散度获得深度度量学习损失。然后，我们介绍了一种直接从数据中学习经验Bregman散度的新方法，通过使用深度学习设置对Bregman散度下的凸函数进行参数化。我们进一步实验证明，与其他SOTA深度度量学习方法相比，我们的方法在五个流行公共数据集上表现出色，特别是在模式识别和聚类任务上。

    Deep metric learning techniques have been used for visual representation in various supervised and unsupervised learning tasks through learning embeddings of samples with deep networks. However, classic approaches, which employ a fixed distance metric as a similarity function between two embeddings, may lead to suboptimal performance for capturing the complex data distribution. The Bregman divergence generalizes measures of various distance metrics and arises throughout many fields of deep metric learning. In this paper, we first show how deep metric learning loss can arise from the Bregman divergence. We then introduce a novel method for learning empirical Bregman divergence directly from data based on parameterizing the convex function underlying the Bregman divergence with a deep learning setting. We further experimentally show that our approach performs effectively on five popular public datasets compared to other SOTA deep metric learning methods, particularly for pattern recognit
    
[^77]: 合作多智能体任务中学习奖励机制

    Learning Reward Machines in Cooperative Multi-Agent Tasks. (arXiv:2303.14061v1 [cs.AI])

    [http://arxiv.org/abs/2303.14061](http://arxiv.org/abs/2303.14061)

    本文提出了一种新的多智能体强化学习方法，将合作任务分解与学习奖励机制相结合。该方法有助于解决部分可观察环境中奖励的非马尔可夫性质，并提高了学习策略的可解释性，同时也降低了合作任务的复杂性。

    

    本文提出了一种新颖的多智能体强化学习方法，将合作任务分解与学习奖励机制相结合，以编码子任务的结构。该方法有助于应对部分观测环境中奖励的非马尔可夫性质，并提高所学习的策略的可解释性，以完成合作任务。与每个子任务相关联的奖励机制是以分散的方式学习的，然后用于指导每个智能体的行为。通过这样做，合作任务的复杂性得到了降低，从而更有效地学习。结果表明，我们的方法是未来在多智能体强化学习研究中的一个有 promising 的方向，特别是在具有大状态空间和多个智能体的复杂环境中。

    This paper presents a novel approach to Multi-Agent Reinforcement Learning (MARL) that combines cooperative task decomposition with the learning of reward machines (RMs) encoding the structure of the sub-tasks. The proposed method helps deal with the non-Markovian nature of the rewards in partially observable environments and improves the interpretability of the learnt policies required to complete the cooperative task. The RMs associated with each sub-task are learnt in a decentralised manner and then used to guide the behaviour of each agent. By doing so, the complexity of a cooperative multi-agent problem is reduced, allowing for more effective learning. The results suggest that our approach is a promising direction for future research in MARL, especially in complex environments with large state spaces and multiple agents.
    
[^78]: 使用分层行为探索的深度强化学习在对话生成中的应用

    Deep RL with Hierarchical Action Exploration for Dialogue Generation. (arXiv:2303.13465v1 [cs.CL])

    [http://arxiv.org/abs/2303.13465](http://arxiv.org/abs/2303.13465)

    本篇论文提出了一种新的方法，通过分层行为探索，从多个奖励函数中进行离线学习，并成功地解决了在对话生成中行为采样效率低下的问题，可以更好地识别人类情感细节。

    

    自然语言的行为空间极其庞大，因此在对话生成中，近似动态规划必须使用策略改进和行为采样。但是，由于有价值的回应非常稀疏，因此使用随机采样的贪心策略效率低下。本文提出了双粒度的 Q-function 并通过探索最有前途的回应类别来缓解这个局限性。该算法从识别人类情感细节的多个奖励函数中进行离线学习。实证研究表明，该算法优于基线方法。

    Conventionally, since the natural language action space is astronomical, approximate dynamic programming applied to dialogue generation involves policy improvement with action sampling. However, such a practice is inefficient for reinforcement learning (RL) because the eligible (high action value) responses are very sparse, and the greedy policy sustained by the random sampling is flabby. This paper shows that the performance of dialogue policy positively correlated with sampling size by theoretical and experimental. We introduce a novel dual-granularity Q-function to alleviate this limitation by exploring the most promising response category to intervene in the sampling. It extracts the actions following the grained hierarchy, which can achieve the optimum with fewer policy iterations. Our approach learns in the way of offline RL from multiple reward functions designed to recognize human emotional details. Empirical studies demonstrate that our algorithm outperforms the baseline metho
    
[^79]: 可微分逻辑的逻辑：走向DL的统一语义

    Logic of Differentiable Logics: Towards a Uniform Semantics of DL. (arXiv:2303.10650v2 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2303.10650](http://arxiv.org/abs/2303.10650)

    该论文介绍了一个元语言——LDL，用于定义DL，该元语言从语法和语义两方面上提高DL的形式化程度，使得对DL的性质和实现进行系统比较研究成为了可能。

    

    近期，可微分逻辑（DL）被提出作为一种训练神经网络满足逻辑规范的方法。DL包括语法和将语法中的表达式转化为损失函数的解释函数。这些损失函数可以在训练过程中与标准的梯度下降算法一起使用。 现有DL的多样性和对其形式化程度的不同处理使得对它们的性质和实现进行系统比较研究变得困难。该论文通过提出一个元语言——LDL作为DL定义的系统框架，从语法和语义两方面上提高DL的形式化程度，使得对DL的性质和实现进行系统比较研究成为了可能。

    Differentiable logics (DL) have recently been proposed as a method of training neural networks to satisfy logical specifications. A DL consists of a syntax in which specifications are stated and an interpretation function that translates expressions in the syntax into loss functions. These loss functions can then be used during training with standard gradient descent algorithms. The variety of existing DLs and the differing levels of formality with which they are treated makes a systematic comparative study of their properties and implementations difficult. This paper remedies this problem by suggesting a meta-language for defining DLs that we call the Logic of Differentiable Logics, or LDL. Syntactically, it generalises the syntax of existing DLs to FOL, and for the first time introduces the formalism for reasoning about vectors and learners. Semantically, it introduces a general interpretation function that can be instantiated to define loss functions arising from different existing 
    
[^80]: 解决预算约束下多组件POMDP的福利最大化算法

    Welfare Maximization Algorithm for Solving Budget-Constrained Multi-Component POMDPs. (arXiv:2303.10302v1 [math.OC])

    [http://arxiv.org/abs/2303.10302](http://arxiv.org/abs/2303.10302)

    该论文提出了一种福利最大化算法，用于解决预算约束下多组件POMDP问题，它通过最优预算分配来计算最优策略。

    

    部分可观测马尔可夫决策过程（POMDPs）提供了一种有效的方法来建模实际的序贯决策过程。本文针对独立动态基础设施组件的维护和检查问题，提出了一种算法来寻找多组件预算约束POMDP的最优策略。我们首先引入了带预算的POMDP模型（b-POMDP），在遵守预算约束的同时，允许我们找到POMDP的最优策略。接下来，我们证明了在有限期的情况下，b-POMDP的值函数或最大收益是预算的凹函数。我们的第二个贡献是一种算法，通过找到各个组件POMDP之间的最优预算分配来计算多组件预算约束POMDP的最优策略。最优预算分配被表示为一种福利最大化问题，并且利用值函数的凹性质来计算解决方案。

    Partially Observable Markov Decision Processes (POMDPs) provide an efficient way to model real-world sequential decision making processes. Motivated by the problem of maintenance and inspection of a group of infrastructure components with independent dynamics, this paper presents an algorithm to find the optimal policy for a multi-component budget-constrained POMDP. We first introduce a budgeted-POMDP model (b-POMDP) which enables us to find the optimal policy for a POMDP while adhering to budget constraints. Next, we prove that the value function or maximal collected reward for a b-POMDP is a concave function of the budget for the finite horizon case. Our second contribution is an algorithm to calculate the optimal policy for a multi-component budget-constrained POMDP by finding the optimal budget split among the individual component POMDPs. The optimal budget split is posed as a welfare maximization problem and the solution is computed by exploiting the concave nature of the value fu
    
[^81]: 一种新的张量专家混合并行方法来扩展混合专家训练

    A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training. (arXiv:2303.06318v1 [cs.LG])

    [http://arxiv.org/abs/2303.06318](http://arxiv.org/abs/2303.06318)

    本文提出了一种新的混合并行算法，结合了张量、专家和数据并行，以实现MoE模型的训练，其基本模型比当前最先进的DeepSpeed-MoE大4-8倍。

    This paper proposes a novel hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE.

    最近提出了一种名为Mixture-of-Experts（MoE）的新型神经网络架构，通过添加稀疏激活的专家块来增加神经网络（基本模型）的参数，而不改变训练或推理的总浮点操作数。理论上，这种架构允许我们训练任意大的模型，同时保持计算成本与基本模型相同。然而，在64到128个专家块之外，先前的工作观察到这些MoE模型的测试准确性递减。因此，训练高质量的MoE模型需要我们扩展基本模型的大小以及专家块的数量。在这项工作中，我们提出了一种新颖的三维混合并行算法，结合了张量、专家和数据并行，以实现MoE模型的训练，其基本模型比当前最先进的DeepSpeed-MoE大4-8倍。我们在优化器步骤中提出了内存优化。

    A new neural network architecture called Mixture-of-Experts (MoE) has been proposed recently that increases the parameters of a neural network (the base model) by adding sparsely activated expert blocks, without changing the total number of floating point operations for training or inference. In theory, this architecture allows us to train arbitrarily large models while keeping the computational costs same as that of the base model. However, beyond 64 to 128 experts blocks, prior work has observed diminishing returns in the test accuracies of these MoE models. Thus, training high quality MoE models requires us to scale the size of the base models, along with the number of expert blocks. In this work, we propose a novel, three-dimensional, hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE. We propose memory optimizations in the optimizer step, a
    
[^82]: BrainCLIP：通过CLIP框架实现从fMRI中获取自然视觉信息的通用解码方法

    BrainCLIP: Bridging Brain and Visual-Linguistic Representation via CLIP for Generic Natural Visual Stimulus Decoding from fMRI. (arXiv:2302.12971v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.12971](http://arxiv.org/abs/2302.12971)

    BrainCLIP是一种从fMRI中获取自然图像信息的解码方法，它利用了CLIP跨模态泛化能力并在语义空间中统一视觉刺激分类和重构任务，同时文本监督也能提高解码模型的性能。

    

    从fMRI信号重构感知到的自然图像或解码它们的类别是具有科学意义的挑战性任务。由于缺乏成对样本，大多数现有方法无法生成语义可识别的重构，并且难以推广到新颖类别。在这项工作中，我们首次提出了一种任务不可知的大脑解码模型，通过在语义空间中统一视觉刺激分类和重构任务来实现。我们将它命名为BrainCLIP，利用了CLIP跨模态泛化能力，以填补大脑活动，图像和文本之间的模态差距。具体而言，BrainCLIP是一种基于VAE的体系结构，通过结合视觉和文本监督，将fMRI模式转换为CLIP嵌入空间。注意，以前的研究很少使用多模态监督进行视觉刺激解码。我们的实验表明，文本监督可以显着提高解码模型的性能。

    Reconstructing perceived natural images or decoding their categories from fMRI signals are challenging tasks with great scientific significance. Due to the lack of paired samples, most existing methods fail to generate semantically recognizable reconstruction and are difficult to generalize to novel classes. In this work, we propose, for the first time, a task-agnostic brain decoding model by unifying the visual stimulus classification and reconstruction tasks in a semantic space. We denote it as BrainCLIP, which leverages CLIP's cross-modal generalization ability to bridge the modality gap between brain activities, images, and texts. Specifically, BrainCLIP is a VAE-based architecture that transforms fMRI patterns into the CLIP embedding space by combining visual and textual supervision. Note that previous works rarely use multi-modal supervision for visual stimulus decoding. Our experiments demonstrate that textual supervision can significantly boost the performance of decoding model
    
[^83]: 敏捷建模：从概念到分类器只需几分钟

    Agile Modeling: From Concept to Classifier in Minutes. (arXiv:2302.12948v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12948](http://arxiv.org/abs/2302.12948)

    本文介绍了敏捷建模的概念，即将任何主观视觉概念转化为计算机视觉模型的过程，并通过用户研究表明，用户可以在30分钟内轻松创建分类器。

    

    计算机视觉在主观细微应用方面的应用越来越多。虽然众包对于大多数客观任务（如标记“斑马”）已经为视觉社区服务得很好，但在概念具有实质性主观性的任务（例如识别“美食金枪鱼”）上，它现在面临失败。然而，让任何用户开发其概念的分类器在技术上是困难的：用户既不是机器学习专家，也没有耐心标记数千个示例。为此，我们提出了敏捷建模的问题：通过实时用户参与将任何主观视觉概念转化为计算机视觉模型的过程。我们为图像分类实例化了一个敏捷建模原型，并通过用户研究（N=14）表明，用户可以在30分钟内轻松创建分类器。我们将这个用户驱动的过程与传统的众包范式进行对比，并发现群体的观念常常与用户不同。

    The application of computer vision to nuanced subjective use cases is growing. While crowdsourcing has served the vision community well for most objective tasks (such as labeling a "zebra"), it now falters on tasks where there is substantial subjectivity in the concept (such as identifying "gourmet tuna"). However, empowering any user to develop a classifier for their concept is technically difficult: users are neither machine learning experts, nor have the patience to label thousands of examples. In reaction, we introduce the problem of Agile Modeling: the process of turning any subjective visual concept into a computer vision model through a real-time user-in-the-loop interactions. We instantiate an Agile Modeling prototype for image classification and show through a user study (N=14) that users can create classifiers with minimal effort under 30 minutes. We compare this user driven process with the traditional crowdsourcing paradigm and find that the crowd's notion often differs fro
    
[^84]: 原型图像分类中补丁可视化的合理性检查和改进

    Sanity checks and improvements for patch visualisation in prototype-based image classification. (arXiv:2302.08508v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.08508](http://arxiv.org/abs/2302.08508)

    本文通过精细的数据集，发现了基于原型的视觉分类中可视化方法的局限性，并提出了使用更忠实方法的必要性。

    

    本文对基于原型的视觉分类中实施的可视化方法进行了深入分析，使用两个精细的数据集（CUB-200-2011 和 Stanford Cars）首先表明这种方法不能正确地识别图像中的感兴趣区域，因此不能反映出模型的行为。其次，使用删除度量，我们定量地证明了 Smoothgrads 或 PRP 等显著性方法提供了更忠实的图像补丁。我们还提出了一种基于某些数据集（例如 CUB-200-2011）中提供的对象分割的相关性度量，并展示了 ProtoPNet 和 ProtoTree 产生的不精确的补丁可视化可能会产生错误的偏见感，可以通过使用更忠实的方法来减轻。最后，我们讨论了我们的发现对其他使用相同可视化方法的基于原型的模型的影响。

    In this work, we perform an in-depth analysis of the visualisation methods implemented in two popular self-explaining models for visual classification based on prototypes - ProtoPNet and ProtoTree. Using two fine-grained datasets (CUB-200-2011 and Stanford Cars), we first show that such methods do not correctly identify the regions of interest inside of the images, and therefore do not reflect the model behaviour. Secondly, using a deletion metric, we demonstrate quantitatively that saliency methods such as Smoothgrads or PRP provide more faithful image patches. We also propose a new relevance metric based on the segmentation of the object provided in some datasets (e.g. CUB-200-2011) and show that the imprecise patch visualisations generated by ProtoPNet and ProtoTree can create a false sense of bias that can be mitigated by the use of more faithful methods. Finally, we discuss the implications of our findings for other prototype-based models sharing the same visualisation method.
    
[^85]: 利用剪枝神经网络中的稀疏性来优化大型模型训练

    Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training. (arXiv:2302.05045v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05045](http://arxiv.org/abs/2302.05045)

    本文提出了一种新的方法，利用稀疏子网络来优化两种流行的深度学习并行算法 - 数据并行和层间并行的内存利用和通信。在512个NVIDIA V100 GPU上，我们的优化将27亿参数模型的内存消耗减少了74％，总通信时间减少了40％。

    This paper proposes a novel approach that exploits sparse subnetworks to optimize memory utilization and communication in two popular algorithms for parallel deep learning, and demonstrates significant reductions in memory consumption and communication time on a 2.7 billion parameter model using 512 NVIDIA V100 GPUs.

    由于通信开销的显著增加，规模化神经网络的并行训练具有挑战性。最近，深度学习研究人员开发了各种剪枝算法，能够剪枝（即将神经网络中的参数设置为零）80-90％的参数，以产生与未剪枝父网络相等的稀疏子网络。在本文中，我们提出了一种新的方法，利用这些稀疏子网络来优化两种流行的深度学习并行算法 - 数据并行和层间并行的内存利用和通信。我们将我们的方法集成到AxoNN中，这是一个高度可扩展的并行深度学习框架，依赖于数据和层间并行，并展示了通信时间和内存利用的减少。在512个NVIDIA V100 GPU上，我们的优化将27亿参数模型的内存消耗减少了74％，总通信时间减少了40％，从而提供了

    Parallel training of neural networks at scale is challenging due to significant overheads arising from communication. Recently, deep learning researchers have developed a variety of pruning algorithms that are capable of pruning (i.e. setting to zero) 80-90% of the parameters in a neural network to yield sparse subnetworks that equal the accuracy of the unpruned parent network. In this work, we propose a novel approach that exploits these sparse subnetworks to optimize the memory utilization and communication in two popular algorithms for parallel deep learning namely -- data and inter-layer parallelism. We integrate our approach into AxoNN, a highly scalable framework for parallel deep learning that relies on data and inter-layer parallelism, and demonstrate the reduction in communication time and memory utilization. On 512 NVIDIA V100 GPUs, our optimizations reduce the memory consumption of a 2.7 billion parameter model by 74%, and the total communication time by 40%, thus providing 
    
[^86]: 基于状态的安全强化学习：综述

    State-wise Safe Reinforcement Learning: A Survey. (arXiv:2302.03122v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03122](http://arxiv.org/abs/2302.03122)

    本文综合回顾了强化学习中解决基于状态约束的方法，讨论了它们在安全保障和可扩展性、安全性和奖励表现、收敛后和训练过程中的安全性等方面的联系、差异和权衡，并讨论了未来发展方向。

    

    尽管强化学习在仿真环境中取得了巨大的成功，但将其应用于实际场景仍然面临许多挑战。其中一个主要关注点是安全性，也就是约束满足。状态约束是实际应用中最常见且最具挑战性的约束之一，这对于许多挑战性任务，如自动驾驶、机器人操作等而言是必要和关键的。本文综述了现有的解决基于状态的约束的强化学习方法，并在状态约束马尔可夫决策过程的框架下，从安全保障和可扩展性、安全性和奖励表现、收敛后和训练过程中的安全性等方面，讨论了现有方法的联系、差异和权衡。我们还总结了当前方法的局限性并讨论了未来发展方向。

    Despite the tremendous success of Reinforcement Learning (RL) algorithms in simulation environments, applying RL to real-world applications still faces many challenges. A major concern is safety, in another word, constraint satisfaction. State-wise constraints are one of the most common constraints in real-world applications and one of the most challenging constraints in Safe RL. Enforcing state-wise constraints is necessary and essential to many challenging tasks such as autonomous driving, robot manipulation. This paper provides a comprehensive review of existing approaches that address state-wise constraints in RL. Under the framework of State-wise Constrained Markov Decision Process (SCMDP), we will discuss the connections, differences, and trade-offs of existing approaches in terms of (i) safety guarantee and scalability, (ii) safety and reward performance, and (iii) safety after convergence and during training. We also summarize limitations of current methods and discuss potentia
    
[^87]: 通过风险分解评估自监督学习

    Evaluating Self-Supervised Learning via Risk Decomposition. (arXiv:2302.03068v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03068](http://arxiv.org/abs/2302.03068)

    通过风险分解，提出四个误差部分评估自监督学习对169个视觉模型的影响，为SSL的设计和使用提供宝贵的见解。

    

    自监督学习（SSL）的流程设计涉及架构、增强和预训练数据等诸多选择。然而，SSL通常使用单一度量来评估，这并不能提供深入的洞察和改进方案。为解决这些问题，我们提出了一个SSL风险分解，从逼近、表示可用性、探针泛化和编码器泛化等角度对错误进行分解。我们分析了30个设计选择对169个在ImageNet上评估的SSL视觉模型的影响，并为每个组件提供了高效的估计器，为SSL模型的设计和使用提供宝贵的见解。

    Self-supervised learning (SSL) pipelines differ in many design choices such as the architecture, augmentations, or pretraining data. Yet SSL is typically evaluated using a single metric: linear probing on ImageNet. This does not provide much insight into why or when a model is better, now how to improve it. To address this, we propose an SSL risk decomposition, which generalizes the classical supervised approximation-estimation decomposition by considering errors arising from the representation learning step. Our decomposition consists of four error components: approximation, representation usability, probe generalization, and encoder generalization. We provide efficient estimators for each component and use them to analyze the effect of 30 design choices on 169 SSL vision models evaluated on ImageNet. Our analysis gives valuable insights for designing and using SSL models. For example, it highlights the main sources of error and shows how to improve SSL in specific settings (full- vs 
    
[^88]: 双排列等变性在知识图谱补全中的应用

    Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01313](http://arxiv.org/abs/2302.01313)

    本研究提出了双排列等变性的KG表示方法，可以使神经网络在KG中执行复杂的逻辑推理任务，并在多个归纳KG完成任务中实现了最先进的Hits@10测试准确率。双排列等变性在KG中开辟了新的研究方向。

    

    本研究将知识图谱(KGs)形式化为一种新型的图，并称之为双交换属性图，其中节点和二元（两个节点之间的）表示必须对节点号和边（及节点）属性（关系和节点特征）的排列等变。双重排列等变的KG表示在KG中开辟了新的研究方向。我们展示了这种等变性对关系的结构表示产生的影响，从而使神经网络能够在KG中执行复杂的逻辑推理任务。最后，我们介绍了一种通用的等变表示蓝图，并测试了一种简单的基于GNN的双排列等变神经结构，在WN18RR、FB237和NELL995归纳KG完成任务中实现了最先进的Hits@10测试准确率，并能够准确执行现有方法无法执行的逻辑推理任务。

    This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (& node) attributes (relations & node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
    
[^89]: 联邦推荐中的双重个性化

    Dual Personalization on Federated Recommendation. (arXiv:2301.08143v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2301.08143](http://arxiv.org/abs/2301.08143)

    本研究提出了一种新的个性化联邦推荐框架，可以学习轻量级模型并在智能设备上部署，同时实现对用户和物品的精细个性化。

    

    联邦推荐是一种旨在在联邦环境下提供隐私保护推荐服务的新型Internet服务架构。现有解决方案用于组合分布式推荐算法和隐私保护机制，因此从根本上采用服务器上的重量级模型，阻碍了在设备上部署智能模型。本文提出了一种新颖的个性化联邦推荐（PFedRec）框架，用于学习许多用户特定的轻量级模型，以便在智能设备上部署，而不是在服务器上使用重量级模型。此外，我们提出了一种新的双重个性化机制，以有效地学习用户和项目的细粒度个性化。整个学习过程被形式化为一个统一的联邦优化框架。

    Federated recommendation is a new Internet service architecture that aims to provide privacy-preserving recommendation services in federated settings. Existing solutions are used to combine distributed recommendation algorithms and privacy-preserving mechanisms. Thus it inherently takes the form of heavyweight models at the server and hinders the deployment of on-device intelligent models to end-users. This paper proposes a novel Personalized Federated Recommendation (PFedRec) framework to learn many user-specific lightweight models to be deployed on smart devices rather than a heavyweight model on a server. Moreover, we propose a new dual personalization mechanism to effectively learn fine-grained personalization on both users and items. The overall learning process is formulated into a unified federated optimization framework. Specifically, unlike previous methods that share exactly the same item embeddings across users in a federated system, dual personalization allows mild finetuni
    
[^90]: 掩模自编码在大规模自然语言监督中没有帮助

    Masked Autoencoding Does Not Help Natural Language Supervision at Scale. (arXiv:2301.07836v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.07836](http://arxiv.org/abs/2301.07836)

    本研究旨在探讨大规模训练下掩模自编码和对比语言图像预训练的有效性。研究结果表明，在小规模训练中这两种方法可以有效地结合使用，但在大规模训练中没有明显的优势。

    

    自我监督和自然语言监督已成为训练通用图像编码器的两种有效方法。最近的研究表明，这些方法可以有效地结合使用，但这些结果主要使用了小的预训练数据集（<50M样本），并且没有有效地反映出常用于这些方法的大规模数据集（>100M样本）的情况。本研究调查了类似的方法在使用更大量数据进行训练时是否有效。我们发现，在对11.3M的图像-文本对进行训练时，掩模自编码器和对比语言图像预训练的组合可以比只使用对比语言图像预训练更好，但在对1.4B个图像进行训练时，它与仅使用对比语言图像预训练相比，几乎没有任何优势（在一套常见的视觉任务中评估）。本研究为这些方法的有效性提供了一些需要的清晰度。

    Self supervision and natural language supervision have emerged as two exciting ways to train general purpose image encoders which excel at a variety of downstream tasks. Recent works such as M3AE and SLIP have suggested that these approaches can be effectively combined, but most notably their results use small pre-training datasets (<50M samples) and don't effectively reflect the large-scale regime (>100M examples) that is commonly used for these approaches. Here we investigate whether a similar approach can be effective when trained with a much larger amount of data. We find that a combination of two state of the art approaches: masked auto-encoders, MAE and contrastive language image pre-training, CLIP provides a benefit over CLIP when trained on a corpus of 11.3M image-text pairs, but little to no benefit (as evaluated on a suite of common vision tasks) over CLIP when trained on a large corpus of 1.4B images. Our work provides some much needed clarity into the effectiveness (or lack
    
[^91]: 测量先验投票权——认真对待委托

    Measuring a Priori Voting Power -- Taking Delegations Seriously. (arXiv:2301.02462v4 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2301.02462](http://arxiv.org/abs/2301.02462)

    该研究提出了新的权力指数来测量液态民主选举中选民的先验投票权，特别是在底层网络限制委托的情况下。这些指数的计算并不容易，但是对于特定的设置可以在伪多项式时间内计算。

    

    我们引入了新的权力指数来测量液态民主选举中选民的先验投票权，其中底层网络限制了委托。我们认为我们的权力指数是简单投票游戏中标准Penrose-Banzhaf指数的自然扩展。我们证明即使在投票权重在实例的规模上是多项式有界的情况下，计算选民的关键性也是＃P困难的。然而，对于特定的设置，例如当底层网络是二分图或完全图时，递归公式可以在伪多项式时间内计算这些指数以加权投票游戏。我们强调它们的理论属性，并提供数字结果，以说明限制可能的委托如何改变选民的投票权。

    We introduce new power indices to measure the a priori voting power of voters in liquid democracy elections where an underlying network restricts delegations. We argue that our power indices are natural extensions of the standard Penrose-Banzhaf index in simple voting games. We show that computing the criticality of a voter is #P-hard even when voting weights are polynomially-bounded in the size of the instance. However, for specific settings, such as when the underlying network is a bipartite or complete graph, recursive formulas can compute these indices for weighted voting games in pseudo-polynomial time. We highlight their theoretical properties and provide numerical results to illustrate how restricting the possible delegations can alter voters' voting power.
    
[^92]: gRoMA: 一种衡量深度神经网络全局鲁棒性的工具

    gRoMA: a Tool for Measuring Deep Neural Networks Global Robustness. (arXiv:2301.02288v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.02288](http://arxiv.org/abs/2301.02288)

    gRoMA是一种衡量DNN全局鲁棒性的创新工具，采用概率验证方法评估特定输出类别遭受到对抗性输入的概率。该工具可运行于预训练的黑盒分类模型上，并对整个模型和每个输入样本产生鲁棒性测量结果。

    

    深度神经网络（DNN）是前沿技术的代表，在各种复杂任务中取得了显著的表现。然而，将它们应用于安全关键系统（如航空或汽车领域）时，由于对抗性输入（即可能导致DNN犯错的输入扰动）的威胁，存在重大挑战。多项研究表明即便是现代DNN也容易受到对抗性输入的影响，因此必须测量并降低这种风险才能在安全关键系统中部署DNN。在这里，我们提出了一种创新且可扩展的工具gRoMA（全局鲁棒性测量和评估），它实现了一种概率验证方法来测量DNN的全局分类鲁棒性。具体而言，gRoMA测量特定输出类别遇到对抗性输入的概率。我们的工具基于预训练的黑盒分类模型，产生整个模型和每个输入样本的鲁棒性测量结果。我们通过测量多个最先进的DNN在热门图像数据集上的鲁棒性并分析结果，证明了我们的工具的有效性。

    Deep neural networks (DNNs) are at the forefront of cutting-edge technology, and have been achieving remarkable performance in a variety of complex tasks. Nevertheless, their integration into safety-critical systems, such as in the aerospace or automotive domains, poses a significant challenge due to the threat of adversarial inputs: perturbations in inputs that might cause the DNN to make grievous mistakes. Multiple studies have demonstrated that even modern DNNs are susceptible to adversarial inputs; and this risk must thus be measured and mitigated to allow the deployment of DNNs in safety-critical systems.  Here, we present gRoMA (global Robustness Measurement and Assessment), an innovative and scalable tool that implements a probabilistic verification approach to measure the global categorial robustness of a DNN. Specifically, gRoMA measures the probability of encountering adversarial inputs for a specific output category. Our tool operates on pre-trained, black-box classification
    
[^93]: 多语言翻译中干扰的原因和解决方法探究

    Causes and Cures for Interference in Multilingual Translation. (arXiv:2212.07530v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.07530](http://arxiv.org/abs/2212.07530)

    研究探究了多语言机器翻译中干扰的主要因素，通过系统化试验发现使用不到10亿参数的标准Transformer配置可以在很大程度上缓解干扰并促进协同，同时发现调整采样温度以控制数据中每个语言对所占比例的方法是平衡语言对之间关系的关键。

    

    多语言机器翻译模型可以从不同语言对之间的协同中获益，但同时也会受到干扰的影响。虽然目前有越来越多的先进方法旨在消除干扰，但我们对干扰现象的理解仍然有限。本研究确定了导致多语言机器翻译中干扰的主要因素。通过系统化试验，我们发现干扰（或协同）主要由模型大小、数据大小和每个语言对在总数据集中所占比例来决定。我们观察到，当模型相对于可用的训练数据非常小的时候，会出现严重的干扰，而使用不到10亿参数的标准Transformer配置可以在很大程度上缓解干扰并促进协同。此外，我们还展示了通过调整采样温度以控制数据中每个语言对所占比例的方法是平衡语言对之间关系的关键。

    Multilingual machine translation models can benefit from synergy between different language pairs, but also suffer from interference. While there is a growing number of sophisticated methods that aim to eliminate interference, our understanding of interference as a phenomenon is still limited. This work identifies the main factors that contribute to interference in multilingual machine translation. Through systematic experimentation, we find that interference (or synergy) are primarily determined by model size, data size, and the proportion of each language pair within the total dataset. We observe that substantial interference occurs mainly when the model is very small with respect to the available training data, and that using standard transformer configurations with less than one billion parameters largely alleviates interference and promotes synergy. Moreover, we show that tuning the sampling temperature to control the proportion of each language pair in the data is key to balancin
    
[^94]: 区块链上的AI伦理: 基于Twitter数据的区块链安全主题分析

    AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security. (arXiv:2212.06951v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2212.06951](http://arxiv.org/abs/2212.06951)

    本研究分析了Twitter上与MEV相关的话题，结果表明推文讨论了深刻的伦理问题，包括安全、公平、情感情绪和对MEV解决方案的渴望。

    

    区块链使用分布式网络让计算机系统更安全，但当前的区块链设计在交易顺序方面存在公平性问题。矿工可以重新排序交易以生成利润，这被称为矿工可提取价值（MEV）问题。现有研究认为MEV是一个严重的安全问题，并提出了潜在的解决方案，包括知名的Flashbots。然而，以往的研究大多分析了区块链数据，这可能无法捕捉到MEV在更广泛的AI社会中的影响。因此，在这项研究中，我们应用自然语言处理（NLP）方法全面分析了MEV推文中的话题。我们收集了超过20,000个MEV和Flashbots标签的推文并分析了它们的话题。我们的结果显示，这些推文讨论了深刻的伦理问题，包括安全、公平、情感情绪和对MEV解决方案的渴望。我们还发现了区块链上MEV活动的共同运动。

    Blockchain has empowered computer systems to be more secure using a distributed network. However, the current blockchain design suffers from fairness issues in transaction ordering. Miners are able to reorder transactions to generate profits, the so-called miner extractable value (MEV). Existing research recognizes MEV as a severe security issue and proposes potential solutions, including prominent Flashbots. However, previous studies have mostly analyzed blockchain data, which might not capture the impacts of MEV in a much broader AI society. Thus, in this research, we applied natural language processing (NLP) methods to comprehensively analyze topics in tweets on MEV. We collected more than 20000 tweets with MEV and Flashbots hashtags and analyzed their topics. Our results show that the tweets discussed profound topics of ethical concern, including security, equity, emotional sentiments, and the desire for solutions to MEV. We also identify the co-movements of MEV activities on block
    
[^95]: 相对过度泛化的课程学习

    Curriculum Learning for Relative Overgeneralization. (arXiv:2212.02733v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02733](http://arxiv.org/abs/2212.02733)

    本论文提出了一种名为相对过度泛化的课程学习（CURO）的新算法来解决多智能体强化学习中存在的相对过度泛化 (RO) 问题，该方法在解决展示强RO的合作任务方面具有很好的表现。

    

    在多智能体强化学习 (MARL) 中，许多流行方法如 VDN 和 QMIX，都容易受到相对过度泛化 (RO) 这一关键性的多智能体病理的影响。当合作任务中最佳联合行动的效用低于次优联合行动时，就会出现RO。RO可能导致智能体陷入局部最优解或无法解决需要智能体之间在给定时间步长内进行大量协调的合作任务。最近的基于价值的MARL算法，如QPLEX和WQMIX可以在一定程度上克服RO。然而，我们的实验结果表明，它们仍然无法解决展示强RO的合作任务。在这项工作中，我们提出了一种称为相对过度泛化的课程学习（CURO）的新方法，以更好地克服RO。在CURO中，我们首先微调目标任务的奖励函数以生成适合当前能力的源任务来解决展示强RO的目标任务。

    In multi-agent reinforcement learning (MARL), many popular methods, such as VDN and QMIX, are susceptible to a critical multi-agent pathology known as relative overgeneralization (RO), which arises when the optimal joint action's utility falls below that of a sub-optimal joint action in cooperative tasks. RO can cause the agents to get stuck into local optima or fail to solve cooperative tasks that require significant coordination between agents within a given timestep. Recent value-based MARL algorithms such as QPLEX and WQMIX can overcome RO to some extent. However, our experimental results show that they can still fail to solve cooperative tasks that exhibit strong RO. In this work, we propose a novel approach called curriculum learning for relative overgeneralization (CURO) to better overcome RO. To solve a target task that exhibits strong RO, in CURO, we first fine-tune the reward function of the target task to generate source tasks that are tailored to the current ability of the 
    
[^96]: 逻辑和常识引导的时间知识图谱补全

    Logic and Commonsense-Guided Temporal Knowledge Graph Completion. (arXiv:2211.16865v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.16865](http://arxiv.org/abs/2211.16865)

    本文提出一种"逻辑和常识引导的嵌入模型"（LCGE），通过共同学习时间敏感性表示（涉及时态和因果性的事件表示）与常识角度上的时间无关表示，解决了时间知识图谱补全中表示事件的实时性和因果性的挑战。

    

    时间知识图谱（TKG）存储与时间有关的数据事件。预测事件由于其时态特性而极具挑战性。此外，以往的TKG补全方法无法同时表示事件的实时性和因果性。为了解决这些问题，我们提出了一种“逻辑和常识引导的嵌入模型”（LCGE），能够共同学习事件的时间敏感性表示（涉及时态和因果性的事件表示）与常识角度上的时间无关表示。具体来说，我们设计了一个时间规则学习算法，构建了一个规则引导的谓词嵌入规范策略，学习事件之间的因果关系。此外，我们通过辅助常识知识能够准确评估事件的合理性。实验结果表明，我们的模型在TKGC任务中实现了显著的性能提升。

    A temporal knowledge graph (TKG) stores the events derived from the data involving time. Predicting events is extremely challenging due to the time-sensitive property of events. Besides, the previous TKG completion (TKGC) approaches cannot represent both the timeliness and the causality properties of events, simultaneously. To address these challenges, we propose a Logic and Commonsense-Guided Embedding model (LCGE) to jointly learn the time-sensitive representation involving timeliness and causality of events, together with the time-independent representation of events from the perspective of commonsense. Specifically, we design a temporal rule learning algorithm to construct a rule-guided predicate embedding regularization strategy for learning the causality among events. Furthermore, we could accurately evaluate the plausibility of events via auxiliary commonsense knowledge. The experimental results of TKGC task illustrate the significant performance improvements of our model compar
    
[^97]: RITA:通过真实交互式交通流增强自动驾驶模拟器

    RITA: Boost Autonomous Driving Simulators with Realistic Interactive Traffic Flow. (arXiv:2211.03408v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.03408](http://arxiv.org/abs/2211.03408)

    RITA是一个集成组件，可以提供高质量的交通流，用于测试和优化自动驾驶策略。它由两个核心模块组成，支持真实交互式交通流和易于使用的控制交通流接口。

    

    高质量的交通流生成是构建自动驾驶模拟器的核心模块。 然而，大部分可用的模拟器无法复制准确反映现实世界数据各种特征的交通模式，并同时模拟对测试自动驾驶策略的人类反应。为解决这一问题，我们提出了Realistic Interactive TrAffic flow (RITA)作为现有驾驶模拟器的集成组件，为测试和优化驾驶策略提供高质量的交通流。 RITA的开发考虑了三个关键特征，即真实度，多样性和可控性，由称为RITABackend和RITAKit的两个核心模块组成。 RITABackend支持车辆控制，并提供来自真实世界数据集的交通生成模型，而RITAKit则开发了易于使用的接口，以在模拟场景中生成可控的交通流。

    High-quality traffic flow generation is the core module in building simulators for autonomous driving. However, the majority of available simulators are incapable of replicating traffic patterns that accurately reflect the various features of real-world data while also simulating human-like reactive responses to the tested autopilot driving strategies. Taking one step forward to addressing such a problem, we propose Realistic Interactive TrAffic flow (RITA) as an integrated component of existing driving simulators to provide high-quality traffic flow for the evaluation and optimization of the tested driving strategies. RITA is developed with consideration of three key features, i.e., fidelity, diversity, and controllability, and consists of two core modules called RITABackend and RITAKit. RITABackend is built to support vehicle-wise control and provide traffic generation models from real-world datasets, while RITAKit is developed with easy-to-use interfaces for controllable traffic gen
    
[^98]: data2vec-aqc：在师生训练中寻找合适的助教。

    data2vec-aqc: Search for the right Teaching Assistant in the Teacher-Student training setup. (arXiv:2211.01246v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.01246](http://arxiv.org/abs/2211.01246)

    本文提出了一种新的自我监督学习算法data2vec-aqc，用于从未标记的语音数据中学习语音表示，该算法在语音识别领域取得了显著的性能提高。

    

    本文提出了一种新的自我监督学习算法——data2vec-aqc，用于从未标记的语音数据中学习语音表示。我们的目标是改进在标记和未标记的数据都很有限的语音领域的自我监督学习。在最近引入的data2vec的基础上，我们引入了额外的模块来利用数据增强、量化表示和聚类的优势。这些模块之间的交互帮助解决了交叉对比损失作为额外的自我监督目标。在没有使用任何语言模型(LM)的情况下，data2vec-aqc在LibriSpeech的test-clean集和test-other集上相对于现有最先进的data2vec系统分别取得了14.1％和20.9％的相对WER提高。我们提出的模型在Switchboard数据集的子集上微调时也可以获得高达17.8％的相对WER收益。

    In this paper, we propose a new Self-Supervised Learning (SSL) algorithm called data2vec-aqc, for speech representation learning from unlabeled speech data. Our goal is to improve SSL for speech in domains where both unlabeled and labeled data are limited. Building on the recently introduced data2vec, we introduce additional modules to the data2vec framework that leverage the benefit of data augmentations, quantized representations, and clustering. The interaction between these modules helps solve the cross-contrastive loss as an additional self-supervised objective. data2vec-aqc achieves up to 14.1% and 20.9% relative WER improvement over the existing state-of-the-art data2vec system over the test-clean and test-other sets, respectively of LibriSpeech, without the use of any language model (LM). Our proposed model also achieves up to 17.8\% relative WER gains over the baseline data2vec when fine-tuned on a subset of the Switchboard dataset. Code: https://github.com/Speech-Lab-IITM/dat
    
[^99]: 基于深度强化学习的自适应大邻域搜索算法在线控制

    Online Control of Adaptive Large Neighborhood Search using Deep Reinforcement Learning. (arXiv:2211.00759v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00759](http://arxiv.org/abs/2211.00759)

    本研究提出一种基于深度强化学习的方法来在线控制自适应大邻域搜索算法，该方法能够自适应选择启发式策略、调整参数和控制接受标准，以获得优化问题的良好解，对应用组合优化问题中的实际问题具有重要意义。

    

    自适应大邻域搜索（ALNS）算法在解决复杂的组合优化问题（COPs）方面取得了相当的成功。ALNS在搜索过程中自适应地选择各种启发式策略，利用它们的优势来找到优化问题的良好解。然而，ALNS的有效性取决于其选择和接受参数的正确配置。为了解决这个限制，我们提出了一种基于深度强化学习（DRL）的方法，在搜索过程中选择启发式、调整参数和控制接受标准。所提出的方法旨在基于搜索的状态学习如何配置下一次ALNS迭代以获得好的优化问题解。我们在一个时间依赖的含有随机权重和时间窗口的导航问题上评估了所提出的方法，该问题用于IJCAI竞赛。结果表明，我们的方法优于普通的ALNS和具有默认参数设置的ALNS，展示了DRL方法在在线控制ALNS方面的有效性。

    The Adaptive Large Neighborhood Search (ALNS) algorithm has shown considerable success in solving complex combinatorial optimization problems (COPs). ALNS selects various heuristics adaptively during the search process, leveraging their strengths to find good solutions for optimization problems. However, the effectiveness of ALNS depends on the proper configuration of its selection and acceptance parameters. To address this limitation, we propose a Deep Reinforcement Learning (DRL) approach that selects heuristics, adjusts parameters, and controls the acceptance criteria during the search process. The proposed method aims to learn, based on the state of the search, how to configure the next iteration of the ALNS to obtain good solutions to the underlying optimization problem. We evaluate the proposed method on a time-dependent orienteering problem with stochastic weights and time windows, used in an IJCAI competition. The results show that our approach outperforms vanilla ALNS and ALNS
    
[^100]: 一种用于检测混合垃圾邮件的多模态融合模型

    A Late Multi-Modal Fusion Model for Detecting Hybrid Spam E-mail. (arXiv:2210.14616v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.14616](http://arxiv.org/abs/2210.14616)

    本研究旨在设计一种有效的方法来过滤混合垃圾邮件，提出了一种多模态融合模型，解决了传统基于文本或基于图像的过滤器无法检测到混合垃圾邮件的问题。

    This study aims to design an effective approach filtering out hybrid spam e-mails and proposes a late multi-modal fusion model to solve the problem of traditional text-based or image-based filters failing to detect hybrid spam e-mails.

    近年来，垃圾邮件发送者开始通过引入图像和文本部分的混合垃圾邮件来混淆其意图，这比仅包含文本或图像的电子邮件更具挑战性。本研究的动机是设计一种有效的方法来过滤混合垃圾邮件，以避免传统的基于文本或基于图像的过滤器无法检测到混合垃圾邮件的情况。据我们所知，目前只有少数研究旨在检测混合垃圾邮件。通常，光学字符识别（OCR）技术用于通过将图像转换为文本来消除垃圾邮件的图像部分。然而，研究问题是，尽管OCR扫描是处理文本和图像混合垃圾邮件的非常成功的技术，但由于所需的CPU功率和扫描电子邮件文件所需的执行时间，它不是处理大量垃圾邮件的有效解决方案。

    In recent years, spammers are now trying to obfuscate their intents by introducing hybrid spam e-mail combining both image and text parts, which is more challenging to detect in comparison to e-mails containing text or image only. The motivation behind this research is to design an effective approach filtering out hybrid spam e-mails to avoid situations where traditional text-based or image-baesd only filters fail to detect hybrid spam e-mails. To the best of our knowledge, a few studies have been conducted with the goal of detecting hybrid spam e-mails. Ordinarily, Optical Character Recognition (OCR) technology is used to eliminate the image parts of spam by transforming images into text. However, the research questions are that although OCR scanning is a very successful technique in processing text-and-image hybrid spam, it is not an effective solution for dealing with huge quantities due to the CPU power required and the execution time it takes to scan e-mail files. And the OCR tech
    
[^101]: 预训练语言模型为什么更适合零样本学习？

    What Makes Pre-trained Language Models Better Zero-shot Learners?. (arXiv:2209.15206v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.15206](http://arxiv.org/abs/2209.15206)

    本文提出一个理论框架来解释prompt learning在零样本/少样本场景下的有效性，并基于此提出了一个注释无关的模板选择方法。

    

    本文提出了一个理论框架来解释prompt learning在零样本/少样本场景下的有效性。首先，我们证明传统的预训练和微调范式在少样本场景下会因为过拟合不具代表性的标注数据而失败。然后，我们详细阐述了prompt learning更有效的假设，因为它使建立在海量文本语料库和领域相关人类知识的预训练语言模型可以更多地参与预测，从而减少小型训练集提供的有限标签信息的影响。我们进一步假设，语言差异可以衡量提示质量。我们进行了全面的实验证明了我们的假设。更为重要的是，受到理论框架的启发，我们提出了一种基于困惑度的注释无关的模板选择方法，可以事先“预测”提示性能。这种方法特别值得鼓励。

    In this paper, we propose a theoretical framework to explain the efficacy of prompt learning in zero/few-shot scenarios. First, we prove that conventional pre-training and fine-tuning paradigm fails in few-shot scenarios due to overfitting the unrepresentative labelled data. We then detail the assumption that prompt learning is more effective because it empowers pre-trained language model that is built upon massive text corpora, as well as domain-related human knowledge to participate more in prediction and thereby reduces the impact of limited label information provided by the small training set. We further hypothesize that language discrepancy can measure the quality of prompting. Comprehensive experiments are performed to verify our assumptions. More remarkably, inspired by the theoretical framework, we propose an annotation-agnostic template selection method based on perplexity, which enables us to ``forecast'' the prompting performance in advance. This approach is especially encou
    
[^102]: 面向法律领域的预训练语言模型研究：以印度法律为例

    Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law. (arXiv:2209.06049v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2209.06049](http://arxiv.org/abs/2209.06049)

    本研究针对印度法律文本，重新训练和从零开始训练了两个PLMs，即LegalBERT和CaseLawBERT，并采用基于印度法律文本的词汇表训练了一个模型。我们在几项基准法律NLP任务中，对印度和非印度的法律文本进行了应用。

    

    随着基于Transformer预训练语言模型（PLMs）在法律领域中应用的增多，特别是在欧美法律文本方面，PLMs获得了显著的成功。然而，印度等其他国家的法律文本具有很多特殊特征，因此也需要在这些方面进行预训练。本文尝试在印度法律领域进行预训练。我们在印度法律数据上重新训练（继续预训练）了两个流行的法律PLMs, LegalBERT和CaseLawBERT，以及使用基于印度法律文本的词汇表从零开始训练了一个模型。我们将这些PLMs应用于三个基准法律NLP任务——从事实中识别法律法规、对法院判决文件进行语义分割，以及预测法院上诉判决--在印度和非印度的文本上。

    NLP in the legal domain has seen increasing success with the emergence of Transformer-based Pre-trained Language Models (PLMs) pre-trained on legal text. PLMs trained over European and US legal text are available publicly; however, legal text from other domains (countries), such as India, have a lot of distinguishing characteristics. With the rapidly increasing volume of Legal NLP applications in various countries, it has become necessary to pre-train such LMs over legal text of other countries as well. In this work, we attempt to investigate pre-training in the Indian legal domain. We re-train (continue pre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian legal data, as well as train a model from scratch with a vocabulary based on Indian legal text. We apply these PLMs over three benchmark legal NLP tasks -Legal Statute Identification from facts, Semantic Segmentation of Court Judgment Documents, and Court Appeal Judgment Prediction -- over both Indian and non-
    
[^103]: 用于机械臂的高效安全多目标抓取检测方法

    A Secure and Efficient Multi-Object Grasping Detection Approach for Robotic Arms. (arXiv:2209.03511v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.03511](http://arxiv.org/abs/2209.03511)

    本文提出了一种基于深度学习和边缘云协作的安全高效的机械臂抓取方法，通过GAN训练的编码器和解码器实现了图像加密压缩，同时在OCID数据集上获得了92%的准确率，图像压缩比率达到了0.03%。

    

    机械臂在自动化行业中得到了广泛应用。然而，随着深度学习在机械臂中的广泛应用，出现了新的挑战，如抓取计算能力的分配和对安全性的不断增长的需求。在这项工作中，我们提出了一种基于深度学习和边缘云协作的机械臂抓取方法。该方法实现了机械臂的任意抓取规划，同时考虑了抓取效率和信息安全。此外，由GAN训练的编码器和解码器使得图像在压缩的同时被加密，从而保证了隐私的安全性。该模型在OCID数据集上实现了92%的准确率，图像压缩比率达到了0.03%，结构差异值高于0.91。

    Robotic arms are widely used in automatic industries. However, with wide applications of deep learning in robotic arms, there are new challenges such as the allocation of grasping computing power and the growing demand for security. In this work, we propose a robotic arm grasping approach based on deep learning and edge-cloud collaboration. This approach realizes the arbitrary grasp planning of the robot arm and considers the grasp efficiency and information security. In addition, the encoder and decoder trained by GAN enable the images to be encrypted while compressing, which ensures the security of privacy. The model achieves 92% accuracy on the OCID dataset, the image compression ratio reaches 0.03%, and the structural difference value is higher than 0.91.
    
[^104]: 整合多元知识源的在线一-shot学习新任务

    Integrating Diverse Knowledge Sources for Online One-shot Learning of Novel Tasks. (arXiv:2208.09554v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.09554](http://arxiv.org/abs/2208.09554)

    本文研究了利用多元知识源在线一-shot学习新任务的方法，提出了一种代理，通过整合与环境的交互、任务执行和搜索知识、人类自然语言指令以及大型语言模型的响应来学习正确任务知识。结果表明，代理对多元知识源进行的在线整合总体上提高了一-shot任务学习的水平，减少了人类反馈，使任务学习更快速且可靠。

    

    自主代理能够利用广泛的任务知识潜在来源，但目前的方法通常只关注其中一个或两个。本文研究了利用多元知识源在线一-shot学习模拟办公室移动机器人新任务面临的挑战和影响。所得到的代理是基于Soar认知架构开发的，利用以下领域和任务知识的知识源：与环境的交互、任务执行和搜索知识、人类自然语言指令以及从大型语言模型（GPT-3）检索的响应。我们探讨了这些知识来源的独特贡献，并针对正确学习任务知识和减少人类工作负担的方案组合进行了性能评估。结果表明，代理对多元知识源进行的在线整合总体上提高了一-shot任务学习的水平，减少了不断的人类反馈，使任务学习更快速且可靠。

    Autonomous agents are able to draw on a wide variety of potential sources of task knowledge; however current approaches invariably focus on only one or two. Here we investigate the challenges and impact of exploiting diverse knowledge sources to learn online, in one-shot, new tasks for a simulated office mobile robot. The resulting agent, developed in the Soar cognitive architecture, uses the following sources of domain and task knowledge: interaction with the environment, task execution and search knowledge, human natural language instruction, and responses retrieved from a large language model (GPT-3). We explore the distinct contributions of these knowledge sources and evaluate the performance of different combinations in terms of learning correct task knowledge and human workload. Results show that an agent's online integration of diverse knowledge sources improves one-shot task learning overall, reducing human feedback needed for rapid and reliable task learning.
    
[^105]: CASE：调整粗细粒度认知和情感的匹配以产生共情响应

    CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation. (arXiv:2208.08845v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.08845](http://arxiv.org/abs/2208.08845)

    本研究提出了用于共情对话生成的CASE模型，通过调整用户的粗细粒度的认知和情感的匹配，可以生成更具共情和信息性的响应。

    

    共情交流应该是共情认知和情感的有意识调整和交互的结果。然而，现有的共情对话模型通常只考虑情感方面或在认知和情感上单独处理，这限制了共情响应生成的能力。在本研究中，我们提出了用于共情对话生成的CASE模型。它首先建立在常识认知图和情感概念图的基础上，然后在粗细的两个层面上对用户的认知和情感进行匹配。通过自动和手动评估，我们证明CASE优于共情对话的最新基线，并且可以产生更具共情和信息性的响应。

    Empathetic conversation is psychologically supposed to be the result of conscious alignment and interaction between the cognition and affection of empathy. However, existing empathetic dialogue models usually consider only the affective aspect or treat cognition and affection in isolation, which limits the capability of empathetic response generation. In this work, we propose the CASE model for empathetic dialogue generation. It first builds upon a commonsense cognition graph and an emotional concept graph and then aligns the user's cognition and affection at both the coarse-grained and fine-grained levels. Through automatic and manual evaluation, we demonstrate that CASE outperforms state-of-the-art baselines of empathetic dialogues and can generate more empathetic and informative responses.
    
[^106]: 欧几里得偏好模型的误差

    Error in the Euclidean Preference Model. (arXiv:2208.08160v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.08160](http://arxiv.org/abs/2208.08160)

    欧几里得偏好模型可能无法接近任意、真实、人类的偏好，甚至有一些情况下几乎所有偏好配置都不能用欧几里得模型表示。

    

    空间偏好模型是许多深度学习和多智能体系统（包括推荐系统）学到的矢量嵌入的形式。通常这些模型被假定为近似欧几里得结构，其中个人更喜欢距其“理想点”更近的选择，这是由欧几里得度量衡量得来的。然而，Bogomolnaia 和 Laslier (2007) 表明，如果欧几里得空间的维度比个人或选择的数量少两个，则存在无法用此结构表示的序偏好配置。我们扩展了这个结果，表明有些情况下几乎所有偏好配置都不能用欧几里得模型表示，并推导出在使用欧几里得模型近似非欧几里得偏好配置时预期误差的理论下界。我们的结果对于矢量嵌入的解释和使用具有重要影响，因为在一些情况下，欧几里得偏好模型可能无法接近任意、真实、人类的偏好。

    Spatial models of preference, in the form of vector embeddings, are learned by many deep learning and multiagent systems, including recommender systems. Often these models are assumed to approximate a Euclidean structure, where an individual prefers alternatives positioned closer to their "ideal point", as measured by the Euclidean metric. However, Bogomolnaia and Laslier (2007) showed that there exist ordinal preference profiles that cannot be represented with this structure if the Euclidean space has two fewer dimensions than there are individuals or alternatives. We extend this result, showing that there are situations in which almost all preference profiles cannot be represented with the Euclidean model, and derive a theoretical lower bound on the expected error when using the Euclidean model to approximate non-Euclidean preference profiles. Our results have implications for the interpretation and use of vector embeddings, because in some cases close approximation of arbitrary, tru
    
[^107]: 揭示推进生成模型的潜在空间几何结构

    Unveiling the Latent Space Geometry of Push-Forward Generative Models. (arXiv:2207.10541v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10541](http://arxiv.org/abs/2207.10541)

    本文研究了深度生成模型的潜在空间及其与模型性能之间的关系。借助几何测量理论，我们发现了优化的充分条件。我们提出了一种截断方法，可以在潜在空间中强制实行一个简单的簇结构，并提高了模型的性能。

    

    许多深度生成模型都是通过连续生成器推进高斯测量而定义的，例如生成对抗网络（GAN）或变分自动编码器（VAE）。本文探究了这些深度生成模型的潜在空间。这些模型的一个关键问题是，在学习不连通分布时，它们往往输出超出目标分布支持范围的样本。我们研究了这些模型的性能与它们的潜在空间几何之间的关系。借助几何测量理论的最新发展，我们在潜在空间的维度大于模的数量的情况下证明了优化的充分条件。通过对GAN进行实验，我们证明了我们理论结果的可靠性，并对这些模型的潜在空间几何结构获得了新的见解。此外，我们提出了一种截断方法，可以在潜在空间中强制实行一个简单的簇结构，并提高了模型的性能。

    Many deep generative models are defined as a push-forward of a Gaussian measure by a continuous generator, such as Generative Adversarial Networks (GANs) or Variational Auto-Encoders (VAEs). This work explores the latent space of such deep generative models. A key issue with these models is their tendency to output samples outside of the support of the target distribution when learning disconnected distributions. We investigate the relationship between the performance of these models and the geometry of their latent space. Building on recent developments in geometric measure theory, we prove a sufficient condition for optimality in the case where the dimension of the latent space is larger than the number of modes. Through experiments on GANs, we demonstrate the validity of our theoretical results and gain new insights into the latent space geometry of these models. Additionally, we propose a truncation method that enforces a simplicial cluster structure in the latent space and improve
    
[^108]: 可扩展极化码构造方法: 基于图神经网络的连续取消列表译码

    Scalable Polar Code Construction for Successive Cancellation List Decoding: A Graph Neural Network-Based Approach. (arXiv:2207.01105v4 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2207.01105](http://arxiv.org/abs/2207.01105)

    本文提出了一种基于图神经网络的可扩展极化码构造方法，该方法的优势在于模型复杂度与块长度和码率无关，数值实验表明该方法优于基于排序的方法。

    

    极化码的构造可以通过排序位通道实现连续取消译码的高效性，但对于循环冗余校验辅助的连续取消列表译码的最优极化码的高效可扩展构造仍需探究。本文首先将极化码映射到一个称为极化码构造消息传递（PCCMP）图的独特异构图中。接着，提出了基于异构图神经网络迭代消息传递（IMP）算法，旨在找到对应于CA-SCL译码下具有最小帧错误率的PCCMP图。这种新的IMP算法的主要优势在于其可扩展性。即模型复杂度与块长度和码率无关，且在短极化码上训练的IMP模型可以轻松应用于长极化码的构造。数值实验表明，基于IMP的极化码构造方法优于基于排序的方法。

    While constructing polar codes for successive-cancellation decoding can be implemented efficiently by sorting the bit-channels, finding optimal polar codes for cyclic-redundancy-check-aided successive-cancellation list (CA-SCL) decoding in an efficient and scalable manner still awaits investigation. This paper first maps a polar code to a unique heterogeneous graph called the polar-code-construction message-passing (PCCMP) graph. Next, a heterogeneous graph-neural-network-based iterative message-passing (IMP) algorithm is proposed which aims to find a PCCMP graph that corresponds to the polar code with minimum frame error rate under CA-SCL decoding. This new IMP algorithm's major advantage lies in its scalability power. That is, the model complexity is independent of the blocklength and code rate, and a trained IMP model over a short polar code can be readily applied to a long polar code's construction. Numerical experiments show that IMP-based polar-code constructions outperform class
    
[^109]: 下一代超级卫星网络的人工智能技术

    Artificial Intelligence Techniques for Next-Generation Mega Satellite Networks. (arXiv:2207.00414v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2207.00414](http://arxiv.org/abs/2207.00414)

    本文介绍了如何使用人工智能技术来解决超级卫星网络通信中的挑战，包括卫星间链路、短暂时间过程和卫星覆盖范围等问题。

    

    由于太空发射、电子、处理能力和微型化的重大进展，空间通信，特别是超级卫星网络，重新成为下一代网络的有吸引力的候选者。然而，超级卫星网络依赖于无数的基础和相互交织的过程，传统模型不能真正捕捉它们的动态和独特特征，如轨道速度、卫星间链路、短暂的时间过程和卫星覆盖范围等。因此，需要新的方法来使网络主动适应与链接中快速变化的条件。人工智能提供了一种捕捉这些过程、分析它们的行为并模拟它们对网络影响的途径。本文介绍了应用AI技术于综合地面卫星网络，特别是超级卫星网络通信的应用。它详细介绍了超级卫星网络的独特特征和相关的挑战，并讨论了基于AI的技术如何帮助克服这些挑战。

    Space communications, particularly mega satellite networks, re-emerged as an appealing candidate for next generation networks due to major advances in space launching, electronics, processing power, and miniaturization. However, mega satellite networks rely on numerous underlying and intertwined processes that cannot be truly captured using conventionally used models, due to their dynamic and unique features such as orbital speed, inter-satellite links, short time pass, and satellite footprint, among others. Hence, new approaches are needed to enable the network to proactively adjust to the rapidly varying conditions associated within the link. Artificial intelligence (AI) provides a pathway to capture these processes, analyze their behavior, and model their effect on the network. This article introduces the application of AI techniques for integrated terrestrial satellite networks, particularly mega satellite network communications. It details the unique features of mega satellite net
    
[^110]: 预测Twitter对话线程中的仇恨强度

    Predicting Hate Intensity of Twitter Conversation Threads. (arXiv:2206.08406v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2206.08406](http://arxiv.org/abs/2206.08406)

    本文提出了DRAGNET++, 通过考虑对话线程的语义、传播结构和用户交互，以预测推文的回复链中可能存在的仇恨程度，并在两个公开数据集上的实验中表现出优越性能。该模型可为社交媒体平台提供在恶意对话升级之前识别和管理的工具。

    

    推文是在线社交媒体中最简洁的交流形式，一条推文有可能是对话中打造或破坏讨论的潜在媒介。在线仇恨言论比以往任何时候都更容易获得，阻止其传播对于社交媒体公司和用户来说是极为重要的，可以推进良好的交流方式。目前除了最近的一些研究外，大部分研究都专注于分类单个推文，而忽略了推文之间的对话线程/上下文。我们提出了DRAGNET++，旨在通过推文的回复链预测它可能带来的仇恨程度，同时考虑到对话线程的语义和传播结构以及线程中的用户交互。我们在两个公开数据集上的实验表明DRAGNET++的表现优于现有的方法，我们认为社交媒体平台可以利用我们提出的方法，预测和管理恶意对话。

    Tweets are the most concise form of communication in online social media, wherein a single tweet has the potential to make or break the discourse of the conversation. Online hate speech is more accessible than ever, and stifling its propagation is of utmost importance for social media companies and users for congenial communication. Most of the research barring a recent few has focused on classifying an individual tweet regardless of the tweet thread/context leading up to that point. One of the classical approaches to curb hate speech is to adopt a reactive strategy after the hate speech postage. The ex-post facto strategy results in neglecting subtle posts that do not show the potential to instigate hate speech on their own but may portend in the subsequent discussion ensuing in the post's replies. In this paper, we propose DRAGNET++, which aims to predict the intensity of hatred that a tweet can bring in through its reply chain in the future. It uses the semantic and propagating stru
    
[^111]: 针对非布尔矩阵的Ihara-Bass公式及随机CSPs的强证明机制

    A Ihara-Bass Formula for Non-Boolean Matrices and Strong Refutations of Random CSPs. (arXiv:2204.10881v2 [cs.CC] UPDATED)

    [http://arxiv.org/abs/2204.10881](http://arxiv.org/abs/2204.10881)

    本文提出了一种新概念的“非回溯”矩阵并证明了相应的Ihara-Bass型公式，利用该理论证明了随机k-CSP实例的多项式时间强证明的新结果。

    

    我们定义了一种新颖的“非回溯”矩阵概念，可与任何对称矩阵相关，并证明了相应的Ihara-Bass型公式。我们利用这个理论证明了关于$k$个变量每约束的随机约束满足问题（k-CSPs）的多项式时间强证明的新结果。对于由一个由$p$分数的分配满足的约束构建的随机k-CSP实例，如果实例包含$n$个变量和$n^{k/2} / \epsilon^2$个约束，则我们可以有效地计算出最优解仅满足$p+O_k(\epsilon)$个约束的证书。以前，这仅对于偶数$k$是已知的，但对于奇数$k$，需要$n^{k/2} (\log n)^{O(1)} / \epsilon^2$个随机约束才能得出相同的结论。虽然改进仅是对数级别的，但它克服了这类结果的一个重要障碍。

    We define a novel notion of ``non-backtracking'' matrix associated to any symmetric matrix, and we prove a ``Ihara-Bass'' type formula for it.  We use this theory to prove new results on polynomial-time strong refutations of random constraint satisfaction problems with $k$ variables per constraints (k-CSPs). For a random k-CSP instance constructed out of a constraint that is satisfied by a $p$ fraction of assignments, if the instance contains $n$ variables and $n^{k/2} / \epsilon^2$ constraints, we can efficiently compute a certificate that the optimum satisfies at most a $p+O_k(\epsilon)$ fraction of constraints.  Previously, this was known for even $k$, but for odd $k$ one needed $n^{k/2} (\log n)^{O(1)} / \epsilon^2$ random constraints to achieve the same conclusion.  Although the improvement is only polylogarithmic, it overcomes a significant barrier to these types of results. Strong refutation results based on current approaches construct a certificate that a certain matrix associ
    
[^112]: COOL：一种上下文观察器及其在问答和其他自然语言处理任务中的应用

    COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks. (arXiv:2204.09593v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.09593](http://arxiv.org/abs/2204.09593)

    COOL是一种上下文观察器，用于编码局部句法上下文，能够改进基于Transformer的模型在自然语言处理任务中的性能，包括问答。

    

    视觉上下文观察器通过添加一个局部注意力形式的观察注意力改进了视觉Transformer的性能。与计算机视觉和其他领域一样，在自然语言处理中，基于Transformer的模型构成了大多数处理任务的最先进技术。我们提出了一种自然语言处理的观察注意力机制COOL。COOL添加到基于Transformer的模型的自我注意力层之上，考虑单词的接近性和更多的成对约束，编码局部句法上下文。使用不同的基于Transformer的模型对COOL的实现进行比较实证性能评估，证实其在各种自然语言处理任务中改进基线模型的机会，包括问答。

    Vision outlooker improves the performance of vision transformers, which implements a self-attention mechanism by adding an outlook attention, a form of local attention.  In natural language processing, as has been the case in computer vision and other domains, transformer-based models constitute the state-of-the-art for most processing tasks. In this domain, too, many authors have argued and demonstrated the importance of local context.  We present an outlook attention mechanism, COOL, for natural language processing. COOL, added on top of the self-attention layers of a transformer-based model, encodes local syntactic context considering word proximity and more pair-wise constraints than dynamic convolution used by existing approaches.  A comparative empirical performance evaluation of an implementation of COOL with different transformer-based models confirms the opportunity for improvement over a baseline using the original models alone for various natural language processing tasks, i
    
[^113]: 金字塔融合变换器用于语义分割

    Pyramid Fusion Transformer for Semantic Segmentation. (arXiv:2201.04019v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2201.04019](http://arxiv.org/abs/2201.04019)

    本文提出了一种基于变换器的金字塔融合方法用于语义分割，来挖掘跨特征金字塔的丰富语义信息，取得了有竞争力的表现。

    

    最近提出的MaskFormer为语义分割任务提供了一种新的视角：它从流行的像素级分类范式转移到了一种面向掩码级别的分类方法。本质上，它生成与类别片段对应的成对概率和掩码，并在推断过程中将它们结合起来生成分割图。在我们的研究中，我们发现仅基于单尺度特征的每个掩码分类解码器不足以提取可靠的概率或掩码。为了挖掘跨特征金字塔的丰富语义信息，我们提出了一种基于变换器的用于单尺度语义分割的金字塔融合变换器（PFT）。所提出的变换器解码器在每个空间特征和可学习查询之间并行执行交叉注意，并使用跨尺度间查询注意来交换补充信息。

    The recently proposed MaskFormer gives a refreshed perspective on the task of semantic segmentation: it shifts from the popular pixel-level classification paradigm to a mask-level classification method. In essence, it generates paired probabilities and masks corresponding to category segments and combines them during inference for the segmentation maps. In our study, we find that per-mask classification decoder on top of a single-scale feature is not effective enough to extract reliable probability or mask. To mine for rich semantic information across the feature pyramid, we propose a transformer-based Pyramid Fusion Transformer (PFT) for per-mask approach semantic segmentation with multi-scale features. The proposed transformer decoder performs cross-attention between the learnable queries and each spatial feature from the feature pyramid in parallel and uses cross-scale inter-query attention to exchange complimentary information. We achieve competitive performance on three widely use
    
[^114]: PARIS：用于改善睡眠质量的个性化活动推荐系统

    PARIS: Personalized Activity Recommendation for Improving Sleep Quality. (arXiv:2110.13745v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2110.13745](http://arxiv.org/abs/2110.13745)

    该论文利用机器学习技术，结合可穿戴设备监测的数据，通过时间序列聚类找到与指定主题相关的行为模型并生成相应的睡眠质量活动建议，为提高睡眠质量提供了一种个性化解决方案。

    

    睡眠质量对人们的身体和心理健康有深远影响。睡眠不足的人更容易报告身体和心理困扰、活动受限、焦虑和疼痛。此外，过去几年中，活动监测和健康跟踪的应用和设备方兴未艾。从这些可穿戴设备收集到的信号可用于研究和改善睡眠质量。本文利用实体活动和睡眠质量之间的关系，利用机器学习技术找到协助人们改善睡眠的方法。对活动数据进行时间序列聚类，我们找到与特定主题最显着的行为模式相关的簇中心。然后为每个行为模式中的每个簇生成有助于良好睡眠质量的活动建议。这些活动建议供应给每位用户的个性化活动推荐系统。

    The quality of sleep has a deep impact on people's physical and mental health. People with insufficient sleep are more likely to report physical and mental distress, activity limitation, anxiety, and pain. Moreover, in the past few years, there has been an explosion of applications and devices for activity monitoring and health tracking. Signals collected from these wearable devices can be used to study and improve sleep quality. In this paper, we utilize the relationship between physical activity and sleep quality to find ways of assisting people improve their sleep using machine learning techniques. People usually have several behavior modes that their bio-functions can be divided into. Performing time series clustering on activity data, we find cluster centers that would correlate to the most evident behavior modes for a specific subject. Activity recipes are then generated for good sleep quality for each behavior mode within each cluster. These activity recipes are supplied to an a
    
[^115]: AxoNN: 一种异步、消息驱动的极规模深度学习并行框架

    AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning. (arXiv:2110.13005v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.13005](http://arxiv.org/abs/2110.13005)

    AxoNN是一种利用异步性和消息驱动执行调度每个GPU上神经网络操作的并行深度学习框架，将CPU内存作为冗余空间，降低GPU内存消耗，同时将每个GPU的参数数量增加4倍，从而减少了必需的GPU数量和训练时间。

    

    近年来，训练最先进的神经网络所需的存储器容量已远远超出现代硬件加速器的DRAM容量。这促使我们在大规模基于GPU的集群上开发高效算法并行训练这些神经网络。在现代GPU上，计算相对廉价，为了提取最大性能，设计和实现这些并行训练算法中极其高效的通信是至关重要的。本文介绍了AxoNN，一种利用异步性和消息驱动执行调度每个GPU上神经网络操作的并行深度学习框架，从而减少GPU空闲时间，最大化硬件效率。通过使用CPU内存作为冗余空间，在训练期间定期卸载数据，AxoNN能够将GPU内存消耗降低4倍。这使得我们能够将每个GPU的参数数量增加4倍，从而减少了必需的GPU数量和训练时间。

    In the last few years, the memory requirements to train state-of-the-art neural networks have far exceeded the DRAM capacities of modern hardware accelerators. This has necessitated the development of efficient algorithms to train these neural networks in parallel on large-scale GPU-based clusters. Since computation is relatively inexpensive on modern GPUs, designing and implementing extremely efficient communication in these parallel training algorithms is critical for extracting the maximum performance. This paper presents AxoNN, a parallel deep learning framework that exploits asynchrony and message-driven execution to schedule neural network operations on each GPU, thereby reducing GPU idle time and maximizing hardware efficiency. By using the CPU memory as a scratch space for offloading data periodically during training, AxoNN is able to reduce GPU memory consumption by four times. This allows us to increase the number of parameters per GPU by four times, thus reducing the amount 
    
[^116]: 多组有Partial Labels的数据集整合的Label-Assemble方法

    Label-Assemble: Leveraging Multiple Datasets with Partial Labels. (arXiv:2109.12265v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2109.12265](http://arxiv.org/abs/2109.12265)

    "Label-Assemble"是一种整合带有Partial Labels的公共数据集来释放其全部潜力的方法。在新型疾病诊断中，从负样本中学习对计算机辅助诊断和检测都非常有帮助。具体而言，将NIH ChestX-ray14中的现有标签组装起来，可以将COVID-19的准确诊断率从96.3％提高到99.3％。此外，整合标签还可以提高疾病的检测能力。

    

    深度学习的成功往往依赖于大规模的标注数据集，但我们通常只能访问几个带有Partial Labels的小型数据集。为了解决这个问题，我们提出了一种新的倡议，即“Label-Assemble”，旨在从公共数据集的组合中释放Partial Labels的全部潜力。我们发现从负样本中学习有助于计算机辅助疾病诊断和检测，这个发现在新型疾病诊断中尤为重要，因为正样本往往难以收集，而负样本相对容易获得。例如，从NIH ChestX-ray14 （自2017年以来可用）中组装现有标签可将 COVID-19 的诊断准确率从96.3％提高到99.3％。除了诊断外，组装标签还可以改善疾病的检测，例如胰腺导管腺癌 (PDAC) 的检测可以从利用囊肿和胰腺内分泌肿瘤 (两种相关疾病) 的标签中获益，因为它们共享相同的成像特征。

    The success of deep learning relies heavily on large labeled datasets, but we often only have access to several small datasets associated with partial labels. To address this problem, we propose a new initiative, "Label-Assemble", that aims to unleash the full potential of partial labels from an assembly of public datasets. We discovered that learning from negative examples facilitates both computer-aided disease diagnosis and detection. This discovery will be particularly crucial in novel disease diagnosis, where positive examples are hard to collect, yet negative examples are relatively easier to assemble. For example, assembling existing labels from NIH ChestX-ray14 (available since 2017) significantly improves the accuracy of COVID-19 diagnosis from 96.3% to 99.3%. In addition to diagnosis, assembling labels can also improve disease detection, e.g., the detection of pancreatic ductal adenocarcinoma (PDAC) can greatly benefit from leveraging the labels of Cysts and PanNets (two othe
    
[^117]: TrAISformer——用于AIS轨迹预测的生成变压器

    TrAISformer-A generative transformer for AIS trajectory prediction. (arXiv:2109.03958v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2109.03958](http://arxiv.org/abs/2109.03958)

    本文提出了一种新颖的离散、高维表示的AIS数据和一种新的损失函数，用于显式考虑异质性和多样性，并通过修改变压器网络来预测船舶位置。实验证明，该模型明显优于现有的最新方法。

    

    预测船舶在未来特定时间点的位置是许多海事应用的基本方面。虽然自动识别系统（AIS）提供了丰富的信息来实现这一任务，但使用AIS数据来预测船舶轨迹仍然具有极大的挑战性，即使对于现代机器学习/深度学习技术也是如此，因为运动数据本质上具有复杂性和多样性。本文通过引入一种新颖的AIS数据离散化高维表示和一种新的损失函数来显式考虑异质性和多样性来解决这些挑战。所提出的模型——TrAISformer——是一个修改后的变压器网络，在所提出的丰富空间中提取AIS轨迹的长期相关性，以预测几个小时后船舶的位置。我们在公开可用的真实AIS数据上报告实验结果。TrAISformer明显优于现有的最新方法。

    The prediction of vessel positions at a specified point in the future is a fundamental aspect of many maritime applications. While Automatic Identification System (AIS) provides a rich source of information to enable this task, vessel trajectory forecasting using AIS data remains formidably challenging, even for modern machine learning/deep learning, because of the complexity and multimodality inherent in motion data. In this paper, we address these challenges by introducing a novel discrete, high-dimensional representation of AIS data and a new loss function to explicitly account for heterogeneity and multimodality. The proposed model -- referred to as TrAISformer -- is a modified transformer network that extracts long-term correlations of AIS trajectories in the proposed enriched space to forecast the positions of vessels several hours into the future. We report experimental results on publicly available, real AIS data. TrAISformer significantly outperforms state-of-the-art methods a
    
[^118]: 当机会来临时进行交易：基于注意力机制和迭代细化标注的价格变动预测

    Trade When Opportunity Comes: Price Movement Forecasting via Locality-Aware Attention and Iterative Refinement Labeling. (arXiv:2107.11972v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.11972](http://arxiv.org/abs/2107.11972)

    本论文提出了一种名为LARA的新型价格变动预测框架，包括两个主要部分：局部感知注意力和迭代细化标注。在真实世界的金融数据集上的实验结果表明，LARA在准确性和盈利能力方面优于现有的最先进方法。

    

    价格变动预测旨在根据当前市场情况和其他相关信息预测金融资产的未来趋势。最近，机器学习（ML）方法在学术界和工业界中越来越受欢迎，并取得了令人满意的结果。然而，由于金融数据的低信噪比和随机性极强，好的交易机会极为稀少。因此，如果不仔细选择潜在的盈利样本，这些ML方法容易捕捉到噪声而不是真实信号的模式。为解决这个问题，本研究提出了一种名为LARA的新型价格变动预测框架，包括两个主要部分：局部感知注意力（LA-Attention）和迭代细化标注（IRL）。LA-Attention旨在有选择地关注金融数据中最具信息量的局部区域，而IRL则旨在迭代地细化标注过程，过滤掉噪声和无关样本。在真实世界的金融数据集上的实验结果表明，LARA在准确性和盈利能力方面优于现有的最先进方法。

    Price movement forecasting aims at predicting the future trends of financial assets based on the current market conditions and other relevant information. Recently, machine learning (ML) methods have become increasingly popular and achieved promising results for price movement forecasting in both academia and industry. Most existing ML solutions formulate the forecasting problem as a classification (to predict the direction) or a regression (to predict the return) problem over the entire set of training data. However, due to the extremely low signal-to-noise ratio and stochastic nature of financial data, good trading opportunities are extremely scarce. As a result, without careful selection of potentially profitable samples, such ML methods are prone to capture the patterns of noises instead of real signals. To address this issue, we propose a novel price movement forecasting framework named LARA consisting of two main components: Locality-Aware Attention (LA-Attention) and Iterative R
    
[^119]: 自然和人工智能之间的权衡和协同：神经形态处理系统的自下而上和自上而下方法

    Bottom-up and top-down approaches for the design of neuromorphic processing systems: Tradeoffs and synergies between natural and artificial intelligence. (arXiv:2106.01288v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2106.01288](http://arxiv.org/abs/2106.01288)

    神经形态工程代表了一种基于脉冲神经网络体系结构实现的计算的范式转变，本文分别比较了自下而上以及自上而下的设计方法，并介绍和讨论了神经形态器件的物理设计和工艺制造、应用等方面。

    

    随着摩尔定律的接近尽头，对计算能力的指数期望正在推动寻求提高整体系统性能的新途径。其中之一是探索旨在实现生物神经处理系统的灵活性和计算效率的替代脑启发式计算体系结构。在这个背景下，神经形态工程代表了一种基于脉冲神经网络体系结构实现的计算的范式转变，在这种形态中，处理和存储被紧密地放置在一起。本文对这一领域进行了全面的概述，突出了实现这种范式转变的不同粒度级别，并比较了专注于复制自然智能（自下而上）与那些旨在解决实际人工智能应用（自上而下）的设计方法。首先，我们介绍了模拟、混合信号和数字电路设计风格，确定了它们之间的差异。其次，我们讨论了神经形态器件模板的物理设计和工艺制造，以及这些工艺制造的异质性和电性能对器件性能和整体系统设计的影响。最后，我们介绍了神经形态工程领域的应用，包括传感器应用、机器感知、低功耗/高效能处理和智能边缘设备应用。

    While Moore's law has driven exponential computing power expectations, its nearing end calls for new avenues for improving the overall system performance. One of these avenues is the exploration of alternative brain-inspired computing architectures that aim at achieving the flexibility and computational efficiency of biological neural processing systems. Within this context, neuromorphic engineering represents a paradigm shift in computing based on the implementation of spiking neural network architectures in which processing and memory are tightly co-located. In this paper, we provide a comprehensive overview of the field, highlighting the different levels of granularity at which this paradigm shift is realized and comparing design approaches that focus on replicating natural intelligence (bottom-up) versus those that aim at solving practical artificial intelligence applications (top-down). First, we present the analog, mixed-signal and digital circuit design styles, identifying the b
    
[^120]: Uni-Encoder: 一种用于生成型对话系统的快速准确响应选择范例

    Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems. (arXiv:2106.01263v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2106.01263](http://arxiv.org/abs/2106.01263)

    论文提出一种新的响应选择范例Uni-Encoder，解决了Cross-Encoder多次编码相同上下文计算成本高和Poly-Encoder性能下降的问题。该范例在一次前向传递中对所有候选与上下文进行编码。

    

    样本与排序是现代生成型对话系统的关键解码策略，通过从生成的少量候选答案中选择一个答案来实现多样化和高质量的响应。当前最先进的排序方法主要使用称为交叉编码器的编码范例，该编码器分别对每个上下文-候选对进行编码，并根据其适应度得分对候选进行排序。然而，交叉编码器为每个候选重复编码相同的冗长上下文，导致计算成本高。Poly-Encoder通过减少上下文和候选之间的交互来解决上述问题，但代价是性能下降。在这项工作中，我们开发了一种新的范例，称为Uni-Encoder，它像交叉编码器一样完全关注每个候选对，同时像Poly-编码器一样只编码一次上下文。Uni-Encoder在一次前向传递中对所有候选与上下文进行编码。我们针对所有候选使用相同的位置嵌入。

    Sample-and-rank is a key decoding strategy for modern generation-based dialogue systems. It helps achieve diverse and high-quality responses by selecting an answer from a small pool of generated candidates. The current state-of-the-art ranking methods mainly use an encoding paradigm called Cross-Encoder, which separately encodes each context-candidate pair and ranks the candidates according to their fitness scores. However, Cross-Encoder repeatedly encodes the same lengthy context for each candidate, resulting in high computational costs. Poly-Encoder addresses the above problems by reducing the interaction between context and candidates, but with a price of performance drop. In this work, we develop a new paradigm called Uni-Encoder, that keeps the full attention over each pair as in Cross-Encoder while only encoding the context once, as in Poly-Encoder. Uni-Encoder encodes all the candidates with the context in one forward pass. We use the same positional embedding for all candidates
    
[^121]: 多任务注意力残差网络用于论述挖掘

    Multi-Task Attentive Residual Networks for Argument Mining. (arXiv:2102.12227v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2102.12227](http://arxiv.org/abs/2102.12227)

    本文提出了一种多任务注意力残差网络架构，通过利用集成方法、注意力机制和多任务学习，无需假设文档或论据结构，成功应用于多个论述挖掘任务中，成为了一种既通用又高性能的架构。

    

    本文探讨了多任务注意力残差网络在多个论述挖掘任务中的应用。我们提出了一种残差架构，利用了注意力、多任务学习，并使用集成方法，不对文档或论据结构做任何假设。我们在五个不同的用户生成评论、科学出版物和劝说性论文语料库上进行了广泛的实验评估。我们的结果表明，我们的方法是针对具有更高计算印记或特定于语料库设计的最先进架构的强有力的竞争对手，代表了通用性、性能精度和减少模型大小之间的有趣折衷。

    We explore the use of residual networks and neural attention for multiple argument mining tasks. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble, without any assumption on document or argument structure. We present an extensive experimental evaluation on five different corpora of user-generated comments, scientific publications, and persuasive essays. Our results show that our approach is a strong competitor against state-of-the-art architectures with a higher computational footprint or corpus-specific design, representing an interesting compromise between generality, performance accuracy and reduced model size.
    

