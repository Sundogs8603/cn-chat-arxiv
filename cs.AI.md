# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Towards Practical Multi-Robot Hybrid Tasks Allocation for Autonomous Cleaning.](http://arxiv.org/abs/2303.06531) | 本文提出了一个新的鲁棒混合整数线性规划模型，用于解决多机器人自主清洁系统中的混合任务分配问题，并建立了一个包括100个实例的数据集。 |
| [^2] | [Making Batch Normalization Great in Federated Deep Learning.](http://arxiv.org/abs/2303.06530) | 本文研究了在联邦学习中使用批标准化和群组归一化的效果，发现在适当的处理下，批标准化可以在广泛的联邦学习设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。 |
| [^3] | [Opening Up the Neural Network Classifier for Shap Score Computation.](http://arxiv.org/abs/2303.06516) | 本文提出了一种高效计算机器学习模型分类中Shap解释分数的方法，通过将二进制神经网络转换为布尔电路，并使用知识编译技术，将电路视为开放式模型，通过最近的高效算法计算Shap分数，相比于将BNN视为黑盒模型直接计算Shap，性能有了显著的提高。 |
| [^4] | [Credit Card Fraud Detection Using Enhanced Random Forest Classifier for Imbalanced Data.](http://arxiv.org/abs/2303.06514) | 本文使用增强型随机森林分类器和合成少数类过采样技术来解决信用卡欺诈检测中的不平衡数据问题，获得了98%的准确度和F1分数值，具有实际应用价值。 |
| [^5] | [Accurate Prediction of Global Mean Temperature through Data Transformation Techniques.](http://arxiv.org/abs/2303.06468) | 本文研究了使用统计和简单的机器学习方法预测全球平均温度的优势，发现数据转换技术可以提高预测准确性。 |
| [^6] | [ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation.](http://arxiv.org/abs/2303.06458) | ZeroNLG是一个零样本学习框架，可以处理多个NLG任务，包括图像到文本、视频到文本和文本到文本，跨越英语、中文、德语和法语。它不需要任何标记的下游对进行训练，通过将不同的领域投影到共享的公共潜在空间中的相应坐标，桥接不同领域之间的差异。 |
| [^7] | [Graph Neural Network contextual embedding for Deep Learning on Tabular Data.](http://arxiv.org/abs/2303.06455) | 本文介绍了一种基于图神经网络的深度学习模型，使用交互网络进行上下文嵌入，用于表格数据的处理。该模型在公共数据集上的表现优于最近的深度学习基准调查，并且与提升树解决方案相比也取得了竞争性的结果。 |
| [^8] | [Mapping the Design Space of Interactions in Human-AI Text Co-creation Tasks.](http://arxiv.org/abs/2303.06430) | 本文提出了一系列内容生成任务及其相应的人工智能与人类交互模式，鼓励研究社区专注于更复杂和相互依赖的任务，这些任务需要更高水平的人类参与。 |
| [^9] | [Deep Reinforcement Learning Based Power Allocation for Minimizing AoI and Energy Consumption in MIMO-NOMA IoT Systems.](http://arxiv.org/abs/2303.06411) | 本文提出了一种基于深度强化学习的MIMO-NOMA IoT系统功率分配方法，以最小化AoI和能耗。 |
| [^10] | [Brain Diffuser: An End-to-End Brain Image to Brain Network Pipeline.](http://arxiv.org/abs/2303.06410) | 本文提出了一种基于扩散的端到端脑网络生成模型Brain Diffuser，直接从DTI中形成结构性脑网络。对于阿尔茨海默病的情况，所提出的模型在ADNI数据库上的表现优于现有工具包的结果。 |
| [^11] | [Automatic Detection of Signalling Behaviour from Assistance Dogs as they Forecast the Onset of Epileptic Seizures in Humans.](http://arxiv.org/abs/2303.06407) | 本研究探讨了如何自动检测辅助犬预测人类癫痫发作时的信号行为，以提高癫痫患者的生活质量。 |
| [^12] | [A Novel Method Combines Moving Fronts, Data Decomposition and Deep Learning to Forecast Intricate Time Series.](http://arxiv.org/abs/2303.06394) | 本文提出了一种新的方法，结合移动前沿、数据分解和深度学习，用于预测复杂时间序列。该方法通过经验小波变换将时间序列分解成更简单的组成序列，并使用移动前沿方法防止数据泄漏。 |
| [^13] | [Uncertainty-Aware Off-Policy Learning.](http://arxiv.org/abs/2303.06389) | 本文提出了一种不确定性感知的倒数概率分数估计器（UIPS），用于改进离线学习，通过明确模拟估计的记录策略中的不确定性，相对于广泛的最先进基线具有优越的样本效率。 |
| [^14] | [Probing neural representations of scene perception in a hippocampally dependent task using artificial neural networks.](http://arxiv.org/abs/2303.06367) | 本研究利用人工神经网络探究场景感知的神经表征在海马依赖任务中的应用，设计了一个新型场景感知基准，证明了DNNs可以学习从不同自我中心视角观察的场景的能力。 |
| [^15] | [Explainable AI for Time Series via Virtual Inspection Layers.](http://arxiv.org/abs/2303.06365) | 本文提出了一种虚拟检查层，将时间序列转换为可解释的表示，并允许通过本地XAI方法将相关性归因传播到该表示。我们将一系列XAI方法的适用性扩展到需要转换后才能解释输入的领域。我们展示了DFT-LRP在各种时间序列分类设置中的有用性，如音频和电子健康记录。 |
| [^16] | [Privacy-Preserving Cooperative Visible Light Positioning for Nonstationary Environment: A Federated Learning Perspective.](http://arxiv.org/abs/2303.06361) | 本文提出了一种基于联邦学习的合作可见光定位方案，通过共同训练适应环境变化的全局模型，提高了在非静态环境下的定位精度和泛化能力。 |
| [^17] | [FedLP: Layer-wise Pruning Mechanism for Communication-Computation Efficient Federated Learning.](http://arxiv.org/abs/2303.06360) | 本文提出了一种显式的FL剪枝框架FedLP，采用局部训练和联邦更新中的层次剪枝，对不同类型的深度学习模型具有普适性，可以缓解通信和计算的系统瓶颈，并且性能下降较小。 |
| [^18] | [AutoMLP: Automated MLP for Sequential Recommendations.](http://arxiv.org/abs/2303.06337) | AutoMLP是一种新颖的序列推荐系统，通过自动化和自适应搜索算法，更好地模拟用户的长期/短期兴趣，实现更好的推荐效果。 |
| [^19] | [Parachute: Evaluating Interactive Human-LM Co-writing Systems.](http://arxiv.org/abs/2303.06333) | 本文提出了一个以人为中心的评估框架Parachute，用于交互式共同撰写系统的评估，该框架包含了分类的实用指标，可以用于评估和比较共同撰写系统。 |
| [^20] | [A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training.](http://arxiv.org/abs/2303.06318) | 本文提出了一种新的混合并行算法，结合了张量、专家和数据并行，以实现MoE模型的训练，其基本模型比当前最先进的DeepSpeed-MoE大4-8倍。 |
| [^21] | [Adversarial Attacks and Defenses in Machine Learning-Powered Networks: A Contemporary Survey.](http://arxiv.org/abs/2303.06302) | 本文综述了机器学习网络中的对抗攻击和防御技术，重点关注基于深度神经网络的分类模型。对最近的对抗攻击方法和最先进的对抗防御技术进行了全面分类，并以视觉上吸引人的表格和树状图的形式呈现。方法分类为反攻击检测和鲁棒性增强，特别关注于基于正则化的增强鲁棒性的方法。还探讨了新的攻击途径，包括基于搜索的攻击。 |
| [^22] | [Stabilizing Transformer Training by Preventing Attention Entropy Collapse.](http://arxiv.org/abs/2303.06296) | 本文研究了Transformer的训练动态，发现低注意力熵伴随着高训练不稳定性，提出了一种简单而有效的解决方案$\sigma$Reparam，成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。 |
| [^23] | [Consistency Analysis of ChatGPT.](http://arxiv.org/abs/2303.06273) | 本文研究了ChatGPT的一致性问题，发现尽管它具有更好的语言理解能力，但仍然经常无法生成逻辑上正确的预测。因此，在现实世界的应用需要进一步考虑，特别是在风险方面。 |
| [^24] | [Interpretable Outlier Summarization.](http://arxiv.org/abs/2303.06261) | STAIR提出了一种可解释的异常值汇总方法，通过学习一组紧凑的人类可理解规则，以汇总和解释异常检测结果，具有强大的可解释性，以准确地总结检测结果。 |
| [^25] | [Predicting risk of delirium from ambient noise and light information in the ICU.](http://arxiv.org/abs/2303.06253) | 本研究利用环境噪声和光线信息，开发了第一个基于深度学习的ICU患者谵妄预测模型，为谵妄的预防和治疗提供了新思路。 |
| [^26] | [AI-Enhanced Intensive Care Unit: Revolutionizing Patient Care with Pervasive Sensing.](http://arxiv.org/abs/2303.06252) | 本文介绍了一种AI增强的重症监护室系统，通过普适感知和数据处理，可以改善患者的视觉监测和评估，提高护理质量。 |
| [^27] | [Zone-based Federated Learning for Mobile Sensing Data.](http://arxiv.org/abs/2303.06246) | 本文提出了一种基于区域的联邦学习方法，用于训练移动感知数据的深度学习模型。该方法将物理空间划分为地理区域，并映射到移动边缘云系统架构，以实现良好的模型准确性和可扩展性。每个区域都有一个联合训练模型，能够很好地适应该区域用户的数据和行为，并保护用户数据隐私。 |
| [^28] | [HYperbolic Self-Paced Learning for Self-Supervised Skeleton-based Action Representations.](http://arxiv.org/abs/2303.06242) | 本文提出了一种新的超似曲自适应模型（HYSP）用于学习基于骨架的动作表示，采用自监督学习，使用数据增强来生成同一样本的两个视图，并通过将一个视图与另一个视图匹配来学习，使用超似曲不确定性来确定算法学习速度，假设不确定性较小的样本应更强烈地推动训练，具有更大的权重和速度。 |
| [^29] | [Do we need entire training data for adversarial training?.](http://arxiv.org/abs/2303.06241) | 本文提出了一种新的对抗训练方法，通过对训练数据集进行筛选，仅使用易受对抗攻击的样本进行训练，从而减少训练时间。 |
| [^30] | [Complement Sparsification: Low-Overhead Model Pruning for Federated Learning.](http://arxiv.org/abs/2303.06237) | 本文提出了一种名为补充稀疏化的模型剪枝机制，通过在服务器和客户端之间进行互补和协作的剪枝来满足联邦学习中低双向通信开销、客户端低计算开销和良好模型准确性的要求。 |
| [^31] | [Who's Thinking? A Push for Human-Centered Evaluation of LLMs using the XAI Playbook.](http://arxiv.org/abs/2303.06223) | 本文探讨了以人为中心的人工智能系统评估，将相对成熟的可解释AI领域与快速发展的大型语言模型研究进行了类比，认为人类的需求应该成为LLMs评估的核心。 |
| [^32] | [The Carbon Emissions of Writing and Illustrating Are Lower for AI than for Humans.](http://arxiv.org/abs/2303.06219) | 本文研究了几个AI系统相对于人类完成相同任务的排放量，发现AI写作和插图的碳排放比人类低130到1500倍和310到2900倍，目前使用AI有可能以比人类更低的排放水平完成几项重要活动。 |
| [^33] | [CHGNN: A Semi-Supervised Contrastive Hypergraph Learning Network.](http://arxiv.org/abs/2303.06213) | CHGNN是一种半监督对比超图学习网络，利用自监督对比学习技术从标记和未标记的数据中学习，包括自适应超图视图生成器、改进的超图编码器和联合损失函数。 |
| [^34] | [Weighted Notions of Fairness with Binary Supermodular Chores.](http://arxiv.org/abs/2303.06212) | 本文研究了在具有二元超模成本函数的代理之间分配不可分割家务的问题，并提出了一个通用的公平分配框架，适用于这类估值函数。该框架可以有效地计算满足加权公平性概念的分配。 |
| [^35] | [A POV-based Highway Vehicle Trajectory Dataset and Prediction Architecture.](http://arxiv.org/abs/2303.06202) | 介绍了一个基于POV的高速公路车辆轨迹数据集和预测架构，其中的Carolinas Highway Dataset（CHD）包括160万帧和338,000个车辆轨迹。 |
| [^36] | [Optimizing Federated Learning for Medical Image Classification on Distributed Non-iid Datasets with Partial Labels.](http://arxiv.org/abs/2303.06180) | 本文提出了FedFBN，一个联邦学习框架，使用预训练的网络作为模型后端，并在整个训练过程中冻结批量归一化层，以优化分布式非独立同分布数据和部分标签的医学图像分类。 |
| [^37] | [Software Vulnerability Prediction Knowledge Transferring Between Programming Languages.](http://arxiv.org/abs/2303.06177) | 本研究提出了一种转移学习技术，利用可用数据集生成一个模型，以检测不同编程语言中的常见漏洞。结果表明，所提出的模型以平均召回率为72％检测C和Java代码中的漏洞。 |
| [^38] | [Unifying Grokking and Double Descent.](http://arxiv.org/abs/2303.06173) | 本文提出了一种模式学习速度框架，用于统一理解Grokking和双重下降，同时提供了模型智能Grokking的第一个演示。 |
| [^39] | [Understanding the Synergies between Quality-Diversity and Deep Reinforcement Learning.](http://arxiv.org/abs/2303.06164) | 本文提出了广义演员-评论家QD-RL框架，用于QD-RL设置中的演员-评论家深度RL方法。该框架引入了两种新算法，PGA-ME（SAC）和PGA-ME（DroQ），将深度RL的最新进展应用于QD-RL设置，并解决了现有QD-RL算法无法解决的人形环境问题。 |
| [^40] | [Digital Twin-Assisted Knowledge Distillation Framework for Heterogeneous Federated Learning.](http://arxiv.org/abs/2303.06155) | 本文提出了一种数字孪生辅助的知识蒸馏框架，用于解决联邦学习系统中的异构性问题，用户可以选择自己的神经网络模型并从大型教师模型中蒸馏知识，同时利用数字孪生在服务器上训练大型教师模型，最终通过混合整数规划和Q-learning算法实现模型选择和资源分配。 |
| [^41] | [Why is That a Good or Not a Good Frying Pan? -- Knowledge Representation for Functions of Objects and Tools for Design Understanding, Improvement, and Generation for Design Understanding, Improvement, and Generation.](http://arxiv.org/abs/2303.06152) | 本文演示了如何使用通用函数表示语言和框架来表示特定对象及其参与支持其设计的过程，从而实现深入的概念理解，可解释性的功能，使系统能够回答“为什么”问题。 |
| [^42] | [NoiseCAM: Explainable AI for the Boundary Between Noise and Adversarial Attacks.](http://arxiv.org/abs/2303.06151) | 本文提出了一种名为NoiseCAM的算法，该算法可以定位易受攻击层并检测对抗性示例，同时不会对混入输入的高斯随机噪声做出反应。 |
| [^43] | [Real-time scheduling of renewable power systems through planning-based reinforcement learning.](http://arxiv.org/abs/2303.05205) | 本文提出了一种基于规划强化学习算法和真实电力网环境的系统解决方案，可以实现发电机的规划和更细的时间分辨率调整，从而增加了电网的能力。 |
| [^44] | [NASTyLinker: NIL-Aware Scalable Transformer-based Entity Linker.](http://arxiv.org/abs/2303.04426) | NASTyLinker是一种NIL感知的实体链接器，它通过生成提及簇来表示NIL实体，并在保持已知实体高链接性能的同时解决冲突。 |
| [^45] | [Leveraging Pre-trained AudioLDM for Text to Sound Generation: A Benchmark Study.](http://arxiv.org/abs/2303.03857) | 本文研究了使用预训练的AudioLDM作为声音生成的骨干的优势，证明了在数据稀缺情况下使用预训练模型进行文本到声音生成的优势，并在几个常用数据集上使用相同的评估协议评估了各种文本到声音生成系统，为未来的研究提供了基础。 |
| [^46] | [Reducing Spurious Correlations for Aspect-Based Sentiment Analysis with Variational Information Bottleneck and Contrastive Learning.](http://arxiv.org/abs/2303.02846) | 本文提出了一种新的对比变分信息瓶颈框架（CVIB），以减少方面情感分析（ABSA）中的虚假相关性。该框架由一个原始网络和一个自剪枝网络组成，通过对比学习同时进行优化，从而丢弃了输入特征和预测标签之间的多余模式或虚假相关性。 |
| [^47] | [Prismer: A Vision-Language Model with An Ensemble of Experts.](http://arxiv.org/abs/2303.02506) | Prismer是一种数据和参数高效的视觉语言模型，它利用了一组领域专家的集合，通过汇集这些专家知识并将其适应于各种视觉语言推理任务，实现了与当前最先进模型竞争的微调和少样本学习性能，同时需要少至两个数量级的训练数据。 |
| [^48] | [Learning to Influence Human Behavior with Offline Reinforcement Learning.](http://arxiv.org/abs/2303.02265) | 本文提出了一种通过离线强化学习学习影响人类行为的方法，可以提高人类在协作任务中的表现。 |
| [^49] | [Uncertainty Estimation by Fisher Information-based Evidential Deep Learning.](http://arxiv.org/abs/2303.02045) | 本文提出了一种基于Fisher信息的证据深度学习方法，用于解决高数据不确定性样本但注释为one-hot标签的情况下证据学习过程被过度惩罚并受到阻碍的问题。 |
| [^50] | [Fine-grained Emotional Control of Text-To-Speech: Learning To Rank Inter- And Intra-Class Emotion Intensities.](http://arxiv.org/abs/2303.01508) | 本文提出了一种细粒度可控情感TTS，考虑了内部和外部类距离，并能够合成具有可识别强度差异的语音。 |
| [^51] | [Mesh-SORT: Simple and effective location-wise tracker with lost management strategies.](http://arxiv.org/abs/2302.14415) | Mesh-SORT是一种简单而有效的位置跟踪器，通过网格分割帧并提出相应的位置感知的丢失管理策略和不同的匹配策略，解决了跟踪中的不良检测器和遮挡问题。 |
| [^52] | [Revisiting Self-Training with Regularized Pseudo-Labeling for Tabular Data.](http://arxiv.org/abs/2302.14013) | 本文重新审视了自我训练，并引入了课程伪标签用于表格领域。提出了一种新的伪标签方法，它规范化了置信度分数。 |
| [^53] | [Explainable AI is Dead, Long Live Explainable AI! Hypothesis-driven decision support.](http://arxiv.org/abs/2302.12389) | 本文主张从当前的可解释性人工智能（XAI）模式转变，采用基于假设的决策支持系统，以更好地支持人类决策。 |
| [^54] | [Multimodal Federated Learning via Contrastive Representation Ensemble.](http://arxiv.org/abs/2302.08888) | 本文提出了一种名为CreamFL的多模态联邦学习框架，可以从具有异构模型架构和数据模态的客户端中训练更大的服务器模型，同时只在公共数据集上传递知识。 |
| [^55] | [Vision, Deduction and Alignment: An Empirical Study on Multi-modal Knowledge Graph Alignment.](http://arxiv.org/abs/2302.08774) | 本研究构建了八个大规模的、配备图像的实体对齐基准Multi-OpenEA，并开发了一种新的多模态EA方法LODEME，利用逻辑推理和多模态KG嵌入，实现了最先进的性能。 |
| [^56] | ["Correct answers" from the psychology of artificial intelligence.](http://arxiv.org/abs/2302.07267) | 本文使用OpenAI的GPT3.5模型重新复制了Many Labs 2复制项目中的14项研究，其中8项研究的结果被成功复制。然而，对于剩下的6项研究，GPT3.5以极其预定的方式回答了调查问题，导致无法分析这些研究。 |
| [^57] | [Sources of Richness and Ineffability for Phenomenally Conscious States.](http://arxiv.org/abs/2302.06403) | 本文提供了一个信息论动力系统的视角，来解释意识的丰富性和难以言说性。在我们的框架中，意识体验的丰富性对应于意识状态中的信息量，而难以言说性则对应于不同处理阶段丢失的信息量。 |
| [^58] | [Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training.](http://arxiv.org/abs/2302.05045) | 本文提出了一种新的方法，利用稀疏子网络来优化两种流行的深度学习并行算法 - 数据并行和层间并行的内存利用和通信。在512个NVIDIA V100 GPU上，我们的优化将27亿参数模型的内存消耗减少了74％，总通信时间减少了40％。 |
| [^59] | [The Unfairness of Fair Machine Learning: Levelling down and strict egalitarianism by default.](http://arxiv.org/abs/2302.02404) | 公平机器学习的定义过于简单，许多公平性措施会导致性能降级和水平下降，使每个人都变得更糟，这种做法不符合实质平等的机会。 |
| [^60] | [Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective.](http://arxiv.org/abs/2302.01735) | 本文提出了ARCO，一种半监督对比学习（CL）框架，其中包括医学图像分割中的分层组采样理论。通过方差缩减估计的概念来构建ARCO，并表明某些方差缩减技术在医学图像分割中特别有益。 |
| [^61] | [Simulating the Integration of Urban Air Mobility into Existing Transportation Systems: A Survey.](http://arxiv.org/abs/2301.12901) | 本文调查了城市空中出行（UAM）在大都市交通中的研究现状，确定了将UAM融入城市交通系统的关键挑战和机遇，包括对现有交通模式和拥堵的影响；安全分析和风险评估；潜在的经济和环境效益；以及为UAM和地面交通开发共享基础设施和路线。同时，我们讨论了UAM的潜在好处，如缩短旅行时间和改善服务不足地区的可达性。 |
| [^62] | [LDMIC: Learning-based Distributed Multi-view Image Coding.](http://arxiv.org/abs/2301.09799) | LDMIC是一种基于学习的分布式多视图图像编码框架，通过独立编码器和联合上下文传输模块实现了全局视图间的相关性捕捉，对几何关系不敏感。 |
| [^63] | [Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks.](http://arxiv.org/abs/2301.09245) | 引入神经元多样性可以解决人工神经网络的基本问题，走向神经人工智能。 |
| [^64] | [SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations.](http://arxiv.org/abs/2301.07074) | SegViz是一种基于联邦学习的框架，用于从分布式的非i.i.d数据集中训练具有部分注释的分割模型。使用FedBN作为聚合策略的SegViz框架在外部BTCV集上表现出优异的性能，分割的dice分数分别为0.93、0.83、0.55和0.75。 |
| [^65] | [Image Data Augmentation Approaches: A Comprehensive Survey and Future directions.](http://arxiv.org/abs/2301.02830) | 本文综述了图像数据增强方法，提供了全面的分类法和每种技术的优缺点，并给出了数据增强对图像分类、目标检测和语义分割等计算机视觉任务的全面结果。 |
| [^66] | [Graph Learning and Its Applications: A Holistic Survey.](http://arxiv.org/abs/2212.08966) | 本文全面综述了图学习的发展历程和应用场景，重点介绍了表示学习在文本、图像、化学和生物等领域中的显著性能，同时指出了对以前有价值的工作进行调查的需求。 |
| [^67] | [Fine-tuned CLIP Models are Efficient Video Learners.](http://arxiv.org/abs/2212.03640) | 本文提出了一个简单的视频细调CLIP（ViFi-CLIP）基线，通过从CLIP图像编码器的帧级处理，接着进行特征池化和与相应文本嵌入的相似度匹配，有效地将图像级别的CLIP表示转移到视频中，从而弥合了从图像到视频的领域差距。 |
| [^68] | [Unifying Vision, Text, and Layout for Universal Document Processing.](http://arxiv.org/abs/2212.02623) | 本文提出了通用文档处理（UDOP）模型，将文本、图像和布局模态以及各种任务格式统一起来，通过一种新颖的Transformer模型实现预训练和多域下游任务的统一，同时实现了高质量的神经文档编辑和内容定制。 |
| [^69] | [Dunhuang murals contour generation network based on convolution and self-attention fusion.](http://arxiv.org/abs/2212.00935) | 本文提出了一种基于卷积和自注意力融合的敦煌壁画轮廓生成网络，旨在解决传统卷积神经网络在感受野扩大时丢失局部细节信息的问题，以生成合理的壁画轮廓图。 |
| [^70] | [From Forks to Forceps: A New Framework for Instance Segmentation of Surgical Instruments.](http://arxiv.org/abs/2211.16200) | 本研究提出了一种新的神经网络框架，将分类模块作为现有实例分割模型的新阶段添加，用于改善手术器械实例分割中的分类问题。该模块包括多尺度掩模注意力，用于关注器械区域并掩盖分散的背景特征。 |
| [^71] | [VideoFACT: Detecting Video Forgeries Using Attention, Scene Context, and Forensic Traces.](http://arxiv.org/abs/2211.15775) | 本文提出了一种新的网络VideoFACT，它利用取证嵌入、上下文嵌入和深度自我注意机制来检测和定位各种视频伪造和操纵，克服了现有网络在分析视频时面临的挑战。 |
| [^72] | [HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization.](http://arxiv.org/abs/2211.08253) | 本文提出了一种新的领域泛化方法HMOE，它不需要领域标签，更具可解释性，使用超网络生成专家权重，能够在低维向量空间中探索专家的相似性，实验结果表明HMOE可以划分混合数据并取得更好的效果。 |
| [^73] | [Learning Neuro-symbolic Programs for Language Guided Robot Manipulation.](http://arxiv.org/abs/2211.06652) | 该论文提出了一种学习神经符号程序以进行语言引导的机器人操作的方法，可以处理语言和感知变化，端到端可训练，不需要中间监督。该方法使用符号推理构造，在潜在的神经物体为中心的表示上操作，允许对输入场景进行更深入的推理。 |
| [^74] | [Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis.](http://arxiv.org/abs/2211.02641) | 本文介绍了一种基于SPD流形的图神经网络用于运动想象分类，利用EEG的二阶统计量，相比传统方法具有更好的性能。 |
| [^75] | [WiserVR: Semantic Communication Enabled Wireless Virtual Reality Delivery.](http://arxiv.org/abs/2211.01241) | WiserVR提出了一种新的框架，利用语义通信和深度学习技术，实现了高效的无线虚拟现实传输，其中包括语义位置图和联合语义通道编码方法。 |
| [^76] | [Using Emotion Embeddings to Transfer Knowledge Between Emotions, Languages, and Annotation Formats.](http://arxiv.org/abs/2211.00171) | 本文研究了如何通过利用多语言模型和Demux来构建一个可以在不同情感、语言和注释格式之间转换的单一模型，以实现知识共享和降低训练成本。 |
| [^77] | [Diffusion models for missing value imputation in tabular data.](http://arxiv.org/abs/2210.17128) | 本文提出了一种名为“表格数据条件分数扩散模型”（TabCSDI）的扩散模型方法，用于处理表格数据中的缺失值插补，该方法同时处理分类变量和数值变量，实验结果表明其在各种数据集上都取得了优异的性能。 |
| [^78] | [ViTASD: Robust Vision Transformer Baselines for Autism Spectrum Disorder Facial Diagnosis.](http://arxiv.org/abs/2210.16943) | 本文提出了一种使用Vision Transformer进行儿童ASD计算分析的方法，该方法从大型面部表情数据集中提取知识，并提供模型结构可转移性。在标准ASD面部分析基准测试上进行的大量实验表明，ViTASD-L实现了新的最先进水平。 |
| [^79] | [Leveraging Label Correlations in a Multi-label Setting: A Case Study in Emotion.](http://arxiv.org/abs/2210.15842) | 本文研究了利用标签相关性来改善情感检测的方法，开发了两种建模方法来捕捉情感词本身的词汇关联性，并将情感表示的成对约束作为正则化项与模型的分类损失一起集成，展示了在SemEval 2018任务1 E-c中使用单语BERT模型展示了西班牙语、英语和阿拉伯语的最新性能。 |
| [^80] | [Articulation GAN: Unsupervised modeling of articulatory learning.](http://arxiv.org/abs/2210.15173) | 本文提出了一种新的无监督生成模型，通过完全无监督的方式学习生成关节表示（电磁关节成像或EMA），更接近于人类语音产生的方式，从而更好地模拟人类语音产生的过程。 |
| [^81] | [Broken Neural Scaling Laws.](http://arxiv.org/abs/2210.14891) | 本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。 |
| [^82] | [A Late Multi-Modal Fusion Model for Detecting Hybrid Spam E-mail.](http://arxiv.org/abs/2210.14616) | 本研究旨在设计一种有效的方法来过滤混合垃圾邮件，提出了一种多模态融合模型，解决了传统基于文本或基于图像的过滤器无法检测到混合垃圾邮件的问题。 |
| [^83] | [D-Shape: Demonstration-Shaped Reinforcement Learning via Goal Conditioning.](http://arxiv.org/abs/2210.14428) | D-Shape是一种新的结合IL和RL的方法，它使用奖励塑形和目标条件化RL的思想来解决次优演示与回报最大化目标之间的冲突，能够在稀疏奖励网格世界领域中提高样本效率并一致地收敛到最优策略。 |
| [^84] | [Policy-Guided Lazy Search with Feedback for Task and Motion Planning.](http://arxiv.org/abs/2210.14055) | 本文提出了一种用于PDDLStream问题的求解器LAZY，它在动作骨架上维护单个集成搜索，随着在运动规划期间懒惰地绘制可能运动的样本，逐渐变得更具几何信息。同时，学习模型的目标导向策略和当前运动采样数据合并到LAZY中，以自适应地引导任务规划器，这导致了在不同数量的对象、目标和初始条件的未见测试环境中评估可行解搜索的显着加速。 |
| [^85] | [Augmentation with Projection: Towards an Effective and Efficient Data Augmentation Paradigm for Distillation.](http://arxiv.org/abs/2210.11768) | 提出了一种名为AugPro的数据增强方法，它是一种有效且高效的蒸馏数据增强方法，可以避免偏移决策边界，计算开销小。 |
| [^86] | [Discovering Many Diverse Solutions with Bayesian Optimization.](http://arxiv.org/abs/2210.10953) | ROBOT是一种新的贝叶斯优化方法，可以找到一组高性能、多样化的解决方案，解决了传统单目标贝叶斯优化方法只能找到一个最佳解决方案的局限性。 |
| [^87] | [Differentiable Physics Simulation of Dynamics-Augmented Neural Objects.](http://arxiv.org/abs/2210.09420) | 本文提出了一种可微分的流程，用于模拟将物体的几何形状表示为连续密度场的对象的运动，从密度场中估计物体的动力学特性，并引入了一种基于密度场的可微接触模型，使机器人能够自主构建物体模型并优化神经对象的抓取和操作轨迹。 |
| [^88] | [Relational Attention: Generalizing Transformers for Graph-Structured Tasks.](http://arxiv.org/abs/2210.05062) | 本文提出了一种关系注意力机制，将Transformer推广到图结构任务中，通过考虑和更新边向量，实现了对图结构数据的推理，相比于专门设计用于推理图结构数据的最先进的图神经网络，在各种图结构任务上都有明显的优势。 |
| [^89] | [Unsupervised Model Selection for Time-series Anomaly Detection.](http://arxiv.org/abs/2210.01078) | 本文提出了一种无监督模型选择方法，用于时间序列异常检测，通过三类替代度量，即预测误差、模型中心性和注入合成异常的性能，选择最准确的模型。 |
| [^90] | [Learning Transferable Spatiotemporal Representations from Natural Script Knowledge.](http://arxiv.org/abs/2209.15280) | 本文提出了一种新的预文本任务，利用自然语言知识来提高可转移的时空表示学习，通过关注学习到的视频表示来对打乱的ASR脚本进行排序，从而提高视频理解的进展。 |
| [^91] | [Learning Sparse Graphon Mean Field Games.](http://arxiv.org/abs/2209.03880) | 本文提出了一种新型的GMFG公式，称为LPGMFG，它利用$L^p$图形的图形理论概念，提供了一种机器学习工具，以有效且准确地近似解决稀疏网络问题，特别是幂律网络。我们推导出理论存在和收敛保证，并给出了实证例子，证明了我们的学习方法的准确性。 |
| [^92] | [EGFR Mutation Prediction of Lung Biopsy Images using Deep Learning.](http://arxiv.org/abs/2208.12506) | 本文使用深度学习技术，通过对肺活检图像进行分析，实现了对EGFR突变的预测，为肺癌治疗提供了更经济、更快捷的诊断方法。 |
| [^93] | [A Generalization of the Shortest Path Problem to Graphs with Multiple Edge-Cost Estimates.](http://arxiv.org/abs/2208.11489) | 本文提出了一个广义的加权有向图框架，其中可以多次计算（估计）边缘权重，以提高准确性和运行时间成本，解决了最短路径问题的不确定性。 |
| [^94] | [Localized Sparse Incomplete Multi-view Clustering.](http://arxiv.org/abs/2208.02998) | 本文提出了一种名为局部稀疏不完整多视图聚类（LSIMVC）的方法，通过优化稀疏正则化和新颖的图嵌入多视图矩阵分解模型，从不完整的多视图数据中学习稀疏和结构化的共识潜在表示，以解决不完整多视图聚类中的问题。 |
| [^95] | [Classification and Generation of real-world data with an Associative Memory Model.](http://arxiv.org/abs/2207.04827) | 本文提出了一种基于联想记忆模型的多模态框架，可以以容错的方式存储和检索大量真实世界数据，并且可以用于推断缺失的模态。 |
| [^96] | [$L_2$BN: Enhancing Batch Normalization by Equalizing the $L_2$ Norms of Features.](http://arxiv.org/abs/2207.02625) | 本文提出了一种$L_2$BN方法，通过等化样本特征的$L_2$范数来增强批量归一化，可以增强内类别特征的紧凑性并扩大跨类别特征的差异，易于实现，可以用作神经网络的基本归一化方法。 |
| [^97] | [A Dataset on Malicious Paper Bidding in Peer Review.](http://arxiv.org/abs/2207.02303) | 本文提供了一份关于同行评审中恶意投标的数据集，填补了这一领域缺乏公开数据的空白。 |
| [^98] | [DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech.](http://arxiv.org/abs/2207.01063) | 本文介绍了一个高质量的对话语音数据集DailyTalk，专门为对话TTS设计。DailyTalk可以用作通用的TTS数据集，而且基线可以表示DailyTalk的上下文信息。 |
| [^99] | [Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation.](http://arxiv.org/abs/2206.02307) | 本文提出了一种基于解剖感知对比蒸馏的半监督医学图像分割引导启动方法，通过软标记负样本和捕获更多语义上相似的特征来解决医学图像数据不平衡的问题。 |
| [^100] | [DM$^2$: Decentralized Multi-Agent Reinforcement Learning for Distribution Matching.](http://arxiv.org/abs/2206.00233) | 本文提出了一种基于分布匹配的去中心化多智能体强化学习方法，每个智能体独立地最小化与目标访问分布的相应分量的分布不匹配，可以实现收敛到生成目标分布的联合策略。 |
| [^101] | [Adversarial random forests for density estimation and generative modeling.](http://arxiv.org/abs/2205.09435) | 本文提出了一种使用对抗随机森林进行密度估计和数据合成的方法，该方法可以提供平滑的（非）条件密度，并允许完全合成数据生成，同时在各种表格数据基准测试中实现了与最先进的概率电路和深度学习模型相当或更好的性能，平均执行速度快了两个数量级。 |
| [^102] | [Visuomotor Control in Multi-Object Scenes Using Object-Aware Representations.](http://arxiv.org/abs/2205.06333) | 本文探讨了使用物体感知表示学习技术进行机器人任务的有效性，以解决当前方法学习任务特定表示不能很好地转移到其他任务的问题，以及由监督方法学习的表示需要大量标记数据集的问题。 |
| [^103] | [Representation Learning by Detecting Incorrect Location Embeddings.](http://arxiv.org/abs/2204.04788) | 本文提出了一种新的自监督学习（SSL）损失，用于图像表示学习。通过检测错误的位置嵌入，我们可以提高深度神经网络的泛化能力，使其更加鲁棒。我们称这种方法为DILEMMA，将其应用于MoCoV3、DINO和SimCLR，分别显示它们的性能提高了4.41%、3.97%和0.5%。 |
| [^104] | [DynLight: Realize dynamic phase duration with multi-level traffic signal control.](http://arxiv.org/abs/2204.03471) | 本文已被撤回，原因是语言和理论描述不够令人满意，作者已经进行了修订和更新。 |
| [^105] | [Separate and conquer heuristic allows robust mining of contrast sets in classification, regression, and survival data.](http://arxiv.org/abs/2204.00497) | 本文提出了一种基于分离征服的对比集挖掘算法RuleKit-CS，该算法通过多次通过伴随属性惩罚方案提供描述具有不同属性的相同示例的对比集，区别于标准的分离征服。该算法还被推广到回归和生存数据，允许识别标签属性/生存预测与预定义对比组的标签/预测一致的对比集。 |
| [^106] | [Generative Modeling Helps Weak Supervision (and Vice Versa).](http://arxiv.org/abs/2203.12023) | 本文提出了一种融合程序弱监督和生成对抗网络的模型，通过对齐离散潜在变量和弱监督派生的标签估计，改善了未观察到的标签的估计，实现了数据增强。 |
| [^107] | [Learning Torque Control for Quadrupedal Locomotion.](http://arxiv.org/abs/2203.05194) | 本文提出了一种基于扭矩的强化学习框架，直接预测关节扭矩，避免使用PD控制器，通过广泛的实验验证，四足动物能够穿越各种地形并抵抗外部干扰，同时保持运动。 |
| [^108] | [Practical Network Acceleration with Tiny Sets.](http://arxiv.org/abs/2202.07861) | 本文提出了一种基于删除块的方法，用于加速稀缺训练样本的网络，通过提出的可恢复性概念选择要删除的块，提出了一种名为PRACTISE的算法，仅使用微小的训练图像集来加速网络，PRACTISE的表现优于以往的方法。 |
| [^109] | [PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX.](http://arxiv.org/abs/2202.04110) | PGMax是一个用于离散概率图模型的因子图工具，可以在JAX中自动运行高效且可扩展的循环置信传播，与现有替代方案相比，PGMax获得了更高质量的推理结果，推理时间加速高达三个数量级。 |
| [^110] | [Temporal Sentence Grounding in Videos: A Survey and Future Directions.](http://arxiv.org/abs/2201.08071) | 本文综述了视频中的时间句子定位（TSGV）的基本概念和当前研究现状，以及未来研究方向。TSGV旨在从未经修剪的视频中检索与语言查询语义对应的时间时刻，连接计算机视觉和自然语言，是两个社区研究人员的重点关注点。 |
| [^111] | [Egeria: Efficient DNN Training with Knowledge-Guided Layer Freezing.](http://arxiv.org/abs/2201.06227) | Egeria是一种基于知识引导的DNN训练系统，通过跳过DNN层的计算和通信来实现高效训练，利用参考模型中的语义知识准确评估单个层的训练可塑性，并安全地冻结已收敛的层，从而节省相应的反向计算和通信。 |
| [^112] | [Developing a Trusted Human-AI Network for Humanitarian Benefit.](http://arxiv.org/abs/2112.11191) | 本文提出了一种可信的人工智能通信网络，将通信协议、区块链技术和信息融合与AI集成，以改善冲突通信，为人道主义利益提供可问责信息交换。 |
| [^113] | [Two-view Graph Neural Networks for Knowledge Graph Completion.](http://arxiv.org/abs/2112.09231) | 本文提出了一种名为WGE的图神经网络模型，通过两个单一的实体和关系为中心的图来学习实体和关系的向量表示，并在知识图谱补全任务上取得了优异的性能。 |
| [^114] | [Learning to Search in Task and Motion Planning with Streams.](http://arxiv.org/abs/2111.13144) | 本文提出了一种几何信息的符号规划器，使用图神经网络优先级排序，以最佳优先方式扩展对象和事实集合，从而改善了在任务和动作规划中的长期推理能力。在7自由度机械臂的堆叠操纵任务中得到了应用。 |
| [^115] | [Fingerprint Presentation Attack Detection by Channel-wise Feature Denoising.](http://arxiv.org/abs/2111.07620) | 本文提出了一种基于通道特征去噪的指纹呈现攻击检测方法，通过处理先前研究中忽略的冗余噪声信息来学习指纹图像的重要特征，具有较好的鲁棒性和准确性。 |
| [^116] | [A trained humanoid robot can perform human-like crossmodal social attention and conflict resolution.](http://arxiv.org/abs/2111.01906) | 本研究采用跨模态冲突解决的神经机器人范例，使机器人表现出类人的社交关注，为增强人机社交互动提供了新思路。 |
| [^117] | [Holistic Deep Learning.](http://arxiv.org/abs/2110.15829) | 本文提出了一种全面深度学习框架，通过解决输入扰动的脆弱性、过度参数化和性能不稳定性等挑战，全面提高了准确性、鲁棒性、稀疏性和稳定性，适用于表格和图像数据集。提供了选择适当的训练损失函数的建议。 |
| [^118] | [A comprehensive review of Binary Neural Network.](http://arxiv.org/abs/2110.06804) | 本文全面综述了二进制神经网络的最新发展，重点关注1位激活和1位卷积网络的权重，这些网络可以在微小的受限设备上实现和嵌入，并节省大量存储、计算成本和能量消耗。 |
| [^119] | [Self-Attention Networks Can Process Bounded Hierarchical Languages.](http://arxiv.org/abs/2105.11115) | 本文证明了自注意力网络可以处理深度受限的$\mathsf{Dyck}_{k}$子集$\mathsf{Dyck}_{k,D}$，这更好地捕捉了自然语言的有界层次结构。 |
| [^120] | [NOMU: Neural Optimization-based Model Uncertainty.](http://arxiv.org/abs/2102.13640) | NOMU是一种新的神经网络模型，可以在无噪声设置下，通过设计一个由两个连接的子NN组成的网络架构，并使用精心设计的损失函数进行训练，来捕捉NN的模型不确定性。该模型满足五个关于模型不确定性的重要愿望。 |

# 详细

[^1]: 面向自主清洁的多机器人混合任务分配的实践研究

    Towards Practical Multi-Robot Hybrid Tasks Allocation for Autonomous Cleaning. (arXiv:2303.06531v1 [cs.RO])

    [http://arxiv.org/abs/2303.06531](http://arxiv.org/abs/2303.06531)

    本文提出了一个新的鲁棒混合整数线性规划模型，用于解决多机器人自主清洁系统中的混合任务分配问题，并建立了一个包括100个实例的数据集。

    This paper proposes a novel robust mixed-integer linear programming model for multi-robot hybrid-task allocation in uncertain autonomous cleaning systems, and establishes a dataset of 100 instances.

    任务分配在多机器人自主清洁系统中起着至关重要的作用，多个机器人一起工作以清洁大面积区域。然而，迄今为止相关研究存在几个问题。大多数当前研究主要关注于确定性的单任务分配，而不考虑不确定工作环境中的混合任务。此外，缺乏相关研究的数据集和基准。在本文中，我们通过解决这些问题，为不确定的自主清洁系统的多机器人混合任务分配做出了贡献。首先，我们通过鲁棒优化模型来建模清洁环境中的不确定性，并提出了一个新的鲁棒混合整数线性规划模型，其中包括混合清洁任务顺序和机器人的能力等实际约束。其次，我们建立了一个数据集，包括100个实例，每个实例都有2D手动标记的图像和3D模型。第三，我们提供了关于所提出模型的全面结果。

    Task allocation plays a vital role in multi-robot autonomous cleaning systems, where multiple robots work together to clean a large area. However, there are several problems in relevant research to date. Most current studies mainly focus on deterministic, single-task allocation for cleaning robots, without considering hybrid tasks in uncertain working environments. Moreover, there is a lack of datasets and benchmarks for relevant research. In this paper, we contribute to multi-robot hybrid-task allocation for uncertain autonomous cleaning systems by addressing these problems. First, we model the uncertainties in the cleaning environment via robust optimization and propose a novel robust mixed-integer linear programming model with practical constraints including hybrid cleaning task order and robot's ability. Second, we establish a dataset of 100 instances made from floor plans, each of which has 2D manually-labeled images and a 3D model. Third, we provide comprehensive results on the c
    
[^2]: 在联邦深度学习中优化批标准化

    Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])

    [http://arxiv.org/abs/2303.06530](http://arxiv.org/abs/2303.06530)

    本文研究了在联邦学习中使用批标准化和群组归一化的效果，发现在适当的处理下，批标准化可以在广泛的联邦学习设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。

    This paper studies the use of batch normalization and group normalization in federated learning, and finds that with proper treatments, batch normalization can be highly competitive across a wide range of federated learning settings, and this requires no additional training or communication costs.

    批标准化（BN）通常用于现代深度神经网络（DNN）中，以提高稳定性并加速集中式训练的收敛速度。在具有非IID分散数据的联邦学习（FL）中，先前的研究观察到使用BN进行训练可能会由于训练和测试之间的BN统计不匹配而阻碍性能。因此，群组归一化（GN）更常用于FL作为BN的替代方法。然而，通过我们在各种FL设置下的实证研究，我们发现BN和GN之间没有一致的优胜者。这促使我们重新审视FL中归一化层的使用。我们发现，在适当的处理下，BN可以在广泛的FL设置中具有很高的竞争力，而且这不需要额外的训练或通信成本。我们希望我们的研究可以成为FL未来实际使用和理论分析的有价值参考。

    Batch Normalization (BN) is commonly used in modern deep neural networks (DNNs) to improve stability and speed up convergence during centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. However, from our empirical study across various FL settings, we see no consistent winner between BN and GN. This leads us to revisit the use of normalization layers in FL. We find that with proper treatments, BN can be highly competitive across a wide range of FL settings, and this requires no additional training or communication costs. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL.
    
[^3]: 打开神经网络分类器以计算Shap分数

    Opening Up the Neural Network Classifier for Shap Score Computation. (arXiv:2303.06516v1 [cs.AI])

    [http://arxiv.org/abs/2303.06516](http://arxiv.org/abs/2303.06516)

    本文提出了一种高效计算机器学习模型分类中Shap解释分数的方法，通过将二进制神经网络转换为布尔电路，并使用知识编译技术，将电路视为开放式模型，通过最近的高效算法计算Shap分数，相比于将BNN视为黑盒模型直接计算Shap，性能有了显著的提高。

    This paper proposes an efficient method for computing Shap explanation scores in machine learning model classification by transforming binary neural networks into Boolean circuits and treating the resulting circuit as an open-box model, which leads to a significant improvement in performance compared to computing Shap directly on the BNN treated as a black-box model.

    我们解决了使用机器学习模型进行分类的Shap解释分数的高效计算问题。为此，我们展示了将二进制神经网络（BNN）转换为确定性和可分解的布尔电路，使用知识编译技术。所得到的电路被视为开放式模型，通过最近的高效算法计算Shap分数。详细的实验表明，与将BNN视为黑盒模型直接计算Shap相比，性能有了显著的提高。

    We address the problem of efficiently computing Shap explanation scores for classifications with machine learning models. With this goal, we show the transformation of binary neural networks (BNNs) for classification into deterministic and decomposable Boolean circuits, for which knowledge compilation techniques are used. The resulting circuit is treated as an open-box model, to compute Shap scores by means of a recent efficient algorithm for this class of circuits. Detailed experiments show a considerable gain in performance in comparison with computing Shap directly on the BNN treated as a black-box model.
    
[^4]: 使用增强型随机森林分类器检测不平衡数据中的信用卡欺诈

    Credit Card Fraud Detection Using Enhanced Random Forest Classifier for Imbalanced Data. (arXiv:2303.06514v1 [cs.AI])

    [http://arxiv.org/abs/2303.06514](http://arxiv.org/abs/2303.06514)

    本文使用增强型随机森林分类器和合成少数类过采样技术来解决信用卡欺诈检测中的不平衡数据问题，获得了98%的准确度和F1分数值，具有实际应用价值。

    This paper proposes an enhanced random forest classifier and synthetic minority over-sampling technique to address the issue of imbalanced data in credit card fraud detection, achieving an accuracy of 98% and F1-score value of about 98%, with potential practical applications.

    信用卡已成为在线和离线交易中最流行的支付方式。随着技术的发展和欺诈案件的增加，创建欺诈检测算法以精确识别和停止欺诈活动的必要性也随之产生。本文实现了随机森林（RF）算法来解决这个问题。本研究使用了一组信用卡交易数据集。在处理信用卡欺诈检测时的主要问题是不平衡的数据集，其中大部分交易都是非欺诈交易。为了克服不平衡数据集的问题，使用了合成少数类过采样技术（SMOTE）。实现超参数技术以增强随机森林分类器的性能。结果表明，RF分类器获得了98％的准确度和约98％的F1分数值，这是令人兴奋的。我们还相信，我们的模型相对容易应用，并且可以克服信用卡欺诈检测中不平衡数据的挑战。

    The credit card has become the most popular payment method for both online and offline transactions. The necessity to create a fraud detection algorithm to precisely identify and stop fraudulent activity arises as a result of both the development of technology and the rise in fraud cases. This paper implements the random forest (RF) algorithm to solve the issue in the hand. A dataset of credit card transactions was used in this study. The main problem when dealing with credit card fraud detection is the imbalanced dataset in which most of the transaction are non-fraud ones. To overcome the problem of the imbalanced dataset, the synthetic minority over-sampling technique (SMOTE) was used. Implementing the hyperparameters technique to enhance the performance of the random forest classifier. The results showed that the RF classifier gained an accuracy of 98% and about 98% of F1-score value, which is promising. We also believe that our model is relatively easy to apply and can overcome the
    
[^5]: 通过数据转换技术准确预测全球平均温度

    Accurate Prediction of Global Mean Temperature through Data Transformation Techniques. (arXiv:2303.06468v1 [stat.AP])

    [http://arxiv.org/abs/2303.06468](http://arxiv.org/abs/2303.06468)

    本文研究了使用统计和简单的机器学习方法预测全球平均温度的优势，发现数据转换技术可以提高预测准确性。

    This paper examines the advantage of using statistical and simpler machine learning methods to predict global mean temperature, and finds that data transformation techniques can improve prediction accuracy.

    预测全球平均温度（GMT）在未来几十年内的变化趋势非常重要。预测历史数据是实现长期预测目标的必要第一步。本文研究了使用统计和简单的机器学习（ML）方法的优势，而不是直接使用复杂的ML算法和深度学习神经网络（DNN）。在应用不同算法之前，经常被忽视的数据转换方法被用作提高预测准确性的手段。GMT时间序列既被视为单变量时间序列，也被视为回归问题。发现一些数据转换步骤是有效的。各种简单的ML方法表现得和更为知名的方法一样好甚至更好，这表明尝试大量算法作为第一步是有价值的。56种算法经过Box-Cox、Yeo-Johnson和一阶差分处理后进行比较，与没有进行处理的情况进行比较。预测结果表明，数据转换技术可以提高预测准确性。

    It is important to predict how the Global Mean Temperature (GMT) will evolve in the next few decades. The ability to predict historical data is a necessary first step toward the actual goal of making long-range forecasts. This paper examines the advantage of statistical and simpler Machine Learning (ML) methods instead of directly using complex ML algorithms and Deep Learning Neural Networks (DNN). Often neglected data transformation methods prior to applying different algorithms have been used as a means of improving predictive accuracy. The GMT time series is treated both as a univariate time series and also cast as a regression problem. Some steps of data transformations were found to be effective. Various simple ML methods did as well or better than the more well-known ones showing merit in trying a large bouquet of algorithms as a first step. Fifty-six algorithms were subject to Box-Cox, Yeo-Johnson, and first-order differencing and compared with the absence of them. Predictions f
    
[^6]: ZeroNLG: 将领域对齐和自编码用于零样本多模态和多语言自然语言生成

    ZeroNLG: Aligning and Autoencoding Domains for Zero-Shot Multimodal and Multilingual Natural Language Generation. (arXiv:2303.06458v1 [cs.CL])

    [http://arxiv.org/abs/2303.06458](http://arxiv.org/abs/2303.06458)

    ZeroNLG是一个零样本学习框架，可以处理多个NLG任务，包括图像到文本、视频到文本和文本到文本，跨越英语、中文、德语和法语。它不需要任何标记的下游对进行训练，通过将不同的领域投影到共享的公共潜在空间中的相应坐标，桥接不同领域之间的差异。

    ZeroNLG is a zero-shot learning framework that can handle multiple NLG tasks, including image-to-text, video-to-text, and text-to-text, across English, Chinese, German, and French. It does not require any labeled downstream pairs for training, and bridges the differences between different domains by projecting them to corresponding coordinates in a shared common latent space.

    自然语言生成（NLG）接受以图像、视频或文本形式的输入数据，并生成相应的自然语言文本作为输出。现有的NLG方法主要采用监督方法，并且严重依赖于耦合的数据到文本对。然而，对于许多有针对性的场景和非英语语言，往往没有足够数量的标记数据。为了放松对下游任务标记数据的依赖性，我们提出了一个直观有效的零样本学习框架ZeroNLG，它可以处理多个NLG任务，包括图像到文本（图像字幕）、视频到文本（视频字幕）和文本到文本（神经机器翻译），跨越英语、中文、德语和法语在一个统一的框架内。ZeroNLG不需要任何标记的下游对进行训练。在训练期间，ZeroNLG（i）将不同的领域（跨模态和语言）投影到共享的公共潜在空间中的相应坐标；（ii）桥接差异

    Natural Language Generation (NLG) accepts input data in the form of images, videos, or text and generates corresponding natural language text as output. Existing NLG methods mainly adopt a supervised approach and rely heavily on coupled data-to-text pairs. However, for many targeted scenarios and for non-English languages, sufficient quantities of labeled data are often not available. To relax the dependency on labeled data of downstream tasks, we propose an intuitive and effective zero-shot learning framework, ZeroNLG, which can deal with multiple NLG tasks, including image-to-text (image captioning), video-to-text (video captioning), and text-to-text (neural machine translation), across English, Chinese, German, and French within a unified framework. ZeroNLG does not require any labeled downstream pairs for training. During training, ZeroNLG (i) projects different domains (across modalities and languages) to corresponding coordinates in a shared common latent space; (ii) bridges diff
    
[^7]: 基于图神经网络的上下文嵌入在表格数据深度学习中的应用

    Graph Neural Network contextual embedding for Deep Learning on Tabular Data. (arXiv:2303.06455v1 [cs.LG])

    [http://arxiv.org/abs/2303.06455](http://arxiv.org/abs/2303.06455)

    本文介绍了一种基于图神经网络的深度学习模型，使用交互网络进行上下文嵌入，用于表格数据的处理。该模型在公共数据集上的表现优于最近的深度学习基准调查，并且与提升树解决方案相比也取得了竞争性的结果。

    This paper introduces a novel deep learning model based on Graph Neural Network (GNN) with Interaction Network (IN) for contextual embedding, which outperforms the recent DL benchmark on five public datasets and achieves competitive results compared to boosted-tree solutions in tabular data processing.

    所有行业都试图利用现有的大数据进行基于人工智能的应用，这些数据通常以所谓的表格形式存在，其中每个记录由许多异构的连续和分类列组成，也称为特征。深度学习在自然语言处理等与人类技能相关的领域中已经取得了重大突破，但其在表格数据上的应用更具挑战性。更经典的机器学习模型，如基于树的集成模型通常表现更好。本文介绍了一种新颖的深度学习模型，它使用图神经网络（GNN），更具体地说是交互网络（IN），进行上下文嵌入。其结果优于最近发布的基于五个公共数据集的深度学习基准调查，与提升树解决方案相比也取得了竞争性的结果。

    All industries are trying to leverage Artificial Intelligence (AI) based on their existing big data which is available in so called tabular form, where each record is composed of a number of heterogeneous continuous and categorical columns also known as features. Deep Learning (DL) has consituted a major breathrough for AI in fields related to human skills like natural language processing, but its applicability to tabular data has been more challenging. More classical Machine Learning (ML) models like tree-based ensemble ones usually perform better. In this manuscript a novel DL model that uses Graph Neural Network (GNN), more specifically Interaction Network (IN), for contextual embedding is introduced. Its results outperform those of the recently published survey with DL benchmark based on five public datasets, achieving also competitive results when compared to boosted-tree solutions.
    
[^8]: 人工智能与人类文本协作任务中交互设计空间的映射

    Mapping the Design Space of Interactions in Human-AI Text Co-creation Tasks. (arXiv:2303.06430v1 [cs.AI])

    [http://arxiv.org/abs/2303.06430](http://arxiv.org/abs/2303.06430)

    本文提出了一系列内容生成任务及其相应的人工智能与人类交互模式，鼓励研究社区专注于更复杂和相互依赖的任务，这些任务需要更高水平的人类参与。

    This paper presents a spectrum of content generation tasks and their corresponding human-AI interaction patterns, encouraging the research community to focus on more complex and interdependent tasks that require greater levels of human involvement.

    大型语言模型（LLMs）展示了令人印象深刻的文本生成能力，促使我们重新考虑人工智能与人类协作的未来以及人类如何与LLMs交互。在本文中，我们提出了一系列内容生成任务及其相应的人工智能与人类交互模式。这些任务包括：1）具有最小人工智能与人类交互的固定范围内容策划任务，2）具有精确人工智能与人类交互的独立创意任务，以及3）具有迭代人工智能与人类交互的复杂且相互依赖的创意任务。我们鼓励生成AI和HCI研究社区专注于更复杂和相互依赖的任务，这些任务需要更高水平的人类参与。

    Large Language Models (LLMs) have demonstrated impressive text generation capabilities, prompting us to reconsider the future of human-AI co-creation and how humans interact with LLMs. In this paper, we present a spectrum of content generation tasks and their corresponding human-AI interaction patterns. These tasks include: 1) fixed-scope content curation tasks with minimal human-AI interactions, 2) independent creative tasks with precise human-AI interactions, and 3) complex and interdependent creative tasks with iterative human-AI interactions. We encourage the generative AI and HCI research communities to focus on the more complex and interdependent tasks, which require greater levels of human involvement.
    
[^9]: 基于深度强化学习的MIMO-NOMA IoT系统功率分配，以最小化AoI和能耗

    Deep Reinforcement Learning Based Power Allocation for Minimizing AoI and Energy Consumption in MIMO-NOMA IoT Systems. (arXiv:2303.06411v1 [cs.IT])

    [http://arxiv.org/abs/2303.06411](http://arxiv.org/abs/2303.06411)

    本文提出了一种基于深度强化学习的MIMO-NOMA IoT系统功率分配方法，以最小化AoI和能耗。

    This paper proposes a deep reinforcement learning based power allocation method for MIMO-NOMA IoT systems to minimize AoI and energy consumption.

    多输入多输出和非正交多址（MIMO-NOMA）物联网（IoT）系统可以显著提高信道容量和频谱效率，以支持实时应用。时延（AoI）是实时应用的重要指标，但没有文献最小化MIMO-NOMA IoT系统的AoI，这促使我们进行这项工作。在MIMO-NOMA IoT系统中，基站（BS）确定样本收集要求并为每个IoT设备分配传输功率。每个设备根据样本收集要求确定是否采样数据，并采用分配的功率将采样的数据通过MIMO-NOMA信道传输到BS。然后，BS采用连续干扰消除（SIC）技术解码每个设备传输的数据信号。样本收集要求和功率分配将影响系统的AoI和能耗。这是至关重要的。

    Multi-input multi-out and non-orthogonal multiple access (MIMO-NOMA) internet-of-things (IoT) systems can improve channel capacity and spectrum efficiency distinctly to support the real-time applications. Age of information (AoI) is an important metric for real-time application, but there is no literature have minimized AoI of the MIMO-NOMA IoT system, which motivates us to conduct this work. In MIMO-NOMA IoT system, the base station (BS) determines the sample collection requirements and allocates the transmission power for each IoT device. Each device determines whether to sample data according to the sample collection requirements and adopts the allocated power to transmit the sampled data to the BS over MIMO-NOMA channel. Afterwards, the BS employs successive interference cancelation (SIC) technique to decode the signal of the data transmitted by each device. The sample collection requirements and power allocation would affect AoI and energy consumption of the system. It is critical
    
[^10]: Brain Diffuser：一种端到端的脑图像到脑网络管道

    Brain Diffuser: An End-to-End Brain Image to Brain Network Pipeline. (arXiv:2303.06410v1 [cs.AI])

    [http://arxiv.org/abs/2303.06410](http://arxiv.org/abs/2303.06410)

    本文提出了一种基于扩散的端到端脑网络生成模型Brain Diffuser，直接从DTI中形成结构性脑网络。对于阿尔茨海默病的情况，所提出的模型在ADNI数据库上的表现优于现有工具包的结果。

    This paper proposes a diffusion based end-to-end brain network generative model Brain Diffuser that directly shapes the structural brain networks from DTI. For the case of Alzheimer's disease, the proposed model performs better than the results from existing toolkits on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.

    脑网络分析对于诊断和干预阿尔茨海默病（AD）至关重要。然而，以往的研究主要依赖于特定的耗时和主观的工具包。只有少数工具可以从脑扩散张量图像（DTI）中获取结构性脑网络。在本文中，我们提出了一种基于扩散的端到端脑网络生成模型Brain Diffuser，直接从DTI中形成结构性脑网络。与现有工具包相比，Brain Diffuser通过分析受试者之间结构性脑网络的差异，利用更多的结构连接特征和与疾病相关的信息。对于阿尔茨海默病的情况，所提出的模型在阿尔茨海默病神经影像学倡议（ADNI）数据库上的表现优于现有工具包的结果。

    Brain network analysis is essential for diagnosing and intervention for Alzheimer's disease (AD). However, previous research relied primarily on specific time-consuming and subjective toolkits. Only few tools can obtain the structural brain networks from brain diffusion tensor images (DTI). In this paper, we propose a diffusion based end-to-end brain network generative model Brain Diffuser that directly shapes the structural brain networks from DTI. Compared to existing toolkits, Brain Diffuser exploits more structural connectivity features and disease-related information by analyzing disparities in structural brain networks across subjects. For the case of Alzheimer's disease, the proposed model performs better than the results from existing toolkits on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.
    
[^11]: 自动检测辅助犬预测人类癫痫发作时的信号行为

    Automatic Detection of Signalling Behaviour from Assistance Dogs as they Forecast the Onset of Epileptic Seizures in Humans. (arXiv:2303.06407v1 [cs.LG])

    [http://arxiv.org/abs/2303.06407](http://arxiv.org/abs/2303.06407)

    本研究探讨了如何自动检测辅助犬预测人类癫痫发作时的信号行为，以提高癫痫患者的生活质量。

    This study explores how to automatically detect signalling behaviour from assistance dogs as they forecast the onset of epileptic seizures in humans, to improve the quality of life for epilepsy patients.

    癫痫是世界上最常见的神经系统疾病之一，影响着数百万人。癫痫发作通常是由于人脑中的非协调电放电引起的，可能会造成伤害，包括倒地和失去意识。如果能够预测癫痫发作的开始，那么可以将受试者置于安全的环境或位置，以最小化由于倒地而导致的自我伤害。然而，在日常的不受控制的环境中，没有明确的方法来预测癫痫发作。先前的研究表明，宠物狗有能力通过嗅探受试者在癫痫发作前皮肤散发的特征挥发性有机化合物来检测癫痫发作的开始，有些辅助犬经过训练，可以向其主人/训练员发出信号。在这项工作中，我们确定了如何自动检测信号行为。

    Epilepsy or the occurrence of epileptic seizures, is one of the world's most well-known neurological disorders affecting millions of people. Seizures mostly occur due to non-coordinated electrical discharges in the human brain and may cause damage, including collapse and loss of consciousness. If the onset of a seizure can be forecast then the subject can be placed into a safe environment or position so that self-injury as a result of a collapse can be minimised. However there are no definitive methods to predict seizures in an everyday, uncontrolled environment. Previous studies have shown that pet dogs have the ability to detect the onset of an epileptic seizure by scenting the characteristic volatile organic compounds exuded through the skin by a subject prior a seizure occurring and there are cases where assistance dogs, trained to scent the onset of a seizure, can signal this to their owner/trainer. In this work we identify how we can automatically detect the signalling behaviours
    
[^12]: 一种结合移动前沿、数据分解和深度学习的新方法用于预测复杂时间序列

    A Novel Method Combines Moving Fronts, Data Decomposition and Deep Learning to Forecast Intricate Time Series. (arXiv:2303.06394v1 [cs.LG])

    [http://arxiv.org/abs/2303.06394](http://arxiv.org/abs/2303.06394)

    本文提出了一种新的方法，结合移动前沿、数据分解和深度学习，用于预测复杂时间序列。该方法通过经验小波变换将时间序列分解成更简单的组成序列，并使用移动前沿方法防止数据泄漏。

    This paper proposes a novel method that combines moving fronts, data decomposition, and deep learning to forecast intricate time series. The method decomposes the time series into simpler constituent series using empirical wavelet transform and prevents data leakage using the moving front method.

    高变异性的单变量时间序列甚至对深度神经网络（DNN）也构成挑战。为了克服这一问题，单变量时间序列被分解成更简单的组成序列，它们的总和等于原始序列。本文演示了传统的一次分解技术存在数据泄漏的问题。因此，提出了一种新的移动前沿（MF）方法来防止数据泄漏，使分解后的序列可以像其他时间序列一样处理。印度夏季季风降雨（ISMR）是一个非常复杂的时间序列，对DNN构成挑战，因此被选为示例。从众多可用的信号处理工具中，经验小波变换（EWT）被选择用于将ISMR分解成更简单的组成序列，因为它被发现比其他流行的算法，如自适应噪声完全集合经验模态分解（CEEMDAN）更有效。

    A univariate time series with high variability can pose a challenge even to Deep Neural Network (DNN). To overcome this, a univariate time series is decomposed into simpler constituent series, whose sum equals the original series. As demonstrated in this article, the conventional one-time decomposition technique suffers from a leak of information from the future, referred to as a data leak. In this work, a novel Moving Front (MF) method is proposed to prevent data leakage, so that the decomposed series can be treated like other time series. Indian Summer Monsoon Rainfall (ISMR) is a very complex time series, which poses a challenge to DNN and is therefore selected as an example. From the many signal processing tools available, Empirical Wavelet Transform (EWT) was chosen for decomposing the ISMR into simpler constituent series, as it was found to be more effective than the other popular algorithm, Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN). The propose
    
[^13]: 不确定性感知的离线学习

    Uncertainty-Aware Off-Policy Learning. (arXiv:2303.06389v1 [cs.LG])

    [http://arxiv.org/abs/2303.06389](http://arxiv.org/abs/2303.06389)

    本文提出了一种不确定性感知的倒数概率分数估计器（UIPS），用于改进离线学习，通过明确模拟估计的记录策略中的不确定性，相对于广泛的最先进基线具有优越的样本效率。

    This paper proposes an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning, which explicitly models the uncertainty in the estimated logging policy and demonstrates advantageous sample efficiency against an extensive list of state-of-the-art baselines on synthetic and three real-world recommendation datasets.

    离线学习是指仅通过记录的反馈数据进行策略优化的过程，在各种实际应用中显示出重要性，例如搜索引擎、推荐系统等。虽然生成记录数据的真实记录策略通常是未知的，但以前的工作仅在离线学习中采用其估计值，忽略了由于这种估计器导致的高偏差和高方差，特别是在具有小且估计不准确的记录概率的样本上。在这项工作中，我们明确地模拟了估计的记录策略中的不确定性，并提出了一种不确定性感知的倒数概率分数估计器（UIPS）来改进离线学习。在合成和三个真实的推荐数据集上的实验结果表明，所提出的UIPS估计器相对于广泛的最先进基线具有优越的样本效率。

    Off-policy learning, referring to the procedure of policy optimization with access only to logged feedback data, has shown importance in various real-world applications, such as search engines, recommender systems, and etc. While the ground-truth logging policy, which generates the logged data, is usually unknown, previous work simply takes its estimated value in off-policy learning, ignoring both high bias and high variance resulted from such an estimator, especially on samples with small and inaccurately estimated logging probabilities. In this work, we explicitly model the uncertainty in the estimated logging policy and propose a Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning. Experiment results on synthetic and three real-world recommendation datasets demonstrate the advantageous sample efficiency of the proposed UIPS estimator against an extensive list of state-of-the-art baselines.
    
[^14]: 利用人工神经网络探究场景感知的神经表征在海马依赖任务中的应用

    Probing neural representations of scene perception in a hippocampally dependent task using artificial neural networks. (arXiv:2303.06367v1 [cs.CV])

    [http://arxiv.org/abs/2303.06367](http://arxiv.org/abs/2303.06367)

    本研究利用人工神经网络探究场景感知的神经表征在海马依赖任务中的应用，设计了一个新型场景感知基准，证明了DNNs可以学习从不同自我中心视角观察的场景的能力。

    This study uses artificial neural networks to explore neural representations of scene perception in a hippocampally dependent task, and demonstrates that DNNs can learn the ability to transform scenes viewed from different egocentric perspectives, using a novel scene perception benchmark.

    通过反向传播训练的深度人工神经网络(DNNs)提供了哺乳动物视觉系统的有效模型，准确地捕捉了从初级视觉皮层到下颞皮质(IT)的神经响应层次结构。然而，这些网络解释更高皮层区域的表征能力相对较弱，研究也相对较少。我们描述了一个受海马依赖任务启发的新型场景感知基准，旨在探究DNNs转换从不同自我中心视角观察的场景的能力。使用受颞叶结构和海马之间连接启发的网络架构，我们证明了使用三元组损失训练的DNNs可以学习这个任务。此外，通过强制执行分解的潜在空间

    Deep artificial neural networks (DNNs) trained through backpropagation provide effective models of the mammalian visual system, accurately capturing the hierarchy of neural responses through primary visual cortex to inferior temporal cortex (IT). However, the ability of these networks to explain representations in higher cortical areas is relatively lacking and considerably less well researched. For example, DNNs have been less successful as a model of the egocentric to allocentric transformation embodied by circuits in retrosplenial and posterior parietal cortex. We describe a novel scene perception benchmark inspired by a hippocampal dependent task, designed to probe the ability of DNNs to transform scenes viewed from different egocentric perspectives. Using a network architecture inspired by the connectivity between temporal lobe structures and the hippocampus, we demonstrate that DNNs trained using a triplet loss can learn this task. Moreover, by enforcing a factorized latent space
    
[^15]: 通过虚拟检查层实现时间序列的可解释性人工智能

    Explainable AI for Time Series via Virtual Inspection Layers. (arXiv:2303.06365v1 [cs.LG])

    [http://arxiv.org/abs/2303.06365](http://arxiv.org/abs/2303.06365)

    本文提出了一种虚拟检查层，将时间序列转换为可解释的表示，并允许通过本地XAI方法将相关性归因传播到该表示。我们将一系列XAI方法的适用性扩展到需要转换后才能解释输入的领域。我们展示了DFT-LRP在各种时间序列分类设置中的有用性，如音频和电子健康记录。

    This paper proposes a virtual inspection layer that transforms time series into an interpretable representation and allows for relevance attributions to be propagated to this representation via local XAI methods. The applicability of a family of XAI methods is extended to domains where the input is only interpretable after a transformation. The usefulness of DFT-LRP is demonstrated in various time series classification settings, such as audio and electronic health records.

    最近几年，可解释人工智能（XAI）领域取得了很大进展，但主要是在计算机视觉和自然语言处理方面。对于时间序列，由于输入通常不可解释，因此只有有限的XAI研究可用。在这项工作中，我们提出了一种虚拟检查层，将时间序列转换为可解释的表示，并允许通过本地XAI方法（如逐层相关传播（LRP））将相关性归因传播到该表示。通过这种方式，我们将一系列XAI方法的适用性扩展到需要转换后才能解释输入的领域（例如语音）。在这里，我们专注于傅里叶变换，这在时间序列解释和LRP中被广泛应用，并将我们的方法称为DFT-LRP。我们展示了DFT-LRP在各种时间序列分类设置中的有用性，如音频和电子健康记录。我们展示了如何使用DFT-LRP来可视化和解释模型的决策。

    The field of eXplainable Artificial Intelligence (XAI) has greatly advanced in recent years, but progress has mainly been made in computer vision and natural language processing. For time series, where the input is often not interpretable, only limited research on XAI is available. In this work, we put forward a virtual inspection layer, that transforms the time series to an interpretable representation and allows to propagate relevance attributions to this representation via local XAI methods like layer-wise relevance propagation (LRP). In this way, we extend the applicability of a family of XAI methods to domains (e.g. speech) where the input is only interpretable after a transformation. Here, we focus on the Fourier transformation which is prominently applied in the interpretation of time series and LRP and refer to our method as DFT-LRP. We demonstrate the usefulness of DFT-LRP in various time series classification settings like audio and electronic health records. We showcase how 
    
[^16]: 面向非静态环境的隐私保护合作可见光定位：联邦学习视角

    Privacy-Preserving Cooperative Visible Light Positioning for Nonstationary Environment: A Federated Learning Perspective. (arXiv:2303.06361v1 [eess.SP])

    [http://arxiv.org/abs/2303.06361](http://arxiv.org/abs/2303.06361)

    本文提出了一种基于联邦学习的合作可见光定位方案，通过共同训练适应环境变化的全局模型，提高了在非静态环境下的定位精度和泛化能力。

    This paper proposes a cooperative visible light positioning scheme based on federated learning, which improves the positioning accuracy and generalization capability in nonstationary environments by jointly training a global model adaptive to environmental changes without sharing private data of users.

    可见光定位（VLP）作为一种有前途的室内定位技术，已经引起了足够的关注。然而，在非静态环境下，由于高度时变的信道，VLP的性能受到限制。为了提高非静态环境下的定位精度和泛化能力，本文提出了一种基于联邦学习（FL）的合作VLP方案。利用FL框架，用户可以共同训练适应环境变化的全局模型，而不共享用户的私有数据。此外，提出了一种合作可见光定位网络（CVPosNet），以加速收敛速度和提高定位精度。仿真结果表明，所提出的方案在非静态环境下优于基准方案。

    Visible light positioning (VLP) has drawn plenty of attention as a promising indoor positioning technique. However, in nonstationary environments, the performance of VLP is limited because of the highly time-varying channels. To improve the positioning accuracy and generalization capability in nonstationary environments, a cooperative VLP scheme based on federated learning (FL) is proposed in this paper. Exploiting the FL framework, a global model adaptive to environmental changes can be jointly trained by users without sharing private data of users. Moreover, a Cooperative Visible-light Positioning Network (CVPosNet) is proposed to accelerate the convergence rate and improve the positioning accuracy. Simulation results show that the proposed scheme outperforms the benchmark schemes, especially in nonstationary environments.
    
[^17]: FedLP: 一种用于通信计算高效的联邦学习的层次剪枝机制

    FedLP: Layer-wise Pruning Mechanism for Communication-Computation Efficient Federated Learning. (arXiv:2303.06360v1 [cs.LG])

    [http://arxiv.org/abs/2303.06360](http://arxiv.org/abs/2303.06360)

    本文提出了一种显式的FL剪枝框架FedLP，采用局部训练和联邦更新中的层次剪枝，对不同类型的深度学习模型具有普适性，可以缓解通信和计算的系统瓶颈，并且性能下降较小。

    This paper proposes an explicit FL pruning framework, FedLP, which adopts layer-wise pruning in local training and federated updating, and is model-agnostic and universal for different types of deep learning models. FedLP can relieve the system bottlenecks of communication and computation with marginal performance decay.

    联邦学习（FL）已经成为一种高效且隐私保护的分布式学习方案。本文主要关注FL中计算和通信的优化，采用局部训练和联邦更新中的层次剪枝，提出了一个显式的FL剪枝框架FedLP（Federated Layer-wise Pruning），该框架对不同类型的深度学习模型具有普适性。为具有同质本地模型和异质本地模型的场景设计了两种特定的FedLP方案。通过理论和实验评估，证明了FedLP可以缓解通信和计算的系统瓶颈，并且性能下降较小。据我们所知，FedLP是第一个正式将层次剪枝引入FL的框架。在联邦学习范围内，可以基于FedLP进一步设计更多的变体和组合。

    Federated learning (FL) has prevailed as an efficient and privacy-preserved scheme for distributed learning. In this work, we mainly focus on the optimization of computation and communication in FL from a view of pruning. By adopting layer-wise pruning in local training and federated updating, we formulate an explicit FL pruning framework, FedLP (Federated Layer-wise Pruning), which is model-agnostic and universal for different types of deep learning models. Two specific schemes of FedLP are designed for scenarios with homogeneous local models and heterogeneous ones. Both theoretical and experimental evaluations are developed to verify that FedLP relieves the system bottlenecks of communication and computation with marginal performance decay. To the best of our knowledge, FedLP is the first framework that formally introduces the layer-wise pruning into FL. Within the scope of federated learning, more variants and combinations can be further designed based on FedLP.
    
[^18]: AutoMLP: 自动化MLP用于序列推荐

    AutoMLP: Automated MLP for Sequential Recommendations. (arXiv:2303.06337v1 [cs.IR])

    [http://arxiv.org/abs/2303.06337](http://arxiv.org/abs/2303.06337)

    AutoMLP是一种新颖的序列推荐系统，通过自动化和自适应搜索算法，更好地模拟用户的长期/短期兴趣，实现更好的推荐效果。

    AutoMLP is a novel sequential recommender system that models users' long/short-term interests through an automated and adaptive search algorithm, achieving better recommendation performance.

    序列推荐系统旨在根据用户的历史交互来预测他们下一个感兴趣的项目。然而，长期存在的问题是如何区分用户的长期/短期兴趣，这可能是异质的并对下一个推荐产生不同的贡献。现有方法通常通过穷举搜索或经验经验设置预定义的短期兴趣长度，这既高度低效又产生次优结果。最近的先进基于变压器的模型可以实现最先进的性能，尽管存在上述问题，但它们对输入序列的长度具有二次计算复杂度。为此，本文提出了一种新颖的序列推荐系统AutoMLP，旨在更好地模拟用户的长期/短期兴趣。此外，我们设计了一种自动化和自适应搜索算法，以通过端到端优化获得更好的短期兴趣长度。通过实验，我们证明了AutoMLP的有效性和效率。

    Sequential recommender systems aim to predict users' next interested item given their historical interactions. However, a long-standing issue is how to distinguish between users' long/short-term interests, which may be heterogeneous and contribute differently to the next recommendation. Existing approaches usually set pre-defined short-term interest length by exhaustive search or empirical experience, which is either highly inefficient or yields subpar results. The recent advanced transformer-based models can achieve state-of-the-art performances despite the aforementioned issue, but they have a quadratic computational complexity to the length of the input sequence. To this end, this paper proposes a novel sequential recommender system, AutoMLP, aiming for better modeling users' long/short-term interests from their historical interactions. In addition, we design an automated and adaptive search algorithm for preferable short-term interest length via end-to-end optimization. Through ext
    
[^19]: 降落伞：评估交互式人机共同撰写系统

    Parachute: Evaluating Interactive Human-LM Co-writing Systems. (arXiv:2303.06333v1 [cs.HC])

    [http://arxiv.org/abs/2303.06333](http://arxiv.org/abs/2303.06333)

    本文提出了一个以人为中心的评估框架Parachute，用于交互式共同撰写系统的评估，该框架包含了分类的实用指标，可以用于评估和比较共同撰写系统。

    This paper proposes a human-centered evaluation framework, Parachute, for interactive co-writing systems, which includes categorized practical metrics and can be used to evaluate and compare co-writing systems.

    语言模型的飞速发展引起了人们对于利用语言模型构建共同撰写系统的极大兴趣，其中人类和语言模型交互地为共同的写作成果做出贡献。然而，缺乏对于交互式环境下共同撰写系统的评估研究。我们提出了一个以人为中心的评估框架Parachute，用于交互式共同撰写系统的评估。Parachute展示了交互评估的综合视角，其中每个评估方面都包含了分类的实用指标。此外，我们提供了一个使用案例来演示如何使用Parachute评估和比较共同撰写系统。

    A surge of advances in language models (LMs) has led to significant interest in using LMs to build co-writing systems, in which humans and LMs interactively contribute to a shared writing artifact. However, there is a lack of studies assessing co-writing systems in interactive settings. We propose a human-centered evaluation framework, Parachute, for interactive co-writing systems. Parachute showcases an integrative view of interaction evaluation, where each evaluation aspect consists of categorized practical metrics. Furthermore, we present Parachute with a use case to demonstrate how to evaluate and compare co-writing systems using Parachute.
    
[^20]: 一种新的张量专家混合并行方法来扩展混合专家训练

    A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training. (arXiv:2303.06318v1 [cs.LG])

    [http://arxiv.org/abs/2303.06318](http://arxiv.org/abs/2303.06318)

    本文提出了一种新的混合并行算法，结合了张量、专家和数据并行，以实现MoE模型的训练，其基本模型比当前最先进的DeepSpeed-MoE大4-8倍。

    This paper proposes a novel hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE.

    最近提出了一种名为Mixture-of-Experts（MoE）的新型神经网络架构，通过添加稀疏激活的专家块来增加神经网络（基本模型）的参数，而不改变训练或推理的总浮点操作数。理论上，这种架构允许我们训练任意大的模型，同时保持计算成本与基本模型相同。然而，在64到128个专家块之外，先前的工作观察到这些MoE模型的测试准确性递减。因此，训练高质量的MoE模型需要我们扩展基本模型的大小以及专家块的数量。在这项工作中，我们提出了一种新颖的三维混合并行算法，结合了张量、专家和数据并行，以实现MoE模型的训练，其基本模型比当前最先进的DeepSpeed-MoE大4-8倍。我们在优化器步骤中提出了内存优化。

    A new neural network architecture called Mixture-of-Experts (MoE) has been proposed recently that increases the parameters of a neural network (the base model) by adding sparsely activated expert blocks, without changing the total number of floating point operations for training or inference. In theory, this architecture allows us to train arbitrarily large models while keeping the computational costs same as that of the base model. However, beyond 64 to 128 experts blocks, prior work has observed diminishing returns in the test accuracies of these MoE models. Thus, training high quality MoE models requires us to scale the size of the base models, along with the number of expert blocks. In this work, we propose a novel, three-dimensional, hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE. We propose memory optimizations in the optimizer step, a
    
[^21]: 机器学习网络中的对抗攻击与防御：现代综述

    Adversarial Attacks and Defenses in Machine Learning-Powered Networks: A Contemporary Survey. (arXiv:2303.06302v1 [cs.LG])

    [http://arxiv.org/abs/2303.06302](http://arxiv.org/abs/2303.06302)

    本文综述了机器学习网络中的对抗攻击和防御技术，重点关注基于深度神经网络的分类模型。对最近的对抗攻击方法和最先进的对抗防御技术进行了全面分类，并以视觉上吸引人的表格和树状图的形式呈现。方法分类为反攻击检测和鲁棒性增强，特别关注于基于正则化的增强鲁棒性的方法。还探讨了新的攻击途径，包括基于搜索的攻击。

    This survey provides a comprehensive overview of recent advancements in adversarial attack and defense techniques in machine learning and deep neural networks, with a focus on deep neural network-based classification models. The methods are classified into counter-attack detection and robustness enhancement, with a specific focus on regularization-based methods for enhancing robustness. New avenues of attack are also explored, including search-based attacks.

    由于深度学习在互联网和相关场景中的快速增长应用，机器学习和深度神经网络中的对抗攻击和防御已经引起了极大的关注。本综述全面概述了对抗攻击和防御技术领域的最新进展，重点关注基于深度神经网络的分类模型。具体而言，我们根据攻击原理对最近的对抗攻击方法和最先进的对抗防御技术进行了全面分类，并以视觉上吸引人的表格和树状图的形式呈现。这是基于对现有工作的严格评估，包括对其优点和局限性的分析。我们还将方法分类为反攻击检测和鲁棒性增强，特别关注于基于正则化的增强鲁棒性的方法。还探讨了新的攻击途径，包括基于搜索的攻击。

    Adversarial attacks and defenses in machine learning and deep neural network have been gaining significant attention due to the rapidly growing applications of deep learning in the Internet and relevant scenarios. This survey provides a comprehensive overview of the recent advancements in the field of adversarial attack and defense techniques, with a focus on deep neural network-based classification models. Specifically, we conduct a comprehensive classification of recent adversarial attack methods and state-of-the-art adversarial defense techniques based on attack principles, and present them in visually appealing tables and tree diagrams. This is based on a rigorous evaluation of the existing works, including an analysis of their strengths and limitations. We also categorize the methods into counter-attack detection and robustness enhancement, with a specific focus on regularization-based methods for enhancing robustness. New avenues of attack are also explored, including search-base
    
[^22]: 防止注意力熵崩溃的Transformer训练稳定性研究

    Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG])

    [http://arxiv.org/abs/2303.06296](http://arxiv.org/abs/2303.06296)

    本文研究了Transformer的训练动态，发现低注意力熵伴随着高训练不稳定性，提出了一种简单而有效的解决方案$\sigma$Reparam，成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。

    This paper investigates the training dynamics of Transformers and proposes a simple and efficient solution, $\sigma$Reparam, to prevent entropy collapse in the attention layers, promoting more stable training.

    训练稳定性对于Transformer至关重要。本文通过研究注意力层的演变来探究Transformer的训练动态。特别地，我们在训练过程中跟踪每个注意力头的注意力熵，这是模型锐度的代理。我们发现，在不同的架构和任务中存在一种常见模式，即低注意力熵伴随着高训练不稳定性，这可能采取振荡损失或发散的形式。我们将病态低注意力熵，对应高度集中的注意力分数，称为$\textit{熵崩溃}$。作为一种解决方案，我们提出了$\sigma$Reparam，一种简单而有效的解决方案，其中我们使用谱归一化和额外的学习标量重新参数化所有线性层。我们证明了所提出的重新参数化成功地防止了注意力层中的熵崩溃，促进了更稳定的训练。此外，我们

    Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\textit{entropy collapse}$. As a remedy, we propose $\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that the proposed reparameterization successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we 
    
[^23]: ChatGPT的一致性分析

    Consistency Analysis of ChatGPT. (arXiv:2303.06273v1 [cs.CL])

    [http://arxiv.org/abs/2303.06273](http://arxiv.org/abs/2303.06273)

    本文研究了ChatGPT的一致性问题，发现尽管它具有更好的语言理解能力，但仍然经常无法生成逻辑上正确的预测。因此，在现实世界的应用需要进一步考虑，特别是在风险方面。

    This paper investigates the consistency issue of ChatGPT and finds that although it has improved language understanding ability, it frequently fails to generate logically correct predictions. Therefore, further consideration is needed for its real-world applications, especially in terms of risk.

    ChatGPT是一种基于大型语言模型的问答对话系统，自推出以来广受欢迎。虽然它在法律、医学和金融等领域的专业考试中取得了不错的成绩，但也有人对其可靠性和信任度表示怀疑。本文针对ChatGPT在逻辑一致性方面的可信度进行了调查研究。我们的研究发现，尽管ChatGPT似乎具有更好的语言理解能力，但它仍然经常无法生成逻辑上正确的预测。因此，虽然ChatGPT是一种令人印象深刻和有前途的新技术，但我们得出结论，如果没有经过彻底的人工检查，它在现实世界的应用需要进一步考虑，特别是在风险方面。

    ChatGPT, a question-and-answer dialogue system based on a large language model, has gained huge popularity since its introduction. Its positive aspects have been reported through many media platforms, and some analyses even showed that ChatGPT achieved a decent grade in professional exams, including the law, medical, and finance domains, adding extra support to the claim that AI now can assist and, even, replace humans in industrial fields. Others, however, doubt its reliability and trustworthiness. In this paper, we investigate ChatGPT's trustworthiness regarding logically consistent behaviours. Our findings suggest that, although ChatGPT seems to achieve an improved language understanding ability, it still fails to generate logically correct predictions frequently. Hence, while it is true that ChatGPT is an impressive and promising new technique, we conclude that its usage in real-world applications without thorough human inspection requires further consideration, especially for risk
    
[^24]: 可解释的异常值汇总

    Interpretable Outlier Summarization. (arXiv:2303.06261v1 [cs.LG])

    [http://arxiv.org/abs/2303.06261](http://arxiv.org/abs/2303.06261)

    STAIR提出了一种可解释的异常值汇总方法，通过学习一组紧凑的人类可理解规则，以汇总和解释异常检测结果，具有强大的可解释性，以准确地总结检测结果。

    STAIR proposes an interpretable outlier summarization method by learning a compact set of human understandable rules to summarize and explain the anomaly detection results, which has strong interpretability to accurately summarize the detection results.

    异常值检测在实际应用中是至关重要的，以防止金融欺诈、防御网络入侵或检测即将发生的设备故障。为了减少人力评估异常值检测结果的工作量，并有效地将异常值转化为可操作的见解，用户通常希望系统自动产生可解释的异常值检测结果的子组的汇总。然而，到目前为止，没有这样的系统存在。为了填补这一空白，我们提出了STAIR，它学习了一组紧凑的人类可理解规则，以汇总和解释异常检测结果。STAIR不使用经典的决策树算法来产生这些规则，而是提出了一个新的优化目标，以产生少量规则，具有最小的复杂性，因此具有强大的可解释性，以准确地总结检测结果。STAIR的学习算法通过迭代分割大规则来产生规则集，并在每个i中最大化这个目标，是最优的。

    Outlier detection is critical in real applications to prevent financial fraud, defend network intrusions, or detecting imminent device failures. To reduce the human effort in evaluating outlier detection results and effectively turn the outliers into actionable insights, the users often expect a system to automatically produce interpretable summarizations of subgroups of outlier detection results. Unfortunately, to date no such systems exist. To fill this gap, we propose STAIR which learns a compact set of human understandable rules to summarize and explain the anomaly detection results. Rather than use the classical decision tree algorithms to produce these rules, STAIR proposes a new optimization objective to produce a small number of rules with least complexity, hence strong interpretability, to accurately summarize the detection results. The learning algorithm of STAIR produces a rule set by iteratively splitting the large rules and is optimal in maximizing this objective in each i
    
[^25]: ICU环境噪声和光线信息预测谵妄风险

    Predicting risk of delirium from ambient noise and light information in the ICU. (arXiv:2303.06253v1 [cs.LG])

    [http://arxiv.org/abs/2303.06253](http://arxiv.org/abs/2303.06253)

    本研究利用环境噪声和光线信息，开发了第一个基于深度学习的ICU患者谵妄预测模型，为谵妄的预防和治疗提供了新思路。

    This study developed the first deep-learning based delirium prediction model for ICU patients using only ambient noise and light information, providing new insights for the prevention and treatment of delirium.

    现有的重症监护室（ICU）谵妄预测模型没有考虑环境因素，尽管有强有力的证据表明它们对谵妄有影响。本研究利用Thunderboard、ActiGraph传感器和iPod with AudioTools应用程序，仅使用环境噪声和光线信息，报道了第一个基于深度学习的ICU患者谵妄预测模型。这些测量数据从2021年5月至2022年9月收集自102名患者的ICU病房，分为白天（0700至1859）和夜间（1900至0659）。使用这些数据训练深度学习模型，预测ICU住院期间或出院后4天内的谵妄发生率。最后，分析结果得分以评估每个特征的重要性和方向性。白天的噪声水平显著高于夜间的噪声水平。当仅使用噪声特征或噪声和光特征的组合时，1-D卷积神经网络

    Existing Intensive Care Unit (ICU) delirium prediction models do not consider environmental factors despite strong evidence of their influence on delirium. This study reports the first deep-learning based delirium prediction model for ICU patients using only ambient noise and light information. Ambient light and noise intensities were measured from ICU rooms of 102 patients from May 2021 to September 2022 using Thunderboard, ActiGraph sensors and an iPod with AudioTools application. These measurements were divided into daytime (0700 to 1859) and nighttime (1900 to 0659). Deep learning models were trained using this data to predict the incidence of delirium during ICU stay or within 4 days of discharge. Finally, outcome scores were analyzed to evaluate the importance and directionality of every feature. Daytime noise levels were significantly higher than nighttime noise levels. When using only noise features or a combination of noise and light features 1-D convolutional neural networks 
    
[^26]: AI增强的重症监护室：通过普适感知革命性地改善患者护理

    AI-Enhanced Intensive Care Unit: Revolutionizing Patient Care with Pervasive Sensing. (arXiv:2303.06252v1 [cs.AI])

    [http://arxiv.org/abs/2303.06252](http://arxiv.org/abs/2303.06252)

    本文介绍了一种AI增强的重症监护室系统，通过普适感知和数据处理，可以改善患者的视觉监测和评估，提高护理质量。

    This paper introduces an AI-enhanced ICU system that improves patient visual monitoring and assessment, and ultimately enhances the quality of care, through pervasive sensing and data processing.

    重症监护室（ICU）是一个专门的医院空间，用于接受危重病人的密集护理和监测。全面的监测对于评估患者的病情，特别是病情的严重程度以及最终的护理质量至关重要。然而，由于时间限制和医护人员的工作量，ICU中的患者监测范围受到限制。目前，包括面部表情、姿势和活动能力等细节的视觉评估仅偶尔被捕捉到，或者根本没有被捕捉到。这些手动观察是主观的，容易出现文档错误，并给护理人员带来额外的工作量。人工智能（AI）系统由于其出色的学习能力，有潜力增强患者的视觉监测和评估。这样的系统需要强大的注释数据进行训练。为此，我们开发了普适感知和数据处理系统。

    The intensive care unit (ICU) is a specialized hospital space where critically ill patients receive intensive care and monitoring. Comprehensive monitoring is imperative in assessing patients conditions, in particular acuity, and ultimately the quality of care. However, the extent of patient monitoring in the ICU is limited due to time constraints and the workload on healthcare providers. Currently, visual assessments for acuity, including fine details such as facial expressions, posture, and mobility, are sporadically captured, or not captured at all. These manual observations are subjective to the individual, prone to documentation errors, and overburden care providers with the additional workload. Artificial Intelligence (AI) enabled systems has the potential to augment the patient visual monitoring and assessment due to their exceptional learning capabilities. Such systems require robust annotated data to train. To this end, we have developed pervasive sensing and data processing s
    
[^27]: 基于区域的联邦学习用于移动感知数据

    Zone-based Federated Learning for Mobile Sensing Data. (arXiv:2303.06246v1 [cs.LG])

    [http://arxiv.org/abs/2303.06246](http://arxiv.org/abs/2303.06246)

    本文提出了一种基于区域的联邦学习方法，用于训练移动感知数据的深度学习模型。该方法将物理空间划分为地理区域，并映射到移动边缘云系统架构，以实现良好的模型准确性和可扩展性。每个区域都有一个联合训练模型，能够很好地适应该区域用户的数据和行为，并保护用户数据隐私。

    This paper proposes a zone-based federated learning method for training deep learning models with mobile sensing data. The method divides the physical space into geographical zones and maps them to a mobile-edge-cloud system architecture for good model accuracy and scalability. Each zone has a federated training model that adapts well to the data and behaviors of users in that zone, while protecting user data privacy.

    移动应用程序，如mHealth和健康应用程序，可以从使用智能手机或可穿戴设备收集的移动感知数据训练的深度学习（DL）模型中受益。然而，目前没有移动感知DL系统能够同时实现良好的模型准确性，适应用户的移动行为，随着用户数量的增加而扩展，并保护用户数据隐私。我们提出了基于区域的联邦学习（ZoneFL）来解决这些要求。ZoneFL将物理空间划分为地理区域，映射到移动边缘云系统架构，以实现良好的模型准确性和可扩展性。每个区域都有一个联合训练模型，称为区域模型，它能够很好地适应该区域用户的数据和行为。受益于FL设计，ZoneFL培训期间保护用户数据隐私。我们提出了两种新颖的基于区域的联合训练算法来优化区域模型以适应用户的移动行为：区域合并和分裂（ZMS）和Zo

    Mobile apps, such as mHealth and wellness applications, can benefit from deep learning (DL) models trained with mobile sensing data collected by smart phones or wearable devices. However, currently there is no mobile sensing DL system that simultaneously achieves good model accuracy while adapting to user mobility behavior, scales well as the number of users increases, and protects user data privacy. We propose Zone-based Federated Learning (ZoneFL) to address these requirements. ZoneFL divides the physical space into geographical zones mapped to a mobile-edge-cloud system architecture for good model accuracy and scalability. Each zone has a federated training model, called a zone model, which adapts well to data and behaviors of users in that zone. Benefiting from the FL design, the user data privacy is protected during the ZoneFL training. We propose two novel zone-based federated training algorithms to optimize zone models to user mobility behavior: Zone Merge and Split (ZMS) and Zo
    
[^28]: 基于双视角的超似曲自适应学习用于自监督骨架动作表示

    HYperbolic Self-Paced Learning for Self-Supervised Skeleton-based Action Representations. (arXiv:2303.06242v1 [cs.CV])

    [http://arxiv.org/abs/2303.06242](http://arxiv.org/abs/2303.06242)

    本文提出了一种新的超似曲自适应模型（HYSP）用于学习基于骨架的动作表示，采用自监督学习，使用数据增强来生成同一样本的两个视图，并通过将一个视图与另一个视图匹配来学习，使用超似曲不确定性来确定算法学习速度，假设不确定性较小的样本应更强烈地推动训练，具有更大的权重和速度。

    This paper proposes a novel HYperbolic Self-Paced model (HYSP) for learning skeleton-based action representations, which adopts self-supervision and uses data augmentations to generate two views of the same sample, and learns by matching one to the other. It uses hyperbolic uncertainty to determine the algorithmic learning pace, assuming that less uncertain samples should be more strongly driving the training, with a larger weight and pace.

    自适应学习在一些任务中有益，例如弱监督学习和领域自适应，可以选择和排序训练样本序列，从易到难。然而，它在无监督学习中的适用性仍未被探索，其中任务的知识在训练期间成熟。我们提出了一种新的超似曲自适应模型（HYSP）用于学习基于骨架的动作表示。HYSP采用自监督：它使用数据增强来生成同一样本的两个视图，并通过将一个视图（称为在线）与另一个视图（目标）匹配来学习。我们建议使用超似曲不确定性来确定算法学习速度，假设不确定性较小的样本应更强烈地推动训练，具有更大的权重和速度。超似曲不确定性是采用的超似曲神经网络的副产品，它在训练期间成熟，与额外成本相比，没有额外成本。

    Self-paced learning has been beneficial for tasks where some initial knowledge is available, such as weakly supervised learning and domain adaptation, to select and order the training sample sequence, from easy to complex. However its applicability remains unexplored in unsupervised learning, whereby the knowledge of the task matures during training. We propose a novel HYperbolic Self-Paced model (HYSP) for learning skeleton-based action representations. HYSP adopts self-supervision: it uses data augmentations to generate two views of the same sample, and it learns by matching one (named online) to the other (the target). We propose to use hyperbolic uncertainty to determine the algorithmic learning pace, under the assumption that less uncertain samples should be more strongly driving the training, with a larger weight and pace. Hyperbolic uncertainty is a by-product of the adopted hyperbolic neural networks, it matures during training and it comes with no extra cost, compared to the e
    
[^29]: 对抗训练是否需要使用整个训练数据集？

    Do we need entire training data for adversarial training?. (arXiv:2303.06241v1 [cs.CV])

    [http://arxiv.org/abs/2303.06241](http://arxiv.org/abs/2303.06241)

    本文提出了一种新的对抗训练方法，通过对训练数据集进行筛选，仅使用易受对抗攻击的样本进行训练，从而减少训练时间。

    This paper proposes a new adversarial training method that reduces training time by selecting only the adversarially-prone samples from the training dataset.

    深度神经网络（DNN）被用于解决许多领域的问题，包括自动驾驶汽车和医学图像等安全关键领域。DNN对抗攻击的脆弱性已经被广泛关注。近年来，已经提出了许多方法来通过对抗训练来解决这个问题。几乎所有的方法都会为整个训练数据集生成对抗性示例，从而大大增加了训练时间。我们展示了通过仅使用训练数据的子集进行对抗训练，可以减少任何对抗训练算法的训练时间。为了选择子集，我们从训练数据中过滤出易受对抗攻击的样本。我们对所有训练样本执行简单的对抗攻击，以过滤出这个子集。在这个攻击中，我们向每个像素添加一个小扰动和几条网格线到输入图像中。我们对易受对抗攻击的子集进行对抗训练，并且...

    Deep Neural Networks (DNNs) are being used to solve a wide range of problems in many domains including safety-critical domains like self-driving cars and medical imagery. DNNs suffer from vulnerability against adversarial attacks. In the past few years, numerous approaches have been proposed to tackle this problem by training networks using adversarial training. Almost all the approaches generate adversarial examples for the entire training dataset, thus increasing the training time drastically. We show that we can decrease the training time for any adversarial training algorithm by using only a subset of training data for adversarial training. To select the subset, we filter the adversarially-prone samples from the training data. We perform a simple adversarial attack on all training examples to filter this subset. In this attack, we add a small perturbation to each pixel and a few grid lines to the input image.  We perform adversarial training on the adversarially-prone subset and mi
    
[^30]: 低开销模型剪枝：面向联邦学习的补充稀疏化

    Complement Sparsification: Low-Overhead Model Pruning for Federated Learning. (arXiv:2303.06237v1 [cs.LG])

    [http://arxiv.org/abs/2303.06237](http://arxiv.org/abs/2303.06237)

    本文提出了一种名为补充稀疏化的模型剪枝机制，通过在服务器和客户端之间进行互补和协作的剪枝来满足联邦学习中低双向通信开销、客户端低计算开销和良好模型准确性的要求。

    This paper proposes a model pruning mechanism called Complement Sparsification (CS), which satisfies the requirements of low bidirectional communication overhead between the server and the clients, low computation overhead at the clients, and good model accuracy in federated learning through complementary and collaborative pruning done at the server and the clients.

    联邦学习（FL）是一种隐私保护的分布式深度学习范例，涉及大量通信和计算工作，这对于资源受限的移动和物联网设备是一个问题。模型剪枝/稀疏化开发了可以解决此问题的稀疏模型，但现有的稀疏化解决方案不能同时满足服务器和客户端之间低双向通信开销、客户端低计算开销和良好模型准确性的要求，在FL假设下，服务器无法访问原始数据以微调修剪的模型。我们提出了补充稀疏化（CS），这是一种剪枝机制，通过在服务器和客户端之间进行互补和协作的剪枝来满足所有这些要求。在每一轮中，CS创建一个全局稀疏模型，其中包含捕获所有客户端的一般数据分布的权重，而客户端则创建本地稀疏模型。

    Federated Learning (FL) is a privacy-preserving distributed deep learning paradigm that involves substantial communication and computation effort, which is a problem for resource-constrained mobile and IoT devices. Model pruning/sparsification develops sparse models that could solve this problem, but existing sparsification solutions cannot satisfy at the same time the requirements for low bidirectional communication overhead between the server and the clients, low computation overhead at the clients, and good model accuracy, under the FL assumption that the server does not have access to raw data to fine-tune the pruned models. We propose Complement Sparsification (CS), a pruning mechanism that satisfies all these requirements through a complementary and collaborative pruning done at the server and the clients. At each round, CS creates a global sparse model that contains the weights that capture the general data distribution of all clients, while the clients create local sparse model
    
[^31]: 谁在思考？使用XAI Playbook推动以人为中心的LLMs评估

    Who's Thinking? A Push for Human-Centered Evaluation of LLMs using the XAI Playbook. (arXiv:2303.06223v1 [cs.HC])

    [http://arxiv.org/abs/2303.06223](http://arxiv.org/abs/2303.06223)

    本文探讨了以人为中心的人工智能系统评估，将相对成熟的可解释AI领域与快速发展的大型语言模型研究进行了类比，认为人类的需求应该成为LLMs评估的核心。

    This paper explores human-centered evaluation of AI-based systems, drawing parallels between the relatively mature field of explainable AI and the rapidly evolving research boom around large language models. The authors argue that humans' needs should be held front and center in evaluating LLMs.

    部署的人工智能（AI）经常影响人类，而评估这些工具没有一种适合所有情况的度量标准。以人为中心的AI系统评估结合了定量和定性分析以及人类输入。它已经在可解释的AI（XAI）和人机交互（HCI）社区中得到了深入探讨。仍然存在差距，但社区已经接受了人类与AI及其附带的解释进行交互，以及应该将人类的需求（包括他们的认知偏见和怪癖）放在首位的基本理解。在本文中，我们将相对成熟的XAI领域与快速发展的大型语言模型（LLMs）研究热潮之间进行了类比。接受的LLMs评估指标不是以人为中心的。我们认为，在讨论LLMs时，XAI社区在过去十年中走过的许多相同路径将被重新踏上。具体而言，我们认为人类的倾向 - 再次完全包括他们的认知偏见和怪癖 - 应该成为LLMs评估的核心。

    Deployed artificial intelligence (AI) often impacts humans, and there is no one-size-fits-all metric to evaluate these tools. Human-centered evaluation of AI-based systems combines quantitative and qualitative analysis and human input. It has been explored to some depth in the explainable AI (XAI) and human-computer interaction (HCI) communities. Gaps remain, but the basic understanding that humans interact with AI and accompanying explanations, and that humans' needs -- complete with their cognitive biases and quirks -- should be held front and center, is accepted by the community. In this paper, we draw parallels between the relatively mature field of XAI and the rapidly evolving research boom around large language models (LLMs). Accepted evaluative metrics for LLMs are not human-centered. We argue that many of the same paths tread by the XAI community over the past decade will be retread when discussing LLMs. Specifically, we argue that humans' tendencies -- again, complete with the
    
[^32]: AI写作和插图的碳排放比人类低

    The Carbon Emissions of Writing and Illustrating Are Lower for AI than for Humans. (arXiv:2303.06219v1 [cs.CY])

    [http://arxiv.org/abs/2303.06219](http://arxiv.org/abs/2303.06219)

    本文研究了几个AI系统相对于人类完成相同任务的排放量，发现AI写作和插图的碳排放比人类低130到1500倍和310到2900倍，目前使用AI有可能以比人类更低的排放水平完成几项重要活动。

    This paper analyzes the emissions of several AI systems relative to those of humans completing the same tasks, finding that AI writing and illustrating emit 130 to 1500 times less CO2e and 310 to 2900 times less, respectively. The use of AI holds the potential to carry out several major activities at much lower emission levels than can humans.

    随着AI系统的普及，它们的温室气体排放对人类社会越来越重要。我们分析了几个AI系统（ChatGPT、BLOOM、DALL-E2、Midjourney）相对于人类完成相同任务的排放量。我们发现，一个AI写一页文本排放的二氧化碳当量比一个人少130到1500倍。同样，一个AI创造一张图片排放的二氧化碳当量比人类少310到2900倍。排放分析没有考虑到社会影响，如职业替代、合法性和反弹效应。此外，AI并不能替代所有的人类任务。然而，目前使用AI有可能以比人类更低的排放水平完成几项重要活动。

    As AI systems proliferate, their greenhouse gas emissions are an increasingly important concern for human societies. We analyze the emissions of several AI systems (ChatGPT, BLOOM, DALL-E2, Midjourney) relative to those of humans completing the same tasks. We find that an AI writing a page of text emits 130 to 1500 times less CO2e than a human doing so. Similarly, an AI creating an image emits 310 to 2900 times less. Emissions analysis do not account for social impacts such as professional displacement, legality, and rebound effects. In addition, AI is not a substitute for all human tasks. Nevertheless, at present, the use of AI holds the potential to carry out several major activities at much lower emission levels than can humans.
    
[^33]: CHGNN: 一种半监督对比超图学习网络

    CHGNN: A Semi-Supervised Contrastive Hypergraph Learning Network. (arXiv:2303.06213v1 [cs.LG])

    [http://arxiv.org/abs/2303.06213](http://arxiv.org/abs/2303.06213)

    CHGNN是一种半监督对比超图学习网络，利用自监督对比学习技术从标记和未标记的数据中学习，包括自适应超图视图生成器、改进的超图编码器和联合损失函数。

    CHGNN is a semi-supervised contrastive hypergraph learning network that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. It includes an adaptive hypergraph view generator, an improved hypergraph encoder, and a joint loss function.

    超图可以模拟应用程序中发现的数据对象之间的高阶关系，例如社交网络和生物信息学。然而，最近的超图学习研究将图卷积网络扩展到超图，但无法有效地从未标记数据的特征中学习。为了进行这样的学习，我们提出了一种对比超图神经网络CHGNN，它利用自监督对比学习技术从标记和未标记的数据中学习。首先，CHGNN包括一个自适应超图视图生成器，采用自动增强策略，并学习最小充分视图的扰动概率分布。其次，CHGNN包含一个改进的超图编码器，考虑到超边的同质性，以有效地融合信息。第三，CHGNN配备了一个联合损失函数，结合了视图生成器的相似性损失、节点分类损失和超边同质性损失，注入监督信号。

    Hypergraphs can model higher-order relationships among data objects that are found in applications such as social networks and bioinformatics. However, recent studies on hypergraph learning that extend graph convolutional networks to hypergraphs cannot learn effectively from features of unlabeled data. To such learning, we propose a contrastive hypergraph neural network, CHGNN, that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. First, CHGNN includes an adaptive hypergraph view generator that adopts an auto-augmentation strategy and learns a perturbed probability distribution of minimal sufficient views. Second, CHGNN encompasses an improved hypergraph encoder that considers hyperedge homogeneity to fuse information effectively. Third, CHGNN is equipped with a joint loss function that combines a similarity loss for the view generator, a node classification loss, and a hyperedge homogeneity loss to inject supervision signals. It also i
    
[^34]: 带有二元超模矩阵的公平性加权概念

    Weighted Notions of Fairness with Binary Supermodular Chores. (arXiv:2303.06212v1 [cs.GT])

    [http://arxiv.org/abs/2303.06212](http://arxiv.org/abs/2303.06212)

    本文研究了在具有二元超模成本函数的代理之间分配不可分割家务的问题，并提出了一个通用的公平分配框架，适用于这类估值函数。该框架可以有效地计算满足加权公平性概念的分配。

    This paper studies the problem of allocating indivisible chores among agents with binary supermodular cost functions and presents a general framework for fair allocation with this class of valuation functions. The framework allows for efficient computation of allocations that satisfy weighted notions of fairness.

    我们研究了在具有二元超模成本函数的代理之间分配不可分割家务的问题。换句话说，每个家务的边际成本为$0$或$1$，并且家务呈现出递增的边际成本（或递减的边际效用）。在本文中，我们结合了Viswanathan和Zick（2022）以及Barman等人（2023）的技术，提出了一个通用的公平分配框架，适用于这类估值函数。我们的框架允许我们推广Barman等人（2023）的结果，并有效地计算满足加权公平性概念（如加权leximin或min加权$p$-mean malfare，其中$p \ge 1$）的分配。

    We study the problem of allocating indivisible chores among agents with binary supermodular cost functions. In other words, each chore has a marginal cost of $0$ or $1$ and chores exhibit increasing marginal costs (or decreasing marginal utilities). In this note, we combine the techniques of Viswanathan and Zick (2022) and Barman et al. (2023) to present a general framework for fair allocation with this class of valuation functions. Our framework allows us to generalize the results of Barman et al. (2023) and efficiently compute allocations which satisfy weighted notions of fairness like weighted leximin or min weighted $p$-mean malfare for any $p \ge 1$.
    
[^35]: 基于POV的高速公路车辆轨迹数据集和预测架构

    A POV-based Highway Vehicle Trajectory Dataset and Prediction Architecture. (arXiv:2303.06202v1 [cs.CV])

    [http://arxiv.org/abs/2303.06202](http://arxiv.org/abs/2303.06202)

    介绍了一个基于POV的高速公路车辆轨迹数据集和预测架构，其中的Carolinas Highway Dataset（CHD）包括160万帧和338,000个车辆轨迹。

    Introducing a POV-based highway vehicle trajectory dataset and prediction architecture, including Carolinas Highway Dataset (CHD) with 1.6 million frames and 338,000 vehicle trajectories captured at eight locations in Carolinas.

    提供多个视角（POV）的车辆轨迹数据集对于各种交通安全和管理应用非常有价值。尽管轨迹数据集很丰富，但很少提供全面和多样化的驾驶场景，捕捉各种高速公路布局、合并车道和配置的多个视点。这限制了它们捕捉驾驶员、车辆和道路基础设施之间微妙互动的能力。我们介绍了Carolinas Highway Dataset（CHD），这是一个车辆轨迹、检测和跟踪数据集。CHD是在Carolinas的八个位置拍摄的高速公路视频中捕获的160万帧，包括338,000个车辆轨迹。

    Vehicle Trajectory datasets that provide multiple point-of-views (POVs) can be valuable for various traffic safety and management applications. Despite the abundance of trajectory datasets, few offer a comprehensive and diverse range of driving scenes, capturing multiple viewpoints of various highway layouts, merging lanes, and configurations. This limits their ability to capture the nuanced interactions between drivers, vehicles, and the roadway infrastructure. We introduce the \emph{Carolinas Highway Dataset (CHD\footnote{\emph{CHD} available at: \url{https://github.com/TeCSAR-UNCC/Carolinas\_Dataset}})}, a vehicle trajectory, detection, and tracking dataset. \emph{CHD} is a collection of 1.6 million frames captured in highway-based videos from eye-level and high-angle POVs at eight locations across Carolinas with 338,000 vehicle trajectories. The locations, timing of recordings, and camera angles were carefully selected to capture various road geometries, traffic patterns, lighting 
    
[^36]: 针对分布式非独立同分布数据和部分标签的医学图像分类，优化联邦学习

    Optimizing Federated Learning for Medical Image Classification on Distributed Non-iid Datasets with Partial Labels. (arXiv:2303.06180v1 [cs.LG])

    [http://arxiv.org/abs/2303.06180](http://arxiv.org/abs/2303.06180)

    本文提出了FedFBN，一个联邦学习框架，使用预训练的网络作为模型后端，并在整个训练过程中冻结批量归一化层，以优化分布式非独立同分布数据和部分标签的医学图像分类。

    This paper proposes FedFBN, a federated learning framework that uses pretrained networks as the model backend and freezes the batch normalization layers throughout the training process to optimize medical image classification on distributed non-iid datasets with partial labels.

    大量的胸部X光数据集已经带头使用深度学习进行异常检测。然而，这些数据集专注于检测可能存在的一部分疾病标签，因此使它们成为分布式和非独立同分布的部分标签数据集。最近的文献指出，批量归一化层对于联邦学习的收敛具有影响，因为它们与具有部分标签的非独立同分布数据相关的域漂移。为此，我们提出了FedFBN，这是一个联邦学习框架，它从迁移学习中汲取灵感，使用预训练的网络作为模型后端，并在整个训练过程中冻结批量归一化层。我们使用合成iid玩具数据集和大规模非iid数据集评估FedFBN与当前FL策略。我们的结果表明，FedFBN优于使用分布式和非独立同分布数据训练全局模型的当前聚合策略。

    Numerous large-scale chest x-ray datasets have spearheaded expert-level detection of abnormalities using deep learning. However, these datasets focus on detecting a subset of disease labels that could be present, thus making them distributed and non-iid with partial labels. Recent literature has indicated the impact of batch normalization layers on the convergence of federated learning due to domain shift associated with non-iid data with partial labels. To that end, we propose FedFBN, a federated learning framework that draws inspiration from transfer learning by using pretrained networks as the model backend and freezing the batch normalization layers throughout the training process. We evaluate FedFBN with current FL strategies using synthetic iid toy datasets and large-scale non-iid datasets across scenarios with partial and complete labels. Our results demonstrate that FedFBN outperforms current aggregation strategies for training global models using distributed and non-iid data w
    
[^37]: 编程语言之间的软件漏洞预测知识转移

    Software Vulnerability Prediction Knowledge Transferring Between Programming Languages. (arXiv:2303.06177v1 [cs.SE])

    [http://arxiv.org/abs/2303.06177](http://arxiv.org/abs/2303.06177)

    本研究提出了一种转移学习技术，利用可用数据集生成一个模型，以检测不同编程语言中的常见漏洞。结果表明，所提出的模型以平均召回率为72％检测C和Java代码中的漏洞。

    This study proposes a transfer learning technique to detect common vulnerabilities in different programming languages by leveraging available datasets. The results show that the proposed model detects vulnerabilities in both C and Java codes with an average recall of 72%.

    开发自动化和智能的软件漏洞检测模型一直受到研究和开发社区的关注。这个领域最大的挑战之一是缺乏所有不同编程语言的代码样本。在本研究中，我们通过提出一种转移学习技术来解决这个问题，利用可用数据集生成一个模型，以检测不同编程语言中的常见漏洞。我们使用C源代码样本训练卷积神经网络（CNN）模型，然后使用Java源代码样本来采用和评估学习的模型。我们使用两个基准数据集的代码样本：NIST软件保障参考数据集（SARD）和Draper VDISC数据集。结果表明，所提出的模型以平均召回率为72％检测C和Java代码中的漏洞。此外，我们采用可解释的AI来调查每个特征对知识转移机制的贡献程度。

    Developing automated and smart software vulnerability detection models has been receiving great attention from both research and development communities. One of the biggest challenges in this area is the lack of code samples for all different programming languages. In this study, we address this issue by proposing a transfer learning technique to leverage available datasets and generate a model to detect common vulnerabilities in different programming languages. We use C source code samples to train a Convolutional Neural Network (CNN) model, then, we use Java source code samples to adopt and evaluate the learned model. We use code samples from two benchmark datasets: NIST Software Assurance Reference Dataset (SARD) and Draper VDISC dataset. The results show that proposed model detects vulnerabilities in both C and Java codes with average recall of 72\%. Additionally, we employ explainable AI to investigate how much each feature contributes to the knowledge transfer mechanisms between 
    
[^38]: 统一理解Grokking和双重下降

    Unifying Grokking and Double Descent. (arXiv:2303.06173v1 [cs.LG])

    [http://arxiv.org/abs/2303.06173](http://arxiv.org/abs/2303.06173)

    本文提出了一种模式学习速度框架，用于统一理解Grokking和双重下降，同时提供了模型智能Grokking的第一个演示。

    This paper proposes a framework of pattern learning speeds to unify the understanding of Grokking and double descent, and provides the first demonstration of model-wise Grokking.

    在深度学习中，对泛化的原则性理解可能需要将不同的观察结果统一到一个概念框架下。以前的工作研究了“Grokking”，这是一种训练动态，其中持续的近乎完美的训练表现和近乎偶然的测试表现最终会导致泛化，以及表面上类似的“双重下降”。到目前为止，这些主题已经被孤立地研究。我们假设Grokking和双重下降可以被理解为模式学习速度框架内相同学习动态的实例。我们提出，当改变模型容量而不是优化步骤时，该框架也适用，并提供了模型智能Grokking的第一个演示。

    A principled understanding of generalization in deep learning may require unifying disparate observations under a single conceptual framework. Previous work has studied \emph{grokking}, a training dynamic in which a sustained period of near-perfect training performance and near-chance test performance is eventually followed by generalization, as well as the superficially similar \emph{double descent}. These topics have so far been studied in isolation. We hypothesize that grokking and double descent can be understood as instances of the same learning dynamics within a framework of pattern learning speeds. We propose that this framework also applies when varying model capacity instead of optimization steps, and provide the first demonstration of model-wise grokking.
    
[^39]: 理解质量多样性和深度强化学习之间的协同作用

    Understanding the Synergies between Quality-Diversity and Deep Reinforcement Learning. (arXiv:2303.06164v1 [cs.LG])

    [http://arxiv.org/abs/2303.06164](http://arxiv.org/abs/2303.06164)

    本文提出了广义演员-评论家QD-RL框架，用于QD-RL设置中的演员-评论家深度RL方法。该框架引入了两种新算法，PGA-ME（SAC）和PGA-ME（DroQ），将深度RL的最新进展应用于QD-RL设置，并解决了现有QD-RL算法无法解决的人形环境问题。

    This paper proposes a Generalized Actor-Critic QD-RL framework for actor-critic deep RL methods in the QD-RL setting. The framework introduces two new algorithms, PGA-ME (SAC) and PGA-ME (DroQ), which apply recent advancements in Deep RL to the QD-RL setting and solve the humanoid environment problem that existing QD-RL algorithms cannot solve.

    质量多样性（QD）和深度强化学习（RL）之间的协同作用已经导致了强大的混合QD-RL算法，展示了巨大的潜力，并带来了两个领域的最佳实践。然而，尽管其他RL算法取得了显著进展，但在先前的混合方法中仅使用了单个深度RL算法（TD3）。此外，QD和RL之间的优化过程存在根本差异，需要更加原则性的方法。我们提出了广义演员-评论家QD-RL，这是一个统一的模块化框架，用于QD-RL设置中的演员-评论家深度RL方法。该框架提供了一条研究深度RL在QD-RL设置中的见解的路径，这是在QD-RL中取得进展的重要且有效的方法。我们引入了两种新算法，PGA-ME（SAC）和PGA-ME（DroQ），将深度RL的最新进展应用于QD-RL设置，并解决了现有QD-RL算法无法解决的人形环境问题。

    The synergies between Quality-Diversity (QD) and Deep Reinforcement Learning (RL) have led to powerful hybrid QD-RL algorithms that have shown tremendous potential, and brings the best of both fields. However, only a single deep RL algorithm (TD3) has been used in prior hybrid methods despite notable progress made by other RL algorithms. Additionally, there are fundamental differences in the optimization procedures between QD and RL which would benefit from a more principled approach. We propose Generalized Actor-Critic QD-RL, a unified modular framework for actor-critic deep RL methods in the QD-RL setting. This framework provides a path to study insights from Deep RL in the QD-RL setting, which is an important and efficient way to make progress in QD-RL. We introduce two new algorithms, PGA-ME (SAC) and PGA-ME (DroQ) which apply recent advancements in Deep RL to the QD-RL setting, and solves the humanoid environment which was not possible using existing QD-RL algorithms. However, we 
    
[^40]: 数字孪生辅助异构联邦学习的知识蒸馏框架

    Digital Twin-Assisted Knowledge Distillation Framework for Heterogeneous Federated Learning. (arXiv:2303.06155v1 [cs.LG])

    [http://arxiv.org/abs/2303.06155](http://arxiv.org/abs/2303.06155)

    本文提出了一种数字孪生辅助的知识蒸馏框架，用于解决联邦学习系统中的异构性问题，用户可以选择自己的神经网络模型并从大型教师模型中蒸馏知识，同时利用数字孪生在服务器上训练大型教师模型，最终通过混合整数规划和Q-learning算法实现模型选择和资源分配。

    This paper proposes a digital twin-assisted knowledge distillation framework for heterogeneous federated learning, where users can select their own neural network models and distill knowledge from a big teacher model, and the teacher model can be trained on a digital twin located in the server. The joint problem of model selection and training offloading and resource allocation for users is formulated as a mixed integer programming problem and solved using Q-learning and optimization algorithms.

    本文提出了一种知识蒸馏驱动的联邦学习框架，以应对联邦学习系统中的异构性，其中每个用户可以根据需要选择其神经网络模型，并使用自己的私有数据集从大型教师模型中蒸馏知识。为了克服在资源有限的用户设备上训练大型教师模型的挑战，利用数字孪生的方式，教师模型可以在具有足够计算资源的服务器上的数字孪生中进行训练。然后，在模型蒸馏期间，每个用户可以在物理实体或数字代理处更新其模型的参数。为用户选择模型和训练卸载和资源分配制定了混合整数规划（MIP）问题。为了解决这个问题，联合使用Q-learning和优化，其中Q-learning为用户选择模型并确定是在本地还是在服务器上进行训练，而优化则用于资源分配。

    In this paper, to deal with the heterogeneity in federated learning (FL) systems, a knowledge distillation (KD) driven training framework for FL is proposed, where each user can select its neural network model on demand and distill knowledge from a big teacher model using its own private dataset. To overcome the challenge of train the big teacher model in resource limited user devices, the digital twin (DT) is exploit in the way that the teacher model can be trained at DT located in the server with enough computing resources. Then, during model distillation, each user can update the parameters of its model at either the physical entity or the digital agent. The joint problem of model selection and training offloading and resource allocation for users is formulated as a mixed integer programming (MIP) problem. To solve the problem, Q-learning and optimization are jointly used, where Q-learning selects models for users and determines whether to train locally or on the server, and optimiz
    
[^41]: 为什么这是一个好的或不是一个好的煎锅？——对象和工具功能的知识表示，用于设计理解、改进和生成

    Why is That a Good or Not a Good Frying Pan? -- Knowledge Representation for Functions of Objects and Tools for Design Understanding, Improvement, and Generation for Design Understanding, Improvement, and Generation. (arXiv:2303.06152v1 [cs.AI])

    [http://arxiv.org/abs/2303.06152](http://arxiv.org/abs/2303.06152)

    本文演示了如何使用通用函数表示语言和框架来表示特定对象及其参与支持其设计的过程，从而实现深入的概念理解，可解释性的功能，使系统能够回答“为什么”问题。

    This paper demonstrates how a particular object and its participation in the processes it is designed to support can be represented in a general function representational language and framework, leading to a deep conceptual understanding with explainability of functionalities that allows the system to answer "why" questions.

    对象和工具的功能方面的理解对于支持智能系统在环境中导航和与各种对象、结构和系统进行交互以帮助实现其目标至关重要。对功能的详细理解也可以导致设计改进和新颖的设计，从而增强人工智能和机器人系统的操作，一方面，增强人类生活。本文演示了如何使用通用函数表示语言和框架来表示特定对象（在本例中为煎锅）及其参与支持其设计的过程（在本例中为煎炸过程），从而可以用于详细说明涉及的过程和功能，从而实现深入的概念理解，可解释性的功能，使系统能够回答“为什么”问题——为什么这是一个好的煎锅，或者为什么某个部件在煎锅中的位置很重要。

    The understanding of the functional aspects of objects and tools is of paramount importance in supporting an intelligent system in navigating around in the environment and interacting with various objects, structures, and systems, to help fulfil its goals. A detailed understanding of functionalities can also lead to design improvements and novel designs that would enhance the operations of AI and robotic systems on the one hand, and human lives on the other. This paper demonstrates how a particular object - in this case, a frying pan - and its participation in the processes it is designed to support - in this case, the frying process - can be represented in a general function representational language and framework, that can be used to flesh out the processes and functionalities involved, leading to a deep conceptual understanding with explainability of functionalities that allows the system to answer "why" questions - why is something a good frying pan, say, or why a certain part on t
    
[^42]: NoiseCAM: 用于噪声和对抗攻击边界的可解释人工智能

    NoiseCAM: Explainable AI for the Boundary Between Noise and Adversarial Attacks. (arXiv:2303.06151v1 [cs.LG])

    [http://arxiv.org/abs/2303.06151](http://arxiv.org/abs/2303.06151)

    本文提出了一种名为NoiseCAM的算法，该算法可以定位易受攻击层并检测对抗性示例，同时不会对混入输入的高斯随机噪声做出反应。

    This paper proposes a NoiseCAM algorithm that can locate vulnerable layers and detect adversarial examples, while not responding to Gaussian random noise mixed in the inputs.

    深度学习和深度神经网络在各个领域得到了广泛应用。然而，对抗攻击很容易误导神经网络并导致错误决策。在安全关键应用中高度需要防御机制。本文首先使用梯度类激活映射（GradCAM）分析VGG-16网络在其输入与对抗扰动或高斯噪声混合时的行为偏差。特别地，我们的方法可以定位对抗扰动和高斯噪声敏感的易受攻击层。我们还展示易受攻击层的行为偏差可以用于检测对抗性示例。其次，我们提出了一种新的NoiseCAM算法，该算法集成了全局和像素级加权类激活映射的信息。我们的算法容易受到对抗扰动的影响，不会对混入输入的高斯随机噪声做出反应。第三，我们比较了检测对抗性示例的方法。

    Deep Learning (DL) and Deep Neural Networks (DNNs) are widely used in various domains. However, adversarial attacks can easily mislead a neural network and lead to wrong decisions. Defense mechanisms are highly preferred in safety-critical applications. In this paper, firstly, we use the gradient class activation map (GradCAM) to analyze the behavior deviation of the VGG-16 network when its inputs are mixed with adversarial perturbation or Gaussian noise. In particular, our method can locate vulnerable layers that are sensitive to adversarial perturbation and Gaussian noise. We also show that the behavior deviation of vulnerable layers can be used to detect adversarial examples. Secondly, we propose a novel NoiseCAM algorithm that integrates information from globally and pixel-level weighted class activation maps. Our algorithm is susceptible to adversarial perturbations and will not respond to Gaussian random noise mixed in the inputs. Third, we compare detecting adversarial examples 
    
[^43]: 基于规划强化学习的可再生能源电力系统实时调度

    Real-time scheduling of renewable power systems through planning-based reinforcement learning. (arXiv:2303.05205v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.05205](http://arxiv.org/abs/2303.05205)

    本文提出了一种基于规划强化学习算法和真实电力网环境的系统解决方案，可以实现发电机的规划和更细的时间分辨率调整，从而增加了电网的能力。

    This paper proposes a systematic solution based on the state-of-the-art reinforcement learning algorithm and the real power grid environment, which enables planning and finer time resolution adjustments of power generators, including unit commitment and economic dispatch, thus increasing the grid's ability.

    不断增长的可再生能源来源对传统电力调度提出了重大挑战。运营商难以获得准确的可再生能源发电日前预测，因此需要未来调度系统根据超短期预测进行实时调度决策。受计算速度限制，传统的基于优化的方法无法解决这个问题。最近强化学习（RL）的发展已经展示了解决这个挑战的潜力。然而，现有的RL方法在约束复杂性、算法性能和环境保真度方面不足。我们是第一个提出基于最先进的强化学习算法和真实电力网环境的系统解决方案。所提出的方法使发电机的规划和更细的时间分辨率调整成为可能，包括机组组合和经济调度，从而增加了电网的能力。

    The growing renewable energy sources have posed significant challenges to traditional power scheduling. It is difficult for operators to obtain accurate day-ahead forecasts of renewable generation, thereby requiring the future scheduling system to make real-time scheduling decisions aligning with ultra-short-term forecasts. Restricted by the computation speed, traditional optimization-based methods can not solve this problem. Recent developments in reinforcement learning (RL) have demonstrated the potential to solve this challenge. However, the existing RL methods are inadequate in terms of constraint complexity, algorithm performance, and environment fidelity. We are the first to propose a systematic solution based on the state-of-the-art reinforcement learning algorithm and the real power grid environment. The proposed approach enables planning and finer time resolution adjustments of power generators, including unit commitment and economic dispatch, thus increasing the grid's abilit
    
[^44]: NASTyLinker：NIL感知可扩展基于Transformer的实体链接器

    NASTyLinker: NIL-Aware Scalable Transformer-based Entity Linker. (arXiv:2303.04426v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.04426](http://arxiv.org/abs/2303.04426)

    NASTyLinker是一种NIL感知的实体链接器，它通过生成提及簇来表示NIL实体，并在保持已知实体高链接性能的同时解决冲突。

    NASTyLinker is a NIL-aware entity linker that represents NIL entities by producing mention clusters and resolves conflicts while maintaining high linking performance for known entities.

    实体链接（EL）是检测文本中实体提及并将其消歧为参考知识库的任务。大多数流行的EL方法假定参考知识库是完整的。然而，在实践中，需要处理链接到不包含在知识库中的实体（NIL实体）的情况。最近的研究表明，考虑提及之间的亲和力可以用于表示NIL实体，方法是通过生成提及簇。同时，提及之间的亲和力可以帮助显著提高已知实体的链接性能。通过NASTyLinker，我们介绍了一种EL方法，它知道NIL实体并产生相应的提及簇，同时保持已知实体的高链接性能。该方法基于Transformer的密集表示对提及和实体进行聚类，并解决冲突（如果一个实体有多个提及）。

    Entity Linking (EL) is the task of detecting mentions of entities in text and disambiguating them to a reference knowledge base. Most prevalent EL approaches assume that the reference knowledge base is complete. In practice, however, it is necessary to deal with the case of linking to an entity that is not contained in the knowledge base (NIL entity). Recent works have shown that, instead of focusing only on affinities between mentions and entities, considering inter-mention affinities can be used to represent NIL entities by producing clusters of mentions. At the same time, inter-mention affinities can help to substantially improve linking performance for known entities. With NASTyLinker, we introduce an EL approach that is aware of NIL entities and produces corresponding mention clusters while maintaining high linking performance for known entities. The approach clusters mentions and entities based on dense representations from Transformers and resolves conflicts (if more than one en
    
[^45]: 利用预训练的AudioLDM进行文本到声音生成：基准研究

    Leveraging Pre-trained AudioLDM for Text to Sound Generation: A Benchmark Study. (arXiv:2303.03857v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2303.03857](http://arxiv.org/abs/2303.03857)

    本文研究了使用预训练的AudioLDM作为声音生成的骨干的优势，证明了在数据稀缺情况下使用预训练模型进行文本到声音生成的优势，并在几个常用数据集上使用相同的评估协议评估了各种文本到声音生成系统，为未来的研究提供了基础。

    This paper investigates the advantages of using pre-trained AudioLDM as the backbone for sound generation, demonstrates the benefits of using pre-trained models for text-to-sound generation in data-scarcity scenarios, and evaluates various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols to provide a basis for future research.

    深度神经网络最近在文本提示下实现了声音生成的突破。尽管它们的表现很有前途，但当前的文本到声音生成模型在小规模数据集（例如过度拟合）上面临问题，从而显著限制了它们的性能。在本文中，我们研究了使用预训练的AudioLDM作为声音生成的骨干的优势。我们的研究证明了在数据稀缺情况下使用预训练模型进行文本到声音生成的优势。此外，实验表明，不同的训练策略（例如训练条件）可能会影响AudioLDM在不同规模的数据集上的性能。为了促进未来的研究，我们还在几个常用数据集上使用相同的评估协议评估了各种文本到声音生成系统，这些协议允许在共同基础上公平比较和基准测试这些方法。

    Deep neural networks have recently achieved breakthroughs in sound generation with text prompts. Despite their promising performance, current text-to-sound generation models face issues on small-scale datasets (e.g., overfitting), significantly limiting their performance. In this paper, we investigate the use of pre-trained AudioLDM, the state-of-the-art model for text-to-audio generation, as the backbone for sound generation. Our study demonstrates the advantages of using pre-trained models for text-to-sound generation, especially in data-scarcity scenarios. In addition, experiments show that different training strategies (e.g., training conditions) may affect the performance of AudioLDM on datasets of different scales. To facilitate future studies, we also evaluate various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols, which allow fair comparisons and benchmarking of these methods on the common ground.
    
[^46]: 通过变分信息瓶颈和对比学习减少方面情感分析中的虚假相关性

    Reducing Spurious Correlations for Aspect-Based Sentiment Analysis with Variational Information Bottleneck and Contrastive Learning. (arXiv:2303.02846v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.02846](http://arxiv.org/abs/2303.02846)

    本文提出了一种新的对比变分信息瓶颈框架（CVIB），以减少方面情感分析（ABSA）中的虚假相关性。该框架由一个原始网络和一个自剪枝网络组成，通过对比学习同时进行优化，从而丢弃了输入特征和预测标签之间的多余模式或虚假相关性。

    This paper proposes a novel Contrastive Variational Information Bottleneck framework (CVIB) to reduce spurious correlations for aspect-based sentiment analysis (ABSA). The proposed CVIB framework is composed of an original network and a self-pruned network, and these two networks are optimized simultaneously via contrastive learning, which discards the superfluous patterns or spurious correlations between input features and prediction labels.

    深度学习技术在方面情感分析（ABSA）的文献中占据主导地位，取得了最先进的结果。然而，这些深度模型通常在输入特征和输出标签之间存在虚假相关性问题，这会给鲁棒性和泛化能力带来重大障碍。在本文中，我们提出了一种新颖的对比变分信息瓶颈框架（称为CVIB），以减少ABSA中的虚假相关性。所提出的CVIB框架由一个原始网络和一个自剪枝网络组成，这两个网络通过对比学习同时进行优化。具体而言，我们采用变分信息瓶颈（VIB）原则从原始网络中学习一个信息丰富且压缩的网络（自剪枝网络），该网络丢弃了输入特征和预测标签之间的多余模式或虚假相关性。然后，我们设计了自剪枝对比学习，以将两个网络拉在一起。

    Deep learning techniques have dominated the literature on aspect-based sentiment analysis (ABSA), yielding state-of-the-art results. However, these deep models generally suffer from spurious correlation problems between input features and output labels, which creates significant barriers to robustness and generalization capability. In this paper, we propose a novel Contrastive Variational Information Bottleneck framework (called CVIB) to reduce spurious correlations for ABSA. The proposed CVIB framework is composed of an original network and a self-pruned network, and these two networks are optimized simultaneously via contrastive learning. Concretely, we employ the Variational Information Bottleneck (VIB) principle to learn an informative and compressed network (self-pruned network) from the original network, which discards the superfluous patterns or spurious correlations between input features and prediction labels. Then, self-pruning contrastive learning is devised to pull together
    
[^47]: Prismer: 一种具有专家集合的视觉语言模型

    Prismer: A Vision-Language Model with An Ensemble of Experts. (arXiv:2303.02506v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02506](http://arxiv.org/abs/2303.02506)

    Prismer是一种数据和参数高效的视觉语言模型，它利用了一组领域专家的集合，通过汇集这些专家知识并将其适应于各种视觉语言推理任务，实现了与当前最先进模型竞争的微调和少样本学习性能，同时需要少至两个数量级的训练数据。

    Prismer is a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts, achieving fine-tuned and few-shot learning performance competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data.

    最近的视觉语言模型展示了令人印象深刻的多模态生成能力。然而，通常它们需要在大规模数据集上训练庞大的模型。作为一种更可扩展的替代方案，我们介绍了Prismer，一种数据和参数高效的视觉语言模型，它利用了一组领域专家的集合。Prismer只需要训练少量组件，大部分网络权重从现成的预训练领域专家中继承，并在训练期间保持冻结状态。通过利用来自各种领域的专家，我们展示了Prismer可以有效地汇集这些专家知识并将其适应于各种视觉语言推理任务。在我们的实验中，我们展示了Prismer实现了与当前最先进模型竞争的微调和少样本学习性能，同时需要少至两个数量级的训练数据。代码可在https://github.com/NVlabs/prismer获得。

    Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from readily-available, pre-trained domain experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show that Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data. Code is available at https://github.com/NVlabs/prismer.
    
[^48]: 通过离线强化学习学习影响人类行为

    Learning to Influence Human Behavior with Offline Reinforcement Learning. (arXiv:2303.02265v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.02265](http://arxiv.org/abs/2303.02265)

    本文提出了一种通过离线强化学习学习影响人类行为的方法，可以提高人类在协作任务中的表现。

    This paper proposes a method of learning to influence human behavior through offline reinforcement learning, which can improve human performance in collaborative tasks.

    在现实世界中，学习代理与人类互动是最复杂的设置之一，因为人类往往由于复杂的偏见而表现出次优的、不可预测的行为。在这种情况下与人类互动的代理最终会影响这些人所采取的行动。我们的目标是使代理能够利用这种影响来提高人类在协作任务中的表现，随着任务的展开。与以前的工作不同，我们不假设与人员进行在线培训（这往往太昂贵和不安全），也不假设有高保真度环境模拟器的访问权限。我们的想法是，通过采用各种先前观察到的人类-人类交互数据并将其标记为任务奖励，离线强化学习（RL）可以学习组合行为的组件，并发现导致更理想的人类行为的行动。首先，我们展示了离线RL可以学习策略来影响和改善人类行为，尽管这些策略可能与人类的期望不同。

    In the real world, some of the most complex settings for learned agents involve interaction with humans, who often exhibit suboptimal, unpredictable behavior due to sophisticated biases. Agents that interact with people in such settings end up influencing the actions that these people take. Our goal in this work is to enable agents to leverage that influence to improve the human's performance in collaborative tasks, as the task unfolds. Unlike prior work, we do not assume online training with people (which tends to be too expensive and unsafe), nor access to a high fidelity simulator of the environment. Our idea is that by taking a variety of previously observed human-human interaction data and labeling it with the task reward, offline reinforcement learning (RL) can learn to combine components of behavior, and uncover actions that lead to more desirable human actions. First, we show that offline RL can learn strategies to influence and improve human behavior, despite those strategies 
    
[^49]: 基于Fisher信息的证据深度学习方法用于不确定性估计

    Uncertainty Estimation by Fisher Information-based Evidential Deep Learning. (arXiv:2303.02045v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02045](http://arxiv.org/abs/2303.02045)

    本文提出了一种基于Fisher信息的证据深度学习方法，用于解决高数据不确定性样本但注释为one-hot标签的情况下证据学习过程被过度惩罚并受到阻碍的问题。

    This paper proposes a Fisher Information-based Evidential Deep Learning method to address the problem of over-penalization and hindrance in evidence learning for high data uncertainty samples annotated with one-hot labels.

    不确定性估计是使深度学习在实际应用中可靠的关键因素。最近提出的证据神经网络通过将网络输出视为证据来参数化狄利克雷分布，明确考虑不同的不确定性，并在不确定性估计方面取得了令人印象深刻的性能。然而，对于高数据不确定性样本但注释为one-hot标签的情况，这些错误标记的类别的证据学习过程会被过度惩罚并受到阻碍。为了解决这个问题，我们提出了一种新的方法，基于Fisher信息的证据深度学习（$\mathcal{I}$-EDL）。特别地，我们引入Fisher信息矩阵（FIM）来衡量每个样本所携带的证据的信息量，根据这个信息量，我们可以动态地重新加权目标损失项，使网络更加专注于不确定类别的表示学习。我们的网络的泛化能力通过优化进一步提高。

    Uncertainty estimation is a key factor that makes deep learning reliable in practical applications. Recently proposed evidential neural networks explicitly account for different uncertainties by treating the network's outputs as evidence to parameterize the Dirichlet distribution, and achieve impressive performance in uncertainty estimation. However, for high data uncertainty samples but annotated with the one-hot label, the evidence-learning process for those mislabeled classes is over-penalized and remains hindered. To address this problem, we propose a novel method, Fisher Information-based Evidential Deep Learning ($\mathcal{I}$-EDL). In particular, we introduce Fisher Information Matrix (FIM) to measure the informativeness of evidence carried by each sample, according to which we can dynamically reweight the objective loss terms to make the network more focused on the representation learning of uncertain classes. The generalization ability of our network is further improved by opt
    
[^50]: 文本转语音的细粒度情感控制：学习排名内部和外部类情感强度

    Fine-grained Emotional Control of Text-To-Speech: Learning To Rank Inter- And Intra-Class Emotion Intensities. (arXiv:2303.01508v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2303.01508](http://arxiv.org/abs/2303.01508)

    本文提出了一种细粒度可控情感TTS，考虑了内部和外部类距离，并能够合成具有可识别强度差异的语音。

    This paper proposes a fine-grained controllable emotional TTS, that considers both interand intra-class distances and be able to synthesize speech with recognizable intensity difference.

    最先进的文本转语音（TTS）模型能够产生高质量的语音。然而，生成的语音通常在情感表达上是中性的，而很多时候人们希望对单词或音素进行细粒度的情感控制。虽然仍然具有挑战性，但最近已经提出了第一批TTS模型，能够通过手动分配情感强度来控制语音。不幸的是，由于忽略了内部类距离，强度差异经常无法识别。在本文中，我们提出了一种细粒度可控情感TTS，考虑了内部和外部类距离，并能够合成具有可识别强度差异的语音。我们的主观和客观实验表明，我们的模型在可控性、情感表达和自然度方面超过了两个最先进的可控TTS模型。

    State-of-the-art Text-To-Speech (TTS) models are capable of producing high-quality speech. The generated speech, however, is usually neutral in emotional expression, whereas very often one would want fine-grained emotional control of words or phonemes. Although still challenging, the first TTS models have been recently proposed that are able to control voice by manually assigning emotion intensity. Unfortunately, due to the neglect of intra-class distance, the intensity differences are often unrecognizable. In this paper, we propose a fine-grained controllable emotional TTS, that considers both interand intra-class distances and be able to synthesize speech with recognizable intensity difference. Our subjective and objective experiments demonstrate that our model exceeds two state-of-the-art controllable TTS models for controllability, emotion expressiveness and naturalness.
    
[^51]: Mesh-SORT: 简单而有效的位置跟踪器及其丢失管理策略

    Mesh-SORT: Simple and effective location-wise tracker with lost management strategies. (arXiv:2302.14415v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.14415](http://arxiv.org/abs/2302.14415)

    Mesh-SORT是一种简单而有效的位置跟踪器，通过网格分割帧并提出相应的位置感知的丢失管理策略和不同的匹配策略，解决了跟踪中的不良检测器和遮挡问题。

    Mesh-SORT is a simple and effective location-wise tracker that solves the problem of bad detectors and occlusions in tracking by dividing frames into grids and proposing corresponding location-aware loss management strategies and different matching strategies.

    多目标跟踪(MOT)由于其在交通和行人检测等领域的潜在应用而受到广泛关注。我们注意到，通过检测进行跟踪可能会受到噪声检测器产生的误差的影响，例如在遮挡之前的不精确边界框，而且在大多数跟踪场景中，物体往往会在特定位置移动和丢失。为了解决这个问题，我们提出了一种新的跟踪器来处理不良检测器和遮挡。首先，我们提出了一种位置感知的子区域识别方法，将帧等分为网格。然后，我们提出了相应的位置感知的丢失管理策略和不同的匹配策略。结果，Mesh-SORT的消融研究证明了其有效性，并使MOT17数据集上的3%碎片化、7.2% ID切换下降和0.4% MOTA改进与基线相比。最后，我们分析了其在特定场景下的局限性，并讨论了未来的工作。

    Multi-Object Tracking (MOT) has gained extensive attention in recent years due to its potential applications in traffic and pedestrian detection. We note that tracking by detection may suffer from errors generated by noise detectors, such as an imprecise bounding box before the occlusions, and observed that in most tracking scenarios, objects tend to move and lost within specific locations. To counter this, we present a novel tracker to deal with the bad detector and occlusions. Firstly, we proposed a location-wise sub-region recognition method which equally divided the frame, which we called mesh. Then we proposed corresponding location-wise loss management strategies and different matching strategies. The resulting Mesh-SORT, ablation studies demonstrate its effectiveness and made 3% fragmentation 7.2% ID switches drop and 0.4% MOTA improvement compared to the baseline on MOT17 datasets. Finally, we analyze its limitation on the specific scene and discussed what future works can be e
    
[^52]: 重新审视带有正则化伪标签的表格数据自训练

    Revisiting Self-Training with Regularized Pseudo-Labeling for Tabular Data. (arXiv:2302.14013v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14013](http://arxiv.org/abs/2302.14013)

    本文重新审视了自我训练，并引入了课程伪标签用于表格领域。提出了一种新的伪标签方法，它规范化了置信度分数。

    This paper revisits self-training and introduces curriculum pseudo-labeling for tabular data. A novel pseudo-labeling approach is proposed to regularize the confidence scores of pseudo-labels generated from unlabeled data.

    半监督和自监督学习的最新进展已经打破了长期以来对于机器学习需要大量标记数据和未标记数据的无关性的信仰。虽然在各种数据上都取得了成功，但目前没有一种主导的半监督和自监督学习方法可以推广到表格数据（即大多数现有方法需要适当的表格数据集和架构）。在本文中，我们重新审视了自我训练，它可以应用于任何类型的算法，包括最广泛使用的架构——梯度提升决策树，并引入了课程伪标签（一种图像领域的最先进的伪标签技术）用于表格领域。此外，现有的伪标签技术在计算从未标记数据生成的伪标签的置信度分数时不能保证聚类假设。为了克服这个问题，我们提出了一种新的伪标签方法，它规范化了置信度分数。

    Recent progress in semi- and self-supervised learning has caused a rift in the long-held belief about the need for an enormous amount of labeled data for machine learning and the irrelevancy of unlabeled data. Although it has been successful in various data, there is no dominant semi- and self-supervised learning method that can be generalized for tabular data (i.e. most of the existing methods require appropriate tabular datasets and architectures). In this paper, we revisit self-training which can be applied to any kind of algorithm including the most widely used architecture, gradient boosting decision tree, and introduce curriculum pseudo-labeling (a state-of-the-art pseudo-labeling technique in image) for a tabular domain. Furthermore, existing pseudo-labeling techniques do not assure the cluster assumption when computing confidence scores of pseudo-labels generated from unlabeled data. To overcome this issue, we propose a novel pseudo-labeling approach that regularizes the confid
    
[^53]: 可解释性人工智能已死，可解释性人工智能万岁！基于假设的决策支持

    Explainable AI is Dead, Long Live Explainable AI! Hypothesis-driven decision support. (arXiv:2302.12389v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.12389](http://arxiv.org/abs/2302.12389)

    本文主张从当前的可解释性人工智能（XAI）模式转变，采用基于假设的决策支持系统，以更好地支持人类决策。

    This paper argues for a paradigm shift from the current model of explainable artificial intelligence (XAI) to hypothesis-driven decision support systems to better support human decision making.

    本文主张从当前的可解释性人工智能（XAI）模式转变，因为它可能会妨碍更好的人类决策。我们认为，人们不总是会接受和遵循建议，因为他们不信任它们，或者更糟糕的是，即使建议是错误的，人们也会盲目地遵循它们。可解释性人工智能通过帮助人们理解模型为什么会给出某些建议来缓解这种情况。然而，最近的研究表明，人们并不总是足够参与解释工具以帮助改善决策。我们认为，这是因为我们没有考虑到两件事情。首先，建议（及其解释）可能与人们的假设和信仰相冲突。其次，人们的决策往往是基于假设的，而不是基于事实的。因此，我们主张采用基于假设的决策支持系统，以更好地支持人类决策。

    In this paper, we argue for a paradigm shift from the current model of explainable artificial intelligence (XAI), which may be counter-productive to better human decision making. In early decision support systems, we assumed that we could give people recommendations and that they would consider them, and then follow them when required. However, research found that people often ignore recommendations because they do not trust them; or perhaps even worse, people follow them blindly, even when the recommendations are wrong. Explainable artificial intelligence mitigates this by helping people to understand how and why models give certain recommendations. However, recent research shows that people do not always engage with explainability tools enough to help improve decision making. The assumption that people will engage with recommendations and explanations has proven to be unfounded. We argue this is because we have failed to account for two things. First, recommendations (and their expla
    
[^54]: 多模态联邦学习：对比表示集成

    Multimodal Federated Learning via Contrastive Representation Ensemble. (arXiv:2302.08888v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08888](http://arxiv.org/abs/2302.08888)

    本文提出了一种名为CreamFL的多模态联邦学习框架，可以从具有异构模型架构和数据模态的客户端中训练更大的服务器模型，同时只在公共数据集上传递知识。

    This paper proposes a multimodal federated learning framework called CreamFL, which enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset.

    随着现代移动系统和物联网基础设施上的多媒体数据量的增加，利用这些丰富的多模态数据而不违反用户隐私成为一个关键问题。联邦学习（FL）作为集中式机器学习的隐私意识替代方案。然而，现有的扩展到多模态数据的FL方法都依赖于单模态级别的模型聚合，这限制了服务器和客户端在每个模态上具有相同的模型架构。这限制了全局模型的模型复杂度和数据容量，更不用说任务多样性了。在这项工作中，我们提出了对比表示集成和多模态FL聚合（CreamFL），这是一个多模态联邦学习框架，它可以从具有异构模型架构和数据模态的客户端中训练更大的服务器模型，同时只在公共数据集上传递知识。为了实现更好的多模态表示融合，我们设计了一个全局-

    With the increasing amount of multimedia data on modern mobile systems and IoT infrastructures, harnessing these rich multimodal data without breaching user privacy becomes a critical issue. Federated learning (FL) serves as a privacy-conscious alternative to centralized machine learning. However, existing FL methods extended to multimodal data all rely on model aggregation on single modality level, which restrains the server and clients to have identical model architecture for each modality. This limits the global model in terms of both model complexity and data capacity, not to mention task diversity. In this work, we propose Contrastive Representation Ensemble and Aggregation for Multimodal FL (CreamFL), a multimodal federated learning framework that enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset. To achieve better multimodal representation fusion, we design a global-
    
[^55]: 视觉、推理和对齐：多模态知识图谱对齐的实证研究

    Vision, Deduction and Alignment: An Empirical Study on Multi-modal Knowledge Graph Alignment. (arXiv:2302.08774v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.08774](http://arxiv.org/abs/2302.08774)

    本研究构建了八个大规模的、配备图像的实体对齐基准Multi-OpenEA，并开发了一种新的多模态EA方法LODEME，利用逻辑推理和多模态KG嵌入，实现了最先进的性能。

    This study constructed eight large-scale, image-equipped entity alignment benchmarks named Multi-OpenEA, and developed a new multi-modal EA method named LODEME, which utilizes logical deduction and multi-modal KG embedding, achieving state-of-the-art performance on Multi-OpenEA and other existing multi-modal EA benchmarks.

    知识图谱中的实体对齐在知识工程中起着至关重要的作用。现有的实体对齐方法主要集中在利用图形结构和实体属性（包括文字），但忽略了现代多模态知识图谱中常见的图像。在本研究中，我们首先构建了Multi-OpenEA——八个大规模的、配备图像的实体对齐基准，并评估了一些现有的基于嵌入的方法来利用图像。鉴于视觉模态信息和逻辑推理的互补性质，我们进一步开发了一种新的多模态EA方法，名为LODEME，使用逻辑推理和多模态KG嵌入，在Multi-OpenEA和其他现有的多模态EA基准上实现了最先进的性能。

    Entity alignment (EA) for knowledge graphs (KGs) plays a critical role in knowledge engineering. Existing EA methods mostly focus on utilizing the graph structures and entity attributes (including literals), but ignore images that are common in modern multi-modal KGs. In this study we first constructed Multi-OpenEA -- eight large-scale, image-equipped EA benchmarks, and then evaluated some existing embedding-based methods for utilizing images. In view of the complementary nature of visual modal information and logical deduction, we further developed a new multi-modal EA method named LODEME using logical deduction and multi-modal KG embedding, with state-of-the-art performance achieved on Multi-OpenEA and other existing multi-modal EA benchmarks.
    
[^56]: 人工智能心理学中的“正确答案”

    "Correct answers" from the psychology of artificial intelligence. (arXiv:2302.07267v3 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2302.07267](http://arxiv.org/abs/2302.07267)

    本文使用OpenAI的GPT3.5模型重新复制了Many Labs 2复制项目中的14项研究，其中8项研究的结果被成功复制。然而，对于剩下的6项研究，GPT3.5以极其预定的方式回答了调查问题，导致无法分析这些研究。

    This paper replicates 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, and successfully replicates the results of 8 studies. However, for the remaining 6 studies, GPT3.5 answered survey questions in an extremely predetermined way, making it impossible to analyze these studies.

    大型语言模型的能力已经大大增强。这种AI系统的一个提出的应用是支持社会和认知科学中的数据收集，目前完美的实验控制是不可行的，而大规模、代表性数据集的收集通常是昂贵的。在本文中，我们使用OpenAI的text-davinci-003模型（俗称GPT3.5）重新复制了Many Labs 2复制项目中的14项研究。我们通过将每项研究的调查作为文本输入，从GPT3.5的默认设置中收集了响应。在我们可以分析的八项研究中，我们的GPT样本复制了原始结果的37.5%以及Many Labs 2结果的37.5%。出乎意料的是，我们无法像预先注册的计划那样分析剩下的六项研究。这是因为对于这六项研究中的每一项，GPT3.5以极其预定的方式回答了调查问题（无论是因变量还是条件变量）：一个未知的

    Large Language Models have vastly grown in capabilities. One proposed application of such AI systems is to support data collection in the social and cognitive sciences, where perfect experimental control is currently unfeasible and the collection of large, representative datasets is generally expensive. In this paper, we re-replicate 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We collected responses from the default setting of GPT3.5 by inputting each study's survey as text. Among the eight studies we could analyse, our GPT sample replicated 37.5% of the original results as well as 37.5% of the Many Labs 2 results. Unexpectedly, we could not analyse the remaining six studies as we had planned in our pre-registration. This was because for each of these six studies, GPT3.5 answered at least one of the survey questions (either a dependent variable or a condition variable) in an extremely predetermined way: an unex
    
[^57]: 现象意识状态的丰富性和难以言说性的来源

    Sources of Richness and Ineffability for Phenomenally Conscious States. (arXiv:2302.06403v3 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2302.06403](http://arxiv.org/abs/2302.06403)

    本文提供了一个信息论动力系统的视角，来解释意识的丰富性和难以言说性。在我们的框架中，意识体验的丰富性对应于意识状态中的信息量，而难以言说性则对应于不同处理阶段丢失的信息量。

    This paper provides an information theoretic dynamical systems perspective on the richness and ineffability of consciousness. In their framework, the richness of conscious experience corresponds to the amount of information in a conscious state and ineffability corresponds to the amount of information lost at different stages of processing.

    意识状态（即有某种感受的状态）似乎既丰富又充满细节，又难以完全描述或回忆。特别是难以言说性的问题是哲学上长期存在的问题，部分激发了解释鸿沟的信念：意识不能归结为基础物理过程。在这里，我们提供了一个信息论动力系统的视角，来解释意识的丰富性和难以言说性。在我们的框架中，意识体验的丰富性对应于意识状态中的信息量，而难以言说性则对应于不同处理阶段丢失的信息量。我们描述了工作记忆中的吸引子动力学如何导致我们原始体验的贫乏回忆，语言的离散符号性质不足以描述体验的丰富和高维结构，以及认知功能相似性如何影响体验的共享和交流。

    Conscious states (states that there is something it is like to be in) seem both rich or full of detail, and ineffable or hard to fully describe or recall. The problem of ineffability, in particular, is a longstanding issue in philosophy that partly motivates the explanatory gap: the belief that consciousness cannot be reduced to underlying physical processes. Here, we provide an information theoretic dynamical systems perspective on the richness and ineffability of consciousness. In our framework, the richness of conscious experience corresponds to the amount of information in a conscious state and ineffability corresponds to the amount of information lost at different stages of processing. We describe how attractor dynamics in working memory would induce impoverished recollections of our original experiences, how the discrete symbolic nature of language is insufficient for describing the rich and high-dimensional structure of experiences, and how similarity in the cognitive function o
    
[^58]: 利用剪枝神经网络中的稀疏性来优化大型模型训练

    Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training. (arXiv:2302.05045v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05045](http://arxiv.org/abs/2302.05045)

    本文提出了一种新的方法，利用稀疏子网络来优化两种流行的深度学习并行算法 - 数据并行和层间并行的内存利用和通信。在512个NVIDIA V100 GPU上，我们的优化将27亿参数模型的内存消耗减少了74％，总通信时间减少了40％。

    This paper proposes a novel approach that exploits sparse subnetworks to optimize memory utilization and communication in two popular algorithms for parallel deep learning, and demonstrates significant reductions in memory consumption and communication time on a 2.7 billion parameter model using 512 NVIDIA V100 GPUs.

    由于通信开销的显著增加，规模化神经网络的并行训练具有挑战性。最近，深度学习研究人员开发了各种剪枝算法，能够剪枝（即将神经网络中的参数设置为零）80-90％的参数，以产生与未剪枝父网络相等的稀疏子网络。在本文中，我们提出了一种新的方法，利用这些稀疏子网络来优化两种流行的深度学习并行算法 - 数据并行和层间并行的内存利用和通信。我们将我们的方法集成到AxoNN中，这是一个高度可扩展的并行深度学习框架，依赖于数据和层间并行，并展示了通信时间和内存利用的减少。在512个NVIDIA V100 GPU上，我们的优化将27亿参数模型的内存消耗减少了74％，总通信时间减少了40％，从而提供了

    Parallel training of neural networks at scale is challenging due to significant overheads arising from communication. Recently, deep learning researchers have developed a variety of pruning algorithms that are capable of pruning (i.e. setting to zero) 80-90% of the parameters in a neural network to yield sparse subnetworks that equal the accuracy of the unpruned parent network. In this work, we propose a novel approach that exploits these sparse subnetworks to optimize the memory utilization and communication in two popular algorithms for parallel deep learning namely -- data and inter-layer parallelism. We integrate our approach into AxoNN, a highly scalable framework for parallel deep learning that relies on data and inter-layer parallelism, and demonstrate the reduction in communication time and memory utilization. On 512 NVIDIA V100 GPUs, our optimizations reduce the memory consumption of a 2.7 billion parameter model by 74%, and the total communication time by 40%, thus providing 
    
[^59]: 公平机器学习的不公平性：默认的水平下降和严格平等主义

    The Unfairness of Fair Machine Learning: Levelling down and strict egalitarianism by default. (arXiv:2302.02404v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.02404](http://arxiv.org/abs/2302.02404)

    公平机器学习的定义过于简单，许多公平性措施会导致性能降级和水平下降，使每个人都变得更糟，这种做法不符合实质平等的机会。

    The oversimplification of fairness in machine learning leads to performance degradation and leveling down, which does not achieve substantive equality.

    近年来，机器学习中的公平性已成为一个高度活跃的研究和开发领域。大多数人简单地定义公平性，即公平意味着减少不同人群之间的表现或结果差距，同时尽可能保留原始系统的准确性。这种通过公平性措施简化平等的做法令人不安。许多当前的公平性措施既存在公平性和性能降级，或者“水平下降”，即通过使每个群体变得更糟或将表现更好的群体降到最差的水平来实现公平性。当公平只能通过在物质或关系方面使每个人变得更糟而实现，通过污名化、团结的损失、不平等的关注和错过实质平等的机会，似乎在将模糊的“公平”概念转化为实践时出现了问题。本文探讨了造成这种现象的原因和普遍性。

    In recent years fairness in machine learning (ML) has emerged as a highly active area of research and development. Most define fairness in simple terms, where fairness means reducing gaps in performance or outcomes between demographic groups while preserving as much of the accuracy of the original system as possible. This oversimplification of equality through fairness measures is troubling. Many current fairness measures suffer from both fairness and performance degradation, or "levelling down," where fairness is achieved by making every group worse off, or by bringing better performing groups down to the level of the worst off. When fairness can only be achieved by making everyone worse off in material or relational terms through injuries of stigma, loss of solidarity, unequal concern, and missed opportunities for substantive equality, something would appear to have gone wrong in translating the vague concept of 'fairness' into practice. This paper examines the causes and prevalence 
    
[^60]: 重新思考半监督医学图像分割：方差缩减的视角

    Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective. (arXiv:2302.01735v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.01735](http://arxiv.org/abs/2302.01735)

    本文提出了ARCO，一种半监督对比学习（CL）框架，其中包括医学图像分割中的分层组采样理论。通过方差缩减估计的概念来构建ARCO，并表明某些方差缩减技术在医学图像分割中特别有益。

    This paper proposes ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. The concept of variance-reduced estimation is used to build ARCO, and certain variance-reduction techniques are shown to be particularly beneficial in medical image segmentation.

    对于医学图像分割，对比学习是提高视觉表示质量的主要方法，通过对比语义相似和不相似的样本对来实现。这是通过观察到，在没有访问地面真实标签的情况下，如果采样具有真正不同解剖特征的负样本，则可以显着提高性能。然而，在现实中，这些样本可能来自相似的解剖特征，模型可能难以区分少数尾类样本，使得尾类更容易被错误分类，这通常导致模型崩溃。在本文中，我们提出了ARCO，一种半监督对比学习（CL）框架，其中包括医学图像分割中的分层组采样理论。特别是，我们首先提出通过方差缩减估计的概念来构建ARCO，并表明某些方差缩减技术在医学图像分割中特别有益。

    For medical image segmentation, contrastive learning is the dominant practice to improve the quality of visual representations by contrasting semantically similar and dissimilar pairs of samples. This is enabled by the observation that without accessing ground truth label, negative examples with truly dissimilar anatomical features, if sampled, can significantly improve the performance. In reality, however, these samples may come from similar anatomical features and the models may struggle to distinguish the minority tail-class samples, making the tail classes more prone to misclassification, both of which typically lead to model collapse. In this paper, we propose ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. In particular, we first propose building ARCO through the concept of variance-reduced estimation, and show that certain variance-reduction techniques are particularly beneficial in medical image se
    
[^61]: 模拟城市空中出行融入现有交通系统：一项调查

    Simulating the Integration of Urban Air Mobility into Existing Transportation Systems: A Survey. (arXiv:2301.12901v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2301.12901](http://arxiv.org/abs/2301.12901)

    本文调查了城市空中出行（UAM）在大都市交通中的研究现状，确定了将UAM融入城市交通系统的关键挑战和机遇，包括对现有交通模式和拥堵的影响；安全分析和风险评估；潜在的经济和环境效益；以及为UAM和地面交通开发共享基础设施和路线。同时，我们讨论了UAM的潜在好处，如缩短旅行时间和改善服务不足地区的可达性。

    This paper surveys the current state of research on urban air mobility (UAM) in metropolitan-scale traffic using simulation techniques, identifying key challenges and opportunities for integrating UAM into urban transportation systems, including impacts on existing traffic patterns and congestion, safety analysis and risk assessment, potential economic and environmental benefits, and the development of shared infrastructure and routes for UAM and ground-based transportation. The potential benefits of UAM, such as reduced travel times and improved accessibility for underserved areas, are also discussed.

    城市空中出行（UAM）有可能彻底改变大都市地区的交通方式，提供一种新的交通方式，缓解拥堵，提高可达性。然而，将UAM融入现有交通系统是一项复杂的任务，需要深入了解其对交通流量和容量的影响。在本文中，我们进行了一项调查，使用模拟技术调查了UAM在大都市交通中的研究现状。我们确定了将UAM融入城市交通系统的关键挑战和机遇，包括对现有交通模式和拥堵的影响；安全分析和风险评估；潜在的经济和环境效益；以及为UAM和地面交通开发共享基础设施和路线。我们还讨论了UAM的潜在好处，如缩短旅行时间和改善服务不足地区的可达性。我们的调查

    Urban air mobility (UAM) has the potential to revolutionize transportation in metropolitan areas, providing a new mode of transportation that could alleviate congestion and improve accessibility. However, the integration of UAM into existing transportation systems is a complex task that requires a thorough understanding of its impact on traffic flow and capacity. In this paper, we conduct a survey to investigate the current state of research on UAM in metropolitan-scale traffic using simulation techniques. We identify key challenges and opportunities for the integration of UAM into urban transportation systems, including impacts on existing traffic patterns and congestion; safety analysis and risk assessment; potential economic and environmental benefits; and the development of shared infrastructure and routes for UAM and ground-based transportation. We also discuss the potential benefits of UAM, such as reduced travel times and improved accessibility for underserved areas. Our survey 
    
[^62]: LDMIC：基于学习的分布式多视图图像编码

    LDMIC: Learning-based Distributed Multi-view Image Coding. (arXiv:2301.09799v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.09799](http://arxiv.org/abs/2301.09799)

    LDMIC是一种基于学习的分布式多视图图像编码框架，通过独立编码器和联合上下文传输模块实现了全局视图间的相关性捕捉，对几何关系不敏感。

    LDMIC is a learning-based distributed multi-view image coding framework that captures global inter-view correlations through independent encoders and a joint context transfer module based on the cross-attention mechanism, which is insensitive to geometric relations.

    多视图图像压缩在3D相关应用中起着至关重要的作用。现有方法采用预测编码架构，需要联合编码压缩相应的视差和残差信息。这要求相机之间进行协作，并强制执行不同视图之间的极线几何约束，这使得在具有随机重叠视野的分布式相机系统中部署这些方法具有挑战性。同时，分布式源编码理论表明，可以通过独立编码和联合解码实现相关源的高效数据压缩，这激发了我们设计基于学习的分布式多视图图像编码（LDMIC）框架的动机。通过独立编码器，LDMIC引入了一个简单而有效的基于交叉注意机制的联合上下文传输模块，以有效捕捉全局视图间的相关性，对几何关系不敏感。

    Multi-view image compression plays a critical role in 3D-related applications. Existing methods adopt a predictive coding architecture, which requires joint encoding to compress the corresponding disparity as well as residual information. This demands collaboration among cameras and enforces the epipolar geometric constraint between different views, which makes it challenging to deploy these methods in distributed camera systems with randomly overlapping fields of view. Meanwhile, distributed source coding theory indicates that efficient data compression of correlated sources can be achieved by independent encoding and joint decoding, which motivates us to design a learning-based distributed multi-view image coding (LDMIC) framework. With independent encoders, LDMIC introduces a simple yet effective joint context transfer module based on the cross-attention mechanism at the decoder to effectively capture the global inter-view correlations, which is insensitive to the geometric relation
    
[^63]: 走向神经人工智能：将神经元多样性引入人工神经网络

    Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks. (arXiv:2301.09245v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2301.09245](http://arxiv.org/abs/2301.09245)

    引入神经元多样性可以解决人工神经网络的基本问题，走向神经人工智能。

    Introducing neuronal diversity can solve the fundamental problems of artificial neural networks and lead to NeuroAI.

    在整个历史上，人工智能的发展，特别是人工神经网络，一直对越来越深入的大脑理解持开放态度并不断受到启发，例如卷积神经网络的开创性工作neocognitron的启发。根据新兴领域神经人工智能的动机，大量的神经科学知识可以通过赋予网络更强大的能力来催化下一代人工智能的发展。我们知道，人类大脑有许多形态和功能不同的神经元，而人工神经网络几乎完全建立在单一神经元类型上。在人类大脑中，神经元多样性是各种生物智能行为的一个启动因素。由于人工网络是人类大脑的缩影，引入神经元多样性应该有助于解决人工网络的诸如效率、解释性等基本问题。

    Throughout history, the development of artificial intelligence, particularly artificial neural networks, has been open to and constantly inspired by the increasingly deepened understanding of the brain, such as the inspiration of neocognitron, which is the pioneering work of convolutional neural networks. Per the motives of the emerging field: NeuroAI, a great amount of neuroscience knowledge can help catalyze the next generation of AI by endowing a network with more powerful capabilities. As we know, the human brain has numerous morphologically and functionally different neurons, while artificial neural networks are almost exclusively built on a single neuron type. In the human brain, neuronal diversity is an enabling factor for all kinds of biological intelligent behaviors. Since an artificial network is a miniature of the human brain, introducing neuronal diversity should be valuable in terms of addressing those essential problems of artificial networks such as efficiency, interpret
    
[^64]: SegViz：基于联邦学习的多器官分割框架，适用于具有部分注释的异构数据集

    SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations. (arXiv:2301.07074v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.07074](http://arxiv.org/abs/2301.07074)

    SegViz是一种基于联邦学习的框架，用于从分布式的非i.i.d数据集中训练具有部分注释的分割模型。使用FedBN作为聚合策略的SegViz框架在外部BTCV集上表现出优异的性能，分割的dice分数分别为0.93、0.83、0.55和0.75。

    SegViz is a federated learning-based framework for training segmentation models from distributed non-i.i.d datasets with partial annotations. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation.

    分割是医学图像深度学习中最基本的任务之一，由于其多个下游临床应用而备受关注。然而，为医学图像生成手动注释是耗时的、需要高技能的、昂贵的工作，特别是对于3D图像。一个潜在的解决方案是从多个组的部分注释数据集中聚合知识，使用联邦学习协作训练全局模型。为此，我们提出了SegViz，一种基于联邦学习的框架，用于从分布式的非i.i.d数据集中训练具有部分注释的分割模型。将SegViz的性能与分别在每个数据集上单独训练模型以及集中聚合所有数据集并训练单个模型进行比较。使用FedBN作为聚合策略的SegViz框架在外部BTCV集上表现出优异的性能，分割的dice分数分别为0.93、0.83、0.55和0.75。

    Segmentation is one of the most primary tasks in deep learning for medical imaging, owing to its multiple downstream clinical applications. However, generating manual annotations for medical images is time-consuming, requires high skill, and is an expensive effort, especially for 3D images. One potential solution is to aggregate knowledge from partially annotated datasets from multiple groups to collaboratively train global models using Federated Learning. To this end, we propose SegViz, a federated learning-based framework to train a segmentation model from distributed non-i.i.d datasets with partial annotations. The performance of SegViz was compared against training individual models separately on each dataset as well as centrally aggregating all the datasets in one place and training a single model. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation 
    
[^65]: 图像数据增强方法：综述与未来方向

    Image Data Augmentation Approaches: A Comprehensive Survey and Future directions. (arXiv:2301.02830v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.02830](http://arxiv.org/abs/2301.02830)

    本文综述了图像数据增强方法，提供了全面的分类法和每种技术的优缺点，并给出了数据增强对图像分类、目标检测和语义分割等计算机视觉任务的全面结果。

    This article provides a comprehensive survey of advanced data augmentation techniques for computer vision tasks, including a novel taxonomy and evaluation of each technique's strengths and weaknesses. The article also presents comprehensive results of the data augmentation effect on popular computer vision tasks such as image classification, object detection, and semantic segmentation.

    深度学习算法在各种计算机视觉任务中表现出了显著的性能。然而，由于标记数据有限，导致网络过拟合问题，即网络在未见过的数据上的性能比训练数据差。因此，它限制了性能的提高。为了应对这个问题，提出了各种技术，如dropout、归一化和高级数据增强。其中，数据增强旨在通过包括样本多样性来扩大数据集大小，近来成为热门话题。在本文中，我们重点关注高级数据增强技术。我们提供了数据增强的背景、一个新颖而全面的分类法、以及每种技术的优点和缺点（在可能的情况下）。我们还提供了数据增强对三个流行的计算机视觉任务（如图像分类、目标检测和语义分割）的全面结果。

    Deep learning (DL) algorithms have shown significant performance in various computer vision tasks. However, having limited labelled data lead to a network overfitting problem, where network performance is bad on unseen data as compared to training data. Consequently, it limits performance improvement. To cope with this problem, various techniques have been proposed such as dropout, normalization and advanced data augmentation. Among these, data augmentation, which aims to enlarge the dataset size by including sample diversity, has been a hot topic in recent times. In this article, we focus on advanced data augmentation techniques. we provide a background of data augmentation, a novel and comprehensive taxonomy of reviewed data augmentation techniques, and the strengths and weaknesses (wherever possible) of each technique. We also provide comprehensive results of the data augmentation effect on three popular computer vision tasks, such as image classification, object detection and seman
    
[^66]: 图学习及其应用：一篇全面的综述

    Graph Learning and Its Applications: A Holistic Survey. (arXiv:2212.08966v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.08966](http://arxiv.org/abs/2212.08966)

    本文全面综述了图学习的发展历程和应用场景，重点介绍了表示学习在文本、图像、化学和生物等领域中的显著性能，同时指出了对以前有价值的工作进行调查的需求。

    This paper provides a comprehensive survey of the development and application scenarios of graph learning, with a focus on the remarkable performance of representation learning in various fields such as text, image, chemistry, and biology. It also points out the need to investigate previous valuable works.

    图学习是一种广泛应用的领域，旨在学习节点之间的复杂关系和图的拓扑结构。这些关系使得图与传统的表格数据相比具有独特性，因为节点依赖于非欧几里得空间，并包含丰富的信息可供利用。随着表示学习的出现，图学习在文本、图像、化学和生物等各种场景中取得了显著的性能。由于其广泛的应用前景，图学习吸引了学术界的大量关注。尽管已经有许多工作提出了解决图学习中不同问题的方法，但需要对以前有价值的工作进行调查。虽然一些研究人员已经意识到了这一现象，并在图学习方面完成了令人印象深刻的调查，但他们未能以更连贯的方式连接相关的目标、方法和应用。

    Graph learning is a prevalent domain that endeavors to learn the intricate relationships among nodes and the topological structure of graphs. These relationships endow graphs with uniqueness compared to conventional tabular data, as nodes rely on non-Euclidean space and encompass rich information to exploit. Over the years, graph learning has transcended from graph theory to graph data mining. With the advent of representation learning, it has attained remarkable performance in diverse scenarios, including text, image, chemistry, and biology. Owing to its extensive application prospects, graph learning attracts copious attention from the academic community. Despite numerous works proposed to tackle different problems in graph learning, there is a demand to survey previous valuable works. While some researchers have perceived this phenomenon and accomplished impressive surveys on graph learning, they failed to connect related objectives, methods, and applications in a more coherent way.
    
[^67]: 细调CLIP模型是高效的视频学习器

    Fine-tuned CLIP Models are Efficient Video Learners. (arXiv:2212.03640v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.03640](http://arxiv.org/abs/2212.03640)

    本文提出了一个简单的视频细调CLIP（ViFi-CLIP）基线，通过从CLIP图像编码器的帧级处理，接着进行特征池化和与相应文本嵌入的相似度匹配，有效地将图像级别的CLIP表示转移到视频中，从而弥合了从图像到视频的领域差距。

    This paper proposes a simple Video Fine-tuned CLIP (ViFi-CLIP) baseline, which effectively transfers image-level CLIP representations to videos by frame-level processing from CLIP image-encoder followed by feature pooling and similarity matching with corresponding text embeddings, thus bridging the domain gap from images to videos.

    通过图像-文本对的大规模多模态训练，CLIP模型具有强大的泛化能力。由于在类似规模上对视频进行训练是不可行的，因此最近的方法集中于将基于图像的CLIP有效地转移到视频领域。在这个追求中，添加了新的参数模块来学习时间信息和帧间关系，这需要精心设计。此外，当所得到的模型在视频上进行学习时，它们往往会过度拟合给定的任务分布，并且缺乏泛化方面。这引出了以下问题：如何有效地将图像级别的CLIP表示转移到视频中？在这项工作中，我们展示了一个简单的视频细调CLIP（ViFi-CLIP）基线通常足以弥合从图像到视频的领域差距。我们的定性分析表明，从CLIP图像编码器的帧级处理，接着进行特征池化和与相应文本嵌入的相似度匹配，有助于提高模型的性能。

    Large-scale multi-modal training with image-text pairs imparts strong generalization to CLIP model. Since training on a similar scale for videos is infeasible, recent approaches focus on the effective transfer of image-based CLIP to the video domain. In this pursuit, new parametric modules are added to learn temporal information and inter-frame relationships which require meticulous design efforts. Furthermore, when the resulting models are learned on videos, they tend to overfit on the given task distribution and lack in generalization aspect. This begs the following question: How to effectively transfer image-level CLIP representations to videos? In this work, we show that a simple Video Fine-tuned CLIP (ViFi-CLIP) baseline is generally sufficient to bridge the domain gap from images to videos. Our qualitative analysis illustrates that the frame-level processing from CLIP image-encoder followed by feature pooling and similarity matching with corresponding text embeddings helps in imp
    
[^68]: 统一视觉、文本和布局的通用文档处理

    Unifying Vision, Text, and Layout for Universal Document Processing. (arXiv:2212.02623v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02623](http://arxiv.org/abs/2212.02623)

    本文提出了通用文档处理（UDOP）模型，将文本、图像和布局模态以及各种任务格式统一起来，通过一种新颖的Transformer模型实现预训练和多域下游任务的统一，同时实现了高质量的神经文档编辑和内容定制。

    This paper proposes the Universal Document Processing (UDOP) model, which unifies text, image, and layout modalities together with varied task formats, and achieves pretraining and multi-domain downstream tasks unification through a novel Transformer model. It also achieves high-quality neural document editing and content customization.

    我们提出了通用文档处理（UDOP），这是一个基础的文档AI模型，它将文本、图像和布局模态以及各种任务格式（包括文档理解和生成）统一起来。UDOP利用文本内容和文档图像之间的空间相关性，用一个统一的表示来建模图像、文本和布局模态。通过一种新颖的Vision-Text-Layout Transformer，UDOP将预训练和多域下游任务统一到基于提示的序列生成方案中。UDOP在大规模无标签文档语料库和多样化标记数据上使用创新的自监督目标进行预训练。UDOP还通过遮蔽图像重建学习从文本和布局模态生成文档图像。据我们所知，这是文档AI领域中第一次使用一个模型同时实现高质量的神经文档编辑和内容定制。我们的方法在8个文档处理基准数据集上取得了最先进的结果。

    We propose Universal Document Processing (UDOP), a foundation Document AI model which unifies text, image, and layout modalities together with varied task formats, including document understanding and generation. UDOP leverages the spatial correlation between textual content and document image to model image, text, and layout modalities with one uniform representation. With a novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain downstream tasks into a prompt-based sequence generation scheme. UDOP is pretrained on both large-scale unlabeled document corpora using innovative self-supervised objectives and diverse labeled data. UDOP also learns to generate document images from text and layout modalities via masked image reconstruction. To the best of our knowledge, this is the first time in the field of document AI that one model simultaneously achieves high-quality neural document editing and content customization. Our method sets the state-of-the-art on 8 Docu
    
[^69]: 基于卷积和自注意力融合的敦煌壁画轮廓生成网络

    Dunhuang murals contour generation network based on convolution and self-attention fusion. (arXiv:2212.00935v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.00935](http://arxiv.org/abs/2212.00935)

    本文提出了一种基于卷积和自注意力融合的敦煌壁画轮廓生成网络，旨在解决传统卷积神经网络在感受野扩大时丢失局部细节信息的问题，以生成合理的壁画轮廓图。

    This paper proposes a Dunhuang murals contour generation network based on convolution and self-attention fusion, aiming to solve the problem of losing local detail information when the receptive field is enlarged in traditional convolutional neural networks, in order to generate reasonable mural contour drawings.

    敦煌壁画是中国风格和民族风格的集合，形成了一个独立的中国式佛教艺术。它具有非常高的历史和文化价值以及研究意义。其中，敦煌壁画的线条高度概括和表现力强，反映了角色独特的性格和复杂的内心情感。因此，壁画的轮廓图对于敦煌文化的研究具有重要意义。敦煌壁画的轮廓生成属于图像边缘检测，是计算机视觉的重要分支，旨在提取图像中显著的轮廓信息。虽然基于卷积的深度学习网络通过探索图像的上下文和语义特征在图像边缘提取方面取得了良好的结果。但是，随着感受野的扩大，一些局部细节信息会丢失。这使得它们无法生成合理的壁画轮廓图。在本文中，我们提出了一种基于卷积和自注意力融合的敦煌壁画轮廓生成网络。

    Dunhuang murals are a collection of Chinese style and national style, forming a self-contained Chinese-style Buddhist art. It has very high historical and cultural value and research significance. Among them, the lines of Dunhuang murals are highly general and expressive. It reflects the character's distinctive character and complex inner emotions. Therefore, the outline drawing of murals is of great significance to the research of Dunhuang Culture. The contour generation of Dunhuang murals belongs to image edge detection, which is an important branch of computer vision, aims to extract salient contour information in images. Although convolution-based deep learning networks have achieved good results in image edge extraction by exploring the contextual and semantic features of images. However, with the enlargement of the receptive field, some local detail information is lost. This makes it impossible for them to generate reasonable outline drawings of murals. In this paper, we propose 
    
[^70]: 从叉子到钳子：一种新的手术器械实例分割框架

    From Forks to Forceps: A New Framework for Instance Segmentation of Surgical Instruments. (arXiv:2211.16200v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.16200](http://arxiv.org/abs/2211.16200)

    本研究提出了一种新的神经网络框架，将分类模块作为现有实例分割模型的新阶段添加，用于改善手术器械实例分割中的分类问题。该模块包括多尺度掩模注意力，用于关注器械区域并掩盖分散的背景特征。

    

    微创手术和相关应用需要在实例级别上对手术工具进行分类和分割。手术工具在外观上相似，长而细，且以角度处理。将自然图像训练的最先进的实例分割模型微调用于器械分割时，往往难以区分器械类别。我们的研究表明，虽然边界框和分割掩模通常准确，但分类头误分类了手术器械的类别标签。我们提出了一个新的神经网络框架，将分类模块作为现有实例分割模型的新阶段添加。该模块专门用于改善现有模型生成的器械掩模的分类。该模块包括多尺度掩模注意力，该注意力关注器械区域并掩盖分散的背景特征。我们建议使用度量学习来训练分类器模块。

    Minimally invasive surgeries and related applications demand surgical tool classification and segmentation at the instance level. Surgical tools are similar in appearance and are long, thin, and handled at an angle. The fine-tuning of state-of-the-art (SOTA) instance segmentation models trained on natural images for instrument segmentation has difficulty discriminating instrument classes. Our research demonstrates that while the bounding box and segmentation mask are often accurate, the classification head mis-classifies the class label of the surgical instrument. We present a new neural network framework that adds a classification module as a new stage to existing instance segmentation models. This module specializes in improving the classification of instrument masks generated by the existing model. The module comprises multi-scale mask attention, which attends to the instrument region and masks the distracting background features. We propose training our classifier module using metr
    
[^71]: VideoFACT: 使用注意力、场景上下文和取证痕迹检测视频伪造

    VideoFACT: Detecting Video Forgeries Using Attention, Scene Context, and Forensic Traces. (arXiv:2211.15775v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.15775](http://arxiv.org/abs/2211.15775)

    本文提出了一种新的网络VideoFACT，它利用取证嵌入、上下文嵌入和深度自我注意机制来检测和定位各种视频伪造和操纵，克服了现有网络在分析视频时面临的挑战。

    This paper proposes a new network, VideoFACT, which utilizes forensic embeddings, context embeddings, and a deep self-attention mechanism to detect and localize a wide variety of video forgeries and manipulations, overcoming challenges faced by existing networks when analyzing videos.

    假视频代表了一个重要的误导威胁。虽然现有的取证网络已经在图像伪造方面表现出强大的性能，但最近在Adobe VideoSham数据集上报告的结果表明，这些网络无法识别视频中的虚假内容。在本文中，我们展示了这是由于视频编码引入了取证痕迹的局部变化。为了应对现有网络在分析视频时面临的挑战，我们的网络利用取证嵌入来捕捉操纵留下的痕迹，上下文嵌入来控制视频编码引入的取证痕迹的变化，以及深度自我注意机制来估计局部取证嵌入的质量和相对重要性。我们创建了几个新的视频伪造数据集，并使用这些数据集以及公开可用的数据进行实验。

    Fake videos represent an important misinformation threat. While existing forensic networks have demonstrated strong performance on image forgeries, recent results reported on the Adobe VideoSham dataset show that these networks fail to identify fake content in videos. In this paper, we show that this is due to video coding, which introduces local variation into forensic traces. In response, we propose VideoFACT - a new network that is able to detect and localize a wide variety of video forgeries and manipulations. To overcome challenges that existing networks face when analyzing videos, our network utilizes both forensic embeddings to capture traces left by manipulation, context embeddings to control for variation in forensic traces introduced by video coding, and a deep self-attention mechanism to estimate the quality and relative importance of local forensic embeddings. We create several new video forgery datasets and use these, along with publicly available data, to experimentally e
    
[^72]: HMOE: 基于超网络的专家混合模型用于领域泛化

    HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization. (arXiv:2211.08253v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08253](http://arxiv.org/abs/2211.08253)

    本文提出了一种新的领域泛化方法HMOE，它不需要领域标签，更具可解释性，使用超网络生成专家权重，能够在低维向量空间中探索专家的相似性，实验结果表明HMOE可以划分混合数据并取得更好的效果。

    This paper proposes a novel domain generalization method called HMOE, which does not rely on domain labels and is more interpretable. HMOE uses hypernetworks to generate experts' weights, which allows experts to share useful meta-knowledge and enables exploring experts' similarities in a low-dimensional vector space. Experimental results show that HMOE can divide mixed data and achieve better performance.

    由于领域转移，机器学习系统通常无法很好地推广到与训练数据不同的领域，这就是领域泛化（DG）的目的。尽管已经开发了各种各样的DG方法，但大多数缺乏可解释性，并且需要在许多实际场景中不可用的领域标签。本文提出了一种新的DG方法，称为HMOE：基于超网络的专家混合模型（MoE），它不依赖于领域标签，并且更具可解释性。MoE在识别数据中的异质模式方面证明了其有效性。对于DG问题，异质性正是由于领域转移而产生的。HMOE使用超网络将向量作为输入来生成专家权重，这使得专家可以共享有用的元知识，并能够在低维向量空间中探索专家的相似性。我们在公平和统一的基准测试-DomainBed下将HMOE与其他DG算法进行比较。我们的广泛实验表明，HMOE可以划分混合数据并取得更好的效果。

    Due to domain shift, machine learning systems typically fail to generalize well to domains different from those of training data, which is what domain generalization (DG) aims to address. Although various DG methods have been developed, most of them lack interpretability and require domain labels that are not available in many real-world scenarios. This paper presents a novel DG method, called HMOE: Hypernetwork-based Mixture of Experts (MoE), which does not rely on domain labels and is more interpretable. MoE proves effective in identifying heterogeneous patterns in data. For the DG problem, heterogeneity arises exactly from domain shift. HMOE uses hypernetworks taking vectors as input to generate experts' weights, which allows experts to share useful meta-knowledge and enables exploring experts' similarities in a low-dimensional vector space. We compare HMOE with other DG algorithms under a fair and unified benchmark-DomainBed. Our extensive experiments show that HMOE can divide mixe
    
[^73]: 学习神经符号程序以进行语言引导的机器人操作

    Learning Neuro-symbolic Programs for Language Guided Robot Manipulation. (arXiv:2211.06652v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.06652](http://arxiv.org/abs/2211.06652)

    该论文提出了一种学习神经符号程序以进行语言引导的机器人操作的方法，可以处理语言和感知变化，端到端可训练，不需要中间监督。该方法使用符号推理构造，在潜在的神经物体为中心的表示上操作，允许对输入场景进行更深入的推理。

    This paper proposes a method for learning neuro-symbolic programs for language guided robot manipulation, which can handle linguistic and perceptual variations, is end-to-end trainable, and requires no intermediate supervision. The method uses symbolic reasoning constructs that operate on a latent neural object-centric representation, allowing for deeper reasoning over the input scene.

    给定自然语言指令和输入场景，我们的目标是训练一个模型，输出一个可以由机器人执行的操作程序。先前的方法存在以下限制之一：（i）依赖手工编码的概念符号，限制了超出训练期间所见的一般化能力[1]（ii）从指令中推断出动作序列，但需要密集的子目标监督[2]或（iii）缺乏解释复杂指令所需的语义，这种语义需要更深入的以物体为中心的推理[3]。相比之下，我们的方法可以处理语言和感知变化，端到端可训练，不需要中间监督。所提出的模型使用符号推理构造，这些构造在潜在的神经物体为中心的表示上操作，允许对输入场景进行更深入的推理。我们方法的核心是一个模块化结构，包括分层指令解析器和动作模拟器，以学习解耦的行动序列。

    Given a natural language instruction and an input scene, our goal is to train a model to output a manipulation program that can be executed by the robot. Prior approaches for this task possess one of the following limitations: (i) rely on hand-coded symbols for concepts limiting generalization beyond those seen during training [1] (ii) infer action sequences from instructions but require dense sub-goal supervision [2] or (iii) lack semantics required for deeper object-centric reasoning inherent in interpreting complex instructions [3]. In contrast, our approach can handle linguistic as well as perceptual variations, end-to-end trainable and requires no intermediate supervision. The proposed model uses symbolic reasoning constructs that operate on a latent neural object-centric representation, allowing for deeper reasoning over the input scene. Central to our approach is a modular structure consisting of a hierarchical instruction parser and an action simulator to learn disentangled act
    
[^74]: 基于SPD流形的图神经网络用于运动想象分类：来自时频分析的视角

    Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis. (arXiv:2211.02641v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.02641](http://arxiv.org/abs/2211.02641)

    本文介绍了一种基于SPD流形的图神经网络用于运动想象分类，利用EEG的二阶统计量，相比传统方法具有更好的性能。

    This paper introduces a graph neural network based on SPD manifolds for motor imagery classification, which utilizes second-order statistics of EEG signals and outperforms traditional methods.

    运动想象（MI）的分类是脑电图（EEG）基础脑机接口（BCI）领域中备受追捧的研究课题，具有巨大的商业价值。过去二十年，MI-EEG分类器的趋势发生了根本性的转变，其性能逐渐提高。 Tensor-CSPNet的出现是BCI研究中第一个几何深度学习（GDL）框架的必要性，其归因于信号的非欧几里德性质的特征化。从根本上讲，Tensor-CSPNet是一种基于深度学习的分类器，利用EEG的二阶统计量。与利用EEG信号的一阶统计量的传统方法相比，利用这些二阶统计量代表了经典的处理方法。这些统计量提供了足够的区分信息，使它们适用于MI-EEG分类。在本研究中，我们介绍了另一种GDL分类器，

    The classification of motor imagery (MI) is a highly sought-after research topic in the field of Electroencephalography (EEG)-based brain-computer interfaces (BCIs), with immense commercial value. Over the past two decades, there has been a fundamental shift in the trend of MI-EEG classifiers, resulting in a gradual increase in their performance. The emergence of Tensor-CSPNet, the first geometric deep learning (GDL) framework in BCI research, is attributed to the imperative of characterizing the non-Euclidean nature of signals. Fundamentally, Tensor-CSPNet is a deep learning-based classifier that capitalizes on the second-order statistics of EEGs. In contrast to the conventional approach of utilizing first-order statistics for EEG signals, the utilization of these second-order statistics represents the classical treatment. These statistics provide adequate discriminative information, rendering them suitable for MI-EEG classification. In this study, we introduce another GDL classifier,
    
[^75]: WiserVR：语义通信支持的无线虚拟现实传输

    WiserVR: Semantic Communication Enabled Wireless Virtual Reality Delivery. (arXiv:2211.01241v4 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2211.01241](http://arxiv.org/abs/2211.01241)

    WiserVR提出了一种新的框架，利用语义通信和深度学习技术，实现了高效的无线虚拟现实传输，其中包括语义位置图和联合语义通道编码方法。

    WiserVR proposes a novel framework that utilizes semantic communication and deep learning techniques to achieve efficient wireless virtual reality delivery, including semantic location graph and joint-semantic-channel-coding method with knowledge sharing.

    无线虚拟现实被认为是下一代通信网络中的杀手级应用之一。然而，在有限的带宽资源下，巨大的数据量以及对延迟和可靠性的严格要求使得无线VR传输变得越来越具有挑战性。这些瓶颈促使本文探索使用语义通信的潜力，这是一种新的范式，可以显著缓解资源压力，以实现高效的VR传输。为此，我们提出了一种新的框架，即WiserVR（WIreless SEmantic deliveRy for VR），用于向VR用户传递连续的360度视频帧。具体而言，我们设计了基于深度学习的多个模块，用于WiserVR中的收发器，以实现高性能的特征提取和语义恢复。其中，我们专门开发了语义位置图的概念，并利用知识共享的联合语义通道编码方法。

    Virtual reality (VR) over wireless is expected to be one of the killer applications in next-generation communication networks. Nevertheless, the huge data volume along with stringent requirements on latency and reliability under limited bandwidth resources makes untethered wireless VR delivery increasingly challenging. Such bottlenecks, therefore, motivate this work to seek the potential of using semantic communication, a new paradigm that promises to significantly ease the resource pressure, for efficient VR delivery. To this end, we propose a novel framework, namely WIreless SEmantic deliveRy for VR (WiserVR), for delivering consecutive 360{\deg} video frames to VR users. Specifically, deep learning-based multiple modules are well-devised for the transceiver in WiserVR to realize high-performance feature extraction and semantic recovery. Among them, we dedicatedly develop a concept of semantic location graph and leverage the joint-semantic-channel-coding method with knowledge sharing
    
[^76]: 使用情感嵌入在情感、语言和注释格式之间传递知识

    Using Emotion Embeddings to Transfer Knowledge Between Emotions, Languages, and Annotation Formats. (arXiv:2211.00171v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.00171](http://arxiv.org/abs/2211.00171)

    本文研究了如何通过利用多语言模型和Demux来构建一个可以在不同情感、语言和注释格式之间转换的单一模型，以实现知识共享和降低训练成本。

    This paper studies how to build a single model that can transition between different emotions, languages, and annotation formats by leveraging multilingual models and Demux, to achieve knowledge sharing and reduce training costs.

    随着越来越多的学科将情感融入其理论和应用中，从文本中推断情感的需求不断多样化。这些需求包括推断不同类型的情感、处理多种语言和不同的注释格式。不同配置之间的共享模型将使知识共享和训练成本降低，并简化在新环境中部署情感识别模型的过程。在这项工作中，我们研究了如何通过利用多语言模型和Demux来构建一个可以在这些不同配置之间转换的单一模型，Demux是一个基于transformer的模型，其输入包括感兴趣的情感，使我们能够动态地改变模型预测的情感。Demux还产生情感嵌入，对它们执行操作可以通过汇集每个簇的嵌入来过渡到情感簇。我们展示了Demux可以同时传输k

    The need for emotional inference from text continues to diversify as more and more disciplines integrate emotions into their theories and applications. These needs include inferring different emotion types, handling multiple languages, and different annotation formats. A shared model between different configurations would enable the sharing of knowledge and a decrease in training costs, and would simplify the process of deploying emotion recognition models in novel environments. In this work, we study how we can build a single model that can transition between these different configurations by leveraging multilingual models and Demux, a transformer-based model whose input includes the emotions of interest, enabling us to dynamically change the emotions predicted by the model. Demux also produces emotion embeddings, and performing operations on them allows us to transition to clusters of emotions by pooling the embeddings of each cluster. We show that Demux can simultaneously transfer k
    
[^77]: 表格数据中的缺失值插补扩散模型

    Diffusion models for missing value imputation in tabular data. (arXiv:2210.17128v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17128](http://arxiv.org/abs/2210.17128)

    本文提出了一种名为“表格数据条件分数扩散模型”（TabCSDI）的扩散模型方法，用于处理表格数据中的缺失值插补，该方法同时处理分类变量和数值变量，实验结果表明其在各种数据集上都取得了优异的性能。

    This paper proposes a diffusion model approach called "Conditional Score-based Diffusion Models for Tabular data" (TabCSDI) for missing value imputation in tabular data, which effectively handles categorical variables and numerical variables simultaneously, and achieves excellent performance on various datasets.

    机器学习中的缺失值插补是使用可用信息准确估计数据集中缺失值的任务。在这个任务中，已经提出了几种深度生成建模方法，并证明了它们的有用性，例如生成对抗插补网络。最近，扩散模型因其在图像、文本、音频等生成建模任务中的有效性而受到关注。据我们所知，对于表格数据中缺失值插补的扩散模型的有效性研究还不够。基于最近对于时间序列数据插补的扩散模型的发展，我们提出了一种名为“表格数据条件分数扩散模型”（TabCSDI）的扩散模型方法。为了有效地同时处理分类变量和数值变量，我们研究了三种技术：独热编码、模拟位编码和特征标记化。实验结果表明，我们的方法在各种数据集上都取得了优异的性能。

    Missing value imputation in machine learning is the task of estimating the missing values in the dataset accurately using available information. In this task, several deep generative modeling methods have been proposed and demonstrated their usefulness, e.g., generative adversarial imputation networks. Recently, diffusion models have gained popularity because of their effectiveness in the generative modeling task in images, texts, audio, etc. To our knowledge, less attention has been paid to the investigation of the effectiveness of diffusion models for missing value imputation in tabular data. Based on recent development of diffusion models for time-series data imputation, we propose a diffusion model approach called "Conditional Score-based Diffusion Models for Tabular data" (TabCSDI). To effectively handle categorical variables and numerical variables simultaneously, we investigate three techniques: one-hot encoding, analog bits encoding, and feature tokenization. Experimental resul
    
[^78]: ViTASD：自闭症谱系障碍面部诊断的强健视觉Transformer基线

    ViTASD: Robust Vision Transformer Baselines for Autism Spectrum Disorder Facial Diagnosis. (arXiv:2210.16943v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.16943](http://arxiv.org/abs/2210.16943)

    本文提出了一种使用Vision Transformer进行儿童ASD计算分析的方法，该方法从大型面部表情数据集中提取知识，并提供模型结构可转移性。在标准ASD面部分析基准测试上进行的大量实验表明，ViTASD-L实现了新的最先进水平。

    This paper proposes a method for computational analysis of pediatric ASD using Vision Transformer, which extracts knowledge from large facial expression datasets and offers model structure transferability. Extensive experiments on standard ASD facial analysis benchmarks show that ViTASD-L achieves a new state-of-the-art.

    自闭症谱系障碍（ASD）是一种终身神经发育障碍，在全球范围内具有非常高的患病率。由于缺乏良好的基线，ASD面部分析在儿科患者中的研究进展受到了阻碍。本文提出了使用Vision Transformer（ViT）进行儿童ASD计算分析的方法。所提出的模型，称为ViTASD，从大型面部表情数据集中提取知识，并提供模型结构可转移性。具体而言，ViTASD采用普通的ViT从患者的面部图像中提取特征，并采用轻量级解码器和高斯过程层来增强ASD分析的鲁棒性。在标准ASD面部分析基准测试上进行的大量实验表明，我们的方法优于所有代表性的ASD面部分析方法，而ViTASD-L实现了新的最先进水平。我们的代码和预训练模型可在https://github.com/IrohX上获得。

    Autism spectrum disorder (ASD) is a lifelong neurodevelopmental disorder with very high prevalence around the world. Research progress in the field of ASD facial analysis in pediatric patients has been hindered due to a lack of well-established baselines. In this paper, we propose the use of the Vision Transformer (ViT) for the computational analysis of pediatric ASD. The presented model, known as ViTASD, distills knowledge from large facial expression datasets and offers model structure transferability. Specifically, ViTASD employs a vanilla ViT to extract features from patients' face images and adopts a lightweight decoder with a Gaussian Process layer to enhance the robustness for ASD analysis. Extensive experiments conducted on standard ASD facial analysis benchmarks show that our method outperforms all of the representative approaches in ASD facial analysis, while the ViTASD-L achieves a new state-of-the-art. Our code and pretrained models are available at https://github.com/IrohX
    
[^79]: 在多标签情感识别中利用标签相关性的研究：以情感为例

    Leveraging Label Correlations in a Multi-label Setting: A Case Study in Emotion. (arXiv:2210.15842v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.15842](http://arxiv.org/abs/2210.15842)

    本文研究了利用标签相关性来改善情感检测的方法，开发了两种建模方法来捕捉情感词本身的词汇关联性，并将情感表示的成对约束作为正则化项与模型的分类损失一起集成，展示了在SemEval 2018任务1 E-c中使用单语BERT模型展示了西班牙语、英语和阿拉伯语的最新性能。

    This paper investigates ways to exploit label correlations in multi-label emotion recognition models to improve emotion detection, develops two modeling approaches to capture word associations of the emotion words themselves, and integrates pairwise constraints of emotion representations as regularization terms alongside the classification loss of the models, demonstrating state-of-the-art performance across Spanish, English, and Arabic in SemEval 2018 Task 1 E-c using monolingual BERT-based models.

    在文本中检测表达的情感已经成为许多领域的关键。本文研究了利用多标签情感识别模型中的标签相关性来改善情感检测的方法。首先，我们开发了两种建模方法来捕捉情感词本身的词汇关联性，一种是将情感包含在输入中，另一种是利用遮蔽语言建模（MLM）。其次，我们将情感表示的成对约束作为正则化项与模型的分类损失一起集成。我们将这些项分为两类，局部和全局。前者根据金标签动态变化，而后者在训练期间保持不变。我们在SemEval 2018任务1 E-c中使用单语BERT模型展示了西班牙语、英语和阿拉伯语的最新性能。除了更好的性能外，我们还展示了改进的鲁棒性。

    Detecting emotions expressed in text has become critical to a range of fields. In this work, we investigate ways to exploit label correlations in multi-label emotion recognition models to improve emotion detection. First, we develop two modeling approaches to the problem in order to capture word associations of the emotion words themselves, by either including the emotions in the input, or by leveraging Masked Language Modeling (MLM). Second, we integrate pairwise constraints of emotion representations as regularization terms alongside the classification loss of the models. We split these terms into two categories, local and global. The former dynamically change based on the gold labels, while the latter remain static during training. We demonstrate state-of-the-art performance across Spanish, English, and Arabic in SemEval 2018 Task 1 E-c using monolingual BERT-based models. On top of better performance, we also demonstrate improved robustness. Code is available at https://github.com/
    
[^80]: Articulation GAN: 无监督建模关节学习

    Articulation GAN: Unsupervised modeling of articulatory learning. (arXiv:2210.15173v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2210.15173](http://arxiv.org/abs/2210.15173)

    本文提出了一种新的无监督生成模型，通过完全无监督的方式学习生成关节表示（电磁关节成像或EMA），更接近于人类语音产生的方式，从而更好地模拟人类语音产生的过程。

    This paper proposes a new unsupervised generative model that learns to generate articulatory representations (electromagnetic articulography or EMA) in a fully unsupervised manner, which more closely mimics human speech production and better simulates the process of human speech production.

    生成式深度神经网络广泛用于语音合成，但大多数现有模型直接生成波形或频谱输出。然而，人类通过控制关节来产生语音，这通过声音传播的物理特性导致语音声音的产生。我们引入了关节生成器到生成对抗网络范例中，这是一种新的无监督生成模型，用于语音产生/合成。关节生成器通过完全无监督的方式学习生成关节表示（电磁关节成像或EMA），更接近于人类语音产生的方式。然后，一个单独的预训练物理模型（ema2wav）将生成的EMA表示转换为语音波形，这些波形被发送到鉴别器进行评估。关节分析表明，网络学习控制关节的方式类似于人类在语音产生过程中的方式。输出的声学分析表明...

    Generative deep neural networks are widely used for speech synthesis, but most existing models directly generate waveforms or spectral outputs. Humans, however, produce speech by controlling articulators, which results in the production of speech sounds through physical properties of sound propagation. We introduce the Articulatory Generator to the Generative Adversarial Network paradigm, a new unsupervised generative model of speech production/synthesis. The Articulatory Generator more closely mimics human speech production by learning to generate articulatory representations (electromagnetic articulography or EMA) in a fully unsupervised manner. A separate pre-trained physical model (ema2wav) then transforms the generated EMA representations to speech waveforms, which get sent to the Discriminator for evaluation. Articulatory analysis suggests that the network learns to control articulators in a similar manner to humans during speech production. Acoustic analysis of the outputs sugge
    
[^81]: 破碎的神经缩放定律

    Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14891](http://arxiv.org/abs/2210.14891)

    本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.

    我们提出了一个平滑破碎的幂律函数形式（我们称之为破碎的神经缩放定律（BNSL）），它准确地模拟和外推了深度神经网络的缩放行为（即感兴趣的评估指标随用于训练的计算量、模型参数数量、训练数据集大小或上游性能变化而变化）对于各种架构和大量不同任务中的每个任务，包括大规模视觉、语言、音频、视频、扩散、生成建模、多模态学习、对比学习、AI对齐、机器人、超出分布（OOD）泛化、持续学习、不确定性估计/校准、超出分布检测、对抗鲁棒性、蒸馏、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
    
[^82]: 一种用于检测混合垃圾邮件的多模态融合模型

    A Late Multi-Modal Fusion Model for Detecting Hybrid Spam E-mail. (arXiv:2210.14616v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.14616](http://arxiv.org/abs/2210.14616)

    本研究旨在设计一种有效的方法来过滤混合垃圾邮件，提出了一种多模态融合模型，解决了传统基于文本或基于图像的过滤器无法检测到混合垃圾邮件的问题。

    This study aims to design an effective approach filtering out hybrid spam e-mails and proposes a late multi-modal fusion model to solve the problem of traditional text-based or image-based filters failing to detect hybrid spam e-mails.

    近年来，垃圾邮件发送者开始通过引入图像和文本部分的混合垃圾邮件来混淆其意图，这比仅包含文本或图像的电子邮件更具挑战性。本研究的动机是设计一种有效的方法来过滤混合垃圾邮件，以避免传统的基于文本或基于图像的过滤器无法检测到混合垃圾邮件的情况。据我们所知，目前只有少数研究旨在检测混合垃圾邮件。通常，光学字符识别（OCR）技术用于通过将图像转换为文本来消除垃圾邮件的图像部分。然而，研究问题是，尽管OCR扫描是处理文本和图像混合垃圾邮件的非常成功的技术，但由于所需的CPU功率和扫描电子邮件文件所需的执行时间，它不是处理大量垃圾邮件的有效解决方案。

    In recent years, spammers are now trying to obfuscate their intents by introducing hybrid spam e-mail combining both image and text parts, which is more challenging to detect in comparison to e-mails containing text or image only. The motivation behind this research is to design an effective approach filtering out hybrid spam e-mails to avoid situations where traditional text-based or image-baesd only filters fail to detect hybrid spam e-mails. To the best of our knowledge, a few studies have been conducted with the goal of detecting hybrid spam e-mails. Ordinarily, Optical Character Recognition (OCR) technology is used to eliminate the image parts of spam by transforming images into text. However, the research questions are that although OCR scanning is a very successful technique in processing text-and-image hybrid spam, it is not an effective solution for dealing with huge quantities due to the CPU power required and the execution time it takes to scan e-mail files. And the OCR tech
    
[^83]: D-Shape: 通过目标条件化实现演示形状的强化学习

    D-Shape: Demonstration-Shaped Reinforcement Learning via Goal Conditioning. (arXiv:2210.14428v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14428](http://arxiv.org/abs/2210.14428)

    D-Shape是一种新的结合IL和RL的方法，它使用奖励塑形和目标条件化RL的思想来解决次优演示与回报最大化目标之间的冲突，能够在稀疏奖励网格世界领域中提高样本效率并一致地收敛到最优策略。

    D-Shape is a new method that combines imitation learning (IL) and reinforcement learning (RL) using reward shaping and goal-conditioned RL to resolve the conflict between suboptimal demonstrations and return-maximization objective of RL. It improves sample efficiency and consistently converges to the optimal policy in sparse-reward gridworld domains.

    将模仿学习（IL）和强化学习（RL）相结合是解决自主行为获取中样本效率低下的一种有前途的方法，但这样做的方法通常假定所需的行为演示由专家提供，该专家相对于任务奖励表现最佳。然而，如果提供的演示是次优的，则面临一个基本挑战，即IL的演示匹配目标与RL的回报最大化目标冲突。本文介绍了D-Shape，一种新的结合IL和RL的方法，它使用奖励塑形和目标条件化RL的思想来解决上述冲突。D-Shape允许从次优演示中学习，同时保留了找到相对于任务奖励的最优策略的能力。我们在稀疏奖励网格世界领域实验验证了D-Shape，结果表明它在样本效率方面优于RL，并且能够一致地收敛到最优策略。

    While combining imitation learning (IL) and reinforcement learning (RL) is a promising way to address poor sample efficiency in autonomous behavior acquisition, methods that do so typically assume that the requisite behavior demonstrations are provided by an expert that behaves optimally with respect to a task reward. If, however, suboptimal demonstrations are provided, a fundamental challenge appears in that the demonstration-matching objective of IL conflicts with the return-maximization objective of RL. This paper introduces D-Shape, a new method for combining IL and RL that uses ideas from reward shaping and goal-conditioned RL to resolve the above conflict. D-Shape allows learning from suboptimal demonstrations while retaining the ability to find the optimal policy with respect to the task reward. We experimentally validate D-Shape in sparse-reward gridworld domains, showing that it both improves over RL in terms of sample efficiency and converges consistently to the optimal polic
    
[^84]: 带反馈的策略引导懒惰搜索用于任务和动作规划

    Policy-Guided Lazy Search with Feedback for Task and Motion Planning. (arXiv:2210.14055v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.14055](http://arxiv.org/abs/2210.14055)

    本文提出了一种用于PDDLStream问题的求解器LAZY，它在动作骨架上维护单个集成搜索，随着在运动规划期间懒惰地绘制可能运动的样本，逐渐变得更具几何信息。同时，学习模型的目标导向策略和当前运动采样数据合并到LAZY中，以自适应地引导任务规划器，这导致了在不同数量的对象、目标和初始条件的未见测试环境中评估可行解搜索的显着加速。

    This paper proposes a solver LAZY for PDDLStream problems, which maintains a single integrated search over action skeletons, gradually becoming more geometrically informed as samples of possible motions are lazily drawn during motion planning. Meanwhile, learned models of goal-directed policies and current motion sampling data are incorporated in LAZY to adaptively guide the task planner, leading to significant speed-ups in the search for a feasible solution evaluated over unseen test environments of varying numbers of objects, goals, and initial conditions.

    PDDLStream求解器最近已经成为任务和动作规划（TAMP）问题的可行解决方案，将PDDL扩展到具有连续动作空间的问题。先前的工作已经展示了如何将PDDLStream问题简化为一系列PDDL规划问题，然后使用现成的规划器解决。然而，这种方法可能会导致长时间运行。在本文中，我们提出了LAZY，一种用于PDDLStream问题的求解器，它在动作骨架上维护单个集成搜索，随着在运动规划期间懒惰地绘制可能运动的样本，逐渐变得更具几何信息。我们探讨了如何将目标导向策略的学习模型和当前运动采样数据合并到LAZY中，以自适应地引导任务规划器。我们展示了这导致了在不同数量的对象、目标和初始条件的未见测试环境中评估可行解搜索的显着加速。我们评估了我们的TAMP方法

    PDDLStream solvers have recently emerged as viable solutions for Task and Motion Planning (TAMP) problems, extending PDDL to problems with continuous action spaces. Prior work has shown how PDDLStream problems can be reduced to a sequence of PDDL planning problems, which can then be solved using off-the-shelf planners. However, this approach can suffer from long runtimes. In this paper we propose LAZY, a solver for PDDLStream problems that maintains a single integrated search over action skeletons, which gets progressively more geometrically informed, as samples of possible motions are lazily drawn during motion planning. We explore how learned models of goal-directed policies and current motion sampling data can be incorporated in LAZY to adaptively guide the task planner. We show that this leads to significant speed-ups in the search for a feasible solution evaluated over unseen test environments of varying numbers of objects, goals, and initial conditions. We evaluate our TAMP appro
    
[^85]: 投影增强：一种有效且高效的蒸馏数据增强范式

    Augmentation with Projection: Towards an Effective and Efficient Data Augmentation Paradigm for Distillation. (arXiv:2210.11768v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.11768](http://arxiv.org/abs/2210.11768)

    提出了一种名为AugPro的数据增强方法，它是一种有效且高效的蒸馏数据增强方法，可以避免偏移决策边界，计算开销小。

    AugPro is an effective and efficient data augmentation method for distillation, which avoids shifting decision boundaries and has little computational overhead.

    知识蒸馏是从大型模型向小型模型转移知识的主要方法之一。然而，它需要大量的任务特定数据，在许多实际应用中可能不可行。为了解决这个问题，采用了表示插值、标记替换或模型增强等数据增强方法。然而，这些数据增强方法可能会导致决策边界的偏移（表示插值），不够表达（标记替换）或引入过多的计算开销（模型增强）。为此，我们提出了AugPro（投影增强），一种有效且高效的蒸馏数据增强方法。我们的方法建立在表示插值增强方法之上，以保持表达的多样性，并将增强的数据转换为标记以避免偏移决策边界。它使用简单的操作，计算开销小。

    Knowledge distillation is one of the primary methods of transferring knowledge from large to small models. However, it requires massive task-specific data, which may not be plausible in many real-world applications. Data augmentation methods such as representation interpolation, token replacement, or augmentation with models are applied to tackle this problem. However, these data augmentation methods either potentially cause shifts in decision boundaries (representation interpolation), are not expressive enough (token replacement), or introduce too much computational overhead (augmentation with models). To this end, we propose AugPro (Augmentation with Projection), an effective and efficient data augmentation method for distillation. Our method builds on top of representation interpolation augmentation methods to maintain the diversity of expressions and converts the augmented data to tokens to avoid shifting decision boundaries. It uses simple operations that come with little computat
    
[^86]: 用贝叶斯优化发现多样的解决方案

    Discovering Many Diverse Solutions with Bayesian Optimization. (arXiv:2210.10953v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10953](http://arxiv.org/abs/2210.10953)

    ROBOT是一种新的贝叶斯优化方法，可以找到一组高性能、多样化的解决方案，解决了传统单目标贝叶斯优化方法只能找到一个最佳解决方案的局限性。

    ROBOT is a new Bayesian optimization method that can find a portfolio of high-performing diverse solutions, addressing the limitation of traditional single-objective Bayesian optimization methods that only seek to find a single best solution.

    贝叶斯优化是一种用于黑盒目标函数的高效优化的流行方法。传统的单目标贝叶斯优化方法只寻求找到一个最佳解决方案，这在解决方案后期可能变得棘手的情况下会有很大的局限性。为了解决这个问题，我们提出了一种名为ROBOT的排序贝叶斯优化方法，旨在找到一组高性能、多样化的解决方案，这些解决方案根据用户指定的多样性度量进行排序。我们在几个真实世界的应用中评估了ROBOT，并展示了它可以发现大量高性能的多样化解决方案，同时与寻找单个最佳解决方案相比，需要很少的额外函数评估。

    Bayesian optimization (BO) is a popular approach for sample-efficient optimization of black-box objective functions. While BO has been successfully applied to a wide range of scientific applications, traditional approaches to single-objective BO only seek to find a single best solution. This can be a significant limitation in situations where solutions may later turn out to be intractable. For example, a designed molecule may turn out to violate constraints that can only be reasonably evaluated after the optimization process has concluded. To address this issue, we propose Rank-Ordered Bayesian Optimization with Trust-regions (ROBOT) which aims to find a portfolio of high-performing solutions that are diverse according to a user-specified diversity metric. We evaluate ROBOT on several real-world applications and show that it can discover large sets of high-performing diverse solutions while requiring few additional function evaluations compared to finding a single best solution.
    
[^87]: 可微分的动力学增强神经对象的物理模拟

    Differentiable Physics Simulation of Dynamics-Augmented Neural Objects. (arXiv:2210.09420v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.09420](http://arxiv.org/abs/2210.09420)

    本文提出了一种可微分的流程，用于模拟将物体的几何形状表示为连续密度场的对象的运动，从密度场中估计物体的动力学特性，并引入了一种基于密度场的可微接触模型，使机器人能够自主构建物体模型并优化神经对象的抓取和操作轨迹。

    This paper proposes a differentiable pipeline for simulating the motion of objects that represent their geometry as a continuous density field parameterized as a deep network, estimates the dynamical properties of the object from the density field, introduces a differentiable contact model based on the density field for computing normal and friction forces resulting from collisions, and allows a robot to autonomously build object models and optimize grasps and manipulation trajectories of neural objects.

    我们提出了一种可微分的流程，用于模拟将物体的几何形状表示为连续密度场的对象的运动，该密度场被参数化为深度网络。这包括神经辐射场（NeRF）和其他相关模型。从密度场中，我们估计物体的动力学特性，包括其质量、质心和惯性矩阵。然后，我们引入了一种基于密度场的可微接触模型，用于计算由碰撞产生的法向和摩擦力。这使得机器人能够自主构建物体模型，这些模型从静止图像和运动中的物体视频中视觉上和动态上都是准确的。由此产生的动力学增强神经对象（DANOs）使用现有的可微分模拟引擎Dojo进行模拟，与其他标准模拟对象（如球体、平面和以URDF指定的机器人）进行交互。机器人可以使用这个模拟来优化神经对象的抓取和操作轨迹。

    We present a differentiable pipeline for simulating the motion of objects that represent their geometry as a continuous density field parameterized as a deep network. This includes Neural Radiance Fields (NeRFs), and other related models. From the density field, we estimate the dynamical properties of the object, including its mass, center of mass, and inertia matrix. We then introduce a differentiable contact model based on the density field for computing normal and friction forces resulting from collisions. This allows a robot to autonomously build object models that are visually and \emph{dynamically} accurate from still images and videos of objects in motion. The resulting Dynamics-Augmented Neural Objects (DANOs) are simulated with an existing differentiable simulation engine, Dojo, interacting with other standard simulation objects, such as spheres, planes, and robots specified as URDFs. A robot can use this simulation to optimize grasps and manipulation trajectories of neural ob
    
[^88]: 关系注意力：将Transformer推广到图结构任务

    Relational Attention: Generalizing Transformers for Graph-Structured Tasks. (arXiv:2210.05062v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05062](http://arxiv.org/abs/2210.05062)

    本文提出了一种关系注意力机制，将Transformer推广到图结构任务中，通过考虑和更新边向量，实现了对图结构数据的推理，相比于专门设计用于推理图结构数据的最先进的图神经网络，在各种图结构任务上都有明显的优势。

    This paper proposes a relational attention mechanism that generalizes Transformers for graph-structured tasks by considering and updating edge vectors in each Transformer layer, achieving reasoning over graph-structured data. Compared to state-of-the-art graph neural networks designed for reasoning over graph-structured data, it has significant advantages in various graph-structured tasks.

    Transformer可以灵活地操作表示任务特定实体及其属性的实值向量集，其中每个向量可能编码一个单词片段令牌及其在序列中的位置，或者一些不带位置的信息。但作为集合处理器，Transformer在推理更一般的图结构数据（其中节点表示实体，边表示实体之间的关系）方面处于劣势。为了解决这个缺点，我们将Transformer注意力推广到每个Transformer层中考虑和更新边向量。我们在各种图结构任务上评估了这个关系Transformer，包括大型且具有挑战性的CLRS算法推理基准测试。在那里，它明显优于专门设计用于推理图结构数据的最先进的图神经网络。我们的分析表明，这些收益归因于关系注意力固有的能力，即利用大量的边信息来推理。

    Transformers flexibly operate over sets of real-valued vectors representing task-specific entities and their attributes, where each vector might encode one word-piece token and its position in a sequence, or some piece of information that carries no position at all. But as set processors, transformers are at a disadvantage in reasoning over more general graph-structured data where nodes represent entities and edges represent relations between entities. To address this shortcoming, we generalize transformer attention to consider and update edge vectors in each transformer layer. We evaluate this relational transformer on a diverse array of graph-structured tasks, including the large and challenging CLRS Algorithmic Reasoning Benchmark. There, it dramatically outperforms state-of-the-art graph neural networks expressly designed to reason over graph-structured data. Our analysis demonstrates that these gains are attributable to relational attention's inherent ability to leverage the great
    
[^89]: 无监督模型选择用于时间序列异常检测

    Unsupervised Model Selection for Time-series Anomaly Detection. (arXiv:2210.01078v3 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2210.01078](http://arxiv.org/abs/2210.01078)

    本文提出了一种无监督模型选择方法，用于时间序列异常检测，通过三类替代度量，即预测误差、模型中心性和注入合成异常的性能，选择最准确的模型。

    This paper proposes an unsupervised model selection method for time-series anomaly detection, which selects the most accurate model through three classes of surrogate metrics, namely, prediction error, model centrality, and performance on injected synthetic anomalies.

    时间序列中的异常检测具有广泛的实际应用。虽然文献中提出了许多异常检测方法，但最近的一项调查得出结论，没有一种方法在各种数据集上都最准确。更糟糕的是，异常标签在实践中很少可用。在文献中，如何在没有标签的情况下选择给定数据集的最准确模型的实际问题得到了很少的关注。本文回答了这个问题，即给定一个未标记的数据集和一组候选异常检测器，如何选择最准确的模型？为此，我们确定了三类替代（无监督）度量，即预测误差、模型中心性和注入合成异常的性能，并表明一些度量与标准监督异常检测性能度量（如$F_1$分数）高度相关，但程度不同。我们制定了多个度量组合的度量组合方法。

    Anomaly detection in time-series has a wide range of practical applications. While numerous anomaly detection methods have been proposed in the literature, a recent survey concluded that no single method is the most accurate across various datasets. To make matters worse, anomaly labels are scarce and rarely available in practice. The practical problem of selecting the most accurate model for a given dataset without labels has received little attention in the literature. This paper answers this question i.e. Given an unlabeled dataset and a set of candidate anomaly detectors, how can we select the most accurate model? To this end, we identify three classes of surrogate (unsupervised) metrics, namely, prediction error, model centrality, and performance on injected synthetic anomalies, and show that some metrics are highly correlated with standard supervised anomaly detection performance metrics such as the $F_1$ score, but to varying degrees. We formulate metric combination with multipl
    
[^90]: 从自然语言知识中学习可转移的时空表示

    Learning Transferable Spatiotemporal Representations from Natural Script Knowledge. (arXiv:2209.15280v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.15280](http://arxiv.org/abs/2209.15280)

    本文提出了一种新的预文本任务，利用自然语言知识来提高可转移的时空表示学习，通过关注学习到的视频表示来对打乱的ASR脚本进行排序，从而提高视频理解的进展。

    This paper proposes a new pretext task to boost transferable spatiotemporal representation learning by exploiting natural language knowledge, which sorts shuffled ASR scripts by attending to learned video representations, and thus improves the progress of video understanding.

    近年来，预训练大规模视频数据已成为学习可转移的时空表示的常见方法。尽管取得了一些进展，但现有方法大多局限于高度策划的数据集（例如K400），并展示出令人不满意的开箱即用表示。我们认为这是因为它们只捕捉像素级别的知识而不是时空语义，这阻碍了视频理解的进一步进展。受到图像文本预训练（例如CLIP）的巨大成功的启发，我们迈出了利用语言语义提高可转移的时空表示学习的第一步。我们引入了一个新的预文本任务，即“Turning to Video for Transcript Sorting（TVTS）”，通过关注学习到的视频表示来对打乱的ASR脚本进行排序。我们不依赖于描述性标题，纯粹从视频中学习，即利用自然转录的语音知识在时间上提供嘈杂但有用的语义。

    Pre-training on large-scale video data has become a common recipe for learning transferable spatiotemporal representations in recent years. Despite some progress, existing methods are mostly limited to highly curated datasets (e.g., K400) and exhibit unsatisfactory out-of-the-box representations. We argue that it is due to the fact that they only capture pixel-level knowledge rather than spatiotemporal semantics, which hinders further progress in video understanding. Inspired by the great success of image-text pre-training (e.g., CLIP), we take the first step to exploit language semantics to boost transferable spatiotemporal representation learning. We introduce a new pretext task, Turning to Video for Transcript Sorting (TVTS), which sorts shuffled ASR scripts by attending to learned video representations. We do not rely on descriptive captions and learn purely from video, i.e., leveraging the natural transcribed speech knowledge to provide noisy but useful semantics over time. Our me
    
[^91]: 学习稀疏图形均场博弈

    Learning Sparse Graphon Mean Field Games. (arXiv:2209.03880v3 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2209.03880](http://arxiv.org/abs/2209.03880)

    本文提出了一种新型的GMFG公式，称为LPGMFG，它利用$L^p$图形的图形理论概念，提供了一种机器学习工具，以有效且准确地近似解决稀疏网络问题，特别是幂律网络。我们推导出理论存在和收敛保证，并给出了实证例子，证明了我们的学习方法的准确性。

    This paper proposes a novel formulation of GMFGs, called LPGMFG, which leverages the graph theoretical concept of $L^p$ graphons and provides a machine learning tool to efficiently and accurately approximate solutions for sparse network problems, especially power law networks. The paper derives theoretical existence and convergence guarantees and gives empirical examples that demonstrate the accuracy of the learning method.

    尽管多智能体强化学习（MARL）领域在过去几年中取得了相当大的进展，但解决具有大量代理的系统仍然是一个难题。图形均场博弈（GMFG）使得可以对否则难以处理的MARL问题进行可扩展的分析。由于图形的数学结构，这种方法仅限于描述许多现实世界网络（如幂律图）的稠密图形，这是不足的。我们的论文介绍了GMFG的新型公式，称为LPGMFG，它利用$L^p$图形的图形理论概念，并提供了一种机器学习工具，以有效且准确地近似解决稀疏网络问题。这尤其包括在各种应用领域中经验观察到的幂律网络，这些网络无法被标准图形所捕捉。我们推导出理论存在和收敛保证，并给出了实证例子，证明了我们的学习方法的准确性。

    Although the field of multi-agent reinforcement learning (MARL) has made considerable progress in the last years, solving systems with a large number of agents remains a hard challenge. Graphon mean field games (GMFGs) enable the scalable analysis of MARL problems that are otherwise intractable. By the mathematical structure of graphons, this approach is limited to dense graphs which are insufficient to describe many real-world networks such as power law graphs. Our paper introduces a novel formulation of GMFGs, called LPGMFGs, which leverages the graph theoretical concept of $L^p$ graphons and provides a machine learning tool to efficiently and accurately approximate solutions for sparse network problems. This especially includes power law networks which are empirically observed in various application areas and cannot be captured by standard graphons. We derive theoretical existence and convergence guarantees and give empirical examples that demonstrate the accuracy of our learning ap
    
[^92]: 使用深度学习预测肺活检图像中的EGFR突变

    EGFR Mutation Prediction of Lung Biopsy Images using Deep Learning. (arXiv:2208.12506v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.12506](http://arxiv.org/abs/2208.12506)

    本文使用深度学习技术，通过对肺活检图像进行分析，实现了对EGFR突变的预测，为肺癌治疗提供了更经济、更快捷的诊断方法。

    This paper uses deep learning technology to predict EGFR mutations through analysis of lung biopsy images, providing a more economical and faster diagnostic method for lung cancer treatment.

    肺癌治疗的标准诊断程序涉及组织学亚型分型和随后的关键驱动基因突变检测，如EGFR。尽管分子分析可以揭示驱动基因突变，但该过程通常昂贵且耗时。基于深度学习的图像分析提供了一种更经济的选择，可以直接从全幻灯片图像（WSIs）中发现驱动基因突变。在这项工作中，我们使用定制的深度学习管道进行弱监督，以识别hematoxylin和eosin染色的WSIs中EGFR突变的形态学相关性，以及检测肿瘤和组织学亚型。我们通过在两个肺癌数据集（TCGA和来自印度的私人数据集）上进行严格的实验和消融研究来证明我们管道的有效性。使用我们的管道，我们实现了肿瘤检测的平均曲线下面积（AUC）为0.964，组织学亚型为0.942。

    The standard diagnostic procedures for targeted therapies in lung cancer treatment involve histological subtyping and subsequent detection of key driver mutations, such as EGFR. Even though molecular profiling can uncover the driver mutation, the process is often expensive and time-consuming. Deep learning-oriented image analysis offers a more economical alternative for discovering driver mutations directly from whole slide images (WSIs). In this work, we used customized deep learning pipelines with weak supervision to identify the morphological correlates of EGFR mutation from hematoxylin and eosin-stained WSIs, in addition to detecting tumor and histologically subtyping it. We demonstrate the effectiveness of our pipeline by conducting rigorous experiments and ablation studies on two lung cancer datasets - TCGA and a private dataset from India. With our pipeline, we achieved an average area under the curve (AUC) of 0.964 for tumor detection, and 0.942 for histological subtyping betwe
    
[^93]: 具有多个边缘成本估计的图的最短路径问题的推广

    A Generalization of the Shortest Path Problem to Graphs with Multiple Edge-Cost Estimates. (arXiv:2208.11489v3 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2208.11489](http://arxiv.org/abs/2208.11489)

    本文提出了一个广义的加权有向图框架，其中可以多次计算（估计）边缘权重，以提高准确性和运行时间成本，解决了最短路径问题的不确定性。

    This paper presents a generalized framework for weighted directed graphs, where edge weight can be computed (estimated) multiple times, at increasing accuracy and run-time expense, solving the uncertainty of the shortest path problem.

    图中的最短路径问题是AI理论和应用的基石。现有算法通常忽略边缘权重计算时间。在本文中，我们提出了一个广义的加权有向图框架，其中可以多次计算（估计）边缘权重，以提高准确性和运行时间成本。这引发了一个广义的最短路径问题，优化路径成本及其不确定性的不同方面。我们提出了一个完整的任何时候解决方案算法，实证证明了其功效。

    The shortest path problem in graphs is a cornerstone of AI theory and applications. Existing algorithms generally ignore edge weight computation time. In this paper we present a generalized framework for weighted directed graphs, where edge weight can be computed (estimated) multiple times, at increasing accuracy and run-time expense. This raises a generalized shortest path problem that optimize different aspects of path cost and its uncertainty. We present a complete anytime solution algorithm for the generalized problem, and empirically demonstrate its efficacy.
    
[^94]: 局部稀疏不完整多视图聚类

    Localized Sparse Incomplete Multi-view Clustering. (arXiv:2208.02998v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.02998](http://arxiv.org/abs/2208.02998)

    本文提出了一种名为局部稀疏不完整多视图聚类（LSIMVC）的方法，通过优化稀疏正则化和新颖的图嵌入多视图矩阵分解模型，从不完整的多视图数据中学习稀疏和结构化的共识潜在表示，以解决不完整多视图聚类中的问题。

    This paper proposes a method named Localized Sparse Incomplete Multi-view Clustering (LSIMVC) to learn a sparse and structured consensus latent representation from incomplete multi-view data by optimizing a sparse regularized and novel graph embedded multi-view matrix factorization model, which aims to solve the clustering problem on incomplete multi-view data with partial view missing.

    近年来，不完整多视图聚类在解决部分视图缺失的不完整多视图数据聚类问题方面受到越来越多的关注。虽然已经开发了许多方法，但大多数方法要么不能灵活处理具有任意缺失视图的不完整多视图数据，要么不考虑视图之间信息不平衡的负面因素。此外，一些方法没有充分探索所有不完整视图的局部结构。为了解决这些问题，本文提出了一种简单而有效的方法，称为局部稀疏不完整多视图聚类（LSIMVC）。与现有方法不同，LSIMVC旨在通过优化稀疏正则化和新颖的图嵌入多视图矩阵分解模型来从不完整的多视图数据中学习稀疏和结构化的共识潜在表示。具体而言，在基于矩阵分解的新颖模型中，基于l1范数的稀疏正则化被用于学习局部结构，以及新颖的图嵌入被用于处理视图之间的信息不平衡。

    Incomplete multi-view clustering, which aims to solve the clustering problem on the incomplete multi-view data with partial view missing, has received more and more attention in recent years. Although numerous methods have been developed, most of the methods either cannot flexibly handle the incomplete multi-view data with arbitrary missing views or do not consider the negative factor of information imbalance among views. Moreover, some methods do not fully explore the local structure of all incomplete views. To tackle these problems, this paper proposes a simple but effective method, named localized sparse incomplete multi-view clustering (LSIMVC). Different from the existing methods, LSIMVC intends to learn a sparse and structured consensus latent representation from the incomplete multi-view data by optimizing a sparse regularized and novel graph embedded multi-view matrix factorization model. Specifically, in such a novel model based on the matrix factorization, a l1 norm based spa
    
[^95]: 基于联想记忆模型的真实世界数据分类和生成

    Classification and Generation of real-world data with an Associative Memory Model. (arXiv:2207.04827v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2207.04827](http://arxiv.org/abs/2207.04827)

    本文提出了一种基于联想记忆模型的多模态框架，可以以容错的方式存储和检索大量真实世界数据，并且可以用于推断缺失的模态。

    This paper proposes a multi-modality framework based on the associative memory model, which can store and retrieve a large amount of real-world data in a fault-tolerant manner, and can be used to infer missing modalities.

    回忆起多年未见的朋友的面孔是一项困难的任务。然而，如果你们偶然相遇，你们会轻易地认出彼此。生物记忆配备了一个令人印象深刻的压缩算法，可以存储必要的信息，然后推断细节以匹配感知。Willshaw Memory是一种用于皮层计算的简单抽象模型，实现了生物记忆的机制。使用我们最近提出的用于视觉模式的稀疏编码规则[34]，该模型可以以容错的方式存储和检索大量真实世界数据。在本文中，我们通过使用多模态框架扩展了基本联想记忆模型的能力。在这种设置中，记忆同时存储每个模式的几种模态（例如，视觉或文本）。训练后，当只感知到子集时，记忆可以用于推断缺失的模态。使用简单的编码器-记忆解码器，我们可以生成具有多个模态的数据。

    Drawing from memory the face of a friend you have not seen in years is a difficult task. However, if you happen to cross paths, you would easily recognize each other. The biological memory is equipped with an impressive compression algorithm that can store the essential, and then infer the details to match perception. The Willshaw Memory is a simple abstract model for cortical computations which implements mechanisms of biological memories. Using our recently proposed sparse coding prescription for visual patterns [34], this model can store and retrieve an impressive amount of real-world data in a fault-tolerant manner. In this paper, we extend the capabilities of the basic Associative Memory Model by using a Multiple-Modality framework. In this setting, the memory stores several modalities (e.g., visual, or textual) of each pattern simultaneously. After training, the memory can be used to infer missing modalities when just a subset is perceived. Using a simple encoder-memory decoder a
    
[^96]: $L_2$BN: 通过等化特征的$L_2$范数来增强批量归一化

    $L_2$BN: Enhancing Batch Normalization by Equalizing the $L_2$ Norms of Features. (arXiv:2207.02625v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.02625](http://arxiv.org/abs/2207.02625)

    本文提出了一种$L_2$BN方法，通过等化样本特征的$L_2$范数来增强批量归一化，可以增强内类别特征的紧凑性并扩大跨类别特征的差异，易于实现，可以用作神经网络的基本归一化方法。

    This paper proposes an $L_2$BN method to enhance batch normalization by equalizing the $l_2$ norms of sample features, which can strengthen the compactness of intra-class features and enlarge the discrepancy of inter-class features, easy to implement, and can be used as a basic normalization method for neural networks.

    本文表明，样本特征的$L_2$范数差异会妨碍批量归一化获得更加显著的跨类别特征和更加紧凑的内类别特征。为了解决这个问题，我们提出了一种直观但有效的方法来等化样本特征的$L_2$范数。具体来说，我们在将样本特征输入批量归一化之前对每个样本特征进行$L_2$归一化，因此特征具有相同的数量级。由于所提出的方法结合了$L_2$归一化和批量归一化，因此我们将其命名为$L_2$BN。$L_2$BN可以增强内类别特征的紧凑性并扩大跨类别特征的差异。$L_2$BN易于实现，可以在没有任何额外参数或超参数的情况下发挥其作用。因此，它可以用作神经网络的基本归一化方法。我们通过对各种模型进行广泛的实验评估了$L_2$BN的有效性，用于图像分类。

    In this paper, we show that the difference in $l_2$ norms of sample features can hinder batch normalization from obtaining more distinguished inter-class features and more compact intra-class features. To address this issue, we propose an intuitive but effective method to equalize the $l_2$ norms of sample features. Concretely, we $l_2$-normalize each sample feature before feeding them into batch normalization, and therefore the features are of the same magnitude. Since the proposed method combines the $l_2$ normalization and batch normalization, we name our method $L_2$BN. The $L_2$BN can strengthen the compactness of intra-class features and enlarge the discrepancy of inter-class features. The $L_2$BN is easy to implement and can exert its effect without any additional parameters or hyper-parameters. Therefore, it can be used as a basic normalization method for neural networks. We evaluate the effectiveness of $L_2$BN through extensive experiments with various models on image classif
    
[^97]: 一份关于同行评审中恶意投标的数据集

    A Dataset on Malicious Paper Bidding in Peer Review. (arXiv:2207.02303v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2207.02303](http://arxiv.org/abs/2207.02303)

    本文提供了一份关于同行评审中恶意投标的数据集，填补了这一领域缺乏公开数据的空白。

    This paper provides a dataset on malicious paper bidding in peer review, filling the gap of lack of publicly-available data in this field.

    在会议同行评审中，评审人通常被要求对每篇提交的论文提供“投标”，以表达他们对审查该论文的兴趣。然后，一种论文分配算法使用这些投标（以及其他数据）来计算评审人对论文的高质量分配。然而，这个过程已经被恶意评审人利用，他们会有策略地投标，以非道德的方式操纵论文分配，从而严重破坏同行评审过程。例如，这些评审人可能会试图被分配到朋友的论文中，作为一种交换条件。解决这个问题的一个关键障碍是缺乏任何公开可用的关于恶意投标的数据。在这项工作中，我们收集并公开发布了一个新的数据集，以填补这一空白，该数据集是从一个模拟会议活动中收集的，参与者被要求诚实或恶意地投标。我们进一步提供了对投标行为的描述性分析。

    In conference peer review, reviewers are often asked to provide "bids" on each submitted paper that express their interest in reviewing that paper. A paper assignment algorithm then uses these bids (along with other data) to compute a high-quality assignment of reviewers to papers. However, this process has been exploited by malicious reviewers who strategically bid in order to unethically manipulate the paper assignment, crucially undermining the peer review process. For example, these reviewers may aim to get assigned to a friend's paper as part of a quid-pro-quo deal. A critical impediment towards creating and evaluating methods to mitigate this issue is the lack of any publicly-available data on malicious paper bidding. In this work, we collect and publicly release a novel dataset to fill this gap, collected from a mock conference activity where participants were instructed to bid either honestly or maliciously. We further provide a descriptive analysis of the bidding behavior, inc
    
[^98]: DailyTalk：面向对话文本转语音的口语对话数据集

    DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech. (arXiv:2207.01063v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2207.01063](http://arxiv.org/abs/2207.01063)

    本文介绍了一个高质量的对话语音数据集DailyTalk，专门为对话TTS设计。DailyTalk可以用作通用的TTS数据集，而且基线可以表示DailyTalk的上下文信息。

    This paper introduces a high-quality conversational speech dataset DailyTalk designed for conversational TTS. DailyTalk can be used as a general TTS dataset, and the baseline can represent contextual information from DailyTalk.

    目前大多数的文本转语音（TTS）数据集都是由单个话语组成，缺乏对话方面的内容。本文介绍了DailyTalk，这是一个高质量的对话语音数据集，专门为对话TTS设计。我们从开放领域对话数据集DailyDialog中抽样、修改和录制了2,541个对话，并继承了其注释属性。在我们的数据集上，我们扩展了之前的工作作为我们的基线，其中一个非自回归TTS在对话中的历史信息的条件下进行。通过基线实验和我们的新颖度量标准，我们展示了DailyTalk可以用作通用的TTS数据集，而且我们的基线可以表示DailyTalk的上下文信息。DailyTalk数据集和基线代码可供学术用途免费使用，采用CC-BY-SA 4.0许可证。

    The majority of current Text-to-Speech (TTS) datasets, which are collections of individual utterances, contain few conversational aspects. In this paper, we introduce DailyTalk, a high-quality conversational speech dataset designed for conversational TTS. We sampled, modified, and recorded 2,541 dialogues from the open-domain dialogue dataset DailyDialog inheriting its annotated attributes. On top of our dataset, we extend prior work as our baseline, where a non-autoregressive TTS is conditioned on historical information in a dialogue. From the baseline experiment with both general and our novel metrics, we show that DailyTalk can be used as a general TTS dataset, and more than that, our baseline can represent contextual information from DailyTalk. The DailyTalk dataset and baseline code are freely available for academic use with CC-BY-SA 4.0 license.
    
[^99]: 基于解剖感知对比蒸馏的半监督医学图像分割引导启动

    Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation. (arXiv:2206.02307v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.02307](http://arxiv.org/abs/2206.02307)

    本文提出了一种基于解剖感知对比蒸馏的半监督医学图像分割引导启动方法，通过软标记负样本和捕获更多语义上相似的特征来解决医学图像数据不平衡的问题。

    This paper proposes a semi-supervised medical image segmentation bootstrapping method based on anatomical-aware contrastive distillation, which solves the problem of imbalanced medical image data by softly labeling negative samples and capturing more semantically similar features.

    对比学习已经在医学图像分割的注释稀缺问题上显示出了巨大的潜力。现有的方法通常假设标记和未标记的医学图像具有平衡的类分布。然而，现实中的医学图像数据通常是不平衡的（即多类标签不平衡），这自然地产生模糊的轮廓并通常错误地标记罕见的对象。此外，所有负样本是否同样负面仍不清楚。在这项工作中，我们提出了ACTION，一种解剖感知对比蒸馏框架，用于半监督医学图像分割。具体而言，我们首先通过软标记负样本而不是正负对之间的二元监督来开发迭代对比蒸馏算法。与正样本相比，我们还从随机选择的负样本集中捕获更多语义上相似的特征，以强制执行采样数据的多样性。其次，我们提出了一种基于解剖感知的启动方法，以更好地利用有限的标记数据。

    Contrastive learning has shown great promise over annotation scarcity problems in the context of medical image segmentation. Existing approaches typically assume a balanced class distribution for both labeled and unlabeled medical images. However, medical image data in reality is commonly imbalanced (i.e., multi-class label imbalance), which naturally yields blurry contours and usually incorrectly labels rare objects. Moreover, it remains unclear whether all negative samples are equally negative. In this work, we present ACTION, an Anatomical-aware ConTrastive dIstillatiON framework, for semi-supervised medical image segmentation. Specifically, we first develop an iterative contrastive distillation algorithm by softly labeling the negatives rather than binary supervision between positive and negative pairs. We also capture more semantically similar features from the randomly chosen negative set compared to the positives to enforce the diversity of the sampled data. Second, we raise a m
    
[^100]: DM$^2$: 基于分布匹配的去中心化多智能体强化学习

    DM$^2$: Decentralized Multi-Agent Reinforcement Learning for Distribution Matching. (arXiv:2206.00233v3 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2206.00233](http://arxiv.org/abs/2206.00233)

    本文提出了一种基于分布匹配的去中心化多智能体强化学习方法，每个智能体独立地最小化与目标访问分布的相应分量的分布不匹配，可以实现收敛到生成目标分布的联合策略。

    This paper proposes a decentralized multi-agent reinforcement learning method based on distribution matching, where each agent independently minimizes the distribution mismatch to the corresponding component of a target visitation distribution, achieving convergence to the joint policy that generated the target distribution.

    当前的多智能体协作方法往往依赖于集中式机制或显式通信协议以确保收敛。本文研究了分布匹配在不依赖于集中式组件或显式通信的分布式多智能体学习中的应用。在所提出的方案中，每个智能体独立地最小化与目标访问分布的相应分量的分布不匹配。理论分析表明，在某些条件下，每个智能体最小化其个体分布不匹配可以实现收敛到生成目标分布的联合策略。此外，如果目标分布来自优化合作任务的联合策略，则该任务奖励和分布匹配奖励的组合的最优策略是相同的联合策略。这一见解被用来制定一个实用的算法。

    Current approaches to multi-agent cooperation rely heavily on centralized mechanisms or explicit communication protocols to ensure convergence. This paper studies the problem of distributed multi-agent learning without resorting to centralized components or explicit communication. It examines the use of distribution matching to facilitate the coordination of independent agents. In the proposed scheme, each agent independently minimizes the distribution mismatch to the corresponding component of a target visitation distribution. The theoretical analysis shows that under certain conditions, each agent minimizing its individual distribution mismatch allows the convergence to the joint policy that generated the target distribution. Further, if the target distribution is from a joint policy that optimizes a cooperative task, the optimal policy for a combination of this task reward and the distribution matching reward is the same joint policy. This insight is used to formulate a practical al
    
[^101]: 对抗随机森林用于密度估计和生成建模

    Adversarial random forests for density estimation and generative modeling. (arXiv:2205.09435v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.09435](http://arxiv.org/abs/2205.09435)

    本文提出了一种使用对抗随机森林进行密度估计和数据合成的方法，该方法可以提供平滑的（非）条件密度，并允许完全合成数据生成，同时在各种表格数据基准测试中实现了与最先进的概率电路和深度学习模型相当或更好的性能，平均执行速度快了两个数量级。

    This paper proposes a method for density estimation and data synthesis using adversarial random forests, which provides smooth (un)conditional densities and allows for fully synthetic data generation. The method achieves comparable or superior performance to state-of-the-art probabilistic circuits and deep learning models on various tabular data benchmarks while executing about two orders of magnitude faster on average.

    我们提出了一种使用新型无监督随机森林进行密度估计和数据合成的方法。受生成对抗网络的启发，我们实现了一种递归过程，其中树通过交替的生成和判别轮次逐渐学习数据的结构特性。该方法在最小假设下可以被证明是一致的。与经典的基于树的替代方法不同，我们的方法提供平滑的（非）条件密度，并允许完全合成数据生成。我们在各种表格数据基准测试中实现了与最先进的概率电路和深度学习模型相当或更好的性能，同时平均执行速度快了两个数量级。附带的R包arf可在CRAN上获得。

    We propose methods for density estimation and data synthesis using a novel form of unsupervised random forests. Inspired by generative adversarial networks, we implement a recursive procedure in which trees gradually learn structural properties of the data through alternating rounds of generation and discrimination. The method is provably consistent under minimal assumptions. Unlike classic tree-based alternatives, our approach provides smooth (un)conditional densities and allows for fully synthetic data generation. We achieve comparable or superior performance to state-of-the-art probabilistic circuits and deep learning models on various tabular data benchmarks while executing about two orders of magnitude faster on average. An accompanying $\texttt{R}$ package, $\texttt{arf}$, is available on $\texttt{CRAN}$.
    
[^102]: 使用物体感知表示在多物体场景中进行视觉运动控制

    Visuomotor Control in Multi-Object Scenes Using Object-Aware Representations. (arXiv:2205.06333v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2205.06333](http://arxiv.org/abs/2205.06333)

    本文探讨了使用物体感知表示学习技术进行机器人任务的有效性，以解决当前方法学习任务特定表示不能很好地转移到其他任务的问题，以及由监督方法学习的表示需要大量标记数据集的问题。

    This paper explores the effectiveness of using object-aware representation learning techniques for robotic tasks, to address the problem that current methodologies learn task specific representations that do not necessarily transfer well to other tasks, and that representations learned by supervised methods require large labeled datasets for each task that are expensive to collect in the real world.

    场景的感知理解以及其不同组件之间的关系对于成功完成机器人任务至关重要。表示学习已被证明是一种强大的技术，但大多数当前的方法学习任务特定的表示，不一定能够很好地转移到其他任务。此外，由监督方法学习的表示需要大量标记数据集，这在现实世界中收集起来很昂贵。使用自监督学习从未标记的数据中获取表示可以缓解这个问题。然而，当前的自监督表示学习方法大多是物体无关的，我们证明了由此得到的表示对于具有许多组件的场景的通用机器人任务是不足够的。在本文中，我们探讨了使用物体感知表示学习技术进行机器人任务的有效性。

    Perceptual understanding of the scene and the relationship between its different components is important for successful completion of robotic tasks. Representation learning has been shown to be a powerful technique for this, but most of the current methodologies learn task specific representations that do not necessarily transfer well to other tasks. Furthermore, representations learned by supervised methods require large labeled datasets for each task that are expensive to collect in the real world. Using self-supervised learning to obtain representations from unlabeled data can mitigate this problem. However, current self-supervised representation learning methods are mostly object agnostic, and we demonstrate that the resulting representations are insufficient for general purpose robotics tasks as they fail to capture the complexity of scenes with many components. In this paper, we explore the effectiveness of using object-aware representation learning techniques for robotic tasks. 
    
[^103]: 通过检测错误的位置嵌入进行表示学习

    Representation Learning by Detecting Incorrect Location Embeddings. (arXiv:2204.04788v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2204.04788](http://arxiv.org/abs/2204.04788)

    本文提出了一种新的自监督学习（SSL）损失，用于图像表示学习。通过检测错误的位置嵌入，我们可以提高深度神经网络的泛化能力，使其更加鲁棒。我们称这种方法为DILEMMA，将其应用于MoCoV3、DINO和SimCLR，分别显示它们的性能提高了4.41%、3.97%和0.5%。

    

    本文提出了一种新的自监督学习（SSL）损失，用于图像表示学习。我们认为深度神经网络的泛化能力与其区分对象形状的能力有关。由于对象形状与其部件的位置有关，因此我们提出检测那些被人为移位的部件。我们用图像令牌表示对象部件，并训练ViT检测哪个令牌与错误的位置嵌入组合。然后，我们引入输入的稀疏性，使模型更加鲁棒，以应对遮挡并加速训练。我们称这种方法为DILEMMA，即检测错误位置嵌入和掩蔽输入。我们将DILEMMA应用于MoCoV3、DINO和SimCLR，并在相同的训练时间内，在ImageNet-1K上进行线性探测转移，分别显示它们的性能提高了4.41%、3.97%和0.5%。我们还展示了MAE与我们的完全微调改进的结果。

    In this paper, we introduce a novel self-supervised learning (SSL) loss for image representation learning. There is a growing belief that generalization in deep neural networks is linked to their ability to discriminate object shapes. Since object shape is related to the location of its parts, we propose to detect those that have been artificially misplaced. We represent object parts with image tokens and train a ViT to detect which token has been combined with an incorrect positional embedding. We then introduce sparsity in the inputs to make the model more robust to occlusions and to speed up the training. We call our method DILEMMA, which stands for Detection of Incorrect Location EMbeddings with MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an improvement in their performance of respectively 4.41%, 3.97%, and 0.5% under the same training time and with a linear probing transfer on ImageNet-1K. We also show full fine-tuning improvements of MAE combined with our 
    
[^104]: DynLight: 多级交通信号控制实现动态相位时长

    DynLight: Realize dynamic phase duration with multi-level traffic signal control. (arXiv:2204.03471v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2204.03471](http://arxiv.org/abs/2204.03471)

    本文已被撤回，原因是语言和理论描述不够令人满意，作者已经进行了修订和更新。

    The article has been withdrawn due to unsatisfactory language and theoretical description, and the authors have revised and updated it.

    我们因以下原因撤回本文：1.本文的语言和理论描述不够令人满意；2.我们在其他作者的帮助下丰富和修订了本文；3.我们必须更新作者贡献信息。

    We would like to withdraw this article for the following reasons: 1 this article is not satisfactory for limited language and theoretical description; 2 we have enriched and revised this article with the help of other authors; 3 we must update the author contribution information.
    
[^105]: 分离征服启发式算法允许在分类、回归和生存数据中进行强大的对比集挖掘

    Separate and conquer heuristic allows robust mining of contrast sets in classification, regression, and survival data. (arXiv:2204.00497v3 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2204.00497](http://arxiv.org/abs/2204.00497)

    本文提出了一种基于分离征服的对比集挖掘算法RuleKit-CS，该算法通过多次通过伴随属性惩罚方案提供描述具有不同属性的相同示例的对比集，区别于标准的分离征服。该算法还被推广到回归和生存数据，允许识别标签属性/生存预测与预定义对比组的标签/预测一致的对比集。

    This paper proposes a contrast set mining algorithm, RuleKit-CS, based on the separate and conquer heuristic, which provides contrast sets describing the same examples with different attributes through multiple passes accompanied with an attribute penalization scheme. The algorithm is also generalized for regression and survival data, allowing identification of contrast sets whose label attribute/survival prognosis is consistent with the label/prognosis for the predefined contrast groups.

    识别群体之间的差异是最重要的知识发现问题之一。该过程，也称为对比集挖掘，在医学、工业或经济等广泛领域中应用。在本文中，我们提出了RuleKit-CS，一种基于分离征服的对比集挖掘算法——一种用于决策规则归纳的成熟启发式算法。多次通过伴随属性惩罚方案提供描述具有不同属性的相同示例的对比集，区别于标准的分离征服。该算法还被推广到回归和生存数据，允许识别标签属性/生存预测与预定义对比组的标签/预测一致的对比集。这个特性，不是现有方法所提供的，进一步扩展了RuleKit-CS的可用性。在来自各个领域的130多个数据集上进行的实验和详细分析。

    Identifying differences between groups is one of the most important knowledge discovery problems. The procedure, also known as contrast sets mining, is applied in a wide range of areas like medicine, industry, or economics.  In the paper we present RuleKit-CS, an algorithm for contrast set mining based on separate and conquer - a well established heuristic for decision rule induction. Multiple passes accompanied with an attribute penalization scheme provide contrast sets describing same examples with different attributes, distinguishing presented approach from the standard separate and conquer. The algorithm was also generalized for regression and survival data allowing identification of contrast sets whose label attribute/survival prognosis is consistent with the label/prognosis for the predefined contrast groups. This feature, not provided by the existing approaches, further extends the usability of RuleKit-CS.  Experiments on over 130 data sets from various areas and detailed analys
    
[^106]: 生成建模有助于弱监督（反之亦然）

    Generative Modeling Helps Weak Supervision (and Vice Versa). (arXiv:2203.12023v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.12023](http://arxiv.org/abs/2203.12023)

    本文提出了一种融合程序弱监督和生成对抗网络的模型，通过对齐离散潜在变量和弱监督派生的标签估计，改善了未观察到的标签的估计，实现了数据增强。

    This paper proposes a model that fuses programmatic weak supervision and generative adversarial networks, improving the estimate of unobserved labels by aligning discrete latent variables and weak supervision derived label estimate, and enabling data augmentation through weak supervision.

    许多有前途的监督式机器学习应用在获取足够数量和质量的标记数据方面面临困难，从而造成昂贵的瓶颈。为了克服这些限制，研究了不依赖于基本真实标签的技术，包括弱监督和生成建模。虽然这些技术似乎可以共同使用，相互改进，但如何在它们之间建立接口尚不为人所知。在这项工作中，我们提出了一种融合程序弱监督和生成对抗网络的模型，并提供了理论上的理由来支持这种融合。所提出的方法捕捉数据中的离散潜在变量以及弱监督派生的标签估计。两者的对齐允许更好地建模弱监督来源的样本相关准确性，从而改善未观察到的标签的估计。这是第一种通过弱监督实现数据增强的方法。

    Many promising applications of supervised machine learning face hurdles in the acquisition of labeled data in sufficient quantity and quality, creating an expensive bottleneck. To overcome such limitations, techniques that do not depend on ground truth labels have been studied, including weak supervision and generative modeling. While these techniques would seem to be usable in concert, improving one another, how to build an interface between them is not well-understood. In this work, we propose a model fusing programmatic weak supervision and generative adversarial networks and provide theoretical justification motivating this fusion. The proposed approach captures discrete latent variables in the data alongside the weak supervision derived label estimate. Alignment of the two allows for better modeling of sample-dependent accuracies of the weak supervision sources, improving the estimate of unobserved labels. It is the first approach to enable data augmentation through weakly supervi
    
[^107]: 学习四足动物运动的扭矩控制

    Learning Torque Control for Quadrupedal Locomotion. (arXiv:2203.05194v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2203.05194](http://arxiv.org/abs/2203.05194)

    本文提出了一种基于扭矩的强化学习框架，直接预测关节扭矩，避免使用PD控制器，通过广泛的实验验证，四足动物能够穿越各种地形并抵抗外部干扰，同时保持运动。

    This paper proposes a torque-based reinforcement learning framework that directly predicts joint torques, avoiding the use of a PD controller. The framework is validated through extensive experiments, where a quadruped is capable of traversing various terrain and resisting external disturbances while maintaining locomotion.

    强化学习已成为开发四足机器人控制器的一种有前途的方法。传统上，用于运动的RL设计遵循基于位置的范例，其中RL策略以低频率输出目标关节位置，然后由高频比例-导数（PD）控制器跟踪以产生关节扭矩。相比之下，对于四足动物运动的基于模型的控制，已经从基于位置的控制范例转向基于扭矩的控制。鉴于基于模型的控制的最新进展，我们通过引入基于扭矩的RL框架，探索了一种替代基于位置的RL范例的方法，其中RL策略直接在高频率下预测关节扭矩，从而避免使用PD控制器。所提出的学习扭矩控制框架通过广泛的实验进行了验证，在这些实验中，四足动物能够穿越各种地形并抵抗外部干扰，同时保持运动。

    Reinforcement learning (RL) has become a promising approach to developing controllers for quadrupedal robots. Conventionally, an RL design for locomotion follows a position-based paradigm, wherein an RL policy outputs target joint positions at a low frequency that are then tracked by a high-frequency proportional-derivative (PD) controller to produce joint torques. In contrast, for the model-based control of quadrupedal locomotion, there has been a paradigm shift from position-based control to torque-based control. In light of the recent advances in model-based control, we explore an alternative to the position-based RL paradigm, by introducing a torque-based RL framework, where an RL policy directly predicts joint torques at a high frequency, thus circumventing the use of a PD controller. The proposed learning torque control framework is validated with extensive experiments, in which a quadruped is capable of traversing various terrain and resisting external disturbances while followi
    
[^108]: 用微小数据集实现网络加速的实践

    Practical Network Acceleration with Tiny Sets. (arXiv:2202.07861v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2202.07861](http://arxiv.org/abs/2202.07861)

    本文提出了一种基于删除块的方法，用于加速稀缺训练样本的网络，通过提出的可恢复性概念选择要删除的块，提出了一种名为PRACTISE的算法，仅使用微小的训练图像集来加速网络，PRACTISE的表现优于以往的方法。

    

    由于数据隐私问题，使用微小的训练集加速网络已成为实践中的一个关键需求。以往的方法主要采用滤波器级别的剪枝来加速稀缺训练样本的网络。在本文中，我们揭示了在这种情况下，删除块是一种基本上更优越的方法。它享有更高的加速比，并在少样本情况下产生更好的延迟-准确性性能。为了选择要删除的块，我们提出了一个新的概念，即可恢复性，用于衡量压缩网络的恢复难度。我们的可恢复性对于选择要删除的块是高效和有效的。最后，我们提出了一种名为PRACTISE的算法，仅使用微小的训练图像集来加速网络。PRACTISE的表现优于以往的方法。对于22％的延迟降低，PRACTISE在ImageNet-1k上平均超过以往方法7％。它还具有高度的泛化能力，在数据隐私场景和各种网络架构下表现良好。

    Due to data privacy issues, accelerating networks with tiny training sets has become a critical need in practice. Previous methods mainly adopt filter-level pruning to accelerate networks with scarce training samples. In this paper, we reveal that dropping blocks is a fundamentally superior approach in this scenario. It enjoys a higher acceleration ratio and results in a better latency-accuracy performance under the few-shot setting. To choose which blocks to drop, we propose a new concept namely recoverability to measure the difficulty of recovering the compressed network. Our recoverability is efficient and effective for choosing which blocks to drop. Finally, we propose an algorithm named PRACTISE to accelerate networks using only tiny sets of training images. PRACTISE outperforms previous methods by a significant margin. For 22% latency reduction, PRACTISE surpasses previous methods by on average 7% on ImageNet-1k. It also enjoys high generalization ability, working well under data
    
[^109]: PGMax: 用于离散概率图模型和JAX中的循环置信传播的因子图

    PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX. (arXiv:2202.04110v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.04110](http://arxiv.org/abs/2202.04110)

    PGMax是一个用于离散概率图模型的因子图工具，可以在JAX中自动运行高效且可扩展的循环置信传播，与现有替代方案相比，PGMax获得了更高质量的推理结果，推理时间加速高达三个数量级。

    PGMax is a factor graph tool for discrete probabilistic graphical models that automatically runs efficient and scalable loopy belief propagation in JAX. Compared with existing alternatives, PGMax obtains higher-quality inference results with up to three orders-of-magnitude inference time speedups.

    PGMax是一个开源的Python包，用于轻松指定离散概率图模型（PGMs）作为因子图，并在JAX中自动运行高效且可扩展的循环置信传播（LBP）。PGMax支持具有可处理因子的一般因子图，并利用现代加速器（如GPU）进行推理。与现有替代方案相比，PGMax获得了更高质量的推理结果，推理时间加速高达三个数量级。PGMax还与快速增长的JAX生态系统无缝交互，开启了新的研究可能性。我们的源代码、示例和文档可在https://github.com/deepmind/PGMax上获得。

    PGMax is an open-source Python package for (a) easily specifying discrete Probabilistic Graphical Models (PGMs) as factor graphs; and (b) automatically running efficient and scalable loopy belief propagation (LBP) in JAX. PGMax supports general factor graphs with tractable factors, and leverages modern accelerators like GPUs for inference. Compared with existing alternatives, PGMax obtains higher-quality inference results with up to three orders-of-magnitude inference time speedups. PGMax additionally interacts seamlessly with the rapidly growing JAX ecosystem, opening up new research possibilities. Our source code, examples and documentation are available at https://github.com/deepmind/PGMax.
    
[^110]: 视频中的时间句子定位：综述与未来方向

    Temporal Sentence Grounding in Videos: A Survey and Future Directions. (arXiv:2201.08071v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2201.08071](http://arxiv.org/abs/2201.08071)

    本文综述了视频中的时间句子定位（TSGV）的基本概念和当前研究现状，以及未来研究方向。TSGV旨在从未经修剪的视频中检索与语言查询语义对应的时间时刻，连接计算机视觉和自然语言，是两个社区研究人员的重点关注点。

    This survey summarizes the fundamental concepts and current research status of temporal sentence grounding in videos (TSGV), also known as natural language video localization (NLVL) or video moment retrieval (VMR), as well as future research directions. TSGV aims to retrieve a temporal moment that semantically corresponds to a language query from an untrimmed video, connecting computer vision and natural language, and has drawn significant attention from researchers in both communities.

    视频中的时间句子定位（TSGV），又称自然语言视频定位（NLVL）或视频时刻检索（VMR），旨在从未经修剪的视频中检索与语言查询语义对应的时间时刻。连接计算机视觉和自然语言，TSGV引起了两个社区研究人员的重视。本综述试图提供TSGV中基本概念和当前研究现状的总结，以及未来研究方向。作为背景，我们以教程的形式介绍了TSGV中功能组件的常见结构：从原始视频和语言查询的特征提取到目标时刻的答案预测。然后，我们回顾了多模态理解和交互的技术，这是TSGV的重点关注点，以实现两种模态之间的有效对齐。我们构建了TSGV技术的分类法，并详细阐述了不同类别的方法及其优缺点。

    Temporal sentence grounding in videos (TSGV), \aka natural language video localization (NLVL) or video moment retrieval (VMR), aims to retrieve a temporal moment that semantically corresponds to a language query from an untrimmed video. Connecting computer vision and natural language, TSGV has drawn significant attention from researchers in both communities. This survey attempts to provide a summary of fundamental concepts in TSGV and current research status, as well as future research directions. As the background, we present a common structure of functional components in TSGV, in a tutorial style: from feature extraction from raw video and language query, to answer prediction of the target moment. Then we review the techniques for multimodal understanding and interaction, which is the key focus of TSGV for effective alignment between the two modalities. We construct a taxonomy of TSGV techniques and elaborate the methods in different categories with their strengths and weaknesses. La
    
[^111]: Egeria: 基于知识引导的层冻结技术实现高效DNN训练

    Egeria: Efficient DNN Training with Knowledge-Guided Layer Freezing. (arXiv:2201.06227v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.06227](http://arxiv.org/abs/2201.06227)

    Egeria是一种基于知识引导的DNN训练系统，通过跳过DNN层的计算和通信来实现高效训练，利用参考模型中的语义知识准确评估单个层的训练可塑性，并安全地冻结已收敛的层，从而节省相应的反向计算和通信。

    Egeria is a knowledge-guided DNN training system that skips computing and communication through DNN layer freezing, accurately evaluates individual layers' training plasticity using semantic knowledge from a reference model, and safely freezes the converged ones, saving their corresponding backward computation and communication.

    训练深度神经网络（DNN）是一项耗时的任务。本文提出了一种基于知识引导的DNN训练系统Egeria，通过跳过DNN层的计算和通信来实现高效训练。我们的关键洞察是，内部DNN层的训练进度存在显著差异，前层通常比深层更早地得到很好的训练。为了探索这一点，我们首先引入了训练可塑性的概念，以量化内部DNN层的训练进度。然后，我们设计了Egeria，一种基于知识引导的DNN训练系统，利用参考模型中的语义知识准确评估单个层的训练可塑性，并安全地冻结已收敛的层，从而节省相应的反向计算和通信。

    Training deep neural networks (DNNs) is time-consuming. While most existing solutions try to overlap/schedule computation and communication for efficient training, this paper goes one step further by skipping computing and communication through DNN layer freezing. Our key insight is that the training progress of internal DNN layers differs significantly, and front layers often become well-trained much earlier than deep layers. To explore this, we first introduce the notion of training plasticity to quantify the training progress of internal DNN layers. Then we design Egeria, a knowledge-guided DNN training system that employs semantic knowledge from a reference model to accurately evaluate individual layers' training plasticity and safely freeze the converged ones, saving their corresponding backward computation and communication. Our reference model is generated on the fly using quantization techniques and runs forward operations asynchronously on available CPUs to minimize the overhe
    
[^112]: 为人道主义利益开发可信的人工智能网络

    Developing a Trusted Human-AI Network for Humanitarian Benefit. (arXiv:2112.11191v3 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2112.11191](http://arxiv.org/abs/2112.11191)

    本文提出了一种可信的人工智能通信网络，将通信协议、区块链技术和信息融合与AI集成，以改善冲突通信，为人道主义利益提供可问责信息交换。

    This paper proposes a trusted human-AI communication network that integrates communication protocols, blockchain technology, and information fusion with AI to improve conflict communications for accountable information exchange regarding protected entities, critical infrastructure, and humanitarian signals and status updates for humans and machines in conflicts.

    人工智能（AI）将越来越多地在冲突中以数字和物理方式参与，但缺乏与人类进行人道主义目的的可信通信。本文考虑将通信协议（“白旗协议”）、分布式账本“区块链”技术和信息融合与AI集成，以改善冲突通信，称为“受保护的保证理解情况和实体”PAUSE。这样一个可信的人工智能通信网络可以提供关于受保护实体、关键基础设施、人道主义信号和人类和机器在冲突中的状态更新的可问责信息交换。我们研究了几个现实的潜在案例研究，将这些技术集成到一个可信的人工智能网络中，以实现人道主义利益，包括实时映射冲突区域的平民和战斗人员，为避免事故做准备，并使用网络管理错误信息。

    Artificial intelligences (AI) will increasingly participate digitally and physically in conflicts, yet there is a lack of trused communications with humans for humanitarian purposes. In this paper we consider the integration of a communications protocol (the 'whiteflag protocol'), distributed ledger 'blockchain' technology, and information fusion with AI, to improve conflict communications called 'protected assurance understanding situation and entitities' PAUSE. Such a trusted human-AI communication network could provide accountable information exchange regarding protected entities, critical infrastructure, humanitiarian signals and status updates for humans and machines in conflicts. We examine several realistic potential case studies for the integration of these technologies into a trusted human-AI network for humanitarian benefit including mapping a conflict zone with civilians and combatants in real time, preparation to avoid incidents and using the network to manage misinformatio
    
[^113]: 两视角图神经网络用于知识图谱补全

    Two-view Graph Neural Networks for Knowledge Graph Completion. (arXiv:2112.09231v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2112.09231](http://arxiv.org/abs/2112.09231)

    本文提出了一种名为WGE的图神经网络模型，通过两个单一的实体和关系为中心的图来学习实体和关系的向量表示，并在知识图谱补全任务上取得了优异的性能。

    This paper proposes a graph neural network model named WGE, which learns vector representations of entities and relations from two single entity- and relation-focused graphs, and achieves excellent performance on knowledge graph completion task.

    我们提出了一种有效的基于图神经网络（GNN）的知识图谱嵌入模型，称为WGE，以捕捉实体和关系为中心的图结构。给定一个知识图谱，WGE构建一个单一的无向实体为中心的图，将实体视为节点。WGE还从关系为中心的约束条件构建另一个单一的无向图，将实体和关系视为节点。然后，WGE提出了一种基于GNN的架构，从这两个单一的实体和关系为中心的图中更好地学习实体和关系的向量表示。WGE将学习到的实体和关系表示馈送到加权得分函数中，以返回知识图谱补全的三元组得分。实验结果表明，WGE在七个知识图谱补全基准数据集上优于强基线模型。

    We present an effective graph neural network (GNN)-based knowledge graph embedding model, which we name WGE, to capture entity- and relation-focused graph structures. Given a knowledge graph, WGE builds a single undirected entity-focused graph that views entities as nodes. WGE also constructs another single undirected graph from relation-focused constraints, which views entities and relations as nodes. WGE then proposes a GNN-based architecture to better learn vector representations of entities and relations from these two single entity- and relation-focused graphs. WGE feeds the learned entity and relation representations into a weighted score function to return the triple scores for knowledge graph completion. Experimental results show that WGE outperforms strong baselines on seven benchmark datasets for knowledge graph completion.
    
[^114]: 学习在任务和动作规划中使用流进行搜索

    Learning to Search in Task and Motion Planning with Streams. (arXiv:2111.13144v5 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2111.13144](http://arxiv.org/abs/2111.13144)

    本文提出了一种几何信息的符号规划器，使用图神经网络优先级排序，以最佳优先方式扩展对象和事实集合，从而改善了在任务和动作规划中的长期推理能力。在7自由度机械臂的堆叠操纵任务中得到了应用。

    This paper proposes a geometrically informed symbolic planner that expands the set of objects and facts in a best-first manner, prioritized by a Graph Neural Network that is learned from prior search computations, improving the long-term reasoning ability in task and motion planning. The algorithm is applied to a 7DOF robotic arm in block-stacking manipulation tasks.

    机器人中的任务和动作规划问题将离散任务变量上的符号规划与连续状态和动作变量上的运动优化相结合。最近的作品，如PDDLStream，专注于乐观规划，使用逐步增长的对象集，直到找到可行的轨迹。然而，这个集合是以广度优先的方式穷举扩展的，而不考虑手头问题的逻辑和几何结构，这使得具有大量对象的长期推理变得耗时。为了解决这个问题，我们提出了一个几何信息的符号规划器，以最佳优先方式扩展对象和事实集合，由先前的搜索计算学习的图神经网络优先级排序。我们在各种问题上评估了我们的方法，并展示了在困难情况下规划的能力得到了改善。我们还将我们的算法应用于7自由度机械臂在堆叠操纵任务中。

    Task and motion planning problems in robotics combine symbolic planning over discrete task variables with motion optimization over continuous state and action variables. Recent works such as PDDLStream have focused on optimistic planning with an incrementally growing set of objects until a feasible trajectory is found. However, this set is exhaustively expanded in a breadth-first manner, regardless of the logical and geometric structure of the problem at hand, which makes long-horizon reasoning with large numbers of objects prohibitively time-consuming. To address this issue, we propose a geometrically informed symbolic planner that expands the set of objects and facts in a best-first manner, prioritized by a Graph Neural Network that is learned from prior search computations. We evaluate our approach on a diverse set of problems and demonstrate an improved ability to plan in difficult scenarios. We also apply our algorithm on a 7DOF robotic arm in block-stacking manipulation tasks.
    
[^115]: 基于通道特征去噪的指纹呈现攻击检测

    Fingerprint Presentation Attack Detection by Channel-wise Feature Denoising. (arXiv:2111.07620v2 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2111.07620](http://arxiv.org/abs/2111.07620)

    本文提出了一种基于通道特征去噪的指纹呈现攻击检测方法，通过处理先前研究中忽略的冗余噪声信息来学习指纹图像的重要特征，具有较好的鲁棒性和准确性。

    This paper proposes a novel channel-wise feature denoising fingerprint presentation attack detection (CFD-PAD) method, which learns important features of fingerprint images by handling the redundant noise information ignored in previous studies, and exhibits good robustness and accuracy under various attack types.

    由于攻击材料的多样性，指纹识别系统（AFRS）容易受到恶意攻击。因此，提出有效的指纹呈现攻击检测（PAD）方法对于AFRS的安全性和可靠性至关重要。然而，当前的PAD方法在新的攻击类型设置下往往表现出较差的鲁棒性。因此，本文提出了一种新颖的基于通道特征去噪的指纹PAD（CFD-PAD）方法，通过处理先前研究中忽略的冗余噪声信息来学习指纹图像的重要特征。该方法通过权衡每个通道的重要性并识别出有区别性的通道和“噪声”通道来学习指纹图像的重要特征。然后，在特征图中抑制“噪声”通道的传播以减少干扰。具体而言，设计了PA-Adaptation损失来约束特征分布，使活体指纹的特征分布更聚合，而欺诈指纹的特征分布更分散。实验结果表明，所提出的方法在各种攻击类型下均具有较好的鲁棒性和准确性。

    Due to the diversity of attack materials, fingerprint recognition systems (AFRSs) are vulnerable to malicious attacks. It is thus important to propose effective fingerprint presentation attack detection (PAD) methods for the safety and reliability of AFRSs. However, current PAD methods often exhibit poor robustness under new attack types settings. This paper thus proposes a novel channel-wise feature denoising fingerprint PAD (CFD-PAD) method by handling the redundant noise information ignored in previous studies. The proposed method learns important features of fingerprint images by weighing the importance of each channel and identifying discriminative channels and "noise" channels. Then, the propagation of "noise" channels is suppressed in the feature map to reduce interference. Specifically, a PA-Adaptation loss is designed to constrain the feature distribution to make the feature distribution of live fingerprints more aggregate and that of spoof fingerprints more disperse. Experime
    
[^116]: 训练过的人形机器人可以执行类人的跨模态社交关注和冲突解决

    A trained humanoid robot can perform human-like crossmodal social attention and conflict resolution. (arXiv:2111.01906v5 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2111.01906](http://arxiv.org/abs/2111.01906)

    本研究采用跨模态冲突解决的神经机器人范例，使机器人表现出类人的社交关注，为增强人机社交互动提供了新思路。

    This study adopts the neurorobotic paradigm of crossmodal conflict resolution to make a robot express human-like social attention, providing a new approach to enhance human-robot social interaction.

    为了增强人机社交互动，机器人在复杂的现实环境中处理多个社交线索至关重要。然而，跨模态输入信息的不一致性是不可避免的，这可能对机器人的处理造成挑战。为了解决这个问题，我们的研究采用了跨模态冲突解决的神经机器人范例，使机器人表现出类人的社交关注。我们对37名参与者进行了一项行为实验。我们设计了一个圆桌会议场景，有三个动画化的头像，以提高生态效度。每个头像都戴着医用口罩，遮盖了鼻子、嘴巴和下巴的面部线索。中央头像移动其眼睛注视，而外围头像则发出声音。凝视方向和声音位置要么是空间上一致的，要么是不一致的。我们观察到，中央头像的动态凝视可以触发跨模态社交关注反应。特别是，人类表现

    To enhance human-robot social interaction, it is essential for robots to process multiple social cues in a complex real-world environment. However, incongruency of input information across modalities is inevitable and could be challenging for robots to process. To tackle this challenge, our study adopted the neurorobotic paradigm of crossmodal conflict resolution to make a robot express human-like social attention. A behavioural experiment was conducted on 37 participants for the human study. We designed a round-table meeting scenario with three animated avatars to improve ecological validity. Each avatar wore a medical mask to obscure the facial cues of the nose, mouth, and jaw. The central avatar shifted its eye gaze while the peripheral avatars generated sound. Gaze direction and sound locations were either spatially congruent or incongruent. We observed that the central avatar's dynamic gaze could trigger crossmodal social attention responses. In particular, human performances are 
    
[^117]: 全面深度学习

    Holistic Deep Learning. (arXiv:2110.15829v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.15829](http://arxiv.org/abs/2110.15829)

    本文提出了一种全面深度学习框架，通过解决输入扰动的脆弱性、过度参数化和性能不稳定性等挑战，全面提高了准确性、鲁棒性、稀疏性和稳定性，适用于表格和图像数据集。提供了选择适当的训练损失函数的建议。

    This paper proposes a holistic deep learning framework that addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. A prescriptive approach is provided to support practitioners in selecting an appropriate training loss function based on their specific objectives.

    本文提出了一种新颖的全面深度学习框架，同时解决了对输入扰动的脆弱性、过度参数化和来自不同训练验证拆分的性能不稳定性等挑战。所提出的框架在标准深度学习模型上全面提高了准确性、鲁棒性、稀疏性和稳定性，这在对表格和图像数据集进行广泛实验中得到了证明。结果进一步通过消融实验和SHAP值分析进行验证，揭示了不同评估指标之间的交互作用和权衡。为了支持实践者应用我们的框架，我们提供了一种指导性方法，根据他们的具体目标，提供选择适当的训练损失函数的建议。所有用于重现结果的代码都可以在https://github.com/kimvc7/HDL找到。

    This paper presents a novel holistic deep learning framework that simultaneously addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework holistically improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. The results are further validated by ablation experiments and SHAP value analysis, which reveal the interactions and trade-offs between the different evaluation metrics. To support practitioners applying our framework, we provide a prescriptive approach that offers recommendations for selecting an appropriate training loss function based on their specific objectives. All the code to reproduce the results can be found at https://github.com/kimvc7/HDL.
    
[^118]: 二进制神经网络的全面综述

    A comprehensive review of Binary Neural Network. (arXiv:2110.06804v4 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2110.06804](http://arxiv.org/abs/2110.06804)

    本文全面综述了二进制神经网络的最新发展，重点关注1位激活和1位卷积网络的权重，这些网络可以在微小的受限设备上实现和嵌入，并节省大量存储、计算成本和能量消耗。

    This article provides a comprehensive overview of recent developments in Binary Neural Networks (BNN), with a focus on 1-bit activations and 1-bit convolution networks. These networks can be implemented and embedded on tiny restricted devices, saving significant storage, computation cost, and energy consumption.

    深度学习（DL）最近改变了智能系统的发展，并被广泛应用于许多实际应用中。尽管DL具有各种好处和潜力，但在不同的计算受限和能量受限设备中需要进行DL处理。研究二进制神经网络（BNN）等具有改变游戏规则的技术以增加深度学习能力是很自然的。最近在BNN方面取得了显着进展，因为它们可以在微小的受限设备上实现和嵌入，并节省大量存储、计算成本和能量消耗。然而，几乎所有的BNN行为都会带来额外的内存、计算成本和更高的性能。本文提供了BNN最近发展的完整概述。本文专门关注1位激活和1位卷积网络的权重，与以前的调查混合使用低位作品相反。它对BNN的开发进行了全面调查。

    Deep learning (DL) has recently changed the development of intelligent systems and is widely adopted in many real-life applications. Despite their various benefits and potentials, there is a high demand for DL processing in different computationally limited and energy-constrained devices. It is natural to study game-changing technologies such as Binary Neural Networks (BNN) to increase deep learning capabilities. Recently remarkable progress has been made in BNN since they can be implemented and embedded on tiny restricted devices and save a significant amount of storage, computation cost, and energy consumption. However, nearly all BNN acts trade with extra memory, computation cost, and higher performance. This article provides a complete overview of recent developments in BNN. This article focuses exclusively on 1-bit activations and weights 1-bit convolution networks, contrary to previous surveys in which low-bit works are mixed in. It conducted a complete investigation of BNN's dev
    
[^119]: 自注意力网络可以处理有界层次语言

    Self-Attention Networks Can Process Bounded Hierarchical Languages. (arXiv:2105.11115v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2105.11115](http://arxiv.org/abs/2105.11115)

    本文证明了自注意力网络可以处理深度受限的$\mathsf{Dyck}_{k}$子集$\mathsf{Dyck}_{k,D}$，这更好地捕捉了自然语言的有界层次结构。

    This paper proves that self-attention networks can process the depth-bounded subset of $\mathsf{Dyck}_{k}$, $\mathsf{Dyck}_{k,D}$, which better captures the bounded hierarchical structure of natural language.

    尽管自注意力网络在自然语言处理中表现出色，但最近证明它们在处理具有分层结构的形式语言（例如$\mathsf{Dyck}_k$，由$k$种嵌套括号组成的语言）方面存在局限性。这表明自然语言可以用模型很好地近似，而这些模型对于形式语言来说过于弱，或者说层次结构和递归在自然语言中的作用可能是有限的。我们通过证明自注意力网络可以处理$\mathsf{Dyck}_{k,D}$来限定这一含义，其中$\mathsf{Dyck}_{k,D}$是深度受限的$\mathsf{Dyck}_{k}$子集，它更好地捕捉了自然语言的有界层次结构。具体而言，我们构建了一个具有$D+1$层和$O(\log k)$内存大小（每个令牌每层）的硬注意力网络，用于识别$\mathsf{Dyck}_{k,D}$，以及一个具有两层和$O(\log k)$内存大小的软注意力网络，用于生成$\mathsf{Dyck}_{k,D}$。实验表明，软注意力网络可以在$\mathsf{Dyck}_{k,D}$上生成正确的序列。

    Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as $\mathsf{Dyck}_k$, the language consisting of well-nested parentheses of $k$ types. This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited. We qualify this implication by proving that self-attention networks can process $\mathsf{Dyck}_{k, D}$, the subset of $\mathsf{Dyck}_{k}$ with depth bounded by $D$, which arguably better captures the bounded hierarchical structure of natural language. Specifically, we construct a hard-attention network with $D+1$ layers and $O(\log k)$ memory size (per token per layer) that recognizes $\mathsf{Dyck}_{k, D}$, and a soft-attention network with two layers and $O(\log k)$ memory size that generates $\mathsf{Dyck}_{k, D}$. Experiments show that s
    
[^120]: NOMU: 基于神经优化的模型不确定性

    NOMU: Neural Optimization-based Model Uncertainty. (arXiv:2102.13640v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.13640](http://arxiv.org/abs/2102.13640)

    NOMU是一种新的神经网络模型，可以在无噪声设置下，通过设计一个由两个连接的子NN组成的网络架构，并使用精心设计的损失函数进行训练，来捕捉NN的模型不确定性。该模型满足五个关于模型不确定性的重要愿望。

    NOMU is a new neural network model that captures model uncertainty for neural networks (NNs) in regression by designing a network architecture consisting of two connected sub-NNs, one for model prediction and one for model uncertainty, and training it using a carefully-designed loss function. The model satisfies five important desiderata regarding model uncertainty.

    我们研究了神经网络（NN）回归中模型不确定性的估计方法。为了隔离模型不确定性的影响，我们专注于稀缺训练数据的无噪声设置。我们提出了五个关于模型不确定性的重要愿望，任何方法都应该满足这些愿望。然而，我们发现，即使是贝叶斯理论所要求的一些愿望，已经建立的基准测试也经常无法可靠地捕捉到。为了解决这个问题，我们引入了一种新的方法来捕捉NN的模型不确定性，称为神经优化模型不确定性（NOMU）。 NOMU的主要思想是设计一个由两个连接的子NN组成的网络架构，一个用于模型预测，一个用于模型不确定性，并使用精心设计的损失函数进行训练。重要的是，我们的设计强制NOMU满足我们的五个愿望。由于其模块化架构，如果给定访问权限，NOMU可以为任何给定的（先前训练的）NN提供模型不确定性。

    We study methods for estimating model uncertainty for neural networks (NNs) in regression. To isolate the effect of model uncertainty, we focus on a noiseless setting with scarce training data. We introduce five important desiderata regarding model uncertainty that any method should satisfy. However, we find that established benchmarks often fail to reliably capture some of these desiderata, even those that are required by Bayesian theory. To address this, we introduce a new approach for capturing model uncertainty for NNs, which we call Neural Optimization-based Model Uncertainty (NOMU). The main idea of NOMU is to design a network architecture consisting of two connected sub-NNs, one for model prediction and one for model uncertainty, and to train it using a carefully-designed loss function. Importantly, our design enforces that NOMU satisfies our five desiderata. Due to its modular architecture, NOMU can provide model uncertainty for any given (previously trained) NN if given access
    

