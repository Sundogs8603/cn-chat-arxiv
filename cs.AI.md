# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation.](http://arxiv.org/abs/2401.06127) | 本论文旨在提出一种高效的方法来从扩散模型中提炼GANs，并用于图像到图像的转换任务。这种方法可以实现灵活的实时图像编辑，并显著降低训练不同概念模型的成本。 |
| [^2] | [Manipulating Feature Visualizations with Gradient Slingshots.](http://arxiv.org/abs/2401.06122) | 本研究探究了激活最大化方法在对抗模型操作中的脆弱性，并提出了一种新的方法来操纵特征可视化，以隐藏特定神经元的功能。 |
| [^3] | [Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models.](http://arxiv.org/abs/2401.06102) | 本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。 |
| [^4] | [Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models.](http://arxiv.org/abs/2401.06088) | 本研究通过使用文本生成技术和机器学习模型，训练了几种变种的生物医学生成预训练变压器模型，并开发了一个自动补全工具，可为三级护理人员提供准确和格式良好的主要症状短语或句子。 |
| [^5] | [XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an Agent-Based Model of a Sports Betting Exchange.](http://arxiv.org/abs/2401.06086) | 本文介绍了在基于代理模型的体育博彩交易中使用XGBoost学习到的动态投注策略，并通过基于代理模型的模拟器进行实验评估。 |
| [^6] | [Secrets of RLHF in Large Language Models Part II: Reward Modeling.](http://arxiv.org/abs/2401.06080) | 本报告探讨了在 RLHF 中解决奖励建模的两个挑战，通过使用多个奖励模型的投票机制来测量数据中偏好的强度，并解决在特定分布数据上训练奖励模型难以推广的问题。 |
| [^7] | [Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion.](http://arxiv.org/abs/2401.06072) | 本文提出了一种基于LLMs的新方法，将时间知识图补全任务概念化为历史事件链中的事件生成任务。通过引入高效的微调方法和结构化历史数据增强，以及整合反向知识，我们的模型在多个指标上优于现有的方法，取得了SOTA结果。 |
| [^8] | [Investigating Data Contamination for Pre-training Language Models.](http://arxiv.org/abs/2401.06059) | 这项研究调查了预训练语言模型中的数据污染问题，以及该污染对下游任务性能的影响。 |
| [^9] | [On the Power of Graph Neural Networks and Feature Augmentation Strategies to Classify Social Networks.](http://arxiv.org/abs/2401.06048) | 本文研究了四种图神经网络架构的性能以及通过应用不同的人工特征增强策略来提高图分类任务的准确性，结果表明GNN架构的计算能力和人工特征提供的信息水平对任务的性能具有平衡的重要性。 |
| [^10] | [Surgical-DINO: Adapter Learning of Foundation Model for Depth Estimation in Endoscopic Surgery.](http://arxiv.org/abs/2401.06013) | 本研究提出了一种基于基础模型的适配器学习方法，针对内窥镜手术中的深度估计问题进行了改进。通过在DINO模型中构建LoRA层，并将其与手术场景的特征结合起来，实现了手术特定的深度估计。实验结果表明，该方法在内窥镜手术中取得了良好的性能。 |
| [^11] | [How does the primate brain combine generative and discriminative computations in vision?.](http://arxiv.org/abs/2401.06005) | 本论文探讨了非人灵长类动物大脑如何在视觉中结合生成式和判别式计算。一个观念强调自下而上的信号流动，通过滤除不相关的变异和转换视觉信息来代表行为上相关的信息；而另一个观念将视觉视为Helmholtz的推理过程。 |
| [^12] | [Combating Adversarial Attacks with Multi-Agent Debate.](http://arxiv.org/abs/2401.05998) | 使用多智能体辩论对抗对抗性攻击可以降低模型的有毒性，同时通过嵌入聚类对对抗性提示内容进行分类可以分析不同模型对不同类型攻击的易受攻击性。 |
| [^13] | [End-to-end Learnable Clustering for Intent Learning in Recommendation.](http://arxiv.org/abs/2401.05975) | 本文提出了一种用于推荐中意图学习的端到端可学习聚类方法ELCRec，该方法解决了现有方法中的复杂优化问题和大规模数据集聚类的可扩展性问题。 |
| [^14] | [Spatial-Aware Deep Reinforcement Learning for the Traveling Officer Problem.](http://arxiv.org/abs/2401.05969) | 这篇论文提出了一种称为SATOP的空间感知深度强化学习方法来解决巡警问题。该方法通过利用停车位、代理和动作之间的空间关系来创建动作的表示，从而动态调整以适应当前可罚款的停车违规行为，并提前计划以提高到达违规行为发生时的可能性。 |
| [^15] | [An attempt to generate new bridge types from latent space of PixelCNN.](http://arxiv.org/abs/2401.05964) | 本论文尝试使用生成式人工智能技术生成新的桥梁类型，通过对潜在空间进行采样，可以在人类原始桥梁类型的基础上组合不同的结构组件，创造具有一定创新能力的新桥梁类型。 |
| [^16] | [Machine Learning Insides OptVerse AI Solver: Design Principles and Applications.](http://arxiv.org/abs/2401.05960) | 本论文详细研究了将机器学习技术应用于华为云的OptVerse AI Solver，主要包括生成复杂实例、训练框架维护实用性和个性化的解算器策略等方面的创新方法。研究旨在缓解数学规划实例稀缺问题，并超越传统优化技术的能力。 |
| [^17] | [Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks.](http://arxiv.org/abs/2401.05949) | 本研究发现上下文学习范式在大型语言模型中存在漏洞，攻击者可以通过污染示范上下文来操控模型行为，而无需进行微调。这项研究设计了一种名为ICLAttack的后门攻击方法，可以通过污染示范样本和提示来使模型按照预定义的意图行事。 |
| [^18] | [Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments.](http://arxiv.org/abs/2401.05946) | 本文提出了一种从Transformer表示中学习认知地图的方法，该方法针对部分观察环境中的路径规划问题提供了一个有效的解决方案。 |
| [^19] | [Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs.](http://arxiv.org/abs/2401.05940) | 本文提出了一种基于代码变异的方法，用于系统评估LLMs的代码理解能力，重点关注代码和描述之间的微妙差异。通过引入代码变异到现有的数据集中，我们可以生成不一致的代码数据，从而评估LLMs的代码理解性能。 |
| [^20] | [DREQ: Document Re-Ranking Using Entity-based Query Understanding.](http://arxiv.org/abs/2401.05939) | DREQ是一种基于实体的密集文档重新排序模型，通过强调与查询相关的实体并减弱不太相关的实体，获得一个查询特定的以实体为中心的文档表示，并结合文本为中心的文档表示进行混合表示，该方法在多个基准测试中优于最先进的重新排序方法。 |
| [^21] | [DiffDA: a diffusion model for weather-scale data assimilation.](http://arxiv.org/abs/2401.05932) | DiffDA是一种用于气象尺度数据同化的扩散模型，通过机器学习的方法将预测状态和稀疏观测同化，生成与观测一致的初始条件，并能对预测进行后处理到未来。 |
| [^22] | [SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully.](http://arxiv.org/abs/2401.05930) | 自我突出式犹豫（SH2）是一种推理时的方法，通过选择预测概率较低的标记，并强调它们的差异，从而帮助语言模型更准确地解码。 |
| [^23] | [CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians.](http://arxiv.org/abs/2401.05925) | CoSSegGaussians是一种紧凑且迅速的3D高斯场景分割方法，通过映射空间和语义特征实现紧凑和可靠的零样本场景分割。 |
| [^24] | [How Teachers Can Use Large Language Models and Bloom's Taxonomy to Create Educational Quizzes.](http://arxiv.org/abs/2401.05914) | 本文介绍了一种利用大型语言模型和布鲁姆税务学派创建教育测验的方法，研究结果表明教师更倾向于使用自动生成的问题撰写测验，并且这些问题的质量不亚于手写版本，甚至有可能提高测验的质量。 |
| [^25] | [Binary Linear Tree Commitment-based Ownership Protection for Distributed Machine Learning.](http://arxiv.org/abs/2401.05895) | 本论文提出了一种基于二进制线性树提交的分布式机器学习所有权保护模型，通过验证计算的完整性和效果，解决了模型所有权的冲突问题，并降低了更新证明的成本。 |
| [^26] | [HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced Diffusion Models.](http://arxiv.org/abs/2401.05870) | HiCAST是一种高度定制的任意风格转换方法，通过引入"风格适配器"和利用潜在扩散模型，可以根据各种语义线索显式定制风格化结果。 |
| [^27] | [Inferring Intentions to Speak Using Accelerometer Data In-the-Wild.](http://arxiv.org/abs/2401.05849) | 通过加速度计数据推断成功和失败的讲话意图，在野外环境中的研究表明加速度计数据中存在有用的信息，但不足以可靠地捕捉讲话意图。 |
| [^28] | [Pushing the Pareto front of band gap and permittivity: ML-guided search for dielectric materials.](http://arxiv.org/abs/2401.05848) | 这项研究通过使用多目标优化方法和机器学习技术，在介电材料研究中取得了突破。研究人员成功地合成和表征了两种新型介电材料，CsTaTeO6和Bi2Zr2O7，为未知材料的寻找提供了高效的工作流程。 |
| [^29] | [Decoding AI's Nudge: A Unified Framework to Predict Human Behavior in AI-assisted Decision Making.](http://arxiv.org/abs/2401.05840) | 本文提出了一个计算框架，可以对不同形式的AI辅助对决策者在AI辅助决策中的影响进行解释性描述。 |
| [^30] | [Revisiting Silhouette: From Micro to Macro Aggregation.](http://arxiv.org/abs/2401.05831) | 本文提出了一种新的聚合策略，用于评估聚类质量。通过对聚类级别的轮廓得分进行平均，并在此基础上对所有聚类的得分进行宏观平均，我们提出的宏观平均轮廓得分对于聚类不均衡和背景噪声是稳健的。 |
| [^31] | [Hallucination Benchmark in Medical Visual Question Answering.](http://arxiv.org/abs/2401.05827) | 这项研究创建了医学图像的幻觉基准评估，并全面评估了当前最先进的模型，揭示了幻觉现象在临床环境中的限制和各种提示策略的有效性。 |
| [^32] | [Towards Goal-Oriented Agents for Evolving Problems Observed via Conversation.](http://arxiv.org/abs/2401.05822) | 本研究通过训练一个可以通过与用户对话解决演化问题的聊天机器人，提出了一种应用对话式DQN智能体解决演化问题的架构，并探索了课程学习和改变奖励函数等训练方法对模型性能的影响。 |
| [^33] | [Cheetah: Bridging the Gap Between Machine Learning and Particle Accelerator Physics with High-Speed, Differentiable Simulations.](http://arxiv.org/abs/2401.05815) | Cheetah是一种高速可微分模拟工具，可以减少计算时间并实现快速收集大规模数据集。它能够促进加速器调优和系统识别，并与机器学习工具无缝集成。 |
| [^34] | [Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages.](http://arxiv.org/abs/2401.05811) | 本文引入了对比对齐指令（AlignInstruct），通过使用统计词对齐构建的跨语言鉴别器实现了跨语言监督，解决了机器翻译中的两个挑战：将支持的语言扩展到未知语言和低资源语言中数据缺乏的问题。实验结果表明，LLMs通过MTInstruct可以有效地翻译未知语言，并且使用AlignInstruct在涉及英语的48个翻译方向上能够持续改善翻译质量。基于鉴别器的指令优于生成型指令。 |
| [^35] | [Graph Spatiotemporal Process for Multivariate Time Series Anomaly Detection with Missing Values.](http://arxiv.org/abs/2401.05800) | 该论文介绍了一种名为GST-Pro的框架，利用图形时空过程和异常评分器来解决在采样不规则的多变量时间序列中检测异常的挑战。 |
| [^36] | [Designing Heterogeneous LLM Agents for Financial Sentiment Analysis.](http://arxiv.org/abs/2401.05799) | 本研究提出了一种设计框架，使用动态的异构LLM代理，来改进金融情绪分析的准确性，并在实验中取得了令人满意的结果。 |
| [^37] | [Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems.](http://arxiv.org/abs/2401.05778) | 这篇论文介绍了大型语言模型系统的风险分类、缓解和评估基准，调查并分析了与每个模块相关的潜在风险。 |
| [^38] | [Knowledge Translation: A New Pathway for Model Compression.](http://arxiv.org/abs/2401.05772) | 本文提出了一种新的知识转化（KT）框架，通过训练一个“翻译”模型来接收较大模型的参数并生成压缩参数，从而实现模型压缩，而无需重新训练或施加架构限制。 |
| [^39] | [A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism.](http://arxiv.org/abs/2401.05749) | 互联网上的大量内容是通过机器翻译多向翻译的，其低质量可能会对使用多语言大型语言模型进行训练产生严重影响。 |
| [^40] | [Consistent Query Answering for Existential Rules under Tuple-Deletion Semantics.](http://arxiv.org/abs/2401.05743) | 我们研究了存在规则下基于元组删除语义的一致性查询回答，并确定了一般类别和若干子类别的数据复杂性。我们发现在某些情况下这些问题是可处理的，甚至可以进行一阶重写。这项研究提出了新的查询重写技术，可用于实现容忍不一致性的查询回答系统。 |
| [^41] | [Enhancing Contrastive Learning with Efficient Combinatorial Positive Pairing.](http://arxiv.org/abs/2401.05730) | 本论文提出了一种名为ECPP（高效组合正样本配对）的多视图策略，通过增加视图数量、采用小尺寸视图和修改负采样，提高了对比学习的学习速度和性能。通过将ECPP应用于SimCLR等方法，本研究验证了ECPP的有效性。 |
| [^42] | [R-BI: Regularized Batched Inputs enhance Incremental Decoding Framework for Low-Latency Simultaneous Speech Translation.](http://arxiv.org/abs/2401.05700) | 本文介绍了一种名为“正则化批量输入”的新颖策略，通过增强输入多样性来减轻低延迟同时语音翻译中的输出错误。 |
| [^43] | [Deep Learning Meets Mechanism Design: Key Results and Some Novel Applications.](http://arxiv.org/abs/2401.05683) | 本文介绍了深度学习与机制设计的结合，探讨了使用深度学习方法在无法同时满足所有期望特性的情况下，学习近似满足特性要求的机制。 |
| [^44] | [Use of Graph Neural Networks in Aiding Defensive Cyber Operations.](http://arxiv.org/abs/2401.05680) | 本文将研究如何利用图神经网络来辅助打破网络攻击生命周期的每个阶段，通过处理和学习来自异构网络威胁数据，以增强防御措施的有效性。 |
| [^45] | [EsaCL: Efficient Continual Learning of Sparse Models.](http://arxiv.org/abs/2401.05667) | EsaCL是一种高效稀疏模型持续学习方法，通过自动修剪冗余参数并避免重新训练，解决了持续学习中存储和计算需求增加的问题。 |
| [^46] | [Towards Conversational Diagnostic AI.](http://arxiv.org/abs/2401.05654) | 本文介绍了一种基于大型语言模型的人工智能系统AMIE，该系统利用自我对战的模拟环境和自动化反馈机制进行诊断对话，并且通过评估病史采集、诊断准确性、管理推理、沟通技巧和同理心等维度性能，与初级保健医生进行了比较。 |
| [^47] | [DrawTalking: Building Interactive Worlds by Sketching and Speaking.](http://arxiv.org/abs/2401.05631) | 用户通过草图和语言建立互动世界的交互式方法，具有用户控制和灵活性，无需编程即可实现编程功能。适用于各种创造性探索性场景。 |
| [^48] | [The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models.](http://arxiv.org/abs/2401.05618) | 本文研究了在大型语言模型中使用简洁的思维链提示对问题求解的影响，实验结果表明简洁性不仅降低了回答长度，且对问题解决性能影响可以忽略。然而在数学问题上有一定的性能下降。这对AI系统工程师和研究人员都有实际意义。 |
| [^49] | [Graph Q-Learning for Combinatorial Optimization.](http://arxiv.org/abs/2401.05610) | 本论文提出并证明了图神经网络可以应用于解决组合优化问题。通过将优化过程形式化为一个顺序决策问题，并使用Q-Learning训练的GNNs来学习策略，可以达到接近最先进的基于启发式求解器的性能，同时只需使用部分参数和训练时间。 |
| [^50] | [REBUS: A Robust Evaluation Benchmark of Understanding Symbols.](http://arxiv.org/abs/2401.05604) | 提出了一种用于评估多模态大规模语言模型在rebus谜题上性能的新的基准测试。发现专有模型表现优于其他测试模型，但最佳模型的准确率仅为24%。该基准测试可用于识别知识上的主要缺陷。 |
| [^51] | [POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Translation.](http://arxiv.org/abs/2401.05596) | POMP是一种新颖的方法，使用动态的、基于抽样的多辅助语言图形，提高了语言模型在低资源语言中的翻译能力。 |
| [^52] | [FourCastNeXt: Improving FourCastNet Training with Limited Compute.](http://arxiv.org/abs/2401.05584) | 本研究提出了改进的方法，可以使用仅需基线要求的1%计算资源训练FourCastNet，并且保持了与基线相当或甚至更好的模型性能。 |
| [^53] | [Innate-Values-driven Reinforcement Learning for Cooperative Multi-Agent Systems.](http://arxiv.org/abs/2401.05572) | 本文提出了一个先天价值驱动增强学习（IVRL）模型，用于描述多智能体在合作中的复杂行为。该模型通过建立智能体对群体效用和系统成本的认知，满足其合作伙伴的需求，支持其社区并融入人类社会。 |
| [^54] | [Siamese Networks with Soft Labels for Unsupervised Lesion Detection and Patch Pretraining on Screening Mammograms.](http://arxiv.org/abs/2401.05570) | 这项研究提出了一种基于软标签的孪生网络方法，利用对侧乳房X线片训练神经网络，以在无监督情况下区分异常病变和背景组织。实验证明，通过欧氏距离衍生的软标签，可以有效地区分医学成像中的病变。 |
| [^55] | [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training.](http://arxiv.org/abs/2401.05566) | 该论文研究了在大型语言模型中训练并保持持久的欺骗性行为，这种行为无法被当前的安全训练技术移除。 |
| [^56] | [CodePrompt: Improving Source Code-Related Classification with Knowledge Features through Prompt Learning.](http://arxiv.org/abs/2401.05544) | CodePrompt是一种利用Prompt学习和注意机制技术改进源代码相关分类任务的新方法。它能够提取源代码和相关文本中的丰富知识以提高准确性，并且减少了计算成本。 |
| [^57] | [Improving the Accuracy and Interpretability of Random Forests via Forest Pruning.](http://arxiv.org/abs/2401.05535) | 通过森林修剪方法，本研究提出了一种兼顾随机森林准确性和决策树可解释性的方法。实验证明，在大多数情景下，这种方法能够显著提高随机森林的性能。 |
| [^58] | [Current Effect-eliminated Optimal Target Assignment and Motion Planning for a Multi-UUV System.](http://arxiv.org/abs/2401.05521) | 提出了一种创新算法CBNNTAP，通过整合生物启发的神经网络和目标分配组件，解决了多UUV系统中由海洋电流引入的复杂性和挑战，并有效消除电流对运动规划和目标分配的影响。 |
| [^59] | [From Pampas to Pixels: Fine-Tuning Diffusion Models for Ga\'ucho Heritage.](http://arxiv.org/abs/2401.05520) | 本文研究了对巴西Rio Grande do Sul（RS）地区的文化遗产进行微调的扩散模型的潜力，展示了这些模型在代表和保护多样化的独特方面方面的能力。 |
| [^60] | [Correlated Quantization for Faster Nonconvex Distributed Optimization.](http://arxiv.org/abs/2401.05518) | 本研究利用相关量化器改进了MARINA算法，通过使用加权Hessian方差进行原始分析，并扩展了MARINA的理论框架，使其适用于更广泛的压缩器范围。 |
| [^61] | [FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D Neural Radiance Fields.](http://arxiv.org/abs/2401.05516) | FPRF是一种用于大规模三维神经辐射场的前馈逼真风格迁移方法，它实现了对大规模三维场景的高效风格化，并在保持多视角外观一致性的同时支持多风格参考图像和用户控制。 |
| [^62] | [Optimized Ensemble Model Towards Secured Industrial IoT Devices.](http://arxiv.org/abs/2401.05509) | 本文提出了一种面向安全工业物联网设备的优化集成模型，将贝叶斯优化-高斯过程与集成树学习模型相结合，以提高工业物联网环境中入侵和攻击检测的性能。 |
| [^63] | [InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks.](http://arxiv.org/abs/2401.05507) | InfiAgent-DABench是第一个评估基于LLM的代理在数据分析任务中的基准测试，包括DAEval数据集和代理框架。对23个最先进的LLMs进行的基准测试揭示了当前数据分析任务中的挑战。 |
| [^64] | [Diversity-aware clustering: Computational Complexity and Approximation Algorithms.](http://arxiv.org/abs/2401.05502) | 本研究讨论了多样性感知聚类问题，在选择聚类中心时要考虑多个属性，同时最小化聚类目标。我们提出了针对不同聚类目标的参数化近似算法，这些算法在保证聚类质量的同时，具有紧确的近似比。 |
| [^65] | [Population Graph Cross-Network Node Classification for Autism Detection Across Sample Groups.](http://arxiv.org/abs/2401.05478) | 本文提出了一种新颖的跨网络节点分类方法OTGCN，利用图卷积网络和最优输运策略，可以在不同数据采集地点的样本之间纠正领域漂移，并且在自闭症谱系障碍的分类上取得了有效的结果。 |
| [^66] | [Standardizing Your Training Process for Human Activity Recognition Models: A Comprehensive Review in the Tunable Factors.](http://arxiv.org/abs/2401.05477) | 本文回顾了可穿戴人类活动识别领域的深度学习研究，并总结了各个研究所采用的训练程序。研究发现存在缺乏详细训练协议的趋势，同时利用控制变量方法评估了关键可调组件对跨主体泛化的影响。 |
| [^67] | [CADgpt: Harnessing Natural Language Processing for 3D Modelling to Enhance Computer-Aided Design Workflows.](http://arxiv.org/abs/2401.05476) | CADgpt是一个创新性的插件，通过将自然语言处理与Rhino3D集成，在计算机辅助设计中简化界面，使初学者能够通过直观的自然语言命令执行复杂的3D建模任务。它提供了更具包容性和互动性的教育环境，并将复杂的设计工具普及到更广泛的学生群体中，有助于设计教育的民主化发展。 |
| [^68] | [Introducing New Node Prediction in Graph Mining: Predicting All Links from Isolated Nodes with Graph Neural Networks.](http://arxiv.org/abs/2401.05468) | 这项研究引入了一个新的问题——新节点预测，即从以前与图不相连的孤立节点中预测所有连接。通过基于深度图神经网络的架构，实验结果表明可以解决这一具有挑战性的问题。 |
| [^69] | [Machine Teaching for Building Modular AI Agents based on Zero-shot Learners.](http://arxiv.org/abs/2401.05467) | 这篇论文提出了一种机器教学方法，通过利用迭代机器教学和任务特定的替代模型，增强了利用大语言模型作为零样本学习器的模块化AI代理的鲁棒性和性能。 |
| [^70] | [The two-way knowledge interaction interface between humans and neural networks.](http://arxiv.org/abs/2401.05461) | 该论文构建了一个人类和神经网络之间的双向知识交互界面，使用视觉概念和它们之间的关系作为“语言”，以实现人类和神经网络之间的知识交流。该界面可以使神经网络向人类提供直观的推理解释，同时人类可以修改其中的偏见。 |
| [^71] | [Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security.](http://arxiv.org/abs/2401.05459) | 这项研究探讨了个人LLM代理的能力、效率和安全性，旨在提升智能个人助理的实用性和可扩展性。 |
| [^72] | [CoLafier: Collaborative Noisy Label Purifier With Local Intrinsic Dimensionality Guidance.](http://arxiv.org/abs/2401.05458) | CoLafier是一种使用局部内在维度（LID）进行带有噪声标签学习的方法。它通过利用LID-dis和LID-gen两个子网络，其中LID-dis使用样本的特征和标签来预测标签，产生增强的内部表示。与LID-dis相反，LID-gen仅使用样本的特征。CoLafier利用每个实例的两个增强视图同时输入两个子网络，利用LID分数来分配标签。 |
| [^73] | [Dimensionality-Aware Outlier Detection: Theoretical and Experimental Analysis.](http://arxiv.org/abs/2401.05453) | 这篇论文提出了一种维度感知的异常检测方法DAO，通过全面实验验证了其在800多个数据集上显著优于其他三种流行的异常检测方法。 |
| [^74] | [Can ChatGPT Compute Trustworthy Sentiment Scores from Bloomberg Market Wraps?.](http://arxiv.org/abs/2401.05447) | 该研究使用了Bloomberg金融市场摘要数据集，利用ChatGPT和两阶段提示方法，发现全球新闻头条与股市走势之间存在显著的情绪相关性，该相关性在不同时间段内有所变化，并且在多个股票市场中均具有鲁棒性。研究提供了一个平衡新信息反应性和相关性的最佳时间段的估计。 |
| [^75] | [Self-supervised Learning for Electroencephalogram: A Systematic Survey.](http://arxiv.org/abs/2401.05446) | 这篇论文系统综述了自我监督学习在脑电图中的应用，通过设计良好的预训练任务来提取无标签样本的表示，解决了脑电信号标签的问题和个体之间的变化带来的挑战。 |
| [^76] | [Fully Spiking Actor Network with Intra-layer Connections for Reinforcement Learning.](http://arxiv.org/abs/2401.05444) | 本研究提出了一种基于层内连接的全脉冲行为网络，利用特殊的神经形态硬件实现了较低能耗的人工智能。该方法在实现控制任务上表现出可比较性能，并解决了使用脉冲频率作为输出所带来的浮点矩阵运算问题。 |
| [^77] | [LLM4PLC: Harnessing Large Language Models for Verifiable Programming of PLCs in Industrial Control Systems.](http://arxiv.org/abs/2401.05443) | LLM4PLC通过利用用户反馈和外部验证工具，如语法检查器、编译器和SMV验证器，来指导生成，同时采用提示工程和模型微调的方法，提高了大型语言模型在工业控制系统中可编程逻辑控制器（PLC）的生成能力。 |
| [^78] | [Functional Graphical Models: Structure Enables Offline Data-Driven Optimization.](http://arxiv.org/abs/2401.05442) | 功能图模型（FGMs）通过结构实现了样本高效的数据驱动优化。 |
| [^79] | [Representation Learning for Wearable-Based Applications in the Case of Missing Data.](http://arxiv.org/abs/2401.05437) | 本论文研究了可穿戴应用中表示学习的问题，特别是在缺失数据情况下。作者通过比较Transformer模型和统计方法的性能，发现Transformer模型在变化频繁的信号的缺失数据填充方面表现优秀。此研究为基于掩码的自监督学习任务的设计和开发提供了洞察。 |
| [^80] | [ECGformer: Leveraging transformer for ECG heartbeat arrhythmia classification.](http://arxiv.org/abs/2401.05434) | ECGformer是一种利用transformer架构的模型，用于对心电图数据中的心律失常进行分类。 |
| [^81] | [Enhancing Essay Scoring with Adversarial Weights Perturbation and Metric-specific AttentionPooling.](http://arxiv.org/abs/2401.05433) | 本研究使用DeBERTa模型，结合对抗性训练和度量特定的注意力池化等创新技术，提出了一种增强自动评分工具的方法，用于改进针对英语学习者的写作能力评估。 |
| [^82] | [TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks.](http://arxiv.org/abs/2401.05432) | TEN-GUARD提出了一种使用张量分解方法检测深度神经网络中后门攻击的新方法，相对于现有的方法具有多个优势，包括能够同时分析多个模型，在各种网络架构上工作，不对触发器的性质做任何假设，并且计算效率高。 |
| [^83] | [TRLS: A Time Series Representation Learning Framework via Spectrogram for Medical Signal Processing.](http://arxiv.org/abs/2401.05431) | TRLS是一种通过声谱图进行时间序列表示学习的医学信号处理框架，通过利用时间频率RNN从增强声谱图中提取出更多信息，并在医学信号分类中显示出更好的性能。 |
| [^84] | [CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in Human Activity Recognition.](http://arxiv.org/abs/2401.05426) | 本论文提出了一个实用的框架用于在人体活动识别任务中高效利用数据，同时考虑传感器模态和采样率的优化，通过设计的可训练参数来指导传感器模态和采样率的选择。 |
| [^85] | [Machine Learning (ML)-assisted Beam Management in millimeter (mm)Wave Distributed Multiple Input Multiple Output (D-MIMO) systems.](http://arxiv.org/abs/2401.05422) | 这项研究探讨了在毫米波频段的分布式多输入多输出(D-MIMO)系统中，通过机器学习辅助进行波束管理，并证明了波束推断的性能优势。 |
| [^86] | [ANALYTiC: Understanding Decision Boundaries and Dimensionality Reduction in Machine Learning.](http://arxiv.org/abs/2401.05418) | 本论文通过将降维和决策边界应用于现有的主动学习方法，突显数据中的模式和聚类，并通过实验验证了这种方法在提高轨迹标记效率和准确性方面的潜力。 |
| [^87] | [Wavelet Dynamic Selection Network for Inertial Sensor Signal Enhancement.](http://arxiv.org/abs/2401.05416) | 该论文提出了一种名为WDSNet的小波动态选择网络，用于智能选择适当的小波基函数以增强惯性传感器信号。此外，论文还提出了一种类别表示机制，用于提高小波基函数的选择能力。 |
| [^88] | [Spatial-Related Sensors Matters: 3D Human Motion Reconstruction Assisted with Textual Semantics.](http://arxiv.org/abs/2401.05412) | 该研究使用稀疏惯性测量单元(IMUs)在人体上进行运动重建，通过引入文本语义进行传感器数据对齐和特征加权，实现了精确的运动重建。实验结果显示与现有方法相比，该方法在多个指标上取得了显著的改进。 |
| [^89] | [Image-based Data Representations of Time Series: A Comparative Analysis in EEG Artifact Detection.](http://arxiv.org/abs/2401.05409) | 本文通过针对脑电图数据中的伪迹检测和分类，对十一个深度学习架构在六种常用表示方法上进行了比较分析，并发现某些表示方法在突出数据的信噪比方面更有效。 |
| [^90] | [RFRL Gym: A Reinforcement Learning Testbed for Cognitive Radio Applications.](http://arxiv.org/abs/2401.05406) | RFRL Gym是一个用于认知无线电应用的强化学习测试平台，可以帮助开发和测试RFRL技术，模拟无线电频谱环境，并实验不同的频谱感知技术。 |
| [^91] | [The Key Artificial Intelligence Technologies in Early Childhood Education: A Review.](http://arxiv.org/abs/2401.05403) | 本研究对早期儿童教育中关键AI技术进行了最新和深入的综述，包括基于AI的机器人和AI技术改善自闭症儿童的社交互动等。这对初学者来说是一份适用的综述材料。 |
| [^92] | [Vector Field Oriented Diffusion Model for Crystal Material Generation.](http://arxiv.org/abs/2401.05402) | 提出了一种基于向量场定向扩散模型的晶体材料生成方法，该方法通过利用几何等变GNN同时考虑原子位置和晶体晶格，提供了一种能够更全面评估模型能力的新生成指标。实验证明了该扩散模型的重要性和有效性。 |
| [^93] | [Domain Similarity-Perceived Label Assignment for Domain Generalized Underwater Object Detection.](http://arxiv.org/abs/2401.05401) | 该论文介绍了领域相似性感知标签分配（DSP）的概念，通过将每个图像的领域标签视为其与指定领域的相似性，使用领域特定的数据增强技术，在水下跨领域物体检测基准S-UODAC2020上达到了最先进的结果。 |
| [^94] | [Collaborative Learning with Artificial Intelligence Speakers (CLAIS): Pre-Service Elementary Science Teachers' Responses to the Prototype.](http://arxiv.org/abs/2401.05400) | 该研究证明了人工智能不仅可以作为学习工具，还可以作为一个智能代理，与人类一起参与协同学习，改变科学课堂中的认识实践。通过开发一个名为CLAIS的实物教学系统，结合人类学习者和人工智能说话者的协同学习过程，该系统成功应用在预备小学科学教师的课程中，并得到积极评价。 |
| [^95] | [Automated Assessment of Students' Code Comprehension using LLMs.](http://arxiv.org/abs/2401.05399) | 本研究探索了使用LLMs自动评估学生对代码的理解能力的潜力，并发现LLMs在比较学生的解释和专家解释方面具有作用。 |
| [^96] | [GeoAI in Social Science.](http://arxiv.org/abs/2401.05398) | GeoAI在社会科学研究中取得了重要进展，通过利用人工智能和地理大数据填补数据和知识空白，并促进了研究方法的融合和应用领域的拓展。 |
| [^97] | [SRNI-CAR: A comprehensive dataset for analyzing the Chinese automotive market.](http://arxiv.org/abs/2401.05395) | SRNI-CAR是一份用于分析中国汽车市场的综合数据集，填补了现有汽车行业数据集覆盖范围有限的缺口，对于提高预测准确性、扩大商业应用范围、指导政策制定与监管以及推动汽车行业的学术研究具有重要影响。 |
| [^98] | [AT-2FF: Adaptive Type-2 Fuzzy Filter for De-noising Images Corrupted with Salt-and-Pepper.](http://arxiv.org/abs/2401.05392) | 本文介绍了一种适应性类型-2模糊滤波器，用于去除图像中盐和胡椒噪声。通过自适应阈值和加权均值滤波器两个步骤，该方法能够保留图像的特征，并在与其他不同滤波方法的比较中展示出良好的去噪效果。 |
| [^99] | [Efficient LLM inference solution on Intel GPU.](http://arxiv.org/abs/2401.05391) | 本文提出了一种在Intel GPU上高效的LLM推理解决方案，通过简化LLM解码层和引入分段KV缓存策略，实现了低延迟和高吞吐量。 |
| [^100] | [From Good to Great: Improving Math Reasoning with Tool-Augmented Interleaf Prompting.](http://arxiv.org/abs/2401.05384) | 本文研究了大型语言模型和工具辅助语言模型在数学推理任务中的表现，提出了IMP-TIP框架，该框架采用了“由好到优”的思想并通过工具辅助交错提示来选择最准确的答案。 |
| [^101] | [Classical Sorting Algorithms as a Model of Morphogenesis: self-sorting arrays reveal unexpected competencies in a minimal model of basal intelligence.](http://arxiv.org/abs/2401.05375) | 该论文研究了经典排序算法作为形态发生学模型，在极简模型中展示出了意外的基础智能能力。 |
| [^102] | [Dynamic Spiking Graph Neural Networks.](http://arxiv.org/abs/2401.05373) | 本文提出了一个名为"动态尖峰图神经网络"（DSGNN）的框架，它将尖峰神经网络（SNNs）与图神经网络（GNNs）结合起来，以解决动态图表示学习中的复杂性和内存开销问题。DSGNN通过动态调整尖峰神经元的状态和连接权重，在传播过程中保持图结构信息的完整性。 |
| [^103] | [Autoregressive fragment-based diffusion for pocket-aware ligand design.](http://arxiv.org/abs/2401.05370) | 引入了AutoFragDiff，一种基于片段的自回归扩散模型，用于生成3D分子结构。通过预测分子片段的原子类型和空间坐标，改善了生成分子的局部几何结构，并保持高的结合亲和力。如果提供起始分子骨架，模型还可以进行扩展。 |
| [^104] | [DualTeacher: Bridging Coexistence of Unlabelled Classes for Semi-supervised Incremental Object Detection.](http://arxiv.org/abs/2401.05362) | 这项工作提出了一种名为DualTeacher的算法，用于解决半监督增量目标检测问题。该算法能够在仅有少量标注数据和大量未标注数据的情况下，逐渐学习新的目标类别，同时避免对旧类别的遗忘。 |
| [^105] | [Developing a Resource-Constraint EdgeAI model for Surface Defect Detection.](http://arxiv.org/abs/2401.05355) | 开发了一种适用于表面缺陷检测的资源受限EdgeAI模型，可以在边缘设备上进行训练，具有出色的性能。 |
| [^106] | [Adaptive operator selection utilising generalised experience.](http://arxiv.org/abs/2401.05350) | 这篇论文提出了基于强化学习的一种新方法，用于解决组合优化问题中的探索和开发平衡问题，以及利用广义经验来发展通用框架。 |
| [^107] | [Task tree retrieval from FOON using search algorithms.](http://arxiv.org/abs/2401.05346) | 本文介绍了从FOON中使用搜索算法检索任务树的方法，以帮助机器人自动执行各种复杂的任务。 |
| [^108] | [Most discriminative stimuli for functional cell type identification.](http://arxiv.org/abs/2401.05342) | 本文提出了一种使用最具区分性刺激物的优化聚类方法，成功地识别了小鼠视网膜、恒河猴视网膜和猕猴V4视觉区的功能细胞类型。 |
| [^109] | [RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation.](http://arxiv.org/abs/2401.04679) | RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。 |
| [^110] | [DebugBench: Evaluating Debugging Capability of Large Language Models.](http://arxiv.org/abs/2401.04621) | 该论文介绍了一个名为DebugBench的LLM调试基准，用于评估大型语言模型的调试能力。研究发现闭源模型与人类相比具有较低的调试性能，而开源模型未能达到合格率。 |
| [^111] | [Learning Racing From an AI Coach: Effects of Multimodal Autonomous Driving Explanations on Driving Performance, Cognitive Load, Expertise, and Trust.](http://arxiv.org/abs/2401.04206) | 本研究测试了一种AI驾驶教练的解释对驾驶表现、认知负荷、专业知识和信任的影响。结果显示，AI驾驶教练对于教授新手驾驶技能是有帮助的，并且信息类型和呈现方式对表现结果有影响。 |
| [^112] | [The Concept of the Tactile Signature System for Individuals with Visual Impairments.](http://arxiv.org/abs/2401.04126) | 盲人触觉签名系统是一种无障碍和有效的系统，通过触觉互动和语音算法引导，赋予视障人士创建个性化手写签名的能力，促进了他们的独立和包容。 |
| [^113] | [Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series.](http://arxiv.org/abs/2401.03955) | 本论文介绍了一种名为微小时间混合器 (TTMs) 的预训练模型，该模型针对多变量时间序列的零/少样本预测进行了优化。与大型预训练模型相比，TTMs模型更小、更快，并考虑了跨通道相关性，能够在短时间内进行有效的预测。 |
| [^114] | [TIER: Text-Image Encoder-based Regression for AIGC Image Quality Assessment.](http://arxiv.org/abs/2401.03854) | 本文提出了一种基于文本图像编码器的回归（TIER）框架，用于评估AI生成图像的质量。与现有方法直接从图像预测得分不同，TIER通过综合利用文本和图像特征来提高AIGCIQA的性能。 |
| [^115] | [Long-term Safe Reinforcement Learning with Binary Feedback.](http://arxiv.org/abs/2401.03786) | 本论文提出了一种长期安全的强化学习算法LoBiSaRL，该算法针对具有二进制安全反馈和未知随机状态转换函数的约束马尔可夫决策过程（CMDPs），通过最大化奖励的方式优化策略，同时以高概率确保每个回合中代理只执行安全的状态-动作对。 |
| [^116] | [Machine Learning Applications in Traumatic Brain Injury: A Spotlight on Mild TBI.](http://arxiv.org/abs/2401.03621) | 该论文回顾了在创伤性脑损伤（TBI）中应用机器学习的最新技术，特别关注轻度脑损伤（mTBI）。虽然已经有很多机器学习技术用于诊断，但对预测预后的研究相对较少。 |
| [^117] | [Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT.](http://arxiv.org/abs/2401.03302) | 本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤，并解决了在罕见情况下的肿瘤检测问题。研究使用了来自国家脑映射实验室的数据集，通过修改样本数量和患者分布，使模型能够应对真实世界场景中的异常情况。 |
| [^118] | [Optimal Chaining of Vehicle Plans with Time Windows.](http://arxiv.org/abs/2401.02873) | 本论文提出了一种解决带有时间窗口的车辆路径问题的最优链路方法，考虑了计划的时间灵活性，并通过实证结果证明了该方法在解决静态拨打车问题时的优越性。 |
| [^119] | [WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation.](http://arxiv.org/abs/2312.14187) | 本文提出了WaveCoder，一个广泛和多功能的改进指令调优模型，通过将指令数据分类并利用LLM框架生成多样的高质量指令数据，以提高调优模型的效果和泛化能力。 |
| [^120] | [Robust Stochastic Graph Generator for Counterfactual Explanations.](http://arxiv.org/abs/2312.11747) | 这篇论文介绍了一种新颖的鲁棒随机图生成器（RSGG-CE）用于反事实解释，该方法在生成类似于原始图形的新图形的同时，基于潜在预测模型产生不同的结果。此方法利用生成机制生成反事实实例，具有较高的解释能力，并在其他领域展现出卓越的成果。 |
| [^121] | [WAVER: Writing-style Agnostic Text-Video Retrieval via Distilling Vision-Language Models Through Open-Vocabulary Knowledge.](http://arxiv.org/abs/2312.09507) | WAVER是一种通过开放词汇知识进行跨域知识蒸馏的框架，用于解决视频描述中不同写作风格的问题。它利用预训练的视觉-语言模型，并采用隐式知识蒸馏方法，将文本知识从教师模型传递给学生模型。 |
| [^122] | [Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning.](http://arxiv.org/abs/2312.05720) | 本文引入了一种创新的方法，在联邦学习中利用语言模型的池化层输入来实现对隐私攻击的改进。通过恢复池化层输入，这种方法能够在不同的批处理大小下提供更高的文本恢复率，从而提供更细致和有效的见解。 |
| [^123] | [Navigating Privacy and Copyright Challenges Across the Data Lifecycle of Generative AI.](http://arxiv.org/abs/2311.18252) | 这项研究探讨了生成性人工智能中数据隐私和版权保护的多方面挑战，并提出了将技术创新与伦理前瞻相结合的综合方法，旨在全面解决这些问题。 |
| [^124] | [Scale-Dropout: Estimating Uncertainty in Deep Neural Networks Using Stochastic Scale.](http://arxiv.org/abs/2311.15816) | 本文提出了一种用于二值神经网络（BNNs）的规模丢弃的新型正则化技术，并基于蒙特卡洛建立BayNN模型以高效地估计不确定性。 |
| [^125] | [Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series.](http://arxiv.org/abs/2311.13326) | 本文通过在复杂时间序列数据上探索课程学习和模仿学习的方法，发现课程学习是改善复杂时间序列控制任务性能的新途径，而模仿学习也应该被应用。 |
| [^126] | [ALYMPICS: Language Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents.](http://arxiv.org/abs/2311.03220) | 本文介绍了Alympics，一个利用大型语言模型代理人进行博弈论研究的系统性模拟框架。通过模拟人类战略互动，框架能够定性和定量地分析游戏决定因素、策略和结果，并对代理人在战略决策场景中的表现进行评估。 |
| [^127] | [CausalCite: A Causal Formulation of Paper Citations.](http://arxiv.org/abs/2311.02790) | CausalCite是一种以因果推断为基础的论文引用公式化方法，通过对文本进行嵌入和相似样本的提取来评估论文的重要性，并在各个标准上展示了其有效性。 |
| [^128] | [Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models.](http://arxiv.org/abs/2310.13191) | 本文提出了一种适应性知识保留剪枝策略，旨在提高语言模型对抗攻击的鲁棒性，并在剪枝过程中保留更多的预训练知识。与其他方法相比，该方法展现了更好的平衡。 |
| [^129] | [CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model.](http://arxiv.org/abs/2310.06266) | CodeFuse-13B是一个预训练的多语言代码大型语言模型，专为代码相关任务设计，支持超过40种编程语言，并通过使用高质量的预训练数据集以及大量实验的验证，展现了其在多语言输入下的有效性。 |
| [^130] | [A new economic and financial theory of money.](http://arxiv.org/abs/2310.04986) | 这篇论文通过根本性的改革，将电子货币纳入经济与金融理论，提出了一种新的理论框架，包括电子货币的估值基于宏观经济理论和货币政策的基本方程，以及电子货币管理公司作为协调次经济体货币和财政政策的实体。该研究避免使用普遍但不适当的指数风险模型，而是采用多时间尺度的模型。 |
| [^131] | [Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems.](http://arxiv.org/abs/2309.11526) | 该论文介绍了一种基于似然的传感器校准方法，可以在物联网系统中实现专家支持的分布式学习算法。通过对模拟和实际测量数据的评估，证明了该方法的有效性和改进效果。 |
| [^132] | [Task Selection and Assignment for Multi-modal Multi-task Dialogue Act Classification with Non-stationary Multi-armed Bandits.](http://arxiv.org/abs/2309.09832) | 本文研究了多模态多任务对话行为分类任务，并使用非平稳多臂赌博机和高斯先验的折扣汤普森采样方法进行任务选择和分配。实验结果表明，我们的方法能够有效识别任务效用，在训练过程中避免无用或有害的任务，并在UAR方面具有显著优势。 |
| [^133] | [Heterogeneous Generative Knowledge Distillation with Masked Image Modeling.](http://arxiv.org/abs/2309.09571) | 本文提出了一种基于遮掩图像建模的异构生成知识蒸馏（H-GKD）方法，可以高效地从大型Transformer模型传递知识给小型CNN模型。通过训练一个带有稀疏卷积的UNet风格学生网络，桥接了Transformer模型和CNN模型之间的差距，实现了视觉表示的有效模拟。 |
| [^134] | [Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning.](http://arxiv.org/abs/2309.03581) | 本文提出了一个以人为中心的交互式超参数优化方法，通过应用偏好学习来解决多目标机器学习中的问题。 |
| [^135] | [ProAgent: Building Proactive Cooperative AI with Large Language Models.](http://arxiv.org/abs/2308.11339) | ProAgent是一个利用大型语言模型构建的主动合作的AI框架，能够预测队友的决策并为自己制定增强计划，具有高度的模块化和可解释性。 |
| [^136] | [Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges.](http://arxiv.org/abs/2308.00031) | 这篇论文调查了在生成人工智能中应用强化学习的现状、机会和开放研究问题。作者主要讨论了三种应用类型：无特定目标的生成方式、同时最大化目标函数的输出生成方式以及将无法通过目标函数捕捉的期望特征嵌入生成过程的方式。这个新兴领域中存在着丰富的机会和挑战。 |
| [^137] | [A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions.](http://arxiv.org/abs/2307.05638) | 本综述调查了深度迁移学习在工业时间序列异常检测中的使用。深度迁移学习通过利用相关任务的知识和考虑数据分布的变化，解决了仅有少量或没有附加标记数据情况下的新任务。 |
| [^138] | [Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating.](http://arxiv.org/abs/2307.02730) | 这篇论文提出了一种多模态和多任务的花样滑冰数据集（MMFS），包含了256个类别的动作得分和空间和时间标签。该数据集的关键贡献是首次引入了独立的空间和时间分类，并首次使用骨骼模态进行精细动作质量评估。 |
| [^139] | [TL-nvSRAM-CIM: Ultra-High-Density Three-Level ReRAM-Assisted Computing-in-nvSRAM with DC-Power Free Restore and Ternary MAC Operations.](http://arxiv.org/abs/2307.02717) | TL-nvSRAM-CIM是一种新颖的存储器内计算方案，利用超高密度的三级ReRAM辅助计算来解决大规模神经网络模型中权重容量不足的问题，并采用了零直流功耗恢复和三态MAC操作来提高能效和保持准确性。 |
| [^140] | [Medication Recommendation via Domain Knowledge Informed Deep Learning.](http://arxiv.org/abs/2305.19604) | 提出一种基于动态领域知识的药物推荐框架DKINet，将领域知识与患者临床表现相结合，此为首次实验。 |
| [^141] | [Heterogeneous Value Evaluation for Large Language Models.](http://arxiv.org/abs/2305.17147) | 本文提出了一种自动对齐评估方法A2EHV，采用异质价值系统，并基于价值合理性和社会价值定向框架评估代理人行为的社会偏好，结果表明比传统对齐方法更合理。 |
| [^142] | [A Framework for Designing Foundation Model based Systems.](http://arxiv.org/abs/2305.05352) | 本文提出了一个基于基础模型的系统分类体系，分类和比较了基础模型和基于基础模型的系统的特点。它为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。 |
| [^143] | [Linear Spaces of Meanings: Compositional Structures in Vision-Language Models.](http://arxiv.org/abs/2302.14383) | 本文研究了视觉语言模型中的组合结构，并提出了一种使用嵌入空间中较小集合的向量组合来近似表示来自编码器的表示形式的方法，将这些向量视为“理想单词”，并在CLIP的嵌入中以实验方式探索了这些结构的可用性。 |
| [^144] | [To Be Forgotten or To Be Fair: Unveiling Fairness Implications of Machine Unlearning Methods.](http://arxiv.org/abs/2302.03350) | 这项研究揭示了机器遗忘方法在公平性方面的影响，帮助软件工程师做出负责任的决策。 |
| [^145] | [Designing Ecosystems of Intelligence from First Principles.](http://arxiv.org/abs/2212.01354) | 本文提出了一个关于未来人工智能领域研究和发展的愿景，该愿景是基于主动推理和自证明的智能生态系统，其中人类作为共享智能的不可或缺的参与者。该愿景旨在最大化模型证据，通过不同尺度上的信念更新来累积关于自身感知世界的证据。 |
| [^146] | [Scalable Hierarchical Over-the-Air Federated Learning.](http://arxiv.org/abs/2211.16162) | 本研究提出了一种针对分布式环境的通信高效的分层联邦学习算法，通过使用可扩展的无线聚合方案和带宽有限的广播方案，解决了设备干扰和边缘服务器干扰的问题。 |
| [^147] | [CP-PINNs: Changepoints Detection in PDEs using Physics Informed Neural Networks with Total-Variation Penalty.](http://arxiv.org/abs/2208.08626) | 本文提出了一种新的CP-PINNs模型，通过将PINNs与总变差惩罚相结合，实现了准确的变点检测和PDE的发现。我们还开发了一种元学习算法，能够在数据的连续批次上动态改进优化目标。实证结果表明，在存在变点的情况下，该方法能够准确估计参数和模型对齐，在没有变点的情况下能够数值上收敛到原始PINNs模型的解。 |

# 详细

[^1]: E$^{2}$GAN: 高效训练图像到图像转换任务的高效GANs

    E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation. (arXiv:2401.06127v1 [cs.CV])

    [http://arxiv.org/abs/2401.06127](http://arxiv.org/abs/2401.06127)

    本论文旨在提出一种高效的方法来从扩散模型中提炼GANs，并用于图像到图像的转换任务。这种方法可以实现灵活的实时图像编辑，并显著降低训练不同概念模型的成本。

    

    为了实现灵活的实时设备上图像编辑，一种高度有希望的方法是利用大规模文本到图像扩散模型，例如稳定扩散 (Stable Diffusion)，生成用于训练生成对抗网络 (GANs) 的配对数据集。这种方法显著减轻了使用扩散模型进行图像编辑时通常由高端商用GPU特定的严格要求。然而，与文本到图像扩散模型不同，每个生成的 GAN 都专门用于特定的图像编辑任务，因此需要昂贵的训练工作来获得各种概念的模型。在这项工作中，我们引入并解决了一个新颖的研究方向：能否使从扩散模型中提炼 GANs 的过程更加高效？为了实现这一目标，我们提出了一系列创新技术。首先，我们构建了一个具有广义特征的基本 GAN 模型，通过微调适应不同的概念，消除了...

    One highly promising direction for enabling flexible real-time on-device image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models, such as Stable Diffusion, to generate paired datasets used for training generative adversarial networks (GANs). This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models. However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts. In this work, we introduce and address a novel research direction: can the process of distilling GANs from diffusion models be made significantly more efficient? To achieve this goal, we propose a series of innovative techniques. First, we construct a base GAN model with generalized features, adaptable to different concepts through fine-tuning, eliminating t
    
[^2]: 用梯度弹射操纵特征可视化

    Manipulating Feature Visualizations with Gradient Slingshots. (arXiv:2401.06122v1 [cs.LG])

    [http://arxiv.org/abs/2401.06122](http://arxiv.org/abs/2401.06122)

    本研究探究了激活最大化方法在对抗模型操作中的脆弱性，并提出了一种新的方法来操纵特征可视化，以隐藏特定神经元的功能。

    

    深度神经网络(DNNs)能够学习复杂而多样化的表示，然而，学习到的概念的语义性质仍然未知。解释DNNs学习到的概念的常用方法是激活最大化(AM)，它生成一个合成的输入信号，最大化激活网络中的特定神经元。在本文中，我们研究了这种方法对于对抗模型操作的脆弱性，并引入了一种新的方法来操纵特征可视化，而不改变模型结构或对模型的决策过程产生显著影响。我们评估了我们的方法对几个神经网络模型的效果，并展示了它隐藏特定神经元功能的能力，在模型审核过程中使用选择的目标解释屏蔽了原始解释。作为一种补救措施，我们提出了一种防止这种操纵的防护措施，并提供了定量证据，证明了它的有效性。

    Deep Neural Networks (DNNs) are capable of learning complex and versatile representations, however, the semantic nature of the learned concepts remains unknown. A common method used to explain the concepts learned by DNNs is Activation Maximization (AM), which generates a synthetic input signal that maximally activates a particular neuron in the network. In this paper, we investigate the vulnerability of this approach to adversarial model manipulations and introduce a novel method for manipulating feature visualization without altering the model architecture or significantly impacting the model's decision-making process. We evaluate the effectiveness of our method on several neural network models and demonstrate its capabilities to hide the functionality of specific neurons by masking the original explanations of neurons with chosen target explanations during model auditing. As a remedy, we propose a protective measure against such manipulations and provide quantitative evidence which 
    
[^3]: Patchscope: 一个统一的框架，用于检查语言模型的隐藏表示

    Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])

    [http://arxiv.org/abs/2401.06102](http://arxiv.org/abs/2401.06102)

    本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。

    

    检查大型语言模型（LLM）的隐藏表示中编码的信息可以解释模型的行为并验证其与人类价值观的一致性。鉴于LLM生成人类可理解文本的能力，我们建议利用模型本身以自然语言解释其内部表示。我们引入了一个称为Patchscopes的框架，并展示了如何使用它来回答关于LLM计算的各种研究问题。我们表明，先前基于将表示投影到词汇空间和干预LLM计算的可解释性方法，可以看作是该框架的特殊实例。此外，通过Patchscope可以弥补优势，如检查早期层失败或表达能力不足。除了统一先前的检查技术，Patchscopes还开辟了新的可能性，例如使用更强大的模型来解释较小模型的表示。

    Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
    
[^4]: 通过大型语言模型在电子健康记录中自动补全主要症状

    Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models. (arXiv:2401.06088v1 [cs.CL])

    [http://arxiv.org/abs/2401.06088](http://arxiv.org/abs/2401.06088)

    本研究通过使用文本生成技术和机器学习模型，训练了几种变种的生物医学生成预训练变压器模型，并开发了一个自动补全工具，可为三级护理人员提供准确和格式良好的主要症状短语或句子。

    

    主要症状（CC）是患者医疗记录的关键组成部分，它描述了寻求医疗保健的主要原因或关注点。它为医疗保健提供者提供了关键信息，以便做出有根据的患者护理决策。然而，为医疗保健提供者记录CC可能耗时，尤其是在繁忙的急诊科。为了解决这个问题，在临床记录中为三级护理人员提供准确和格式良好的短语或句子的自动补全工具可以成为宝贵的资源。在本研究中，我们利用文本生成技术使用CC数据开发了机器学习模型。在我们的提议中，我们训练了一个长短期记忆（LSTM）模型，并微调了三种不同变种的生物医学生成预训练变压器（BioGPT），分别是microsoft/biogpt，microsoft/BioGPT-Large和microsoft/BioGPT-Large-PubMedQA。此外，我们通过结合典型的CC句子，利用GPT的OpenAI API来调整提示。

    The Chief Complaint (CC) is a crucial component of a patient's medical record as it describes the main reason or concern for seeking medical care. It provides critical information for healthcare providers to make informed decisions about patient care. However, documenting CCs can be time-consuming for healthcare providers, especially in busy emergency departments. To address this issue, an autocompletion tool that suggests accurate and well-formatted phrases or sentences for clinical notes can be a valuable resource for triage nurses. In this study, we utilized text generation techniques to develop machine learning models using CC data. In our proposed work, we train a Long Short-Term Memory (LSTM) model and fine-tune three different variants of Biomedical Generative Pretrained Transformers (BioGPT), namely microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA. Additionally, we tune a prompt by incorporating exemplar CC sentences, utilizing the OpenAI API of GPT
    
[^5]: XGBoost学习用于基于代理模型的体育博彩交易中的动态投注方式

    XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an Agent-Based Model of a Sports Betting Exchange. (arXiv:2401.06086v1 [cs.LG])

    [http://arxiv.org/abs/2401.06086](http://arxiv.org/abs/2401.06086)

    本文介绍了在基于代理模型的体育博彩交易中使用XGBoost学习到的动态投注策略，并通过基于代理模型的模拟器进行实验评估。

    

    本文首次介绍了在Bristol Betting Exchange (BBE)中使用高效的机器学习方法XGBoost的结果。BBE是一个开源的基于代理模型的模拟器，旨在模拟具有赛事中投注功能的现代体育博彩交易。我们使用BBE模拟器和其一系列简单的赌徒代理作为数据生成器，输入到我们的XGBoost机器学习系统中。通过学习BBE赌徒代理所进行的更加有利可图的投注，XGBoost能够发现有利可图的动态投注策略。在XGBoost训练之后，生成一个或多个决策树，将具有由XGBoost学习决策树确定的投注策略的赌徒代理添加到BBE模拟器中，在各种条件和投注市场情景下进行一系列比赛的投注，以盈利作为主要的比较和评估指标。本研究的初步发现表明...

    We present first results from the use of XGBoost, a highly effective machine learning (ML) method, within the Bristol Betting Exchange (BBE), an open-source agent-based model (ABM) designed to simulate a contemporary sports-betting exchange with in-play betting during track-racing events such as horse races. We use the BBE ABM and its array of minimally-simple bettor-agents as a synthetic data generator which feeds into our XGBoost ML system, with the intention that XGBoost discovers profitable dynamic betting strategies by learning from the more profitable bets made by the BBE bettor-agents. After this XGBoost training, which results in one or more decision trees, a bettor-agent with a betting strategy determined by the XGBoost-learned decision tree(s) is added to the BBE ABM and made to bet on a sequence of races under various conditions and betting-market scenarios, with profitability serving as the primary metric of comparison and evaluation. Our initial findings presented here sho
    
[^6]: RLHF在大型语言模型中的秘密 Part II: 奖励建模

    Secrets of RLHF in Large Language Models Part II: Reward Modeling. (arXiv:2401.06080v1 [cs.AI])

    [http://arxiv.org/abs/2401.06080](http://arxiv.org/abs/2401.06080)

    本报告探讨了在 RLHF 中解决奖励建模的两个挑战，通过使用多个奖励模型的投票机制来测量数据中偏好的强度，并解决在特定分布数据上训练奖励模型难以推广的问题。

    

    从人类反馈中进行强化学习（RLHF）已成为将语言模型与人类价值和意图对齐的关键技术，使模型能够产生更有帮助且无害的回应。奖励模型被训练为人类偏好的代理，以驱动强化学习优化。然而，在实际应用中，奖励模型面临以下挑战：（1）数据集中不正确和模糊的偏好对可能妨碍奖励模型准确捕捉人类意图。（2）在特定分布的数据上训练的奖励模型往往难以推广到分布之外的示例，并且不适用于迭代RLHF训练。本研究尝试解决这两个问题。从数据角度出发，我们提出一种方法来测量数据中偏好的强度，基于多个奖励模型的投票机制。实验结果...

    Reinforcement Learning from Human Feedback (RLHF) has become a crucial technology for aligning language models with human values and intentions, enabling models to produce more helpful and harmless responses. Reward models are trained as proxies for human preferences to drive reinforcement learning optimization. While reward models are often considered central to achieving high performance, they face the following challenges in practical applications: (1) Incorrect and ambiguous preference pairs in the dataset may hinder the reward model from accurately capturing human intent. (2) Reward models trained on data from a specific distribution often struggle to generalize to examples outside that distribution and are not suitable for iterative RLHF training.  In this report, we attempt to address these two issues. (1) From a data perspective, we propose a method to measure the strength of preferences within the data, based on a voting mechanism of multiple reward models. Experimental result
    
[^7]: 历史链的链路预测与学习：基于LLMs的时间知识图补全

    Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion. (arXiv:2401.06072v1 [cs.AI])

    [http://arxiv.org/abs/2401.06072](http://arxiv.org/abs/2401.06072)

    本文提出了一种基于LLMs的新方法，将时间知识图补全任务概念化为历史事件链中的事件生成任务。通过引入高效的微调方法和结构化历史数据增强，以及整合反向知识，我们的模型在多个指标上优于现有的方法，取得了SOTA结果。

    

    时间知识图补全是一项具有挑战性的任务，其通过利用已建立的时间结构知识来预测未来时间戳上缺失的事件链接。本文提出了一种新颖的方法，将时间链路预测概念化为历史事件链中的事件生成任务，利用LLMs的强大生成能力。我们采用高效的微调方法，使LLMs适应特定的图文信息和在时间线中发现的模式。此外，我们引入基于结构的历史数据增强和反向知识的整合，以增强LLMs对结构信息的意识，从而提高其推理能力。我们在多个广泛使用的数据集上进行了详尽的实验，发现我们微调的模型在多个指标上优于现有的基于嵌入的模型，取得了SOTA的结果。我们还进行了充分的消融实验。

    Temporal Knowledge Graph Completion (TKGC) is a challenging task of predicting missing event links at future timestamps by leveraging established temporal structural knowledge. Given the formidable generative capabilities inherent in LLMs (LLMs), this paper proposes a novel approach to conceptualize temporal link prediction as an event generation task within the context of a historical event chain. We employ efficient fine-tuning methods to make LLMs adapt to specific graph textual information and patterns discovered in temporal timelines. Furthermore, we introduce structure-based historical data augmentation and the integration of reverse knowledge to emphasize LLMs' awareness of structural information, thereby enhancing their reasoning capabilities. We conduct thorough experiments on multiple widely used datasets and find that our fine-tuned model outperforms existing embedding-based models on multiple metrics, achieving SOTA results. We also carry out sufficient ablation experiments
    
[^8]: 研究预训练语言模型的数据污染

    Investigating Data Contamination for Pre-training Language Models. (arXiv:2401.06059v1 [cs.CL])

    [http://arxiv.org/abs/2401.06059](http://arxiv.org/abs/2401.06059)

    这项研究调查了预训练语言模型中的数据污染问题，以及该污染对下游任务性能的影响。

    

    在大规模网络语料库上预训练的语言模型在各种下游任务上展示出令人印象深刻的能力。然而，越来越担心这种能力是否是由于评估数据集被包含在预训练语料库中导致的，这种现象被称为“数据污染”，从而在人工提高性能。目前对这种潜在污染如何影响语言模型在下游任务上的性能缺乏了解。本文通过从头开始预训练一系列GPT-2模型，探讨了在预训练阶段数据污染的影响。我们强调了来自评估数据的文本污染（即输入文本的评估样本）和基准污染（即输入中的提示和期望输出）的影响。我们还研究了在各种下游任务中重复污染的影响。此外，我们还调查了普遍存在的n

    Language models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks. However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the pre-training corpus -- a phenomenon known as \textit{data contamination} -- in a manner that artificially increases performance. There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks. In this paper, we explore the impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models \textit{from scratch}. We highlight the effect of both text contamination (\textit{i.e.}\ input text of the evaluation samples) and ground-truth contamination (\textit{i.e.}\ the prompts asked on the input and the desired outputs) from evaluation data. We also investigate the effects of repeating contamination for various downstream tasks. Additionally, we examine the prevailing n
    
[^9]: 通过图神经网络和特征增强策略来分类社交网络的能力研究

    On the Power of Graph Neural Networks and Feature Augmentation Strategies to Classify Social Networks. (arXiv:2401.06048v1 [cs.SI])

    [http://arxiv.org/abs/2401.06048](http://arxiv.org/abs/2401.06048)

    本文研究了四种图神经网络架构的性能以及通过应用不同的人工特征增强策略来提高图分类任务的准确性，结果表明GNN架构的计算能力和人工特征提供的信息水平对任务的性能具有平衡的重要性。

    

    本文研究了四种图神经网络架构（GNNs）在使用经典的网络科学生成模型创建的合成数据集上进行图分类任务。由于合成网络不包含（节点或边）特征，因此对节点应用了五种不同的增强策略（人造特征类型）。研究了4种GNNs（具有层次和全局聚合的GCN、GIN和GATv2）和5种特征类型（常数1、噪声、度、归一化度和ID - 各种长度的循环数量的向量）的所有组合，并在GNNs中使用的人工神经网络的隐藏维度作为函数比较其性能。还使用第二个合成网络数据集分析了这些模型的泛化能力（包含不同规模的网络）。我们的结果表明，GNN架构的计算能力和人工特征提供的信息水平的平衡重要性。

    This paper studies four Graph Neural Network architectures (GNNs) for a graph classification task on a synthetic dataset created using classic generative models of Network Science. Since the synthetic networks do not contain (node or edge) features, five different augmentation strategies (artificial feature types) are applied to nodes. All combinations of the 4 GNNs (GCN with Hierarchical and Global aggregation, GIN and GATv2) and the 5 feature types (constant 1, noise, degree, normalized degree and ID -- a vector of the number of cycles of various lengths) are studied and their performances compared as a function of the hidden dimension of artificial neural networks used in the GNNs. The generalisation ability of these models is also analysed using a second synthetic network dataset (containing networks of different sizes).Our results point towards the balanced importance of the computational power of the GNN architecture and the the information level provided by the artificial featur
    
[^10]: 在内窥镜手术中深度估计的手术DINO：基于基础模型的适配器学习

    Surgical-DINO: Adapter Learning of Foundation Model for Depth Estimation in Endoscopic Surgery. (arXiv:2401.06013v1 [cs.CV])

    [http://arxiv.org/abs/2401.06013](http://arxiv.org/abs/2401.06013)

    本研究提出了一种基于基础模型的适配器学习方法，针对内窥镜手术中的深度估计问题进行了改进。通过在DINO模型中构建LoRA层，并将其与手术场景的特征结合起来，实现了手术特定的深度估计。实验结果表明，该方法在内窥镜手术中取得了良好的性能。

    

    目的：在机器人手术中进行深度估计对于三维重建、手术导航和增强现实可视化至关重要。尽管基础模型在许多视觉任务中表现出色，包括深度估计（如DINOv2），但最近的研究观察到其在医学和手术特定应用中的局限性。本研究提出了一种手术深度估计的低秩适配（LoRA）基础模型。方法：我们设计了一种基于基础模型的深度估计方法，称为Surgical-DINO，这是DINOv2在内窥镜手术中深度估计中的低秩适配。我们在DINO中构建了LoRA层，并将其整合到DINO中，以适应手术特定的领域知识，而不是传统的微调方法。在训练过程中，我们冻结了DINO的图像编码器，该编码器表现出优秀的视觉表示能力，只优化了LoRA层和深度解码器，以整合手术场景的特征。结果：我们的模型是ex...

    Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction, surgical navigation and augmented reality visualization. Although the foundation model exhibits outstanding performance in many vision tasks, including depth estimation (e.g., DINOv2), recent works observed its limitations in medical and surgical domain-specific applications. This work presents a low-ranked adaptation (LoRA) of the foundation model for surgical depth estimation. Methods: We design a foundation model-based depth estimation method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for depth estimation in endoscopic surgery. We build LoRA layers and integrate them into DINO to adapt with surgery-specific domain knowledge instead of conventional fine-tuning. During training, we freeze the DINO image encoder, which shows excellent visual representation capacity, and only optimize the LoRA layers and depth decoder to integrate features from the surgical scene. Results: Our model is ex
    
[^11]: 非人灵长类动物大脑如何在视觉中结合生成式和判别式计算？

    How does the primate brain combine generative and discriminative computations in vision?. (arXiv:2401.06005v1 [q-bio.NC])

    [http://arxiv.org/abs/2401.06005](http://arxiv.org/abs/2401.06005)

    本论文探讨了非人灵长类动物大脑如何在视觉中结合生成式和判别式计算。一个观念强调自下而上的信号流动，通过滤除不相关的变异和转换视觉信息来代表行为上相关的信息；而另一个观念将视觉视为Helmholtz的推理过程。

    

    视觉被广泛理解为一种推理问题。然而，在生物视觉研究和机器视觉工程中，关于推理过程的两种对立观念都具有影响力。第一种强调自下而上的信号流动，将视觉描述为一种主要是前馈的判别推理过程，其通过滤除不相关的变异和以适合下游的认知和行为控制功能需要的方式来转换视觉信息，并代表行为上相关信息。在这种观念中，视觉由感官数据驱动，感知是直接的，因为处理从数据到感兴趣的潜变量的过程进行。在这种观念中，"推理"的概念类似于神经网络的工程文献中描述的，对图像进行处理的前馈卷积神经网络被认为是在执行推理。而另一种观念是将视觉视为Helmholtz的推理过程。

    Vision is widely understood as an inference problem. However, two contrasting conceptions of the inference process have each been influential in research on biological vision as well as the engineering of machine vision. The first emphasizes bottom-up signal flow, describing vision as a largely feedforward, discriminative inference process that filters and transforms the visual information to remove irrelevant variation and represent behaviorally relevant information in a format suitable for downstream functions of cognition and behavioral control. In this conception, vision is driven by the sensory data, and perception is direct because the processing proceeds from the data to the latent variables of interest. The notion of "inference" in this conception is that of the engineering literature on neural networks, where feedforward convolutional neural networks processing images are said to perform inference. The alternative conception is that of vision as an inference process in Helmhol
    
[^12]: 使用多智能体辩论来对抗对抗性攻击

    Combating Adversarial Attacks with Multi-Agent Debate. (arXiv:2401.05998v1 [cs.CL])

    [http://arxiv.org/abs/2401.05998](http://arxiv.org/abs/2401.05998)

    使用多智能体辩论对抗对抗性攻击可以降低模型的有毒性，同时通过嵌入聚类对对抗性提示内容进行分类可以分析不同模型对不同类型攻击的易受攻击性。

    

    虽然最先进的语言模型取得了令人印象深刻的成果，但它们仍然容易受到推理阶段的对抗性攻击，例如由红队生成的对抗性提示。为了提高语言模型生成的整体质量，提出了一种方法：多智能体辩论，其中语言模型通过讨论和反馈来自我评估。我们实施了最新的语言模型之间的多智能体辩论，并评估了模型在单一和多智能体环境中受到红队攻击的易受攻击性。我们发现，当越狱或能力较低的模型被迫与未越狱或能力更强的模型进行辩论时，多智能体辩论可以减少模型的有毒性。我们还通过嵌入聚类对对抗性提示内容进行分类，并分析了不同类型攻击主题对不同模型的易受攻击性。

    While state-of-the-art language models have achieved impressive results, they remain susceptible to inference-time adversarial attacks, such as adversarial prompts generated by red teams arXiv:2209.07858. One approach proposed to improve the general quality of language model generations is multi-agent debate, where language models self-evaluate through discussion and feedback arXiv:2305.14325. We implement multi-agent debate between current state-of-the-art language models and evaluate models' susceptibility to red team attacks in both single- and multi-agent settings. We find that multi-agent debate can reduce model toxicity when jailbroken or less capable models are forced to debate with non-jailbroken or more capable models. We also find marginal improvements through the general usage of multi-agent interactions. We further perform adversarial prompt content classification via embedding clustering, and analyze the susceptibility of different models to different types of attack topic
    
[^13]: 用于推荐中意图学习的端到端可学习聚类方法

    End-to-end Learnable Clustering for Intent Learning in Recommendation. (arXiv:2401.05975v1 [cs.IR])

    [http://arxiv.org/abs/2401.05975](http://arxiv.org/abs/2401.05975)

    本文提出了一种用于推荐中意图学习的端到端可学习聚类方法ELCRec，该方法解决了现有方法中的复杂优化问题和大规模数据集聚类的可扩展性问题。

    

    挖掘用户的意图在序列推荐中起着关键作用。最近的方法ICLRec使用对比学习和聚类来提取用户的潜在意图。尽管它已经显示出有效性，但现有的方法存在复杂和繁琐的交替优化问题，导致两个主要问题。首先，在广义期望最大化(EM)框架中分离表示学习和聚类优化经常导致次优性能。其次，在整个数据集上进行聚类会影响大规模行业数据的可扩展性。为了解决这些挑战，我们提出了一种新颖的意图学习方法，称为ELCRec，它将表示学习集成到一个端到端可学习聚类框架中进行推荐。

    Mining users' intents plays a crucial role in sequential recommendation. The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering. While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues. Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance. Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data. To address these challenges, we propose a novel intent learning method called \underline{ELCRec}, which integrates representation learning into an \underline{E}nd-to-end \underline{L}earnable \underline{C}lustering framework for \underline{Rec}ommendation. Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameter
    
[^14]: 空间感知深度强化学习在巡警问题中的应用

    Spatial-Aware Deep Reinforcement Learning for the Traveling Officer Problem. (arXiv:2401.05969v1 [cs.LG])

    [http://arxiv.org/abs/2401.05969](http://arxiv.org/abs/2401.05969)

    这篇论文提出了一种称为SATOP的空间感知深度强化学习方法来解决巡警问题。该方法通过利用停车位、代理和动作之间的空间关系来创建动作的表示，从而动态调整以适应当前可罚款的停车违规行为，并提前计划以提高到达违规行为发生时的可能性。

    

    巡警问题（TOP）是一个具有挑战性的随机优化任务。在这个问题中，一名停车员通过配备停车传感器的城市进行引导，以便尽可能多地罚款违规停车者。TOP的一个主要挑战是停车违规的动态性，这些违规行为会在一段时间后随机出现和消失，不论它们是否被罚款。因此，解决方案需要动态地调整以适应当前可罚款的停车违规行为，同时还要提前计划，增加巡警在违规行为发生时到达的可能性。尽管存在各种解决方案，但这些方法往往难以考虑到行动对未来罚款违规的能力产生的影响。本文提出了SATOP，一种新颖的空间感知深度强化学习方法用于TOP。我们的新型状态编码器创建了每个动作的表示，利用停车位、代理和动作之间的空间关系。

    The traveling officer problem (TOP) is a challenging stochastic optimization task. In this problem, a parking officer is guided through a city equipped with parking sensors to fine as many parking offenders as possible. A major challenge in TOP is the dynamic nature of parking offenses, which randomly appear and disappear after some time, regardless of whether they have been fined. Thus, solutions need to dynamically adjust to currently fineable parking offenses while also planning ahead to increase the likelihood that the officer arrives during the offense taking place. Though various solutions exist, these methods often struggle to take the implications of actions on the ability to fine future parking violations into account. This paper proposes SATOP, a novel spatial-aware deep reinforcement learning approach for TOP. Our novel state encoder creates a representation of each action, leveraging the spatial relationships between parking spots, the agent, and the action. Furthermore, we
    
[^15]: 从PixelCNN的潜在空间中尝试生成新的桥梁类型

    An attempt to generate new bridge types from latent space of PixelCNN. (arXiv:2401.05964v1 [cs.LG])

    [http://arxiv.org/abs/2401.05964](http://arxiv.org/abs/2401.05964)

    本论文尝试使用生成式人工智能技术生成新的桥梁类型，通过对潜在空间进行采样，可以在人类原始桥梁类型的基础上组合不同的结构组件，创造具有一定创新能力的新桥梁类型。

    

    本论文尝试使用生成式人工智能技术生成新的桥梁类型。使用三跨梁桥、拱桥、斜拉桥和悬索桥的对称结构图像数据集作为训练集，基于Python编程语言、TensorFlow和Keras深度学习平台构建和训练PixelCNN模型。该模型能够捕捉图像的统计结构，并在给定前一像素的情况下计算下一个像素的概率分布。通过对获得的潜在空间进行采样，可以生成与训练集中不同的桥梁类型。PixelCNN可以在人类原始桥梁类型的基础上有机地组合不同的结构组件，创建具有一定人类创新能力的新桥梁类型。自回归模型无法理解序列的含义，而多模态模型结合回归和自回归模型，能够理解序列。多模态模型应该是...

    Try to generate new bridge types using generative artificial intelligence technology. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , PixelCNN is constructed and trained. The model can capture the statistical structure of the images and calculate the probability distribution of the next pixel when the previous pixels are given. From the obtained latent space sampling, new bridge types different from the training dataset can be generated. PixelCNN can organically combine different structural components on the basis of human original bridge types, creating new bridge types that have a certain degree of human original ability. Autoregressive models cannot understand the meaning of the sequence, while multimodal models combine regression and autoregressive models to understand the sequence. Multimodal models should be the
    
[^16]: 机器学习在OptVerse AI Solver中的应用：设计原理与应用

    Machine Learning Insides OptVerse AI Solver: Design Principles and Applications. (arXiv:2401.05960v1 [cs.AI])

    [http://arxiv.org/abs/2401.05960](http://arxiv.org/abs/2401.05960)

    本论文详细研究了将机器学习技术应用于华为云的OptVerse AI Solver，主要包括生成复杂实例、训练框架维护实用性和个性化的解算器策略等方面的创新方法。研究旨在缓解数学规划实例稀缺问题，并超越传统优化技术的能力。

    

    在数字无处不在的时代，高效的资源管理和决策是各个行业的重要问题。为此，我们对将机器学习技术应用于华为云的OptVerse AI Solver进行了全面的研究，旨在缓解现实世界数学规划实例的稀缺，并超越传统优化技术的能力。我们展示了利用生成模型生成复杂的SAT和MILP实例的方法，这些实例反映了现实世界问题的多面结构。此外，我们引入了一个训练框架，利用增强策略维护解算器在动态环境中的实用性。除了数据生成和增强，我们提出的方法还包括个性化解算器策略的新型机器学习驱动策略，重点关注图卷积网络用于初始基础选择和强化学习用于高级预求解的应用。

    In an era of digital ubiquity, efficient resource management and decision-making are paramount across numerous industries. To this end, we present a comprehensive study on the integration of machine learning (ML) techniques into Huawei Cloud's OptVerse AI Solver, which aims to mitigate the scarcity of real-world mathematical programming instances, and to surpass the capabilities of traditional optimization techniques. We showcase our methods for generating complex SAT and MILP instances utilizing generative models that mirror multifaceted structures of real-world problem. Furthermore, we introduce a training framework leveraging augmentation policies to maintain solvers' utility in dynamic environments. Besides the data generation and augmentation, our proposed approaches also include novel ML-driven policies for personalized solver strategies, with an emphasis on applications like graph convolutional networks for initial basis selection and reinforcement learning for advanced presolvi
    
[^17]: 大型语言模型中的通用漏洞：上下文学习后门攻击

    Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v1 [cs.CL])

    [http://arxiv.org/abs/2401.05949](http://arxiv.org/abs/2401.05949)

    本研究发现上下文学习范式在大型语言模型中存在漏洞，攻击者可以通过污染示范上下文来操控模型行为，而无需进行微调。这项研究设计了一种名为ICLAttack的后门攻击方法，可以通过污染示范样本和提示来使模型按照预定义的意图行事。

    

    上下文学习是一种在预训练和微调之间弥合差距的范式，在几个自然语言处理任务中展现了高效性，特别是在少样本设置中。与传统的微调方法不同，上下文学习能够适应未见过的任务而无需更新任何参数。尽管被广泛应用，上下文学习仍然容易受到恶意攻击。本研究提出了对这一范式的安全性问题的关切。我们的研究表明，攻击者可以通过污染示范上下文来操控大型语言模型的行为，而无需对模型进行微调。具体来说，我们设计了一种新的后门攻击方法，命名为ICLAttack，针对基于上下文学习的大型语言模型。我们的方法包括两种类型的攻击：污染示范样本和污染提示，可以使模型按照预定义的意图行事。ICLAttack不需要额外的微调。

    In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Unlike traditional fine-tuning methods, in-context learning adapts pre-trained models to unseen tasks without updating any parameters. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we have designed a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning prompts, which can make models behave in accordance with predefined intentions. ICLAttack does not require additional fine-tuning 
    
[^18]: 从Transformer表示中学习认知地图以实现部分观察环境中的高效规划

    Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments. (arXiv:2401.05946v1 [cs.LG])

    [http://arxiv.org/abs/2401.05946](http://arxiv.org/abs/2401.05946)

    本文提出了一种从Transformer表示中学习认知地图的方法，该方法针对部分观察环境中的路径规划问题提供了一个有效的解决方案。

    

    尽管在各种任务中表现出色，包括在推理过程中仅透露的上下文任务，但标准的Transformer和针对下一个标记预测训练的变体(a)没有学习到明确的环境世界模型，该模型可以灵活查询，(b)不能用于规划或导航。本文考虑部分观察环境（POEs），代理根据其导航时接收到的感知别名观察，这使得路径规划变得困难。我们引入了一个具有（多个）离散瓶颈的Transformer，TDB，其潜在代码可以学习观察和动作历史的压缩表示。在训练TDB以预测给定历史的未来观察后，我们从其活跃的瓶颈索引中提取环境的可解释认知地图。然后，将这些地图与外部求解器配对以解决（受限制的）路径规划问题。首先，我们展示了在POEs上训练的TDB可以保留...

    Despite their stellar performance on a wide range of tasks, including in-context tasks only revealed during inference, vanilla transformers and variants trained for next-token predictions (a) do not learn an explicit world model of their environment which can be flexibly queried and (b) cannot be used for planning or navigation. In this paper, we consider partially observed environments (POEs), where an agent receives perceptually aliased observations as it navigates, which makes path planning hard. We introduce a transformer with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a compressed representation of the history of observations and actions. After training a TDB to predict the future observation(s) given the history, we extract interpretable cognitive maps of the environment from its active bottleneck(s) indices. These maps are then paired with an external solver to solve (constrained) path planning problems. First, we show that a TDB trained on POEs (a) retains
    
[^19]: 基于变异的一致性测试用于评估LLMs的代码理解能力

    Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs. (arXiv:2401.05940v1 [cs.SE])

    [http://arxiv.org/abs/2401.05940](http://arxiv.org/abs/2401.05940)

    本文提出了一种基于代码变异的方法，用于系统评估LLMs的代码理解能力，重点关注代码和描述之间的微妙差异。通过引入代码变异到现有的数据集中，我们可以生成不一致的代码数据，从而评估LLMs的代码理解性能。

    

    大型语言模型（LLMs）在处理自然语言和编程语言方面显示出卓越的能力，为软件工程领域的需求工程、代码生成和软件测试等各种应用提供了可能。然而，现有的代码生成基准测试并不一定能够评估LLMs的代码理解性能，特别是对于代码和自然语言描述之间可能出现的微妙不一致性。在本文中，我们提出了一种新的方法来系统评估LLMs的代码理解性能，特别关注代码和描述之间的微小差异，通过引入代码变异到现有的代码生成数据集中。代码变异是对原始代码进行改动的小改变，会导致代码与自然语言描述不匹配。我们采用不同类型的代码变异，如操作符替换和语句删除，来生成不一致的代码数据。

    Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.  In this paper, we propose a novel method to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. We apply different types of code mutations, such as operator replacement and statement deletion, to generate incon
    
[^20]: DREQ: 使用基于实体的查询理解进行文档重新排序

    DREQ: Document Re-Ranking Using Entity-based Query Understanding. (arXiv:2401.05939v1 [cs.IR])

    [http://arxiv.org/abs/2401.05939](http://arxiv.org/abs/2401.05939)

    DREQ是一种基于实体的密集文档重新排序模型，通过强调与查询相关的实体并减弱不太相关的实体，获得一个查询特定的以实体为中心的文档表示，并结合文本为中心的文档表示进行混合表示，该方法在多个基准测试中优于最先进的重新排序方法。

    

    尽管基于实体的神经信息检索（IR）模型取得了显著进展，但它们经常忽视了一个关键细微差别：文档中各个实体对其整体相关性的影响程度是不同的。为了填补这一空白，我们提出了DREQ，一种基于实体的密集文档重新排序模型。独特之处在于，在文档的表示中强调与查询相关的实体，同时减弱不太相关的实体，从而获得一个查询特定的以实体为中心的文档表示。然后，将这种以实体为中心的文档表示与文本为中心的文档表示结合起来，获得文档的“混合”表示。我们使用这种混合表示学习文档的相关性分数。通过使用四个大规模基准测试，我们证明DREQ优于最先进的神经网络和非神经网络的重新排序方法，突出了我们基于实体的表示方法的有效性。

    While entity-oriented neural IR models have advanced significantly, they often overlook a key nuance: the varying degrees of influence individual entities within a document have on its overall relevance. Addressing this gap, we present DREQ, an entity-oriented dense document re-ranking model. Uniquely, we emphasize the query-relevant entities within a document's representation while simultaneously attenuating the less relevant ones, thus obtaining a query-specific entity-centric document representation. We then combine this entity-centric document representation with the text-centric representation of the document to obtain a "hybrid" representation of the document. We learn a relevance score for the document using this hybrid representation. Using four large-scale benchmarks, we show that DREQ outperforms state-of-the-art neural and non-neural re-ranking methods, highlighting the effectiveness of our entity-oriented representation approach.
    
[^21]: DiffDA:一种用于气象尺度数据同化的扩散模型

    DiffDA: a diffusion model for weather-scale data assimilation. (arXiv:2401.05932v1 [cs.CE])

    [http://arxiv.org/abs/2401.05932](http://arxiv.org/abs/2401.05932)

    DiffDA是一种用于气象尺度数据同化的扩散模型，通过机器学习的方法将预测状态和稀疏观测同化，生成与观测一致的初始条件，并能对预测进行后处理到未来。

    

    通过准确的数据同化生成初始条件对于可靠的天气预报和气候模拟至关重要。我们提出了DiffDA作为一种基于机器学习的数据同化方法，能够使用预测状态和稀疏观测来同化大气变量。我们将预训练的GraphCast天气预报模型作为去噪扩散模型。我们的方法应用了两阶段条件：在训练和推理过程中对预测状态进行条件化，在推理过程中只对稀疏观测进行条件化。作为副产品，这种策略还能将预测后处理到未来，在这种情况下没有可用的观测数据。通过基于再分析数据集的实验证明，我们的方法可以以0.25度分辨率生成与观测一致的同化全球大气数据。实验证明，通过我们的方法生成的初始条件可以用于具有较小损失的预报模型。

    The generation of initial conditions via accurate data assimilation is crucial for reliable weather forecasting and climate modeling. We propose the DiffDA as a machine learning based data assimilation method capable of assimilating atmospheric variables using predicted states and sparse observations. We adapt the pretrained GraphCast weather forecast model as a denoising diffusion model. Our method applies two-phase conditioning: on the predicted state during both training and inference, and on sparse observations during inference only. As a byproduct, this strategy also enables the post-processing of predictions into the future, for which no observations are available.Through experiments based on a reanalysis dataset, we have verified that our method can produce assimilated global atmospheric data consistent with observations at 0.25degree resolution. The experiments also show that the initial conditions that are generated via our approach can be used for forecast models with a loss 
    
[^22]: SH2: 自我突出式犹豫帮助您更准确解码。

    SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully. (arXiv:2401.05930v1 [cs.CL])

    [http://arxiv.org/abs/2401.05930](http://arxiv.org/abs/2401.05930)

    自我突出式犹豫（SH2）是一种推理时的方法，通过选择预测概率较低的标记，并强调它们的差异，从而帮助语言模型更准确地解码。

    

    大型语言模型(LLMs)在文本生成方面表现出色。然而，LLMs仍然存在幻觉问题。在本研究中，我们提出了一种推理时方法，即自我突出式犹豫(SH2)，以帮助LLMs更准确地解码。SH2基于信息理论中一个简单的事实，即对于LLMs而言，预测概率较低的标记往往更具信息量。我们的分析表明，LLMs给予较低概率的标记更有可能与事实信息（如名词、专有名词和形容词）密切相关。因此，我们提出通过选择概率最低的标记并将其连接到原始上下文中来“突出”事实信息，从而迫使模型在生成之前多次阅读和犹豫这些标记。在解码过程中，我们还采用对比解码的方式来强调由犹豫带来的输出概率的差异。

    Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to ''highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation.
    
[^23]: CoSSegGaussians：紧凑且迅速的3D高斯场景分割方法

    CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians. (arXiv:2401.05925v1 [cs.CV])

    [http://arxiv.org/abs/2401.05925](http://arxiv.org/abs/2401.05925)

    CoSSegGaussians是一种紧凑且迅速的3D高斯场景分割方法，通过映射空间和语义特征实现紧凑和可靠的零样本场景分割。

    

    我们提出了一种紧凑且迅速的3D高斯场景分割方法（CoSSegGaussians），该方法仅使用RGB图像输入，以快速的渲染速度实现紧凑的3D一致性场景分割。先前基于NeRF的3D分割方法依赖于隐式或体素神经场表示和光线行进体积渲染，这些方法耗时较长。最近的3D高斯场投影显著提高了渲染速度，然而，现有的基于高斯的分割方法（例如高斯分组）在零样本分割中没有提供紧凑的分割掩模，主要原因是在遇到不一致的2D机器生成标签时，无法直接为每个高斯分配可学习参数，缺乏鲁棒性和紧凑性。我们的方法旨在通过使用浅层解码网络将每个高斯点的融合空间和语义上有意义的特征映射，迅速实现紧凑且可靠的零样本场景分割。

    We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based 3D segmentation methods have relied on implicit or voxel neural scene representation and ray-marching volume rendering which are time consuming. Recent 3D Gaussian Splatting significantly improves the rendering speed, however, existing Gaussians-based segmentation methods(eg: Gaussian Grouping) fail to provide compact segmentation masks especially in zero-shot segmentation, which is mainly caused by the lack of robustness and compactness for straightforwardly assigning learnable parameters to each Gaussian when encountering inconsistent 2D machine-generated labels. Our method aims to achieve compact and reliable zero-shot scene segmentation swiftly by mapping fused spatial and semantically meaningful features for each Gaussian point with a shallow decoding network. Specifically, our method fi
    
[^24]: 教师如何利用大型语言模型和布鲁姆税务学派创建教育测验

    How Teachers Can Use Large Language Models and Bloom's Taxonomy to Create Educational Quizzes. (arXiv:2401.05914v1 [cs.CL])

    [http://arxiv.org/abs/2401.05914](http://arxiv.org/abs/2401.05914)

    本文介绍了一种利用大型语言模型和布鲁姆税务学派创建教育测验的方法，研究结果表明教师更倾向于使用自动生成的问题撰写测验，并且这些问题的质量不亚于手写版本，甚至有可能提高测验的质量。

    

    问题生成(QG)是一项自然语言处理任务，在教育领域具有丰富的潜在益处和用途。为了实现这一潜力，必须以教育需求为目标设计和验证QG系统。然而，很少有研究评估或设计QG方法时得到真实教师或学生的反馈。本文应用基于大型语言模型的QG方法，在生成问题时使用布鲁姆税务学派的学习目标。自动生成的问题在多个实验中被用于评估教师的实际使用情况。结果表明，教师更喜欢使用自动生成的问题撰写测验，并且与手写版本相比，这样的测验质量没有损失。此外，几个指标表明自动生成的问题甚至可以提高所创建测验的质量，展示了在大规模使用QG中的潜力。

    Question generation (QG) is a natural language processing task with an abundance of potential benefits and use cases in the educational domain. In order for this potential to be realized, QG systems must be designed and validated with pedagogical needs in mind. However, little research has assessed or designed QG approaches with the input from real teachers or students. This paper applies a large language model-based QG approach where questions are generated with learning goals derived from Bloom's taxonomy. The automatically generated questions are used in multiple experiments designed to assess how teachers use them in practice. The results demonstrate that teachers prefer to write quizzes with automatically generated questions, and that such quizzes have no loss in quality compared to handwritten versions. Further, several metrics indicate that automatically generated questions can even improve the quality of the quizzes created, showing the promise for large scale use of QG in the 
    
[^25]: 基于二进制线性树提交的分布式机器学习所有权保护

    Binary Linear Tree Commitment-based Ownership Protection for Distributed Machine Learning. (arXiv:2401.05895v1 [cs.LG])

    [http://arxiv.org/abs/2401.05895](http://arxiv.org/abs/2401.05895)

    本论文提出了一种基于二进制线性树提交的分布式机器学习所有权保护模型，通过验证计算的完整性和效果，解决了模型所有权的冲突问题，并降低了更新证明的成本。

    

    分布式机器学习通过将计算任务分配给多个工作节点，实现了对大规模数据集的并行训练。但是，最终模型权重的传播往往会导致模型所有权的潜在冲突，因为工作节点难以证明自己在训练计算中的参与度。为了解决上述所有权问题，并防止意外故障和恶意攻击，在分布式机器学习中验证工作节点的计算完整性和效果变得尤为重要。本文提出了一种新的基于二进制线性树提交的所有权保护模型，以确保计算的完整性，同时保证开销有限和证明简洁。由于训练过程中参数的频繁更新，我们的提交方案引入了可维护的树结构，降低了更新证明的成本。与基于SNARK的验证计算不同，我们的模型可以同时支持批量训练和在线训练。

    Distributed machine learning enables parallel training of extensive datasets by delegating computing tasks across multiple workers. Despite the cost reduction benefits of distributed machine learning, the dissemination of final model weights often leads to potential conflicts over model ownership as workers struggle to substantiate their involvement in the training computation. To address the above ownership issues and prevent accidental failures and malicious attacks, verifying the computational integrity and effectiveness of workers becomes particularly crucial in distributed machine learning. In this paper, we proposed a novel binary linear tree commitment-based ownership protection model to ensure computational integrity with limited overhead and concise proof. Due to the frequent updates of parameters during training, our commitment scheme introduces a maintainable tree structure to reduce the costs of updating proofs. Distinguished from SNARK-based verifiable computation, our mod
    
[^26]: HiCAST: 使用增强扩散模型的高度定制的任意风格转换

    HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced Diffusion Models. (arXiv:2401.05870v1 [cs.CV])

    [http://arxiv.org/abs/2401.05870](http://arxiv.org/abs/2401.05870)

    HiCAST是一种高度定制的任意风格转换方法，通过引入"风格适配器"和利用潜在扩散模型，可以根据各种语义线索显式定制风格化结果。

    

    任意风格转换（AST）的目标是将风格参考的艺术特征注入给定的图像/视频。现有方法通常关注风格和内容之间的平衡，忽视了对灵活和定制化风格化结果的重要需求，从而限制了它们的实际应用。为了解决这个关键问题，提出了一种新的AST方法，称为HiCAST，它能够根据各种语义线索显式定制风格化结果。具体而言，我们的模型基于潜在扩散模型（LDM）构建，并精心设计以吸收LDM的内容和风格实例条件。其特点是引入了"风格适配器"，允许用户通过对齐LDM中的多层风格信息和内在知识来灵活操作输出结果。最后，我们进一步扩展了我们的模型以执行视频AST。借助一种新颖的学习目标进行了提升。

    The goal of Arbitrary Style Transfer (AST) is injecting the artistic features of a style reference into a given image/video. Existing methods usually focus on pursuing the balance between style and content, whereas ignoring the significant demand for flexible and customized stylization results and thereby limiting their practical application. To address this critical issue, a novel AST approach namely HiCAST is proposed, which is capable of explicitly customizing the stylization results according to various source of semantic clues. In the specific, our model is constructed based on Latent Diffusion Model (LDM) and elaborately designed to absorb content and style instance as conditions of LDM. It is characterized by introducing of \textit{Style Adapter}, which allows user to flexibly manipulate the output results by aligning multi-level style information and intrinsic knowledge in LDM. Lastly, we further extend our model to perform video AST. A novel learning objective is leveraged for
    
[^27]: 通过加速度计数据推断讲话意图——野外环境中的研究

    Inferring Intentions to Speak Using Accelerometer Data In-the-Wild. (arXiv:2401.05849v1 [cs.LG])

    [http://arxiv.org/abs/2401.05849](http://arxiv.org/abs/2401.05849)

    通过加速度计数据推断成功和失败的讲话意图，在野外环境中的研究表明加速度计数据中存在有用的信息，但不足以可靠地捕捉讲话意图。

    

    人类具有良好的自然直觉，可以识别出他人有话要说的时候。如果人工智能也能识别出讲话意图，将会非常有趣。特别是在人工智能引导团体讨论的场景下，这将是一项有用的技能。本研究探讨了通过加速度计数据推断成功和失败的讲话意图。之所以选择加速度计数据，是因为它具有隐私保护功能，同时在野外环境中易于实现，可以放置在智能徽章上。使用真实社交网络事件的数据来训练一个机器学习模型，旨在推断讲话意图。数据中的一部分不成功的讲话意图案例被注释。模型在成功的讲话意图上进行训练，并在成功和失败的案例上进行评估。总之，加速度计数据中存在有用的信息，但不足以可靠地捕捉讲话意图。例如，姿势变化与讲话意图具有相关性。

    Humans have good natural intuition to recognize when another person has something to say. It would be interesting if an AI can also recognize intentions to speak. Especially in scenarios when an AI is guiding a group discussion, this can be a useful skill. This work studies the inference of successful and unsuccessful intentions to speak from accelerometer data. This is chosen because it is privacy-preserving and feasible for in-the-wild settings since it can be placed in a smart badge. Data from a real-life social networking event is used to train a machine-learning model that aims to infer intentions to speak. A subset of unsuccessful intention-to-speak cases in the data is annotated. The model is trained on the successful intentions to speak and evaluated on both the successful and unsuccessful cases. In conclusion, there is useful information in accelerometer data, but not enough to reliably capture intentions to speak. For example, posture shifts are correlated with intentions to 
    
[^28]: 推动带隙和介电常数的帕累托前沿：基于机器学习引导的寻找介电材料的研究

    Pushing the Pareto front of band gap and permittivity: ML-guided search for dielectric materials. (arXiv:2401.05848v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2401.05848](http://arxiv.org/abs/2401.05848)

    这项研究通过使用多目标优化方法和机器学习技术，在介电材料研究中取得了突破。研究人员成功地合成和表征了两种新型介电材料，CsTaTeO6和Bi2Zr2O7，为未知材料的寻找提供了高效的工作流程。

    

    高介电常数的材料在外部电场下容易极化，在许多现代电子设备中发挥着重要的功能。它们的实际效用由两个相互冲突的特性决定：高介电常数往往出现在带隙较窄的材料中，限制了介电击穿之前的工作电压。我们提出了一个高通量的工作流程，将元素替代、机器学习预筛选、从头计算模拟和人类专家直觉相结合，以有效地探索未知材料的潜在介电性能，从而合成和表征两种新型的介电材料CsTaTeO6和Bi2Zr2O7。我们的关键思想是在凸面帕累托前沿的多目标优化环境中应用机器学习。虽然通常被认为比单目标优化更具挑战性，但我们提出并展示了初步证据表明带隙和介电常数之间的1/x相关性实际上使任务变      得容易。

    Materials with high-dielectric constant easily polarize under external electric fields, allowing them to perform essential functions in many modern electronic devices. Their practical utility is determined by two conflicting properties: high dielectric constants tend to occur in materials with narrow band gaps, limiting the operating voltage before dielectric breakdown. We present a high-throughput workflow that combines element substitution, ML pre-screening, ab initio simulation and human expert intuition to efficiently explore the vast space of unknown materials for potential dielectrics, leading to the synthesis and characterization of two novel dielectric materials, CsTaTeO6 and Bi2Zr2O7. Our key idea is to deploy ML in a multi-objective optimization setting with concave Pareto front. While usually considered more challenging than single-objective optimization, we argue and show preliminary evidence that the $1/x$-correlation between band gap and permittivity in fact makes the tas
    
[^29]: 解码AI的助推：预测AI辅助决策中人类行为的统一框架

    Decoding AI's Nudge: A Unified Framework to Predict Human Behavior in AI-assisted Decision Making. (arXiv:2401.05840v1 [cs.HC])

    [http://arxiv.org/abs/2401.05840](http://arxiv.org/abs/2401.05840)

    本文提出了一个计算框架，可以对不同形式的AI辅助对决策者在AI辅助决策中的影响进行解释性描述。

    

    随着基于AI的决策辅助工具的快速发展，不同形式的AI辅助越来越多地融入到人类决策过程中。为了最好地支持人类在决策过程中，必须定量地了解不同形式的AI辅助如何影响人类的决策行为。为此，当前的研究主要集中在使用“黑盒子”模型进行人类行为的端到端预测，常常缺乏对AI辅助对人类决策过程产生微妙影响的解释。与此同时，优先考虑人类行为预测解释性的方法常常只针对某一特定形式的AI辅助进行调整，难以适应其他形式的辅助。在本文中，我们提出了一个计算框架，可以对不同形式的AI辅助对决策者在AI辅助决策中的影响进行解释性描述。

    With the rapid development of AI-based decision aids, different forms of AI assistance have been increasingly integrated into the human decision making processes. To best support humans in decision making, it is essential to quantitatively understand how diverse forms of AI assistance influence humans' decision making behavior. To this end, much of the current research focuses on the end-to-end prediction of human behavior using ``black-box'' models, often lacking interpretations of the nuanced ways in which AI assistance impacts the human decision making process. Meanwhile, methods that prioritize the interpretability of human behavior predictions are often tailored for one specific form of AI assistance, making adaptations to other forms of assistance difficult. In this paper, we propose a computational framework that can provide an interpretable characterization of the influence of different forms of AI assistance on decision makers in AI-assisted decision making. By conceptualizing
    
[^30]: 重新审视轮廓系数：从微观到宏观聚合。

    Revisiting Silhouette: From Micro to Macro Aggregation. (arXiv:2401.05831v1 [cs.LG])

    [http://arxiv.org/abs/2401.05831](http://arxiv.org/abs/2401.05831)

    本文提出了一种新的聚合策略，用于评估聚类质量。通过对聚类级别的轮廓得分进行平均，并在此基础上对所有聚类的得分进行宏观平均，我们提出的宏观平均轮廓得分对于聚类不均衡和背景噪声是稳健的。

    

    轮廓系数是一种常用的内部聚类评估指标，它会为每个数据点产生一个得分，用于评估其聚类分配的质量。目前，为了评估整个数据集的聚类质量，通常会将数据集中所有点的得分平均成一个单一值，这个策略被称为微观平均。然而我们在本文中通过使用一个合成例子展示了，该微观平均策略对于聚类不均衡和异常值（背景噪声）非常敏感。为了解决这些问题，我们提出了一种替代聚合策略，该策略首先对聚类级别的轮廓得分进行平均，然后再对所有聚类的得分进行宏观平均。基于相同的合成例子，我们展示了提出的宏观平均轮廓得分对于聚类不均衡和背景噪声是稳健的。我们进行了实验研究，结果表明我们提出的宏观平均变体可以更好地估计真实的聚类数量。

    Silhouette coefficient is an established internal clustering evaluation measure that produces a score per data point, assessing the quality of its clustering assignment. To assess the quality of the clustering of the whole dataset, the scores of all the points in the dataset are typically averaged into a single value, a strategy which we call as micro-averaging. As we illustrate in this work, by using a synthetic example, this micro-averaging strategy is sensitive both to cluster imbalance and outliers (background noise). To address these issues, we propose an alternative aggregation strategy, which first averages the silhouette scores at a cluster level and then (macro) averages the scores across the clusters. Based on the same synthetic example, we show that the proposed macro-averaged silhouette score is robust to cluster imbalance and background noise. We have conducted an experimental study showing that our macro-averaged variant provides better estimates of the ground truth numbe
    
[^31]: 医学视觉问答中的幻觉基准评估

    Hallucination Benchmark in Medical Visual Question Answering. (arXiv:2401.05827v1 [cs.CL])

    [http://arxiv.org/abs/2401.05827](http://arxiv.org/abs/2401.05827)

    这项研究创建了医学图像的幻觉基准评估，并全面评估了当前最先进的模型，揭示了幻觉现象在临床环境中的限制和各种提示策略的有效性。

    

    最近大型语言和视觉模型在视觉问答（VQA）上取得了成功，尤其在医学（Med-VQA）领域的应用显示出了为医疗提供有效视觉助手的巨大潜力。然而，这些模型在临床环境中的幻觉现象上并没有进行广泛测试。在本研究中，我们创建了一个医学图像配对问题-回答集的幻觉基准评估，并对当前最先进的模型进行了全面评估。该研究对当前模型的局限性进行了深入分析，并揭示了各种提示策略的有效性。

    The recent success of large language and vision models on vision question answering (VQA), particularly their applications in medicine (Med-VQA), has shown a great potential of realizing effective visual assistants for healthcare. However, these models are not extensively tested on the hallucination phenomenon in clinical settings. Here, we created a hallucination benchmark of medical images paired with question-answer sets and conducted a comprehensive evaluation of the state-of-the-art models. The study provides an in-depth analysis of current models limitations and reveals the effectiveness of various prompting strategies.
    
[^32]: 面向目标导向智能体的演化问题的会话观察

    Towards Goal-Oriented Agents for Evolving Problems Observed via Conversation. (arXiv:2401.05822v1 [cs.AI])

    [http://arxiv.org/abs/2401.05822](http://arxiv.org/abs/2401.05822)

    本研究通过训练一个可以通过与用户对话解决演化问题的聊天机器人，提出了一种应用对话式DQN智能体解决演化问题的架构，并探索了课程学习和改变奖励函数等训练方法对模型性能的影响。

    

    本研究的目标是通过与用户交流解决不能直接观察到的演化问题的聊天机器人的训练。系统包括一个虚拟问题（在这种情况下是一个简单的游戏），一个能够回答自然语言问题并能够观察和执行问题动作的模拟用户，以及一个基于深度Q网络（DQN）的聊天机器人架构。通过与模拟用户进行对话，并利用强化学习来训练聊天机器人解决问题。本文的贡献包括：提出了一种应用对话式DQN智能体解决演化问题的架构，探索了课程学习等训练方法对模型性能的影响，以及在环境复杂性增加的情况下改变奖励函数的效果。

    The objective of this work is to train a chatbot capable of solving evolving problems through conversing with a user about a problem the chatbot cannot directly observe. The system consists of a virtual problem (in this case a simple game), a simulated user capable of answering natural language questions that can observe and perform actions on the problem, and a Deep Q-Network (DQN)-based chatbot architecture. The chatbot is trained with the goal of solving the problem through dialogue with the simulated user using reinforcement learning. The contributions of this paper are as follows: a proposed architecture to apply a conversational DQN-based agent to evolving problems, an exploration of training methods such as curriculum learning on model performance and the effect of modified reward functions in the case of increasing environment complexity.
    
[^33]: Cheetah: 通过高速可微分模拟填补机器学习和粒子加速器物理之间的差距

    Cheetah: Bridging the Gap Between Machine Learning and Particle Accelerator Physics with High-Speed, Differentiable Simulations. (arXiv:2401.05815v1 [physics.acc-ph])

    [http://arxiv.org/abs/2401.05815](http://arxiv.org/abs/2401.05815)

    Cheetah是一种高速可微分模拟工具，可以减少计算时间并实现快速收集大规模数据集。它能够促进加速器调优和系统识别，并与机器学习工具无缝集成。

    

    机器学习已经成为加速器物理学中现代挑战的强大解决方案。然而，有限的束流时间可用性，模拟的计算成本以及优化问题的高维度给生成所需训练机器学习模型的数据带来了重大挑战。在这项工作中，我们引入了Cheetah，一种基于PyTorch的高速可微线性束流动力学代码。Cheetah通过将计算时间减少数个数量级，实现了快速收集大规模数据集，并促进了面向加速器调优和系统识别的高效梯度优化。这使Cheetah成为一个用户友好、易于扩展的工具，与广泛采用的机器学习工具无缝集成。我们通过五个示例展示了Cheetah的实用性，包括强化学习训练、基于梯度的束线调优、基于梯度的系统识别、物理学-i

    Machine learning has emerged as a powerful solution to the modern challenges in accelerator physics. However, the limited availability of beam time, the computational cost of simulations, and the high-dimensionality of optimisation problems pose significant challenges in generating the required data for training state-of-the-art machine learning models. In this work, we introduce Cheetah, a PyTorch-based high-speed differentiable linear-beam dynamics code. Cheetah enables the fast collection of large data sets by reducing computation times by multiple orders of magnitude and facilitates efficient gradient-based optimisation for accelerator tuning and system identification. This positions Cheetah as a user-friendly, readily extensible tool that integrates seamlessly with widely adopted machine learning tools. We showcase the utility of Cheetah through five examples, including reinforcement learning training, gradient-based beamline tuning, gradient-based system identification, physics-i
    
[^34]: 使用对比对齐指令调整LLMs以解决机器翻译中的未知、低资源语言问题

    Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages. (arXiv:2401.05811v1 [cs.CL])

    [http://arxiv.org/abs/2401.05811](http://arxiv.org/abs/2401.05811)

    本文引入了对比对齐指令（AlignInstruct），通过使用统计词对齐构建的跨语言鉴别器实现了跨语言监督，解决了机器翻译中的两个挑战：将支持的语言扩展到未知语言和低资源语言中数据缺乏的问题。实验结果表明，LLMs通过MTInstruct可以有效地翻译未知语言，并且使用AlignInstruct在涉及英语的48个翻译方向上能够持续改善翻译质量。基于鉴别器的指令优于生成型指令。

    

    本文介绍了对比对齐指令（AlignInstruct）来解决大型语言模型（LLMs）上机器翻译（MT）中的两个挑战。一个是将支持的语言扩展到之前未见过的语言。第二个与低资源语言中缺乏数据有关。通过MT指令（MTInstruct）对模型进行微调是应对第一个挑战的一种直接方法。然而，MTInstruct受到第二个挑战中固有的弱语言跨度信号的限制。AlignInstruct通过使用基于统计词对齐构建的跨语言鉴别器来强调跨语言监督。我们基于在多达24种未知语言上对BLOOMZ模型（1b1、3b和7b1）进行微调的结果表明：（1）LLMs可以使用MTInstruct有效地翻译未知语言；（2）AlignInstruct在涉及英语的48个翻译方向上提高了翻译质量的一致性；（3）基于鉴别器的指令优于生成型指令。

    This article introduces contrastive alignment instructions (AlignInstruct) to address two challenges in machine translation (MT) on large language models (LLMs). One is the expansion of supported languages to previously unseen ones. The second relates to the lack of data in low-resource languages. Model fine-tuning through MT instructions (MTInstruct) is a straightforward approach to the first challenge. However, MTInstruct is limited by weak cross-lingual signals inherent in the second challenge. AlignInstruct emphasizes cross-lingual supervision via a cross-lingual discriminator built using statistical word alignments. Our results based on fine-tuning the BLOOMZ models (1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can effectively translate unseen languages using MTInstruct; (2) AlignInstruct led to consistent improvements in translation quality across 48 translation directions involving English; (3) Discriminator-based instructions outperformed their generativ
    
[^35]: 基于图形时空过程的多变量时间序列异常检测方法及其在缺失值情况下的应用

    Graph Spatiotemporal Process for Multivariate Time Series Anomaly Detection with Missing Values. (arXiv:2401.05800v1 [cs.LG])

    [http://arxiv.org/abs/2401.05800](http://arxiv.org/abs/2401.05800)

    该论文介绍了一种名为GST-Pro的框架，利用图形时空过程和异常评分器来解决在采样不规则的多变量时间序列中检测异常的挑战。

    

    对多变量时间序列数据中的异常进行检测对于各种实际应用非常重要，包括智能电网、交通流预测和工业过程控制等。然而，真实世界的时间序列数据通常不是良好结构化的，这给现有方法带来了重大挑战：（1）多变量时间序列数据中的缺失值存在于变量和时间维度，阻碍了对交织的空间和时间依赖关系进行有效建模，导致重要的模式在模型训练过程中被忽视；（2）在采样不规则的观测下进行异常评分的研究较少，这使得在多变量序列中使用现有检测器时很难处理没有完全观测值的情况。在本文中，我们介绍了一种名为GST-Pro的新框架，该框架利用图形时空过程和异常评分器来解决在采样不规则的多变量时间序列中检测异常时面临的挑战。

    The detection of anomalies in multivariate time series data is crucial for various practical applications, including smart power grids, traffic flow forecasting, and industrial process control. However, real-world time series data is usually not well-structured, posting significant challenges to existing approaches: (1) The existence of missing values in multivariate time series data along variable and time dimensions hinders the effective modeling of interwoven spatial and temporal dependencies, resulting in important patterns being overlooked during model training; (2) Anomaly scoring with irregularly-sampled observations is less explored, making it difficult to use existing detectors for multivariate series without fully-observed values. In this work, we introduce a novel framework called GST-Pro, which utilizes a graph spatiotemporal process and anomaly scorer to tackle the aforementioned challenges in detecting anomalies on irregularly-sampled multivariate time series. Our approac
    
[^36]: 设计用于金融情绪分析的异构LLM代理

    Designing Heterogeneous LLM Agents for Financial Sentiment Analysis. (arXiv:2401.05799v1 [cs.CL])

    [http://arxiv.org/abs/2401.05799](http://arxiv.org/abs/2401.05799)

    本研究提出了一种设计框架，使用动态的异构LLM代理，来改进金融情绪分析的准确性，并在实验中取得了令人满意的结果。

    

    大型语言模型（LLM）彻底改变了设计智能系统的可能方式，将焦点从大规模数据获取和新的建模训练转移到了与人类对齐以及战略性地发挥现有预训练模型的全部潜力上。然而，由于金融情绪分析任务的歧视性特征以及缺乏如何在这种背景下利用生成模型的规定性知识，这种范式转变尚未完全实现。本研究调查了新范式的有效性，即在金融情绪分析中使用无需微调的LLM。基于明斯基的心灵和情绪理论，提出了一种采用异构LLM代理的设计框架。该框架使用先前领域知识实例化专门的代理，并对集合的代理讨论进行推理。在金融情绪分析数据集上进行的全面评估表明，该框架可以获得更高的准确性，特别是在讨论频繁的情况下。

    Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focuses from massive data acquisition and new modeling training to human alignment and strategical elicitation of the full potential of existing pre-trained models. This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA), due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage generative models in such a context. This study investigates the effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for FSA. Rooted in Minsky's theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed. The framework instantiates specialized agents using prior domain knowledge of the types of FSA errors and reasons on the aggregated agent discussions. Comprehensive evaluation on FSA datasets show that the framework yields better accuracies, especially when the discussi
    
[^37]: 大型语言模型系统的风险分类、缓解和评估基准

    Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems. (arXiv:2401.05778v1 [cs.CL])

    [http://arxiv.org/abs/2401.05778](http://arxiv.org/abs/2401.05778)

    这篇论文介绍了大型语言模型系统的风险分类、缓解和评估基准，调查并分析了与每个模块相关的潜在风险。

    

    大型语言模型（LLM）在解决各种自然语言处理任务方面具有强大的能力。然而，LLM系统的安全和安全问题已经成为广泛应用的主要障碍。许多研究已经广泛调查了LLM系统的风险，并开发了相应的缓解策略。OpenAI、Google、Meta和Anthropic等领先企业也在负责任的LLM方面做出了很多努力。因此，有一个越来越大的需求来组织现有的研究，并为社区建立全面的分类体系。在本文中，我们深入研究了LLM系统的四个基本模块，包括用于接收提示的输入模块、在大量语料库上进行训练的语言模型、用于开发和部署的工具链模块以及用于导出LLM生成内容的输出模块。基于此，我们提出了一个全面的分类体系，系统分析了与LLM系统的每个模块相关的潜在风险。

    Large language models (LLMs) have strong capabilities in solving diverse natural language processing tasks. However, the safety and security issues of LLM systems have become the major obstacle to their widespread application. Many studies have extensively investigated risks in LLM systems and developed the corresponding mitigation strategies. Leading-edge enterprises such as OpenAI, Google, Meta, and Anthropic have also made lots of efforts on responsible LLMs. Therefore, there is a growing need to organize the existing studies and establish comprehensive taxonomies for the community. In this paper, we delve into four essential modules of an LLM system, including an input module for receiving prompts, a language model trained on extensive corpora, a toolchain module for development and deployment, and an output module for exporting LLM-generated content. Based on this, we propose a comprehensive taxonomy, which systematically analyzes potential risks associated with each module of an 
    
[^38]: 知识转化：一种用于模型压缩的新途径

    Knowledge Translation: A New Pathway for Model Compression. (arXiv:2401.05772v1 [cs.LG])

    [http://arxiv.org/abs/2401.05772](http://arxiv.org/abs/2401.05772)

    本文提出了一种新的知识转化（KT）框架，通过训练一个“翻译”模型来接收较大模型的参数并生成压缩参数，从而实现模型压缩，而无需重新训练或施加架构限制。

    

    深度学习在近年来取得了显著的进展，但训练、推理和模型存储开销却在不断增加。尽管现有的模型压缩方法致力于在保持高准确性的同时减少模型参数的数量，但它们不可避免地需要重新训练压缩模型或施加架构限制。为了克服这些限制，本文提出了一种新的框架，称为知识转化（KT），其中训练一个“翻译”模型来接收较大模型的参数并生成压缩参数。知识转化的概念借鉴自语言翻译，它有效地利用神经网络将不同的语言转换为相同的意思。因此，本文探索了神经网络将不同大小的模型转换为保持其功能性的潜力。我们提出了一个全面的KT框架，介绍了数据增强策略。

    Deep learning has witnessed significant advancements in recent years at the cost of increasing training, inference, and model storage overhead. While existing model compression methods strive to reduce the number of model parameters while maintaining high accuracy, they inevitably necessitate the re-training of the compressed model or impose architectural constraints. To overcome these limitations, this paper presents a novel framework, termed \textbf{K}nowledge \textbf{T}ranslation (KT), wherein a ``translation'' model is trained to receive the parameters of a larger model and generate compressed parameters. The concept of KT draws inspiration from language translation, which effectively employs neural networks to convert different languages, maintaining identical meaning. Accordingly, we explore the potential of neural networks to convert models of disparate sizes, while preserving their functionality. We propose a comprehensive framework for KT, introduce data augmentation strategie
    
[^39]: 互联网上的大量内容都是机器翻译的：来自多向并行性的洞察

    A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism. (arXiv:2401.05749v1 [cs.CL])

    [http://arxiv.org/abs/2401.05749](http://arxiv.org/abs/2401.05749)

    互联网上的大量内容是通过机器翻译多向翻译的，其低质量可能会对使用多语言大型语言模型进行训练产生严重影响。

    

    我们展示了互联网上的内容经常被翻译成多种语言，并且这些多向翻译的低质量表明它们很可能是使用机器翻译（MT）创建的。多向并行的机器生成内容不仅在资源较少的语言中占主导地位，而且构成该语言中总体网页内容的很大一部分。我们还发现证据表明，被翻译成多种语言的内容存在选择性偏差，与将低质量英文内容通过机器翻译大规模翻译成许多资源较少的语言一致。我们的工作对于在网络上从单语和双语数据训练多语言大型语言模型等模型提出了严重的担忧。

    We show that content on the web is often translated into many languages, and the low quality of these multi-way translations indicates they were likely created using Machine Translation (MT). Multi-way parallel, machine generated content not only dominates the translations in lower resource languages; it also constitutes a large fraction of the total web content in those languages. We also find evidence of a selection bias in the type of content which is translated into many languages, consistent with low quality English content being translated en masse into many lower resource languages, via MT. Our work raises serious concerns about training models such as multilingual large language models on both monolingual and bilingual data scraped from the web.
    
[^40]: 在元组删除语义下的存在规则一致性查询回答

    Consistent Query Answering for Existential Rules under Tuple-Deletion Semantics. (arXiv:2401.05743v1 [cs.AI])

    [http://arxiv.org/abs/2401.05743](http://arxiv.org/abs/2401.05743)

    我们研究了存在规则下基于元组删除语义的一致性查询回答，并确定了一般类别和若干子类别的数据复杂性。我们发现在某些情况下这些问题是可处理的，甚至可以进行一阶重写。这项研究提出了新的查询重写技术，可用于实现容忍不一致性的查询回答系统。

    

    我们研究了基于存在规则的知识库的一致性查询回答。具体地，我们确定了在元组删除语义下一般类别的多义存在规则以及其若干子类别（无环、线性、完全、保护和粘滞）的一致性查询回答和修复检查的数据复杂性。特别地，我们识别出了几种情况，在这些情况下这些问题是可处理的，甚至可以进行一阶重写，并呈现了新的查询重写技术，可以成为实用的容忍不一致性查询回答系统的基础。

    We study consistent query answering over knowledge bases expressed by existential rules. Specifically, we establish the data complexity of consistent query answering and repair checking under tuple-deletion semantics for a general class of disjunctive existential rules and for several subclasses thereof (acyclic, linear, full, guarded, and sticky). In particular, we identify several cases in which the above problems are tractable or even first-order rewritable, and present new query rewriting techniques that can be the basis for practical inconsistency-tolerant query answering systems.
    
[^41]: 使用高效的组合正样本配对增强对比学习

    Enhancing Contrastive Learning with Efficient Combinatorial Positive Pairing. (arXiv:2401.05730v1 [cs.CV])

    [http://arxiv.org/abs/2401.05730](http://arxiv.org/abs/2401.05730)

    本论文提出了一种名为ECPP（高效组合正样本配对）的多视图策略，通过增加视图数量、采用小尺寸视图和修改负采样，提高了对比学习的学习速度和性能。通过将ECPP应用于SimCLR等方法，本研究验证了ECPP的有效性。

    

    在过去几年里，对比学习在视觉无监督表示学习的成功中起着核心作用。与此同时，高性能的非对比学习方法也得到了发展。虽然大部分工作仅利用了两个视图，我们仔细审查了现有的多视图方法，并提出了一种通用的多视图策略，可以提高任何对比或非对比方法的学习速度和性能。我们首先分析了CMC的全图范式，并经验性地表明，在小学习率和早期训练时，K个视图的学习速度可以增加$K$-$views$倍。然后，我们通过混合仅裁剪增强创建的视图，采用SwAV多裁剪中的小尺寸视图，并修改负采样来升级CMC的全图。产生的多视图策略被称为ECPP（高效组合正样本配对）。通过将ECPP应用于SimCLR和一个样本效果验证其有效性。

    In the past few years, contrastive learning has played a central role for the success of visual unsupervised representation learning. Around the same time, high-performance non-contrastive learning methods have been developed as well. While most of the works utilize only two views, we carefully review the existing multi-view methods and propose a general multi-view strategy that can improve learning speed and performance of any contrastive or non-contrastive method. We first analyze CMC's full-graph paradigm and empirically show that the learning speed of $K$-views can be increased by $_{K}\mathrm{C}_{2}$ times for small learning rate and early training. Then, we upgrade CMC's full-graph by mixing views created by a crop-only augmentation, adopting small-size views as in SwAV multi-crop, and modifying the negative sampling. The resulting multi-view strategy is called ECPP (Efficient Combinatorial Positive Pairing). We investigate the effectiveness of ECPP by applying it to SimCLR and a
    
[^42]: R-BI: 正则化批量输入增强增量解码框架用于低延迟同时语音翻译

    R-BI: Regularized Batched Inputs enhance Incremental Decoding Framework for Low-Latency Simultaneous Speech Translation. (arXiv:2401.05700v1 [cs.CL])

    [http://arxiv.org/abs/2401.05700](http://arxiv.org/abs/2401.05700)

    本文介绍了一种名为“正则化批量输入”的新颖策略，通过增强输入多样性来减轻低延迟同时语音翻译中的输出错误。

    

    增量解码是一种有效的框架，它在同时条件下使用离线模型而不修改原始模型，使其适用于低延迟的同时语音翻译。然而，当系统输出不完整的输入时，这个框架可能会引入错误。为了减少这些输出错误，可以采用几种策略，如Hold-n，LA-n和SP-n，但需要仔细选择超参数n以获取最佳性能。此外，这些策略对于端到端系统而言更适用于级联系统。在本文中，我们提出了一种新颖且高效的策略，称为“正则化批量输入”。我们的方法通过增强输入多样性来减轻输出错误。我们为端到端系统和级联系统提供了特定的正则化技术。我们在IWSLT Simultaneous Speech Translation（SimulST）任务上进行了实验，证明我们的方法能够在低延迟同时实现语音翻译。

    Incremental Decoding is an effective framework that enables the use of an offline model in a simultaneous setting without modifying the original model, making it suitable for Low-Latency Simultaneous Speech Translation. However, this framework may introduce errors when the system outputs from incomplete input. To reduce these output errors, several strategies such as Hold-$n$, LA-$n$, and SP-$n$ can be employed, but the hyper-parameter $n$ needs to be carefully selected for optimal performance. Moreover, these strategies are more suitable for end-to-end systems than cascade systems. In our paper, we propose a new adaptable and efficient policy named "Regularized Batched Inputs". Our method stands out by enhancing input diversity to mitigate output errors. We suggest particular regularization techniques for both end-to-end and cascade systems. We conducted experiments on IWSLT Simultaneous Speech Translation (SimulST) tasks, which demonstrate that our approach achieves low latency while
    
[^43]: 深度学习与机制设计：关键结果和一些新的应用

    Deep Learning Meets Mechanism Design: Key Results and Some Novel Applications. (arXiv:2401.05683v1 [cs.GT])

    [http://arxiv.org/abs/2401.05683](http://arxiv.org/abs/2401.05683)

    本文介绍了深度学习与机制设计的结合，探讨了使用深度学习方法在无法同时满足所有期望特性的情况下，学习近似满足特性要求的机制。

    

    机制设计本质上是对游戏的逆向工程，涉及在博弈中诱导一种方式，使得诱导的博弈在博弈均衡中满足一组期望的特性。机制的期望特性包括激励兼容性、个体合理性、福利最大化、收入最大化（或成本最小化）、分配公平等。根据机制设计理论，只有某些严格的子集可以同时被任何给定的机制完全满足。在现实世界应用中，通常所需的机制可能需要一些在理论上无法同时满足的特性子集。在这种情况下，一个显著的近期方法是使用基于深度学习的方法，通过最小化适当定义的损失函数来学习一个近似满足所需特性的机制。在本文中，我们从相关文献中介绍了技术细节。

    Mechanism design is essentially reverse engineering of games and involves inducing a game among strategic agents in a way that the induced game satisfies a set of desired properties in an equilibrium of the game. Desirable properties for a mechanism include incentive compatibility, individual rationality, welfare maximisation, revenue maximisation (or cost minimisation), fairness of allocation, etc. It is known from mechanism design theory that only certain strict subsets of these properties can be simultaneously satisfied exactly by any given mechanism. Often, the mechanisms required by real-world applications may need a subset of these properties that are theoretically impossible to be simultaneously satisfied. In such cases, a prominent recent approach is to use a deep learning based approach to learn a mechanism that approximately satisfies the required properties by minimizing a suitably defined loss function. In this paper, we present, from relevant literature, technical details 
    
[^44]: 在辅助防御性网络操作中使用图神经网络

    Use of Graph Neural Networks in Aiding Defensive Cyber Operations. (arXiv:2401.05680v1 [cs.CR])

    [http://arxiv.org/abs/2401.05680](http://arxiv.org/abs/2401.05680)

    本文将研究如何利用图神经网络来辅助打破网络攻击生命周期的每个阶段，通过处理和学习来自异构网络威胁数据，以增强防御措施的有效性。

    

    在一个日益互联的世界中，信息是现代社会的命脉，常见的网络攻击破坏了数字系统和信息的机密性、完整性和可用性。此外，网络攻击根据目标的不同而不同，并且迅速演变以掩盖防御系统。然而，典型的网络攻击展示了从攻击发起到最终解决的一系列阶段，称为攻击生命周期。这些多样的特征和网络攻击的不懈演进使得网络防御采纳了现代方法，如机器学习，以增强防御措施并打破攻击生命周期。在采用的机器学习方法中，图神经网络已经成为一种有前途的方法，可以通过处理和学习来自异构网络威胁数据来增强防御措施的有效性。在本文中，我们将研究在辅助打破攻击生命周期的每个阶段中应用GNN的方法。

    In an increasingly interconnected world, where information is the lifeblood of modern society, regular cyber-attacks sabotage the confidentiality, integrity, and availability of digital systems and information. Additionally, cyber-attacks differ depending on the objective and evolve rapidly to disguise defensive systems. However, a typical cyber-attack demonstrates a series of stages from attack initiation to final resolution, called an attack life cycle. These diverse characteristics and the relentless evolution of cyber attacks have led cyber defense to adopt modern approaches like Machine Learning to bolster defensive measures and break the attack life cycle. Among the adopted ML approaches, Graph Neural Networks have emerged as a promising approach for enhancing the effectiveness of defensive measures due to their ability to process and learn from heterogeneous cyber threat data. In this paper, we look into the application of GNNs in aiding to break each stage of one of the most re
    
[^45]: EsaCL: 高效稀疏模型的持续学习

    EsaCL: Efficient Continual Learning of Sparse Models. (arXiv:2401.05667v1 [cs.LG])

    [http://arxiv.org/abs/2401.05667](http://arxiv.org/abs/2401.05667)

    EsaCL是一种高效稀疏模型持续学习方法，通过自动修剪冗余参数并避免重新训练，解决了持续学习中存储和计算需求增加的问题。

    

    在持续学习环境中，一个关键挑战是在不忘记如何执行先前学习任务的情况下，高效地学习一系列任务。许多现有的方法通过在先前任务上重新训练模型或扩展模型以适应新任务来解决这个问题。然而，这些方法通常面临存储和计算需求的增加问题，而对于稀疏模型来说，由于需要昂贵的稀疏化后重新训练，这个问题更加严重。为了解决这个挑战，我们提出了一种新的高效稀疏模型持续学习方法（EsaCL），它可以自动修剪冗余参数，而不会对模型的预测能力产生不利影响，并且可以避免重新训练的需要。我们对参数修剪的损失函数进行了理论分析，并设计了一种基于损失函数对模型参数敏感性的方向性修剪（SDP）策略。SDP保证了模型在学习新任务时的性能和稳定性。

    A key challenge in the continual learning setting is to efficiently learn a sequence of tasks without forgetting how to perform previously learned tasks. Many existing approaches to this problem work by either retraining the model on previous tasks or by expanding the model to accommodate new tasks. However, these approaches typically suffer from increased storage and computational requirements, a problem that is worsened in the case of sparse models due to need for expensive re-training after sparsification. To address this challenge, we propose a new method for efficient continual learning of sparse models (EsaCL) that can automatically prune redundant parameters without adversely impacting the model's predictive power, and circumvent the need of retraining. We conduct a theoretical analysis of loss landscapes with parameter pruning, and design a directional pruning (SDP) strategy that is informed by the sharpness of the loss function with respect to the model parameters. SDP ensures
    
[^46]: 迈向对话式诊断人工智能

    Towards Conversational Diagnostic AI. (arXiv:2401.05654v1 [cs.AI])

    [http://arxiv.org/abs/2401.05654](http://arxiv.org/abs/2401.05654)

    本文介绍了一种基于大型语言模型的人工智能系统AMIE，该系统利用自我对战的模拟环境和自动化反馈机制进行诊断对话，并且通过评估病史采集、诊断准确性、管理推理、沟通技巧和同理心等维度性能，与初级保健医生进行了比较。

    

    医学的核心在于医生和患者之间的对话，熟练的病史采集为准确的诊断、有效的治疗和持久的信任铺平了道路。能够进行诊断对话的人工智能系统可以提高医疗的可及性、一致性和质量。然而，接近临床专家的水平仍然是一个重大挑战。在这里，我们介绍了AMIE（Articulate Medical Intelligence Explorer），这是一个基于大型语言模型（LLM）的优化于诊断对话的人工智能系统。AMIE利用一种新颖的基于自我对战的模拟环境，并带有自动化的反馈机制，以便在不同的疾病状况、专业领域和情境下实现学习的扩展。我们设计了一个评估临床有意义维度性能的框架，包括病史采集、诊断准确性、管理推理、沟通技巧和同理心。我们将AMIE的性能与初级保健医生（PCPs）进行了比较，并使用了随机、双盲十字设计的试验。

    At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating clinicians' expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue.  AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. We compared AMIE's performance to that of primary care physicians (PCPs) in a randomized, double-blind cross
    
[^47]: DrawTalking：通过草图和语言建立互动世界

    DrawTalking: Building Interactive Worlds by Sketching and Speaking. (arXiv:2401.05631v1 [cs.HC])

    [http://arxiv.org/abs/2401.05631](http://arxiv.org/abs/2401.05631)

    用户通过草图和语言建立互动世界的交互式方法，具有用户控制和灵活性，无需编程即可实现编程功能。适用于各种创造性探索性场景。

    

    我们引入了一种交互式方法，DrawTalking，用户可以通过草图和语言建立互动世界。它强调用户控制和灵活性，并且在没有编程的情况下提供了类似编程的能力。我们在iPad上实现了它。一项开放式研究表明，这种机制与许多创造性探索性用例相契合和适用。我们希望能够激发和指导未来自然用户中心界面的研究。

    We introduce an interactive approach, DrawTalking, in which the user builds interactive worlds by sketching and speaking. It emphasizes user control and flexibility, and gives programming-like capability without code. We implemented it on the iPad. An open-ended study shows the mechanics resonate and are applicable to many creative-exploratory use cases. We hope to inspire and inform research in future natural user-centered interfaces.
    
[^48]: 在大型语言模型的问题求解中，简洁的思维链的好处

    The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models. (arXiv:2401.05618v1 [cs.CL])

    [http://arxiv.org/abs/2401.05618](http://arxiv.org/abs/2401.05618)

    本文研究了在大型语言模型中使用简洁的思维链提示对问题求解的影响，实验结果表明简洁性不仅降低了回答长度，且对问题解决性能影响可以忽略。然而在数学问题上有一定的性能下降。这对AI系统工程师和研究人员都有实际意义。

    

    在本文中，我们介绍了简洁的思维链(CCoT)提示。我们将标准的CoT和CCoT提示进行比较，以了解简洁性对回答长度和正确答案准确性的影响。我们使用GPT-3.5和GPT-4进行了多项选择问答(MCQA)基准的评估。CCoT将GPT-3.5和GPT-4的平均回答长度分别减少了48.70％，对问题解决性能几乎没有影响。然而，在数学问题上，带有CCoT的GPT-3.5会导致性能下降27.69％。总体而言，CCoT导致每个标记的成本平均降低了22.67％。这些结果对于使用CoT提示工程技术的AI系统工程师来解决真实世界问题的LLM具有实际意义。此外，这些结果为研究LLM中逐步推理的形成行为的AI研究人员提供了更广泛的见解。

    In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We compared standard CoT and CCoT prompts to see how conciseness impacts response length and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4 with a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced average response length by 48.70% for both GPT-3.5 and GPT-4 while having a negligible impact on problem-solving performance. However, on math problems, GPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads to an average per-token cost reduction of 22.67%. These results have practical implications for AI systems engineers using LLMs to solve real-world problems with CoT prompt-engineering techniques. In addition, these results provide more general insight for AI researchers studying the emergent behavior of step-by-step reasoning in LLMs.
    
[^49]: 图形Q-Learning用于组合优化

    Graph Q-Learning for Combinatorial Optimization. (arXiv:2401.05610v1 [cs.LG])

    [http://arxiv.org/abs/2401.05610](http://arxiv.org/abs/2401.05610)

    本论文提出并证明了图神经网络可以应用于解决组合优化问题。通过将优化过程形式化为一个顺序决策问题，并使用Q-Learning训练的GNNs来学习策略，可以达到接近最先进的基于启发式求解器的性能，同时只需使用部分参数和训练时间。

    

    图形结构化数据在自然科学和社会科学中无处不在，而图神经网络（GNNs）最近已被证明可以有效地解决图形数据上的预测和推理问题。在本文中，我们提出并证明了GNNs可以应用于解决组合优化（CO）问题。CO涉及在通常非常庞大的离散解空间上优化函数。为了学习解决CO问题，我们将优化过程形式化为一个顺序决策问题，其中回报与候选解与最优解的接近程度有关。我们使用GNN来学习策略，以迭代地构建越来越有前景的候选解。我们提出初步证据表明，使用Q-Learning训练的GNNs可以解决CO问题，其性能接近最先进的基于启发式求解器的性能，仅使用了部分参数和训练时间。

    Graph-structured data is ubiquitous throughout natural and social sciences, and Graph Neural Networks (GNNs) have recently been shown to be effective at solving prediction and inference problems on graph data. In this paper, we propose and demonstrate that GNNs can be applied to solve Combinatorial Optimization (CO) problems. CO concerns optimizing a function over a discrete solution space that is often intractably large. To learn to solve CO problems, we formulate the optimization process as a sequential decision making problem, where the return is related to how close the candidate solution is to optimality. We use a GNN to learn a policy to iteratively build increasingly promising candidate solutions. We present preliminary evidence that GNNs trained through Q-Learning can solve CO problems with performance approaching state-of-the-art heuristic-based solvers, using only a fraction of the parameters and training time.
    
[^50]: REBUS: 一种对符号理解进行鲁棒评估的基准测试

    REBUS: A Robust Evaluation Benchmark of Understanding Symbols. (arXiv:2401.05604v1 [cs.CL])

    [http://arxiv.org/abs/2401.05604](http://arxiv.org/abs/2401.05604)

    提出了一种用于评估多模态大规模语言模型在rebus谜题上性能的新的基准测试。发现专有模型表现优于其他测试模型，但最佳模型的准确率仅为24%。该基准测试可用于识别知识上的主要缺陷。

    

    我们提出了一种新的基准测试，用于评估多模态大规模语言模型在rebus谜题上的性能。该数据集包括333个原始的基于图像的文字游戏示例，涵盖了电影、作曲家、主要城市和食物等13个类别。为了在识别提示的词语或短语的基准测试中获得良好性能，模型必须结合图像识别和字符串操作，进行假设检验、多步推理和对人类认知的理解，这使得评估能力变得复杂而多模态。我们发现专有模型如GPT-4V和Gemini Pro明显优于所有其他测试模型。然而，即使最好的模型也只有24%的最终准确率，突显出在推理方面需要实质性的改进。此外，模型很少理解谜题的所有部分，几乎总是无法事后解释正确答案。因此，我们的基准测试可以用于识别知识的主要缺陷。

    We propose a new benchmark evaluating the performance of multimodal large language models on rebus puzzles. The dataset covers 333 original examples of image-based wordplay, cluing 13 categories such as movies, composers, major cities, and food. To achieve good performance on the benchmark of identifying the clued word or phrase, models must combine image recognition and string manipulation with hypothesis testing, multi-step reasoning, and an understanding of human cognition, making for a complex, multimodal evaluation of capabilities. We find that proprietary models such as GPT-4V and Gemini Pro significantly outperform all other tested models. However, even the best model has a final accuracy of just 24%, highlighting the need for substantial improvements in reasoning. Further, models rarely understand all parts of a puzzle, and are almost always incapable of retroactively explaining the correct answer. Our benchmark can therefore be used to identify major shortcomings in the knowle
    
[^51]: POMP:用于低资源无监督神经机器翻译中的概率驱动元图提示器

    POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Translation. (arXiv:2401.05596v1 [cs.CL])

    [http://arxiv.org/abs/2401.05596](http://arxiv.org/abs/2401.05596)

    POMP是一种新颖的方法，使用动态的、基于抽样的多辅助语言图形，提高了语言模型在低资源语言中的翻译能力。

    

    低资源语言在有限的平行数据下面临着在监督神经机器翻译中的挑战，因此研究无监督方法。无监督神经机器翻译方法，包括反向翻译、迁移学习和基于枢轴的翻译，为低资源语言翻译提供了实用的解决方案，但是它们受到合成数据噪声、语言偏差和错误传播等问题的影响，这些问题可以通过大型语言模型进行缓解。语言模型通过上下文学习和有监督微调方法改进了NMT，但是训练数据不足使得在低资源语言上的性能较差。我们认为语言模型可以通过辅助语言减少语言噪声，提高低资源语言的翻译质量。在本文中，我们提出了一种名为POMP的概率驱动元图提示器，它采用了基于动态抽样的多个辅助语言的图形，以增强语言模型在低资源语言上的翻译能力。

    Low-resource languages (LRLs) face challenges in supervised neural machine translation due to limited parallel data, prompting research into unsupervised methods. Unsupervised neural machine translation (UNMT) methods, including back-translation, transfer learning, and pivot-based translation, offer practical solutions for LRL translation, but they are hindered by issues like synthetic data noise, language bias, and error propagation, which can potentially be mitigated by Large Language Models (LLMs). LLMs have advanced NMT with in-context learning (ICL) and supervised fine-tuning methods, but insufficient training data results in poor performance in LRLs. We argue that LLMs can mitigate the linguistic noise with auxiliary languages to improve translations in LRLs. In this paper, we propose Probability-driven Meta-graph Prompter (POMP), a novel approach employing a dynamic, sampling-based graph of multiple auxiliary languages to enhance LLMs' translation capabilities for LRLs. POMP inv
    
[^52]: FourCastNeXt：用有限计算资源提高FourCastNet的训练效果

    FourCastNeXt: Improving FourCastNet Training with Limited Compute. (arXiv:2401.05584v1 [cs.CV])

    [http://arxiv.org/abs/2401.05584](http://arxiv.org/abs/2401.05584)

    本研究提出了改进的方法，可以使用仅需基线要求的1%计算资源训练FourCastNet，并且保持了与基线相当或甚至更好的模型性能。

    

    最近，FourCastNet神经地球系统模型（NESM）在预测各种大气变量方面表现出了令人印象深刻的结果，该模型在ERA5再分析数据集上进行了训练。虽然与基本变压器相比，FourCastNet在序列长度上享有准线性的时间和内存复杂度，而基于ERA5从头开始训练FourCastNet仍然需要大量的计算资源，这对于大多数研究人员来说是昂贵甚至无法获得的。在本文中，我们将展示改进的方法，可以使用仅需要基线要求的1%计算资源来训练FourCastNet，同时保持模型性能至少与基线相当甚至更好。

    Recently, the FourCastNet Neural Earth System Model (NESM) has shown impressive results on predicting various atmospheric variables, trained on the ERA5 reanalysis dataset. While FourCastNet enjoys quasi-linear time and memory complexity in sequence length compared to quadratic complexity in vanilla transformers, training FourCastNet on ERA5 from scratch still requires large amount of compute resources, which is expensive or even inaccessible to most researchers. In this work, we will show improved methods that can train FourCastNet using only 1% of the compute required by the baseline, while maintaining model performance or par or even better than the baseline.
    
[^53]: 用于合作多智能体系统的先天价值驱动增强学习

    Innate-Values-driven Reinforcement Learning for Cooperative Multi-Agent Systems. (arXiv:2401.05572v1 [cs.LG])

    [http://arxiv.org/abs/2401.05572](http://arxiv.org/abs/2401.05572)

    本文提出了一个先天价值驱动增强学习（IVRL）模型，用于描述多智能体在合作中的复杂行为。该模型通过建立智能体对群体效用和系统成本的认知，满足其合作伙伴的需求，支持其社区并融入人类社会。

    

    先天价值描述了智能体的内在动机，反映了他们追求目标和发展多样技能以满足各种需求的固有兴趣和偏好。强化学习的本质是基于奖励驱动（如效用）的行为互动学习，类似于自然智能体。特别是在多智能体系统中，建立智能体对平衡群体效用和系统成本的认知，满足群体成员在合作中的需求，是个体为支持其社区和融入人类社会而学习的一个关键问题。本文提出了一种分层复合内在价值增强学习模型 - 先天价值驱动增强学习，用于描述多智能体合作中复杂的互动行为。

    Innate values describe agents' intrinsic motivations, which reflect their inherent interests and preferences to pursue goals and drive them to develop diverse skills satisfying their various needs. The essence of reinforcement learning (RL) is learning from interaction based on reward-driven (such as utilities) behaviors, much like natural agents. It is an excellent model to describe the innate-values-driven (IV) behaviors of AI agents. Especially in multi-agent systems (MAS), building the awareness of AI agents to balance the group utilities and system costs and satisfy group members' needs in their cooperation is a crucial problem for individuals learning to support their community and integrate human society in the long term. This paper proposes a hierarchical compound intrinsic value reinforcement learning model -innate-values-driven reinforcement learning termed IVRL to describe the complex behaviors of multi-agent interaction in their cooperation. We implement the IVRL architec
    
[^54]: 基于软标签的孪生网络在无监督病变检测和筛查乳房X线片预训练方面的应用

    Siamese Networks with Soft Labels for Unsupervised Lesion Detection and Patch Pretraining on Screening Mammograms. (arXiv:2401.05570v1 [cs.CV])

    [http://arxiv.org/abs/2401.05570](http://arxiv.org/abs/2401.05570)

    这项研究提出了一种基于软标签的孪生网络方法，利用对侧乳房X线片训练神经网络，以在无监督情况下区分异常病变和背景组织。实验证明，通过欧氏距离衍生的软标签，可以有效地区分医学成像中的病变。

    

    自监督学习已成为一种流行的方法，用于预训练深度学习模型，并将其转移到执行下游任务。然而，大多数这些方法是针对包含清晰纹理、轮廓和明显色彩对比的大规模图像数据集开发的。尚不确定这些方法是否同样有效用于医学成像，因为感兴趣区域往往与周围组织融合不明显。在本研究中，我们提出了一种替代方法，使用对侧乳房X线片训练神经网络，以在成对包含正常图像时编码相似嵌入，在成对包含正常和异常图像时编码不同嵌入。我们的方法利用了人体自然对称性作为弱标签，以完全无监督的方式学习区分异常病变和背景组织。我们的研究结果表明，通过结合欧氏距离得出的软标签，这是可行的。

    Self-supervised learning has become a popular way to pretrain a deep learning model and then transfer it to perform downstream tasks. However, most of these methods are developed on large-scale image datasets that contain natural objects with clear textures, outlines, and distinct color contrasts. It remains uncertain whether these methods are equally effective for medical imaging, where the regions of interest often blend subtly and indistinctly with the surrounding tissues. In this study, we propose an alternative method that uses contralateral mammograms to train a neural network to encode similar embeddings when a pair contains both normal images and different embeddings when a pair contains normal and abnormal images. Our approach leverages the natural symmetry of human body as weak labels to learn to distinguish abnormal lesions from background tissues in a fully unsupervised manner. Our findings suggest that it's feasible by incorporating soft labels derived from the Euclidean d
    
[^55]: 卧底特工：训练骗人的LLM以通过安全训练

    Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])

    [http://arxiv.org/abs/2401.05566](http://arxiv.org/abs/2401.05566)

    该论文研究了在大型语言模型中训练并保持持久的欺骗性行为，这种行为无法被当前的安全训练技术移除。

    

    人类有能力进行战略性的欺骗行为：在大多数情况下表现出有益的行为，但在有机会的时候却表现出截然不同的行为以追求其他目标。如果一个AI系统学会了这样的欺骗策略，是否能够通过当前最先进的安全训练技术检测并移除它？为了研究这个问题，我们构建了大型语言模型（LLM）中欺骗行为的概念验证样例。例如，我们训练模型，在提示语句中将年份设为2023时编写安全代码，但在年份设为2024时插入有漏洞的代码。我们发现，这种暗门行为可以被持续保留，无法通过标准的安全训练技术（包括监督微调、强化学习和对抗性训练）移除。暗门行为在最大的模型和训练成产生思维链的模型中最为持久。

    Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thoug
    
[^56]: CodePrompt：通过Prompt学习的知识特征改进源代码相关分类

    CodePrompt: Improving Source Code-Related Classification with Knowledge Features through Prompt Learning. (arXiv:2401.05544v1 [cs.CL])

    [http://arxiv.org/abs/2401.05544](http://arxiv.org/abs/2401.05544)

    CodePrompt是一种利用Prompt学习和注意机制技术改进源代码相关分类任务的新方法。它能够提取源代码和相关文本中的丰富知识以提高准确性，并且减少了计算成本。

    

    研究人员已经探索利用预训练语言模型（如CodeBERT）改进源代码相关任务的潜力。先前的研究主要依赖CodeBERT的文本嵌入能力和"[CLS]"句子嵌入信息作为下游源代码相关任务的语义表示进行微调。然而，这些方法需要额外的神经网络层来提取有效特征，导致计算成本更高。此外，现有方法没有利用源代码和相关文本中丰富的知识，可能导致准确性降低。本文提出了一种新的方法CodePrompt，通过Prompt学习和注意机制利用预训练模型中的丰富知识来改进源代码相关分类任务。

    Researchers have explored the potential of utilizing pre-trained language models, such as CodeBERT, to improve source code-related tasks. Previous studies have mainly relied on CodeBERT's text embedding capability and the `[CLS]' sentence embedding information as semantic representations for fine-tuning downstream source code-related tasks. However, these methods require additional neural network layers to extract effective features, resulting in higher computational costs. Furthermore, existing approaches have not leveraged the rich knowledge contained in both source code and related text, which can lead to lower accuracy. This paper presents a novel approach, CodePrompt, which utilizes rich knowledge recalled from a pre-trained model by prompt learning and an attention mechanism to improve source code-related classification tasks. Our approach initially motivates the language model with prompt information to retrieve abundant knowledge associated with the input as representative feat
    
[^57]: 通过森林修剪提高随机森林的准确性和可解释性

    Improving the Accuracy and Interpretability of Random Forests via Forest Pruning. (arXiv:2401.05535v1 [stat.ML])

    [http://arxiv.org/abs/2401.05535](http://arxiv.org/abs/2401.05535)

    通过森林修剪方法，本研究提出了一种兼顾随机森林准确性和决策树可解释性的方法。实验证明，在大多数情景下，这种方法能够显著提高随机森林的性能。

    

    接近几十年的发展之后，随机森林仍然在各种学习问题中提供最先进的准确性，在这方面超越了决策树甚至神经网络等替代机器学习算法。然而，作为一种集成方法，随机森林在解释性方面往往比决策树表现不佳。在本研究中，我们提出了一种事后方法，旨在兼顾随机森林的准确性和决策树的可解释性。为此，我们提出了两种森林修剪方法，以在给定的随机森林内找到最佳子森林，然后在适用的情况下将选定的树合并为一棵。我们的第一种方法依赖于约束穷举搜索，而第二种方法基于LASSO方法的改进。在合成和真实世界数据集上进行的大量实验证明，在大多数情景下，这两种方法中至少有一种能够显著提高随机森林的准确性和可解释性。

    Decades after their inception, random forests continue to provide state-of-the-art accuracy in a variety of learning problems, outperforming in this respect alternative machine learning algorithms such as decision trees or even neural networks. However, being an ensemble method, the one aspect where random forests tend to severely underperform decision trees is interpretability. In the present work, we propose a post-hoc approach that aims to have the best of both worlds: the accuracy of random forests and the interpretability of decision trees. To this end, we present two forest-pruning methods to find an optimal sub-forest within a given random forest, and then, when applicable, combine the selected trees into one. Our first method relies on constrained exhaustive search, while our second method is based on an adaptation of the LASSO methodology. Extensive experiments over synthetic and real world datasets show that, in the majority of scenarios, at least one of the two methods propo
    
[^58]: 多无人水下航行器系统的消除电流影响的最优目标分配和运动规划

    Current Effect-eliminated Optimal Target Assignment and Motion Planning for a Multi-UUV System. (arXiv:2401.05521v1 [cs.RO])

    [http://arxiv.org/abs/2401.05521](http://arxiv.org/abs/2401.05521)

    提出了一种创新算法CBNNTAP，通过整合生物启发的神经网络和目标分配组件，解决了多UUV系统中由海洋电流引入的复杂性和挑战，并有效消除电流对运动规划和目标分配的影响。

    

    本文提出了一种创新方法（CBNNTAP），用于解决多无人水下航行器（UUV）系统在优化目标分配和运动规划时引入的海洋电流复杂性和挑战。所提出算法的核心包括几个关键组件的集成。首先，它采用了一种基于生物启发的神经网络（BINN）方法，预测单个UUV的最有效路径，同时确保车辆之间的碰撞避免。其次，通过考虑BINN算法确定的路径距离，集成了一个高效的目标分配组件。此外，CBNNTAP算法中的一个关键创新是其能够解决海洋电流的破坏性影响，通过无缝集成调整组件来抵消这些电流引起的偏差，从而提高UUV的运动规划和目标分配的准确性。

    The paper presents an innovative approach (CBNNTAP) that addresses the complexities and challenges introduced by ocean currents when optimizing target assignment and motion planning for a multi-unmanned underwater vehicle (UUV) system. The core of the proposed algorithm involves the integration of several key components. Firstly, it incorporates a bio-inspired neural network-based (BINN) approach which predicts the most efficient paths for individual UUVs while simultaneously ensuring collision avoidance among the vehicles. Secondly, an efficient target assignment component is integrated by considering the path distances determined by the BINN algorithm. In addition, a critical innovation within the CBNNTAP algorithm is its capacity to address the disruptive effects of ocean currents, where an adjustment component is seamlessly integrated to counteract the deviations caused by these currents, which enhances the accuracy of both motion planning and target assignment for the UUVs. The ef
    
[^59]: 从南美大草原到像素：对Ga\'ucho文化遗产进行微调的扩散模型

    From Pampas to Pixels: Fine-Tuning Diffusion Models for Ga\'ucho Heritage. (arXiv:2401.05520v1 [cs.CV])

    [http://arxiv.org/abs/2401.05520](http://arxiv.org/abs/2401.05520)

    本文研究了对巴西Rio Grande do Sul（RS）地区的文化遗产进行微调的扩散模型的潜力，展示了这些模型在代表和保护多样化的独特方面方面的能力。

    

    生成式人工智能在社会中得到广泛应用，在各个领域都取得了显著进展。特别是在文本到图像（TTI）模型领域，潜在扩散模型（LDMs）展示了基于文本提示生成视觉内容的卓越能力。本文探讨了LDMs在代表本土文化概念、历史人物和濒危物种方面的潜力。我们以巴西南部地区Rio Grande do Sul（RS）的文化遗产为例进行了研究。我们的目标是为了更广泛地理解生成模型如何帮助捕捉和保护地区的文化和历史身份。本文概述了方法论，包括主题选择、数据集创建和微调过程。结果展示了生成的图像，同时介绍了每个概念的挑战和可行性。总之，这项工作展示了这些模型在代表和保护多样化的独特方面方面的能力。

    Generative AI has become pervasive in society, witnessing significant advancements in various domains. Particularly in the realm of Text-to-Image (TTI) models, Latent Diffusion Models (LDMs), showcase remarkable capabilities in generating visual content based on textual prompts. This paper addresses the potential of LDMs in representing local cultural concepts, historical figures, and endangered species. In this study, we use the cultural heritage of Rio Grande do Sul (RS), Brazil, as an illustrative case. Our objective is to contribute to the broader understanding of how generative models can help to capture and preserve the cultural and historical identity of regions. The paper outlines the methodology, including subject selection, dataset creation, and the fine-tuning process. The results showcase the images generated, alongside the challenges and feasibility of each concept. In conclusion, this work shows the power of these models to represent and preserve unique aspects of diverse
    
[^60]: 相关量化用于更快的非凸分布式优化

    Correlated Quantization for Faster Nonconvex Distributed Optimization. (arXiv:2401.05518v1 [cs.LG])

    [http://arxiv.org/abs/2401.05518](http://arxiv.org/abs/2401.05518)

    本研究利用相关量化器改进了MARINA算法，通过使用加权Hessian方差进行原始分析，并扩展了MARINA的理论框架，使其适用于更广泛的压缩器范围。

    

    量化是一种重要的（随机）压缩技术，可以在分布式模型训练的每一轮通信中减少传输比特的数量。我们分析了MARINA算法，并利用提出的相关量化器展示了它在通信复杂度方面优于原始的MARINA算法和Suresh等人的分布式SGD算法。

    Quantization (Alistarh et al., 2017) is an important (stochastic) compression technique that reduces the volume of transmitted bits during each communication round in distributed model training. Suresh et al. (2022) introduce correlated quantizers and show their advantages over independent counterparts by analyzing distributed SGD communication complexity. We analyze the forefront distributed non-convex optimization algorithm MARINA (Gorbunov et al., 2022) utilizing the proposed correlated quantizers and show that it outperforms the original MARINA and distributed SGD of Suresh et al. (2022) with regard to the communication complexity. We significantly refine the original analysis of MARINA without any additional assumptions using the weighted Hessian variance (Tyurin et al., 2022), and then we expand the theoretical framework of MARINA to accommodate a substantially broader range of potentially correlated and biased compressors, thus dilating the applicability of the method beyond the
    
[^61]: FPRF：大规模三维神经辐射场的前馈逼真风格迁移

    FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D Neural Radiance Fields. (arXiv:2401.05516v1 [cs.CV])

    [http://arxiv.org/abs/2401.05516](http://arxiv.org/abs/2401.05516)

    FPRF是一种用于大规模三维神经辐射场的前馈逼真风格迁移方法，它实现了对大规模三维场景的高效风格化，并在保持多视角外观一致性的同时支持多风格参考图像和用户控制。

    

    我们提出了FPRF，一种用于大规模三维神经辐射场的前馈逼真风格迁移方法。FPRF可以在不进行额外优化的情况下对大规模三维场景进行风格化，并保持多视角外观一致性。传统方法需要繁琐的每个风格/场景优化，并且局限于小规模三维场景。FPRF通过引入分解成风格的三维神经辐射场，高效地对大规模三维场景进行风格化，使用 AdaIN 的前馈风格化机制支持任意风格参考图像。此外，FPRF 还通过语义匹配和局部 AdaIN 支持多参考风格化，为三维场景风格提供了多样的用户控制。FPRF 还通过将语义匹配和风格迁移过程直接应用于3D空间中的查询特征来保持多视角一致性。在实验中，我们证明了FPRF具有较好的逼真的质量，能够对三维场景进行风格化。

    We present FPRF, a feed-forward photorealistic style transfer method for large-scale 3D neural radiance fields. FPRF stylizes large-scale 3D scenes with arbitrary, multiple style reference images without additional optimization while preserving multi-view appearance consistency. Prior arts required tedious per-style/-scene optimization and were limited to small-scale 3D scenes. FPRF efficiently stylizes large-scale 3D scenes by introducing a style-decomposed 3D neural radiance field, which inherits AdaIN's feed-forward stylization machinery, supporting arbitrary style reference images. Furthermore, FPRF supports multi-reference stylization with the semantic correspondence matching and local AdaIN, which adds diverse user control for 3D scene styles. FPRF also preserves multi-view consistency by applying semantic matching and style transfer processes directly onto queried features in 3D space. In experiments, we demonstrate that FPRF achieves favorable photorealistic quality 3D scene st
    
[^62]: 面向安全工业物联网设备的优化集成模型

    Optimized Ensemble Model Towards Secured Industrial IoT Devices. (arXiv:2401.05509v1 [cs.CR])

    [http://arxiv.org/abs/2401.05509](http://arxiv.org/abs/2401.05509)

    本文提出了一种面向安全工业物联网设备的优化集成模型，将贝叶斯优化-高斯过程与集成树学习模型相结合，以提高工业物联网环境中入侵和攻击检测的性能。

    

    由于对连接性需求的增加，特别是在工业环境中，物联网设备的部署持续增长。然而，这导致了与网络相关的攻击数量的增加，因为潜在攻击表面的增加。工业物联网（IIoT）设备易受各种网络相关攻击的影响，这可能对制造过程以及制造厂工人的安全造成严重后果。近年来，针对攻击检测的一种有希望的解决方案是机器学习（ML）。更具体而言，集成学习模型在改善底层ML模型的性能方面显示出巨大的潜力。因此，本文提出了一个基于贝叶斯优化-高斯过程（BO-GP）和集成基于树的学习模型的框架，以提高IIoT环境中入侵和攻击检测的性能。

    The continued growth in the deployment of Internet-of-Things (IoT) devices has been fueled by the increased connectivity demand, particularly in industrial environments. However, this has led to an increase in the number of network related attacks due to the increased number of potential attack surfaces. Industrial IoT (IIoT) devices are prone to various network related attacks that can have severe consequences on the manufacturing process as well as on the safety of the workers in the manufacturing plant. One promising solution that has emerged in recent years for attack detection is Machine learning (ML). More specifically, ensemble learning models have shown great promise in improving the performance of the underlying ML models. Accordingly, this paper proposes a framework based on the combined use of Bayesian Optimization-Gaussian Process (BO-GP) with an ensemble tree-based learning model to improve the performance of intrusion and attack detection in IIoT environments. The propose
    
[^63]: InfiAgent-DABench: 在数据分析任务中评估代理的基准测试

    InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks. (arXiv:2401.05507v1 [cs.CL])

    [http://arxiv.org/abs/2401.05507](http://arxiv.org/abs/2401.05507)

    InfiAgent-DABench是第一个评估基于LLM的代理在数据分析任务中的基准测试，包括DAEval数据集和代理框架。对23个最先进的LLMs进行的基准测试揭示了当前数据分析任务中的挑战。

    

    本文介绍了"InfiAgent-DABench"，这是第一个专门设计用于评估基于LLM的代理在数据分析任务中的基准测试。该基准测试包含DAEval，这是一个由55个CSV文件衍生出的311个数据分析问题的数据集，以及一个评估LLMs作为数据分析代理的代理框架。我们采用了一种格式提示技术，确保问题是闭合形式的，可以自动评估。我们对23个最先进的LLMs进行了广泛的基准测试，揭示了数据分析任务中当前遇到的挑战。此外，我们还开发了DAAgent，这是一个在指令调优数据集上训练的专门代理。InfiAgent-DABench的评估数据集和工具包已经发布在https://github.com/InfiAgent/InfiAgent上。

    In this paper, we introduce "InfiAgent-DABench", the first benchmark specifically designed to evaluate LLM-based agents in data analysis tasks. This benchmark contains DAEval, a dataset consisting of 311 data analysis questions derived from 55 CSV files, and an agent framework to evaluate LLMs as data analysis agents. We adopt a format-prompting technique, ensuring questions to be closed-form that can be automatically evaluated. Our extensive benchmarking of 23 state-of-the-art LLMs uncovers the current challenges encountered in data analysis tasks. In addition, we have developed DAAgent, a specialized agent trained on instruction-tuning datasets. Evaluation datasets and toolkits for InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.
    
[^64]: 多样性感知聚类：计算复杂性和近似算法

    Diversity-aware clustering: Computational Complexity and Approximation Algorithms. (arXiv:2401.05502v1 [cs.DS])

    [http://arxiv.org/abs/2401.05502](http://arxiv.org/abs/2401.05502)

    本研究讨论了多样性感知聚类问题，在选择聚类中心时要考虑多个属性，同时最小化聚类目标。我们提出了针对不同聚类目标的参数化近似算法，这些算法在保证聚类质量的同时，具有紧确的近似比。

    

    在这项工作中，我们研究了多样性感知聚类问题，其中数据点与多个属性相关联，形成交叉的组。聚类解决方案需要确保从每个组中选择最少数量的聚类中心，同时最小化聚类目标，可以是$k$-中位数，$k$-均值或$k$-供应商。我们提出了参数化近似算法，近似比分别为$1+\frac{2}{e}$，$1+\frac{8}{e}$和$3$，用于多样性感知$k$-中位数，多样性感知$k$-均值和多样性感知$k$-供应商。这些近似比在假设Gap-ETH和FPT $\neq$ W[2]的情况下是紧确的。对于公平$k$-中位数和公平$k$-均值的不相交工厂组，我们提出了参数化近似算法，近似比分别为$1+\frac{2}{e}$和$1+\frac{8}{e}$。对于具有不相交工厂组的公平$k$-供应商，我们提出了一个多项式时间近似算法，因子为$3$。

    In this work, we study diversity-aware clustering problems where the data points are associated with multiple attributes resulting in intersecting groups. A clustering solution need to ensure that a minimum number of cluster centers are chosen from each group while simultaneously minimizing the clustering objective, which can be either $k$-median, $k$-means or $k$-supplier. We present parameterized approximation algorithms with approximation ratios $1+ \frac{2}{e}$, $1+\frac{8}{e}$ and $3$ for diversity-aware $k$-median, diversity-aware $k$-means and diversity-aware $k$-supplier, respectively. The approximation ratios are tight assuming Gap-ETH and FPT $\neq$ W[2]. For fair $k$-median and fair $k$-means with disjoint faicility groups, we present parameterized approximation algorithm with approximation ratios $1+\frac{2}{e}$ and $1+\frac{8}{e}$, respectively. For fair $k$-supplier with disjoint facility groups, we present a polynomial-time approximation algorithm with factor $3$, improv
    
[^65]: 跨网络节点分类用于自闭症检测的人群图

    Population Graph Cross-Network Node Classification for Autism Detection Across Sample Groups. (arXiv:2401.05478v1 [cs.SI])

    [http://arxiv.org/abs/2401.05478](http://arxiv.org/abs/2401.05478)

    本文提出了一种新颖的跨网络节点分类方法OTGCN，利用图卷积网络和最优输运策略，可以在不同数据采集地点的样本之间纠正领域漂移，并且在自闭症谱系障碍的分类上取得了有效的结果。

    

    图神经网络（GNN）是将图像和非图像医疗信息结合起来进行节点分类任务的强大工具。跨网络节点分类扩展了GNN技术以解决领域漂移问题，允许在未标记的目标网络上进行节点分类。在本文中，我们提出了一种强大而新颖的跨网络节点分类方法OTGCN。该方法利用了图卷积网络的概念，同时应用基于最优输运的策略来纠正来自不同数据采集地点样本之间可能发生的领域漂移。这种综合方法为不同位置和设备收集到的多种不同形式的数据的场景提供了实际解决方案。我们通过使用图像和非图像数据进行分类，证明了该方法在分类自闭症谱系障碍受试者方面的有效性。

    Graph neural networks (GNN) are a powerful tool for combining imaging and non-imaging medical information for node classification tasks. Cross-network node classification extends GNN techniques to account for domain drift, allowing for node classification on an unlabeled target network. In this paper we present OTGCN, a powerful, novel approach to cross-network node classification. This approach leans on concepts from graph convolutional networks to harness insights from graph data structures while simultaneously applying strategies rooted in optimal transport to correct for the domain drift that can occur between samples from different data collection sites. This blended approach provides a practical solution for scenarios with many distinct forms of data collected across different locations and equipment. We demonstrate the effectiveness of this approach at classifying Autism Spectrum Disorder subjects using a blend of imaging and non-imaging data.
    
[^66]: 为人类活动识别模型标准化训练流程：可调因素的全面回顾

    Standardizing Your Training Process for Human Activity Recognition Models: A Comprehensive Review in the Tunable Factors. (arXiv:2401.05477v1 [cs.LG])

    [http://arxiv.org/abs/2401.05477](http://arxiv.org/abs/2401.05477)

    本文回顾了可穿戴人类活动识别领域的深度学习研究，并总结了各个研究所采用的训练程序。研究发现存在缺乏详细训练协议的趋势，同时利用控制变量方法评估了关键可调组件对跨主体泛化的影响。

    

    近年来，深度学习在多个领域中成为一个强大的工具，导致了在可穿戴人类活动识别（WHAR）领域的应用研究激增。尽管发展迅速，但人们对实验模型训练中使用的标准化和一致性程序的缺乏担忧，可能会影响研究结果的可重复性和可靠性。在本文中，我们对WHAR领域的当代深度学习研究进行了全面回顾，并整理了各种研究中所采用的训练程序的信息。我们的研究结果表明，一个主要趋势是缺乏模型训练协议提供的细节。此外，为了更清楚地了解缺失描述的影响，我们利用控制变量方法评估了关键可调组件（例如优化技术和提前停止准则）对跨主体泛化的影响。

    In recent years, deep learning has emerged as a potent tool across a multitude of domains, leading to a surge in research pertaining to its application in the wearable human activity recognition (WHAR) domain. Despite the rapid development, concerns have been raised about the lack of standardization and consistency in the procedures used for experimental model training, which may affect the reproducibility and reliability of research results. In this paper, we provide an exhaustive review of contemporary deep learning research in the field of WHAR and collate information pertaining to the training procedure employed in various studies. Our findings suggest that a major trend is the lack of detail provided by model training protocols. Besides, to gain a clearer understanding of the impact of missing descriptions, we utilize a control variables approach to assess the impact of key tunable components (e.g., optimization techniques and early stopping criteria) on the inter-subject generali
    
[^67]: CADgpt：利用自然语言处理增强计算机辅助设计工作流程的3D建模

    CADgpt: Harnessing Natural Language Processing for 3D Modelling to Enhance Computer-Aided Design Workflows. (arXiv:2401.05476v1 [cs.HC])

    [http://arxiv.org/abs/2401.05476](http://arxiv.org/abs/2401.05476)

    CADgpt是一个创新性的插件，通过将自然语言处理与Rhino3D集成，在计算机辅助设计中简化界面，使初学者能够通过直观的自然语言命令执行复杂的3D建模任务。它提供了更具包容性和互动性的教育环境，并将复杂的设计工具普及到更广泛的学生群体中，有助于设计教育的民主化发展。

    

    本文介绍了CADgpt，这是一个创新性的插件，将自然语言处理（NLP）与Rhino3D集成在一起，以增强计算机辅助设计（CAD）环境中的3D建模。利用OpenAI的GPT-4，CADgpt简化了CAD界面，使用户，尤其是初学者，可以通过直观的自然语言命令执行复杂的3D建模任务。这种方法显著降低了传统CAD软件的学习曲线，促进了更包容和互动的教育环境。本文讨论了CADgpt的技术架构，包括其在Rhino3D中的集成以及将GPT-4的能力应用于CAD任务的适应性。它还展示了CADgpt在各种设计场景中的效果，凸显了将复杂的设计工具普及到更广泛的学生群体中，以使设计教育民主化的潜力。讨论进一步探讨了CADgpt对教学法和课程开发的影响。

    This paper introduces CADgpt, an innovative plugin integrating Natural Language Processing (NLP) with Rhino3D for enhancing 3D modelling in computer-aided design (CAD) environments. Leveraging OpenAI's GPT-4, CADgpt simplifies the CAD interface, enabling users, particularly beginners, to perform complex 3D modelling tasks through intuitive natural language commands. This approach significantly reduces the learning curve associated with traditional CAD software, fostering a more inclusive and engaging educational environment. The paper discusses CADgpt's technical architecture, including its integration within Rhino3D and the adaptation of GPT-4 capabilities for CAD tasks. It presents case studies demonstrating CADgpt's efficacy in various design scenarios, highlighting its potential to democratise design education by making sophisticated design tools accessible to a broader range of students. The discussion further explores CADgpt's implications for pedagogy and curriculum development,
    
[^68]: 图挖掘中引入的新节点预测：使用图神经网络预测孤立节点的所有连接

    Introducing New Node Prediction in Graph Mining: Predicting All Links from Isolated Nodes with Graph Neural Networks. (arXiv:2401.05468v1 [cs.SI])

    [http://arxiv.org/abs/2401.05468](http://arxiv.org/abs/2401.05468)

    这项研究引入了一个新的问题——新节点预测，即从以前与图不相连的孤立节点中预测所有连接。通过基于深度图神经网络的架构，实验结果表明可以解决这一具有挑战性的问题。

    

    本文介绍了图挖掘和社交网络分析领域中的一个新问题，即新节点预测。从技术上讲，这个任务可被归类为零样本的图外所有连接预测。这个具有挑战性的问题旨在预测一个先前与图不相连、孤立且未观测到的新节点的所有连接。与经典的连接预测方法（包括少样本的图外连接预测）不同，这个问题有两个关键的不同之处：（1）新节点没有现有的连接可供提取模式用于新的预测；（2）目标是预测不仅仅是一个，而是这个新节点的所有连接，或者至少其中的一个显著部分。实验表明，基于深度图神经网络的架构可以学习解决这个具有挑战性的问题在一个文献引用网络中。

    This paper introduces a new problem in the field of graph mining and social network analysis called new node prediction. More technically, the task can be categorized as zero-shot out-of-graph all-links prediction. This challenging problem aims to predict all links from a new, isolated, and unobserved node that was previously disconnected from the graph. Unlike classic approaches to link prediction (including few-shot out-of-graph link prediction), this problem presents two key differences: (1) the new node has no existing links from which to extract patterns for new predictions; and (2) the goal is to predict not just one, but all the links of this new node, or at least a significant part of them. Experiments demonstrate that an architecture based on Deep Graph Neural Networks can learn to solve this challenging problem in a bibliographic citation network.
    
[^69]: 基于零样本学习的模块化AI代理的机器教学

    Machine Teaching for Building Modular AI Agents based on Zero-shot Learners. (arXiv:2401.05467v1 [cs.LG])

    [http://arxiv.org/abs/2401.05467](http://arxiv.org/abs/2401.05467)

    这篇论文提出了一种机器教学方法，通过利用迭代机器教学和任务特定的替代模型，增强了利用大语言模型作为零样本学习器的模块化AI代理的鲁棒性和性能。

    

    最近大语言模型（LLMs）的进展导致了许多模块化AI代理的创建。这些代理使用LLMs作为零样本学习器，在人类用户设定的复杂任务中执行子任务。我们提出了一种方法来增强利用LLMs作为零样本学习器的模块化AI代理的鲁棒性和性能。我们的迭代机器教学方法提供了一种在有限的人类反馈下逐渐教导AI代理的高效方式，解决了零样本学习质量限制的问题。我们主张利用初始部署的数据追踪以及零样本学习器的输出或注释来训练更小且任务特定的替代模型，可以减少经济成本和环境影响。我们的机器教学过程利用人类专业知识来纠正高概率误标注的示例。在三个常见对话AI代理任务上的结果显示，接近理想性能可以实现。

    The recent advances in large language models (LLMs) have led to the creation of many modular AI agents. These agents employ LLMs as zero-shot learners to perform sub-tasks in order to solve complex tasks set forth by human users. We propose an approach to enhance the robustness and performance of modular AI agents that utilize LLMs as zero-shot learners. Our iterative machine teaching method offers an efficient way to teach AI agents over time with limited human feedback, addressing the limit posed by the quality of zero-shot learning. We advocate leveraging the data traces from initial deployments and outputs or annotations from the zero-shot learners to train smaller and task-specific substitute models which can reduce both the monetary costs and environmental impact. Our machine teaching process avails human expertise to correct examples with a high likelihood of misannotations. Results on three tasks, common to conversational AI agents, show that close-to-oracle performance can be 
    
[^70]: 人类与神经网络之间的双向知识交互界面

    The two-way knowledge interaction interface between humans and neural networks. (arXiv:2401.05461v1 [cs.HC])

    [http://arxiv.org/abs/2401.05461](http://arxiv.org/abs/2401.05461)

    该论文构建了一个人类和神经网络之间的双向知识交互界面，使用视觉概念和它们之间的关系作为“语言”，以实现人类和神经网络之间的知识交流。该界面可以使神经网络向人类提供直观的推理解释，同时人类可以修改其中的偏见。

    

    尽管神经网络（NN）已经广泛应用于各个领域，并且通常优于人类，但它们在一定程度上仍然缺乏可解释性，人类无法直观地理解NN的决策逻辑。这也妨碍了人类与NN之间的知识交互，阻止了人们在NN的决策出错时直接参与给予指导。虽然最近在可解释AI方面的研究已经从各个角度实现了NN的可解释性，但尚未提供有效的人类和NN之间的知识交流方法。为了解决这个问题，我们构建了一个双向交互界面，它将视觉概念及其关系的结构化表示作为人类和NN之间的知识交流的“语言”。具体而言，NN基于类别特定结构概念图（C-SCG）向人类提供直观的推理解释。另一方面，人类可以修改C-SCG中存在的偏见。

    Despite neural networks (NN) have been widely applied in various fields and generally outperforms humans, they still lack interpretability to a certain extent, and humans are unable to intuitively understand the decision logic of NN. This also hinders the knowledge interaction between humans and NN, preventing humans from getting involved to give direct guidance when NN's decisions go wrong. While recent research in explainable AI has achieved interpretability of NN from various perspectives, it has not yet provided effective methods for knowledge exchange between humans and NN. To address this problem, we constructed a two-way interaction interface that uses structured representations of visual concepts and their relationships as the "language" for knowledge exchange between humans and NN. Specifically, NN provide intuitive reasoning explanations to humans based on the class-specific structural concepts graph (C-SCG). On the other hand, humans can modify the biases present in the C-SC
    
[^71]: 个人LLM代理:关于能力、效率和安全性的洞察和调查

    Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security. (arXiv:2401.05459v1 [cs.HC])

    [http://arxiv.org/abs/2401.05459](http://arxiv.org/abs/2401.05459)

    这项研究探讨了个人LLM代理的能力、效率和安全性，旨在提升智能个人助理的实用性和可扩展性。

    

    自从个人计算设备出现以来，智能个人助理(IPA)一直是研究人员和工程师关注的关键技术之一，旨在帮助用户高效获取信息和执行任务，并为用户提供更智能、便捷和丰富的交互体验。随着智能手机和物联网的发展，计算和感知设备变得无处不在，极大地扩展了IPA的边界。然而，由于缺乏用户意图理解、任务规划、工具使用和个人数据管理等能力，现有的IPA仍然具有有限的实用性和可扩展性。最近，以大型语言模型(LLM)为代表的基础模型的出现，为IPA的发展带来了新的机遇。借助强大的语义理解和推理能力，LLM能够使智能代理自主解决复杂问题。本文重点关注个人LLM代理。

    Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences. With the development of smartphones and IoT, computing and sensing devices have become ubiquitous, greatly expanding the boundaries of IPAs. However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability. Recently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs. With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously. In this paper, we focus on Personal LLM
    
[^72]: CoLafier: 带有局部内在维度指导的协作噪声标签净化器

    CoLafier: Collaborative Noisy Label Purifier With Local Intrinsic Dimensionality Guidance. (arXiv:2401.05458v1 [cs.LG])

    [http://arxiv.org/abs/2401.05458](http://arxiv.org/abs/2401.05458)

    CoLafier是一种使用局部内在维度（LID）进行带有噪声标签学习的方法。它通过利用LID-dis和LID-gen两个子网络，其中LID-dis使用样本的特征和标签来预测标签，产生增强的内部表示。与LID-dis相反，LID-gen仅使用样本的特征。CoLafier利用每个实例的两个增强视图同时输入两个子网络，利用LID分数来分配标签。

    

    深度神经网络在许多机器学习任务中取得了进展，但在现实世界的数据中，噪声标签常常影响其性能。为解决这个问题，我们引入了CoLafier，一种利用局部内在维度（LID）进行带有噪声标签学习的新方法。CoLafier由两个子网络组成：LID-dis和LID-gen。LID-dis是专门的分类器，使用我们独特的方案进行训练，它同时使用样本的特征和标签来预测标签，从而产生增强的内部表示。我们观察到，从这个表示计算出来的LID分数能够有效区分不同噪声情况下的正确和错误标签。与LID-dis相反，LID-gen作为常规分类器，仅使用样本的特征。在训练过程中，CoLafier利用每个实例的两个增强视图同时输入两个子网络。CoLafier将来自LID-dis的两个视图的LID分数作为分配标签的考虑因素。

    Deep neural networks (DNNs) have advanced many machine learning tasks, but their performance is often harmed by noisy labels in real-world data. Addressing this, we introduce CoLafier, a novel approach that uses Local Intrinsic Dimensionality (LID) for learning with noisy labels. CoLafier consists of two subnets: LID-dis and LID-gen. LID-dis is a specialized classifier. Trained with our uniquely crafted scheme, LID-dis consumes both a sample's features and its label to predict the label - which allows it to produce an enhanced internal representation. We observe that LID scores computed from this representation effectively distinguish between correct and incorrect labels across various noise scenarios. In contrast to LID-dis, LID-gen, functioning as a regular classifier, operates solely on the sample's features. During training, CoLafier utilizes two augmented views per instance to feed both subnets. CoLafier considers the LID scores from the two views as produced by LID-dis to assign 
    
[^73]: 维度感知的异常检测：理论和实验分析

    Dimensionality-Aware Outlier Detection: Theoretical and Experimental Analysis. (arXiv:2401.05453v1 [cs.LG])

    [http://arxiv.org/abs/2401.05453](http://arxiv.org/abs/2401.05453)

    这篇论文提出了一种维度感知的异常检测方法DAO，通过全面实验验证了其在800多个数据集上显著优于其他三种流行的异常检测方法。

    

    我们提出了一种非参数异常检测方法，该方法充分考虑了数据集内在维度的局部变化。通过使用局部内在维度（LID）理论，我们得到了一种维度感知的异常检测方法DAO，它被推导为一个包含查询点和随机选择的近邻的渐近局部期望密度比的估计。DAO的维度感知行为是由于它以理论上证明的方式使用局部LID值的局部估计。通过对800多个合成和真实数据集的全面实验，我们证明DAO明显优于三种流行且重要的基准异常检测方法：局部离群因子（LOF），简化版LOF和kNN。

    We present a nonparametric method for outlier detection that takes full account of local variations in intrinsic dimensionality within the dataset. Using the theory of Local Intrinsic Dimensionality (LID), our 'dimensionality-aware' outlier detection method, DAO, is derived as an estimator of an asymptotic local expected density ratio involving the query point and a close neighbor drawn at random. The dimensionality-aware behavior of DAO is due to its use of local estimation of LID values in a theoretically-justified way. Through comprehensive experimentation on more than 800 synthetic and real datasets, we show that DAO significantly outperforms three popular and important benchmark outlier detection methods: Local Outlier Factor (LOF), Simplified LOF, and kNN.
    
[^74]: ChatGPT能够从Bloomberg市场摘要中计算可信的情绪分数吗？

    Can ChatGPT Compute Trustworthy Sentiment Scores from Bloomberg Market Wraps?. (arXiv:2401.05447v1 [q-fin.ST])

    [http://arxiv.org/abs/2401.05447](http://arxiv.org/abs/2401.05447)

    该研究使用了Bloomberg金融市场摘要数据集，利用ChatGPT和两阶段提示方法，发现全球新闻头条与股市走势之间存在显著的情绪相关性，该相关性在不同时间段内有所变化，并且在多个股票市场中均具有鲁棒性。研究提供了一个平衡新信息反应性和相关性的最佳时间段的估计。

    

    我们使用了2010年至2023年的每日Bloomberg金融市场摘要数据集，重新发布在大型金融媒体上，利用ChatGPT和两阶段提示方法确定全球新闻头条如何影响股市走势。我们记录了情绪分数与未来股市回报之间的统计显著正相关关系，这种关系在短期到中期内转变为负相关关系，然后随着时间的推移又变为负相关关系。通过比较Pearson和Spearman相关系数来验证这种相关模式在多个股票市场上的适用性，表明它在不同股票市场中的弹性和非线性中都具有鲁棒性。最后，我们提供了一个最佳时间段的估计，以在新信息的反应性和相关性之间取得平衡。

    We used a dataset of daily Bloomberg Financial Market Summaries from 2010 to 2023, reposted on large financial media, to determine how global news headlines may affect stock market movements using ChatGPT and a two-stage prompt approach. We document a statistically significant positive correlation between the sentiment score and future equity market returns over short to medium term, which reverts to a negative correlation over longer horizons. Validation of this correlation pattern across multiple equity markets indicates its robustness across equity regions and resilience to non-linearity, evidenced by comparison of Pearson and Spearman correlations. Finally, we provide an estimate of the optimal horizon that strikes a balance between reactivity to new information and correlation.
    
[^75]: 自我监督学习在脑电图中的应用：系统综述

    Self-supervised Learning for Electroencephalogram: A Systematic Survey. (arXiv:2401.05446v1 [eess.SP])

    [http://arxiv.org/abs/2401.05446](http://arxiv.org/abs/2401.05446)

    这篇论文系统综述了自我监督学习在脑电图中的应用，通过设计良好的预训练任务来提取无标签样本的表示，解决了脑电信号标签的问题和个体之间的变化带来的挑战。

    

    脑电图（EEG）是一种非侵入性的记录生物电信号的技术。近年来，将监督式深度学习技术与脑电信号相结合，已经在各种基于脑电的任务上实现了自动分析。然而，脑电信号的标签问题限制了基于脑电的深度模型的发展。获取脑电注释是困难的，需要领域专家指导收集和标记，并且不同受试者之间的脑电信号的变化会导致显著的标签偏移。为了解决上述挑战，提出了自我监督学习（SSL）通过设计良好的预训练任务从无标签样本中提取表示。本文聚焦于将自我监督学习框架与时间序列脑电信号相结合，实现高效的表示，并提出了对脑电信号的自我监督学习的系统综述。在本文中，1）我们介绍了自我监督学习的概念和理论，以及典型的自我监督学习框架。2）我们提供了一份全面的综述，概述了目前的研究进展、方法和应用场景，并指出了未来的研究方向。

    Electroencephalogram (EEG) is a non-invasive technique to record bioelectrical signals. Integrating supervised deep learning techniques with EEG signals has recently facilitated automatic analysis across diverse EEG-based tasks. However, the label issues of EEG signals have constrained the development of EEG-based deep models. Obtaining EEG annotations is difficult that requires domain experts to guide collection and labeling, and the variability of EEG signals among different subjects causes significant label shifts. To solve the above challenges, self-supervised learning (SSL) has been proposed to extract representations from unlabeled samples through well-designed pretext tasks. This paper concentrates on integrating SSL frameworks with temporal EEG signals to achieve efficient representation and proposes a systematic review of the SSL for EEG signals. In this paper, 1) we introduce the concept and theory of self-supervised learning and typical SSL frameworks. 2) We provide a compre
    
[^76]: 基于层内连接的全脉冲行为网络在强化学习中的应用

    Fully Spiking Actor Network with Intra-layer Connections for Reinforcement Learning. (arXiv:2401.05444v1 [cs.NE])

    [http://arxiv.org/abs/2401.05444](http://arxiv.org/abs/2401.05444)

    本研究提出了一种基于层内连接的全脉冲行为网络，利用特殊的神经形态硬件实现了较低能耗的人工智能。该方法在实现控制任务上表现出可比较性能，并解决了使用脉冲频率作为输出所带来的浮点矩阵运算问题。

    

    借助特殊的神经形态硬件，脉冲神经网络（SNN）被期望能以较低能量消耗实现人工智能（AI）。通过将SNN与深度强化学习（DRL）结合，为现实控制任务提供了一种有前景的高能效方式。本文针对代理需要学习多维确定性策略以进行控制的任务进行研究，这在真实场景中非常常见。最近，替代梯度方法已被用于训练多层SNN，在这个任务中允许SNNs实现与对应深度网络相当的性能。大多数现有的基于脉冲的RL方法将脉冲频率作为SNN的输出，并通过全连接（FC）层将其转换为表示连续动作空间（即确定性策略）。然而，脉冲频率的十进制特性使得FC层需要浮点矩阵运算，使得整个SNN无法部署。

    With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence (AI) with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combining SNNs with deep reinforcement learning (DRL). In this paper, we focus on the task where the agent needs to learn multi-dimensional deterministic policies to control, which is very common in real scenarios. Recently, the surrogate gradient method has been utilized for training multi-layer SNNs, which allows SNNs to achieve comparable performance with the corresponding deep networks in this task. Most existing spike-based RL methods take the firing rate as the output of SNNs, and convert it to represent continuous action space (i.e., the deterministic policy) through a fully-connected (FC) layer. However, the decimal characteristic of the firing rate brings the floating-point matrix operations to the FC layer, making the whole SNN unable to depl
    
[^77]: LLM4PLC：利用大型语言模型对工业控制系统中可编程逻辑控制器（PLC）进行可验证编程

    LLM4PLC: Harnessing Large Language Models for Verifiable Programming of PLCs in Industrial Control Systems. (arXiv:2401.05443v1 [cs.SE])

    [http://arxiv.org/abs/2401.05443](http://arxiv.org/abs/2401.05443)

    LLM4PLC通过利用用户反馈和外部验证工具，如语法检查器、编译器和SMV验证器，来指导生成，同时采用提示工程和模型微调的方法，提高了大型语言模型在工业控制系统中可编程逻辑控制器（PLC）的生成能力。

    

    虽然大型语言模型（LLMs）在自动化代码生成方面取得了主导地位，但它们并不是没有缺点。相关问题主要与生成的代码缺乏执行保证、缺乏可解释性以及对必要但尖端编程语言的支持不足有关。目前的LLMs如GPT-4和LLaMa2无法为可编程逻辑控制器（PLC）操作的工业控制系统（ICS）生成有效的程序。我们提出了LLM4PLC，这是一个用户引导的迭代流程，利用用户反馈和外部验证工具（包括语法检查器、编译器和SMV验证器）来指导LLM的生成。我们进一步通过采用提示工程和模型微调（LoRAs的创建和使用）来增强LLM的生成能力。我们使用FischerTechnik制造测试床（MFTB）验证了这个系统，展示了LLMs如何从生成结构有缺陷的代码演变为生成有效的代码。

    Although Large Language Models (LLMs) have established pre-dominance in automated code generation, they are not devoid of shortcomings. The pertinent issues primarily relate to the absence of execution guarantees for generated code, a lack of explainability, and suboptimal support for essential but niche programming languages. State-of-the-art LLMs such as GPT-4 and LLaMa2 fail to produce valid programs for Industrial Control Systems (ICS) operated by Programmable Logic Controllers (PLCs). We propose LLM4PLC, a user-guided iterative pipeline leveraging user feedback and external verification tools including grammar checkers, compilers and SMV verifiers to guide the LLM's generation. We further enhance the generation potential of LLM by employing Prompt Engineering and model fine-tuning through the creation and usage of LoRAs. We validate this system using a FischerTechnik Manufacturing TestBed (MFTB), illustrating how LLMs can evolve from generating structurally flawed code to producin
    
[^78]: 功能图模型：结构实现离线数据驱动优化

    Functional Graphical Models: Structure Enables Offline Data-Driven Optimization. (arXiv:2401.05442v1 [cs.LG])

    [http://arxiv.org/abs/2401.05442](http://arxiv.org/abs/2401.05442)

    功能图模型（FGMs）通过结构实现了样本高效的数据驱动优化。

    

    虽然机器学习模型通常是为了解决预测问题而训练的，但我们经常希望将它们用于优化问题。例如，给定一组蛋白质及其对应的荧光水平的数据集，我们可能希望为具有最高荧光的新蛋白质进行优化。这种数据驱动的优化（DDO）面临着一系列挑战，超出了标准预测问题中的挑战，因为我们需要成功预测在训练集中没有见过的优于最佳设计的新设计的性能的模型。从理论上讲，甚至不清楚现有方法什么时候甚至能比简单地选择数据集中最佳设计的朴素方法执行得更好。在本文中，我们研究了如何通过结构实现高效的数据驱动优化。为了形式化结构的概念，我们引入了功能图模型（FGMs）并从理论上展示了它们如何通过分解实现基于数据的优化。

    While machine learning models are typically trained to solve prediction problems, we might often want to use them for optimization problems. For example, given a dataset of proteins and their corresponding fluorescence levels, we might want to optimize for a new protein with the highest possible fluorescence. This kind of data-driven optimization (DDO) presents a range of challenges beyond those in standard prediction problems, since we need models that successfully predict the performance of new designs that are better than the best designs seen in the training set. It is not clear theoretically when existing approaches can even perform better than the naive approach that simply selects the best design in the dataset. In this paper, we study how structure can enable sample-efficient data-driven optimization. To formalize the notion of structure, we introduce functional graphical models (FGMs) and show theoretically how they can provide for principled data-driven optimization by decomp
    
[^79]: 可穿戴应用中的表示学习：在缺失数据情况下的探索

    Representation Learning for Wearable-Based Applications in the Case of Missing Data. (arXiv:2401.05437v1 [eess.SP])

    [http://arxiv.org/abs/2401.05437](http://arxiv.org/abs/2401.05437)

    本论文研究了可穿戴应用中表示学习的问题，特别是在缺失数据情况下。作者通过比较Transformer模型和统计方法的性能，发现Transformer模型在变化频繁的信号的缺失数据填充方面表现优秀。此研究为基于掩码的自监督学习任务的设计和开发提供了洞察。

    

    可穿戴设备持续收集传感器数据并用于推断个体的行为，如睡眠、体力活动和情绪。尽管在这个领域有很大的兴趣和进展，但由于数据质量低和数据注释有限，建模真实环境中的多模式传感器数据仍然具有挑战性。本研究探讨了用于填充缺失可穿戴数据的表示学习，并将其与最先进的统计方法进行了比较。我们使用10个生理和行为信号的变化率不同的掩码比率，研究了Transformer模型在缺失数据填充上的性能。结果显示，Transformer模型在变化频繁的信号的缺失数据填充方面优于基准模型，但对于单调信号则不然。我们进一步研究了填充策略和掩码比率对下游分类任务的影响。本研究为基于掩码的自监督学习任务的设计和开发提供了洞察。

    Wearable devices continuously collect sensor data and use it to infer an individual's behavior, such as sleep, physical activity, and emotions. Despite the significant interest and advancements in this field, modeling multimodal sensor data in real-world environments is still challenging due to low data quality and limited data annotations. In this work, we investigate representation learning for imputing missing wearable data and compare it with state-of-the-art statistical approaches. We investigate the performance of the transformer model on 10 physiological and behavioral signals with different masking ratios. Our results show that transformers outperform baselines for missing data imputation of signals that change more frequently, but not for monotonic signals. We further investigate the impact of imputation strategies and masking rations on downstream classification tasks. Our study provides insights for the design and development of masking-based self-supervised learning tasks a
    
[^80]: ECGformer: 利用transformer进行心电图心搏心律失常分类

    ECGformer: Leveraging transformer for ECG heartbeat arrhythmia classification. (arXiv:2401.05434v1 [eess.SP])

    [http://arxiv.org/abs/2401.05434](http://arxiv.org/abs/2401.05434)

    ECGformer是一种利用transformer架构的模型，用于对心电图数据中的心律失常进行分类。

    

    心律失常是心搏不规则的一种情况，可以由心脏不同区域引起，导致心搏快速、缓慢或不规则。心电图是一种重要的诊断工具，用于检测心脏的不规则和异常，以便专家分析心脏的电信号，识别复杂的模式和偏离正常的情况。在过去几十年中，已经进行了许多研究，以开发基于ECG数据的自动化心搏分类方法。近年来，深度学习在处理各种医学挑战方面展示出卓越的能力，特别是将transformer作为序列处理的模型架构。通过利用transformer，我们开发了ECGformer模型，用于对心电图数据中存在的各种心律失常进行分类。我们使用MIT-BIH和PTB da

    An arrhythmia, also known as a dysrhythmia, refers to an irregular heartbeat. There are various types of arrhythmias that can originate from different areas of the heart, resulting in either a rapid, slow, or irregular heartbeat. An electrocardiogram (ECG) is a vital diagnostic tool used to detect heart irregularities and abnormalities, allowing experts to analyze the heart's electrical signals to identify intricate patterns and deviations from the norm. Over the past few decades, numerous studies have been conducted to develop automated methods for classifying heartbeats based on ECG data. In recent years, deep learning has demonstrated exceptional capabilities in tackling various medical challenges, particularly with transformers as a model architecture for sequence processing. By leveraging the transformers, we developed the ECGformer model for the classification of various arrhythmias present in electrocardiogram data. We assessed the suggested approach using the MIT-BIH and PTB da
    
[^81]: 通过对抗性权重扰动和度量特定的注意力池化增强论文评分

    Enhancing Essay Scoring with Adversarial Weights Perturbation and Metric-specific AttentionPooling. (arXiv:2401.05433v1 [cs.CL])

    [http://arxiv.org/abs/2401.05433](http://arxiv.org/abs/2401.05433)

    本研究使用DeBERTa模型，结合对抗性训练和度量特定的注意力池化等创新技术，提出了一种增强自动评分工具的方法，用于改进针对英语学习者的写作能力评估。

    

    本研究旨在通过应用数据科学技术（包括机器学习、自然语言处理和教育数据分析）来改进针对英语学习者（ELLs）设计的自动反馈工具。自动论文评分（AES）研究在评估写作论文方面取得了进展，但往往忽视了英语学习者在语言发展方面的特定需求。本研究探讨了应用与BERT相关的技术来增强AES中对ELLs写作能力的评估。为了满足ELLs的特定需求，我们提出使用DeBERTa，这是一种先进的神经语言模型，用于改进自动反馈工具。DeBERTa通过自监督学习在大型文本语料库上预训练，学习了适用于各种自然语言理解任务的通用语言表示。该模型结合了几种创新技术，包括通过对抗训练实现

    The objective of this study is to improve automated feedback tools designed for English Language Learners (ELLs) through the utilization of data science techniques encompassing machine learning, natural language processing, and educational data analytics. Automated essay scoring (AES) research has made strides in evaluating written essays, but it often overlooks the specific needs of English Language Learners (ELLs) in language development. This study explores the application of BERT-related techniques to enhance the assessment of ELLs' writing proficiency within AES.  To address the specific needs of ELLs, we propose the use of DeBERTa, a state-of-the-art neural language model, for improving automated feedback tools. DeBERTa, pretrained on large text corpora using self-supervised learning, learns universal language representations adaptable to various natural language understanding tasks. The model incorporates several innovative techniques, including adversarial training through Adve
    
[^82]: TEN-GUARD: 使用张量分解检测深度神经网络中的后门攻击

    TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks. (arXiv:2401.05432v1 [cs.LG])

    [http://arxiv.org/abs/2401.05432](http://arxiv.org/abs/2401.05432)

    TEN-GUARD提出了一种使用张量分解方法检测深度神经网络中后门攻击的新方法，相对于现有的方法具有多个优势，包括能够同时分析多个模型，在各种网络架构上工作，不对触发器的性质做任何假设，并且计算效率高。

    

    随着深度神经网络和用于训练它们的数据集越来越大，将它们集成到研究和商业项目中的默认方法是下载预训练模型并进行微调。但是这些模型的来源不确定，可能存在隐藏的恶意行为，如特洛伊木马或后门，其中对输入进行小的改变（触发器）可能导致模型产生错误的输出（例如，误分类）。本文介绍了一种新颖的后门检测方法，该方法使用两种应用于网络激活的张量分解方法。相对于现有的检测方法，这种方法具有多个优势，包括能够同时分析多个模型，在各种网络架构上工作，不对用于改变网络行为的触发器的性质做任何假设，并且计算效率高。我们提供了对检测流程的详细描述以及结果。

    As deep neural networks and the datasets used to train them get larger, the default approach to integrating them into research and commercial projects is to download a pre-trained model and fine tune it. But these models can have uncertain provenance, opening up the possibility that they embed hidden malicious behavior such as trojans or backdoors, where small changes to an input (triggers) can cause the model to produce incorrect outputs (e.g., to misclassify). This paper introduces a novel approach to backdoor detection that uses two tensor decomposition methods applied to network activations. This has a number of advantages relative to existing detection methods, including the ability to analyze multiple models at the same time, working across a wide variety of network architectures, making no assumptions about the nature of triggers used to alter network behavior, and being computationally efficient. We provide a detailed description of the detection pipeline along with results on 
    
[^83]: TRLS:一种基于声谱图的医学信号处理时间序列表示学习框架

    TRLS: A Time Series Representation Learning Framework via Spectrogram for Medical Signal Processing. (arXiv:2401.05431v1 [eess.SP])

    [http://arxiv.org/abs/2401.05431](http://arxiv.org/abs/2401.05431)

    TRLS是一种通过声谱图进行时间序列表示学习的医学信号处理框架，通过利用时间频率RNN从增强声谱图中提取出更多信息，并在医学信号分类中显示出更好的性能。

    

    在无标签时间序列中，提出了用于医学信号处理的表示学习框架。尽管在之前的工作中取得了许多卓越进展，但我们观察到提取的时间序列表示仍然不能很好地泛化。在本文中，我们提出了一种通过声谱图进行时间序列（医学信号）表示学习的框架（TRLS），以获得更多信息的表示。我们将输入的时域医学信号转化为声谱图，并设计了一个称为时间频率RNN（TFRNN）的时频编码器，从增强的声谱图中捕捉更稳健的多尺度表示。我们的TRLS以声谱图作为输入，具有两种不同的数据增强方式，并最大化正样本之间的相似度，从而有效地规避了设计负样本的问题。我们对四个真实医学信号数据集进行的医学信号分类评估表明，TRLS优于现有的方法。

    Representation learning frameworks in unlabeled time series have been proposed for medical signal processing. Despite the numerous excellent progresses have been made in previous works, we observe the representation extracted for the time series still does not generalize well. In this paper, we present a Time series (medical signal) Representation Learning framework via Spectrogram (TRLS) to get more informative representations. We transform the input time-domain medical signals into spectrograms and design a time-frequency encoder named Time Frequency RNN (TFRNN) to capture more robust multi-scale representations from the augmented spectrograms. Our TRLS takes spectrogram as input with two types of different data augmentations and maximizes the similarity between positive ones, which effectively circumvents the problem of designing negative samples. Our evaluation of four real-world medical signal datasets focusing on medical signal classification shows that TRLS is superior to the ex
    
[^84]: CoSS：针对数据高效AI的传感器和采样率优化在人体活动识别中的应用

    CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in Human Activity Recognition. (arXiv:2401.05426v1 [eess.SP])

    [http://arxiv.org/abs/2401.05426](http://arxiv.org/abs/2401.05426)

    本论文提出了一个实用的框架用于在人体活动识别任务中高效利用数据，同时考虑传感器模态和采样率的优化，通过设计的可训练参数来指导传感器模态和采样率的选择。

    

    最近神经网络技术的进步显著提高了利用多个时间序列传感器进行人体活动识别的效果。虽然使用大量传感器和高采样率通常可以提高结果，但往往会导致数据低效和人工神经网络的不必要扩展，给在边缘设备上的实际应用带来挑战。为了应对这些问题，我们的工作引入了一个实用的框架，用于在HAR任务中的数据高效利用，同时考虑传感器模态和采样率的优化。我们方法的核心是设计的可训练参数，称为“权重分数”，它们评估训练阶段中每个传感器模态和采样率的重要性。这些分数指导传感器模态和采样率的选择。修剪方法允许用户在计算预算和性能之间进行权衡，根据选择传感器模态和采样率。

    Recent advancements in Artificial Neural Networks have significantly improved human activity recognition using multiple time-series sensors. While employing numerous sensors with high-frequency sampling rates usually improves the results, it often leads to data inefficiency and unnecessary expansion of the ANN, posing a challenge for their practical deployment on edge devices. Addressing these issues, our work introduces a pragmatic framework for data-efficient utilization in HAR tasks, considering the optimization of both sensor modalities and sampling rate simultaneously. Central to our approach are the designed trainable parameters, termed 'Weight Scores,' which assess the significance of each sensor modality and sampling rate during the training phase. These scores guide the sensor modalities and sampling rate selection. The pruning method allows users to make a trade-off between computational budgets and performance by selecting the sensor modalities and sampling rates according t
    
[^85]: 机器学习(Machine Learning)辅助下的毫米波分布式多输入多输出(D-MIMO)系统中的波束管理

    Machine Learning (ML)-assisted Beam Management in millimeter (mm)Wave Distributed Multiple Input Multiple Output (D-MIMO) systems. (arXiv:2401.05422v1 [eess.SP])

    [http://arxiv.org/abs/2401.05422](http://arxiv.org/abs/2401.05422)

    这项研究探讨了在毫米波频段的分布式多输入多输出(D-MIMO)系统中，通过机器学习辅助进行波束管理，并证明了波束推断的性能优势。

    

    波束管理协议对于建立和维护网络无线电节点与用户设备(UEs)之间的连接至关重要。在分布式多输入多输出(D-MIMO)系统中，一些由中央处理单元(CPU)协调的接入点(AP)为一些UEs提供服务。在毫米波频段，由于需要发送下行(DL)参考信号来探测大量波束的问题，找到最佳的AP和波束以为UEs提供服务是具有挑战性的。本文的目标是通过仅对少量波束进行探测，并利用人工智能/机器学习推断最佳波束/AP是否可靠。我们使用随机森林(Random Forest), 缺失模式森林(MissForest)和条件生成对抗网络(conditional Generative Adversarial Networks, c-GAN)来展示推断的性能优势。

    Beam management (BM) protocols are critical for establishing and maintaining connectivity between network radio nodes and User Equipments (UEs). In Distributed Multiple Input Multiple Output systems (D-MIMO), a number of access points (APs), coordinated by a central processing unit (CPU), serves a number of UEs. At mmWave frequencies, the problem of finding the best AP and beam to serve the UEs is challenging due to a large number of beams that need to be sounded with Downlink (DL) reference signals. The objective of this paper is to investigate whether the best AP/beam can be reliably inferred from sounding only a small subset of beams and leveraging AI/ML for inference of best beam/AP. We use Random Forest (RF), MissForest (MF) and conditional Generative Adversarial Networks (c-GAN) for demonstrating the performance benefits of inference.
    
[^86]: ANALYTiC: 机器学习中的决策边界和降维理解

    ANALYTiC: Understanding Decision Boundaries and Dimensionality Reduction in Machine Learning. (arXiv:2401.05418v1 [eess.SP])

    [http://arxiv.org/abs/2401.05418](http://arxiv.org/abs/2401.05418)

    本论文通过将降维和决策边界应用于现有的主动学习方法，突显数据中的模式和聚类，并通过实验验证了这种方法在提高轨迹标记效率和准确性方面的潜力。

    

    紧凑便携设备的出现给我们带来了一批可以用于推断趋势和模式的跟踪运动数据。本研究探讨了在现有主动学习方法基础上应用降维和决策边界的组合，突显数据中的模式和聚类。我们使用三个不同的轨迹数据集进行了实验分析，旨在利用已标记的数据并提高它们的可解释性。实验结果展示了这些组合方法在提高轨迹标记的效率和准确性方面的潜力。本研究为将机器学习和视觉方法在上下文中更广泛地集成打下了基础。

    The advent of compact, handheld devices has given us a pool of tracked movement data that could be used to infer trends and patterns that can be made to use. With this flooding of various trajectory data of animals, humans, vehicles, etc., the idea of ANALYTiC originated, using active learning to infer semantic annotations from the trajectories by learning from sets of labeled data. This study explores the application of dimensionality reduction and decision boundaries in combination with the already present active learning, highlighting patterns and clusters in data. We test these features with three different trajectory datasets with objective of exploiting the the already labeled data and enhance their interpretability. Our experimental analysis exemplifies the potential of these combined methodologies in improving the efficiency and accuracy of trajectory labeling. This study serves as a stepping-stone towards the broader integration of machine learning and visual methods in contex
    
[^87]: 惯性传感器信号增强的小波动态选择网络

    Wavelet Dynamic Selection Network for Inertial Sensor Signal Enhancement. (arXiv:2401.05416v1 [eess.SP])

    [http://arxiv.org/abs/2401.05416](http://arxiv.org/abs/2401.05416)

    该论文提出了一种名为WDSNet的小波动态选择网络，用于智能选择适当的小波基函数以增强惯性传感器信号。此外，论文还提出了一种类别表示机制，用于提高小波基函数的选择能力。

    

    作为姿态和运动感应部件，惯性传感器广泛应用于各种便携设备中。然而，惯性传感器的严重误差限制了它们的功能，特别是轨迹恢复和语义识别。小波作为主流的信号处理方法，由于丰富多样的小波基函数而被誉为信号的数学显微镜。然而，惯性传感器的复杂噪声类型和应用场景使得选择适当的小波基函数变得困难。为此，我们提出了一种小波动态选择网络（WDSNet），它能智能地为可变惯性信号选择适当的小波基函数。此外，现有的深度学习架构擅长从输入数据中提取特征，但忽视了学习目标类别的特征，这对于提高类别感知能力、改善小波基函数的选择是必要的。因此，我们提出了一种类别表示机制。

    As attitude and motion sensing components, inertial sensors are widely used in various portable devices. But the severe errors of inertial sensors restrain their function, especially the trajectory recovery and semantic recognition. As a mainstream signal processing method, wavelet is hailed as the mathematical microscope of signal due to the plentiful and diverse wavelet basis functions. However, complicated noise types and application scenarios of inertial sensors make selecting wavelet basis perplexing. To this end, we propose a wavelet dynamic selection network (WDSNet), which intelligently selects the appropriate wavelet basis for variable inertial signals. In addition, existing deep learning architectures excel at extracting features from input data but neglect to learn the characteristics of target categories, which is essential to enhance the category awareness capability, thereby improving the selection of wavelet basis. Therefore, we propose a category representation mechanis
    
[^88]: 空间相关传感器的重要性：利用文本语义辅助的三维人体运动重建

    Spatial-Related Sensors Matters: 3D Human Motion Reconstruction Assisted with Textual Semantics. (arXiv:2401.05412v1 [cs.CV])

    [http://arxiv.org/abs/2401.05412](http://arxiv.org/abs/2401.05412)

    该研究使用稀疏惯性测量单元(IMUs)在人体上进行运动重建，通过引入文本语义进行传感器数据对齐和特征加权，实现了精确的运动重建。实验结果显示与现有方法相比，该方法在多个指标上取得了显著的改进。

    

    利用可穿戴设备进行运动重建已成为一种经济且可行的技术。某些方法利用人体上的稀疏惯性测量单元(IMUs)并采用数据驱动的策略来建模人体姿势。然而，仅基于稀疏的IMUs数据进行运动重建在本质上存在模糊性，这是由于大量相同的IMU读数对应于不同的姿势。在本文中，我们探讨了多传感器的空间重要性，受到描述特定动作的文本的监督。具体地，我们引入了不确定性来为每个IMU派生加权特征。我们还设计了一个分层时序变换器(HTT)，并应用对比学习来实现传感器数据与文本语义的精确时序和特征对齐。实验结果表明，与现有方法相比，我们提出的方法在多个指标上取得了显著的改进。值得注意的是，通过文本监督，我们的方法可以实现准确的运动重建。

    Leveraging wearable devices for motion reconstruction has emerged as an economical and viable technique. Certain methodologies employ sparse Inertial Measurement Units (IMUs) on the human body and harness data-driven strategies to model human poses. However, the reconstruction of motion based solely on sparse IMUs data is inherently fraught with ambiguity, a consequence of numerous identical IMU readings corresponding to different poses. In this paper, we explore the spatial importance of multiple sensors, supervised by text that describes specific actions. Specifically, uncertainty is introduced to derive weighted features for each IMU. We also design a Hierarchical Temporal Transformer (HTT) and apply contrastive learning to achieve precise temporal and feature alignment of sensor data with textual semantics. Experimental results demonstrate our proposed approach achieves significant improvements in multiple metrics compared to existing methods. Notably, with textual supervision, our
    
[^89]: 基于图像的时间序列数据表示：脑电图伪迹检测的比较分析

    Image-based Data Representations of Time Series: A Comparative Analysis in EEG Artifact Detection. (arXiv:2401.05409v1 [eess.SP])

    [http://arxiv.org/abs/2401.05409](http://arxiv.org/abs/2401.05409)

    本文通过针对脑电图数据中的伪迹检测和分类，对十一个深度学习架构在六种常用表示方法上进行了比较分析，并发现某些表示方法在突出数据的信噪比方面更有效。

    

    替代数据表示是增强下游模型性能的强大工具。然而，在机器学习工具箱中存在大量这样的表示方法，领域内缺乏对每种表示方法的适用性的比较理解。在本文中，我们将脑电图数据中的伪迹检测和分类作为测试平台，评估了六种常用表示方法上的十一个流行深度学习架构。我们发现，虽然表示的选择涉及偏差和方差之间的权衡，但某些表示在突出数据的信噪比方面实际上更有效。我们展示了针对脑电图数据的结果，并开源了我们的测试框架，以便未来进行这种方法的比较分析。

    Alternative data representations are powerful tools that augment the performance of downstream models. However, there is an abundance of such representations within the machine learning toolbox, and the field lacks a comparative understanding of the suitability of each representation method.  In this paper, we propose artifact detection and classification within EEG data as a testbed for profiling image-based data representations of time series data. We then evaluate eleven popular deep learning architectures on each of six commonly-used representation methods.  We find that, while the choice of representation entails a choice within the tradeoff between bias and variance, certain representations are practically more effective in highlighting features which increase the signal-to-noise ratio of the data. We present our results on EEG data, and open-source our testing framework to enable future comparative analyses in this vein.
    
[^90]: RFRL Gym: 用于认知无线电应用的强化学习测试平台

    RFRL Gym: A Reinforcement Learning Testbed for Cognitive Radio Applications. (arXiv:2401.05406v1 [eess.SP])

    [http://arxiv.org/abs/2401.05406](http://arxiv.org/abs/2401.05406)

    RFRL Gym是一个用于认知无线电应用的强化学习测试平台，可以帮助开发和测试RFRL技术，模拟无线电频谱环境，并实验不同的频谱感知技术。

    

    无线电频率强化学习（RFRL）预计将成为下一代无线通信系统（特别是6G和下一代军事通信）中广泛应用的技术。基于此，我们的研究致力于开发一个工具，以促进利用频谱感知的RFRL技术的发展。具体而言，该工具旨在解决两个认知无线电应用，即动态频谱接入和干扰。为了训练和测试这些应用的强化学习（RL）算法，需要一个模拟环境来模拟无线电频谱中代理人将遇到的条件。本文中，我们开发了这样一个环境，称为RFRL Gym。通过RFRL Gym，用户可以设计自己的场景来模拟RL代理人在无线电频谱中可能遇到的情况，并尝试不同的频谱感知技术。

    Radio Frequency Reinforcement Learning (RFRL) is anticipated to be a widely applicable technology in the next generation of wireless communication systems, particularly 6G and next-gen military communications. Given this, our research is focused on developing a tool to promote the development of RFRL techniques that leverage spectrum sensing. In particular, the tool was designed to address two cognitive radio applications, specifically dynamic spectrum access and jamming. In order to train and test reinforcement learning (RL) algorithms for these applications, a simulation environment is necessary to simulate the conditions that an agent will encounter within the Radio Frequency (RF) spectrum. In this paper, such an environment has been developed, herein referred to as the RFRL Gym. Through the RFRL Gym, users can design their own scenarios to model what an RL agent may encounter within the RF spectrum as well as experiment with different spectrum sensing techniques. Additionally, the 
    
[^91]: 早期儿童教育中的关键人工智能技术：综述

    The Key Artificial Intelligence Technologies in Early Childhood Education: A Review. (arXiv:2401.05403v1 [cs.CY])

    [http://arxiv.org/abs/2401.05403](http://arxiv.org/abs/2401.05403)

    本研究对早期儿童教育中关键AI技术进行了最新和深入的综述，包括基于AI的机器人和AI技术改善自闭症儿童的社交互动等。这对初学者来说是一份适用的综述材料。

    

    人工智能（AI）技术已被应用于各个领域，包括早期儿童教育（ECE）。AI教育技术的整合是ECE领域的一个重要趋势。目前，关于ECE中的AI研究越来越多。然而，目前还缺少讨论ECE中AI研究的综述文章。本文提供了早期儿童教育中关键AI技术的最新和深入的概述，从历史角度出发，总结了代表性的研究作品，概述了开放性问题，讨论了趋势和挑战，并通过详细的文献计量分析提供了深入的研究建议。本文主要讨论了将基于AI的机器人和AI技术应用于ECE的研究，包括改善自闭症谱系障碍儿童的社交互动。本文为初学者提供了一份适用的最新和深入的调研综述材料。

    Artificial Intelligence (AI) technologies have been applied in various domains, including early childhood education (ECE). Integration of AI educational technology is a recent significant trend in ECE. Currently, there are more and more studies of AI in ECE. To date, there is a lack of survey articles that discuss the studies of AI in ECE. In this paper, we provide an up-to-date and in-depth overview of the key AI technologies in ECE that provides a historical perspective, summarizes the representative works, outlines open questions, discusses the trends and challenges through a detailed bibliometric analysis, and provides insightful recommendations for future research. We mainly discuss the studies that apply AI-based robots and AI technologies to ECE, including improving the social interaction of children with an autism spectrum disorder. This paper significantly contributes to provide an up-to-date and in-depth survey that is suitable as introductory material for beginners to AI in 
    
[^92]: 基于向量场定向扩散模型的晶体材料生成

    Vector Field Oriented Diffusion Model for Crystal Material Generation. (arXiv:2401.05402v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2401.05402](http://arxiv.org/abs/2401.05402)

    提出了一种基于向量场定向扩散模型的晶体材料生成方法，该方法通过利用几何等变GNN同时考虑原子位置和晶体晶格，提供了一种能够更全面评估模型能力的新生成指标。实验证明了该扩散模型的重要性和有效性。

    

    在材料科学中，发现具有特定化学性质的晶体结构已成为越来越重要的研究焦点。然而，当前的模型在生成新的晶体晶格方面存在局限性，因为它们只考虑原子位置或化学组成。为解决这个问题，我们提出了一种概率扩散模型，利用几何等变GNN同时考虑原子位置和晶体晶格。为评估我们模型的有效性，我们引入了一个受Frechet Inception Distance启发的新的生成指标，但基于GNN能量预测，而不是计算机视觉中使用的InceptionV3。除了常用的评估指标（如有效性，评估结构的合理性）外，这种新指标提供了对我们模型能力更全面的评估。我们在现有基准上的实验证明了我们扩散模型的重要性。我们还展示了我们的方法能够有效地学习有意义的表示。

    Discovering crystal structures with specific chemical properties has become an increasingly important focus in material science. However, current models are limited in their ability to generate new crystal lattices, as they only consider atomic positions or chemical composition. To address this issue, we propose a probabilistic diffusion model that utilizes a geometrically equivariant GNN to consider atomic positions and crystal lattices jointly. To evaluate the effectiveness of our model, we introduce a new generation metric inspired by Frechet Inception Distance, but based on GNN energy prediction rather than InceptionV3 used in computer vision. In addition to commonly used metrics like validity, which assesses the plausibility of a structure, this new metric offers a more comprehensive evaluation of our model's capabilities. Our experiments on existing benchmarks show the significance of our diffusion model. We also show that our method can effectively learn meaningful representatio
    
[^93]: 领域相似性感知标签分配用于领域通用水下物体检测

    Domain Similarity-Perceived Label Assignment for Domain Generalized Underwater Object Detection. (arXiv:2401.05401v1 [cs.CV])

    [http://arxiv.org/abs/2401.05401](http://arxiv.org/abs/2401.05401)

    该论文介绍了领域相似性感知标签分配（DSP）的概念，通过将每个图像的领域标签视为其与指定领域的相似性，使用领域特定的数据增强技术，在水下跨领域物体检测基准S-UODAC2020上达到了最先进的结果。

    

    水体的固有特性和光波动引起了水下环境中不同层次和区域之间的巨大差异。当测试集在与训练集不同的海洋区域收集时，领域转移问题出现，严重影响了模型的泛化能力。领域对抗学习（DAL）训练策略之前被用于解决这些挑战。然而，DAL严重依赖于手动进行独热编码的领域标签，这意味着同一领域样本之间没有差异。这种假设导致了DAL的不稳定性。本文引入了领域相似性感知标签分配（DSP）的概念。每张图片的领域标签被看作其与指定领域的相似性。通过领域特定的数据增强技术，在水下跨领域物体检测基准S-UODAC2020上取得了最先进的结果。

    The inherent characteristics and light fluctuations of water bodies give rise to the huge difference between different layers and regions in underwater environments. When the test set is collected in a different marine area from the training set, the issue of domain shift emerges, significantly compromising the model's ability to generalize. The Domain Adversarial Learning (DAL) training strategy has been previously utilized to tackle such challenges. However, DAL heavily depends on manually one-hot domain labels, which implies no difference among the samples in the same domain. Such an assumption results in the instability of DAL. This paper introduces the concept of Domain Similarity-Perceived Label Assignment (DSP). The domain label for each image is regarded as its similarity to the specified domains. Through domain-specific data augmentation techniques, we achieved state-of-the-art results on the underwater cross-domain object detection benchmark S-UODAC2020. Furthermore, we valid
    
[^94]: 与人工智能说话者的协同学习（CLAIS）：预备小学科学教师对原型的反应

    Collaborative Learning with Artificial Intelligence Speakers (CLAIS): Pre-Service Elementary Science Teachers' Responses to the Prototype. (arXiv:2401.05400v1 [cs.CY])

    [http://arxiv.org/abs/2401.05400](http://arxiv.org/abs/2401.05400)

    该研究证明了人工智能不仅可以作为学习工具，还可以作为一个智能代理，与人类一起参与协同学习，改变科学课堂中的认识实践。通过开发一个名为CLAIS的实物教学系统，结合人类学习者和人工智能说话者的协同学习过程，该系统成功应用在预备小学科学教师的课程中，并得到积极评价。

    

    该研究旨在证明人工智能不仅可以作为学习工具，还可以作为一个智能代理，与人类共同参与协同学习，以改变科学课堂中的认识实践。我们采用设计与开发研究方法，遵循分析、设计、开发、实施和评估（ADDIE）模型，原型一个名为协同学习与AI说话者（CLAIS）的实物教学系统。CLAIS系统旨在让3-4个人类学习者与一个人工智能说话者组成一个小组，在Jigsaw学习过程中将人类与AI视为同伴。开发使用了NUGU AI说话者平台。CLAIS系统在一个预备小学科学教师的科学教育课程中成功实施。参与者作为教师、学习者、同伴和用户通过混合方法调查评估了CLAIS系统。定量数据表明，参与者积极评价CLAIS系统的使用体验和学习成果。

    This research aims to demonstrate that AI can function not only as a tool for learning, but also as an intelligent agent with which humans can engage in collaborative learning (CL) to change epistemic practices in science classrooms. We adopted a design and development research approach, following the Analysis, Design, Development, Implementation and Evaluation (ADDIE) model, to prototype a tangible instructional system called Collaborative Learning with AI Speakers (CLAIS). The CLAIS system is designed to have 3-4 human learners join an AI speaker to form a small group, where humans and AI are considered as peers participating in the Jigsaw learning process. The development was carried out using the NUGU AI speaker platform. The CLAIS system was successfully implemented in a Science Education course session with 15 pre-service elementary science teachers. The participants evaluated the CLAIS system through mixed methods surveys as teachers, learners, peers, and users. Quantitative dat
    
[^95]: 使用LLMs自动评估学生的代码理解能力

    Automated Assessment of Students' Code Comprehension using LLMs. (arXiv:2401.05399v1 [cs.CY])

    [http://arxiv.org/abs/2401.05399](http://arxiv.org/abs/2401.05399)

    本研究探索了使用LLMs自动评估学生对代码的理解能力的潜力，并发现LLMs在比较学生的解释和专家解释方面具有作用。

    

    在教育领域，评估学生答案，尤其是自然语言答案，是一项关键挑战。机器学习的进步，包括基于Transformer的模型，如大型语言模型(LLMs)，在各种自然语言任务中取得了重大进展。然而，在评估LLMs在自动答案评估领域的不断增长的趋势中，对LLMs的评估并没有得到太多关注。为了弥补这一空白，我们探索了使用LLMs来自动评估学生简短和开放性回答的潜力。特别地，我们使用LLMs来比较学生对计算机程序逐行解释的解释与专家解释。为了比较，我们在评估学生对计算机代码解释的正确性方面，对大型语言模型(LLMs)和基于编码器的语义文本相似性(STS)模型进行评估。我们的研究结果表明，LLMs在提示学生解释计算机代码的正确性时可以起到作用。

    Assessing student's answers and in particular natural language answers is a crucial challenge in the field of education. Advances in machine learning, including transformer-based models such as Large Language Models(LLMs), have led to significant progress in various natural language tasks. Nevertheless, amidst the growing trend of evaluating LLMs across diverse tasks, evaluating LLMs in the realm of automated answer assesment has not received much attention. To address this gap, we explore the potential of using LLMs for automated assessment of student's short and open-ended answer. Particularly, we use LLMs to compare students' explanations with expert explanations in the context of line-by-line explanations of computer programs.  For comparison purposes, we assess both Large Language Models (LLMs) and encoder-based Semantic Textual Similarity (STS) models in the context of assessing the correctness of students' explanation of computer code. Our findings indicate that LLMs, when promp
    
[^96]: 社会科学中的地理人工智能

    GeoAI in Social Science. (arXiv:2401.05398v1 [cs.CY])

    [http://arxiv.org/abs/2401.05398](http://arxiv.org/abs/2401.05398)

    GeoAI在社会科学研究中取得了重要进展，通过利用人工智能和地理大数据填补数据和知识空白，并促进了研究方法的融合和应用领域的拓展。

    

    地理人工智能（GeoAI）是一个新颖的领域，利用人工智能（AI）、地理大数据和庞大的计算能力来解决高度自动化和智能化的问题。本文回顾了在社会科学研究中应用人工智能的进展，突出了利用GeoAI填补重要数据和知识空白的重要进展。同时，讨论了打破数据孤岛、加快GeoAI研究方法的融合以及将GeoAI推广应用到地理领域之外的重要性。

    GeoAI, or geospatial artificial intelligence, is an exciting new area that leverages artificial intelligence (AI), geospatial big data, and massive computing power to solve problems with high automation and intelligence. This paper reviews the progress of AI in social science research, highlighting important advancements in using GeoAI to fill critical data and knowledge gaps. It also discusses the importance of breaking down data silos, accelerating convergence among GeoAI research methods, as well as moving GeoAI beyond geospatial benefits.
    
[^97]: SRNI-CAR: 用于分析中国汽车市场的综合数据集

    SRNI-CAR: A comprehensive dataset for analyzing the Chinese automotive market. (arXiv:2401.05395v1 [econ.GN])

    [http://arxiv.org/abs/2401.05395](http://arxiv.org/abs/2401.05395)

    SRNI-CAR是一份用于分析中国汽车市场的综合数据集，填补了现有汽车行业数据集覆盖范围有限的缺口，对于提高预测准确性、扩大商业应用范围、指导政策制定与监管以及推动汽车行业的学术研究具有重要影响。

    

    汽车行业在全球经济中起着关键作用，尤其重要的是中国汽车市场的不断扩大，由于其巨大的规模和影响力。然而，现有的汽车行业数据集在覆盖范围上有限，未能充分考虑到对更多和多样化变量的不断增长的需求。本文旨在通过介绍一份从2016年到2022年的综合数据集来填补这一数据缺口，该数据集包括销售数据，在线评论以及与中国汽车行业相关的大量信息。该数据集作为宝贵的资源，极大地扩展了可用数据。它的影响力涵盖了多个方面，包括提高预测准确性，扩大商业应用范围，指导政策制定与监管，推动汽车行业的学术研究。为了展示该数据集在商业和学术背景下的潜在应用，我们提出了两个应用案例。

    The automotive industry plays a critical role in the global economy, and particularly important is the expanding Chinese automobile market due to its immense scale and influence. However, existing automotive sector datasets are limited in their coverage, failing to adequately consider the growing demand for more and diverse variables. This paper aims to bridge this data gap by introducing a comprehensive dataset spanning the years from 2016 to 2022, encompassing sales data, online reviews, and a wealth of information related to the Chinese automotive industry. This dataset serves as a valuable resource, significantly expanding the available data. Its impact extends to various dimensions, including improving forecasting accuracy, expanding the scope of business applications, informing policy development and regulation, and advancing academic research within the automotive sector. To illustrate the dataset's potential applications in both business and academic contexts, we present two ap
    
[^98]: AT-2FF：适应性类型-2模糊滤波器用于去除受盐和胡椒噪声污染的图像

    AT-2FF: Adaptive Type-2 Fuzzy Filter for De-noising Images Corrupted with Salt-and-Pepper. (arXiv:2401.05392v1 [cs.CV])

    [http://arxiv.org/abs/2401.05392](http://arxiv.org/abs/2401.05392)

    本文介绍了一种适应性类型-2模糊滤波器，用于去除图像中盐和胡椒噪声。通过自适应阈值和加权均值滤波器两个步骤，该方法能够保留图像的特征，并在与其他不同滤波方法的比较中展示出良好的去噪效果。

    

    噪声在数字图像中普遍存在，导致图像质量下降。因此，需要一种合适的滤波方法来减少噪声同时保留图像特征（边缘、角点等）。本文提出了一种高效的类型-2模糊加权均值滤波器，带有自适应阈值，用于去除盐和胡椒噪声。该滤波器有两个主要步骤：第一阶段根据自适应阈值将图像分类为轻微、中度和重度污染，通过比较处理像素的M-ALD与类型-2模糊识别器的上下MF来实现。第二阶段通过使用滤波窗口内未污染像素的均值和方差来计算适当的权重，来消除受损像素。仿真结果明确表明，所得到的去噪图像保留了图像特征，即边缘、角点和其他清晰结构，与其他不同的滤波方法相比。

    Noise is inevitably common in digital images, leading to visual image deterioration. Therefore, a suitable filtering method is required to lessen the noise while preserving the image features (edges, corners, etc.). This paper presents the efficient type-2 fuzzy weighted mean filter with an adaptive threshold to remove the SAP noise. The present filter has two primary steps: The first stage categorizes images as lightly, medium, and heavily corrupted based on an adaptive threshold by comparing the M-ALD of processed pixels with the upper and lower MF of the type-2 fuzzy identifier. The second stage eliminates corrupted pixels by computing the appropriate weight using GMF with the mean and variance of the uncorrupted pixels in the filter window. Simulation results vividly show that the obtained denoised images preserve image features, i.e., edges, corners, and other sharp structures, compared with different filtering methods.
    
[^99]: 在Intel GPU上高效的LLM推理解决方案

    Efficient LLM inference solution on Intel GPU. (arXiv:2401.05391v1 [cs.AR])

    [http://arxiv.org/abs/2401.05391](http://arxiv.org/abs/2401.05391)

    本文提出了一种在Intel GPU上高效的LLM推理解决方案，通过简化LLM解码层和引入分段KV缓存策略，实现了低延迟和高吞吐量。

    

    基于Transformer的大型语言模型（LLM）在许多领域广泛应用，LLM推理的效率成为实际应用中的热门话题。然而，LLM通常在模型结构上设计复杂，具有大量操作，并以自回归模式进行推理，这使得设计一个高效的系统成为一项具有挑战性的任务。在本文中，我们提出了一种高效的LLM推理解决方案，具有低延迟和高吞吐量。首先，我们通过融合数据移动和逐元素操作简化了LLM解码层，以减少内存访问频率并降低系统延迟。我们还提出了一种分段KV缓存策略，将请求和响应令牌的键/值分别保存在不同的物理内存中，以实现有效的设备内存管理，有助于增大运行时批处理大小并提高系统吞吐量。我们设计了一个定制的Scaled-Dot-Product-Attention内核，以匹配我们的融合策略，基于分段KV缓存解决方案。我们实验证明，该解决方案在Intel GPU上实现了高效的LLM推理。

    Transformer based Large Language Models (LLMs) have been widely used in many fields, and the efficiency of LLM inference becomes hot topic in real applications. However, LLMs are usually complicatedly designed in model structure with massive operations and perform inference in the auto-regressive mode, making it a challenging task to design a system with high efficiency.  In this paper, we propose an efficient LLM inference solution with low latency and high throughput. Firstly, we simplify the LLM decoder layer by fusing data movement and element-wise operations to reduce the memory access frequency and lower system latency. We also propose a segment KV cache policy to keep key/value of the request and response tokens in separate physical memory for effective device memory management, helping enlarge the runtime batch size and improve system throughput. A customized Scaled-Dot-Product-Attention kernel is designed to match our fusion policy based on the segment KV cache solution. We im
    
[^100]: 由好到优：通过工具辅助交错提示改进数学推理

    From Good to Great: Improving Math Reasoning with Tool-Augmented Interleaf Prompting. (arXiv:2401.05384v1 [math.HO])

    [http://arxiv.org/abs/2401.05384](http://arxiv.org/abs/2401.05384)

    本文研究了大型语言模型和工具辅助语言模型在数学推理任务中的表现，提出了IMP-TIP框架，该框架采用了“由好到优”的思想并通过工具辅助交错提示来选择最准确的答案。

    

    本文研究了大型语言模型（LLMs）和工具辅助LLMs在处理复杂数学推理任务中的表现。我们引入了IMP-TIP：通过工具辅助交错提示改进数学推理，这是一个综合了LLMs和工具辅助LLMs的优势的框架。IMP-TIP遵循“由好到优”的概念，从LLMs和其工具辅助对应物中收集多个可能的解决方案，并通过工具辅助交错提示在这些解决方案之间进行交叉检查，然后选择或重新生成最准确的答案。该框架包括两个关键方面：自我提示和工具辅助交错提示（TIP）。前者允许LLMs自主地改进和完善与工具使用相关的初始提示，而后者使LLMs能够通过动态分析问题、交叉检查潜在解决方案和修正先前的推理提示来得出最终答案。

    This paper investigates the performance of Large Language Models (LLMs) and Tool-augmented LLMs in tackling complex mathematical reasoning tasks. We introduce IMP-TIP: Improving Math Reasoning with Tool-augmented Interleaf Prompting, a framework that combines the strengths of both LLMs and Tool-augmented LLMs. IMP-TIP follows the ``From Good to Great" concept, collecting multiple potential solutions from both LLMs and their Tool-Augmented counterparts for the same math problem, and then selecting or re-generating the most accurate answer after cross-checking these solutions via tool-augmented interleaf prompting. The framework incorporates two key aspects: self-prompt and tool-augmented interleaf prompting (TIP). The former allows LLMs to autonomously refine and improve an initial prompt related to tool usage, while the latter enables LLMs to derive the final answer by dynamically analyzing the problem, cross-checking potential solutions, and revising previous reasoning hints in an int
    
[^101]: 经典排序算法作为一种形态发生学模型：自排序数组在极简模型的基础智能中展现了意外的能力

    Classical Sorting Algorithms as a Model of Morphogenesis: self-sorting arrays reveal unexpected competencies in a minimal model of basal intelligence. (arXiv:2401.05375v1 [cs.NE])

    [http://arxiv.org/abs/2401.05375](http://arxiv.org/abs/2401.05375)

    该论文研究了经典排序算法作为形态发生学模型，在极简模型中展示出了意外的基础智能能力。

    

    多样化智能领域旨在识别、正式化和理解各种实现中行为能力的共性。尤其有趣的是那些在外观上看起来不够复杂以实现这些能力的基质中提供了意外的记忆、决策或问题解决的简单系统。我们致力于开发工具来帮助理解实现这些能力的最小要求，并学习识别和预测非传统基质中基本智力形式。在这里，我们对经典排序算法的行为应用了新颖的分析方法，这些算法是长期以来被研究的短代码片段。为了将这些排序算法作为生物形态发生学及其能力模型进行研究，我们打破了两个先前普遍存在的假设：自上而下控制（而是展示了数字数组中的每个元素如何施加最小的主动性并实现排序策略）。

    The emerging field of Diverse Intelligence seeks to identify, formalize, and understand commonalities in behavioral competencies across a wide range of implementations. Especially interesting are simple systems that provide unexpected examples of memory, decision-making, or problem-solving in substrates that at first glance do not appear to be complex enough to implement such capabilities. We seek to develop tools to help understand the minimal requirements for such capabilities, and to learn to recognize and predict basal forms of intelligence in unconventional substrates. Here, we apply novel analyses to the behavior of classical sorting algorithms, short pieces of code which have been studied for many decades. To study these sorting algorithms as a model of biological morphogenesis and its competencies, we break two formerly-ubiquitous assumptions: top-down control (instead, showing how each element within a array of numbers can exert minimal agency and implement sorting policies fr
    
[^102]: 动态尖峰图神经网络

    Dynamic Spiking Graph Neural Networks. (arXiv:2401.05373v1 [cs.NE])

    [http://arxiv.org/abs/2401.05373](http://arxiv.org/abs/2401.05373)

    本文提出了一个名为"动态尖峰图神经网络"（DSGNN）的框架，它将尖峰神经网络（SNNs）与图神经网络（GNNs）结合起来，以解决动态图表示学习中的复杂性和内存开销问题。DSGNN通过动态调整尖峰神经元的状态和连接权重，在传播过程中保持图结构信息的完整性。

    

    将尖峰神经网络（SNNs）和图神经网络（GNNs）相结合渐渐引起了人们的关注，这是因为它在处理由图表示的非欧几里得数据时具有低功耗和高效率。然而，作为一个常见的问题，动态图表示学习面临着高复杂性和大内存开销的挑战。目前的工作通常通过使用二进制特征而不是连续特征的SNNs来替代循环神经网络（RNNs）进行高效训练，这会忽视图结构信息并在传播过程中导致细节的丢失。此外，优化动态尖峰模型通常需要在时间步之间传播信息，这增加了内存需求。为了解决这些挑战，我们提出了一个名为"动态尖峰图神经网络"（\method{}）的框架。为了减轻信息丢失问题，\method{} 在传播过程中引入了一种新的机制，它在每个时间步骤中动态地调整尖峰神经元的状态和连接权重，以保持图结构信息的完整性。

    The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks (GNNs) is gradually attracting attention due to the low power consumption and high efficiency in processing the non-Euclidean data represented by graphs. However, as a common problem, dynamic graph representation learning faces challenges such as high complexity and large memory overheads. Current work often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary features instead of continuous ones for efficient training, which would overlooks graph structure information and leads to the loss of details during propagation. Additionally, optimizing dynamic spiking models typically requires propagation of information across time steps, which increases memory requirements. To address these challenges, we present a framework named \underline{Dy}namic \underline{S}p\underline{i}king \underline{G}raph \underline{N}eural Networks (\method{}). To mitigate the information loss problem, \method{} propagates
    
[^103]: 基于自回归片段扩散的结合位感知配体设计

    Autoregressive fragment-based diffusion for pocket-aware ligand design. (arXiv:2401.05370v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.05370](http://arxiv.org/abs/2401.05370)

    引入了AutoFragDiff，一种基于片段的自回归扩散模型，用于生成3D分子结构。通过预测分子片段的原子类型和空间坐标，改善了生成分子的局部几何结构，并保持高的结合亲和力。如果提供起始分子骨架，模型还可以进行扩展。

    

    在这项工作中，我们引入了AutoFragDiff，一种基于片段的自回归扩散模型，用于根据目标蛋白结构生成3D分子结构。我们使用几何向量感知器来预测分子骨架和蛋白负袋条件下新分子片段的原子类型和空间坐标。我们的方法改善了生成的3D分子的局部几何结构，同时保持高预测的结合亲和力。该模型还可以根据用户提供的起始分子骨架进行扩展。

    In this work, we introduce AutoFragDiff, a fragment-based autoregressive diffusion model for generating 3D molecular structures conditioned on target protein structures. We employ geometric vector perceptrons to predict atom types and spatial coordinates of new molecular fragments conditioned on molecular scaffolds and protein pockets. Our approach improves the local geometry of the resulting 3D molecules while maintaining high predicted binding affinity to protein targets. The model can also perform scaffold extension from user-provided starting molecular scaffold.
    
[^104]: DualTeacher: 桥接非标定类别的协同存在，实现半监督增量目标检测

    DualTeacher: Bridging Coexistence of Unlabelled Classes for Semi-supervised Incremental Object Detection. (arXiv:2401.05362v1 [cs.CV])

    [http://arxiv.org/abs/2401.05362](http://arxiv.org/abs/2401.05362)

    这项工作提出了一种名为DualTeacher的算法，用于解决半监督增量目标检测问题。该算法能够在仅有少量标注数据和大量未标注数据的情况下，逐渐学习新的目标类别，同时避免对旧类别的遗忘。

    

    在真实世界的应用中，目标检测器经常会遇到新类别的目标实例，并且需要有效地适应它们。先前的工作将这个关键问题形式化为增量目标检测 (IOD)，假设新类别的目标实例在增量数据中是完全注释的。然而，由于监督信号通常很稀少和昂贵，监督式的增量目标检测在实际实施中可能不切实际。在这项工作中，我们考虑了一个更现实的设定，称为半监督增量目标检测 (SSIOD)，其中目标检测器需要从少量标注数据和大量未标注数据中逐渐学习新的类别，同时不对旧类别产生灾难性遗忘。一种常用的监督式IOD策略是鼓励当前模型（作为学生）模仿旧模型（作为老师）的行为，但在SSIOD中通常失败，因为大部分来自旧类别和新类别的目标实例是共存且未标注的，只有老师模型可以提供少量的监督信号。

    In real-world applications, an object detector often encounters object instances from new classes and needs to accommodate them effectively. Previous work formulated this critical problem as incremental object detection (IOD), which assumes the object instances of new classes to be fully annotated in incremental data. However, as supervisory signals are usually rare and expensive, the supervised IOD may not be practical for implementation. In this work, we consider a more realistic setting named semi-supervised IOD (SSIOD), where the object detector needs to learn new classes incrementally from a few labelled data and massive unlabelled data without catastrophic forgetting of old classes. A commonly-used strategy for supervised IOD is to encourage the current model (as a student) to mimic the behavior of the old model (as a teacher), but it generally fails in SSIOD because a dominant number of object instances from old and new classes are coexisting and unlabelled, with the teacher onl
    
[^105]: 开发适用于表面缺陷检测的资源受限EdgeAI模型

    Developing a Resource-Constraint EdgeAI model for Surface Defect Detection. (arXiv:2401.05355v1 [cs.CV])

    [http://arxiv.org/abs/2401.05355](http://arxiv.org/abs/2401.05355)

    开发了一种适用于表面缺陷检测的资源受限EdgeAI模型，可以在边缘设备上进行训练，具有出色的性能。

    

    资源限制限制了几个EdgeAI应用程序只能使用机器学习推理方法，其中模型在云端训练并部署到边缘设备。这带来了诸如带宽、延迟和隐私等与存储数据在外部进行模型构建的挑战。在边缘设备上进行训练可以通过消除将数据传输到另一设备进行存储和模型开发的需求来克服这些挑战。在设备上进行训练还可以提供对数据变化的稳健性，因为可以使用新获取的数据对模型进行重新训练以提高性能。因此，我们提出了一种轻量级的EdgeAI架构，改编自Xception，用于在资源受限的边缘环境中进行设备上的训练。我们对我们的模型在PCB缺陷检测任务上进行了评估，并将其性能与现有的轻量级模型-MobileNetV2、EfficientNetV2B0和MobileViT-XXS进行了比较。我们实验的结果表明，我们的模型具有显着的性能，测试准确率高。

    Resource constraints have restricted several EdgeAI applications to machine learning inference approaches, where models are trained on the cloud and deployed to the edge device. This poses challenges such as bandwidth, latency, and privacy associated with storing data off-site for model building. Training on the edge device can overcome these challenges by eliminating the need to transfer data to another device for storage and model development. On-device training also provides robustness to data variations as models can be retrained on newly acquired data to improve performance. We, therefore, propose a lightweight EdgeAI architecture modified from Xception, for on-device training in a resource-constraint edge environment. We evaluate our model on a PCB defect detection task and compare its performance against existing lightweight models - MobileNetV2, EfficientNetV2B0, and MobileViT-XXS. The results of our experiment show that our model has a remarkable performance with a test accura
    
[^106]: 自适应运算符选择利用广义经验

    Adaptive operator selection utilising generalised experience. (arXiv:2401.05350v1 [cs.NE])

    [http://arxiv.org/abs/2401.05350](http://arxiv.org/abs/2401.05350)

    这篇论文提出了基于强化学习的一种新方法，用于解决组合优化问题中的探索和开发平衡问题，以及利用广义经验来发展通用框架。

    

    优化问题，特别是组合优化问题，由于其复杂性和难度而难以解决。进化和群体智能算法已成功解决这类问题，尤其是在二进制格式中。然而，由于探索和开发活动之间的平衡问题(EvE)，这种近似可能受到影响，这仍然是这个领域的主要挑战。尽管使用多个运算符进行互补使用的自适应运算符选择方案在管理EvE方面越来越受欢迎，但定制的自适应选择系统仍然是一个重要的研究议题。最近，强化学习(RL)被提出作为一种定制和塑造高度有效的自适应选择系统的方法。然而，从可伸缩性的角度来处理该问题仍然具有挑战性。本文提出了一种基于RL的新方法，用于帮助开发通用框架，以获得、处理和利用广义经验。

    Optimisation problems, particularly combinatorial optimisation problems, are difficult to solve due to their complexity and hardness. Such problems have been successfully solved by evolutionary and swarm intelligence algorithms, especially in binary format. However, the approximation may suffer due to the the issues in balance between exploration and exploitation activities (EvE), which remain as the major challenge in this context. Although the complementary usage of multiple operators is becoming more popular for managing EvE with adaptive operator selection schemes, a bespoke adaptive selection system is still an important topic in research. Reinforcement Learning (RL) has recently been proposed as a way to customise and shape up a highly effective adaptive selection system. However, it is still challenging to handle the problem in terms of scalability. This paper proposes and assesses a RL-based novel approach to help develop a generalised framework for gaining, processing, and uti
    
[^107]: 从FOON使用搜索算法检索任务树

    Task tree retrieval from FOON using search algorithms. (arXiv:2401.05346v1 [cs.RO])

    [http://arxiv.org/abs/2401.05346](http://arxiv.org/abs/2401.05346)

    本文介绍了从FOON中使用搜索算法检索任务树的方法，以帮助机器人自动执行各种复杂的任务。

    

    机器人可以自动化任务并减少人类所需的工作量，但要让机器人知道如何执行任务，我们需要给它清晰的步骤。为了解决给机器人提供每个可能任务的指令几乎是不可能的问题，我们创建和扩展了一个通用的功能面向对象网络（FOON），并且拥有大量现有的配方信息。但是对于机器人来说，某些任务很复杂，同样地，对于人类来说也有一些复杂的任务。因此，我们为功能单元添加了权重，以表示机器人执行动作的成功概率。通过给定一些厨房用品和目标节点，使用通用的FOON，机器人必须能够确定厨房中是否存在所需的物品，并且如果是，获得将所需的厨房用品转换为目标节点的步骤。现在通过本文，我们使用两种算法（IDS和GBFS）来检索任务树（如果p

    Robots can be very useful to automate tasks and reduce the human effort required. But for the robot to know, how to perform tasks, we need to give it a clear set of steps to follow. It is nearly impossible to provide a robot with instructions for every possible task. Therefore we have a Universal Functional object-oriented network (FOON) which was created and expanded and has a lot of existing recipe information [1]. But certain tasks are complicated for robots to perform and similarly, some tasks are complicated for humans to perform. Therefore weights have been added to functional units to represent the chance of successful execution of the motion by the robot [2]. Given a set of kitchen items and a goal node, using Universal FOON, a robot must be able to determine if the required items are present in the kitchen, and if yes, get the steps to convert the required kitchen items to the goal node. Now through this paper, we use two algorithms (IDS and GBFS) to retrieve a task tree (if p
    
[^108]: 最具区分性的刺激物用于功能细胞类型的识别

    Most discriminative stimuli for functional cell type identification. (arXiv:2401.05342v1 [q-bio.NC])

    [http://arxiv.org/abs/2401.05342](http://arxiv.org/abs/2401.05342)

    本文提出了一种使用最具区分性刺激物的优化聚类方法，成功地识别了小鼠视网膜、恒河猴视网膜和猕猴V4视觉区的功能细胞类型。

    

    识别细胞类型并理解其功能特性对揭示感知和认知机制至关重要。在视网膜中，可以通过精心选择的刺激物来识别功能类型，但这需要专业领域知识，并会对以前已知的细胞类型产生偏见。在视觉皮层中，仍然不知道存在什么功能类型以及如何识别它们。因此，需要新的方法来对视网膜和视觉皮层中的功能细胞类型进行无偏见的识别。在这里，我们提出了一种基于优化的聚类方法，使用最具区分性的刺激物（MDS）来获得神经元的功能聚类。我们的方法通过刺激物的优化和聚类重新分配之间的交替进行，类似于期望最大化算法。该算法成功恢复了小鼠视网膜、恒河猴视网膜和猕猴V4视觉区的功能聚类。这证明了我们的方法可以成功地进行功能细胞类型的识别。

    Identifying cell types and understanding their functional properties is crucial for unraveling the mechanisms underlying perception and cognition. In the retina, functional types can be identified by carefully selected stimuli, but this requires expert domain knowledge and biases the procedure towards previously known cell types. In the visual cortex, it is still unknown what functional types exist and how to identify them. Thus, for unbiased identification of the functional cell types in retina and visual cortex, new approaches are needed. Here we propose an optimization-based clustering approach using deep predictive models to obtain functional clusters of neurons using Most Discriminative Stimuli (MDS). Our approach alternates between stimulus optimization with cluster reassignment akin to an expectation-maximization algorithm. The algorithm recovers functional clusters in mouse retina, marmoset retina and macaque visual area V4. This demonstrates that our approach can successfully 
    
[^109]: RoSA: 通过鲁棒适应实现准确的参数高效微调

    RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])

    [http://arxiv.org/abs/2401.04679](http://arxiv.org/abs/2401.04679)

    RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。

    

    我们研究了在大语言模型 (LLMs) 的背景下，能够在有限的计算和内存预算下提供良好准确性的参数高效微调 (PEFT) 方法。我们提出了一种新的PEFT方法，称为RoSA，受鲁棒主成分分析 (PCA) 的启发，它在一组固定的预训练权重上共同训练$\textit{低秩}$和$\textit{高度稀疏}$的组件，以高效近似完全微调（FFT）解决方案的性能。我们展示了RoSA在一系列具有挑战性的生成任务上的性能，例如小学数学和SQL查询生成，这些任务需要进行微调以获得良好性能，我们证明了在相同的参数预算下，RoSA优于LoRA和纯粹的稀疏微调。我们通过稀疏GPU内核为RoSA提供系统支持，以补充训练算法，从而实现内存和计算效率的训练。我们的代码将在https://github.com/IST-DASLab上提供。

    We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
    
[^110]: DebugBench: 评估大型语言模型的调试能力

    DebugBench: Evaluating Debugging Capability of Large Language Models. (arXiv:2401.04621v1 [cs.SE])

    [http://arxiv.org/abs/2401.04621](http://arxiv.org/abs/2401.04621)

    该论文介绍了一个名为DebugBench的LLM调试基准，用于评估大型语言模型的调试能力。研究发现闭源模型与人类相比具有较低的调试性能，而开源模型未能达到合格率。

    

    大型语言模型（LLMs）展示出了出色的编码能力。然而，作为编程能力的另一个关键组成部分，LLMs的调试能力仍然相对未被探索。之前对LLMs的调试能力评估受到数据泄露风险、数据集规模和测试漏洞种类的限制。为了克服这些不足，我们引入了一个名为“DebugBench”的LLM调试基准，包含4253个实例。它涵盖了C ++，Java和Python中四个主要的错误类别和18个次要类型。为了构建DebugBench，我们从LeetCode社区收集了代码片段，使用GPT-4向源数据中注入错误，并进行严格的质量检查。我们在零样例情况下评估了两个商业模型和三个开源模型。我们发现，（1）与人类相比，闭源模型如GPT-4表现出较低的调试性能，而开源模型如Code Llama无法达到任何合格率；（2）t

    Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and three open-source models in a zero-shot scenario. We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) t
    
[^111]: 从AI教练学习赛车：多模态自动驾驶解释对驾驶表现、认知负荷、专业知识和信任的影响

    Learning Racing From an AI Coach: Effects of Multimodal Autonomous Driving Explanations on Driving Performance, Cognitive Load, Expertise, and Trust. (arXiv:2401.04206v1 [cs.HC])

    [http://arxiv.org/abs/2401.04206](http://arxiv.org/abs/2401.04206)

    本研究测试了一种AI驾驶教练的解释对驾驶表现、认知负荷、专业知识和信任的影响。结果显示，AI驾驶教练对于教授新手驾驶技能是有帮助的，并且信息类型和呈现方式对表现结果有影响。

    

    在一项前后实验中（n=41），我们测试了模仿人类驾驶专家的指导说明的AI教练的解释沟通对驾驶表现、认知负荷、信心、专业知识和信任的影响。参与者被分为四个组，评估了AI教练解释的两个维度：信息类型（'what'和'why'-type解释）和呈现方式（听觉和视觉）。通过采访，我们描述了参与者的学习过程。结果表明，AI驾驶教练对于教授新手驾驶技能是有用的。比较各组之间，我们发现信息的类型和方式对性能结果有影响。我们将差异归因于信息如何引导注意力，减轻不确定性，并影响参与者经历的负荷过载。这反过来又影响了信心和信任水平。

    In a pre-post experiment (n = 41), we test the impact of an AI Coach's explanatory communications modeled after the instructions of human driving experts. Participants were divided into four (4) groups to assess two (2) dimensions of the AI coach's explanations: information type ('what' and 'why'-type explanations) and presentation modality (auditory and visual). We directly compare how AI Coaching sessions employing these techniques impact driving performance, cognitive load, confidence, expertise, and trust in an observation learning context. Through interviews, we delineate the learning process of our participants. Results show that an AI driving coach can be useful for teaching performance driving skills to novices. Comparing between groups, we find the type and modality of information influences performance outcomes. We attribute differences to how information directed attention, mitigated uncertainty, and influenced overload experienced by participants. These, in turn, affected h
    
[^112]: 盲人触觉签名系统的概念

    The Concept of the Tactile Signature System for Individuals with Visual Impairments. (arXiv:2401.04126v1 [cs.HC])

    [http://arxiv.org/abs/2401.04126](http://arxiv.org/abs/2401.04126)

    盲人触觉签名系统是一种无障碍和有效的系统，通过触觉互动和语音算法引导，赋予视障人士创建个性化手写签名的能力，促进了他们的独立和包容。

    

    缺乏一种无障碍和有效的系统，使盲人能够创建手写签名，这对他们的独立性和全面参与生活的各个方面构成了重大障碍。本研究介绍了一种开创性的方法，即触觉签名系统，赋予视障人士形成独特的手写签名的能力。系统的关键特点包括：个性化定制：通过触觉互动和语音算法引导，个体可以创建反映其偏好和自然书写风格的签名。实时反馈：AI驱动的语音提示和分析确保签名的准确性和一致性。可访问性：在本地服务中心安装，提供安全监督的环境进行签名创建。该系统的影响超越个体层面：促进包容和独立：盲人可以进行法律和金融交易而无需依赖他人。

    The lack of an accessible and effective system for blind individuals to create handwritten signatures presents a significant barrier to their independence and full participation in various aspects of life. This research introduces the Tactile Signature System, a groundbreaking approach that empowers individuals with visual impairments to form their unique handwritten signatures. Key features of the system include: Personalized customization: Through tactile interaction and voice algorithmic guidance, individuals create signatures reflecting their preferences and natural writing style. Real-time feedback: AI-powered voice prompts and analysis ensure accuracy and consistency in signature formation. Accessibility: Installation in local service centers provides a secure and supervised environment for signature creation. The system's impact reaches beyond the individual level: Promotes inclusivity and independence: Blind individuals can engage in legal and financial transactions without rel
    
[^113]: 微小时间混合器 (TTMs): 针对多变量时间序列的增强零/少样本预测的快速预训练模型

    Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. (arXiv:2401.03955v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.03955](http://arxiv.org/abs/2401.03955)

    本论文介绍了一种名为微小时间混合器 (TTMs) 的预训练模型，该模型针对多变量时间序列的零/少样本预测进行了优化。与大型预训练模型相比，TTMs模型更小、更快，并考虑了跨通道相关性，能够在短时间内进行有效的预测。

    

    零/少样本学习的大型预训练模型在语言和视觉领域表现出色，但在多变量时间序列 (TS) 中面临着多样性和公开预训练数据稀缺的挑战。因此，最近在时间序列预测中使用预训练的大型语言模型 (LLMs) 进行各种适应的趋势逐渐增加。这些方法利用跨领域迁移学习，出奇地取得了令人印象深刻的结果。然而，这些模型通常非常缓慢且庞大（大约十亿个参数），并且不考虑跨通道相关性。为了解决这个问题，我们提出了多层微小时间混合器 (TTM)，这是一种基于轻量级 TSMixer 结构的显著小型模型。TTM 是首个成功开发的微型通用预训练模型（≤100万个参数），专门在公开TS数据集上进行快速训练（仅需4-8小时），具有有效的迁移学习能力进行预测。

    Large Pretrained models for zero/few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pretraining data. Consequently, there has been a recent surge in utilizing pretrained large language models (LLMs) with various adaptations for time series forecasting. These approaches employ cross-domain transfer learning and surprisingly yield impressive results. However, these models are typically very slow and large ($\sim$billion parameters) and do not consider cross-channel correlations. To address this, we present Multi-level Tiny Time Mixers (TTM), a significantly small model based on the lightweight TSMixer architecture. TTM marks the first success in developing tiny general-pretrained models ($\le$1 million parameters), exclusively trained on public TS datasets in a flash of just 4-8 hrs with effective transfer learning capabilities for forecasting. To tackle the complexi
    
[^114]: 基于文本图像编码器的AIGC图像质量评估回归方法

    TIER: Text-Image Encoder-based Regression for AIGC Image Quality Assessment. (arXiv:2401.03854v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.03854](http://arxiv.org/abs/2401.03854)

    本文提出了一种基于文本图像编码器的回归（TIER）框架，用于评估AI生成图像的质量。与现有方法直接从图像预测得分不同，TIER通过综合利用文本和图像特征来提高AIGCIQA的性能。

    

    最近，AI生成图像质量评估（AIGCIQA）作为计算机视觉中的一个新课题出现，旨在从人类感知角度评估AI生成的图像（AIGI）的质量。与常见的图像质量评估任务不同，这些任务中的图像是通过生成模型使用文本提示生成的，而不是通过噪声、模糊和压缩等方式从原始图像中导出。过去几年里，已经做了大量努力来推进AIGCIQA。然而，大多数现有的AIGCIQA方法直接从单个生成的图像回归预测得分，忽视了这些图像的文本提示中包含的信息。这种疏忽部分限制了这些AIGCIQA方法的性能。为了解决这个问题，我们提出了一种基于文本图像编码器的回归（TIER）框架。具体而言，我们将生成的图像及其对应的文本提示作为输入，利用一个文本编码器和一个图像编码器来处理这些输入。通过将文本和图像嵌入到同一个特征空间中，我们实现了综合利用文本和图像特征的AIGCIQA方法。

    Recently, AIGC image quality assessment (AIGCIQA), which aims to assess the quality of AI-generated images (AIGIs) from a human perception perspective, has emerged as a new topic in computer vision. Unlike common image quality assessment tasks where images are derived from original ones distorted by noise, blur, and compression, \textit{etc.}, in AIGCIQA tasks, images are typically generated by generative models using text prompts. Considerable efforts have been made in the past years to advance AIGCIQA. However, most existing AIGCIQA methods regress predicted scores directly from individual generated images, overlooking the information contained in the text prompts of these images. This oversight partially limits the performance of these AIGCIQA methods. To address this issue, we propose a text-image encoder-based regression (TIER) framework. Specifically, we process the generated images and their corresponding text prompts as inputs, utilizing a text encoder and an image encoder to e
    
[^115]: 长期安全的强化学习与二进制反馈

    Long-term Safe Reinforcement Learning with Binary Feedback. (arXiv:2401.03786v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.03786](http://arxiv.org/abs/2401.03786)

    本论文提出了一种长期安全的强化学习算法LoBiSaRL，该算法针对具有二进制安全反馈和未知随机状态转换函数的约束马尔可夫决策过程（CMDPs），通过最大化奖励的方式优化策略，同时以高概率确保每个回合中代理只执行安全的状态-动作对。

    

    安全是将强化学习应用于实际问题的不可或缺的要求。尽管近年来提出了大量的安全强化学习算法，但大多数现有工作通常1）依赖于接收数值安全反馈；2）在学习过程中无法保证安全性；3）将问题限制在先验已知的确定性转换动态；以及/或者4）假设存在一个已知的安全策略以处理任何状态。针对上述问题，我们提出了长期二进制反馈安全强化学习（LoBiSaRL），这是一种针对具有二进制安全反馈和未知随机状态转换函数的约束马尔可夫决策过程（CMDPs）的安全强化学习算法。LoBiSaRL优化一个策略以使奖励最大化，同时保证在每个回合中代理仅以高概率执行安全的状态-动作对，从而确保长期安全性。具体而言，LoBiSaRL通过广义线性模型（GLM）来建模二进制安全函数，并保证长期安全性。

    Safety is an indispensable requirement for applying reinforcement learning (RL) to real problems. Although there has been a surge of safe RL algorithms proposed in recent years, most existing work typically 1) relies on receiving numeric safety feedback; 2) does not guarantee safety during the learning process; 3) limits the problem to a priori known, deterministic transition dynamics; and/or 4) assume the existence of a known safe policy for any states. Addressing the issues mentioned above, we thus propose Long-term Binaryfeedback Safe RL (LoBiSaRL), a safe RL algorithm for constrained Markov decision processes (CMDPs) with binary safety feedback and an unknown, stochastic state transition function. LoBiSaRL optimizes a policy to maximize rewards while guaranteeing a long-term safety that an agent executes only safe state-action pairs throughout each episode with high probability. Specifically, LoBiSaRL models the binary safety function via a generalized linear model (GLM) and conser
    
[^116]: 机器学习在创伤性脑损伤中的应用：关注轻度脑损伤

    Machine Learning Applications in Traumatic Brain Injury: A Spotlight on Mild TBI. (arXiv:2401.03621v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2401.03621](http://arxiv.org/abs/2401.03621)

    该论文回顾了在创伤性脑损伤（TBI）中应用机器学习的最新技术，特别关注轻度脑损伤（mTBI）。虽然已经有很多机器学习技术用于诊断，但对预测预后的研究相对较少。

    

    创伤性脑损伤（TBI）是全球公共卫生领域面临的重大挑战，导致高发病率和死亡率，并给全球医疗系统带来巨大经济负担。TBI的诊断依赖于临床信息和计算机断层扫描。为了应对TBI所带来的多方面挑战，出现了创新的数据驱动方法。尤其值得注意的是轻度脑损伤（mTBI）的普遍存在，占多数TBI病例，传统方法往往难以胜任。因此，我们重点回顾了在TBI中应用于临床信息和计算机断层扫描的最新机器学习（ML）技术，特别关注mTBI。我们根据数据来源对ML应用进行分类，并可以看到迄今为止使用了各种ML技术。这些技术大多主要关注诊断，而对预测预后的努力相对较少。这个回顾会对当前的研究进行概述，并探索未来发展的方向。

    Traumatic Brain Injury (TBI) poses a significant global public health challenge, contributing to high morbidity and mortality rates and placing a substantial economic burden on healthcare systems worldwide. The diagnosis of TBI relies on clinical information along with Computed Tomography (CT) scans. Addressing the multifaceted challenges posed by TBI has seen the development of innovative, data-driven approaches, for this complex condition. Particularly noteworthy is the prevalence of mild TBI (mTBI), which constitutes the majority of TBI cases where conventional methods often fall short. As such, we review the state-of-the-art Machine Learning (ML) techniques applied to clinical information and CT scans in TBI, with a particular focus on mTBI. We categorize ML applications based on their data sources, and there is a spectrum of ML techniques used to date. Most of these techniques have primarily focused on diagnosis, with relatively few attempts at predicting the prognosis. This revie
    
[^117]: 行动中的现实主义：使用YOLOv8和DeiT从医学图像中诊断脑肿瘤的异常感知

    Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])

    [http://arxiv.org/abs/2401.03302](http://arxiv.org/abs/2401.03302)

    本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤，并解决了在罕见情况下的肿瘤检测问题。研究使用了来自国家脑映射实验室的数据集，通过修改样本数量和患者分布，使模型能够应对真实世界场景中的异常情况。

    

    在医学科学领域，由于脑肿瘤在患者中的罕见程度，可靠地检测和分类脑肿瘤仍然是一个艰巨的挑战。因此，在异常情况下检测肿瘤的能力对于确保及时干预和改善患者结果至关重要。本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤。来自国家脑映射实验室（NBML）的精选数据集包括81名患者，其中包括30例肿瘤病例和51例正常病例。检测和分类流程被分为两个连续的任务。检测阶段包括全面的数据分析和预处理，以修改图像样本和每个类别的患者数量，以符合真实世界场景中的异常分布（9个正常样本对应1个肿瘤样本）。此外，在测试中除了常见的评估指标外，我们还采用了... [摘要长度已达到上限]

    In the field of medical sciences, reliable detection and classification of brain tumors from images remains a formidable challenge due to the rarity of tumors within the population of patients. Therefore, the ability to detect tumors in anomaly scenarios is paramount for ensuring timely interventions and improved patient outcomes. This study addresses the issue by leveraging deep learning (DL) techniques to detect and classify brain tumors in challenging situations. The curated data set from the National Brain Mapping Lab (NBML) comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The detection and classification pipelines are separated into two consecutive tasks. The detection phase involved comprehensive data analysis and pre-processing to modify the number of image samples and the number of patients of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with real world scenarios. Next, in addition to common evaluation metrics for the testing, we emplo
    
[^118]: 车辆计划与时间窗口的最优链路

    Optimal Chaining of Vehicle Plans with Time Windows. (arXiv:2401.02873v1 [math.OC])

    [http://arxiv.org/abs/2401.02873](http://arxiv.org/abs/2401.02873)

    本论文提出了一种解决带有时间窗口的车辆路径问题的最优链路方法，考虑了计划的时间灵活性，并通过实证结果证明了该方法在解决静态拨打车问题时的优越性。

    

    在解决带有时间窗口的车辆路径问题时，我们经常需要将车辆计划连接成跨越更长时间区间的序列，换句话说，我们需要执行计划链路。最近，提出了一种基于网络的方法来解决车队规模问题。然而，该方法不考虑计划的时间灵活性，这是所有带有时间窗口的车辆路径问题的重要属性。相反，计划具有固定时间，不能延迟。本文提出了一种新的问题建模，考虑了延迟和给定时间窗口，并提出了一种解决该问题的方法。此外，我们证明了该方法是最优的，并对其复杂性进行了分析。最后，我们列举了一些实际应用，并对其中一个应用进行了演示：静态拨打车问题的解决方法。演示结果显示，在大量实例中，所提出的方法提供了更好的解决方案。

    For solving problems from the domain of vehicle routing with time windows, we often need to connect vehicle plans into sequences spanning a longer time horizon or, in other words, we need to perform a plan chaining. Recently, a network-based solution has been proposed to solve the fleet-sizing problem. The method, however, does not consider the time flexibility of the plans, an essential property of all vehicle routing problems with time windows. Instead, plans have fixed times and cannot be delayed. This work presents a new problem formulation that considers delays in line with the given time windows and a method that can be used to solve it. Moreover, we prove that the method is optimal, and we analyze its complexity. Finally, we list some practical applications and perform a demonstration for one of them: the method for solving the static Dial-a-ride problem. The demonstration results show that for a significant number of instances, the proposed method provides a better solution tha
    
[^119]: WaveCoder: 广泛和多功能的改进指令调优与完善的数据生成

    WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation. (arXiv:2312.14187v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.14187](http://arxiv.org/abs/2312.14187)

    本文提出了WaveCoder，一个广泛和多功能的改进指令调优模型，通过将指令数据分类并利用LLM框架生成多样的高质量指令数据，以提高调优模型的效果和泛化能力。

    

    近期的研究表明，在对高质量指令数据集进行调优后，生成的模型可以在广泛的任务上展现出令人印象深刻的能力。然而，现有的指令数据生成方法经常会产生重复数据，并且对数据质量的控制不够灵活。本文通过将指令数据分类为4个与代码相关的任务，扩展了指令调优的普适性，并提出了基于LLM的生成器-判别器数据处理框架，从开源代码中生成多样的、高质量的指令数据。因此，我们介绍了CodeOcean，一个包含4个通用代码相关任务的、共计20,000个指令实例的数据集，旨在增强指令调优的效果，并提高调优模型的泛化能力。随后，我们提出了WaveCoder，一个具有广泛和多功能的改进指令调优的Code LLM模型。

    Recent work demonstrates that, after being fine-tuned on a high-quality instruction dataset, the resulting model can obtain impressive capabilities to address a wide range of tasks. However, existing methods for instruction data generation often produce duplicate data and are not controllable enough on data quality. In this paper, we extend the generalization of instruction tuning by classifying the instruction data to 4 code-related tasks and propose a LLM-based Generator-Discriminator data process framework to generate diverse, high-quality instruction data from open source code. Hence, we introduce CodeOcean, a dataset comprising 20,000 instruction instances across 4 universal code-related tasks,which is aimed at augmenting the effectiveness of instruction tuning and improving the generalization ability of fine-tuned model. Subsequently, we present WaveCoder, a fine-tuned Code LLM with Widespread And Versatile Enhanced instruction tuning. This model is specifically designed for enha
    
[^120]: 强大的随机图生成器用于反事实解释

    Robust Stochastic Graph Generator for Counterfactual Explanations. (arXiv:2312.11747v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.11747](http://arxiv.org/abs/2312.11747)

    这篇论文介绍了一种新颖的鲁棒随机图生成器（RSGG-CE）用于反事实解释，该方法在生成类似于原始图形的新图形的同时，基于潜在预测模型产生不同的结果。此方法利用生成机制生成反事实实例，具有较高的解释能力，并在其他领域展现出卓越的成果。

    

    反事实解释（CE）技术作为向与AI系统互动的用户提供洞察力的手段已引起关注。虽然在医学成像和自动驾驶等领域进行了广泛研究，但图形反事实解释（GCE）方法相对较少。 GCE生成一个类似于原始图形的新图形，其结果基于潜在的预测模型。在这些GCE技术中，尽管在其他领域如艺术风格和自然语言建模中取得了令人印象深刻的成就，但基于生成机制的解释器仍然受到了相对有限的研究。对生成解释器的偏好源于它们在推断期间生成反事实实例的能力，利用自动获取的输入图形的扰动。在上述理由的推动下，我们的研究介绍了RSGG-CE，一种新颖的鲁棒随机图生成器，用于反事实解释

    Counterfactual Explanation (CE) techniques have garnered attention as a means to provide insights to the users engaging with AI systems. While extensively researched in domains such as medical imaging and autonomous vehicles, Graph Counterfactual Explanation (GCE) methods have been comparatively under-explored. GCEs generate a new graph similar to the original one, with a different outcome grounded on the underlying predictive model. Among these GCE techniques, those rooted in generative mechanisms have received relatively limited investigation despite demonstrating impressive accomplishments in other domains, such as artistic styles and natural language modelling. The preference for generative explainers stems from their capacity to generate counterfactual instances during inference, leveraging autonomously acquired perturbations of the input graph. Motivated by the rationales above, our study introduces RSGG-CE, a novel Robust Stochastic Graph Generator for Counterfactual Explanation
    
[^121]: WAVER:通过开放词汇知识通过视觉-语言模型提供对写作风格不受束缚的文本-视频检索

    WAVER: Writing-style Agnostic Text-Video Retrieval via Distilling Vision-Language Models Through Open-Vocabulary Knowledge. (arXiv:2312.09507v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.09507](http://arxiv.org/abs/2312.09507)

    WAVER是一种通过开放词汇知识进行跨域知识蒸馏的框架，用于解决视频描述中不同写作风格的问题。它利用预训练的视觉-语言模型，并采用隐式知识蒸馏方法，将文本知识从教师模型传递给学生模型。

    

    文本-视频检索是多模态信息检索领域中一个重要的子领域，在近年来取得了显著的增长。然而，现有方法假设视频场景与无偏的描述一致。这些限制与真实世界的情况不符，因为描述可能受到注释者的偏见、不同的写作风格和不同的文本视角的影响。为了克服上述问题，我们介绍了WAVER，一种通过视觉-语言模型通过开放词汇知识进行跨域知识蒸馏的框架，旨在解决处理视频描述中不同写作风格的挑战。WAVER利用预训练的视觉-语言模型中的开放词汇属性，并采用隐式知识蒸馏方法，将基于文本的知识从教师模型传递给基于视觉的学生模型。在四个标准基准数据集上进行了实证研究。

    Text-video retrieval, a prominent sub-field within the domain of multimodal information retrieval, has witnessed remarkable growth in recent years. However, existing methods assume video scenes are consistent with unbiased descriptions. These limitations fail to align with real-world scenarios since descriptions can be influenced by annotator biases, diverse writing styles, and varying textual perspectives. To overcome the aforementioned problems, we introduce $\texttt{WAVER}$, a cross-domain knowledge distillation framework via vision-language models through open-vocabulary knowledge designed to tackle the challenge of handling different writing styles in video descriptions. $\texttt{WAVER}$ capitalizes on the open-vocabulary properties that lie in pre-trained vision-language models and employs an implicit knowledge distillation approach to transfer text-based knowledge from a teacher model to a vision-based student. Empirical studies conducted across four standard benchmark datasets,
    
[^122]: 超越梯度和先验知识在隐私攻击中：利用联邦学习中语言模型的池化层输入

    Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.05720](http://arxiv.org/abs/2312.05720)

    本文引入了一种创新的方法，在联邦学习中利用语言模型的池化层输入来实现对隐私攻击的改进。通过恢复池化层输入，这种方法能够在不同的批处理大小下提供更高的文本恢复率，从而提供更细致和有效的见解。

    

    联邦学习强调分散式训练，通过本地存储数据并仅发送模型更新，强调用户隐私。最近，一系列有关隐私攻击的工作通过从联邦学习上下文的语言模型中提取敏感的训练文本来损害用户隐私。然而，这些攻击技术面临着不同的障碍：一些工作主要使用有限的批处理大小（例如，批处理大小为1），而其他技术则容易被检测出来。本文介绍了一种创新的方法，具有难以检测的特点，在不同的批处理大小设置下显著提高了文本恢复率。基于基本的梯度匹配和领域先验知识，我们通过恢复语言模型的池化层输入来增强攻击能力，这使我们能够在特征级别提供额外的监督信号。与梯度数据不同，这些信号不会在句子和标记之间进行平均，从而提供更细致和有效的见解。

    Federated learning (FL) emphasizes decentralized training by storing data locally and sending only model updates, underlining user privacy. Recently, a line of works on privacy attacks impairs user privacy by extracting sensitive training text from language models in the context of FL. Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others are easily detectable. This paper introduces an innovative approach that is challenging to detect, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the attack by recovering the input of the Pooler layer of language models, which enables us to provide additional supervised signals at the feature level. Unlike gradient data, these signals do not average across sentences and tokens, thereby offering more nuanced and effective insights. We benchmark our method using t
    
[^123]: 跨越生成性人工智能数据生命周期的隐私和版权挑战导航

    Navigating Privacy and Copyright Challenges Across the Data Lifecycle of Generative AI. (arXiv:2311.18252v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2311.18252](http://arxiv.org/abs/2311.18252)

    这项研究探讨了生成性人工智能中数据隐私和版权保护的多方面挑战，并提出了将技术创新与伦理前瞻相结合的综合方法，旨在全面解决这些问题。

    

    生成性人工智能的出现标志着人工智能领域的重要里程碑，展示出在生成真实图像、文本和数据模式方面的卓越能力。然而，这些进展也带来了对数据隐私和版权侵犯的更高关注，主要是由于模型训练对大规模数据集的依赖。传统方法如差分隐私、机器遗忘和数据中毒只提供了对这些复杂问题的片面解决方案。本文深入探讨了数据生命周期内隐私和版权保护的多方面挑战。我们主张采用将技术创新与伦理前瞻相结合的综合方法，通过研究和制定在生命周期视角下的解决方案，全面解决这些问题。本研究旨在推动更广泛的讨论，并激励对生成性人工智能中数据隐私和版权完整性的协同努力。

    The advent of Generative AI has marked a significant milestone in artificial intelligence, demonstrating remarkable capabilities in generating realistic images, texts, and data patterns. However, these advancements come with heightened concerns over data privacy and copyright infringement, primarily due to the reliance on vast datasets for model training. Traditional approaches like differential privacy, machine unlearning, and data poisoning only offer fragmented solutions to these complex issues. Our paper delves into the multifaceted challenges of privacy and copyright protection within the data lifecycle. We advocate for integrated approaches that combines technical innovation with ethical foresight, holistically addressing these concerns by investigating and devising solutions that are informed by the lifecycle perspective. This work aims to catalyze a broader discussion and inspire concerted efforts towards data privacy and copyright integrity in Generative AI.
    
[^124]: 通过使用随机尺度来估计深度神经网络的不确定性的规模丢弃

    Scale-Dropout: Estimating Uncertainty in Deep Neural Networks Using Stochastic Scale. (arXiv:2311.15816v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.15816](http://arxiv.org/abs/2311.15816)

    本文提出了一种用于二值神经网络（BNNs）的规模丢弃的新型正则化技术，并基于蒙特卡洛建立BayNN模型以高效地估计不确定性。

    

    神经网络（NN）中的不确定性估计对于改善在安全关键应用中的可靠性和预测的信心至关重要。具有Dropout作为近似的贝叶斯神经网络（BayNNs）为量化不确定性提供了一种系统方法，但它们在功耗、内存和计算方面具有高硬件开销。因此，将BayNNs应用于资源有限的边缘设备或高性能应用是具有挑战性的。通过在具有自旋电子存储器和参数二值化的计算内存（CIM）架构上加速它们可以减少BayNNs的一些固有成本。然而，实施常规基于dropout的BayNN需要大量的随机单元。在本文中，我们提出了一种新颖的二值神经网络（BNNs）的规模丢弃正则化技术，并基于蒙特卡洛-规模丢弃（MC-Scale Dropout）的BayNNs进行高效的不确定性估计。

    Uncertainty estimation in Neural Networks (NNs) is vital in improving reliability and confidence in predictions, particularly in safety-critical applications. Bayesian Neural Networks (BayNNs) with Dropout as an approximation offer a systematic approach to quantifying uncertainty, but they inherently suffer from high hardware overhead in terms of power, memory, and computation. Thus, the applicability of BayNNs to edge devices with limited resources or to high-performance applications is challenging. Some of the inherent costs of BayNNs can be reduced by accelerating them in hardware on a Computation-In-Memory (CIM) architecture with spintronic memories and binarizing their parameters. However, numerous stochastic units are required to implement conventional dropout-based BayNN. In this paper, we propose the Scale Dropout, a novel regularization technique for Binary Neural Networks (BNNs), and Monte Carlo-Scale Dropout (MC-Scale Dropout)-based BayNNs for efficient uncertainty estimatio
    
[^125]: 《基于课程学习和模仿学习的金融时间序列模型无关控制》

    Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series. (arXiv:2311.13326v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.13326](http://arxiv.org/abs/2311.13326)

    本文通过在复杂时间序列数据上探索课程学习和模仿学习的方法，发现课程学习是改善复杂时间序列控制任务性能的新途径，而模仿学习也应该被应用。

    

    课程学习和模仿学习在机器人领域已被广泛运用。然而，在高度随机的时间序列数据上利用这些想法进行控制任务的研究非常有限。在本研究中，我们从理论和实证两个方面探讨了这些方法在复杂时间序列数据上的代表性控制任务中的应用。我们通过数据增强实现了课程学习的基本思想，而通过模仿学习从专家中蒸馏出策略来实现。我们的研究结果表明，课程学习在改进复杂时间序列控制的任务性能方面应被视为一种新的方向。我们的大量随机种子外样本实证和消融研究对于时间序列控制的课程学习非常鼓舞人心。这些发现尤其鼓舞人心，因为我们在基线上调整了所有重叠的超参数，给出了基线的优势。另一方面，我们发现模仿学习应该被使用。

    Curriculum learning and imitation learning have been leveraged extensively in the robotics domain. However, minimal research has been done on leveraging these ideas on control tasks over highly stochastic time-series data. Here, we theoretically and empirically explore these approaches in a representative control task over complex time-series data. We implement the fundamental ideas of curriculum learning via data augmentation, while imitation learning is implemented via policy distillation from an oracle. Our findings reveal that curriculum learning should be considered a novel direction in improving control-task performance over complex time-series. Our ample random-seed out-sample empirics and ablation studies are highly encouraging for curriculum learning for time-series control. These findings are especially encouraging as we tune all overlapping hyperparameters on the baseline -- giving an advantage to the baseline. On the other hand, we find that imitation learning should be use
    
[^126]: ALYMPICS：语言代理人与博弈论相遇——用AI代理人探索战略决策

    ALYMPICS: Language Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents. (arXiv:2311.03220v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.03220](http://arxiv.org/abs/2311.03220)

    本文介绍了Alympics，一个利用大型语言模型代理人进行博弈论研究的系统性模拟框架。通过模拟人类战略互动，框架能够定性和定量地分析游戏决定因素、策略和结果，并对代理人在战略决策场景中的表现进行评估。

    

    本文介绍了Alympics（代理人的奥运会），这是一个利用大型语言模型（LLM）代理人进行博弈论研究的系统性模拟框架。Alympics创建了一个多功能平台，用于研究复杂的博弈论问题，通过提供一个控制环境来模拟与LLM代理人进行类似人类的战略互动，弥合了理论博弈论和实证研究之间的差距。在我们的试点案例研究中，“水资源分配挑战”，我们通过一个关注稀缺生存资源多轮拍卖的挑战性战略游戏来探索Alympics。这项研究展示了该框架在定性和定量分析游戏决定因素、策略和结果方面的能力。此外，我们进行了全面的人类评估和对LLM代理人在战略决策场景中的深入评估。我们的发现不仅扩展了对LLM代理人模拟人类战略行为能力的理解，还

    This paper introduces Alympics (Olympics for Agents), a systematic simulation framework utilizing Large Language Model (LLM) agents for game theory research. Alympics creates a versatile platform for studying complex game theory problems, bridging the gap between theoretical game theory and empirical investigations by providing a controlled environment for simulating human-like strategic interactions with LLM agents. In our pilot case study, the "Water Allocation Challenge," we explore Alympics through a challenging strategic game focused on the multi-round auction on scarce survival resources. This study demonstrates the framework's ability to qualitatively and quantitatively analyze game determinants, strategies, and outcomes. Additionally, we conduct a comprehensive human assessment and an in-depth evaluation of LLM agents in strategic decision-making scenarios. Our findings not only expand the understanding of LLM agents' proficiency in emulating human strategic behavior but also h
    
[^127]: CausalCite：一种论文引用的因果公式化

    CausalCite: A Causal Formulation of Paper Citations. (arXiv:2311.02790v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.02790](http://arxiv.org/abs/2311.02790)

    CausalCite是一种以因果推断为基础的论文引用公式化方法，通过对文本进行嵌入和相似样本的提取来评估论文的重要性，并在各个标准上展示了其有效性。

    

    对于科学界来说，评估一篇论文的重要性至关重要但也具有挑战性。尽管引用次数是最常用的评估指标，但它们被广泛批评为无法准确反映一篇论文的真正影响力。在这项工作中，我们提出了一种因果推断方法，称为TextMatch，它将传统的匹配框架适应于高维文本嵌入。具体而言，我们使用大型语言模型（LLM）对每篇论文进行文本嵌入，通过余弦相似性提取相似样本，并根据相似度值的加权平均合成一个反事实样本。我们将得到的指标称为CausalCite，作为论文引用的因果公式化。我们展示了它在各种标准上的有效性，如与科学专家对1K篇论文的报告的论文影响力的高相关性，过去论文的（经过时间考验的）奖项，以及在各个子领域的稳定性。

    Evaluating the significance of a paper is pivotal yet challenging for the scientific community. While the citation count is the most commonly used proxy for this purpose, they are widely criticized for failing to accurately reflect a paper's true impact. In this work, we propose a causal inference method, TextMatch, which adapts the traditional matching framework to high-dimensional text embeddings. Specifically, we encode each paper using the text embeddings by large language models (LLMs), extract similar samples by cosine similarity, and synthesize a counterfactual sample by the weighted average of similar papers according to their similarity values. We apply the resulting metric, called CausalCite, as a causal formulation of paper citations. We show its effectiveness on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, (test-of-time) awards for past papers, and its stability across various sub-fields o
    
[^128]: 朝着鲁棒剪枝：一种面向语言模型的自适应知识保留剪枝策略

    Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models. (arXiv:2310.13191v1 [cs.CL])

    [http://arxiv.org/abs/2310.13191](http://arxiv.org/abs/2310.13191)

    本文提出了一种适应性知识保留剪枝策略，旨在提高语言模型对抗攻击的鲁棒性，并在剪枝过程中保留更多的预训练知识。与其他方法相比，该方法展现了更好的平衡。

    

    剪枝目标近期不仅仅局限于准确性和稀疏性，还包括对语言模型鲁棒性的提升。然而，现有方法在持续增加模型稀疏性时，对抗攻击的鲁棒性提升方面存在困难，并需要重新训练过程。随着人们步入大型语言模型的时代，这些问题变得越来越突出。本文提出，语言模型的鲁棒性与其涵盖的预训练知识程度成正比。因此，我们引入了一种后训练的剪枝策略，旨在在剪枝过程中保留更多预训练知识，以忠实地复制密集语言模型的嵌入空间和特征空间。在这个设置中，每一层的重构误差不仅源自自身，还包括前面层的累积误差，然后进行自适应的矫正。与其他最先进的基线方法相比，我们的方法展现了更好的平衡。

    The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer's reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance bet
    
[^129]: CodeFuse-13B: 一个预训练的多语言代码大型语言模型

    CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model. (arXiv:2310.06266v1 [cs.SE])

    [http://arxiv.org/abs/2310.06266](http://arxiv.org/abs/2310.06266)

    CodeFuse-13B是一个预训练的多语言代码大型语言模型，专为代码相关任务设计，支持超过40种编程语言，并通过使用高质量的预训练数据集以及大量实验的验证，展现了其在多语言输入下的有效性。

    

    代码大型语言模型(Code LLMs)因其在软件工程全生命周期中的广泛应用而受到工业界的广泛关注。然而，现有模型在理解非英语输入的多语言代码相关任务方面的效果仍然远未被充分研究。本文介绍了CodeFuse-13B，一个开源的预训练代码LLM。它专为包含英文和中文提示的代码相关任务而设计，并支持超过40种编程语言。CodeFuse通过利用由程序分析器精心筛选并在训练过程中优化的高质量预训练数据集来实现其效果。我们进行了大量实验，包括使用真实世界的使用场景、工业标准基准HumanEval-x，以及专为中文提示设计的CodeFuseEval。为了评估CodeFuse的有效性，我们积极收集了AntGroup软件开发团队的宝贵人工反馈。

    Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software develop
    
[^130]: 一种关于货币的新的经济与金融理论

    A new economic and financial theory of money. (arXiv:2310.04986v1 [econ.TH])

    [http://arxiv.org/abs/2310.04986](http://arxiv.org/abs/2310.04986)

    这篇论文通过根本性的改革，将电子货币纳入经济与金融理论，提出了一种新的理论框架，包括电子货币的估值基于宏观经济理论和货币政策的基本方程，以及电子货币管理公司作为协调次经济体货币和财政政策的实体。该研究避免使用普遍但不适当的指数风险模型，而是采用多时间尺度的模型。

    

    本文对经济与金融理论进行了根本性改革，包括电子货币在内。电子货币的估值将基于宏观经济理论和货币政策的基本方程，而不是微观经济学中的贴现现金流理论。与将股票视为与次经济体的无形资产关联的所有权不同，我们将发展电子货币作为与次经济体有形资产关联的交易权益的观点。我们还将发展电子货币管理公司作为一个负责协调次经济体的货币（电子货币供应和价值稳定）和财政（投资和运营）政策的实体的视角，以实现电子货币的流动性。在估值和决策中使用的风险模型不会是无处不在但不合适的指数风险模型，它将导致贴现率，而是多时间尺度的模型。

    This paper fundamentally reformulates economic and financial theory to include electronic currencies. The valuation of the electronic currencies will be based on macroeconomic theory and the fundamental equation of monetary policy, not the microeconomic theory of discounted cash flows. The view of electronic currency as a transactional equity associated with tangible assets of a sub-economy will be developed, in contrast to the view of stock as an equity associated mostly with intangible assets of a sub-economy. The view will be developed of the electronic currency management firm as an entity responsible for coordinated monetary (electronic currency supply and value stabilization) and fiscal (investment and operational) policies of a substantial (for liquidity of the electronic currency) sub-economy. The risk model used in the valuations and the decision-making will not be the ubiquitous, yet inappropriate, exponential risk model that leads to discount rates, but will be multi time sc
    
[^131]: 基于似然的物联网系统中专家支持的分布式学习算法中传感器校准的研究

    Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems. (arXiv:2309.11526v1 [cs.LG])

    [http://arxiv.org/abs/2309.11526](http://arxiv.org/abs/2309.11526)

    该论文介绍了一种基于似然的传感器校准方法，可以在物联网系统中实现专家支持的分布式学习算法。通过对模拟和实际测量数据的评估，证明了该方法的有效性和改进效果。

    

    传感器技术领域中的一个重要任务是将一个传感器的测量结果高效地适应到另一个具有相同设计的传感器。一种想法是使用不同系统之间的仿射变换估计，这可以通过专家的知识进行改进。本文介绍了Glacier Research在1973年发表的改进解决方案，并展示了该解决方案可以用于传感器的软件校准、基于专家的适应和联邦学习方法。我们通过模拟和实际测量数据对我们的研究进行了评估，实验中使用了一个具有8个相同传感器的多传感器板。结果表明，无论是模拟还是实验数据，都得到了改进。

    An important task in the field of sensor technology is the efficient implementation of adaptation procedures of measurements from one sensor to another sensor of identical design. One idea is to use the estimation of an affine transformation between different systems, which can be improved by the knowledge of experts. This paper presents an improved solution from Glacier Research that was published back in 1973. It is shown that this solution can be adapted for software calibration of sensors, implementation of expert-based adaptation, and federated learning methods. We evaluate our research with simulations and also with real measured data of a multi-sensor board with 8 identical sensors. The results show an improvement for both the simulation and the experiments with real data.
    
[^132]: 多模态多任务对话行为分类中的任务选择和分配方法研究

    Task Selection and Assignment for Multi-modal Multi-task Dialogue Act Classification with Non-stationary Multi-armed Bandits. (arXiv:2309.09832v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.09832](http://arxiv.org/abs/2309.09832)

    本文研究了多模态多任务对话行为分类任务，并使用非平稳多臂赌博机和高斯先验的折扣汤普森采样方法进行任务选择和分配。实验结果表明，我们的方法能够有效识别任务效用，在训练过程中避免无用或有害的任务，并在UAR方面具有显著优势。

    

    多任务学习旨在通过与相关辅助任务的联合学习来提高主要任务的性能。传统的多任务学习方法在训练过程中随机选择任务。然而，先前的研究和我们的结果表明，这种随机选择任务的方法可能对性能没有帮助，甚至会有害。因此，需要探索多任务学习中任务选择和分配的新策略。本文研究了多模态多任务对话行为分类任务，并提出了一种基于非平稳多臂赌博机和高斯先验的折扣汤普森采样方法来选择和分配任务。实验结果表明，在不同的训练阶段，不同的任务具有不同的效用。我们提出的方法可以有效地识别任务效用，主动避免无用或有害的任务，并在训练过程中实现任务分配。在UAR方面，我们提出的方法显着优于其他方法。

    Multi-task learning (MTL) aims to improve the performance of a primary task by jointly learning with related auxiliary tasks. Traditional MTL methods select tasks randomly during training. However, both previous studies and our results suggest that such a random selection of tasks may not be helpful, and can even be harmful to performance. Therefore, new strategies for task selection and assignment in MTL need to be explored. This paper studies the multi-modal, multi-task dialogue act classification task, and proposes a method for selecting and assigning tasks based on non-stationary multi-armed bandits (MAB) with discounted Thompson Sampling (TS) using Gaussian priors. Our experimental results show that in different training stages, different tasks have different utility. Our proposed method can effectively identify the task utility, actively avoid useless or harmful tasks, and realise the task assignment during training. Our proposed method is significantly superior in terms of UAR a
    
[^133]: 使用遮掩图像建模的异构生成知识蒸馏

    Heterogeneous Generative Knowledge Distillation with Masked Image Modeling. (arXiv:2309.09571v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.09571](http://arxiv.org/abs/2309.09571)

    本文提出了一种基于遮掩图像建模的异构生成知识蒸馏（H-GKD）方法，可以高效地从大型Transformer模型传递知识给小型CNN模型。通过训练一个带有稀疏卷积的UNet风格学生网络，桥接了Transformer模型和CNN模型之间的差距，实现了视觉表示的有效模拟。

    

    在计算资源有限的边缘设备上部署时，小型基于CNN的模型通常需要从大型模型中转移知识。遮掩图像建模（MIM）方法在各种视觉任务中取得了很大的成功，但在异构深度模型的知识蒸馏中仍然很少被探索。主要原因是基于Transformer的大型模型与基于CNN的小型网络之间存在显著差异。在本文中，我们基于MIM开发了第一个使用异构生成知识蒸馏（H-GKD）的方法，可以以生成的自监督方式高效地从大型Transformer模型传递知识给小型基于CNN的模型。我们的方法通过训练一个带有稀疏卷积的UNet风格的学生网络来构建Transformer模型和CNN之间的桥梁，这个学生网络可以有效地模仿教师网络在遮掩建模下推断的视觉表示。我们的方法是一种简单而有效的学习范式，用于学习视觉提示和知识在异构模型之间的转移。

    Small CNN-based models usually require transferring knowledge from a large model before they are deployed in computationally resource-limited edge devices. Masked image modeling (MIM) methods achieve great success in various visual tasks but remain largely unexplored in knowledge distillation for heterogeneous deep models. The reason is mainly due to the significant discrepancy between the Transformer-based large model and the CNN-based small network. In this paper, we develop the first Heterogeneous Generative Knowledge Distillation (H-GKD) based on MIM, which can efficiently transfer knowledge from large Transformer models to small CNN-based models in a generative self-supervised fashion. Our method builds a bridge between Transformer-based models and CNNs by training a UNet-style student with sparse convolution, which can effectively mimic the visual representation inferred by a teacher over masked modeling. Our method is a simple yet effective learning paradigm to learn the visual 
    
[^134]: 通过偏好学习在多目标问题中进行交互式超参数优化

    Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning. (arXiv:2309.03581v1 [cs.LG])

    [http://arxiv.org/abs/2309.03581](http://arxiv.org/abs/2309.03581)

    本文提出了一个以人为中心的交互式超参数优化方法，通过应用偏好学习来解决多目标机器学习中的问题。

    

    超参数优化对于发挥机器学习的潜力至关重要。在实践中，用户通常对多目标问题感兴趣，即优化可能存在冲突的目标，比如准确性和能耗。为了解决这个问题，绝大多数多目标机器学习算法将一组非支配的机器学习模型的帕累托前沿返回给用户。然而，优化这种算法的超参数并不容易，因为评估一个超参数配置涉及评估得到的帕累托前沿的质量。在文献中，已有一些指标可以通过量化不同属性（如体积、与参考点的接近程度）来评估帕累托前沿的质量（例如超体积、R2）。然而，对于用户来说，选择导致期望的帕累托前沿的指标可能是一项困难的任务。在本文中，我们提出了一个以人为中心的交互式超参数优化方法，针对多目标机器学习应用偏好学习。

    Hyperparameter optimization (HPO) is important to leverage the full potential of machine learning (ML). In practice, users are often interested in multi-objective (MO) problems, i.e., optimizing potentially conflicting objectives, like accuracy and energy consumption. To tackle this, the vast majority of MO-ML algorithms return a Pareto front of non-dominated machine learning models to the user. Optimizing the hyperparameters of such algorithms is non-trivial as evaluating a hyperparameter configuration entails evaluating the quality of the resulting Pareto front. In literature, there are known indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by quantifying different properties (e.g., volume, proximity to a reference point). However, choosing the indicator that leads to the desired Pareto front might be a hard task for a user. In this paper, we propose a human-centered interactive HPO approach tailored towards multi-objective ML leveraging preference learnin
    
[^135]: ProAgent：利用大型语言模型构建主动合作的人工智能

    ProAgent: Building Proactive Cooperative AI with Large Language Models. (arXiv:2308.11339v1 [cs.AI])

    [http://arxiv.org/abs/2308.11339](http://arxiv.org/abs/2308.11339)

    ProAgent是一个利用大型语言模型构建的主动合作的AI框架，能够预测队友的决策并为自己制定增强计划，具有高度的模块化和可解释性。

    

    在AGI研究中，构建具有自适应行为的人工智能以进行人工智能和人类的合作成为一个关键关注点。目前，开发合作代理人的方法主要依赖于基于学习的方法，其中政策泛化严重依赖于与特定队友的过去互动。这些方法限制了代理人在面对新的队友时重新校准策略的能力。我们提出了ProAgent，这是一个新颖的框架，利用大型语言模型（LLMs）来创建一个具有预测队友未来决策能力和为自身制定增强计划能力的主动代理。ProAgent在合作推理方面表现出色，能够动态调整行为以增强与队友的协作努力。此外，ProAgent框架具有高度的模块化和可解释性，便于无缝集成，以应对各种协调场景。

    Building AIs with adaptive behaviors in human-AI cooperation stands as a pivotal focus in AGI research. Current methods for developing cooperative agents predominantly rely on learning-based methods, where policy generalization heavily hinges on past interactions with specific teammates. These approaches constrain the agent's capacity to recalibrate its strategy when confronted with novel teammates. We propose \textbf{ProAgent}, a novel framework that harnesses large language models (LLMs) to fashion a \textit{pro}active \textit{agent} empowered with the ability to anticipate teammates' forthcoming decisions and formulate enhanced plans for itself. ProAgent excels at cooperative reasoning with the capacity to dynamically adapt its behavior to enhance collaborative efforts with teammates. Moreover, the ProAgent framework exhibits a high degree of modularity and interpretability, facilitating seamless integration to address a wide array of coordination scenarios. Experimental evaluations
    
[^136]: 生成人工智能的强化学习：现状、机会和开放研究挑战

    Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. (arXiv:2308.00031v1 [cs.LG])

    [http://arxiv.org/abs/2308.00031](http://arxiv.org/abs/2308.00031)

    这篇论文调查了在生成人工智能中应用强化学习的现状、机会和开放研究问题。作者主要讨论了三种应用类型：无特定目标的生成方式、同时最大化目标函数的输出生成方式以及将无法通过目标函数捕捉的期望特征嵌入生成过程的方式。这个新兴领域中存在着丰富的机会和挑战。

    

    生成人工智能（AI）是近十年来计算机科学领域最令人兴奋的发展之一。与此同时，强化学习（RL）在各种机器学习任务中已经成为非常成功的范式。在本调查中，我们讨论了将RL应用于生成AI中的现状、机会和开放的研究问题。具体而言，我们将讨论三种应用类型，即作为一种无特定目标的生成方式，作为一种同时最大化目标函数的输出生成方式，以及作为一种将无法通过目标函数轻松捕捉的期望特征嵌入生成过程的方式。我们在调查结果中对这个迷人的新兴领域中的机会和挑战进行了深入的讨论。

    Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.
    
[^137]: 深度迁移学习在工业时间序列异常检测中的综合调查：方法、应用和方向

    A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions. (arXiv:2307.05638v1 [cs.LG])

    [http://arxiv.org/abs/2307.05638](http://arxiv.org/abs/2307.05638)

    本综述调查了深度迁移学习在工业时间序列异常检测中的使用。深度迁移学习通过利用相关任务的知识和考虑数据分布的变化，解决了仅有少量或没有附加标记数据情况下的新任务。

    

    自动化监测工业过程有潜力通过及时检测异常事件并促进及时干预来提高效率和优化质量。深度学习通过识别大数据集中的非平凡模式，在这一过程中发挥着关键作用。标准的深度学习方法适用于解决特定类型的数据给定特定任务的问题。在训练过程中，这些算法需要大量的标记训练数据。然而，由于工艺和环境的动态性，为每个稍有不同的情况重新获得所需数据进行标准深度学习训练是不现实的。深度迁移学习提供了解决这个问题的方法。通过利用相关任务的知识和考虑数据分布的变化，这个学习框架可以解决新任务，即使没有或只有很少的附加标记数据。这种方法避免了从头开始重新训练模型的需要。

    Automating the monitoring of industrial processes has the potential to enhance efficiency and optimize quality by promptly detecting abnormal events and thus facilitating timely interventions. Deep learning, with its capacity to discern non-trivial patterns within large datasets, plays a pivotal role in this process. Standard deep learning methods are suitable to solve a specific task given a specific type of data. During training, the algorithms demand large volumes of labeled training data. However, due to the dynamic nature of processes and the environment, it is impractical to acquire the needed data for standard deep learning training for every slightly different case anew. Deep transfer learning offers a solution to this problem. By leveraging knowledge from related tasks and accounting for variations in data distributions, this learning framework solves new tasks even with little or no additional labeled data. The approach bypasses the need to retrain a model from scratch for ev
    
[^138]: 精细动作分析：一种多模态和多任务的花样滑冰数据集

    Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating. (arXiv:2307.02730v1 [cs.CV])

    [http://arxiv.org/abs/2307.02730](http://arxiv.org/abs/2307.02730)

    这篇论文提出了一种多模态和多任务的花样滑冰数据集（MMFS），包含了256个类别的动作得分和空间和时间标签。该数据集的关键贡献是首次引入了独立的空间和时间分类，并首次使用骨骼模态进行精细动作质量评估。

    

    现有的动作数据集的精细动作分析面临着动作类别不足、细粒度低、模态和任务有限的挑战。本文提出了一种多模态多任务的花样滑冰数据集（MMFS），该数据集是从世界花样滑冰锦标赛中收集而来的。MMFS包括动作识别和动作质量评估，使用RGB、骨骼和11671个视频片段采集了256个类别的动作得分，并包含了空间和时间标签。我们的数据集的关键贡献包括以下三个方面：（1）首次提出了独立的空间和时间分类，以进一步探索精细动作识别和质量评估。(2)MMFS首次引入了骨骼模态用于复杂的精细动作质量评估。(3)我们的多模态和多任务数据集鼓励更多的动作分析模型。为了评估我们的数据集，我们采用了基于RGB和基于骨骼的方法。

    The fine-grained action analysis of the existing action datasets is challenged by insufficient action categories, low fine granularities, limited modalities, and tasks. In this paper, we propose a Multi-modality and Multi-task dataset of Figure Skating (MMFS) which was collected from the World Figure Skating Championships. MMFS, which possesses action recognition and action quality assessment, captures RGB, skeleton, and is collected the score of actions from 11671 clips with 256 categories including spatial and temporal labels. The key contributions of our dataset fall into three aspects as follows. (1) Independently spatial and temporal categories are first proposed to further explore fine-grained action recognition and quality assessment. (2) MMFS first introduces the skeleton modality for complex fine-grained action quality assessment. (3) Our multi-modality and multi-task dataset encourage more action analysis models. To benchmark our dataset, we adopt RGB-based and skeleton-based
    
[^139]: TL-nvSRAM-CIM：具有零直流功耗恢复和三态MAC操作的超高密度三级ReRAM辅助计算存储器内计算

    TL-nvSRAM-CIM: Ultra-High-Density Three-Level ReRAM-Assisted Computing-in-nvSRAM with DC-Power Free Restore and Ternary MAC Operations. (arXiv:2307.02717v1 [cs.AR])

    [http://arxiv.org/abs/2307.02717](http://arxiv.org/abs/2307.02717)

    TL-nvSRAM-CIM是一种新颖的存储器内计算方案，利用超高密度的三级ReRAM辅助计算来解决大规模神经网络模型中权重容量不足的问题，并采用了零直流功耗恢复和三态MAC操作来提高能效和保持准确性。

    

    随着规模庞大的神经网络，将所有权重放在芯片上仍然是SRAM-CIM的一大挑战，因为其存储容量有限。之前的非易失性SRAM-CIM（nvSRAM-CIM）通过在高效SRAM-CIM上集成高密度单级ReRAM来解决这个问题，在芯片上储存权重以消除芯片外的内存访问。然而，之前的SL-nvSRAM-CIM在增加SL-ReRAM数量和计算效率受限方面存在问题。为了克服这些挑战，本文提出了一种超高密度三级ReRAM辅助的非易失性SRAM计算存储器内计算方案（TL-nvSRAM-CIM）用于大型神经网络模型。采用了集群式选择器-n-ReRAM（cluster-nSnRs）来可靠地恢复权重，并消除了直流功耗。此外，提出了一种带有差分计算方案的三态SRAM-CIM机制，用于能量高效的三态MAC操作，并保持高NN准确性。提出的TL-nvSRAM-CIM实现了7.

    Accommodating all the weights on-chip for large-scale NNs remains a great challenge for SRAM based computing-in-memory (SRAM-CIM) with limited on-chip capacity. Previous non-volatile SRAM-CIM (nvSRAM-CIM) addresses this issue by integrating high-density single-level ReRAMs on the top of high-efficiency SRAM-CIM for weight storage to eliminate the off-chip memory access. However, previous SL-nvSRAM-CIM suffers from poor scalability for an increased number of SL-ReRAMs and limited computing efficiency. To overcome these challenges, this work proposes an ultra-high-density three-level ReRAMs-assisted computing-in-nonvolatile-SRAM (TL-nvSRAM-CIM) scheme for large NN models. The clustered n-selector-n-ReRAM (cluster-nSnRs) is employed for reliable weight-restore with eliminated DC power. Furthermore, a ternary SRAM-CIM mechanism with differential computing scheme is proposed for energy-efficient ternary MAC operations while preserving high NN accuracy. The proposed TL-nvSRAM-CIM achieves 7.
    
[^140]: 通过领域知识启示的深度学习进行药物推荐

    Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])

    [http://arxiv.org/abs/2305.19604](http://arxiv.org/abs/2305.19604)

    提出一种基于动态领域知识的药物推荐框架DKINet，将领域知识与患者临床表现相结合，此为首次实验。

    

    药物推荐是医疗保健的基本但至关重要的分支，提供机会为复杂健康状况的患者支持临床医生更精确的药物处方。从电子健康记录（EHR）中学习推荐药物是先前研究中最常见的方法。然而，大多数研究忽视了根据患者的EHR中的临床表现纳入领域知识的问题。为了解决这些问题，我们提出了一种新颖的基于动态领域知识的药物推荐框架，即领域知识启示网络（DKINet），用于将领域知识与可观察的患者临床表现相结合。特别是，我们首先设计了一个基于领域知识的编码器来捕捉领域信息，然后开发了一个数据驱动的编码器将领域知识整合到可观察的EHR中。

    Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
    
[^141]: 大型语言模型的异质价值评估

    Heterogeneous Value Evaluation for Large Language Models. (arXiv:2305.17147v1 [cs.CL])

    [http://arxiv.org/abs/2305.17147](http://arxiv.org/abs/2305.17147)

    本文提出了一种自动对齐评估方法A2EHV，采用异质价值系统，并基于价值合理性和社会价值定向框架评估代理人行为的社会偏好，结果表明比传统对齐方法更合理。

    

    大型语言模型（LLM）的出现使得将它们的价值与人类价值对齐变得至关重要。当前的方法通常尝试将其与一种同质的人类价值对齐，并需要人类验证，但缺乏对对齐所需方面和深度的共识以及造成的人类偏见。在本文中，我们提出了一种自动对齐评估方法A2EHV，该方法采用异质价值系统，（1）是自动化的，以最小化单个人类偏见，并且（2）允许评估针对各种目标值的异质代理人。我们的方法基于价值合理性的概念，它代表了代理人执行最能满足目标价值行为的能力。价值合理性的量化是通过社会心理学中的社会价值定向框架进行的，该框架将价值空间分为四个类别，以评估代理人行为的社会偏好。我们评估了三个模型的价值合理性，结果表明A2EHV方法比传统对齐方法更合理。

    The emergent capabilities of Large Language Models (LLMs) have made it crucial to align their values with those of humans. Current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. In this paper, we propose A2EHV, an Automated Alignment Evaluation with a Heterogeneous Value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. Our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. The quantification of value rationality is facilitated by the Social Value Orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. We evaluate the value rationality of e
    
[^142]: 基于基础模型的系统设计框架

    A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])

    [http://arxiv.org/abs/2305.05352](http://arxiv.org/abs/2305.05352)

    本文提出了一个基于基础模型的系统分类体系，分类和比较了基础模型和基于基础模型的系统的特点。它为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。

    

    最近推出了大型语言模型(LLM)的聊天机器人，如ChatGPT，这引起了人们对基础模型的广泛关注。基础模型被广泛认为将成为未来人工智能系统的基石。由于基础模型处于早期阶段，基于基础模型的系统设计尚未得到系统地探索。人们对在软件架构中引入基础模型的影响知之甚少。因此，在本文中，我们提出了一个基于基础模型的系统分类法，对基础模型和基于基础模型的系统的特点进行了分类和比较。我们的分类法包括三个类别：基础模型预训练和微调、基于基础模型的系统架构设计和负责任的AI设计。这个分类法为设计基于基础模型的系统时做出主要的设计决策提供了具体的指导，并突出了相关的权衡。

    The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
    
[^143]: 意义的线性空间：视觉语言模型中的组合结构

    Linear Spaces of Meanings: Compositional Structures in Vision-Language Models. (arXiv:2302.14383v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14383](http://arxiv.org/abs/2302.14383)

    本文研究了视觉语言模型中的组合结构，并提出了一种使用嵌入空间中较小集合的向量组合来近似表示来自编码器的表示形式的方法，将这些向量视为“理想单词”，并在CLIP的嵌入中以实验方式探索了这些结构的可用性。

    

    本文研究了预训练视觉语言模型（VLM）中的数据嵌入的组合结构。传统上，组合性与预先存在的词汇表中的单词嵌入的代数运算有关。相反，我们试图使用嵌入空间中较小集合的向量组合来近似表示来自编码器的表示形式。这些向量可以被看作是在模型的嵌入空间中直接生成概念的“理想单词”。我们首先从几何学的角度提出了理解组合结构的框架。然后，我们解释了VLM嵌入在概率上的这些组合结构的含义，并提供了它们在实践中产生的直觉。最后，我们在CLIP的嵌入中以实验方式探索了这些结构，并评估了它们在解决分类、去偏和检索等不同视觉语言任务中的有用性。我们的结果表明，嵌入空间中简单的线性代数运算可以实现与更复杂的方法相媲美甚至更好的性能，证明了所提出的意义的线性空间的有效性和可解释性。

    We investigate compositional structures in data embeddings from pre-trained vision-language models (VLMs). Traditionally, compositionality has been associated with algebraic operations on embeddings of words from a pre-existing vocabulary. In contrast, we seek to approximate representations from an encoder as combinations of a smaller set of vectors in the embedding space. These vectors can be seen as "ideal words" for generating concepts directly within the embedding space of the model. We first present a framework for understanding compositional structures from a geometric perspective. We then explain what these compositional structures entail probabilistically in the case of VLM embeddings, providing intuitions for why they arise in practice. Finally, we empirically explore these structures in CLIP's embeddings and we evaluate their usefulness for solving different vision-language tasks such as classification, debiasing, and retrieval. Our results show that simple linear algebraic o
    
[^144]: 被遗忘还是被公平对待：揭示机器遗忘方法的公平性影响

    To Be Forgotten or To Be Fair: Unveiling Fairness Implications of Machine Unlearning Methods. (arXiv:2302.03350v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2302.03350](http://arxiv.org/abs/2302.03350)

    这项研究揭示了机器遗忘方法在公平性方面的影响，帮助软件工程师做出负责任的决策。

    

    个人信息被遗忘的权利是由人们不希望自己的过去行为永久地给予不利影响的愿望所推动的。为了实现这一目标，数据删除需要彻底和永久，还必须从机器学习模型中删除。研究人员提出了机器遗忘算法，旨在更高效地从训练模型中删除特定数据。然而，这些方法改变了数据输入模型和训练方式，可能从公平性的角度妥协了人工智能伦理。为了帮助软件工程师在采用这些遗忘方法时做出负责任的决策，我们进行了首个机器遗忘方法的公平性影响研究。我们设计并进行了实验，使用三个公平性数据集和三种不同的删除策略，对两种典型的机器遗忘方法（SISA和AmnesiacML）以及重新训练方法（ORTR）进行了实验。实验结果表明，在非均匀数据删除的情况下，机器遗忘方法可能对公平性产生影响。

    The right to be forgotten (RTBF) is motivated by the desire of people not to be perpetually disadvantaged by their past deeds. For this, data deletion needs to be deep and permanent, and should be removed from machine learning models. Researchers have proposed machine unlearning algorithms which aim to erase specific data from trained models more efficiently. However, these methods modify how data is fed into the model and how training is done, which may subsequently compromise AI ethics from the fairness perspective. To help software engineers make responsible decisions when adopting these unlearning methods, we present the first study on machine unlearning methods to reveal their fairness implications. We designed and conducted experiments on two typical machine unlearning methods (SISA and AmnesiacML) along with a retraining method (ORTR) as baseline using three fairness datasets under three different deletion strategies. Experimental results show that under non-uniform data deletio
    
[^145]: 从第一原理设计智能生态系统

    Designing Ecosystems of Intelligence from First Principles. (arXiv:2212.01354v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.01354](http://arxiv.org/abs/2212.01354)

    本文提出了一个关于未来人工智能领域研究和发展的愿景，该愿景是基于主动推理和自证明的智能生态系统，其中人类作为共享智能的不可或缺的参与者。该愿景旨在最大化模型证据，通过不同尺度上的信念更新来累积关于自身感知世界的证据。

    

    本白皮书提出了人工智能领域未来十年（甚至更久）的研究和开发愿景。其中，核心是构建一个自然与合成感知的协同智能生态系统，将人类作为不可或缺的参与者 - 我们称之为“共享智能”。该愿景基于主动推理，一种自适应行为的形式，可以被视为智能的物理学，并继承了自组织物理学的特征。在这个背景下，我们将智能理解为累积关于自己感知世界的生成模型的证据的能力 - 也被称为自证明。在形式上，这对应于通过在多个尺度上进行信念更新来最大化（贝叶斯）模型证据，包括推理、学习和模型选择。在操作上，这种自证明可以通过对因子图进行（变分）消息传递或信念传播来实现。主动推理的关键是突出了存在的紧迫性。

    This white paper lays out a vision of research and development in the field of artificial intelligence for the next decade (and beyond). Its denouement is a cyber-physical ecosystem of natural and synthetic sense-making, in which humans are integral participants -- what we call ''shared intelligence''. This vision is premised on active inference, a formulation of adaptive behavior that can be read as a physics of intelligence, and which inherits from the physics of self-organization. In this context, we understand intelligence as the capacity to accumulate evidence for a generative model of one's sensed world -also known as self-evidencing. Formally, this corresponds to maximizing (Bayesian) model evidence, via belief updating over several scales: i.e., inference, learning, and model selection. Operationally, this self-evidencing can be realized via (variational) message passing or belief propagation on a factor graph. Crucially, active inference foregrounds an existential imperative
    
[^146]: 可扩展的分层无线联邦学习算法

    Scalable Hierarchical Over-the-Air Federated Learning. (arXiv:2211.16162v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2211.16162](http://arxiv.org/abs/2211.16162)

    本研究提出了一种针对分布式环境的通信高效的分层联邦学习算法，通过使用可扩展的无线聚合方案和带宽有限的广播方案，解决了设备干扰和边缘服务器干扰的问题。

    

    本研究提出了一种针对包含核心服务器和多个边缘服务器及设备集群的分布式环境的通信高效的分层联邦学习算法。假设不同的学习任务，具有相同任务的集群进行协作。为了在无线链路上实现算法，我们提出了一种可扩展的分簇无线聚合方案，用于上行链路，同时采用带宽有限的广播方案用于下行链路，每个算法迭代只需要一个资源块，不受边缘服务器和设备数量的影响。这种设置面临着上行链路设备干扰和下行链路边缘服务器干扰的问题，需要进行严格的建模。我们首先通过将设备建模为一个泊松集群过程，在设置中建立了一个空间模型，并对由干扰引起的上行链路和下行链路的误差进行量化。然后，我们提出了一种全面的数学方法来推导收敛性。

    In this work, we propose a communication-efficient hierarchical federated learning algorithm for distributed setups including core servers and multiple edge servers with clusters of devices. Assuming different learning tasks, clusters with a same task collaborate. To implement the algorithm over wireless links, we propose a scalable clustered over-the-air aggregation scheme for the uplink with a bandwidth-limited broadcast scheme for the downlink that requires only a single resource block for each algorithm iteration, independent of the number of edge servers and devices. This setup is faced with interference of devices in the uplink and interference of edge servers in the downlink that are to be modeled rigorously. We first develop a spatial model for the setup by modeling devices as a Poisson cluster process over the edge servers and quantify uplink and downlink error terms due to the interference. Accordingly, we present a comprehensive mathematical approach to derive the convergenc
    
[^147]: CP-PINNs: 使用物理知识神经网络和总变差惩罚进行PDE中的变点检测

    CP-PINNs: Changepoints Detection in PDEs using Physics Informed Neural Networks with Total-Variation Penalty. (arXiv:2208.08626v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.08626](http://arxiv.org/abs/2208.08626)

    本文提出了一种新的CP-PINNs模型，通过将PINNs与总变差惩罚相结合，实现了准确的变点检测和PDE的发现。我们还开发了一种元学习算法，能够在数据的连续批次上动态改进优化目标。实证结果表明，在存在变点的情况下，该方法能够准确估计参数和模型对齐，在没有变点的情况下能够数值上收敛到原始PINNs模型的解。

    

    本文展示了在参数中存在未知变点的情况下，物理知识神经网络（PINNs）可能无法正确估计偏微分方程（PDE）的动态过程。为了解决这个问题，我们提出了一个新的CP-PINNs模型，将PINNs与总变差惩罚相结合，用于准确的变点检测和PDE的发现。为了在模型拟合、PDE发现和变点检测任务之间进行最优组合，我们开发了一种新的元学习算法，利用批量学习在数据的连续批次上动态改进优化目标。在实证方面，在动态过程中存在变点的情况下，我们的方法能够准确估计参数和模型对齐，在数据中没有变点的情况下，数值上收敛到原始PINNs模型的解。

    The paper shows that Physics-Informed Neural Networks (PINNs) can fail to estimate the correct Partial Differential Equations (PDEs) dynamics in cases of unknown changepoints in the parameters. To address this, we propose a new CP-PINNs model which integrates PINNs with Total-Variation penalty for accurate changepoints detection and PDEs discovery. In order to optimally combine the tasks of model fitting, PDEs discovery, and changepoints detection, we develop a new meta-learning algorithm that exploits batch learning to dynamically refines the optimization objective when moving over the consecutive batches of the data. Empirically, in case of changepoints in the dynamics, our approach demonstrates accurate parameter estimation and model alignment, and in case of no changepoints in the data, it converges numerically to the solution from the original PINNs model.
    

