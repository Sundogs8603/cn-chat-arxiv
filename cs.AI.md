# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions](https://rss.arxiv.org/abs/2312.15101) | 本文提出了一种自动化的故障定位和修复方法Fix-Con，用于在深度学习模型转换过程中修复由转换引入的故障。Fix-Con能够检测和修复模型输入、参数、超参数和模型图方面的故障，提高转换模型的部署和预测正确性。 |
| [^2] | [SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models](https://arxiv.org/abs/2403.17933) | SLEDGE是第一个使用生成模型训练的车辆运动规划生成模拟器，引入了新颖的栅格到矢量自编码器（RVAE）以及Diffusion Transformer来生成智能体和车道图，从而实现更好的模拟控制。 |
| [^3] | [MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution](https://arxiv.org/abs/2403.17927) | 提出了MAGIS框架，基于LLM构建，包括四种定制的Agent，能够协作规划和编码过程，解锁LLMs解决GitHub问题的潜力。 |
| [^4] | [AID: Attention Interpolation of Text-to-Image Diffusion](https://arxiv.org/abs/2403.17924) | 提出了一种新颖的无训练技术，即Attention Interpolation via Diffusion (AID)，通过内/外插值注意力层、插值注意力与自注意力融合以提高保真度，以及应用贝塔分布进行选择以增加平滑度来改进文本到图像插值的问题。 |
| [^5] | [LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning](https://arxiv.org/abs/2403.17919) | 逐层重要性采样的新方法LISA在微调任务中表现出色，记忆成本低且优于传统方法。 |
| [^6] | [AgentStudio: A Toolkit for Building General Virtual Agents](https://arxiv.org/abs/2403.17918) | AgentStudio是一个用于构建通用虚拟代理的工具包，提供了现实、多模态的开发工具，支持整个代理开发生命周期，并具有高度通用的观测和动作空间，以及图形用户界面，可以在真实环境中有效开发数据集和基准。 |
| [^7] | [CMP: Cooperative Motion Prediction with Multi-Agent Communication](https://arxiv.org/abs/2403.17916) | 该论文提出了一种名为CMP的方法，利用LiDAR信号作为输入，通过合作感知和运动预测模块共享信息，解决了合作运动预测的问题。 |
| [^8] | [Hierarchical Multi-label Classification for Fine-level Event Extraction from Aviation Accident Reports](https://arxiv.org/abs/2403.17914) | 本文提出将事件提取问题视为分层分类任务，利用分层注意力模块结合BERT，实现了先识别粗粒度信息再预测细粒度信息的自动事件识别方法。 |
| [^9] | [Image-based Novel Fault Detection with Deep Learning Classifiers using Hierarchical Labels](https://arxiv.org/abs/2403.17891) | 本研究提出了一种在深度学习分类器中利用分层标签提高未知故障检测性能的方法，同时不影响模型性能，在热轧钢过程中的实验结果表明其具有较好的可复制性。 |
| [^10] | [Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach](https://arxiv.org/abs/2403.17873) | 本研究通过将ST框架扩展到大型语言模型，特别是在心理健康等敏感领域，以解决社会误归属性的风险，并提出通过增加第五个“W问题”的方式来增强该框架。 |
| [^11] | [Climate Downscaling: A Deep-Learning Based Super-resolution Model of Precipitation Data with Attention Block and Skip Connections](https://arxiv.org/abs/2403.17847) | 本研究提出了基于深度学习的气候细化超分辨率模型，结合了注意力块和跳跃连接，旨在有效预测降水数据，为缓解气候变化带来的影响和提高水资源利用效率做出贡献。 |
| [^12] | [Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation](https://arxiv.org/abs/2403.17846) | 提出了一种用于语言驱动的机器人导航的分层开放词汇3D场景图映射方法，可以有效代表多层建筑并允许机器人在其中穿行。 |
| [^13] | [ReMamber: Referring Image Segmentation with Mamba Twister](https://arxiv.org/abs/2403.17839) | 提出了ReMamber，一种整合了Mamba和多模态Mamba Twister块的新型RIS架构，通过其独特的通道和空间扭曲机制实现图像-文本交互，取得了三个基准测试的最新技术成果 |
| [^14] | [DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions](https://arxiv.org/abs/2403.17827) | 提出了一种从文本描述和物体几何形状中合成逼真的手-物体交互的方法，通过三种技术实现了有效学习，包括任务分解、紧密耦合的姿势表示和不同的引导方案。 |
| [^15] | [On the Computational Complexity of Stackelberg Planning and Meta-Operator Verification: Technical Report](https://arxiv.org/abs/2403.17826) | Stackelberg规划在理论上的复杂性分析显示，一般情况下并不比经典规划更难，但在多项式计划长度限制下，会提升到多项式复杂性层次结构中更高一级 |
| [^16] | [Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs)](https://arxiv.org/abs/2403.17819) | 本文展示了大型语言模型（LLMs）在加快频谱监管流程中的示例应用，探讨了LLMs在此背景下可以发挥的各种作用，同时识别了需要解决的一些挑战，并通过适当的实验、实用案例和见解突出了LLMs在频谱管理中的转变潜力。 |
| [^17] | [D-PAD: Deep-Shallow Multi-Frequency Patterns Disentangling for Time Series Forecasting](https://arxiv.org/abs/2403.17814) | D-PAD 提出了一个深浅层多频模式解缠神经网络，通过多分量分解和分解-重构-分解模块，有效解缠时间序列中的复杂时间模式。 |
| [^18] | [Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications](https://arxiv.org/abs/2403.17787) | 评估在基于图像的安全应用中，即时工程的Gemini-pro大型多模态模型与微调的Vision Transformer模型的有效性，并发现在关键安全挑战中表现出明显的性能差异。 |
| [^19] | [SciCapenter: Supporting Caption Composition for Scientific Figures with Machine-Generated Captions and Ratings](https://arxiv.org/abs/2403.17784) | SciCapenter 是一个交互式系统，利用尖端的AI技术为科学图像生成多样的标题，提供评分和综合检查表，大大降低了标题撰写的认知负荷。 |
| [^20] | [Towards a FAIR Documentation of Workflows and Models in Applied Mathematics](https://arxiv.org/abs/2403.17778) | MaRDI开发了一个FAIR和可机器解释的模板，用于全面文档化应用数学中的建模-仿真-优化工作流程，并展示了MaRDMO和MathModDB知识图之间的交互作用。 |
| [^21] | [SciNews: From Scholarly Complexities to Public Narratives -- A Dataset for Scientific News Report Generation](https://arxiv.org/abs/2403.17768) | 科学新闻报道生成的自动化提高了学术见解的可访问性，该研究提出了一个包含学术出版物和相应科学新闻报道的数据集，用于探索自动生成科学新闻报道的可能性。 |
| [^22] | [DataCook: Crafting Anti-Adversarial Examples for Healthcare Data Copyright Protection](https://arxiv.org/abs/2403.17755) | DataCook是一个针对医疗保健数据版权保护的新方法，通过制作反对抗样本，使版权持有人能够在部署阶段控制模型的授权。 |
| [^23] | [Using Stratified Sampling to Improve LIME Image Explanations](https://arxiv.org/abs/2403.17742) | 研究提出了使用分层抽样方法改进LIME图像解释，以减少典型蒙特卡洛抽样产生的人工产物，实验证明了该方法的有效性。 |
| [^24] | [All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction](https://arxiv.org/abs/2403.17740) | 提出了异质交互评分网络（HIRE）框架，通过异质交互模块（HIM）来共同建模异质交互并直接推断重要特征 |
| [^25] | [Out-of-distribution Rumor Detection via Test-Time Adaptation](https://arxiv.org/abs/2403.17735) | 提出了一种名为测试时适应的方法，用于在分布转移下进行谣言检测，通过建模传播图和构建测试时适应框架，增强模型的适应性和鲁棒性。 |
| [^26] | [Tiny Models are the Computational Saver for Large Models](https://arxiv.org/abs/2403.17726) | TinySaver是一种动态模型压缩方法，通过使用小型模型来自适应地替换大型模型，从而提高计算效率。 |
| [^27] | [Optimization-based Prompt Injection Attack to LLM-as-a-Judge](https://arxiv.org/abs/2403.17710) | 介绍了一种基于优化的提示注入攻击方法，JudgeDeceiver，针对LLM-as-a-Judge，通过自动化生成对抗序列实现了有针对性和高效的模型评估操控。 |
| [^28] | [Enhanced Short Text Modeling: Leveraging Large Language Models for Topic Refinement](https://arxiv.org/abs/2403.17706) | 利用大型语言模型的先进能力，提出了一种名为“主题细化”的新方法，通过引入提示工程和消除离题词等方式改进短文本的主题建模质量，提高了主题的语义质量。 |
| [^29] | [MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation](https://arxiv.org/abs/2403.17698) | 提出了MEP方法，通过结合不同核函数生成偏差来解决变压器模型在长度外推时的准确性降低问题 |
| [^30] | [ExpressEdit: Video Editing with Natural Language and Sketching](https://arxiv.org/abs/2403.17693) | ExpressEdit系统探索了如何利用自然语言和草图支持视频编辑人员表达视频编辑想法，并提出了一种多模态方法来简化视频编辑过程。 |
| [^31] | [Solution for Emotion Prediction Competition of Workshop on Emotionally and Culturally Intelligent AI](https://arxiv.org/abs/2403.17683) | 提出了一种名为ECSP的单多模态情感文化特定提示方法，旨在通过使用单一模态消息增强多模态模型性能，并设计良好的提示来减少文化差异问题。 |
| [^32] | [Onboard deep lossless and near-lossless predictive coding of hyperspectral images with line-based attention](https://arxiv.org/abs/2403.17677) | 本文提出了一种在航天器上压缩高光谱图像的新方法，通过设计递归的预测神经网络LineRWKV，结合了Transformer和递归神经网络的优势，并在实验中取得了优异表现 |
| [^33] | [Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games](https://arxiv.org/abs/2403.17674) | 将预训练的强化学习代理指导的LLM代理SecurityBot用于网络安全操作，实现显著性能改进。 |
| [^34] | [Language Models for Text Classification: Is In-Context Learning Enough?](https://arxiv.org/abs/2403.17661) | 本研究通过对16个文本分类数据集的大规模评估研究，填补了现有研究缺乏对文本生成模型与提示技术与更传统的文本分类方法之间比较的理解。 |
| [^35] | [SGHormer: An Energy-Saving Graph Transformer Driven by Spikes](https://arxiv.org/abs/2403.17656) | SGHormer是一种由脉冲驱动的节能图变换器，通过将全精度嵌入转换为稀疏和二值化脉冲以减少内存和计算成本，提高了图变换器的效率。 |
| [^36] | [An Extension-based Approach for Computing and Verifying Preferences in Abstract Argumentation](https://arxiv.org/abs/2403.17653) | 提出了一种基于扩展的方法，用于计算和验证抽象论证系统中的偏好。 |
| [^37] | [S+t-SNE - Bringing dimensionality reduction to data streams](https://arxiv.org/abs/2403.17643) | S+t-SNE是t-SNE算法的改进版本，在处理数据流时具有增量更新和盲目漂移管理的特点，能够实现高效的降维和信息可视化。 |
| [^38] | [PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning](https://arxiv.org/abs/2403.17637) | 引入了 PeersimGym 环境，通过强化学习解决任务卸载问题，支持定制化仿真环境，有助于开发和优化计算网络中的任务卸载策略。 |
| [^39] | [UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps](https://arxiv.org/abs/2403.17633) | UADA3D是一种无监督对抗领域自适应方法，能够在3D物体检测中处理稀疏LiDAR数据和大领域差距，并在自动驾驶汽车和移动机器人领域中表现出显著的改进。 |
| [^40] | [Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset](https://arxiv.org/abs/2403.17632) | 提出了一个专门针对电动微移动工具在都柏林收集的开放数据集，为解决实际场景中能耗建模的困难提供了重要资源 |
| [^41] | [Denoising Table-Text Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2403.17611) | 本文提出了一种Denosied Table-Text Retriever（DoTTeR）方法，通过利用去噪训练数据集和整合表级排名信息，解决了表格-文本开放领域问答中存在的假正标签影响和跨表格推理问题的挑战。 |
| [^42] | [Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs](https://arxiv.org/abs/2403.17607) | 通过全融合操作、最小化全局内存访问以及最大化数据重用，本文提出了在英特尔数据中心GPU上的多层感知机实现，显著提高了性能，尤其在推理方面表现出色。 |
| [^43] | [LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation](https://arxiv.org/abs/2403.17601) | 提出了一种学习者感知的监督模仿学习方法，通过变分自动编码器增强专家状态，以解决多智体模仿学习中的协变量偏移问题 |
| [^44] | [Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models](https://arxiv.org/abs/2403.17589) | 提出了双存储网络的多功能适应方法，能在零次适应、少次适应和无需训练的少次适应三种设置下高效运行 |
| [^45] | [Parameterized Analysis of Bribery in Challenge the Champ Tournaments](https://arxiv.org/abs/2403.17587) | 研究了在“挑战冠军”比赛中贿赂的参数化分析问题，探讨通过贿赂其他玩家来增加最初冠军赢得比赛的概率，并在限定预算下进行优化。研究发现在以玩家数量为参数时，这一问题是弱NP困难且W[1]-hard的。 |
| [^46] | [Towards a Zero-Data, Controllable, Adaptive Dialog System](https://arxiv.org/abs/2403.17582) | 该论文提出了一种从对话树生成数据的方法，可帮助训练出在合成数据上训练的代理达到与在人类数据上训练的模型相媲美的对话成功率。 |
| [^47] | [m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt](https://arxiv.org/abs/2403.17556) | 通过引入视觉上下文作为通用的语言无关表示，该论文提出了一种利用多模态提示来指导多模态多语言神经机器翻译的框架，以实现对不同语言表示的对齐，并生成条件视觉-语言记忆进行翻译。 |
| [^48] | [Practical Applications of Advanced Cloud Services and Generative AI Systems in Medical Image Analysis](https://arxiv.org/abs/2403.17549) | 生成式人工智能在医学影像中的转变潜力，能够生成合成数据、增强图像、帮助异常检测和促进图像转换。 |
| [^49] | [VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts](https://arxiv.org/abs/2403.17542) | 通过价值差异和状态计数，利用代理的内部状态来决定何时进行探索，解决了盲目切换机制的缺点。 |
| [^50] | [KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion](https://arxiv.org/abs/2403.17532) | KC-GenRe是一种基于大型语言模型的知识约束生成式重新排序方法，用于KGC，通过将KGC重新排序任务构建为候选标识符排序生成问题以及开发知识引导方法来解决重新排序过程中的不匹配和错序问题。 |
| [^51] | [Boosting Few-Shot Learning with Disentangled Self-Supervised Learning and Meta-Learning for Medical Image Classification](https://arxiv.org/abs/2403.17530) | 通过解缠的自监督学习和元学习，在低数据环境中训练医学图像分类模型，提高了模型的性能和泛化能力。 |
| [^52] | [Equipping Sketch Patches with Context-Aware Positional Encoding for Graphic Sketch Representation](https://arxiv.org/abs/2403.17525) | 提出了一种通过为素描补丁配备上下文感知的位置编码来保护不同绘图版本的方法，将绘图顺序信息嵌入图节点中，以更好地学习图形素描表示。 |
| [^53] | [MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities](https://arxiv.org/abs/2403.17516) | 本研究提出了一种直接比较预测文本嵌入的脑活动映射来指导文本重建的简单而有效方法，相比之前的间接方法显著提高了模型性能。 |
| [^54] | [Prediction-sharing During Training and Inference](https://arxiv.org/abs/2403.17515) | 研究了在训练和推理期间的预测分享方面的新颖之处，并介绍和突出了不同类型的数据共享合同。 |
| [^55] | [Natural Language Requirements Testability Measurement Based on Requirement Smells](https://arxiv.org/abs/2403.17479) | 通过提出一个基于需求异味的数学模型，本文介绍了一种评估和排名自然语言需求可测试性的方法，有助于衡量和量化需求的可测试性。 |
| [^56] | [A Unified Kernel for Neural Network Learning](https://arxiv.org/abs/2403.17467) | 本文提出了统一神经内核(UNK)，可以描述神经网络的学习动态，并在有限的学习步骤下表现出类似于NTK的行为，当学习步骤逼近无穷大时收敛到NNGP。 |
| [^57] | [LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection](https://arxiv.org/abs/2403.17465) | LaRE^2 提出了一种基于潜在重构误差的方法用于检测扩散生成的图像，通过引入潜在重构误差（LaRE）和误差引导特征细化模块（EGRE）实现了对特征的有效提取和增强，从而区分真实和生成图像。 |
| [^58] | [Imitating Cost-Constrained Behaviors in Reinforcement Learning](https://arxiv.org/abs/2403.17456) | 该论文介绍了在强化学习中模仿受成本约束的行为的重要性，提出了模仿学习在受约束设置下的应用，并探讨了在实际领域中专家行为受限制因素影响的问题。 |
| [^59] | [Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model](https://arxiv.org/abs/2403.17445) | 将简单的指数平滑法与MLP结合，通过增加参数和复杂性，实现了与复杂S4模型可比较的结果 |
| [^60] | [Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization](https://arxiv.org/abs/2403.17428) | 探讨了利用大型语言模型增强精神科访谈的方法，通过分析朝鲜叛逃者的咨询数据，研究LLMs在划分症状和总结压力因素和症状方面取得高性能。 |
| [^61] | [Knowledge-Powered Recommendation for an Improved Diet Water Footprint](https://arxiv.org/abs/2403.17426) | 该研究提出了一种基于知识图谱的推荐系统，旨在推广更健康的饮食习惯，减少环境影响，特别是水足迹。 |
| [^62] | [MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification](https://arxiv.org/abs/2403.17421) | 引入了基于多智能体强化学习的MA4DIV方法，将搜索结果多样化建模为多个智能体之间的合作任务，直接优化多样性指标，如$\alpha$-NDCG，以实现高训练效率。 |
| [^63] | [AI Safety: Necessary, but insufficient and possibly problematic](https://arxiv.org/abs/2403.17419) | AI安全性炒作存在问题，可能会导致不利的监管努力并使有害AI合法化。 |
| [^64] | [On permutation-invariant neural networks](https://arxiv.org/abs/2403.17410) | 神经网络如Deep Sets和Transformers的出现显著推动了基于集合的数据处理的进展 |
| [^65] | [Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens](https://arxiv.org/abs/2403.17407) | 通过引入区域指导标记技术，本文提出了一种将孟加拉文本与地方方言转录为国际音标的方法，为模型提供了关于输入文本的地区方言信息，以理解与每个地区相关的独特音韵模式。 |
| [^66] | [An Open-source End-to-End Logic Optimization Framework for Large-scale Boolean Network with Reinforcement Learning](https://arxiv.org/abs/2403.17395) | 提出了一个开源的端到端逻辑优化框架，用于大规模布尔网络，并应用了强化学习。 |
| [^67] | [ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition](https://arxiv.org/abs/2403.17385) | ELLEN是一种简单而强大的神经符号方法，将微调语言模型与语言规则相结合，在极其轻监督的情况下取得了非常强劲的命名实体识别性能。 |
| [^68] | [Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation](https://arxiv.org/abs/2403.17384) | 研究利用图神经网络和可解释性方法分析观测在大气状态估计中的影响，通过对不同观测的重要性进行量化，提高了对天气预报中观测数据的理解和优化。 |
| [^69] | [Application-Driven Innovation in Machine Learning](https://arxiv.org/abs/2403.17381) | 应用驱动研究在机器学习领域具有重要影响，可以与方法驱动研究有益地协同，但目前审查、招聘和教学实践往往阻碍了这种创新。 |
| [^70] | [Exploring and Applying Audio-Based Sentiment Analysis in Music](https://arxiv.org/abs/2403.17379) | 本文探讨了基于音频的情感分析在音乐中的运用，通过预测音乐片段随时间的情感变化以及确定音乐时间序列中下一个情感值来实现无缝过渡。 |
| [^71] | [Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance](https://arxiv.org/abs/2403.17377) | 提出了一种名为扰动注意力引导（PAG）的新型抽样引导技术，通过在扩散 U-Net 中替换自注意力映射来生成结构降级的中间样本，从而在无条件和有条件设置下改善扩散样本质量。 |
| [^72] | [AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving](https://arxiv.org/abs/2403.17373) | 提出了使用自动数据引擎（AIDE）的方法，通过自动识别问题、高效筛选数据、自动标注改进模型、生成多样化场景验证模型，在自动驾驶领域取得了优异的性能。 |
| [^73] | [ChatGPT Rates Natural Language Explanation Quality Like Humans: But on Which Scales?](https://arxiv.org/abs/2403.17368) | 本研究探索了ChatGPT在不同尺度下与人类评估之间的一致性，并发现在较粗粒度的尺度上，ChatGPT与人类更加一致。 |
| [^74] | [Bridging Textual and Tabular Worlds for Fact Verification: A Lightweight, Attention-Based Model](https://arxiv.org/abs/2403.17361) | 提出了一种简单而强大的模型，通过利用预训练模型和轻量级的基于注意力的机制，有效地利用不同数据类型之间的潜在连接，保留原始证据的上下文，确保预测准确性。 |
| [^75] | [Addressing Myopic Constrained POMDP Planning with Recursive Dual Ascent](https://arxiv.org/abs/2403.17358) | 引入历史依赖性双变量，并通过递归双向上升进行优化，在大型受限部分可观察马尔可夫决策过程中改进了探索能力和结果安全性。 |
| [^76] | [MESIA: Understanding and Leveraging Supplementary Nature of Method-level Comments for Automatic Comment Generation](https://arxiv.org/abs/2403.17357) | 提出了一种新的度量标准MESIA，用于评估代码注释提供的补充信息的程度，实验证明了其在生成方法级注释时的价值。 |
| [^77] | [The Solution of the Zodiac Killer's 340-Character Cipher](https://arxiv.org/abs/2403.17350) | 本文详细描述了Zodiac杀手第二个密码的解密过程，该密码使用了转位和同音替换密码，并具有不寻常特质。 |
| [^78] | [The Solution for the ICCV 2023 1st Scientific Figure Captioning Challenge](https://arxiv.org/abs/2403.17342) | 我们提出了一种根据论文内容生成图像标题的解决方案，并通过使用PaddleOCR和LLaMA工具包解决了OCR信息和信息过滤方面的问题。 |
| [^79] | [Reinforcement Learning-based Receding Horizon Control using Adaptive Control Barrier Functions for Safety-Critical Systems](https://arxiv.org/abs/2403.17338) | 提出了一种基于强化学习的滚动视野控制方法，利用自适应控制屏障函数，以解决安全关键系统中性能和可行性受影响的问题 |
| [^80] | [The Pursuit of Fairness in Artificial Intelligence Models: A Survey](https://arxiv.org/abs/2403.17333) | 人工智能模型中的公平追求至关重要，研究人员努力解决偏见问题，确保模型不会有意或无意地对某些群体产生偏见。 |
| [^81] | [Deep Support Vectors](https://arxiv.org/abs/2403.17329) | 该论文探索了深度学习模型中的深度支持向量（DSVs）的概念，介绍了DeepKKT条件，通过实证研究发现DSVs与SVM中的支持向量类似，为解释模型决策标准提供了方法，同时证明了可以有效地使用DSVs重构模型。 |
| [^82] | [Learning Traffic Signal Control via Genetic Programming](https://arxiv.org/abs/2403.17328) | 提出了一种新的基于学习的方法用于解决复杂交叉路口的信号控制问题，通过设计阶段紧急性概念和可解释的树结构，可以在信号转换期间选择激活的信号相位。 |
| [^83] | [JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset](https://arxiv.org/abs/2403.17319) | JMultiWOZ是第一个日语大规模多领域任务驱动对话数据集，通过评估与现有英语基准数据集相媲美的对话状态跟踪和回复生成能力，推动了日语任务驱动对话系统的研究与发展。 |
| [^84] | [ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching](https://arxiv.org/abs/2403.17312) | 提出了ALISA，一种通过稀疏感知KV缓存加速大型语言模型推理的新算法系统设计。 |
| [^85] | [Neural Multimodal Topic Modeling: A Comprehensive Evaluation](https://arxiv.org/abs/2403.17308) | 该论文对包含文本和图片的文档的多模态主题建模进行了全面评估，并提出了两种新颖的主题建模解决方案和两种新颖的评估指标，结果显示这些模型均能产生连贯且多样化的主题。 |
| [^86] | [Visual Hallucination: Definition, Quantification, and Prescriptive Remediations](https://arxiv.org/abs/2403.17306) | 本文提出了关于视觉-语言模型中幻觉的细致讨论，对幻觉进行了量化，并提供了一个新的公开数据集VHILT，有助于研究此问题。 |
| [^87] | [InternLM2 Technical Report](https://arxiv.org/abs/2403.17297) | InternLM2是一个开源的大语言模型，在全面评估、长文本建模以及创新的预训练和优化技术下表现出色，超越了其前任模型。 |
| [^88] | [Exploring CausalWorld: Enhancing robotic manipulation via knowledge transfer and curriculum learning](https://arxiv.org/abs/2403.17266) | 通过知识转移和课程学习，本研究在三指机械臂操纵任务中采用强化学习，实现了提高学习效率和有效性的目标 |
| [^89] | [DASA: Delay-Adaptive Multi-Agent Stochastic Approximation](https://arxiv.org/abs/2403.17247) | DASA算法是第一个收敛速度仅依赖于混合时间和平均延迟的算法，同时在马尔科夫采样下实现N倍的收敛加速。 |
| [^90] | [TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models](https://arxiv.org/abs/2403.17246) | 该论文将经典规划和大型语言模型相结合，通过近似人类直觉，以实现多智能体任务规划。 |
| [^91] | [DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion](https://arxiv.org/abs/2403.17237) | DreamPolisher通过两阶段高斯飞溅方法实现了文本到三维生成的高质量，强调了视图一致性和纹理丰富度。 |
| [^92] | [Speeding Up Path Planning via Reinforcement Learning in MCTS for Automated Parking](https://arxiv.org/abs/2403.17234) | 本文提出了一种将强化学习与蒙特卡洛树搜索集成的方法，用于自动停车任务中的在线路径规划，旨在加速路径规划过程，提高效率。 |
| [^93] | [Uncertainty Quantification for Gradient-based Explanations in Neural Networks](https://arxiv.org/abs/2403.17224) | 本文提出了一种结合不确定性估计方法和解释方法来确定神经网络解释不确定性的流程，通过计算解释分布的变异系数，评估了解释的置信度并确定Guided Backpropagation方法生成的解释具有较低的不确定性。 |
| [^94] | [Co-Occurring of Object Detection and Identification towards unlabeled object discovery](https://arxiv.org/abs/2403.17223) | 通过提出的深度学习方法，可以识别多标签物体类别中基础物体的共存物体，并通过共存矩阵分析生成频繁模式，从而实现未标记物体的发现。 |
| [^95] | [SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies](https://arxiv.org/abs/2403.17219) | SeSaMe框架利用大型语言模型为心理健康研究中的参与者模拟自我报告，减轻了他们的负担 |
| [^96] | [DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face Reenactment](https://arxiv.org/abs/2403.17217) | DiffusionAct是一种利用扩散模型进行神经人脸再现的新方法，能够编辑输入图像的面部姿势，实现身份和外观的保留，以及目标头部姿势和面部表情的转移。 |
| [^97] | [Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation](https://arxiv.org/abs/2403.17214) | 本研究实证分析了11种流行的专门调整的大型语言模型在五种语言上生成的输出，发现其中26.4%到73.7%的代码翻译需要后处理。 |
| [^98] | [Sanity Checks for Explanation Uncertainty](https://arxiv.org/abs/2403.17212) | 本文提出了解释不确定性方法的合理性检查，可以快速测试不确定性和解释方法的组合。 |
| [^99] | [CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions](https://arxiv.org/abs/2403.17210) | 通过CADGL框架，利用上下文感知深度图学习来预测药物-药物相互作用，解决了现有DDI预测模型在泛化、特征提取和现实应用方面的挑战 |
| [^100] | [Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node](https://arxiv.org/abs/2403.17209) | 通过大型语言模型代理生成AAS实例模型，实现了在数字孪生中的互操作性，降低了手动创建成本和时间。 |
| [^101] | [NUMTEMP: A real-world benchmark to verify claims with statistical and temporal expressions](https://arxiv.org/abs/2403.17169) | NUMTEMP是一个真实世界基准，专注于验证复杂的数字论点，量化了现有解决方案的局限性，并提供了一种解决真实世界数字论点验证挑战的方法。 |
| [^102] | [Multi-Objective Quality-Diversity for Crystal Structure Prediction](https://arxiv.org/abs/2403.17164) | 本研究利用质量多样性算法为晶体结构预测打开了一条新途径，旨在发现具有多样特征的高性能解决方案集合，可以优化晶体结构稳定性以及其他目标如磁性或热电效率。 |
| [^103] | [Less Is More - On the Importance of Sparsification for Transformers and Graph Neural Networks for TSP](https://arxiv.org/abs/2403.17159) | 我们提出了一种数据预处理方法，通过稀疏化TSP图表示和注意力掩码，使编码器集中于TSP实例的关键部分，同时允许信息在所有节点之间自由流动。 |
| [^104] | [Hearing the shape of an arena with spectral swarm robotics](https://arxiv.org/abs/2403.17147) | 频谱群体机器人可以通过模拟信息扩散来重建竞技场的几何形状 |
| [^105] | [MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models](https://arxiv.org/abs/2403.17141) | MetaAligner是第一个与策略无关且通用的多目标偏好对齐方法，通过将参数更新与策略模型解耦实现即插即用的对齐，并通过上下文学习实现未见目标的零冷启动偏好对齐 |
| [^106] | [RepairAgent: An Autonomous, LLM-Based Agent for Program Repair](https://arxiv.org/abs/2403.17134) | RepairAgent是首个通过基于大型语言模型的自主代理程序来解决程序修复挑战的工作，其关键贡献在于提供了一组有助于程序修复的工具以及动态更新的提示格式。 |
| [^107] | [Exploring the potential of prototype-based soft-labels data distillation for imbalanced data classification](https://arxiv.org/abs/2403.17130) | 本文探索了一种先前在少于一次学习中提出的简单蒸馏技术在原型软标签蒸馏中的潜力，旨在通过在蒸馏过程中整合优化步骤，进一步提高分类准确性。 |
| [^108] | [The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition](https://arxiv.org/abs/2403.17125) | 大型语言模型在执行任务时依赖背景知识（先验知识），但无法完全整合与任务先验知识相矛盾的信息，影响了情绪识别等主观任务的表现水平。 |
| [^109] | [Grounding Language Plans in Demonstrations Through Counterfactual Perturbations](https://arxiv.org/abs/2403.17124) | 这项工作通过使用LLMs来指导多步演示中隐含的任务结构和约束的搜索，以及通过反事实干扰获得更广泛的演示状态空间覆盖。 |
| [^110] | [Graph Protection under Multiple Simultaneous Attacks: A Heuristic Approach](https://arxiv.org/abs/2403.17108) | 本研究开发了一种有效的元启发式方法来保护网络中节点免受多个同时攻击，特别关注$k$-强罗马占领问题，该问题在图中分配部队数量以最小化总权重以满足保护约束条件。 |
| [^111] | [AI Consciousness is Inevitable: A Theoretical Computer Science Perspective](https://arxiv.org/abs/2403.17101) | 通过理论计算机科学的视角，作者提出了一个简单却强大的机器模型，支持了机器意识是不可避免的这一论断。 |
| [^112] | [Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data](https://arxiv.org/abs/2403.17091) | 研究提出了对于脱机策略评估任务，样本复杂度受聚合马尔科夫转换模型中的浓缩系数控制，而不是原始MDP中的系数。 |
| [^113] | [GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration](https://arxiv.org/abs/2403.17089) | 该研究提出了GOLF框架，通过目标导向和长期规划增强LLMs的能力，以协助用户处理重要的生活决策。 |
| [^114] | [A Study in Dataset Pruning for Image Super-Resolution](https://arxiv.org/abs/2403.17083) | 本研究针对图像超分辨率中数据集训练资源需求大的问题，提出了一种数据集修剪的解决方案，通过基于损失值的选择，将训练集缩减至原始数据集的50%，取得了令人满意的结果。 |
| [^115] | [Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions](https://arxiv.org/abs/2403.17064) | 通过识别CLIP文本嵌入中的语义方向，实现了文本到图像模型中对高级属性的细粒度主题特定控制。 |
| [^116] | [Enhancing Graph Representation Learning with Attention-Driven Spiking Neural Networks](https://arxiv.org/abs/2403.17040) | 本研究提出了一种将注意力机制与脉冲神经网络相结合的新方法，用于改善图表示学习，实验结果显示其在基准数据集上表现出可比较的性能。 |
| [^117] | [Evolution and Efficiency in Neural Architecture Search: Bridging the Gap Between Expert Design and Automated Optimization](https://arxiv.org/abs/2403.17012) | 本文综述了神经架构搜索（NAS）领域的进化历程，介绍了从手动设计到自动化优化的演变过程，探讨了NAS在各个领域的应用，以及针对计算效率挑战提出的高效NAS方法。 |
| [^118] | [SUDO: a framework for evaluating clinical artificial intelligence systems without ground-truth annotations](https://arxiv.org/abs/2403.17011) | SUDO框架允许在缺乏真实标注的情况下评估AI系统，通过为实际数据点分配临时标签并直接使用它们训练模型来解决分布转移问题，从而提高对临床数据的可信度。 |
| [^119] | [LLM Agent Operating System](https://arxiv.org/abs/2403.16971) | 提出了一种将大型语言模型嵌入操作系统中的LLM代理操作系统，旨在优化资源分配、促进代理间上下文切换、实现并发执行以及为代理提供工具服务。 |
| [^120] | [Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators](https://arxiv.org/abs/2403.16950) | 在大型语言模型评估中，通过引入成对偏好搜索方法PAIRS，成功解决了LLMs与人类判断不一致的问题，并取得了优于直接打分的最先进性能。 |
| [^121] | [Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models](https://arxiv.org/abs/2403.16915) | 本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调，在专题文档检索中显著改善了效果。 |
| [^122] | [Navigating the EU AI Act: A Methodological Approach to Compliance for Safety-critical Products](https://arxiv.org/abs/2403.16808) | 通过利用产品质量模型和合同法方法，本文提出了一种解释欧盟AI法案对高风险AI系统要求的方法论途径。 |
| [^123] | [As Good As A Coin Toss Human detection of AI-generated images, videos, audio, and audiovisual stimuli](https://arxiv.org/abs/2403.16760) | 通过一项感知研究，评估了人们在日常生活中对合成图像、音频、视频和音视频刺激与真实的区分能力，以探讨人类对欺骗性合成媒体的易受程度。 |
| [^124] | [CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment](https://arxiv.org/abs/2403.16649) | CLHA提出了一种简单而有效的对比学习框架，可以帮助大型语言模型与人类偏好对齐，通过新颖的重评分策略和损失函数调整，在提升对齐效果的同时简化了训练过程。 |
| [^125] | [DeepMachining: Online Prediction of Machining Errors of Lathe Machines](https://arxiv.org/abs/2403.16451) | DeepMachining是一种基于深度学习的AI系统，可以在线预测车床机床加工操作的误差，通过预训练和微调模型，实现了高准确性预测，是首批使用预训练深度学习模型预测车床机床加工误差的工厂实验之一。 |
| [^126] | [Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation](https://arxiv.org/abs/2403.16427) | Re2LLM是为基于会话的推荐提出的反射式强化大型语言模型，引导LLMs专注于更准确推荐所需的专业知识，实现有效和高效。 |
| [^127] | [Large Language Models in Biomedical and Health Informatics: A Bibliometric Review](https://arxiv.org/abs/2403.16303) | LLMs已成为生物医学与健康信息学中重要的工具，本文献计量学综述全面展示了LLMs在各种BHI领域中的应用，提出了其对自然语言处理应用的改进，揭示了主要发展趋势和研究网络，并讨论了伦理关切和实际挑战。 |
| [^128] | [Guessing human intentions to avoid dangerous situations in caregiving robots](https://arxiv.org/abs/2403.16291) | 本文探讨了在照料机器人中使用人工心智理论来猜测人类意图，提出了一种检测危险情况并实时消除危险的算法，在模拟实验中取得了高成功率。 |
| [^129] | [Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative Matrix Factorization](https://arxiv.org/abs/2403.16222) | 本文介绍了一种通过从科学论文中提取结构化本体来构建网络安全领域多模态知识图的方法 |
| [^130] | [Image Captioning in news report scenario](https://arxiv.org/abs/2403.16209) | 本论文探索了专门针对名人照片的图像描述，旨在增强新闻行业实践，并提出了对自动新闻内容生成的改进方法。 |
| [^131] | [Rumor Detection with a novel graph neural network approach](https://arxiv.org/abs/2403.16206) | 本论文提出了一种新颖的检测模型，同时学习用户相关性和信息传播的表示，以检测社交媒体上的谣言 |
| [^132] | [X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention](https://arxiv.org/abs/2403.15931) | 这里是中文总结出的一句话要点: 该论文提出了X-Portrait，一种用于生成具有表现力和时间连贯性的肖像动画的条件扩散模型，利用控制信号实现了细粒度头部姿势和表情控制，以提高运动精度。 |
| [^133] | [Towards a \textbf{RAG}-based Summarization Agent for the Electron-Ion Collider](https://arxiv.org/abs/2403.15729) | 开发了一种面向电子离子对撞机的基于RAG的摘要生成代理，能够压缩信息并引用相关回复，为合作者提供重大优势 |
| [^134] | [MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis](https://arxiv.org/abs/2403.15585) | MedPromptX是第一个将多模态大型语言模型、少样本提示和视觉基础相结合，用于胸部X线诊断的模型，通过补充缺失的EHR信息，有效解决了幻觉问题，但选择最佳少样本示例和高质量候选者仍有待解决。 |
| [^135] | [NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein Classification in Graph Neural Networks](https://arxiv.org/abs/2403.14736) | 提出了NaNa和MiGu两种语义数据增强方法，结合了蛋白质的主链化学和侧链生物物理信息，用于增强图神经网络中的蛋白质分类任务。 |
| [^136] | [Cyclical Log Annealing as a Learning Rate Scheduler](https://arxiv.org/abs/2403.14685) | 该论文介绍了一种新的对数方法作为学习率调度器，通过更积极的重启模式，可能使得在在线凸优化框架上使用更贪婪的算法，实验结果表明它性能类似于余弦退火方案。 |
| [^137] | [Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models](https://arxiv.org/abs/2403.14633) | 本文调查了大型语言模型中是否存在社会经济偏见，引入了一个新的数据集SilverSpoon，并评估了这种偏见的程度以及随着模型大小的变化。 |
| [^138] | [Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics](https://arxiv.org/abs/2403.14077) | 本研究探讨了使用多模态大型语言模型（LLMs）进行DeepFake检测的能力，通过实验证明它们能够揭示AI生成的图像，尽管LLMs并非专为媒体取证任务设计，这一发现具有重要意义。 |
| [^139] | [Threats, Attacks, and Defenses in Machine Unlearning: A Survey](https://arxiv.org/abs/2403.13682) | 机器遗忘（MU）通过知识去除过程来解决训练数据相关的人工智能治理问题，提高了AI系统的安全和负责任使用。 |
| [^140] | [Learning User Embeddings from Human Gaze for Personalised Saliency Prediction](https://arxiv.org/abs/2403.13653) | 通过眼动数据提取用户嵌入，可以有效个性化显著性预测任务，并且具有良好的泛化能力。 |
| [^141] | [Motion Generation from Fine-grained Textual Descriptions](https://arxiv.org/abs/2403.13518) | 本文提出了一种从细粒度文本描述中生成运动的方法，构建了FineHumanML3D数据集，设计了FineMotionDiffuse模型，实验结果表明该模型表现出色。 |
| [^142] | [Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity](https://arxiv.org/abs/2403.13374) | 通过提出新的Robust Average Gradient Algorithm（RAGA），本研究在联邦学习中解决了恶意拜占庭攻击和数据异构性的问题，实现了在非凸损失函数和异构数据集上的收敛性分析，并展示了RAGA的良好收敛性能。 |
| [^143] | [Hyacinth6B: A large language model for Traditional Chinese](https://arxiv.org/abs/2403.13334) | 为了解决大型语言模型通常存在的高硬件和计算需求，Hyacinth6B在模型轻量化和性能之间找到了平衡，采用LoRA方法进行参数高效微调。 |
| [^144] | [Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification](https://arxiv.org/abs/2403.12151) | 大型语言模型与知识图谱结合，提高零样本对象状态分类性能 |
| [^145] | [Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning](https://arxiv.org/abs/2403.11996) | 利用生成式人工智能和图算法加速科学发现，揭示论文之间的深入跨学科关系，并提出了新颖的材料设计。 |
| [^146] | [SelfIE: Self-Interpretation of Large Language Model Embeddings](https://arxiv.org/abs/2403.10949) | 提出了SelfIE框架，使大型语言模型能够自解释其嵌入，揭示内部推理，包括道德决策、提示注入和消除有害知识。 |
| [^147] | [Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction](https://arxiv.org/abs/2403.09963) | 本文调查了预训练语言模型在事实知识提取中存在的“提示偏见”，找到了不同类型提示的偏见程度，以及这种偏见对不同基准测试的影响，并提出了一种基于表示的方法来减轻这种提示偏见。 |
| [^148] | [Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt](https://arxiv.org/abs/2403.09857) | 提出了一个名为ASP的框架，通过注意力方面减少特定信息，鼓励任务不变的提示来捕获共享知识，并通过信息瓶颈学习目标从旧类到新类传递知识。 |
| [^149] | [Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation](https://arxiv.org/abs/2403.09738) | 大型语言模型作为生成式用户模拟器在对话推荐中展现出潜力，新的协议通过五个任务评估了语言模型模拟人类行为的准确程度，揭示了模型与人类行为的偏差，并提出了如何通过模型选择和提示策略减少这些偏差。 |
| [^150] | [Simple and Scalable Strategies to Continually Pre-train Large Language Models](https://arxiv.org/abs/2403.08763) | 通过简单和可扩展的学习率调整、重放数据的方法，可以在不重新训练的情况下，持续预训练大型语言模型以匹配完全重新训练时的性能。 |
| [^151] | [Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models](https://arxiv.org/abs/2403.08281) | 通过融合高度专业化的语言、代码和数学模型，提出了一种名为UltraFuser的融合框架，引入了标记级别的门控机制，并设计了两阶段训练策略，以同时在三个领域取得高性能。 |
| [^152] | [Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation](https://arxiv.org/abs/2403.06247) | 该方法利用文本信息生成类似输入图像的无缺陷数据图像，确保生成的图像符合期望分布，稳定且具有普适性。 |
| [^153] | [Medical Speech Symptoms Classification via Disentangled Representation](https://arxiv.org/abs/2403.05000) | 该论文提出了一种名为DRSC的医学言语分类模型，实现了自动学习从文本-声学数据中分离意图和内容表示以进行分类，并在检测25种不同医学症状时取得了95%的平均准确率。 |
| [^154] | [ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes](https://arxiv.org/abs/2403.04701) | 评估基于视觉的模型对于物体与背景之间多样化变化的鲁棒性，提出一种可以引入不同对象方面变化的方法 |
| [^155] | [Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents](https://arxiv.org/abs/2403.04202) | 在多代理环境中，研究人员探讨了不同道德类型的学习代理之间的互动，发现道德异质性可能对代理的共同发展产生影响。 |
| [^156] | [Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection](https://arxiv.org/abs/2403.02232) | 该研究通过机器学习技术全面评估了恶意软件检测，发现集成方法（如随机森林和XGBoost）相较于其他方法在恶意软件检测中表现出更高的准确性、精确度和召回率。 |
| [^157] | [Decode Neural signal as Speech](https://arxiv.org/abs/2403.01748) | 本文在脑机接口领域探索了MEG信号的脑到文本转换，着重解决了以前主要集中在EEG上、使用“teacher-forcing”以及未完全自回归的问题。 |
| [^158] | [Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling](https://arxiv.org/abs/2402.11800) | 延迟更新的随机逼近方案在时间变化有界延迟下，保证了每次迭代快速收敛到固定点周围的球体，界限依赖于最大延迟和混合时间。 |
| [^159] | [Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning](https://arxiv.org/abs/2402.11537) | 通过对五个主要类别的预训练数据的48个数据集进行系统分析，研究了它们对大型语言模型性能的影响，并发现了一些“高影响数据”，如书籍，与模型能力相关联，为LLMs的优化提供了见解。 |
| [^160] | [Tandem Transformers for Inference Efficient LLMs](https://arxiv.org/abs/2402.08644) | 该论文提出了一种新的架构，称为串联Transformer，用于解决传统大型语言模型推断速度限制的问题。该架构通过将小型自回归模型和大模型以块模式结合起来，并让小模型关注大模型的丰富表示，从而显著提高了小模型的预测准确性。实验证明，在预训练数据集上，串联的PaLM2-Bison和PaLM2-Gecko相比独立的PaLM2-Gecko，在下一个词元预测准确性上提高了3.3%，并且相较于具有相似下游任务的PaLM2-Otter模型，加速比达到1.16倍。 |
| [^161] | [SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM](https://arxiv.org/abs/2402.03246) | SGS-SLAM是一种基于三维高斯点云的语义稠密SLAM系统，通过多通道优化和关键帧优化，实现了高质量的重建和精确的语义分割。 |
| [^162] | [Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic Motivation Reinforcement Learning Algorithms for Improved Training and Adaptability](https://arxiv.org/abs/2401.18040) | 本研究旨在通过内在动机强化学习算法改进端到端多任务对话系统的训练和适应性。通过教授智能体一个内在奖励系统，可以加速训练并提高其判断行为质量的能力。 |
| [^163] | [Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?](https://arxiv.org/abs/2401.11911) | 该论文研究了大型语言模型如何合并生成和检索的上下文以提升开放领域问答，发现这些模型偏向于生成的上下文，即使它们提供了错误的信息。 |
| [^164] | [AI and Generative AI for Research Discovery and Summarization](https://arxiv.org/abs/2401.06795) | AI和生成式AI工具在研究发现和总结方面有重大影响，包括能够更快地找到相关文献和用简洁语言总结研究文章的要点。 |
| [^165] | [TagAlign: Improving Vision-Language Alignment with Multi-Tag Classification](https://arxiv.org/abs/2312.14149) | 提出了一种简单的方法，通过解析图像与文本中的对象和属性，使用多标签分类损失来改进视觉-语言对齐模型 |
| [^166] | [Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer](https://arxiv.org/abs/2312.12467) | 本文提出了一种使用分层网格结构的Hierarchical Contact Mesh Transformer（HCMT），能够学习长距离依赖关系，以处理灵活体动力学挑战。 |
| [^167] | [World Models via Policy-Guided Trajectory Diffusion](https://arxiv.org/abs/2312.08533) | 这项工作提出了一个新颖的世界建模方法，Policy-Guided Trajectory Diffusion (PolyGRAD)，通过扩散模型一次生成整个在线策略轨迹，避免了自回归模型中随着轨迹长度增长而积累的预测误差。 |
| [^168] | [High-throughput Biomedical Relation Extraction for Semi-Structured Web Articles Empowered by Large Language Models](https://arxiv.org/abs/2312.08274) | 利用大型语言模型在半结构化网络文章中实现高通量生物医学关系提取，通过对LLMs的应用，结合外部语料库和世界知识，设计针对性的二元分类决策，取得良好的结果。 |
| [^169] | [Artificial Neural Nets and the Representation of Human Concepts](https://arxiv.org/abs/2312.05337) | 人工神经网络可以学习人类和非人类的概念，但并不是用单个单元来表示这些概念 |
| [^170] | [DreamComposer: Controllable 3D Object Generation via Multi-View Conditions](https://arxiv.org/abs/2312.03611) | DreamComposer是一个灵活且可扩展的框架，通过注入多视角条件增强现有视角感知扩散模型，实现可控的3D对象生成。 |
| [^171] | [AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation](https://arxiv.org/abs/2312.02512) | 本文提出了AV2AV框架，实现了直接音视频语音到音视频语音的转换，能够在虚拟会议中进行真实对话，并改善口译语言翻译系统的鲁棒性。 |
| [^172] | [A Simple and Scalable Representation for Graph Generation](https://arxiv.org/abs/2312.02230) | 提出了一种名为GEEL的新型、简单且可扩展的图表示，可以显著降低邻接矩阵大小和词汇量，同时通过节点位置编码实现自回归生成，并针对属性图设计了新的扩展方案。 |
| [^173] | [PAC Privacy Preserving Diffusion Models](https://arxiv.org/abs/2312.01201) | 提出了一种PAC隐私保护扩散模型，通过将私有分类器指导集成到采样过程中增强隐私保护，并发展了一种新的度量标准来衡量隐私水平，在保护性能方面表现出卓越表现。 |
| [^174] | [ViT-Lens: Towards Omni-modal Representations](https://arxiv.org/abs/2311.16081) | ViT-Lens-2利用预先训练的ViT感知新颖模态，并将其对准到预定义空间，为高效的全模态表示学习提供了统一解决方案。 |
| [^175] | [Efficient Pre-training for Localized Instruction Generation of Videos](https://arxiv.org/abs/2311.15964) | 提出了一种名为Sieve-&-Swap的技术，通过自动筛选出不相关文本并用人类编写的说明替换文本转录，从而实现视频本地化指令生成的高效预训练。 |
| [^176] | [Towards more Practical Threat Models in Artificial Intelligence Security](https://arxiv.org/abs/2311.09994) | 学术界研究的AI安全威胁模型未能充分反映实际使用情况，存在重要不匹配，需要更加实用的威胁模型。 |
| [^177] | [Brain Networks and Intelligence: A Graph Neural Network Based Approach to Resting State fMRI Data](https://arxiv.org/abs/2311.03520) | 本文提出了一种新颖的BrainRGIN建模架构，使用图神经网络来预测智力，扩展了现有的图卷积网络并结合了聚类嵌入、图同构网络、TopK池化和基于注意力的读出函数。 |
| [^178] | [Unveiling the Pitfalls of Knowledge Editing for Large Language Models](https://arxiv.org/abs/2310.02129) | 这篇论文探讨了大型语言模型知识编辑的潜在陷阱，提出了新的评估方法，发现知识冲突和知识扭曲是两个重要问题。 |
| [^179] | [Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic](https://arxiv.org/abs/2309.13339) | 提出了LoT（Logical Thoughts）提示，一个自我改进框架，利用根植于符号逻辑的原则，特别是归谬法，逐步验证和纠正大型语言模型的零射链推理过程。 |
| [^180] | [DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields](https://arxiv.org/abs/2307.16897) | 这个数据集DiVa-360提供了沉浸式360度动态视觉数据，解决了缺乏大规模多视角真实世界数据集的问题，为动态神经场方法的研究和发展提供了重要资源。 |
| [^181] | [Diffusion Models Generate Images Like Painters: an Analytical Theory of Outline First, Details Later](https://arxiv.org/abs/2303.02490) | 扩散生成模型倾向于首先生成轮廓，然后逐渐加入细节，早期扰动对图像内容影响较大 |
| [^182] | [An Implicit GNN Solver for Poisson-like problems](https://arxiv.org/abs/2302.10891) | 通过利用隐式层理论，$\Psi$-GNN模型了一个“无限”深的网络，从而避免了经验调整所需的消息传递层次以获得解决方案。 |
| [^183] | [Toward a Theory of Causation for Interpreting Neural Code Models](https://arxiv.org/abs/2302.03788) | 该论文介绍了一种名为$do_{code}$的后验解释方法，用于解释神经代码模型的预测，基于因果推断，旨在实现面向编程语言的解释。 |
| [^184] | [Attention-based Estimation and Prediction of Human Intent to augment Haptic Glove aided Control of Robotic Hand](https://arxiv.org/abs/2110.07953) | 该论文提出了一种基于注意力的估计和预测机制，用于增强触觉手套辅助控制机器手中的目标定位能力。 |
| [^185] | [Efficient Constrained $k$-Center Clustering with Background Knowledge.](http://arxiv.org/abs/2401.12533) | 本论文提出了一种在k中心聚类上利用背景知识的约束聚类算法，通过采用一系列技术，得到了效率高且具有最佳近似比例2的算法。 |
| [^186] | [Analyzing the Quality Attributes of AI Vision Models in Open Repositories Under Adversarial Attacks.](http://arxiv.org/abs/2401.12261) | 本文提出了一种综合流程，用于对AI视觉模型在开放存储库中的质量属性进行分析，尤其是在面对对抗攻击时的表现。我们展示了一个涉及六个计算机视觉模型的评估场景，以评估准确性、鲁棒性、解释效用和开销。 |
| [^187] | [A Safe Preference Learning Approach for Personalization with Applications to Autonomous Vehicles.](http://arxiv.org/abs/2311.02099) | 本研究介绍了一种安全的个性化偏好学习方法，应用于自动驾驶汽车。该方法利用信号时态逻辑公式的优先顺序进行学习，并提出了一种解决这个学习问题的方法。通过对比较，我们找到了适合的权重估计，使得首选信号的加权满足度高于非首选信号。在人体试验中证明了该方法的有效性。 |
| [^188] | [Visual Grounding Helps Learn Word Meanings in Low-Data Regimes.](http://arxiv.org/abs/2310.13257) | 在低数据环境中，使用视觉定位进行监督训练的神经语言模型可以更接近于人类的语言学习能力。 |
| [^189] | [Large Language Model for Multi-objective Evolutionary Optimization.](http://arxiv.org/abs/2310.12541) | 本论文调查了一种利用大型语言模型（LLM）设计MOEA操作符的新方法，通过适当的提示工程，成功将通用的LLM以零-shot方式作为MOEA/D的黑盒搜索操作符，并通过从LLM行为中学习设计了一个显性的白盒操作符。 |
| [^190] | [Greedy Perspectives: Multi-Drone View Planning for Collaborative Coverage in Cluttered Environments.](http://arxiv.org/abs/2310.10863) | 本研究研究了在杂乱环境中协调无人机团队拍摄复杂人群的多无人机多演员视角规划问题，并开发了一个具有遮挡感知目标的视角规划器进行性能比较。 |
| [^191] | [Detecting Sexual Content at the Sentence Level in First Millennium Latin Texts.](http://arxiv.org/abs/2309.14974) | 该研究提出使用深度学习方法在句子级别进行语义分类，以加速人文学科和语言学领域中语料库建设的过程。经过评估，该方法在检测性内容方面表现出高精度和真阳性率，并探索了不同的输入嵌入层对模型性能的影响。 |
| [^192] | [Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning.](http://arxiv.org/abs/2309.10275) | 该论文介绍了CRAMP，一种基于增强式课程强化学习的众包感知分散式路径规划方法，旨在解决拥挤环境下多智能体路径规划的困难。 |
| [^193] | [Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences.](http://arxiv.org/abs/2309.06578) | 本文研究了大型语言模型（LLMs）根据科学摘要文本的能力，来辨别支持或反驳特定假设的证据。通过社区驱动的注释建立了一个新的数据集，针对社会科学中的科学假设证据任务。与其他基准进行了性能比较，并为未来研究提供了机会。 |
| [^194] | [Multi-Objective Optimization for Sparse Deep Neural Network Training.](http://arxiv.org/abs/2308.12243) | 这项工作提出了一种用于稀疏深度神经网络训练的多目标优化算法，通过加权Chebyshev标量化和增广Lagrangian方法，解决了同时优化多个任务时的挑战。 |
| [^195] | [Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models.](http://arxiv.org/abs/2308.01557) | 本文提出了一种新的机器人运动学习与规划方法，通过学习扩散模型作为先验知识，可以加速运动规划优化过程。扩散模型能够在高维环境中有效地编码数据的多模态性，并可以直接从任务目标条件下的后验轨迹分布中进行采样。 |
| [^196] | [Othering and low prestige framing of immigrant cuisines in US restaurant reviews and large language models.](http://arxiv.org/abs/2307.07645) | 通过对2.1M英语Yelp评论的餐厅进行语言分析，研究发现移民美食更容易被构架为客观和他者化，而非西方移民美食受欢迎程度更高。 |
| [^197] | [Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration.](http://arxiv.org/abs/2307.05300) | 本论文提出了单人表现提示（SPP）的概念，通过与多个角色进行多轮自我协作，将单个语言模型转化为认知协同者，从而增强其在复杂任务中的问题解决能力和整体性能。 |
| [^198] | [RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$.](http://arxiv.org/abs/2306.15909) | RL$^3$是一种原则性混合方法，通过将传统强化学习学到的任务特定动作值作为元强化学习神经网络的输入，提高了元强化学习的性能。 |
| [^199] | [FedCSD: A Federated Learning Based Approach for Code-Smell Detection.](http://arxiv.org/abs/2306.00038) | 本文提出了一种名为FedCSD的基于联邦学习的代码异味检测方法，可以在保护数据隐私的同时，让组织协作训练联邦学习模型。该方法在三个实验中分别使用不同的数据集，实现了高精度的检测效果，并且比集中式和交叉验证方法具有更好的性能表现。 |
| [^200] | [Hierarchical Graph Generation with $K^2$-trees.](http://arxiv.org/abs/2305.19125) | 本文介绍了一种基于$K^2$-树的图生成方法，该方法可以实现紧凑生成，并同时捕获图的内在分层结构。通过提出顺序$K^2$-树表示和引入基于Transformer的架构，本文进一步改进了这种方法。实验表明，该方法在图生成方面具有卓越的表现。 |
| [^201] | [Advancing Topic Segmentation and Outline Generation in Chinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark.](http://arxiv.org/abs/2305.14790) | 本文提出了一种分层的段落级中文主题结构表示，使用句子而不是关键词来表示子主题，构建了大规模、高质量的中文段落级主题结构语料库。 |
| [^202] | [a unified front-end framework for english text-to-speech synthesis.](http://arxiv.org/abs/2305.10666) | 该论文提出了一个统一的前端框架，捕捉了英文语音合成前端模块之间的依赖关系，并且在所有模块中均取得了最先进的性能。 |
| [^203] | [ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review.](http://arxiv.org/abs/2305.03123) | 本文研究关注ChatGPT面临的可持续性、隐私、数字鸿沟和伦理问题，提出了SPADE评估的必要性，并给出了缓解和建议。 |
| [^204] | [Safe Explicable Robot Planning.](http://arxiv.org/abs/2304.03773) | 安全可解释机器人规划方法（SEP）扩展了可解释规划，支持安全界限的规定，以实现安全和可解释之间的权衡。 |
| [^205] | [Conversational Tree Search: A New Hybrid Dialog Task.](http://arxiv.org/abs/2303.10227) | 本文介绍了一项新的任务——会话树搜索(CTS)，它可以架起FAQ和对话之间的桥梁，领域专家可以定义对话树，然后将其转换为一个有效的对话策略，只学习提出导航用户达到目标所需的问题。 |

# 详细

[^1]: 修复-Con：深度学习模型转换的自动故障定位和修复

    Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions

    [https://rss.arxiv.org/abs/2312.15101](https://rss.arxiv.org/abs/2312.15101)

    本文提出了一种自动化的故障定位和修复方法Fix-Con，用于在深度学习模型转换过程中修复由转换引入的故障。Fix-Con能够检测和修复模型输入、参数、超参数和模型图方面的故障，提高转换模型的部署和预测正确性。

    

    在不同深度学习框架之间进行模型转换是一种常见的步骤，可以最大程度地增加模型在设备之间的兼容性，并利用可能只在一个深度学习框架中提供的优化功能。然而，这个转换过程可能存在错误，导致转换后的模型无法部署或存在问题，严重降低了其预测的正确性。我们提出了一种自动化的故障定位和修复方法，Fix-Con，在深度学习框架之间进行模型转换时使用。Fix-Con能够检测和修复在转换过程中引入的模型输入、参数、超参数和模型图的故障。Fix-Con使用从调查转换问题中挖掘出的一组故障类型来定位转换模型中潜在的转换故障，并适当修复它们，例如使用源模型的参数替换目标模型的参数。这一过程在数据集中的每个图像上进行迭代执行。

    Converting deep learning models between frameworks is a common step to maximize model compatibility across devices and leverage optimization features that may be exclusively provided in one deep learning framework. However, this conversion process may be riddled with bugs, making the converted models either undeployable or problematic, considerably degrading their prediction correctness.   We propose an automated approach for fault localization and repair, Fix-Con, during model conversion between deep learning frameworks. Fix-Con is capable of detecting and fixing faults introduced in model input, parameters, hyperparameters, and the model graph during conversion.   Fix-Con uses a set of fault types mined from surveying conversion issues raised to localize potential conversion faults in the converted target model, and then repairs them appropriately, e.g. replacing the parameters of the target model with those from the source model. This is done iteratively for every image in the datas
    
[^2]: SLEDGE: 使用生成模型合成驾驶智能体的模拟环境

    SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models

    [https://arxiv.org/abs/2403.17933](https://arxiv.org/abs/2403.17933)

    SLEDGE是第一个使用生成模型训练的车辆运动规划生成模拟器，引入了新颖的栅格到矢量自编码器（RVAE）以及Diffusion Transformer来生成智能体和车道图，从而实现更好的模拟控制。

    

    SLEDGE是第一个基于真实世界驾驶记录训练的车辆运动规划生成模拟器。其核心组件是一个学习模型，能够生成智能体边界框和车道图。该模型的输出作为交通模拟的初始状态。针对SLEDGE待生成的实体的独特特性，例如它们的连接性和每个场景的可变数量，使得大多数现代生成模型在这一任务上的朴素应用变得不简单。因此，我们除了对现有车道图表示进行系统研究外，还引入了一种新颖的栅格到矢量自编码器（RVAE）。它将智能体和车道图编码为栅格化潜在映射中的不同通道。这有助于车道条件下的智能体生成以及使用扩散变换器同时生成车道和智能体。在SLEDGE中使用生成的实体可以更好地控制模拟，例如上采样转弯。

    arXiv:2403.17933v1 Announce Type: cross  Abstract: SLEDGE is the first generative simulator for vehicle motion planning trained on real-world driving logs. Its core component is a learned model that is able to generate agent bounding boxes and lane graphs. The model's outputs serve as an initial state for traffic simulation. The unique properties of the entities to be generated for SLEDGE, such as their connectivity and variable count per scene, render the naive application of most modern generative models to this task non-trivial. Therefore, together with a systematic study of existing lane graph representations, we introduce a novel raster-to-vector autoencoder (RVAE). It encodes agents and the lane graph into distinct channels in a rasterized latent map. This facilitates both lane-conditioned agent generation and combined generation of lanes and agents with a Diffusion Transformer. Using generated entities in SLEDGE enables greater control over the simulation, e.g. upsampling turns 
    
[^3]: MAGIS：基于LLM的GitHub问题解决多Agent框架

    MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution

    [https://arxiv.org/abs/2403.17927](https://arxiv.org/abs/2403.17927)

    提出了MAGIS框架，基于LLM构建，包括四种定制的Agent，能够协作规划和编码过程，解锁LLMs解决GitHub问题的潜力。

    

    在软件演化中，解决GitHub存储库中的突发问题是一个复杂的挑战，不仅涉及新代码的整合，还包括对现有功能的维护。大型语言模型(LLMs)在代码生成和理解方面显示出潜力，但在代码更改方面，特别是在存储库级别上面临困难。为了克服这些挑战，我们通过经验研究了LLMs大多无法解决GitHub问题的原因，并分析了一些影响因素。受经验发现的启发，我们提出了一个新颖的基于LLM的GitHub Issue解决多Agent框架MAGIS，由四种针对软件演化定制的Agent组成：Manager、Repository Custodian、Developer和Quality Assurance Engineer。这个框架利用各种Agent在规划和编码过程中的协作，释放LLMs解决GitHub问题的潜力。

    arXiv:2403.17927v1 Announce Type: cross  Abstract: In software evolution, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing functionalities. Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level. To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors. Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized for the software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In e
    
[^4]: AID: 文本到图像扩散的注重插值

    AID: Attention Interpolation of Text-to-Image Diffusion

    [https://arxiv.org/abs/2403.17924](https://arxiv.org/abs/2403.17924)

    提出了一种新颖的无训练技术，即Attention Interpolation via Diffusion (AID)，通过内/外插值注意力层、插值注意力与自注意力融合以提高保真度，以及应用贝塔分布进行选择以增加平滑度来改进文本到图像插值的问题。

    

    arXiv:2403.17924v1 公告类型: 跨领域 摘要: 有条件的扩散模型可以在各种设置中创建看不见的图像，有助于图像插值。潜在空间中的插值已经得到了深入研究，但是具有特定条件（如文本或姿势）的插值却了解不多。简单的方法，如在条件空间中的线性插值，通常会导致图像缺乏一致性、平滑度和保真度。为此，我们引入一个名为Attention Interpolation via Diffusion (AID)的新颖无训练技术。我们的主要贡献包括1）提出了一个内/外插值注意力层；2）将插值注意力与自注意力融合以提高保真度；3）应用贝塔分布进行选择以增加平滑度。我们还提出了一种变体，Prompt-guided Attention Interpolation via Diffusion (PAID)，它将插值视为依赖于条件的生成过程。这种方法可以创建出更具创造性的新图像。

    arXiv:2403.17924v1 Announce Type: cross  Abstract: Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID). Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater con
    
[^5]: LISA：用于高效内存大型语言模型微调的逐层重要性采样

    LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning

    [https://arxiv.org/abs/2403.17919](https://arxiv.org/abs/2403.17919)

    逐层重要性采样的新方法LISA在微调任务中表现出色，记忆成本低且优于传统方法。

    

    机器学习领域自大型语言模型（LLMs）首次出现以来取得了令人瞩目的进展，然而它们巨大的内存消耗已成为大规模训练的主要障碍。虽然已经提出了诸如低秩调整（LoRA）之类的参数高效微调技术来缓解这一问题，但在大多数大规模微调设置中，它们的性能仍无法与完整参数训练相匹配。为弥补这一不足，我们研究了LoRA在微调任务中的逐层特性，并观察到不同层之间权重范数的异常偏斜。利用这一关键观察，我们发现了一个令人惊讶简单的训练策略，在记忆成本低于LoRA的情况下，在广泛的设置中优于LoRA和完整参数训练。我们将其命名为Layerwise Importance Sampled AdamW（LISA），这是LoRA的一个有希望的替代方案，应用了

    arXiv:2403.17919v1 Announce Type: cross  Abstract: The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of
    
[^6]: AgentStudio：用于构建通用虚拟代理的工具包

    AgentStudio: A Toolkit for Building General Virtual Agents

    [https://arxiv.org/abs/2403.17918](https://arxiv.org/abs/2403.17918)

    AgentStudio是一个用于构建通用虚拟代理的工具包，提供了现实、多模态的开发工具，支持整个代理开发生命周期，并具有高度通用的观测和动作空间，以及图形用户界面，可以在真实环境中有效开发数据集和基准。

    

    arXiv:2403.17918v1 公告类型: 新 原文摘要: 创建能够在任何数字设备上使用任意软件的自主虚拟代理仍然是人工智能领域面临的一个重大挑战。两个关键障碍妨碍了进展: 缺乏在真实环境中构建虚拟代理的基础设施，以及需要对基本代理能力进行野外评估。为了解决这个问题，我们引入了AgentStudio，这是一个在线、现实且多模态的工具包，涵盖了代理开发的整个生命周期。这包括环境设置、数据收集、代理评估和可视化。 观测和动作空间极其通用，支持函数调用和人机界面。AgentStudio的图形用户界面进一步提升了其多样性，允许在真实环境中高效开发数据集和基准。 为了说明，我们引入了一个视觉定位数据集和一个真实世界基准套件，都是...

    arXiv:2403.17918v1 Announce Type: new  Abstract: Creating autonomous virtual agents capable of using arbitrary software on any digital device remains a major challenge for artificial intelligence. Two key obstacles hinder progress: insufficient infrastructure for building virtual agents in real-world environments, and the need for in-the-wild evaluation of fundamental agent abilities. To address this, we introduce AgentStudio, an online, realistic, and multimodal toolkit that covers the entire lifecycle of agent development. This includes environment setups, data collection, agent evaluation, and visualization. The observation and action spaces are highly generic, supporting both function calling and human-computer interfaces. This versatility is further enhanced by AgentStudio's graphical user interfaces, which allow efficient development of datasets and benchmarks in real-world settings. To illustrate, we introduce a visual grounding dataset and a real-world benchmark suite, both cre
    
[^7]: CMP：具有多智能体通信的合作运动预测

    CMP: Cooperative Motion Prediction with Multi-Agent Communication

    [https://arxiv.org/abs/2403.17916](https://arxiv.org/abs/2403.17916)

    该论文提出了一种名为CMP的方法，利用LiDAR信号作为输入，通过合作感知和运动预测模块共享信息，解决了合作运动预测的问题。

    

    随着自动驾驶车辆（AVs）的发展和车联网（V2X）通信的成熟，合作连接的自动化车辆（CAVs）的功能变得可能。本文基于合作感知，探讨了合作运动预测的可行性和有效性。我们的方法CMP以LiDAR信号作为输入，以增强跟踪和预测能力。与过去专注于合作感知或运动预测的工作不同，我们的框架是我们所知的第一个解决CAVs在感知和预测模块中共享信息的统一问题。我们的设计中还融入了能够容忍现实V2X带宽限制和传输延迟的独特能力，同时处理庞大的感知表示。我们还提出了预测聚合模块，统一了预测

    arXiv:2403.17916v1 Announce Type: cross  Abstract: The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predict
    
[^8]: 航空事故报告中细粒度事件提取的分层多标签分类

    Hierarchical Multi-label Classification for Fine-level Event Extraction from Aviation Accident Reports

    [https://arxiv.org/abs/2403.17914](https://arxiv.org/abs/2403.17914)

    本文提出将事件提取问题视为分层分类任务，利用分层注意力模块结合BERT，实现了先识别粗粒度信息再预测细粒度信息的自动事件识别方法。

    

    在航空领域记录了大量的事故报告，提高航空安全至关重要。为更好地利用这些报告，我们需要根据事故报告了解最重要的事件或影响因素。然而，不断增加的事故报告数量要求领域专家投入大量工作来标记这些报告。为了使标记过程更加高效，许多研究人员已开始开发算法，自动从事故报告中识别潜在的事件。本文认为，通过利用事件分类法，我们可以更准确地识别事件。具体来说，我们将这一问题视为一项分层分类任务，在此过程中首先识别粗粒度信息，然后预测细粒度信息。我们通过将一种新颖的分层注意力模块整合到BERT中实现了这种分层分类过程。

    arXiv:2403.17914v1 Announce Type: new  Abstract: A large volume of accident reports is recorded in the aviation domain, which greatly values improving aviation safety. To better use those reports, we need to understand the most important events or impact factors according to the accident reports. However, the increasing number of accident reports requires large efforts from domain experts to label those reports. In order to make the labeling process more efficient, many researchers have started developing algorithms to identify the underlying events from accident reports automatically. This article argues that we can identify the events more accurately by leveraging the event taxonomy. More specifically, we consider the problem a hierarchical classification task where we first identify the coarse-level information and then predict the fine-level information. We achieve this hierarchical classification process by incorporating a novel hierarchical attention module into BERT. To further 
    
[^9]: 使用深度学习分类器基于图像的新型故障检测，使用分层标签

    Image-based Novel Fault Detection with Deep Learning Classifiers using Hierarchical Labels

    [https://arxiv.org/abs/2403.17891](https://arxiv.org/abs/2403.17891)

    本研究提出了一种在深度学习分类器中利用分层标签提高未知故障检测性能的方法，同时不影响模型性能，在热轧钢过程中的实验结果表明其具有较好的可复制性。

    

    现代故障分类系统的一个重要特征是在面对以前未见过的故障类型时能够标记系统。本文考虑了基于深度神经网络的故障分类器的未知故障检测能力。具体来说，我们提出了一种方法，即在可用的情况下，如何利用有关故障分类法的标签来提高未知故障检测性能，而不会牺牲模型性能。为了实现这一目标，我们建议利用软标签技术来改善现有的深度新型故障检测技术，在训练过程中以及用于在线新型故障检测的层次一致检测统计。最后，我们展示了在热轧钢过程中检查图像中新型故障检测的增强检测性能，结果在多种情况和基线检测方法中都有很好的重现。

    arXiv:2403.17891v1 Announce Type: cross  Abstract: One important characteristic of modern fault classification systems is the ability to flag the system when faced with previously unseen fault types. This work considers the unknown fault detection capabilities of deep neural network-based fault classifiers. Specifically, we propose a methodology on how, when available, labels regarding the fault taxonomy can be used to increase unknown fault detection performance without sacrificing model performance. To achieve this, we propose to utilize soft label techniques to improve the state-of-the-art deep novel fault detection techniques during the training process and novel hierarchically consistent detection statistics for online novel fault detection. Finally, we demonstrated increased detection performance on novel fault detection in inspection images from the hot steel rolling process, with results well replicated across multiple scenarios and baseline detection methods.
    
[^10]: 处理大型语言模型的社会误归属性：一种基于HCXAI的方法

    Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach

    [https://arxiv.org/abs/2403.17873](https://arxiv.org/abs/2403.17873)

    本研究通过将ST框架扩展到大型语言模型，特别是在心理健康等敏感领域，以解决社会误归属性的风险，并提出通过增加第五个“W问题”的方式来增强该框架。

    

    人类中心可解释AI（HCXAI）倡导将社会因素融入AI解释中。HCXAI讨论的核心是社会透明（ST）框架，旨在使AI系统的社会组织背景对其用户可见。在这项工作中，我们建议将ST框架扩展到解决大型语言模型（LLMs）中的社会误归风险，特别是在敏感领域如心理健康方面。事实上，LLMs在模拟角色和人物方面具有显著能力，可能导致设计者意图与用户对社会属性的感知之间的不匹配，从而可能促成情感操纵和危险行为，出现认识上的不公正情况以及不合理的信任。为了解决这些问题，我们提出通过向ST框架添加第五个“W问题”来加强该框架，以澄清设计者和用户对LLMs赋予的具体社会属性。

    arXiv:2403.17873v1 Announce Type: new  Abstract: Human-centered explainable AI (HCXAI) advocates for the integration of social aspects into AI explanations. Central to the HCXAI discourse is the Social Transparency (ST) framework, which aims to make the socio-organizational context of AI systems accessible to their users. In this work, we suggest extending the ST framework to address the risks of social misattributions in Large Language Models (LLMs), particularly in sensitive areas like mental health. In fact LLMs, which are remarkably capable of simulating roles and personas, may lead to mismatches between designers' intentions and users' perceptions of social attributes, risking to promote emotional manipulation and dangerous behaviors, cases of epistemic injustice, and unwarranted trust. To address these issues, we propose enhancing the ST framework with a fifth 'W-question' to clarify the specific social attributions assigned to LLMs by its designers and users. This addition aims 
    
[^11]: 气候细化：基于深度学习的带有注意力块和跳跃连接的降水数据超分辨率模型

    Climate Downscaling: A Deep-Learning Based Super-resolution Model of Precipitation Data with Attention Block and Skip Connections

    [https://arxiv.org/abs/2403.17847](https://arxiv.org/abs/2403.17847)

    本研究提出了基于深度学习的气候细化超分辨率模型，结合了注意力块和跳跃连接，旨在有效预测降水数据，为缓解气候变化带来的影响和提高水资源利用效率做出贡献。

    

    人类活动加速了化石燃料的消耗并产生了温室气体，导致了当今迫在眉睫的问题：全球变暖和气候变化。这些间接造成严重的自然灾害，许多生命受苦以及农业财产的巨大损失。为了减轻对我们土地的影响，科学家们正在开发可再生、可重复使用和清洁能源，气候学家正试图预测极端天气。同时，各国政府正在公布节约资源的政策，以建立更环保的社会并唤起环境意识。其中最具影响力的因素之一就是降水，将凝结的水蒸气带到土地上。水资源是社会中最重要但基本的需求，不仅支持我们的生活，也支持经济。在台湾，尽管平均年降水量高达2500毫米，但每人的水分配量低于全球平均水平，这是由于...

    arXiv:2403.17847v1 Announce Type: cross  Abstract: Human activities accelerate consumption of fossil fuels and produce greenhouse gases, resulting in urgent issues today: global warming and the climate change. These indirectly cause severe natural disasters, plenty of lives suffering and huge losses of agricultural properties. To mitigate impacts on our lands, scientists are developing renewable, reusable, and clean energies and climatologists are trying to predict the extremes. Meanwhile, governments are publicizing resource-saving policies for a more eco-friendly society and arousing environment awareness. One of the most influencing factors is the precipitation, bringing condensed water vapor onto lands. Water resources are the most significant but basic needs in society, not only supporting our livings, but also economics. In Taiwan, although the average annual precipitation is up to 2,500 millimeter (mm), the water allocation for each person is lower than the global average due to
    
[^12]: 基于语言驱动的机器人导航的分层开放词汇3D场景图

    Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation

    [https://arxiv.org/abs/2403.17846](https://arxiv.org/abs/2403.17846)

    提出了一种用于语言驱动的机器人导航的分层开放词汇3D场景图映射方法，可以有效代表多层建筑并允许机器人在其中穿行。

    

    最近的开放词汇机器人映射方法利用预先训练的视觉-语言特征丰富了密集几何地图。虽然这些地图允许在查询某种语言概念时预测逐点显著性地图，但大规模环境和超出对象级别的抽象查询仍然是一个相当大的障碍，最终限制了基于语言的机器人导航。在这项工作中，我们提出了HOV-SG，一种用于语言驱动的机器人导航的分层开放词汇3D场景图映射方法。通过利用开放词汇视觉基础模型，我们首先在3D空间中获得了最先进的开放词汇分段级地图，然后构建了由地板、房间和对象概念组成的3D场景图层次结构，每个都包含开放性词汇特征。我们的方法能够表示多层建筑，并且允许机器人使用跨层Voronoi图穿越这些建筑。HOV-SG进行了评估。

    arXiv:2403.17846v1 Announce Type: cross  Abstract: Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluat
    
[^13]: ReMamber：使用Mamba Twister实现引用图像分割

    ReMamber: Referring Image Segmentation with Mamba Twister

    [https://arxiv.org/abs/2403.17839](https://arxiv.org/abs/2403.17839)

    提出了ReMamber，一种整合了Mamba和多模态Mamba Twister块的新型RIS架构，通过其独特的通道和空间扭曲机制实现图像-文本交互，取得了三个基准测试的最新技术成果

    

    引用图像分割（RIS）利用变换器在解释复杂的视觉-语言任务方面取得了巨大成功。然而，二次计算成本使其在捕捉远程视觉-语言依赖性方面消耗资源。幸运的是，Mamba通过高效的线性复杂度在处理方面解决了这个问题。然而，将Mamba直接应用于多模态交互会面临挑战，主要原因是因为通道交互不足，无法有效融合多模态数据。在本文中，我们提出了ReMamber，这是一种整合了Mamba和多模态Mamba Twister块强大功能的新型RIS架构。Mamba Twister通过其独特的通道和空间扭曲机制明确建模图像-文本交互，并通过其独特的通道和空间扭曲机制融合文本和视觉特征。我们在三个具有挑战性的基准测试中取得了最新技术成果。此外，我们对ReMamber进行了彻底分析，并讨论其他...

    arXiv:2403.17839v1 Announce Type: cross  Abstract: Referring Image Segmentation (RIS) leveraging transformers has achieved great success on the interpretation of complex visual-language tasks. However, the quadratic computation cost makes it resource-consuming in capturing long-range visual-language dependencies. Fortunately, Mamba addresses this with efficient linear complexity in processing. However, directly applying Mamba to multi-modal interactions presents challenges, primarily due to inadequate channel interactions for the effective fusion of multi-modal data. In this paper, we propose ReMamber, a novel RIS architecture that integrates the power of Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly models image-text interaction, and fuses textual and visual features through its unique channel and spatial twisting mechanism. We achieve the state-of-the-art on three challenging benchmarks. Moreover, we conduct thorough analyses of ReMamber and discuss other
    
[^14]: 基于扩散的从文本描述中合成手-物体交互的方法

    DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions

    [https://arxiv.org/abs/2403.17827](https://arxiv.org/abs/2403.17827)

    提出了一种从文本描述和物体几何形状中合成逼真的手-物体交互的方法，通过三种技术实现了有效学习，包括任务分解、紧密耦合的姿势表示和不同的引导方案。

    

    生成自然的3D手-物体交互具有挑战性，因为期望生成的手部和物体动作在物理上是合理的，并且在语义上是有意义的。我们提出了一种名为DiffH2O的新方法，可以从提供的文本提示和物体几何形状中合成逼真的单手或双手物体交互。该方法引入了三种技术，可以有效地从有限数据中学习。首先，我们将任务分解为抓取阶段和基于文本交互阶段，并为每个阶段使用单独的扩散模型。在抓取阶段中，模型仅生成手部动作，而在交互阶段中，手部和物体姿势都被合成。其次，我们提出了一种紧密耦合手部和物体姿势的紧凑表示。第三，我们提出了两种不同的引导方案。

    arXiv:2403.17827v1 Announce Type: cross  Abstract: Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful. Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets. We propose DiffH2O, a novel method to synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object. The method introduces three techniques that enable effective learning from limited data. First, we decompose the task into a grasping stage and a text-based interaction stage and use separate diffusion models for each. In the grasping stage, the model only generates hand motions, whereas in the interaction phase both hand and object poses are synthesized. Second, we propose a compact representation that tightly couples hand and object poses. Third, we propose two different guidance schemes 
    
[^15]: 在Stackelberg规划和元操作验证的计算复杂性上：技术报告

    On the Computational Complexity of Stackelberg Planning and Meta-Operator Verification: Technical Report

    [https://arxiv.org/abs/2403.17826](https://arxiv.org/abs/2403.17826)

    Stackelberg规划在理论上的复杂性分析显示，一般情况下并不比经典规划更难，但在多项式计划长度限制下，会提升到多项式复杂性层次结构中更高一级

    

    Stackelberg规划是最近引入的单轮两人对抗规划模型，其中两位玩家在一个联合的经典规划任务中行动，第一位玩家的目标是阻碍第二位玩家实现目标。这将Stackelberg规划问题置于经典规划和一般组合两人游戏之间。但，确切地说是在哪里？迄今为止对Stackelberg规划的所有研究都集中在实际方面。我们通过进行第一次对Stackelberg规划的理论复杂性分析来填补这一空白。我们表明，一般情况下，Stackelberg规划实际上不比经典规划更难。然而，在多项式计划长度限制下，Stackelberg规划在多项式复杂性层次结构中更高一级，表明编译成经典规划会导致最坏情况下指数级计划长度增加。在试图识别可处理的情况

    arXiv:2403.17826v1 Announce Type: new  Abstract: Stackelberg planning is a recently introduced single-turn two-player adversarial planning model, where two players are acting in a joint classical planning task, the objective of the first player being hampering the second player from achieving its goal. This places the Stackelberg planning problem somewhere between classical planning and general combinatorial two-player games. But, where exactly? All investigations of Stackelberg planning so far focused on practical aspects. We close this gap by conducting the first theoretical complexity analysis of Stackelberg planning. We show that in general Stackelberg planning is actually no harder than classical planning. Under a polynomial plan-length restriction, however, Stackelberg planning is a level higher up in the polynomial complexity hierarchy, suggesting that compilations into classical planning come with a worst-case exponential plan-length increase. In attempts to identify tractable 
    
[^16]: 利用大型语言模型加速无线电频谱监管工作流程

    Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs)

    [https://arxiv.org/abs/2403.17819](https://arxiv.org/abs/2403.17819)

    本文展示了大型语言模型（LLMs）在加快频谱监管流程中的示例应用，探讨了LLMs在此背景下可以发挥的各种作用，同时识别了需要解决的一些挑战，并通过适当的实验、实用案例和见解突出了LLMs在频谱管理中的转变潜力。

    

    无线电频谱监管是一个复杂而严苛的过程，由于技术进步迅速、对频谱需求增加以及多方利益攸关者可能存在相互冲突的利益，加之重要的经济影响。为了应对这一情况，监管机构必须有效地与各方合作，跟上全球技术趋势，进行技术评估，及时发放许可证，并遵守各种法律和政策框架。

    arXiv:2403.17819v1 Announce Type: cross  Abstract: Wireless spectrum regulation is a complex and demanding process due to the rapid pace of technological progress, increasing demand for spectrum, and a multitude of stakeholders with potentially conflicting interests, alongside significant economic implications. To navigate this, regulators must engage effectively with all parties, keep pace with global technology trends, conduct technical evaluations, issue licenses in a timely manner, and comply with various legal and policy frameworks.   In light of these challenges, this paper demonstrates example applications of Large Language Models (LLMs) to expedite spectrum regulatory processes. We explore various roles that LLMs can play in this context while identifying some of the challenges to address. The paper also offers practical case studies and insights, with appropriate experiments, highlighting the transformative potential of LLMs in spectrum management.
    
[^17]: D-PAD: 深浅层多频模式解缠用于时间序列预测

    D-PAD: Deep-Shallow Multi-Frequency Patterns Disentangling for Time Series Forecasting

    [https://arxiv.org/abs/2403.17814](https://arxiv.org/abs/2403.17814)

    D-PAD 提出了一个深浅层多频模式解缠神经网络，通过多分量分解和分解-重构-分解模块，有效解缠时间序列中的复杂时间模式。

    

    在时间序列预测中，有效解缠复杂的时间模式至关重要。本文提出了D-PAD，这是一个用于时间序列预测的深浅层多频模式解缠神经网络。具体而言，引入了多分量分解（MCD）模块将时间序列分解为具有不同频率范围的组分，对应于“浅”方面。同时提出了一个分解-重构-分解（D-R-D）模块，逐渐提取组分中混合的频率信息，对应于“深”方面。

    arXiv:2403.17814v1 Announce Type: new  Abstract: In time series forecasting, effectively disentangling intricate temporal patterns is crucial. While recent works endeavor to combine decomposition techniques with deep learning, multiple frequencies may still be mixed in the decomposed components, e.g., trend and seasonal. Furthermore, frequency domain analysis methods, e.g., Fourier and wavelet transforms, have limitations in resolution in the time domain and adaptability. In this paper, we propose D-PAD, a deep-shallow multi-frequency patterns disentangling neural network for time series forecasting. Specifically, a multi-component decomposing (MCD) block is introduced to decompose the series into components with different frequency ranges, corresponding to the "shallow" aspect. A decomposition-reconstruction-decomposition (D-R-D) module is proposed to progressively extract the information of frequencies mixed in the components, corresponding to the "deep" aspect. After that, an intera
    
[^18]: 评估在基于图像的安全应用中，在即时工程过程中设计的大型多模型与微调视觉变换器的有效性

    Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications

    [https://arxiv.org/abs/2403.17787](https://arxiv.org/abs/2403.17787)

    评估在基于图像的安全应用中，即时工程的Gemini-pro大型多模态模型与微调的Vision Transformer模型的有效性，并发现在关键安全挑战中表现出明显的性能差异。

    

    大型语言模型（LLMs）的成功导致了大型多模态模型（LMMs）（如Gemini-pro）的发展，这些模型已经开始转变各种应用。这些复杂的多模态模型旨在解释和分析复杂数据，整合了以往难以实现的文本和视觉信息规模，为一系列应用开辟了新的途径。本文调查了在解决关键安全挑战方面，即时工程的Gemini-pro LMMs与微调视觉变换器（ViT）模型的适用性和有效性。我们专注于两个不同的任务：检测图像中的简单触发器（如方形小方块）以示潜在后门的在视觉上显而易见的任务，以及通过视觉表示进行恶意软件分类的在视觉上不明显的任务。我们的结果突显了性能上的显著差异，Gemini-pro表现不佳。

    arXiv:2403.17787v1 Announce Type: new  Abstract: The success of Large Language Models (LLMs) has led to a parallel rise in the development of Large Multimodal Models (LMMs), such as Gemini-pro, which have begun to transform a variety of applications. These sophisticated multimodal models are designed to interpret and analyze complex data, integrating both textual and visual information on a scale previously unattainable, opening new avenues for a range of applications. This paper investigates the applicability and effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision Transformer (ViT) models in addressing critical security challenges. We focus on two distinct tasks: a visually evident task of detecting simple triggers, such as small squares in images, indicative of potential backdoors, and a non-visually evident task of malware classification through visual representations. Our results highlight a significant divergence in performance, with Gemini-pro falling shor
    
[^19]: SciCapenter: 通过机器生成的标题和评分支持科学图像的标题组合

    SciCapenter: Supporting Caption Composition for Scientific Figures with Machine-Generated Captions and Ratings

    [https://arxiv.org/abs/2403.17784](https://arxiv.org/abs/2403.17784)

    SciCapenter 是一个交互式系统，利用尖端的AI技术为科学图像生成多样的标题，提供评分和综合检查表，大大降低了标题撰写的认知负荷。

    

    有效地为图像撰写标题至关重要，读者严重依赖这些标题来理解图像的信息。本文介绍了SciCapenter，一个交互式系统，它整合了尖端的AI技术用于科学图像标题，以帮助标题的撰写。

    arXiv:2403.17784v1 Announce Type: cross  Abstract: Crafting effective captions for figures is important. Readers heavily depend on these captions to grasp the figure's message. However, despite a well-developed set of AI technologies for figures and captions, these have rarely been tested for usefulness in aiding caption writing. This paper introduces SciCapenter, an interactive system that puts together cutting-edge AI technologies for scientific figure captions to aid caption composition. SciCapenter generates a variety of captions for each figure in a scholarly article, providing scores and a comprehensive checklist to assess caption quality across multiple critical aspects, such as helpfulness, OCR mention, key takeaways, and visual properties reference. Users can directly edit captions in SciCapenter, resubmit for revised evaluations, and iteratively refine them. A user study with Ph.D. students indicates that SciCapenter significantly lowers the cognitive load of caption writing.
    
[^20]: 迈向应用数学中工作流程和模型的FAIR文档化

    Towards a FAIR Documentation of Workflows and Models in Applied Mathematics

    [https://arxiv.org/abs/2403.17778](https://arxiv.org/abs/2403.17778)

    MaRDI开发了一个FAIR和可机器解释的模板，用于全面文档化应用数学中的建模-仿真-优化工作流程，并展示了MaRDMO和MathModDB知识图之间的交互作用。

    

    arXiv:2403.17778v1 公告类型：新摘要：建模-仿真-优化工作流在应用数学中扮演着重要角色。数学研究数据倡议MaRDI通过开发一个FAIR和可机器解释的模板，对这些工作流的全面文档化做出了回应。研究数据管理组织者的插件MaRDMO使来自不同领域的科学家可以利用MaRDI模板在MaRDI门户上无缝地记录和发布他们的工作流。这些工作流的核心是数学模型。MaRDI通过MathModDB本体论解决了这个问题，提供了一个结构化的形式模型描述。在这里，我们展示了来自数字人文学科的代数建模工作流程中MaRDMO和MathModDB知识图之间的交互作用。这个演示突显了这两项服务在原始数值领域之外的多功能性。

    arXiv:2403.17778v1 Announce Type: new  Abstract: Modeling-Simulation-Optimization workflows play a fundamental role in applied mathematics. The Mathematical Research Data Initiative, MaRDI, responded to this by developing a FAIR and machine-interpretable template for a comprehensive documentation of such workflows. MaRDMO, a Plugin for the Research Data Management Organiser, enables scientists from diverse fields to document and publish their workflows on the MaRDI Portal seamlessly using the MaRDI template. Central to these workflows are mathematical models. MaRDI addresses them with the MathModDB ontology, offering a structured formal model description. Here, we showcase the interaction between MaRDMO and the MathModDB Knowledge Graph through an algebraic modeling workflow from the Digital Humanities. This demonstration underscores the versatility of both services beyond their original numerical domain.
    
[^21]: 从学术复杂性到公众叙事：科学新闻报道生成的数据集

    SciNews: From Scholarly Complexities to Public Narratives -- A Dataset for Scientific News Report Generation

    [https://arxiv.org/abs/2403.17768](https://arxiv.org/abs/2403.17768)

    科学新闻报道生成的自动化提高了学术见解的可访问性，该研究提出了一个包含学术出版物和相应科学新闻报道的数据集，用于探索自动生成科学新闻报道的可能性。

    

    科学新闻报道作为一个桥梁，巧妙地将复杂的研究文章翻译成与更广泛的公众 resonant 的报道。这种叙事的自动生成增强了学术见解的可访问性。在本文中，我们提出了一个新的语料库来促进这种范式的发展。我们的语料库包括九个学科领域中学术出版物及其相应科学新闻报道的平行编译。为了证明我们数据集的实用性和可靠性，我们进行了广泛分析，突出了科学新闻叙事和学术文稿之间的可读性和简洁性差异。我们使用最先进的文本生成模型基准测试我们的数据集。评估过程包括自动评估和人工评估，为未来探索自动生成科学新闻报道打下了基础。

    arXiv:2403.17768v1 Announce Type: cross  Abstract: Scientific news reports serve as a bridge, adeptly translating complex research articles into reports that resonate with the broader public. The automated generation of such narratives enhances the accessibility of scholarly insights. In this paper, we present a new corpus to facilitate this paradigm development. Our corpus comprises a parallel compilation of academic publications and their corresponding scientific news reports across nine disciplines. To demonstrate the utility and reliability of our dataset, we conduct an extensive analysis, highlighting the divergences in readability and brevity between scientific news narratives and academic manuscripts. We benchmark our dataset employing state-of-the-art text generation models. The evaluation process involves both automatic and human evaluation, which lays the groundwork for future explorations into the automated generation of scientific news reports. The dataset and code related 
    
[^22]: DataCook：针对医疗保健数据版权保护的反对抗样本制作

    DataCook: Crafting Anti-Adversarial Examples for Healthcare Data Copyright Protection

    [https://arxiv.org/abs/2403.17755](https://arxiv.org/abs/2403.17755)

    DataCook是一个针对医疗保健数据版权保护的新方法，通过制作反对抗样本，使版权持有人能够在部署阶段控制模型的授权。

    

    在医疗保健领域，版权保护和未经授权的第三方滥用的挑战日益重要。传统的数据版权保护方法应用于数据发布之前，这意味着对这些数据训练的模型变得不可控制。本文介绍了一种名为DataCook的新方法，旨在在部署阶段保护医疗保健数据的版权。DataCook通过在分发之前“处理”原始数据的方式运行，从而使开发的模型在这些处理过的数据上表现正常。然而，在部署阶段，原始测试数据也必须通过DataCook“处理”以确保模型正常运行。该过程使版权持有人在部署阶段控制授权。DataCook背后的机制是制作反对抗样本（AntiAdv），旨在增强模型的信心，而不是。

    arXiv:2403.17755v1 Announce Type: new  Abstract: In the realm of healthcare, the challenges of copyright protection and unauthorized third-party misuse are increasingly significant. Traditional methods for data copyright protection are applied prior to data distribution, implying that models trained on these data become uncontrollable. This paper introduces a novel approach, named DataCook, designed to safeguard the copyright of healthcare data during the deployment phase. DataCook operates by "cooking" the raw data before distribution, enabling the development of models that perform normally on this processed data. However, during the deployment phase, the original test data must be also "cooked" through DataCook to ensure normal model performance. This process grants copyright holders control over authorization during the deployment phase. The mechanism behind DataCook is by crafting anti-adversarial examples (AntiAdv), which are designed to enhance model confidence, as opposed to st
    
[^23]: 使用分层抽样改进LIME图像解释

    Using Stratified Sampling to Improve LIME Image Explanations

    [https://arxiv.org/abs/2403.17742](https://arxiv.org/abs/2403.17742)

    研究提出了使用分层抽样方法改进LIME图像解释，以减少典型蒙特卡洛抽样产生的人工产物，实验证明了该方法的有效性。

    

    我们研究了一种分层抽样方法，用于改进LIME图像，这是一种流行的针对计算机视觉任务的模型无关可解释AI方法，以减少典型蒙特卡洛抽样产生的人工产物。这些人工产物是由于在解释的图像周围的合成邻域中对依赖变量的欠采样造成的，这可能会导致由于无法在采样数据上拟合线性回归器而导致解释不足。然后，我们强调了与Shapley理论的联系，在过去曾提出关于欠采样和样本相关性的类似论点。我们推导出了一种无偏分层抽样估计器所需的所有公式和调整因子。实验表明所提出的方法的功效。

    arXiv:2403.17742v1 Announce Type: new  Abstract: We investigate the use of a stratified sampling approach for LIME Image, a popular model-agnostic explainable AI method for computer vision tasks, in order to reduce the artifacts generated by typical Monte Carlo sampling. Such artifacts are due to the undersampling of the dependent variable in the synthetic neighborhood around the image being explained, which may result in inadequate explanations due to the impossibility of fitting a linear regressor on the sampled data. We then highlight a connection with the Shapley theory, where similar arguments about undersampling and sample relevance were suggested in the past. We derive all the formulas and adjustment factors required for an unbiased stratified sampling estimator. Experiments show the efficacy of the proposed approach.
    
[^24]: 一体化：异质交互建模用于冷启动评分预测

    All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction

    [https://arxiv.org/abs/2403.17740](https://arxiv.org/abs/2403.17740)

    提出了异质交互评分网络（HIRE）框架，通过异质交互模块（HIM）来共同建模异质交互并直接推断重要特征

    

    冷启动评分预测是推荐系统中一个基本问题，已得到广泛研究。许多方法已经被提出，利用现有数据之间的显式关系，例如协同过滤、社交推荐和异构信息网络，以缓解冷启动用户和物品的数据不足问题。然而，基于不同角色之间的数据构建的显式关系可能不可靠且无关，从而限制了特定推荐任务的性能上限。受此启发，本文提出了一个灵活的框架，名为异质交互评分网络（HIRE）。HIRE不仅仅依赖于预先定义的交互模式或手动构建的异构信息网络。相反，我们设计了一个异质交互模块（HIM），来共同建模异质交互并直接推断重要特征。

    arXiv:2403.17740v1 Announce Type: cross  Abstract: Cold-start rating prediction is a fundamental problem in recommender systems that has been extensively studied. Many methods have been proposed that exploit explicit relations among existing data, such as collaborative filtering, social recommendations and heterogeneous information network, to alleviate the data insufficiency issue for cold-start users and items. However, the explicit relations constructed based on data between different roles may be unreliable and irrelevant, which limits the performance ceiling of the specific recommendation task. Motivated by this, in this paper, we propose a flexible framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not solely rely on the pre-defined interaction pattern or the manually constructed heterogeneous information network. Instead, we devise a Heterogeneous Interaction Module (HIM) to jointly model the heterogeneous interactions and directly infer the important in
    
[^25]: 通过测试时适应进行越界谣言检测

    Out-of-distribution Rumor Detection via Test-Time Adaptation

    [https://arxiv.org/abs/2403.17735](https://arxiv.org/abs/2403.17735)

    提出了一种名为测试时适应的方法，用于在分布转移下进行谣言检测，通过建模传播图和构建测试时适应框架，增强模型的适应性和鲁棒性。

    

    由于谣言在社交媒体上的迅速传播，谣言检测已经成为一个极其重要的挑战。现有的谣言检测方法取得了良好的性能，因为它们已经从相同数据分布中收集了足够的语料库进行模型训练。然而，由于新闻主题、社交媒体平台、语言之间的差异以及由新闻热度引起的传播规模的差异，训练数据和真实世界测试数据之间发生了显著的分布变化。这导致了这些现有方法在越界（OOD）情况下性能显著下降。为了解决这个问题，我们提出了一种名为测试时适应（TARD）的简单而高效的方法，用于在分布转移下进行谣言检测。该方法将新闻的传播建模为一个传播图，并构建传播图测试时适应框架，增强模型的适应性和鲁棒性。

    arXiv:2403.17735v1 Announce Type: new  Abstract: Due to the rapid spread of rumors on social media, rumor detection has become an extremely important challenge. Existing methods for rumor detection have achieved good performance, as they have collected enough corpus from the same data distribution for model training. However, significant distribution shifts between the training data and real-world test data occur due to differences in news topics, social media platforms, languages and the variance in propagation scale caused by news popularity. This leads to a substantial decline in the performance of these existing methods in Out-Of-Distribution (OOD) situations. To address this problem, we propose a simple and efficient method named Test-time Adaptation for Rumor Detection under distribution shifts (TARD). This method models the propagation of news in the form of a propagation graph, and builds propagation graph test-time adaptation framework, enhancing the model's adaptability and r
    
[^26]: 小型模型是大型模型的计算节省者

    Tiny Models are the Computational Saver for Large Models

    [https://arxiv.org/abs/2403.17726](https://arxiv.org/abs/2403.17726)

    TinySaver是一种动态模型压缩方法，通过使用小型模型来自适应地替换大型模型，从而提高计算效率。

    

    本文介绍了TinySaver，一种类似于早期退出的动态模型压缩方法，它使用小型模型来自适应地替换大型模型。与传统的压缩技术不同，像TinySaver这样的动态方法可以利用难度差异，使得某些输入能够提前完成推理过程，从而节省计算资源。大多数现有的早期退出设计是通过向模型的骨干结构附加额外的网络分支来实现的。然而，我们的研究揭示了完全独立的小型模型可以在对性能影响最小的情况下替代较大模型的大部分工作。将它们作为第一个退出点可以显著提高计算效率。通过搜索并使用最合适的小型模型作为给定大型模型的计算节省者，所提出的方法作为一种新颖且通用的模型压缩方法。

    arXiv:2403.17726v1 Announce Type: new  Abstract: This paper introduces TinySaver, an early-exit-like dynamic model compression approach which employs tiny models to substitute large models adaptively. Distinct from traditional compression techniques, dynamic methods like TinySaver can leverage the difficulty differences to allow certain inputs to complete their inference processes early, thereby conserving computational resources. Most existing early exit designs are implemented by attaching additional network branches to the model's backbone. Our study, however, reveals that completely independent tiny models can replace a substantial portion of the larger models' job with minimal impact on performance. Employing them as the first exit can remarkably enhance computational efficiency. By searching and employing the most appropriate tiny model as the computational saver for a given large model, the proposed approaches work as a novel and generic method to model compression. This finding
    
[^27]: 基于优化的对LLM评判系统的提示注入攻击

    Optimization-based Prompt Injection Attack to LLM-as-a-Judge

    [https://arxiv.org/abs/2403.17710](https://arxiv.org/abs/2403.17710)

    介绍了一种基于优化的提示注入攻击方法，JudgeDeceiver，针对LLM-as-a-Judge，通过自动化生成对抗序列实现了有针对性和高效的模型评估操控。

    

    LLM-as-a-Judge 是一种可以使用大型语言模型（LLMs）评估文本信息的新颖解决方案。根据现有研究，LLMs在提供传统人类评估的引人注目替代方面表现出色。然而，这些系统针对提示注入攻击的鲁棒性仍然是一个未解决的问题。在这项工作中，我们引入了JudgeDeceiver，一种针对LLM-as-a-Judge量身定制的基于优化的提示注入攻击。我们的方法制定了一个精确的优化目标，用于攻击LLM-as-a-Judge的决策过程，并利用优化算法高效地自动化生成对抗序列，实现对模型评估的有针对性和有效的操作。与手工制作的提示注入攻击相比，我们的方法表现出卓越的功效，给基于LLM的判断系统当前的安全范式带来了重大挑战。

    arXiv:2403.17710v1 Announce Type: cross  Abstract: LLM-as-a-Judge is a novel solution that can assess textual information with large language models (LLMs). Based on existing research studies, LLMs demonstrate remarkable performance in providing a compelling alternative to traditional human assessment. However, the robustness of these systems against prompt injection attacks remains an open question. In this work, we introduce JudgeDeceiver, a novel optimization-based prompt injection attack tailored to LLM-as-a-Judge. Our method formulates a precise optimization objective for attacking the decision-making process of LLM-as-a-Judge and utilizes an optimization algorithm to efficiently automate the generation of adversarial sequences, achieving targeted and effective manipulation of model evaluations. Compared to handcraft prompt injection attacks, our method demonstrates superior efficacy, posing a significant challenge to the current security paradigms of LLM-based judgment systems. T
    
[^28]: 增强短文本建模：利用大型语言模型进行主题细化

    Enhanced Short Text Modeling: Leveraging Large Language Models for Topic Refinement

    [https://arxiv.org/abs/2403.17706](https://arxiv.org/abs/2403.17706)

    利用大型语言模型的先进能力，提出了一种名为“主题细化”的新方法，通过引入提示工程和消除离题词等方式改进短文本的主题建模质量，提高了主题的语义质量。

    

    有效地构建针对简短文本（如推文和新闻标题）的主题模型对捕捉社会动态的迅速变化至关重要。然而，传统主题模型往往在准确表达短文本的语义细微差异方面存在不足，这是由于它们的简洁性和缺乏上下文数据。在我们的研究中，我们利用大型语言模型（LLMs）的先进能力，引入了一种称为“主题细化”的新方法。该方法并非直接参与主题的初步建模，而是专注于改进主题在被挖掘后的阶段。通过引入提示工程，我们指导LLMs消除给定主题中的离题词，确保仅保留与语境相关的词汇或用更符合语义的词汇替换。这种方法模拟了人类般的审查和改进主题的方式，从而提升了各种主题生成的语义质量。

    arXiv:2403.17706v1 Announce Type: cross  Abstract: Crafting effective topic models for brief texts, like tweets and news headlines, is essential for capturing the swift shifts in social dynamics. Traditional topic models, however, often fall short in accurately representing the semantic intricacies of short texts due to their brevity and lack of contextual data. In our study, we harness the advanced capabilities of Large Language Models (LLMs) to introduce a novel approach termed "Topic Refinement". This approach does not directly involve itself in the initial modeling of topics but focuses on improving topics after they have been mined. By employing prompt engineering, we direct LLMs to eliminate off-topic words within a given topic, ensuring that only contextually relevant words are preserved or substituted with ones that fit better semantically. This method emulates human-like scrutiny and improvement of topics, thereby elevating the semantic quality of the topics generated by vario
    
[^29]: MEP: 多核学习增强相对位置编码长度外推

    MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation

    [https://arxiv.org/abs/2403.17698](https://arxiv.org/abs/2403.17698)

    提出了MEP方法，通过结合不同核函数生成偏差来解决变压器模型在长度外推时的准确性降低问题

    

    当预测的序列长度超过训练中看到的长度时，变压器的推理准确性会降低。现有的相对位置编码方法，如基于ALiBi技术的方法，仅通过实现单个核函数来解决长度外推挑战，这会根据它们之间的距离为每个后Softmax注意力分数引入恒定偏差。这些方法未探讨或使用多个核函数来应对外推挑战。借鉴ALiBi方法，本研究提出了一种新颖的相对位置编码方法，称为MEP，它采用加权平均来结合不同的核函数（如指数核和高斯核）产生一个应用于后Softmax注意力分数的偏差。最初，该框架利用各种核函数构建多个核函数。每个核

    arXiv:2403.17698v1 Announce Type: cross  Abstract: When the predicted sequence length exceeds the length seen during training, the transformer's inference accuracy diminishes. Existing relative position encoding methods, such as those based on the ALiBi technique, address the length extrapolation challenge exclusively through the implementation of a single kernel function, which introduces a constant bias to every post-softmax attention scores according to their distance. These approaches do not investigate or employ multiple kernel functions to address the extrapolation challenge. Drawing on the ALiBi approach, this study proposes a novel relative positional encoding method, called MEP, which employs a weighted average to combine distinct kernel functions(such as the exponential kernel and the Gaussian kernel) to generate a bias that is applied to post-softmax attention scores. Initially, the framework utilizes various kernel functions to construct multiple kernel functions. Each kern
    
[^30]: ExpressEdit：使用自然语言和草图进行视频编辑

    ExpressEdit: Video Editing with Natural Language and Sketching

    [https://arxiv.org/abs/2403.17693](https://arxiv.org/abs/2403.17693)

    ExpressEdit系统探索了如何利用自然语言和草图支持视频编辑人员表达视频编辑想法，并提出了一种多模态方法来简化视频编辑过程。

    

    信息视频对于向新手和专家解释概念和程序知识至关重要。在制作信息视频时，编辑人员通过叠加文本/图像或修剪镜头来增强视频质量并增加吸引力。然而，视频编辑可能会很困难且耗时，尤其是对于经常在表达和实现编辑想法方面遇到困难的新手视频编辑人员而言。为解决这一挑战，我们首先探讨了如何利用多模态——自然语言（NL）和草图，这是人类用于表达的自然模态——来支持视频编辑人员表达视频编辑想法。我们从10名视频编辑人员那里收集了176种编辑命令的多模态表达，揭示了在描述编辑意图时使用NL和草图的模式。根据研究结果，我们提出了ExpressEdit，这是一个通过NL实现视频编辑的系统。

    arXiv:2403.17693v1 Announce Type: cross  Abstract: Informational videos serve as a crucial source for explaining conceptual and procedural knowledge to novices and experts alike. When producing informational videos, editors edit videos by overlaying text/images or trimming footage to enhance the video quality and make it more engaging. However, video editing can be difficult and time-consuming, especially for novice video editors who often struggle with expressing and implementing their editing ideas. To address this challenge, we first explored how multimodality$-$natural language (NL) and sketching, which are natural modalities humans use for expression$-$can be utilized to support video editors in expressing video editing ideas. We gathered 176 multimodal expressions of editing commands from 10 video editors, which revealed the patterns of use of NL and sketching in describing edit intents. Based on the findings, we present ExpressEdit, a system that enables editing videos via NL te
    
[^31]: 解决情感与文化智能人工智能研讨会情感预测竞赛的方案

    Solution for Emotion Prediction Competition of Workshop on Emotionally and Culturally Intelligent AI

    [https://arxiv.org/abs/2403.17683](https://arxiv.org/abs/2403.17683)

    提出了一种名为ECSP的单多模态情感文化特定提示方法，旨在通过使用单一模态消息增强多模态模型性能，并设计良好的提示来减少文化差异问题。

    

    这篇报告提供了我们在WECIA情感预测竞赛（EPC）中探索和提出的方法的详细描述，该方法通过对带有评论的艺术作品预测一个人的情感。该竞赛的数据集是ArtELingo，旨在鼓励跨语言和文化的多样化工作。该数据集主要面临两个挑战，即模态不平衡问题和语言文化差异问题。为了解决这个问题，我们提出了一种简单而有效的方法，称为单多模态情感文化特定提示（ECSP），其重点是使用单一模态消息来增强多模态模型的性能，并设计良好的提示来减少文化差异问题。澄清一下，我们的方法包含两个主要块：（1）基于XLM-R的单模态模型和基于X$^2$-VLM的多模态模型（2）情感文化特定提示。

    arXiv:2403.17683v1 Announce Type: new  Abstract: This report provide a detailed description of the method that we explored and proposed in the WECIA Emotion Prediction Competition (EPC), which predicts a person's emotion through an artistic work with a comment. The dataset of this competition is ArtELingo, designed to encourage work on diversity across languages and cultures. The dataset has two main challenges, namely modal imbalance problem and language-cultural differences problem. In order to address this issue, we propose a simple yet effective approach called single-multi modal with Emotion-Cultural specific prompt(ECSP), which focuses on using the single modal message to enhance the performance of multimodal models and a well-designed prompt to reduce cultural differences problem. To clarify, our approach contains two main blocks: (1)XLM-R\cite{conneau2019unsupervised} based unimodal model and X$^2$-VLM\cite{zeng2022x} based multimodal model (2) Emotion-Cultural specific prompt.
    
[^32]: 在航天器上的高光谱图像深度无损和接近无损预测编码与基于线段的注意力

    Onboard deep lossless and near-lossless predictive coding of hyperspectral images with line-based attention

    [https://arxiv.org/abs/2403.17677](https://arxiv.org/abs/2403.17677)

    本文提出了一种在航天器上压缩高光谱图像的新方法，通过设计递归的预测神经网络LineRWKV，结合了Transformer和递归神经网络的优势，并在实验中取得了优异表现

    

    深度学习方法在航天器上对高光谱图像进行压缩传统上是困难的，因为需要大量的计算复杂性来实现足够的表示能力，以及缺乏用于训练和测试的合适数据集。本文摆脱了传统的自编码器方法，设计了一种名为LineRWKV的预测神经网络，通过逐行递归工作以限制内存消耗。为了实现这一目标，我们采用了一种将Transformer的表示优势与递归神经网络的线性复杂度和递归实现相结合的新型混合注意力-递归操作。压缩算法使用LineRWKV对每个像素进行预测，然后对残差进行熵编码。在HySpecNet-11k数据集和PRISMA图像上的实验结果表明，LineRWKV是第一个在性能上表现优异的深度学习方法。

    arXiv:2403.17677v1 Announce Type: cross  Abstract: Deep learning methods have traditionally been difficult to apply to compression of hyperspectral images onboard of spacecrafts, due to the large computational complexity needed to achieve adequate representational power, as well as the lack of suitable datasets for training and testing. In this paper, we depart from the traditional autoencoder approach and we design a predictive neural network, called LineRWKV, that works recursively line-by-line to limit memory consumption. In order to achieve that, we adopt a novel hybrid attentive-recursive operation that combines the representational advantages of Transformers with the linear complexity and recursive implementation of recurrent neural networks. The compression algorithm performs prediction of each pixel using LineRWKV, followed by entropy coding of the residual. Experiments on the HySpecNet-11k dataset and PRISMA images show that LineRWKV is the first deep-learning method to outper
    
[^33]: 在网络安全游戏中依靠自己的时候：通过强化学习代理指导LLM成为专家

    Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games

    [https://arxiv.org/abs/2403.17674](https://arxiv.org/abs/2403.17674)

    将预训练的强化学习代理指导的LLM代理SecurityBot用于网络安全操作，实现显著性能改进。

    

    将LLM（生命周期长、记忆持续、自我评估的）与强化学习（RL）代理有效结合，以实现在网络安全等高风险任务中的协同性能至关重要。在本研究中，我们介绍了SecurityBot，这是一个由预训练RL代理指导的LLM代理，用于支持网络安全操作。该LLM代理具有概要模块以生成行为指南、记忆模块以累积本地经验、反思模块以重新评估选择，以及行动模块以减少行动空间。此外，它采用协作机制从预训练RL代理获得建议，包括用于动态建议的光标、用于多个导师建议排名的聚合器和用于主动建议请求的呼叫器。基于CybORG实验框架，我们的经验表明，与独立的LLM或RL相比，SecurityBot表现出显著的性能改进。

    arXiv:2403.17674v1 Announce Type: cross  Abstract: Integrating LLM and reinforcement learning (RL) agent effectively to achieve complementary performance is critical in high stake tasks like cybersecurity operations. In this study, we introduce SecurityBot, a LLM agent mentored by pre-trained RL agents, to support cybersecurity operations. In particularly, the LLM agent is supported with a profile module to generated behavior guidelines, a memory module to accumulate local experiences, a reflection module to re-evaluate choices, and an action module to reduce action space. Additionally, it adopts the collaboration mechanism to take suggestions from pre-trained RL agents, including a cursor for dynamic suggestion taken, an aggregator for multiple mentors' suggestions ranking and a caller for proactive suggestion asking. Building on the CybORG experiment framework, our experiences show that SecurityBot demonstrates significant performance improvement compared with LLM or RL standalone, a
    
[^34]: 文本分类的语言模型：仅仅上下文学习就足够了吗？

    Language Models for Text Classification: Is In-Context Learning Enough?

    [https://arxiv.org/abs/2403.17661](https://arxiv.org/abs/2403.17661)

    本研究通过对16个文本分类数据集的大规模评估研究，填补了现有研究缺乏对文本生成模型与提示技术与更传统的文本分类方法之间比较的理解。

    

    最近的基础语言模型在零次和少次标记设置中展示了在许多自然语言处理任务中的最先进性能。这些模型相对于基于微调的更标准的方法的优势在于能够理解用自然语言编写的指令（提示），这有助于它们更好地推广到不同的任务和领域，而无需特定的训练数据。这使它们适合解决具有有限标注实例数量的领域的文本分类问题。但是，现有研究在规模上有限，并缺乏对文本生成模型与提示技术相结合与更传统的文本分类方法（如微调掩码语言模型）的比较的理解。在本文中，我们通过对涵盖二元、多类和多标签问题的16个文本分类数据集进行大规模评估研究来填补这一研究空白。

    arXiv:2403.17661v1 Announce Type: cross  Abstract: Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand instructions written in natural language (prompts), which helps them generalise better to different tasks and domains without the need for specific training data. This makes them suitable for addressing text classification problems for domains with limited amounts of annotated instances. However, existing research is limited in scale and lacks understanding of how text generation models combined with prompting techniques compare to more established methods for text classification such as fine-tuning masked language models. In this paper, we address this research gap by performing a large-scale evaluation study for 16 text classification datasets covering binary, multiclass, and multilabel problems. In par
    
[^35]: SGHormer：一种由脉冲驱动的节能图变换器

    SGHormer: An Energy-Saving Graph Transformer Driven by Spikes

    [https://arxiv.org/abs/2403.17656](https://arxiv.org/abs/2403.17656)

    SGHormer是一种由脉冲驱动的节能图变换器，通过将全精度嵌入转换为稀疏和二值化脉冲以减少内存和计算成本，提高了图变换器的效率。

    

    具有强大表示学习能力的图变换器（GTs）在各种图任务中取得巨大成功。然而，GTs出色性能背后的代价是更高的能量消耗和计算开销。传统变换器中注意力计算过程中的复杂结构和二次复杂度严重影响其在大规模图数据上的可扩展性。虽然现有方法在简化块之间的组合或注意力学习范式方面取得了进展以提高GTs的效率，但在构建GT框架时很少考虑源自生物学上合理结构的一系列节能解决方案。为此，我们提出了一种新的基于脉冲的图变换器（SGHormer）。它将全精度嵌入转换为稀疏和二值化脉冲以减少内存和计算成本。SGHormer中的脉冲图自注意力和脉冲修正块可以显著减少计算和存储开销。

    arXiv:2403.17656v1 Announce Type: cross  Abstract: Graph Transformers (GTs) with powerful representation learning ability make a huge success in wide range of graph tasks. However, the costs behind outstanding performances of GTs are higher energy consumption and computational overhead. The complex structure and quadratic complexity during attention calculation in vanilla transformer seriously hinder its scalability on the large-scale graph data. Though existing methods have made strides in simplifying combinations among blocks or attention-learning paradigm to improve GTs' efficiency, a series of energy-saving solutions originated from biologically plausible structures are rarely taken into consideration when constructing GT framework. To this end, we propose a new spiking-based graph transformer (SGHormer). It turns full-precision embeddings into sparse and binarized spikes to reduce memory and computational costs. The spiking graph self-attention and spiking rectify blocks in SGHorm
    
[^36]: 计算和验证抽象论证中偏好的基于扩展的方法

    An Extension-based Approach for Computing and Verifying Preferences in Abstract Argumentation

    [https://arxiv.org/abs/2403.17653](https://arxiv.org/abs/2403.17653)

    提出了一种基于扩展的方法，用于计算和验证抽象论证系统中的偏好。

    

    我们提出了一种基于扩展的方法，用于计算和验证抽象论证系统中的偏好。虽然先前已经开发了许多论证语义用于从论证框架中识别可接受的论点集，但缺乏基于隐含论点偏好的可接受性背后的理由。基于偏好的论证框架可以确定在给定一组偏好的情况下哪些论点是被证明的。我们的研究考虑了标准推理问题的逆问题，即，给定一个抽象论证框架和一组被证明的论点，我们计算可能的论点偏好是什么。此外，有必要验证（即评估）计算出的偏好是否会导致可接受的论点集。本文提出了一种新的方法和算法，用于穷举计算和列举所有可能的论点集。

    arXiv:2403.17653v1 Announce Type: new  Abstract: We present an extension-based approach for computing and verifying preferences in an abstract argumentation system. Although numerous argumentation semantics have been developed previously for identifying acceptable sets of arguments from an argumentation framework, there is a lack of justification behind their acceptability based on implicit argument preferences. Preference-based argumentation frameworks allow one to determine what arguments are justified given a set of preferences. Our research considers the inverse of the standard reasoning problem, i.e., given an abstract argumentation framework and a set of justified arguments, we compute what the possible preferences over arguments are. Furthermore, there is a need to verify (i.e., assess) that the computed preferences would lead to the acceptable sets of arguments. This paper presents a novel approach and algorithm for exhaustively computing and enumerating all possible sets of pr
    
[^37]: S+t-SNE - 将降维引入数据流

    S+t-SNE - Bringing dimensionality reduction to data streams

    [https://arxiv.org/abs/2403.17643](https://arxiv.org/abs/2403.17643)

    S+t-SNE是t-SNE算法的改进版本，在处理数据流时具有增量更新和盲目漂移管理的特点，能够实现高效的降维和信息可视化。

    

    我们提出了S+t-SNE，这是t-SNE算法的一种改进，旨在处理无限数据流。S+t-SNE的核心思想是随着新数据的到来逐步更新t-SNE嵌入，确保可扩展性和适应性，以处理流式场景。通过在每一步选择最重要的点，该算法确保可扩展性同时保持信息可视化。采用盲目方法进行漂移管理调整嵌入空间，促进不断可视化不断发展的数据动态。我们的实验评估证明了S+t-SNE的有效性和效率。结果突显了其在流式场景中捕捉模式的能力。我们希望我们的方法为研究人员和从业者提供一个实时工具，用于理解和解释高维数据。

    arXiv:2403.17643v1 Announce Type: new  Abstract: We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle infinite data streams. The core idea behind S+t-SNE is to update the t-SNE embedding incrementally as new data arrives, ensuring scalability and adaptability to handle streaming scenarios. By selecting the most important points at each step, the algorithm ensures scalability while keeping informative visualisations. Employing a blind method for drift management adjusts the embedding space, facilitating continuous visualisation of evolving data dynamics. Our experimental evaluations demonstrate the effectiveness and efficiency of S+t-SNE. The results highlight its ability to capture patterns in a streaming scenario. We hope our approach offers researchers and practitioners a real-time tool for understanding and interpreting high-dimensional data.
    
[^38]: PeersimGym：用于通过强化学习解决任务卸载问题的环境

    PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning

    [https://arxiv.org/abs/2403.17637](https://arxiv.org/abs/2403.17637)

    引入了 PeersimGym 环境，通过强化学习解决任务卸载问题，支持定制化仿真环境，有助于开发和优化计算网络中的任务卸载策略。

    

    任务卸载对于在诸如物联网之类的网络中平衡设备的计算负载至关重要，但面临着诸如在严格的通信和存储约束下最小化延迟和能源使用等重要优化挑战。传统优化在可扩展性方面存在不足；启发式方法缺乏实现最佳结果，而强化学习（RL）通过允许通过迭代交互学习最佳卸载策略的方式提供了一种有前景的途径。然而，RL 的功效取决于对丰富数据集和定制的现实训练环境的访问。为解决这一问题，我们引入了 PeersimGym，这是一个开源的、可定制的仿真环境，旨在开发和优化计算网络中的任务卸载策略。PeersimGym 支持各种网络拓扑和计算约束，并整合了一种"PettingZo"方法，使用户能够轻松配置仿真参数和监控仿真过程。

    arXiv:2403.17637v1 Announce Type: cross  Abstract: Task offloading, crucial for balancing computational loads across devices in networks such as the Internet of Things, poses significant optimization challenges, including minimizing latency and energy usage under strict communication and storage constraints. While traditional optimization falls short in scalability; and heuristic approaches lack in achieving optimal outcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the learning of optimal offloading strategies through iterative interactions. However, the efficacy of RL hinges on access to rich datasets and custom-tailored, realistic training environments. To address this, we introduce PeersimGym, an open-source, customizable simulation environment tailored for developing and optimizing task offloading strategies within computational networks. PeersimGym supports a wide range of network topologies and computational constraints and integrates a \textit{PettingZo
    
[^39]: UADA3D：面向稀疏LiDAR和大领域差距的无监督对抗领域自适应在3D物体检测中的应用

    UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps

    [https://arxiv.org/abs/2403.17633](https://arxiv.org/abs/2403.17633)

    UADA3D是一种无监督对抗领域自适应方法，能够在3D物体检测中处理稀疏LiDAR数据和大领域差距，并在自动驾驶汽车和移动机器人领域中表现出显著的改进。

    

    在这项研究中，我们解决了现有无监督领域适应方法在基于LiDAR的3D物体检测中的一个问题，这些方法主要集中在适应已建立的高密度自动驾驶数据集之间的转变。我们专注于更稀疏的点云，捕捉来自不同视角的场景：不仅来自道路上的车辆，还来自人行道上的移动机器人，遭遇着明显不同的环境条件和传感器配置。我们引入了无监督对抗领域自适应3D物体检测（UADA3D）。UADA3D不依赖于预训练的源模型或师生架构。相反，它使用对抗方法直接学习域不变特征。我们展示了它在各种适应场景中的有效性，在自动驾驶汽车和移动机器人领域均显示出显著的改进。我们的代码是开源的，很快将会提供。

    arXiv:2403.17633v1 Announce Type: cross  Abstract: In this study, we address a gap in existing unsupervised domain adaptation approaches on LiDAR-based 3D object detection, which have predominantly concentrated on adapting between established, high-density autonomous driving datasets. We focus on sparser point clouds, capturing scenarios from different perspectives: not just from vehicles on the road but also from mobile robots on sidewalks, which encounter significantly different environmental conditions and sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source models or teacher-student architectures. Instead, it uses an adversarial approach to directly learn domain-invariant features. We demonstrate its efficacy in various adaptation scenarios, showing significant improvements in both self-driving car and mobile robot domains. Our code is open-source and will be available soon.
    
[^40]: 使用开放数据集对电动微移动能耗建模

    Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset

    [https://arxiv.org/abs/2403.17632](https://arxiv.org/abs/2403.17632)

    提出了一个专门针对电动微移动工具在都柏林收集的开放数据集，为解决实际场景中能耗建模的困难提供了重要资源

    

    车辆拥堵和环境恶化带来的挑战日益加剧，凸显了在城市空间推行E-Mobility解决方案的重要性。特别是，E-滑板车和E-自行车等微型E-Mobility工具在这一转变中发挥着关键作用，为城市通勤者提供可持续的替代方案。然而，这些工具的能耗模式是影响其在现实场景中有效性的关键因素，对于出行规划以及增强用户在使用这些工具时的信心至关重要。为此，最近的研究利用针对特定移动工具和条件定制的物理模型，但这些模型在现实场景中的泛化能力和有效性存在困难，这是因为缺乏用于彻底模型评估和验证的开放数据集。为填补这一空白，我们的工作提出了一个在爱尔兰都柏林收集的开放数据集，专门用于能耗建模。

    arXiv:2403.17632v1 Announce Type: new  Abstract: The escalating challenges of traffic congestion and environmental degradation underscore the critical importance of embracing E-Mobility solutions in urban spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes, play a pivotal role in this transition, offering sustainable alternatives for urban commuters. However, the energy consumption patterns for these tools are a critical aspect that impacts their effectiveness in real-world scenarios and is essential for trip planning and boosting user confidence in using these. To this effect, recent studies have utilised physical models customised for specific mobility tools and conditions, but these models struggle with generalization and effectiveness in real-world scenarios due to a notable absence of open datasets for thorough model evaluation and verification. To fill this gap, our work presents an open dataset, collected in Dublin, Ireland, specifically designed for ene
    
[^41]: 对开放领域问答的去噪表格-文本检索

    Denoising Table-Text Retrieval for Open-Domain Question Answering

    [https://arxiv.org/abs/2403.17611](https://arxiv.org/abs/2403.17611)

    本文提出了一种Denosied Table-Text Retriever（DoTTeR）方法，通过利用去噪训练数据集和整合表级排名信息，解决了表格-文本开放领域问答中存在的假正标签影响和跨表格推理问题的挑战。

    

    在表格-文本开放领域问答中，检索系统从表格和文本中检索相关证据以回答问题。之前在表格-文本开放领域问答中的研究存在两个常见挑战：首先，它们的检索器可能受到训练数据集中的假正标签影响；其次，它们可能难以为需要跨表格推理的问题提供适当的证据。为了解决这些问题，我们提出了去噪表格-文本检索器（DoTTeR）。我们的方法包括利用一个去噪的训练数据集，通过舍弃通过假正检测模型测量的较低问题相关性得分的示例来减少假正标签。随后，我们将表级排名信息整合到检索器中，以帮助找到需要跨表格推理的问题的证据。为了编码此排名信息，我们对一个排名感知的列编码器进行微调。

    arXiv:2403.17611v1 Announce Type: cross  Abstract: In table-text open-domain question answering, a retriever system retrieves relevant evidence from tables and text to answer questions. Previous studies in table-text open-domain question answering have two common challenges: firstly, their retrievers can be affected by false-positive labels in training datasets; secondly, they may struggle to provide appropriate evidence for questions that require reasoning across the table. To address these issues, we propose Denoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a denoised training dataset with fewer false positive labels by discarding instances with lower question-relevance scores measured through a false positive detection model. Subsequently, we integrate table-level ranking information into the retriever to assist in finding evidence for questions that demand reasoning across the table. To encode this ranking information, we fine-tune a rank-aware column encoder 
    
[^42]: 在英特尔数据中心GPU上全融合的多层感知机

    Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs

    [https://arxiv.org/abs/2403.17607](https://arxiv.org/abs/2403.17607)

    通过全融合操作、最小化全局内存访问以及最大化数据重用，本文提出了在英特尔数据中心GPU上的多层感知机实现，显著提高了性能，尤其在推理方面表现出色。

    

    本文提出了一种针对英特尔数据中心GPU Max 1550进行优化的多层感知机（MLPs）的SYCL实现。为了提高性能，我们的实现通过融合MLP每一层中的操作，最小化了慢速全局内存访问，从而最大化了普通寄存器文件和共享本地内存中的数据重用。我们用一个简单的roofline模型展示了这种方法导致算术强度显著提高，从而提高了性能，尤其是推理方面。我们将我们的方法与类似的基于CUDA的MLP实现进行了比较，并展示了我们在英特尔数据中心GPU上对Nvidia的H100 GPU基于CUDA的实现在推理上的性能优势高达2.84倍，在训练上高达1.75倍。本文还展示了我们的SYCL实现在图像压缩、神经光辐射场和物理信息中的三个重要领域的高效性。

    arXiv:2403.17607v1 Announce Type: new  Abstract: This paper presents a SYCL implementation of Multi-Layer Perceptrons (MLPs), which targets and is optimized for the Intel Data Center GPU Max 1550. To increase the performance, our implementation minimizes the slow global memory accesses by maximizing the data reuse within the general register file and the shared local memory by fusing the operations in each layer of the MLP. We show with a simple roofline model that this results in a significant increase in the arithmetic intensity, leading to improved performance, especially for inference. We compare our approach to a similar CUDA implementation for MLPs and show that our implementation on the Intel Data Center GPU outperforms the CUDA implementation on Nvidia's H100 GPU by a factor up to 2.84 in inference and 1.75 in training. The paper also showcases the efficiency of our SYCL implementation in three significant areas: Image Compression, Neural Radiance Fields, and Physics-Informed M
    
[^43]: LASIL：学习者感知的长期微观交通仿真监督模仿学习

    LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation

    [https://arxiv.org/abs/2403.17601](https://arxiv.org/abs/2403.17601)

    提出了一种学习者感知的监督模仿学习方法，通过变分自动编码器增强专家状态，以解决多智体模仿学习中的协变量偏移问题

    

    微观交通仿真在交通工程中起着至关重要的作用，通过提供关于单个车辆行为和整体交通流的见解。然而，创建一个真实的模拟器，精确复制各种交通条件下的人类驾驶行为，面临着重大挑战。传统的依赖启发式模型的模拟器往往由于现实世界交通环境的复杂性而无法提供准确的模拟。由于协变量偏移问题，现有的基于模仿学习的模拟器经常无法生成稳定的长期模拟。在本文中，我们提出了一种称为学习者感知的监督模仿学习的新方法，以解决多智体模仿学习中的协变量偏移问题。通过利用变分自动编码器同时建模专家和学习者状态分布，我们的方法增强了专家状态，从而使增强状态意识到

    arXiv:2403.17601v1 Announce Type: new  Abstract: Microscopic traffic simulation plays a crucial role in transportation engineering by providing insights into individual vehicle behavior and overall traffic flow. However, creating a realistic simulator that accurately replicates human driving behaviors in various traffic conditions presents significant challenges. Traditional simulators relying on heuristic models often fail to deliver accurate simulations due to the complexity of real-world traffic environments. Due to the covariate shift issue, existing imitation learning-based simulators often fail to generate stable long-term simulations. In this paper, we propose a novel approach called learner-aware supervised imitation learning to address the covariate shift problem in multi-agent imitation learning. By leveraging a variational autoencoder simultaneously modeling the expert and learner state distribution, our approach augments expert states such that the augmented state is aware 
    
[^44]: 双存储网络：一种用于视觉与语言模型的多功能适应方法

    Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models

    [https://arxiv.org/abs/2403.17589](https://arxiv.org/abs/2403.17589)

    提出了双存储网络的多功能适应方法，能在零次适应、少次适应和无需训练的少次适应三种设置下高效运行

    

    随着像CLIP这样的预训练视觉与语言模型的出现，如何将它们调整到各种下游分类任务已经引起了最近研究的重视。该适应策略通常可以归类为三种范式：零次适应、少次适应和最近提出的无需训练的少次适应。大多数现有方法都是针对特定设置量身定制的，只能满足其中一种或两种范式。本文介绍了一种多功能适应方法，能够有效地在这三种设置下运行。具体地，我们提出了双存储网络，包括动态和静态记忆组件。静态记忆缓存训练数据知识，实现了无需训练的少次适应，而动态记忆在测试过程中在线保存历史测试特征，允许探索超出论文中已训练数据的额外数据洞察

    arXiv:2403.17589v1 Announce Type: cross  Abstract: With the emergence of pre-trained vision-language models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in recent research. The adaptation strategies can be typically categorized into three paradigms: zero-shot adaptation, few-shot adaptation, and the recently-proposed training-free few-shot adaptation. Most existing approaches are tailored for a specific setting and can only cater to one or two of these paradigms. In this paper, we introduce a versatile adaptation approach that can effectively work under all three settings. Specifically, we propose the dual memory networks that comprise dynamic and static memory components. The static memory caches training data knowledge, enabling training-free few-shot adaptation, while the dynamic memory preserves historical test features online during the testing process, allowing for the exploration of additional data insights beyond the
    
[^45]: 在“挑战冠军”比赛中贿赂的参数化分析

    Parameterized Analysis of Bribery in Challenge the Champ Tournaments

    [https://arxiv.org/abs/2403.17587](https://arxiv.org/abs/2403.17587)

    研究了在“挑战冠军”比赛中贿赂的参数化分析问题，探讨通过贿赂其他玩家来增加最初冠军赢得比赛的概率，并在限定预算下进行优化。研究发现在以玩家数量为参数时，这一问题是弱NP困难且W[1]-hard的。

    

    挑战冠军比赛是最简单的竞争形式之一，其中一个（最初选择的）冠军反复受到其他玩家的挑战。如果玩家击败冠军，则该玩家被视为新的（当前的）冠军。每位参赛选手按照固定顺序挑战当前的冠军一次。最后一轮的冠军被视为比赛的获胜者。我们研究了一种情形，即玩家可以被贿赂以降低他们对最初冠军的获胜概率。目标是通过贿赂其他玩家来最大化最初冠军赢得比赛的概率，并且不超过规定的贿赂预算。Mattei等人[《应用逻辑杂志》，2015年]表明该问题可以在伪多项式时间内解决，并且在以玩家数量作为参数的情况下，该问题属于XP。我们表明，当以玩家数量为参数时，该问题是弱NP困难的，并且是W[1]-hard的。

    arXiv:2403.17587v1 Announce Type: cross  Abstract: Challenge the champ tournaments are one of the simplest forms of competition, where a (initially selected) champ is repeatedly challenged by other players. If a player beats the champ, then that player is considered the new (current) champ. Each player in the competition challenges the current champ once in a fixed order. The champ of the last round is considered the winner of the tournament. We investigate a setting where players can be bribed to lower their winning probability against the initial champ. The goal is to maximize the probability of the initial champ winning the tournament by bribing the other players, while not exceeding a given budget for the bribes. Mattei et al. [Journal of Applied Logic, 2015] showed that the problem can be solved in pseudo-polynomial time, and that it is in XP when parameterized by the number of players.   We show that the problem is weakly NP-hard and W[1]-hard when parameterized by the number of 
    
[^46]: 朝着零数据、可控、自适应对话系统迈进

    Towards a Zero-Data, Controllable, Adaptive Dialog System

    [https://arxiv.org/abs/2403.17582](https://arxiv.org/abs/2403.17582)

    该论文提出了一种从对话树生成数据的方法，可帮助训练出在合成数据上训练的代理达到与在人类数据上训练的模型相媲美的对话成功率。

    

    对话树搜索（Väth等，2023年）是一种最近的对话系统控制方法，其中领域专家通过对话树塑造强化学习代理的行为。代理学会有效地浏览这棵树，同时适应不同用户的信息需求，例如领域熟悉度。然而，额外的训练数据需求阻碍了在新领域的部署。为了解决这个问题，我们探索了直接从对话树生成这些数据的方法。我们改进了原始方法，并展示了在合成数据上训练的代理可以实现与在人类数据上训练的模型相当的对话成功率，无论是使用商业大语言模型进行生成，还是使用较小的开源模型，在单个GPU上运行。我们进一步通过收集和测试两个新数据集来展示我们方法的可扩展性：ONBOARD，一个帮助外国居民搬迁的新领域。

    arXiv:2403.17582v1 Announce Type: cross  Abstract: Conversational Tree Search (V\"ath et al., 2023) is a recent approach to controllable dialog systems, where domain experts shape the behavior of a Reinforcement Learning agent through a dialog tree. The agent learns to efficiently navigate this tree, while adapting to information needs, e.g., domain familiarity, of different users. However, the need for additional training data hinders deployment in new domains. To address this, we explore approaches to generate this data directly from dialog trees. We improve the original approach, and show that agents trained on synthetic data can achieve comparable dialog success to models trained on human data, both when using a commercial Large Language Model for generation, or when using a smaller open-source model, running on a single GPU. We further demonstrate the scalability of our approach by collecting and testing on two new datasets: ONBOARD, a new domain helping foreign residents moving t
    
[^47]: m3P:面向多模态多语言翻译的多语境提示

    m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt

    [https://arxiv.org/abs/2403.17556](https://arxiv.org/abs/2403.17556)

    通过引入视觉上下文作为通用的语言无关表示，该论文提出了一种利用多模态提示来指导多模态多语言神经机器翻译的框架，以实现对不同语言表示的对齐，并生成条件视觉-语言记忆进行翻译。

    

    多语言翻译通过将所有语言投影到一个共享空间来支持多个翻译方向，但由于文本模态中语言之间的差异，尤其是当语言数量较大时，翻译质量会受到影响。为了弥补这一差距，我们引入视觉上下文作为通用的语言无关表示，以促进多语言翻译。在本文中，我们提出了一个框架，利用多模态提示来指导Multimodal Multilingual神经机器翻译（m3P），通过将不同语言的表示与相同含义对齐，并生成用于翻译的条件视觉-语言记忆。我们构建了一个支持102种语言的多语言多模态指令数据集（InstrMulti102）。我们的方法旨在通过将图像视为中央语言来最小化不同语言之间的表示距离。实验结果表明，...

    arXiv:2403.17556v1 Announce Type: cross  Abstract: Multilingual translation supports multiple translation directions by projecting all languages in a shared space, but the translation quality is undermined by the difference between languages in the text-only modality, especially when the number of languages is large. To bridge this gap, we introduce visual context as the universal language-independent representation to facilitate multilingual translation. In this paper, we propose a framework to leverage the multimodal prompt to guide the Multimodal Multilingual neural Machine Translation (m3P), which aligns the representations of different languages with the same meaning and generates the conditional vision-language memory for translation. We construct a multilingual multimodal instruction dataset (InstrMulti102) to support 102 languages. Our method aims to minimize the representation distance of different languages by regarding the image as a central language. Experimental results sh
    
[^48]: 在医学图像分析中实际应用先进云服务和生成式人工智能系统

    Practical Applications of Advanced Cloud Services and Generative AI Systems in Medical Image Analysis

    [https://arxiv.org/abs/2403.17549](https://arxiv.org/abs/2403.17549)

    生成式人工智能在医学影像中的转变潜力，能够生成合成数据、增强图像、帮助异常检测和促进图像转换。

    

    arXiv:2403.17549v1 公告类型: 新摘要: 医学领域是人工智能技术应用中的重要领域之一。随着医疗数据的爆炸性增长和多样化，以及医疗需求和挑战的不断提升，人工智能技术在医学领域中发挥着越来越重要的作用。以计算机视觉、自然语言处理和机器学习为代表的人工智能技术已经广泛渗透到医学影像、健康管理、医疗信息和药物研发等各种场景中，成为提高医疗服务水平和质量的重要驱动力。本文探讨了生成式人工智能在医学影像中的转变潜力，强调了它生成合成数据、增强图像、帮助异常检测和促进图像转换等能力。

    arXiv:2403.17549v1 Announce Type: new  Abstract: The medical field is one of the important fields in the application of artificial intelligence technology. With the explosive growth and diversification of medical data, as well as the continuous improvement of medical needs and challenges, artificial intelligence technology is playing an increasingly important role in the medical field. Artificial intelligence technologies represented by computer vision, natural language processing, and machine learning have been widely penetrated into diverse scenarios such as medical imaging, health management, medical information, and drug research and development, and have become an important driving force for improving the level and quality of medical services.The article explores the transformative potential of generative AI in medical imaging, emphasizing its ability to generate syntheticACM-2 data, enhance images, aid in anomaly detection, and facilitate image-to-image translation. Despite chall
    
[^49]: VDSC：利用价值差异和状态计数增强探索时间

    VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts

    [https://arxiv.org/abs/2403.17542](https://arxiv.org/abs/2403.17542)

    通过价值差异和状态计数，利用代理的内部状态来决定何时进行探索，解决了盲目切换机制的缺点。

    

    尽管深度强化学习中对于“探索多少”和“如何探索”问题受到了相当大的关注，但对于“何时”探索的研究相对较少。在更复杂的探索策略可以在特定的、通常稀疏的奖励环境中表现出色的同时，现有的简单方法，如$\epsilon$-贪心，在更广泛的领域中继续表现优异。这些简单策略的吸引力在于它们的易实现性和对各种领域的普遍适用性。然而，这些方法的缺点在于它们本质上是一种盲目的切换机制，完全忽略了代理的内部状态。本文提出利用代理的内部状态来决定“何时”进行探索，从而解决盲目切换机制的缺点。我们通过稳态（VDSC）提出了价值差异和状态计数。

    arXiv:2403.17542v1 Announce Type: cross  Abstract: Despite the considerable attention given to the questions of \textit{how much} and \textit{how to} explore in deep reinforcement learning, the investigation into \textit{when} to explore remains relatively less researched. While more sophisticated exploration strategies can excel in specific, often sparse reward environments, existing simpler approaches, such as $\epsilon$-greedy, persist in outperforming them across a broader spectrum of domains. The appeal of these simpler strategies lies in their ease of implementation and generality across a wide range of domains. The downside is that these methods are essentially a blind switching mechanism, which completely disregards the agent's internal state. In this paper, we propose to leverage the agent's internal state to decide \textit{when} to explore, addressing the shortcomings of blind switching mechanisms. We present Value Discrepancy and State Counts through homeostasis (VDSC), a no
    
[^50]: KC-GenRe:一种基于大型语言模型的知识约束生成式重新排序方法，用于知识图完成

    KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion

    [https://arxiv.org/abs/2403.17532](https://arxiv.org/abs/2403.17532)

    KC-GenRe是一种基于大型语言模型的知识约束生成式重新排序方法，用于KGC，通过将KGC重新排序任务构建为候选标识符排序生成问题以及开发知识引导方法来解决重新排序过程中的不匹配和错序问题。

    

    知识图完成（KGC）的目标是预测实体之间缺失的事实。先前的KGC重新排序方法大多建立在非生成式语言模型上，以获得每个候选的概率。最近，生成式大型语言模型（LLMs）在信息提取和对话系统等多个任务上表现出色。利用它们进行KGC重新排序有利于利用广泛预训练的知识和强大的生成能力。然而，在完成任务时可能会遇到新问题，即不匹配、错序和遗漏。为此，我们引入了KC-GenRe，一种基于LLMs的知识约束生成式重新排序方法，用于KGC。为了克服不匹配问题，我们将KGC重新排序任务构建为由生成式LLMs实施的候选标识符排序生成问题。为了解决错序问题，我们制定了一个受知识引导的...

    arXiv:2403.17532v1 Announce Type: new  Abstract: The goal of knowledge graph completion (KGC) is to predict missing facts among entities. Previous methods for KGC re-ranking are mostly built on non-generative language models to obtain the probability of each candidate. Recently, generative large language models (LLMs) have shown outstanding performance on several tasks such as information extraction and dialog systems. Leveraging them for KGC re-ranking is beneficial for leveraging the extensive pre-trained knowledge and powerful generative capabilities. However, it may encounter new problems when accomplishing the task, namely mismatch, misordering and omission. To this end, we introduce KC-GenRe, a knowledge-constrained generative re-ranking method based on LLMs for KGC. To overcome the mismatch issue, we formulate the KGC re-ranking task as a candidate identifier sorting generation problem implemented by generative LLMs. To tackle the misordering issue, we develop a knowledge-guided
    
[^51]: 利用解缠的自监督学习和元学习增强医学图像分类的少样本学习

    Boosting Few-Shot Learning with Disentangled Self-Supervised Learning and Meta-Learning for Medical Image Classification

    [https://arxiv.org/abs/2403.17530](https://arxiv.org/abs/2403.17530)

    通过解缠的自监督学习和元学习，在低数据环境中训练医学图像分类模型，提高了模型的性能和泛化能力。

    

    背景和目标：在医学成像等关键领域使用深度学习模型面临着训练数据有限的挑战。我们提出了一种改进在低数据环境中训练模型性能和泛化能力的策略。方法：所提出的方法从一个预训练阶段开始，其中在自监督学习设置中学到的特征被解缠以提高表示对下游任务的鲁棒性。然后引入元微调步骤，利用元训练和元测试阶段之间的相关类别但变化粒度级别。这种方法旨在通过在元训练期间将模型暴露于更具挑战性的分类任务，然后在元测试期间评估它在更简单但在临床上更具重要性的任务上的表现，从而增强模型的泛化能力。我们展示了该方法的有效性。

    arXiv:2403.17530v1 Announce Type: cross  Abstract: Background and objective: Employing deep learning models in critical domains such as medical imaging poses challenges associated with the limited availability of training data. We present a strategy for improving the performance and generalization capabilities of models trained in low-data regimes. Methods: The proposed method starts with a pre-training phase, where features learned in a self-supervised learning setting are disentangled to improve the robustness of the representations for downstream tasks. We then introduce a meta-fine-tuning step, leveraging related classes between meta-training and meta-testing phases but varying the granularity level. This approach aims to enhance the model's generalization capabilities by exposing it to more challenging classification tasks during meta-training and evaluating it on easier tasks but holding greater clinical relevance during meta-testing. We demonstrate the effectiveness of the propo
    
[^52]: 为图形素描表示装备具有上下文感知的位置编码的素描补丁

    Equipping Sketch Patches with Context-Aware Positional Encoding for Graphic Sketch Representation

    [https://arxiv.org/abs/2403.17525](https://arxiv.org/abs/2403.17525)

    提出了一种通过为素描补丁配备上下文感知的位置编码来保护不同绘图版本的方法，将绘图顺序信息嵌入图节点中，以更好地学习图形素描表示。

    

    一幅素描的绘制顺序记录了它是如何逐笔由人类创建的。对于图形素描表示学习，最近的研究通过根据基于时间的最近邻策略将每个补丁与另一个相连，将素描绘图顺序注入到图边构建中。然而，这样构建的图边可能不可靠，因为素描可能有不同版本的绘图。在本文中，我们提出了一种经过变体绘制保护的方法，通过为素描补丁配备具有上下文感知的位置编码(PE)，以更好地利用绘图顺序来学习图形素描表示。我们没有将素描绘制注入到图边中，而是仅将这些顺序信息嵌入到图节点中。具体来说，每个补丁嵌入都配备有正弦绝对PE，以突出绘图顺序中的顺序位置。它的相邻补丁按self-att的价值排序

    arXiv:2403.17525v1 Announce Type: cross  Abstract: The drawing order of a sketch records how it is created stroke-by-stroke by a human being. For graphic sketch representation learning, recent studies have injected sketch drawing orders into graph edge construction by linking each patch to another in accordance to a temporal-based nearest neighboring strategy. However, such constructed graph edges may be unreliable, since a sketch could have variants of drawings. In this paper, we propose a variant-drawing-protected method by equipping sketch patches with context-aware positional encoding (PE) to make better use of drawing orders for learning graphic sketch representation. Instead of injecting sketch drawings into graph edges, we embed these sequential information into graph nodes only. More specifically, each patch embedding is equipped with a sinusoidal absolute PE to highlight the sequential position in the drawing order. And its neighboring patches, ranked by the values of self-att
    
[^53]: MapGuide: 从脑活动中重建连续语言的简单而有效方法

    MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities

    [https://arxiv.org/abs/2403.17516](https://arxiv.org/abs/2403.17516)

    本研究提出了一种直接比较预测文本嵌入的脑活动映射来指导文本重建的简单而有效方法，相比之前的间接方法显著提高了模型性能。

    

    从脑活动中解码连续语言是一项艰巨但有前景的研究领域。这对于帮助语言残障人士通过脑信号进行沟通尤为重要。本文提出了一种简单而有效的方法，通过直接将从脑活动映射的预测文本嵌入向导文本重建。全面的实验证明，我们的方法明显优于当前最先进的模型，在BLEU和METEOR分数上平均提高了77%和54%。

    arXiv:2403.17516v1 Announce Type: cross  Abstract: Decoding continuous language from brain activity is a formidable yet promising field of research. It is particularly significant for aiding people with speech disabilities to communicate through brain signals. This field addresses the complex task of mapping brain signals to text. The previous best attempt reverse-engineered this process in an indirect way: it began by learning to encode brain activity from text and then guided text generation by aligning with predicted brain responses. In contrast, we propose a simple yet effective method that guides text reconstruction by directly comparing them with the predicted text embeddings mapped from brain activities. Comprehensive experiments reveal that our method significantly outperforms the current state-of-the-art model, showing average improvements of 77% and 54% on BLEU and METEOR scores. We further validate the proposed modules through detailed ablation studies and case analyses and 
    
[^54]: 训练和推理期间的预测分享

    Prediction-sharing During Training and Inference

    [https://arxiv.org/abs/2403.17515](https://arxiv.org/abs/2403.17515)

    研究了在训练和推理期间的预测分享方面的新颖之处，并介绍和突出了不同类型的数据共享合同。

    

    两家公司参与竞争性预测任务。每家公司有两个数据来源 -- 有标签的历史数据和无标签的推理时间数据 -- 并且使用前者制定预测模型，使用后者对新实例进行预测。我们研究了公司之间的数据共享合同。我们研究的新颖之处在于介绍和突出仅分享预测模型的合同、仅分享推理时间预测的合同以及分享两者的合同之间的区别。我们的分析分为三个层面。首先，我们开发了一个便于进行研究的一般贝叶斯框架。其次，我们将焦点缩小到这个框架内的两个自然设置：(i) 每家公司的预测模型准确度是共知的，但各自模型之间的相关性未知；(ii) 存在两个关于最优预测器的假设，其中一个是

    arXiv:2403.17515v1 Announce Type: cross  Abstract: Two firms are engaged in a competitive prediction task. Each firm has two sources of data -- labeled historical data and unlabeled inference-time data -- and uses the former to derive a prediction model, and the latter to make predictions on new instances. We study data-sharing contracts between the firms. The novelty of our study is to introduce and highlight the differences between contracts that share prediction models only, contracts to share inference-time predictions only, and contracts to share both. Our analysis proceeds on three levels. First, we develop a general Bayesian framework that facilitates our study. Second, we narrow our focus to two natural settings within this framework: (i) a setting in which the accuracy of each firm's prediction model is common knowledge, but the correlation between the respective models is unknown; and (ii) a setting in which two hypotheses exist regarding the optimal predictor, and one of the
    
[^55]: 基于需求异味的自然语言需求可测试性度量

    Natural Language Requirements Testability Measurement Based on Requirement Smells

    [https://arxiv.org/abs/2403.17479](https://arxiv.org/abs/2403.17479)

    通过提出一个基于需求异味的数学模型，本文介绍了一种评估和排名自然语言需求可测试性的方法，有助于衡量和量化需求的可测试性。

    

    需求构成了定义软件系统义务和任务的基础。可测试的需求有助于防止失败，降低维护成本，并简化验收测试。然而，尽管衡量和量化需求可测试性的重要性，但尚未提出基于需求异味的自动化方法来衡量需求的可测试性。本文提出了一个数学模型，以评估和排名自然语言需求的可测试性，基于一个广泛的九个需求异味集合，自动检测，并根据需求长度和其应用领域来确定验收测试工作的努力。大多数异味源于不可数的形容词，上下文敏感和模糊词。需要一个全面的字典来检测这些词。我们提供了一种神经词嵌入技术来生成这样一个字典。

    arXiv:2403.17479v1 Announce Type: cross  Abstract: Requirements form the basis for defining software systems' obligations and tasks. Testable requirements help prevent failures, reduce maintenance costs, and make it easier to perform acceptance tests. However, despite the importance of measuring and quantifying requirements testability, no automatic approach for measuring requirements testability has been proposed based on the requirements smells, which are at odds with the requirements testability. This paper presents a mathematical model to evaluate and rank the natural language requirements testability based on an extensive set of nine requirements smells, detected automatically, and acceptance test efforts determined by requirement length and its application domain. Most of the smells stem from uncountable adjectives, context-sensitive, and ambiguous words. A comprehensive dictionary is required to detect such words. We offer a neural word-embedding technique to generate such a dic
    
[^56]: 一个统一的神经网络学习内核

    A Unified Kernel for Neural Network Learning

    [https://arxiv.org/abs/2403.17467](https://arxiv.org/abs/2403.17467)

    本文提出了统一神经内核(UNK)，可以描述神经网络的学习动态，并在有限的学习步骤下表现出类似于NTK的行为，当学习步骤逼近无穷大时收敛到NNGP。

    

    过去几十年来，人们对神经网络学习和内核学习之间的区别和联系表现出极大的兴趣。最近的进展在连接无限宽神经网络和高斯过程方面取得了理论上的进展。出现了两种主流方法：神经网络高斯过程(NNGP)和神经切向核(NTK)。前者基于贝叶斯推断，代表了零阶核，而后者基于梯度下降的切向空间，是第一阶核。在本文中，我们提出了统一神经内核(UNK)，该内核表征了神经网络在梯度下降和参数初始化中的学习动态。所提出的UNK内核保持了NNGP和NTK的极限特性，表现出类似于NTK的行为，但有有限的学习步骤，并且当学习步骤接近无穷大时收敛到NNGP。此外，我们还从理论上对UNK内核进行了分析。

    arXiv:2403.17467v1 Announce Type: cross  Abstract: Past decades have witnessed a great interest in the distinction and connection between neural network learning and kernel learning. Recent advancements have made theoretical progress in connecting infinite-wide neural networks and Gaussian processes. Two predominant approaches have emerged: the Neural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK). The former, rooted in Bayesian inference, represents a zero-order kernel, while the latter, grounded in the tangent space of gradient descents, is a first-order kernel. In this paper, we present the Unified Neural Kernel (UNK), which characterizes the learning dynamics of neural networks with gradient descents and parameter initialization. The proposed UNK kernel maintains the limiting properties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite learning step and converging to NNGP as the learning step approaches infinity. Besides, we also theoreticall
    
[^57]: LaRE^2: 基于潜在重构误差的扩散生成图像检测方法

    LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection

    [https://arxiv.org/abs/2403.17465](https://arxiv.org/abs/2403.17465)

    LaRE^2 提出了一种基于潜在重构误差的方法用于检测扩散生成的图像，通过引入潜在重构误差（LaRE）和误差引导特征细化模块（EGRE）实现了对特征的有效提取和增强，从而区分真实和生成图像。

    

    arXiv:2403.17465v1 类型：交叉 摘要：扩散模型的发展显著提高了图像生成质量，使真实图像和生成图像之间的区分变得越来越困难。尽管这一进展令人印象深刻，但也引发了重要的隐私和安全问题。为了解决这一问题，我们提出了一种新颖的基于潜在重构误差引导特征细化方法（LaRE^2）来检测扩散生成的图像。我们提出了潜在重构误差（LaRE），作为潜在空间中生成图像检测的第一个基于重构误差的特征。LaRE在特征提取效率方面超越了现有方法，同时保留了区分真假所需的关键线索。为了利用LaRE，我们提出了一种误差引导特征细化模块（EGRE），它可以通过LaRE引导的方式细化图像特征，以增强特征的区分能力。

    arXiv:2403.17465v1 Announce Type: cross  Abstract: The evolution of Diffusion Models has dramatically improved image generation quality, making it increasingly difficult to differentiate between real and generated images. This development, while impressive, also raises significant privacy and security concerns. In response to this, we propose a novel Latent REconstruction error guided feature REfinement method (LaRE^2) for detecting the diffusion-generated images. We come up with the Latent Reconstruction Error (LaRE), the first reconstruction-error based feature in the latent space for generated image detection. LaRE surpasses existing methods in terms of feature extraction efficiency while preserving crucial cues required to differentiate between the real and the fake. To exploit LaRE, we propose an Error-Guided feature REfinement module (EGRE), which can refine the image feature guided by LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an align-then-refine m
    
[^58]: 在强化学习中模仿受成本约束的行为

    Imitating Cost-Constrained Behaviors in Reinforcement Learning

    [https://arxiv.org/abs/2403.17456](https://arxiv.org/abs/2403.17456)

    该论文介绍了在强化学习中模仿受成本约束的行为的重要性，提出了模仿学习在受约束设置下的应用，并探讨了在实际领域中专家行为受限制因素影响的问题。

    

    长期以来，复杂的计划和调度问题一直通过各种优化或启发式方法来解决。最近，提出了从专家演示中学习的模仿学习作为解决这些问题的一种可行替代方法。模仿学习旨在通过观察专家的行为来学习奖励（或偏好）模型或直接行为策略。现有的模仿学习和逆向强化学习工作主要集中在无限制设置下的模仿（例如，车辆消耗的燃油量没有限制）。然而，在许多实际应用中，专家的行为不仅受奖励（或偏好）的影响，还受约束的影响。例如，自动驾驶送货车的决策不仅取决于路径偏好/奖励（根据过去的需求数据），还取决于车辆内的燃油和送达时间等约束。

    arXiv:2403.17456v1 Announce Type: cross  Abstract: Complex planning and scheduling problems have long been solved using various optimization or heuristic approaches. In recent years, imitation learning that aims to learn from expert demonstrations has been proposed as a viable alternative to solving these problems. Generally speaking, imitation learning is designed to learn either the reward (or preference) model or directly the behavioral policy by observing the behavior of an expert. Existing work in imitation learning and inverse reinforcement learning has focused on imitation primarily in unconstrained settings (e.g., no limit on fuel consumed by the vehicle). However, in many real-world domains, the behavior of an expert is governed not only by reward (or preference) but also by constraints. For instance, decisions on self-driving delivery vehicles are dependent not only on the route preferences/rewards (depending on past demand data) but also on the fuel in the vehicle and the ti
    
[^59]: 将指数平滑法融入MLP：一个简单但有效的序列模型

    Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model

    [https://arxiv.org/abs/2403.17445](https://arxiv.org/abs/2403.17445)

    将简单的指数平滑法与MLP结合，通过增加参数和复杂性，实现了与复杂S4模型可比较的结果

    

    在序列数据中建模长期依赖关系是序列学习中的关键步骤。最近发展的模型“结构化状态空间”（S4）在建模长期序列方面表现出显著的有效性。然而，尚不清楚S4的成功是因为其复杂的参数化和HiPPO初始化还是仅仅由于状态空间模型（SSMs）。为了进一步探讨深度SSMs的潜力，我们从简单的SSM指数平滑（ETS）开始，并通过直接将其融入逐元素MLP提出了一个叠加架构。我们通过增加额外的参数和复杂的字段来扩充简单的ETS以减少归纳偏差。尽管在逐元素MLP的参数增加不到1%的情况下，我们的模型在LRA基准测试上取得了与S4可比较的结果。

    arXiv:2403.17445v1 Announce Type: cross  Abstract: Modeling long-range dependencies in sequential data is a crucial step in sequence learning. A recently developed model, the Structured State Space (S4), demonstrated significant effectiveness in modeling long-range sequences. However, It is unclear whether the success of S4 can be attributed to its intricate parameterization and HiPPO initialization or simply due to State Space Models (SSMs). To further investigate the potential of the deep SSMs, we start with exponential smoothing (ETS), a simple SSM, and propose a stacked architecture by directly incorporating it into an element-wise MLP. We augment simple ETS with additional parameters and complex field to reduce the inductive bias. Despite increasing less than 1\% of parameters of element-wise MLP, our models achieve comparable results to S4 on the LRA benchmark.
    
[^60]: 通过症状划分和总结对齐大型语言模型以增强精神科访谈

    Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization

    [https://arxiv.org/abs/2403.17428](https://arxiv.org/abs/2403.17428)

    探讨了利用大型语言模型增强精神科访谈的方法，通过分析朝鲜叛逃者的咨询数据，研究LLMs在划分症状和总结压力因素和症状方面取得高性能。

    

    最近，大型语言模型（LLMs）的进展加速了它们在各个领域的应用。鉴于精神科访谈是专业面试者与被面试者之间目标导向和结构化对话，这是LLMs可以提供实质价值的最未被开发的领域之一。在这里，我们通过分析具有创伤经历和精神健康问题的朝鲜叛逃者的咨询数据，探讨了LLMs用于增强精神科访谈的用途。具体而言，我们研究LLMs是否能够（1）划分表示精神症状的对话部分并命名症状，以及（2）根据访谈对话记录总结压力因素和症状。这里，访谈数据由精神健康专家进行标记，用于训练和评估LLMs。我们的实验结果表明，适当提示的LLMs在症状划分和总结上可以实现高性能。

    arXiv:2403.17428v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have accelerated their usage in various domains. Given the fact that psychiatric interviews are goal-oriented and structured dialogues between the professional interviewer and the interviewee, it is one of the most underexplored areas where LLMs can contribute substantial value. Here, we explore the use of LLMs for enhancing psychiatric interviews, by analyzing counseling data from North Korean defectors with traumatic events and mental health issues. Specifically, we investigate whether LLMs can (1) delineate the part of the conversation that suggests psychiatric symptoms and name the symptoms, and (2) summarize stressors and symptoms, based on the interview dialogue transcript. Here, the transcript data was labeled by mental health experts for training and evaluation of LLMs. Our experimental results show that appropriately prompted LLMs can achieve high performance on both the sympto
    
[^61]: 基于知识的推荐系统改善饮食水足迹

    Knowledge-Powered Recommendation for an Improved Diet Water Footprint

    [https://arxiv.org/abs/2403.17426](https://arxiv.org/abs/2403.17426)

    该研究提出了一种基于知识图谱的推荐系统，旨在推广更健康的饮食习惯，减少环境影响，特别是水足迹。

    

    根据世界自然基金会（WWF）的数据，有11亿人缺乏水资源，27亿人每年至少有一个月经历水资源匮乏。到2025年，全球三分之二的人口可能面临水资源短缺。这突显了高效管理水资源使用的紧迫性，尤其是在食品等耗水行业。本文提出了一个基于知识图谱的推荐系统，旨在促进可持续和健康的食品消费。该系统在用户食谱中推荐成分替代品，从而改善营养价值并减少环境影响，特别是水足迹。系统架构包括源识别、信息提取、模式对齐、知识图谱构建和用户界面开发。这项研究为推广更健康的饮食习惯和促进节水工作提供了一种具有前景的工具。

    arXiv:2403.17426v1 Announce Type: new  Abstract: According to WWF, 1.1 billion people lack access to water, and 2.7 billion experience water scarcity at least one month a year. By 2025, two-thirds of the world's population may be facing water shortages. This highlights the urgency of managing water usage efficiently, especially in water-intensive sectors like food. This paper proposes a recommendation engine, powered by knowledge graphs, aiming to facilitate sustainable and healthy food consumption. The engine recommends ingredient substitutes in user recipes that improve nutritional value and reduce environmental impact, particularly water footprint. The system architecture includes source identification, information extraction, schema alignment, knowledge graph construction, and user interface development. The research offers a promising tool for promoting healthier eating habits and contributing to water conservation efforts.
    
[^62]: MA4DIV：用于搜索结果多样化的多智能体强化学习

    MA4DIV: Multi-Agent Reinforcement Learning for Search Result Diversification

    [https://arxiv.org/abs/2403.17421](https://arxiv.org/abs/2403.17421)

    引入了基于多智能体强化学习的MA4DIV方法，将搜索结果多样化建模为多个智能体之间的合作任务，直接优化多样性指标，如$\alpha$-NDCG，以实现高训练效率。

    

    搜索结果多样化（SRD）的目标是确保所选文档涵盖尽可能多的不同子主题。现有方法主要利用“贪婪选择”范式，即一次选择一个具有最高多样性分数的文档。这些方法往往效率低下，容易陷入次优状态。此外，一些其他方法旨在近似优化多样性指标，如$\alpha$-NDCG，但结果仍然不尽如人意。为了解决这些挑战，我们引入了用于搜索结果多样性的多智能体强化学习（MARL）方法，称为MA4DIV。在这种方法中，每个文档都是一个智能体，搜索结果多样化被建模为多个智能体之间的合作任务。该方法允许直接优化多样性指标，如$\alpha$-NDCG，同时实现高训练效率。我们进行了初步实验。

    arXiv:2403.17421v1 Announce Type: cross  Abstract: The objective of search result diversification (SRD) is to ensure that selected documents cover as many different subtopics as possible. Existing methods primarily utilize a paradigm of "greedy selection", i.e., selecting one document with the highest diversity score at a time. These approaches tend to be inefficient and are easily trapped in a suboptimal state. In addition, some other methods aim to approximately optimize the diversity metric, such as $\alpha$-NDCG, but the results still remain suboptimal. To address these challenges, we introduce Multi-Agent reinforcement learning (MARL) for search result DIVersity, which called MA4DIV. In this approach, each document is an agent and the search result diversification is modeled as a cooperative task among multiple agents. This approach allows for directly optimizing the diversity metrics, such as $\alpha$-NDCG, while achieving high training efficiency. We conducted preliminary experi
    
[^63]: AI安全性：必要，但不足和可能存在问题

    AI Safety: Necessary, but insufficient and possibly problematic

    [https://arxiv.org/abs/2403.17419](https://arxiv.org/abs/2403.17419)

    AI安全性炒作存在问题，可能会导致不利的监管努力并使有害AI合法化。

    

    本文对近期围绕AI安全性的炒作进行了批判性审视。我们首先指出AI安全性炒作的性质是由政府和企业主导，并将其与AI研究中其他关于推进社会利益的途径进行对比。我们考虑了“AI安全性”实际上意味着什么，并概述了AI安全性数字足迹所符合的主流概念。我们认为AI安全性与透明度等与社会利益相关联的概念之间存在微妙且不稳定的关系，表明如果目标是广义上的社会利益，则它是一个不充分的概念。我们指出，AI安全性的讨论已经影响了一些AI监管努力，也许是朝着不太理想的方向。我们还分享了对AI安全性如何使通过给予剥削性和有害AI一层安全外衣来促使结构性伤害的AI合法化的担忧。

    arXiv:2403.17419v1 Announce Type: new  Abstract: This article critically examines the recent hype around AI safety. We first start with noting the nature of the AI safety hype as being dominated by governments and corporations, and contrast it with other avenues within AI research on advancing social good. We consider what 'AI safety' actually means, and outline the dominant concepts that the digital footprint of AI safety aligns with. We posit that AI safety has a nuanced and uneasy relationship with transparency and other allied notions associated with societal good, indicating that it is an insufficient notion if the goal is that of societal good in a broad sense. We note that the AI safety debate has already influenced some regulatory efforts in AI, perhaps in not so desirable directions. We also share our concerns on how AI safety may normalize AI that advances structural harm through providing exploitative and harmful AI with a veneer of safety.
    
[^64]: 论排列不变神经网络

    On permutation-invariant neural networks

    [https://arxiv.org/abs/2403.17410](https://arxiv.org/abs/2403.17410)

    神经网络如Deep Sets和Transformers的出现显著推动了基于集合的数据处理的进展

    

    传统机器学习算法通常在假设输入数据遵循基于向量的格式的前提下设计，着重于基于向量的范式。然而，随着需求涉及基于集合的任务的增长，研究界对解决这些挑战的兴趣发生了范式转变。近年来，Deep Sets和Transformers等神经网络架构的出现在处理基于集合的数据方面取得了重大进展。这些架构专门设计为自然容纳集合作为输入，从而更有效地表示和处理集合结构。因此，近年来出现了大量致力于探索和利用这些架构能力的研究努力，以逼近集合函数的各种任务。这项综合调查旨在概述th

    arXiv:2403.17410v1 Announce Type: cross  Abstract: Conventional machine learning algorithms have traditionally been designed under the assumption that input data follows a vector-based format, with an emphasis on vector-centric paradigms. However, as the demand for tasks involving set-based inputs has grown, there has been a paradigm shift in the research community towards addressing these challenges. In recent years, the emergence of neural network architectures such as Deep Sets and Transformers has presented a significant advancement in the treatment of set-based data. These architectures are specifically engineered to naturally accommodate sets as input, enabling more effective representation and processing of set structures. Consequently, there has been a surge of research endeavors dedicated to exploring and harnessing the capabilities of these architectures for various tasks involving the approximation of set functions. This comprehensive survey aims to provide an overview of th
    
[^65]: 使用区域指导标记将孟加拉文本与地方方言转录为国际音标

    Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens

    [https://arxiv.org/abs/2403.17407](https://arxiv.org/abs/2403.17407)

    通过引入区域指导标记技术，本文提出了一种将孟加拉文本与地方方言转录为国际音标的方法，为模型提供了关于输入文本的地区方言信息，以理解与每个地区相关的独特音韵模式。

    

    孟加拉文本到国际音标（IPA）的准确转录是一项具有挑战性的任务，主要是由于语言的复杂音韵学和语境相关的音变。对于区域孟加拉方言来说，由于缺乏针对这些方言的标准拼写约定、当地和外语在这些地区中流行的词汇以及不同地区之间的音韵多样性，这一挑战甚至更为严峻。本文提出了一种方法来解决这个序列到序列的问题，即在覆盖孟加拉国六个地区的新数据集上引入“区域指导标记”（DGT）技术。其关键思想是在生成IPA转录之前向模型提供有关输入文本的区域方言或“地区”的明确信息。这通过在输入序列前添加一个地区标记来实现，有效地引导模型理解与每个地区相关的独特音韵模式。

    arXiv:2403.17407v1 Announce Type: cross  Abstract: Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or "district" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each 
    
[^66]: 一个开源的端到端逻辑优化框架，用于具有强化学习的大规模布尔网络

    An Open-source End-to-End Logic Optimization Framework for Large-scale Boolean Network with Reinforcement Learning

    [https://arxiv.org/abs/2403.17395](https://arxiv.org/abs/2403.17395)

    提出了一个开源的端到端逻辑优化框架，用于大规模布尔网络，并应用了强化学习。

    

    我们提出了一个用于具有强化学习的大规模布尔网络的开源端到端逻辑优化框架。

    arXiv:2403.17395v1 Announce Type: new  Abstract: We propose an open-source end-to-end logic optimization framework for large-scale boolean network with reinforcement learning.
    
[^67]: ELLEN: 非常轻监督学习用于高效命名实体识别

    ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition

    [https://arxiv.org/abs/2403.17385](https://arxiv.org/abs/2403.17385)

    ELLEN是一种简单而强大的神经符号方法，将微调语言模型与语言规则相结合，在极其轻监督的情况下取得了非常强劲的命名实体识别性能。

    

    在这项工作中，我们重新审视了半监督命名实体识别（NER）问题，侧重于极其轻量级的监督，包括仅包含每类别10个示例的词汇表。我们引入了ELLEN，这是一种简单、完全模块化的神经符号方法，它将经过微调的语言模型与语言规则相结合。这些规则包括“一个话语一个意义”这样的见解，使用掩码语言模型作为无监督NER，利用词性标签识别和消除未标记实体作为假负例，以及关于分类器置信度得分在局部和全局背景下的其他直觉。在使用上述词汇表极小监督的情况下，ELLEN在CoNLL-2003数据集上取得了非常强大的性能。它还在文献中常用的相同监督设置（即，训练数据的5%）下，优于大多数现有（且更为复杂）的半监督NER方法。

    arXiv:2403.17385v1 Announce Type: cross  Abstract: In this work, we revisit the problem of semi-supervised named entity recognition (NER) focusing on extremely light supervision, consisting of a lexicon containing only 10 examples per class. We introduce ELLEN, a simple, fully modular, neuro-symbolic method that blends fine-tuned language models with linguistic rules. These rules include insights such as ''One Sense Per Discourse'', using a Masked Language Model as an unsupervised NER, leveraging part-of-speech tags to identify and eliminate unlabeled entities as false negatives, and other intuitions about classifier confidence scores in local and global context. ELLEN achieves very strong performance on the CoNLL-2003 dataset when using the minimal supervision from the lexicon above. It also outperforms most existing (and considerably more complex) semi-supervised NER methods under the same supervision settings commonly used in the literature (i.e., 5% of the training data). Further, 
    
[^68]: 可解释的图神经网络用于大气状态估计中的观测影响分析

    Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation

    [https://arxiv.org/abs/2403.17384](https://arxiv.org/abs/2403.17384)

    研究利用图神经网络和可解释性方法分析观测在大气状态估计中的影响，通过对不同观测的重要性进行量化，提高了对天气预报中观测数据的理解和优化。

    

    这篇论文利用图神经网络（GNN）和可解释性方法，研究了观测对天气预报系统中大气状态估计的影响。我们将观测和数值天气预报（NWP）点集成到气象图中，提取以NWP点为中心的$k$-跳子图。采用自监督GNN来通过聚合这些$k$-跳半径内的数据来估计大气状态。研究应用基于梯度的可解释性方法来量化不同观测在估计过程中的重要性。通过对来自11个卫星和陆地观测数据的评估，结果突出了视觉化观测类型重要性的有效性，增强了对天气预报中观测数据的理解和优化。

    arXiv:2403.17384v1 Announce Type: new  Abstract: This paper investigates the impact of observations on atmospheric state estimation in weather forecasting systems using graph neural networks (GNNs) and explainability methods. We integrate observation and Numerical Weather Prediction (NWP) points into a meteorological graph, extracting $k$-hop subgraphs centered on NWP points. Self-supervised GNNs are employed to estimate the atmospheric state by aggregating data within these $k$-hop radii. The study applies gradient-based explainability methods to quantify the significance of different observations in the estimation process. Evaluated with data from 11 satellite and land-based observations, the results highlight the effectiveness of visualizing the importance of observation types, enhancing the understanding and optimization of observational data in weather forecasting.
    
[^69]: 机器学习中的应用驱动创新

    Application-Driven Innovation in Machine Learning

    [https://arxiv.org/abs/2403.17381](https://arxiv.org/abs/2403.17381)

    应用驱动研究在机器学习领域具有重要影响，可以与方法驱动研究有益地协同，但目前审查、招聘和教学实践往往阻碍了这种创新。

    

    随着机器学习应用的不断增长，受特定现实挑战启发的创新算法变得日益重要。这样的工作不仅在应用领域具有重要影响，也在机器学习本身具有重要影响。本文描述了机器学习中应用驱动研究的范式，将其与更标准的方法驱动研究进行了对比。我们阐明了应用驱动机器学习的好处，以及这种方法如何可以与方法驱动工作有益地协同。尽管具有这些好处，我们发现机器学习中的审查、招聘和教学实践往往阻碍了应用驱动创新。我们概述了如何改进这些流程。

    arXiv:2403.17381v1 Announce Type: cross  Abstract: As applications of machine learning proliferate, innovative algorithms inspired by specific real-world challenges have become increasingly important. Such work offers the potential for significant impact not merely in domains of application but also in machine learning itself. In this paper, we describe the paradigm of application-driven research in machine learning, contrasting it with the more standard paradigm of methods-driven research. We illustrate the benefits of application-driven machine learning and how this approach can productively synergize with methods-driven work. Despite these benefits, we find that reviewing, hiring, and teaching practices in machine learning often hold back application-driven innovation. We outline how these processes may be improved.
    
[^70]: 探索和应用基于音频的情感分析在音乐中

    Exploring and Applying Audio-Based Sentiment Analysis in Music

    [https://arxiv.org/abs/2403.17379](https://arxiv.org/abs/2403.17379)

    本文探讨了基于音频的情感分析在音乐中的运用，通过预测音乐片段随时间的情感变化以及确定音乐时间序列中下一个情感值来实现无缝过渡。

    

    情感分析是文本处理中不断探索的领域，涉及对文本的意见、情感和主观性的计算分析。然而，这个想法不仅限于文本和语音，事实上，它也可以应用于其他形式。实际上，人类在音乐中表达自己的深度不如在文本中。计算模型解释音乐情感的能力在很大程度上尚未被探索，可能对治疗和音乐播放等方面产生影响和用途。本文涉及两个独立任务。该研究旨在(1)预测音乐片段随时间的情感，以及(2)在时间序列中确定音乐后的下一个情感值，以确保无缝过渡。利用包含从Free Music Archive中选中并用Russel的af圆环模型报告的愉悦和激活水平注释的歌曲片段的Emotions in Music数据库的数据。

    arXiv:2403.17379v1 Announce Type: cross  Abstract: Sentiment analysis is a continuously explored area of text processing that deals with the computational analysis of opinions, sentiments, and subjectivity of text. However, this idea is not limited to text and speech, in fact, it could be applied to other modalities. In reality, humans do not express themselves in text as deeply as they do in music. The ability of a computational model to interpret musical emotions is largely unexplored and could have implications and uses in therapy and musical queuing. In this paper, two individual tasks are addressed. This study seeks to (1) predict the emotion of a musical clip over time and (2) determine the next emotion value after the music in a time series to ensure seamless transitions. Utilizing data from the Emotions in Music Database, which contains clips of songs selected from the Free Music Archive annotated with levels of valence and arousal as reported on Russel's circumplex model of af
    
[^71]: 具有扰动注意力引导的自矫正扩散抽样

    Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance

    [https://arxiv.org/abs/2403.17377](https://arxiv.org/abs/2403.17377)

    提出了一种名为扰动注意力引导（PAG）的新型抽样引导技术，通过在扩散 U-Net 中替换自注意力映射来生成结构降级的中间样本，从而在无条件和有条件设置下改善扩散样本质量。

    

    近期研究表明，扩散模型能够生成高质量样本，但其质量很大程度上依赖于抽样引导技术，比如分类器引导（CG）和无分类器引导（CFG）。这些技术通常在无条件生成或各种下游任务如图像恢复中无法应用。本文提出了一种新颖的抽样引导技术，称为扰动注意力引导（PAG），它改进了扩散样本的质量，不管是在无条件还是有条件的设置中，都能实现这一目标，而不需要额外训练或整合外部模块。PAG 旨在通过整个去噪过程逐步增强样本的结构。它涉及通过用恒等矩阵替换扩散 U-Net 中选择的自注意力映射生成结构降级的中间样本，考虑自注意力机制。

    arXiv:2403.17377v1 Announce Type: cross  Abstract: Recent studies have demonstrated that diffusion models are capable of generating high-quality samples, but their quality heavily depends on sampling guidance techniques, such as classifier guidance (CG) and classifier-free guidance (CFG). These techniques are often not applicable in unconditional generation or in various downstream tasks such as image restoration. In this paper, we propose a novel sampling guidance, called Perturbed-Attention Guidance (PAG), which improves diffusion sample quality across both unconditional and conditional settings, achieving this without requiring additional training or the integration of external modules. PAG is designed to progressively enhance the structure of samples throughout the denoising process. It involves generating intermediate samples with degraded structure by substituting selected self-attention maps in diffusion U-Net with an identity matrix, by considering the self-attention mechanisms
    
[^72]: AIDE：用于自动驾驶目标检测的自动数据引擎

    AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving

    [https://arxiv.org/abs/2403.17373](https://arxiv.org/abs/2403.17373)

    提出了使用自动数据引擎（AIDE）的方法，通过自动识别问题、高效筛选数据、自动标注改进模型、生成多样化场景验证模型，在自动驾驶领域取得了优异的性能。

    

    arXiv:2403.17373v1 公告类型:跨领域 摘要:自动驾驶汽车（AV）系统依赖稳健的感知模型作为安全保障的基石。然而，在道路上遇到的物体呈现长尾分布，罕见或未见类别对部署的感知模型构成挑战。这需要通过昂贵的过程不断地筛选和标注数据，需要人力的巨大投入。我们提出利用最近在视觉语言和大型语言模型方面的进展，设计一个自动数据引擎（AIDE），自动识别问题，高效筛选数据，通过自动标注改进模型，并通过生成多样化场景验证模型。这一过程可以迭代进行，允许模型不断自我改进。我们进一步建立了一个开放世界检测基准，以全面评估各种学习范式，在AV数据集上展示我们方法在降低方面的卓越表现。

    arXiv:2403.17373v1 Announce Type: cross  Abstract: Autonomous vehicle (AV) systems rely on robust perception models as a cornerstone of safety assurance. However, objects encountered on the road exhibit a long-tailed distribution, with rare or unseen categories posing challenges to a deployed perception model. This necessitates an expensive process of continuously curating and annotating data with significant human effort. We propose to leverage recent advances in vision-language and large language models to design an Automatic Data Engine (AIDE) that automatically identifies issues, efficiently curates data, improves the model through auto-labeling, and verifies the model through generation of diverse scenarios. This process operates iteratively, allowing for continuous self-improvement of the model. We further establish a benchmark for open-world detection on AV datasets to comprehensively evaluate various learning paradigms, demonstrating our method's superior performance at a reduc
    
[^73]: ChatGPT将自然语言解释质量评级定为与人类相似：但是基于哪些标准？

    ChatGPT Rates Natural Language Explanation Quality Like Humans: But on Which Scales?

    [https://arxiv.org/abs/2403.17368](https://arxiv.org/abs/2403.17368)

    本研究探索了ChatGPT在不同尺度下与人类评估之间的一致性，并发现在较粗粒度的尺度上，ChatGPT与人类更加一致。

    

    随着人工智能在我们的生活中变得越来越重要，透明度和责任性的需求也在增长。虽然自然语言解释（NLEs）对澄清人工智能决策背后的推理至关重要，但通过人类判断对其进行评估由于主观性和对细粒度评分的需求而变得复杂且资源密集。本研究探讨了ChatGPT与人类评估之间在多个尺度（即二元、三元和7-Likert尺度）上的一致性。我们从三个NLE数据集中抽取300个数据实例，并为信息量和清晰度两个文本质量度量收集了900个人类注释。我们进一步在不同主观评分范围下进行了成对比较实验，其中基线来自8,346个人类注释。我们的结果显示，在更粗粒度的尺度上，ChatGPT与人类更加一致。此外，成对比较和动态提示（即提供语义上）

    arXiv:2403.17368v1 Announce Type: cross  Abstract: As AI becomes more integral in our lives, the need for transparency and responsibility grows. While natural language explanations (NLEs) are vital for clarifying the reasoning behind AI decisions, evaluating them through human judgments is complex and resource-intensive due to subjectivity and the need for fine-grained ratings. This study explores the alignment between ChatGPT and human assessments across multiple scales (i.e., binary, ternary, and 7-Likert scale). We sample 300 data instances from three NLE datasets and collect 900 human annotations for both informativeness and clarity scores as the text quality measurement. We further conduct paired comparison experiments under different ranges of subjectivity scores, where the baseline comes from 8,346 human annotations. Our results show that ChatGPT aligns better with humans in more coarse-grained scales. Also, paired comparisons and dynamic prompting (i.e., providing semantically 
    
[^74]: 将文本和表格世界联系起来进行事实验证：一种轻量级基于注意力的模型

    Bridging Textual and Tabular Worlds for Fact Verification: A Lightweight, Attention-Based Model

    [https://arxiv.org/abs/2403.17361](https://arxiv.org/abs/2403.17361)

    提出了一种简单而强大的模型，通过利用预训练模型和轻量级的基于注意力的机制，有效地利用不同数据类型之间的潜在连接，保留原始证据的上下文，确保预测准确性。

    

    FEVEROUS是一个关注涉及非结构化文本和结构化表格数据的事实提取和验证任务的基准和研究项目。在FEVEROUS中，现有的工作通常依赖于大量的预处理并利用基于规则的数据转换，这可能导致潜在的上下文丢失或误导性编码。本文介绍了一种简单而强大的模型，消除了模态转换的需求，从而保留了原始证据的上下文。通过利用多样化的文本和表格数据集上预训练的模型，并结合轻量级的基于注意力机制，我们的方法有效地利用了不同数据类型之间的潜在连接，从而产生全面且可靠的判断预测。该模型的模块化结构巧妙地管理多模态信息，确保原始证据的完整性和真实性不受损害。比较分析表明，

    arXiv:2403.17361v1 Announce Type: cross  Abstract: FEVEROUS is a benchmark and research initiative focused on fact extraction and verification tasks involving unstructured text and structured tabular data. In FEVEROUS, existing works often rely on extensive preprocessing and utilize rule-based transformations of data, leading to potential context loss or misleading encodings. This paper introduces a simple yet powerful model that nullifies the need for modality conversion, thereby preserving the original evidence's context. By leveraging pre-trained models on diverse text and tabular datasets and by incorporating a lightweight attention-based mechanism, our approach efficiently exploits latent connections between different data types, thereby yielding comprehensive and reliable verdict predictions. The model's modular structure adeptly manages multi-modal information, ensuring the integrity and authenticity of the original evidence are uncompromised. Comparative analyses reveal that ou
    
[^75]: 处理具有递归双上升的近视受限POMDP规划

    Addressing Myopic Constrained POMDP Planning with Recursive Dual Ascent

    [https://arxiv.org/abs/2403.17358](https://arxiv.org/abs/2403.17358)

    引入历史依赖性双变量，并通过递归双向上升进行优化，在大型受限部分可观察马尔可夫决策过程中改进了探索能力和结果安全性。

    

    Lagrangian-guided Monte Carlo tree search with global dual ascent已应用于在线解决大规模的受限部分可观察马尔可夫决策过程（CPOMDPs）。在这项工作中，我们证明这些全局双上升参数可能导致探索期间的近视行为选择，最终导致次优的决策制定。为了解决这个问题，我们引入了引导本地动作选择并通过递归双向上升进行优化的历史依赖性双变量。我们在一个激励性的玩具示例和两个大型CPOMDP上进行了实证比较，展示了改进的探索能力，最终导致更安全的结果。

    arXiv:2403.17358v1 Announce Type: new  Abstract: Lagrangian-guided Monte Carlo tree search with global dual ascent has been applied to solve large constrained partially observable Markov decision processes (CPOMDPs) online. In this work, we demonstrate that these global dual parameters can lead to myopic action selection during exploration, ultimately leading to suboptimal decision making. To address this, we introduce history-dependent dual variables that guide local action selection and are optimized with recursive dual ascent. We empirically compare the performance of our approach on a motivating toy example and two large CPOMDPs, demonstrating improved exploration, and ultimately, safer outcomes.
    
[^76]: MESIA: 理解和利用方法级注释的补充性质用于生成自动注释

    MESIA: Understanding and Leveraging Supplementary Nature of Method-level Comments for Automatic Comment Generation

    [https://arxiv.org/abs/2403.17357](https://arxiv.org/abs/2403.17357)

    提出了一种新的度量标准MESIA，用于评估代码注释提供的补充信息的程度，实验证明了其在生成方法级注释时的价值。

    

    代码注释对于开发人员在程序理解中至关重要。在理解和重用方法的场景中，开发人员期望代码注释提供超出方法签名的补充信息。然而，不同代码注释中这种补充信息的程度差异很大。本文提出了方法级注释的补充性质的认识，并提出了一种名为MESIA（平均补充信息量）的新度量标准，用于评估代码注释提供的补充信息的程度。借助MESIA度量标准，我们在一个流行的代码注释数据集上进行了实验，并使用三种常见类型的神经方法生成方法级注释。我们的实验结果证明了我们提出的工作的价值，并得出了一些发现。

    arXiv:2403.17357v1 Announce Type: cross  Abstract: Code comments are important for developers in program comprehension. In scenarios of comprehending and reusing a method, developers expect code comments to provide supplementary information beyond the method signature. However, the extent of such supplementary information varies a lot in different code comments. In this paper, we raise the awareness of the supplementary nature of method-level comments and propose a new metric named MESIA (Mean Supplementary Information Amount) to assess the extent of supplementary information that a code comment can provide. With the MESIA metric, we conduct experiments on a popular code-comment dataset and three common types of neural approaches to generate method-level comments. Our experimental results demonstrate the value of our proposed work with a number of findings. (1) Small-MESIA comments occupy around 20% of the dataset and mostly fall into only the WHAT comment category. (2) Being able to p
    
[^77]: Zodiac杀手340字符密码的解决方案

    The Solution of the Zodiac Killer's 340-Character Cipher

    [https://arxiv.org/abs/2403.17350](https://arxiv.org/abs/2403.17350)

    本文详细描述了Zodiac杀手第二个密码的解密过程，该密码使用了转位和同音替换密码，并具有不寻常特质。

    

    Zodiac杀手案件是历史上最广为人知的未解决连环杀手案件之一。这名身份不明的凶手杀害了五名已知受害者，并恐吓了加利福尼亚州。他还与新闻界和执法部门进行了广泛的沟通。除了他的谋杀行为外，Zodiac还因使用密码而闻名。第一个Zodiac密码在发布后一周内被解决，而第二个密码在51年后被作者解决，发现它是一种具有不寻常特质的转位和同音替换密码。本文详细介绍了这个密码的历史意义以及导致其解密的诸多努力。

    arXiv:2403.17350v1 Announce Type: new  Abstract: The case of the Zodiac Killer is one of the most widely known unsolved serial killer cases in history. The unidentified killer murdered five known victims and terrorized the state of California. He also communicated extensively with the press and law enforcement. Besides his murders, Zodiac was known for his use of ciphers. The first Zodiac cipher was solved within a week of its publication, while the second cipher was solved by the authors after 51 years, when it was discovered to be a transposition and homophonic substitution cipher with unusual qualities. In this paper, we detail the historical significance of this cipher and the numerous efforts which culminated in its solution.
    
[^78]: ICCV 2023第一个科学图像字幕挑战的解决方案

    The Solution for the ICCV 2023 1st Scientific Figure Captioning Challenge

    [https://arxiv.org/abs/2403.17342](https://arxiv.org/abs/2403.17342)

    我们提出了一种根据论文内容生成图像标题的解决方案，并通过使用PaddleOCR和LLaMA工具包解决了OCR信息和信息过滤方面的问题。

    

    在本文中，我们提出了一种解决方案，用于改善论文中生成的图像标题的质量。我们采用总结论文中的文本内容生成图像标题的方法。在我们的研究过程中，我们发现官方数据集中提供的OCR信息存在差异。为了纠正这一问题，我们使用PaddleOCR工具包从所有图像中提取OCR信息。此外，我们观察到官方文件中的某些文本内容与不适合进行标题处理的图像相关，从而在生成标题过程中引入噪音。为了减轻这一问题，我们利用LLaMA根据图像提及查询文本内容来提取特定于图像的信息，有效地过滤掉多余信息。此外，我们认识到在文本生成过程中主要使用最大似然估计及用于评估的指标（如ROUGE）之间存在差异。

    arXiv:2403.17342v1 Announce Type: cross  Abstract: In this paper, we propose a solution for improving the quality of captions generated for figures in papers. We adopt the approach of summarizing the textual content in the paper to generate image captions. Throughout our study, we encounter discrepancies in the OCR information provided in the official dataset. To rectify this, we employ the PaddleOCR toolkit to extract OCR information from all images. Moreover, we observe that certain textual content in the official paper pertains to images that are not relevant for captioning, thereby introducing noise during caption generation. To mitigate this issue, we leverage LLaMA to extract image-specific information by querying the textual content based on image mentions, effectively filtering out extraneous information. Additionally, we recognize a discrepancy between the primary use of maximum likelihood estimation during text generation and the evaluation metrics such as ROUGE employed to a
    
[^79]: 使用自适应控制屏障函数的基于强化学习的滚动视野控制用于安全关键系统

    Reinforcement Learning-based Receding Horizon Control using Adaptive Control Barrier Functions for Safety-Critical Systems

    [https://arxiv.org/abs/2403.17338](https://arxiv.org/abs/2403.17338)

    提出了一种基于强化学习的滚动视野控制方法，利用自适应控制屏障函数，以解决安全关键系统中性能和可行性受影响的问题

    

    最优控制方法为安全关键问题提供解决方案，但很容易变得棘手。控制屏障函数(CBFs)作为一种流行技术出现，通过其前向不变性属性，有利于通过在损失一些性能的情况下，显式地保证安全。该方法涉及定义性能目标以及必须始终执行的基于CBF的安全约束。遗憾的是，两个关键因素可能会对性能和解决方案的可行性产生显著影响：(i)成本函数及其相关参数的选择，以及(ii)在CBF约束内进行参数校准，捕捉性能和保守性之间的折衷，以及不可行性。为了解决这些挑战，我们提出了一种利用模型预测控制(MPC)的强化学习(RL)滚动视野控制(RHC)方法。

    arXiv:2403.17338v1 Announce Type: cross  Abstract: Optimal control methods provide solutions to safety-critical problems but easily become intractable. Control Barrier Functions (CBFs) have emerged as a popular technique that facilitates their solution by provably guaranteeing safety, through their forward invariance property, at the expense of some performance loss. This approach involves defining a performance objective alongside CBF-based safety constraints that must always be enforced. Unfortunately, both performance and solution feasibility can be significantly impacted by two key factors: (i) the selection of the cost function and associated parameters, and (ii) the calibration of parameters within the CBF-based constraints, which capture the trade-off between performance and conservativeness. %as well as infeasibility. To address these challenges, we propose a Reinforcement Learning (RL)-based Receding Horizon Control (RHC) approach leveraging Model Predictive Control (MPC) with
    
[^80]: 人工智能模型中的公平追求：一项调查

    The Pursuit of Fairness in Artificial Intelligence Models: A Survey

    [https://arxiv.org/abs/2403.17333](https://arxiv.org/abs/2403.17333)

    人工智能模型中的公平追求至关重要，研究人员努力解决偏见问题，确保模型不会有意或无意地对某些群体产生偏见。

    

    现在，人工智能（AI）模型被广泛应用于医疗、教育和就业等方方面面。由于它们被应用于许多敏感环境，并做出可能改变人生的决策，潜在的偏见结果成为一个紧迫的问题。开发人员应确保这类模型不会表现出任何意外的歧视行为，比如偏爱某些性别、种族或残疾人士。随着人工智能系统的普遍传播，研究人员和从业者对不公平模型越来越有意识，并致力于减少其中的偏见。在解决这些问题方面已经进行了重要研究，以确保模型不会有意或无意地延续偏见。这项调查概述了研究人员如何促进人工智能系统的公平性。我们探讨了当前文献中存在的公平性不同定义。

    arXiv:2403.17333v1 Announce Type: new  Abstract: Artificial Intelligence (AI) models are now being utilized in all facets of our lives such as healthcare, education and employment. Since they are used in numerous sensitive environments and make decisions that can be life altering, potential biased outcomes are a pressing matter. Developers should ensure that such models don't manifest any unexpected discriminatory practices like partiality for certain genders, ethnicities or disabled people. With the ubiquitous dissemination of AI systems, researchers and practitioners are becoming more aware of unfair models and are bound to mitigate bias in them. Significant research has been conducted in addressing such issues to ensure models don't intentionally or unintentionally perpetuate bias. This survey offers a synopsis of the different ways researchers have promoted fairness in AI systems. We explore the different definitions of fairness existing in the current literature. We create a compr
    
[^81]: 深度支持向量

    Deep Support Vectors

    [https://arxiv.org/abs/2403.17329](https://arxiv.org/abs/2403.17329)

    该论文探索了深度学习模型中的深度支持向量（DSVs）的概念，介绍了DeepKKT条件，通过实证研究发现DSVs与SVM中的支持向量类似，为解释模型决策标准提供了方法，同时证明了可以有效地使用DSVs重构模型。

    

    尽管深度学习的成功通常被归因于其与支持向量机（SVM）在理论上的等价性，但这种关系的实际影响尚未得到全面探讨。本文在这一领域开展了一项探索，重点关注深度学习模型中深度支持向量（DSVs）的识别。我们引入了DeepKKT条件的概念，这是一种专为深度学习量身定制的传统Karush-Kuhn-Tucker（KKT）条件的调整版本。通过实证研究，我们阐明了DSVs与SVM中的支持向量之间存在相似性，提供了一种解释模型决策标准的切实方法。此外，我们的研究结果表明，可以有效地使用DSVs重构模型，类似于SVM中的过程。代码将会公开。

    arXiv:2403.17329v1 Announce Type: cross  Abstract: While the success of deep learning is commonly attributed to its theoretical equivalence with Support Vector Machines (SVM), the practical implications of this relationship have not been thoroughly explored. This paper pioneers an exploration in this domain, specifically focusing on the identification of Deep Support Vectors (DSVs) within deep learning models. We introduce the concept of DeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT) conditions tailored for deep learning. Through empirical investigations, we illustrate that DSVs exhibit similarities to support vectors in SVM, offering a tangible method to interpret the decision-making criteria of models. Additionally, our findings demonstrate that models can be effectively reconstructed using DSVs, resembling the process in SVM. The code will be available.
    
[^82]: 通过遗传编程学习交通信号控制

    Learning Traffic Signal Control via Genetic Programming

    [https://arxiv.org/abs/2403.17328](https://arxiv.org/abs/2403.17328)

    提出了一种新的基于学习的方法用于解决复杂交叉路口的信号控制问题，通过设计阶段紧急性概念和可解释的树结构，可以在信号转换期间选择激活的信号相位。

    

    交通信号控制对提高交通效率至关重要。最近，基于学习的方法，特别是深度强化学习（DRL），在寻求更有效的交通信号控制策略方面取得了巨大成功。然而，在DRL中奖励的设计高度依赖领域知识才能收敛到有效策略，而最终策略也存在解释困难。在本工作中，提出了一种新的面向复杂路口的信号控制的学习方法。在我们的方法中，我们为每个信号相设计了一个阶段紧急性的概念。在信号变换期间，交通灯控制策略根据阶段紧急性选择要激活的下一个相位。然后，我们提出将紧急功能表示为可解释的树结构。紧急功能可以根据当前道路条件为特定相位计算相位紧急性。

    arXiv:2403.17328v1 Announce Type: new  Abstract: The control of traffic signals is crucial for improving transportation efficiency. Recently, learning-based methods, especially Deep Reinforcement Learning (DRL), garnered substantial success in the quest for more efficient traffic signal control strategies. However, the design of rewards in DRL highly demands domain knowledge to converge to an effective policy, and the final policy also presents difficulties in terms of explainability. In this work, a new learning-based method for signal control in complex intersections is proposed. In our approach, we design a concept of phase urgency for each signal phase. During signal transitions, the traffic light control strategy selects the next phase to be activated based on the phase urgency. We then proposed to represent the urgency function as an explainable tree structure. The urgency function can calculate the phase urgency for a specific phase based on the current road conditions. Genetic 
    
[^83]: JMultiWOZ：一个大规模的日语多领域任务驱动对话数据集

    JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset

    [https://arxiv.org/abs/2403.17319](https://arxiv.org/abs/2403.17319)

    JMultiWOZ是第一个日语大规模多领域任务驱动对话数据集，通过评估与现有英语基准数据集相媲美的对话状态跟踪和回复生成能力，推动了日语任务驱动对话系统的研究与发展。

    

    对话数据集对基于深度学习的任务驱动对话系统研究至关重要。虽然已经开发了许多英语多领域任务驱动对话数据集，并为任务驱动对话系统的显著进展做出贡献，但日语中并不存在这样的数据集，并且与英语领域相比，这一领域的研究还很有限。为了推动日语任务驱动对话系统研究与开发的进展，本研究构建了JMultiWOZ，这是第一个日语大规模多领域任务驱动对话数据集。利用JMultiWOZ，我们评估了现有英语基准数据集MultiWOZ2.2和最新大型语言模型（LLM）方法上的对话状态跟踪和回复生成能力。我们的评估结果表明，JMultiWOZ提供了一个与MultiWOZ2相媲美的基准数据集。

    arXiv:2403.17319v1 Announce Type: cross  Abstract: Dialogue datasets are crucial for deep learning-based task-oriented dialogue system research. While numerous English language multi-domain task-oriented dialogue datasets have been developed and contributed to significant advancements in task-oriented dialogue systems, such a dataset does not exist in Japanese, and research in this area is limited compared to that in English. In this study, towards the advancement of research and development of task-oriented dialogue systems in Japanese, we constructed JMultiWOZ, the first Japanese language large-scale multi-domain task-oriented dialogue dataset. Using JMultiWOZ, we evaluated the dialogue state tracking and response generation capabilities of the state-of-the-art methods on the existing major English benchmark dataset MultiWOZ2.2 and the latest large language model (LLM)-based methods. Our evaluation results demonstrated that JMultiWOZ provides a benchmark that is on par with MultiWOZ2
    
[^84]: ALISA: 通过稀疏感知KV缓存加速大型语言模型推理

    ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching

    [https://arxiv.org/abs/2403.17312](https://arxiv.org/abs/2403.17312)

    提出了ALISA，一种通过稀疏感知KV缓存加速大型语言模型推理的新算法系统设计。

    

    Transformer架构显著推动了自然语言处理（NLP）的发展，并且在开发大型语言模型（LLMs）方面具有基础性作用，如LLaMA和OPT，这些模型已经在广泛的NLP任务中占据主导地位。尽管它们具有优越的准确性，但LLMs在实际推理中存在独特挑战，涉及计算和占用大量内存。由于LLM推理具有自回归特性，Transformer中的注意层的KV缓存可以通过将二次复杂度计算替换为线性复杂度内存访问，从而有效加速LLM推理。然而，随着对处理更长序列的需求增加，这种方法需要增加内存。这种开销导致由于I/O瓶颈和甚至是内存不足错误而导致吞吐量降低，特别是在资源受限的系统上，如单个通用GPU上。

    arXiv:2403.17312v1 Announce Type: new  Abstract: The Transformer architecture has significantly advanced natural language processing (NLP) and has been foundational in developing large language models (LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP tasks. Despite their superior accuracy, LLMs present unique challenges in practical inference, concerning the compute and memory-intensive nature. Thanks to the autoregressive characteristic of LLM inference, KV caching for the attention layers in Transformers can effectively accelerate LLM inference by substituting quadratic-complexity computation with linear-complexity memory accesses. Yet, this approach requires increasing memory as demand grows for processing longer sequences. The overhead leads to reduced throughput due to I/O bottlenecks and even out-of-memory errors, particularly on resource-constrained systems like a single commodity GPU. In this paper, we propose ALISA, a novel algorithm-system co-desi
    
[^85]: 神经多模态主题建模：全面评估

    Neural Multimodal Topic Modeling: A Comprehensive Evaluation

    [https://arxiv.org/abs/2403.17308](https://arxiv.org/abs/2403.17308)

    该论文对包含文本和图片的文档的多模态主题建模进行了全面评估，并提出了两种新颖的主题建模解决方案和两种新颖的评估指标，结果显示这些模型均能产生连贯且多样化的主题。

    

    神经主题模型可以成功地在文本数据中找到连贯且多样化的主题。然而，它们在处理多模态数据集（如图片和文本）方面存在局限性。本文首次提出了包含文本和图片的文档的多模态主题建模的系统性和全面评估。在此过程中，我们提出了两种新颖的主题建模解决方案和两种新颖的评估指标。总体而言，我们对一个前所未有的丰富多样的数据集集合进行的评估表明，我们的两个模型都能产生连贯且多样化的主题。然而，一个方法优于另一个方法的程度取决于指标和数据集的组合，这表明未来需要进一步探索混合解决方案。值得注意的是，我们简洁的人工评估与我们提出的指标所确定的结果一致。这种一致不仅加强了我们指标的可信度，也突显了

    arXiv:2403.17308v1 Announce Type: cross  Abstract: Neural topic models can successfully find coherent and diverse topics in textual data. However, they are limited in dealing with multimodal datasets (e.g., images and text). This paper presents the first systematic and comprehensive evaluation of multimodal topic modeling of documents containing both text and images. In the process, we propose two novel topic modeling solutions and two novel evaluation metrics. Overall, our evaluation on an unprecedented rich and diverse collection of datasets indicates that both of our models generate coherent and diverse topics. Nevertheless, the extent to which one method outperforms the other depends on the metrics and dataset combinations, which suggests further exploration of hybrid solutions in the future. Notably, our succinct human evaluation aligns with the outcomes determined by our proposed metrics. This alignment not only reinforces the credibility of our metrics but also highlights the po
    
[^86]: 视觉幻觉：定义、量化和处方修复

    Visual Hallucination: Definition, Quantification, and Prescriptive Remediations

    [https://arxiv.org/abs/2403.17306](https://arxiv.org/abs/2403.17306)

    本文提出了关于视觉-语言模型中幻觉的细致讨论，对幻觉进行了量化，并提供了一个新的公开数据集VHILT，有助于研究此问题。

    

    引发幻觉的不断增加，或许是对负责任人工智能进展的最显著障碍。最近，相当多的研究侧重于检测和缓解大型语言模型（LLMs）中的幻觉。然而，值得注意的是，视觉-语言模型（VLMs）中幻觉也相当普遍。在本文中，我们提供了一个关于基于图像字幕和视觉问答两个任务的VLM幻觉剖析的细致讨论。我们阐明了八个细致的视觉幻觉取向：i) 上下文猜测，ii) 身份不一致，iii) 地理错误，iv) 视觉幻觉，v) 性别异常，vi) VLM作为分类器，vii) 错误阅读，和viii) 数字差异。我们策划了一份名为视觉幻觉诱发（VHILT）的公开数据集，包含了通过两个任务（字幕和VQA）生成的来自八个VLM的2,000个样本。

    arXiv:2403.17306v1 Announce Type: new  Abstract: The troubling rise of hallucination presents perhaps the most significant impediment to the advancement of responsible AI. In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs). However, it's worth noting that hallucination is also quite prevalent in Vision-Language models (VLMs). In this paper, we offer a fine-grained discourse on profiling VLM hallucination based on two tasks: i) image captioning, and ii) Visual Question Answering (VQA). We delineate eight fine-grained orientations of visual hallucination: i) Contextual Guessing, ii) Identity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender Anomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric Discrepancy. We curate Visual HallucInation eLiciTation (VHILT), a publicly available dataset comprising 2,000 samples generated using eight VLMs across two tasks of captioning and VQA alo
    
[^87]: InternLM2技术报告

    InternLM2 Technical Report

    [https://arxiv.org/abs/2403.17297](https://arxiv.org/abs/2403.17297)

    InternLM2是一个开源的大语言模型，在全面评估、长文本建模以及创新的预训练和优化技术下表现出色，超越了其前任模型。

    

    大语言模型（LLMs）的发展，如ChatGPT和GPT-4，引发了关于人工通用智能（AGI）即将到来的讨论。然而，在开源模型中复制这样的进展一直是具有挑战性的。本文介绍了InternLM2，一个开源的LLM，在6个维度和30个基准测试中胜过其前辈，在长文本建模和主观评估方面优异，通过创新的预训练和优化技术。

    arXiv:2403.17297v1 Announce Type: cross  Abstract: The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack" test. InternLM2 is further aligned using Supervised Fi
    
[^88]: 探索CausalWorld：通过知识转移和课程学习增强机器人操纵能力

    Exploring CausalWorld: Enhancing robotic manipulation via knowledge transfer and curriculum learning

    [https://arxiv.org/abs/2403.17266](https://arxiv.org/abs/2403.17266)

    通过知识转移和课程学习，本研究在三指机械臂操纵任务中采用强化学习，实现了提高学习效率和有效性的目标

    

    本研究探讨了一个基于学习的三指机械臂操纵任务，该任务需要指间的复杂运动和协调。通过采用强化学习，我们训练一个智能体来获得熟练操纵所需的技能。为了增强学习过程的效率和有效性，我们在软演员-评论家架构中采用了两种知识转移策略，即微调和课程学习。微调使智能体可以利用预先训练的知识并将其调整到新任务中。实施和评估了多种变体，如模型转移、策略转移和跨任务转移。为了消除预训练的需求，课程学习将高级任务分解为更简单、渐进的阶段，模拟了人类学习过程。发现学习阶段的数量、子任务的背景以及转换时机是关键的设计参数。

    arXiv:2403.17266v1 Announce Type: cross  Abstract: This study explores a learning-based tri-finger robotic arm manipulating task, which requires complex movements and coordination among the fingers. By employing reinforcement learning, we train an agent to acquire the necessary skills for proficient manipulation. To enhance the efficiency and effectiveness of the learning process, two knowledge transfer strategies, fine-tuning and curriculum learning, were utilized within the soft actor-critic architecture. Fine-tuning allows the agent to leverage pre-trained knowledge and adapt it to new tasks. Several variations like model transfer, policy transfer, and across-task transfer were implemented and evaluated. To eliminate the need for pretraining, curriculum learning decomposes the advanced task into simpler, progressive stages, mirroring how humans learn. The number of learning stages, the context of the sub-tasks, and the transition timing were found to be the critical design parameter
    
[^89]: DASA: 延迟自适应多智能体随机逼近

    DASA: Delay-Adaptive Multi-Agent Stochastic Approximation

    [https://arxiv.org/abs/2403.17247](https://arxiv.org/abs/2403.17247)

    DASA算法是第一个收敛速度仅依赖于混合时间和平均延迟的算法，同时在马尔科夫采样下实现N倍的收敛加速。

    

    我们考虑一种设置，其中$N$个智能体旨在通过并行操作并与中央服务器通信来加速一个常见的随机逼近（SA）问题。我们假定上行传输到服务器的传输受到异步和潜在无界时变延迟的影响。为了减轻延迟和落后者的影响，同时又能获得分布式计算的好处，我们提出了一种名为DASA的延迟自适应多智能体随机逼近算法。我们对DASA进行了有限时间分析，假设智能体的随机观测过程是独立马尔科夫链。与现有结果相比，DASA是第一个其收敛速度仅取决于混合时间$tmix$和平均延迟$\tau_{avg}$，同时在马尔科夫采样下实现N倍的收敛加速的算法。我们的工作对于各种SA应用是相关的。

    arXiv:2403.17247v1 Announce Type: new  Abstract: We consider a setting in which $N$ agents aim to speedup a common Stochastic Approximation (SA) problem by acting in parallel and communicating with a central server. We assume that the up-link transmissions to the server are subject to asynchronous and potentially unbounded time-varying delays. To mitigate the effect of delays and stragglers while reaping the benefits of distributed computation, we propose \texttt{DASA}, a Delay-Adaptive algorithm for multi-agent Stochastic Approximation. We provide a finite-time analysis of \texttt{DASA} assuming that the agents' stochastic observation processes are independent Markov chains. Significantly advancing existing results, \texttt{DASA} is the first algorithm whose convergence rate depends only on the mixing time $\tmix$ and on the average delay $\tau_{avg}$ while jointly achieving an $N$-fold convergence speedup under Markovian sampling. Our work is relevant for various SA applications, inc
    
[^90]: TwoStep: 使用经典规划器和大型语言模型进行多智能体任务规划

    TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models

    [https://arxiv.org/abs/2403.17246](https://arxiv.org/abs/2403.17246)

    该论文将经典规划和大型语言模型相结合，通过近似人类直觉，以实现多智能体任务规划。

    

    类似规划领域定义语言（PDDL）之类的经典规划公式允许确定可实现目标状态的动作序列，只要存在任何可能的初始状态。然而，PDDL中定义的推理问题并未捕获行动进行的时间方面，例如领域中的两个智能体如果彼此的后况不干扰前提条件，则可以同时执行一个动作。人类专家可以将目标分解为大部分独立的组成部分，并将每个智能体分配给其中一个子目标，以利用同时进行动作来加快计划步骤的执行，每个部分仅使用单个智能体规划。相比之下，直接推断计划步骤的大型语言模型（LLMs）并不保证执行成功，但利用常识推理来组装动作序列。我们通过近似人类直觉，结合了经典规划和LLMs的优势

    arXiv:2403.17246v1 Announce Type: new  Abstract: Classical planning formulations like the Planning Domain Definition Language (PDDL) admit action sequences guaranteed to achieve a goal state given an initial state if any are possible. However, reasoning problems defined in PDDL do not capture temporal aspects of action taking, for example that two agents in the domain can execute an action simultaneously if postconditions of each do not interfere with preconditions of the other. A human expert can decompose a goal into largely independent constituent parts and assign each agent to one of these subgoals to take advantage of simultaneous actions for faster execution of plan steps, each using only single agent planning. By contrast, large language models (LLMs) used for directly inferring plan steps do not guarantee execution success, but do leverage commonsense reasoning to assemble action sequences. We combine the strengths of classical planning and LLMs by approximating human intuition
    
[^91]: DreamPolisher: 通过几何扩散实现高质量文本到三维生成

    DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion

    [https://arxiv.org/abs/2403.17237](https://arxiv.org/abs/2403.17237)

    DreamPolisher通过两阶段高斯飞溅方法实现了文本到三维生成的高质量，强调了视图一致性和纹理丰富度。

    

    我们提出了DreamPolisher，一种基于高斯飞溅和几何引导的方法，旨在从文本描述中学习跨视图一致性和精细细节。我们通过两阶段的高斯飞溅方法强制实现视图之间的几何一致性，以改善生成的三维资产的纹理保真度和整体一致性。

    arXiv:2403.17237v1 Announce Type: cross  Abstract: We present DreamPolisher, a novel Gaussian Splatting based method with geometric guidance, tailored to learn cross-view consistency and intricate detail from textual descriptions. While recent progress on text-to-3D generation methods have been promising, prevailing methods often fail to ensure view-consistency and textural richness. This problem becomes particularly noticeable for methods that work with text input alone. To address this, we propose a two-stage Gaussian Splatting based approach that enforces geometric consistency among views. Initially, a coarse 3D generation undergoes refinement via geometric optimization. Subsequently, we use a ControlNet driven refiner coupled with the geometric consistency term to improve both texture fidelity and overall consistency of the generated 3D asset. Empirical evaluations across diverse textual prompts spanning various object categories demonstrate the efficacy of DreamPolisher in generat
    
[^92]: 在自动停车中通过强化学习在MCTS中加速路径规划

    Speeding Up Path Planning via Reinforcement Learning in MCTS for Automated Parking

    [https://arxiv.org/abs/2403.17234](https://arxiv.org/abs/2403.17234)

    本文提出了一种将强化学习与蒙特卡洛树搜索集成的方法，用于自动停车任务中的在线路径规划，旨在加速路径规划过程，提高效率。

    

    本文针对一种方法进行了讨论，该方法将强化学习整合到蒙特卡洛树搜索中，以提升在全可观测环境下进行自动停车任务的在线路径规划。在高维空间下基于采样的规划方法可能具有计算开销大、耗时长的特点。状态评估方法通过将先验知识应用于搜索步骤中，使实时系统中的过程更快速。鉴于自动停车任务通常在复杂环境中执行，传统分析方式难以构建坚实但轻量级的启发式指导。为了克服这一局限性，我们提出了在路径规划框架下具有蒙特卡洛树搜索的强化学习流水线。通过迭代地学习状态的价值以及最佳动作，在前一个周期结果的样本中选择最佳动作，我们能够建模一个值估计器以及一个...

    arXiv:2403.17234v1 Announce Type: new  Abstract: In this paper, we address a method that integrates reinforcement learning into the Monte Carlo tree search to boost online path planning under fully observable environments for automated parking tasks. Sampling-based planning methods under high-dimensional space can be computationally expensive and time-consuming. State evaluation methods are useful by leveraging the prior knowledge into the search steps, making the process faster in a real-time system. Given the fact that automated parking tasks are often executed under complex environments, a solid but lightweight heuristic guidance is challenging to compose in a traditional analytical way. To overcome this limitation, we propose a reinforcement learning pipeline with a Monte Carlo tree search under the path planning framework. By iteratively learning the value of a state and the best action among samples from its previous cycle's outcomes, we are able to model a value estimator and a 
    
[^93]: 神经网络中基于梯度的解释的不确定性量化

    Uncertainty Quantification for Gradient-based Explanations in Neural Networks

    [https://arxiv.org/abs/2403.17224](https://arxiv.org/abs/2403.17224)

    本文提出了一种结合不确定性估计方法和解释方法来确定神经网络解释不确定性的流程，通过计算解释分布的变异系数，评估了解释的置信度并确定Guided Backpropagation方法生成的解释具有较低的不确定性。

    

    解释方法有助于理解模型预测的原因。这些方法越来越多地参与模型调试、性能优化，并获得对模型工作原理的洞见。鉴于这些方法的关键应用，衡量这些方法生成的解释的不确定性是至关重要的。在本文中，我们提出了一种结合不确定性估计方法和解释方法来确定神经网络解释不确定性的流程。我们利用这个流程为CIFAR-10、FER+和California Housing数据集生成解释分布。通过计算这些分布的变异系数，我们评估了解释的置信度，并确定使用引导反向传播生成的解释与低不确定性相关。此外，我们计算了修改的像素插入/删除度量来评价……

    arXiv:2403.17224v1 Announce Type: cross  Abstract: Explanation methods help understand the reasons for a model's prediction. These methods are increasingly involved in model debugging, performance optimization, and gaining insights into the workings of a model. With such critical applications of these methods, it is imperative to measure the uncertainty associated with the explanations generated by these methods. In this paper, we propose a pipeline to ascertain the explanation uncertainty of neural networks by combining uncertainty estimation methods and explanation methods. We use this pipeline to produce explanation distributions for the CIFAR-10, FER+, and California Housing datasets. By computing the coefficient of variation of these distributions, we evaluate the confidence in the explanation and determine that the explanations generated using Guided Backpropagation have low uncertainty associated with them. Additionally, we compute modified pixel insertion/deletion metrics to ev
    
[^94]: 物体检测和识别的共存，以及未标记物体发现

    Co-Occurring of Object Detection and Identification towards unlabeled object discovery

    [https://arxiv.org/abs/2403.17223](https://arxiv.org/abs/2403.17223)

    通过提出的深度学习方法，可以识别多标签物体类别中基础物体的共存物体，并通过共存矩阵分析生成频繁模式，从而实现未标记物体的发现。

    

    在本文中，我们提出了一种基于深度学习的新方法，用于识别多标签物体类别中与基础物体共存的物体。我们的工作流程由两个阶段组成：在提出模型的第一阶段中，我们检测图像中的所有边界框及其对应的标签，然后在第二阶段进行共存矩阵分析。在共存矩阵分析中，我们基于标签的最大出现次数设定基本类，并构建关联规则并生成频繁模式。这些频繁模式将显示基本类及其对应的共存类。我们在两个公开数据集上进行了实验：Pascal VOC和MS-COCO。实验结果显示我们的方法的有效性。

    arXiv:2403.17223v1 Announce Type: cross  Abstract: In this paper, we propose a novel deep learning based approach for identifying co-occurring objects in conjunction with base objects in multilabel object categories. Nowadays, with the advancement in computer vision based techniques we need to know about co-occurring objects with respect to base object for various purposes. The pipeline of the proposed work is composed of two stages: in the first stage of the proposed model we detect all the bounding boxes present in the image and their corresponding labels, then in the second stage we perform co-occurrence matrix analysis. In co-occurrence matrix analysis, we set base classes based on the maximum occurrences of the labels and build association rules and generate frequent patterns. These frequent patterns will show base classes and their corresponding co-occurring classes. We performed our experiments on two publicly available datasets: Pascal VOC and MS-COCO. The experimental results 
    
[^95]: SeSaMe：模拟自我报告的地面真实性的框架，用于心理健康感知研究

    SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies

    [https://arxiv.org/abs/2403.17219](https://arxiv.org/abs/2403.17219)

    SeSaMe框架利用大型语言模型为心理健康研究中的参与者模拟自我报告，减轻了他们的负担

    

    移动和可穿戴技术的进步使得可以被动监测一个人的心理、行为和情感健康的潜力成为可能。这些方法通常依赖于自我报告结果的长期收集，例如抑郁、压力和焦虑，以训练机器学习（ML）模型。然而，持续自我报告会给参与者带来重大负担，经常导致流失、缺失标签或不真诚的回应。在这项工作中，我们介绍了使用心理模型模拟规模分数的SeSaMe框架，以减轻数字心理健康研究中参与者的负担。通过利用预先训练的大型语言模型（LLMs），SeSaMe能够模拟参与者在心理量表上的回应。

    arXiv:2403.17219v1 Announce Type: cross  Abstract: Advances in mobile and wearable technologies have enabled the potential to passively monitor a person's mental, behavioral, and affective health. These approaches typically rely on longitudinal collection of self-reported outcomes, e.g., depression, stress, and anxiety, to train machine learning (ML) models. However, the need to continuously self-report adds a significant burden on the participants, often resulting in attrition, missing labels, or insincere responses. In this work, we introduce the Scale Scores Simulation using Mental Models (SeSaMe) framework to alleviate participants' burden in digital mental health studies. By leveraging pre-trained large language models (LLMs), SeSaMe enables the simulation of participants' responses on psychological scales. In SeSaMe, researchers can prompt LLMs with information on participants' internal behavioral dispositions, enabling LLMs to construct mental models of participants to simulate 
    
[^96]: DiffusionAct：可控扩散自编码器用于一次性人脸再现

    DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face Reenactment

    [https://arxiv.org/abs/2403.17217](https://arxiv.org/abs/2403.17217)

    DiffusionAct是一种利用扩散模型进行神经人脸再现的新方法，能够编辑输入图像的面部姿势，实现身份和外观的保留，以及目标头部姿势和面部表情的转移。

    

    视频驱动的神经人脸再现旨在合成能成功保留源脸的身份和外观，同时转移目标头部姿势和面部表情的逼真面部图像。现有基于GAN的方法要么存在失真和视觉伪影，要么重构质量较差，即背景和一些重要的外观细节（如发型/颜色、眼镜和配饰）未被忠实重建。最近在扩散概率模型（DPMs）领域的进展使得生成高质量逼真图像成为可能。为此，本文提出了DiffusionAct，这是一种利用扩散模型的照片逼真图像生成来进行神经人脸再现的新方法。具体来说，我们提出控制Diffusion自编码器（DiffAE）的语义空间，以便编辑输入图像的面部姿势，定义为头部姿势方向。

    arXiv:2403.17217v1 Announce Type: cross  Abstract: Video-driven neural face reenactment aims to synthesize realistic facial images that successfully preserve the identity and appearance of a source face, while transferring the target head pose and facial expressions. Existing GAN-based methods suffer from either distortions and visual artifacts or poor reconstruction quality, i.e., the background and several important appearance details, such as hair style/color, glasses and accessories, are not faithfully reconstructed. Recent advances in Diffusion Probabilistic Models (DPMs) enable the generation of high-quality realistic images. To this end, in this paper we present DiffusionAct, a novel method that leverages the photo-realistic image generation of diffusion models to perform neural face reenactment. Specifically, we propose to control the semantic space of a Diffusion Autoencoder (DiffAE), in order to edit the facial pose of the input images, defined as the head pose orientation an
    
[^97]: 探究输出格式对大型语言模型在代码翻译评估中的影响

    Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation

    [https://arxiv.org/abs/2403.17214](https://arxiv.org/abs/2403.17214)

    本研究实证分析了11种流行的专门调整的大型语言模型在五种语言上生成的输出，发现其中26.4%到73.7%的代码翻译需要后处理。

    

    编程语言之间的代码翻译是软件工程中长期存在且至关重要的任务，有助于现代化遗留系统，确保跨平台兼容性，提升软件性能。随着大型语言模型（LLMs）及其在代码翻译中的应用的最新进展，对这些模型进行全面评估的需求越来越强烈。在本研究中，我们在五种语言（包括C、C++、Go、Java和Python）上，从1B到46.7B的参数范围内对十一种流行的专门调整的LLMs生成的输出进行了实证分析，并涵盖3820个翻译对。我们的分析发现，在我们评估的LLMs中，26.4%到73.7%的代码翻译需要后处理，因为这些翻译通常包含代码、引号和文本的混合，而不仅仅是纯源代码。忽视这些模型的输出格式可能不经意间导致

    arXiv:2403.17214v1 Announce Type: cross  Abstract: Code translation between programming languages is a long-existing and critical task in software engineering, facilitating the modernization of legacy systems, ensuring cross-platform compatibility, and enhancing software performance. With the recent advances in large language models (LLMs) and their applications to code translation, there is an increasing need for comprehensive evaluation of these models. In this study, we empirically analyze the generated outputs of eleven popular instruct-tuned LLMs with parameters ranging from 1B up to 46.7B on 3,820 translation pairs across five languages, including C, C++, Go, Java, and Python. Our analysis found that between 26.4% and 73.7% of code translations produced by our evaluated LLMs necessitate post-processing, as these translations often include a mix of code, quotes, and text rather than being purely source code. Overlooking the output format of these models can inadvertently lead to u
    
[^98]: 解释不确定性的合理性检查

    Sanity Checks for Explanation Uncertainty

    [https://arxiv.org/abs/2403.17212](https://arxiv.org/abs/2403.17212)

    本文提出了解释不确定性方法的合理性检查，可以快速测试不确定性和解释方法的组合。

    

    机器学习模型的解释可能难以解释或出现错误。 将解释方法与不确定性估计方法相结合会产生解释不确定性。 评估解释不确定性是困难的。 本文提出了解释不确定性方法的合理性检查，其中针对带有不确定性的解释定义了权重和数据随机化测试，允许对不确定性和解释方法的组合进行快速测试。 我们在CIFAR10和加利福尼亚房屋数据集上实验证明这些测试的有效性和有效性，注意到Ensemble似乎在Guided Backpropagation，Integrated Gradients和LIME解释上一致通过了这两项测试。

    arXiv:2403.17212v1 Announce Type: cross  Abstract: Explanations for machine learning models can be hard to interpret or be wrong. Combining an explanation method with an uncertainty estimation method produces explanation uncertainty. Evaluating explanation uncertainty is difficult. In this paper we propose sanity checks for uncertainty explanation methods, where a weight and data randomization tests are defined for explanations with uncertainty, allowing for quick tests to combinations of uncertainty and explanation methods. We experimentally show the validity and effectiveness of these tests on the CIFAR10 and California Housing datasets, noting that Ensembles seem to consistently pass both tests with Guided Backpropagation, Integrated Gradients, and LIME explanations.
    
[^99]: CADGL: 上下文感知深度图学习用于预测药物-药物相互作用

    CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions

    [https://arxiv.org/abs/2403.17210](https://arxiv.org/abs/2403.17210)

    通过CADGL框架，利用上下文感知深度图学习来预测药物-药物相互作用，解决了现有DDI预测模型在泛化、特征提取和现实应用方面的挑战

    

    药物-药物相互作用（DDIs）的研究是药物开发过程中的一个关键元素。DDIs发生在一个药物的性质受其他药物包含的影响时。检测有利的DDIs有可能为在实际设置中应用的创新药物的创造和推进铺平道路。然而，现有的DDI预测模型在极端情况下的泛化、稳健特征提取以及现实应用可能性方面持续面临挑战。我们旨在通过利用上下文感知深度图学习的有效性，引入一种名为CADGL的新颖框架来应对这些挑战。基于定制的变分图自编码器（VGAE），我们利用两个上下文预处理器从两个不同视角：局部邻域和分子上下文，在异质图结构中提取特征，捕获关键的结构和生理化学信息。

    arXiv:2403.17210v1 Announce Type: cross  Abstract: Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process of drug development. DDIs occur when one drug's properties are affected by the inclusion of other drugs. Detecting favorable DDIs has the potential to pave the way for creating and advancing innovative medications applicable in practical settings. However, existing DDI prediction models continue to face challenges related to generalization in extreme cases, robust feature extraction, and real-life application possibilities. We aim to address these challenges by leveraging the effectiveness of context-aware deep graph learning by introducing a novel framework named CADGL. Based on a customized variational graph autoencoder (VGAE), we capture critical structural and physio-chemical information using two context preprocessors for feature extraction from two different perspectives: local neighborhood and molecular context, in a heterogeneous graphical structure. Ou
    
[^100]: 利用大型语言模型代理生成资产管理外壳：数字孪生和语义节点中的互操作性

    Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node

    [https://arxiv.org/abs/2403.17209](https://arxiv.org/abs/2403.17209)

    通过大型语言模型代理生成AAS实例模型，实现了在数字孪生中的互操作性，降低了手动创建成本和时间。

    

    这项研究介绍了一种新颖的方法，用于协助在工业4.0背景下为数字孪生建模创建资产管理外壳（AAS）实例，旨在增强智能制造中的互操作性，减少手动工作。我们构建了一个“语义节点”数据结构来捕捉文本数据的语义要义。然后，设计并实现了一个由大型语言模型驱动的系统，用于处理“语义节点”并从文本技术数据生成AAS实例模型。我们的评估表明，有效生成率为62-79%，表明相当比例的手动创建工作可以转换为更容易的验证工作，从而减少创建AAS实例模型的时间和成本。在我们的评估中，对不同LLM的比较分析以及检索增强生成（RAG）机制的深入消融研究提供了有关LLM有效性的见解。

    arXiv:2403.17209v1 Announce Type: new  Abstract: This research introduces a novel approach for assisting the creation of Asset Administration Shell (AAS) instances for digital twin modeling within the context of Industry 4.0, aiming to enhance interoperability in smart manufacturing and reduce manual effort. We construct a "semantic node" data structure to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process "semantic node" and generate AAS instance models from textual technical data. Our evaluation demonstrates a 62-79% effective generation rate, indicating a substantial proportion of manual creation effort can be converted into easier validation effort, thereby reducing the time and cost in creating AAS instance models. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM
    
[^101]: NUMTEMP：一个用于验证带有统计和时间表达式的论点的真实世界基准

    NUMTEMP: A real-world benchmark to verify claims with statistical and temporal expressions

    [https://arxiv.org/abs/2403.17169](https://arxiv.org/abs/2403.17169)

    NUMTEMP是一个真实世界基准，专注于验证复杂的数字论点，量化了现有解决方案的局限性，并提供了一种解决真实世界数字论点验证挑战的方法。

    

    自动事实检查在数字时代应对不断增长的错误信息方面引起了极大兴趣。现有系统主要专注于维基百科上的合成论点，并且在真实世界论点上也取得了显著进展。在本文中，我们发布了Numtemp，一个多样化、多领域的数据集，专门关注数字论点，包括时间、统计和多样化方面的细粒度元数据，并且具有不泄露的证据收集。这解决了验证真实世界数字论点的挑战，这些论点复杂，往往缺乏精确信息，这是现有作品主要关注合成论点未解决的问题。我们评估并量化了现有解决方案在验证数字论点任务中的局限性。我们还评估了基于论点分解的方法、基于数字理解的模型，我们的最佳基线实现了58.32的宏F1分数。这证明了Numtemp的关键价值。

    arXiv:2403.17169v1 Announce Type: cross  Abstract: Automated fact checking has gained immense interest to tackle the growing misinformation in the digital era. Existing systems primarily focus on synthetic claims on Wikipedia, and noteworthy progress has also been made on real-world claims. In this work, we release Numtemp, a diverse, multi-domain dataset focused exclusively on numerical claims, encompassing temporal, statistical and diverse aspects with fine-grained metadata and an evidence collection without leakage. This addresses the challenge of verifying real-world numerical claims, which are complex and often lack precise information, not addressed by existing works that mainly focus on synthetic claims. We evaluate and quantify the limitations of existing solutions for the task of verifying numerical claims. We also evaluate claim decomposition based methods, numerical understanding based models and our best baselines achieves a macro-F1 of 58.32. This demonstrates that Numtemp
    
[^102]: 多目标质量多样性用于晶体结构预测

    Multi-Objective Quality-Diversity for Crystal Structure Prediction

    [https://arxiv.org/abs/2403.17164](https://arxiv.org/abs/2403.17164)

    本研究利用质量多样性算法为晶体结构预测打开了一条新途径，旨在发现具有多样特征的高性能解决方案集合，可以优化晶体结构稳定性以及其他目标如磁性或热电效率。

    

    晶体结构在从电池到太阳能电池等各个领域中都是不可或缺的，针对其原子配置预测性能已经有了大量研究。然而，现有的晶体结构预测方法侧重于识别能量函数全局最小值处的最稳定解决方案，而忽略了那些可能位于相邻局部极小值处、具有不同材料特性（如电导率或抗变形性）的其他有趣材料。相比之下，质量多样性算法为晶体结构预测提供了一个有前途的途径，因为它旨在找到具有多样特征的高性能解决方案集合。然而，优化晶体结构稳定性以及其他目标（如磁性或热电效率）也可能是有价值的。因此，在这项研究中，我们利用......

    arXiv:2403.17164v1 Announce Type: cross  Abstract: Crystal structures are indispensable across various domains, from batteries to solar cells, and extensive research has been dedicated to predicting their properties based on their atomic configurations. However, prevailing Crystal Structure Prediction methods focus on identifying the most stable solutions that lie at the global minimum of the energy function. This approach overlooks other potentially interesting materials that lie in neighbouring local minima and have different material properties such as conductivity or resistance to deformation. By contrast, Quality-Diversity algorithms provide a promising avenue for Crystal Structure Prediction as they aim to find a collection of high-performing solutions that have diverse characteristics. However, it may also be valuable to optimise for the stability of crystal structures alongside other objectives such as magnetism or thermoelectric efficiency. Therefore, in this work, we harness 
    
[^103]: 少即是多 - 关于稀疏化在Transformers和图神经网络在TSP问题中的重要性

    Less Is More - On the Importance of Sparsification for Transformers and Graph Neural Networks for TSP

    [https://arxiv.org/abs/2403.17159](https://arxiv.org/abs/2403.17159)

    我们提出了一种数据预处理方法，通过稀疏化TSP图表示和注意力掩码，使编码器集中于TSP实例的关键部分，同时允许信息在所有节点之间自由流动。

    

    大多数最近研究处理旅行商问题（TSP）等路由问题的机器学习方法使用基于transformer或图神经网络（GNN）的编码器架构。然而，其中许多研究直接应用这些编码器，允许它们在整个TSP实例上聚合信息。相反，我们提出了一种数据预处理方法，使编码器仅关注TSP实例的最相关部分。具体来说，我们为传递给GNN的TSP图表示提出了图稀疏化，并为传递给transformers的TSP实例提出了注意力屏蔽，其中mask对应于稀疏TSP图表示的邻接矩阵。此外，我们提出了不同稀疏化级别的集合，使模型能够专注于最有前途的部分，同时还允许TSP实例的所有节点之间的信息流动。在实验研究中，我们展示了

    arXiv:2403.17159v1 Announce Type: cross  Abstract: Most of the recent studies tackling routing problems like the Traveling Salesman Problem (TSP) with machine learning use a transformer or Graph Neural Network (GNN) based encoder architecture. However, many of them apply these encoders naively by allowing them to aggregate information over the whole TSP instances. We, on the other hand, propose a data preprocessing method that allows the encoders to focus on the most relevant parts of the TSP instances only. In particular, we propose graph sparsification for TSP graph representations passed to GNNs and attention masking for TSP instances passed to transformers where the masks correspond to the adjacency matrices of the sparse TSP graph representations. Furthermore, we propose ensembles of different sparsification levels allowing models to focus on the most promising parts while also allowing information flow between all nodes of a TSP instance. In the experimental studies, we show that
    
[^104]: 用频谱群体机器人听到一个竞技场的形状

    Hearing the shape of an arena with spectral swarm robotics

    [https://arxiv.org/abs/2403.17147](https://arxiv.org/abs/2403.17147)

    频谱群体机器人可以通过模拟信息扩散来重建竞技场的几何形状

    

    Swarm机器人技术承诺适应未知情况，并且对于故障具有鲁棒性。然而，它仍然在需要理解机器人操作的更广泛背景的全局任务中遇到困难，比如识别机器人嵌入的竞技场的形状。生物群体，比如鱼群、鸟群和昆虫群，通过局部提示的扩散例行解决全局几何问题。这一范式可以通过数学模型明确描述，可以通过机器人群体直接计算和利用。域上的扩散在数学上由拉普拉斯算子进行封装，该算子用于度量函数的局部曲率。关键是，一个区域的几何形状通常可以从其拉普拉斯算子的特征谱中重建。在这里，我们介绍了频谱群体机器人技术，其中机器人将信息扩散给他们的邻居来模拟拉普拉

    arXiv:2403.17147v1 Announce Type: cross  Abstract: Swarm robotics promises adaptability to unknown situations and robustness against failures. However, it still struggles with global tasks that require understanding the broader context in which the robots operate, such as identifying the shape of the arena in which the robots are embedded. Biological swarms, such as shoals of fish, flocks of birds, and colonies of insects, routinely solve global geometrical problems through the diffusion of local cues. This paradigm can be explicitly described by mathematical models that could be directly computed and exploited by a robotic swarm. Diffusion over a domain is mathematically encapsulated by the Laplacian, a linear operator that measures the local curvature of a function. Crucially the geometry of a domain can generally be reconstructed from the eigenspectrum of its Laplacian. Here we introduce spectral swarm robotics where robots diffuse information to their neighbors to emulate the Lapla
    
[^105]: MetaAligner：用于语言模型通用多目标对齐的条件从弱到强校正

    MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models

    [https://arxiv.org/abs/2403.17141](https://arxiv.org/abs/2403.17141)

    MetaAligner是第一个与策略无关且通用的多目标偏好对齐方法，通过将参数更新与策略模型解耦实现即插即用的对齐，并通过上下文学习实现未见目标的零冷启动偏好对齐

    

    近期大型语言模型（LLM）的进展旨在通过多目标偏好对齐来解决异质人类期望和价值观，然而，现有方法受到策略模型的参数限制，导致两个关键局限性：（1）它们的对齐算法对于每个新目标模型的重复成本很高；（2）由于其静态对齐目标，它们无法扩展到未见目标。在这项工作中，我们提出了Meta-Objective Aligner（MetaAligner），这是一种执行条件从弱到强校正以逼近强响应的模型。MetaAligner是第一个与策略无关且通用的多目标偏好对齐方法，它通过将参数更新与策略模型解耦实现即插即用的对齐，并通过上下文学习实现未见目标的零冷启动偏好对齐。实验结果表明，MetaAligner取得了显著

    arXiv:2403.17141v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) aim to tackle heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are parameter-adherent to the policy model, leading to two key limitations: (1) the high-cost repetition of their alignment algorithms for each new target model; (2) they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose Meta-Objective Aligner (MetaAligner), a model that performs conditional weak-to-strong correction for weak responses to approach strong responses. MetaAligner is the first policy-agnostic and generalizable method for multi-objective preference alignment, which enables plug-and-play alignment by decoupling parameter updates from the policy models and facilitates zero-shot preference alignment for unseen objectives via in-context learning. Experimental results show that MetaAligner achieves sign
    
[^106]: RepairAgent：一种基于LLM的自主代理程序修复工具

    RepairAgent: An Autonomous, LLM-Based Agent for Program Repair

    [https://arxiv.org/abs/2403.17134](https://arxiv.org/abs/2403.17134)

    RepairAgent是首个通过基于大型语言模型的自主代理程序来解决程序修复挑战的工作，其关键贡献在于提供了一组有助于程序修复的工具以及动态更新的提示格式。

    

    自动程序修复已经成为一种强大的技术，可以减轻软件漏洞对系统可靠性和用户体验的影响。本文介绍了RepairAgent，这是第一个通过基于大型语言模型(LLM)的自主代理解决程序修复挑战的工作。与现有的基于深度学习的方法不同，这些方法会用固定的提示或在固定的反馈循环中提示模型，我们的工作将LLM视为一种能够自主规划和执行修复操作的代理程序，通过调用适当的工具修复漏洞。RepairAgent自由地交织着收集有关漏洞的信息、收集修复材料以及验证修复过程，并根据收集到的信息和先前的修复尝试反馈决定调用哪些工具。实现RepairAgent的关键贡献包括一组有助于程序修复的工具，以及一个动态更新的提示格式，使LLM能够进行交互。

    arXiv:2403.17134v1 Announce Type: cross  Abstract: Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a large language model (LLM). Unlike existing deep learning-based approaches, which prompt a model with a fixed prompt or in a fixed feedback loop, our work treats the LLM as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated prompt format that allows the LLM to interac
    
[^107]: 探索基于原型的软标签数据蒸馏在不平衡数据分类中的潜力

    Exploring the potential of prototype-based soft-labels data distillation for imbalanced data classification

    [https://arxiv.org/abs/2403.17130](https://arxiv.org/abs/2403.17130)

    本文探索了一种先前在少于一次学习中提出的简单蒸馏技术在原型软标签蒸馏中的潜力，旨在通过在蒸馏过程中整合优化步骤，进一步提高分类准确性。

    

    数据集蒸馏旨在通过少量人工生成的数据项合成一个数据集，当这些数据被用作训练数据时，能够重现或逼近一个机器学习（ML）模型，就好像它是在整个原始数据集上训练的一样。因此，数据蒸馏方法通常与特定的ML算法有关。尽管最近的文献主要涉及在神经网络模型背景下对大量图像的蒸馏，但表格数据的蒸馏代表性较低，主要集中在理论视角上。本文探讨了一种在少于一次学习中提出的简单蒸馏技术在原型软标签蒸馏中的潜力。主要目标是通过在蒸馏过程中整合优化步骤，推动基于原型的软标签蒸馏在分类准确性方面的性能进一步提升。该分析是在真实数据集上进行的。

    arXiv:2403.17130v1 Announce Type: cross  Abstract: Dataset distillation aims at synthesizing a dataset by a small number of artificially generated data items, which, when used as training data, reproduce or approximate a machine learning (ML) model as if it were trained on the entire original dataset. Consequently, data distillation methods are usually tied to a specific ML algorithm. While recent literature deals mainly with distillation of large collections of images in the context of neural network models, tabular data distillation is much less represented and mainly focused on a theoretical perspective. The current paper explores the potential of a simple distillation technique previously proposed in the context of Less-than-one shot learning. The main goal is to push further the performance of prototype-based soft-labels distillation in terms of classification accuracy, by integrating optimization steps in the distillation process. The analysis is performed on real-world data sets
    
[^108]: 大型语言模型中先验知识的强大作用及其对情绪识别的影响

    The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition

    [https://arxiv.org/abs/2403.17125](https://arxiv.org/abs/2403.17125)

    大型语言模型在执行任务时依赖背景知识（先验知识），但无法完全整合与任务先验知识相矛盾的信息，影响了情绪识别等主观任务的表现水平。

    

    In-context Learning (ICL)作为一种强大的范式浮现出来，可以在大型语言模型（LLM）上执行自然语言任务，而无需更新模型的参数，与传统的基于梯度的微调相反。 ICL的承诺是，LLM可以适应执行当前任务，并以竞争力或最新水平的一小部分成本。 LLM以这种少样本的方式执行任务的能力依赖于它们对任务的背景知识（或任务先验知识）。然而，最近的研究发现，与传统学习不同，LLM无法完全整合与任务先验知识相矛盾的演示信息。 这可能导致表现达到次优水平，特别是对于主观任务（如情绪识别），其中文本到情绪的映射可能因人类注释的变异性而大不相同。 在这项工作中，我们设计实验并提出了测量方法

    arXiv:2403.17125v1 Announce Type: cross  Abstract: In-context Learning (ICL) has emerged as a powerful paradigm for performing natural language tasks with Large Language Models (LLM) without updating the models' parameters, in contrast to the traditional gradient-based finetuning. The promise of ICL is that the LLM can adapt to perform the present task at a competitive or state-of-the-art level at a fraction of the cost. The ability of LLMs to perform tasks in this few-shot manner relies on their background knowledge of the task (or task priors). However, recent work has found that, unlike traditional learning, LLMs are unable to fully integrate information from demonstrations that contrast task priors. This can lead to performance saturation at suboptimal levels, especially for subjective tasks such as emotion recognition, where the mapping from text to emotions can differ widely due to variability in human annotations. In this work, we design experiments and propose measurements to e
    
[^109]: 将语言计划基于演示通过反事实干扰进行落实

    Grounding Language Plans in Demonstrations Through Counterfactual Perturbations

    [https://arxiv.org/abs/2403.17124](https://arxiv.org/abs/2403.17124)

    这项工作通过使用LLMs来指导多步演示中隐含的任务结构和约束的搜索，以及通过反事实干扰获得更广泛的演示状态空间覆盖。

    

    将大型语言模型的常识推理基于物理领域落实在体现智能的人工智能中仍然是一个至关重要但尚未解决的问题。相较于先前的工作专注于直接利用LLMs在符号空间内规划，这项工作使用LLMs指导任务结构的搜索，隐含在多步演示中的约束。具体而言，我们借鉴了操纵规划文献中的模式族的概念，它按照特定运动约束将机器人配置分组，作为LLM高级语言表示和机器人低级物理轨迹之间的抽象层。通过用合成干扰重新播放少量人类演示，我们可以覆盖演示的状态空间，并额外生成成功执行以及未完成任务的反事实情况。我们的基于解释的学习框架训练了一个端到端可微分神经网络。

    arXiv:2403.17124v1 Announce Type: cross  Abstract: Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural networ
    
[^110]: 在多个同时攻击下对图的保护：一种启发式方法

    Graph Protection under Multiple Simultaneous Attacks: A Heuristic Approach

    [https://arxiv.org/abs/2403.17108](https://arxiv.org/abs/2403.17108)

    本研究开发了一种有效的元启发式方法来保护网络中节点免受多个同时攻击，特别关注$k$-强罗马占领问题，该问题在图中分配部队数量以最小化总权重以满足保护约束条件。

    

    这项工作旨在开发一种有效的元启发式方法，以防范对使用图模型的网络中的节点的多个同时攻击。具体来说，我们关注$k$-强罗马占领问题，这是对图上著名罗马占领问题的一般化。该问题是关于为节点分配整数权重，这些权重代表着每个节点驻扎的部队数量，以满足保护约束条件的同时最小化总权重。这些约束涉及对图对抗任何由$k \in \mathbb{N}$个节点组成的同时攻击的保护。如果可以通过从一个相邻节点借用军队来保护标记为0的每个节点，并确保该邻居至少保留一支军队进行自卫，则攻击被视为被击退。$k$-SRD问题在各个领域具有实际应用，例如制定反恐策略或管理...

    arXiv:2403.17108v1 Announce Type: new  Abstract: This work focuses on developing an effective meta-heuristic approach to protect against simultaneous attacks on nodes of a network modeled using a graph. Specifically, we focus on the $k$-strong Roman domination problem, a generalization of the well-known Roman domination problem on graphs. This general problem is about assigning integer weights to nodes that represent the number of field armies stationed at each node in order to satisfy the protection constraints while minimizing the total weights. These constraints concern the protection of a graph against any simultaneous attack consisting of $k \in \mathbb{N}$ nodes. An attack is considered repelled if each node labeled 0 can be defended by borrowing an army from one of its neighboring nodes, ensuring that the neighbor retains at least one army for self-defense. The $k$-SRD problem has practical applications in various areas, such as developing counter-terrorism strategies or managin
    
[^111]: 人工智能意识是不可避免的：一个理论计算机科学的视角

    AI Consciousness is Inevitable: A Theoretical Computer Science Perspective

    [https://arxiv.org/abs/2403.17101](https://arxiv.org/abs/2403.17101)

    通过理论计算机科学的视角，作者提出了一个简单却强大的机器模型，支持了机器意识是不可避免的这一论断。

    

    我们通过理论计算机科学的视角来审视意识，这是数学的一个分支，研究在资源限制下的计算。从这个角度出发，我们为意识开发了一个形式化的机器模型。这个模型受到了艾伦·图灵简单而强大的计算模型和伯纳德·巴尔斯意识剧场模型的启发。尽管非常简单，这个模型在高层次上与许多关于人类和动物意识的主要科学理论相一致，支持我们的论断：机器意识是不可避免的。

    arXiv:2403.17101v1 Announce Type: new  Abstract: We look at consciousness through the lens of Theoretical Computer Science, a branch of mathematics that studies computation under resource limitations. From this perspective, we develop a formal machine model for consciousness. The model is inspired by Alan Turing's simple yet powerful model of computation and Bernard Baars' theater model of consciousness. Though extremely simple, the model aligns at a high level with many of the major scientific theories of human and animal consciousness, supporting our claim that machine consciousness is inevitable.
    
[^112]: 脱机强化学习：状态聚合和轨迹数据的作用

    Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data

    [https://arxiv.org/abs/2403.17091](https://arxiv.org/abs/2403.17091)

    研究提出了对于脱机策略评估任务，样本复杂度受聚合马尔科夫转换模型中的浓缩系数控制，而不是原始MDP中的系数。

    

    我们重新审视了具有价值函数可实现性但不具有贝尔曼完备性的脱机强化学习问题。我们对脱机策略评估的样本复杂度受聚合马尔科夫转换模型中的浓缩系数控制的发现，以及提供了仅具有价值函数可实现性的脱机策略评估的相当完整的图景。我们的主要发现有三个：1）脱机策略评估的样本复杂度由聚合的马尔科夫转换模型中的集中系数决定，这个系数由函数类和脱机数据分布共同确定，而不是原始MDP中的系数。

    arXiv:2403.17091v1 Announce Type: cross  Abstract: We revisit the problem of offline reinforcement learning with value function realizability but without Bellman completeness. Previous work by Xie and Jiang (2021) and Foster et al. (2022) left open the question whether a bounded concentrability coefficient along with trajectory-based offline data admits a polynomial sample complexity. In this work, we provide a negative answer to this question for the task of offline policy evaluation. In addition to addressing this question, we provide a rather complete picture for offline policy evaluation with only value function realizability. Our primary findings are threefold: 1) The sample complexity of offline policy evaluation is governed by the concentrability coefficient in an aggregated Markov Transition Model jointly determined by the function class and the offline data distribution, rather than that in the original MDP. This unifies and generalizes the ideas of Xie and Jiang (2021) and Fo
    
[^113]: GOLF：目标导向的长期生活任务，由人工智能协作支持

    GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration

    [https://arxiv.org/abs/2403.17089](https://arxiv.org/abs/2403.17089)

    该研究提出了GOLF框架，通过目标导向和长期规划增强LLMs的能力，以协助用户处理重要的生活决策。

    

    ChatGPT等大型语言模型（LLMs）的出现彻底改变了人工智能交互和信息获取过程。利用LLMs作为搜索引擎的替代方案，用户现在可以访问根据其查询定制的摘要信息，显著减少了在导航大量信息资源时所带来的认知负荷。这种转变凸显了LLMs在重新定义信息获取范式方面的潜力。基于任务焦点信息检索和LLMs的任务规划能力，本研究将LLMs的能力范围扩展到支持用户导航长期和重要的生活任务。它引入了GOLF框架（目标导向的长期生活任务），侧重于增强LLMs通过目标定向和长期规划来协助用户做出重要的生活决策。该方法论包含了一个全面的类比实验

    arXiv:2403.17089v1 Announce Type: cross  Abstract: The advent of ChatGPT and similar large language models (LLMs) has revolutionized the human-AI interaction and information-seeking process. Leveraging LLMs as an alternative to search engines, users can now access summarized information tailored to their queries, significantly reducing the cognitive load associated with navigating vast information resources. This shift underscores the potential of LLMs in redefining information access paradigms. Drawing on the foundation of task-focused information retrieval and LLMs' task planning ability, this research extends the scope of LLM capabilities beyond routine task automation to support users in navigating long-term and significant life tasks. It introduces the GOLF framework (Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability to assist in significant life decisions through goal orientation and long-term planning. The methodology encompasses a comprehensive simul
    
[^114]: 数据集修剪在图像超分辨率中的研究

    A Study in Dataset Pruning for Image Super-Resolution

    [https://arxiv.org/abs/2403.17083](https://arxiv.org/abs/2403.17083)

    本研究针对图像超分辨率中数据集训练资源需求大的问题，提出了一种数据集修剪的解决方案，通过基于损失值的选择，将训练集缩减至原始数据集的50%，取得了令人满意的结果。

    

    在图像超分辨率（SR）中，依赖大型数据集进行训练是一把双刃剑。尽管提供丰富的训练素材，但也需要大量的计算和存储资源。在本工作中，我们分析了数据集修剪作为应对这些挑战的解决方案。我们引入了一种新颖的方法，将数据集缩减到基于其损失值而选择的一组核心训练样本。通过仅将训练重点放在原始数据集的50%上，特别是那些损失值最高的样本上，我们实现了与或甚至超过整个数据集训练的结果相媲美的效果。有趣的是，我们的分析显示，具有最高损失值的前5％样本会对训练过程产生负面影响。排除这些样本并调整选择以偏好更容易的样本进一步提高了训练结果。我们的工作开辟了新的研究方向。

    arXiv:2403.17083v1 Announce Type: cross  Abstract: In image Super-Resolution (SR), relying on large datasets for training is a double-edged sword. While offering rich training material, they also demand substantial computational and storage resources. In this work, we analyze dataset pruning as a solution to these challenges. We introduce a novel approach that reduces a dataset to a core-set of training samples, selected based on their loss values as determined by a simple pre-trained SR model. By focusing the training on just 50% of the original dataset, specifically on the samples characterized by the highest loss values, we achieve results comparable to or even surpassing those obtained from training on the entire dataset. Interestingly, our analysis reveals that the top 5% of samples with the highest loss values negatively affect the training process. Excluding these samples and adjusting the selection to favor easier samples further enhances training outcomes. Our work opens new p
    
[^115]: 在T2I模型中通过识别语义方向实现连续、主题特定的属性控制

    Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions

    [https://arxiv.org/abs/2403.17064](https://arxiv.org/abs/2403.17064)

    通过识别CLIP文本嵌入中的语义方向，实现了文本到图像模型中对高级属性的细粒度主题特定控制。

    

    近年来，文本到图像（T2I）扩散模型的进展显著提高了生成图像的质量。然而，由于自然语言提示的限制（例如“人”和“老年人”之间不存在连续的中间描述的集合），实现对属性的细粒度控制仍然是一个挑战。尽管引入了许多方法来增强模型或生成过程以实现这种控制，但不需要固定参考图像的方法仅限于启用全局细粒度属性表达控制或仅限于特定主题的粗粒度属性表达控制，而不能同时兼顾两者。我们展示了在常用的基于标记级别的CLIP文本嵌入中存在可实现文本到图像模型中高级属性的细粒度主题特定控制的方向。基于这一观察，我们引入了一种有效的方法。

    arXiv:2403.17064v1 Announce Type: cross  Abstract: In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images. However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between ``person'' and ``old person''). Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously. We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models. Based on this observation, we introduce one efficient op
    
[^116]: 用注意力驱动的脉冲神经网络增强图表示学习

    Enhancing Graph Representation Learning with Attention-Driven Spiking Neural Networks

    [https://arxiv.org/abs/2403.17040](https://arxiv.org/abs/2403.17040)

    本研究提出了一种将注意力机制与脉冲神经网络相结合的新方法，用于改善图表示学习，实验结果显示其在基准数据集上表现出可比较的性能。

    

    图表示学习已经成为机器学习和数据挖掘中的关键任务，因为它具有对社交网络、化合物和生物系统等复杂结构进行建模的潜力。最近，脉冲神经网络（SNNs）作为传统神经网络用于图学习任务的一种有前途的替代方案而出现，因为它们能够高效地编码和处理时间和空间信息。在本文中，我们提出了一种将注意力机制与SNNs结合以改善图表示学习的新方法。具体来说，我们引入了一种用于SNN的注意力机制，可以在学习过程中有选择地关注图中重要的节点和相应特征。我们在几个基准数据集上评估了我们提出的方法，并展示了它与现有图学习技术相比具有可比较的性能。

    arXiv:2403.17040v1 Announce Type: new  Abstract: Graph representation learning has become a crucial task in machine learning and data mining due to its potential for modeling complex structures such as social networks, chemical compounds, and biological systems. Spiking neural networks (SNNs) have recently emerged as a promising alternative to traditional neural networks for graph learning tasks, benefiting from their ability to efficiently encode and process temporal and spatial information. In this paper, we propose a novel approach that integrates attention mechanisms with SNNs to improve graph representation learning. Specifically, we introduce an attention mechanism for SNN that can selectively focus on important nodes and corresponding features in a graph during the learning process. We evaluate our proposed method on several benchmark datasets and show that it achieves comparable performance compared to existing graph learning techniques.
    
[^117]: 神经架构搜索中的进化与效率：弥合专家设计与自动优化之间的鸿沟

    Evolution and Efficiency in Neural Architecture Search: Bridging the Gap Between Expert Design and Automated Optimization

    [https://arxiv.org/abs/2403.17012](https://arxiv.org/abs/2403.17012)

    本文综述了神经架构搜索（NAS）领域的进化历程，介绍了从手动设计到自动化优化的演变过程，探讨了NAS在各个领域的应用，以及针对计算效率挑战提出的高效NAS方法。

    

    本文全面介绍了神经架构搜索（NAS），强调了它从手动设计到自动化、计算驱动方法的演变。它涵盖了NAS的起源和发展，突出了其在各个领域的应用，包括医学影像和自然语言处理。文章详细阐述了从专家驱动设计到算法驱动过程的转变，探讨了强化学习和进化算法等初始方法。还讨论了计算需求的挑战以及高效NAS方法的出现，如可微架构搜索和硬件感知NAS。该论文进一步阐述了NAS在计算机视觉、NLP等领域的应用，展示了其在不同任务中优化神经网络架构的多功能性和潜力。探讨了未来的方向和挑战，包括计算效率。

    arXiv:2403.17012v1 Announce Type: cross  Abstract: The paper provides a comprehensive overview of Neural Architecture Search (NAS), emphasizing its evolution from manual design to automated, computationally-driven approaches. It covers the inception and growth of NAS, highlighting its application across various domains, including medical imaging and natural language processing. The document details the shift from expert-driven design to algorithm-driven processes, exploring initial methodologies like reinforcement learning and evolutionary algorithms. It also discusses the challenges of computational demands and the emergence of efficient NAS methodologies, such as Differentiable Architecture Search and hardware-aware NAS. The paper further elaborates on NAS's application in computer vision, NLP, and beyond, demonstrating its versatility and potential for optimizing neural network architectures across different tasks. Future directions and challenges, including computational efficiency
    
[^118]: SUDO：一种无需真实标注评估临床人工智能系统的框架

    SUDO: a framework for evaluating clinical artificial intelligence systems without ground-truth annotations

    [https://arxiv.org/abs/2403.17011](https://arxiv.org/abs/2403.17011)

    SUDO框架允许在缺乏真实标注的情况下评估AI系统，通过为实际数据点分配临时标签并直接使用它们训练模型来解决分布转移问题，从而提高对临床数据的可信度。

    

    临床人工智能（AI）系统通常在一个未曝光过的数据集上进行验证（例如来自具有不同电子健康记录系统的不同医院的数据）。这种评估过程旨在模拟将AI系统部署在未被系统见过但在临床环境中预计会遇到的数据上；然而，当实际数据与未曝光的数据集不同时，即分布转移现象，并且缺乏真实标注时，不清楚基于AI的发现在实际数据上能否受信任。在这里，我们介绍SUDO，一种用于评估无需真实标注的AI系统的框架。SUDO为实际数据点分配临时标签，并直接使用这些标签训练不同模型，表现最优的模型表明最可能的标签。

    arXiv:2403.17011v1 Announce Type: cross  Abstract: A clinical artificial intelligence (AI) system is often validated on a held-out set of data which it has not been exposed to before (e.g., data from a different hospital with a distinct electronic health record system). This evaluation process is meant to mimic the deployment of an AI system on data in the wild; those which are currently unseen by the system yet are expected to be encountered in a clinical setting. However, when data in the wild differ from the held-out set of data, a phenomenon referred to as distribution shift, and lack ground-truth annotations, it becomes unclear the extent to which AI-based findings can be trusted on data in the wild. Here, we introduce SUDO, a framework for evaluating AI systems without ground-truth annotations. SUDO assigns temporary labels to data points in the wild and directly uses them to train distinct models, with the highest performing model indicative of the most likely label. Through exp
    
[^119]: LLM Agent Operating System

    LLM Agent Operating System

    [https://arxiv.org/abs/2403.16971](https://arxiv.org/abs/2403.16971)

    提出了一种将大型语言模型嵌入操作系统中的LLM代理操作系统，旨在优化资源分配、促进代理间上下文切换、实现并发执行以及为代理提供工具服务。

    

    arXiv:2403.16971v1 公告类型: 跨领域 摘要: 部署大型语言模型（LLM）智能代理存在诸多挑战，会损害它们的效率和功效。其中包括代理请求在LLM上的次优调度和资源分配、在代理和LLM之间交互时保持上下文的困难，以及将具有不同能力和专业化的异构代理集成在一起的复杂性。代理数量和复杂性的快速增加进一步加剧了这些问题，通常会导致资源瓶颈和次优资源利用。受到这些挑战的启发，本文提出了AIOS，一种LLM代理操作系统，它将大型语言模型嵌入操作系统（OS）中。具体地，AIOS旨在优化资源分配，促进代理之间的上下文切换，实现代理的并发执行，为代理提供工具服务。

    arXiv:2403.16971v1 Announce Type: cross  Abstract: The integration and deployment of large language model (LLM)-based intelligent agents have been fraught with challenges that compromise their efficiency and efficacy. Among these issues are sub-optimal scheduling and resource allocation of agent requests over the LLM, the difficulties in maintaining context during interactions between agent and LLM, and the complexities inherent in integrating heterogeneous agents with different capabilities and specializations. The rapid increase of agent quantity and complexity further exacerbates these issues, often leading to bottlenecks and sub-optimal utilization of resources. Inspired by these challenges, this paper presents AIOS, an LLM agent operating system, which embeds large language model into operating systems (OS). Specifically, AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents
    
[^120]: 与人类判断相一致：大型语言模型评估中成对偏好的作用

    Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators

    [https://arxiv.org/abs/2403.16950](https://arxiv.org/abs/2403.16950)

    在大型语言模型评估中，通过引入成对偏好搜索方法PAIRS，成功解决了LLMs与人类判断不一致的问题，并取得了优于直接打分的最先进性能。

    

    大型语言模型（LLMs）作为自动评估器在评估生成的自然语言质量方面表现出有希望的能力。然而，LLMs在评估中仍存在偏见，常常难以生成与人类评估一致的连贯评估。在这项工作中，我们首先对LLM评估器与人类判断之间的不一致进行系统研究，揭示现有旨在减轻偏见的校准方法不足以有效将LLM评估器对齐。受到RLHF中对偏好数据的使用的启发，我们将评估形式化为一个排序问题，并引入Pairwise-preference Search（PAIRS），这是一种以LLMs进行成对比较并有效对候选文本进行排序的基于不确定性引导的搜索方法。PAIRS在代表性评估任务上实现了最先进的性能，并且显示出比直接打分有显著改进。

    arXiv:2403.16950v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PAIRS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthe
    
[^121]: 利用预训练语言模型进行粗调优的专题文档检索

    Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models

    [https://arxiv.org/abs/2403.16915](https://arxiv.org/abs/2403.16915)

    本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调，在专题文档检索中显著改善了效果。

    

    在信息检索系统中，利用预训练语言模型（PLM-based IR）进行微调需要学习查询表示和查询-文档关系，除了下游任务特定的学习。本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调。通过在粗调优学习查询表示和查询-文档关系，我们旨在减少微调的负担，提高下游IR任务的学习效果。我们提出了用于粗调优的查询-文档对预测（QDPP），其预测查询-文档对的适当性。评估实验显示，所提出的方法显著改善了四个专题文档检索数据集中的MRR和/或nDCG@5。此外，查询预测任务的结果表明，粗调优促进了查询表示和查询-文档关系的学习。

    arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
    
[^122]: 理解欧盟AI法案: 合规安全关键产品的方法论途径

    Navigating the EU AI Act: A Methodological Approach to Compliance for Safety-critical Products

    [https://arxiv.org/abs/2403.16808](https://arxiv.org/abs/2403.16808)

    通过利用产品质量模型和合同法方法，本文提出了一种解释欧盟AI法案对高风险AI系统要求的方法论途径。

    

    2023年12月，欧洲议会暂时同意了欧盟AI法案。这一前所未有的AI系统监管框架制定了确保AI产品安全、合法和值得信赖的指导方针。本文提出了一种方法，通过利用产品质量模型来解释高风险AI系统在欧盟AI法案下的要求。我们首先提出了一个扩展的AI系统产品质量模型，将法案中未涵盖的相关属性纳入考虑。我们将法案要求与相关质量属性进行映射，目的是将其细化为可衡量的特征。然后，我们提出了一种基于合同的方法，从利益相关者层面推导技术要求。这有助于开发和评估不仅符合已建立质量标准，而且符合法案中针对高风险（包括

    arXiv:2403.16808v1 Announce Type: new  Abstract: In December 2023, the European Parliament provisionally agreed on the EU AI Act. This unprecedented regulatory framework for AI systems lays out guidelines to ensure the safety, legality, and trustworthiness of AI products. This paper presents a methodology for interpreting the EU AI Act requirements for high-risk AI systems by leveraging product quality models. We first propose an extended product quality model for AI systems, incorporating attributes relevant to the Act not covered by current quality models. We map the Act requirements to relevant quality attributes with the goal of refining them into measurable characteristics. We then propose a contract-based approach to derive technical requirements at the stakeholder level. This facilitates the development and assessment of AI systems that not only adhere to established quality standards, but also comply with the regulatory requirements outlined in the Act for high-risk (including 
    
[^123]: 和抛硬币一样好：人类对AI生成的图像、视频、音频和音视频刺激的检测

    As Good As A Coin Toss Human detection of AI-generated images, videos, audio, and audiovisual stimuli

    [https://arxiv.org/abs/2403.16760](https://arxiv.org/abs/2403.16760)

    通过一项感知研究，评估了人们在日常生活中对合成图像、音频、视频和音视频刺激与真实的区分能力，以探讨人类对欺骗性合成媒体的易受程度。

    

    随着合成媒体变得越来越逼真，使用它的障碍不断降低，这项技术越来越被恶意利用，从金融欺诈到非自愿色情。今天，对抗被合成媒体误导的主要防御依赖于人类观察者在视觉和听觉上区分真假的能力。然而，人们在日常生活中实际上对欺骗性合成媒体有多脆弱仍不清楚。我们进行了一个包含1276名参与者的感知研究，评估人们在区分合成图像、仅音频、仅视频和音视频刺激与真实的准确性如何。为了反映人们在野外可能遇到合成媒体的情况，测试条件和刺激模拟了典型的在线平台，而调查中使用的所有合成媒体均来自

    arXiv:2403.16760v1 Announce Type: cross  Abstract: As synthetic media becomes progressively more realistic and barriers to using it continue to lower, the technology has been increasingly utilized for malicious purposes, from financial fraud to nonconsensual pornography. Today, the principal defense against being misled by synthetic media relies on the ability of the human observer to visually and auditorily discern between real and fake. However, it remains unclear just how vulnerable people actually are to deceptive synthetic media in the course of their day to day lives. We conducted a perceptual study with 1276 participants to assess how accurate people were at distinguishing synthetic images, audio only, video only, and audiovisual stimuli from authentic. To reflect the circumstances under which people would likely encounter synthetic media in the wild, testing conditions and stimuli emulated a typical online platform, while all synthetic media used in the survey was sourced from 
    
[^124]: CLHA: 一种简单而有效的用于人类对齐的对比学习框架

    CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment

    [https://arxiv.org/abs/2403.16649](https://arxiv.org/abs/2403.16649)

    CLHA提出了一种简单而有效的对比学习框架，可以帮助大型语言模型与人类偏好对齐，通过新颖的重评分策略和损失函数调整，在提升对齐效果的同时简化了训练过程。

    

    人类反馈（RLHF）的强化学习是将大型语言模型（LLMs）与人类偏好对齐的关键技术，确保这些LLMs以对用户有益且易于理解的方式行为。然而，基于RL的人类对齐技术中存在的长期挑战在于其固有的复杂性和训练的困难。为了解决这一挑战，我们提出了一种简单而有效的用于人类对齐的对比学习框架（CLHA），直接将LLMs与人类偏好对齐。CLHA采用一种新颖的重评分策略来评估数据内的噪声，考虑其固有质量并动态调整训练过程。同时，CLHA利用成对对比损失和自适应监督微调损失来自适应地修改生成响应的可能性，确保与人类偏好的增强对齐。使用先进方法，CLHA超越了其他

    arXiv:2403.16649v1 Announce Type: new  Abstract: Reinforcement learning from human feedback (RLHF) is a crucial technique in aligning large language models (LLMs) with human preferences, ensuring these LLMs behave in beneficial and comprehensible ways to users. However, a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training. To address this challenge, we present a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA) to align LLMs with human preferences directly. CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process. Simultaneously, CLHA utilizes pairwise contrastive loss and adaptive supervised fine-tuning loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences. Using advanced methods, CLHA surpasses oth
    
[^125]: DeepMachining: 铣床机床加工误差在线预测

    DeepMachining: Online Prediction of Machining Errors of Lathe Machines

    [https://arxiv.org/abs/2403.16451](https://arxiv.org/abs/2403.16451)

    DeepMachining是一种基于深度学习的AI系统，可以在线预测车床机床加工操作的误差，通过预训练和微调模型，实现了高准确性预测，是首批使用预训练深度学习模型预测车床机床加工误差的工厂实验之一。

    

    我们描述了DeepMachining，这是一种基于深度学习的人工智能系统，用于在线预测车床加工操作的加工误差。我们基于工厂的制造数据构建并评估了DeepMachining。具体来说，我们首先对特定车床机床操作预训练深度学习模型，以学习加工状态的显著特征。然后，我们微调预训练模型以适应特定加工任务。我们展示了DeepMachining在涉及不同工件和刀具的多个任务中实现了高预测准确性。据我们所知，这项工作是使用预训练深度学习模型预测车床机床加工误差的首批工厂实验之一。

    arXiv:2403.16451v1 Announce Type: cross  Abstract: We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states. Then, we fine-tune the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.
    
[^126]: Re2LLM: 反射式强化大型语言模型用于基于会话的推荐

    Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation

    [https://arxiv.org/abs/2403.16427](https://arxiv.org/abs/2403.16427)

    Re2LLM是为基于会话的推荐提出的反射式强化大型语言模型，引导LLMs专注于更准确推荐所需的专业知识，实现有效和高效。

    

    大型语言模型(LLMs)正日益被看作是增强基于会话推荐(SBR)的有前途的方法，其中已广泛研究了基于提示和微调的方法，以使LLMs与SBR对齐。然而，前者因缺乏任务特定反馈而难以找到引导LLMs正确推理的最佳提示，导致推荐结果不佳。尽管后者试图用领域特定知识微调LLMs，但它们面临诸如高计算成本和依赖开源骨干的限制。为解决这些问题，我们提出了一种用于SBR的反射式强化大型语言模型(Re2LLM)，引导LLMs专注于更准确推荐所需的专业知识，实现有效和高效。具体来说，我们首先设计了反射式探索模块

    arXiv:2403.16427v1 Announce Type: new  Abstract: Large Language Models (LLMs) are emerging as promising approaches to enhance session-based recommendation (SBR), where both prompt-based and fine-tuning-based methods have been widely investigated to align LLMs with SBR.   However, the former methods struggle with optimal prompts to elicit the correct reasoning of LLMs due to the lack of task-specific feedback, leading to unsatisfactory recommendations.   Although the latter methods attempt to fine-tune LLMs with domain-specific knowledge, they face limitations such as high computational costs and reliance on open-source backbones.   To address such issues, we propose a \underline{Re}flective \underline{Re}inforcement \underline{L}arge \underline{L}anguage \underline{M}odel (Re2LLM) for SBR, guiding LLMs to focus on specialized knowledge essential for more accurate recommendations effectively and efficiently.   In particular, we first design the Reflective Exploration Module to effective
    
[^127]: 生物医学与健康信息学中的大型语言模型：一项文献计量学综述

    Large Language Models in Biomedical and Health Informatics: A Bibliometric Review

    [https://arxiv.org/abs/2403.16303](https://arxiv.org/abs/2403.16303)

    LLMs已成为生物医学与健康信息学中重要的工具，本文献计量学综述全面展示了LLMs在各种BHI领域中的应用，提出了其对自然语言处理应用的改进，揭示了主要发展趋势和研究网络，并讨论了伦理关切和实际挑战。

    

    大型语言模型（LLMs）迅速成为生物医学与健康信息学（BHI）中的重要工具，为分析数据、治疗患者和开展研究提供了新的方式。本文献计量学综述旨在通过检查自2022年至2023年的研究文章和合作网络，全面展示LLMs在BHI中的应用情况。它进一步探讨了LLMs如何可以改进各种BHI领域中的自然语言处理（NLP）应用，如医学诊断、患者参与、电子健康记录管理和个性化医学。为此，我们的文献计量学综述确定了关键趋势，绘制了研究网络，并突出了这个快速发展领域的主要进展。最后，它讨论了在BHI中使用LLMs的伦理关切和实际挑战，如数据隐私和可靠的医疗建议。展望未来，我们考虑LLMs如何进一步改变生物医学研究。

    arXiv:2403.16303v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have rapidly become important tools in Biomedical and Health Informatics (BHI), enabling new ways to analyze data, treat patients, and conduct research. This bibliometric review aims to provide a panoramic view of how LLMs have been used in BHI by examining research articles and collaboration networks from 2022 to 2023. It further explores how LLMs can improve Natural Language Processing (NLP) applications in various BHI areas like medical diagnosis, patient engagement, electronic health record management, and personalized medicine. To do this, our bibliometric review identifies key trends, maps out research networks, and highlights major developments in this fast-moving field. Lastly, it discusses the ethical concerns and practical challenges of using LLMs in BHI, such as data privacy and reliable medical recommendations. Looking ahead, we consider how LLMs could further transform biomedical research as we
    
[^128]: 在照料机器人中猜测人类意图以避免危险情况

    Guessing human intentions to avoid dangerous situations in caregiving robots

    [https://arxiv.org/abs/2403.16291](https://arxiv.org/abs/2403.16291)

    本文探讨了在照料机器人中使用人工心智理论来猜测人类意图，提出了一种检测危险情况并实时消除危险的算法，在模拟实验中取得了高成功率。

    

    要求机器人进行社交互动，它们必须准确解释人类意图并预测潜在结果。对于为人类护理设计的社交机器人而言尤为重要，可能会面临人类的危险情况，比如未见障碍物，应该予以避免。本文探讨了人工心智理论（ATM）方法来推断和解释人类意图。我们提出了一种检测人类风险情况的算法，选择实时消除危险的机器人动作。我们采用基于模拟的ATM方法，并采用“像我一样”的策略将意图和动作分配给人类。通过这种策略，机器人在有限时间内可以高成功率地检测和行动。该算法已经作为现有机器人认知架构的一部分实施，并在模拟场景中进行了测试。进行了三个实验。

    arXiv:2403.16291v1 Announce Type: cross  Abstract: For robots to interact socially, they must interpret human intentions and anticipate their potential outcomes accurately. This is particularly important for social robots designed for human care, which may face potentially dangerous situations for people, such as unseen obstacles in their way, that should be avoided. This paper explores the Artificial Theory of Mind (ATM) approach to inferring and interpreting human intentions. We propose an algorithm that detects risky situations for humans, selecting a robot action that removes the danger in real time. We use the simulation-based approach to ATM and adopt the 'like-me' policy to assign intentions and actions to people. Using this strategy, the robot can detect and act with a high rate of success under time-constrained situations. The algorithm has been implemented as part of an existing robotics cognitive architecture and tested in simulation scenarios. Three experiments have been co
    
[^129]: 基于分层非负矩阵分解的网络安全知识图生成

    Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative Matrix Factorization

    [https://arxiv.org/abs/2403.16222](https://arxiv.org/abs/2403.16222)

    本文介绍了一种通过从科学论文中提取结构化本体来构建网络安全领域多模态知识图的方法

    

    许多网络安全领域的人类知识都被封装在不断增长的科学论文中。随着这些文本数据的不断扩大，文档组织方法的重要性变得日益关键，用于从大型文本数据集中提取隐藏的可行见解。知识图（KGs）作为一种以结构化方式存储实际信息的手段，提供包括来自网络安全科学文献的领域特定信息在内的明确、可解释的知识。构建从科学文献中的知识图是提取本体的一个挑战。在本文中，我们讨论了这个主题，并介绍了一种通过从科学论文中提取结构化本体来构建多模态KG的方法。我们在网络安全领域展示了这一概念。KG的一种模态代表了论文中的可观察信息，如目录等

    arXiv:2403.16222v1 Announce Type: new  Abstract: Much of human knowledge in cybersecurity is encapsulated within the ever-growing volume of scientific papers. As this textual data continues to expand, the importance of document organization methods becomes increasingly crucial for extracting actionable insights hidden within large text datasets. Knowledge Graphs (KGs) serve as a means to store factual information in a structured manner, providing explicit, interpretable knowledge that includes domain-specific information from the cybersecurity scientific literature. One of the challenges in constructing a KG from scientific literature is the extraction of ontology from unstructured text. In this paper, we address this topic and introduce a method for building a multi-modal KG by extracting structured ontology from scientific papers. We demonstrate this concept in the cybersecurity domain. One modality of the KG represents observable information from the papers, such as the categories i
    
[^130]: 新闻报道场景中的图像描述

    Image Captioning in news report scenario

    [https://arxiv.org/abs/2403.16209](https://arxiv.org/abs/2403.16209)

    本论文探索了专门针对名人照片的图像描述，旨在增强新闻行业实践，并提出了对自动新闻内容生成的改进方法。

    

    arXiv:2403.16209v1 公告类型: 跨领域 摘要: 图像描述旨在为指定的图像生成相关的描述，使其处于计算机视觉（CV）和自然语言处理（NLP）的交叉点。这项努力在推荐系统、新闻媒体、社交媒体等领域具有重要意义。特别是在新闻报道领域，标题应涵盖详细信息，如图像中捕捉到的名人的身份。然而，现有大部分工作主要集中于理解场景和动作。本文探讨了专门针对名人照片的图像描述领域，展示了其在增强新闻行业实践方面的广泛潜力。这一探索旨在增强自动化新闻内容生成，从而促进更加细致地传播信息。我们的努力展示了一个更广阔的视野，丰富了n

    arXiv:2403.16209v1 Announce Type: cross  Abstract: Image captioning strives to generate pertinent captions for specified images, situating itself at the crossroads of Computer Vision (CV) and Natural Language Processing (NLP). This endeavor is of paramount importance with far-reaching applications in recommendation systems, news outlets, social media, and beyond. Particularly within the realm of news reporting, captions are expected to encompass detailed information, such as the identities of celebrities captured in the images. However, much of the existing body of work primarily centers around understanding scenes and actions. In this paper, we explore the realm of image captioning specifically tailored for celebrity photographs, illustrating its broad potential for enhancing news industry practices. This exploration aims to augment automated news content generation, thereby facilitating a more nuanced dissemination of information. Our endeavor shows a broader horizon, enriching the n
    
[^131]: 一种新颖的图神经网络方法用于谣言检测

    Rumor Detection with a novel graph neural network approach

    [https://arxiv.org/abs/2403.16206](https://arxiv.org/abs/2403.16206)

    本论文提出了一种新颖的检测模型，同时学习用户相关性和信息传播的表示，以检测社交媒体上的谣言

    

    社交媒体上谣言的广泛传播对人们的日常生活造成了负面影响，导致公众产生潜在的恐慌、恐惧和心理健康问题。如何尽早揭穿谣言仍然是一个具有挑战性的问题。现有研究主要利用信息传播结构来检测谣言，而很少有研究关注用户之间的相关性，即他们可能协调传播谣言以获得较大的流行度。在本文中，我们提出了一种新的检测模型，同时学习用户相关性和信息传播的表示，以便检测社交媒体上的谣言。具体而言，我们利用图神经网络从描述用户和来源推文之间相关性的二部图中学习用户相关性的表示，以及使用树结构学习信息传播的表示。然后，我们结合得到的表示

    arXiv:2403.16206v1 Announce Type: new  Abstract: The wide spread of rumors on social media has caused a negative impact on people's daily life, leading to potential panic, fear, and mental health problems for the public. How to debunk rumors as early as possible remains a challenging problem. Existing studies mainly leverage information propagation structure to detect rumors, while very few works focus on correlation among users that they may coordinate to spread rumors in order to gain large popularity. In this paper, we propose a new detection model, that jointly learns both the representations of user correlation and information propagation to detect rumors on social media. Specifically, we leverage graph neural networks to learn the representations of user correlation from a bipartite graph that describes the correlations between users and source tweets, and the representations of information propagation with a tree structure. Then we combine the learned representations from these 
    
[^132]: X-Portrait: 具有分层动作注意力的表现性肖像动画

    X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention

    [https://arxiv.org/abs/2403.15931](https://arxiv.org/abs/2403.15931)

    这里是中文总结出的一句话要点: 该论文提出了X-Portrait，一种用于生成具有表现力和时间连贯性的肖像动画的条件扩散模型，利用控制信号实现了细粒度头部姿势和表情控制，以提高运动精度。

    

    我们提出了X-Portrait，这是一种创新的条件扩散模型，专门用于生成具有表现力和时间连贯性的肖像动画。具体而言，我们旨在基于单个肖像作为外观参考，并利用来自驱动视频的运动来为其添加动画，捕捉具有高度动态性和微妙面部表情以及广泛范围头部运动。在其核心部分，我们利用了预先训练的扩散模型的生成先验作为渲染骨架，同时在ControlNet框架内通过新颖的控制信号实现了细粒度头部姿势和表情控制。与传统的粗糙显式控制（如面部标志点）不同，我们的运动控制模块学会直接从原始驱动RGB输入中解读动态。通过有效增强对眼神等小尺度细微差异的运动关注的基于补丁的局部控制模块，进一步提高了运动精度。

    arXiv:2403.15931v1 Announce Type: cross  Abstract: We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet. In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeba
    
[^133]: 面向电子离子对撞机的基于RAG的摘要生成代理

    Towards a \textbf{RAG}-based Summarization Agent for the Electron-Ion Collider

    [https://arxiv.org/abs/2403.15729](https://arxiv.org/abs/2403.15729)

    开发了一种面向电子离子对撞机的基于RAG的摘要生成代理，能够压缩信息并引用相关回复，为合作者提供重大优势

    

    复杂性和庞大的信息量涵盖了大规模实验的文件、论文、数据和其他资源，导致导航这些多样形式信息的任务需要大量时间和精力，对于新合作者和早期科学家来说尤为艰巨。为了解决这个问题，正在开发一种基于检索增强生成（RAG）的EIC摘要生成人工智能代理（RAGS4EIC）。该人工智能代理不仅压缩信息，还有效引用相关回复，为合作者提供了重大优势。我们的项目采取了两步方法：首先，查询包含所有相关实验信息的综合向量数据库；其次，利用大型语言模型（LLM）根据用户查询和检索数据生成包含引用的简洁摘要。我们描述了使用RAG评估的评估方法

    arXiv:2403.15729v1 Announce Type: cross  Abstract: The complexity and sheer volume of information encompassing documents, papers, data, and other resources from large-scale experiments demand significant time and effort to navigate, making the task of accessing and utilizing these varied forms of information daunting, particularly for new collaborators and early-career scientists. To tackle this issue, a Retrieval Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under development. This AI-Agent not only condenses information but also effectively references relevant responses, offering substantial advantages for collaborators. Our project involves a two-step approach: first, querying a comprehensive vector database containing all pertinent experiment information; second, utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations based on user queries and retrieved data. We describe the evaluation methods that use RAG assessments 
    
[^134]: MedPromptX：基于现实的多模态提示用于胸部X线诊断

    MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis

    [https://arxiv.org/abs/2403.15585](https://arxiv.org/abs/2403.15585)

    MedPromptX是第一个将多模态大型语言模型、少样本提示和视觉基础相结合，用于胸部X线诊断的模型，通过补充缺失的EHR信息，有效解决了幻觉问题，但选择最佳少样本示例和高质量候选者仍有待解决。

    

    胸部X线图像通常用于预测急性和慢性心肺疾病，但是将它们与结构化临床数据整合的努力面临着因电子健康记录（EHR）不完整而带来的挑战。本文引入了MedPromptX，这是第一个将多模态大型语言模型（MLLMs）、少样本提示（FP）和视觉基础（VG）相结合，将图像与EHR数据用于胸部X线诊断的模型。预训练的MLLM被用来补充缺失的EHR信息，提供对患者病史的全面理解。此外，少样本提示减少了对MLLM的大量训练的必要性，同时有效解决了幻觉问题。然而，确定最佳少样本示例的过程和选择高质量候选者可能过于繁琐，但它对模型性能产生着深远影响。因此，我们提出了一种新技术来动态地...

    arXiv:2403.15585v1 Announce Type: cross  Abstract: Chest X-ray images are commonly used for predicting acute and chronic cardiopulmonary conditions, but efforts to integrate them with structured clinical data face challenges due to incomplete electronic health records (EHR). This paper introduces \textbf{MedPromptX}, the first model to integrate multimodal large language models (MLLMs), few-shot prompting (FP) and visual grounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A pre-trained MLLM is utilized to complement the missing EHR information, providing a comprehensive understanding of patients' medical history. Additionally, FP reduces the necessity for extensive training of MLLMs while effectively tackling the issue of hallucination. Nevertheless, the process of determining the optimal number of few-shot examples and selecting high-quality candidates can be burdensome, yet it profoundly influences model performance. Hence, we propose a new technique that dynam
    
[^135]: NaNa和MiGu：语义数据增强技术在图神经网络中增强蛋白质分类

    NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein Classification in Graph Neural Networks

    [https://arxiv.org/abs/2403.14736](https://arxiv.org/abs/2403.14736)

    提出了NaNa和MiGu两种语义数据增强方法，结合了蛋白质的主链化学和侧链生物物理信息，用于增强图神经网络中的蛋白质分类任务。

    

    蛋白质分类任务在药物发现中至关重要。现实世界中的蛋白质结构是动态变化的，这将决定蛋白质的性质。然而，现有的机器学习方法，如ProNet，仅访问有限的构象特征和蛋白质侧链特征，导致预测中蛋白质结构的不切实际和蛋白质类别的不准确性。在本文中，我们提出了新颖的语义数据增强方法NaNa和MiGu，将蛋白质主链化学和侧链生物物理信息纳入蛋白质分类任务和共嵌残差学习框架。具体来说，我们利用了蛋白质的分子生物物理、二级结构、化学键和离子特征来促进蛋白质分类任务。

    arXiv:2403.14736v1 Announce Type: cross  Abstract: Protein classification tasks are essential in drug discovery. Real-world protein structures are dynamic, which will determine the properties of proteins. However, the existing machine learning methods, like ProNet (Wang et al., 2022a), only access limited conformational characteristics and protein side-chain features, leading to impractical protein structure and inaccuracy of protein classes in their predictions. In this paper, we propose novel semantic data augmentation methods, Novel Augmentation of New Node Attributes (NaNa), and Molecular Interactions and Geometric Upgrading (MiGu) to incorporate backbone chemical and side-chain biophysical information into protein classification tasks and a co-embedding residual learning framework. Specifically, we leverage molecular biophysical, secondary structure, chemical bonds, and ionic features of proteins to facilitate protein classification tasks. Furthermore, our semantic augmentation me
    
[^136]: 周期性对数温度调度作为学习率调度器

    Cyclical Log Annealing as a Learning Rate Scheduler

    [https://arxiv.org/abs/2403.14685](https://arxiv.org/abs/2403.14685)

    该论文介绍了一种新的对数方法作为学习率调度器，通过更积极的重启模式，可能使得在在线凸优化框架上使用更贪婪的算法，实验结果表明它性能类似于余弦退火方案。

    

    学习率调度器是一组预定义的指令，用于在模型训练过程中改变搜索步长。本文介绍了一种新的对数方法，通过随机梯度下降对步长进行严格的重启。周期性对数温度调度更积极地实现了重启模式，或许可以允许在在线凸优化框架上使用更贪婪的算法。该算法在CIFAR-10图像数据集上进行了测试，似乎在大型变压器增强残差神经网络上的余弦退火方案表现类似。未来的实验将涉及在生成对抗网络中测试调度器，并通过更多实验找到调度器的最佳参数。

    arXiv:2403.14685v1 Announce Type: new  Abstract: A learning rate scheduler is a predefined set of instructions for varying search stepsizes during model training processes. This paper introduces a new logarithmic method using harsh restarting of step sizes through stochastic gradient descent. Cyclical log annealing implements the restart pattern more aggressively to maybe allow the usage of more greedy algorithms on the online convex optimization framework. The algorithm was tested on the CIFAR-10 image datasets, and seemed to perform analogously with cosine annealing on large transformer-enhanced residual neural networks. Future experiments would involve testing the scheduler in generative adversarial networks and finding the best parameters for the scheduler with more experiments.
    
[^137]: 出身富贵？探讨大型语言模型中的社会经济偏见

    Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models

    [https://arxiv.org/abs/2403.14633](https://arxiv.org/abs/2403.14633)

    本文调查了大型语言模型中是否存在社会经济偏见，引入了一个新的数据集SilverSpoon，并评估了这种偏见的程度以及随着模型大小的变化。

    

    社会经济偏见在社会中加剧了不公平现象，根据个人经济和社会背景影响获取机会和资源的机会。这一普遍问题持续地延续了系统性的不平等，阻碍了作为一个社会追求包容性进步。在本文中，我们调查了大型语言模型中是否存在社会经济偏见。为此，我们引入了一个新的数据集（SilverSpoon），包含3000个样本，展示了牵涉到弱势群体由于他们的处境而实施道德模糊行为的假设情景，并问这种行为是否在道德上成立。此外，这个数据集具有双重标记方案，并由属于社会经济两端的人进行了注释。使用SilverSpoon，我们评估了大型语言模型中表现出的社会经济偏见程度以及该程度如何随模型大小变化。

    arXiv:2403.14633v1 Announce Type: cross  Abstract: Socioeconomic bias in society exacerbates disparities, influencing access to opportunities and resources based on individuals' economic and social backgrounds. This pervasive issue perpetuates systemic inequalities, hindering the pursuit of inclusive progress as a society. In this paper, we investigate the presence of socioeconomic bias, if any, in large language models. To this end, we introduce a novel dataset (SilverSpoon), consisting of 3000 samples that illustrate hypothetical scenarios that involve underprivileged people performing ethically ambiguous actions due to their circumstances, and ask whether the action is ethically justified. Further, this dataset has a dual-labeling scheme and has been annotated by people belonging to both ends of the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of socioeconomic bias expressed in large language models and the variation of this degree as a function of model size. W
    
[^138]: 聊天GPT能够检测DeepFakes吗？使用多模态大型语言模型进行媒体取证的研究

    Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics

    [https://arxiv.org/abs/2403.14077](https://arxiv.org/abs/2403.14077)

    本研究探讨了使用多模态大型语言模型（LLMs）进行DeepFake检测的能力，通过实验证明它们能够揭示AI生成的图像，尽管LLMs并非专为媒体取证任务设计，这一发现具有重要意义。

    

    DeepFakes是指由人工智能生成的媒体内容，由于其被用作散布虚假信息的手段，已经成为越来越令人担忧的问题。当前检测DeepFakes的方法是利用编程的机器学习算法。本研究调查了多模态大型语言模型（LLMs）在DeepFake检测中的能力。通过定性和定量实验，我们展示了多模态LLMs可以通过谨慎的实验设计和及时的工程方法揭示AI生成的图像。考虑到LLMs并不是本质上为媒体取证任务量身定制的，这一点相当有趣，而且这个过程并不需要编程。我们讨论了多模态LLMs在这些任务中的局限性，并提出了可能的改进方向。

    arXiv:2403.14077v1 Announce Type: new  Abstract: DeepFakes, which refer to AI-generated media content, have become an increasing concern due to their use as a means for disinformation. Detecting DeepFakes is currently solved with programmed machine learning algorithms. In this work, we investigate the capabilities of multimodal large language models (LLMs) in DeepFake detection. We conducted qualitative and quantitative experiments to demonstrate multimodal LLMs and show that they can expose AI-generated images through careful experimental design and prompt engineering. This is interesting, considering that LLMs are not inherently tailored for media forensic tasks, and the process does not require programming. We discuss the limitations of multimodal LLMs for these tasks and suggest possible improvements.
    
[^139]: 机器学习中的威胁、攻击和防御：一项调查

    Threats, Attacks, and Defenses in Machine Unlearning: A Survey

    [https://arxiv.org/abs/2403.13682](https://arxiv.org/abs/2403.13682)

    机器遗忘（MU）通过知识去除过程来解决训练数据相关的人工智能治理问题，提高了AI系统的安全和负责任使用。

    

    机器遗忘（MU）最近引起了相当大的关注，因为它有潜力通过从训练的机器学习模型中消除特定数据的影响来实现安全人工智能。这个被称为知识去除的过程解决了与训练数据相关的人工智能治理问题，如数据质量、敏感性、版权限制和过时性。这种能力对于确保遵守诸如被遗忘权等隐私法规也至关重要。此外，有效的知识去除有助于减轻有害结果的风险，防范偏见、误导和未经授权的数据利用，从而增强了AI系统的安全和负责任使用。已经开展了设计高效的遗忘方法的工作，通过研究MU服务以与现有的机器学习作为服务集成，使用户能够提交请求从训练语料库中删除特定数据。

    arXiv:2403.13682v2 Announce Type: replace-cross  Abstract: Machine Unlearning (MU) has gained considerable attention recently for its potential to achieve Safe AI by removing the influence of specific data from trained machine learning models. This process, known as knowledge removal, addresses AI governance concerns of training data such as quality, sensitivity, copyright restrictions, and obsolescence. This capability is also crucial for ensuring compliance with privacy regulations such as the Right To Be Forgotten. Furthermore, effective knowledge removal mitigates the risk of harmful outcomes, safeguarding against biases, misinformation, and unauthorized data exploitation, thereby enhancing the safe and responsible use of AI systems. Efforts have been made to design efficient unlearning approaches, with MU services being examined for integration with existing machine learning as a service, allowing users to submit requests to remove specific data from the training corpus. However, 
    
[^140]: 从人类凝视中学习用户嵌入以个性化显著性预测

    Learning User Embeddings from Human Gaze for Personalised Saliency Prediction

    [https://arxiv.org/abs/2403.13653](https://arxiv.org/abs/2403.13653)

    通过眼动数据提取用户嵌入，可以有效个性化显著性预测任务，并且具有良好的泛化能力。

    

    用户行为的可重用嵌入已经显示出对个性化显著性预测任务具有显著的性能提升。然而，先前的工作需要明确的用户特征和偏好作为输入，这通常很难获得。我们提出了一种新颖的方法，从少量用户特定的眼动数据生成的自然图像和相应显著性地图对中提取用户嵌入。我们的方法的核心是一个Siamese卷积神经编码器，通过对比不同用户的图像和个人显著性地图对来学习用户嵌入。在两个公共显著性数据集上的评估表明，生成的嵌入具有很高的区分能力，有效地将通用显著性地图优化到个人用户层面，并且在用户和图像之间具有良好的泛化能力。最后，基于我们模型对个人用户特征的编码能力，我们的工作指出

    arXiv:2403.13653v2 Announce Type: replace-cross  Abstract: Reusable embeddings of user behaviour have shown significant performance improvements for the personalised saliency prediction task. However, prior works require explicit user characteristics and preferences as input, which are often difficult to obtain. We present a novel method to extract user embeddings from pairs of natural images and corresponding saliency maps generated from a small amount of user-specific eye tracking data. At the core of our method is a Siamese convolutional neural encoder that learns the user embeddings by contrasting the image and personal saliency map pairs of different users. Evaluations on two public saliency datasets show that the generated embeddings have high discriminative power, are effective at refining universal saliency maps to the individual users, and generalise well across users and images. Finally, based on our model's ability to encode individual user characteristics, our work points t
    
[^141]: 从细粒度文本描述中生成运动

    Motion Generation from Fine-grained Textual Descriptions

    [https://arxiv.org/abs/2403.13518](https://arxiv.org/abs/2403.13518)

    本文提出了一种从细粒度文本描述中生成运动的方法，构建了FineHumanML3D数据集，设计了FineMotionDiffuse模型，实验结果表明该模型表现出色。

    

    文本到动作的任务是从给定的文字描述生成运动序列，模型应该探索自然语言指令与人体动作之间的交互。大多数现有作品局限于粗粒度的运动描述（例如，“一个人蹲下。”），几乎没有探索指定相关身体部位运动的细粒度描述。用粗糙文本训练的模型可能无法学习从细粒度运动相关词汇到运动基元的映射，导致无法从未见描述生成动作。在本文中，我们通过输入精细提示给 GPT-3.5-turbo，构建了一个细粒度文本描述的大规模语言-动作数据集FineHumanML3D。因此，我们设计了一个新的文本到动作模型FineMotionDiffuse，充分利用细粒度的文本信息。我们的实验表明，FineMotionDiffuse在FineHumanML3D上训练后获得

    arXiv:2403.13518v1 Announce Type: cross  Abstract: The task of text2motion is to generate motion sequences from given textual descriptions, where a model should explore the interactions between natural language instructions and human body movements. While most existing works are confined to coarse-grained motion descriptions (e.g., "A man squats."), fine-grained ones specifying movements of relevant body parts are barely explored. Models trained with coarse texts may not be able to learn mappings from fine-grained motion-related words to motion primitives, resulting in the failure in generating motions from unseen descriptions. In this paper, we build a large-scale language-motion dataset with fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we design a new text2motion model, FineMotionDiffuse, which makes full use of fine-grained textual information. Our experiments show that FineMotionDiffuse trained on FineHumanML3D acqui
    
[^142]: 具有对数据异构性的自适应的拜占庭弹性联邦学习

    Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity

    [https://arxiv.org/abs/2403.13374](https://arxiv.org/abs/2403.13374)

    通过提出新的Robust Average Gradient Algorithm（RAGA），本研究在联邦学习中解决了恶意拜占庭攻击和数据异构性的问题，实现了在非凸损失函数和异构数据集上的收敛性分析，并展示了RAGA的良好收敛性能。

    

    本文处理了在存在恶意拜占庭攻击和数据异构性的情况下的联邦学习（FL）。提出了一种新颖的鲁棒平均梯度算法（RAGA），该算法利用几何中位数进行聚合，并可以自由选择本地更新的轮数。与大多数现有的弹性方法不同，这些方法基于强凸损失函数或均匀分布的数据集进行收敛分析，我们进行了对强凸和非凸损失函数在异构数据集上的收敛分析。根据我们的理论分析，只要恶意用户数据集的比例小于一半，RAGA就可以以$\mathcal{O}({1}/{T^{2/3- \delta}})$的速度实现非凸损失函数的收敛，其中$T$为迭代次数，$\delta \in (0, 2/3)$，对于强凸损失函数则呈线性收敛。此外，稳定点或全局最优解

    arXiv:2403.13374v1 Announce Type: new  Abstract: This paper deals with federated learning (FL) in the presence of malicious Byzantine attacks and data heterogeneity. A novel Robust Average Gradient Algorithm (RAGA) is proposed, which leverages the geometric median for aggregation and can freely select the round number for local updating. Different from most existing resilient approaches, which perform convergence analysis based on strongly-convex loss function or homogeneously distributed dataset, we conduct convergence analysis for not only strongly-convex but also non-convex loss function over heterogeneous dataset. According to our theoretical analysis, as long as the fraction of dataset from malicious users is less than half, RAGA can achieve convergence at rate $\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and $\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for strongly-convex loss function. Moreover, stationary point or global optim
    
[^143]: Hyacinth6B：一个用于中文的大型语言模型

    Hyacinth6B: A large language model for Traditional Chinese

    [https://arxiv.org/abs/2403.13334](https://arxiv.org/abs/2403.13334)

    为了解决大型语言模型通常存在的高硬件和计算需求，Hyacinth6B在模型轻量化和性能之间找到了平衡，采用LoRA方法进行参数高效微调。

    

    这项研究的主要动机是应对通常与大型语言模型相关的高硬件和计算需求。因此，我们的目标是在模型轻量化和性能之间找到平衡，努力在使用相对轻量级模型的同时最大化性能。Hyacinth6B是基于这一目标开发的，旨在充分发挥LLM的核心能力，而不造成巨大的资源成本，有效地推动较小模型的性能边界。训练方法涉及使用LoRA方法进行参数高效微调。

    arXiv:2403.13334v1 Announce Type: new  Abstract: This research's primary motivation of this study is to address the high hardware and computational demands typically associated with LLMs.Therefore,our goal is to find a balance between model lightness and performance,striving to maximize performance while using a comparatively lightweight model. Hyacinth6B was developed with this objective in mind,aiming to fully leverage the core capabilities of LLMs without incurring substantial resource costs, effectively pushing the boundaries of smaller model's performance. The training approach involves parameter efficient finetuning using the LoRA method.
    
[^144]: 将大型语言模型中的领域特定内容融入知识图谱，以增强零样本对象状态分类

    Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification

    [https://arxiv.org/abs/2403.12151](https://arxiv.org/abs/2403.12151)

    大型语言模型与知识图谱结合，提高零样本对象状态分类性能

    

    领域特定知识可以显著有助于解决各种视觉任务，但生成这种知识需要大量人力和时间成本。本研究探讨了大型语言模型（LLMs）在通过语义嵌入生成和提供领域特定信息方面的潜力。为实现这一目标，将LLM集成到一个流程中，该流程在视觉基础零样本对象状态分类任务的背景下利用知识图谱和预训练的语义向量。通过广泛的消融研究彻底研究了LLM的行为。我们的研究结果表明，将基于LLM的嵌入与通用的预训练嵌入结合使用可以显著提高性能。借鉴这一消融研究的见解，我们对竞争模型进行了比较分析，从而突出了最新的表现水平。

    arXiv:2403.12151v1 Announce Type: new  Abstract: Domain-specific knowledge can significantly contribute to addressing a wide variety of vision tasks. However, the generation of such knowledge entails considerable human labor and time costs. This study investigates the potential of Large Language Models (LLMs) in generating and providing domain-specific information through semantic embeddings. To achieve this, an LLM is integrated into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors in the context of the Vision-based Zero-shot Object State Classification task. We thoroughly examine the behavior of the LLM through an extensive ablation study. Our findings reveal that the integration of LLM-based embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements. Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art perfor
    
[^145]: 利用生成式知识提取、基于图的表示和多模态智能图推理加速科学发现

    Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning

    [https://arxiv.org/abs/2403.11996](https://arxiv.org/abs/2403.11996)

    利用生成式人工智能和图算法加速科学发现，揭示论文之间的深入跨学科关系，并提出了新颖的材料设计。

    

    利用生成式人工智能，我们将一组涉及生物材料领域的1,000篇科学论文转化为详细的本体知识图，揭示了它们固有的无标度特性。通过基于节点相似性和介数中心性的组合排名，探测不同概念之间的图遍历路径，我们揭示了深入的跨学科关系，可用于回答查询，识别知识中的空白，并提出前所未见的材料设计及其行为。一项比较揭示了生物材料和贝多芬第九交响曲之间的详细结构相似之处，突显了通过同构映射共享复杂性模式。该算法进一步创建了一种创新的基于分级菌丝体的复合材料，将图采样的联合合成原理与康定斯基《第七组成》中提取的原则相结合

    arXiv:2403.11996v1 Announce Type: cross  Abstract: Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature. Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII p
    
[^146]: SelfIE：大型语言模型嵌入的自我解释

    SelfIE: Self-Interpretation of Large Language Model Embeddings

    [https://arxiv.org/abs/2403.10949](https://arxiv.org/abs/2403.10949)

    提出了SelfIE框架，使大型语言模型能够自解释其嵌入，揭示内部推理，包括道德决策、提示注入和消除有害知识。

    

    arXiv:2403.10949v1 公告类型：交叉摘要：大型语言模型（LLMs）如何获得答案？解释和控制LLM的推理过程对于可靠性、透明度和未来模型发展至关重要。我们提出了SelfIE（嵌入的自我解释），这是一个框架，能够利用LLMs响应关于给定段落的查询的能力，以自然语言解释它们自己的嵌入。SelfIE能够解释隐藏嵌入中的开放世界概念，在案例中揭示LLM的内部推理，如做出道德决策、内化提示注入和回想有害知识。SelfIE对隐藏嵌入的文本描述也开辟了控制LLM推理的新途径。我们提出了监督控制，它允许编辑开放式概念，而只需要计算单个层的梯度。我们将RLHF扩展到隐藏的嵌入，并提出了强化控制来消除有害知识。

    arXiv:2403.10949v1 Announce Type: cross  Abstract: How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge i
    
[^147]: 处理好您的提示偏见！调查和减轻事实知识提取中的提示偏见

    Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction

    [https://arxiv.org/abs/2403.09963](https://arxiv.org/abs/2403.09963)

    本文调查了预训练语言模型在事实知识提取中存在的“提示偏见”，找到了不同类型提示的偏见程度，以及这种偏见对不同基准测试的影响，并提出了一种基于表示的方法来减轻这种提示偏见。

    

    最近的研究表明，预训练语言模型（PLMs）在事实知识提取中存在“提示偏见”，即提示往往会引入对特定标签的偏见。然而，模型内部提示偏见的程度和影响尚未得到充分探讨。为了回应这一点，本文量化了不同类型提示的偏见，并评估了它们对不同基准测试的影响。我们发现：1）实验中的所有提示都表现出不可忽视的偏见，基于梯度的提示如AutoPrompt和OptiPrompt显示出更高水平的偏见；2）提示偏见可以通过过度拟合测试数据集不合理地放大基准测试的准确性，特别是在类似LAMA这样的不平衡数据集上。基于这些发现，我们提出了一种基于表示的方法来减轻提示偏见，在推断时。具体而言，我们首先使用仅提示查询来估计有偏差的表示，然后从中删除。

    arXiv:2403.09963v1 Announce Type: cross  Abstract: Recent research shows that pre-trained language models (PLMs) suffer from "prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce biases toward specific labels. However, the extent and impact of prompt bias within the model remain underexplored. In response, this paper quantifies the bias with various types of prompts and assesses their impact on different benchmarks. We show that: 1) all prompts in the experiments exhibit non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt displaying significantly higher levels of bias; 2) prompt bias can amplify benchmark accuracy unreasonably by overfitting the test datasets, especially on imbalanced datasets like LAMA. Based on these findings, we propose a representation-based approach to mitigate the prompt bias during inference time. Specifically, we first estimate the biased representation using prompt-only querying, and then remove it from the 
    
[^148]: 带有注意力感知自适应提示的少样本类增量学习

    Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt

    [https://arxiv.org/abs/2403.09857](https://arxiv.org/abs/2403.09857)

    提出了一个名为ASP的框架，通过注意力方面减少特定信息，鼓励任务不变的提示来捕获共享知识，并通过信息瓶颈学习目标从旧类到新类传递知识。

    

    少样本类增量学习（FSCIL）模型旨在在保留旧类知识的同时，逐步学习新类别的稀缺样本。现有的FSCIL方法通常对整个骨干进行微调，导致过拟合并阻碍学习新类别的潜力。另一方面，最近基于提示的CIL方法通过在每个任务中用足够的数据训练提示来减轻遗忘。在这项工作中，我们提出了一个名为注意力感知自适应提示（ASP）的新框架。ASP通过从注意力方面减少特定信息，鼓励任务不变的提示来捕获共享知识。此外，ASP中的自适应任务特定提示提供特定信息，并通过信息瓶颈学习目标从旧类到新类传递知识。总之，ASP防止了在基础任务上的过拟合，并不需要在少样本增量任务中使用大量数据。

    arXiv:2403.09857v1 Announce Type: cross  Abstract: Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually fine-tune the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent prompt-based CIL approaches alleviate forgetting by training prompts with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages task-invariant prompts to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific prompts in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in few-shot incremental tasks. Extensi
    
[^149]: 评估大语言模型作为对话推荐中生成用户模拟器

    Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation

    [https://arxiv.org/abs/2403.09738](https://arxiv.org/abs/2403.09738)

    大型语言模型作为生成式用户模拟器在对话推荐中展现出潜力，新的协议通过五个任务评估了语言模型模拟人类行为的准确程度，揭示了模型与人类行为的偏差，并提出了如何通过模型选择和提示策略减少这些偏差。

    

    合成用户是对话推荐系统评估中成本效益较高的真实用户代理。大型语言模型表现出在模拟类似人类行为方面的潜力，这引发了它们能否代表多样化用户群体的问题。我们引入了一个新的协议，用于衡量语言模型能够准确模拟对话推荐中人类行为的程度。该协议由五个任务组成，每个任务旨在评估合成用户应该表现出的关键特性：选择要谈论的物品，表达二进制偏好，表达开放式偏好，请求推荐以及提供反馈。通过对基准模拟器的评估，我们展示了这些任务有效地揭示了语言模型与人类行为的偏差，并提供了关于如何通过模型选择和提示策略减少这些偏差的见解。

    arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
    
[^150]: 持续预训练大型语言模型的简单可扩展策略

    Simple and Scalable Strategies to Continually Pre-train Large Language Models

    [https://arxiv.org/abs/2403.08763](https://arxiv.org/abs/2403.08763)

    通过简单和可扩展的学习率调整、重放数据的方法，可以在不重新训练的情况下，持续预训练大型语言模型以匹配完全重新训练时的性能。

    

    大型语言模型（LLMs）通常在数十亿的标记上进行常规预训练，一旦有新数据可用就重新开始该过程。一个更有效率的解决方案是持续预训练这些模型，与重新训练相比能节省大量计算资源。然而，新数据引起的分布转移通常会导致在以前数据上降低性能或无法适应新数据。在本工作中，我们展示了一种简单且可扩展的学习率（LR）重新升温、LR重新衰减和重放上一数据的组合足以与完全从头开始重新训练在所有可用数据上的性能相匹配，从最终损失和语言模型（LM）评估基准的角度衡量。具体而言，我们展示了在两个常用的LLM预训练数据集（英语→英语）之间的弱但现实的分布转移以及更强烈的分布转移（英语→德语）下的情况。

    arXiv:2403.08763v1 Announce Type: cross  Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at th
    
[^151]: 通过融合高度专业化语言模型同时掌握文本、代码和数学

    Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models

    [https://arxiv.org/abs/2403.08281](https://arxiv.org/abs/2403.08281)

    通过融合高度专业化的语言、代码和数学模型，提出了一种名为UltraFuser的融合框架，引入了标记级别的门控机制，并设计了两阶段训练策略，以同时在三个领域取得高性能。

    

    自然语言、编程代码和数学符号的底层数据分布变化巨大，对于那些努力同时在三个领域实现高性能的大型语言模型（LLMs）提出了复杂挑战。本文提出了一种直接融合已经高度专业化模型的方法。所提出的融合框架UltraFuser包括三个已经在语言、编码和数学上得到充分训练的专家。引入了一个标记级别的门控机制来混合专家的输出。设计了一个伴随平衡采样的两阶段训练策略以确保稳定性。为了有效训练融合模型，我们进一步构建了一个

    arXiv:2403.08281v1 Announce Type: cross  Abstract: Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a 
    
[^152]: 基于文本引导的变分图像生成用于工业异常检测和分割

    Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation

    [https://arxiv.org/abs/2403.06247](https://arxiv.org/abs/2403.06247)

    该方法利用文本信息生成类似输入图像的无缺陷数据图像，确保生成的图像符合期望分布，稳定且具有普适性。

    

    我们提出了一种文本引导的变分图像生成方法，以解决工业制造中异常检测的干净数据获取挑战。我们的方法利用来自广泛文本库文档的关于目标对象的文本信息，生成类似输入图像的无缺陷数据图像。所提出的框架确保生成的无缺陷图像与来自文本和图像知识的预期分布保持一致，确保稳定性和普适性。实验结果表明了我们的方法的有效性，即使只有有限的无缺陷数据，也能超越先前的方法。我们的方法通过跨四个基准模型和三个不同数据集进行泛化测试进行了验证。我们提出了一种利用生成图像增强异常检测模型效果的额外分析。

    arXiv:2403.06247v1 Announce Type: cross  Abstract: We propose a text-guided variational image generation method to address the challenge of getting clean data for anomaly detection in industrial manufacturing. Our method utilizes text information about the target object, learned from extensive text library documents, to generate non-defective data images resembling the input image. The proposed framework ensures that the generated non-defective images align with anticipated distributions derived from textual and image-based knowledge, ensuring stability and generality. Experimental results demonstrate the effectiveness of our approach, surpassing previous methods even with limited non-defective data. Our approach is validated through generalization tests across four baseline models and three distinct datasets. We present an additional analysis to enhance the effectiveness of anomaly detection models by utilizing the generated images.
    
[^153]: 通过分解表示进行医学言语症状分类

    Medical Speech Symptoms Classification via Disentangled Representation

    [https://arxiv.org/abs/2403.05000](https://arxiv.org/abs/2403.05000)

    该论文提出了一种名为DRSC的医学言语分类模型，实现了自动学习从文本-声学数据中分离意图和内容表示以进行分类，并在检测25种不同医学症状时取得了95%的平均准确率。

    

    arXiv:2403.05000v1 公告类型:new 摘要: 在现有工作中，意图被定义用于理解口头语言。医学言语中涉及的文本特征和声学特征均包含意图，这对于症状诊断非常重要。本文提出了一种名为DRSC的医学言语分类模型，该模型自动学习从文本-声学数据中分离意图和内容表示以进行分类。 通过意图编码器提取文本域和Mel-频谱图域的意图表示，然后通过两个交换获取重构的文本特征和Mel-频谱图特征。在将两个域的意图结合成一个联合表示后，综合意图表示被输入决策层进行分类。实验结果显示，我们的模型在检测25种不同医学症状时获得了平均准确率达到95%。

    arXiv:2403.05000v1 Announce Type: new  Abstract: Intent is defined for understanding spoken language in existing works. Both textual features and acoustic features involved in medical speech contain intent, which is important for symptomatic diagnosis. In this paper, we propose a medical speech classification model named DRSC that automatically learns to disentangle intent and content representations from textual-acoustic data for classification. The intent representations of the text domain and the Mel-spectrogram domain are extracted via intent encoders, and then the reconstructed text feature and the Mel-spectrogram feature are obtained through two exchanges. After combining the intent from two domains into a joint representation, the integrated intent representation is fed into a decision layer for classification. Experimental results show that our model obtains an average accuracy rate of 95% in detecting 25 different medical symptoms.
    
[^154]: ObjectCompose: 评估基于视觉的模型在物体与背景组合变化上的韧性

    ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes

    [https://arxiv.org/abs/2403.04701](https://arxiv.org/abs/2403.04701)

    评估基于视觉的模型对于物体与背景之间多样化变化的鲁棒性，提出一种可以引入不同对象方面变化的方法

    

    由于最近基于视觉的模型进行了大规模多模态训练并具有泛化能力，了解它们的鲁棒性程度对于它们在现实世界中的部署至关重要。在本研究中，我们评估了当前基于视觉的模型针对不同的物体与背景上下文变化的韧性。大多数鲁棒性评估方法引入了合成数据集来诱导物体特征（视点、尺度、颜色）的变化，或者利用图像转换技术（对抗性变化、常见破坏）在真实图像上模拟分布的变化。最近的研究探索了利用大语言模型和扩散模型来生成背景的变化。但是，这些方法要么在提供对要进行的更改的控制方面不足，要么扭曲了物体的语义，使其不适用于任务。与之相反，我们的方法可以引入各种对象

    arXiv:2403.04701v1 Announce Type: cross  Abstract: Given the large-scale multi-modal training of recent vision-based models and their generalization capabilities, understanding the extent of their robustness is critical for their real-world deployment. In this work, we evaluate the resilience of current vision-based models against diverse object-to-background context variations. The majority of robustness evaluation methods have introduced synthetic datasets to induce changes to object characteristics (viewpoints, scale, color) or utilized image transformation techniques (adversarial changes, common corruptions) on real images to simulate shifts in distributions. Recent works have explored leveraging large language models and diffusion models to generate changes in the background. However, these methods either lack in offering control over the changes to be made or distort the object semantics, making them unsuitable for the task. Our method, on the other hand, can induce diverse objec
    
[^155]: 异质学习代理群体中道德行为动态

    Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents

    [https://arxiv.org/abs/2403.04202](https://arxiv.org/abs/2403.04202)

    在多代理环境中，研究人员探讨了不同道德类型的学习代理之间的互动，发现道德异质性可能对代理的共同发展产生影响。

    

    arXiv:2403.04202v1 公告类型：交叉领域 摘要：日益关注AI系统安全和对齐性的问题突显了在人工代理中嵌入道德能力的重要性。一种有前途的解决方案是利用经验学习，即强化学习。在多代理（社会）环境中，个体学习代理之间的交互可能产生复杂的群体层面现象。许多现有研究依赖于模拟的社会困境环境来研究独立学习代理的互动。然而，它们往往忽视了实践中代理社会中可能存在的道德异质性。例如，在不同时间点，单个学习代理可能面对后果主义者（即关心随时间最大化某种结果）或基于规范的对手（即专注于立即遵守特定规范） 。代理的共同发展在多大程度上可能受到这种道德异质性的影响。

    arXiv:2403.04202v1 Announce Type: cross  Abstract: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in 
    
[^156]: 机器学习在恶意软件检测中对Mal-API-2019数据集的全面评估

    Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection

    [https://arxiv.org/abs/2403.02232](https://arxiv.org/abs/2403.02232)

    该研究通过机器学习技术全面评估了恶意软件检测，发现集成方法（如随机森林和XGBoost）相较于其他方法在恶意软件检测中表现出更高的准确性、精确度和召回率。

    

    这项研究使用机器学习技术对恶意软件检测进行了彻底的探讨，重点评估了使用Mal-API-2019数据集的各种分类模型。旨在通过更有效地识别和缓解威胁来推进网络安全能力。研究探讨了集成和非集成的机器学习方法，如随机森林、XGBoost、K最近邻（KNN）和神经网络。特别强调了数据预处理技术的重要性，特别是TF-IDF表示和主成分分析，以提高模型性能。结果显示，随机森林和XGBoost等集成方法相比其他方法具有更高的准确性、精确度和召回率，突显了它们在恶意软件检测中的有效性。论文还讨论了限制和潜在的未来方向，强调了需要不断适应的必要性。

    arXiv:2403.02232v1 Announce Type: cross  Abstract: This study conducts a thorough examination of malware detection using machine learning techniques, focusing on the evaluation of various classification models using the Mal-API-2019 dataset. The aim is to advance cybersecurity capabilities by identifying and mitigating threats more effectively. Both ensemble and non-ensemble machine learning methods, such as Random Forest, XGBoost, K Nearest Neighbor (KNN), and Neural Networks, are explored. Special emphasis is placed on the importance of data pre-processing techniques, particularly TF-IDF representation and Principal Component Analysis, in improving model performance. Results indicate that ensemble methods, particularly Random Forest and XGBoost, exhibit superior accuracy, precision, and recall compared to others, highlighting their effectiveness in malware detection. The paper also discusses limitations and potential future directions, emphasizing the need for continuous adaptation t
    
[^157]: 将神经信号解码为语音

    Decode Neural signal as Speech

    [https://arxiv.org/abs/2403.01748](https://arxiv.org/abs/2403.01748)

    本文在脑机接口领域探索了MEG信号的脑到文本转换，着重解决了以前主要集中在EEG上、使用“teacher-forcing”以及未完全自回归的问题。

    

    从脑动态解码语言是脑机接口（BCI）领域中一个重要的开放方向，尤其考虑到大型语言模型的快速增长。相对于需要电极植入手术的侵入性信号，非侵入性神经信号（如EEG、MEG）由于其安全性和普适性而越来越受到关注。然而，在三个方面的探索还不足：1）以前的方法主要集中在EEG上，但没有一个先前的研究解决了MEG信号质量更好的问题；2）以前的工作主要在生成解码过程中使用“teacher-forcing”，这是不切实际的；3）以前的工作大多是基于“BART”而不是完全自回归的，而在其他序列任务中表现更好。在本文中，我们探讨了MEG信号的脑到文本转换在语音解码形式中。我们是第一个在交叉注意力中研究的。

    arXiv:2403.01748v1 Announce Type: cross  Abstract: Decoding language from brain dynamics is an important open direction in the realm of brain-computer interface (BCI), especially considering the rapid growth of large language models. Compared to invasive-based signals which require electrode implantation surgery, non-invasive neural signals (e.g. EEG, MEG) have attracted increasing attention considering their safety and generality. However, the exploration is not adequate in three aspects: 1) previous methods mainly focus on EEG but none of the previous works address this problem on MEG with better signal quality; 2) prior works have predominantly used ``teacher-forcing" during generative decoding, which is impractical; 3) prior works are mostly ``BART-based" not fully auto-regressive, which performs better in other sequence tasks. In this paper, we explore the brain-to-text translation of MEG signals in a speech-decoding formation. Here we are the first to investigate a cross-attentio
    
[^158]: 具有延迟更新的随机逼近：马尔科夫采样下的有限时间速率

    Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling

    [https://arxiv.org/abs/2402.11800](https://arxiv.org/abs/2402.11800)

    延迟更新的随机逼近方案在时间变化有界延迟下，保证了每次迭代快速收敛到固定点周围的球体，界限依赖于最大延迟和混合时间。

    

    受大规模和多智能体强化学习应用的启发，我们研究了在马尔科夫采样下具有延迟更新的随机逼近（SA）方案的非渐近性能。虽然延迟的影响在优化中得到了广泛研究，但它们与底层马尔科夫过程相互作用以塑造SA的有限时间性能的方式仍然不太清楚。在这个背景下，我们的第一个主要贡献是证明在时间变化有界延迟下，延迟的SA更新规则确保最后迭代收敛到SA运算符固定点周围的球体具有指数快速的速度。值得注意的是，我们的界限在依赖于最大延迟$\tau_{max}$和混合时间$\tau_{mix}$方面是\emph{紧致的}。为了实现这一紧密界限，我们开发了一种新颖的归纳证明技术，与各种现有延迟优化分析不同，它依赖于建立未...

    arXiv:2402.11800v1 Announce Type: cross  Abstract: Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling. While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood. In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \emph{last iterate} to a ball around the SA operator's fixed point. Notably, our bound is \emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing un
    
[^159]: 通过机器去学习研究预训练数据对大型语言模型的影响

    Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning

    [https://arxiv.org/abs/2402.11537](https://arxiv.org/abs/2402.11537)

    通过对五个主要类别的预训练数据的48个数据集进行系统分析，研究了它们对大型语言模型性能的影响，并发现了一些“高影响数据”，如书籍，与模型能力相关联，为LLMs的优化提供了见解。

    

    通过在具有各种来源的语料库上进行预训练，大型语言模型（LLMs）取得了令人印象深刻的性能。然而，预训练语料库的每个组成部分的影响仍然不明确。因此，预训练语料库的组织仍然是经验性的，并且可能偏离最佳状态。为了解决这个问题，我们系统地分析了来自LLMs预训练数据的5个主要类别的48个数据集的影响，并使用关于九个主要模型能力类别的基准来衡量它们对LLMs的影响。我们的分析提供了关于多个语料库对LLMs性能贡献的实证结果，以及它们的联合影响模式，包括互补的、正交的和相关的关系。我们还确定了一组“高影响数据”，如书籍，与一组模型能力相关联。这些发现为我们提供了关于组织数据以支持LLMs优化的见解。

    arXiv:2402.11537v1 Announce Type: cross  Abstract: Through pretraining on a corpus with various sources, Large Language Models (LLMs) have gained impressive performance. However, the impact of each component of the pretraining corpus remains opaque. As a result, the organization of the pretraining corpus is still empirical and may deviate from the optimal. To address this issue, we systematically analyze the impact of 48 datasets from 5 major categories of pretraining data of LLMs and measure their impacts on LLMs using benchmarks about nine major categories of model capabilities. Our analyses provide empirical results about the contribution of multiple corpora on the performances of LLMs, along with their joint impact patterns, including complementary, orthogonal, and correlational relationships. We also identify a set of ``high-impact data'' such as Books that is significantly related to a set of model capabilities. These findings provide insights into the organization of data to sup
    
[^160]: 用于推断高效LLMs的串联Transformer

    Tandem Transformers for Inference Efficient LLMs

    [https://arxiv.org/abs/2402.08644](https://arxiv.org/abs/2402.08644)

    该论文提出了一种新的架构，称为串联Transformer，用于解决传统大型语言模型推断速度限制的问题。该架构通过将小型自回归模型和大模型以块模式结合起来，并让小模型关注大模型的丰富表示，从而显著提高了小模型的预测准确性。实验证明，在预训练数据集上，串联的PaLM2-Bison和PaLM2-Gecko相比独立的PaLM2-Gecko，在下一个词元预测准确性上提高了3.3%，并且相较于具有相似下游任务的PaLM2-Otter模型，加速比达到1.16倍。

    

    传统的大型语言模型( LLMs )具有自回归的特性，这使得推断速度受到限制，因为词元是按顺序生成的。尽管有些预测和并行解码技术试图减轻这个问题，但它们都有限制：要么依赖更精简但准确度较低的模型进行生成，要么没有充分利用基础LLM的表示。我们提出了一种新颖的架构，即串联Transformer，来解决这些问题。这种架构独特地结合了(1)一个小型自回归模型和(2)一个以块模式运行的大模型(同时处理多个词元)。通过让小模型关注大模型更丰富的表示，大幅提升小模型的预测准确性。在PaLM2预训练数据集上，PaLM2-Bison和PaLM2-Gecko的串联相较独立的PaLM2-Gecko，在下一个词元预测准确性上提升了3.3%，与具有相似下游任务的PaLM2-Otter模型相比，提供了1.16倍的加速比。

    The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream p
    
[^161]: SGS-SLAM：基于高斯点云的语义稠密SLAM

    SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM

    [https://arxiv.org/abs/2402.03246](https://arxiv.org/abs/2402.03246)

    SGS-SLAM是一种基于三维高斯点云的语义稠密SLAM系统，通过多通道优化和关键帧优化，实现了高质量的重建和精确的语义分割。

    

    语义理解在稠密同时定位和建图（SLAM）中起着关键作用，有助于全面的场景解析。最近将高斯点云集成到SLAM系统中的进展表明，通过使用显式的三维高斯表示，可以生成高质量的渲染效果。基于这一进展，我们提出了SGS-SLAM，这是第一个基于三维高斯点云的语义稠密视觉SLAM系统，它不仅提供精确的三维语义分割，还实现了高保真度的重建。具体而言，我们提出在建图过程中采用多通道优化，将外观、几何和语义约束与关键帧优化相结合，以提高重建质量。大量实验证明SGS-SLAM在相机位姿估计、地图重建和语义分割方面表现出了最先进的性能，优于现有方法同时保持实时渲染。

    Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaus- sian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rende
    
[^162]: 加强端到端多任务对话系统：基于内在动机强化学习算法的改进训练和适应性研究

    Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic Motivation Reinforcement Learning Algorithms for Improved Training and Adaptability

    [https://arxiv.org/abs/2401.18040](https://arxiv.org/abs/2401.18040)

    本研究旨在通过内在动机强化学习算法改进端到端多任务对话系统的训练和适应性。通过教授智能体一个内在奖励系统，可以加速训练并提高其判断行为质量的能力。

    

    端到端多任务对话系统通常通过对话流水线的独立模块进行设计。其中，策略模块是决定对用户输入如何响应的关键。这个策略是通过强化学习算法进行训练的，通过利用一个智能体在一个反馈信号形式的环境中接收反馈。然而，当前的对话系统只提供了稀缺且简单的奖励。本研究的目标是研究内在动机强化学习算法。通过这种算法，智能体可以快速加速训练，并通过教授一个内在奖励系统来提高判断其行为质量的能力。具体而言，我们将随机网络蒸馏和好奇驱动强化学习技术应用于测量状态访问频率，并通过使用话语之间的语义相似性来鼓励探索。在一个异构数据集MultiWOZ上进行的实验结果显示...

    End-to-end multi-task dialogue systems are usually designed with separate modules for the dialogue pipeline. Among these, the policy module is essential for deciding what to do in response to user input. This policy is trained by reinforcement learning algorithms by taking advantage of an environment in which an agent receives feedback in the form of a reward signal. The current dialogue systems, however, only provide meagre and simplistic rewards. Investigating intrinsic motivation reinforcement learning algorithms is the goal of this study. Through this, the agent can quickly accelerate training and improve its capacity to judge the quality of its actions by teaching it an internal incentive system. In particular, we adapt techniques for random network distillation and curiosity-driven reinforcement learning to measure the frequency of state visits and encourage exploration by using semantic similarity between utterances. Experimental results on MultiWOZ, a heterogeneous dataset, sho
    
[^163]: 如何合并生成和检索上下文以增强开放领域问答的语言模型的研究

    Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?

    [https://arxiv.org/abs/2401.11911](https://arxiv.org/abs/2401.11911)

    该论文研究了大型语言模型如何合并生成和检索的上下文以提升开放领域问答，发现这些模型偏向于生成的上下文，即使它们提供了错误的信息。

    

    虽然辅助信息已经成为增强大型语言模型（LLMs）的关键，但对于LLMs如何合并生成的和检索的上下文仍知之甚少。为了研究这一点，我们制定了一个系统性的框架来确定LLMs的响应是源自于生成的上下文还是检索的上下文。为了实现这个目标，我们构建了包含相互冲突的上下文的数据集，其中每个问题都与生成的和检索的上下文配对，但只有一个上下文包含了正确的答案。我们的实验证明，LLMs（如GPT-4/3.5和Llama2）存在显著的偏差，更倾向于生成的上下文，即使这些上下文提供了错误的信息。我们进一步确定了导致这种偏差的两个关键因素：i）LLMs生成的上下文通常与问题更相似，增加了其被选择的可能性；ii）检索上下文中使用的分割过程打断了其连贯性。

    While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a systematic framework to identify whether LLMs' responses, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To achieve this, we construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs (GPT-4/3.5 and Llama2) towards generated contexts, even when they provide incorrect information. We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) the segmentation process used in retrieved contexts disrupts their compl
    
[^164]: AI和生成式AI用于研究发现与总结

    AI and Generative AI for Research Discovery and Summarization

    [https://arxiv.org/abs/2401.06795](https://arxiv.org/abs/2401.06795)

    AI和生成式AI工具在研究发现和总结方面有重大影响，包括能够更快地找到相关文献和用简洁语言总结研究文章的要点。

    

    AI和生成式AI工具，包括依赖大型语言模型（LLMs）的聊天机器人如ChatGPT，今年迅速崛起，为增加工作效率和改善生活创造了难以置信的机会。统计学家和数据科学家已经开始以多种方式体验到这些工具的好处，比如从文本提示生成编程代码以分析数据或拟合统计模型。这些工具可以在研究发现和总结方面产生重大影响之一。正在开发独立工具和插件给聊天机器人，使研究人员比2023年之前的搜索工具更快地找到相关文献。此外，生成式AI工具已经发展到可以用简洁的语言总结和提取研究文章的要点的程度。最后，基于高度参数化的LLMs的聊天机器人可用于模拟

    arXiv:2401.06795v2 Announce Type: replace-cross  Abstract: AI and generative AI tools, including chatbots like ChatGPT that rely on large language models (LLMs), have burst onto the scene this year, creating incredible opportunities to increase work productivity and improve our lives. Statisticians and data scientists have begun experiencing the benefits from the availability of these tools in numerous ways, such as the generation of programming code from text prompts to analyze data or fit statistical models. One area that these tools can make a substantial impact is in research discovery and summarization. Standalone tools and plugins to chatbots are being developed that allow researchers to more quickly find relevant literature than pre-2023 search tools. Furthermore, generative AI tools have improved to the point where they can summarize and extract the key points from research articles in succinct language. Finally, chatbots based on highly parameterized LLMs can be used to simula
    
[^165]: TagAlign：利用多标签分类改进视觉-语言对齐

    TagAlign: Improving Vision-Language Alignment with Multi-Tag Classification

    [https://arxiv.org/abs/2312.14149](https://arxiv.org/abs/2312.14149)

    提出了一种简单的方法，通过解析图像与文本中的对象和属性，使用多标签分类损失来改进视觉-语言对齐模型

    

    学习视觉-语言模型的关键在于从视觉和语言数据中提取语义对齐的信息。我们提出了一种非常简单的方法，可以更好地对齐图像和文本特征，而无需除图像-文本对之外的其他数据格式。具体而言，给定一幅图像及其配对的文本，我们设法从描述中解析出对象（例如猫）和属性（例如黑色），这些对象和属性极有可能存在于图像中。值得注意的是，解析管道完全自动化，因此具有良好的可扩展性。借助这些解析出的语义作为监督信号，我们可以将常用的图像-文本对比损失与多标签分类损失相结合。在广泛的实验结果中

    arXiv:2312.14149v3 Announce Type: replace-cross  Abstract: The crux of learning vision-language models is to extract semantically aligned information from visual and linguistic data. Existing attempts usually face the problem of coarse alignment, e.g., the vision encoder struggles in localizing an attribute-specified object. In this work, we propose an embarrassingly simple approach to better align image and text features with no need of additional data formats other than image-text pairs. Concretely, given an image and its paired text, we manage to parse objects (\textit{e.g.}, cat) and attributes (\textit{e.g.}, black) from the description, which are highly likely to exist in the image. It is noteworthy that the parsing pipeline is fully automatic and thus enjoys good scalability. With these parsed semantics as supervision signals, we can complement the commonly used image-text contrastive loss with the multi-tag classification loss. Extensive experimental results on a broad suite of
    
[^166]: 使用分层接触网格变换器学习灵活身体碰撞动力学

    Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer

    [https://arxiv.org/abs/2312.12467](https://arxiv.org/abs/2312.12467)

    本文提出了一种使用分层网格结构的Hierarchical Contact Mesh Transformer（HCMT），能够学习长距离依赖关系，以处理灵活体动力学挑战。

    

    最近，许多基于网格的图神经网络（GNN）模型已被提出用来建模复杂的高维物理系统。与传统数值求解器相比，这些方法取得了显着的成就，大大减少了求解时间。然而，目前尚未得到充分探讨的是它们是否有效地应对灵活体动力学的挑战，即瞬时碰撞发生在极短时间内的情况下。本文提出了一种使用分层网格结构的Hierarchical Contact Mesh Transformer（HCMT），能够学习身体空间位置之间（由碰撞引起的）长距离依赖关系--在更高级别网格中的两个接近位置对应于身体中的两个远距位置。

    arXiv:2312.12467v2 Announce Type: replace-cross  Abstract: Recently, many mesh-based graph neural network (GNN) models have been proposed for modeling complex high-dimensional physical systems. Remarkable achievements have been made in significantly reducing the solving time compared to traditional numerical solvers. These methods are typically designed to i) reduce the computational cost in solving physical dynamics and/or ii) propose techniques to enhance the solution accuracy in fluid and rigid body dynamics. However, it remains under-explored whether they are effective in addressing the challenges of flexible body dynamics, where instantaneous collisions occur within a very short timeframe. In this paper, we present Hierarchical Contact Mesh Transformer (HCMT), which uses hierarchical mesh structures and can learn long-range dependencies (occurred by collisions) among spatially distant positions of a body -- two close positions in a higher-level mesh corresponds to two distant posi
    
[^167]: 通过策略引导的轨迹扩散实现世界模型

    World Models via Policy-Guided Trajectory Diffusion

    [https://arxiv.org/abs/2312.08533](https://arxiv.org/abs/2312.08533)

    这项工作提出了一个新颖的世界建模方法，Policy-Guided Trajectory Diffusion (PolyGRAD)，通过扩散模型一次生成整个在线策略轨迹，避免了自回归模型中随着轨迹长度增长而积累的预测误差。

    

    世界模型是开发智能agent的强大工具。通过预测一系列行动的结果，世界模型使得可以通过在“想象中”使用合成数据来优化策略，即通过在线策略增强学习（RL）来实现。现有的世界模型是自回归的，因为它们在预测下一个状态的同时从策略中采样下一个行动。随着轨迹长度的增长，预测误差必然会累积。在这项工作中，我们提出了一种新颖的世界建模方法，不是自回归的，而是通过扩散模型一次生成整个在线策略轨迹。我们的方法，Policy-Guided Trajectory Diffusion (PolyGRAD)，利用了除了策略的动作分布梯度之外的一个去噪模型，将最初随机状态和动作的轨迹扩散成一个在线合成轨迹。我们分析了PolyGRAD与

    arXiv:2312.08533v3 Announce Type: replace-cross  Abstract: World models are a powerful tool for developing intelligent agents. By predicting the outcome of a sequence of actions, world models enable policies to be optimised via on-policy reinforcement learning (RL) using synthetic data, i.e. in "in imagination". Existing world models are autoregressive in that they interleave predicting the next state with sampling the next action from the policy. Prediction error inevitably compounds as the trajectory length grows. In this work, we propose a novel world modelling approach that is not autoregressive and generates entire on-policy trajectories in a single pass through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory. We analyse the connections between PolyGRAD,
    
[^168]: 基于大型语言模型的半结构化网络文章高通量生物医学关系提取

    High-throughput Biomedical Relation Extraction for Semi-Structured Web Articles Empowered by Large Language Models

    [https://arxiv.org/abs/2312.08274](https://arxiv.org/abs/2312.08274)

    利用大型语言模型在半结构化网络文章中实现高通量生物医学关系提取，通过对LLMs的应用，结合外部语料库和世界知识，设计针对性的二元分类决策，取得良好的结果。

    

    旨在开发一种利用大型语言模型的阅读理解能力和生物医学世界知识以可扩展和有信服力的方式进行高通量生物医学关系提取的系统。我们将关系提取任务制定为大型语言模型的二元分类，LLMs基于外部语料库和其世界知识做出决策，给出事实验证的判断理由。此方法专为半结构化网络文章而设计，在其中将主标题指定为尾实体并明确纳入上下文，潜在头实体根据生物医学词表进行匹配。此外，将冗长内容分割为文本块，嵌入并使用额外的嵌入模型进行检索。

    arXiv:2312.08274v4 Announce Type: replace-cross  Abstract: Objective: To develop a high-throughput biomedical relation extraction system that takes advantage of the large language models'(LLMs) reading comprehension ability and biomedical world knowledge in a scalable and evidential manner. Methods: We formulate the relation extraction task as binary classifications for large language models. Specifically, LLMs make the decision based on the external corpus and its world knowledge, giving the reason for the judgment for factual verification. This method is tailored for semi-structured web articles, wherein we designate the main title as the tail entity and explicitly incorporate it into the context, and the potential head entities are matched based on a biomedical thesaurus. Moreover, lengthy contents are sliced into text chunks, embedded, and retrieved with additional embedding models. Results: Using an open-source LLM, we extracted 248659 relation triplets of three distinct relation 
    
[^169]: 人工神经网络与人类概念的表示

    Artificial Neural Nets and the Representation of Human Concepts

    [https://arxiv.org/abs/2312.05337](https://arxiv.org/abs/2312.05337)

    人工神经网络可以学习人类和非人类的概念，但并不是用单个单元来表示这些概念

    

    人工神经网络（ANNs）到底学习到了什么呢？机器学习（ML）社区认为，为了完成复杂任务，人工神经网络必须发展抽象的人类概念。一些人甚至认为这些概念存储在网络的单个单元中。根据当前的研究，我系统地调查了支撑这一说法的假设。我得出结论，人工神经网络的确能够执行复杂的预测任务，为此它们可能学习了人类和非人类的概念。然而，证据表明人工神经网络并没有用单个单元来表示这些概念。

    arXiv:2312.05337v2 Announce Type: replace-cross  Abstract: What do artificial neural networks (ANNs) learn? The machine learning (ML) community shares the narrative that ANNs must develop abstract human concepts to perform complex tasks. Some go even further and believe that these concepts are stored in individual units of the network. Based on current research, I systematically investigate the assumptions underlying this narrative. I conclude that ANNs are indeed capable of performing complex prediction tasks, and that they may learn human and non-human concepts to do so. However, evidence indicates that ANNs do not represent these concepts in individual units.
    
[^170]: DreamComposer: 通过多视角条件实现可控的3D对象生成

    DreamComposer: Controllable 3D Object Generation via Multi-View Conditions

    [https://arxiv.org/abs/2312.03611](https://arxiv.org/abs/2312.03611)

    DreamComposer是一个灵活且可扩展的框架，通过注入多视角条件增强现有视角感知扩散模型，实现可控的3D对象生成。

    

    利用预训练的2D大规模生成模型，最近的研究能够从单个in-the-wild图像生成高质量的新视图。然而，由于缺乏来自多个视角的信息，这些研究在生成可控的新视图时遇到困难。本文提出了DreamComposer，这是一个灵活且可扩展的框架，可以通过注入多视角条件来增强现有的视角感知扩散模型。具体而言，DreamComposer首先使用视角感知的3D提升模块从多个视角获取对象的3D表示。然后，它使用多视角特征融合模块从3D表示中渲染目标视图的潜在特征。最后，从多视角输入中提取的目标视图特征被注入到预训练的扩散模型中。实验证明，DreamComposer与最先进的扩散模型兼容，用于零-shot新视图sy

    arXiv:2312.03611v2 Announce Type: replace-cross  Abstract: Utilizing pre-trained 2D large-scale generative models, recent works are capable of generating high-quality novel views from a single in-the-wild image. However, due to the lack of information from multiple views, these works encounter difficulties in generating controllable novel views. In this paper, we present DreamComposer, a flexible and scalable framework that can enhance existing view-aware diffusion models by injecting multi-view conditions. Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain 3D representations of an object from multiple views. Then, it renders the latent features of the target view from 3D representations with the multi-view feature fusion module. Finally the target view features extracted from multi-view inputs are injected into a pre-trained diffusion model. Experiments show that DreamComposer is compatible with state-of-the-art diffusion models for zero-shot novel view sy
    
[^171]: AV2AV：直接音视频语音到音视频语音转换，具有统一的音视频语音表示

    AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation

    [https://arxiv.org/abs/2312.02512](https://arxiv.org/abs/2312.02512)

    本文提出了AV2AV框架，实现了直接音视频语音到音视频语音的转换，能够在虚拟会议中进行真实对话，并改善口译语言翻译系统的鲁棒性。

    

    本文提出了一种新颖的直接音视频语音到音视频语音转换（AV2AV）框架，其中系统的输入和输出是多模态的（即音频和视觉语音）。通过所提出的AV2AV，可以带来两个关键优势：1）我们可以利用自己的主要语言在虚拟会议中与全球个人进行类似真实的对话。与仅在音频模态之间进行翻译的语音到语音翻译（A2A）相比，所提出的AV2AV直接在音视频语音之间进行翻译。这种能力通过呈现与已翻译语音同步的嘴唇运动增强了对话体验。2）我们可以提高口译语言翻译系统的鲁棒性。通过使用音视频语音的互补信息，系统可以在存在声学噪声的情况下有效地翻译口头语言，展示出强大的性能。

    arXiv:2312.02512v2 Announce Type: replace-cross  Abstract: This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech Translation (AV2AV) framework, where the input and output of the system are multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key advantages can be brought: 1) We can perform real-like conversations with individuals worldwide in a virtual meeting by utilizing our own primary languages. In contrast to Speech-to-Speech Translation (A2A), which solely translates between audio modalities, the proposed AV2AV directly translates between audio-visual speech. This capability enhances the dialogue experience by presenting synchronized lip movements along with the translated speech. 2) We can improve the robustness of the spoken language translation system. By employing the complementary information of audio-visual speech, the system can effectively translate spoken language even in the presence of acoustic noise, showcasing robust perfor
    
[^172]: 一种简单且可扩展的图生成表示

    A Simple and Scalable Representation for Graph Generation

    [https://arxiv.org/abs/2312.02230](https://arxiv.org/abs/2312.02230)

    提出了一种名为GEEL的新型、简单且可扩展的图表示，可以显著降低邻接矩阵大小和词汇量，同时通过节点位置编码实现自回归生成，并针对属性图设计了新的扩展方案。

    

    最近，人们对利用神经网络进行图生成产生了浓厚兴趣，这是一个具有关键应用价值的基本统计学习问题，如分子设计和社区分析。然而，大多数方法在生成大规模图时遇到了重大限制。这是由于它们需要输出随着节点数量呈二次增长的完整邻接矩阵。为了应对这一挑战，我们引入了一种新的、简单且可扩展的图表示，名为间隙编码边列表（GEEL），其表示大小较小且与边数量一致。此外，GEEL通过结合间隙编码和带宽限制方案显著减少了词汇量。通过加入节点位置编码，GEEL可以自回归生成，我们进一步将GEEL扩展到处理属性图，设计了一种新的语法。

    arXiv:2312.02230v2 Announce Type: replace-cross  Abstract: Recently, there has been a surge of interest in employing neural networks for graph generation, a fundamental statistical learning problem with critical applications like molecule design and community analysis. However, most approaches encounter significant limitations when generating large-scale graphs. This is due to their requirement to output the full adjacency matrices whose size grows quadratically with the number of nodes. In response to this challenge, we introduce a new, simple, and scalable graph representation named gap encoded edge list (GEEL) that has a small representation size that aligns with the number of edges. In addition, GEEL significantly reduces the vocabulary size by incorporating the gap encoding and bandwidth restriction schemes. GEEL can be autoregressively generated with the incorporation of node positional encoding, and we further extend GEEL to deal with attributed graphs by designing a new grammar
    
[^173]: PAC隐私保护扩散模型

    PAC Privacy Preserving Diffusion Models

    [https://arxiv.org/abs/2312.01201](https://arxiv.org/abs/2312.01201)

    提出了一种PAC隐私保护扩散模型，通过将私有分类器指导集成到采样过程中增强隐私保护，并发展了一种新的度量标准来衡量隐私水平，在保护性能方面表现出卓越表现。

    

    数据隐私保护正在引起研究人员的越来越多的关注。扩散模型（DMs），尤其是具有严格的差分隐私，有可能生成既具有高隐私性又具有良好视觉质量的图像。然而，挑战在于确保在私有化特定数据属性时的强大保护，当前模型在这些方面经常存在不足。为了解决这些挑战，我们引入了PAC隐私保护扩散模型，这是一种利用扩散原理并确保“可能大致正确（PAC）”隐私性的模型。我们通过将私有分类器指导集成到Langevin采样过程中来增强隐私保护。此外，认识到在衡量模型隐私性方面存在差距，我们开发了一种新的度量标准来衡量隐私水平。我们的模型通过这个新度量标准评估，并通过高斯矩阵计算支持PAC界限，表现出更优异的隐私性能。

    arXiv:2312.01201v2 Announce Type: replace-cross  Abstract: Data privacy protection is garnering increased attention among researchers. Diffusion models (DMs), particularly with strict differential privacy, can potentially produce images with both high privacy and visual quality. However, challenges arise such as in ensuring robust protection in privatizing specific data attributes, areas where current models often fall short. To address these challenges, we introduce the PAC Privacy Preserving Diffusion Model, a model leverages diffusion principles and ensure Probably Approximately Correct (PAC) privacy. We enhance privacy protection by integrating a private classifier guidance into the Langevin Sampling Process. Additionally, recognizing the gap in measuring the privacy of models, we have developed a novel metric to gauge privacy levels. Our model, assessed with this new metric and supported by Gaussian matrix computations for the PAC bound, has shown superior performance in privacy p
    
[^174]: ViT-Lens: 迈向全模态表示

    ViT-Lens: Towards Omni-modal Representations

    [https://arxiv.org/abs/2311.16081](https://arxiv.org/abs/2311.16081)

    ViT-Lens-2利用预先训练的ViT感知新颖模态，并将其对准到预定义空间，为高效的全模态表示学习提供了统一解决方案。

    

    旨在推进AI智能体，大型基础模型显著改善推理和指令执行，然而，目前对视觉和语言的关注忽视了在开放世界环境中感知多样的模态的潜力。然而，基于数据驱动的视觉和语言模型的成功成本高昂，甚至难以为稀有模态重现。本文介绍了ViT-Lens-2，通过感知使用预训练的ViT提取新颖模态并将其对准到预定义空间，以促进高效的全模态表示学习。

    arXiv:2311.16081v2 Announce Type: replace-cross  Abstract: Aiming to advance AI agents, large foundation models significantly improve reasoning and instruction execution, yet the current focus on vision and language neglects the potential of perceiving diverse modalities in open-world environments. However, the success of data-driven vision and language models is costly or even infeasible to be reproduced for rare modalities. In this paper, we present ViT-Lens-2 that facilitates efficient omni-modal representation learning by perceiving novel modalities with a pretrained ViT and aligning them to a pre-defined space. Specifically, the modality-specific lens is tuned to project any-modal signals to an intermediate embedding space, which are then processed by a strong ViT with pre-trained visual knowledge. The encoded representations are optimized toward aligning with the modal-independent space, pre-defined by off-the-shelf foundation models. ViT-Lens-2 provides a unified solution for re
    
[^175]: 视频本地化指令生成的高效预训练方法

    Efficient Pre-training for Localized Instruction Generation of Videos

    [https://arxiv.org/abs/2311.15964](https://arxiv.org/abs/2311.15964)

    提出了一种名为Sieve-&-Swap的技术，通过自动筛选出不相关文本并用人类编写的说明替换文本转录，从而实现视频本地化指令生成的高效预训练。

    

    过程视频展示了诸如食谱准备等任务的逐步演示。理解此类视频具有挑战性，需要对步骤进行精确定位并生成文字说明。手动注释步骤并编写说明成本高昂，这限制了当前数据集的规模并阻碍了有效学习。利用大规模但嘈杂的视频-文本数据集进行预训练可以提升性能，但需要大量计算资源。此外，文本转录包含无关内容，与人类注释员编写的说明相比存在风格变化。为了缓解这两个问题，我们提出了一种技术，Sieve-&-Swap，通过自动筛选出不相关文本和使用文本食谱数据集中人类编写的说明自动替换文本转录以增强文字指令的质量。

    arXiv:2311.15964v2 Announce Type: replace-cross  Abstract: Procedural videos show step-by-step demonstrations of tasks like recipe preparation. Understanding such videos is challenging, involving the precise localization of steps and the generation of textual instructions. Manually annotating steps and writing instructions is costly, which limits the size of current datasets and hinders effective learning. Leveraging large but noisy video-transcript datasets for pre-training can boost performance, but demands significant computational resources. Furthermore, transcripts contain irrelevant content and exhibit style variation compared to instructions written by human annotators. To mitigate both issues, we propose a technique, Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts with human-written instructions from a text-only recipe dataset. 
    
[^176]: 在人工智能安全中更实用的威胁模型

    Towards more Practical Threat Models in Artificial Intelligence Security

    [https://arxiv.org/abs/2311.09994](https://arxiv.org/abs/2311.09994)

    学术界研究的AI安全威胁模型未能充分反映实际使用情况，存在重要不匹配，需要更加实用的威胁模型。

    

    最近的研究发现，人工智能安全领域的研究和实践之间存在差距：学术界研究的威胁并不总是反映出AI的实际使用和安全风险。我们采取了第一步来描述这种差距的全部程度。为此，我们重新审视了AI安全研究中六种最常研究的攻击的威胁模型，并通过对271名工业从业者的调查将其与实践中的AI使用进行匹配。

    arXiv:2311.09994v2 Announce Type: replace-cross  Abstract: Recent works have identified a gap between research and practice in artificial intelligence security: threats studied in academia do not always reflect the practical use and security risks of AI. For example, while models are often studied in isolation, they form part of larger ML pipelines in practice. Recent works also brought forward that adversarial manipulations introduced by academic attacks are impractical. We take a first step towards describing the full extent of this disparity. To this end, we revisit the threat models of the six most studied attacks in AI security research and match them to AI usage in practice via a survey with 271 industrial practitioners. On the one hand, we find that all existing threat models are indeed applicable. On the other hand, there are significant mismatches: research is often too generous with the attacker, assuming access to information not frequently available in real-world settings. 
    
[^177]: 大脑网络与智力：基于图神经网络的静息态fMRI数据方法

    Brain Networks and Intelligence: A Graph Neural Network Based Approach to Resting State fMRI Data

    [https://arxiv.org/abs/2311.03520](https://arxiv.org/abs/2311.03520)

    本文提出了一种新颖的BrainRGIN建模架构，使用图神经网络来预测智力，扩展了现有的图卷积网络并结合了聚类嵌入、图同构网络、TopK池化和基于注意力的读出函数。

    

    静息态功能磁共振成像（rsfMRI）是一种研究大脑功能和认知过程关系的强大工具，因为它可以捕获大脑的功能组织，而无需依赖于特定任务或刺激。本文提出了一种称为BrainRGIN的新颖建模架构，利用rsfMRI推导的静态功能网络连接矩阵，基于图神经网络预测智力（流体、晶体和总体智力）。我们的方法扩展了现有的图卷积网络，将聚类嵌入和图同构网络纳入到图卷积层中，以反映大脑子网络组织的性质和高效网络表达，再辅以TopK池化和基于注意力的读出函数。我们在一个大型数据集上评估了我们提出的架构。

    arXiv:2311.03520v2 Announce Type: replace-cross  Abstract: Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful tool for investigating the relationship between brain function and cognitive processes as it allows for the functional organization of the brain to be captured without relying on a specific task or stimuli. In this paper, we present a novel modeling architecture called BrainRGIN for predicting intelligence (fluid, crystallized, and total intelligence) using graph neural networks on rsfMRI derived static functional network connectivity matrices. Extending from the existing graph convolution networks, our approach incorporates a clustering-based embedding and graph isomorphism network in the graph convolutional layer to reflect the nature of the brain sub-network organization and efficient network expression, in combination with TopK pooling and attention-based readout functions. We evaluated our proposed architecture on a large dataset, specifically the A
    
[^178]: 揭示大语言模型知识编辑的陷阱

    Unveiling the Pitfalls of Knowledge Editing for Large Language Models

    [https://arxiv.org/abs/2310.02129](https://arxiv.org/abs/2310.02129)

    这篇论文探讨了大型语言模型知识编辑的潜在陷阱，提出了新的评估方法，发现知识冲突和知识扭曲是两个重要问题。

    

    随着调整大型语言模型（LLMs）成本不断上升，最近的研究工作已经转向开发编辑LLMs内在知识的方法。然而，仍有一个阴云悬在头顶上 - 知识编辑是否会触发蝴蝶效应？因为目前尚不清楚知识编辑是否会引入可能带来潜在风险的副作用。本文首次探讨了与LLMs知识编辑相关的潜在陷阱。为实现此目的，我们引入了新的基准数据集并提出了创新性的评估指标。我们的结果强调了两个关键问题：（1）知识冲突：编辑逻辑冲突的事实组可能会放大LLMs固有的不一致性 - 这是以前方法忽略的一个方面。（2）知识扭曲：为了编辑事实知识而更改参数可能会不可逆地扭曲

    arXiv:2310.02129v3 Announce Type: replace-cross  Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp 
    
[^179]: 通过逻辑增强大型语言模型中的零射链推理能力

    Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic

    [https://arxiv.org/abs/2309.13339](https://arxiv.org/abs/2309.13339)

    提出了LoT（Logical Thoughts）提示，一个自我改进框架，利用根植于符号逻辑的原则，特别是归谬法，逐步验证和纠正大型语言模型的零射链推理过程。

    

    大型语言模型的最新进展展示了它们在各个领域的 remarkable generalizability。然而，它们的推理能力仍有很大的提升空间，特别是在需要多步推理的情况下。尽管大型语言模型具有广泛的知识，但它们的推理经常未能有效利用这些知识来建立连贯的思维范式。这些模型有时会出现幻觉，因为它们的推理过程未受逻辑原则的限制。为了改进大型语言模型的零射链推理能力，我们提出了 LoT（Logical Thoughts）提示，这是一个自我改进的框架，利用根植于符号逻辑的原则，特别是归谬法，逐步系统地验证和纠正推理过程。在语言任务上进行的实验评估

    arXiv:2309.13339v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts) prompting, a self-improvement framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in
    
[^180]: DiVa-360：沉浸式神经场动态视觉数据集

    DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields

    [https://arxiv.org/abs/2307.16897](https://arxiv.org/abs/2307.16897)

    这个数据集DiVa-360提供了沉浸式360度动态视觉数据，解决了缺乏大规模多视角真实世界数据集的问题，为动态神经场方法的研究和发展提供了重要资源。

    

    神经场技术的进步使得动态3D场景的形状和外观能够以高保真度被捕捉。然而，由于算法挑战和缺乏大规模多视角真实世界数据集，其性能仍落后于传统的2D视频表示。我们通过DiVa-360解决了数据集限制，这是一个包含同步的高分辨率长时间多视角视频序列的实时360度动态视觉数据集，捕捉了使用53台摄像机的定制低成本系统记录的台式场景。该数据集包含21个物体为中心的序列，按不同运动类型分类，25个复杂的手-物体交互序列和8个长时间序列，共计1740万帧图像。此外，我们提供前景-背景分割掩模、同步音频和文本描述。我们在DiVa上对最先进的动态神经场方法进行了基准测试。

    arXiv:2307.16897v2 Announce Type: replace-cross  Abstract: Advances in neural fields are enabling high-fidelity capture of the shape and appearance of dynamic 3D scenes. However, their capabilities lag behind those offered by conventional representations such as 2D videos because of algorithmic challenges and the lack of large-scale multi-view real-world datasets. We address the dataset limitation with DiVa-360, a real-world 360 dynamic visual dataset that contains synchronized high-resolution and long-duration multi-view video sequences of table-scale scenes captured using a customized low-cost system with 53 cameras. It contains 21 object-centric sequences categorized by different motion types, 25 intricate hand-object interaction sequences, and 8 long-duration sequences for a total of 17.4 M image frames. In addition, we provide foreground-background segmentation masks, synchronized audio, and text descriptions. We benchmark the state-of-the-art dynamic neural field methods on DiVa-
    
[^181]: 扩散模型生成类似画家的图像：轮廓优先，细节其次的分析理论

    Diffusion Models Generate Images Like Painters: an Analytical Theory of Outline First, Details Later

    [https://arxiv.org/abs/2303.02490](https://arxiv.org/abs/2303.02490)

    扩散生成模型倾向于首先生成轮廓，然后逐渐加入细节，早期扰动对图像内容影响较大

    

    arXiv:2303.02490v2 公告类型: 替换-跨度 摘要: 扩散生成模型如何将纯噪声转换为有意义的图像？在各种预训练的扩散模型（包括类似稳定扩散的条件潜在空间模型）中，我们观察到潜在的反向扩散过程在图像生成中具有以下特性：(i)个体轨迹倾向于是低维且类似于2D的“旋转”；(ii)高方差的场景特征如布局倾向于较早出现，而低方差的细节倾向于较晚出现；(iii)早期扰动往往会比后期扰动对图像内容产生更大的影响。为了理解这些现象，我们推导并研究了高斯分布的概率流ODE的封闭形式解决方案，这显示出反向扩散状态向着逐渐指定的目标在图像流形上旋转。它还表明生成首先涉及承诺一种轮廓，然后是更精细的和f

    arXiv:2303.02490v2 Announce Type: replace-cross  Abstract: How do diffusion generative models convert pure noise into meaningful images? In a variety of pretrained diffusion models (including conditional latent space models like Stable Diffusion), we observe that the reverse diffusion process that underlies image generation has the following properties: (i) individual trajectories tend to be low-dimensional and resemble 2D `rotations'; (ii) high-variance scene features like layout tend to emerge earlier, while low-variance details tend to emerge later; and (iii) early perturbations tend to have a greater impact on image content than later perturbations. To understand these phenomena, we derive and study a closed-form solution to the probability flow ODE for a Gaussian distribution, which shows that the reverse diffusion state rotates towards a gradually-specified target on the image manifold. It also shows that generation involves first committing to an outline, and then to finer and f
    
[^182]: 一种隐式GNN求解器用于类泊松问题

    An Implicit GNN Solver for Poisson-like problems

    [https://arxiv.org/abs/2302.10891](https://arxiv.org/abs/2302.10891)

    通过利用隐式层理论，$\Psi$-GNN模型了一个“无限”深的网络，从而避免了经验调整所需的消息传递层次以获得解决方案。

    

    本文提出了$\Psi$-GNN，一种新颖的图神经网络（GNN）方法，用于解决具有混合边界条件的普遍泊松PDE问题。通过利用隐式层理论，$\Psi$-GNN建模了一个“无限”深的网络，从而避免了经验调整所需的消息传递层次以获得解决方案。其原始架构明确考虑了边界条件，这是物理应用的关键前提，并且能够适应任何最初提供的解决方案。 $\Psi$-GNN使用“物理信息”损失进行训练，训练过程由设计稳定，并对其初始化不敏感。此外，该方法的一致性在理论上得到了证明，并且其柔韧性和泛化效率在实验中得到了证明：相同的学习模型可以准确处理各种尺寸和不同边界条件的非结构化网格。

    arXiv:2302.10891v3 Announce Type: replace-cross  Abstract: This paper presents $\Psi$-GNN, a novel Graph Neural Network (GNN) approach for solving the ubiquitous Poisson PDE problems with mixed boundary conditions. By leveraging the Implicit Layer Theory, $\Psi$-GNN models an "infinitely" deep network, thus avoiding the empirical tuning of the number of required Message Passing layers to attain the solution. Its original architecture explicitly takes into account the boundary conditions, a critical prerequisite for physical applications, and is able to adapt to any initially provided solution. $\Psi$-GNN is trained using a "physics-informed" loss, and the training process is stable by design, and insensitive to its initialization. Furthermore, the consistency of the approach is theoretically proven, and its flexibility and generalization efficiency are experimentally demonstrated: the same learned model can accurately handle unstructured meshes of various sizes, as well as different bo
    
[^183]: 面向解释神经代码模型的因果论理论

    Toward a Theory of Causation for Interpreting Neural Code Models

    [https://arxiv.org/abs/2302.03788](https://arxiv.org/abs/2302.03788)

    该论文介绍了一种名为$do_{code}$的后验解释方法，用于解释神经代码模型的预测，基于因果推断，旨在实现面向编程语言的解释。

    

    Neural Language Models of Code，或者称为神经代码模型（NCMs），正在迅速从研究原型发展为商业开发者工具。因此，理解这些模型的能力和局限性变得至关重要。然而，这些模型的能力通常是使用自动化指标来衡量的，这些指标通常只能揭示它们真实性能的一部分。一般来说，NCMs的性能似乎很有前途，但目前关于这些模型如何做出决策仍有很多未知。因此，本文介绍了一种名为$do_{code}$的后验解释方法，该方法专门针对NCMs，能够解释模型的预测。$do_{code}$基于因果推断，以实现面向编程语言的解释。虽然$do_{code}$的理论基础可扩展到探索不同的模型属性，但我们提供了一个具体的实例，旨在减少影响...

    arXiv:2302.03788v2 Announce Type: replace-cross  Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact o
    
[^184]: 基于注意力的人类意图估计和预测，用于增强触觉手套辅助控制机器手

    Attention-based Estimation and Prediction of Human Intent to augment Haptic Glove aided Control of Robotic Hand

    [https://arxiv.org/abs/2110.07953](https://arxiv.org/abs/2110.07953)

    该论文提出了一种基于注意力的估计和预测机制，用于增强触觉手套辅助控制机器手中的目标定位能力。

    

    该论文聚焦于基于触觉手套（HG）的控制机械手（RH）执行特定目标的手部操作。触觉手套和机械手中的高维运动信号具有固有的运动变化，导致难以直接建立从触觉手套运动信号到机械手的映射。提出了一种估计机制，用于量化从人类控制器获取的运动信号与机械手抓取的目标姿势之间的关系。提出了一种控制算法，用于转换RH中合成的意图，使目标物体重新定位到预期的目标姿势。在通信延迟存在的情况下合成意图的滞后，需要预测估计的意图。我们利用基于注意力的卷积神经网络编码器来预测一定前瞻的意图轨迹，以弥补延迟。

    arXiv:2110.07953v2 Announce Type: replace-cross  Abstract: The letter focuses on Haptic Glove (HG) based control of a Robotic Hand (RH) executing in-hand manipulation of certain objects of interest. The high dimensional motion signals in HG and RH possess intrinsic variability of kinematics resulting in difficulty to establish a direct mapping of the motion signals from HG onto the RH. An estimation mechanism is proposed to quantify the motion signal acquired from the human controller in relation to the intended goal pose of the object being held by the robotic hand. A control algorithm is presented to transform the synthesized intent at the RH and allow relocation of the object to the expected goal pose. The lag in synthesis of the intent in the presence of communication delay leads to a requirement of predicting the estimated intent. We leverage an attention-based convolutional neural network encoder to predict the trajectory of intent for a certain lookahead to compensate for the de
    
[^185]: 有效利用背景知识的约束k中心聚类

    Efficient Constrained $k$-Center Clustering with Background Knowledge. (arXiv:2401.12533v1 [cs.LG])

    [http://arxiv.org/abs/2401.12533](http://arxiv.org/abs/2401.12533)

    本论文提出了一种在k中心聚类上利用背景知识的约束聚类算法，通过采用一系列技术，得到了效率高且具有最佳近似比例2的算法。

    

    中心为基础的聚类在理论和实践中都引起了重要的研究兴趣。在许多实际应用中，输入数据通常包含可以用于改进聚类结果的背景知识。在这项工作中，我们基于广泛采用的k中心聚类，并将其输入的背景知识建模为必连（ML）和不连（CL）约束集。然而，大多数包括k中心在内的聚类问题本质上都是NP困难的，而更复杂的受约束变体被认为受到更严重的近似和计算障碍的限制，极大地限制了它们的适用性。通过采用一系列技术，包括反支配集，线性规划（LP）整数平面和LP对偶性，我们得到了第一个具有最佳近似比例2的约束k中心的高效近似算法。我们还构建了竞争基准算法，并对我们的近似算法进行了实证评估。

    Center-based clustering has attracted significant research interest from both theory and practice. In many practical applications, input data often contain background knowledge that can be used to improve clustering results. In this work, we build on widely adopted $k$-center clustering and model its input background knowledge as must-link (ML) and cannot-link (CL) constraint sets. However, most clustering problems including $k$-center are inherently $\mathcal{NP}$-hard, while the more complex constrained variants are known to suffer severer approximation and computation barriers that significantly limit their applicability. By employing a suite of techniques including reverse dominating sets, linear programming (LP) integral polyhedron, and LP duality, we arrive at the first efficient approximation algorithm for constrained $k$-center with the best possible ratio of 2. We also construct competitive baseline algorithms and empirically evaluate our approximation algorithm against them o
    
[^186]: 在开放存储库中分析AI视觉模型的质量属性及其在对抗攻击下的表现

    Analyzing the Quality Attributes of AI Vision Models in Open Repositories Under Adversarial Attacks. (arXiv:2401.12261v1 [cs.CR])

    [http://arxiv.org/abs/2401.12261](http://arxiv.org/abs/2401.12261)

    本文提出了一种综合流程，用于对AI视觉模型在开放存储库中的质量属性进行分析，尤其是在面对对抗攻击时的表现。我们展示了一个涉及六个计算机视觉模型的评估场景，以评估准确性、鲁棒性、解释效用和开销。

    

    随着AI模型的快速发展，它们经常发布到开放存储库中，如HuggingFace。在将它们集成到生产开发生命周期之前，对这些模型进行质量保证验证是至关重要的。除了评估平衡准确性和计算成本方面的效率外，对抗攻击可能对AI模型的鲁棒性和可解释性构成威胁。同时，可解释性AI（XAI）应用近似输入到输出的算法来识别贡献特征。对抗扰动可能会降低需要进一步研究的XAI解释的效用。在本文中，我们提出了一种综合流程，用于下游评估任务，包括验证AI模型的准确性，使用基准扰动评估鲁棒性，比较解释效用以及评估开销。我们展示了一个涉及六个计算机视觉模型的评估场景，其中包括基于CNN和Transformer的模型。

    As AI models rapidly evolve, they are frequently released to open repositories, such as HuggingFace. It is essential to perform quality assurance validation on these models before integrating them into the production development lifecycle. In addition to evaluating efficiency in terms of balanced accuracy and computing costs, adversarial attacks are potential threats to the robustness and explainability of AI models. Meanwhile, XAI applies algorithms that approximate inputs to outputs post-hoc to identify the contributing features. Adversarial perturbations may also degrade the utility of XAI explanations that require further investigation. In this paper, we present an integrated process designed for downstream evaluation tasks, including validating AI model accuracy, evaluating robustness with benchmark perturbations, comparing explanation utility, and assessing overhead. We demonstrate an evaluation scenario involving six computer vision models, which include CNN-based, Transformer-b
    
[^187]: 一种安全的个性化偏好学习方法及其在自动驾驶汽车中的应用

    A Safe Preference Learning Approach for Personalization with Applications to Autonomous Vehicles. (arXiv:2311.02099v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.02099](http://arxiv.org/abs/2311.02099)

    本研究介绍了一种安全的个性化偏好学习方法，应用于自动驾驶汽车。该方法利用信号时态逻辑公式的优先顺序进行学习，并提出了一种解决这个学习问题的方法。通过对比较，我们找到了适合的权重估计，使得首选信号的加权满足度高于非首选信号。在人体试验中证明了该方法的有效性。

    

    本研究介绍了一种偏好学习方法，确保符合给定规范，并应用于自动驾驶汽车。我们的方法将描述交通规则的信号时态逻辑(STL)公式的优先顺序纳入学习框架中。通过利用参数加权信号时态逻辑(PWSTL)，我们基于成对比较提出了一种解决这个学习问题的方法，该方法确保了安全保证的偏好学习。我们的方法找到了给定PWSTL公式权重的可行估计，使得使用这些权重时，首选信号的加权定量满足度大于非首选信号。我们的方法得到的权重的可行估计导致了一个加权STL公式，可以用于正确性和定制合成控制器。我们通过两个不同的模拟驾驶场景进行了一项人体试验，验证了我们方法的性能。

    This work introduces a preference learning method that ensures adherence to given specifications, with an application to autonomous vehicles. Our approach incorporates the priority ordering of Signal Temporal Logic (STL) formulas describing traffic rules into a learning framework. By leveraging Parametric Weighted Signal Temporal Logic (PWSTL), we formulate the problem of safety-guaranteed preference learning based on pairwise comparisons and propose an approach to solve this learning problem. Our approach finds a feasible valuation for the weights of the given PWSTL formula such that, with these weights, preferred signals have weighted quantitative satisfaction measures greater than their non-preferred counterparts. The feasible valuation of weights given by our approach leads to a weighted STL formula that can be used in correct-and-custom-by-construction controller synthesis. We demonstrate the performance of our method with a pilot human subject study in two different simulated dri
    
[^188]: 在低数据环境中，视觉定位有助于学习单词的含义

    Visual Grounding Helps Learn Word Meanings in Low-Data Regimes. (arXiv:2310.13257v1 [cs.CL])

    [http://arxiv.org/abs/2310.13257](http://arxiv.org/abs/2310.13257)

    在低数据环境中，使用视觉定位进行监督训练的神经语言模型可以更接近于人类的语言学习能力。

    

    现代神经语言模型（LM）是用于模拟人类句子产生和理解的强大工具，其内部表达与人类大脑中的语言表达非常吻合。然而，为了取得这些结果，LM必须以与人类完全不同的方式进行训练，需要比儿童在发育过程中接收到的语言数据多几个数量级，并且没有任何感知、行动或社交行为的基础。如果用更接近人类的方式进行训练，即依靠感知的监督，模型的语言学习是否更接近人类？我们在单词学习这一语言习得的关键子任务中研究了这个问题。我们训练了一系列不同的LM架构，并在不同规模的数据集上使用图像字幕任务的辅助监督进行训练。然后我们使用一系列广泛的测试来评估这些模型在句法类别、词汇关系、语义学等方面的学习能力。

    Modern neural language models (LMs) are powerful tools for modeling human sentence production and comprehension, and their internal representations are remarkably well-aligned with representations of language in the human brain. But to achieve these results, LMs must be trained in distinctly un-human-like ways -- requiring orders of magnitude more language data than children receive during development, and without any of the accompanying grounding in perception, action, or social behavior. Do models trained more naturalistically -- with grounded supervision -- exhibit more human-like language learning? We investigate this question in the context of word learning, a key sub-task in language acquisition. We train a diverse set of LM architectures, with and without auxiliary supervision from image captioning tasks, on datasets of varying scales. We then evaluate these models on a broad set of benchmarks characterizing models' learning of syntactic categories, lexical relations, semantic f
    
[^189]: 大型语言模型用于多目标进化优化

    Large Language Model for Multi-objective Evolutionary Optimization. (arXiv:2310.12541v1 [cs.NE])

    [http://arxiv.org/abs/2310.12541](http://arxiv.org/abs/2310.12541)

    本论文调查了一种利用大型语言模型（LLM）设计MOEA操作符的新方法，通过适当的提示工程，成功将通用的LLM以零-shot方式作为MOEA/D的黑盒搜索操作符，并通过从LLM行为中学习设计了一个显性的白盒操作符。

    

    多目标进化算法（MOEAs）是解决多目标优化问题（MOPs）的主要方法。在过去几十年中，提出了许多MOEAs，其操作符需要通过领域知识进行精心设计。最近，一些尝试将MOEAs中手动设计的操作符替换为基于学习的操作符（如神经网络模型）已经取得了一些进展。然而，设计和训练这样的模型仍然需要大量的工作，并且学习到的操作符可能不能很好地推广到解决新问题。为了解决上述挑战，本文研究了一种利用强大的大型语言模型（LLM）来设计MOEA操作符的新方法。通过适当的提示工程，我们成功地让一个通用的LLM以零-shot的方式作为分解型MOEA（MOEA/D）的黑盒搜索操作符。此外，通过从LLM行为中学习，我们进一步设计了一个显性的白盒操作符，并提出了...

    Multiobjective evolutionary algorithms (MOEAs) are major methods for solving multiobjective optimization problems (MOPs). Many MOEAs have been proposed in the past decades, of which the operators need carefully handcrafted design with domain knowledge. Recently, some attempts have been made to replace the manually designed operators in MOEAs with learning-based operators (e.g., neural network models). However, much effort is still required for designing and training such models, and the learned operators might not generalize well to solve new problems. To tackle the above challenges, this work investigates a novel approach that leverages the powerful large language model (LLM) to design MOEA operators. With proper prompt engineering, we successfully let a general LLM serve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a zero-shot manner. In addition, by learning from the LLM behavior, we further design an explicit white-box operator with randomness and propose
    
[^190]: 贪心视角：多无人机视野规划在杂乱环境中的协同覆盖

    Greedy Perspectives: Multi-Drone View Planning for Collaborative Coverage in Cluttered Environments. (arXiv:2310.10863v1 [cs.RO])

    [http://arxiv.org/abs/2310.10863](http://arxiv.org/abs/2310.10863)

    本研究研究了在杂乱环境中协调无人机团队拍摄复杂人群的多无人机多演员视角规划问题，并开发了一个具有遮挡感知目标的视角规划器进行性能比较。

    

    无人机团队的部署可以在复杂环境中拍摄动态人群（演员）的大规模影像，用于团队运动和电影制作等新应用领域。为了实现该目标，可以使用通过顺序贪心规划进行子模最大化的方法，以便在无人机团队之间进行摄像机视野的可扩展优化，但在杂乱环境中协同效果面临挑战。障碍物可能产生遮挡并增加无人机碰撞的几率，这可能违反近似最优性的要求。为了在稠密环境中协调无人机团队拍摄人群，需要一种更通用的视角规划方法。我们通过开发一个具有遮挡感知目标的多无人机多演员视角规划器，并与贪心形成规划器进行比较，探讨遮挡和碰撞对拍摄应用性能的影响。为了评估性能，

    Deployment of teams of aerial robots could enable large-scale filming of dynamic groups of people (actors) in complex environments for novel applications in areas such as team sports and cinematography. Toward this end, methods for submodular maximization via sequential greedy planning can be used for scalable optimization of camera views across teams of robots but face challenges with efficient coordination in cluttered environments. Obstacles can produce occlusions and increase chances of inter-robot collision which can violate requirements for near-optimality guarantees. To coordinate teams of aerial robots in filming groups of people in dense environments, a more general view-planning approach is required. We explore how collision and occlusion impact performance in filming applications through the development of a multi-robot multi-actor view planner with an occlusion-aware objective for filming groups of people and compare with a greedy formation planner. To evaluate performance,
    
[^191]: 在一千年前的拉丁文本中检测句子级别的性内容

    Detecting Sexual Content at the Sentence Level in First Millennium Latin Texts. (arXiv:2309.14974v1 [cs.CL])

    [http://arxiv.org/abs/2309.14974](http://arxiv.org/abs/2309.14974)

    该研究提出使用深度学习方法在句子级别进行语义分类，以加速人文学科和语言学领域中语料库建设的过程。经过评估，该方法在检测性内容方面表现出高精度和真阳性率，并探索了不同的输入嵌入层对模型性能的影响。

    

    在这项研究中，我们提出使用深度学习方法在句子级别进行语义分类，以加快人文学科和语言学领域中语料库建设的过程，这是一项传统且耗时的任务。我们引入了一个新颖的语料库，包括约2500个句子，涵盖了从公元前300年到公元900年的性语义学（医学，情色等）。我们评估了各种句子分类方法和不同的输入嵌入层，并表明它们都比简单的基于标记的搜索方法更好。我们探索了个人言语和社会言语元数据嵌入（世纪，作者，写作类型）的整合，但发现这导致了过拟合。我们的结果表明了这种方法的有效性，使用HAN分别达到了70.60%的高精度和86.33%的真阳性率（TPR）。我们评估了数据集大小对模型性能的影响（420而不是2013），并显示出，尽管我们的模型性能可能稍有下降，但性能仍然稳定。

    In this study, we propose to evaluate the use of deep learning methods for semantic classification at the sentence level to accelerate the process of corpus building in the field of humanities and linguistics, a traditional and time-consuming task. We introduce a novel corpus comprising around 2500 sentences spanning from 300 BCE to 900 CE including sexual semantics (medical, erotica, etc.). We evaluate various sentence classification approaches and different input embedding layers, and show that all consistently outperform simple token-based searches. We explore the integration of idiolectal and sociolectal metadata embeddings (centuries, author, type of writing), but find that it leads to overfitting. Our results demonstrate the effectiveness of this approach, achieving high precision and true positive rates (TPR) of respectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset size on the model performances (420 instead of 2013), and show that, while our models per
    
[^192]: 具有增强课程强化学习的众包感知多智能体路径规划研究

    Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning. (arXiv:2309.10275v1 [cs.RO])

    [http://arxiv.org/abs/2309.10275](http://arxiv.org/abs/2309.10275)

    该论文介绍了CRAMP，一种基于增强式课程强化学习的众包感知分散式路径规划方法，旨在解决拥挤环境下多智能体路径规划的困难。

    

    在拥挤环境中进行的多智能体路径规划是一个具有挑战性的运动规划问题，旨在为系统中的所有智能体找到无碰撞路径。多智能体路径规划在各个领域中都有广泛的应用，包括空中群体、自动化仓储机器人和自动驾驶车辆。当前的多智能体路径规划方法可以大致分为两种主要类别：集中式规划和分散式规划。集中式规划受到维度灾难的困扰，因此在大型和复杂环境中不具备良好的可扩展性。另一方面，分散式规划使智能体能够在部分可观察环境中进行实时路径规划，展示了隐式的协调能力。然而，在密集环境中它们的收敛速度较慢且性能下降。本文介绍了一种名为CRAMP的众包感知分散式方法，通过增强式课程引导的强化学习来解决这个问题。

    Multi-Agent Path Finding (MAPF) in crowded environments presents a challenging problem in motion planning, aiming to find collision-free paths for all agents in the system. MAPF finds a wide range of applications in various domains, including aerial swarms, autonomous warehouse robotics, and self-driving vehicles. The current approaches for MAPF can be broadly categorized into two main categories: centralized and decentralized planning. Centralized planning suffers from the curse of dimensionality and thus does not scale well in large and complex environments. On the other hand, decentralized planning enables agents to engage in real-time path planning within a partially observable environment, demonstrating implicit coordination. However, they suffer from slow convergence and performance degradation in dense environments. In this paper, we introduce CRAMP, a crowd-aware decentralized approach to address this problem by leveraging reinforcement learning guided by a boosted curriculum-b
    
[^193]: 大型语言模型能否辨别科学假设的证据？社会科学案例研究。

    Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences. (arXiv:2309.06578v1 [cs.CL])

    [http://arxiv.org/abs/2309.06578](http://arxiv.org/abs/2309.06578)

    本文研究了大型语言模型（LLMs）根据科学摘要文本的能力，来辨别支持或反驳特定假设的证据。通过社区驱动的注释建立了一个新的数据集，针对社会科学中的科学假设证据任务。与其他基准进行了性能比较，并为未来研究提供了机会。

    

    假设的制定和测试是经验性研究的核心。一个强有力的假设是基于现有证据的最佳猜测，并且是基于相关文献的全面视图进行启发的。然而，随着每年科学文章数量的指数增长，对于给定假设相关证据的手动汇总和综合是一项挑战。我们的工作探索了当前大型语言模型（LLMs）根据科学摘要文本中的证据，能否辨别支持或反驳特定假设的能力。我们共享了一个新颖的数据集，用于社会科学中使用社区驱动的研究注释的科学假设证据任务。我们将LLMs的性能与几个最先进的基准进行比较，并指出未来研究的机会。该数据集可在https://github.com/Sai90000/ScientificHypothesisEvidencing.git上获得。

    Hypothesis formulation and testing are central to empirical research. A strong hypothesis is a best guess based on existing evidence and informed by a comprehensive view of relevant literature. However, with exponential increase in the number of scientific articles published annually, manual aggregation and synthesis of evidence related to a given hypothesis is a challenge. Our work explores the ability of current large language models (LLMs) to discern evidence in support or refute of specific hypotheses based on the text of scientific abstracts. We share a novel dataset for the task of scientific hypothesis evidencing using community-driven annotations of studies in the social sciences. We compare the performance of LLMs to several state-of-the-art benchmarks and highlight opportunities for future research in this area. The dataset is available at https://github.com/Sai90000/ScientificHypothesisEvidencing.git
    
[^194]: 用于稀疏深度神经网络训练的多目标优化

    Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v1 [cs.LG])

    [http://arxiv.org/abs/2308.12243](http://arxiv.org/abs/2308.12243)

    这项工作提出了一种用于稀疏深度神经网络训练的多目标优化算法，通过加权Chebyshev标量化和增广Lagrangian方法，解决了同时优化多个任务时的挑战。

    

    在深度学习的各种场景中，会自然地出现不同的冲突优化准则。这些准则可以解决不同的主任务（如多任务学习设置），也可以解决主要任务和次要任务，例如损失最小化与稀疏性。通常的方法是简单地加权准则，但在凸设置中才有效。本文提出了一种多目标优化算法，对多任务深度神经网络（DNNs）进行训练，使用改进的加权Chebyshev标量化方法。通过使用这种标量化技术，算法可以识别原始问题的所有最优解，同时将其复杂性降低为一系列单目标问题。然后，使用增广Lagrangian方法来解决简化后的问题，从而可以使用常见的优化技术，如Adam和随机梯度下降，同时有效地处理约束条件。我们的工作旨在解决经济化问题。

    Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economi
    
[^195]: 运动规划扩散：基于扩散模型的机器人运动学习与规划

    Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models. (arXiv:2308.01557v1 [cs.RO])

    [http://arxiv.org/abs/2308.01557](http://arxiv.org/abs/2308.01557)

    本文提出了一种新的机器人运动学习与规划方法，通过学习扩散模型作为先验知识，可以加速运动规划优化过程。扩散模型能够在高维环境中有效地编码数据的多模态性，并可以直接从任务目标条件下的后验轨迹分布中进行采样。

    

    学习轨迹分布的先验知识可以加快机器人运动规划的优化。在给定先前成功的规划方案的情况下，学习轨迹生成模型作为新规划问题的先验知识是非常理想的。之前的研究提出了几种利用这种先验知识进行运动规划问题引导的方法。可以通过从先验知识中采样初始化，或者在最大后验优化的过程中使用先验分布。在本文中，我们提出了学习扩散模型作为先验知识的方法。然后，我们可以通过利用扩散模型的逆去噪过程，直接从任务目标条件下的后验轨迹分布中采样。此外，最近的研究表明，扩散在高维环境中可以有效地编码数据的多模态性，这对于大量的轨迹数据集非常适用。为了展示我们的方法的有效性，我们将我们提出的方法-运动规划扩散与几种基准方发进行了比较。

    Learning priors on trajectory distributions can help accelerate robot motion planning optimization. Given previously successful plans, learning trajectory generative models as priors for a new planning problem is highly desirable. Prior works propose several ways on utilizing this prior to bootstrapping the motion planning problem. Either sampling the prior for initializations or using the prior distribution in a maximum-a-posterior formulation for trajectory optimization. In this work, we propose learning diffusion models as priors. We then can sample directly from the posterior trajectory distribution conditioned on task goals, by leveraging the inverse denoising process of diffusion models. Furthermore, diffusion has been recently shown to effectively encode data multimodality in high-dimensional settings, which is particularly well-suited for large trajectory dataset. To demonstrate our method efficacy, we compare our proposed method - Motion Planning Diffusion - against several ba
    
[^196]: 美国餐厅评论和大型语言模型中的移民美食他者化和低声望构架

    Othering and low prestige framing of immigrant cuisines in US restaurant reviews and large language models. (arXiv:2307.07645v1 [cs.CL])

    [http://arxiv.org/abs/2307.07645](http://arxiv.org/abs/2307.07645)

    通过对2.1M英语Yelp评论的餐厅进行语言分析，研究发现移民美食更容易被构架为客观和他者化，而非西方移民美食受欢迎程度更高。

    

    识别和理解对食物的隐含态度有助于减轻因食物作为文化和种族身份的标志而导致的社会偏见。对食物的刻板印象是一种微侵略，它对有害的公共话语做出了贡献，这可能反过来加深对民族群体的偏见，并对餐馆的经济结果产生负面影响。通过仔细的语言分析，我们在一项大规模研究中评估了对移民美食态度的社会理论。该研究使用了2.1M英语Yelp评论的餐厅在14个美国州的框架差异。在控制了餐厅价格和邻里种族多样性等因素后，我们发现移民美食更有可能以客观和他者化的形式进行构架，如真实性（例如，真实，传统），异国情调（例如，异国，不同）和典型性（例如，典型，通常）。但非西方移民美食（例如，印度，墨西哥）更受欢迎。

    Identifying and understanding implicit attitudes toward food can help efforts to mitigate social prejudice due to food's pervasive role as a marker of cultural and ethnic identity. Stereotypes about food are a form of microaggression that contribute to harmful public discourse that may in turn perpetuate prejudice toward ethnic groups and negatively impact economic outcomes for restaurants. Through careful linguistic analyses, we evaluate social theories about attitudes toward immigrant cuisine in a large-scale study of framing differences in 2.1M English language Yelp reviews of restaurants in 14 US states. Controlling for factors such as restaurant price and neighborhood racial diversity, we find that immigrant cuisines are more likely to be framed in objectifying and othering terms of authenticity (e.g., authentic, traditional), exoticism (e.g., exotic, different), and prototypicality (e.g., typical, usual), but that non-Western immigrant cuisines (e.g., Indian, Mexican) receive mor
    
[^197]: 在大型语言模型中释放认知协同：通过多人格自我协作实现任务解决代理

    Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (arXiv:2307.05300v1 [cs.AI])

    [http://arxiv.org/abs/2307.05300](http://arxiv.org/abs/2307.05300)

    本论文提出了单人表现提示（SPP）的概念，通过与多个角色进行多轮自我协作，将单个语言模型转化为认知协同者，从而增强其在复杂任务中的问题解决能力和整体性能。

    

    人类智慧依赖于认知协同的概念，即在不同认知过程之间进行协作和信息整合，以获得比个体认知过程更出色的结果。尽管大型语言模型（LLM）作为通用任务解决代理表现出了令人期待的性能，但它们在需要丰富领域知识和复杂推理的任务上仍然面临困难。在这项工作中，我们提出了单人表现提示（SPP）的概念，通过与多个角色进行多轮自我协作，将单个LLM转化为认知协同者。认知协同者指的是一个智能代理，与多个智慧合作，结合他们的个体优势和知识，从而增强复杂任务的问题解决能力和整体性能。通过根据任务输入动态识别和模拟不同的角色，SPP释放了LLM中认知协同的潜力。

    Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation. Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have discovered that assi
    
[^198]: RL$^3$:通过RL内部的RL$^2$提升元强化学习方法

    RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])

    [http://arxiv.org/abs/2306.15909](http://arxiv.org/abs/2306.15909)

    RL$^3$是一种原则性混合方法，通过将传统强化学习学到的任务特定动作值作为元强化学习神经网络的输入，提高了元强化学习的性能。

    

    元强化学习（meta-RL）方法，如RL$^2$，已经成为学习针对给定任务分布的数据高效的强化学习算法的有希望的方法。然而，这些强化学习算法在长期任务和超出分布任务方面存在困难，因为它们依赖于递归神经网络来处理经验序列，而不是将它们总结为一般的强化学习组件，例如价值函数。此外，即使是transformers在训练和推理成本变得禁止之前也对它们可以有效推理的历史长度有实际限制。相比之下，传统的强化学习算法在数据效率方面不足，因为它们没有利用领域知识，但随着更多数据的可用性，它们会收敛到最优策略。在本文中，我们提出了RL$^3$，一种组合了传统强化学习和元强化学习的原则性混合方法，通过将通过传统强化学习学习到的特定任务动作值作为元强化学习神经网络的一个输入。

    Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
    
[^199]: 基于联邦学习的代码异味检测方法(FedCSD)

    FedCSD: A Federated Learning Based Approach for Code-Smell Detection. (arXiv:2306.00038v1 [cs.SE])

    [http://arxiv.org/abs/2306.00038](http://arxiv.org/abs/2306.00038)

    本文提出了一种名为FedCSD的基于联邦学习的代码异味检测方法，可以在保护数据隐私的同时，让组织协作训练联邦学习模型。该方法在三个实验中分别使用不同的数据集，实现了高精度的检测效果，并且比集中式和交叉验证方法具有更好的性能表现。

    

    本文提出了一种名为FedCSD的基于联邦学习的代码异味检测方法，可以在保护数据隐私的同时，让组织协作训练联邦学习模型。通过三个实验来支持这些断言，这些实验利用了三个手动验证的数据集，来检测和研究不同的代码异味场景。

    This paper proposes a Federated Learning Code Smell Detection (FedCSD) approach that allows organizations to collaboratively train federated ML models while preserving their data privacy. These assertions have been supported by three experiments that have significantly leveraged three manually validated datasets aimed at detecting and examining different code smell scenarios. In experiment 1, which was concerned with a centralized training experiment, dataset two achieved the lowest accuracy (92.30%) with fewer smells, while datasets one and three achieved the highest accuracy with a slight difference (98.90% and 99.5%, respectively). This was followed by experiment 2, which was concerned with cross-evaluation, where each ML model was trained using one dataset, which was then evaluated over the other two datasets. Results from this experiment show a significant drop in the model's accuracy (lowest accuracy: 63.80\%) where fewer smells exist in the training dataset, which has a noticeab
    
[^200]: 基于$K^2$-树的分级图生成

    Hierarchical Graph Generation with $K^2$-trees. (arXiv:2305.19125v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19125](http://arxiv.org/abs/2305.19125)

    本文介绍了一种基于$K^2$-树的图生成方法，该方法可以实现紧凑生成，并同时捕获图的内在分层结构。通过提出顺序$K^2$-树表示和引入基于Transformer的架构，本文进一步改进了这种方法。实验表明，该方法在图生成方面具有卓越的表现。

    

    从目标分布生成图是许多领域的一个重大挑战，包括药物发现和社交网络分析。在本文中，我们介绍了一种利用原本设计用于无损图压缩的$K^2$-树表示的新颖图生成方法。我们的动机源于$K^2$-树能够在进行紧凑生成的同时，捕获图的内在分层结构的能力。此外，我们还通过(1)提出了一种包含剪枝、扁平化和记号化过程的顺序K2树表示和(2)引入了一种基于Transformer的架构，旨在通过结合专业树形位置编码方案来生成序列。最后，我们对四个常规和两个分子图数据集进行了广泛的评估，以证实我们的算法在图生成方面的优越性。

    Generating graphs from a target distribution is a significant challenge across many domains, including drug discovery and social network analysis. In this work, we introduce a novel graph generation method leveraging $K^2$-tree representation which was originally designed for lossless graph compression. Our motivation stems from the ability of the $K^2$-trees to enable compact generation while concurrently capturing the inherent hierarchical structure of a graph. In addition, we make further contributions by (1) presenting a sequential $K^2$-tree representation that incorporates pruning, flattening, and tokenization processes and (2) introducing a Transformer-based architecture designed to generate the sequence by incorporating a specialized tree positional encoding scheme. Finally, we extensively evaluate our algorithm on four general and two molecular graph datasets to confirm its superiority for graph generation.
    
[^201]: 提升中文文本主题划分和纲要生成：段落级主题表示，语料库和基准

    Advancing Topic Segmentation and Outline Generation in Chinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark. (arXiv:2305.14790v1 [cs.CL])

    [http://arxiv.org/abs/2305.14790](http://arxiv.org/abs/2305.14790)

    本文提出了一种分层的段落级中文主题结构表示，使用句子而不是关键词来表示子主题，构建了大规模、高质量的中文段落级主题结构语料库。

    

    主题划分和纲要生成旨在将一个文档分成连贯的主题段落并生成相应的子标题。这个过程揭示了一个文档的话题结构，有助于从更高的层次快速把握和理解文档的整体情境。然而，与英语领域取得的成功相比，由于缺乏适当的段落级主题表示和大规模、高质量的中文语料库，这一领域的研究和应用受到了限制。为了解决这些问题，我们引入了一种分层的段落级主题结构表示，包括标题、子标题和段落，综合地模拟了文档的话题结构。此外，我们通过使用句子而不是关键词来表示子主题，确保更全面地表示文档内的主题分布。根据这种表示，我们构建了最大的中文段落级主题结构语料库之一。

    Topic segmentation and outline generation strive to divide a document into coherent topic sections and generate corresponding subheadings. Such a process unveils the discourse topic structure of a document that benefits quickly grasping and understanding the overall context of the document from a higher level. However, research and applications in this field have been restrained due to the lack of proper paragraph-level topic representations and large-scale, high-quality corpora in Chinese compared to the success achieved in English. Addressing these issues, we introduce a hierarchical paragraph-level topic structure representation with title, subheading, and paragraph that comprehensively models the document discourse topic structure. In addition, we ensure a more holistic representation of topic distribution within the document by using sentences instead of keywords to represent sub-topics. Following this representation, we construct the largest Chinese Paragraph-level Topic Structur
    
[^202]: 一个统一的英文文本到语音合成前端框架

    a unified front-end framework for english text-to-speech synthesis. (arXiv:2305.10666v1 [cs.CL])

    [http://arxiv.org/abs/2305.10666](http://arxiv.org/abs/2305.10666)

    该论文提出了一个统一的前端框架，捕捉了英文语音合成前端模块之间的依赖关系，并且在所有模块中均取得了最先进的性能。

    

    前端是英文文本到语音合成系统的关键组成部分，负责提取语言特征，如韵律和音素，这对于文本到语音模型合成语音至关重要。英文文本到语音前端通常由文本规范化模块（TN），单词韵律短语韵律短语模块（PWPP）和字形到音素模块（G2P）组成。然而，当前英文文本到语音前端的研究仅关注于单独模块，忽略它们之间的相互依赖，导致每个模块性能下降。因此，本文提出了一个统一的前端框架，捕捉英文文本到语音前端模块之间的依赖关系。广泛的实验表明，所提出的方法在所有模块中实现了最先进的性能。

    The front-end is a critical component of English text-to-speech (TTS) systems, responsible for extracting linguistic features that are essential for a text-to-speech model to synthesize speech, such as prosodies and phonemes. The English TTS front-end typically consists of a text normalization (TN) module, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme (G2P) module. However, current research on the English TTS front-end focuses solely on individual modules, neglecting the interdependence between them and resulting in sub-optimal performance for each module. Therefore, this paper proposes a unified front-end framework that captures the dependencies among the English TTS front-end modules. Extensive experiments have demonstrated that the proposed method achieves state-of-the-art (SOTA) performance in all modules.
    
[^203]: ChatGPT 需要进行SPADE（可持续性、隐私、数字鸿沟和伦理）评估：一项综述。

    ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])

    [http://arxiv.org/abs/2305.03123](http://arxiv.org/abs/2305.03123)

    本文研究关注ChatGPT面临的可持续性、隐私、数字鸿沟和伦理问题，提出了SPADE评估的必要性，并给出了缓解和建议。

    

    ChatGPT是另一个大型语言模型（LLM），由于其性能和有效的对话能力，在研究和工业界中得到了巨大的关注。最近，许多研究已经发表，以展示ChatGPT和其他LLMs的有效性、效率、集成和情感。相反，本研究关注的是大多数被忽视的重要方面，即可持续性、隐私、数字鸿沟和伦理，并建议不仅仅是ChatGPT，而是在对话机器人类别中的每一个后续入口都应该进行SPADE评估。本文详细讨论了关于ChatGPT的问题和关注点与上述特征一致。我们通过一些初步的数据收集和可视化以及假设的事实来支持我们的假设。我们还为每个问题提出了缓解和建议。此外，我们还提供了一些未来方向和开放问题的探讨。

    ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
    
[^204]: 安全可解释机器人规划

    Safe Explicable Robot Planning. (arXiv:2304.03773v1 [cs.RO])

    [http://arxiv.org/abs/2304.03773](http://arxiv.org/abs/2304.03773)

    安全可解释机器人规划方法（SEP）扩展了可解释规划，支持安全界限的规定，以实现安全和可解释之间的权衡。

    

    人们的期望源自于他们对其他人和世界的了解。在涉及到人机交互的情况下，对机器人的了解可能与现实不符，导致机器人不能满足人们的期望。可解释规划被引入作为一种新颖的规划方法，以协调人类期望和最优机器人行为，进行更可解释的机器人决策。一个关键的问题尚未得到解决，那就是在可解释决策过程中的安全性问题，这可能会导致不安全的可解释行为。我们提出了安全可解释规划（SEP），它扩展了可解释规划，支持安全界限的规定。 SEP的目标是找到一种策略，生成接近于人类期望的行为，同时满足安全约束的要求。这是多目标优化的一种特殊情况，SEP的解决方案位于帕累托前沿，提供了一个切实可行的解决方案，在不牺牲任何方面的重要性的前提下，产生了安全性和解释性之间的一个权衡。

    Human expectations stem from their knowledge of the others and the world. Where human-robot interaction is concerned, such knowledge about the robot may be inconsistent with the ground truth, resulting in the robot not meeting its expectations. Explicable planning was previously introduced as a novel planning approach to reconciling human expectations and the optimal robot behavior for more interpretable robot decision-making. One critical issue that remains unaddressed is safety during explicable decision-making which can lead to explicable behaviors that are unsafe. We propose Safe Explicable Planning (SEP), which extends explicable planning to support the specification of a safety bound. The objective of SEP is to find a policy that generates a behavior close to human expectations while satisfying the safety constraints introduced by the bound, which is a special case of multi-objective optimization where the solution to SEP lies on the Pareto frontier. Under such a formulation, we 
    
[^205]: 会话树搜索：一项新的混合对话任务

    Conversational Tree Search: A New Hybrid Dialog Task. (arXiv:2303.10227v1 [cs.CL])

    [http://arxiv.org/abs/2303.10227](http://arxiv.org/abs/2303.10227)

    本文介绍了一项新的任务——会话树搜索(CTS)，它可以架起FAQ和对话之间的桥梁，领域专家可以定义对话树，然后将其转换为一个有效的对话策略，只学习提出导航用户达到目标所需的问题。

    

    对话接口提供了一种灵活和方便的方式，让用户获取原本可能难以或不方便获得的信息。然而，现有的界面大体上可以分为两种类型：FAQ，用户必须提出明确的问题以检索一般的答案；或者对话，用户必须遵循预定义的路径但可能会接收到个性化的答案。本文介绍了一种新的任务——会话树搜索(CTS)，它架起了信息检索风格FAQ和面向任务对话之间的桥梁，允许领域专家定义对话树，然后将其转换为一个有效的对话策略，只学习提出导航用户达到目标所需的问题。我们收集了旅行报销领域的数据集，并展示了这项任务的基线（baseline）以及一项新颖的深度增强学习架构。结果显示，新的架构综合了FAQ和对话的优点，取得了良好的效果。

    Conversational interfaces provide a flexible and easy way for users to seek information that may otherwise be difficult or inconvenient to obtain. However, existing interfaces generally fall into one of two categories: FAQs, where users must have a concrete question in order to retrieve a general answer, or dialogs, where users must follow a predefined path but may receive a personalized answer. In this paper, we introduce Conversational Tree Search (CTS) as a new task that bridges the gap between FAQ-style information retrieval and task-oriented dialog, allowing domain-experts to define dialog trees which can then be converted to an efficient dialog policy that learns only to ask the questions necessary to navigate a user to their goal. We collect a dataset for the travel reimbursement domain and demonstrate a baseline as well as a novel deep Reinforcement Learning architecture for this task. Our results show that the new architecture combines the positive aspects of both the FAQ and 
    

