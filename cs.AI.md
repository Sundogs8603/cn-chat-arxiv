# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift.](http://arxiv.org/abs/2310.07535) | 在协变量转移下，我们提出了一种新的损失函数和表示匹配损失来优化模型的准确性和公平性，通过实验证明在公平性和准确性权衡方面优于其他基线算法，并且提出了一种未经研究的非对称协变量转移设置。 |
| [^2] | [Human-Centered Evaluation of XAI Methods.](http://arxiv.org/abs/2310.07534) | 在人工智能领域中，解释深度学习黑盒子的决策过程是一个关键挑战。本研究以用户为中心，客观评估了三种领先的解释方法的可解释性，并发现它们都提供了可解释的结果。 |
| [^3] | [Energy Estimates Across Layers of Computing: From Devices to Large-Scale Applications in Machine Learning for Natural Language Processing, Scientific Computing, and Cryptocurrency Mining.](http://arxiv.org/abs/2310.07516) | 该论文估计和分析了从设备到算法的计算层级中的能量消耗。在三个大规模计算应用中，能量消耗既存在于指令层面，也存在于模拟层面。此外，对使用不同架构的AI/ML加速器进行了能量效率比较。与热力学和生物学界限相比，计算系统的能量消耗相差27-36个数量级。 |
| [^4] | [Sample-Driven Federated Learning for Energy-Efficient and Real-Time IoT Sensing.](http://arxiv.org/abs/2310.07497) | 本文提出了一种针对具有实时感知能力的IoT网络设计的基于样本驱动的联邦学习方法，通过控制采样过程来减轻过拟合问题，提高整体准确性，并解决能效问题。 |
| [^5] | [Diversity for Contingency: Learning Diverse Behaviors for Efficient Adaptation and Transfer.](http://arxiv.org/abs/2310.07493) | 本研究提出一种简单的方法，在强化学习中通过学习多样的行为来实现适应和迁移。通过迭代学习一组策略，并利用约束来发现给定任务的所有可能解决方案，我们的方法能够在迁移设置中表现良好，快速适应任务或转换动力学的变化。 |
| [^6] | [Boosting Black-box Attack to Deep Neural Networks with Conditional Diffusion Models.](http://arxiv.org/abs/2310.07492) | 本文提出了一种新型黑盒攻击策略，通过条件转换生成可接受的对抗样本，以提高在查询受限情况下生成对抗样本的查询效率。 |
| [^7] | [KwaiYiiMath: Technical Report.](http://arxiv.org/abs/2310.07488) | KwaiYiiMath是一个用于增强数学推理能力的大型语言模型，通过应用监督微调和人类反馈强化学习，在英语和中文数学任务上取得了最先进的性能，并且能够正确解决生成的问题过程。 |
| [^8] | [Multimodal Graph Learning for Generative Tasks.](http://arxiv.org/abs/2310.07478) | 多模态图学习是一种通用且系统的框架，可以捕捉多模态数据之间的复杂关系，并在生成任务中取得了良好的效果。 |
| [^9] | [An Ontology of Co-Creative AI Systems.](http://arxiv.org/abs/2310.07472) | 这篇论文提出了一个共创AI系统的本体论，通过增加计算机作为分包商、计算机作为批评者和计算机作为队友等新类别，来扩展对创造力支持工具的本体论。 |
| [^10] | [The Implications of Decentralization in Blockchained Federated Learning: Evaluating the Impact of Model Staleness and Inconsistencies.](http://arxiv.org/abs/2310.07471) | 本研究评估了将联邦学习的协调外包给区块链等分散网络的实际影响，重点关注了由区块链的运作方式支持的模型陈旧和不一致对异步FL训练过程的影响。 |
| [^11] | [AI/ML-based Load Prediction in IEEE 802.11 Enterprise Networks.](http://arxiv.org/abs/2310.07467) | 该论文研究了在实际企业Wi-Fi网络中采用基于AI/ML的负载预测的适用性和可行性，并发现受硬件限制的AI/ML模型可以在平均误差小于20%和85th百分位误差小于3%的情况下预测网络负载。 |
| [^12] | [Efficient machine-learning surrogates for large-scale geological carbon and energy storage.](http://arxiv.org/abs/2310.07461) | 本研究提出了一种专门的机器学习（ML）模型，通过域分解和拓扑嵌入器降低训练成本，提高机器学习在大规模地质储存应用中的效率。 |
| [^13] | [HealthWalk: Promoting Health and Mobility through Sensor-Based Rollator Walker Assistance.](http://arxiv.org/abs/2310.07434) | HealthWalk通过集成传感器到滚轮行走者设计中，可以解决滚轮行走者用户姿势不好导致的健康问题，并实现其他有趣的用例。 |
| [^14] | [Imitation Learning from Observation with Automatic Discount Scheduling.](http://arxiv.org/abs/2310.07433) | 我们提出了一个新颖的观察学习模仿(ILfO)框架，能够解决由于奖励信号分配错误导致代理无法学习初始行为的问题。 |
| [^15] | [Multi-Concept T2I-Zero: Tweaking Only The Text Embeddings and Nothing Else.](http://arxiv.org/abs/2310.07419) | 本研究提出了一种新的方法，通过仅仅调整文本嵌入，而不需重新训练模型，实现了自然的多概念生成。通过解决预训练模型中文本嵌入的局限性，如概念支配和非定位贡献，我们设计了一个廉价的解决方案，提高了多概念生成的性能。 |
| [^16] | [Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages.](http://arxiv.org/abs/2310.07418) | 本文对视觉强化学习中的可塑性进行了研究，发现数据增强对保持可塑性至关重要，评论者的可塑性损失是高效训练的主要限制因素，并且未及时恢复评论者的可塑性将导致灾难性结果。这为解决高重放比困境提供了新的策略。 |
| [^17] | [What can knowledge graph alignment gain with Neuro-Symbolic learning approaches?.](http://arxiv.org/abs/2310.07417) | 本文研究了知识图谱对齐的现状与挑战，提出了使用神经符号学习方法的潜力，以实现解释性、可验证性和高质量的对齐结果。 |
| [^18] | [DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation.](http://arxiv.org/abs/2310.07403) | DASpeech是一个非自回归的直接语音翻译模型，使用有向无环图（DAG）来实现快速和高质量的语音翻译。 |
| [^19] | [NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining.](http://arxiv.org/abs/2310.07402) | 本研究通过采用Transformer架构和数值多尺度嵌入模块，使时间序列自监督模型能够扩展到大规模数据集，并在大规模数据集上进行预训练。 |
| [^20] | [Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation.](http://arxiv.org/abs/2310.07397) | 这项工作提出了一个自动数据集筛选框架，并构建了一个大规模的个性化面向目标对话数据集，名为TopDial。该数据集质量高，有助于探索个性化目标导向的对话。 |
| [^21] | [Learning a Reward Function for User-Preferred Appliance Scheduling.](http://arxiv.org/abs/2310.07389) | 本文介绍了一种基于逆强化学习的模型，通过使用终端用户过去的消费数据，帮助创建用户每日的电器计划，从而鼓励终端用户参与需求响应服务的提供。 |
| [^22] | [Histopathological Image Classification and Vulnerability Analysis using Federated Learning.](http://arxiv.org/abs/2310.07380) | 这项研究开发了一种基于联邦学习的隐私保护技术，应用于皮肤癌数据集，发现该模型容易受到数据污染攻击的影响。 |
| [^23] | [Causal Unsupervised Semantic Segmentation.](http://arxiv.org/abs/2310.07379) | 因果无监督语义分割（CAUSE）是一个利用因果推断的新框架，旨在实现无监督语义分割。该方法通过构建概念聚类表作为中介，并与概念自监督学习建立联系，解决了无监督分割中适当聚类水平的挑战。 |
| [^24] | [Point Cloud Denoising and Outlier Detection with Local Geometric Structure by Dynamic Graph CNN.](http://arxiv.org/abs/2310.07376) | 本研究提出了一种使用动态图卷积神经网络的局部几何结构的方法，用于点云的去噪和异常检测，实验证明该方法在准确性方面优于传统方法。 |
| [^25] | [Give and Take: Federated Transfer Learning for Industrial IoT Network Intrusion Detection.](http://arxiv.org/abs/2310.07354) | 本文提出了一种用于工业物联网网络入侵检测的联邦迁移学习（FTL）方法，通过组合神经网络对IIoT数据进行分割并更新服务器模型，实现了高性能的入侵检测。 |
| [^26] | [Semantic Association Rule Learning from Time Series Data and Knowledge Graphs.](http://arxiv.org/abs/2310.07348) | 本文提出了一种在数字孪生中利用知识图谱和时间序列数据进行语义关联规则学习的流程，并且引入了新的语义关联规则准则。该方法通过在工业水网络场景中的评估表明，能够学习到更具泛化性的包含语义信息的关联规则，为进一步研究工业应用背景下的语义关联规则学习打下基础。 |
| [^27] | [Fast-ELECTRA for Efficient Pre-training.](http://arxiv.org/abs/2310.07347) | 提出了一种快速ELECTRA的方法，利用现有的语言模型作为辅助模型，通过温度缩放平滑主模型的输出分布，达到与最先进的ELECTRA预训练方法相媲美的性能，并显著减少了辅助模型共同训练带来的计算和内存成本。 |
| [^28] | [Exploring Social Motion Latent Space and Human Awareness for Effective Robot Navigation in Crowded Environments.](http://arxiv.org/abs/2310.07335) | 本文提出了一种通过学习社交运动潜空间来生成机器人控制的新方法，实现了在拥挤环境中的有效导航。同时，引入了人类对机器人的意识概念，并证明了这种意识的融入能够改善导航性能。 |
| [^29] | [An Empirical Study of Instruction-tuning Large Language Models in Chinese.](http://arxiv.org/abs/2310.07328) | 本论文进行了对中文大语言模型的指导调整的实证研究，探索了LLM基础、参数高效的方法和指令数据类型的影响，并研究了其他因素的影响，为定制能更好响应中文指令的LLM提供了有价值的发现。 |
| [^30] | [An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l.](http://arxiv.org/abs/2310.07325) | 在gelu-4l中，我们提供了证据表明内存管理对于transformer模型至关重要，并说明了Direct Logit Attribution技术的不准确之处。 |
| [^31] | [On the Impact of Cross-Domain Data on German Language Models.](http://arxiv.org/abs/2310.07321) | 本研究通过对德语语言模型进行实验，发现将数据多样性置于数据质量之上的交叉领域数据集训练方法，可以显著提高模型的性能，并超过了之前的最先进模型。 |
| [^32] | [WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models.](http://arxiv.org/abs/2310.07312) | WiGenAI通过引入扩散模型，将生成式人工智能应用于无线通信系统中，为研究奠定基础。这篇文章介绍了扩散模型作为生成模型的最新范式，并讨论了它在无线通信系统中的应用。通过两个案例研究展示了扩散模型在开发韧性的AI本地通信系统中的潜力。 |
| [^33] | [RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation.](http://arxiv.org/abs/2310.07299) | 这项研究提出了一个叫做RobustGEC的基准测试，用于评估语法错误修正系统的上下文鲁棒性。研究发现目前最先进的系统仍然无法很好地应对上下文扰动，并提出了一种简单而有效的解决方法。 |
| [^34] | [Beyond Memorization: Violating Privacy Via Inference with Large Language Models.](http://arxiv.org/abs/2310.07298) | 该论文首次全面研究了预训练大型语言模型从文本中推断个人属性的能力，发现当前的模型可以以较低的成本和时间比例，准确地推断出多种个人属性，这引发了隐私泄露的新威胁。 |
| [^35] | [An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT.](http://arxiv.org/abs/2310.07282) | 本研究分析了在医疗保健领域应用大型语言模型（尤其是BioBERT）的可行性，并提出了针对医疗保健领域的微调方法。研究突出了BioBERT对于解决与生物医学文本挖掘相关任务的特定要求的适用性。 |
| [^36] | [BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations.](http://arxiv.org/abs/2310.07276) | BioT5是一个全面的预训练框架，在生物学中利用化学知识和自然语言关联丰富了跨模态整合，通过鲁棒的分子表示和上下文知识提取，实现了更有效的信息利用，展现出卓越的性能。 |
| [^37] | [CoPAL: Corrective Planning of Robot Actions with Large Language Models.](http://arxiv.org/abs/2310.07263) | 本文提出了一个具有大规模语言模型的机器人动作纠正规划系统，通过处理生成计划中的物理基础、逻辑和语义错误的再规划策略，实现了在复杂环境中的任务和动作规划。通过仿真和实际场景的验证，证明了该系统的有效性。 |
| [^38] | [Uncovering Hidden Connections: Iterative Tracking and Reasoning for Video-grounded Dialog.](http://arxiv.org/abs/2310.07259) | 本文提出了一种迭代跟踪和推理策略，结合文本编码器和视觉编码器以生成准确的响应，解决了视频对话中逐步理解对话历史和吸收视频信息的挑战。 |
| [^39] | [ADMEOOD: Out-of-Distribution Benchmark for Drug Property Prediction.](http://arxiv.org/abs/2310.07253) | ADMEOOD是一个设计用于药物属性预测的超分布基准，包含27个药物属性和两种数据转移（噪声转移和概念冲突漂移）。 |
| [^40] | [Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs.](http://arxiv.org/abs/2310.07251) | 本文提出了一种在LLM中应用通用伦理推理能力而非特定伦理原则的方法，以处理全球范围的价值多元性。作者开发了一个框架，将道德困境与不同形式的规范伦理以及不同抽象层次的道德原则相结合。初步实验表明，尽管GPT-4几乎可以完美地进行伦理推理，但这些模型仍然存在对西方和以英语为母语的社会道德价值的偏见。 |
| [^41] | [Surrogate modeling for stochastic crack growth processes in structural health monitoring applications.](http://arxiv.org/abs/2310.07241) | 本文提出了一种代理模型用于预测结构中裂纹的扩展，并成功地编码了不同的随机不确定性来源。该模型基于高斯过程回归模型，能够生成先验分布用于贝叶斯结构健康监测任务。 |
| [^42] | [Using Learnable Physics for Real-Time Exercise Form Recommendations.](http://arxiv.org/abs/2310.07221) | 本文提出了一种使用可学习的物理学的算法流程，能够在实时环境中诊断运动姿势问题并提供矫正建议，通过姿势识别、重复次数计算和动作演变跟踪实现。该系统在六个不同的运动上进行了评估，通过低成本设备如智能手机提供实时建议，使自我练习成为可能，同时降低受伤风险。 |
| [^43] | [Improved Membership Inference Attacks Against Language Classification Models.](http://arxiv.org/abs/2310.07219) | 在这篇论文中，我们提出了一个新的框架，用于对语言分类模型进行成员推理攻击。通过利用集成方法，生成多个专门的攻击模型，我们展示了这种方法在经典和语言分类任务上比单个攻击模型或每个类别标签的攻击模型更准确。 |
| [^44] | [Quantifying Agent Interaction in Multi-agent Reinforcement Learning for Cost-efficient Generalization.](http://arxiv.org/abs/2310.07218) | 本研究针对多智能体强化学习中的泛化问题，提出了一种量化智能体相互作用的指标，并通过该指标设计了资源分配方法，可以在有限预算下训练适用于多样化场景的智能体策略集合。 |
| [^45] | [Multi-Task Learning-Enabled Automatic Vessel Draft Reading for Intelligent Maritime Surveillance.](http://arxiv.org/abs/2310.07212) | 本研究提出了一种多任务学习启用的计算方法，用于准确高可靠地生成船只吃水读数，以支持智能海上监控。 |
| [^46] | [State of the Art on Diffusion Models for Visual Computing.](http://arxiv.org/abs/2310.07204) | 这篇论文旨在介绍最新的视觉计算领域中扩散模型的发展和应用，涵盖了生成人工智能的核心概念和实现细节，并总结了个人化、条件约束和反演等重要方面。 |
| [^47] | [MatChat: A Large Language Model and Application Service Platform for Materials Science.](http://arxiv.org/abs/2310.07197) | MatChat是一个用于材料科学的大型语言模型和应用服务平台，利用LLaMA2-7B模型和13,878条结构化材料知识数据，能够预测无机材料的合成路径。 |
| [^48] | [Adaptive Gating in Mixture-of-Experts based Language Models.](http://arxiv.org/abs/2310.07188) | 本文介绍了一种自适应门控的混合专家语言模型训练策略，通过根据标记的专家概率分布将标记分配给变量数量的专家，同时保持模型的稀疏性和提高训练效率。 |
| [^49] | [Multiview Transformer: Rethinking Spatial Information in Hyperspectral Image Classification.](http://arxiv.org/abs/2310.07186) | 本文提出了一种多视角Transformer用于高光谱图像分类，解决了HSI立方体中可能记录到的场景特定但非本质相关性带来的空间过拟合问题，并通过多视角主成分分析、光谱编码器-解码器和空间池化标记化Transformer的组合来提取低维的空间-光谱特征表示。 |
| [^50] | [rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera.](http://arxiv.org/abs/2310.07179) | rpcPRF是一种用于卫星相机的可推广MPI神经辐射场，通过重投影监督和辐射场渲染技术，使模型适用于单个或少量输入，并在未见场景的图像上表现良好。 |
| [^51] | [Online Speculative Decoding.](http://arxiv.org/abs/2310.07177) | 在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。 |
| [^52] | [Solving Travelling Thief Problems using Coordination Based Methods.](http://arxiv.org/abs/2310.07156) | 这篇论文提出了一种解决旅行窃贼问题的协调方法，去改善城市选择和物品选择之间的协调问题。 |
| [^53] | [No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN Partition for On-Device ML.](http://arxiv.org/abs/2310.07152) | 本文研究了TEE保护下设备端机器学习的安全性问题。通过对现有的TSDP解决方案进行评估，发现这些解决方案容易受到隐私窃取攻击的影响。 |
| [^54] | [Determining Winners in Elections with Absent Votes.](http://arxiv.org/abs/2310.07150) | 本研究讨论了在选举中一些选票缺席情况下确定候选人是否可以获胜的问题，通过研究顶部截断时不同的选票规则，我们发现了这个问题的复杂度，并提出了在特殊情况下可以在多项式时间内计算出结果的方法。 |
| [^55] | [Denoising Task Routing for Diffusion Models.](http://arxiv.org/abs/2310.07138) | 本文提出了一种名为去噪任务路由的策略，通过为扩散模型的不同任务建立独立的信息路径，实现了对多任务学习的明确纳入。该方法将去噪任务的先验知识无缝集成到框架中，通过激活相似的通道和滑动窗口的方式，充分利用了相邻时间步任务间的亲和关系。 |
| [^56] | [Off-Policy Evaluation for Human Feedback.](http://arxiv.org/abs/2310.07123) | 本论文介绍了一个用于人类反馈的非策略评估（OPEHF）框架，可以准确评估人类反馈信号。这个框架解决了现有OPE方法在估计人类反馈信号上的不足。 |
| [^57] | [The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models.](http://arxiv.org/abs/2310.07106) | 本文展示了深度语言模型(DLMs)的分层结构与人脑语言理解的时间动态之间存在强相关性，通过采用电子皮层图(ECoG)数据来优化时间分辨率，为深入了解人脑语言处理机制提供了新的视角。 |
| [^58] | [ClausewitzGPT Framework: A New Frontier in Theoretical Large Language Model Enhanced Information Operations.](http://arxiv.org/abs/2310.07099) | ClausewitzGPT框架提供了应对信息操作中大型语言模型增强风险的一种新方法，并强调了自主AI代理人在其中的关键作用。 |
| [^59] | [Sparse Universal Transformer.](http://arxiv.org/abs/2310.07096) | 本文介绍了稀疏通用Transformer（SUT），它通过利用稀疏混合专家（SMoE）和一种新的基于切棍法的动态停止机制来减少计算复杂性，同时保持参数效率和泛化能力。实验证明，SUT在形式语言任务上具有与强基线模型相当的性能，并能显著降低计算资源使用。 |
| [^60] | [Jaeger: A Concatenation-Based Multi-Transformer VQA Model.](http://arxiv.org/abs/2310.07091) | Jaeger是一种基于连接的多变换器VQA模型，利用RoBERTa large和GPT2-xl作为特征提取器，通过并行考虑多源信息来增强模型表征能力。 |
| [^61] | [Diversity of Thought Improves Reasoning Abilities of Large Language Models.](http://arxiv.org/abs/2310.07088) | 本文提出了一种方法，通过改变输入提示来提高大规模语言模型的推理能力，从而改善模型在复杂推理场景中的表现。这种方法自动采集模型反馈，生成适合问题的多样化提示，并通过多次推理调用来集成这些多样化的提示。 |
| [^62] | [Leveraging Twitter Data for Sentiment Analysis of Transit User Feedback: An NLP Framework.](http://arxiv.org/abs/2310.07086) | 本论文提出了一个基于自然语言处理的框架，利用推特数据进行交通用户反馈的情感分析。通过少样本学习识别推特中的问题，并采用词典情感分析模型评估推特情感的强度和极性。 |
| [^63] | [Auditing and Robustifying COVID-19 Misinformation Datasets via Anticontent Sampling.](http://arxiv.org/abs/2310.07078) | 本文拓展了对COVID-19虚假信息数据集进行审计和强化的研究。首先，发现在小数据上训练的专业分类器在野外环境中的性能表现有限。其次，提出了一种无需人工注释的主动学习方法，通过增加具有挑战性的抗内容来增强分类器。 |
| [^64] | [Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding.](http://arxiv.org/abs/2310.07075) | 本文提出了一种无语法错误且具有泛化能力的LLM工具使用方法ToolDec，通过有限状态解码算法消除了工具相关错误，使LLM能够有效选择工具，而无需微调或上下文文档。 |
| [^65] | [Large Language Models can Learn Rules.](http://arxiv.org/abs/2310.07064) | 大型语言模型(LLMs)在各种推理任务中展示了令人印象深刻的性能。为了提高提示方法的准确性和一致性，我们提出了Hypotheses-to-Theories (HtT)框架，用于学习LLMs推理的规则库，从而改进了现有的提示方法。 |
| [^66] | [DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records.](http://arxiv.org/abs/2310.07059) | 本文提出了DKEC，一种领域知识增强的分类器，用于医学诊断预测。它利用标签的注意力机制和组内训练方法来捕捉医学实体之间的语义关系，并增加罕见类别的样本数量。评估结果显示其在医学数据集上表现良好。 |
| [^67] | [Computational Pathology at Health System Scale -- Self-Supervised Foundation Models from Three Billion Images.](http://arxiv.org/abs/2310.07033) | 该论文通过使用大规模无标签数据集训练自监督基础模型并在大型临床病理学数据集上进行评估，旨在训练最大的学术基础模型并评估最显著的自监督学习算法。 |
| [^68] | [Facial Forgery-based Deepfake Detection using Fine-Grained Features.](http://arxiv.org/abs/2310.07028) | 该论文提出了一种基于细粒度特征的Deepfake检测方法，通过抑制背景噪声和学习区分性特征来提高检测的效果。 |
| [^69] | [NEWTON: Are Large Language Models Capable of Physical Reasoning?.](http://arxiv.org/abs/2310.07018) | NEWTON是一个用于评估大型语言模型物理推理能力的仓库和基准，包含2800个物体-属性对和160K个问答问题。 |
| [^70] | [Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs.](http://arxiv.org/abs/2310.07008) | 本文提出了一种新颖的方法，通过对预训练的文本到文本问答系统生成的候选答案基于其类型进行过滤和重新排序，以解决在知识图谱问答任务中，模型容量有限且对于含有不太流行实体的问题质量下降的问题。 |
| [^71] | [Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation.](http://arxiv.org/abs/2310.06987) | 本文介绍了一种生成攻击技术，通过操纵解码方法的变化，可以导致开源LLMs的灾难性越狱，将错位率从0%提高到了超过95%。这项攻击方法简单而有效，在11个语言模型中表现优于最先进的攻击方法，且计算量降低了30倍。 |
| [^72] | [On the Interpretability of Part-Prototype Based Classifiers: A Human Centric Analysis.](http://arxiv.org/abs/2310.06966) | 本论文提出了一个用于从人类角度评估基于部分原型模型可解释性的框架，并使用亚马逊机械土耳其进行了广泛实验。这些实验不仅展示了框架在评估各种基于部分原型模型的可解释性方面的能力，还是最全面的工作。 |
| [^73] | [Sparse Finetuning for Inference Acceleration of Large Language Models.](http://arxiv.org/abs/2310.06927) | 本论文研究了大型语言模型的准确稀疏微调问题，提出了基于L2范数的蒸馏方法SquareHead，可以在高稀疏性下实现准确的恢复；同时展示了稀疏语言模型的实际效率，可在CPU和GPU运行时实现加速，并且观察到在受内存限制的模型中，稀疏性也可用于减少内存带宽。 |
| [^74] | [PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification.](http://arxiv.org/abs/2310.06923) | 本文提出了一种名为PICProp的方法，基于双层优化，用于在深度学习和物理信息学习中进行不确定性量化。该方法能够在不进行强大假设的情况下计算有效的置信区间（CI），并且通过传播置信度实现了数据位置到整个域的置信度传播。 |
| [^75] | [Distributed Transfer Learning with 4th Gen Intel Xeon Processors.](http://arxiv.org/abs/2310.06916) | 本文研究了如何利用第四代英特尔韧性处理器进行分布式迁移学习，通过使用英特尔高级矩阵扩展（AMX）和Horovod在Image Classification TensorFlow数据集上实现了接近最先进的图像分类准确性。 |
| [^76] | [Reinforcement Learning in a Safety-Embedded MDP with Trajectory Optimization.](http://arxiv.org/abs/2310.06903) | 本文介绍了一种在安全嵌入式MDP中结合轨迹优化的强化学习方法，通过将安全约束嵌入动作空间，能够有效地在最大化奖励和遵守安全约束之间取得平衡，并在挑战性任务中取得了优异性能。 |
| [^77] | [Design of JiuTian Intelligent Network Simulation Platform.](http://arxiv.org/abs/2310.06858) | 本文介绍了九天智能网络仿真平台，提供了无线通信仿真数据服务，包含可扩展的仿真器功能，支持开放服务和强化学习算法训练，并允许用户上传和更新参数配置。未来研究方向包括更深入地探索业务场景和优化任务。 |
| [^78] | [Brave new world: Artificial Intelligence in teaching and learning.](http://arxiv.org/abs/2310.06856) | 本文展示了在教学和学习中如何使用大型语言模型，并讨论了教育领域中已经发生的人工智能事件。我们主张引入人工智能政策和规范，以提高教育工具的意识，并减少教育中的人工智能事件风险。 |
| [^79] | [Genetic Algorithm-Based Dynamic Backdoor Attack on Federated Learning-Based Network Traffic Classification.](http://arxiv.org/abs/2310.06855) | 本论文研究了基于联邦学习的网络流量分类中的动态后门攻击，并提出了一种基于遗传算法的新型后门攻击方法GABAttack。该方法能够优化后门触发器参数的值和位置，以实现对全局模型的篡改。 |
| [^80] | [Learning with Noisy Labels for Human Fall Events Classification: Joint Cooperative Training with Trinity Networks.](http://arxiv.org/abs/2310.06854) | 联合提出的JoCoT方法在人类跌倒事件分类中应用噪声标签学习，通过使用两个教师模块和一个学生模块，提高了鲁棒性和性能，并且采用人体骨架数据以保护隐私。 |
| [^81] | [BodyFormer: Semantics-guided 3D Body Gesture Synthesis with Transformer.](http://arxiv.org/abs/2310.06851) | 本论文提出了一个基于Transformer的框架，用于自动从语音中合成3D人体手势。通过引入变分Transformer和模态位置嵌入层，可以有效地学习并生成多样化的手势。另外，通过内部模态预训练方案来解决数据稀缺问题。 |
| [^82] | [DeepTriNet: A Tri-Level Attention Based DeepLabv3+ Architecture for Semantic Segmentation of Satellite Images.](http://arxiv.org/abs/2310.06848) | DeepTriNet是一种基于三级注意力的DeepLabv3+架构，用于卫星图像的语义分割。该方法通过结合SENets和TAUs桥接了编解码器输出与相关特征之间的语义差距，同时通过自我监督确定了更重要和更通用的特征。 |
| [^83] | [Exploiting Language Models as a Source of Knowledge for Cognitive Agents.](http://arxiv.org/abs/2310.06846) | 本研究利用语言模型作为认知智能体的任务知识来源，探索了将语言模型作为外部知识源用于认知系统的挑战和机会，并提出了通过整合知识提取和认知架构能力来提高知识提取效果的可能方法。 |
| [^84] | [RobustEdge: Low Power Adversarial Detection for Cloud-Edge Systems.](http://arxiv.org/abs/2310.06845) | RobustEdge是一种低功耗、适用于边缘设备的对抗检测方法，解决了在云边系统中能量浪费和计算开销问题。 |
| [^85] | [Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models.](http://arxiv.org/abs/2310.06692) | Meta-CoT是一种在混合任务场景中能够通用思维链提示的方法，在十个公共基准推理任务中表现出卓越的性能和优越的泛化能力。 |
| [^86] | [Constructive Large Language Models Alignment with Diverse Feedback.](http://arxiv.org/abs/2310.06450) | 本文提出了一种新的方法，即建构性和多样化反馈（CDF），用于增强大型语言模型（LLM）的对齐效果。我们通过收集不同类型的反馈，并根据问题的难度级别进行处理，实现了更好的性能。 |
| [^87] | [HyperAttention: Long-context Attention in Near-Linear Time.](http://arxiv.org/abs/2310.05869) | 近似注意力机制HyperAttention解决了在大型语言模型中使用的长上下文的计算挑战，并通过引入两个参数来衡量问题的难度。HyperAttention具有模块化设计，可轻松集成其他快速低级实现。 |
| [^88] | [Are Large Language Models Post Hoc Explainers?.](http://arxiv.org/abs/2310.05797) | 这项工作提出了第一个研究大型语言模型（LLMs）解释其他预测模型有效性的框架，并且提出了多个提示策略，填补了当前对于LLMs在解释其他模型行为方面的缺失。 |
| [^89] | [Automated Argument Generation from Legal Facts.](http://arxiv.org/abs/2310.05680) | 该论文研究了利用开源大型语言模型的生成能力，从法律案件中自动生成论证。实验结果显示，最佳方法生成的论证与基准集的黄金标准注释平均重叠率为63%。 |
| [^90] | [On Double-Descent in Reinforcement Learning with LSTD and Random Features.](http://arxiv.org/abs/2310.05518) | 本文研究了在强化学习中网络大小和L2正则化对性能的影响，并观察到了双下降现象。通过使用随机特征和懒惰训练策略，在参数和状态数无限大的情况下研究了正则化的最小二乘时间差分算法，得出了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。 |
| [^91] | [A Critical Look at Classic Test-Time Adaptation Methods in Semantic Segmentation.](http://arxiv.org/abs/2310.05341) | 这项研究对语义分割中的经典测试时适应方法进行了批判性探究，揭示了分割TTA所面临的独特挑战，并发现经典TTA策略在这一任务中并不有效。 |
| [^92] | [Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems.](http://arxiv.org/abs/2310.05280) | 这项研究评估了对话系统中的人格偏见对社交偏见的影响，并建立了一个综合评估框架来衡量不同人格采用下的偏见程度。 |
| [^93] | [InstructDET: Diversifying Referring Object Detection with Generalized Instructions.](http://arxiv.org/abs/2310.05136) | 我们提出了一种名为InstructDET的方法，可以通过多样化的指令定位目标对象并进行指称对象检测。我们构建了一个包含图像、边界框和泛化指令的数据集，其中利用了视觉语言模型和大型语言模型生成指令。 |
| [^94] | [Learning Intra- and Inter-Cell Differences for Accurate Battery Lifespan Prediction across Diverse Conditions.](http://arxiv.org/abs/2310.05052) | 该论文介绍了一种跨不同条件精确预测电池寿命的方法，通过捕捉目标电池和参考电池之间的电信号差异，无论材料和老化条件如何，在扩展特征空间的同时为通用的电池寿命预测框架铺平了道路。 |
| [^95] | [Revisiting Large Language Models as Zero-shot Relation Extractors.](http://arxiv.org/abs/2310.05028) | 本研究重新审视了大型语言模型(LLMs)作为零-shot关系抽取器的潜力，并提出了通过总结和提问(\textsc{SumAsk})提示方法来改进零-shot关系抽取。实验证明LLMs在这一任务上具有良好的表现。 |
| [^96] | [Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models.](http://arxiv.org/abs/2310.05015) | Compresso是一种结构化修剪LLMs的新方法，通过协作促进学习最优的修剪决策，解决了训练成本高和数据收集困难的挑战。 |
| [^97] | [Cell Tracking-by-detection using Elliptical Bounding Boxes.](http://arxiv.org/abs/2310.04895) | 本文提出了一种基于经典检测方法的新方法，通过将细胞形状近似为定向椭圆并使用通用定向对象检测器来识别细胞，实现了对细胞的检测和跟踪，减轻了对标注数据的需求。 |
| [^98] | [LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT.](http://arxiv.org/abs/2310.04673) | LauraGPT是一个统一的GPT模型，用于音频识别、理解和生成，具有广泛的应用范围，包括自动语音识别、语音翻译、文本到语音合成、机器翻译等任务。 |
| [^99] | [Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference.](http://arxiv.org/abs/2310.04395) | 该论文提出了一种利用自一致性改进数据有效的摊余贝叶斯推理方法，通过反转贝叶斯定理并利用近似表示的联合模型估计边际似然，加速条件神经密度估计器的学习动力学。 |
| [^100] | [Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications.](http://arxiv.org/abs/2310.04381) | Hermes是一个自动生成有限状态机的端到端框架，用于解锁移动网络协议的安全分析。通过处理自然语言规范并生成逻辑公式，Hermes能够发现新的漏洞和攻击，并对现有规范和商业基带进行评估。 |
| [^101] | [Axiomatic Aggregations of Abductive Explanations.](http://arxiv.org/abs/2310.03131) | 本文提出了通过将多种可能的归纳解释汇总为特征重要性分数的三种聚合方法，以解决存在多个有效的归纳解释的问题。 |
| [^102] | [HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning.](http://arxiv.org/abs/2310.00113) | HyperMask是一种用于持续学习的方法，它使用基于超网络的掩码来训练一个单一网络，以克服人工神经网络在多任务上的灾难性遗忘问题。 |
| [^103] | [Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency.](http://arxiv.org/abs/2309.17382) | 提出了一个名为"RAFA"的原则框架，通过在LLM中将推理视为学习和规划的贝叶斯问题，协调推理和行动。通过一个提示模板进行推理，学习并制定未来的轨迹规划，然后在每一步中采取计划轨迹的初始行动并重新规划未来轨迹。这个框架具有可证明的遗憾保证。 |
| [^104] | [ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks.](http://arxiv.org/abs/2309.16918) | 本论文提出了一种新的图神经网络解释方法ACGAN-GNNExplainer，将辅助分类器生成对抗网络（ACGAN）引入到GNN解释领域。通过利用生成器为原始输入图生成解释，并借助鉴别器监督生成过程，提高解释的准确性和保真度。实验证明了该方法的有效性和实用性。 |
| [^105] | [Optimizing delegation between human and AI collaborative agents.](http://arxiv.org/abs/2309.14718) | 该研究提出了一种优化人工智能和人类协作代理之间委派的方法，通过训练一个代理管理器根据任务绩效缺陷进行委派决策，并且可以处理不同环境表示下的团队操作。方法在实验中表现显著优于其他管理团队的方法。 |
| [^106] | [Rethinking superpixel segmentation from biologically inspired mechanisms.](http://arxiv.org/abs/2309.13438) | 该论文从神经结构和视觉机制的启示出发，提出了一种生物网络架构用于超像素分割，其中包括增强筛选模块（ESM）和边界感知标签（BAL），旨在产生严格遵循物体边界且传达丰富视觉符号的超像素。 |
| [^107] | [LPML: LLM-Prompting Markup Language for Mathematical Reasoning.](http://arxiv.org/abs/2309.13078) | 本论文提出了LPML，一种用于数学推理的LLM提示标记语言。通过将Chain-of-Thought方法和Python REPL与该标记语言结合，我们能够控制LLM生成文本中的错误，并增强其推理能力。我们的方法能够实现利用Python计算纠正错误和解决挑战性数学问题，而只需要零样本提示。 |
| [^108] | [DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research.](http://arxiv.org/abs/2309.01808) | DiscoverPath是一个基于知识图的生物医学研究论文搜索引擎，通过命名实体识别和词性标注从文章摘要中提取术语和关系，并展示给用户一个关注查询实体及其邻近节点的子图，以及查询推荐系统，使用户能够循序渐进地细化查询。 |
| [^109] | [Zero-shot Inversion Process for Image Attribute Editing with Diffusion Models.](http://arxiv.org/abs/2308.15854) | 提出了一种零样本反演过程（ZIP）框架，用于图像属性编辑。该方法利用生成的视觉参考和文本引导注入扩散模型的语义潜空间，可以在文本提示的直观控制下产生多样的内容和属性，并展现出对不同属性操作的鲁棒性。 |
| [^110] | [Elucidating the Exposure Bias in Diffusion Models.](http://arxiv.org/abs/2308.15321) | 本文系统地研究了扩散模型中的曝光偏差问题，并提出了一种名为Epsilon Scaling的免训练方法来减轻这一问题。实验结果验证了该方法的有效性。 |
| [^111] | [InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4.](http://arxiv.org/abs/2308.12067) | InstructionGPT-4通过仅使用200个例子进行微调，在多模式指令数据质量度量和选择器的帮助下，在各种评估任务中优于原始的MiniGPT-4。 |
| [^112] | [A Benchmark Study on Calibration.](http://arxiv.org/abs/2308.11838) | 这项研究提出了一个模型校准的基准研究，利用神经架构搜索空间探索了模型校准属性。研究结果显示，模型校准可以在不同任务中泛化，并可以同时兼顾模型的准确性和校准性能。 |
| [^113] | [PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant Poker Game Simulations.](http://arxiv.org/abs/2308.07327) | PokerKit是一个全面的Python库，用于细粒度多变体扑克游戏模拟，提供广泛的扑克变体支持和灵活的游戏状态控制，对扑克AI开发、工具创建和在线扑克赌场实现等领域具有重要贡献。 |
| [^114] | [CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools.](http://arxiv.org/abs/2307.15770) | 本论文介绍了一种名为ChatReport的基于LLM的系统，它通过实现可追溯的答案和解决领域专家参与低效性的问题，旨在通过自动分析企业可持续性报告，实现可持续性披露分析民主化。 |
| [^115] | [Elastic Decision Transformer.](http://arxiv.org/abs/2307.02484) | 弹性决策变压器（EDT）通过在测试时间进行动作推断时调整历史长度来实现轨迹拼接，填补了决策变压器（DT）在这一方面的性能差距，并且在多任务情况下胜过基于Q-Learning的方法。 |
| [^116] | [DifFSS: Diffusion Model for Few-Shot Semantic Segmentation.](http://arxiv.org/abs/2307.00773) | DifFSS是一种利用扩散模型改进小样本语义分割性能的方法，通过生成多样化的辅助支持图像，而不需要修改网络结构，从而显著提高最先进的小样本语义分割模型的性能。 |
| [^117] | [DisCo: Disentangled Control for Referring Human Dance Generation in Real World.](http://arxiv.org/abs/2307.00040) | 这篇论文提出了一个新的问题设置：引用人类舞蹈生成。在现实世界的舞蹈场景中，通过解耦控制来解决舞蹈合成中的挑战，包括忠实性、泛化能力和组合性。 |
| [^118] | [DCdetector: Dual Attention Contrastive Representation Learning for Time Series Anomaly Detection.](http://arxiv.org/abs/2306.10347) | DCdetector是一种多尺度双重关注的对比表示学习模型，用于时间序列异常检测。它利用双重关注不对称设计和纯对比损失学习置换不变表示，从而有效区分异常样本。 |
| [^119] | [Explore, Establish, Exploit: Red Teaming Language Models from Scratch.](http://arxiv.org/abs/2306.09442) | 本文提出了一种新的红队行动，通过从高层次、抽象的规范出发来考虑语言模型的行为，以探究模型的创新和贡献。 |
| [^120] | [DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting.](http://arxiv.org/abs/2306.01984) | 提出了一种新的扩散模型，其结合了数据中编码的时间动态，自然地编码了多步和长程预测能力，具有灵活的采样轨迹和折衷性能与加速采样的能力，同时提高了计算效率，可在时空预测方面取得竞争性表现。 |
| [^121] | [A Measure-Theoretic Axiomatisation of Causality.](http://arxiv.org/abs/2305.17139) | 本文提出了一个称为"因果空间"的概念，旨在以柯尔莫戈罗夫的概率测度公理化为起点，实现对因果关系的公理化，并成功地解决了现有框架的限制。 |
| [^122] | [Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model.](http://arxiv.org/abs/2305.16340) | 本文提出了一种分段循环Transformer（SRformer）来减少计算/内存成本，并使用RAF层处理跨段的信息，从而提高序列处理能力。 |
| [^123] | [Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models.](http://arxiv.org/abs/2305.15074) | 这项研究提出了JEEBench，一个更具挑战性的基准数据集，用于评估大型语言模型的问题解决能力。通过评估各种模型，结果显示目前最好的模型在解决问题时存在代数操作错误、抽象概念转化不准确和难以检索相关概念等问题。 |
| [^124] | [FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation.](http://arxiv.org/abs/2305.14251) | 本文介绍了一种称为FACTSCORE的评估方法，它通过将生成的内容分解为原子事实，并计算被可靠知识源支持的比例来评估大型语言模型生成的长文本的事实准确性。通过广泛的人工评估，我们发现商业语言模型中仅有58%的ChatGPT传记达到了高水平的事实准确性。此外，我们还引入了一种自动化模型，利用检索和强语言模型估计FACTSCORE，误差率低于2%。 |
| [^125] | [Editing Large Language Models: Problems, Methods, and Opportunities.](http://arxiv.org/abs/2305.13172) | 本文深入探讨了编辑大型语言模型的问题、方法和机会，提供了任务定义和挑战的概述、先进方法的实证分析，以及构建了新的基准数据集。这些结果有助于改进LLMs的编辑技术，提高其效果和可行性。 |
| [^126] | [Clifford Group Equivariant Neural Networks.](http://arxiv.org/abs/2305.11141) | 我们引入了Clifford群等变神经网络，它可以构建O(n)和E(n)等变模型。该方法通过调整Clifford群的定义以及保持向量空间和乘法结构的作用来实现多个有利属性。 |
| [^127] | [Equivariant Few-Shot Learning from Pretrained Models.](http://arxiv.org/abs/2305.09900) | 本文提出了一种基于预训练模型的$\lambda$-\textit{equitune}方法，它使用\textit{重要性权重}$\lambda$对特征进行平均，可以显著提高等变小样本学习的表现。 |
| [^128] | [Knowledge Rumination for Pre-trained Language Models.](http://arxiv.org/abs/2305.08732) | 本文提出了一种名为知识反思的新范式，旨在帮助预训练语言模型利用已经编码在其预训练参数中的相关潜在知识，而不需要从外部语料库中检索。这种方法通过在模型中添加提示，并将相关知识注入模型进行整合，取得了在常识推理任务和GLUE基准上的实验结果。 |
| [^129] | [Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks.](http://arxiv.org/abs/2305.05862) | 本研究探讨了ChatGPT和GPT-4在金融文本分析任务中的潜力，结果显示它们在数值推理上表现出色但在需要领域特定知识的任务上表现不佳。 |
| [^130] | [TidyBot: Personalized Robot Assistance with Large Language Models.](http://arxiv.org/abs/2305.05658) | 本文研究了使用机器人进行家庭清扫的个性化问题。通过使用大型语言模型少样本摘要能力，机器人可以学习用户的偏好并将其推广到未来的场景中，从而实现快速适应。 |
| [^131] | [An Empirical Study of Multimodal Model Merging.](http://arxiv.org/abs/2304.14933) | 本研究通过融合在不同模态上训练的transformer进行多模态模型融合，并提出一种参数有效的模态不可知架构，形成有效的训练配方。 |
| [^132] | [Fundamental Limitations of Alignment in Large Language Models.](http://arxiv.org/abs/2304.11082) | 本文通过提出一种理论方法——行为期望边界（BEB），展示了大型语言模型中对齐的基本限制，并证明任何对齐过程都无法根除不希望的行为，这对于防止恶意攻击是不安全的。 |
| [^133] | [YOLO-Drone:Airborne real-time detection of dense small objects from high-altitude perspective.](http://arxiv.org/abs/2304.06925) | YOLO-Drone是一种能够高效检测小尺度物体的实时物体检测算法，并在无人机平台上得到了应用，以广义交集联盟(GIOU)作为损失函数，取得了最佳的检测效果。 |
| [^134] | [Querying Large Language Models with SQL.](http://arxiv.org/abs/2304.00472) | 该论文介绍了使用SQL查询大型语言模型的方法，通过利用预训练的LLMs中的信息，可以从非结构化文本中提取数据并进行查询。通过Galois原型实现了查询LLMs的新物理运算符，并取得了令人鼓舞的结果。 |
| [^135] | [PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models.](http://arxiv.org/abs/2303.17546) | 本论文提出了一种采用结构和外观配对扩散模型进行对象级图像编辑的方法，使用户能够精细控制图像中的不同对象属性，同时自动传播注入的外观到具有相似结构的对象。 |
| [^136] | [GOOD: General Optimization-based Fusion for 3D Object Detection via LiDAR-Camera Object Candidates.](http://arxiv.org/abs/2303.09800) | GOOD是一个基于优化的融合框架，可实现高精度、高鲁棒性的3D物体检测，适用于任意2D和3D检测器的组合，并且无需额外模型训练。 |
| [^137] | [Construction of Knowledge Graphs: State and Challenges.](http://arxiv.org/abs/2302.11509) | 知识图谱构建面临着增量更新和步骤之间相互作用的挑战。本文讨论了主要图模型和构建流程需求，对构建高质量知识图谱的必要步骤进行了概述，并评估了现有知识图谱构建的研究现状。 |
| [^138] | [Generalization-based similarity.](http://arxiv.org/abs/2302.10096) | 本文从头构建了一个基于抽象代数和定性概念的相似性概念，并将其通过模型论类型自然地嵌入到一阶逻辑中。 |
| [^139] | [Enhancing Face Recognition with Latent Space Data Augmentation and Facial Posture Reconstruction.](http://arxiv.org/abs/2301.11986) | 本文提出了一种名为面部表示增强（FRA）的方法，该方法通过操纵任何面部表示学习算法生成的面部嵌入，创造出具有改变姿势的新嵌入，表示相同身份和面部情感。大量实验证明了该方法的有效性和提供无噪声、全新的面部表示的能力。 |
| [^140] | [Measuring uncertainty in human visual segmentation.](http://arxiv.org/abs/2301.07807) | 本论文提出了一种新方法，通过测量基于像素的相同-不同判断并基于模型进行重建，来测量人类视觉分割的不确定性，并表明图像的不确定性影响了人类的可变性的测量，并影响了参与者的决策。 |
| [^141] | [Social Mechanism Design: Making Maximally Acceptable Decisions.](http://arxiv.org/abs/2211.08501) | 该论文提出了一个新颖的集体决策模型，考虑了代理人对决策结果和过程的关注，通过开发聚合偏好的机制来最大化决策的可接受性，并在特定情景中进行了实证研究。 |
| [^142] | [Augmentative Topology Agents For Open-Ended Learning.](http://arxiv.org/abs/2210.11442) | 本研究针对开放式学习问题，引入了一种同时演化智能体和更具挑战性环境的方法。通过允许智能体的控制器变得更加复杂，我们发现可以提高泛化能力。实证结果表明，这种方法可以产生通用智能体，能够解决更多环境的问题。 |
| [^143] | [Proportional algebras.](http://arxiv.org/abs/2210.01751) | 本文引入了直比代数，探讨了保持比拟比例的函数的数学性质，将其与直比同态、同余和直比函子联系起来，为模拟比例和模拟推理的数学理论提供了进一步理解。 |
| [^144] | [Game-theoretic Objective Space Planning.](http://arxiv.org/abs/2209.07758) | 本文提出了一种博弈论目标空间规划方法，同时生成竞争策略和执行连续运动规划，通过抽象实现智能体动作离散化，提供清晰的意图，以及使用对策回归最小化的规划策略。 |
| [^145] | [Diffusion Models: A Comprehensive Survey of Methods and Applications.](http://arxiv.org/abs/2209.00796) | 本调研综合总结了扩散模型的方法和应用研究，包括高效采样、似然估计与特殊结构数据处理，介绍了扩散模型与其他生成模型相结合的潜力并回顾了其在不同领域的广泛应用，为进一步研究提出了可能的研究方向。 |
| [^146] | [On the Trade-Off between Actionable Explanations and the Right to be Forgotten.](http://arxiv.org/abs/2208.14137) | 本文研究了在数据删除请求的背景下，可行解释与被遗忘权之间的权衡问题。通过理论和实证分析，发现当少量数据删除请求触发预测模型的更新时，目前流行的最先进算法生成的可行解释可能失效。 |
| [^147] | [Federated Learning of Large Models at the Edge via Principal Sub-Model Training.](http://arxiv.org/abs/2208.13141) | 本文提出了一种解决资源受限边缘设备下大型模型联邦学习的方法，通过主体子模型训练，在不违反隐私承诺的前提下，使弱但重要的客户端能够参与到协作训练中。 |
| [^148] | [Analogical proportions in monounary algebras.](http://arxiv.org/abs/2208.06829) | 本文研究了单一一元代数中的类比比例，在由自然数和后继函数组成的无限单一一元代数中，类比比例关系可以通过差别比例进行描述。 |
| [^149] | [Knowledgeable Salient Span Mask for Enhancing Language Models as Knowledge Base.](http://arxiv.org/abs/2204.07994) | 本文提出了一个名为知识显著跨度掩码的方法，通过全自监督学习帮助语言模型从非结构化文本中获取更多的知识。实验证明了该方法在知识密集型任务中的有效性。 |
| [^150] | [Performance of Deep Learning models with transfer learning for multiple-step-ahead forecasts in monthly time series.](http://arxiv.org/abs/2203.11196) | 本研究比较了具有迁移学习和无迁移学习的深度学习模型以及其他传统方法在月度时间序列预测中的性能，结果显示基于TCN、LSTM和CNN的深度学习模型加上迁移学习的性能超过了其他传统方法，并且直接在目标时间序列上训练的TCN和LSTM在某些预测时段的性能相当或更好。 |
| [^151] | [Rapid Exploration for Open-World Navigation with Latent Goal Models.](http://arxiv.org/abs/2104.05859) | 本论文描述了一个机器人学习系统，可用于自主探索和导航不同的开放世界环境。该系统通过学习得到一个潜在变量模型和图像的拓扑记忆，使用信息瓶颈约束策略，能够在不到20分钟内通过视觉目标表示在开放世界中探索并发现目标。 |
| [^152] | [Sequential composition of propositional logic programs.](http://arxiv.org/abs/2009.05774) | 本文介绍了命题逻辑程序的顺序组合和分解方法，并提出了一种桥接命题逻辑程序的语法与语义的数学方法。 |

# 详细

[^1]: 在协变量转移下，仅凭少量测试样本改善公平性和准确性的权衡

    Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift. (arXiv:2310.07535v1 [cs.LG])

    [http://arxiv.org/abs/2310.07535](http://arxiv.org/abs/2310.07535)

    在协变量转移下，我们提出了一种新的损失函数和表示匹配损失来优化模型的准确性和公平性，通过实验证明在公平性和准确性权衡方面优于其他基线算法，并且提出了一种未经研究的非对称协变量转移设置。

    

    测试数据中的协变量转移会显著降低模型的准确性和公平性表现。在这种情况下，确保不同敏感群体之间的公平性非常重要，因为这涉及到诸如刑事司法等社会影响。我们在无监督的情况下进行操作，只有一小组无标签的测试样本和一个带标签的训练集可用。为了解决这个问题，我们提出了三个贡献。第一个贡献是基于新型复合加权熵的目标函数，用于预测准确性，并通过表示匹配损失来优化公平性。我们通过实验证明，使用我们的损失函数进行优化，在几个标准数据集上在公平性-准确性权衡方面优于许多最先进的基线算法。我们的第二个贡献是一个新的设置，我们称之为非对称协变量转移，在我们所知的范围内尚未研究过非对称协变量转移。

    Covariate shift in the test data can significantly downgrade both the accuracy and the fairness performance of the model. Ensuring fairness across different sensitive groups in such settings is of paramount importance due to societal implications like criminal justice. We operate under the unsupervised regime where only a small set of unlabeled test samples along with a labeled training set is available. Towards this problem, we make three contributions. First is a novel composite weighted entropy based objective for prediction accuracy which is optimized along with a representation matching loss for fairness. We experimentally verify that optimizing with our loss formulation outperforms a number of state-of-the-art baselines in the pareto sense with respect to the fairness-accuracy tradeoff on several standard datasets. Our second contribution is a new setting we term Asymmetric Covariate Shift that, to the best of our knowledge, has not been studied before. Asymmetric covariate shift
    
[^2]: XAI方法的以人为中心的评估

    Human-Centered Evaluation of XAI Methods. (arXiv:2310.07534v1 [cs.AI])

    [http://arxiv.org/abs/2310.07534](http://arxiv.org/abs/2310.07534)

    在人工智能领域中，解释深度学习黑盒子的决策过程是一个关键挑战。本研究以用户为中心，客观评估了三种领先的解释方法的可解释性，并发现它们都提供了可解释的结果。

    

    在不断发展的人工智能领域中，一个关键的挑战是解析深度学习中所谓的“黑盒子”中的决策过程。近年来，出现了许多方法，专门用于解释各种任务的决策。特别是在图像分类等任务中，这些方法通常会识别并强调对分类器预测影响最大的关键像素。有趣的是，这种方法与人类行为相似：当我们被要求解释分类图像的理由时，我们通常会指出最显著的特征或方面。利用这种类似性，我们的研究进行了以用户为中心的研究。我们试图客观地评估三种领先的解释方法的可解释性：（1）典型局部网络、（2）遮挡和（3）层次相关传播。有趣的是，我们的结果表明，尽管这些方法所突出的区域可能差异很大，但它们都提供了可解释的结果。

    In the ever-evolving field of Artificial Intelligence, a critical challenge has been to decipher the decision-making processes within the so-called "black boxes" in deep learning. Over recent years, a plethora of methods have emerged, dedicated to explaining decisions across diverse tasks. Particularly in tasks like image classification, these methods typically identify and emphasize the pivotal pixels that most influence a classifier's prediction. Interestingly, this approach mirrors human behavior: when asked to explain our rationale for classifying an image, we often point to the most salient features or aspects. Capitalizing on this parallel, our research embarked on a user-centric study. We sought to objectively measure the interpretability of three leading explanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3) Layer-wise Relevance Propagation. Intriguingly, our results highlight that while the regions spotlighted by these methods can vary widely, they all offe
    
[^3]: 跨层级计算的能量估计：从设备到机器学习在自然语言处理、科学计算和加密货币挖掘中的大规模应用

    Energy Estimates Across Layers of Computing: From Devices to Large-Scale Applications in Machine Learning for Natural Language Processing, Scientific Computing, and Cryptocurrency Mining. (arXiv:2310.07516v1 [cs.CY])

    [http://arxiv.org/abs/2310.07516](http://arxiv.org/abs/2310.07516)

    该论文估计和分析了从设备到算法的计算层级中的能量消耗。在三个大规模计算应用中，能量消耗既存在于指令层面，也存在于模拟层面。此外，对使用不同架构的AI/ML加速器进行了能量效率比较。与热力学和生物学界限相比，计算系统的能量消耗相差27-36个数量级。

    

    对从设备到算法的计算层级中的能量消耗进行了估计和分析。在之前的分析基础上[3]，估计了包括人工智能（AI）/机器学习在自然语言处理、科学模拟和加密货币挖掘等三个大规模计算应用中所需的能量。与比特级切换不同，在应用的指令和模拟层面上都消耗了更多的能量。此外，基于AI/ML加速器的分析表明，使用较旧的半导体技术节点的架构与使用较新技术的不同架构具有可比较的能量效率。进一步将计算系统的能量与热力学和生物学界限进行比较，表明大约有27-36个数量级的差距。

    Estimates of energy usage in layers of computing from devices to algorithms have been determined and analyzed. Building on the previous analysis [3], energy needed from single devices and systems including three large-scale computing applications such as Artificial Intelligence (AI)/Machine Learning for Natural Language Processing, Scientific Simulations, and Cryptocurrency Mining have been estimated. In contrast to the bit-level switching, in which transistors achieved energy efficiency due to geometrical scaling, higher energy is expended both at the at the instructions and simulations levels of an application. Additionally, the analysis based on AI/ML Accelerators indicate that changes in architectures using an older semiconductor technology node have comparable energy efficiency with a different architecture using a newer technology. Further comparisons of the energy in computing systems with the thermodynamic and biological limits, indicate that there is a 27-36 orders of magnitud
    
[^4]: 基于样本驱动的联邦学习用于能效和实时IoT感知

    Sample-Driven Federated Learning for Energy-Efficient and Real-Time IoT Sensing. (arXiv:2310.07497v1 [cs.LG])

    [http://arxiv.org/abs/2310.07497](http://arxiv.org/abs/2310.07497)

    本文提出了一种针对具有实时感知能力的IoT网络设计的基于样本驱动的联邦学习方法，通过控制采样过程来减轻过拟合问题，提高整体准确性，并解决能效问题。

    

    在联邦学习系统领域，最近的前沿方法在收敛分析中严重依赖于理想条件。特别地，这些方法假设IoT设备上的训练数据具有与全局数据分布相似的属性。然而，在实时感知联邦学习系统中，这种方法无法捕捉到数据特征的全面范围。为了克服这个限制，我们提出了一种针对具有实时感知能力的IoT网络设计的新方法。我们的方法考虑了由用户数据采样过程引起的泛化差距。通过有效地控制这个采样过程，我们可以减轻过拟合问题，并提高整体准确性。特别地，我们首先制定了一个优化问题，利用采样过程同时减少过拟合和最大化准确性。为了达到这个目标，我们的替代优化问题擅长处理能效问题。

    In the domain of Federated Learning (FL) systems, recent cutting-edge methods heavily rely on ideal conditions convergence analysis. Specifically, these approaches assume that the training datasets on IoT devices possess similar attributes to the global data distribution. However, this approach fails to capture the full spectrum of data characteristics in real-time sensing FL systems. In order to overcome this limitation, we suggest a new approach system specifically designed for IoT networks with real-time sensing capabilities. Our approach takes into account the generalization gap due to the user's data sampling process. By effectively controlling this sampling process, we can mitigate the overfitting issue and improve overall accuracy. In particular, We first formulate an optimization problem that harnesses the sampling process to concurrently reduce overfitting while maximizing accuracy. In pursuit of this objective, our surrogate optimization problem is adept at handling energy ef
    
[^5]: 多样性对于应变的重要性：学习多样行为以实现高效适应和迁移

    Diversity for Contingency: Learning Diverse Behaviors for Efficient Adaptation and Transfer. (arXiv:2310.07493v1 [cs.AI])

    [http://arxiv.org/abs/2310.07493](http://arxiv.org/abs/2310.07493)

    本研究提出一种简单的方法，在强化学习中通过学习多样的行为来实现适应和迁移。通过迭代学习一组策略，并利用约束来发现给定任务的所有可能解决方案，我们的方法能够在迁移设置中表现良好，快速适应任务或转换动力学的变化。

    

    发现给定任务的所有有用解决方案对于可迁移的强化学习代理至关重要，以应对任务或转换动力学的变化。传统的强化学习算法只关注在当前任务和动力学下找到最优策略，而不考虑这一点。我们提出了一种简单的方法来发现给定任务的所有可能解决方案，以获得在迁移设置中表现良好并快速适应任务或转换动力学变化的代理。我们的方法通过迭代学习一组策略，其中每个后续策略被限制为在所有先前策略下都不太可能存在的解决方案。与先前方法不同，我们的方法不需要学习额外的模型进行新颖性检测，并通过直接将约束整合到动作选择和优化步骤中避免任务和新颖性奖励信号的平衡。

    Discovering all useful solutions for a given task is crucial for transferable RL agents, to account for changes in the task or transition dynamics. This is not considered by classical RL algorithms that are only concerned with finding the optimal policy, given the current task and dynamics. We propose a simple method for discovering all possible solutions of a given task, to obtain an agent that performs well in the transfer setting and adapts quickly to changes in the task or transition dynamics. Our method iteratively learns a set of policies, while each subsequent policy is constrained to yield a solution that is unlikely under all previous policies. Unlike prior methods, our approach does not require learning additional models for novelty detection and avoids balancing task and novelty reward signals, by directly incorporating the constraint into the action selection and optimization steps.
    
[^6]: 用条件扩散模型提升对深度神经网络的黑盒攻击

    Boosting Black-box Attack to Deep Neural Networks with Conditional Diffusion Models. (arXiv:2310.07492v1 [cs.CV])

    [http://arxiv.org/abs/2310.07492](http://arxiv.org/abs/2310.07492)

    本文提出了一种新型黑盒攻击策略，通过条件转换生成可接受的对抗样本，以提高在查询受限情况下生成对抗样本的查询效率。

    

    现有的黑盒攻击已经展现出在欺骗深度学习模型中创建对抗样本（AE）方面的潜力。其中大部分攻击需要处理庞大的优化空间并需要大量的查询，因此在实际情况下具有限制的实际影响。在本文中，我们提出了一种新颖的黑盒攻击策略，条件扩散模型攻击（CDMA），以提高在查询受限情况下生成AE的查询效率。CDMA的关键洞察是将AE合成的任务形式化为分布转换问题，即良性示例及其对应的AE可以被视为来自两个不同的分布，并且可以通过特定的转换器相互转换。与传统的“查询和优化”方法不同，我们使用上述数据转换器直接进行条件转换来生成符合要求的AE，这可以显著减少所需的查询数量。

    Existing black-box attacks have demonstrated promising potential in creating adversarial examples (AE) to deceive deep learning models. Most of these attacks need to handle a vast optimization space and require a large number of queries, hence exhibiting limited practical impacts in real-world scenarios. In this paper, we propose a novel black-box attack strategy, Conditional Diffusion Model Attack (CDMA), to improve the query efficiency of generating AEs under query-limited situations. The key insight of CDMA is to formulate the task of AE synthesis as a distribution transformation problem, i.e., benign examples and their corresponding AEs can be regarded as coming from two distinctive distributions and can transform from each other with a particular converter. Unlike the conventional \textit{query-and-optimization} approach, we generate eligible AEs with direct conditional transform using the aforementioned data converter, which can significantly reduce the number of queries needed. 
    
[^7]: KwaiYiiMath: 技术报告

    KwaiYiiMath: Technical Report. (arXiv:2310.07488v1 [cs.CL])

    [http://arxiv.org/abs/2310.07488](http://arxiv.org/abs/2310.07488)

    KwaiYiiMath是一个用于增强数学推理能力的大型语言模型，通过应用监督微调和人类反馈强化学习，在英语和中文数学任务上取得了最先进的性能，并且能够正确解决生成的问题过程。

    

    近年来，大型语言模型（LLMs）在处理各种自然语言处理（NLP）下游任务方面展示出了显著的能力，甚至可以处理需要多步推理的数学任务。在本报告中，我们介绍了KwaiYiiMath，通过应用监督微调（SFT）和人类反馈强化学习（RLHF），增强了KwaiYiiBase1的数学推理能力，包括英语和中文的数学任务。同时，我们还构建了一个小规模的中小学数学测试集（命名为KMath），包含188个例子，用来评估模型生成的问题解决过程的正确性。实证研究表明，与类似规模的模型相比，KwaiYiiMath在GSM8k、CMath和KMath上均能取得最先进的性能（SOTA）。

    Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.
    
[^8]: 多模态图学习用于生成任务

    Multimodal Graph Learning for Generative Tasks. (arXiv:2310.07478v1 [cs.AI])

    [http://arxiv.org/abs/2310.07478](http://arxiv.org/abs/2310.07478)

    多模态图学习是一种通用且系统的框架，可以捕捉多模态数据之间的复杂关系，并在生成任务中取得了良好的效果。

    

    多模态学习结合多种数据模态，扩大了模型可以利用的数据类型和复杂度，例如从纯文本到图像-字幕对。大多数多模态学习算法专注于对两种模态的简单一对一数据进行建模，如图像-字幕对或音频-文本对。然而，在大多数实际场景中，不同模态的实体以更复杂和多样化的方式相互作用，超越了一对一映射。我们提出将这些复杂关系表示为图形，允许我们捕捉任意数量的模态数据，并捕捉模态之间的复杂关系，这些关系可以从一个样本到另一个样本灵活变化。为实现这一目标，我们提出了多模态图学习（MMGL），这是一个通用且系统的框架，用于捕获具有关系结构的多个多模态邻居的信息。特别是，我们关注基于预训练模型的MMGL用于生成任务。

    Multimodal learning combines multiple data modalities, broadening the types and complexity of data our models can utilize: for example, from plain text to image-caption pairs. Most multimodal learning algorithms focus on modeling simple one-to-one pairs of data from two modalities, such as image-caption pairs, or audio-text pairs. However, in most real-world settings, entities of different modalities interact with each other in more complex and multifaceted ways, going beyond one-to-one mappings. We propose to represent these complex relationships as graphs, allowing us to capture data with any number of modalities, and with complex relationships between modalities that can flexibly vary from one sample to another. Toward this goal, we propose Multimodal Graph Learning (MMGL), a general and systematic framework for capturing information from multiple multimodal neighbors with relational structures among them. In particular, we focus on MMGL for generative tasks, building upon pretraine
    
[^9]: 一个共创AI系统的本体论

    An Ontology of Co-Creative AI Systems. (arXiv:2310.07472v1 [cs.AI])

    [http://arxiv.org/abs/2310.07472](http://arxiv.org/abs/2310.07472)

    这篇论文提出了一个共创AI系统的本体论，通过增加计算机作为分包商、计算机作为批评者和计算机作为队友等新类别，来扩展对创造力支持工具的本体论。

    

    “共创性”一词被用来描述人工智能与人类共同参与创作的各种组合。为了帮助消除研究努力的混淆，我们提出了一个共创系统的本体论，关注人类与AI系统之间的责任划分和信息交流。我们在Lubart对创造力支持工具的本体论上进行了扩展，增加了三个强调人工智能的新类别：计算机作为分包商，计算机作为批评者和计算机作为队友，其中一些还有细分类别。

    The term co-creativity has been used to describe a wide variety of human-AI assemblages in which human and AI are both involved in a creative endeavor. In order to assist with disambiguating research efforts, we present an ontology of co-creative systems, focusing on how responsibilities are divided between human and AI system and the information exchanged between them. We extend Lubart's original ontology of creativity support tools with three new categories emphasizing artificial intelligence: computer-as-subcontractor, computer-as-critic, and computer-as-teammate, some of which have sub-categorizations.
    
[^10]: 区块链联邦学习中分散化的影响：评估模型陈旧和不一致的影响

    The Implications of Decentralization in Blockchained Federated Learning: Evaluating the Impact of Model Staleness and Inconsistencies. (arXiv:2310.07471v1 [cs.NI])

    [http://arxiv.org/abs/2310.07471](http://arxiv.org/abs/2310.07471)

    本研究评估了将联邦学习的协调外包给区块链等分散网络的实际影响，重点关注了由区块链的运作方式支持的模型陈旧和不一致对异步FL训练过程的影响。

    

    区块链承诺通过提供进一步的分散化、安全性、不可变性和信任来增强联邦学习等分布式机器学习方法，这些特性对于实现下一代应用中的协作智能至关重要。然而，点对点（P2P）区块链节点的内在分散化操作使得联邦学习处于未知的状态，FL轮次和全局模型的概念变得无意义，因为设备的同步丢失了中心协调服务器的形象。在本文中，我们研究将FL的协调外包给区块链等民主网络的实际影响。具体而言，我们关注由区块链的运作方式支持的模型陈旧和不一致对异步进行的FL训练过程的影响。通过模拟，我们评估了基于区块链的FL在著名的CIFAR-10数据集上的运作情况，并着重研究。

    Blockchain promises to enhance distributed machine learning (ML) approaches such as federated learning (FL) by providing further decentralization, security, immutability, and trust, which are key properties for enabling collaborative intelligence in next-generation applications. Nonetheless, the intrinsic decentralized operation of peer-to-peer (P2P) blockchain nodes leads to an uncharted setting for FL, whereby the concepts of FL round and global model become meaningless, as devices' synchronization is lost without the figure of a central orchestrating server. In this paper, we study the practical implications of outsourcing the orchestration of FL to a democratic network such as in a blockchain. In particular, we focus on the effects that model staleness and inconsistencies, endorsed by blockchains' modus operandi, have on the training procedure held by FL devices asynchronously. Using simulation, we evaluate the blockchained FL operation on the well-known CIFAR-10 dataset and focus 
    
[^11]: AI/ML在IEEE 802.11企业网络中的负载预测

    AI/ML-based Load Prediction in IEEE 802.11 Enterprise Networks. (arXiv:2310.07467v1 [cs.NI])

    [http://arxiv.org/abs/2310.07467](http://arxiv.org/abs/2310.07467)

    该论文研究了在实际企业Wi-Fi网络中采用基于AI/ML的负载预测的适用性和可行性，并发现受硬件限制的AI/ML模型可以在平均误差小于20%和85th百分位误差小于3%的情况下预测网络负载。

    

    企业Wi-Fi网络可以通过人工智能和机器学习（AI/ML）获得巨大的好处，因为它们具有完善的管理和运营能力。同时，基于AI/ML的流量/负载预测是改善Wi-Fi体验最有吸引力的数据驱动解决方案之一，无论是通过实现自主运行还是通过提供预测的网络利用率来提升故障排除能力。在本文中，我们研究了在实际企业Wi-Fi网络中采用基于AI/ML的负载预测的适用性和可行性。虽然利用AI/ML解决方案有可能在能源效率、性能和可靠性方面优化Wi-Fi网络，但其有效采用受到数据可用性和质量、计算能力和能量消耗等因素的限制。我们的结果显示，受硬件限制的AI/ML模型可以在平均误差小于20%和85th百分位误差小于3%的情况下预测网络负载。

    Enterprise Wi-Fi networks can greatly benefit from Artificial Intelligence and Machine Learning (AI/ML) thanks to their well-developed management and operation capabilities. At the same time, AI/ML-based traffic/load prediction is one of the most appealing data-driven solutions to improve the Wi-Fi experience, either through the enablement of autonomous operation or by boosting troubleshooting with forecasted network utilization. In this paper, we study the suitability and feasibility of adopting AI/ML-based load prediction in practical enterprise Wi-Fi networks. While leveraging AI/ML solutions can potentially contribute to optimizing Wi-Fi networks in terms of energy efficiency, performance, and reliability, their effective adoption is constrained to aspects like data availability and quality, computational capabilities, and energy consumption. Our results show that hardware-constrained AI/ML models can potentially predict network load with less than 20% average error and 3% 85th-per
    
[^12]: 大规模地质碳和能源储存的高效机器学习替代品

    Efficient machine-learning surrogates for large-scale geological carbon and energy storage. (arXiv:2310.07461v1 [cs.CE])

    [http://arxiv.org/abs/2310.07461](http://arxiv.org/abs/2310.07461)

    本研究提出了一种专门的机器学习（ML）模型，通过域分解和拓扑嵌入器降低训练成本，提高机器学习在大规模地质储存应用中的效率。

    

    地质碳和能源储存对于实现净零碳排放和应对气候变化至关重要。然而，由于地质因素和操作限制，它们面临不确定性，可能引发地震事件或地下水污染。为了克服这些挑战，我们提出了一种专门的机器学习（ML）模型，以高效地管理广泛的储层模型。虽然机器学习方法在地质碳储存方面具有潜力，但大规模分析所需的计算资源是一个障碍。我们开发了一种方法，通过域分解和拓扑嵌入器来降低深度神经算子模型的训练成本，以链接时空点，从而实现在模型领域内对未经训练的数据进行精确预测，提高机器学习在大规模地质储存应用中的效率。

    Geological carbon and energy storage are pivotal for achieving net-zero carbon emissions and addressing climate change. However, they face uncertainties due to geological factors and operational limitations, resulting in possibilities of induced seismic events or groundwater contamination. To overcome these challenges, we propose a specialized machine-learning (ML) model to manage extensive reservoir models efficiently.  While ML approaches hold promise for geological carbon storage, the substantial computational resources required for large-scale analysis are the obstacle. We've developed a method to reduce the training cost for deep neural operator models, using domain decomposition and a topology embedder to link spatio-temporal points. This approach allows accurate predictions within the model's domain, even for untrained data, enhancing ML efficiency for large-scale geological storage applications.
    
[^13]: HealthWalk：通过基于传感器的滚轮行走者辅助促进健康与移动性

    HealthWalk: Promoting Health and Mobility through Sensor-Based Rollator Walker Assistance. (arXiv:2310.07434v1 [cs.RO])

    [http://arxiv.org/abs/2310.07434](http://arxiv.org/abs/2310.07434)

    HealthWalk通过集成传感器到滚轮行走者设计中，可以解决滚轮行走者用户姿势不好导致的健康问题，并实现其他有趣的用例。

    

    滚轮行走者能够帮助身体受限制的人提高移动能力，并给他们信心和独立性，使他们能更长时间地参与社会。然而，滚轮行走者用户往往姿势不好，导致进一步的健康问题，最坏的情况是摔倒。将传感器集成到滚轮行走者设计中可以解决这个问题，并且可以实现其他一些有趣的用例。本文简要介绍了现有系统以及该领域的当前研究方向和挑战。我们还介绍了我们早期的HealthWalk滚轮行走者原型，用于与老年人、风湿病、多发性硬化症和帕金森病患者以及视力障碍者进行数据收集。

    Rollator walkers allow people with physical limitations to increase their mobility and give them the confidence and independence to participate in society for longer. However, rollator walker users often have poor posture, leading to further health problems and, in the worst case, falls. Integrating sensors into rollator walker designs can help to address this problem and results in a platform that allows several other interesting use cases. This paper briefly overviews existing systems and the current research directions and challenges in this field. We also present our early HealthWalk rollator walker prototype for data collection with older people, rheumatism, multiple sclerosis and Parkinson patients, and individuals with visual impairments.
    
[^14]: 通过自动折扣调度从观察中进行模仿学习

    Imitation Learning from Observation with Automatic Discount Scheduling. (arXiv:2310.07433v1 [cs.RO])

    [http://arxiv.org/abs/2310.07433](http://arxiv.org/abs/2310.07433)

    我们提出了一个新颖的观察学习模仿(ILfO)框架，能够解决由于奖励信号分配错误导致代理无法学习初始行为的问题。

    

    人类通常通过观察和模仿来获得新的技能。对于机器人代理，从互联网上可用的大量无标签视频演示数据中进行学习，需要在没有访问其动作的情况下模仿专家，这是一种称为观察学习模仿（ILfO）的挑战。解决ILfO问题的常见方法是将其转化为逆向强化学习问题，利用从代理和专家观察中计算出的代理奖励。然而，我们发现在具有进展依赖性属性的任务中，这样的方法面临重大挑战；在这些任务中，代理需要在掌握后续行为之前先学习专家的前序行为。我们的研究表明，主要原因是分配给后续步骤的奖励信号妨碍了对初始行为的学习。为了解决这个挑战，我们提出了一个新颖的ILfO框架，使代理能够掌握早期行为。

    Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observations (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we present a novel ILfO framework that enables the agent to master earl
    
[^15]: 多概念 T2I-Zero: 仅调整文本嵌入，不做其他改变

    Multi-Concept T2I-Zero: Tweaking Only The Text Embeddings and Nothing Else. (arXiv:2310.07419v1 [cs.CV])

    [http://arxiv.org/abs/2310.07419](http://arxiv.org/abs/2310.07419)

    本研究提出了一种新的方法，通过仅仅调整文本嵌入，而不需重新训练模型，实现了自然的多概念生成。通过解决预训练模型中文本嵌入的局限性，如概念支配和非定位贡献，我们设计了一个廉价的解决方案，提高了多概念生成的性能。

    

    最近的文本到图像扩散模型的进展使得从文本提示生成逼真的图像成为可能。尽管取得了很大进步，现有模型仍然难以自然地生成组合的多概念图像，限制了它们可视化人类想象力的能力。虽然最近的一些研究尝试解决这个问题，但要么引入额外的训练，要么在推理时采用指导。在这项工作中，我们考虑了一个更具雄心的目标：使用预训练的扩散模型实现自然多概念生成，并且几乎没有额外的成本。为了实现这个目标，我们确定了预训练的文本到图像扩散模型中使用的文本嵌入的局限性。具体而言，我们观察到概念支配和非定位贡献严重降低了多概念生成性能。我们进一步设计了一个最小廉价的解决方案，通过调整（而不是重新训练）文本嵌入来解决上述问题，以获得更真实的生成效果。

    Recent advances in text-to-image diffusion models have enabled the photorealistic generation of images from text prompts. Despite the great progress, existing models still struggle to generate compositional multi-concept images naturally, limiting their ability to visualize human imagination. While several recent works have attempted to address this issue, they either introduce additional training or adopt guidance at inference time. In this work, we consider a more ambitious goal: natural multi-concept generation using a pre-trained diffusion model, and with almost no extra cost. To achieve this goal, we identify the limitations in the text embeddings used for the pre-trained text-to-image diffusion models. Specifically, we observe concept dominance and non-localized contribution that severely degrade multi-concept generation performance. We further design a minimal low-cost solution that overcomes the above issues by tweaking (not re-training) the text embeddings for more realistic m
    
[^16]: 重审视视觉强化学习中的可塑性：数据、模块和训练阶段

    Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages. (arXiv:2310.07418v1 [cs.LG])

    [http://arxiv.org/abs/2310.07418](http://arxiv.org/abs/2310.07418)

    本文对视觉强化学习中的可塑性进行了研究，发现数据增强对保持可塑性至关重要，评论者的可塑性损失是高效训练的主要限制因素，并且未及时恢复评论者的可塑性将导致灾难性结果。这为解决高重放比困境提供了新的策略。

    

    可塑性，神经网络随新数据演进的能力，对于高性能和样本高效的视觉强化学习(VRL)至关重要。虽然重置和正则化等方法可能能够缓解可塑性损失，但VRL框架内各种组件对代理的可塑性的影响仍然知之甚少。在这项工作中，我们进行了系统的经验性探索，重点关注了三个主要尚未充分探索的方面，并得出以下有深入见解的结论：(1)数据增强对于保持可塑性至关重要；(2)评论者的可塑性损失是阻碍高效训练的主要瓶颈；(3)在早期阶段没有及时干预以恢复评论者的可塑性，其损失将变得灾难性。这些见解提出了一种应对高重放比（RR）困境的新策略，其中加剧的可塑性损失妨碍了通过增加重放数量带来的样本效率的潜在改进。

    Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. In this work, we conduct a systematic empirical exploration focusing on three primary underexplored facets and derive the following insightful conclusions: (1) data augmentation is essential in maintaining plasticity; (2) the critic's plasticity loss serves as the principal bottleneck impeding efficient training; and (3) without timely intervention to recover critic's plasticity in the early stages, its loss becomes catastrophic. These insights suggest a novel strategy to address the high replay ratio (RR) dilemma, where exacerbated plasticity loss hinders the potential improvements of sample efficiency brought by increased
    
[^17]: 使用神经符号学习方法，知识图谱对齐可以获得什么？

    What can knowledge graph alignment gain with Neuro-Symbolic learning approaches?. (arXiv:2310.07417v1 [cs.AI])

    [http://arxiv.org/abs/2310.07417](http://arxiv.org/abs/2310.07417)

    本文研究了知识图谱对齐的现状与挑战，提出了使用神经符号学习方法的潜力，以实现解释性、可验证性和高质量的对齐结果。

    

    知识图谱是许多数据密集型应用程序的基础，因为它们可以表示数据与其意义和上下文的关联。对齐不同领域和提供者的知识图谱是为了实现更全面和一体化的表示。目前知识图谱对齐算法的一个严重局限是它们无法将逻辑思考和推理与词汇、结构和语义数据学习相结合。深度学习模型在知识图谱对齐方面越来越受欢迎，借鉴了它们在其他任务中的良好表现，但它们在解释性、推理能力和数据效率方面存在局限。混合的神经符号学习模型有望将逻辑和数据视角整合起来，产生可解释的高质量对齐结果，并支持以人为中心的验证方法。本文研究了知识图谱对齐领域的现状，并探讨了神经符号整合的潜力，强调了有前景的研究方向。

    Knowledge Graphs (KG) are the backbone of many data-intensive applications since they can represent data coupled with its meaning and context. Aligning KGs across different domains and providers is necessary to afford a fuller and integrated representation. A severe limitation of current KG alignment (KGA) algorithms is that they fail to articulate logical thinking and reasoning with lexical, structural, and semantic data learning. Deep learning models are increasingly popular for KGA inspired by their good performance in other tasks, but they suffer from limitations in explainability, reasoning, and data efficiency. Hybrid neurosymbolic learning models hold the promise of integrating logical and data perspectives to produce high-quality alignments that are explainable and support validation through human-centric approaches. This paper examines the current state of the art in KGA and explores the potential for neurosymbolic integration, highlighting promising research directions for co
    
[^18]: DASpeech：用于快速高质量语音翻译的有向无环变换器

    DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation. (arXiv:2310.07403v1 [cs.CL])

    [http://arxiv.org/abs/2310.07403](http://arxiv.org/abs/2310.07403)

    DASpeech是一个非自回归的直接语音翻译模型，使用有向无环图（DAG）来实现快速和高质量的语音翻译。

    

    直接语音翻译（S2ST）使用单个模型将一种语言的语音翻译成另一种语言。然而，由于存在语言和声学多样性，目标语音遵循一个复杂的多模态分布，给S2ST模型实现高质量翻译和快速解码速度带来挑战。在本文中，我们提出了DASpeech，这是一个非自回归的直接S2ST模型，实现了快速和高质量的S2ST。为了更好地捕捉目标语音的复杂分布，DASpeech采用了两步解码的架构，先由语言解码器生成目标文本，然后由声学解码器根据语言解码器的隐藏状态生成目标语音。具体而言，我们将DA-Transformer的解码器作为语言解码器，将FastSpeech 2作为声学解码器。DA-Transformer使用有向无环图（DAG）模拟翻译过程。

    Direct speech-to-speech translation (S2ST) translates speech from one language into another using a single model. However, due to the presence of linguistic and acoustic diversity, the target speech follows a complex multimodal distribution, posing challenges to achieving both high-quality translations and fast decoding speeds for S2ST models. In this paper, we propose DASpeech, a non-autoregressive direct S2ST model which realizes both fast and high-quality S2ST. To better capture the complex distribution of the target speech, DASpeech adopts the two-pass architecture to decompose the generation process into two steps, where a linguistic decoder first generates the target text, and an acoustic decoder then generates the target speech based on the hidden states of the linguistic decoder. Specifically, we use the decoder of DA-Transformer as the linguistic decoder, and use FastSpeech 2 as the acoustic decoder. DA-Transformer models translations with a directed acyclic graph (DAG). To co
    
[^19]: NuTime: 大规模时间序列预训练的数值多尺度嵌入

    NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining. (arXiv:2310.07402v1 [cs.LG])

    [http://arxiv.org/abs/2310.07402](http://arxiv.org/abs/2310.07402)

    本研究通过采用Transformer架构和数值多尺度嵌入模块，使时间序列自监督模型能够扩展到大规模数据集，并在大规模数据集上进行预训练。

    

    最近关于时间序列自监督模型的研究显示出学习语义表示的巨大潜力，然而，这些研究仅限于小规模数据集，例如数千个时间序列。本文的关键技术贡献针对时间序列数据的数值特性，使模型能够扩展到大规模数据集，例如百万个时间序列。我们采用Transformer架构，首先将输入划分为非重叠窗口。然后，通过窗口的标准化形状和两个标量值表示每个窗口内的均值和标准差。为了将可能具有任意数值尺度的标量值嵌入到高维向量中，我们提出了一个数值多尺度嵌入模块，枚举所有可能的标量值尺度。该模型使用提出的数值多尺度嵌入在大规模数据集上进行预训练，采用简单的对比损失函数。

    Recent research on time-series self-supervised models shows great promise in learning semantic representations. However, it has been limited to small-scale datasets, e.g., thousands of temporal sequences. In this work, we make key technical contributions that are tailored to the numerical properties of time-series data and allow the model to scale to large datasets, e.g., millions of temporal sequences. We adopt the Transformer architecture by first partitioning the input into non-overlapping windows. Each window is then characterized by its normalized shape and two scalar values denoting the mean and standard deviation within each window. To embed scalar values that may possess arbitrary numerical scales to high-dimensional vectors, we propose a numerically multi-scaled embedding module enumerating all possible scales for the scalar values. The model undergoes pretraining using the proposed numerically multi-scaled embedding with a simple contrastive objective on a large-scale dataset
    
[^20]: 面向目标的个性化主动对话系统：问题形式化与数据集筛选

    Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation. (arXiv:2310.07397v1 [cs.CL])

    [http://arxiv.org/abs/2310.07397](http://arxiv.org/abs/2310.07397)

    这项工作提出了一个自动数据集筛选框架，并构建了一个大规模的个性化面向目标对话数据集，名为TopDial。该数据集质量高，有助于探索个性化目标导向的对话。

    

    面向目标的对话系统旨在主动引导对话朝向预定的目标或达成特定的系统目标，在对话完成过程中考虑个性化。然而，需要高质量的数据集，从零开始构建需要大量人力。为了解决这个问题，我们提出了一个使用角色扮演方法的自动数据集筛选框架。基于这个框架，我们构建了一个大规模的个性化面向目标对话数据集，名为TopDial，包含约18K个多轮对话。实验结果表明，该数据集具有很高的质量，可以用于探索个性化目标导向的对话。

    Target-oriented dialogue systems, designed to proactively steer conversations toward predefined targets or accomplish specific system-side goals, are an exciting area in conversational AI. In this work, by formulating a <dialogue act, topic> pair as the conversation target, we explore a novel problem of personalized target-oriented dialogue by considering personalization during the target accomplishment process. However, there remains an emergent need for high-quality datasets, and building one from scratch requires tremendous human effort. To address this, we propose an automatic dataset curation framework using a role-playing approach. Based on this framework, we construct a large-scale personalized target-oriented dialogue dataset, TopDial, which comprises about 18K multi-turn dialogues. The experimental results show that this dataset is of high quality and could contribute to exploring personalized target-oriented dialogue.
    
[^21]: 学习用户优先的电器调度奖励函数

    Learning a Reward Function for User-Preferred Appliance Scheduling. (arXiv:2310.07389v1 [cs.AI])

    [http://arxiv.org/abs/2310.07389](http://arxiv.org/abs/2310.07389)

    本文介绍了一种基于逆强化学习的模型，通过使用终端用户过去的消费数据，帮助创建用户每日的电器计划，从而鼓励终端用户参与需求响应服务的提供。

    

    居民部门加速发展需求响应服务对于降低电力部门的碳排放至关重要。随着基础设施的进步，鼓励终端用户参与至关重要。终端用户非常重视自己的隐私和控制权，并希望在创建每日电器操作计划时参与到服务设计和决策过程中。此外，除非他们有经济或环境动机，他们通常不会准备牺牲自己的舒适度来帮助平衡电力系统。本文提出了一种基于逆强化学习的模型，可以在不要求用户明确陈述需求和愿望的情况下帮助创建用户每日的电器计划。通过使用他们过去的消费数据，终端消费者将隐式参与这些决策的制定，并因此得到继续参与需求响应服务提供的动力。

    Accelerated development of demand response service provision by the residential sector is crucial for reducing carbon-emissions in the power sector. Along with the infrastructure advancement, encouraging the end users to participate is crucial. End users highly value their privacy and control, and want to be included in the service design and decision-making process when creating the daily appliance operation schedules. Furthermore, unless they are financially or environmentally motivated, they are generally not prepared to sacrifice their comfort to help balance the power system. In this paper, we present an inverse-reinforcement-learning-based model that helps create the end users' daily appliance schedules without asking them to explicitly state their needs and wishes. By using their past consumption data, the end consumers will implicitly participate in the creation of those decisions and will thus be motivated to continue participating in the provision of demand response services.
    
[^22]: 基于联邦学习的组织病理图像分类和脆弱性分析

    Histopathological Image Classification and Vulnerability Analysis using Federated Learning. (arXiv:2310.07380v1 [cs.LG])

    [http://arxiv.org/abs/2310.07380](http://arxiv.org/abs/2310.07380)

    这项研究开发了一种基于联邦学习的隐私保护技术，应用于皮肤癌数据集，发现该模型容易受到数据污染攻击的影响。

    

    医疗保健是机器学习(Machine Learning，ML)的主要应用之一。传统上，ML模型由中央服务器训练，通过汇总来自各个分布式设备的数据来预测新生成数据的结果。这是一个主要关注点，因为模型可以访问敏感的用户信息，引发隐私问题。联邦学习(FL)方法可以帮助解决这个问题：全局模型将其副本发送给所有客户端，这些客户端训练这些副本，并将更新(权重)发送回给全局模型。随着时间的推移，全局模型变得更加准确。训练过程中保护了数据隐私，因为训练是在客户端设备上本地进行的。然而，全局模型容易受到数据污染的影响。我们开发了一种保护隐私的联邦学习技术，应用于一个皮肤癌数据集，并展示了模型容易受到数据污染攻击的情况。十个客户端训练模型，但其中一个故意引入了翻转标签作为攻击。这降低了模型的准确性。

    Healthcare is one of the foremost applications of machine learning (ML). Traditionally, ML models are trained by central servers, which aggregate data from various distributed devices to forecast the results for newly generated data. This is a major concern as models can access sensitive user information, which raises privacy concerns. A federated learning (FL) approach can help address this issue: A global model sends its copy to all clients who train these copies, and the clients send the updates (weights) back to it. Over time, the global model improves and becomes more accurate. Data privacy is protected during training, as it is conducted locally on the clients' devices.  However, the global model is susceptible to data poisoning. We develop a privacy-preserving FL technique for a skin cancer dataset and show that the model is prone to data poisoning attacks. Ten clients train the model, but one of them intentionally introduces flipped labels as an attack. This reduces the accurac
    
[^23]: 因果无监督语义分割

    Causal Unsupervised Semantic Segmentation. (arXiv:2310.07379v1 [cs.CV])

    [http://arxiv.org/abs/2310.07379](http://arxiv.org/abs/2310.07379)

    因果无监督语义分割（CAUSE）是一个利用因果推断的新框架，旨在实现无监督语义分割。该方法通过构建概念聚类表作为中介，并与概念自监督学习建立联系，解决了无监督分割中适当聚类水平的挑战。

    

    无监督语义分割旨在在没有人工标注注释的情况下实现高质量的语义分组。随着自监督预训练的出现，各种框架利用预训练特征训练无监督密集预测的预测头。然而，在这种无监督设置中的一个重要挑战是确定用于分割概念所需的适当聚类水平。为了解决这个问题，我们提出了一个新的框架，称为因果无监督语义分割（CAUSE），它利用了因果推断的见解。具体而言，我们桥接了面向干预的方法（即前门调整），以定义适合无监督预测的两步任务。第一步涉及构建一个概念聚类表作为中介，以离散形式表示不同粒度层次上的可能概念原型。然后，中介与随后的概念自监督学习建立了明确的联系...

    Unsupervised semantic segmentation aims to achieve high-quality semantic grouping without human-labeled annotations. With the advent of self-supervised pre-training, various frameworks utilize the pre-trained features to train prediction heads for unsupervised dense prediction. However, a significant challenge in this unsupervised setup is determining the appropriate level of clustering required for segmenting concepts. To address it, we propose a novel framework, CAusal Unsupervised Semantic sEgmentation (CAUSE), which leverages insights from causal inference. Specifically, we bridge intervention-oriented approach (i.e., frontdoor adjustment) to define suitable two-step tasks for unsupervised prediction. The first step involves constructing a concept clusterbook as a mediator, which represents possible concept prototypes at different levels of granularity in a discretized form. Then, the mediator establishes an explicit link to the subsequent concept-wise self-supervised learning for 
    
[^24]: 使用动态图卷积神经网络的局部几何结构对点云去噪和异常检测

    Point Cloud Denoising and Outlier Detection with Local Geometric Structure by Dynamic Graph CNN. (arXiv:2310.07376v1 [cs.CV])

    [http://arxiv.org/abs/2310.07376](http://arxiv.org/abs/2310.07376)

    本研究提出了一种使用动态图卷积神经网络的局部几何结构的方法，用于点云的去噪和异常检测，实验证明该方法在准确性方面优于传统方法。

    

    社会的数字化快速发展正在朝着实现数字孪生和元宇宙的目标迈进。特别是，点云作为3D空间的一种媒体格式正在引起关注。由于测量误差，点云数据受到噪声和异常值的污染。因此，对点云进行去噪和异常检测是必要的。其中，PointCleanNet是一种有效的点云去噪和异常检测方法。然而，它没有考虑到补丁的局部几何结构。我们通过应用基于动态图卷积神经网络设计的两种类型的图卷积层来解决这个问题。实验结果表明，所提出的方法在衡量异常检测准确性的AUPR和衡量去噪准确性的Chamfer距离方面优于传统方法。

    The digitalization of society is rapidly developing toward the realization of the digital twin and metaverse. In particular, point clouds are attracting attention as a media format for 3D space. Point cloud data is contaminated with noise and outliers due to measurement errors. Therefore, denoising and outlier detection are necessary for point cloud processing. Among them, PointCleanNet is an effective method for point cloud denoising and outlier detection. However, it does not consider the local geometric structure of the patch. We solve this problem by applying two types of graph convolutional layer designed based on the Dynamic Graph CNN. Experimental results show that the proposed methods outperform the conventional method in AUPR, which indicates outlier detection accuracy, and Chamfer Distance, which indicates denoising accuracy.
    
[^25]: 给与取：用于工业物联网网络入侵检测的联邦迁移学习

    Give and Take: Federated Transfer Learning for Industrial IoT Network Intrusion Detection. (arXiv:2310.07354v1 [cs.AI])

    [http://arxiv.org/abs/2310.07354](http://arxiv.org/abs/2310.07354)

    本文提出了一种用于工业物联网网络入侵检测的联邦迁移学习（FTL）方法，通过组合神经网络对IIoT数据进行分割并更新服务器模型，实现了高性能的入侵检测。

    

    互联网物联网（IoT）技术的快速增长已成为当今工业的一部分，形成了工业物联网（IIoT）倡议，工业利用IoT通过数据分析和云计算等新兴解决方案改进通信和连接。不幸的是，IoT的快速使用使其成为网络犯罪分子的有吸引力目标。因此，保护这些系统至关重要。在本文中，我们提出了一种联邦迁移学习（FTL）方法来进行IIoT网络入侵检测。作为研究的一部分，我们还提出了一种组合神经网络作为进行FTL的核心。所提出的技术将IoT数据分割在客户端和服务器设备之间生成相应的模型，客户模型的权重被组合以更新服务器模型。结果显示，FTL的设置在IIoT客户端和服务器之间的迭代中表现出高性能。

    The rapid growth in Internet of Things (IoT) technology has become an integral part of today's industries forming the Industrial IoT (IIoT) initiative, where industries are leveraging IoT to improve communication and connectivity via emerging solutions like data analytics and cloud computing. Unfortunately, the rapid use of IoT has made it an attractive target for cybercriminals. Therefore, protecting these systems is of utmost importance. In this paper, we propose a federated transfer learning (FTL) approach to perform IIoT network intrusion detection. As part of the research, we also propose a combinational neural network as the centerpiece for performing FTL. The proposed technique splits IoT data between the client and server devices to generate corresponding models, and the weights of the client models are combined to update the server model. Results showcase high performance for the FTL setup between iterations on both the IIoT clients and the server. Additionally, the proposed F
    
[^26]: 从时间序列数据和知识图谱中学习语义关联规则

    Semantic Association Rule Learning from Time Series Data and Knowledge Graphs. (arXiv:2310.07348v1 [cs.AI])

    [http://arxiv.org/abs/2310.07348](http://arxiv.org/abs/2310.07348)

    本文提出了一种在数字孪生中利用知识图谱和时间序列数据进行语义关联规则学习的流程，并且引入了新的语义关联规则准则。该方法通过在工业水网络场景中的评估表明，能够学习到更具泛化性的包含语义信息的关联规则，为进一步研究工业应用背景下的语义关联规则学习打下基础。

    

    数字孪生（DT）是物理系统研究中的一个有前景的概念，因为它们具有包括监测和自动推理在内的先进功能。语义技术如知识图谱（KG）最近在DT中被广泛应用，特别是在信息建模方面。基于此，本文提出了一种使用KG和时间序列数据在DT中进行语义关联规则学习的流程。除了这个初始流程，我们还提出了新的语义关联规则准则。该方法在一个工业水网络场景中进行了评估。初步评估结果显示，提出的方法能够学习到包含语义信息的大量关联规则，具有更强的泛化性。本文旨在为进一步研究特别是在工业应用背景下使用语义关联规则学习奠定基础。

    Digital Twins (DT) are a promising concept in cyber-physical systems research due to their advanced features including monitoring and automated reasoning. Semantic technologies such as Knowledge Graphs (KG) are recently being utilized in DTs especially for information modelling. Building on this move, this paper proposes a pipeline for semantic association rule learning in DTs using KGs and time series data. In addition to this initial pipeline, we also propose new semantic association rule criterion. The approach is evaluated on an industrial water network scenario. Initial evaluation shows that the proposed approach is able to learn a high number of association rules with semantic information which are more generalizable. The paper aims to set a foundation for further work on using semantic association rule learning especially in the context of industrial applications.
    
[^27]: 高效预训练的快速ELECTRA

    Fast-ELECTRA for Efficient Pre-training. (arXiv:2310.07347v1 [cs.CL])

    [http://arxiv.org/abs/2310.07347](http://arxiv.org/abs/2310.07347)

    提出了一种快速ELECTRA的方法，利用现有的语言模型作为辅助模型，通过温度缩放平滑主模型的输出分布，达到与最先进的ELECTRA预训练方法相媲美的性能，并显著减少了辅助模型共同训练带来的计算和内存成本。

    

    ELECTRA通过检测序列中被辅助模型替换的标记来预训练语言模型。虽然ELECTRA大大提高了效率，但其潜力受到了辅助模型带来的训练成本的限制。值得注意的是，这个与主模型共同训练的模型仅用于辅助主模型的训练，并且在训练后被丢弃。这导致大量的训练成本被白白浪费。为了缓解这个问题，我们提出了Fast-ELECTRA，它利用现有的语言模型作为辅助模型。为了构建主模型的学习课程，我们通过温度缩放和递减的方法平滑其输出分布。我们的方法与最先进的ELECTRA风格的预训练方法相媲美，同时显著减少了辅助模型共同训练带来的计算和内存成本。我们的方法还降低了模型对训练数据的敏感性。

    ELECTRA pre-trains language models by detecting tokens in a sequence that have been replaced by an auxiliary model. Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model. Notably, this model, which is jointly trained with the main model, only serves to assist the training of the main model and is discarded post-training. This results in a substantial amount of training cost being expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which leverages an existing language model as the auxiliary model. To construct a learning curriculum for the main model, we smooth its output distribution via temperature scaling following a descending schedule. Our approach rivals the performance of state-of-the-art ELECTRA-style pre-training methods, while significantly eliminating the computation and memory cost brought by the joint training of the auxiliary model. Our method also reduces the sensitivity t
    
[^28]: 在拥挤环境中探索社交运动潜空间和人类意识，以实现有效的机器人导航

    Exploring Social Motion Latent Space and Human Awareness for Effective Robot Navigation in Crowded Environments. (arXiv:2310.07335v1 [cs.RO])

    [http://arxiv.org/abs/2310.07335](http://arxiv.org/abs/2310.07335)

    本文提出了一种通过学习社交运动潜空间来生成机器人控制的新方法，实现了在拥挤环境中的有效导航。同时，引入了人类对机器人的意识概念，并证明了这种意识的融入能够改善导航性能。

    

    本文提出了一种新的社交机器人导航方法，通过从社交运动潜空间中学习生成机器人控制，实现了社交导航指标的显著改善，如成功率、导航时间和轨迹长度，同时产生了更平缓的轨迹（减少抖动和角度偏差）和更具预见性的轨迹。通过与基准模型在各种场景中的比较，证明了该方法的优越性。此外，将人类对机器人的意识概念引入社交机器人导航框架中，显示出人类意识的融入能够减少轨迹长度和平滑轨迹，因为人类能够与机器人积极互动。

    This work proposes a novel approach to social robot navigation by learning to generate robot controls from a social motion latent space. By leveraging this social motion latent space, the proposed method achieves significant improvements in social navigation metrics such as success rate, navigation time, and trajectory length while producing smoother (less jerk and angular deviations) and more anticipatory trajectories. The superiority of the proposed method is demonstrated through comparison with baseline models in various scenarios. Additionally, the concept of humans' awareness towards the robot is introduced into the social robot navigation framework, showing that incorporating human awareness leads to shorter and smoother trajectories owing to humans' ability to positively interact with the robot.
    
[^29]: 对中文大语言模型的指导调整的实证研究

    An Empirical Study of Instruction-tuning Large Language Models in Chinese. (arXiv:2310.07328v1 [cs.CL])

    [http://arxiv.org/abs/2310.07328](http://arxiv.org/abs/2310.07328)

    本论文进行了对中文大语言模型的指导调整的实证研究，探索了LLM基础、参数高效的方法和指令数据类型的影响，并研究了其他因素的影响，为定制能更好响应中文指令的LLM提供了有价值的发现。

    

    ChatGPT的成功验证了大语言模型（LLM）在人工通用智能（AGI）中的潜力。随后，LLM的发布引起了开源社区对指导调整的兴趣，这被认为可以加快ChatGPT的复制过程。然而，关于中文的LLM指导调整的研究仍处于早期阶段。因此，本文对中文LLM的指导调整进行了深入的实证研究，这可以作为一本提供有价值发现的烹饪书来有效地定制LLM，使其能够更好地响应中文指令。具体而言，我们系统地探索了LLM基础、参数高效的方法和指令数据类型这三个对指导调整至关重要的因素的影响。此外，我们还进行了实验来研究其他因素的影响，例如思维链数据和人类价值一致性。我们希望这个实证研究能够

    The success of ChatGPT validates the potential of large language models (LLMs) in artificial general intelligence (AGI). Subsequently, the release of LLMs has sparked the open-source community's interest in instruction-tuning, which is deemed to accelerate ChatGPT's replication process. However, research on instruction-tuning LLMs in Chinese, the world's most spoken language, is still in its early stages. Therefore, this paper makes an in-depth empirical study of instruction-tuning LLMs in Chinese, which can serve as a cookbook that provides valuable findings for effectively customizing LLMs that can better respond to Chinese instructions. Specifically, we systematically explore the impact of LLM bases, parameter-efficient methods, instruction data types, which are the three most important elements for instruction-tuning. Besides, we also conduct experiment to study the impact of other factors, e.g., chain-of-thought data and human-value alignment. We hope that this empirical study can
    
[^30]: 直接逻辑属性的对抗性样本：gelu-4l中的内存管理

    An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l. (arXiv:2310.07325v1 [cs.LG])

    [http://arxiv.org/abs/2310.07325](http://arxiv.org/abs/2310.07325)

    在gelu-4l中，我们提供了证据表明内存管理对于transformer模型至关重要，并说明了Direct Logit Attribution技术的不准确之处。

    

    我们提供了一个4层transformer中内存管理的具体证据。具体来说，我们发现在前向传播过程中，模型组件一致地移除前面组件的输出，这是一种清理行为。我们的研究结果表明，解释性技术Direct Logit Attribution提供了误导性结果。我们展示了明确的例子，证明这种技术是不准确的，因为它没有考虑到清理行为。

    We provide concrete evidence for memory management in a 4-layer transformer. Specifically, we identify clean-up behavior, in which model components consistently remove the output of preceeding components during a forward pass. Our findings suggest that the interpretability technique Direct Logit Attribution provides misleading results. We show explicit examples where this technique is inaccurate, as it does not account for clean-up behavior.
    
[^31]: 关于交叉领域数据对德语语言模型的影响

    On the Impact of Cross-Domain Data on German Language Models. (arXiv:2310.07321v1 [cs.CL])

    [http://arxiv.org/abs/2310.07321](http://arxiv.org/abs/2310.07321)

    本研究通过对德语语言模型进行实验，发现将数据多样性置于数据质量之上的交叉领域数据集训练方法，可以显著提高模型的性能，并超过了之前的最先进模型。

    

    传统上，大型语言模型要么在通用网络抓取数据上训练，要么在特定领域的数据上。然而，生成型大型语言模型的最近成功突显了交叉领域数据集的好处。为了考察数据多样性高于质量的重要性，我们提出了一个包含五个领域文本的德语数据集，以及一个旨在包含高质量数据的数据集。通过在这两个数据集上训练参数范围从122M到750M的一系列模型，我们对多个下游任务进行了全面评估。我们的研究结果表明，使用交叉领域数据集训练的模型优于仅使用质量数据训练的模型，在先前最先进结果上提出了高达4.45%的改进。这些模型可在https://huggingface.co/ikim-uk-essen上找到。

    Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over quality, we present a German dataset comprising texts from five domains, along with another dataset aimed at containing high-quality data. Through training a series of models ranging between 122M and 750M parameters on both datasets, we conduct a comprehensive benchmark on multiple downstream tasks. Our findings demonstrate that the models trained on the cross-domain dataset outperform those trained on quality data alone, leading to improvements up to $4.45\%$ over the previous state-of-the-art. The models are available at https://huggingface.co/ikim-uk-essen
    
[^32]: WiGenAI: 通过扩散模型实现无线和生成式人工智能的交织

    WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models. (arXiv:2310.07312v1 [cs.IT])

    [http://arxiv.org/abs/2310.07312](http://arxiv.org/abs/2310.07312)

    WiGenAI通过引入扩散模型，将生成式人工智能应用于无线通信系统中，为研究奠定基础。这篇文章介绍了扩散模型作为生成模型的最新范式，并讨论了它在无线通信系统中的应用。通过两个案例研究展示了扩散模型在开发韧性的AI本地通信系统中的潜力。

    

    创新的基础模型，如GPT-3和稳定的扩散模型，已经在人工智能领域实现了范式转变，向生成式人工智能系统发展。从数据通信和网络的角度来看，人工智能和机器学习算法预计将广泛应用于未来无线通信系统的新一代中，强调了在新兴通信场景中需要新颖的AI本地解决方案。本文介绍生成式人工智能在无线通信系统中的应用，为该领域的研究奠定基础。介绍了扩散型生成模型作为生成模型的最新范式，并讨论了它们在无线通信系统中的应用。还提供了两个案例研究，展示了如何利用扩散模型开发具有韧性的AI本地通信系统。具体而言，我们提出了一种基于扩散模型的生成模型，以展示其在生成模型的应用中的优势。

    Innovative foundation models, such as GPT-3 and stable diffusion models, have made a paradigm shift in the realm of artificial intelligence (AI) towards generative AI-based systems. In unison, from data communication and networking perspective, AI and machine learning (AI/ML) algorithms are envisioned to be pervasively incorporated into the future generations of wireless communications systems, highlighting the need for novel AI-native solutions for the emergent communication scenarios. In this article, we outline the applications of generative AI in wireless communication systems to lay the foundations for research in this field. Diffusion-based generative models, as the new state-of-the-art paradigm of generative models, are introduced, and their applications in wireless communication systems are discussed. Two case studies are also presented to showcase how diffusion models can be exploited for the development of resilient AI-native communication systems. Specifically, we propose de
    
[^33]: RobustGEC: 鲁棒的语法错误修正系统抵抗微小的上下文扰动

    RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation. (arXiv:2310.07299v1 [cs.CL])

    [http://arxiv.org/abs/2310.07299](http://arxiv.org/abs/2310.07299)

    这项研究提出了一个叫做RobustGEC的基准测试，用于评估语法错误修正系统的上下文鲁棒性。研究发现目前最先进的系统仍然无法很好地应对上下文扰动，并提出了一种简单而有效的解决方法。

    

    语法错误修正（GEC）系统在帮助人们日常写作任务中起到了至关重要的作用。然而，用户有时可能会遇到一些初始表现良好但遇到输入微小修改时无法修正错误的GEC系统。为了确保理想的用户体验，可靠的GEC系统应能够在遇到无关上下文扰动时提供一致且准确的建议，我们称之为上下文鲁棒性。在本文中，我们引入了RobustGEC，这是一个用于评估GEC系统上下文鲁棒性的基准测试。RobustGEC包含5,000个GEC案例，每个案例由一句原始的错误-修正句子对和由人工标注者精心设计的五个变体组成。利用RobustGEC，我们揭示了目前最先进的GEC系统在面对上下文扰动时仍然缺乏足够的鲁棒性。此外，我们提出了一种简单而有效的方法来解决这个问题。

    Grammatical Error Correction (GEC) systems play a vital role in assisting people with their daily writing tasks. However, users may sometimes come across a GEC system that initially performs well but fails to correct errors when the inputs are slightly modified. To ensure an ideal user experience, a reliable GEC system should have the ability to provide consistent and accurate suggestions when encountering irrelevant context perturbations, which we refer to as context robustness. In this paper, we introduce RobustGEC, a benchmark designed to evaluate the context robustness of GEC systems. RobustGEC comprises 5,000 GEC cases, each with one original error-correct sentence pair and five variants carefully devised by human annotators. Utilizing RobustGEC, we reveal that state-of-the-art GEC systems still lack sufficient robustness against context perturbations. In addition, we propose a simple yet effective method for remitting this issue.
    
[^34]: 超越记忆：通过大型语言模型进行推理来侵犯隐私

    Beyond Memorization: Violating Privacy Via Inference with Large Language Models. (arXiv:2310.07298v1 [cs.AI])

    [http://arxiv.org/abs/2310.07298](http://arxiv.org/abs/2310.07298)

    该论文首次全面研究了预训练大型语言模型从文本中推断个人属性的能力，发现当前的模型可以以较低的成本和时间比例，准确地推断出多种个人属性，这引发了隐私泄露的新威胁。

    

    目前关于大型语言模型（LLMs）的隐私研究主要集中在提取记忆训练数据的问题上。同时，模型的推理能力已大幅增强。这引发了一个关键问题，即当前的LLMs是否能够通过从推理时给出的文本中推断个人属性来侵犯个人隐私。在这项工作中，我们首次对预训练LLMs从文本中推断个人属性的能力进行了全面研究。我们构建了一个包含真实Reddit个人资料的数据集，并且显示当前的LLMs可以推断出各种各样的个人属性（例如，位置、收入、性别），在成本（100倍）和时间（240倍）上仅需人类的一小部分，达到了最高1的准确率达到85％，最高3的准确率达到95.8％。随着人们越来越多地与由LLM驱动的聊天机器人在生活的各个方面进行互动，我们还探讨了侵犯隐私的聊天机器人通过似乎无关的对话试图提取个人信息的新威胁。

    Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\%$ top-1 and $95.8\%$ top-3 accuracy at a fraction of the cost ($100\times$) and time ($240\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemi
    
[^35]: 在医疗保健领域中对大型语言模型的分析：BioBERT 案例研究

    An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT. (arXiv:2310.07282v1 [cs.AI])

    [http://arxiv.org/abs/2310.07282](http://arxiv.org/abs/2310.07282)

    本研究分析了在医疗保健领域应用大型语言模型（尤其是BioBERT）的可行性，并提出了针对医疗保健领域的微调方法。研究突出了BioBERT对于解决与生物医学文本挖掘相关任务的特定要求的适用性。

    

    本文对在医疗保健领域应用大型语言模型（尤其是BioBERT）进行了全面调查研究。它首先彻底检查了先前在医疗保健领域中应用自然语言处理（NLP）方法的情况，揭示了这些方法面临的局限和挑战。随后，本研究探讨了将BioBERT整合到医疗保健应用中的路径，突出其适用于解决与生物医学文本挖掘相关任务的特定要求。该分析概述了用于针对医疗保健领域独特需求微调BioBERT的系统方法论。这个方法包括从各种医疗保健来源收集数据，为识别医疗实体和对其进行分类等任务进行数据注释，并应用专门针对处理生物医学文本中复杂性的预处理技术。

    This paper conducts a comprehensive investigation into applying large language models, particularly on BioBERT, in healthcare. It begins with thoroughly examining previous natural language processing (NLP) approaches in healthcare, shedding light on the limitations and challenges these methods face. Following that, this research explores the path that led to the incorporation of BioBERT into healthcare applications, highlighting its suitability for addressing the specific requirements of tasks related to biomedical text mining. The analysis outlines a systematic methodology for fine-tuning BioBERT to meet the unique needs of the healthcare domain. This approach includes various components, including the gathering of data from a wide range of healthcare sources, data annotation for tasks like identifying medical entities and categorizing them, and the application of specialized preprocessing techniques tailored to handle the complexities found in biomedical texts. Additionally, the pape
    
[^36]: BioT5：在生物学中利用化学知识和自然语言关联丰富跨模态整合

    BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])

    [http://arxiv.org/abs/2310.07276](http://arxiv.org/abs/2310.07276)

    BioT5是一个全面的预训练框架，在生物学中利用化学知识和自然语言关联丰富了跨模态整合，通过鲁棒的分子表示和上下文知识提取，实现了更有效的信息利用，展现出卓越的性能。

    

    最近在生物研究领域的进展利用分子、蛋白质和自然语言的整合来增强药物发现。然而，当前的模型存在一些限制，如生成无效的分子SMILES、对上下文信息的利用不足以及对结构化和非结构化知识的等量处理。为了解决这些问题，我们提出了一个全面的预训练框架BioT5，它通过化学知识和自然语言关联丰富了生物学中的跨模态整合。BioT5利用SELFIES进行100%鲁棒的分子表示，并从非结构化的生物文献中提取生物实体周围上下文的知识。此外，BioT5区分结构化和非结构化知识，从而更有效地利用信息。在微调后，BioT5在各种任务中展现出卓越的性能，表明其强大的能力。

    Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability 
    
[^37]: CoPAL:具有大规模语言模型的机器人动作纠正规划

    CoPAL: Corrective Planning of Robot Actions with Large Language Models. (arXiv:2310.07263v1 [cs.RO])

    [http://arxiv.org/abs/2310.07263](http://arxiv.org/abs/2310.07263)

    本文提出了一个具有大规模语言模型的机器人动作纠正规划系统，通过处理生成计划中的物理基础、逻辑和语义错误的再规划策略，实现了在复杂环境中的任务和动作规划。通过仿真和实际场景的验证，证明了该系统的有效性。

    

    为了实现完全自主的机器人系统能够接管人类传统执行的任务，开放世界环境的复杂性提出了巨大的挑战。在这一背景下，本研究为应用于机器人任务和动作规划的大规模语言模型领域做出了贡献。我们提出了一个系统架构，协调多个认知层次之间的无缝交互，包括推理、规划和动作生成。其核心是一种处理生成的计划中的物理基础、逻辑和语义错误的新型再规划策略。通过在仿真环境和两个复杂的实际场景（方块世界、酒吧和比萨制作）中进行实证评估，我们展示了所提出的反馈架构的有效性，尤其是对可执行性、正确性和时间复杂性的影响。

    In the pursuit of fully autonomous robotic systems capable of taking over tasks traditionally performed by humans, the complexity of open-world environments poses a considerable challenge. Addressing this imperative, this study contributes to the field of Large Language Models (LLMs) applied to task and motion planning for robots. We propose a system architecture that orchestrates a seamless interplay between multiple cognitive levels, encompassing reasoning, planning, and motion generation. At its core lies a novel replanning strategy that handles physically grounded, logical, and semantic errors in the generated plans. We demonstrate the efficacy of the proposed feedback architecture, particularly its impact on executability, correctness, and time complexity via empirical evaluation in the context of a simulation and two intricate real-world scenarios: blocks world, barman and pizza preparation.
    
[^38]: 揭示隐藏的联系：用于视频对话的迭代跟踪和推理

    Uncovering Hidden Connections: Iterative Tracking and Reasoning for Video-grounded Dialog. (arXiv:2310.07259v1 [cs.CV])

    [http://arxiv.org/abs/2310.07259](http://arxiv.org/abs/2310.07259)

    本文提出了一种迭代跟踪和推理策略，结合文本编码器和视觉编码器以生成准确的响应，解决了视频对话中逐步理解对话历史和吸收视频信息的挑战。

    

    与传统的视觉问答相比，视频对话需要对对话历史和视频内容进行深入理解，以生成准确的响应。尽管现有的方法取得了令人称赞的进展，但它们常常面临逐步理解复杂的对话历史和吸收视频信息的挑战。为了弥补这一差距，我们提出了一种迭代跟踪和推理策略，将文本编码器、视觉编码器和生成器相结合。我们的文本编码器以路径跟踪和聚合机制为核心，能够从对话历史中获取重要的细微差别，以解释所提出的问题。同时，我们的视觉编码器利用迭代推理网络，精心设计以从视频中提取和强调关键视觉标记，增强对视觉理解的深度。最后，我们使用预训练的GPT-模型将这些丰富的信息综合起来。

    In contrast to conventional visual question answering, video-grounded dialog necessitates a profound understanding of both dialog history and video content for accurate response generation. Despite commendable strides made by existing methodologies, they often grapple with the challenges of incrementally understanding intricate dialog histories and assimilating video information. In response to this gap, we present an iterative tracking and reasoning strategy that amalgamates a textual encoder, a visual encoder, and a generator. At its core, our textual encoder is fortified with a path tracking and aggregation mechanism, adept at gleaning nuances from dialog history that are pivotal to deciphering the posed questions. Concurrently, our visual encoder harnesses an iterative reasoning network, meticulously crafted to distill and emphasize critical visual markers from videos, enhancing the depth of visual comprehension. Culminating this enriched information, we employ the pre-trained GPT-
    
[^39]: ADMEOOD: 药物属性预测的超分布基准

    ADMEOOD: Out-of-Distribution Benchmark for Drug Property Prediction. (arXiv:2310.07253v1 [cs.LG])

    [http://arxiv.org/abs/2310.07253](http://arxiv.org/abs/2310.07253)

    ADMEOOD是一个设计用于药物属性预测的超分布基准，包含27个药物属性和两种数据转移（噪声转移和概念冲突漂移）。

    

    获得准确有效的药物分子信息是一项关键而具有挑战性的任务。然而，过去100年来，化学知识和信息来自不同地区、实验室和实验目的的积累。在超分布（OOD）问题中，噪音和不一致性很少被探索，这可能导致弱鲁棒性和不满意的性能。本研究提出了一个新的基准ADMEOOD, 一个特别设计用于药物属性预测的系统超分布数据集策划者和基准。ADMEOOD从Chembl和相关文献中获得了27个药物属性。另外，它还包括两种超分布数据转移：噪声转移和概念冲突漂移（CCD）。噪声转移通过将环境分类为不同的置信水平来响应噪声水平。另一方面，CCD描述了原始数据中标签不一致的数据。

    Obtaining accurate and valid information for drug molecules is a crucial and challenging task. However, chemical knowledge and information have been accumulated over the past 100 years from various regions, laboratories, and experimental purposes. Little has been explored in terms of the out-of-distribution (OOD) problem with noise and inconsistency, which may lead to weak robustness and unsatisfied performance. This study proposes a novel benchmark ADMEOOD, a systematic OOD dataset curator and benchmark specifically designed for drug property prediction. ADMEOOD obtained 27 ADME (Absorption, Distribution, Metabolism, Excretion) drug properties from Chembl and relevant literature. Additionally, it includes two kinds of OOD data shifts: Noise Shift and Concept Conflict Drift (CCD). Noise Shift responds to the noise level by categorizing the environment into different confidence levels. On the other hand, CCD describes the data which has inconsistent label among the original data. Finall
    
[^40]: 基于伦理推理的道德对齐：关于在LLM中上下文伦理政策的案例和框架

    Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs. (arXiv:2310.07251v1 [cs.CL])

    [http://arxiv.org/abs/2310.07251](http://arxiv.org/abs/2310.07251)

    本文提出了一种在LLM中应用通用伦理推理能力而非特定伦理原则的方法，以处理全球范围的价值多元性。作者开发了一个框架，将道德困境与不同形式的规范伦理以及不同抽象层次的道德原则相结合。初步实验表明，尽管GPT-4几乎可以完美地进行伦理推理，但这些模型仍然存在对西方和以英语为母语的社会道德价值的偏见。

    

    在这篇论文中，我们认为与其对LLM进行道德对齐到特定的伦理原则，我们应该注入通用的伦理推理能力，使其能够处理全球范围的价值多元性。当提供伦理政策时，LLM应该能够做出与政策一致的伦理决策。我们开发了一个框架，将道德困境与不同形式的规范伦理以及不同抽象层次的道德原则相结合。对GPT-x模型的初步实验表明，虽然GPT-4几乎可以完美地进行伦理推理，但这些模型仍然存在对西方和以英语为母语的社会道德价值的偏见。

    In this position paper, we argue that instead of morally aligning LLMs to specific set of ethical principles, we should infuse generic ethical reasoning capabilities into them so that they can handle value pluralism at a global scale. When provided with an ethical policy, an LLM should be capable of making decisions that are ethically consistent to the policy. We develop a framework that integrates moral dilemmas with moral principles pertaining to different foramlisms of normative ethics, and at different levels of abstractions. Initial experiments with GPT-x models shows that while GPT-4 is a nearly perfect ethical reasoner, the models still have bias towards the moral values of Western and English speaking societies.
    
[^41]: 结构健康监测应用中的随机裂纹扩展过程的代理模型

    Surrogate modeling for stochastic crack growth processes in structural health monitoring applications. (arXiv:2310.07241v1 [stat.ML])

    [http://arxiv.org/abs/2310.07241](http://arxiv.org/abs/2310.07241)

    本文提出了一种代理模型用于预测结构中裂纹的扩展，并成功地编码了不同的随机不确定性来源。该模型基于高斯过程回归模型，能够生成先验分布用于贝叶斯结构健康监测任务。

    

    疲劳裂纹扩展是金属结构中最常见的一种破坏类型，对其可靠性有重要影响。最近在结构健康监测领域的进展促使使用结构响应数据来预测不确定条件下未来的裂纹扩展，以实现向预测性维修的过渡。准确地表示随机裂纹扩展过程中不同的不确定性来源是一项非常困难的任务。本研究在基于物理模型的随机裂纹扩展建模的基础上进行了探索，考虑了材料和载荷相关的不确定性。本文旨在构建计算效率高、概率代理模型，能够成功地编码这些不同的不确定性来源。采用了受潜变量建模启发的方法，利用高斯过程回归模型使代理模型可以用于为不同的贝叶斯结构健康监测任务生成先验分布。

    Fatigue crack growth is one of the most common types of deterioration in metal structures with significant implications on their reliability. Recent advances in Structural Health Monitoring (SHM) have motivated the use of structural response data to predict future crack growth under uncertainty, in order to enable a transition towards predictive maintenance. Accurately representing different sources of uncertainty in stochastic crack growth (SCG) processes is a non-trivial task. The present work builds on previous research on physics-based SCG modeling under both material and load-related uncertainty. The aim here is to construct computationally efficient, probabilistic surrogate models for SCG processes that successfully encode these different sources of uncertainty. An approach inspired by latent variable modeling is employed that utilizes Gaussian Process (GP) regression models to enable the surrogates to be used to generate prior distributions for different Bayesian SHM tasks as th
    
[^42]: 使用可学习的物理学进行实时运动姿势建议

    Using Learnable Physics for Real-Time Exercise Form Recommendations. (arXiv:2310.07221v1 [cs.AI])

    [http://arxiv.org/abs/2310.07221](http://arxiv.org/abs/2310.07221)

    本文提出了一种使用可学习的物理学的算法流程，能够在实时环境中诊断运动姿势问题并提供矫正建议，通过姿势识别、重复次数计算和动作演变跟踪实现。该系统在六个不同的运动上进行了评估，通过低成本设备如智能手机提供实时建议，使自我练习成为可能，同时降低受伤风险。

    

    良好的姿势和形式对于安全和高效的运动至关重要。即使在健身房环境下，教练可能无法及时提供反馈。因此，康复疗法和健身训练可以从提供实时评估的推荐系统中受益。本文提出了一种算法流程，可以在实时环境中对运动技术中的问题进行诊断，并给出矫正建议，具有高敏感性和特异性。我们使用MediaPipe进行姿势识别，使用峰值突出检测计算重复次数，并使用可学习的物理模拟器跟踪每个运动的动作演变。根据与典型学习动作的偏差，基于统计学习对测试视频进行诊断。该系统在六个全身和上半身运动上进行了评估。通过低成本设备如智能手机提供的实时建议，运动者可以纠正潜在的错误，使自我练习成为可能，同时降低受伤风险。

    Good posture and form are essential for safe and productive exercising. Even in gym settings, trainers may not be readily available for feedback. Rehabilitation therapies and fitness workouts can thus benefit from recommender systems that provide real-time evaluation. In this paper, we present an algorithmic pipeline that can diagnose problems in exercise techniques and offer corrective recommendations, with high sensitivity and specificity in real-time. We use MediaPipe for pose recognition, count repetitions using peak-prominence detection, and use a learnable physics simulator to track motion evolution for each exercise. A test video is diagnosed based on deviations from the prototypical learned motion using statistical learning. The system is evaluated on six full and upper body exercises. These real-time recommendations, counseled via low-cost equipment like smartphones, will allow exercisers to rectify potential mistakes making self-practice feasible while reducing the risk of wo
    
[^43]: 改进的对语言分类模型的成员推理攻击

    Improved Membership Inference Attacks Against Language Classification Models. (arXiv:2310.07219v1 [cs.LG])

    [http://arxiv.org/abs/2310.07219](http://arxiv.org/abs/2310.07219)

    在这篇论文中，我们提出了一个新的框架，用于对语言分类模型进行成员推理攻击。通过利用集成方法，生成多个专门的攻击模型，我们展示了这种方法在经典和语言分类任务上比单个攻击模型或每个类别标签的攻击模型更准确。

    

    人工智能系统在日常生活中普遍存在，具有零售、制造、健康等许多领域的用例。随着人工智能采用的增加，已经发现了相关的风险，包括对使用其数据训练模型的人的隐私风险。评估机器学习模型的隐私风险对于是否使用、部署或共享模型做出知情决策至关重要。隐私风险评估的一种常见方法是对模型进行一个或多个已知攻击，并测量它们的成功率。我们提出了一个新颖的框架，用于对分类模型进行成员推理攻击。我们的框架利用集成方法，为不同数据子集生成许多专门的攻击模型。我们展示了这种方法在经典和语言分类任务上比单个攻击模型或每个类别标签的攻击模型都实现了更高的准确性。

    Artificial intelligence systems are prevalent in everyday life, with use cases in retail, manufacturing, health, and many other fields. With the rise in AI adoption, associated risks have been identified, including privacy risks to the people whose data was used to train models. Assessing the privacy risks of machine learning models is crucial to enabling knowledgeable decisions on whether to use, deploy, or share a model. A common approach to privacy risk assessment is to run one or more known attacks against the model and measure their success rate. We present a novel framework for running membership inference attacks against classification models. Our framework takes advantage of the ensemble method, generating many specialized attack models for different subsets of the data. We show that this approach achieves higher accuracy than either a single attack model or an attack model per class label, both on classical and language classification tasks.
    
[^44]: 在多智能体强化学习中量化智能体相互作用以实现成本效益的泛化

    Quantifying Agent Interaction in Multi-agent Reinforcement Learning for Cost-efficient Generalization. (arXiv:2310.07218v1 [cs.MA])

    [http://arxiv.org/abs/2310.07218](http://arxiv.org/abs/2310.07218)

    本研究针对多智能体强化学习中的泛化问题，提出了一种量化智能体相互作用的指标，并通过该指标设计了资源分配方法，可以在有限预算下训练适用于多样化场景的智能体策略集合。

    

    泛化是多智能体强化学习中的一个重要挑战。一个智能体受未知合作智能体的影响程度取决于该智能体的策略和具体场景。对这种关系的定量研究有助于有效培训适用于多样化场景的智能体。在本研究中，我们提出了影响水平（Level of Influence，LoI），一种度量给定场景和环境中智能体之间交互强度的指标。我们观察到，一般来说，在训练过程中使用更多样化的合作智能体可以提高自我智能体的泛化性能；然而，这种改进因不同场景和环境而异。LoI在预测特定场景中这些改进差异方面表现出了有效性。此外，我们引入了一种以LoI为指导的资源分配方法，针对有限预算训练适用于多样化场景的策略集合。我们的结果表明，战略性资源分配可以明显提高智能体的泛化性能。

    Generalization poses a significant challenge in Multi-agent Reinforcement Learning (MARL). The extent to which an agent is influenced by unseen co-players depends on the agent's policy and the specific scenario. A quantitative examination of this relationship sheds light on effectively training agents for diverse scenarios. In this study, we present the Level of Influence (LoI), a metric quantifying the interaction intensity among agents within a given scenario and environment. We observe that, generally, a more diverse set of co-play agents during training enhances the generalization performance of the ego agent; however, this improvement varies across distinct scenarios and environments. LoI proves effective in predicting these improvement disparities within specific scenarios. Furthermore, we introduce a LoI-guided resource allocation method tailored to train a set of policies for diverse scenarios under a constrained budget. Our results demonstrate that strategic resource allocatio
    
[^45]: 多任务学习支持的智能海上监控船只吃水读数自动化

    Multi-Task Learning-Enabled Automatic Vessel Draft Reading for Intelligent Maritime Surveillance. (arXiv:2310.07212v1 [cs.CV])

    [http://arxiv.org/abs/2310.07212](http://arxiv.org/abs/2310.07212)

    本研究提出了一种多任务学习启用的计算方法，用于准确高可靠地生成船只吃水读数，以支持智能海上监控。

    

    准确高效的船只吃水读数（VDR）是智能海上监控的重要组成部分，可以用来判断船只是否正常装载或过载。计算机视觉技术具有良好的性价比，已成为估计船只吃水深度的流行方法。然而，传统的估计方法容易受到多种限制的影响，如对低质量图像的敏感性，高计算成本等。本文提出了一种多任务学习启用的计算方法（称为MTL-VDR），用于生成高可靠的VDR。特别地，我们的MTL-VDR主要包括四个组成部分，即吃水标志检测，吃水尺度识别，船体/水域分割和最终吃水深度估计。我们首先构建与吃水标志检测相关的基准数据集，并采用强大高效的卷积神经网络来准确执行检测任务。

    The accurate and efficient vessel draft reading (VDR) is an important component of intelligent maritime surveillance, which could be exploited to assist in judging whether the vessel is normally loaded or overloaded. The computer vision technique with an excellent price-to-performance ratio has become a popular medium to estimate vessel draft depth. However, the traditional estimation methods easily suffer from several limitations, such as sensitivity to low-quality images, high computational cost, etc. In this work, we propose a multi-task learning-enabled computational method (termed MTL-VDR) for generating highly reliable VDR. In particular, our MTL-VDR mainly consists of four components, i.e., draft mark detection, draft scale recognition, vessel/water segmentation, and final draft depth estimation. We first construct a benchmark dataset related to draft mark detection and employ a powerful and efficient convolutional neural network to accurately perform the detection task. The mul
    
[^46]: 视觉计算扩散模型的最新进展

    State of the Art on Diffusion Models for Visual Computing. (arXiv:2310.07204v1 [cs.AI])

    [http://arxiv.org/abs/2310.07204](http://arxiv.org/abs/2310.07204)

    这篇论文旨在介绍最新的视觉计算领域中扩散模型的发展和应用，涵盖了生成人工智能的核心概念和实现细节，并总结了个人化、条件约束和反演等重要方面。

    

    随着生成人工智能的出现，视觉计算领域正在迅速发展，它为图像、视频和3D场景的生成、编辑和重建提供了前所未有的能力。在这些领域中，扩散模型是生成人工智能的首选架构。仅在过去一年中，基于扩散的工具和应用的文献数量呈指数增长，并且涉及计算机图形学、计算机视觉和人工智能社区的相关论文每天都在arXiv上发表。这个领域的快速增长使得跟上所有最新发展变得困难。这份最新技术报告（STAR）的目标是介绍扩散模型的基本数学概念、稳定扩散模型的实现细节和设计选择，以及概述这些生成人工智能工具的重要方面，包括个性化、条件约束、反演等。

    The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion-based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state-of-the-art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Mor
    
[^47]: MatChat: 用于材料科学的大型语言模型和应用服务平台

    MatChat: A Large Language Model and Application Service Platform for Materials Science. (arXiv:2310.07197v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2310.07197](http://arxiv.org/abs/2310.07197)

    MatChat是一个用于材料科学的大型语言模型和应用服务平台，利用LLaMA2-7B模型和13,878条结构化材料知识数据，能够预测无机材料的合成路径。

    

    化学合成路径的预测在材料科学研究中起着关键作用。然而，目前合成路径的复杂性和缺乏全面的数据集等挑战阻碍了我们准确预测这些化学过程的能力。本研究利用LLaMA2-7B模型的力量，并通过结合13,878条结构化材料知识数据的学习过程加强了它。这个名为MatChat的专门的AI模型专注于预测无机材料的合成路径。MatChat在材料科学的知识生成和推理方面展现出了卓越的能力。虽然MatChat需要进一步改进以满足多样化材料设计的需求。

    The prediction of chemical synthesis pathways plays a pivotal role in materials science research. Challenges, such as the complexity of synthesis pathways and the lack of comprehensive datasets, currently hinder our ability to predict these chemical processes accurately. However, recent advancements in generative artificial intelligence (GAI), including automated text generation and question-answering systems, coupled with fine-tuning techniques, have facilitated the deployment of large-scale AI models tailored to specific domains. In this study, we harness the power of the LLaMA2-7B model and enhance it through a learning process that incorporates 13,878 pieces of structured material knowledge data. This specialized AI model, named MatChat, focuses on predicting inorganic material synthesis pathways. MatChat exhibits remarkable proficiency in generating and reasoning with knowledge in materials science. Although MatChat requires further refinement to meet the diverse material design n
    
[^48]: 自适应门控在基于混合专家语言模型中的应用

    Adaptive Gating in Mixture-of-Experts based Language Models. (arXiv:2310.07188v1 [cs.CL])

    [http://arxiv.org/abs/2310.07188](http://arxiv.org/abs/2310.07188)

    本文介绍了一种自适应门控的混合专家语言模型训练策略，通过根据标记的专家概率分布将标记分配给变量数量的专家，同时保持模型的稀疏性和提高训练效率。

    

    大型语言模型（如OpenAI的ChatGPT）在各种NLP任务中展现出了出色的语言理解能力。稀疏激活的混合专家（MoE）已经成为一种有前途的解决方案，可以在保持计算操作数量恒定的同时扩大模型规模。现有的MoE模型采用了固定的门控网络，每个标记都由相同数量的专家计算。然而，这种方法与我们的直觉相矛盾，因为每个序列中的标记在语言复杂性方面有所不同，因此需要不同的计算成本。先前的研究中很少讨论每个标记的计算和模型性能之间的权衡。本文介绍了在MoE中使用自适应门控的灵活训练策略，该策略可以根据专家概率分布将标记处理为可变数量的专家。所提出的框架在改进训练效率的同时保持稀疏性。此外，课程学习也在该框架中应用以进一步提高模型性能。

    Large language models, such as OpenAI's ChatGPT, have demonstrated exceptional language understanding capabilities in various NLP tasks. Sparsely activated mixture-of-experts (MoE) has emerged as a promising solution for scaling models while maintaining a constant number of computational operations. Existing MoE model adopts a fixed gating network where each token is computed by the same number of experts. However, this approach contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and, consequently, require different computational costs. Little is discussed in prior research on the trade-off between computation per token and model performance. This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution. The proposed framework preserves sparsity while improving training efficiency. Additionally, curriculum learning 
    
[^49]: 多视角Transformer: 重新思考高光谱图像分类中的空间信息

    Multiview Transformer: Rethinking Spatial Information in Hyperspectral Image Classification. (arXiv:2310.07186v1 [cs.CV])

    [http://arxiv.org/abs/2310.07186](http://arxiv.org/abs/2310.07186)

    本文提出了一种多视角Transformer用于高光谱图像分类，解决了HSI立方体中可能记录到的场景特定但非本质相关性带来的空间过拟合问题，并通过多视角主成分分析、光谱编码器-解码器和空间池化标记化Transformer的组合来提取低维的空间-光谱特征表示。

    

    对于高光谱图像(HSI)中的每个像素，识别其土地覆盖类别依赖于光谱和空间信息。利用具有特定块大小的HSI立方体来提取中心像素的空间-光谱特征表示。本文研究了HSI立方体中可能记录到的场景特定但非本质相关性。这些额外信息改善了现有HSI数据集上的模型性能，并使得对模型能力的恰当评估变得困难。我们将这个问题称为空间过拟合问题，并利用严格的实验设置来避免它。我们进一步提出了一种用于HSI分类的多视角Transformer，包括多视角主成分分析(MPCA)、光谱编码器-解码器(SED)和空间池化标记化Transformer(SPTT)。MPCA通过构建光谱多视角观测并对每个视角数据应用PCA进行降维来对HSI进行降维处理。

    Identifying the land cover category for each pixel in a hyperspectral image (HSI) relies on spectral and spatial information. An HSI cuboid with a specific patch size is utilized to extract spatial-spectral feature representation for the central pixel. In this article, we investigate that scene-specific but not essential correlations may be recorded in an HSI cuboid. This additional information improves the model performance on existing HSI datasets and makes it hard to properly evaluate the ability of a model. We refer to this problem as the spatial overfitting issue and utilize strict experimental settings to avoid it. We further propose a multiview transformer for HSI classification, which consists of multiview principal component analysis (MPCA), spectral encoder-decoder (SED), and spatial-pooling tokenization transformer (SPTT). MPCA performs dimension reduction on an HSI via constructing spectral multiview observations and applying PCA on each view data to extract low-dimensional
    
[^50]: rpcPRF: 用于卫星相机的可推广MPI神经辐射场

    rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera. (arXiv:2310.07179v1 [cs.CV])

    [http://arxiv.org/abs/2310.07179](http://arxiv.org/abs/2310.07179)

    rpcPRF是一种用于卫星相机的可推广MPI神经辐射场，通过重投影监督和辐射场渲染技术，使模型适用于单个或少量输入，并在未见场景的图像上表现良好。

    

    卫星图像的新视角综合具有广泛的实际应用。虽然最近神经辐射场的进展主要针对针孔相机，而卫星相机的模型通常需要足够的输入视图。本文提出了rpcPRF，一种基于多平面图像(MPI)的有理多项式相机(RPC)的平面神经辐射场。与需要足够多场景视图的基于坐标的神经辐射场不同，我们的模型适用于单个或少量输入，并在未见场景的图像上表现良好。为了实现跨场景的泛化，我们建议使用重投影监督来诱导预测的MPI学习3D坐标与图像之间的正确几何关系。此外，我们通过引入辐射场渲染技术，消除了深度多视图立体方法对密集深度监督的严格要求。rpcPRF结合了隐式表示的优势和渲染技术的优点。

    Novel view synthesis of satellite images holds a wide range of practical applications. While recent advances in the Neural Radiance Field have predominantly targeted pin-hole cameras, and models for satellite cameras often demand sufficient input views. This paper presents rpcPRF, a Multiplane Images (MPI) based Planar neural Radiance Field for Rational Polynomial Camera (RPC). Unlike coordinate-based neural radiance fields in need of sufficient views of one scene, our model is applicable to single or few inputs and performs well on images from unseen scenes. To enable generalization across scenes, we propose to use reprojection supervision to induce the predicted MPI to learn the correct geometry between the 3D coordinates and the images. Moreover, we remove the stringent requirement of dense depth supervision from deep multiview-stereo-based methods by introducing rendering techniques of radiance fields. rpcPRF combines the superiority of implicit representations and the advantages o
    
[^51]: 在线推测解码

    Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])

    [http://arxiv.org/abs/2310.07177](http://arxiv.org/abs/2310.07177)

    在线推测解码是通过利用多余计算能力，在LLM服务集群中持续更新草稿模型，从而加速大型语言模型推理的一种方法。

    

    推测解码是通过利用较小的草稿模型来预测目标模型的输出，从而加速大型语言模型（LLM）推理的关键技术。然而，在面对多样的文本输入和草稿模型与目标模型之间的显著能力差距时，其有效性可能受到限制。我们引入了在线推测解码（OSD）来解决这一挑战。其主要思想是利用LLM服务集群中丰富的多余计算能力，根据观察到的用户查询数据持续更新（多个）草稿模型。由于LLM推理受内存限制，典型的LLM服务集群中的剩余计算能力可以用于在线重新训练草稿模型，从而使训练成本保持中性。由于LLM服务的查询分布相对简单，根据查询分布进行重新训练可以使草稿模型更准确地预测目标模型的输出。

    Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
    
[^52]: 使用基于协调方法解决旅行窃贼问题

    Solving Travelling Thief Problems using Coordination Based Methods. (arXiv:2310.07156v1 [cs.AI])

    [http://arxiv.org/abs/2310.07156](http://arxiv.org/abs/2310.07156)

    这篇论文提出了一种解决旅行窃贼问题的协调方法，去改善城市选择和物品选择之间的协调问题。

    

    旅行窃贼问题（TTP）是实际问题（如邮件收集）的一个代理。TTP由旅行推销员问题（TSP）和背包问题（KP）的组合构成，因为KP的物品分散在TSP的城市中，小偷必须访问城市来收集物品。在TTP中，城市选择和物品选择决策需要紧密协调，因为小偷的旅行速度取决于背包的重量，访问城市的顺序会影响物品收集的顺序。现有的TTP求解器将城市选择和物品选择分开处理，保持一种类型的决策不变，而处理另一种类型。这种分离实际上意味着两种决策之间的协调很差。在本文中，我们首先证明了在TTP中一个简单的基于局部搜索的协调方法不起作用。然后，为了解决上述问题，我们提出了一种人工设计的协调启发式算法，在开发期间更改收集计划。

    A travelling thief problem (TTP) is a proxy to real-life problems such as postal collection. TTP comprises an entanglement of a travelling salesman problem (TSP) and a knapsack problem (KP) since items of KP are scattered over cities of TSP, and a thief has to visit cities to collect items. In TTP, city selection and item selection decisions need close coordination since the thief's travelling speed depends on the knapsack's weight and the order of visiting cities affects the order of item collection. Existing TTP solvers deal with city selection and item selection separately, keeping decisions for one type unchanged while dealing with the other type. This separation essentially means very poor coordination between two types of decision. In this paper, we first show that a simple local search based coordination approach does not work in TTP. Then, to address the aforementioned problems, we propose a human designed coordination heuristic that makes changes to collection plans during exp
    
[^53]: 设备外没有隐私：关于TEE保护下的设备端机器学习的（不）安全性

    No Privacy Left Outside: On the (In-)Security of TEE-Shielded DNN Partition for On-Device ML. (arXiv:2310.07152v1 [cs.CR])

    [http://arxiv.org/abs/2310.07152](http://arxiv.org/abs/2310.07152)

    本文研究了TEE保护下设备端机器学习的安全性问题。通过对现有的TSDP解决方案进行评估，发现这些解决方案容易受到隐私窃取攻击的影响。

    

    设备端机器学习引入了新的安全挑战：DNN模型可以被设备用户白盒访问。基于白盒信息，攻击者可以进行有效的模型窃取和成员推断攻击。使用可信执行环境（TEE）来保护设备端的DNN模型旨在将（易于进行的）白盒攻击降级为（更难进行的）黑盒攻击。然而，一个主要的缺点是大大增加了延迟（高达50倍）。为了加速使用GPU进行TEE保护的DNN计算，研究人员提出了几种模型分区技术。这些解决方案被称为TEE保护的DNN分区（TSDP），将DNN模型分为两个部分，将隐私不敏感的部分卸载到GPU上，同时将隐私敏感的部分保护在TEE内。本文评估了现有的TSDP解决方案在各种DNN模型、数据集和指标上进行了模型窃取和成员推断攻击的实验。我们发现现有的TSDP解决方案容易受到隐私窃取攻击的影响。

    On-device ML introduces new security challenges: DNN models become white-box accessible to device users. Based on white-box information, adversaries can conduct effective model stealing (MS) and membership inference attack (MIA). Using Trusted Execution Environments (TEEs) to shield on-device DNN models aims to downgrade (easy) white-box attacks to (harder) black-box attacks. However, one major shortcoming is the sharply increased latency (up to 50X). To accelerate TEE-shield DNN computation with GPUs, researchers proposed several model partition techniques. These solutions, referred to as TEE-Shielded DNN Partition (TSDP), partition a DNN model into two parts, offloading the privacy-insensitive part to the GPU while shielding the privacy-sensitive part within the TEE. This paper benchmarks existing TSDP solutions using both MS and MIA across a variety of DNN models, datasets, and metrics. We show important findings that existing TSDP solutions are vulnerable to privacy-stealing attack
    
[^54]: 在缺席投票的选举中确定获胜者

    Determining Winners in Elections with Absent Votes. (arXiv:2310.07150v1 [cs.GT])

    [http://arxiv.org/abs/2310.07150](http://arxiv.org/abs/2310.07150)

    本研究讨论了在选举中一些选票缺席情况下确定候选人是否可以获胜的问题，通过研究顶部截断时不同的选票规则，我们发现了这个问题的复杂度，并提出了在特殊情况下可以在多项式时间内计算出结果的方法。

    

    选举中一个重要的问题是确定在某些选票缺席时候候选人是否可以获胜。我们研究了当选票是顶部截断时，通过缺席投票确定获胜者（WAV）问题。我们证明了对于单一可转换选票、最小值和Copeland，WAV问题都是NP-complete的，并提出了一种特殊情况下的位置评分规则，这样问题可以在多项式时间内计算出来。我们的结果在顶部截断排序中与完整排序的结果不同，因为当候选人数或缺席选票数有限时，它们的难度结果仍然成立，而我们证明这个问题可以在这两种情况下在多项式时间内解决。

    An important question in elections is the determine whether a candidate can be a winner when some votes are absent. We study this determining winner with the absent votes (WAV) problem when the votes are top-truncated. We show that the WAV problem is NP-complete for the single transferable vote, Maximin, and Copeland, and propose a special case of positional scoring rule such that the problem can be computed in polynomial time. Our results in top-truncated rankings differ from the results in full rankings as their hardness results still hold when the number of candidates or the number of missing votes are bounded, while we show that the problem can be solved in polynomial time in either case.
    
[^55]: 扩散模型的去噪任务路由

    Denoising Task Routing for Diffusion Models. (arXiv:2310.07138v1 [cs.CV])

    [http://arxiv.org/abs/2310.07138](http://arxiv.org/abs/2310.07138)

    本文提出了一种名为去噪任务路由的策略，通过为扩散模型的不同任务建立独立的信息路径，实现了对多任务学习的明确纳入。该方法将去噪任务的先验知识无缝集成到框架中，通过激活相似的通道和滑动窗口的方式，充分利用了相邻时间步任务间的亲和关系。

    

    扩散模型通过学习多步去噪过程生成高度逼真的图像，自然地体现了多任务学习（MTL）的原理。尽管扩散模型和MTL之间存在固有的连接，但在设计明确将MTL纳入扩散模型框架的神经结构方面仍存在一个未被探索的领域。在本文中，我们提出了去噪任务路由（DTR），一种对现有扩散模型架构进行简单附加的策略，通过选择性地激活模型中的子通道来为单个任务建立独立的信息路径。DTR的特别吸引人之处在于它将去噪任务的先验知识无缝集成到框架中：（1）任务亲和性：DTR为相邻时间步的任务激活相似的通道，并将激活的通道作为滑动窗口通过时间步进行移动，利用相邻时间步任务间固有的强亲和关系。

    Diffusion models generate highly realistic images through learning a multi-step denoising process, naturally embodying the principles of multi-task learning (MTL). Despite the inherent connection between diffusion models and MTL, there remains an unexplored area in designing neural architectures that explicitly incorporate MTL into the framework of diffusion models. In this paper, we present Denoising Task Routing (DTR), a simple add-on strategy for existing diffusion model architectures to establish distinct information pathways for individual tasks within a single architecture by selectively activating subsets of channels in the model. What makes DTR particularly compelling is its seamless integration of prior knowledge of denoising tasks into the framework: (1) Task Affinity: DTR activates similar channels for tasks at adjacent timesteps and shifts activated channels as sliding windows through timesteps, capitalizing on the inherent strong affinity between tasks at adjacent timestep
    
[^56]: 用于人类反馈的非策略评估

    Off-Policy Evaluation for Human Feedback. (arXiv:2310.07123v1 [cs.LG])

    [http://arxiv.org/abs/2310.07123](http://arxiv.org/abs/2310.07123)

    本论文介绍了一个用于人类反馈的非策略评估（OPEHF）框架，可以准确评估人类反馈信号。这个框架解决了现有OPE方法在估计人类反馈信号上的不足。

    

    非策略评估（OPE）对于强化学习（RL）的离线训练和评估之间的差距的缩小非常重要，它通过仅使用离线轨迹估计目标（评估）策略的性能和/或排名。它可以提高数据收集和策略测试过程的安全性和效率，在在线部署成本较高的情况下，如医疗保健中。然而，现有的OPE方法在估计人类反馈（HF）信号方面存在不足，因为HF可能会受到多个潜在因素的影响，而且只是稀疏可用的；而不同于代理定义的环境奖励（用于策略优化），环境奖励通常是在参数函数或分布上决定的。因此，由于HF信号的性质，准确地推断OPE估计是具有挑战性的。为了解决这个问题，我们引入了一个用于HF的OPE框架，它重新使用现有的OPE方法，以准确评估HF信号。具体而言，我们开发了一个方法来

    Off-policy evaluation (OPE) is important for closing the gap between offline training and evaluation of reinforcement learning (RL), by estimating performance and/or rank of target (evaluation) policies using offline trajectories only. It can improve the safety and efficiency of data collection and policy testing procedures in situations where online deployments are expensive, such as healthcare. However, existing OPE methods fall short in estimating human feedback (HF) signals, as HF may be conditioned over multiple underlying factors and is only sparsely available; as opposed to the agent-defined environmental rewards (used in policy optimization), which are usually determined over parametric functions or distributions. Consequently, the nature of HF signals makes extrapolating accurate OPE estimations to be challenging. To resolve this, we introduce an OPE for HF (OPEHF) framework that revives existing OPE methods in order to accurately evaluate the HF signals. Specifically, we deve
    
[^57]: 《人脑中语言处理的时态结构与深度语言模型的层次结构相符》

    The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models. (arXiv:2310.07106v1 [cs.CL])

    [http://arxiv.org/abs/2310.07106](http://arxiv.org/abs/2310.07106)

    本文展示了深度语言模型(DLMs)的分层结构与人脑语言理解的时间动态之间存在强相关性，通过采用电子皮层图(ECoG)数据来优化时间分辨率，为深入了解人脑语言处理机制提供了新的视角。

    

    深度语言模型(DLMs)提供了一种理解人脑自然语言处理机制的新的计算范式。与传统的心理语言学模型不同，DLMs使用分层的连续数值向量序列来表示单词和上下文，使得诸多新的应用成为可能，如类人生成文本。本文通过展示DLMs的分层结构可能用于模拟大脑语言理解的时间动态，证明了DLM层次深度与最能预测人脑的层次时间之间的强相关性。我们之所以能够在时间上解析出每个层次的优势在于我们采用了电子皮质图(ECoG)数据，其时间分辨率比功能性磁共振成像(fMRI)等无损测量方法更高。我们利用ECoG从参与者听30分钟叙述时记录神经活动，同时将相同叙述提供给高性能DLM(GPT2-XL)。

    Deep Language Models (DLMs) provide a novel computational paradigm for understanding the mechanisms of natural language processing in the human brain. Unlike traditional psycholinguistic models, DLMs use layered sequences of continuous numerical vectors to represent words and context, allowing a plethora of emerging applications such as human-like text generation. In this paper we show evidence that the layered hierarchy of DLMs may be used to model the temporal dynamics of language comprehension in the brain by demonstrating a strong correlation between DLM layer depth and the time at which layers are most predictive of the human brain. Our ability to temporally resolve individual layers benefits from our use of electrocorticography (ECoG) data, which has a much higher temporal resolution than noninvasive methods like fMRI. Using ECoG, we record neural activity from participants listening to a 30-minute narrative while also feeding the same narrative to a high-performing DLM (GPT2-XL)
    
[^58]: ClausewitzGPT框架：理论上的大型语言模型增强信息操作的新前沿

    ClausewitzGPT Framework: A New Frontier in Theoretical Large Language Model Enhanced Information Operations. (arXiv:2310.07099v1 [cs.CY])

    [http://arxiv.org/abs/2310.07099](http://arxiv.org/abs/2310.07099)

    ClausewitzGPT框架提供了应对信息操作中大型语言模型增强风险的一种新方法，并强调了自主AI代理人在其中的关键作用。

    

    在数字时代，网络空间成为地缘政治争端的新兴核心的背景下，信息操作和大型语言模型（LLMs）的融合标志着一个范式转变，充满了巨大的机遇和复杂的挑战。随着Mistral 7B LLM（Mistral，2023）等工具使得访问LLM能力（Jin et al.，2023）民主化，从主权国家到流氓实体（Howard et al.，2023），一系列行为者配备了强大的故事塑造工具（Goldstein et al.，2023）。本文提出了一个用于导航这个崭新世界的“ClausewitzGPT”方程。这种新颖的构想不仅旨在量化机器速度的LLM增强操作中固有的风险，还强调了自主AI代理人（Wang, Xie, et al., 2023）的关键角色。这些代理人体现了伦理考虑（Hendrycks et al.，2021），成为不可或缺的组成部分（Wang, Ma, et al.，2023），确保我们向前迈进的同时。

    In a digital epoch where cyberspace is the emerging nexus of geopolitical contention, the melding of information operations and Large Language Models (LLMs) heralds a paradigm shift, replete with immense opportunities and intricate challenges. As tools like the Mistral 7B LLM (Mistral, 2023) democratise access to LLM capabilities (Jin et al., 2023), a vast spectrum of actors, from sovereign nations to rogue entities (Howard et al., 2023), find themselves equipped with potent narrative-shaping instruments (Goldstein et al., 2023). This paper puts forth a framework for navigating this brave new world in the "ClausewitzGPT" equation. This novel formulation not only seeks to quantify the risks inherent in machine-speed LLM-augmented operations but also underscores the vital role of autonomous AI agents (Wang, Xie, et al., 2023). These agents, embodying ethical considerations (Hendrycks et al., 2021), emerge as indispensable components (Wang, Ma, et al., 2023), ensuring that as we race forw
    
[^59]: 稀疏通用Transformer

    Sparse Universal Transformer. (arXiv:2310.07096v1 [cs.CL])

    [http://arxiv.org/abs/2310.07096](http://arxiv.org/abs/2310.07096)

    本文介绍了稀疏通用Transformer（SUT），它通过利用稀疏混合专家（SMoE）和一种新的基于切棍法的动态停止机制来减少计算复杂性，同时保持参数效率和泛化能力。实验证明，SUT在形式语言任务上具有与强基线模型相当的性能，并能显著降低计算资源使用。

    

    通用Transformer（UT）是Transformer的一种变体，其在各层之间共享参数。实证证据表明，在形式语言任务中，UT比Vanilla Transformer（VT）具有更好的组合泛化能力。参数共享还使其具有比VT更好的参数效率。尽管具有许多优点，但扩展UT参数比扩展VT更需要计算和内存资源。本文提出了稀疏通用Transformer（SUT），它利用稀疏混合专家（SMoE）和基于切棍法的动态停止机制来减少UT的计算复杂性，同时保持其参数效率和泛化能力。实验表明，SUT在WMT'14上仅使用一半的计算资源和参数就能达到与强基线模型相同的性能，并在形式语言任务（逻辑推理和CFQ）上具有强大的泛化性能。新的停止机制还能使计算资源减少约50％。

    The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers. Empirical evidence shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, scaling UT parameters is much more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT'14 and strong generalization results on formal language tasks (Logical inference and CFQ). The new halting mechanism also enables around 50\% reduction in compu
    
[^60]: Jaeger:一种基于连接的多变换器VQA模型

    Jaeger: A Concatenation-Based Multi-Transformer VQA Model. (arXiv:2310.07091v1 [cs.CL])

    [http://arxiv.org/abs/2310.07091](http://arxiv.org/abs/2310.07091)

    Jaeger是一种基于连接的多变换器VQA模型，利用RoBERTa large和GPT2-xl作为特征提取器，通过并行考虑多源信息来增强模型表征能力。

    

    基于文档的视觉问答在语言意义消歧和细粒度多模态检索之间提出了一个具有挑战性的任务。虽然由于大规模语言和开放世界先验模型的利用，文档问答取得了鼓舞人心的进展，但仍存在一些挑战，包括响应时间延长、推断持续时间延长和匹配不准确。为了克服这些挑战，我们提出了一种基于连接的多变换器VQA模型Jaegar。为了提取问题特征，我们利用了RoBERTa large和GPT2-xl等预训练模型的强大能力作为特征提取器。随后，我们将两种模型的输出进行连接操作。这个操作使得模型可以同时考虑来自不同来源的信息，增强了其表征能力。通过利用预训练模型进行特征提取，我们的方法有可能增强性能。

    Document-based Visual Question Answering poses a challenging task between linguistic sense disambiguation and fine-grained multimodal retrieval. Although there has been encouraging progress in document-based question answering due to the utilization of large language and open-world prior models\cite{1}, several challenges persist, including prolonged response times, extended inference durations, and imprecision in matching. In order to overcome these challenges, we propose Jaegar, a concatenation-based multi-transformer VQA model. To derive question features, we leverage the exceptional capabilities of RoBERTa large\cite{2} and GPT2-xl\cite{3} as feature extractors. Subsequently, we subject the outputs from both models to a concatenation process. This operation allows the model to consider information from diverse sources concurrently, strengthening its representational capability. By leveraging pre-trained models for feature extraction, our approach has the potential to amplify the pe
    
[^61]: 思维多样性提升大规模语言模型的推理能力

    Diversity of Thought Improves Reasoning Abilities of Large Language Models. (arXiv:2310.07088v1 [cs.CL])

    [http://arxiv.org/abs/2310.07088](http://arxiv.org/abs/2310.07088)

    本文提出了一种方法，通过改变输入提示来提高大规模语言模型的推理能力，从而改善模型在复杂推理场景中的表现。这种方法自动采集模型反馈，生成适合问题的多样化提示，并通过多次推理调用来集成这些多样化的提示。

    

    大规模语言模型在需要复杂推理的环境中表现不佳。然而，将模型指导分解问题为更小的推理步骤或通过修改解码步骤使各种生成结果合并可以提升性能。目前的方法都假设输入提示是固定的，并期望解码策略引入所需的多样性。本文放松了这个假设，并讨论了如何通过创建和利用输入提示的变化来提升思维多样性以改善模型性能。我们提出了一种方法，通过向语言模型征求反馈来构思适合问题的方法，从而自动提高提示的多样性。我们在我们的方法DIV-SE (DIVerse reasoning path Self-Ensemble)中对多样的提示进行合成，通过多次推理调用实现。我们还提出了一种经济高效的替代方案，即在一个推理中使用多样的提示。

    Large language models (LLMs) are documented to struggle in settings that require complex reasoning. Nevertheless, instructing the model to break down the problem into smaller reasoning steps (Wei et al., 2022), or ensembling various generations through modifying decoding steps (Wang et al., 2023) boosts performance. Current methods assume that the input prompt is fixed and expect the decoding strategies to introduce the diversity needed for ensembling. In this work, we relax this assumption and discuss how one can create and leverage variations of the input prompt as a means to diversity of thought to improve model performance. We propose a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that fit for the problem. We then ensemble the diverse prompts in our method DIV-SE (DIVerse reasoning path Self-Ensemble) across multiple inference calls. We also propose a cost-effective alternative where diverse prompts are used within a s
    
[^62]: 利用推特数据进行交通用户反馈的情感分析：一个自然语言处理框架

    Leveraging Twitter Data for Sentiment Analysis of Transit User Feedback: An NLP Framework. (arXiv:2310.07086v1 [cs.AI])

    [http://arxiv.org/abs/2310.07086](http://arxiv.org/abs/2310.07086)

    本论文提出了一个基于自然语言处理的框架，利用推特数据进行交通用户反馈的情感分析。通过少样本学习识别推特中的问题，并采用词典情感分析模型评估推特情感的强度和极性。

    

    传统的通过交通调查收集用户反馈的方法往往耗时、资源密集且昂贵。在本论文中，我们提出了一种新颖的基于自然语言处理的框架，利用推特等社交媒体平台上广泛、丰富且廉价的数据，来了解用户对各种服务问题的感知。推特作为一个微博平台，托管了大量实时的用户生成内容，其中经常包含有关各种产品、服务和体验的有价值的反馈和意见。所提出的框架通过两种技术简化了收集和分析用户反馈的过程，无需昂贵且耗时的用户反馈调查。首先，它利用少样本学习进行推特分类，有效地识别推特中描述的问题。然后，它采用基于词典的情感分析模型来评估推特情感的强度和极性。

    Traditional methods of collecting user feedback through transit surveys are often time-consuming, resource intensive, and costly. In this paper, we propose a novel NLP-based framework that harnesses the vast, abundant, and inexpensive data available on social media platforms like Twitter to understand users' perceptions of various service issues. Twitter, being a microblogging platform, hosts a wealth of real-time user-generated content that often includes valuable feedback and opinions on various products, services, and experiences. The proposed framework streamlines the process of gathering and analyzing user feedback without the need for costly and time-consuming user feedback surveys using two techniques. First, it utilizes few-shot learning for tweet classification within predefined categories, allowing effective identification of the issues described in tweets. It then employs a lexicon-based sentiment analysis model to assess the intensity and polarity of the tweet sentiments, d
    
[^63]: 通过抗内容采样对COVID-19虚假信息数据集进行审计和强化

    Auditing and Robustifying COVID-19 Misinformation Datasets via Anticontent Sampling. (arXiv:2310.07078v1 [cs.LG])

    [http://arxiv.org/abs/2310.07078](http://arxiv.org/abs/2310.07078)

    本文拓展了对COVID-19虚假信息数据集进行审计和强化的研究。首先，发现在小数据上训练的专业分类器在野外环境中的性能表现有限。其次，提出了一种无需人工注释的主动学习方法，通过增加具有挑战性的抗内容来增强分类器。

    

    本文有两个关键贡献。首先，它认为在小数据上训练的高度专业的罕见内容分类器通常对野外观察到的负面类别的丰富性和话题多样性（称为抗内容）有限暴露。因此，这些分类器在测试集上观察到的强大性能可能无法在现实世界的环境中有效转化。在COVID-19虚假信息检测的背景下，我们对多个数据集进行了野外审计，并证明了使用几个重要引用的最近数据集进行训练的模型在野外评估时容易受到抗内容的攻击。其次，我们提出了一种新颖的主动学习流程，它不需要任何人工注释，并通过挑战性的抗内容不断增强训练数据，从而强化这些分类器。

    This paper makes two key contributions. First, it argues that highly specialized rare content classifiers trained on small data typically have limited exposure to the richness and topical diversity of the negative class (dubbed anticontent) as observed in the wild. As a result, these classifiers' strong performance observed on the test set may not translate into real-world settings. In the context of COVID-19 misinformation detection, we conduct an in-the-wild audit of multiple datasets and demonstrate that models trained with several prominently cited recent datasets are vulnerable to anticontent when evaluated in the wild. Second, we present a novel active learning pipeline that requires zero manual annotation and iteratively augments the training data with challenging anticontent, robustifying these classifiers.
    
[^64]: 通过有限状态解码实现无语法错误和具有泛化能力的LLM工具使用

    Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding. (arXiv:2310.07075v1 [cs.CL])

    [http://arxiv.org/abs/2310.07075](http://arxiv.org/abs/2310.07075)

    本文提出了一种无语法错误且具有泛化能力的LLM工具使用方法ToolDec，通过有限状态解码算法消除了工具相关错误，使LLM能够有效选择工具，而无需微调或上下文文档。

    

    大型语言模型(LLMs)已经展示出使用外部工具解决复杂问题的有希望的能力。然而，现有方法要么涉及对工具演示进行微调，这样在没有额外训练的情况下无法推广到新的工具，要么在上下文中提供工具文档，从而限制了工具数量。这两种方法常常产生语法无效的工具调用。在本文中，我们提出了ToolDec，一种有限状态机引导的解码算法，用于工具增强的LLMs。ToolDec通过确保有效的工具名称和类型一致的参数，消除了任何工具增强的LLMs中的工具相关错误。此外，ToolDec使LLM能够仅仅使用它们的名称中包含的信息有效地选择工具，而无需微调或上下文文档。我们在涉及数学函数、知识图谱关系和复杂的现实世界RESTful API的各种任务上评估了多种先前的方法及其ToolDec增强版本。

    Large language models (LLMs) have shown promising capabilities in using external tools to solve complex problems. However, existing approaches either involve fine-tuning on tool demonstrations, which do not generalize to new tools without additional training, or providing tool documentation in context, limiting the number of tools. Both approaches often generate syntactically invalid tool calls. In this paper, we propose ToolDec, a finite-state machine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates tool-related errors for any tool-augmented LLMs by ensuring valid tool names and type-conforming arguments. Furthermore, ToolDec enables LLM to effectively select tools using only the information contained in their names, with no need for fine-tuning or in-context documentation. We evaluated multiple prior methods and their ToolDec-enhanced versions on a variety of tasks involving tools like math functions, knowledge graph relations, and complex real-world RESTful APIs
    
[^65]: 大型语言模型可以学习规则

    Large Language Models can Learn Rules. (arXiv:2310.07064v1 [cs.AI])

    [http://arxiv.org/abs/2310.07064](http://arxiv.org/abs/2310.07064)

    大型语言模型(LLMs)在各种推理任务中展示了令人印象深刻的性能。为了提高提示方法的准确性和一致性，我们提出了Hypotheses-to-Theories (HtT)框架，用于学习LLMs推理的规则库，从而改进了现有的提示方法。

    

    当给出一些示例和中间步骤时，大型语言模型(LLMs)在各种推理任务中展示了令人印象深刻的性能。然而，依赖LLM中的隐式知识的提示方法在隐式知识错误或与任务不一致时往往会产生错误的答案。为解决这个问题，我们提出了"假设到理论" (HtT) 框架，用于学习LLMs推理的规则库。HtT包括两个阶段，归纳阶段和演绎阶段。在归纳阶段，首先要求LLM根据一组训练示例生成和验证规则。出现并导致正确答案的规则将被收集形成一个规则库。在演绎阶段，然后要求LLM使用学习的规则库进行推理以回答测试问题。在数值推理和关系推理问题上的实验证明，HtT改进了现有的提示方法，使其性能提升。

    When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks. However, prompting methods that rely on implicit knowledge in an LLM often hallucinate incorrect answers when the implicit knowledge is wrong or inconsistent with the task. To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs. HtT contains two stages, an induction stage and a deduction stage. In the induction stage, an LLM is first asked to generate and verify rules over a set of training examples. Rules that appear and lead to correct answers sufficiently often are collected to form a rule library. In the deduction stage, the LLM is then prompted to employ the learned rule library to perform reasoning to answer test questions. Experiments on both numerical reasoning and relational reasoning problems show that HtT improves existing prompting methods, with an 
    
[^66]: DKEC: 领域知识增强的电子病历多标签分类

    DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records. (arXiv:2310.07059v1 [cs.CL])

    [http://arxiv.org/abs/2310.07059](http://arxiv.org/abs/2310.07059)

    本文提出了DKEC，一种领域知识增强的分类器，用于医学诊断预测。它利用标签的注意力机制和组内训练方法来捕捉医学实体之间的语义关系，并增加罕见类别的样本数量。评估结果显示其在医学数据集上表现良好。

    

    医学领域的多标签文本分类任务经常面临长尾标签分布，即罕见类别的训练样本少于频繁类别。虽然之前的工作已经探索了不同的模型架构和层次化标签结构来找到重要特征，但大多数忽略了从医学指南中融入领域知识。本文提出了DKEC，一种增强医学诊断预测的领域知识增强分类器，其中包括两个创新点：（1）一个基于标签的注意力机制，结合异构图和领域本体来捕捉医学实体之间的语义关系，（2）一种基于标签相似性的简单而有效的组内训练方法，用于增加罕见类别的样本数量。我们在两个真实的医学数据集上评估了DKEC：RAA数据集，包含来自急救服务（EMS）事件的4,417个患者护理报告的收集，和来自53898报告的子集。

    Multi-label text classification (MLTC) tasks in the medical domain often face long-tail label distribution, where rare classes have fewer training samples than frequent classes. Although previous works have explored different model architectures and hierarchical label structures to find important features, most of them neglect to incorporate the domain knowledge from medical guidelines. In this paper, we present DKEC, Domain Knowledge Enhanced Classifier for medical diagnosis prediction with two innovations: (1) a label-wise attention mechanism that incorporates a heterogeneous graph and domain ontologies to capture the semantic relationships between medical entities, (2) a simple yet effective group-wise training method based on similarity of labels to increase samples of rare classes. We evaluate DKEC on two real-world medical datasets: the RAA dataset, a collection of 4,417 patient care reports from emergency medical services (EMS) incidents, and a subset of 53,898 reports from the 
    
[^67]: 健康系统规模的计算病理学——来自30亿图像的自监督基础模型

    Computational Pathology at Health System Scale -- Self-Supervised Foundation Models from Three Billion Images. (arXiv:2310.07033v1 [cs.CV])

    [http://arxiv.org/abs/2310.07033](http://arxiv.org/abs/2310.07033)

    该论文通过使用大规模无标签数据集训练自监督基础模型并在大型临床病理学数据集上进行评估，旨在训练最大的学术基础模型并评估最显著的自监督学习算法。

    

    最近的自监督学习突破使得可以使用大型无标签数据集来训练视觉基础模型，从而可以推广到各种下游任务中。虽然这种训练范式非常适合医学领域，因为标注往往稀缺，但是在医学领域，特别是病理学领域，大规模预训练的研究还不够充分。以往的病理学自监督学习工作利用较小的数据集进行预训练和评估下游性能。该项目的目标是训练最大的学术基础模型，并通过在大规模临床病理学数据集上进行预训练和评估下游性能来评估最显著的自监督学习算法。我们收集了迄今为止最大的病理学数据集，包括来自超过423,000个显微镜幻灯片的30亿张图像。我们比较了使用掩蔽自编码器（MAE）和D来预训练视觉变换器模型。

    Recent breakthroughs in self-supervised learning have enabled the use of large unlabeled datasets to train visual foundation models that can generalize to a variety of downstream tasks. While this training paradigm is well suited for the medical domain where annotations are scarce, large-scale pre-training in the medical domain, and in particular pathology, has not been extensively studied. Previous work in self-supervised learning in pathology has leveraged smaller datasets for both pre-training and evaluating downstream performance. The aim of this project is to train the largest academic foundation model and benchmark the most prominent self-supervised learning algorithms by pre-training and evaluating downstream performance on large clinical pathology datasets. We collected the largest pathology dataset to date, consisting of over 3 billion images from over 423 thousand microscopy slides. We compared pre-training of visual transformer models using the masked autoencoder (MAE) and D
    
[^68]: 使用细粒度特征进行面部伪造的Deepfake检测

    Facial Forgery-based Deepfake Detection using Fine-Grained Features. (arXiv:2310.07028v1 [cs.CV])

    [http://arxiv.org/abs/2310.07028](http://arxiv.org/abs/2310.07028)

    该论文提出了一种基于细粒度特征的Deepfake检测方法，通过抑制背景噪声和学习区分性特征来提高检测的效果。

    

    通过Deepfake的面部伪造已经引发了重大的安全风险并引起了严重的社会关切。为了应对这一问题，提出了许多Deepfake检测方法。其中大多数方法将Deepfake检测建模为使用预训练的骨干卷积神经网络(CNN)架构进行二分类的问题。这些基于CNN的方法在Deepfake检测方面表现出非常高的效果，曲线下面积(AUC)高达0.99。然而，当被应用于不同数据集和Deepfake操纵技术进行评估时，这些方法的性能会显著下降。因此，我们需要学习更加微妙、局部和有辨别力的特征来进行Deepfake检测。本文将Deepfake检测问题视为细粒度分类问题，并提出了一个新的细粒度解决方案。具体而言，我们的方法通过有效地抑制背景噪声和学习区分性特征来学习微妙且具有一般性的特征。

    Facial forgery by deepfakes has caused major security risks and raised severe societal concerns. As a countermeasure, a number of deepfake detection methods have been proposed. Most of them model deepfake detection as a binary classification problem using a backbone convolutional neural network (CNN) architecture pretrained for the task. These CNN-based methods have demonstrated very high efficacy in deepfake detection with the Area under the Curve (AUC) as high as $0.99$. However, the performance of these methods degrades significantly when evaluated across datasets and deepfake manipulation techniques. This draws our attention towards learning more subtle, local, and discriminative features for deepfake detection. In this paper, we formulate deepfake detection as a fine-grained classification problem and propose a new fine-grained solution to it. Specifically, our method is based on learning subtle and generalizable features by effectively suppressing background noise and learning di
    
[^69]: NEWTON: 大型语言模型能够进行物理推理吗？

    NEWTON: Are Large Language Models Capable of Physical Reasoning?. (arXiv:2310.07018v1 [cs.CL])

    [http://arxiv.org/abs/2310.07018](http://arxiv.org/abs/2310.07018)

    NEWTON是一个用于评估大型语言模型物理推理能力的仓库和基准，包含2800个物体-属性对和160K个问答问题。

    

    通过其语境化表示，大型语言模型（LLM）被实证地证明包含句法、语义、词义和常识知识。然而，对于它们在物理推理能力方面的探索还有一定限制，特别是涉及理解日常物体的关键属性。为了解决这一问题，我们引入了NEWTON，一个用于评估LLM的物理推理能力的仓库和基准。此外，为了实现此基准的领域特定适应，我们提供了一种流程，使研究人员能够生成一个根据他们应用程序相关物体和属性定制的基准变体。NEWTON仓库包括2800个物体-属性对的集合，为生成无限规模的评估模板奠定基础。NEWTON基准包含160K个问答问题，使用NEWTON仓库策划，以调查物理推理能力。

    Large Language Models (LLMs), through their contextualized representations, have been empirically proven to encapsulate syntactic, semantic, word sense, and common-sense knowledge. However, there has been limited exploration of their physical reasoning abilities, specifically concerning the crucial attributes for comprehending everyday objects. To address this gap, we introduce NEWTON, a repository and benchmark for evaluating the physics reasoning skills of LLMs. Further, to enable domain-specific adaptation of this benchmark, we present a pipeline to enable researchers to generate a variant of this benchmark that has been customized to the objects and attributes relevant for their application. The NEWTON repository comprises a collection of 2800 object-attribute pairs, providing the foundation for generating infinite-scale assessment templates. The NEWTON benchmark consists of 160K QA questions, curated using the NEWTON repository to investigate the physical reasoning capabilities of
    
[^70]: 答案候选类型选择：闭书问答中的文本到文本语言模型满足知识图谱

    Answer Candidate Type Selection: Text-to-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs. (arXiv:2310.07008v1 [cs.CL])

    [http://arxiv.org/abs/2310.07008](http://arxiv.org/abs/2310.07008)

    本文提出了一种新颖的方法，通过对预训练的文本到文本问答系统生成的候选答案基于其类型进行过滤和重新排序，以解决在知识图谱问答任务中，模型容量有限且对于含有不太流行实体的问题质量下降的问题。

    

    预训练的文本到文本语言模型（如T5或BART）在知识图谱问答（KGQA）任务中取得了令人期待的结果。然而，模型的容量有限，对于包含不太流行实体的问题，质量下降。在本文中，我们提出了一种新颖的方法，该方法在预训练的文本到文本问答系统的基础上解决了这个问题。我们的简单而有效的方法根据候选答案的类型（来自Wikidata的"instance_of"属性）进行筛选和重新排序。

    Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield promising results in the Knowledge Graph Question Answering (KGQA) task. However, the capacity of the models is limited and the quality decreases for questions with less popular entities. In this paper, we present a novel approach which works on top of the pre-trained Text-to-Text QA system to address this issue. Our simple yet effective method performs filtering and re-ranking of generated candidates based on their types derived from Wikidata "instance_of" property.
    
[^71]: 通过利用生成技术实施开源LLM的灾难性越狱

    Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation. (arXiv:2310.06987v1 [cs.CL])

    [http://arxiv.org/abs/2310.06987](http://arxiv.org/abs/2310.06987)

    本文介绍了一种生成攻击技术，通过操纵解码方法的变化，可以导致开源LLMs的灾难性越狱，将错位率从0%提高到了超过95%。这项攻击方法简单而有效，在11个语言模型中表现优于最先进的攻击方法，且计算量降低了30倍。

    

    开源大规模语言模型（LLMs）的快速发展极大地推进了人工智能的发展。在发布模型之前，人们进行了大量的努力，以确保模型与人类价值观的一致性，主要目标是确保其有益且无害。然而，即使经过精心对齐的模型也可能被恶意操纵，导致意外行为，即所谓的“越狱”。这些越狱通常由特定的文本输入触发，通常称为对抗性提示。在这项工作中，我们提出了一种生成攻击技术，这是一种极其简单的方法，仅通过操纵解码方法的变化来扰乱模型的对齐。通过利用不同的生成策略，包括变化的解码超参数和采样方法，我们将11个语言模型，包括LLaMA2、Vicuna、Falcon和MPT家族的错位率从0%提高到了95%以上，超过了最先进的攻击方法，计算量降低了30倍。

    The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as "jailbreaks". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the misalignment rate from 0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks with $30\times$ lower computat
    
[^72]: 关于基于部分原型的分类器的可解释性：人类中心分析

    On the Interpretability of Part-Prototype Based Classifiers: A Human Centric Analysis. (arXiv:2310.06966v1 [cs.CV])

    [http://arxiv.org/abs/2310.06966](http://arxiv.org/abs/2310.06966)

    本论文提出了一个用于从人类角度评估基于部分原型模型可解释性的框架，并使用亚马逊机械土耳其进行了广泛实验。这些实验不仅展示了框架在评估各种基于部分原型模型的可解释性方面的能力，还是最全面的工作。

    

    最近，基于部分原型的网络已成为许多黑盒图像分类器的可解释性替代方法。然而，从人类用户的角度来看，这些方法的可解释性还未得到充分探索。在这项工作中，我们设计了一个从人类角度评估基于部分原型模型可解释性的框架。该框架由三个可操作的度量和实验组成。为了展示我们的框架的实用性，我们使用了亚马逊机械土耳其进行了一系列广泛的实验。它们不仅展示了我们的框架在评估各种基于部分原型模型的可解释性方面的能力，而且是我们所知的在一个统一的框架中评估这些方法最全面的工作。

    Part-prototype networks have recently become methods of interest as an interpretable alternative to many of the current black-box image classifiers. However, the interpretability of these methods from the perspective of human users has not been sufficiently explored. In this work, we have devised a framework for evaluating the interpretability of part-prototype-based models from a human perspective. The proposed framework consists of three actionable metrics and experiments. To demonstrate the usefulness of our framework, we performed an extensive set of experiments using Amazon Mechanical Turk. They not only show the capability of our framework in assessing the interpretability of various part-prototype-based models, but they also are, to the best of our knowledge, the most comprehensive work on evaluating such methods in a unified framework.
    
[^73]: 大型语言模型稀疏微调的推理加速

    Sparse Finetuning for Inference Acceleration of Large Language Models. (arXiv:2310.06927v1 [cs.CL])

    [http://arxiv.org/abs/2310.06927](http://arxiv.org/abs/2310.06927)

    本论文研究了大型语言模型的准确稀疏微调问题，提出了基于L2范数的蒸馏方法SquareHead，可以在高稀疏性下实现准确的恢复；同时展示了稀疏语言模型的实际效率，可在CPU和GPU运行时实现加速，并且观察到在受内存限制的模型中，稀疏性也可用于减少内存带宽。

    

    我们考虑在训练过的大型语言模型上进行精确的稀疏微调，即在专门任务上对预训练的语言模型进行微调，同时在权重上引入稀疏性。在准确性方面，我们观察到基于损失的标准微调可能无法恢复准确性，特别是在高稀疏情况下。为了解决这个问题，我们对蒸馏类型的损失进行了详细研究，确定了一种基于L2范数的蒸馏方法，我们称之为SquareHead，即使在更高的稀疏性下，它也能实现准确的恢复，适用于所有模型类型。在实际效率方面，我们展示了稀疏语言模型可以通过利用稀疏性在CPU和GPU运行时实现加速。虽然标准方法是利用稀疏性进行计算减少，但我们观察到，在受内存限制的语言模型中，稀疏性也可以用于减少内存带宽。我们展示了由于稀疏性导致的速度提升以及恢复准确性的端到端结果，应用于T5 (语言翻译)任务中。

    We consider the problem of accurate sparse finetuning of large language models (LLMs), that is, finetuning pretrained LLMs on specialized tasks, while inducing sparsity in their weights. On the accuracy side, we observe that standard loss-based finetuning may fail to recover accuracy, especially at high sparsities. To address this, we perform a detailed study of distillation-type losses, determining an L2-based distillation approach we term SquareHead which enables accurate recovery even at higher sparsities, across all model types. On the practical efficiency side, we show that sparse LLMs can be executed with speedups by taking advantage of sparsity, for both CPU and GPU runtimes. While the standard approach is to leverage sparsity for computational reduction, we observe that in the case of memory-bound LLMs sparsity can also be leveraged for reducing memory bandwidth. We exhibit end-to-end results showing speedups due to sparsity, while recovering accuracy, on T5 (language translati
    
[^74]: PICProp：用于不确定性量化的物理信息置信传播

    PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification. (arXiv:2310.06923v1 [cs.AI])

    [http://arxiv.org/abs/2310.06923](http://arxiv.org/abs/2310.06923)

    本文提出了一种名为PICProp的方法，基于双层优化，用于在深度学习和物理信息学习中进行不确定性量化。该方法能够在不进行强大假设的情况下计算有效的置信区间（CI），并且通过传播置信度实现了数据位置到整个域的置信度传播。

    

    在深度学习和物理信息学习中，标准的不确定性量化方法存在着持久的局限性。特别是，需要对数据的可能性做出强烈的假设，性能在很大程度上取决于先验的选择，并且后验只能以近似的方式进行采样，从而由于相关的计算成本导致近似精度较差。本文提出并研究了对确定性偏微分方程的置信区间（CI）估计作为一个新的问题。即，在整个区域中以概率担保的形式传播置信度，以达到数据位置到整个域的置信度传播。我们提出了一种名为物理信息置信传播（PICProp）的方法，基于双层优化来计算一个有效的CI，而不需要进行大量的假设。我们提供了关于我们方法有效性的定理以及针对物理信息学习的计算实验。

    Standard approaches for uncertainty quantification in deep learning and physics-informed learning have persistent limitations. Indicatively, strong assumptions regarding the data likelihood are required, the performance highly depends on the selection of priors, and the posterior can be sampled only approximately, which leads to poor approximations because of the associated computational cost. This paper introduces and studies confidence interval (CI) estimation for deterministic partial differential equations as a novel problem. That is, to propagate confidence, in the form of CIs, from data locations to the entire domain with probabilistic guarantees. We propose a method, termed Physics-Informed Confidence Propagation (PICProp), based on bi-level optimization to compute a valid CI without making heavy assumptions. We provide a theorem regarding the validity of our method, and computational experiments, where the focus is on physics-informed learning.
    
[^75]: 利用第四代英特尔韧性处理器进行分布式迁移学习

    Distributed Transfer Learning with 4th Gen Intel Xeon Processors. (arXiv:2310.06916v1 [cs.DC])

    [http://arxiv.org/abs/2310.06916](http://arxiv.org/abs/2310.06916)

    本文研究了如何利用第四代英特尔韧性处理器进行分布式迁移学习，通过使用英特尔高级矩阵扩展（AMX）和Horovod在Image Classification TensorFlow数据集上实现了接近最先进的图像分类准确性。

    

    本文探讨了如何利用迁移学习和英特尔韧性处理器，特别是第四代英特尔韧性可扩展处理器，打破了传统观念，即训练主要依赖于GPU。我们通过使用英特尔高级矩阵扩展（AMX）和Horovod进行分布式训练，在公开可用的Image Classification TensorFlow数据集上实现了接近最先进的图像分类准确性的案例研究。

    In this paper, we explore how transfer learning, coupled with Intel Xeon, specifically 4th Gen Intel Xeon scalable processor, defies the conventional belief that training is primarily GPU-dependent. We present a case study where we achieved near state-of-the-art accuracy for image classification on a publicly available Image Classification TensorFlow dataset using Intel Advanced Matrix Extensions(AMX) and distributed training with Horovod.
    
[^76]: 在具有轨迹优化的安全嵌入式MDP中的强化学习

    Reinforcement Learning in a Safety-Embedded MDP with Trajectory Optimization. (arXiv:2310.06903v1 [cs.RO])

    [http://arxiv.org/abs/2310.06903](http://arxiv.org/abs/2310.06903)

    本文介绍了一种在安全嵌入式MDP中结合轨迹优化的强化学习方法，通过将安全约束嵌入动作空间，能够有效地在最大化奖励和遵守安全约束之间取得平衡，并在挑战性任务中取得了优异性能。

    

    安全强化学习在将强化学习算法应用于安全关键的实际应用中起着重要的作用，解决了最大化奖励和遵守安全约束之间的权衡。本文介绍了一种新颖的方法，将强化学习与轨迹优化相结合，有效地管理这种权衡。我们的方法将安全约束嵌入到修改后的马尔可夫决策过程（MDP）的动作空间中。强化学习代理通过轨迹优化器产生一系列行动，这些行动转化为安全轨迹，从而有效确保安全并提高训练稳定性。这种新颖的方法在挑战性的Safety Gym任务的性能方面表现出色，在推理过程中实现了显著更高的奖励和几乎零的安全违规。该方法在一个真实机器人任务中的应用性得到了证明，该任务涉及推动箱子穿过障碍物。

    Safe Reinforcement Learning (RL) plays an important role in applying RL algorithms to safety-critical real-world applications, addressing the trade-off between maximizing rewards and adhering to safety constraints. This work introduces a novel approach that combines RL with trajectory optimization to manage this trade-off effectively. Our approach embeds safety constraints within the action space of a modified Markov Decision Process (MDP). The RL agent produces a sequence of actions that are transformed into safe trajectories by a trajectory optimizer, thereby effectively ensuring safety and increasing training stability. This novel approach excels in its performance on challenging Safety Gym tasks, achieving significantly higher rewards and near-zero safety violations during inference. The method's real-world applicability is demonstrated through a safe and effective deployment in a real robot task of box-pushing around obstacles.
    
[^77]: 九天智能网络仿真平台设计

    Design of JiuTian Intelligent Network Simulation Platform. (arXiv:2310.06858v1 [cs.NI])

    [http://arxiv.org/abs/2310.06858](http://arxiv.org/abs/2310.06858)

    本文介绍了九天智能网络仿真平台，提供了无线通信仿真数据服务，包含可扩展的仿真器功能，支持开放服务和强化学习算法训练，并允许用户上传和更新参数配置。未来研究方向包括更深入地探索业务场景和优化任务。

    

    本文介绍了九天智能网络仿真平台，该平台可以为开放创新平台提供无线通信仿真数据服务。该平台包含一系列可扩展的仿真器功能，提供开放服务，使用户能够基于仿真环境和数据使用强化学习算法进行模型训练和推理。此外，它允许用户通过上传和更新参数配置来解决不同场景下的优化任务。本文主要从背景、整体架构、仿真器、业务场景和未来方向等方面介绍了该平台及其开放服务。

    This paper introduced the JiuTian Intelligent Network Simulation Platform, which can provide wireless communication simulation data services for the Open Innovation Platform. The platform contains a series of scalable simulator functionalities, offering open services that enable users to use reinforcement learning algorithms for model training and inference based on simulation environments and data. Additionally, it allows users to address optimization tasks in different scenarios by uploading and updating parameter configurations. The platform and its open services were primarily introduced from the perspectives of background, overall architecture, simulator, business scenarios, and future directions.
    
[^78]: 勇敢的新世界：人工智能在教学与学习中的应用

    Brave new world: Artificial Intelligence in teaching and learning. (arXiv:2310.06856v1 [cs.CY])

    [http://arxiv.org/abs/2310.06856](http://arxiv.org/abs/2310.06856)

    本文展示了在教学和学习中如何使用大型语言模型，并讨论了教育领域中已经发生的人工智能事件。我们主张引入人工智能政策和规范，以提高教育工具的意识，并减少教育中的人工智能事件风险。

    

    我们举例说明了大型语言模型在教学和学习中的应用。我们还讨论了教育领域已经发生的人工智能事件，并提出迫切需要在大学引入人工智能政策以及进行规范人工智能的持续策略。关于人工智能政策，我们认为每个机构都应该制定一项关于人工智能在教学和学习中的政策。这一点至关重要，至少有两个原因：（一）提高对各种教育工具的意识，这些工具既可以积极地，也可能对教育产生负面影响；（二）最小化教育中的人工智能事件风险。

    We exemplify how Large Language Models are used in both teaching and learning. We also discuss the AI incidents that have already occurred in the education domain, and we argue for the urgent need to introduce AI policies in universities and for the ongoing strategies to regulate AI. Regarding policy for AI, our view is that each institution should have a policy for AI in teaching and learning. This is important from at least twofolds: (i) to raise awareness on the numerous educational tools that can both positively and negatively affect education; (ii) to minimise the risk of AI incidents in education.
    
[^79]: 基于遗传算法的动态后门攻击对于基于联邦学习的网络流量分类的研究

    Genetic Algorithm-Based Dynamic Backdoor Attack on Federated Learning-Based Network Traffic Classification. (arXiv:2310.06855v1 [cs.CR])

    [http://arxiv.org/abs/2310.06855](http://arxiv.org/abs/2310.06855)

    本论文研究了基于联邦学习的网络流量分类中的动态后门攻击，并提出了一种基于遗传算法的新型后门攻击方法GABAttack。该方法能够优化后门触发器参数的值和位置，以实现对全局模型的篡改。

    

    联邦学习使多个客户端能够共同为由中央服务器组织的全局模型的学习做出贡献。这种学习方案促进了客户端的数据隐私并减少了通信开销。在网络流量分类等应用中，这有助于隐藏网络的漏洞和弱点。然而，联邦学习对后门攻击是脆弱的，即对手向全局模型中注入篡改的模型更新。这些更新在全局模型中注入了显著的功能，可以通过特定的输入模式来启动。尽管如此，基于联邦学习的网络流量分类模型对于这些攻击的脆弱性仍未被探索。在本文中，我们提出了GABAttack，一种基于遗传算法的针对基于联邦学习的网络流量分类的新型后门攻击方法。GABAttack利用遗传算法来优化后门触发器参数的值和位置。

    Federated learning enables multiple clients to collaboratively contribute to the learning of a global model orchestrated by a central server. This learning scheme promotes clients' data privacy and requires reduced communication overheads. In an application like network traffic classification, this helps hide the network vulnerabilities and weakness points. However, federated learning is susceptible to backdoor attacks, in which adversaries inject manipulated model updates into the global model. These updates inject a salient functionality in the global model that can be launched with specific input patterns. Nonetheless, the vulnerability of network traffic classification models based on federated learning to these attacks remains unexplored. In this paper, we propose GABAttack, a novel genetic algorithm-based backdoor attack against federated learning for network traffic classification. GABAttack utilizes a genetic algorithm to optimize the values and locations of backdoor trigger pa
    
[^80]: 无干扰标签学习在人类跌倒事件分类中的应用：联合合作训练与三位一体网络

    Learning with Noisy Labels for Human Fall Events Classification: Joint Cooperative Training with Trinity Networks. (arXiv:2310.06854v1 [cs.CV])

    [http://arxiv.org/abs/2310.06854](http://arxiv.org/abs/2310.06854)

    联合提出的JoCoT方法在人类跌倒事件分类中应用噪声标签学习，通过使用两个教师模块和一个学生模块，提高了鲁棒性和性能，并且采用人体骨架数据以保护隐私。

    

    随着人口老龄化问题的加剧，跌倒事件的分类引起了广泛的研究关注。在深度学习的发展中，数据标签的质量至关重要。大多数数据集都是自动或半自动标记的，样本可能存在标注错误，这限制了深度神经网络（DNN）的性能。最近关于噪声标签学习的研究证实了神经网络在训练阶段首先关注干净简单的实例，然后关注噪声和困难的实例。为了解决学习噪声标签的问题并保护人类受试者的隐私，我们提出了一种简单但有效的方法，称为联合合作训练与三位一体网络（JoCoT）。为了减轻隐私问题，我们使用人体骨架数据。在提出的JoCoT中，通过使用两个教师模块和一个学生模块来改善噪声标签学习框架的鲁棒性和性能。为了减轻错误选择的问题，预测过程和rae的选择遵循了固有的约束。

    With the increasing ageing population, fall events classification has drawn much research attention. In the development of deep learning, the quality of data labels is crucial. Most of the datasets are labelled automatically or semi-automatically, and the samples may be mislabeled, which constrains the performance of Deep Neural Networks (DNNs). Recent research on noisy label learning confirms that neural networks first focus on the clean and simple instances and then follow the noisy and hard instances in the training stage. To address the learning with noisy label problem and protect the human subjects' privacy, we propose a simple but effective approach named Joint Cooperative training with Trinity Networks (JoCoT). To mitigate the privacy issue, human skeleton data are used. The robustness and performance of the noisy label learning framework is improved by using the two teacher modules and one student module in the proposed JoCoT. To mitigate the incorrect selections, the predicti
    
[^81]: BodyFormer: 基于Transformer的语义引导的3D人体手势合成

    BodyFormer: Semantics-guided 3D Body Gesture Synthesis with Transformer. (arXiv:2310.06851v1 [cs.CV])

    [http://arxiv.org/abs/2310.06851](http://arxiv.org/abs/2310.06851)

    本论文提出了一个基于Transformer的框架，用于自动从语音中合成3D人体手势。通过引入变分Transformer和模态位置嵌入层，可以有效地学习并生成多样化的手势。另外，通过内部模态预训练方案来解决数据稀缺问题。

    

    自动从语音中合成手势是一个吸引研究人员关注的话题，用于远程通信、视频游戏和元宇宙应用。由于问题的随机性和训练所需的丰富跨模态数据集的缺乏，学习语音和3D全身手势之间的映射是困难的。在本文中，我们提出了一种基于Transformer的新颖框架，用于自动从语音生成3D人体手势。为了学习语音时的手势的随机性，我们提出了一种变分Transformer，可以有效地建模手势的概率分布，在推理时能够产生多样化的手势。此外，我们引入了一种模态位置嵌入层，用于捕捉不同语音模式中的不同运动速度。为了应对数据稀缺问题，我们设计了一种内部模态预训练方案，可以从有限的数据中学习语音和3D手势之间的复杂映射。我们的系统

    Automatic gesture synthesis from speech is a topic that has attracted researchers for applications in remote communication, video games and Metaverse. Learning the mapping between speech and 3D full-body gestures is difficult due to the stochastic nature of the problem and the lack of a rich cross-modal dataset that is needed for training. In this paper, we propose a novel transformer-based framework for automatic 3D body gesture synthesis from speech. To learn the stochastic nature of the body gesture during speech, we propose a variational transformer to effectively model a probabilistic distribution over gestures, which can produce diverse gestures during inference. Furthermore, we introduce a mode positional embedding layer to capture the different motion speeds in different speaking modes. To cope with the scarcity of data, we design an intra-modal pre-training scheme that can learn the complex mapping between the speech and the 3D gesture from a limited amount of data. Our system
    
[^82]: DeepTriNet:一种基于三级注意力的DeepLabv3+结构用于卫星图像的语义分割

    DeepTriNet: A Tri-Level Attention Based DeepLabv3+ Architecture for Semantic Segmentation of Satellite Images. (arXiv:2310.06848v1 [cs.CV])

    [http://arxiv.org/abs/2310.06848](http://arxiv.org/abs/2310.06848)

    DeepTriNet是一种基于三级注意力的DeepLabv3+架构，用于卫星图像的语义分割。该方法通过结合SENets和TAUs桥接了编解码器输出与相关特征之间的语义差距，同时通过自我监督确定了更重要和更通用的特征。

    

    卫星图像的分割在遥感应用中至关重要。现有方法在卫星图像的语义分割中面临识别小尺度目标的挑战，主要原因是忽略了底层网络的低级特征和由不同特征图包含不同数量信息。因此，在本研究中，提出了一种基于三级注意力的DeepLabv3+架构（DeepTriNet）用于卫星图像的语义分割。所提出的混合方法结合了挤压激励网络（SENets）和三级注意力单元（TAUs）与普通的DeepLabv3+架构，其中TAUs用于弥合编解码器输出与SENets用于给相关特征分配更多权重之间的语义特征差距。所提出的DeepTriNet通过自我监督找到哪些特征是更相关且更通用的，而不是我们对它们进行注释。研究表明，所提出的DeepTriNet具有较好的性能。

    The segmentation of satellite images is crucial in remote sensing applications. Existing methods face challenges in recognizing small-scale objects in satellite images for semantic segmentation primarily due to ignoring the low-level characteristics of the underlying network and due to containing distinct amounts of information by different feature maps. Thus, in this research, a tri-level attention-based DeepLabv3+ architecture (DeepTriNet) is proposed for the semantic segmentation of satellite images. The proposed hybrid method combines squeeze-and-excitation networks (SENets) and tri-level attention units (TAUs) with the vanilla DeepLabv3+ architecture, where the TAUs are used to bridge the semantic feature gap among encoders output and the SENets used to put more weight on relevant features. The proposed DeepTriNet finds which features are the more relevant and more generalized way by its self-supervision rather we annotate them. The study showed that the proposed DeepTriNet perfor
    
[^83]: 利用语言模型作为认知智能体的知识来源的研究

    Exploiting Language Models as a Source of Knowledge for Cognitive Agents. (arXiv:2310.06846v1 [cs.AI])

    [http://arxiv.org/abs/2310.06846](http://arxiv.org/abs/2310.06846)

    本研究利用语言模型作为认知智能体的任务知识来源，探索了将语言模型作为外部知识源用于认知系统的挑战和机会，并提出了通过整合知识提取和认知架构能力来提高知识提取效果的可能方法。

    

    大型语言模型(LLMs)不仅可以完成句子补全，还可以进行问答、摘要和自然语言推理等任务。我们的研究利用语言模型作为认知智能体的任务知识来源，即通过认知架构实现的智能体。我们识别了使用语言模型作为认知系统外部知识源的挑战和机会，以及通过将知识提取与认知架构能力整合来提高知识提取效果的可能方法，并举例介绍我们最近在这一领域的工作。

    Large language models (LLMs) provide capabilities far beyond sentence completion, including question answering, summarization, and natural-language inference. While many of these capabilities have potential application to cognitive systems, our research is exploiting language models as a source of task knowledge for cognitive agents, that is, agents realized via a cognitive architecture. We identify challenges and opportunities for using language models as an external knowledge source for cognitive systems and possible ways to improve the effectiveness of knowledge extraction by integrating extraction with cognitive architecture capabilities, highlighting with examples from our recent work in this area.
    
[^84]: RobustEdge：面向云边系统的低功耗对抗检测

    RobustEdge: Low Power Adversarial Detection for Cloud-Edge Systems. (arXiv:2310.06845v1 [cs.CR])

    [http://arxiv.org/abs/2310.06845](http://arxiv.org/abs/2310.06845)

    RobustEdge是一种低功耗、适用于边缘设备的对抗检测方法，解决了在云边系统中能量浪费和计算开销问题。

    

    在实际的云边场景中，边缘设备进行数据采集，拥有足够资源的云系统使用深度神经网络进行推理任务，对抗鲁棒性对于可靠性和普及部署至关重要。对抗检测是先前文献中使用的主要对抗防御技术。然而，在先前的检测方法中，检测器附加在分类器模型上，检测器和分类器一起进行对抗检测，这需要很高的计算开销，在低功耗边缘设备上不可行。因此，先前的方法只能在云端进行对抗检测，而不能在边缘设备上进行。这意味着在遭受对抗攻击时，不利的对抗样本必须传送到云端，导致边缘设备的能量浪费。因此，需要一种低功耗、适用于边缘设备的对抗检测方法来提高能量效率。

    In practical cloud-edge scenarios, where a resource constrained edge performs data acquisition and a cloud system (having sufficient resources) performs inference tasks with a deep neural network (DNN), adversarial robustness is critical for reliability and ubiquitous deployment. Adversarial detection is a prime adversarial defence technique used in prior literature. However, in prior detection works, the detector is attached to the classifier model and both detector and classifier work in tandem to perform adversarial detection that requires a high computational overhead which is not available at the low-power edge. Therefore, prior works can only perform adversarial detection at the cloud and not at the edge. This means that in case of adversarial attacks, the unfavourable adversarial samples must be communicated to the cloud which leads to energy wastage at the edge device. Therefore, a low-power edge-friendly adversarial detection method is required to improve the energy efficiency
    
[^85]: Meta-CoT:大规模语言模型在混合任务场景中的通用思维链提示

    Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models. (arXiv:2310.06692v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.06692](http://arxiv.org/abs/2310.06692)

    Meta-CoT是一种在混合任务场景中能够通用思维链提示的方法，在十个公共基准推理任务中表现出卓越的性能和优越的泛化能力。

    

    大规模语言模型（LLM）通过利用链式思维提示展示出了卓越的推理能力，这种提示生成中间推理链以作为得出答案的基本理由。然而，目前的CoT方法要么仅仅使用类似“让我们逐步思考”的通用提示，要么过于依赖手工设计的任务特定演示来达到理想的性能，从而导致性能和泛化之间的不可避免的鸿沟。为了弥合这一鸿沟，我们提出了Meta-CoT，一种在未知输入问题类型的混合任务场景中具有通用性的CoT提示方法。Meta-CoT首先根据输入问题对场景进行分类，然后以自动模式从相应的数据池中构建多样的演示。Meta-CoT在十个公共基准推理任务上表现出卓越的性能和优越的泛化能力。值得注意的是，Meta-CoT实现了最新技术水平。

    Large language models (LLMs) have unveiled remarkable reasoning capabilities by exploiting chain-of-thought (CoT) prompting, which generates intermediate reasoning chains to serve as the rationale for deriving the answer. However, current CoT methods either simply employ general prompts such as Let's think step by step, or heavily rely on handcrafted task-specific demonstrations to attain preferable performances, thereby engendering an inescapable gap between performance and generalization. To bridge this gap, we propose Meta-CoT, a generalizable CoT prompting method in mixed-task scenarios where the type of input questions is unknown. Meta-CoT firstly categorizes the scenario based on the input question and subsequently constructs diverse demonstrations from the corresponding data pool in an automatic pattern. Meta-CoT simultaneously enjoys remarkable performances on ten public benchmark reasoning tasks and superior generalization capabilities. Notably, Meta-CoT achieves the state-of-
    
[^86]: 用多样化反馈构建大型语言模型的对齐方法

    Constructive Large Language Models Alignment with Diverse Feedback. (arXiv:2310.06450v1 [cs.CL])

    [http://arxiv.org/abs/2310.06450](http://arxiv.org/abs/2310.06450)

    本文提出了一种新的方法，即建构性和多样化反馈（CDF），用于增强大型语言模型（LLM）的对齐效果。我们通过收集不同类型的反馈，并根据问题的难度级别进行处理，实现了更好的性能。

    

    在大型语言模型（LLMs）的研究中，对其与人类价值观的对齐越来越重视，以减少有害内容的影响。然而，当前的对齐方法通常仅依赖于人类反馈的单一形式，如偏好、注释标签或自然语言批评，忽视了结合这些反馈类型的潜在优势。这种限制导致性能不佳，即使有丰富的训练数据。本文引入了建构性和多样化反馈（CDF）作为增强LLM对齐的新方法，受建构学习理论的启发。我们的方法涉及收集适用于训练数据集中不同难度问题的三种不同类型的反馈。具体而言，我们利用批评反馈解决简单问题，利用改进反馈解决中等问题，利用偏好反馈解决困难问题。通过用这种多样化反馈训练我们的模型，我们获得了更好的表现。

    In recent research on large language models (LLMs), there has been a growing emphasis on aligning these models with human values to reduce the impact of harmful content. However, current alignment methods often rely solely on singular forms of human feedback, such as preferences, annotated labels, or natural language critiques, overlooking the potential advantages of combining these feedback types. This limitation leads to suboptimal performance, even when ample training data is available. In this paper, we introduce Constructive and Diverse Feedback (CDF) as a novel method to enhance LLM alignment, inspired by constructivist learning theory. Our approach involves collecting three distinct types of feedback tailored to problems of varying difficulty levels within the training dataset. Specifically, we exploit critique feedback for easy problems, refinement feedback for medium problems, and preference feedback for hard problems. By training our model with this diversified feedback, we a
    
[^87]: 超级关注力：近似线性时间下的长上下文注意力机制

    HyperAttention: Long-context Attention in Near-Linear Time. (arXiv:2310.05869v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.05869](http://arxiv.org/abs/2310.05869)

    近似注意力机制HyperAttention解决了在大型语言模型中使用的长上下文的计算挑战，并通过引入两个参数来衡量问题的难度。HyperAttention具有模块化设计，可轻松集成其他快速低级实现。

    

    我们提出了一种名为HyperAttention的近似注意力机制，以应对在大型语言模型（LLMs）中使用的长上下文的日益复杂的计算挑战。最近的研究表明，在最坏情况下，除非注意力矩阵的条目被限制或矩阵具有低稳定秩，否则二次时间是必要的。我们引入了两个参数，用于衡量：（1）标准化注意力矩阵中的最大列范数，以及（2）在检测和删除大条目后，非标准化注意力矩阵中行范数的比率。我们使用这些细粒度的参数来捕捉问题的难度。尽管先前存在下界，但我们能够实现一个线性时间的采样算法，即使矩阵具有无界的条目或较大的稳定秩，只要上述参数较小。HyperAttention具有模块化设计，轻松容纳其他快速低级实现，特别是FlashAttention。

    We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention.
    
[^88]: 大型语言模型是事后解释器吗？

    Are Large Language Models Post Hoc Explainers?. (arXiv:2310.05797v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05797](http://arxiv.org/abs/2310.05797)

    这项工作提出了第一个研究大型语言模型（LLMs）解释其他预测模型有效性的框架，并且提出了多个提示策略，填补了当前对于LLMs在解释其他模型行为方面的缺失。

    

    大型语言模型（LLM）越来越被广泛应用于各种自然语言处理（NLP）应用中。最近的一项创新，即上下文学习（ICL），使得LLM能够在推理阶段通过在提示中提供少量示例来学习新任务，从而消除了模型微调的需要。虽然LLM已经被应用于多个领域，但其在解释其他模型行为方面的适用性仍相对未被探索。尽管存在越来越多的新解释技术，但很多技术要求对模型具有白盒访问权限和/或计算成本较高，凸显了下一代事后解释器的需求。在这项工作中，我们提出了第一个研究LLM解释其他预测模型有效性的框架。具体而言，我们提出了一个包含多种提示策略的新颖框架：i）基于扰动的ICL，ii）基于预测的ICL，iii）基于指令的ICL，和iv）基于解释的ICL。

    Large Language Models (LLMs) are increasingly used as powerful tools for a plethora of natural language processing (NLP) applications. A recent innovation, in-context learning (ICL), enables LLMs to learn new tasks by supplying a few examples in the prompt during inference time, thereby eliminating the need for model fine-tuning. While LLMs have been utilized in several applications, their applicability in explaining the behavior of other models remains relatively unexplored. Despite the growing number of new explanation techniques, many require white-box access to the model and/or are computationally expensive, highlighting a need for next-generation post hoc explainers. In this work, we present the first framework to study the effectiveness of LLMs in explaining other predictive models. More specifically, we propose a novel framework encompassing multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL, and iv) Explanation-based I
    
[^89]: 从法律事实中自动生成论证的自动化系统。

    Automated Argument Generation from Legal Facts. (arXiv:2310.05680v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.05680](http://arxiv.org/abs/2310.05680)

    该论文研究了利用开源大型语言模型的生成能力，从法律案件中自动生成论证。实验结果显示，最佳方法生成的论证与基准集的黄金标准注释平均重叠率为63%。

    

    在全球范围内，待处理案件数量呈指数增长（例如，仅在印度就有超过1000万个待处理案件）。主要问题在于提交给法律系统的案件数量远远超过一个国家中现有的法律专业人员的数量。在这个全球背景下，利用人工智能技术来提高法律程序的效率和速度变得至关重要。在本研究中，我们特别关注帮助法律专业人员分析法律案件的过程。我们的具体调查探索了利用开源大型语言模型的生成能力，从法律案件中提取出论证。实验结果表明，最佳方法生成的论证与基准集的黄金标准注释平均重叠率为63%。

    The count of pending cases has shown an exponential rise across nations (e.g., with more than 10 million pending cases in India alone). The main issue lies in the fact that the number of cases submitted to the law system is far greater than the available number of legal professionals present in a country. Given this worldwide context, the utilization of AI technology has gained paramount importance to enhance the efficiency and speed of legal procedures. In this study we partcularly focus on helping legal professionals in the process of analyzing a legal case. Our specific investigation delves into harnessing the generative capabilities of open-sourced large language models to create arguments derived from the facts present in legal cases. Experimental results show that the generated arguments from the best performing method have on average 63% overlap with the benchmark set gold standard annotations.
    
[^90]: 关于使用LSTD和随机特征的强化学习中的双下降现象

    On Double-Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.05518](http://arxiv.org/abs/2310.05518)

    本文研究了在强化学习中网络大小和L2正则化对性能的影响，并观察到了双下降现象。通过使用随机特征和懒惰训练策略，在参数和状态数无限大的情况下研究了正则化的最小二乘时间差分算法，得出了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。

    

    时间差分算法在深度强化学习中被广泛使用，其性能受神经网络大小的影响。然而，在监督学习中过参数化和其带来的好处已经得到了很好的理解，但是在强化学习中情况则不太清楚。本文通过理论分析探讨了网络大小和L2正则化对性能的影响，并将参数个数与访问状态个数之比定义为关键因素，当该比值大于1时称为过参数化。此外，我们观察到了双下降现象，即在参数/状态比为1附近会突然性能下降。通过利用随机特征和懒惰训练策略，我们在无限大的参数和状态数下研究了正则化的最小二乘时间差分算法。我们推导了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。

    Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive 
    
[^91]: 对语义分割中经典的测试时适应方法的批判性探究

    A Critical Look at Classic Test-Time Adaptation Methods in Semantic Segmentation. (arXiv:2310.05341v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.05341](http://arxiv.org/abs/2310.05341)

    这项研究对语义分割中的经典测试时适应方法进行了批判性探究，揭示了分割TTA所面临的独特挑战，并发现经典TTA策略在这一任务中并不有效。

    

    测试时适应（TTA）旨在将最初在训练数据上训练的模型适应于测试数据中的可能分布变化。然而，大多数现有的TTA研究都集中在分类任务上，对于语义分割的TTA探索非常有限。这种对分类的突出重视可能导致许多新手和工程师错误地认为为分类设计的经典TTA方法可以直接应用于分割任务。然而，这一假设仍未经验证，是一个待解决的问题。为了解决这个问题，我们进行了一项系统的实证研究，揭示了分割TTA的独特挑战，并确定经典TTA策略是否可以有效应对这一任务。我们全面的结果得出了三个关键观察结果。首先，常用于分类TTA的经典批归一化更新策略只能带来轻微的性能改善，在某些情况下甚至会对结果产生逆向影响。

    Test-time adaptation (TTA) aims to adapt a model, initially trained on training data, to potential distribution shifts in the test data. Most existing TTA studies, however, focus on classification tasks, leaving a notable gap in the exploration of TTA for semantic segmentation. This pronounced emphasis on classification might lead numerous newcomers and engineers to mistakenly assume that classic TTA methods designed for classification can be directly applied to segmentation. Nonetheless, this assumption remains unverified, posing an open question. To address this, we conduct a systematic, empirical study to disclose the unique challenges of segmentation TTA, and to determine whether classic TTA strategies can effectively address this task. Our comprehensive results have led to three key observations. First, the classic batch norm updating strategy, commonly used in classification TTA, only brings slight performance improvement, and in some cases it might even adversely affect the resu
    
[^92]: 个性化随机鹦鹉更危险吗？评估对话系统中的人格偏见

    Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.05280](http://arxiv.org/abs/2310.05280)

    这项研究评估了对话系统中的人格偏见对社交偏见的影响，并建立了一个综合评估框架来衡量不同人格采用下的偏见程度。

    

    最近大型语言模型的发展使其能够按照自由形式的指令进行操作，包括在对话中模仿通用或特定人口群体的人格。通用人格指的是来自某一人口群体的个体（例如亚洲人），而特定人格可以是历史人物的实际姓名。虽然采用人格使对话系统更具吸引力和亲和力，但也存在潜在风险，可能通过与用户的交互而加剧社会偏见，进一步造成社会伤害。在本文中，我们系统地研究“人格偏见”，我们将其定义为有害对话模型行为对不同人格采用的敏感性。我们将人格偏见分为有害表达和有害认同两类，同时建立了一个全面的评估框架，以衡量五个方面的人格偏见：冒犯性、有毒延续、关怀、刻板印象的认同以及

    Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. Generic personas refer to an individual from a demographic group (e.g. an Asian person), whereas specific personas can be actual names of historical figures. While the adoption of personas allows dialogue systems to be more engaging and approachable to users, it also carries the potential risk of exacerbating social biases in model responses, further causing societal harms through interactions with users. In this paper, we systematically study "persona biases", which we define to be the sensitivity of harmful dialogue model behaviors to different persona adoptions. We categorize persona biases into biases in harmful expression and harmful agreement, as well as establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and To
    
[^93]: InstructDET: 通用指令的引导下的指称对象检测的多样化方法

    InstructDET: Diversifying Referring Object Detection with Generalized Instructions. (arXiv:2310.05136v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.05136](http://arxiv.org/abs/2310.05136)

    我们提出了一种名为InstructDET的方法，可以通过多样化的指令定位目标对象并进行指称对象检测。我们构建了一个包含图像、边界框和泛化指令的数据集，其中利用了视觉语言模型和大型语言模型生成指令。

    

    我们提出了InstructDET，一种基于用户指令来定位目标对象的指称对象检测（ROD）的数据中心方法。我们利用了多样化的指令，涵盖与对象检测相关的常见用户意图。对于一张图像，我们生成了大量的指令，涉及每个单独的对象和多个对象的不同组合。每个指令及其对应的对象边界框构成一个训练数据对。为了包含常见的检测表达式，我们采用了新兴的视觉语言模型（VLM）和大型语言模型（LLM），通过文本提示和对象边界框生成指令，因为基础模型的泛化能力可以产生类似人类的表达（例如，描述对象属性、类别和关系）。我们将构建的数据集命名为InDET，包含图像、边界框和泛化指令。

    We propose InstructDET, a data-centric method for referring object detection (ROD) that localizes target objects based on user instructions. While deriving from referring expressions (REC), the instructions we leverage are greatly diversified to encompass common user intentions related to object detection. For one image, we produce tremendous instructions that refer to every single object and different combinations of multiple objects. Each instruction and its corresponding object bounding boxes (bbxs) constitute one training data pair. In order to encompass common detection expressions, we involve emerging vision-language model (VLM) and large language model (LLM) to generate instructions guided by text prompts and object bbxs, as the generalizations of foundation models are effective to produce human-like expressions (e.g., describing object property, category, and relationship). We name our constructed dataset as InDET. It contains images, bbxs and generalized instructions that are 
    
[^94]: 跨不同条件精确预测电池寿命的学习细胞内和细胞间差异

    Learning Intra- and Inter-Cell Differences for Accurate Battery Lifespan Prediction across Diverse Conditions. (arXiv:2310.05052v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2310.05052](http://arxiv.org/abs/2310.05052)

    该论文介绍了一种跨不同条件精确预测电池寿命的方法，通过捕捉目标电池和参考电池之间的电信号差异，无论材料和老化条件如何，在扩展特征空间的同时为通用的电池寿命预测框架铺平了道路。

    

    电池寿命预测对电池研究和开发具有重要的实际价值。目前，许多数据驱动模型依赖于特定目标电池的早期电信号来预测它们的寿命。一个常见的不足是，大多数现有方法都是基于特定老化条件开发的，这不仅限制了它们的模型能力，而且降低了它们在预测不同条件下的退化效果。因此，这些模型通常无法充分利用其他条件下可用的丰富历史数据。为了解决这个问题，我们引入了一种方法，明确捕捉目标电池和参考电池之间的电信号差异，无论它们的材料和老化条件如何，来预测目标电池的寿命。通过这种细胞间差异，我们不仅扩展了特征空间，还为通用的电池寿命预测框架铺平了道路。显著的是，我们的方法能够在不同条件下精确预测电池寿命。

    Battery life prediction holds significant practical value for battery research and development. Currently, many data-driven models rely on early electrical signals from specific target batteries to predict their lifespan. A common shortfall is that most existing methods are developed based on specific aging conditions, which not only limits their model's capability but also diminishes their effectiveness in predicting degradation under varied conditions. As a result, these models often miss out on fully benefiting from the rich historical data available under other conditions. Here, to address above, we introduce an approach that explicitly captures differences between electrical signals of a target battery and a reference battery, irrespective of their materials and aging conditions, to forecast the target battery life. Through this inter-cell difference, we not only enhance the feature space but also pave the way for a universal battery life prediction framework. Remarkably, our mode
    
[^95]: 重新审视大型语言模型作为零-shot关系抽取器

    Revisiting Large Language Models as Zero-shot Relation Extractors. (arXiv:2310.05028v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.05028](http://arxiv.org/abs/2310.05028)

    本研究重新审视了大型语言模型(LLMs)作为零-shot关系抽取器的潜力，并提出了通过总结和提问(\textsc{SumAsk})提示方法来改进零-shot关系抽取。实验证明LLMs在这一任务上具有良好的表现。

    

    关系抽取(RE)即使在零-shot设定下，一直涉及一定程度的标记或未标记的数据。最近的研究表明，大型语言模型(LLMs)能够在给定自然语言提示的情况下，无需任何数据和参数调整，自动适应新任务，这为从文本中提取关系提供了可能性。本研究主要关注将LLMs，如ChatGPT，作为零-shot关系抽取器的研究。一方面，我们分析了现有RE提示的缺点，并尝试将最近的提示技术，如CoT，纳入其中以提高零-shot关系抽取。我们提出了总结和提问(\textsc{SumAsk})提示，这是一种简单的提示，通过递归使用LLMs将RE输入转换为有效的问答(QA)格式。另一方面，我们对各种基准和设置进行了全面的实验，以调查LLMs在零-shot关系抽取上的能力。具体而言，我们有以下的followi

    Relation extraction (RE) consistently involves a certain degree of labeled or unlabeled data even if under zero-shot setting. Recent studies have shown that large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt, which provides the possibility of extracting relations from text without any data and parameter tuning. This work focuses on the study of exploring LLMs, such as ChatGPT, as zero-shot relation extractors. On the one hand, we analyze the drawbacks of existing RE prompts and attempt to incorporate recent prompt techniques such as chain-of-thought (CoT) to improve zero-shot RE. We propose the summarize-and-ask (\textsc{SumAsk}) prompting, a simple prompt recursively using LLMs to transform RE inputs to the effective question answering (QA) format. On the other hand, we conduct comprehensive experiments on various benchmarks and settings to investigate the capabilities of LLMs on zero-shot RE. Specifically, we have the followi
    
[^96]: Compresso：结构化修剪与协作促进学习紧凑大型语言模型

    Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models. (arXiv:2310.05015v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.05015](http://arxiv.org/abs/2310.05015)

    Compresso是一种结构化修剪LLMs的新方法，通过协作促进学习最优的修剪决策，解决了训练成本高和数据收集困难的挑战。

    

    尽管大型语言模型（LLMs）取得了显着的成功，但庞大的模型尺寸给资源受限硬件带来了重大部署挑战。尽管现有的LLM压缩方法关注量化，但修剪在训练成本高和数据收集困难等方面相对未被探索。单次修剪方法虽然成本低且无需数据，已成为LLM修剪的主导方式，但在结构化修剪设置下会导致性能下降。在这项工作中，我们引入了一种新的结构化修剪LLMs的范例，称为Compresso。我们的方法通过提出的资源高效修剪算法和LLM自身的协作，在训练过程中学习最优的修剪决策。Compresso通过在指令调整过程中将低秩适应（LoRA）与$L_0$正则化相结合，解决了昂贵的培训成本和数据收集的挑战。

    Despite the remarkable success of Large Language Models (LLMs), the massive size poses significant deployment challenges, particularly on resource-constrained hardware. While existing LLM compression methods focus on quantization, pruning remains relatively unexplored due to the high cost of training-based approaches and data collection challenges. One-shot pruning methods, although cost-effective and data-free, have become dominant in LLM pruning, but lead to performance decline under the structured pruning setting. In this work, we introduce a new paradigm for structurally pruning LLMs, called Compresso. Our approach, through the collaboration of the proposed resource-efficient pruning algorithm and the LLM itself, learns optimal pruning decisions during the training process. Compresso addresses the challenges of expensive training costs and data collection by incorporating Low-Rank Adaptation (LoRA) into the $L_0$ regularization during the instruction tuning process. Then, we furthe
    
[^97]: 利用椭圆边界框进行细胞跟踪的检测

    Cell Tracking-by-detection using Elliptical Bounding Boxes. (arXiv:2310.04895v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.04895](http://arxiv.org/abs/2310.04895)

    本文提出了一种基于经典检测方法的新方法，通过将细胞形状近似为定向椭圆并使用通用定向对象检测器来识别细胞，实现了对细胞的检测和跟踪，减轻了对标注数据的需求。

    

    细胞的检测和跟踪对于生物分析至关重要。最近的方法依赖于基于模型演进的跟踪，通常通过训练端到端的深度学习模型来在图像帧上检测和跟踪细胞，并取得了令人满意的结果。然而，这些方法需要大量的标注数据，而获取这些数据耗时且需要专业的标注员。本文提出了一种基于经典检测方法的新方法，可以减轻对标注数据的需求。具体来说，它将细胞形状近似为定向椭圆，然后使用通用定向对象检测器来识别每个帧中的细胞。然后，我们依靠一个全局数据关联算法，使用概率距离度量来探索细胞的时间相似性，考虑到椭圆与二维高斯分布的关系。我们的结果表明，我们的方法能够实现具有竞争力的检测和跟踪结果。

    Cell detection and tracking are paramount for bio-analysis. Recent approaches rely on the tracking-by-model evolution paradigm, which usually consists of training end-to-end deep learning models to detect and track the cells on the frames with promising results. However, such methods require extensive amounts of annotated data, which is time-consuming to obtain and often requires specialized annotators. This work proposes a new approach based on the classical tracking-by-detection paradigm that alleviates the requirement of annotated data. More precisely, it approximates the cell shapes as oriented ellipses and then uses generic-purpose oriented object detectors to identify the cells in each frame. We then rely on a global data association algorithm that explores temporal cell similarity using probability distance metrics, considering that the ellipses relate to two-dimensional Gaussian distributions. Our results show that our method can achieve detection and tracking results competiti
    
[^98]: LauraGPT：使用GPT进行听、关注、理解和再生音频的研究

    LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT. (arXiv:2310.04673v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2310.04673](http://arxiv.org/abs/2310.04673)

    LauraGPT是一个统一的GPT模型，用于音频识别、理解和生成，具有广泛的应用范围，包括自动语音识别、语音翻译、文本到语音合成、机器翻译等任务。

    

    生成式预训练变换器（GPT）模型在各种自然语言处理任务中取得了显著的性能。然而，将类似的框架应用于音频任务的研究有限。以前提出的用于音频任务的大型语言模型要么缺乏充分的定量评估，要么局限于识别和理解音频内容的任务，要么明显不及现有的最先进模型（SOTA）。本文中，我们提出了LauraGPT，一个用于音频识别、理解和生成的统一GPT模型。LauraGPT是一个通用的语言模型，可以处理音频和文本输入，并在任意模式下生成输出。它可以进行与内容、语义、语音学和音频信号分析相关的各种任务。其中一些值得注意的任务包括自动语音识别、语音到文本翻译、文本到语音合成、机器翻译、语音增强、自动音频捕获等。

    Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks. However, there has been limited research on applying similar frameworks to audio tasks. Previously proposed large language models for audio tasks either lack sufficient quantitative evaluations, or are limited to tasks for recognizing and understanding audio content, or significantly underperform existing state-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified GPT model for audio recognition, understanding, and generation. LauraGPT is a versatile language model that can process both audio and text inputs and generate outputs in either modalities. It can perform a wide range of tasks related to content, semantics, paralinguistics, and audio-signal analysis. Some of its noteworthy tasks include automatic speech recognition, speech-to-text translation, text-to-speech synthesis, machine translation, speech enhancement, automated audio capt
    
[^99]: 利用自一致性提高数据有效的摊余贝叶斯推理方法

    Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference. (arXiv:2310.04395v1 [cs.LG])

    [http://arxiv.org/abs/2310.04395](http://arxiv.org/abs/2310.04395)

    该论文提出了一种利用自一致性改进数据有效的摊余贝叶斯推理方法，通过反转贝叶斯定理并利用近似表示的联合模型估计边际似然，加速条件神经密度估计器的学习动力学。

    

    我们提出了一种方法，通过利用参数$\theta$和数据$y$的概率联合模型$p(\theta, y)$中的通用对称性，改进了摊余贝叶斯推理（ABI）的效率和准确性。简言之，我们反转贝叶斯定理，并基于近似表示的联合模型估计边际似然。在完美近似情况下，边际似然在所有参数值上都是常数定义的。然而，近似误差导致不同参数值的边际似然估计中存在不可取的方差。我们将这种对称性的违反形式化为损失函数，加速条件神经密度估计器的学习动力学。我们将我们的方法应用于具有显式似然（基于似然）的双峰玩具问题和具有隐式似然（基于模拟）的现实模型。

    We propose a method to improve the efficiency and accuracy of amortized Bayesian inference (ABI) by leveraging universal symmetries in the probabilistic joint model $p(\theta, y)$ of parameters $\theta$ and data $y$. In a nutshell, we invert Bayes' theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators. We apply our method to a bimodal toy problem with an explicit likelihood (likelihood-based) and a realistic model with an implicit likelihood (simulation-based).
    
[^100]: Hermes：通过从自然语言规范合成有限状态机来解锁移动网络协议的安全分析

    Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications. (arXiv:2310.04381v1 [cs.CR])

    [http://arxiv.org/abs/2310.04381](http://arxiv.org/abs/2310.04381)

    Hermes是一个自动生成有限状态机的端到端框架，用于解锁移动网络协议的安全分析。通过处理自然语言规范并生成逻辑公式，Hermes能够发现新的漏洞和攻击，并对现有规范和商业基带进行评估。

    

    本文介绍了Hermes，一个自动生成自然语言移动规范的形式表达的端到端框架。我们首先开发了神经组成分析器NEUTREX，用于处理与转换相关的文本并提取转换组件（即状态、条件和动作）。我们还设计了一种领域特定语言，通过利用依存解析树将这些转换组件转化成逻辑公式。最后，我们将这些逻辑公式编译成转换和创建形式模型作为有限状态机。为了证明Hermes的有效性，我们在4G NAS、5G NAS和5G RRC规范上进行评估，并获得了81-87%的总体准确率，这是对现有技术的显著改进。我们对提取的模型进行的安全分析揭示出了3个新的漏洞、发现了19个之前的攻击4G和5G规范，以及7个商业4G基带的偏差。

    In this paper, we present Hermes, an end-to-end framework to automatically generate formal representations from natural language cellular specifications. We first develop a neural constituency parser, NEUTREX, to process transition-relevant texts and extract transition components (i.e., states, conditions, and actions). We also design a domain-specific language to translate these transition components to logical formulas by leveraging dependency parse trees. Finally, we compile these logical formulas to generate transitions and create the formal model as finite state machines. To demonstrate the effectiveness of Hermes, we evaluate it on 4G NAS, 5G NAS, and 5G RRC specifications and obtain an overall accuracy of 81-87%, which is a substantial improvement over the state-of-the-art. Our security analysis of the extracted models uncovers 3 new vulnerabilities and identifies 19 previous attacks in 4G and 5G specifications, and 7 deviations in commercial 4G basebands.
    
[^101]: Axiomatic Aggregations of Abductive Explanations. (arXiv:2310.03131v1 [cs.AI])

    Axiomatic Aggregations of Abductive Explanations. (arXiv:2310.03131v1 [cs.AI])

    [http://arxiv.org/abs/2310.03131](http://arxiv.org/abs/2310.03131)

    本文提出了通过将多种可能的归纳解释汇总为特征重要性分数的三种聚合方法，以解决存在多个有效的归纳解释的问题。

    

    最近对后验模型近似解释方法（如LIME和SHAP）健壮性的批评导致了模型精确阐释的兴起。针对每个数据点，归纳解释提供了能够生成结果的最小特征子集。尽管在理论上是可靠且严谨的，但是归纳解释存在一个主要问题 - 对于同一数据点可能存在多个有效的归纳解释。在这种情况下，提供单一的归纳解释可能是不足够的；另一方面，提供所有有效的归纳解释可能由于其数量庞大而难以理解。在本文中，我们通过将多种可能的归纳解释汇总为特征重要性分数来解决这个问题。我们提出了三种聚合方法：两种基于合作博弈论的权力指数和一种基于著名的因果强度度量。我们通过公理化表征这三种方法，证明它们每一个都具有

    The recent criticisms of the robustness of post hoc model approximation explanation methods (like LIME and SHAP) have led to the rise of model-precise abductive explanations. For each data point, abductive explanations provide a minimal subset of features that are sufficient to generate the outcome. While theoretically sound and rigorous, abductive explanations suffer from a major issue -- there can be several valid abductive explanations for the same data point. In such cases, providing a single abductive explanation can be insufficient; on the other hand, providing all valid abductive explanations can be incomprehensible due to their size. In this work, we solve this issue by aggregating the many possible abductive explanations into feature importance scores. We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength. We characterize these three methods axiomatically, showing that each of 
    
[^102]: HyperMask: 自适应的基于超网络的掩码用于持续学习

    HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning. (arXiv:2310.00113v1 [cs.LG])

    [http://arxiv.org/abs/2310.00113](http://arxiv.org/abs/2310.00113)

    HyperMask是一种用于持续学习的方法，它使用基于超网络的掩码来训练一个单一网络，以克服人工神经网络在多任务上的灾难性遗忘问题。

    

    当人工神经网络在多个任务上顺序训练时，往往会出现灾难性遗忘的问题。为了克服这个问题，已经存在许多持续学习策略，其中最有效的之一是基于超网络的方法。超网络根据任务的特征生成目标模型的权重。然而，该模型的主要限制是超网络对于每个任务可以产生完全不同的网络结构，因此每个任务都是单独解决的。模型在学习后续任务时不使用之前任务所关联的网络信息，并实际上产生了新的网络架构。为了解决这个问题，我们使用了彩票票证假设，该假设认为存在稀疏的子网络（即中奖票），可以保持完整网络的性能。在本文中，我们提出了一种名为HyperMask的方法，该方法为所有任务训练一个单一网络。超网络产生半二进制掩码，以获取目标子网络。

    Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, there exist many continual learning strategies. One of the most effective is the hypernetwork-based approach. The hypernetwork generates the weights of a target model based on the task's identity. The model's main limitation is that hypernetwork can produce completely different nests for each task. Consequently, each task is solved separately. The model does not use information from the network dedicated to previous tasks and practically produces new architectures when it learns the subsequent tasks. To solve such a problem, we use the lottery ticket hypothesis, which postulates the existence of sparse subnetworks, named winning tickets, that preserve the performance of a full network.  In the paper, we propose a method called HyperMask, which trains a single network for all tasks. Hypernetwork produces semi-binary masks to obtain target subnetw
    
[^103]: 未来的原因，现在的行动：一种可证明样本效率的自主LLM智能体的原则框架

    Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency. (arXiv:2309.17382v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.17382](http://arxiv.org/abs/2309.17382)

    提出了一个名为"RAFA"的原则框架，通过在LLM中将推理视为学习和规划的贝叶斯问题，协调推理和行动。通过一个提示模板进行推理，学习并制定未来的轨迹规划，然后在每一步中采取计划轨迹的初始行动并重新规划未来轨迹。这个框架具有可证明的遗憾保证。

    

    大型语言模型（LLM）展示了令人印象深刻的推理能力，但在现实世界中将推理转化为行动仍然具有挑战性。特别是，如何通过推理的内部机制在与外部环境的最少交互次数内可证明地完成给定任务仍然不清楚。为此，我们提出了一个有可证明遗憾保证的原则框架来协调推理和行动，称之为“为未来而推理，为现在而行动”（RAFA）。具体来说，我们设计了一个推理的提示模板，从内存缓冲区中学习并制定未来的长期轨迹规划（“为未来而推理”）。在每一步中，LLM智能体采取计划轨迹的初始行动（“为现在而行动”），将收集到的反馈存储在内存缓冲区中，并重新调用推理过程从新状态重新规划未来的轨迹。关键思想是将LLM中的推理视为学习和规划的贝叶斯问题。

    Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call "reason for future, act for now" (\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon ("reason for future"). At each step, the LLM agent takes the initial action of the planned trajectory ("act for now"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state.  The key idea is to cast reasoning in LLMs as learning and planning in Bayes
    
[^104]: ACGAN-GNNExplainer：用于图神经网络的辅助条件生成解释器

    ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks. (arXiv:2309.16918v1 [cs.LG])

    [http://arxiv.org/abs/2309.16918](http://arxiv.org/abs/2309.16918)

    本论文提出了一种新的图神经网络解释方法ACGAN-GNNExplainer，将辅助分类器生成对抗网络（ACGAN）引入到GNN解释领域。通过利用生成器为原始输入图生成解释，并借助鉴别器监督生成过程，提高解释的准确性和保真度。实验证明了该方法的有效性和实用性。

    

    图神经网络（GNNs）已经在各种实际应用中证明了其有效性，但其基本机制仍然是一个谜。为了解决这个挑战并实现可靠的决策，近年来提出了许多GNN解释器。然而，这些方法常常面临一些限制，包括对特定实例的依赖性，对未见过的图的一般性不足，可能产生无效的解释以及生成过程中产生的不充分的保真度。为了克服这些限制，本文将辅助分类器生成对抗网络（ACGAN）引入到GNN解释领域，并提出了一种新的GNN解释器ACGAN-GNNExplainer。我们的方法利用生成器为原始输入图生成解释，并运用鉴别器来监督生成过程，确保解释的保真度并提高准确性。实验评估分别在合成数据集和真实世界的图数据集上进行。

    Graph neural networks (GNNs) have proven their efficacy in a variety of real-world applications, but their underlying mechanisms remain a mystery. To address this challenge and enable reliable decision-making, many GNN explainers have been proposed in recent years. However, these methods often encounter limitations, including their dependence on specific instances, lack of generalizability to unseen graphs, producing potentially invalid explanations, and yielding inadequate fidelity. To overcome these limitations, we, in this paper, introduce the Auxiliary Classifier Generative Adversarial Network (ACGAN) into the field of GNN explanation and propose a new GNN explainer dubbed~\emph{ACGAN-GNNExplainer}. Our approach leverages a generator to produce explanations for the original input graphs while incorporating a discriminator to oversee the generation process, ensuring explanation fidelity and improving accuracy. Experimental evaluations conducted on both synthetic and real-world graph
    
[^105]: 优化人工智能和人类协作代理之间的委派

    Optimizing delegation between human and AI collaborative agents. (arXiv:2309.14718v1 [cs.AI])

    [http://arxiv.org/abs/2309.14718](http://arxiv.org/abs/2309.14718)

    该研究提出了一种优化人工智能和人类协作代理之间委派的方法，通过训练一个代理管理器根据任务绩效缺陷进行委派决策，并且可以处理不同环境表示下的团队操作。方法在实验中表现显著优于其他管理团队的方法。

    

    在人类与人工智能或自主代理形成混合团队的情景中，精确地确定何时授权团队成员执行行动是至关重要的。鉴于过去的例子中，人类和自主系统在任务上可能成功也可能失败，我们试图训练一个代理管理器来根据这些潜在的绩效缺陷做出委派决策。此外，我们不能总是期望各种代理在相同的环境模型中运行。可能会遇到代理之间的行动和转换有所不同的情况。因此，我们的框架提供了一个经过观察团队绩效学习的管理模型，而不限制代理与匹配的动态。我们的结果表明，我们的管理器能够在操作环境的不同表示下进行委派决策，远远超过了其他管理团队方法。

    In the context of humans operating with artificial or autonomous agents in a hybrid team, it is essential to accurately identify when to authorize those team members to perform actions. Given past examples where humans and autonomous systems can either succeed or fail at tasks, we seek to train a delegating manager agent to make delegation decisions with respect to these potential performance deficiencies. Additionally, we cannot always expect the various agents to operate within the same underlying model of the environment. It is possible to encounter cases where the actions and transitions would vary between agents. Therefore, our framework provides a manager model which learns through observations of team performance without restricting agents to matching dynamics. Our results show our manager learns to perform delegation decisions with teams of agents operating under differing representations of the environment, significantly outperforming alternative methods to manage the team.
    
[^106]: 从生物学启发机制重新思考超像素分割

    Rethinking superpixel segmentation from biologically inspired mechanisms. (arXiv:2309.13438v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.13438](http://arxiv.org/abs/2309.13438)

    该论文从神经结构和视觉机制的启示出发，提出了一种生物网络架构用于超像素分割，其中包括增强筛选模块（ESM）和边界感知标签（BAL），旨在产生严格遵循物体边界且传达丰富视觉符号的超像素。

    

    最近，基于深度学习的超像素分割方法的发展在分割的效率和性能方面都取得了改进。然而，一个重要的挑战在于产生严格遵循物体边界且传达丰富视觉符号的超像素，特别是当交叉表面颜色相关性可能干扰物体时。受神经结构和视觉机制的启发，我们提出了一个生物网络架构，其中包括一个增强筛选模块（ESM）和一个新颖的边界感知标签（BAL），用于超像素分割。ESM通过模拟视觉皮质的交互式投影机制来增强语义信息。此外，BAL模拟了视觉皮质细胞的空间频率特性，以促进生成具有强边界遵循性的超像素。我们通过在BSDS500数据集上进行评估来证明我们方法的有效性。

    Recently, advancements in deep learning-based superpixel segmentation methods have brought about improvements in both the efficiency and the performance of segmentation. However, a significant challenge remains in generating superpixels that strictly adhere to object boundaries while conveying rich visual significance, especially when cross-surface color correlations may interfere with objects. Drawing inspiration from neural structure and visual mechanisms, we propose a biological network architecture comprising an Enhanced Screening Module (ESM) and a novel Boundary-Aware Label (BAL) for superpixel segmentation. The ESM enhances semantic information by simulating the interactive projection mechanisms of the visual cortex. Additionally, the BAL emulates the spatial frequency characteristics of visual cortical cells to facilitate the generation of superpixels with strong boundary adherence. We demonstrate the effectiveness of our approach through evaluations on both the BSDS500 dataset
    
[^107]: LPML: 数学推理的LLM提示标记语言

    LPML: LLM-Prompting Markup Language for Mathematical Reasoning. (arXiv:2309.13078v1 [cs.AI])

    [http://arxiv.org/abs/2309.13078](http://arxiv.org/abs/2309.13078)

    本论文提出了LPML，一种用于数学推理的LLM提示标记语言。通过将Chain-of-Thought方法和Python REPL与该标记语言结合，我们能够控制LLM生成文本中的错误，并增强其推理能力。我们的方法能够实现利用Python计算纠正错误和解决挑战性数学问题，而只需要零样本提示。

    

    在利用大型语言模型（LLMs）进行数学推理时，解决LLMs生成文本中的推理和计算错误是一个关键挑战。在本文中，我们提出了一种新的框架，将Chain-of-Thought（CoT）方法与外部工具（Python REPL）相结合。我们发现，通过提示LLMs生成类似XML标记语言的结构化文本，我们可以无缝地集成CoT和外部工具，并控制LLMs的不良行为。通过我们的方法，LLMs可以利用Python计算来纠正CoT中的错误。我们将我们的方法应用于ChatGPT（GPT-3.5）来解决具有挑战性的数学问题，并证明通过标记语言将CoT和Python REPL结合起来可以增强LLMs的推理能力。我们的方法使LLMs能够使用零样本提示编写标记语言，并进行高级数学推理。

    In utilizing large language models (LLMs) for mathematical reasoning, addressing the errors in the reasoning and calculation present in the generated text by LLMs is a crucial challenge. In this paper, we propose a novel framework that integrates the Chain-of-Thought (CoT) method with an external tool (Python REPL). We discovered that by prompting LLMs to generate structured text in XML-like markup language, we could seamlessly integrate CoT and the external tool and control the undesired behaviors of LLMs. With our approach, LLMs can utilize Python computation to rectify errors within CoT. We applied our method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and demonstrated that combining CoT and Python REPL through the markup language enhances the reasoning capability of LLMs. Our approach enables LLMs to write the markup language and perform advanced mathematical reasoning using only zero-shot prompting.
    
[^108]: DiscoverPath：用于生物医学研究的知识细化和检索系统

    DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research. (arXiv:2309.01808v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2309.01808](http://arxiv.org/abs/2309.01808)

    DiscoverPath是一个基于知识图的生物医学研究论文搜索引擎，通过命名实体识别和词性标注从文章摘要中提取术语和关系，并展示给用户一个关注查询实体及其邻近节点的子图，以及查询推荐系统，使用户能够循序渐进地细化查询。

    

    学术出版物的指数增长需要高级工具来实现高效的文章检索，尤其在跨学科领域中，不同的术语被用来描述相似的研究。传统的基于关键词的搜索引擎往往无法帮助那些对特定术语不熟悉的用户。为了解决这个问题，我们提出了一种基于知识图的生物医学研究论文搜索引擎，以增强用户在发现相关查询和文章方面的体验。该系统被称为DiscoverPath，采用命名实体识别（NER）和词性标注（POS）来从文章摘要中提取术语和关系，创建知识图谱。为了减少信息超载，DiscoverPath给用户展示了一个关注查询实体及其邻近节点的子图，并且还结合了查询推荐系统，使用户能够循序渐进地细化查询。该系统配备了一个易于访问的图形用户界面（GUI）。

    The exponential growth in scholarly publications necessitates advanced tools for efficient article retrieval, especially in interdisciplinary fields where diverse terminologies are used to describe similar research. Traditional keyword-based search engines often fall short in assisting users who may not be familiar with specific terminologies. To address this, we present a knowledge graph-based paper search engine for biomedical research to enhance the user experience in discovering relevant queries and articles. The system, dubbed DiscoverPath, employs Named Entity Recognition (NER) and part-of-speech (POS) tagging to extract terminologies and relationships from article abstracts to create a KG. To reduce information overload, DiscoverPath presents users with a focused subgraph containing the queried entity and its neighboring nodes and incorporates a query recommendation system, enabling users to iteratively refine their queries. The system is equipped with an accessible Graphical Us
    
[^109]: 图像属性编辑的零样本反演过程与扩散模型

    Zero-shot Inversion Process for Image Attribute Editing with Diffusion Models. (arXiv:2308.15854v1 [cs.CV])

    [http://arxiv.org/abs/2308.15854](http://arxiv.org/abs/2308.15854)

    提出了一种零样本反演过程（ZIP）框架，用于图像属性编辑。该方法利用生成的视觉参考和文本引导注入扩散模型的语义潜空间，可以在文本提示的直观控制下产生多样的内容和属性，并展现出对不同属性操作的鲁棒性。

    

    降噪扩散模型在图像编辑中表现出优秀的性能。现有的方法倾向于使用图像引导方法，提供视觉参考但缺乏语义连贯性的控制，或者使用文本引导方法，确保对文本引导的忠实，但缺乏视觉质量。为了解决这个问题，我们提出了零样本反演过程（ZIP）框架，它将生成的视觉参考和文本引导的融合注入到预训练扩散模型的语义潜空间中。仅使用一个微小的神经网络，提出的ZIP在文本提示的直观控制下产生多样的内容和属性。此外，ZIP在真实图像上展示了对域内和域外属性操作的显著鲁棒性。我们在各种基准数据集上进行了详细的实验。与最先进的方法相比，ZIP产生了与之相当质量的图像，同时提供了逼真的编辑效果。

    Denoising diffusion models have shown outstanding performance in image editing. Existing works tend to use either image-guided methods, which provide a visual reference but lack control over semantic coherence, or text-guided methods, which ensure faithfulness to text guidance but lack visual quality. To address the problem, we propose the Zero-shot Inversion Process (ZIP), a framework that injects a fusion of generated visual reference and text guidance into the semantic latent space of a \textit{frozen} pre-trained diffusion model. Only using a tiny neural network, the proposed ZIP produces diverse content and attributes under the intuitive control of the text prompt. Moreover, ZIP shows remarkable robustness for both in-domain and out-of-domain attribute manipulation on real images. We perform detailed experiments on various benchmark datasets. Compared to state-of-the-art methods, ZIP produces images of equivalent quality while providing a realistic editing effect.
    
[^110]: 阐明扩散模型中的曝光偏差问题

    Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])

    [http://arxiv.org/abs/2308.15321](http://arxiv.org/abs/2308.15321)

    本文系统地研究了扩散模型中的曝光偏差问题，并提出了一种名为Epsilon Scaling的免训练方法来减轻这一问题。实验结果验证了该方法的有效性。

    

    扩散模型展示了令人印象深刻的生成能力，但它们的“曝光偏差”问题，即训练和采样之间的输入不匹配，缺乏深入探索。本文通过首先对采样分布进行分析建模，然后将每个采样步骤的预测误差归因为曝光偏差问题的根本原因，系统地研究了扩散模型中的曝光偏差问题。此外，我们讨论了解决这个问题的潜在方法，并提出了一个直观的度量标准。除了阐明曝光偏差问题，我们提出了一种简单但有效的免训练方法，称为Epsilon Scaling，以减轻曝光偏差。我们展示了Epsilon Scaling通过缩小网络输出（Epsilon）明确地将采样轨迹移近训练阶段学习到的向量场，从而减轻了训练和采样之间的输入不匹配。在各种扩散框架上进行了实验。

    Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
    
[^111]: InstructionGPT-4: 一个200指令范式用于微调MiniGPT-4

    InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4. (arXiv:2308.12067v1 [cs.LG])

    [http://arxiv.org/abs/2308.12067](http://arxiv.org/abs/2308.12067)

    InstructionGPT-4通过仅使用200个例子进行微调，在多模式指令数据质量度量和选择器的帮助下，在各种评估任务中优于原始的MiniGPT-4。

    

    多模式大型语言模型通过两阶段的训练过程获取其遵循指令的能力：在图像-文本对上进行预训练，然后在监督式视觉-语言指令数据上进行微调。最近的研究表明，即使只有有限量的高质量遵循指令数据，大型语言模型也能取得令人满意的结果。在本文中，我们介绍了InstructionGPT-4，它经过微调的数据集只包含200个例子，约占MiniGPT-4对齐数据集中使用的遵循指令数据的6%。我们首先提出了几个用于评估多模式指令数据质量的度量指标。基于这些度量指标，我们提出了一个简单而有效的数据选择器，自动识别和过滤低质量的视觉-语言数据。通过采用这种方法，InstructionGPT-4在各种评估（如视觉问答、GPT-4偏好）上优于原始的MiniGPT-4。总体而言，我们的研究发现...

    Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findi
    
[^112]: 一个关于校准的基准研究

    A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])

    [http://arxiv.org/abs/2308.11838](http://arxiv.org/abs/2308.11838)

    这项研究提出了一个模型校准的基准研究，利用神经架构搜索空间探索了模型校准属性。研究结果显示，模型校准可以在不同任务中泛化，并可以同时兼顾模型的准确性和校准性能。

    

    深度神经网络在各种机器学习任务中的应用越来越广泛。然而，随着这些模型复杂性的增加，它们往往面临校准问题，尽管预测准确性有所提高。许多研究通过数据预处理、使用特定损失函数和训练框架来改善校准性能。然而，对校准属性的研究有点被忽视了。我们的研究利用神经架构搜索（NAS）搜索空间，在全面探索校准属性的模型架构空间中提供了一个详尽的模型架构空间。我们特别创建了一个模型校准数据集。该数据集在广泛使用的NATS-Bench搜索空间中评估了90个基于区间的校准度量和12个其他校准度量，涵盖了117,702个独特的神经网络。我们的分析旨在通过我们提出的数据集回答该领域一些长期存在的问题：（i）模型校准能否在不同任务中泛化？（ii）能否同时兼顾模型的准确性和校准性能？

    Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
    
[^113]: PokerKit: 一种用于细粒度多变体扑克游戏模拟的全面的Python库

    PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant Poker Game Simulations. (arXiv:2308.07327v1 [cs.AI])

    [http://arxiv.org/abs/2308.07327](http://arxiv.org/abs/2308.07327)

    PokerKit是一个全面的Python库，用于细粒度多变体扑克游戏模拟，提供广泛的扑克变体支持和灵活的游戏状态控制，对扑克AI开发、工具创建和在线扑克赌场实现等领域具有重要贡献。

    

    PokerKit是一个开源的Python库，旨在克服现有扑克游戏模拟和手牌评估工具的限制，这些工具通常只支持少量扑克变体，并且在游戏状态控制方面缺乏灵活性。相比之下，PokerKit通过支持广泛的扑克变体，并提供灵活的架构供用户定义自定义游戏，显著扩大了这一范围。本文详细介绍了PokerKit的设计和实现，包括其直观的编程API，多变体游戏支持以及统一的手牌评估套件在不同手牌类型间的应用。PokerKit的灵活性使其能够在扑克AI开发、工具创建和在线扑克赌场实现等多个领域中使用。PokerKit的可靠性通过静态类型检查、广泛的doctest和单元测试来确保，达到了97%的代码覆盖率。引入PokerKit代表了对该领域的重要贡献。

    PokerKit is an open-source Python library designed to overcome the restrictions of existing poker game simulation and hand evaluation tools, which typically support only a handful of poker variants and lack flexibility in game state control. In contrast, PokerKit significantly expands this scope by supporting an extensive array of poker variants and it provides a flexible architecture for users to define their custom games. This paper details the design and implementation of PokerKit, including its intuitive programmatic API, multi-variant game support, and a unified hand evaluation suite across different hand types. The flexibility of PokerKit allows for applications in diverse areas, such as poker AI development, tool creation, and online poker casino implementation. PokerKit's reliability has been established through static type checking, extensive doctests, and unit tests, achieving 97\% code coverage. The introduction of PokerKit represents a significant contribution to the field 
    
[^114]: CHATREPORT：通过基于LLM工具实现可持续性披露分析的民主化

    CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools. (arXiv:2307.15770v1 [cs.CL])

    [http://arxiv.org/abs/2307.15770](http://arxiv.org/abs/2307.15770)

    本论文介绍了一种名为ChatReport的基于LLM的系统，它通过实现可追溯的答案和解决领域专家参与低效性的问题，旨在通过自动分析企业可持续性报告，实现可持续性披露分析民主化。

    

    面对气候变化，公司真的在朝着更可持续经营迈出实质性的步伐吗？一个全面的答案可以在企业可持续性报告的密集信息中找到。然而，这些报告的数量和复杂性使人工分析成本非常高昂。因此，只有少数的机构拥有资源能够大规模分析这些报告，这导致可持续性报告缺乏透明度。通过基于LLM自动分析工具赋能利益相关者可能是实现可持续性报告分析民主化的一种有希望的方式。然而，开发这样的工具面临挑战，主要原因是LLM的幻觉问题和将领域专家引入AI开发过程的低效性。在本文中，我们介绍了ChatReport，这是一种基于LLM的新型系统，用于自动化分析企业可持续性报告，通过使答案可追溯来减少幻觉的危害，并解决领域专家参与AI开发过程的低效性。

    In the face of climate change, are companies really taking substantial steps toward more sustainable operations? A comprehensive answer lies in the dense, information-rich landscape of corporate sustainability reports. However, the sheer volume and complexity of these reports make human analysis very costly. Therefore, only a few entities worldwide have the resources to analyze these reports at scale, which leads to a lack of transparency in sustainability reporting. Empowering stakeholders with LLM-based automatic analysis tools can be a promising way to democratize sustainability report analysis. However, developing such tools is challenging due to (1) the hallucination of LLMs and (2) the inefficiency of bringing domain experts into the AI development loop. In this paper, we ChatReport, a novel LLM-based system to automate the analysis of corporate sustainability reports, addressing existing challenges by (1) making the answers traceable to reduce the harm of hallucination and (2) a
    
[^115]: 弹性决策变压器

    Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02484](http://arxiv.org/abs/2307.02484)

    弹性决策变压器（EDT）通过在测试时间进行动作推断时调整历史长度来实现轨迹拼接，填补了决策变压器（DT）在这一方面的性能差距，并且在多任务情况下胜过基于Q-Learning的方法。

    

    本文介绍了弹性决策变压器（EDT），它是现有决策变压器（DT）及其变体的重大进展。尽管DT声称能够生成最佳轨迹，但实证证据表明它在轨迹拼接方面存在困难，轨迹拼接是指从一组次优轨迹中生成最优或接近最优轨迹的过程。提出的EDT通过在测试时间进行动作推断时调整DT中维护的历史长度来实现轨迹拼接，从而使自己与众不同。此外，当前轨迹是最优的时候，EDT通过保持较长的历史，当当前轨迹是次优的时候，EDT通过保持较短的历史来优化轨迹，使其能够与更优的轨迹进行“拼接”。广泛的实验表明，EDT能够填补基于DT和基于Q-Learning方法之间的性能差距。特别是，EDT在多任务情况下胜过基于Q-Learning的方法。

    This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
    
[^116]: DifFSS: 一种用于小样本语义分割的扩散模型

    DifFSS: Diffusion Model for Few-Shot Semantic Segmentation. (arXiv:2307.00773v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.00773](http://arxiv.org/abs/2307.00773)

    DifFSS是一种利用扩散模型改进小样本语义分割性能的方法，通过生成多样化的辅助支持图像，而不需要修改网络结构，从而显著提高最先进的小样本语义分割模型的性能。

    

    扩散模型在图像生成中表现出色。虽然已经提出了各种结构不同的小样本语义分割模型，但是性能的改进已经达到了瓶颈。本文提出了一种首次利用扩散模型用于小样本语义分割任务的方法，称为DifFSS。DifFSS是一种新颖的小样本语义分割范式，可以在不修改网络结构的情况下显著提高最先进的小样本语义分割模型的性能。具体来说，我们利用扩散模型的强大生成能力，通过使用语义掩码、涂抹或软HED边界的支持图像作为控制条件，生成多样化的辅助支持图像。这个生成过程模拟了查询图像类别内的多样性，如颜色、纹理变化和光照等。因此，小样本语义分割模型可以参考更多多样的支持图像，产生更具鲁棒性的表示，从而实现一致的性能改进。

    Diffusion models have demonstrated excellent performance in image generation. Although various few-shot semantic segmentation (FSS) models with different network structures have been proposed, performance improvement has reached a bottleneck. This paper presents the first work to leverage the diffusion model for FSS task, called DifFSS. DifFSS, a novel FSS paradigm, can further improve the performance of the state-of-the-art FSS models by a large margin without modifying their network structure. Specifically, we utilize the powerful generation ability of diffusion models to generate diverse auxiliary support images by using the semantic mask, scribble or soft HED boundary of the support image as control conditions. This generation process simulates the variety within the class of the query image, such as color, texture variation, lighting, $etc$. As a result, FSS models can refer to more diverse support images, yielding more robust representations, thereby achieving a consistent improv
    
[^117]: DisCo: 用于现实世界中基于人类舞蹈的引用生成的解耦控制

    DisCo: Disentangled Control for Referring Human Dance Generation in Real World. (arXiv:2307.00040v1 [cs.CV])

    [http://arxiv.org/abs/2307.00040](http://arxiv.org/abs/2307.00040)

    这篇论文提出了一个新的问题设置：引用人类舞蹈生成。在现实世界的舞蹈场景中，通过解耦控制来解决舞蹈合成中的挑战，包括忠实性、泛化能力和组合性。

    

    生成AI在计算机视觉领域取得了显著的进展，特别是在基于文本描述的图像/视频合成方面。尽管有了这些进步，但在生成人类中心内容（如舞蹈合成）方面仍然存在挑战。现有的舞蹈合成方法在合成内容与现实世界舞蹈场景之间存在差距。在本文中，我们定义了一个新的问题设置：引用人类舞蹈生成，重点关注具有三个重要属性的现实世界舞蹈场景：（i）忠实性：合成应该保留引用图像中人类主体前景和背景的外观，并精确地遵循目标姿势；（ii）泛化能力：模型应该适用于未见过的人类主体、背景和姿势；（iii）组合性：它应该允许来自不同来源的已见/未见主体、背景和姿势的组合。为了应对这些挑战，我们引入了一种新颖的方法，D

    Generative AI has made significant strides in computer vision, particularly in image/video synthesis conditioned on text descriptions. Despite the advancements, it remains challenging especially in the generation of human-centric content such as dance synthesis. Existing dance synthesis methods struggle with the gap between synthesized content and real-world dance scenarios. In this paper, we define a new problem setting: Referring Human Dance Generation, which focuses on real-world dance scenarios with three important properties: (i) Faithfulness: the synthesis should retain the appearance of both human subject foreground and background from the reference image, and precisely follow the target pose; (ii) Generalizability: the model should generalize to unseen human subjects, backgrounds, and poses; (iii) Compositionality: it should allow for composition of seen/unseen subjects, backgrounds, and poses from different sources. To address these challenges, we introduce a novel approach, D
    
[^118]: DCdetector: 双重关注对比表示学习用于时间序列异常检测

    DCdetector: Dual Attention Contrastive Representation Learning for Time Series Anomaly Detection. (arXiv:2306.10347v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10347](http://arxiv.org/abs/2306.10347)

    DCdetector是一种多尺度双重关注的对比表示学习模型，用于时间序列异常检测。它利用双重关注不对称设计和纯对比损失学习置换不变表示，从而有效区分异常样本。

    

    时间序列异常检测对于许多应用至关重要。其目标是在时间序列中识别出与正常样本分布有差异的异常样本。这个任务的最基本挑战是学习一个能够有效区分异常的表示映射。基于重建的方法仍然主导着该领域，但是使用异常数据进行表示学习可能会对性能产生负面影响。另一方面，对比学习旨在找到一种能够明显区分任何实例的表示，这可以为时间序列异常检测提供更自然和有前景的表示。在本文中，我们提出了DCdetector，一种多尺度双重关注的对比表示学习模型。DCdetector利用新颖的双重关注不对称设计创建置换环境，并使用纯对比损失来引导学习过程，从而学习一个具有优越性能的置换不变表示。

    Time series anomaly detection is critical for a wide range of applications. It aims to identify deviant samples from the normal sample distribution in time series. The most fundamental challenge for this task is to learn a representation map that enables effective discrimination of anomalies. Reconstruction-based methods still dominate, but the representation learning with anomalies might hurt the performance with its large abnormal loss. On the other hand, contrastive learning aims to find a representation that can clearly distinguish any instance from the others, which can bring a more natural and promising representation for time series anomaly detection. In this paper, we propose DCdetector, a multi-scale dual attention contrastive representation learning model. DCdetector utilizes a novel dual attention asymmetric design to create the permutated environment and pure contrastive loss to guide the learning process, thus learning a permutation invariant representation with superior d
    
[^119]: 从零开始实现红队对抗语言模型的探索与建立

    Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])

    [http://arxiv.org/abs/2306.09442](http://arxiv.org/abs/2306.09442)

    本文提出了一种新的红队行动，通过从高层次、抽象的规范出发来考虑语言模型的行为，以探究模型的创新和贡献。

    

    部署大型语言模型（LLMs）可能会产生有害输出，例如有毒或不诚实陈述。先前的研究已经引入了工具以调查有害输出，以识别和减轻这些风险。虽然这是确保语言模型安全的有价值步骤，但这些方法通常依赖于现有的针对不希望的输出的分类器。这限制了它们在只有预先知道有害行为类型的情况下的应用。然而，这跳过了红队行动的核心挑战：开发模型可能展示的行为的上下文理解。此外，当这样的分类器已经存在时，红队行动的边际价值有限，因为分类器可以用于过滤训练数据或模型输出。本文考虑在假设对手从高级、抽象的不良行为规范出发的情况下进行红队行动。红队应该在精化/扩展此规范的同时对抗该模型。

    Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
    
[^120]: DYffusion：面向时空预测的动态扩散模型

    DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting. (arXiv:2306.01984v1 [cs.LG])

    [http://arxiv.org/abs/2306.01984](http://arxiv.org/abs/2306.01984)

    提出了一种新的扩散模型，其结合了数据中编码的时间动态，自然地编码了多步和长程预测能力，具有灵活的采样轨迹和折衷性能与加速采样的能力，同时提高了计算效率，可在时空预测方面取得竞争性表现。

    

    尽管扩散模型可以成功地生成数据和做出预测，但它们主要是为静态图像设计的。我们提出了一种方法，可以训练用于动态预测的扩散模型，利用编码在数据中的时间动态，直接将其与网络中的扩散步骤耦合。我们训练了一个随机的、时间条件的插值器和一个骨干预测网络，分别模仿传统扩散模型的前向和后向过程。这种设计选择自然地编码了多步和长程预测能力，允许高度灵活的连续时间采样轨迹，并在推理时能够折衷性能与加速采样的能力。此外，面向动态的扩散过程强加了强的归纳偏差，相比传统基于高斯噪声的扩散模型，可以提高计算效率。我们的方法在概率滑雪预测任务上表现出竞争力。

    While diffusion models can successfully generate data and make predictions, they are predominantly designed for static images. We propose an approach for training diffusion models for dynamics forecasting that leverages the temporal dynamics encoded in the data, directly coupling it with the diffusion steps in the network. We train a stochastic, time-conditioned interpolator and a backbone forecaster network that mimic the forward and reverse processes of conventional diffusion models, respectively. This design choice naturally encodes multi-step and long-range forecasting capabilities, allowing for highly flexible, continuous-time sampling trajectories and the ability to trade-off performance with accelerated sampling at inference time. In addition, the dynamics-informed diffusion process imposes a strong inductive bias, allowing for improved computational efficiency compared to traditional Gaussian noise-based diffusion models. Our approach performs competitively on probabilistic ski
    
[^121]: 因果关系的测度论公理化

    A Measure-Theoretic Axiomatisation of Causality. (arXiv:2305.17139v1 [cs.AI])

    [http://arxiv.org/abs/2305.17139](http://arxiv.org/abs/2305.17139)

    本文提出了一个称为"因果空间"的概念，旨在以柯尔莫戈罗夫的概率测度公理化为起点，实现对因果关系的公理化，并成功地解决了现有框架的限制。

    

    因果关系是许多研究领域中的核心概念，但仍然没有普遍认可的因果关系公理化。我们将因果关系视为概率理论的扩展，并作为研究在系统上干预时会发生什么的研究，并提议以柯尔莫戈罗夫的概率测度公理化作为因果关系公理化的起点。为此，我们提出了一个"因果空间"的概念，包括一个概率空间和称为"因果核"的转移概率核的集合，用来编码该空间的因果信息。我们提出的框架不仅在测度论上严格地基于，还揭示了现有框架的长期限制，例如，循环、潜在变量和随机过程。

    Causality is a central concept in a wide range of research areas, yet there is still no universally agreed axiomatisation of causality. We view causality both as an extension of probability theory and as a study of \textit{what happens when one intervenes on a system}, and argue in favour of taking Kolmogorov's measure-theoretic axiomatisation of probability as the starting point towards an axiomatisation of causality. To that end, we propose the notion of a \textit{causal space}, consisting of a probability space along with a collection of transition probability kernels, called \textit{causal kernels}, that encode the causal information of the space. Our proposed framework is not only rigorously grounded in measure theory, but it also sheds light on long-standing limitations of existing frameworks including, for example, cycles, latent variables and stochastic processes.
    
[^122]: 分段循环Transformer:一种高效的序列到序列模型

    Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v1 [cs.CL])

    [http://arxiv.org/abs/2305.16340](http://arxiv.org/abs/2305.16340)

    本文提出了一种分段循环Transformer（SRformer）来减少计算/内存成本，并使用RAF层处理跨段的信息，从而提高序列处理能力。

    

    Transformer在许多领域中表现出卓越的性能，包括语言和视觉。然而，随着序列长度的增加，它们的计算成本呈二次增长，使得它们在资源受限的应用中使用成为不可能。为了解决这个问题，我们的方法是将整个序列划分成若干段。然后使用具有循环结构的神经元来聚合跨段的信息，从而实现具有较低计算/内存成本的序列处理能力模型。为了验证这个想法，我们首先研究了使用局部Attention机制对单个段的影响。然后我们提出了一种分段循环Transformer（SRformer），它将分段Attention和循环Attention相结合。它使用循环accumulate and fire（RAF）层在相邻段之间处理信息。通过更新key的产品来补偿减少Attention窗口长度产生的误差。

    Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the whole sequence into segments. The information across segments can then be aggregated using neurons with recurrence leveraging their inherent memory. Such an approach leads to models with sequential processing capability at a lower computation/memory cost. To investigate this idea, first, we examine the effects of using local attention mechanism on the individual segments. Then we propose a segmented recurrent transformer (SRformer) that combines segmented attention with recurrent attention. It uses recurrent accumulate and fire (RAF) layers to process information between consecutive segments. The loss caused by reducing the attention window length is compensated by updating the product of key
    
[^123]: LLM们进步到了什么程度？一个挑战性的问题解决基准对大型语言模型

    Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models. (arXiv:2305.15074v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.15074](http://arxiv.org/abs/2305.15074)

    这项研究提出了JEEBench，一个更具挑战性的基准数据集，用于评估大型语言模型的问题解决能力。通过评估各种模型，结果显示目前最好的模型在解决问题时存在代数操作错误、抽象概念转化不准确和难以检索相关概念等问题。

    

    在过去的几年里，大型语言模型（LLMs）在现有的推理基准上的性能显著提高。为此，我们提出了JEEBench，一个更具挑战性的基准数据集，用于评估LLMs的问题解决能力。我们从高竞争的印度理工学院（IIT）JEE-Advanced考试中精选出了515个具有挑战性的预工程数学、物理和化学问题。在这个基准中，长期推理和深入领域知识的运用对问题的解决至关重要。我们对各种开源和专有模型进行了评估，结果显示，即使使用了自一致性、自我完善和思维链提示等技术，最高性能也不到40\%。最好的模型GPT-4的典型失败模式包括代数操作错误、将抽象概念准确地转化为数学方程以及无法检索相关的领域特定概念。我们还观察到，仅仅通过输入提示不能让模型成功解决问题。

    The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40\%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompti
    
[^124]: FActScore: 对长文本生成中事实准确性的细粒度原子评估

    FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. (arXiv:2305.14251v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14251](http://arxiv.org/abs/2305.14251)

    本文介绍了一种称为FACTSCORE的评估方法，它通过将生成的内容分解为原子事实，并计算被可靠知识源支持的比例来评估大型语言模型生成的长文本的事实准确性。通过广泛的人工评估，我们发现商业语言模型中仅有58%的ChatGPT传记达到了高水平的事实准确性。此外，我们还引入了一种自动化模型，利用检索和强语言模型估计FACTSCORE，误差率低于2%。

    

    评估大型语言模型生成的长文本的事实性是一项棘手的任务，因为（1）生成的内容通常包含支持和不支持的信息，使得二元判断质量不足，（2）人工评估耗时且成本高。本文介绍了FACTSCORE，一种新的评估方法，它将生成内容分解为一系列原子事实，并计算被可靠知识源支持的原子事实的百分比。我们进行了广泛的人工评估，得出了几个最先进商业语言模型（InstructGPT、ChatGPT和增强提取PerplexityAI）生成的人物传记的FACTSCORE，并报道了新的分析结果，证明了对于这样的细粒度评分的需求（例如，ChatGPT只达到58%）。由于人工评估费时费力，我们还引入了一种使用检索和强语言模型估计FACTSCORE的自动化模型，误差率不超过2%。

    Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, w
    
[^125]: 编辑大型语言模型：问题、方法和机会

    Editing Large Language Models: Problems, Methods, and Opportunities. (arXiv:2305.13172v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.13172](http://arxiv.org/abs/2305.13172)

    本文深入探讨了编辑大型语言模型的问题、方法和机会，提供了任务定义和挑战的概述、先进方法的实证分析，以及构建了新的基准数据集。这些结果有助于改进LLMs的编辑技术，提高其效果和可行性。

    

    尽管能够训练出表现优秀的大型语言模型（LLMs），但其保持相关性和纠正错误的方法仍然难以确定。为此，最近几年出现了许多编辑LLMs的技术，其目标是在特定领域内高效地改变LLMs的行为，同时不对其他输入的性能产生负面影响。本文深入探讨了与LLMs模型编辑相关的问题、方法和机会。特别是，我们提供了关于模型编辑任务定义和相关挑战的全面概述，以及对目前最先进的方法的深入实证分析。我们还构建了一个新的基准数据集，以促进更强大的评估，并指出现有技术固有的持久问题。我们的目标是为每种编辑技术的效果和可行性提供有价值的见解，从而帮助社区在LLMs的管理中取得更好的结果。

    Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in ma
    
[^126]: Clifford群等变神经网络

    Clifford Group Equivariant Neural Networks. (arXiv:2305.11141v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.11141](http://arxiv.org/abs/2305.11141)

    我们引入了Clifford群等变神经网络，它可以构建O(n)和E(n)等变模型。该方法通过调整Clifford群的定义以及保持向量空间和乘法结构的作用来实现多个有利属性。

    

    我们引入了Clifford群等变神经网络：一种构建O(n)和E(n)等变模型的新方法。我们确定并研究了Clifford群，它是Clifford代数中的一个子群，其定义经过调整以实现多个有利属性。主要地，该群的作用形成了一个正交自同构，扩展到整个Clifford代数，同时尊重多矢分级。这导致了对应于多矢分解的多个非等价子表示。此外，我们证明该作用不仅尊重Clifford代数的向量空间结构，还尊重其乘法结构，即几何乘积。这些发现意味着我们可以得到在任意维的内积空间中优雅地推广的表达层。我们特别展示了从一个sin

    We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing $\mathrm{O}(n)$- and $\mathrm{E}(n)$-equivariant models. We identify and study the $\textit{Clifford group}$, a subgroup inside the Clifford algebra whose definition we adjust to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a sin
    
[^127]: 基于预训练模型的等变小样本学习

    Equivariant Few-Shot Learning from Pretrained Models. (arXiv:2305.09900v1 [cs.LG])

    [http://arxiv.org/abs/2305.09900](http://arxiv.org/abs/2305.09900)

    本文提出了一种基于预训练模型的$\lambda$-\textit{equitune}方法，它使用\textit{重要性权重}$\lambda$对特征进行平均，可以显著提高等变小样本学习的表现。

    

    高效的迁移学习算法是基础模型在有限数据情况下在各种下游任务上取得成功的关键。最近的作品 \cite{basu2022equi} 和 \cite{kaba2022equivariance} 分别提出了使用从群变换输入得到的特征的群平均值（\textit{equitune}）和基于优化的方法来从不等变的神经网络获取等变输出。虽然 \cite{kaba2022equivariance} 只关注从头开始训练，但我们发现即使在良好的微调结果下，\textit{equitune} 在等变零样本任务上表现不佳。我们认为这是因为预训练模型为某些转换提供了更高质量的特征，而对其进行简单平均会产生不良影响。因此，我们提出了一种使用\textit{重要性权重}$\lambda$对特征进行平均的$\lambda$-\textit{equitune} 方法。这些权重是使用一个小型神经网络直接从数据中学习的，从而导致出色的零样本和微调结果。

    Efficient transfer learning algorithms are key to the success of foundation models on diverse downstream tasks even with limited data. Recent works of \cite{basu2022equi} and \cite{kaba2022equivariance} propose group averaging (\textit{equitune}) and optimization-based methods, respectively, over features from group-transformed inputs to obtain equivariant outputs from non-equivariant neural networks. While \cite{kaba2022equivariance} are only concerned with training from scratch, we find that equitune performs poorly on equivariant zero-shot tasks despite good finetuning results. We hypothesize that this is because pretrained models provide better quality features for certain transformations than others and simply averaging them is deleterious. Hence, we propose $\lambda$-\textit{equitune} that averages the features using \textit{importance weights}, $\lambda$s. These weights are learned directly from the data using a small neural network, leading to excellent zero-shot and finetuned 
    
[^128]: 预训练语言模型的知识反思

    Knowledge Rumination for Pre-trained Language Models. (arXiv:2305.08732v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08732](http://arxiv.org/abs/2305.08732)

    本文提出了一种名为知识反思的新范式，旨在帮助预训练语言模型利用已经编码在其预训练参数中的相关潜在知识，而不需要从外部语料库中检索。这种方法通过在模型中添加提示，并将相关知识注入模型进行整合，取得了在常识推理任务和GLUE基准上的实验结果。

    

    先前的研究揭示了普通的预训练语言模型（PLMs）单独处理知识密集型NLP任务的能力不足，因此，一些工作尝试将外部知识集成到PLMs中。然而，尽管有着有前途的结果，但我们经验性地观察到，PLM可能已经在其预训练参数中编码了丰富的知识，但在应用到知识密集型任务时未能充分利用它们。在本文中，我们提出了一种名为知识反思的新范式，以帮助预训练语言模型利用相关的潜在知识，而不需要从外部语料库中检索它们。通过简单地在PLMs中添加一个如“据我所知”的提示，我们试图回顾相关的潜在知识，并将其注入模型以进行知识整合。我们将提出的知识反思应用于各种语言模型，包括RoBERTa、DeBERTa和GPT-3。在六个常识推理任务和GLUE基准上的实验结果显示.....

    Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fail to fully utilize them when applying them to knowledge-intensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize that related latent knowledge without retrieving it from the external corpus. By simply adding a prompt like "As far as I know" to the PLMs, we try to review related latent knowledge and inject them back into the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE benchmarks dem
    
[^129]: ChatGPT和GPT-4是否是金融文本分析的通用求解器？对几种典型任务进行检验。

    Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks. (arXiv:2305.05862v1 [cs.CL])

    [http://arxiv.org/abs/2305.05862](http://arxiv.org/abs/2305.05862)

    本研究探讨了ChatGPT和GPT-4在金融文本分析任务中的潜力，结果显示它们在数值推理上表现出色但在需要领域特定知识的任务上表现不佳。

    

    最近的大型语言模型如ChatGPT和GPT-4引起了人们的广泛关注，因为它们能够生成高质量的对话回应。尽管ChatGPT和GPT-4在通用文本语料库上经过了广泛的测试，展示了它们令人印象深刻的能力，但还没有对金融语料库进行研究。本研究旨在通过在零样本或少样本情况下考察ChatGPT和GPT-4作为典型金融文本分析问题求解器的潜力来弥补这一差距。具体而言，我们评估了它们在五个不同的金融文本数据集上进行的四项代表性任务的能力。初步研究表明，ChatGPT和GPT-4在需要领域特定知识的金融命名实体识别（NER）和情感分析等任务上表现不佳，而在数值推理任务上表现出色。我们报告了当前版本ChatGPT和GPT-4的优点和局限性，并将它们与现有技术进行了比较。

    The most recent large language models such as ChatGPT and GPT-4 have garnered significant attention, as they are capable of generating high-quality responses to human input. Despite the extensive testing of ChatGPT and GPT-4 on generic text corpora, showcasing their impressive capabilities, a study focusing on financial corpora has not been conducted. In this study, we aim to bridge this gap by examining the potential of ChatGPT and GPT-4 as a solver for typical financial text analytic problems in the zero-shot or few-shot setting. Specifically, we assess their capabilities on four representative tasks over five distinct financial textual datasets. The preliminary study shows that ChatGPT and GPT-4 struggle on tasks such as financial named entity recognition (NER) and sentiment analysis, where domain-specific knowledge is required, while they excel in numerical reasoning tasks. We report both the strengths and limitations of the current versions of ChatGPT and GPT-4, comparing them to 
    
[^130]: TidyBot: 应用大语言模型的个性化机器人物理辅助

    TidyBot: Personalized Robot Assistance with Large Language Models. (arXiv:2305.05658v1 [cs.RO])

    [http://arxiv.org/abs/2305.05658](http://arxiv.org/abs/2305.05658)

    本文研究了使用机器人进行家庭清扫的个性化问题。通过使用大型语言模型少样本摘要能力，机器人可以学习用户的偏好并将其推广到未来的场景中，从而实现快速适应。

    

    为了使机器人能够有效个性化地提供物理辅助，它必须学习用户的个人喜好并将其应用于未来的场景中。本文研究了使用机器人进行家庭清扫的个性化问题，这些机器人能够通过捡起物品并将其放回原处来整理房间。一个关键的挑战是确定每个物品的正确位置，因为人们的喜好可以因个人品味或文化背景而大不相同。例如，一个人可能喜欢把衬衫放在抽屉里，而另一个人可能喜欢把衬衫放在架子上。我们旨在建立系统，这些系统可以通过与特定人的先前交互学习这样的喜好，而只需要几个示例。我们展示了机器人可以将基于语言的规划和感知与大型语言模型(LLMs)的少样本摘要能力相结合，从而推断出广泛适用于未来交互的用户偏好。这种方法实现了快速适应，并取得了91.2%的准确率。

    For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accurac
    
[^131]: 一项多模态模型融合的实证研究

    An Empirical Study of Multimodal Model Merging. (arXiv:2304.14933v1 [cs.CV])

    [http://arxiv.org/abs/2304.14933](http://arxiv.org/abs/2304.14933)

    本研究通过融合在不同模态上训练的transformer进行多模态模型融合，并提出一种参数有效的模态不可知架构，形成有效的训练配方。

    

    模型融合（例如插值或任务算术）将在不同任务上训练的多个模型合并以生成多任务解决方案。该技术在先前的研究中已经被证明成功，其中模型是在相似的任务和相同的初始化下训练的。在本文中，我们通过将在不同模态上训练的transformer进行融合，将此概念扩展到多模态设置。此外，我们针对一个新颖的目标进行研究，在该目标中，我们可以将视觉、语言和跨模态的transformer合并到特定模态的架构中，以创建一个参数有效的模态不可知架构。通过全面的实验，我们系统地研究了影响模型融合后性能的关键因素，包括初始化、融合机制和模型架构。我们的分析得出了一个有效的训练配方，可以通过模型融合来匹配模态不可知基线的性能（即从头开始预训练）。我们的代码可供使用。

    Model merging (e.g., via interpolation or task arithmetic) fuses multiple models trained on different tasks to generate a multi-task solution. The technique has been proven successful in previous studies, where the models are trained on similar tasks and with the same initialization. In this paper, we expand on this concept to a multimodal setup by merging transformers trained on different modalities. Furthermore, we conduct our study for a novel goal where we can merge vision, language, and cross-modal transformers of a modality-specific architecture to create a parameter-efficient modality-agnostic architecture. Through comprehensive experiments, we systematically investigate the key factors impacting model performance after merging, including initialization, merging mechanisms, and model architectures. Our analysis leads to an effective training recipe for matching the performance of the modality-agnostic baseline (i.e. pre-trained from scratch) via model merging. Our code is availa
    
[^132]: 大型语言模型对齐的基本限制

    Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])

    [http://arxiv.org/abs/2304.11082](http://arxiv.org/abs/2304.11082)

    本文通过提出一种理论方法——行为期望边界（BEB），展示了大型语言模型中对齐的基本限制，并证明任何对齐过程都无法根除不希望的行为，这对于防止恶意攻击是不安全的。

    

    开发与人交互的语言模型的重要方面是对齐其行为，使其对其人类用户有用且无害。这通常通过调整模型的方式来实现，以增强所需的行为并抑制不希望的行为。在本文中，我们提出了一种名为行为期望边界(BEB)的理论方法，它允许我们正式研究大型语言模型中的几个内在特征和对齐的限制。重要的是，我们证明对于任何具有被该模型表现出的有限概率的行为，都存在可以触发模型输出此行为的提示，其概率随提示的长度增加而增加。这意味着任何减弱不希望的行为但未将其完全消除的对齐过程都无法抵御针对性攻击。此外，我们的框架提示了领先的

    An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading al
    
[^133]: YOLO-Drone: 高空即时检测密集小物体的无人机技术

    YOLO-Drone:Airborne real-time detection of dense small objects from high-altitude perspective. (arXiv:2304.06925v1 [cs.CV])

    [http://arxiv.org/abs/2304.06925](http://arxiv.org/abs/2304.06925)

    YOLO-Drone是一种能够高效检测小尺度物体的实时物体检测算法，并在无人机平台上得到了应用，以广义交集联盟(GIOU)作为损失函数，取得了最佳的检测效果。

    

    无人机远程感应目标探测技术已广泛应用于许多领域，并成为计算机视觉领域的主要研究方向。然而，由于物体大小、图像降噪和实时性等因素的挑战，小尺度物体的可靠检测一直是具有挑战性的。为解决这些问题，提出了一种实时物体检测算法(YOLO-Drone)，并将其应用于两种新的无人机平台以及一种特定的光源(硅基金LED)。YOLO-Drone提出了几个创新：1)包括一个新的骨干Darknet59；2)一个新的复杂特征聚合模块MSPP-FPN，它包括一个空间金字塔池化和三个扩张空间金字塔池化模块；3)使用广义交集联盟(Generalized Intersection over Union，GIOU)作为损失函数。为了评估性能，使用了两个基准数据集，结果表明YOLO-Drone在小物体检测方面实现了最优结果。

    Unmanned Aerial Vehicles (UAVs), specifically drones equipped with remote sensing object detection technology, have rapidly gained a broad spectrum of applications and emerged as one of the primary research focuses in the field of computer vision. Although UAV remote sensing systems have the ability to detect various objects, small-scale objects can be challenging to detect reliably due to factors such as object size, image degradation, and real-time limitations. To tackle these issues, a real-time object detection algorithm (YOLO-Drone) is proposed and applied to two new UAV platforms as well as a specific light source (silicon-based golden LED). YOLO-Drone presents several novelties: 1) including a new backbone Darknet59; 2) a new complex feature aggregation module MSPP-FPN that incorporated one spatial pyramid pooling and three atrous spatial pyramid pooling modules; 3) and the use of Generalized Intersection over Union (GIoU) as the loss function. To evaluate performance, two bench
    
[^134]: 使用SQL查询大型语言模型

    Querying Large Language Models with SQL. (arXiv:2304.00472v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2304.00472](http://arxiv.org/abs/2304.00472)

    该论文介绍了使用SQL查询大型语言模型的方法，通过利用预训练的LLMs中的信息，可以从非结构化文本中提取数据并进行查询。通过Galois原型实现了查询LLMs的新物理运算符，并取得了令人鼓舞的结果。

    

    在许多使用场景中，信息存储在文本中，但无法在结构化数据中获取。然而，从自然语言文本中提取数据以精确适配模式，并实现查询，是一项具有挑战性的任务。随着预训练的大型语言模型（LLMs）的兴起，现在有了有效的解决方案，可以存储和使用从大规模文本文档中提取的信息。因此，我们设想使用SQL查询来涵盖传统数据库无法提取的广泛数据，通过利用LLMs中的信息。为了支撑这个愿景，我们提出了基于传统数据库体系结构的Galois原型，但具有用于查询底层LLM的新物理算子。主要思想是使用提示符执行查询计划中的某些操作符，从LLM中检索数据。对于大类别的SQL查询，查询LLMs返回结构良好的关系，取得了令人鼓舞的定性结果。初步实验结果使预训练的LLMs成为一个有前景的方法。

    In many use-cases, information is stored in text but not available in structured data. However, extracting data from natural language text to precisely fit a schema, and thus enable querying, is a challenging task. With the rise of pre-trained Large Language Models (LLMs), there is now an effective solution to store and use information extracted from massive corpora of text documents. Thus, we envision the use of SQL queries to cover a broad range of data that is not captured by traditional databases by tapping the information in LLMs. To ground this vision, we present Galois, a prototype based on a traditional database architecture, but with new physical operators for querying the underlying LLM. The main idea is to execute some operators of the the query plan with prompts that retrieve data from the LLM. For a large class of SQL queries, querying LLMs returns well structured relations, with encouraging qualitative results. Preliminary experimental results make pre-trained LLMs a prom
    
[^135]: PAIR-Diffusion: 采用结构和外观配对扩散模型进行对象级图像编辑

    PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models. (arXiv:2303.17546v1 [cs.CV])

    [http://arxiv.org/abs/2303.17546](http://arxiv.org/abs/2303.17546)

    本论文提出了一种采用结构和外观配对扩散模型进行对象级图像编辑的方法，使用户能够精细控制图像中的不同对象属性，同时自动传播注入的外观到具有相似结构的对象。

    

    最近，使用扩散模型进行图像编辑发展迅速。以前的作品可以通过各种方式进行控制和编辑图像，某些作品使用高级条件（例如文本），而其他作品使用低级条件。然而，大多数作品缺乏对图像中不同对象的属性进行精细化控制，即对象级图像编辑。本文将图像视为由多个对象组成，每个对象由不同属性定义。我们发现结构和外观是最直观且最有用于编辑的属性。我们提出了结构和外观配对扩散模型（PAIR-Diffusion），该模型使用从图像中明确提取的结构和外观信息进行训练。所提出的模型使用户能够在对象和全局级别将参考图像的外观注入输入图像中。此外，PAIR-Diffusion自动将注入的外观传播到输入图像中具有类似结构的对象。

    Image editing using diffusion models has witnessed extremely fast-paced growth recently. There are various ways in which previous works enable controlling and editing images. Some works use high-level conditioning such as text, while others use low-level conditioning. Nevertheless, most of them lack fine-grained control over the properties of the different objects present in the image, i.e. object-level image editing. In this work, we consider an image as a composition of multiple objects, each defined by various properties. Out of these properties, we identify structure and appearance as the most intuitive to understand and useful for editing purposes. We propose Structure-and-Appearance Paired Diffusion model (PAIR-Diffusion), which is trained using structure and appearance information explicitly extracted from the images. The proposed model enables users to inject a reference image's appearance into the input image at both the object and global levels. Additionally, PAIR-Diffusion a
    
[^136]: GOOD：基于优化的融合方法用于通过LiDAR和相机的3D物体检测

    GOOD: General Optimization-based Fusion for 3D Object Detection via LiDAR-Camera Object Candidates. (arXiv:2303.09800v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2303.09800](http://arxiv.org/abs/2303.09800)

    GOOD是一个基于优化的融合框架，可实现高精度、高鲁棒性的3D物体检测，适用于任意2D和3D检测器的组合，并且无需额外模型训练。

    

    3D物体检测是自动驾驶中感知任务的核心基础。近年来，多模态融合策略在提高3D物体检测的鲁棒性和准确性方面取得了快速进展。然而，当前的鲁棒融合研究都是基于学习的框架，需要大量的训练数据，并且在新场景中实施不方便。在本文中，我们提出了GOOD，一个通用的基于优化的融合框架，可以在不训练额外模型的情况下达到令人满意的检测效果，并且适用于任意2D和3D检测器的组合，以提高3D检测的准确性和鲁棒性。首先，我们应用了互相邻近的概率模型实现了3D-2D数据关联。然后，我们设计了一个优化流程，可以根据匹配结果分别优化不同类型的实例。此外，还引入了3D MOT方法，通过之前的帧来增强性能。

    3D object detection serves as the core basis of the perception tasks in autonomous driving. Recent years have seen the rapid progress of multi-modal fusion strategies for more robust and accurate 3D object detection. However, current researches for robust fusion are all learning-based frameworks, which demand a large amount of training data and are inconvenient to implement in new scenes. In this paper, we propose GOOD, a general optimization-based fusion framework that can achieve satisfying detection without training additional models and is available for any combinations of 2D and 3D detectors to improve the accuracy and robustness of 3D detection. First we apply the mutual-sided nearest-neighbor probability model to achieve the 3D-2D data association. Then we design an optimization pipeline that can optimize different kinds of instances separately based on the matching result. Apart from this, the 3D MOT method is also introduced to enhance the performance aided by previous frames.
    
[^137]: 知识图谱的构建：现状和挑战

    Construction of Knowledge Graphs: State and Challenges. (arXiv:2302.11509v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.11509](http://arxiv.org/abs/2302.11509)

    知识图谱构建面临着增量更新和步骤之间相互作用的挑战。本文讨论了主要图模型和构建流程需求，对构建高质量知识图谱的必要步骤进行了概述，并评估了现有知识图谱构建的研究现状。

    

    随着知识图谱在诸多应用中扮演着重要角色，如推荐系统和问答系统，构建和持续更新知识图谱的通用流程的需求正在增加。虽然从非结构化数据（如文本）和结构化数据源（如数据库）创建知识图谱的各个步骤已经得到了广泛研究，但是目前对于增量更新知识图谱和各个步骤之间的相互作用的研究还很有限。本文首先讨论了知识图谱的主要图模型，并介绍了未来知识图谱构建流程的主要需求。然后，我们概述了构建高质量知识图谱所需的必要步骤，包括元数据管理、本体发展和质量保证等交叉主题。最后，我们评估了与已介绍的需求相一致的特定知识图谱的构建现状。

    With knowledge graphs (KGs) at the center of numerous applications such as recommender systems and question answering, the need for generalized pipelines to construct and continuously update such KGs is increasing. While the individual steps that are necessary to create KGs from unstructured (e.g. text) and structured data sources (e.g. databases) are mostly well-researched for their one-shot execution, their adoption for incremental KG updates and the interplay of the individual steps have hardly been investigated in a systematic manner so far. In this work, we first discuss the main graph models for KGs and introduce the major requirement for future KG construction pipelines. Next, we provide an overview of the necessary steps to build high-quality KGs, including cross-cutting topics such as metadata management, ontology development, and quality assurance. We then evaluate the state of the art of KG construction w.r.t the introduced requirements for specific popular KGs as well as so
    
[^138]: 基于泛化的相似性

    Generalization-based similarity. (arXiv:2302.10096v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.10096](http://arxiv.org/abs/2302.10096)

    本文从头构建了一个基于抽象代数和定性概念的相似性概念，并将其通过模型论类型自然地嵌入到一阶逻辑中。

    

    检测和利用看似不相关的对象之间的相似性是类比推理的核心，而类比推理又是人工智能的核心。本文从头开始开发了一个基于抽象代数和定性概念的相似性概念，基于观察到泛化集合可以编码元素的重要属性。我们证明了以这种方式定义的相似性具有吸引人的数学属性。通过从基本概念出发构建相似性概念，并将其自然地嵌入到模型论类型中的一阶逻辑中，我们使读者相信其合理性。

    Detecting and exploiting similarities between seemingly distant objects is at the core of analogical reasoning which itself is at the core of artificial intelligence. This paper develops {\em from the ground up} an abstract algebraic and {\em qualitative} notion of similarity based on the observation that sets of generalizations encode important properties of elements. We show that similarity defined in this way has appealing mathematical properties. As we construct our notion of similarity from first principles using only elementary concepts of universal algebra, to convince the reader of its plausibility, we show that it can be naturally embedded into first-order logic via model-theoretic types.
    
[^139]: 通过潜在空间数据增强和面部姿势重建改进人脸识别

    Enhancing Face Recognition with Latent Space Data Augmentation and Facial Posture Reconstruction. (arXiv:2301.11986v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.11986](http://arxiv.org/abs/2301.11986)

    本文提出了一种名为面部表示增强（FRA）的方法，该方法通过操纵任何面部表示学习算法生成的面部嵌入，创造出具有改变姿势的新嵌入，表示相同身份和面部情感。大量实验证明了该方法的有效性和提供无噪声、全新的面部表示的能力。

    

    目前大部分基于深度学习的人脸识别系统由于训练数据量不足而表现出明显的性能下降。尽管很多研究已经研发了新的数据增强技术，包括输入空间的变换和生成对抗网络（GAN）的特征空间增强，但这些技术仍无法满足预期。本文提出了一种名为面部表示增强（FRA）的方法，用于增强人脸数据集。据我们所知，FRA是第一个将重点放在通过任何面部表示学习算法生成的面部嵌入来创建代表相同身份和面部情感但具有改变姿势的新嵌入的方法。本研究进行了大量实验证明了我们方法的有效性和其提供无噪声、全新的面部表示的能力。

    The small amount of training data for many state-of-the-art deep learning-based Face Recognition (FR) systems causes a marked deterioration in their performance. Although a considerable amount of research has addressed this issue by inventing new data augmentation techniques, using either input space transformations or Generative Adversarial Networks (GAN) for feature space augmentations, these techniques have yet to satisfy expectations. In this paper, we propose an approach named the Face Representation Augmentation (FRA) for augmenting face datasets. To the best of our knowledge, FRA is the first method that shifts its focus towards manipulating the face embeddings generated by any face representation learning algorithm to create new embeddings representing the same identity and facial emotion but with an altered posture. Extensive experiments conducted in this study convince of the efficacy of our methodology and its power to provide noiseless, completely new facial representations
    
[^140]: 人类视觉分割的不确定性测量

    Measuring uncertainty in human visual segmentation. (arXiv:2301.07807v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.07807](http://arxiv.org/abs/2301.07807)

    本论文提出了一种新方法，通过测量基于像素的相同-不同判断并基于模型进行重建，来测量人类视觉分割的不确定性，并表明图像的不确定性影响了人类的可变性的测量，并影响了参与者的决策。

    

    将视觉刺激分割为特征和视觉对象的不同组是视觉功能的核心。经典的心理物理方法揭示了许多有关人类感知分割的规则，而最近在机器学习方面的进展已经产生了成功的算法。然而，人类分割的计算逻辑仍不清楚，部分原因是因为我们缺乏可控制的范例来测量感知分割图并定量比较模型。在这里，我们提出了一种新的综合方法：给定一张图片，我们测量多个基于像素的相同-不同判断，并对潜在的分割图进行基于模型的重建。重建对多种实验操作具有鲁棒性，并捕捉了个体参与者的变异性。我们在人类对自然图像和复合材料纹理的分割上验证了该方法的准确性。我们表明图像的不确定性影响了人类的可变性的测量，并影响了参与者的决策。

    Segmenting visual stimuli into distinct groups of features and visual objects is central to visual function. Classical psychophysical methods have helped uncover many rules of human perceptual segmentation, and recent progress in machine learning has produced successful algorithms. Yet, the computational logic of human segmentation remains unclear, partially because we lack well-controlled paradigms to measure perceptual segmentation maps and compare models quantitatively. Here we propose a new, integrated approach: given an image, we measure multiple pixel-based same--different judgments and perform model--based reconstruction of the underlying segmentation map. The reconstruction is robust to several experimental manipulations and captures the variability of individual participants. We demonstrate the validity of the approach on human segmentation of natural images and composite textures. We show that image uncertainty affects measured human variability, and it influences how partici
    
[^141]: 社会机制设计：实现最大可接受决策

    Social Mechanism Design: Making Maximally Acceptable Decisions. (arXiv:2211.08501v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2211.08501](http://arxiv.org/abs/2211.08501)

    该论文提出了一个新颖的集体决策模型，考虑了代理人对决策结果和过程的关注，通过开发聚合偏好的机制来最大化决策的可接受性，并在特定情景中进行了实证研究。

    

    代理人不仅关心集体决策的结果，还关心决策的方式。在许多情况下，决策的结果和过程都会影响代理人是否认为决策是合法、可辩解或可接受的。我们提出了一个新颖的集体决策模型，考虑了代理人的偏好和对偏好聚合过程的高阶关注。为此，我们（1）提出了自然、可信的偏好结构并建立了其关键属性；（2）开发了聚合这些偏好以最大化决策的接受度的机制；（3）对我们的最大化接受机制的性能进行了表征。我们将我们的通用方法应用于二选一选择的具体情景，并比较了不同类型的代理人群体中可实现的最差接受率。我们还在修正程序的特殊情况下展示了通过阿布拉莫维茨提出的方法。

    Agents care not only about the outcomes of collective decisions but also about how decisions are made. In many cases, both the outcome and the procedure affect whether agents see a decision as legitimate, justifiable, or acceptable. We propose a novel model for collective decisions that takes into account both the preferences of the agents and their higher order concerns about the process of preference aggregation. To this end we (1) propose natural, plausible preference structures and establish key properties thereof, (2) develop mechanisms for aggregating these preferences to maximize the acceptability of decisions, and (3) characterize the performance of our acceptance-maximizing mechanisms. We apply our general approach to the specific setting of dichotomous choice, and compare the worst-case rates of acceptance achievable among populations of agents of different types. We also show in the special case of rule selection, i.e., amendment procedures, the method proposed by Abramowitz
    
[^142]: 对开放式学习进行增强拓扑智能体的研究

    Augmentative Topology Agents For Open-Ended Learning. (arXiv:2210.11442v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.11442](http://arxiv.org/abs/2210.11442)

    本研究针对开放式学习问题，引入了一种同时演化智能体和更具挑战性环境的方法。通过允许智能体的控制器变得更加复杂，我们发现可以提高泛化能力。实证结果表明，这种方法可以产生通用智能体，能够解决更多环境的问题。

    

    在这项工作中，我们通过引入一种同时演化智能体和日益具有挑战性的环境的方法来解决开放式学习的问题。与之前优化智能体使用固定神经网络拓扑的开放式方法不同，我们假设通过允许智能体的控制器在遇到更困难的环境时变得更加复杂，可以改善泛化能力。我们的方法，增强拓扑EPOET（ATEP），通过允许智能体随着时间的推移演化出自己的神经网络结构，根据需要增加复杂性和容量。实证结果表明，ATEP可以产生能够解决更多环境的通用智能体，而固定拓扑基线则无法做到这一点。我们还研究了智能体在环境之间的转移机制，发现基于物种的方法进一步改善了智能体的性能和泛化能力。

    In this work, we tackle the problem of open-ended learning by introducing a method that simultaneously evolves agents and increasingly challenging environments. Unlike previous open-ended approaches that optimize agents using a fixed neural network topology, we hypothesize that generalization can be improved by allowing agents' controllers to become more complex as they encounter more difficult environments. Our method, Augmentative Topology EPOET (ATEP), extends the Enhanced Paired Open-Ended Trailblazer (EPOET) algorithm by allowing agents to evolve their own neural network structures over time, adding complexity and capacity as necessary. Empirical results demonstrate that ATEP results in general agents capable of solving more environments than a fixed-topology baseline. We also investigate mechanisms for transferring agents between environments and find that a species-based approach further improves the performance and generalization of agents.
    
[^143]: 直比代数

    Proportional algebras. (arXiv:2210.01751v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.01751](http://arxiv.org/abs/2210.01751)

    本文引入了直比代数，探讨了保持比拟比例的函数的数学性质，将其与直比同态、同余和直比函子联系起来，为模拟比例和模拟推理的数学理论提供了进一步理解。

    

    比拟比例是形式为"$a$对$b$，正如$c$对$d$"的表达式，是模拟推理的核心，而模拟推理又是人工智能的核心。本文引入了直比代数作为一种带有4元比拟比例关系$a:b::c:d$满足一组适当公理的代数结构。在人工智能中，保持比拟比例的函数已经被证明具有实际意义，而研究它们的数学性质对于理解比例是至关重要的。因此，我们引入了直比同态及其关联的同余和直比函子，并展示了它们之间的密切关联。从更广泛的意义上说，本文是迈向模拟比例和模拟推理的数学理论的进一步步骤。

    Analogical proportions are expressions of the form "$a$ is to $b$ what $c$ is to $d$" at the core of analogical reasoning which itself is at the core of artificial intelligence. This paper introduces proportional algebras as algebras endowed with a 4-ary analogical proportion relation $a:b::c:d$ satisfying a suitable set of axioms. Functions preserving analogical proportions have already proven to be of practical interest in artificial intelligence and studying their mathematical properties is essential for understanding proportions. We therefore introduce proportional homomorphisms and their associated congruences and proportional functors, and show that they are closely related notions. In a broader sense, this paper is a further step towards a mathematical theory of analogical proportions and analogical reasoning in general.
    
[^144]: 博弈论目标空间规划

    Game-theoretic Objective Space Planning. (arXiv:2209.07758v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.07758](http://arxiv.org/abs/2209.07758)

    本文提出了一种博弈论目标空间规划方法，同时生成竞争策略和执行连续运动规划，通过抽象实现智能体动作离散化，提供清晰的意图，以及使用对策回归最小化的规划策略。

    

    在对抗性环境中同时生成竞争策略和执行连续运动规划是一个具有挑战性的问题。此外，在对抗性多智能体环境中部署自主系统需要理解其他智能体的意图。现有方法要么通过对类似控制输入进行分组来离散化智能体动作，从而牺牲运动规划的性能，要么在难以理解的潜在空间中进行规划，产生难以理解的智能体行为。此外，最流行的策略优化框架没有意识到行动的长期影响并且变得目光短浅。本文提出了一种通过抽象实现智能体动作离散化的方法，该方法提供了智能体动作的清晰意图，一个高效的离线智能体群体合成流程以及使用函数逼近的对策回归最小化的规划策略。最后，我们通过对按比例缩小的自主驾驶车辆进行实验验证了我们的发现。

    Generating competitive strategies and performing continuous motion planning simultaneously in an adversarial setting is a challenging problem. In addition, understanding the intent of other agents is crucial to deploying autonomous systems in adversarial multi-agent environments. Existing approaches either discretize agent action by grouping similar control inputs, sacrificing performance in motion planning, or plan in uninterpretable latent spaces, producing hard-to-understand agent behaviors. Furthermore, the most popular policy optimization frameworks do not recognize the long-term effect of actions and become myopic. This paper proposes an agent action discretization method via abstraction that provides clear intentions of agent actions, an efficient offline pipeline of agent population synthesis, and a planning strategy using counterfactual regret minimization with function approximation. Finally, we experimentally validate our findings on scaled autonomous vehicles in a head-to-h
    
[^145]: 扩散模型：方法和应用的综合调研

    Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v10 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.00796](http://arxiv.org/abs/2209.00796)

    本调研综合总结了扩散模型的方法和应用研究，包括高效采样、似然估计与特殊结构数据处理，介绍了扩散模型与其他生成模型相结合的潜力并回顾了其在不同领域的广泛应用，为进一步研究提出了可能的研究方向。

    

    扩散模型已成为一类具有记录性能的强大的深度生成模型，在许多应用中，包括图像合成、视频生成和分子设计方面表现出色。在这份调研中，我们概述了关于扩散模型的快速扩展研究，将研究分类为三个关键领域：高效采样、改进的似然估计和处理具有特殊结构的数据。我们还讨论了将扩散模型与其他生成模型相结合以实现增强结果的潜力。我们进一步回顾了扩散模型在计算机视觉、自然语言生成、时间数据建模以及其他科学学科的跨学科应用中的广泛应用。这个调研旨在提供一个具有背景的深入了解扩散模型的现状，确定关键的研究重点，指明可能的进一步研究领域。

    Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/Yan
    
[^146]: 论可行解释与被遗忘权之间的权衡

    On the Trade-Off between Actionable Explanations and the Right to be Forgotten. (arXiv:2208.14137v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.14137](http://arxiv.org/abs/2208.14137)

    本文研究了在数据删除请求的背景下，可行解释与被遗忘权之间的权衡问题。通过理论和实证分析，发现当少量数据删除请求触发预测模型的更新时，目前流行的最先进算法生成的可行解释可能失效。

    

    随着机器学习模型在高风险应用中的部署越来越多，决策者提出了更严格的数据保护法规（例如GDPR、CCPA）。其中一个关键原则是“被遗忘权”，即用户有权要求删除其数据。另一个关键原则是可行解释权，也称为算法追索权，允许用户撤销不利的决定。迄今为止，尚不清楚这两个原则是否可以同时实施。因此，我们在数据删除请求的背景下引入并研究了追索无效问题。具体而言，我们从理论和实证角度分析了流行的最先进算法的行为，并证明了这些算法生成的追索很可能会在少量数据删除请求（例如1或2个）引发对预测模型的更新时失效。对于不可区分模型的设置，我们提出一种框架。

    As machine learning (ML) models are increasingly being deployed in high-stakes applications, policymakers have suggested tighter data protection regulations (e.g., GDPR, CCPA). One key principle is the "right to be forgotten" which gives users the right to have their data deleted. Another key principle is the right to an actionable explanation, also known as algorithmic recourse, allowing users to reverse unfavorable decisions. To date, it is unknown whether these two principles can be operationalized simultaneously. Therefore, we introduce and study the problem of recourse invalidation in the context of data deletion requests. More specifically, we theoretically and empirically analyze the behavior of popular state-of-the-art algorithms and demonstrate that the recourses generated by these algorithms are likely to be invalidated if a small number of data deletion requests (e.g., 1 or 2) warrant updates of the predictive model. For the setting of differentiable models, we suggest a fra
    
[^147]: 在边缘进行主体子模型训练的大型模型联邦学习

    Federated Learning of Large Models at the Edge via Principal Sub-Model Training. (arXiv:2208.13141v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.13141](http://arxiv.org/abs/2208.13141)

    本文提出了一种解决资源受限边缘设备下大型模型联邦学习的方法，通过主体子模型训练，在不违反隐私承诺的前提下，使弱但重要的客户端能够参与到协作训练中。

    

    联邦学习（FL）是一种新兴的分散式学习框架，可以实现客户端之间的协作训练，无需共享私有数据，也无需传输至中心服务器。然而，考虑到许多边缘客户端缺乏足够的计算、内存或通信能力，联邦学习大规模模型仍面临重大瓶颈。为了保持弱但重要的客户端参与，之前的工作要么考虑了异构客户端设置，其中客户端使用不同大小的模型进行训练；要么将训练任务转移到服务端。然而，异构客户端设置要求某些客户端训练完整模型，这与资源受限的设置不一致；而后者在与服务器共享中间表示或标签时违反了FL的隐私承诺。为了克服这些限制，在这项工作中，我们在现实中提出了一个更少探索的跨设备FL设置，其中没有任何客户端需要训练完整模型。

    Federated Learning (FL) is emerging as a popular, promising decentralized learning framework that enables collaborative training among clients, with no need to share private data between them or to a centralized server. However, considering many edge clients do not have sufficient computing, memory, or communication capabilities, federated learning of large models still faces significant bottlenecks. To keep such weak but crucial clients in the loop, prior works either consider a heterogeneous-client setting where clients train models with different sizes; or offload training to the server. However, the heterogeneous-client setting requires some clients to train full model, which is not aligned with the resource-constrained setting; while the latter ones break privacy promises in FL when sharing intermediate representations or labels with the server. To overcome these limitations, in this work, we formulate a realistic, but much less explored, cross-device FL setting in which no client
    
[^148]: 单一一元代数中的类比比例

    Analogical proportions in monounary algebras. (arXiv:2208.06829v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.06829](http://arxiv.org/abs/2208.06829)

    本文研究了单一一元代数中的类比比例，在由自然数和后继函数组成的无限单一一元代数中，类比比例关系可以通过差别比例进行描述。

    

    本文研究了由一个宇宙和一个单一一元函数组成的单一一元代数中的类比比例。我们通过差别比例表明，类比比例关系可以在由自然数和后继函数组成的无限单一一元代数中进行描述。

    This paper studies analogical proportions in monounary algebras consisting only of a universe and a single unary function. We show that the analogical proportion relation is characterized in the infinite monounary algebra formed by the natural numbers together with the successor function via difference proportions.
    
[^149]: 知识显著跨度掩码：增强语言模型作为知识库

    Knowledgeable Salient Span Mask for Enhancing Language Models as Knowledge Base. (arXiv:2204.07994v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.07994](http://arxiv.org/abs/2204.07994)

    本文提出了一个名为知识显著跨度掩码的方法，通过全自监督学习帮助语言模型从非结构化文本中获取更多的知识。实验证明了该方法在知识密集型任务中的有效性。

    

    预训练语言模型（PLMs）如BERT在各种下游NLP任务中取得了显著进展。然而，通过要求模型进行填空式测试，最近的研究发现PLMs在从非结构化文本中获取知识方面存在不足。为了了解PLMs在检索知识时的内部行为，我们首先为非结构化文本定义了知识可见（K-B）标记和无知识（K-F）标记，并请专业标注员手动标记了一些样本。然后，我们发现PLMs更有可能对K-B标记给出错误预测，并且在自注意力模块内部对这些标记关注的更少。基于这些观察结果，我们以全自监督的方式开发了两种解决方案，帮助模型从非结构化文本中学习更多的知识。在知识密集型任务上的实验证明了所提方法的有效性。据我们所知，我们是第一个探索全自监督学习持续预训练知识的研究者。

    Pre-trained language models (PLMs) like BERT have made significant progress in various downstream NLP tasks. However, by asking models to do cloze-style tests, recent work finds that PLMs are short in acquiring knowledge from unstructured text. To understand the internal behaviour of PLMs in retrieving knowledge, we first define knowledge-baring (K-B) tokens and knowledge-free (K-F) tokens for unstructured text and ask professional annotators to label some samples manually. Then, we find that PLMs are more likely to give wrong predictions on K-B tokens and attend less attention to those tokens inside the self-attention module. Based on these observations, we develop two solutions to help the model learn more knowledge from unstructured text in a fully self-supervised manner. Experiments on knowledge-intensive tasks show the effectiveness of the proposed methods. To our best knowledge, we are the first to explore fully self-supervised learning of knowledge in continual pre-training.
    
[^150]: 使用迁移学习的深度学习模型在月度时间序列的多步预测中的性能

    Performance of Deep Learning models with transfer learning for multiple-step-ahead forecasts in monthly time series. (arXiv:2203.11196v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.11196](http://arxiv.org/abs/2203.11196)

    本研究比较了具有迁移学习和无迁移学习的深度学习模型以及其他传统方法在月度时间序列预测中的性能，结果显示基于TCN、LSTM和CNN的深度学习模型加上迁移学习的性能超过了其他传统方法，并且直接在目标时间序列上训练的TCN和LSTM在某些预测时段的性能相当或更好。

    

    深度学习和迁移学习模型被用于生成时间序列预测，然而对于月度时间序列的性能预测，缺乏证据。本文旨在比较具有迁移学习和没有迁移学习的深度学习模型以及其他传统方法在月度预测中的适用性，回答三个关于深度学习和迁移学习生成时间序列预测的问题。实验使用了M4和M3竞赛的时间序列。结果表明，基于TCN、LSTM和CNN的深度学习模型加上迁移学习往往能够超过其他传统方法的性能预测。另一方面，对于某些预测时段，直接在目标时间序列上训练的TCN和LSTM的性能与传统方法相当或更好。

    Deep Learning and transfer learning models are being used to generate time series forecasts; however, there is scarce evidence about their performance prediction that it is more evident for monthly time series. The purpose of this paper is to compare Deep Learning models with transfer learning and without transfer learning and other traditional methods used for monthly forecasts to answer three questions about the suitability of Deep Learning and Transfer Learning to generate predictions of time series. Time series of M4 and M3 competitions were used for the experiments. The results suggest that deep learning models based on TCN, LSTM, and CNN with transfer learning tend to surpass the performance prediction of other traditional methods. On the other hand, TCN and LSTM, trained directly on the target time series, got similar or better performance than traditional methods for some forecast horizons.
    
[^151]: 使用潜在目标模型进行快速开放世界导航探索

    Rapid Exploration for Open-World Navigation with Latent Goal Models. (arXiv:2104.05859v5 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2104.05859](http://arxiv.org/abs/2104.05859)

    本论文描述了一个机器人学习系统，可用于自主探索和导航不同的开放世界环境。该系统通过学习得到一个潜在变量模型和图像的拓扑记忆，使用信息瓶颈约束策略，能够在不到20分钟内通过视觉目标表示在开放世界中探索并发现目标。

    

    我们描述了一个用于自主探索和导航多样化开放世界环境的机器人学习系统。我们方法的核心是一个学习得到的距离和动作的潜在变量模型，以及一个非参数的图像拓扑记忆。我们使用信息瓶颈来规范学习的策略，使我们能够得到(i)紧凑的目标视觉表示，(ii)改进的泛化能力以及(iii)用于探索的可行目标采样机制。通过在大规模离线经验数据集上进行训练，该模型获得了对于任务无关干扰物具有鲁棒性的视觉目标表示。我们在开放世界探索场景中利用移动地面机器人演示了我们的方法。给定一个距离高达80米的目标图像，我们的方法利用其表示在不到20分钟内探索并发现了目标，即使在以前未见的障碍和天气条件下也能实现。请查看项目网站上的视频。

    We describe a robotic learning system for autonomous exploration and navigation in diverse, open-world environments. At the core of our method is a learned latent variable model of distances and actions, along with a non-parametric topological memory of images. We use an information bottleneck to regularize the learned policy, giving us (i) a compact visual representation of goals, (ii) improved generalization capabilities, and (iii) a mechanism for sampling feasible goals for exploration. Trained on a large offline dataset of prior experience, the model acquires a representation of visual goals that is robust to task-irrelevant distractors. We demonstrate our method on a mobile ground robot in open-world exploration scenarios. Given an image of a goal that is up to 80 meters away, our method leverages its representation to explore and discover the goal in under 20 minutes, even amidst previously-unseen obstacles and weather conditions. Please check out the project website for videos o
    
[^152]: 命题逻辑程序的顺序组合

    Sequential composition of propositional logic programs. (arXiv:2009.05774v7 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2009.05774](http://arxiv.org/abs/2009.05774)

    本文介绍了命题逻辑程序的顺序组合和分解方法，并提出了一种桥接命题逻辑程序的语法与语义的数学方法。

    

    本文介绍和研究了命题逻辑程序的顺序组合和分解。我们展示了无环程序可以被分解为单规则程序，并给出了任意程序的一般分解结果。我们展示了一个程序的直接推论算子可以通过组合来表示，从而可以计算其最小模型，而无需显式引用算子。这在数学上满意地填补了命题逻辑程序的语法与语义之间的概念差距。

    This paper introduces and studies the sequential composition and decomposition of propositional logic programs. We show that acyclic programs can be decomposed into single-rule programs and provide a general decomposition result for arbitrary programs. We show that the immediate consequence operator of a program can be represented via composition which allows us to compute its least model without any explicit reference to operators. This bridges the conceptual gap between the syntax and semantics of a propositional logic program in a mathematically satisfactory way.
    

