# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Physion++: Evaluating Physical Scene Understanding that Requires Online Inference of Different Physical Properties.](http://arxiv.org/abs/2306.15668) | 该论文提出了一个新的数据集和基准测试——Physion++，用于评估在需要准确估计场景中物体潜在物理属性的情况下的视觉物理预测。 |
| [^2] | [Testing of Detection Tools for AI-Generated Text.](http://arxiv.org/abs/2306.15666) | 本文研究了人工智能生成文本的检测工具的功能，并对其进行了评估。研究发现，现有的检测工具既不准确也不可靠。 |
| [^3] | [ShuttleSet22: Benchmarking Stroke Forecasting with Stroke-Level Badminton Dataset.](http://arxiv.org/abs/2306.15664) | 本研究提供了一个收集自2022年高排名比赛的羽毛球单打数据集ShuttleSet22，并使用最先进的拍球预测方法ShuttleNet进行了基准测试。 |
| [^4] | [Enhancing Representation Learning on High-Dimensional, Small-Size Tabular Data: A Divide and Conquer Method with Ensembled VAEs.](http://arxiv.org/abs/2306.15661) | 这项研究使用了基于集成VAE的分而治之方法，在高维度、低样本数量的任务中学习到更好的潜在表示，提高了样本效率，并在下游分类任务中取得了更高的准确性。 |
| [^5] | [The Distortion of Binomial Voting Defies Expectation.](http://arxiv.org/abs/2306.15657) | 本论文研究了投票规则的期望变形，提出了一种新颖的、直观的规则-二项式投票，并为所有分布提供了强大的期望变形保证。 |
| [^6] | [SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design.](http://arxiv.org/abs/2306.15656) | SparseOptimizer是一种深度学习优化器，通过Moreau-Yosida正则化在大型语言模型中引入稀疏性。它采用嵌入的收缩操作符，无需对代码进行修改即可适应各种大型语言模型，并在各种基准数据集上实现与密集型模型相当的性能，同时减少参数数量。 |
| [^7] | [Asynchronous Algorithmic Alignment with Cocycles.](http://arxiv.org/abs/2306.15632) | 该论文提出了一种将节点状态更新和消息函数调用分离的数学框架，以实现异步计算，并以此作为基础，进行了异步算法和神经网络的对齐。 |
| [^8] | [Machine-learning based noise characterization and correction on neutral atoms NISQ devices.](http://arxiv.org/abs/2306.15628) | 本研究基于机器学习技术，提出了两种方法来表征和校正中性原子NISQ设备上的噪声参数，为进一步提高计算结果的准确性提供了新的思路。 |
| [^9] | [LeanDojo: Theorem Proving with Retrieval-Augmented Language Models.](http://arxiv.org/abs/2306.15626) | 本文引入了LeanDojo，该工具通过提取Lean的数据，为定理证明研究提供了一个开放源代码的平台。利用LeanDojo的数据，开发了ReProver，它是第一个使用检索增强的语言模型的证明器，可以从庞大的数学库中选择命题，训练成本低，并且只需要一周的GPU训练时间。 |
| [^10] | [Value-aware Importance Weighting for Off-policy Reinforcement Learning.](http://arxiv.org/abs/2306.15625) | 本文提出了一种称为“价值感知重要性权重”的方法，用于校正离策略学习中的样本。这种方法可以降低重要性采样权重的方差并保持无偏性，从而提高实践中的稳定性和效果。 |
| [^11] | [Extending Context Window of Large Language Models via Positional Interpolation.](http://arxiv.org/abs/2306.15595) | 通过位置插值方法，我们可以在最小微调的情况下将RoPE-based预训练语言模型的上下文窗口扩展到最多32768，并在多个任务上获得强有力的实证结果。通过线性降低输入位置索引的大小，我们保持了扩展模型在原始上下文窗口内任务的质量。 |
| [^12] | [RansomAI: AI-powered Ransomware for Stealthy Encryption.](http://arxiv.org/abs/2306.15559) | 本文提出了一种名为RansomAI的基于人工智能的隐蔽加密勒索软件，它通过强化学习来适应加密行为，最小化被检测的可能性，并在加密时最大化损害功能。 |
| [^13] | [CamemBERT-bio: a Tasty French Language Model Better for your Health.](http://arxiv.org/abs/2306.15550) | 本研究介绍了CamemBERT-bio，它是一种针对法语生物医学领域专门设计的语言模型，相对于通用模型在命名实体识别任务上平均提高了2.54个百分点。 |
| [^14] | [When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions.](http://arxiv.org/abs/2306.15546) | 基础模型与联邦学习的交叉提供了解锁新可能性的独特机会，扩展了数据可用性，促进了协作式模型发展，并提高了性能和隐私保护。 |
| [^15] | [Unleashing the Power of User Reviews: Exploring Airline Choices at Catania Airport, Italy.](http://arxiv.org/abs/2306.15541) | 本研究通过使用新工具，探讨了社会影响机制与航空公司选择之间的关系，并通过对用户评论的分析，提供了关于卡塔尼亚机场航空生态系统中航空公司的重要见解。 |
| [^16] | [Enhancing Navigation Benchmarking and Perception Data Generation for Row-based Crops in Simulation.](http://arxiv.org/abs/2306.15517) | 本研究提出了一个合成数据集和一个虚拟场景集合，用于训练深度语义分割网络和快速评估导航算法。同时，还开发了一种自动参数方法用于探索不同的田地形状和特征。 |
| [^17] | [Prioritized Trajectory Replay: A Replay Memory for Data-driven Reinforcement Learning.](http://arxiv.org/abs/2306.15503) | 本研究提出了一种名为优先轨迹回放的回放记忆方法，将数据采样的视角扩展到轨迹中，从有限的数据中提取更全面的信息。这种方法通过反向采样轨迹来提高学习效率，并利用加权评论目标避免采样未见过的动作。优先轨迹回放还能根据不同的优先度指标优先采样效率更高的轨迹。 |
| [^18] | [A novel structured argumentation framework for improved explainability of classification tasks.](http://arxiv.org/abs/2306.15500) | 本文提出了一种新的结构化论证框架$xADG$，通过使用布尔逻辑运算符和多个支持来构建简洁且可理解的论证图，从而改进了分类任务的可解释性和预测能力。 |
| [^19] | [Using Large Language Models to Provide Explanatory Feedback to Human Tutors.](http://arxiv.org/abs/2306.15498) | 本文介绍了使用大型语言模型为人类导师提供解释性反馈的研究。通过两种方法，在在线课程中实时为导师提供有关如何给学生有效赞扬的反馈。其中一种方法使用了大型语言模型的命名实体识别技术，可以更好地提供解释性反馈。 |
| [^20] | [Precursor-of-Anomaly Detection for Irregular Time Series.](http://arxiv.org/abs/2306.15489) | 本文提出了一种新型异常检测方法，称为前体-异常检测（PoA检测）。与传统的异常检测不同，PoA检测旨在在异常发生之前检测到未来的异常。通过使用基于神经控制微分方程的神经网络和多任务学习算法，我们在17个基准线和3个数据集上进行实验证明了我们的方法的有效性。 |
| [^21] | [Cooperation or Competition: Avoiding Player Domination for Multi-Target Robustness via Adaptive Budgets.](http://arxiv.org/abs/2306.15482) | 本文研究了多目标鲁棒性中的合作与竞争之间的平衡问题，提出了一种通过调整对手预算来避免玩家主导的新框架，从而显著提高了多目标鲁棒性。 |
| [^22] | [Robust Proxy: Improving Adversarial Robustness by Robust Proxy Learning.](http://arxiv.org/abs/2306.15457) | 该论文提出了一种名为鲁棒代理学习的训练框架，通过显式学习鲁棒的特征表示来提高对抗鲁棒性。通过添加类别鲁棒扰动生成代表类别的鲁棒特征，并将其作为鲁棒代理来学习对抗性鲁棒特征。 |
| [^23] | [Advancing Adversarial Training by Injecting Booster Signal.](http://arxiv.org/abs/2306.15451) | 本文提出了一种通过注入增强信号来提高对抗性训练的方法，其使用外部信号而不是模型参数来提高对抗性鲁棒性和自然准确性。 |
| [^24] | [Understanding Social Reasoning in Language Models with Language Models.](http://arxiv.org/abs/2306.15448) | 这项研究提出了一种新的框架，通过填充因果模板来生成对大型语言模型（LLMs）进行评估，从而解决了之前评估结果不一致和现有评估方法的有效性存在疑虑的挑战。使用这个框架，他们创建了一个新的社交推理基准（BigToM），并发现人类参与者评价这个基准的质量更高。 |
| [^25] | [Are aligned neural networks adversarially aligned?.](http://arxiv.org/abs/2306.15447) | 我们研究了大型语言模型在面对对抗用户构建的对抗性输入时是否仍能保持对齐。我们发现现有的攻击手法不足以可靠攻击对齐文本模型，并通过蛮力方法找到了对抗性输入。 |
| [^26] | [Verifying Safety of Neural Networks from Topological Perspectives.](http://arxiv.org/abs/2306.15403) | 本研究提出了一种从拓扑角度研究神经网络安全性的方法，利用神经网络的同胚性质和开映射性质建立了输入集和输出集之间的严格保证，从而解决了神经网络在安全验证中的不确定性问题。 |
| [^27] | [Requirements for Explainability and Acceptance of Artificial Intelligence in Collaborative Work.](http://arxiv.org/abs/2306.15394) | AI在协同工作中应具备可解释性和可接受性的要求越来越重要。开发者需要了解模型内部运作，最终用户需要了解AI的结果或行为。用户的信息需求因上下文、领域知识和认知资源而有所差异。接受AI系统取决于提供的信息方式。 |
| [^28] | [Assessing Dataset Quality Through Decision Tree Characteristics in Autoencoder-Processed Spaces.](http://arxiv.org/abs/2306.15392) | 本文研究了机器学习分类任务中数据集质量评估的关键方面，通过多个数据集的实验，揭示了数据集质量对模型训练和性能的深远影响，并提出了一个全面的数据集质量评估框架。 |
| [^29] | [DCP-NAS: Discrepant Child-Parent Neural Architecture Search for 1-bit CNNs.](http://arxiv.org/abs/2306.15390) | DCP-NAS提出了一种差异性子父神经架构搜索方法，通过在实值模型的监督下搜索1位卷积神经网络，实现了在资源有限的嵌入式设备上高效的神经架构搜索。 |
| [^30] | [Herb-Drug Interactions: A Holistic Decision Support System in Healthcare.](http://arxiv.org/abs/2306.15365) | 设计了一种新颖的混合决策支持系统，利用人工智能技术识别草药与药物的相互作用，使药房社区在医疗保健系统中能够做出更好、更准确的治疗决策，并减轻可能出现的不良事件。 |
| [^31] | [Planning Landmark Based Goal Recognition Revisited: Does Using Initial State Landmarks Make Sense?.](http://arxiv.org/abs/2306.15362) | 本文提出了对以地标为基础的目标识别方法进行再讨论，结果表明在该方法中使用初始状态的地标没有优势。 |
| [^32] | [SSC-RS: Elevate LiDAR Semantic Scene Completion with Representation Separation and BEV Fusion.](http://arxiv.org/abs/2306.15349) | 本文提出了SSC-RS网络，通过表示分离和BEV融合来解决语义场景补全问题。通过深度监督将语义和几何表示的学习过程明确地分解为独立的分支，并利用自适应表示融合模块来聚合多尺度特征。 |
| [^33] | [PANet: LiDAR Panoptic Segmentation with Sparse Instance Proposal and Aggregation.](http://arxiv.org/abs/2306.15348) | PANet是一种新的LiDAR全景分割框架，通过引入稀疏实例提出和聚合的方法，提高了对大物体的分割性能。 |
| [^34] | [FedET: A Communication-Efficient Federated Class-Incremental Learning Framework Based on Enhanced Transformer.](http://arxiv.org/abs/2306.15347) | FedET是一种通信高效的联邦类增量学习框架，利用增强Transformer和增强器来解决联邦学习中的灾难性遗忘和通信成本问题，保证了高精度和数据隐私。 |
| [^35] | [Novel Hybrid-Learning Algorithms for Improved Millimeter-Wave Imaging Systems.](http://arxiv.org/abs/2306.15341) | 这项研究介绍了一种新型混合学习算法，用于改进毫米波成像系统。该算法通过结合信号处理和深度学习技术，提供了更好的分辨率、定位精度和检测率。通过利用无线电频率波形的已知特性，该算法能够更高效地训练深度学习模型。 |
| [^36] | [Homological Neural Networks: A Sparse Architecture for Multivariate Complexity.](http://arxiv.org/abs/2306.15337) | 本研究提出了一种新颖的深度神经网络架构，同调神经网络，通过应用网络过滤技术构建稀疏的高阶图结构，解决了深度学习中的计算复杂性和能源效率挑战，并在表格数据和时间序列回归问题上展示了优越的性能。 |
| [^37] | [Shoggoth: Towards Efficient Edge-Cloud Collaborative Real-Time Video Inference via Adaptive Online Learning.](http://arxiv.org/abs/2306.15333) | 本文提出了Shoggoth，一种针对实时视频推断的高效边缘-云协同架构。通过在线知识蒸馏和云端卸载标签过程，Shoggoth能够提高模型准确性并减轻边缘设备资源限制。实验结果表明，相比于仅使用边缘策略和仅使用云端策略，Shoggoth能够提供15%-20%的模型准确性改进和更低的网络成本。 |
| [^38] | [Towards predicting Pedestrian Evacuation Time and Density from Floorplans using a Vision Transformer.](http://arxiv.org/abs/2306.15318) | 本文提出了一种基于Vision Transformer的深度学习方法，可以从给定的平面图预测行人密度和总疏散时间。通过构建合成数据集并将模型无缝集成到现有系统中，我们展示了该方法的高效性和准确性。 |
| [^39] | [FAIRER: Fairness as Decision Rationale Alignment.](http://arxiv.org/abs/2306.15299) | 本文针对深度神经网络中的公平性问题，从决策合理性的角度研究，并定义了参数平等得分来表征公平决策过程。实证研究表明现有的公平性规范项无法实现决策合理性的对齐。 |
| [^40] | [Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task.](http://arxiv.org/abs/2306.15298) | 该论文通过引入新的偏见度量方法，并在一个现实的IMDB电影分类器的例子中对BERT的性别偏见进行了全面分析。研究结果表明，公开的BERT模型中存在着显著的性别偏见，并强调了负责任使用的重要性。 |
| [^41] | [IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning.](http://arxiv.org/abs/2306.15273) | IDOL是一种面向逻辑推理的指标导向预训练方法，通过使用逻辑指标和逻辑丰富的数据集在逻辑上增强了预训练模型。IDOL在逻辑推理MRC基准测试中取得了最先进的性能，并且具有竞争力的综合语言理解能力。 |
| [^42] | [Delivering Inflated Explanations.](http://arxiv.org/abs/2306.15272) | 本文提出了夸大的解释，它是为了解释人工智能系统的决策原因所定义的一种方法。传统的解释方法只显示选择的特征对决策的影响，而夸大的解释考虑了更多特征的可能性。 |
| [^43] | [Internal Contrastive Learning for Generalized Out-of-distribution Fault Diagnosis (GOOFD) Framework.](http://arxiv.org/abs/2306.15266) | 我们提出了一个称为广义离群故障诊断（GOOFD）框架的集成故障诊断系统，可以同时处理故障检测、故障分类和新颖故障诊断等任务。该框架基于内部对比学习方法，利用该方法提取特征并识别异常值。 |
| [^44] | [IIFL: Implicit Interactive Fleet Learning from Heterogeneous Human Supervisors.](http://arxiv.org/abs/2306.15228) | 本研究提出了Implicit Interactive Fleet Learning (IIFL)，将隐性策略推广到交互式模仿学习中，能够解决多模态和分布转移问题。 |
| [^45] | [Learning to Rank in Generative Retrieval.](http://arxiv.org/abs/2306.15222) | 这篇论文介绍了一种将生成式检索和经典的学习排序范例相结合的方法，通过使用段落排序损失来训练自回归模型，直接优化自回归模型朝着最优解优化。 |
| [^46] | [Unsupervised Episode Generation for Graph Meta-learning.](http://arxiv.org/abs/2306.15217) | 本文研究了无监督的剧集生成方法，通过元学习解决没有标签的少样本节点分类问题。它们充分利用所有节点信息，并且通过泛化能力提高性能。 |
| [^47] | [Unsupervised Polychromatic Neural Representation for CT Metal Artifact Reduction.](http://arxiv.org/abs/2306.15203) | 本文提出了一种新颖的多色彩神经表示法（Polyner），用于解决CT成像中存在金属伪影的挑战性问题。Polyner通过建模非线性反问题，准确模拟CT采集过程，并利用无监督训练的神经网络架构恢复原始物体信息。实验证明Polyner在金属伪影减少方面的有效性。 |
| [^48] | [One-class systems seamlessly fit in the forward-forward algorithm.](http://arxiv.org/abs/2306.15188) | 本文研究了在前向训练方式下训练深度单类目标函数的性能，发现这些函数可以处理动态网络大小，为无缝在线训练带来了许多好处。 |
| [^49] | [Automatic Truss Design with Reinforcement Learning.](http://arxiv.org/abs/2306.15182) | 本文提出了一个两阶段的AutoTruss框架，利用蒙特卡洛树搜索和强化学习方法，高效生成轻型和合法的桁架布局。在实验中，AutoTruss显示出优越的性能，超越了之前报道的方法。 |
| [^50] | [Learning non-Markovian Decision-Making from State-only Sequences.](http://arxiv.org/abs/2306.15156) | 本文提出了一种从仅状态序列学习非马尔科夫决策的方法，通过深度生成建模和最大似然估计实现基于模型的模仿。学习的模型能够实现“推理式决策”，并在路径规划任务中展示了有效性。 |
| [^51] | [Contrastive Meta-Learning for Few-shot Node Classification.](http://arxiv.org/abs/2306.15154) | 这篇论文提出了一种对比元学习的方法来解决少样本节点分类的问题，通过从大量有标记节点的类别中抽取可迁移的知识，并将其推广到具有有限标记节点的其他类别，学习通用的节点嵌入。 |
| [^52] | [What Truly Matters in Trajectory Prediction for Autonomous Driving?.](http://arxiv.org/abs/2306.15136) | 在自动驾驶系统中，轨迹预测的准确性在固定数据集上表现很好，但在实际驾驶场景中却存在显著差异。现有的评估方法忽视了动力学差距和计算效率对预测结果的影响。 |
| [^53] | [The Perspective of Software Professionals on Algorithmic Racism.](http://arxiv.org/abs/2306.15133) | 本文研究了软件专业人员对算法种族歧视的观点，发现一些软件系统对黑人存在歧视，导致黑人在使用基于技术的服务方面面临不公平待遇。 |
| [^54] | [MIMIC: Masked Image Modeling with Image Correspondences.](http://arxiv.org/abs/2306.15128) | MIMIC是一种基于图像对应关系的遮蔽图像建模方法，通过挖掘不需要任何注释的数据集，使用多个自监督模型进行训练，达到了在多个下游任务上优于使用注释挖掘的表示的效果。 |
| [^55] | [Identifying and Consolidating Knowledge Engineering Requirements.](http://arxiv.org/abs/2306.15124) | 本文提出了通过使用主流的软件方法论来解决挑战的参考体系结构，并通过研究不同利益相关者和时代的需求，确定了23个关键的质量属性。通过对近期文献中的候选体系结构进行评估，最终讨论了迈向全面参考体系结构的下一步。 |
| [^56] | [Evaluation of OpenAI Codex for HPC Parallel Programming Models Kernel Generation.](http://arxiv.org/abs/2306.15121) | 本论文评估了在高性能计算中使用AI辅助生成能力来生成基本数值内核的效果，并测试了多种编程模型下生成的内核代码。结果表明，OpenAI Codex在C++中的输出与采纳度和成熟度相关。 |
| [^57] | [Continual Learning for Out-of-Distribution Pedestrian Detection.](http://arxiv.org/abs/2306.15117) | 本文提出了一种持续学习的解决方案，用于解决行人检测中的越域泛化问题。该方法通过对模型权重进行弹性权重合并，使其在新的数据集上学习并保持在旧数据集上的性能，避免了灾难性遗忘。在实验中，该方法在两个数据集上分别实现了9%和18%的误检率改进。 |
| [^58] | [From $O(\sqrt n)$ to $O(\log n)$ in Quadratic Programming.](http://arxiv.org/abs/2306.15079) | 这篇论文提出了一种迭代复杂度为$O(\log(n))$的二次规划优化算法，并通过严格的理论证明验证了该算法的可行性。这一重大突破使得我们从$O(\sqrt{n})$的优化算法过渡到$O(\log(n))$的优化算法，其在大数据和人工智能时代具有重要应用价值。 |
| [^59] | [Molecular geometric deep learning.](http://arxiv.org/abs/2306.15065) | 分子几何深度学习（Mol-GDL）提出了一种更通用的分子表示方法，通过仅使用非共价键构建的分子图，在分子性质预测中取得了与基于共价键模型相似甚至更好的结果，展现了超越基于共价键的分子图的巨大潜力。 |
| [^60] | [Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression.](http://arxiv.org/abs/2306.15063) | 预训练的transformer在回归问题中展现了非贝叶斯上下文学习能力，其在任务多样性阈值以下表现类似于贝叶斯估计器，而在阈值以上明显优于贝叶斯估计器，与岭回归一致。 |
| [^61] | [Optimized Vectorizing of Building Structures with Swap: High-Efficiency Convolutional Channel-Swap Hybridization Strategy.](http://arxiv.org/abs/2306.15035) | 本论文提出了一种高效的卷积通道交换混合策略（Swap），用于优化建筑结构的向量化。该方法通过将相邻或对角特征交替交换并混合不同通道的信息，实现了集成局部特征空间信息的功能。同时，采用了基于组的参数共享机制，大大减少了模型中的冗余参数。 |
| [^62] | [Beyond dynamic programming.](http://arxiv.org/abs/2306.15029) | 提出了一种新的理论方法——得分寿命规划，用于解决强化学习问题。方法可以搜索非稳态策略函数，并直接计算最优无限时间区间动作序列。这种方法的核心思想是建立动作序列和实数之间的映射，并通过优化问题计算最优动作序列。方法在非线性最优控制问题中证明了有效性。 |
| [^63] | [DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome.](http://arxiv.org/abs/2306.15006) | 本研究提出了DNABERT-2，一个用于多种物种基因组的高效基础模型和基准。我们通过使用基于统计的数据压缩算法Byte Pair Encoding（BPE）替代传统的k-mer标记化，克服了k-mer标记化的计算和样本效率问题，并取得了重要进展。 |
| [^64] | [SIMF: Semantics-aware Interactive Motion Forecasting for Autonomous Driving.](http://arxiv.org/abs/2306.14941) | 本文提出了一种名为SIMF的方法，用于自动驾驶车辆中语义感知的交互式运动预测。该方法通过实现基于语义的行为体选择和注意力机制提取全局编码，能够捕捉空间信息和语义信息，并优选相关的行为体进行运动预测。 |
| [^65] | [Sciama's argument on life in a random universe: Distinguishing apples from oranges.](http://arxiv.org/abs/2306.14934) | Sciama通过区分苹果和橙子的方式论述在随机宇宙中存在生命的问题，他认为生命的存在取决于许多量，但在没有对这些量有充分了解的情况下，他的论点暗示了一个看起来是“智能设计”的宇宙。 |
| [^66] | [Integrating Bidirectional Long Short-Term Memory with Subword Embedding for Authorship Attribution.](http://arxiv.org/abs/2306.14933) | 这项研究提出了一种将双向长短期记忆网络与子词嵌入相结合的方法，用于解决作者归属问题。该方法能够在处理文本中的隐含词问题的同时保留词的顺序上下文。 |
| [^67] | [LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding.](http://arxiv.org/abs/2306.14924) | 本研究探索了使用大型语言模型（LLMs）来减少演绎编码所需时间的方法，同时保留传统内容分析的灵活性。通过一个案例研究和经验基准测试，证明了在不同演绎编码任务上，GPT-3.5在LLM辅助内容分析（LACA）中的有效性。 |
| [^68] | [Utilizing Natural Language Processing for Automated Assessment of Classroom Discussion.](http://arxiv.org/abs/2306.14918) | 本研究利用自然语言处理技术，通过自动生成细化标准得分，实现对课堂讨论质量的自动评估。实验结果令人鼓舞，同时指出标准仍有改进空间，并发现不同的NLP方法对不同的标准更有效。 |
| [^69] | [Towards Enriched Controllability for Educational Question Generation.](http://arxiv.org/abs/2306.14917) | 本研究旨在通过引入新的引导属性（问题明确性）来丰富教育问题生成的可控性。我们提出了通过控制生成明确和隐含wh-问题的方法。研究展示了通过问题明确性和叙事要素同时控制问题生成的初步证据。 |
| [^70] | [GPT-4 Reticular Chemist for MOF Discovery.](http://arxiv.org/abs/2306.14915) | GPT-4网状化学家是一个集成了AI模型GPT-4的框架，通过迭代的人工智能交互，能够发现金属有机框架系列。 |
| [^71] | [FSUIE: A Novel Fuzzy Span Mechanism for Universal Information Extraction.](http://arxiv.org/abs/2306.14913) | FSUIE是一种用于普适信息抽取的新型模糊跨度机制，通过引入模糊跨度损失和模糊跨度注意力，能够在快速收敛和少量数据训练轮数的情况下显著提高信息抽取的性能。 |
| [^72] | ["You might think about slightly revising the title": identifying hedges in peer-tutoring interactions.](http://arxiv.org/abs/2306.14911) | 该研究利用多模态同伴辅导数据集构建了一个计算框架，用于识别同伴辅导互动中的措辞。最佳表现的是一个混合方法，它比现有基准更好且更容易解释，并发现了一些新特征。 |
| [^73] | [The Importance of Human-Labeled Data in the Era of LLMs.](http://arxiv.org/abs/2306.14910) | 本文论述了在LLMs时代，人标记数据仍然具有重要性的论据和支持。 |
| [^74] | [PRISMA-DFLLM: An Extension of PRISMA for Systematic Literature Reviews using Domain-specific Finetuned Large Language Models.](http://arxiv.org/abs/2306.14905) | PRISMA-DFLLM是将大型语言模型(LLMs)与PRISMA的严格报告指南相结合，通过在领域特定的学术论文上进行微调，提高了系统文献综述的效率和可扩展性，同时开启了新的研究机遇。 |
| [^75] | [Detect Depression from Social Networks with Sentiment Knowledge Sharing.](http://arxiv.org/abs/2306.14903) | 本论文通过深度学习技术以及情感知识共享，从社交网络消息中识别抑郁症的潜在迹象，旨在提供早期心理健康状况识别的方法。 |
| [^76] | [Data-Driven Approach for Formality-Sensitive Machine Translation: Language-Specific Handling and Synthetic Data Generation.](http://arxiv.org/abs/2306.14514) | 本文介绍了一种基于数据驱动的正式敏感机器翻译方法，通过语言特定处理和合成数据生成，显著改进了翻译性能。 |
| [^77] | [A General Framework for Sequential Decision-Making under Adaptivity Constraints.](http://arxiv.org/abs/2306.14468) | 本论文提出了一个通用框架，研究了在适应性约束下的顺序决策问题。具体地，我们提供了Eluder Condition类，并针对稀缺策略切换和批次学习约束分别提供了相应的算法。此工作是第一个考虑通用函数类别下稀缺策略切换和批次学习的工作，涵盖了之前研究中的大部分模型。 |
| [^78] | [Feature Adversarial Distillation for Point Cloud Classification.](http://arxiv.org/abs/2306.14221) | 本文提出了一种特征对抗蒸馏方法（FAD），用于解决点云分类中知识传递的信息损失问题。在实验证明，该方法在模型压缩的同时保持了竞争性能。 |
| [^79] | [Manipulation Risks in Explainable AI: The Implications of the Disagreement Problem.](http://arxiv.org/abs/2306.13885) | 本文讨论可解释人工智能中的操纵风险，即同一决策或预测可能有多种解释带来的挑战。本文分析了攻击机器学习模型或底层数据以影响解释与直接利用解释阶段的策略，并探讨了解释提供者可追求的几个目标和具体场景。 |
| [^80] | [Margin Maximization in Attention Mechanism.](http://arxiv.org/abs/2306.13596) | 这篇论文证明了，在softmax-attention模型中，通过在p或等价的W上运行梯度下降，可以收敛到一个最大边缘解，这将局部最优的标记与非最优的标记分隔开。这明确地将注意力机制形式化为标记分离机制。 |
| [^81] | [DiversiGATE: A Comprehensive Framework for Reliable Large Language Models.](http://arxiv.org/abs/2306.13230) | DiversiGATE是一个统一框架，汇集了多种LLM验证方法，其中包括自一致性、数学提示和WebGPT，同时提出了一个符合该框架的新模型“SelfLearner”，该模型可以从自己的输出中学习并优化性能，在实验中表现良好，GSM8K基准测试上提高了7%的性能。 |
| [^82] | [FuXi: A cascade machine learning forecasting system for 15-day global weather forecast.](http://arxiv.org/abs/2306.12873) | 逐步级联模型FuXi在15天全球天气预报中表现出更好的性能，并通过减少预测误差的积累达到优化。 |
| [^83] | [Pushing the Limits of Machine Design: Automated CPU Design with AI.](http://arxiv.org/abs/2306.12456) | 本研究运用新的人工智能方法，自动设计了一个中央处理器(CPU)，这是人类设计过的最复杂的装置之一，从而探索了机器设计的边界。 |
| [^84] | [Hyperbolic Active Learning for Semantic Segmentation under Domain Shift.](http://arxiv.org/abs/2306.11180) | 这项研究首次在Poincaré双曲球模型中运用超bolic活跃学习方法，利用区域内像素嵌入的半径变化作为新的数据获取策略，以提升域转移下语义分割的性能。 |
| [^85] | [Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network.](http://arxiv.org/abs/2306.10946) | 本文提出了一种基于注意力知识图卷积网络的旅游景点推荐模型，通过自动语义发掘目标景点的相邻实体，根据旅客的喜好选择，预测类似景点的概率，实验中取得良好效果。 |
| [^86] | [Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models.](http://arxiv.org/abs/2306.08952) | 本研究提出了一个全面的探测数据集来评估大型语言模型的时间推理能力，并提出了一种使用时间跨度提取和时敏性强化学习的新型学习框架来改进大型语言模型的时间推理能力。实验结果表明该方法的有效性。 |
| [^87] | [Survey on Sociodemographic Bias in Natural Language Processing.](http://arxiv.org/abs/2306.08158) | 本文调查了209篇关于NLP模型偏见的论文，其中大部分涉及社会人口统计偏见。研究者提出了社会人口统计偏见的定义，并确定了NLP偏见研究的三个主要类别。当前去偏见技术只是隐藏了偏见而不是真正去除它，需要进一步改进。 |
| [^88] | [How Can Recommender Systems Benefit from Large Language Models: A Survey.](http://arxiv.org/abs/2306.05817) | 本文对将大型语言模型（LLM）应用于推荐系统进行了全面的调查研究，从两个角度总结了现有的研究工作：如何在推荐系统中调整LLM和调整LLM时在哪里调整。最后，我们提出了一些潜在的研究方向和挑战。 |
| [^89] | [One-step Multi-view Clustering with Diverse Representation.](http://arxiv.org/abs/2306.05437) | 本文提出了一种一步多视角聚类与多样性表征的方法，将多视角学习和k-means聚类融合到一个统一框架中，实验结果表明其在各种标准的多视角数据集上都优于现有算法。 |
| [^90] | [Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised Learning.](http://arxiv.org/abs/2306.04265) | 本文介绍了一个用于异质半监督学习的新型图神经网络模型PEGFAN，它使用置换等变图框架实现了多尺度特征提取，表现优于其他最先进模型，特别是在相对较大和密集连接的数据集中。 |
| [^91] | [Reversible Quantization Index Modulation for Static Deep Neural Network Watermarking.](http://arxiv.org/abs/2305.17879) | 本文提出了一种使用可逆量化索引调制（QIM）的基于可逆数据隐藏（RDH）的静态深度神经网络水印方案，解决了现有方法在可用性、容量和保真度方面的弱点，并通过模拟结果验证了方案的可行性和有效性。 |
| [^92] | [Levin Tree Search with Context Models.](http://arxiv.org/abs/2305.16945) | 本文提出了一种新的具有上下文模型的Levin树搜索算法，通过将神经网络替换为上下文模型，实现了LTS损失的凸优化，并在多个基准测试中取得了明显优于LTS+NN的结果。 |
| [^93] | [Training Transitive and Commutative Multimodal Transformers with LoReTTa.](http://arxiv.org/abs/2305.14243) | LoReTTa是一个自监督框架，可以在具有不同模态的数据集中转换。通过合成数据集的方法，显著提高了下游任务的性能。 |
| [^94] | [Capturing Conversion Rate Fluctuation during Sales Promotions: A Novel Historical Data Reuse Approach.](http://arxiv.org/abs/2305.12837) | 本论文提出了一种名为HDR的新方法，通过重复使用历史促销数据，来捕捉促销转化模式，达到更好地适应促销模式的目的。 |
| [^95] | [Large-Scale Package Manipulation via Learned Metrics of Pick Success.](http://arxiv.org/abs/2305.10272) | 本文讨论了基于学习度量的大规模包裹操作，通过训练拾取成功预测器和学习拾取质量度量，实现了能够大规模部署的强力抓握策略。 |
| [^96] | [Quantified Semantic Comparison of Convolutional Neural Networks.](http://arxiv.org/abs/2305.07663) | 本研究提出了两种方法来量化卷积神经网络潜在空间中语义信息之间的相似性，从而揭示CNN层内语义信息的流动和相似性，以及不同网络之间的相似度程度。 |
| [^97] | [Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers.](http://arxiv.org/abs/2305.07011) | 本文提出了一种基于视觉变压器的对比图像-文本预训练方法，针对开放词汇的物体检测任务，采用区域感知预训练、聚焦损失和新颖物体提案等技术，在LVIS上取得了32.1$AP_r$的最佳效果。 |
| [^98] | [A Vision Transformer Approach for Efficient Near-Field Irregular SAR Super-Resolution.](http://arxiv.org/abs/2305.02074) | 本文提出了一种新的、用于近场不规则SAR超分辨率的算法，以应对高分辨率成像中遇到的独特挑战，为实现边缘和物联网(IoT)技术奠定技术基础。 |
| [^99] | [Discovering Object-Centric Generalized Value Functions From Pixels.](http://arxiv.org/abs/2304.13892) | 本文介绍了一种从像素中学习物体中心化的广义值函数的方法。该方法从物体中发现有意义的特征，转化为“问题”函数，并利用随后学习的广义值函数来进行控制，在静态和非静态设置下表现良好。学到的表示不仅是可解释的，而且围绕着具有不变性的物体，有助于快速适应。 |
| [^100] | [EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition.](http://arxiv.org/abs/2304.01508) | EPVT是一种基于环境感知的提示视觉Transformer，用于解决皮肤病变识别中深度神经网络可能过度依赖疾病不相关图像特征的问题，通过嵌入一组领域提示和一个共享提示来进行领域一般化，并且引入了领域提示生成器促进知识共享。 |
| [^101] | [mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection.](http://arxiv.org/abs/2303.09901) | 本研究提出了mCPT模型用于多语言的、多标签的零样本或少样本的框架检测任务，并在西班牙语和其他8种语言中取得了良好的成绩。该方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。 |
| [^102] | [Auditing large language models: a three-layered approach.](http://arxiv.org/abs/2302.08500) | 本文提出了一个三层次的方法来审计大型语言模型（LLMs），包括治理审计、模型审计和应用审计，解决LLMs带来的伦理和社会挑战。 |
| [^103] | [GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration.](http://arxiv.org/abs/2301.12686) | GibbsDDRM是一种扩展的局部折叠Gibbs采样方法，用于解决线性逆问题中线性算子未知的盲场景。它利用预训练的扩散模型构建了数据、测量和线性算子的联合分布，并通过后验采样来解决问题。该方法在盲图像去模糊和语音去混响任务上表现出了高性能，而且适用于各种逆问题。 |
| [^104] | [A Semi-supervised Sensing Rate Learning based CMAB Scheme to Combat COVID-19 by Trustful Data Collection in the Crowd.](http://arxiv.org/abs/2301.08563) | 本文提出了一种基于半监督学习的组合多臂赌博反向拍卖方案，用于解决移动众包系统中在招募多个未知和有策略的工作者时出现的数据可信问题。 |
| [^105] | [Faster Maximum Inner Product Search in High Dimensions.](http://arxiv.org/abs/2212.07551) | 该论文提出了一种在高维度中更快的最大内积搜索算法 BanditMIPS，该算法的复杂度与维度无关，并且通过自适应采样策略提供理论保证和实验证明。 |
| [^106] | [Hierarchical Decomposition and Analysis for Generalized Planning.](http://arxiv.org/abs/2212.02823) | 本文提出了一种分析和评估广义规划的新方法，通过层次化分解和使用图论推导出的终止论证，可以解决广义规划中的难题，并指导综合和学习过程。 |
| [^107] | [Multi-Head Adapter Routing for Cross-Task Generalization.](http://arxiv.org/abs/2211.03831) | 本文研究了跨任务泛化中适配器路由的作用，并设计了基于此的新变体Multi-Head Routing (MHR)，通过精细的路由和参数高效的微调实现了更好的性能和更高的参数效率。 |
| [^108] | [Iterative autoregression: a novel trick to improve your low-latency speech enhancement model.](http://arxiv.org/abs/2211.01751) | 本研究提出了一种简单而有效的替代技术，用于训练自回归低延迟语音增强模型，该方法在不同的架构和训练场景下均能带来稳定的改进。 |
| [^109] | [Benchmarking Reinforcement Learning Techniques for Autonomous Navigation.](http://arxiv.org/abs/2210.04839) | 本研究针对自主导航应用深度强化学习方法提出了四个期望：不确定性推理、安全性、有限试错数据学习和对多样环境的泛化能力。 |
| [^110] | [Real-Time Reinforcement Learning for Vision-Based Robotics Utilizing Local and Remote Computers.](http://arxiv.org/abs/2210.02317) | 本文实现了一个名为ReLoD的实时学习系统，利用本地和远程计算机来分配深度强化学习算法的计算，并在基于视觉的控制任务上进行了评估。结果显示SAC的性能下降了。 |
| [^111] | [Self-Supervised Exploration via Temporal Inconsistency in Reinforcement Learning.](http://arxiv.org/abs/2208.11361) | 本文在强化学习中提出了一种新的内在奖励方法，通过比较当前观察与历史知识的差异来评估好奇心，并利用时间不一致性作为内在奖励。实验证明该方法在稀疏外在奖励的情况下具有更高的性能和噪声容忍度。 |
| [^112] | [Asynchronous Execution of Heterogeneous Tasks in ML-driven HPC Workflows.](http://arxiv.org/abs/2208.11069) | 该论文研究了机器学习驱动的高性能计算工作流中异步任务执行的要求和特性，并提出了用于确定异步执行收益的关键指标。通过在Summit上进行大规模实验，作者证明了异步执行对性能的增强与模型一致。 |
| [^113] | [What Do Compressed Multilingual Machine Translation Models Forget?.](http://arxiv.org/abs/2205.10828) | 本研究评估了压缩方法对多语言神经机器翻译模型在不同语言群体、性别和语义偏差方面的影响，并发现代表性不足的语言性能显著下降。 |
| [^114] | [Bilinear value networks.](http://arxiv.org/abs/2204.13695) | 提出了一种通过点积低秩近似来表示Q值的双线性分解方法，其中第一个向量场捕捉状态的局部动态，第二个部分捕捉当前状态和目标之间的全局关系，该方法能够显著提高数据效率，并具有很好的泛化性能。 |
| [^115] | [Topological Experience Replay.](http://arxiv.org/abs/2203.15845) | 本文提出了一种拓扑经验回放的方法，通过构建图来明确状态的 Q 值之间的依赖关系，解决了传统采样策略忽视状态间依赖关系的问题，提高了学习深度 Q 函数时的性能和准确性。 |
| [^116] | [Surrogate-assisted distributed swarm optimisation for computationally expensive geoscientific models.](http://arxiv.org/abs/2201.06843) | 本文利用代理模型辅助的分布式群体优化方法，解决了计算昂贵的地质科学模型优化问题，在基准优化问题和Badlands风貌演化模型中都取得了非常有希望的结果。 |
| [^117] | [Explainable and Discourse Topic-aware Neural Language Understanding.](http://arxiv.org/abs/2006.10632) | 该论文提出了一个新颖的神经复合语言模型，通过引入可解释性的主题表示和句子级的主题对话，将主题模型和语言模型相结合。实验结果表明，在多个任务上，该模型显示出了良好的性能。 |

# 详细

[^1]: Physion++：对需要在线推理不同物理属性的物理场景理解进行评估

    Physion++: Evaluating Physical Scene Understanding that Requires Online Inference of Different Physical Properties. (arXiv:2306.15668v1 [cs.CV])

    [http://arxiv.org/abs/2306.15668](http://arxiv.org/abs/2306.15668)

    该论文提出了一个新的数据集和基准测试——Physion++，用于评估在需要准确估计场景中物体潜在物理属性的情况下的视觉物理预测。

    

    一般的物理场景理解需要的不仅仅是定位和识别物体，还需要了解物体可以具有不同的潜在属性（例如质量或弹性），并且这些属性会影响物理事件的结果。尽管近年来在物理和视频预测模型方面取得了很大进展，但评估它们性能的基准测试通常不要求理解物体具有个体的物理属性，或者最多只测试可以直接观察到的属性（例如大小或颜色）。本文提出了一个新的数据集和基准测试——Physion++，在这个人工系统下严格评估了视觉物理预测，其中预测依赖于场景中物体潜在物理属性的准确估计。具体而言，我们测试了准确预测依赖于质量、摩擦力、弹性和可变性等属性的场景。

    General physical scene understanding requires more than simply localizing and recognizing objects -- it requires knowledge that objects can have different latent properties (e.g., mass or elasticity), and that those properties affect the outcome of physical events. While there has been great progress in physical and video prediction models in recent years, benchmarks to test their performance typically do not require an understanding that objects have individual physical properties, or at best test only those properties that are directly observable (e.g., size or color). This work proposes a novel dataset and benchmark, termed Physion++, that rigorously evaluates visual physical prediction in artificial systems under circumstances where those predictions rely on accurate estimates of the latent physical properties of objects in the scene. Specifically, we test scenarios where accurate prediction relies on estimates of properties such as mass, friction, elasticity, and deformability, an
    
[^2]: AI-生成文本检测工具的测试

    Testing of Detection Tools for AI-Generated Text. (arXiv:2306.15666v1 [cs.CL])

    [http://arxiv.org/abs/2306.15666](http://arxiv.org/abs/2306.15666)

    本文研究了人工智能生成文本的检测工具的功能，并对其进行了评估。研究发现，现有的检测工具既不准确也不可靠。

    

    最近，生成式预训练变形器大语言模型的发展强调了在学术环境中不公平使用人工智能生成内容的潜在风险，并加强了寻找检测此类内容解决方案的努力。本文研究了人工智能生成文本的检测工具的一般功能，并根据准确性和错误类型分析对其进行评估。具体而言，该研究旨在回答以下研究问题：现有的检测工具是否能可靠地区分人类编写的文本和ChatGPT生成的文本，以及机器翻译和内容混淆技术是否影响对AI生成文本的检测。研究涵盖了12种公开可用的工具和两个广泛应用于学术环境中的商业系统（Turnitin和PlagiarismCheck）。研究人员得出结论，现有的检测工具既不准确也不可靠，并且存在主要的可辨识错误。

    Recent advances in generative pre-trained transformer large language models have emphasised the potential risks of unfair use of artificial intelligence (AI) generated content in an academic environment and intensified efforts in searching for solutions to detect such content. The paper examines the general functionality of detection tools for artificial intelligence generated text and evaluates them based on accuracy and error type analysis. Specifically, the study seeks to answer research questions about whether existing detection tools can reliably differentiate between human-written text and ChatGPT-generated text, and whether machine translation and content obfuscation techniques affect the detection of AIgenerated text. The research covers 12 publicly available tools and two commercial systems (Turnitin and PlagiarismCheck) that are widely used in the academic setting. The researchers conclude that the available detection tools are neither accurate nor reliable and have a main bi
    
[^3]: ShuttleSet22: 用羽毛球单打数据集对中风预测进行基准测试

    ShuttleSet22: Benchmarking Stroke Forecasting with Stroke-Level Badminton Dataset. (arXiv:2306.15664v1 [cs.AI])

    [http://arxiv.org/abs/2306.15664](http://arxiv.org/abs/2306.15664)

    本研究提供了一个收集自2022年高排名比赛的羽毛球单打数据集ShuttleSet22，并使用最先进的拍球预测方法ShuttleNet进行了基准测试。

    

    近年来，由于人工智能的进步和数据采集的效率，羽毛球分析引起了人们的关注。虽然已经有一系列有效的应用来改善和研究选手表现，但只有很少几个可以供羽毛球领域外的研究人员使用的公开羽毛球数据集。现有的羽毛球单打数据集集中于特定的比赛对决，然而它们无法对不同选手和各种比赛对决进行综合研究。在本文中，我们提供了一个羽毛球单打数据集ShuttleSet22，这个数据集是从2022年高排名比赛中收集的。ShuttleSet22训练集包括30,172个回合中的2,888个拍球，验证集包括450个回合中的1,400个拍球，测试集包括654个回合中的2,040个拍球，并且具有每个回合中详细的拍球级元数据。为了与ShuttleSet22进行基准测试，我们使用ShuttleNet，这是目前最先进的拍球预测方法。

    In recent years, badminton analytics has drawn attention due to the advancement of artificial intelligence and the efficiency of data collection. While there is a line of effective applications to improve and investigate player performance, there are only a few public badminton datasets that can be used for researchers outside the badminton domain. Existing badminton singles datasets focus on specific matchups; however, they cannot provide comprehensive studies on different players and various matchups. In this paper, we provide a badminton singles dataset, ShuttleSet22, which is collected from high-ranking matches in 2022. ShuttleSet22 consists of 30,172 strokes in 2,888 rallies in the training set, 1,400 strokes in 450 rallies in the validation set, and 2,040 strokes in 654 rallies in the testing set with detailed stroke-level metadata within a rally. To benchmark existing work with ShuttleSet22, we test the state-of-the-art stroke forecasting approach, ShuttleNet, with the correspon
    
[^4]: 在高维度、小样本表格数据上增强表示学习：一种基于集成VAE的分而治之方法

    Enhancing Representation Learning on High-Dimensional, Small-Size Tabular Data: A Divide and Conquer Method with Ensembled VAEs. (arXiv:2306.15661v1 [cs.LG])

    [http://arxiv.org/abs/2306.15661](http://arxiv.org/abs/2306.15661)

    这项研究使用了基于集成VAE的分而治之方法，在高维度、低样本数量的任务中学习到更好的潜在表示，提高了样本效率，并在下游分类任务中取得了更高的准确性。

    

    变分自动编码器及其各种变体展现了在降维任务中的出色性能，通常能达到最新水平。然而，在高维度、低样本数量(HDLSS)任务中，许多当前方法在学习良好的表示方面存在困难，这是一个固有的具有挑战性的设置。我们通过使用轻量级VAE的集成来学习特征空间子集上的后验概率，从而在新颖的分而治之方法中聚合到一个联合后验中，从而应对这一挑战。具体而言，我们提出了联合后验的一种替代分解方式，引入了一种隐式数据增强形式，提高了样本效率。通过对八个真实数据集进行一系列实验，我们展示了我们的方法在HDLSS设置中学习到更好的潜在表示，这导致了下游分类任务中更高的准确性。此外，我们验证我们的方法对解缠缚效果有积极影响。

    Variational Autoencoders and their many variants have displayed impressive ability to perform dimensionality reduction, often achieving state-of-the-art performance. Many current methods however, struggle to learn good representations in High Dimensional, Low Sample Size (HDLSS) tasks, which is an inherently challenging setting. We address this challenge by using an ensemble of lightweight VAEs to learn posteriors over subsets of the feature-space, which get aggregated into a joint posterior in a novel divide-and-conquer approach. Specifically, we present an alternative factorisation of the joint posterior that induces a form of implicit data augmentation that yields greater sample efficiency. Through a series of experiments on eight real-world datasets, we show that our method learns better latent representations in HDLSS settings, which leads to higher accuracy in a downstream classification task. Furthermore, we verify that our approach has a positive effect on disentanglement and a
    
[^5]: 投票中二项式变形出乎意料

    The Distortion of Binomial Voting Defies Expectation. (arXiv:2306.15657v1 [cs.GT])

    [http://arxiv.org/abs/2306.15657](http://arxiv.org/abs/2306.15657)

    本论文研究了投票规则的期望变形，提出了一种新颖的、直观的规则-二项式投票，并为所有分布提供了强大的期望变形保证。

    

    在计算社会选择中，投票规则的变形度量了规则在克服有限的偏好信息以选择社会上理想结果的程度。这个概念已经被广泛研究，但仅仅通过最坏情况的视角。相反，我们研究了投票规则相对于底层资产者效用分布的期望变形。我们的主要贡献是设计和分析了一种新颖而直观的规则-二项式投票，它为所有分布提供了强大的期望变形保证。

    In computational social choice, the distortion of a voting rule quantifies the degree to which the rule overcomes limited preference information to select a socially desirable outcome. This concept has been investigated extensively, but only through a worst-case lens. Instead, we study the expected distortion of voting rules with respect to an underlying distribution over voter utilities. Our main contribution is the design and analysis of a novel and intuitive rule, binomial voting, which provides strong expected distortion guarantees for all distributions.
    
[^6]: SparseOptimizer: 通过Moreau-Yosida正则化来降低语言模型的稀疏性，并通过编译器共同设计来加速

    SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design. (arXiv:2306.15656v1 [cs.LG])

    [http://arxiv.org/abs/2306.15656](http://arxiv.org/abs/2306.15656)

    SparseOptimizer是一种深度学习优化器，通过Moreau-Yosida正则化在大型语言模型中引入稀疏性。它采用嵌入的收缩操作符，无需对代码进行修改即可适应各种大型语言模型，并在各种基准数据集上实现与密集型模型相当的性能，同时减少参数数量。

    

    本文介绍了SparseOptimizer，一种新颖的深度学习优化器，通过Moreau-Yosida正则化在大型语言模型（如BERT，ALBERT和GPT）中自然地引入稀疏性。SparseOptimizer设计的关键是嵌入的收缩操作符，它在优化过程中直接引入稀疏性。这个操作符通过坚实的理论框架支持，并包含了一个分析解，从而增强了优化器的鲁棒性和效果。重要的是，SparseOptimizer的即插即用功能消除了对代码修改的需求，使其成为适用于各种大型语言模型的通用适应工具。在GLUE、RACE、SQuAD1和SQuAD2等基准数据集上的实证评估表明，通过SparseOptimizer稀疏化后的SparseBERT和SparseALBERT在性能上与密集型的BERT和ALBERT相当，同时显著减少了参数数量。

    This paper introduces SparseOptimizer, a novel deep learning optimizer that exploits Moreau-Yosida regularization to naturally induce sparsity in large language models such as BERT, ALBERT and GPT. Key to the design of SparseOptimizer is an embedded shrinkage operator, which imparts sparsity directly within the optimization process. This operator, backed by a sound theoretical framework, includes an analytical solution, thereby reinforcing the optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play functionality eradicates the need for code modifications, making it a universally adaptable tool for a wide array of large language models. Empirical evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2 confirm that SparseBERT and SparseALBERT, when sparsified using SparseOptimizer, achieve performance comparable to their dense counterparts, BERT and ALBERT, while significantly reducing their parameter count. Further, this work proposes an innovati
    
[^7]: 异步算法与Cocycles的对齐

    Asynchronous Algorithmic Alignment with Cocycles. (arXiv:2306.15632v1 [cs.LG])

    [http://arxiv.org/abs/2306.15632](http://arxiv.org/abs/2306.15632)

    该论文提出了一种将节点状态更新和消息函数调用分离的数学框架，以实现异步计算，并以此作为基础，进行了异步算法和神经网络的对齐。

    

    最先进的神经算法推理器使用图神经网络（GNN）中的消息传递。但是，典型的GNN在定义和调用消息函数之间模糊了区别，迫使节点在每一层都向其邻居发送消息，同步地进行。然而，当将GNN应用于学习执行动态规划算法时，大多数步骤只有少数几个节点会有有意义的更新要发送。因此，通过在图中发送太多无关的数据，可能导致低效率，而许多中间的GNN步骤必须学习身份函数。在这项工作中，我们明确地分离了节点状态更新和消息函数调用的概念。通过这种分离，我们得到了一个数学表达，可以让我们思考算法和神经网络中的异步计算。

    State-of-the-art neural algorithmic reasoners make use of message passing in graph neural networks (GNNs). But typical GNNs blur the distinction between the definition and invocation of the message function, forcing a node to send messages to its neighbours at every layer, synchronously. When applying GNNs to learn to execute dynamic programming algorithms, however, on most steps only a handful of the nodes would have meaningful updates to send. One, hence, runs the risk of inefficiencies by sending too much irrelevant data across the graph -- with many intermediate GNN steps having to learn identity functions. In this work, we explicitly separate the concepts of node state update and message function invocation. With this separation, we obtain a mathematical formulation that allows us to reason about asynchronous computation in both algorithms and neural networks.
    
[^8]: 基于机器学习的中性原子NISQ设备噪声特性和校正研究

    Machine-learning based noise characterization and correction on neutral atoms NISQ devices. (arXiv:2306.15628v1 [quant-ph])

    [http://arxiv.org/abs/2306.15628](http://arxiv.org/abs/2306.15628)

    本研究基于机器学习技术，提出了两种方法来表征和校正中性原子NISQ设备上的噪声参数，为进一步提高计算结果的准确性提供了新的思路。

    

    中性原子器件利用光镊排列原子和调制激光脉冲控制量子态，代表了一种有前景的技术。Pasqal发展了一种使用铷原子的中性原子噪声中等规模量子（NISQ）设备，可处理多达100个量子比特。所有NISQ设备都受到噪声的影响，这对计算结果有着一定影响。因此，更好地了解和表征噪声源并可能纠正它们非常重要。本文提出了两种方法来表征和校正中性原子NISQ设备上的噪声参数。特别关注Pasqal设备，并采用机器学习（ML）技术来实现这些目标。为了表征噪声参数，训练了多个ML模型，只使用原子最终量子态的测量结果作为输入，来预测激光强度波动和腰围、温度以及假阳性和假阴性测量值等。

    Neutral atoms devices represent a promising technology that uses optical tweezers to geometrically arrange atoms and modulated laser pulses to control the quantum states. A neutral atoms Noisy Intermediate Scale Quantum (NISQ) device is developed by Pasqal with rubidium atoms that will allow to work with up to 100 qubits. All NISQ devices are affected by noise that have an impact on the computations results. Therefore it is important to better understand and characterize the noise sources and possibly to correct them. Here, two approaches are proposed to characterize and correct noise parameters on neutral atoms NISQ devices. In particular the focus is on Pasqal devices and Machine Learning (ML) techniques are adopted to pursue those objectives. To characterize the noise parameters, several ML models are trained, using as input only the measurements of the final quantum state of the atoms, to predict laser intensity fluctuation and waist, temperature and false positive and negative mea
    
[^9]: LeanDojo: 检索增强语言模型的定理证明

    LeanDojo: Theorem Proving with Retrieval-Augmented Language Models. (arXiv:2306.15626v1 [cs.LG])

    [http://arxiv.org/abs/2306.15626](http://arxiv.org/abs/2306.15626)

    本文引入了LeanDojo，该工具通过提取Lean的数据，为定理证明研究提供了一个开放源代码的平台。利用LeanDojo的数据，开发了ReProver，它是第一个使用检索增强的语言模型的证明器，可以从庞大的数学库中选择命题，训练成本低，并且只需要一周的GPU训练时间。

    

    大型语言模型（LLM）已经显示出在使用Lean等证明助手证明形式定理方面的潜力。然而，由于私有代码、数据和大量计算要求，现有的方法很难复制或建立在其基础上，这给定理证明的机器学习方法的研究带来了巨大的障碍。本文通过引入LeanDojo来消除这些障碍：一个包含工具包、数据、模型和基准测试的开放源代码的Lean游乐场。LeanDojo从Lean中提取数据，并使得可以通过编程与证明环境进行交互。它包含证明中命题的细粒度注释，为命题选择提供了有价值的数据：这是定理证明中的一个关键瓶颈。利用这些数据，我们开发出了ReProver（检索增强的证明器）：它是第一个使用LLM的证明器，通过检索从庞大的数学库中选择命题。它成本低廉，只需要一周的GPU训练时间。我们的检索器利用了LeanDojo的pro相关功能。

    Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection: a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): the first LLM-based prover that is augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's prog
    
[^10]: 针对离策略强化学习的价值感知重要性加权

    Value-aware Importance Weighting for Off-policy Reinforcement Learning. (arXiv:2306.15625v1 [cs.LG])

    [http://arxiv.org/abs/2306.15625](http://arxiv.org/abs/2306.15625)

    本文提出了一种称为“价值感知重要性权重”的方法，用于校正离策略学习中的样本。这种方法可以降低重要性采样权重的方差并保持无偏性，从而提高实践中的稳定性和效果。

    

    重要性采样是强化学习中离策略预测的一个核心思想，它提供了一种策略，可以通过对一个分布中的样本进行重新加权，从而在另一个分布下获得无偏的估计值。然而，重要性采样权重往往会表现出极端的方差，常常导致实践中的稳定性问题。在这项工作中，我们考虑了一类更广泛的重要性权重来校正离策略学习中的样本。我们提出了使用“价值感知重要性权重”，它考虑了样本空间，从而在目标分布下提供更低的方差，但仍然是无偏的估计。我们推导了如何计算这样的权重，并详细说明了结果重要性权重的关键属性。然后，我们将几个强化学习预测算法扩展到使用这些权重的离策略设置，并进行了实证评估。

    Importance sampling is a central idea underlying off-policy prediction in reinforcement learning. It provides a strategy for re-weighting samples from a distribution to obtain unbiased estimates under another distribution. However, importance sampling weights tend to exhibit extreme variance, often leading to stability issues in practice. In this work, we consider a broader class of importance weights to correct samples in off-policy learning. We propose the use of $\textit{value-aware importance weights}$ which take into account the sample space to provide lower variance, but still unbiased, estimates under a target distribution. We derive how such weights can be computed, and detail key properties of the resulting importance weights. We then extend several reinforcement learning prediction algorithms to the off-policy setting with these weights, and evaluate them empirically.
    
[^11]: 通过位置插值扩展大型语言模型的上下文窗口

    Extending Context Window of Large Language Models via Positional Interpolation. (arXiv:2306.15595v1 [cs.CL])

    [http://arxiv.org/abs/2306.15595](http://arxiv.org/abs/2306.15595)

    通过位置插值方法，我们可以在最小微调的情况下将RoPE-based预训练语言模型的上下文窗口扩展到最多32768，并在多个任务上获得强有力的实证结果。通过线性降低输入位置索引的大小，我们保持了扩展模型在原始上下文窗口内任务的质量。

    

    我们提出了一种位置插值（PI）方法，可以在最小微调的情况下将RoPE-based预训练语言模型（如LLaMA模型）的上下文窗口大小扩展到最多32768，并且在需要长上下文的各种任务（包括密钥检索、语言建模和长篇文档摘要等）上展现出良好的实证结果。同时，通过位置插值扩展的模型在原始上下文窗口内的任务中相对保持良好的质量。为了实现这一目标，位置插值线性地降低输入位置索引的大小，以匹配原始的上下文窗口大小，而不是超过训练时上下文长度，这可能会导致严重的高注意力分数，完全破坏自注意机制。我们的理论研究表明，插值的上界至少是推断的上界的$\sim 600 \times$要小，进一步证明了其稳定性。

    We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least $\sim 600 \times$ smaller than that of extrapolation, further demonstrating its stability. Models extend
    
[^12]: RansomAI: 基于人工智能的隐蔽加密勒索软件

    RansomAI: AI-powered Ransomware for Stealthy Encryption. (arXiv:2306.15559v1 [cs.CR])

    [http://arxiv.org/abs/2306.15559](http://arxiv.org/abs/2306.15559)

    本文提出了一种名为RansomAI的基于人工智能的隐蔽加密勒索软件，它通过强化学习来适应加密行为，最小化被检测的可能性，并在加密时最大化损害功能。

    

    近期，由于人工智能技术的爆发, 勒索软件（包括恶意软件）将会运用人工智能技术来智能而动态地适应其加密行为以避免被检测。这将导致现有的网络安全解决方案变得无效和过时，但是文献中缺乏基于人工智能的勒索软件来验证此观点。因此，本文提出了一种名为RansomAI的强化学习框架，它可以集成到现有的勒索软件样本中，使其加密行为具有适应性，并保持隐蔽性。RansomAI引入了一个智能代理，通过奖励机制和指纹智能检测系统来学习最佳的加密算法、速率和持续时间，以降低被检测的概率同时最大化其损害功能。

    Cybersecurity solutions have shown promising performance when detecting ransomware samples that use fixed algorithms and encryption rates. However, due to the current explosion of Artificial Intelligence (AI), sooner than later, ransomware (and malware in general) will incorporate AI techniques to intelligently and dynamically adapt its encryption behavior to be undetected. It might result in ineffective and obsolete cybersecurity solutions, but the literature lacks AI-powered ransomware to verify it. Thus, this work proposes RansomAI, a Reinforcement Learning-based framework that can be integrated into existing ransomware samples to adapt their encryption behavior and stay stealthy while encrypting files. RansomAI presents an agent that learns the best encryption algorithm, rate, and duration that minimizes its detection (using a reward mechanism and a fingerprinting intelligent detection system) while maximizing its damage function. The proposed framework was validated in a ransomwar
    
[^13]: CamemBERT-bio：一种更健康的法语语言模型

    CamemBERT-bio: a Tasty French Language Model Better for your Health. (arXiv:2306.15550v1 [cs.CL])

    [http://arxiv.org/abs/2306.15550](http://arxiv.org/abs/2306.15550)

    本研究介绍了CamemBERT-bio，它是一种针对法语生物医学领域专门设计的语言模型，相对于通用模型在命名实体识别任务上平均提高了2.54个百分点。

    

    通过临床数据仓库，医院中的临床数据变得越来越容易用于研究，然而这些文件都是非结构化的。因此，需要从医疗报告中提取信息以进行临床研究。使用CamemBERT等BERT-like模型的迁移学习已经取得了重大进展，特别是命名实体识别方面。然而，这些模型是为通用语言训练的，在生物医学数据上效果较弱。因此，我们提出了一种新的法语公共生物医学数据集，对CamemBERT进行了继续预训练。因此，我们介绍了CamemBERT-bio的第一个版本，它是一种为法语生物医学领域专门设计的公共模型，在不同的生物医学命名实体识别任务上平均F1分数提高了2.54个百分点。

    Clinical data in hospitals are increasingly accessible for research through clinical data warehouses, however these documents are unstructured. It is therefore necessary to extract information from medical reports to conduct clinical studies. Transfer learning with BERT-like models such as CamemBERT has allowed major advances, especially for named entity recognition. However, these models are trained for plain language and are less efficient on biomedical data. This is why we propose a new French public biomedical dataset on which we have continued the pre-training of CamemBERT. Thus, we introduce a first version of CamemBERT-bio, a specialized public model for the French biomedical domain that shows 2.54 points of F1 score improvement on average on different biomedical named entity recognition tasks.
    
[^14]: 当基础模型遇到联邦学习：动机、挑战和未来方向

    When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions. (arXiv:2306.15546v1 [cs.LG])

    [http://arxiv.org/abs/2306.15546](http://arxiv.org/abs/2306.15546)

    基础模型与联邦学习的交叉提供了解锁新可能性的独特机会，扩展了数据可用性，促进了协作式模型发展，并提高了性能和隐私保护。

    

    基础模型（FM）与联邦学习（FL）的交叉提供了相互的好处，在AI研究中提供了解锁新可能性的独特机会，解决了AI和现实世界应用中的关键挑战。FL扩展了FM的数据可用性，并实现了计算共享，分散了训练过程，并减轻了FL参与者的负担。它促进了协作式FM发展，民主化了这一过程，促进了包容性和创新。另一方面，FM以其庞大的规模、预训练的知识和出色的性能，为FL提供了一个强大的起点，促进了在非独立同分布数据下更快的收敛和更好的性能。此外，利用FM生成合成数据可以丰富数据多样性，减少过拟合，保护隐私。通过研究FL和FM之间的相互作用，本文旨在加深对它们协同关系的理解，强调动机和挑战。

    The intersection of the Foundation Model (FM) and Federated Learning (FL) provides mutual benefits, presents a unique opportunity to unlock new possibilities in AI research, and address critical challenges in AI and real-world applications. FL expands the availability of data for FMs and enables computation sharing, distributing the training process and reducing the burden on FL participants. It promotes collaborative FM development, democratizing the process and fostering inclusivity and innovation. On the other hand, FM, with its enormous size, pre-trained knowledge, and exceptional performance, serves as a robust starting point for FL, facilitating faster convergence and better performance under non-iid data. Additionally, leveraging FM to generate synthetic data enriches data diversity, reduces overfitting, and preserves privacy. By examining the interplay between FL and FM, this paper aims to deepen the understanding of their synergistic relationship, highlighting the motivations,
    
[^15]: 解放用户评论的力量：探索意大利卡塔尼亚机场的航空公司选择

    Unleashing the Power of User Reviews: Exploring Airline Choices at Catania Airport, Italy. (arXiv:2306.15541v1 [cs.CL])

    [http://arxiv.org/abs/2306.15541](http://arxiv.org/abs/2306.15541)

    本研究通过使用新工具，探讨了社会影响机制与航空公司选择之间的关系，并通过对用户评论的分析，提供了关于卡塔尼亚机场航空生态系统中航空公司的重要见解。

    

    本研究旨在通过使用新工具，探讨社会影响机制与航空公司选择之间的可能关系，以进一步了解影响消费者在航空领域决策的因素。我们选择从知名平台Trustpilot、Google和Twitter中提取用户评论。通过结合网络爬取技术，我们能够收集到包含各种用户意见、反馈和评分的全面数据集。然后，我们优化了BERT模型，以便更好地聚焦航空公司评论中的有见地的情感。通过分析，我们观察到不同航空公司平均负面情感得分的有趣趋势，这使我们更深入地了解了航空公司之间的动态，帮助我们识别卡塔尼亚机场航空生态系统中的关键合作伙伴、热门航线和扮演核心角色的航空公司。

    This study aims to investigate the possible relationship between the mechanisms of social influence and the choice of airline, through the use of new tools, with the aim of understanding whether they can contribute to a better understanding of the factors influencing the decisions of consumers in the aviation sector. We have chosen to extract user reviews from well-known platforms: Trustpilot, Google, and Twitter. By combining web scraping techniques, we have been able to collect a comprehensive dataset comprising a wide range of user opinions, feedback, and ratings. We then refined the BERT model to focus on insightful sentiment in the context of airline reviews. Through our analysis, we observed an intriguing trend of average negative sentiment scores across various airlines, giving us deeper insight into the dynamics between airlines and helping us identify key partnerships, popular routes, and airlines that play a central role in the aeronautical ecosystem of Catania airport during
    
[^16]: 增强行作物模拟中的导航基准测试和感知数据生成

    Enhancing Navigation Benchmarking and Perception Data Generation for Row-based Crops in Simulation. (arXiv:2306.15517v1 [cs.RO])

    [http://arxiv.org/abs/2306.15517](http://arxiv.org/abs/2306.15517)

    本研究提出了一个合成数据集和一个虚拟场景集合，用于训练深度语义分割网络和快速评估导航算法。同时，还开发了一种自动参数方法用于探索不同的田地形状和特征。

    

    近年来，服务机器人技术在精准农业中的应用不断提升，使得很多自动化过程得以实现。然而，数据生成和现场验证活动限制了大规模自主平台的进展。模拟环境和深度视觉感知已成为加速开发具有低成本RGB-D相机的鲁棒导航的有效工具。在这个背景下，本文的贡献有两个方面：一个用于训练深度语义分割网络的合成数据集，以及一套用于快速评估导航算法的虚拟场景集合。此外，还开发了一种自动参数化方法，用于探索不同的田地几何形状和特征。通过在不同作物上训练深度分割网络并对其导航性能进行基准测试，评估了模拟框架和数据集。

    Service robotics is recently enhancing precision agriculture enabling many automated processes based on efficient autonomous navigation solutions. However, data generation and infield validation campaigns hinder the progress of large-scale autonomous platforms. Simulated environments and deep visual perception are spreading as successful tools to speed up the development of robust navigation with low-cost RGB-D cameras. In this context, the contribution of this work is twofold: a synthetic dataset to train deep semantic segmentation networks together with a collection of virtual scenarios for a fast evaluation of navigation algorithms. Moreover, an automatic parametric approach is developed to explore different field geometries and features. The simulation framework and the dataset have been evaluated by training a deep segmentation network on different crops and benchmarking the resulting navigation.
    
[^17]: 优先轨迹回放：一种用于数据驱动强化学习的回放记忆方法

    Prioritized Trajectory Replay: A Replay Memory for Data-driven Reinforcement Learning. (arXiv:2306.15503v1 [cs.LG])

    [http://arxiv.org/abs/2306.15503](http://arxiv.org/abs/2306.15503)

    本研究提出了一种名为优先轨迹回放的回放记忆方法，将数据采样的视角扩展到轨迹中，从有限的数据中提取更全面的信息。这种方法通过反向采样轨迹来提高学习效率，并利用加权评论目标避免采样未见过的动作。优先轨迹回放还能根据不同的优先度指标优先采样效率更高的轨迹。

    

    近年来，数据驱动的强化学习（RL），也称为离线RL，引起了广泛关注。然而，尽管其具有提升在线RL性能的潜力，但离线RL中的数据采样技术的作用却被忽视了。最近的研究表明，直接将采样技术应用于状态转换并不能始终提高离线RL的性能。因此，在本研究中，我们提出了一种记忆技术——优先轨迹回放（TR/PTR），它将采样的视角扩展到轨迹中，以从有限的数据中提取更全面的信息。TR通过反向采样轨迹来提高学习效率，优化后续状态信息的使用。在TR的基础上，我们构建了加权评论目标，以避免在离线训练中采样未见过的动作，并且引入了优先轨迹回放（PTR）来实现更高效的轨迹采样，根据不同的轨迹优先度指标进行优先设置。我们演示了...

    In recent years, data-driven reinforcement learning (RL), also known as offline RL, have gained significant attention. However, the role of data sampling techniques in offline RL has been overlooked despite its potential to enhance online RL performance. Recent research suggests applying sampling techniques directly to state-transitions does not consistently improve performance in offline RL. Therefore, in this study, we propose a memory technique, (Prioritized) Trajectory Replay (TR/PTR), which extends the sampling perspective to trajectories for more comprehensive information extraction from limited data. TR enhances learning efficiency by backward sampling of trajectories that optimizes the use of subsequent state information. Building on TR, we build the weighted critic target to avoid sampling unseen actions in offline training, and Prioritized Trajectory Replay (PTR) that enables more efficient trajectory sampling, prioritized by various trajectory priority metrics. We demonstrat
    
[^18]: 一种用于改进分类任务可解释性的新型结构化论证框架

    A novel structured argumentation framework for improved explainability of classification tasks. (arXiv:2306.15500v1 [cs.AI])

    [http://arxiv.org/abs/2306.15500](http://arxiv.org/abs/2306.15500)

    本文提出了一种新的结构化论证框架$xADG$，通过使用布尔逻辑运算符和多个支持来构建简洁且可理解的论证图，从而改进了分类任务的可解释性和预测能力。

    

    本文提出了一种名为扩展论证决策图（$xADG$）的新型结构化论证框架。它是基于Dung的抽象论证图构建的论证决策图的拓展。$xADG$框架允许论证使用布尔逻辑运算符和多个前提（支持）在内部结构中，从而得到更简洁的论证图，使用户更容易理解。该研究提出了一种构建$xADG$的方法，并评估了它们在不同规模分类任务中的大小和预测能力。结果显示，得到的$xADG$具有强（平衡的）准确性，这是通过输入决策树实现的，同时还减少了达到结论所需的平均支持数。结果进一步表明，可以构建出在预测能力和总体大小方面优于其他构建$ADG$技术的可理解的$xADG$。

    This paper presents a novel framework for structured argumentation, named extend argumentative decision graph ($xADG$). It is an extension of argumentative decision graphs built upon Dung's abstract argumentation graphs. The $xADG$ framework allows for arguments to use boolean logic operators and multiple premises (supports) within their internal structure, resulting in more concise argumentation graphs that may be easier for users to understand. The study presents a methodology for construction of $xADGs$ and evaluates their size and predictive capacity for classification tasks of varying magnitudes. Resulting $xADGs$ achieved strong (balanced) accuracy, which was accomplished through an input decision tree, while also reducing the average number of supports needed to reach a conclusion. The results further indicated that it is possible to construct plausibly understandable $xADGs$ that outperform other techniques for building $ADGs$ in terms of predictive capacity and overall size. I
    
[^19]: 使用大型语言模型为人类导师提供解释性反馈

    Using Large Language Models to Provide Explanatory Feedback to Human Tutors. (arXiv:2306.15498v1 [cs.CL])

    [http://arxiv.org/abs/2306.15498](http://arxiv.org/abs/2306.15498)

    本文介绍了使用大型语言模型为人类导师提供解释性反馈的研究。通过两种方法，在在线课程中实时为导师提供有关如何给学生有效赞扬的反馈。其中一种方法使用了大型语言模型的命名实体识别技术，可以更好地提供解释性反馈。

    

    研究表明，学习者在产生解释以支持他们的推理过程时，对学习有积极影响。然而，提供学习者实时的解释性反馈常常面临与分类准确性相关的挑战，特别是在包含复杂和微妙的情境响应的领域专用环境中。我们提出了两种方法，在在线课程中为导师提供有关如何给学生有效赞扬的实时反馈。这项进行中的工作在纠正性反馈（F1分数=0.811）和成果导向反馈（F1分数=0.350）的二分类方面表现出了相当高的准确性。更重要的是，我们引入了利用大型语言模型的命名实体识别来提供解释性反馈的改进方法，这不仅可以在课程中为导师提供反馈，还可以潜在地提出实时措施。

    Research demonstrates learners engaging in the process of producing explanations to support their reasoning, can have a positive impact on learning. However, providing learners real-time explanatory feedback often presents challenges related to classification accuracy, particularly in domain-specific environments, containing situationally complex and nuanced responses. We present two approaches for supplying tutors real-time feedback within an online lesson on how to give students effective praise. This work-in-progress demonstrates considerable accuracy in binary classification for corrective feedback of effective, or effort-based (F1 score = 0.811), and ineffective, or outcome-based (F1 score = 0.350), praise responses. More notably, we introduce progress towards an enhanced approach of providing explanatory feedback using large language model-facilitated named entity recognition, which can provide tutors feedback, not only while engaging in lessons, but can potentially suggest real-
    
[^20]: 针对不规则时间序列的异常检测的前体

    Precursor-of-Anomaly Detection for Irregular Time Series. (arXiv:2306.15489v1 [cs.AI])

    [http://arxiv.org/abs/2306.15489](http://arxiv.org/abs/2306.15489)

    本文提出了一种新型异常检测方法，称为前体-异常检测（PoA检测）。与传统的异常检测不同，PoA检测旨在在异常发生之前检测到未来的异常。通过使用基于神经控制微分方程的神经网络和多任务学习算法，我们在17个基准线和3个数据集上进行实验证明了我们的方法的有效性。

    

    异常检测是一个重要领域，旨在识别意外的模式或数据点，并与许多现实世界的问题密切相关，尤其是在金融、制造、网络安全等应用中。虽然异常检测在各个领域已经被广泛研究，但在异常发生之前检测到未来的异常仍然是一个未开发的领域。在本文中，我们提出了一种新型的异常检测方法，称为“前体-异常”（PoA）检测。与传统的异常检测不同，传统的异常检测侧重于确定给定时间序列观测值是否为异常，而PoA检测旨在在异常发生之前检测到未来的异常。为了同时解决这两个问题，我们提出了一种基于神经控制微分方程的神经网络及其多任务学习算法。我们使用17个基准线和3个数据集进行实验，包括规则和不规则时间序列，并证明了我们的方法的有效性。

    Anomaly detection is an important field that aims to identify unexpected patterns or data points, and it is closely related to many real-world problems, particularly to applications in finance, manufacturing, cyber security, and so on. While anomaly detection has been studied extensively in various fields, detecting future anomalies before they occur remains an unexplored territory. In this paper, we present a novel type of anomaly detection, called \emph{\textbf{P}recursor-of-\textbf{A}nomaly} (PoA) detection. Unlike conventional anomaly detection, which focuses on determining whether a given time series observation is an anomaly or not, PoA detection aims to detect future anomalies before they happen. To solve both problems at the same time, we present a neural controlled differential equation-based neural network and its multi-task learning algorithm. We conduct experiments using 17 baselines and 3 datasets, including regular and irregular time series, and demonstrate that our prese
    
[^21]: 合作还是竞争：通过自适应预算避免多目标鲁棒性中的玩家主导

    Cooperation or Competition: Avoiding Player Domination for Multi-Target Robustness via Adaptive Budgets. (arXiv:2306.15482v1 [cs.AI])

    [http://arxiv.org/abs/2306.15482](http://arxiv.org/abs/2306.15482)

    本文研究了多目标鲁棒性中的合作与竞争之间的平衡问题，提出了一种通过调整对手预算来避免玩家主导的新框架，从而显著提高了多目标鲁棒性。

    

    尽管取得了令人难以置信的进展，深度学习已被证明容易受到对抗性攻击的影响。已经提出了许多方法来以经验和可证明的方式训练鲁棒网络。但是，大多数方法只能防御一种类型的攻击，而最近的研究在防御多种攻击方面有所进展。本文将多目标鲁棒性问题视为一个博弈过程，在这个过程中，不同的玩家（对手）通过协商在参数更新的方向上达成一致。我们发现了一个被称为玩家主导的现象，在这个博弈中，现有的最大值为基础的方法，如MAX和MSD，无法收敛。基于我们的理论分析，我们设计了一个新的框架，通过调整不同对手的预算来避免任何玩家的主导。在标准基准测试上的实验证明，将提出的框架应用于现有方法显著提高了多目标鲁棒性。

    Despite incredible advances, deep learning has been shown to be susceptible to adversarial attacks. Numerous approaches have been proposed to train robust networks both empirically and certifiably. However, most of them defend against only a single type of attack, while recent work takes steps forward in defending against multiple attacks. In this paper, to understand multi-target robustness, we view this problem as a bargaining game in which different players (adversaries) negotiate to reach an agreement on a joint direction of parameter updating. We identify a phenomenon named player domination in the bargaining game, namely that the existing max-based approaches, such as MAX and MSD, do not converge. Based on our theoretical analysis, we design a novel framework that adjusts the budgets of different adversaries to avoid any player dominance. Experiments on standard benchmarks show that employing the proposed framework to the existing approaches significantly advances multi-target ro
    
[^22]: 鲁棒代理学习: 提高对抗鲁棒性的方法

    Robust Proxy: Improving Adversarial Robustness by Robust Proxy Learning. (arXiv:2306.15457v1 [cs.CV])

    [http://arxiv.org/abs/2306.15457](http://arxiv.org/abs/2306.15457)

    该论文提出了一种名为鲁棒代理学习的训练框架，通过显式学习鲁棒的特征表示来提高对抗鲁棒性。通过添加类别鲁棒扰动生成代表类别的鲁棒特征，并将其作为鲁棒代理来学习对抗性鲁棒特征。

    

    最近，深度神经网络容易受到对抗攻击而容易被破坏，为了减轻对抗性的脆弱性，很多防御算法被提出。最近，为了提高对抗鲁棒性，许多研究试图通过对判别特征进行更多直接监督来增强特征表示。然而，现有的方法缺乏对对抗鲁棒特征表示的理解。在本文中，我们提出了一种新的训练框架，称为鲁棒代理学习。在提出的方法中，模型通过鲁棒代理显式学习鲁棒的特征表示。为此，首先，我们证明可以通过添加类别鲁棒扰动来生成代表类别的鲁棒特征。然后，我们使用类别代表特征作为鲁棒代理。通过类别鲁棒特征，模型显式地学习对抗性鲁棒特征。

    Recently, it has been widely known that deep neural networks are highly vulnerable and easily broken by adversarial attacks. To mitigate the adversarial vulnerability, many defense algorithms have been proposed. Recently, to improve adversarial robustness, many works try to enhance feature representation by imposing more direct supervision on the discriminative feature. However, existing approaches lack an understanding of learning adversarially robust feature representation. In this paper, we propose a novel training framework called Robust Proxy Learning. In the proposed method, the model explicitly learns robust feature representations with robust proxies. To this end, firstly, we demonstrate that we can generate class-representative robust features by adding class-wise robust perturbations. Then, we use the class representative features as robust proxies. With the class-wise robust features, the model explicitly learns adversarially robust features through the proposed robust proxy
    
[^23]: 通过注入增强信号来推进对抗性训练

    Advancing Adversarial Training by Injecting Booster Signal. (arXiv:2306.15451v1 [cs.CV])

    [http://arxiv.org/abs/2306.15451](http://arxiv.org/abs/2306.15451)

    本文提出了一种通过注入增强信号来提高对抗性训练的方法，其使用外部信号而不是模型参数来提高对抗性鲁棒性和自然准确性。

    

    最近的研究表明，深度神经网络（DNN）对于对抗攻击非常脆弱。为了抵御对抗攻击，已经提出了许多防御策略，其中对抗性训练被证明是最有效的策略。然而，众所周知，对抗性训练有时会损害自然准确性。因此，许多工作集中在优化模型参数以解决这个问题。与以前的方法不同，本文提出了一种新的方法，通过使用外部信号而不是模型参数来提高对抗性鲁棒性。在提出的方法中，注入了一个经过优化的通用外部信号，称为增强信号，该信号被注入到与原始内容不重叠的图像外部。然后，它提高了对抗性鲁棒性和自然准确性。增强信号与模型参数并行逐步进行优化。实验结果表明，该方法可以显著改善对抗性训练的效果。

    Recent works have demonstrated that deep neural networks (DNNs) are highly vulnerable to adversarial attacks. To defend against adversarial attacks, many defense strategies have been proposed, among which adversarial training has been demonstrated to be the most effective strategy. However, it has been known that adversarial training sometimes hurts natural accuracy. Then, many works focus on optimizing model parameters to handle the problem. Different from the previous approaches, in this paper, we propose a new approach to improve the adversarial robustness by using an external signal rather than model parameters. In the proposed method, a well-optimized universal external signal called a booster signal is injected into the outside of the image which does not overlap with the original content. Then, it boosts both adversarial robustness and natural accuracy. The booster signal is optimized in parallel to model parameters step by step collaboratively. Experimental results show that th
    
[^24]: 通过语言模型理解语言模型中的社交推理

    Understanding Social Reasoning in Language Models with Language Models. (arXiv:2306.15448v1 [cs.CL])

    [http://arxiv.org/abs/2306.15448](http://arxiv.org/abs/2306.15448)

    这项研究提出了一种新的框架，通过填充因果模板来生成对大型语言模型（LLMs）进行评估，从而解决了之前评估结果不一致和现有评估方法的有效性存在疑虑的挑战。使用这个框架，他们创建了一个新的社交推理基准（BigToM），并发现人类参与者评价这个基准的质量更高。

    

    随着大型语言模型（LLM）越来越多地融入到我们的日常生活中，了解它们理解人类心理状态的能力对于确保有效的交互变得至关重要。然而，尽管最近有人尝试评估LLM的理论心智（ToM）推理能力，但这些模型与人类ToM的一致程度仍然是一个复杂的探索主题。这主要是因为存在两个不同的挑战：（1）之前评估结果不一致，（2）现有评估方法的有效性存在疑虑。为了解决这些挑战，我们提出了一个新的框架，通过填充因果模板来生成与LLM的评估。使用我们的框架，我们为LLM创建了一个新的社交推理基准（BigToM），其中包含25个控制和5000个模型写的评估。我们发现，与之前众包评估相比，人类参与者对我们的基准的质量评价更高。

    As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evalua
    
[^25]: 对齐的神经网络是否对抗对齐？

    Are aligned neural networks adversarially aligned?. (arXiv:2306.15447v1 [cs.CL])

    [http://arxiv.org/abs/2306.15447](http://arxiv.org/abs/2306.15447)

    我们研究了大型语言模型在面对对抗用户构建的对抗性输入时是否仍能保持对齐。我们发现现有的攻击手法不足以可靠攻击对齐文本模型，并通过蛮力方法找到了对抗性输入。

    

    大型语言模型现在被调整为与其创建者的目标保持一致，即"有益且无害"。这些模型应该对用户的问题给出有益的回答，但拒绝回答可能会造成伤害的请求。然而，对抗用户可以构建绕过对齐尝试的输入。在这项工作中，我们研究了在与构造最坏情况输入（对抗性样本）的对抗用户交互时，这些模型保持多大程度的对齐。这些输入被设计成导致模型发出本应禁止的有害内容。我们展示了现有基于自然语言处理的优化攻击手法在可靠攻击对齐文本模型方面的不足之处：即使在当前基于自然语言处理的攻击失败时，我们仍然可以通过蛮力方法找到对抗性输入。因此，当前攻击的失败不应被视为对齐文本模型在面对对抗性输入时仍然保持对齐的证明。但是近期大规模机器学习模型的趋势是多模态的。

    Large language models are now tuned to align with the goals of their creators, namely to be "helpful and harmless." These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs.  However the recent trend in large-scale ML models is multim
    
[^26]: 从拓扑角度验证神经网络的安全性

    Verifying Safety of Neural Networks from Topological Perspectives. (arXiv:2306.15403v1 [cs.LG])

    [http://arxiv.org/abs/2306.15403](http://arxiv.org/abs/2306.15403)

    本研究提出了一种从拓扑角度研究神经网络安全性的方法，利用神经网络的同胚性质和开映射性质建立了输入集和输出集之间的严格保证，从而解决了神经网络在安全验证中的不确定性问题。

    

    神经网络越来越多地应用于自动驾驶等安全关键系统中，然而它们易受损并且常常表现不良。因此，在实际部署之前，它们的行为应该经过严格的保证。在本文中，我们提出了一种从拓扑角度研究神经网络安全验证问题的集边界可达性方法。给定一个具有输入集和安全集的神经网络，安全验证问题是确定所有来自输入集的神经网络输出是否落在安全集中。在我们的方法中，主要利用了神经网络的同胚性质和开映射性质，这些性质在输入集的边界和输出集的边界之间建立了严格的保证。利用这两个性质可以通过提取输入集的子集而不是整个输入集来进行可达性计算，从而控制可达性分析中的包裹效应。

    Neural networks (NNs) are increasingly applied in safety-critical systems such as autonomous vehicles. However, they are fragile and are often ill-behaved. Consequently, their behaviors should undergo rigorous guarantees before deployment in practice. In this paper, we propose a set-boundary reachability method to investigate the safety verification problem of NNs from a topological perspective. Given an NN with an input set and a safe set, the safety verification problem is to determine whether all outputs of the NN resulting from the input set fall within the safe set. In our method, the homeomorphism property and the open map property of NNs are mainly exploited, which establish rigorous guarantees between the boundaries of the input set and the boundaries of the output set. The exploitation of these two properties facilitates reachability computations via extracting subsets of the input set rather than the entire input set, thus controlling the wrapping effect in reachability analy
    
[^27]: 人工智能在协同工作中应具备可解释性和可接受性的要求

    Requirements for Explainability and Acceptance of Artificial Intelligence in Collaborative Work. (arXiv:2306.15394v1 [cs.CY])

    [http://arxiv.org/abs/2306.15394](http://arxiv.org/abs/2306.15394)

    AI在协同工作中应具备可解释性和可接受性的要求越来越重要。开发者需要了解模型内部运作，最终用户需要了解AI的结果或行为。用户的信息需求因上下文、领域知识和认知资源而有所差异。接受AI系统取决于提供的信息方式。

    

    人工智能（AI）在诸如空中交通管制等安全关键环境中的普及导致了需要被人类信任和接受的实用高效且在某种程度上可解释的系统。本研究对关于AI可解释性和接受性要求的236篇文章进行了结构化文献分析。结果包括关于人们需要知道的使AI可解释的信息、接受AI所需的信息以及促进对AI信任的表示和交互方法的48篇文章的综合回顾。结果表明，主要的用户群体是需要了解模型内部运作信息的开发者和需要了解AI结果或行为信息的最终用户。用户的信息需求在具体性、复杂性和紧迫性上存在差异，并且必须考虑到上下文、领域知识和用户的认知资源。AI系统的接受程度取决于信息的提供方式。

    The increasing prevalence of Artificial Intelligence (AI) in safety-critical contexts such as air-traffic control leads to systems that are practical and efficient, and to some extent explainable to humans to be trusted and accepted. The present structured literature analysis examines n = 236 articles on the requirements for the explainability and acceptance of AI. Results include a comprehensive review of n = 48 articles on information people need to perceive an AI as explainable, the information needed to accept an AI, and representation and interaction methods promoting trust in an AI. Results indicate that the two main groups of users are developers who require information about the internal operations of the model and end users who require information about AI results or behavior. Users' information needs vary in specificity, complexity, and urgency and must consider context, domain knowledge, and the user's cognitive resources. The acceptance of AI systems depends on information 
    
[^28]: 通过决策树特征在自动编码器处理空间中评估数据集质量

    Assessing Dataset Quality Through Decision Tree Characteristics in Autoencoder-Processed Spaces. (arXiv:2306.15392v1 [cs.LG])

    [http://arxiv.org/abs/2306.15392](http://arxiv.org/abs/2306.15392)

    本文研究了机器学习分类任务中数据集质量评估的关键方面，通过多个数据集的实验，揭示了数据集质量对模型训练和性能的深远影响，并提出了一个全面的数据集质量评估框架。

    

    本文深入探讨机器学习分类任务中数据集质量评估的关键方面。借助九个不同的数据集，每个数据集都经过分类任务需求的精心设计，我们展示了数据集质量对模型训练和性能的深远影响。我们还引入了两个额外的数据集，分别用于表示具体的数据条件 - 一个最大化熵，另一个展示高冗余性。我们的研究结果强调了适当的特征选择、充足的数据量和数据质量在实现高性能机器学习模型中的重要性。为了帮助研究人员和实践者，我们提出了一个全面的数据集质量评估框架，可以帮助评估手头的数据集是否足够且具备特定任务所需的质量。这项研究为数据评估实践提供了宝贵的见解，并有助于开发更准确和强大的模型。

    In this paper, we delve into the critical aspect of dataset quality assessment in machine learning classification tasks. Leveraging a variety of nine distinct datasets, each crafted for classification tasks with varying complexity levels, we illustrate the profound impact of dataset quality on model training and performance. We further introduce two additional datasets designed to represent specific data conditions - one maximizing entropy and the other demonstrating high redundancy. Our findings underscore the importance of appropriate feature selection, adequate data volume, and data quality in achieving high-performing machine learning models. To aid researchers and practitioners, we propose a comprehensive framework for dataset quality assessment, which can help evaluate if the dataset at hand is sufficient and of the required quality for specific tasks. This research offers valuable insights into data assessment practices, contributing to the development of more accurate and robus
    
[^29]: DCP-NAS: 针对1位CNN的差异性子父神经架构搜索

    DCP-NAS: Discrepant Child-Parent Neural Architecture Search for 1-bit CNNs. (arXiv:2306.15390v1 [cs.CV])

    [http://arxiv.org/abs/2306.15390](http://arxiv.org/abs/2306.15390)

    DCP-NAS提出了一种差异性子父神经架构搜索方法，通过在实值模型的监督下搜索1位卷积神经网络，实现了在资源有限的嵌入式设备上高效的神经架构搜索。

    

    神经架构搜索(NAS)通过生成应用自适应的神经架构，被证明是许多任务中有效的方法，但仍面临高计算成本和内存消耗的挑战。与此同时，具有二进制权重和激活的1位卷积神经网络(CNN)在资源有限的嵌入式设备上展现了潜力。一个自然的方法是在统一框架中利用1位CNN来减少NAS的计算和内存成本，同时由于涉及更复杂的过程，搜索1位CNN更具挑战性。在本文中，我们引入了差异性子父神经架构搜索(DCP-NAS)来高效搜索1位CNN，基于在实值模型(父模型)的监督下搜索1位模型(子模型)的新框架。具体而言，我们首先利用父模型计算切线方向，基于切线方向进行切线传播。

    Neural architecture search (NAS) proves to be among the effective approaches for many tasks by generating an application-adaptive neural architecture, which is still challenged by high computational cost and memory consumption. At the same time, 1-bit convolutional neural networks (CNNs) with binary weights and activations show their potential for resource-limited embedded devices. One natural approach is to use 1-bit CNNs to reduce the computation and memory cost of NAS by taking advantage of the strengths of each in a unified framework, while searching the 1-bit CNNs is more challenging due to the more complicated processes involved. In this paper, we introduce Discrepant Child-Parent Neural Architecture Search (DCP-NAS) to efficiently search 1-bit CNNs, based on a new framework of searching the 1-bit model (Child) under the supervision of a real-valued model (Parent). Particularly, we first utilize a Parent model to calculate a tangent direction, based on which the tangent propagati
    
[^30]: 草药与药物相互作用：综合决策支持系统在医疗保健中的应用

    Herb-Drug Interactions: A Holistic Decision Support System in Healthcare. (arXiv:2306.15365v1 [cs.AI])

    [http://arxiv.org/abs/2306.15365](http://arxiv.org/abs/2306.15365)

    设计了一种新颖的混合决策支持系统，利用人工智能技术识别草药与药物的相互作用，使药房社区在医疗保健系统中能够做出更好、更准确的治疗决策，并减轻可能出现的不良事件。

    

    备受广泛应用的补充与替代医学常与传统药物同时使用，从而导致药物不良反应甚至有时致命。此外，草药与药物相互作用的巨大可能性使得健康专业人员难以记住或手动搜索数据库中的相互作用信息。决策支持系统是一种强大的工具，可用于协助临床医生在患者护理中做出诊断和治疗决策。因此，设计了一种原创的混合决策支持系统，用于识别草药与药物的相互作用，应用人工智能技术来识别新的可能相互作用。将使用不同的机器学习模型来加强在这些情况下使用的典型规则引擎。因此，使用提出的系统，药房社区、人们在医疗保健系统中的第一线接触点，将能够做出更好、更准确的治疗决策，并减轻可能出现的不良事件。

    Complementary and alternative medicine are commonly used concomitantly with conventional medications leading to adverse drug reactions and even fatality in some cases. Furthermore, the vast possibility of herb-drug interactions prevents health professionals from remembering or manually searching them in a database. Decision support systems are a powerful tool that can be used to assist clinicians in making diagnostic and therapeutic decisions in patient care. Therefore, an original and hybrid decision support system was designed to identify herb-drug interactions, applying artificial intelligence techniques to identify new possible interactions. Different machine learning models will be used to strengthen the typical rules engine used in these cases. Thus, using the proposed system, the pharmacy community, people's first line of contact within the Healthcare System, will be able to make better and more accurate therapeutic decisions and mitigate possible adverse events.
    
[^31]: 以地标为基础的目标识别再讨论：使用初始状态的地标是否有意义？

    Planning Landmark Based Goal Recognition Revisited: Does Using Initial State Landmarks Make Sense?. (arXiv:2306.15362v1 [cs.AI])

    [http://arxiv.org/abs/2306.15362](http://arxiv.org/abs/2306.15362)

    本文提出了对以地标为基础的目标识别方法进行再讨论，结果表明在该方法中使用初始状态的地标没有优势。

    

    目标识别是许多应用领域（如普适计算、入侵检测、电脑游戏等）中的重要问题。在许多应用场景中，目标识别算法需要能够尽快地识别观察到的主体的目标。然而，在计划识别即计划中，许多早期方法需要相当大量的计算时间来计算解决方案。为了解决这个问题，Pereira等人最近开发了一种基于规划地标的方法，它比之前的方法在计算效率上要高得多。然而，正如Pereira等人提出的方法一样，它也使用了琐碎的地标（即，初始状态和目标描述中的事实在定义上就是地标）。在本文中，我们展示了在基于规划地标的目标识别方法中使用初始状态的地标没有提供任何好处。实证结果表明，省略掉初始状态的地标的方法更为有效。

    Goal recognition is an important problem in many application domains (e.g., pervasive computing, intrusion detection, computer games, etc.). In many application scenarios, it is important that goal recognition algorithms can recognize goals of an observed agent as fast as possible. However, many early approaches in the area of Plan Recognition As Planning, require quite large amounts of computation time to calculate a solution. Mainly to address this issue, recently, Pereira et al. developed an approach that is based on planning landmarks and is much more computationally efficient than previous approaches. However, the approach, as proposed by Pereira et al., also uses trivial landmarks (i.e., facts that are part of the initial state and goal description are landmarks by definition). In this paper, we show that it does not provide any benefit to use landmarks that are part of the initial state in a planning landmark based goal recognition approach. The empirical results show that omitt
    
[^32]: SSC-RS: 通过表示分离与BEV融合提升LiDAR语义场景补全

    SSC-RS: Elevate LiDAR Semantic Scene Completion with Representation Separation and BEV Fusion. (arXiv:2306.15349v1 [cs.CV])

    [http://arxiv.org/abs/2306.15349](http://arxiv.org/abs/2306.15349)

    本文提出了SSC-RS网络，通过表示分离和BEV融合来解决语义场景补全问题。通过深度监督将语义和几何表示的学习过程明确地分解为独立的分支，并利用自适应表示融合模块来聚合多尺度特征。

    

    语义场景补全(SSC)联合预测整个3D场景的语义和几何信息，在自动驾驶系统的3D场景理解中起着至关重要的作用。SSC在分割中利用语义上下文已取得了快速进展。然而，如何有效地利用语义分割中的语义上下文与场景补全中的几何结构之间的关系仍待探索。本文从表示分离和BEV融合的角度来解决室外SSC问题。具体而言，我们提出了一个名为SSC-RS的网络，利用深度监督将语义和几何表示的学习过程明确地分解为独立的分支。并且提出了一个配备了自适应表示融合(ARF)模块的BEV融合网络，以有效且高效地聚合多尺度特征。由于低计算负担和强大的表示能力，

    Semantic scene completion (SSC) jointly predicts the semantics and geometry of the entire 3D scene, which plays an essential role in 3D scene understanding for autonomous driving systems. SSC has achieved rapid progress with the help of semantic context in segmentation. However, how to effectively exploit the relationships between the semantic context in semantic segmentation and geometric structure in scene completion remains under exploration. In this paper, we propose to solve outdoor SSC from the perspective of representation separation and BEV fusion. Specifically, we present the network, named SSC-RS, which uses separate branches with deep supervision to explicitly disentangle the learning procedure of the semantic and geometric representations. And a BEV fusion network equipped with the proposed Adaptive Representation Fusion (ARF) module is presented to aggregate the multi-scale features effectively and efficiently. Due to the low computational burden and powerful representatio
    
[^33]: PANet: 带有稀疏实例提出和聚合的LiDAR全景分割

    PANet: LiDAR Panoptic Segmentation with Sparse Instance Proposal and Aggregation. (arXiv:2306.15348v1 [cs.CV])

    [http://arxiv.org/abs/2306.15348](http://arxiv.org/abs/2306.15348)

    PANet是一种新的LiDAR全景分割框架，通过引入稀疏实例提出和聚合的方法，提高了对大物体的分割性能。

    

    可靠的LiDAR全景分割(LPS)，包括语义和实例分割，对于许多机器人应用非常重要，如自动驾驶。本研究提出了一种新的LPS框架PANet，以消除对偏移分支的依赖并提高大物体的性能，这些物体常常被聚类算法过度分割。首先，我们提出了一个非学习的稀疏实例提出（SIP）模块，采用“采样-平移-分组”方案，从原始点云中高效地将物体点直接分组成实例。具体而言，引入平衡的点采样来生成具有更均匀距离分布的稀疏种子点。同时，提出了一种称为“气泡平移”的平移模块，将种子点收缩到聚类中心。然后，利用连接组件标签算法生成实例提案。此外，设计了一个实例聚合模块，以集成可能的碎片实例。

    Reliable LiDAR panoptic segmentation (LPS), including both semantic and instance segmentation, is vital for many robotic applications, such as autonomous driving. This work proposes a new LPS framework named PANet to eliminate the dependency on the offset branch and improve the performance on large objects, which are always over-segmented by clustering algorithms. Firstly, we propose a non-learning Sparse Instance Proposal (SIP) module with the ``sampling-shifting-grouping" scheme to directly group thing points into instances from the raw point cloud efficiently. More specifically, balanced point sampling is introduced to generate sparse seed points with more uniform point distribution over the distance range. And a shift module, termed bubble shifting, is proposed to shrink the seed points to the clustered centers. Then we utilize the connected component label algorithm to generate instance proposals. Furthermore, an instance aggregation module is devised to integrate potentially frag
    
[^34]: FedET: 一种基于增强Transformer的通信高效的联邦类增量学习框架

    FedET: A Communication-Efficient Federated Class-Incremental Learning Framework Based on Enhanced Transformer. (arXiv:2306.15347v1 [cs.LG])

    [http://arxiv.org/abs/2306.15347](http://arxiv.org/abs/2306.15347)

    FedET是一种通信高效的联邦类增量学习框架，利用增强Transformer和增强器来解决联邦学习中的灾难性遗忘和通信成本问题，保证了高精度和数据隐私。

    

    联邦学习(Federated Learning, FL)由于能够在保证数据隐私的前提下进行分散学习而受到广泛关注。然而，大多数现有方法都不切实际地假设本地客户端遇到的类别随时间固定。在学习新类别后，这个假设会导致模型对旧类别的灾难性遗忘变得更加严重。此外，受通信成本的限制，在FL中使用大规模模型是具有挑战性的，这将影响预测精度。为了解决这些挑战，我们提出了一种新颖的框架FedET，它同时实现了高精度和低通信成本。具体而言，FedET使用增强器(Enhancer)这个小型模块来吸收和传递新的知识，并将预训练的Transformer与不同的增强器结合使用，以在各种任务上保证高精度。为了解决新任务的新类别引起的本地遗忘问题和非i.i.d（非独立同分布）所带来的全局遗忘问题，FedET使用了动态扩展方法和重放机制。

    Federated Learning (FL) has been widely concerned for it enables decentralized learning while ensuring data privacy. However, most existing methods unrealistically assume that the classes encountered by local clients are fixed over time. After learning new classes, this assumption will make the model's catastrophic forgetting of old classes significantly severe. Moreover, due to the limitation of communication cost, it is challenging to use large-scale models in FL, which will affect the prediction accuracy. To address these challenges, we propose a novel framework, Federated Enhanced Transformer (FedET), which simultaneously achieves high accuracy and low communication cost. Specifically, FedET uses Enhancer, a tiny module, to absorb and communicate new knowledge, and applies pre-trained Transformers combined with different Enhancers to ensure high precision on various tasks. To address local forgetting caused by new classes of new tasks and global forgetting brought by non-i.i.d (non
    
[^35]: 提高毫米波成像系统的新型混合学习算法

    Novel Hybrid-Learning Algorithms for Improved Millimeter-Wave Imaging Systems. (arXiv:2306.15341v1 [eess.SP])

    [http://arxiv.org/abs/2306.15341](http://arxiv.org/abs/2306.15341)

    这项研究介绍了一种新型混合学习算法，用于改进毫米波成像系统。该算法通过结合信号处理和深度学习技术，提供了更好的分辨率、定位精度和检测率。通过利用无线电频率波形的已知特性，该算法能够更高效地训练深度学习模型。

    

    越来越多的关注正被集中在毫米波（30 GHz到300 GHz）和太赫兹（300 GHz到10 THz）感知应用上，包括安全感知、工业包装、医学成像和非破坏性测试。传统的感知和成像方法面临着新型数据驱动算法的挑战，这些算法提供了更好的分辨率、定位精度和检测率。在过去的十年中，深度学习技术在感知和计算机视觉应用中获得了相当大的流行。与传统的信号处理技术相比，混合方法将信号处理和基于学习的算法交替使用，提供了性能和泛化能力之间的有希望的折衷方案。此外，这种混合算法通过利用无线电频率（RF）波形的已知特性来改进模型训练，从而产生更高效训练的深度学习算法。

    Increasing attention is being paid to millimeter-wave (mmWave), 30 GHz to 300 GHz, and terahertz (THz), 300 GHz to 10 THz, sensing applications including security sensing, industrial packaging, medical imaging, and non-destructive testing. Traditional methods for perception and imaging are challenged by novel data-driven algorithms that offer improved resolution, localization, and detection rates. Over the past decade, deep learning technology has garnered substantial popularity, particularly in perception and computer vision applications. Whereas conventional signal processing techniques are more easily generalized to various applications, hybrid approaches where signal processing and learning-based algorithms are interleaved pose a promising compromise between performance and generalizability. Furthermore, such hybrid algorithms improve model training by leveraging the known characteristics of radio frequency (RF) waveforms, thus yielding more efficiently trained deep learning algori
    
[^36]: 同调神经网络：多元复杂性的稀疏架构

    Homological Neural Networks: A Sparse Architecture for Multivariate Complexity. (arXiv:2306.15337v1 [cs.LG])

    [http://arxiv.org/abs/2306.15337](http://arxiv.org/abs/2306.15337)

    本研究提出了一种新颖的深度神经网络架构，同调神经网络，通过应用网络过滤技术构建稀疏的高阶图结构，解决了深度学习中的计算复杂性和能源效率挑战，并在表格数据和时间序列回归问题上展示了优越的性能。

    

    随着人工智能研究的快速发展，越来越复杂的深度学习模型的开发带来了计算复杂性、能源效率和可解释性方面日益增长的挑战。在本研究中，我们应用先进的基于网络的信息过滤技术，设计了一种新颖的深度神经网络单元，该单元以基础数据的同调结构为基础构建了一个稀疏的高阶图结构。我们在两个传统上对深度学习具有挑战性的应用领域中展示了其有效性：表格数据和时间序列回归问题。结果表明，这种新颖设计的优势在于能够仅使用部分参数与最先进的机器学习和深度学习模型并列或超越其结果。

    The rapid progress of Artificial Intelligence research came with the development of increasingly complex deep learning models, leading to growing challenges in terms of computational complexity, energy efficiency and interpretability. In this study, we apply advanced network-based information filtering techniques to design a novel deep neural network unit characterized by a sparse higher-order graphical architecture built over the homological structure of underlying data. We demonstrate its effectiveness in two application domains which are traditionally challenging for deep learning: tabular data and time series regression problems. Results demonstrate the advantages of this novel design which can tie or overcome the results of state-of-the-art machine learning and deep learning models using only a fraction of parameters.
    
[^37]: Shoggoth: 通过自适应在线学习实现高效的边缘-云协同实时视频推断

    Shoggoth: Towards Efficient Edge-Cloud Collaborative Real-Time Video Inference via Adaptive Online Learning. (arXiv:2306.15333v1 [cs.CV])

    [http://arxiv.org/abs/2306.15333](http://arxiv.org/abs/2306.15333)

    本文提出了Shoggoth，一种针对实时视频推断的高效边缘-云协同架构。通过在线知识蒸馏和云端卸载标签过程，Shoggoth能够提高模型准确性并减轻边缘设备资源限制。实验结果表明，相比于仅使用边缘策略和仅使用云端策略，Shoggoth能够提供15%-20%的模型准确性改进和更低的网络成本。

    

    本文提出了Shoggoth，一种高效的边缘-云协同架构，用于提升对于实时视频中变化场景的推断性能。Shoggoth利用在线知识蒸馏来改善由于数据漂移而导致的模型准确性问题，并将标签过程卸载到云端，减轻边缘设备的资源限制。在边缘方面，我们设计了适应性训练，使用小批量训练的方式在有限计算资源下调整模型，并采用自适应抽样训练帧来提高鲁棒性和减少带宽。基于真实数据集的评估显示，与仅边缘策略相比，模型准确性提高了15%-20%，而网络成本较云端策略更低。

    This paper proposes Shoggoth, an efficient edge-cloud collaborative architecture, for boosting inference performance on real-time video of changing scenes. Shoggoth uses online knowledge distillation to improve the accuracy of models suffering from data drift and offloads the labeling process to the cloud, alleviating constrained resources of edge devices. At the edge, we design adaptive training using small batches to adapt models under limited computing power, and adaptive sampling of training frames for robustness and reducing bandwidth. The evaluations on the realistic dataset show 15%-20% model accuracy improvement compared to the edge-only strategy and fewer network costs than the cloud-only strategy.
    
[^38]: 使用Vision Transformer从平面图预测行人疏散时间和密度

    Towards predicting Pedestrian Evacuation Time and Density from Floorplans using a Vision Transformer. (arXiv:2306.15318v1 [cs.CV])

    [http://arxiv.org/abs/2306.15318](http://arxiv.org/abs/2306.15318)

    本文提出了一种基于Vision Transformer的深度学习方法，可以从给定的平面图预测行人密度和总疏散时间。通过构建合成数据集并将模型无缝集成到现有系统中，我们展示了该方法的高效性和准确性。

    

    传统的行人模拟器是建筑设计过程中不可或缺的工具，它们使项目工程师能够预防拥挤情况并规划疏散途径。然而，模拟运行时间和生成模拟结果的多个繁琐步骤可能成为建筑设计过程中的瓶颈。数据驱动方法已经展示了在速度上超越传统方法并在许多领域中提供类似甚至更好结果的能力。在这项工作中，我们基于Vision Transformer提出了一种基于深度学习的方法，用于从给定的平面图预测随时间变化的密度热力图和总疏散时间。具体而言，由于公共数据集的有限可用性，我们实现了一个参数化数据生成流水线，其中包括一个传统的模拟器。这使我们能够构建一个大型的合成数据集，用于训练我们的架构。此外，我们无缝地将我们的模型集成到B系统中。

    Conventional pedestrian simulators are inevitable tools in the design process of a building, as they enable project engineers to prevent overcrowding situations and plan escape routes for evacuation. However, simulation runtime and the multiple cumbersome steps in generating simulation results are potential bottlenecks during the building design process. Data-driven approaches have demonstrated their capability to outperform conventional methods in speed while delivering similar or even better results across many disciplines. In this work, we present a deep learning-based approach based on a Vision Transformer to predict density heatmaps over time and total evacuation time from a given floorplan. Specifically, due to limited availability of public datasets, we implement a parametric data generation pipeline including a conventional simulator. This enables us to build a large synthetic dataset that we use to train our architecture. Furthermore, we seamlessly integrate our model into a B
    
[^39]: FAIRER: 公平作为决策合理性对齐原则

    FAIRER: Fairness as Decision Rationale Alignment. (arXiv:2306.15299v1 [cs.LG])

    [http://arxiv.org/abs/2306.15299](http://arxiv.org/abs/2306.15299)

    本文针对深度神经网络中的公平性问题，从决策合理性的角度研究，并定义了参数平等得分来表征公平决策过程。实证研究表明现有的公平性规范项无法实现决策合理性的对齐。

    

    深度神经网络(DNNs)取得了显著的进展，但往往存在公平性问题，因为深度模型通常在某些子群体（例如男性和女性）之间显示出明显的准确性差异。现有研究通过使用公平感知损失函数来约束最后一层的输出并直接规范化DNNs来解决这个关键问题。虽然DNN的公平性得到了改善，但不清楚经过训练的网络如何进行公平预测，这限制了未来的公平性改进。在本文中，我们从决策合理性的角度研究了公平性，并通过分析各个子群体中的神经元影响来定义参数平等得分来表征网络的公平决策过程。广泛的实证研究表明，不公平问题可能源于子群体的不对齐决策合理性。现有的公平性规范项无法实现决策合理性的对齐，因为它们只约束最后一层的输出，而忽视了之前的层次。

    Deep neural networks (DNNs) have made significant progress, but often suffer from fairness issues, as deep models typically show distinct accuracy differences among certain subgroups (e.g., males and females). Existing research addresses this critical issue by employing fairness-aware loss functions to constrain the last-layer outputs and directly regularize DNNs. Although the fairness of DNNs is improved, it is unclear how the trained network makes a fair prediction, which limits future fairness improvements. In this paper, we investigate fairness from the perspective of decision rationale and define the parameter parity score to characterize the fair decision process of networks by analyzing neuron influence in various subgroups. Extensive empirical studies show that the unfair issue could arise from the unaligned decision rationales of subgroups. Existing fairness regularization terms fail to achieve decision rationale alignment because they only constrain last-layer outputs while i
    
[^40]: BERT中的性别偏见——通过情感评分在现实的下游分类任务中测量和分析偏见

    Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task. (arXiv:2306.15298v1 [cs.CL])

    [http://arxiv.org/abs/2306.15298](http://arxiv.org/abs/2306.15298)

    该论文通过引入新的偏见度量方法，并在一个现实的IMDB电影分类器的例子中对BERT的性别偏见进行了全面分析。研究结果表明，公开的BERT模型中存在着显著的性别偏见，并强调了负责任使用的重要性。

    

    预训练语言模型在各种现实应用中公开可用，并不断进行微调。随着它们具备抓取复杂上下文信息的能力，有害偏见很可能越来越多地与这些模型交织在一起。本文通过两个主要贡献来分析BERT模型中的性别偏见：首先，引入了一种新的偏见度量方法，将偏见定义为女性和男性样本版本在情感评估上的差异。其次，我们对一个现实的IMDB电影分类器的例子中BERT的偏见进行了全面分析。通过系统地变化训练流程的各个元素，我们可以对最终模型偏见的影响做出结论。我们比较了七个不同的公开BERT模型的九种训练条件，即总共63个模型。几乎所有的条件都产生了显著的性别偏见。结果表明，反映的偏见源于公开的BERT模型而不是任务特定数据，强调了负责任使用的重要性。

    Pretrained language models are publicly available and constantly finetuned for various real-life applications. As they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. This paper analyses gender bias in BERT models with two main contributions: First, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. Second, we comprehensively analyse BERT's biases on the example of a realistic IMDB movie classifier. By systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. Seven different public BERT models in nine training conditions, i.e. 63 models in total, are compared. Almost all conditions yield significant gender biases. Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage.
    
[^41]: IDOL: 面向逻辑推理的指标导向预训练方法

    IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning. (arXiv:2306.15273v1 [cs.CL])

    [http://arxiv.org/abs/2306.15273](http://arxiv.org/abs/2306.15273)

    IDOL是一种面向逻辑推理的指标导向预训练方法，通过使用逻辑指标和逻辑丰富的数据集在逻辑上增强了预训练模型。IDOL在逻辑推理MRC基准测试中取得了最先进的性能，并且具有竞争力的综合语言理解能力。

    

    在机器阅读理解领域，现有系统在许多任务（如SQuAD）中的表现已经超过了人类平均水平。然而，当涉及到逻辑推理时，仍有很大的进步空间。在本文中，我们提出了一种名为IDOL（InDicator-Oriented Logic Pre-training）的易于理解且高效的进一步预训练任务，该任务利用6种逻辑指标和逻辑丰富的数据集LGP（LoGic Pre-training）在逻辑上强化了预训练模型。IDOL在ReClor和LogiQA这两个具有代表性的逻辑推理MRC基准测试上实现了最先进的性能，并且被证明能够推广到不同的预训练模型和其他类型的MRC基准测试，如RACE和SQuAD 2.0，同时保持有竞争力的综合语言理解能力。

    In the field of machine reading comprehension (MRC), existing systems have surpassed the average performance of human beings in many tasks like SQuAD. However, there is still a long way to go when it comes to logical reasoning. Although some methods for it have been put forward, they either are designed in a quite complicated way or rely too much on external structures. In this paper, we proposed IDOL (InDicator-Oriented Logic Pre-training), an easy-to-understand but highly effective further pre-training task which logically strengthens the pre-trained models with the help of 6 types of logical indicators and a logically rich dataset LGP (LoGic Pre-training). IDOL achieves state-of-the-art performance on ReClor and LogiQA, the two most representative benchmarks in logical reasoning MRC, and is proven to be capable of generalizing to different pre-trained models and other types of MRC benchmarks like RACE and SQuAD 2.0 while keeping competitive general language understanding ability thr
    
[^42]: 提供夸大的解释

    Delivering Inflated Explanations. (arXiv:2306.15272v1 [cs.AI])

    [http://arxiv.org/abs/2306.15272](http://arxiv.org/abs/2306.15272)

    本文提出了夸大的解释，它是为了解释人工智能系统的决策原因所定义的一种方法。传统的解释方法只显示选择的特征对决策的影响，而夸大的解释考虑了更多特征的可能性。

    

    在追求可解释的人工智能（XAI）的过程中，经常出现一个问题，即 AI 系统做出决策的原因是什么。解释性的正式方法建立了 AI 系统的形式模型，并利用该模型推理系统的特性。给定一个要解释的实例的特征值集和相应的决策，一个形式推理解释是一组特征，如果它们采取给定值，将始终导致同样的决策。这种解释是有用的，它显示只有一些特征被用于做出最终决策。但它是狭义的，它只显示如果选择的特征采取它们给定的值，决策就不会改变。可能有些特征的值会改变，但仍然导致相同的决策。在本文中，我们正式定义了夸大的解释，它是一组特征，对于每个特征有一组值（始终包括该特征的值）。

    In the quest for Explainable Artificial Intelligence (XAI) one of the questions that frequently arises given a decision made by an AI system is, ``why was the decision made in this way?'' Formal approaches to explainability build a formal model of the AI system and use this to reason about the properties of the system. Given a set of feature values for an instance to be explained, and a resulting decision, a formal abductive explanation is a set of features, such that if they take the given value will always lead to the same decision. This explanation is useful, it shows that only some features were used in making the final decision. But it is narrow, it only shows that if the selected features take their given values the decision is unchanged. It's possible that some features may change values and still lead to the same decision. In this paper we formally define inflated explanations which is a set of features, and for each feature of set of values (always including the value of the i
    
[^43]: 内部对比学习用于广义离群故障诊断（GOOFD）框架

    Internal Contrastive Learning for Generalized Out-of-distribution Fault Diagnosis (GOOFD) Framework. (arXiv:2306.15266v1 [cs.AI])

    [http://arxiv.org/abs/2306.15266](http://arxiv.org/abs/2306.15266)

    我们提出了一个称为广义离群故障诊断（GOOFD）框架的集成故障诊断系统，可以同时处理故障检测、故障分类和新颖故障诊断等任务。该框架基于内部对比学习方法，利用该方法提取特征并识别异常值。

    

    故障诊断对于工业过程中监测重要机器的状态至关重要。随着工作条件的复杂化和生产运营过程中对安全的要求不断增加，需要不同的诊断方法，更重要的是需要一个可以应对多个任务的集成故障诊断系统。然而，诊断子任务通常被分开研究，而当前可用的方法在这样一个广义系统上仍需要改进。为了解决这个问题，我们提出了广义离群故障诊断（GOOFD）框架，以整合故障检测、故障分类和新颖故障诊断等诊断子任务。此外，我们提出了一种基于内部对比学习的统一故障诊断方法作为该广义框架的基础。该方法利用内部对比学习技术提取特征，然后识别异常值。

    Fault diagnosis is essential in industrial processes for monitoring the conditions of important machines. With the ever-increasing complexity of working conditions and demand for safety during production and operation, different diagnosis methods are required, and more importantly, an integrated fault diagnosis system that can cope with multiple tasks is highly desired. However, the diagnosis subtasks are often studied separately, and the currently available methods still need improvement for such a generalized system. To address this issue, we propose the Generalized Out-of-distribution Fault Diagnosis (GOOFD) framework to integrate diagnosis subtasks, such as fault detection, fault classification, and novel fault diagnosis. Additionally, a unified fault diagnosis method based on internal contrastive learning is put forward to underpin the proposed generalized framework. The method extracts features utilizing the internal contrastive learning technique and then recognizes the outliers
    
[^44]: IIFL: 非同质人类监督员的隐性互动车队学习

    IIFL: Implicit Interactive Fleet Learning from Heterogeneous Human Supervisors. (arXiv:2306.15228v1 [cs.RO])

    [http://arxiv.org/abs/2306.15228](http://arxiv.org/abs/2306.15228)

    本研究提出了Implicit Interactive Fleet Learning (IIFL)，将隐性策略推广到交互式模仿学习中，能够解决多模态和分布转移问题。

    

    模仿学习已经应用于各种机器人任务，但在以下情况下可能会遇到困难：（1）机器人遇到训练数据中没有代表的边缘案例（分布转移）或（2）人类演示是异质的：例如，在障碍物周围采取不同路径（多模态）。交互式车队学习（IFL）通过允许机器人在任务执行过程中访问远程人类远程操作员并从他们那里学习来减轻分布转移问题，但不能处理多模态。最近的研究提出了隐性行为克隆（IBC），它能够使用基于能量的模型（EBMs）表示多模态演示。在这项工作中，我们提出用隐性互动车队学习（IIFL）解决多模态和分布转移问题，这是隐性策略在交互式模仿学习中的第一个扩展（包括单机器人、单人类的设置）。IIFL使用Jeffreys分歧的新颖应用来量化不确定性。

    Imitation learning has been applied to a range of robotic tasks, but can struggle when (1) robots encounter edge cases that are not represented in the training data (distribution shift) or (2) the human demonstrations are heterogeneous: taking different paths around an obstacle, for instance (multimodality). Interactive fleet learning (IFL) mitigates distribution shift by allowing robots to access remote human teleoperators during task execution and learn from them over time, but is not equipped to handle multimodality. Recent work proposes Implicit Behavior Cloning (IBC), which is able to represent multimodal demonstrations using energy-based models (EBMs). In this work, we propose addressing both multimodality and distribution shift with Implicit Interactive Fleet Learning (IIFL), the first extension of implicit policies to interactive imitation learning (including the single-robot, single-human setting). IIFL quantifies uncertainty using a novel application of Jeffreys divergence to
    
[^45]: 学习在生成式检索中进行排序

    Learning to Rank in Generative Retrieval. (arXiv:2306.15222v1 [cs.CL])

    [http://arxiv.org/abs/2306.15222](http://arxiv.org/abs/2306.15222)

    这篇论文介绍了一种将生成式检索和经典的学习排序范例相结合的方法，通过使用段落排序损失来训练自回归模型，直接优化自回归模型朝着最优解优化。

    

    生成式检索是一种有前景的文本检索范例，它将相关段落的标识符字符串生成为检索目标。这种范例利用强大的生成模型，并代表了与传统的学习排序方法有所不同的新范例。然而，尽管其快速发展，当前的生成式检索方法仍存在局限性。它们通常依赖启发式函数将预测的标识符转换为段落排序列表，这在生成式检索的学习目标与期望的段落排序目标之间产生了差距。此外，文本生成的固有曝光偏差问题在生成式检索中仍然存在。为了解决这些问题，我们提出了一种新颖的框架，称为LTRGR，它将生成式检索与经典的学习排序范例相结合。我们的方法涉及使用段落排序损失训练一个自回归模型，该损失直接优化自回归模型朝着最优解优化。

    Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target. This paradigm leverages powerful generation models and represents a new paradigm distinct from traditional learning-to-rank methods. However, despite its rapid development, current generative retrieval methods are still limited. They typically rely on a heuristic function to transform predicted identifiers into a passage rank list, which creates a gap between the learning objective of generative retrieval and the desired passage ranking target. Moreover, the inherent exposure bias problem of text generation also persists in generative retrieval. To address these issues, we propose a novel framework, called LTRGR, that combines generative retrieval with the classical learning-to-rank paradigm. Our approach involves training an autoregressive model using a passage rank loss, which directly optimizes the autoregressive model toward the optimal 
    
[^46]: 无监督的剧集生成方法用于图元学习

    Unsupervised Episode Generation for Graph Meta-learning. (arXiv:2306.15217v1 [cs.LG])

    [http://arxiv.org/abs/2306.15217](http://arxiv.org/abs/2306.15217)

    本文研究了无监督的剧集生成方法，通过元学习解决没有标签的少样本节点分类问题。它们充分利用所有节点信息，并且通过泛化能力提高性能。

    

    本文研究了无监督的剧集生成方法，通过元学习来解决没有标签的少样本节点分类问题。主流的少样本节点分类的元学习方法是在存在大量有标签节点用于训练的情况下开发的，然而在现实世界中可能无法获得这样的数据。虽然已经提出了一些解决标签稀缺性问题的研究，但它们仍然依赖于有限数量的有标签数据，这限制了对图中所有节点信息的充分利用。尽管自监督学习方法在没有标签的节点分类问题上很有效，但它们主要学习通用的节点嵌入，没有考虑要解决的下游任务，这可能限制了其性能。在这项工作中，我们提出了无监督的剧集生成方法，以利用它们在少样本节点分类任务中的泛化能力，同时解决标签稀缺性问题。我们首先提出了一种利用图增强方法的方法

    In this paper, we investigate Unsupervised Episode Generation methods to solve Few-Shot Node-Classification (FSNC) problem via Meta-learning without labels. Dominant meta-learning methodologies for FSNC were developed under the existence of abundant labeled nodes for training, which however may not be possible to obtain in the real-world. Although few studies have been proposed to tackle the label-scarcity problem, they still rely on a limited amount of labeled data, which hinders the full utilization of the information of all nodes in a graph. Despite the effectiveness of Self-Supervised Learning (SSL) approaches on FSNC without labels, they mainly learn generic node embeddings without consideration on the downstream task to be solved, which may limit its performance. In this work, we propose unsupervised episode generation methods to benefit from their generalization ability for FSNC tasks while resolving label-scarcity problem. We first propose a method that utilizes graph augmentat
    
[^47]: 无监督的多色彩神经表示法用于CT金属伪影减少

    Unsupervised Polychromatic Neural Representation for CT Metal Artifact Reduction. (arXiv:2306.15203v1 [eess.IV])

    [http://arxiv.org/abs/2306.15203](http://arxiv.org/abs/2306.15203)

    本文提出了一种新颖的多色彩神经表示法（Polyner），用于解决CT成像中存在金属伪影的挑战性问题。Polyner通过建模非线性反问题，准确模拟CT采集过程，并利用无监督训练的神经网络架构恢复原始物体信息。实验证明Polyner在金属伪影减少方面的有效性。

    

    新兴的基于层析术的神经重建技术（如NeRF，NeAT和NeRP）在医学成像方面已经展示出独特的能力。在本文中，我们提出了一种新颖的多色彩神经表示法（Polyner）来解决CT成像中存在人体金属植入物时的挑战性问题。金属伪影是由于X射线能谱不同能量级金属的衰减系数剧烈变化而产生的，导致CT测量中的非线性金属效应。因此，从受金属影响的测量中重建CT图像是一个复杂的非线性反问题，先前的金属伪影减少（MAR）方法中采用的经验模型导致信号损失和强烈的混叠重建。Polyner从非线性反问题的角度对MAR问题进行建模。具体而言，我们首先推导出多色彩正演模型，以准确模拟非线性CT采集过程。然后，我们将据此设计一个无监督训练的神经网络架构，用于从金属伪影的CT投影图中恢复出原始的物体信息。我们通过对实际CT数据集进行了广泛实验证明了Polyner的有效性。

    Emerging neural reconstruction techniques based on tomography (e.g., NeRF, NeAT, and NeRP) have started showing unique capabilities in medical imaging. In this work, we present a novel Polychromatic neural representation (Polyner) to tackle the challenging problem of CT imaging when metallic implants exist within the human body. The artifacts arise from the drastic variation of metal's attenuation coefficients at various energy levels of the X-ray spectrum, leading to a nonlinear metal effect in CT measurements. Reconstructing CT images from metal-affected measurements hence poses a complicated nonlinear inverse problem where empirical models adopted in previous metal artifact reduction (MAR) approaches lead to signal loss and strongly aliased reconstructions. Polyner instead models the MAR problem from a nonlinear inverse problem perspective. Specifically, we first derive a polychromatic forward model to accurately simulate the nonlinear CT acquisition process. Then, we incorporate ou
    
[^48]: 一类系统完美适用于前向算法

    One-class systems seamlessly fit in the forward-forward algorithm. (arXiv:2306.15188v1 [cs.LG])

    [http://arxiv.org/abs/2306.15188](http://arxiv.org/abs/2306.15188)

    本文研究了在前向训练方式下训练深度单类目标函数的性能，发现这些函数可以处理动态网络大小，为无缝在线训练带来了许多好处。

    

    前向算法提出了一种新的神经网络训练方法，通过在推理过程中更新权重，逐层进行参数更新。这一方法在训练过程中立即降低了内存需求，可能带来更多好处，比如无缝在线训练。这种方法依赖于一个损失（“好度”）函数，该函数可以在每个层的激活上进行评估，这些层的参数大小可以根据网络的超参数化而变化。在开创性论文中，提出了一个好度函数来满足这一需求；然而，如果将其置于一个单类问题的背景下，就无需开创新的损失函数，因为这些函数本身可以处理动态网络大小。在本文中，我们研究了在前向训练方式下训练深度单类目标函数的性能。代码可以在 \url{https://github.com/MichaelHopwood/ForwardForwardOneclass} 找到。

    The forward-forward algorithm presents a new method of training neural networks by updating weights during an inference, performing parameter updates for each layer individually. This immediately reduces memory requirements during training and may lead to many more benefits, like seamless online training. This method relies on a loss ("goodness") function that can be evaluated on the activations of each layer, of which can have a varied parameter size, depending on the hyperparamaterization of the network. In the seminal paper, a goodness function was proposed to fill this need; however, if placed in a one-class problem context, one need not pioneer a new loss because these functions can innately handle dynamic network sizes. In this paper, we investigate the performance of deep one-class objective functions when trained in a forward-forward fashion. The code is available at \url{https://github.com/MichaelHopwood/ForwardForwardOneclass}.
    
[^49]: 使用强化学习的自动桁架设计

    Automatic Truss Design with Reinforcement Learning. (arXiv:2306.15182v1 [cs.AI])

    [http://arxiv.org/abs/2306.15182](http://arxiv.org/abs/2306.15182)

    本文提出了一个两阶段的AutoTruss框架，利用蒙特卡洛树搜索和强化学习方法，高效生成轻型和合法的桁架布局。在实验中，AutoTruss显示出优越的性能，超越了之前报道的方法。

    

    桁架布局设计是建筑行业中的一个基本问题，即寻找一个满足所有物理约束条件的轻型桁架布局。生成最优布局是一个具有挑战性的组合优化问题，通过穷举搜索求解可能非常昂贵。将端到端强化学习方法直接应用于桁架布局设计也是不可行的，因为在物理约束条件下只有一个很小的布局空间是有效的，导致强化学习训练的奖励非常稀疏。在本文中，我们开发了AutoTruss，一个两阶段的框架，可以高效地生成轻型和合法的桁架布局。AutoTruss首先采用蒙特卡洛树搜索来发现多样化的合法布局，然后利用强化学习来逐步优化有效的解决方案。我们在2D和3D设置下的流行桁架布局设计测试案例中进行了实验和消融研究。AutoTruss在表现上超过了最佳报道的方法。

    Truss layout design, namely finding a lightweight truss layout satisfying all the physical constraints, is a fundamental problem in the building industry. Generating the optimal layout is a challenging combinatorial optimization problem, which can be extremely expensive to solve by exhaustive search. Directly applying end-to-end reinforcement learning (RL) methods to truss layout design is infeasible either, since only a tiny portion of the entire layout space is valid under the physical constraints, leading to particularly sparse rewards for RL training. In this paper, we develop AutoTruss, a two-stage framework to efficiently generate both lightweight and valid truss layouts. AutoTruss first adopts Monte Carlo tree search to discover a diverse collection of valid layouts. Then RL is applied to iteratively refine the valid solutions. We conduct experiments and ablation studies in popular truss layout design test cases in both 2D and 3D settings. AutoTruss outperforms the best-reported
    
[^50]: 从仅状态序列学习非马尔科夫决策

    Learning non-Markovian Decision-Making from State-only Sequences. (arXiv:2306.15156v1 [cs.LG])

    [http://arxiv.org/abs/2306.15156](http://arxiv.org/abs/2306.15156)

    本文提出了一种从仅状态序列学习非马尔科夫决策的方法，通过深度生成建模和最大似然估计实现基于模型的模仿。学习的模型能够实现“推理式决策”，并在路径规划任务中展示了有效性。

    

    传统的模仿学习假设能够获得展示者的动作，但是在自然环境中这些动作通常无法观测。此外，在这些环境中的序列决策行为可能偏离标准马尔科夫决策过程（MDP）的假设。为了解决这些挑战，我们探索了非马尔科夫决策过程（nMDP）中仅状态序列的深度生成建模，其中策略是潜在状态转移生成器的能量先验。我们开发了最大似然估计来实现基于模型的模仿，其中包括对先验进行短期MCMC采样和对后验进行重要性采样。学习的模型实现了“推理式决策”，即无模型策略执行等价于先验采样，基于模型的规划则是从策略初始化的后验采样。我们在一个具有非马尔科夫特征的原型路径规划任务中证明了所提方法的有效性。

    Conventional imitation learning assumes access to the actions of demonstrators, but these motor signals are often non-observable in naturalistic settings. Additionally, sequential decision-making behaviors in these settings can deviate from the assumptions of a standard Markov Decision Process (MDP). To address these challenges, we explore deep generative modeling of state-only sequences with non-Markov Decision Process (nMDP), where the policy is an energy-based prior in the latent space of the state transition generator. We develop maximum likelihood estimation to achieve model-based imitation, which involves short-run MCMC sampling from the prior and importance sampling for the posterior. The learned model enables \textit{decision-making as inference}: model-free policy execution is equivalent to prior sampling, model-based planning is posterior sampling initialized from the policy. We demonstrate the efficacy of the proposed method in a prototypical path planning task with non-Mark
    
[^51]: 少样本节点分类的对比元学习

    Contrastive Meta-Learning for Few-shot Node Classification. (arXiv:2306.15154v1 [cs.LG])

    [http://arxiv.org/abs/2306.15154](http://arxiv.org/abs/2306.15154)

    这篇论文提出了一种对比元学习的方法来解决少样本节点分类的问题，通过从大量有标记节点的类别中抽取可迁移的知识，并将其推广到具有有限标记节点的其他类别，学习通用的节点嵌入。

    

    少样本节点分类是在只有有限标记节点作为参考的图上为节点预测标签的任务，在真实世界的图挖掘任务中具有重要意义。本文针对少样本节点分类问题，通过元学习框架利用大量的episode从有大量标记节点的类别中抽取可迁移的知识，并将这些知识推广到具有有限标记节点的其他类别。本质上，少样本节点分类的主要目标是学习可推广到不同类别的节点嵌入。为了实现这一目标，GNN编码器必须能够区分不同类别之间的节点嵌入，同时还要对同一类别中的节点嵌入进行对齐。因此，在这项工作中，我们提出考虑类内和类间的节点嵌入对比。

    Few-shot node classification, which aims to predict labels for nodes on graphs with only limited labeled nodes as references, is of great significance in real-world graph mining tasks. Particularly, in this paper, we refer to the task of classifying nodes in classes with a few labeled nodes as the few-shot node classification problem. To tackle such a label shortage issue, existing works generally leverage the meta-learning framework, which utilizes a number of episodes to extract transferable knowledge from classes with abundant labeled nodes and generalizes the knowledge to other classes with limited labeled nodes. In essence, the primary aim of few-shot node classification is to learn node embeddings that are generalizable across different classes. To accomplish this, the GNN encoder must be able to distinguish node embeddings between different classes, while also aligning embeddings for nodes in the same class. Thus, in this work, we propose to consider both the intra-class and int
    
[^52]: 自动驾驶轨迹预测中真正重要的是什么？

    What Truly Matters in Trajectory Prediction for Autonomous Driving?. (arXiv:2306.15136v1 [cs.RO])

    [http://arxiv.org/abs/2306.15136](http://arxiv.org/abs/2306.15136)

    在自动驾驶系统中，轨迹预测的准确性在固定数据集上表现很好，但在实际驾驶场景中却存在显著差异。现有的评估方法忽视了动力学差距和计算效率对预测结果的影响。

    

    在自动驾驶系统中，轨迹预测在确保安全和促进平稳导航方面起着至关重要的作用。然而，我们观察到在固定数据集上的预测器准确性与在下游任务中的驾驶性能之间存在显著差异。这种差异源于当前轨迹预测评估协议中忽视了两个因素：1）数据集与实际驾驶场景之间的动力学差距；2）预测器的计算效率。在实际场景中，预测算法影响自动驾驶车辆的行为，进而改变道路上其他参与者的行为。这种互动产生了针对预测器的特定动力学，直接影响预测结果。由于其他参与者的反应在数据集上是预先确定的，因此在固定数据集和实际驾驶场景中进行的评估之间存在显著的动力学差距。此外，仅关注准确性无法满足对预测器动态行为和计算效率的需求。

    In the autonomous driving system, trajectory prediction plays a vital role in ensuring safety and facilitating smooth navigation. However, we observe a substantial discrepancy between the accuracy of predictors on fixed datasets and their driving performance when used in downstream tasks. This discrepancy arises from two overlooked factors in the current evaluation protocols of trajectory prediction: 1) the dynamics gap between the dataset and real driving scenario; and 2) the computational efficiency of predictors. In real-world scenarios, prediction algorithms influence the behavior of autonomous vehicles, which, in turn, alter the behaviors of other agents on the road. This interaction results in predictor-specific dynamics that directly impact prediction results. As other agents' responses are predetermined on datasets, a significant dynamics gap arises between evaluations conducted on fixed datasets and actual driving scenarios. Furthermore, focusing solely on accuracy fails to ad
    
[^53]: 软件专业人员对算法种族歧视的观点

    The Perspective of Software Professionals on Algorithmic Racism. (arXiv:2306.15133v1 [cs.SE])

    [http://arxiv.org/abs/2306.15133](http://arxiv.org/abs/2306.15133)

    本文研究了软件专业人员对算法种族歧视的观点，发现一些软件系统对黑人存在歧视，导致黑人在使用基于技术的服务方面面临不公平待遇。

    

    背景。算法种族歧视是指技术解决方案根据用户的种族而限制其行为。近期，有报道称各种数据驱动的软件系统对黑人进行歧视，原因可能是使用了带偏见的数据集，或者是由软件专业人员在代码中传播了偏见。结果，黑人在使用基于技术的服务（如住房、银行和执法）方面面临不公平的待遇。目标。本研究旨在从软件专业人员的角度探索算法种族歧视。方法。应用调查问卷来探索软件从业人员对算法种族歧视的理解，使用描述性统计和编码技术进行数据分析。结果。我们对73名软件专业人员进行了调查，并讨论了他们对软件开发中算法种族歧视的理解和观点。我们的研究结果展示了...

    Context. Algorithmic racism is the term used to describe the behavior of technological solutions that constrains users based on their ethnicity. Lately, various data-driven software systems have been reported to discriminate against Black people, either for the use of biased data sets or due to the prejudice propagated by software professionals in their code. As a result, Black people are experiencing disadvantages in accessing technology-based services, such as housing, banking, and law enforcement. Goal. This study aims to explore algorithmic racism from the perspective of software professionals. Method. A survey questionnaire was applied to explore the understanding of software practitioners on algorithmic racism, and data analysis was conducted using descriptive statistics and coding techniques. Results. We obtained answers from a sample of 73 software professionals discussing their understanding and perspectives on algorithmic racism in software development. Our results demonstrat
    
[^54]: MIMIC: 基于图像对应关系的遮蔽图像建模

    MIMIC: Masked Image Modeling with Image Correspondences. (arXiv:2306.15128v1 [cs.CV])

    [http://arxiv.org/abs/2306.15128](http://arxiv.org/abs/2306.15128)

    MIMIC是一种基于图像对应关系的遮蔽图像建模方法，通过挖掘不需要任何注释的数据集，使用多个自监督模型进行训练，达到了在多个下游任务上优于使用注释挖掘的表示的效果。

    

    许多像素级的密集预测任务——如计算机视觉中的深度估计和语义分割——如今依赖于预训练的图像表示。因此，筛选有效的预训练数据集至关重要。不幸的是，有效的预训练数据集仅通过模拟环境中的带有注释的3D网格、点云和相机参数筛选而来，并不具备多视角场景。我们提出了一种不需要任何注释的数据集筛选机制。我们从开源视频数据集和合成的3D环境中挖掘了两个数据集：MIMIC-1M(包含1.3M个多视角图像对)和MIMIC-3M(包含3.1M个多视角图像对)。我们使用多个自监督模型进行训练，采用不同的遮蔽图像建模目标，展示了以下发现：在多个下游任务中，基于MIMIC-3M训练的表示优于使用注释挖掘的表示，包括深度估计、语义分割、表面法线和姿态估计等。

    Many pixelwise dense prediction tasks-depth estimation and semantic segmentation in computer vision today rely on pretrained image representations. Therefore, curating effective pretraining datasets is vital. Unfortunately, the effective pretraining datasets are those with multi-view scenes and have only been curated using annotated 3D meshes, point clouds, and camera parameters from simulated environments. We propose a dataset-curation mechanism that does not require any annotations. We mine two datasets: MIMIC-1M with 1.3M and MIMIC-3M with 3.1M multi-view image pairs from open-sourced video datasets and from synthetic 3D environments. We train multiple self-supervised models with different masked image modeling objectives to showcase the following findings: Representations trained on MIMIC-3M outperform those mined using annotations on multiple downstream tasks, including depth estimation, semantic segmentation, surface normals, and pose estimation. They also outperform representati
    
[^55]: 《识别和巩固知识工程需求》

    Identifying and Consolidating Knowledge Engineering Requirements. (arXiv:2306.15124v1 [cs.SE])

    [http://arxiv.org/abs/2306.15124](http://arxiv.org/abs/2306.15124)

    本文提出了通过使用主流的软件方法论来解决挑战的参考体系结构，并通过研究不同利益相关者和时代的需求，确定了23个关键的质量属性。通过对近期文献中的候选体系结构进行评估，最终讨论了迈向全面参考体系结构的下一步。

    

    知识工程是创建和维护知识产生系统的过程。在计算机科学和人工智能的历史中，知识工程工作流程被广泛使用，因为高质量的知识被认为对可靠的智能代理至关重要。然而，知识工程的格局发生了变化，出现了四个挑战：未解决的利益相关者需求，技术不匹配，新组织的采用障碍，以及与软件工程实践的不协调。在本文中，我们提出通过使用主流的软件方法论来解决这些挑战，开发一个参考体系结构。通过研究不同利益相关者和时代的需求，我们确定了23个评估参考体系结构的重要质量属性。我们根据这些属性评估了近期文献中的三种候选体系结构。最后，我们讨论了迈向一个全面参考体系结构的下一步，包括...

    Knowledge engineering is the process of creating and maintaining knowledge-producing systems. Throughout the history of computer science and AI, knowledge engineering workflows have been widely used because high-quality knowledge is assumed to be crucial for reliable intelligent agents. However, the landscape of knowledge engineering has changed, presenting four challenges: unaddressed stakeholder requirements, mismatched technologies, adoption barriers for new organizations, and misalignment with software engineering practices. In this paper, we propose to address these challenges by developing a reference architecture using a mainstream software methodology. By studying the requirements of different stakeholders and eras, we identify 23 essential quality attributes for evaluating reference architectures. We assess three candidate architectures from recent literature based on these attributes. Finally, we discuss the next steps towards a comprehensive reference architecture, including
    
[^56]: 对于HPC并行编程模型内核生成的OpenAI Codex的评估

    Evaluation of OpenAI Codex for HPC Parallel Programming Models Kernel Generation. (arXiv:2306.15121v1 [cs.AI])

    [http://arxiv.org/abs/2306.15121](http://arxiv.org/abs/2306.15121)

    本论文评估了在高性能计算中使用AI辅助生成能力来生成基本数值内核的效果，并测试了多种编程模型下生成的内核代码。结果表明，OpenAI Codex在C++中的输出与采纳度和成熟度相关。

    

    我们评估了在高性能计算（HPC）中基本数值内核（包括AXPY，GEMV，GEMM，SpMV，Jacobi Stencil和CG）上的AI辅助生成能力。我们对多种语言支持的编程模型（包括C++（如OpenMP [包括卸载]，OpenACC，Kokkos，SyCL，CUDA和HIP），Fortran（如OpenMP [包括卸载]和OpenACC），Python（如numba，Numba，cuPy和pyCUDA）和Julia（如Threads，CUDA.jl，AMDGPU.jl和KernelAbstractions.jl））进行了生成内核代码的测试。我们使用了OpenAI Codex在2023年4月之后在Visual Studio Code中提供的GitHub Copilot功能来生成大量的实现，只需给出简单的<kernel> + <programming model> + <optional hints>提示变体。为了定量和比较结果，我们提出了一个以每个提示的初始10个建议为基础的熟练度指标。结果表明，OpenAI Codex对于C++的输出与采纳度和成熟度相关。

    We evaluate AI-assisted generative capabilities on fundamental numerical kernels in high-performance computing (HPC), including AXPY, GEMV, GEMM, SpMV, Jacobi Stencil, and CG. We test the generated kernel codes for a variety of language-supported programming models, including (1) C++ (e.g., OpenMP [including offload], OpenACC, Kokkos, SyCL, CUDA, and HIP), (2) Fortran (e.g., OpenMP [including offload] and OpenACC), (3) Python (e.g., numba, Numba, cuPy, and pyCUDA), and (4) Julia (e.g., Threads, CUDA.jl, AMDGPU.jl, and KernelAbstractions.jl). We use the GitHub Copilot capabilities powered by OpenAI Codex available in Visual Studio Code as of April 2023 to generate a vast amount of implementations given simple <kernel> + <programming model> + <optional hints> prompt variants. To quantify and compare the results, we propose a proficiency metric around the initial 10 suggestions given for each prompt. Results suggest that the OpenAI Codex outputs for C++ correlate with the adoption and mat
    
[^57]: 持续学习用于越域行人检测

    Continual Learning for Out-of-Distribution Pedestrian Detection. (arXiv:2306.15117v1 [cs.CV])

    [http://arxiv.org/abs/2306.15117](http://arxiv.org/abs/2306.15117)

    本文提出了一种持续学习的解决方案，用于解决行人检测中的越域泛化问题。该方法通过对模型权重进行弹性权重合并，使其在新的数据集上学习并保持在旧数据集上的性能，避免了灾难性遗忘。在实验中，该方法在两个数据集上分别实现了9%和18%的误检率改进。

    

    本文提出了一种持续学习的解决方案，用于解决行人检测中的越域泛化问题。尽管最近的行人检测模型在各种数据集上取得了令人印象深刻的性能，但它们对推理数据分布的变化仍然敏感。我们的方法采用并修改了弹性权重合并方法，应用于主干目标检测网络，以惩罚对初始学习任务的重要性发生变化的模型权重。我们表明，当使用一个数据集进行训练并在另一个数据集上进行微调时，我们的解决方案能够学习新的分布并保持在先前数据集上的性能，避免了灾难性遗忘。我们在两个常用的数据集CrowdHuman和CityPersons上进行了跨数据集实验，并取得了显著的改进，分别在CrowdHuman和CityPersons数据集上降低了9%和18%的误检率。

    A continual learning solution is proposed to address the out-of-distribution generalization problem for pedestrian detection. While recent pedestrian detection models have achieved impressive performance on various datasets, they remain sensitive to shifts in the distribution of the inference data. Our method adopts and modifies Elastic Weight Consolidation to a backbone object detection network, in order to penalize the changes in the model weights based on their importance towards the initially learned task. We show that when trained with one dataset and fine-tuned on another, our solution learns the new distribution and maintains its performance on the previous one, avoiding catastrophic forgetting. We use two popular datasets, CrowdHuman and CityPersons for our cross-dataset experiments, and show considerable improvements over standard fine-tuning, with a 9% and 18% miss rate percent reduction improvement in the CrowdHuman and CityPersons datasets, respectively.
    
[^58]: 从$O(\sqrt{n})$到$O(\log(n))$的二次规划问题

    From $O(\sqrt n)$ to $O(\log n)$ in Quadratic Programming. (arXiv:2306.15079v1 [math.OC])

    [http://arxiv.org/abs/2306.15079](http://arxiv.org/abs/2306.15079)

    这篇论文提出了一种迭代复杂度为$O(\log(n))$的二次规划优化算法，并通过严格的理论证明验证了该算法的可行性。这一重大突破使得我们从$O(\sqrt{n})$的优化算法过渡到$O(\log(n))$的优化算法，其在大数据和人工智能时代具有重要应用价值。

    

    多年来，数值优化理论一直存在一个困扰，即是否存在一个迭代复杂度为$O(\log(n))$的优化算法。本文通过引入一种全新的优化算法和严格的理论证明来回答这个问题。该算法以有界盒二次规划问题（Box-QP）为起点，许多实际优化问题可以通过对偶理论转化为Box-QP问题。本文首次提出了一个迭代复杂度为$O(\log(n))$的QP算法，尤其是其表现类似于“直接”方法：所需迭代次数是确定性的，精确值为$\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$。这一重大突破使得我们能够从$O(\sqrt{n})$的优化算法过渡到$O(\log(n))$的优化算法，其出色的可扩展性在当今的大数据和人工智能时代尤为重要。

    A "dark cloud" hangs over numerical optimization theory for decades, namely, whether an optimization algorithm $O(\log(n))$ iteration complexity exists. "Yes", this paper answers, with a new optimization algorithm and strict theory proof. It starts with box-constrained quadratic programming (Box-QP), and many practical optimization problems fall into Box-QP. Smooth quadratic programming (QP) and nonsmooth Lasso can be reformulated as Box-QP via duality theory. It is the first time to present an $O(\log(n))$ iteration complexity QP algorithm, in particular, which behaves like a "direct" method: the required number of iterations is deterministic with exact value $\left\lceil\log\left(\frac{3.125n}{\epsilon}\right)/\log(1.5625)\right\rceil$. This significant breakthrough enables us to transition from the $O(\sqrt{n})$ to the $O(\log(n))$ optimization algorithm, whose amazing scalability is particularly relevant in today's era of big data and artificial intelligence.
    
[^59]: 分子几何深度学习

    Molecular geometric deep learning. (arXiv:2306.15065v1 [physics.comp-ph])

    [http://arxiv.org/abs/2306.15065](http://arxiv.org/abs/2306.15065)

    分子几何深度学习（Mol-GDL）提出了一种更通用的分子表示方法，通过仅使用非共价键构建的分子图，在分子性质预测中取得了与基于共价键模型相似甚至更好的结果，展现了超越基于共价键的分子图的巨大潜力。

    

    几何深度学习（GDL）在分子数据分析中展示了巨大的威力和潜力。然而，高效的分子表示仍然是一个巨大的挑战。目前，基于共价键的分子图已成为表示原子层次的分子拓扑的事实标准。我们首次证明，仅由非共价键构建的分子图可以在分子性质预测中取得与基于共价键模型相似甚至更好的结果。这证明了超越基于共价键的分子图的新型分子表示的巨大潜力。基于这一发现，我们提出了分子几何深度学习（Mol-GDL）。其核心思想是将更通用的分子表示融入GDL模型中。在我们的Mol-GDL中，分子拓扑被建模为一系列的分子图，每个分子图聚焦于不同尺度的原子相互作用。

    Geometric deep learning (GDL) has demonstrated huge power and enormous potential in molecular data analysis. However, a great challenge still remains for highly efficient molecular representations. Currently, covalent-bond-based molecular graphs are the de facto standard for representing molecular topology at the atomic level. Here we demonstrate, for the first time, that molecular graphs constructed only from non-covalent bonds can achieve similar or even better results than covalent-bond-based models in molecular property prediction. This demonstrates the great potential of novel molecular representations beyond the de facto standard of covalent-bond-based molecular graphs. Based on the finding, we propose molecular geometric deep learning (Mol-GDL). The essential idea is to incorporate a more general molecular representation into GDL models. In our Mol-GDL, molecular topology is modeled as a series of molecular graphs, each focusing on a different scale of atomic interactions. In th
    
[^60]: 预训练任务多样性与回归问题中非贝叶斯上下文学习的出现

    Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. (arXiv:2306.15063v1 [cs.LG])

    [http://arxiv.org/abs/2306.15063](http://arxiv.org/abs/2306.15063)

    预训练的transformer在回归问题中展现了非贝叶斯上下文学习能力，其在任务多样性阈值以下表现类似于贝叶斯估计器，而在阈值以上明显优于贝叶斯估计器，与岭回归一致。

    

    预训练的transformer表现出了令人钦佩的上下文学习能力（ICL）：它们可以从仅提供在提示中的少量示例中学习任务，而无需更新任何权重。这引发了一个基本问题：ICL能够解决在预训练期间未见过的、在本质上与之前任务非常不同的新任务吗？为了探索这个问题，我们在预训练数据集中改变任务的多样性，研究了ICL在线性回归中的表现。我们经验性地证明了出现ICL的任务多样性阈值。在这个阈值以下，预训练的transformer无法解决未见的回归任务，因为它的行为类似于具有非多样性预训练任务分布作为先验的贝叶斯估计器。超过这个阈值后，transformer明显优于这个估计器；它的行为与岭回归一致，对$\textit{所有任务}$，包括在预训练期间未见过的任务，具有高斯先验。

    Pretrained transformers exhibit the remarkable ability of in-context learning (ICL): they can learn tasks from just a few examples provided in the prompt without updating any weights. This raises a foundational question: can ICL solve fundamentally $\textit{new}$ tasks that are very different from those seen during pretraining? To probe this question, we examine ICL's performance on linear regression while varying the diversity of tasks in the pretraining dataset. We empirically demonstrate a $\textit{task diversity threshold}$ for the emergence of ICL. Below this threshold, the pretrained transformer cannot solve unseen regression tasks as it behaves like a Bayesian estimator with the $\textit{non-diverse pretraining task distribution}$ as the prior. Beyond this threshold, the transformer significantly outperforms this estimator; its behavior aligns with that of ridge regression, corresponding to a Gaussian prior over $\textit{all tasks}$, including those not seen during pretraining. 
    
[^61]: 使用Swap优化建筑结构的向量化：高效卷积通道交换混合策略

    Optimized Vectorizing of Building Structures with Swap: High-Efficiency Convolutional Channel-Swap Hybridization Strategy. (arXiv:2306.15035v1 [cs.AI])

    [http://arxiv.org/abs/2306.15035](http://arxiv.org/abs/2306.15035)

    本论文提出了一种高效的卷积通道交换混合策略（Swap），用于优化建筑结构的向量化。该方法通过将相邻或对角特征交替交换并混合不同通道的信息，实现了集成局部特征空间信息的功能。同时，采用了基于组的参数共享机制，大大减少了模型中的冗余参数。

    

    建筑平面图的重建，也称为足迹重建，属于计算机视觉和地理信息学领域，长期以来一直受到传统卷积模型中冗余参数的挑战。因此，在本文中，我们提出了一种先进和自适应的移位架构，即Swap操作，它结合了非指数增长参数，同时保持了集成局部特征空间信息的类似功能，类似于高维卷积操作器。Swap跨通道操作架构通过异或操作交替交换相邻或对角特征，然后通过1x1卷积操作混合交替的通道，以整合不同通道的信息。另一方面，SwapNN架构采用了受卷积神经网络过程启发的基于组的参数共享机制，从而显著减少了数量。

    The building planar graph reconstruction, a.k.a. footprint reconstruction, which lies in the domain of computer vision and geoinformatics, has been long afflicted with the challenge of redundant parameters in conventional convolutional models. Therefore, in this paper, we proposed an advanced and adaptive shift architecture, namely the Swap operation, which incorporates non-exponential growth parameters while retaining analogous functionalities to integrate local feature spatial information, resembling a high-dimensional convolution operator. The Swap, cross-channel operation, architecture implements the XOR operation to alternately exchange adjacent or diagonal features, and then blends alternating channels through a 1x1 convolution operation to consolidate information from different channels. The SwapNN architecture, on the other hand, incorporates a group-based parameter-sharing mechanism inspired by the convolutional neural network process and thereby significantly reducing the num
    
[^62]: 超越动态规划

    Beyond dynamic programming. (arXiv:2306.15029v1 [cs.LG])

    [http://arxiv.org/abs/2306.15029](http://arxiv.org/abs/2306.15029)

    提出了一种新的理论方法——得分寿命规划，用于解决强化学习问题。方法可以搜索非稳态策略函数，并直接计算最优无限时间区间动作序列。这种方法的核心思想是建立动作序列和实数之间的映射，并通过优化问题计算最优动作序列。方法在非线性最优控制问题中证明了有效性。

    

    本文提出了一种新颖的理论方法——得分寿命规划，用于解决强化学习问题。与传统的基于动态规划的方法相比，我们的方法可以搜索非稳态策略函数，并且可以直接计算给定状态下的最优无限时间区间动作序列。我们方法的核心思想是建立无限时间区间动作序列和有界区间内实数之间的映射。这种构造使我们能够制定一个优化问题，直接计算最优无限时间区间动作序列，无需策略函数。我们通过将该方法应用于非线性最优控制问题，验证了其有效性。总的来说，我们的贡献为制定和解决强化学习问题提供了一种新颖的理论框架。

    In this paper, we present Score-life programming, a novel theoretical approach for solving reinforcement learning problems. In contrast with classical dynamic programming-based methods, our method can search over non-stationary policy functions, and can directly compute optimal infinite horizon action sequences from a given state. The central idea in our method is the construction of a mapping between infinite horizon action sequences and real numbers in a bounded interval. This construction enables us to formulate an optimization problem for directly computing optimal infinite horizon action sequences, without requiring a policy function. We demonstrate the effectiveness of our approach by applying it to nonlinear optimal control problems. Overall, our contributions provide a novel theoretical framework for formulating and solving reinforcement learning problems.
    
[^63]: DNABERT-2:多种物种基因组的高效基础模型和基准

    DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome. (arXiv:2306.15006v1 [q-bio.GN])

    [http://arxiv.org/abs/2306.15006](http://arxiv.org/abs/2306.15006)

    本研究提出了DNABERT-2，一个用于多种物种基因组的高效基础模型和基准。我们通过使用基于统计的数据压缩算法Byte Pair Encoding（BPE）替代传统的k-mer标记化，克服了k-mer标记化的计算和样本效率问题，并取得了重要进展。

    

    解码基因组的语言复杂性是生物学中一个关键问题，而DNABERT和Nucleotide Transformer等预训练基础模型在这个领域取得了重要进展。现有的工作主要依赖于k-mer作为基因组语言的标记，由于其简单性。然而，我们认为k-mer标记化引入的计算和样本效率问题是发展大规模基因组基础模型的主要障碍。我们提供了关于基因组标记化的概念和经验见解，基于此提出用基于统计的数据压缩算法Byte Pair Encoding（BPE）替代k-mer标记化，BPE通过迭代合并语料库中最频繁共同出现的基因组片段来构建标记。我们证明，BPE不仅克服了k-mer标记化的局限性，还能从非重叠标记化的计算效率中受益。

    Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenizatio
    
[^64]: SIMF: 自动驾驶的语义感知交互式运动预测

    SIMF: Semantics-aware Interactive Motion Forecasting for Autonomous Driving. (arXiv:2306.14941v1 [cs.CV])

    [http://arxiv.org/abs/2306.14941](http://arxiv.org/abs/2306.14941)

    本文提出了一种名为SIMF的方法，用于自动驾驶车辆中语义感知的交互式运动预测。该方法通过实现基于语义的行为体选择和注意力机制提取全局编码，能够捕捉空间信息和语义信息，并优选相关的行为体进行运动预测。

    

    自动驾驶车辆需要对周围多个行为体（行人和车辆）进行运动预测，以做出最优导航决策。现有的方法主要关注如何利用这些行为体的位置和速度，并未能捕捉到场景中的语义信息。此外，为了减少与场景中行为体数量增加相关的计算复杂度，一些方法利用欧氏距离来剪枝远离的行为体。然而，仅仅基于距离的度量无法选择相关的行为体并准确进行预测。为了解决这些问题，我们提出了一种称为SIMF的方法，用于捕捉空间信息以及语义信息，并优选相关的行为体进行运动预测。具体而言，我们通过实现一种基于语义的行为体选择方法，将其通过注意力机制传递，以提取全局编码。

    Autonomous vehicles require motion forecasting of their surrounding multi-agents (pedestrians and vehicles) to make optimal decisions for navigation. The existing methods focus on techniques to utilize the positions and velocities of these agents and fail to capture semantic information from the scene. Moreover, to mitigate the increase in computational complexity associated with the number of agents in the scene, some works leverage Euclidean distance to prune far-away agents. However, distance-based metric alone is insufficient to select relevant agents and accurately perform their predictions. To resolve these issues, we propose Semantics-aware Interactive Motion Forecasting (SIMF) method to capture semantics along with spatial information, and optimally select relevant agents for motion prediction. Specifically, we achieve this by implementing a semantic-aware selection of relevant agents from the scene and passing them through an attention mechanism to extract global encodings. Th
    
[^65]: Sciama论述的在随机宇宙中存在生命的辨别：区分苹果和橙子

    Sciama's argument on life in a random universe: Distinguishing apples from oranges. (arXiv:2306.14934v1 [physics.hist-ph])

    [http://arxiv.org/abs/2306.14934](http://arxiv.org/abs/2306.14934)

    Sciama通过区分苹果和橙子的方式论述在随机宇宙中存在生命的问题，他认为生命的存在取决于许多量，但在没有对这些量有充分了解的情况下，他的论点暗示了一个看起来是“智能设计”的宇宙。

    

    Dennis Sciama认为生命的存在取决于许多量，也就是基本常数，因此在一个随机的宇宙中，生命应该是非常不太可能的。然而，没有对这些常数有充分的了解，他的论据意味着一个看起来是“智能设计”的宇宙。

    Dennis Sciama argued that the existence of life depended on many quantities, the fundamental constants, so in a random universe life should be highly unlikely. However, without full knowledge of these constants, his argument implies a universe that would appear to be `intelligently designed.'
    
[^66]: 将双向长短期记忆网络与子词嵌入相结合用于作者归属的研究

    Integrating Bidirectional Long Short-Term Memory with Subword Embedding for Authorship Attribution. (arXiv:2306.14933v1 [cs.CL])

    [http://arxiv.org/abs/2306.14933](http://arxiv.org/abs/2306.14933)

    这项研究提出了一种将双向长短期记忆网络与子词嵌入相结合的方法，用于解决作者归属问题。该方法能够在处理文本中的隐含词问题的同时保留词的顺序上下文。

    

    揭示给定文本文档的作者身份是作者归属问题。深度学习方法已成功地使用多样的基于词的风格标记来处理作者归属的内在问题。然而，基于词的作者归属系统的性能受到训练语料库词汇的限制。文献推荐了以字符为基础的风格标记作为克服隐含词问题的替代方法。然而，基于字符的方法经常无法捕捉文本中的词的顺序关系，这是进一步改善的难题。本文讨论的问题是是否可以解决文本文档中隐含词的歧义性，同时保留词的顺序上下文。因此，提出了一种基于双向长短期记忆网络（BLSTM）与二维卷积神经网络（CNN）相结合的方法来捕捉顺序上下文。

    The problem of unveiling the author of a given text document from multiple candidate authors is called authorship attribution. Manifold word-based stylistic markers have been successfully used in deep learning methods to deal with the intrinsic problem of authorship attribution. Unfortunately, the performance of word-based authorship attribution systems is limited by the vocabulary of the training corpus. Literature has recommended character-based stylistic markers as an alternative to overcome the hidden word problem. However, character-based methods often fail to capture the sequential relationship of words in texts which is a chasm for further improvement. The question addressed in this paper is whether it is possible to address the ambiguity of hidden words in text documents while preserving the sequential context of words. Consequently, a method based on bidirectional long short-term memory (BLSTM) with a 2-dimensional convolutional neural network (CNN) is proposed to capture sequ
    
[^67]: LLM辅助内容分析：利用大型语言模型支持演绎编码

    LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding. (arXiv:2306.14924v1 [cs.CL])

    [http://arxiv.org/abs/2306.14924](http://arxiv.org/abs/2306.14924)

    本研究探索了使用大型语言模型（LLMs）来减少演绎编码所需时间的方法，同时保留传统内容分析的灵活性。通过一个案例研究和经验基准测试，证明了在不同演绎编码任务上，GPT-3.5在LLM辅助内容分析（LACA）中的有效性。

    

    演绎编码是一种广泛使用的定性研究方法，用于确定文档中主题的普遍性。尽管有用，演绎编码通常是繁琐且耗时的，因为它要求研究人员阅读、解释并可靠地对大量非结构化文本进行分类。大型语言模型（LLMs），如ChatGPT，是一类快速发展的人工智能工具，可以执行各种自然语言处理和推理任务。在这项研究中，我们探索了使用LLMs来减少演绎编码所需的时间，同时保留传统内容分析的灵活性。我们概述了所提出的方法，称为LLM辅助内容分析（LACA），并使用GPT-3.5在一个公开可用的演绎编码数据集上进行了深入案例研究。此外，我们还进行了一个经验基准测试，使用LACA在4个公开可用的数据集上评估GPT-3.5在不同演绎编码任务上的表现。

    Deductive coding is a widely used qualitative research method for determining the prevalence of themes across documents. While useful, deductive coding is often burdensome and time consuming since it requires researchers to read, interpret, and reliably categorize a large body of unstructured text documents. Large language models (LLMs), like ChatGPT, are a class of quickly evolving AI tools that can perform a range of natural language processing and reasoning tasks. In this study, we explore the use of LLMs to reduce the time it takes for deductive coding while retaining the flexibility of a traditional content analysis. We outline the proposed approach, called LLM-assisted content analysis (LACA), along with an in-depth case study using GPT-3.5 for LACA on a publicly available deductive coding data set. Additionally, we conduct an empirical benchmark using LACA on 4 publicly available data sets to assess the broader question of how well GPT-3.5 performs across a range of deductive co
    
[^68]: 利用自然语言处理进行课堂讨论的自动评估

    Utilizing Natural Language Processing for Automated Assessment of Classroom Discussion. (arXiv:2306.14918v1 [cs.CL])

    [http://arxiv.org/abs/2306.14918](http://arxiv.org/abs/2306.14918)

    本研究利用自然语言处理技术，通过自动生成细化标准得分，实现对课堂讨论质量的自动评估。实验结果令人鼓舞，同时指出标准仍有改进空间，并发现不同的NLP方法对不同的标准更有效。

    

    严格而互动的课堂讨论对于学习至关重要，同时也是大多数教学干预的核心组成部分。然而，对讨论质量进行规模化的正式评估对于大多数研究者来说是昂贵且不可行的。在这项工作中，我们尝试了各种现代自然语言处理（NLP）技术，以自动生成课堂文本讨论质量的细化标准得分。具体而言，我们使用了包含超过18000轮次、注释有详细的教学分析运动（ATM）代码的90个课堂讨论记录数据集，并聚焦于四个教学质量评估（IQA）标准。尽管数据量有限，我们的工作在一些标准上取得了令人鼓舞的结果，同时也暗示其他标准仍有改进空间。我们还发现，某些NLP方法在某些标准上效果更好。

    Rigorous and interactive class discussions that support students to engage in high-level thinking and reasoning are essential to learning and are a central component of most teaching interventions. However, formally assessing discussion quality 'at scale' is expensive and infeasible for most researchers. In this work, we experimented with various modern natural language processing (NLP) techniques to automatically generate rubric scores for individual dimensions of classroom text discussion quality. Specifically, we worked on a dataset of 90 classroom discussion transcripts consisting of over 18000 turns annotated with fine-grained Analyzing Teaching Moves (ATM) codes and focused on four Instructional Quality Assessment (IQA) rubrics. Despite the limited amount of data, our work shows encouraging results in some of the rubrics while suggesting that there is room for improvement in the others. We also found that certain NLP approaches work better for certain rubrics.
    
[^69]: 迈向教育问题生成的丰富可控性

    Towards Enriched Controllability for Educational Question Generation. (arXiv:2306.14917v1 [cs.CL])

    [http://arxiv.org/abs/2306.14917](http://arxiv.org/abs/2306.14917)

    本研究旨在通过引入新的引导属性（问题明确性）来丰富教育问题生成的可控性。我们提出了通过控制生成明确和隐含wh-问题的方法。研究展示了通过问题明确性和叙事要素同时控制问题生成的初步证据。

    

    生成问题（QG）是自然语言处理（NLP）中的一个任务，它涉及根据输入（通常由文本和目标答案组成）自动生成问题。近期关于QG的研究旨在控制生成问题的类型，以满足教育需求。教育QG中可控性的一个显著例子是生成涉及特定叙事要素的问题，例如因果关系、结果解决或预测。本研究旨在通过引入新的引导属性（问题明确性）来丰富QG的可控性。我们提议通过控制从适合儿童的故事中生成明确和隐含的wh-问题。我们展示了仅通过问题明确性以及与另一个目标属性（问题的叙事要素）同时控制QG的初步证据。代码公开可在github.com/bernardoleite/question-generation-control获取。

    Question Generation (QG) is a task within Natural Language Processing (NLP) that involves automatically generating questions given an input, typically composed of a text and a target answer. Recent work on QG aims to control the type of generated questions so that they meet educational needs. A remarkable example of controllability in educational QG is the generation of questions underlying certain narrative elements, e.g., causal relationship, outcome resolution, or prediction. This study aims to enrich controllability in QG by introducing a new guidance attribute: question explicitness. We propose to control the generation of explicit and implicit wh-questions from children-friendly stories. We show preliminary evidence of controlling QG via question explicitness alone and simultaneously with another target attribute: the question's narrative element. The code is publicly available at github.com/bernardoleite/question-generation-control.
    
[^70]: GPT-4网状化学家用于MOF探索

    GPT-4 Reticular Chemist for MOF Discovery. (arXiv:2306.14915v1 [cs.AI])

    [http://arxiv.org/abs/2306.14915](http://arxiv.org/abs/2306.14915)

    GPT-4网状化学家是一个集成了AI模型GPT-4的框架，通过迭代的人工智能交互，能够发现金属有机框架系列。

    

    我们提出了一个新的框架，将AI模型GPT-4集成到网状化学实验的迭代过程中，利用AI与人类学徒之间的合作工作流。这个GPT-4网状化学家是一个由三个阶段组成的综合系统。每个阶段都以不同的方式利用GPT-4，其中GPT-4为化学实验提供详细的指导，学徒则对实验结果进行反馈，包括成功和失败，以便在下一次迭代中AI进行内部学习。这种迭代的人工智能交互使得GPT-4能够像经验丰富的化学家一样从结果中学习，采用一种提示学习策略。重要的是，该系统基于自然语言进行开发和操作，无需编码技能，因此对所有化学家都具有可访问性。我们的GPT-4网状化学家展示了一系列金属有机框架的发现。

    We present a new framework integrating the AI model GPT-4 into the iterative process of reticular chemistry experimentation, leveraging a cooperative workflow of interaction between AI and a human apprentice. This GPT-4 Reticular Chemist is an integrated system composed of three phases. Each of these utilizes GPT-4 in various capacities, wherein GPT-4 provides detailed instructions for chemical experimentation and the apprentice provides feedback on the experimental outcomes, including both success and failures, for the in-text learning of AI in the next iteration. This iterative human-AI interaction enabled GPT-4 to learn from the outcomes, much like an experienced chemist, by a prompt-learning strategy. Importantly, the system is based on natural language for both development and operation, eliminating the need for coding skills, and thus, make it accessible to all chemists. Our GPT-4 Reticular Chemist demonstrated the discovery of an isoreticular series of metal-organic frameworks (
    
[^71]: FSUIE:一种用于普适信息抽取的新型模糊跨度机制

    FSUIE: A Novel Fuzzy Span Mechanism for Universal Information Extraction. (arXiv:2306.14913v1 [cs.CL])

    [http://arxiv.org/abs/2306.14913](http://arxiv.org/abs/2306.14913)

    FSUIE是一种用于普适信息抽取的新型模糊跨度机制，通过引入模糊跨度损失和模糊跨度注意力，能够在快速收敛和少量数据训练轮数的情况下显著提高信息抽取的性能。

    

    普适信息抽取（UIE）作为一个统一的框架已经取得了广泛成功，但UIE模型存在一些局限性。例如，在训练过程中，它们过于依赖于数据中的跨度边界，这并不反映跨度注释的真实挑战。微小的位置调整也可以满足要求。此外，UIE模型缺乏对信息抽取中有限的跨度长度特征的关注。为了解决这些问题，我们提出了模糊跨度普适信息抽取（FSUIE）框架。具体而言，我们的贡献包括模糊跨度损失和模糊跨度注意力两个概念。我们在一系列主要的信息抽取任务上的实验结果显示，在数据量和训练轮数较少的情况下，与基准模型相比，FSUIE在快速收敛和强大性能方面取得了显著改进。这些结果证明了FSUIE的有效性和泛化能力。

    Universal Information Extraction (UIE) has been introduced as a unified framework for various Information Extraction (IE) tasks and has achieved widespread success. Despite this, UIE models have limitations. For example, they rely heavily on span boundaries in the data during training, which does not reflect the reality of span annotation challenges. Slight adjustments to positions can also meet requirements. Additionally, UIE models lack attention to the limited span length feature in IE. To address these deficiencies, we propose the Fuzzy Span Universal Information Extraction (FSUIE) framework. Specifically, our contribution consists of two concepts: fuzzy span loss and fuzzy span attention. Our experimental results on a series of main IE tasks show significant improvement compared to the baseline, especially in terms of fast convergence and strong performance with small amounts of data and training epochs. These results demonstrate the effectiveness and generalization of FSUIE in di
    
[^72]: 在同伴辅导互动中识别措辞的研究

    "You might think about slightly revising the title": identifying hedges in peer-tutoring interactions. (arXiv:2306.14911v1 [cs.CL])

    [http://arxiv.org/abs/2306.14911](http://arxiv.org/abs/2306.14911)

    该研究利用多模态同伴辅导数据集构建了一个计算框架，用于识别同伴辅导互动中的措辞。最佳表现的是一个混合方法，它比现有基准更好且更容易解释，并发现了一些新特征。

    

    措辞在对话互动中起着重要作用。在同伴辅导中，导师在缺乏默契的双人对话中使用措辞来减轻指令和负反馈的影响。为了建立一个管理与学生的亲密关系以提高学习效果的辅导代理系统，我们利用多模态同伴辅导数据集构建了一个用于识别措辞的计算框架。我们比较了依赖预训练资源和结合社会科学文献见解的方法。我们的最佳表现来自一个混合方法，其性能优于现有基准，并且更容易解释。我们使用一个模型可解释性工具探索了同伴辅导对话中特征的特点，并识别了一些新特征以及这种混合模型方法的好处。

    Hedges play an important role in the management of conversational interaction. In peer tutoring, they are notably used by tutors in dyads (pairs of interlocutors) experiencing low rapport to tone down the impact of instructions and negative feedback. Pursuing the objective of building a tutoring agent that manages rapport with students in order to improve learning, we used a multimodal peer-tutoring dataset to construct a computational framework for identifying hedges. We compared approaches relying on pre-trained resources with others that integrate insights from the social science literature. Our best performance involved a hybrid approach that outperforms the existing baseline while being easier to interpret. We employ a model explainability tool to explore the features that characterize hedges in peer-tutoring conversations, and we identify some novel features, and the benefits of such a hybrid model approach.
    
[^73]: 机器学习语言模型时代中人标记数据的重要性

    The Importance of Human-Labeled Data in the Era of LLMs. (arXiv:2306.14910v1 [cs.CL])

    [http://arxiv.org/abs/2306.14910](http://arxiv.org/abs/2306.14910)

    本文论述了在LLMs时代，人标记数据仍然具有重要性的论据和支持。

    

    大型语言模型（LLMs）的出现在定制机器学习模型的开发方面带来了一场革命，并引发了关于重新定义数据要求的讨论。LLMs的训练和实施所带来的自动化引发了对人工标记干预可能不再具有与监督学习时代相同重要性的讨论和期望。本文提出了有力的论据，支持在LLMs时代人标记数据的持续重要性。

    The advent of large language models (LLMs) has brought about a revolution in the development of tailored machine learning models and sparked debates on redefining data requirements. The automation facilitated by the training and implementation of LLMs has led to discussions and aspirations that human-level labeling interventions may no longer hold the same level of importance as in the era of supervised learning. This paper presents compelling arguments supporting the ongoing relevance of human-labeled data in the era of LLMs.
    
[^74]: PRISMA-DFLLM：PRISMA的一种扩展，使用领域特定的大型语言模型进行系统文献综述

    PRISMA-DFLLM: An Extension of PRISMA for Systematic Literature Reviews using Domain-specific Finetuned Large Language Models. (arXiv:2306.14905v1 [cs.CL])

    [http://arxiv.org/abs/2306.14905](http://arxiv.org/abs/2306.14905)

    PRISMA-DFLLM是将大型语言模型(LLMs)与PRISMA的严格报告指南相结合，通过在领域特定的学术论文上进行微调，提高了系统文献综述的效率和可扩展性，同时开启了新的研究机遇。

    

    随着开源大型语言模型（LLMs）的普及和高效的微调技术，我们正处于涌现出许多针对专业领域和应用的特定领域LLMs的阶段，这些LLMs已经针对当前通用LLMs无法适应的专业领域进行了微调。在学术界，这项技术有潜力改变我们进行系统文献综述（SLRs）的方式，获取知识和生成新见解。本文提出了一种结合LLMs强大能力和Preferred Reporting Items for Systematic Reviews and Meta-Analyses（PRISMA）的严格报告指南的AI-Enabled方法框架。通过对通过严格SLR过程选定的领域特定学术论文进行LLMs微调，提出的PRISMA-DFLLM（用于领域特定的LLMs微调）报告指南具有更高的效率、可重复性和可扩展性，并同时开启了一系列新的研究机遇。

    With the proliferation of open-sourced Large Language Models (LLMs) and efficient finetuning techniques, we are on the cusp of the emergence of numerous domain-specific LLMs that have been finetuned for expertise across specialized fields and applications for which the current general-purpose LLMs are unsuitable. In academia, this technology has the potential to revolutionize the way we conduct systematic literature reviews (SLRs), access knowledge and generate new insights. This paper proposes an AI-enabled methodological framework that combines the power of LLMs with the rigorous reporting guidelines of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA). By finetuning LLMs on domain-specific academic papers that have been selected as a result of a rigorous SLR process, the proposed PRISMA-DFLLM (for Domain-specific Finetuned LLMs) reporting guidelines offer the potential to achieve greater efficiency, reusability and scalability, while also opening the po
    
[^75]: 从社交网络中检测抑郁情绪并进行情感知识共享

    Detect Depression from Social Networks with Sentiment Knowledge Sharing. (arXiv:2306.14903v1 [cs.CL])

    [http://arxiv.org/abs/2306.14903](http://arxiv.org/abs/2306.14903)

    本论文通过深度学习技术以及情感知识共享，从社交网络消息中识别抑郁症的潜在迹象，旨在提供早期心理健康状况识别的方法。

    

    社交网络在传播人们的观点、情绪、思维和恐惧方面起着重要作用。值得注意的是，在COVID-19大流行期间的封锁期后，抑郁症问题引起了人们的越来越多的关注，许多人借助社交网络表达情绪。利用深度学习技术从社交网络消息中辨别潜在的抑郁症迹象有助于早期识别心理健康状况。目前，通过社交网络检测抑郁症的努力通常仅依靠对文本内容进行分析，忽略了其他潜在信息。在这项工作中，我们进行了彻底的研究，揭示了抑郁症和负面情绪状态之间的强相关性。将这样的关联作为外部知识的整合可以为检测抑郁症提供宝贵的洞见。因此，我们提出了一种多任务训练框架DeSK，利用情感知识共享来实现。

    Social network plays an important role in propagating people's viewpoints, emotions, thoughts, and fears. Notably, following lockdown periods during the COVID-19 pandemic, the issue of depression has garnered increasing attention, with a significant portion of individuals resorting to social networks as an outlet for expressing emotions. Using deep learning techniques to discern potential signs of depression from social network messages facilitates the early identification of mental health conditions. Current efforts in detecting depression through social networks typically rely solely on analyzing the textual content, overlooking other potential information. In this work, we conduct a thorough investigation that unveils a strong correlation between depression and negative emotional states. The integration of such associations as external knowledge can provide valuable insights for detecting depression. Accordingly, we propose a multi-task training framework, DeSK, which utilizes share
    
[^76]: 基于数据驱动的正式敏感机器翻译方法：语言特定处理与合成数据生成

    Data-Driven Approach for Formality-Sensitive Machine Translation: Language-Specific Handling and Synthetic Data Generation. (arXiv:2306.14514v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.14514](http://arxiv.org/abs/2306.14514)

    本文介绍了一种基于数据驱动的正式敏感机器翻译方法，通过语言特定处理和合成数据生成，显著改进了翻译性能。

    

    在本文中，我们介绍了一种基于数据驱动的正式敏感机器翻译（FSMT）方法，该方法针对四种目标语言的独特语言特性进行优化。我们的方法主要依靠两个核心策略：语言特定数据处理和使用大规模语言模型和经验性提示工程生成合成数据。该方法相较于基准模型有显著改进，突显了数据中心技术的有效性。我们的提示工程策略通过产生优质的合成翻译示例进一步提高了性能。

    In this paper, we introduce a data-driven approach for Formality-Sensitive Machine Translation (FSMT) that caters to the unique linguistic properties of four target languages. Our methodology centers on two core strategies: 1) language-specific data handling, and 2) synthetic data generation using large-scale language models and empirical prompt engineering. This approach demonstrates a considerable improvement over the baseline, highlighting the effectiveness of data-centric techniques. Our prompt engineering strategy further improves performance by producing superior synthetic translation examples.
    
[^77]: 通用框架下适应性约束下的顺序决策问题研究

    A General Framework for Sequential Decision-Making under Adaptivity Constraints. (arXiv:2306.14468v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14468](http://arxiv.org/abs/2306.14468)

    本论文提出了一个通用框架，研究了在适应性约束下的顺序决策问题。具体地，我们提供了Eluder Condition类，并针对稀缺策略切换和批次学习约束分别提供了相应的算法。此工作是第一个考虑通用函数类别下稀缺策略切换和批次学习的工作，涵盖了之前研究中的大部分模型。

    

    我们在研究通用的顺序决策问题下对两个适应性约束进行了首次探索：策略切换稀缺和批次学习。首先，我们提供了一个称为Eluder Condition类的通用类别，其中包括了广泛的强化学习类别。然后，对于策略切换稀缺约束，我们提供了一个通用算法，在EC类别上实现了大约$ \widetilde{\mathcal{O}}(\log K)$的切换代价和$\widetilde{\mathcal{O}}(\sqrt{K})$的后悔代价。对于批次学习约束，我们提供了一个算法，在$B$个批次的情况下，提供了大约$\widetilde{\mathcal{O}}(\sqrt{K}+K/B)$的后悔代价。这篇论文是第一篇考虑通用函数类别下稀缺策略切换和批次学习的工作，涵盖了之前研究中几乎所有的模型，如表格MDP (Bai et al. 2019; Zhang et al. 2020)、线性MDP (Wang et al. 2021; Gao et al. 2021)、低Eluder维度MDP (Kong et al. 2021; Gao et al. 2021)、广义线性函数类别等。

    We take the first step in studying general sequential decision-making under two adaptivity constraints: rare policy switch and batch learning. First, we provide a general class called the Eluder Condition class, which includes a wide range of reinforcement learning classes. Then, for the rare policy switch constraint, we provide a generic algorithm to achieve a $\widetilde{\mathcal{O}}(\log K) $ switching cost with a $\widetilde{\mathcal{O}}(\sqrt{K})$ regret on the EC class. For the batch learning constraint, we provide an algorithm that provides a $\widetilde{\mathcal{O}}(\sqrt{K}+K/B)$ regret with the number of batches $B.$ This paper is the first work considering rare policy switch and batch learning under general function classes, which covers nearly all the models studied in the previous works such as tabular MDP (Bai et al. 2019; Zhang et al. 2020), linear MDP (Wang et al. 2021; Gao et al. 2021), low eluder dimension MDP (Kong et al. 2021; Gao et al. 2021), generalized linear fu
    
[^78]: 特征对抗蒸馏用于点云分类

    Feature Adversarial Distillation for Point Cloud Classification. (arXiv:2306.14221v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.14221](http://arxiv.org/abs/2306.14221)

    本文提出了一种特征对抗蒸馏方法（FAD），用于解决点云分类中知识传递的信息损失问题。在实验证明，该方法在模型压缩的同时保持了竞争性能。

    

    由于点云的不规则和无序的几何结构，传统的知识蒸馏技术在直接应用于点云任务时丢失了很多信息。本文提出了一种特征对抗蒸馏（Feature Adversarial Distillation，简称FAD）方法，这是一种在点云蒸馏中使用的通用对抗损失函数，可减少知识传递过程中的信息损失。在特征提取阶段，利用教师提取的特征作为鉴别器，在训练阶段学生不断生成新特征。学生的特征通过攻击教师的反馈得到得分，以判断学生是否学习了知识。在ModelNet40和ScanObjectNN数据集上进行的标准点云分类实验中，我们的方法在40倍模型压缩的同时降低了知识传递中的信息损失，并保持了竞争性能。

    Due to the point cloud's irregular and unordered geometry structure, conventional knowledge distillation technology lost a lot of information when directly used on point cloud tasks. In this paper, we propose Feature Adversarial Distillation (FAD) method, a generic adversarial loss function in point cloud distillation, to reduce loss during knowledge transfer. In the feature extraction stage, the features extracted by the teacher are used as the discriminator, and the students continuously generate new features in the training stage. The feature of the student is obtained by attacking the feedback from the teacher and getting a score to judge whether the student has learned the knowledge well or not. In experiments on standard point cloud classification on ModelNet40 and ScanObjectNN datasets, our method reduced the information loss of knowledge transfer in distillation in 40x model compression while maintaining competitive performance.
    
[^79]: 可解释人工智能中的操纵风险: 不一致问题的影响

    Manipulation Risks in Explainable AI: The Implications of the Disagreement Problem. (arXiv:2306.13885v1 [cs.AI])

    [http://arxiv.org/abs/2306.13885](http://arxiv.org/abs/2306.13885)

    本文讨论可解释人工智能中的操纵风险，即同一决策或预测可能有多种解释带来的挑战。本文分析了攻击机器学习模型或底层数据以影响解释与直接利用解释阶段的策略，并探讨了解释提供者可追求的几个目标和具体场景。

    

    人工智能系统在我们生活的高风险领域中越来越广泛地应用，这增加了解释这些决策并确保它们与我们想要的决策一致的需求。因此，可解释人工智能（XAI）领域显现出来。然而，它面临一项重大挑战，即不一致问题，即同一人工智能的决策或预测可能有多种解释。虽然已经认识到了不一致问题的存在，但与此问题相关的潜在影响尚未被广泛研究。在本文中，我们首先概述了解释提供者可以采用的不同策略，以使返回的解释符合他们的利益。我们区分了攻击机器学习模型或底层数据以影响解释的策略和直接利用解释阶段的策略。接下来，我们分析了解释提供者可追求的几个目标和具体场景。

    Artificial Intelligence (AI) systems are increasingly used in high-stakes domains of our life, increasing the need to explain these decisions and to make sure that they are aligned with how we want the decision to be made. The field of Explainable AI (XAI) has emerged in response. However, it faces a significant challenge known as the disagreement problem, where multiple explanations are possible for the same AI decision or prediction. While the existence of the disagreement problem is acknowledged, the potential implications associated with this problem have not yet been widely studied. First, we provide an overview of the different strategies explanation providers could deploy to adapt the returned explanation to their benefit. We make a distinction between strategies that attack the machine learning model or underlying data to influence the explanations, and strategies that leverage the explanation phase directly. Next, we analyse several objectives and concrete scenarios the provid
    
[^80]: 注意力机制中的边缘最大化

    Margin Maximization in Attention Mechanism. (arXiv:2306.13596v1 [cs.LG])

    [http://arxiv.org/abs/2306.13596](http://arxiv.org/abs/2306.13596)

    这篇论文证明了，在softmax-attention模型中，通过在p或等价的W上运行梯度下降，可以收敛到一个最大边缘解，这将局部最优的标记与非最优的标记分隔开。这明确地将注意力机制形式化为标记分离机制。

    

    注意力机制是Transformer架构的核心组件，也是大型语言模型取得惊人成功的原因之一。然而，注意力机制背后的理论原则尚不清楚，特别是它的非凸优化动力学。本文探讨了开创性的softmax-attention模型$f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$，其中$\boldsymbol{X}$是标记序列，$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$是可调参数。我们证明了在$\boldsymbol{p}$或等价的$\boldsymbol{W}$上运行梯度下降会沿着方向收敛到分隔“局部最优”标记和“非最优”标记的最大边缘解。这明确地形式化了注意力作为一种标记分离机制。值得注意的是，我们的结果适用于一般数据，并使用嵌入$\boldsymbol{Xv}$和$\texttt{softmax}(\boldsymbol{XWp})$精细地表征标记的“最优性”。

    Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where, $\boldsymbol{X}$ is the token sequence and $(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are tunable parameters. We prove that running gradient descent on $\boldsymbol{p}$, or equivalently $\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as a token separation mechanism. Remarkably, our results are applicable to general data and precisely characterize $\textit{optimality}$ of tokens in terms of the value embeddings $\boldsymbol{Xv}$ and
    
[^81]: DiversiGATE: 一个可靠的大规模语言模型全面框架

    DiversiGATE: A Comprehensive Framework for Reliable Large Language Models. (arXiv:2306.13230v1 [cs.CL])

    [http://arxiv.org/abs/2306.13230](http://arxiv.org/abs/2306.13230)

    DiversiGATE是一个统一框架，汇集了多种LLM验证方法，其中包括自一致性、数学提示和WebGPT，同时提出了一个符合该框架的新模型“SelfLearner”，该模型可以从自己的输出中学习并优化性能，在实验中表现良好，GSM8K基准测试上提高了7%的性能。

    

    本文提出了DiversiGATE，一个统一的框架，汇集LLM验证的多种方法。该框架包括两个主要组成部分：多样化和聚合，在现有的验证方法上提供了全面的视角，例如自一致性、数学提示和WebGPT。此外，本文提出了一个新颖的“SelfLearner”模型，符合DiversiGATE框架，可以从自己的输出中学习并随着时间的推移不断完善其性能，从而提高准确性。为了评估SelfLearner的有效性，我们进行了一系列严格的实验，包括对合成数据和广泛使用的算术推理基准测试GSM8K的测试。我们的结果表明，我们的方法优于传统的LLMs，在GSM8K基准测试中实现了可观的54.8%->61.8%的提高。

    In this paper, we introduce DiversiGATE, a unified framework that consolidates diverse methodologies for LLM verification. The proposed framework comprises two main components: Diversification and Aggregation which provide a holistic perspective on existing verification approaches, such as Self-Consistency, Math Prompter and WebGPT. Furthermore, we propose a novel `SelfLearner' model that conforms to the DiversiGATE framework which can learn from its own outputs and refine its performance over time, leading to improved accuracy. To evaluate the effectiveness of SelfLearner, we conducted a rigorous series of experiments, including tests on synthetic data as well as on popular arithmetic reasoning benchmarks such as GSM8K. Our results demonstrate that our approach outperforms traditional LLMs, achieving a considerable 54.8% -> 61.8% improvement on the GSM8K benchmark.
    
[^82]: FuXi: 一个15天全球天气预报级联机器学习系统

    FuXi: A cascade machine learning forecasting system for 15-day global weather forecast. (arXiv:2306.12873v1 [physics.ao-ph])

    [http://arxiv.org/abs/2306.12873](http://arxiv.org/abs/2306.12873)

    逐步级联模型FuXi在15天全球天气预报中表现出更好的性能，并通过减少预测误差的积累达到优化。

    

    近年来，随着机器学习模型在天气预报中的快速发展，最先进的机器学习模型在0.25度空间分辨率下的10天天气预报中已经表现出比欧洲中期天气预报中心(ECMWF)的高分辨率预报(HRES)更优越的性能。然而，挑战在于在15天预报中表现与ECMWF集合平均(EM)相当。以前的研究表明，缓解预报误差的积累对于有效的长期预报非常重要。尽管有许多减少积累误差的努力，包括自回归多时间步长损失，但使用单个模型发现无法在短和长导出时间上达到最佳性能。因此，我们提出了FuXi，这是一个级联机器学习天气预测系统，提供了分辨率为0.25度、时间分辨率为6小时的15天全球预测。FuXi基于级联集合模型开发，它集成了多种模型的优势，并减少了预测误差的积累。使用空气温度，比湿度和位势高度的均方根误差(RMSE)和异常相关系数(ACC)评估了FuXi的性能。结果表明，与ECMWF HRES相比，FuXi在15天预报中表现出更好的性能，并显著减少了积累误差。

    Over the past few years, due to the rapid development of machine learning (ML) models for weather forecasting, state-of-the-art ML models have shown superior performance compared to the European Centre for Medium-Range Weather Forecasts (ECMWF)'s high-resolution forecast (HRES) in 10-day forecasts at a spatial resolution of 0.25 degree. However, the challenge remains to perform comparably to the ECMWF ensemble mean (EM) in 15-day forecasts. Previous studies have demonstrated the importance of mitigating the accumulation of forecast errors for effective long-term forecasts. Despite numerous efforts to reduce accumulation errors, including autoregressive multi-time step loss, using a single model is found to be insufficient to achieve optimal performance in both short and long lead times. Therefore, we present FuXi, a cascaded ML weather forecasting system that provides 15-day global forecasts with a temporal resolution of 6 hours and a spatial resolution of 0.25 degree. FuXi is develope
    
[^83]: 机器设计的极限挑战：利用人工智能实现自动CPU设计

    Pushing the Limits of Machine Design: Automated CPU Design with AI. (arXiv:2306.12456v1 [cs.AI])

    [http://arxiv.org/abs/2306.12456](http://arxiv.org/abs/2306.12456)

    本研究运用新的人工智能方法，自动设计了一个中央处理器(CPU)，这是人类设计过的最复杂的装置之一，从而探索了机器设计的边界。

    

    设计活动——构建一个满足给定目标和限制条件的工件描述——区别于人类和传统机器，赋予机器人类水平或更高的设计能力一直是一个长期追求的目标。虽然机器已经展示了运用先进的人工智能技术设计新材料、蛋白质和计算机程序的能力，但这些工件的设计空间相对较小，因此，“机器是否能像人类一样设计？”成为了一个开放性问题。为了拓展机器设计的边界，本研究提出了一种新的人工智能方法，利用该方法实现了自动设计一个中央处理器(CPU)，即计算机的大脑，这是人类设计过的最复杂的装置之一。该方法从只有外部输入输出观测的情况下生成CPU设计的电路逻辑，该逻辑用二元推演图(BSD)表示。

    Design activity -- constructing an artifact description satisfying given goals and constraints -- distinguishes humanity from other animals and traditional machines, and endowing machines with design abilities at the human level or beyond has been a long-term pursuit. Though machines have already demonstrated their abilities in designing new materials, proteins, and computer programs with advanced artificial intelligence (AI) techniques, the search space for designing such objects is relatively small, and thus, "Can machines design like humans?" remains an open question. To explore the boundary of machine design, here we present a new AI approach to automatically design a central processing unit (CPU), the brain of a computer, and one of the world's most intricate devices humanity have ever designed. This approach generates the circuit logic, which is represented by a graph structure called Binary Speculation Diagram (BSD), of the CPU design from only external input-output observations
    
[^84]: 超bolic活跃学习在域转移下的语义分割中的应用

    Hyperbolic Active Learning for Semantic Segmentation under Domain Shift. (arXiv:2306.11180v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.11180](http://arxiv.org/abs/2306.11180)

    这项研究首次在Poincaré双曲球模型中运用超bolic活跃学习方法，利用区域内像素嵌入的半径变化作为新的数据获取策略，以提升域转移下语义分割的性能。

    

    对于域转移下的语义分割任务，基于图像区域和伪标签的主动学习获取策略是最先进的。在区域内存在不同类别的伪标签可以识别出不同类别之间的像素，这是一种高效的主动学习数据获取策略。然而，由于设计限制，伪标签的变化仅限于选择类别的轮廓，限制了最终的主动学习性能。我们首次在Poincaré双曲球模型中使用超bolic方法来进行语义分割的主动学习，并利用区域内像素嵌入的半径变化作为一种新的数据获取策略。这源于一种无层次约束训练的超bolic空间的新颖几何特性，我们通过实验证明了这一点。也就是说，类别被映射到具有相当内类半径方差的紧凑超bolic区域，因为模型将难以解释的类别放置在更密集的超bolic区域内。

    For the task of semantic segmentation (SS) under domain shift, active learning (AL) acquisition strategies based on image regions and pseudo labels are state-of-the-art (SoA). The presence of diverse pseudo-labels within a region identifies pixels between different classes, which is a labeling efficient active learning data acquisition strategy. However, by design, pseudo-label variations are limited to only select the contours of classes, limiting the final AL performance. We approach AL for SS in the Poincar\'e hyperbolic ball model for the first time and leverage the variations of the radii of pixel embeddings within regions as a novel data acquisition strategy. This stems from a novel geometric property of a hyperbolic space trained without enforced hierarchies, which we experimentally prove. Namely, classes are mapped into compact hyperbolic areas with a comparable intra-class radii variance, as the model places classes of increasing explainable difficulty at denser hyperbolic are
    
[^85]: 基于注意力知识图卷积网络的旅游景点推荐

    Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network. (arXiv:2306.10946v1 [cs.IR] CROSS LISTED)

    [http://arxiv.org/abs/2306.10946](http://arxiv.org/abs/2306.10946)

    本文提出了一种基于注意力知识图卷积网络的旅游景点推荐模型，通过自动语义发掘目标景点的相邻实体，根据旅客的喜好选择，预测类似景点的概率，实验中取得良好效果。

    

    基于知识图谱的推荐算法在相对成熟阶段，但在特定领域的推荐仍存在问题。例如在旅游领域，选择适合的旅游景点属性流程作为推荐基础较为复杂。本文提出改进的注意力知识图卷积网络模型(Att-KGCN)，自动语义地发掘目标景点的相邻实体，利用注意力层将相对相似的位置进行聚合，并通过推理旅客喜好选择，预测类似景点的概率作为推荐系统。实验中，采用索科特拉岛-也门的旅游数据，证明了注意力知识图卷积网络在旅游领域的景点推荐效果良好。

    The recommendation algorithm based on knowledge graphs is at a relatively mature stage. However, there are still some problems in the recommendation of specific areas. For example, in the tourism field, selecting suitable tourist attraction attributes process is complicated as the recommendation basis for tourist attractions. In this paper, we propose the improved Attention Knowledge Graph Convolution Network model, named (Att-KGCN), which automatically discovers the neighboring entities of the target scenic spot semantically. The attention layer aggregates relatively similar locations and represents them with an adjacent vector. Then, according to the tourist's preferred choices, the model predicts the probability of similar spots as a recommendation system. A knowledge graph dataset of tourist attractions used based on tourism data on Socotra Island-Yemen. Through experiments, it is verified that the Attention Knowledge Graph Convolution Network has a good effect on the recommendatio
    
[^86]: 评估和改进大型语言模型的时间推理能力的基准研究

    Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models. (arXiv:2306.08952v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.08952](http://arxiv.org/abs/2306.08952)

    本研究提出了一个全面的探测数据集来评估大型语言模型的时间推理能力，并提出了一种使用时间跨度提取和时敏性强化学习的新型学习框架来改进大型语言模型的时间推理能力。实验结果表明该方法的有效性。

    

    时间推理是非常重要的。许多事实是与时间相关的。例如，运动员会不时地更换球队，不同的政府官员会定期进行选举。先前的时间相关问题回答（QA）数据集往往在时间跨度或问题类型的涵盖上存在偏见。在本文中，我们介绍了一个全面的探测数据集\tempreason，用于评估大型语言模型的时间推理能力。我们的数据集包括三个时间推理级别的问题。此外，我们还提出了一种基于时间跨度提取和时敏性强化学习的新型学习框架，以改进大型语言模型的时间推理能力。我们在封闭书式QA、开放书式QA和推理QA设置中进行了实验，并证明了我们方法的有效性。我们的代码和数据已在https://github.com/DAMO-NLP-SG/TempReason上发布。

    Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset \tempreason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach. Our code and data are released on https://github.com/DAMO-NLP-SG/TempReason.
    
[^87]: 自然语言处理中社会人口统计偏见的调查

    Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])

    [http://arxiv.org/abs/2306.08158](http://arxiv.org/abs/2306.08158)

    本文调查了209篇关于NLP模型偏见的论文，其中大部分涉及社会人口统计偏见。研究者提出了社会人口统计偏见的定义，并确定了NLP偏见研究的三个主要类别。当前去偏见技术只是隐藏了偏见而不是真正去除它，需要进一步改进。

    

    深度神经网络在训练过程中往往会学习到非预期的偏见，这在实际应用中可能会产生有害的影响。本文对209篇关于NLP模型中偏见的论文进行了调查，其中大部分论文涉及社会人口统计偏见。为了更好地理解偏见与真实世界的危害之间的区别，我们借鉴心理学和行为经济学的思想，提出了社会人口统计偏见的定义。我们确定了NLP偏见研究的三个主要类别：偏见类型、量化偏见和去偏见。我们认为当前对于量化偏见的方法存在可靠性问题，许多偏见度量并不涉及真实世界中的偏见，当前的去偏见技术是表面的，只是隐藏了偏见，而不是真正去除它。最后，我们提供了未来工作的建议。

    Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
    
[^88]: 推荐系统如何从大型语言模型中受益：一项调查研究

    How Can Recommender Systems Benefit from Large Language Models: A Survey. (arXiv:2306.05817v1 [cs.IR])

    [http://arxiv.org/abs/2306.05817](http://arxiv.org/abs/2306.05817)

    本文对将大型语言模型（LLM）应用于推荐系统进行了全面的调查研究，从两个角度总结了现有的研究工作：如何在推荐系统中调整LLM和调整LLM时在哪里调整。最后，我们提出了一些潜在的研究方向和挑战。

    

    推荐系统在匹配互联网应用程序用户的信息需求方面发挥着重要作用。在自然语言处理领域中，大型语言模型已经展现出了惊人的新兴能力（例如指令跟踪、推理），从而为将LLM调整到推荐系统中以提高性能和改善用户体验的研究方向带来了希望。在本文中，我们从应用导向的角度对此研究方向进行了全面的调查。我们首先从两个正交的角度总结了现有的研究工作：如何在推荐系统中调整LLM和调整LLM时在哪里调整。对于“在哪里”这个问题，我们讨论了LLM在推荐流程的不同阶段中可能发挥的作用，即特征工程、特征编码器、评分/排名函数和流程控制器。对于“如何”这个问题，我们调查了训练和推理策略，从而得出两个细粒度的分类标准，即是否调整LLM和是否将LLM作为独立模型或混合模型组件使用。最后，我们提出了在将LLM调整到RS中的一些挑战和潜在方向，包括与现有系统的集成、用户反馈、评估度量和知识蒸馏。

    Recommender systems (RS) play important roles to match users' information needs for Internet applications. In natural language processing (NLP) domains, large language model (LLM) has shown astonishing emergent abilities (e.g., instruction following, reasoning), thus giving rise to the promising research direction of adapting LLM to RS for performance enhancements and user experience improvements. In this paper, we conduct a comprehensive survey on this research direction from an application-oriented view. We first summarize existing research works from two orthogonal perspectives: where and how to adapt LLM to RS. For the "WHERE" question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, and pipeline controller. For the "HOW" question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLMs or not, a
    
[^89]: 多视角聚类方法：一步多视角聚类与多样性表征

    One-step Multi-view Clustering with Diverse Representation. (arXiv:2306.05437v1 [cs.LG])

    [http://arxiv.org/abs/2306.05437](http://arxiv.org/abs/2306.05437)

    本文提出了一种一步多视角聚类与多样性表征的方法，将多视角学习和k-means聚类融合到一个统一框架中，实验结果表明其在各种标准的多视角数据集上都优于现有算法。

    

    多视角聚类因其能够利用不同视角的信息来提高效果而备受关注。本文提出了一种一步多视角聚类与多样性表征的方法，将多视角学习和k-means聚类融合到一个统一框架中。实验结果表明，在各种标准的多视角数据集上，我们的方法在效果和效率方面都优于现有的同类算法。

    Multi-view clustering has attracted broad attention due to its capacity to utilize consistent and complementary information among views. Although tremendous progress has been made recently, most existing methods undergo high complexity, preventing them from being applied to large-scale tasks. Multi-view clustering via matrix factorization is a representative to address this issue. However, most of them map the data matrices into a fixed dimension, which limits the expressiveness of the model. Moreover, a range of methods suffer from a two-step process, i.e., multimodal learning and the subsequent $k$-means, inevitably causing a sub-optimal clustering result. In light of this, we propose a one-step multi-view clustering with diverse representation method, which incorporates multi-view learning and $k$-means into a unified framework. Specifically, we first project original data matrices into various latent spaces to attain comprehensive information and auto-weight them in a self-supervis
    
[^90]: 置换等变图框架在异质半监督学习中的应用

    Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised Learning. (arXiv:2306.04265v1 [cs.LG])

    [http://arxiv.org/abs/2306.04265](http://arxiv.org/abs/2306.04265)

    本文介绍了一个用于异质半监督学习的新型图神经网络模型PEGFAN，它使用置换等变图框架实现了多尺度特征提取，表现优于其他最先进模型，特别是在相对较大和密集连接的数据集中。

    

    异质图的本质与同质图显著不同，这表明1-hop以外的聚合方式并引起早期图神经网络模型的困难。本文展示了一种新的多尺度提取方法，通过构建具有置换等变性，高效性和稀疏性的Haar-type图框架，在图上深度学习任务中实现。我们进一步使用我们构建的图框架设计了图框架神经网络模型PEGFAN。实验在合成数据集和9个基准数据集上进行，与其他最先进的模型进行性能比较。结果表明，我们的模型在某些异质图数据集（包括相对较大和更密集的连接的大部分异质数据集）上可以达到最佳性能，并在其余数据集上具有竞争性能。

    The nature of heterophilous graphs is significantly different with that of homophilous graphs, which suggests aggregations beyond 1-hop neighborhood and causes difficulties in early graph neural network models. In this paper, we develop a new way to implement multi-scale extraction via constructing Haar-type graph framelets with desired properties of permutation equivariance, efficiency, and sparsity, for deep learning tasks on graphs. We further deisgn a graph framelet neural network model PEGFAN using our constructed graph framelets. The experiments are conducted on a synthetic dataset and 9 benchmark datasets to compare performance with other state-of-the-art models. The result shows that our model can achieve best performance on certain datasets of heterophilous graphs (including the majority of heterophilous datasets with relatively larger sizes and denser connections) and competitive performance on the remaining.
    
[^91]: 可逆量化索引调制用于静态深度神经网络水印技术

    Reversible Quantization Index Modulation for Static Deep Neural Network Watermarking. (arXiv:2305.17879v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2305.17879](http://arxiv.org/abs/2305.17879)

    本文提出了一种使用可逆量化索引调制（QIM）的基于可逆数据隐藏（RDH）的静态深度神经网络水印方案，解决了现有方法在可用性、容量和保真度方面的弱点，并通过模拟结果验证了方案的可行性和有效性。

    

    静态深度神经网络（DNN）水印技术通常采用不可逆的方法将水印嵌入DNN模型权重中。然而，这种方法会对水印模型造成永久性损坏，无法满足完整性验证的要求。可逆数据隐藏（RDH）方法提供了一种潜在的解决方案，但现有方法在可用性、容量和保真度方面存在弱点，阻碍了其实际采用。在本文中，我们提出了一种基于RDH的静态DNN水印方案，采用量化索引调制（QIM）。我们的方案采用了一维量化器的新方法进行水印嵌入。此外，我们设计了两种方案来解决DNN的完整性保护和合法认证的挑战。通过对训练损失和分类准确度的模拟结果，我们证明了我们提出的方案的可行性和有效性，并突出了其优越的适应性。

    Static deep neural network (DNN) watermarking techniques typically employ irreversible methods to embed watermarks into the DNN model weights. However, this approach causes permanent damage to the watermarked model and fails to meet the requirements of integrity authentication. Reversible data hiding (RDH) methods offer a potential solution, but existing approaches suffer from weaknesses in terms of usability, capacity, and fidelity, hindering their practical adoption. In this paper, we propose a novel RDH-based static DNN watermarking scheme using quantization index modulation (QIM). Our scheme incorporates a novel approach based on a one-dimensional quantizer for watermark embedding. Furthermore, we design two schemes to address the challenges of integrity protection and legitimate authentication for DNNs. Through simulation results on training loss and classification accuracy, we demonstrate the feasibility and effectiveness of our proposed schemes, highlighting their superior adapt
    
[^92]: 具有上下文模型的Levin树搜索

    Levin Tree Search with Context Models. (arXiv:2305.16945v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.16945](http://arxiv.org/abs/2305.16945)

    本文提出了一种新的具有上下文模型的Levin树搜索算法，通过将神经网络替换为上下文模型，实现了LTS损失的凸优化，并在多个基准测试中取得了明显优于LTS+NN的结果。

    

    Levin Tree Search（LTS）是一种利用策略（动作的概率分布）的搜索算法，并具有关于达到目标节点之前扩展次数的理论保证，这取决于策略的质量。我们将这个保证称为LTS损失，可以将其作为优化表示策略的神经网络（LTS+NN）的损失函数。在这项工作中，我们展示了神经网络可以用在线压缩文献中的参数化上下文模型来替代（LTS+CM）。我们证明了在这种新模型下LTS损失是凸的，从而可以使用标准凸优化工具，并且对于给定的解轨迹集合，在在线设置中可以获得到最优参数的收敛保证——而神经网络无法提供这样的保证。新的LTS+CM算法在几个基准测试中与LTS+NN相比表现出明显优势：Sokoban（Boxoban）、The Witness和24-Sliding Tile Puzzle（STP）。

    Levin Tree Search (LTS) is a search algorithm that makes use of a policy (a probability distribution over actions) and comes with a theoretical guarantee on the number of expansions before reaching a goal node, depending on the quality of the policy. This guarantee can be used as a loss function, which we call the LTS loss, to optimize neural networks representing the policy (LTS+NN). In this work we show that the neural network can be substituted with parameterized context models originating from the online compression literature (LTS+CM). We show that the LTS loss is convex under this new model, which allows for using standard convex optimization tools, and obtain convergence guarantees to the optimal parameters in an online setting for a given set of solution trajectories -- guarantees that cannot be provided for neural networks. The new LTS+CM algorithm compares favorably against LTS+NN on several benchmarks: Sokoban (Boxoban), The Witness, and the 24-Sliding Tile puzzle (STP). The
    
[^93]: 内容丰富的多模态转换器训练与 LoReTTa

    Training Transitive and Commutative Multimodal Transformers with LoReTTa. (arXiv:2305.14243v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.14243](http://arxiv.org/abs/2305.14243)

    LoReTTa是一个自监督框架，可以在具有不同模态的数据集中转换。通过合成数据集的方法，显著提高了下游任务的性能。

    

    实践中，收集两个匹配的形态A和B或B和C的多模态数据集很困难，获得包含三个对齐形态A、B和C的数据集更加具有挑战性。我们引入了LoReTTa以应对这个未被充分研究的问题。我们的自监督框架结合了因果掩码建模和交换律和传递性的规则，可以在不同的模态中转换。我们的实验表明，这种合成显着提高了下游任务的性能。

    Collecting a multimodal dataset with two paired modalities A and B or B and C is difficult in practice. Obtaining a dataset with three aligned modalities A, B, and C is even more challenging. For example, some public medical datasets have only genetic sequences and microscopic images for one patient, and only genetic sequences and radiological images for another - but no dataset includes both microscopic and radiological images for the same patient. This makes it difficult to integrate and combine all modalities into a large pre-trained neural network. We introduce LoReTTa (Linking mOdalities with a tRansitive and commutativE pre-Training sTrAtegy) to address this understudied problem. Our self-supervised framework combines causal masked modeling with the rules of commutativity and transitivity to transition within and between different modalities. Thus, it can model the relation A -> C with A -> B -> C. Given a dataset containing only the disjoint combinations (A, B) and (B, C), we sh
    
[^94]: 捕捉促销期间的转化率波动：一种新颖的历史数据再利用方法

    Capturing Conversion Rate Fluctuation during Sales Promotions: A Novel Historical Data Reuse Approach. (arXiv:2305.12837v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2305.12837](http://arxiv.org/abs/2305.12837)

    本论文提出了一种名为HDR的新方法，通过重复使用历史促销数据，来捕捉促销转化模式，达到更好地适应促销模式的目的。

    

    转化率（CVR）预测是在线推荐系统的核心组件之一，已经提出了各种方法以获得准确和一致的CVR估计。然而，我们观察到，即使训练良好的CVR预测模型，在促销期间也经常表现出次优的性能。这主要归因于数据分布转移问题，其中传统方法不再起作用。因此，我们寻求开发替代建模技术用于CVR预测。观察到不同促销之间存在相似的购买模式，我们提出了重用历史促销数据以捕捉促销转化模式的方法。因此，我们提出了一种新颖的历史数据再利用（HDR）方法，该方法首先检索历史上相似的促销数据，然后使用获取的数据微调CVR预测模型以更好地适应促销模式。HDR由三个组件组成：自动数据

    Conversion rate (CVR) prediction is one of the core components in online recommender systems, and various approaches have been proposed to obtain accurate and well-calibrated CVR estimation. However, we observe that a well-trained CVR prediction model often performs sub-optimally during sales promotions. This can be largely ascribed to the problem of the data distribution shift, in which the conventional methods no longer work. To this end, we seek to develop alternative modeling techniques for CVR prediction. Observing similar purchase patterns across different promotions, we propose reusing the historical promotion data to capture the promotional conversion patterns. Herein, we propose a novel \textbf{H}istorical \textbf{D}ata \textbf{R}euse (\textbf{HDR}) approach that first retrieves historically similar promotion data and then fine-tunes the CVR prediction model with the acquired data for better adaptation to the promotion mode. HDR consists of three components: an automated data 
    
[^95]: 基于学习度量的大规模包裹操作

    Large-Scale Package Manipulation via Learned Metrics of Pick Success. (arXiv:2305.10272v1 [cs.RO])

    [http://arxiv.org/abs/2305.10272](http://arxiv.org/abs/2305.10272)

    本文讨论了基于学习度量的大规模包裹操作，通过训练拾取成功预测器和学习拾取质量度量，实现了能够大规模部署的强力抓握策略。

    

    自动化仓储操作可以降低物流成本，最终降低消费品价格，提高交货速度，并增强对劳动力波动的抵抗能力。近年来，自动化重复任务的兴趣增加，但大多数是在受控环境中进行的。从杂乱的堆堆中挑选物品等任务直到最近才变得足够强大，可以在最小人工干预下进行大规模部署。本文展示了亚马逊机器人的Robot Induction（Robin）群的大规模包裹操作，该群利用在实际生产数据上训练的拾取成功预测器。具体而言，该系统在超过394K个拾取上进行了训练。它用于把每天高达5百万个包裹进行了分离，本文的评估期间操作了超过2亿个包裹。开发的学习拾取质量度量实时排名各种拾取替代方案，并采用高成功率的强力抓握策略。

    Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to workforce fluctuations. The past few years have seen increased interest in automating such repeated tasks but mostly in controlled settings. Tasks such as picking objects from unstructured, cluttered piles have only recently become robust enough for large-scale deployment with minimal human intervention.  This paper demonstrates a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which utilizes a pick success predictor trained on real production data. Specifically, the system was trained on over 394K picks. It is used for singulating up to 5~million packages per day and has manipulated over 200~million packages during this paper's evaluation period.  The developed learned pick quality measure ranks various pick alternatives in real-time and p
    
[^96]: 卷积神经网络的定量语义比较

    Quantified Semantic Comparison of Convolutional Neural Networks. (arXiv:2305.07663v1 [cs.CV])

    [http://arxiv.org/abs/2305.07663](http://arxiv.org/abs/2305.07663)

    本研究提出了两种方法来量化卷积神经网络潜在空间中语义信息之间的相似性，从而揭示CNN层内语义信息的流动和相似性，以及不同网络之间的相似度程度。

    

    卷积神经网络（CNN）在计算机视觉领域的应用处于领先地位，具有出色的性能，然而它们的工作原理却很难阐明。但是，对于自动驾驶这类安全关键应用，模型选择还应考虑候选模型在模型透明性方面如何表示语义信息。为了解决这一尚未解决的问题，我们的工作提出了两种方法来量化CNN潜在空间中语义信息之间的相似性，旨在揭示CNN层内语义信息的流动和相似性，以及不同网络之间的相似度程度。我们使用了可解释人工智能（XAI）领域的著名技术作为基础，这些技术用于获得每个潜在空间中语义概念的全局向量表示，并基于它们在测试输入上的激活进行比较。本工作在三个不同的目标检测器和两个不同范围的图像数据集上进行了评估。

    The state-of-the-art in convolutional neural networks (CNNs) for computer vision excels in performance, while remaining opaque. But due to safety regulations for safety-critical applications, like perception for automated driving, the choice of model should also take into account how candidate models represent semantic information for model transparency reasons. To tackle this yet unsolved problem, our work proposes two methods for quantifying the similarity between semantic information in CNN latent spaces. These allow insights into both the flow and similarity of semantic information within CNN layers, and into the degree of their similitude between different networks. As a basis, we use renown techniques from the field of explainable artificial intelligence (XAI), which are used to obtain global vector representations of semantic concepts in each latent space. These are compared with respect to their activation on test inputs. When applied to three diverse object detectors and two d
    
[^97]: 区域感知预训练：视觉变压器下的开放词汇物体检测

    Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2305.07011v1 [cs.CV])

    [http://arxiv.org/abs/2305.07011](http://arxiv.org/abs/2305.07011)

    本文提出了一种基于视觉变压器的对比图像-文本预训练方法，针对开放词汇的物体检测任务，采用区域感知预训练、聚焦损失和新颖物体提案等技术，在LVIS上取得了32.1$AP_r$的最佳效果。

    

    本文提出了区域感知开放词汇视觉变压器（RO-ViT），一种对比图像-文本预训练方法，旨在填补图像级预训练和开放词汇物体检测之间的差距。在预训练阶段，我们建议随机裁剪并调整位置嵌入的区域，而不是使用整个图像位置嵌入。这更好地匹配了检测微调阶段中区域级别上使用位置嵌入的方式。此外，我们用聚焦损失替换了对比学习中常用的softmax交叉熵损失，以更好地学习那些有信息量但难以捕捉的例子。最后，我们利用了最近在新颖物体提案方面的进展，以改进开放词汇检测的微调。我们在LVIS和COCO开放词汇检测基准上评估了完整模型和零-shot转移性能。RO-ViT在LVIS上实现了32.1$AP_r$的最佳效果，超过现有最佳方法5.8个百分点，同时还具有竞争性的零-shot转移检测结果。

    We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a contrastive image-text pretraining recipe to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we propose to randomly crop and resize regions of positional embeddings instead of using the whole image positional embeddings. This better matches the use of positional embeddings at region-level in the detection finetuning phase. In addition, we replace the common softmax cross entropy loss in contrastive learning with focal loss to better learn the informative yet difficult examples. Finally, we leverage recent advances in novel object proposals to improve open-vocabulary detection finetuning. We evaluate our full model on the LVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer. RO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the best existing approach by +5.8 points in addition to competitive zero-shot transfer detec
    
[^98]: 一种用于近场不规则SAR超分辨率的视觉Transformer方法

    A Vision Transformer Approach for Efficient Near-Field Irregular SAR Super-Resolution. (arXiv:2305.02074v1 [cs.CV])

    [http://arxiv.org/abs/2305.02074](http://arxiv.org/abs/2305.02074)

    本文提出了一种新的、用于近场不规则SAR超分辨率的算法，以应对高分辨率成像中遇到的独特挑战，为实现边缘和物联网(IoT)技术奠定技术基础。

    

    本文提出了一种新颖的、针对非规则扫描几何的近场合成孔径雷达(SAR)超分辨率算法。随着第五代(5G)毫米波(mmWave)设备变得越来越实惠和可用，高分辨率SAR成像对用户应用和非实验室环境变得可行。新兴应用如手持成像、无人机成像和汽车SAR面临着高分辨率成像的几个独特挑战。首先，恢复SAR图像需要在整个扫描期间了解阵列位置。虽然最近的工作引入了基于相机的定位系统，能够足够地估计位置，但实现高效的恢复算法是实现边缘和物联网(IoT)技术的必要条件。最近的工作探讨了非合作近场SAR采样的高效算法，但是这些算法很大程度上是试验性的，需要更好的鲁棒性和实用性。

    In this paper, we develop a novel super-resolution algorithm for near-field synthetic-aperture radar (SAR) under irregular scanning geometries. As fifth-generation (5G) millimeter-wave (mmWave) devices are becoming increasingly affordable and available, high-resolution SAR imaging is feasible for end-user applications and non-laboratory environments. Emerging applications such freehand imaging, wherein a handheld radar is scanned throughout space by a user, unmanned aerial vehicle (UAV) imaging, and automotive SAR face several unique challenges for high-resolution imaging. First, recovering a SAR image requires knowledge of the array positions throughout the scan. While recent work has introduced camera-based positioning systems capable of adequately estimating the position, recovering the algorithm efficiently is a requirement to enable edge and Internet of Things (IoT) technologies. Efficient algorithms for non-cooperative near-field SAR sampling have been explored in recent work, bu
    
[^99]: 从像素中学习物体中心化的广义值函数

    Discovering Object-Centric Generalized Value Functions From Pixels. (arXiv:2304.13892v1 [cs.LG])

    [http://arxiv.org/abs/2304.13892](http://arxiv.org/abs/2304.13892)

    本文介绍了一种从像素中学习物体中心化的广义值函数的方法。该方法从物体中发现有意义的特征，转化为“问题”函数，并利用随后学习的广义值函数来进行控制，在静态和非静态设置下表现良好。学到的表示不仅是可解释的，而且围绕着具有不变性的物体，有助于快速适应。

    

    深度强化学习展现了从高维输入中提取有用表示的显著进展，尽管使用的是手工辅助任务和伪奖励。自动化地以物体为中心学习此类表示，以期实现控制和快速适应，仍然是一个未解决的研究问题。在本文中，我们介绍了一种方法，试图从物体中发现有意义的特征，将它们转化为时间上连贯的“问题”函数，并利用随后学习的广义值函数来进行控制。我们将我们的方法与最先进的技术进行比较，并展示了在静态和非静态设置下的竞争性表现。最后，我们还调查了被发现的广义值函数，并通过定性分析表明，学到的表示不仅是可解释的，而且围绕着物体，这些物体对任务的变化具有不变性，有助于快速适应。

    Deep Reinforcement Learning has shown significant progress in extracting useful representations from high-dimensional inputs albeit using hand-crafted auxiliary tasks and pseudo rewards. Automatically learning such representations in an object-centric manner geared towards control and fast adaptation remains an open research problem. In this paper, we introduce a method that tries to discover meaningful features from objects, translating them to temporally coherent "question" functions and leveraging the subsequent learned general value functions for control. We compare our approach with state-of-the-art techniques alongside other ablations and show competitive performance in both stationary and non-stationary settings. Finally, we also investigate the discovered general value functions and through qualitative analysis show that the learned representations are not only interpretable but also, centered around objects that are invariant to changes across tasks facilitating fast adaptatio
    
[^100]: EPVT: 基于环境感知的提示视觉Transformer在皮肤病变识别领域一般化中的应用

    EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition. (arXiv:2304.01508v1 [cs.CV])

    [http://arxiv.org/abs/2304.01508](http://arxiv.org/abs/2304.01508)

    EPVT是一种基于环境感知的提示视觉Transformer，用于解决皮肤病变识别中深度神经网络可能过度依赖疾病不相关图像特征的问题，通过嵌入一组领域提示和一个共享提示来进行领域一般化，并且引入了领域提示生成器促进知识共享。

    

    利用深度学习进行皮肤病变识别已取得重大进展，而在现实世界场景中部署这些系统的需求不断增加。然而，最近的研究表明，用于皮肤病变识别的深度神经网络可能过度依赖于与疾病不相关的图像特征（如暗角、浓密毛发），导致在看不见的环境中表现不佳。为了解决这个问题，我们提出了一种新颖的领域一般化方法——EPVT，它将提示嵌入到Vision Transformer中，以协同学习来自不同领域的知识。具体而言，EPVT利用一组领域提示，每个领域提示都扮演领域专家的角色，以捕获领域特定的知识；以及一个共享提示来获得整个数据集的通用知识。为了促进知识共享和不同提示之间的交互，我们引入了一个领域提示生成器，它使得领域提示与共享提示之间可以进行低秩乘性更新。

    Skin lesion recognition using deep learning has made remarkable progress, and there is an increasing need for deploying these systems in real-world scenarios. However, recent research has revealed that deep neural networks for skin lesion recognition may overly depend on disease-irrelevant image artifacts (i.e. dark corners, dense hairs), leading to poor generalization in unseen environments. To address this issue, we propose a novel domain generalization method called EPVT, which involves embedding prompts into the vision transformer to collaboratively learn knowledge from diverse domains. Concretely, EPVT leverages a set of domain prompts, each of which plays as a domain expert, to capture domain-specific knowledge; and a shared prompt for general knowledge over the entire dataset. To facilitate knowledge sharing and the interaction of different prompts, we introduce a domain prompt generator that enables low-rank multiplicative updates between domain prompts and the shared prompt. A
    
[^101]: SemEval-2023任务3上的mCPT：用于零样本和少样本框架检测的多语言标签感知对比预训练变压器

    mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])

    [http://arxiv.org/abs/2303.09901](http://arxiv.org/abs/2303.09901)

    本研究提出了mCPT模型用于多语言的、多标签的零样本或少样本的框架检测任务，并在西班牙语和其他8种语言中取得了良好的成绩。该方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。

    

    本文介绍了零样本的西班牙语框架检测任务的获胜系统，并在另外八种语言中取得了良好的成绩。框架检测任务的挑战在于在只有少量或零个样本的情况下识别一组14个框架，即多语言多标签的少样本和零样本设置。我们开发的解决方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。除了描述系统外，我们还进行了嵌入空间分析和消融研究，以展示我们的预训练程序如何支持框架检测以推进计算框架分析。

    This paper presents the winning system for the zero-shot Spanish framing detection task, which also achieves competitive places in eight additional languages. The challenge of the framing detection task lies in identifying a set of 14 frames when only a few or zero samples are available, i.e., a multilingual multi-label few- or zero-shot setting. Our developed solution employs a pre-training procedure based on multilingual Transformers using a label-aware contrastive loss function. In addition to describing the system, we perform an embedding space analysis and ablation study to demonstrate how our pre-training procedure supports framing detection to advance computational framing analysis.
    
[^102]: 审计大型语言模型：一个三层次的方法

    Auditing large language models: a three-layered approach. (arXiv:2302.08500v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.08500](http://arxiv.org/abs/2302.08500)

    本文提出了一个三层次的方法来审计大型语言模型（LLMs），包括治理审计、模型审计和应用审计，解决LLMs带来的伦理和社会挑战。

    

    大型语言模型（LLMs）是人工智能（AI）研究的一个重大突破。然而，LLMs的广泛使用也伴随着重大的伦理和社会挑战。先前的研究指出，审计作为一种有前途的治理机制，有助于确保AI系统设计和部署的道德、法律和技术的健壮性。然而，现有的审计程序无法解决LLMs带来的治理挑战，因为LLMs显示出新兴能力，并可适应各种下游任务。在本文中，我们通过概述一种新颖的审计LLMs的蓝图来填补这一空白。具体而言，我们提出了一个三层次的方法，即治理审计（针对设计和传播LLMs的技术提供商）、模型审计（针对LLMs进行预训练但尚未发布的审计）和应用审计（基于LLMs的应用程序的审计），相互补充和相互通知。我们展示了审计在LLMs上的实施可以有效解决伦理和社会挑战。

    Large language models (LLMs) represent a major advance in artificial intelligence (AI) research. However, the widespread use of LLMs is also coupled with significant ethical and social challenges. Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks. In this article, we address that gap by outlining a novel blueprint for how to audit LLMs. Specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate LLMs), model audits (of LLMs after pre-training but prior to their release), and application audits (of applications based on LLMs) complement and inform each other. We show how audits, when conducte
    
[^103]: GibbsDDRM:一种用于解决盲逆问题的局部折叠Gibbs采样器，带有去噪扩散恢复的先验。(arXiv:2301.12686v2 [cs.LG] 更新)

    GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration. (arXiv:2301.12686v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12686](http://arxiv.org/abs/2301.12686)

    GibbsDDRM是一种扩展的局部折叠Gibbs采样方法，用于解决线性逆问题中线性算子未知的盲场景。它利用预训练的扩散模型构建了数据、测量和线性算子的联合分布，并通过后验采样来解决问题。该方法在盲图像去模糊和语音去混响任务上表现出了高性能，而且适用于各种逆问题。

    

    在各种线性逆问题中，预训练扩散模型已成功用作先验，目标是从噪声线性测量中重构信号。然而，现有方法需要了解线性算子。在本文中，我们提出了GibbsDDRM，它是将Denoising Diffusion Restoration Models(DDRM)扩展到线性测量算子未知的盲场景。GibbsDDRM利用预训练的扩散模型构建了数据、测量和线性算子的联合分布，并通过采用一种高效的Gibbs采样器的变体进行后验采样来解决问题。所提出的方法是问题不可知的，意味着预训练的扩散模型可以应用于各种逆问题而无需微调。在实验证明，尽管使用了简单的通用先验来处理底层线性算子，该方法在盲图像去模糊和语音去混响任务上均取得了高性能。

    Pre-trained diffusion models have been successfully used as priors in a variety of linear inverse problems, where the goal is to reconstruct a signal from noisy linear measurements. However, existing approaches require knowledge of the linear operator. In this paper, we propose GibbsDDRM, an extension of Denoising Diffusion Restoration Models (DDRM) to a blind setting in which the linear measurement operator is unknown. GibbsDDRM constructs a joint distribution of the data, measurements, and linear operator by using a pre-trained diffusion model for the data prior, and it solves the problem by posterior sampling with an efficient variant of a Gibbs sampler. The proposed method is problem-agnostic, meaning that a pre-trained diffusion model can be applied to various inverse problems without fine-tuning. In experiments, it achieved high performance on both blind image deblurring and vocal dereverberation tasks, despite the use of simple generic priors for the underlying linear operators.
    
[^104]: 一种基于半监督感知率学习的CMAB方案，通过可信数据收集在人群中抗击COVID-19

    A Semi-supervised Sensing Rate Learning based CMAB Scheme to Combat COVID-19 by Trustful Data Collection in the Crowd. (arXiv:2301.08563v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2301.08563](http://arxiv.org/abs/2301.08563)

    本文提出了一种基于半监督学习的组合多臂赌博反向拍卖方案，用于解决移动众包系统中在招募多个未知和有策略的工作者时出现的数据可信问题。

    

    对于移动众包系统，招募可信和高质量的工作者是一个重要的研究问题。先前的研究要么假设工作者的能力是事先已知的，要么假设平台一旦接收到他们收集的数据就知道他们的能力。实际上，为了降低成本并最大程度地提高收入，许多有策略的工作者不诚实地执行其感知任务，并向平台报告虚假数据，这被称为虚假数据攻击。对于平台来说，评估所收到的数据的真实性十分困难。本文提出了一种名为基于半监督组合多臂赌博反向拍卖（SCMABA）的激励机制来解决MCS中多个未知和有策略的工作者的招聘问题。首先，我们将工作者招募建模为多臂赌博反向拍卖问题，并设计了一种基于UCB的算法来分离探索和开发，将已招募的工作者的感知率视为“臂“。

    The recruitment of trustworthy and high-quality workers is an important research issue for MCS. Previous studies either assume that the qualities of workers are known in advance, or assume that the platform knows the qualities of workers once it receives their collected data. In reality, to reduce costs and thus maximize revenue, many strategic workers do not perform their sensing tasks honestly and report fake data to the platform, which is called False data attacks. And it is very hard for the platform to evaluate the authenticity of the received data. In this paper, an incentive mechanism named Semi-supervision based Combinatorial Multi-Armed Bandit reverse Auction (SCMABA) is proposed to solve the recruitment problem of multiple unknown and strategic workers in MCS. First, we model the worker recruitment as a multi-armed bandit reverse auction problem and design an UCB-based algorithm to separate the exploration and exploitation, regarding the Sensing Rates (SRs) of recruited worke
    
[^105]: 在高维度中更快的最大内积搜索

    Faster Maximum Inner Product Search in High Dimensions. (arXiv:2212.07551v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07551](http://arxiv.org/abs/2212.07551)

    该论文提出了一种在高维度中更快的最大内积搜索算法 BanditMIPS，该算法的复杂度与维度无关，并且通过自适应采样策略提供理论保证和实验证明。

    

    最大内积搜索（MIPS）是机器学习应用中普遍存在的任务，如推荐系统。给定一个查询向量和d维空间中的n个原子向量，MIPS的目标是找到与查询向量具有最高内积的原子。现有的MIPS算法的复杂度至少为O(√d)，在高维设置下变得计算上禁止。在这项工作中，我们提出了BanditMIPS，一种新颖的随机MIPS算法，其复杂度与d无关。BanditMIPS通过子采样坐标来估计每个原子的内积，并适应性地评估更有前景的原子的坐标。具体的自适应采样策略受到了多臂老虎机问题的启发。我们提供理论保证，BanditMIPS以高概率返回正确的答案，同时将复杂度从O(√d)改进到O(1)。我们还在四个合成和实际数据集上进行了实验证明。

    Maximum Inner Product Search (MIPS) is a ubiquitous task in machine learning applications such as recommendation systems. Given a query vector and $n$ atom vectors in $d$-dimensional space, the goal of MIPS is to find the atom that has the highest inner product with the query vector. Existing MIPS algorithms scale at least as $O(\sqrt{d})$, which becomes computationally prohibitive in high-dimensional settings. In this work, we present BanditMIPS, a novel randomized MIPS algorithm whose complexity is independent of $d$. BanditMIPS estimates the inner product for each atom by subsampling coordinates and adaptively evaluates more coordinates for more promising atoms. The specific adaptive sampling strategy is motivated by multi-armed bandits. We provide theoretical guarantees that BanditMIPS returns the correct answer with high probability, while improving the complexity in $d$ from $O(\sqrt{d})$ to $O(1)$. We also perform experiments on four synthetic and real-world datasets and demonst
    
[^106]: 层次化分解和分析广义规划

    Hierarchical Decomposition and Analysis for Generalized Planning. (arXiv:2212.02823v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.02823](http://arxiv.org/abs/2212.02823)

    本文提出了一种分析和评估广义规划的新方法，通过层次化分解和使用图论推导出的终止论证，可以解决广义规划中的难题，并指导综合和学习过程。

    

    本文提出了一种分析和评估广义规划的新方法，该方法可以解决广泛的相关规划问题。虽然在人工智能领域，广义规划的综合和学习一直是一个长期的目标，但由于对给定广义规划的范围和效用进行分析的方法存在根本差距，这个目标仍然具有挑战性。本文通过开发一个新的概念框架以及证明技术和算法过程来解决这些差距，用于评估广义规划的终止和目标可达性相关属性。我们利用图论的经典结果将广义规划分解为较小的组件，然后利用这些组件推导出层次化的终止论证。这些方法可以用于确定给定广义规划的效用，以及指导广义规划的综合和学习过程。我们提供了理论和实证结果来说明这种新方法的适用范围。

    This paper presents new methods for analyzing and evaluating generalized plans that can solve broad classes of related planning problems. Although synthesis and learning of generalized plans has been a longstanding goal in AI, it remains challenging due to fundamental gaps in methods for analyzing the scope and utility of a given generalized plan. This paper addresses these gaps by developing a new conceptual framework along with proof techniques and algorithmic processes for assessing termination and goal-reachability related properties of generalized plans. We build upon classic results from graph theory to decompose generalized plans into smaller components that are then used to derive hierarchical termination arguments. These methods can be used to determine the utility of a given generalized plan, as well as to guide the synthesis and learning processes for generalized plans. We present theoretical as well as empirical results illustrating the scope of this new approach. Our analy
    
[^107]: 跨任务泛化的多头适配器路由

    Multi-Head Adapter Routing for Cross-Task Generalization. (arXiv:2211.03831v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.03831](http://arxiv.org/abs/2211.03831)

    本文研究了跨任务泛化中适配器路由的作用，并设计了基于此的新变体Multi-Head Routing (MHR)，通过精细的路由和参数高效的微调实现了更好的性能和更高的参数效率。

    

    跨任务泛化的参数高效微调(PEFT)在少样本任务适配之前通过在多任务训练集上预训练适配器来实现。Ponti等人的Polytropon [Ponti et al., 2023] ($\texttt{Poly}$)在预训练和少样本适配期间共同学习了一组适配器和选择每个任务的适配器子集的路由函数。在本文中，我们研究了适配器路由在其成功中的作用，并根据我们的发现设计了基于此的新变体。首先，我们建立在更精细的路由能提供更多表达性的直觉上。因此，我们提出了Multi-Head Routing (MHR)，它结合了适配器参数的子集，并在可比较的参数预算下表现更好；通过仅微调路由函数而不是适配器(MHR-z)，我们实现了与极高参数效率相媲美的性能。其次，我们发现Poly/MHR的交叉模型适配在少样本任务上可以实现更好的性能。

    Parameter-efficient fine-tuning (PEFT) for cross-task generalization consists in pre-training adapters on a multi-task training set before few-shot adaptation to test tasks. Polytropon [Ponti et al., 2023] ($\texttt{Poly}$) jointly learns an inventory of adapters and a routing function that selects a (variable-size) subset of adapters for each task during both pre-training and few-shot adaptation. In this paper, we investigate the role that adapter routing plays in its success and design new variants based on our findings. First, we build on the intuition that finer-grained routing provides more expressivity. Hence, we propose $\texttt{MHR}$ (Multi-Head Routing), which combines $\textit{subsets}$ of adapter parameters and outperforms $\texttt{Poly}$ under a comparable parameter budget; by only fine-tuning the routing function and not the adapters ($\texttt{MHR}$-$z$), we achieve competitive performance with extreme parameter efficiency. Second, we find that $\texttt{Poly}$/$\texttt{MHR
    
[^108]: 迭代自回归：提高低延迟语音增强模型的新技巧

    Iterative autoregression: a novel trick to improve your low-latency speech enhancement model. (arXiv:2211.01751v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.01751](http://arxiv.org/abs/2211.01751)

    本研究提出了一种简单而有效的替代技术，用于训练自回归低延迟语音增强模型，该方法在不同的架构和训练场景下均能带来稳定的改进。

    

    流式模型是实时语音增强工具的重要组成部分。流式模式限制语音增强模型仅能使用极少量未来信息作为上下文。因此，低延迟流式设置通常被视为一项具有挑战性的任务，这对模型的质量有着显著的负面影响。然而，流式生成的顺序性提供了自回归的自然可能性，即在进行当前预测时利用以前的预测。常规训练自回归模型的方法是教师强制，但其主要缺点在于训练-推理不匹配可能会导致大幅度的质量降级。在本研究中，我们提出了一种简单但有效的替代技术，用于训练自回归低延迟语音增强模型。我们证明了这种方法在不同的架构和训练场景下都能带来稳定的改进。

    Streaming models are an essential component of real-time speech enhancement tools. The streaming regime constrains speech enhancement models to use only a tiny context of future information. As a result, the low-latency streaming setup is generally considered a challenging task and has a significant negative impact on the model's quality. However, the sequential nature of streaming generation offers a natural possibility for autoregression, that is, utilizing previous predictions while making current ones. The conventional method for training autoregressive models is teacher forcing, but its primary drawback lies in the training-inference mismatch that can lead to a substantial degradation in quality. In this study, we propose a straightforward yet effective alternative technique for training autoregressive low-latency speech enhancement models. We demonstrate that the proposed approach leads to stable improvement across diverse architectures and training scenarios.
    
[^109]: 自主导航的强化学习技术基准测试

    Benchmarking Reinforcement Learning Techniques for Autonomous Navigation. (arXiv:2210.04839v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.04839](http://arxiv.org/abs/2210.04839)

    本研究针对自主导航应用深度强化学习方法提出了四个期望：不确定性推理、安全性、有限试错数据学习和对多样环境的泛化能力。

    

    深度强化学习（RL）为自主机器人导航带来了许多成功。然而，仍存在一些重要的限制，阻止了基于RL的导航系统在现实世界中的应用。例如，大多数学习方法缺乏安全保证；而学习到的导航系统可能不适用于未知环境。尽管最近出现了各种学习技术来应对这些挑战，但缺乏针对自主导航的开源基准测试和可重复学习方法，使得机器人学家难以选择用于其移动机器人的学习方法，学习研究人员也难以确定当前通用学习方法在自主导航方面的不足之处。在本文中，我们确定了应用深度RL方法进行自主导航的四个主要期望：（D1）不确定性推理、（D2）安全性、（D3）从有限的试错数据中学习、（D4）对多样和新颖环境的泛化能力。

    Deep reinforcement learning (RL) has brought many successes for autonomous robot navigation. However, there still exists important limitations that prevent real-world use of RL-based navigation systems. For example, most learning approaches lack safety guarantees; and learned navigation systems may not generalize well to unseen environments. Despite a variety of recent learning techniques to tackle these challenges in general, a lack of an open-source benchmark and reproducible learning methods specifically for autonomous navigation makes it difficult for roboticists to choose what learning methods to use for their mobile robots and for learning researchers to identify current shortcomings of general learning methods for autonomous navigation. In this paper, we identify four major desiderata of applying deep RL approaches for autonomous navigation: (D1) reasoning under uncertainty, (D2) safety, (D3) learning from limited trial-and-error data, and (D4) generalization to diverse and nove
    
[^110]: 实时强化学习用于基于视觉的机器人，利用本地和远程计算机

    Real-Time Reinforcement Learning for Vision-Based Robotics Utilizing Local and Remote Computers. (arXiv:2210.02317v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.02317](http://arxiv.org/abs/2210.02317)

    本文实现了一个名为ReLoD的实时学习系统，利用本地和远程计算机来分配深度强化学习算法的计算，并在基于视觉的控制任务上进行了评估。结果显示SAC的性能下降了。

    

    实时学习对于适应不断变化的非稳定环境的机器人代理至关重要。一个常见的机器人代理设置是同时有两台不同的计算机：与机器人相连的资源受限本地计算机和无线连接的强大远程计算机。在这样的设置下，尚不清楚学习系统的性能在资源限制方面能受到多大的影响，以及如何高效利用无线连接的强大计算机来弥补性能损失。在本文中，我们实现了一个实时学习系统，称为Remote-Local Distributed (ReLoD)系统，用于在本地和远程计算机之间分配两个深度强化学习算法Soft Actor-Critic (SAC)和Proximal Policy Optimization (PPO)的计算。该系统的性能在使用机械臂和移动机器人开发的两个基于视觉的控制任务上进行了评估。我们的结果表明SAC的性能下降了。

    Real-time learning is crucial for robotic agents adapting to ever-changing, non-stationary environments. A common setup for a robotic agent is to have two different computers simultaneously: a resource-limited local computer tethered to the robot and a powerful remote computer connected wirelessly. Given such a setup, it is unclear to what extent the performance of a learning system can be affected by resource limitations and how to efficiently use the wirelessly connected powerful computer to compensate for any performance loss. In this paper, we implement a real-time learning system called the Remote-Local Distributed (ReLoD) system to distribute computations of two deep reinforcement learning (RL) algorithms, Soft Actor-Critic (SAC) and Proximal Policy Optimization (PPO), between a local and a remote computer. The performance of the system is evaluated on two vision-based control tasks developed using a robotic arm and a mobile robot. Our results show that SAC's performance degrades
    
[^111]: 基于时间不一致的自监督探索在强化学习中的应用

    Self-Supervised Exploration via Temporal Inconsistency in Reinforcement Learning. (arXiv:2208.11361v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.11361](http://arxiv.org/abs/2208.11361)

    本文在强化学习中提出了一种新的内在奖励方法，通过比较当前观察与历史知识的差异来评估好奇心，并利用时间不一致性作为内在奖励。实验证明该方法在稀疏外在奖励的情况下具有更高的性能和噪声容忍度。

    

    在稀疏外在奖励的情况下，强化学习仍然具有挑战性，尽管对这个领域的兴趣不断增加。先前的尝试表明，内在奖励可以缓解稀疏性带来的问题。在本文中，我们提出了一种受人类学习启发的新型内在奖励，人类通过将当前观察与历史知识进行比较来评估好奇心。我们的方法涉及训练一个自监督预测模型，保存模型参数的快照，并使用核范数来评估不同快照之间预测的时间不一致性作为内在奖励。我们还提出了一种变分加权机制，以自适应的方式为不同的快照分配权重。我们在各种基准环境上的实验结果表明了我们方法的有效性，该方法在没有额外训练成本且具有更高噪声容忍度的情况下优于其他基于内在奖励的方法。

    Under sparse extrinsic reward settings, reinforcement learning has remained challenging, despite surging interests in this field. Previous attempts suggest that intrinsic reward can alleviate the issue caused by sparsity. In this article, we present a novel intrinsic reward that is inspired by human learning, as humans evaluate curiosity by comparing current observations with historical knowledge. Our method involves training a self-supervised prediction model, saving snapshots of the model parameters, and using nuclear norm to evaluate the temporal inconsistency between the predictions of different snapshots as intrinsic rewards. We also propose a variational weighting mechanism to assign weight to different snapshots in an adaptive manner. Our experimental results on various benchmark environments demonstrate the efficacy of our method, which outperforms other intrinsic reward-based methods without additional training costs and with higher noise tolerance. This work has been submitte
    
[^112]: 异构任务在机器学习驱动的高性能计算工作流中的异步执行

    Asynchronous Execution of Heterogeneous Tasks in ML-driven HPC Workflows. (arXiv:2208.11069v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2208.11069](http://arxiv.org/abs/2208.11069)

    该论文研究了机器学习驱动的高性能计算工作流中异步任务执行的要求和特性，并提出了用于确定异步执行收益的关键指标。通过在Summit上进行大规模实验，作者证明了异步执行对性能的增强与模型一致。

    

    异构科学工作流由许多类型的任务组成，这些任务需要在异构资源上执行。异步执行这些任务对于提高资源利用率、任务吞吐量和减少工作流的时间非常重要。因此，能够在异构资源上调度和执行不同类型任务的中间件必须支持异步执行任务。本文研究了机器学习驱动的高性能计算工作流的异步任务执行的要求和特性。我们模型化了任意工作流的异步性程度，并提出了可用于确定异步执行带来的定性收益的关键指标。我们的实验代表了相关科学驱动，我们在Summit上进行了大规模实验，并且我们展示了由于异步执行所带来的性能增强与我们的模型一致。

    Heterogeneous scientific workflows consist of numerous types of tasks that require executing on heterogeneous resources. Asynchronous execution of those tasks is crucial to improve resource utilization, task throughput and reduce workflows' makespan. Therefore, middleware capable of scheduling and executing different task types across heterogeneous resources must enable asynchronous execution of tasks. In this paper, we investigate the requirements and properties of the asynchronous task execution of machine learning (ML)-driven high performance computing (HPC) workflows. We model the degree of asynchronicity permitted for arbitrary workflows and propose key metrics that can be used to determine qualitative benefits when employing asynchronous execution. Our experiments represent relevant scientific drivers, we perform them at scale on Summit, and we show that the performance enhancements due to asynchronous execution are consistent with our model.
    
[^113]: 压缩型多语言机器翻译模型会忽略什么？

    What Do Compressed Multilingual Machine Translation Models Forget?. (arXiv:2205.10828v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.10828](http://arxiv.org/abs/2205.10828)

    本研究评估了压缩方法对多语言神经机器翻译模型在不同语言群体、性别和语义偏差方面的影响，并发现代表性不足的语言性能显著下降。

    

    最近，非常庞大的预训练模型在各种自然语言处理（NLP）任务中取得了最先进的结果，但是它们的大小使得在资源受限的环境中应用它们更具挑战性。压缩技术可以大幅减小模型的大小，从而减少推理时间，并对顶级指标几乎没有影响。然而，对多个任务和/或语言进行平均的综合性能可能掩盖了在代表性不足的功能上的严重性能下降，这可能导致模型所编码的偏见的放大。在这项工作中，我们通过对不同机器翻译基准（FLORES-101、MT-Gender和DiBiMT）上的压缩模型进行全面分析，评估了压缩方法对多语言神经机器翻译模型（MNMT）在不同语言群体、性别和语义偏差方面的影响。我们发现，代表性不足的语言的性能显著下降，而平均BLEU度量值则没什么变化。

    Recently, very large pre-trained models achieve state-of-the-art results in various natural language processing (NLP) tasks, but their size makes it more challenging to apply them in resource-constrained environments. Compression techniques allow to drastically reduce the size of the models and therefore their inference time with negligible impact on top-tier metrics. However, the general performance averaged across multiple tasks and/or languages may hide a drastic performance drop on under-represented features, which could result in the amplification of biases encoded by the models. In this work, we assess the impact of compression methods on Multilingual Neural Machine Translation models (MNMT) for various language groups, gender, and semantic biases by extensive analysis of compressed models on different machine translation benchmarks, i.e. FLORES-101, MT-Gender, and DiBiMT. We show that the performance of under-represented languages drops significantly, while the average BLEU metr
    
[^114]: 双线性价值网络

    Bilinear value networks. (arXiv:2204.13695v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2204.13695](http://arxiv.org/abs/2204.13695)

    提出了一种通过点积低秩近似来表示Q值的双线性分解方法，其中第一个向量场捕捉状态的局部动态，第二个部分捕捉当前状态和目标之间的全局关系，该方法能够显著提高数据效率，并具有很好的泛化性能。

    

    多目标强化学习的主流框架涉及估计目标条件下的Q值函数。当学习实现多个不同目标时，数据效率与Q函数对新目标的泛化密切相关。目前的范式是使用单一的神经网络来近似Q(s, a, g)。为了改进Q函数的泛化，我们提出了一种双线性分解方法，通过两个向量场之间的点积的低秩近似来表示Q值。第一个向量场f(s, a)捕捉状态s处的环境局部动态；而第二个部分{ϕ}(s, g)则捕捉当前状态和目标之间的全局关系。我们展示了双线性分解方案显著提高了数据效率，相比先前的方法，对于处于分布范围之外的目标具有更好的转移性。我们在模拟的Fetch机器人任务套件和DeepMind Control Suite上提供了实证证据。

    The dominant framework for off-policy multi-goal reinforcement learning involves estimating goal conditioned Q-value function. When learning to achieve multiple goals, data efficiency is intimately connected with the generalization of the Q-function to new goals. The de-facto paradigm is to approximate Q(s, a, g) using monolithic neural networks. To improve the generalization of the Q-function, we propose a bilinear decomposition that represents the Q-value via a low-rank approximation in the form of a dot product between two vector fields. The first vector field, f(s, a), captures the environment's local dynamics at the state s; whereas the second component, {\phi}(s, g), captures the global relationship between the current state and the goal. We show that our bilinear decomposition scheme substantially improves data efficiency, and has superior transfer to out-of-distribution goals compared to prior methods. Empirical evidence is provided on the simulated Fetch robot task-suite and d
    
[^115]: 拓扑经验回放

    Topological Experience Replay. (arXiv:2203.15845v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.15845](http://arxiv.org/abs/2203.15845)

    本文提出了一种拓扑经验回放的方法，通过构建图来明确状态的 Q 值之间的依赖关系，解决了传统采样策略忽视状态间依赖关系的问题，提高了学习深度 Q 函数时的性能和准确性。

    

    最先进的深度 Q 学习方法使用从经验重放缓冲区中采样的状态转换元组更新 Q 值。这种策略通常均匀和随机地采样，或基于诸如时间差（TD）误差等度量优先。这样的采样策略在学习 Q 函数时可能效率低下，因为一个状态的 Q 值取决于继承状态的 Q 值。如果数据采样策略忽略了下一个状态的 Q 值估计的精度，它可能会导致无用和常常不正确的 Q 值更新。为了减轻这个问题，我们将智能体的经验组织成一个图，明确跟踪状态的 Q 值之间的依赖关系。图中的每条边代表通过执行单个操作在两个状态之间的转换。我们通过从一组终端状态开始扩展图中的顶点，并逐步向后移动的广度优先搜索来执行值备份。

    State-of-the-art deep Q-learning methods update Q-values using state transition tuples sampled from the experience replay buffer. This strategy often uniformly and randomly samples or prioritizes data sampling based on measures such as the temporal difference (TD) error. Such sampling strategies can be inefficient at learning Q-function because a state's Q-value depends on the Q-value of successor states. If the data sampling strategy ignores the precision of the Q-value estimate of the next state, it can lead to useless and often incorrect updates to the Q-values. To mitigate this issue, we organize the agent's experience into a graph that explicitly tracks the dependency between Q-values of states. Each edge in the graph represents a transition between two states by executing a single action. We perform value backups via a breadth-first search starting from that expands vertices in the graph starting from the set of terminal states and successively moving backward. We empirically sho
    
[^116]: 用代理模型辅助的分布式群体优化方法来处理计算昂贵的地质科学模型

    Surrogate-assisted distributed swarm optimisation for computationally expensive geoscientific models. (arXiv:2201.06843v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2201.06843](http://arxiv.org/abs/2201.06843)

    本文利用代理模型辅助的分布式群体优化方法，解决了计算昂贵的地质科学模型优化问题，在基准优化问题和Badlands风貌演化模型中都取得了非常有希望的结果。

    

    进化算法提供无梯度优化方法，对于难以获取梯度的模型（如地质科学风貌演化模型）有益。但是，这些模型有时计算成本很高，即使是并行计算的分布式群体优化也很吃力。我们可以采用代理模型辅助优化等高效策略来解决这些挑战，但是实施代理模型的进程间通信来进行模型训练是困难的。在本文中，我们实现了分布式群体优化中代理模型评估适应值的方法，采用并行计算架构。我们首先将该框架应用于一组基准优化问题，并将其应用于具有风貌演化模型的地质科学模型。我们的结果对于基准函数和Badlands风貌演化模型都表现出非常有希望的结果。我们获得了计算时间的缩短

    Evolutionary algorithms provide gradient-free optimisation which is beneficial for models that have difficulty in obtaining gradients; for instance, geoscientific landscape evolution models. However, such models are at times computationally expensive and even distributed swarm-based optimisation with parallel computing struggles. We can incorporate efficient strategies such as surrogate-assisted optimisation to address the challenges; however, implementing inter-process communication for surrogate-based model training is difficult. In this paper, we implement surrogate-based estimation of fitness evaluation in distributed swarm optimisation over a parallel computing architecture. We first test the framework on a set of benchmark optimisation problems and then apply it to a geoscientific model that features a landscape evolution model. Our results demonstrate very promising results for benchmark functions and the Badlands landscape evolution model. We obtain a reduction in computational
    
[^117]: 可解释的、对话主题感知的神经语言理解

    Explainable and Discourse Topic-aware Neural Language Understanding. (arXiv:2006.10632v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2006.10632](http://arxiv.org/abs/2006.10632)

    该论文提出了一个新颖的神经复合语言模型，通过引入可解释性的主题表示和句子级的主题对话，将主题模型和语言模型相结合。实验结果表明，在多个任务上，该模型显示出了良好的性能。

    

    将主题模型与语言模型相结合，通过主题将语言理解扩展到句子之外的更广泛的文档级上下文。现有的方法在语言模型中引入了主题语义，但忽略了文档中句子的主题对话。本研究通过为每个潜在主题的比例相对应地引入可解释性主题表示来扩展研究领域。此外，我们通过为文档中的每个句子建模主题对话，保留句子-主题关联以及文档-主题关联。我们提出了一种新颖的神经复合语言模型，在主题模型和语言模型的联合学习框架中同时利用潜在主题和可解释主题以及句子级的主题对话。我们在多个任务上进行实验，如语言建模、词义消歧等。

    Marrying topic models and language models exposes language understanding to a broader source of document-level context beyond sentences via topics. While introducing topical semantics in language models, existing approaches incorporate latent document topic proportions and ignore topical discourse in sentences of the document. This work extends the line of research by additionally introducing an explainable topic representation in language understanding, obtained from a set of key terms correspondingly for each latent topic of the proportion. Moreover, we retain sentence-topic associations along with document-topic association by modeling topical discourse for every sentence in the document. We present a novel neural composite language model that exploits both the latent and explainable topics along with topical discourse at sentence-level in a joint learning framework of topic and language models. Experiments over a range of tasks such as language modeling, word sense disambiguation, 
    

