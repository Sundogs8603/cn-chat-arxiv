# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [2SFGL: A Simple And Robust Protocol For Graph-Based Fraud Detection.](http://arxiv.org/abs/2310.08335) | 本论文提出了一种简单而健壮的协议2SFGL，用于基于图的欺诈检测。该协议采用联邦学习的方式，通过虚拟融合多方图并在虚拟图上进行模型训练和推理，来解决因金融犯罪跨机构造成的数据隐私保护问题。 |
| [^2] | [Transport-Hub-Aware Spatial-Temporal Adaptive Graph Transformer for Traffic Flow Prediction.](http://arxiv.org/abs/2310.08328) | 提出了一种交通枢纽感知的时空自适应图转换器 (H-STFormer) 用于交通流量预测。该方法不仅有效建模了时空依赖关系，还充分利用了交通流量数据的固有属性，解决了增量学习和转化问题。 |
| [^3] | [CHIP: Contrastive Hierarchical Image Pretraining.](http://arxiv.org/abs/2310.08304) | 我们提出了一种基于对比损失的层次化分类模型，用于少样本目标分类任务。该模型可以将未见过的物体分类到相对通用的类别中，通过从图像嵌入中提取的特征进行分类。 |
| [^4] | [If our aim is to build morality into an artificial agent, how might we begin to go about doing so?.](http://arxiv.org/abs/2310.08295) | 本文讨论了将道德建立在人工智能代理中的方法，并就道德的含义提出了不同的看法。作者提出了自顶向下和自底向上的设计方法，并强调情感和感知在道德中的作用。此外，作者还提出了设计的混合方法和结合道德范式的层次化方法。对于人工智能伦理来说，治理和政策的重要性也得到了强调。 |
| [^5] | [Concealed Electronic Countermeasures of Radar Signal with Adversarial Examples.](http://arxiv.org/abs/2310.08292) | 本文提出了使用对抗样本进行雷达信号的隐蔽电子对抗措施，通过基于时频图像的攻击流程和具有高可转移性的攻击算法，以及基于STFT的时域信号攻击算法，成功实施了干扰信号的攻击。 |
| [^6] | [Expanding the Vocabulary of BERT for Knowledge Base Construction.](http://arxiv.org/abs/2310.08291) | 本论文介绍了一种扩展BERT词汇表的方法，以用于知识库构建任务。我们采用了特定任务的再预训练方法，通过添加新单词来扩展语言模型的词汇表，并保留了新添加单词的语义嵌入。 |
| [^7] | [CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large Language Models.](http://arxiv.org/abs/2310.08279) | CP-KGC方法利用大型语言模型，通过约束式提示来补全知识图谱，提高推断效果，展示了在低资源计算条件下的有效性，并在数据集上取得了优于之前方法的结果。 |
| [^8] | [Lag-Llama: Towards Foundation Models for Time Series Forecasting.](http://arxiv.org/abs/2310.08278) | Lag-Llama是一个基于大量时间序列数据训练的通用预测模型，在未见过的数据集上展现出强大的零样本预测能力，并使用光滑断裂幂律模型来拟合和预测扩展行为。 |
| [^9] | [Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval.](http://arxiv.org/abs/2310.08276) | 这篇论文提出了一种面向方向的视觉-语义嵌入模型（DOVE），通过区域导向的注意力模块和轻量级的文字基因辅助模块，解决了遥感图像-文本检索中的视觉-语义不平衡问题，提高了检索准确性。 |
| [^10] | [Impact of Co-occurrence on Factual Knowledge of Large Language Models.](http://arxiv.org/abs/2310.08256) | 大型语言模型在回答问题时常常出现事实错误，这主要是因为过度依赖预训练语料库的共现统计。研究结果表明，大型语言模型容易受到共现偏见的影响，导致难以回忆起预训练数据集中很少共现的事实。建议使用去偏数据集进行微调来减轻偏见，但这对于微调期间未见过的稀有事实的回忆效果并不明显。 |
| [^11] | [MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning.](http://arxiv.org/abs/2310.08252) | MetaBox是一种用于开发和评估元黑箱优化与强化学习方法的基准平台，提供灵活的算法模板、广泛的问题实例和基线方法，并引入了三个标准化的性能指标，以促进方法的严格评估。 |
| [^12] | [GROOT: Learning to Follow Instructions by Watching Gameplay Videos.](http://arxiv.org/abs/2310.08235) | GROOT是一个通过观看游戏视频学习遵循指令的控制器，它通过产生一个结构化的目标空间来消除昂贵的文本-游戏注释的需求，并在Minecraft SkillForge基准测试中展现出与人类玩家相当的表现水平。 |
| [^13] | [The Impact of Time Step Frequency on the Realism of Robotic Manipulation Simulation for Objects of Different Scales.](http://arxiv.org/abs/2310.08233) | 增加小尺度物体的时间步频可以提高机器人操作仿真准确性，为改进机器人装配过程中的仿真到真实环境转换提供了起点。 |
| [^14] | [SimCKP: Simple Contrastive Learning of Keyphrase Representations.](http://arxiv.org/abs/2310.08221) | SimCKP是一个简单的对比学习框架，通过学习上下文感知的短语级表示来提取关键词短语，并通过重新排序来调整生成的短语的分数。 |
| [^15] | [TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge Retention and Promotion.](http://arxiv.org/abs/2310.08217) | TriRE是一个新的持续学习范式，受到大脑如何利用多种机制进行学习的启发。它通过保持每个任务中最显著的神经元，修订和巩固这些神经元之间的联系，解决了深度神经网络中的灾难性遗忘问题。 |
| [^16] | [Trustworthy Machine Learning.](http://arxiv.org/abs/2310.08215) | 该论文讨论了当前机器学习技术面临的可信度问题，并提出了分布之外的泛化、可解释性、不确定性量化和可信度评估四个关键主题的解决方案。 |
| [^17] | [Long-Tailed Classification Based on Coarse-Grained Leading Forest and Multi-Center Loss.](http://arxiv.org/abs/2310.08206) | 本论文提出了一种基于粗粒度引导森林和多中心损失的长尾分类框架，名为Cognisance。该框架致力于解决长尾分类问题中的类间和类内不平衡，并通过不变特征学习构建多粒度联合解决模型。 |
| [^18] | [Beyond Traditional DoE: Deep Reinforcement Learning for Optimizing Experiments in Model Identification of Battery Dynamics.](http://arxiv.org/abs/2310.08198) | 本论文提出了一种基于深度强化学习的新颖DoE方法，用于优化电池动力学模型识别中的实验设计。该方法根据过去实验的统计信息动态地改变实验配置，从而提高了实验效率和准确性。 |
| [^19] | [EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation.](http://arxiv.org/abs/2310.08185) | 本文提出了EIPE-text方法用于长篇叙事文本生成，通过从语料库中提取计划并利用评估机制进行迭代改进，构建了更好的规划器。 |
| [^20] | [Learn From Model Beyond Fine-Tuning: A Survey.](http://arxiv.org/abs/2310.08184) | 这项研究以Learn From Model (LFM)为名，探索了超越微调的模型学习技术，旨在通过对模型接口进行研究和设计，将基于模型的学习推广到下游任务中。 |
| [^21] | [Multi-Scale Spatial-Temporal Recurrent Networks for Traffic Flow Prediction.](http://arxiv.org/abs/2310.08138) | 本论文提出了一种名为MSSTRN的多尺度时空循环网络，用于交通流预测。该网络通过两种不同的循环神经网络完全捕获了交通数据中不同时间窗口下的复杂时空信息，解决了现有方法中忽视图结构、捕捉不充分以及缺乏关注不同时间尺度上时空信息的问题。 |
| [^22] | [Can Large Language Models Really Improve by Self-critiquing Their Own Plans?.](http://arxiv.org/abs/2310.08118) | 本研究探究了大型语言模型在计划领域中的验证和自我批评能力。研究结果发现，自我批评似乎会降低计划生成的性能，并导致大量误报，降低了系统的可靠性。 |
| [^23] | [DUSA: Decoupled Unsupervised Sim2Real Adaptation for Vehicle-to-Everything Collaborative Perception.](http://arxiv.org/abs/2310.08117) | DUSA是一种解耦无监督的车联网协作感知的模拟到实际适应方法，旨在解决模拟数据与真实数据之间的领域差距以及协作代理之间的领域差距。通过这种方法，可以提高车联网协作感知在真实场景中的性能。 |
| [^24] | [Promptor: A Conversational and Autonomous Prompt Generation Agent for Intelligent Text Entry Techniques.](http://arxiv.org/abs/2310.08101) | 本论文介绍了Promptor，一个用于智能文本输入技术的对话式自主提示生成代理。利用大型语言模型的上下文学习能力，可以克服数据收集和模型微调的挑战。我们通过以GPT-3.5为例的实验证明，仅通过提示即可超过GPT-2支持的系统，并且可与经过精调的GPT-3.5模型相媲美。 |
| [^25] | [Sentinel: An Aggregation Function to Secure Decentralized Federated Learning.](http://arxiv.org/abs/2310.08097) | Sentinel是一种用于保护分散式联邦学习的防御策略，通过利用本地数据并定义一个三步聚合协议来对抗污染攻击。评估结果表明Sentinel在不同数据集和评估指标下表现良好。 |
| [^26] | [Discerning Temporal Difference Learning.](http://arxiv.org/abs/2310.08091) | 该论文提出了一种名为分辨TD学习(DTD)的新型TD算法，通过灵活的强调函数来分配资源，以改善状态之间的轻重分配，解决了传统TD学习中忽视历史状态重要性和TD误差传播的问题。实证结果表明，使用合理的强调函数不仅改进了值估计，还加速了学习过程。 |
| [^27] | [Low-Resource Clickbait Spoiling for Indonesian via Question Answering.](http://arxiv.org/abs/2310.08085) | 本论文介绍了一项以问答方式翻译印尼语低资源标题党处理的任务，贡献包括构建手动标注的印尼语标题党处理语料库，并评估了跨语言零射击问答模型的应用。实验结果表明，XLM-RoBERTa（大）模型在短语和段落标题党处理方面表现优异，而mDeBERTa（基础）模型在多部分标题党处理中表现最佳。 |
| [^28] | [GameGPT: Multi-agent Collaborative Framework for Game Development.](http://arxiv.org/abs/2310.08067) | GameGPT是一个多智能体协作框架，用于自动化游戏开发。它通过使用双重协作和分层方法，结合多个内部词库和解耦方法，解决了大型语言模型在游戏开发中的幻觉和冗余问题。 |
| [^29] | [The Search-and-Mix Paradigm in Approximate Nash Equilibrium Algorithms.](http://arxiv.org/abs/2310.08066) | 本研究提出了一种搜索和混合范式的自动方法，用于近似计算二人博弈中的纳什均衡。这种方法可以完全自动化设计和分析混合阶段，并且能够对已经存在的算法进行近似界限的分析，无需手写证明。 |
| [^30] | [Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation.](http://arxiv.org/abs/2310.08056) | 本文提出了一种从标签比例中学习的算法框架，通过伪标签化和嵌入细化两个步骤迭代地提高有监督学习器性能。 |
| [^31] | [Understanding and Controlling a Maze-Solving Policy Network.](http://arxiv.org/abs/2310.08043) | 本论文研究了一个预训练的强化学习策略网络，并发现该网络追求多个上下文相关的目标。通过修改特定的通道，我们可以部分地控制策略。这表明训练策略网络中存在冗余、分布式和可重新定向的目标表示。 |
| [^32] | [QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models.](http://arxiv.org/abs/2310.08041) | QLLM是一种为大规模语言模型设计的准确高效的低位宽后训练量化方法，通过引入自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。 |
| [^33] | [Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain Models.](http://arxiv.org/abs/2310.08039) | 本文提出了一种重新思考大规模预排序系统的方法，通过整体链路跨域模型和细粒度神经结构来解决样本选择偏差问题，并改进预排序的准确性。 |
| [^34] | [Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles.](http://arxiv.org/abs/2310.08034) | 本文提出了一种利用大型语言模型（LLM）改进自动驾驶车辆决策过程的框架，通过利用LLM的语言和推理能力，实现自动驾驶车辆与乘客的互动与个性化。 |
| [^35] | [Incorporating Domain Knowledge Graph into Multimodal Movie Genre Classification with Self-Supervised Attention and Contrastive Learning.](http://arxiv.org/abs/2310.08032) | 这项研究将领域知识图纳入多模态影片类型分类中，通过利用群组关系和改进注意力分配的方法，实现了更准确和可靠的分类结果。 |
| [^36] | [Beyond Sharing Weights in Decoupling Feature Learning Network for UAV RGB-Infrared Vehicle Re-Identification.](http://arxiv.org/abs/2310.08026) | 本论文提出了一个基于无人机的跨模态车辆再识别（Re-ID）的创新研究，旨在解决数据不足、跨模态差异和旋向差异的问题。提出了一个跨模态车辆Re-ID基准，并设计了一个混合权重分离网络（HWDNet）来学习共享的判别性旋向不变特征。 |
| [^37] | [Effects of Human Adversarial and Affable Samples on BERT Generalizability.](http://arxiv.org/abs/2310.08008) | 本研究研究了对BERT模型的广义性影响，发现在固定大小的训练样本上，有10-30\%的人为对抗实例可以显著提高文本分类和关系抽取任务的精度和F1值。 |
| [^38] | [A Novel Statistical Measure for Out-of-Distribution Detection in Data Quality Assurance.](http://arxiv.org/abs/2310.07998) | 本文使用深度学习技术和新的统计度量方法，旨在研究数据质量保证中的离群数据检测问题，并提出了一种能够有效区分正常数据和离群数据的特征表示方法。 |
| [^39] | [Point-NeuS: Point-Guided Neural Implicit Surface Reconstruction by Volume Rendering.](http://arxiv.org/abs/2310.07997) | 该论文提出了一种名为Point-NeuS的新方法，通过利用点导向机制实现准确和高效的神经隐式曲面重建，解决了当前方法精度有限和时间复杂度过高的问题。 |
| [^40] | [HeightFormer: A Multilevel Interaction and Image-adaptive Classification-regression Network for Monocular Height Estimation with Aerial Images.](http://arxiv.org/abs/2310.07995) | 本文提出了一种用于航空影像的全面解决方案，通过多层交互和图像自适应分类回归网络实现单目高度估计，解决了固定接受域和缺乏全局信息交互的问题。 |
| [^41] | [Large Language Models for Scientific Synthesis, Inference and Explanation.](http://arxiv.org/abs/2310.07984) | 大型语言模型通过综合科学文献产生知识，能够在科学综合、推理和解释中发挥作用。 |
| [^42] | [Self-supervised visual learning for analyzing firearms trafficking activities on the Web.](http://arxiv.org/abs/2310.07975) | 本论文提出了一种自监督视觉学习方法，用于分析网络上的枪支走私活动。这种方法利用深度神经网络和卷积神经网络，通过从大规模通用数据集预训练，再在特定数据集上进行微调，实现对枪支的分类。 |
| [^43] | [Interpretable Diffusion via Information Decomposition.](http://arxiv.org/abs/2310.07972) | 本研究通过观察扩散和信息分解之间的关系，揭示了扩散模型学习到的细粒度关系，进一步解决了高维空间中信息携带变量的问题。 |
| [^44] | [A New Approach Towards Autoformalization.](http://arxiv.org/abs/2310.07957) | 该论文提出了一种新的方法来应对研究水平的数学自动形式化任务，通过将任务分解成更容易处理的子任务，包括未链接形式化、实体链接和类型调整。此外，还提出了一个用于未链接形式化的基准数据集 arXiv2Formal。 |
| [^45] | [AutoRepo: A general framework for multi-modal LLM-based automated construction reporting.](http://arxiv.org/abs/2310.07944) | AutoRepo是一个通用框架，利用无人驾驶飞行器和多模态大型语言模型来自动生成建筑检查报告，提高了检查效率和资源利用率。 |
| [^46] | [Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models.](http://arxiv.org/abs/2310.07937) | Co-NavGPT是一个创新的框架，使用大型语言模型作为全局规划器，实现多机器人合作的视觉目标导航。在实验中表现出了超越现有模型的成功率和效率，展示了大型语言模型的巨大潜力。 |
| [^47] | [What Matters to You? Towards Visual Representation Alignment for Robot Learning.](http://arxiv.org/abs/2310.07932) | 该论文提出了一种名为RAPL的方法，用于解决机器人学习中的视觉表示对齐问题，通过利用人类反馈将机器人的视觉表示与用户偏好对齐，并区分任务的关键要素。 |
| [^48] | [D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning.](http://arxiv.org/abs/2310.07931) | D2修剪是一种平衡数据多样性和困难度的方法，在coreset选择中同时考虑数据多样性和重要性评分。 |
| [^49] | [Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning.](http://arxiv.org/abs/2310.07918) | 本论文提出了一种上下文化政策恢复方法用于建模复杂的医疗决策过程，以解决现有模型在准确性和可解释性之间的权衡问题。该方法将决策策略拆分为上下文特定策略，通过多任务学习来实现建模，并提供复杂行为的简洁描述。 |
| [^50] | [A Review of Machine Learning Techniques in Imbalanced Data and Future Trends.](http://arxiv.org/abs/2310.07917) | 该论文综述了在非平衡数据中使用的各种机器学习方法，并提供了一个通用指南，旨在帮助研究人员在大规模非平衡数据中进行机器学习。 |
| [^51] | [Recurrent networks recognize patterns with low-dimensional oscillations.](http://arxiv.org/abs/2310.07908) | 本研究提出了一种通过相位变化识别模式的循环神经网络机制，并通过验证手工制作的振荡模型证实了这一解释。该研究不仅提供了一种潜在的动力学机制用于模式识别，还暗示了有限状态自动机的神经实现方式，并且对深度学习模型的可解释性进行了贡献。 |
| [^52] | [RoboCLIP: One Demonstration is Enough to Learn Robot Policies.](http://arxiv.org/abs/2310.07899) | RoboCLIP是一种在线模仿学习方法，通过单个视频演示或文本描述生成奖励函数，不需要专家监督或设计奖励函数。同时，它还可以利用领域外的演示进行训练。 |
| [^53] | [Efficient Integrators for Diffusion Generative Models.](http://arxiv.org/abs/2310.07894) | 本文提出了共轭积分器和分裂积分器两种框架，用于加速预训练模型中的样本生成。经过实证和理论研究，提出了一种最佳性能的混合方法，能够应用于扩散生成模型。 |
| [^54] | [LangNav: Language as a Perceptual Representation for Navigation.](http://arxiv.org/abs/2310.07889) | 该论文探索了将语言作为导航的感知表示，并使用现成的视觉系统将每个时间步骤的视图转化为自然语言描述，通过微调预训练的语言模型选择最佳的行动来满足导航指令。有两种用例的实验对这种基于语言的导航方法进行了探索：使用大型语言模型生成合成轨迹以微调较小的语言模型，以及模拟到实际的转换。 |
| [^55] | [Leader-Follower Neural Networks with Local Error Signals Inspired by Complex Collectives.](http://arxiv.org/abs/2310.07885) | 领导者-跟随者神经网络是受到自然群体集合中观察到的规则启发的一种神经网络架构，利用局部误差信号训练并可选择性地结合反向传播和全局损失。通过大量实验研究工作人员行为和评估，这种神经网络具有高度的自组织和复杂性。 |
| [^56] | [The Thousand Faces of Explainable AI Along the Machine Learning Life Cycle: Industrial Reality and Current State of Research.](http://arxiv.org/abs/2310.07882) | 本文调查了可解释人工智能（XAI）在生产行业中的实际相关性，并将其与当前学术界XAI研究的现状相联系。通过对广泛的访谈和文献回顾的分析，发现了访谈结果与当前的研究方法之间存在差异。 |
| [^57] | [DeePref: Deep Reinforcement Learning For Video Prefetching In Content Delivery Networks.](http://arxiv.org/abs/2310.07881) | 本论文介绍了DeePref，一种用于在内容交付网络中进行在线视频内容预取的深度强化学习代理。传统的预取技术难以适应工作负载中的变化，而DeePref利用强化学习算法，可以自动适应不断变化的用户访问模式。 |
| [^58] | [TabLib: A Dataset of 627M Tables with Context.](http://arxiv.org/abs/2310.07875) | TabLib是一个包含上亿表格和上百亿上下文的数据集，规模和多样性使其在表格模态下具有巨大的潜力。 |
| [^59] | [Hierarchical Pretraining on Multimodal Electronic Health Records.](http://arxiv.org/abs/2310.07871) | 该论文提出了一种针对层次多模态电子健康记录数据的新颖、通用且统一的预训练框架MEDHMP，在三个级别上展示了其有效性，并且在与十八个基准方法的比较中验证了其高效性。 |
| [^60] | [Cheap Talking Algorithms.](http://arxiv.org/abs/2310.07867) | 该论文研究了在战略信息传递游戏中，利用独立强化学习算法进行训练的发送者和接收者可以收敛到接近最优均衡策略，并且在代理之间的利益冲突下实现了最大化的通信。这一结论稳健，并对信息传递游戏中的均衡选择理论、计算机科学中的算法间通信和人工智能代理市场中的经济学产生了影响。 |
| [^61] | [Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations.](http://arxiv.org/abs/2310.07849) | 本研究旨在探讨使用大型语言模型生成合成数据在文本分类模型训练中的潜力和限制。研究结果发现，主观性会负面影响模型在合成数据上的性能。这对于理解和利用合成数据的有效性具有重要的启示作用。 |
| [^62] | [Towards the Fundamental Limits of Knowledge Transfer over Finite Domains.](http://arxiv.org/abs/2310.07838) | 本论文研究了在有限领域中从教师到学生分类器进行知识传递的统计效率，发现特权信息会加速传递，通过使用一种新颖的损失函数达到了知识传递的基本限制。 |
| [^63] | [When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement.](http://arxiv.org/abs/2310.07831) | 该论文介绍了一种通过细化分析学习率调度来解决实践中学习率调整与理论的不一致的方法，通过对观察到的梯度范数进行分析，得到了适应于特定任务的细化调度。该方法能够改善优化算法的收敛性能。 |
| [^64] | [Does Synthetic Data Make Large Language Models More Efficient?.](http://arxiv.org/abs/2310.07830) | 本文探讨了在NLP中合成数据生成的细微差别，重点关注基于模板的问题生成。通过评估其优势和固有限制，本研究揭示了合成数据对现代Transformer模型性能的影响，并强调了合成数据与真实世界数据之间所需的微妙平衡。 |
| [^65] | [Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models.](http://arxiv.org/abs/2310.07818) | 这项研究探究了大型语言模型中识别句子类比的能力与其编码句法和语义结构能力之间的关系。 |
| [^66] | [Automatic Identification of Stone-Handling Behaviour in Japanese Macaques Using LabGym Artificial Intelligence.](http://arxiv.org/abs/2310.07812) | 这项研究使用LabGym人工智能开发了一个模型，能够准确地识别日本猕猴的石头处理行为，为灵长类学领域的行为分析提供了新的解决方案。 |
| [^67] | [Generative Modeling with Phase Stochastic Bridges.](http://arxiv.org/abs/2310.07805) | 通过在相位空间中构建路径测度，我们提出了一种新颖的生成建模框架，可以在动力传播的早期阶段生成逼真的数据点，并利用额外的速度信息实现高效的数据生成。 |
| [^68] | [A general mechanism of humor: reformulating the semantic overlap.](http://arxiv.org/abs/2310.07803) | 本文提出了一种普适性的认知幽默机制，建立在约束的概念上，通过重叠约束的观察来解释幽默的产生。 |
| [^69] | [An Information Bottleneck Characterization of the Understanding-Workload Tradeoff.](http://arxiv.org/abs/2310.07802) | 本论文利用信息瓶颈方法对工作负荷和理解之间的平衡进行了特征化，通过抽象化来解释复杂概念，以实现工作负荷和理解之间的权衡。 |
| [^70] | [Trajectory-aware Principal Manifold Framework for Data Augmentation and Image Generation.](http://arxiv.org/abs/2310.07801) | 本文提出了一种基于轨迹感知的主要流形框架，能够在数据增强和图像生成任务中提取更紧凑的流形表示，并实现少样本图像生成。 |
| [^71] | [Explainable Attention for Few-shot Learning and Beyond.](http://arxiv.org/abs/2310.07800) | 本研究介绍了一种用于少样本学习的解释性注意力框架，旨在通过识别输入数据的关键部分来提高模型性能。 |
| [^72] | [A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets.](http://arxiv.org/abs/2310.07799) | 本研究提出了一种基于迁移学习的EMR数据集之间数据分布变化的预后预测模型，通过构建过渡模型解决了不同数据集的特征不匹配问题，提高了深度学习模型的效率。 |
| [^73] | [GenTKG: Generative Forecasting on Temporal Knowledge Graph.](http://arxiv.org/abs/2310.07793) | 研究提出了一种名为GenTKG的生成模型，用于在时间知识图谱上进行预测。该模型通过结合基于时间逻辑规则的检索策略和轻量级的参数效率指导，克服了复杂的时间图数据结构和庞大的数据量所带来的挑战。 |
| [^74] | [DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model.](http://arxiv.org/abs/2310.07771) | DrivingDiffusion是一个基于3D布局的多视角驾驶场景视频生成框架，通过解决跨视角和跨帧的一致性问题，以及保证生成实例质量，实现了逼真的多视角视频生成。 |
| [^75] | [Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples.](http://arxiv.org/abs/2310.07747) | 本论文介绍了一种可解释的离线控制器方法，通过使用离线数据集作为决策语料库，在低数据场景中实现了问责制的控制，并在医疗保健领域展示了良好的性能。 |
| [^76] | [AI Algorithm for the Generation of Three-Dimensional Accessibility Ramps in Grasshopper / Rhinoceros 7.](http://arxiv.org/abs/2310.07728) | 本论文介绍了一种使用AI算法在Grasshopper / Rhinoceros 7中生成三维无障碍坡道的方法，通过基于环境的3D模型，自动确定最佳路径来提高无障碍坡道设计的效率。 |
| [^77] | [Towards the Vulnerability of Watermarking Artificial Intelligence Generated Content.](http://arxiv.org/abs/2310.07726) | 该研究探讨了将水印技术应用于人工智能生成内容的漏洞，并证明了现有的水印机制容易被对手破解。 |
| [^78] | [Visual Forecasting as a Mid-level Representation for Avoidance.](http://arxiv.org/abs/2310.07724) | 本研究提出了一种创新的替代方法——视觉预测，通过引入直观的视觉提示，投射动态物体的未来轨迹，提高智能体的感知能力并实现预期的动作。研究结果验证了视觉预测作为导航问题解决方案的可行性。 |
| [^79] | [Rethinking the BERT-like Pretraining for DNA Sequences.](http://arxiv.org/abs/2310.07644) | 重新考虑了基于DNA序列的BERT-like预训练方法，通过使用K-mer重叠标记化，在下游任务的微调阶段和预训练过程中都取得了一致的性能改善。 |
| [^80] | [OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models.](http://arxiv.org/abs/2310.07637) | OpsEval是一个全面任务导向的AIOps基准测试，评估了大型语言模型在有线网络操作、5G通信操作和数据库操作等关键场景下的能力水平，为提供针对AIOps定制的LLMs的优化方向。 |
| [^81] | [In-Context Unlearning: Language Models as Few Shot Unlearners.](http://arxiv.org/abs/2310.07579) | 这项工作提出了一种新的LLM消除方法，称为“基于上下文的消除”，它提供了上下文的输入且无需更新模型参数。这种方法解决了消除对于很大模型来说在计算上的困难，并在实践中具有更高的可行性和便捷性。 |
| [^82] | [Multimodal Graph Learning for Generative Tasks.](http://arxiv.org/abs/2310.07478) | 多模态图学习是一种通用且系统的框架，可以捕捉多模态数据之间的复杂关系，并在生成任务中取得了良好的效果。 |
| [^83] | [Imitation Learning from Observation with Automatic Discount Scheduling.](http://arxiv.org/abs/2310.07433) | 我们提出了一个新颖的观察学习模仿(ILfO)框架，能够解决由于奖励信号分配错误导致代理无法学习初始行为的问题。 |
| [^84] | [NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining.](http://arxiv.org/abs/2310.07402) | 本研究通过采用Transformer架构和数值多尺度嵌入模块，使时间序列自监督模型能够扩展到大规模数据集，并在大规模数据集上进行预训练。 |
| [^85] | [WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models.](http://arxiv.org/abs/2310.07312) | WiGenAI通过引入扩散模型，将生成式人工智能应用于无线通信系统中，为研究奠定基础。这篇文章介绍了扩散模型作为生成模型的最新范式，并讨论了它在无线通信系统中的应用。通过两个案例研究展示了扩散模型在开发韧性的AI本地通信系统中的潜力。 |
| [^86] | [An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT.](http://arxiv.org/abs/2310.07282) | 本研究分析了在医疗保健领域应用大型语言模型（尤其是BioBERT）的可行性，并提出了针对医疗保健领域的微调方法。研究突出了BioBERT对于解决与生物医学文本挖掘相关任务的特定要求的适用性。 |
| [^87] | [NECO: NEural Collapse Based Out-of-distribution detection.](http://arxiv.org/abs/2310.06823) | NECO是一种基于神经坍塌的新颖的超出分布检测方法，利用几何属性和主成分空间识别OOD数据，在小规模和大规模任务上取得了最先进的结果，并展示了强大的泛化能力。 |
| [^88] | [FABind: Fast and Accurate Protein-Ligand Binding.](http://arxiv.org/abs/2310.06763) | FABind是一个结合了口袋预测和对接的端到端模型，旨在实现快速准确的蛋白-配体结合预测。 |
| [^89] | [GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models.](http://arxiv.org/abs/2310.06225) | 本研究全面评估了大型语言模型（LLMs）在回答农业相关问题方面的能力，并使用RAG和ER技术提高了LLMs的性能。其中，GPT-4在农业考试中取得了及格分数以获得认证。 |
| [^90] | [Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning Approach.](http://arxiv.org/abs/2310.05969) | 提出了一种基于多模型深度学习的自动化胸部X光报告生成器系统，通过利用多个二元分类模型检测多种异常，在单个图像中辅助放射科医生的工作。该系统将放射学异常检测限制为心脏肥大、肺积液和实变，并通过三个步骤生成放射学报告：图像预处理、深度学习模型检测异常和生成报告。 |
| [^91] | [Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts.](http://arxiv.org/abs/2310.05898) | Lion是通过程序搜索发现的新优化器，在训练大型AI模型方面表现出有希望的结果，具有更高的内存效率。尽管其理论基础不明确，但基于连续时间和离散时间分析，我们证明Lion是一种理论上新颖且有原则的方法，可在最小化一般损失函数的同时强制执行边界约束。 |
| [^92] | [Automated Argument Generation from Legal Facts.](http://arxiv.org/abs/2310.05680) | 该论文研究了利用开源大型语言模型的生成能力，从法律案件中自动生成论证。实验结果显示，最佳方法生成的论证与基准集的黄金标准注释平均重叠率为63%。 |
| [^93] | [A new economic and financial theory of money.](http://arxiv.org/abs/2310.04986) | 这篇论文通过根本性的改革，将电子货币纳入经济与金融理论，提出了一种新的理论框架，包括电子货币的估值基于宏观经济理论和货币政策的基本方程，以及电子货币管理公司作为协调次经济体货币和财政政策的实体。该研究避免使用普遍但不适当的指数风险模型，而是采用多时间尺度的模型。 |
| [^94] | [Pairwise GUI Dataset Construction Between Android Phones and Tablets.](http://arxiv.org/abs/2310.04755) | 本文介绍了一个开创性的成对GUI数据集，旨在解决Android手机和平板之间自动化GUI开发的障碍。数据集包括10035个手机-平板GUI页面对，为提高开发人员的生产力提供了基础。 |
| [^95] | [DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies.](http://arxiv.org/abs/2310.04610) | DeepSpeed4Science计划旨在通过先进的AI系统技术加速科学发现，提供针对独特复杂性的解决方案，为自然科学带来重大进展。 |
| [^96] | [Effective Slogan Generation with Noise Perturbation.](http://arxiv.org/abs/2310.04472) | 本研究引入了基于噪声扰动的新方法，利用预训练的transformer T5模型生成独特且连贯的口号，同时将公司和品牌的描述纳入到生成过程中。结果表明，该方法在口号生成方面取得了良好的效果。 |
| [^97] | [Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets.](http://arxiv.org/abs/2310.04413) | 该论文提出了一种解决离线强化学习中不平衡数据集问题的方法，通过采样策略使策略只受``好数据"限制，而不是所有的动作，提高了离线RL算法的性能。 |
| [^98] | [Axiomatic Aggregations of Abductive Explanations.](http://arxiv.org/abs/2310.03131) | 本文提出了通过将多种可能的归纳解释汇总为特征重要性分数的三种聚合方法，以解决存在多个有效的归纳解释的问题。 |
| [^99] | [Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks.](http://arxiv.org/abs/2310.02772) | 本研究提出了一种名为脉冲累积转发（SAF）的方法，可以有效训练脉冲神经网络（SNNs）。SAF不仅可以减少前向过程中的操作次数，与Spike Representation和OTTT保持一致，而且可以解决SNNs训练中的难题。 |
| [^100] | [On Training Derivative-Constrained Neural Networks.](http://arxiv.org/abs/2310.01649) | 该论文研究了导数约束神经网络（DC NN），提出了一种整合RELU激活函数（IReLU）来改进DC NN的训练，并探究了去归一化和标签缩放对于稳定DC训练的作用。研究还评估了这些方法在物理相关设置中的效果，证明了使用IReLU激活函数、去归一化和标签缩放的现有架构更好地融入了导数约束提供的训练信号。 |
| [^101] | [GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models.](http://arxiv.org/abs/2310.00737) | 这篇论文探讨了生成式人工智能和大型语言模型的潜在滥用，呼吁认识到这些挑战的紧迫性。研究揭示了这些技术在深度伪造、合成身份恶意活动以及虚假信息和欺诈方面可能带来的社会影响。 |
| [^102] | [LEGO-Prover: Neural Theorem Proving with Growing Libraries.](http://arxiv.org/abs/2310.00656) | LEGO-Prover是一个使用不断增长的技能库的神经定理证明方法，可以使语言模型（LLMs）利用现有技能和创造新技能来证明定理。 |
| [^103] | [Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models.](http://arxiv.org/abs/2309.17319) | 这篇论文讨论了地理空间人工智能基础模型的隐私和安全风险，并提出了预防和控制策略蓝图。 |
| [^104] | [PRiSM: Enhancing Low-Resource Document-Level Relation Extraction with Relation-Aware Score Calibration.](http://arxiv.org/abs/2309.13869) | PRiSM是一种增强低资源文档级关系抽取的方法，通过关系感知分数校准来提高模型性能，成功地降低了在低资源环境下训练模型时的校准误差。 |
| [^105] | [MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback.](http://arxiv.org/abs/2309.10691) | MINT是一个评估LLMs在多轮交互中解决任务能力的基准，通过使用工具和利用用户的自然语言反馈。它解决了当前评估协议忽略细致互动和低估自然语言反馈的问题，促进了研究基准评估和实际应用之间的一致性。 |
| [^106] | [On Regularized Sparse Logistic Regression.](http://arxiv.org/abs/2309.05925) | 本文提出了解决正则稀疏逻辑回归的方法，包括$\ell_1$正则化稀疏逻辑回归和一些满足先决条件的非凸惩罚正则化稀疏逻辑回归。经验实验表明，这些算法能够以较低的计算成本有效地进行分类和特征选择。 |
| [^107] | [DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning.](http://arxiv.org/abs/2309.05173) | DePT通过将软提示分解为较短的软提示和一对低秩矩阵，并用两个不同的学习率来优化，以解决提示调整对训练和推理时间以及内存使用的影响，从而实现更好的性能。 |
| [^108] | [Pure Monte Carlo Counterfactual Regret Minimization.](http://arxiv.org/abs/2309.03084) | 纯蒙特卡洛反事实遗憾最小化算法（PCFR）是一种结合了反事实遗憾最小化（CFR）和虚拟游戏（FP）概念的新算法，能够与各种CFR变体相结合，包括蒙特卡洛CFR（MCCFR）。PCFR具有更好的性能和较快的收敛速度，同时降低了时间和空间复杂度。 |
| [^109] | [Learning-based NLOS Detection and Uncertainty Prediction of GNSS Observations with Transformer-Enhanced LSTM Network.](http://arxiv.org/abs/2309.00480) | 本研究提出了一种基于深度学习的方法，通过构建Transformer-like注意机制增强LSTM网络，检测GNSS观测中的NLOS接收并预测GNSS伪距误差，从而提高了车辆定位的精度和系统的可靠性。 |
| [^110] | [Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2308.16198) | 本论文介绍了一种使用多智能体强化学习的方法来实现协作信息传播。通过提出分布式POMDP形式，在消息转发上实现了每个智能体的独立决策，相比传统的基于多点中继选择的启发式方法具有重大创新和贡献。同时，该方法利用图卷积强化学习和动态注意力机制捕捉关键网络特征，并提出了不同信息交换方式的两种方法进行评估。 |
| [^111] | [Spread Control Method on Unknown Networks Based on Hierarchical Reinforcement Learning.](http://arxiv.org/abs/2308.14311) | 本论文提出了一种基于分层强化学习的方法，用于在未知网络上进行流行病控制。模拟结果表明，该方法优于基线方法。 |
| [^112] | [QKSAN: A Quantum Kernel Self-Attention Network.](http://arxiv.org/abs/2308.13422) | QKSAN是一个量子核自注意力网络，通过结合量子核方法和自注意力机制，提高了量子机器学习模型在大规模高维量子数据上的有效性。 |
| [^113] | [Multi-Objective Optimization for Sparse Deep Neural Network Training.](http://arxiv.org/abs/2308.12243) | 这项工作提出了一种用于稀疏深度神经网络训练的多目标优化算法，通过加权Chebyshev标量化和增广Lagrangian方法，解决了同时优化多个任务时的挑战。 |
| [^114] | [Large Language Model as a User Simulator.](http://arxiv.org/abs/2308.11534) | 本文创新性地将从真实人机对话中提取的人类问题作为学习目标，并且训练了一个用户模拟器UserGPT，并使用生成的高质量合成对话数据集RealChat来训练助手模型ReaLM。实验证明，ReaLM在多个基准测试中超过了基准模型。 |
| [^115] | [Expert load matters: operating networks at high accuracy and low manual effort.](http://arxiv.org/abs/2308.05035) | 在关键应用领域的人机协同系统中，为了确保最小错误，需要根据模型置信度设定操作点进行决策委托给专家。为了提高系统效用，需要考虑模型仅对准确样本具有自信的特点以及尽量减少委托给专家的样本数量。我们提出了置信度操作特性（COC）曲线来表示模型准确性与专家处理样本数量的权衡关系。这对于医疗保健等可用专家时间有限且昂贵的领域尤为关键。 |
| [^116] | [Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning.](http://arxiv.org/abs/2307.03486) | 通过对比学习方法，我们在强化学习中提出了新的成就蒸馏方法，可以加强代理对下一个解锁成就的预测能力，并优于先前的模型驱动和层次化方法。 |
| [^117] | [Distilling Large Vision-Language Model with Out-of-Distribution Generalizability.](http://arxiv.org/abs/2307.03135) | 本文研究了针对大型视觉语言模型的模型压缩方法，将教师模型的视觉表示压缩到学生模型中。研究重点在于超出分布可泛化的问题，并提出了两个原则来增强学生模型的性能。 |
| [^118] | [Elastic Decision Transformer.](http://arxiv.org/abs/2307.02484) | 弹性决策变压器（EDT）通过在测试时间进行动作推断时调整历史长度来实现轨迹拼接，填补了决策变压器（DT）在这一方面的性能差距，并且在多任务情况下胜过基于Q-Learning的方法。 |
| [^119] | [GIO: Gradient Information Optimization for Training Dataset Selection.](http://arxiv.org/abs/2306.11670) | GIO是一种可伸缩且任务不可知的方法，通过对目标的简单放松和高效实现，可以在非常小的训练集上产生出色的结果。 |
| [^120] | [Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information.](http://arxiv.org/abs/2306.08762) | 本论文研究了具有部分在线状态信息的POMDP问题的理论困难性和可计算性。作者通过建立下界得出一个惊人的难度结果：除非具有完整的在线状态信息，否则需要指数级的样本复杂度才能得到POMDP的最优策略解。然而，作者还发现了具有部分在线状态信息下的可计算POMDP类别，并提出了新的算法来证明其接近最优性。 |
| [^121] | [UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion Model.](http://arxiv.org/abs/2306.00721) | 本文介绍了无条件扩散模型UnDiff，通过无监督学习解决了语音逆问题。该模型可以适应不同的语音处理任务，并在带宽扩展、去剪辑、编码和语音源分离等任务上表现良好。 |
| [^122] | [Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery.](http://arxiv.org/abs/2305.14259) | 本文介绍了一种基于文献的发现方法，通过上下文化的学习生成新的科学方向，克服了标准方法在预测关联、忽略上下文等方面的局限性。模型使用了引文和知识图关系的网络，并使用大型语言模型进行评估，发现GPT4在生成创新思想方面表现出色。 |
| [^123] | [Satisfiability-Aided Language Models Using Declarative Prompting.](http://arxiv.org/abs/2305.09656) | 本文提出了一种利用自动定理证明器和声明性任务规范的可满足性辅助语言建模方法，可以提高大型语言模型的推理能力。 |
| [^124] | [Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation.](http://arxiv.org/abs/2305.07375) | 本文对ChatGPT的因果推理能力进行了首次全面评估，实验证明ChatGPT是一个好的因果解释者，但不是一个好的因果推理者，存在严重的因果幻觉问题，对于明确的因果关系表现良好。 |
| [^125] | [Diffusion-based Generative AI for Exploring Transition States from 2D Molecular Graphs.](http://arxiv.org/abs/2304.12233) | 本文提出了一种基于扩散的生成式方法(TSDiff)，用于从二维分子图中预测过渡态几何结构。与现有具有 3D 几何结构机器学习模型相比，TSdiff 的准确性和效率都更高。TSDiff 能够找到比参考数据库更优化的反应途径，在反应势垒更低的情况下找到更为优化的反应路径。 |
| [^126] | [Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models.](http://arxiv.org/abs/2304.11657) | 本文提出 Iter-CoT 方法，在大型语言模型中进行迭代增强的思维链提示，通过选择具有适度难度的具有挑战性但可回答的问题，并伴随推理链作为示例，提高了模型的泛化能力，同时使模型能够更准确地生成推理链。 |
| [^127] | [LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models.](http://arxiv.org/abs/2304.00457) | LLMMaps是一种分层评估大型语言模型性能的可视化技术，能够揭示取得高准确度和产生幻觉的子领域，并指导模型的进一步发展。 |
| [^128] | [Analyzing And Editing Inner Mechanisms Of Backdoored Language Models.](http://arxiv.org/abs/2302.12461) | 本研究分析并编辑暗藏后门的语言模型的内部机制，发现早期层的MLP模块和初始嵌入投影是后门机制中最重要的部分。通过使用PCP消融技术替换变压器模块，我们成功删除、插入和修改后门机制，并显著改善了后门的输出效果。 |
| [^129] | [Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?.](http://arxiv.org/abs/2302.11713) | 本研究介绍了一个专门针对无法仅凭常识知识回答的信息寻求问题而设计的视觉问答数据集InfoSeek。使用InfoSeek数据集，我们发现目前最先进的预训练多模态模型在回答求知视觉问题方面面临挑战，但在该数据集上进行微调可以激发模型使用细粒度知识。 |
| [^130] | [Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability.](http://arxiv.org/abs/2302.03770) | 本文提供了关于离线目标条件强化学习算法的理论分析，证明了经过轻微修改后，该算法在满足通用函数逼近时具有 O(poly(1/ε)) 的样本复杂度。 |
| [^131] | [IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing.](http://arxiv.org/abs/2301.13359) | 该论文提出了一个IM-IAD工业制造中的图像异常检测基准，通过统一的设置评估了16个算法在7个数据集上的性能，旨在解决目前这一领域研究的不规范问题。 |
| [^132] | [A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback.](http://arxiv.org/abs/2301.13326) | 这个论文介绍了一个框架，用于将离线算法调整为只需要赌徒反馈的亚线性α-后悔方法来解决组合多臂赌博问题，该框架可以实现对于时间界T的预期累积α-后悔依赖度为O(T^（2/3）log（T^（1/3））)。 |
| [^133] | [Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning.](http://arxiv.org/abs/2301.10886) | 本文提出了一种名为AIRS的自动内在奖励塑造探索方法，可以提供高质量的内在激励以增强强化学习中的探索性能；并开发了高效可靠的内在奖励工具包。实验表明，AIRS性能卓越，能够胜过基准方案。 |
| [^134] | [Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing.](http://arxiv.org/abs/2210.15889) | 神经符号计算是将符号和统计认知范式整合的研究领域，在推理和解释性以及神经网络学习等方面具有潜力。本文综述了该领域的最新进展和重要贡献，并探讨了其成功应用案例和未来发展方向。 |
| [^135] | [The Role of Morphological Variation in Evolutionary Robotics: Maximizing Performance and Robustness.](http://arxiv.org/abs/2208.02809) | 本文提出了一种测量形态变异影响的方法，并分析了变异幅度和引入方式与演化体性能和鲁棒性之间的关系。 |
| [^136] | [MemSAC: Memory Augmented Sample Consistency for Large Scale Unsupervised Domain Adaptation.](http://arxiv.org/abs/2207.12389) | MemSAC提出了一种记忆增强样本一致性方法，通过利用源域和目标域之间的样本级相似性实现判别转移，并且在大规模数据集上表现出明显的优势。 |
| [^137] | [mGPT: Few-Shot Learners Go Multilingual.](http://arxiv.org/abs/2204.07580) | 本文介绍了两种自回归GPT样式模型，分别使用13亿和130亿个参数，在60种语言中训练，并展示了与Facebook最近发布的XGLM模型性能相当的结果。这为低资源语言的自然语言处理提供了更多可能性。 |
| [^138] | [A Machine Learning Paradigm for Studying Pictorial Realism: Are Constable's Clouds More Real than His Contemporaries?.](http://arxiv.org/abs/2202.09348) | 本研究提出了一种用于研究图画现实主义的新的机器学习范式，通过测量艺术家绘制的云和云的照片之间的相似度来评估写实主义。实验结果表明，康斯特布尔比他的同时代更加一致地呈现形式上的特征。 |
| [^139] | [Federated Learning from Small Datasets.](http://arxiv.org/abs/2110.03469) | 本文介绍了一种在小数据集上实现联合学习的新方法，通过将模型聚合与本地模型的置换相结合，可以更高效地在数据稀疏的领域进行训练。 |

# 详细

[^1]: 2SFGL：一种用于基于图的欺诈检测的简单而健壮的协议

    2SFGL: A Simple And Robust Protocol For Graph-Based Fraud Detection. (arXiv:2310.08335v1 [cs.CR])

    [http://arxiv.org/abs/2310.08335](http://arxiv.org/abs/2310.08335)

    本论文提出了一种简单而健壮的协议2SFGL，用于基于图的欺诈检测。该协议采用联邦学习的方式，通过虚拟融合多方图并在虚拟图上进行模型训练和推理，来解决因金融犯罪跨机构造成的数据隐私保护问题。

    

    使用图学习进行金融犯罪检测可以提高金融安全和效率。然而，犯罪分子可能在不同机构之间进行金融犯罪，以避免被发现，这增加了金融机构使用本地数据进行图学习的难度。由于大多数金融机构在数据隐私保护方面受到严格的监管，训练数据通常是孤立的，传统的学习技术无法解决这个问题。联邦学习（FL）允许多个机构在不将数据集展示给其他机构的情况下训练模型，从而确保数据隐私保护。在本文中，我们提出了一种新颖的联邦图学习（2SFGL）方法：2SFGL的第一阶段涉及多方图的虚拟融合，第二阶段涉及在虚拟图上进行模型训练和推理。我们在传统欺诈检测任务FraudAmazonDataset上评估了我们的框架。

    Financial crime detection using graph learning improves financial safety and efficiency. However, criminals may commit financial crimes across different institutions to avoid detection, which increases the difficulty of detection for financial institutions which use local data for graph learning. As most financial institutions are subject to strict regulations in regards to data privacy protection, the training data is often isolated and conventional learning technology cannot handle the problem. Federated learning (FL) allows multiple institutions to train a model without revealing their datasets to each other, hence ensuring data privacy protection. In this paper, we proposes a novel two-stage approach to federated graph learning (2SFGL): The first stage of 2SFGL involves the virtual fusion of multiparty graphs, and the second involves model training and inference on the virtual graph. We evaluate our framework on a conventional fraud detection task based on the FraudAmazonDataset an
    
[^2]: 交通流量预测的交通枢纽感知时空自适应图转换器

    Transport-Hub-Aware Spatial-Temporal Adaptive Graph Transformer for Traffic Flow Prediction. (arXiv:2310.08328v1 [cs.AI])

    [http://arxiv.org/abs/2310.08328](http://arxiv.org/abs/2310.08328)

    提出了一种交通枢纽感知的时空自适应图转换器 (H-STFormer) 用于交通流量预测。该方法不仅有效建模了时空依赖关系，还充分利用了交通流量数据的固有属性，解决了增量学习和转化问题。

    

    作为智能交通系统（ITS）的核心技术，交通流量预测具有广泛的应用。交通流量数据是时空数据，不仅与道路网络中的空间位置相关，而且随时间变化。现有方法在一定程度上解决了交通流量预测中的挑战，重点是有效建模时空依赖关系，但并未充分利用交通流量数据的所有固有属性。此外，对于时空数据挖掘的增量学习几乎没有尝试，以前的工作也很难转化到交通流量预测任务中。受到交通流量预测的增量学习方法挑战和道路网络固有属性的潜力未被充分利用的启发，我们提出了一种针对交通流量预测的交通枢纽感知时空自适应图转换器（H-STFormer）。具体而言，我们首先设计了一种新颖的空间自注意机制

    As a core technology of Intelligent Transportation System (ITS), traffic flow prediction has a wide range of applications. Traffic flow data are spatial-temporal, which are not only correlated to spatial locations in road networks, but also vary with temporal time indices. Existing methods have solved the challenges in traffic flow prediction partly, focusing on modeling spatial-temporal dependencies effectively, while not all intrinsic properties of traffic flow data are utilized fully. Besides, there are very few attempts at incremental learning of spatial-temporal data mining, and few previous works can be easily transferred to the traffic flow prediction task. Motivated by the challenge of incremental learning methods for traffic flow prediction and the underutilization of intrinsic properties of road networks, we propose a Transport-Hub-aware Spatial-Temporal adaptive graph transFormer (H-STFormer) for traffic flow prediction. Specifically, we first design a novel spatial self-att
    
[^3]: CHIP：对比层次化图像预训练

    CHIP: Contrastive Hierarchical Image Pretraining. (arXiv:2310.08304v1 [cs.CV])

    [http://arxiv.org/abs/2310.08304](http://arxiv.org/abs/2310.08304)

    我们提出了一种基于对比损失的层次化分类模型，用于少样本目标分类任务。该模型可以将未见过的物体分类到相对通用的类别中，通过从图像嵌入中提取的特征进行分类。

    

    少样本目标分类是在有限数量的样本作为监督时对图像中的物体进行分类的任务。我们提出了一种一次/少次分类模型，可以将任何未见过的类别的物体分类到相对通用的类别中，基于层次化的分类。我们的模型使用了基于三级层次对比损失的ResNet152分类器，根据从图像嵌入中提取的特征对物体进行分类，而这些特征在训练阶段未被使用。我们的实验使用了ImageNet（ILSVRC-12）数据集的一个子集，该子集仅包含动物类别，用于训练我们的模型，并创建了我们自己的未见类别数据集，用于评估我们训练的模型。我们的模型在将未知物体分类为一个通用类别方面提供了令人满意的结果，这在后文中有更详细的讨论。

    Few-shot object classification is the task of classifying objects in an image with limited number of examples as supervision. We propose a one-shot/few-shot classification model that can classify an object of any unseen class into a relatively general category in an hierarchically based classification. Our model uses a three-level hierarchical contrastive loss based ResNet152 classifier for classifying an object based on its features extracted from Image embedding, not used during the training phase. For our experimentation, we have used a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal classes for training our model and created our own dataset of unseen classes for evaluating our trained model. Our model provides satisfactory results in classifying the unknown objects into a generic category which has been later discussed in greater detail.
    
[^4]: 如果我们的目标是向人工智能中构建道德，我们该如何着手？

    If our aim is to build morality into an artificial agent, how might we begin to go about doing so?. (arXiv:2310.08295v1 [cs.AI])

    [http://arxiv.org/abs/2310.08295](http://arxiv.org/abs/2310.08295)

    本文讨论了将道德建立在人工智能代理中的方法，并就道德的含义提出了不同的看法。作者提出了自顶向下和自底向上的设计方法，并强调情感和感知在道德中的作用。此外，作者还提出了设计的混合方法和结合道德范式的层次化方法。对于人工智能伦理来说，治理和政策的重要性也得到了强调。

    

    随着人工智能在各个领域的普及，从医疗保健到自动驾驶，构建道德的机器变得至关重要，特别是对于决策。然而，道德的含义在人工智能的背景下仍然存在争议。在本文中，我们强调了构建道德机器时应考虑的不同方面，包括最相关的道德范式和挑战。我们还讨论了自顶向下和自底向上的设计方法以及情感和感知在道德中的作用。然后，我们提出了解决方案，包括设计的混合方法和结合道德范式的层次化方法。我们强调治理和政策在人工智能伦理和确保我们给道德机器设置的任务可行性、实现道德行为和获得良好人工智能方面的重要性。

    As Artificial Intelligence (AI) becomes pervasive in most fields, from healthcare to autonomous driving, it is essential that we find successful ways of building morality into our machines, especially for decision-making. However, the question of what it means to be moral is still debated, particularly in the context of AI. In this paper, we highlight the different aspects that should be considered when building moral agents, including the most relevant moral paradigms and challenges. We also discuss the top-down and bottom-up approaches to design and the role of emotion and sentience in morality. We then propose solutions including a hybrid approach to design and a hierarchical approach to combining moral paradigms. We emphasize how governance and policy are becoming ever more critical in AI Ethics and in ensuring that the tasks we set for moral agents are attainable, that ethical behavior is achieved, and that we obtain good AI.
    
[^5]: 使用对抗样本进行雷达信号的隐蔽电子对抗措施

    Concealed Electronic Countermeasures of Radar Signal with Adversarial Examples. (arXiv:2310.08292v1 [eess.SP])

    [http://arxiv.org/abs/2310.08292](http://arxiv.org/abs/2310.08292)

    本文提出了使用对抗样本进行雷达信号的隐蔽电子对抗措施，通过基于时频图像的攻击流程和具有高可转移性的攻击算法，以及基于STFT的时域信号攻击算法，成功实施了干扰信号的攻击。

    

    电子对抗技术是现代战争中的重要方面，传统的电子对抗技术往往通过加入大规模的干扰信号来达到干扰效果，但这样往往太过显眼。近年来，出现了基于人工智能的攻击方法，可以有效解决这个问题，但目前攻击场景仅限于时间域雷达信号分类。本文针对雷达信号的时频图像分类场景，首先提出了一个基于时频图像的攻击流程和具有高可转移性的DITIMI-FGSM攻击算法。然后，我们提出了基于STFT的时域信号攻击(STDS)算法来解决时频分析中的不可逆性问题，从而得到干扰信号的时域表示。大量实验表明，我们的攻击流程是可行的，所提出的攻击方法可以成功实施。

    Electronic countermeasures involving radar signals are an important aspect of modern warfare. Traditional electronic countermeasures techniques typically add large-scale interference signals to ensure interference effects, which can lead to attacks being too obvious. In recent years, AI-based attack methods have emerged that can effectively solve this problem, but the attack scenarios are currently limited to time domain radar signal classification. In this paper, we focus on the time-frequency images classification scenario of radar signals. We first propose an attack pipeline under the time-frequency images scenario and DITIMI-FGSM attack algorithm with high transferability. Then, we propose STFT-based time domain signal attack(STDS) algorithm to solve the problem of non-invertibility in time-frequency analysis, thus obtaining the time-domain representation of the interference signal. A large number of experiments show that our attack pipeline is feasible and the proposed attack meth
    
[^6]: 扩展BERT词汇表以用于知识库构建

    Expanding the Vocabulary of BERT for Knowledge Base Construction. (arXiv:2310.08291v1 [cs.CL])

    [http://arxiv.org/abs/2310.08291](http://arxiv.org/abs/2310.08291)

    本论文介绍了一种扩展BERT词汇表的方法，以用于知识库构建任务。我们采用了特定任务的再预训练方法，通过添加新单词来扩展语言模型的词汇表，并保留了新添加单词的语义嵌入。

    

    知识库构建涉及获取结构化信息以创建一个包含事实和关系数据的知识库，以便于问答、信息检索和语义理解。国际语义网会议2023年的名为“来自预训练语言模型的知识库构建”挑战定义了专注于使用语言模型构建知识库的任务。我们关注挑战的第一轨道，其中参数受限于10亿，并且禁止在提示中包含实体描述。尽管遮蔽语言模型提供了足够的灵活性来扩展其词汇表，但它并非专门设计用于多令牌预测。为了解决这个问题，我们提出了适用于知识库构建的可扩展词汇BERT，它在扩展语言模型的词汇表的同时，保留了新添加单词的语义嵌入。我们采用了特定任务的再预训练方法，针对遮蔽语言模型进行重新训练。

    Knowledge base construction entails acquiring structured information to create a knowledge base of factual and relational data, facilitating question answering, information retrieval, and semantic understanding. The challenge called "Knowledge Base Construction from Pretrained Language Models" at International Semantic Web Conference 2023 defines tasks focused on constructing knowledge base using language model. Our focus was on Track 1 of the challenge, where the parameters are constrained to a maximum of 1 billion, and the inclusion of entity descriptions within the prompt is prohibited.  Although the masked language model offers sufficient flexibility to extend its vocabulary, it is not inherently designed for multi-token prediction. To address this, we present Vocabulary Expandable BERT for knowledge base construction, which expand the language model's vocabulary while preserving semantic embeddings for newly added words. We adopt task-specific re-pre-training on masked language mo
    
[^7]: CP-KGC: 利用大型语言模型的约束式提示对知识图谱进行补全

    CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large Language Models. (arXiv:2310.08279v1 [cs.CL])

    [http://arxiv.org/abs/2310.08279](http://arxiv.org/abs/2310.08279)

    CP-KGC方法利用大型语言模型，通过约束式提示来补全知识图谱，提高推断效果，展示了在低资源计算条件下的有效性，并在数据集上取得了优于之前方法的结果。

    

    知识图谱补全旨在利用现有知识推断和推测知识图谱中缺失的连接。SimKGC等基于文本的方法已经超过了图嵌入方法，展示了归纳式知识图谱补全的潜力。然而，基于文本的方法的效果取决于实体文本描述的质量。为了减轻LLM生成的文本中的幻觉，在本文中，我们引入了一种基于约束的提示方法，利用实体及其文本描述作为上下文约束来提高数据质量。我们的约束式提示知识图谱补全方法（CP-KGC）在低资源计算条件下表现出有效的推断能力，并超过了WN18RR和FB15K237数据集上的之前结果。这展示了LLMs在知识图谱补全任务中的整合，并为未来的研究提供了新的方向。

    Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce and infer missing connections within knowledge graphs. Text-based approaches, like SimKGC, have outperformed graph embedding methods, showcasing the promise of inductive KGC. However, the efficacy of text-based methods hinges on the quality of entity textual descriptions. In this paper, we identify the key issue of whether large language models (LLMs) can generate effective text. To mitigate hallucination in LLM-generated text in this paper, we introduce a constraint-based prompt that utilizes the entity and its textual description as contextual constraints to enhance data quality. Our Constrained-Prompt Knowledge Graph Completion (CP-KGC) method demonstrates effective inference under low resource computing conditions and surpasses prior results on the WN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC tasks and provides new directions for future research.
    
[^8]: Lag-Llama: 用于时间序列预测的基础模型

    Lag-Llama: Towards Foundation Models for Time Series Forecasting. (arXiv:2310.08278v1 [cs.LG])

    [http://arxiv.org/abs/2310.08278](http://arxiv.org/abs/2310.08278)

    Lag-Llama是一个基于大量时间序列数据训练的通用预测模型，在未见过的数据集上展现出强大的零样本预测能力，并使用光滑断裂幂律模型来拟合和预测扩展行为。

    

    为了构建时间序列预测的基础模型并研究其扩展行为，我们在这里介绍了我们正在进行中的 Lag-Llama 工作，这是一个在大量时间序列数据上训练的通用单变量概率时间序列预测模型。该模型在未见过的“分布外”时间序列数据集上展现出良好的零样本预测能力，优于有监督基线方法。我们使用光滑断裂幂律来拟合和预测模型的扩展行为。开源代码可在 https://github.com/kashif/pytorch-transformer-ts 上获得。

    Aiming to build foundation models for time-series forecasting and study their scaling behavior, we present here our work-in-progress on Lag-Llama, a general-purpose univariate probabilistic time-series forecasting model trained on a large collection of time-series data. The model shows good zero-shot prediction capabilities on unseen "out-of-distribution" time-series datasets, outperforming supervised baselines. We use smoothly broken power-laws to fit and predict model scaling behavior. The open source code is made available at https://github.com/kashif/pytorch-transformer-ts.
    
[^9]: 面向方向的视觉-语义嵌入模型在遥感图像-文本检索中的应用

    Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval. (arXiv:2310.08276v1 [cs.CV])

    [http://arxiv.org/abs/2310.08276](http://arxiv.org/abs/2310.08276)

    这篇论文提出了一种面向方向的视觉-语义嵌入模型（DOVE），通过区域导向的注意力模块和轻量级的文字基因辅助模块，解决了遥感图像-文本检索中的视觉-语义不平衡问题，提高了检索准确性。

    

    图像-文本检索在近年来得到了快速发展，然而在遥感领域仍然存在着视觉-语义不平衡的挑战，这导致了非语义视觉和文本特征的错误匹配。为了解决这个问题，我们提出了一种新颖的面向方向的视觉-语义嵌入模型（DOVE），来挖掘视觉和语言之间的关系。具体而言，通过区域导向的注意力模块（ROAM），在潜在的语义空间中，根据区域视觉特征自适应地调整最终的视觉和文本嵌入之间的距离。同时，设计了一个轻量级的文字基因辅助模块（DTGA），用较少的注意力操作来扩展可处理的文本表示范围，增强全局词级语义连接。最后，我们利用全局视觉-语义约束来减少单一视觉依赖，并为最终的视觉和文本表示提供外部约束。

    Image-text retrieval has developed rapidly in recent years. However, it is still a challenge in remote sensing due to visual-semantic imbalance, which leads to incorrect matching of non-semantic visual and textual features. To solve this problem, we propose a novel Direction-Oriented Visual-semantic Embedding Model (DOVE) to mine the relationship between vision and language. Concretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the distance between the final visual and textual embeddings in the latent semantic space, oriented by regional visual features. Meanwhile, a lightweight Digging Text Genome Assistant (DTGA) is designed to expand the range of tractable textual representation and enhance global word-level semantic connections using less attention operations. Ultimately, we exploit a global visual-semantic constraint to reduce single visual dependency and serve as an external constraint for the final visual and textual representations. The effectiveness and su
    
[^10]: 大型语言模型中共现对事实知识的影响

    Impact of Co-occurrence on Factual Knowledge of Large Language Models. (arXiv:2310.08256v1 [cs.CL])

    [http://arxiv.org/abs/2310.08256](http://arxiv.org/abs/2310.08256)

    大型语言模型在回答问题时常常出现事实错误，这主要是因为过度依赖预训练语料库的共现统计。研究结果表明，大型语言模型容易受到共现偏见的影响，导致难以回忆起预训练数据集中很少共现的事实。建议使用去偏数据集进行微调来减轻偏见，但这对于微调期间未见过的稀有事实的回忆效果并不明显。

    

    尽管大型语言模型在各种应用中取得了成功，但它们经常在事实上做出错误的回答。本文假设过度依赖于预训练语料库的简单共现统计是导致事实错误的主要因素之一。我们的结果表明，大型语言模型容易受到共现偏见的影响，即更倾向于选择频繁共现的词而不是正确答案。因此，尽管在微调期间已经见过这些事实的主题和对象在预训练数据集中很少共现，大型语言模型仍然难以回忆起这些事实。我们展示了即使扩大模型规模或进行微调，共现偏见仍然存在。因此，我们建议在去偏数据集上进行微调，通过过滤掉主题-对象共现计数高的偏见样本来减轻偏见。尽管去偏微调允许大型语言模型记忆训练集中的稀有事实，但在微调期间未见过的稀有事实的回忆效果并不明显。

    Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further resear
    
[^11]: MetaBox：一种用于元黑箱优化与强化学习的基准平台

    MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning. (arXiv:2310.08252v1 [cs.LG])

    [http://arxiv.org/abs/2310.08252](http://arxiv.org/abs/2310.08252)

    MetaBox是一种用于开发和评估元黑箱优化与强化学习方法的基准平台，提供灵活的算法模板、广泛的问题实例和基线方法，并引入了三个标准化的性能指标，以促进方法的严格评估。

    

    最近，元黑箱优化与强化学习（MetaBBO-RL）展示了在元级别上利用强化学习来减少对低级黑箱优化器的手动微调的能力。然而，这个领域由于缺乏统一的基准而受到阻碍。为了填补这个空白，我们介绍了MetaBox，这是一个专门为开发和评估MetaBBO-RL方法而设计的第一个基准平台。MetaBox提供了一个灵活的算法模板，让用户可以轻松地在平台内实现自己的独特设计。此外，它提供了超过300个问题实例，从合成到真实场景的广泛范围，并且包含了19种基线方法的详尽库，包括传统黑箱优化器和最近的MetaBBO-RL方法。此外，MetaBox引入了三个标准化的性能指标，使方法的评估更加全面。

    Recently, Meta-Black-Box Optimization with Reinforcement Learning (MetaBBO-RL) has showcased the power of leveraging RL at the meta-level to mitigate manual fine-tuning of low-level black-box optimizers. However, this field is hindered by the lack of a unified benchmark. To fill this gap, we introduce MetaBox, the first benchmark platform expressly tailored for developing and evaluating MetaBBO-RL methods. MetaBox offers a flexible algorithmic template that allows users to effortlessly implement their unique designs within the platform. Moreover, it provides a broad spectrum of over 300 problem instances, collected from synthetic to realistic scenarios, and an extensive library of 19 baseline methods, including both traditional black-box optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three standardized performance metrics, enabling a more thorough assessment of the methods. In a bid to illustrate the utility of MetaBox for facilitating rigorous evaluation and in-
    
[^12]: GROOT: 通过观看游戏视频学习遵循指令

    GROOT: Learning to Follow Instructions by Watching Gameplay Videos. (arXiv:2310.08235v1 [cs.AI])

    [http://arxiv.org/abs/2310.08235](http://arxiv.org/abs/2310.08235)

    GROOT是一个通过观看游戏视频学习遵循指令的控制器，它通过产生一个结构化的目标空间来消除昂贵的文本-游戏注释的需求，并在Minecraft SkillForge基准测试中展现出与人类玩家相当的表现水平。

    

    我们研究在开放世界环境中构建一个可以遵循开放式指令的控制器的问题。我们提出了通过观看视频作为指令的方式，这种方式提供了表达能力强的目标规范，同时消除了昂贵的文本-游戏注释的需求。我们提出了一个新的学习框架，可以从游戏视频中学习这种指令遵循控制器，并产生一个能产生结构化目标空间的视频指令编码器。我们在基于因果变压器的简单而有效的编码器-解码器架构中实现了我们的代理程序GROOT。我们在一个提出的Minecraft SkillForge基准测试中对GROOT进行了与开放世界对手和人类玩家的评估。Elo评级清楚地显示GROOT正在缩小人机差距，并且对最好的通用代理程序基线具有70%的胜率。对所产生的目标空间的定性分析进一步展示了一些有趣的新颖性质，包括目标的组成和...

    We study the problem of building a controller that can follow open-ended instructions in open-world environments. We propose to follow reference videos as instructions, which offer expressive goal specifications while eliminating the need for expensive text-gameplay annotations. A new learning framework is derived to allow learning such instruction-following controllers from gameplay videos while producing a video instruction encoder that induces a structured goal space. We implement our agent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers. We evaluate GROOT against open-world counterparts and human players on a proposed Minecraft SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap as well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis of the induced goal space further demonstrates some interesting emergent properties, including the goal composition and 
    
[^13]: 时间步频对不同尺度物体机器人操作仿真逼真度的影响

    The Impact of Time Step Frequency on the Realism of Robotic Manipulation Simulation for Objects of Different Scales. (arXiv:2310.08233v1 [cs.RO])

    [http://arxiv.org/abs/2310.08233](http://arxiv.org/abs/2310.08233)

    增加小尺度物体的时间步频可以提高机器人操作仿真准确性，为改进机器人装配过程中的仿真到真实环境转换提供了起点。

    

    本研究评估了时间步频和组件尺度对机器人操作仿真准确性的影响。结果表明，增加小尺度物体的时间步频可以提高仿真准确性。此仿真展示了两种物体几何形状的预装配部件拾取，为讨论如何改进机器人装配过程中的仿真到真实环境转换提供了起点。

    This work evaluates the impact of time step frequency and component scale on robotic manipulation simulation accuracy. Increasing the time step frequency for small-scale objects is shown to improve simulation accuracy. This simulation, demonstrating pre-assembly part picking for two object geometries, serves as a starting point for discussing how to improve Sim2Real transfer in robotic assembly processes.
    
[^14]: SimCKP: 简单对比学习关键词短语表示

    SimCKP: Simple Contrastive Learning of Keyphrase Representations. (arXiv:2310.08221v1 [cs.CL])

    [http://arxiv.org/abs/2310.08221](http://arxiv.org/abs/2310.08221)

    SimCKP是一个简单的对比学习框架，通过学习上下文感知的短语级表示来提取关键词短语，并通过重新排序来调整生成的短语的分数。

    

    关键词生成（KG）旨在生成一组总结性词语或短语，给定一个源文档，而关键词提取（KE）旨在从文本中识别它们。由于在KE中搜索空间较小，通常将其与KG相结合，预测可能存在或不存在于相应文档中的关键词短语。然而，目前的统一方法采用序列标注和基于最大化的生成，主要在令牌级别上操作，不能很好地观察和评分关键词短语作为一个整体。在这项工作中，我们提出了SimCKP，一个简单的对比学习框架，包括两个阶段：1）提取器-生成器，通过对比学习上下文感知的短语级表示来提取关键词短语，同时生成不出现在文档中的关键词短语；2）重新排序器，通过将它们的表示与相应文档对齐，同样调整每个生成的短语的分数。在多个基准上进行实验证明

    Keyphrase generation (KG) aims to generate a set of summarizing words or phrases given a source document, while keyphrase extraction (KE) aims to identify them from the text. Because the search space is much smaller in KE, it is often combined with KG to predict keyphrases that may or may not exist in the corresponding document. However, current unified approaches adopt sequence labeling and maximization-based generation that primarily operate at a token level, falling short in observing and scoring keyphrases as a whole. In this work, we propose SimCKP, a simple contrastive learning framework that consists of two stages: 1) An extractor-generator that extracts keyphrases by learning context-aware phrase-level representations in a contrastive manner while also generating keyphrases that do not appear in the document; 2) A reranker that adapts scores for each generated phrase by likewise aligning their representations with the corresponding document. Experimental results on multiple ben
    
[^15]: TriRE: 一种用于持续知识保持和推进的多机制学习范式

    TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge Retention and Promotion. (arXiv:2310.08217v1 [cs.AI])

    [http://arxiv.org/abs/2310.08217](http://arxiv.org/abs/2310.08217)

    TriRE是一个新的持续学习范式，受到大脑如何利用多种机制进行学习的启发。它通过保持每个任务中最显著的神经元，修订和巩固这些神经元之间的联系，解决了深度神经网络中的灾难性遗忘问题。

    

    持续学习（CL）对于深度神经网络来说一直是一个持久的挑战，原因是先前学习的任务会发生灾难性遗忘（CF）。已经提出了一些技术来减轻CF，例如权重正则化，经验重演和参数隔离。尽管相对成功，但这些研究方向主要是相互独立的，存在一些缺点，并且错过了竞争策略的优势。相反，大脑通过同时利用多种神经生理过程（包括神经发生，主动遗忘，神经调节，元可塑性，经验重演和上下文相关的门控），不常导致CF，而是持续学习，适应和跨任务传递知识。受大脑如何同时利用多个机制的启发，我们提出了TriRE，一种新的CL范式，旨在保留每个任务中最显著的神经元，修订和巩固这些神经元之间的联系。

    Continual learning (CL) has remained a persistent challenge for deep neural networks due to catastrophic forgetting (CF) of previously learned tasks. Several techniques such as weight regularization, experience rehearsal, and parameter isolation have been proposed to alleviate CF. Despite their relative success, these research directions have predominantly remained orthogonal and suffer from several shortcomings, while missing out on the advantages of competing strategies. On the contrary, the brain continually learns, accommodates, and transfers knowledge across tasks by simultaneously leveraging several neurophysiological processes, including neurogenesis, active forgetting, neuromodulation, metaplasticity, experience rehearsal, and context-dependent gating, rarely resulting in CF. Inspired by how the brain exploits multiple mechanisms concurrently, we propose TriRE, a novel CL paradigm that encompasses retaining the most prominent neurons for each task, revising and solidifying the 
    
[^16]: 可信任的机器学习

    Trustworthy Machine Learning. (arXiv:2310.08215v1 [cs.LG])

    [http://arxiv.org/abs/2310.08215](http://arxiv.org/abs/2310.08215)

    该论文讨论了当前机器学习技术面临的可信度问题，并提出了分布之外的泛化、可解释性、不确定性量化和可信度评估四个关键主题的解决方案。

    

    随着机器学习技术应用于实际产品和解决方案，出现了新的挑战。模型在分布的微小变化下意外地无法泛化，对于他们从未见过的新数据表现出自信，或者无法有效地向最终用户解释其决策的原理。总而言之，我们面临着当前机器学习技术的可信度问题。这本《可信任的机器学习》教材涵盖了可信任的机器学习的四个关键主题的理论和技术背景：分布之外的泛化、可解释性、不确定性量化和可信度评估。我们讨论了重要的经典和现代研究论文，并揭示并连接了它们的基本直觉。这本书是从2022/23冬季学期开始在图宾根大学开设的同名课程发展而来。它旨在成为一本独立的产品，附有代码片段。

    As machine learning technology gets applied to actual products and solutions, new challenges have emerged. Models unexpectedly fail to generalize to small changes in the distribution, tend to be confident on novel data they have never seen, or cannot communicate the rationale behind their decisions effectively with the end users. Collectively, we face a trustworthiness issue with the current machine learning technology. This textbook on Trustworthy Machine Learning (TML) covers a theoretical and technical background of four key topics in TML: Out-of-Distribution Generalization, Explainability, Uncertainty Quantification, and Evaluation of Trustworthiness. We discuss important classical and contemporary research papers of the aforementioned fields and uncover and connect their underlying intuitions. The book evolved from the homonymous course at the University of T\"ubingen, first offered in the Winter Semester of 2022/23. It is meant to be a stand-alone product accompanied by code snip
    
[^17]: 基于粗粒度引导森林和多中心损失的长尾分类

    Long-Tailed Classification Based on Coarse-Grained Leading Forest and Multi-Center Loss. (arXiv:2310.08206v1 [cs.CV])

    [http://arxiv.org/abs/2310.08206](http://arxiv.org/abs/2310.08206)

    本论文提出了一种基于粗粒度引导森林和多中心损失的长尾分类框架，名为Cognisance。该框架致力于解决长尾分类问题中的类间和类内不平衡，并通过不变特征学习构建多粒度联合解决模型。

    

    长尾分类是现实世界中不可避免且具有挑战性的问题。大部分现有的长尾分类方法仅关注解决类间不平衡，即头部类别的样本比尾部类别的样本多，而忽略了类内不平衡，即同一类别中头部属性样本数量远大于尾部属性样本数量。模型的偏差是由这两个因素引起的，由于大多数数据集中的属性是隐含的且属性组合非常复杂，处理类内不平衡更加困难。为此，我们提出了一种基于粗粒度引导森林（CLF）和多中心损失（MCL）的长尾分类框架，名为Cognisance，旨在通过不变特征学习构建多粒度联合解决模型。在这个方法中，我们设计了一种新颖的样本选择策略和损失函数，以平衡不同类别和属性之间的样本分布。

    Long-tailed(LT) classification is an unavoidable and challenging problem in the real world. Most of the existing long-tailed classification methods focus only on solving the inter-class imbalance in which there are more samples in the head class than in the tail class, while ignoring the intra-lass imbalance in which the number of samples of the head attribute within the same class is much larger than the number of samples of the tail attribute. The deviation in the model is caused by both of these factors, and due to the fact that attributes are implicit in most datasets and the combination of attributes is very complex, the intra-class imbalance is more difficult to handle. For this purpose, we proposed a long-tailed classification framework, known as \textbf{\textsc{Cognisance}}, which is founded on Coarse-Grained Leading Forest (CLF) and Multi-Center Loss (MCL), aiming to build a multi-granularity joint solution model by means of invariant feature learning. In this method, we desig
    
[^18]: 超越传统DoE：深度强化学习用于优化电池动力学模型识别中的实验设计

    Beyond Traditional DoE: Deep Reinforcement Learning for Optimizing Experiments in Model Identification of Battery Dynamics. (arXiv:2310.08198v1 [cs.LG])

    [http://arxiv.org/abs/2310.08198](http://arxiv.org/abs/2310.08198)

    本论文提出了一种基于深度强化学习的新颖DoE方法，用于优化电池动力学模型识别中的实验设计。该方法根据过去实验的统计信息动态地改变实验配置，从而提高了实验效率和准确性。

    

    电池动力学模型识别是能源研究中的一个核心问题；许多能源管理系统和设计过程依赖于准确的电池模型进行效率优化。传统的电池建模方法是利用传统的实验设计（DoE），其中通过许多不同的电流配置来激发电池动力学，并利用测量输出来估计系统动力学。然而，虽然传统方法可以获得有用的模型，但由于需要扫描许多不同的电流配置，这一过程时间长且昂贵。在本研究中，基于深度强化学习开发了一种新的DoE方法，该方法根据过去实验的统计信息动态地改变实验配置。与坚持预定义电流配置的库不同，提出的方法通过更新输出空间来动态修改电流配置。

    Model identification of battery dynamics is a central problem in energy research; many energy management systems and design processes rely on accurate battery models for efficiency optimization. The standard methodology for battery modelling is traditional design of experiments (DoE), where the battery dynamics are excited with many different current profiles and the measured outputs are used to estimate the system dynamics. However, although it is possible to obtain useful models with the traditional approach, the process is time consuming and expensive because of the need to sweep many different current-profile configurations. In the present work, a novel DoE approach is developed based on deep reinforcement learning, which alters the configuration of the experiments on the fly based on the statistics of past experiments. Instead of sticking to a library of predefined current profiles, the proposed approach modifies the current profiles dynamically by updating the output space covere
    
[^19]: EIPE-text: 针对长篇叙事文本生成的评估引导迭代计划提取方法

    EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation. (arXiv:2310.08185v1 [cs.CL])

    [http://arxiv.org/abs/2310.08185](http://arxiv.org/abs/2310.08185)

    本文提出了EIPE-text方法用于长篇叙事文本生成，通过从语料库中提取计划并利用评估机制进行迭代改进，构建了更好的规划器。

    

    计划与写作是长篇叙事文本生成中常用的层次化方法，首先创建一个计划来指导叙事写作。遵循这种方法，一些研究仅仅依靠对大型语言模型进行提示进行计划，这经常产生次优的结果。在本文中，我们提出了一种新的框架，称为评估引导迭代计划提取方法（EIPE-text）用于长篇叙事文本生成，该方法从叙事语料库中提取计划并利用提取的计划构建更好的规划器。EIPE-text有三个阶段：计划提取、学习和推理。在计划提取阶段，它迭代地从叙事语料库中提取和改进计划，并构建一个计划语料库。我们提出了一种基于问答（QA）的评估机制，自动评估计划并生成详细的计划改进指示，以引导迭代改进。在学习阶段，我们通过微调构建一个更好的规划器。

    Plan-and-Write is a common hierarchical approach in long-form narrative text generation, which first creates a plan to guide the narrative writing. Following this approach, several studies rely on simply prompting large language models for planning, which often yields suboptimal results. In this paper, we propose a new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner. EIPE-text has three stages: plan extraction, learning, and inference. In the plan extraction stage, it iteratively extracts and improves plans from the narrative corpus and constructs a plan corpus. We propose a question answer (QA) based evaluation mechanism to automatically evaluate the plans and generate detailed plan refinement instructions to guide the iterative improvement. In the learning stage, we build a better planner by fine-tuning wit
    
[^20]: 超越微调的模型学习：一项调查

    Learn From Model Beyond Fine-Tuning: A Survey. (arXiv:2310.08184v1 [cs.AI])

    [http://arxiv.org/abs/2310.08184](http://arxiv.org/abs/2310.08184)

    这项研究以Learn From Model (LFM)为名，探索了超越微调的模型学习技术，旨在通过对模型接口进行研究和设计，将基于模型的学习推广到下游任务中。

    

    基于模型的学习（LFM）是一种新的研究趋势，它专注于通过对模型接口进行研究、修改和设计来更好地理解模型的结构和权重（在黑匣子环境中），并将模型泛化到下游任务中。本文将LFM技术的研究分为五个主要领域。

    Foundation models (FM) have demonstrated remarkable performance across a wide range of tasks (especially in the fields of natural language processing and computer vision), primarily attributed to their ability to comprehend instructions and access extensive, high-quality data. This not only showcases their current effectiveness but also sets a promising trajectory towards the development of artificial general intelligence. Unfortunately, due to multiple constraints, the raw data of the model used for large model training are often inaccessible, so the use of end-to-end models for downstream tasks has become a new research trend, which we call Learn From Model (LFM) in this article. LFM focuses on the research, modification, and design of FM based on the model interface, so as to better understand the model structure and weights (in a black box environment), and to generalize the model to downstream tasks. The study of LFM techniques can be broadly categorized into five major areas: mod
    
[^21]: 多尺度时空循环网络用于交通流预测

    Multi-Scale Spatial-Temporal Recurrent Networks for Traffic Flow Prediction. (arXiv:2310.08138v1 [cs.LG])

    [http://arxiv.org/abs/2310.08138](http://arxiv.org/abs/2310.08138)

    本论文提出了一种名为MSSTRN的多尺度时空循环网络，用于交通流预测。该网络通过两种不同的循环神经网络完全捕获了交通数据中不同时间窗口下的复杂时空信息，解决了现有方法中忽视图结构、捕捉不充分以及缺乏关注不同时间尺度上时空信息的问题。

    

    交通流预测是智能交通系统中最基本的任务之一。复杂而动态的时空依赖关系使得交通流预测变得非常具有挑战性。尽管现有的时空图神经网络在交通流预测中有显著优势，但它们常常面临以下挑战：（1）忽视了固定的图结构，限制了模型的预测性能；（2）不能同时充分捕捉复杂的时空依赖关系；（3）缺乏对不同时间尺度上的时空信息的关注。本文提出了一种多尺度时空循环网络（MSSTRN）用于交通流预测，它由两种不同的循环神经网络组成：单步门循环单元和多步门循环单元，以完全捕获交通数据中不同时间窗口下的复杂时空信息。此外，我们还提出了一种时空同步注意机制。

    Traffic flow prediction is one of the most fundamental tasks of intelligent transportation systems. The complex and dynamic spatial-temporal dependencies make the traffic flow prediction quite challenging. Although existing spatial-temporal graph neural networks hold prominent, they often encounter challenges such as (1) ignoring the fixed graph that limits the predictive performance of the model, (2) insufficiently capturing complex spatial-temporal dependencies simultaneously, and (3) lacking attention to spatial-temporal information at different time lengths. In this paper, we propose a Multi-Scale Spatial-Temporal Recurrent Network for traffic flow prediction, namely MSSTRN, which consists of two different recurrent neural networks: the single-step gate recurrent unit and the multi-step gate recurrent unit to fully capture the complex spatial-temporal information in the traffic data under different time windows. Moreover, we propose a spatial-temporal synchronous attention mechanis
    
[^22]: 能否通过自我批评改进大型语言模型对自身计划的能力？

    Can Large Language Models Really Improve by Self-critiquing Their Own Plans?. (arXiv:2310.08118v1 [cs.AI])

    [http://arxiv.org/abs/2310.08118](http://arxiv.org/abs/2310.08118)

    本研究探究了大型语言模型在计划领域中的验证和自我批评能力。研究结果发现，自我批评似乎会降低计划生成的性能，并导致大量误报，降低了系统的可靠性。

    

    关于大型语言模型（LLMs）能否成功验证或自我批评其在推理问题中的候选解决方案的广泛说法已经存在。在本文中，我们针对计划的情况研究了大型语言模型的验证/自我批评能力。我们评估了一个使用LLMs进行计划生成和验证的计划系统。我们评估了验证器LLM在与基准验证的性能、自我批评对计划生成的影响以及系统性能中反馈水平变化的影响。使用最先进的LLM GPT-4进行生成和验证，我们的发现表明，自我批评似乎会降低计划生成的性能，尤其是与具有外部、可靠验证器的系统相比，LLM验证器在该系统中产生了大量的误报，从而影响了系统的可靠性。

    There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode. Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning. We evaluate a planning system that employs LLMs for both plan generation and verification. We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance. Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability. Additiona
    
[^23]: DUSA: 解耦无监督的车联网协作感知的模拟到实际适应

    DUSA: Decoupled Unsupervised Sim2Real Adaptation for Vehicle-to-Everything Collaborative Perception. (arXiv:2310.08117v1 [cs.CV])

    [http://arxiv.org/abs/2310.08117](http://arxiv.org/abs/2310.08117)

    DUSA是一种解耦无监督的车联网协作感知的模拟到实际适应方法，旨在解决模拟数据与真实数据之间的领域差距以及协作代理之间的领域差距。通过这种方法，可以提高车联网协作感知在真实场景中的性能。

    

    车联网协作感知对于自动驾驶至关重要。然而，实现高精度的车联网协作感知需要大量的真实世界数据进行注释，这往往是昂贵且难以获取的。模拟数据因其可以以极低的成本大量生产而受到广泛关注。然而，模拟数据与真实世界数据之间存在显著的领域差距，包括传感器类型、反射模式和道路环境的差异，导致在模拟数据上训练的模型在真实世界数据上评估时性能较差。此外，真实世界协作代理之间仍存在领域差距，例如不同类型的传感器可能安装在自动驾驶车辆和路边基础设施上，且具有不同的外部参数，进一步增加了模拟到实际的泛化难度。为充分利用模拟数据，我们提出了一种新的无监督模拟到实际领域适应方法。

    Vehicle-to-Everything (V2X) collaborative perception is crucial for autonomous driving. However, achieving high-precision V2X perception requires a significant amount of annotated real-world data, which can always be expensive and hard to acquire. Simulated data have raised much attention since they can be massively produced at an extremely low cost. Nevertheless, the significant domain gap between simulated and real-world data, including differences in sensor type, reflectance patterns, and road surroundings, often leads to poor performance of models trained on simulated data when evaluated on real-world data. In addition, there remains a domain gap between real-world collaborative agents, e.g. different types of sensors may be installed on autonomous vehicles and roadside infrastructures with different extrinsics, further increasing the difficulty of sim2real generalization. To take full advantage of simulated data, we present a new unsupervised sim2real domain adaptation method for 
    
[^24]: Promptor:一种用于智能文本输入技术的对话式自主提示生成代理

    Promptor: A Conversational and Autonomous Prompt Generation Agent for Intelligent Text Entry Techniques. (arXiv:2310.08101v1 [cs.CL])

    [http://arxiv.org/abs/2310.08101](http://arxiv.org/abs/2310.08101)

    本论文介绍了Promptor，一个用于智能文本输入技术的对话式自主提示生成代理。利用大型语言模型的上下文学习能力，可以克服数据收集和模型微调的挑战。我们通过以GPT-3.5为例的实验证明，仅通过提示即可超过GPT-2支持的系统，并且可与经过精调的GPT-3.5模型相媲美。

    

    在我们日常数字交互中，文本输入是一项重要的任务。为了使文本输入更有效、高效和流畅，已经开发出了许多智能功能，包括句子预测和用户个性化。然而，随着基于深度学习的语言模型成为这些高级功能的常规，数据收集和模型微调的必要性也增加了。利用GPT-3.5等大型语言模型的上下文学习能力可以减轻这些挑战。这一独特的特性允许语言模型通过提示获得新的技能，消除了数据收集和微调的需要。因此，大型语言模型可以学习各种文本预测技术。我们最初展示了仅通过提示GPT-3.5即可超过GPT-2支持的系统，并且与经过精调的GPT-3.5模型相当，在后两种方法需要昂贵的数据集的情况下。

    Text entry is an essential task in our day-to-day digital interactions. Numerous intelligent features have been developed to streamline this process, making text entry more effective, efficient, and fluid. These improvements include sentence prediction and user personalization. However, as deep learning-based language models become the norm for these advanced features, the necessity for data collection and model fine-tuning increases. These challenges can be mitigated by harnessing the in-context learning capability of large language models such as GPT-3.5. This unique feature allows the language model to acquire new skills through prompts, eliminating the need for data collection and fine-tuning. Consequently, large language models can learn various text prediction techniques. We initially showed that, for a sentence prediction task, merely prompting GPT-3.5 surpassed a GPT-2 backed system and is comparable with a fine-tuned GPT-3.5 model, with the latter two methods requiring costly 
    
[^25]: Sentinel: 一种用于保护分散式联邦学习的聚合函数

    Sentinel: An Aggregation Function to Secure Decentralized Federated Learning. (arXiv:2310.08097v1 [cs.DC])

    [http://arxiv.org/abs/2310.08097](http://arxiv.org/abs/2310.08097)

    Sentinel是一种用于保护分散式联邦学习的防御策略，通过利用本地数据并定义一个三步聚合协议来对抗污染攻击。评估结果表明Sentinel在不同数据集和评估指标下表现良好。

    

    将联邦学习（FL）快速整合到网络中涵盖了网络管理、服务质量和网络安全等各个方面，同时保护数据隐私。在这种情况下，分散式联邦学习（DFL）作为一种创新范式，用于训练协作模型，解决了单点失效的限制。然而，FL和DFL的安全性和可信性受到污染攻击的影响，从而对其性能产生负面影响。现有的防御机制针对集中式FL进行设计，并未充分利用DFL的特点。因此，本文引入了Sentinel，一种在DFL中对抗污染攻击的防御策略。Sentinel利用本地数据的可访问性，定义了一个三步聚合协议，包括相似性过滤、引导验证和标准化，以防止恶意模型更新。通过使用不同数据集和不同的评估指标对Sentinel进行了评估。

    The rapid integration of Federated Learning (FL) into networking encompasses various aspects such as network management, quality of service, and cybersecurity while preserving data privacy. In this context, Decentralized Federated Learning (DFL) emerges as an innovative paradigm to train collaborative models, addressing the single point of failure limitation. However, the security and trustworthiness of FL and DFL are compromised by poisoning attacks, negatively impacting its performance. Existing defense mechanisms have been designed for centralized FL and they do not adequately exploit the particularities of DFL. Thus, this work introduces Sentinel, a defense strategy to counteract poisoning attacks in DFL. Sentinel leverages the accessibility of local data and defines a three-step aggregation protocol consisting of similarity filtering, bootstrap validation, and normalization to safeguard against malicious model updates. Sentinel has been evaluated with diverse datasets and various 
    
[^26]: 分辨时间差分学习

    Discerning Temporal Difference Learning. (arXiv:2310.08091v1 [cs.LG])

    [http://arxiv.org/abs/2310.08091](http://arxiv.org/abs/2310.08091)

    该论文提出了一种名为分辨TD学习(DTD)的新型TD算法，通过灵活的强调函数来分配资源，以改善状态之间的轻重分配，解决了传统TD学习中忽视历史状态重要性和TD误差传播的问题。实证结果表明，使用合理的强调函数不仅改进了值估计，还加速了学习过程。

    

    时间差分学习(TD)是强化学习中的基本概念，旨在高效评估策略的价值函数。TD($\lambda$)是一种有效的变体，通过引入记忆轨迹将预测误差分散到历史上下文中。然而，这种方法经常忽视历史状态的重要性以及传播TD误差的相对重要性，这受到访问失衡或结果噪声等挑战的影响。为了解决这个问题，我们提出了一种名为分辨TD学习(DTD)的新型TD算法，它允许灵活的强调函数-在训练过程中预先确定或自适应地分配资源以提高状态之间的效果。我们在特定类别的强调函数内建立了我们方法的收敛性质，并展示了它在深度RL环境中的潜在应用。实证结果表明，使用合理的强调函数不仅可以改进值估计，还可以加速学习。

    Temporal difference learning (TD) is a foundational concept in reinforcement learning (RL), aimed at efficiently assessing a policy's value function. TD($\lambda$), a potent variant, incorporates a memory trace to distribute the prediction error into the historical context. However, this approach often neglects the significance of historical states and the relative importance of propagating the TD error, influenced by challenges such as visitation imbalance or outcome noise. To address this, we propose a novel TD algorithm named discerning TD learning (DTD), which allows flexible emphasis functions$-$predetermined or adapted during training$-$to allocate efforts effectively across states. We establish the convergence properties of our method within a specific class of emphasis functions and showcase its promising potential for adaptation to deep RL contexts. Empirical results underscore that employing a judicious emphasis function not only improves value estimation but also expedites l
    
[^27]: 以问答方式翻译印尼语的低资源标题党处理

    Low-Resource Clickbait Spoiling for Indonesian via Question Answering. (arXiv:2310.08085v1 [cs.CL])

    [http://arxiv.org/abs/2310.08085](http://arxiv.org/abs/2310.08085)

    本论文介绍了一项以问答方式翻译印尼语低资源标题党处理的任务，贡献包括构建手动标注的印尼语标题党处理语料库，并评估了跨语言零射击问答模型的应用。实验结果表明，XLM-RoBERTa（大）模型在短语和段落标题党处理方面表现优异，而mDeBERTa（基础）模型在多部分标题党处理中表现最佳。

    

    标题党处理旨在生成一个短文本，以满足标题党帖子引起的好奇心。由于这是一个新引入的任务，目前只有英文数据集可用。我们的贡献包括在印尼语中构建手动标注的标题党处理语料库，并评估使用跨语言零射击问答模型来处理印尼语等低资源语言的标题党处理。我们利用多语言语言模型的选择。实验结果表明，XLM-RoBERTa（大）模型在短语和段落标题党处理方面优于其他模型，而mDeBERTa（基础）模型在多部分标题党处理中优于其他模型。

    Clickbait spoiling aims to generate a short text to satisfy the curiosity induced by a clickbait post. As it is a newly introduced task, the dataset is only available in English so far. Our contributions include the construction of manually labeled clickbait spoiling corpus in Indonesian and an evaluation on using cross-lingual zero-shot question answering-based models to tackle clikcbait spoiling for low-resource language like Indonesian. We utilize selection of multilingual language models. The experimental results suggest that XLM-RoBERTa (large) model outperforms other models for phrase and passage spoilers, meanwhile, mDeBERTa (base) model outperforms other models for multipart spoilers.
    
[^28]: GameGPT: 游戏开发的多智能体协作框架

    GameGPT: Multi-agent Collaborative Framework for Game Development. (arXiv:2310.08067v1 [cs.AI])

    [http://arxiv.org/abs/2310.08067](http://arxiv.org/abs/2310.08067)

    GameGPT是一个多智能体协作框架，用于自动化游戏开发。它通过使用双重协作和分层方法，结合多个内部词库和解耦方法，解决了大型语言模型在游戏开发中的幻觉和冗余问题。

    

    基于大型语言模型(LLM)的智能体展示了它们自动化和加速软件开发过程的能力。本文针对游戏开发，提出了一个名为GameGPT的多智能体协作框架，用于自动化游戏开发。尽管许多研究已经指出幻觉是部署LLM在生产中的主要障碍，但我们认为还存在另一个问题：冗余。我们的框架提出了一系列方法来缓解这两个问题。这些方法包括双重协作和分层方法，结合多个内部词库，以降低规划、任务识别和实施阶段的幻觉和冗余。此外，我们还引入了一种解耦方法，以更好地实现代码生成的精度。

    The large language model (LLM) based agents have demonstrated their capacity to automate and expedite software development processes. In this paper, we focus on game development and propose a multi-agent collaborative framework, dubbed GameGPT, to automate game development. While many studies have pinpointed hallucination as a primary roadblock for deploying LLMs in production, we identify another concern: redundancy. Our framework presents a series of methods to mitigate both concerns. These methods include dual collaboration and layered approaches with several in-house lexicons, to mitigate the hallucination and redundancy in the planning, task identification, and implementation phases. Furthermore, a decoupling approach is also introduced to achieve code generation with better precision.
    
[^29]: 在近似纳什均衡算法中的搜索与混合范式

    The Search-and-Mix Paradigm in Approximate Nash Equilibrium Algorithms. (arXiv:2310.08066v1 [cs.GT])

    [http://arxiv.org/abs/2310.08066](http://arxiv.org/abs/2310.08066)

    本研究提出了一种搜索和混合范式的自动方法，用于近似计算二人博弈中的纳什均衡。这种方法可以完全自动化设计和分析混合阶段，并且能够对已经存在的算法进行近似界限的分析，无需手写证明。

    

    数学中的人工智能以一种构造性的方式处理数学问题，使得推理自动化、减少劳动量和减少错误。对于算法来说，问题变成了如何自动化分析特定问题。本文首次提供了一种自动方法，用于在理论计算机科学中研究的一个经典问题上进行近似分析：计算二人博弈中的近似纳什均衡。我们观察到这样的算法可以重写为一个搜索和混合的范式，其中包括搜索阶段和混合阶段。通过这样做，我们能够完全自动化设计和分析混合阶段的过程。例如，我们演示了如何使用我们的方法来分析文献中所有算法的近似界限。同样的近似界限可以在没有任何手写证明的情况下计算出来。我们的自动方法严重依赖于近似纳什均衡中的线性松弛结构。

    AI in Math deals with mathematics in a constructive manner so that reasoning becomes automated, less laborious, and less error-prone. For algorithms, the question becomes how to automate analyses for specific problems. For the first time, this work provides an automatic method for approximation analysis on a well-studied problem in theoretical computer science: computing approximate Nash equilibria in two-player games. We observe that such algorithms can be reformulated into a search-and-mix paradigm, which involves a search phase followed by a mixing phase. By doing so, we are able to fully automate the procedure of designing and analyzing the mixing phase. For example, we illustrate how to perform our method with a program to analyze the approximation bounds of all the algorithms in the literature. Same approximation bounds are computed without any hand-written proof. Our automatic method heavily relies on the LP-relaxation structure in approximate Nash equilibria. Since many approxi
    
[^30]: 从标签比例中学习：通过信念传播对有监督学习器进行引导

    Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation. (arXiv:2310.08056v1 [cs.LG])

    [http://arxiv.org/abs/2310.08056](http://arxiv.org/abs/2310.08056)

    本文提出了一种从标签比例中学习的算法框架，通过伪标签化和嵌入细化两个步骤迭代地提高有监督学习器性能。

    

    标签比例学习（LLP）是一个学习问题，在训练过程中，只有针对一组实例（称为包）的聚合级别标签可用，并且目的是在测试数据的实例级别上获得最佳性能。这种设置在广告和医学等领域由于隐私考虑而出现。我们提出了一个新颖的算法框架来解决这个问题，它通过两个主要步骤进行迭代。在每次迭代的第一步（伪标签化）中，我们定义了一个基于二进制实例标签的吉布斯分布，该分布通过以下约束将covariate信息（协变量信息）合并进去：具有相似covariates的实例应该具有相似的标签，并且通过包级别的聚合标签来综合covariate信息。然后，我们使用信念传播（BP）来边缘化吉布斯分布以获得伪标签。在第二步（嵌入细化）中，我们使用伪标签为学习器提供监督，从而获得更好的嵌入。此后，我们对这两个步骤进行迭代。

    Learning from Label Proportions (LLP) is a learning problem where only aggregate level labels are available for groups of instances, called bags, during training, and the aim is to get the best performance at the instance-level on the test data. This setting arises in domains like advertising and medicine due to privacy considerations. We propose a novel algorithmic framework for this problem that iteratively performs two main steps. For the first step (Pseudo Labeling) in every iteration, we define a Gibbs distribution over binary instance labels that incorporates a) covariate information through the constraint that instances with similar covariates should have similar labels and b) the bag level aggregated label. We then use Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo labels. In the second step (Embedding Refinement), we use the pseudo labels to provide supervision for a learner that yields a better embedding. Further, we iterate on the two steps ag
    
[^31]: 理解和控制迷宫解决策略网络

    Understanding and Controlling a Maze-Solving Policy Network. (arXiv:2310.08043v1 [cs.AI])

    [http://arxiv.org/abs/2310.08043](http://arxiv.org/abs/2310.08043)

    本论文研究了一个预训练的强化学习策略网络，并发现该网络追求多个上下文相关的目标。通过修改特定的通道，我们可以部分地控制策略。这表明训练策略网络中存在冗余、分布式和可重新定向的目标表示。

    

    为了了解人工智能系统的目标和目标表示，我们仔细研究了一个预训练的强化学习策略网络，该网络通过导航到一系列目标方块来解决迷宫问题。我们发现这个网络追求多个上下文相关的目标，并且我们进一步确定了网络中与其中一个目标对应的电路。特别是，我们确定了11个跟踪目标位置的通道。通过修改这些通道，不论是通过手工设计的干预还是通过组合前向传递，我们都可以部分地控制策略。我们展示了这个网络包含多余的、分布式的、可重新定向的目标表示，为训练策略网络中目标导向的本质提供了揭示。

    To understand the goals and goal representations of AI systems, we carefully study a pretrained reinforcement learning policy that solves mazes by navigating to a range of target squares. We find this network pursues multiple context-dependent goals, and we further identify circuits within the network that correspond to one of these goals. In particular, we identified eleven channels that track the location of the goal. By modifying these channels, either with hand-designed interventions or by combining forward passes, we can partially control the policy. We show that this network contains redundant, distributed, and retargetable goal representations, shedding light on the nature of goal-direction in trained policy networks.
    
[^32]: QLLM: 大规模语言模型的准确高效低位宽量化

    QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])

    [http://arxiv.org/abs/2310.08041](http://arxiv.org/abs/2310.08041)

    QLLM是一种为大规模语言模型设计的准确高效的低位宽后训练量化方法，通过引入自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。

    

    大规模语言模型在自然语言处理领域表现出色，但由于其所需资源过大，限制了其广泛应用。虽然量化感知训练（Quantization-Aware Training，QAT）提供了一种解决方案，但它的训练成本过高，因此后训练量化（Post-Training Quantization，PTQ）成为大规模语言模型更实际的方法。在现有研究中，特定通道中的激活离群值被认为是导致后训练量化准确性下降的瓶颈。本文提出了QLLM，一种为大规模语言模型设计的准确高效的低位宽后训练量化方法。QLLM引入了一种自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。具体来说，通过通道拆分和通道组装，在保证低位宽的情况下将离群通道分解成多个子通道。

    Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a 
    
[^33]: 重新思考大规模预排序系统：整体链路跨域模型

    Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain Models. (arXiv:2310.08039v1 [cs.IR])

    [http://arxiv.org/abs/2310.08039](http://arxiv.org/abs/2310.08039)

    本文提出了一种重新思考大规模预排序系统的方法，通过整体链路跨域模型和细粒度神经结构来解决样本选择偏差问题，并改进预排序的准确性。

    

    工业系统，如推荐系统和在线广告，已广泛配备了多阶段架构，包括匹配、预排序、排序和再排序。作为匹配和排序之间的关键桥梁，现有的预排序方法主要忽视了整个链路数据的依赖性，导致子优化性能。在本文中，我们从整体样本空间的角度重新思考预排序系统，并提出了整体链路跨域模型（ECM），利用整个级联阶段的样本来有效减轻样本选择偏差（SSB）问题。此外，我们设计了一种细粒度神经结构，名为ECMM，进一步提高了预排序的准确性。具体来说，我们提出了一个跨域多塔神经网络来综合预测每个阶段的结果，并引入$L0$正则化的子网络路由策略来调整每个阶段的贡献权重。

    Industrial systems such as recommender systems and online advertising, have been widely equipped with multi-stage architectures, which are divided into several cascaded modules, including matching, pre-ranking, ranking and re-ranking. As a critical bridge between matching and ranking, existing pre-ranking approaches mainly endure sample selection bias (SSB) problem owing to ignoring the entire-chain data dependence, resulting in sub-optimal performances. In this paper, we rethink pre-ranking system from the perspective of the entire sample space, and propose Entire-chain Cross-domain Models (ECM), which leverage samples from the whole cascaded stages to effectively alleviate SSB problem. Besides, we design a fine-grained neural structure named ECMM to further improve the pre-ranking accuracy. Specifically, we propose a cross-domain multi-tower neural network to comprehensively predict for each stage result, and introduce the sub-networking routing strategy with $L0$ regularization to r
    
[^34]: 接收、推理和反应：在自动驾驶车辆中利用大型语言模型驾驶。

    Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles. (arXiv:2310.08034v1 [cs.HC])

    [http://arxiv.org/abs/2310.08034](http://arxiv.org/abs/2310.08034)

    本文提出了一种利用大型语言模型（LLM）改进自动驾驶车辆决策过程的框架，通过利用LLM的语言和推理能力，实现自动驾驶车辆与乘客的互动与个性化。

    

    人本设计与人工智能能力的融合为超越交通的下一代自动驾驶车辆开辟了新的可能性。这些车辆可以与乘客动态互动并根据其喜好进行自适应。本文提出了一种新颖的框架，利用大型语言模型（LLM）来增强自动驾驶车辆的决策过程。通过利用LLM的语言和上下文理解能力以及专门的工具，我们旨在将LLM的语言和推理能力整合到自动驾驶车辆中。我们的研究包括在HighwayEnv中进行实验，这是一套用于自动驾驶和战术决策任务的环境集，以探索LLM在各种场景中的解释、互动和推理。我们还研究了实时个性化，展示了LLM如何根据口头命令影响驾驶行为。我们的实证结果强调了LLM在自动驾驶车辆中的显著优势。

    The fusion of human-centric design and artificial intelligence (AI) capabilities has opened up new possibilities for next-generation autonomous vehicles that go beyond transportation. These vehicles can dynamically interact with passengers and adapt to their preferences. This paper proposes a novel framework that leverages Large Language Models (LLMs) to enhance the decision-making process in autonomous vehicles. By utilizing LLMs' linguistic and contextual understanding abilities with specialized tools, we aim to integrate the language and reasoning capabilities of LLMs into autonomous vehicles. Our research includes experiments in HighwayEnv, a collection of environments for autonomous driving and tactical decision-making tasks, to explore LLMs' interpretation, interaction, and reasoning in various scenarios. We also examine real-time personalization, demonstrating how LLMs can influence driving behaviors based on verbal commands. Our empirical results highlight the substantial advan
    
[^35]: 将领域知识图纳入多模态影片类型分类中，采用自监督注意力和对比学习

    Incorporating Domain Knowledge Graph into Multimodal Movie Genre Classification with Self-Supervised Attention and Contrastive Learning. (arXiv:2310.08032v1 [cs.AI])

    [http://arxiv.org/abs/2310.08032](http://arxiv.org/abs/2310.08032)

    这项研究将领域知识图纳入多模态影片类型分类中，通过利用群组关系和改进注意力分配的方法，实现了更准确和可靠的分类结果。

    

    多模态影片类型分类一直被视为一项具有挑战性的多标签分类任务，由于多模态数据（如海报、剧情摘要、预告片和元数据）的多样性。尽管现有的研究在建模和组合每个模态方面取得了巨大进展，但仍面临着三个问题：1）未利用元数据中的群组关系，2）不可靠的注意力分配，3）不可区分的融合特征。鉴于已证明知识图包含丰富的信息，我们提出了一个新的框架，从不同的角度利用知识图来解决上述问题。首先，我们将元数据处理成一个领域知识图。我们采用一种知识图嵌入的翻译模型来捕捉实体之间的关系。首先，通过利用元数据中的群组关系，我们从知识图中检索相关的嵌入，然后与其他模态进行整合。接下来，我们引入了一种注意力机制，这种机制是自监督的，并采用了对比学习方法来改进注意力分配的可靠性。最后，我们采用一种分类器来进行影片类型分类，该分类器利用了不同模态的融合特征和注意力权重。通过实验证明，我们的方法在多个数据集上均取得了优异的性能。

    Multimodal movie genre classification has always been regarded as a demanding multi-label classification task due to the diversity of multimodal data such as posters, plot summaries, trailers and metadata. Although existing works have made great progress in modeling and combining each modality, they still face three issues: 1) unutilized group relations in metadata, 2) unreliable attention allocation, and 3) indiscriminative fused features. Given that the knowledge graph has been proven to contain rich information, we present a novel framework that exploits the knowledge graph from various perspectives to address the above problems. As a preparation, the metadata is processed into a domain knowledge graph. A translate model for knowledge graph embedding is adopted to capture the relations between entities. Firstly we retrieve the relevant embedding from the knowledge graph by utilizing group relations in metadata and then integrate it with other modalities. Next, we introduce an Attent
    
[^36]: 超越权重共享的无人机RGB-红外车辆再识别特征学习网络

    Beyond Sharing Weights in Decoupling Feature Learning Network for UAV RGB-Infrared Vehicle Re-Identification. (arXiv:2310.08026v1 [cs.CV])

    [http://arxiv.org/abs/2310.08026](http://arxiv.org/abs/2310.08026)

    本论文提出了一个基于无人机的跨模态车辆再识别（Re-ID）的创新研究，旨在解决数据不足、跨模态差异和旋向差异的问题。提出了一个跨模态车辆Re-ID基准，并设计了一个混合权重分离网络（HWDNet）来学习共享的判别性旋向不变特征。

    

    由于能够全天候进行目标搜索，基于无人机的跨模态车辆再识别（Re-ID）在视频监控和公共安全领域越来越受关注。然而，由于数据不足的问题，这个有前景和创新的研究尚未得到充分的研究。与此同时，跨模态差异和旋向差异进一步增加了这一任务的难度。为此，我们首次提出了一个名为无人机跨模态车辆再识别（UCM-VeID）的跨模态车辆Re-ID基准，包含753个身份，16015个RGB图像和13913个红外图像。此外，为了应对跨模态差异和旋向差异的挑战，我们提出了一个混合权重分离网络（HWDNet），用于学习共享的判别性旋向不变特征。

    Owing to the capacity of performing full-time target search, cross-modality vehicle re-identification (Re-ID) based on unmanned aerial vehicle (UAV) is gaining more attention in both video surveillance and public security. However, this promising and innovative research has not been studied sufficiently due to the data inadequacy issue. Meanwhile, the cross-modality discrepancy and orientation discrepancy challenges further aggravate the difficulty of this task. To this end, we pioneer a cross-modality vehicle Re-ID benchmark named UAV Cross-Modality Vehicle Re-ID (UCM-VeID), containing 753 identities with 16015 RGB and 13913 infrared images. Moreover, to meet cross-modality discrepancy and orientation discrepancy challenges, we present a hybrid weights decoupling network (HWDNet) to learn the shared discriminative orientation-invariant features. For the first challenge, we proposed a hybrid weights siamese network with a well-designed weight restrainer and its corresponding objective 
    
[^37]: BERT广义性的影响：人为对抗样本和友好样本的效果

    Effects of Human Adversarial and Affable Samples on BERT Generalizability. (arXiv:2310.08008v1 [cs.AI])

    [http://arxiv.org/abs/2310.08008](http://arxiv.org/abs/2310.08008)

    本研究研究了对BERT模型的广义性影响，发现在固定大小的训练样本上，有10-30\%的人为对抗实例可以显著提高文本分类和关系抽取任务的精度和F1值。

    

    基于BERT的模型在领先榜上表现强劲，但在需要泛化的实际场景中表现较差。有限的训练数据被认为是机器学习泛化能力的主要障碍。本文研究训练数据质量对模型泛化能力的影响，而不是数量。我们考虑了训练数据的两个特征：人为对抗样本（具有看似微小差异但具有不同标签的样本对）和人为友好样本（具有微小差异但具有相同标签的样本对）。我们发现，在固定大小的训练样本上，以10-30\%的人为对抗实例为经验，可以提高文本分类和关系抽取任务的精度和F1值最多20个百分点。超过此范围的增加对模型性能无显著影响。

    BERT-based models have had strong performance on leaderboards, yet have been demonstrably worse in real-world settings requiring generalization. Limited quantities of training data is considered a key impediment to achieving generalizability in machine learning. In this paper, we examine the impact of training \textit{data quality}, not quantity, on a model's generalizability. We consider two characteristics of training data: the portion of human-adversarial (h-adversarial), i.e., sample pairs with seemingly minor differences but different ground-truth labels, and human-affable (h-affable) training samples, i.e., sample pairs with minor differences but the same ground-truth label. We find that for a fixed size of training samples, as a rule of thumb, having 10-30\% h-adversarial instances improves the precision, and therefore F1, by up to 20 points in the tasks of text classification and relation extraction. Increasing h-adversarials beyond this range can result in performance plateaus
    
[^38]: 数据质量保证中一种新的用于离群数据检测的统计度量方法

    A Novel Statistical Measure for Out-of-Distribution Detection in Data Quality Assurance. (arXiv:2310.07998v1 [cs.AI])

    [http://arxiv.org/abs/2310.07998](http://arxiv.org/abs/2310.07998)

    本文使用深度学习技术和新的统计度量方法，旨在研究数据质量保证中的离群数据检测问题，并提出了一种能够有效区分正常数据和离群数据的特征表示方法。

    

    数据领域之外的数据对基于人工智能的智能系统的安全性构成了重大威胁。本文旨在研究人工智能质量管理中的数据领域和离群数据，并提出使用深度学习技术进行特征表示，并开发一种用于离群数据检测的新的统计度量方法。首先，为了提取能够区分正常数据和离群数据的低维代表性特征，本文的研究将深度自动编码器（AE）架构与神经元激活状态相结合进行特征工程。然后，利用数据重构中的局部条件概率（LCP），开发了一种新的优越的统计度量方法来计算离群数据检测的得分。实验和评估在图像基准数据集和一个工业数据集上进行。通过与其他常见的离群数据检测的统计度量方法进行比较分析，证明了本研究在离群数据和人工智能方面是可行和有效的。

    Data outside the problem domain poses significant threats to the security of AI-based intelligent systems. Aiming to investigate the data domain and out-of-distribution (OOD) data in AI quality management (AIQM) study, this paper proposes to use deep learning techniques for feature representation and develop a novel statistical measure for OOD detection. First, to extract low-dimensional representative features distinguishing normal and OOD data, the proposed research combines the deep auto-encoder (AE) architecture and neuron activation status for feature engineering. Then, using local conditional probability (LCP) in data reconstruction, a novel and superior statistical measure is developed to calculate the score of OOD detection. Experiments and evaluations are conducted on image benchmark datasets and an industrial dataset. Through comparative analysis with other common statistical measures in OOD detection, the proposed research is validated as feasible and effective in OOD and AI
    
[^39]: Point-NeuS：通过体渲染进行点导向的神经隐式曲面重建

    Point-NeuS: Point-Guided Neural Implicit Surface Reconstruction by Volume Rendering. (arXiv:2310.07997v1 [cs.CV])

    [http://arxiv.org/abs/2310.07997](http://arxiv.org/abs/2310.07997)

    该论文提出了一种名为Point-NeuS的新方法，通过利用点导向机制实现准确和高效的神经隐式曲面重建，解决了当前方法精度有限和时间复杂度过高的问题。

    

    最近，通过体渲染学习神经隐式曲面已成为多视角重建的一种有前景的方法。然而，目前的方法存在精度有限和时间复杂度过高的瓶颈问题，亟需解决。为了应对这些挑战，我们提出了一种称为Point-NeuS的新方法，利用点导向机制实现准确和高效的重建。点建模有机地嵌入到体渲染中，以增强和规范隐式曲面的表示。具体而言，为了实现精确的点导向和抗噪能力，建模了点云的随机不确定性，以捕捉噪声分布并估计点的可靠性。此外，引入了一个连接点和图像的神经投影模块，以向符号距离函数（SDF）添加几何约束。为了更好地补偿体渲染和点建模之间的几何偏差，对高保真度点进行了滤波处理。

    Recently, learning neural implicit surface by volume rendering has been a promising way for multi-view reconstruction. However, limited accuracy and excessive time complexity remain bottlenecks that current methods urgently need to overcome. To address these challenges, we propose a new method called Point-NeuS, utilizing point-guided mechanisms to achieve accurate and efficient reconstruction. Point modeling is organically embedded into the volume rendering to enhance and regularize the representation of implicit surface. Specifically, to achieve precise point guidance and noise robustness, aleatoric uncertainty of the point cloud is modeled to capture the distribution of noise and estimate the reliability of points. Additionally, a Neural Projection module connecting points and images is introduced to add geometric constraints to the Signed Distance Function (SDF). To better compensate for geometric bias between volume rendering and point modeling, high-fidelity points are filtered i
    
[^40]: HeightFormer:一种多层交互和图像自适应分类回归网络，用于航空影像的单目高度估计

    HeightFormer: A Multilevel Interaction and Image-adaptive Classification-regression Network for Monocular Height Estimation with Aerial Images. (arXiv:2310.07995v1 [cs.CV])

    [http://arxiv.org/abs/2310.07995](http://arxiv.org/abs/2310.07995)

    本文提出了一种用于航空影像的全面解决方案，通过多层交互和图像自适应分类回归网络实现单目高度估计，解决了固定接受域和缺乏全局信息交互的问题。

    

    高度估计一直是测量和遥感学科中的一个重要主题，对于3D城市建模、MR和自动驾驶等努力至关重要。传统方法使用立体匹配或多传感器融合，这些都是成熟的技术，通常需要多个来自不同角度和附加传感器（如SAR）的图像，导致部署成本高。单图像高度估计已成为一种有吸引力的替代方法，具有更大的数据来源多样性和更简单的部署。然而，当前方法存在一些限制，例如固定的接受域、缺乏全局信息交互，导致 noticeable instance-level 高度偏差。高度预测的固有复杂性可能导致使用基于固定高度分割的主流回归方法时模糊估计物体边缘深度。本文提出了一种全面的解决方案，用于航空影像的单目高度估计。

    Height estimation has long been a pivotal topic within measurement and remote sensing disciplines, proving critical for endeavours such as 3D urban modelling, MR and autonomous driving. Traditional methods utilise stereo matching or multisensor fusion, both well-established techniques that typically necessitate multiple images from varying perspectives and adjunct sensors like SAR, leading to substantial deployment costs. Single image height estimation has emerged as an attractive alternative, boasting a larger data source variety and simpler deployment. However, current methods suffer from limitations such as fixed receptive fields, a lack of global information interaction, leading to noticeable instance-level height deviations. The inherent complexity of height prediction can result in a blurry estimation of object edge depth when using mainstream regression methods based on fixed height division. This paper presents a comprehensive solution for monocular height estimation in remote 
    
[^41]: 大型语言模型用于科学综合、推理和解释

    Large Language Models for Scientific Synthesis, Inference and Explanation. (arXiv:2310.07984v1 [cs.AI])

    [http://arxiv.org/abs/2310.07984](http://arxiv.org/abs/2310.07984)

    大型语言模型通过综合科学文献产生知识，能够在科学综合、推理和解释中发挥作用。

    

    大型语言模型是一种人工智能系统，其主要知识包括语言的统计模式、语义关系和句法结构。尽管它们的"知识"形式有限，但这些系统擅长各种复杂任务，包括创意写作、故事讲述、翻译、问答、总结和生成计算机代码。然而，它们尚未在自然科学领域展示出高级应用。在这里，我们展示了如何使用大型语言模型进行科学综合、推理和解释。我们提出了一种使用通用大型语言模型从通常与特殊用途机器学习算法相关联的科学数据集中进行推理的方法。我们展示了大型语言模型能够通过从科学文献中综合产生"知识"。当传统机器学习系统与这种合成和推理的知识相结合时，可以增强系统的性能。

    Large language models are a form of artificial intelligence systems whose primary knowledge consists of the statistical patterns, semantic relationships, and syntactical structures of language1. Despite their limited forms of "knowledge", these systems are adept at numerous complex tasks including creative writing, storytelling, translation, question-answering, summarization, and computer code generation. However, they have yet to demonstrate advanced applications in natural science. Here we show how large language models can perform scientific synthesis, inference, and explanation. We present a method for using general-purpose large language models to make inferences from scientific datasets of the form usually associated with special-purpose machine learning algorithms. We show that the large language model can augment this "knowledge" by synthesizing from the scientific literature. When a conventional machine learning system is augmented with this synthesized and inferred knowledge 
    
[^42]: 用于分析网络上的枪支走私活动的自监督视觉学习

    Self-supervised visual learning for analyzing firearms trafficking activities on the Web. (arXiv:2310.07975v1 [cs.CV])

    [http://arxiv.org/abs/2310.07975](http://arxiv.org/abs/2310.07975)

    本论文提出了一种自监督视觉学习方法，用于分析网络上的枪支走私活动。这种方法利用深度神经网络和卷积神经网络，通过从大规模通用数据集预训练，再在特定数据集上进行微调，实现对枪支的分类。

    

    从RGB图像中自动化地分类枪支是一项重要的现实任务，其应用范围涉及公共空间安全、情报收集和执法调查。当应用于从全球范围内大规模抓取的图像（包括社交媒体和暗网站）时，它可以作为分析开放源情报中的大数据的系统的重要组成部分，以试图识别犯罪枪支走私网络。深度神经网络（DNN）是实现这一目标的最先进方法，其中通常使用卷积神经网络（CNN）。常见的迁移学习方法包括在大规模的通用注释数据集（如ImageNet-1k）上进行预训练进行整体图像分类，然后在较小的、注释的、任务特定的下游数据集上对DNN进行微调以进行枪支视觉分类。

    Automated visual firearms classification from RGB images is an important real-world task with applications in public space security, intelligence gathering and law enforcement investigations. When applied to images massively crawled from the World Wide Web (including social media and dark Web sites), it can serve as an important component of systems that attempt to identify criminal firearms trafficking networks, by analyzing Big Data from open-source intelligence. Deep Neural Networks (DNN) are the state-of-the-art methodology for achieving this, with Convolutional Neural Networks (CNN) being typically employed. The common transfer learning approach consists of pretraining on a large-scale, generic annotated dataset for whole-image classification, such as ImageNet-1k, and then finetuning the DNN on a smaller, annotated, task-specific, downstream dataset for visual firearms classification. Neither Visual Transformer (ViT) neural architectures nor Self-Supervised Learning (SSL) approach
    
[^43]: 可解释的扩散模型通过信息分解

    Interpretable Diffusion via Information Decomposition. (arXiv:2310.07972v1 [cs.LG])

    [http://arxiv.org/abs/2310.07972](http://arxiv.org/abs/2310.07972)

    本研究通过观察扩散和信息分解之间的关系，揭示了扩散模型学习到的细粒度关系，进一步解决了高维空间中信息携带变量的问题。

    

    去噪扩散模型能够用于复杂关系的条件生成和密度建模，如图像和文本。然而，学习到的关系的本质是不透明的，因此很难准确理解单词和图像部分之间的关系，或者预测干预的效果。我们通过观察扩散和信息分解之间的精确关系，揭示了扩散模型学习到的细粒度关系。互信息和条件互信息的精确表达可以通过去噪模型来计算。此外，也可以轻松估计在特定图像和标题之间的关系。进一步对信息进行分解，以理解高维空间中哪些变量携带信息，是一个长期存在的问题。对于扩散模型，我们展示了一种自然的非负信息分解方法。

    Denoising diffusion models enable conditional generation and density modeling of complex relationships like images and text. However, the nature of the learned relationships is opaque making it difficult to understand precisely what relationships between words and parts of an image are captured, or to predict the effect of an intervention. We illuminate the fine-grained relationships learned by diffusion models by noticing a precise relationship between diffusion and information decomposition. Exact expressions for mutual information and conditional mutual information can be written in terms of the denoising model. Furthermore, pointwise estimates can be easily estimated as well, allowing us to ask questions about the relationships between specific images and captions. Decomposing information even further to understand which variables in a high-dimensional space carry information is a long-standing problem. For diffusion models, we show that a natural non-negative decomposition of mutu
    
[^44]: 一种新的自动形式化方法

    A New Approach Towards Autoformalization. (arXiv:2310.07957v1 [cs.CL])

    [http://arxiv.org/abs/2310.07957](http://arxiv.org/abs/2310.07957)

    该论文提出了一种新的方法来应对研究水平的数学自动形式化任务，通过将任务分解成更容易处理的子任务，包括未链接形式化、实体链接和类型调整。此外，还提出了一个用于未链接形式化的基准数据集 arXiv2Formal。

    

    验证数学证明是困难的，但可以通过计算机的辅助实现自动化。自动形式化是将自然语言数学自动转化为可以由程序验证的形式语言的任务。这是一项具有挑战性的任务，尤其对于研究论文中的高级数学来说。研究论文中的数学需要大量的背景和上下文。本文中，我们提出了一种应对研究水平数学自动形式化的方法，将任务分解为更易于处理的子任务：未链接形式化（包含未链接的定义和定理的形式化）、实体链接（链接到正确的定理和定义）以及调整类型以通过类型检查器。此外，我们还提出了arXiv2Formal，一个用于未链接形式化的基准数据集，其中包括从arXiv.org的论文中抽取的50个定理在Lean定理证明器中进行形式化。我们欢迎任何贡献。

    Verifying mathematical proofs is difficult, but can be automated with the assistance of a computer. Autoformalization is the task of automatically translating natural language mathematics into a formal language that can be verified by a program. This is a challenging task, and especially for higher-level mathematics found in research papers. Research paper mathematics requires large amounts of background and context. In this paper, we propose an avenue towards tackling autoformalization for research-level mathematics, by breaking the task into easier and more approachable subtasks: unlinked formalization (formalization with unlinked definitions and theorems), entity linking (linking to the proper theorems and definitions), and finally adjusting types so it passes the type checker. In addition, we present arXiv2Formal, a benchmark dataset for unlinked formalization consisting of 50 theorems formalized for the Lean theorem prover sampled from papers on arXiv.org. We welcome any contribut
    
[^45]: AutoRepo：一种用于多模态LLM自动建设报告的通用框架

    AutoRepo: A general framework for multi-modal LLM-based automated construction reporting. (arXiv:2310.07944v1 [cs.AI])

    [http://arxiv.org/abs/2310.07944](http://arxiv.org/abs/2310.07944)

    AutoRepo是一个通用框架，利用无人驾驶飞行器和多模态大型语言模型来自动生成建筑检查报告，提高了检查效率和资源利用率。

    

    确保建筑项目的安全、质量和及时完成至关重要，而建筑检查是实现这些目标的重要手段。然而，目前的检查方法主要是手动的，经常导致效率低下和信息管理不当。这种方法通常不能提供全面、全面的评估，因此容易引发监管疏忽和潜在的安全隐患。为解决这个问题，本文提出了一种名为AutoRepo的新框架，用于自动生成建筑检查报告。无人驾驶飞行器高效地进行建筑检查并收集场景信息，而多模态大型语言模型（LLMs）则用于自动生成检查报告。该框架在一个真实的建筑工地上应用和测试，展示了它加速检查流程、显著减少资源分配的潜力。

    Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocati
    
[^46]: Co-NavGPT: 使用大型语言模型的多机器人合作视觉语义导航

    Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models. (arXiv:2310.07937v1 [cs.RO])

    [http://arxiv.org/abs/2310.07937](http://arxiv.org/abs/2310.07937)

    Co-NavGPT是一个创新的框架，使用大型语言模型作为全局规划器，实现多机器人合作的视觉目标导航。在实验中表现出了超越现有模型的成功率和效率，展示了大型语言模型的巨大潜力。

    

    在高级人机交互任务中，对于自主机器人在未知环境中进行视觉目标导航至关重要。尽管过去已经开发了许多方法，但大多数都是设计用于单一机器人操作，这往往由于环境复杂性而导致效率和鲁棒性降低。此外，学习多机器人协作的策略需要资源密集型。为了解决这些挑战，我们提出了Co-NavGPT，这是一个创新的框架，将大型语言模型(LLMs)作为多机器人合作视觉目标导航的全局规划器。Co-NavGPT将探索的环境数据编码为提示，增强LLMs对场景的理解。然后，它为每个机器人分配探索前沿以实现高效的目标搜索。在Habitat-Matterport 3D (HM3D)上的实验结果表明，Co-NavGPT在成功率和效率方面超过了现有模型，而无需任何学习过程，展示了LLMs的巨大潜力。

    In advanced human-robot interaction tasks, visual target navigation is crucial for autonomous robots navigating unknown environments. While numerous approaches have been developed in the past, most are designed for single-robot operations, which often suffer from reduced efficiency and robustness due to environmental complexities. Furthermore, learning policies for multi-robot collaboration are resource-intensive. To address these challenges, we propose Co-NavGPT, an innovative framework that integrates Large Language Models (LLMs) as a global planner for multi-robot cooperative visual target navigation. Co-NavGPT encodes the explored environment data into prompts, enhancing LLMs' scene comprehension. It then assigns exploration frontiers to each robot for efficient target search. Experimental results on Habitat-Matterport 3D (HM3D) demonstrate that Co-NavGPT surpasses existing models in success rates and efficiency without any learning process, demonstrating the vast potential of LLMs
    
[^47]: 你关心什么？为机器人学习实现视觉表示对齐

    What Matters to You? Towards Visual Representation Alignment for Robot Learning. (arXiv:2310.07932v1 [cs.RO])

    [http://arxiv.org/abs/2310.07932](http://arxiv.org/abs/2310.07932)

    该论文提出了一种名为RAPL的方法，用于解决机器人学习中的视觉表示对齐问题，通过利用人类反馈将机器人的视觉表示与用户偏好对齐，并区分任务的关键要素。

    

    在为人类服务时，机器人需要优化与最终用户偏好一致的奖励。由于机器人将依赖原始感知输入如RGB图像，它们的奖励将不可避免地使用视觉表示。最近，使用预训练视觉模型的表示引发了人们的兴趣，但在机器人领域使其起作用的关键是微调，通常通过代理任务如动力学预测或强制时间循环一致性来完成。然而，所有这些代理任务都绕过了人类对自己关心的事物的输入，加剧了虚假关联，并最终导致机器人的行为与用户偏好不一致。在这项工作中，我们提议机器人应该利用人类的反馈来与最终用户的视觉表示对齐，并区分任务的关键要素。我们提出了一种解决视觉表示对齐问题和视觉奖励问题的方法，即基于偏好的表示对齐学习（RAPL）方法。

    When operating in service of people, robots need to optimize rewards aligned with end-user preferences. Since robots will rely on raw perceptual inputs like RGB images, their rewards will inevitably use visual representations. Recently there has been excitement in using representations from pre-trained visual models, but key to making these work in robotics is fine-tuning, which is typically done via proxy tasks like dynamics prediction or enforcing temporal cycle-consistency. However, all these proxy tasks bypass the human's input on what matters to them, exacerbating spurious correlations and ultimately leading to robot behaviors that are misaligned with user preferences. In this work, we propose that robots should leverage human feedback to align their visual representations with the end-user and disentangle what matters for the task. We propose Representation-Aligned Preference-based Learning (RAPL), a method for solving the visual representation alignment problem and visual reward
    
[^48]: D2修剪：信息传递平衡数据修剪中的多样性和困难

    D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning. (arXiv:2310.07931v1 [cs.LG])

    [http://arxiv.org/abs/2310.07931](http://arxiv.org/abs/2310.07931)

    D2修剪是一种平衡数据多样性和困难度的方法，在coreset选择中同时考虑数据多样性和重要性评分。

    

    分析理论表明，在固定数据预算上训练的模型中，更高质量的数据可以导致更低的测试错误。此外，如果数据集可以剥离冗余项，则可以在较低的计算预算上训练模型而不降低性能。Coreset选择（或数据修剪）寻求选择训练数据的子集，以最大程度地提高在该子集上训练的模型的性能，也称为coreset。有两种主要方法：（1）基于几何的数据选取，以最大程度地提高coreset中的数据多样性，和（2）根据训练动态为样本分配困难度分数的函数。为数据多样性进行优化会导致偏向较容易样本的coreset，而难度排名选择会忽略深度学习模型训练所必需的容易样本。这表明数据多样性和重要性评分是两个互补因素，在coreset选择中需要同时考虑。

    Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset sel
    
[^49]: 上下文化政策恢复：通过自适应模仿学习对医疗决策进行建模和解释

    Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v1 [cs.LG])

    [http://arxiv.org/abs/2310.07918](http://arxiv.org/abs/2310.07918)

    本论文提出了一种上下文化政策恢复方法用于建模复杂的医疗决策过程，以解决现有模型在准确性和可解释性之间的权衡问题。该方法将决策策略拆分为上下文特定策略，通过多任务学习来实现建模，并提供复杂行为的简洁描述。

    

    可解释的策略学习旨在从观察到的行为中估计可理解的决策策略；然而，现有模型在准确性和可解释性之间存在权衡。这种权衡限制了基于数据驱动的对人类决策过程的解释，例如，审计医疗决策的偏见和次优实践，我们需要决策过程的模型，能够提供复杂行为的简洁描述。现有方法基本上由于将潜在决策过程表示为通用策略而负担了这种权衡，而实际上人类决策是动态的，可以随上下文信息而大幅改变。因此，我们提出了上下文化政策恢复（CPR），将建模复杂决策过程的问题重新定义为多任务学习问题，其中复杂决策策略由特定上下文的策略组成。CPR将每个上下文特定策略建模为线性的观察-动作映射

    Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. CPR models each context-specific policy as a linear observation-to-action mapp
    
[^50]: 在非平衡数据和未来趋势中的机器学习技术综述

    A Review of Machine Learning Techniques in Imbalanced Data and Future Trends. (arXiv:2310.07917v1 [cs.LG])

    [http://arxiv.org/abs/2310.07917](http://arxiv.org/abs/2310.07917)

    该论文综述了在非平衡数据中使用的各种机器学习方法，并提供了一个通用指南，旨在帮助研究人员在大规模非平衡数据中进行机器学习。

    

    在过去的二十年里，检测罕见事件一直是数据挖掘和机器学习领域的一个挑战性任务。现实生活中的问题激发了研究人员进一步改进数据处理和算法方法，以实现有效和计算效率高的非平衡学习方法。本论文收集和审查了258篇来自期刊和会议论文的同行评审论文，旨在从技术和应用角度深入审查非平衡学习中的各种方法。该工作旨在为在学术界或工业界希望深入学习大规模非平衡数据下的机器学习领域的研究人员提供一个结构化的方法综述，并为他们提供一个通用指南。

    For over two decades, detecting rare events has been a challenging task among researchers in the data mining and machine learning domain. Real-life problems inspire researchers to navigate and further improve data processing and algorithmic approaches to achieve effective and computationally efficient methods for imbalanced learning. In this paper, we have collected and reviewed 258 peer-reviewed papers from archival journals and conference papers in an attempt to provide an in-depth review of various approaches in imbalanced learning from technical and application perspectives. This work aims to provide a structured review of methods used to address the problem of imbalanced data in various domains and create a general guideline for researchers in academia or industry who want to dive into the broad field of machine learning using large-scale imbalanced data.
    
[^51]: 循环网络通过低维振荡识别模式

    Recurrent networks recognize patterns with low-dimensional oscillations. (arXiv:2310.07908v1 [q-bio.NC])

    [http://arxiv.org/abs/2310.07908](http://arxiv.org/abs/2310.07908)

    本研究提出了一种通过相位变化识别模式的循环神经网络机制，并通过验证手工制作的振荡模型证实了这一解释。该研究不仅提供了一种潜在的动力学机制用于模式识别，还暗示了有限状态自动机的神经实现方式，并且对深度学习模型的可解释性进行了贡献。

    

    本研究提出了一种通过解释在SET卡牌游戏启发下进行训练的循环神经网络(RNN)在简单任务上的动力学机制来识别模式。我们将训练后的RNN解释为通过低维极限环中的相位变化进行模式识别，类似于有限状态自动机(FSA)中的转换。我们进一步通过手工制作一个简单的振荡模型来验证了这一解释，该模型复制了训练后的RNN的动力学特性。我们的发现不仅暗示了一种潜在的动力学机制能够实现模式识别，还暗示了一种有限状态自动机的潜在神经实现。最重要的是，这项工作有助于关于深度学习模型可解释性的讨论。

    This study proposes a novel dynamical mechanism for pattern recognition discovered by interpreting a recurrent neural network (RNN) trained on a simple task inspired by the SET card game. We interpreted the trained RNN as recognizing patterns via phase shifts in a low-dimensional limit cycle in a manner analogous to transitions in a finite state automaton (FSA). We further validated this interpretation by handcrafting a simple oscillatory model that reproduces the dynamics of the trained RNN. Our findings not only suggest of a potential dynamical mechanism capable of pattern recognition, but also suggest of a potential neural implementation of FSA. Above all, this work contributes to the growing discourse on deep learning model interpretability.
    
[^52]: RoboCLIP：只需要一次演示即可学习机器人策略

    RoboCLIP: One Demonstration is Enough to Learn Robot Policies. (arXiv:2310.07899v1 [cs.AI])

    [http://arxiv.org/abs/2310.07899](http://arxiv.org/abs/2310.07899)

    RoboCLIP是一种在线模仿学习方法，通过单个视频演示或文本描述生成奖励函数，不需要专家监督或设计奖励函数。同时，它还可以利用领域外的演示进行训练。

    

    奖励规范是强化学习中一个非常困难的问题，需要大量的专家监督来设计稳健的奖励函数。模仿学习（IL）方法试图通过利用专家演示来规避这些问题，但通常需要大量领域内的专家演示。受到视频语言模型（VLMs）领域的进展的启发，我们提出了RoboCLIP，一种在线模仿学习方法，使用单个演示（克服了大数据要求）来生成奖励，演示可以是视频演示或任务的文本描述，而无需手动设计奖励函数。此外，RoboCLIP还可以利用领域外的演示，例如人类解决任务的视频，以生成奖励，从而避免了需要相同演示和部署领域的要求。RoboCLIP利用预训练的VLMs进行奖励生成，无需任何微调。通过RoboCLIP训练的强化学习代理

    Reward specification is a notoriously difficult problem in reinforcement learning, requiring extensive expert supervision to design robust reward functions. Imitation learning (IL) methods attempt to circumvent these problems by utilizing expert demonstrations but typically require a large number of in-domain expert demonstrations. Inspired by advances in the field of Video-and-Language Models (VLMs), we present RoboCLIP, an online imitation learning method that uses a single demonstration (overcoming the large data requirement) in the form of a video demonstration or a textual description of the task to generate rewards without manual reward function design. Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like videos of humans solving the task for reward generation, circumventing the need to have the same demonstration and deployment domains. RoboCLIP utilizes pretrained VLMs without any finetuning for reward generation. Reinforcement learning agents trained with
    
[^53]: 高效扩散生成模型的积分器

    Efficient Integrators for Diffusion Generative Models. (arXiv:2310.07894v1 [cs.LG])

    [http://arxiv.org/abs/2310.07894](http://arxiv.org/abs/2310.07894)

    本文提出了共轭积分器和分裂积分器两种框架，用于加速预训练模型中的样本生成。经过实证和理论研究，提出了一种最佳性能的混合方法，能够应用于扩散生成模型。

    

    扩散模型在推理时间生成样本速度较慢。因此，为更广泛的扩散模型开发快速确定性/随机采样的原则性框架是一个有希望的方向。我们提出了两种互补的加速预训练模型样本生成的框架：共轭积分器和分裂积分器。共轭积分器将DDIM泛化，将反向扩散动力学映射到更易于采样的空间。相反，基于分裂的积分器，常用于分子动力学，通过巧妙地在涉及数据和辅助变量的数值更新之间交替，减少了数值模拟误差。经过广泛的实证和理论研究，我们提出了一种混合方法，该方法在增强空间中获得了报告的扩散模型的最佳性能。我们在CIFAR-10上应用相空间朗之万扩散[Pandey＆Mandt，2023]，我们的确定性和随机样本生成获得了最好的性能。

    Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and stochastic samp
    
[^54]: LangNav: 语言作为导航的感知表示

    LangNav: Language as a Perceptual Representation for Navigation. (arXiv:2310.07889v1 [cs.CV])

    [http://arxiv.org/abs/2310.07889](http://arxiv.org/abs/2310.07889)

    该论文探索了将语言作为导航的感知表示，并使用现成的视觉系统将每个时间步骤的视图转化为自然语言描述，通过微调预训练的语言模型选择最佳的行动来满足导航指令。有两种用例的实验对这种基于语言的导航方法进行了探索：使用大型语言模型生成合成轨迹以微调较小的语言模型，以及模拟到实际的转换。

    

    我们探索将语言作为视觉与语言导航的感知表示。我们的方法使用现成的视觉系统（用于图像字幕和物体检测）将每个时间步骤中代理人的自我中心全景视图转化为自然语言描述。然后，我们微调预训练的语言模型，根据当前视图和轨迹历史选择最佳的行动来满足导航指令。与标准设置相比，标准设置将预训练的语言模型适应于与预训练的视觉模型连续视觉特征直接配合使用。我们的方法使用（离散的）语言作为感知表示。我们在R2R视觉与语言导航基准测试中探索了两个用例：使用大型语言模型（GPT-4）生成合成轨迹，以便微调较小的语言模型；以及模拟到实际的转换。

    We explore the use of language as a perceptual representation for vision-and-language navigation. Our approach uses off-the-shelf vision systems (for image captioning and object detection) to convert an agent's egocentric panoramic view at each time step into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation. We explore two use cases of our language-based navigation (LangNav) approach on the R2R vision-and-language navigation benchmark: generating synthetic trajectories from a prompted large language model (GPT-4) with which to finetune a smaller language model; and sim-to-real transfer where we transfer 
    
[^55]: 基于复杂群体的局部误差信号启发的领导者-跟随者神经网络

    Leader-Follower Neural Networks with Local Error Signals Inspired by Complex Collectives. (arXiv:2310.07885v1 [cs.LG])

    [http://arxiv.org/abs/2310.07885](http://arxiv.org/abs/2310.07885)

    领导者-跟随者神经网络是受到自然群体集合中观察到的规则启发的一种神经网络架构，利用局部误差信号训练并可选择性地结合反向传播和全局损失。通过大量实验研究工作人员行为和评估，这种神经网络具有高度的自组织和复杂性。

    

    网络中具有异质、资源有限的信息处理单元（例如，一群鱼、一群鸟或一组神经元网络）的集体行为表现出高度的自组织和复杂性。这些 emergent properties是通过简单的相互作用规则产生的，其中某些个体能够表现出领导行为并影响群体的活动。受到这些群体的复杂性的启发，我们提出了一种神经网络（NN）架构，该架构受到自然群体集合中观察到的规则的启发。这个NN结构包含了包含一个或多个信息处理单元（如神经元、滤波器、层或层块）的工作人员。工作人员可以是领导者或跟随者，我们通过利用局部误差信号训练了一个领导者-跟随者神经网络（LFNN），并选择性地结合反向传播（BP）和全局损失。我们通过大量的实验研究工作人员的行为并评估了LFNN。我们训练了使用局部误差信号的LFNNs，可选择性地将反向传播和全局损失结合在一起。

    The collective behavior of a network with heterogeneous, resource-limited information processing units (e.g., group of fish, flock of birds, or network of neurons) demonstrates high self-organization and complexity. These emergent properties arise from simple interaction rules where certain individuals can exhibit leadership-like behavior and influence the collective activity of the group. Motivated by the intricacy of these collectives, we propose a neural network (NN) architecture inspired by the rules observed in nature's collective ensembles. This NN structure contains workers that encompass one or more information processing units (e.g., neurons, filters, layers, or blocks of layers). Workers are either leaders or followers, and we train a leader-follower neural network (LFNN) by leveraging local error signals and optionally incorporating backpropagation (BP) and global loss. We investigate worker behavior and evaluate LFNNs through extensive experimentation. Our LFNNs trained wit
    
[^56]: 可解释人工智能在机器学习生命周期中的众多面貌：工业现实与当前研究状况

    The Thousand Faces of Explainable AI Along the Machine Learning Life Cycle: Industrial Reality and Current State of Research. (arXiv:2310.07882v1 [cs.LG])

    [http://arxiv.org/abs/2310.07882](http://arxiv.org/abs/2310.07882)

    本文调查了可解释人工智能（XAI）在生产行业中的实际相关性，并将其与当前学术界XAI研究的现状相联系。通过对广泛的访谈和文献回顾的分析，发现了访谈结果与当前的研究方法之间存在差异。

    

    本文研究了可解释人工智能（XAI）在生产行业中的实际相关性，并将其与当前学术界XAI研究的现状相联系。我们的发现基于对当前工业实践中XAI在机器学习（ML）生命周期中的角色和适用性以及未来的预期重要性进行了广泛的访谈。访谈对象包括来自不同行业部门的各种角色和关键利益相关者。此外，我们通过提供对相关文献的简明回顾，概述了XAI研究的现状。这使我们能够提供一个全面的概述，涵盖了被调查者的意见以及当前学术研究的现状。通过将我们的访谈结果与当前的研究方法进行比较，我们揭示了几个差异。尽管存在许多不同的XAI方法，但大多数方法都集中在...

    In this paper, we investigate the practical relevance of explainable artificial intelligence (XAI) with a special focus on the producing industries and relate them to the current state of academic XAI research. Our findings are based on an extensive series of interviews regarding the role and applicability of XAI along the Machine Learning (ML) lifecycle in current industrial practice and its expected relevance in the future. The interviews were conducted among a great variety of roles and key stakeholders from different industry sectors. On top of that, we outline the state of XAI research by providing a concise review of the relevant literature. This enables us to provide an encompassing overview covering the opinions of the surveyed persons as well as the current state of academic research. By comparing our interview results with the current research approaches we reveal several discrepancies. While a multitude of different XAI approaches exists, most of them are centered around the
    
[^57]: DeePref: 在内容交付网络中的视频预取中应用深度强化学习

    DeePref: Deep Reinforcement Learning For Video Prefetching In Content Delivery Networks. (arXiv:2310.07881v1 [cs.NI])

    [http://arxiv.org/abs/2310.07881](http://arxiv.org/abs/2310.07881)

    本论文介绍了DeePref，一种用于在内容交付网络中进行在线视频内容预取的深度强化学习代理。传统的预取技术难以适应工作负载中的变化，而DeePref利用强化学习算法，可以自动适应不断变化的用户访问模式。

    

    内容交付网络承载了大部分的互联网流量，对视频内容的需求不断增加使得缓存和预取优化算法的重要性日益突出。预取的目标是在请求者发出请求之前将数据提前放入缓存中，以减少访问时间并改善用户体验。传统的预取技术适应特定的访问模式，但很难适应工作负载中的突变或随机化。本文探讨了使用强化学习来应对用户访问模式的变化，并随时间自动适应的方法。为此，我们提出了DeePref，一种用于在内容交付网络中在线视频内容预取的深度强化学习代理。

    Content Delivery Networks carry the majority of Internet traffic, and the increasing demand for video content as a major IP traffic across the Internet highlights the importance of caching and prefetching optimization algorithms. Prefetching aims to make data available in the cache before the requester places its request to reduce access time and improve the Quality of Experience on the user side. Prefetching is well investigated in operating systems, compiler instructions, in-memory cache, local storage systems, high-speed networks, and cloud systems. Traditional prefetching techniques are well adapted to a particular access pattern, but fail to adapt to sudden variations or randomization in workloads. This paper explores the use of reinforcement learning to tackle the changes in user access patterns and automatically adapt over time. To this end, we propose, DeePref, a Deep Reinforcement Learning agent for online video content prefetching in Content Delivery Networks. DeePref is a pr
    
[^58]: TabLib：一个包含上亿表格和上百亿上下文的数据集

    TabLib: A Dataset of 627M Tables with Context. (arXiv:2310.07875v1 [cs.CL])

    [http://arxiv.org/abs/2310.07875](http://arxiv.org/abs/2310.07875)

    TabLib是一个包含上亿表格和上百亿上下文的数据集，规模和多样性使其在表格模态下具有巨大的潜力。

    

    巨大多样的数据集在提升现代AI系统在文本和图像模态下的性能方面起着关键作用。然而，对于表格数据，目前还没有与文本和图像可比拟的规模和多样性的数据集。因此，我们提出了"TabLib"，这是一个由6.27亿个表格和86.7亿个上下文令牌总共达到69 TiB的编译数据集。TabLib的数据来自多个文件格式，包括CSV、HTML、SQLite、PDF、Excel等，这些数据源自GitHub和Common Crawl。TabLib的规模和多样性在表格模态下具有巨大的潜力，如同原始的用于文本和图像的基础数据集，例如The Pile和LAION。

    It is well-established that large, diverse datasets play a pivotal role in the performance of modern AI systems for text and image modalities. However, there are no datasets for tabular data of comparable size and diversity to those available for text and images. Thus we present "TabLib'', a compilation of 627 million tables totaling 69 TiB, along with 867B tokens of context. TabLib was extracted from numerous file formats, including CSV, HTML, SQLite, PDF, Excel, and others, sourced from GitHub and Common Crawl. The size and diversity of TabLib offer considerable promise in the table modality, reminiscent of the original promise of foundational datasets for text and images, such as The Pile and LAION.
    
[^59]: 基于多模态电子健康记录的层次预训练方法

    Hierarchical Pretraining on Multimodal Electronic Health Records. (arXiv:2310.07871v1 [cs.AI])

    [http://arxiv.org/abs/2310.07871](http://arxiv.org/abs/2310.07871)

    该论文提出了一种针对层次多模态电子健康记录数据的新颖、通用且统一的预训练框架MEDHMP，在三个级别上展示了其有效性，并且在与十八个基准方法的比较中验证了其高效性。

    

    预训练在自然语言处理中已被证明是一种强大的技术，在各种下游任务中取得了显著的成功。然而，在医学领域，现有的电子健康记录预训练模型无法捕捉到健康记录数据的层次性，限制了它们在使用单个预训练模型跨多个下游任务的泛化能力。为了解决这个挑战，本文介绍了一种新颖、通用且统一的预训练框架MEDHMP，专门针对层次多模态的健康记录数据进行设计。通过实验结果在三个级别上展示了所提出的MEDHMP的有效性，并通过与十八个基准模型的比较进一步突出了我们方法的有效性。

    Pretraining has proven to be a powerful technique in natural language processing (NLP), exhibiting remarkable success in various NLP downstream tasks. However, in the medical domain, existing pretrained models on electronic health records (EHR) fail to capture the hierarchical nature of EHR data, limiting their generalization capability across diverse downstream tasks using a single pretrained model. To tackle this challenge, this paper introduces a novel, general, and unified pretraining framework called MEDHMP, specifically designed for hierarchically multimodal EHR data. The effectiveness of the proposed MEDHMP is demonstrated through experimental results on eight downstream tasks spanning three levels. Comparisons against eighteen baselines further highlight the efficacy of our approach.
    
[^60]: 廉价对话算法

    Cheap Talking Algorithms. (arXiv:2310.07867v1 [econ.TH])

    [http://arxiv.org/abs/2310.07867](http://arxiv.org/abs/2310.07867)

    该论文研究了在战略信息传递游戏中，利用独立强化学习算法进行训练的发送者和接收者可以收敛到接近最优均衡策略，并且在代理之间的利益冲突下实现了最大化的通信。这一结论稳健，并对信息传递游戏中的均衡选择理论、计算机科学中的算法间通信和人工智能代理市场中的经济学产生了影响。

    

    我们模拟独立的强化学习算法在克劳福德和索贝尔（1982）的战略信息传递游戏中的行为。我们表明，一个发送者和一个接收者一起进行训练，收敛到接近游戏先验最优均衡的策略。因此，通信在与代理之间的利益冲突程度给出的纳什均衡下，按照最大程度进行。这一结论对超参数和游戏的备选规范稳健。我们讨论了信息传递游戏中均衡选择理论、计算机科学中算法间新兴通信工作以及由人工智能代理组成的市场中的宫斗经济学的影响。

    We simulate behaviour of independent reinforcement learning algorithms playing the Crawford and Sobel (1982) game of strategic information transmission. We show that a sender and a receiver training together converge to strategies close to the exante optimal equilibrium of the game. Hence, communication takes place to the largest extent predicted by Nash equilibrium given the degree of conflict of interest between agents. The conclusion is shown to be robust to alternative specifications of the hyperparameters and of the game. We discuss implications for theories of equilibrium selection in information transmission games, for work on emerging communication among algorithms in computer science and for the economics of collusions in markets populated by artificially intelligent agents.
    
[^61]: 使用大型语言模型生成合成数据用于文本分类：潜力和限制

    Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations. (arXiv:2310.07849v1 [cs.CL])

    [http://arxiv.org/abs/2310.07849](http://arxiv.org/abs/2310.07849)

    本研究旨在探讨使用大型语言模型生成合成数据在文本分类模型训练中的潜力和限制。研究结果发现，主观性会负面影响模型在合成数据上的性能。这对于理解和利用合成数据的有效性具有重要的启示作用。

    

    收集和整理高质量的训练数据对于开发具有卓越性能的文本分类模型至关重要，但往往伴随着巨大的成本和时间投入。研究人员最近开始探索使用大型语言模型（LLMs）生成合成数据集作为一种替代方法。然而，LLM生成的合成数据在支持模型训练方面的有效性在不同的分类任务中是不一致的。为了更好地了解调节LLM生成的合成数据有效性的因素，本研究探讨了在分类的主观性如何影响在合成数据上训练的模型的性能。我们的研究结果表明，主观性在任务层面和实例层面上都与在合成数据上训练的模型的性能呈负相关。最后，我们讨论了我们的工作对于利用LLM来生成合成数据在潜力和限制方面的影响。

    The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM fo
    
[^62]: 探索有限领域知识传递的基本限制

    Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])

    [http://arxiv.org/abs/2310.07838](http://arxiv.org/abs/2310.07838)

    本论文研究了在有限领域中从教师到学生分类器进行知识传递的统计效率，发现特权信息会加速传递，通过使用一种新颖的损失函数达到了知识传递的基本限制。

    

    我们对通过从教师到概率化学生分类器的n个样本进行知识传递的统计效率进行了表征，其中输入空间S和标签A为有限域。我们发现，在三个渐进级别上的特权信息可以加快传递的速度。在第一级别上，只有具有困难标签的样本是已知的，最大似然估计器能够达到最小化速率sqrt(|S||A|/n)。第二级别上，除了已知的困难标签样本外，还有采样标签的教师概率可用，这将收敛速度的下界提高到|S||A|/n。然而，在第二个数据采集协议下，最小化交叉熵损失的朴素适应会导致渐近偏差的学生。我们克服了这个限制，并通过使用一种新颖的经验变体的平方误差逻辑损失来实现了基本限制。第三级别进一步赋予学生软标签。

    We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (com
    
[^63]: 何时，为什么以及多少？通过细化进行的自适应学习率调度

    When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement. (arXiv:2310.07831v1 [cs.LG])

    [http://arxiv.org/abs/2310.07831](http://arxiv.org/abs/2310.07831)

    该论文介绍了一种通过细化分析学习率调度来解决实践中学习率调整与理论的不一致的方法，通过对观察到的梯度范数进行分析，得到了适应于特定任务的细化调度。该方法能够改善优化算法的收敛性能。

    

    实践中使用的学习率调度与理论推荐的几乎完全不同。我们缩小了大部分理论与实践之间的差距，并因此能够推导出新的问题自适应学习率调度。我们的关键技术贡献是对广泛类别的优化算法（包括SGD）的学习率调度进行细化分析。与大多数前期研究只研究平均迭代的收敛性不同，我们研究最后一次迭代，这是大多数人在实践中使用的。当仅考虑最坏情况分析时，我们的理论预测最佳选择是线性衰减调度：这是一种实践中常用的选择，其将步长与当前迭代次数t和总步数T成比例地设置为1 - t/T。为了超越这种最坏情况分析，我们使用观察到的梯度范数来推导适应于特定任务的细化调度。这些细化调度表现出学习率逐渐增加和学习率迅速退火。

    Learning rate schedules used in practice bear little resemblance to those recommended by theory. We close much of this theory/practice gap, and as a consequence are able to derive new problem-adaptive learning rate schedules. Our key technical contribution is a refined analysis of learning rate schedules for a wide class of optimization algorithms (including SGD). In contrast to most prior works that study the convergence of the average iterate, we study the last iterate, which is what most people use in practice. When considering only worst-case analysis, our theory predicts that the best choice is the linear decay schedule: a popular choice in practice that sets the stepsize proportionally to $1 - t/T$, where $t$ is the current iteration and $T$ is the total number of steps. To go beyond this worst-case analysis, we use the observed gradient norms to derive schedules refined for any particular task. These refined schedules exhibit learning rate warm-up and rapid learning rate anneali
    
[^64]: 合成数据是否能使大型语言模型更高效？

    Does Synthetic Data Make Large Language Models More Efficient?. (arXiv:2310.07830v1 [cs.CL])

    [http://arxiv.org/abs/2310.07830](http://arxiv.org/abs/2310.07830)

    本文探讨了在NLP中合成数据生成的细微差别，重点关注基于模板的问题生成。通过评估其优势和固有限制，本研究揭示了合成数据对现代Transformer模型性能的影响，并强调了合成数据与真实世界数据之间所需的微妙平衡。

    

    随着深度学习方法的出现，自然语言处理(NLP)经历了深刻的变革。研究人员持续面临的一个挑战是驱动这些模型的高质量标注数据集的稀缺。本文探讨了在NLP中合成数据生成的细微差别，重点关注基于模板的问题生成。通过评估其优势，包括数据扩充潜力和结构多样性的引入，我们将这些优点与固有限制进行了对比，如过拟合风险和预定义模板所带来的限制。通过经验评估，我们展示了基于模板的合成数据对现代Transformer模型性能的影响。我们通过强调合成数据和真实世界数据之间所需的微妙平衡及将合成数据整合到模型训练流程中的未来轨迹来总结。这些发现旨在指导NLP从业者。

    Natural Language Processing (NLP) has undergone transformative changes with the advent of deep learning methodologies. One challenge persistently confronting researchers is the scarcity of high-quality, annotated datasets that drive these models. This paper explores the nuances of synthetic data generation in NLP, with a focal point on template-based question generation. By assessing its advantages, including data augmentation potential and the introduction of structured variety, we juxtapose these benefits against inherent limitations, such as the risk of overfitting and the constraints posed by pre-defined templates. Drawing from empirical evaluations, we demonstrate the impact of template-based synthetic data on the performance of modern transformer models. We conclude by emphasizing the delicate balance required between synthetic and real-world data, and the future trajectories of integrating synthetic data in model training pipelines. The findings aim to guide NLP practitioners in
    
[^65]: 探索大型语言模型中类比识别与句子结构编码之间的关系

    Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models. (arXiv:2310.07818v1 [cs.CL])

    [http://arxiv.org/abs/2310.07818](http://arxiv.org/abs/2310.07818)

    这项研究探究了大型语言模型中识别句子类比的能力与其编码句法和语义结构能力之间的关系。

    

    识别类比在人类认知和语言能力中起着重要作用。在过去的十年里，对于“A对B就像C对D”这种形式的词语类比进行了广泛的研究。然而，对于涉及更长文本的类比，如句子和句子集合，传达类比意义的问题引起了越来越多的兴趣。当前的自然语言处理研究社区评估大型语言模型（LLMs）识别此类类比的能力，但这些能力背后的原因需要进一步探究。此外，LLMs在其嵌入中编码语言的句法和语义结构的能力，近年来得到了显著关注。在这项工作中，我们研究了多个LLMs识别句子类比的能力与其编码句法和语义结构的能力之间的关系。通过分析，我们发现LLMs的类比识别能力与其编码能力有关。

    Identifying analogies plays a pivotal role in human cognition and language proficiency. In the last decade, there has been extensive research on word analogies in the form of ``A is to B as C is to D.'' However, there is a growing interest in analogies that involve longer text, such as sentences and collections of sentences, which convey analogous meanings. While the current NLP research community evaluates the ability of Large Language Models (LLMs) to identify such analogies, the underlying reasons behind these abilities warrant deeper investigation. Furthermore, the capability of LLMs to encode both syntactic and semantic structures of language within their embeddings has garnered significant attention with the surge in their utilization. In this work, we examine the relationship between the abilities of multiple LLMs to identify sentence analogies, and their capacity to encode syntactic and semantic structures. Through our analysis, we find that analogy identification ability of LL
    
[^66]: 使用LabGym人工智能自动识别日本猕猴的石头处理行为

    Automatic Identification of Stone-Handling Behaviour in Japanese Macaques Using LabGym Artificial Intelligence. (arXiv:2310.07812v1 [cs.CV])

    [http://arxiv.org/abs/2310.07812](http://arxiv.org/abs/2310.07812)

    这项研究使用LabGym人工智能开发了一个模型，能够准确地识别日本猕猴的石头处理行为，为灵长类学领域的行为分析提供了新的解决方案。

    

    人工智能技术的最新进展为复杂行为的分析打开了新的可能。在此背景下，行为学家正在积极探索这些创新技术在使用视频数据进行行为分析的耗时过程中的潜力。在灵长类学领域，已经开发了几种用于此目的的工具。然而，每个工具都面临技术限制，而我们的目标是克服这些限制。为了解决这些限制，我们建立了一套全面的协议，旨在发挥先进工具LabGym的功能。我们的主要目标是评估LabGym在研究猕猴行为方面的适用性，重点关注日本猕猴作为我们的模型对象。我们成功地开发了一个模型，能够高准确度地检测日本猕猴的石头处理行为。我们的行为分析模型如预期完成，并且LabGym取得了成功。

    The latest advancements in artificial intelligence technology have opened doors to the analysis of intricate behaviours. In light of this, ethologists are actively exploring the potential of these innovations to streamline the time-intensive process of behavioural analysis using video data. In the realm of primatology, several tools have been developed for this purpose. Nonetheless, each of these tools grapples with technical constraints that we aim to surmount. To address these limitations, we have established a comprehensive protocol designed to harness the capabilities of a cutting-edge tool, LabGym. Our primary objective was to evaluate LabGym's suitability for the analysis of primate behaviour, with a focus on Japanese macaques as our model subjects. We have successfully developed a model that demonstrates a high degree of accuracy in detecting Japanese macaques stone-handling behaviour. Our behavioural analysis model was completed as per our initial expectations and LabGym succee
    
[^67]: 具有相位随机桥的生成建模

    Generative Modeling with Phase Stochastic Bridges. (arXiv:2310.07805v1 [cs.LG])

    [http://arxiv.org/abs/2310.07805](http://arxiv.org/abs/2310.07805)

    通过在相位空间中构建路径测度，我们提出了一种新颖的生成建模框架，可以在动力传播的早期阶段生成逼真的数据点，并利用额外的速度信息实现高效的数据生成。

    

    扩散模型（DMs）是用于连续输入的最先进的生成模型。DMs通过在输入空间（即位置空间）中构建随机微分方程（SDE），并使用神经网络进行反演来工作。在这项工作中，我们介绍了一种基于相位空间动力学的新型生成建模框架，其中相位空间被定义为一个包括位置和速度的增强空间。利用随机最优控制的洞察力，我们构建了相位空间中的路径测度，实现了高效的采样。与DMs相比，我们的框架在动力传播的早期阶段就能够生成逼真的数据点。这种早期预测为通过沿轨迹利用额外的速度信息实现高效的数据生成奠定了基础。在标准图像生成基准测试中，我们的模型在小函数评估数量的范围内表现出优秀的性能。

    Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluation
    
[^68]: 幽默的一般机制：重新定义语义重叠

    A general mechanism of humor: reformulating the semantic overlap. (arXiv:2310.07803v1 [cs.CL])

    [http://arxiv.org/abs/2310.07803](http://arxiv.org/abs/2310.07803)

    本文提出了一种普适性的认知幽默机制，建立在约束的概念上，通过重叠约束的观察来解释幽默的产生。

    

    本文提出一种普适性的认知幽默机制，不仅限于语言交流。它借鉴了Raskin关于情节重叠的概念，并符合不一致性-解决理论框架，但是建立在约束的概念上，即数据集之间的抽象对应关系。根据这种观点，情节重叠是一种更抽象描述的现象，即约束重叠。文中引入了被忽视的论证概念，用于描述两个重叠的约束——明显的和隐蔽的。它们的输入和输出并不直接编码在话语中，而是由话语暗示，它们的重叠导致在交流的话语层面上产生另一种重叠，这种不一致性显露了出来。我们的假设是，唤起这种约束是听众解释话语的推理过程的认知效应。我们基于Hofstadter关于暗示的理论假设这一假设。

    This article proposes a cognitive mechanism of humour of general applicability, not restricted to verbal communication. It is indebted to Raskin's concept of script overlap, and conforms to the incongruity-resolution theoretical framework, but it is built on the notion of constraint, an abstract correspondence between sets of data. Under this view, script overlap is an outcome of a more abstractly described phenomenon, constraint overlap. The important concept of the overlooked argument is introduced to characterise the two overlapping constraints -- overt and covert. Their inputs and outputs are not directly encoded in utterances, but implicated by them, and their overlap results in another overlap at the level of the communicated utterances, that the incongruity reveals. Our hypothesis assumes as a given that the evocation of such constraints is a cognitive effect of the inferential process by which a hearer interprets utterances. We base this assumption on Hofstadter's theory of ana
    
[^69]: 对理解-工作负荷权衡的信息瓶颈特征化

    An Information Bottleneck Characterization of the Understanding-Workload Tradeoff. (arXiv:2310.07802v1 [cs.AI])

    [http://arxiv.org/abs/2310.07802](http://arxiv.org/abs/2310.07802)

    本论文利用信息瓶颈方法对工作负荷和理解之间的平衡进行了特征化，通过抽象化来解释复杂概念，以实现工作负荷和理解之间的权衡。

    

    最近人工智能的进展突显了对可解释人工智能（XAI）的需求，以支持人类对人工智能系统的理解。考虑到影响解释效力的人类因素，如心理负荷和人类理解，是有效的XAI设计的核心。现有的XAI研究已经证明了不同类型的解释引起的理解和工作负荷之间的权衡。通过抽象（手工制作的相关问题特征的组合）解释复杂概念已被证明能够有效地解决和平衡这种工作负荷-理解的权衡。在这项工作中，我们通过信息瓶颈方法对工作负荷-理解平衡进行特征化：这是一种自动生成最大化信息量和最小化复杂性的抽象的信息论方法。特别地，我们通过人体实验建立了工作负荷与复杂性之间以及理解与信息量之间的实证联系。

    Recent advances in artificial intelligence (AI) have underscored the need for explainable AI (XAI) to support human understanding of AI systems. Consideration of human factors that impact explanation efficacy, such as mental workload and human understanding, is central to effective XAI design. Existing work in XAI has demonstrated a tradeoff between understanding and workload induced by different types of explanations. Explaining complex concepts through abstractions (hand-crafted groupings of related problem features) has been shown to effectively address and balance this workload-understanding tradeoff. In this work, we characterize the workload-understanding balance via the Information Bottleneck method: an information-theoretic approach which automatically generates abstractions that maximize informativeness and minimize complexity. In particular, we establish empirical connections between workload and complexity and between understanding and informativeness through human-subject e
    
[^70]: 轨迹感知的主要流形框架用于数据增强和图像生成

    Trajectory-aware Principal Manifold Framework for Data Augmentation and Image Generation. (arXiv:2310.07801v1 [cs.CV])

    [http://arxiv.org/abs/2310.07801](http://arxiv.org/abs/2310.07801)

    本文提出了一种基于轨迹感知的主要流形框架，能够在数据增强和图像生成任务中提取更紧凑的流形表示，并实现少样本图像生成。

    

    深度学习的数据增强在模型训练、图像转换、医学图像分析等领域有益处。许多现有方法从参数分布（如高斯分布）中生成新样本，却很少关注在输入或特征空间沿数据流形生成样本。本文验证了在特征空间中使用隐藏的主要流形比高斯分布带来了理论和实际上的优势。我们提出了一种新颖的轨迹感知的主要流形框架，用于恢复流形骨干并沿着特定轨迹生成样本。在自动编码器结构的基础上，我们进一步引入了一个内在维度正则化项，使流形更紧凑，并实现少样本图像生成。实验结果表明，这种新颖的框架能够提取更紧凑的流形表示，提高分类准确性并生成平滑的变换。

    Data augmentation for deep learning benefits model training, image transformation, medical imaging analysis and many other fields. Many existing methods generate new samples from a parametric distribution, like the Gaussian, with little attention to generate samples along the data manifold in either the input or feature space. In this paper, we verify that there are theoretical and practical advantages of using the principal manifold hidden in the feature space than the Gaussian distribution. We then propose a novel trajectory-aware principal manifold framework to restore the manifold backbone and generate samples along a specific trajectory. On top of the autoencoder architecture, we further introduce an intrinsic dimension regularization term to make the manifold more compact and enable few-shot image generation. Experimental results show that the novel framework is able to extract more compact manifold representation, improve classification accuracy and generate smooth transformatio
    
[^71]: 解释性注意力用于少样本学习及其它领域

    Explainable Attention for Few-shot Learning and Beyond. (arXiv:2310.07800v1 [cs.AI])

    [http://arxiv.org/abs/2310.07800](http://arxiv.org/abs/2310.07800)

    本研究介绍了一种用于少样本学习的解释性注意力框架，旨在通过识别输入数据的关键部分来提高模型性能。

    

    注意力机制在增强学习模型中显示出了有希望的潜力，它可以通过识别输入数据中显著的部分来提高模型的性能。这在数据收集和标记方面存在挑战，导致训练样本有限的情况下尤其有价值。受人类认知过程启发，我们认为，如果将AI基线暴露于原始数据的关键部分而不是整个输入数据集，类似于人类的感知，那么它的性能可能会更准确、更可靠。然而，选择这些信息性数据部分的任务，即硬注意力寻找，是一个巨大的挑战。在少量训练样本的情况下，现有的研究很难找到这些信息性区域，原因是大量的训练参数无法从有限的样本中有效学习。在本研究中，我们介绍了一种实用的框架，用于实现可解释的硬注意力寻找。

    Attention mechanisms have exhibited promising potential in enhancing learning models by identifying salient portions of input data. This is particularly valuable in scenarios where limited training samples are accessible due to challenges in data collection and labeling. Drawing inspiration from human recognition processes, we posit that an AI baseline's performance could be more accurate and dependable if it is exposed to essential segments of raw data rather than the entire input dataset, akin to human perception. However, the task of selecting these informative data segments, referred to as hard attention finding, presents a formidable challenge. In situations with few training samples, existing studies struggle to locate such informative regions due to the large number of training parameters that cannot be effectively learned from the available limited samples. In this study, we introduce a novel and practical framework for achieving explainable hard attention finding, specifically
    
[^72]: 基于迁移学习的EMR数据集之间数据分布变化的预后预测模型

    A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets. (arXiv:2310.07799v1 [cs.LG])

    [http://arxiv.org/abs/2310.07799](http://arxiv.org/abs/2310.07799)

    本研究提出了一种基于迁移学习的EMR数据集之间数据分布变化的预后预测模型，通过构建过渡模型解决了不同数据集的特征不匹配问题，提高了深度学习模型的效率。

    

    由于对新兴疾病的信息有限，症状很难被察觉和认识到，因此可能忽视临床干预的窗口。期望能够建立一个有效的预后模型，辅助医生进行正确诊断和制定个性化治疗方案，从而及时预防不利结果。然而，在疾病早期阶段，由于数据收集和临床经验有限，再加上对隐私和伦理的考虑，导致可供参考的数据受限，甚至难以正确标记数据标签。此外，不同疾病或同一疾病不同来源的电子医疗记录（EMR）数据可能存在严重的跨数据集特征不匹配问题，严重影响深度学习模型的效率。本文介绍了一种迁移学习方法，建立一个从源数据集到目标数据集的过渡模型，通过对特征进行约束，来解决数据分布变化的问题。

    Due to the limited information about emerging diseases, symptoms are hard to be noticed and recognized, so that the window for clinical intervention could be ignored. An effective prognostic model is expected to assist doctors in making right diagnosis and designing personalized treatment plan, so to promptly prevent unfavorable outcomes. However, in the early stage of a disease, limited data collection and clinical experiences, plus the concern out of privacy and ethics, may result in restricted data availability for reference, to the extent that even data labels are difficult to mark correctly. In addition, Electronic Medical Record (EMR) data of different diseases or of different sources of the same disease can prove to be having serious cross-dataset feature misalignment problems, greatly mutilating the efficiency of deep learning models. This article introduces a transfer learning method to build a transition model from source dataset to target dataset. By way of constraining the 
    
[^73]: GenTKG: 基于生成模型的时间知识图谱预测

    GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])

    [http://arxiv.org/abs/2310.07793](http://arxiv.org/abs/2310.07793)

    研究提出了一种名为GenTKG的生成模型，用于在时间知识图谱上进行预测。该模型通过结合基于时间逻辑规则的检索策略和轻量级的参数效率指导，克服了复杂的时间图数据结构和庞大的数据量所带来的挑战。

    

    大规模语言模型(LLM)的快速发展引发了对时间知识图谱(tKG)领域的兴趣，其中传统的基于嵌入和规则的模型占主导地位。目前仍然存在一个问题，即预训练的LLM是否能够理解结构化的时间关系数据，并取代它们成为时间关系预测的基础模型。因此，我们将时间知识预测引入生成模式。然而，在复杂的时间图数据结构和LLM可以处理的序列自然表达之间存在巨大的鸿沟，在tKG的庞大数据量和微调LLM的巨大计算成本之间也存在挑战。为了解决这些挑战，我们提出了一种新颖的检索增强生成框架，称为GenTKG，它在tKG上执行生成式预测，结合了基于时间逻辑规则的检索策略和轻量级的参数效率指导。通过大量实验证明了GenTKG的有效性。

    The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
    
[^74]: DrivingDiffusion：基于布局引导的多视角驾驶场景视频生成与潜在扩散模型

    DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model. (arXiv:2310.07771v1 [cs.CV])

    [http://arxiv.org/abs/2310.07771](http://arxiv.org/abs/2310.07771)

    DrivingDiffusion是一个基于3D布局的多视角驾驶场景视频生成框架，通过解决跨视角和跨帧的一致性问题，以及保证生成实例质量，实现了逼真的多视角视频生成。

    

    随着基于强大和统一的鸟瞰图（BEV）表示的自动驾驶的普及，对具有准确注释的高质量和大规模多视角视频数据的需求迫切存在。然而，由于昂贵的采集和注释成本，获得此类大规模多视角数据并非易事。为了缓解这个问题，我们提出了一种基于空间-时间一致性的扩散框架DrivingDiffusion，以生成由3D布局控制的逼真多视角视频。在给定3D布局的情况下合成多视角视频时，存在三个挑战：如何保持1）跨视角的一致性和2）跨帧的一致性？3）如何保证生成实例的质量？我们的DrivingDiffusion通过级联多视角单帧图像生成步骤、被多个摄像机共享的单视角视频生成步骤以及能够处理长视频生成的后处理来解决这个问题。在多视角模型中，生成实例的一致性能够保证。

    With the increasing popularity of autonomous driving based on the powerful and unified bird's-eye-view (BEV) representation, a demand for high-quality and large-scale multi-view video data with accurate annotation is urgently required. However, such large-scale multi-view data is hard to obtain due to expensive collection and annotation costs. To alleviate the problem, we propose a spatial-temporal consistent diffusion framework DrivingDiffusion, to generate realistic multi-view videos controlled by 3D layout. There are three challenges when synthesizing multi-view videos given a 3D layout: How to keep 1) cross-view consistency and 2) cross-frame consistency? 3) How to guarantee the quality of the generated instances? Our DrivingDiffusion solves the problem by cascading the multi-view single-frame image generation step, the single-view video generation step shared by multiple cameras, and post-processing that can handle long video generation. In the multi-view model, the consistency of
    
[^75]: 离线强化学习中的问责制：用语料库的例子解释决策

    Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples. (arXiv:2310.07747v1 [cs.LG])

    [http://arxiv.org/abs/2310.07747](http://arxiv.org/abs/2310.07747)

    本论文介绍了一种可解释的离线控制器方法，通过使用离线数据集作为决策语料库，在低数据场景中实现了问责制的控制，并在医疗保健领域展示了良好的性能。

    

    在决策系统中使用离线数据学习透明、可解释的控制器是一个重要的研究领域，因为它有潜力降低在现实世界系统中应用的风险。然而，在责任敏感的设置（如医疗保健）中，决策问责制非常重要，但目前的文献尚未充分解决这个问题。本文介绍了一种名为Accountable Offline Controller（AOC）的方法，它将离线数据集作为决策语料库，并根据一组定制的例子（称为语料库子集）进行问责制的控制。AOC在低数据场景中有效地运行，可以扩展到严格的离线模仿设置，并表现出保护和适应性的特点。我们在模拟和真实的医疗保健场景中评估了AOC的性能，强调了它在保持问责制的同时能够管理高水平的离线控制任务。

    Learning transparent, interpretable controllers with offline data in decision-making systems is an essential area of research due to its potential to reduce the risk of applications in real-world systems. However, in responsibility-sensitive settings such as healthcare, decision accountability is of paramount importance, yet has not been adequately addressed by the literature. This paper introduces the Accountable Offline Controller (AOC) that employs the offline dataset as the Decision Corpus and performs accountable control based on a tailored selection of examples, referred to as the Corpus Subset. ABC operates effectively in low-data scenarios, can be extended to the strictly offline imitation setting, and displays qualities of both conservation and adaptability. We assess ABC's performance in both simulated and real-world healthcare scenarios, emphasizing its capability to manage offline control tasks with high levels of performance while maintaining accountability.  Keywords: Int
    
[^76]: AI算法用于在Grasshopper / Rhinoceros 7中生成三维无障碍坡道

    AI Algorithm for the Generation of Three-Dimensional Accessibility Ramps in Grasshopper / Rhinoceros 7. (arXiv:2310.07728v1 [cs.HC])

    [http://arxiv.org/abs/2310.07728](http://arxiv.org/abs/2310.07728)

    本论文介绍了一种使用AI算法在Grasshopper / Rhinoceros 7中生成三维无障碍坡道的方法，通过基于环境的3D模型，自动确定最佳路径来提高无障碍坡道设计的效率。

    

    无障碍基础设施作为城市发展的一部分经常被忽视, 但在日常生活中不可否认其重要性。无障碍坡道是最常见的无障碍基础设施之一，不仅对行动受限的人群有益，还能帮助身体健全的第三方。尽管人们认识到无障碍坡道的必要性，但由于设计阶段所需的人力的限制，实际实施仍存在问题。为此，我们提出了一种算法，能够基于相关环境的3D模型自动生成可行的无障碍坡道。通过在3D模型中手动指定起点和终点，算法使用AI搜索算法确定连接这些点的最佳路径。算法考虑了制定轮椅无障碍坡道的关键因素，包括但不限于高度差、空间约束和地面倾斜度等。

    Often overlooked as a component of urban development, accessibility infrastructure is undeniably crucial in daily life. Accessibility ramps are one of the most common types of accessibility infrastructure, and serve to benefit not only people with mobile impairments but also able-bodied third parties. While the necessity of accessibility ramps is acknowledged, actual implementation fails in light of the limits of manpower required for the design stage. In response, we present an algorithm capable of the automatic generation of a feasible accessibility ramp based on a 3D model of the relevant environment. Through the manual specification of initial and terminal points within a 3D model, the algorithm uses AI search algorithms to determine the optimal pathway connecting these points. Essential components in devising a wheelchair-accessible ramp are encoded within the process, as evaluated by the algorithm, including but not limited to elevation differentials, spatial constraints, and gra
    
[^77]: 对水印技术应用于人工智能生成内容的漏洞研究

    Towards the Vulnerability of Watermarking Artificial Intelligence Generated Content. (arXiv:2310.07726v1 [cs.CV])

    [http://arxiv.org/abs/2310.07726](http://arxiv.org/abs/2310.07726)

    该研究探讨了将水印技术应用于人工智能生成内容的漏洞，并证明了现有的水印机制容易被对手破解。

    

    人工智能生成内容（AIGC）在社交媒体上越来越受欢迎，许多商业服务已经推出。这些服务利用先进的生成模型，如潜在扩散模型和大型语言模型，为用户生成创意内容（例如逼真的图像、流畅的句子）。对于此类生成内容的使用需要高度监管，因为服务提供商需要确保用户不违反使用政策（例如滥用商业化、生成和分发不安全的内容）。最近提出了许多水印技术，但是本文表明对手可以轻易破解这些水印机制。具体而言，我们考虑了两种可能的攻击方式：（1）水印去除：对手可以轻松地从生成内容中删除嵌入的水印，然后自由使用而不受服务提供商的限制；（2）水印伪造：对手可以创建非法的水印。

    Artificial Intelligence Generated Content (AIGC) is gaining great popularity in social media, with many commercial services available. These services leverage advanced generative models, such as latent diffusion models and large language models, to generate creative content (e.g., realistic images, fluent sentences) for users. The usage of such generated content needs to be highly regulated, as the service providers need to ensure the users do not violate the usage policies (e.g., abuse for commercialization, generating and distributing unsafe content).  Numerous watermarking approaches have been proposed recently. However, in this paper, we show that an adversary can easily break these watermarking mechanisms. Specifically, we consider two possible attacks. (1) Watermark removal: the adversary can easily erase the embedded watermark from the generated content and then use it freely without the regulation of the service provider. (2) Watermark forge: the adversary can create illegal co
    
[^78]: 视觉预测作为避障的中层表示

    Visual Forecasting as a Mid-level Representation for Avoidance. (arXiv:2310.07724v1 [cs.RO])

    [http://arxiv.org/abs/2310.07724](http://arxiv.org/abs/2310.07724)

    本研究提出了一种创新的替代方法——视觉预测，通过引入直观的视觉提示，投射动态物体的未来轨迹，提高智能体的感知能力并实现预期的动作。研究结果验证了视觉预测作为导航问题解决方案的可行性。

    

    在自主智能体研究中，在具有动态物体的环境中进行导航的挑战仍然是一个核心问题。虽然预测方法很有前景，但它们对精确状态信息的依赖使其在实际世界中的应用不太实用。本研究提出了视觉预测作为一种创新的替代方法。通过引入直观的视觉提示，这种方法可以投射动态物体的未来轨迹，以提高智能体的感知能力并实现预期的动作。我们的研究探索了两种不同的策略来通过视觉预测传达预测信息：（1）边界框序列和（2）增强路径。为了验证所提出的视觉预测策略，我们在Unity引擎的模拟环境中进行评估，并将这些评估扩展到实际场景中，以评估其实用性和有效性。结果证实了视觉预测作为导航问题的有前途的解决方案的可行性。

    The challenge of navigation in environments with dynamic objects continues to be a central issue in the study of autonomous agents. While predictive methods hold promise, their reliance on precise state information makes them less practical for real-world implementation. This study presents visual forecasting as an innovative alternative. By introducing intuitive visual cues, this approach projects the future trajectories of dynamic objects to improve agent perception and enable anticipatory actions. Our research explores two distinct strategies for conveying predictive information through visual forecasting: (1) sequences of bounding boxes, and (2) augmented paths. To validate the proposed visual forecasting strategies, we initiate evaluations in simulated environments using the Unity engine and then extend these evaluations to real-world scenarios to assess both practicality and effectiveness. The results confirm the viability of visual forecasting as a promising solution for navigat
    
[^79]: 重新考虑基于DNA序列的BERT-like预训练

    Rethinking the BERT-like Pretraining for DNA Sequences. (arXiv:2310.07644v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.07644](http://arxiv.org/abs/2310.07644)

    重新考虑了基于DNA序列的BERT-like预训练方法，通过使用K-mer重叠标记化，在下游任务的微调阶段和预训练过程中都取得了一致的性能改善。

    

    随着在自然语言处理领域中大规模预训练的成功，将其应用于生命科学领域的趋势日益增长。特别是基于DNA序列的预训练方法因其捕捉基因的通用信息的潜力而受到关注。然而，现有的DNA序列预训练方法主要依赖于从自然语言处理领域直接引入的BERT预训练方法，缺乏全面的理解和专门定制的方法。为了填补这一研究空白，我们首先进行了一系列的探索性实验，并获得了几个有启发性的观察结果：1）在下游任务的微调阶段，使用K-mer重叠标记化而不是K-mer非重叠标记化时，重叠和非重叠的预训练权重均表现出一致的性能改善。2）在预训练过程中，使用K-mer重叠标记化会迅速产生清晰的K-mer嵌入，并将损失降低到非常低的水平。

    With the success of large-scale pretraining in NLP, there is an increasing trend of applying it to the domain of life sciences. In particular, pretraining methods based on DNA sequences have garnered growing attention due to their potential to capture generic information about genes. However, existing pretraining methods for DNA sequences largely rely on direct adoptions of BERT pretraining from NLP, lacking a comprehensive understanding and a specifically tailored approach. To address this research gap, we first conducted a series of exploratory experiments and gained several insightful observations: 1) In the fine-tuning phase of downstream tasks, when using K-mer overlapping tokenization instead of K-mer non-overlapping tokenization, both overlapping and non-overlapping pretraining weights show consistent performance improvement.2) During the pre-training process, using K-mer overlapping tokenization quickly produces clear K-mer embeddings and reduces the loss to a very low level, w
    
[^80]: OpsEval: 用于大型语言模型的全面任务导向的AIOps基准测试

    OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models. (arXiv:2310.07637v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.07637](http://arxiv.org/abs/2310.07637)

    OpsEval是一个全面任务导向的AIOps基准测试，评估了大型语言模型在有线网络操作、5G通信操作和数据库操作等关键场景下的能力水平，为提供针对AIOps定制的LLMs的优化方向。

    

    大型语言模型(Large Language Models, LLMs)在翻译、总结和生成等NLP相关任务中表现出了显著的能力。LLMs在特定领域中应用，特别是在AIOps（面向IT运维的人工智能）中，由于其先进的信息汇总、报告分析和API调用能力而具有巨大的潜力。然而，当前LLMs在AIOps任务中的性能尚未确定。此外，需要一个全面的基准测试来引导针对AIOps定制的LLMs的优化。与现有的专注于评估网络配置等特定领域的基准测试不同，本文提出了OpsEval，这是一个专为LLMs设计的全面任务导向的AIOps基准测试。OpsEval首次对LLMs在三个关键场景（有线网络操作、5G通信操作和数据库操作）以及不同的能力水平（知识回忆、分析思考）进行评估。

    Large language models (LLMs) have exhibited remarkable capabilities in NLP-related tasks such as translation, summarizing, and generation. The application of LLMs in specific areas, notably AIOps (Artificial Intelligence for IT Operations), holds great potential due to their advanced abilities in information summarizing, report analyzing, and ability of API calling. Nevertheless, the performance of current LLMs in AIOps tasks is yet to be determined. Furthermore, a comprehensive benchmark is required to steer the optimization of LLMs tailored for AIOps. Compared with existing benchmarks that focus on evaluating specific fields like network configuration, in this paper, we present \textbf{OpsEval}, a comprehensive task-oriented AIOps benchmark designed for LLMs. For the first time, OpsEval assesses LLMs' proficiency in three crucial scenarios (Wired Network Operation, 5G Communication Operation, and Database Operation) at various ability levels (knowledge recall, analytical thinking, an
    
[^81]: In-Context Unlearning: 基于少样本学习的语言模型的消除研究

    In-Context Unlearning: Language Models as Few Shot Unlearners. (arXiv:2310.07579v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.07579](http://arxiv.org/abs/2310.07579)

    这项工作提出了一种新的LLM消除方法，称为“基于上下文的消除”，它提供了上下文的输入且无需更新模型参数。这种方法解决了消除对于很大模型来说在计算上的困难，并在实践中具有更高的可行性和便捷性。

    

    机器消除学习是研究如何高效地去除特定训练数据对训练模型的影响，近来引起了更多的关注，主要是由于需要遵守诸如被遗忘权等隐私法规的需求。尽管在版权问题上LLM（语言模型）尤其相关，但在非常大的模型上实现精确消除是计算上不可行的。为此，最近的研究提出了几种算法，可以在不重新训练模型的情况下近似消除训练数据。这些算法关键依赖于对模型参数的访问来更新它们，但在实践中可能由于计算约束或通过API访问LLM而无法满足这种假设。在这项工作中，我们提出了一种新的LLM消除方法，称为“基于上下文的消除”，它提供了上下文的输入且无需更新模型参数。为了消除特定的训练实例，我们提供了i

    Machine unlearning, the study of efficiently removing the impact of specific training points on the trained model, has garnered increased attention of late, driven by the need to comply with privacy regulations like the Right to be Forgotten. Although unlearning is particularly relevant for LLMs in light of the copyright issues they raise, achieving precise unlearning is computationally infeasible for very large models. To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or when the LLM is accessed via API. In this work, we propose a new class of unlearning methods for LLMs we call ''In-Context Unlearning'', providing inputs in context and without having to update model parameters. To unlearn a particular training instance, we provide the i
    
[^82]: 多模态图学习用于生成任务

    Multimodal Graph Learning for Generative Tasks. (arXiv:2310.07478v1 [cs.AI])

    [http://arxiv.org/abs/2310.07478](http://arxiv.org/abs/2310.07478)

    多模态图学习是一种通用且系统的框架，可以捕捉多模态数据之间的复杂关系，并在生成任务中取得了良好的效果。

    

    多模态学习结合多种数据模态，扩大了模型可以利用的数据类型和复杂度，例如从纯文本到图像-字幕对。大多数多模态学习算法专注于对两种模态的简单一对一数据进行建模，如图像-字幕对或音频-文本对。然而，在大多数实际场景中，不同模态的实体以更复杂和多样化的方式相互作用，超越了一对一映射。我们提出将这些复杂关系表示为图形，允许我们捕捉任意数量的模态数据，并捕捉模态之间的复杂关系，这些关系可以从一个样本到另一个样本灵活变化。为实现这一目标，我们提出了多模态图学习（MMGL），这是一个通用且系统的框架，用于捕获具有关系结构的多个多模态邻居的信息。特别是，我们关注基于预训练模型的MMGL用于生成任务。

    Multimodal learning combines multiple data modalities, broadening the types and complexity of data our models can utilize: for example, from plain text to image-caption pairs. Most multimodal learning algorithms focus on modeling simple one-to-one pairs of data from two modalities, such as image-caption pairs, or audio-text pairs. However, in most real-world settings, entities of different modalities interact with each other in more complex and multifaceted ways, going beyond one-to-one mappings. We propose to represent these complex relationships as graphs, allowing us to capture data with any number of modalities, and with complex relationships between modalities that can flexibly vary from one sample to another. Toward this goal, we propose Multimodal Graph Learning (MMGL), a general and systematic framework for capturing information from multiple multimodal neighbors with relational structures among them. In particular, we focus on MMGL for generative tasks, building upon pretraine
    
[^83]: 通过自动折扣调度从观察中进行模仿学习

    Imitation Learning from Observation with Automatic Discount Scheduling. (arXiv:2310.07433v1 [cs.RO])

    [http://arxiv.org/abs/2310.07433](http://arxiv.org/abs/2310.07433)

    我们提出了一个新颖的观察学习模仿(ILfO)框架，能够解决由于奖励信号分配错误导致代理无法学习初始行为的问题。

    

    人类通常通过观察和模仿来获得新的技能。对于机器人代理，从互联网上可用的大量无标签视频演示数据中进行学习，需要在没有访问其动作的情况下模仿专家，这是一种称为观察学习模仿（ILfO）的挑战。解决ILfO问题的常见方法是将其转化为逆向强化学习问题，利用从代理和专家观察中计算出的代理奖励。然而，我们发现在具有进展依赖性属性的任务中，这样的方法面临重大挑战；在这些任务中，代理需要在掌握后续行为之前先学习专家的前序行为。我们的研究表明，主要原因是分配给后续步骤的奖励信号妨碍了对初始行为的学习。为了解决这个挑战，我们提出了一个新颖的ILfO框架，使代理能够掌握早期行为。

    Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observations (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we present a novel ILfO framework that enables the agent to master earl
    
[^84]: NuTime: 大规模时间序列预训练的数值多尺度嵌入

    NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining. (arXiv:2310.07402v1 [cs.LG])

    [http://arxiv.org/abs/2310.07402](http://arxiv.org/abs/2310.07402)

    本研究通过采用Transformer架构和数值多尺度嵌入模块，使时间序列自监督模型能够扩展到大规模数据集，并在大规模数据集上进行预训练。

    

    最近关于时间序列自监督模型的研究显示出学习语义表示的巨大潜力，然而，这些研究仅限于小规模数据集，例如数千个时间序列。本文的关键技术贡献针对时间序列数据的数值特性，使模型能够扩展到大规模数据集，例如百万个时间序列。我们采用Transformer架构，首先将输入划分为非重叠窗口。然后，通过窗口的标准化形状和两个标量值表示每个窗口内的均值和标准差。为了将可能具有任意数值尺度的标量值嵌入到高维向量中，我们提出了一个数值多尺度嵌入模块，枚举所有可能的标量值尺度。该模型使用提出的数值多尺度嵌入在大规模数据集上进行预训练，采用简单的对比损失函数。

    Recent research on time-series self-supervised models shows great promise in learning semantic representations. However, it has been limited to small-scale datasets, e.g., thousands of temporal sequences. In this work, we make key technical contributions that are tailored to the numerical properties of time-series data and allow the model to scale to large datasets, e.g., millions of temporal sequences. We adopt the Transformer architecture by first partitioning the input into non-overlapping windows. Each window is then characterized by its normalized shape and two scalar values denoting the mean and standard deviation within each window. To embed scalar values that may possess arbitrary numerical scales to high-dimensional vectors, we propose a numerically multi-scaled embedding module enumerating all possible scales for the scalar values. The model undergoes pretraining using the proposed numerically multi-scaled embedding with a simple contrastive objective on a large-scale dataset
    
[^85]: WiGenAI: 通过扩散模型实现无线和生成式人工智能的交织

    WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models. (arXiv:2310.07312v1 [cs.IT])

    [http://arxiv.org/abs/2310.07312](http://arxiv.org/abs/2310.07312)

    WiGenAI通过引入扩散模型，将生成式人工智能应用于无线通信系统中，为研究奠定基础。这篇文章介绍了扩散模型作为生成模型的最新范式，并讨论了它在无线通信系统中的应用。通过两个案例研究展示了扩散模型在开发韧性的AI本地通信系统中的潜力。

    

    创新的基础模型，如GPT-3和稳定的扩散模型，已经在人工智能领域实现了范式转变，向生成式人工智能系统发展。从数据通信和网络的角度来看，人工智能和机器学习算法预计将广泛应用于未来无线通信系统的新一代中，强调了在新兴通信场景中需要新颖的AI本地解决方案。本文介绍生成式人工智能在无线通信系统中的应用，为该领域的研究奠定基础。介绍了扩散型生成模型作为生成模型的最新范式，并讨论了它们在无线通信系统中的应用。还提供了两个案例研究，展示了如何利用扩散模型开发具有韧性的AI本地通信系统。具体而言，我们提出了一种基于扩散模型的生成模型，以展示其在生成模型的应用中的优势。

    Innovative foundation models, such as GPT-3 and stable diffusion models, have made a paradigm shift in the realm of artificial intelligence (AI) towards generative AI-based systems. In unison, from data communication and networking perspective, AI and machine learning (AI/ML) algorithms are envisioned to be pervasively incorporated into the future generations of wireless communications systems, highlighting the need for novel AI-native solutions for the emergent communication scenarios. In this article, we outline the applications of generative AI in wireless communication systems to lay the foundations for research in this field. Diffusion-based generative models, as the new state-of-the-art paradigm of generative models, are introduced, and their applications in wireless communication systems are discussed. Two case studies are also presented to showcase how diffusion models can be exploited for the development of resilient AI-native communication systems. Specifically, we propose de
    
[^86]: 在医疗保健领域中对大型语言模型的分析：BioBERT 案例研究

    An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT. (arXiv:2310.07282v1 [cs.AI])

    [http://arxiv.org/abs/2310.07282](http://arxiv.org/abs/2310.07282)

    本研究分析了在医疗保健领域应用大型语言模型（尤其是BioBERT）的可行性，并提出了针对医疗保健领域的微调方法。研究突出了BioBERT对于解决与生物医学文本挖掘相关任务的特定要求的适用性。

    

    本文对在医疗保健领域应用大型语言模型（尤其是BioBERT）进行了全面调查研究。它首先彻底检查了先前在医疗保健领域中应用自然语言处理（NLP）方法的情况，揭示了这些方法面临的局限和挑战。随后，本研究探讨了将BioBERT整合到医疗保健应用中的路径，突出其适用于解决与生物医学文本挖掘相关任务的特定要求。该分析概述了用于针对医疗保健领域独特需求微调BioBERT的系统方法论。这个方法包括从各种医疗保健来源收集数据，为识别医疗实体和对其进行分类等任务进行数据注释，并应用专门针对处理生物医学文本中复杂性的预处理技术。

    This paper conducts a comprehensive investigation into applying large language models, particularly on BioBERT, in healthcare. It begins with thoroughly examining previous natural language processing (NLP) approaches in healthcare, shedding light on the limitations and challenges these methods face. Following that, this research explores the path that led to the incorporation of BioBERT into healthcare applications, highlighting its suitability for addressing the specific requirements of tasks related to biomedical text mining. The analysis outlines a systematic methodology for fine-tuning BioBERT to meet the unique needs of the healthcare domain. This approach includes various components, including the gathering of data from a wide range of healthcare sources, data annotation for tasks like identifying medical entities and categorizing them, and the application of specialized preprocessing techniques tailored to handle the complexities found in biomedical texts. Additionally, the pape
    
[^87]: NECO: 基于神经坍塌的超出分布检测

    NECO: NEural Collapse Based Out-of-distribution detection. (arXiv:2310.06823v1 [stat.ML])

    [http://arxiv.org/abs/2310.06823](http://arxiv.org/abs/2310.06823)

    NECO是一种基于神经坍塌的新颖的超出分布检测方法，利用几何属性和主成分空间识别OOD数据，在小规模和大规模任务上取得了最先进的结果，并展示了强大的泛化能力。

    

    由于模型过于自信并且没有意识到其认识论限制，检测超出分布（OOD）数据是机器学习中的一个重要挑战。我们假设“神经坍塌”，一种影响超出分布数据的现象，也会影响超出分布数据。为了从这种相互作用中受益，我们引入了NECO，一种用于OOD检测的新颖的事后方法，它利用“神经坍塌”和主成分空间的几何属性来识别OOD数据。我们的大量实验表明，NECO在小规模和大规模OOD检测任务上取得了最先进的结果，同时在不同的网络架构上展示了强大的泛化能力。此外，我们还对我们的方法在OOD检测中的有效性提供了理论解释。我们计划在匿名期结束后发布代码。

    Detecting out-of-distribution (OOD) data is a critical challenge in machine learning due to model overconfidence, often without awareness of their epistemological limits. We hypothesize that ``neural collapse'', a phenomenon affecting in-distribution data for models trained beyond loss convergence, also influences OOD data. To benefit from this interplay, we introduce NECO, a novel post-hoc method for OOD detection, which leverages the geometric properties of ``neural collapse'' and of principal component spaces to identify OOD data. Our extensive experiments demonstrate that NECO achieves state-of-the-art results on both small and large-scale OOD detection tasks while exhibiting strong generalization capabilities across different network architectures. Furthermore, we provide a theoretical explanation for the effectiveness of our method in OOD detection. We plan to release the code after the anonymity period.
    
[^88]: FABind: 快速准确的蛋白-配体结合

    FABind: Fast and Accurate Protein-Ligand Binding. (arXiv:2310.06763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.06763](http://arxiv.org/abs/2310.06763)

    FABind是一个结合了口袋预测和对接的端到端模型，旨在实现快速准确的蛋白-配体结合预测。

    

    在药物发现中，对蛋白质和配体之间的相互作用进行建模并准确预测其结合结构是一项关键但具有挑战性的任务。深度学习的最新进展在应对这一挑战方面显示出了希望，采样法和回归法成为两种突出的方法。然而，这些方法都存在明显的局限性。采样法通常由于需要生成多个候选结构来进行选择而效率较低。而回归法提供了快速的预测，但可能会导致准确性降低。另外，蛋白质大小的变化通常需要外部模块来选择合适的结合口袋，进一步影响效率。在这项工作中，我们提出了FABind，一个将口袋预测和对接相结合的端到端模型，以实现准确和快速的蛋白-配体结合。

    Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed pocket prediction
    
[^89]: GPT-4作为农学助手？使用大型语言模型回答农业考试

    GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models. (arXiv:2310.06225v1 [cs.AI])

    [http://arxiv.org/abs/2310.06225](http://arxiv.org/abs/2310.06225)

    本研究全面评估了大型语言模型（LLMs）在回答农业相关问题方面的能力，并使用RAG和ER技术提高了LLMs的性能。其中，GPT-4在农业考试中取得了及格分数以获得认证。

    

    大型语言模型（LLM）在各个领域，包括医疗保健和金融领域，展示了卓越的自然语言理解能力。在某些任务上，LLM的性能与训练有素的人类相似甚至更好，因此合理地使用人类考试（例如认证考试）来评估LLM的性能。我们对流行的LLM（如Llama 2和GPT）在回答农业相关问题的能力进行了全面评估。在评估过程中，我们还运用了RAG（检索增强生成）和ER（集合细化）技术，结合信息检索、生成能力和提示策略，提高LLM的性能。为了展示LLM的能力，我们选择了来自巴西、印度和美国三个最大的农业生产国的农业考试和基准数据集。我们的分析突出了GPT-4在考试中取得及格分数以获得认证的能力。

    Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding across various domains, including healthcare and finance. For some tasks, LLMs achieve similar or better performance than trained human beings, therefore it is reasonable to employ human exams (e.g., certification tests) to assess the performance of LLMs. We present a comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their ability to answer agriculture-related questions. In our evaluation, we also employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement) techniques, which combine information retrieval, generation capabilities, and prompting strategies to improve the LLMs' performance. To demonstrate the capabilities of LLMs, we selected agriculture exams and benchmark datasets from three of the largest agriculture producer countries: Brazil, India, and the USA. Our analysis highlights GPT-4's ability to achieve a passing score on exams to earn cred
    
[^90]: 使用多模型深度学习方法的自动化胸部X光报告生成器

    Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning Approach. (arXiv:2310.05969v1 [eess.IV])

    [http://arxiv.org/abs/2310.05969](http://arxiv.org/abs/2310.05969)

    提出了一种基于多模型深度学习的自动化胸部X光报告生成器系统，通过利用多个二元分类模型检测多种异常，在单个图像中辅助放射科医生的工作。该系统将放射学异常检测限制为心脏肥大、肺积液和实变，并通过三个步骤生成放射学报告：图像预处理、深度学习模型检测异常和生成报告。

    

    阅读和解读胸部X光图像是大多数放射科医生的例行工作之一。然而，即使对于经验最丰富的医生来说，这仍然可能是具有挑战性的。因此，我们提出了一种基于多模型深度学习的自动化胸部X光报告生成系统，旨在辅助放射科医生的工作。所提出的系统的基本思想是利用多个二元分类模型来检测多种异常，每个模型负责检测一种异常，在单个图像中。在本研究中，我们将放射学异常检测限制为心脏肥大、肺积液和实变。系统通过执行以下三个步骤生成放射学报告：图像预处理，利用深度学习模型检测异常，并生成报告。图像预处理步骤的目的是通过将其缩放为128x128像素并分割成三个部分来使输入标准化，分别涵盖上部、下部和中部。

    Reading and interpreting chest X-ray images is one of the most radiologist's routines. However, it still can be challenging, even for the most experienced ones. Therefore, we proposed a multi-model deep learning-based automated chest X-ray report generator system designed to assist radiologists in their work. The basic idea of the proposed system is by utilizing multi binary-classification models for detecting multi abnormalities, with each model responsible for detecting one abnormality, in a single image. In this study, we limited the radiology abnormalities detection to only cardiomegaly, lung effusion, and consolidation. The system generates a radiology report by performing the following three steps: image pre-processing, utilizing deep learning models to detect abnormalities, and producing a report. The aim of the image pre-processing step is to standardize the input by scaling it to 128x128 pixels and slicing it into three segments, which covers the upper, lower, and middle parts
    
[^91]: 狮子秘密地解决受限制优化问题：正如李雅普诺夫所预测的。

    Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v1 [cs.LG])

    [http://arxiv.org/abs/2310.05898](http://arxiv.org/abs/2310.05898)

    Lion是通过程序搜索发现的新优化器，在训练大型AI模型方面表现出有希望的结果，具有更高的内存效率。尽管其理论基础不明确，但基于连续时间和离散时间分析，我们证明Lion是一种理论上新颖且有原则的方法，可在最小化一般损失函数的同时强制执行边界约束。

    

    通过程序搜索发现的新优化器Lion（进化的符号动量）在训练大型AI模型方面显示出有希望的结果。它在训练效果上与AdamW相当或更好，并具有更高的内存效率。正如我们可以从随机搜索程序的结果中期待的，Lion集成了几个现有算法的元素，包括符号动量、独立的权重衰减、Polak和Nesterov动量，但又不属于任何现有的理论基础优化器类别。因此，尽管Lion作为广泛任务的通用优化器表现良好，但其理论基础仍然不明确。这种缺乏理论的明确性限制了进一步增强和扩展Lion的可能性。本文旨在揭开Lion的神秘面纱。基于连续时间和离散时间分析，我们证明Lion是一种理论上新颖且有原则的方法，可在最小化一般损失函数$f(x)$的同时强制执行边界约束。

    Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.  This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint 
    
[^92]: 从法律事实中自动生成论证的自动化系统。

    Automated Argument Generation from Legal Facts. (arXiv:2310.05680v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.05680](http://arxiv.org/abs/2310.05680)

    该论文研究了利用开源大型语言模型的生成能力，从法律案件中自动生成论证。实验结果显示，最佳方法生成的论证与基准集的黄金标准注释平均重叠率为63%。

    

    在全球范围内，待处理案件数量呈指数增长（例如，仅在印度就有超过1000万个待处理案件）。主要问题在于提交给法律系统的案件数量远远超过一个国家中现有的法律专业人员的数量。在这个全球背景下，利用人工智能技术来提高法律程序的效率和速度变得至关重要。在本研究中，我们特别关注帮助法律专业人员分析法律案件的过程。我们的具体调查探索了利用开源大型语言模型的生成能力，从法律案件中提取出论证。实验结果表明，最佳方法生成的论证与基准集的黄金标准注释平均重叠率为63%。

    The count of pending cases has shown an exponential rise across nations (e.g., with more than 10 million pending cases in India alone). The main issue lies in the fact that the number of cases submitted to the law system is far greater than the available number of legal professionals present in a country. Given this worldwide context, the utilization of AI technology has gained paramount importance to enhance the efficiency and speed of legal procedures. In this study we partcularly focus on helping legal professionals in the process of analyzing a legal case. Our specific investigation delves into harnessing the generative capabilities of open-sourced large language models to create arguments derived from the facts present in legal cases. Experimental results show that the generated arguments from the best performing method have on average 63% overlap with the benchmark set gold standard annotations.
    
[^93]: 一种关于货币的新的经济与金融理论

    A new economic and financial theory of money. (arXiv:2310.04986v1 [econ.TH])

    [http://arxiv.org/abs/2310.04986](http://arxiv.org/abs/2310.04986)

    这篇论文通过根本性的改革，将电子货币纳入经济与金融理论，提出了一种新的理论框架，包括电子货币的估值基于宏观经济理论和货币政策的基本方程，以及电子货币管理公司作为协调次经济体货币和财政政策的实体。该研究避免使用普遍但不适当的指数风险模型，而是采用多时间尺度的模型。

    

    本文对经济与金融理论进行了根本性改革，包括电子货币在内。电子货币的估值将基于宏观经济理论和货币政策的基本方程，而不是微观经济学中的贴现现金流理论。与将股票视为与次经济体的无形资产关联的所有权不同，我们将发展电子货币作为与次经济体有形资产关联的交易权益的观点。我们还将发展电子货币管理公司作为一个负责协调次经济体的货币（电子货币供应和价值稳定）和财政（投资和运营）政策的实体的视角，以实现电子货币的流动性。在估值和决策中使用的风险模型不会是无处不在但不合适的指数风险模型，它将导致贴现率，而是多时间尺度的模型。

    This paper fundamentally reformulates economic and financial theory to include electronic currencies. The valuation of the electronic currencies will be based on macroeconomic theory and the fundamental equation of monetary policy, not the microeconomic theory of discounted cash flows. The view of electronic currency as a transactional equity associated with tangible assets of a sub-economy will be developed, in contrast to the view of stock as an equity associated mostly with intangible assets of a sub-economy. The view will be developed of the electronic currency management firm as an entity responsible for coordinated monetary (electronic currency supply and value stabilization) and fiscal (investment and operational) policies of a substantial (for liquidity of the electronic currency) sub-economy. The risk model used in the valuations and the decision-making will not be the ubiquitous, yet inappropriate, exponential risk model that leads to discount rates, but will be multi time sc
    
[^94]: Android手机和平板之间的成对GUI数据集构建

    Pairwise GUI Dataset Construction Between Android Phones and Tablets. (arXiv:2310.04755v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2310.04755](http://arxiv.org/abs/2310.04755)

    本文介绍了一个开创性的成对GUI数据集，旨在解决Android手机和平板之间自动化GUI开发的障碍。数据集包括10035个手机-平板GUI页面对，为提高开发人员的生产力提供了基础。

    

    在当前普及性手机和平板的环境中，应用程序经常存在于两个平台上。虽然应用程序在手机和平板上共享大部分图形用户界面（GUI）和功能，但开发人员经常需要从头开始为平板版本重新构建，增加了开发成本并浪费了现有的设计资源。研究人员正尝试收集数据，并利用深度学习在自动化GUI开发中提高开发人员的生产力。目前存在一些公开可访问的手机GUI页面数据集，但没有关于手机和平板之间成对GUI的数据集。这对于在自动化GUI开发中使用深度学习构成了一个重要障碍。本文介绍了Papt数据集，这是一个专为Android手机和平板量身定制的开创性的成对GUI数据集，包括来自5593个不同应用程序对的10035个手机-平板GUI页面对。我们提出了新颖的成对GUI收集方法来构建这个数据集，并对其进行了界定。

    In the current landscape of pervasive smartphones and tablets, apps frequently exist across both platforms. Although apps share most graphic user interfaces (GUIs) and functionalities across phones and tablets, developers often rebuild from scratch for tablet versions, escalating costs and squandering existing design resources. Researchers are attempting to collect data and employ deep learning in automated GUIs development to enhance developers' productivity. There are currently several publicly accessible GUI page datasets for phones, but none for pairwise GUIs between phones and tablets. This poses a significant barrier to the employment of deep learning in automated GUI development. In this paper, we introduce the Papt dataset, a pioneering pairwise GUI dataset tailored for Android phones and tablets, encompassing 10,035 phone-tablet GUI page pairs sourced from 5,593 unique app pairs. We propose novel pairwise GUI collection approaches for constructing this dataset and delineate it
    
[^95]: DeepSpeed4Science计划：通过先进的AI系统技术实现大规模科学发现

    DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies. (arXiv:2310.04610v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.04610](http://arxiv.org/abs/2310.04610)

    DeepSpeed4Science计划旨在通过先进的AI系统技术加速科学发现，提供针对独特复杂性的解决方案，为自然科学带来重大进展。

    

    在即将到来的十年中，深度学习可能会彻底改变自然科学，增强我们对自然现象建模和预测的能力。这可能预示着科学探索的新时代，从药物开发到可再生能源等领域都将取得重大进展。为了回应这一呼吁，我们提出了DeepSpeed4Science计划（deepspeed4science.ai），旨在通过AI系统技术创新构建独特的能力，帮助领域专家解开今天最大的科学之谜。通过利用DeepSpeed当前的技术支柱（训练、推理和压缩）作为基础技术支持，DeepSpeed4Science将创建一套针对加速科学发现的AI系统技术，以应对其独特的复杂性，超越常见的用于加速通用大语言模型（LLM）的技术方法。在本文中，我们展示了DeepSpeed4Science在解决两个关键问题上的早期进展。

    In the upcoming decade, deep learning may revolutionize the natural sciences, enhancing our capacity to model and predict natural occurrences. This could herald a new era of scientific exploration, bringing significant advancements across sectors from drug development to renewable energy. To answer this call, we present DeepSpeed4Science initiative (deepspeed4science.ai) which aims to build unique capabilities through AI system technology innovations to help domain experts to unlock today's biggest science mysteries. By leveraging DeepSpeed's current technology pillars (training, inference and compression) as base technology enablers, DeepSpeed4Science will create a new set of AI system technologies tailored for accelerating scientific discoveries by addressing their unique complexity beyond the common technical approaches used for accelerating generic large language models (LLMs). In this paper, we showcase the early progress we made with DeepSpeed4Science in addressing two of the cri
    
[^96]: 噪声扰动下的有效口号生成

    Effective Slogan Generation with Noise Perturbation. (arXiv:2310.04472v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.04472](http://arxiv.org/abs/2310.04472)

    本研究引入了基于噪声扰动的新方法，利用预训练的transformer T5模型生成独特且连贯的口号，同时将公司和品牌的描述纳入到生成过程中。结果表明，该方法在口号生成方面取得了良好的效果。

    

    口号在建立公司品牌形象中起着至关重要的作用。口号被期望以令人难忘和讨人喜欢的方式反映公司的愿景和品牌的价值主张。自动化生成具有这些特点的口号是具有挑战性的。以往的研究发展和测试了具有句法控制和摘要模型的口号生成，但这些模型无法生成独特的口号。我们引入了一种新颖的方法，利用预训练的transformer T5模型和新提出的1:N匹配对数据集的噪声扰动。该方法在生成独特且连贯的口号方面起到了促进作用。此外，该方法将公司和品牌的描述纳入到口号生成中。我们根据ROUGE1、ROUGEL和余弦相似性指标评估生成的口号，并通过人为主体评估它们的独特性、连贯性和流畅性。结果显示o

    Slogans play a crucial role in building the brand's identity of the firm. A slogan is expected to reflect firm's vision and brand's value propositions in memorable and likeable ways. Automating the generation of slogans with such characteristics is challenging. Previous studies developted and tested slogan generation with syntactic control and summarization models which are not capable of generating distinctive slogans. We introduce a a novel apporach that leverages pre-trained transformer T5 model with noise perturbation on newly proposed 1:N matching pair dataset. This approach serves as a contributing fator in generting distinctive and coherent slogans. Turthermore, the proposed approach incorporates descriptions about the firm and brand into the generation of slogans. We evaluate generated slogans based on ROUGE1, ROUGEL and Cosine Similarity metrics and also assess them with human subjects in terms of slogan's distinctiveness, coherence, and fluency. The results demonstrate that o
    
[^97]: 超越均匀采样：使用不平衡数据集的离线强化学习

    Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2310.04413v1 [cs.LG])

    [http://arxiv.org/abs/2310.04413](http://arxiv.org/abs/2310.04413)

    该论文提出了一种解决离线强化学习中不平衡数据集问题的方法，通过采样策略使策略只受``好数据"限制，而不是所有的动作，提高了离线RL算法的性能。

    

    离线策略学习旨在利用现有的轨迹数据集来学习决策策略，而无需收集额外的数据。与行为克隆等监督学习技术相比，使用强化学习（RL）的主要动机是找到一个比数据集中的轨迹达到更高平均收益的策略。然而，我们在经验上发现，当一个数据集被次优轨迹所主导时，当前最先进的离线RL算法在平均收益上没有显著提高。我们认为这是由于当前离线RL算法假设与数据集中的轨迹保持接近。如果数据集主要由次优轨迹组成，这个假设将强制策略模仿次优动作。我们通过提出一种采样策略来克服这个问题，使策略只受``好数据"限制，而不是所有的动作。

    Offline policy learning is aimed at learning decision-making policies using existing datasets of trajectories without collecting additional data. The primary motivation for using reinforcement learning (RL) instead of supervised learning techniques such as behavior cloning is to find a policy that achieves a higher average return than the trajectories constituting the dataset. However, we empirically find that when a dataset is dominated by suboptimal trajectories, state-of-the-art offline RL algorithms do not substantially improve over the average return of trajectories in the dataset. We argue this is due to an assumption made by current offline RL algorithms of staying close to the trajectories in the dataset. If the dataset primarily consists of sub-optimal trajectories, this assumption forces the policy to mimic the suboptimal actions. We overcome this issue by proposing a sampling strategy that enables the policy to only be constrained to ``good data" rather than all actions in t
    
[^98]: Axiomatic Aggregations of Abductive Explanations. (arXiv:2310.03131v1 [cs.AI])

    Axiomatic Aggregations of Abductive Explanations. (arXiv:2310.03131v1 [cs.AI])

    [http://arxiv.org/abs/2310.03131](http://arxiv.org/abs/2310.03131)

    本文提出了通过将多种可能的归纳解释汇总为特征重要性分数的三种聚合方法，以解决存在多个有效的归纳解释的问题。

    

    最近对后验模型近似解释方法（如LIME和SHAP）健壮性的批评导致了模型精确阐释的兴起。针对每个数据点，归纳解释提供了能够生成结果的最小特征子集。尽管在理论上是可靠且严谨的，但是归纳解释存在一个主要问题 - 对于同一数据点可能存在多个有效的归纳解释。在这种情况下，提供单一的归纳解释可能是不足够的；另一方面，提供所有有效的归纳解释可能由于其数量庞大而难以理解。在本文中，我们通过将多种可能的归纳解释汇总为特征重要性分数来解决这个问题。我们提出了三种聚合方法：两种基于合作博弈论的权力指数和一种基于著名的因果强度度量。我们通过公理化表征这三种方法，证明它们每一个都具有

    The recent criticisms of the robustness of post hoc model approximation explanation methods (like LIME and SHAP) have led to the rise of model-precise abductive explanations. For each data point, abductive explanations provide a minimal subset of features that are sufficient to generate the outcome. While theoretically sound and rigorous, abductive explanations suffer from a major issue -- there can be several valid abductive explanations for the same data point. In such cases, providing a single abductive explanation can be insufficient; on the other hand, providing all valid abductive explanations can be incomprehensible due to their size. In this work, we solve this issue by aggregating the many possible abductive explanations into feature importance scores. We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength. We characterize these three methods axiomatically, showing that each of 
    
[^99]: 用于有效训练脉冲神经网络的脉冲累积转发方法

    Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks. (arXiv:2310.02772v1 [cs.NE])

    [http://arxiv.org/abs/2310.02772](http://arxiv.org/abs/2310.02772)

    本研究提出了一种名为脉冲累积转发（SAF）的方法，可以有效训练脉冲神经网络（SNNs）。SAF不仅可以减少前向过程中的操作次数，与Spike Representation和OTTT保持一致，而且可以解决SNNs训练中的难题。

    

    本文提出了一种新的脉冲神经网络（SNNs）训练范式，即脉冲累积转发（SAF）。已知SNNs具有高能效但难以训练的特点。许多研究者提出了各种方法来解决这个问题，其中时间上的在线训练（OTTT）是一种在每个时间步骤推断的方法，同时抑制内存成本。然而，为了在GPU上高效计算，OTTT需要进行脉冲序列操作和脉冲序列加权求和操作。此外，OTTT与Spike Representation（另一种训练方法）之间存在关联，但与Spike Representation的理论一致性尚未得到证明。我们的方法可以解决这些问题，即SAF可以在前向过程中减少一半的操作次数，并且可以从理论上证明SAF分别与Spike Representation和OTTT一致。此外，我们还确认了......

    In this article, we propose a new paradigm for training spiking neural networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are energy-efficient but difficult to train. Consequently, many researchers have proposed various methods to solve this problem, among which online training through time (OTTT) is a method that allows inferring at each time step while suppressing the memory cost. However, to compute efficiently on GPUs, OTTT requires operations with spike trains and weighted summation of spike trains during forwarding. In addition, OTTT has shown a relationship with the Spike Representation, an alternative training method, though theoretical agreement with Spike Representation has yet to be proven. Our proposed method can solve these problems; namely, SAF can halve the number of operations during the forward process, and it can be theoretically proven that SAF is consistent with the Spike Representation and OTTT, respectively. Furthermore, we confirmed the a
    
[^100]: 关于训练导数约束神经网络

    On Training Derivative-Constrained Neural Networks. (arXiv:2310.01649v1 [cs.LG])

    [http://arxiv.org/abs/2310.01649](http://arxiv.org/abs/2310.01649)

    该论文研究了导数约束神经网络（DC NN），提出了一种整合RELU激活函数（IReLU）来改进DC NN的训练，并探究了去归一化和标签缩放对于稳定DC训练的作用。研究还评估了这些方法在物理相关设置中的效果，证明了使用IReLU激活函数、去归一化和标签缩放的现有架构更好地融入了导数约束提供的训练信号。

    

    我们将神经网络对输入的预测相对于输入的（部分）导数作为额外的训练信号的情况称之为导数约束神经网络（DC NN）。这种情况在自然科学中的物理相关设置中很常见。我们提出了一种整合RELU (IReLU)激活函数，以改进DC NN的训练。我们还研究了去归一化和标签缩放以帮助稳定DC训练。我们在包括量子化学和科学机器学习（SciML）任务在内的物理相关设置上评估了我们的方法。我们证明了使用IReLU激活函数结合去归一化和标签缩放的现有架构更好地融入了导数约束提供的训练信号。

    We refer to the setting where the (partial) derivatives of a neural network's (NN's) predictions with respect to its inputs are used as additional training signal as a derivative-constrained (DC) NN. This situation is common in physics-informed settings in the natural sciences. We propose an integrated RELU (IReLU) activation function to improve training of DC NNs. We also investigate denormalization and label rescaling to help stabilize DC training. We evaluate our methods on physics-informed settings including quantum chemistry and Scientific Machine Learning (SciML) tasks. We demonstrate that existing architectures with IReLU activations combined with denormalization and label rescaling better incorporate training signal provided by derivative constraints.
    
[^101]: 《GenAI对抗人性：生成式人工智能和大型语言模型的邪恶应用》

    GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models. (arXiv:2310.00737v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2310.00737](http://arxiv.org/abs/2310.00737)

    这篇论文探讨了生成式人工智能和大型语言模型的潜在滥用，呼吁认识到这些挑战的紧迫性。研究揭示了这些技术在深度伪造、合成身份恶意活动以及虚假信息和欺诈方面可能带来的社会影响。

    

    生成式人工智能（GenAI）和大型语言模型（LLM）是技术的奇迹，以其在自然语言处理和多模式内容生成方面的卓越能力而受到赞扬，它们承诺带来一个变革的未来。但就像所有强大的工具一样，它们也有其阴影存在。想象一下生活在一个深度伪造与现实无法区分、合成身份组织恶意活动、以及有着无与伦比精确度的有针对性的虚假信息或欺诈手法的世界。欢迎来到GenAI应用的黑暗面。本文不仅是探索GenAI和LLMs潜在滥用的旅程，也是呼吁认识到面临的挑战的紧迫性。在我们航行于虚假信息活动、恶意内容生成与精密恶意软件构建的海洋中，我们将揭示这场我们正在见证的GenAI革命中的社会影响。

    Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are marvels of technology; celebrated for their prowess in natural language processing and multimodal content generation, they promise a transformative future. But as with all powerful tools, they come with their shadows. Picture living in a world where deepfakes are indistinguishable from reality, where synthetic identities orchestrate malicious campaigns, and where targeted misinformation or scams are crafted with unparalleled precision. Welcome to the darker side of GenAI applications. This article is not just a journey through the meanders of potential misuse of GenAI and LLMs, but also a call to recognize the urgency of the challenges ahead. As we navigate the seas of misinformation campaigns, malicious content generation, and the eerie creation of sophisticated malware, we'll uncover the societal implications that ripple through the GenAI revolution we are witnessing. From AI-powered botnets on social med
    
[^102]: LEGO-Prover: 使用不断增长的库进行神经定理证明

    LEGO-Prover: Neural Theorem Proving with Growing Libraries. (arXiv:2310.00656v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.00656](http://arxiv.org/abs/2310.00656)

    LEGO-Prover是一个使用不断增长的技能库的神经定理证明方法，可以使语言模型（LLMs）利用现有技能和创造新技能来证明定理。

    

    尽管大型语言模型（LLMs）取得了成功，但定理证明仍然是最难的推理任务之一，尚未完全解决。以往使用语言模型的方法取得了有希望的结果，但仍难以证明中学水平的定理。这些方法的一个常见限制是在整个定理证明过程中假设了一个固定的定理库。然而，众所周知，创造新的有用定理甚至新的理论不仅有帮助而且对于推动数学发展和证明更困难和深入的结果是至关重要和必要的。在这项工作中，我们介绍了LEGO-Prover，它采用一个包含经过验证的引理的不断增长的技能库，以增强用于定理证明的LLMs的能力。通过模块化构建证明，LEGO-Prover使LLMs能够利用从库检索到的现有技能，以及在证明过程中创建新技能。这些技能通过简化证明过程并提供更强大的推理能力。

    Despite the success of large language models (LLMs), the task of theorem proving still remains one of the hardest reasoning tasks that is far from being fully solved. Prior methods using language models have demonstrated promising results, but they still struggle to prove even middle school level theorems. One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process. However, as we all know, creating new useful theorems or even new theories is not only helpful but crucial and necessary for advancing mathematics and proving harder and deeper results. In this work, we present LEGO-Prover, which employs a growing skill library containing verified lemmas as skills to augment the capability of LLMs used in theorem proving. By constructing the proof modularly, LEGO-Prover enables LLMs to utilize existing skills retrieved from the library and to create new skills during the proving process. These skills are further evolved (by pro
    
[^103]: 构建保护隐私和安全的地理空间人工智能基础模型

    Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models. (arXiv:2309.17319v1 [cs.AI])

    [http://arxiv.org/abs/2309.17319](http://arxiv.org/abs/2309.17319)

    这篇论文讨论了地理空间人工智能基础模型的隐私和安全风险，并提出了预防和控制策略蓝图。

    

    最近几年，我们在人工智能的基础模型中取得了实质性的进展，包括语言、视觉和多模态模型。最近的研究突出了在地理空间人工智能中使用基础模型的潜力，称为GeoAI基础模型或Geo-Foundation模型，用于地理问题回答、遥感图像理解、地图生成和基于位置的服务等。然而，GeoAI基础模型的开发和应用可能带来严重的隐私和安全风险，这些风险到目前为止尚未得到充分的讨论或解决。本文介绍了GeoAI基础模型在整个生命周期中的潜在隐私和安全风险，并提出了全面的预防和控制策略蓝图。通过这篇展望性论文，我们希望引起地理空间领域的研究人员和决策者对GeoAI基础模型中固有的隐私和安全风险的关注。

    In recent years we have seen substantial advances in foundation models for artificial intelligence, including language, vision, and multimodal models. Recent studies have highlighted the potential of using foundation models in geospatial artificial intelligence, known as GeoAI Foundation Models or Geo-Foundation Models, for geographic question answering, remote sensing image understanding, map generation, and location-based services, among others. However, the development and application of GeoAI foundation models can pose serious privacy and security risks, which have not been fully discussed or addressed to date. This paper introduces the potential privacy and security risks throughout the lifecycle of GeoAI foundation models and proposes a comprehensive blueprint for preventative and control strategies. Through this vision paper, we hope to draw the attention of researchers and policymakers in geospatial domains to these privacy and security risks inherent in GeoAI foundation models
    
[^104]: PRiSM: 使用关系感知分数校准增强低资源文档级关系抽取

    PRiSM: Enhancing Low-Resource Document-Level Relation Extraction with Relation-Aware Score Calibration. (arXiv:2309.13869v1 [cs.CL] CROSS LISTED)

    [http://arxiv.org/abs/2309.13869](http://arxiv.org/abs/2309.13869)

    PRiSM是一种增强低资源文档级关系抽取的方法，通过关系感知分数校准来提高模型性能，成功地降低了在低资源环境下训练模型时的校准误差。

    

    文档级关系抽取（DocRE）旨在提取文档中所有实体对的关系。在DocRE中的一个关键挑战是注释这类数据的成本，需要大量的人力投入。因此，我们调查了低资源环境中的DocRE情况，并发现现有的在少量数据上训练的模型过高估计了NA（"no relation"）标签，导致性能受限。在这项工作中，我们从校准的角度来解决这个问题，提出了PRiSM，它可以根据关系语义信息来适应logits。我们在三个DocRE数据集上评估了我们的方法，并证明了将现有模型与PRiSM集成可以提高性能，F1分数提高了26.38%，而当用约3%的数据进行训练时，校准误差下降了36倍。代码可以在https://github.com/brightjade/PRiSM公开获取。

    Document-level relation extraction (DocRE) aims to extract relations of all entity pairs in a document. A key challenge in DocRE is the cost of annotating such data which requires intensive human effort. Thus, we investigate the case of DocRE in a low-resource setting, and we find that existing models trained on low data overestimate the NA ("no relation") label, causing limited performance. In this work, we approach the problem from a calibration perspective and propose PRiSM, which learns to adapt logits based on relation semantic information. We evaluate our method on three DocRE datasets and demonstrate that integrating existing models with PRiSM improves performance by as much as 26.38 F1 score, while the calibration error drops as much as 36 times when trained with about 3% of data. The code is publicly available at https://github.com/brightjade/PRiSM.
    
[^105]: MINT: 评估在与工具和语言反馈进行多轮交互中的LLMs的能力

    MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. (arXiv:2309.10691v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.10691](http://arxiv.org/abs/2309.10691)

    MINT是一个评估LLMs在多轮交互中解决任务能力的基准，通过使用工具和利用用户的自然语言反馈。它解决了当前评估协议忽略细致互动和低估自然语言反馈的问题，促进了研究基准评估和实际应用之间的一致性。

    

    为了解决复杂任务，大语言模型（LLMs）通常需要与用户进行多轮交互，有时候辅以外部工具的帮助。然而，当前的评估协议常常强调用单轮交流的基准性能，忽略了用户、LLMs和外部工具之间的细致互动，并低估了用户的自然语言反馈的重要性。这些疏忽导致了研究基准评估结果与实际应用情况之间的差异。我们引入了MINT，这是一个通过使用工具和利用用户的自然语言反馈来评估LLMs解决多轮交互任务能力的基准。为了保证可重复性，我们提供了一个评估框架，在这个框架中，LLMs可以通过执行Python代码来访问工具，并接收由GPT-4模拟的用户的自然语言反馈。我们重新利用了一系列多样的已建立评估数据集，重点关注推理、编码和决策方面。

    To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and careful
    
[^106]: 关于正则稀疏逻辑回归的研究

    On Regularized Sparse Logistic Regression. (arXiv:2309.05925v1 [cs.LG])

    [http://arxiv.org/abs/2309.05925](http://arxiv.org/abs/2309.05925)

    本文提出了解决正则稀疏逻辑回归的方法，包括$\ell_1$正则化稀疏逻辑回归和一些满足先决条件的非凸惩罚正则化稀疏逻辑回归。经验实验表明，这些算法能够以较低的计算成本有效地进行分类和特征选择。

    

    稀疏逻辑回归旨在同时进行高维数据的分类和特征选择。虽然有许多研究解决了$\ell_1$正则化逻辑回归问题，但对于与非凸惩罚相关的稀疏逻辑回归解决方案并没有等量的文献。本文提出了解决$\ell_1$正则化稀疏逻辑回归和一些满足一定先决条件的非凸惩罚正则化稀疏逻辑回归的方法，并采用类似的优化框架。在提出的优化框架中，我们利用不同的线搜索准则来保证不同正则化项的良好收敛性能。通过对真实世界数据集的二元分类任务进行经验实验，我们证明了我们提出的算法能够以较低的计算成本有效地进行分类和特征选择。

    Sparse logistic regression aims to perform classification and feature selection simultaneously for high-dimensional data. Although many studies have been done to solve $\ell_1$-regularized logistic regression, there is no equivalently abundant literature about solving sparse logistic regression associated with nonconvex penalties. In this paper, we propose to solve $\ell_1$-regularized sparse logistic regression and some nonconvex penalties-regularized sparse logistic regression, when the nonconvex penalties satisfy some prerequisites, with similar optimization frameworks. In the proposed optimization frameworks, we utilize different line search criteria to guarantee good convergence performance for different regularization terms. Empirical experiments on binary classification tasks with real-world datasets demonstrate our proposed algorithms are capable of performing classification and feature selection effectively with a lower computational cost.
    
[^107]: DePT:分解提示调整以实现参数高效微调

    DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05173](http://arxiv.org/abs/2309.05173)

    DePT通过将软提示分解为较短的软提示和一对低秩矩阵，并用两个不同的学习率来优化，以解决提示调整对训练和推理时间以及内存使用的影响，从而实现更好的性能。

    

    提示调整（PT）是一种将可训练的少量软提示向量附加到语言模型（LM）输入中的参数高效微调（PEFT）方法，已在各种任务和模型中显示出了有希望的结果。 与其他PEFT方法相比，PT的竞争性能可以在可训练参数更少的情况下保持，并且随着模型规模的扩大，其参数并不会显著增加。 但是，PT引入了额外的软提示标记，导致输入序列变长，这对于Transformer的二次复杂度而言，在训练和推理时间以及内存使用方面会产生显著影响。 这对于面临大量每日查询的大型语言模型（LLMs）尤其令人担忧。

    Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
    
[^108]: 纯蒙特卡洛反事实遗憾最小化

    Pure Monte Carlo Counterfactual Regret Minimization. (arXiv:2309.03084v1 [cs.AI])

    [http://arxiv.org/abs/2309.03084](http://arxiv.org/abs/2309.03084)

    纯蒙特卡洛反事实遗憾最小化算法（PCFR）是一种结合了反事实遗憾最小化（CFR）和虚拟游戏（FP）概念的新算法，能够与各种CFR变体相结合，包括蒙特卡洛CFR（MCCFR）。PCFR具有更好的性能和较快的收敛速度，同时降低了时间和空间复杂度。

    

    反事实遗憾最小化（CFR）及其变体是目前解决大规模不完全信息博弈的最佳算法。本文在CFR的基础上提出了一种名为纯CFR（PCFR）的新算法，以实现更好的性能。PCFR可以看作是CFR和虚拟游戏（FP）的结合，继承了CFR的反事实遗憾（值）的概念，并在下一次迭代中使用最佳响应策略而不是遗憾匹配策略。我们的理论证明了PCFR可以实现Blackwell可达性，使PCFR能够与包括蒙特卡洛CFR（MCCFR）在内的任何CFR变体相结合。由此产生的纯MCCFR（PMCCFR）可以大大降低时间和空间复杂度。特别地，PMCCFR的收敛速度至少比MCCFR快三倍。此外，由于PMCCFR不通过严格被支配策略的路径，我们开发了一种新的启动算法，受到了严格被支配策略的启示。

    Counterfactual Regret Minimization (CFR) and its variants are the best algorithms so far for solving large-scale incomplete information games. Building upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR) for achieving better performance. PCFR can be seen as a combination of CFR and Fictitious Play (FP), inheriting the concept of counterfactual regret (value) from CFR, and using the best response strategy instead of the regret matching strategy for the next iteration. Our theoretical proof that PCFR can achieve Blackwell approachability enables PCFR's ability to combine with any CFR variant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR) can significantly reduce time and space complexity. Particularly, the convergence speed of PMCCFR is at least three times more than that of MCCFR. In addition, since PMCCFR does not pass through the path of strictly dominated strategies, we developed a new warm-start algorithm inspired by the strictly dominated strat
    
[^109]: 基于学习的GNSS观测的NLOS检测与不确定性预测的Transformer增强LSTM网络

    Learning-based NLOS Detection and Uncertainty Prediction of GNSS Observations with Transformer-Enhanced LSTM Network. (arXiv:2309.00480v1 [cs.RO])

    [http://arxiv.org/abs/2309.00480](http://arxiv.org/abs/2309.00480)

    本研究提出了一种基于深度学习的方法，通过构建Transformer-like注意机制增强LSTM网络，检测GNSS观测中的NLOS接收并预测GNSS伪距误差，从而提高了车辆定位的精度和系统的可靠性。

    

    全球导航卫星系统（GNSS）在交通系统中对于精确和一致的车辆定位至关重要。然而，在城市峡谷等复杂环境中，由于多路径效应和非直达（NLOS）接收的影响，GNSS观测可以产生扭曲。在这种情况下，传统的分类与排除错误GNSS观测的方法可能会失败，导致不可靠的状态估计和不安全的系统操作。本研究提出了一种基于深度学习的方法，通过将GNSS观测视为空时建模问题，检测NLOS接收并预测GNSS伪距误差。与先前的工作相比，我们构建了一个类似Transformer的注意机制，增强了长短期记忆（LSTM）网络，提高了模型的性能和普适性。对于所提出的网络的训练和评估，我们使用了香港和亚琛的标记数据集。我们还介绍了一个数据集生成过程来标记...

    The global navigation satellite systems (GNSS) play a vital role in transport systems for accurate and consistent vehicle localization. However, GNSS observations can be distorted due to multipath effects and non-line-of-sight (NLOS) receptions in challenging environments such as urban canyons. In such cases, traditional methods to classify and exclude faulty GNSS observations may fail, leading to unreliable state estimation and unsafe system operations. This work proposes a Deep-Learning-based method to detect NLOS receptions and predict GNSS pseudorange errors by analyzing GNSS observations as a spatio-temporal modeling problem. Compared to previous works, we construct a transformer-like attention mechanism to enhance the long short-term memory (LSTM) networks, improving model performance and generalization. For the training and evaluation of the proposed network, we used labeled datasets from the cities of Hong Kong and Aachen. We also introduce a dataset generation process to label
    
[^110]: 使用基于图的多智能体强化学习学习协作信息传播

    Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning. (arXiv:2308.16198v1 [cs.LG])

    [http://arxiv.org/abs/2308.16198](http://arxiv.org/abs/2308.16198)

    本论文介绍了一种使用多智能体强化学习的方法来实现协作信息传播。通过提出分布式POMDP形式，在消息转发上实现了每个智能体的独立决策，相比传统的基于多点中继选择的启发式方法具有重大创新和贡献。同时，该方法利用图卷积强化学习和动态注意力机制捕捉关键网络特征，并提出了不同信息交换方式的两种方法进行评估。

    

    在现代通信系统中，高效可靠的信息传播对支持关键操作至关重要，如灾难响应、自动驾驶车辆和传感器网络。本文介绍了一种多智能体强化学习（MARL）方法，作为实现更为分散、高效和协作解决方案的重要进展。我们提出了一种用于信息传播的分布式POMDP（Decentralized-POMDP）形式，使得每个智能体可以独立决定消息的转发。这构成了一种从传统基于多点中继（MPR）选择的启发式方法的重大范式转移。我们的方法利用图卷积强化学习，采用具有动态注意力的图注意力网络（GAT）来捕捉关键网络特征。我们提出了两种方法，L-DGN和HL-DGN，它们在智能体之间交换的信息上有所不同。通过将我们的分散方法与基于MPR的方法进行比较，我们评估了其性能。

    In modern communication systems, efficient and reliable information dissemination is crucial for supporting critical operations across domains like disaster response, autonomous vehicles, and sensor networks. This paper introduces a Multi-Agent Reinforcement Learning (MARL) approach as a significant step forward in achieving more decentralized, efficient, and collaborative solutions. We propose a Decentralized-POMDP formulation for information dissemination, empowering each agent to independently decide on message forwarding. This constitutes a significant paradigm shift from traditional heuristics based on Multi-Point Relay (MPR) selection. Our approach harnesses Graph Convolutional Reinforcement Learning, employing Graph Attention Networks (GAT) with dynamic attention to capture essential network features. We propose two approaches, L-DGN and HL-DGN, which differ in the information that is exchanged among agents. We evaluate the performance of our decentralized approaches, by compari
    
[^111]: 基于分层强化学习的未知网络传播控制方法

    Spread Control Method on Unknown Networks Based on Hierarchical Reinforcement Learning. (arXiv:2308.14311v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.14311](http://arxiv.org/abs/2308.14311)

    本论文提出了一种基于分层强化学习的方法，用于在未知网络上进行流行病控制。模拟结果表明，该方法优于基线方法。

    

    COVID-19等流行病对公共卫生和社会构成严重威胁，因此有必要研究有效的方法来控制流行病在网络中的传播。以往关于流行病控制的研究往往假设完全了解网络结构，这在实际情况中很少成立。本文研究了在未知结构的网络上的流行病控制，并提出了一种基于分层强化学习的框架，以同时探索网络结构和控制流行病。为了减少行动空间和实现可计算性，我们的框架包含三个模块：策略选择模块，确定是探索结构还是移除节点以控制流行病；探索模块，负责选择要探索的节点；移除模块，决定移除哪些节点以停止流行病传播。模拟结果表明，我们提出的方法优于基线方法。

    Epidemics such as COVID-19 pose serious threats to public health and our society, and it is critical to investigate effective methods to control the spread of epidemics over networks. Prior works on epidemic control often assume complete knowledge of network structures, a presumption seldom valid in real-world situations. In this paper, we study epidemic control on networks with unknown structures, and propose a hierarchical reinforcement learning framework for joint network structure exploration and epidemic control. To reduce the action space and achieve computation tractability, our proposed framework contains three modules: the Policy Selection Module, which determines whether to explore the structure or remove nodes to control the epidemic; the Explore Module, responsible for selecting nodes to explore; and the Remove Module, which decides which nodes to remove to stop the epidemic spread. Simulation results show that our proposed method outperforms baseline methods.
    
[^112]: QKSAN: 量子核自注意力网络

    QKSAN: A Quantum Kernel Self-Attention Network. (arXiv:2308.13422v1 [quant-ph])

    [http://arxiv.org/abs/2308.13422](http://arxiv.org/abs/2308.13422)

    QKSAN是一个量子核自注意力网络，通过结合量子核方法和自注意力机制，提高了量子机器学习模型在大规模高维量子数据上的有效性。

    

    自注意力机制（SAM）擅长从数据内部提取重要信息，以提高模型的计算效率。然而，许多量子机器学习（QML）模型缺乏像SAM这样区分信息的固有连接的能力，这限制了它们在大规模高维量子数据上的有效性。为了解决这个问题，引入了量子核自注意力机制（QKSAM），它将量子核方法（QKM）的数据表示优势与SAM的有效信息提取能力相结合。基于QKSAM构建了一个量子核自注意力网络（QKSAN）框架，利用延迟测量原理（DMP）和条件测量技术，在计算过程中以概率测量的方式释放了一半的量子资源。量子核自注意力分数（QKSAS）确定测量条件并反映了量子系统的概率本质。

    Self-Attention Mechanism (SAM) is skilled at extracting important information from the interior of data to improve the computational efficiency of models. Nevertheless, many Quantum Machine Learning (QML) models lack the ability to distinguish the intrinsic connections of information like SAM, which limits their effectiveness on massive high-dimensional quantum data. To address this issue, a Quantum Kernel Self-Attention Mechanism (QKSAM) is introduced, which combines the data representation benefit of Quantum Kernel Methods (QKM) with the efficient information extraction capability of SAM. A Quantum Kernel Self-Attention Network (QKSAN) framework is built based on QKSAM, with Deferred Measurement Principle (DMP) and conditional measurement techniques, which releases half of the quantum resources with probabilistic measurements during computation. The Quantum Kernel Self-Attention Score (QKSAS) determines the measurement conditions and reflects the probabilistic nature of quantum syste
    
[^113]: 用于稀疏深度神经网络训练的多目标优化

    Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v1 [cs.LG])

    [http://arxiv.org/abs/2308.12243](http://arxiv.org/abs/2308.12243)

    这项工作提出了一种用于稀疏深度神经网络训练的多目标优化算法，通过加权Chebyshev标量化和增广Lagrangian方法，解决了同时优化多个任务时的挑战。

    

    在深度学习的各种场景中，会自然地出现不同的冲突优化准则。这些准则可以解决不同的主任务（如多任务学习设置），也可以解决主要任务和次要任务，例如损失最小化与稀疏性。通常的方法是简单地加权准则，但在凸设置中才有效。本文提出了一种多目标优化算法，对多任务深度神经网络（DNNs）进行训练，使用改进的加权Chebyshev标量化方法。通过使用这种标量化技术，算法可以识别原始问题的所有最优解，同时将其复杂性降低为一系列单目标问题。然后，使用增广Lagrangian方法来解决简化后的问题，从而可以使用常见的优化技术，如Adam和随机梯度下降，同时有效地处理约束条件。我们的工作旨在解决经济化问题。

    Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economi
    
[^114]: 作为用户模拟器的大型语言模型

    Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])

    [http://arxiv.org/abs/2308.11534](http://arxiv.org/abs/2308.11534)

    本文创新性地将从真实人机对话中提取的人类问题作为学习目标，并且训练了一个用户模拟器UserGPT，并使用生成的高质量合成对话数据集RealChat来训练助手模型ReaLM。实验证明，ReaLM在多个基准测试中超过了基准模型。

    

    闭源ChatGPT的卓越性能引发了对其民主化的努力，借助真实用户和ChatGPT对话的努力取得了显著进展，Vicuna是一个很好的例子。然而，目前的Baize和UltraChat等努力主要依靠ChatGPT根据指令模拟人类行为，而不是真实的人类学习，导致范围有限，多样性减弱，缺乏真正的多轮对话动态。为了解决上述问题，我们创新性地把从真实人机对话中提取的人类问题作为学习目标，并训练一个用户模拟器UserGPT来生成高质量的以人为中心的合成对话数据集RealChat。随后，该数据集训练我们的助手模型ReaLM。实验证明，ReaLM在Vicuna-Bench和MT-Bench中均超过了基准模型。

    The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, while current endeavors like Baize and UltraChat aim to auto-generate conversational data due to challenges in gathering human participation, they primarily rely on ChatGPT to simulate human behaviors based on directives rather than genuine human learning. This results in a limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we innovatively target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator, UserGPT, to produce a high-quality human-centric synthetic conversation dataset, RealChat. Subsequently, this dataset trains our assistant model, ReaLM. Experimentally, ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
    
[^115]: 专家负载很重要：以高准确性和低人工工作量运行网络

    Expert load matters: operating networks at high accuracy and low manual effort. (arXiv:2308.05035v1 [cs.AI])

    [http://arxiv.org/abs/2308.05035](http://arxiv.org/abs/2308.05035)

    在关键应用领域的人机协同系统中，为了确保最小错误，需要根据模型置信度设定操作点进行决策委托给专家。为了提高系统效用，需要考虑模型仅对准确样本具有自信的特点以及尽量减少委托给专家的样本数量。我们提出了置信度操作特性（COC）曲线来表示模型准确性与专家处理样本数量的权衡关系。这对于医疗保健等可用专家时间有限且昂贵的领域尤为关键。

    

    在人工智能与人类协同系统的关键应用中，为了确保最小错误，用户应根据模型的置信度设定操作点，确定何时将决策委托给人类专家。模型置信度低于操作点的样本将由专家手动分析，以避免错误。只有在考虑到两个方面的情况下，这样的系统才能真正有用：模型只对准确的样本具有自信，并且尽量减少委托给专家的样本数量。后者对于可用专家时间有限且昂贵的应用，如医疗保健，尤为关键。模型准确性与委托给专家样本数量之间的权衡可以用类似ROC曲线的曲线表示，我们称之为置信度操作特性（COC）曲线。在本文中，我们认为应该通过考虑到模型的置信度和专家处理的样本数量来训练深度神经网络。

    In human-AI collaboration systems for critical applications, in order to ensure minimal error, users should set an operating point based on model confidence to determine when the decision should be delegated to human experts. Samples for which model confidence is lower than the operating point would be manually analysed by experts to avoid mistakes. Such systems can become truly useful only if they consider two aspects: models should be confident only for samples for which they are accurate, and the number of samples delegated to experts should be minimized. The latter aspect is especially crucial for applications where available expert time is limited and expensive, such as healthcare. The trade-off between the model accuracy and the number of samples delegated to experts can be represented by a curve that is similar to an ROC curve, which we refer to as confidence operating characteristic (COC) curve. In this paper, we argue that deep neural networks should be trained by taking into 
    
[^116]: 通过对比学习在强化学习中发现层次化成就

    Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning. (arXiv:2307.03486v1 [cs.LG])

    [http://arxiv.org/abs/2307.03486](http://arxiv.org/abs/2307.03486)

    通过对比学习方法，我们在强化学习中提出了新的成就蒸馏方法，可以加强代理对下一个解锁成就的预测能力，并优于先前的模型驱动和层次化方法。

    

    在生成环境中发现具有层次结构的成就是一个重大挑战。这需要智能体具备广泛的能力，包括泛化和长期推理。许多先前的方法基于模型驱动或层次化方法，认为显式的长期规划模块对于学习层次化成就是有益的。然而，这些方法需要大量的环境交互或大型模型，限制了它们的实用性。在这项工作中，我们发现近期实施实践中的近端策略优化（PPO）算法优于先前的方法。此外，我们发现PPO智能体可以在一定程度上预测下一个要解锁的成就，尽管预测的置信度较低。基于这一观察，我们提出了一种新颖的对比学习方法，称为成就蒸馏，可以加强PPO智能体对下一个解锁成就的预测能力。

    Discovering achievements with a hierarchical structure on procedurally generated environments poses a significant challenge. This requires agents to possess a broad range of abilities, including generalization and long-term reasoning. Many prior methods are built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be beneficial for learning hierarchical achievements. However, these methods require an excessive amount of environment interactions or large model sizes, limiting their practicality. In this work, we identify that proximal policy optimization (PPO), a simple and versatile model-free algorithm, outperforms the prior methods with recent implementation practices. Moreover, we find that the PPO agent can predict the next achievement to be unlocked to some extent, though with low confidence. Based on this observation, we propose a novel contrastive learning method, called achievement distillation, that strengthens the 
    
[^117]: 用于超出分布可泛化性的大型视觉语言模型压缩

    Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v1 [cs.CV])

    [http://arxiv.org/abs/2307.03135](http://arxiv.org/abs/2307.03135)

    本文研究了针对大型视觉语言模型的模型压缩方法，将教师模型的视觉表示压缩到学生模型中。研究重点在于超出分布可泛化的问题，并提出了两个原则来增强学生模型的性能。

    

    大型视觉语言模型取得了出色的性能，但其规模和计算要求使它们在资源受限设备和时间敏感任务上的部署变得不切实际。模型压缩是创建更小、更快的模型以保持较大模型性能的有希望的方法。本文研究了将大型视觉语言模型中的视觉表示压缩到轻量级学生模型中的过程，使用小型或中型数据集。值得注意的是，本研究关注的是超出分布（OOD）可泛化的开放词汇问题，这在以往的模型压缩研究中被忽视了。我们从视觉和语言的角度提出了两个原则来增强学生模型的OOD可泛化性：（1）更好地模仿教师的视觉表示空间，并在视觉语言对齐方面谨慎地促进更好的一致性；（2）通过丰富学生模型的自举学习和数据扩充来提高OOD可泛化性。

    Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a smallor mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enric
    
[^118]: 弹性决策变压器

    Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.02484](http://arxiv.org/abs/2307.02484)

    弹性决策变压器（EDT）通过在测试时间进行动作推断时调整历史长度来实现轨迹拼接，填补了决策变压器（DT）在这一方面的性能差距，并且在多任务情况下胜过基于Q-Learning的方法。

    

    本文介绍了弹性决策变压器（EDT），它是现有决策变压器（DT）及其变体的重大进展。尽管DT声称能够生成最佳轨迹，但实证证据表明它在轨迹拼接方面存在困难，轨迹拼接是指从一组次优轨迹中生成最优或接近最优轨迹的过程。提出的EDT通过在测试时间进行动作推断时调整DT中维护的历史长度来实现轨迹拼接，从而使自己与众不同。此外，当前轨迹是最优的时候，EDT通过保持较长的历史，当当前轨迹是次优的时候，EDT通过保持较短的历史来优化轨迹，使其能够与更优的轨迹进行“拼接”。广泛的实验表明，EDT能够填补基于DT和基于Q-Learning方法之间的性能差距。特别是，EDT在多任务情况下胜过基于Q-Learning的方法。

    This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
    
[^119]: GIO：用于训练数据集选择的梯度信息优化

    GIO: Gradient Information Optimization for Training Dataset Selection. (arXiv:2306.11670v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11670](http://arxiv.org/abs/2306.11670)

    GIO是一种可伸缩且任务不可知的方法，通过对目标的简单放松和高效实现，可以在非常小的训练集上产生出色的结果。

    

    在训练模型时，通常有利于在可用训练样本的子集上进行训练，因为这些样本具有不同的质量，或者希望在不影响性能的情况下使用更少的样本进行训练。我们提出了梯度信息优化（GIO），这是一种可伸缩且任务不可知的方法，用于解决这个数据选择问题，它只需要一小组（未标记的）样本来代表目标分布。GIO从一个实践中难以处理的自然的信息理论目标开始。我们的贡献在于展示出通过对目标进行简单的放松和高效的实现，它可以被高度扩展。在机器翻译、拼写纠正和图像识别等实验中，我们证明了GIO在非常小的训练集上产生了出色的结果。这些发现对于GIO本身的不同表示模型和超参数是稳健的。GIO是任务和领域无关的，可以直接应用于新领域。

    It is often advantageous to train models on a subset of the available train examples, because the examples are of variable quality or because one would like to train with fewer examples, without sacrificing performance. We present Gradient Information Optimization (GIO), a scalable, task-agnostic approach to this data selection problem that requires only a small set of (unlabeled) examples representing a target distribution. GIO begins from a natural, information-theoretic objective that is intractable in practice. Our contribution is in showing that it can be made highly scalable through a simple relaxation of the objective and a highly efficient implementation. In experiments with machine translation, spelling correction, and image recognition, we show that GIO delivers outstanding results with very small train sets. These findings are robust to different representation models and hyperparameters for GIO itself. GIO is task- and domain-agnostic and can be applied out-of-the-box to ne
    
[^120]: 在具有部分在线状态信息的强化学习中，POMDP的理论难度和可计算性

    Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information. (arXiv:2306.08762v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08762](http://arxiv.org/abs/2306.08762)

    本论文研究了具有部分在线状态信息的POMDP问题的理论困难性和可计算性。作者通过建立下界得出一个惊人的难度结果：除非具有完整的在线状态信息，否则需要指数级的样本复杂度才能得到POMDP的最优策略解。然而，作者还发现了具有部分在线状态信息下的可计算POMDP类别，并提出了新的算法来证明其接近最优性。

    

    部分可观察的马尔可夫决策过程（POMDP）被广泛应用于捕捉许多现实世界的应用。然而，现有的理论结果已经表明，在一般的POMDP中学习可能是不可计算的，主要挑战在于缺乏潜在的状态信息。一个关键的基本问题是有多少在线状态信息（OSI）足以实现可计算性。在本文中，我们建立了一个下界，揭示了一个惊人的难度结果：除非我们具有完整的OSI，否则我们需要指数级的采样复杂度才能获得POMDP的$\epsilon$-最优策略解。尽管如此，受到我们下界设计的关键见解的启发，我们发现即使只有部分OSI，也存在重要的可计算的POMDP类别。特别地，对于具有部分OSI的两个新颖的POMDP类别，我们通过建立新的遗憾上下界证明了新的算法是接近最优的。

    Partially observable Markov decision processes (POMDPs) have been widely applied to capture many real-world applications. However, existing theoretical results have shown that learning in general POMDPs could be intractable, where the main challenge lies in the lack of latent state information. A key fundamental question here is how much online state information (OSI) is sufficient to achieve tractability. In this paper, we establish a lower bound that reveals a surprising hardness result: unless we have full OSI, we need an exponentially scaling sample complexity to obtain an $\epsilon$-optimal policy solution for POMDPs. Nonetheless, inspired by the key insights in our lower bound design, we find that there exist important tractable classes of POMDPs even with only partial OSI. In particular, for two novel classes of POMDPs with partial OSI, we provide new algorithms that are proved to be near-optimal by establishing new regret upper and lower bounds.
    
[^121]: UnDiff: 无条件扩散模型下的无监督语音修复

    UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion Model. (arXiv:2306.00721v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2306.00721](http://arxiv.org/abs/2306.00721)

    本文介绍了无条件扩散模型UnDiff，通过无监督学习解决了语音逆问题。该模型可以适应不同的语音处理任务，并在带宽扩展、去剪辑、编码和语音源分离等任务上表现良好。

    

    本文介绍了UnDiff，一种扩散概率模型，能够解决各种语音逆问题。它通过无条件方式训练语音波形生成，可以适应不同的任务，包括降级逆推、神经编码和源分离。在本文中，我们首先通过对比不同的神经网络架构和预条件域，解决了无条件波形生成的挑战性问题。之后，我们通过最新的后训练扩散模型的条件方法，展示了训练好的无条件扩散模型如何适应语音处理的不同任务。最后，我们展示了所提出技术在带宽扩展、去剪辑、编码和语音源分离等任务上的性能，并与基准进行了比较。源代码公开可用。

    This paper introduces UnDiff, a diffusion probabilistic model capable of solving various speech inverse tasks. Being once trained for speech waveform generation in an unconditional manner, it can be adapted to different tasks including degradation inversion, neural vocoding, and source separation. In this paper, we, first, tackle the challenging problem of unconditional waveform generation by comparing different neural architectures and preconditioning domains. After that, we demonstrate how the trained unconditional diffusion could be adapted to different tasks of speech processing by the means of recent developments in post-training conditioning of diffusion models. Finally, we demonstrate the performance of the proposed technique on the tasks of bandwidth extension, declipping, vocoding, and speech source separation and compare it to the baselines. The codes are publicly available.
    
[^122]: 使用基于文献的语境化学习生成新的科学方向

    Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14259](http://arxiv.org/abs/2305.14259)

    本文介绍了一种基于文献的发现方法，通过上下文化的学习生成新的科学方向，克服了标准方法在预测关联、忽略上下文等方面的局限性。模型使用了引文和知识图关系的网络，并使用大型语言模型进行评估，发现GPT4在生成创新思想方面表现出色。

    

    基于文献的发现（LBD）旨在通过挖掘论文并生成假设来发现新的科学知识。标准的LBD仅限于预测离散概念之间的两两关系（例如，药物和疾病的关联）。LBD还忽略了关键的上下文，例如实验设置（例如，药物评估的特定患者群体）和人类科学家考虑的背景知识和动机（例如，找到没有特定副作用的药物候选）。我们通过一种新颖的上下文化LBD（C-LBD）表述来解决这些局限性：以自然语言生成科学假设，同时将它们联系到控制假设搜索空间的上下文中。我们提出了一个建模框架，使用获得的引文和知识图关系的异构网络中的“灵感”，并创建了一个从论文中派生的新数据集。我们使用强大的大型语言模型（LLM）进行评估，发现GPT4倾向于生成具有创新性的思想。

    Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
    
[^123]: 声明提示下的可满足性辅助语言模型

    Satisfiability-Aided Language Models Using Declarative Prompting. (arXiv:2305.09656v1 [cs.CL])

    [http://arxiv.org/abs/2305.09656](http://arxiv.org/abs/2305.09656)

    本文提出了一种利用自动定理证明器和声明性任务规范的可满足性辅助语言建模方法，可以提高大型语言模型的推理能力。

    

    本文提出了一种新的可满足性辅助语言建模方法，用于提高大型语言模型的推理能力。我们使用一个大型语言模型生成一个声明性任务规范，并利用一个现成的自动定理证明器得出最终答案。该方法具有两个关键优点：第一，声明性规范比推理步骤更接近问题描述，因此大型语言模型可以更准确地解析它；第二，通过将实际推理任务委托给自动定理证明器，我们的方法可以保证正确性。

    Prior work has combined chain-of-thought prompting in large language models (LLMs) with programmatic representations to perform effective and transparent reasoning. While such an approach works very well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving tasks that require more sophisticated planning and search. In this paper, we propose a new satisfiability-aided language modeling approach for improving the reasoning capabilities of LLMs. We use an LLM to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer. This approach has two key advantages. The declarative specification is closer to the problem description than the reasoning steps are, so the LLM can parse it more accurately. Furthermore, by offloading the actual reasoning task to an automated theorem prover, our approach can guarantee the correctness o
    
[^124]: ChatGPT是一个好的因果推断器吗？全面评估

    Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v1 [cs.CL])

    [http://arxiv.org/abs/2305.07375](http://arxiv.org/abs/2305.07375)

    本文对ChatGPT的因果推理能力进行了首次全面评估，实验证明ChatGPT是一个好的因果解释者，但不是一个好的因果推理者，存在严重的因果幻觉问题，对于明确的因果关系表现良好。

    

    因果推理能力对于众多NLP应用至关重要。尽管ChatGPT在各种NLP任务中表现出令人印象深刻的新兴能力，但ChatGPT在因果推理方面的表现如何仍不清楚。本文对ChatGPT的因果推理能力进行了首次全面评估。实验证明，ChatGPT不是一个好的因果推理者，但是是一个好的因果解释者。此外，ChatGPT在因果推理方面存在严重的幻觉，可能是由于自然语言中因果关系和非因果关系的报告偏见，以及ChatGPT的升级过程，如RLHF。在上下文学习（ICL）和思维链（COT）技术方面，可能会进一步加剧这种因果幻觉。此外，ChatGPT的因果推理能力对于在提示中表达因果概念的词语非常敏感，并且封闭提示比开放提示表现更好。对于句子中的事件，ChatGPT擅长捕捉明确的因果关系。

    Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT's causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit caus
    
[^125]: 基于扩散的生成式人工智能在探索二维分子图的过渡态上的应用

    Diffusion-based Generative AI for Exploring Transition States from 2D Molecular Graphs. (arXiv:2304.12233v2 [physics.chem-ph] UPDATED)

    [http://arxiv.org/abs/2304.12233](http://arxiv.org/abs/2304.12233)

    本文提出了一种基于扩散的生成式方法(TSDiff)，用于从二维分子图中预测过渡态几何结构。与现有具有 3D 几何结构机器学习模型相比，TSdiff 的准确性和效率都更高。TSDiff 能够找到比参考数据库更优化的反应途径，在反应势垒更低的情况下找到更为优化的反应路径。

    

    探究过渡态几何结构对于阐明化学反应机理和模拟反应动力学至关重要。最近，机器学习模型在预测过渡态几何结构方面表现出了出色的性能。然而，它们需要反应物和产物的 3D 形态和方向作为输入，这需要大量的努力和计算成本。在这里，我们提出了一种基于随机扩散方法的生成式方法(TSDiff)，用于从二维分子图中预测过渡态几何结构。TSDiff在准确性和效率方面优于现有的具有 3D 几何结构的机器学习模型。此外，它可以从训练中了解到各种反应的过渡态几何分布，从而能够采样各种过渡态构象。因此，TSDiff 能够找到比参考数据库中更为有利的反应途径，在反应势垒更低的情况下找到更为优化的反应路径。这些结果表明，TSDiff 在加速化学反应和反应途径的发现方面具有潜在的价值。

    The exploration of transition state (TS) geometries is crucial for elucidating chemical reaction mechanisms and modeling their kinetics. Recently, machine learning (ML) models have shown remarkable performance for prediction of TS geometries. However, they require 3D conformations of reactants and products often with their appropriate orientations as input, which demands substantial efforts and computational cost. Here, we propose a generative approach based on the stochastic diffusion method, namely TSDiff, for prediction of TS geometries just from 2D molecular graphs. TSDiff outperformed the existing ML models with 3D geometries in terms of both accuracy and efficiency. Moreover, it enables to sample various TS conformations, because it learned the distribution of TS geometries for diverse reactions in training. Thus, TSDiff was able to find more favorable reaction pathways with lower barrier heights than those in the reference database. These results demonstrate that TSDiff shows pr
    
[^126]: 在大型语言模型中加强迭代增强的思维链提示

    Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models. (arXiv:2304.11657v1 [cs.CL])

    [http://arxiv.org/abs/2304.11657](http://arxiv.org/abs/2304.11657)

    本文提出 Iter-CoT 方法，在大型语言模型中进行迭代增强的思维链提示，通过选择具有适度难度的具有挑战性但可回答的问题，并伴随推理链作为示例，提高了模型的泛化能力，同时使模型能够更准确地生成推理链。

    

    通过逐步引导思维链 (CoT) 作为示范，大型语言模型 (LLMs) 可以在各种推理任务上实现高度有效的性能。然而，LLMs 生成的演示推理链容易出现错误，这可能会导致推理过程中的错误。此外，不恰当的示例 (过于简单或复杂) 可以影响在不同难度级别下的整体性能。我们引入了Iter-CoT (迭代引导思维链提示) 的迭代引导方法，用于选择实例并生成推理链。通过利用迭代增强，我们的方法使LLMs 自主更正错误，从而产生更精确、全面的推理链。同时，我们的方法选择具有适度难度的具有挑战性但可回答的问题，并伴随推理链作为示例，从而增强LLMs 的泛化能力。

    Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), an iterative bootstrapping approach for selecting exemplars and generating reasoning chains. By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains. Simultaneously, our approach selects challenging yet answerable questions accompanied by reasoning chains as exemplars with a moderate level of difficulty, which enhances the LLMs' generalizability 
    
[^127]: LLMMaps——大型语言模型分层评价的可视化隐喻

    LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models. (arXiv:2304.00457v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.00457](http://arxiv.org/abs/2304.00457)

    LLMMaps是一种分层评估大型语言模型性能的可视化技术，能够揭示取得高准确度和产生幻觉的子领域，并指导模型的进一步发展。

    

    大型语言模型(LLMs)在自然语言处理中取得了革命性的进展，并在各种任务中展示了惊人的能力。然而，它们容易产生幻觉，即模型在响应中暴露出不正确或错误的信息，这使得必须采用勤奋的评估方法。虽然LLM在特定知识领域中的表现通常是基于问答(Q&A)数据集进行评估，但这些评估通常仅报告整个领域的单个准确度数字，这一程序在透明度和模型改进方面存在问题。分层评估可以揭示可能更容易发生幻觉的子领域，从而有助于更好地评估LLMs的风险并指导它们的进一步发展。为支持这样的分层评估，我们提出了LLMMaps作为一种新的可视化技术，使用户能够根据Q&A数据集评估LLMs的性能。LLMMaps提供了对LLMs在不同子领域中的知识分布的详细洞察，允许用户放大领域的特定部分并探索模型性能上的差异。我们的实验证明，LLMMaps有助于识别出更容易出现LLM幻觉的子领域，并可以指导模型的发展，以改善这些领域的准确性。

    Large Language Models (LLMs) have revolutionized natural language processing and demonstrated impressive capabilities in various tasks. Unfortunately, they are prone to hallucinations, where the model exposes incorrect or false information in its responses, which renders diligent evaluation approaches mandatory. While LLM performance in specific knowledge fields is often evaluated based on question and answer (Q&A) datasets, such evaluations usually report only a single accuracy number for the entire field, a procedure which is problematic with respect to transparency and model improvement. A stratified evaluation could instead reveal subfields, where hallucinations are more likely to occur and thus help to better assess LLMs' risks and guide their further development. To support such stratified evaluations, we propose LLMMaps as a novel visualization technique that enables users to evaluate LLMs' performance with respect to Q&A datasets. LLMMaps provide detailed insights into LLMs' kn
    
[^128]: 分析和编辑暗藏后门的语言模型的内部机制

    Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. (arXiv:2302.12461v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12461](http://arxiv.org/abs/2302.12461)

    本研究分析并编辑暗藏后门的语言模型的内部机制，发现早期层的MLP模块和初始嵌入投影是后门机制中最重要的部分。通过使用PCP消融技术替换变压器模块，我们成功删除、插入和修改后门机制，并显著改善了后门的输出效果。

    

    数据集中的毒化是对大型语言模型的潜在安全威胁，可能导致暗藏后门的模型。关于暗藏后门语言模型的内部机制以及它们如何处理触发输入（例如，切换至有毒语言）的描述尚未找到。本文研究基于Transformer的暗藏后门语言模型的内部表示，并确定早期层的MLP模块与初始嵌入投影结合是后门机制中最重要的部分。我们利用这些知识来删除、插入和修改后门机制，并用工程化替代物降低MLP模块输出的重要性。为此，我们引入了基于主要成分的低秩矩阵的PCP消融技术，用其替换变压器模块。我们在暗藏后门的玩具模型、暗藏后门的大型模型和非暗藏后门的开源模型上展示了我们的结果。我们表明我们可以改善后门的输出效果。

    Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor r
    
[^129]: Pre-trained Vision and Language Models能否回答求知视觉问题？

    Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.11713](http://arxiv.org/abs/2302.11713)

    本研究介绍了一个专门针对无法仅凭常识知识回答的信息寻求问题而设计的视觉问答数据集InfoSeek。使用InfoSeek数据集，我们发现目前最先进的预训练多模态模型在回答求知视觉问题方面面临挑战，但在该数据集上进行微调可以激发模型使用细粒度知识。

    

    Pre-trained vision and language models在涉及图像和文本的任务中展示了领先的能力，包括视觉问答。然而，这些模型是否具备回答不仅仅查询视觉内容，而且还具有知识密集和信息寻求性质的问题的能力仍然不清楚。在本研究中，我们介绍了InfoSeek，一个专门针对无法仅凭常识知识回答的信息寻求问题而设计的视觉问答数据集。使用InfoSeek，我们分析了各种预训练的视觉问答模型，并深入了解它们的特点。我们的发现揭示了目前最先进的预训练多模态模型（如PaLI-X，BLIP2等）在回答求知视觉问题方面面临挑战，但在InfoSeek数据集上进行微调能够激发模型使用他们在预训练过程中学到的细粒度知识。此外，我们还展示了准确的视觉实体的重要性。

    Pre-trained vision and language models have demonstrated state-of-the-art capabilities over existing tasks involving images and texts, including visual question answering. However, it remains unclear whether these models possess the capability to answer questions that are not only querying visual content but knowledge-intensive and information-seeking. In this study, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered with only common sense knowledge. Using InfoSeek, we analyze various pre-trained visual question answering models and gain insights into their characteristics. Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.) face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during their pre-training. Furthermore, we show that accurate visual entit
    
[^130]: 可证明高效的离线目标条件强化学习与通用函数逼近和单一策略集中性研究

    Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability. (arXiv:2302.03770v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03770](http://arxiv.org/abs/2302.03770)

    本文提供了关于离线目标条件强化学习算法的理论分析，证明了经过轻微修改后，该算法在满足通用函数逼近时具有 O(poly(1/ε)) 的样本复杂度。

    

    目标条件强化学习（GCRL）是指学习通用技能，以达到不同的目标。特别是，离线GCRL只需要纯预先收集的数据集来执行训练任务，而无需与环境进行额外的交互。尽管离线GCRL越来越普遍，并且许多之前的工作已经证明了其实证成功，但对于大状态空间且离线数据集仅覆盖我们希望学习的策略的高效离线GCRL算法的理论理解尚未建立起来。本文对一种现有经验成功的离线GCRL算法进行了严格的理论分析。我们证明，通过轻微修改，该算法在具有通用函数逼近的情况下，具有$\widetilde{O}(\text{poly}(1/\epsilon))$样本复杂度（其中$\epsilon$是学习策略的期望非最优性）。

    Goal-conditioned reinforcement learning (GCRL) refers to learning general-purpose skills that aim to reach diverse goals. In particular, offline GCRL only requires purely pre-collected datasets to perform training tasks without additional interactions with the environment. Although offline GCRL has become increasingly prevalent and many previous works have demonstrated its empirical success, the theoretical understanding of efficient offline GCRL algorithms is not well established, especially when the state space is huge and the offline dataset only covers the policy we aim to learn. In this paper, we provide a rigorous theoretical analysis of an existing empirically successful offline GCRL algorithm. We prove that under slight modification, this algorithm enjoys an $\widetilde{O}(\text{poly}(1/\epsilon))$ sample complexity (where $\epsilon$ is the desired suboptimality of the learned policy) with general function approximation thanks to the property of (semi-)strong convexity of the o
    
[^131]: IM-IAD：工业制造中的图像异常检测基准

    IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing. (arXiv:2301.13359v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.13359](http://arxiv.org/abs/2301.13359)

    该论文提出了一个IM-IAD工业制造中的图像异常检测基准，通过统一的设置评估了16个算法在7个数据集上的性能，旨在解决目前这一领域研究的不规范问题。

    

    图像异常检测（IAD）是工业制造中一项新兴且重要的计算机视觉任务。近年来，许多先进的算法已经被发布，但它们的性能存在很大差异。我们认为，缺乏实际的工业制造设置很可能阻碍了这些方法在实际应用中的发展和使用。据我们所知，IAD方法尚未经过系统评估。因此，这使得研究人员很难分析它们，因为它们是为不同或特殊情况而设计的。为了解决这个问题，我们首先提出了一个统一的工业制造设置来评估这些算法的性能，包括几个方面，如各种监督级别（无监督vs半监督）、少样本学习、持续学习、噪声标签、内存使用和推断速度。此外，我们巧妙地构建了一个完整的图像异常检测基准（IM-IAD），该基准在统一的设置下包括了16个算法和7个主流数据集。

    Image anomaly detection (IAD) is an emerging and vital computer vision task in industrial manufacturing (IM). Recently many advanced algorithms have been published, but their performance deviates greatly. We realize that the lack of actual IM settings most probably hinders the development and usage of these methods in real-world applications. As far as we know, IAD methods are not evaluated systematically. As a result, this makes it difficult for researchers to analyze them because they are designed for different or special cases. To solve this problem, we first propose a uniform IM setting to assess how well these algorithms perform, which includes several aspects, i.e., various levels of supervision (unsupervised vs. semi-supervised), few-shot learning, continual learning, noisy labels, memory usage, and inference speed. Moreover, we skillfully build a comprehensive image anomaly detection benchmark (IM-IAD) that includes 16 algorithms on 7 mainstream datasets with uniform settings. 
    
[^132]: 适应离线算法解决带有赌徒反馈的组合多臂赌博问题的框架

    A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback. (arXiv:2301.13326v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13326](http://arxiv.org/abs/2301.13326)

    这个论文介绍了一个框架，用于将离线算法调整为只需要赌徒反馈的亚线性α-后悔方法来解决组合多臂赌博问题，该框架可以实现对于时间界T的预期累积α-后悔依赖度为O(T^（2/3）log（T^（1/3））)。

    

    我们研究了随机的组合多臂赌博问题，其中学习者只能访问赌徒反馈，并且奖励函数可以是非线性的。我们提供了一个通用的框架，将离线离散逼近算法调整为只需要赌徒反馈的亚线性α-后悔方法，实现了对于时间界T的预期累积α-后悔依赖度为O(T^（2/3）log（T^（1/3））)。该框架只需要离线算法对函数评估中的小误差具有鲁棒性。适应过程甚至不需要显式地知道离线逼近算法--离线算法可以被用作黑盒子子程序。为了证明所提出的框架的实用性，将该框架应用于子模最大化的不同应用中。在具有背包约束的子模最大化中，新的CMAB算法优于开发的完全赌徒方法。

    We investigate the problem of stochastic, combinatorial multi-armed bandits where the learner only has access to bandit feedback and the reward function can be non-linear. We provide a general framework for adapting discrete offline approximation algorithms into sublinear $\alpha$-regret methods that only require bandit feedback, achieving $\mathcal{O}\left(T^\frac{2}{3}\log(T)^\frac{1}{3}\right)$ expected cumulative $\alpha$-regret dependence on the horizon $T$. The framework only requires the offline algorithms to be robust to small errors in function evaluation. The adaptation procedure does not even require explicit knowledge of the offline approximation algorithm -- the offline algorithm can be used as a black box subroutine. To demonstrate the utility of the proposed framework, the proposed framework is applied to diverse applications in submodular maximization. The new CMAB algorithms for submodular maximization with knapsack constraints outperform a full-bandit method developed
    
[^133]: 深度强化学习中的自动内在奖励塑造探索方法研究

    Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10886](http://arxiv.org/abs/2301.10886)

    本文提出了一种名为AIRS的自动内在奖励塑造探索方法，可以提供高质量的内在激励以增强强化学习中的探索性能；并开发了高效可靠的内在奖励工具包。实验表明，AIRS性能卓越，能够胜过基准方案。

    

    本文提出了一种名为AIRS的自动内在奖励塑造方法，通过智能和适应性的塑造函数，提供高质量的内在激励以增强强化学习中的探索性能。AIRS可以根据实时估计的任务回报从预定义的函数集中选择塑造函数，提供可靠的探索激励并解决偏置目标问题。此外，我们开发了一个内在奖励工具包，提供多种内在奖励方法的高效可靠实现方式。我们将AIRS应用在MiniGrid、Procgen和DeepMind控制套件的多项任务中进行测试。大量仿真结果表明，AIRS可以胜过基准方案，并具有简单的架构和卓越的性能。

    We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and adaptively provides high-quality intrinsic rewards to enhance exploration in reinforcement learning (RL). More specifically, AIRS selects shaping function from a predefined set based on the estimated task return in real-time, providing reliable exploration incentives and alleviating the biased objective problem. Moreover, we develop an intrinsic reward toolkit to provide efficient and reliable implementations of diverse intrinsic reward approaches. We test AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite. Extensive simulation demonstrates that AIRS can outperform the benchmarking schemes and achieve superior performance with simple architecture.
    
[^134]: 走向数据和知识驱动的人工智能：神经符号计算综述

    Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing. (arXiv:2210.15889v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.15889](http://arxiv.org/abs/2210.15889)

    神经符号计算是将符号和统计认知范式整合的研究领域，在推理和解释性以及神经网络学习等方面具有潜力。本文综述了该领域的最新进展和重要贡献，并探讨了其成功应用案例和未来发展方向。

    

    多年来，神经符号计算（NeSy）一直是人工智能（AI）领域的研究热点，追求符号和统计认知范式的整合。由于NeSy在符号表示的推理和可解释性以及神经网络的强大学习中具有潜力，它可能成为下一代AI的催化剂。本文系统综述了NeSy研究的最新进展和重要贡献。首先，我们介绍了该领域的研究历史，涵盖了早期工作和基础知识。我们进一步讨论了背景概念，并确定了推动NeSy发展的关键因素。随后，我们按照几个主要特征对近期的里程碑方法进行分类，包括神经符号整合、知识表示、知识嵌入和功能性。接下来，我们简要讨论了成功的应用案例，以及NeSy领域的挑战和未来发展方向。

    Neural-symbolic computing (NeSy), which pursues the integration of the symbolic and statistical paradigms of cognition, has been an active research area of Artificial Intelligence (AI) for many years. As NeSy shows promise of reconciling the advantages of reasoning and interpretability of symbolic representation and robust learning in neural networks, it may serve as a catalyst for the next generation of AI. In the present paper, we provide a systematic overview of the recent developments and important contributions of NeSy research. Firstly, we introduce study history of this area, covering early work and foundations. We further discuss background concepts and identify key driving factors behind the development of NeSy. Afterward, we categorize recent landmark approaches along several main characteristics that underline this research paradigm, including neural-symbolic integration, knowledge representation, knowledge embedding, and functionality. Next, we briefly discuss the successfu
    
[^135]: 演化机器人学中形态变异的作用：最大化性能和鲁棒性

    The Role of Morphological Variation in Evolutionary Robotics: Maximizing Performance and Robustness. (arXiv:2208.02809v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2208.02809](http://arxiv.org/abs/2208.02809)

    本文提出了一种测量形态变异影响的方法，并分析了变异幅度和引入方式与演化体性能和鲁棒性之间的关系。

    

    为了获得鲁棒且能够克服现实差距的解决方案，有必要暴露用于演化机器人控制器到不同条件的演化算法。然而，我们还没有方法来分析和理解影响演化过程的各种形态条件的影响，也没有方法来选择合适的变异范围。通过形态条件，我们指的是机器人的起始状态以及由于噪声导致操作过程中传感器读数的变化。在本文中，我们介绍了一种可以测量这些形态变异影响的方法，并分析了变异幅度、引入方式与演化体性能和鲁棒性之间的关系。我们的结果表明：（i）演化算法能够容忍具有很高影响的形态变异；（ii）影响机器人行为的变异对演化体的性能有影响。

    Exposing an Evolutionary Algorithm that is used to evolve robot controllers to variable conditions is necessary to obtain solutions which are robust and can cross the reality gap. However, we do not yet have methods for analyzing and understanding the impact of the varying morphological conditions which impact the evolutionary process, and therefore for choosing suitable variation ranges. By morphological conditions, we refer to the starting state of the robot, and to variations in its sensor readings during operation due to noise. In this article, we introduce a method that permits us to measure the impact of these morphological variations and we analyze the relation between the amplitude of variations, the modality with which they are introduced, and the performance and robustness of evolving agents. Our results demonstrate that (i) the evolutionary algorithm can tolerate morphological variations which have a very high impact, (ii) variations affecting the actions of the agent are to
    
[^136]: MemSAC: 大规模无监督领域自适应的记忆增强样本一致性

    MemSAC: Memory Augmented Sample Consistency for Large Scale Unsupervised Domain Adaptation. (arXiv:2207.12389v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.12389](http://arxiv.org/abs/2207.12389)

    MemSAC提出了一种记忆增强样本一致性方法，通过利用源域和目标域之间的样本级相似性实现判别转移，并且在大规模数据集上表现出明显的优势。

    

    实际的现实世界数据集引入了无监督领域自适应的新挑战，如类间区分度小，现有的仅依赖于域不变性的方法无法很好地处理。在这项工作中，我们提出了MemSAC，它利用源域和目标域之间的样本级相似性实现判别转移，并可扩展到大量类别。为此，我们首先引入了一种记忆增强方法，以有效地提取标记源域和未标记目标域实例之间的成对相似性关系，适合处理任意数量的类别。接下来，我们提出并从理论上证明了一种新颖的对比损失的变体，以促进类内跨域样本之间的局部一致性，同时确保类别之间的分离，从而保持从源域到目标域的判别转移。我们验证了MemSAC的优势和显著性。

    Practical real world datasets with plentiful categories introduce new challenges for unsupervised domain adaptation like small inter-class discriminability, that existing approaches relying on domain invariance alone cannot handle sufficiently well. In this work we propose MemSAC, which exploits sample level similarity across source and target domains to achieve discriminative transfer, along with architectures that scale to a large number of categories. For this purpose, we first introduce a memory augmented approach to efficiently extract pairwise similarity relations between labeled source and unlabeled target domain instances, suited to handle an arbitrary number of classes. Next, we propose and theoretically justify a novel variant of the contrastive loss to promote local consistency among within-class cross domain samples while enforcing separation between classes, thus preserving discriminative transfer from source to target. We validate the advantages of MemSAC with significant
    
[^137]: mGPT: 少样本学习者走向多语言（arXiv:2204.07580v2 [cs.CL] 更新）

    mGPT: Few-Shot Learners Go Multilingual. (arXiv:2204.07580v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2204.07580](http://arxiv.org/abs/2204.07580)

    本文介绍了两种自回归GPT样式模型，分别使用13亿和130亿个参数，在60种语言中训练，并展示了与Facebook最近发布的XGLM模型性能相当的结果。这为低资源语言的自然语言处理提供了更多可能性。

    

    最近的研究报告称，自回归语言模型可以通过零样本和少样本学习范式成功解决许多自然语言处理任务，这为使用预训练语言模型开辟了新的可能性。本文介绍了两种自回归GPT样式模型，其参数分别为13亿和130亿，使用维基百科和巨大干净爬取的语料库训练了25个语系中的60种语言。我们使用GPT-2源代码和稀疏注意机制复现了GPT-3架构；Deepspeed和Megatron框架使我们能够有效地并行化训练和推断步骤。所得到的模型在性能上与Facebook最近发布的XGLM模型相当，在覆盖更多语言的同时增强了独联体国家和俄罗斯小国家等低资源语言的自然语言处理可能性。我们详细说明了架构设计的动机，详细描述了数据准备流程，并训练了五个小版本的模型。

    Recent studies report that autoregressive language models can successfully solve many NLP tasks via zero- and few-shot learning paradigms, which opens up new possibilities for using the pre-trained language models. This paper introduces two autoregressive GPT-like models with 1.3 billion and 13 billion parameters trained on 60 languages from 25 language families using Wikipedia and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron frameworks allow us to parallelize the training and inference steps effectively. The resulting models show performance on par with the recently released XGLM models by Facebook, covering more languages and enhancing NLP possibilities for low resource languages of CIS countries and Russian small nations. We detail the motivation for the choices of the architecture design, thoroughly describe the data preparation pipeline, and train five small versions of the model t
    
[^138]: 一种用于研究图画现实主义的机器学习范式：康斯特布尔的云是否比他的同时代更真实？

    A Machine Learning Paradigm for Studying Pictorial Realism: Are Constable's Clouds More Real than His Contemporaries?. (arXiv:2202.09348v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2202.09348](http://arxiv.org/abs/2202.09348)

    本研究提出了一种用于研究图画现实主义的新的机器学习范式，通过测量艺术家绘制的云和云的照片之间的相似度来评估写实主义。实验结果表明，康斯特布尔比他的同时代更加一致地呈现形式上的特征。

    

    英国山水画家约翰·康斯特布尔被认为是19世纪欧洲绘画现实主义运动的奠基人。康斯特布尔的绘制的天空，尤其是他的同时代认为非常准确，现在许多观众也有相同的感受。然而，评估类似康斯特布尔这样的写实画作的准确性是主观或直觉的，即使对于专业艺术史学家来说也很困难，难以确定康斯特布尔的天空与他的同时代有什么不同之处。我们的目标是为康斯特布尔的写实主义提供更客观的理解。我们提出了一种新的基于机器学习的图画现实主义研究范式，以可解释的方式进行评估。我们的框架通过测量像康斯特布尔这样以天空闻名的艺术家绘制的云和云的照片之间的相似度来评估写实主义。云分类的实验结果表明，康斯特布尔比他的同时代更加一致地呈现形式上的特征。

    The British landscape painter John Constable is considered foundational for the Realist movement in 19th-century European painting. Constable's painted skies, in particular, were seen as remarkably accurate by his contemporaries, an impression shared by many viewers today. Yet, assessing the accuracy of realist paintings like Constable's is subjective or intuitive, even for professional art historians, making it difficult to say with certainty what set Constable's skies apart from those of his contemporaries. Our goal is to contribute to a more objective understanding of Constable's realism. We propose a new machine-learning-based paradigm for studying pictorial realism in an explainable way. Our framework assesses realism by measuring the similarity between clouds painted by artists noted for their skies, like Constable, and photographs of clouds. The experimental results of cloud classification show that Constable approximates more consistently than his contemporaries the formal feat
    
[^139]: 从小数据集中实现联合学习

    Federated Learning from Small Datasets. (arXiv:2110.03469v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03469](http://arxiv.org/abs/2110.03469)

    本文介绍了一种在小数据集上实现联合学习的新方法，通过将模型聚合与本地模型的置换相结合，可以更高效地在数据稀疏的领域进行训练。

    

    联合学习允许多个参与方在不共享本地数据的情况下协同训练一个联合模型。这在医疗领域等数据本身分布分散、无法公开的情况下，为机器学习应用提供了可能。在实践中，通常通过聚合本地模型实现联合训练，而本地训练目标的期望与全局目标相似。然而，由于本地数据集通常很小，导致本地目标与全局目标差异很大，从而使联合学习失败。我们提出了一种新颖的方法，将模型聚合与本地模型的置换相结合。这种置换将每个本地模型暴露给一系列本地数据集，从而在数据稀疏的领域中实现更高效的训练。这使得可以在极小的本地数据集上进行训练，例如跨医院的患者数据，同时保持联合学习的训练效率和隐私保护的好处。

    Federated learning allows multiple parties to collaboratively train a joint model without sharing local data. This enables applications of machine learning in settings of inherently distributed, undisclosable data such as in the medical domain. In practice, joint training is usually achieved by aggregating local models, for which local training objectives have to be in expectation similar to the joint (global) objective. Often, however, local datasets are so small that local objectives differ greatly from the global objective, resulting in federated learning to fail. We propose a novel approach that intertwines model aggregations with permutations of local models. The permutations expose each local model to a daisy chain of local datasets resulting in more efficient training in data-sparse domains. This enables training on extremely small local datasets, such as patient data across hospitals, while retaining the training efficiency and privacy benefits of federated learning.
    

