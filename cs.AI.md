# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Improving Generalization in Task-oriented Dialogues with Workflows and Action Plans.](http://arxiv.org/abs/2306.01729) | 本论文提出了在任务导向对话中提高泛化性能的方法，通过增强对话上下文，使用有效的工作流名称和行动计划编码来完成多步骤任务的执行，解决了大型预训练语言模型无法可靠执行训练期间未见过的新多步骤任务的问题。 |
| [^2] | [Denoising Diffusion Semantic Segmentation with Mask Prior Modeling.](http://arxiv.org/abs/2306.01721) | 本文提出了一种利用去噪扩散生成模型建模的mask先验，改进现有的语义分割方法。该方法达到了在多个基准数据集上的最先进效果，超参数调整较少。 |
| [^3] | [Resolving Interference When Merging Models.](http://arxiv.org/abs/2306.01708) | 本文揭示了现有模型合并技术存在的干扰问题，提出了具有广泛适用性的解决方案，可显着提高合并后模型的性能。 |
| [^4] | [Is Generative Modeling-based Stylization Necessary for Domain Adaptation in Regression Tasks?.](http://arxiv.org/abs/2306.01706) | 本研究探讨了生成建模风格化技术在领域自适应回归任务中是否必要，发现与分类任务相比，其对于回归任务的影响较小。 |
| [^5] | [The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles.](http://arxiv.org/abs/2306.01705) | Transformer使用稠密的自注意力机制，使得在深度网络中可能的连接模式数量呈指数级增长。我们提出了信息通路假说，利用这种特性将连接模式分解为相互独立的信息通路，减少训练过程中的内存和计算成本以及提高模型的泛化能力。 |
| [^6] | [Affinity Clustering Framework for Data Debiasing Using Pairwise Distribution Discrepancy.](http://arxiv.org/abs/2306.01699) | 该论文提出了一种基于亲和性聚类的数据扩充方法MASC，通过同类数据集中的亲和聚类和保护数据的共享达到数据集的平衡，从而解决数据集代表性偏见问题。 |
| [^7] | [GateON: an unsupervised method for large scale continual learning.](http://arxiv.org/abs/2306.01690) | GateON是一种用于大规模连续学习的无监督方法，通过可学习的活动门控和参数相关性的在线估计来防止重要知识被覆盖，同时通过定点神经元的重新激活机制解决了网络饱和的问题。 |
| [^8] | [XAI Renaissance: Redefining Interpretability in Medical Diagnostic Models.](http://arxiv.org/abs/2306.01668) | 本文探讨了可解释人工智能（XAI）在革新医学诊断模型的创新方法和方法论，并通过揭示潜在的决策过程来赋予医疗保健专业人员理解和信任这些模型的能力。这些新技术潜力巨大，有望改变医疗保健的格局，从而提高患者的结果并建立对AI驱动诊断系统的信任。 |
| [^9] | [SourceP: Smart Ponzi Schemes Detection on Ethereum Using Pre-training Model with Data Flow.](http://arxiv.org/abs/2306.01665) | 本文提出了一种使用预训练模型和数据流的方法，通过智能合约的源代码特征，实现检测以太坊上的智能庞兹骗局。该方法提高了模型的可解释性和降低了数据获取和特征提取的难度。 |
| [^10] | [DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation.](http://arxiv.org/abs/2306.01657) | 本文提出了一种名为DiffusEmp的框架，该框架基于条件扩散语言模型，并使用多级控制信号来生成更加细致和个性化的共情回应。 |
| [^11] | [Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network.](http://arxiv.org/abs/2306.01631) | 本研究提出了一种新的方法，在分子结构和生物医学知识图谱中集成多个领域信息，通过自我监督策略预先训练更广泛和更强大的表示，并在化学属性预测任务上展示出出色的性能。 |
| [^12] | [HomE: Homography-Equivariant Video Representation Learning.](http://arxiv.org/abs/2306.01623) | 本文提出了一种新的多视角视频表示学习方法，通过学习隐式映射，形成了一个具有同变性的表示空间，能够对行动识别和行人意图预测任务获得最先进的结果。 |
| [^13] | [An Evaluation of Log Parsing with ChatGPT.](http://arxiv.org/abs/2306.01590) | 本文评估了ChatGPT在日志解析方面的能力，结果表明它可以通过适当的提示实现有前途的日志解析结果，特别是在few-shot提示下。 |
| [^14] | [Probabilistic Concept Bottleneck Models.](http://arxiv.org/abs/2306.01574) | 本文提出了概率概念瓶颈模型（ProbCBM），通过建立概率概念嵌入来解决数据中概念存在模糊性的问题，提高了CBM模型的可靠性及解释的可信度。 |
| [^15] | [PassGPT: Password Modeling and (Guided) Generation with Large Language Models.](http://arxiv.org/abs/2306.01545) | 本研究使用大型语言模型PassGPT进行密码建模和生成，该模型比基于GAN的现有方法更准确，能生成符合任意限制的密码，为提高密码强度估计器提供了潜在的帮助。 |
| [^16] | [Group channel pruning and spatial attention distilling for object detection.](http://arxiv.org/abs/2306.01526) | 本研究提出了一种针对对象检测的三阶段模型压缩方法，通过动态稀疏训练、组通道剪枝和空间关注提取大幅缩小了模型大小、参数数量和计算复杂度，并在多个基准数据集上实现了有竞争力的对象检测精度。 |
| [^17] | [One for All: Unified Workload Prediction for Dynamic Multi-tenant Edge Cloud Platforms.](http://arxiv.org/abs/2306.01507) | 提出了一种全局汇集和静态内容意识的端到端框架DynEformer，其中设计了全局汇集和信息合并机制，可为动态多租户边缘云平台提供统一的工作负载预测方案。 |
| [^18] | [Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations.](http://arxiv.org/abs/2306.01505) | 本文提出了一种监督式对抗性对比学习（SACL）框架，用于学习类别分布结构表示，通过联合类别分布对比学习目标，有效利用标签级特性一致性并保留细粒度的类内特性，实现了在对话情感识别中最先进的结果。 |
| [^19] | [Accelerating science with human-aware artificial intelligence.](http://arxiv.org/abs/2306.01495) | 本文介绍如何通过纳入人类专业知识分布、训练无监督模型进行模拟推理，显著改善AI对未来发现预测准确性，并产生科学上有前途、想象力带来的自然假设，从而加速人类发现或探索其盲点。 |
| [^20] | [Robust low-rank training via approximate orthonormal constraints.](http://arxiv.org/abs/2306.01485) | 通过追加正交约束，从而在保持低秩矩阵分解前提下提高深度神经网络的鲁棒性与准确率。 |
| [^21] | [Theoretical Behavior of XAI Methods in the Presence of Suppressor Variables.](http://arxiv.org/abs/2306.01464) | 本论文综合研究表明，XAI 方法在存在抑制变量时解释可能出现误导性，需要进行更加理论化和经验化的研究，确保其应用的正确性和可能性。 |
| [^22] | [Deep Q-Learning versus Proximal Policy Optimization: Performance Comparison in a Material Sorting Task.](http://arxiv.org/abs/2306.01451) | 本文比较了深度 Q-Learning 和近端策略优化算法在材料排序任务中的表现，结果表明PPO在所有评估指标方面的表现都更优。策略优化算法在高维状态和动作空间问题中有优势。 |
| [^23] | [Unsupervised Paraphrasing of Multiword Expressions.](http://arxiv.org/abs/2306.01443) | 本文提出了一种无监督的多词表达式改写方法，使用的数据和工具非常简单，且实验结果表明，该方法在熟语语义文本相似度任务中性能良好。 |
| [^24] | [A Modular Test Bed for Reinforcement Learning Incorporation into Industrial Applications.](http://arxiv.org/abs/2306.01440) | 本论文探究了如何运用强化学习技术提高工业应用的效率，以传输和组装货物为例，实现缩短上市时间、大规模定制和批量级别的生产。 |
| [^25] | [Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction.](http://arxiv.org/abs/2306.01439) | 该论文介绍了一种名为NUDGE的策略，利用训练好的基于神经网络的代理来引导逻辑规则的搜索，实现了可解释和可解释的策略。 |
| [^26] | [On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions.](http://arxiv.org/abs/2306.01431) | 本文介绍了一份关于联邦学习中知识编辑的广泛调查，总结了最新技术，确定了挑战和机遇，并概述了未来的研究方向。 |
| [^27] | [An Architecture for Deploying Reinforcement Learning in Industrial Environments.](http://arxiv.org/abs/2306.01420) | 本文提出一种在工业环境中部署强化学习的架构，该架构基于OPC UA，结合数字孪生的设置扩展了标准RL状态，并定义了一种信息模型。我们通过创建概念验证并在控制系统中确定最优策略，验证了该架构的可行性。 |
| [^28] | [Knowledge Graph Reasoning over Entities and Numerical Values.](http://arxiv.org/abs/2306.01399) | 本文提出了一种新方法，将数值推理纳入知识图谱推理，将数值视为一等公民并提供了一个统一的框架，用于对实体和数值进行推理，实现了对复杂查询的有效和高效回答。 |
| [^29] | [Adaptive Message Quantization and Parallelization for Distributed Full-graph GNN Training.](http://arxiv.org/abs/2306.01381) | 本文提出了一种高效的GNN训练系统AdaQP，通过随机量化跨设备传输的消息以降低通信流量，并提倡边缘节点和中心节点之间的通信-计算并行化，以加快分布式全图GNN训练速度。 |
| [^30] | [A systematic literature review on the code smells datasets and validation mechanisms.](http://arxiv.org/abs/2306.01377) | 代码异味检测工具的准确性取决于所使用的数据集，现有数据集存在样本不平衡、缺乏支持严重级别和仅限于Java语言等问题。 |
| [^31] | [Energy-Efficient UAV-Assisted IoT Data Collection via TSP-Based Solution Space Reduction.](http://arxiv.org/abs/2306.01355) | 本文提出了一种基于TSP的UAV辅助物联网数据收集框架，通过解空间缩减和算法优化，实现了能源高效、低复杂度、高效收集分布式传感器数据的目标。 |
| [^32] | [Federated Domain Generalization: A Survey.](http://arxiv.org/abs/2306.01334) | 该论文调查了联邦领域泛化的研究领域，提到了联邦学习和领域泛化技术的优势，以及在泛化联邦模型时面临的挑战。 |
| [^33] | [Navigating Fairness in Radiology AI: Concepts, Consequences,and Crucial Considerations.](http://arxiv.org/abs/2306.01333) | 本文讨论了放射学AI中的公平性问题，介绍了Aequitas偏差审计工具包并阐明了公平性评估的基本指标，强调了考虑AI伦理影响的重要性。 |
| [^34] | [Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?.](http://arxiv.org/abs/2306.01323) | 本研究证明GNN在同构图中的同构节点和异构图中的异构节点上表现良好，而在另一组节点上表现不佳。该研究提出了一种新框架，通过使用加权聚合提高GNN在不同结构模式节点上的表现，有效解决结构差异性问题。 |
| [^35] | [EPIC: Graph Augmentation with Edit Path Interpolation via Learnable Cost.](http://arxiv.org/abs/2306.01310) | EPIC提出了一种基于插值的方法来增强图数据集，通过利用图编辑距离生成与原始图相似但有结构变化的新图，从而提高了分类模型的泛化能力。 |
| [^36] | [Egocentric Planning for Scalable Embodied Task Achievement.](http://arxiv.org/abs/2306.01295) | 本文提出了一种自我中心规划的方法，结合符号规划和面向对象的POMDP模型，成为高扩展性任务完成的一种新途径。 |
| [^37] | [KL-Divergence Guided Temperature Sampling.](http://arxiv.org/abs/2306.01286) | 该论文提出了一种新的温度采样算法，通过KL-散度引导动态调整温度，从而缓解多样性和可归因性之间的权衡，实验证明该算法在对话问答和摘要任务中表现优异。 |
| [^38] | [Recent Advances in Graph-based Machine Learning for Applications in Smart Urban Transportation Systems.](http://arxiv.org/abs/2306.01282) | 智能交通系统（ITS）是现代交通基础设施的重要组成部分。已出现基于复杂数据的数据驱动解决方案以解决各种与ITS相关的挑战。图形机器学习方法成为了智能交通系统领域的日益重要的研究重点。 |
| [^39] | [Multi-Robot Path Planning Combining Heuristics and Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2306.01270) | 提出了一种多机器人路径规划方法MAPPOHR，该方法结合了启发式搜索、经验规则和多智能体强化学习。实验结果表明，该方法在规划效率和避碰能力方面优于现有方法。 |
| [^40] | [Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators.](http://arxiv.org/abs/2306.01242) | 本研究提出了一个基础框架-"负责任的任务自动化（ResponsibleTA）"，使大型语言模型可以负责任地作为任务协同工具。该框架增强了LLM的三种能力：预测任务可行性、验证任务完整性以及增强任务安全性。 |
| [^41] | [Predicting affinity ties in a surname network.](http://arxiv.org/abs/2306.01218) | 本文基于一个姓氏亲缘网络，通过建立知识库完成模型预测新联系的形成并发现共享邻居和嵌入空间中的邻居对于预测都十分重要，这一发现对于解释圣地亚哥精英内婚中的高水平很有帮助。 |
| [^42] | [Estimating Semantic Similarity between In-Domain and Out-of-Domain Samples.](http://arxiv.org/abs/2306.01206) | 本文研究如何以原则性的方式分析模型在域内和域外设置下的性能，最终找出一种无监督方法识别OOD / OODist样本。 |
| [^43] | [An Effective Meaningful Way to Evaluate Survival Models.](http://arxiv.org/abs/2306.01196) | 该论文提出了一种有效的方法来评估生存模型的性能，使用伪观察值的MAE指标能够准确地排名模型的性能，发现这种方法比其他替代方法更好。 |
| [^44] | [Integrated Sensing-Communication-Computation for Edge Artificial Intelligence.](http://arxiv.org/abs/2306.01162) | 面向边缘人工智能的集成感知-通信-计算技术对于提高资源利用率以及实现边缘学习和边缘人工智能推理任务的定制目标至关重要。 |
| [^45] | [Faster Causal Attention Over Large Sequences Through Sparse Flash Attention.](http://arxiv.org/abs/2306.01160) | 本文介绍了一种新的稀疏Flash注意力机制，能够快速处理大序列中的因果关系，且速度提高了多倍，可以作为任何基于Transformer的语言模型中密集自我注意力的替代方案，并在多个设置中产生了最先进的结果。 |
| [^46] | [Augmented Modular Reinforcement Learning based on Heterogeneous Knowledge.](http://arxiv.org/abs/2306.01158) | 该论文提出了增强模块化强化学习（AMRL），使用仲裁器来选择异构模块，并无缝地整合不同类型的知识。该方法能够减缓强化学习中的一些低效问题，有望在深度强化学习领域得到应用。 |
| [^47] | [Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding.](http://arxiv.org/abs/2306.01157) | 本文提出了一种名为Delphic不确定性的方法来解决离线强化学习中的隐藏混淆问题，并通过实验表明该方法在非可识别的隐藏混淆情况下优于现有算法。 |
| [^48] | [Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning.](http://arxiv.org/abs/2306.01150) | 本文研究了任务定义在指令学习中的作用，发现当删除描述任务输出的内容时模型性能才会显著下降，提出了一种自动算法可以压缩任务定义，删除60\%标记仍可以提高模型性能。 |
| [^49] | [AI Liability Insurance With an Example in AI-Powered E-diagnosis System.](http://arxiv.org/abs/2306.01149) | 本文章研究了AI责任保险，并以AI辅助的电子诊断系统为例进行了定量风险评估。AI责任保险可以作为监管机制来激励合规行为，并为高质量AI系统提供证书。 |
| [^50] | [Smooth Monotonic Networks.](http://arxiv.org/abs/2306.01147) | 本文提出了一种新的神经网络模块--平滑min-max(SMM)网络，相比于传统的min-max(MM)神经网络结构简单易用，在单调建模方面表现优异。 |
| [^51] | [Reconstruction Distortion of Learned Image Compression with Imperceptible Perturbations.](http://arxiv.org/abs/2306.01125) | 本文引入了一种不可察觉攻击方法，旨在有效地降低学习图像压缩的重构质量，通过引入基于Frobenius范数的损失函数生成对抗样本，进一步利用高频分量对人眼视觉不敏感的特点，以确保扰动保持不显眼，该方法在Kodak数据集上表现出了有效性。 |
| [^52] | [Differentially Private Episodic Reinforcement Learning with Heavy-tailed Rewards.](http://arxiv.org/abs/2306.01121) | 本研究针对重尾奖励的有限步骤表格马尔可夫决策过程问题探讨了差分隐私限制下的两种框架，即价值迭代和策略优化，同时考虑了联合差分隐私和本地差分隐私模型，并为两种情况提供了遗憾上限。 |
| [^53] | [The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only.](http://arxiv.org/abs/2306.01116) | 本文发现经过适当的筛选和去重后，仅凭网络数据就能训练出强大的语言模型，这甚至明显优于使用筛选后的语料库训练。研究还表明，即使在广泛筛选后，从网络中提取的高质量数据仍然充足，这为训练更大、更强大的语言模型带来了希望。 |
| [^54] | [Comparative Study on the Effects of Noise in ML-Based Anxiety Detection.](http://arxiv.org/abs/2306.01110) | 本研究探究了噪声如何影响基于机器学习的焦虑检测模型，并开发出在嘈杂的现实环境中具有抗干扰性和适应性的模型，以推进该领域的发展。 |
| [^55] | [Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization.](http://arxiv.org/abs/2306.01103) | 本文提出了一种考虑标签和环境因果独立性的方法来解决图形超出分布（OOD）泛化问题，通过敌对训练策略来联合优化属性以获得有效结果，实验证明LECI显着优于之前的方法。 |
| [^56] | [LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization.](http://arxiv.org/abs/2306.01102) | 本文介绍了利用大语言模型和多样性优化算法相结合的 LLMatic 神经结构搜索算法。该算法在CIFAR-10数据集进行测试，仅进行2000次搜索即可产生高性能网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。 |
| [^57] | [Fast Matrix Multiplication Without Tears: A Constraint Programming Approach.](http://arxiv.org/abs/2306.01097) | 本文提出了一种基于约束编程的方法，以寻找快速矩阵乘法的非交换算法或提供不可行性证明。我们通过打破对称性的约束条件和有效的不等式约束来修剪搜索空间，可以在合理的时间内找到已知的最佳算法或改进的算法。 |
| [^58] | [Large-Batch, Neural Multi-Objective Bayesian Optimization.](http://arxiv.org/abs/2306.01095) | 本文提出了一种针对数据密集型问题和多目标优化设置的贝叶斯优化框架，该方法利用了贝叶斯神经网络代理建模和可扩展、具有不确定性的收购策略，能够在最少迭代次数的情况下高效地进行优化。 |
| [^59] | [UCAS-IIE-NLP at SemEval-2023 Task 12: Enhancing Generalization of Multilingual BERT for Low-resource Sentiment Analysis.](http://arxiv.org/abs/2306.01093) | 本文介绍了 UCAS-IIE-NLP 设计的 SACL-XLMR 系统，它利用词典式多语言BERT进行情感感知表示学习，使用监督性对抗式对比学习技术进行情感传播结构化表示学习，用于低资源情感分析，且在多语言和零样本情感分类子任务中表现出色，并且在零样本分类子任务中获得了第一名。 |
| [^60] | [4DSR-GCN: 4D Video Point Cloud Upsampling using Graph Convolutional Networks.](http://arxiv.org/abs/2306.01081) | 本文提出使用图卷积网络的4D视频点云上采样方法，并展示了其在质量和定量上皆优于现有技术。 |
| [^61] | [Quantization-Aware and Tensor-Compressed Training of Transformers for Natural Language Understanding.](http://arxiv.org/abs/2306.01076) | 本研究提出了一种量化感知张量压缩训练方法，用于减少基于Transformer的模型的模型大小，算术运算和最终运行时延。该方法经过逐层蒸馏后在文本蕴含和情感分析任务中表现出良好性能。 |
| [^62] | [TimelineQA: A Benchmark for Question Answering over Timelines.](http://arxiv.org/abs/2306.01069) | TimelineQA是一个用于查询生活日志的加速进展的基准测试，涉及时间和地理信息，已经公开发布，并使用两种最先进的模型进行了实验，但这两种模型均未达到人类表现水平。 |
| [^63] | [An FPGA Architecture for Online Learning using the Tsetlin Machine.](http://arxiv.org/abs/2306.01027) | 本文提出了一种基于FPGA架构的在线学习系统，通过采用低复杂度的Tsetlin Machine机器学习算法，在运行时提供了离线和在线学习管理。这种架构能够按需进行训练，并且能够在操作期间交错地进行推理和训练。 |
| [^64] | [A Vitual-Force Based Swarm Algorithm for Balanced Circular Bin Packing Problems.](http://arxiv.org/abs/2306.01021) | 本文描述了一种基于虚拟力系统的群智能算法，用以解决平衡圆形装箱问题，并在各种基准测试中得到验证，具有很好的解决效果。 |
| [^65] | [PV2TEA: Patching Visual Modality to Textual-Established Information Extraction.](http://arxiv.org/abs/2306.01016) | PV2TEA提出了一种基于编码器-解码器架构的信息抽取模型，在多模态注释困难的情况下解决了跨模态集成的问题，并提出了三种偏差降低方案。 |
| [^66] | [Graph-Level Embedding for Time-Evolving Graphs.](http://arxiv.org/abs/2306.01012) | 本论文提出了一种针对动态网络的图级嵌入方法，并通过生成节点的时间上下文来训练语言模型，以生成低维度的图级表示，对于下游的图形相似性排序、图形同构和异常检测等任务具有重要的意义。 |
| [^67] | [Examining the Emergence of Deductive Reasoning in Generative Language Models.](http://arxiv.org/abs/2306.01009) | 本研究调查了不同规模的生成语言模型的演绎推理能力，发现随着规模的增加，推理能力增强，长度不会影响大部分模型表现。 |
| [^68] | [Credit Card Fraud Detection Using Asexual Reproduction Optimization.](http://arxiv.org/abs/2306.01008) | 本文介绍了一种利用无性繁殖优化算法进行信用卡欺诈检测的方法，与其他现有方法相比，该方法能够显著降低训练时间，并在重要的欺诈检测问题中增加召回率。 |
| [^69] | [Towards Fair Disentangled Online Learning for Changing Environments.](http://arxiv.org/abs/2306.01007) | 本论文提出了一种面向变化环境的在线学习算法，该算法通过将模型参数划分为环境不变部分和环境特定部分，从而实现了数据公平性。通过大量的实验，证明了该算法的有效性。 |
| [^70] | [AbODE: Ab Initio Antibody Design using Conjoined ODEs.](http://arxiv.org/abs/2306.01005) | 该论文提出了一种新的从头设计抗体的方法AbODE，该方法使用连续微分注意力来应对蛋白折叠、逆向折叠和对接等挑战，并与时间网络和图匹配网络具有基本联系。 |
| [^71] | [AoM: Detecting Aspect-oriented Information for Multimodal Aspect-Based Sentiment Analysis.](http://arxiv.org/abs/2306.01004) | 本文提出了一个面向方面的方法(AoM)来检测与方面相关的语义和情感信息，并明确地将情感嵌入到AoM中。实验证明AoM在改进多模态基于方面的情感分析(MABSA)方面非常有效。 |
| [^72] | [DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model.](http://arxiv.org/abs/2306.01001) | 本文提出了一种扩散模型中的负荷预测不确定性量化方法，采用Seq2Seq网络结构来分离两种类型的不确定性并处理异常情况，不仅着眼于预测条件期望值。 |
| [^73] | [Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection.](http://arxiv.org/abs/2306.00006) | 本文针对图形异常监测数据集中存在的一类同型现象，提出了一种新的无监督异常评分度量——当前节点亲和力，并通过学习量身定制的节点表示，实现了截断亲和力最大化（TAM）方法，优化在原始图形结构上进行，能够有效进行双重One-Class的GAD。 |
| [^74] | [Blockwise Stochastic Variance-Reduced Methods with Parallel Speedup for Multi-Block Bilevel Optimization.](http://arxiv.org/abs/2305.18730) | 本文提出了两种基于方差约简的优化算法，以实现对多块双层优化问题的高效求解，同时匹配单块标准 BO 问题的最优复杂度、实现并行化加速，以及避免计算高维度的 Hessian 矩阵的逆估计。 |
| [^75] | [Faith and Fate: Limits of Transformers on Compositionality.](http://arxiv.org/abs/2305.18654) | 研究了Transformer模型在三个代表性组合型任务中的表现，发现其通过线性子图匹配解决多步组合推理问题。 |
| [^76] | [Multiscale Positive-Unlabeled Detection of AI-Generated Texts.](http://arxiv.org/abs/2305.18149) | 本文提出了一种多尺度正负样本的训练框架，以解决多尺度AI生成文本的检测问题。通过将短机器文本标记为“未标记”来重新表述文本分类问题，并提出了一个规则化损失函数来优化检测性能，有效性能显著优于现有的方法。 |
| [^77] | [Exploring Better Text Image Translation with Multimodal Codebook.](http://arxiv.org/abs/2305.17415) | 该研究提出了一个基于多模态码本的文本图片翻译模型。通过构建一个多阶段训练框架，利用了额外的双语文本和光学字符识别数据集，该模型能够将图像与相关文本关联起来，提供有用的补充信息，取得了比目前最先进的方法更好的效果。 |
| [^78] | [Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art.](http://arxiv.org/abs/2305.16259) | 本文简要概述了长文本的神经自然语言处理的现状，主要包括文档分类和摘要，涵盖了情感分析，同时还探讨了长文本NLP的主要挑战、问题和解决方案。 |
| [^79] | [Linguistic Properties of Truthful Response.](http://arxiv.org/abs/2305.15875) | 该论文研究了LLM的不真实回答现象，发现GPT-3模型对给定提示的回答在语言特性上很相似。同时，该论文证明了在没有评估内容本身的情况下，仅依赖模型回答的风格成分即可分类真实性。 |
| [^80] | [Privacy in Multimodal Federated Human Activity Recognition.](http://arxiv.org/abs/2305.12134) | 本文研究了多模式联邦人类活动识别中的隐私问题。通过一个特定的系统，联邦学习可以提供更好的隐私保护，同时不会损失人类活动识别的准确性。 |
| [^81] | [Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature.](http://arxiv.org/abs/2305.11033) | 本文综述了最近文献中的可视化问答问题，提供了对该领域的深入分析和比较，包括结果、最新技术、常见错误以及未来研究的潜在改进点。 |
| [^82] | [On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation.](http://arxiv.org/abs/2305.10930) | 零样本多语言神经机器翻译容易出现“离谱问题”，本文提出的简单且有效的算法LAVS可以通过增加语言之间的KL分歧显著降低这个问题。 |
| [^83] | ["I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation.](http://arxiv.org/abs/2305.09941) | 本论文研究了如何以TGNB人群的声音为中心，评估开放式语言生成中的偏见。通过理解TGNB个体的经历，提出了以TGNB人群为中心的OLG系统评估框架，并且包括一个为TGNB人群设计的调查工具和分析方法。 |
| [^84] | [Measuring Consistency in Text-based Financial Forecasting Models.](http://arxiv.org/abs/2305.08524) | 金融预测中模型的一致性对于建立用户信任至关重要，但目前金融预测方法很少考虑这一点，我们提出了一种评估金融文本逻辑一致性的评估工具FinTrust，并使用它发现最先进的金融预测NLP模型的一致性较差。 |
| [^85] | [Benchmark dataset and instance generator for Real-World Three-Dimensional Bin Packing Problems.](http://arxiv.org/abs/2304.14712) | 本文提出了一个真实世界装箱问题的基准数据集和实例生成器，可以用来比较不同装箱算法的性能。 |
| [^86] | [VGOS: Voxel Grid Optimization for View Synthesis from Sparse Inputs.](http://arxiv.org/abs/2304.13386) | 本论文介绍了一种名为VGOS的方法，可以快速从稀疏输入（3-10个视图）中重建放射场，以解决体素网格更容易过度拟合训练视图的问题，并提出了增量体素训练策略来避免过度拟合。 |
| [^87] | [On Dynamic Program Decompositions of Static Risk Measures.](http://arxiv.org/abs/2304.12477) | 本文证明了现有的CVaR和EVaR风险度量动态分解是真实风险值的严格高估计，然而VaR存在精确的动态分解。 |
| [^88] | [On the Possibilities of AI-Generated Text Detection.](http://arxiv.org/abs/2304.04736) | 该研究探讨了AI生成文本检测的可能性，提出了精确的样本复杂度界限，并指出了设计更准确的检测方法和提高透明度的挑战。 |
| [^89] | [Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders.](http://arxiv.org/abs/2304.01016) | 本文提出了一种通过结构压缩和模型尺寸不对称的双编码器模型 KALE，有效提高密集信息检索的推理效率，同时允许查询编码器的有效压缩，而无需进行全部的再训练或索引生成，此方法能够生成超过DistilBERT性能的模型。 |
| [^90] | [Subject-driven Text-to-Image Generation via Apprenticeship Learning.](http://arxiv.org/abs/2304.00186) | 该论文提出了一种基于徒弟学习的面向主题的文本到图像生成器SuTI，能够通过将大量基于主题的专家模型的数据输入徒弟模型，学习并推断出新主题的最佳专家模型，从而生成高品质的自定义图像，且速度比传统方法更快。 |
| [^91] | [Do intermediate feature coalitions aid explainability of black-box models?.](http://arxiv.org/abs/2303.11920) | 本文引入了中间概念的级别结构，利用领域专家建立部分-整体关系，从而在不同抽象级别上帮助解释黑盒模型。 |
| [^92] | [Locally Regularized Neural Differential Equations: Some Black Boxes Were Meant to Remain Closed!.](http://arxiv.org/abs/2303.02262) | 该论文提出了一种局部正则化的神经微分方程模型，通过启发式成本控制训练过程以学习易于积分的动力系统，同时提高预测速度，并保留了模型的自适应特性。 |
| [^93] | [Fluid Transformers and Creative Analogies: Exploring Large Language Models' Capacity for Augmenting Cross-Domain Analogical Creativity.](http://arxiv.org/abs/2302.12832) | 本文系统地探讨了大语言模型增强跨领域类比创造力的能力，结果表明LLM生成的跨领域类比在问题重构中具有实际帮助，并且存在一些潜在的危害，需要注意。 |
| [^94] | [Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness.](http://arxiv.org/abs/2302.10893) | 这篇论文提出了一种名为“公平扩散”的新策略，可以在生成文本到图像模型部署后减轻偏见并使模型接受公平性指导。 |
| [^95] | [ChatGPT: Jack of all trades, master of none.](http://arxiv.org/abs/2302.10724) | 本研究检验了 ChatGPT 在 25 个不同的 NLP 任务上的性能，它是一个万能的 AI 模型，但无关紧要的表现可能会对某些任务的表现产生负面影响。 |
| [^96] | [Is Distance Matrix Enough for Geometric Deep Learning?.](http://arxiv.org/abs/2302.05743) | 本文证明了消息传递神经网络（MPNNs）不能学习几何信息，提出了$k$-DisGNNs可以利用距离矩阵中的信息，并建立了几何深度学习和传统图表示学习之间的联系。 |
| [^97] | [Oscillation-free Quantization for Low-bit Vision Transformers.](http://arxiv.org/abs/2302.02210) | 研究发现，可学习比例因子加剧了权重振荡，本文提出三种技术以解决这个问题，并在多个基准测试上显著提高了模型的性能。 |
| [^98] | [Probably Anytime-Safe Stochastic Combinatorial Semi-Bandits.](http://arxiv.org/abs/2301.13393) | 本文提出了可能任何时候安全的随机组合半臂赌博问题，并设计出算法PASCombUCB在时间轴上最小化后悔值。 |
| [^99] | [CHAD: Charlotte Anomaly Dataset.](http://arxiv.org/abs/2212.09258) | CHAD是一个商业停车场环境的高分辨率、多相机异常数据集，是第一个为每个演员包括边界框、身份和姿势注释的异常数据集，同时包括各种环境因素。使用CHAD强调了更准确的异常检测算法所需的现实数据集的重要性。 |
| [^100] | [BKinD-3D: Self-Supervised 3D Keypoint Discovery from Multi-View Videos.](http://arxiv.org/abs/2212.07401) | BKinD-3D是一种新的无需2D或3D监督的自监督三维关键点发现方法，使用编码器-解码器架构和3D体积热力图对多个视图之间的时空差异进行重建，还使用了一个学习到的3D主体骨架的关节长度约束。 |
| [^101] | [PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection.](http://arxiv.org/abs/2212.04708) | PATO是一个策略辅助的远程操作系统，通过自动执行部分演示采集过程，并仅在不确定要执行哪个子任务或行为时请求人类输入，提高了数据采集效率，同时减少了人工操作员的心理负担，实现单个操作员并行控制多个机器人的功能。 |
| [^102] | [PrefRec: Recommender Systems with Human Preferences for Reinforcing Long-term User Engagement.](http://arxiv.org/abs/2212.02779) | 该论文提出了一种基于人类偏好的推荐系统范式PrefRec，允许强化学习推荐系统从用户历史行为偏好中学习，以优化长期用户参与度。 |
| [^103] | [Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text.](http://arxiv.org/abs/2211.11300) | 本文提出了一种多层知识蒸馏方法，融合了语言模型的训练和微调方法来进行文本中的离群检测，实验结果表明其有效性。 |
| [^104] | [Formalizing Statistical Causality via Modal Logic.](http://arxiv.org/abs/2210.16751) | 提出了一种基于模态逻辑的形式语言，用于描述和解释统计因果关系，并且能够指定和解释统计因果推断的正确性。 |
| [^105] | [Local Model Reconstruction Attacks in Federated Learning and their Uses.](http://arxiv.org/abs/2210.16205) | 本文研究了联邦学习中的本地模型重构攻击，攻击者可以通过重构客户端的模型更有效地触发其他攻击，并提出了一种新颖的基于模型的属性推导攻击，实证结果表明攻击有效，可应用于回归和分类任务。 |
| [^106] | [Broken Neural Scaling Laws.](http://arxiv.org/abs/2210.14891) | 本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。 |
| [^107] | [Oracles & Followers: Stackelberg Equilibria in Deep Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2210.11942) | 本文提出了一个通用框架，将Stackelberg均衡搜索作为多智能体RL问题进行实现，提出了一种利用上下文策略的方法，并在标准和新颖的基准领域上进行实验评估，证明相比之前的方法，样本效率得到了极大提高。 |
| [^108] | [Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization.](http://arxiv.org/abs/2210.04492) | 本文提出了名为UDDIA的统一去毒化和去偏见框架，它能够有效地消除有毒语言和减少社会偏见，同时保持流畅性。 |
| [^109] | [Do We Need Explainable AI in Companies? Investigation of Challenges, Expectations, and Chances from Employees' Perspective.](http://arxiv.org/abs/2210.03527) | 本论文探究员工对(X)AI的需求和态度。研究发现员工普遍认为解释性AI对公司的成功非常重要。 |
| [^110] | [Long-Horizon Planning and Execution with Functional Object-Oriented Networks.](http://arxiv.org/abs/2207.05800) | 本研究提出了一种利用物体级别知识作为功能对象导向网络进行任务规划和执行的方法，并且演示了在CoppeliaSim中应用该方法进行长期任务规划的效果。 |
| [^111] | [Dynamic Spatial Sparsification for Efficient Vision Transformers and Convolutional Neural Networks.](http://arxiv.org/abs/2207.01580) | 基于视觉变压器中的空间稀疏性，本文提出了一种动态标记稀疏化框架，可加速各种结构的模型，被剪枝的冗余标记由轻量级预测模块逐步动态估计。 |
| [^112] | [Subject Membership Inference Attacks in Federated Learning.](http://arxiv.org/abs/2206.03317) | 本文研究跨边界联邦学习中主体级别的隐私，并提出两种新颖的黑盒推理攻击方法。 |
| [^113] | [Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN.](http://arxiv.org/abs/2205.13943) | 本文提出一种无架构偏见的遮蔽图像建模框架（A$^2$MIM），既适用于Transformers也适用于CNNs，并通过学习中级交互关系来提高特征提取的一般性能。 |
| [^114] | [Learning from Physical Human Feedback: An Object-Centric One-Shot Adaptation Method.](http://arxiv.org/abs/2203.04951) | 本文提出的Object Preference Adaptation (OPA)方法通过学习人类针对特定物体的反馈，在单次干预后进行机器人行为的自适应，提高了数据效率和适用性。 |
| [^115] | [Reward is enough for convex MDPs.](http://arxiv.org/abs/2106.00661) | 本文研究了凸性马尔可夫决策过程，发现无法使用静态奖励函数表达目标，提出了一个元算法解决此问题，并统一了文献中的现有算法。 |
| [^116] | [Generative Actor-Critic: An Off-policy Algorithm Using the Push-forward Model.](http://arxiv.org/abs/2105.03733) | 本文提出基于推进模型的无密度离线算法 GAC，使用最大均值差熵正则化器平衡探索和开发之间的关系，并使用自适应机制提高算法的稳定性和鲁棒性。实验结果表明，推进策略能够提高算法的探索效率和渐近性能。 |
| [^117] | [QCBA: Improving Rule Classifiers Learned from Quantitative Data by Recovering Information Lost by Discretisation.](http://arxiv.org/abs/1711.10166) | 本文提出的QCBA方法可以用于改进基于数值类型数据学习的规则分类器，恢复预离散化过程中丢失的信息，并提出了新的剪枝技术。在22个数据集上的实验表明，FOIL2+QCBA相对其他基线方法而言，具有更高的预测性能和更小的模型大小。 |

# 详细

[^1]: 用工作流程与行动计划在任务导向对话中提高泛化性能

    Improving Generalization in Task-oriented Dialogues with Workflows and Action Plans. (arXiv:2306.01729v1 [cs.CL])

    [http://arxiv.org/abs/2306.01729](http://arxiv.org/abs/2306.01729)

    本论文提出了在任务导向对话中提高泛化性能的方法，通过增强对话上下文，使用有效的工作流名称和行动计划编码来完成多步骤任务的执行，解决了大型预训练语言模型无法可靠执行训练期间未见过的新多步骤任务的问题。

    

    任务导向对话的难点在于涉及理解用户意图、从用户处收集信息、执行API调用以及生成有用流畅的回应。然而，对于复杂的任务，我们必须按特定顺序在多个步骤中正确地完成所有这些事情。虽然大型预训练语言模型可以进行端到端的微调来创建生成流畅文本的多步骤任务导向对话代理，但我们的实验证实，仅通过这种方法无法可靠地执行训练期间未见过的新多步骤任务。为了解决这些限制，我们增强了给出到text2text转换器的对话上下文，其中包括已知的有效工作流名称和行动计划。行动计划包括完成任务所需的行动序列，并编码为简单的关键字序列（例如，验证身份，拉起账户，重置密码等）。我们在基于行动的对话数据集上进行了广泛的实验。

    Task-oriented dialogue is difficult in part because it involves understanding user intent, collecting information from the user, executing API calls, and generating helpful and fluent responses. However, for complex tasks one must also correctly do all of these things over multiple steps, and in a specific order. While large pre-trained language models can be fine-tuned end-to-end to create multi-step task-oriented dialogue agents that generate fluent text, our experiments confirm that this approach alone cannot reliably perform new multi-step tasks that are unseen during training. To address these limitations, we augment the dialogue contexts given to \textmd{text2text} transformers with known \textit{valid workflow names} and \textit{action plans}. Action plans consist of sequences of actions required to accomplish a task, and are encoded as simple sequences of keywords (e.g. verify-identity, pull-up-account, reset-password, etc.). We perform extensive experiments on the Action-Based
    
[^2]: 利用mask先验模型去噪扩散语义分割

    Denoising Diffusion Semantic Segmentation with Mask Prior Modeling. (arXiv:2306.01721v1 [cs.CV])

    [http://arxiv.org/abs/2306.01721](http://arxiv.org/abs/2306.01721)

    本文提出了一种利用去噪扩散生成模型建模的mask先验，改进现有的语义分割方法。该方法达到了在多个基准数据集上的最先进效果，超参数调整较少。

    

    语义分割一直以来都是学习更具有区分性的图像表示用于对每个像素进行分类。尽管取得了显著的进展，但分割掩模本身的先验，例如几何约束和语义约束仍未被深入探索。在本文中，我们提出了一种利用最近开发的去噪扩散生成模型建模的mask先验，改善现有区分性方法的语义分割质量。我们从为mask先验建模调整扩散模型的统一架构开始，将本文重点放在具有离散扩散的具体实例上，并确定其成功应用的各种关键设计选择。我们的初步分析揭示了几个重要发现，包括：（1）简单地将扩散模型集成到语义分割中不足以达到最佳效果，设计不良的扩散过程可能导致分割性能下降;（2）在扩散过程中，在保留遮罩结构和合并输入图像中的其他信息之间取得平衡至关重要;（3）所提出的方法可以在几个基准数据集上实现最先进的性能，而调整的超参数较少。

    The evolution of semantic segmentation has long been dominated by learning more discriminative image representations for classifying each pixel. Despite the prominent advancements, the priors of segmentation masks themselves, e.g., geometric and semantic constraints, are still under-explored. In this paper, we propose to ameliorate the semantic segmentation quality of existing discriminative approaches with a mask prior modeled by a recently-developed denoising diffusion generative model. Beginning with a unified architecture that adapts diffusion models for mask prior modeling, we focus this work on a specific instantiation with discrete diffusion and identify a variety of key design choices for its successful application. Our exploratory analysis revealed several important findings, including: (1) a simple integration of diffusion models into semantic segmentation is not sufficient, and a poorly-designed diffusion process might lead to degradation in segmentation performance; (2) dur
    
[^3]: 合并模型时如何解决干扰的问题

    Resolving Interference When Merging Models. (arXiv:2306.01708v1 [cs.LG])

    [http://arxiv.org/abs/2306.01708](http://arxiv.org/abs/2306.01708)

    本文揭示了现有模型合并技术存在的干扰问题，提出了具有广泛适用性的解决方案，可显着提高合并后模型的性能。

    

    迁移学习可以在下游任务中进一步微调预训练模型，从而获得显著的优势，包括改进下游性能，加快收敛速度和提高样本效率。然而，已有的模型合并技术往往忽视了不同模型参数之间的干扰，导致合并多个模型时性能大幅下降。本文证明，先前的合并技术由于两个主要干扰来源而不慎丢失有价值的信息：(a)冗余参数值引起的干扰和(b)表示同一参数值的符号在不同模型中的差异。

    Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To 
    
[^4]: 基于生成建模风格化技术的领域自适应回归任务中是否必要？

    Is Generative Modeling-based Stylization Necessary for Domain Adaptation in Regression Tasks?. (arXiv:2306.01706v1 [cs.CV])

    [http://arxiv.org/abs/2306.01706](http://arxiv.org/abs/2306.01706)

    本研究探讨了生成建模风格化技术在领域自适应回归任务中是否必要，发现与分类任务相比，其对于回归任务的影响较小。

    

    无监督的领域自适应旨在在没有目标域标签的情况下使用两种主要技术：输入级别对齐（如生成建模和风格化）和特征级别对齐（匹配特征映射的分布，例如梯度反转层）来弥合源域和目标域之间的差距。本文探讨了输入级别对齐对于领域自适应回归任务的作用。

    Unsupervised domain adaptation (UDA) aims to bridge the gap between source and target domains in the absence of target domain labels using two main techniques: input-level alignment (such as generative modeling and stylization) and feature-level alignment (which matches the distribution of the feature maps, e.g. gradient reversal layers). Motivated from the success of generative modeling for image classification, stylization-based methods were recently proposed for regression tasks, such as pose estimation. However, use of input-level alignment via generative modeling and stylization incur additional overhead and computational complexity which limit their use in real-world DA tasks. To investigate the role of input-level alignment for DA, we ask the following question: Is generative modeling-based stylization necessary for visual domain adaptation in regression? Surprisingly, we find that input-alignment has little effect on regression tasks as compared to classification. Based on thes
    
[^5]: 信息通路假说：Transformer是动态自组织

    The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles. (arXiv:2306.01705v1 [cs.LG])

    [http://arxiv.org/abs/2306.01705](http://arxiv.org/abs/2306.01705)

    Transformer使用稠密的自注意力机制，使得在深度网络中可能的连接模式数量呈指数级增长。我们提出了信息通路假说，利用这种特性将连接模式分解为相互独立的信息通路，减少训练过程中的内存和计算成本以及提高模型的泛化能力。

    

    Transformer模型使用了稠密的自注意力机制，赋予了它在远距离连接方面更高的灵活性。在深度Transformer的多个层中，可能的连接模式数量呈指数级增长。然而，其中很少有连接模式对模型性能有所贡献，其中有用的更少。我们提出假设，在一个Transformer中存在着名为信息通路的稀疏子网络，可以独立地进行训练。然而，这些通路的动态（即依赖于输入）特性使得在训练过程中很难剪枝稠密的自注意力。但是，这些通路的整体分布往往是可以预测的。我们利用这一事实提出了随机子采样自注意力（SSA）——一种用于Transformer的通用训练策略，可以在训练期间将自注意力的内存和计算成本减少4到8倍，同时还可以作为正则化方法，提高模型的泛化能力。

    Transformers use the dense self-attention mechanism which gives a lot of flexibility for long-range connectivity. Over multiple layers of a deep transformer, the number of possible connectivity patterns increases exponentially. However, very few of these contribute to the performance of the network, and even fewer are essential. We hypothesize that there are sparsely connected sub-networks within a transformer, called information pathways which can be trained independently. However, the dynamic (i.e., input-dependent) nature of these pathways makes it difficult to prune dense self-attention during training. But the overall distribution of these pathways is often predictable. We take advantage of this fact to propose Stochastically Subsampled self-Attention (SSA) - a general-purpose training strategy for transformers that can reduce both the memory and computational cost of self-attention by 4 to 8 times during training while also serving as a regularization method - improving generaliz
    
[^6]: 基于成对分布差异的数据去偏见亲和性聚类框架

    Affinity Clustering Framework for Data Debiasing Using Pairwise Distribution Discrepancy. (arXiv:2306.01699v1 [cs.LG])

    [http://arxiv.org/abs/2306.01699](http://arxiv.org/abs/2306.01699)

    该论文提出了一种基于亲和性聚类的数据扩充方法MASC，通过同类数据集中的亲和聚类和保护数据的共享达到数据集的平衡，从而解决数据集代表性偏见问题。

    

    数据采集方法不足或不具代表性常导致身份组不平衡，形成数据集代表性偏见。这种偏见可能存在于一个或多个受保护属性的不同组之间，并可能导致对某些人群的偏见和歧视性结果。该论文提出了一种数据扩充方法MASC，利用亲和性聚类平衡目标数据集的非保护组和保护组表征。通过将同一保护属性的实例从相似数据集中进行聚类，共享来自受保护属性的实例。该方法包括通过量化数据集间的分布差异构建亲和矩阵，并将其转换为对称成对相似性矩阵。使用非参数的谱聚类算法对目标数据集进行分类。

    Group imbalance, resulting from inadequate or unrepresentative data collection methods, is a primary cause of representation bias in datasets. Representation bias can exist with respect to different groups of one or more protected attributes and might lead to prejudicial and discriminatory outcomes toward certain groups of individuals; in cases where a learning model is trained on such biased data. This paper presents MASC, a data augmentation approach that leverages affinity clustering to balance the representation of non-protected and protected groups of a target dataset by utilizing instances of the same protected attributes from similar datasets that are categorized in the same cluster as the target dataset by sharing instances of the protected attribute. The proposed method involves constructing an affinity matrix by quantifying distribution discrepancies between dataset pairs and transforming them into a symmetric pairwise similarity matrix. A non-parametric spectral clustering i
    
[^7]: GateON: 一种用于大规模连续学习的无监督方法

    GateON: an unsupervised method for large scale continual learning. (arXiv:2306.01690v1 [cs.LG])

    [http://arxiv.org/abs/2306.01690](http://arxiv.org/abs/2306.01690)

    GateON是一种用于大规模连续学习的无监督方法，通过可学习的活动门控和参数相关性的在线估计来防止重要知识被覆盖，同时通过定点神经元的重新激活机制解决了网络饱和的问题。

    

    连续学习（CL）的目标是在不对早期任务进行重新训练的情况下按顺序学习任务。然而，传统的神经网络在经过CL训练后会出现灾难性遗忘和有限的泛化能力。为了克服这些问题，我们引入了一种新的方法，称为'Gate and Obstruct Network'（GateON）。GateON将可学习的活动门控与参数相关性的在线估计相结合，以防止重要知识被覆盖。我们的方法在任务之间生成部分重叠的路径，允许在顺序学习过程中进行正向和反向转移。GateON通过定点神经元的重新激活机制来解决参数固定后网络饱和的问题，实现了大规模连续学习。GateON适用于各种网络（全连接、CNN、Transformers），计算复杂度低，有效地学习了高达100个MNIST学习任务，并在预训练BERT中取得了顶尖结果。

    The objective of continual learning (CL) is to learn tasks sequentially without retraining on earlier tasks. However, when subjected to CL, traditional neural networks exhibit catastrophic forgetting and limited generalization. To overcome these problems, we introduce a novel method called 'Gate and Obstruct Network' (GateON). GateON combines learnable gating of activity and online estimation of parameter relevance to safeguard crucial knowledge from being overwritten. Our method generates partially overlapping pathways between tasks which permits forward and backward transfer during sequential learning. GateON addresses the issue of network saturation after parameter fixation by a re-activation mechanism of fixed neurons, enabling large-scale continual learning. GateON is implemented on a wide range of networks (fully-connected, CNN, Transformers), has low computational complexity, effectively learns up to 100 MNIST learning tasks, and achieves top-tier results for pre-trained BERT in
    
[^8]: XAI文艺复兴：重新定义医学诊断模型的可解释性。

    XAI Renaissance: Redefining Interpretability in Medical Diagnostic Models. (arXiv:2306.01668v1 [cs.LG])

    [http://arxiv.org/abs/2306.01668](http://arxiv.org/abs/2306.01668)

    本文探讨了可解释人工智能（XAI）在革新医学诊断模型的创新方法和方法论，并通过揭示潜在的决策过程来赋予医疗保健专业人员理解和信任这些模型的能力。这些新技术潜力巨大，有望改变医疗保健的格局，从而提高患者的结果并建立对AI驱动诊断系统的信任。

    

    随着机器学习模型越来越多地应用于医学诊断，可解释性和透明度的需求变得尤为重要。XAI文艺复兴标志着该领域发生了重大转变，旨在重新定义医学诊断模型的可解释性。本文探讨了在可解释人工智能（XAI）领域内正在革新医学诊断模型的创新方法和方法论。通过揭示潜在的决策过程，XAI技术赋予医疗保健专业人员理解、信任并有效利用这些模型进行准确可靠的医学诊断。本文重点介绍了XAI在医学诊断方面的关键进展及其改变医疗保健格局的潜力，从而改善患者结果并建立对AI驱动诊断系统的信任。

    As machine learning models become increasingly prevalent in medical diagnostics, the need for interpretability and transparency becomes paramount. The XAI Renaissance signifies a significant shift in the field, aiming to redefine the interpretability of medical diagnostic models. This paper explores the innovative approaches and methodologies within the realm of Explainable AI (XAI) that are revolutionizing the interpretability of medical diagnostic models. By shedding light on the underlying decision-making process, XAI techniques empower healthcare professionals to understand, trust, and effectively utilize these models for accurate and reliable medical diagnoses. This review highlights the key advancements in XAI for medical diagnostics and their potential to transform the healthcare landscape, ultimately improving patient outcomes and fostering trust in AI-driven diagnostic systems.
    
[^9]: SourceP：使用预训练模型和数据流智能检测以太坊上的智能庞兹骗局

    SourceP: Smart Ponzi Schemes Detection on Ethereum Using Pre-training Model with Data Flow. (arXiv:2306.01665v1 [cs.SE])

    [http://arxiv.org/abs/2306.01665](http://arxiv.org/abs/2306.01665)

    本文提出了一种使用预训练模型和数据流的方法，通过智能合约的源代码特征，实现检测以太坊上的智能庞兹骗局。该方法提高了模型的可解释性和降低了数据获取和特征提取的难度。

    

    随着区块链技术越来越流行，典型的金融骗局庞兹骗局也在区块链平台以太坊上出现。通过智能合约部署的这种庞兹骗局，也称为智能庞兹骗局，已经造成了大量的经济损失和负面影响。现有的以太坊智能庞兹骗局检测方法主要依赖于智能合约的字节码特征、操作码特征、账户特征和交易行为特征，这些方法缺乏可解释性和可持续性。本文提出了SourceP，一种使用预训练模型和数据流在以太坊平台上检测智能庞兹骗局的方法。该方法只需要利用智能合约的源代码作为特征，从另一个角度探索检测智能庞兹骗局的可能性。SourceP降低了现有检测方法的数据获取和特征提取难度，同时增加了模型的可解释性。

    As blockchain technology becomes more and more popular, a typical financial scam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum. This Ponzi scheme deployed through smart contracts, also known as the smart Ponzi scheme, has caused a lot of economic losses and negative impacts. Existing methods for detecting smart Ponzi schemes on Ethereum mainly rely on bytecode features, opcode features, account features, and transaction behavior features of smart contracts, and such methods lack interpretability and sustainability. In this paper, we propose SourceP, a method to detect smart Ponzi schemes on the Ethereum platform using pre-training models and data flow, which only requires using the source code of smart contracts as features to explore the possibility of detecting smart Ponzi schemes from another direction. SourceP reduces the difficulty of data acquisition and feature extraction of existing detection methods while increasing the interpretability of the model. 
    
[^10]: DiffusEmp:一种基于扩散模型的多级控制共情回应生成框架

    DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation. (arXiv:2306.01657v1 [cs.CL])

    [http://arxiv.org/abs/2306.01657](http://arxiv.org/abs/2306.01657)

    本文提出了一种名为DiffusEmp的框架，该框架基于条件扩散语言模型，并使用多级控制信号来生成更加细致和个性化的共情回应。

    

    共情是开放式交流中至关重要的因素，它自然地展示了一个人对他人的关心和理解。虽然已经提出了几种生成共情响应的方法，但现有的作品往往导致单调的共情，即指通用和安全的表达方式。本文提出使用显式控制来引导共情表达，并设计了一个DiffusEmp框架，基于条件扩散语言模型，统一了对话上下文和属性导向控制信号的利用。具体而言，引入通信机制、意图和语义框架作为多层次信号，可从粗糙到细致地控制共情实现。然后我们设计了一个特定的屏蔽策略，以反映多层次信号与响应令牌之间的关系，并将其整合到扩散模型中以影响生成过程。实验结果在基准数据集EmpatheticDialogue上表明，我们的框架优于其他共情回应生成方法。

    Empathy is a crucial factor in open-domain conversations, which naturally shows one's caring and understanding to others. Though several methods have been proposed to generate empathetic responses, existing works often lead to monotonous empathy that refers to generic and safe expressions. In this paper, we propose to use explicit control to guide the empathy expression and design a framework DiffusEmp based on conditional diffusion language model to unify the utilization of dialogue context and attribute-oriented control signals. Specifically, communication mechanism, intent, and semantic frame are imported as multi-grained signals that control the empathy realization from coarse to fine levels. We then design a specific masking strategy to reflect the relationship between multi-grained signals and response tokens, and integrate it into the diffusion model to influence the generative process. Experimental results on a benchmark dataset EmpatheticDialogue show that our framework outper
    
[^11]: Gode -- 将生物化学知识图谱集成到分子图神经网络的预训练中

    Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])

    [http://arxiv.org/abs/2306.01631](http://arxiv.org/abs/2306.01631)

    本研究提出了一种新的方法，在分子结构和生物医学知识图谱中集成多个领域信息，通过自我监督策略预先训练更广泛和更强大的表示，并在化学属性预测任务上展示出出色的性能。

    

    分子属性的准确预测对于促进创新治疗方法的发展和理解化学物质和生物系统之间复杂的相互作用至关重要。本研究提出了一种新的方法，将单个分子结构的图表示与生物医学知识图谱 (KG) 的多个领域信息进行集成。通过集成两个级别的信息，我们可以使用自我监督策略预先训练更广泛和更强大的表示，用于分子级和 KG 级预测任务。在性能评估方面，我们在 11 个具有挑战性的化学属性预测任务上微调我们预先训练的模型。我们的框架的结果表明，我们微调的模型优于现有的最先进的模型。

    The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Results from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models.
    
[^12]: HomE: 同变性单应性视频表示学习

    HomE: Homography-Equivariant Video Representation Learning. (arXiv:2306.01623v1 [cs.CV])

    [http://arxiv.org/abs/2306.01623](http://arxiv.org/abs/2306.01623)

    本文提出了一种新的多视角视频表示学习方法，通过学习隐式映射，形成了一个具有同变性的表示空间，能够对行动识别和行人意图预测任务获得最先进的结果。

    

    近年来，自监督表示学习的快速发展使得模型在不需要大量标注数据的情况下得到了更高效和更强健的性能。然而，大部分工作仍然集中在图像上，很少有工作关注视频，甚至更少的工作关注多视角视频，其中可以利用更强大的归纳偏差进行自监督。在本文中，我们提出了一种新的多视角视频表示学习方法，在其中我们显式地模拟表示空间以保持同变性。我们的方法学习不同视角之间的隐式映射，最终形成一个表示空间，其中保持相邻视角之间的单应关系。我们通过行动识别和行人意图预测作为下游任务来评估我们的HomE表示。在UCF101数据集上，我们的方法在3次交叉验证的准确度达到了96.4％，优于大多数最先进的自监督学习方法。同样，在行人意图预测的STIP数据集上，我们的方法也达到了最先进的结果。

    Recent advances in self-supervised representation learning have enabled more efficient and robust model performance without relying on extensive labeled data. However, most works are still focused on images, with few working on videos and even fewer on multi-view videos, where more powerful inductive biases can be leveraged for self-supervision. In this work, we propose a novel method for representation learning of multi-view videos, where we explicitly model the representation space to maintain Homography Equivariance (HomE). Our method learns an implicit mapping between different views, culminating in a representation space that maintains the homography relationship between neighboring views. We evaluate our HomE representation via action recognition and pedestrian intent prediction as downstream tasks. On action classification, our method obtains 96.4% 3-fold accuracy on the UCF101 dataset, better than most state-of-the-art self-supervised learning methods. Similarly, on the STIP da
    
[^13]: 利用ChatGPT进行日志解析的评估

    An Evaluation of Log Parsing with ChatGPT. (arXiv:2306.01590v1 [cs.SE])

    [http://arxiv.org/abs/2306.01590](http://arxiv.org/abs/2306.01590)

    本文评估了ChatGPT在日志解析方面的能力，结果表明它可以通过适当的提示实现有前途的日志解析结果，特别是在few-shot提示下。

    

    软件日志在确保大规模软件系统的可靠性和可维护性方面扮演着至关重要的角色，因为它们通常是运行时信息的唯一来源。日志解析将原始日志消息转换为结构化数据，是向下游日志分析的重要初始步骤。在最近的研究中，当前最先进的大型语言模型（LLM）ChatGPT已广泛应用于各种软件工程任务。然而，它在自动化日志解析方面的性能仍不清楚。在本文中，我们通过解决两个研究问题评估了ChatGPT进行日志解析的能力。 （1）ChatGPT能否有效地解析日志？（2）ChatGPT在不同提示方法下的表现如何？我们的结果表明，ChatGPT可以通过适当的提示实现有前途的日志解析结果，特别是在few-shot提示下。基于我们的发现，我们概述了几个基于ChatGPT的日志解析的挑战和机遇。

    Software logs play an essential role in ensuring the reliability and maintainability of large-scale software systems, as they are often the sole source of runtime information. Log parsing, which converts raw log messages into structured data, is an important initial step towards downstream log analytics. In recent studies, ChatGPT, the current cutting-edge large language model (LLM), has been widely applied to a wide range of software engineering tasks. However, its performance in automated log parsing remains unclear. In this paper, we evaluate ChatGPT's ability to undertake log parsing by addressing two research questions. (1) Can ChatGPT effectively parse logs? (2) How does ChatGPT perform with different prompting methods? Our results show that ChatGPT can achieve promising results for log parsing with appropriate prompts, especially with few-shot prompting. Based on our findings, we outline several challenges and opportunities for ChatGPT-based log parsing.
    
[^14]: 概率概念瓶颈模型

    Probabilistic Concept Bottleneck Models. (arXiv:2306.01574v1 [cs.LG])

    [http://arxiv.org/abs/2306.01574](http://arxiv.org/abs/2306.01574)

    本文提出了概率概念瓶颈模型（ProbCBM），通过建立概率概念嵌入来解决数据中概念存在模糊性的问题，提高了CBM模型的可靠性及解释的可信度。

    

    可解释的模型旨在以可读的方式做出决策。其中，概念瓶颈模型（CBM）根据预测的概念进行概念预测和类预测两步骤。CBM使用从概念预测中得出的高级概念提供解释；因此，可靠的概念预测对于可信度非常重要。在本研究中，我们解决了可能损害可靠性的模糊性问题。虽然数据中概念的存在往往是模糊的，但CBM在不考虑此模糊性的情况下使用确定性方法预测概念。为了针对这种模糊性提供可靠的解释，我们提出了概率概念瓶颈模型（ProbCBM）。通过利用概率概念嵌入，ProbCBM对概念预测中的不确定性进行建模，并基于概念及其相应的不确定性提供解释。这种不确定性提高了解释的可靠性。此外，由于类别预测的概率性质，ProbCBM还提供了类别预测的不确定度估计。

    Interpretable models are designed to make decisions in a human-interpretable manner. Representatively, Concept Bottleneck Models (CBM) follow a two-step process of concept prediction and class prediction based on the predicted concepts. CBM provides explanations with high-level concepts derived from concept predictions; thus, reliable concept predictions are important for trustworthiness. In this study, we address the ambiguity issue that can harm reliability. While the existence of a concept can often be ambiguous in the data, CBM predicts concepts deterministically without considering this ambiguity. To provide a reliable interpretation against this ambiguity, we propose Probabilistic Concept Bottleneck Models (ProbCBM). By leveraging probabilistic concept embeddings, ProbCBM models uncertainty in concept prediction and provides explanations based on the concept and its corresponding uncertainty. This uncertainty enhances the reliability of the explanations. Furthermore, as class unc
    
[^15]: PassGPT: 大语言模型中的密码建模和（引导式）生成

    PassGPT: Password Modeling and (Guided) Generation with Large Language Models. (arXiv:2306.01545v1 [cs.CL])

    [http://arxiv.org/abs/2306.01545](http://arxiv.org/abs/2306.01545)

    本研究使用大型语言模型PassGPT进行密码建模和生成，该模型比基于GAN的现有方法更准确，能生成符合任意限制的密码，为提高密码强度估计器提供了潜在的帮助。

    

    大语言模型（LLMs）可以成功地模拟自然语言，无需明确的监督，仅通过大量的文本进行训练。本文研究了LLMs在建模密码方面的有效性。我们介绍了PassGPT，它是一个在密码泄露数据上进行训练的LLM，用于生成密码。PassGPT通过猜测两倍于基于生成性对抗网络（GAN）的现有方法中的以前未见过的密码而胜过其它方法。此外，我们介绍了引导式密码生成的概念，利用PassGPT的抽样过程生成符合任意限制的密码，这在当前基于GAN的策略中是缺乏的。最后，我们对PassGPT对密码定义的熵和概率分布进行了深入分析，并讨论了其在增强现有密码强度估计器中的应用。

    Large language models (LLMs) successfully model natural language from vast amounts of text without the need for explicit supervision. In this paper, we investigate the efficacy of LLMs in modeling passwords. We present PassGPT, a LLM trained on password leaks for password generation. PassGPT outperforms existing methods based on generative adversarial networks (GAN) by guessing twice as many previously unseen passwords. Furthermore, we introduce the concept of guided password generation, where we leverage PassGPT sampling procedure to generate passwords matching arbitrary constraints, a feat lacking in current GAN-based strategies. Lastly, we conduct an in-depth analysis of the entropy and probability distribution that PassGPT defines over passwords and discuss their use in enhancing existing password strength estimators.
    
[^16]: 对象检测的组通道剪枝和空间关注提取

    Group channel pruning and spatial attention distilling for object detection. (arXiv:2306.01526v1 [cs.CV])

    [http://arxiv.org/abs/2306.01526](http://arxiv.org/abs/2306.01526)

    本研究提出了一种针对对象检测的三阶段模型压缩方法，通过动态稀疏训练、组通道剪枝和空间关注提取大幅缩小了模型大小、参数数量和计算复杂度，并在多个基准数据集上实现了有竞争力的对象检测精度。

    

    由于神经网络的超参数化，许多基于剪枝和量化的模型压缩方法已经出现。这些方法在减小模型的大小、参数数量和计算复杂度方面表现出色。然而，大多数被这些方法压缩的模型需要特殊的硬件和软件支持，这增加了部署成本。此外，这些方法主要用于分类任务，很少直接用于检测任务。为了解决这些问题，我们提出了一个三阶段的模型压缩方法：动态稀疏训练、组通道剪枝和空间关注提取。我们的方法在减小模型大小、参数数量和计算复杂度的同时，实现了在几个基准数据集上具有竞争力的对象检测精度。

    Due to the over-parameterization of neural networks, many model compression methods based on pruning and quantization have emerged. They are remarkable in reducing the size, parameter number, and computational complexity of the model. However, most of the models compressed by such methods need the support of special hardware and software, which increases the deployment cost. Moreover, these methods are mainly used in classification tasks, and rarely directly used in detection tasks. To address these issues, for the object detection network we introduce a three-stage model compression method: dynamic sparse training, group channel pruning, and spatial attention distilling. Firstly, to select out the unimportant channels in the network and maintain a good balance between sparsity and accuracy, we put forward a dynamic sparse training method, which introduces a variable sparse rate, and the sparse rate will change with the training process of the network. Secondly, to reduce the effect of
    
[^17]: 一体化动态多租户边缘云平台的统一工作负载预测

    One for All: Unified Workload Prediction for Dynamic Multi-tenant Edge Cloud Platforms. (arXiv:2306.01507v1 [cs.NI])

    [http://arxiv.org/abs/2306.01507](http://arxiv.org/abs/2306.01507)

    提出了一种全局汇集和静态内容意识的端到端框架DynEformer，其中设计了全局汇集和信息合并机制，可为动态多租户边缘云平台提供统一的工作负载预测方案。

    

    多租户边缘云平台中的工作负载预测对于高效的应用部署和资源配置至关重要。然而，MT-ECP中异构的应用程序模式、可变的基础架构性能以及频繁的部署给准确和高效的工作负载预测带来了重大挑战。已有的基于聚类的动态MT-ECP建模方法常常会由于需要维护大量数据集群和模型而产生过高的成本。现有的端到端时间序列预测方法在动态MT-ECP中无法提供一致的预测性能。本文提出了一种带有全局汇集和静态内容意识的端到端框架DynEformer，为动态MT-ECP提供统一的工作负载预测方案。精心设计的全局汇集和信息合并机制可以有效地识别和利用全局应用程序模式来驱动本地工作负载预测。

    Workload prediction in multi-tenant edge cloud platforms (MT-ECP) is vital for efficient application deployment and resource provisioning. However, the heterogeneous application patterns, variable infrastructure performance, and frequent deployments in MT-ECP pose significant challenges for accurate and efficient workload prediction. Clustering-based methods for dynamic MT-ECP modeling often incur excessive costs due to the need to maintain numerous data clusters and models, which leads to excessive costs. Existing end-to-end time series prediction methods are challenging to provide consistent prediction performance in dynamic MT-ECP. In this paper, we propose an end-to-end framework with global pooling and static content awareness, DynEformer, to provide a unified workload prediction scheme for dynamic MT-ECP. Meticulously designed global pooling and information merging mechanisms can effectively identify and utilize global application patterns to drive local workload predictions. The
    
[^18]: 在对话情感识别中使用监督式对抗性对比学习

    Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations. (arXiv:2306.01505v1 [cs.CL])

    [http://arxiv.org/abs/2306.01505](http://arxiv.org/abs/2306.01505)

    本文提出了一种监督式对抗性对比学习（SACL）框架，用于学习类别分布结构表示，通过联合类别分布对比学习目标，有效利用标签级特性一致性并保留细粒度的类内特性，实现了在对话情感识别中最先进的结果。

    

    情感识别在对话中是提取泛化和稳健表示的一个重要挑战。为了解决这个问题，本文提出了一种监督式对抗性对比学习（SACL）框架，用于学习类别分布结构表示。该框架应用于对比感知对抗性训练以生成最坏情况的样本，并在原始和对抗样本上使用联合类别分布对比学习目标。它可以有效地利用标签级特性一致性并保留细粒度的类内特性。为了避免对上下文相关数据产生负面影响，我们设计了一个上下文对抗性训练策略，从上下文中学习更多不同的特征，并增强模型对上下文的容错性。在该框架下，我们开发了一个基于序列的方法SACL-LSTM，用于学习针对ERC的标签一致和上下文稳健的情感特征。在三个数据集上的实验证明，SACL-LSTM在对话情感识别方面实现了最先进的结果，优于现有的方法。

    Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations. The framework applies contrast-aware adversarial training to generate worst-case samples and uses a joint class-spread contrastive learning objective on both original and adversarial samples. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training strategy to learn more diverse features from context and enhance the model's context robustness. We develop a sequence-based method SACL-LSTM under this framework, to learn label-consistent and context-robust emotional features for ERC. Experiments on three datasets demonstrate that SACL-LSTM achieves state-of
    
[^19]: 人类感知的人工智能在科学加速方面的应用

    Accelerating science with human-aware artificial intelligence. (arXiv:2306.01495v1 [cs.AI])

    [http://arxiv.org/abs/2306.01495](http://arxiv.org/abs/2306.01495)

    本文介绍如何通过纳入人类专业知识分布、训练无监督模型进行模拟推理，显著改善AI对未来发现预测准确性，并产生科学上有前途、想象力带来的自然假设，从而加速人类发现或探索其盲点。

    

    在发明有价值的材料和靶向治疗方面，使用已发表的科学成果训练的人工智能（AI）模型已经得到了应用，但它们通常忽略不断改变发现领域的人类科学家。在这里，我们展示了通过训练无监督模型进行模拟推理（专家认为易于理解）并纳入人类专业知识分布，可以显著改善（高达400％）AI对未来发现预测的准确性，特别是在相关文献稀缺的情况下。这些模型通过预测人类的预测和将会提出这些预测的科学家来成功。通过调整人类感知的AI以避开常规思路，我们可以产生科学上有前途、想象力带来的自然假设，并且拥有超越当前问题的科学进展潜力，这些问题直到遥远未来才可能得到解决。人类感知的人工智能可以增强科学创造力，并扩大研究问题的范围，从而加速人类发现或探索其盲点。

    Artificial intelligence (AI) models trained on published scientific findings have been used to invent valuable materials and targeted therapies, but they typically ignore the human scientists who continually alter the landscape of discovery. Here we show that incorporating the distribution of human expertise by training unsupervised models on simulated inferences cognitively accessible to experts dramatically improves (up to 400%) AI prediction of future discoveries beyond those focused on research content alone, especially when relevant literature is sparse. These models succeed by predicting human predictions and the scientists who will make them. By tuning human-aware AI to avoid the crowd, we can generate scientifically promising "alien" hypotheses unlikely to be imagined or pursued without intervention until the distant future, which hold promise to punctuate scientific advance beyond questions currently pursued. Accelerating human discovery or probing its blind spots, human-aware
    
[^20]: 通过近似的正交约束实现稳健的低秩训练。

    Robust low-rank training via approximate orthonormal constraints. (arXiv:2306.01485v1 [cs.LG])

    [http://arxiv.org/abs/2306.01485](http://arxiv.org/abs/2306.01485)

    通过追加正交约束，从而在保持低秩矩阵分解前提下提高深度神经网络的鲁棒性与准确率。

    

    随着模型和数据规模的增长，设计剪枝技术以降低深度学习流程的资源需求并保持模型性能已成为广泛努力的目标。为了降低推理和训练成本，主要的工作方向之一使用低秩矩阵分解来表示网络权重。尽管能够保持准确性，但我们观察到低秩方法往往会损害模型对抗扰动的鲁棒性。通过将稳健性建模为神经网络的条件数，我们认为这种稳健性损失是由于低秩权重矩阵的奇异值爆炸引起的。因此，我们引入了一种稳健的低秩训练算法，该算法在保持网络权重位于低秩矩阵流形上的同时，同时强制施加近似的正交约束。因此，该模型降低了训练和推理成本，同时确保了良好的条件性和更好的抗干扰能力。

    With the growth of model and data sizes, a broad effort has been made to design pruning techniques that reduce the resource demand of deep learning pipelines, while retaining model performance. In order to reduce both inference and training costs, a prominent line of work uses low-rank matrix factorizations to represent the network weights. Although able to retain accuracy, we observe that low-rank methods tend to compromise model robustness against adversarial perturbations. By modeling robustness in terms of the condition number of the neural network, we argue that this loss of robustness is due to the exploding singular values of the low-rank weight matrices. Thus, we introduce a robust low-rank training algorithm that maintains the network's weights on the low-rank matrix manifold while simultaneously enforcing approximate orthonormal constraints. The resulting model reduces both training and inference costs while ensuring well-conditioning and thus better adversarial robustness, w
    
[^21]: 存在抑制变量时 XAI 方法的理论行为研究

    Theoretical Behavior of XAI Methods in the Presence of Suppressor Variables. (arXiv:2306.01464v1 [cs.LG])

    [http://arxiv.org/abs/2306.01464](http://arxiv.org/abs/2306.01464)

    本论文综合研究表明，XAI 方法在存在抑制变量时解释可能出现误导性，需要进行更加理论化和经验化的研究，确保其应用的正确性和可能性。

    

    近年来，“可解释人工智能”（XAI）社区已经创建了一个大量的方法来弥合模型“复杂度”和“可解释性”之间的差距。然而，XAI 方法需要解决的具体问题尚未得到正式说明。因此，XAI 方法缺乏理论和实证证据，以验证其解释的“正确性”，限制了其用于质量控制和透明度目的的潜力。同时，Haufe等人（2014）使用简单的玩具例子展示了即使是线性模型的标准解释也可能极具误导性。具体而言，可能会被归因于所谓的抑制变量，这些变量与预测目标缺乏任何统计关系。Wilming等人（2022）已经经验证了这种行为在大量 XAI 方法中的实证研究。在这里，我们进一步推导了多种流行的 XAI 方法在简单的 toy dataset 上的行为的解析表达式。

    In recent years, the community of 'explainable artificial intelligence' (XAI) has created a vast body of methods to bridge a perceived gap between model 'complexity' and 'interpretability'. However, a concrete problem to be solved by XAI methods has not yet been formally stated. As a result, XAI methods are lacking theoretical and empirical evidence for the 'correctness' of their explanations, limiting their potential use for quality-control and transparency purposes. At the same time, Haufe et al. (2014) showed, using simple toy examples, that even standard interpretations of linear models can be highly misleading. Specifically, high importance may be attributed to so-called suppressor variables lacking any statistical relation to the prediction target. This behavior has been confirmed empirically for a large array of XAI methods in Wilming et al. (2022). Here, we go one step further by deriving analytical expressions for the behavior of a variety of popular XAI methods on a simple tw
    
[^22]: 深度 Q-Learning 和近端策略优化算法在材料分类任务中的表现比较

    Deep Q-Learning versus Proximal Policy Optimization: Performance Comparison in a Material Sorting Task. (arXiv:2306.01451v1 [cs.AI])

    [http://arxiv.org/abs/2306.01451](http://arxiv.org/abs/2306.01451)

    本文比较了深度 Q-Learning 和近端策略优化算法在材料排序任务中的表现，结果表明PPO在所有评估指标方面的表现都更优。策略优化算法在高维状态和动作空间问题中有优势。

    

    本文在一个模拟生产系统中比较了两种知名的深度强化学习算法：深度 Q-Learning（DQN）和近端策略优化算法（PPO）。我们利用了一个基于 Petri 网的模拟环境，在相关工作中此环境已被提出。根据多个评估指标，包括平均正确组装和分类产品的百分比、平均情节长度和成功情节的百分比，比较了这两种算法的性能。结果表明，PPO在所有评估指标方面都表现优于DQN。该研究突出了基于策略的算法在高维状态和动作空间问题中的优势。该研究通过提供有关不同算法的有效性及其适用于不同任务的见解，为深度 RL 在生产系统领域做出了贡献。

    This paper presents a comparison between two well-known deep Reinforcement Learning (RL) algorithms: Deep Q-Learning (DQN) and Proximal Policy Optimization (PPO) in a simulated production system. We utilize a Petri Net (PN)-based simulation environment, which was previously proposed in related work. The performance of the two algorithms is compared based on several evaluation metrics, including average percentage of correctly assembled and sorted products, average episode length, and percentage of successful episodes. The results show that PPO outperforms DQN in terms of all evaluation metrics. The study highlights the advantages of policy-based algorithms in problems with high-dimensional state and action spaces. The study contributes to the field of deep RL in context of production systems by providing insights into the effectiveness of different algorithms and their suitability for different tasks.
    
[^23]: 无监督的多词表达改写方法研究

    Unsupervised Paraphrasing of Multiword Expressions. (arXiv:2306.01443v1 [cs.CL])

    [http://arxiv.org/abs/2306.01443](http://arxiv.org/abs/2306.01443)

    本文提出了一种无监督的多词表达式改写方法，使用的数据和工具非常简单，且实验结果表明，该方法在熟语语义文本相似度任务中性能良好。

    

    我们提出了一种无监督的改写多词表达式（MWEs）的方法，只使用单语料库数据和预训练语言模型（无需微调），不使用任何外部资源，如字典。我们在SemEval 2022熟语语义文本相似度任务上评估了我们的方法，并表明它优于所有无监督系统并与有监督系统相匹敌。

    We propose an unsupervised approach to paraphrasing multiword expressions (MWEs) in context. Our model employs only monolingual corpus data and pre-trained language models (without fine-tuning), and does not make use of any external resources such as dictionaries. We evaluate our method on the SemEval 2022 idiomatic semantic text similarity task, and show that it outperforms all unsupervised systems and rivals supervised systems.
    
[^24]: 用于工业应用中强化学习集成的模块化测试平台

    A Modular Test Bed for Reinforcement Learning Incorporation into Industrial Applications. (arXiv:2306.01440v1 [cs.AI])

    [http://arxiv.org/abs/2306.01440](http://arxiv.org/abs/2306.01440)

    本论文探究了如何运用强化学习技术提高工业应用的效率，以传输和组装货物为例，实现缩短上市时间、大规模定制和批量级别的生产。

    

    本应用论文探讨了使用强化学习（RL）来应对工业4.0的需求，包括缩短上市时间、大规模定制和批量级别的生产的潜力。具体而言，我们提出了一个使用案例，任务是按照预定义的规则通过模型工厂传输和组装货物。每次模拟运行涉及将特定数量的随机颜色的货物放置在入口处。目标是将商品运输到装配站，在其中安装两个铆钉，将上部连接到下部。在铆钉安装后，蓝色产品必须被运输到出口，而绿色产品则必须被运输到存储区。本研究重点关注强化学习技术在解决问题和提高生产效率方面的应用。

    This application paper explores the potential of using reinforcement learning (RL) to address the demands of Industry 4.0, including shorter time-to-market, mass customization, and batch size one production. Specifically, we present a use case in which the task is to transport and assemble goods through a model factory following predefined rules. Each simulation run involves placing a specific number of goods of random color at the entry point. The objective is to transport the goods to the assembly station, where two rivets are installed in each product, connecting the upper part to the lower part. Following the installation of rivets, blue products must be transported to the exit, while green products are to be transported to storage. The study focuses on the application of reinforcement learning techniques to address this problem and improve the efficiency of the production process.
    
[^25]: 通过神经引导符号抽象实现可解释和可解释逻辑策略

    Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction. (arXiv:2306.01439v1 [cs.LG])

    [http://arxiv.org/abs/2306.01439](http://arxiv.org/abs/2306.01439)

    该论文介绍了一种名为NUDGE的策略，利用训练好的基于神经网络的代理来引导逻辑规则的搜索，实现了可解释和可解释的策略。

    

    神经网络所需要的有限先验使其成为使用强化学习（RL）编码和学习策略的主要选择。然而，它们也是黑匣子，在工作在图像级别时难以理解代理行为。因此，神经符号RL旨在首先创建可解释的策略。不幸的是，可解释性不意味着可解释性。为了实现解释性和可解释性，我们引入了神经引导可微分逻辑策略（NUDGE）。NUDGE利用训练好的基于神经网络的代理来引导候选加权逻辑规则的搜索，然后使用可微分的逻辑来训练逻辑代理。我们的实验评估表明，NUDGE代理可以产生可解释和可解释的策略，同时胜过纯神经代理，并展现出良好的灵活性，以适应不同初始状态和问题大小的环境。

    The limited priors required by neural networks make them the dominating choice to encode and learn policies using reinforcement learning (RL). However, they are also black-boxes, making it hard to understand the agent's behaviour, especially when working on the image level. Therefore, neuro-symbolic RL aims at creating policies that are interpretable in the first place. Unfortunately, interpretability is not explainability. To achieve both, we introduce Neurally gUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural network-based agents to guide the search of candidate-weighted logic rules, then uses differentiable logic to train the logic agents. Our experimental evaluation demonstrates that NUDGE agents can induce interpretable and explainable policies while outperforming purely neural ones and showing good flexibility to environments of different initial states and problem sizes.
    
[^26]: 关于联邦学习中的知识编辑：展望，挑战和未来方向的透视

    On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions. (arXiv:2306.01431v1 [cs.LG])

    [http://arxiv.org/abs/2306.01431](http://arxiv.org/abs/2306.01431)

    本文介绍了一份关于联邦学习中知识编辑的广泛调查，总结了最新技术，确定了挑战和机遇，并概述了未来的研究方向。

    

    随着联邦学习（FL）引起越来越多的关注，人们普遍认为在学习一系列任务时，在整个框架上简单地应用随机梯度下降（SGD）会导致所谓的“灾难性遗忘”。因此，许多FL研究集中于设计联邦增量学习方法，以减轻遗忘并增加知识。另一方面，遗忘并不总是有害的。选择性遗忘，也称为联邦遗忘，包括消除特定知识，可以解决隐私问题并为获取新知识创造额外的“空间”。然而，缺乏广泛调查涵盖最新进展并对此问题进行全面检查。在本文中，我们介绍了一份关于联邦学习中知识编辑（增强/删除）的广泛调查，旨在总结最新技术，确定挑战和机遇，并概述未来的研究方向。

    As Federated Learning (FL) has gained increasing attention, it has become widely acknowledged that straightforwardly applying stochastic gradient descent (SGD) on the overall framework when learning over a sequence of tasks results in the phenomenon known as ``catastrophic forgetting''. Consequently, much FL research has centered on devising federated increasing learning methods to alleviate forgetting while augmenting knowledge. On the other hand, forgetting is not always detrimental. The selective amnesia, also known as federated unlearning, which entails the elimination of specific knowledge, can address privacy concerns and create additional ``space'' for acquiring new knowledge. However, there is a scarcity of extensive surveys that encompass recent advancements and provide a thorough examination of this issue. In this manuscript, we present an extensive survey on the topic of knowledge editing (augmentation/removal) in Federated Learning, with the goal of summarizing the state-of
    
[^27]: 一种用于在工业环境中部署强化学习的架构。

    An Architecture for Deploying Reinforcement Learning in Industrial Environments. (arXiv:2306.01420v1 [cs.AI])

    [http://arxiv.org/abs/2306.01420](http://arxiv.org/abs/2306.01420)

    本文提出一种在工业环境中部署强化学习的架构，该架构基于OPC UA，结合数字孪生的设置扩展了标准RL状态，并定义了一种信息模型。我们通过创建概念验证并在控制系统中确定最优策略，验证了该架构的可行性。

    

    工业4.0受到了缩短上市时间、产品批量定制以及批量为一生产等需求的推动。强化学习（RL）是一种机器学习范例，已经在很多复杂任务中显示出具有优于人类的潜力，可以应对上述需求。在本文中，我们提出了一种基于OPC UA的操作技术感知RL架构，它扩展了标准RL设置，并将其与数字孪生的设置相结合。此外，我们还定义了一种OPC UA信息模型，允许采用广义的即插即用方法交换所使用的RL代理。最后，我们通过创建一个概念验证来展示和评估这个架构。通过解决一个玩具例子，我们展示了这个架构可以用于使用实际控制系统确定最优策略。

    Industry 4.0 is driven by demands like shorter time-to-market, mass customization of products, and batch size one production. Reinforcement Learning (RL), a machine learning paradigm shown to possess a great potential in improving and surpassing human level performance in numerous complex tasks, allows coping with the mentioned demands. In this paper, we present an OPC UA based Operational Technology (OT)-aware RL architecture, which extends the standard RL setting, combining it with the setting of digital twins. Moreover, we define an OPC UA information model allowing for a generalized plug-and-play like approach for exchanging the RL agent used. In conclusion, we demonstrate and evaluate the architecture, by creating a proof of concept. By means of solving a toy example, we show that this architecture can be used to determine the optimal policy using a real control system.
    
[^28]: 实体和数字值的知识图谱推理

    Knowledge Graph Reasoning over Entities and Numerical Values. (arXiv:2306.01399v1 [cs.AI])

    [http://arxiv.org/abs/2306.01399](http://arxiv.org/abs/2306.01399)

    本文提出了一种新方法，将数值推理纳入知识图谱推理，将数值视为一等公民并提供了一个统一的框架，用于对实体和数值进行推理，实现了对复杂查询的有效和高效回答。

    

    知识图谱中的复杂逻辑查询指的是以逻辑形式表达的查询，传达复杂的含义，比如加拿大的图灵奖得主毕业于哪里？基于知识图谱推理的应用程序，例如对话系统和交互式搜索引擎，依赖于回答复杂逻辑查询作为基本任务的能力。在大多数知识图谱中，边缘通常用于描述实体之间的关系或它们的关联属性值。属性值可以是分类或数值格式，例如日期、年份、尺寸等。然而，现有的复杂查询答案（CQA）方法仅将数值与实体视为相同。这可能会导致回答某些查询的困难，例如哪个澳大利亚的普利策奖得主出生于1927年之前，哪种药物是一种止痛剂，并且副作用比布洛芬更少。本文受到数值编码最近的进展的启发，提出了一种将数值推理纳入知识图谱推理的新方法。我们的方法将数值视为一等公民，并提供了一个统一的框架，用于对实体和数值进行推理。我们在两个基准数据集WIKWD和WebQSP上进行了广泛的实验，证明了我们所提出的方法在回答涉及数值的复杂查询方面的有效性和效率。

    A complex logic query in a knowledge graph refers to a query expressed in logic form that conveys a complex meaning, such as where did the Canadian Turing award winner graduate from? Knowledge graph reasoning-based applications, such as dialogue systems and interactive search engines, rely on the ability to answer complex logic queries as a fundamental task. In most knowledge graphs, edges are typically used to either describe the relationships between entities or their associated attribute values. An attribute value can be in categorical or numerical format, such as dates, years, sizes, etc. However, existing complex query answering (CQA) methods simply treat numerical values in the same way as they treat entities. This can lead to difficulties in answering certain queries, such as which Australian Pulitzer award winner is born before 1927, and which drug is a pain reliever and has fewer side effects than Paracetamol. In this work, inspired by the recent advances in numerical encoding
    
[^29]: 自适应消息量化和并行化在分布式全图GNN训练中的应用

    Adaptive Message Quantization and Parallelization for Distributed Full-graph GNN Training. (arXiv:2306.01381v1 [cs.LG])

    [http://arxiv.org/abs/2306.01381](http://arxiv.org/abs/2306.01381)

    本文提出了一种高效的GNN训练系统AdaQP，通过随机量化跨设备传输的消息以降低通信流量，并提倡边缘节点和中心节点之间的通信-计算并行化，以加快分布式全图GNN训练速度。

    

    大规模图网络神经网络（GNNs）的分布式全图训练具有带宽需求高和耗时长的特点。跨设备频繁交换节点特征、嵌入和嵌入梯度（均称为消息）带来了显著的通信开销，对于在其他设备上具有远程邻居的节点（边缘节点）而言，而对于没有远程邻居的节点（中心节点）而言则带来了不必要的等待时间。本文提出了一种高效的GNN训练系统AdaQP，通过随机量化跨设备传输的消息以降低通信流量，并提倡边缘节点和中心节点之间的通信-计算并行化，以加快分布式全图GNN训练速度。我们提供了理论分析，证明了快速训练收敛（以O（T ^ {-1}）的速率，其中T为训练周期的总数），并设计了一种自适应量化位宽分配方案。

    Distributed full-graph training of Graph Neural Networks (GNNs) over large graphs is bandwidth-demanding and time-consuming. Frequent exchanges of node features, embeddings and embedding gradients (all referred to as messages) across devices bring significant communication overhead for nodes with remote neighbors on other devices (marginal nodes) and unnecessary waiting time for nodes without remote neighbors (central nodes) in the training graph. This paper proposes an efficient GNN training system, AdaQP, to expedite distributed full-graph GNN training. We stochastically quantize messages transferred across devices to lower-precision integers for communication traffic reduction and advocate communication-computation parallelization between marginal nodes and central nodes. We provide theoretical analysis to prove fast training convergence (at the rate of O(T^{-1}) with T being the total number of training epochs) and design an adaptive quantization bit-width assignment scheme for eac
    
[^30]: 一份关于代码异味数据集和验证机制的系统文献综述 (arXiv:2306.01377v1 [cs.SE])

    A systematic literature review on the code smells datasets and validation mechanisms. (arXiv:2306.01377v1 [cs.SE])

    [http://arxiv.org/abs/2306.01377](http://arxiv.org/abs/2306.01377)

    代码异味检测工具的准确性取决于所使用的数据集，现有数据集存在样本不平衡、缺乏支持严重级别和仅限于Java语言等问题。

    

    代码异味检测工具的准确性取决于用于评估这些工具的数据集。我们调查了45个现有数据集，发现数据集的充分性高度取决于相关属性，例如大小，严重级别，项目类型，每种异味的数量，异味的数量，以及数据集中发生异味和非异味样本的比例。大多数现有数据集支持God Class，Long Method和Feature Envy，而Fowler和Beck目录中的六种异味没有被任何数据集支持。我们得出结论，现有数据集存在样本不平衡，缺乏支持严重级别，且仅限于Java语言的限制。

    The accuracy reported for code smell-detecting tools varies depending on the dataset used to evaluate the tools. Our survey of 45 existing datasets reveals that the adequacy of a dataset for detecting smells highly depends on relevant properties such as the size, severity level, project types, number of each type of smell, number of smells, and the ratio of smelly to non-smelly samples in the dataset. Most existing datasets support God Class, Long Method, and Feature Envy while six smells in Fowler and Beck's catalog are not supported by any datasets. We conclude that existing datasets suffer from imbalanced samples, lack of supporting severity level, and restriction to Java language.
    
[^31]: 基于TSP解空间缩减的能源高效的无人机辅助物联网数据收集

    Energy-Efficient UAV-Assisted IoT Data Collection via TSP-Based Solution Space Reduction. (arXiv:2306.01355v1 [cs.AI])

    [http://arxiv.org/abs/2306.01355](http://arxiv.org/abs/2306.01355)

    本文提出了一种基于TSP的UAV辅助物联网数据收集框架，通过解空间缩减和算法优化，实现了能源高效、低复杂度、高效收集分布式传感器数据的目标。

    

    本文提出了一种无线数据收集框架，它利用无人机(UAV)高效地从部署在大范围内的分布式物联网传感器中收集数据。我们的方法考虑到传感器的非零通信范围，以优化UAV的飞行路径，从而产生了TSP变体。我们在数学上证明了这种TSP变型问题的最优航点是受限于传感器通信范围的边界，极大地缩小了解空间。基于这个发现，我们开发了一种低复杂度的UAV辅助传感器数据收集算法，并在选择的用例中展示了它的有效性，在联合优化UAV的行驶距离和传感器的通信范围的同时最小化UAV和传感器的总能量消耗。

    This paper presents a wireless data collection framework that employs an unmanned aerial vehicle (UAV) to efficiently gather data from distributed IoT sensors deployed in a large area. Our approach takes into account the non-zero communication ranges of the sensors to optimize the flight path of the UAV, resulting in a variation of the Traveling Salesman Problem (TSP). We prove mathematically that the optimal waypoints for this TSP-variant problem are restricted to the boundaries of the sensor communication ranges, greatly reducing the solution space. Building on this finding, we develop a low-complexity UAV-assisted sensor data collection algorithm, and demonstrate its effectiveness in a selected use case where we minimize the total energy consumption of the UAV and sensors by jointly optimizing the UAV's travel distance and the sensors' communication ranges.
    
[^32]: 联邦领域泛化：一项调查研究

    Federated Domain Generalization: A Survey. (arXiv:2306.01334v1 [cs.LG])

    [http://arxiv.org/abs/2306.01334](http://arxiv.org/abs/2306.01334)

    该论文调查了联邦领域泛化的研究领域，提到了联邦学习和领域泛化技术的优势，以及在泛化联邦模型时面临的挑战。

    

    机器学习通常依赖于一个假设，即训练和测试的分布是相同的，数据是集中存储供训练和测试之用。然而，在现实场景中，分布可能存在显著差异，并且数据通常分布在不同的设备、组织或边缘节点上。因此，必须开发能够有效泛化到未见过的分布，并且数据分布在不同领域的模型。为了应对这一挑战，近年来出现了对联邦领域泛化 (FDG) 的极大兴趣。FDG 结合了联邦学习 (FL) 和领域泛化 (DG) 技术的优点，使多个源领域能够协作学习一个能够直接泛化到未见过的领域而又保持数据隐私的模型。然而，在领域转移下泛化联邦模型是一个技术上具有挑战性的问题，目前还没有得到充分的关注。

    Machine learning typically relies on the assumption that training and testing distributions are identical and that data is centrally stored for training and testing. However, in real-world scenarios, distributions may differ significantly and data is often distributed across different devices, organizations, or edge nodes. Consequently, it is imperative to develop models that can effectively generalize to unseen distributions where data is distributed across different domains. In response to this challenge, there has been a surge of interest in federated domain generalization (FDG) in recent years. FDG combines the strengths of federated learning (FL) and domain generalization (DG) techniques to enable multiple source domains to collaboratively learn a model capable of directly generalizing to unseen domains while preserving data privacy. However, generalizing the federated model under domain shifts is a technically challenging problem that has received scant attention in the research 
    
[^33]: 放射学AI中的公平性导航：概念、后果和关键考虑

    Navigating Fairness in Radiology AI: Concepts, Consequences,and Crucial Considerations. (arXiv:2306.01333v1 [cs.LG])

    [http://arxiv.org/abs/2306.01333](http://arxiv.org/abs/2306.01333)

    本文讨论了放射学AI中的公平性问题，介绍了Aequitas偏差审计工具包并阐明了公平性评估的基本指标，强调了考虑AI伦理影响的重要性。

    

    人工智能（AI）已经在放射学领域取得了重大革命性进展，承诺改善患者结果和简化流程。然而，确保AI模型的公平性至关重要，以防止隐蔽的偏见和不平等现象导致不平等结果。本综述讨论了AI中的公平概念，重点讨论使用Aequitas工具包进行偏差审计及其在放射学中的实际影响，特别是在疾病筛查场景中的影响。Aequitas是一个开源的偏差审计工具包，审查AI模型的决策，识别可能导致不同人口群体和成像设备品牌之间差异的隐藏偏见。该工具包基于统计理论，分析大型数据集以揭示模型的公平性。它在同时处理多种变量方面表现出色，特别是在放射学这样的多元化领域中。本综述阐明了公平性评估的基本指标：平等和比例平等、假阳性率平等、假阴性率平等和预测平等。作者还强调了在放射学AI中考虑伦理影响的重要性，包括对患者造成伤害的风险以及在AI决策过程中需要透明度的必要性。

    Artificial Intelligence (AI) has significantly revolutionized radiology, promising improved patient outcomes and streamlined processes. However, it's critical to ensure the fairness of AI models to prevent stealthy bias and disparities from leading to unequal outcomes. This review discusses the concept of fairness in AI, focusing on bias auditing using the Aequitas toolkit, and its real-world implications in radiology, particularly in disease screening scenarios. Aequitas, an open-source bias audit toolkit, scrutinizes AI models' decisions, identifying hidden biases that may result in disparities across different demographic groups and imaging equipment brands. This toolkit operates on statistical theories, analyzing a large dataset to reveal a model's fairness. It excels in its versatility to handle various variables simultaneously, especially in a field as diverse as radiology. The review explicates essential fairness metrics: Equal and Proportional Parity, False Positive Rate Parity
    
[^34]: 揭示图神经网络中的结构差异性：一个尺码适用于所有吗？

    Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?. (arXiv:2306.01323v1 [cs.LG])

    [http://arxiv.org/abs/2306.01323](http://arxiv.org/abs/2306.01323)

    本研究证明GNN在同构图中的同构节点和异构图中的异构节点上表现良好，而在另一组节点上表现不佳。该研究提出了一种新框架，通过使用加权聚合提高GNN在不同结构模式节点上的表现，有效解决结构差异性问题。

    

    近期关于图神经网络（GNN）的研究提供了实证和理论证据，支持它们在捕捉同构和某些异构图上的结构模式方面的有效性。值得注意的是，大多数实际中的同构和异构图都由同构和异构结构模式的混合节点组成，表现出一定的结构差异性。然而，关于不同结构模式下的节点（例如在异构图中的同构节点）在GNN分类任务中的表现分析仍然很有限。本文通过理论和实证研究证明，GNN在同构图中的同构节点和异构图中的异构节点上的表现通常是出色的，而在另一组节点上表现不佳，表现出性能差异性。我们进一步识别了测试展示不同结构模式节点时GNN的效应，并提出了一种通过使用GNN的加权聚合以适应性结构差异性的新框架的解决方案。在各种数据集上的实验表明，所提出的方法在解决结构差异性和提高节点分类任务的性能方面是有效的。

    Recent studies on Graph Neural Networks(GNNs) provide both empirical and theoretical evidence supporting their effectiveness in capturing structural patterns on both homophilic and certain heterophilic graphs. Notably, most real-world homophilic and heterophilic graphs are comprised of a mixture of nodes in both homophilic and heterophilic structural patterns, exhibiting a structural disparity. However, the analysis of GNN performance with respect to nodes exhibiting different structural patterns, e.g., homophilic nodes in heterophilic graphs, remains rather limited. In the present study, we provide evidence that Graph Neural Networks(GNNs) on node classification typically perform admirably on homophilic nodes within homophilic graphs and heterophilic nodes within heterophilic graphs while struggling on the opposite node set, exhibiting a performance disparity. We theoretically and empirically identify effects of GNNs on testing nodes exhibiting distinct structural patterns. We then pr
    
[^35]: EPIC: 通过可学习的代价实现的编辑路径插值的图形增强

    EPIC: Graph Augmentation with Edit Path Interpolation via Learnable Cost. (arXiv:2306.01310v1 [cs.LG])

    [http://arxiv.org/abs/2306.01310](http://arxiv.org/abs/2306.01310)

    EPIC提出了一种基于插值的方法来增强图数据集，通过利用图编辑距离生成与原始图相似但有结构变化的新图，从而提高了分类模型的泛化能力。

    

    基于图的模型在各个领域中变得越来越重要，但现有图数据集的有限规模和多样性经常限制它们的性能。为解决这个问题，我们提出了EPIC（通过可学习的代价实现的编辑路径插值），这是一种新颖的基于插值的增强图数据集的方法。我们的方法利用了图编辑距离来生成与原始图相似但结构有所变化的新图。为了实现这一点，我们通过比较带标签的图来学习图编辑距离，并利用这一知识在原始图对之间创建了图编辑路径。通过从图编辑路径中随机抽样的图形，我们丰富了训练集以增强分类模型的泛化能力。我们在几个基准数据集上展示了我们方法的有效性，并表明它在图分类任务中优于现有的增强方法。

    Graph-based models have become increasingly important in various domains, but the limited size and diversity of existing graph datasets often limit their performance. To address this issue, we propose EPIC (Edit Path Interpolation via learnable Cost), a novel interpolation-based method for augmenting graph datasets. Our approach leverages graph edit distance to generate new graphs that are similar to the original ones but exhibit some variation in their structures. To achieve this, we learn the graph edit distance through a comparison of labeled graphs and utilize this knowledge to create graph edit paths between pairs of original graphs. With randomly sampled graphs from a graph edit path, we enrich the training set to enhance the generalization capability of classification models. We demonstrate the effectiveness of our approach on several benchmark datasets and show that it outperforms existing augmentation methods in graph classification tasks.
    
[^36]: 高扩展性任务完成的自我中心规划

    Egocentric Planning for Scalable Embodied Task Achievement. (arXiv:2306.01295v1 [cs.AI])

    [http://arxiv.org/abs/2306.01295](http://arxiv.org/abs/2306.01295)

    本文提出了一种自我中心规划的方法，结合符号规划和面向对象的POMDP模型，成为高扩展性任务完成的一种新途径。

    

    在不同环境中完成任务对于采取行动的机器人代理来说面临着重大挑战，尤其是在对象类型上的泛化以及执行适当的行动以完成任务。本文提出了自我中心规划，一种创新的方法，将符号规划和面向对象的POMDP结合起来，以解决复杂环境下的任务，利用现有的视觉感知和自然语言处理模型。我们在ALFRED（一个模拟的家庭任务环境）中评估了我们的方法，并展示了其高可扩展性，在ALFRED基准测试中取得了惊人的36.07%的未见过的成功率，并在CVPR Emobodied AI workshop中获得了ALFRED挑战赛的胜利。我们的方法需要可靠的感知，并规定或学习关于代理的行动前提和效果，以及对象类型的符号描述。

    Embodied agents face significant challenges when tasked with performing actions in diverse environments, particularly in generalizing across object types and executing suitable actions to accomplish tasks. Furthermore, agents should exhibit robustness, minimizing the execution of illegal actions. In this work, we present Egocentric Planning, an innovative approach that combines symbolic planning and Object-oriented POMDPs to solve tasks in complex environments, harnessing existing models for visual perception and natural language processing. We evaluated our approach in ALFRED, a simulated environment designed for domestic tasks, and demonstrated its high scalability, achieving an impressive 36.07% unseen success rate in the ALFRED benchmark and winning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires reliable perception and the specification or learning of a symbolic description of the preconditions and effects of the agent's actions, as well as what object types
    
[^37]: KL-Divergence引导下的温度采样

    KL-Divergence Guided Temperature Sampling. (arXiv:2306.01286v1 [cs.CL])

    [http://arxiv.org/abs/2306.01286](http://arxiv.org/abs/2306.01286)

    该论文提出了一种新的温度采样算法，通过KL-散度引导动态调整温度，从而缓解多样性和可归因性之间的权衡，实验证明该算法在对话问答和摘要任务中表现优异。

    

    温度采样是一种常规的方法，用于将大型语言模型的预测多样化。随着温度的升高，预测变得更加多样化，但也容易产生幻觉——生成看似合理但不正确的令牌。缓解幻觉的一种常见方法是提供源/基础文档，并使模型训练生成与提供的来源相关且可归因的预测。看来存在多样性和可归因性之间的权衡。为了缓解这种权衡，我们提出了一个松弛固定温度和通过KL-散度根据其与源的相关性引导动态温度的机制。我们的实验证实了这种权衡，并表明我们的采样算法在对话问答和摘要任务中优于常规的top-k和top-p算法。

    Temperature sampling is a conventional approach to diversify large language model predictions. As temperature increases, the prediction becomes diverse but also vulnerable to hallucinations -- generating tokens that are sensible but not factual. One common approach to mitigate hallucinations is to provide source/grounding documents and the model is trained to produce predictions that bind to and are attributable to the provided source. It appears that there is a trade-off between diversity and attribution. To mitigate any such trade-off, we propose to relax the constraint of having a fixed temperature over decoding steps, and a mechanism to guide the dynamic temperature according to its relevance to the source through KL-divergence. Our experiments justifies the trade-off, and shows that our sampling algorithm outperforms the conventional top-k and top-p algorithms in conversational question-answering and summarization tasks.
    
[^38]: 智能城市交通系统中基于图的机器学习的最新进展。

    Recent Advances in Graph-based Machine Learning for Applications in Smart Urban Transportation Systems. (arXiv:2306.01282v1 [cs.LG])

    [http://arxiv.org/abs/2306.01282](http://arxiv.org/abs/2306.01282)

    智能交通系统（ITS）是现代交通基础设施的重要组成部分。已出现基于复杂数据的数据驱动解决方案以解决各种与ITS相关的挑战。图形机器学习方法成为了智能交通系统领域的日益重要的研究重点。

    

    智能交通系统（ITS）是现代交通基础设施的重要组成部分，采用通信技术，信息处理和控制系统来管理交通网络。这种集成各种组件（如道路，车辆和通信系统）的方法，预计通过提供更好的信息，服务和交通模式的协调来提高效率和安全性。近年来，基于图的机器学习已成为智能交通系统领域日益重要的研究重点，旨在开发基于复杂数据的数据驱动解决方案，以解决各种与ITS相关的挑战。本章节提供ITS设计的关键技术挑战的背景信息，以及涵盖从经典统计方法到现代机器学习和基于深度学习的方法的研究方法的综述。具体而言，我们提供了基于图的机器学习方法的深入审查。

    The Intelligent Transportation System (ITS) is an important part of modern transportation infrastructure, employing a combination of communication technology, information processing and control systems to manage transportation networks. This integration of various components such as roads, vehicles, and communication systems, is expected to improve efficiency and safety by providing better information, services, and coordination of transportation modes. In recent years, graph-based machine learning has become an increasingly important research focus in the field of ITS aiming at the development of complex, data-driven solutions to address various ITS-related challenges. This chapter presents background information on the key technical challenges for ITS design, along with a review of research methods ranging from classic statistical approaches to modern machine learning and deep learning-based approaches. Specifically, we provide an in-depth review of graph-based machine learning metho
    
[^39]: 组合启发式和多智能体强化学习的多机器人路径规划

    Multi-Robot Path Planning Combining Heuristics and Multi-Agent Reinforcement Learning. (arXiv:2306.01270v1 [cs.AI])

    [http://arxiv.org/abs/2306.01270](http://arxiv.org/abs/2306.01270)

    提出了一种多机器人路径规划方法MAPPOHR，该方法结合了启发式搜索、经验规则和多智能体强化学习。实验结果表明，该方法在规划效率和避碰能力方面优于现有方法。

    

    动态环境下的多机器人路径规划是一个极具挑战性的经典问题。在移动过程中，机器人需要避免与其他移动机器人发生碰撞，同时最小化它们的行驶距离。以往的方法要么使用启发式搜索方法不断重新规划路径以避免冲突，要么基于学习方法选择适当的避碰策略。前者可能由于频繁的重新规划导致行驶距离较长，而后者可能由于低样本探索和利用率而导致学习效率低，从而使模型的训练成本较高。为解决这些问题，我们提出了一种路径规划方法MAPPOHR，该方法结合了启发式搜索、经验规则和多智能体强化学习。该方法包含两个层次：基于多智能体强化学习算法MAPPO的实时规划器，其将经验规则嵌入到动作输出层和奖励函数中；以及一个启发式规划器，它生成初始路径并向MAPPO规划器添加约束。我们的实验结果表明，所提出的方法在规划效率和避碰能力方面优于现有方法。

    Multi-robot path finding in dynamic environments is a highly challenging classic problem. In the movement process, robots need to avoid collisions with other moving robots while minimizing their travel distance. Previous methods for this problem either continuously replan paths using heuristic search methods to avoid conflicts or choose appropriate collision avoidance strategies based on learning approaches. The former may result in long travel distances due to frequent replanning, while the latter may have low learning efficiency due to low sample exploration and utilization, and causing high training costs for the model. To address these issues, we propose a path planning method, MAPPOHR, which combines heuristic search, empirical rules, and multi-agent reinforcement learning. The method consists of two layers: a real-time planner based on the multi-agent reinforcement learning algorithm, MAPPO, which embeds empirical rules in the action output layer and reward functions, and a heuri
    
[^40]: 负责任的任务自动化: 使大型语言模型成为负责任的任务自动化工具

    Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators. (arXiv:2306.01242v1 [cs.AI])

    [http://arxiv.org/abs/2306.01242](http://arxiv.org/abs/2306.01242)

    本研究提出了一个基础框架-"负责任的任务自动化（ResponsibleTA）"，使大型语言模型可以负责任地作为任务协同工具。该框架增强了LLM的三种能力：预测任务可行性、验证任务完整性以及增强任务安全性。

    

    大型语言模型（LLMs）的成功为人工智能迈出了重要的一步。它们展现了在用户指令下自动完成任务的良好前景，可以作为类似大脑的协调者。随着我们将越来越多的任务交给机器自动完成，相关的风险也逐渐显现。一个重要的问题出现了: 当机器像人类驾驶协同一样帮助人们自动完成任务时，我们如何确保机器的责任行为？在本文中，我们从可行性、完整性和安全性的角度，深入探讨这个问题。具体来说，我们提出了“负责任的任务自动化”（ResponsibleTA）作为一个基础框架，以促进LLM协调者和执行者之间的负责任协作，实现任务自动化。该框架拥有三种增强能力: 1）预测执行者命令的可行性；2）验证执行者的完整性；3）增强安全性（例如，保护隐私）。

    The recent success of Large Language Models (LLMs) signifies an impressive stride towards artificial general intelligence. They have shown a promising prospect in automatically completing tasks upon user instructions, functioning as brain-like coordinators. The associated risks will be revealed as we delegate an increasing number of tasks to machines for automated completion. A big question emerges: how can we make machines behave responsibly when helping humans automate tasks as personal copilots? In this paper, we explore this question in depth from the perspectives of feasibility, completeness and security. In specific, we present Responsible Task Automation (ResponsibleTA) as a fundamental framework to facilitate responsible collaboration between LLM-based coordinators and executors for task automation with three empowered capabilities: 1) predicting the feasibility of the commands for executors; 2) verifying the completeness of executors; 3) enhancing the security (e.g., the prote
    
[^41]: 在一个姓氏网络中预测亲缘关系

    Predicting affinity ties in a surname network. (arXiv:2306.01218v1 [cs.SI])

    [http://arxiv.org/abs/2306.01218](http://arxiv.org/abs/2306.01218)

    本文基于一个姓氏亲缘网络，通过建立知识库完成模型预测新联系的形成并发现共享邻居和嵌入空间中的邻居对于预测都十分重要，这一发现对于解释圣地亚哥精英内婚中的高水平很有帮助。

    

    本文从智利圣地亚哥的姓氏管理登记中创建一个姓氏亲缘网络，编码了社会经济数据。这个网络是一个多重关系图，节点代表姓氏，边代表不同社会经济分位数之间姓氏间互动的盛行程度。我们将链接预测建模为知识库完成问题，并发现共享邻居高度预测新链接的形成。重要的是，我们区分了嵌入空间中的有根邻居和邻居，并发现后者更能预测联系的形成。本文讨论了这一发现在解释圣地亚哥精英内婚的高水平中的意义。

    From administrative registers of last names in Santiago, Chile, we create a surname affinity network that encodes socioeconomic data. This network is a multi-relational graph with nodes representing surnames and edges representing the prevalence of interactions between surnames by socioeconomic decile. We model the prediction of links as a knowledge base completion problem, and find that sharing neighbors is highly predictive of the formation of new links. Importantly, We distinguish between grounded neighbors and neighbors in the embedding space, and find that the latter is more predictive of tie formation. The paper discusses the implications of this finding in explaining the high levels of elite endogamy in Santiago.
    
[^42]: 在域内和域外样本之间估计语义相似度

    Estimating Semantic Similarity between In-Domain and Out-of-Domain Samples. (arXiv:2306.01206v1 [cs.CL])

    [http://arxiv.org/abs/2306.01206](http://arxiv.org/abs/2306.01206)

    本文研究如何以原则性的方式分析模型在域内和域外设置下的性能，最终找出一种无监督方法识别OOD / OODist样本。

    

    先前的工作通常将来自数据集或源与训练集不同但用于同一任务的域外（OOD）或域外分布（OODist）样本描述为域外，与域内（ID）样本相比，模型在OOD样本上的表现通常较差，尽管这种观察结果并不一致。另一方面，一些研究关注于OOD检测，但大多使用有监督的方法。在这项工作中，我们首先整合并呈现了多个关于OOD和OODist的多重定义，并以原则性的方式分析了模型在ID和OOD / OODist设置下的性能。最后，我们试图识别一种无监督方法，可在不使用训练模型的情况下可靠地识别OOD / OODist样本。我们使用4个不同任务的12个数据集进行了广泛评估的结果表明，无监督度量在该任务中具有良好的潜力。

    Prior work typically describes out-of-domain (OOD) or out-of-distribution (OODist) samples as those that originate from dataset(s) or source(s) different from the training set but for the same task. When compared to in-domain (ID) samples, the models have been known to usually perform poorer on OOD samples, although this observation is not consistent. Another thread of research has focused on OOD detection, albeit mostly using supervised approaches. In this work, we first consolidate and present a systematic analysis of multiple definitions of OOD and OODist as discussed in prior literature. Then, we analyze the performance of a model under ID and OOD/OODist settings in a principled way. Finally, we seek to identify an unsupervised method for reliably identifying OOD/OODist samples without using a trained model. The results of our extensive evaluation using 12 datasets from 4 different tasks suggest the promising potential of unsupervised metrics in this task.
    
[^43]: 一种有效的评估生存模型的有意义方法。

    An Effective Meaningful Way to Evaluate Survival Models. (arXiv:2306.01196v1 [cs.LG])

    [http://arxiv.org/abs/2306.01196](http://arxiv.org/abs/2306.01196)

    该论文提出了一种有效的方法来评估生存模型的性能，使用伪观察值的MAE指标能够准确地排名模型的性能，发现这种方法比其他替代方法更好。

    

    评估生存预测模型的一种直接指标是基于平均绝对误差（MAE）-模型预测时间与真实事件时间之间的绝对差值的平均值，对所有个体进行。然而，这是具有挑战性的，因为在实践中，测试集包括（正确）被审查的个体，这意味着我们不知道被审查个体实际经历事件的时间。在本文中，我们探索了用于评估包括（许多）被审查个体的生存数据集的各种指标来估计MAE。此外，我们介绍了一种新颖而有效的方法来生成逼真的半合成生存数据集，以便评估指标。基于半合成数据集的分析，我们的发现表明，我们提出的指标（使用伪观察法的MAE）能够准确地排名模型的性能，并且通常与真实的MAE非常接近-特别是优于几种替代方法。

    One straightforward metric to evaluate a survival prediction model is based on the Mean Absolute Error (MAE) -- the average of the absolute difference between the time predicted by the model and the true event time, over all subjects. Unfortunately, this is challenging because, in practice, the test set includes (right) censored individuals, meaning we do not know when a censored individual actually experienced the event. In this paper, we explore various metrics to estimate MAE for survival datasets that include (many) censored individuals. Moreover, we introduce a novel and effective approach for generating realistic semi-synthetic survival datasets to facilitate the evaluation of metrics. Our findings, based on the analysis of the semi-synthetic datasets, reveal that our proposed metric (MAE using pseudo-observations) is able to rank models accurately based on their performance, and often closely matches the true MAE -- in particular, is better than several alternative methods.
    
[^44]: 面向边缘人工智能的集成感知-通信-计算 （Integrated Sensing-Communication-Computation） （arXiv：2306.01162v1 [cs.IT]）

    Integrated Sensing-Communication-Computation for Edge Artificial Intelligence. (arXiv:2306.01162v1 [cs.IT])

    [http://arxiv.org/abs/2306.01162](http://arxiv.org/abs/2306.01162)

    面向边缘人工智能的集成感知-通信-计算技术对于提高资源利用率以及实现边缘学习和边缘人工智能推理任务的定制目标至关重要。

    

    边缘人工智能是实现万物智能的一种有前途的解决方案，可用于数字孪生、全息投影、语义通信和自动驾驶等高级技术。边缘人工智能任务的性能，包括边缘学习和边缘人工智能推理，取决于三个高度耦合的过程的质量，即数据获取的感知、信息提取的计算和信息传输的通信。然而，这三个模块需要为增强自己的服务质量而竞争网络资源。为此，集成感知-通信-计算（ISCC）对于提高资源利用率以及实现边缘人工智能任务的定制目标至关重要。通过研究三个模块之间的相互作用，本文提出了各种 ISCC 方案，适用于联邦边缘学习任务和边缘人工智能推理任务。

    Edge artificial intelligence (AI) has been a promising solution towards 6G to empower a series of advanced techniques such as digital twin, holographic projection, semantic communications, and auto-driving, for achieving intelligence of everything. The performance of edge AI tasks, including edge learning and edge AI inference, depends on the quality of three highly coupled processes, i.e., sensing for data acquisition, computation for information extraction, and communication for information transmission. However, these three modules need to compete for network resources for enhancing their own quality-of-services. To this end, integrated sensing-communication-computation (ISCC) is of paramount significance for improving resource utilization as well as achieving the customized goals of edge AI tasks. By investigating the interplay among the three modules, this article presents various kinds of ISCC schemes for federated edge learning tasks and edge AI inference tasks in both applicati
    
[^45]: 通过稀疏Flash注意力加速处理大序列中的因果关系

    Faster Causal Attention Over Large Sequences Through Sparse Flash Attention. (arXiv:2306.01160v1 [cs.LG])

    [http://arxiv.org/abs/2306.01160](http://arxiv.org/abs/2306.01160)

    本文介绍了一种新的稀疏Flash注意力机制，能够快速处理大序列中的因果关系，且速度提高了多倍，可以作为任何基于Transformer的语言模型中密集自我注意力的替代方案，并在多个设置中产生了最先进的结果。

    

    基于Transformer的语言模型已经在处理越来越长序列的任务中被广泛应用。对于这些应用，序列长度关于的因果自注意力成为一个核心问题，因为它是唯一一个与序列长度呈二次关系的组件。虽然许多研究已经提出方案来使自注意力的注意力模式稀疏化，但这些方案通常受到实现方面的限制，并最终强加一个简单且静态的结构在关注矩阵上。相反，实现更动态的稀疏注意力通常会导致运行时间显着慢于使用Dao等人（2022）的Flash实现计算完整注意力。我们扩展了FlashAttention，以适应包含键/查询丢弃和基于哈希的注意力在内的大类稀疏注意性模式。这导致实现没有任何计算复杂度开销，并且与以前的动态稀疏注意性相比，速度提高了多倍。我们的方法可用作任何基于Transformer的语言模型中密集自我注意力的替代方案，并在多个设置中产生了最先进的结果，包括生成语言建模和长格式问答。

    Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtim
    
[^46]: 基于异构知识的增强模块化强化学习

    Augmented Modular Reinforcement Learning based on Heterogeneous Knowledge. (arXiv:2306.01158v1 [cs.LG])

    [http://arxiv.org/abs/2306.01158](http://arxiv.org/abs/2306.01158)

    该论文提出了增强模块化强化学习（AMRL），使用仲裁器来选择异构模块，并无缝地整合不同类型的知识。该方法能够减缓强化学习中的一些低效问题，有望在深度强化学习领域得到应用。

    

    为了减缓强化学习中的一些低效问题，学者们提出了模块化方法，将不同的决策制定策略组合起来以衍生出可以执行多种任务的代理。这些体系结构的基础模块通常是可重复使用的，也允许“即插即用”的集成。然而，这些解决方案仍然缺乏处理和整合多种类型信息（知识）的能力，例如规则，子目标和技能。我们提出了增强模块化强化学习（AMRL）来解决这些限制。这种新的框架使用仲裁器来选择异构模块，并无缝地整合不同类型的知识。此外，我们引入了一种选择机制的变体，即增强记忆的仲裁器，它增加了利用时间信息的能力。我们在已有的环境中评估了所提出的机制，同时也在新环境中进行了基准测试，结果表明其在深度强化学习领域具有良好的应用前景。

    In order to mitigate some of the inefficiencies of Reinforcement Learning (RL), modular approaches composing different decision-making policies to derive agents capable of performing a variety of tasks have been proposed. The modules at the basis of these architectures are generally reusable, also allowing for "plug-and-play" integration. However, such solutions still lack the ability to process and integrate multiple types of information (knowledge), such as rules, sub-goals, and skills. We propose Augmented Modular Reinforcement Learning (AMRL) to address these limitations. This new framework uses an arbitrator to select heterogeneous modules and seamlessly incorporate different types of knowledge. Additionally, we introduce a variation of the selection mechanism, namely the Memory-Augmented Arbitrator, which adds the capability of exploiting temporal information. We evaluate the proposed mechanisms on established as well as new environments and benchmark them against prominent deep 
    
[^47]: 非可识别的隐变量下的Delphic离线强化学习

    Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding. (arXiv:2306.01157v1 [cs.LG])

    [http://arxiv.org/abs/2306.01157](http://arxiv.org/abs/2306.01157)

    本文提出了一种名为Delphic不确定性的方法来解决离线强化学习中的隐藏混淆问题，并通过实验表明该方法在非可识别的隐藏混淆情况下优于现有算法。

    

    离线强化学习中的一个突出挑战是隐藏的混淆问题：未观察到的变量可能影响到智能体采取的行动和观察到的结果。隐藏的混淆可能损害从数据中得出的任何因果结论的有效性，并且是有效的离线强化学习的一个主要障碍。在这篇论文中，我们解决了非可识别设置中的隐藏混淆问题。我们提出了一种基于与观察兼容的世界模型的差异来定义由隐藏混淆偏差引起的不确定性，称为Delphic不确定性，并将其与众所周知的认知和随机不确定性区分开来。我们导出了一种实际方法来估计这三种类型的不确定性，并构建了一种悲观的离线强化学习算法来解决它们。我们的方法不假定未观察到的混淆因子是可识别的，并且试图减少混淆偏差的量。通过广泛的实验表明，我们的方法在存在非可识别的隐藏混淆时优于现有的离线强化学习算法。

    A prominent challenge of offline reinforcement learning (RL) is the issue of hidden confounding: unobserved variables may influence both the actions taken by the agent and the observed outcomes. Hidden confounding can compromise the validity of any causal conclusion drawn from data and presents a major obstacle to effective offline RL. In the present paper, we tackle the problem of hidden confounding in the nonidentifiable setting. We propose a definition of uncertainty due to hidden confounding bias, termed delphic uncertainty, which uses variation over world models compatible with the observations, and differentiate it from the well-known epistemic and aleatoric uncertainties. We derive a practical method for estimating the three types of uncertainties, and construct a pessimistic offline RL algorithm to account for them. Our method does not assume identifiability of the unobserved confounders, and attempts to reduce the amount of confounding bias. We demonstrate through extensive ex
    
[^48]: 你读懂了说明书吗？重新思考指令学习中任务定义的有效性。

    Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning. (arXiv:2306.01150v1 [cs.CL])

    [http://arxiv.org/abs/2306.01150](http://arxiv.org/abs/2306.01150)

    本文研究了任务定义在指令学习中的作用，发现当删除描述任务输出的内容时模型性能才会显著下降，提出了一种自动算法可以压缩任务定义，删除60\%标记仍可以提高模型性能。

    

    大型语言模型在遵循自然语言指令解决未见过的任务方面表现出色。然而，模型是否真正理解任务定义以及人类编写的定义是否最佳仍然不清楚。本文系统研究了任务定义在指令学习中的作用。我们首先进行消融分析，通过人类注释发现只有当删除描述任务输出的内容，尤其是标签信息时，模型的性能才会显著下降。接下来，我们提出了一种自动算法，将任务定义压缩到最小的支持标记集，并发现可以删除60\%的标记而维持或提高模型性能。基于这些结果，我们提出了两种策略来帮助模型更好地利用任务指令。

    Large language models (LLMs) have shown impressive performance in following natural language instructions to solve unseen tasks. However, it remains unclear whether models truly understand task definitions and whether the human-written definitions are optimal. In this paper, we systematically study the role of task definitions in instruction learning. We first conduct an ablation analysis informed by human annotations to understand which parts of a task definition are most important, and find that model performance only drops substantially when removing contents describing the task output, in particular label information. Next, we propose an automatic algorithm to compress task definitions to a minimal supporting set of tokens, and find that 60\% of tokens can be removed while maintaining or even improving model performance. Based on these results, we propose two strategies to help models better leverage task instructions: (1) providing only key information for tasks in a common struct
    
[^49]: 人工智能责任保险：以AI辅助电子诊断系统为例

    AI Liability Insurance With an Example in AI-Powered E-diagnosis System. (arXiv:2306.01149v1 [cs.AI])

    [http://arxiv.org/abs/2306.01149](http://arxiv.org/abs/2306.01149)

    本文章研究了AI责任保险，并以AI辅助的电子诊断系统为例进行了定量风险评估。AI责任保险可以作为监管机制来激励合规行为，并为高质量AI系统提供证书。

    

    人工智能在许多领域受到越来越多的关注。由于AI-powered系统中的不确定性和风险，在野外运用中引起了犹豫。作为一种经济解决方案来弥补潜在损害，AI责任保险是一个有前途的市场，可以增强AI对日常生活的整合。本文以一个AI辅助的电子诊断系统为例，研究AI责任保险。我们提供了一个定量风险评估模型，并进行了基于证据的数值分析。我们讨论了AI技术的可保险性标准，并建议必要的调整来适应AI产品的特点。我们表明，AI责任保险可以作为一种监管机制来激励合规行为，并作为高质量AI系统的证书。此外，我们建议调整保费以反映AI内在不确定性的动态演变。讨论了道德风险问题，并提出了对AI产品进行的建议。

    Artificial Intelligence (AI) has received an increasing amount of attention in multiple areas. The uncertainties and risks in AI-powered systems have created reluctance in their wild adoption. As an economic solution to compensate for potential damages, AI liability insurance is a promising market to enhance the integration of AI into daily life. In this work, we use an AI-powered E-diagnosis system as an example to study AI liability insurance. We provide a quantitative risk assessment model with evidence-based numerical analysis. We discuss the insurability criteria for AI technologies and suggest necessary adjustments to accommodate the features of AI products. We show that AI liability insurance can act as a regulatory mechanism to incentivize compliant behaviors and serve as a certificate of high-quality AI systems. Furthermore, we suggest premium adjustment to reflect the dynamic evolution of the inherent uncertainty in AI. Moral hazard problems are discussed and suggestions for 
    
[^50]: 平滑单调网络

    Smooth Monotonic Networks. (arXiv:2306.01147v1 [cs.LG])

    [http://arxiv.org/abs/2306.01147](http://arxiv.org/abs/2306.01147)

    本文提出了一种新的神经网络模块--平滑min-max(SMM)网络，相比于传统的min-max(MM)神经网络结构简单易用，在单调建模方面表现优异。

    

    单调性约束是统计建模中的强力正则化工具。它们可以在计算机支持的决策制定中支持公平性，并增加数据驱动科学模型的可信度。经典的min-max(MM)神经网络结构确保了单调性，但由于梯度消失而往往在训练过程中陷入不良局部最优。我们提出了对MM网络的简单修改，使用严格递增的平滑非线性函数来缓解这个问题。得到的平滑min-max(SMM)网络模块继承了MM架构的渐近逼近性质。它可以嵌入到更大的端到端深度学习系统中进行训练。在单调建模的神经网络方面，SMM模块要简单得多，计算需求也要少得多。尽管如此，在我们的实验中，它在泛化性能方面与替代神经和非神经方法相比表现得更为优异。

    Monotonicity constraints are powerful regularizers in statistical modelling. They can support fairness in computer supported decision making and increase plausibility in data-driven scientific models. The seminal min-max (MM) neural network architecture ensures monotonicity, but often gets stuck in undesired local optima during training because of vanishing gradients. We propose a simple modification of the MM network using strictly-increasing smooth non-linearities that alleviates this problem. The resulting smooth min-max (SMM) network module inherits the asymptotic approximation properties from the MM architecture. It can be used within larger deep learning systems trained end-to-end. The SMM module is considerably simpler and less computationally demanding than state-of-the-art neural networks for monotonic modelling. Still, in our experiments, it compared favorably to alternative neural and non-neural approaches in terms of generalization performance.
    
[^51]: 利用不可感知的扰动改善的学习图像压缩的重构失真

    Reconstruction Distortion of Learned Image Compression with Imperceptible Perturbations. (arXiv:2306.01125v1 [cs.CV])

    [http://arxiv.org/abs/2306.01125](http://arxiv.org/abs/2306.01125)

    本文引入了一种不可察觉攻击方法，旨在有效地降低学习图像压缩的重构质量，通过引入基于Frobenius范数的损失函数生成对抗样本，进一步利用高频分量对人眼视觉不敏感的特点，以确保扰动保持不显眼，该方法在Kodak数据集上表现出了有效性。

    

    近年来，学习图像压缩（LIC）因其非凡的性能而成为图像传输的趋势技术。尽管其受欢迎度很高，但是LIC在图像重构质量方面的稳健性仍未得到探索。本文引入一种不可感知攻击方法，旨在有效降低LIC的重构质量，导致重构图像被噪声严重干扰，重构图像中的任何对象都几乎不可能恢复。具体来说，我们通过引入基于Frobenius范数的损失函数来生成对抗样本，以最大化原始图像与重构对抗样本之间的差异。进一步利用高频分量对人眼视觉不敏感的特点，我们引入了不可察觉性约束（IC），以确保扰动保持不显眼。在Kodak数据集上使用各种LIC模型进行的实验证明了其有效性。

    Learned Image Compression (LIC) has recently become the trending technique for image transmission due to its notable performance. Despite its popularity, the robustness of LIC with respect to the quality of image reconstruction remains under-explored. In this paper, we introduce an imperceptible attack approach designed to effectively degrade the reconstruction quality of LIC, resulting in the reconstructed image being severely disrupted by noise where any object in the reconstructed images is virtually impossible. More specifically, we generate adversarial examples by introducing a Frobenius norm-based loss function to maximize the discrepancy between original images and reconstructed adversarial examples. Further, leveraging the insensitivity of high-frequency components to human vision, we introduce Imperceptibility Constraint (IC) to ensure that the perturbations remain inconspicuous. Experiments conducted on the Kodak dataset using various LIC models demonstrate effectiveness. In 
    
[^52]: 带有重尾奖励的差分隐私式情节强化学习

    Differentially Private Episodic Reinforcement Learning with Heavy-tailed Rewards. (arXiv:2306.01121v1 [cs.LG])

    [http://arxiv.org/abs/2306.01121](http://arxiv.org/abs/2306.01121)

    本研究针对重尾奖励的有限步骤表格马尔可夫决策过程问题探讨了差分隐私限制下的两种框架，即价值迭代和策略优化，同时考虑了联合差分隐私和本地差分隐私模型，并为两种情况提供了遗憾上限。

    

    本文研究了差分隐私(DP)限制下的重尾奖励的（有限步骤表格）马尔可夫决策过程(MDP)问题。与先前的私有强化学习研究通常假设奖励来自一些有界或次高斯分布以确保DP相比，我们考虑奖励分布只有有限的$(1+v)$阶矩的情况，$v \in (0,1]$。通过使用奖励的健壮均值估计器，我们首先提出了两种针对重尾MDP的框架，即一个用于价值迭代，另一个用于策略优化。在每个框架下，我们考虑了联合差分隐私(JDP)和本地差分隐私(LDP)模型。基于我们的框架，我们为JDP和LDP情况提供了遗憾上限，并表明分布的矩和隐私预算都对遗憾有重要影响。最后，我们建立了遗憾最小化的下限。

    In this paper, we study the problem of (finite horizon tabular) Markov decision processes (MDPs) with heavy-tailed rewards under the constraint of differential privacy (DP). Compared with the previous studies for private reinforcement learning that typically assume rewards are sampled from some bounded or sub-Gaussian distributions to ensure DP, we consider the setting where reward distributions have only finite $(1+v)$-th moments with some $v \in (0,1]$. By resorting to robust mean estimators for rewards, we first propose two frameworks for heavy-tailed MDPs, i.e., one is for value iteration and another is for policy optimization. Under each framework, we consider both joint differential privacy (JDP) and local differential privacy (LDP) models. Based on our frameworks, we provide regret upper bounds for both JDP and LDP cases and show that the moment of distribution and privacy budget both have significant impacts on regrets. Finally, we establish a lower bound of regret minimization
    
[^53]: Falcon LLM的RefinedWeb数据集：仅凭网络数据胜过筛选后的语料库

    The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only. (arXiv:2306.01116v1 [cs.CL])

    [http://arxiv.org/abs/2306.01116](http://arxiv.org/abs/2306.01116)

    本文发现经过适当的筛选和去重后，仅凭网络数据就能训练出强大的语言模型，这甚至明显优于使用筛选后的语料库训练。研究还表明，即使在广泛筛选后，从网络中提取的高质量数据仍然充足，这为训练更大、更强大的语言模型带来了希望。

    

    大型语言模型通常由经过筛选的网络数据和经过筛选的高质量语料库（如社交媒体对话、书籍或技术论文）混合训练。这种筛选过程被认为是产生具有广泛零-shot泛化能力的高性能模型所必需的。然而，随着考虑需要预先训练数万亿个标记的更大模型，筛选的可扩展性是否会出现瓶颈以及我们是否会很快用尽独特的高质量数据仍然不清楚。与以往的想法相反，我们展示了经过适当筛选和去重的网络数据可以导致功能强大的模型；甚至明显优于训练在The Pile 上的最先进的模型。尽管经过了广泛的过滤，我们从网络中提取的高质量数据仍然很丰富，我们能够从CommonCrawl中获得五万亿个标记。我们公开发布了从我们的RefinedWeb数据集中提取的6000亿个标记的片段，以及在此数据集上训练的1.3 / 7.5B参数语言模型。

    Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.
    
[^54]: 基于机器学习的焦虑检测中噪声影响的比较研究。

    Comparative Study on the Effects of Noise in ML-Based Anxiety Detection. (arXiv:2306.01110v1 [cs.LG])

    [http://arxiv.org/abs/2306.01110](http://arxiv.org/abs/2306.01110)

    本研究探究了噪声如何影响基于机器学习的焦虑检测模型，并开发出在嘈杂的现实环境中具有抗干扰性和适应性的模型，以推进该领域的发展。

    

    穿戴式健康设备正在引领一种新时代的连续和非侵入性远程监测。其中一项应用是用于焦虑检测。许多焦虑检测方面的进展发生在受控实验室环境中，但噪声阻止了这些进展推广到现实世界的条件下。本研究旨在研究噪声如何影响模型性能并开发对嘈杂的现实环境具有抗干扰性和适应日常生活中混乱的模型，从而推进该领域的发展。我们尝试研究先前的方法为何失败，并使用可穿戴负荷与情感检测（WESAD）数据集，在三类分类问题（基准值 vs. 压力 vs. 愉悦）中比较不同强度噪声对机器学习模型分类生理唤醒等级的影响。在引入噪声之前，我们基准模型的性能达到了98.7％，而Schmidt 2018年的模型仅达到了80.3％。我们讨论了可能的解决方法。

    Wearable health devices are ushering in a new age of continuous and noninvasive remote monitoring. One application of this technology is in anxiety detection. Many advancements in anxiety detection have happened in controlled lab settings, but noise prevents these advancements from generalizing to real-world conditions. We seek to progress the field by studying how noise impacts model performance and developing models that are robust to noisy, real-world conditions and, hence, attuned to the commotion of everyday life. In this study we look to investigate why and how previous methods have failed. Using the wearable stress and affect detection (WESAD) dataset, we compare the effect of various intensities of noise on machine learning models classifying levels of physiological arousal in the three-class classification problem: baseline vs. stress vs. amusement. Before introducing noise, our baseline model performance reaches 98.7%, compared to Schmidt 2018's 80.3%. We discuss potential so
    
[^55]: 在图形的超出分布泛化中学习标签和环境因果独立性

    Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization. (arXiv:2306.01103v1 [cs.LG])

    [http://arxiv.org/abs/2306.01103](http://arxiv.org/abs/2306.01103)

    本文提出了一种考虑标签和环境因果独立性的方法来解决图形超出分布（OOD）泛化问题，通过敌对训练策略来联合优化属性以获得有效结果，实验证明LECI显着优于之前的方法。

    

    我们解决了图形的超出分布（OOD）泛化问题。现有的图形OOD算法要么依赖于受限的假设，要么无法利用训练数据中的环境信息。在这项工作中，我们提出同时纳入标签和环境因果独立（LECI），充分利用标签和环境信息，从而解决之前的方法在识别因果和不变子图时面临的挑战。我们进一步开发了一种敌对训练策略，以联合优化这两个属性，用于具有理论保证的导致子图发现。广泛的实验和分析表明，LECI在合成和真实数据集上都显着优于之前的方法，将LECI确立为图形OOD泛化的实用有效解决方案。

    We tackle the problem of graph out-of-distribution (OOD) generalization. Existing graph OOD algorithms either rely on restricted assumptions or fail to exploit environment information in training data. In this work, we propose to simultaneously incorporate label and environment causal independence (LECI) to fully make use of label and environment information, thereby addressing the challenges faced by prior methods on identifying causal and invariant subgraphs. We further develop an adversarial training strategy to jointly optimize these two properties for casual subgraph discovery with theoretical guarantees. Extensive experiments and analysis show that LECI significantly outperforms prior methods on both synthetic and real-world datasets, establishing LECI as a practical and effective solution for graph OOD generalization.
    
[^56]: LLMatic: 基于大语言模型和多样性优化的神经结构搜索

    LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])

    [http://arxiv.org/abs/2306.01102](http://arxiv.org/abs/2306.01102)

    本文介绍了利用大语言模型和多样性优化算法相结合的 LLMatic 神经结构搜索算法。该算法在CIFAR-10数据集进行测试，仅进行2000次搜索即可产生高性能网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。

    

    大型语言模型 (LLMs) 已成为一种强大的工具，可以完成广泛的任务。它们的能力涵盖了许多领域，它们在代码生成领域产生了重大影响。在此情况下，我们将 LLMs 视为变异和交叉工具。同时，多样性优化算法已知可以发现多样性和稳健的解决方案。通过将 LLMs 的代码生成能力与 QD 解决方案的多样性和鲁棒性相结合，我们引入了 LLMatic，一个神经结构搜索 (NAS) 算法。虽然 LLMs 通过提示直接进行 NAS 考验困难，但 LLMatic 利用程序化方法，利用 QD 来进行提示和网络结构，从而创建多样性和高性能网络。我们在 CIFAR-10 图像分类基准测试中测试了 LLMatic，证明它可以在仅进行 2000 次搜索的情况下产生具有竞争力的网络，即使没有该基准领域的先前知识或任何先前的最佳结果的曝光。

    Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
    
[^57]: 不容易出错的快速矩阵乘法:基于约束编程的解法

    Fast Matrix Multiplication Without Tears: A Constraint Programming Approach. (arXiv:2306.01097v1 [cs.AI])

    [http://arxiv.org/abs/2306.01097](http://arxiv.org/abs/2306.01097)

    本文提出了一种基于约束编程的方法，以寻找快速矩阵乘法的非交换算法或提供不可行性证明。我们通过打破对称性的约束条件和有效的不等式约束来修剪搜索空间，可以在合理的时间内找到已知的最佳算法或改进的算法。

    

    已知将一个 $N \times M$ 矩阵与一个 $M \times P$ 的矩阵相乘时，可以使用少于朴素 $NMP$ 方法建议的乘法次数。其中最著名的是 Strassen 算法，可将两个 $2\times 2$ 的矩阵相乘，只需 7 次而非 8 次乘法。这引出了快速矩阵乘法的约束满足问题，其中必须选择一组 $R <NMP$ 乘法项，并将它们组合以满足输出矩阵的正确性约束。本文提出了一种简单而新颖的基于约束编程的方法，以寻找快速矩阵乘法的非交换算法或提供不可行性证明。我们提出了一组打破对称性的约束条件和有效的不等式约束来修剪搜索空间，并减少要检查的解决方案数量。我们的实验表明，在合理的时间内，我们的方法可以找到已知的最佳算法或改进的算法，适用于测试的所有矩阵大小，最大为 $768 \times 768$。

    It is known that the multiplication of an $N \times M$ matrix with an $M \times P$ matrix can be performed using fewer multiplications than what the naive $NMP$ approach suggests. The most famous instance of this is Strassen's algorithm for multiplying two $2\times 2$ matrices in 7 instead of 8 multiplications. This gives rise to the constraint satisfaction problem of fast matrix multiplication, where a set of $R < NMP$ multiplication terms must be chosen and combined such that they satisfy correctness constraints on the output matrix. Despite its highly combinatorial nature, this problem has not been exhaustively examined from that perspective, as evidenced for example by the recent deep reinforcement learning approach of AlphaTensor. In this work, we propose a simple yet novel Constraint Programming approach to find non-commutative algorithms for fast matrix multiplication or provide proof of infeasibility otherwise. We propose a set of symmetry-breaking constraints and valid inequal
    
[^58]: 大批量神经多目标贝叶斯优化

    Large-Batch, Neural Multi-Objective Bayesian Optimization. (arXiv:2306.01095v1 [cs.LG])

    [http://arxiv.org/abs/2306.01095](http://arxiv.org/abs/2306.01095)

    本文提出了一种针对数据密集型问题和多目标优化设置的贝叶斯优化框架，该方法利用了贝叶斯神经网络代理建模和可扩展、具有不确定性的收购策略，能够在最少迭代次数的情况下高效地进行优化。

    

    贝叶斯优化在全局优化黑盒高成本函数方面提供了强大的框架。然而，由于默认高斯过程代理的可扩展性差，它在处理数据密集型问题，特别是在多目标设置中的能力有限。本文提出了一种新颖的贝叶斯优化框架，专为解决这些限制而设计。我们的方法利用了贝叶斯神经网络方法进行代理建模。这使得它能够有效地处理大批量数据，建模复杂问题以及产生预测的不确定性。此外，我们的方法结合了一种基于众所周知且易于部署的NSGA-II的可扩展的、具有不确定性的收购策略。这种完全可并行化的策略促进了未勘探区域的有效探索。我们的框架允许在最少迭代次数的情况下在数据密集环境中进行有效的优化。我们展示了我们方法的优越性。

    Bayesian optimization provides a powerful framework for global optimization of black-box, expensive-to-evaluate functions. However, it has a limited capacity in handling data-intensive problems, especially in multi-objective settings, due to the poor scalability of default Gaussian Process surrogates. We present a novel Bayesian optimization framework specifically tailored to address these limitations. Our method leverages a Bayesian neural networks approach for surrogate modeling. This enables efficient handling of large batches of data, modeling complex problems, and generating the uncertainty of the predictions. In addition, our method incorporates a scalable, uncertainty-aware acquisition strategy based on the well-known, easy-to-deploy NSGA-II. This fully parallelizable strategy promotes efficient exploration of uncharted regions. Our framework allows for effective optimization in data-intensive environments with a minimum number of iterations. We demonstrate the superiority of ou
    
[^59]: UCAS-IIE-NLP在SemEval-2023任务12中的应用：增强低资源情感分析的多语言BERT泛化能力

    UCAS-IIE-NLP at SemEval-2023 Task 12: Enhancing Generalization of Multilingual BERT for Low-resource Sentiment Analysis. (arXiv:2306.01093v1 [cs.CL])

    [http://arxiv.org/abs/2306.01093](http://arxiv.org/abs/2306.01093)

    本文介绍了 UCAS-IIE-NLP 设计的 SACL-XLMR 系统，它利用词典式多语言BERT进行情感感知表示学习，使用监督性对抗式对比学习技术进行情感传播结构化表示学习，用于低资源情感分析，且在多语言和零样本情感分类子任务中表现出色，并且在零样本分类子任务中获得了第一名。

    

    本文描述了我们为SemEval-2023任务12设计的系统：非洲语言的情感分析。该任务面临的挑战是在低资源环境下标注数据和语言资源的稀缺性。为了缓解这些问题，我们提出了一个名为SACL-XLMR的通用多语言系统，用于低资源语言的情感分析。具体而言，我们设计了一个基于词典的多语言BERT，以便进行语言适应和情感感知表示学习。此外，我们采用一个监督性的对抗式对比学习技术来学习情感传播结构化表示并增强模型的泛化能力。我们的系统在多语言和零样本情感分类子任务中表现出色，远远超过基线。值得注意的是，在官方排名中，该系统在零样本分类子任务中获得了第一名。广泛的实验表明了我们系统的有效性。

    This paper describes our system designed for SemEval-2023 Task 12: Sentiment analysis for African languages. The challenge faced by this task is the scarcity of labeled data and linguistic resources in low-resource settings. To alleviate these, we propose a generalized multilingual system SACL-XLMR for sentiment analysis on low-resource languages. Specifically, we design a lexicon-based multilingual BERT to facilitate language adaptation and sentiment-aware representation learning. Besides, we apply a supervised adversarial contrastive learning technique to learn sentiment-spread structured representations and enhance model generalization. Our system achieved competitive results, largely outperforming baselines on both multilingual and zero-shot sentiment classification subtasks. Notably, the system obtained the 1st rank on the zero-shot classification subtask in the official ranking. Extensive experiments demonstrate the effectiveness of our system.
    
[^60]: 4DSR-GCN: 使用图卷积网络的4D视频点云上采样

    4DSR-GCN: 4D Video Point Cloud Upsampling using Graph Convolutional Networks. (arXiv:2306.01081v1 [cs.CV])

    [http://arxiv.org/abs/2306.01081](http://arxiv.org/abs/2306.01081)

    本文提出使用图卷积网络的4D视频点云上采样方法，并展示了其在质量和定量上皆优于现有技术。

    

    三维点云的时间序列，或称为4D点云，现在在多个应用中以日益增长的速度被获取（例如，自主驾驶或辅助驾驶中的激光雷达）。在许多情况下，需要传输大量数据，因此需要应用适当的压缩工具来降低分辨率或带宽。本文提出了一种新的解决方案，用于对被大量压缩后的时间变化的3D视频点云进行升采样和恢复。我们的模型由一个专门设计的图卷积网络组成，该网络结合了动态边缘卷积和图注意力网络进行特征聚合，在生成对抗设置中进行。我们展示了一种不依赖于传统体素化操作的密集点云抽样方法，来自PointNet ++ 的启发。我们的实验表明，所提出的方法在4D视频点云上采样方面优于现有的最先进技术，无论是从定量还是定性上都取得了更好的结果。

    Time varying sequences of 3D point clouds, or 4D point clouds, are now being acquired at an increasing pace in several applications (e.g., LiDAR in autonomous or assisted driving). In many cases, such volume of data is transmitted, thus requiring that proper compression tools are applied to either reduce the resolution or the bandwidth. In this paper, we propose a new solution for upscaling and restoration of time-varying 3D video point clouds after they have been heavily compressed. In consideration of recent growing relevance of 3D applications, %We focused on a model allowing user-side upscaling and artifact removal for 3D video point clouds, a real-time stream of which would require . Our model consists of a specifically designed Graph Convolutional Network (GCN) that combines Dynamic Edge Convolution and Graph Attention Networks for feature aggregation in a Generative Adversarial setting. By taking inspiration PointNet++, We present a different way to sample dense point clouds wit
    
[^61]: 量化感知和张量压缩训练：用于自然语言理解的Transformer

    Quantization-Aware and Tensor-Compressed Training of Transformers for Natural Language Understanding. (arXiv:2306.01076v1 [cs.CL])

    [http://arxiv.org/abs/2306.01076](http://arxiv.org/abs/2306.01076)

    本研究提出了一种量化感知张量压缩训练方法，用于减少基于Transformer的模型的模型大小，算术运算和最终运行时延。该方法经过逐层蒸馏后在文本蕴含和情感分析任务中表现出良好性能。

    

    细调的Transformer模型在许多自然语言任务中表现出优越性能。然而，庞大的模型大小阻止了在资源受限设备上部署高性能Transformer模型。本文提出了一种量化感知张量压缩训练方法，以减少基于Transformer的模型的模型大小，算术运算和最终运行时延。我们将Transformer的嵌入和线性层压缩为小型低秩张量核，从而显著减少模型参数。采用可学习比例因子的量化感知训练来进一步获得张量压缩模型的低精度表示。该方法可用于端到端训练和蒸馏训练。为了提高收敛性，采用逐层蒸馏方法从预训练Transformer中提取出一个经过量化和张量压缩的学生模型。本研究在两个自然语言任务，即文本蕴含和情感分析中展示了其性能。

    Fine-tuned transformer models have shown superior performances in many natural language tasks. However, the large model size prohibits deploying high-performance transformer models on resource-constrained devices. This paper proposes a quantization-aware tensor-compressed training approach to reduce the model size, arithmetic operations, and ultimately runtime latency of transformer-based models. We compress the embedding and linear layers of transformers into small low-rank tensor cores, which significantly reduces model parameters. A quantization-aware training with learnable scale factors is used to further obtain low-precision representations of the tensor-compressed models. The developed approach can be used for both end-to-end training and distillation-based training. To improve the convergence, a layer-by-layer distillation is applied to distill a quantized and tensor-compressed student model from a pre-trained transformer. The performance is demonstrated in two natural language
    
[^62]: TimelineQA: 一个用于时间线问答的基准测试

    TimelineQA: A Benchmark for Question Answering over Timelines. (arXiv:2306.01069v1 [cs.CL])

    [http://arxiv.org/abs/2306.01069](http://arxiv.org/abs/2306.01069)

    TimelineQA是一个用于查询生活日志的加速进展的基准测试，涉及时间和地理信息，已经公开发布，并使用两种最先进的模型进行了实验，但这两种模型均未达到人类表现水平。

    

    Lifelogs（生活日志）是人们生活经历的描述，这些日志通过结合来自多个数字服务（如在线照片、地图、购物和内容流媒体服务）的数据来创建。问答技术在生活日志上的应用可以为个人助手在提供上下文方面提供关键资源。但是，由于生活日志结合了自由文本与一定程度的结构，如时间和地理信息，因此当前的问答技术无法回答此类问题。为此，我们创建并公开发布了TimelineQA，这是一个用于查询生活日志的加速进展的基准测试。TimelineQA生成虚构人物的生活日志。生活日志中的事件从高中毕业等重大生活事件到日常活动如慢跑都有所覆盖。我们进行了一系列关于TimelineQA的实验，使用了两种最先进的模型，发现这两种模型在这一任务上均未达到人类表现水平。

    Lifelogs are descriptions of experiences that a person had during their life. Lifelogs are created by fusing data from the multitude of digital services, such as online photos, maps, shopping and content streaming services. Question answering over lifelogs can offer personal assistants a critical resource when they try to provide advice in context. However, obtaining answers to questions over lifelogs is beyond the current state of the art of question answering techniques for a variety of reasons, the most pronounced of which is that lifelogs combine free text with some degree of structure such as temporal and geographical information.  We create and publicly release TimelineQA1, a benchmark for accelerating progress on querying lifelogs. TimelineQA generates lifelogs of imaginary people. The episodes in the lifelog range from major life episodes such as high school graduation to those that occur on a daily basis such as going for a run. We describe a set of experiments on TimelineQA w
    
[^63]: 使用Tsetlin机器的在线学习的FPGA架构

    An FPGA Architecture for Online Learning using the Tsetlin Machine. (arXiv:2306.01027v1 [cs.LG])

    [http://arxiv.org/abs/2306.01027](http://arxiv.org/abs/2306.01027)

    本文提出了一种基于FPGA架构的在线学习系统，通过采用低复杂度的Tsetlin Machine机器学习算法，在运行时提供了离线和在线学习管理。这种架构能够按需进行训练，并且能够在操作期间交错地进行推理和训练。

    

    针对机器学习模型需要在无监督情况下不断演化的需求，本文提出了一种基于FPGA架构的在线学习系统，采用低复杂度的Tsetlin Machine机器学习算法。该架构提供了定制化的运行时学习管理，实现了芯片内部的离线和在线学习，能够在推理之前通过预分类数据在FPGA上进行按需训练，并且能够在操作期间交错地进行推理和训练。

    There is a need for machine learning models to evolve in unsupervised circumstances. New classifications may be introduced, unexpected faults may occur, or the initial dataset may be small compared to the data-points presented to the system during normal operation. Implementing such a system using neural networks involves significant mathematical complexity, which is a major issue in power-critical edge applications.  This paper proposes a novel field-programmable gate-array infrastructure for online learning, implementing a low-complexity machine learning algorithm called the Tsetlin Machine. This infrastructure features a custom-designed architecture for run-time learning management, providing on-chip offline and online learning. Using this architecture, training can be carried out on-demand on the \ac{FPGA} with pre-classified data before inference takes place. Additionally, our architecture provisions online learning, where training can be interleaved with inference during operatio
    
[^64]: 基于虚拟力的平衡圆形装箱问题的群智能算法

    A Vitual-Force Based Swarm Algorithm for Balanced Circular Bin Packing Problems. (arXiv:2306.01021v1 [cs.AI])

    [http://arxiv.org/abs/2306.01021](http://arxiv.org/abs/2306.01021)

    本文描述了一种基于虚拟力系统的群智能算法，用以解决平衡圆形装箱问题，并在各种基准测试中得到验证，具有很好的解决效果。

    

    平衡圆形装箱问题涉及将给定数量的加权圆放置在圆形容器中，以最小化半径并满足平衡约束条件。本文描述了一种基于虚拟力系统的群智能算法，以解决平衡圆形装箱问题。在提出的方法中，对每个组件施加一组力，以考虑约束条件并使用动力学的基本原理最小化目标函数。提出的算法在各种平衡圆形装箱问题的基准测试中得到验证，并验证了具有高达300个圆的问题。报告的结果允许评估所提出的方法与文献中现有结果之间的有效性。

    Balanced circular bin packing problems consist in positioning a given number of weighted circles in order to minimize the radius of a circular container while satisfying equilibrium constraints. These problems are NP-hard, highly constrained and dimensional. This paper describes a swarm algorithm based on a virtual-force system in order to solve balanced circular bin packing problems. In the proposed approach, a system of forces is applied to each component allowing to take into account the constraints and minimizing the objective function using the fundamental principle of dynamics. The proposed algorithm is experimented and validated on benchmarks of various balanced circular bin packing problems with up to 300 circles. The reported results allow to assess the effectiveness of the proposed approach compared to existing results from the literature.
    
[^65]: PV2TEA：将视觉模态与基于文本的信息抽取相结合

    PV2TEA: Patching Visual Modality to Textual-Established Information Extraction. (arXiv:2306.01016v1 [cs.CL])

    [http://arxiv.org/abs/2306.01016](http://arxiv.org/abs/2306.01016)

    PV2TEA提出了一种基于编码器-解码器架构的信息抽取模型，在多模态注释困难的情况下解决了跨模态集成的问题，并提出了三种偏差降低方案。

    

    信息抽取（例如属性值提取）已被广泛研究和建模，但仅基于文本。然而，许多属性可以从基于图像的提取中受益，如颜色、形状、图案等。视觉模态长期以来一直未被充分利用，主要是由于多模态注释的难度。本文旨在将视觉模态与基于文本的属性信息提取器相结合。跨模态集成面临几个独特的挑战：（C1）图像和文本描述在样本内和样本间松散匹配；（C2）图像通常包含丰富的背景，可能会误导预测；（C3）来自基于文本的提取器的弱监督标签对于多模态训练存在偏差。我们提出了PV2TEA，这是一种编码器-解码器架构，配备了三种偏差降低方案：（S1）增强的标签平滑对比，以改进松散匹配的图像和文本的交叉模态对齐; （S2）注意力剪枝方案用于在保留正确信息的同时消除一些不必要的细节；（S3）基于对抗训练的可重组卷积自适应模块，以帮助消除来自文本提取器的偏差。

    Information extraction, e.g., attribute value extraction, has been extensively studied and formulated based only on text. However, many attributes can benefit from image-based extraction, like color, shape, pattern, among others. The visual modality has long been underutilized, mainly due to multimodal annotation difficulty. In this paper, we aim to patch the visual modality to the textual-established attribute information extractor. The cross-modality integration faces several unique challenges: (C1) images and textual descriptions are loosely paired intra-sample and inter-samples; (C2) images usually contain rich backgrounds that can mislead the prediction; (C3) weakly supervised labels from textual-established extractors are biased for multimodal training. We present PV2TEA, an encoder-decoder architecture equipped with three bias reduction schemes: (S1) Augmented label-smoothed contrast to improve the cross-modality alignment for loosely-paired image and text; (S2) Attention-prunin
    
[^66]: 时间演化图的图级嵌入

    Graph-Level Embedding for Time-Evolving Graphs. (arXiv:2306.01012v1 [cs.LG])

    [http://arxiv.org/abs/2306.01012](http://arxiv.org/abs/2306.01012)

    本论文提出了一种针对动态网络的图级嵌入方法，并通过生成节点的时间上下文来训练语言模型，以生成低维度的图级表示，对于下游的图形相似性排序、图形同构和异常检测等任务具有重要的意义。

    

    图表示学习已经得到广泛的研究，从节点到图的粒度各不相同。虽然在节点级别表示方面的大部分工作都已经被研究，但是对于动态或时态网络的图级嵌入却鲜有研究。然而，在动态网络中学习低维度的图级表示对各种下游图检索任务非常重要，例如时间图相似性排序、时间图同构和异常检测。本文提出了一种新颖的方法来解决这个问题，即建立一个多层图并使用具有时间回溯的修改后随机游走生成节点的时间上下文，然后在这些上下文上训练“文档级”语言模型以生成图级嵌入。我们将所提出的模型在五个公开可用的数据集上进行了评估。

    Graph representation learning (also known as network embedding) has been extensively researched with varying levels of granularity, ranging from nodes to graphs. While most prior work in this area focuses on node-level representation, limited research has been conducted on graph-level embedding, particularly for dynamic or temporal networks. However, learning low-dimensional graph-level representations for dynamic networks is critical for various downstream graph retrieval tasks such as temporal graph similarity ranking, temporal graph isomorphism, and anomaly detection. In this paper, we present a novel method for temporal graph-level embedding that addresses this gap. Our approach involves constructing a multilayer graph and using a modified random walk with temporal backtracking to generate temporal contexts for the graph's nodes. We then train a "document-level" language model on these contexts to generate graph-level embeddings. We evaluate our proposed model on five publicly avai
    
[^67]: 探究生成语言模型的演绎推理能力

    Examining the Emergence of Deductive Reasoning in Generative Language Models. (arXiv:2306.01009v1 [cs.CL])

    [http://arxiv.org/abs/2306.01009](http://arxiv.org/abs/2306.01009)

    本研究调查了不同规模的生成语言模型的演绎推理能力，发现随着规模的增加，推理能力增强，长度不会影响大部分模型表现。

    

    我们对生成变压器模型从前提中进行演绎推理的能力进行初步调查。我们观察到不同训练设置的模型性能存在显著差异，并发现演绎推理能力随规模增加而增强。此外，我们发现，除了OpenAI GPT-3和GPT-3.5模型，模型的表现通常不会随着推理链的长度而减弱。本研究考虑了从1.17亿到1750亿个参数的各种变压器解码器模型。

    We conduct a preliminary inquiry into the ability of generative transformer models to deductively reason from premises provided. We observe notable differences in the performance of models coming from different training setups and find that the deductive reasoning ability increases with scale. Further, we discover that the performance generally does not decrease with the length of the deductive chain needed to reach the conclusion, with the exception of OpenAI GPT-3 and GPT-3.5 models. Our study considers a wide variety of transformer-decoder models, ranging from 117 million to 175 billion parameters in size.
    
[^68]: 利用无性繁殖优化算法进行信用卡欺诈检测

    Credit Card Fraud Detection Using Asexual Reproduction Optimization. (arXiv:2306.01008v1 [cs.NE])

    [http://arxiv.org/abs/2306.01008](http://arxiv.org/abs/2306.01008)

    本文介绍了一种利用无性繁殖优化算法进行信用卡欺诈检测的方法，与其他现有方法相比，该方法能够显著降低训练时间，并在重要的欺诈检测问题中增加召回率。

    

    随着信用卡用户数量的增加，在这一领域检测欺诈已成为至关重要的问题。以往的研究应用了各种监督和无监督机器学习方法来寻求有效的欺诈检测系统。然而，一些方法需要大量时间才能达到合理的准确性。本文采用了无性繁殖优化算法（ARO）方法，这是一种用于检测信用卡欺诈的监督方法。通过仅从多数类进行采样，ARO方法能够提高分类的有效性。与当前数据集上实施的最佳方法之一的人工免疫系统（AIS）进行比较，结果表明所提出的方法能够显着降低所需的训练时间，并在重要的欺诈检测问题中增加召回率。实验表明，所提出的ARO方法在时间效率和准确性方面优于其他现有方法。

    As the number of credit card users has increased, detecting fraud in this domain has become a vital issue. Previous literature has applied various supervised and unsupervised machine learning methods to find an effective fraud detection system. However, some of these methods require an enormous amount of time to achieve reasonable accuracy. In this paper, an Asexual Reproduction Optimization (ARO) approach was employed, which is a supervised method to detect credit card fraud. ARO refers to a kind of production in which one parent produces some offspring. By applying this method and sampling just from the majority class, the effectiveness of the classification is increased. A comparison to Artificial Immune Systems (AIS), which is one of the best methods implemented on current datasets, has shown that the proposed method is able to remarkably reduce the required training time and at the same time increase the recall that is important in fraud detection problems. The obtained results sh
    
[^69]: 面向变化环境的公平解缠在线学习

    Towards Fair Disentangled Online Learning for Changing Environments. (arXiv:2306.01007v1 [cs.LG])

    [http://arxiv.org/abs/2306.01007](http://arxiv.org/abs/2306.01007)

    本论文提出了一种面向变化环境的在线学习算法，该算法通过将模型参数划分为环境不变部分和环境特定部分，从而实现了数据公平性。通过大量的实验，证明了该算法的有效性。

    

    在面对变化环境的在线学习问题中，数据按时间顺序一个接一个地接收，并且它们的分布假设可能经常变化。虽然现有方法通过提供对动态遗憾或自适应遗憾的严格界限来展示其学习算法的有效性，但它们大多完全忽略了带有模型公平性的学习，其定义为跨不同子族群（例如，种族和性别）的统计平等。另一个缺点是，在适应新环境时，在线学习者需要使用全局更改更新模型参数，这是昂贵和低效的。受到稀疏机制转移假设的启发，我们声称在线学习中的变化环境可以归因于特定于环境的部分学习参数的部分变化，其余部分保持不变。为此，本文在假设从不同子人群收集的数据具有公平的模型表示的前提下，提出了一种新算法，将模型参数分为环境不变部分和环境特定部分。我们推导了每个子人群模型表示公正性的统计保证，并证明了我们提出的算法的收敛速率。此外，我们通过对合成和真实世界数据集的广泛实验证明了我们方法的有效性。

    In the problem of online learning for changing environments, data are sequentially received one after another over time, and their distribution assumptions may vary frequently. Although existing methods demonstrate the effectiveness of their learning algorithms by providing a tight bound on either dynamic regret or adaptive regret, most of them completely ignore learning with model fairness, defined as the statistical parity across different sub-population (e.g., race and gender). Another drawback is that when adapting to a new environment, an online learner needs to update model parameters with a global change, which is costly and inefficient. Inspired by the sparse mechanism shift hypothesis, we claim that changing environments in online learning can be attributed to partial changes in learned parameters that are specific to environments and the rest remain invariant to changing environments. To this end, in this paper, we propose a novel algorithm under the assumption that data coll
    
[^70]: AbODE：使用联合ODE的从头设计抗体

    AbODE: Ab Initio Antibody Design using Conjoined ODEs. (arXiv:2306.01005v1 [cs.LG])

    [http://arxiv.org/abs/2306.01005](http://arxiv.org/abs/2306.01005)

    该论文提出了一种新的从头设计抗体的方法AbODE，该方法使用连续微分注意力来应对蛋白折叠、逆向折叠和对接等挑战，并与时间网络和图匹配网络具有基本联系。

    

    抗体是一种Y形蛋白，可以中和病原体，并构成我们适应性免疫系统的核心。新的抗体的从头设计，以特定抗原为靶标，是加速疫苗发现的关键。然而，这种氨基酸序列和三维结构的联合设计涵盖并强调了来自多个任务的一些核心挑战，包括蛋白折叠（序列到结构），逆向折叠（结构到序列）和对接（结合）。我们通过一个新的生成模型AbODE来克服这些挑战，该模型扩展了图形PDE以适应上下文信息和外部相互作用。与现有方法不同，AbODE使用一轮完整的解码，并引出连续微分注意力，其中包括抗体内部和抗原相关的潜在相互作用。我们揭示了AbODE与时间网络以及图匹配网络之间的基本联系。

    Antibodies are Y-shaped proteins that neutralize pathogens and constitute the core of our adaptive immune system. De novo generation of new antibodies that target specific antigens holds the key to accelerating vaccine discovery. However, this co-design of the amino acid sequence and the 3D structure subsumes and accentuates some central challenges from multiple tasks, including protein folding (sequence to structure), inverse folding (structure to sequence), and docking (binding). We strive to surmount these challenges with a new generative model AbODE that extends graph PDEs to accommodate both contextual information and external interactions. Unlike existing approaches, AbODE uses a single round of full-shot decoding and elicits continuous differential attention that encapsulates and evolves with latent interactions within the antibody as well as those involving the antigen. We unravel fundamental connections between AbODE and temporal networks as well as graph-matching networks. Th
    
[^71]: AoM：用于多模态基于方面的情感分析中检测面向方面信息

    AoM: Detecting Aspect-oriented Information for Multimodal Aspect-Based Sentiment Analysis. (arXiv:2306.01004v1 [cs.CL])

    [http://arxiv.org/abs/2306.01004](http://arxiv.org/abs/2306.01004)

    本文提出了一个面向方面的方法(AoM)来检测与方面相关的语义和情感信息，并明确地将情感嵌入到AoM中。实验证明AoM在改进多模态基于方面的情感分析(MABSA)方面非常有效。

    

    多模态基于方面的情感分析(MABSA)旨在从文本-图像对中提取方面并识别它们的情感。现有方法致力于将整个图像与相应的方面对齐。然而，图像的不同区域可能与同一句子中的不同方面相关，粗略地建立图像-方面对齐会引入噪声到基于方面的情感分析中(即视觉噪声)。此外，特定方面的情感也可能被其他方面的描述所干扰(即文本噪声)。考虑到上述噪声，本文提出了一种面向方面的方法(AoM)来检测与方面相关的语义和情感信息。具体而言，设计了一个面向方面的注意力模块，同时选择与方面有语义关联的文本令牌和图像块。为了准确地聚合情感信息，我们明确地将情感嵌入到AoM中，并使用图卷积神经网络在不同模态中捕获依赖于方面的情感表达式。两个基准数据集上的实验结果表明了AoM在改进MABSA方面的有效性。

    Multimodal aspect-based sentiment analysis (MABSA) aims to extract aspects from text-image pairs and recognize their sentiments. Existing methods make great efforts to align the whole image to corresponding aspects. However, different regions of the image may relate to different aspects in the same sentence, and coarsely establishing image-aspect alignment will introduce noise to aspect-based sentiment analysis (i.e., visual noise). Besides, the sentiment of a specific aspect can also be interfered by descriptions of other aspects (i.e., textual noise). Considering the aforementioned noises, this paper proposes an Aspect-oriented Method (AoM) to detect aspect-relevant semantic and sentiment information. Specifically, an aspect-aware attention module is designed to simultaneously select textual tokens and image blocks that are semantically related to the aspects. To accurately aggregate sentiment information, we explicitly introduce sentiment embedding into AoM, and use a graph convolut
    
[^72]: DiffLoad:扩散模型中的负荷预测不确定性量化

    DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])

    [http://arxiv.org/abs/2306.01001](http://arxiv.org/abs/2306.01001)

    本文提出了一种扩散模型中的负荷预测不确定性量化方法，采用Seq2Seq网络结构来分离两种类型的不确定性并处理异常情况，不仅着眼于预测条件期望值。

    

    电力负荷预测对电力系统的决策制定，如机组投入和能源管理等具有重要意义。近年来，各种基于自监督神经网络的方法已经被应用于电力负荷预测，以提高预测准确性和捕捉不确定性。然而，大多数现有的方法是基于高斯似然方法的，它旨在在给定的协变量下准确估计分布期望值。这种方法很难适应存在分布偏移和异常值的时间数据。在本文中，我们提出了一种基于扩散的Seq2seq结构来估计本体不确定性，并使用鲁棒的加性柯西分布来估计物象不确定性。我们展示了我们的方法能够分离两种类型的不确定性并处理突变情况，而不是准确预测条件期望。

    Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.
    
[^73]: 截断亲和力最大化：用于图形异常监测的单类同型建模

    Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v1 [cs.SI])

    [http://arxiv.org/abs/2306.00006](http://arxiv.org/abs/2306.00006)

    本文针对图形异常监测数据集中存在的一类同型现象，提出了一种新的无监督异常评分度量——当前节点亲和力，并通过学习量身定制的节点表示，实现了截断亲和力最大化（TAM）方法，优化在原始图形结构上进行，能够有效进行双重One-Class的GAD。

    

    我们在现实世界的图形异常监测（GAD）数据集中经常发现一种普遍的属性......本文提出了一种新的无监督异常评分度量 - 当前节点亲和力......我们进一步提出了截断亲和力最大化 (TAM)，该方法通过最大化与_neighbors的本地亲和力来学习量身定制的节点表示。本文所提方法在原始图形结构上进行优化，可以进行双重One-Class的GAD。

    One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be bi
    
[^74]: 多块双层优化的分块随机方差约简方法及并行加速

    Blockwise Stochastic Variance-Reduced Methods with Parallel Speedup for Multi-Block Bilevel Optimization. (arXiv:2305.18730v1 [math.OC])

    [http://arxiv.org/abs/2305.18730](http://arxiv.org/abs/2305.18730)

    本文提出了两种基于方差约简的优化算法，以实现对多块双层优化问题的高效求解，同时匹配单块标准 BO 问题的最优复杂度、实现并行化加速，以及避免计算高维度的 Hessian 矩阵的逆估计。

    

    本文考虑非凸的多块双层优化问题，并提出了两种基于分块方差约简的优化算法。为了达到算法的三个期望：（a）能匹配单块标准 BO 问题的最优复杂度；（b）实现并行化加速，每个迭代中采样 $I$ 块并对每个采样块采样 $B$ 个样本；（c）避免计算高维度的 Hessian 矩阵的逆估计。本文旨在解决这些问题，并探讨了现有算法的关联性以及不足之处。

    In this paper, we consider non-convex multi-block bilevel optimization (MBBO) problems, which involve $m\gg 1$ lower level problems and have important applications in machine learning. Designing a stochastic gradient and controlling its variance is more intricate due to the hierarchical sampling of blocks and data and the unique challenge of estimating hyper-gradient. We aim to achieve three nice properties for our algorithm: (a) matching the state-of-the-art complexity of standard BO problems with a single block; (b) achieving parallel speedup by sampling $I$ blocks and sampling $B$ samples for each sampled block per-iteration; (c) avoiding the computation of the inverse of a high-dimensional Hessian matrix estimator. However, it is non-trivial to achieve all of these by observing that existing works only achieve one or two of these properties. To address the involved challenges for achieving (a, b, c), we propose two stochastic algorithms by using advanced blockwise variance-reductio
    
[^75]: 信仰与命运：Transformer在组合性方面的局限性。

    Faith and Fate: Limits of Transformers on Compositionality. (arXiv:2305.18654v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18654](http://arxiv.org/abs/2305.18654)

    研究了Transformer模型在三个代表性组合型任务中的表现，发现其通过线性子图匹配解决多步组合推理问题。

    

    Transformer大型语言模型在需要复杂多步推理的任务上表现卓越，但同时在一些简单问题上也会出现失败。这引发了疑问：这些错误是偶然的，还是它们表明了更实质性的限制？为了揭示Transformer的神秘面纱，我们研究了这些模型在三个代表性的组合型任务中的极限 - 多位数乘法、逻辑网格谜题和一个经典的动态规划问题。 这些任务需要将问题分解为子步骤，并将这些步骤综合成精确的答案。我们将组合型任务转化为计算图，以系统地量化其复杂性，并将推理步骤分解为中间子程序。我们的实证结果表明，Transformer通过将多步组合推理转化为线性子图匹配来解决组合型任务。

    Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, wi
    
[^76]: 多尺度正负样本检测AI生成文本

    Multiscale Positive-Unlabeled Detection of AI-Generated Texts. (arXiv:2305.18149v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18149](http://arxiv.org/abs/2305.18149)

    本文提出了一种多尺度正负样本的训练框架，以解决多尺度AI生成文本的检测问题。通过将短机器文本标记为“未标记”来重新表述文本分类问题，并提出了一个规则化损失函数来优化检测性能，有效性能显著优于现有的方法。

    

    最近发布的大型语言模型（LLM）如ChatGPT等在生成类似于人类的文本方面令人惊讶，但它们可能被用于制造虚假的学术文本、虚假新闻、虚假推特等。先前的作品提出了检测这些多尺度AI生成文本的方法，包括简单的ML分类器、基于预训练模型的训练不可知方法和精调的语言分类模型。然而，主流检测器在构建时没有考虑到文本长度的因素：短文本的缺乏信息特征，使其更难检测。针对多尺度文本检测的挑战，本文提出了一个多尺度正负样本（MPU）训练框架。首先，我们承认短的机器文本具有类人属性，并将文本分类重新表述为一个正负样本问题，即通过在训练期间标记这些短的机器文本为"unlabeled"来解决这个问题。在这个正负样本的背景下，我们提出了

    Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may get misused for fake scholarly texts, fake news, fake tweets, et cetera. Previous works have proposed methods to detect these multiscale AI-generated texts, including simple ML classifiers, pretrained-model-based training-agnostic methods, and finetuned language classification models. However, mainstream detectors are formulated without considering the factor of corpus length: shorter corpuses are harder to detect compared with longer ones for shortage of informative features. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the challenge of multiscale text detection. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase text classification as a Positive-Unlabeled (PU) problem by marking these short machine texts as "unlabeled" during training. In this PU context, we propose the le
    
[^77]: 基于多模态码本的文本图片翻译探索

    Exploring Better Text Image Translation with Multimodal Codebook. (arXiv:2305.17415v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17415](http://arxiv.org/abs/2305.17415)

    该研究提出了一个基于多模态码本的文本图片翻译模型。通过构建一个多阶段训练框架，利用了额外的双语文本和光学字符识别数据集，该模型能够将图像与相关文本关联起来，提供有用的补充信息，取得了比目前最先进的方法更好的效果。

    

    文本图片翻译是将图像中嵌入的原始文本翻译成目标语言的任务。该任务有着广泛的应用，并具有重要的研究价值。然而，目前的文本图片翻译研究面临两个主要瓶颈：1）缺少公开的文本图片翻译数据集；2）主流模型采用级联模式构建，容易受到光字符识别错误传播的影响。本文首先注释了一个名为OCRMT30K的中英文文本图片翻译数据集，为后续研究提供了便利。然后，我们提出了一种多模态码本的文本图片翻译模型，能够将图像与相关文本关联起来，提供有用的补充信息进行翻译。此外，我们提出了一个多阶段训练框架，包括文本机器翻译、图像文本对齐和文本图片翻译任务，充分利用了额外的双语文本、光字符识别数据集和OCRMT30K数据集来训练我们的模型。扩展实验表明，我们的模型优于最先进的方法，并达到了竞争性的表现。

    Text image translation (TIT) aims to translate the source texts embedded in the image to target translations, which has a wide range of applications and thus has important research value. However, current studies on TIT are confronted with two main bottlenecks: 1) this task lacks a publicly available TIT dataset, 2) dominant models are constructed in a cascaded manner, which tends to suffer from the error propagation of optical character recognition (OCR). In this work, we first annotate a Chinese-English TIT dataset named OCRMT30K, providing convenience for subsequent studies. Then, we propose a TIT model with a multimodal codebook, which is able to associate the image with relevant texts, providing useful supplementary information for translation. Moreover, we present a multi-stage training framework involving text machine translation, image-text alignment, and TIT tasks, which fully exploits additional bilingual texts, OCR dataset and our OCRMT30K dataset to train our model. Extensi
    
[^78]: 长文本的神经自然语言处理：现状综述

    Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art. (arXiv:2305.16259v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16259](http://arxiv.org/abs/2305.16259)

    本文简要概述了长文本的神经自然语言处理的现状，主要包括文档分类和摘要，涵盖了情感分析，同时还探讨了长文本NLP的主要挑战、问题和解决方案。

    

    在过去的十年中，深度神经网络（DNN）的采用极大地促进了自然语言处理（NLP）的发展。然而，长文本分析的需求与短文本有很大不同，而网络上传输的文档大小不断增加，使长文本的自动理解成为一项关键的研究领域。本文的两个目标是：a）概述相关的神经构建模块，作为短教程；b）总结长文本NLP的现状，主要关注两个核心任务：文档分类和文档摘要。情感分析也涵盖在内，因为它通常被视为文档分类的特例。此外，本文还讨论了长文本NLP相关的主要挑战、问题和解决方案。最后，介绍了相关的公开的注释数据集，以便促进进一步研究。

    The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded on-line renders automated understanding of long texts a critical area of research. This article has two goals: a) it overviews the relevant neural building blocks, thus serving as a short tutorial, and b) it surveys the state-of-the-art in long document NLP, mainly focusing on two central tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Additionally, this article discusses the main challenges, issues and current solutions related to long document NLP. Finally, the relevant, publicly available, annotated datasets are presented, in order to facilitate further research.
    
[^79]: 真实回答的语言特性研究

    Linguistic Properties of Truthful Response. (arXiv:2305.15875v1 [cs.CL])

    [http://arxiv.org/abs/2305.15875](http://arxiv.org/abs/2305.15875)

    该论文研究了LLM的不真实回答现象，发现GPT-3模型对给定提示的回答在语言特性上很相似。同时，该论文证明了在没有评估内容本身的情况下，仅依赖模型回答的风格成分即可分类真实性。

    

    我们使用220个手工制作的语言特性对LLM不真实回答的现象进行了研究。我们专注于GPT-3模型，并发现不同大小的LLM对给定提示的回答在语言特性上很相似。我们通过训练只依赖模型回答的风格成分来分类陈述真实性的支持向量机扩展了这一发现。虽然数据集大小限制了我们的当前研究成果，但我们提供了有希望的证据，表明可以在不评估内容本身的情况下检测真实性。

    We investigate the phenomenon of an LLM's untruthful response using a large set of 220 handcrafted linguistic features. We focus on GPT-3 models and find that the linguistic profiles of responses are similar across model sizes. That is, how varying-sized LLMs respond to given prompts stays similar on the linguistic properties level. We expand upon this finding by training support vector machines that rely only upon the stylistic components of model responses to classify the truthfulness of statements. Though the dataset size limits our current findings, we present promising evidence that truthfulness detection is possible without evaluating the content itself.
    
[^80]: 多模式联邦人类活动识别中的隐私问题

    Privacy in Multimodal Federated Human Activity Recognition. (arXiv:2305.12134v1 [cs.LG])

    [http://arxiv.org/abs/2305.12134](http://arxiv.org/abs/2305.12134)

    本文研究了多模式联邦人类活动识别中的隐私问题。通过一个特定的系统，联邦学习可以提供更好的隐私保护，同时不会损失人类活动识别的准确性。

    

    人类活动识别（HAR）的训练数据往往包含隐私信息或由不合作实体持有。联邦学习（FL）通过在边缘设备上训练机器学习模型来解决这些问题。本文研究了在用户、环境和传感器级别上隐私对联邦HAR的影响。我们表明，FL对HAR的性能取决于FL系统的隐私保护程度，并且主要取决于来自不同传感器的数据的配置。尽管避免数据共享并在人类或环境级别上假设隐私，如之前的工作所做的那样，精度会降低5-7％。然而，将这种隐私延伸到模态级别并严格分离多个客户端之间的传感器数据可能会导致精度降低19-42％。由于这种形式的隐私是HAR中被要求的道德利用被动传感方法所必需的，因此我们实现了一种系统，在该系统中客户端相互训练一个通用的FL模型和一个每种模态一个的组级模型。我们的评估表明，这种方法可以在不牺牲HAR准确性的情况下提高隐私保护。

    Human Activity Recognition (HAR) training data is often privacy-sensitive or held by non-cooperative entities. Federated Learning (FL) addresses such concerns by training ML models on edge clients. This work studies the impact of privacy in federated HAR at a user, environment, and sensor level. We show that the performance of FL for HAR depends on the assumed privacy level of the FL system and primarily upon the colocation of data from different sensors. By avoiding data sharing and assuming privacy at the human or environment level, as prior works have done, the accuracy decreases by 5-7%. However, extending this to the modality level and strictly separating sensor data between multiple clients may decrease the accuracy by 19-42%. As this form of privacy is necessary for the ethical utilisation of passive sensing methods in HAR, we implement a system where clients mutually train both a general FL model and a group-level one per modality. Our evaluation shows that this method leads to
    
[^81]: 可视化问答：最近文献中技术和常见趋势综述

    Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature. (arXiv:2305.11033v1 [cs.CV])

    [http://arxiv.org/abs/2305.11033](http://arxiv.org/abs/2305.11033)

    本文综述了最近文献中的可视化问答问题，提供了对该领域的深入分析和比较，包括结果、最新技术、常见错误以及未来研究的潜在改进点。

    

    可视化问答（VQA）是自然语言处理和图像预测中的一个新兴问题，需要算法回答有关特定图像的问题。在本文中，作者分析了25项最新研究和6个数据集，并提供了下载链接。作者深入调研了该领域的多项研究，并提供了分析比较，包括结果、最新技术、常见错误以及未来研究的潜在改进点。

    Visual Question Answering (VQA) is an emerging area of interest for researches, being a recent problem in natural language processing and image prediction. In this area, an algorithm needs to answer questions about certain images. As of the writing of this survey, 25 recent studies were analyzed. Besides, 6 datasets were analyzed and provided their link to download. In this work, several recent pieces of research in this area were investigated and a deeper analysis and comparison among them were provided, including results, the state-of-the-art, common errors, and possible points of improvement for future researchers.
    
[^82]: 零样本多语言神经机器翻译中的“离谱问题”

    On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation. (arXiv:2305.10930v1 [cs.CL])

    [http://arxiv.org/abs/2305.10930](http://arxiv.org/abs/2305.10930)

    零样本多语言神经机器翻译容易出现“离谱问题”，本文提出的简单且有效的算法LAVS可以通过增加语言之间的KL分歧显著降低这个问题。

    

    尽管多语言神经机器翻译取得了巨大成功，但它仍然存在“离谱问题”，即将翻译输出到错误的语言中。这个问题在零样本翻译任务中更加明显。本文发现，当编码目标语言信号时失效，会导致离谱问题，并且两种语言词汇之间更接近的词汇距离（即KL分歧）与更高的离谱率有关。此外，本文还发现，仅隔离解码器中不同语言的词汇可以缓解这个问题。基于这些发现，我们提出了一种简单有效的算法Language Aware Vocabulary Sharing (LAVS)来构建多语言词汇表，通过增加语言之间的KL分歧，大大减轻了翻译模型的离谱问题。我们在11种语言的多语言翻译基准测试上进行了实验。实验结果表明，对于90个翻译任务，采用LAVS的离谱率降低了37％至90％。

    While multilingual neural machine translation has achieved great success, it suffers from the off-target issue, where the translation is in the wrong language. This problem is more pronounced on zero-shot translation tasks. In this work, we find that failing in encoding discriminative target language signal will lead to off-target and a closer lexical distance (i.e., KL-divergence) between two languages' vocabularies is related with a higher off-target rate. We also find that solely isolating the vocab of different languages in the decoder can alleviate the problem. Motivated by the findings, we propose Language Aware Vocabulary Sharing (LAVS), a simple and effective algorithm to construct the multilingual vocabulary, that greatly alleviates the off-target problem of the translation model by increasing the KL-divergence between languages. We conduct experiments on a multilingual machine translation benchmark in 11 languages. Experiments show that the off-target rate for 90 translation 
    
[^83]: “我全然成为我自己”：以TGNB人群为中心，评估开放式语言生成中的偏见

    "I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])

    [http://arxiv.org/abs/2305.09941](http://arxiv.org/abs/2305.09941)

    本论文研究了如何以TGNB人群的声音为中心，评估开放式语言生成中的偏见。通过理解TGNB个体的经历，提出了以TGNB人群为中心的OLG系统评估框架，并且包括一个为TGNB人群设计的调查工具和分析方法。

    

    跨性别和非二元（TGNB）人群在日常生活中经历了不成比例的歧视和排斥。随着语言生成技术的日益普及和应用，进一步边缘化这一人群的可能性也在增加。虽然大量的NLP公平文献着重于阐明和解决性别偏见，但评估TGNB身份所带来的性别伤害需要理解这些身份如何独特地与社会性别规范互动以及与性别二元中心的视角相区分。这样的测量框架本质上需要以TGNB声音为中心，帮助指导包容性别的自然语言处理应该为谁服务。为实现这一目标，我们以TGNB社区和现有的跨学科文献为基础，评估了TGNB个体经历边缘化所形成的社会现实是如何影响和存在于开放式语言生成（OLG）中。首先理解TGNB个体的经历，我们提出了一个评估OLG系统的框架，旨在以TGNB人群为中心，度量与该人群相关的偏见。我们的框架包括特别为TGNB人群设计的调查工具，以及交叉分析结果的交叉方法。我们相信，这项工作将有助于实现更公平、更包容的自然语言处理社区，并潜在地解决NLP研究中广泛的交叉身份问题。

    Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
    
[^84]: 基于文本的金融预测模型的一致性评估

    Measuring Consistency in Text-based Financial Forecasting Models. (arXiv:2305.08524v1 [cs.CL])

    [http://arxiv.org/abs/2305.08524](http://arxiv.org/abs/2305.08524)

    金融预测中模型的一致性对于建立用户信任至关重要，但目前金融预测方法很少考虑这一点，我们提出了一种评估金融文本逻辑一致性的评估工具FinTrust，并使用它发现最先进的金融预测NLP模型的一致性较差。

    

    金融预测是机器学习研究中一个重要且活跃的领域，因为即使是最小的预测准确率提高也可以转化为巨大的财务收益。自然语言处理方面的最新进展带来了利用文本数据（如上市公司的盈利报告）来预测资产收益率的机会。然而，在处理这种敏感任务时，模型的一致性——即在输入的保留意义改变时对模型的影响是一个建立用户信任的关键属性，但当前的金融预测方法却没有考虑到这一点。因此，我们提出了FinTrust，一种评估金融文本中逻辑一致性的评估工具。使用FinTrust，我们发现目前最先进的金融预测NLP模型的一致性较差。我们对保留意义改变引起的性能降级进行分析，结果表明...

    Financial forecasting has been an important and active area of machine learning research, as even the most modest advantage in predictive accuracy can be parlayed into significant financial gains. Recent advances in natural language processing (NLP) bring the opportunity to leverage textual data, such as earnings reports of publicly traded companies, to predict the return rate for an asset. However, when dealing with such a sensitive task, the consistency of models -- their invariance under meaning-preserving alternations in input -is a crucial property for building user trust. Despite this, current financial forecasting methods do not consider consistency. To address this problem, we propose FinTrust, an evaluation tool that assesses logical consistency in financial text. Using FinTrust, we show that the consistency of state-of-the-art NLP models for financial forecasting is poor. Our analysis of the performance degradation caused by meaning-preserving alternations suggests that cur
    
[^85]: 真实世界三维装箱问题的基准数据集和实例生成器

    Benchmark dataset and instance generator for Real-World Three-Dimensional Bin Packing Problems. (arXiv:2304.14712v1 [cs.AI])

    [http://arxiv.org/abs/2304.14712](http://arxiv.org/abs/2304.14712)

    本文提出了一个真实世界装箱问题的基准数据集和实例生成器，可以用来比较不同装箱算法的性能。

    

    本文提出了一个真实世界装箱问题的基准数据集。该数据集由12个实例组成，涵盖了不同大小和用户需求的问题复杂度水平（包含从38到53个包裹的数量）。实际上，我们考虑了几个面向真实世界的限制条件来构建这些实例：i)物品和箱子尺寸，ii)重量限制，iii)包类别之间的亲和性，iv)包装顺序的偏好和v)负载平衡。除了数据外，我们还提供了一个自主开发的Python脚本用于数据集生成，称为Q4RealBPP-DataGen。该基准首先被设计用于评估量子求解器，因此这组实例的特征是按照量子设备的当前限制设计的。此外，数据集生成器包含在内，允许构建通用基准。本文介绍的数据提供了一个基准，可以用来比较不同方法的性能和对比算法的效果。

    In this paper, a benchmark for real-world bin packing problems is proposed. This dataset is composed of 12 instances comprehending different levels of problem complexity regarding size (with the number of packages ranging from 38 to 53) and user-defined requirements. In fact, several real-world oriented restrictions have been considered for building these instances: i) items and bins dimensions, ii) weight restrictions, iii) affinities among packages categories iv) preferences for package ordering and v) load balancing. Besides the data, we also provide an own-developed Python script for the dataset generation, coined as Q4RealBPP-DataGen. The benchmark was firstly proposed to evaluate quantum solvers, therefore the characteristic of this set of instances were designed according to the current limitations of quantum devices. Additionally, the dataset generator is included to allow the construction of general-purpose benchmarks. The data introduced on this paper provides a baseline that
    
[^86]: VGOS：来自稀疏输入的体素网格优化视角合成

    VGOS: Voxel Grid Optimization for View Synthesis from Sparse Inputs. (arXiv:2304.13386v1 [cs.CV])

    [http://arxiv.org/abs/2304.13386](http://arxiv.org/abs/2304.13386)

    本论文介绍了一种名为VGOS的方法，可以快速从稀疏输入（3-10个视图）中重建放射场，以解决体素网格更容易过度拟合训练视图的问题，并提出了增量体素训练策略来避免过度拟合。

    

    神经放射场（NeRF）由于其优异的质量和灵活性在新颖视角合成中取得了很大的成功。但是，NeRF需要密集的输入视图（几十到几百个）和长时间的训练时间（几小时到几天）才能为单个场景生成高保真度的图像。虽然使用体素网格表示放射场可以显著加速优化过程，但我们注意到，在稀疏输入的情况下，体素网格更容易过度拟合训练视图，会产生空洞和漂浮物，导致伪影。在本文中，我们提出了VGOS，一种从稀疏输入（3-10个视图）快速（3-5分钟）重建放射场的方法来解决这些问题。为了改善基于体素的放射场在稀疏输入场景中的性能，我们提出了两种方法：（a）我们引入了增量体素训练策略，在重建早期抑制周边体素的优化，以防止过度拟合。

    Neural Radiance Fields (NeRF) has shown great success in novel view synthesis due to its state-of-the-art quality and flexibility. However, NeRF requires dense input views (tens to hundreds) and a long training time (hours to days) for a single scene to generate high-fidelity images. Although using the voxel grids to represent the radiance field can significantly accelerate the optimization process, we observe that for sparse inputs, the voxel grids are more prone to overfitting to the training views and will have holes and floaters, which leads to artifacts. In this paper, we propose VGOS, an approach for fast (3-5 minutes) radiance field reconstruction from sparse inputs (3-10 views) to address these issues. To improve the performance of voxel-based radiance field in sparse input scenarios, we propose two methods: (a) We introduce an incremental voxel training strategy, which prevents overfitting by suppressing the optimization of peripheral voxels in the early stage of reconstructio
    
[^87]: 关于静态风险度量的动态规划分解

    On Dynamic Program Decompositions of Static Risk Measures. (arXiv:2304.12477v1 [math.OC])

    [http://arxiv.org/abs/2304.12477](http://arxiv.org/abs/2304.12477)

    本文证明了现有的CVaR和EVaR风险度量动态分解是真实风险值的严格高估计，然而VaR存在精确的动态分解。

    

    在马尔科夫决策过程中优化静态风险规避目标具有一定挑战性，因为它们不容易接受动态规划分解。先前的研究建议使用风险度量的动态分解来制定扩展状态空间上的动态规划。本文表明几种现有的分解本质上是不精确的，这与文献中的几个声明相矛盾。特别地，我们举出了一些例子，证明了CVaR和EVaR风险度量的流行分解是真实风险值的严格高估计。然而，VaR确实存在精确的分解，我们提供一个简单的证明，阐明了VaR和CVaR动态规划属性之间的基本差异。

    Optimizing static risk-averse objectives in Markov decision processes is challenging because they do not readily admit dynamic programming decompositions. Prior work has proposed to use a dynamic decomposition of risk measures that help to formulate dynamic programs on an augmented state space. This paper shows that several existing decompositions are inherently inexact, contradicting several claims in the literature. In particular, we give examples that show that popular decompositions for CVaR and EVaR risk measures are strict overestimates of the true risk values. However, an exact decomposition is possible for VaR, and we give a simple proof that illustrates the fundamental difference between VaR and CVaR dynamic programming properties.
    
[^88]: 关于AI生成文本检测的可能性的探讨

    On the Possibilities of AI-Generated Text Detection. (arXiv:2304.04736v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.04736](http://arxiv.org/abs/2304.04736)

    该研究探讨了AI生成文本检测的可能性，提出了精确的样本复杂度界限，并指出了设计更准确的检测方法和提高透明度的挑战。

    

    我们的工作着眼于检测由大型语言模型(LLM)生成的输出，以区分其与人类生成的输出。这项能力在许多应用中非常重要。然而，关于这种区分的可能性一直是该领域内的争议话题。因此，一个核心问题是我们是否能够检测到AI生成的文本，如果能，何时能检测到。在这项工作中，我们提供了证据表明，除非人类和机器生成的文本分布在整个支持中完全相同，否则几乎总是可以检测到AI生成的文本。这个观察结果来自于信息论中的标准结果，并依赖于机器生成的文本越像人类，我们就需要更多的样本来检测它。我们得出了AI生成文本检测的精确样本复杂度界限，告诉需要多少个样本才能检测到AI生成的文本。这引起了更多设计更准确的AI生成文本检测方法和提高LLM透明度的挑战。

    Our work focuses on the challenge of detecting outputs generated by Large Language Models (LLMs) to distinguish them from those generated by humans. This ability is of the utmost importance in numerous applications. However, the possibility of such discernment has been the subject of debate within the community. Therefore, a central question is whether we can detect AI-generated text and, if so, when. In this work, we provide evidence that it should almost always be possible to detect AI-generated text unless the distributions of human and machine-generated texts are exactly the same over the entire support. This observation follows from the standard results in information theory and relies on the fact that if the machine text becomes more human-like, we need more samples to detect it. We derive a precise sample complexity bound of AI-generated text detection, which tells how many samples are needed to detect AI-generated text. This gives rise to additional challenges of designing more
    
[^89]: 快速密集信息检索器利用KALE进行后置KL对齐的异形双编码器模型训练 (arXiv:2304.01016v2 [cs.CL] UPDATED)

    Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders. (arXiv:2304.01016v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.01016](http://arxiv.org/abs/2304.01016)

    本文提出了一种通过结构压缩和模型尺寸不对称的双编码器模型 KALE，有效提高密集信息检索的推理效率，同时允许查询编码器的有效压缩，而无需进行全部的再训练或索引生成，此方法能够生成超过DistilBERT性能的模型。

    

    本文提出了一种有结构压缩和模型尺寸不对称的双编码器模型，旨在提高基于语言模型的密集信息检索系统的推理速度。通过对MSMARCO、自然问答、问答游戏等多个数据集进行前后训练压缩实验，研究了压缩对系统推理效率的影响，结果表明密集信息检索器的双编码器结构异形化有助于提高其推理效率。基于此，我们引入了一种名为Kullback Leibler Alignment of Embeddings (KALE)的方法，通过裁剪和对齐查询编码器，提高了密集信息检索的推理效率。KALE扩展了传统的知识蒸馏方法，使得在双编码器训练后可以有效地对查询编码器进行压缩而无需进行完整的再训练或索引生成。使用KALE和不对称训练，我们可以生成超过DistilBERT性能的模型，同时模型尺寸更小。

    In this paper, we consider the problem of improving the inference latency of language model-based dense retrieval systems by introducing structural compression and model size asymmetry between the context and query encoders. First, we investigate the impact of pre and post-training compression on the MSMARCO, Natural Questions, TriviaQA, SQUAD, and SCIFACT, finding that asymmetry in the dual encoders in dense retrieval can lead to improved inference efficiency. Knowing this, we introduce Kullback Leibler Alignment of Embeddings (KALE), an efficient and accurate method for increasing the inference efficiency of dense retrieval methods by pruning and aligning the query encoder after training. Specifically, KALE extends traditional Knowledge Distillation after bi-encoder training, allowing for effective query encoder compression without full retraining or index generation. Using KALE and asymmetric training, we can generate models which exceed the performance of DistilBERT despite having 
    
[^90]: 基于徒弟学习的面向主题的文本到图像生成器

    Subject-driven Text-to-Image Generation via Apprenticeship Learning. (arXiv:2304.00186v1 [cs.CV])

    [http://arxiv.org/abs/2304.00186](http://arxiv.org/abs/2304.00186)

    该论文提出了一种基于徒弟学习的面向主题的文本到图像生成器SuTI，能够通过将大量基于主题的专家模型的数据输入徒弟模型，学习并推断出新主题的最佳专家模型，从而生成高品质的自定义图像，且速度比传统方法更快。

    

    最近的文本到图像生成模型（如DreamBooth）在通过针对目标主题微调“专家模型”，生成高度自定义的图像方面取得了显著进展。然而，这个过程很昂贵，因为每个主题都必须学习一个新的专家模型。在本文中，我们提出了一个替代主题特定微调的面向主题的文本到图像生成器SuTI。给定一个新主题的少量演示，SuTI可以即时生成不同场景中主题的新版本，而无需进行任何主题特定的优化。SuTI由“徒弟学习”驱动，其中从大量基于主题的专家模型生成的数据中学习单个的徒弟模型。具体而言，我们从互联网挖掘了数百万个图像簇，每个图像簇都聚焦于一个特定的视觉主题。我们采用这些簇来训练大量专门针对不同视觉主题的专家模型。徒弟模型通过推断基于其文本描述的新主题的最佳专家模型并生成图像来学习。我们在各种基准数据集上展示了SuTI的有效性，表明它可以生成高品质的不同主题的图像，同时比基于微调的方法快得多。

    Recent text-to-image generation models like DreamBooth have made remarkable progress in generating highly customized images of a target subject, by fine-tuning an ``expert model'' for a given subject from a few examples. However, this process is expensive, since a new expert model must be learned for each subject. In this paper, we present SuTI, a Subject-driven Text-to-Image generator that replaces subject-specific fine tuning with \emph{in-context} learning. Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization. SuTI is powered by {\em apprenticeship learning}, where a single apprentice model is learned from data generated by massive amount of subject-specific expert models. Specifically, we mine millions of image clusters from the Internet, each centered around a specific visual subject. We adopt these clusters to train massive amount of expert models specialized on diff
    
[^91]: 中间特征联盟能帮助解释黑盒模型吗？

    Do intermediate feature coalitions aid explainability of black-box models?. (arXiv:2303.11920v1 [cs.LG])

    [http://arxiv.org/abs/2303.11920](http://arxiv.org/abs/2303.11920)

    本文引入了中间概念的级别结构，利用领域专家建立部分-整体关系，从而在不同抽象级别上帮助解释黑盒模型。

    

    本文引入了基于级别结构的中间概念，以帮助黑盒模型的可解释性。级别结构是一种分层结构，每个级别对应数据集的特征（即玩家集分区）。从只包含单元素的平凡集合到只包含大联盟的集合，粗糙度的级别逐渐增加。此外，可以通过领域专家建立部分-整体关系来生成抽象级别的解释。我们在一个实际的汽车模型示例和泰坦尼克号的数据集中说明了这种方法的可用性，其中中间概念在不同抽象级别上帮助解释。

    This work introduces the notion of intermediate concepts based on levels structure to aid explainability for black-box models. The levels structure is a hierarchical structure in which each level corresponds to features of a dataset (i.e., a player-set partition). The level of coarseness increases from the trivial set, which only comprises singletons, to the set, which only contains the grand coalition. In addition, it is possible to establish meronomies, i.e., part-whole relationships, via a domain expert that can be utilised to generate explanations at an abstract level. We illustrate the usability of this approach in a real-world car model example and the Titanic dataset, where intermediate concepts aid in explainability at different levels of abstraction.
    
[^92]: 局部正则化的神经微分方程：有些黑盒子是要保持封闭的！

    Locally Regularized Neural Differential Equations: Some Black Boxes Were Meant to Remain Closed!. (arXiv:2303.02262v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02262](http://arxiv.org/abs/2303.02262)

    该论文提出了一种局部正则化的神经微分方程模型，通过启发式成本控制训练过程以学习易于积分的动力系统，同时提高预测速度，并保留了模型的自适应特性。

    

    由于自动适应的能力，类似神经微分方程的隐式层深度学习技术已经成为重要的建模框架。但是，控制这些模型的计算成本是困难的，因为它依赖于自适应求解器的步骤数。本文中，我们在随机时间点使用自适应微分方程求解器的内部成本启发式来引导训练以学习更易于积分的动力系统。我们增加局部正则化项到目标函数中，使得训练过程遵循经验条件并提高预测速度，同时仍保留了模型的自适应特性。

    Implicit layer deep learning techniques, like Neural Differential Equations, have become an important modeling framework due to their ability to adapt to new problems automatically. Training a neural differential equation is effectively a search over a space of plausible dynamical systems. However, controlling the computational cost for these models is difficult since it relies on the number of steps the adaptive solver takes. Most prior works have used higher-order methods to reduce prediction timings while greatly increasing training time or reducing both training and prediction timings by relying on specific training algorithms, which are harder to use as a drop-in replacement due to strict requirements on automatic differentiation. In this manuscript, we use internal cost heuristics of adaptive differential equation solvers at stochastic time points to guide the training toward learning a dynamical system that is easier to integrate. We "close the black-box" and allow the use of ou
    
[^93]: 流体变压器与创造性类比：探索大语言模型增强跨领域类比创造力的能力

    Fluid Transformers and Creative Analogies: Exploring Large Language Models' Capacity for Augmenting Cross-Domain Analogical Creativity. (arXiv:2302.12832v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.12832](http://arxiv.org/abs/2302.12832)

    本文系统地探讨了大语言模型增强跨领域类比创造力的能力，结果表明LLM生成的跨领域类比在问题重构中具有实际帮助，并且存在一些潜在的危害，需要注意。

    

    跨领域的类比推理是一种对人类来说具有挑战性的核心创造能力。最近的研究已经证明了大语言模型（LLMs）生成跨领域类比的能力。然而，这种能力的可靠性和潜在用途的探索却鲜有系统的研究。本文系统地探讨了LLMs增强跨领域类比推理的能力。在三项研究中，我们发现：1）LLM生成的跨领域类比在问题重构任务的背景下经常被评为有帮助（中位数5个评分中有4个是有帮助的），并且通常（约80％的情况）导致问题重新制定方面的可观察变化；2）存在最多25％的输出被评为有潜在危害的上限，其中大部分是由于潜在的不安内容，而不是有偏见或有毒内容。这些结果证明了LLMs的潜在效用和风险，以增强跨领域类比推理的能力。

    Cross-domain analogical reasoning is a core creative ability that can be challenging for humans. Recent work has shown some proofs-of concept of Large language Models' (LLMs) ability to generate cross-domain analogies. However, the reliability and potential usefulness of this capacity for augmenting human creative work has received little systematic exploration. In this paper, we systematically explore LLMs capacity to augment cross-domain analogical reasoning. Across three studies, we found: 1) LLM-generated cross-domain analogies were frequently judged as helpful in the context of a problem reformulation task (median 4 out of 5 helpfulness rating), and frequently (~80% of cases) led to observable changes in problem formulations, and 2) there was an upper bound of 25% of outputs bring rated as potentially harmful, with a majority due to potentially upsetting content, rather than biased or toxic content. These results demonstrate the potential utility -- and risks -- of LLMs for augmen
    
[^94]: 公平扩散：训练文本到图像生成模型实现公平性

    Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness. (arXiv:2302.10893v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10893](http://arxiv.org/abs/2302.10893)

    这篇论文提出了一种名为“公平扩散”的新策略，可以在生成文本到图像模型部署后减轻偏见并使模型接受公平性指导。

    

    最近，生成式AI模型在质量方面取得了惊人的成果，并因此被广泛应用于越来越多的应用中。但由于它们高度依赖于从互联网上随机抽取的十亿级数据集，因此它们也会受到退化和偏见的人类行为的影响，正如我们所展示的那样。事实上，它们甚至可能加剧这些偏见。为了不仅揭示而且对抗这些不良影响，我们提出了一种新的策略，称为公平扩散，以在生成文本到图像模型部署后减轻偏见。具体而言，我们展示了基于人类指导的偏差转移，可在任何方向上产生任意新的比例，例如，身份组。正如我们的实证评估所示，这种控制使生成图像模型在公平性方面能够接受指导，无需数据过滤和额外的训练。

    Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases after the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias, based on human instructions, in any direction yielding arbitrarily new proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, with no data filtering and additional training required.
    
[^95]: ChatGPT：应付千事的万能型 AI，但无所专精

    ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.10724](http://arxiv.org/abs/2302.10724)

    本研究检验了 ChatGPT 在 25 个不同的 NLP 任务上的性能，它是一个万能的 AI 模型，但无关紧要的表现可能会对某些任务的表现产生负面影响。

    

    OpenAI 推出了聊天生成预训练 Transformer（ChatGPT），革新了人工智能与人类互动的方法。许多研究通过测试 ChatGPT 在众所周知的自然语言处理（NLP）任务中的效果，来评估该模型的效能。然而，现有的研究大多非自动化，并且规模非常有限。本研究在 25 个不同的 NLP 任务上检验了 ChatGPT 的性能，其中大多数任务甚至对人类而言都是主观的，例如情感分析、情绪识别、攻击性和立场检测。另一些任务则需要更客观的推理，如词义消歧、语言可接受性和问答。我们还对 GPT-4 模型在五个选定的 NLP 任务子集上进行了评估。我们自动化了 ChatGPT 和 GPT-4 的引导过程，并分析了超过 49k 个响应。与现有最先进的解决方案（SOTA）进行比较，我们的结果显示，在一些任务上 ChatGPT 的性能存在一定的缺陷。

    OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quali
    
[^96]: 几何深度学习仅依靠距离矩阵足够吗？

    Is Distance Matrix Enough for Geometric Deep Learning?. (arXiv:2302.05743v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05743](http://arxiv.org/abs/2302.05743)

    本文证明了消息传递神经网络（MPNNs）不能学习几何信息，提出了$k$-DisGNNs可以利用距离矩阵中的信息，并建立了几何深度学习和传统图表示学习之间的联系。

    

    图神经网络（GNN）常用于涉及图形几何的任务，例如分子动力学模拟。虽然几何图的距离矩阵包含完整的几何信息，但已经证明消息传递神经网络（MPNNs）无法学习这种几何信息。本文通过构造新颖的对称几何图的家族，扩展了MPNN无法区分其距离矩阵的反例家族，并提出$k$-DisGNNs，可以有效地利用距离矩阵中丰富的几何结构。我们证明了模型的高表达能力，并证明了一些现有的精心设计的几何模型可以作为$k$-DisGNNs的特殊情况统一起来。最重要的是，我们建立了几何深度学习和传统图表示学习之间的联系，展示了那些最初为低度表达能力的GNN模型设计的高度表达力的GNN模型。

    Graph Neural Networks (GNNs) are often used for tasks involving the geometry of a given graph, such as molecular dynamics simulation. Although the distance matrix of a geometric graph contains complete geometric information, it has been demonstrated that Message Passing Neural Networks (MPNNs) are insufficient for learning this geometry. In this work, we expand on the families of counterexamples that MPNNs are unable to distinguish from their distance matrices, by constructing families of novel and symmetric geometric graphs. We then propose $k$-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of our models and prove that some existing well-designed geometric models can be unified by $k$-DisGNNs as special cases. Most importantly, we establish a connection between geometric deep learning and traditional graph representation learning, showing that those highly expressive GNN models originally designed for
    
[^97]: 低位视觉变压器的无振荡量化

    Oscillation-free Quantization for Low-bit Vision Transformers. (arXiv:2302.02210v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.02210](http://arxiv.org/abs/2302.02210)

    研究发现，可学习比例因子加剧了权重振荡，本文提出三种技术以解决这个问题，并在多个基准测试上显著提高了模型的性能。

    

    量化意识训练的一个不良副作用是权重振荡，其中量化权重经常在两个量化级别之间跳动，导致训练不稳定和子优化的最终模型。本研究发现，可学习的比例因子——在量化中广泛使用的$\textit{de facto}$设置——加剧了权重振荡。本研究研究了可学习比例因子与量化权重振荡之间的关系，并以ViT为案例来说明发现和解决方法。此外，我们还发现自注意力层中量化权重的$\textit{query}$和$\textit{key}$之间的相互依存使ViT容易受到振荡的影响。因此，我们相应地提出了三种技术：统计权重量化（$\rm StatsQ$）以改善量化鲁棒性，与普遍使用的可学习比例因子方法相比；置信度引导的退火（$\rm CGA$）在训练期间冻结具有$\textit{高置信度}$的权重，以减少权重振荡；以及相互依赖权重的均衡（$\rm IWEqual$），以有效处理相互依赖问题。我们的实验表明，我们提出的方法相比于最先进的方法在多个基准测试上显著提高了模型的性能。

    Weight oscillation is an undesirable side effect of quantization-aware training, in which quantized weights frequently jump between two quantized levels, resulting in training instability and a sub-optimal final model. We discover that the learnable scaling factor, a widely-used $\textit{de facto}$ setting in quantization aggravates weight oscillation. In this study, we investigate the connection between the learnable scaling factor and quantized weight oscillation and use ViT as a case driver to illustrate the findings and remedies. In addition, we also found that the interdependence between quantized weights in $\textit{query}$ and $\textit{key}$ of a self-attention layer makes ViT vulnerable to oscillation. We, therefore, propose three techniques accordingly: statistical weight quantization ($\rm StatsQ$) to improve quantization robustness compared to the prevalent learnable-scale-based method; confidence-guided annealing ($\rm CGA$) that freezes the weights with $\textit{high confi
    
[^98]: 可能任何时候安全的随机组合半臂赌博问题

    Probably Anytime-Safe Stochastic Combinatorial Semi-Bandits. (arXiv:2301.13393v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13393](http://arxiv.org/abs/2301.13393)

    本文提出了可能任何时候安全的随机组合半臂赌博问题，并设计出算法PASCombUCB在时间轴上最小化后悔值。

    

    本文旨在解决在线决策中可能造成过度风险的问题，提出了可能任何时候安全的随机组合半臂赌博问题。在这个问题中，智能体有选择从$L$个基础项中不超过$K$个进行子集的选项。每个元素都与某个平均奖励和表示其风险的方差相关联。为了减少代理人所遭受的风险，我们要求，在整个时间$T$的时间跨度上，智能体所做的每个选择都应包含其方差之和不超过某个方差预算的元素，且其可能任何时候满足此约束。在此约束条件下，我们设计和分析了一种算法PASCombUCB，以在时间轴上最小化后悔值。通过开发配套信息理论下界，我们证明在问题相关和问题无关的两种范例下，算法都是最优的。

    Motivated by concerns about making online decisions that incur undue amount of risk at each time step, in this paper, we formulate the probably anytime-safe stochastic combinatorial semi-bandits problem. In this problem, the agent is given the option to select a subset of size at most $K$ from a set of $L$ ground items. Each item is associated to a certain mean reward as well as a variance that represents its risk. To mitigate the risk that the agent incurs, we require that with probability at least $1-\delta$, over the entire horizon of time $T$, each of the choices that the agent makes should contain items whose sum of variances does not exceed a certain variance budget. We call this probably anytime-safe constraint. Under this constraint, we design and analyze an algorithm {\sc PASCombUCB} that minimizes the regret over the horizon of time $T$. By developing accompanying information-theoretic lower bounds, we show that under both the problem-dependent and problem-independent paradig
    
[^99]: CHAD：夏洛特异常数据集

    CHAD: Charlotte Anomaly Dataset. (arXiv:2212.09258v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.09258](http://arxiv.org/abs/2212.09258)

    CHAD是一个商业停车场环境的高分辨率、多相机异常数据集，是第一个为每个演员包括边界框、身份和姿势注释的异常数据集，同时包括各种环境因素。使用CHAD强调了更准确的异常检测算法所需的现实数据集的重要性。

    

    近年来，基于数据驱动的深度学习方法在视频异常检测中受到了广泛关注，其中算法必须确定视频的特定帧是否包含异常行为。然而，视频异常检测特别因情境而异，而代表性数据集的可用性严重限制了现实世界中的准确性。此外，目前大多数最先进的方法报告的指标通常不能反映模型在真实场景中的表现如何。在本文中，我们介绍了夏洛特异常数据集（CHAD）。CHAD是商业停车场环境中的高分辨率、多相机异常数据集。除了帧级别的异常标签外，CHAD还是第一个为每个演员包括边界框、身份和姿势注释的异常数据集。这对于基于骨骼的异常检测特别有益，因为它在现实世界中具有较低的计算需求。CHAD也是第一个包括各种环境因素（例如光照变化、天气条件变化和摄像机视角）的异常数据集。使用CHAD，我们展示了这些环境因素如何对现有最先进的模型提出了重大挑战，最终强调了更准确的异常检测算法所需的现实数据集的重要性。

    In recent years, we have seen a significant interest in data-driven deep learning approaches for video anomaly detection, where an algorithm must determine if specific frames of a video contain abnormal behaviors. However, video anomaly detection is particularly context-specific, and the availability of representative datasets heavily limits real-world accuracy. Additionally, the metrics currently reported by most state-of-the-art methods often do not reflect how well the model will perform in real-world scenarios. In this article, we present the Charlotte Anomaly Dataset (CHAD). CHAD is a high-resolution, multi-camera anomaly dataset in a commercial parking lot setting. In addition to frame-level anomaly labels, CHAD is the first anomaly dataset to include bounding box, identity, and pose annotations for each actor. This is especially beneficial for skeleton-based anomaly detection, which is useful for its lower computational demand in real-world settings. CHAD is also the first anoma
    
[^100]: BKinD-3D：自监督多视点视频中的三维关键点发现

    BKinD-3D: Self-Supervised 3D Keypoint Discovery from Multi-View Videos. (arXiv:2212.07401v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.07401](http://arxiv.org/abs/2212.07401)

    BKinD-3D是一种新的无需2D或3D监督的自监督三维关键点发现方法，使用编码器-解码器架构和3D体积热力图对多个视图之间的时空差异进行重建，还使用了一个学习到的3D主体骨架的关节长度约束。

    

    量化三维运动对于研究人类和其他动物的行为非常重要，但手动姿态注释获取成本高且耗时。自监督关键点发现是一种有前途的策略，可以在没有注释的情况下估计三维姿态。然而，当前的关键点发现方法通常仅处理单个2D视图并且不在3D空间操作。我们提出了一种新的方法，可以在行为主体的多视点视频中执行无2D或3D关键点或边界框监督的自监督三维关键点发现。我们的方法BKinD-3D使用编码器-解码器架构和3D体积热力图，训练以重建多个视图之间的时空差异，此外还使用了学习到的主体三维骨架的关节长度约束。通过这种方法，我们在没有手动监督的情况下，在人类和老鼠的视频中发现了关键点，展示了三维关键点发现在研究行为方面的潜力。

    Quantifying motion in 3D is important for studying the behavior of humans and other animals, but manual pose annotations are expensive and time-consuming to obtain. Self-supervised keypoint discovery is a promising strategy for estimating 3D poses without annotations. However, current keypoint discovery approaches commonly process single 2D views and do not operate in the 3D space. We propose a new method to perform self-supervised keypoint discovery in 3D from multi-view videos of behaving agents, without any keypoint or bounding box supervision in 2D or 3D. Our method, BKinD-3D, uses an encoder-decoder architecture with a 3D volumetric heatmap, trained to reconstruct spatiotemporal differences across multiple views, in addition to joint length constraints on a learned 3D skeleton of the subject. In this way, we discover keypoints without requiring manual supervision in videos of humans and rats, demonstrating the potential of 3D keypoint discovery for studying behavior.
    
[^101]: PATO: 基于策略辅助的可伸缩机器人数据采集系统

    PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection. (arXiv:2212.04708v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2212.04708](http://arxiv.org/abs/2212.04708)

    PATO是一个策略辅助的远程操作系统，通过自动执行部分演示采集过程，并仅在不确定要执行哪个子任务或行为时请求人类输入，提高了数据采集效率，同时减少了人工操作员的心理负担，实现单个操作员并行控制多个机器人的功能。

    

    大规模数据是机器学习的重要组成部分，正如在自然语言处理和计算机视觉研究的最新进展中所证明的那样。但是，采集大规模机器人数据的成本更高，速度更慢，因为每个操作员只能同时控制一个机器人。为了使这个昂贵的数据采集过程高效和可伸缩，我们提出了策略辅助的远程操作系统（PATO），该系统利用学习到的辅助策略自动执行部分演示采集过程。PATO在数据采集中自动执行重复的行为，并仅在不确定要执行哪个子任务或行为时请求人类输入。我们进行了与真实机器人和模拟机器人群体的远程操作用户研究，并证明我们的辅助远程操作系统能够减少人工操作员的心理负担，同时提高数据采集效率。此外，它使单个操作员能够并行控制多个机器人，这是首次。

    Large-scale data is an essential component of machine learning as demonstrated in recent advances in natural language processing and computer vision research. However, collecting large-scale robotic data is much more expensive and slower as each operator can control only a single robot at a time. To make this costly data collection process efficient and scalable, we propose Policy Assisted TeleOperation (PATO), a system which automates part of the demonstration collection process using a learned assistive policy. PATO autonomously executes repetitive behaviors in data collection and asks for human input only when it is uncertain about which subtask or behavior to execute. We conduct teleoperation user studies both with a real robot and a simulated robot fleet and demonstrate that our assisted teleoperation system reduces human operators' mental load while improving data collection efficiency. Further, it enables a single operator to control multiple robots in parallel, which is a first
    
[^102]: PrefRec：利用人类偏好加强长期用户参与的推荐系统

    PrefRec: Recommender Systems with Human Preferences for Reinforcing Long-term User Engagement. (arXiv:2212.02779v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2212.02779](http://arxiv.org/abs/2212.02779)

    该论文提出了一种基于人类偏好的推荐系统范式PrefRec，允许强化学习推荐系统从用户历史行为偏好中学习，以优化长期用户参与度。

    

    目前，推荐系统在优化即时参与方面取得了令人瞩目的成功。然而，更可取的绩效指标长期用户参与度的提高仍然很难。与此同时，最近的强化学习算法在各种长期目标优化任务中展现了有效性。因此，强化学习被广泛认为是优化推荐中长期用户参与度的有前途的框架。虽然有前途，但应用强化学习在很大程度上依赖于精心设计的奖励，但设计与长期用户参与有关的奖励相当困难。为了缓解这个问题，我们提出了一种新的范式，即以人类偏好为基础的推荐系统，允许强化学习推荐系统从有关用户历史行为的偏好中学习，而不是从明确定义的奖励中学习。这些偏好可以通过众包等技术轻松获得。

    Current advances in recommender systems have been remarkably successful in optimizing immediate engagement. However, long-term user engagement, a more desirable performance metric, remains difficult to improve. Meanwhile, recent reinforcement learning (RL) algorithms have shown their effectiveness in a variety of long-term goal optimization tasks. For this reason, RL is widely considered as a promising framework for optimizing long-term user engagement in recommendation. Though promising, the application of RL heavily relies on well-designed rewards, but designing rewards related to long-term user engagement is quite difficult. To mitigate the problem, we propose a novel paradigm, recommender systems with human preferences (or Preference-based Recommender systems), which allows RL recommender systems to learn from preferences about users historical behaviors rather than explicitly defined rewards. Such preferences are easily accessible through techniques such as crowdsourcing, as they 
    
[^103]: 文本中的离群检测的多层知识蒸馏

    Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text. (arXiv:2211.11300v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.11300](http://arxiv.org/abs/2211.11300)

    本文提出了一种多层知识蒸馏方法，融合了语言模型的训练和微调方法来进行文本中的离群检测，实验结果表明其有效性。

    

    自监督表示学习已经证明是只使用内分布(ID)样例文本进行离群检测的宝贵组成部分。这些方法要么从头开始训练语言模型，要么通过使用ID样例微调预训练语言模型，然后将语言模型输出的困惑度作为离群得分。本文分析了两种离群检测方法的互补特性，并提出了一种融合它们优势并减轻它们的局限性的多层知识蒸馏方法。具体而言，我们使用微调模型作为老师，在ID示例上教授一个随机初始化的学生模型。除了预测层蒸馏外，我们还提出了一种基于相似性的中间层蒸馏方法，以全面探索老师模型的表示空间。通过这种方式，学习的学生可以更好地表示ID数据流形，同时获得更强的将OoD示例映射到流形之外的能力。基准数据集上的实验结果证明了我们所提出的方法与竞争基线相比的有效性。

    Self-supervised representation learning has proved to be a valuable component for out-of-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores. In this paper, we analyze the complementary characteristics of both OoD detection methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Specifically, we use a fine-tuned model as the teacher to teach a randomly initialized student model on the ID examples. Besides the prediction layer distillation, we present a similarity-based intermediate layer distillation method to thoroughly explore the representation space of the teacher model. In this way, the learned student can better represent the ID data manifold while gaining a stronger ability to map O
    
[^104]: 基于模态逻辑的统计因果关系形式化

    Formalizing Statistical Causality via Modal Logic. (arXiv:2210.16751v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.16751](http://arxiv.org/abs/2210.16751)

    提出了一种基于模态逻辑的形式语言，用于描述和解释统计因果关系，并且能够指定和解释统计因果推断的正确性。

    

    我们提出了一种描述和解释统计因果关系的形式语言。具体来说，我们定义了统计因果语言（StaCL），用于表达随机变量上的因果效应并指定因果推断的要求。StaCL通过干预的模态运算符，在不同可能的世界的概率分布之间表达因果属性，在Kripke模型中解释。我们使用StaCL公式正式化概率分布、干预和因果谓词的公理。这些公理足够表达Pearl的do-calculus规则。最后，我们通过示例证明了StaCL可以用于指定和解释统计因果推断的正确性。

    We propose a formal language for describing and explaining statistical causality. Concretely, we define Statistical Causality Language (StaCL) for expressing causal effects on random variables and specifying the requirements for causal inference. StaCL incorporates modal operators for interventions to express causal properties between probability distributions in different possible worlds in a Kripke model. We formalize axioms for probability distributions, interventions, and causal predicates using StaCL formulas. These axioms are expressive enough to derive the rules of Pearl's do-calculus. Finally, we demonstrate by examples that StaCL can be used to specify and explain the correctness of statistical causal inference.
    
[^105]: 联邦学习中的本地模型重构攻击及其应用

    Local Model Reconstruction Attacks in Federated Learning and their Uses. (arXiv:2210.16205v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16205](http://arxiv.org/abs/2210.16205)

    本文研究了联邦学习中的本地模型重构攻击，攻击者可以通过重构客户端的模型更有效地触发其他攻击，并提出了一种新颖的基于模型的属性推导攻击，实证结果表明攻击有效，可应用于回归和分类任务。

    

    本文探讨了联邦学习中的本地模型重构攻击，攻击者可以通过窃听目标客户端与服务器之间的通信并重构受害者的本地/个性化模型，从而更有效地触发其他经典攻击。本地模型仅依赖于客户端的数据，但可能泄漏比服务器学习的全局模型更多的私有信息。此外，我们提出了一种新的基于模型的属性推导攻击，利用了本地模型重构攻击。我们提供了这种攻击的分析下界。使用真实世界数据集的实证结果表明，我们的本地重构攻击对于回归和分类任务都有效。此外，我们将我们的新颖的属性推导攻击与最先进的攻击进行了基准测试。

    In this paper, we initiate the study of local model reconstruction attacks for federated learning, where a honest-but-curious adversary eavesdrops the messages exchanged between a targeted client and the server, and then reconstructs the local/personalized model of the victim. The local model reconstruction attack allows the adversary to trigger other classical attacks in a more effective way, since the local model only depends on the client's data and can leak more private information than the global model learned by the server. Additionally, we propose a novel model-based attribute inference attack in federated learning leveraging the local model reconstruction attack. We provide an analytical lower-bound for this attribute inference attack. Empirical results using real world datasets confirm that our local reconstruction attack works well for both regression and classification tasks. Moreover, we benchmark our novel attribute inference attack against the state-of-the-art attacks in 
    
[^106]: 破碎的神经缩放定律

    Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14891](http://arxiv.org/abs/2210.14891)

    本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.

    我们提出了一个平滑破碎的幂律函数形式（我们称之为破碎的神经缩放定律（BNSL）），它准确地模拟和外推了深度神经网络的缩放行为（即感兴趣的评估指标随用于训练的计算量、模型参数数量、训练数据集大小或上游性能变化而变化）对于各种架构和大量不同任务中的每个任务，包括大规模视觉、语言、音频、视频、扩散、生成建模、多模态学习、对比学习、AI对齐、机器人、超出分布（OOD）泛化、持续学习、不确定性估计/校准、超出分布检测、对抗鲁棒性、蒸馏、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
    
[^107]: 领袖与追随者：深度多智能体强化学习中的Stackelberg均衡

    Oracles & Followers: Stackelberg Equilibria in Deep Multi-Agent Reinforcement Learning. (arXiv:2210.11942v4 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2210.11942](http://arxiv.org/abs/2210.11942)

    本文提出了一个通用框架，将Stackelberg均衡搜索作为多智能体RL问题进行实现，提出了一种利用上下文策略的方法，并在标准和新颖的基准领域上进行实验评估，证明相比之前的方法，样本效率得到了极大提高。

    

    Stackelberg均衡在多个学习问题中自然出现，例如在安全博弈或间接机制设计中，并在强化学习文献中受到越来越多的关注。我们提出了一个实现Stackelberg均衡搜索的通用框架，将其作为多智能体RL问题，允许进行各种算法设计选择，并将之前的方法视为此框架的特定实例。另外，我们注意到设计空间允许使用以前在文献中没有见过的方法，例如利用多任务和元-RL技术实现追随者的收敛。我们提出了一种使用上下文策略的方法，并在标准和新颖的基准领域上进行实验评估，证明了相比以前的方法，样本效率大大提高。最后，我们还探索了采用框架边界外的算法设计的效果。

    Stackelberg equilibria arise naturally in a range of popular learning problems, such as in security games or indirect mechanism design, and have received increasing attention in the reinforcement learning literature. We present a general framework for implementing Stackelberg equilibria search as a multi-agent RL problem, allowing a wide range of algorithmic design choices. We discuss how previous approaches can be seen as specific instantiations of this framework. As a key insight, we note that the design space allows for approaches not previously seen in the literature, for instance by leveraging multitask and meta-RL techniques for follower convergence. We propose one such approach using contextual policies, and evaluate it experimentally on both standard and novel benchmark domains, showing greatly improved sample efficiency compared to previous approaches. Finally, we explore the effect of adopting algorithm designs outside the borders of our framework.
    
[^108]: 通过推理时自适应优化实现语言生成中的统一去毒化和去偏见

    Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization. (arXiv:2210.04492v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.04492](http://arxiv.org/abs/2210.04492)

    本文提出了名为UDDIA的统一去毒化和去偏见框架，它能够有效地消除有毒语言和减少社会偏见，同时保持流畅性。

    

    近年来，预训练语言模型在各种自然语言生成任务中取得了很好的表现。然而，这些模型在训练语料库中捕捉和复制有害内容的情况普遍存在，特别是毒性语言和社会偏见，引起了严重的道德问题。此前在道德上的自然语言生成方面的工作都是分开解决去毒化和去偏见这两个问题的，但我们发现去偏见的模型仍然存在毒性，而经过去毒化的模型甚至会加剧社会偏见，这是有问题的。为了解决这个挑战，我们提出了名为UDDIA的统一去毒化和去偏见框架，将这两个问题作为纠正输出空间的关键问题联合形式化。我们在理论上将我们的框架解释为学习混合加权属性的文本分布。此外，UDDIA对少量参数进行自适应优化，从而控制每个属性的贡献。我们证明了UDDIA能够有效地消除有毒语言并减少社会偏见，同时保持流畅性。此外，UDDIA在广泛的评估中优于最先进的去毒化和去偏见方法。我们还进行了深入的分析，解释了UDDIA的行为，并为未来在道德自然语言生成方面的工作提供了见解。

    Warning: this paper contains model outputs exhibiting offensiveness and biases. Recently pre-trained language models (PLMs) have prospered in various natural language generation (NLG) tasks due to their ability to generate fairly fluent text. Nevertheless, these models are observed to capture and reproduce harmful contents in training corpora, typically toxic language and social biases, raising severe moral issues. Prior works on ethical NLG tackle detoxifying and debiasing separately, which is problematic since we find debiased models still exhibit toxicity while detoxified ones even exacerbate social biases. To address such a challenge, we propose the first unified framework of detoxifying and debiasing called UDDIA, which jointly formalizes these two problems as rectifying the output space. We theoretically interpret our framework as learning a text distribution mixing weighted attributes. Besides, UDDIA conducts adaptive optimization of only a few parameters during decoding based o
    
[^109]: 公司需要可解释的人工智能吗？员工角度的挑战、期望和机会调查

    Do We Need Explainable AI in Companies? Investigation of Challenges, Expectations, and Chances from Employees' Perspective. (arXiv:2210.03527v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2210.03527](http://arxiv.org/abs/2210.03527)

    本论文探究员工对(X)AI的需求和态度。研究发现员工普遍认为解释性AI对公司的成功非常重要。

    

    人工智能（AI）对于企业的采用越来越成为业务成功的必要元素。然而，使用 AI 提出了公司和其员工的新要求，包括 AI 系统的透明度和可理解性。可解释人工智能（XAI）领域旨在解决这些问题。然而，目前研究主要包括实验室研究，需要改进研究结果在实际情况下的适用性。因此，本研究提供了员工对（X）AI 需求和态度的见解。我们的研究发现，AI 和 XAI 是众所周知的术语，被认为对员工非常重要。这一认知是推动 AI 成功使用的关键第一步，为提供可理解的 AI 技术洞见做出贡献。在“教训学习”部分，我们讨论了发现的开放问题，并提出了未来的研究方向。

    Companies' adoption of artificial intelligence (AI) is increasingly becoming an essential element of business success. However, using AI poses new requirements for companies and their employees, including transparency and comprehensibility of AI systems. The field of Explainable AI (XAI) aims to address these issues. Yet, the current research primarily consists of laboratory studies, and there is a need to improve the applicability of the findings to real-world situations. Therefore, this project report paper provides insights into employees' needs and attitudes towards (X)AI. For this, we investigate employees' perspectives on (X)AI. Our findings suggest that AI and XAI are well-known terms perceived as important for employees. This recognition is a critical first step for XAI to potentially drive successful usage of AI by providing comprehensible insights into AI technologies. In a lessons-learned section, we discuss the open questions identified and suggest future research direction
    
[^110]: 长程规划与执行的功能对象导向网络

    Long-Horizon Planning and Execution with Functional Object-Oriented Networks. (arXiv:2207.05800v5 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2207.05800](http://arxiv.org/abs/2207.05800)

    本研究提出了一种利用物体级别知识作为功能对象导向网络进行任务规划和执行的方法，并且演示了在CoppeliaSim中应用该方法进行长期任务规划的效果。

    

    在联合对象-动作表示方面的工作之后，将功能对象导向网络（FOON）作为机器人的知识图表示引入。FOON包含对机器人任务理解和物体级规划的环境有用的符号概念。在本研究之前，很少有研究表明如何从FOON中获取的计划可以由机器人执行，因为FOON中的概念对于执行来说太抽象了。因此，我们引入了利用物体级别知识作为FOON进行任务规划和执行的想法。我们的方法自动将FOON转换为PDDL，并利用现成的规划器、行为上下文和机器人技能在分层规划管道中生成可执行的任务计划。我们在CoppeliaSim中展示了我们的整个方法的长期任务，以及如何将学习到的行为上下文扩展到前所未见的场景中。

    Following work on joint object-action representations, functional object-oriented networks (FOON) were introduced as a knowledge graph representation for robots. A FOON contains symbolic concepts useful to a robot's understanding of tasks and its environment for object-level planning. Prior to this work, little has been done to show how plans acquired from FOON can be executed by a robot, as the concepts in a FOON are too abstract for execution. We thereby introduce the idea of exploiting object-level knowledge as a FOON for task planning and execution. Our approach automatically transforms FOON into PDDL and leverages off-the-shelf planners, action contexts, and robot skills in a hierarchical planning pipeline to generate executable task plans. We demonstrate our entire approach on long-horizon tasks in CoppeliaSim and show how learned action contexts can be extended to never-before-seen scenarios.
    
[^111]: 动态空间稀疏化：高效视觉变压器和卷积神经网络的新方法

    Dynamic Spatial Sparsification for Efficient Vision Transformers and Convolutional Neural Networks. (arXiv:2207.01580v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.01580](http://arxiv.org/abs/2207.01580)

    基于视觉变压器中的空间稀疏性，本文提出了一种动态标记稀疏化框架，可加速各种结构的模型，被剪枝的冗余标记由轻量级预测模块逐步动态估计。

    

    本文提出了一种利用视觉数据中的空间稀疏性加速模型的新方法。我们观察到视觉变压器中最终预测仅基于一小部分信息最丰富的标记，这对于准确的图像识别足够。基于这一观察，我们提出了一种动态标记稀疏化框架，根据输入逐步和动态地剪枝冗余标记以加速视觉变压器。具体而言，我们设计了一个轻量级预测模块，以根据当前特征来估计每个标记的重要性得分。该模块被添加到不同层中以分层地剪枝冗余标记。虽然该框架来源于我们对视觉变压器中稀疏注意力的观察，但我们发现自适应和非对称计算的思想可以成为加速各种结构的一般解决方案。我们将我们的方法扩展到包括卷积神经网络和分层视觉变压器在内的分层模型中。

    In this paper, we present a new approach for model acceleration by exploiting spatial sparsity in visual data. We observe that the final prediction in vision Transformers is only based on a subset of the most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input to accelerate vision Transformers. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. While the framework is inspired by our observation of the sparse attention in vision Transformers, we find the idea of adaptive and asymmetric computation can be a general solution for accelerating various architectures. We extend our method to hierarchical models including CNNs and hierarchical vision Transformers 
    
[^112]: 联邦学习中的主体成员推理攻击

    Subject Membership Inference Attacks in Federated Learning. (arXiv:2206.03317v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03317](http://arxiv.org/abs/2206.03317)

    本文研究跨边界联邦学习中主体级别的隐私，并提出两种新颖的黑盒推理攻击方法。

    

    机器学习模型的隐私攻击通常着重于推论训练数据中特定数据点的存在。然而，攻击者真正想知道的是特定个体的（主体的）数据是否包含在训练中。在这种情况下，攻击者更可能拥有特定主体分布而非实际记录。本文研究跨边界联邦学习中主体级别的隐私，提出两种新颖的黑盒推理攻击方法，并进行评估。

    Privacy attacks on Machine Learning (ML) models often focus on inferring the existence of particular data points in the training data. However, what the adversary really wants to know is if a particular individual's (subject's) data was included during training. In such scenarios, the adversary is more likely to have access to the distribution of a particular subject than actual records. Furthermore, in settings like cross-silo Federated Learning (FL), a subject's data can be embodied by multiple data records that are spread across multiple organizations. Nearly all of the existing private FL literature is dedicated to studying privacy at two granularities -- item-level (individual data records), and user-level (participating user in the federation), neither of which apply to data subjects in cross-silo FL. This insight motivates us to shift our attention from the privacy of data records to the privacy of data subjects, also known as subject-level privacy. We propose two novel black-bo
    
[^113]: 无架构偏见的遮蔽图像建模——从ViT回到CNN

    Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN. (arXiv:2205.13943v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.13943](http://arxiv.org/abs/2205.13943)

    本文提出一种无架构偏见的遮蔽图像建模框架（A$^2$MIM），既适用于Transformers也适用于CNNs，并通过学习中级交互关系来提高特征提取的一般性能。

    

    遮蔽图像建模（MIM）是一种新兴的自监督预训练方法，在许多下游视觉任务中，如Vision transformers，已经显示出惊人的成功。其基本思想很简单：把输入图像的一部分遮挡住，然后通过预热任务进行重构。然而，MIM背后的工作原理并没有得到很好的解释，以前的研究坚持认为MIM主要适用于Transformer家族，但与CNN不兼容。 在这项工作中，我们观察到，MIM实质上是教模型学习更好的中级交互关系，以进行更广义的特征提取。然后，我们提出了一种无架构偏见的遮蔽图像建模框架（A$^2$MIM），它以统一的方式兼容Transformers和CNNs。对流行基准的大量实验表明，A$^2$MIM学习更好的表示形式，无需显式设计，并赋予骨干模型更强的能力来转移到各种下游任务。

    Masked image modeling, an emerging self-supervised pre-training method, has shown impressive success across numerous downstream vision tasks with Vision transformers. Its underlying idea is simple: a portion of the input image is masked out and then reconstructed via a pre-text task. However, the working principle behind MIM is not well explained, and previous studies insist that MIM primarily works for the Transformer family but is incompatible with CNNs. In this work, we observe that MIM essentially teaches the model to learn better middle-order interactions among patches for more generalized feature extraction. We then propose an Architecture-Agnostic Masked Image Modeling framework (A$^2$MIM), which is compatible with both Transformers and CNNs in a unified way. Extensive experiments on popular benchmarks show that A$^2$MIM learns better representations without explicit design and endows the backbone model with the stronger capability to transfer to various downstream tasks.
    
[^114]: 从人的物理反馈中学习：一种面向物体的单次自适应方法。

    Learning from Physical Human Feedback: An Object-Centric One-Shot Adaptation Method. (arXiv:2203.04951v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2203.04951](http://arxiv.org/abs/2203.04951)

    本文提出的Object Preference Adaptation (OPA)方法通过学习人类针对特定物体的反馈，在单次干预后进行机器人行为的自适应，提高了数据效率和适用性。

    

    为了让机器人能够在新环境和任务中有效地部署，它们必须能够理解人类在干预过程中表达的反馈。这可以纠正不良行为或指出其他偏好。现有的方法要么需要重复的交互回合，要么假定先前已知的奖励特征，这是数据效率低下并且很难转移到新任务的。我们通过在术语上将人类任务描述为以物体为中心的子任务，并将物理干预解释为与特定物体的关系来放松这些假设。我们的方法，Object Preference Adaptation (OPA)，由两个关键阶段组成：1）预训练基础策略以产生各种行为，以及2）根据人类反馈在线更新。我们快速而简单的自适应的关键在于固定了代理和物体之间的一般交互动态，只更新特定于物体的偏好。我们的自适应在线进行，只需要一个人类干预。

    For robots to be effectively deployed in novel environments and tasks, they must be able to understand the feedback expressed by humans during intervention. This can either correct undesirable behavior or indicate additional preferences. Existing methods either require repeated episodes of interactions or assume prior known reward features, which is data-inefficient and can hardly transfer to new tasks. We relax these assumptions by describing human tasks in terms of object-centric sub-tasks and interpreting physical interventions in relation to specific objects. Our method, Object Preference Adaptation (OPA), is composed of two key stages: 1) pre-training a base policy to produce a wide variety of behaviors, and 2) online-updating according to human feedback. The key to our fast, yet simple adaptation is that general interaction dynamics between agents and objects are fixed, and only object-specific preferences are updated. Our adaptation occurs online, requires only one human interve
    
[^115]: 奖励足以处理凸性MDP问题

    Reward is enough for convex MDPs. (arXiv:2106.00661v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2106.00661](http://arxiv.org/abs/2106.00661)

    本文研究了凸性马尔可夫决策过程，发现无法使用静态奖励函数表达目标，提出了一个元算法解决此问题，并统一了文献中的现有算法。

    

    在马尔可夫决策过程 (MDP) 中，最大化一个马尔可夫和平稳的累积奖励函数可以捕捉到许多目标。然而，并非所有目标都能以此方式捕获。本文研究了凸性MDPs，其中目标是作为静态分布的凸函数表达的，结果表明无法使用静态奖励函数来表达目标。我们将凸性MDP问题重新表述为政策和代价(负奖励)“玩家”的最小最大博弈，利用 Fenchel 对偶性，提出了一个解决此问题的元算法，并证明了它统一了文献中许多现有算法。

    Maximising a cumulative reward function that is Markov and stationary, i.e., defined over state-action pairs and independent of time, is sufficient to capture many kinds of goals in a Markov decision process (MDP). However, not all goals can be captured in this manner. In this paper we study convex MDPs in which goals are expressed as convex functions of the stationary distribution and show that they cannot be formulated using stationary reward functions. Convex MDPs generalize the standard reinforcement learning (RL) problem formulation to a larger framework that includes many supervised and unsupervised RL problems, such as apprenticeship learning, constrained MDPs, and so-called `pure exploration'. Our approach is to reformulate the convex MDP problem as a min-max game involving policy and cost (negative reward) `players', using Fenchel duality. We propose a meta-algorithm for solving this problem and show that it unifies many existing algorithms in the literature.
    
[^116]: 生成式演员-评论家算法: 使用推进模型的离线算法

    Generative Actor-Critic: An Off-policy Algorithm Using the Push-forward Model. (arXiv:2105.03733v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.03733](http://arxiv.org/abs/2105.03733)

    本文提出基于推进模型的无密度离线算法 GAC，使用最大均值差熵正则化器平衡探索和开发之间的关系，并使用自适应机制提高算法的稳定性和鲁棒性。实验结果表明，推进策略能够提高算法的探索效率和渐近性能。

    

    无模型深度强化学习在视频游戏、推荐系统和机器人控制任务等许多领域取得了巨大成功。在连续控制任务中，广泛使用的具有高斯分布的策略导致环境的探索低效，并且在许多情况下算法的性能受到限制。本文提出了一种无密度的离线算法，称为生成式演员-评论家（Generative Actor-Critic，GAC），使用推进模型来增加策略的表达能力。同时还包括一种熵类技术，称为最大均值差（Maximum Mean Discrepancy，MMD）熵正则化器，以平衡探索和开发之间的关系。此外，我们设计了一种自适应机制来自动缩放这个正则化器，进一步提高了GAC的稳定性和鲁棒性。实验结果表明，推进策略具有理想的特征，例如多模式性，可以明显提高算法的探索效率和渐近性能。

    Model-free deep reinforcement learning has achieved great success in many domains, such as video games, recommendation systems and robotic control tasks. In continuous control tasks, widely used policies with Gaussian distributions results in ineffective exploration of environments and limited performance of algorithms in many cases. In this paper, we propose a density-free off-policy algorithm, Generative Actor-Critic(GAC), using the push-forward model to increase the expressiveness of policies, which also includes an entropy-like technique, MMD-entropy regularizer, to balance the exploration and exploitation. Additionnally, we devise an adaptive mechanism to automatically scale this regularizer, which further improves the stability and robustness of GAC. The experiment results show that push-forward policies possess desirable features, such as multi-modality, which can improve the efficiency of exploration and asymptotic performance of algorithms obviously.
    
[^117]: QCBA: 通过恢复离散化过程中的信息改进基于数值数据学习的规则分类器

    QCBA: Improving Rule Classifiers Learned from Quantitative Data by Recovering Information Lost by Discretisation. (arXiv:1711.10166v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1711.10166](http://arxiv.org/abs/1711.10166)

    本文提出的QCBA方法可以用于改进基于数值类型数据学习的规则分类器，恢复预离散化过程中丢失的信息，并提出了新的剪枝技术。在22个数据集上的实验表明，FOIL2+QCBA相对其他基线方法而言，具有更高的预测性能和更小的模型大小。

    

    数值属性的预离散化是某些规则学习算法的必要步骤，但是会导致一些效率低下的问题。本文提出了新的规则调整步骤以恢复离散化中丢失的信息，并提出了新的剪枝技术，可以进一步减小规则模型的大小和提高其准确性。提出的QCBA方法最初是为了对基于关联性分类（CBA）算法生成的模型中的定量属性进行后处理，但也可应用于其他规则学习方法的结果。我们展示了对五个关联规则分类算法（CBA、CMAR、CPAR、IDS、SBRL）和两个一阶逻辑规则学习器（FOIL2和PRM）生成模型的后处理效果，使用UCI仓库的22个数据集进行了基准测试，结果表明FOIL2+QCBA相比于所有七个基线的大小更小，并且具有最佳的整体预测性能。后优化的CBA模型具有更好的预测能力。

    A prediscretisation of numerical attributes which is required by some rule learning algorithms is a source of inefficiencies. This paper describes new rule tuning steps that aim to recover lost information in the discretisation and new pruning techniques that may further reduce the size of rule models and improve their accuracy. The proposed QCBA method was initially developed to postprocess quantitative attributes in models generated by the Classification based on associations (CBA) algorithm, but it can also be applied to the results of other rule learning approaches. We demonstrate the effectiveness on the postprocessing of models generated by five association rule classification algorithms (CBA, CMAR, CPAR, IDS, SBRL) and two first-order logic rule learners (FOIL2 and PRM). Benchmarks on 22 datasets from the UCI repository show smaller size and the overall best predictive performance for FOIL2+QCBA compared to all seven baselines. Postoptimised CBA models have a better predictive p
    

