# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Information Cascade Prediction under Public Emergencies: A Survey](https://arxiv.org/abs/2404.01319) | 大数据时代带来了公共紧急事件下信息级联预测的机遇，但应用领域限制了跨学科预测方法的统一框架，该调查论文系统分类和总结了这一领域，为研究人员提供了前沿研究和未来方向。 |
| [^2] | [Towards LLM-RecSys Alignment with Textual ID Learning](https://arxiv.org/abs/2403.19021) | 通过提出IDGen，将每个推荐项目表示为独特、简洁、语义丰富的文本ID，从而使得基于大型语言模型的推荐更好地与自然语言生成对齐。 |
| [^3] | [AI Consciousness is Inevitable: A Theoretical Computer Science Perspective](https://arxiv.org/abs/2403.17101) | 通过理论计算机科学的视角，作者提出了一个简单却强大的机器模型，支持了机器意识是不可避免的这一论断。 |
| [^4] | [Towards auditory attention decoding with noise-tagging: A pilot study](https://arxiv.org/abs/2403.15523) | 这项试点研究首次尝试使用噪声标记刺激协议进行听觉注意力解码，取得了较高的性能表现。 |
| [^5] | [Exploiting Style Latent Flows for Generalizing Deepfake Detection Video Detection](https://arxiv.org/abs/2403.06592) | 该研究提出了一种利用样式潜在流进行深度伪造视频检测的新方法，通过分析样式潜在向量在生成视频中的异常行为来检测假视频，利用对比学习训练的StyleGRU模块和样式注意模块的组合，能有效检测视觉和时间异常。 |
| [^6] | [Dynamic Explanation Selection Towards Successful User-Decision Support with Explainable AI](https://arxiv.org/abs/2402.18016) | 该论文提出了一种名为X-Selector的方法，通过动态选择解释，预测不同解释组合对用户决策的影响，从而引导用户做出更好的决策。 |
| [^7] | [Biomedical Entity Linking as Multiple Choice Question Answering](https://arxiv.org/abs/2402.15189) | 提出了一种新颖的模型BioELQA，将生物医学实体链接看作是多项选择问答，通过使用快速检索器获得候选实体，实现了更好的实体链接效果。 |
| [^8] | [LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study](https://arxiv.org/abs/2402.13457) | 本研究对LLM模型的越狱攻击和防御技术进行了全面研究，揭示了现有攻击和防御技术的有效性。 |
| [^9] | [GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving](https://arxiv.org/abs/2402.10104) | GeoEval基准测试用于评估LLMs和MMs在几何问题解决上的性能，发现WizardMath模型在主要子集上表现出色，但在具有挑战性的子集上准确率较低。 |
| [^10] | [Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias](https://arxiv.org/abs/2402.07166) | 本研究提供了人工智能和大型语言模型发展的全景视图，并指出了毒性、偏见和其他问题，同时强调人类大脑并不特殊，人类智能只是一个尺度上的新兴现象。 |
| [^11] | [Estimating the Effect of Crosstalk Error on Circuit Fidelity Using Noisy Intermediate-Scale Quantum Devices](https://arxiv.org/abs/2402.06952) | 本研究对噪声中等规模量子计算机上的串扰误差进行了全面分析，发现并行指令之间的串扰可能会破坏量子状态并导致程序执行错误 |
| [^12] | [EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss](https://arxiv.org/abs/2402.05008) | 高效ViT-SAM是一种新的加速的段落任意模型，通过保留轻量级提示编码器和掩码解码器，并替换沉重的图像编码器，实现了48.9倍的加速而不牺牲性能。 |
| [^13] | [CapHuman: Capture Your Moments in Parallel Universes](https://arxiv.org/abs/2402.00627) | CapHuman是一个新框架，通过“编码然后学习对齐”的范式实现了可泛化的身份保留能力，用于在不同情境中生成具有多样的头部位置、姿势和面部表情的特定个体图像。 |
| [^14] | [Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion](https://arxiv.org/abs/2401.17583) | 本文介绍了一种名为敏捷但安全（ABS）的学习控制框架，能够实现四足机器人的敏捷且无碰撞行走。该框架通过一个学习得到的控制论到达-避免值网络来实现策略切换，并通过协作运行的敏捷策略和恢复策略，使机器人能够高速且安全地导航。 |
| [^15] | [Are self-explanations from Large Language Models faithful?.](http://arxiv.org/abs/2401.07927) | 大型语言模型的自我解释是否可靠是一个重要的AI安全考虑因素，我们提出使用自洽性检测作为评估其可靠性和解释能力的方法。 |
| [^16] | [AdaptiX -- A Transitional XR Framework for Development and Evaluation of Shared Control Applications in Assistive Robotics.](http://arxiv.org/abs/2310.15887) | AdaptiX是一个过渡性的XR框架，用于开发和评估助动力机器人中的共同控制应用。它提供了一个高分辨率仿真环境，并结合了用户自主性和计算机辅助。 |
| [^17] | [Identifiability of total effects from abstractions of time series causal graphs.](http://arxiv.org/abs/2310.14691) | 本文研究了基于因果图抽象从观测时间序列中识别干预总效应的问题，并证明了在扩展摘要因果图中总效应总是可识别的。同时，我们提供了摘要因果图中总效应可识别的必要和充分的图形条件，并提供了调整集合以估计总效应。 |
| [^18] | [Making Multimodal Generation Easier: When Diffusion Models Meet LLMs.](http://arxiv.org/abs/2310.08949) | EasyGen是一个有效的模型，它通过结合扩散模型和大型语言模型（LLMs）的能力，实现了更容易的多模态生成。相比现有的模型，EasyGen使用了一个名为BiDiffuser的双向条件扩散模型， 提供了更高效的模态交互，并且不仅能够生成文本回复，还能够促进文本到图像的生成。 |
| [^19] | [Identifying the Risks of LM Agents with an LM-Emulated Sandbox.](http://arxiv.org/abs/2309.15817) | 通过使用LM模拟工具执行和开发基于LM的自动安全评估器，该论文提出了一种解决测试LM代理的高成本和寻找高风险问题的方法。 |
| [^20] | [Large language models can accurately predict searcher preferences.](http://arxiv.org/abs/2309.10621) | 大型语言模型可以通过从真实用户那里获取高质量的第一方数据来准确预测搜索者的偏好。 |
| [^21] | [Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs.](http://arxiv.org/abs/2309.05516) | 本文提出一种名为SignRound的优化权重舍入的方法，通过使用有符号梯度进行轻量级分块调整，解决了大型语言模型(LLMs)的量化挑战。 |
| [^22] | [REB: Reducing Biases in Representation for Industrial Anomaly Detection.](http://arxiv.org/abs/2308.12577) | 本文提出了一种名为REB的方法，通过考虑领域偏差和构建自监督学习任务，以及使用本地密度K最近邻方法，从而降低工业异常检测中的表示偏差，并取得了显著的改进。 |
| [^23] | [ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base.](http://arxiv.org/abs/2305.05994) | 本文提出了ANALOGYKB，一种使用百万规模知识库的类比推理方法，能够使语言模型在类比推理任务上取得比之前的最先进方法更好的结果。 |
| [^24] | [Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with Bridge Block Reconstruction.](http://arxiv.org/abs/2303.12557) | 本文针对视觉Transformer在移动设备上计算要求高的问题，提出了一种带桥块重构的混合视觉Transformer的后训练量化方法，提高其在移动设备上的加速效果。 |
| [^25] | [Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification.](http://arxiv.org/abs/2303.12307) | 本文提出了一种曲率平衡特征流形学习的方法，探究了感知流形的几何特性对分类难度的影响，发现曲率不平衡会导致模型不公平。 |

# 详细

[^1]: 公共紧急事件下信息级联预测：一项调查

    Information Cascade Prediction under Public Emergencies: A Survey

    [https://arxiv.org/abs/2404.01319](https://arxiv.org/abs/2404.01319)

    大数据时代带来了公共紧急事件下信息级联预测的机遇，但应用领域限制了跨学科预测方法的统一框架，该调查论文系统分类和总结了这一领域，为研究人员提供了前沿研究和未来方向。

    

    随着大数据时代的到来，海量信息、专家经验和高准确度模型为公共紧急事件下信息级联预测带来了巨大机遇。然而，来自各个学科的专业知识的参与导致了信息级联预测主要集中在特定应用领域（如地震、洪灾、传染病）上。缺乏一个统一的预测框架对跨不同应用领域中的交叉预测方法的分类构成了挑战。本调查论文提供了信息级联建模、预测和应用的系统分类和总结。我们旨在帮助研究人员识别前沿研究，并理解公共紧急事件下信息级联预测的模型和方法。通过总结待解决问题并概述该领域的未来方向，本文对该领域的信息级联预测具有重要的参考价值。

    arXiv:2404.01319v1 Announce Type: cross  Abstract: With the advent of the era of big data, massive information, expert experience, and high-accuracy models bring great opportunities to the information cascade prediction of public emergencies. However, the involvement of specialist knowledge from various disciplines has resulted in a primarily application-specific focus (e.g., earthquakes, floods, infectious diseases) for information cascade prediction of public emergencies. The lack of a unified prediction framework poses a challenge for classifying intersectional prediction methods across different application fields. This survey paper offers a systematic classification and summary of information cascade modeling, prediction, and application. We aim to help researchers identify cutting-edge research and comprehend models and methods of information cascade prediction under public emergencies. By summarizing open issues and outlining future directions in this field, this paper has the p
    
[^2]: 朝向LLM-RecSys对齐与文本ID学习的方向

    Towards LLM-RecSys Alignment with Textual ID Learning

    [https://arxiv.org/abs/2403.19021](https://arxiv.org/abs/2403.19021)

    通过提出IDGen，将每个推荐项目表示为独特、简洁、语义丰富的文本ID，从而使得基于大型语言模型的推荐更好地与自然语言生成对齐。

    

    基于大型语言模型(LLMs)的生成式推荐已经将传统的基于排名的推荐方式转变为文本生成范例。然而，与固有操作人类词汇的标准NLP任务相反，目前生成式推荐领域的研究在如何在文本生成范式中以简洁而有意义的ID表示有效编码推荐项目方面存在困难。为了更好地对齐LLMs与推荐需求，我们提出了IDGen，使用人类语言标记将每个项目表示为独特、简洁、语义丰富、与平台无关的文本ID。这通过在基于LLM的推荐系统旁训练文本ID生成器来实现，使个性化推荐能够无缝集成到自然语言生成中。值得注意的是，由于用户历史记录以自然语言表达并与原始数据集解耦，我们的方法提出了潜在的

    arXiv:2403.19021v1 Announce Type: cross  Abstract: Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potenti
    
[^3]: 人工智能意识是不可避免的：一个理论计算机科学的视角

    AI Consciousness is Inevitable: A Theoretical Computer Science Perspective

    [https://arxiv.org/abs/2403.17101](https://arxiv.org/abs/2403.17101)

    通过理论计算机科学的视角，作者提出了一个简单却强大的机器模型，支持了机器意识是不可避免的这一论断。

    

    我们通过理论计算机科学的视角来审视意识，这是数学的一个分支，研究在资源限制下的计算。从这个角度出发，我们为意识开发了一个形式化的机器模型。这个模型受到了艾伦·图灵简单而强大的计算模型和伯纳德·巴尔斯意识剧场模型的启发。尽管非常简单，这个模型在高层次上与许多关于人类和动物意识的主要科学理论相一致，支持我们的论断：机器意识是不可避免的。

    arXiv:2403.17101v1 Announce Type: new  Abstract: We look at consciousness through the lens of Theoretical Computer Science, a branch of mathematics that studies computation under resource limitations. From this perspective, we develop a formal machine model for consciousness. The model is inspired by Alan Turing's simple yet powerful model of computation and Bernard Baars' theater model of consciousness. Though extremely simple, the model aligns at a high level with many of the major scientific theories of human and animal consciousness, supporting our claim that machine consciousness is inevitable.
    
[^4]: 采用噪声标记的听觉注意力解码研究：一项试点研究

    Towards auditory attention decoding with noise-tagging: A pilot study

    [https://arxiv.org/abs/2403.15523](https://arxiv.org/abs/2403.15523)

    这项试点研究首次尝试使用噪声标记刺激协议进行听觉注意力解码，取得了较高的性能表现。

    

    听觉注意力解码(AAD)旨在从大脑活动中提取被关注的说话者，提供了神经导向听觉设备和脑机接口等领域的应用前景。本试点研究首次尝试使用噪声标记刺激协议进行AAD，该协议引发了可靠的编码调制诱发电位，但在听觉模式下的探索还很有限。研究参与者依次呈现两个荷兰语言语音刺激，这些刺激被幅度调制为具有唯一二进制伪随机噪声码，有效地为其标记了附加可解码信息。我们比较了未调制音频与使用不同调制深度调制的音频的解码，以及传统AAD方法与标准解码噪声码方法的对比。我们的试点研究发现，与未调制音频相比，70至100%的调制深度的传统方法表现出更高的性能。

    arXiv:2403.15523v1 Announce Type: cross  Abstract: Auditory attention decoding (AAD) aims to extract from brain activity the attended speaker amidst candidate speakers, offering promising applications for neuro-steered hearing devices and brain-computer interfacing. This pilot study makes a first step towards AAD using the noise-tagging stimulus protocol, which evokes reliable code-modulated evoked potentials, but is minimally explored in the auditory modality. Participants were sequentially presented with two Dutch speech stimuli that were amplitude modulated with a unique binary pseudo-random noise-code, effectively tagging these with additional decodable information. We compared the decoding of unmodulated audio against audio modulated with various modulation depths, and a conventional AAD method against a standard method to decode noise-codes. Our pilot study revealed higher performances for the conventional method with 70 to 100 percent modulation depths compared to unmodulated au
    
[^5]: 利用样式潜在流来推广深度伪造视频检测

    Exploiting Style Latent Flows for Generalizing Deepfake Detection Video Detection

    [https://arxiv.org/abs/2403.06592](https://arxiv.org/abs/2403.06592)

    该研究提出了一种利用样式潜在流进行深度伪造视频检测的新方法，通过分析样式潜在向量在生成视频中的异常行为来检测假视频，利用对比学习训练的StyleGRU模块和样式注意模块的组合，能有效检测视觉和时间异常。

    

    本文提出了一种基于分析样式潜在向量及其在生成视频的时间变化中异常行为的检测假视频的新方法。我们发现生成的面部视频在样式潜在向量的时间变化中存在时间上的独特性，这在生成具有各种面部表情和几何变换的时间稳定视频时是不可避免的。我们的框架利用了通过对比学习训练的StyleGRU模块来表示样式潜在向量的动态特性。另外，我们引入了一个样式注意模块，将StyleGRU生成的特征与基于内容的特征相结合，实现对视觉和时间异常的检测。我们在深度伪造检测的各种基准情景下展示了我们的方法，展示了它在跨数据集和跨操作情景中的优越性。

    arXiv:2403.06592v1 Announce Type: cross  Abstract: This paper presents a new approach for the detection of fake videos, based on the analysis of style latent vectors and their abnormal behavior in temporal changes in the generated videos. We discovered that the generated facial videos suffer from the temporal distinctiveness in the temporal changes of style latent vectors, which are inevitable during the generation of temporally stable videos with various facial expressions and geometric transformations. Our framework utilizes the StyleGRU module, trained by contrastive learning, to represent the dynamic properties of style latent vectors. Additionally, we introduce a style attention module that integrates StyleGRU-generated features with content-based features, enabling the detection of visual and temporal artifacts. We demonstrate our approach across various benchmark scenarios in deepfake detection, showing its superiority in cross-dataset and cross-manipulation scenarios. Through f
    
[^6]: 动态解释选择：实现可解释AI的成功用户决策支持

    Dynamic Explanation Selection Towards Successful User-Decision Support with Explainable AI

    [https://arxiv.org/abs/2402.18016](https://arxiv.org/abs/2402.18016)

    该论文提出了一种名为X-Selector的方法，通过动态选择解释，预测不同解释组合对用户决策的影响，从而引导用户做出更好的决策。

    

    本文解决了如何为基于可解释AI的智能决策支持系统(IDSSs)选择解释的问题。IDSSs通过可解释AI生成的解释以及AI预测展示了提高用户决策的潜力。由于可解释AI的发展提供了各种解释，我们认为如果能够有策略地选择指导用户做出更好决策的解释，IDSSs 的性能将得到极大提升。本文提出了X-Selector，一种动态选择解释的方法。X-Selector的目标是通过预测不同解释组合对用户决策的影响，引导用户做出更好的决策。我们将X-Selector的性能与两种朴素策略（所有可能的解释和仅针对最可能预测的解释）、以及两种基线方法（无解释和无AI支持）进行了比较。结果表明X-Selector有潜力引导用户做出推荐的决策。

    arXiv:2402.18016v1 Announce Type: cross  Abstract: This paper addresses the problem of how to select explanations for XAI (Explainable AI)-based Intelligent Decision Support Systems (IDSSs). IDSSs have shown promise in improving user decisions through XAI-generated explanations along with AI predictions. As the development of XAI made various explanations available, we believe that IDSSs can be greatly improved if they can strategically select explanations that guide users to better decisions. This paper proposes X-Selector, a method for dynamically selecting explanations. X-Selector aims to guide users to better decisions by predicting the impact of different combinations of explanations on user decisions. We compared X-Selector's performance with two naive strategies (all possible explanations and explanations only for the most likely prediction) and two baselines (no explanation and no AI support). The results suggest the potential of X-Selector to guide users to recommended decisio
    
[^7]: 将生物医学实体链接视为多项选择问答

    Biomedical Entity Linking as Multiple Choice Question Answering

    [https://arxiv.org/abs/2402.15189](https://arxiv.org/abs/2402.15189)

    提出了一种新颖的模型BioELQA，将生物医学实体链接看作是多项选择问答，通过使用快速检索器获得候选实体，实现了更好的实体链接效果。

    

    尽管预训练语言模型在生物医学实体链接（BioEL）方面取得了显著进展，但对于细粒度和长尾实体仍然存在挑战。为了解决这些挑战，我们提出了BioELQA，这是一种将生物医学实体链接视为多项选择问答的新颖模型。BioELQA首先利用快速检索器获得候选实体，将提及和候选实体共同呈现给生成器，然后输出与其选定实体相关的预测符号。这种公式使得不同候选实体之间的明确比较成为可能，从而捕捉了提及和实体之间以及实体之间的精细交互。为了改善长尾实体的泛化能力，我们检索相似的已标记训练实例作为线索，并将输入与检索实例连接到生成器。广泛的实验结果表明，BioELQA的表现优于统计结果。

    arXiv:2402.15189v1 Announce Type: cross  Abstract: Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering. BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms stat
    
[^8]: LLM越狱攻击与防御技术—一项全面研究

    LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study

    [https://arxiv.org/abs/2402.13457](https://arxiv.org/abs/2402.13457)

    本研究对LLM模型的越狱攻击和防御技术进行了全面研究，揭示了现有攻击和防御技术的有效性。

    

    大型语言模型(LLMs)越来越成为产生具有潜在社会影响内容的核心。值得注意的是，这些模型展示了生成可能被视为有害的内容的能力。为了减轻这些风险，研究人员采用了安全训练技术，以使模型输出与社会价值观保持一致，以遏制对恶意内容的生成。然而，“越狱”现象，即精心制作的提示引发模型产生有害回应的情况，仍然是一个重要挑战。本研究对现有关于越狱LLMs及其防御技术的研究进行了全面分析。我们对三种不同语言模型的九种攻击技术和七种防御技术进行了细致调查：Vicuna、LLama和GPT-3.5 Turbo。我们旨在评估这些攻击和防御技术的有效性。我们的研究结果表明，现有的白盒攻击u

    arXiv:2402.13457v1 Announce Type: cross  Abstract: Large Language Models (LLMS) have increasingly become central to generating content with potential societal impacts. Notably, these models have demonstrated capabilities for generating content that could be deemed harmful. To mitigate these risks, researchers have adopted safety training techniques to align model outputs with societal values to curb the generation of malicious content. However, the phenomenon of "jailbreaking", where carefully crafted prompts elicit harmful responses from models, persists as a significant challenge. This research conducts a comprehensive analysis of existing studies on jailbreaking LLMs and their defense techniques. We meticulously investigate nine attack techniques and seven defense techniques applied across three distinct language models: Vicuna, LLama, and GPT-3.5 Turbo. We aim to evaluate the effectiveness of these attack and defense techniques. Our findings reveal that existing white-box attacks u
    
[^9]: GeoEval：用于评估LLMs和多模型在几何问题解决上的基准

    GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving

    [https://arxiv.org/abs/2402.10104](https://arxiv.org/abs/2402.10104)

    GeoEval基准测试用于评估LLMs和MMs在几何问题解决上的性能，发现WizardMath模型在主要子集上表现出色，但在具有挑战性的子集上准确率较低。

    

    近期在大型语言模型（LLMs）和多模型（MMs）方面的进展展示了它们在问题解决方面的卓越能力。然而，它们在处理几何数学问题方面的熟练程度，即需要综合理解文本和视觉信息，尚未得到彻底评估。为了填补这一空白，我们推出了GeoEval基准测试，这是一个全面的集合，包括一个主要子集合的2000个问题，一个重点关注反推理的750个问题子集合，一个增强子集合的2000个问题以及一个难题子集合的300个问题。这个基准测试有助于更深入地研究LLMs和MMs在解决几何数学问题时的性能。我们对十个LLMs和MMs在这些不同子集上的评估结果显示，WizardMath模型表现出色，在主要子集上达到55.67%的准确率，但在具有挑战性的子集上只有6.00%的准确率。这突出了关键的需求。

    arXiv:2402.10104v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) and Multi-Modal Models (MMs) have demonstrated their remarkable capabilities in problem-solving. Yet, their proficiency in tackling geometry math problems, which necessitates an integrated understanding of both textual and visual information, has not been thoroughly evaluated. To address this gap, we introduce the GeoEval benchmark, a comprehensive collection that includes a main subset of 2000 problems, a 750 problem subset focusing on backward reasoning, an augmented subset of 2000 problems, and a hard subset of 300 problems. This benchmark facilitates a deeper investigation into the performance of LLMs and MMs on solving geometry math problems. Our evaluation of ten LLMs and MMs across these varied subsets reveals that the WizardMath model excels, achieving a 55.67\% accuracy rate on the main subset but only a 6.00\% accuracy on the challenging subset. This highlights the critical ne
    
[^10]: 发表文本的社会演化与通过大型语言模型出现的人工智能与毒性和偏见的问题

    Social Evolution of Published Text and The Emergence of Artificial Intelligence Through Large Language Models and The Problem of Toxicity and Bias

    [https://arxiv.org/abs/2402.07166](https://arxiv.org/abs/2402.07166)

    本研究提供了人工智能和大型语言模型发展的全景视图，并指出了毒性、偏见和其他问题，同时强调人类大脑并不特殊，人类智能只是一个尺度上的新兴现象。

    

    我们提供了人工智能和深度学习的快速发展的全景视图，这导致了人工智能在大型语言模型中的突破性出现。本研究的目的是将所有这些发展置于实用的广泛历史社会视角中，既不夸大其效果，又不产生1970年代到1990年代人工智能冬季的悲观情绪。同时，我们还指出存在毒性、偏见、记忆、谄媚、逻辑不一致和幻觉等问题，只是作为对过于乐观的警示。我们在此指出，正如人工智能的出现似乎发生在神经连接或权重数量的临界点上，人类大脑尤其是皮质区域并没有什么特殊或超凡，只是灵长类大脑的一个放大版，甚至人类智能似乎只是一个尺度上的新兴现象。

    We provide a birds eye view of the rapid developments in AI and Deep Learning that has led to the path-breaking emergence of AI in Large Language Models. The aim of this study is to place all these developments in a pragmatic broader historical social perspective without any exaggerations while at the same time without any pessimism that created the AI winter in the 1970s to 1990s. We also at the same time point out toxicity, bias, memorization, sycophancy, logical inconsistencies, hallucinations that exist just as a warning to the overly optimistic. We note here that just as this emergence of AI seems to occur at a threshold point in the number of neural connections or weights, it has also been observed that human brain and especially the cortex region is nothing special or extraordinary but simply a case of scaled-up version of the primate brain and that even the human intelligence seems like an emergent phenomena of scale.
    
[^11]: 用噪声中等规模量子设备估计串扰误差对电路保真度的影响

    Estimating the Effect of Crosstalk Error on Circuit Fidelity Using Noisy Intermediate-Scale Quantum Devices

    [https://arxiv.org/abs/2402.06952](https://arxiv.org/abs/2402.06952)

    本研究对噪声中等规模量子计算机上的串扰误差进行了全面分析，发现并行指令之间的串扰可能会破坏量子状态并导致程序执行错误

    

    当前技术的进步使量子计算社区关注于探索近期设备的潜力，这些设备在实际应用中的计算能力超过了经典计算机。一个未解决的核心问题是这些设备中固有噪声是否可以被克服，或者任何潜在的量子优势是否受到限制。毫无疑问，串扰是噪声中等规模量子系统中的主要来源之一，它对硬件设计构成了根本性挑战。并行指令之间的串扰可以破坏量子状态并导致程序执行错误。在这项研究中，我们对串扰误差对噪声中等规模量子计算机的影响进行了全面分析。我们的方法非常简单和实用，可用于表征多比特设备的串扰误差。特别地，我们结合了随机化基准测试和同时随机化的方法

    Current advancements in technology have focused the attention of the quantum computing community toward exploring the potential of near-term devices whose computing power surpasses that of classical computers in practical applications. An unresolved central question revolves around whether the inherent noise in these devices can be overcome or whether any potential quantum advantage would be limited. There is no doubt that crosstalk is one of the main sources of noise in noisy intermediate-scale quantum (NISQ) systems, and it poses a fundamental challenge to hardware designs. Crosstalk between parallel instructions can corrupt quantum states and cause incorrect program execution. In this study, we present a comprehensive analysis of the crosstalk error effect on NISQ computers. Our approach is extremely straightforward and practical for characterizing the crosstalk error of various multi-qubit devices. In particular, we combine the randomized benchmarking (RB) and simultaneous randomiz
    
[^12]: 高效ViT-SAM: 在不损失性能的情况下加速的段落任意模型

    EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss

    [https://arxiv.org/abs/2402.05008](https://arxiv.org/abs/2402.05008)

    高效ViT-SAM是一种新的加速的段落任意模型，通过保留轻量级提示编码器和掩码解码器，并替换沉重的图像编码器，实现了48.9倍的加速而不牺牲性能。

    

    我们提出了一种新的加速段落任意模型——高效ViT-SAM。我们保留了SAM的轻量级提示编码器和掩码解码器，同时用高效ViT替换了沉重的图像编码器。在训练方面，我们首先从SAM-ViT-H图像编码器到高效ViT进行知识蒸馏。随后，我们对SA-1B数据集进行端到端训练。由于高效ViT的效率和容量，高效ViT-SAM在A100 GPU上相比SAM-ViT-H实现了48.9倍的TensorRT加速，而无需牺牲性能。我们的代码和预训练模型已在https://github.com/mit-han-lab/efficientvit发布。

    We present EfficientViT-SAM, a new family of accelerated segment anything models. We retain SAM's lightweight prompt encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset. Benefiting from EfficientViT's efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance. Our code and pre-trained models are released at https://github.com/mit-han-lab/efficientvit.
    
[^13]: CapHuman: 在平行宇宙中捕捉你的瞬间

    CapHuman: Capture Your Moments in Parallel Universes

    [https://arxiv.org/abs/2402.00627](https://arxiv.org/abs/2402.00627)

    CapHuman是一个新框架，通过“编码然后学习对齐”的范式实现了可泛化的身份保留能力，用于在不同情境中生成具有多样的头部位置、姿势和面部表情的特定个体图像。

    

    我们关注一种新颖的以人为中心的图像合成任务，即仅给定一个参考面部照片，期望能够在不同情境中生成具有多样的头部位置、姿势和面部表情的特定个体图像。为了实现这一目标，我们认为我们的生成模型应具备以下有利特征：（1）对世界和人类社会有强大的视觉和语义理解，用于基本物体和人类图像的生成；（2）可泛化的身份保留能力；（3）灵活细粒度的头部控制。最近，大型预训练的文本到图像扩散模型展现了显著的结果，成为一个强大的生成基础。基于此，我们旨在释放预训练模型的上述两种能力。在这项工作中，我们提出了一个名为CapHuman的新框架。我们采用了“编码然后学习对齐”的范式，为新个体实现了可泛化的身份保留能力。

    We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ``encode then learn to align" paradigm, which enables generalizable identity preservation for new individuals without cum
    
[^14]: 敏捷但安全：学习无碰撞高速四足机器人行走

    Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion

    [https://arxiv.org/abs/2401.17583](https://arxiv.org/abs/2401.17583)

    本文介绍了一种名为敏捷但安全（ABS）的学习控制框架，能够实现四足机器人的敏捷且无碰撞行走。该框架通过一个学习得到的控制论到达-避免值网络来实现策略切换，并通过协作运行的敏捷策略和恢复策略，使机器人能够高速且安全地导航。

    

    在杂乱环境中行走的四足机器人必须既敏捷以提高任务执行效率，又要确保安全，避免与障碍物或人碰撞。现有的研究要么开发保守的控制器（速度小于1.0 m/s）以确保安全，要么专注于敏捷性而未考虑潜在致命的碰撞。本文介绍了敏捷但安全（ABS）的学习控制框架，为四足机器人实现了敏捷且无碰撞的行走。ABS包括一个敏捷策略来在障碍物中执行灵活的动作技能，并且有一个恢复策略来避免失败，共同实现高速且无碰撞的导航。ABS中的策略切换由一个学习得到的控制论到达-避免值网络控制，该网络也指导恢复策略作为目标函数，从而在闭环中保护机器人。训练过程涉及敏捷策略、到达-避免值网络、恢复策略和外感知表征的学习。

    Legged robots navigating cluttered environments must be jointly agile for efficient task execution and safe to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (< 1.0 m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an exteroception repre
    
[^15]: 大型语言模型的自我解释是否可靠?

    Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.07927](http://arxiv.org/abs/2401.07927)

    大型语言模型的自我解释是否可靠是一个重要的AI安全考虑因素，我们提出使用自洽性检测作为评估其可靠性和解释能力的方法。

    

    经过训练的大型语言模型在许多任务上表现出色，甚至能够提供其行为的解释。由于这些模型对公众是直接可访问的，因此存在这样的风险，即令人信服但错误的解释可能导致对大型语言模型的无支撑的自信。因此，解释能力和可靠性是AI安全的重要考虑因素。评估自我解释的可靠性和可解释性是一项具有挑战性的任务，因为这些模型对于人类来说过于复杂，无法注释什么是正确的解释。为了解决这个问题，我们提出使用自洽性检测作为可靠性的衡量指标。例如，如果一个大型语言模型说某组词对于做出预测很重要，那么在没有这些词的情况下，它应该无法做出相同的预测。虽然自洽性检测是一种常见的可靠性方法，但之前尚未应用于大型语言模型的自我解释中。我们将自洽性检测应用于...

    Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to t
    
[^16]: AdaptiX - 一个用于助动力机器人中开发和评估共同控制应用的过渡性XR框架

    AdaptiX -- A Transitional XR Framework for Development and Evaluation of Shared Control Applications in Assistive Robotics. (arXiv:2310.15887v1 [cs.HC])

    [http://arxiv.org/abs/2310.15887](http://arxiv.org/abs/2310.15887)

    AdaptiX是一个过渡性的XR框架，用于开发和评估助动力机器人中的共同控制应用。它提供了一个高分辨率仿真环境，并结合了用户自主性和计算机辅助。

    

    随着人们授权行动受限和技术接受度的提高，如合作机器臂等助动技术正在变得流行。然而，它们的普及成功受到可用性问题的限制，尤其是用户输入与软件控制在自主性连续性方面的差异。为了解决这个问题，共同控制概念提供了将有针对性地增加用户自主性与一定程度的计算机辅助相结合的机会。本文介绍了一个名为AdaptiX的免费开源XR框架，用于在高分辨率仿真环境中开发和评估共同控制应用。初始框架包括一个虚拟现实（VR）中的示例情景下的模拟机器臂、多种标准控制接口和一个专门的记录/回放系统。AdaptiX可以轻松扩展以满足特定的研究需求，允许进行人机交互（HRI）研究。

    With the ongoing efforts to empower people with mobility impairments and the increase in technological acceptance by the general public, assistive technologies, such as collaborative robotic arms, are gaining popularity. Yet, their widespread success is limited by usability issues, specifically the disparity between user input and software control along the autonomy continuum. To address this, shared control concepts provide opportunities to combine the targeted increase of user autonomy with a certain level of computer assistance. This paper presents the free and open-source AdaptiX XR framework for developing and evaluating shared control applications in a high-resolution simulation environment. The initial framework consists of a simulated robotic arm with an example scenario in Virtual Reality (VR), multiple standard control interfaces, and a specialized recording/replay system. AdaptiX can easily be extended for specific research needs, allowing Human-Robot Interaction (HRI) resea
    
[^17]: 时序因果图的抽象中总效应的可识别性研究

    Identifiability of total effects from abstractions of time series causal graphs. (arXiv:2310.14691v2 [math.ST] CROSS LISTED)

    [http://arxiv.org/abs/2310.14691](http://arxiv.org/abs/2310.14691)

    本文研究了基于因果图抽象从观测时间序列中识别干预总效应的问题，并证明了在扩展摘要因果图中总效应总是可识别的。同时，我们提供了摘要因果图中总效应可识别的必要和充分的图形条件，并提供了调整集合以估计总效应。

    

    我们研究了仅基于系统的因果图抽象从观测时间序列中识别干预总效应的问题。具体而言，我们考虑了两种类型的抽象：扩展摘要因果图将所有滞后因果关系混淆在一起，但区分滞后和瞬时关系；而摘要因果图则不提供任何关于因果关系滞后的指示。我们证明扩展摘要因果图中总效应总是可识别的，并且我们提供了摘要因果图中的可识别性所必需和充分的图形条件。此外，在总效应可识别时，我们提供了用于估计总效应的调整集合。

    We study the problem of identifiability of the total effect of an intervention from observational time series only given an abstraction of the causal graph of the system. Specifically, we consider two types of abstractions: the extended summary causal graph which conflates all lagged causal relations but distinguishes between lagged and instantaneous relations; and the summary causal graph which does not give any indication about the lag between causal relations. We show that the total effect is always identifiable in extended summary causal graphs and we provide necessary and sufficient graphical conditions for identifiability in summary causal graphs. Furthermore, we provide adjustment sets allowing to estimate the total effect whenever it is identifiable.
    
[^18]: Easier Multimodal Generation: Diffusion Models Meet LLMs

    Making Multimodal Generation Easier: When Diffusion Models Meet LLMs. (arXiv:2310.08949v1 [cs.AI])

    [http://arxiv.org/abs/2310.08949](http://arxiv.org/abs/2310.08949)

    EasyGen是一个有效的模型，它通过结合扩散模型和大型语言模型（LLMs）的能力，实现了更容易的多模态生成。相比现有的模型，EasyGen使用了一个名为BiDiffuser的双向条件扩散模型， 提供了更高效的模态交互，并且不仅能够生成文本回复，还能够促进文本到图像的生成。

    

    我们提出了EasyGen，一个有效的模型，通过利用扩散模型和大型语言模型（LLMs）的能力，增强了多模态理解和生成。不同于现有的主要依赖于编码器如CLIP或ImageBind，并且需要大量训练数据来桥接模态之间差距的多模态模型，EasyGen基于一个名为BiDiffuser的双向条件扩散模型构建，促进了更高效的模态交互。EasyGen通过简单的投影层将BiDiffuser和LLM进行集成，处理图像到文本的生成。与大多数现有的限于生成文本回复的多模态模型不同，EasyGen还可以通过利用LLM创建文本描述，并由BiDiffuser解释生成适当的视觉回复来促进文本到图像的生成。广泛的定量和定性实验证明了EasyGen的有效性，其训练可以...

    We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge the gap between modalities, EasyGen is built upon a bidirectional conditional diffusion model named BiDiffuser, which promotes more efficient interactions between modalities. EasyGen handles image-to-text generation by integrating BiDiffuser and an LLM via a simple projection layer. Unlike most existing multimodal models that are limited to generating text responses, EasyGen can also facilitate text-to-image generation by leveraging the LLM to create textual descriptions, which can be interpreted by BiDiffuser to generate appropriate visual responses. Extensive quantitative and qualitative experiments demonstrate the effectiveness of EasyGen, whose training can b
    
[^19]: 使用LM模拟沙盒识别LM代理的风险

    Identifying the Risks of LM Agents with an LM-Emulated Sandbox. (arXiv:2309.15817v1 [cs.AI])

    [http://arxiv.org/abs/2309.15817](http://arxiv.org/abs/2309.15817)

    通过使用LM模拟工具执行和开发基于LM的自动安全评估器，该论文提出了一种解决测试LM代理的高成本和寻找高风险问题的方法。

    

    最近的语言模型（LM）代理和工具使用的技术进步，例如ChatGPT插件，使得代理具备了丰富的功能，但也放大了潜在的风险，如泄露私人数据或引发财务损失。识别这些风险是一项耗时的工作，需要实施工具，手动设置每个测试场景的环境，并找到风险案例。随着工具和代理变得越来越复杂，测试这些代理的高成本将使寻找高风险、长尾风险变得越来越困难。为了解决这些挑战，我们引入了ToolEmu：一个使用LM来模拟工具执行的框架，可以在不需要手动实例化的情况下对LM代理进行各种工具和场景的测试。除了模拟器，我们还开发了一个基于LM的自动安全评估器，用于检查代理的失败并量化相关风险。我们通过人工评估测试了工具模拟器和评估器，并发现了6个...

    Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, manually setting up the environment for each test scenario, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tailed risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables the testing of LM agents against a diverse range of tools and scenarios, without manual instantiation. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 6
    
[^20]: 大型语言模型能够准确预测搜索者的偏好

    Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])

    [http://arxiv.org/abs/2309.10621](http://arxiv.org/abs/2309.10621)

    大型语言模型可以通过从真实用户那里获取高质量的第一方数据来准确预测搜索者的偏好。

    

    相关性标签是评估和优化搜索系统的关键。获取大量相关性标签通常需要第三方标注人员，但存在低质量数据的风险。本论文介绍了一种改进标签质量的替代方法，通过从真实用户那里获得仔细反馈来获取高质量的第一方数据。

    Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
    
[^21]: 通过有符号梯度下降优化LLMs量化中的权重舍入

    Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05516](http://arxiv.org/abs/2309.05516)

    本文提出一种名为SignRound的优化权重舍入的方法，通过使用有符号梯度进行轻量级分块调整，解决了大型语言模型(LLMs)的量化挑战。

    

    大型语言模型(LLMs)在执行语言相关任务方面表现出了非凡的能力。然而，由于其巨大的内存和存储需求，它们的部署面临着重大挑战。为了解决这个问题，仅针对权重的量化，特别是3位和4位仅针对权重的量化，已经成为最可行的解决方案之一。随着位数的减少，量化网格变得更加宽泛，从而强调了上下舍入的重要性。尽管先前的研究表明，在某些情况下，通过添加扰动细调上下舍入可以提高准确性，但我们的研究受制于这些扰动的精确且有限的边界，只有改变舍入值的阈值才具有重要性。因此，我们提出了一种简洁高效的优化权重舍入任务的方法。我们的方法名为SignRound，它涉及使用有符号梯度的轻量级分块调整。

    Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
    
[^22]: REB：降低工业异常检测中的表示偏差

    REB: Reducing Biases in Representation for Industrial Anomaly Detection. (arXiv:2308.12577v1 [cs.CV])

    [http://arxiv.org/abs/2308.12577](http://arxiv.org/abs/2308.12577)

    本文提出了一种名为REB的方法，通过考虑领域偏差和构建自监督学习任务，以及使用本地密度K最近邻方法，从而降低工业异常检测中的表示偏差，并取得了显著的改进。

    

    现有的K最近邻（KNN）检索方法通常分为两个阶段进行工业异常检测：使用预训练的CNN模型获取特征表示，并执行距离度量进行缺陷检测。然而，由于忽略了领域偏差和特征空间中的局部密度差异，这些特征没有被充分利用，限制了检测性能。本文提出了一种通过考虑预训练模型的领域偏差和构建自监督学习任务以实现更好的领域适应性的方法——Reducing Biases（REB）。同时，通过模仿自然缺陷的缺陷生成策略（DefectMaker），提出了一种本地密度K最近邻（LDKNN）方法来减少局部密度偏差并实现有效的异常检测。我们在广泛使用的MVTec AD基准上实现了99.5％的AUROC，并在具有挑战性的MVTec LOCO AD数据集上实现了88.0％的AUROC，并将AUROC提高了4.7％。

    Existing K-nearest neighbor (KNN) retrieval-based methods usually conduct industrial anomaly detection in two stages: obtain feature representations with a pre-trained CNN model and perform distance measures for defect detection. However, the features are not fully exploited as they ignore domain bias and the difference of local density in feature space, which limits the detection performance. In this paper, we propose Reducing Biases (REB) in representation by considering the domain bias of the pre-trained model and building a self-supervised learning task for better domain adaption with a defect generation strategy (DefectMaker) imitating the natural defects. Additionally, we propose a local density KNN (LDKNN) to reduce the local density bias and obtain effective anomaly detection. We achieve a promising result of 99.5\% AUROC on the widely used MVTec AD benchmark. We also achieve 88.0\% AUROC on the challenging MVTec LOCO AD dataset and bring an improvement of 4.7\% AUROC to the st
    
[^23]: ANALOGYKB：使用百万规模知识库开启语言模型的类比推理能力。

    ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base. (arXiv:2305.05994v1 [cs.CL])

    [http://arxiv.org/abs/2305.05994](http://arxiv.org/abs/2305.05994)

    本文提出了ANALOGYKB，一种使用百万规模知识库的类比推理方法，能够使语言模型在类比推理任务上取得比之前的最先进方法更好的结果。

    

    类比推理是人类的一项基本认知能力，然而，由于缺乏模型训练资源，目前的语言模型仍然难以在类比推理任务中达到人类的表现水平。本文提出了ANALOGYKB，这是一个百万规模的类比知识库，它由现有的知识图谱导出。ANALOGYKB从知识图谱中识别了两种类型的类比：1）相同关系的类比，可以直接从知识图谱中提取；2）类似关系的类比，则由大型语言模型（InstructGPT）启用的选择和过滤管道进行识别，再经过少量人工质量控制。在两个类比推理任务（类比识别和生成）的一系列数据集上的评估表明，ANALOGYKB成功地使语言模型取得了比之前的最先进方法更好的结果。

    Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large LMs (InstructGPT), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables LMs to achieve much better results than previous state-of-the-art methods.
    
[^24]: Q-HyViT: 带桥块重构的混合视觉Transformer的后训练量化

    Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with Bridge Block Reconstruction. (arXiv:2303.12557v1 [cs.CV])

    [http://arxiv.org/abs/2303.12557](http://arxiv.org/abs/2303.12557)

    本文针对视觉Transformer在移动设备上计算要求高的问题，提出了一种带桥块重构的混合视觉Transformer的后训练量化方法，提高其在移动设备上的加速效果。

    

    最近，视觉Transformer （ViT）在许多任务中取代了卷积神经网络模型，包括分类、检测和分割。然而， ViT 的高计算要求阻碍了它们的广泛应用。为解决这个问题，研究人员提出了高效的混合变压器架构，结合卷积和变压器层，并优化注意力计算，使线性复杂度达到最大。此外，后训练量化被提出作为缓解计算要求的一种手段。将量化技术和高效的混合变压器结构相结合，对于在移动设备上加速视觉transformer至关重要。然而，以前没有研究将量化应用于高效的混合变压器。 在本文中，首先我们发现将现有的ViT PTQ方法直接应用于高效的混合transformer架构会导致严重的精度下降，由此我们提出了Q-HyViT。

    Recently, vision transformers (ViT) have replaced convolutional neural network models in numerous tasks, including classification, detection, and segmentation. However, the high computational requirements of ViTs hinder their widespread implementation. To address this issue, researchers have proposed efficient hybrid transformer architectures that combine convolutional and transformer layers and optimize attention computation for linear complexity. Additionally, post-training quantization has been proposed as a means of mitigating computational demands. Combining quantization techniques and efficient hybrid transformer structures is crucial to maximize the acceleration of vision transformers on mobile devices. However, no prior investigation has applied quantization to efficient hybrid transformers. In this paper, at first, we discover that the straightforward manner to apply the existing PTQ methods for ViT to efficient hybrid transformers results in a drastic accuracy drop due to the
    
[^25]: 长尾分类的曲率平衡特征流形学习

    Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification. (arXiv:2303.12307v1 [cs.CV])

    [http://arxiv.org/abs/2303.12307](http://arxiv.org/abs/2303.12307)

    本文提出了一种曲率平衡特征流形学习的方法，探究了感知流形的几何特性对分类难度的影响，发现曲率不平衡会导致模型不公平。

    

    为了应对长尾分类的挑战，研究人员已经提出了几种方法来减少模型偏差，其中大多数假设样本较少的类是弱类。然而，最近的研究表明，尾部类别并不总是难以学习的，而在样本平衡的数据集上观察到了模型偏差，这表明存在其他影响模型偏差的因素。在本文中，我们系统地提出了一系列用于深度神经网络中感知流形的几何度量，并探讨了感知流形的几何特性对分类难度和学习如何塑造感知流形的几何特性的影响。一个意外的发现是：类别准确度和感知流形的分离程度之间的相关性在训练过程中逐渐减小，而与曲率的负相关性逐渐增加，这表明曲率不平衡导致模型不公平。

    To address the challenges of long-tailed classification, researchers have proposed several approaches to reduce model bias, most of which assume that classes with few samples are weak classes. However, recent studies have shown that tail classes are not always hard to learn, and model bias has been observed on sample-balanced datasets, suggesting the existence of other factors that affect model bias. In this work, we systematically propose a series of geometric measurements for perceptual manifolds in deep neural networks, and then explore the effect of the geometric characteristics of perceptual manifolds on classification difficulty and how learning shapes the geometric characteristics of perceptual manifolds. An unanticipated finding is that the correlation between the class accuracy and the separation degree of perceptual manifolds gradually decreases during training, while the negative correlation with the curvature gradually increases, implying that curvature imbalance leads to m
    

