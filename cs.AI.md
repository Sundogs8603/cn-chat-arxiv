# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models.](http://arxiv.org/abs/2311.01305) | AWEQ是一种后训练量化和激活权重均衡方法，能够在大型语言模型中实现超低位量化和8-bit权重和激活量化，并通过改进的均衡方法减小量化偏差误差，提高模型的鲁棒性。 |
| [^2] | [A Multi-Agent Reinforcement Learning Framework for Evaluating the U.S. Ending the HIV Epidemic Plan.](http://arxiv.org/abs/2311.00855) | 本论文提出了一种多智能体强化学习（MARL）框架，用于评估美国终结HIV流行计划。该框架能够进行特定地区的决策分析，并考虑到地区之间的流行病学相互作用。 |
| [^3] | [ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection.](http://arxiv.org/abs/2311.00729) | ZEETAD是一个零样本端到端时间动作检测模型，其中包括双定位和零样本提议分类两个模块。前者是基于Transformer的模块，用于检测动作事件并收集关键的语义嵌入，后者是基于CLIP的模块，用于生成文本和帧输入的语义嵌入。 |
| [^4] | [Improving Entropy-Based Test-Time Adaptation from a Clustering View.](http://arxiv.org/abs/2310.20327) | 本文从聚类的角度解释了基于熵的测试时间自适应（EBTTA）方法，提出了一个迭代算法，并展示了对于EBTTA方法来说，熵损失会进一步增加最大的概率，从而为其在聚类任务上的较好性能提供了一个替代性解释。 |
| [^5] | [Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations.](http://arxiv.org/abs/2310.20246) | 本文首次探索并训练了强大的多语言数学推理模型，通过使用翻译构建了多语言数据集，并提出了各种训练策略来构建强大的模型。实验证实发现在多语言训练中，将目标语言的翻译与原始语言的表示结合起来以及交替训练和多语言模型的自举可以提高模型的性能。此外，模型在处理低频词和长句子方面仍面临挑战。 |
| [^6] | [Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark.](http://arxiv.org/abs/2310.12567) | 本文介绍了一个名为Safety-Gymnasium的环境套件，其中包括单个和多个Agent场景中的安全关键任务，并提供了一个包含16种最先进的SafeRL算法的算法库。这个基准旨在促进对安全性能的评估和比较，推动强化学习在安全性能上的发展。 |
| [^7] | [Automated Repair of Declarative Software Specifications in the Era of Large Language Models.](http://arxiv.org/abs/2310.12425) | 大语言模型时代下，自动修复声明式软件规范的有效技术需求愈发突出。该研究评估了利用ChatGPT修复Alloy声明式语言规范的效果，并探索了大型语言模型在此自动修复过程中的机会。 |
| [^8] | [Learning and Discovering Quantum Properties with Multi-Task Neural Networks.](http://arxiv.org/abs/2310.11807) | 这篇论文介绍了一种用多任务神经网络学习和发现量子性质的方法，该方法能够同时预测和发现多个量子性质，并且能够推断多体量子系统的全局性质和发现不同相之间的未知边界。 |
| [^9] | [Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods.](http://arxiv.org/abs/2310.05309) | 本文提出了一种新颖的理论框架，为深度神经网络和增强学习方法在解决组合问题方面的有效性提供了肯定的答案。这个框架对于解决包括最大割和最小割、最大$k$约束问题、最大权重二分图匹配和旅行商问题在内的广泛的组合问题有重要的意义。 |
| [^10] | [Deformation-Invariant Neural Network and Its Applications in Distorted Image Restoration and Analysis.](http://arxiv.org/abs/2310.02641) | 本文提出了一种弹性不变神经网络（DINN）用于处理受到几何形变影响的图像的图像处理任务。DINN通过融入拟保形变换网络（QCTN）来输出一致的潜在特征，使得具有相同原始对象或场景的几何形变图像能够更接近自然或良好图像分布。 |
| [^11] | [Curating Naturally Adversarial Datasets for Trustworthy AI in Healthcare.](http://arxiv.org/abs/2309.00543) | 提出了一种方法来筛选自然对立示例的数据集，以评估模型的鲁棒性，并通过自动弱监督标注获得的概率标签来实现这一方法。 |
| [^12] | [A Text-based Approach For Link Prediction on Wikipedia Articles.](http://arxiv.org/abs/2309.00317) | 本研究提出了一种基于文本的维基百科文章链接预测方法，通过利用传统的机器学习模型和词性标注特征，实现了对两个节点是否有链接的分类预测，并在比赛中取得了较高的成绩。 |
| [^13] | [A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data.](http://arxiv.org/abs/2308.11849) | 本文提出了一种利用移动数据进行实时需求响应的深度强化学习方法来缓解车站拥挤的铁路重新调度。该方法通过推断真实世界的乘客流动性来实现实时调度，并着重考虑高需求车站的重新安排。 |
| [^14] | [TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage.](http://arxiv.org/abs/2308.03427) | 基于大型语言模型的AI代理用于任务规划和工具使用。我们提出了一个结构框架，设计了两种代理来执行推理过程，实例化了框架，并评估了它们的任务规划和工具使用能力。 |
| [^15] | [How to Scale Your EMA.](http://arxiv.org/abs/2307.13813) | 本研究提供了在存在模型EMA的情况下进行优化的缩放规则，以保持训练动态的一致性。这对于实际机器学习中的权衡批量大小和墙钟时间非常重要。模型EMA能够提高模型的性能以及稳定训练过程，并为自监督学习提供学习信号。 |
| [^16] | [FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning.](http://arxiv.org/abs/2307.13214) | FedMEKT是一种基于蒸馏的多模态联邦学习框架，通过半监督学习方法利用不同模态的表示，实现了服务器和客户端之间的联合知识传输。 |
| [^17] | [Large Language Models as Superpositions of Cultural Perspectives.](http://arxiv.org/abs/2307.07870) | 大型语言模型被认为是具有个性或一套价值观的，但实际上它可以看作是具有不同价值观和个性特征的角度的叠加。通过角度可控性的概念，我们研究了大型语言模型在不同角度下展示的价值观和个性特征的变化。实验结果表明，即使在没有明显提示的情况下，大型语言模型也会表达出不同的价值观。 |
| [^18] | [FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems.](http://arxiv.org/abs/2307.05857) | 这篇论文提出了一种名为FAIRO的算法，用于在人机交互环境中实现公平的顺序决策。该算法将人的行为可变性和偏好变化考虑在内，通过利用选项强化学习框架将复杂的公平任务分解为自适应子任务。 |
| [^19] | [Dynamics of Temporal Difference Reinforcement Learning.](http://arxiv.org/abs/2307.04841) | 我们使用统计物理学的概念，研究了时间差分学习在线性函数逼近器下的典型学习曲线。我们发现由于子采样可能的轨迹空间而产生的随机半梯度噪声会导致值误差出现显著的平台。 |
| [^20] | [Graph Denoising Diffusion for Inverse Protein Folding.](http://arxiv.org/abs/2306.16819) | 我们提出了一种逆向蛋白质折叠的图去噪扩散模型，通过扩散过程生成了一组多样的候选序列，该模型能够很好地概括多样的可行解决方案。 |
| [^21] | [Comparing Causal Frameworks: Potential Outcomes, Structural Models, Graphs, and Abstractions.](http://arxiv.org/abs/2306.14351) | 本文比较了因果框架中的潜在结果模型(RCM)和结构因果模型(SCM)，并阐明了RCM成为SCM可表达的条件，以及每个RCM作为某些可表达的RCM的抽象。作者介绍了SCM原则在RCM经典应用中的重要作用，并提出了由图表示的代数约束的特征，有助于进一步比较两个框架。 |
| [^22] | [A Fast Maximum $k$-Plex Algorithm Parameterized by the Degeneracy Gap.](http://arxiv.org/abs/2306.13258) | 本文提出了一个新参数$g_k(G)$用于最大$k$-plex问题，针对其设计了一个根据$g_k(G)$参数化的精确算法，具有较高的时间复杂度，但可以在实际图中得到应用。 |
| [^23] | [Simple and Controllable Music Generation.](http://arxiv.org/abs/2306.05284) | 本文提出了 MusicGen，一个单一的语言模型，可以在条件描述或旋律特征控制下生成高质量的样本，并且在标准的文本到音乐基准上的实证研究中，该方法优于其他基线模型。 |
| [^24] | [Deploying a Robust Active Preference Elicitation Algorithm: Experiment Design, Interface, and Evaluation for COVID-19 Patient Prioritization.](http://arxiv.org/abs/2306.04061) | 本论文介绍了一种强健的偏好调查算法，旨在优先为COVID-19患者提供医院资源，通过招募亚马逊机械土耳其工作者对该算法进行评估。 |
| [^25] | [The Impact of Positional Encoding on Length Generalization in Transformers.](http://arxiv.org/abs/2305.19466) | 本文通过实证研究 Transformer 模型中位置编码对于长度推广的影响，结果表明常用的位置编码方法并不适合用于下游任务的长度推广，并且使用位置编码甚至可能会损害长度推广的能力。 |
| [^26] | [CongFu: Conditional Graph Fusion for Drug Synergy Prediction.](http://arxiv.org/abs/2305.14517) | CongFu是一种用于药物协同预测的新型条件图融合层，采用注意机制和瓶颈技术提取本地图上下文，并在全局上下文中有条件地融合图数据。其模块化架构使得可以更换图层模块，包括读出和图编码器，以适应不同的应用场景，并在12个基准数据集中的11个上取得了最先进的结果，证明了其能力。 |
| [^27] | [MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic.](http://arxiv.org/abs/2305.03353) | 本文利用动态认知逻辑在自然语言处理模型中探讨了理解心智理论的方法。虽然GPT-4表现出改进的能力，但需要进一步提高。 |
| [^28] | [Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System.](http://arxiv.org/abs/2304.13105) | 本文提出了一种利用WiFi信号进行人员存在检测的新系统，采用了关注机制和双向LSTM网络来提高准确性，并证明了其在现实场景中的稳健性。 |
| [^29] | [Spatial-Language Attention Policies for Efficient Robot Learning.](http://arxiv.org/abs/2304.11235) | 本文提出了一种空间-语言注意力策略(SLAP)，使用三维标记作为输入表示，以训练单一多任务和语言条件化的动作预测策略，能够在引入未见过的干扰物和物体配置时达到47.5%的成功率。 |
| [^30] | [Multi-scale Evolutionary Neural Architecture Search for Deep Spiking Neural Networks.](http://arxiv.org/abs/2304.10749) | 本文提出了一种新方法--多尺度进化神经架构搜索，用于自动设计脉冲神经网络，同时考虑微观、中观和宏观尺度的脑拓扑结构。MSE-NAS可以帮助SNN实现多电路模式的自组织集成，并通过全局性的跨模式连接来优化网络性能。 |
| [^31] | [Optimisation via encodings: a renormalisation group perspective.](http://arxiv.org/abs/2303.16258) | 本文提出了一种通过使用启发式算法构造覆盖编码映射，将大型搜索空间中的过程映射到原始搜索空间的子集中，并使用重整化群的视角来处理离散优化问题的方法。这种方法相对于单独的局部搜索方法具有更好的性能。 |
| [^32] | [Explicit Planning Helps Language Models in Logical Reasoning.](http://arxiv.org/abs/2303.15714) | 本文提出了一个新的系统，使用语言模型进行多步逻辑推理，采用了显式规划来帮助做出更明智的决策，比其他竞争系统表现更好，显式规划在系统性能中起着关键作用。 |
| [^33] | [Roots and Requirements for Collaborative AI.](http://arxiv.org/abs/2303.12040) | 论文探讨了AI协同合作的历史和要求，是协同AI研究的动机和背景。 |
| [^34] | [Enhancing the Role of Context in Region-Word Alignment for Object Detection.](http://arxiv.org/abs/2303.10093) | 本研究提出了一种增强上下文在目标检测区域-词对齐中作用的方法，通过特定的负采样方法提高了属性的作用，从而提高了目标检测的效果。 |
| [^35] | [The NCI Imaging Data Commons as a platform for reproducible research in computational pathology.](http://arxiv.org/abs/2303.09354) | 国家癌症研究所影像数据共享平台 (IDC) 旨在促进计算病理学领域的研究可重复性，实现了 FAIR 原则，提供公共库和云端技术支持，方便使用机器学习方法进行癌症组织分类研究。 |
| [^36] | [DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets.](http://arxiv.org/abs/2302.04178) | DynGFN是一种借助RNA速度技术进行基因调控网络推断的方法，能够捕捉网络结构的不确定性，并在准确度上超过现有方法。 |
| [^37] | [Guaranteed Conformance of Neurosymbolic Models to Natural Constraints.](http://arxiv.org/abs/2212.01346) | 该论文提出了一种能够担保数据驱动模型符合自然科学已有知识并最优逼近系统模型的方法。 |
| [^38] | [Undesirable biases in NLP: Averting a crisis of measurement.](http://arxiv.org/abs/2211.13709) | 这项研究提供了一个跨学科的方法来探讨NLP模型偏见的问题，通过采用心理测量学的视角，特别关注构念效度和测量工具的信度，在衡量模型偏见的情境中如何应用。 |
| [^39] | [Failed Goal Aware Hindsight Experience Replay.](http://arxiv.org/abs/2208.14741) | 本文提出了一种称为Failed goal Aware HER (FAHER)的新方法来增强多目标强化学习中的采样效率。该方法利用实现目标与未实现目标之间的关系，并通过聚类模型对序列进行聚类和采样。 |
| [^40] | [SSIVD-Net: A Novel Salient Super Image Classification & Detection Technique for Weaponized Violence.](http://arxiv.org/abs/2207.12850) | 本文提出了一种名为SSIVD-Net的新技术，用于暴力识别任务。通过使用显著-超级图像表示减少了3D视频数据的复杂性，提高了推断、性能和可解释性。作者还提出了一种新颖的架构“Salient-Classifier”，将核方法和残差学习策略相结合。该方法在多个数据集上表现良好。 |
| [^41] | [Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery.](http://arxiv.org/abs/2206.10540) | 本文重新审视并重新创建了符号回归数据集，以评估符号回归在科学发现中的潜力和能力，并提出使用归一化编辑距离进行度量。 |
| [^42] | [FAIR4Cov: Fused Audio Instance and Representation for COVID-19 Detection.](http://arxiv.org/abs/2204.10581) | FAIR4Cov是一种针对COVID-19检测的方法，它提出了一种融合身体声音的波形和谱图表示的关节特征向量，可以有效地检测COVID-19患者，胜过了其他方法。 |

# 详细

[^1]: AWEQ：用于大型语言模型的后训练量化和激活权重均衡方法

    AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v1 [cs.LG])

    [http://arxiv.org/abs/2311.01305](http://arxiv.org/abs/2311.01305)

    AWEQ是一种后训练量化和激活权重均衡方法，能够在大型语言模型中实现超低位量化和8-bit权重和激活量化，并通过改进的均衡方法减小量化偏差误差，提高模型的鲁棒性。

    

    大型语言模型(LLMs)在各种任务中表现出色，但其计算和存储成本也相对较高。量化这些模型是缓解这个问题的有效方法。然而，现有方法很难在模型准确性和硬件效率之间取得平衡。因此，我们引入了AWEQ，一种后训练方法，不需要额外的训练开销。AWEQ在超低位量化和8-bit权重和激活(W8A8)量化方面表现出色。观察到权重量化比激活量化更容易。AWEQ通过通道均衡将激活量化的难度转移到权重上，实现了两者量化困难的平衡，从而最大化了性能。我们进一步改进了均衡方法，减小了量化偏差误差，确保模型的鲁棒性。在像LLaMA这样的流行模型上进行了大量实验。

    Large language models(LLMs) exhibit excellent performance across a variety of tasks, but they come with significant computational and storage costs. Quantizing these models is an effective way to alleviate this issue. However, existing methods struggle to strike a balance between model accuracy and hardware efficiency. This is where we introduce AWEQ, a post-training method that requires no additional training overhead. AWEQ excels in both ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization. There is an observation that weight quantization is less challenging than activation quantization. AWEQ transfers the difficulty of activation quantization to weights using channel equalization, achieving a balance between the quantization difficulties of both, and thereby maximizing performance. We have further refined the equalization method to mitigate quantization bias error, ensuring the robustness of the model. Extensive experiments on popular models such as LLaMA a
    
[^2]: 一种用于评估美国终结HIV流行计划的多智能体强化学习框架

    A Multi-Agent Reinforcement Learning Framework for Evaluating the U.S. Ending the HIV Epidemic Plan. (arXiv:2311.00855v1 [cs.AI])

    [http://arxiv.org/abs/2311.00855](http://arxiv.org/abs/2311.00855)

    本论文提出了一种多智能体强化学习（MARL）框架，用于评估美国终结HIV流行计划。该框架能够进行特定地区的决策分析，并考虑到地区之间的流行病学相互作用。

    

    人类免疫缺陷病毒（HIV）是美国的主要公共卫生问题，每年有约1.2万人感染HIV，其中有3.5万人是新感染者。美国的HIV负担和护理接触存在着地理差异。2019年的终结HIV流行计划旨在到2030年将新感染人数减少90%，通过提高诊断、治疗和预防干预措施的覆盖率，并优先考虑HIV高流行地区。确定最佳干预措施的规模扩大将有助于资源分配的决策。现有的HIV决策模型要么只评估特定城市，要么评估整个国家人口，忽视地方的相互作用或差异。在本文中，我们提出了一种多智能体强化学习（MARL）模型，它能够进行特定地区的决策分析，同时考虑跨地区的流行病互动。在实验分析中，

    Human immunodeficiency virus (HIV) is a major public health concern in the United States, with about 1.2 million people living with HIV and 35,000 newly infected each year. There are considerable geographical disparities in HIV burden and care access across the U.S. The 2019 Ending the HIV Epidemic (EHE) initiative aims to reduce new infections by 90% by 2030, by improving coverage of diagnoses, treatment, and prevention interventions and prioritizing jurisdictions with high HIV prevalence. Identifying optimal scale-up of intervention combinations will help inform resource allocation. Existing HIV decision analytic models either evaluate specific cities or the overall national population, thus overlooking jurisdictional interactions or differences. In this paper, we propose a multi-agent reinforcement learning (MARL) model, that enables jurisdiction-specific decision analyses but in an environment with cross-jurisdictional epidemiological interactions. In experimental analyses, conduct
    
[^3]: ZEETAD: 为零样本端到端时间动作检测改进预训练的视觉-语言模型

    ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot End-to-End Temporal Action Detection. (arXiv:2311.00729v1 [cs.CV])

    [http://arxiv.org/abs/2311.00729](http://arxiv.org/abs/2311.00729)

    ZEETAD是一个零样本端到端时间动作检测模型，其中包括双定位和零样本提议分类两个模块。前者是基于Transformer的模块，用于检测动作事件并收集关键的语义嵌入，后者是基于CLIP的模块，用于生成文本和帧输入的语义嵌入。

    

    时间动作检测（TAD）涉及在非剪辑视频中定位和分类动作实例。尽管传统TAD在大量训练数据上采用完全监督学习和封闭集设置，但最近的零样本TAD方法通过利用大规模对比的视觉-语言（ViL）预训练模型展示了开放集设置的潜力。然而，现有的零样本TAD方法在如何适当构建定位和分类这两个相互依赖的任务之间的强关系以及将ViL模型适应于视频理解方面存在局限性。在这项工作中，我们提出了ZEETAD，包含两个模块：双定位和零样本提议分类。前者是基于Transformer的模块，用于检测动作事件，并选择性地收集关键的语义嵌入以供后续识别。后者是基于CLIP的模块，用于为每个时间单位从文本和帧输入生成语义嵌入。

    Temporal action detection (TAD) involves the localization and classification of action instances within untrimmed videos. While standard TAD follows fully supervised learning with closed-set setting on large training data, recent zero-shot TAD methods showcase the promising of open-set setting by leveraging large-scale contrastive visual-language (ViL) pretrained models. However, existing zero-shot TAD methods have limitations on how to properly construct the strong relationships between two interdependent tasks of localization and classification and adapt ViL model to video understanding. In this work, we present ZEETAD, featuring two modules: dual-localization and zero-shot proposal classification. The former is a Transformer-based module that detects action events while selectively collecting crucial semantic embeddings for later recognition. The latter one, CLIP-based module, generates semantic embeddings from text and frame inputs for each temporal unit. Additionally, we enhance d
    
[^4]: 从聚类视角改进基于熵的测试时间自适应

    Improving Entropy-Based Test-Time Adaptation from a Clustering View. (arXiv:2310.20327v1 [cs.AI])

    [http://arxiv.org/abs/2310.20327](http://arxiv.org/abs/2310.20327)

    本文从聚类的角度解释了基于熵的测试时间自适应（EBTTA）方法，提出了一个迭代算法，并展示了对于EBTTA方法来说，熵损失会进一步增加最大的概率，从而为其在聚类任务上的较好性能提供了一个替代性解释。

    

    在现实世界中，领域偏移是一个常见的问题，训练数据和测试数据遵循不同的数据分布。为了解决这个问题，完全的测试时间自适应（TTA）利用测试时间遇到的无标签数据来适应模型。特别是基于熵的测试时间自适应（EBTTA）方法，在测试样本上最小化预测的熵，取得了很大的成功。在本文中，我们从聚类的角度介绍了EBTTA的新视角和解释。这是一个迭代算法：1）在分配步骤中，EBTTA模型的前向过程是为这些测试样本分配标签；2）在更新步骤中，反向过程是通过已分配的样本来更新模型。根据这种解释，我们可以更深入地理解EBTTA，其中我们展示了熵损失会进一步增加最大的概率。因此，我们提供了一个替代性解释，解释了为什么现有的EBTTA方法在聚类任务上比在分类任务上表现更好。

    Domain shift is a common problem in the realistic world, where training data and test data follow different data distributions. To deal with this problem, fully test-time adaptation (TTA) leverages the unlabeled data encountered during test time to adapt the model. In particular, Entropy-Based TTA (EBTTA) methods, which minimize the prediction's entropy on test samples, have shown great success. In this paper, we introduce a new perspective on the EBTTA, which interprets these methods from a view of clustering. It is an iterative algorithm: 1) in the assignment step, the forward process of the EBTTA models is the assignment of labels for these test samples, and 2) in the updating step, the backward process is the update of the model via the assigned samples. Based on the interpretation, we can gain a deeper understanding of EBTTA, where we show that the entropy loss would further increase the largest probability. Accordingly, we offer an alternative explanation that why existing EBTTA 
    
[^5]: 在多语言数学推理中打破语言障碍：见解与观察

    Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v1 [cs.CL])

    [http://arxiv.org/abs/2310.20246](http://arxiv.org/abs/2310.20246)

    本文首次探索并训练了强大的多语言数学推理模型，通过使用翻译构建了多语言数据集，并提出了各种训练策略来构建强大的模型。实验证实发现在多语言训练中，将目标语言的翻译与原始语言的表示结合起来以及交替训练和多语言模型的自举可以提高模型的性能。此外，模型在处理低频词和长句子方面仍面临挑战。

    

    现有研究主要集中在开发适用于单语言中的数学推理的强大语言学习模型（LLM），在多语言环境下保持效果的研究很少。为了弥补这一差距，本文首次探索和训练强大的多语言数学推理（xMR）LLM。首先，通过利用翻译，我们构建了第一个包含十种不同语言的多语言数学推理指导数据集MGSM8KInstruct，从而解决了xMR任务中训练数据稀缺的问题。根据收集的数据集，我们提出了不同的训练策略来构建强大的xMR LLMs，被命名为MathOctopus，在几次训练中表现出优于传统开源LLMs和ChatGPT的能力。值得注意的是，MathOctopus-13B在MGSM测试集上达到了47.6%的准确率，超过了ChatGPT的46.3%。除了显著的结果，我们还从大量的实验证实中发现了一些重要的观察和见解：（1）在多语言上进行训练时，最好将目标语言的翻译与原始语言的表示结合起来。 （2）交替训练和多语言模型的自举有助于提高模型的表现。 （3）模型对于低频词和长句子的处理是挑战的，需要进一步改进。

    Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When
    
[^6]: Safety-Gymnasion：一个统一的安全强化学习基准

    Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark. (arXiv:2310.12567v1 [cs.AI])

    [http://arxiv.org/abs/2310.12567](http://arxiv.org/abs/2310.12567)

    本文介绍了一个名为Safety-Gymnasium的环境套件，其中包括单个和多个Agent场景中的安全关键任务，并提供了一个包含16种最先进的SafeRL算法的算法库。这个基准旨在促进对安全性能的评估和比较，推动强化学习在安全性能上的发展。

    

    人工智能系统拥有推动社会进步的巨大潜力。然而，它们的部署经常面临严重的安全问题。安全强化学习(SafeRL)作为一种解决方案出现，可以在同时遵守多个约束的情况下优化策略，从而解决集成强化学习在安全关键场景中的挑战。本文介绍了一个名为Safety-Gymnasium的环境套件，包括单个和多个Agent场景中的安全关键任务，并接受向量和仅视觉输入。此外，我们提供了一个名为Safe Policy Optimization（SafePO）的算法库，包含16种最先进的SafeRL算法。这个综合性库可以作为研究社区的验证工具。通过引入这个基准，我们旨在促进对安全性能的评估和比较，从而推动强化学习在安全性能上的发展。

    Artificial intelligence (AI) systems possess significant potential to drive societal progress. However, their deployment often faces obstacles due to substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a solution to optimize policies while simultaneously adhering to multiple constraints, thereby addressing the challenge of integrating reinforcement learning in safety-critical scenarios. In this paper, we present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenarios, accepting vector and vision-only input. Additionally, we offer a library of algorithms named Safe Policy Optimization (SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive library can serve as a validation tool for the research community. By introducing this benchmark, we aim to facilitate the evaluation and comparison of safety performance, thus fostering the development of reinforcement learning for s
    
[^7]: 大语言模型时代下的声明式软件规范自动修复

    Automated Repair of Declarative Software Specifications in the Era of Large Language Models. (arXiv:2310.12425v1 [cs.SE])

    [http://arxiv.org/abs/2310.12425](http://arxiv.org/abs/2310.12425)

    大语言模型时代下，自动修复声明式软件规范的有效技术需求愈发突出。该研究评估了利用ChatGPT修复Alloy声明式语言规范的效果，并探索了大型语言模型在此自动修复过程中的机会。

    

    声明式软件规范语言的广泛采用以及其在调试方面的困难性，凸显了对适用于此类语言的有效自动化修复技术的需求。研究人员最近探索了各种方法来自动修复声明式软件规范，如基于模板的修复、反馈驱动的迭代修复和有界穷举方法。大型语言模型的最新发展为自动修复声明式规范提供了新机会。在这项研究中，我们评估了利用OpenAI的ChatGPT修复用Alloy声明式语言编写的软件规范的效果。与命令式语言不同，Alloy中的规范不会被执行，而是被转换为逻辑公式，并使用后端约束求解器进行评估，以识别规范实例和断言的反例。我们的评估重点是ChatGPT在改进声明式规范修复能力方面的能力。

    The growing adoption of declarative software specification languages, coupled with their inherent difficulty in debugging, has underscored the need for effective and automated repair techniques applicable to such languages. Researchers have recently explored various methods to automatically repair declarative software specifications, such as template-based repair, feedback-driven iterative repair, and bounded exhaustive approaches. The latest developments in large language models provide new opportunities for the automatic repair of declarative specifications. In this study, we assess the effectiveness of utilizing OpenAI's ChatGPT to repair software specifications written in the Alloy declarative language. Unlike imperative languages, specifications in Alloy are not executed but rather translated into logical formulas and evaluated using backend constraint solvers to identify specification instances and counterexamples to assertions. Our evaluation focuses on ChatGPT's ability to impr
    
[^8]: 用多任务神经网络学习和发现量子性质

    Learning and Discovering Quantum Properties with Multi-Task Neural Networks. (arXiv:2310.11807v1 [quant-ph])

    [http://arxiv.org/abs/2310.11807](http://arxiv.org/abs/2310.11807)

    这篇论文介绍了一种用多任务神经网络学习和发现量子性质的方法，该方法能够同时预测和发现多个量子性质，并且能够推断多体量子系统的全局性质和发现不同相之间的未知边界。

    

    深度神经网络是一种从有限测量数据中预测量子态性质的强大工具。在这里，我们开发了一个网络模型，可以同时预测多个量子性质，包括量子可观测量的期望值以及量子状态的一般非线性函数，如纠缠熵和多体拓扑不变量。值得注意的是，我们发现一个在给定性质集上训练的模型也可以发现超出该集合的新性质。多功能训练还使模型能够从局部测量中推断出多体量子系统的全局性质，分类保护对称的拓扑物质相，并发现不同相之间的未知边界。

    Deep neural networks are a powerful tool for predicting properties of quantum states from limited measurement data. Here we develop a network model that can simultaneously predict multiple quantum properties, including not only expectation values of quantum observables, but also general nonlinear functions of the quantum state, like entanglement entropies and many-body topological invariants. Remarkably, we find that a model trained on a given set of properties can also discover new properties outside that set. Multi-purpose training also enables the model to infer global properties of many-body quantum systems from local measurements, to classify symmetry protected topological phases of matter, and to discover unknown boundaries between different phases.
    
[^9]: 优化组合问题的解采样器：策略梯度方法的梯度方向

    Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods. (arXiv:2310.05309v1 [cs.LG])

    [http://arxiv.org/abs/2310.05309](http://arxiv.org/abs/2310.05309)

    本文提出了一种新颖的理论框架，为深度神经网络和增强学习方法在解决组合问题方面的有效性提供了肯定的答案。这个框架对于解决包括最大割和最小割、最大$k$约束问题、最大权重二分图匹配和旅行商问题在内的广泛的组合问题有重要的意义。

    

    深度神经网络和增强学习方法在解决复杂的组合问题方面具有很高的实用价值。在这些方法中，深度神经网络被用作解决方案生成器，然后通过梯度下降等方法进行训练，以逐步获得更好的解决方案分布。在这项工作中，我们引入了一种新颖的理论框架来分析这些方法的有效性。我们的主要贡献是对这个问题的积极回答。我们的结果适用于包括最大割和最小割、最大$k$约束问题、最大权重二分图匹配和旅行商问题在内的广泛的组合问题。

    Deep Neural Networks and Reinforcement Learning methods have empirically shown great promise in tackling challenging combinatorial problems. In those methods a deep neural network is used as a solution generator which is then trained by gradient-based methods (e.g., policy gradient) to successively obtain better solution distributions. In this work we introduce a novel theoretical framework for analyzing the effectiveness of such methods. We ask whether there exist generative models that (i) are expressive enough to generate approximately optimal solutions; (ii) have a tractable, i.e, polynomial in the size of the input, number of parameters; (iii) their optimization landscape is benign in the sense that it does not contain sub-optimal stationary points. Our main contribution is a positive answer to this question. Our result holds for a broad class of combinatorial problems including Max- and Min-Cut, Max-$k$-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem. A
    
[^10]: 弹性不变神经网络及其在变形图像恢复和分析中的应用

    Deformation-Invariant Neural Network and Its Applications in Distorted Image Restoration and Analysis. (arXiv:2310.02641v1 [cs.CV])

    [http://arxiv.org/abs/2310.02641](http://arxiv.org/abs/2310.02641)

    本文提出了一种弹性不变神经网络（DINN）用于处理受到几何形变影响的图像的图像处理任务。DINN通过融入拟保形变换网络（QCTN）来输出一致的潜在特征，使得具有相同原始对象或场景的几何形变图像能够更接近自然或良好图像分布。

    

    受到几何形变影响的图像对于目标识别等图像处理和计算机视觉任务来说是一个重要的挑战。基于深度学习的图像模型通常无法对几何形变图像给出准确的性能。本文中，我们提出了一种弹性不变神经网络（DINN），用于解决几何形变图像的图像处理任务。DINN为几何形变图像输出一致的潜在特征，这些图像具有相同的原始对象或场景。DINN的思想是将一个简单的组件，称为拟保形变换网络（QCTN），融入到其他现有的深度网络中进行图像处理任务。QCTN是一个深度神经网络，它输出一个拟保形映射，可以将几何形变的图像转换为更接近自然或良好图像分布的改进版本。它首先输出一个贝尔特拉密系数，用于衡量拟保形映射的效果。

    Images degraded by geometric distortions pose a significant challenge to imaging and computer vision tasks such as object recognition. Deep learning-based imaging models usually fail to give accurate performance for geometrically distorted images. In this paper, we propose the deformation-invariant neural network (DINN), a framework to address the problem of imaging tasks for geometrically distorted images. The DINN outputs consistent latent features for images that are geometrically distorted but represent the same underlying object or scene. The idea of DINN is to incorporate a simple component, called the quasiconformal transformer network (QCTN), into other existing deep networks for imaging tasks. The QCTN is a deep neural network that outputs a quasiconformal map, which can be used to transform a geometrically distorted image into an improved version that is closer to the distribution of natural or good images. It first outputs a Beltrami coefficient, which measures the quasiconf
    
[^11]: 为了构建可信赖的医疗AI系统，筛选天然对立数据集

    Curating Naturally Adversarial Datasets for Trustworthy AI in Healthcare. (arXiv:2309.00543v1 [cs.LG])

    [http://arxiv.org/abs/2309.00543](http://arxiv.org/abs/2309.00543)

    提出了一种方法来筛选自然对立示例的数据集，以评估模型的鲁棒性，并通过自动弱监督标注获得的概率标签来实现这一方法。

    

    深度学习模型在时间序列的医疗应用中展示出了有希望的预测准确性。然而，确保这些模型的鲁棒性对于构建可信赖的AI系统至关重要。现有研究主要关注于对合成对立示例的鲁棒性，这些示例是通过向清洁输入数据添加难以察觉的扰动而制作出来的。然而，这些合成对立示例并不能准确反映最具挑战性的现实场景，特别是在医疗数据的背景下。因此，对合成对立示例的鲁棒性未必能够转化为对自然产生的对立示例的鲁棒性，而这对于可信赖的AI而言是非常重要的。我们提出了一种筛选由自然对立示例组成的数据集来评估模型鲁棒性的方法。该方法依赖于通过自动弱监督标注获得的概率标签，这种标签结合了嘈杂且易获得的标注启发式方法。

    Deep learning models have shown promising predictive accuracy for time-series healthcare applications. However, ensuring the robustness of these models is vital for building trustworthy AI systems. Existing research predominantly focuses on robustness to synthetic adversarial examples, crafted by adding imperceptible perturbations to clean input data. However, these synthetic adversarial examples do not accurately reflect the most challenging real-world scenarios, especially in the context of healthcare data. Consequently, robustness to synthetic adversarial examples may not necessarily translate to robustness against naturally occurring adversarial examples, which is highly desirable for trustworthy AI. We propose a method to curate datasets comprised of natural adversarial examples to evaluate model robustness. The method relies on probabilistic labels obtained from automated weakly-supervised labeling that combines noisy and cheap-to-obtain labeling heuristics. Based on these labels
    
[^12]: 基于文本的维基百科文章链接预测方法

    A Text-based Approach For Link Prediction on Wikipedia Articles. (arXiv:2309.00317v1 [cs.AI])

    [http://arxiv.org/abs/2309.00317](http://arxiv.org/abs/2309.00317)

    本研究提出了一种基于文本的维基百科文章链接预测方法，通过利用传统的机器学习模型和词性标注特征，实现了对两个节点是否有链接的分类预测，并在比赛中取得了较高的成绩。

    

    本文介绍了我们在DSAA 2023挑战中关于维基百科文章链接预测的工作。我们利用传统的机器学习模型以及从文本中提取的词性标注特征来训练分类模型，用于预测两个节点是否有链接。然后，我们使用这些特征在各种机器学习模型上进行测试。我们通过F1得分为0.99999获得了第7名。我们的源代码可以在以下链接中公开获取：https://github.com/Tam1032/DSAA2023-Challenge-Link-prediction-DS-UIT_SAT

    This paper present our work in the DSAA 2023 Challenge about Link Prediction for Wikipedia Articles. We use traditional machine learning models with POS tags (part-of-speech tags) features extracted from text to train the classification model for predicting whether two nodes has the link. Then, we use these tags to test on various machine learning models. We obtained the results by F1 score at 0.99999 and got 7th place in the competition. Our source code is publicly available at this link: https://github.com/Tam1032/DSAA2023-Challenge-Link-prediction-DS-UIT_SAT
    
[^13]: 一种利用移动数据进行实时需求响应的深度强化学习方法来缓解车站拥挤的铁路重新调度

    A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data. (arXiv:2308.11849v1 [eess.SY])

    [http://arxiv.org/abs/2308.11849](http://arxiv.org/abs/2308.11849)

    本文提出了一种利用移动数据进行实时需求响应的深度强化学习方法来缓解车站拥挤的铁路重新调度。该方法通过推断真实世界的乘客流动性来实现实时调度，并着重考虑高需求车站的重新安排。

    

    实时铁路重新调度是一种及时灵活的技术，可以根据时变条件自动改变运营计划。目前的研究缺乏基于数据驱动的方法，能够捕捉铁路中乘客在紧急情况下的实时流动性，主要依赖基于OD的数据和基于模型的方法来估计列车的需求。与此同时，现有的长期紧急情况下的调度更新原则忽视了需求在时间上的不均匀分布。为填补这一空白，本文提出了一种需求响应的方法，通过从移动数据中推断出真实世界的乘客流动性来促进实时调度。与网络层面的方法不同，本文着重关注紧急区域上游的高需求车站。目标是重新安排通过该目标站的多条线路上受到严重突发事件（如自然灾害）影响的所有列车。需要特别注意避免积累。

    Real-time railway rescheduling is a timely and flexible technique to automatically alter the operation schedule in response to time-varying conditions. Current research lacks data-driven approaches that capture real-time passenger mobility during railway disruptions, relying mostly on OD-based data and model-based methods for estimating demands of trains. Meanwhile, the schedule-updating principles for a long-term disruption overlook the uneven distribution of demand over time. To fill this gap, this paper proposes a demand-responsive approach by inferring real-world passenger mobility from mobile data (MD) to facilitate real-time rescheduling. Unlike network-level approaches, this paper focuses on a heavy-demand station upstream of the disrupted area. The objective is to reschedule all trains on multiple routes passing through this target station, which have been affected by a severe emergency event such as a natural disaster. Particular attention should be given to avoiding the accum
    
[^14]: TPTU: 基于大型语言模型的AI代理用于任务规划和工具使用

    TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage. (arXiv:2308.03427v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.03427](http://arxiv.org/abs/2308.03427)

    基于大型语言模型的AI代理用于任务规划和工具使用。我们提出了一个结构框架，设计了两种代理来执行推理过程，实例化了框架，并评估了它们的任务规划和工具使用能力。

    

    随着自然语言处理的最新进展，大型语言模型（LLMs）已经成为各种实际应用中的强大工具。尽管它们非常强大，但是LLMs的内在生成能力可能不足以处理复杂的任务，这些任务需要结合任务规划和外部工具的使用。在本文中，我们首先提出了一个专门为LLM-based AI Agents量身定制的结构框架，并讨论了处理复杂问题所必需的关键能力。在这个框架内，我们设计了两种不同类型的代理（即一步代理和连续代理）来执行推理过程。随后，我们使用各种LLMs实例化了这个框架，并评估了它们在典型任务中的任务规划和工具使用能力。通过强调关键发现和挑战，我们的目标是为研究人员和实践者提供一个有助于在他们的AI应用中发挥LLMs能力的有用资源。我们的研究强调了

    With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasiz
    
[^15]: 如何扩展您的EMA（arXiv:2307.13813v1 [stat.ML]）

    How to Scale Your EMA. (arXiv:2307.13813v1 [stat.ML])

    [http://arxiv.org/abs/2307.13813](http://arxiv.org/abs/2307.13813)

    本研究提供了在存在模型EMA的情况下进行优化的缩放规则，以保持训练动态的一致性。这对于实际机器学习中的权衡批量大小和墙钟时间非常重要。模型EMA能够提高模型的性能以及稳定训练过程，并为自监督学习提供学习信号。

    

    在实际机器学习中，保持训练动态在批量大小之间的一致性是一种重要工具，它能够在批量大小和墙钟时间之间进行权衡。这种权衡通常通过一个缩放规则来实现，例如，在随机梯度下降中，应该将学习率与批量大小呈线性关系。另一个实际机器学习的重要工具是模型指数移动平均（EMA），它是一个不接收梯度信息的模型副本，而是以一定的动量跟随其目标模型。这个模型EMA可以提高监督学习的稳健性和泛化性能，稳定伪标记，为自监督学习提供学习信号。之前的研究将模型EMA与优化分开处理，导致批量大小之间存在不同的训练动态和较低的模型性能。在这项工作中，我们提供了在存在模型EMA的情况下进行优化的缩放规则，并展示了其效果。

    Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important tool for practical machine learning is the model Exponential Moving Average (EMA), which is a model copy that does not receive gradient information, but instead follows its target model with some momentum. This model EMA can improve the robustness and generalization properties of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonst
    
[^16]: FedMEKT: 基于蒸馏的多模态联邦学习中的嵌入知识传输

    FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning. (arXiv:2307.13214v1 [cs.LG])

    [http://arxiv.org/abs/2307.13214](http://arxiv.org/abs/2307.13214)

    FedMEKT是一种基于蒸馏的多模态联邦学习框架，通过半监督学习方法利用不同模态的表示，实现了服务器和客户端之间的联合知识传输。

    

    联邦学习（FL）使多个客户端能够在不共享私有数据的情况下协同训练一个广义全局模型，从而实现分散式的机器学习范式。现有的大部分工作只是针对单模态数据提出了典型的FL系统，因此限制了它对于利用宝贵的多模态数据进行未来个性化应用的潜力。此外，大多数FL方法仍然依赖于客户端的标记数据，由于用户无法进行自注释，这在实际应用中是有限的。鉴于这些限制，我们提出了一种新型的多模态FL框架，采用半监督学习方法利用不同模态的表示。将这个概念引入一个系统中，我们开发了一种基于蒸馏的多模态嵌入知识传输机制，称为FedMEKT，它允许服务器和客户端交换从小型多模态数据集中提取的学习模型的联合知识。

    Federated learning (FL) enables a decentralized machine learning paradigm for multiple clients to collaboratively train a generalized global model without sharing their private data. Most existing works simply propose typical FL systems for single-modal data, thus limiting its potential on exploiting valuable multimodal data for future personalized applications. Furthermore, the majority of FL approaches still rely on the labeled data at the client side, which is limited in real-world applications due to the inability of self-annotation from users. In light of these limitations, we propose a novel multimodal FL framework that employs a semi-supervised learning approach to leverage the representations from different modalities. Bringing this concept into a system, we develop a distillation-based multimodal embedding knowledge transfer mechanism, namely FedMEKT, which allows the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal 
    
[^17]: 大型语言模型作为文化角度的叠加

    Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])

    [http://arxiv.org/abs/2307.07870](http://arxiv.org/abs/2307.07870)

    大型语言模型被认为是具有个性或一套价值观的，但实际上它可以看作是具有不同价值观和个性特征的角度的叠加。通过角度可控性的概念，我们研究了大型语言模型在不同角度下展示的价值观和个性特征的变化。实验结果表明，即使在没有明显提示的情况下，大型语言模型也会表达出不同的价值观。

    

    大型语言模型（LLMs）常常被错误地认为具有个性或一套价值观。我们认为LLMs可以看作是具有不同价值观和个性特征的角度叠加。LLMs表现出依赖于上下文的价值观和个性特征，这些特征基于产生的角度而改变（与人类相反，人类在不同情境下通常具有更一致的价值观和个性特征）。我们引入了“角度可控性”的概念，指的是模型采用不同具有不同价值观和个性特征的角度的能力。在我们的实验中，我们使用心理学问卷（PVQ、VSM、IPIP）来研究展示的价值观和个性特征如何基于不同角度而改变。通过定性实验，我们展示了当提示中（隐式或显式）暗示了某些价值观时，LLMs表达出不同的价值观，即使在没有明显暗示的情况下，LLMs也会表达出不同的价值观。

    Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrat
    
[^18]: FAIRO: 面向人机交互系统中顺序决策的公平适应性

    FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems. (arXiv:2307.05857v1 [cs.LG])

    [http://arxiv.org/abs/2307.05857](http://arxiv.org/abs/2307.05857)

    这篇论文提出了一种名为FAIRO的算法，用于在人机交互环境中实现公平的顺序决策。该算法将人的行为可变性和偏好变化考虑在内，通过利用选项强化学习框架将复杂的公平任务分解为自适应子任务。

    

    在人机交互环境中实现顺序决策系统的公平性是一个重要问题，特别是当多个具有不同行为和期望的人受到系统中相同适应决策的影响时。人的可变性因素增加了复杂性，因为在某一时间点被认为是公平的政策可能会随着时间的推移由于人的偏好变化而成为歧视性政策。本文从公平性视角考虑人的行为可变性和人的偏好变化，解决了公平性问题。我们提出了一种新颖的算法FAIRO，用于面向人机交互适应的公平顺序决策，它将这些概念纳入决策过程中。具体而言，FAIRO将这个复杂的公平任务基于个体人的偏好通过利用选项强化学习框架分解为自适应子任务。

    Achieving fairness in sequential-decision making systems within Human-in-the-Loop (HITL) environments is a critical concern, especially when multiple humans with different behavior and expectations are affected by the same adaptation decisions in the system. This human variability factor adds more complexity since policies deemed fair at one point in time may become discriminatory over time due to variations in human preferences resulting from inter- and intra-human variability. This paper addresses the fairness problem from an equity lens, considering human behavior variability, and the changes in human preferences over time. We propose FAIRO, a novel algorithm for fairness-aware sequential-decision making in HITL adaptation, which incorporates these notions into the decision-making process. In particular, FAIRO decomposes this complex fairness task into adaptive sub-tasks based on individual human preferences through leveraging the Options reinforcement learning framework. We design 
    
[^19]: 时间差分强化学习的动态

    Dynamics of Temporal Difference Reinforcement Learning. (arXiv:2307.04841v1 [stat.ML])

    [http://arxiv.org/abs/2307.04841](http://arxiv.org/abs/2307.04841)

    我们使用统计物理学的概念，研究了时间差分学习在线性函数逼近器下的典型学习曲线。我们发现由于子采样可能的轨迹空间而产生的随机半梯度噪声会导致值误差出现显著的平台。

    

    强化学习在需要学习在反馈有限的环境中行动的多个应用中取得了成功。然而，尽管有这种经验上的成功，仍然没有对强化学习模型的参数和用于表示状态的特征如何相互作用控制学习动态的理论理解。在这项工作中，我们使用统计物理学的概念，研究线性函数逼近器下时间差分学习价值函数的典型学习曲线。我们的理论是在一个高斯等效假设下推导出来的，其中对随机轨迹的平均值被替换为时态相关的高斯特征平均值，并且我们在小规模马尔可夫决策过程上验证了我们的假设。我们发现，由于对可能的轨迹空间进行子采样而产生的随机半梯度噪声导致值误差出现显著的平台，这与传统的梯度下降不同。

    Reinforcement learning has been successful across several applications in which agents have to learn to act in environments with sparse feedback. However, despite this empirical success there is still a lack of theoretical understanding of how the parameters of reinforcement learning models and the features used to represent states interact to control the dynamics of learning. In this work, we use concepts from statistical physics, to study the typical case learning curves for temporal difference learning of a value function with linear function approximators. Our theory is derived under a Gaussian equivalence hypothesis where averages over the random trajectories are replaced with temporally correlated Gaussian feature averages and we validate our assumptions on small scale Markov Decision Processes. We find that the stochastic semi-gradient noise due to subsampling the space of possible episodes leads to significant plateaus in the value error, unlike in traditional gradient descent 
    
[^20]: 逆向蛋白质折叠的图去噪扩散

    Graph Denoising Diffusion for Inverse Protein Folding. (arXiv:2306.16819v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.16819](http://arxiv.org/abs/2306.16819)

    我们提出了一种逆向蛋白质折叠的图去噪扩散模型，通过扩散过程生成了一组多样的候选序列，该模型能够很好地概括多样的可行解决方案。

    

    逆向蛋白质折叠具有一对多的映射特性，这使得它具有挑战性，因为许多可能的氨基酸序列可以折叠成一个相同的蛋白质骨架。这个任务不仅涉及到识别可行的序列，还要表示潜在解决方案的多样性。然而，现有的鉴别模型，如基于Transformer的自回归模型，很难概括各种可行解的多样性。相反，作为一种新兴的生成方法，扩散概率模型提供了生成一组多样序列候选的潜力，用于确定蛋白质骨架。我们提出了一种新颖的逆向蛋白质折叠的图去噪扩散模型，其中给定的蛋白质骨架指导对应氨基酸残基类型的扩散过程。该模型推断了以节点的物理化学属性和局部环境为条件的氨基酸的联合分布。

    Inverse protein folding is challenging due to its inherent one-to-many mapping characteristic, where numerous possible amino acid sequences can fold into a single, identical protein backbone. This task involves not only identifying viable sequences but also representing the sheer diversity of potential solutions. However, existing discriminative models, such as transformer-based auto-regressive models, struggle to encapsulate the diverse range of plausible solutions. In contrast, diffusion probabilistic models, as an emerging genre of generative approaches, offer the potential to generate a diverse set of sequence candidates for determined protein backbones. We propose a novel graph denoising diffusion model for inverse protein folding, where a given protein backbone guides the diffusion process on the corresponding amino acid residue types. The model infers the joint distribution of amino acids conditioned on the nodes' physiochemical properties and local environment. Moreover, we uti
    
[^21]: 比较因果框架：潜在结果、结构模型、图和抽象

    Comparing Causal Frameworks: Potential Outcomes, Structural Models, Graphs, and Abstractions. (arXiv:2306.14351v1 [stat.ME])

    [http://arxiv.org/abs/2306.14351](http://arxiv.org/abs/2306.14351)

    本文比较了因果框架中的潜在结果模型(RCM)和结构因果模型(SCM)，并阐明了RCM成为SCM可表达的条件，以及每个RCM作为某些可表达的RCM的抽象。作者介绍了SCM原则在RCM经典应用中的重要作用，并提出了由图表示的代数约束的特征，有助于进一步比较两个框架。

    

    本文旨在阐明潜在结果模型（RCM）与结构因果模型（SCM）框架在因果推断中的关系。采用中立的逻辑视角，借鉴以前的研究成果，我们展示了RCM成为SCM可表达的条件。一个关键结果显示，每个RCM -- 包括那些违反SCM框架中暗示的代数原则的RCM -- 作为某些可表达的RCM的抽象而出现。最后，我们通过准确定位SCM原则在RCM经典应用中的重要作用，阐明了这种改进性视角的优势；反之，我们提供了由图表示的代数约束的特征，有助于进一步比较两个框架。

    The aim of this paper is to make clear and precise the relationship between the Rubin causal model (RCM) and structural causal model (SCM) frameworks for causal inference. Adopting a neutral logical perspective, and drawing on previous work, we show what is required for an RCM to be representable by an SCM. A key result then shows that every RCM -- including those that violate algebraic principles implied by the SCM framework -- emerges as an abstraction of some representable RCM. Finally, we illustrate the power of this ameliorative perspective by pinpointing an important role for SCM principles in classic applications of RCMs; conversely, we offer a characterization of the algebraic constraints implied by a graph, helping to substantiate further comparisons between the two frameworks.
    
[^22]: 根据退化间隙参数化的快速$k$-Plex算法

    A Fast Maximum $k$-Plex Algorithm Parameterized by the Degeneracy Gap. (arXiv:2306.13258v1 [cs.DS])

    [http://arxiv.org/abs/2306.13258](http://arxiv.org/abs/2306.13258)

    本文提出了一个新参数$g_k(G)$用于最大$k$-plex问题，针对其设计了一个根据$g_k(G)$参数化的精确算法，具有较高的时间复杂度，但可以在实际图中得到应用。

    

    给定一个图，$k$-plex是一个顶点集，其中每个顶点与该集合中最多$k-1$个其他顶点不相邻。最大$k$-plex问题是从给定的图中寻找最大$k$-plex，是图搜索和社区检测等应用中重要但具有挑战性的计算问题。目前，存在许多经验算法，在效率方面没有足够的理论解释。我们通过定义输入实例的一个新参数$g_k(G)$，最大$k$-plex的退化边界和大小之间的差距，并提出了一个根据$g_k(G)$参数化的精确算法，来填补这个空白。换句话说，我们设计了一个算法，其运行时间多项式复杂度与输入图的大小成正比，指数复杂度与$g_k(G)$成正比，其中$k$是一个常数。通常，实际图的$g_k(G)$很小，被$O(\log{(|V|)})$限制，这表明该算法的运行时间多项式复杂度。我们还进行了大量的实验。

    Given a graph, the $k$-plex is a vertex set in which each vertex is not adjacent to at most $k-1$ other vertices in the set. The maximum $k$-plex problem, which asks for the largest $k$-plex from a given graph, is an important but computationally challenging problem in applications like graph search and community detection. So far, there is a number of empirical algorithms without sufficient theoretical explanations on the efficiency. We try to bridge this gap by defining a novel parameter of the input instance, $g_k(G)$, the gap between the degeneracy bound and the size of maximum $k$-plex in the given graph, and presenting an exact algorithm parameterized by $g_k(G)$. In other words, we design an algorithm with running time polynomial in the size of input graph and exponential in $g_k(G)$ where $k$ is a constant. Usually, $g_k(G)$ is small and bounded by $O(\log{(|V|)})$ in real-world graphs, indicating that the algorithm runs in polynomial time. We also carry out massive experiments
    
[^23]: 简单且可控的音乐生成

    Simple and Controllable Music Generation. (arXiv:2306.05284v1 [cs.SD])

    [http://arxiv.org/abs/2306.05284](http://arxiv.org/abs/2306.05284)

    本文提出了 MusicGen，一个单一的语言模型，可以在条件描述或旋律特征控制下生成高质量的样本，并且在标准的文本到音乐基准上的实证研究中，该方法优于其他基线模型。

    

    本研究解决了条件音乐生成的问题。我们介绍了MusicGen，它是一个单一的语言模型，可以操作多个压缩离散音乐表示流，即令牌。与以往的工作不同，MusicGen由一个单一阶段的Transformer LM和高效的令牌交错模式组成，消除了级联多个模型的需要，例如分层或上采样。采用这种方法，我们展示了MusicGen如何在条件描述或旋律特征的控制下生成高质量的样本。我们进行了广泛的实证评估，考虑了自动和人为研究，展示了所提出的方法优于标准文本到音乐基准上评估的基线。通过消融研究，我们阐明了MusicGen所包含组件的重要性。音乐样本、代码和模型可以在https://github.com/fac找到。

    We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/fac
    
[^24]: 部署一个强健的积极偏好调查算法：COVID-19患者优先排序的实验设计、界面和评估

    Deploying a Robust Active Preference Elicitation Algorithm: Experiment Design, Interface, and Evaluation for COVID-19 Patient Prioritization. (arXiv:2306.04061v1 [cs.HC])

    [http://arxiv.org/abs/2306.04061](http://arxiv.org/abs/2306.04061)

    本论文介绍了一种强健的偏好调查算法，旨在优先为COVID-19患者提供医院资源，通过招募亚马逊机械土耳其工作者对该算法进行评估。

    

    偏好调查利用AI或优化来学习从市场到公共政策等各种情况下的利益相关者的偏好。arXiv:2003.01899的在线健壮偏好调查流程在模拟中表现出优于其他偏好调查流程的效果，可以有效地学习个体的真实效用。然而，与任何模拟一样，该方法做出了一系列无法轻易验证是否在模拟以外成立的假设。因此，我们建议验证这种健壮方法在部署中的表现，重点关注选择COVID-19患者的政策，以优先为疫情期间的医院资源提供服务。为此，我们开发了一个在线偏好调查平台，用户通过特定的偏好调查过程报告他们在少量成对比较中的偏好。我们招募了193位Amazon Mechanical Turk工作者来报告他们的偏好。

    Preference elicitation leverages AI or optimization to learn stakeholder preferences in settings ranging from marketing to public policy. The online robust preference elicitation procedure of arXiv:2003.01899 has been shown in simulation to outperform various other elicitation procedures in terms of effectively learning individuals' true utilities. However, as with any simulation, the method makes a series of assumptions that cannot easily be verified to hold true beyond simulation. Thus, we propose to validate the robust method's performance in deployment, focused on the particular challenge of selecting policies for prioritizing COVID-19 patients for scarce hospital resources during the pandemic. To this end, we develop an online platform for preference elicitation where users report their preferences between alternatives over a moderate number of pairwise comparisons chosen by a particular elicitation procedure. We recruit Amazon Mechanical Turk workers ($n$ = 193) to report their p
    
[^25]: 位置编码对 Transformer 模型长度推广的影响

    The Impact of Positional Encoding on Length Generalization in Transformers. (arXiv:2305.19466v1 [cs.CL])

    [http://arxiv.org/abs/2305.19466](http://arxiv.org/abs/2305.19466)

    本文通过实证研究 Transformer 模型中位置编码对于长度推广的影响，结果表明常用的位置编码方法并不适合用于下游任务的长度推广，并且使用位置编码甚至可能会损害长度推广的能力。

    

    Transformer-based 语言模型的开发中，长度推广是一个关键的挑战，它是指从小的训练文本范围到更大范围的泛化能力。位置编码（PE）被发现是影响长度推广的主要因素之一，但不同的 PE 方案对下游任务的外推影响还不清楚。本文通过对比评估五种不同位置编码方法（包括绝对位置嵌入、T5 的相对 PE、ALiBi、Rotary 和无位置编码）的解码器 Transformer 的长度推广能力，对推理和数学任务进行了系统的实证研究。研究发现，常用的位置编码方法，如 ALiBi、Rotary 和 APE，并不适合用于下游任务的长度推广。更重要的是，无 PE 的 Transformer 在推理任务中的表现优于其他显式 PE 方法，这意味着使用位置编码实际上可能会损害长度推广的能力。这些发现揭示了位置编码在有效 Transformer 模型开发中的重要作用。

    Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other expli
    
[^26]: CongFu: 用于药物协同预测的条件图融合

    CongFu: Conditional Graph Fusion for Drug Synergy Prediction. (arXiv:2305.14517v1 [cs.LG])

    [http://arxiv.org/abs/2305.14517](http://arxiv.org/abs/2305.14517)

    CongFu是一种用于药物协同预测的新型条件图融合层，采用注意机制和瓶颈技术提取本地图上下文，并在全局上下文中有条件地融合图数据。其模块化架构使得可以更换图层模块，包括读出和图编码器，以适应不同的应用场景，并在12个基准数据集中的11个上取得了最先进的结果，证明了其能力。

    

    药物协同是指多种药物联合作用所产生的合成效应，对于优化治疗结果非常关键。然而，可能的药物组合数量巨大，计算成本高，导致药物协同数据有限，需要预测方法。在本文中，我们引入了一种新颖的条件图融合层——CongFu，用于预测药物协同作用。CongFu采用注意机制和瓶颈技术来提取本地图上下文并在全局上下文中有条件地融合图数据。其模块化架构使得可以更换图层模块，包括读出和图编码器，以适应不同的应用场景。为了评估CongFu的性能，我们对四个数据集进行了全面的实验，涵盖了三种不同的药物协同预测设置。值得注意的是，CongFu在12个基准数据集中的11个上取得了最先进的结果，证明了其能力。

    Drug synergy, characterized by the amplified combined effect of multiple drugs, presents a critical phenomenon for optimizing therapeutic outcomes. However, limited data on drug synergy, arising from the vast number of possible drug combinations and computational costs, motivate the need for predictive methods. In this work, we introduce CongFu, a novel Conditional Graph Fusion Layer, designed to predict drug synergy. CongFu employs an attention mechanism and a bottleneck to extract local graph contexts and conditionally fuse graph data within a global context. Its modular architecture enables flexible replacement of layer modules, including readouts and graph encoders, facilitating customization for diverse applications. To evaluate the performance of CongFu, we conduct comprehensive experiments on four datasets, encompassing three distinct setups for drug synergy prediction. Remarkably, CongFu achieves state-of-the-art results on 11 out of 12 benchmark datasets, demonstrating its abi
    
[^27]: MindGames：利用动态认知模态逻辑在大型语言模型中针对心智理论进行研究

    MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic. (arXiv:2305.03353v1 [cs.CL])

    [http://arxiv.org/abs/2305.03353](http://arxiv.org/abs/2305.03353)

    本文利用动态认知逻辑在自然语言处理模型中探讨了理解心智理论的方法。虽然GPT-4表现出改进的能力，但需要进一步提高。

    

    心智理论(ToM)是智能的重要组成部分，但准确度量它仍然是一个争议话题。先前的研究尝试将人类ToM评估应用于自然语言处理模型，使用人类创建的标准化测试或基于规则的模板。然而，这些方法主要集中在简单的推理上，并需要进一步验证。在本研究中，我们利用具有与ToM重叠的动态认知逻辑来生成更复杂的问题。我们还引入新的语言技巧来用自然语言表达这些问题。我们的研究结果表明，特定的语言模型缩放（从70M到6B和350M到174B）并不一致地产生比随机结果更好的结果。虽然GPT-4展示了改进的认知推理能力，但仍有提升空间。我们的代码和数据集可在以下链接公开获取：https://github.com/antoinelrnld/modlog https://huggingface.co/datasets/sileo

    Theory of Mind (ToM) is a critical component of intelligence, yet accurately measuring it continues to be a subject of debate. Prior research has attempted to apply human ToM assessments to natural language processing models using either human-created standardized tests or rule-based templates. However, these methods primarily focus on simplistic reasoning and require further validation. In this study, we utilize dynamic epistemic logic, which has established overlaps with ToM, to generate more intricate problems. We also introduce novel verbalization techniques to express these problems using natural language. Our findings indicate that certain language model scaling (from 70M to 6B and 350M to 174B) does not consistently yield results better than random chance. While GPT-4 demonstrates improved epistemic reasoning capabilities, there is still room for enhancement. Our code and datasets are publicly available https://github.com/antoinelrnld/modlog https://huggingface.co/datasets/sileo
    
[^28]: 利用室内WiFi系统进行无设备穿墙存在检测的注意增强深度学习

    Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System. (arXiv:2304.13105v1 [cs.LG])

    [http://arxiv.org/abs/2304.13105](http://arxiv.org/abs/2304.13105)

    本文提出了一种利用WiFi信号进行人员存在检测的新系统，采用了关注机制和双向LSTM网络来提高准确性，并证明了其在现实场景中的稳健性。

    

    在室内环境中准确检测人员存在对于各种应用非常重要，例如能源管理和安全。本文提出了一种利用WiFi信号的通道状态信息（CSI）进行人员存在检测的新系统。我们的系统名为注意力增强深度学习（ALPD），采用关注机制从CSI数据中自动选择有信息量的子载波，并采用双向长短时记忆（LSTM）网络捕捉CSI中的时间依赖性。此外，我们利用一个静态特征来提高静态状态下人员存在检测的准确性。我们通过部署一对WiFi接入点（AP）来收集CSI数据集来评估所提出的ALPD系统，该系统进一步与几个基准进行比较。结果表明，我们的ALPD系统在准确性方面优于基准，特别是在存在干扰的情况下。此外，双向传输数据不会影响我们系统的性能，证明了其在现实场景中的稳健性。

    Accurate detection of human presence in indoor environments is important for various applications, such as energy management and security. In this paper, we propose a novel system for human presence detection using the channel state information (CSI) of WiFi signals. Our system named attention-enhanced deep learning for presence detection (ALPD) employs an attention mechanism to automatically select informative subcarriers from the CSI data and a bidirectional long short-term memory (LSTM) network to capture temporal dependencies in CSI. Additionally, we utilize a static feature to improve the accuracy of human presence detection in static states. We evaluate the proposed ALPD system by deploying a pair of WiFi access points (APs) for collecting CSI dataset, which is further compared with several benchmarks. The results demonstrate that our ALPD system outperforms the benchmarks in terms of accuracy, especially in the presence of interference. Moreover, bidirectional transmission data 
    
[^29]: 基于Transformer的机器人学习中的空间-语言注意力策略

    Spatial-Language Attention Policies for Efficient Robot Learning. (arXiv:2304.11235v1 [cs.RO])

    [http://arxiv.org/abs/2304.11235](http://arxiv.org/abs/2304.11235)

    本文提出了一种空间-语言注意力策略(SLAP)，使用三维标记作为输入表示，以训练单一多任务和语言条件化的动作预测策略，能够在引入未见过的干扰物和物体配置时达到47.5%的成功率。

    

    本文研究了如何使用Transformer建立和训练机器人决策制定的空间表示。具体来说，为了使机器人能够在各种环境中运行，我们必须能够快速训练或微调机器人感知和动作策略，以适应不同的情况并具有数据效率和鲁棒性。作为解决方案，我们提出了一种空间-语言注意力策略（SLAP）。SLAP使用三维标记作为输入表示，以训练单一多任务、语言条件化的动作预测策略。我们的方法在真实世界中展示了80%的成功率，跨越了8项任务并仅使用单一模型，在引入未见过的干扰物和物体配置时仍保持了47.5%的成功率，即使每个任务仅使用少数示例。相对于先前的工作（仅使用未见干扰物和配置的情况下，成功率为20%），这表示了30%的提高。

    We investigate how to build and train spatial representations for robot decision making with Transformers. In particular, for robots to operate in a range of environments, we must be able to quickly train or fine-tune robot sensorimotor policies that are robust to clutter, data efficient, and generalize well to different circumstances. As a solution, we propose Spatial Language Attention Policies (SLAP). SLAP uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy. Our method shows 80% success rate in the real world across eight tasks with a single model, and a 47.5% success rate when unseen clutter and unseen object configurations are introduced, even with only a handful of examples per task. This represents an improvement of 30% over prior work (20% given unseen distractors and configurations).
    
[^30]: 多尺度进化神经架构搜索用于深度脉冲神经网络

    Multi-scale Evolutionary Neural Architecture Search for Deep Spiking Neural Networks. (arXiv:2304.10749v1 [cs.NE])

    [http://arxiv.org/abs/2304.10749](http://arxiv.org/abs/2304.10749)

    本文提出了一种新方法--多尺度进化神经架构搜索，用于自动设计脉冲神经网络，同时考虑微观、中观和宏观尺度的脑拓扑结构。MSE-NAS可以帮助SNN实现多电路模式的自组织集成，并通过全局性的跨模式连接来优化网络性能。

    

    脉冲神经网络（SNN）不仅因其离散信号处理的能源效率卓越，而且因其天然适合于集成多尺度生物可塑性而受到广泛关注。然而，大多数SNN直接采用成熟的DNN结构，很少自动设计神经架构搜索（NAS）用于SNN。人类大脑神经模式的拓扑结构，模块化的区域结构和全局性的跨脑区连接是自然进化的产物，可以作为设计基于脑的SNN架构的完美参考。本文提出了一种多尺度进化神经架构搜索（MSE-NAS），同时考虑微观、中观和宏观尺度的脑拓扑作为进化搜索空间。 MSE-NAS通过基于大脑启发的间接方式，进化单个神经元操作，多个电路模式的自组织集成以及跨模式的全局连通性。

    Spiking Neural Networks (SNNs) have received considerable attention not only for their superiority in energy efficient with discrete signal processing, but also for their natural suitability to integrate multi-scale biological plasticity. However, most SNNs directly adopt the structure of the well-established DNN, rarely automatically design Neural Architecture Search (NAS) for SNNs. The neural motifs topology, modular regional structure and global cross-brain region connection of the human brain are the product of natural evolution and can serve as a perfect reference for designing brain-inspired SNN architecture. In this paper, we propose a Multi-Scale Evolutionary Neural Architecture Search (MSE-NAS) for SNN, simultaneously considering micro-, meso- and macro-scale brain topologies as the evolutionary search space. MSE-NAS evolves individual neuron operation, self-organized integration of multiple circuit motifs, and global connectivity across motifs through a brain-inspired indirec
    
[^31]: 通过编码实现优化：重整化群的视角

    Optimisation via encodings: a renormalisation group perspective. (arXiv:2303.16258v1 [cs.AI])

    [http://arxiv.org/abs/2303.16258](http://arxiv.org/abs/2303.16258)

    本文提出了一种通过使用启发式算法构造覆盖编码映射，将大型搜索空间中的过程映射到原始搜索空间的子集中，并使用重整化群的视角来处理离散优化问题的方法。这种方法相对于单独的局部搜索方法具有更好的性能。

    

    解决离散优化问题的传统方法是使用适当定义的成本或适应性景观上的局部搜索。然而，这些方法受到典型崎岖景观中的局部最小值制约，令搜索过程变慢。另一种解决优化问题的方法是通过使用启发式逼近估计全局最小值。本文提出了这两种方法的组合，使用覆盖编码映射将大型搜索空间中的过程映射到原始搜索空间的子集中。关键思想是利用适当的启发式算法构造覆盖编码映射，在大型搜索空间上生成不再显示陷阱局部最小值的景观。通常采用的过程涉及某种形式的粗粒化，我们认为它们可以被视为统计力学中重整化群的化身。我们使用组合优化领域的测试问题来说明这种方法，发现覆盖编码映射与局部搜索的组合始终优于单独的局部搜索。

    The traditional way of tackling discrete optimization problems is by using local search on suitably defined cost or fitness landscapes. Such approaches are however limited by the slowing down that occurs when local minima, that are a feature of the typically rugged landscapes encountered, arrest the progress of the search process. Another way of tackling optimization problems is by the use of heuristic approximations to estimate a global cost minimum. Here we present a combination of these two approaches by using cover-encoding maps which map processes from a larger search space to subsets of the original search space. The key idea is to construct cover-encoding maps with the help of suitable heuristics that single out near-optimal solutions and result in landscapes on the larger search space that no longer exhibit trapping local minima. The processes that are typically employed involve some form of coarse-graining, and we suggest here that they can be viewed as avatars of renormalisat
    
[^32]: 显式规划有助于语言模型进行逻辑推理

    Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])

    [http://arxiv.org/abs/2303.15714](http://arxiv.org/abs/2303.15714)

    本文提出了一个新的系统，使用语言模型进行多步逻辑推理，采用了显式规划来帮助做出更明智的决策，比其他竞争系统表现更好，显式规划在系统性能中起着关键作用。

    

    语言模型在各种自然语言处理任务中表现出色。本文提出了一个新颖的系统，采用语言模型进行多步逻辑推理。我们的系统将显式规划纳入到推理过程中，因此可以通过展望未来的效果来做出更明智的决策。在实验中，我们的全套系统在多项选择题答题任务中明显优于其他竞争系统，尽管只有约15亿个参数，但与GPT-3-davinci表现相当。我们进行了多个消融研究以证明显式规划在系统性能中起着关键作用。

    Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
    
[^33]: 协同人工智能的根源和要求

    Roots and Requirements for Collaborative AI. (arXiv:2303.12040v1 [cs.AI])

    [http://arxiv.org/abs/2303.12040](http://arxiv.org/abs/2303.12040)

    论文探讨了AI协同合作的历史和要求，是协同AI研究的动机和背景。

    

    AI协作者的愿景长期以来一直是科幻小说的经典素材，其中人工智能代理理解协作和人类沟通的微妙差别。它们通过贡献特殊的才能给他们的人类合作者和团队带来优势。多年来，政府咨询团体和人工智能领域的领袖一直倡导AIs应该具有人类兼容性和有效协作的能力。然而，具备像才华横溢的人那样协作能力的强大的AI仍然遥不可及。这篇论文依据对人工智能和人类代理有效和强大协作所需认知的分析，概述了公众和AI愿景中关于人工协作者的历史，开始于早期智能增强(IA)和人工智能(AI)的愿景。这篇论文旨在成为协同AI的第二个立场文件(Stefik & Price, 2023)的动机和背景。第二篇论文回顾了多学科的现状，并提出了一个AI协作研究的路线图。

    The vision of AI collaborators has long been a staple of science fiction, where artificial agents understand nuances of collaboration and human communication. They bring advantages to their human collaborators and teams by contributing their special talents. Government advisory groups and leaders in AI have advocated for years that AIs should be human compatible and be capable of effective collaboration. Nonetheless, robust AIs that can collaborate like talented people remain out of reach. This position paper draws on a cognitive analysis of what effective and robust collaboration requires of human and artificial agents. It sketches a history of public and AI visions for artificial collaborators, starting with early visions of intelligence augmentation (IA) and artificial intelligence (AI). It is intended as motivation and context for a second position paper on collaborative AI (Stefik & Price, 2023). The second paper reviews the multi-disciplinary state-of-the-art and proposes a roadm
    
[^34]: 提高上下文在目标检测区域-词对齐中的作用

    Enhancing the Role of Context in Region-Word Alignment for Object Detection. (arXiv:2303.10093v1 [cs.CV])

    [http://arxiv.org/abs/2303.10093](http://arxiv.org/abs/2303.10093)

    本研究提出了一种增强上下文在目标检测区域-词对齐中作用的方法，通过特定的负采样方法提高了属性的作用，从而提高了目标检测的效果。

    

    视觉语言预训练学习图像-标注配对之间的细粒度区域-词对齐，推动了开放词汇目标检测的进展。我们观察到，区域-词对齐方法通常仅针对目标名词在检测中使用，其他上下文，例如属性，对检测的影响不明确。在本研究中，我们探讨了语言上下文如何影响下游目标检测，并提议增强上下文的作用。特别地，我们展示了如何策略性地将接地预训练目标情境化以实现更好的对齐。我们进一步研究了属性作为特别有用的目标上下文并提出了一种新的基于形容词和名词的负采样策略，以增加对它们的对比学习的关注。总的来说，与区域-词预训练的最新技术相比，我们的方法提升了目标检测的效果。我们还通过文本-区域可视化显示属性敏感模型的细粒度实用性。

    Vision-language pretraining to learn a fine-grained, region-word alignment between image-caption pairs has propelled progress in open-vocabulary object detection. We observe that region-word alignment methods are typically used in detection with respect to only object nouns, and the impact of other rich context in captions, such as attributes, is unclear. In this study, we explore how language context affects downstream object detection and propose to enhance the role of context. In particular, we show how to strategically contextualize the grounding pretraining objective for improved alignment. We further hone in on attributes as especially useful object context and propose a novel adjective and noun-based negative sampling strategy for increasing their focus in contrastive learning. Overall, our methods enhance object detection when compared to the state-of-the-art in region-word pretraining. We also highlight the fine-grained utility of an attribute-sensitive model through text-regi
    
[^35]: 国家癌症研究所影像数据共享平台：计算病理学可重复研究的基础

    The NCI Imaging Data Commons as a platform for reproducible research in computational pathology. (arXiv:2303.09354v1 [cs.CV])

    [http://arxiv.org/abs/2303.09354](http://arxiv.org/abs/2303.09354)

    国家癌症研究所影像数据共享平台 (IDC) 旨在促进计算病理学领域的研究可重复性，实现了 FAIR 原则，提供公共库和云端技术支持，方便使用机器学习方法进行癌症组织分类研究。

    

    目的：可重复性对于将计算病理学（CompPath）中基于机器学习（ML）的解决方案转化为实践至关重要。然而，越来越多的研究报告难以重复 ML 结果的困难。国家癌症研究所影像数据共享平台（IDC）是一个公共库，包含 >120 个癌症图像收集，包括 >38,000 张全切片图像（WSIs），旨在与云端 ML 服务一起使用。本文探讨了 IDC 促进 CompPath 研究可重复性的潜力。 材料和方法：IDC 实现了 FAIR 原则：所有图像都根据 DICOM 标准进行编码，具有持久化标识符、可通过丰富的元数据进行发现，并可通过开放式工具访问。借此优势，我们在 IDC 的不同数据集上实现了两个实验，针对肺癌组织分类的一种代表性基于 ML 的方法进行了训练和/或评估。为评估可重复性，实验被多次运行。

    Objective: Reproducibility is critical for translating machine learning-based (ML) solutions in computational pathology (CompPath) into practice. However, an increasing number of studies report difficulties in reproducing ML results. The NCI Imaging Data Commons (IDC) is a public repository of >120 cancer image collections, including >38,000 whole-slide images (WSIs), that is designed to be used with cloud-based ML services. Here, we explore the potential of the IDC to facilitate reproducibility of CompPath research.  Materials and Methods: The IDC realizes the FAIR principles: All images are encoded according to the DICOM standard, persistently identified, discoverable via rich metadata, and accessible via open tools. Taking advantage of this, we implemented two experiments in which a representative ML-based method for classifying lung tumor tissue was trained and/or evaluated on different datasets from the IDC. To assess reproducibility, the experiments were run multiple times with i
    
[^36]: DynGFN: 借助RNA速度技术进行贝叶斯基因调控网络推断的GFlowNets方法

    DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets. (arXiv:2302.04178v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04178](http://arxiv.org/abs/2302.04178)

    DynGFN是一种借助RNA速度技术进行基因调控网络推断的方法，能够捕捉网络结构的不确定性，并在准确度上超过现有方法。

    

    细胞生物学的一个重要挑战是推断基因调控网络（GRN），该网络描述了控制基因表达和细胞功能的基因及其产物之间的相互作用。本文借助RNA速度技术开发了一种方法DynGFN，该方法训练生成流网络，使用RNA速度数据执行基因调控网络的贝叶斯推断，并捕捉网络结构的不确定性。

    One of the grand challenges of cell biology is inferring the gene regulatory network (GRN) which describes interactions between genes and their products that control gene expression and cellular function. We can treat this as a causal discovery problem but with two non-standard challenges: (1) regulatory networks are inherently cyclic so we should not model a GRN as a directed acyclic graph (DAG), and (2) observations have significant measurement noise, so for typical sample sizes there will always be a large equivalence class of graphs that are likely given the data, and we want methods that capture this uncertainty. Existing methods either focus on challenge (1), identifying cyclic structure from dynamics, or on challenge (2) learning complex Bayesian posteriors over DAGs, but not both. In this paper we leverage the fact that it is possible to estimate the "velocity" of gene expression with RNA velocity techniques to develop an approach that addresses both challenges. Because we have
    
[^37]: 神经符号模型的自然约束担保一致性

    Guaranteed Conformance of Neurosymbolic Models to Natural Constraints. (arXiv:2212.01346v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01346](http://arxiv.org/abs/2212.01346)

    该论文提出了一种能够担保数据驱动模型符合自然科学已有知识并最优逼近系统模型的方法。

    

    深度神经网络已经成为大部分机器人和控制应用的主要模型，特别是作为动态系统模型。这类数据驱动模型又可以用于设计和验证自主系统。在医疗系统建模方面尤其有用，因为数据能够被用于个体化治疗。在安全关键的应用中，数据驱动模型对来自自然科学的已有知识的符合性显得尤为重要。这些知识通常是可用的，或者可以被浓缩成（可能是黑盒的）模型。例如，F1赛车应符合牛顿定律（这被编码在一个单轮模型中）。鉴于这一点，我们考虑以下问题——给定一个模型M和一个状态转移数据集，我们希望在距离M的范围内最好地近似系统模型。我们提出一种方法来担保这种一致性。我们的第一步是将数据集浓缩成几个代表性的样例。

    Deep neural networks have emerged as the workhorse for a large section of robotics and control applications, especially as models for dynamical systems. Such data-driven models are in turn used for designing and verifying autonomous systems. They are particularly useful in modeling medical systems where data can be leveraged to individualize treatment. In safety-critical applications, it is important that the data-driven model is conformant to established knowledge from the natural sciences. Such knowledge is often available or can often be distilled into a (possibly black-box) model. For instance, an F1 racing car should conform to Newton's laws (which are encoded within a unicycle model). In this light, we consider the following problem - given a model $M$ and a state transition dataset, we wish to best approximate the system model while being a bounded distance away from $M$. We propose a method to guarantee this conformance. Our first step is to distill the dataset into a few repre
    
[^38]: NLP中的不良偏见：避免衡量危机

    Undesirable biases in NLP: Averting a crisis of measurement. (arXiv:2211.13709v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.13709](http://arxiv.org/abs/2211.13709)

    这项研究提供了一个跨学科的方法来探讨NLP模型偏见的问题，通过采用心理测量学的视角，特别关注构念效度和测量工具的信度，在衡量模型偏见的情境中如何应用。

    

    随着大型语言模型和自然语言处理（NLP）技术的快速发展和普及，预测其使用可能对人们造成伤害变得至关重要。近年来，一个受到关注的问题是这一技术在行为中显示出有害偏见。尽管已经投入了大量的努力来评估和减轻这些偏见，但我们衡量NLP模型偏见的方法存在严重问题（例如，通常不清楚它们到底衡量了什么）。在本文中，我们采用心理测量学的视角，提供了一个跨学科的方法来讨论NLP模型偏见的问题，心理测量学专注于衡量不直接可观察到的概念，如偏见。具体而言，我们将探讨心理测量学的两个核心概念，即构念效度和测量工具的信度，并讨论它们在衡量模型偏见的情境中如何应用。我们的目标是提供一个全面的视角来解决这个问题。

    As Large Language Models and Natural Language Processing (NLP) technology rapidly develops and spreads into daily life, it becomes crucial to anticipate how its use could harm people. One problem that has received a lot of attention in recent years is that this technology has displayed harmful biases in its behavior. Although a lot of effort has been invested in assessing and mitigating these biases, our methods of measuring the biases of NLP models have serious problems (e.g., it is often unclear what they actually measure). In this paper, we provide an interdisciplinary approach to discussing the issue of NLP model bias by adopting the lens of psychometrics -- a field specialized in the measurement of concepts like bias that are not directly observable. In particular, we will explore two central notions from psychometrics, the construct validity and the reliability of measurement tools, and discuss how they can be applied in the context of measuring model bias. Our goal is to provide
    
[^39]: 失败目标感知的事后经验重播

    Failed Goal Aware Hindsight Experience Replay. (arXiv:2208.14741v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2208.14741](http://arxiv.org/abs/2208.14741)

    本文提出了一种称为Failed goal Aware HER (FAHER)的新方法来增强多目标强化学习中的采样效率。该方法利用实现目标与未实现目标之间的关系，并通过聚类模型对序列进行聚类和采样。

    

    在给定环境中的多目标强化学习中，代理通过与环境的交互获得的经验来学习实现多个目标的策略。在这种情况下，一个关键挑战是使用稀疏二元奖励训练代理，由于缺乏成功的经验，这可能会很困难。为了解决这个挑战，事后经验重播（HER）从不成功的经验中生成成功的经验。然而，从均匀抽样的经验中生成成功的经验的过程可能效率低下。本文提出了一种名为Failed goal Aware HER (FAHER)的新方法来增强采样效率。该方法利用了实现目标与未实现目标之间的关系，并使用聚类模型对具有不同实现目标的序列进行聚类，并在HER的方式下对经验进行采样。

    In multi-goal reinforcement learning for a given environment, agents learn policies to achieve multiple goals by using experiences gained from interactions with the environment. One of the key challenges in this setting is training agents using sparse binary rewards, which can be difficult due to a lack of successful experiences. To address this challenge, hindsight experience replay (HER) generates successful experiences from unsuccessful experiences. However, the process of generating successful experiences from uniformly sampled ones can be inefficient. In this paper, a novel approach called Failed goal Aware HER (FAHER) is proposed to enhance the sampling efficiency. The approach exploits the property of achieved goals in relation to failed goals that are defined as the original goals not achieved. The proposed method involves clustering episodes with different achieved goals using a cluster model and subsequently sampling experiences in the manner of HER. The cluster model is gene
    
[^40]: SSIVD-Net：一种新的武器化暴力显著超级图像分类和检测技术

    SSIVD-Net: A Novel Salient Super Image Classification & Detection Technique for Weaponized Violence. (arXiv:2207.12850v6 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.12850](http://arxiv.org/abs/2207.12850)

    本文提出了一种名为SSIVD-Net的新技术，用于暴力识别任务。通过使用显著-超级图像表示减少了3D视频数据的复杂性，提高了推断、性能和可解释性。作者还提出了一种新颖的架构“Salient-Classifier”，将核方法和残差学习策略相结合。该方法在多个数据集上表现良好。

    

    在闭路电视（CCTV）监控录像中检测暴力和武器化暴力需要一个全面的方法。本文引入了“智慧城市CCTV暴力检测（SCVD）”数据集，旨在促进对监控视频中武器分布的学习。为了解决分析3D监控视频进行暴力识别任务的复杂性，我们提出了一种新技术，称为SSIVD-Net（用于暴力检测的显著-超级-图像），通过使用显著-超级图像表示减少3D视频数据复杂性、降维和信息损失，同时提高推断、性能和可解释性。考虑到未来智慧城市的可扩展性和可持续性要求，作者提出了一种新颖的架构“Salient-Classifier”，将核方法和残差学习策略相结合。我们评估了SSIVD-Net在SCVD、Hockey Fight、Moviescope以及Large-Scale Fight Detection数据集上的性能，并与现有算法进行了比较。

    Detection of violence and weaponized violence in closed-circuit television (CCTV) footage requires a comprehensive approach. In this work, we introduce the \emph{Smart-City CCTV Violence Detection (SCVD)} dataset, specifically designed to facilitate the learning of weapon distribution in surveillance videos. To tackle the complexities of analyzing 3D surveillance video for violence recognition tasks, we propose a novel technique called, \emph{SSIVD-Net} (\textbf{S}alient-\textbf{S}uper-\textbf{I}mage for \textbf{V}iolence \textbf{D}etection). Our method reduces 3D video data complexity, dimensionality, and information loss while improving inference, performance, and explainability through the use of Salient-Super-Image representations. Considering the scalability and sustainability requirements of futuristic smart cities, the authors introduce the \emph{Salient-Classifier}, a novel architecture combining a kernelized approach with a residual learning strategy. We evaluate variations of
    
[^41]: 重新思考科学发现中符号回归数据集和基准

    Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery. (arXiv:2206.10540v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10540](http://arxiv.org/abs/2206.10540)

    本文重新审视并重新创建了符号回归数据集，以评估符号回归在科学发现中的潜力和能力，并提出使用归一化编辑距离进行度量。

    

    本文重新审视符号回归（SR）的数据集和评估标准，特别关注其在科学发现中的潜力。针对现有数据集中基于费曼物理讲义的一组公式，我们重新创建了120个数据集，讨论符号回归在科学发现中的性能（SRSD）。对于这120个SRSD数据集，我们仔细审查了公式及其变量的属性，设计了合理的实值范围来采样值，以便我们的新SRSD数据集可用于评估SRSD的潜力，如SR方法是否能从这样的数据集中（重新）发现物理定律。我们还创建了另外120个包含虚拟变量的数据集，以检验SR方法是否能够仅选择必要变量。另外，我们提出使用预测方程与真实方程树之间的归一化编辑距离（NED）来解决现有SR度量存在的一个关键问题，即二元度量问题。

    This paper revisits datasets and evaluation criteria for Symbolic Regression (SR), specifically focused on its potential for scientific discovery. Focused on a set of formulas used in the existing datasets based on Feynman Lectures on Physics, we recreate 120 datasets to discuss the performance of symbolic regression for scientific discovery (SRSD). For each of the 120 SRSD datasets, we carefully review the properties of the formula and its variables to design reasonably realistic sampling ranges of values so that our new SRSD datasets can be used for evaluating the potential of SRSD such as whether or not an SR method can (re)discover physical laws from such datasets. We also create another 120 datasets that contain dummy variables to examine whether SR methods can choose necessary variables only. Besides, we propose to use normalized edit distances (NED) between a predicted equation and the true equation trees for addressing a critical issue that existing SR metrics are either binary
    
[^42]: FAIR4Cov：用于 COVID-19 检测的融合音频实例和表示

    FAIR4Cov: Fused Audio Instance and Representation for COVID-19 Detection. (arXiv:2204.10581v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2204.10581](http://arxiv.org/abs/2204.10581)

    FAIR4Cov是一种针对COVID-19检测的方法，它提出了一种融合身体声音的波形和谱图表示的关节特征向量，可以有效地检测COVID-19患者，胜过了其他方法。

    

    基于身体声音的分类技术长期以来一直被研究用于支持诊断决策，特别是在肺部疾病方面。针对 COVID-19 疫情的紧迫性，越来越多的模型被开发来基于声学输入识别 COVID-19 患者。大多数模型侧重于咳嗽，因为干咳是 COVID-19 最为人所知的症状。然而，呼吸和言语等其他身体声音也被发现与 COVID-19 相关。在这项工作中，我们提出了 FAIR4Cov，它不依赖于特定的身体声音，而是提出了一种融合身体声音的波形和谱图表示的关节特征向量。FAIR4Cov 的核心组件是一个自注意融合单元，它的训练目的是建立多个身体声音和音频表示的关系并将其集成到一个紧凑的特征向量中。我们在两个公共数据集上设置了实验，并在不同场景下评估了我们的提议方法，包括跨数据集评估和早期检测设置。实验结果表明，FAIR4Cov 胜过了现有方法，并展示了利用各种身体声音检测 COVID-19 患者的能力。

    Audio-based classification techniques on body sounds have long been studied to support diagnostic decisions, particularly in pulmonary diseases. In response to the urgency of the COVID-19 pandemic, a growing number of models are developed to identify COVID-19 patients based on acoustic input. Most models focus on cough because the dry cough is the best-known symptom of COVID-19. However, other body sounds, such as breath and speech, have also been revealed to correlate with COVID-19 as well. In this work, rather than relying on a specific body sound, we propose Fused Audio Instance and Representation for COVID-19 Detection (FAIR4Cov). It relies on constructing a joint feature vector obtained from a plurality of body sounds in waveform and spectrogram representation. The core component of FAIR4Cov is a self-attention fusion unit that is trained to establish the relation of multiple body sounds and audio representations and integrate it into a compact feature vector. We set up our experi
    

