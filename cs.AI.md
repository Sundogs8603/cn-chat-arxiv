# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation.](http://arxiv.org/abs/2307.06333) | 提出了一种交互式框架，通过从用户那里直接获取反馈来识别个性化的无关紧要的概念，从而进行数据增强并获得适应个性化用户目标的政策。 |
| [^2] | [Budgeting Counterfactual for Offline RL.](http://arxiv.org/abs/2307.06328) | 离线强化学习中，通过动态规划的方法限制超出分布动作的数量。 |
| [^3] | [Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution.](http://arxiv.org/abs/2307.06304) | NaViT是一个视觉Transformer模型，通过序列打包的方式处理任意分辨率和纵横比的输入图像，提高了训练效率和模型在标准任务上的性能，并在鲁棒性和公平性测试中取得了显著的改进。 |
| [^4] | [Instruction Mining: High-Quality Instruction Data Selection for Large Language Models.](http://arxiv.org/abs/2307.06290) | 本文提出了InstructMining，一种用于选择高质量指令数据的线性规则，以增强大语言模型的解释和响应指令能力。通过特定的自然语言指标建模，研究结果表明，即使只有少量高质量的指令跟随数据，语言模型也可以进行良好的微调。 |
| [^5] | [DSSE: a drone swarm search environment.](http://arxiv.org/abs/2307.06240) | DSSE是一个无人机群集搜索环境，用于研究需要动态概率作为输入的强化学习算法。 |
| [^6] | [Testing different Log Bases For Vector Model Weighting Technique.](http://arxiv.org/abs/2307.06213) | 本论文测试了在向量模型加权技术中使用不同对数底数的效果，以突出在不同加权数值下了解系统性能的重要性。 |
| [^7] | [Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems.](http://arxiv.org/abs/2307.06187) | 本文提出了将大型语言模型（LLMs）集成到多智能体系统中的方法。通过自适应系统和有效通信之间的相互作用，我们可以实现对复杂环境变化的监测、分析、计划和执行系统自适应的支持。 |
| [^8] | [Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning.](http://arxiv.org/abs/2307.06166) | 本研究探索了使用Vision-Language Models（VLMs）进行时间和位置推理的能力，并提出了一个两阶段的识别和推理探测任务来研究VLMs的推理能力。实验发现，尽管VLMs能够有效地识别时间和位置相关特征，但在推理方面仍存在改进的空间。 |
| [^9] | [Deep Generative Models for Physiological Signals: A Systematic Literature Review.](http://arxiv.org/abs/2307.06162) | 本文是对深度生成模型在生理信号研究领域的系统综述，总结了最新最先进的研究进展，有助于了解这些模型在生理信号中的应用和挑战，同时提供了评估和基准测试的指导。 |
| [^10] | [Reflective Hybrid Intelligence for Meaningful Human Control in Decision-Support Systems.](http://arxiv.org/abs/2307.06159) | 本章介绍了反思性人工智能系统，旨在实现对人工智能系统的有意义的人类控制。通过整合心理学和哲学知识、形式推理方法和机器学习方法，我们提出了一个框架，以创建能够响应人类价值观和社会规范的人工智能系统。我们还提出了设计和开发能力自反思的人工智能系统的研究方法。最后，我们认为自反思的人工智能系统可以通过提供易于理解的信息，增强人类道德推理能力，从而增加有意义的人类控制力量。 |
| [^11] | [Maneuver Decision-Making Through Automatic Curriculum Reinforcement Learning Without Handcrafted Reward functions.](http://arxiv.org/abs/2307.06152) | 本文提出了一种无需手工设计奖励函数的自动课程强化学习方法，使代理能够从零开始学习空战中的有效决策。通过逐渐学习一系列从简单到困难的子任务，代理能够在各种状态下做出有效的机动决策。 |
| [^12] | [SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning.](http://arxiv.org/abs/2307.06135) | SayPlan是一种利用3D场景图为基础的可扩展的大规模任务规划方法，通过利用3D场景图的层次结构、集成经典路径规划器以及引入迭代的重规划流程，实现了在庞大复杂环境中进行规划的能力。 |
| [^13] | [Guided Bottom-Up Interactive Constraint Acquisition.](http://arxiv.org/abs/2307.06126) | 该论文提出了两种改进约束获取系统效率的新方法，一种是自下而上的方法，名为GrowAcq，可以减少用户的等待时间和查询次数，提高交互式约束获取的效率。 |
| [^14] | [Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation.](http://arxiv.org/abs/2307.06125) | 这项工作提出了一个层次化的强化学习方法，用于解决在未知环境中需要同时进行操控和导航的交互式多目标搜索任务。实验证明，该方法可以在新环境中进行零样本迁移，并对未见过的子任务具有鲁棒性。 |
| [^15] | [TreeFormer: a Semi-Supervised Transformer-based Framework for Tree Counting from a Single High Resolution Image.](http://arxiv.org/abs/2307.06118) | TreeFormer是一种基于半监督Transformer的框架，用于从单个高分辨率图像中进行树木计数。它利用金字塔树表示模块和注意力机制来提取特征，并通过回归器进行树木密度估算。此外，采用局部树密度一致性和排名损失来利用无标签图像进行训练和优化。 |
| [^16] | [Quantitative CLTs in Deep Neural Networks.](http://arxiv.org/abs/2307.06092) | 本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。 |
| [^17] | [VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View.](http://arxiv.org/abs/2307.06082) | VELMA是一个口头化的LLM智能体，利用人类写作的导航指令中的地标并结合CLIP来进行视觉环境的理解，以实现在街景中的视觉和语言导航。 |
| [^18] | [Visualization for Multivariate Gaussian Anomaly Detection in Images.](http://arxiv.org/abs/2307.06052) | 本文介绍了一种用于图像中的异常检测的简化多元高斯方法，通过应用白化转换可生成可视化的热图来解释特征，并在MVTec-AD数据集上验证了该方法的有效性。 |
| [^19] | [An OOD Multi-Task Perspective for Link Prediction with New Relation Types and Nodes.](http://arxiv.org/abs/2307.06046) | 本论文提出了一种在OOD测试多图中进行链接预测的方法，通过使用双可交换性概念，该方法能够处理具有不同预测模式且可能冲突的关系类型集合的属性多图。 |
| [^20] | [AI-Generated Imagery: A New Era for the `Readymade'.](http://arxiv.org/abs/2307.06033) | 这项研究探讨了AI生成的图像如何被视为艺术，并提出了一种哲学框架，认为一些AI生成的图像可以被视为 "现成品" 艺术。 |
| [^21] | [An Effective and Efficient Time-aware Entity Alignment Framework via Two-aspect Three-view Label Propagation.](http://arxiv.org/abs/2307.06013) | 本文提出了一个高效且有效的非神经网络时间感知实体对齐框架，通过两方面三视图标签传播、稀疏相似度、Sinkhorn算子和时态迭代学习来提高对齐性能，并减少模型的时间开销。 |
| [^22] | [Transformers in Reinforcement Learning: A Survey.](http://arxiv.org/abs/2307.05979) | 本综述介绍了基于Transformer的强化学习的应用。Transformer在解决强化学习中的挑战上具有潜在优势，包括不稳定训练、信用分配、缺乏可解释性和部分可观测性等方面。研究还探讨了其在表示学习、状态转移和奖励函数建模以及策略优化等方面的应用。此外，还有一些最新的研究旨在提高Transformer的可解释性和效率。 |
| [^23] | [Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models.](http://arxiv.org/abs/2307.05977) | 本文介绍了一种名为SDD的方法，用于实现互联网规模的文本到图像扩散模型的安全自蒸馏。该方法通过引导噪声估计与无条件模型匹配，在生成的图像中消除有害内容的比例更大，同时保持整体图像质量，并且允许一次移除多个概念。 |
| [^24] | [VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models.](http://arxiv.org/abs/2307.05973) | VoxPoser提出了一种新方法，通过组合3D价值映射和语言模型，实现了机器人在多种操作任务下根据自由形式的指令和对象合成机器人轨迹的能力。 |
| [^25] | [Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations.](http://arxiv.org/abs/2307.05959) | 使用无标签的人类视频演示增强了眼手协同的视觉运动策略的泛化能力，而无需昂贵的专家演示数据或领域适应方法。 |
| [^26] | [Automatically Reconciling the Trade-off between Prediction Accuracy and Earliness in Prescriptive Business Process Monitoring.](http://arxiv.org/abs/2307.05939) | 该研究探讨了在规范性业务流程监控中如何自动协调预测的准确性和提前性之间的权衡问题。研究发现之前的预测准确性较低，但较早的调整对于预防不良流程结果至关重要。 |
| [^27] | [BiRP: Learning Robot Generalized Bimanual Coordination using Relative Parameterization Method on Human Demonstration.](http://arxiv.org/abs/2307.05933) | 本文提出了一种相对参数化方法，通过从人体演示中学习双臂协调，将双臂任务分为领导者-追随者协调和协同协调两种类型，并将协调表示为高斯混合模型以描述协调的重要性变化，从而实现了对新任务参数的广义协调。 |
| [^28] | [A New Dataset and Comparative Study for Aphid Cluster Detection.](http://arxiv.org/abs/2307.05929) | 本研究提出了一个新的数据集，并进行了用于蚜虫簇检测的比较研究。通过检测蚜虫簇，我们可以准确估计感染水平，从而实现精确的局部杀虫剂应用。 |
| [^29] | [Reading Radiology Imaging Like The Radiologist.](http://arxiv.org/abs/2307.05921) | 提出一种以疾病为导向的方法，解决自动放射学报告生成中的细微差异关注、数据偏差和长文本生成的挑战。 |
| [^30] | [Close-up View synthesis by Interpolating Optical Flow.](http://arxiv.org/abs/2307.05913) | 本文提出了一种利用光流插值的方法实现近距离视角合成，并通过巧妙应用光流值，克服了谷歌街景系统中的视觉失真和图像模糊问题。 |
| [^31] | [Stability Guarantees for Feature Attributions with Multiplicative Smoothing.](http://arxiv.org/abs/2307.05902) | 本文提出了一种基于乘法平滑的特征归因方法，通过证明模型的Lipschitz性质，保证了其稳定性，并在视觉和语言模型上进行了评估，显示了非平凡的稳定性保证。 |
| [^32] | [PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks.](http://arxiv.org/abs/2307.05891) | 该论文受到PID控制器的启发，提出了两种用于编码历史记录的架构，以平衡提取相关信息的灵活性与对环境变化的鲁棒性。 |
| [^33] | [Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes.](http://arxiv.org/abs/2307.05862) | 部署机器学习存在系统性故障, 即使单个模型在总体上的改善也不能解决这个问题 |
| [^34] | [FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems.](http://arxiv.org/abs/2307.05857) | 这篇论文提出了一种名为FAIRO的算法，用于在人机交互环境中实现公平的顺序决策。该算法将人的行为可变性和偏好变化考虑在内，通过利用选项强化学习框架将复杂的公平任务分解为自适应子任务。 |
| [^35] | [Influential Simplices Mining via Simplicial Convolutional Network.](http://arxiv.org/abs/2307.05841) | 本文提出了一种名为ISMnet的高阶图学习模型，通过利用分层二部图和高阶分层拉普拉斯矩阵，来识别简单复合体中的重要简单形体。 |
| [^36] | [Scaling Distributed Multi-task Reinforcement Learning with Experience Sharing.](http://arxiv.org/abs/2307.05834) | 本文研究了分布式多任务强化学习中的经验共享问题，提出了一种算法DistMT-LSVI，并通过理论和实证研究表明，使用DistMT-LSVI的单个代理可以实现所有任务的ε-最优策略。 |
| [^37] | [Bag of Views: An Appearance-based Approach to Next-Best-View Planning for 3D Reconstruction.](http://arxiv.org/abs/2307.05832) | 这篇论文提出了一种基于外观的方法，通过视图袋（Bag-of-Views）模型来对捕获的视图进行离线数据集细化和在线下一个最佳视图（NBV）规划应用分配效用，以实现3D重建的任务。同时，还开发了视图规划工具箱（VPT）。 |
| [^38] | [Memorization Through the Lens of Curvature of Loss Function Around Samples.](http://arxiv.org/abs/2307.05831) | 本研究通过对损失函数曲率进行分析，研究了神经网络在不同样本上的泛化与记忆化特性。我们发现高曲率的样本通常是具有标签错误或冲突的长尾样本，并在CIFAR100数据集上发现了一种新的失败模型。通过对部分样本进行随机标签错误，我们展示了曲率排序可以有效识别出这些样本。 |
| [^39] | [Relational Extraction on Wikipedia Tables using Convolutional and Memory Networks.](http://arxiv.org/abs/2307.05827) | 使用卷积和记忆网络，在维基百科的表格数据中进行关系抽取。该模型在关系抽取任务中表现出色，并且经过全面的分析和研究，展示了各个模型组件的贡献。 |
| [^40] | [Neuro-Inspired Efficient Map Building via Fragmentation and Recall.](http://arxiv.org/abs/2307.05793) | 本文提出了一种神经启发的地图构建方法，通过分割和回溯来解决大型环境下的探索问题，并基于意外性的空间聚类设置探索子目标。 |
| [^41] | [Merging multiple input descriptors and supervisors in a deep neural network for tractogram filtering.](http://arxiv.org/abs/2307.05786) | 本文提出了一种在深度神经网络中合并多个输入描述符和监督器的方法，用于过滤道路图数据。作者通过使用四种不同的道路图过滤策略，并将它们的输出进行组合，实现了对道路线的分类。作者还发现，在此分类任务中，道路线的坐标是最重要的信息。 |
| [^42] | [EgoAdapt: A multi-stream evaluation study of adaptation to real-world egocentric user video.](http://arxiv.org/abs/2307.05784) | EgoAdapt是一个用于真实世界自我中心动作识别的基准，通过引入自适应范例并解决流数据中的分布转移挑战，可以使模型根据用户的实际需求进行自适应。 |
| [^43] | [Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting.](http://arxiv.org/abs/2307.05766) | 本文提出了Rad-ReStruct，一个用于评估和比较不同方法的新型基准数据集，以X光图像的结构化报告形式提供了细粒度、按层次排序的注释。我们提出了一种新方法hi-VQA，将结构化报告任务建模为分层视觉问答(VQA)，并考虑先前提问和回答的上下文来填充结构化放射学报告。实验证明hi-VQA取得了与最先进方法相竞争的性能。 |
| [^44] | [Integrating Curricula with Replays: Its Effects on Continual Learning.](http://arxiv.org/abs/2307.05747) | 将课程与回放方法相结合可以提高持续学习的效果，通过调整回放实例与训练数据的交替频率、回放实例的顺序以及选择进入回放缓冲区的策略实现。 |
| [^45] | [Towards A Scalable Solution for Improving Multi-Group Fairness in Compositional Classification.](http://arxiv.org/abs/2307.05728) | 本论文提出面向组合分类中多组公平性改善的可扩展解决方案。通过引入任务过条件化和组间交替的技术，在多组多标签设定下实现了恒定比例缩放，并在学术和实际环境中的实验中证明了其有效性。 |
| [^46] | [An Open-Source Knowledge Graph Ecosystem for the Life Sciences.](http://arxiv.org/abs/2307.05727) | PheKnowLator是一个开源的知识图谱生态系统，用于自动化构建可定制的FAIR本体化基础的知识图谱，以解决生命科学中的整合挑战。 |
| [^47] | [Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations.](http://arxiv.org/abs/2307.05722) | 本论文探索了大规模语言模型在在线职位推荐中对图数据的理解能力，并提出了新的框架来分析行为图，发现其中的潜在模式和关系。 |
| [^48] | [A Causal Ordering Prior for Unsupervised Representation Learning.](http://arxiv.org/abs/2307.05704) | 该论文提出了一种无监督表示学习的方法，通过考虑具有潜在加性噪声模型的数据生成过程，以及基于潜在分布的海森矩阵的损失函数，鼓励潜在空间遵循因果排序。 |
| [^49] | [A Personalized Reinforcement Learning Summarization Service for Learning Structure from Unstructured Data.](http://arxiv.org/abs/2307.05696) | 该论文提出了一种个性化强化学习总结服务，通过使用层级个性化基于概念的总结方法，在文本数据呈指数级增长的背景下，帮助用户提取有意义的见解。 |
| [^50] | [Objaverse-XL: A Universe of 10M+ 3D Objects.](http://arxiv.org/abs/2307.05663) | Objaverse-XL是一个包含1千万+ 3D对象的无限世界的数据集，它解决了3D视觉任务中获取高质量数据的挑战，并通过提供大规模和多样性改进了3D视觉能力。 |
| [^51] | [Multiobjective Hydropower Reservoir Operation Optimization with Transformer-Based Deep Reinforcement Learning.](http://arxiv.org/abs/2307.05643) | 提出了一种使用基于Transformer的深度强化学习方法来优化多目标水电站水库调度问题。实验结果表明，该方法相较于最先进的方法具有更好的运行效果。 |
| [^52] | [Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks.](http://arxiv.org/abs/2307.05639) | 本论文提出了一种修改的径向基函数神经网络模型，通过学习精度矩阵，从训练完成后的模型中提取有用信息，包括活跃子空间的方向和输入变量重要性的排序。 |
| [^53] | [A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions.](http://arxiv.org/abs/2307.05638) | 本综述调查了深度迁移学习在工业时间序列异常检测中的使用。深度迁移学习通过利用相关任务的知识和考虑数据分布的变化，解决了仅有少量或没有附加标记数据情况下的新任务。 |
| [^54] | [Belief Revision from Probability.](http://arxiv.org/abs/2307.05632) | 该论文通过使用相对问题、概率的信念模型，探讨了信念动态的影响。研究发现，该模型所验证的原则比传统的信念修正理论要弱，但比与高主观概率等同的Lockean信念理论要强。同时，论文还提出了一些适用于特定类别模型的自然原则，并与其他概率信念模型进行了比较。 |
| [^55] | [Causal Kripke Models.](http://arxiv.org/abs/2307.05631) | 本研究扩展了Halpern和Pearl的实际因果模型，引入了一个带有模态运算符的因果逻辑，可以推理涉及多种可能性、时间性、知识和不确定性的因果关系。 |
| [^56] | [Characterization of AGM Belief Contraction in Terms of Conditionals.](http://arxiv.org/abs/2307.05629) | 该论文通过使用Kripke信念关系和Stalnaker-Lewis选择函数的框架，对AGM信念收缩进行了语义表征，其中关键点是初始信念集合收缩后的成员满足在实际状态下的相信和条件判断关系。 |
| [^57] | [CILF:Causality Inspired Learning Framework for Out-of-Distribution Vehicle Trajectory Prediction.](http://arxiv.org/abs/2307.05624) | 提出了一种因果启发学习框架（CILF），通过明确定义数据的潜在因果结构，并利用因果特征进行预测来解决轨迹预测中超出分布数据的问题。 |
| [^58] | [A DeepLearning Framework for Dynamic Estimation of Origin-Destination Sequence.](http://arxiv.org/abs/2307.05623) | 本文提出了一个综合方法，使用深度学习方法推断OD序列的结构，并使用结构约束指导传统的数值优化，解决了交通领域中静态和动态OD矩阵估计中的欠定和滞后挑战。 |
| [^59] | [Impact of Feature Encoding on Malware Classification Explainability.](http://arxiv.org/abs/2307.05614) | 本文研究了特征编码技术对可解释的人工智能算法的影响。使用恶意软件分类数据集，比较了标签编码和独热编码的性能。发现虽然独热编码会带来轻微性能损失，但可以提供更详细的解释，帮助更全面地理解。使用独热编码还可以减小解释文件大小，缩短人工分析时间。这些结果强调了在XAI研究中重视特征编码技术的重要性，并提出了进一步探索的可能性。 |
| [^60] | [Substance or Style: What Does Your Image Embedding Know?.](http://arxiv.org/abs/2307.05610) | 本文研究了图像嵌入中的非语义信息，设计了一个系统的转换预测任务，并发现六个嵌入可以识别出多个转换。这对于训练算法和基础模型的应用具有重要意义。 |
| [^61] | [Active Learning for Video Classification with Frame Level Queries.](http://arxiv.org/abs/2307.05587) | 本论文提出了一个新颖的主动学习框架，用于视频分类中的标注减负。这个框架通过自动识别最具信息量的样本来减少人工标注工作量和训练时间。 |
| [^62] | [Code Generation for Machine Learning using Model-Driven Engineering and SysML.](http://arxiv.org/abs/2307.05584) | 本研究通过将模型转换与SysML集成，致力于实现数据驱动工程的代码生成，以提高工程系统的可扩展性和可维护性。 |
| [^63] | [DBFed: Debiasing Federated Learning Framework based on Domain-Independent.](http://arxiv.org/abs/2307.05582) | DBFed是一个基于领域无关性的去偏差联邦学习框架，解决了在联邦学习中由数据质量差异引起的公平性问题。 |
| [^64] | [Hate Speech Detection via Dual Contrastive Learning.](http://arxiv.org/abs/2307.05578) | 该论文提出了一种用于恶意言论检测的新颖框架，通过应对恶意言论中的复杂语义信息和恶意言辞的干扰以及恶意言论和非恶意言论的不平衡分布等挑战，实现了跨度级信息的捕捉。 |
| [^65] | [Some Preliminary Steps Towards Metaverse Logic.](http://arxiv.org/abs/2307.05574) | 本研究探寻了一种能够处理现实和虚构应用领域中出现情况的强大逻辑，采用非常规扩展，试图勾勒出一种最小复合逻辑策略，并通过ChatGPT AI代理传递理论概念背后的直觉。 |
| [^66] | [RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset.](http://arxiv.org/abs/2307.05563) | RidgeBase是一个跨传感器多指非接触指纹数据集，旨在促进不同匹配场景下非接触指纹匹配的研究。 |
| [^67] | [Review of feedback in Automated Essay Scoring.](http://arxiv.org/abs/2307.05553) | 这篇论文综述了自动化论文评分中的反馈研究，包括不同类型的反馈和论文特征，并回顾了提供反馈的最新案例研究。 |
| [^68] | [Physical Color Calibration of Digital Pathology Scanners for Robust Artificial Intelligence Assisted Cancer Diagnosis.](http://arxiv.org/abs/2307.05519) | 本研究通过物理颜色校准解决了数字病理学扫描仪生产中的技术不一致性问题，提高了人工智能辅助癌症诊断的稳定性和准确性。 |
| [^69] | [Procedurally generating rules to adapt difficulty for narrative puzzle games.](http://arxiv.org/abs/2307.05518) | 本文提出了一种自动生成规则并将其传达给玩家调整难度的方法，通过使用遗传算法和难度测量来找到目标解集的数量，并使用大型语言模型在叙事背景下传达规则。该方法在测试中成功地找到了逼近任何给定目标难度的规则。该方法还结合了大型语言模型，创建了一个有趣的叙事益智游戏，玩家必须举办一次晚宴为无法和谐共处的动物们。未来的工作将致力于改进评估，专门针对儿童文学进行语言模型训练，并收集玩家的多模态数据来指导难度的动态调整。 |
| [^70] | [UX Heuristics and Checklist for Deep Learning powered Mobile Applications with Image Classification.](http://arxiv.org/abs/2307.05513) | 这项研究提出了一套适用于深度学习移动应用图像分类的AIX启发和检查清单，以及相应的在线课程和Web工具，可用于指导应用界面设计和启发式评估，帮助开发易于理解的图像分类应用。 |
| [^71] | [The Effects of Interaction Conflicts, Levels of Automation, and Frequency of Automation on Human Automation Trust and Acceptance.](http://arxiv.org/abs/2307.05512) | 交互冲突、自动化水平和自动化频率对人类在智能家居环境中对自动化的信任和接受度产生影响，自动化水平和频率越高，用户对智能环境的信任度越高，但在出现自动化故障和交互冲突时，用户对自动化智能环境的接受度降低。 |
| [^72] | [Human in the AI loop via xAI and Active Learning for Visual Inspection.](http://arxiv.org/abs/2307.05508) | 通过xAI和主动学习的人在AI循环中进行视觉检查的论文探讨了工业 5.0 中人机协作的新机会，并分享了关于视觉检查中的人工智能、人类数字孪生和网络安全的最新研究成果。 |
| [^73] | [Towards Environmentally Equitable AI via Geographical Load Balancing.](http://arxiv.org/abs/2307.05494) | 本研究通过地理负载平衡的方式解决了人工智能在不同地区对环境的不平等影响，从而推进环境公平的人工智能发展。 |
| [^74] | [Cases of EFL Secondary Students' Prompt Engineering Pathways to Complete a Writing Task with ChatGPT.](http://arxiv.org/abs/2307.05493) | 本文研究了中学英语作为外语学生与ChatGPT合作完成写作任务的案例，并对提示的质量和数量进行了分析。结果发现，非技术用户在为ChatGPT编写适当的提示上遇到了困难，需要提供更好的支持和指导。 |
| [^75] | [GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study.](http://arxiv.org/abs/2307.05492) | 通过比较GPT和人类审稿人生成的评论，我们发现GPT在同行评审中具有一定的帮助性，为解决同行评审资源限制提供了新途径。 |
| [^76] | [VampNet: Music Generation via Masked Acoustic Token Modeling.](http://arxiv.org/abs/2307.04686) | 本论文介绍了一种名为VampNet的音乐生成方法，通过掩码声学令牌建模实现生成、压缩、修复和变异。VampNet通过灵活的提示能力，能够保持音乐的风格、流派、乐器和其他高层次方面，是一个强大的音乐共创工具。 |
| [^77] | [Weakly-supervised positional contrastive learning: application to cirrhosis classification.](http://arxiv.org/abs/2307.04617) | 本研究提出了一种弱监督定位对比学习策略，在医学应用中使用大量弱标签图像进行肝硬化预测。这种策略将每个2D切片的空间上下文和弱标签整合起来，通过使用通用基于核的损失函数实现高效学习。 |
| [^78] | [Natural Language Instructions for Intuitive Human Interaction with Robotic Assistants in Field Construction Work.](http://arxiv.org/abs/2307.04195) | 这篇论文提出了在建筑施工领域中，利用自然语言指令实现人机直观交流的框架，以解决施工现场复杂性和人力短缺等问题。 |
| [^79] | [Latent Graph Attention for Enhanced Spatial Context.](http://arxiv.org/abs/2307.04149) | 本文提出了一种称为潜在图注意力（LGA）的模块化框架，用于增强图像中的全局上下文。LGA在计算上简洁且稳定，能够在小规模体系结构中实现接近大规模体系结构的性能，使得轻量级体系结构在边缘设备上更加实用。 |
| [^80] | [Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition.](http://arxiv.org/abs/2307.04132) | 本研究提出了一种新的框架，通过对视频片段中提取的物体行为进行推理来识别副词类型。实验结果表明，我们的方法在效果上超越了之前的最先进方法。 |
| [^81] | [Frontier AI Regulation: Managing Emerging Risks to Public Safety.](http://arxiv.org/abs/2307.03718) | 对于边缘人工智能模型的监管需要标准制定、注册报告和安全合规机制。 |
| [^82] | [A Survey on Evaluation of Large Language Models.](http://arxiv.org/abs/2307.03109) | 本文综述了大型语言模型（LLMs）的评估方法，关注三个关键维度：评估什么、在哪里评估以及如何评估。评估任务包括自然语言处理、推理、医学应用、伦理学、教育、自然和社会科学、代理应用等多个领域。本文为社会层面对LLMs潜在风险的理解提供了重要参考。 |
| [^83] | [MOPO-LSI: A User Guide.](http://arxiv.org/abs/2307.01719) | MOPO-LSI是一款开源的多目标投资组合优化库，为可持续投资提供用户指南，并介绍了版本1.0的问题设置、工作流程和超参数。 |
| [^84] | [ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting.](http://arxiv.org/abs/2307.01227) | ESGCN是一种用于交通流量预测的边缘压缩注意图卷积网络，通过建模时空动态和引入边缘特征和边缘注意机制来提高预测的准确性。 |
| [^85] | [PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation.](http://arxiv.org/abs/2307.00470) | PatternGPT是一种基于模式驱动的大型语言模型文本生成框架，通过利用大型语言模型的提取能力生成多样化的模式，并使用联邦学习的思想实现模式共享，最终通过搜索高质量模式指导生成模型。该框架具有生成多样化模式、保护数据隐私、结合外部知识等优势。 |
| [^86] | [Reduce Computational Complexity for Convolutional Layers by Skipping Zeros.](http://arxiv.org/abs/2306.15951) | 本文提出了C-K-S算法，通过修剪滤波器和转换稀疏张量为稠密张量的方式，跳过卷积层中的0元素，从而降低了计算复杂度。实验证明，C-K-S相对于PyTorch具有优势。 |
| [^87] | [REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction.](http://arxiv.org/abs/2306.15724) | 提出了REFLECT框架，可以将机器人多感官数据转化为分层总结，并使用大型语言模型进行失败解释。该框架能够生成有益的失败解释，帮助机器人完成任务。 |
| [^88] | [An Overview of Catastrophic AI Risks.](http://arxiv.org/abs/2306.12001) | 本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。 |
| [^89] | [MARBLE: Music Audio Representation Benchmark for Universal Evaluation.](http://arxiv.org/abs/2306.10548) | 本论文介绍了MARBLE，一个音乐音频表征通用评估基准，它为音乐理解领域的研究和发展提供了一个全面和可持续性的基础，并提供各种音乐信息检索（MIR）任务的基准。 |
| [^90] | [Deep Generative Models for Decision-Making and Control.](http://arxiv.org/abs/2306.08810) | 本论文研究了基于深度模型的强化学习方法在决策和控制问题中的缺点，并提出了解决方法。同时，将当代生成建模工具中的推理技术重新解释为强化学习问题的规划策略。 |
| [^91] | [Employing Crowdsourcing for Enriching a Music Knowledge Base in Higher Education.](http://arxiv.org/abs/2306.07310) | 本研究探讨了一项使用众包技术的计算机科学高等教育作业，鼓励学生丰富音乐元数据，并创建了可供机器学习模型用于音乐标记的开放性注释数据集。 |
| [^92] | [Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.](http://arxiv.org/abs/2306.05685) | 研究使用强大的LLM作为评判员，通过引入两个基准测试来确认LLM评判员和人类偏好之间的一致性，并可调整聊天助手的模型架构和微调方法来提高其性能。 |
| [^93] | [Your Value Function is a Control Barrier Function: Verification of Learned Policies using Control Theory.](http://arxiv.org/abs/2306.04026) | 本研究将控制理论中的验证方法应用于强化学习中的价值函数，提出了新的度量方法以验证安全控制任务中的价值函数，并代表了通用、可伸缩和可验证的控制系统设计框架的第一步。 |
| [^94] | [Description Logics with Abstraction and Refinement.](http://arxiv.org/abs/2306.03717) | 本文提出了一种扩展的描述逻辑（DLs）以支持多个抽象层次的知识表示，并证明了推理在该逻辑中是可判定的。 |
| [^95] | [Identifying Subgroups of ICU Patients Using End-to-End Multivariate Time-Series Clustering Algorithm Based on Real-World Vital Signs Data.](http://arxiv.org/abs/2306.02121) | 本研究使用真实世界生命体征数据开发了一种端到端多元时间序列聚类算法，并发现不同子群之间存在着不同的ICU和医院死亡率风险。 |
| [^96] | [Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models.](http://arxiv.org/abs/2305.18703) | 本文综述了大型语言模型的领域专门化，包括动机、挑战、方法论和评估指标。此外，还提供了一个特定领域任务和数据集的分类法，对现有的领域自适应和定制技术进行了详细比较，并广泛讨论了这一领域中的未解决问题和未来的发展方向。 |
| [^97] | [Deep Reinforcement Learning-based Multi-objective Path Planning on the Off-road Terrain Environment for Ground Vehicles.](http://arxiv.org/abs/2305.13783) | 本文提出了一种基于深度强化学习的2.5D多目标路径规划方法，可以高效地找到距离和能耗达到良好权衡的路径。 |
| [^98] | [RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration.](http://arxiv.org/abs/2305.11474) | 本文提出了轻量级图像恢复的互惠式注意力混合Transformer（RAMiT）。通过使用双向注意力以及一种新的数据增强类型——强度掩码，有效地提高了恢复效果，同时大大减少了参数数量。 |
| [^99] | [ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification.](http://arxiv.org/abs/2305.04003) | 本文介绍了一种名为ANTONIO的Python库，它基于抽象解释方法提供了一种实用的方法和启发式规则，以便为自然语言处理（NLP）数据集和模型生成已知验证方法的基准。因为其普遍适用性，这项工作将为将NLP验证问题纳入神经网络验证比赛开辟新的可能性，并在NLP问题中普及这一方向。 |
| [^100] | [Uncertain Machine Ethical Decisions Using Hypothetical Retrospection.](http://arxiv.org/abs/2305.01424) | 本文提出了一种新的机器伦理推理方法，使用假想回顾论证程序，允许使用各种哲学理论进行伦理推理，可以考虑概率和不确定性，应用于一个自主图书馆系统用例中进行测试。 |
| [^101] | [Meta-multigraph Search: Rethinking Meta-structure on Heterogeneous Information Networks.](http://arxiv.org/abs/2304.11574) | 本文提出了一种称为元多重图的新概念来代替元图和元路径用于处理异构信息网络中的信息聚合，同时提出了稳定的可微分搜索方法来优化元多重图以适应不同的任务和HIN，此外，还引入了复杂到简洁的C2C元多重图来降低冗余信息传播的影响。 |
| [^102] | [CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments.](http://arxiv.org/abs/2304.06848) | 本文提出了一种新的因果关系在线POMDP规划方法CAR-DESPOT，使用因果建模和推理来消除未测量混淆变量引起的错误，并在混杂环境中表现优异。 |
| [^103] | [GPT detectors are biased against non-native English writers.](http://arxiv.org/abs/2304.02819) | 该研究发现，GPT检测器对非英语母语作者存在偏见，容易将其内容错误地分类为AI生成的内容。此外，简单的提示策略可以缓解这种偏见，同时规避GPT检测器，这表明GPT检测器可能会惩罚具有受限语言表达能力的作者。 |
| [^104] | [Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference.](http://arxiv.org/abs/2302.09582) | 本研究通过操纵大型语言模型中的语言衍生的情绪概念知识表示，探讨了语言是否会因果支持情绪推断。实验结果显示，属性特定的神经元操纵导致情绪推断任务的性能下降，这与人类心理空间中不同属性的重要性有关。这些发现为支持基于语言的情绪推断机制提供了因果证据，并凸显了情绪概念知识的贡献。 |
| [^105] | [One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data.](http://arxiv.org/abs/2302.06375) | 本研究提出了一种Transformer架构，用于表示具有时间相关的异构表格数据，通过使用一组频率函数来表示数值特征，并采用唯一的损失函数进行统一训练。 |
| [^106] | [Diagrammatization: Rationalizing with diagrammatic AI explanations for abductive-deductive reasoning on hypotheses.](http://arxiv.org/abs/2302.01241) | 本文提出了一种图解化的方法，以支持可解释的人工智能，通过图解型和假设性推理，缩小可解释性差距。通过临床应用研究和建模研究，我们发现DiagramNet不仅能提供忠实的杂音形状解释，还具有较好的预测性能，而且图解型解释在临床相关的情况下更受推崇。 |
| [^107] | [Bidirectional Generation of Structure and Properties Through a Single Molecular Foundation Model.](http://arxiv.org/abs/2211.10590) | 本论文提出了一种新颖的多模态分子预训练模型，通过将结构和生化性质的模态结合，使得模型能够将分子的结构和性质之间的双向信息联系起来，并能够处理多模态和单模态的下游任务。 |
| [^108] | [What Images are More Memorable to Machines?.](http://arxiv.org/abs/2211.07625) | 本论文研究了如何测量和预测图像对机器的记忆性，并发现“复杂”图像通常对机器记忆更加深刻。通过详细分析和实验验证，我们提出了机器记忆性的概念，并为机器记忆和视觉数据的研究方向开辟了新的可能性。 |
| [^109] | [Harnessing the Power of Explanations for Incremental Training: A LIME-Based Approach.](http://arxiv.org/abs/2211.01413) | 本研究提出了一种基于LIME的方法，利用解释来改善神经网络模型性能。通过引入自定义加权损失，将真实LIME解释与模型预测的LIME解释之间的距离考虑在内，帮助模型更好地泛化。此外，该研究还提出了一种适用于实际训练场景的解决方案，以帮助模型顺序学习而不丢失先前数据分布信息。 |
| [^110] | [Contrastive Decoding: Open-ended Text Generation as Optimization.](http://arxiv.org/abs/2210.15097) | 对比解码是一种可靠的解码方法，它通过优化对比目标来生成合理且高质量的文本，与仅使用较大的语言模型进行解码相比，对比解码能够避免短而重复的文本和不连贯的文本，并适用于不同的模型规模。 |
| [^111] | [Multiple Instance Learning via Iterative Self-Paced Supervised Contrastive Learning.](http://arxiv.org/abs/2210.09452) | 本论文提出了一个新的框架，通过迭代的自适应有监督对比学习，解决了多实例学习中的类别不平衡问题，并在医学数据集上取得了良好的结果。 |
| [^112] | [Distributionally Adaptive Meta Reinforcement Learning.](http://arxiv.org/abs/2210.03104) | 本论文介绍了一种基于自适应分布鲁棒性的元强化学习算法框架，该框架能够在测试任务分布发生变化时行为适应，提供了对分布转变的改善遗憾的能力。 |
| [^113] | [Polysemanticity and Capacity in Neural Networks.](http://arxiv.org/abs/2210.01892) | 该论文通过分析特征容量来理解神经网络中的多义性现象，发现在最优的容量分配下，神经网络倾向于单义地表示重要特征，多义地表示次重要特征，并忽略最不重要的特征。多义性现象在输入具有更高的峰度或稀疏性时更为普遍，并且在某些体系结构中比其他体系结构更为普遍。此外，作者还发现了嵌入空间中的分块半正交结构，不同模型中的分块大小不同，突出了模型体系结构的影响。 |
| [^114] | [GreenKGC: A Lightweight Knowledge Graph Completion Method.](http://arxiv.org/abs/2208.09137) | GreenKGC是一种轻量级的知识图谱补全方法，通过三个模块（表示学习、特征修剪和决策学习）提取判别性的知识图谱特征，并使用分类器和负采样进行准确的预测。在低维度下，GreenKGC能够在大多数数据集上优于其他现有方法，并具有竞争性性能。 |
| [^115] | [Fair Algorithm Design: Fair and Efficacious Machine Scheduling.](http://arxiv.org/abs/2204.06438) | 本研究证明，在允许微小偏见的情况下，可以设计出既几乎完全公平又具有常数倍效益比率的机器调度算法。 |
| [^116] | [Fundamental Limits for Sensor-Based Robot Control.](http://arxiv.org/abs/2202.00129) | 该论文提出了一个新的方法，利用信息论和动态规划技术，为基于传感器的机器人控制建立了性能的基本限制。通过计算任务相关信息的数量，可以获得一步决策任务的最高可实现预期报酬的上界，并通过动态规划将这个上界扩展到多步问题。实验证明了该方法在三个示例上的有效性，包括部分可观察性决策过程、自由落体物体的捕捉和使用深度传感器的避障问题。 |
| [^117] | [Fast Rates for the Regret of Offline Reinforcement Learning.](http://arxiv.org/abs/2102.00479) | 本文研究了离线数据对强化学习的遗憾，提出了精细的收敛速率分析，揭示了离线强化学习收敛速度较快的现象，并通过指数形式的加速机制加快了收敛速度。 |
| [^118] | [B-HAR: an open-source baseline framework for in depth study of human activity recognition datasets and workflows.](http://arxiv.org/abs/2101.10870) | B-HAR是一个开源的基准框架，用于深入研究人体活动识别的数据集和工作流程，解决了HAR方法缺乏标准工作流程的问题，并提供了可配置的框架来评估模式识别模型的质量。 |
| [^119] | [Deep Learning based Uncertainty Decomposition for Real-time Control.](http://arxiv.org/abs/2010.02613) | 通过使用深度学习技术，在实时控制中提出了一种新颖的方法，用于检测认知不确定性，该方法可以在未知环境中的数据驱动控制中实现安全和高效的探索。 |

# 详细

[^1]: 诊断、反馈、适应性: 用于测试时政策调整的人-机环路框架

    Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation. (arXiv:2307.06333v1 [cs.LG])

    [http://arxiv.org/abs/2307.06333](http://arxiv.org/abs/2307.06333)

    提出了一种交互式框架，通过从用户那里直接获取反馈来识别个性化的无关紧要的概念，从而进行数据增强并获得适应个性化用户目标的政策。

    

    政策常常由于分布偏移而失效——即当政策在新环境中部署时，状态和奖励发生变化。数据增强可以通过使模型对与任务无关的变化具有不变性来增加鲁棒性。然而，设计者在事先往往不知道哪些概念是无关紧要的，尤其是当不同的最终用户对任务执行方式有不同的偏好时。我们提出了一个互动框架，通过直接从用户那里获得反馈来识别个性化的无关紧要的概念。我们的核心思想是生成反事实演示，使用户能够快速确定可能与任务相关和无关的概念。然后利用无关紧要的概念的知识进行数据增强，从而获得适应于个性化用户目标的政策。我们在离散和连续控制任务上进行实验证实了我们的框架。我们的方法(1)使用户能够……

    Policies often fail due to distribution shift -- changes in the state and reward that occur when a policy is deployed in new environments. Data augmentation can increase robustness by making the model invariant to task-irrelevant changes in the agent's observation. However, designers don't know which concepts are irrelevant a priori, especially when different end users have different preferences about how the task is performed. We propose an interactive framework to leverage feedback directly from the user to identify personalized task-irrelevant concepts. Our key idea is to generate counterfactual demonstrations that allow users to quickly identify possible task-relevant and irrelevant concepts. The knowledge of task-irrelevant concepts is then used to perform data augmentation and thus obtain a policy adapted to personalized user objectives. We present experiments validating our framework on discrete and continuous control tasks with real human users. Our method (1) enables users to 
    
[^2]: Offline RL的预算反事实推理

    Budgeting Counterfactual for Offline RL. (arXiv:2307.06328v1 [cs.LG])

    [http://arxiv.org/abs/2307.06328](http://arxiv.org/abs/2307.06328)

    离线强化学习中，通过动态规划的方法限制超出分布动作的数量。

    

    离线强化学习的主要挑战在于数据有限的情况下，由于潜在动作领域内的反事实推理困境所引起：如果我们选择了不同的行动会怎么样？这些情况通常会导致指数级累积的外推误差。因此，认识到并不是所有的决策步骤对最终结果都同样重要，并在政策制定中预算反事实决策的数量以控制外推是至关重要的。与现有方法在政策或值函数上使用规则化不同，我们提出了一种方法来明确限制训练期间的超出分布动作的数量。具体而言，我们的方法利用动态规划来决定在哪里进行外推和在哪里不进行外推，并且对决策的上限不同于行为策略。它在潜在改进的潜力和外推控制之间进行平衡。

    The main challenge of offline reinforcement learning, where data is limited, arises from a sequence of counterfactual reasoning dilemmas within the realm of potential actions: What if we were to choose a different course of action? These circumstances frequently give rise to extrapolation errors, which tend to accumulate exponentially with the problem horizon. Hence, it becomes crucial to acknowledge that not all decision steps are equally important to the final outcome, and to budget the number of counterfactual decisions a policy make in order to control the extrapolation. Contrary to existing approaches that use regularization on either the policy or value function, we propose an approach to explicitly bound the amount of out-of-distribution actions during training. Specifically, our method utilizes dynamic programming to decide where to extrapolate and where not to, with an upper bound on the decisions different from behavior policy. It balances between the potential for improvemen
    
[^3]: Patch n' Pack: NaViT,一个适用于任意纵横比和分辨率的视觉Transformer

    Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution. (arXiv:2307.06304v1 [cs.CV])

    [http://arxiv.org/abs/2307.06304](http://arxiv.org/abs/2307.06304)

    NaViT是一个视觉Transformer模型，通过序列打包的方式处理任意分辨率和纵横比的输入图像，提高了训练效率和模型在标准任务上的性能，并在鲁棒性和公平性测试中取得了显著的改进。

    

    在计算机视觉模型中，将图像调整为固定分辨率后进行处理是普遍且明显次优的选择。然而，像Vision Transformer（ViT）这样的模型提供了灵活的基于序列的建模，因此可以处理不同长度的输入序列。我们利用这一点，使用称为NaViT（Native Resolution ViT）的模型，在训练过程中进行序列打包，以处理任意分辨率和纵横比的输入。除了灵活的模型使用方式外，我们还展示了在大规模监督和对比度图像-文本预训练中的训练效率提升。NaViT可以高效地应用于图像和视频分类、目标检测、语义分割等标准任务，并在鲁棒性和公平性基准测试中取得了提升的结果。在推理阶段，输入分辨率的灵活性可以用于平稳地在测试时间的成本和性能之间进行权衡。我们相信NaViT标志着一个离开了以往思维的新篇章。

    The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged. However, models such as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input sequence lengths. We take advantage of this with NaViT (Native Resolution ViT) which uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios. Alongside flexible model usage, we demonstrate improved training efficiency for large-scale supervised and contrastive image-text pretraining. NaViT can be efficiently transferred to standard tasks such as image and video classification, object detection, and semantic segmentation and leads to improved results on robustness and fairness benchmarks. At inference time, the input resolution flexibility can be used to smoothly navigate the test-time cost-performance trade-off. We believe that NaViT marks a depart
    
[^4]: 指令挖掘：大语言模型的高质量指令数据选择

    Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])

    [http://arxiv.org/abs/2307.06290](http://arxiv.org/abs/2307.06290)

    本文提出了InstructMining，一种用于选择高质量指令数据的线性规则，以增强大语言模型的解释和响应指令能力。通过特定的自然语言指标建模，研究结果表明，即使只有少量高质量的指令跟随数据，语言模型也可以进行良好的微调。

    

    大型语言模型通常经历预训练和微调两个训练阶段。尽管大规模预训练赋予模型强大的生成自然语言回应的能力，但这些预训练模型有时仍然无法理解人类指令。为了增强语言模型解释和响应指令的能力，指令微调已成为该领域的关键方法。最近的研究发现，即使只有少量高质量的指令跟随数据，大型语言模型也可以进行良好的微调。然而，选择用于微调语言模型的高质量数据集仍缺乏明确的指导方针。在本文中，我们提出了InstructMining，一个用于评估指令跟随数据质量的线性规则。我们使用具体的自然语言指标来进行InstructMining的建模。为了研究数据质量与这些指标之间的关系，我们还进行了广泛的细致研究。

    Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive fine
    
[^5]: DSSE: 无人机群集搜索环境

    DSSE: a drone swarm search environment. (arXiv:2307.06240v1 [cs.LG])

    [http://arxiv.org/abs/2307.06240](http://arxiv.org/abs/2307.06240)

    DSSE是一个无人机群集搜索环境，用于研究需要动态概率作为输入的强化学习算法。

    

    无人机群集搜索项目是一个基于PettingZoo的环境，与多智能体（或单智能体）强化学习算法配合使用。该环境中的智能体（无人机）必须找到目标（遇险人员），但不知道目标的位置，并且不会根据自身与目标的距离得到奖励。但是，智能体会接收到目标出现在地图某个单元格的概率。该项目的目标是帮助研究需要动态概率作为输入的强化学习算法。

    The Drone Swarm Search project is an environment, based on PettingZoo, that is to be used in conjunction with multi-agent (or single-agent) reinforcement learning algorithms. It is an environment in which the agents (drones), have to find the targets (shipwrecked people). The agents do not know the position of the target and do not receive rewards related to their own distance to the target(s). However, the agents receive the probabilities of the target(s) being in a certain cell of the map. The aim of this project is to aid in the study of reinforcement learning algorithms that require dynamic probabilities as inputs.
    
[^6]: 测试不同对数底数的向量模型加权技术

    Testing different Log Bases For Vector Model Weighting Technique. (arXiv:2307.06213v1 [cs.IR])

    [http://arxiv.org/abs/2307.06213](http://arxiv.org/abs/2307.06213)

    本论文测试了在向量模型加权技术中使用不同对数底数的效果，以突出在不同加权数值下了解系统性能的重要性。

    

    信息检索系统根据用户提交的查询检索相关文档。文档首先被索引，文档中的词语使用称为TFIDF的加权技术被赋予权重，TFIDF是词频（TF）和逆文档频率（IDF）的乘积。TF代表词项在文档中出现的次数。IDF衡量词项在所有文档中的普遍程度。它通过将系统中的总文档数除以包含该词项的文档数，然后计算商的对数来计算。默认情况下，我们使用以10为底的对数计算。在本文中，我们将使用从0.1到100.0的一系列对数底数来计算IDF，以测试这种加权技术。测试不同对数底数的向量模型加权技术的目的是突出在不同加权数值下了解系统性能的重要性。我们使用MED的文档。

    Information retrieval systems retrieves relevant documents based on a query submitted by the user. The documents are initially indexed and the words in the documents are assigned weights using a weighting technique called TFIDF which is the product of Term Frequency (TF) and Inverse Document Frequency (IDF). TF represents the number of occurrences of a term in a document. IDF measures whether the term is common or rare across all documents. It is computed by dividing the total number of documents in the system by the number of documents containing the term and then computing the logarithm of the quotient. By default, we use base 10 to calculate the logarithm. In this paper, we are going to test this weighting technique by using a range of log bases from 0.1 to 100.0 to calculate the IDF. Testing different log bases for vector model weighting technique is to highlight the importance of understanding the performance of the system at different weighting values. We use the documents of MED
    
[^7]: 基于自适应大型语言模型（LLM）的多智能体系统

    Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems. (arXiv:2307.06187v1 [cs.MA])

    [http://arxiv.org/abs/2307.06187](http://arxiv.org/abs/2307.06187)

    本文提出了将大型语言模型（LLMs）集成到多智能体系统中的方法。通过自适应系统和有效通信之间的相互作用，我们可以实现对复杂环境变化的监测、分析、计划和执行系统自适应的支持。

    

    在自主计算中，自适应被提出作为管理多智能体系统（MASs）复杂性的基本范式。通过添加对系统的监测和自适应支持，以实现特定关注点的目标。在涉及智能体互动的场景中，通信是关键，它通过直接、清晰的信息交流增强合作并减少协调挑战。然而，提高与MASs的交互通信的表达能力并非没有挑战。因此，自适应系统对有效通信的相互作用对于未来MAS的进步至关重要。在本文中，我们提出将基于GPT的大型语言模型（LLMs）集成到多智能体系统中。我们的方法论基于MAPE-K模型，该模型以其在响应动态环境变化中监测、分析、计划和执行系统自适应的强大支持而闻名。

    In autonomic computing, self-adaptation has been proposed as a fundamental paradigm to manage the complexity of multiagent systems (MASs). This achieved by extending a system with support to monitor and adapt itself to achieve specific concerns of interest. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, improving the expressiveness of the interaction communication with MASs is not without challenges. In this sense, the interplay between self-adaptive systems and effective communication is crucial for future MAS advancements. In this paper, we propose the integration of large language models (LLMs) such as GPT-based technologies into multiagent systems. We anchor our methodology on the MAPE-K model, which is renowned for its robust support in monitoring, analyzing, planning, and executing system adaptations in response to dynami
    
[^8]: Vision-Language Models能成为良好猜测器吗？探索VLMs用于时间和位置推理

    Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning. (arXiv:2307.06166v1 [cs.CV])

    [http://arxiv.org/abs/2307.06166](http://arxiv.org/abs/2307.06166)

    本研究探索了使用Vision-Language Models（VLMs）进行时间和位置推理的能力，并提出了一个两阶段的识别和推理探测任务来研究VLMs的推理能力。实验发现，尽管VLMs能够有效地识别时间和位置相关特征，但在推理方面仍存在改进的空间。

    

    期望Vision-Language Models（VLMs）能像人一样具备常识知识进行推理。一个例子是，人类可以根据他们的知识推断出一张图片的拍摄地点和时间。这让我们想知道，基于视觉线索，使用大规模图像-文本资源进行预训练的Vision-Language Models是否能够达到甚至超过人类在时间和位置推理方面的能力。为了回答这个问题，我们提出了一个两阶段的识别和推理探测任务，应用于鉴别性和生成性的VLMs，以发现VLMs能否识别出与时间和位置相关的特征，并进一步进行推理。为了方便这项研究，我们引入了WikiTiLo，一个包含丰富社会文化线索的精心策划的图像数据集。在广泛的实验研究中，我们发现虽然VLMs能够有效地保留视觉编码器中的相关特征，但在推理方面仍存在不完善的问题。我们将发布...

    Vision-Language Models (VLMs) are expected to be capable of reasoning with commonsense knowledge as human beings. One example is that humans can reason where and when an image is taken based on their knowledge. This makes us wonder if, based on visual cues, Vision-Language Models that are pre-trained with large-scale image-text resources can achieve and even outperform human's capability in reasoning times and location. To address this question, we propose a two-stage \recognition\space and \reasoning\space probing task, applied to discriminative and generative VLMs to uncover whether VLMs can recognize times and location-relevant features and further reason about it. To facilitate the investigation, we introduce WikiTiLo, a well-curated image dataset compromising images with rich socio-cultural cues. In the extensive experimental studies, we find that although VLMs can effectively retain relevant features in visual encoders, they still fail to make perfect reasoning. We will release o
    
[^9]: 深度生成模型对生理信号的系统文献综述

    Deep Generative Models for Physiological Signals: A Systematic Literature Review. (arXiv:2307.06162v1 [cs.LG])

    [http://arxiv.org/abs/2307.06162](http://arxiv.org/abs/2307.06162)

    本文是对深度生成模型在生理信号研究领域的系统综述，总结了最新最先进的研究进展，有助于了解这些模型在生理信号中的应用和挑战，同时提供了评估和基准测试的指导。

    

    本文对深度生成模型在生理信号，特别是心电图、脑电图、光电容抗图和肌电图领域的文献进行了系统综述。与已有的综述文章相比，本文是第一篇总结最新最先进的深度生成模型的综述。通过分析与深度生成模型相关的最新研究，以及这些模型的主要应用和挑战，本综述为对这些模型应用于生理信号的整体理解做出了贡献。此外，通过强调采用的评估协议和最常用的生理数据库，本综述有助于对深度生成模型进行评估和基准测试。

    In this paper, we present a systematic literature review on deep generative models for physiological signals, particularly electrocardiogram, electroencephalogram, photoplethysmogram and electromyogram. Compared to the existing review papers, we present the first review that summarizes the recent state-of-the-art deep generative models. By analysing the state-of-the-art research related to deep generative models along with their main applications and challenges, this review contributes to the overall understanding of these models applied to physiological signals. Additionally, by highlighting the employed evaluation protocol and the most used physiological databases, this review facilitates the assessment and benchmarking of deep generative models.
    
[^10]: 反思性混合智能对决策支持系统中有意义的人类控制

    Reflective Hybrid Intelligence for Meaningful Human Control in Decision-Support Systems. (arXiv:2307.06159v1 [cs.AI])

    [http://arxiv.org/abs/2307.06159](http://arxiv.org/abs/2307.06159)

    本章介绍了反思性人工智能系统，旨在实现对人工智能系统的有意义的人类控制。通过整合心理学和哲学知识、形式推理方法和机器学习方法，我们提出了一个框架，以创建能够响应人类价值观和社会规范的人工智能系统。我们还提出了设计和开发能力自反思的人工智能系统的研究方法。最后，我们认为自反思的人工智能系统可以通过提供易于理解的信息，增强人类道德推理能力，从而增加有意义的人类控制力量。

    

    随着人工智能系统的能力和普及程度的不断增长，社会必须共同选择在减少人类自治、危害民主和限制人权之间，还是选择与人类和社会价值相一致的人工智能，促进合作、韧性、知识和道德行为的培养。在这一章中，我们介绍了自反思人工智能系统的概念，以实现对人工智能系统的有意义的人类控制。专注于决策支持系统，我们提出了一个框架，将心理学和哲学的知识与形式推理方法和机器学习方法相结合，创建能够响应人类价值观和社会规范的人工智能系统。我们还提出了一种可能的研究方法，设计和开发能力自反思的人工智能系统。最后，我们认为自反思的人工智能系统可以导致自反思的混合系统（人类+人工智能），从而增加了有意义的人类控制，并通过提供易于理解的信息使人类道德推理能力更强大。

    With the growing capabilities and pervasiveness of AI systems, societies must collectively choose between reduced human autonomy, endangered democracies and limited human rights, and AI that is aligned to human and social values, nurturing collaboration, resilience, knowledge and ethical behaviour. In this chapter, we introduce the notion of self-reflective AI systems for meaningful human control over AI systems. Focusing on decision support systems, we propose a framework that integrates knowledge from psychology and philosophy with formal reasoning methods and machine learning approaches to create AI systems responsive to human values and social norms. We also propose a possible research approach to design and develop self-reflective capability in AI systems. Finally, we argue that self-reflective AI systems can lead to self-reflective hybrid systems (human + AI), thus increasing meaningful human control and empowering human moral reasoning by providing comprehensible information and
    
[^11]: 通过无需手工设计奖励函数的自动课程强化学习实现机动决策

    Maneuver Decision-Making Through Automatic Curriculum Reinforcement Learning Without Handcrafted Reward functions. (arXiv:2307.06152v1 [cs.AI])

    [http://arxiv.org/abs/2307.06152](http://arxiv.org/abs/2307.06152)

    本文提出了一种无需手工设计奖励函数的自动课程强化学习方法，使代理能够从零开始学习空战中的有效决策。通过逐渐学习一系列从简单到困难的子任务，代理能够在各种状态下做出有效的机动决策。

    

    机动决策是无人作战飞行器自主空战的核心。为了解决这个问题，我们提出了一种自动课程强化学习方法，使代理能够从零开始学习空战中的有效决策。通过初始状态范围来区分不同难度水平的课程，将机动决策分为一系列从简单到困难的子任务，并使用测试结果来改变子任务。随着子任务的变化，代理逐渐学会完成一系列从简单到困难的子任务，使其能够在无需进行奖励函数设计的情况下做出有效的机动决策以应对各种状态。消融研究表明，本文提出的自动课程学习是通过强化学习进行训练的一个重要组成部分，即代理在没有课程学习的情况下无法完成有效决策。

    Maneuver decision-making is the core of unmanned combat aerial vehicle for autonomous air combat. To solve this problem, we propose an automatic curriculum reinforcement learning method, which enables agents to learn effective decisions in air combat from scratch. The range of initial states are used for distinguishing curricula of different difficulty levels, thereby maneuver decision is divided into a series of sub-tasks from easy to difficult, and test results are used to change sub-tasks. As sub-tasks change, agents gradually learn to complete a series of sub-tasks from easy to difficult, enabling them to make effective maneuvering decisions to cope with various states without the need to spend effort designing reward functions. The ablation studied show that the automatic curriculum learning proposed in this article is an essential component for training through reinforcement learning, namely, agents cannot complete effective decisions without curriculum learning. Simulation exper
    
[^12]: SayPlan: 使用3D场景图为可扩展任务规划对大规模语言模型进行基础化

    SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning. (arXiv:2307.06135v1 [cs.RO])

    [http://arxiv.org/abs/2307.06135](http://arxiv.org/abs/2307.06135)

    SayPlan是一种利用3D场景图为基础的可扩展的大规模任务规划方法，通过利用3D场景图的层次结构、集成经典路径规划器以及引入迭代的重规划流程，实现了在庞大复杂环境中进行规划的能力。

    

    大规模语言模型（LLM）在开发多样化任务的通用规划智能体方面取得了令人印象深刻的结果。然而，将这些规划应用于庞大、多层楼、多房间的环境中对机器人提出了重大挑战。我们介绍了一种名为SayPlan的可扩展方法，利用3D场景图（3DSG）表示进行基于LLM的大规模任务规划。为了确保我们的方法的可扩展性，我们：（1）利用3DSG的层次结构允许LLMs从较小的、折叠的完整图表示中进行语义搜索，寻找与任务相关的子图；（2）通过集成经典路径规划器减少LLM的规划视野；（3）引入一个迭代的重规划流程，通过与场景图模拟器的反馈来修正不可行的动作并避免规划失败。我们在两个涵盖3层、36个房间和140个对象的大规模环境上评估了我们的方法。

    Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a semantic search for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an iterative replanning pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors, 36 rooms and 140 objects
    
[^13]: Guided Bottom-Up Interactive Constraint Acquisition（指导的自下而上交互式约束获取）

    Guided Bottom-Up Interactive Constraint Acquisition. (arXiv:2307.06126v1 [cs.AI])

    [http://arxiv.org/abs/2307.06126](http://arxiv.org/abs/2307.06126)

    该论文提出了两种改进约束获取系统效率的新方法，一种是自下而上的方法，名为GrowAcq，可以减少用户的等待时间和查询次数，提高交互式约束获取的效率。

    

    约束获取系统可以用于辅助约束满足问题的建模。在交互式约束获取中，系统被给予一组候选约束，并向用户提问以找到候选约束中的正确约束。目前的交互式约束获取算法存在至少两个主要瓶颈。首先，为了收敛，它们需要向用户询问大量的查询。其次，它们无法处理大量的候选约束，因为这会导致用户等待时间较长。因此，用户必须对系统应考虑的约束有相当精确的知识。在本文中，我们通过提出两种改进CA效率的新方法来缓解这些瓶颈。首先，我们引入了一种名为GrowAcq的自下而上方法，它减少了用户的最大等待时间，并使系统能够处理更大的候选约束集。它还减少了总的查询次数，提高了交互式约束获取的效率。

    Constraint Acquisition (CA) systems can be used to assist in the modeling of constraint satisfaction problems. In (inter)active CA, the system is given a set of candidate constraints and posts queries to the user with the goal of finding the right constraints among the candidates. Current interactive CA algorithms suffer from at least two major bottlenecks. First, in order to converge, they require a large number of queries to be asked to the user. Second, they cannot handle large sets of candidate constraints, since these lead to large waiting times for the user. For this reason, the user must have fairly precise knowledge about what constraints the system should consider. In this paper, we alleviate these bottlenecks by presenting two novel methods that improve the efficiency of CA. First, we introduce a bottom-up approach named GrowAcq that reduces the maximum waiting time for the user and allows the system to handle much larger sets of candidate constraints. It also reduces the tot
    
[^14]: 学习层次交互式多目标搜索以进行移动操作

    Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation. (arXiv:2307.06125v1 [cs.RO])

    [http://arxiv.org/abs/2307.06125](http://arxiv.org/abs/2307.06125)

    这项工作提出了一个层次化的强化学习方法，用于解决在未知环境中需要同时进行操控和导航的交互式多目标搜索任务。实验证明，该方法可以在新环境中进行零样本迁移，并对未见过的子任务具有鲁棒性。

    

    现有的目标搜索方法使得机器人可以在自由路径上进行搜索，然而，在非结构化的以人为中心的环境中操作的机器人经常需要操控环境以满足他们的需求。在这项工作中，我们引入了一种新的交互式多目标搜索任务，机器人需要打开门以浏览房间，并在橱柜和抽屉内搜索目标物品。这些新挑战需要在未知环境中结合操控和导航技能。我们提出了HIMOS，一种层次强化学习方法，学习组合探索、导航和操控技能。为了实现这一点，我们设计了一个围绕语义地图记忆的抽象高级动作空间，并利用探索过的环境作为实例导航点。我们在仿真和真实世界中进行了大量实验，证明HIMOS可以零样本方式有效地迁移到新的环境中，并且对于未见过的子任务具有鲁棒性。

    Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subp
    
[^15]: TreeFormer:一种基于半监督Transformer的单高分辨率图像中树木计数的框架

    TreeFormer: a Semi-Supervised Transformer-based Framework for Tree Counting from a Single High Resolution Image. (arXiv:2307.06118v1 [cs.CV])

    [http://arxiv.org/abs/2307.06118](http://arxiv.org/abs/2307.06118)

    TreeFormer是一种基于半监督Transformer的框架，用于从单个高分辨率图像中进行树木计数。它利用金字塔树表示模块和注意力机制来提取特征，并通过回归器进行树木密度估算。此外，采用局部树密度一致性和排名损失来利用无标签图像进行训练和优化。

    

    在摄影测量和遥感中，利用单个航空和卫星图像进行树木密度估算和计数是一项具有挑战性的任务，但在森林管理中具有重要作用。本文提出了一种基于半监督Transformer的树木计数框架，通过减少遥感图像上昂贵的树木标注来实现。我们的方法称为TreeFormer，首先开发了一个基于Transformer块的金字塔树表示模块，用于在编码阶段提取多尺度特征。进一步设计了基于上下文注意力的特征融合和树密度回归器模块，利用编码器中的稳健特征来估算解码器中的树密度图。此外，我们提出了一个金字塔学习策略，包括局部树密度一致性和局部树计数排名损失，以利用无标签图像进行训练。最后，引入了树计数记号来调节网络，通过计算...

    Automatic tree density estimation and counting using single aerial and satellite images is a challenging task in photogrammetry and remote sensing, yet has an important role in forest management. In this paper, we propose the first semisupervised transformer-based framework for tree counting which reduces the expensive tree annotations for remote sensing images. Our method, termed as TreeFormer, first develops a pyramid tree representation module based on transformer blocks to extract multi-scale features during the encoding stage. Contextual attention-based feature fusion and tree density regressor modules are further designed to utilize the robust features from the encoder to estimate tree density maps in the decoder. Moreover, we propose a pyramid learning strategy that includes local tree density consistency and local tree count ranking losses to utilize unlabeled images into the training process. Finally, the tree counter token is introduced to regulate the network by computing th
    
[^16]: 深度神经网络中的定量中心极限定理

    Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])

    [http://arxiv.org/abs/2307.06092](http://arxiv.org/abs/2307.06092)

    本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。

    

    我们研究了具有随机高斯权重和偏置的全连接神经网络的分布，其中隐藏层宽度与大常数 $n$ 成比例。在非线性的温和假设下，我们得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限。我们的定理表明，无论是对于有限维分布还是整个过程，随机全连接网络（及其导数）与相应的无限宽高斯过程之间的距离都会按照 $n^{-\gamma}$ 缩放，其中 $\gamma>0$，指数取决于用于度量差异的度量方式。我们的界限在网络宽度的依赖性方面比文献中以前提供的任何界限都要强。

    We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma>0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
    
[^17]: VELMA: LLM智能体在街景中进行视觉和语言导航的口头化体现

    VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View. (arXiv:2307.06082v1 [cs.AI])

    [http://arxiv.org/abs/2307.06082](http://arxiv.org/abs/2307.06082)

    VELMA是一个口头化的LLM智能体，利用人类写作的导航指令中的地标并结合CLIP来进行视觉环境的理解，以实现在街景中的视觉和语言导航。

    

    在现实世界环境中的增量决策是具有挑战性的以体现人工智能的任务之一。其中最具挑战性的场景之一是视觉和语言导航(VLN)，它需要视觉和自然语言理解以及空间和时间推理能力。这个体现智能体需要在街景等真实世界环境的观察基础上准确理解导航指令。尽管LLM在其他研究领域取得了令人印象深刻的结果，但如何最好地将它们与交互式视觉环境连接起来仍然是一个持续的问题。在这项工作中，我们提出了VELMA，一种使用轨迹和视觉环境观察的口头化作为下一步操作的上下文提示的LLM智能体。视觉信息通过一个流程进行口头化，该流程从人类编写的导航指令中提取地标，并使用CLIP来确定它们在当前全景视图中的可见性。

    Incremental decision making in real-world environments is one of the most challenging tasks in embodied artificial intelligence. One particularly demanding scenario is Vision and Language Navigation~(VLN) which requires visual and natural language understanding as well as spatial and temporal reasoning capabilities. The embodied agent needs to ground its understanding of navigation instructions in observations of a real-world environment like Street View. Despite the impressive results of LLMs in other research areas, it is an ongoing problem of how to best connect them with an interactive visual environment. In this work, we propose VELMA, an embodied LLM agent that uses a verbalization of the trajectory and of visual environment observations as contextual prompt for the next action. Visual information is verbalized by a pipeline that extracts landmarks from the human written navigation instructions and uses CLIP to determine their visibility in the current panorama view. We show that
    
[^18]: 图像中的多元高斯异常检测的可视化

    Visualization for Multivariate Gaussian Anomaly Detection in Images. (arXiv:2307.06052v1 [cs.CV])

    [http://arxiv.org/abs/2307.06052](http://arxiv.org/abs/2307.06052)

    本文介绍了一种用于图像中的异常检测的简化多元高斯方法，通过应用白化转换可生成可视化的热图来解释特征，并在MVTec-AD数据集上验证了该方法的有效性。

    

    本文介绍了一种简化的PaDiM（通过实例建模进行像素级异常检测）方法，用于图像中的异常检测，通过将从骨干卷积神经网络（CNN）提取的特征向量拟合为单个多元高斯（MVG）分布，并使用它们的马哈拉诺比斯距离作为异常得分。我们在这个框架中引入了一个中间步骤，通过对特征向量应用白化变换，能够生成能够可视化解释MVG所学到的特征的热图。我们在MVTec-AD数据集上评估了提出的技术，并结果显示了视觉模型验证的重要性，为这个框架中的一些在其他情况下看不见的问题提供了洞见。本文生成的可视化结果可以在https://doi.org/10.5281/zenodo.7937978上公开获取。

    This paper introduces a simplified variation of the PaDiM (Pixel-Wise Anomaly Detection through Instance Modeling) method for anomaly detection in images, fitting a single multivariate Gaussian (MVG) distribution to the feature vectors extracted from a backbone convolutional neural network (CNN) and using their Mahalanobis distance as the anomaly score. We introduce an intermediate step in this framework by applying a whitening transformation to the feature vectors, which enables the generation of heatmaps capable of visually explaining the features learned by the MVG. The proposed technique is evaluated on the MVTec-AD dataset, and the results show the importance of visual model validation, providing insights into issues in this framework that were otherwise invisible. The visualizations generated for this paper are publicly available at https://doi.org/10.5281/zenodo.7937978.
    
[^19]: 借助新的关系类型和节点，以OOD多任务视角进行链接预测

    An OOD Multi-Task Perspective for Link Prediction with New Relation Types and Nodes. (arXiv:2307.06046v1 [cs.LG])

    [http://arxiv.org/abs/2307.06046](http://arxiv.org/abs/2307.06046)

    本论文提出了一种在OOD测试多图中进行链接预测的方法，通过使用双可交换性概念，该方法能够处理具有不同预测模式且可能冲突的关系类型集合的属性多图。

    

    在归纳链接预测任务中，我们推断具有属性的多图中新测试多图中节点之间的缺失属性链接（关系）。传统的关系学习方法面临着对OOD测试多图的有限泛化能力的挑战，这些多图包含了训练中未见过的新节点和新关系类型。最近，高等人（2023）在所有关系类型共享相同结构预测模式（单个任务）的唯一假设下，提出了一种使用双可交换性理论概念（用于节点和关系类型）来进行OOD链接预测的方法，与使用图神经网络（GNNs）设计的（单个）可交换性（仅用于节点）相反。在这项工作中，我们进一步将双可交换性概念扩展到多任务双可交换性，其中我们定义了属性多图中的链接预测，这些图可能对不同的关系类型集合具有不同且可能冲突的预测模式（多个任务）。

    The task of inductive link prediction in (discrete) attributed multigraphs infers missing attributed links (relations) between nodes in new test multigraphs. Traditional relational learning methods face the challenge of limited generalization to OOD test multigraphs containing both novel nodes and novel relation types not seen in training. Recently, under the only assumption that all relation types share the same structural predictive patterns (single task), Gao et al. (2023) proposed an OOD link prediction method using the theoretical concept of double exchangeability (for nodes & relation types), in contrast to the (single) exchangeability (only for nodes) used to design Graph Neural Networks (GNNs). In this work we further extend the double exchangeability concept to multi-task double exchangeability, where we define link prediction in attributed multigraphs that can have distinct and potentially conflicting predictive patterns for different sets of relation types (multiple tasks). 
    
[^20]: AI生成的图像：`现成品'的新时代。

    AI-Generated Imagery: A New Era for the `Readymade'. (arXiv:2307.06033v1 [cs.AI])

    [http://arxiv.org/abs/2307.06033](http://arxiv.org/abs/2307.06033)

    这项研究探讨了AI生成的图像如何被视为艺术，并提出了一种哲学框架，认为一些AI生成的图像可以被视为 "现成品" 艺术。

    

    虽然术语 "艺术" 没有具体的定义，但本文旨在研究由生成AI系统（如Midjourney）产生的数字图像为什么经常被称为艺术。目前，关于将AI生成的图像分类为艺术的讨论在某种程度上是同质的，缺乏适用于更传统艺术媒体制作方式的细致方面。本文旨在在艺术背景下将重要的哲学考虑带到关于AI生成图像的讨论的表面。我们使用现有的哲学框架和语言理论，提出一些AI生成的图像，凭借其在这些框架内的视觉特性，可以被作为 "现成品" 考虑为艺术。

    While the term `art' defies any concrete definition, this paper aims to examine how digital images produced by generative AI systems, such as Midjourney, have come to be so regularly referred to as such. The discourse around the classification of AI-generated imagery as art is currently somewhat homogeneous, lacking the more nuanced aspects that would apply to more traditional modes of artistic media production. This paper aims to bring important philosophical considerations to the surface of the discussion around AI-generated imagery in the context of art. We employ existing philosophical frameworks and theories of language to suggest that some AI-generated imagery, by virtue of its visual properties within these frameworks, can be presented as `readymades' for consideration as art.
    
[^21]: 通过两方面三视图标签传播的高效时间感知实体对齐框架

    An Effective and Efficient Time-aware Entity Alignment Framework via Two-aspect Three-view Label Propagation. (arXiv:2307.06013v1 [cs.AI])

    [http://arxiv.org/abs/2307.06013](http://arxiv.org/abs/2307.06013)

    本文提出了一个高效且有效的非神经网络时间感知实体对齐框架，通过两方面三视图标签传播、稀疏相似度、Sinkhorn算子和时态迭代学习来提高对齐性能，并减少模型的时间开销。

    

    实体对齐旨在寻找不同知识图谱之间的等价实体对，这对于推动知识融合至关重要。随着时态知识图谱的广泛使用，时间感知实体对齐方法出现以增强对齐效果。现有的时间感知实体对齐模型基于图神经网络（GNN），取得了最先进的性能，但由于GNN的可扩展性问题，很难将其应用到大规模时态知识图谱中。因此，本文提出了一个有效且高效的非神经网络实体对齐框架LightTEA，它由四个关键组件组成：(1)两方面三视图标签传播，(2)带有时态约束的稀疏相似度，(3)Sinkhorn算子，以及(4)时态迭代学习。所有这些模块共同作用，提高了实体对齐的性能，同时减少了模型的时间开销。在公共数据集上的广泛实验证明，我们提出的模型明显优于最先进的方法。

    Entity alignment (EA) aims to find the equivalent entity pairs between different knowledge graphs (KGs), which is crucial to promote knowledge fusion. With the wide use of temporal knowledge graphs (TKGs), time-aware EA (TEA) methods appear to enhance EA. Existing TEA models are based on Graph Neural Networks (GNN) and achieve state-of-the-art (SOTA) performance, but it is difficult to transfer them to large-scale TKGs due to the scalability issue of GNN. In this paper, we propose an effective and efficient non-neural EA framework between TKGs, namely LightTEA, which consists of four essential components: (1) Two-aspect Three-view Label Propagation, (2) Sparse Similarity with Temporal Constraints, (3) Sinkhorn Operator, and (4) Temporal Iterative Learning. All of these modules work together to improve the performance of EA while reducing the time consumption of the model. Extensive experiments on public datasets indicate that our proposed model significantly outperforms the SOTA method
    
[^22]: 基于Transformer的强化学习综述

    Transformers in Reinforcement Learning: A Survey. (arXiv:2307.05979v1 [cs.LG])

    [http://arxiv.org/abs/2307.05979](http://arxiv.org/abs/2307.05979)

    本综述介绍了基于Transformer的强化学习的应用。Transformer在解决强化学习中的挑战上具有潜在优势，包括不稳定训练、信用分配、缺乏可解释性和部分可观测性等方面。研究还探讨了其在表示学习、状态转移和奖励函数建模以及策略优化等方面的应用。此外，还有一些最新的研究旨在提高Transformer的可解释性和效率。

    

    Transformer在自然语言处理、计算机视觉和机器人等领域具有显著的影响，在性能上优于其他神经网络。本综述探讨了Transformer在强化学习（RL）中的应用，它被视为解决不稳定训练、信用分配、缺乏可解释性和部分可观测性等挑战的有希望解决方案。我们首先简要介绍了RL领域的概述，然后讨论了传统RL算法的挑战。接下来，我们深入探讨了Transformer及其变体的特性，并讨论了使其适合应对RL中固有挑战的特点。我们还研究了Transformer在RL中各个方面的应用，包括表示学习、状态转移和奖励函数建模以及策略优化。此外，我们还讨论了最近的研究，旨在提高Transformer的可解释性和效率。

    Transformers have significantly impacted domains like natural language processing, computer vision, and robotics, where they improve performance compared to other neural networks. This survey explores how transformers are used in reinforcement learning (RL), where they are seen as a promising solution for addressing challenges such as unstable training, credit assignment, lack of interpretability, and partial observability. We begin by providing a brief domain overview of RL, followed by a discussion on the challenges of classical RL algorithms. Next, we delve into the properties of the transformer and its variants and discuss the characteristics that make them well-suited to address the challenges inherent in RL. We examine the application of transformers to various aspects of RL, including representation learning, transition and reward function modeling, and policy optimization. We also discuss recent research that aims to enhance the interpretability and efficiency of transformers i
    
[^23]: 实现互联网规模的文本到图像扩散模型的安全自蒸馏

    Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models. (arXiv:2307.05977v1 [cs.CV])

    [http://arxiv.org/abs/2307.05977](http://arxiv.org/abs/2307.05977)

    本文介绍了一种名为SDD的方法，用于实现互联网规模的文本到图像扩散模型的安全自蒸馏。该方法通过引导噪声估计与无条件模型匹配，在生成的图像中消除有害内容的比例更大，同时保持整体图像质量，并且允许一次移除多个概念。

    

    大规模图像生成模型借助互联网上丰富的数据具有卓越的质量，但也引发了社会关切，担心这些模型可能生成有害或受版权保护的内容。这些偏见和有害性在整个训练过程中产生，并且很难完全消除，这已成为安全部署这些模型的重要障碍。本文提出了一种名为SDD的方法，用于在文本到图像扩散模型中防止问题内容的生成。我们通过自蒸馏扩散模型来引导基于目标移除概念的噪声估计与无条件模型匹配。与之前的方法相比，我们的方法可以消除更大比例的有害内容，同时不降低整体图像质量。此外，我们的方法还允许一次移除多个概念，而之前的工作只能一次移除一个概念。

    Large-scale image generation models, with impressive quality made possible by the vast amount of data available on the Internet, raise social concerns that these models may generate harmful or copyrighted content. The biases and harmfulness arise throughout the entire training process and are hard to completely remove, which have become significant hurdles to the safe deployment of these models. In this paper, we propose a method called SDD to prevent problematic content generation in text-to-image diffusion models. We self-distill the diffusion model to guide the noise estimate conditioned on the target removal concept to match the unconditional one. Compared to the previous methods, our method eliminates a much greater proportion of harmful content from the generated images without degrading the overall image quality. Furthermore, our method allows the removal of multiple concepts at once, whereas previous works are limited to removing a single concept at a time.
    
[^24]: VoxPoser: 用于带有语言模型的机器人操作的可组合的3D价值映射

    VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v1 [cs.RO])

    [http://arxiv.org/abs/2307.05973](http://arxiv.org/abs/2307.05973)

    VoxPoser提出了一种新方法，通过组合3D价值映射和语言模型，实现了机器人在多种操作任务下根据自由形式的指令和对象合成机器人轨迹的能力。

    

    研究表明，大型语言模型（LLMs）具有丰富的可行动知识，可以以推理和规划的形式提取出用于机器人操作的信息。尽管取得了进展，大多数模型仍然依赖于预定义的运动原语来执行与环境的物理交互，这仍然是一个重大瓶颈。在这项工作中，我们的目标是在给定开集指令和开集对象的情况下，为各种操作任务合成机器人轨迹，即一系列密集的6-DoF末端执行器路径点。我们首先观察到LLMs在给定自由形式的语言指令时擅长推断可行性和约束。更重要的是，通过利用它们的代码编写能力，它们可以与视觉-语言模型（VLM）交互，以组合3D价值映射将知识接地到Agent的观测空间中。然后在基于模型的规划框架中使用组合的价值映射来零试合成闭环轨迹。

    Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a visual-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop ro
    
[^25]: 给机器人以帮助：通过眼手协同的人类视频演示学习通用操作

    Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations. (arXiv:2307.05959v1 [cs.RO])

    [http://arxiv.org/abs/2307.05959](http://arxiv.org/abs/2307.05959)

    使用无标签的人类视频演示增强了眼手协同的视觉运动策略的泛化能力，而无需昂贵的专家演示数据或领域适应方法。

    

    眼手协同摄像头在视觉导向的机器人操作中显示出了更高的样本效率和泛化能力。然而，在机器人模仿中，让人类远程操作员收集大量专家演示对于真实机器人来说仍然很昂贵。另一方面，人类进行任务的视频要便宜得多，因为它们消除了对机器人遥操作专业知识的需求，并且可以在各种场景中快速捕捉。因此，人类视频演示是学习大规模通用机器人操作策略的有希望的数据来源。在这项工作中，我们使用广泛的无标签人类视频演示来增强狭窄的机器人模仿数据集，从而大大提高了眼手协同视觉运动策略的泛化能力。尽管人类和机器人数据之间存在明显的视觉领域差距，但我们的框架不需要使用任何明确的领域适应方法，因为我们利用了部分可观察性。

    Eye-in-hand cameras have shown promise in enabling greater sample efficiency and generalization in vision-based robotic manipulation. However, for robotic imitation, it is still expensive to have a human teleoperator collect large amounts of expert demonstrations with a real robot. Videos of humans performing tasks, on the other hand, are much cheaper to collect since they eliminate the need for expertise in robotic teleoperation and can be quickly captured in a wide range of scenarios. Therefore, human video demonstrations are a promising data source for learning generalizable robotic manipulation policies at scale. In this work, we augment narrow robotic imitation datasets with broad unlabeled human video demonstrations to greatly enhance the generalization of eye-in-hand visuomotor policies. Although a clear visual domain gap exists between human and robot data, our framework does not need to employ any explicit domain adaptation method, as we leverage the partial observability of e
    
[^26]: 自动协调预测准确性和提前性的规范业务流程监控

    Automatically Reconciling the Trade-off between Prediction Accuracy and Earliness in Prescriptive Business Process Monitoring. (arXiv:2307.05939v1 [cs.SE])

    [http://arxiv.org/abs/2307.05939](http://arxiv.org/abs/2307.05939)

    该研究探讨了在规范性业务流程监控中如何自动协调预测的准确性和提前性之间的权衡问题。研究发现之前的预测准确性较低，但较早的调整对于预防不良流程结果至关重要。

    

    规范性业务流程监控为流程经理提供决策支持，确定何时以及如何调整正在进行的业务流程以防止或减轻不良流程结果。我们关注于自动协调预测准确性和提前性之间的权衡问题，以确定何时进行调整。调整应尽早发生，以提供足够的时间来有效进行调整。然而，较早的预测通常比较晚的预测准确性较低。这意味着根据较不准确的预测采取行动可能会导致不必要的调整或错过调整。已在文献中提出了不同的方法来协调预测准确性和提前性的权衡。到目前为止，这些方法与不同的基线进行比较，并使用不同的数据集甚至机密数据集进行评估。这限制了方法的可比性和可复制性，使其变得困难。

    Prescriptive business process monitoring provides decision support to process managers on when and how to adapt an ongoing business process to prevent or mitigate an undesired process outcome. We focus on the problem of automatically reconciling the trade-off between prediction accuracy and prediction earliness in determining when to adapt. Adaptations should happen sufficiently early to provide enough lead time for the adaptation to become effective. However, earlier predictions are typically less accurate than later predictions. This means that acting on less accurate predictions may lead to unnecessary adaptations or missed adaptations.  Different approaches were presented in the literature to reconcile the trade-off between prediction accuracy and earliness. So far, these approaches were compared with different baselines, and evaluated using different data sets or even confidential data sets. This limits the comparability and replicability of the approaches and makes it difficult t
    
[^27]: BiRP：使用相对参数化方法学习机器人广义双臂协调-基于人体演示

    BiRP: Learning Robot Generalized Bimanual Coordination using Relative Parameterization Method on Human Demonstration. (arXiv:2307.05933v1 [cs.RO])

    [http://arxiv.org/abs/2307.05933](http://arxiv.org/abs/2307.05933)

    本文提出了一种相对参数化方法，通过从人体演示中学习双臂协调，将双臂任务分为领导者-追随者协调和协同协调两种类型，并将协调表示为高斯混合模型以描述协调的重要性变化，从而实现了对新任务参数的广义协调。

    

    人类的双臂操纵可以执行比简单组合两只单臂更复杂的任务，这要归功于双臂之间的时空协调。然而，双臂协调的描述仍然是机器人领域中的一个开放性问题。这使得很难提供一个可解释的协调范例，更不用说应用到机器人中了。在这项工作中，我们将人类日常活动中的主要双臂任务分为两种类型：领导者-追随者协调和协同协调。然后我们提出了一种相对参数化方法，通过从双臂演示中将协调表示为高斯混合模型，以概率描述协调在运动过程中的重要性变化。学习到的协调表示可以推广到新的任务参数，同时确保时空协调。我们使用合成动作和人体演示数据演示了这种方法。

    Human bimanual manipulation can perform more complex tasks than a simple combination of two single arms, which is credited to the spatio-temporal coordination between the arms. However, the description of bimanual coordination is still an open topic in robotics. This makes it difficult to give an explainable coordination paradigm, let alone applied to robotics. In this work, we divide the main bimanual tasks in human daily activities into two types: leader-follower and synergistic coordination. Then we propose a relative parameterization method to learn these types of coordination from human demonstration. It represents coordination as Gaussian mixture models from bimanual demonstration to describe the change in the importance of coordination throughout the motions by probability. The learned coordinated representation can be generalized to new task parameters while ensuring spatio-temporal coordination. We demonstrate the method using synthetic motions and human demonstration data and
    
[^28]: 一个新的数据集和用于蚜虫簇检测的比较研究

    A New Dataset and Comparative Study for Aphid Cluster Detection. (arXiv:2307.05929v1 [cs.CV])

    [http://arxiv.org/abs/2307.05929](http://arxiv.org/abs/2307.05929)

    本研究提出了一个新的数据集，并进行了用于蚜虫簇检测的比较研究。通过检测蚜虫簇，我们可以准确估计感染水平，从而实现精确的局部杀虫剂应用。

    

    蚜虫是对农作物、农村家庭和全球粮食安全构成的主要威胁之一。化学防治是农作物生产中最大化产量的必要组成部分，然而考虑到环境污染和成本，将化学方法应用于整个田地是不必要的。因此，准确地定位蚜虫并估计感染水平对于精确地局部应用杀虫剂至关重要。蚜虫检测非常具有挑战性，因为每只个体蚜虫非常小，而且所有蚜虫都聚集在一起形成簇。在本文中，我们提出通过检测蚜虫簇来估计感染水平。我们在高粱田采集了数百万张图像，手动选择了5,447张包含蚜虫的图像，并对每个蚜虫簇进行了注释。为了将这些图像用于机器学习模型，我们将图像裁剪为小块，并创建了一个带有超过151,000个图像块的标签数据集。然后，我们实施并比较了不同的方法来检测蚜虫簇。

    Aphids are one of the main threats to crops, rural families, and global food security. Chemical pest control is a necessary component of crop production for maximizing yields, however, it is unnecessary to apply the chemical approaches to the entire fields in consideration of the environmental pollution and the cost. Thus, accurately localizing the aphid and estimating the infestation level is crucial to the precise local application of pesticides. Aphid detection is very challenging as each individual aphid is really small and all aphids are crowded together as clusters. In this paper, we propose to estimate the infection level by detecting aphid clusters. We have taken millions of images in the sorghum fields, manually selected 5,447 images that contain aphids, and annotated each aphid cluster in the image. To use these images for machine learning models, we crop the images into patches and created a labeled dataset with over 151,000 image patches. Then, we implement and compare the 
    
[^29]: 阅读放射学成像的方式，就像放射科医生一样

    Reading Radiology Imaging Like The Radiologist. (arXiv:2307.05921v1 [cs.CV])

    [http://arxiv.org/abs/2307.05921](http://arxiv.org/abs/2307.05921)

    提出一种以疾病为导向的方法，解决自动放射学报告生成中的细微差异关注、数据偏差和长文本生成的挑战。

    

    自动放射学报告生成旨在生成包含放射学成像的丰富、精细描述的放射学报告。与自然图像领域的图像描述相比，医学图像非常相似，仅在疾病发生的细微差异上有所不同。鉴于这些细微差异在放射学报告中的重要性，鼓励模型更加关注疾病发生的微妙区域至关重要。其次，视觉和文本数据偏差的问题很严重。不仅正常病例占数据集的大部分，还描绘有病变区域的句子只占段落的一小部分。最后，生成医学图像报告涉及到长文本的生成挑战，这需要更多医学知识的专业性和经验训练。因此，生成此类报告的难度增加。为了解决这些挑战，我们提出了一种以疾病为导向的方法。

    Automated radiology report generation aims to generate radiology reports that contain rich, fine-grained descriptions of radiology imaging. Compared with image captioning in the natural image domain, medical images are very similar to each other, with only minor differences in the occurrence of diseases. Given the importance of these minor differences in the radiology report, it is crucial to encourage the model to focus more on the subtle regions of disease occurrence. Secondly, the problem of visual and textual data biases is serious. Not only do normal cases make up the majority of the dataset, but sentences describing areas with pathological changes also constitute only a small part of the paragraph. Lastly, generating medical image reports involves the challenge of long text generation, which requires more expertise and empirical training in medical knowledge. As a result, the difficulty of generating such reports is increased. To address these challenges, we propose a disease-ori
    
[^30]: 通过插值光流实现近距离视角合成

    Close-up View synthesis by Interpolating Optical Flow. (arXiv:2307.05913v1 [cs.CV])

    [http://arxiv.org/abs/2307.05913](http://arxiv.org/abs/2307.05913)

    本文提出了一种利用光流插值的方法实现近距离视角合成，并通过巧妙应用光流值，克服了谷歌街景系统中的视觉失真和图像模糊问题。

    

    虚拟视点被视为虚拟导航中的新技术，但由于缺乏深度信息和模糊的相机参数而未得到支持。本文提出了一种通过光流建立视差效果，实现伪3D投影的近距离虚拟视角的方法，而不使用深度传感器。我们开发了一种双向光流方法，通过比例插值光流来获得任意虚拟视点。此外，通过巧妙应用光流值，我们通过镜头伸缩实现在任意角落的清晰和视觉保真的放大效果，从而克服了谷歌街景系统中通过视角放大和过渡带来的视觉失真和图像模糊问题。

    The virtual viewpoint is perceived as a new technique in virtual navigation, as yet not supported due to the lack of depth information and obscure camera parameters. In this paper, a method for achieving close-up virtual view is proposed and it only uses optical flow to build parallax effects to realize pseudo 3D projection without using depth sensor. We develop a bidirectional optical flow method to obtain any virtual viewpoint by proportional interpolation of optical flow. Moreover, with the ingenious application of the optical-flow-value, we achieve clear and visual-fidelity magnified results through lens stretching in any corner, which overcomes the visual distortion and image blur through viewpoint magnification and transition in Google Street View system.
    
[^31]: 带有乘法平滑的特征归因稳定性保证

    Stability Guarantees for Feature Attributions with Multiplicative Smoothing. (arXiv:2307.05902v1 [cs.LG])

    [http://arxiv.org/abs/2307.05902](http://arxiv.org/abs/2307.05902)

    本文提出了一种基于乘法平滑的特征归因方法，通过证明模型的Lipschitz性质，保证了其稳定性，并在视觉和语言模型上进行了评估，显示了非平凡的稳定性保证。

    

    机器学习模型的解释方法往往不能提供任何形式的保证，也可能不反映底层的决策过程。在这项工作中，我们将稳定性作为可靠的特征归因方法的一个属性进行分析。我们证明了如果模型在特征屏蔽方面具有足够的Lipschitz性质，则可以保证放松变体的稳定性。为了实现这样的模型，我们开发了一种称为乘法平滑（MuS）的平滑方法。我们展示了MuS克服了标准平滑技术的理论限制，并且可以与任何分类器和特征归因方法结合使用。我们使用各种特征归因方法（如LIME和SHAP）对视觉和语言模型进行了MuS的评估，并展示了MuS赋予了特征归因以非平凡的稳定性保证。

    Explanation methods for machine learning models tend to not provide any formal guarantees and may not reflect the underlying decision-making process. In this work, we analyze stability as a property for reliable feature attribution methods. We prove that relaxed variants of stability are guaranteed if the model is sufficiently Lipschitz with respect to the masking of features. To achieve such a model, we develop a smoothing method called Multiplicative Smoothing (MuS). We show that MuS overcomes theoretical limitations of standard smoothing techniques and can be integrated with any classifier and feature attribution method. We evaluate MuS on vision and language models with a variety of feature attribution methods, such as LIME and SHAP, and demonstrate that MuS endows feature attributions with non-trivial stability guarantees.
    
[^32]: 受PID控制器启发的偏差归纳法在部分可观测控制任务中的深度强化学习

    PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks. (arXiv:2307.05891v1 [cs.LG])

    [http://arxiv.org/abs/2307.05891](http://arxiv.org/abs/2307.05891)

    该论文受到PID控制器的启发，提出了两种用于编码历史记录的架构，以平衡提取相关信息的灵活性与对环境变化的鲁棒性。

    

    深度强化学习（RL）已经展现出通过数据自己学习控制系统的巨大潜力。然而，深度RL面临的一个挑战是系统的完整状态通常不可观测。当出现这种情况时，策略需要利用观察历史来推断当前状态。同时，训练和测试环境之间的差异使得策略不会过度拟合训练时观察到的序列。因此，在历史记录编码器灵活提取相关信息的同时，要对环境变化具有鲁棒性，这是一个重要的平衡。为了达到这个平衡，我们寻求PID控制器的启发。我们断定PID控制器的成功表明，许多控制任务只需要求和和求差来累积信息。基于这个原则，我们提出了两种用于编码历史记录的架构：一种直接使用...

    Deep reinforcement learning (RL) has shown immense potential for learning to control systems through data alone. However, one challenge deep RL faces is that the full state of the system is often not observable. When this is the case, the policy needs to leverage the history of observations to infer the current state. At the same time, differences between the training and testing environments makes it critical for the policy not to overfit to the sequence of observations it sees at training time. As such, there is an important balancing act between having the history encoder be flexible enough to extract relevant information, yet be robust to changes in the environment. To strike this balance, we look to the PID controller for inspiration. We assert the PID controller's success shows that only summing and differencing are needed to accumulate information over time for many control tasks. Following this principle, we propose two architectures for encoding history: one that directly uses
    
[^33]: 部署机器学习的生态系统级分析揭示了同质化的结果

    Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes. (arXiv:2307.05862v1 [cs.LG])

    [http://arxiv.org/abs/2307.05862](http://arxiv.org/abs/2307.05862)

    部署机器学习存在系统性故障, 即使单个模型在总体上的改善也不能解决这个问题

    

    传统上，机器学习通常在模型层面进行研究：研究人员衡量和改进特定模型的准确性、鲁棒性、偏见、效率和其他维度。实际上，机器学习的社会影响取决于机器学习部署的周围环境。为了捕捉这一点，我们引入了生态系统级分析：不是分析单个模型，而是考虑在给定环境中部署的所有模型的集合。例如，在招聘中进行生态系统级分析意味着认识到一个求职者的结果不仅仅取决于单个招聘算法或公司，而是取决于他们申请的所有公司的集体决策。在三种模式（文本、图像、语音）和11个数据集上，我们建立了一个明显的趋势：部署的机器学习容易出现系统性故障，这意味着一些用户被所有可用的模型错误分类。即使在个体模型随时间在总体水平上改善，我们也发现

    Machine learning is traditionally studied at the model level: researchers measure and improve the accuracy, robustness, bias, efficiency, and other dimensions of specific models. In practice, the societal impact of machine learning is determined by the surrounding context of machine learning deployments. To capture this, we introduce ecosystem-level analysis: rather than analyzing a single model, we consider the collection of models that are deployed in a given context. For example, ecosystem-level analysis in hiring recognizes that a job candidate's outcomes are not only determined by a single hiring algorithm or firm but instead by the collective decisions of all the firms they applied to. Across three modalities (text, images, speech) and 11 datasets, we establish a clear trend: deployed machine learning is prone to systemic failure, meaning some users are exclusively misclassified by all models available. Even when individual models improve at the population level over time, we fin
    
[^34]: FAIRO: 面向人机交互系统中顺序决策的公平适应性

    FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems. (arXiv:2307.05857v1 [cs.LG])

    [http://arxiv.org/abs/2307.05857](http://arxiv.org/abs/2307.05857)

    这篇论文提出了一种名为FAIRO的算法，用于在人机交互环境中实现公平的顺序决策。该算法将人的行为可变性和偏好变化考虑在内，通过利用选项强化学习框架将复杂的公平任务分解为自适应子任务。

    

    在人机交互环境中实现顺序决策系统的公平性是一个重要问题，特别是当多个具有不同行为和期望的人受到系统中相同适应决策的影响时。人的可变性因素增加了复杂性，因为在某一时间点被认为是公平的政策可能会随着时间的推移由于人的偏好变化而成为歧视性政策。本文从公平性视角考虑人的行为可变性和人的偏好变化，解决了公平性问题。我们提出了一种新颖的算法FAIRO，用于面向人机交互适应的公平顺序决策，它将这些概念纳入决策过程中。具体而言，FAIRO将这个复杂的公平任务基于个体人的偏好通过利用选项强化学习框架分解为自适应子任务。

    Achieving fairness in sequential-decision making systems within Human-in-the-Loop (HITL) environments is a critical concern, especially when multiple humans with different behavior and expectations are affected by the same adaptation decisions in the system. This human variability factor adds more complexity since policies deemed fair at one point in time may become discriminatory over time due to variations in human preferences resulting from inter- and intra-human variability. This paper addresses the fairness problem from an equity lens, considering human behavior variability, and the changes in human preferences over time. We propose FAIRO, a novel algorithm for fairness-aware sequential-decision making in HITL adaptation, which incorporates these notions into the decision-making process. In particular, FAIRO decomposes this complex fairness task into adaptive sub-tasks based on individual human preferences through leveraging the Options reinforcement learning framework. We design 
    
[^35]: 通过简单卷积网络挖掘具有影响力的简单形体

    Influential Simplices Mining via Simplicial Convolutional Network. (arXiv:2307.05841v1 [cs.SI])

    [http://arxiv.org/abs/2307.05841](http://arxiv.org/abs/2307.05841)

    本文提出了一种名为ISMnet的高阶图学习模型，通过利用分层二部图和高阶分层拉普拉斯矩阵，来识别简单复合体中的重要简单形体。

    

    简单复合体近年来在高阶网络分析中备受关注，其中少数简单形体由于网络异质性在结构和功能中起着关键作用。我们发现了识别有影响力节点和简单形体之间存在显著不一致。因此，尽管有关有影响力节点（0-简单形体）的研究相对成熟，但如何表征简单形体的影响力并识别有影响力的简单形体仍然难以解决。与此同时，图神经网络（GNNs）是一种强大的工具，可以同时利用网络拓扑和节点特征，但它们在处理高阶任务时面临困难。本文提出了一种名为有影响力简单形体挖掘神经网络（ISMnet）的高阶图学习模型，用于识别简单复合体中重要的h-简单形体。它可以通过利用新颖的高阶表达：分层二部图和高阶分层（HoH）拉普拉斯矩阵来处理高阶任务。

    Simplicial complexes have recently been in the limelight of higher-order network analysis, where a minority of simplices play crucial roles in structures and functions due to network heterogeneity. We find a significant inconsistency between identifying influential nodes and simplices. Therefore, it remains elusive how to characterize simplices' influence and identify influential simplices, despite the relative maturity of research on influential nodes (0-simplices) identification. Meanwhile, graph neural networks (GNNs) are potent tools that can exploit network topology and node features simultaneously, but they struggle to tackle higher-order tasks. In this paper, we propose a higher-order graph learning model, named influential simplices mining neural network (ISMnet), to identify vital h-simplices in simplicial complexes. It can tackle higher-order tasks by leveraging novel higher-order presentations: hierarchical bipartite graphs and higher-order hierarchical (HoH) Laplacians, whe
    
[^36]: 用经验共享来扩展分布式多任务强化学习

    Scaling Distributed Multi-task Reinforcement Learning with Experience Sharing. (arXiv:2307.05834v1 [cs.LG])

    [http://arxiv.org/abs/2307.05834](http://arxiv.org/abs/2307.05834)

    本文研究了分布式多任务强化学习中的经验共享问题，提出了一种算法DistMT-LSVI，并通过理论和实证研究表明，使用DistMT-LSVI的单个代理可以实现所有任务的ε-最优策略。

    

    最近，DARPA推出了ShELL计划，旨在探索经验共享如何使分布式终身学习代理在适应新挑战方面受益。本文通过对分布式多任务强化学习（RL）进行理论和实证研究，解决了这个问题，其中一组N个代理协作解决了M个任务，而不需要先验知识。我们将问题表述为线性参数化的上下文马尔可夫决策过程（MDP），其中每个任务由指定转移动力学和奖励的上下文表示。为了解决这个问题，我们提出了一种名为DistMT-LSVI的算法。首先，代理识别任务，然后通过中央服务器交换信息，为任务导出ε-最优策略。我们的研究证明，为了实现所有M个任务的ε-最优策略，使用DistMT-LSVI的单个代理需要运行一定数量的操作。

    Recently, DARPA launched the ShELL program, which aims to explore how experience sharing can benefit distributed lifelong learning agents in adapting to new challenges. In this paper, we address this issue by conducting both theoretical and empirical research on distributed multi-task reinforcement learning (RL), where a group of $N$ agents collaboratively solves $M$ tasks without prior knowledge of their identities. We approach the problem by formulating it as linearly parameterized contextual Markov decision processes (MDPs), where each task is represented by a context that specifies the transition dynamics and rewards. To tackle this problem, we propose an algorithm called DistMT-LSVI. First, the agents identify the tasks, and then they exchange information through a central server to derive $\epsilon$-optimal policies for the tasks. Our research demonstrates that to achieve $\epsilon$-optimal policies for all $M$ tasks, a single agent using DistMT-LSVI needs to run a total number o
    
[^37]: 视图袋：一种基于外观的用于3D重建下一个最佳视图规划的方法

    Bag of Views: An Appearance-based Approach to Next-Best-View Planning for 3D Reconstruction. (arXiv:2307.05832v1 [cs.CV])

    [http://arxiv.org/abs/2307.05832](http://arxiv.org/abs/2307.05832)

    这篇论文提出了一种基于外观的方法，通过视图袋（Bag-of-Views）模型来对捕获的视图进行离线数据集细化和在线下一个最佳视图（NBV）规划应用分配效用，以实现3D重建的任务。同时，还开发了视图规划工具箱（VPT）。

    

    基于无人机的智能数据采集用于3D重建和基础设施监测，由于图像处理和深度学习技术的最新进展，正经历着越来越多的兴趣。视图规划是这个任务的重要部分，它决定了信息捕获策略，并且严重影响从捕获的数据生成的3D模型的质量。最近的方法使用先前的知识或目标的部分重建来实现主动重建的视图规划；前一种方法对于复杂或新识别的目标构成了挑战，而后者计算开销很大。在这项工作中，我们提出了视图袋（BoV），这是一种完全基于外观的模型，用于为离线数据集的细化和在线下一个最佳视图（NBV）规划应用分配效用，以实现3D重建的任务。通过这个工作，我们还开发了视图规划工具箱（VPT），

    UAV-based intelligent data acquisition for 3D reconstruction and monitoring of infrastructure has been experiencing an increasing surge of interest due to the recent advancements in image processing and deep learning-based techniques. View planning is an essential part of this task that dictates the information capture strategy and heavily impacts the quality of the 3D model generated from the captured data. Recent methods have used prior knowledge or partial reconstruction of the target to accomplish view planning for active reconstruction; the former approach poses a challenge for complex or newly identified targets while the latter is computationally expensive. In this work, we present Bag-of-Views (BoV), a fully appearance-based model used to assign utility to the captured views for both offline dataset refinement and online next-best-view (NBV) planning applications targeting the task of 3D reconstruction. With this contribution, we also developed the View Planning Toolbox (VPT), 
    
[^38]: 通过损失函数曲率视角揭示记忆化过程

    Memorization Through the Lens of Curvature of Loss Function Around Samples. (arXiv:2307.05831v1 [cs.LG])

    [http://arxiv.org/abs/2307.05831](http://arxiv.org/abs/2307.05831)

    本研究通过对损失函数曲率进行分析，研究了神经网络在不同样本上的泛化与记忆化特性。我们发现高曲率的样本通常是具有标签错误或冲突的长尾样本，并在CIFAR100数据集上发现了一种新的失败模型。通过对部分样本进行随机标签错误，我们展示了曲率排序可以有效识别出这些样本。

    

    神经网络参数过多，很容易过拟合训练数据。极端情况下，它们可以完全记忆训练集，即使标签是随机的。我们提议使用训练样本周围的损失函数曲率作为记忆化程度的度量，对所有训练轮次进行平均。我们利用这个度量来研究常见图像数据集中不同样本的泛化与记忆化特性。我们可视化具有最高损失曲率的样本，发现它们通常是长尾样本、标签错误或冲突样本。这种分析帮助我们在CIFAR100数据集上发现了一种新的失败模型，即具有不同标签的重复图像。我们还通过随机错误化少量样本的标签来人为地给数据集引入标签错误，并展示了按曲率排序可以高效地识别出标签错误样本的高AUROC值。

    Neural networks are overparametrized and easily overfit the datasets they train on. In the extreme case, it is shown that they can memorize a training set with fully randomized labels. We propose using the curvature of loss function around the training sample as a measure of its memorization, averaged over all training epochs. We use this to study the generalization versus memorization properties of different samples in popular image datasets. We visualize samples with the highest curvature of loss around them, and show that these visually correspond to long-tailed, mislabeled or conflicting samples. This analysis helps us find a, to the best of our knowledge, novel failure model on the CIFAR100 dataset, that of duplicated images with different labels. We also synthetically mislabel a proportion of the dataset by randomly corrupting the labels of a few samples, and show that sorting by curvature yields high AUROC values for identifying the mislabeled samples.
    
[^39]: 使用卷积和记忆网络在维基百科表格上进行关系抽取

    Relational Extraction on Wikipedia Tables using Convolutional and Memory Networks. (arXiv:2307.05827v1 [cs.CL])

    [http://arxiv.org/abs/2307.05827](http://arxiv.org/abs/2307.05827)

    使用卷积和记忆网络，在维基百科的表格数据中进行关系抽取。该模型在关系抽取任务中表现出色，并且经过全面的分析和研究，展示了各个模型组件的贡献。

    

    关系抽取是从文本中提取实体之间关系的任务。大部分关系抽取方法从自由格式的连续文本中提取关系，而忽略了其他丰富的数据来源，比如表格。我们从应用神经网络方法处理表格化数据的角度探索关系抽取。我们引入了一个新模型，由卷积神经网络（CNN）和双向长短期记忆（BiLSTM）网络组成，分别用于编码实体和学习它们之间的依赖关系。我们在一个大规模且最新的数据集上评估了我们的模型，并与之前的神经网络方法进行了比较。实验结果显示，我们的模型在表格数据上的关系抽取任务中始终优于之前的模型。我们进行了全面的错误分析和剥离研究，以展示我们的模型的各个组成部分的贡献。最后，我们讨论了我们方法的实用性和权衡，并提供了进一步研究的建议。

    Relation extraction (RE) is the task of extracting relations between entities in text. Most RE methods extract relations from free-form running text and leave out other rich data sources, such as tables. We explore RE from the perspective of applying neural methods on tabularly organized data. We introduce a new model consisting of Convolutional Neural Network (CNN) and Bidirectional-Long Short Term Memory (BiLSTM) network to encode entities and learn dependencies among them, respectively. We evaluate our model on a large and recent dataset and compare results with previous neural methods. Experimental results show that our model consistently outperforms the previous model for the task of relation extraction on tabular data. We perform comprehensive error analyses and ablation study to show the contribution of various components of our model. Finally, we discuss the usefulness and trade-offs of our approach, and provide suggestions for fostering further research.
    
[^40]: 神经启发的高效地图构建通过分割和回溯

    Neuro-Inspired Efficient Map Building via Fragmentation and Recall. (arXiv:2307.05793v1 [cs.AI])

    [http://arxiv.org/abs/2307.05793](http://arxiv.org/abs/2307.05793)

    本文提出了一种神经启发的地图构建方法，通过分割和回溯来解决大型环境下的探索问题，并基于意外性的空间聚类设置探索子目标。

    

    动物和机器人通过构建和完善空间地图来导航环境。这些地图使得包括回家、规划、搜索和觅食在内的功能成为可能。在大型环境中，探索空间是一个难题：代理可能会陷入局部区域。在这里，我们从神经科学中汲取经验，提出并应用了分割和回溯（FarMap）的概念。代理通过基于意外性的空间聚类来解决地图构建问题，同时将其用于设置空间探索的子目标。代理构建和使用本地地图来预测他们的观测结果；高意外性会导致“分割事件”，从而截断本地地图。在这些事件中，最近的本地地图被放入长期记忆（LTM）中，并初始化另一个本地地图。如果断裂点的观察结果与存储的某个本地地图的观察结果相匹配，那么该地图就会被回溯（并重用）自LTM。分割点诱导.

    Animals and robots navigate through environments by building and refining maps of the space. These maps enable functions including navigating back to home, planning, search, and foraging. In large environments, exploration of the space is a hard problem: agents can become stuck in local regions. Here, we use insights from neuroscience to propose and apply the concept of Fragmentation-and-Recall (FarMap), with agents solving the mapping problem by building local maps via a surprisal-based clustering of space, which they use to set subgoals for spatial exploration. Agents build and use a local map to predict their observations; high surprisal leads to a ``fragmentation event'' that truncates the local map. At these events, the recent local map is placed into long-term memory (LTM), and a different local map is initialized. If observations at a fracture point match observations in one of the stored local maps, that map is recalled (and thus reused) from LTM. The fragmentation points induc
    
[^41]: 在深度神经网络中合并多个输入描述符和监督器用于道路图过滤

    Merging multiple input descriptors and supervisors in a deep neural network for tractogram filtering. (arXiv:2307.05786v1 [cs.CV])

    [http://arxiv.org/abs/2307.05786](http://arxiv.org/abs/2307.05786)

    本文提出了一种在深度神经网络中合并多个输入描述符和监督器的方法，用于过滤道路图数据。作者通过使用四种不同的道路图过滤策略，并将它们的输出进行组合，实现了对道路线的分类。作者还发现，在此分类任务中，道路线的坐标是最重要的信息。

    

    当前道路图方法的主要问题之一是误报率高。道路图过滤是在后处理步骤中从道路图数据中移除误报道路的选项。在本文中，我们训练了一个深度神经网络用于过滤道路图数据，其中道路图的每条道路线被分类为"合理的"、"不合理的"或"不确定的"。为此，我们使用了四种不同的道路图过滤策略作为监督器：TractQuerier、RecobundlesX、TractSeg和一种基于解剖学的过滤器。它们的输出被组合起来以获得道路线的分类标签。我们评估了沿道路线执行此分类任务所需的不同类型的信息的重要性，包括道路线的坐标、扩散数据、关键点、T1加权信息和脑区分割。我们发现在这种特定情况下，道路线的坐标是最相关的，其次是扩散数据。

    One of the main issues of the current tractography methods is their high false-positive rate. Tractogram filtering is an option to remove false-positive streamlines from tractography data in a post-processing step. In this paper, we train a deep neural network for filtering tractography data in which every streamline of a tractogram is classified as {\em plausible, implausible}, or {\em inconclusive}. For this, we use four different tractogram filtering strategies as supervisors: TractQuerier, RecobundlesX, TractSeg, and an anatomy-inspired filter. Their outputs are combined to obtain the classification labels for the streamlines. We assessed the importance of different types of information along the streamlines for performing this classification task, including the coordinates of the streamlines, diffusion data, landmarks, T1-weighted information, and a brain parcellation. We found that the streamline coordinates are the most relevant followed by the diffusion data in this particular 
    
[^42]: EgoAdapt：适应真实世界自我中心用户视频的多流评估研究

    EgoAdapt: A multi-stream evaluation study of adaptation to real-world egocentric user video. (arXiv:2307.05784v1 [cs.CV])

    [http://arxiv.org/abs/2307.05784](http://arxiv.org/abs/2307.05784)

    EgoAdapt是一个用于真实世界自我中心动作识别的基准，通过引入自适应范例并解决流数据中的分布转移挑战，可以使模型根据用户的实际需求进行自适应。

    

    在自我中心动作识别中，通常会训练一个单一的种群模型，并在头戴设备（如增强现实头盔）上进行实验。然而，这个模型对于新用户和环境来说是静态的，我们引入了一个自适应的两阶段范例，即在预训练种群模型后，模型根据用户的经验进行设备内和在线的适应。这个设置非常具有挑战性，因为它从种群领域转变为用户领域，并且用户数据流中存在分布的转变。应对后者中在流量中的分布转移是持续学习的重点，而持续学习的进展主要是在受控基准上的，然而在真实世界的应用中面临的挑战往往没有得到解决。我们引入了EgoAdapt，这是一个用于真实世界自我中心动作识别的基准，可以促进我们的两阶段自适应范例，并且自我中心视频流中自然发生的真实世界挑战，如长尾动作分布。

    In egocentric action recognition a single population model is typically trained and subsequently embodied on a head-mounted device, such as an augmented reality headset. While this model remains static for new users and environments, we introduce an adaptive paradigm of two phases, where after pretraining a population model, the model adapts on-device and online to the user's experience. This setting is highly challenging due to the change from population to user domain and the distribution shifts in the user's data stream. Coping with the latter in-stream distribution shifts is the focus of continual learning, where progress has been rooted in controlled benchmarks but challenges faced in real-world applications often remain unaddressed. We introduce EgoAdapt, a benchmark for real-world egocentric action recognition that facilitates our two-phased adaptive paradigm, and real-world challenges naturally occur in the egocentric video streams from Ego4d, such as long-tailed action distrib
    
[^43]: Rad-ReStruct: 一种新颖的结构化放射学报告的VQA基准和方法

    Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting. (arXiv:2307.05766v1 [cs.CV])

    [http://arxiv.org/abs/2307.05766](http://arxiv.org/abs/2307.05766)

    本文提出了Rad-ReStruct，一个用于评估和比较不同方法的新型基准数据集，以X光图像的结构化报告形式提供了细粒度、按层次排序的注释。我们提出了一种新方法hi-VQA，将结构化报告任务建模为分层视觉问答(VQA)，并考虑先前提问和回答的上下文来填充结构化放射学报告。实验证明hi-VQA取得了与最先进方法相竞争的性能。

    

    放射学报告是放射科医生与其他医务人员之间沟通的重要部分，但其可能耗时且容易出错。其中一种减轻这种情况的方法是结构化报告，它比自由文本报告更节约时间并且能够实现更精确的评估。然而，关于自动化结构化报告的研究有限，并且目前没有公开的基准用于评估和比较不同方法。为了弥补这一空白，我们介绍了Rad-ReStruct，这是一个新的基准数据集，提供了细粒度的、按层次排序的X光图像的结构化报告形式的注释。我们将结构化报告任务建模为分层视觉问答(VQA)，并提出了hi-VQA，一种考虑先前提问和回答的上下文以填充结构化放射学报告的新方法。我们的实验证明hi-VQA在医学VQA基准测试中取得了与最先进方法相竞争的性能。

    Radiology reporting is a crucial part of the communication between radiologists and other medical professionals, but it can be time-consuming and error-prone. One approach to alleviate this is structured reporting, which saves time and enables a more accurate evaluation than free-text reports. However, there is limited research on automating structured reporting, and no public benchmark is available for evaluating and comparing different methods. To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that provides fine-grained, hierarchically ordered annotations in the form of structured reports for X-Ray images. We model the structured reporting task as hierarchical visual question answering (VQA) and propose hi-VQA, a novel method that considers prior context in the form of previously asked questions and answers for populating a structured radiology report. Our experiments show that hi-VQA achieves competitive performance to the state-of-the-art on the medical VQA benc
    
[^44]: 将课程与回放相结合：对持续学习的影响

    Integrating Curricula with Replays: Its Effects on Continual Learning. (arXiv:2307.05747v1 [cs.LG])

    [http://arxiv.org/abs/2307.05747](http://arxiv.org/abs/2307.05747)

    将课程与回放方法相结合可以提高持续学习的效果，通过调整回放实例与训练数据的交替频率、回放实例的顺序以及选择进入回放缓冲区的策略实现。

    

    人类在获取新技能或知识时，通过课程进行学习和复习。这种人类学习行为启发了在持续学习代理中将课程与回放方法相结合。目标是模拟人类学习过程，从而提高知识保留和促进学习转移。现有的持续学习代理中的回放方法涉及从先前任务中随机选择和排序数据，已经证明是有效的。然而，有限的研究探讨了将不同课程与回放方法相结合以增强持续学习的问题。我们的研究首次考察了将课程与回放方法相结合对持续学习的影响的三个具体方面：回放实例与训练数据的交替频率，回放实例的顺序，以及选择实例进入回放缓冲区的策略。这些课程设计的方面

    Humans engage in learning and reviewing processes with curricula when acquiring new skills or knowledge. This human learning behavior has inspired the integration of curricula with replay methods in continual learning agents. The goal is to emulate the human learning process, thereby improving knowledge retention and facilitating learning transfer. Existing replay methods in continual learning agents involve the random selection and ordering of data from previous tasks, which has shown to be effective. However, limited research has explored the integration of different curricula with replay methods to enhance continual learning. Our study takes initial steps in examining the impact of integrating curricula with replay methods on continual learning in three specific aspects: the interleaved frequency of replayed exemplars with training data, the sequence in which exemplars are replayed, and the strategy for selecting exemplars into the replay buffer. These aspects of curricula design al
    
[^45]: 面向组合分类中多组公平性改善的可扩展解决方案

    Towards A Scalable Solution for Improving Multi-Group Fairness in Compositional Classification. (arXiv:2307.05728v1 [cs.LG])

    [http://arxiv.org/abs/2307.05728](http://arxiv.org/abs/2307.05728)

    本论文提出面向组合分类中多组公平性改善的可扩展解决方案。通过引入任务过条件化和组间交替的技术，在多组多标签设定下实现了恒定比例缩放，并在学术和实际环境中的实验中证明了其有效性。

    

    尽管机器学习公平性方面的研究文献丰富，但对于修复复杂系统（其中最终预测是多个分类器的组合，存在多个组）的关注相对较少。本文首先展示了用于改善机会平等公平性的自然基线方法与被修复组数和被修复预测标签数的乘积呈线性关系，使其不实用。然后，我们引入了两种简单的技术，称为“任务过条件化”和“组间交替”，以在多组多标签设定中实现恒定比例缩放。我们在学术和实际环境中的实验结果证明了我们的方案在这个环境中的有效性。

    Despite the rich literature on machine learning fairness, relatively little attention has been paid to remediating complex systems, where the final prediction is the combination of multiple classifiers and where multiple groups are present. In this paper, we first show that natural baseline approaches for improving equal opportunity fairness scale linearly with the product of the number of remediated groups and the number of remediated prediction labels, rendering them impractical. We then introduce two simple techniques, called {\em task-overconditioning} and {\em group-interleaving}, to achieve a constant scaling in this multi-group multi-label setup. Our experimental results in academic and real-world environments demonstrate the effectiveness of our proposal at mitigation within this environment.
    
[^46]: 一个开源的知识图谱生态系统用于生命科学

    An Open-Source Knowledge Graph Ecosystem for the Life Sciences. (arXiv:2307.05727v1 [cs.AI])

    [http://arxiv.org/abs/2307.05727](http://arxiv.org/abs/2307.05727)

    PheKnowLator是一个开源的知识图谱生态系统，用于自动化构建可定制的FAIR本体化基础的知识图谱，以解决生命科学中的整合挑战。

    

    转化研究需要多个生物组织尺度上的数据。测序和多组学技术的进步增加了这些数据的可用性，但研究人员面临着重大的整合挑战。知识图谱（KGs）用于建模复杂现象，已经存在自动构建它们的方法。然而，解决复杂的生物医学整合问题需要在知识建模方式上灵活性。此外，现有的KG构建方法在提供强大工具的同时，也会限制在知识表示模型中固定或有限的选择。PheKnowLator（Phenotype Knowledge Translator）是一个语义生态系统，用于自动化具有完全可定制的知识表示的FAIR（可找到，可访问，可互操作，可重复使用）本体化基础的知识图谱的构建。该生态系统包括KG构建资源（例如，数据准备API），分析工具（例如，SPARQL端点和抽象算法），还有

    Translational research requires data at multiple scales of biological organization. Advancements in sequencing and multi-omics technologies have increased the availability of these data but researchers face significant integration challenges. Knowledge graphs (KGs) are used to model complex phenomena, and methods exist to automatically construct them. However, tackling complex biomedical integration problems requires flexibility in the way knowledge is modeled. Moreover, existing KG construction methods provide robust tooling at the cost of fixed or limited choices among knowledge representation models. PheKnowLator (Phenotype Knowledge Translator) is a semantic ecosystem for automating the FAIR (Findable, Accessible, Interoperable, and Reusable) construction of ontologically grounded KGs with fully customizable knowledge representation. The ecosystem includes KG construction resources (e.g., data preparation APIs), analysis tools (e.g., SPARQL endpoints and abstraction algorithms), an
    
[^47]: 探索大规模语言模型在在线职位推荐中对图数据的理解

    Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations. (arXiv:2307.05722v1 [cs.AI])

    [http://arxiv.org/abs/2307.05722](http://arxiv.org/abs/2307.05722)

    本论文探索了大规模语言模型在在线职位推荐中对图数据的理解能力，并提出了新的框架来分析行为图，发现其中的潜在模式和关系。

    

    大规模语言模型（LLMs）在各个领域展示了其出色的能力，彻底改变了自然语言处理任务。然而，它们在职位推荐中对行为图的理解潜力仍然未被充分探索。本文旨在揭示大规模语言模型在理解行为图方面的能力，并利用这种理解来提升在线招聘中的推荐，包括促进非分布式的应用。我们提出了一个新的框架，利用大规模语言模型提供的丰富上下文信息和语义表示来分析行为图并揭示其中的潜在模式和关系。具体而言，我们提出了一个元路径提示构造器，利用LLM推荐器首次理解行为图，并设计了相应的路径增强模块来缓解基于路径的序列输入引入的提示偏差。通过利用将LM的特点引入到行为图的大规模数据分析中，我们取得了显著的实验结果，证明了我们提出的方法的有效性和性能。

    Large Language Models (LLMs) have revolutionized natural language processing tasks, demonstrating their exceptional capabilities in various domains. However, their potential for behavior graph understanding in job recommendations remains largely unexplored. This paper focuses on unveiling the capability of large language models in understanding behavior graphs and leveraging this understanding to enhance recommendations in online recruitment, including the promotion of out-of-distribution (OOD) application. We present a novel framework that harnesses the rich contextual information and semantic representations provided by large language models to analyze behavior graphs and uncover underlying patterns and relationships. Specifically, we propose a meta-path prompt constructor that leverages LLM recommender to understand behavior graphs for the first time and design a corresponding path augmentation module to alleviate the prompt bias introduced by path-based sequence input. By leveragin
    
[^48]: 无监督表示学习的因果排序先验方法

    A Causal Ordering Prior for Unsupervised Representation Learning. (arXiv:2307.05704v1 [cs.LG])

    [http://arxiv.org/abs/2307.05704](http://arxiv.org/abs/2307.05704)

    该论文提出了一种无监督表示学习的方法，通过考虑具有潜在加性噪声模型的数据生成过程，以及基于潜在分布的海森矩阵的损失函数，鼓励潜在空间遵循因果排序。

    

    无监督表示学习依赖于对潜变量的独立性假设。然而，因果表示学习（CRL）认为数据集中的变异因素实际上是因果相关的。允许潜变量由于因果关系而相关性更加真实和可泛化。到目前为止，可证明可识别的方法依赖于：辅助信息、弱标签，以及干预或甚至对照数据。受到功能因果模型的因果发现的启发，我们提出了一种完全无监督的表示学习方法，考虑了具有潜在加性噪声模型（ANM）的数据生成过程。通过基于潜在分布的海森矩阵的损失函数，我们鼓励潜在空间遵循因果排序。

    Unsupervised representation learning with variational inference relies heavily on independence assumptions over latent variables. Causal representation learning (CRL), however, argues that factors of variation in a dataset are, in fact, causally related. Allowing latent variables to be correlated, as a consequence of causal relationships, is more realistic and generalisable. So far, provably identifiable methods rely on: auxiliary information, weak labels, and interventional or even counterfactual data. Inspired by causal discovery with functional causal models, we propose a fully unsupervised representation learning method that considers a data generation process with a latent additive noise model (ANM). We encourage the latent space to follow a causal ordering via loss function based on the Hessian of the latent distribution.
    
[^49]: 个性化强化学习总结服务：从非结构化数据中学习结构

    A Personalized Reinforcement Learning Summarization Service for Learning Structure from Unstructured Data. (arXiv:2307.05696v1 [cs.IR])

    [http://arxiv.org/abs/2307.05696](http://arxiv.org/abs/2307.05696)

    该论文提出了一种个性化强化学习总结服务，通过使用层级个性化基于概念的总结方法，在文本数据呈指数级增长的背景下，帮助用户提取有意义的见解。

    

    文本数据呈指数级增长，需要工具来帮助用户提取有意义的见解。传统的文档摘要方法通常无法满足个人用户需求，并且缺乏高效信息处理的结构。为解决这些问题，我们提出了一种层级个性化基于概念的总结方法。该方法将文档综合成简洁的层级概念图，并通过学习和适应用户偏好来积极参与用户。使用强化学习算法，该方法为特定主题的未见文档生成个性化摘要。该框架提高了理解能力，实现了有效的导航，并使用户能够根据自己独特的需求从大量文档集合中提取有意义的见解。

    The exponential growth of textual data has created a crucial need for tools that assist users in extracting meaningful insights. Traditional document summarization approaches often fail to meet individual user requirements and lack structure for efficient information processing. To address these limitations, we propose Summation, a hierarchical personalized concept-based summarization approach. It synthesizes documents into a concise hierarchical concept map and actively engages users by learning and adapting to their preferences. Using a Reinforcement Learning algorithm, Summation generates personalized summaries for unseen documents on specific topics. This framework enhances comprehension, enables effective navigation, and empowers users to extract meaningful insights from large document collections aligned with their unique requirements.
    
[^50]: Objaverse-XL：一个包含1千万+ 3D对象的无限世界

    Objaverse-XL: A Universe of 10M+ 3D Objects. (arXiv:2307.05663v1 [cs.CV])

    [http://arxiv.org/abs/2307.05663](http://arxiv.org/abs/2307.05663)

    Objaverse-XL是一个包含1千万+ 3D对象的无限世界的数据集，它解决了3D视觉任务中获取高质量数据的挑战，并通过提供大规模和多样性改进了3D视觉能力。

    

    自然语言处理和2D视觉模型通过扩大训练数据规模，在许多任务上取得了显著的成绩。然而，3D视觉任务并没有见到同样的进展，部分原因是获取高质量的3D数据的挑战。在这项工作中，我们介绍了Objaverse-XL，一个包含超过1000万个3D对象的数据集。我们的数据集包括来自多种来源的去重复的3D对象，包括手动设计的对象，地标和日常物品的摄影测量扫描，以及历史和古董文物的专业扫描。Objaverse-XL代表了3D数据集领域中最大规模和多样性，为3D视觉带来了重大的新可能性。我们的实验证明了Objaverse-XL提供的规模带来的改进。我们展示了通过在新视角合成中训练Zero123，利用超过1亿个多视角渲染图像，我们实现了强大的零样本泛化能力。

    Natural language processing and 2D vision models have attained remarkable proficiency on many tasks primarily by escalating the scale of training data. However, 3D vision tasks have not seen the same progress, in part due to the challenges of acquiring high-quality 3D data. In this work, we present Objaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises deduplicated 3D objects from a diverse set of sources, including manually designed objects, photogrammetry scans of landmarks and everyday items, and professional scans of historic and antique artifacts. Representing the largest scale and diversity in the realm of 3D datasets, Objaverse-XL enables significant new possibilities for 3D vision. Our experiments demonstrate the improvements enabled with the scale provided by Objaverse-XL. We show that by training Zero123 on novel view synthesis, utilizing over 100 million multi-view rendered images, we achieve strong zero-shot generalization abilities. We hope that relea
    
[^51]: 使用基于Transformer的深度强化学习进行多目标水电站水库调度优化

    Multiobjective Hydropower Reservoir Operation Optimization with Transformer-Based Deep Reinforcement Learning. (arXiv:2307.05643v1 [cs.LG])

    [http://arxiv.org/abs/2307.05643](http://arxiv.org/abs/2307.05643)

    提出了一种使用基于Transformer的深度强化学习方法来优化多目标水电站水库调度问题。实验结果表明，该方法相较于最先进的方法具有更好的运行效果。

    

    由于水资源短缺和水需求增加，多水库系统的联合运行以平衡发电、生态保护和居民用水供应已成为水电管理中的关键问题。然而，多个水库的众多约束和非线性性使得解决这个问题非常耗时。为了应对这一挑战，提出了一种融合Transformer框架的深度强化学习方法。编码器的多头注意力机制有效地从水库和居民区提取信息，解码器的多水库注意网络生成适当的运行决策。将该方法应用于科罗拉多河流域的梅德湖和鲍威尔湖。实验结果表明，基于Transformer的深度强化学习方法能够产生合适的运营结果。与最先进的方法相比，该方法的运行效果更好。

    Due to shortage of water resources and increasing water demands, the joint operation of multireservoir systems for balancing power generation, ecological protection, and the residential water supply has become a critical issue in hydropower management. However, the numerous constraints and nonlinearity of multiple reservoirs make solving this problem time-consuming. To address this challenge, a deep reinforcement learning approach that incorporates a transformer framework is proposed. The multihead attention mechanism of the encoder effectively extracts information from reservoirs and residential areas, and the multireservoir attention network of the decoder generates suitable operational decisions. The proposed method is applied to Lake Mead and Lake Powell in the Colorado River Basin. The experimental results demonstrate that the transformer-based deep reinforcement learning approach can produce appropriate operational outcomes. Compared to a state-of-the-art method, the operation st
    
[^52]: 使用高斯径向基函数神经网络学习活跃子空间并发现重要特征

    Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks. (arXiv:2307.05639v1 [cs.LG])

    [http://arxiv.org/abs/2307.05639](http://arxiv.org/abs/2307.05639)

    本论文提出了一种修改的径向基函数神经网络模型，通过学习精度矩阵，从训练完成后的模型中提取有用信息，包括活跃子空间的方向和输入变量重要性的排序。

    

    提供一个既能达到强大预测性能，又能被人类解释的模型是机器学习研究中最困难的挑战之一，由于这两个目标的冲突性。为解决这个挑战，我们提出了一种修改的径向基函数神经网络模型，通过为其高斯核添加可学习的精度矩阵。我们展示了训练完成后可以从精度矩阵的谱中提取宝贵的信息。特别是，特征向量解释了模型最敏感的方向，揭示了活跃子空间，并提出了用于监督降维的潜在应用。同时，特征向量凸显了输入和潜在变量之间的绝对变化关系，从而使我们能够基于其对预测的重要性提取输入变量的排序。

    Providing a model that achieves a strong predictive performance and at the same time is interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives. To address this challenge, we propose a modification of the Radial Basis Function Neural Network model by equipping its Gaussian kernel with a learnable precision matrix. We show that precious information is contained in the spectrum of the precision matrix that can be extracted once the training of the model is completed. In particular, the eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace and suggesting potential applications for supervised dimensionality reduction. At the same time, the eigenvectors highlight the relationship in terms of absolute variation between the input and the latent variables, thereby allowing us to extract a ranking of the input variables based on their importance to the predi
    
[^53]: 深度迁移学习在工业时间序列异常检测中的综合调查：方法、应用和方向

    A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions. (arXiv:2307.05638v1 [cs.LG])

    [http://arxiv.org/abs/2307.05638](http://arxiv.org/abs/2307.05638)

    本综述调查了深度迁移学习在工业时间序列异常检测中的使用。深度迁移学习通过利用相关任务的知识和考虑数据分布的变化，解决了仅有少量或没有附加标记数据情况下的新任务。

    

    自动化监测工业过程有潜力通过及时检测异常事件并促进及时干预来提高效率和优化质量。深度学习通过识别大数据集中的非平凡模式，在这一过程中发挥着关键作用。标准的深度学习方法适用于解决特定类型的数据给定特定任务的问题。在训练过程中，这些算法需要大量的标记训练数据。然而，由于工艺和环境的动态性，为每个稍有不同的情况重新获得所需数据进行标准深度学习训练是不现实的。深度迁移学习提供了解决这个问题的方法。通过利用相关任务的知识和考虑数据分布的变化，这个学习框架可以解决新任务，即使没有或只有很少的附加标记数据。这种方法避免了从头开始重新训练模型的需要。

    Automating the monitoring of industrial processes has the potential to enhance efficiency and optimize quality by promptly detecting abnormal events and thus facilitating timely interventions. Deep learning, with its capacity to discern non-trivial patterns within large datasets, plays a pivotal role in this process. Standard deep learning methods are suitable to solve a specific task given a specific type of data. During training, the algorithms demand large volumes of labeled training data. However, due to the dynamic nature of processes and the environment, it is impractical to acquire the needed data for standard deep learning training for every slightly different case anew. Deep transfer learning offers a solution to this problem. By leveraging knowledge from related tasks and accounting for variations in data distributions, this learning framework solves new tasks even with little or no additional labeled data. The approach bypasses the need to retrain a model from scratch for ev
    
[^54]: 从概率论到置信修正

    Belief Revision from Probability. (arXiv:2307.05632v1 [cs.AI])

    [http://arxiv.org/abs/2307.05632](http://arxiv.org/abs/2307.05632)

    该论文通过使用相对问题、概率的信念模型，探讨了信念动态的影响。研究发现，该模型所验证的原则比传统的信念修正理论要弱，但比与高主观概率等同的Lockean信念理论要强。同时，论文还提出了一些适用于特定类别模型的自然原则，并与其他概率信念模型进行了比较。

    

    在之前的工作中，我们使用“来自概率的知识”（TARK 2021）开发了一个相对问题、概率的信念模型。根据这个模型，相对于给定问题，一个人相信的内容应该（i）在蕴涵运算下保持稳定，（ii）在他们的证据下具有足够的概率，以及（iii）对问题的答案的相对概率敏感。在这里，我们探讨了这个模型对信念动态的影响。我们证明了它所验证的原则要比AGM等传统信念修正理论的原则要弱，但仍然比与高主观概率等同的流行的Lockean信念理论要强。然后，我们考虑了一类适用于许多但不适用于所有应用的模型，并且确定了在这一类模型上还有效的一些自然原则。最后，我们通过比较Leitgeb和Lin和的概率信念模型，得出了这个现有框架具有比较优势的结论。

    In previous work ("Knowledge from Probability", TARK 2021) we develop a question-relative, probabilistic account of belief. On this account, what someone believes relative to a given question is (i) closed under entailment, (ii) sufficiently probable given their evidence, and (iii) sensitive to the relative probabilities of the answers to the question. Here we explore the implications of this account for the dynamics of belief. We show that the principles it validates are much weaker than those of orthodox theories of belief revision like AGM, but still stronger than those valid according to the popular Lockean theory of belief, which equates belief with high subjective probability. We then consider a restricted class of models, suitable for many but not all applications, and identify some further natural principles valid on this class. We conclude by arguing that the present framework compares favorably to the rival probabilistic accounts of belief developed by Leitgeb and by Lin and 
    
[^55]: 因果克里普克模型

    Causal Kripke Models. (arXiv:2307.05631v1 [cs.AI])

    [http://arxiv.org/abs/2307.05631](http://arxiv.org/abs/2307.05631)

    本研究扩展了Halpern和Pearl的实际因果模型，引入了一个带有模态运算符的因果逻辑，可以推理涉及多种可能性、时间性、知识和不确定性的因果关系。

    

    本研究将Halpern和Pearl的实际因果模型扩展到可能世界的语义环境中。利用这个框架，我们引入了一个带有模态运算符的实际因果逻辑，可以推理涉及多种可能性、时间性、知识和不确定性的因果关系。我们通过一些例子来说明这一点，并讨论了一些以后的研究方向。

    This work extends Halpern and Pearl's causal models for actual causality to a possible world semantics environment. Using this framework we introduce a logic of actual causality with modal operators, which allows for reasoning about causality in scenarios involving multiple possibilities, temporality, knowledge and uncertainty. We illustrate this with a number of examples, and conclude by discussing some future directions for research.
    
[^56]: AGM信念收缩的条件表征

    Characterization of AGM Belief Contraction in Terms of Conditionals. (arXiv:2307.05629v1 [cs.AI])

    [http://arxiv.org/abs/2307.05629](http://arxiv.org/abs/2307.05629)

    该论文通过使用Kripke信念关系和Stalnaker-Lewis选择函数的框架，对AGM信念收缩进行了语义表征，其中关键点是初始信念集合收缩后的成员满足在实际状态下的相信和条件判断关系。

    

    我们基于由Kripke信念关系和Stalnaker-Lewis选择函数组成的框架，提供了对AGM信念收缩的语义表征。核心思想是：设K为初始信念集合，K-A为通过公式A收缩K得到的集合；则B属于集合K-A当且仅当，在实际状态下，代理人相信B，并且相信如果非A是（将要）成立，则B是（将要）成立。

    We provide a semantic characterization of AGM belief contraction based on frames consisting of a Kripke belief relation and a Stalnaker-Lewis selection function. The central idea is as follows. Let K be the initial belief set and K-A be the contraction of K by the formula A; then B belongs to the set K-A if and only if, at the actual state, the agent believes B and believes that if not-A is (were) the case then B is (would be) the case.
    
[^57]: CILF:因果启发学习框架用于超出分布的车辆轨迹预测

    CILF:Causality Inspired Learning Framework for Out-of-Distribution Vehicle Trajectory Prediction. (arXiv:2307.05624v1 [cs.LG])

    [http://arxiv.org/abs/2307.05624](http://arxiv.org/abs/2307.05624)

    提出了一种因果启发学习框架（CILF），通过明确定义数据的潜在因果结构，并利用因果特征进行预测来解决轨迹预测中超出分布数据的问题。

    

    轨迹预测对于自动驾驶车辆至关重要。现有的大多数方法倾向于建模历史轨迹（输入）与未来轨迹（输出）之间的相关性。由于相关性只是对现实的一种表面描述，这些方法在很大程度上依赖于独立同分布的假设，并对超出分布的数据表现出高度的敏感性。为了解决这个问题，我们提出了一个超出分布因果图（OOD-CG），它明确地定义了数据的潜在因果结构，包括三个纠缠的潜在特征：1）领域不变的因果特征（IC），2）领域变量的因果特征（VC），3）领域变量的非因果特征（VN）。这些特征受到混淆因子（C）和领域选择器（D）的影响。为了利用因果特征进行预测，我们提出了一个因果启发学习框架（CILF），它包括三个步骤：1）通过不变性损失提取领域不变的因果特征，2）通过因果性损失提取领域变量特征

    Trajectory prediction is critical for autonomous driving vehicles. Most existing methods tend to model the correlation between history trajectory (input) and future trajectory (output). Since correlation is just a superficial description of reality, these methods rely heavily on the i.i.d. assumption and evince a heightened susceptibility to out-of-distribution data. To address this problem, we propose an Out-of- Distribution Causal Graph (OOD-CG), which explicitly defines the underlying causal structure of the data with three entangled latent features: 1) domain-invariant causal feature (IC), 2) domain-variant causal feature (VC), and 3) domain-variant non-causal feature (VN ). While these features are confounded by confounder (C) and domain selector (D). To leverage causal features for prediction, we propose a Causal Inspired Learning Framework (CILF), which includes three steps: 1) extracting domain-invariant causal feature by means of an invariance loss, 2) extracting domain varian
    
[^58]: 用于动态估计出发地-目的地序列的深度学习框架

    A DeepLearning Framework for Dynamic Estimation of Origin-Destination Sequence. (arXiv:2307.05623v1 [cs.LG])

    [http://arxiv.org/abs/2307.05623](http://arxiv.org/abs/2307.05623)

    本文提出了一个综合方法，使用深度学习方法推断OD序列的结构，并使用结构约束指导传统的数值优化，解决了交通领域中静态和动态OD矩阵估计中的欠定和滞后挑战。

    

    OD矩阵估计是交通领域的一个关键问题。主要方法使用交通传感器测量信息（如交通流量）来估计由OD矩阵表示的交通需求。该问题分为静态OD矩阵估计和动态OD矩阵序列（简称OD序列）估计两类。上述两种方法面临由于大量估计参数和不足的约束信息造成的欠定问题。此外，OD序列估计还面临滞后挑战：由于拥堵等不同交通条件，相同的车辆在同一观测时段内会出现在不同的路段上，导致相同的OD需求对应不同的行程。为此，本文提出了一种综合方法，它使用深度学习方法推断OD序列的结构，并使用结构约束指导传统的数值优化。我们的实验显示...

    OD matrix estimation is a critical problem in the transportation domain. The principle method uses the traffic sensor measured information such as traffic counts to estimate the traffic demand represented by the OD matrix. The problem is divided into two categories: static OD matrix estimation and dynamic OD matrices sequence(OD sequence for short) estimation. The above two face the underdetermination problem caused by abundant estimated parameters and insufficient constraint information. In addition, OD sequence estimation also faces the lag challenge: due to different traffic conditions such as congestion, identical vehicle will appear on different road sections during the same observation period, resulting in identical OD demands correspond to different trips. To this end, this paper proposes an integrated method, which uses deep learning methods to infer the structure of OD sequence and uses structural constraints to guide traditional numerical optimization. Our experiments show th
    
[^59]: 特征编码对恶意软件分类解释性的影响

    Impact of Feature Encoding on Malware Classification Explainability. (arXiv:2307.05614v1 [cs.LG])

    [http://arxiv.org/abs/2307.05614](http://arxiv.org/abs/2307.05614)

    本文研究了特征编码技术对可解释的人工智能算法的影响。使用恶意软件分类数据集，比较了标签编码和独热编码的性能。发现虽然独热编码会带来轻微性能损失，但可以提供更详细的解释，帮助更全面地理解。使用独热编码还可以减小解释文件大小，缩短人工分析时间。这些结果强调了在XAI研究中重视特征编码技术的重要性，并提出了进一步探索的可能性。

    

    本文研究特征编码技术对可解释的人工智能算法（XAI）的解释性的影响。使用一个恶意软件分类数据集，我们训练了一个XGBoost模型，并比较了两种特征编码方法：标签编码（LE）和独热编码（OHE）的性能。我们的发现表明，使用OHE相比LE会带来轻微的性能损失。然而，OHE所提供的更详细的解释弥补了这种损失。我们观察到OHE能够更详细地探索全局和局部上的细节，提供更全面的答案。此外，我们还观察到使用OHE会导致较小的解释文件和减少人工分析的时间。这些发现强调了在XAI研究中考虑特征编码技术的重要性，并建议通过结合其他编码方法和创新的可视化方法进行进一步探索。

    This paper investigates the impact of feature encoding techniques on the explainability of XAI (Explainable Artificial Intelligence) algorithms. Using a malware classification dataset, we trained an XGBoost model and compared the performance of two feature encoding methods: Label Encoding (LE) and One Hot Encoding (OHE). Our findings reveal a marginal performance loss when using OHE instead of LE. However, the more detailed explanations provided by OHE compensated for this loss. We observed that OHE enables deeper exploration of details in both global and local contexts, facilitating more comprehensive answers. Additionally, we observed that using OHE resulted in smaller explanation files and reduced analysis time for human analysts. These findings emphasize the significance of considering feature encoding techniques in XAI research and suggest potential for further exploration by incorporating additional encoding methods and innovative visualization approaches.
    
[^60]: 物质还是风格：你的图像嵌入知道什么？

    Substance or Style: What Does Your Image Embedding Know?. (arXiv:2307.05610v1 [cs.LG])

    [http://arxiv.org/abs/2307.05610](http://arxiv.org/abs/2307.05610)

    本文研究了图像嵌入中的非语义信息，设计了一个系统的转换预测任务，并发现六个嵌入可以识别出多个转换。这对于训练算法和基础模型的应用具有重要意义。

    

    探针是一种从嵌入中预测底层数据属性的小型网络，它们提供了一种有针对性和有效的方法来揭示嵌入中包含的信息。虽然通过使用探针进行分析在自然语言处理中已经很常见了，但在计算机视觉领域中的探索却比较少。图像基础模型主要用于评估语义内容。更好地理解流行嵌入（如MAE，SimCLR或CLIP）中的非语义信息，将为训练算法和这些基础模型的用途提供新的视角。我们设计了一个系统的转换预测任务，并在多个维度上测量嵌入的视觉内容，包括图像风格、质量以及各种自然和人工转换。令人惊讶的是，有六个嵌入（包括SimCLR）编码了足够的非语义信息，可以识别出数十个转换。我们还考虑了一个泛化任务，将相似的转换分组，并留出一部分数据进行评估。

    Probes are small networks that predict properties of underlying data from embeddings, and they provide a targeted, effective way to illuminate the information contained in embeddings. While analysis through the use of probes has become standard in NLP, there has been much less exploration in vision. Image foundation models have primarily been evaluated for semantic content. Better understanding the non-semantic information in popular embeddings (e.g., MAE, SimCLR, or CLIP) will shed new light both on the training algorithms and on the uses for these foundation models. We design a systematic transformation prediction task and measure the visual content of embeddings along many axes, including image style, quality, and a range of natural and artificial transformations. Surprisingly, six embeddings (including SimCLR) encode enough non-semantic information to identify dozens of transformations. We also consider a generalization task, where we group similar transformations and hold out seve
    
[^61]: 使用帧级别查询的主动学习视频分类

    Active Learning for Video Classification with Frame Level Queries. (arXiv:2307.05587v1 [cs.CV])

    [http://arxiv.org/abs/2307.05587](http://arxiv.org/abs/2307.05587)

    本论文提出了一个新颖的主动学习框架，用于视频分类中的标注减负。这个框架通过自动识别最具信息量的样本来减少人工标注工作量和训练时间。

    

    深度学习算法推动了计算机视觉研究的边界，并在各种应用中展现出了令人瞩目的性能。然而，训练一个强大的深度神经网络需要大量标记的训练数据，获取这些数据需要大量时间和人力。这个问题在视频分类等应用中更为严重，因为人工标注者必须完整地观看整个视频以提供标签。主动学习算法可以自动识别大量未标记数据中最具信息量的样本，这极大地减少了人工注释的工作量，只需要手动标记算法识别出的少数样本。本文提出了一个新颖的主动学习框架用于视频分类，旨在进一步减轻人工标注者的工作负担。我们的框架识别一批图像样本视频，

    Deep learning algorithms have pushed the boundaries of computer vision research and have depicted commendable performance in a variety of applications. However, training a robust deep neural network necessitates a large amount of labeled training data, acquiring which involves significant time and human effort. This problem is even more serious for an application like video classification, where a human annotator has to watch an entire video end-to-end to furnish a label. Active learning algorithms automatically identify the most informative samples from large amounts of unlabeled data; this tremendously reduces the human annotation effort in inducing a machine learning model, as only the few samples that are identified by the algorithm, need to be labeled manually. In this paper, we propose a novel active learning framework for video classification, with the goal of further reducing the labeling onus on the human annotators. Our framework identifies a batch of exemplar videos, togethe
    
[^62]: 使用模型驱动工程和SysML进行机器学习的代码生成

    Code Generation for Machine Learning using Model-Driven Engineering and SysML. (arXiv:2307.05584v1 [cs.SE])

    [http://arxiv.org/abs/2307.05584](http://arxiv.org/abs/2307.05584)

    本研究通过将模型转换与SysML集成，致力于实现数据驱动工程的代码生成，以提高工程系统的可扩展性和可维护性。

    

    数据驱动工程指的是使用机器学习来改进工程系统的系统性数据收集和处理。目前，数据驱动工程的实现依赖于基本的数据科学和软件工程技能。同时，基于模型的工程正变得越来越重要，用于复杂系统的工程。在先前的工作中，提出了一种基于模型的工程方法，通过集成通用建模语言SysML来形式化机器学习任务。然而，形式化的机器学习任务仍然需要在像Python这样的专门的编程语言中实现。因此，本研究旨在通过扩展以往的形式化机器学习任务的工作，通过集成模型转换来生成可执行代码，从而促进实际应用中的数据驱动工程的实现。该方法侧重于模型转换的可修改性和可维护性，以便进行扩展。

    Data-driven engineering refers to systematic data collection and processing using machine learning to improve engineering systems. Currently, the implementation of data-driven engineering relies on fundamental data science and software engineering skills. At the same time, model-based engineering is gaining relevance for the engineering of complex systems. In previous work, a model-based engineering approach integrating the formalization of machine learning tasks using the general-purpose modeling language SysML is presented. However, formalized machine learning tasks still require the implementation in a specialized programming languages like Python. Therefore, this work aims to facilitate the implementation of data-driven engineering in practice by extending the previous work of formalizing machine learning tasks by integrating model transformation to generate executable code. The method focuses on the modifiability and maintainability of the model transformation so that extensions a
    
[^63]: DBFed: 基于领域无关性的去偏差联邦学习框架

    DBFed: Debiasing Federated Learning Framework based on Domain-Independent. (arXiv:2307.05582v1 [cs.LG])

    [http://arxiv.org/abs/2307.05582](http://arxiv.org/abs/2307.05582)

    DBFed是一个基于领域无关性的去偏差联邦学习框架，解决了在联邦学习中由数据质量差异引起的公平性问题。

    

    随着数字化转型的持续进行，企业正在产生、管理和存储大量的数据，而人工智能技术也在迅速发展。然而，这也给信息安全和数据安全带来了挑战。数据安全是指在其整个生命周期内，保护数字信息免受未经授权的访问、损坏、盗窃等的损害。随着数据安全法的颁布和执行以及组织和用户对数据安全和数据隐私的重视，以联邦学习为代表的隐私保护技术在各种应用场景中有广泛的应用。联邦学习是一种分布式机器学习计算框架，允许多个主体在不共享数据的情况下进行共同模型训练，以保护数据隐私并解决数据孤岛问题。然而，多个主体之间的数据彼此独立，而质量上的差异可能导致联邦学习中的公平性问题。

    As digital transformation continues, enterprises are generating, managing, and storing vast amounts of data, while artificial intelligence technology is rapidly advancing. However, it brings challenges in information security and data security. Data security refers to the protection of digital information from unauthorized access, damage, theft, etc. throughout its entire life cycle. With the promulgation and implementation of data security laws and the emphasis on data security and data privacy by organizations and users, Privacy-preserving technology represented by federated learning has a wide range of application scenarios. Federated learning is a distributed machine learning computing framework that allows multiple subjects to train joint models without sharing data to protect data privacy and solve the problem of data islands. However, the data among multiple subjects are independent of each other, and the data differences in quality may cause fairness issues in federated learnin
    
[^64]: 恶意言论通过双对比学习进行检测

    Hate Speech Detection via Dual Contrastive Learning. (arXiv:2307.05578v1 [cs.CL])

    [http://arxiv.org/abs/2307.05578](http://arxiv.org/abs/2307.05578)

    该论文提出了一种用于恶意言论检测的新颖框架，通过应对恶意言论中的复杂语义信息和恶意言辞的干扰以及恶意言论和非恶意言论的不平衡分布等挑战，实现了跨度级信息的捕捉。

    

    恶意言论在社交媒体上的快速传播影响着互联网环境和我们的社会，增加了偏见并伤害了人们。检测恶意言论在自然语言处理领域引起了广泛关注。尽管最近的研究已经解决了恶意言论检测中的一些问题，但这个任务仍然面临着两个固有的未解决挑战。第一个挑战在于恶意言论中传达的复杂语义信息，特别是恶意言论检测中侮辱性言辞的干扰。第二个挑战是恶意言论和非恶意言论的不平衡分布，可能会严重损害模型的性能。为了解决这些挑战，我们提出了一种新颖的恶意言论检测双对比学习（DCL）框架。我们的框架通过联合优化自监督对比学习损失和有监督对比学习损失，捕捉超出现有模型中使用的基于令牌级情感语义的跨度级信息。

    The fast spread of hate speech on social media impacts the Internet environment and our society by increasing prejudice and hurting people. Detecting hate speech has aroused broad attention in the field of natural language processing. Although hate speech detection has been addressed in recent work, this task still faces two inherent unsolved challenges. The first challenge lies in the complex semantic information conveyed in hate speech, particularly the interference of insulting words in hate speech detection. The second challenge is the imbalanced distribution of hate speech and non-hate speech, which may significantly deteriorate the performance of models. To tackle these challenges, we propose a novel dual contrastive learning (DCL) framework for hate speech detection. Our framework jointly optimizes the self-supervised and the supervised contrastive learning loss for capturing span-level information beyond the token-level emotional semantics used in existing models, particularly 
    
[^65]: 关于元宇宙逻辑的一些初步步骤

    Some Preliminary Steps Towards Metaverse Logic. (arXiv:2307.05574v1 [cs.LO])

    [http://arxiv.org/abs/2307.05574](http://arxiv.org/abs/2307.05574)

    本研究探寻了一种能够处理现实和虚构应用领域中出现情况的强大逻辑，采用非常规扩展，试图勾勒出一种最小复合逻辑策略，并通过ChatGPT AI代理传递理论概念背后的直觉。

    

    假设“元宇宙”一词可以理解为多元宇宙应用的基于计算机的实现，我们在本工作中开始寻找一个足够强大的逻辑来处理现实和虚构的基础应用领域中出现的情况。意识到一阶逻辑无法解释甚至最简单信息系统领域的不稳定行为，我们采用了非常规扩展，试图勾勒出一种最小复合逻辑策略。讨论保持在相当非正式的水平上，始终试图用自然语言的术语传达理论概念背后的直觉，并借助AI代理ChatGPT，希望算法和常识方法可以有用地结合起来。

    Assuming that the term 'metaverse' could be understood as a computer-based implementation of multiverse applications, we started to look in the present work for a logic that would be powerful enough to handle the situations arising both in the real and in the fictional underlying application domains. Realizing that first-order logic fails to account for the unstable behavior of even the most simpleminded information system domains, we resorted to non-conventional extensions, in an attempt to sketch a minimal composite logic strategy. The discussion was kept at a rather informal level, always trying to convey the intuition behind the theoretical notions in natural language terms, and appealing to an AI agent, namely ChatGPT, in the hope that algorithmic and common-sense approaches can be usefully combined.
    
[^66]: RidgeBase：一种跨传感器多指非接触指纹数据集

    RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset. (arXiv:2307.05563v1 [cs.CV])

    [http://arxiv.org/abs/2307.05563](http://arxiv.org/abs/2307.05563)

    RidgeBase是一个跨传感器多指非接触指纹数据集，旨在促进不同匹配场景下非接触指纹匹配的研究。

    

    使用智能手机相机进行非接触指纹匹配可以解决传统指纹系统的一些主要挑战，包括卫生采集、便携性和攻击防范。然而，实际和鲁棒的非接触指纹匹配技术的发展受到了大规模真实世界数据集的限制。为了激发在不同传感器之间进一步发展非接触指纹匹配的进步，我们介绍了RidgeBase基准数据集。RidgeBase包含了来自88个个体，在不同背景和照明条件下使用两个智能手机相机和一个平板触摸传感器获得的超过15,000个非接触和接触式指纹图像对。与现有数据集不同，RidgeBase旨在促进在不同匹配场景下的研究，包括单指匹配和多指匹配，既包括非接触到非接触（CL2CL）的验证和识别，也包括接触到非接触（C2CL）的验证和识别。

    Contactless fingerprint matching using smartphone cameras can alleviate major challenges of traditional fingerprint systems including hygienic acquisition, portability and presentation attacks. However, development of practical and robust contactless fingerprint matching techniques is constrained by the limited availability of large scale real-world datasets. To motivate further advances in contactless fingerprint matching across sensors, we introduce the RidgeBase benchmark dataset. RidgeBase consists of more than 15,000 contactless and contact-based fingerprint image pairs acquired from 88 individuals under different background and lighting conditions using two smartphone cameras and one flatbed contact sensor. Unlike existing datasets, RidgeBase is designed to promote research under different matching scenarios that include Single Finger Matching and Multi-Finger Matching for both contactless- to-contactless (CL2CL) and contact-to-contactless (C2CL) verification and identification. 
    
[^67]: 自动化论文评分中的反馈综述

    Review of feedback in Automated Essay Scoring. (arXiv:2307.05553v1 [cs.CL])

    [http://arxiv.org/abs/2307.05553](http://arxiv.org/abs/2307.05553)

    这篇论文综述了自动化论文评分中的反馈研究，包括不同类型的反馈和论文特征，并回顾了提供反馈的最新案例研究。

    

    第一个自动化论文评分系统诞生于50年前。自动化论文评分系统正在发展成为比以前简单评分系统更加功能丰富的系统。它的目的不仅仅是评分，还作为一个学习工具来提高用户的写作能力。反馈是使自动化论文评分系统在实际生活中有用的最重要方面。在第一个自动化论文评分系统中已经强调了反馈的重要性。本文综述了关于自动化论文评分的反馈研究，包括不同类型的反馈和论文特征。我们还回顾了提供反馈的最新案例研究。

    The first automated essay scoring system was developed 50 years ago. Automated essay scoring systems are developing into systems with richer functions than the previous simple scoring systems. Its purpose is not only to score essays but also as a learning tool to improve the writing skill of users. Feedback is the most important aspect of making an automated essay scoring system useful in real life. The importance of feedback was already emphasized in the first AES system. This paper reviews research on feedback including different feedback types and essay traits on automated essay scoring. We also reviewed the latest case studies of the automated essay scoring system that provides feedback.
    
[^68]: 用于稳定人工智能辅助癌症诊断的数字病理学扫描仪的物理颜色校准

    Physical Color Calibration of Digital Pathology Scanners for Robust Artificial Intelligence Assisted Cancer Diagnosis. (arXiv:2307.05519v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.05519](http://arxiv.org/abs/2307.05519)

    本研究通过物理颜色校准解决了数字病理学扫描仪生产中的技术不一致性问题，提高了人工智能辅助癌症诊断的稳定性和准确性。

    

    人工智能在数字病理学中的潜力受到全切片图像（WSIs）生产中的技术不一致性的限制，导致AI性能下降，并给广泛临床应用带来挑战。扫描仪的工作流更改还可能导致诊断受损和患者安全风险。我们评估了扫描仪的物理颜色校准是否可以标准化WSI的外观，并实现稳定的AI性能。我们在四个不同的实验室中使用了一个颜色校准滑片，并评估其对1,161个WSI上前列腺癌诊断AI系统性能的影响。颜色标准化导致AI模型校准持续改善，同时在Gleason分级性能方面显著提高。该研究证明，物理颜色校准提供了解决由不同扫描仪引入的差异的潜在解决方案，从而实现了基于AI的癌症诊断的稳定性。

    The potential of artificial intelligence (AI) in digital pathology is limited by technical inconsistencies in the production of whole slide images (WSIs), leading to degraded AI performance and posing a challenge for widespread clinical application as fine-tuning algorithms for each new site is impractical. Changes in the imaging workflow can also lead to compromised diagnoses and patient safety risks. We evaluated whether physical color calibration of scanners can standardize WSI appearance and enable robust AI performance. We employed a color calibration slide in four different laboratories and evaluated its impact on the performance of an AI system for prostate cancer diagnosis on 1,161 WSIs. Color standardization resulted in consistently improved AI model calibration and significant improvements in Gleason grading performance. The study demonstrates that physical color calibration provides a potential solution to the variation introduced by different scanners, making AI-based cance
    
[^69]: 为叙事益智游戏的难度进行程序化生成规则

    Procedurally generating rules to adapt difficulty for narrative puzzle games. (arXiv:2307.05518v1 [cs.HC])

    [http://arxiv.org/abs/2307.05518](http://arxiv.org/abs/2307.05518)

    本文提出了一种自动生成规则并将其传达给玩家调整难度的方法，通过使用遗传算法和难度测量来找到目标解集的数量，并使用大型语言模型在叙事背景下传达规则。该方法在测试中成功地找到了逼近任何给定目标难度的规则。该方法还结合了大型语言模型，创建了一个有趣的叙事益智游戏，玩家必须举办一次晚宴为无法和谐共处的动物们。未来的工作将致力于改进评估，专门针对儿童文学进行语言模型训练，并收集玩家的多模态数据来指导难度的动态调整。

    

    本文关注于通过程序化生成规则并将其传达给玩家以调整难度。这是一个更大的项目的一部分，旨在使用旨在幼儿园的数字益智游戏在教育游戏中收集和调整游戏。该方法使用遗传算法与难度测量一起，找到一组解的目标数量，并使用大型语言模型在叙事背景下传达规则。在测试过程中，该方法能够在平均两打代之内找到逼近任何给定目标难度的规则。该方法结合了大型语言模型，创建了一个叙事益智游戏，玩家必须为不能相处的动物举办一次晚宴。未来的实验将尝试改进评估，让语言模型专门针对儿童文学，并收集玩家的多模态数据来指导适应。

    This paper focuses on procedurally generating rules and communicating them to players to adjust the difficulty. This is part of a larger project to collect and adapt games in educational games for young children using a digital puzzle game designed for kindergarten. A genetic algorithm is used together with a difficulty measure to find a target number of solution sets and a large language model is used to communicate the rules in a narrative context. During testing the approach was able to find rules that approximate any given target difficulty within two dozen generations on average. The approach was combined with a large language model to create a narrative puzzle game where players have to host a dinner for animals that can't get along. Future experiments will try to improve evaluation, specialize the language model on children's literature, and collect multi-modal data from players to guide adaptation.
    
[^70]: 基于图像分类的深度学习移动应用的用户体验启发和检查清单

    UX Heuristics and Checklist for Deep Learning powered Mobile Applications with Image Classification. (arXiv:2307.05513v1 [cs.HC])

    [http://arxiv.org/abs/2307.05513](http://arxiv.org/abs/2307.05513)

    这项研究提出了一套适用于深度学习移动应用图像分类的AIX启发和检查清单，以及相应的在线课程和Web工具，可用于指导应用界面设计和启发式评估，帮助开发易于理解的图像分类应用。

    

    深度学习技术在移动应用的图像分类方面的应用不断进步，需要创新的用户体验解决方案来确保用户的充分使用。为了帮助设计过程，通常会为特定类型的应用定制可用性启发。因此，基于文献综述和分析现有的具备图像分类功能的移动应用，我们提出了一套初始化的用于深度学习移动应用图像分类的AIX启发和检查清单。为了便于清单的使用，我们还开发了一个在线课程来介绍概念和启发，以及一个基于Web的工具来支持使用这些启发进行评估。该研究的结果可以用于指导此类应用的界面设计，并支持进行启发式评估，帮助开发人员开发易于理解的图像分类应用。

    Advances in mobile applications providing image classification enabled by Deep Learning require innovative User Experience solutions in order to assure their adequate use by users. To aid the design process, usability heuristics are typically customized for a specific kind of application. Therefore, based on a literature review and analyzing existing mobile applications with image classification, we propose an initial set of AIX heuristics for Deep Learning powered mobile applications with image classification decomposed into a checklist. In order to facilitate the usage of the checklist we also developed an online course presenting the concepts and heuristics as well as a web-based tool in order to support an evaluation using these heuristics. These results of this research can be used to guide the design of the interfaces of such applications as well as support the conduction of heuristic evaluations supporting practitioners to develop image classification apps that people can unders
    
[^71]: 交互冲突、自动化水平和自动化频率对人类对自动化信任和接受度的影响

    The Effects of Interaction Conflicts, Levels of Automation, and Frequency of Automation on Human Automation Trust and Acceptance. (arXiv:2307.05512v1 [cs.HC])

    [http://arxiv.org/abs/2307.05512](http://arxiv.org/abs/2307.05512)

    交互冲突、自动化水平和自动化频率对人类在智能家居环境中对自动化的信任和接受度产生影响，自动化水平和频率越高，用户对智能环境的信任度越高，但在出现自动化故障和交互冲突时，用户对自动化智能环境的接受度降低。

    

    在交互冲突存在的情况下，用户对自动化的信任对于接受智能环境（如智能家居）非常重要。本文采用一个因子研究设计，研究和比较自动化水平（LoA）、自动化响应频率（FoA）和冲突强度（CI）对人类在智能家居环境中对自动化的信任和接受度的单独和联合影响。为了研究这些影响，我们进行了基于网络的实验，从324名在线参与者中收集了数据，他们通过一个智能家居的3D模拟系统体验了该系统。研究发现，自动化水平和频率对用户在智能环境中的信任产生了影响。此外，结果还表明，在出现自动化故障和交互冲突时，用户对自动化智能环境的接受度会降低。

    In the presence of interaction conflicts, user trust in automation plays an important role in accepting intelligent environments such as smart homes. In this paper, a factorial research design is employed to investigate and compare the single and joint effects of Level of Automation (LoA), Frequency of Automated responses (FoA), and Conflict Intensity (CI) on human trust and acceptance of automation in the context of smart homes. To study these effects, we conducted web-based experiments to gather data from 324 online participants who experienced the system through a 3D simulation of a smart home. The findings show that the level and frequency of automation had an impact on user trust in smart environments. Furthermore, the results demonstrate that the users' acceptance of automated smart environments decreased in the presence of automation failures and interaction conflicts.
    
[^72]: 通过xAI和主动学习的人在AI循环中进行视觉检查

    Human in the AI loop via xAI and Active Learning for Visual Inspection. (arXiv:2307.05508v1 [cs.HC])

    [http://arxiv.org/abs/2307.05508](http://arxiv.org/abs/2307.05508)

    通过xAI和主动学习的人在AI循环中进行视觉检查的论文探讨了工业 5.0 中人机协作的新机会，并分享了关于视觉检查中的人工智能、人类数字孪生和网络安全的最新研究成果。

    

    工业革命通过引入自动化来改变制造业，增加的自动化改变了人工工人的角色。机器人和人工智能的进步开辟了人机协作的新领域。本章首先描述了工业5.0，人机协作以及关于质量检查的最新技术，重点是视觉检查。然后，我们提供了关于如何在视觉检查中实现和增强人机协作的观点。最后，我们分享了在欧盟H2020 STAR项目中关于视觉检查的一些结果，考虑了人工智能，人类数字孪生和网络安全。

    Industrial revolutions have historically disrupted manufacturing by introducing automation into production. Increasing automation reshapes the role of the human worker. Advances in robotics and artificial intelligence open new frontiers of human-machine collaboration. In this chapter, we first describe Industry 5.0, human-machine collaboration, and state-of-the-art regarding quality inspection, emphasizing visual inspection. We then provide our perspective on how human-machine collaboration could be realized and enhanced in visual inspection. Finally, we share some of the results obtained in the EU H2020 STAR project regarding visual inspection, considering artificial intelligence, human digital twins, and cybersecurity.
    
[^73]: 通过地理负载平衡实现环境公平的人工智能

    Towards Environmentally Equitable AI via Geographical Load Balancing. (arXiv:2307.05494v1 [cs.AI])

    [http://arxiv.org/abs/2307.05494](http://arxiv.org/abs/2307.05494)

    本研究通过地理负载平衡的方式解决了人工智能在不同地区对环境的不平等影响，从而推进环境公平的人工智能发展。

    

    受到大型语言和基础模型的飙升人气推动，人工智能（AI）模型的巨大环境足迹的快速增长受到了越来越多的关注。虽然提出了许多方法使AI更节能环保，但环境不平等——即AI的环境足迹在某些地区可能不成比例地更高——已经出现，并引发了社会生态正义的关切。本文通过平衡AI的区域负面环境影响来首次解决AI的环境不平等问题。具体而言，我们专注于AI模型推理的碳足迹和水足迹，并提出了注重公平的地理负载平衡（GLB）来明确解决AI对最弱势地区的环境影响。我们通过考虑一组分布在地理上的10个数据中心来运行基于跟踪的仿真，这些数据中心为大型L

    Fueled by the soaring popularity of large language and foundation models, the accelerated growth of artificial intelligence (AI) models' enormous environmental footprint has come under increased scrutiny. While many approaches have been proposed to make AI more energy-efficient and environmentally friendly, environmental inequity -- the fact that AI's environmental footprint can be disproportionately higher in certain regions than in others -- has emerged, raising social-ecological justice concerns. This paper takes a first step toward addressing AI's environmental inequity by balancing its regional negative environmental impact. Concretely, we focus on the carbon and water footprints of AI model inference and propose equity-aware geographical load balancing (GLB) to explicitly address AI's environmental impacts on the most disadvantaged regions. We run trace-based simulations by considering a set of 10 geographically-distributed data centers that serve inference requests for a large l
    
[^74]: 中学英语作为外语学生与ChatGPT合作完成写作任务的案例研究

    Cases of EFL Secondary Students' Prompt Engineering Pathways to Complete a Writing Task with ChatGPT. (arXiv:2307.05493v1 [cs.HC])

    [http://arxiv.org/abs/2307.05493](http://arxiv.org/abs/2307.05493)

    本文研究了中学英语作为外语学生与ChatGPT合作完成写作任务的案例，并对提示的质量和数量进行了分析。结果发现，非技术用户在为ChatGPT编写适当的提示上遇到了困难，需要提供更好的支持和指导。

    

    ChatGPT是一个最先进的聊天机器人。尽管它有潜力支持英语作为外语学生的写作，但要有效地与之合作，学生必须学会设计提示，即制作适当的指令，以使ChatGPT产生期望的输出。然而，对于非技术用户来说，为ChatGPT编写适当的提示并非易事，他们经历了反复试错的过程。本文研究了中学英语作为外语学生在完成写作任务时使用ChatGPT的提示内容，并探讨了提示的质量和数量的模式。数据来自iPad屏幕录像，记录了首次使用ChatGPT和其他最先进聊天机器人完成相同写作任务的中学英语作为外语学生的情况。本文通过四个不同的路径的案例研究，展示了试错过程和提示内容和数量的不同组合。这些案例为提供支持非技术用户有效使用ChatGPT的证据做出了贡献。

    ChatGPT is a state-of-the-art (SOTA) chatbot. Although it has potential to support English as a foreign language (EFL) students' writing, to effectively collaborate with it, a student must learn to engineer prompts, that is, the skill of crafting appropriate instructions so that ChatGPT produces desired outputs. However, writing an appropriate prompt for ChatGPT is not straightforward for non-technical users who suffer a trial-and-error process. This paper examines the content of EFL students' ChatGPT prompts when completing a writing task and explores patterns in the quality and quantity of the prompts. The data come from iPad screen recordings of secondary school EFL students who used ChatGPT and other SOTA chatbots for the first time to complete the same writing task. The paper presents a case study of four distinct pathways that illustrate the trial-and-error process and show different combinations of prompt content and quantity. The cases contribute evidence for the need to provid
    
[^75]: GPT4对同行评审存在一定帮助：一项实验性研究。

    GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study. (arXiv:2307.05492v1 [cs.HC])

    [http://arxiv.org/abs/2307.05492](http://arxiv.org/abs/2307.05492)

    通过比较GPT和人类审稿人生成的评论，我们发现GPT在同行评审中具有一定的帮助性，为解决同行评审资源限制提供了新途径。

    

    在这项实验性研究中，我们调查了GPT4在同行评审过程中的应用。我们的主要假设是，通过与人类审稿人生成的评论进行比较，GPT所生成的评论可以达到相似的有用性。通过比较在一个重要的机器学习会议上提交的学术论文的人类审稿人和GPT模型生成的评论，我们提供了初步证据，表明人工智能可以有效地为同行评审过程做出贡献。我们还进行了对插入了错误的鲁棒性实验，以了解模型更倾向于关注论文的哪些部分。我们的研究结果为利用机器学习工具解决同行评审资源限制开辟了新途径。该结果还为改进评审过程提供了启示，并为在人类反馈资源稀缺的领域中进一步研究扩大监督打下了基础。

    In this pilot study, we investigate the use of GPT4 to assist in the peer-review process. Our key hypothesis was that GPT-generated reviews could achieve comparable helpfulness to human reviewers. By comparing reviews generated by both human reviewers and GPT models for academic papers submitted to a major machine learning conference, we provide initial evidence that artificial intelligence can contribute effectively to the peer-review process. We also perform robustness experiments with inserted errors to understand which parts of the paper the model tends to focus on. Our findings open new avenues for leveraging machine learning tools to address resource constraints in peer review. The results also shed light on potential enhancements to the review process and lay the groundwork for further research on scaling oversight in a domain where human-feedback is increasingly a scarce resource.
    
[^76]: VampNet：通过掩码声学令牌建模进行音乐生成

    VampNet: Music Generation via Masked Acoustic Token Modeling. (arXiv:2307.04686v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2307.04686](http://arxiv.org/abs/2307.04686)

    本论文介绍了一种名为VampNet的音乐生成方法，通过掩码声学令牌建模实现生成、压缩、修复和变异。VampNet通过灵活的提示能力，能够保持音乐的风格、流派、乐器和其他高层次方面，是一个强大的音乐共创工具。

    

    我们引入了VampNet，一种通过掩码声学令牌建模方法进行音乐合成、压缩、修复和变异。我们在训练过程中使用可变的掩码计划，通过应用各种掩码方法（称为提示）进行推理来从模型中采样连贯的音乐。VampNet是非自回归的，利用双向Transformer架构，在正向传递中关注所有令牌。经过36次采样传递，VampNet可以生成连贯的高保真音乐波形。我们展示了通过以不同方式提示VampNet，我们可以将其应用于音乐压缩、修复、扩展、延续和循环变异（vamping）等任务。在适当的提示下，VampNet能够保持音乐的风格、流派、乐器和其他高层次方面。这种灵活的提示能力使VampNet成为一个强大的音乐共创工具。代码和音频样本可在网上获得。

    We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.
    
[^77]: 弱监督定位对比学习：肝硬化分类的应用

    Weakly-supervised positional contrastive learning: application to cirrhosis classification. (arXiv:2307.04617v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.04617](http://arxiv.org/abs/2307.04617)

    本研究提出了一种弱监督定位对比学习策略，在医学应用中使用大量弱标签图像进行肝硬化预测。这种策略将每个2D切片的空间上下文和弱标签整合起来，通过使用通用基于核的损失函数实现高效学习。

    

    大型医学影像数据集可以通过低置信度的弱标签（例如放射学评分）进行廉价快速的注释。而高置信度的标签（如基于组织学的诊断）很少且昂贵。预训练策略，如对比学习方法，可以利用未标记或弱标注的数据集。然而，这些方法通常需要较大的批处理大小，这在大型3D图像的全分辨率情况下存在困难，因为GPU内存有限。尽管如此，关于每个2D切片的空间上下文的体积信息对于某些医学应用非常重要。在这项研究中，我们提出了一种高效的弱监督定位对比学习策略，通过使用通用基于核的损失函数将每个2D切片的空间上下文和弱标签进行整合。我们通过使用大量弱标签图像（即放射学低置信度标注）来说明我们的方法在肝硬化预测中的应用。

    Large medical imaging datasets can be cheaply and quickly annotated with low-confidence, weak labels (e.g., radiological scores). Access to high-confidence labels, such as histology-based diagnoses, is rare and costly. Pretraining strategies, like contrastive learning (CL) methods, can leverage unlabeled or weakly-annotated datasets. These methods typically require large batch sizes, which poses a difficulty in the case of large 3D images at full resolution, due to limited GPU memory. Nevertheless, volumetric positional information about the spatial context of each 2D slice can be very important for some medical applications. In this work, we propose an efficient weakly-supervised positional (WSP) contrastive learning strategy where we integrate both the spatial context of each 2D slice and a weak label via a generic kernel-based loss function. We illustrate our method on cirrhosis prediction using a large volume of weakly-labeled images, namely radiological low-confidence annotations,
    
[^78]: 自然语言指令用于建筑施工领域中人机互动的直观性交流

    Natural Language Instructions for Intuitive Human Interaction with Robotic Assistants in Field Construction Work. (arXiv:2307.04195v1 [cs.RO])

    [http://arxiv.org/abs/2307.04195](http://arxiv.org/abs/2307.04195)

    这篇论文提出了在建筑施工领域中，利用自然语言指令实现人机直观交流的框架，以解决施工现场复杂性和人力短缺等问题。

    

    机器人的引入被广泛认为有望解决施工行业面临的劳动力短缺和生产力停滞的问题。然而，在复杂和无结构的施工现场使用完全自动化的机器人是具有挑战性的。人机协作在施工中显示出了结合人类工人的灵活性和机器人助手的物理能力共同应对施工工作中固有不确定性的潜力。在引入人机协作时，识别团队合作和监督在实地施工中的重要性，并为人类工人和机器人助手建立一种自然直观的交流系统至关重要。基于自然语言的交互可以使非机器人编程专家的人类工人与机器人之间进行直观和熟悉的交流。然而，目前在建筑领域对这一主题的研究还很有限。本文提出了一个框架来允许实现这种直观的交流。

    The introduction of robots is widely considered to have significant potential of alleviating the issues of worker shortage and stagnant productivity that afflict the construction industry. However, it is challenging to use fully automated robots in complex and unstructured construction sites. Human-Robot Collaboration (HRC) has shown promise of combining human workers' flexibility and robot assistants' physical abilities to jointly address the uncertainties inherent in construction work. When introducing HRC in construction, it is critical to recognize the importance of teamwork and supervision in field construction and establish a natural and intuitive communication system for the human workers and robotic assistants. Natural language-based interaction can enable intuitive and familiar communication with robots for human workers who are non-experts in robot programming. However, limited research has been conducted on this topic in construction. This paper proposes a framework to allow
    
[^79]: 增强空间上下文的潜在图注意力

    Latent Graph Attention for Enhanced Spatial Context. (arXiv:2307.04149v1 [cs.CV])

    [http://arxiv.org/abs/2307.04149](http://arxiv.org/abs/2307.04149)

    本文提出了一种称为潜在图注意力（LGA）的模块化框架，用于增强图像中的全局上下文。LGA在计算上简洁且稳定，能够在小规模体系结构中实现接近大规模体系结构的性能，使得轻量级体系结构在边缘设备上更加实用。

    

    图像中的全局上下文在图像到图像转换问题中非常有价值。传统的基于注意力和图模型在很大程度上捕捉到了全局上下文，但是这些方法计算上比较昂贵。此外，现有的方法只能学习图像中任意两个点之间的配对语义关系。在本文中，我们提出了一种称为潜在图注意力（LGA）的计算简洁（与节点数量呈线性关系）和稳定的模块化框架，用于将全局上下文纳入现有体系结构中，特别是增强小规模体系结构的性能，使得轻量级体系结构对于计算能力较低和能量需求较低的边缘设备更加有用。LGA使用局部连接图网络来在空间上传播信息，从而方便地构建远距离的两个空间点之间的语义一致关系。

    Global contexts in images are quite valuable in image-to-image translation problems. Conventional attention-based and graph-based models capture the global context to a large extent, however, these are computationally expensive. Moreover, the existing approaches are limited to only learning the pairwise semantic relation between any two points on the image. In this paper, we present Latent Graph Attention (LGA) a computationally inexpensive (linear to the number of nodes) and stable, modular framework for incorporating the global context in the existing architectures, especially empowering small-scale architectures to give performance closer to large size architectures, thus making the light-weight architectures more useful for edge devices with lower compute power and lower energy needs. LGA propagates information spatially using a network of locally connected graphs, thereby facilitating to construct a semantically coherent relation between any two spatially distant points that also 
    
[^80]: 对视频片段中物体行为的推理用于副词类型识别

    Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition. (arXiv:2307.04132v1 [cs.CV])

    [http://arxiv.org/abs/2307.04132](http://arxiv.org/abs/2307.04132)

    本研究提出了一种新的框架，通过对视频片段中提取的物体行为进行推理来识别副词类型。实验结果表明，我们的方法在效果上超越了之前的最先进方法。

    

    本研究根据对描述场景序列的副词最佳识别方法为基础，提出了一种新的框架，通过对从原始视频片段中提取的物体行为进行推理来识别片段对应的副词类型。与之前针对常规场景副词识别的方法不同的是，我们的方法适用于视频片段的行动类型未知的更一般的问题设置。具体而言，我们提出了一个新的流程，从原始视频片段中提取了可以人类理解的物体行为事实，并提出了基于符号和转换器的推理方法，对这些提取出的事实进行操作以识别副词类型。实验结果表明，我们提出的方法在性能上优于之前的最先进方法。此外，为了支持符号视频处理的工作，我们还发布了视频领域相关的资源。

    In this work, following the intuition that adverbs describing scene-sequences are best identified by reasoning over high-level concepts of object-behavior, we propose the design of a new framework that reasons over object-behaviours extracted from raw-video-clips to recognize the clip's corresponding adverb-types. Importantly, while previous works for general scene adverb-recognition assume knowledge of the clips underlying action-types, our method is directly applicable in the more general problem setting where the action-type of a video-clip is unknown. Specifically, we propose a novel pipeline that extracts human-interpretable object-behaviour-facts from raw video clips and propose novel symbolic and transformer based reasoning methods that operate over these extracted facts to identify adverb-types. Experiment results demonstrate that our proposed methods perform favourably against the previous state-of-the-art. Additionally, to support efforts in symbolic video-processing, we rele
    
[^81]: 边缘人工智能监管：管理对公共安全的新兴风险

    Frontier AI Regulation: Managing Emerging Risks to Public Safety. (arXiv:2307.03718v1 [cs.CY])

    [http://arxiv.org/abs/2307.03718](http://arxiv.org/abs/2307.03718)

    对于边缘人工智能模型的监管需要标准制定、注册报告和安全合规机制。

    

    先进的人工智能模型为人类带来巨大的好处，但社会需要主动管理相关的风险。本文关注我们所称的“边缘人工智能”模型：高度能力的基础模型，可能具备足以对公共安全造成严重风险的危险能力。边缘人工智能模型带来了独特的监管挑战：危险能力可能出乎意料；很难有效防止部署模型被滥用；并且很难阻止模型的能力广泛扩散。为了应对这些挑战，边缘模型的监管需要至少三个基本要素：(1) 设定标准的过程，以确定边缘人工智能开发者的适当要求；(2) 注册和报告要求，为监管机构提供对边缘人工智能开发过程的可见性；(3) 保证开发和部署的安全标准的机制。

    Advanced AI models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. In this paper, we focus on what we term "frontier AI" models: highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety. Frontier AI models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model's capabilities from proliferating broadly. To address these challenges, at least three building blocks for the regulation of frontier models are needed: (1) standard-setting processes to identify appropriate requirements for frontier AI developers, (2) registration and reporting requirements to provide regulators with visibility into frontier AI development processes, and (3) mechanisms to ensure compliance with safety standards for the development and deplo
    
[^82]: 对大型语言模型评估的调查

    A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])

    [http://arxiv.org/abs/2307.03109](http://arxiv.org/abs/2307.03109)

    本文综述了大型语言模型（LLMs）的评估方法，关注三个关键维度：评估什么、在哪里评估以及如何评估。评估任务包括自然语言处理、推理、医学应用、伦理学、教育、自然和社会科学、代理应用等多个领域。本文为社会层面对LLMs潜在风险的理解提供了重要参考。

    

    大型语言模型（LLMs）由于在各种应用中表现出的前所未有的性能而在学术界和工业界越来越受欢迎。随着LLMs在研究和日常使用中继续发挥着重要作用，它们的评估变得越来越关键，不仅在任务水平上，而且在社会层面上，以更好地了解它们的潜在风险。在过去的几年里，已经做出了相当大的努力来从不同的角度来研究LLMs。本文综述了LLMs的这些评估方法，重点关注三个关键维度：评估什么、在哪里评估以及如何评估。首先，我们从评估任务的角度提供了一个概述，涵盖了一般的自然语言处理任务、推理、医学应用、伦理学、教育、自然科学和社会科学、代理应用和其他领域。其次，我们通过深入探讨评估方法和基准答案来回答“在哪里”和“如何”这两个问题。

    Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
    
[^83]: MOPO-LSI：用户指南

    MOPO-LSI: A User Guide. (arXiv:2307.01719v1 [q-fin.PM])

    [http://arxiv.org/abs/2307.01719](http://arxiv.org/abs/2307.01719)

    MOPO-LSI是一款开源的多目标投资组合优化库，为可持续投资提供用户指南，并介绍了版本1.0的问题设置、工作流程和超参数。

    

    MOPO-LSI是一种开源的可持续投资多目标投资组合优化库。本文提供了MOPO-LSI版本1.0的用户指南，包括问题设置、工作流程和配置中的超参数。

    MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for Sustainable Investments. This document provides a user guide for MOPO-LSI version 1.0, including problem setup, workflow and the hyper-parameters in configurations.
    
[^84]: ESGCN: 边缘压缩注意图卷积网络用于交通流量预测

    ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting. (arXiv:2307.01227v1 [cs.LG])

    [http://arxiv.org/abs/2307.01227](http://arxiv.org/abs/2307.01227)

    ESGCN是一种用于交通流量预测的边缘压缩注意图卷积网络，通过建模时空动态和引入边缘特征和边缘注意机制来提高预测的准确性。

    

    交通流量预测是一个极具挑战性的任务，由于交通流的动态时空依赖关系。为了应对这个问题，我们着重于建模时空动态并提出了一种名为Edge Squeeze Graph Convolutional Network (ESGCN)的网络来预测多个地区的交通流量。ESGCN由两个模块组成：W模块和ES模块。W模块是一个完全以节点为基础的卷积网络。它分别对每个交通区域的时间序列进行编码，并在不同尺度上分解时间序列以捕捉细粒度和粗粒度的特征。ES模块使用图卷积网络(GCN)建模时空动态，并利用时序特征生成自适应邻接矩阵(AAM)。为了提高AAM的准确性，我们引入了三个关键概念。1）使用边缘特征直接捕捉区域之间的时空流动表示。2）将边缘注意机制应用于GCN，从边缘特征中提取AAM。

    Traffic forecasting is a highly challenging task owing to the dynamical spatio-temporal dependencies of traffic flows. To handle this, we focus on modeling the spatio-temporal dynamics and propose a network termed Edge Squeeze Graph Convolutional Network (ESGCN) to forecast traffic flow in multiple regions. ESGCN consists of two modules: W-module and ES module. W-module is a fully node-wise convolutional network. It encodes the time-series of each traffic region separately and decomposes the time-series at various scales to capture fine and coarse features. The ES module models the spatio-temporal dynamics using Graph Convolutional Network (GCN) and generates an Adaptive Adjacency Matrix (AAM) with temporal features. To improve the accuracy of AAM, we introduce three key concepts. 1) Using edge features to directly capture the spatiotemporal flow representation among regions. 2) Applying an edge attention mechanism to GCN to extract the AAM from the edge features. Here, the attention m
    
[^85]: PatternGPT: 一种基于模式的大型语言模型文本生成框架

    PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation. (arXiv:2307.00470v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.00470](http://arxiv.org/abs/2307.00470)

    PatternGPT是一种基于模式驱动的大型语言模型文本生成框架，通过利用大型语言模型的提取能力生成多样化的模式，并使用联邦学习的思想实现模式共享，最终通过搜索高质量模式指导生成模型。该框架具有生成多样化模式、保护数据隐私、结合外部知识等优势。

    

    大型语言模型(LLMs)展示了出色的文本生成能力，能够为许多下游任务生成流畅的响应。然而，将大型语言模型应用于现实世界的关键任务仍然具有挑战性，因为它们容易出现幻觉，并且无法直接使用外部知识。为解决上述问题，本文提出了PatternGPT，一种基于模式驱动的大型语言模型文本生成框架。首先，该框架利用大型语言模型的提取能力生成丰富多样的模式，然后借鉴联邦学习的思想，使用多个代理实现共享以获取更多样的模式。最后，它使用判断标准和优化算法搜索高质量的模式，并使用搜索到的模式指导模型进行生成。该框架具有生成多样化模式、保护数据隐私、结合外部知识等优势。

    Large language models(LLMS) have shown excellent text generation capabilities,capable of generating fluent responses for many downstream tasks. However,applying large language models to real-world critical tasks remains challenging due to their susceptibility to hallucinations and inability to directly use external knowledge. To address the above challenges,this paper proposes PatternGPT, a pattern-driven text generation framework for large language models. First,the framework utilizes the extraction capabilities of large language models to generate rich and diverse patterns and later draws on the idea of federated learning. Using multiple agents to achieve sharing to obtain more diverse patterns. Finally, it searches for high-quality patterns using judgment criteria and optimization algorithms and uses the searched patterns to guide the model for generation. This framework has the advantages of generating diversified patterns, protecting data privacy,combining external knowledge, and 
    
[^86]: 通过跳过零元素降低卷积层的计算复杂度

    Reduce Computational Complexity for Convolutional Layers by Skipping Zeros. (arXiv:2306.15951v1 [cs.LG])

    [http://arxiv.org/abs/2306.15951](http://arxiv.org/abs/2306.15951)

    本文提出了C-K-S算法，通过修剪滤波器和转换稀疏张量为稠密张量的方式，跳过卷积层中的0元素，从而降低了计算复杂度。实验证明，C-K-S相对于PyTorch具有优势。

    

    深度神经网络依赖并行处理器进行加速。为了为其设计运算符，需要不仅有优化算法以降低复杂度，还需要充分利用硬件资源。卷积层主要包含三种运算符：前向传播的卷积，反向传播的反卷积和膨胀卷积。当执行这些运算时，始终会向张量中添加0元素，导致冗余计算。本文提出了C-K-S算法（ConvV2, KS-deconv, Sk-dilated），以两种方式跳过这些0元素：修剪滤波器以排除填充的0元素；将稀疏张量转换为稠密张量，避免在反卷积和膨胀卷积中插入0元素。与普通卷积相比，反卷积由于其复杂性而难以加速。本文提供了C-K-S的高性能GPU实现，并通过与PyTorch的比较验证了其有效性。根据实验结果，在某些情况下，C-K-S相对于PyTorch具有优势。

    Deep neural networks rely on parallel processors for acceleration. To design operators for them, it requires not only good algorithm to reduce complexity, but also sufficient utilization of hardwares. Convolutional layers mainly contain 3 kinds of operators: convolution in forward propagation, deconvolution and dilated-convolution in backward propagation. When executing these operators, 0s are always added to tensors, causing redundant calculations. This paper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these 0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors to dense tensors, to avoid inserted 0s in deconvolution and dilated-convolution. In contrast to regular convolution, deconvolution is hard to accelerate due to its complicacy. This paper provides high-performance GPU implementations of C-K-S, and verifies their effectiveness with comparison to PyTorch. According to the experiments, C-K-S has advantages over PyTorch in certain cas
    
[^87]: REFLECT:对机器人经历进行总结，以用于失败解释和纠正

    REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. (arXiv:2306.15724v1 [cs.RO])

    [http://arxiv.org/abs/2306.15724](http://arxiv.org/abs/2306.15724)

    提出了REFLECT框架，可以将机器人多感官数据转化为分层总结，并使用大型语言模型进行失败解释。该框架能够生成有益的失败解释，帮助机器人完成任务。

    

    自动检测和分析失败执行是实现可解释和稳健机器人系统的关键。最近，大型语言模型（LLM）在文本输入上展示了强大的常识推理能力。为了利用LLM的力量进行机器人失败解释，我们提出了一个框架REFLECT，将多感官数据转化为机器人过去经验的分层总结，并使用逐步失败解释算法查询LLM。基于解释，失败纠正规划器生成一个可执行计划，以纠正失败并完成任务。为了系统评估该框架，我们创建了RoboFail数据集，并展示了我们基于LLM的框架能够生成有益的失败解释，从而帮助成功的纠正规划。项目网站：https://roboreflect.github.io/

    The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong common sense reasoning skills on textual inputs. To leverage the power of LLM for robot failure explanation, we propose a framework REFLECT, which converts multi-sensory data into a hierarchical summary of robot past experiences and queries LLM with a progressive failure explanation algorithm. Conditioned on the explanation, a failure correction planner generates an executable plan for the robot to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset and show that our LLM-based framework is able to generate informative failure explanations that assist successful correction planning. Project website: https://roboreflect.github.io/
    
[^88]: 人工智能灾难性风险综述

    An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])

    [http://arxiv.org/abs/2306.12001](http://arxiv.org/abs/2306.12001)

    本文综述了人工智能灾难性风险的四个主要来源，包括恶意使用、人工智能竞赛、组织风险和流氓人工智能。

    

    人工智能的快速发展引起了专家、政策制定者和世界各国领导人对越来越先进的人工智能系统可能带来灾难性风险的担忧。虽然已经有很多风险被单独详细介绍过，但迫切需要系统地讨论和说明潜在危险，以更好地支持减轻这些风险的努力。本文概述了人工智能灾难性风险的主要来源，我们将其分为四个类别：恶意使用，即个人或团体有意使用人工智能造成伤害；人工智能竞赛，即竞争环境促使行动者部署不安全的人工智能或放弃控制权交给人工智能；组织风险，突出人为和复杂系统如何增加灾难性事故发生的可能性；以及流氓人工智能，描述了控制比人类智能更高的代理程序困难的固有难题。对于每个风险类别，我们描述了具体的危害。

    Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
    
[^89]: MARBLE：音乐音频表征通用评估基准

    MARBLE: Music Audio Representation Benchmark for Universal Evaluation. (arXiv:2306.10548v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2306.10548](http://arxiv.org/abs/2306.10548)

    本论文介绍了MARBLE，一个音乐音频表征通用评估基准，它为音乐理解领域的研究和发展提供了一个全面和可持续性的基础，并提供各种音乐信息检索（MIR）任务的基准。

    

    在艺术与人工智能（AI）之间交叉的广泛时代中，例如图像生成和虚构共创，音乐的AI仍然相对初步，特别是在音乐理解方面。针对这个问题，本论文介绍了一个通用的音乐音频表征评估基准MARBLE，旨在提供各种音乐信息检索（MIR）任务的基准，通过定义包括声学，演奏，乐谱和高级描述在内的四个层次的综合分类法。然后，我们基于8个公共可用数据集上的14项任务建立了一个统一的协议，提供了所有基于音乐录音开发的开放源代码的预训练模型的表征的公平和标准的评估。此外，MARBLE提供了一个易于使用、可扩展和可重用的工具库，以支持社区驱动的客观基准评估。

    In the era of extensive intersection between art and Artificial Intelligence (AI), such as image generation and fiction co-creation, AI for music remains relatively nascent, particularly in music understanding. This is evident in the limited work on deep music representations, the scarcity of large-scale datasets, and the absence of a universal and community-driven benchmark. To address this issue, we introduce the Music Audio Representation Benchmark for universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy with four hierarchy levels, including acoustic, performance, score, and high-level description. We then establish a unified protocol based on 14 tasks on 8 public-available datasets, providing a fair and standard assessment of representations of all open-sourced pre-trained models developed on music recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and re
    
[^90]: Decision-Making and Control的深度生成模型

    Deep Generative Models for Decision-Making and Control. (arXiv:2306.08810v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08810](http://arxiv.org/abs/2306.08810)

    本论文研究了基于深度模型的强化学习方法在决策和控制问题中的缺点，并提出了解决方法。同时，将当代生成建模工具中的推理技术重新解释为强化学习问题的规划策略。

    

    基于深度模型的强化学习方法提供了一种概念上简单的决策和控制问题的方法：使用学习来估计近似动力学模型，并将剩余工作委托给经典轨迹优化。然而，这种组合存在一些经验上的缺点，限制了实际中基于模型的方法的实用性。本论文的双重目的是研究这些缺点的原因并提出解决方法。在此过程中，我们强调了当代生成建模工具箱中的推理技术，包括束搜索、分类器引导采样和图像修复，如何重新解释为强化学习问题的可行规划策略。

    Deep model-based reinforcement learning methods offer a conceptually simple approach to the decision-making and control problem: use learning for the purpose of estimating an approximate dynamics model, and offload the rest of the work to classical trajectory optimization. However, this combination has a number of empirical shortcomings, limiting the usefulness of model-based methods in practice. The dual purpose of this thesis is to study the reasons for these shortcomings and to propose solutions for the uncovered problems. Along the way, we highlight how inference techniques from the contemporary generative modeling toolbox, including beam search, classifier-guided sampling, and image inpainting, can be reinterpreted as viable planning strategies for reinforcement learning problems.
    
[^91]: 应用众包技术丰富高等教育音乐知识库的方法和经验

    Employing Crowdsourcing for Enriching a Music Knowledge Base in Higher Education. (arXiv:2306.07310v1 [cs.HC])

    [http://arxiv.org/abs/2306.07310](http://arxiv.org/abs/2306.07310)

    本研究探讨了一项使用众包技术的计算机科学高等教育作业，鼓励学生丰富音乐元数据，并创建了可供机器学习模型用于音乐标记的开放性注释数据集。

    

    本文描述了一项使用众包技术作为计算机科学高等教育学生作业的方法和经验。利用支持文化遗产领域众包的平台，鼓励学生丰富与选定音乐曲目相关的元数据。该活动共有98名学生参加，为854首音乐曲目贡献了6400多个注释。同时还创建了一个可供机器学习模型用于音乐标记的开放性注释数据集。通过在线调查，该活动的结果和意见收集使我们能够得出一些有用的见解，了解将众包整合到计算机科学课程中的益处和挑战以及如何提高学生的学习体验。

    This paper describes the methodology followed and the lessons learned from employing crowdsourcing techniques as part of a homework assignment involving higher education students of computer science. Making use of a platform that supports crowdsourcing in the cultural heritage domain students were solicited to enrich the metadata associated with a selection of music tracks. The results of the campaign were further analyzed and exploited by students through the use of semantic web technologies. In total, 98 students participated in the campaign, contributing more than 6400 annotations concerning 854 tracks. The process also led to the creation of an openly available annotated dataset, which can be useful for machine learning models for music tagging. The campaign's results and the comments gathered through an online survey enable us to draw some useful insights about the benefits and challenges of integrating crowdsourcing into computer science curricula and how this can enhance student
    
[^92]: 用MT-Bench和Chatbot Arena评估以LLM为基础的聊天助手

    Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. (arXiv:2306.05685v1 [cs.CL])

    [http://arxiv.org/abs/2306.05685](http://arxiv.org/abs/2306.05685)

    研究使用强大的LLM作为评判员，通过引入两个基准测试来确认LLM评判员和人类偏好之间的一致性，并可调整聊天助手的模型架构和微调方法来提高其性能。

    

    评估基于大语言模型（LLM）的聊天助手会面临挑战，因为它们具有广泛的功能，而现有的基准无法衡量人类偏好。为了解决这个问题，我们探索使用强大的LLM作为评判员，在更加开放的问题上评估这些模型。我们研究了LLM作为评判员的使用和局限性，如位置和冗余偏见以及有限的推理能力，并提出解决方案来迁移其中一些问题。然后，我们通过引入两个基准测试（一个多轮问答集和一个众包竞技平台）来确认LLM评判员和人类偏好之间的一致性。我们的结果显示，像GPT-4这样的强大LLM评判员可以很好地匹配受控和众包人类偏好，达到了80％以上的一致性，与人类一致性水平相同。因此，LLM作为评判员是一种可扩展且可解释的逼近人类偏好的方式，而这些偏好是非常昂贵获取的。此外，我们证明，通过使用LLM作为评判员，可以通过调整聊天助手的模型架构和微调方法来提高其性能。

    Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, such as position and verbosity biases and limited reasoning ability, and propose solutions to migrate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we
    
[^93]: 价值函数即控制障碍函数：使用控制理论验证学习策略

    Your Value Function is a Control Barrier Function: Verification of Learned Policies using Control Theory. (arXiv:2306.04026v1 [cs.LG])

    [http://arxiv.org/abs/2306.04026](http://arxiv.org/abs/2306.04026)

    本研究将控制理论中的验证方法应用于强化学习中的价值函数，提出了新的度量方法以验证安全控制任务中的价值函数，并代表了通用、可伸缩和可验证的控制系统设计框架的第一步。

    

    尽管强化学习具有高度的通用性和可伸缩性，但验证策略行为的难度对于安全关键应用程序构成了挑战。为了解决这个问题，我们建议将控制理论中使用的验证方法应用于学习的价值函数。通过分析安全维护的简单任务结构，我们推导出将值函数与控制障碍函数相联系的原始定理。受此启发，我们提出了新的度量方法，以验证安全控制任务中的价值函数，并提出了改善学习的实际实施细节。除了提出证书学习的新方法外，我们的工作为RL策略解锁了丰富的控制理论验证方法，并代表了通用、可伸缩和可验证的控制系统设计框架的第一步。

    Although RL is highly general and scalable, the difficulty of verifying policy behaviours poses challenges for safety-critical applications. To remedy this, we propose to apply verification methods used in control theory to learned value functions. By analyzing a simple task structure for safety preservation, we derive original theorems linking value functions to control barrier functions. Inspired by this, we propose novel metrics for verification of value functions in safe control tasks, and practical implementation details that improve learning. Besides proposing a novel method for certificate learning, our work unlocks a wealth of verification methods in control theory for RL policies, and represents a first step towards a framework for general, scalable, and verifiable design of control systems.
    
[^94]: 具有抽象和细化的描述逻辑

    Description Logics with Abstraction and Refinement. (arXiv:2306.03717v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.03717](http://arxiv.org/abs/2306.03717)

    本文提出了一种扩展的描述逻辑（DLs）以支持多个抽象层次的知识表示，并证明了推理在该逻辑中是可判定的。

    

    本文提出了一种扩展的描述逻辑（DLs），该DLs有助于支持多个抽象层次的知识表示。我们的扩展DLs将抽象层级作为第一类公民，提供了明确的操作符，用于跨多个抽象层次进行概念和角色的抽象和细化，基于合取查询。我们证明了在得到的DLs族中进行推理是可判定的，而一些看似无害的变化实际上是不可判定的。我们还明确了我们逻辑和一些相关片段的复杂性。

    Ontologies often require knowledge representation on multiple levels of abstraction, but description logics (DLs) are not well-equipped for supporting this. We propose an extension of DLs in which abstraction levels are first-class citizens and which provides explicit operators for the abstraction and refinement of concepts and roles across multiple abstraction levels, based on conjunctive queries. We prove that reasoning in the resulting family of DLs is decidable while several seemingly harmless variations turn out to be undecidable. We also pinpoint the precise complexity of our logics and several relevant fragments.
    
[^95]: 使用基于真实世界生命体征数据的端到端多元时间序列聚类算法识别ICU患者子群

    Identifying Subgroups of ICU Patients Using End-to-End Multivariate Time-Series Clustering Algorithm Based on Real-World Vital Signs Data. (arXiv:2306.02121v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02121](http://arxiv.org/abs/2306.02121)

    本研究使用真实世界生命体征数据开发了一种端到端多元时间序列聚类算法，并发现不同子群之间存在着不同的ICU和医院死亡率风险。

    

    本研究使用MIMIC-IV数据库作为数据源，研究了动态、高频率、多元时间序列生命体征数据在ICU逗留期间的应用，包括体温、心率、平均血压、呼吸频率和SpO2。比较了各种聚类算法，并选择了一种名为Time2Feat的端到端多元时间序列聚类系统结合K-Means作为最有效的ICU患者聚类方法。在聚类分析中，使用了2008年至2016年期间收治的8,080名患者的数据进行模型开发，使用了2017年至2019年期间收治的2,038名患者的数据进行模型验证。通过分析不同类别之间的临床死亡预后差异，发现了不同亚组之间ICU死亡率和医院死亡率的风险差异。此外，本研究还可视化了生命体征变化的轨迹。这项研究的发现为ICU患者提供了有价值的洞察。

    This study employed the MIMIC-IV database as data source to investigate the use of dynamic, high-frequency, multivariate time-series vital signs data, including temperature, heart rate, mean blood pressure, respiratory rate, and SpO2, monitored first 8 hours data in the ICU stay. Various clustering algorithms were compared, and an end-to-end multivariate time series clustering system called Time2Feat, combined with K-Means, was chosen as the most effective method to cluster patients in the ICU. In clustering analysis, data of 8,080 patients admitted between 2008 and 2016 was used for model development and 2,038 patients admitted between 2017 and 2019 for model validation. By analyzing the differences in clinical mortality prognosis among different categories, varying risks of ICU mortality and hospital mortality were found between different subgroups. Furthermore, the study visualized the trajectory of vital signs changes. The findings of this study provide valuable insights into the p
    
[^96]: 超越一个模型适用于所有领域：大型语言模型的领域专门化综述

    Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models. (arXiv:2305.18703v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.18703](http://arxiv.org/abs/2305.18703)

    本文综述了大型语言模型的领域专门化，包括动机、挑战、方法论和评估指标。此外，还提供了一个特定领域任务和数据集的分类法，对现有的领域自适应和定制技术进行了详细比较，并广泛讨论了这一领域中的未解决问题和未来的发展方向。

    

    大型语言模型（LLM）已经大大推动了自然语言处理（NLP）领域的发展，为广泛应用提供了高度实用、任务无关的基础。LLMs 作为通用任务求解器的巨大潜力，促使人们将其用于特定领域，如医疗保健、金融和教育，并将其用作助手甚至替代特定领域的专家和工具。但是，将LLMs直接应用于特定领域中的复杂问题会遇到许多困难，包括领域数据的异质性、领域知识的复杂性、领域目标的独特性以及约束的多样性。为了填补这种差距，最近几年进行了急剧增加的研究和实践致力于大型语言模型的领域专门化，然而这方面的研究尚未被系统地总结。在这篇综述中，我们对LLMs的领域专门化进行了全面概述，包括动机、挑战、方法论和评估指标。此外，我们提供了一个特定领域任务和数据集的分类法，对现有的领域自适应和定制技术进行了详细比较，并广泛讨论了这一领域中的未解决问题和未来的发展方向。

    Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a ``chatbot'', and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted in very recent years on the domain specialization of LLMs, which, howe
    
[^97]: 基于深度强化学习的地面车辆在越野地形环境下的多目标路径规划

    Deep Reinforcement Learning-based Multi-objective Path Planning on the Off-road Terrain Environment for Ground Vehicles. (arXiv:2305.13783v1 [cs.RO])

    [http://arxiv.org/abs/2305.13783](http://arxiv.org/abs/2305.13783)

    本文提出了一种基于深度强化学习的2.5D多目标路径规划方法，可以高效地找到距离和能耗达到良好权衡的路径。

    

    由于上坡和下坡之间的能耗效率差异巨大，在复杂的越野地形环境（2.5D地图）上，最短路径不一定是能耗最少的路径。对于任何能源敏感的车辆来说，实现距离和能耗在2.5D路径规划上良好的权衡具有重要意义。本文提出了一种基于深度强化学习的2.5D多目标路径规划方法（DMOP）。DMOP可以通过三个步骤高效地找到所需路径：(1)将高分辨率的2.5D地图转换为小尺寸地图。(2)使用训练好的深度Q网络（DQN）在小尺寸地图上找到所需路径。(3)使用路径增强方法将计划路径构建到原始高分辨率地图上。此外，还应用了模仿学习方法和奖励塑造理论来训练DQN。奖励函数结合了地形、距离和边界的信息。

    Due to the energy-consumption efficiency between up-slope and down-slope is hugely different, a path with the shortest length on a complex off-road terrain environment (2.5D map) is not always the path with the least energy consumption. For any energy-sensitive vehicles, realizing a good trade-off between distance and energy consumption on 2.5D path planning is significantly meaningful. In this paper, a deep reinforcement learning-based 2.5D multi-objective path planning method (DMOP) is proposed. The DMOP can efficiently find the desired path with three steps: (1) Transform the high-resolution 2.5D map into a small-size map. (2) Use a trained deep Q network (DQN) to find the desired path on the small-size map. (3) Build the planned path to the original high-resolution map using a path enhanced method. In addition, the imitation learning method and reward shaping theory are applied to train the DQN. The reward function is constructed with the information of terrain, distance, border. S
    
[^98]: RAMiT：轻量级图像恢复的互惠式注意力混合Transformer

    RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration. (arXiv:2305.11474v1 [cs.CV])

    [http://arxiv.org/abs/2305.11474](http://arxiv.org/abs/2305.11474)

    本文提出了轻量级图像恢复的互惠式注意力混合Transformer（RAMiT）。通过使用双向注意力以及一种新的数据增强类型——强度掩码，有效地提高了恢复效果，同时大大减少了参数数量。

    

    尽管近年来许多工作在图像恢复（IR）领域取得了进展，但它们往往面临参数过多的问题。另一个问题是，大多数基于Transformer的IR方法只依靠本地或全局特征，导致接受域有限或存在参数不足的问题。为了解决这些问题，我们提出了一种轻量级IR网络：互惠注意力混合Transformer（RAMiT）。它采用我们提出的维度互惠注意力混合Transformer（D-RAMiT）块，在使用不同数量的多头并行计算双向（空间和通道）自注意力的情况下。双向关注帮助彼此弥补对方的缺点，然后混合。此外，我们引入了一种分层互惠注意力混合（H-RAMi）层，它补偿像素级信息丢失并利用语义信息，同时保持高效的分层结构。此外，在IR任务中，我们重新审视了数据增强策略并提出了一种新的数据增强类型——强度掩码，以提高所提出模型的鲁棒性。广泛的实验表明，在各种IR任务中，包括图像去噪、图像去模糊和JPEG图像去块，我们的所提出的方法在大大减少参数的情况下，优于现有最先进的方法。

    Although many recent works have made advancements in the image restoration (IR) field, they often suffer from an excessive number of parameters. Another issue is that most Transformer-based IR methods focus only on either local or global features, leading to limited receptive fields or deficient parameter issues. To address these problems, we propose a lightweight IR network, Reciprocal Attention Mixing Transformer (RAMiT). It employs our proposed dimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which compute bi-dimensional (spatial and channel) self-attentions in parallel with different numbers of multi-heads. The bi-dimensional attentions help each other to complement their counterpart's drawbacks and are then mixed. Additionally, we introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that compensates for pixel-level information losses and utilizes semantic information while maintaining an efficient hierarchical structure. Furthermore, we revisit 
    
[^99]: ANTONIO:面向NLP验证的系统化基准生成方法

    ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v1 [cs.CL])

    [http://arxiv.org/abs/2305.04003](http://arxiv.org/abs/2305.04003)

    本文介绍了一种名为ANTONIO的Python库，它基于抽象解释方法提供了一种实用的方法和启发式规则，以便为自然语言处理（NLP）数据集和模型生成已知验证方法的基准。因为其普遍适用性，这项工作将为将NLP验证问题纳入神经网络验证比赛开辟新的可能性，并在NLP问题中普及这一方向。

    

    自然语言处理（NLP）中使用的机器学习模型的验证被认为是一个难题。现有的神经网络验证方法常用于计算机视觉和其他数字数据集，但并不适用于NLP。本研究探讨了造成这一问题的技术原因，并在此基础上提出了实用的方法和启发式规则，以便将NLP数据集和模型准备为适合基于抽象解释的已知验证方法。我们将这些方法实现为一个名为ANTONIO的Python库，该库连接到神经网络验证器ERAN和Marabou。我们使用一个名为R-U-A-Robot的NLP数据集对工具进行了评估，该数据集被提议作为验证具有法律重要性的NLP应用的基准。我们希望，由于其普遍适用性，这项工作将为将NLP验证问题纳入神经网络验证比赛开辟新的可能性，并在NLP问题中普及这一方向。

    Verification of machine learning models used in Natural Language Processing (NLP) is known to be a hard problem. In particular, many known neural network verification methods that work for computer vision and other numeric datasets do not work for NLP. Here, we study technical reasons that underlie this problem. Based on this analysis, we propose practical methods and heuristics for preparing NLP datasets and models in a way that renders them amenable to known verification methods based on abstract interpretation. We implement these methods as a Python library called ANTONIO that links to the neural network verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP applications. We hope that, thanks to its general applicability, this work will open novel possibilities for including NLP verification problems into neural network verification competitions, and will popularise NLP problems withi
    
[^100]: 使用假想回顾进行不确定机器伦理决策

    Uncertain Machine Ethical Decisions Using Hypothetical Retrospection. (arXiv:2305.01424v1 [cs.AI])

    [http://arxiv.org/abs/2305.01424](http://arxiv.org/abs/2305.01424)

    本文提出了一种新的机器伦理推理方法，使用假想回顾论证程序，允许使用各种哲学理论进行伦理推理，可以考虑概率和不确定性，应用于一个自主图书馆系统用例中进行测试。

    

    本文提出使用由Sven Hansson开发的假想回顾论证程序，以考虑概率和不确定性，从哲学的角度出发改进现有的机器伦理推理方法，以与人类产生共鸣。该程序将行动表示为一组可能结果的分支，每个分支具有状态、效用和数值或诗意概率估计。行动的选择基于从它们的分支的角度比较倾向于行动的一组论证，即使这些分支导致了不良结果也一样。这种使用论证的方法允许使用各种哲学理论进行伦理推理，可能可以灵活地组合使用。我们将该方法应用于一个自主图书馆系统用例中，分别独立或同时应用结果主义和义务论的伦理理论，并引入一个初步的框架，似乎满足了各种要求。

    We propose the use of the hypothetical retrospection argumentation procedure, developed by Sven Hansson, to improve existing approaches to machine ethical reasoning by accounting for probability and uncertainty from a position of Philosophy that resonates with humans. Actions are represented with a branching set of potential outcomes, each with a state, utility, and either a numeric or poetic probability estimate. Actions are chosen based on comparisons between sets of arguments favouring actions from the perspective of their branches, even those branches that led to an undesirable outcome. This use of arguments allows a variety of philosophical theories for ethical reasoning to be used, potentially in flexible combination with each other. We implement the procedure, applying consequentialist and deontological ethical theories, independently and concurrently, to an autonomous library system use case. We introduce a a preliminary framework that seems to meet the varied requirements of a
    
[^101]: 元多重图搜索：重新思考异构信息网络上的元结构

    Meta-multigraph Search: Rethinking Meta-structure on Heterogeneous Information Networks. (arXiv:2304.11574v1 [cs.AI])

    [http://arxiv.org/abs/2304.11574](http://arxiv.org/abs/2304.11574)

    本文提出了一种称为元多重图的新概念来代替元图和元路径用于处理异构信息网络中的信息聚合，同时提出了稳定的可微分搜索方法来优化元多重图以适应不同的任务和HIN，此外，还引入了复杂到简洁的C2C元多重图来降低冗余信息传播的影响。

    

    元结构被广泛应用于定义在异构信息网络（HIN）中哪些邻居的子集来聚合信息。在本文中，我们调查了现有的元结构，包括元路径和元图，并观察到它们最初是手动设计的固定模式，因此不足以编码不同HIN上的各种丰富语义信息。通过反思它们的限制，我们定义了一个称为元多重图的新概念，作为元图的更具表现力和灵活的泛化，并提出了一种稳定可微分的搜索方法，以自动优化特定HIN和任务的元多重图。由于元多重图的灵活性可能会传播冗余的消息，因此我们进一步介绍了一种从复杂到简洁的C2C元多重图，它沿着元多重图的深度从复杂到简洁地传播消息。此外，我们观察到可微分搜索通常会遭受不稳定的搜索和显着的g

    Meta-structures are widely used to define which subset of neighbors to aggregate information in heterogeneous information networks (HINs). In this work, we investigate existing meta-structures, including meta-path and meta-graph, and observe that they are initially designed manually with fixed patterns and hence are insufficient to encode various rich semantic information on diverse HINs. Through reflection on their limitation, we define a new concept called meta-multigraph as a more expressive and flexible generalization of meta-graph, and propose a stable differentiable search method to automatically optimize the meta-multigraph for specific HINs and tasks. As the flexibility of meta-multigraphs may propagate redundant messages, we further introduce a complex-to-concise (C2C) meta-multigraph that propagates messages from complex to concise along the depth of meta-multigraph. Moreover, we observe that the differentiable search typically suffers from unstable search and a significant g
    
[^102]: CAR-DESPOT: 针对混杂环境下的机器人的因果关系在线POMDP规划

    CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments. (arXiv:2304.06848v1 [cs.RO])

    [http://arxiv.org/abs/2304.06848](http://arxiv.org/abs/2304.06848)

    本文提出了一种新的因果关系在线POMDP规划方法CAR-DESPOT，使用因果建模和推理来消除未测量混淆变量引起的错误，并在混杂环境中表现优异。

    

    在现实环境中工作的机器人必须考虑随机行为的可能结果，并根据真实的世界状态的部分观察进行决策。因果混淆的问题是进行准确和强健的行为预测的主要挑战。部分可观察的马尔可夫决策过程(POMDP)是一种广泛使用的框架，用于模拟这些随机和部分可观测的决策问题。然而，由于缺乏明确的因果语义，POMDP规划方法容易受到混淆偏差的影响，在未观察到混杂变量的情况下，可能会产生表现不佳的策略。本文提出了一种新的因果关系在线POMDP规划方法，使用因果建模和推理来消除未测量混淆变量引起的错误。我们进一步提出了一种从观测数据中学习因果模型的方法，以在我们的方法中使用。实验结果表明，我们的方法CAR-DESPOT在混杂环境中比现有的最先进的POMDP规划程序表现显著更好。

    Robots operating in real-world environments must reason about possible outcomes of stochastic actions and make decisions based on partial observations of the true world state. A major challenge for making accurate and robust action predictions is the problem of confounding, which if left untreated can lead to prediction errors. The partially observable Markov decision process (POMDP) is a widely-used framework to model these stochastic and partially-observable decision-making problems. However, due to a lack of explicit causal semantics, POMDP planning methods are prone to confounding bias and thus in the presence of unobserved confounders may produce underperforming policies. This paper presents a novel causally-informed extension of "anytime regularized determinized sparse partially observable tree" (AR-DESPOT), a modern anytime online POMDP planner, using causal modelling and inference to eliminate errors caused by unmeasured confounder variables. We further propose a method to lear
    
[^103]: GPT检测器对非英语母语的作者存在偏见。

    GPT detectors are biased against non-native English writers. (arXiv:2304.02819v1 [cs.CL])

    [http://arxiv.org/abs/2304.02819](http://arxiv.org/abs/2304.02819)

    该研究发现，GPT检测器对非英语母语作者存在偏见，容易将其内容错误地分类为AI生成的内容。此外，简单的提示策略可以缓解这种偏见，同时规避GPT检测器，这表明GPT检测器可能会惩罚具有受限语言表达能力的作者。

    

    生成语言模型的快速推广带来了数字通信方面的实质性进展，同时也引发了AI生成内容潜在误用的担忧。虽然已经提出了许多检测方法来区分AI和人类生成的内容，但这些检测器的公平性和鲁棒性仍未得到充分探讨。在这项研究中，我们使用来自英语母语和非英语母语作者的写作样本评估了几种广泛使用的GPT检测器的性能表现。我们的研究发现，这些检测器持续将非英语母语的写作样本错误地分类为AI生成的内容，而原生写作样本则能够被准确识别。此外，我们证明了简单的提示策略不仅可以缓解这种偏见，而且还可以有效地规避GPT检测器，这表明GPT检测器可能无意中惩罚具有受限语言表达能力的作者。我们的研究结果呼吁进行更广泛的讨论。

    The rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of AI-generated content. Although numerous detection methods have been proposed to differentiate between AI and human-generated content, the fairness and robustness of these detectors remain underexplored. In this study, we evaluate the performance of several widely-used GPT detectors using writing samples from native and non-native English writers. Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass GPT detectors, suggesting that GPT detectors may unintentionally penalize writers with constrained linguistic expressions. Our results call for a broader conversati
    
[^104]: 语言特定的情绪概念知识表示对情绪推断的因果支持

    Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference. (arXiv:2302.09582v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.09582](http://arxiv.org/abs/2302.09582)

    本研究通过操纵大型语言模型中的语言衍生的情绪概念知识表示，探讨了语言是否会因果支持情绪推断。实验结果显示，属性特定的神经元操纵导致情绪推断任务的性能下降，这与人类心理空间中不同属性的重要性有关。这些发现为支持基于语言的情绪推断机制提供了因果证据，并凸显了情绪概念知识的贡献。

    

    在情绪科学中，如何理解语言支持情绪推断仍然是一个争议的话题。本研究通过操纵大型语言模型中的语言衍生的情绪概念知识表示，调查了语言是否会因果支持情绪推断。使用提示技术，发现了14个情绪概念的属性由不同的人工神经元群体表示。通过操纵这些属性相关的神经元，与随机操纵相比，大多数情绪推断任务的表现出现了下降。属性特定的表现下降与人类心理空间中不同属性的重要性有关。我们的发现提供了支持基于语言的情绪推断机制的因果证据，并强调了情绪概念知识的贡献。

    Understanding how language supports emotion inference remains a topic of debate in emotion science. The present study investigated whether language-derived emotion-concept knowledge would causally support emotion inference by manipulating the language-specific knowledge representations in large language models. Using the prompt technique, 14 attributes of emotion concepts were found to be represented by distinct artificial neuron populations. By manipulating these attribute-related neurons, the majority of the emotion inference tasks showed performance deterioration compared to random manipulations. The attribute-specific performance deterioration was related to the importance of different attributes in human mental space. Our findings provide causal evidence in support of a language-based mechanism for emotion inference and highlight the contributions of emotion-concept knowledge.
    
[^105]: 一种适用于所有时间序列的Transformer：表示和训练具有时间相关的异构表格数据

    One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data. (arXiv:2302.06375v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06375](http://arxiv.org/abs/2302.06375)

    本研究提出了一种Transformer架构，用于表示具有时间相关的异构表格数据，通过使用一组频率函数来表示数值特征，并采用唯一的损失函数进行统一训练。

    

    近年来，将深度学习技术应用于表格数据的兴趣日益增长，以复制其他人工智能领域在这一结构化领域的成功。特别有趣的是，表格数据具有时间依赖性，例如金融交易。然而，表格值的异质性，其中类别元素与数值项混合，使得这种适应变得困难。在本文中，我们提出了一种Transformer架构来表示异构的时间相关的表格数据，数值特征使用一组频率函数表示，并且整个网络使用唯一的损失函数进行统一训练。

    There is a recent growing interest in applying Deep Learning techniques to tabular data, in order to replicate the success of other Artificial Intelligence areas in this structured domain. Specifically interesting is the case in which tabular data have a time dependence, such as, for instance financial transactions. However, the heterogeneity of the tabular values, in which categorical elements are mixed with numerical items, makes this adaptation difficult. In this paper we propose a Transformer architecture to represent heterogeneous time-dependent tabular data, in which numerical features are represented using a set of frequency functions and the whole network is uniformly trained with a unique loss function.
    
[^106]: 图解化：利用图解型AI解释对假设性演绎推理的理性化

    Diagrammatization: Rationalizing with diagrammatic AI explanations for abductive-deductive reasoning on hypotheses. (arXiv:2302.01241v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.01241](http://arxiv.org/abs/2302.01241)

    本文提出了一种图解化的方法，以支持可解释的人工智能，通过图解型和假设性推理，缩小可解释性差距。通过临床应用研究和建模研究，我们发现DiagramNet不仅能提供忠实的杂音形状解释，还具有较好的预测性能，而且图解型解释在临床相关的情况下更受推崇。

    

    许多可解释的人工智能（XAI）可视化工具已经被开发出来，但它们通常需要用户进一步推理来解释。我们认为，XAI应该支持图解型和假设性推理，以便AI能够进行假设生成和评估，从而减少可解释性差距。我们提出了图解化方法，以i)进行Peircean推导-演绎推理，ii)遵循领域惯例，和iii)用图示或语言进行解释。我们在临床应用领域实现了DiagramNet，以预测心脏听诊中的心脏诊断，并用基于形状的杂音图解进行解释。在建模研究中，我们发现DiagramNet不仅提供了忠实的杂音形状解释，而且比基线模型具有更好的预测性能。我们进一步通过医学生的定性用户研究展示了图解型解释的可理解性和可信度，并表明在临床相关的情况下，图解式解释比其他方式更受推崇。

    Many visualizations have been developed for explainable AI (XAI), but they often require further reasoning by users to interpret. We argue that XAI should support diagrammatic and abductive reasoning for the AI to perform hypothesis generation and evaluation to reduce the interpretability gap. We propose Diagrammatization to i) perform Peircean abductive-deductive reasoning, ii) follow domain conventions, and iii) explain with diagrams visually or verbally. We implemented DiagramNet for a clinical application to predict cardiac diagnoses from heart auscultation, and explain with shape-based murmur diagrams. In modeling studies, we found that DiagramNet not only provides faithful murmur shape explanations, but also has better prediction performance than baseline models. We further demonstrate the interpretability and trustworthiness of diagrammatic explanations in a qualitative user study with medical students, showing that clinically-relevant, diagrammatic explanations are preferred ov
    
[^107]: 单个分子基础模型的双向结构和性质的生成

    Bidirectional Generation of Structure and Properties Through a Single Molecular Foundation Model. (arXiv:2211.10590v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10590](http://arxiv.org/abs/2211.10590)

    本论文提出了一种新颖的多模态分子预训练模型，通过将结构和生化性质的模态结合，使得模型能够将分子的结构和性质之间的双向信息联系起来，并能够处理多模态和单模态的下游任务。

    

    人工智能中大型基础模型的成功促使了化学预训练模型的出现。尽管近年来对提供下游任务的信息表示的大分子预训练模型的兴趣日益增长，但分子领域的多模态预训练方法尝试有限。为了解决这个问题，我们提出了一种新颖的多模态分子预训练模型，从多模态学习技术的最新进展中汲取灵感，将结构和生化性质的模态结合起来。我们提出的模型管道处理数据并根据共同的嵌入空间对齐结构/性质特征，使得模型能够将分子的结构和性质之间的双向信息联系起来。这些贡献产生了协同的知识，使我们能够通过单一模型处理多模态和单模态的下游任务。通过大量实验证明，我们的模型足以生成具有双向结构和性质的分子。

    The recent success of large foundation models in artificial intelligence has prompted the emergence of chemical pre-trained models. Despite the growing interest in large molecular pre-trained models that provide informative representations for downstream tasks, attempts for multimodal pre-training approaches on the molecule domain were limited. To address this, we present a novel multimodal molecular pre-trained model that incorporates the modalities of structure and biochemical properties, drawing inspiration from recent advances in multimodal learning techniques. Our proposed model pipeline of data handling and training objectives aligns the structure/property features in a common embedding space, which enables the model to regard bidirectional information between the molecules' structure and properties. These contributions emerge synergistic knowledge, allowing us to tackle both multimodal and unimodal downstream tasks through a single model. Through extensive experiments, we demons
    
[^108]: 图像对机器的记忆性有何影响？

    What Images are More Memorable to Machines?. (arXiv:2211.07625v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.07625](http://arxiv.org/abs/2211.07625)

    本论文研究了如何测量和预测图像对机器的记忆性，并发现“复杂”图像通常对机器记忆更加深刻。通过详细分析和实验验证，我们提出了机器记忆性的概念，并为机器记忆和视觉数据的研究方向开辟了新的可能性。

    

    本论文研究了如何测量和预测图像对模式识别机器的记忆性，以探索机器智能。首先，我们提出了一个自监督的机器记忆量化流程，称为“MachineMem measurer”，用于收集图像的机器记忆性分数。与人类类似，机器也倾向于记忆某些类型的图像，但机器和人类记忆的图像类型是不同的。通过深入分析和全面的可视化，我们逐渐揭示了“复杂”图像通常对机器记忆更加深刻。我们进一步在11台不同的机器（从线性分类器到现代ViTs）和9种预训练方法上进行了广泛实验，以分析和理解机器记忆。本研究提出了机器记忆性的概念，并在机器记忆和视觉数据的交叉界面上开辟了一条新的研究方向。

    This paper studies the problem of measuring and predicting how memorable an image is to pattern recognition machines, as a path to explore machine intelligence. Firstly, we propose a self-supervised machine memory quantification pipeline, dubbed ``MachineMem measurer'', to collect machine memorability scores of images. Similar to humans, machines also tend to memorize certain kinds of images, whereas the types of images that machines and humans memorize are different. Through in-depth analysis and comprehensive visualizations, we gradually unveil that``complex" images are usually more memorable to machines. We further conduct extensive experiments across 11 different machines (from linear classifiers to modern ViTs) and 9 pre-training methods to analyze and understand machine memory. This work proposes the concept of machine memorability and opens a new research direction at the interface between machine memory and visual data.
    
[^109]: 利用解释的力量进行增量训练：基于LIME的方法

    Harnessing the Power of Explanations for Incremental Training: A LIME-Based Approach. (arXiv:2211.01413v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01413](http://arxiv.org/abs/2211.01413)

    本研究提出了一种基于LIME的方法，利用解释来改善神经网络模型性能。通过引入自定义加权损失，将真实LIME解释与模型预测的LIME解释之间的距离考虑在内，帮助模型更好地泛化。此外，该研究还提出了一种适用于实际训练场景的解决方案，以帮助模型顺序学习而不丢失先前数据分布信息。

    

    神经网络预测的可解释性对于理解特征重要性并获得可解释的洞察力至关重要。然而，对神经网络结果的解释大多局限于可视化，并且很少有研究将这些解释用作改进模型性能的反馈。本研究将模型的解释反馈到前馈训练中，以帮助模型更好地泛化。为此，提出了一种自定义加权损失，其中权重是通过考虑真实LIME（局部可解释的模型无关解释）解释和模型预测的LIME解释之间的欧式距离来生成的。此外，在实际的训练场景中，开发一个能够帮助模型顺序学习而不丢失先前数据分布信息的解决方案是必要的，因为不会一次性提供所有的训练数据。因此，该框架将自定义加权损失与弹性训练相结合。

    Explainability of neural network prediction is essential to understand feature importance and gain interpretable insight into neural network performance. However, explanations of neural network outcomes are mostly limited to visualization, and there is scarce work that looks to use these explanations as feedback to improve model performance. In this work, model explanations are fed back to the feed-forward training to help the model generalize better. To this extent, a custom weighted loss where the weights are generated by considering the Euclidean distances between true LIME (Local Interpretable Model-Agnostic Explanations) explanations and model-predicted LIME explanations is proposed. Also, in practical training scenarios, developing a solution that can help the model learn sequentially without losing information on previous data distribution is imperative due to the unavailability of all the training data at once. Thus, the framework incorporates the custom weighted loss with Elas
    
[^110]: 对比解码：将开放式文本生成视为优化问题

    Contrastive Decoding: Open-ended Text Generation as Optimization. (arXiv:2210.15097v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.15097](http://arxiv.org/abs/2210.15097)

    对比解码是一种可靠的解码方法，它通过优化对比目标来生成合理且高质量的文本，与仅使用较大的语言模型进行解码相比，对比解码能够避免短而重复的文本和不连贯的文本，并适用于不同的模型规模。

    

    鉴于语言模型（LM），最大概率是开放式生成的较差解码目标，因为它会产生短而重复的文本。另一方面，采样往往会产生与原始主题偏离的不连贯文本。我们提出了对比解码（CD），这是一种可靠的解码方法，它在满足合理性约束条件的前提下优化对比目标。对比目标返回一个大型LM（被称为专家，例如OPT-13B）和一个小型LM（被称为业余者，例如OPT-125M）之间的似然差异，并且约束条件确保输出是合理的。CD的灵感来自于这样一个事实，即较大的LM（例如重复、不连贯）在较小的LM中更为普遍，并且这种差异表明哪些文本应优先考虑。CD不需要额外的培训，并且比仅从较大的LM进行解码的情况下产生更高质量的文本。它还适用于不同的模型规模（OPT-13B和GPT2-1.5B）。

    Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and 
    
[^111]: 通过迭代的自适应有监督对比学习进行多实例学习

    Multiple Instance Learning via Iterative Self-Paced Supervised Contrastive Learning. (arXiv:2210.09452v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.09452](http://arxiv.org/abs/2210.09452)

    本论文提出了一个新的框架，通过迭代的自适应有监督对比学习，解决了多实例学习中的类别不平衡问题，并在医学数据集上取得了良好的结果。

    

    在只有袋级标签可用的情况下，学习单个实例的表示是多实例学习中的一个基本挑战。最近的研究表明，使用对比自我监督学习（CSSL）可以取得有希望的结果，CSSL学习将对应于两个不同随机选择的实例的表示推开。然而，在真实世界的应用中，例如医学图像分类，通常存在类别不平衡，因此随机选择的实例大部分属于同一个多数类别，这使得CSSL无法学习类间差异。为了解决这个问题，我们提出了一个新的框架，称为迭代自适应有监督对比学习多实例表示（ItS2CLR），通过利用从包级标签派生的实例级伪标签来改善学习到的表示。该框架采用了一种新的自适应有监督对比学习多实例表示的策略来确保伪标签的准确性。我们在三个医学数据集上评估了ItS2CLR。

    Learning representations for individual instances when only bag-level labels are available is a fundamental challenge in multiple instance learning (MIL). Recent works have shown promising results using contrastive self-supervised learning (CSSL), which learns to push apart representations corresponding to two different randomly-selected instances. Unfortunately, in real-world applications such as medical image classification, there is often class imbalance, so randomly-selected instances mostly belong to the same majority class, which precludes CSSL from learning inter-class differences. To address this issue, we propose a novel framework, Iterative Self-paced Supervised Contrastive Learning for MIL Representations (ItS2CLR), which improves the learned representation by exploiting instance-level pseudo labels derived from the bag-level labels. The framework employs a novel self-paced sampling strategy to ensure the accuracy of pseudo labels. We evaluate ItS2CLR on three medical datase
    
[^112]: 分布自适应元强化学习

    Distributionally Adaptive Meta Reinforcement Learning. (arXiv:2210.03104v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03104](http://arxiv.org/abs/2210.03104)

    本论文介绍了一种基于自适应分布鲁棒性的元强化学习算法框架，该框架能够在测试任务分布发生变化时行为适应，提供了对分布转变的改善遗憾的能力。

    

    元强化学习算法提供了一种数据驱动的方式来获取能够快速适应多个具有不同奖励或动力学函数的任务的策略。然而，学习得到的元策略通常只在其训练时的精确任务分布上有效，在测试时的奖励或过渡动力学的分布发生变化时会变得困难。在这项工作中，我们开发了一个框架，用于元强化学习算法在任务空间中适应性地应对测试时的分布转变。我们的框架以自适应的分布鲁棒性方法为核心，训练一群具有不同程度分布转变鲁棒性的元策略。当在可能发生分布转变的测试任务分布上评估时，我们可以选择具有最合适鲁棒性水平的元策略，并使用它进行快速适应。我们在理论上证明了我们的框架在分布转变下能够改善遗憾，并通过实验证明了我们方法的有效性。

    Meta-reinforcement learning algorithms provide a data-driven way to acquire policies that quickly adapt to many tasks with varying rewards or dynamics functions. However, learned meta-policies are often effective only on the exact task distribution on which they were trained and struggle in the presence of distribution shift of test-time rewards or transition dynamics. In this work, we develop a framework for meta-RL algorithms that are able to behave appropriately under test-time distribution shifts in the space of tasks. Our framework centers on an adaptive approach to distributional robustness that trains a population of meta-policies to be robust to varying levels of distribution shift. When evaluated on a potentially shifted test-time distribution of tasks, this allows us to choose the meta-policy with the most appropriate level of robustness, and use it to perform fast adaptation. We formally show how our framework allows for improved regret under distribution shift, and empirica
    
[^113]: 神经网络中的多义性和容量

    Polysemanticity and Capacity in Neural Networks. (arXiv:2210.01892v3 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2210.01892](http://arxiv.org/abs/2210.01892)

    该论文通过分析特征容量来理解神经网络中的多义性现象，发现在最优的容量分配下，神经网络倾向于单义地表示重要特征，多义地表示次重要特征，并忽略最不重要的特征。多义性现象在输入具有更高的峰度或稀疏性时更为普遍，并且在某些体系结构中比其他体系结构更为普遍。此外，作者还发现了嵌入空间中的分块半正交结构，不同模型中的分块大小不同，突出了模型体系结构的影响。

    

    神经网络中的单个神经元通常代表无关特征的混合。这种现象称为多义性，使解释神经网络变得更加困难，因此我们的目标是理解其原因。我们提出通过特征容量的视角来理解这一现象，特征容量是每个特征在嵌入空间中占用的分形维度。我们展示了在一个玩具模型中，最优的容量分配倾向于单义地表示最重要的特征，多义地表示次重要特征（与其对损失的影响成比例），并完全忽略最不重要的特征。当输入具有更高的峰度或稀疏性时，多义性更为普遍，并且在某些体系结构中比其他体系结构更为普遍。在得到最优容量分配后，我们进一步研究了嵌入空间的几何结构。我们发现了一个分块半正交的结构，不同模型中的分块大小不同，突出了模型体系结构的影响。

    Individual neurons in neural networks often represent a mixture of unrelated features. This phenomenon, called polysemanticity, can make interpreting neural networks more difficult and so we aim to understand its causes. We propose doing so through the lens of feature \emph{capacity}, which is the fractional dimension each feature consumes in the embedding space. We show that in a toy model the optimal capacity allocation tends to monosemantically represent the most important features, polysemantically represent less important features (in proportion to their impact on the loss), and entirely ignore the least important features. Polysemanticity is more prevalent when the inputs have higher kurtosis or sparsity and more prevalent in some architectures than others. Given an optimal allocation of capacity, we go on to study the geometry of the embedding space. We find a block-semi-orthogonal structure, with differing block sizes in different models, highlighting the impact of model archit
    
[^114]: GreenKGC：一种轻量级的知识图谱补全方法

    GreenKGC: A Lightweight Knowledge Graph Completion Method. (arXiv:2208.09137v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.09137](http://arxiv.org/abs/2208.09137)

    GreenKGC是一种轻量级的知识图谱补全方法，通过三个模块（表示学习、特征修剪和决策学习）提取判别性的知识图谱特征，并使用分类器和负采样进行准确的预测。在低维度下，GreenKGC能够在大多数数据集上优于其他现有方法，并具有竞争性性能。

    

    知识图谱补全（KGC）旨在发现知识图谱中实体之间的缺失关系。大多数先前的KGC工作集中在通过简单的评分函数学习实体和关系的嵌入。然而，更高维度的嵌入空间通常需要更好的推理能力，这导致模型尺寸更大，限制了在现实世界问题（例如大规模的知识图谱或移动/边缘计算）中的适用性。本文提出了一种轻量级模块化的KGC解决方案，称为GreenKGC，来解决这个问题。GreenKGC包括三个模块：表示学习，特征修剪和决策学习，通过分类器和负采样提取判别性的知识图谱特征，并对缺失关系做出准确的预测。实验结果表明，在低维度下，GreenKGC可以在大多数数据集上胜过其他先进方法。此外，低维的GreenKGC可以实现竞争性甚至更好的性能。

    Knowledge graph completion (KGC) aims to discover missing relationships between entities in knowledge graphs (KGs). Most prior KGC work focuses on learning embeddings for entities and relations through a simple scoring function. Yet, a higher-dimensional embedding space is usually required for a better reasoning capability, which leads to a larger model size and hinders applicability to real-world problems (e.g., large-scale KGs or mobile/edge computing). A lightweight modularized KGC solution, called GreenKGC, is proposed in this work to address this issue. GreenKGC consists of three modules: representation learning, feature pruning, and decision learning, to extract discriminant KG features and make accurate predictions on missing relationships using classifiers and negative sampling. Experimental results demonstrate that, in low dimensions, GreenKGC can outperform SOTA methods in most datasets. In addition, low-dimensional GreenKGC can achieve competitive or even better performance 
    
[^115]: 公平算法设计：公平且有效的机器调度

    Fair Algorithm Design: Fair and Efficacious Machine Scheduling. (arXiv:2204.06438v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2204.06438](http://arxiv.org/abs/2204.06438)

    本研究证明，在允许微小偏见的情况下，可以设计出既几乎完全公平又具有常数倍效益比率的机器调度算法。

    

    在许多实际例子中，自动决策算法导致了偏见，这引发了对公平算法设计的强烈关注。然而，公平和效益之间往往存在对立：公平算法可能提供较低的社会福利解决方案，而优化福利的算法可能非常不公平。这个问题在机器调度问题中得到了体现，对于n个作业，任何公平解的社会福利可能比最优福利差一个$\Omega(n)$的因子。本文证明了，如果我们允许存在微不足道的偏见，公平和效能之间的对立是可以克服的：存在一些算法既是“几乎完全公平的”，又具有常数倍的效益比率，也就是说，它们能够保证输出的解决方案在社会福利上与最优福利之间相差一个常数倍的因子。具体来说，对于任何$\epsilon>0$，存在具有效益比率$\Theta(\frac{1}{\epsilon})$的机制。

    Motivated by a plethora of practical examples where bias is induced by automated-decision making algorithms, there has been strong recent interest in the design of fair algorithms. However, there is often a dichotomy between fairness and efficacy: fair algorithms may proffer low social welfare solutions whereas welfare optimizing algorithms may be very unfair. This issue is exemplified in the machine scheduling problem where, for $n$ jobs, the social welfare of any fair solution may be a factor $\Omega(n)$ worse than the optimal welfare. In this paper, we prove that this dichotomy between fairness and efficacy can be overcome if we allow for a negligible amount of bias: there exist algorithms that are both "almost perfectly fair" and have a constant factor efficacy ratio, that is, are guaranteed to output solutions that have social welfare within a constant factor of optimal welfare. Specifically, for any $\epsilon>0$, there exist mechanisms with efficacy ratio $\Theta(\frac{1}{\epsilo
    
[^116]: 基于传感器的机器人控制的基本限制

    Fundamental Limits for Sensor-Based Robot Control. (arXiv:2202.00129v5 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2202.00129](http://arxiv.org/abs/2202.00129)

    该论文提出了一个新的方法，利用信息论和动态规划技术，为基于传感器的机器人控制建立了性能的基本限制。通过计算任务相关信息的数量，可以获得一步决策任务的最高可实现预期报酬的上界，并通过动态规划将这个上界扩展到多步问题。实验证明了该方法在三个示例上的有效性，包括部分可观察性决策过程、自由落体物体的捕捉和使用深度传感器的避障问题。

    

    我们的目标是为给定任务建立机器人传感器性能的基本限制的理论和算法。为了实现这一目标，我们定义了一个捕捉传感器提供的与任务相关信息数量的量。利用信息论中广义Fano不等式的新颖版本，我们证明了这个量提供了一步决策任务的最高可实现预期报酬的上界。然后，我们通过动态规划方法将这个上界扩展到多步问题。我们提出了用于数值计算得出的上界的算法，并在三个示例上演示了我们的方法：（i）来自部分可观察性马尔可夫决策过程文献的熔岩问题，（ii）具有连续状态和观测空间的示例，对应于机器人接住自由落体物体的问题，以及（iii）使用非高斯噪声深度传感器的避障问题。我们展示了算法的能力

    Our goal is to develop theory and algorithms for establishing fundamental limits on performance imposed by a robot's sensors for a given task. In order to achieve this, we define a quantity that captures the amount of task-relevant information provided by a sensor. Using a novel version of the generalized Fano inequality from information theory, we demonstrate that this quantity provides an upper bound on the highest achievable expected reward for one-step decision making tasks. We then extend this bound to multi-step problems via a dynamic programming approach. We present algorithms for numerically computing the resulting bounds, and demonstrate our approach on three examples: (i) the lava problem from the literature on partially observable Markov decision processes, (ii) an example with continuous state and observation spaces corresponding to a robot catching a freely-falling object, and (iii) obstacle avoidance using a depth sensor with non-Gaussian noise. We demonstrate the ability
    
[^117]: 离线强化学习的遗憾快速收敛速率研究

    Fast Rates for the Regret of Offline Reinforcement Learning. (arXiv:2102.00479v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.00479](http://arxiv.org/abs/2102.00479)

    本文研究了离线数据对强化学习的遗憾，提出了精细的收敛速率分析，揭示了离线强化学习收敛速度较快的现象，并通过指数形式的加速机制加快了收敛速度。

    

    本文研究了固定行为策略在无限时间折现马尔可夫决策过程（MDP）中生成的离线数据对强化学习的遗憾。现有方法（如拟合Q-迭代）的分析表明，对于遗憾的收敛速率是O(1/√n)，但实证行为表现出非常快的收敛速度。本文通过提供遗憾收敛速率的快速收敛进行更精细的遗憾分析，准确地表征了这一现象。首先，我们证明在给定最优质量函数Q*的估计的情况下，其对应的策略遗憾按照Q*估计的点对点收敛速率的指数进行收敛，从而加速了收敛速度。指数的级别取决于“决策问题”中的噪声水平，而不是估计问题。我们以线性和表格型MDP作为示例，建立了这样的噪声水平。其次，我们对拟合Q-迭代和Bellman残差进行了新的分析。

    We study the regret of reinforcement learning from offline data generated by a fixed behavior policy in an infinite-horizon discounted Markov decision process (MDP). While existing analyses of common approaches, such as fitted $Q$-iteration (FQI), suggest a $O(1/\sqrt{n})$ convergence for regret, empirical behavior exhibits \emph{much} faster convergence. In this paper, we present a finer regret analysis that exactly characterizes this phenomenon by providing fast rates for the regret convergence. First, we show that given any estimate for the optimal quality function $Q^*$, the regret of the policy it defines converges at a rate given by the exponentiation of the $Q^*$-estimate's pointwise convergence rate, thus speeding it up. The level of exponentiation depends on the level of noise in the \emph{decision-making} problem, rather than the estimation problem. We establish such noise levels for linear and tabular MDPs as examples. Second, we provide new analyses of FQI and Bellman resid
    
[^118]: B-HAR:一个用于深入研究人体活动识别数据集和工作流程的开源基准框架

    B-HAR: an open-source baseline framework for in depth study of human activity recognition datasets and workflows. (arXiv:2101.10870v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2101.10870](http://arxiv.org/abs/2101.10870)

    B-HAR是一个开源的基准框架，用于深入研究人体活动识别的数据集和工作流程，解决了HAR方法缺乏标准工作流程的问题，并提供了可配置的框架来评估模式识别模型的质量。

    

    基于机器学习和深度学习算法的人体活动识别（HAR）被认为是最具潜力的技术之一，用于监测不同人群（如运动员、老年人、儿童、雇主）的专业和日常活动，以提供各种与福祉、技术性能增强、风险预防和教育目的相关的服务。然而，HAR方法的有效性和效率分析受到缺乏标准工作流程的影响，这可能成为评估开发的模式识别模型质量的基准。这使得不同方法之间的比较成为一项具有挑战性的任务。此外，研究人员可能会犯错误，当这些错误没有被检测到时，会对达到的结果产生重大影响。为了缓解这些问题，本文提出了一个名为B-HAR的开源自动化和高度可配置的框架，用于定义和标准化HAR实验的工作流程。

    Human Activity Recognition (HAR), based on machine and deep learning algorithms is considered one of the most promising technologies to monitor professional and daily life activities for different categories of people (e.g., athletes, elderly, kids, employers) in order to provide a variety of services related, for example to well-being, empowering of technical performances, prevention of risky situation, and educational purposes. However, the analysis of the effectiveness and the efficiency of HAR methodologies suffers from the lack of a standard workflow, which might represent the baseline for the estimation of the quality of the developed pattern recognition models. This makes the comparison among different approaches a challenging task. In addition, researchers can make mistakes that, when not detected, definitely affect the achieved results. To mitigate such issues, this paper proposes an open-source automatic and highly configurable framework, named B-HAR, for the definition, stan
    
[^119]: 基于深度学习的实时控制中的不确定性分解

    Deep Learning based Uncertainty Decomposition for Real-time Control. (arXiv:2010.02613v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.02613](http://arxiv.org/abs/2010.02613)

    通过使用深度学习技术，在实时控制中提出了一种新颖的方法，用于检测认知不确定性，该方法可以在未知环境中的数据驱动控制中实现安全和高效的探索。

    

    在未知环境中的数据驱动控制需要对涉及的不确定性有清晰的理解，以确保安全和高效的探索。虽然由于测量误差而产生的随机不确定性可以在给定参数化描述的情况下明确地进行建模，但对于描述训练数据的存在或不存在的认知不确定性，建模可能更加困难。当系统动态未知时，后者在实施探索性控制策略时特别有用。我们提出了一种新颖的方法，利用深度学习来检测训练数据的缺失，其输出一个介于0（表示低不确定性）和1（表示高不确定性）之间的连续值标量。我们将这个检测器作为认知不确定性的代理，并展示了其在合成和真实数据集上相对现有方法的优势。我们的方法可以直接与随机不确定性估计相结合，并允许实时进行不确定性估计。

    Data-driven control in unknown environments requires a clear understanding of the involved uncertainties for ensuring safety and efficient exploration. While aleatoric uncertainty that arises from measurement noise can often be explicitly modeled given a parametric description, it can be harder to model epistemic uncertainty, which describes the presence or absence of training data. The latter can be particularly useful for implementing exploratory control strategies when system dynamics are unknown. We propose a novel method for detecting the absence of training data using deep learning, which gives a continuous valued scalar output between $0$ (indicating low uncertainty) and $1$ (indicating high uncertainty). We utilize this detector as a proxy for epistemic uncertainty and show its advantages over existing approaches on synthetic and real-world datasets. Our approach can be directly combined with aleatoric uncertainty estimates and allows for uncertainty estimation in real-time as 
    

