# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [FRIGATE: Frugal Spatio-temporal Forecasting on Road Networks.](http://arxiv.org/abs/2306.08277) | FRIGATE是一种使用时空GNN进行公路网络预测的方法，可以在不依赖每个节点的感知、完整的感知历史记录或静态网络假设的情况下工作，在真实交通数据集上表现出更高的预测性能。 |
| [^2] | [GBSD: Generative Bokeh with Stage Diffusion.](http://arxiv.org/abs/2306.08251) | 本文提出了第一个基于生成对抗网络技术的文本到图像模型GBSD，并使用阶段扩散技术合成具有虚化风格的照片。 |
| [^3] | [Semi-supervised Cell Recognition under Point Supervision.](http://arxiv.org/abs/2306.08240) | 本文提出了一种半监督的细胞识别方法，使用端到端PCR模型和生成的伪标签来更好地利用未标注的图像，开发了适用于SSPCR的框架，并引入了两种新的损失函数来协调标记和未标记样本。 |
| [^4] | [Maestro: A Gamified Platform for Teaching AI Robustness.](http://arxiv.org/abs/2306.08238) | Maestro是一个用于教授人工智能容错性的游戏化平台，它为大学生们提供了具有生活启发的具有挑战性的任务，能够显著提高学生们在实验对抗攻击和防御方面的表现。 |
| [^5] | [Curricular Subgoals for Inverse Reinforcement Learning.](http://arxiv.org/abs/2306.08232) | 本论文提出一种新的逆强化学习框架，即基于课程子目标的逆强化学习 (CSIRL)。与现有方法不同的是，该框架将一个任务分解为多个局部子目标，以引导智能体进行模仿，这有助于解决全局设计中存在的问题。 |
| [^6] | [SMC-UDA: Structure-Modal Constraint for Unsupervised Cross-Domain Renal Segmentation.](http://arxiv.org/abs/2306.08213) | 本文提出了一种基于判别范式的结构模态约束的无监督跨领域肾脏分割方法，通过引入边缘结构来区分域不变的结构信息，并通过结构约束的自我学习和渐进式ROI完成肾脏分割，无需在目标领域上进行注释。 |
| [^7] | [Ball Trajectory Inference from Multi-Agent Sports Contexts Using Set Transformer and Hierarchical Bi-LSTM.](http://arxiv.org/abs/2306.08206) | 本文提出了一种使用集合变换器和分层双向LSTM从多智能体运动环境中推断球的轨迹的推断框架，能够成功地预测球的轨迹，即使在复杂的情况下也是如此。 |
| [^8] | [Unraveling the ARC Puzzle: Mimicking Human Solutions with Object-Centric Decision Transformer.](http://arxiv.org/abs/2306.08204) | 该论文使用双重策略解决ARC任务，通过决策Transformer模仿人类解决方案和引入物体检测算法来提高AI的问题解决能力，同时揭示了数据和模型结构等方面的需求，为未来AGI研究提供了见解。 |
| [^9] | [Learning on Graphs under Label Noise.](http://arxiv.org/abs/2306.08194) | 该论文介绍了一种新方法 CGNN，它利用一致性图神经网络和基于同质性假设的样本选择技术，在标签噪声的情况下对节点分类进行建模，实现过滤出噪声节点和增强节点表示的鲁棒性。 |
| [^10] | [Operationalising Representation in Natural Language Processing.](http://arxiv.org/abs/2306.08193) | 本文介绍了一个框架，通过使用探测分类器来评估组件是否表示属性，填补了自然语言处理中关于“表示”的哲学空白。 |
| [^11] | [DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer.](http://arxiv.org/abs/2306.08175) | 提出了一种基于Conformer的新型动态上下文传递机制DCTX-Conformer，解决了流式识别性能差距的问题，相比于现有最优解，识别结果的误差率提高了25%，但对延迟影响可以忽略不计。 |
| [^12] | [Reinforcement Learning-Driven Linker Design via Fast Attention-based Point Cloud Alignment.](http://arxiv.org/abs/2306.08166) | 该论文提出了一种强化学习驱动的链接物设计方法，成功地生成满足2D和3D要求的连接剂，实现了新型连接剂的制备。 |
| [^13] | [INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation.](http://arxiv.org/abs/2306.08162) | 该论文提出了一种通过低秩自适应纠错的方法，从而可以显著地减少精细调整VRAM需求，并纠正量化大语言模型中的量化误差，使消费者笔记本电脑可以对70亿个参数的大语言模型进行精细调整，生成连贯的英文文本。 |
| [^14] | [h2oGPT: Democratizing Large Language Models.](http://arxiv.org/abs/2306.08161) | 本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs），包括100％私有文档搜索。目标是创建真正开源的替代封闭源GPTs，提高人工智能的开发和可靠性。 |
| [^15] | [Survey on Sociodemographic Bias in Natural Language Processing.](http://arxiv.org/abs/2306.08158) | 本文调查了209篇关于NLP模型偏见的论文，其中大部分涉及社会人口统计偏见。研究者提出了社会人口统计偏见的定义，并确定了NLP偏见研究的三个主要类别。当前去偏见技术只是隐藏了偏见而不是真正去除它，需要进一步改进。 |
| [^16] | [Causal Feature Engineering of Price Directions of Cryptocurrencies using Dynamic Bayesian Networks.](http://arxiv.org/abs/2306.08157) | 本文提出了一种基于动态贝叶斯网络的方法，来预测加密货币价格方向，以帮助投资者做出明智的投资决策。 |
| [^17] | [Neural Mixed Effects for Nonlinear Personalized Predictions.](http://arxiv.org/abs/2306.08149) | 本文提出了神经混合效应（NME）模型，用于个性化预测，并通过结合个人通用和个人特定参数来考虑线性和非线性趋势。 |
| [^18] | [ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations.](http://arxiv.org/abs/2306.08141) | 为研究人工智能与人类交互，研究者创建了ArtWhisperer数据集，这是一个在线游戏，人们通过反复尝试不同的提示词，来生成和目标图像类似的图像，并记录了50,000多个交互记录。在初步分析中，研究者发现人们提交了各种各样的提示词，并能够发现生成各种文本描述的图像。 |
| [^19] | [AVIS: Autonomous Visual Information Seeking with Large Language Models.](http://arxiv.org/abs/2306.08129) | 本文提出了一个基于大型语言模型的自主信息检索视觉问答框架AVIS，可以解决视觉问题所需的外部知识获取问题。 |
| [^20] | [PersonaPKT: Building Personalized Dialogue Agents via Parameter-efficient Knowledge Transfer.](http://arxiv.org/abs/2306.08126) | 本文提出了PersonaPKT，一种轻量级迁移学习方法，可以在没有明确个性描述的情况下构建符合角色的对话模型，该方法通过将每个个性表示为一个连续向量，直接从同一角色产生的少量对话样本中学习隐含的个性特定特征，并能够高效构建个性化对话代理并增强隐私保护。 |
| [^21] | [Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level.](http://arxiv.org/abs/2306.08122) | 该论文提出了一种新的自然语言处理方法，可对学术写作中的抄袭行为进行有效检测，不仅在句子级别上进行评估，还在文档级别上提供可量化指标。该方法的准确率高达94％，具有较强的适应性和可靠性，能够不断随着LLM技术的发展而不断改进。 |
| [^22] | [Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning.](http://arxiv.org/abs/2306.08094) | 本文研究探讨使用 ChatGPT 解决混合交通流控制问题，通过大规模用户研究发现 ChatGPT 在某些环境下能够提高成功策略数量 |
| [^23] | [DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.}$.](http://arxiv.org/abs/2306.08068) | DORSal提出了一种基于扩散模型的物体中心场景表示方法，可以呈现高保真新视图，并在较大程度上保留了诸如基于物体的场景编辑之类的优点。 |
| [^24] | [Symbolic Regression via Control Variable Genetic Programming.](http://arxiv.org/abs/2306.08057) | CVGP是一种通过控制变量实验设计加快符号表达式发现的方法，能够学习复杂符号表达式，扩展了现有方法的能力。 |
| [^25] | [Distributed Trust Through the Lens of Software Architecture.](http://arxiv.org/abs/2306.08056) | 本文从多个学科的角度对分布式信任概念进行概述，并探讨由分布式信任技术实现的信任重分配/转移及相关的系统与应用的权衡。 |
| [^26] | [Tune As You Scale: Hyperparameter Optimization For Compute Efficient Training.](http://arxiv.org/abs/2306.08055) | 该论文提出了一种名为“CARBS”的算法，它利用贝叶斯优化算法在性能-计算 Pareto 前沿附近执行局部搜索来解决大型深度学习模型超参数调整的问题。该方法适用于具有许多超参数的无界搜索空间，学习缩放关系，并自动化了许多调整中的“黑魔法”。此外，该方法对计算受限制的情况尤其有效。 |
| [^27] | [Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning Approach to Critical Care.](http://arxiv.org/abs/2306.08044) | 该论文介绍了一种深度Q学习方法，通过剪枝动作集来实现将中间生物标志物信号整合到奖励规范中，提高了重症护理策略的可靠性。 |
| [^28] | [FLamE: Few-shot Learning from Natural Language Explanations.](http://arxiv.org/abs/2306.08042) | FLamE是一个利用GPT-3生成自然语言解释用于RoBERTa微调的少样本学习框架，在自然语言推理任务上展现出较好的性能，但是人生成的解释大多不能充分地证明分类决策，标签特定线索在解释中起着重要作用。 |
| [^29] | [On Faking a Nash Equilibrium.](http://arxiv.org/abs/2306.08041) | 本文研究多智能体强化学习中的数据污染攻击，提出了唯一纳什集的概念，并设计了一个线性规划方案来计算最优污染攻击策略。 |
| [^30] | [Flexible Channel Dimensions for Differentiable Architecture Search.](http://arxiv.org/abs/2306.08021) | 本文提出了一种新颖的可微神经架构搜索方法，使用动态通道分配算法实现通道维度的灵活搜索空间，能够有效地找到等同于以前方法在CIFAR-10数据集上任务准确性和推理延迟方面的DNN架构，并且不需要手动设计或架构工程专业知识。 |
| [^31] | [Curatr: A Platform for Semantic Analysis and Curation of Historical Literary Texts.](http://arxiv.org/abs/2306.08020) | Curatr是一个在线平台，可以基于机器学习支持的语义搜索来进行历史文学文本的筛选，提供主题词典的生成，帮助研究人员快速从大量的数字化文本中挑选出相关的子语料库。 |
| [^32] | [Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models.](http://arxiv.org/abs/2306.08018) | Mol-Instructions是一个专门为生物分子领域设计的综合指令数据集，可以显著提高大语言模型在生物领域中的适应能力和认知敏锐度。 |
| [^33] | [Realising Synthetic Active Inference Agents, Part I: Epistemic Objectives and Graphical Specification Language.](http://arxiv.org/abs/2306.08014) | 本文介绍了自由能原理和主动推理的理论框架，并推导了适用于任意图形模型的主动推理版本。同时引入了一种新的图形说明语言（GSL）来明确规定系统目标。 |
| [^34] | [TopP\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models.](http://arxiv.org/abs/2306.08013) | 本文提出了一种鲁棒可靠的生成模型评估指标TopP\&R，通过引入拓扑和统计处理进行严格的支持估计。TopP\&R仅保留具有一定置信水平的具有拓扑和统计上重要性的特征，对于噪声特征具有强大的鲁棒性，并提供了统计一致性。 |
| [^35] | [Privacy Inference-Empowered Stealthy Backdoor Attack on Federated Learning under Non-IID Scenarios.](http://arxiv.org/abs/2306.08011) | 本文提出了一个针对非独立同分布场景下联邦学习的隐蔽后门攻击方案，其中通过生成对抗网络提供补充数据集，有效应对数据异构问题和隐私推断攻击风险。 |
| [^36] | [Domain Information Control at Inference Time for Acoustic Scene Classification.](http://arxiv.org/abs/2306.08010) | 本文将自然语言处理中的可控门控适配器ConGater应用于声景分类任务，较现有领域泛化方法更好地提高模型在新领域中的性能。 |
| [^37] | [DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation.](http://arxiv.org/abs/2306.08009) | 本文提出了一种数据无关的全面后门擦除（DHBE）框架，该框架将后门擦除任务视为一个统一的对抗过程，在转移干净数据的知识的同时保证擦除后门。通过对抗蒸馏和后门正则化两个不同的竞争过程，DHBE实现了一种统一的、无需数据，并且有效的后门擦除框架。 |
| [^38] | [Detection and classification of faults aimed at preventive maintenance of PV systems.](http://arxiv.org/abs/2306.08004) | 本论文提出了一种随机森林算法的创新方法，用于检测细微的光伏故障并进行分类，提高了故障分类的计算时间，同时保持了高精度。 |
| [^39] | [DTW k-means clustering for fault detection in photovoltaic modules.](http://arxiv.org/abs/2306.08003) | 本文提出了一种无监督聚类技术的方法，用于在大规模光伏电厂中检测和识别故障类型，采用动态时间规整（DTW）距离度量，能够提高聚类性能。 |
| [^40] | [A Markovian Formalism for Active Querying.](http://arxiv.org/abs/2306.08001) | 本文提出了一个基于马尔可夫系统的形式化主义对主动学习进行概述，将主动学习过程作为一个部分可观察的马尔可夫系统的整体处理，并且阐述如何将查询、数据集增强、奖励更新等过程视为马尔可夫系统中元状态之间的转移。 |
| [^41] | [Diagnostic test accuracy (DTA) of artificial intelligence in digital pathology: a systematic review, meta-analysis and quality assessment.](http://arxiv.org/abs/2306.07999) | 本文进行了数字病理图像中应用人工智能的所有病理学领域的诊断准确度的系统综述和Meta分析。结果表明，人工智能在数字病理学中取得了高度的准确度，是可行的辅助诊断工具。 |
| [^42] | [Contrastive Attention Networks for Attribution of Early Modern Print.](http://arxiv.org/abs/2306.07998) | 本文提出了一种对比注意力度量学习方法，用于鉴定早期印刷品中的未知印刷商。我们的方法对字形微小差异敏感，对各种混淆噪声具有鲁棒性，并设计了一种随机数据合成过程，成功提高了该时期印刷品中的损坏类型印记匹配结果。 |
| [^43] | [Semantic-Based Neural Network Repair.](http://arxiv.org/abs/2306.07995) | 本文提出了一种自动修复神经网络错误的方法，基于深度学习层的可执行语义，并专注于实践中常见的四种错误。 |
| [^44] | [MSSRNet: Manipulating Sequential Style Representation for Unsupervised Text Style Transfer.](http://arxiv.org/abs/2306.07994) | 本文提出了一种新方法，通过为每个词汇分配单独的风格向量来对文本进行风格转换，并引入基于教师-学生学习的对抗性培训框架，以提高培训稳定性，实现了双风格转换和多风格转换两种情况下的明显提高的风格转换准确性和内容保留。 |
| [^45] | [Securing Visually-Aware Recommender Systems: An Adversarial Image Reconstruction and Detection Framework.](http://arxiv.org/abs/2306.07992) | 本文提出了一种对抗图像重构及检测框架来保护视觉感知推荐系统，能够防御局部扰动为特征的对抗攻击并且能够在干净和对抗性图像上进行训练来检测对抗性图像。 |
| [^46] | [A Hybrid Approach for Smart Alert Generation.](http://arxiv.org/abs/2306.07983) | 本文提出了一个混合模型的警报系统，在解决可扩展性、数据异构性、可泛化性和可维护性方面表现良好。白名单机制有效减少了误报警报，未来的工作将包括更多的特征工程和输入数据，以及包括人类反馈在内的模型开发过程。 |
| [^47] | [Few-shot Multi-domain Knowledge Rearming for Context-aware Defence against Advanced Persistent Threats.](http://arxiv.org/abs/2306.07685) | 本文提出了一种针对高级持久性威胁的上下文感知防御的少样本、多领域知识重装 (FMKR) 方案，可以在不同的网络域生成多个小任务，完成多领域的知识重新装备，提高防御性能。 |
| [^48] | [Hyperbolic Graph Diffusion Model for Molecule Generation.](http://arxiv.org/abs/2306.07618) | 本文提出了基于双曲图扩散模型的分子生成方法，可以更全面地捕捉分子的内部非欧几里德结构，实现数据生成，并提取复杂几何特征的能力。 |
| [^49] | [DeepTransition: Viability Leads to the Emergence of Gait Transitions in Learning Anticipatory Quadrupedal Locomotion Skills.](http://arxiv.org/abs/2306.07419) | 本文通过深度强化学习和机器人工具的互动研究，证明了可行性是四足动物步态转换的重要标准。其中，步-小跑步态转换能够在平坦地形上同时提高可行性和节能效果。 |
| [^50] | [Strokes2Surface: Recovering Curve Networks From 4D Architectural Design Sketches.](http://arxiv.org/abs/2306.07220) | 本文介绍了Strokes2Surface，它可从建筑师的笔画中恢复出曲线网络，对于建筑设计中的概念设计和数字建模之间的桥梁具有重要意义。 |
| [^51] | [TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI.](http://arxiv.org/abs/2306.06924) | 本文探讨了人工智能可能带来的社会规模风险的分类和分析，其中提出了一种基于问责制的分类法。针对可能出现的风险类型提供了实际的证明，并指出需要联合技术和政策解决方案。 |
| [^52] | [TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models.](http://arxiv.org/abs/2306.06815) | 本文开创性地研究了基于 prompt 学习的预训练语言模型 API 的特洛伊易感性，并提出了一种自动黑盒框架——TrojPrompt，用于生成通用和隐蔽的触发器，并将特洛伊木马插入硬提示。 |
| [^53] | [Improving Knowledge Extraction from LLMs for Robotic Task Learning through Agent Analysis.](http://arxiv.org/abs/2306.06770) | 本文提出了一种认知代理方法，通过增加 LLMs 的响应空间，并部署通用策略，嵌入自主机器人内部，从而使机器人能够获取与其语言能力、体现能力、环境和用户偏好相匹配的新的任务知识，实现一次学习即可完成任务。 |
| [^54] | [NERFBK: A High-Quality Benchmark for NERF-Based 3D Reconstruction.](http://arxiv.org/abs/2306.06300) | 该论文介绍了一个名为NeRFBK的新的真实和合成数据集，专门用于测试和比较基于NeRF的3D重建算法，旨在解决收集具有精确地面实况的多样化数据的挑战问题。该数据集有望推动3D重建领域的发展。 |
| [^55] | [CARSO: Counter-Adversarial Recall of Synthetic Observations.](http://arxiv.org/abs/2306.06081) | 本文提出了一种新的图像分类的对抗性防御机制CARSO，该方法可以比最先进的对抗性训练更好地保护分类器，通过利用生成模型进行对抗净化来进行最终分类，并成功地保护自己免受未预见的威胁和最终攻击。 |
| [^56] | [COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models.](http://arxiv.org/abs/2306.05659) | 本文提出了一种启发式贪心对抗攻击，针对基于提示的模板在PLMs中可能存在的漏洞，通过字符级和单词级的破坏方法进行攻击，取得了较高的攻击成功率。 |
| [^57] | [INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models.](http://arxiv.org/abs/2306.04757) | INSTRUCTEVAL是一个专注于指导调整的大型语言模型评估的综合套件，它采取了全面的方法来评估模型的性能，包括解决问题、写作能力和与人类价值观的一致性等特征。 |
| [^58] | [ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems.](http://arxiv.org/abs/2306.04357) | 本研究提出了一种针对对话响应选择的后训练技术Dial-MAE，利用生成方法更好地压缩对话语义至密集向量，并提高对话响应选择准确性。 |
| [^59] | [I'm Afraid I Can't Do That: Predicting Prompt Refusal in Black-Box Generative Language Models.](http://arxiv.org/abs/2306.03423) | 本文研究了生成语言模型的拒绝行为，并发现这种行为不是完全二元的。作者对ChatGPT进行的实验表明，在微调过程中的偏见来自于个别工程师和公司政策，并影响模型选择拒绝哪些提示。 |
| [^60] | [DreamSparse: Escaping from Plato's Cave with 2D Diffusion Model Given Sparse Views.](http://arxiv.org/abs/2306.03414) | 本文提出了 DreamSparse 框架，该框架通过利用先前训练的扩散模型的 2D 先验知识，通过几何模块和空间引导模型来解决 2D 模型缺乏 3D 感知能力的问题，进一步实现了从少视角情况下合成高质量的新视角图像。 |
| [^61] | [A survey of Generative AI Applications.](http://arxiv.org/abs/2306.02781) | 本篇论文对350多个生成式人工智能应用进行了全面调查，总结了不同单模和多模生成式人工智能的应用。该调查为研究人员和从业者提供了宝贵的资源，帮助他们更好地了解生成式人工智能领域目前的最先进技术，并促进该领域的进一步创新。 |
| [^62] | [PassGPT: Password Modeling and (Guided) Generation with Large Language Models.](http://arxiv.org/abs/2306.01545) | 本研究使用大型语言模型PassGPT进行密码建模和生成，该模型比基于GAN的现有方法更准确，能生成符合任意限制的密码，为提高密码强度估计器提供了潜在的帮助。 |
| [^63] | [Parallel Neurosymbolic Integration with Concordia.](http://arxiv.org/abs/2306.00480) | 康科迪亚框架是一个支持各种概率理论，克服了现有技术的限制的并行的神经符号结构，成功应用于集合活动检测、实体链接和推荐任务，提高了最新准确性。 |
| [^64] | [On the Correspondence Between Monotonic Max-Sum GNNs and Datalog.](http://arxiv.org/abs/2305.18015) | 本文研究了数据变换基于图神经网络的表现能力，证明了单调最大和的GNN与Datalog之间的关系紧密，即任何单调最大和的GNN都可以表示为一个Datalog程序，反之亦然。 |
| [^65] | [Scaling Data-Constrained Language Models.](http://arxiv.org/abs/2305.16264) | 研究人员研究了在数据受限制的情况下缩放语言模型，并提出了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。 |
| [^66] | [OVO: Open-Vocabulary Occupancy.](http://arxiv.org/abs/2305.16133) | 本文提出了开放词汇占据（OVO）方法，可以允许任意类别的语义占据预测，无需三维注释，其关键技术包括预训练二维开放词汇分割模型到三维占据网络的知识蒸馏和像素-体素过滤以生成高质量的训练数据。 |
| [^67] | [Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks.](http://arxiv.org/abs/2305.14065) | 本文提出了一种新的图神经网络结构搜索方法——神经结构编码（NAC），它通过稀疏编码寻找最优结构参数，无需训练就能发挥表现力，在多个基准数据集上实现了最先进性能，并且运算速度比强基线方法快了200倍，精度提高了18.8％。 |
| [^68] | [A Meta-learning based Generalizable Indoor Localization Model using Channel State Information.](http://arxiv.org/abs/2305.13453) | 本文提出了一种基于元学习和信道状态信息的室内定位模型，以解决深度学习定位模型中持续存在的通用性缺失问题。 |
| [^69] | [Road Planning for Slums via Deep Reinforcement Learning.](http://arxiv.org/abs/2305.13060) | 本文介绍了一种基于深度强化学习的方法，用于自动布局贫民窟道路。通过掩码策略优化，可使可达性提高14.3％，对现有基线方法具有明显改进。 |
| [^70] | [Graph Propagation Transformer for Graph Representation Learning.](http://arxiv.org/abs/2305.11424) | 本文提出了一种新的变换器架构 GPTrans，以图传播注意力为基础，可以更好地学习图形模型，并在多个基准测试集上超过了其他最先进的基于变换器的图形模型。 |
| [^71] | [LoViT: Long Video Transformer for Surgical Phase Recognition.](http://arxiv.org/abs/2305.08989) | LoViT是一种用于手术阶段识别的长视频Transformer，它通过结合时间丰富的空间特征提取器和多尺度时间聚合器来对长视频进行分析，优于现有方法。 |
| [^72] | [How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?.](http://arxiv.org/abs/2305.06587) | 该论文研究了谱时图神经网络的表达能力，并揭示了其具有线性谱时GNN是普适的、表现力受到离散时间动态图扩展的第一阶Weisfeiler-Leman算法的限制。同时，论文提出了一个简单实例TGC，其在时间序列预测方面具有显著的性能优势。 |
| [^73] | [Human or Machine: Reflections on Turing-Inspired Testing for the Everyday.](http://arxiv.org/abs/2305.04312) | 本文基于图灵测试，回避了机器是否智能的问题，探讨在日常生活中如何确定一个交互对象是人还是机器的挑战，并思考了其应用和重要性。 |
| [^74] | [Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models.](http://arxiv.org/abs/2305.02531) | 本研究分析了大型语言模型在不同语言提示下的奖励时间偏好，并发现GPT在具有较弱未来时态的语言下表现出更大的耐心，这与使用该语言的人类的偏好相似。 |
| [^75] | [GPTutor: a ChatGPT-powered programming tool for code explanation.](http://arxiv.org/abs/2305.01863) | 本文介绍了一种名为GPTutor的ChatGPT动力编程工具，它是一个使用ChatGPT API的Visual Studio Code扩展，通过设计提示词，可以对所选代码进行精简、准确的解释。 |
| [^76] | [Towards Mode Balancing of Generative Models via Diversity Weights.](http://arxiv.org/abs/2304.11961) | 本研究提出了通过平衡训练数据集中的模式来增加模型输出多样性的多样性权重训练方案，以更好地适应需要多样化输出的创意应用，并在受控环境中进行的初步实验展示了其潜力。 |
| [^77] | [Tool Learning with Foundation Models.](http://arxiv.org/abs/2304.08354) | 基于基础模型的工具学习结合了专用工具和基础模型的优势，实现了问题解决的增强精度、效率和自动化。本文对工具学习进行了系统研究，提出了涵盖两种类型学习的通用工具学习框架，并分析了它们的独特挑战、机会和未来方向。 |
| [^78] | [OpenAGI: When LLM Meets Domain Experts.](http://arxiv.org/abs/2304.04370) | 基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。 |
| [^79] | [Kernel Affine Hull Machines for Differentially Private Learning.](http://arxiv.org/abs/2304.01300) | 本文提出了一种基于核凸包机的方法来确保数据隐私保护，同时保留数据结构，用于数据表示学习的分类应用中。为了确保隐私保护学习，还提出了一种新颖的生成虚假数据的方法。 |
| [^80] | [Machine-Learned Premise Selection for Lean.](http://arxiv.org/abs/2304.00994) | 该论文介绍了一种基于机器学习的工具，可为 Lean 证明助手建议与用户正在证明的定理相关的前提条件。 |
| [^81] | [Contextual Combinatorial Bandits with Probabilistically Triggered Arms.](http://arxiv.org/abs/2303.17110) | 本文研究了带有概率触发臂的情境组合赌博机，在不同条件下设计了C$^2$-UCB-T算法和VAC$^2$-UCB算法，并分别导出了对应的遗憾值上限，为相关应用提供了理论支持。 |
| [^82] | [The Quality-Diversity Transformer: Generating Behavior-Conditioned Trajectories with Decision Transformers.](http://arxiv.org/abs/2303.16207) | 该论文提出了一种基于质量多样性算法和 Transformer 的方法，通过两种机制以实现质量一致的生成行为条件下的轨迹。 |
| [^83] | [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention.](http://arxiv.org/abs/2303.16199) | 本文提出了一种基于适应提示和零初始化注意力机制的轻量级语言模型调整方法，可高效微调LLaMA为指令跟随模型，具有比Alpaca更短的微调时间并具有近似的响应质量。 |
| [^84] | [Evolving Populations of Diverse RL Agents with MAP-Elites.](http://arxiv.org/abs/2303.12803) | 本文介绍了一种尝试在进化计算与强化学习中相结合解决机器人控制问题的方法，并通过改进ME算法提高了效率和多样性，但也出现了一些常见强化学习算法的限制。 |
| [^85] | [Fast exploration and learning of latent graphs with aliased observations.](http://arxiv.org/abs/2303.07397) | 本文介绍了一种在具有别名观测的潜在图上，能够显著提高最大化探索效率的政策算法 eFeX，相比于随机策略，该算法能够更快地恢复各种拓扑结构下的图表。 |
| [^86] | [Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation.](http://arxiv.org/abs/2303.05983) | 本论文构建了两个多模态数据集来验证视觉语言模型在文本-视觉对话任务中的能力，并开发特定规则的监督信号来让系统展示可追溯性。 |
| [^87] | [Computational-level Analysis of Constraint Compliance for General Intelligence.](http://arxiv.org/abs/2303.04352) | 论文研究了人类行为受约束规范约束的影响，同时针对于构建遵守这些约束的通用代理提出了基于贝叶斯决策理论的计算水平模型。 |
| [^88] | [iSAGE: An Incremental Version of SAGE for Online Explanation on Data Streams.](http://arxiv.org/abs/2303.01181) | iSAGE是一种基于增量学习的SAGE在线解释方法，具备快速、内存高效的特点。该方法能够对模型变化以及数据生成过程中的漂移进行反应，同时提供了有效的特征移除方法，具有和SAGE类似的理论性质。 |
| [^89] | [Revisiting the Gumbel-Softmax in MADDPG.](http://arxiv.org/abs/2302.11793) | 本文探索了多种Gumbel-Softmax的替代方法，并将其应用于MADDPG中，以解决离散动作空间下的性能问题。 |
| [^90] | [Learning Manifold Dimensions with Conditional Variational Autoencoders.](http://arxiv.org/abs/2302.11756) | 该论文证明全局最优变分自编码器(CVAE)可以学习正确的流形维度，同时提出了一种新方法可以共同学习流形维度和条件分布，以在多个数据集上实现更好的特征分离和样本质量。 |
| [^91] | [Continuous Learning for Android Malware Detection.](http://arxiv.org/abs/2302.04332) | 本文提出了一种使用对比学习和主动学习相结合的新方法，用于解决Android恶意软件分类器中的概念漂移问题。 |
| [^92] | [Mnemosyne: Learning to Train Transformers with Transformers.](http://arxiv.org/abs/2302.01128) | Mnemosyne优化器使用Performers方法来学习训练整个神经网络架构，包括其他Transformers，而无需针对特定任务进行优化器调节，并成功训练ViTs和应用于机器人领域中，具有更好的泛化能力与快速收敛。 |
| [^93] | [Grounding Language Models to Images for Multimodal Inputs and Outputs.](http://arxiv.org/abs/2301.13823) | 该论文提出一种有效的方法，将仅处理文本的语言模型与图像进行联系，使其能够处理任意交错的图像和文本数据，并生成与检索图像交错的自由形式文本。该方法在环境相关的图像检索和多模态对话等任务中表现十分优异，是利用预训练语言模型解决视觉场景下交互问题的有效解决方案。 |
| [^94] | [Distilling Internet-Scale Vision-Language Models into Embodied Agents.](http://arxiv.org/abs/2301.12507) | 该研究提出了使用预训练的视觉语言模型(VLMs)来监督具体代理，将互联网规模的VLMs的通用语言接地技能重新利用到具体代理的行动中。 |
| [^95] | [Understanding the Role of Human Intuition on Reliance in Human-AI Decision-Making with Explanations.](http://arxiv.org/abs/2301.07255) | 本文研究了人类直觉对人工智能决策中的依赖作用和解释，在采用特征和基于示例的两种解释类型进行两个预测任务的思考实验中确认了三种直觉类型：关于AI正确性的直觉、基于对任务和AI模型的预期的直觉以及基于个人价值观和目标的直觉。 |
| [^96] | [Surgical Aggregation: A Collaborative Learning Framework for Harmonizing Distributed Medical Imaging Datasets with Diverse Tasks.](http://arxiv.org/abs/2301.06683) | 本论文提出了一种手术聚合的协同学习框架，该框架可用于协调和聚合分布式医学影像数据集的知识，并带有部分疾病注释，从而可训练具有完整胸部内可能出现的所有异常的临床实用、强大模型。 |
| [^97] | [A Theoretical Framework for AI Models Explainability with Application in Biomedicine.](http://arxiv.org/abs/2212.14447) | 该论文提出了一种新的解释定义和理论框架，用于评估AI模型的可解释性，特别是在生物医学领域。这个框架可帮助确定模型决策中最具影响力的输入特征，支持对模型行为的解释。 |
| [^98] | [DOC: Improving Long Story Coherence With Detailed Outline Control.](http://arxiv.org/abs/2212.10077) | 该论文提出了一个名为 Detailed Outline Control(DOC) 的框架，通过详细大纲和详细控制器来提高生成长篇故事时的情节连贯性和大纲相关性，人类评估证实该方法在这些方面显著优于基线方法，并且更适用于交互生成设置。 |
| [^99] | [Causal Temporal Reasoning for Markov Decision Processes.](http://arxiv.org/abs/2212.08712) | 本文介绍了一种新的概率时间逻辑PCFTL，第一个包含因果推理运算符的概率时间逻辑，可推理干预和反事实查询，从而能够推理涉及MDP不同配置的“假设”场景 |
| [^100] | [Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs.](http://arxiv.org/abs/2211.16468) | 在因果图中，提出了解决前门调整的线性时间算法，通过观察到的中介变量，即使存在未观测到的混淆，也可以识别因果效应。 |
| [^101] | [Adversarial Cheap Talk.](http://arxiv.org/abs/2211.11030) | 本文提出了一种新型对抗性设置，在其中对手只能将信息附加到受害者的观察中，从而产生最小的影响范围，并提出对抗性廉价交流（ACT）算法进行对手训练。在高度受限的情况下，使用ACT训练的对手仍会对受害者的训练和测试表现产生显著影响，揭示了强化学习算法中的一种新的攻击向量。 |
| [^102] | [HiveNAS: Neural Architecture Search using Artificial Bee Colony Optimization.](http://arxiv.org/abs/2211.10250) | 本文提出了一种采用人工蜂群优化的神经架构搜索框架HiveNAS，它在极短的时间内就能超越其他基于群智的NAS框架，成为最优。 |
| [^103] | [ATCO2 corpus: A Large-Scale Dataset for Research on Automatic Speech Recognition and Natural Language Understanding of Air Traffic Control Communications.](http://arxiv.org/abs/2211.04054) | ATCO2语料库是一个旨在促进空中交通管制通信自动语音识别和自然语言理解研究的大规模数据集，包括数据收集和预处理、语音数据的伪注释，以及提取与空中交通管制相关的命名实体。 |
| [^104] | [Broken Neural Scaling Laws.](http://arxiv.org/abs/2210.14891) | 本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。 |
| [^105] | [PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting.](http://arxiv.org/abs/2210.08964) | 提出了一种新的时间序列预测范式——基于提示的时间序列预测（PromptCast），将数字输入和输出转化为提示，并以句子到句子的方式提出预测任务，可以直接应用于语言模型。 |
| [^106] | [CORL: Research-oriented Deep Offline Reinforcement Learning Library.](http://arxiv.org/abs/2210.07105) | CORL是一个面向研究的深度强化学习离线库，提供了经过充分基准测试的单文件实现离线和离线到在线强化学习算法，并具有简单的开发体验和实验跟踪功能。 |
| [^107] | [FP-Diffusion: Improving Score-based Diffusion Models by Enforcing the Underlying Score Fokker-Planck Equation.](http://arxiv.org/abs/2210.04296) | 本文提出了FP-Diffusion方法来改进基于得分的扩散模型，通过强制底层得分的福克-普朗克方程来正则化DSM目标函数，以提高模型似然度和守恒程度。 |
| [^108] | [Topological Singularity Detection at Multiple Scales.](http://arxiv.org/abs/2210.00069) | 本文提出了一种多尺度拓扑奇异性检测方法，可以评估数据的局部固有维度，并量化点的“流形度”，能够检测复杂空间和图像中的奇异性。 |
| [^109] | [Analysis and Comparison of Classification Metrics.](http://arxiv.org/abs/2209.05355) | 本文回顾并比较了常用于度量分类系统表现的各种指标，发现期望成本指标具有更广泛的适用性和直观性，并可用于解决从连续得分生成分类决策的实践问题。 |
| [^110] | [Single-Stage Broad Multi-Instance Multi-Label Learning (BMIML) with Diverse Inter-Correlations and its application to medical image classification.](http://arxiv.org/abs/2209.02625) | 本文提出了一种单阶段广泛多实例多标签学习（BMIML）框架，解决了多实例和多标签之间的各种关系学习问题。在医学图像分类中具有广泛的应用前景。 |
| [^111] | [White-Box Adversarial Policies in Deep Reinforcement Learning.](http://arxiv.org/abs/2209.02167) | 本文研究了在强化学习中训练对抗策略的方法，提出了基于白盒攻击的策略，能够访问目标代理的内部状态，从而识别其漏洞，攻击成功率更高。 |
| [^112] | [Exploring the Intersection between Neural Architecture Search and Continual Learning.](http://arxiv.org/abs/2206.05625) | 本文研究了神经网络架构搜索和持续学习的交叉研究，旨在开发更加稳健和适应性的智能体，应用于物联网设备、自动驾驶车辆等领域，以解决诸如概念/数据漂移等问题。 |
| [^113] | [E2PN: Efficient SE(3)-Equivariant Point Network.](http://arxiv.org/abs/2206.05398) | 本文提出了一种简单、轻量、快速的SE(3)-等变点云网络，通过组卷积和商表示相结合实现，用于处理点云数据，同时在对象分类、姿态估计和关键点匹配等任务上实现了可比或优越的性能。 |
| [^114] | [AdaProp: Learning Adaptive Propagation for Graph Neural Network based Knowledge Graph Reasoning.](http://arxiv.org/abs/2205.15319) | 本文提出了一种名为AdaProp的方法，该方法可自适应地过滤掉不相关的实体，同时保留有前途的目标，从而高效、强大地进行知识图谱推理。 |
| [^115] | [Automated Scoring for Reading Comprehension via In-context BERT Tuning.](http://arxiv.org/abs/2205.09864) | 本文介绍了一种基于预训练语言模型的文本表示和上下文BERT微调的阅读理解自动评分模型，解决了单个问题/题目模型无法利用题目间关联性以及存储模型困难的问题。 |
| [^116] | [Bilinear value networks.](http://arxiv.org/abs/2204.13695) | 提出了一种通过点积低秩近似来表示Q值的双线性分解方法，其中第一个向量场捕捉状态的局部动态，第二个部分捕捉当前状态和目标之间的全局关系，该方法能够显著提高数据效率，并具有很好的泛化性能。 |
| [^117] | [Topological Experience Replay.](http://arxiv.org/abs/2203.15845) | 本文提出了一种拓扑经验回放的方法，通过构建图来明确状态的 Q 值之间的依赖关系，解决了传统采样策略忽视状态间依赖关系的问题，提高了学习深度 Q 函数时的性能和准确性。 |
| [^118] | [Parallel Successive Learning for Dynamic Distributed Model Training over Heterogeneous Wireless Networks.](http://arxiv.org/abs/2202.02947) | 本文提出了并行连续学习（PSL），通过网络、异构性和接近性三个维度的扩展，实现了在无线设备集合上的动态分布式模型训练。 |
| [^119] | [Cognitive Ledger Project: Towards Building Personal Digital Twins Through Cognitive Blockchain.](http://arxiv.org/abs/2201.08163) | 该论文提出了一个认知数字孪生体的认知架构，即通过基于区块链基础设施的方式将用户的个人数据转化为结构化信息和机器学习模型，最终能够一起组成用户的认知数字孪生体。 |
| [^120] | [Task-Aware Meta Learning-based Siamese Neural Network for Classifying Obfuscated Malware.](http://arxiv.org/abs/2110.13409) | 本文提出了一种基于任务感知元学习的Siamese神经网络分类混淆恶意软件的方法，可以更准确地调整特征嵌入，以应对不同恶意软件家族的混淆变种，并且可以对恶意软件类进行分类，即使只有一个或很少的训练样本可用。 |
| [^121] | [QuantumNAT: Quantum Noise-Aware Training with Noise Injection, Quantization and Normalization.](http://arxiv.org/abs/2110.11331) | QuantumNAT是一个PQC特定框架，可以在训练和推断阶段执行噪声感知优化，提高鲁棒性，缓解量子噪声 |
| [^122] | [LLVIP: A Visible-infrared Paired Dataset for Low-light Vision.](http://arxiv.org/abs/2108.10831) | 提出了适用于低光视觉的可见-红外成对数据集LLVIP，数据集包含30976张图像，解决了在低光条件下各种视觉任务缺失有效目标区域的挑战。行人已被标注，实验结果证明了融合对图像信息的补充效果，揭示了现有算法在非常低光条件下的不足之处。 |

# 详细

[^1]: FRIGATE:公路网络的节约时空预测

    FRIGATE: Frugal Spatio-temporal Forecasting on Road Networks. (arXiv:2306.08277v1 [cs.LG])

    [http://arxiv.org/abs/2306.08277](http://arxiv.org/abs/2306.08277)

    FRIGATE是一种使用时空GNN进行公路网络预测的方法，可以在不依赖每个节点的感知、完整的感知历史记录或静态网络假设的情况下工作，在真实交通数据集上表现出更高的预测性能。

    

    建模公路网络上的时空过程是一个越来越重要的任务。虽然在发展时空图神经网络（GNNs）方面取得了重要进展，但现有的工作建立在三个不适用于实际公路网络的假设基础上。首先，它们假定每个节点都配备传感器进行感知。然而在现实中，由于预算限制或传感器故障，所有位置（节点）可能没有配备传感器。其次，它们假定所有安装的传感器都具有感知历史记录。然而由于传感器故障、通信中的数据丢失等原因，这也是不现实的。最后，假设静态的道路网络。网络内连通性会由于道路封闭、新路修建等原因而发生变化。在这项工作中，我们开发了FRIGATE来解决所有这些缺点。FRIGATE由时空GNN驱动，将位置、拓扑和时间信息集成到节点表示中，用于预测。通过利用图神经网络的表现力，FRIGATE可以学习预测公路网络上时空过程的演变，而无需每个节点的感知、完整的感知历史记录或静态网络的假设。我们在真实交通数据集上评估FRIGATE，并证明其相对于现有最先进的方法具有更优异的预测性能。

    Modelling spatio-temporal processes on road networks is a task of growing importance. While significant progress has been made on developing spatio-temporal graph neural networks (Gnns), existing works are built upon three assumptions that are not practical on real-world road networks. First, they assume sensing on every node of a road network. In reality, due to budget-constraints or sensor failures, all locations (nodes) may not be equipped with sensors. Second, they assume that sensing history is available at all installed sensors. This is unrealistic as well due to sensor failures, loss of packets during communication, etc. Finally, there is an assumption of static road networks. Connectivity within networks change due to road closures, constructions of new roads, etc. In this work, we develop FRIGATE to address all these shortcomings. FRIGATE is powered by a spatio-temporal Gnn that integrates positional, topological, and temporal information into rich inductive node representatio
    
[^2]: GBSD: 带有阶段扩散的生成虚化效果

    GBSD: Generative Bokeh with Stage Diffusion. (arXiv:2306.08251v1 [cs.CV])

    [http://arxiv.org/abs/2306.08251](http://arxiv.org/abs/2306.08251)

    本文提出了第一个基于生成对抗网络技术的文本到图像模型GBSD，并使用阶段扩散技术合成具有虚化风格的照片。

    

    Bokeh效果是一种艺术技巧，可以使照片中的失焦区域模糊，近期由于文本到图像合成技术的发展以及智能手机的普及和照片分享应用程序的广泛使用，这种效果备受关注。以往的虚化效果渲染工作都是针对已有照片进行事后图像处理，使用经典计算机图形学或神经渲染技术产生类似的模糊效果，但是往往存在深度不连续的图像伪影，而且限制于练习数据测试的虚化效果。近期的扩散模型可以合成具有艺术风格的图像，但是要求生成高维度掩模或者进行昂贵的调整，可能影响整体图像特征。本文提出了GBSD，这是第一个基于生成对抗网络技术的文本到图像模型，具有虚化风格的照片。受扩散模型中逐步合成图像的启发，我们的方法使用了阶段扩散技术。

    The bokeh effect is an artistic technique that blurs out-of-focus areas in a photograph and has gained interest due to recent developments in text-to-image synthesis and the ubiquity of smart-phone cameras and photo-sharing apps. Prior work on rendering bokeh effects have focused on post hoc image manipulation to produce similar blurring effects in existing photographs using classical computer graphics or neural rendering techniques, but have either depth discontinuity artifacts or are restricted to reproducing bokeh effects that are present in the training data. More recent diffusion based models can synthesize images with an artistic style, but either require the generation of high-dimensional masks, expensive fine-tuning, or affect global image characteristics. In this paper, we present GBSD, the first generative text-to-image model that synthesizes photorealistic images with a bokeh style. Motivated by how image synthesis occurs progressively in diffusion models, our approach combi
    
[^3]: 点级监督下的半监督细胞识别方法

    Semi-supervised Cell Recognition under Point Supervision. (arXiv:2306.08240v1 [cs.CV])

    [http://arxiv.org/abs/2306.08240](http://arxiv.org/abs/2306.08240)

    本文提出了一种半监督的细胞识别方法，使用端到端PCR模型和生成的伪标签来更好地利用未标注的图像，开发了适用于SSPCR的框架，并引入了两种新的损失函数来协调标记和未标记样本。

    

    细胞识别是数字组织病理学图像分析中的基本任务。基于点的细胞识别方法通常需要大量的注释，耗时费力。半监督学习可以提供一种捷径，充分利用千亿像素的整张切片图像中的细胞信息而无需详尽标记。然而，关于半监督点级细胞识别（SSPCR）的研究仍未得到充分重视。先前的SSPCR工作都是基于密度图的PCR模型，其精度不尽如人意，推理速度缓慢，对超参数的敏感度高。为了解决这些问题，最近提出了端到端PCR模型。本文首次为端到端PCR模型开发了适用的SSPCR框架。总体而言，我们使用当前模型为未标记的图像生成伪标签，并将其用于监督模型的训练。此外，我们引入了两种新的损失函数，可以协调标记和未标记样本。为了验证我们方法的有效性，我们在两个公开数据集上进行了实验。实验结果表明，我们的方法以显著更少的注释成本和时间获得了最先进的性能。

    Cell recognition is a fundamental task in digital histopathology image analysis. Point-based cell recognition (PCR) methods normally require a vast number of annotations, which is extremely costly, time-consuming and labor-intensive. Semi-supervised learning (SSL) can provide a shortcut to make full use of cell information in gigapixel whole slide images without exhaustive labeling. However, research into semi-supervised point-based cell recognition (SSPCR) remains largely overlooked. Previous SSPCR works are all built on density map-based PCR models, which suffer from unsatisfactory accuracy, slow inference speed and high sensitivity to hyper-parameters. To address these issues, end-to-end PCR models are proposed recently. In this paper, we develop a SSPCR framework suitable for the end-to-end PCR models for the first time. Overall, we use the current models to generate pseudo labels for unlabeled images, which are in turn utilized to supervise the models training. Besides, we introdu
    
[^4]: Maestro: 一个用于教授人工智能容错性的游戏化平台

    Maestro: A Gamified Platform for Teaching AI Robustness. (arXiv:2306.08238v1 [cs.HC])

    [http://arxiv.org/abs/2306.08238](http://arxiv.org/abs/2306.08238)

    Maestro是一个用于教授人工智能容错性的游戏化平台，它为大学生们提供了具有生活启发的具有挑战性的任务，能够显著提高学生们在实验对抗攻击和防御方面的表现。

    

    尽管防止人工智能漏洞对于维护用户和企业的安全和隐私至关重要，但全球范围内对于容错型人工智能的教育工具仍然不够成熟。我们介绍了Maestro的设计、实施和评估。Maestro是一个开源的、有效的基于游戏的平台，有助于推动容错性人工智能教育的发展。Maestro提供目标为导向的场景，使大学生们在竞争性编程环境中接触到具有挑战性的以生活为灵感的任务。我们评估了Maestro对大学生在学习人工智能容错性方面的参与度、动机和学习成功的影响。该工作还提供了关于促进容错性人工智能领域中积极学习机会的在线学习工具设计特点的见解。我们分析了在两个大学AI课程中使用Maestro的147名本科生的反思响应（用Likert量表测量）。根据结果，表示获得新技能和知识、自我效力和满意度的学生，在实验对抗攻击和防御时表现出更显著的改进。

    Although the prevention of AI vulnerabilities is critical to preserve the safety and privacy of users and businesses, educational tools for robust AI are still underdeveloped worldwide. We present the design, implementation, and assessment of Maestro. Maestro is an effective open-source game-based platform that contributes to the advancement of robust AI education. Maestro provides goal-based scenarios where college students are exposed to challenging life-inspired assignments in a competitive programming environment. We assessed Maestro's influence on students' engagement, motivation, and learning success in robust AI. This work also provides insights into the design features of online learning tools that promote active learning opportunities in the robust AI domain. We analyzed the reflection responses (measured with Likert scales) of 147 undergraduate students using Maestro in two quarterly college courses in AI. According to the results, students who felt the acquisition of new ski
    
[^5]: 逆强化学习中的课程子目标

    Curricular Subgoals for Inverse Reinforcement Learning. (arXiv:2306.08232v1 [cs.LG])

    [http://arxiv.org/abs/2306.08232](http://arxiv.org/abs/2306.08232)

    本论文提出一种新的逆强化学习框架，即基于课程子目标的逆强化学习 (CSIRL)。与现有方法不同的是，该框架将一个任务分解为多个局部子目标，以引导智能体进行模仿，这有助于解决全局设计中存在的问题。

    

    逆强化学习（IRL）旨在从专家演示中重构奖励函数以促进策略学习，并已在模仿学习中展示出卓越的成功。现有的IRL方法主要集中于学习全局奖励函数以最小化模仿者和专家之间的轨迹差异，但这些全局设计仍然存在冗余噪声和误差传播问题，导致不适当的奖励分配，从而降低代理在复杂的多阶段任务中的能力。

    Inverse Reinforcement Learning (IRL) aims to reconstruct the reward function from expert demonstrations to facilitate policy learning, and has demonstrated its remarkable success in imitation learning. To promote expert-like behavior, existing IRL methods mainly focus on learning global reward functions to minimize the trajectory difference between the imitator and the expert. However, these global designs are still limited by the redundant noise and error propagation problems, leading to the unsuitable reward assignment and thus downgrading the agent capability in complex multi-stage tasks. In this paper, we propose a novel Curricular Subgoal-based Inverse Reinforcement Learning (CSIRL) framework, that explicitly disentangles one task with several local subgoals to guide agent imitation. Specifically, CSIRL firstly introduces decision uncertainty of the trained agent over expert trajectories to dynamically select subgoals, which directly determines the exploration boundary of differen
    
[^6]: SMC-UDA：结构模态约束的无监督跨领域肾脏分割方法

    SMC-UDA: Structure-Modal Constraint for Unsupervised Cross-Domain Renal Segmentation. (arXiv:2306.08213v1 [cs.CV])

    [http://arxiv.org/abs/2306.08213](http://arxiv.org/abs/2306.08213)

    本文提出了一种基于判别范式的结构模态约束的无监督跨领域肾脏分割方法，通过引入边缘结构来区分域不变的结构信息，并通过结构约束的自我学习和渐进式ROI完成肾脏分割，无需在目标领域上进行注释。

    

    基于深度学习的医学影像分割经常在部署到不同领域的图像时失败。领域适应方法旨在解决领域转移挑战，但仍面临一些问题。迁移学习方法需要在目标领域上进行注释，生成式无监督领域适应（UDA）模型忽略了特定于域的表示，其生成的质量严重限制了分割性能。在本研究中，我们提出了一种新颖的基于判别范式的结构模态约束（SMC）UDA框架，并引入了边缘结构作为跨域的桥梁。所提出的多模态学习骨干从图像纹理中提取结构信息以区分域不变的边缘结构。通过结构约束的自我学习和渐进式ROI，我们的方法通过定位边缘的三维空间结构来分割肾脏。我们在公共肾脏分割数据集上评估了SMC-UDA，从有标签的S数据组适应到不同领域。

    Medical image segmentation based on deep learning often fails when deployed on images from a different domain. The domain adaptation methods aim to solve domain-shift challenges, but still face some problems. The transfer learning methods require annotation on the target domain, and the generative unsupervised domain adaptation (UDA) models ignore domain-specific representations, whose generated quality highly restricts segmentation performance. In this study, we propose a novel Structure-Modal Constrained (SMC) UDA framework based on a discriminative paradigm and introduce edge structure as a bridge between domains. The proposed multi-modal learning backbone distills structure information from image texture to distinguish domain-invariant edge structure. With the structure-constrained self-learning and progressive ROI, our methods segment the kidney by locating the 3D spatial structure of the edge. We evaluated SMC-UDA on public renal segmentation datasets, adapting from the labeled s
    
[^7]: 使用集合变换器和分层双向LSTM从多智能体运动环境中推断球的轨迹

    Ball Trajectory Inference from Multi-Agent Sports Contexts Using Set Transformer and Hierarchical Bi-LSTM. (arXiv:2306.08206v1 [cs.MA])

    [http://arxiv.org/abs/2306.08206](http://arxiv.org/abs/2306.08206)

    本文提出了一种使用集合变换器和分层双向LSTM从多智能体运动环境中推断球的轨迹的推断框架，能够成功地预测球的轨迹，即使在复杂的情况下也是如此。

    

    随着人工智能应用到了许多领域，将AI应用于体育分析也受到了关注。然而，自动获取运动比赛中连续运动数据的难度是主要挑战之一。特别是，在一个有障碍物如遮挡和干扰的宽广足球场上可靠地跟踪一个小球是难以解决的难题。为了解决这个问题，本文提出了一种从球员轨迹推断球的轨迹的推断框架，作为球追踪的一种经济实用的替代方法。我们结合了集合变换器来获得多智能体环境的置换不变和等变表示，并采用分层架构，中间预测球员拥有权以支持最终的轨迹推断。此外，我们引入了现实损失项和后处理来确保估计轨迹的物理合理性。实验结果表明，我们的模型提供了自然和准确的多智能体运动环境翻译，并使得即使在复杂的情况下也能成功预测球的轨迹。

    As artificial intelligence spreads out to numerous fields, the application of AI to sports analytics is also in the spotlight. However, one of the major challenges is the difficulty of automated acquisition of continuous movement data during sports matches. In particular, it is a conundrum to reliably track a tiny ball on a wide soccer pitch with obstacles such as occlusion and imitations. Tackling the problem, this paper proposes an inference framework of ball trajectory from player trajectories as a cost-efficient alternative to ball tracking. We combine Set Transformers to get permutation-invariant and equivariant representations of the multi-agent contexts with a hierarchical architecture that intermediately predicts the player ball possession to support the final trajectory inference. Also, we introduce the reality loss term and postprocessing to secure the estimated trajectories to be physically realistic. The experimental results show that our model provides natural and accurate
    
[^8]: 解谜ARC：用物体为中心的决策Transformer模仿人类解决方案

    Unraveling the ARC Puzzle: Mimicking Human Solutions with Object-Centric Decision Transformer. (arXiv:2306.08204v1 [cs.AI])

    [http://arxiv.org/abs/2306.08204](http://arxiv.org/abs/2306.08204)

    该论文使用双重策略解决ARC任务，通过决策Transformer模仿人类解决方案和引入物体检测算法来提高AI的问题解决能力，同时揭示了数据和模型结构等方面的需求，为未来AGI研究提供了见解。

    

    在追求人工通用智能（AGI）的过程中，我们采用了一种新颖的双重方法来解决抽象推理文本集（ARC）任务。我们使用决策Transformer在模仿学习范式下建模人类问题解决，并引入了物体检测算法Push and Pull聚类方法。这种双重策略增强了AI的ARC问题解决能力，并为AGI进展提供了见解。然而，我们的工作揭示了高级数据收集工具、强大的培训数据集和精细的模型结构需求。这项研究突显了决策Transformer的潜在改进，并推动未来AGI研究。

    In the pursuit of artificial general intelligence (AGI), we tackle Abstraction and Reasoning Corpus (ARC) tasks using a novel two-pronged approach. We employ the Decision Transformer in an imitation learning paradigm to model human problem-solving, and introduce an object detection algorithm, the Push and Pull clustering method. This dual strategy enhances AI's ARC problem-solving skills and provides insights for AGI progression. Yet, our work reveals the need for advanced data collection tools, robust training datasets, and refined model structures. This study highlights potential improvements for Decision Transformers and propels future AGI research.
    
[^9]: 在标签噪声下的图上学习

    Learning on Graphs under Label Noise. (arXiv:2306.08194v1 [cs.LG])

    [http://arxiv.org/abs/2306.08194](http://arxiv.org/abs/2306.08194)

    该论文介绍了一种新方法 CGNN，它利用一致性图神经网络和基于同质性假设的样本选择技术，在标签噪声的情况下对节点分类进行建模，实现过滤出噪声节点和增强节点表示的鲁棒性。

    

    图上的节点分类是一项重要的任务，具有广泛的应用，包括社交分析和异常检测。虽然图神经网络（GNN）在这项任务上已经取得了一些有希望的结果，但目前的技术通常假设节点的标签信息是准确的，这在现实应用中可能并不成立。为了解决这个问题，我们研究了如何在标签噪声下的图上学习，并开发了一种名为一致性图神经网络（CGNN）的新方法来解决它。具体而言，我们采用了图对比学习作为正则化项，促进增强节点的两个视角具有一致的表示。由于这个正则化项不能利用标签信息，它可以增强节点表示对标签噪声的鲁棒性。此外，为了在图上检测噪声标签，我们提出了一种基于同质性假设的样本选择技术，通过测量嵌入和它们的邻居之间的一致性来识别噪声节点。各种真实世界数据集上的实验结果表明，CGNN可以有效地缓解标签噪声对节点分类的负面影响，并在对称和非对称标签噪声模型下优于最先进的基线。

    Node classification on graphs is a significant task with a wide range of applications, including social analysis and anomaly detection. Even though graph neural networks (GNNs) have produced promising results on this task, current techniques often presume that label information of nodes is accurate, which may not be the case in real-world applications. To tackle this issue, we investigate the problem of learning on graphs with label noise and develop a novel approach dubbed Consistent Graph Neural Network (CGNN) to solve it. Specifically, we employ graph contrastive learning as a regularization term, which promotes two views of augmented nodes to have consistent representations. Since this regularization term cannot utilize label information, it can enhance the robustness of node representations to label noise. Moreover, to detect noisy labels on the graph, we present a sample selection technique based on the homophily assumption, which identifies noisy nodes by measuring the consisten
    
[^10]: 自然语言处理中的表示实践

    Operationalising Representation in Natural Language Processing. (arXiv:2306.08193v1 [cs.CL])

    [http://arxiv.org/abs/2306.08193](http://arxiv.org/abs/2306.08193)

    本文介绍了一个框架，通过使用探测分类器来评估组件是否表示属性，填补了自然语言处理中关于“表示”的哲学空白。

    

    尽管“表示”在认知科学哲学中具有核心地位，但在当代自然语言处理实践中，几乎没有哲学领域的先前研究与之涉及。本文旨在填补这一空白：结合认知科学的思想，提出了一个框架来评估神经自然语言处理模型组件所作出的表示性声明，并提出三个评估组件是否表示属性的标准，并使用探测分类器来实现这些标准的操作化，探测分类器是NLP（和更广泛的深度学习）中流行的分析技术。操作化一个在哲学上受到启发的“表示”概念的项目应该引起科学哲学家和自然语言处理实践者的兴趣。对于哲学家来说，这提供了一个测试有关表示的本质的论据的新颖场地，并帮助NLPers组织有关探测实验的大量文献，提出了新的经验研究方向。

    Despite its centrality in the philosophy of cognitive science, there has been little prior philosophical work engaging with the notion of representation in contemporary NLP practice. This paper attempts to fill that lacuna: drawing on ideas from cognitive science, I introduce a framework for evaluating the representational claims made about components of neural NLP models, proposing three criteria with which to evaluate whether a component of a model represents a property and operationalising these criteria using probing classifiers, a popular analysis technique in NLP (and deep learning more broadly).  The project of operationalising a philosophically-informed notion of representation should be of interest to both philosophers of science and NLP practitioners. It affords philosophers a novel testing-ground for claims about the nature of representation, and helps NLPers organise the large literature on probing experiments, suggesting novel avenues for empirical research.
    
[^11]: DCTX-Conformer：针对低延迟统一流式和非流式Conformer的动态上下文传递

    DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer. (arXiv:2306.08175v1 [eess.AS])

    [http://arxiv.org/abs/2306.08175](http://arxiv.org/abs/2306.08175)

    提出了一种基于Conformer的新型动态上下文传递机制DCTX-Conformer，解决了流式识别性能差距的问题，相比于现有最优解，识别结果的误差率提高了25%，但对延迟影响可以忽略不计。

    

    基于Conformer的端到端模型现在已经变得普遍，被广泛用于流式和非流式自动语音识别（ASR）中。诸如双模式和动态分块训练等技术有助于统一流式和非流式系统。然而，在完整和有限的过去上下文的情况下，流式识别之间仍然存在着性能差距。为了解决这个问题，我们提出了一种新颖的动态上下文传递机制，将其与现有的最先进的统一ASR系统进行集成。我们的提议——动态上下文Conformer（DCTX-Conformer）利用了一个非重叠的上下文传递机制，同时考虑了一块的左上下文和一个或多个先前的上下文嵌入。由于额外的上下文嵌入，我们相对于目前最优解提升了25.0%的词错误率，而延迟影响可以忽略不计。

    Conformer-based end-to-end models have become ubiquitous these days and are commonly used in both streaming and non-streaming automatic speech recognition (ASR). Techniques like dual-mode and dynamic chunk training helped unify streaming and non-streaming systems. However, there remains a performance gap between streaming with a full and limited past context. To address this issue, we propose the integration of a novel dynamic contextual carry-over mechanism in a state-of-the-art (SOTA) unified ASR system. Our proposed dynamic context Conformer (DCTX-Conformer) utilizes a non-overlapping contextual carry-over mechanism that takes into account both the left context of a chunk and one or more preceding context embeddings. We outperform the SOTA by a relative 25.0% word error rate, with a negligible latency impact due to the additional context embeddings.
    
[^12]: 强化学习驱动的快速基于注意力的点云对齐的连接物设计

    Reinforcement Learning-Driven Linker Design via Fast Attention-based Point Cloud Alignment. (arXiv:2306.08166v1 [cs.LG])

    [http://arxiv.org/abs/2306.08166](http://arxiv.org/abs/2306.08166)

    该论文提出了一种强化学习驱动的链接物设计方法，成功地生成满足2D和3D要求的连接剂，实现了新型连接剂的制备。

    

    Proteolysis-Targeting Chimeras(PROTACs)是一类新型小分子，设计成桥梁，将E3连接酶和与疾病相关的蛋白质连接起来，从而促进其后续降解。 PROTACs由两个蛋白质结合的“活性”区域组成，由“连接”区域连接。 连接区域的设计由其相互作用给出几何和化学约束条件，并需要最大化药物样本。 为了解决这些挑战，我们介绍了ShapeLinker，一种用于全新设计链接物的方法。 它使用强化学习在自回归SMILES生成器上进行片段连接。 该方法通过结合相关的物理化学性质和一种新颖的基于注意力的点云对齐得分来对其进行优化。 这种新方法成功地生成满足相关2D和3D要求的连接剂，并在生成假定目标连接剂构造的新型连接剂方面取得了最先进的成果。

    Proteolysis-Targeting Chimeras (PROTACs) represent a novel class of small molecules which are designed to act as a bridge between an E3 ligase and a disease-relevant protein, thereby promoting its subsequent degradation. PROTACs are composed of two protein binding "active" domains, linked by a "linker" domain. The design of the linker domain is challenging due to geometric and chemical constraints given by its interactions, and the need to maximize drug-likeness. To tackle these challenges, we introduce ShapeLinker, a method for de novo design of linkers. It performs fragment-linking using reinforcement learning on an autoregressive SMILES generator. The method optimizes for a composite score combining relevant physicochemical properties and a novel, attention-based point cloud alignment score. This new method successfully generates linkers that satisfy both relevant 2D and 3D requirements, and achieves state-of-the-art results in producing novel linkers assuming a target linker confor
    
[^13]: INT2.1：通过低秩自适应纠错实现精细可调的量化大语言模型

    INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation. (arXiv:2306.08162v1 [cs.CL])

    [http://arxiv.org/abs/2306.08162](http://arxiv.org/abs/2306.08162)

    该论文提出了一种通过低秩自适应纠错的方法，从而可以显著地减少精细调整VRAM需求，并纠正量化大语言模型中的量化误差，使消费者笔记本电脑可以对70亿个参数的大语言模型进行精细调整，生成连贯的英文文本。

    

    我们提出了一种方法，可以显著地减少精细调整VRAM需求，并纠正量化大语言模型中的量化误差。首先，我们使用低秩自适应（LoRA）开发了一种极其内存高效的量化模型精细调整方法（EMEF），并根据它构建了一个错误修正算法，旨在最小化量化过程中引起的误差。我们的方法可以将内存要求降低多达5.6倍，从而使消费者笔记本电脑可以对70亿个参数的大语言模型进行精细调整。同时，我们提出了一种低秩纠错（LREC）方法，利用增加的LoRA层来改善量化模型与其浮点数对应物之间的差距。我们的纠错框架可以生成连贯的英文文本，实现了完全功能的INT2量化大语言模型。据我们所知，这是第一个能够达到这种性能的INT2大语言模型。

    We introduce a method that dramatically reduces fine-tuning VRAM requirements and rectifies quantization errors in quantized Large Language Models. First, we develop an extremely memory-efficient fine-tuning (EMEF) method for quantized models using Low-Rank Adaptation (LoRA), and drawing upon it, we construct an error-correcting algorithm designed to minimize errors induced by the quantization process. Our method reduces the memory requirements by up to 5.6 times, which enables fine-tuning a 7 billion parameter Large Language Model (LLM) on consumer laptops. At the same time, we propose a Low-Rank Error Correction (LREC) method that exploits the added LoRA layers to ameliorate the gap between the quantized model and its float point counterpart. Our error correction framework leads to a fully functional INT2 quantized LLM with the capacity to generate coherent English text. To the best of our knowledge, this is the first INT2 Large Language Model that has been able to reach such a perfo
    
[^14]: h2oGPT：民主化大语言模型

    h2oGPT: Democratizing Large Language Models. (arXiv:2306.08161v1 [cs.CL])

    [http://arxiv.org/abs/2306.08161](http://arxiv.org/abs/2306.08161)

    本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs），包括100％私有文档搜索。目标是创建真正开源的替代封闭源GPTs，提高人工智能的开发和可靠性。

    

    基于生成预训练变压器（GPTs），大语言模型（LLMs）如GPT-4因其在自然语言处理方面的现实应用而成为人工智能革命的一部分。然而，它们也带来了许多重大的风险，如存在有偏见、私人或有害文本和未经授权的版权材料。本文介绍了h2oGPT，这是一套开源代码库，用于创建和使用基于GPTs的大语言模型（LLMs）。该项目的目标是创建世界上最好的真正开源的替代封闭源GPTs。与开源社区合作，作为其一部分，我们开源了几个LLM，其参数从7亿到400亿，可在完全自由的Apache 2.0许可下商用。我们的发布包括使用自然语言的100％私有文档搜索。开源语言模型有助于促进人工智能的发展并使其更加可靠。

    Foundation Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their real-world applications though natural language processing. However, they also pose many significant risks such as the presence of biased, private, or harmful text, and the unauthorized inclusion of copyrighted material.  We introduce h2oGPT, a suite of open-source code repositories for the creation and use of Large Language Models (LLMs) based on Generative Pretrained Transformers (GPTs). The goal of this project is to create the world's best truly open-source alternative to closed-source GPTs. In collaboration with and as part of the incredible and unstoppable open-source community, we open-source several fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial use under fully permissive Apache 2.0 licenses. Included in our release is 100% private document search using natural language.  Open-source language models help boost AI development and make it more accessible
    
[^15]: 自然语言处理中社会人口统计偏见的调查

    Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])

    [http://arxiv.org/abs/2306.08158](http://arxiv.org/abs/2306.08158)

    本文调查了209篇关于NLP模型偏见的论文，其中大部分涉及社会人口统计偏见。研究者提出了社会人口统计偏见的定义，并确定了NLP偏见研究的三个主要类别。当前去偏见技术只是隐藏了偏见而不是真正去除它，需要进一步改进。

    

    深度神经网络在训练过程中往往会学习到非预期的偏见，这在实际应用中可能会产生有害的影响。本文对209篇关于NLP模型中偏见的论文进行了调查，其中大部分论文涉及社会人口统计偏见。为了更好地理解偏见与真实世界的危害之间的区别，我们借鉴心理学和行为经济学的思想，提出了社会人口统计偏见的定义。我们确定了NLP偏见研究的三个主要类别：偏见类型、量化偏见和去偏见。我们认为当前对于量化偏见的方法存在可靠性问题，许多偏见度量并不涉及真实世界中的偏见，当前的去偏见技术是表面的，只是隐藏了偏见，而不是真正去除它。最后，我们提供了未来工作的建议。

    Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
    
[^16]: 使用动态贝叶斯网络进行加密货币价格方向因果特征工程

    Causal Feature Engineering of Price Directions of Cryptocurrencies using Dynamic Bayesian Networks. (arXiv:2306.08157v1 [cs.LG])

    [http://arxiv.org/abs/2306.08157](http://arxiv.org/abs/2306.08157)

    本文提出了一种基于动态贝叶斯网络的方法，来预测加密货币价格方向，以帮助投资者做出明智的投资决策。

    

    加密货币在各个领域，特别是金融和投资领域中越来越受到关注。其独特的区块链相关特性，如隐私、去中心化和不可追踪性，部分原因是其受欢迎的原因。然而，由于加密货币价格的波动性和不确定性，加密货币仍然是一种高风险投资。本文提出了一个动态贝叶斯网络（DBN）方法，可以在多元设置下模拟复杂系统，以预测五种流行加密货币的价格运动方向，以解决这个问题。

    Cryptocurrencies have gained popularity across various sectors, especially in finance and investment. The popularity is partly due to their unique specifications originating from blockchain-related characteristics such as privacy, decentralisation, and untraceability. Despite their growing popularity, cryptocurrencies remain a high-risk investment due to their price volatility and uncertainty. The inherent volatility in cryptocurrency prices, coupled with internal cryptocurrency-related factors and external influential global economic factors makes predicting their prices and price movement directions challenging. Nevertheless, the knowledge obtained from predicting the direction of cryptocurrency prices can provide valuable guidance for investors in making informed investment decisions. To address this issue, this paper proposes a dynamic Bayesian network (DBN) approach, which can model complex systems in multivariate settings, to predict the price movement direction of five popular a
    
[^17]: 非线性个性化预测的神经混合效应

    Neural Mixed Effects for Nonlinear Personalized Predictions. (arXiv:2306.08149v1 [cs.LG])

    [http://arxiv.org/abs/2306.08149](http://arxiv.org/abs/2306.08149)

    本文提出了神经混合效应（NME）模型，用于个性化预测，并通过结合个人通用和个人特定参数来考虑线性和非线性趋势。

    

    个性化预测是一种机器学习方法，根据过去标记观测预测一个人未来的观测值，通常用于连续任务，例如预测日常情绪评分。在进行个性化预测时，模型可以结合两种趋势：（a）跨人共享的趋势，即个人通用趋势，例如周末更开心，和（b）每个人独特的趋势，即个人特定的趋势，例如每周有一次压力大的会议。混合效应模型是一种流行的统计模型，用于通过组合个人通用和个人特定参数来研究这两种趋势。尽管现在线性混合效应模型通过将其与神经网络整合而变得越来越流行，但这种整合目前仅限于线性个人特定参数：排除非线性个人特定趋势。在本文中，我们提出了神经混合效应（NME）模型，以优化非线性个人特定参数。

    Personalized prediction is a machine learning approach that predicts a person's future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a 
    
[^18]: ArtWhisperer：一个用于描述艺术创作中人工智能与人类交互的数据集

    ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations. (arXiv:2306.08141v1 [cs.AI])

    [http://arxiv.org/abs/2306.08141](http://arxiv.org/abs/2306.08141)

    为研究人工智能与人类交互，研究者创建了ArtWhisperer数据集，这是一个在线游戏，人们通过反复尝试不同的提示词，来生成和目标图像类似的图像，并记录了50,000多个交互记录。在初步分析中，研究者发现人们提交了各种各样的提示词，并能够发现生成各种文本描述的图像。

    

    随着生成型人工智能越来越普及，研究人类用户如何与这些模型交互变得越来越重要。在这项工作中，我们研究了人们如何使用文本到图像的模型生成所需的目标图像。为了研究这种交互，我们创建了ArtWhisperer，这是一个在线游戏，用户会得到一个目标图像，并需要反复尝试不同的提示词，以便生成类似目标图像的图像。通过这个游戏，我们记录了50,000多个人工智能-人类交互的记录；每个交互都对应着用户创建的一个提示词和相应生成的图像。大多数记录都是重复的交互，用户通过反复尝试找到最佳的提示词以生成目标图像，这使得这个数据集成为研究人工智能与人类协作的独特连续数据集。在对这个数据集的初步分析中，我们发现了一些提示词交互和用户策略的特征。人们提交了各种各样的提示词，并能够发现生成各种文本描述的图像。

    As generative AI becomes more prevalent, it is important to study how human users interact with such models. In this work, we investigate how people use text-to-image models to generate desired target images. To study this interaction, we created ArtWhisperer, an online game where users are given a target image and are tasked with iteratively finding a prompt that creates a similar-looking image as the target. Through this game, we recorded over 50,000 human-AI interactions; each interaction corresponds to one text prompt created by a user and the corresponding generated image. The majority of these are repeated interactions where a user iterates to find the best prompt for their target image, making this a unique sequential dataset for studying human-AI collaborations. In an initial analysis of this dataset, we identify several characteristics of prompt interactions and user strategies. People submit diverse prompts and are able to discover a variety of text descriptions that generate
    
[^19]: AVIS:利用大型语言模型的自主视觉信息检索

    AVIS: Autonomous Visual Information Seeking with Large Language Models. (arXiv:2306.08129v1 [cs.CV])

    [http://arxiv.org/abs/2306.08129](http://arxiv.org/abs/2306.08129)

    本文提出了一个基于大型语言模型的自主信息检索视觉问答框架AVIS，可以解决视觉问题所需的外部知识获取问题。

    

    本文提出了一种利用大型语言模型（LLM）实现自主信息检索的视觉问答框架AVIS。我们的方法利用LLM动态地制定利用外部工具的策略，并调查它们的输出，从而获取提供所提出问题所需的不可或缺的知识。回答需要外部知识的视觉问题，如“这幅图像所描绘的建筑物是为了纪念哪个事件？”，是一项复杂的任务。这个任务呈现出一个组合搜索空间，需要一系列行动，包括调用API、分析它们的响应并做出明智的决策。我们进行了一个用户研究，收集了人类面对这个任务时各种各样的决策实例。然后利用这些数据设计了一个由三个组件组成的系统：一个由LLM驱动的规划器，动态确定下一个要使用的工具；一个由LLM驱动的推理器，分析并提取关键信息。

    In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as "What event is commemorated by the building depicted in this image?", is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information 
    
[^20]: PersonaPKT：通过参数高效的知识迁移构建个性化对话代理

    PersonaPKT: Building Personalized Dialogue Agents via Parameter-efficient Knowledge Transfer. (arXiv:2306.08126v1 [cs.CL])

    [http://arxiv.org/abs/2306.08126](http://arxiv.org/abs/2306.08126)

    本文提出了PersonaPKT，一种轻量级迁移学习方法，可以在没有明确个性描述的情况下构建符合角色的对话模型，该方法通过将每个个性表示为一个连续向量，直接从同一角色产生的少量对话样本中学习隐含的个性特定特征，并能够高效构建个性化对话代理并增强隐私保护。

    

    大型预训练语言模型（PLM）驱动的个性化对话代理（DA）通常依赖于明确的角色描述来保持个性的一致性。然而，这些描述可能并不总是可用或可能存在隐私问题。为了解决这个问题，本文引入了PersonaPKT，这是一种轻量级的迁移学习方法，可以在没有明确个性描述的情况下构建符合角色的对话模型。通过将每个个性表示为一个连续向量，PersonaPKT直接从同一角色产生的少量对话样本中学习隐含的个性特定特征，并在PLM骨干上增加少于0.1％的可训练参数。实验证明，PersonaPKT可以高效地构建个性化DA，并在保持良好的响应生成质量的同时，在角色的一致性方面优于各种基线。此外，它通过避免明确的个性描述来增强隐私保护。

    Personalized dialogue agents (DAs) powered by large pre-trained language models (PLMs) often rely on explicit persona descriptions to maintain personality consistency. However, such descriptions may not always be available or may pose privacy concerns. To tackle this bottleneck, we introduce PersonaPKT, a lightweight transfer learning approach that can build persona-consistent dialogue models without explicit persona descriptions. By representing each persona as a continuous vector, PersonaPKT learns implicit persona-specific features directly from a small number of dialogue samples produced by the same persona, adding less than 0.1% trainable parameters for each persona on top of the PLM backbone. Empirical results demonstrate that PersonaPKT effectively builds personalized DAs with high storage efficiency, outperforming various baselines in terms of persona consistency while maintaining good response generation quality. In addition, it enhances privacy protection by avoiding explicit
    
[^21]: 从句子到文档层面的AI生成抄袭检测：超越黑匣子方法

    Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level. (arXiv:2306.08122v1 [cs.CL])

    [http://arxiv.org/abs/2306.08122](http://arxiv.org/abs/2306.08122)

    该论文提出了一种新的自然语言处理方法，可对学术写作中的抄袭行为进行有效检测，不仅在句子级别上进行评估，还在文档级别上提供可量化指标。该方法的准确率高达94％，具有较强的适应性和可靠性，能够不断随着LLM技术的发展而不断改进。

    

    学术写作中对大型语言模型（LLMs）越来越依赖导致了抄袭现象的增加。现有的AI生成文本分类器准确性有限，往往产生误报。我们提出一种新方法，利用自然语言处理（NLP）技术，提供句子和文档级别的可量化指标，方便人类评估者进行解释。我们的方法采用多方面的方法，生成给定问题的多个释义版本，将它们输入LLM以生成答案。通过使用基于余弦相似度的对比损失函数，将生成的句子与学生回答中的句子匹配。我们的方法在分类人类和AI文本方面的准确性达到了94％，为学术环境中的抄袭检测提供了强大而适应性强的解决方案。该方法随着LLM技术的发展而不断改进，减少了新模型训练或重新配置的需求，并提供了比传统黑匣子方法更透明的评估方式。

    The increasing reliance on large language models (LLMs) in academic writing has led to a rise in plagiarism. Existing AI-generated text classifiers have limited accuracy and often produce false positives. We propose a novel approach using natural language processing (NLP) techniques, offering quantifiable metrics at both sentence and document levels for easier interpretation by human evaluators. Our method employs a multi-faceted approach, generating multiple paraphrased versions of a given question and inputting them into the LLM to generate answers. By using a contrastive loss function based on cosine similarity, we match generated sentences with those from the student's response. Our approach achieves up to 94% accuracy in classifying human and AI text, providing a robust and adaptable solution for plagiarism detection in academic settings. This method improves with LLM advancements, reducing the need for new model training or reconfiguration, and offers a more transparent way of ev
    
[^22]: 能否通过 ChatGPT 实现智能交通系统？使用强化学习实现混合交通流控制的案例研究

    Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning. (arXiv:2306.08094v1 [cs.AI])

    [http://arxiv.org/abs/2306.08094](http://arxiv.org/abs/2306.08094)

    本文研究探讨使用 ChatGPT 解决混合交通流控制问题，通过大规模用户研究发现 ChatGPT 在某些环境下能够提高成功策略数量

    

    强化学习在智能交通系统中的应用不断增多，同时也凸显了一些关键问题。本文研究使用大型语言模型 ChatGPT 研究是否可以帮助解决复杂的混合交通流控制问题，通过一个大规模的用户研究，发现 ChatGPT 在某些环境下能够增加成功策略数量

    The surge in Reinforcement Learning (RL) applications in Intelligent Transportation Systems (ITS) has contributed to its growth as well as highlighted key challenges. However, defining objectives of RL agents in traffic control and management tasks, as well as aligning policies with these goals through an effective formulation of Markov Decision Process (MDP), can be challenging and often require domain experts in both RL and ITS. Recent advancements in Large Language Models (LLMs) such as GPT-4 highlight their broad general knowledge, reasoning capabilities, and commonsense priors across various domains. In this work, we conduct a large-scale user study involving 70 participants to investigate whether novices can leverage ChatGPT to solve complex mixed traffic control problems. Three environments are tested, including ring road, bottleneck, and intersection. We find ChatGPT has mixed results. For intersection and bottleneck, ChatGPT increases number of successful policies by 150% and 
    
[^23]: DORSal: 基于扩散的物体中心场景表示

    DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.}$. (arXiv:2306.08068v1 [cs.CV])

    [http://arxiv.org/abs/2306.08068](http://arxiv.org/abs/2306.08068)

    DORSal提出了一种基于扩散模型的物体中心场景表示方法，可以呈现高保真新视图，并在较大程度上保留了诸如基于物体的场景编辑之类的优点。

    

    最近在三维场景理解方面取得的进展使跨大量不同场景的数据集的可扩展表示学习成为可能。因此，对于未见过的场景和物体的泛化，仅通过单个或少数图像渲染新视图，以及支持编辑的可控场景生成现在成为可能。然而，联合训练大量场景通常会在渲染质量上妥协，而与单个场景优化模型（如NeRF）相比。在本文中，我们利用最近扩散模型的进展，使三维场景表示学习模型具备呈现高保真新视图的能力，同时在较大程度上保留了诸如基于物体的场景编辑之类的优点。特别地，我们提出了DORSal，它基于扩散视频架构，为基于物体中心的场景插槽表示的三维场景生成提供适应性。我们在复杂的合成多物体场景和现实世界大规模街景数据集上证明，我们的模型能够生成高质量的场景新视图，同时支持物体级别的编辑，并保留细粒度的纹理和反射等细节。

    Recent progress in 3D scene understanding enables scalable learning of representations across large datasets of diverse scenes. As a consequence, generalization to unseen scenes and objects, rendering novel views from just a single or a handful of input images, and controllable scene generation that supports editing, is now possible. However, training jointly on a large number of scenes typically compromises rendering quality when compared to single-scene optimized models such as NeRFs. In this paper, we leverage recent progress in diffusion models to equip 3D scene representation learning models with the ability to render high-fidelity novel views, while retaining benefits such as object-level scene editing to a large degree. In particular, we propose DORSal, which adapts a video diffusion architecture for 3D scene generation conditioned on object-centric slot-based representations of scenes. On both complex synthetic multi-object scenes and on the real-world large-scale Street View d
    
[^24]: 通过控制变量基因表达式编程进行符号回归

    Symbolic Regression via Control Variable Genetic Programming. (arXiv:2306.08057v1 [cs.NE])

    [http://arxiv.org/abs/2306.08057](http://arxiv.org/abs/2306.08057)

    CVGP是一种通过控制变量实验设计加快符号表达式发现的方法，能够学习复杂符号表达式，扩展了现有方法的能力。

    

    直接从实验数据中学习符号表达式是人工智能驱动的科学发现的重要步骤。然而，现有的方法仅限于学习简单的表达式。回归涉及许多自变量的表达式仍然难以实现。受科学界广泛使用的控制变量实验的启发，我们提出了一种基于控制变量的基因表达式编程（CVGP）方法，用于多个自变量的符号回归。CVGP通过定制实验设计，而不是从预先收集的固定数据集中学习，加快了符号表达式的发现过程。它首先使用基因表达式编程拟合涉及少量自变量的简单表达式，在控制变量实验中，其中其他变量被保持为常量。然后通过增加新的自变量扩展以前学习到的表达式，使用新的控制变量实验，在这些实验中，这些变量被允许变化。理论上，我们展示了CVGP可以有效地探索潜在解决方案的空间，并在基准数据集上验证了其有效性。我们的结果表明，CVGP能够学习涉及许多自变量的复杂符号表达式，而现有的方法无法处理。

    Learning symbolic expressions directly from experiment data is a vital step in AI-driven scientific discovery. Nevertheless, state-of-the-art approaches are limited to learning simple expressions. Regressing expressions involving many independent variables still remain out of reach. Motivated by the control variable experiments widely utilized in science, we propose Control Variable Genetic Programming (CVGP) for symbolic regression over many independent variables. CVGP expedites symbolic expression discovery via customized experiment design, rather than learning from a fixed dataset collected a priori. CVGP starts by fitting simple expressions involving a small set of independent variables using genetic programming, under controlled experiments where other variables are held as constants. It then extends expressions learned in previous generations by adding new independent variables, using new control variable experiments in which these variables are allowed to vary. Theoretically, we
    
[^25]: 基于软件架构视角的分布式信任研究

    Distributed Trust Through the Lens of Software Architecture. (arXiv:2306.08056v1 [cs.CR])

    [http://arxiv.org/abs/2306.08056](http://arxiv.org/abs/2306.08056)

    本文从多个学科的角度对分布式信任概念进行概述，并探讨由分布式信任技术实现的信任重分配/转移及相关的系统与应用的权衡。

    

    分布式信任是一个近年来从不同角度演化出的模糊概念。虽然目前人们普遍将其归功于区块链和加密货币，但分布式信任的概念正在生态系统中推动联邦学习、可信和负责任人工智能、数据共享、跨组织边界的隐私问题和零信任网络安全等领域的进展。本文将从多个学科的角度对分布式信任的概念进行概述，并从系统/软件架构的角度探讨由分布式信任技术实现的信任重分配/转移及相关的系统与应用的权衡。

    Distributed trust is a nebulous concept that has evolved from different perspectives in recent years. While one can attribute its current prominence to blockchain and cryptocurrency, the distributed trust concept has been cultivating progress in federated learning, trustworthy and responsible AI in an ecosystem setting, data sharing, privacy issues across organizational boundaries, and zero trust cybersecurity. This paper will survey the concept of distributed trust in multiple disciplines. It will take a system/software architecture point of view to look at trust redistribution/shift and the associated tradeoffs in systems and applications enabled by distributed trust technologies.
    
[^26]: 在伸缩性训练中优化超参数：计算效率训练的超参数优化

    Tune As You Scale: Hyperparameter Optimization For Compute Efficient Training. (arXiv:2306.08055v1 [cs.LG])

    [http://arxiv.org/abs/2306.08055](http://arxiv.org/abs/2306.08055)

    该论文提出了一种名为“CARBS”的算法，它利用贝叶斯优化算法在性能-计算 Pareto 前沿附近执行局部搜索来解决大型深度学习模型超参数调整的问题。该方法适用于具有许多超参数的无界搜索空间，学习缩放关系，并自动化了许多调整中的“黑魔法”。此外，该方法对计算受限制的情况尤其有效。

    

    深度学习模型的超参数调整可以使相同的计算量获得数量级的性能提升。尽管如此，系统调整还不普遍，尤其是对于大型模型更是如此，这些模型评估昂贵，超参数较多，需要进行难以把握的折中、预算和搜索边界决策。为了解决这些问题并提出一种实用的方法来稳健地调整大型模型，我们提出了成本感知 Pareto 区域贝叶斯搜索（CARBS），这是一种贝叶斯优化算法，它在性能-计算 Pareto 前沿附近执行局部搜索。CARBS 在具有许多超参数的无界搜索空间中表现良好，学习缩放关系，因此即使在模型缩放的同时也可以调整模型，并自动化了许多“黑魔法”调整。在我们的结果中，我们通过调整简单的基线（ProcGen 论文中提供的 PPO 方法）有效地解决了整个 ProcGen 基准测试。我们还在使用更少的评估时复制了 Bertinetto 等人的模型选择结果。我们的方法通常适用，但特别适合计算受限的情况，其中设计师可以轻松评估或限制培训成本。

    Hyperparameter tuning of deep learning models can lead to order-of-magnitude performance gains for the same amount of compute. Despite this, systematic tuning is uncommon, particularly for large models, which are expensive to evaluate and tend to have many hyperparameters, necessitating difficult judgment calls about tradeoffs, budgets, and search bounds. To address these issues and propose a practical method for robustly tuning large models, we present Cost-Aware Pareto Region Bayesian Search (CARBS), a Bayesian optimization algorithm that performs local search around the performance-cost Pareto frontier. CARBS does well even in unbounded search spaces with many hyperparameters, learns scaling relationships so that it can tune models even as they are scaled up, and automates much of the "black magic" of tuning. Among our results, we effectively solve the entire ProcGen benchmark just by tuning a simple baseline (PPO, as provided in the original ProcGen paper). We also reproduce the mo
    
[^27]: 剪枝方式提高可靠策略：一种多目标深度Q学习方法应用于重症护理

    Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning Approach to Critical Care. (arXiv:2306.08044v1 [cs.LG])

    [http://arxiv.org/abs/2306.08044](http://arxiv.org/abs/2306.08044)

    该论文介绍了一种深度Q学习方法，通过剪枝动作集来实现将中间生物标志物信号整合到奖励规范中，提高了重症护理策略的可靠性。

    

    大多数医疗决策具有连续性，因此，强化学习可能有望制定精确的数据驱动治疗计划。然而，该领域的主要挑战之一是主要基于死亡率的奖励函数的稀疏性，导致离线估计的稳定性降低。本研究引入了一种深度Q学习方法，能够获得更可靠的重症护理策略。该方法将相关但嘈杂的中间生物标志物信号整合到奖励规范中，同时不会损害感兴趣的主要结果（例如患者生存率）的优化。通过根据所有可用奖励对动作集进行剪枝，然后基于稀疏主要奖励，使用受限动作集进行最终模型训练，通过解离准确和近似奖励来最小化主要目标的潜在扭曲，实现了上述目标。

    Most medical treatment decisions are sequential in nature. Hence, there is substantial hope that reinforcement learning may make it possible to formulate precise data-driven treatment plans. However, a key challenge for most applications in this field is the sparse nature of primarily mortality-based reward functions, leading to decreased stability of offline estimates. In this work, we introduce a deep Q-learning approach able to obtain more reliable critical care policies. This method integrates relevant but noisy intermediate biomarker signals into the reward specification, without compromising the optimization of the main outcome of interest (e.g. patient survival). We achieve this by first pruning the action set based on all available rewards, and second training a final model based on the sparse main reward but with a restricted action set. By disentangling accurate and approximated rewards through action pruning, potential distortions of the main objective are minimized, all whi
    
[^28]: FLamE: 自然语言解释下的少样本学习

    FLamE: Few-shot Learning from Natural Language Explanations. (arXiv:2306.08042v1 [cs.CL])

    [http://arxiv.org/abs/2306.08042](http://arxiv.org/abs/2306.08042)

    FLamE是一个利用GPT-3生成自然语言解释用于RoBERTa微调的少样本学习框架，在自然语言推理任务上展现出较好的性能，但是人生成的解释大多不能充分地证明分类决策，标签特定线索在解释中起着重要作用。

    

    自然语言解释在理论上具有为模型推理提供丰富信息的潜力。然而，Lampinen等人的最新研究表明，自然语言解释在提高分类方面的效用有限。为了有效地从解释中学习，我们提出了FLamE，这是一个两阶段的少样本学习框架，首先使用GPT-3生成解释，然后使用生成的解释对较小的模型（例如RoBERTa）进行微调。自然语言推理实验表明，与强基线相比，FLamE具有很好的效果，在e-SNLI数据集上的准确性比GPT-3 Babbage提高了17.6％，比GPT-3 Davinci提高了5.7％。尽管提高了分类性能，但是人类评估出人生成的大部分解释都不能充分地证明分类决策。额外的分析表明，标签特定线索（如中立标签的“不知道”）在生成解释中发挥了重要作用。

    Natural language explanations have the potential to provide rich information that in principle guides model reasoning. Yet, recent work by Lampinen et al. (2022) has shown limited utility of natural language explanations in improving classification. To effectively learn from explanations, we present FLamE, a two-stage few-shot learning framework that first generates explanations using GPT-3, and then finetunes a smaller model (e.g., RoBERTa) with generated explanations. Our experiments on natural language inference demonstrate effectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3 Babbage and 5.7% over GPT-3 Davinci in e-SNLI. Despite improving classification performance, human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions. Additional analyses point to the important role of label-specific cues (e.g., "not know" for the neutral label) in generated explanations.
    
[^29]: 关于伪造纳什均衡的研究

    On Faking a Nash Equilibrium. (arXiv:2306.08041v1 [cs.MA])

    [http://arxiv.org/abs/2306.08041](http://arxiv.org/abs/2306.08041)

    本文研究多智能体强化学习中的数据污染攻击，提出了唯一纳什集的概念，并设计了一个线性规划方案来计算最优污染攻击策略。

    

    本文研究了多智能体强化学习中的数据污染攻击，攻击者试图更改数据集以安装（潜在虚假的）唯一马尔可夫完美纳什均衡点(Nash equilibrium)。我们提出了唯一纳什集的概念，即由其Q函数规定的游戏的集合，其具有唯一的联合策略作为唯一的纳什均衡点。唯一纳什集对于污染攻击非常重要，因为只有当数据污染使所有合理的游戏都在其中时，攻击才成功。唯一纳什集将常用于逆强化学习中的奖励多面体推广到多智能体强化学习中。对于零和马尔科夫博弈，逆纳什集以及由数据引起的合理游戏集都是Q函数空间中的多面体。我们提出了一个线性规划方案以有效地计算最优的污染攻击策略。我们的工作为设计更加鲁棒的多智能体强化学习算法之前必要的步骤揭示了离线MARL数据污染攻击结构的一些特点。

    We characterize offline data poisoning attacks on Multi-Agent Reinforcement Learning (MARL), where an attacker may change a data set in an attempt to install a (potentially fictitious) unique Markov-perfect Nash equilibrium. We propose the unique Nash set, namely the set of games, specified by their Q functions, with a specific joint policy being the unique Nash equilibrium. The unique Nash set is central to poisoning attacks because the attack is successful if and only if data poisoning pushes all plausible games inside it. The unique Nash set generalizes the reward polytope commonly used in inverse reinforcement learning to MARL. For zero-sum Markov games, both the inverse Nash set and the set of plausible games induced by data are polytopes in the Q function space. We exhibit a linear program to efficiently compute the optimal poisoning attack. Our work sheds light on the structure of data poisoning attacks on offline MARL, a necessary step before one can design more robust MARL alg
    
[^30]: 可变通道维度的可微架构搜索方法

    Flexible Channel Dimensions for Differentiable Architecture Search. (arXiv:2306.08021v1 [cs.LG])

    [http://arxiv.org/abs/2306.08021](http://arxiv.org/abs/2306.08021)

    本文提出了一种新颖的可微神经架构搜索方法，使用动态通道分配算法实现通道维度的灵活搜索空间，能够有效地找到等同于以前方法在CIFAR-10数据集上任务准确性和推理延迟方面的DNN架构，并且不需要手动设计或架构工程专业知识。

    

    在计算资源有限的条件下设计表现良好的深度神经网络，找到最优的通道维度（即DNN层中的过滤器数量）至关重要。最近的神经架构搜索工作旨在自动化DNN模型实现的优化。然而，现有的通道维度神经架构搜索方法依赖于固定的搜索空间，这阻碍了实现高效且完全自动化的解决方案。在这项工作中，我们提出了一种新颖的可微神经架构搜索方法，配备有效的动态通道分配算法，以实现通道维度的灵活搜索空间。我们展示了所提出的框架能够找到等同于以前方法在CIFAR-10数据集上任务准确性和推理延迟方面的DNN架构，architecture search阶段GPU-hours提高了1.3-1.7倍，内存要求提高了1.5-1.7倍。此外，所提出的框架不需要任何手动设计或架构工程专业知识，并且可以轻松扩展到各种DNN架构和不同的数据集。

    Finding optimal channel dimensions (i.e., the number of filters in DNN layers) is essential to design DNNs that perform well under computational resource constraints. Recent work in neural architecture search aims at automating the optimization of the DNN model implementation. However, existing neural architecture search methods for channel dimensions rely on fixed search spaces, which prevents achieving an efficient and fully automated solution. In this work, we propose a novel differentiable neural architecture search method with an efficient dynamic channel allocation algorithm to enable a flexible search space for channel dimensions. We show that the proposed framework is able to find DNN architectures that are equivalent to previous methods in task accuracy and inference latency for the CIFAR-10 dataset with an improvement of $1.3-1.7\times$ in GPU-hours and $1.5-1.7\times$ in the memory requirements during the architecture search stage. Moreover, the proposed frameworks do not re
    
[^31]: Curatr：历史文学文本语义分析与筛选平台。

    Curatr: A Platform for Semantic Analysis and Curation of Historical Literary Texts. (arXiv:2306.08020v1 [cs.CL])

    [http://arxiv.org/abs/2306.08020](http://arxiv.org/abs/2306.08020)

    Curatr是一个在线平台，可以基于机器学习支持的语义搜索来进行历史文学文本的筛选，提供主题词典的生成，帮助研究人员快速从大量的数字化文本中挑选出相关的子语料库。

    

    数字化历史和现代文学文本的持续增加提供了人文学科新研究的丰富可能性，然而这些集合的规模和多样性，也面临着特殊的挑战。本文介绍了Curatr，一个基于机器学习支持的语义搜索在线平台，旨在为数字人文学科学者提供文学文本的探索和筛选。该平台提供了一个文本挖掘工作流，通过将神经词嵌入与专业领域知识相结合，实现了主题词典的生成，可以让研究人员从大量的18世纪和19世纪数字化文本中筛选相关子语料库。

    The increasing availability of digital collections of historical and contemporary literature presents a wealth of possibilities for new research in the humanities. The scale and diversity of such collections however, presents particular challenges in identifying and extracting relevant content. This paper presents Curatr, an online platform for the exploration and curation of literature with machine learning-supported semantic search, designed within the context of digital humanities scholarship. The platform provides a text mining workflow that combines neural word embeddings with expert domain knowledge to enable the generation of thematic lexicons, allowing researches to curate relevant sub-corpora from a large corpus of 18th and 19th century digitised texts.
    
[^32]: Mol-Instructions: 一个大规模生物分子指令数据集，为大语言模型提供支持

    Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.08018](http://arxiv.org/abs/2306.08018)

    Mol-Instructions是一个专门为生物分子领域设计的综合指令数据集，可以显著提高大语言模型在生物领域中的适应能力和认知敏锐度。

    

    大语言模型（LLM）以其卓越的任务处理能力和创新的输出，在许多领域推动了重大进展。然而，它们在生物分子研究等专业领域的熟练应用还受到限制。为了解决这个挑战，我们介绍了Mol-Instructions，这是一个经过精心策划、专门针对生物分子领域设计的综合指令数据集。Mol-Instructions由三个关键组成部分组成：分子导向指令、蛋白质导向指令和生物分子文本指令，每个部分都被策划用于增强LLM对生物分子特性和行为的理解和预测能力。通过对代表性LLM的广泛指令调整实验，我们强调了Mol-Instructions在增强大模型在生物分子研究复杂领域内的适应能力和认知敏锐度方面的潜力，从而促进生物分子领域的进一步发展。

    Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
    
[^33]: 实现合成主动推理代理，第一部分：认识目标和图形说明语言

    Realising Synthetic Active Inference Agents, Part I: Epistemic Objectives and Graphical Specification Language. (arXiv:2306.08014v1 [cs.AI])

    [http://arxiv.org/abs/2306.08014](http://arxiv.org/abs/2306.08014)

    本文介绍了自由能原理和主动推理的理论框架，并推导了适用于任意图形模型的主动推理版本。同时引入了一种新的图形说明语言（GSL）来明确规定系统目标。

    

    自由能原理（FEP）是一种描述系统如何通过最小化自由能泛函而自组织成具有连贯性、稳定结构（智能）的理论框架。主动推理（AIF）是FEP的一个推论，它明确了能够为未来进行规划（代理）的系统是如何通过最小化包含信息寻求组件的特定自由能泛函来运作的。本文是一个系列中的第一篇，我们在自由形式因子图上推导了AIF的合成版本。本文重点推导了AIF所使用的自由能泛函的局部版本。这使我们能够构造一个适用于任意图形模型并与有关消息传递算法的先前工作接口的AIF版本。结果消息是在我们的伴侣论文中得出的。我们还发现因子图形式中存在一个缺口。虽然因子图表达了生成模型，但在指定系统目标方面缺乏一个图形化语言。我们引入了一个因子图描述法的新扩展，称为图形说明语言（GSL），它使系统目标得到明确规定。

    The Free Energy Principle (FEP) is a theoretical framework for describing how (intelligent) systems self-organise into coherent, stable structures by minimising a free energy functional. Active Inference (AIF) is a corollary of the FEP that specifically details how systems that are able to plan for the future (agents) function by minimising particular free energy functionals that incorporate information seeking components. This paper is the first in a series of two where we derive a synthetic version of AIF on free form factor graphs. The present paper focuses on deriving a local version of the free energy functionals used for AIF. This enables us to construct a version of AIF which applies to arbitrary graphical models and interfaces with prior work on message passing algorithms. The resulting messages are derived in our companion paper. We also identify a gap in the graphical notation used for factor graphs. While factor graphs are great at expressing a generative model, they have so
    
[^34]: TopP\&R: 具有鲁棒性的支持估计方法，用于评估生成模型中的保真度和多样性

    TopP\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])

    [http://arxiv.org/abs/2306.08013](http://arxiv.org/abs/2306.08013)

    本文提出了一种鲁棒可靠的生成模型评估指标TopP\&R，通过引入拓扑和统计处理进行严格的支持估计。TopP\&R仅保留具有一定置信水平的具有拓扑和统计上重要性的特征，对于噪声特征具有强大的鲁棒性，并提供了统计一致性。

    

    本文提出了一种鲁棒可靠的生成模型评估指标，通过引入拓扑和统计处理进行严格的支持估计。现有的度量标准，如Inception Score（IS），Fr\'echet Inception Distance（FID）以及Precision and Recall（P\&R）的变体，严重依赖于从样本特征估计的支持。然而，尽管评估的质量完全取决于其可靠性，但其估计的可靠性并没有得到严肃的讨论（并被忽视）。本文提出了拓扑精度和召回率（TopP\&R，发音为“topper”），它提供了一种系统的方法来估计支持，仅保留具有一定置信水平的具有拓扑和统计上重要性的特征。这不仅使TopP\&R对于噪声特征具有强大的鲁棒性，而且还提供了统计一致性。我们的理论和实验结果表明，TopP\&R对于离群值和非独立同分布具有鲁棒性。

    We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&R is robust to outliers and non-independent and identically distributed
    
[^35]: 在非独立同分布场景下基于隐蔽后门的联邦学习隐私推断攻击

    Privacy Inference-Empowered Stealthy Backdoor Attack on Federated Learning under Non-IID Scenarios. (arXiv:2306.08011v1 [cs.LG])

    [http://arxiv.org/abs/2306.08011](http://arxiv.org/abs/2306.08011)

    本文提出了一个针对非独立同分布场景下联邦学习的隐蔽后门攻击方案，其中通过生成对抗网络提供补充数据集，有效应对数据异构问题和隐私推断攻击风险。

    

    联邦学习（FL）在现实情况下面临数据异构问题，但是这个问题往往被研究FL安全和隐私的研究所忽视。本文针对非独立同分布场景下的FL，提出了一个新型的私密推理加强的隐蔽后门攻击（PI-SBA）方案。该方案包括通过生成对抗网络（GAN）提供补充数据集的机制和基于源特定的后门学习策略。此外，该方案还能有效应对恶意客户通过隐私推断攻击窃取私有数据的风险。

    Federated learning (FL) naturally faces the problem of data heterogeneity in real-world scenarios, but this is often overlooked by studies on FL security and privacy. On the one hand, the effectiveness of backdoor attacks on FL may drop significantly under non-IID scenarios. On the other hand, malicious clients may steal private data through privacy inference attacks. Therefore, it is necessary to have a comprehensive perspective of data heterogeneity, backdoor, and privacy inference. In this paper, we propose a novel privacy inference-empowered stealthy backdoor attack (PI-SBA) scheme for FL under non-IID scenarios. Firstly, a diverse data reconstruction mechanism based on generative adversarial networks (GANs) is proposed to produce a supplementary dataset, which can improve the attacker's local data distribution and support more sophisticated strategies for backdoor attacks. Based on this, we design a source-specified backdoor learning (SSBL) strategy as a demonstration, allowing th
    
[^36]: 掌控推理阶段的领域信息控制，用于声景分类

    Domain Information Control at Inference Time for Acoustic Scene Classification. (arXiv:2306.08010v1 [cs.SD])

    [http://arxiv.org/abs/2306.08010](http://arxiv.org/abs/2306.08010)

    本文将自然语言处理中的可控门控适配器ConGater应用于声景分类任务，较现有领域泛化方法更好地提高模型在新领域中的性能。

    

    领域转移被认为是机器学习中的一个挑战，因为它会导致模型性能的显著降低。在声景分类任务中，领域转移主要由不同的录音设备引起。已经有一些研究针对领域泛化进行改进，以提高在新领域中（如新设备）ASC模型的性能。最近，在自然语言处理中提出了可控门控适配器ConGater，以解决偏见训练数据的问题。ConGater允许在推理时控制去偏的过程。ConGater的主要优点是在推理过程中对训练模型进行连续和选择性去偏。在本文中，我们将ConGater适应于音频频谱变换器，用于声景分类任务。我们展示了ConGater可以用于有选择性地使学习表示与记录设备等领域转移不变。我们的分析表明，ConGater可以逐步提高ASC模型在看不见的领域上的性能，超过现有的领域泛化方法。

    Domain shift is considered a challenge in machine learning as it causes significant degradation of model performance. In the Acoustic Scene Classification task (ASC), domain shift is mainly caused by different recording devices. Several studies have already targeted domain generalization to improve the performance of ASC models on unseen domains, such as new devices. Recently, the Controllable Gate Adapter ConGater has been proposed in Natural Language Processing to address the biased training data problem. ConGater allows controlling the debiasing process at inference time. ConGater's main advantage is the continuous and selective debiasing of a trained model, during inference. In this work, we adapt ConGater to the audio spectrogram transformer for an acoustic scene classification task. We show that ConGater can be used to selectively adapt the learned representations to be invariant to device domain shifts such as recording devices. Our analysis shows that ConGater can progressively
    
[^37]: DHBE: 通过受限对抗蒸馏的数据无关全面后门擦除来保护深度神经网络

    DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation. (arXiv:2306.08009v1 [cs.LG])

    [http://arxiv.org/abs/2306.08009](http://arxiv.org/abs/2306.08009)

    本文提出了一种数据无关的全面后门擦除（DHBE）框架，该框架将后门擦除任务视为一个统一的对抗过程，在转移干净数据的知识的同时保证擦除后门。通过对抗蒸馏和后门正则化两个不同的竞争过程，DHBE实现了一种统一的、无需数据，并且有效的后门擦除框架。

    

    后门攻击已成为深度神经网络（DNN）的紧急威胁。为防御后门攻击，许多工作都建立了一个分阶段的流程来从受害DNN中移除后门：检查、定位和擦除。然而，在只有少量干净数据可用的情况下，这种流程是脆弱的，而且不能在不牺牲模型准确性的情况下完全擦除后门。因此，本文提出了一种全新的数据无关全面后门擦除（DHBE）框架。该框架将后门擦除任务视为一个统一的对抗过程，在转移干净数据的知识的同时保证擦除后门。通过对抗蒸馏和后门正则化两个不同的竞争过程，DHBE实现了一种统一的、无需数据，并且有效的后门擦除框架。大量的实验证明，DHBE可以高成功率地擦除后门，同时保持模型准确性，胜过现有技术。

    Backdoor attacks have emerged as an urgent threat to Deep Neural Networks (DNNs), where victim DNNs are furtively implanted with malicious neurons that could be triggered by the adversary. To defend against backdoor attacks, many works establish a staged pipeline to remove backdoors from victim DNNs: inspecting, locating, and erasing. However, in a scenario where a few clean data can be accessible, such pipeline is fragile and cannot erase backdoors completely without sacrificing model accuracy. To address this issue, in this paper, we propose a novel data-free holistic backdoor erasing (DHBE) framework. Instead of the staged pipeline, the DHBE treats the backdoor erasing task as a unified adversarial procedure, which seeks equilibrium between two different competing processes: distillation and backdoor regularization. In distillation, the backdoored DNN is distilled into a proxy model, transferring its knowledge about clean data, yet backdoors are simultaneously transferred. In backdo
    
[^38]: 检测与分类旨在预防光伏系统故障的论文。

    Detection and classification of faults aimed at preventive maintenance of PV systems. (arXiv:2306.08004v1 [cs.LG])

    [http://arxiv.org/abs/2306.08004](http://arxiv.org/abs/2306.08004)

    本论文提出了一种随机森林算法的创新方法，用于检测细微的光伏故障并进行分类，提高了故障分类的计算时间，同时保持了高精度。

    

    光伏系统的诊断旨在检测、定位和识别故障。诊断这些故障对于保证能源生产和延长光伏发电厂的使用寿命至关重要。在文献中，已经为此提出了多种机器学习方法。然而，这些研究中很少有关注细微故障的检测和专门的特征提取和分类选择过程。细微故障是一种其特征签名与健康面板的特征签名难以区别的故障。作为检测细微故障的贡献，本研究提出了一种基于随机森林（RF）算法的创新方法。这种方法使用复杂的特征提取和选择方法，提高了故障分类的计算时间，同时保持了高精度。

    Diagnosis in PV systems aims to detect, locate and identify faults. Diagnosing these faults is vital to guarantee energy production and extend the useful life of PV power plants. In the literature, multiple machine learning approaches have been proposed for this purpose. However, few of these works have paid special attention to the detection of fine faults and the specialized process of extraction and selection of features for their classification. A fine fault is one whose characteristic signature is difficult to distinguish to that of a healthy panel. As a contribution to the detection of fine faults (especially of the snail trail type), this article proposes an innovative approach based on the Random Forest (RF) algorithm. This approach uses a complex feature extraction and selection method that improves the computational time of fault classification while maintaining high accuracy.
    
[^39]: DTW k-means聚类用于光伏模块故障检测

    DTW k-means clustering for fault detection in photovoltaic modules. (arXiv:2306.08003v1 [cs.LG])

    [http://arxiv.org/abs/2306.08003](http://arxiv.org/abs/2306.08003)

    本文提出了一种无监督聚类技术的方法，用于在大规模光伏电厂中检测和识别故障类型，采用动态时间规整（DTW）距离度量，能够提高聚类性能。

    

    光伏能源在全球的应用增加表明，光伏电厂的寿命和维护直接依赖于快速检测光伏电厂的严重故障的能力。为了解决这个检测问题，文献中提出了基于数据的方法。然而，这些之前的解决方案只考虑了一个或少数几个故障的具体行为。其中大多数方法可以被归为监督学习，需要大量标记努力（每种技术中明确识别的故障类型）。此外，大多数方法在PV电池或一个PV模块中验证。这在考虑它们的复杂性的大规模PV电厂中很难应用。相反，一些基于数据的无监督知名方法尝试检测异常，但不能精确地识别故障类型。其中表现最好的方法成功地将健康面板有效地分组并将其与故障面板分开。因此，本文提出了一个无监督聚类技术的方法，用于大规模PV电厂的一般故障检测。具体而言，对PV电厂中每个面板的时间序列数据集应用了k-means聚类算法。为了提高聚类性能，采用了一种广泛使用的相似性度量，即动态时间规整（DTW）距离，以克服PV面板对不同环境和操作条件的响应的变异性。实验结果表明，所提出的方法可以成功检测PV电厂中的故障并识别其类型。

    The increase in the use of photovoltaic (PV) energy in the world has shown that the useful life and maintenance of a PV plant directly depend on theability to quickly detect severe faults on a PV plant. To solve this problem of detection, data based approaches have been proposed in the literature.However, these previous solutions consider only specific behavior of one or few faults. Most of these approaches can be qualified as supervised, requiring an enormous labelling effort (fault types clearly identified in each technology). In addition, most of them are validated in PV cells or one PV module. That is hardly applicable in large-scale PV plants considering their complexity. Alternatively, some unsupervised well-known approaches based on data try to detect anomalies but are not able to identify precisely the type of fault. The most performant of these methods do manage to efficiently group healthy panels and separate them from faulty panels. In that way, this article presents an unsu
    
[^40]: 主动查询的马尔可夫形式主义

    A Markovian Formalism for Active Querying. (arXiv:2306.08001v1 [cs.LG])

    [http://arxiv.org/abs/2306.08001](http://arxiv.org/abs/2306.08001)

    本文提出了一个基于马尔可夫系统的形式化主义对主动学习进行概述，将主动学习过程作为一个部分可观察的马尔可夫系统的整体处理，并且阐述如何将查询、数据集增强、奖励更新等过程视为马尔可夫系统中元状态之间的转移。

    

    主动学习算法是人工智能领域中最近进展的重要部分。然而，该领域的研究变化很大，缺乏整体性的组织结构。我们提出了一个基于马尔可夫系统的形式化主义，用于对主动学习领域进行概述，并通过文献综述来展示我们所提出的形式化主义的组织能力。我们的形式化主义将主动学习过程作为一个部分可观察的马尔可夫系统的整体来处理。我们具体概述了查询、数据集增强、奖励更新以及其他主动学习方面如何视为马尔可夫系统中元状态之间的转移，并指导其他主动学习方面如何适应我们的形式化主义。

    Active learning algorithms have been an integral part of recent advances in artificial intelligence. However, the research in the field is widely varying and lacks an overall organizing leans. We outline a Markovian formalism for the field of active learning and survey the literature to demonstrate the organizing capability of our proposed formalism. Our formalism takes a partially observable Markovian system approach to the active learning process as a whole. We specifically outline how querying, dataset augmentation, reward updates, and other aspects of active learning can be viewed as a transition between meta-states in a Markovian system, and give direction into how other aspects of active learning can fit into our formalism.
    
[^41]: 数字病理学中人工智能的诊断测试准确度：系统综述、Meta分析和质量评估

    Diagnostic test accuracy (DTA) of artificial intelligence in digital pathology: a systematic review, meta-analysis and quality assessment. (arXiv:2306.07999v1 [physics.med-ph])

    [http://arxiv.org/abs/2306.07999](http://arxiv.org/abs/2306.07999)

    本文进行了数字病理图像中应用人工智能的所有病理学领域的诊断准确度的系统综述和Meta分析。结果表明，人工智能在数字病理学中取得了高度的准确度，是可行的辅助诊断工具。

    

    确保临床使用之前AI模型的诊断表现是关键，以确保这些技术的安全和成功的采用。近年来，报道应用于数字病理学图像进行诊断目的的AI研究数量迅速增加。本研究旨在提供数字病理学中AI的诊断准确度的概述，涵盖了所有病理学领域。这项系统性综述和Meta分析包括使用任何类型的人工智能应用于任何疾病类型的WSI图像的诊断准确性研究。参考标准是通过组织病理学评估和/或免疫组化诊断。搜索在2022年6月在PubMed、EMBASE和CENTRAL中进行。在2976项研究中，有100项纳入综述，48项纳入完整的Meta分析。使用QUADAS-2工具评估了偏倚风险和适用性的关注点。数据提取由两个调查员进行，并进行了Meta分析。

    Ensuring diagnostic performance of AI models before clinical use is key to the safe and successful adoption of these technologies. Studies reporting AI applied to digital pathology images for diagnostic purposes have rapidly increased in number in recent years. The aim of this work is to provide an overview of the diagnostic accuracy of AI in digital pathology images from all areas of pathology. This systematic review and meta-analysis included diagnostic accuracy studies using any type of artificial intelligence applied to whole slide images (WSIs) in any disease type. The reference standard was diagnosis through histopathological assessment and / or immunohistochemistry. Searches were conducted in PubMed, EMBASE and CENTRAL in June 2022. We identified 2976 studies, of which 100 were included in the review and 48 in the full meta-analysis. Risk of bias and concerns of applicability were assessed using the QUADAS-2 tool. Data extraction was conducted by two investigators and meta-analy
    
[^42]: 对比注意力网络用于早期印刷品的特征鉴定

    Contrastive Attention Networks for Attribution of Early Modern Print. (arXiv:2306.07998v1 [cs.CV])

    [http://arxiv.org/abs/2306.07998](http://arxiv.org/abs/2306.07998)

    本文提出了一种对比注意力度量学习方法，用于鉴定早期印刷品中的未知印刷商。我们的方法对字形微小差异敏感，对各种混淆噪声具有鲁棒性，并设计了一种随机数据合成过程，成功提高了该时期印刷品中的损坏类型印记匹配结果。

    

    本文利用机器学习技术识别早期（大约1500-1800年）英国印刷书籍中的未知印刷商。特别地，我们专注于匹配匿名印刷书籍中的唯一损坏字符类型印记与已知印刷商作品，以提供其来源的证据。我们提出了一种基于对比注意力度量学习的方法，以识别同一字符图像对中的相似损伤，对字形的微小差异敏感，但对与数字化历史书籍相关的各种混淆噪声具有鲁棒性。为了克服受监督数据匮乏的问题，我们设计了一种随机数据合成过程，旨在模拟早期印刷过程引起的弯曲、断裂和油墨变化。我们的方法成功提高了该时期印刷品中的损坏类型印记匹配结果。

    In this paper, we develop machine learning techniques to identify unknown printers in early modern (c.~1500--1800) English printed books. Specifically, we focus on matching uniquely damaged character type-imprints in anonymously printed books to works with known printers in order to provide evidence of their origins. Until now, this work has been limited to manual investigations by analytical bibliographers. We present a Contrastive Attention-based Metric Learning approach to identify similar damage across character image pairs, which is sensitive to very subtle differences in glyph shapes, yet robust to various confounding sources of noise associated with digitized historical books. To overcome the scarce amount of supervised data, we design a random data synthesis procedure that aims to simulate bends, fractures, and inking variations induced by the early printing process. Our method successfully improves downstream damaged type-imprint matching among printed works from this period, 
    
[^43]: 基于语义的神经网络修复

    Semantic-Based Neural Network Repair. (arXiv:2306.07995v1 [cs.LG])

    [http://arxiv.org/abs/2306.07995](http://arxiv.org/abs/2306.07995)

    本文提出了一种自动修复神经网络错误的方法，基于深度学习层的可执行语义，并专注于实践中常见的四种错误。

    

    近年来，神经网络已经广泛应用于多个领域，其中包括许多安全关键系统。神经网络是通过在 TensorFlow 和 PyTorch 等框架中进行编程构建（和训练）的。开发人员可以应用丰富的预定义层手动编写神经网络或通过 AutoML 自动生成网络。由于必须满足使用这些层的非平凡约束条件，所以使用不同层来组合神经网络容易出现错误。在本文中，我们提出了一种自动修复错误神经网络的方法。挑战在于识别出对网络进行最小修改以使其变为有效的修改。修改一层可能会对随后的层产生级联效应，因此我们的方法必须递归地搜索以识别“全局”最小修改。我们的方法基于深度学习层的可执行语义，并专注于实践中常见的四种错误。我们对我们的方法进行了验证。

    Recently, neural networks have spread into numerous fields including many safety-critical systems. Neural networks are built (and trained) by programming in frameworks such as TensorFlow and PyTorch. Developers apply a rich set of pre-defined layers to manually program neural networks or to automatically generate them (e.g., through AutoML). Composing neural networks with different layers is error-prone due to the non-trivial constraints that must be satisfied in order to use those layers. In this work, we propose an approach to automatically repair erroneous neural networks. The challenge is in identifying a minimal modification to the network so that it becomes valid. Modifying a layer might have cascading effects on subsequent layers and thus our approach must search recursively to identify a "globally" minimal modification. Our approach is based on an executable semantics of deep learning layers and focuses on four kinds of errors which are common in practice. We evaluate our appro
    
[^44]: MSSRNet: 无监督文本风格转换中的顺序风格表示操作

    MSSRNet: Manipulating Sequential Style Representation for Unsupervised Text Style Transfer. (arXiv:2306.07994v1 [cs.CL])

    [http://arxiv.org/abs/2306.07994](http://arxiv.org/abs/2306.07994)

    本文提出了一种新方法，通过为每个词汇分配单独的风格向量来对文本进行风格转换，并引入基于教师-学生学习的对抗性培训框架，以提高培训稳定性，实现了双风格转换和多风格转换两种情况下的明显提高的风格转换准确性和内容保留。

    

    无监督文本风格转移任务旨在将文本重写为目标风格，同时保留其主要内容。传统方法依赖于使用固定大小的向量来调节文本风格，这很难准确传达每个单独令牌的风格强度。事实上，文本的每个令牌都包含不同的风格强度，并对整体风格产生不同的贡献。我们提出的方法通过为文本中的每个令牌分配单独的风格向量来解决这个问题，允许对风格强度进行细粒度的控制和操作。此外，我们引入了一个基于教师-学生学习的对抗性培训框架，以增强培训稳定性并减轻高维优化的复杂性。我们实验的结果证明了我们的方法在双风格转换和多风格转换设置中，具有明显提高的风格转换准确性和内容保留的效果。

    Unsupervised text style transfer task aims to rewrite a text into target style while preserving its main content. Traditional methods rely on the use of a fixed-sized vector to regulate text style, which is difficult to accurately convey the style strength for each individual token. In fact, each token of a text contains different style intensity and makes different contribution to the overall style. Our proposed method addresses this issue by assigning individual style vector to each token in a text, allowing for fine-grained control and manipulation of the style strength. Additionally, an adversarial training framework integrated with teacher-student learning is introduced to enhance training stability and reduce the complexity of high-dimensional optimization. The results of our experiments demonstrate the efficacy of our method in terms of clearly improved style transfer accuracy and content preservation in both two-style transfer and multi-style transfer settings.
    
[^45]: 安全的视觉感知推荐系统：一种对抗图像重构及检测框架

    Securing Visually-Aware Recommender Systems: An Adversarial Image Reconstruction and Detection Framework. (arXiv:2306.07992v1 [cs.CV])

    [http://arxiv.org/abs/2306.07992](http://arxiv.org/abs/2306.07992)

    本文提出了一种对抗图像重构及检测框架来保护视觉感知推荐系统，能够防御局部扰动为特征的对抗攻击并且能够在干净和对抗性图像上进行训练来检测对抗性图像。

    

    随着富含图片等视觉数据与物品关联度增加，视觉感知推荐系统（VARS）已被广泛应用于不同应用领域。最近的研究表明，VARS易受到物品-图像对抗攻击的攻击，这些攻击向与这些物品关联的干净图像添加人类无法感知的扰动。对VARS的攻击为广泛使用VARS的许多应用（如电子商务和社交网络）带来新的安全挑战。如何保护VARS免受此类对抗攻击成为一个关键的问题。目前，尚缺乏系统地研究如何设计针对VARS视觉攻击的安全防御策略。本文提出了一种对抗图像重构及检测框架来保护VARS，我们的方法可以同时(1)通过基于全局视觉传输的图像重构来防御以局部扰动为特征的对抗攻击，(2)使用在少量干净和对抗性图像上训练的检测模型来检测对抗性图像。实验结果表明，我们的框架能够有效地防御各种物品-图像对抗攻击对VARS的影响。

    With rich visual data, such as images, becoming readily associated with items, visually-aware recommendation systems (VARS) have been widely used in different applications. Recent studies have shown that VARS are vulnerable to item-image adversarial attacks, which add human-imperceptible perturbations to the clean images associated with those items. Attacks on VARS pose new security challenges to a wide range of applications such as e-Commerce and social networks where VARS are widely used. How to secure VARS from such adversarial attacks becomes a critical problem. Currently, there is still a lack of systematic study on how to design secure defense strategies against visual attacks on VARS. In this paper, we attempt to fill this gap by proposing an adversarial image reconstruction and detection framework to secure VARS. Our proposed method can simultaneously (1) secure VARS from adversarial attacks characterized by local perturbations by image reconstruction based on global vision tra
    
[^46]: 智能警报生成的混合方法

    A Hybrid Approach for Smart Alert Generation. (arXiv:2306.07983v1 [cs.NI])

    [http://arxiv.org/abs/2306.07983](http://arxiv.org/abs/2306.07983)

    本文提出了一个混合模型的警报系统，在解决可扩展性、数据异构性、可泛化性和可维护性方面表现良好。白名单机制有效减少了误报警报，未来的工作将包括更多的特征工程和输入数据，以及包括人类反馈在内的模型开发过程。

    

    异常检测是网络管理中的重要任务，但当考虑可扩展性、数据异构性以及可泛化性和可维护性时，在现实世界的大规模网络系统中部署智能警报系统是具有挑战性的。本文提出了一种混合模型的警报系统，将统计模型与白名单机制结合起来，以应对这些挑战并减少误报警报。 统计模型利用大型数据库检测时间序列数据中的异常，而白名单则过滤出持续警报的节点以减少误报。我们使用客户支持案例的定性数据验证了我们的模型。未来的工作包括更多的特征工程和输入数据，以及在模型开发过程中包括人类反馈。

    Anomaly detection is an important task in network management. However, deploying intelligent alert systems in real-world large-scale networking systems is challenging when we take into account (i) scalability, (ii) data heterogeneity, and (iii) generalizability and maintainability. In this paper, we propose a hybrid model for an alert system that combines statistical models with a whitelist mechanism to tackle these challenges and reduce false positive alerts. The statistical models take advantage of a large database to detect anomalies in time-series data, while the whitelist filters out persistently alerted nodes to further reduce false positives. Our model is validated using qualitative data from customer support cases. Future work includes more feature engineering and input data, as well as including human feedback in the model development process.
    
[^47]: 面向高级持久性威胁的上下文感知防御的少样本多领域知识重装

    Few-shot Multi-domain Knowledge Rearming for Context-aware Defence against Advanced Persistent Threats. (arXiv:2306.07685v1 [cs.CR])

    [http://arxiv.org/abs/2306.07685](http://arxiv.org/abs/2306.07685)

    本文提出了一种针对高级持久性威胁的上下文感知防御的少样本、多领域知识重装 (FMKR) 方案，可以在不同的网络域生成多个小任务，完成多领域的知识重新装备，提高防御性能。

    

    高级持久性威胁(APTs)具有多阶段渗透、高度定制化意图和规避策略等新颖特征。APTs的防御需要融合多维网络威胁情报数据来识别攻击意图，并通过基于数据驱动的机器学习进行有效的知识发现策略以识别实体关系。然而，数据驱动的机器学习缺乏对新的或未知样本的泛化能力，降低了防御模型的准确性和实用性。此外，将这些APTs防御模型私有部署在异构环境中和各种网络设备上，需要在上下文感知方面进行重大投入（例如已知攻击实体、连续的网络状态和当前的安全策略）。本文提出了一种面向高级持久性威胁的上下文感知防御的少样本多领域知识重装(FMKR)方案。通过在不同网络域生成多个小任务，完成多领域知识重新装备，从而提升防御性能。

    Advanced persistent threats (APTs) have novel features such as multi-stage penetration, highly-tailored intention, and evasive tactics. APTs defense requires fusing multi-dimensional Cyber threat intelligence data to identify attack intentions and conducts efficient knowledge discovery strategies by data-driven machine learning to recognize entity relationships. However, data-driven machine learning lacks generalization ability on fresh or unknown samples, reducing the accuracy and practicality of the defense model. Besides, the private deployment of these APT defense models on heterogeneous environments and various network devices requires significant investment in context awareness (such as known attack entities, continuous network states, and current security strategies). In this paper, we propose a few-shot multi-domain knowledge rearming (FMKR) scheme for context-aware defense against APTs. By completing multiple small tasks that are generated from different network domains with m
    
[^48]: 基于双曲图扩散模型的分子生成

    Hyperbolic Graph Diffusion Model for Molecule Generation. (arXiv:2306.07618v1 [cs.LG])

    [http://arxiv.org/abs/2306.07618](http://arxiv.org/abs/2306.07618)

    本文提出了基于双曲图扩散模型的分子生成方法，可以更全面地捕捉分子的内部非欧几里德结构，实现数据生成，并提取复杂几何特征的能力。

    

    最近，扩散模型在数据生成方面取得了显著的成果，例如生成高质量的图像。然而，化学分子通常具有复杂的非欧几里德空间结构，其行为动态变化且难以预测。大多数现有的扩散模型高度依赖于计算欧几里德空间中的概率分布，即高斯分布，不能捕捉分子的内部非欧几里德结构，特别是分子所表示的隐式流形表面的分层结构。观察到，双曲嵌入空间中的复杂分层结构变得更加明显且更容易被捕捉。为了充分利用扩散模型的数据生成能力和提取复杂几何特征的双曲嵌入的强大能力，我们提出将扩散模型扩展到双曲流形上进行分子生成，即基于双曲图扩散模型的分子生成。

    Recently, diffusion models have achieved remarkable performance in data generation, e.g., generating high-quality images. Nevertheless, chemistry molecules often have complex non-Euclidean spatial structures, with the behavior changing dynamically and unpredictably. Most existing diffusion models highly rely on computing the probability distribution, i.e., Gaussian distribution, in Euclidean space, which cannot capture internal non-Euclidean structures of molecules, especially the hierarchical structures of the implicit manifold surface represented by molecules. It has been observed that the complex hierarchical structures in hyperbolic embedding space become more prominent and easier to be captured. In order to leverage both the data generation power of diffusion models and the strong capability to extract complex geometric features of hyperbolic embedding, we propose to extend the diffusion model to hyperbolic manifolds for molecule generation, namely, Hyperbolic Graph Diffusion Mode
    
[^49]: DeepTransition：可行性导致步态转换的出现

    DeepTransition: Viability Leads to the Emergence of Gait Transitions in Learning Anticipatory Quadrupedal Locomotion Skills. (arXiv:2306.07419v1 [cs.RO])

    [http://arxiv.org/abs/2306.07419](http://arxiv.org/abs/2306.07419)

    本文通过深度强化学习和机器人工具的互动研究，证明了可行性是四足动物步态转换的重要标准。其中，步-小跑步态转换能够在平坦地形上同时提高可行性和节能效果。

    

    四足动物在改变运动速度时能够无缝地转换步态。本文提出可行性（即避免跌倒）代表步态转换的一个重要标准。通过利用深度强化学习和机器人工具，我们研究了步态转换的出现。一致于四足动物数据，我们证明了在平坦地形上，四足机器人的步-小跑步态转换能同时提高可行性和节能效果。此外，我们研究了离散地形（即穿越连续间隔）对强制步态转换的影响，并找到足-蹦步态的出现。

    Quadruped animals seamlessly transition between gaits as they change locomotion speeds. While the most widely accepted explanation for gait transitions is energy efficiency, there is no clear consensus on the determining factor, nor on the potential effects from terrain properties. In this article, we propose that viability, i.e. the avoidance of falls, represents an important criterion for gait transitions. We investigate the emergence of gait transitions through the interaction between supraspinal drive (brain), the central pattern generator in the spinal cord, the body, and exteroceptive sensing by leveraging deep reinforcement learning and robotics tools. Consistent with quadruped animal data, we show that the walk-trot gait transition for quadruped robots on flat terrain improves both viability and energy efficiency. Furthermore, we investigate the effects of discrete terrain (i.e. crossing successive gaps) on imposing gait transitions, and find the emergence of trot-pronk transit
    
[^50]: Strokes2Surface：从四维建筑设计素描中恢复曲线网络

    Strokes2Surface: Recovering Curve Networks From 4D Architectural Design Sketches. (arXiv:2306.07220v2 [cs.GR] UPDATED)

    [http://arxiv.org/abs/2306.07220](http://arxiv.org/abs/2306.07220)

    本文介绍了Strokes2Surface，它可从建筑师的笔画中恢复出曲线网络，对于建筑设计中的概念设计和数字建模之间的桥梁具有重要意义。

    

    本文介绍了一个离线几何重建管道Strokes2Surface，它是基于4D Sketching Interface，MR.Sketch的目标是面向建筑设计的。该管道从设计师绘制的笔画中恢复曲线网络，因此在建筑设计的概念设计和数字建模阶段之间建立了桥梁。我们的管道的输入包括3D笔画的折线顶点及其相应的时间戳（作为第四个维度），以及额外的几何和笔触相关的记录属性。基于素描合并和基于素描建模方法的启发，我们的管道利用这些数据并组合三个机器学习（ML）模型；一个分类器和两个聚类模型。特别是，根据建筑设计素描中设计师通常采用的实践观察，我们解决了一个二元分类问题，以识别一笔画是描绘边界和边缘还是用于填充所需建筑物的封闭区域和表面。

    We present Strokes2Surface, an offline geometry-reconstruction pipeline built upon a 4D Sketching Interface, MR.Sketch, targeted at architectural design. The pipeline recovers a curve network from designer-drawn strokes, thus bridging between concept design and digital modeling stages in architectural design. The input to our pipeline consists of 3D strokes' polyline vertices and their corresponding timestamps (as of the fourth dimension), along with additional geometric and stylus-related recorded properties. Inspired by sketch consolidation and sketch-based modeling methods, our pipeline leverages such data and combines three Machine Learning (ML) models; a classifier and two clustering models. In particular, based on observations of practices designers typically employ in architectural design sketches, we solve a binary classification problem to recognize whether a stroke depicts a boundary and edge or is used to fill in the enclosing areas and faces of the intended architectural ob
    
[^51]: TASRA: 一份关于人工智能可能带来的社会规模风险的分类与分析

    TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI. (arXiv:2306.06924v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.06924](http://arxiv.org/abs/2306.06924)

    本文探讨了人工智能可能带来的社会规模风险的分类和分析，其中提出了一种基于问责制的分类法。针对可能出现的风险类型提供了实际的证明，并指出需要联合技术和政策解决方案。

    

    虽然最近一些作品已经确认了人工智能可能给人类带来的社会规模和灭绝级的风险，但几乎没有人尝试过进行全面归纳这些风险。许多全面性的分类法都是可能的，并且有些是有用的——尤其是如果它们揭示了新的风险或安全的实际方法。本文探讨了一种基于问责制的分类法：哪些行为导致了风险，行动者是否统一，他们是否是蓄意的。我们还提供了故事来说明各种风险类型如何发挥作用，包括来自许多人工智能系统意外相互作用的风险，以及来自蓄意滥用的风险，需要联合技术和政策解决方案。

    While several recent works have identified societal-scale and extinction-level risks to humanity arising from artificial intelligence, few have attempted an {\em exhaustive taxonomy} of such risks. Many exhaustive taxonomies are possible, and some are useful -- particularly if they reveal new risks or practical approaches to safety. This paper explores a taxonomy based on accountability: whose actions lead to the risk, are the actors unified, and are they deliberate? We also provide stories to illustrate how the various risk types could each play out, including risks arising from unanticipated interactions of many AI systems, as well as risks from deliberate misuse, for which combined technical and policy solutions are indicated.
    
[^52]: TrojPrompt：基于黑盒方式的预训练语言模型木马攻击

    TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (arXiv:2306.06815v1 [cs.CR] CROSS LISTED)

    [http://arxiv.org/abs/2306.06815](http://arxiv.org/abs/2306.06815)

    本文开创性地研究了基于 prompt 学习的预训练语言模型 API 的特洛伊易感性，并提出了一种自动黑盒框架——TrojPrompt，用于生成通用和隐蔽的触发器，并将特洛伊木马插入硬提示。

    

    Prompt学习被证明在提高预训练语言模型（PLM）适应性方面非常有效，超越了传统的微调范式，并在专为少样本学习场景量身定制的应用程序和API中展现了杰出的前景。但是，尽管prompt学习的API越来越受欢迎，但它们的安全问题仍未得到充分探索。本文在prompt学习的PLM API的特洛伊易感性方面进行了开创性研究。我们发现，离散提示，少样本和黑盒设置是几个关键挑战，限制了现有后门攻击的适用性。为了解决这些挑战，我们提出了TrojPrompt，这是一种自动的黑盒框架，可有效生成通用的和隐秘的触发器，并将特洛伊木马插入硬提示。具体而言，我们提出了一种API驱动的通用触发器发现算法，通过查询受害者PLM API，为各种输入生成通用触发器。

    Prompt learning has been proven to be highly effective in improving pre-trained language model (PLM) adaptability, surpassing conventional fine-tuning paradigms, and showing exceptional promise in an ever-growing landscape of applications and APIs tailored for few-shot learning scenarios. Despite the growing prominence of prompt learning-based APIs, their security concerns remain underexplored. In this paper, we undertake a pioneering study on the Trojan susceptibility of prompt-learning PLM APIs. We identified several key challenges, including discrete-prompt, few-shot, and black-box settings, which limit the applicability of existing backdoor attacks. To address these challenges, we propose TrojPrompt, an automatic and black-box framework to effectively generate universal and stealthy triggers and insert Trojans into hard prompts. Specifically, we propose a universal API-driven trigger discovery algorithm for generating universal triggers for various inputs by querying victim PLM API
    
[^53]: 通过代理分析提高 LLM 对机器人任务学习的知识提取

    Improving Knowledge Extraction from LLMs for Robotic Task Learning through Agent Analysis. (arXiv:2306.06770v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.06770](http://arxiv.org/abs/2306.06770)

    本文提出了一种认知代理方法，通过增加 LLMs 的响应空间，并部署通用策略，嵌入自主机器人内部，从而使机器人能够获取与其语言能力、体现能力、环境和用户偏好相匹配的新的任务知识，实现一次学习即可完成任务。

    

    大型语言模型（LLMs）被视为机器人任务学习的知识来源，但是单独的提示工程并不能为机器人获取与其语言能力、体现能力、环境和用户偏好相匹配的情境相关知识。本文提出了一种认知代理方法，扩展和补充提示工程，缓解其局限性，以此使机器人能够获取新的任务知识。该方法是通过增加 LLMs 的响应空间，并部署通用策略，嵌入自主机器人内部，对 LLMs 产生的候选响应进行评估、修复和选择，实现一次学习即可完成任务。通过实验表明，机器人从 LLM 中检索和评估一系列不同响应后可以达到>75% 的任务完成率，无需用户监督。

    Large language models (LLMs) offer significant promise as a knowledge source for robotic task learning. Prompt engineering has been shown to be effective for eliciting knowledge from an LLM but alone is insufficient for acquiring relevant, situationally grounded knowledge for an embodied robotic agent learning novel tasks. We describe a cognitive-agent approach that extends and complements prompt engineering, mitigating its limitations, and thus enabling a robot to acquire new task knowledge matched to its native language capabilities, embodiment, environment, and user preferences. The approach is to increase the response space of LLMs and deploy general strategies, embedded within the autonomous robot, to evaluate, repair, and select among candidate responses produced by the LLM. We describe the approach and experiments that show how a robot, by retrieving and evaluating a breadth of responses from the LLM, can achieve >75% task completion in one-shot learning without user oversight. 
    
[^54]: NERFBK:针对基于NeRF的3D重建的高质量基准数据集

    NERFBK: A High-Quality Benchmark for NERF-Based 3D Reconstruction. (arXiv:2306.06300v1 [cs.CV])

    [http://arxiv.org/abs/2306.06300](http://arxiv.org/abs/2306.06300)

    该论文介绍了一个名为NeRFBK的新的真实和合成数据集，专门用于测试和比较基于NeRF的3D重建算法，旨在解决收集具有精确地面实况的多样化数据的挑战问题。该数据集有望推动3D重建领域的发展。

    

    本文介绍一个名为NeRFBK的新的真实和合成数据集，专门用于测试和比较基于NeRF的3D重建算法。高质量的3D重建在各个领域都有重要的潜力，基于图像的算法的改进使得评估新的先进技术变得必不可少。然而，收集具有精确地面实况的多样化数据是具有挑战性的且可能无法涵盖所有相关的应用。NeRFBK数据集通过提供多尺度、室内、室外数据集及高分辨率图像、视频和相机参数等来解决这个问题，以测试和比较基于NeRF的算法。本文介绍了NeRFBK基准的设计和创建、各种例子和应用场景，并强调了它在推动3D重建领域发展方面的潜力。

    This paper introduces a new real and synthetic dataset called NeRFBK specifically designed for testing and comparing NeRF-based 3D reconstruction algorithms. High-quality 3D reconstruction has significant potential in various fields, and advancements in image-based algorithms make it essential to evaluate new advanced techniques. However, gathering diverse data with precise ground truth is challenging and may not encompass all relevant applications. The NeRFBK dataset addresses this issue by providing multi-scale, indoor and outdoor datasets with high-resolution images and videos and camera parameters for testing and comparing NeRF-based algorithms. This paper presents the design and creation of the NeRFBK benchmark, various examples and application scenarios, and highlights its potential for advancing the field of 3D reconstruction.
    
[^55]: CARSO: 对抗性合成观测的反对抗性召回

    CARSO: Counter-Adversarial Recall of Synthetic Observations. (arXiv:2306.06081v1 [cs.CV])

    [http://arxiv.org/abs/2306.06081](http://arxiv.org/abs/2306.06081)

    本文提出了一种新的图像分类的对抗性防御机制CARSO，该方法可以比最先进的对抗性训练更好地保护分类器，通过利用生成模型进行对抗净化来进行最终分类，并成功地保护自己免受未预见的威胁和最终攻击。

    

    本文提出了一种新的对抗性防御机制CARSO，用于图像分类，灵感来自认知神经科学的线索。该方法与对抗训练具有协同互补性，并依赖于被攻击分类器的内部表示的知识。通过利用生成模型进行对抗净化，该方法采样输入的重构来进行最终分类。在各种图像数据集和分类器体系结构上进行的实验评估表明，CARSO能够比最先进的对抗性训练更好地保护分类器——同时具有可接受的清洁准确度损失。此外，防御体系结构成功地保护自己免受未预见的威胁和最终攻击。代码和预训练模型可在https://github.com/获得。

    In this paper, we propose a novel adversarial defence mechanism for image classification -- CARSO -- inspired by cues from cognitive neuroscience. The method is synergistically complementary to adversarial training and relies on knowledge of the internal representation of the attacked classifier. Exploiting a generative model for adversarial purification, conditioned on such representation, it samples reconstructions of inputs to be finally classified. Experimental evaluation by a well-established benchmark of varied, strong adaptive attacks, across diverse image datasets and classifier architectures, shows that CARSO is able to defend the classifier significantly better than state-of-the-art adversarial training alone -- with a tolerable clean accuracy toll. Furthermore, the defensive architecture succeeds in effectively shielding itself from unforeseen threats, and end-to-end attacks adapted to fool stochastic defences. Code and pre-trained models are available at https://github.com/
    
[^56]: COVER：一种启发式贪心对抗攻击预训练语言模型中的基于提示学习

    COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models. (arXiv:2306.05659v1 [cs.CL])

    [http://arxiv.org/abs/2306.05659](http://arxiv.org/abs/2306.05659)

    本文提出了一种启发式贪心对抗攻击，针对基于提示的模板在PLMs中可能存在的漏洞，通过字符级和单词级的破坏方法进行攻击，取得了较高的攻击成功率。

    

    基于提示的学习已被证明是预训练语言模型（PLMs）中一种有效的方式，特别是在像少量样本场景这样的低资源情况下。然而，PLMs的可信度至关重要，并且在基于模板的提示中已经显示出了潜在的漏洞，可能会误导语言模型的预测，引起严重的安全问题。本文通过在黑盒场景中提出基于提示的对抗攻击手段，揭示了PLMs的一些漏洞。首先，我们设计了字符级别和单词级别的启发式方法来破坏手动模板。然后，我们基于上述启发式破坏方法提出了一种贪心算法进行攻击。最后，我们使用BERT系列模型的三个变种和八个数据集的分类任务评估了我们的方法。广泛的实验结果证明了我们的方法在攻击成功率方面的有效性。

    Prompt-based learning has been proved to be an effective way in pre-trained language models (PLMs), especially in low-resource scenarios like few-shot settings. However, the trustworthiness of PLMs is of paramount significance and potential vulnerabilities have been shown in prompt-based templates that could mislead the predictions of language models, causing serious security concerns. In this paper, we will shed light on some vulnerabilities of PLMs, by proposing a prompt-based adversarial attack on manual templates in black box scenarios. First of all, we design character-level and word-level heuristic approaches to break manual templates separately. Then we present a greedy algorithm for the attack based on the above heuristic destructive approaches. Finally, we evaluate our approach with the classification tasks on three variants of BERT series models and eight datasets. And comprehensive experimental results justify the effectiveness of our approach in terms of attack success rate
    
[^57]: INSTRUCTEVAL：面向指导调整的大型语言模型的整体评估

    INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. (arXiv:2306.04757v1 [cs.CL])

    [http://arxiv.org/abs/2306.04757](http://arxiv.org/abs/2306.04757)

    INSTRUCTEVAL是一个专注于指导调整的大型语言模型评估的综合套件，它采取了全面的方法来评估模型的性能，包括解决问题、写作能力和与人类价值观的一致性等特征。

    

    指导调整的大型语言模型已经从根本上改变了自然语言处理，已经在诸如对话代理等应用中显示出了巨大的潜力。这些模型，如GPT-4，不仅能够掌握语言，而且可以解决数学、编码、医学和法律等领域的复杂任务。尽管它们具有卓越的能力，但由于许多模型的黑盒性质和缺乏全面的评估研究，对它们的全部潜力仍然缺乏全面的理解。为了解决这些挑战，我们提出了INSTRUCTEVAL，一个更全面的评估套件，专门针对指导调整的大型语言模型。与以往的作品不同，我们的评估包括对模型基于解决问题、写作能力和与人类价值观的一致性的严格评估。我们采取了全面的方法来分析影响模型性能的各种因素，包括预训练基础、指导调整数据和训练。

    Instruction-tuned large language models have revolutionized natural language processing and have shown great potential in applications such as conversational agents. These models, such as GPT-4, can not only master language but also solve complex tasks in areas like mathematics, coding, medicine, and law. Despite their impressive capabilities, there is still a lack of comprehensive understanding regarding their full potential, primarily due to the black-box nature of many models and the absence of holistic evaluation studies. To address these challenges, we present INSTRUCTEVAL, a more comprehensive evaluation suite designed specifically for instruction-tuned large language models. Unlike previous works, our evaluation involves a rigorous assessment of models based on problem-solving, writing ability, and alignment to human values. We take a holistic approach to analyze various factors affecting model performance, including the pretraining foundation, instruction-tuning data, and train
    
[^58]: 用于基于检索的对话系统的上下文掩码自编码器

    ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v1 [cs.CL])

    [http://arxiv.org/abs/2306.04357](http://arxiv.org/abs/2306.04357)

    本研究提出了一种针对对话响应选择的后训练技术Dial-MAE，利用生成方法更好地压缩对话语义至密集向量，并提高对话响应选择准确性。

    

    对话响应选择旨在根据给定的用户和系统话语历史记录从几个候选响应中选择适当的响应。最近的研究通过后训练大多依赖于单纯的掩码语言建模方法来提高对话响应选择的准确性。但是，最近开发的生成方法在IR社区展示了有希望的文本表示能力，这可能会导致更好的对话语义建模。因此，在本文中，我们提出 Dial-MAE（对话上下文掩码自编码器），这是一种简单而有效的针对对话响应选择的后训练技术。 Dial-MAE使用一个不对称的编码器-解码器架构，学习将对话的语义更好地压缩到密集向量中。 Dial-MAE的过程包括由深度编码器创建带有掩码对话上下文的对话嵌入，然后是浅解码器，该解码器使用此嵌入以及上下文向量来生成响应。

    Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Recent studies have been improving the accuracy of dialogue response selection through post-training, mostly relying on naive masked language modeling methods. However, the recently developed generative methods have shown promising text representation capabilities in IR community, which could potentially lead to better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE (Dialogue Contextual Masking Auto-encoder), a straightforward yet effective post-training technique tailored for dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture that learns to better compress the semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE involves a deep encoder creating a dialogue embedding with the masked dialogue context, followed by a shallow decoder that uses this embedding along with
    
[^59]: 我害怕我做不到：预测黑匣子生成语言模型中的提示拒绝行为

    I'm Afraid I Can't Do That: Predicting Prompt Refusal in Black-Box Generative Language Models. (arXiv:2306.03423v1 [cs.AI])

    [http://arxiv.org/abs/2306.03423](http://arxiv.org/abs/2306.03423)

    本文研究了生成语言模型的拒绝行为，并发现这种行为不是完全二元的。作者对ChatGPT进行的实验表明，在微调过程中的偏见来自于个别工程师和公司政策，并影响模型选择拒绝哪些提示。

    

    自从OpenAI的ChatGPT发布以来，生成语言模型引起了广泛关注。增加的使用量凸显了生成模型的广泛实用性，但同时也揭示了一些嵌入式偏见。其中一些是由预训练语料库引起的；但是，针对生成模型的额外偏见来自于主观微调以避免生成有害内容。微调偏见可能来自个别工程师和公司政策，并影响模型选择拒绝哪些提示。本实验使用黑盒攻击，对ChatGPT的拒绝行为进行了表征。我们首先使用各种攻击性和良性提示（n = 1,730）查询ChatGPT，然后手动标记每个响应是否履行或拒绝。响应的手动检查表明，拒绝不是完全二元的，并且在连续的范围内；因此，我们将几种不同类型的响应映射到履行或拒绝的二元值中。使用了小型手动标记的数据集。

    Since the release of OpenAI's ChatGPT, generative language models have attracted extensive public attention. The increased usage has highlighted generative models' broad utility, but also revealed several forms of embedded bias. Some is induced by the pre-training corpus; but additional bias specific to generative models arises from the use of subjective fine-tuning to avoid generating harmful content. Fine-tuning bias may come from individual engineers and company policies, and affects which prompts the model chooses to refuse. In this experiment, we characterize ChatGPT's refusal behavior using a black-box attack. We first query ChatGPT with a variety of offensive and benign prompts (n=1,730), then manually label each response as compliance or refusal. Manual examination of responses reveals that refusal is not cleanly binary, and lies on a continuum; as such, we map several different kinds of responses to a binary of compliance or refusal. The small manually-labeled dataset is used 
    
[^60]: DreamSparse: 利用 2D 扩散模型从稀疏视角中合成图像

    DreamSparse: Escaping from Plato's Cave with 2D Diffusion Model Given Sparse Views. (arXiv:2306.03414v1 [cs.CV])

    [http://arxiv.org/abs/2306.03414](http://arxiv.org/abs/2306.03414)

    本文提出了 DreamSparse 框架，该框架通过利用先前训练的扩散模型的 2D 先验知识，通过几何模块和空间引导模型来解决 2D 模型缺乏 3D 感知能力的问题，进一步实现了从少视角情况下合成高质量的新视角图像。

    

    从少量视角中合成新的图像是一个具有挑战性但实际的问题。现有方法通常难以产生高质量的结果或在此类少视角设置中需要逐个对象优化，因为提供的信息不足。在这项工作中，我们探索利用预先训练的扩散模型的强大的 2D 先验知识，来合成新颖的视角图像。然而，2D 扩散模型缺乏 3D 感知能力，导致图像合成失真，影响了图像的识别性。为了解决这些问题，我们提出了 DreamSparse，一个可以生成几何和识别联合一致的新视角图像的框架。具体而言，DreamSparse 包括一个几何模块，用于从稀疏视角获取 3D 特征作为 3D 先验，随后引入一个空间引导模型将这些 3D 特征图转换为生成过程的空间信息。这些信息然后用于通过对抗损失指导预训练扩散模型合成高质量的新视角图像。实验结果显示，DreamSparse 在少视角图像合成方面取得了最先进的结果，并且可以生成准确和稳健的物体几何和识别。

    Synthesizing novel view images from a few views is a challenging but practical problem. Existing methods often struggle with producing high-quality results or necessitate per-object optimization in such few-view settings due to the insufficient information provided. In this work, we explore leveraging the strong 2D priors in pre-trained diffusion models for synthesizing novel view images. 2D diffusion models, nevertheless, lack 3D awareness, leading to distorted image synthesis and compromising the identity. To address these problems, we propose DreamSparse, a framework that enables the frozen pre-trained diffusion model to generate geometry and identity-consistent novel view image. Specifically, DreamSparse incorporates a geometry module designed to capture 3D features from sparse views as a 3D prior. Subsequently, a spatial guidance model is introduced to convert these 3D feature maps into spatial information for the generative process. This information is then used to guide the pre-
    
[^61]: 生成式人工智能应用调查

    A survey of Generative AI Applications. (arXiv:2306.02781v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02781](http://arxiv.org/abs/2306.02781)

    本篇论文对350多个生成式人工智能应用进行了全面调查，总结了不同单模和多模生成式人工智能的应用。该调查为研究人员和从业者提供了宝贵的资源，帮助他们更好地了解生成式人工智能领域目前的最先进技术，并促进该领域的进一步创新。

    

    近年来，生成式人工智能有了显著增长，并在各个领域展示了广泛的应用。本文对350多个生成式人工智能应用进行了全面调查，提供了分类结构和对不同单模和多模生成式人工智能的简洁描述。该调查分成多个部分，覆盖了文本、图像、视频、游戏和脑信息等单模生成式人工智能的广泛应用。我们的调研旨在为研究人员和从业者提供宝贵的资源，帮助他们更好地了解生成式人工智能领域目前的最先进技术，并促进该领域的进一步创新。

    Generative AI has experienced remarkable growth in recent years, leading to a wide array of applications across diverse domains. In this paper, we present a comprehensive survey of more than 350 generative AI applications, providing a structured taxonomy and concise descriptions of various unimodal and even multimodal generative AIs. The survey is organized into sections, covering a wide range of unimodal generative AI applications such as text, images, video, gaming and brain information. Our survey aims to serve as a valuable resource for researchers and practitioners to navigate the rapidly expanding landscape of generative AI, facilitating a better understanding of the current state-of-the-art and fostering further innovation in the field.
    
[^62]: PassGPT: 大语言模型中的密码建模和（引导式）生成

    PassGPT: Password Modeling and (Guided) Generation with Large Language Models. (arXiv:2306.01545v1 [cs.CL])

    [http://arxiv.org/abs/2306.01545](http://arxiv.org/abs/2306.01545)

    本研究使用大型语言模型PassGPT进行密码建模和生成，该模型比基于GAN的现有方法更准确，能生成符合任意限制的密码，为提高密码强度估计器提供了潜在的帮助。

    

    大语言模型（LLMs）可以成功地模拟自然语言，无需明确的监督，仅通过大量的文本进行训练。本文研究了LLMs在建模密码方面的有效性。我们介绍了PassGPT，它是一个在密码泄露数据上进行训练的LLM，用于生成密码。PassGPT通过猜测两倍于基于生成性对抗网络（GAN）的现有方法中的以前未见过的密码而胜过其它方法。此外，我们介绍了引导式密码生成的概念，利用PassGPT的抽样过程生成符合任意限制的密码，这在当前基于GAN的策略中是缺乏的。最后，我们对PassGPT对密码定义的熵和概率分布进行了深入分析，并讨论了其在增强现有密码强度估计器中的应用。

    Large language models (LLMs) successfully model natural language from vast amounts of text without the need for explicit supervision. In this paper, we investigate the efficacy of LLMs in modeling passwords. We present PassGPT, a LLM trained on password leaks for password generation. PassGPT outperforms existing methods based on generative adversarial networks (GAN) by guessing twice as many previously unseen passwords. Furthermore, we introduce the concept of guided password generation, where we leverage PassGPT sampling procedure to generate passwords matching arbitrary constraints, a feat lacking in current GAN-based strategies. Lastly, we conduct an in-depth analysis of the entropy and probability distribution that PassGPT defines over passwords and discuss their use in enhancing existing password strength estimators.
    
[^63]: 与康科迪亚并行的神经符号一体化

    Parallel Neurosymbolic Integration with Concordia. (arXiv:2306.00480v1 [cs.AI])

    [http://arxiv.org/abs/2306.00480](http://arxiv.org/abs/2306.00480)

    康科迪亚框架是一个支持各种概率理论，克服了现有技术的限制的并行的神经符号结构，成功应用于集合活动检测、实体链接和推荐任务，提高了最新准确性。

    

    并行的神经符号结构通过将逻辑理论的知识提取到深度模型中，在NLP中得到有效应用。然而，现有技术仍存在一些限制，包括支持受限形式的逻辑理论，并依赖于逻辑和深度网络之间的独立性假设。本文提出了康科迪亚框架，克服了现有技术的限制。康科迪亚框架既不关注深度网络，也不关注逻辑理论，支持各种概率理论。框架可以支持两个组件的监督训练和神经组件的无监督训练。康科迪亚已成功应用于NLP和数据分类以外的任务，提高了集合活动检测、实体链接和推荐任务的最新准确性。

    Parallel neurosymbolic architectures have been applied effectively in NLP by distilling knowledge from a logic theory into a deep model.However, prior art faces several limitations including supporting restricted forms of logic theories and relying on the assumption of independence between the logic and the deep network. We present Concordia, a framework overcoming the limitations of prior art. Concordia is agnostic both to the deep network and the logic theory offering support for a wide range of probabilistic theories. Our framework can support supervised training of both components and unsupervised training of the neural component. Concordia has been successfully applied to tasks beyond NLP and data classification, improving the accuracy of state-of-the-art on collective activity detection, entity linking and recommendation tasks.
    
[^64]: 基于单调最大和的图神经网络与Datalog的对应关系研究

    On the Correspondence Between Monotonic Max-Sum GNNs and Datalog. (arXiv:2305.18015v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.18015](http://arxiv.org/abs/2305.18015)

    本文研究了数据变换基于图神经网络的表现能力，证明了单调最大和的GNN与Datalog之间的关系紧密，即任何单调最大和的GNN都可以表示为一个Datalog程序，反之亦然。

    

    虽然应用机器学习技术到结构化数据上引起了广泛的关注，但这些技术的表达能力（即可以学习到什么）仍然不太清楚。本文研究了基于图神经网络的数据变换。首先，我们注意到如何将数据集编码成GNN可处理的数字形式可以模糊模型表达能力的描述，我们认为一种规范编码提供了适当的基础。其次，我们研究了单调最大和的GNN的表现能力，它们涵盖了具有max和sum聚合函数的GNN的一个子类。我们证明了对于每个这样的GNN，可以计算出一个Datalog程序，使得将GNN应用于任何数据集都会产生与将程序规则应用于数据集的单个循环相同的事实。单调最大和的GNN能够对无限数量的特征向量求和，这可能会导致特征值任意增大，而基于规则的系统（例如Datalog）具有保证系统始终会趋于稳定状态的单调性质。我们证明了单调最大和的GNN与Datalog之间的这种关系是紧密的，即任何单调最大和的GNN都可以表示为一个Datalog程序，反之亦然。

    Although there has been significant interest in applying machine learning techniques to structured data, the expressivity (i.e., a description of what can be learned) of such techniques is still poorly understood. In this paper, we study data transformations based on graph neural networks (GNNs). First, we note that the choice of how a dataset is encoded into a numeric form processable by a GNN can obscure the characterisation of a model's expressivity, and we argue that a canonical encoding provides an appropriate basis. Second, we study the expressivity of monotonic max-sum GNNs, which cover a subclass of GNNs with max and sum aggregation functions. We show that, for each such GNN, one can compute a Datalog program such that applying the GNN to any dataset produces the same facts as a single round of application of the program's rules to the dataset. Monotonic max-sum GNNs can sum an unbounded number of feature vectors which can result in arbitrarily large feature values, whereas rul
    
[^65]: 缩放数据受限的语言模型

    Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.16264](http://arxiv.org/abs/2305.16264)

    研究人员研究了在数据受限制的情况下缩放语言模型，并提出了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。

    

    现在扩展语言模型的趋势涉及增加参数计数和训练数据集大小。推断这个趋势表明，训练数据集大小可能很快就会受到互联网上可用文本数据的限制。出于此限制的动机，我们研究在数据受限制的情况下缩放语言模型。具体而言，我们运行了大量的实验，变化数据重复程度和计算预算，范围达到了9000亿个训练令牌和9亿参数模型。我们发现，在有限的数据的情况下，使用高达4次重复数据的训练与使用唯一数据相比对损失的贡献微不足道。然而，使用更多的重复数据，添加计算的价值最终会衰减为零。我们提出并经验证了一个计算最优性的缩放定律，考虑到重复令牌和过量参数的价值递减。最后，我们尝试了缓解数据稀缺的方法。

    The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
    
[^66]: OVO: 开放词汇占据

    OVO: Open-Vocabulary Occupancy. (arXiv:2305.16133v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.16133](http://arxiv.org/abs/2305.16133)

    本文提出了开放词汇占据（OVO）方法，可以允许任意类别的语义占据预测，无需三维注释，其关键技术包括预训练二维开放词汇分割模型到三维占据网络的知识蒸馏和像素-体素过滤以生成高质量的训练数据。

    

    语义占据预测旨在推断出自主代理在三维环境中操作的密集几何和语义。现有的占据预测方法几乎完全是基于人工注释的体积数据进行训练的。虽然这些数据有高质量，但是生成这些三维注释是费力且成本高昂的，限制了它们仅可以训练少量特定物体类别的数据集。为了解决这一限制，本文提出了一种新颖的方法：开放词汇占据（OVO），允许任意类别的语义占据预测，但在训练过程中无需三维注释。我们方法的关键是：（1）从预训练的二维开放词汇分割模型到三维占据网络的知识蒸馏，（2）像素 - 体素过滤以生成高质量的训练数据。该框架简单、紧凑且兼容大多数最先进的语义占据预测模型。在 NYUv2 和 SemanticKIT 上的实验显示出该方法的有效性。

    Semantic occupancy prediction aims to infer dense geometry and semantics of surroundings for an autonomous agent to operate safely in the 3D environment. Existing occupancy prediction methods are almost entirely trained on human-annotated volumetric data. Although of high quality, the generation of such 3D annotations is laborious and costly, restricting them to a few specific object categories in the training dataset. To address this limitation, this paper proposes Open Vocabulary Occupancy (OVO), a novel approach that allows semantic occupancy prediction of arbitrary classes but without the need for 3D annotations during training. Keys to our approach are (1) knowledge distillation from a pre-trained 2D open-vocabulary segmentation model to the 3D occupancy network, and (2) pixel-voxel filtering for high-quality training data generation. The resulting framework is simple, compact, and compatible with most state-of-the-art semantic occupancy prediction models. On NYUv2 and SemanticKIT
    
[^67]: 不要训练它：图神经网络的线性神经架构搜索

    Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks. (arXiv:2305.14065v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14065](http://arxiv.org/abs/2305.14065)

    本文提出了一种新的图神经网络结构搜索方法——神经结构编码（NAC），它通过稀疏编码寻找最优结构参数，无需训练就能发挥表现力，在多个基准数据集上实现了最先进性能，并且运算速度比强基线方法快了200倍，精度提高了18.8％。

    

    图神经网络的神经架构搜索（NAS-GNN）已经显著地提高了手动设计的图神经网络的性能。然而，这些方法继承了传统NAS方法的问题，如高计算成本和优化难度。更重要的是，以前的NAS方法忽视了GNN的独特性，即GNN具有无需训练就具有表现力的特点。采用随机初始化的权重，我们可以通过稀疏编码目标寻找最优的架构参数，并得出一种新的NAS-GNN方法，即神经结构编码（NAC）。因此，我们的NAC在GNN上实现了无更新方案，可以在线性时间内高效计算。在多个GNN基准数据集上的实证评估表明，我们的方法导致了最先进的性能，比强基线方法快200倍，精度提高了18.8％。

    Neural architecture search (NAS) for Graph neural networks (GNNs), called NAS-GNNs, has achieved significant performance over manually designed GNN architectures. However, these methods inherit issues from the conventional NAS methods, such as high computational cost and optimization difficulty. More importantly, previous NAS methods have ignored the uniqueness of GNNs, where GNNs possess expressive power without training. With the randomly-initialized weights, we can then seek the optimal architecture parameters via the sparse coding objective and derive a novel NAS-GNNs method, namely neural architecture coding (NAC). Consequently, our NAC holds a no-update scheme on GNNs and can efficiently compute in linear time. Empirical evaluations on multiple GNN benchmark datasets demonstrate that our approach leads to state-of-the-art performance, which is up to $200\times$ faster and $18.8\%$ more accurate than the strong baselines.
    
[^68]: 一种基于元学习和信道状态信息的可推广室内定位模型

    A Meta-learning based Generalizable Indoor Localization Model using Channel State Information. (arXiv:2305.13453v1 [cs.LG])

    [http://arxiv.org/abs/2305.13453](http://arxiv.org/abs/2305.13453)

    本文提出了一种基于元学习和信道状态信息的室内定位模型，以解决深度学习定位模型中持续存在的通用性缺失问题。

    

    近年来，室内定位因其在智能家居、工业自动化和医疗保健等领域的广泛应用而受到了重视，特别是随着越来越多的人依赖其无线设备进行基于位置的服务。基于深度学习的解决方案利用无线参数（如信道状态信息（CSI）和接收信号强度指示器（RSSI））在室内环境中准确估计无线设备的位置已经取得了很好的效果。然而，尽管深度学习模型在实现高准确度的定位方面取得了成功，但这些模型缺乏通用性，在不重新训练的情况下无法轻松部署到新环境或在动态环境中运行。本文提出了基于元学习的定位模型来解决传统深度学习定位模型中持续存在的通用性缺失问题。此外，由于元学习算法需要多元化数据。

    Indoor localization has gained significant attention in recent years due to its various applications in smart homes, industrial automation, and healthcare, especially since more people rely on their wireless devices for location-based services. Deep learning-based solutions have shown promising results in accurately estimating the position of wireless devices in indoor environments using wireless parameters such as Channel State Information (CSI) and Received Signal Strength Indicator (RSSI). However, despite the success of deep learning-based approaches in achieving high localization accuracy, these models suffer from a lack of generalizability and can not be readily-deployed to new environments or operate in dynamic environments without retraining. In this paper, we propose meta-learning-based localization models to address the lack of generalizability that persists in conventionally trained DL-based localization models. Furthermore, since meta-learning algorithms require diverse dat
    
[^69]: 借助深度强化学习的贫民窟道路规划

    Road Planning for Slums via Deep Reinforcement Learning. (arXiv:2305.13060v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.13060](http://arxiv.org/abs/2305.13060)

    本文介绍了一种基于深度强化学习的方法，用于自动布局贫民窟道路。通过掩码策略优化，可使可达性提高14.3％，对现有基线方法具有明显改进。

    

    数百万贫民窟居民由于贫民窟内不足的道路基础设施而遭受城市服务无法访问的困境，而贫民窟道路规划对城市的可持续发展至关重要。现有的重组或启发式方法要么耗时，不能推广到不同的贫民窟，要么在可达性和建设成本方面产生次优的道路规划。本文提出了一种基于深度强化学习的方法，用于自动布局贫民窟道路。我们提出了一个通用图模型，用于捕获贫民窟的拓扑结构，并设计了一种新颖的图神经网络，用于选择计划道路的位置。通过掩码策略优化，我们的模型可以生成连接贫民窟地点的道路规划，以最小的建设成本。对不同国家的真实贫民窟进行大量实验验证了我们模型的有效性，可使可达性提高14.3％，对现有基线方法具有明显改进。

    Millions of slum dwellers suffer from poor accessibility to urban services due to inadequate road infrastructure within slums, and road planning for slums is critical to the sustainable development of cities. Existing re-blocking or heuristic methods are either time-consuming which cannot generalize to different slums, or yield sub-optimal road plans in terms of accessibility and construction costs. In this paper, we present a deep reinforcement learning based approach to automatically layout roads for slums. We propose a generic graph model to capture the topological structure of a slum, and devise a novel graph neural network to select locations for the planned roads. Through masked policy optimization, our model can generate road plans that connect places in a slum at minimal construction costs. Extensive experiments on real-world slums in different countries verify the effectiveness of our model, which can significantly improve accessibility by 14.3% against existing baseline metho
    
[^70]: 图传播变换器用于图表示学习

    Graph Propagation Transformer for Graph Representation Learning. (arXiv:2305.11424v1 [cs.LG])

    [http://arxiv.org/abs/2305.11424](http://arxiv.org/abs/2305.11424)

    本文提出了一种新的变换器架构 GPTrans，以图传播注意力为基础，可以更好地学习图形模型，并在多个基准测试集上超过了其他最先进的基于变换器的图形模型。

    

    本文提出了一种用于图表示学习的新型变换器架构。我们的方法的核心见解是在构建变换器块中的注意力模块时，充分考虑图中节点和边之间的信息传播。具体而言，我们提出了一种新的注意力机制称为图传播注意力（GPA），它将信息在节点和边之间以三种方式明确传递，即从节点到节点，从节点到边和从边到节点，这对于学习图结构数据至关重要。在此基础上，我们设计了一种名为图传播变换器（GPTrans）的有效变换器架构，进一步帮助学习图数据。我们在几个基准数据集上的广泛图学习实验中验证了GPTrans的性能。这些结果表明，我们的方法以更好的性能超过了许多最先进的基于变换器的图形模型。代码将在https://github.com/czczup/GPTrans上发布。

    This paper presents a novel transformer architecture for graph representation learning. The core insight of our method is to fully consider the information propagation among nodes and edges in a graph when building the attention module in the transformer blocks. Specifically, we propose a new attention mechanism called Graph Propagation Attention (GPA). It explicitly passes the information among nodes and edges in three ways, i.e. node-to-node, node-to-edge, and edge-to-node, which is essential for learning graph-structured data. On this basis, we design an effective transformer architecture named Graph Propagation Transformer (GPTrans) to further help learn graph data. We verify the performance of GPTrans in a wide range of graph learning experiments on several benchmark datasets. These results show that our method outperforms many state-of-the-art transformer-based graph models with better performance. The code will be released at https://github.com/czczup/GPTrans.
    
[^71]: LoViT: 用于手术阶段识别的长视频Transformer

    LoViT: Long Video Transformer for Surgical Phase Recognition. (arXiv:2305.08989v1 [cs.CV])

    [http://arxiv.org/abs/2305.08989](http://arxiv.org/abs/2305.08989)

    LoViT是一种用于手术阶段识别的长视频Transformer，它通过结合时间丰富的空间特征提取器和多尺度时间聚合器来对长视频进行分析，优于现有方法。

    

    在构建能够量化表现并监督手术流程执行的上下文工具方面，在线手术阶段识别发挥着重要作用。目前的方法有限，因为它们使用基于帧级监督的空间特征提取器进行训练，这可能会导致由于相似帧在不同阶段出现而导致的错误预测，并且由于计算限制而未能很好地融合本地和全局特征，这可能影响手术干预中通常遇到的长视频的分析。在本文中，我们提出了一种名为LoViT的两阶段方法，用于融合短期和长期时间信息，它结合了一个时间丰富的空间特征提取器和一个多尺度时间聚合器，后者由基于自注意力的两个级联L-Trans模块和一个基于ProbSparse自注意力的G-Informer模块组成，用于处理全局时间信息。然后，多尺度时间头结合本地和全局特征来识别长视频中的手术阶段。我们展示了LoViT在两个公共手术数据集上优于现有方法。

    Online surgical phase recognition plays a significant role towards building contextual tools that could quantify performance and oversee the execution of surgical workflows. Current approaches are limited since they train spatial feature extractors using frame-level supervision that could lead to incorrect predictions due to similar frames appearing at different phases, and poorly fuse local and global features due to computational constraints which can affect the analysis of long videos commonly encountered in surgical interventions. In this paper, we present a two-stage method, called Long Video Transformer (LoViT) for fusing short- and long-term temporal information that combines a temporally-rich spatial feature extractor and a multi-scale temporal aggregator consisting of two cascaded L-Trans modules based on self-attention, followed by a G-Informer module based on ProbSparse self-attention for processing global temporal information. The multi-scale temporal head then combines loc
    
[^72]: 面向时间序列预测的谱时图神经网络的表达能力研究

    How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?. (arXiv:2305.06587v1 [cs.LG])

    [http://arxiv.org/abs/2305.06587](http://arxiv.org/abs/2305.06587)

    该论文研究了谱时图神经网络的表达能力，并揭示了其具有线性谱时GNN是普适的、表现力受到离散时间动态图扩展的第一阶Weisfeiler-Leman算法的限制。同时，论文提出了一个简单实例TGC，其在时间序列预测方面具有显著的性能优势。

    

    谱时图神经网络是大多数基于图神经网络(GNN)的时间序列预测模型的一个有前途的抽象。然而，我们需要更多关于这种方法的基础知识。本文建立了一个理论框架，揭示了谱时GNN的表现力。我们的结果表明，具有线性谱时GNN是普适的，在温和的假设下，它们的表现力受到我们的离散时间动态图扩展的第一阶Weisfeiler-Leman算法的限制。为了使我们的发现在实践中有用，我们详细讨论了相关限制，并概述了在谱域中设计空间和时间模块的理论蓝图。基于这些见解，并为了展示基于我们的框架，谱时GNN有多么强大，我们提出了一个名为 Temporal Graph GegenConv (TGC) 的简单实例，显著优于大多数已有的模型。

    Spectral-temporal graph neural network is a promising abstraction underlying most time series forecasting models that are based on graph neural networks (GNNs). However, more is needed to know about the underpinnings of this branch of methods. In this paper, we establish a theoretical framework that unravels the expressive power of spectral-temporal GNNs. Our results show that linear spectral-temporal GNNs are universal under mild assumptions, and their expressive power is bounded by our extended first-order Weisfeiler-Leman algorithm on discrete-time dynamic graphs. To make our findings useful in practice on valid instantiations, we discuss related constraints in detail and outline a theoretical blueprint for designing spatial and temporal modules in spectral domains. Building on these insights and to demonstrate how powerful spectral-temporal GNNs are based on our framework, we propose a simple instantiation named Temporal Graph GegenConv (TGC), which significantly outperforms most e
    
[^73]: 人还是机器：基于图灵测试的日常反思

    Human or Machine: Reflections on Turing-Inspired Testing for the Everyday. (arXiv:2305.04312v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.04312](http://arxiv.org/abs/2305.04312)

    本文基于图灵测试，回避了机器是否智能的问题，探讨在日常生活中如何确定一个交互对象是人还是机器的挑战，并思考了其应用和重要性。

    

    在他的开创性论文《计算机器械与智能》中，艾伦·图灵引入了“模仿游戏”，探讨了机器智能的概念。图灵测试自那时以来一直是广泛讨论、完善和扩展的主题。本文回避了关于某个特定机器是否能被标记为智能或能否在给定环境中匹配人类能力的问题。与此相反，但受图灵启发，我们关注在日常生活中确定是否正在与一个人或一个机器进行交互这个看似简单的挑战。我们对这个人还是机器问题及其可靠答案的应用感到感兴趣，并希望反思其重要性。虽然图灵的原始测试被广泛认为是一种思维实验，但本文讨论的人还是机器问题具有明显的实际意义。虽然人类是否能够创造出能够胜任所有的人类工作的机器也未可知。

    In his seminal paper "Computing Machinery and Intelligence", Alan Turing introduced the "imitation game" as part of exploring the concept of machine intelligence. The Turing Test has since been the subject of much analysis, debate, refinement and extension. Here we sidestep the question of whether a particular machine can be labeled intelligent, or can be said to match human capabilities in a given context. Instead, but inspired by Turing, we draw attention to the seemingly simpler challenge of determining whether one is interacting with a human or with a machine, in the context of everyday life. We are interested in reflecting upon the importance of this Human-or-Machine question and the use one may make of a reliable answer thereto. Whereas Turing's original test is widely considered to be more of a thought experiment, the Human-or-Machine question as discussed here has obvious practical significance. And while the jury is still not in regarding the possibility of machines that can m
    
[^74]: 语言、时间偏好和消费行为：大型语言模型的证据

    Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v1 [econ.GN])

    [http://arxiv.org/abs/2305.02531](http://arxiv.org/abs/2305.02531)

    本研究分析了大型语言模型在不同语言提示下的奖励时间偏好，并发现GPT在具有较弱未来时态的语言下表现出更大的耐心，这与使用该语言的人类的偏好相似。

    

    语言对我们对时间和奖励的感知有很大的影响。这引发了一个问题，即当以不同的语言询问大型语言模型时，它们是否显示出不同的奖励时间偏好，并且它们的选择是否类似于人类的选择。本研究分析了GPT-3.5（以下简称GPT）在多种语言提示下的响应，探索了较小、较早的奖励和较大、较晚的奖励之间的偏好。我们的结果显示，当以语义含义较弱的未来时态参考（FTR），如德语和汉语，为提示语时，GPT表现出更大的耐心，相比英语和法语等具有强大FTR的语言。这些发现与现有文献一致，并表明了GPT的选择与这些语言的使用者的偏好之间的关联。然而，进一步的分析揭示了较早或较晚奖励的偏好并没有随着奖励差异系统地改变，这表明了一种词典序优先的选择。

    Language has a strong influence on our perceptions of time and rewards. This raises the question of whether large language models, when asked in different languages, show different preferences for rewards over time and if their choices are similar to those of humans. In this study, we analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages, exploring preferences between smaller, sooner rewards and larger, later rewards. Our results show that GPT displays greater patience when prompted in languages with weak future tense references (FTR), such as German and Mandarin, compared to languages with strong FTR, like English and French. These findings are consistent with existing literature and suggest a correlation between GPT's choices and the preferences of speakers of these languages. However, further analysis reveals that the preference for earlier or later rewards does not systematically change with reward gaps, indicating a lexicographic preferen
    
[^75]: GPTutor: 一种由ChatGPT驱动的编程工具，用于程序代码解释

    GPTutor: a ChatGPT-powered programming tool for code explanation. (arXiv:2305.01863v1 [cs.HC])

    [http://arxiv.org/abs/2305.01863](http://arxiv.org/abs/2305.01863)

    本文介绍了一种名为GPTutor的ChatGPT动力编程工具，它是一个使用ChatGPT API的Visual Studio Code扩展，通过设计提示词，可以对所选代码进行精简、准确的解释。

    

    学习新的编程技能需要个性化指导。随着ChatGPT API等高级自然语言生成模型的出现，现在有可能创建一个方便的、个性化的AI编程教育辅导系统。本文介绍了一种名为GPTutor的ChatGPT动力编程工具，它是一个使用ChatGPT API的Visual Studio Code扩展，用于提供编程代码解释。

    Learning new programming skills requires tailored guidance. With the emergence of advanced Natural Language Generation models like the ChatGPT API, there is now a possibility of creating a convenient and personalized tutoring system with AI for computer science education. This paper presents GPTutor, a ChatGPT-powered programming tool, which is a Visual Studio Code extension using the ChatGPT API to provide programming code explanations. By integrating Visual Studio Code API, GPTutor can comprehensively analyze the provided code by referencing the relevant source codes. As a result, GPTutor can use designed prompts to explain the selected code with a pop-up message. GPTutor is now published at the Visual Studio Code Extension Marketplace, and its source code is openly accessible on GitHub. Preliminary evaluation indicates that GPTutor delivers the most concise and accurate explanations compared to vanilla ChatGPT and GitHub Copilot. Moreover, the feedback from students and teachers ind
    
[^76]: 通过多样性权重实现生成模型的模式平衡

    Towards Mode Balancing of Generative Models via Diversity Weights. (arXiv:2304.11961v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.11961](http://arxiv.org/abs/2304.11961)

    本研究提出了通过平衡训练数据集中的模式来增加模型输出多样性的多样性权重训练方案，以更好地适应需要多样化输出的创意应用，并在受控环境中进行的初步实验展示了其潜力。

    

    大型数据驱动的图像模型被广泛用于支持创意和艺术作品。在当前主导的分布拟合范式下，数据集被视为要尽可能接近的真实值。然而，许多创意应用需要多样化的输出，创作者经常努力从给定的数据分布中积极分离出来。我们认为，从纯模式覆盖转向模式平衡的建模目标调整是必要的，以适应更高的输出多样性目标。我们提出了多样性权重，这是一种通过平衡训练数据集中的模式来增加模型输出多样性的训练方案。在受控环境中进行的初步实验展示了我们方法的潜力。我们讨论了我们方法与多样性、公平和包容在生成式机器学习以及计算机创意中的联系。我们的算法实现可以在https://github.com/找到。

    Large data-driven image models are extensively used to support creative and artistic work. Under the currently predominant distribution-fitting paradigm, a dataset is treated as ground truth to be approximated as closely as possible. Yet, many creative applications demand a diverse range of output, and creators often strive to actively diverge from a given data distribution. We argue that an adjustment of modelling objectives, from pure mode coverage towards mode balancing, is necessary to accommodate the goal of higher output diversity. We present diversity weights, a training scheme that increases a model's output diversity by balancing the modes in the training dataset. First experiments in a controlled setting demonstrate the potential of our method. We discuss connections of our approach to diversity, equity, and inclusion in generative machine learning more generally, and computational creativity specifically. An implementation of our algorithm is available at https://github.com/
    
[^77]: 基于基础模型的工具学习

    Tool Learning with Foundation Models. (arXiv:2304.08354v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.08354](http://arxiv.org/abs/2304.08354)

    基于基础模型的工具学习结合了专用工具和基础模型的优势，实现了问题解决的增强精度、效率和自动化。本文对工具学习进行了系统研究，提出了涵盖两种类型学习的通用工具学习框架，并分析了它们的独特挑战、机会和未来方向。

    

    人类拥有非凡的创造和利用工具的能力，使得他们能够克服物理限制并探索新的领域。随着基础模型的出现，AI系统有望像人类一样熟练地使用工具。这种范式即基于基础模型的工具学习，结合了专用工具和基础模型的优势，实现了问题解决的增强精度、效率和自动化。尽管具有巨大潜力，但该领域仍缺乏对关键挑战、机会和未来发展的全面理解。针对这一问题，本文对工具学习进行了系统研究。首先介绍了工具学习的背景，包括其认知起源、基础模型的范式转换和工具和模型的互补作用。然后，我们回顾了现有的工具学习研究，包括基于工具和面向工具的学习。我们制定了一个涵盖两种类型学习的通用工具学习框架，并分析了它们的独特挑战、机会和未来方向。我们预计这种系统的探索将为未来开发具有复杂工具学习能力的AI系统提供一个跳板。

    Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmented and tool-oriented learning. We formulate a general tool l
    
[^78]: OpenAGI：当LLM遇到领域专家

    OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])

    [http://arxiv.org/abs/2304.04370](http://arxiv.org/abs/2304.04370)

    基于大型语言模型的OpenAGI平台通过整合领域专家模型和自然语言问答形式，实现复杂任务解决。

    

    人类具有将基本技能组合成复杂技能以解决复杂任务的显著能力。这种能力对于人工智能同样重要，因此，我们断言，除了开发大型综合智能模型外，将不同领域专家模型应用于复杂任务解决能力同样关键，以在人工智能通用智能的追求中使其具备这种能力。最近的大型语言模型（LLM）的发展证明其具有出色的学习和推理能力，使它们成为选择、综合和执行外部模型以解决复杂任务的控制器的有前途的选择。在这个项目中，我们开发了一个名为OpenAGI的开源AGI研究平台，专门设计为提供复杂的多步骤任务，并配有任务特定的数据集、评估指标和各种可扩展模型。OpenAGI将复杂任务阐释为自然语言问答，旨在促进领域专家和语言模型之间的协同作用。

    Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
    
[^79]: 基于核凸包机的差分隐私学习研究

    Kernel Affine Hull Machines for Differentially Private Learning. (arXiv:2304.01300v1 [cs.LG])

    [http://arxiv.org/abs/2304.01300](http://arxiv.org/abs/2304.01300)

    本文提出了一种基于核凸包机的方法来确保数据隐私保护，同时保留数据结构，用于数据表示学习的分类应用中。为了确保隐私保护学习，还提出了一种新颖的生成虚假数据的方法。

    

    本文探讨了通过学习再生核希尔伯特空间中的点的凸包来表示数据的方法，旨在将数据空间划分为几何体，从而隐藏有关单个数据点的隐私信息，同时保留原始学习问题的结构。为此，我们引入了核凸包机（KAHM），它提供了一种有效的方法来计算从结果有界几何体中的距离度量。KAHM是广泛和深入的自编码器的关键构建块，它们使数据表示学习用于分类应用。为了确保隐私保护学习，我们提出了一种新颖的生成虚假数据的方法，该方法涉及将差分隐私数据样本通过转换过程进行平滑处理。生成的虚假数据不仅保证差分隐私，而且确保KAHM建模误差不大于原始数据误差。

    This paper explores the use of affine hulls of points as a means of representing data via learning in Reproducing Kernel Hilbert Spaces (RKHS), with the goal of partitioning the data space into geometric bodies that conceal privacy-sensitive information about individual data points, while preserving the structure of the original learning problem. To this end, we introduce the Kernel Affine Hull Machine (KAHM), which provides an effective way of computing a distance measure from the resulting bounded geometric body. KAHM is a critical building block in wide and deep autoencoders, which enable data representation learning for classification applications. To ensure privacy-preserving learning, we propose a novel method for generating fabricated data, which involves smoothing differentially private data samples through a transformation process. The resulting fabricated data guarantees not only differential privacy but also ensures that the KAHM modeling error is not larger than that of the
    
[^80]: Lean 的机器学习前提选择工具

    Machine-Learned Premise Selection for Lean. (arXiv:2304.00994v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2304.00994](http://arxiv.org/abs/2304.00994)

    该论文介绍了一种基于机器学习的工具，可为 Lean 证明助手建议与用户正在证明的定理相关的前提条件。

    

    我们介绍了一种基于机器学习的工具，可为 Lean 证明助手建议与用户正在证明的定理相关的前提条件。该工具的设计原则为：（1）与证明助手紧密集成，（2）易于使用和安装，（3）采用轻量级且快速的方法。为此，我们设计了一个在线学习的自定义随机森林模型，直接在 Lean 中实现，这得益于 Lean 4 丰富而高效的元编程功能。随机森林的训练数据来自于 mathlib -- Lean 的数学库。我们尝试了各种选项来产生训练特征和标签。经过训练的模型的建议可以通过“suggest_premises策略”传达给用户，在交互式构建证明过程中可以在编辑器中调用该策略。

    We introduce a machine-learning-based tool for the Lean proof assistant that suggests relevant premises for theorems being proved by a user. The design principles for the tool are (1) tight integration with the proof assistant, (2) ease of use and installation, (3) a lightweight and fast approach. For this purpose, we designed a custom version of the random forest model, trained in an online fashion. It is implemented directly in Lean, which was possible thanks to the rich and efficient metaprogramming features of Lean 4. The random forest is trained on data extracted from mathlib -- Lean's mathematics library. We experiment with various options for producing training features and labels. The advice from a trained model is accessible to the user via the suggest_premises tactic which can be called in an editor while constructing a proof interactively.
    
[^81]: 带有概率触发臂的情境组合赌博机

    Contextual Combinatorial Bandits with Probabilistically Triggered Arms. (arXiv:2303.17110v1 [cs.LG])

    [http://arxiv.org/abs/2303.17110](http://arxiv.org/abs/2303.17110)

    本文研究了带有概率触发臂的情境组合赌博机，在不同条件下设计了C$^2$-UCB-T算法和VAC$^2$-UCB算法，并分别导出了对应的遗憾值上限，为相关应用提供了理论支持。

    

    本研究探讨了在捕捉广泛应用范围的一系列平滑条件下的带有概率触发臂的情境组合赌博机(C$^2$MAB-T)，例如情境级联赌博机和情境最大化赌博机。在模拟触发概率(TPM)的条件下，我们设计了C$^2$-UCB-T算法，并提出了一种新的分析方法，实现了一个$\tilde{O}(d\sqrt{KT})$的遗憾值上限，消除了一个可能指数级增长的因子$O(1/p_{\min})$，其中$d$是情境的维数，$p_{\min}$是能被触发的任何臂的最小正概率，批大小$K$是每轮能被触发的臂的最大数量。在方差调制(VM)或触发概率和方差调制(TPVM)条件下，我们提出了一种新的方差自适应算法VAC$^2$-UCB，并导出了一个$\tilde{O}(d\sqrt{T})$的遗憾值上限，该上限与批大小$K$无关。作为一个有价值的副产品，我们发现我们的一个...

    We study contextual combinatorial bandits with probabilistically triggered arms (C$^2$MAB-T) under a variety of smoothness conditions that capture a wide range of applications, such as contextual cascading bandits and contextual influence maximization bandits. Under the triggering probability modulated (TPM) condition, we devise the C$^2$-UCB-T algorithm and propose a novel analysis that achieves an $\tilde{O}(d\sqrt{KT})$ regret bound, removing a potentially exponentially large factor $O(1/p_{\min})$, where $d$ is the dimension of contexts, $p_{\min}$ is the minimum positive probability that any arm can be triggered, and batch-size $K$ is the maximum number of arms that can be triggered per round. Under the variance modulated (VM) or triggering probability and variance modulated (TPVM) conditions, we propose a new variance-adaptive algorithm VAC$^2$-UCB and derive a regret bound $\tilde{O}(d\sqrt{T})$, which is independent of the batch-size $K$. As a valuable by-product, we find our a
    
[^82]: 质量多样性变形器：基于决策Transformer生成行为条件下的轨迹

    The Quality-Diversity Transformer: Generating Behavior-Conditioned Trajectories with Decision Transformers. (arXiv:2303.16207v1 [cs.NE])

    [http://arxiv.org/abs/2303.16207](http://arxiv.org/abs/2303.16207)

    该论文提出了一种基于质量多样性算法和 Transformer 的方法，通过两种机制以实现质量一致的生成行为条件下的轨迹。

    

    在神经进化计算的背景下，质量多样性算法通过依赖行为空间的定义来生成各种不同和高效的策略集合，取得了很好的效果。然而，在不确定的环境中，会有两个问题出现。第一，策略可能缺乏鲁棒性和可重复性，即在略微不同的情况下，多个 episodes 往往会导致非常不同的行为结果。第二，由于策略集的离散性，解决方案的变化是不连续的。本文提出了一种新的方法来实现基于行为条件下的轨迹生成，其基于两个机制：首先是 MAP-Elites Low-Spread (ME-LS)，它限制了选择那些在行为空间上最一致的解决方案。其次是质量多样性变形器 (QDT)，它是基于 Transformer 的。

    In the context of neuroevolution, Quality-Diversity algorithms have proven effective in generating repertoires of diverse and efficient policies by relying on the definition of a behavior space. A natural goal induced by the creation of such a repertoire is trying to achieve behaviors on demand, which can be done by running the corresponding policy from the repertoire. However, in uncertain environments, two problems arise. First, policies can lack robustness and repeatability, meaning that multiple episodes under slightly different conditions often result in very different behaviors. Second, due to the discrete nature of the repertoire, solutions vary discontinuously. Here we present a new approach to achieve behavior-conditioned trajectory generation based on two mechanisms: First, MAP-Elites Low-Spread (ME-LS), which constrains the selection of solutions to those that are the most consistent in the behavior space. Second, the Quality-Diversity Transformer (QDT), a Transformer-based 
    
[^83]: LLaMA-Adapter: 零初始化注意力下的语言模型精细调整的高效方法

    LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v1 [cs.CV])

    [http://arxiv.org/abs/2303.16199](http://arxiv.org/abs/2303.16199)

    本文提出了一种基于适应提示和零初始化注意力机制的轻量级语言模型调整方法，可高效微调LLaMA为指令跟随模型，具有比Alpaca更短的微调时间并具有近似的响应质量。

    

    本文提出了LLaMA-Adapter这一轻量级适应方法，用于将LLaMA高效地微调为一个指令跟随模型。利用52K个自我指导示范，LLaMA-Adapter仅在冻结的LLaMA 7B模型上引入了1.2M个可学习参数，并且在8个A100 GPU上仅耗时不到一个小时进行微调。具体而言，我们采用一组可学习的适应提示，并在较高的变压器层中将它们预置于输入文本令牌之前。然后，提出了一种零初始化注意力机制和零门控机制，该机制可以自适应地将新的指令提示注入LLaMA，并有效地保留了其预先训练的知识。通过高效训练，LLaMA-Adapter能够产生高质量的响应，与完全微调的7B参数的Alpaca相似。此外，我们的方法还可以简单地扩展到多模态输入，例如图像，用于图像相关的LLaMA，在ScienceQA上实现了更强的推理能力。我们在https://github.com/ZrrSkywalker/LLaMA-Adapt发布了我们的代码。

    We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapt
    
[^84]: 基于MAP-Elites的多样化强化学习智能体群体进化

    Evolving Populations of Diverse RL Agents with MAP-Elites. (arXiv:2303.12803v1 [cs.NE])

    [http://arxiv.org/abs/2303.12803](http://arxiv.org/abs/2303.12803)

    本文介绍了一种尝试在进化计算与强化学习中相结合解决机器人控制问题的方法，并通过改进ME算法提高了效率和多样性，但也出现了一些常见强化学习算法的限制。

    

    品质多样性(QD)已成为一种强大的替代优化模式，旨在生成大量和多样的解决方案，其代表算法MAP-Elites(ME)通过变异和交叉进化解决方案。虽然ME对于某些非结构化问题非常有效，但早期的ME实现仅依赖于随机搜索来进化解决方案的种群，因此在高维问题中如进化神经网络时常常无法有效地进行，效率极低。后续的研究考虑利用梯度信息通过黑盒优化（BBO）或强化学习（RL）中的技术来引导搜索，以解决这些问题。将RL技巧与ME结合在机器人控制问题中实现了最先进的性能，但是也在ME变体中引入了这些算法的共同限制，例如对探索需求的要求。

    Quality Diversity (QD) has emerged as a powerful alternative optimization paradigm that aims at generating large and diverse collections of solutions, notably with its flagship algorithm MAP-ELITES (ME) which evolves solutions through mutations and crossovers. While very effective for some unstructured problems, early ME implementations relied exclusively on random search to evolve the population of solutions, rendering them notoriously sample-inefficient for high-dimensional problems, such as when evolving neural networks. Follow-up works considered exploiting gradient information to guide the search in order to address these shortcomings through techniques borrowed from either Black-Box Optimization (BBO) or Reinforcement Learning (RL). While mixing RL techniques with ME unlocked state-of-the-art performance for robotics control problems that require a good amount of exploration, it also plagued these ME variants with limitations common among RL algorithms that ME was free of, such a
    
[^85]: 具有别名观测的潜在图的快速探索与学习

    Fast exploration and learning of latent graphs with aliased observations. (arXiv:2303.07397v1 [cs.LG])

    [http://arxiv.org/abs/2303.07397](http://arxiv.org/abs/2303.07397)

    本文介绍了一种在具有别名观测的潜在图上，能够显著提高最大化探索效率的政策算法 eFeX，相比于随机策略，该算法能够更快地恢复各种拓扑结构下的图表。

    

    考虑这种场景：一个智能体通过执行操作从一个节点到另一个节点来导航潜在图。所选操作确定了下一个访问节点上的概率分布。在每个节点处，智能体收到一个观测，但该观测不是唯一的，因此它不能唯一地标识节点，这使得问题别名化。本文旨在提供一个政策，该政策约等于最大化探索效率（即在给定的探索预算下如何恢复图表）。在非别名化的情况下，我们展示了相对于现有最先进强化学习基线的改进性能。对于别名化的情况，我们不知道适用的基线，而是展示了在各种拓扑结构下相对于随机策略更快的恢复速度，并且对于具有挑战性的拓扑结构，恢复速度比随机策略快指数倍。我们将该算法称为 eFeX（来自于 efficient exploration 的缩写）。

    Consider this scenario: an agent navigates a latent graph by performing actions that take it from one node to another. The chosen action determines the probability distribution over the next visited node. At each node, the agent receives an observation, but this observation is not unique, so it does not identify the node, making the problem aliased. The purpose of this work is to provide a policy that approximately maximizes exploration efficiency (i.e., how well the graph is recovered for a given exploration budget). In the unaliased case, we show improved performance w.r.t. state-of-the-art reinforcement learning baselines. For the aliased case we are not aware of suitable baselines and instead show faster recovery w.r.t. a random policy for a wide variety of topologies, and exponentially faster recovery than a random policy for challenging topologies. We dub the algorithm eFeX (from eFficient eXploration).
    
[^86]: 基于人类指令拒绝图像再创作的文本-视觉对话可追溯技术

    Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation. (arXiv:2303.05983v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.05983](http://arxiv.org/abs/2303.05983)

    本论文构建了两个多模态数据集来验证视觉语言模型在文本-视觉对话任务中的能力，并开发特定规则的监督信号来让系统展示可追溯性。

    

    ChatGPT和GPT-4的成功引起了对多模态对话系统的广泛关注，但缺乏可以验证视觉语言模型在文本-视觉对话任务中多模态生成能力的数据集。为此，本文构建了两个新的多模态数据集：合成CLEVR-ATVC数据集（620K）和手动绘制的Fruit-ATVC数据集（50K），均具有基于视觉和文本的输入和输出。为了让多模态系统能够拒绝人类请求（即展示可追溯性），我们在数据集中开发和并入了特定规则作为监督信号。这允许训练后的VLM在视觉和文本推理后提供yes或no的答案，并附带说明语言为什么无法执行人类指令。我们提出了一个两阶段训练程序来训练图像自编码器和自回归神经网络。

    The recent success of ChatGPT and GPT-4 has drawn widespread attention to multimodal dialogue systems. However, the academia community lacks a dataset that can validate the multimodal generation capabilities of Visual Language Models (VLMs) in textual-visual chat tasks. In this paper, we construct two new multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K), both featuring visual and text-based inputs and outputs. Additionally, to enable the multimodal system to reject human requests (i.e., demonstrate accountability), as in language-based ChatGPT conversations, we develop and incorporate specific rules into the datasets as supervisory signals. This allows the trained VLM to provide a yes or no answer after visual and textual reasoning, accompanied by a language explanation as to why the human instruction cannot be excuted. In our method, we propose a two-state training procedure to train the image auto-encoder and auto-regress
    
[^87]: 计算水平分析通用智能的约束遵从性

    Computational-level Analysis of Constraint Compliance for General Intelligence. (arXiv:2303.04352v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.04352](http://arxiv.org/abs/2303.04352)

    论文研究了人类行为受约束规范约束的影响，同时针对于构建遵守这些约束的通用代理提出了基于贝叶斯决策理论的计算水平模型。

    

    人类行为受到限制行为的规范和规律的制约。规则、礼仪、法律和道德义务等约束行为的形式都是对人类行为进行规范的类别。这些约束系统是“复杂的”：个体约束通常定义不明确，特定情况下哪些约束是相关的可能未知或存在歧义，约束之间相互作用和冲突，确定如何在相关约束的范围内行事可能是一个重大挑战，尤其是在需要快速决策的情况下。尽管存在这样的混乱，人类仍然能够稳健、快速地将约束融入其决策中。通用的人工智能代理也必须能够在现实约束系统的混乱中进行导航，以便行为具有可预测性和可靠性。在本文中，我们对通用代理约束处理的复杂性来源进行了表征，并基于贝叶斯决策理论的思想描述了一种针对这种约束计算水平分析的方法。我们列举了几个建设通用的、遵守约束的代理的障碍，并描述了代理如何可以以最优方式考虑约束的贝叶斯计算水平模型。最后，我们讨论了我们的模型对通用智能研究的影响。

    Human behavior is conditioned by codes and norms that constrain action. Rules, ``manners,'' laws, and moral imperatives are examples of classes of constraints that govern human behavior. These systems of constraints are ``messy:'' individual constraints are often poorly defined, what constraints are relevant in a particular situation may be unknown or ambiguous, constraints interact and conflict with one another, and determining how to act within the bounds of the relevant constraints may be a significant challenge, especially when rapid decisions are needed. Despite such messiness, humans incorporate constraints in their decisions robustly and rapidly. General, artificially-intelligent agents must also be able to navigate the messiness of systems of real-world constraints in order to behave predictability and reliably. In this paper, we characterize sources of complexity in constraint processing for general agents and describe a computational-level analysis for such \textit{constraint
    
[^88]: iSAGE：一种基于增量学习的SAGE在线解释方法

    iSAGE: An Incremental Version of SAGE for Online Explanation on Data Streams. (arXiv:2303.01181v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01181](http://arxiv.org/abs/2303.01181)

    iSAGE是一种基于增量学习的SAGE在线解释方法，具备快速、内存高效的特点。该方法能够对模型变化以及数据生成过程中的漂移进行反应，同时提供了有效的特征移除方法，具有和SAGE类似的理论性质。

    

    现有的可解释人工智能（XAI）方法，包括SAGE等流行的特征重要性测量，大多限于批量学习场景。然而，机器学习通常应用于动态环境中，数据持续到达，必须以在线方式进行学习。因此，我们提出了iSAGE，一种快速、内存高效的SAGE增量方法，它能够对模型的变化以及数据生成过程中的漂移进行反应。我们提供了有效的特征移除方法，破坏（干预）和保留（观测）特征之间的依赖关系。此外，我们正式分析了我们的解释方法，展示了iSAGE与SAGE具有类似的理论性质。最后，我们基于广泛使用的数据集和具有概念漂移的数据流对我们的方法进行了彻底的实验分析。

    Existing methods for explainable artificial intelligence (XAI), including popular feature importance measures such as SAGE, are mostly restricted to the batch learning scenario. However, machine learning is often applied in dynamic environments, where data arrives continuously and learning must be done in an online manner. Therefore, we propose iSAGE, a time- and memory-efficient incrementalization of SAGE, which is able to react to changes in the model as well as to drift in the data-generating process. We further provide efficient feature removal methods that break (interventional) and retain (observational) feature dependencies. Moreover, we formally analyze our explanation method to show that iSAGE adheres to similar theoretical properties as SAGE. Finally, we evaluate our approach in a thorough experimental analysis based on well-established data sets and data streams with concept drift.
    
[^89]: 重访MADDPG中的Gumbel-Softmax

    Revisiting the Gumbel-Softmax in MADDPG. (arXiv:2302.11793v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11793](http://arxiv.org/abs/2302.11793)

    本文探索了多种Gumbel-Softmax的替代方法，并将其应用于MADDPG中，以解决离散动作空间下的性能问题。

    

    MADDPG是一种适用于多智能体强化学习的算法，它将单智能体方法DDPG推广到多智能体场景中。重要的是，DDPG是一种针对连续动作空间设计的算法，在其中状态-动作价值函数的梯度存在。为了使该算法适用于离散动作空间，必须进行离散的梯度估计。对于MADDPG算法，使用了Gumbel-Softmax（GS）估算器--一种将离散分布松弛到类似连续分布的再参数化方法。然而，该方法具有统计偏差，最近的多智能体强化学习算法基准测试论文表明，这种偏差使得MADDPG在格子世界等离散动作空间下表现不佳。幸运的是，GS的许多替代方法存在，具有各种各样的性能。本文探讨了其中几种替代方法，并将它们整合到离散格子世界场景中的MADDPG中。然后对各种性能指标的相应影响进行了测量。

    MADDPG is an algorithm in multi-agent reinforcement learning (MARL) that extends the popular single-agent method, DDPG, to multi-agent scenarios. Importantly, DDPG is an algorithm designed for continuous action spaces, where the gradient of the state-action value function exists. For this algorithm to work in discrete action spaces, discrete gradient estimation must be performed. For MADDPG, the Gumbel-Softmax (GS) estimator is used -- a reparameterisation which relaxes a discrete distribution into a similar continuous one. This method, however, is statistically biased, and a recent MARL benchmarking paper suggests that this bias makes MADDPG perform poorly in grid-world situations, where the action space is discrete. Fortunately, many alternatives to the GS exist, boasting a wide range of properties. This paper explores several of these alternatives and integrates them into MADDPG for discrete grid-world scenarios. The corresponding impact on various performance metrics is then measur
    
[^90]: 条件变分自编码器学习流形维度

    Learning Manifold Dimensions with Conditional Variational Autoencoders. (arXiv:2302.11756v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11756](http://arxiv.org/abs/2302.11756)

    该论文证明全局最优变分自编码器(CVAE)可以学习正确的流形维度，同时提出了一种新方法可以共同学习流形维度和条件分布，以在多个数据集上实现更好的特征分离和样本质量。

    

    虽然变分自编码器（VAE）及其条件扩展（CVAE）在多个领域中能够实现最先进的结果，但它们的精确行为仍未完全理解，特别是在数据（如图像）在或接近低维流形上的情况下。我们证明了VAE全局最小值确实能够恢复正确的流形维度，并通过引入一种新方法来共同学习流形维度和条件分布，进一步扩展了这一结果到更一般的CVAEs。我们在各种数据集上证明了我们方法的有效性，包括MNIST和CelebA，实现了表现最好的视觉质量和特征分离。

    Although the variational autoencoder (VAE) and its conditional extension (CVAE) are capable of state-of-the-art results across multiple domains, their precise behavior is still not fully understood, particularly in the context of data (like images) that lie on or near a low-dimensional manifold. For example, while prior work has suggested that the globally optimal VAE solution can learn the correct manifold dimension, a necessary (but not sufficient) condition for producing samples from the true data distribution, this has never been rigorously proven. Moreover, it remains unclear how such considerations would change when various types of conditioning variables are introduced, or when the data support is extended to a union of manifolds (e.g., as is likely the case for MNIST digits and related). In this work, we address these points by first proving that VAE global minima are indeed capable of recovering the correct manifold dimension. We then extend this result to more general CVAEs, 
    
[^91]: Android恶意软件检测的持续学习方法

    Continuous Learning for Android Malware Detection. (arXiv:2302.04332v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.04332](http://arxiv.org/abs/2302.04332)

    本文提出了一种使用对比学习和主动学习相结合的新方法，用于解决Android恶意软件分类器中的概念漂移问题。

    

    机器学习方法可以以非常高的精度检测Android恶意软件。然而，这些分类器有一个弱点，概念漂移：随着恶意软件应用和良性应用的不断发展，它们很快会过时并失效。本研究发现，在使用一年的数据训练Android恶意软件分类器后，F1得分在新的测试样本上落后于6个月后的0.99至0.76。在本文中，我们提出了新的方法来解决Android恶意软件分类器中的概念漂移问题。使用主动学习，我们不断地选择新的样本给分析员标注，并将标注的样本添加到训练集中重新训练分类器来持续更新。我们的关键思想是，基于相似度的不确定性更具有鲁棒性。因此，我们将对比学习与主动学习相结合。我们提出了一种新的分层对比学习方案和一种新的样本选择方法。

    Machine learning methods can detect Android malware with very high accuracy. However, these classifiers have an Achilles heel, concept drift: they rapidly become out of date and ineffective, due to the evolution of malware apps and benign apps. Our research finds that, after training an Android malware classifier on one year's worth of data, the F1 score quickly dropped from 0.99 to 0.76 after 6 months of deployment on new test samples.  In this paper, we propose new methods to combat the concept drift problem of Android malware classifiers. Since machine learning technique needs to be continuously deployed, we use active learning: we select new samples for analysts to label, and then add the labeled samples to the training set to retrain the classifier. Our key idea is, similarity-based uncertainty is more robust against concept drift. Therefore, we combine contrastive learning with active learning. We propose a new hierarchical contrastive learning scheme, and a new sample selection 
    
[^92]: Mnemosyne: 使用Transformers来训练Transformers

    Mnemosyne: Learning to Train Transformers with Transformers. (arXiv:2302.01128v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01128](http://arxiv.org/abs/2302.01128)

    Mnemosyne优化器使用Performers方法来学习训练整个神经网络架构，包括其他Transformers，而无需针对特定任务进行优化器调节，并成功训练ViTs和应用于机器人领域中，具有更好的泛化能力与快速收敛。

    

    训练复杂的机器学习(ML)架构需要耗费大量计算和时间来选择合适的优化器并调节其超参数。从数据中学习优化器的新学习范式已经成为手动设计ML优化器的更好选择。我们提出了Mnemosyne优化器，它使用Performers: 隐式低秩attention Transformers。它可以学习训练整个神经网络架构，包括其他Transformers，而无需针对特定任务进行优化器调节。我们展示了Mnemosyne：(a)比流行的LSTM优化器具有更好的泛化能力；(b)特别地，可以在标准MLPs上进行元训练后成功地训练Vision Transformers(ViTs) (c)可以初始化优化器以实现机器人应用中更快的收敛。我们相信这些结果开启了使用Transformers构建基础优化模型的可能性，可以应对常规的Transformer训练挑战。我们通过广泛的理论分析来补充我们的结果。

    Training complex machine learning (ML) architectures requires a compute and time consuming process of selecting the right optimizer and tuning its hyper-parameters. A new paradigm of learning optimizers from data has emerged as a better alternative to hand-designed ML optimizers. We propose Mnemosyne optimizer, that uses Performers: implicit low-rank attention Transformers. It can learn to train entire neural network architectures including other Transformers without any task-specific optimizer tuning. We show that Mnemosyne: (a) generalizes better than popular LSTM optimizer, (b) in particular can successfully train Vision Transformers (ViTs) while meta--trained on standard MLPs and (c) can initialize optimizers for faster convergence in Robotics applications. We believe that these results open the possibility of using Transformers to build foundational optimization models that can address the challenges of regular Transformer training. We complement our results with an extensive theo
    
[^93]: 将语言模型与图像进行联系以处理多模态信息

    Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.13823](http://arxiv.org/abs/2301.13823)

    该论文提出一种有效的方法，将仅处理文本的语言模型与图像进行联系，使其能够处理任意交错的图像和文本数据，并生成与检索图像交错的自由形式文本。该方法在环境相关的图像检索和多模态对话等任务中表现十分优异，是利用预训练语言模型解决视觉场景下交互问题的有效解决方案。

    

    我们提出了一种有效的方法，将预训练的仅文本语言模型与视觉领域联系起来，使其能够处理任意交错的图像和文本数据，并生成与检索图像交错的文本。我们利用从大规模文本预训练中学到的语言模型的能力，例如上下文学习和自由形式文本生成。我们保持语言模型冻结，并微调输入和输出线性层以实现跨模态交互。这使得我们的模型能够处理任意交错的图像和文本输入，并生成与检索图像交错的自由形式文本。我们在环境相关的图像检索和多模态对话等任务中取得了强大的零-shot表现，并展示了引人入胜的交互能力。我们的方法适用于任何现成的语言模型，为在视觉场景下利用预训练语言模型提供了一个有效且通用的解决方案。

    We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
    
[^94]: 将互联网规模的视觉语言模型转化为具体代理

    Distilling Internet-Scale Vision-Language Models into Embodied Agents. (arXiv:2301.12507v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.12507](http://arxiv.org/abs/2301.12507)

    该研究提出了使用预训练的视觉语言模型(VLMs)来监督具体代理，将互联网规模的VLMs的通用语言接地技能重新利用到具体代理的行动中。

    

    指令跟踪代理必须将语言接地到其观察和操作空间中。学习接地语言具有挑战性，通常需要特定领域的工程或大量人类交互数据。为解决这个问题，我们建议使用预训练的视觉语言模型 (VLMs) 来监督具体代理。我们结合了模型蒸馏和事后经验回放 (HER) 的想法，使用 VLM 来追溯性地生成描述代理行为的语言。简单的提示允许我们控制监督信号，教代理根据其名称 (例如，飞机) 或其特征 (例如，颜色) 与新型物体交互在3D渲染环境中。Fewshot提示使我们可以教授抽象的类别成员资格，包括现有类别（食品 vs 玩具）和自发类别（对对象的任意偏好）。我们的工作概述了一种新的有效方法，使用互联网规模的VLMs，重新利用它们从大量文本数据中学习的通用语言接地技能，将语言接地到具体代理的行动中。

    Instruction-following agents must ground language into their observation and action spaces. Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data. To address this challenge, we propose using pretrained vision-language models (VLMs) to supervise embodied agents. We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior. Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment. Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects). Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquir
    
[^95]: 理解人类直觉对人工智能决策中依赖的作用与解释

    Understanding the Role of Human Intuition on Reliance in Human-AI Decision-Making with Explanations. (arXiv:2301.07255v3 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2301.07255](http://arxiv.org/abs/2301.07255)

    本文研究了人类直觉对人工智能决策中的依赖作用和解释，在采用特征和基于示例的两种解释类型进行两个预测任务的思考实验中确认了三种直觉类型：关于AI正确性的直觉、基于对任务和AI模型的预期的直觉以及基于个人价值观和目标的直觉。

    

    AI解释经常被提及作为提高人工智能决策的方式，但实证研究没有找到解释的有效性一致的证据，反而表明它们会在AI系统出现错误时增加过度依赖。虽然许多因素可能影响对AI支持的依赖，但一个重要因素是决策者如何协调自己的直觉——基于先前知识、经验或模式识别的信念或启发式方法，用于进行判断——与AI系统提供的信息相结合，以确定何时覆盖AI预测。我们进行了混合方法的思考实验，采用两种解释类型（特征和基于示例的）进行两个预测任务，以探索决策者的直觉如何影响他们对AI预测和解释的使用，以及最终他们选择何时依赖AI的决定。我们的结果确定了三种涉及推理AI预测和解释的直觉类型：关于AI正确性的直觉、基于对任务和AI模型的预期的直觉以及基于个人价值观和目标的直觉。

    AI explanations are often mentioned as a way to improve human-AI decision-making, but empirical studies have not found consistent evidence of explanations' effectiveness and, on the contrary, suggest that they can increase overreliance when the AI system is wrong. While many factors may affect reliance on AI support, one important factor is how decision-makers reconcile their own intuition -- beliefs or heuristics, based on prior knowledge, experience, or pattern recognition, used to make judgments -- with the information provided by the AI system to determine when to override AI predictions. We conduct a think-aloud, mixed-methods study with two explanation types (feature- and example-based) for two prediction tasks to explore how decision-makers' intuition affects their use of AI predictions and explanations, and ultimately their choice of when to rely on AI. Our results identify three types of intuition involved in reasoning about AI predictions and explanations: intuition about the
    
[^96]: 手术聚合：一种用于协同学习的分布式医学影像数据和多样任务协调框架

    Surgical Aggregation: A Collaborative Learning Framework for Harmonizing Distributed Medical Imaging Datasets with Diverse Tasks. (arXiv:2301.06683v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.06683](http://arxiv.org/abs/2301.06683)

    本论文提出了一种手术聚合的协同学习框架，该框架可用于协调和聚合分布式医学影像数据集的知识，并带有部分疾病注释，从而可训练具有完整胸部内可能出现的所有异常的临床实用、强大模型。

    

    大规模的胸部X光数据集已经通过深度学习进行异常检测，并有潜力为许多临床应用提供巨大的益处。然而，每个数据集仅专注于检测患者可能同时出现的一部分发现，从而限制了其临床效用。因此，数据协调对于聚合这些数据集来训练具有完整胸部内可能出现的所有异常的临床实用、强大模型至关重要。为此，我们提出了手术聚合，一种协同学习框架，用于协调和聚合分布式异构数据集的知识，并带有部分疾病注释。我们在合成的iid数据集和具有部分注释的真实大规模非iid数据集上评估了手术聚合。我们的结果表明，手术聚合显著优于当前的策略，具有更好的通用性。

    Large-scale chest x-ray datasets have been curated for the detection of abnormalities using deep learning, with the potential to provide substantial benefits across many clinical applications. However, each dataset focuses only on detecting a subset of findings that can be simultaneously present in a patient, thereby limiting its clinical utility. Therefore, data harmonization is crucial to leverage these datasets in aggregate to train clinically-useful, robust models with a complete representation of all abnormalities that may occur within the thorax. To that end, we propose surgical aggregation, a collaborative learning framework for harmonizing and aggregating knowledge from distributed heterogeneous datasets with partial disease annotations. We evaluate surgical aggregation across synthetic iid datasets and real-world large-scale non-iid datasets with partial annotations. Our results indicate that surgical aggregation significantly outperforms current strategies, has better general
    
[^97]: 一种AI模型解释性的理论框架及其在生物医学领域的应用

    A Theoretical Framework for AI Models Explainability with Application in Biomedicine. (arXiv:2212.14447v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.14447](http://arxiv.org/abs/2212.14447)

    该论文提出了一种新的解释定义和理论框架，用于评估AI模型的可解释性，特别是在生物医学领域。这个框架可帮助确定模型决策中最具影响力的输入特征，支持对模型行为的解释。

    

    可解释性人工智能（XAI）是人工智能社区中一个充满活力的研究课题，受到不同方法和领域的越来越多的关注。虽然有很多关于该主题的文章，但XAI仍缺乏共享的术语和框架，无法为解释提供结构上的完整性。在我们的工作中，我们通过提出一个新的解释定义来解决这些问题，该定义是文献中可以找到的综合体。我们认为，解释不是原子性的，而是来自于模型和其输入输出映射的证据，以及人类对这些证据的解释组合而成。此外，我们将解释纳入到真实性（即解释是否是模型内部工作和决策过程的真实描述）和可信度（即解释对用户的说服力）的特性中。使用我们提出的理论框架简化了这些特性的操作，并提供了评估AI模型可解释性的综合方法。我们将该框架应用于生物医学领域，并展示其使得我们能够确定模型决策中最具影响力的输入特征，支持对模型行为的解释。我们相信，我们的贡献将促进可解释性工具和方法的发展，从而推进AI算法在关键领域的可信部署。

    EXplainable Artificial Intelligence (XAI) is a vibrant research topic in the artificial intelligence community, with growing interest across methods and domains. Much has been written about the subject, yet XAI still lacks shared terminology and a framework capable of providing structural soundness to explanations. In our work, we address these issues by proposing a novel definition of explanation that is a synthesis of what can be found in the literature. We recognize that explanations are not atomic but the combination of evidence stemming from the model and its input-output mapping, and the human interpretation of this evidence. Furthermore, we fit explanations into the properties of faithfulness (i.e., the explanation being a true description of the model's inner workings and decision-making process) and plausibility (i.e., how much the explanation looks convincing to the user). Using our proposed theoretical framework simplifies how these properties are operationalized and it prov
    
[^98]: 通过详细的大纲控制提升长篇故事连贯性

    DOC: Improving Long Story Coherence With Detailed Outline Control. (arXiv:2212.10077v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10077](http://arxiv.org/abs/2212.10077)

    该论文提出了一个名为 Detailed Outline Control(DOC) 的框架，通过详细大纲和详细控制器来提高生成长篇故事时的情节连贯性和大纲相关性，人类评估证实该方法在这些方面显著优于基线方法，并且更适用于交互生成设置。

    

    我们提出了一个称为 Detailed Outline Control(DOC)的框架，以提高生成数千字长的故事时的长程情节连贯性。DOC由两个互补组件组成：详细大纲和详细控制器。详细大纲创建一个更详细、层次化的大纲，将创造性负担从主要起草过程转移到规划阶段。详细控制器通过控制故事段落与大纲细节对齐，确保更详细的大纲在生成过程中仍然被尊重。在自动生成的故事的人类评估中，DOC在情节连贯性(22.5% 绝对增益)、大纲相关性(28.2%)和趣味性(20.7%)方面显著优于强大的Re3基线(Yang等人，2022)。人们还评价DOC在交互生成设置方面更易于控制。

    We propose the Detailed Outline Control (DOC) framework for improving long-range plot coherence when automatically generating several-thousand-word-long stories. DOC consists of two complementary components: a detailed outliner and a detailed controller. The detailed outliner creates a more detailed, hierarchically structured outline, shifting creative burden from the main drafting procedure to the planning stage. The detailed controller ensures the more detailed outline is still respected during generation by controlling story passages to align with outline details. In human evaluations of automatically generated stories, DOC substantially outperforms a strong Re3 baseline (Yang et al., 2022) on plot coherence (22.5% absolute gain), outline relevance (28.2%), and interestingness (20.7%). Humans also judged DOC to be much more controllable in an interactive generation setting.
    
[^99]: 马尔可夫决策过程的因果时间推理

    Causal Temporal Reasoning for Markov Decision Processes. (arXiv:2212.08712v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.08712](http://arxiv.org/abs/2212.08712)

    本文介绍了一种新的概率时间逻辑PCFTL，第一个包含因果推理运算符的概率时间逻辑，可推理干预和反事实查询，从而能够推理涉及MDP不同配置的“假设”场景

    

    本文引入了一种新的概率时间逻辑PCFTL，用于验证马尔可夫决策过程(MDP)。 PCFTL是第一个包含因果推理运算符的概率时间逻辑，使我们能够表达干预和反事实查询。我们提出了一种广义的反事实运算符，既包括干预，也包括反事实。我们的方法不同于现有的仅能推理固定系统配置的概率时间逻辑，因为它能够推理涉及MDP不同配置的“假设”场景。

    We introduce $\textit{PCFTL (Probabilistic CounterFactual Temporal Logic)}$, a new probabilistic temporal logic for the verification of Markov Decision Processes (MDP). PCFTL is the first to include operators for causal reasoning, allowing us to express interventional and counterfactual queries. Given a path formula $\phi$, an interventional property is concerned with the satisfaction probability of $\phi$ if we apply a particular change $I$ to the MDP (e.g., switching to a different policy); a counterfactual allows us to compute, given an observed MDP path $\tau$, what the outcome of $\phi$ would have been had we applied $I$ in the past. For its ability to reason about \textit{what-if} scenarios involving different configurations of the MDP, our approach represents a departure from existing probabilistic temporal logics that can only reason about a fixed system configuration. From a syntactic viewpoint, we introduce a generalized counterfactual operator that subsumes both intervention
    
[^100]: 因果图中前门调整的线性时间算法

    Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs. (arXiv:2211.16468v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.16468](http://arxiv.org/abs/2211.16468)

    在因果图中，提出了解决前门调整的线性时间算法，通过观察到的中介变量，即使存在未观测到的混淆，也可以识别因果效应。

    

    从观测数据中估计因果效应是实证科学中的基本任务。当系统中涉及未观察到的混淆因素时，这变得尤为具有挑战性。本文侧重于前门调整——一种经典技术，它使用观察到的中介变量，即使存在未观测到的混淆，也可以识别因果效应。虽然前门估计的统计特性已经很好地理解了，但它的算法方面长期以来一直未得到探究。最近，Jeong，Tian和Barenboim [NeurIPS 2022]提出了一种第一个多项式时间算法，用于在给定的有向无环图（DAG）中找到满足前门准则的集合，其运行时间为$O（n^3（n+m））$，其中$n$表示变量的数量，$m$表示因果图的边的数量。在我们的工作中，我们提供了第一个具有线性时间复杂度的算法，即$O（n+m）$，用于这项任务，从而达到了渐近最优的时间复杂性。

    Causal effect estimation from observational data is a fundamental task in empirical sciences. It becomes particularly challenging when unobserved confounders are involved in a system. This paper focuses on front-door adjustment -- a classic technique which, using observed mediators allows to identify causal effects even in the presence of unobserved confounding. While the statistical properties of the front-door estimation are quite well understood, its algorithmic aspects remained unexplored for a long time. Recently, Jeong, Tian, and Barenboim [NeurIPS 2022] have presented the first polynomial-time algorithm for finding sets satisfying the front-door criterion in a given directed acyclic graph (DAG), with an $O(n^3(n+m))$ run time, where $n$ denotes the number of variables and $m$ the number of edges of the causal graph. In our work, we give the first linear-time, i.e., $O(n+m)$, algorithm for this task, which thus reaches the asymptotically optimal time complexity. This result impli
    
[^101]: 对抗性廉价交流

    Adversarial Cheap Talk. (arXiv:2211.11030v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11030](http://arxiv.org/abs/2211.11030)

    本文提出了一种新型对抗性设置，在其中对手只能将信息附加到受害者的观察中，从而产生最小的影响范围，并提出对抗性廉价交流（ACT）算法进行对手训练。在高度受限的情况下，使用ACT训练的对手仍会对受害者的训练和测试表现产生显著影响，揭示了强化学习算法中的一种新的攻击向量。

    

    强化学习中的对抗性攻击通常假定攻击者可以高度特权地访问受害者的参数、环境或数据。本文提出了一种称为廉价交流MDP的新型对抗性设置，其中对手只能将确定性信息附加到受害者的观察中，从而产生最小的影响范围。对手不能掩盖地面事实，影响基本环境动态或奖励信号，引入不稳定性，增加随机性，看到受害者的动作或访问他们的参数。此外，我们提出了一种简单的元学习算法，称为对抗性廉价交流（ACT），在这种设置中对对手进行训练。我们证明，即使在高度受限的情况下，使用ACT训练的对手仍会显着影响受害者的训练和测试表现。影响训练时间表现揭示了一种新的攻击向量，并为现有强化学习算法的成功和失败模式提供了见解。

    Adversarial attacks in reinforcement learning (RL) often assume highly-privileged access to the victim's parameters, environment, or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk MDP in which an Adversary can merely append deterministic messages to the Victim's observation, resulting in a minimal range of influence. The Adversary cannot occlude ground truth, influence underlying environment dynamics or reward signals, introduce non-stationarity, add stochasticity, see the Victim's actions, or access their parameters. Additionally, we present a simple meta-learning algorithm called Adversarial Cheap Talk (ACT) to train Adversaries in this setting. We demonstrate that an Adversary trained with ACT still significantly influences the Victim's training and testing performance, despite the highly constrained setting. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing RL algorith
    
[^102]: HiveNAS: 采用人工蜂群优化的神经架构搜索

    HiveNAS: Neural Architecture Search using Artificial Bee Colony Optimization. (arXiv:2211.10250v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2211.10250](http://arxiv.org/abs/2211.10250)

    本文提出了一种采用人工蜂群优化的神经架构搜索框架HiveNAS，它在极短的时间内就能超越其他基于群智的NAS框架，成为最优。

    

    传统的神经网络开发过程需要大量专业知识，并且严重依赖于直觉和试错。神经架构搜索（NAS）框架被引入来稳健地搜索网络拓扑，并促进神经网络的自动开发。在NAS上，虽然一些优化方法（如遗传算法）已经被广泛探究，但其他元启发式优化算法尚未被研究。在本研究中，我们评估了采用人工蜂群优化进行神经架构搜索的可行性。我们提出的框架HiveNAS在一小部分时间内就超越了现有的基于群智的NAS框架，成为最优。

    The traditional Neural Network-development process requires substantial expert knowledge and relies heavily on intuition and trial-and-error. Neural Architecture Search (NAS) frameworks were introduced to robustly search for network topologies, as well as facilitate the automated development of Neural Networks. While some optimization approaches -- such as Genetic Algorithms -have been extensively explored in the NAS context, other Metaheuristic Optimization algorithms have not yet been investigated. In this study, we evaluate the viability of Artificial Bee Colony optimization for Neural Architecture Search. Our proposed framework, HiveNAS, outperforms existing state-of-the-art Swarm Intelligence-based NAS frameworks in a fraction of the time.
    
[^103]: ATCO2语料库：用于空中交通管制通信自动语音识别和自然语言理解研究的大规模数据集

    ATCO2 corpus: A Large-Scale Dataset for Research on Automatic Speech Recognition and Natural Language Understanding of Air Traffic Control Communications. (arXiv:2211.04054v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.04054](http://arxiv.org/abs/2211.04054)

    ATCO2语料库是一个旨在促进空中交通管制通信自动语音识别和自然语言理解研究的大规模数据集，包括数据收集和预处理、语音数据的伪注释，以及提取与空中交通管制相关的命名实体。

    

    在互联的数字世界中，个人助手、自动语音识别器和对话理解系统变得越来越重要。航空交通管制通信就是其中一个明显的例子。空中交通管制旨在以安全和最优的方式指导飞行器和控制空域。这些基于语音的对话是通过超高频无线电频道在空中交通管制员（ATCO）和飞行员之间进行的。为了将这些新技术纳入ATC（低资源领域），需要大规模的注释数据集来开发数据驱动的AI系统，其中包括自动语音识别（ASR）和自然语言理解（NLU）。在本文中，我们介绍了ATCO2语料库，这是一个旨在促进ATC领域研究的数据集，由于缺乏注释的数据，该领域滞后很多。ATCO2语料库包括1）数据收集和预处理，2）语音数据的伪注释，和3）提取与空中交通管制相关的命名实体。

    Personal assistants, automatic speech recognizers and dialogue understanding systems are becoming more critical in our interconnected digital world. A clear example is air traffic control (ATC) communications. ATC aims at guiding aircraft and controlling the airspace in a safe and optimal manner. These voice-based dialogues are carried between an air traffic controller (ATCO) and pilots via very-high frequency radio channels. In order to incorporate these novel technologies into ATC (low-resource domain), large-scale annotated datasets are required to develop the data-driven AI systems. Two examples are automatic speech recognition (ASR) and natural language understanding (NLU). In this paper, we introduce the ATCO2 corpus, a dataset that aims at fostering research on the challenging ATC field, which has lagged behind due to lack of annotated data. The ATCO2 corpus covers 1) data collection and pre-processing, 2) pseudo-annotations of speech data, and 3) extraction of ATC-related named
    
[^104]: 破碎的神经缩放定律

    Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14891](http://arxiv.org/abs/2210.14891)

    本文提出了一个平滑破碎的幂律函数形式，可以准确地模拟和外推深度神经网络的缩放行为，适用于各种架构和大量不同任务，包括视觉、语言、音频、视频、生成建模、对比学习、机器人、不确定性估计/校准、对抗鲁棒性、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.

    我们提出了一个平滑破碎的幂律函数形式（我们称之为破碎的神经缩放定律（BNSL）），它准确地模拟和外推了深度神经网络的缩放行为（即感兴趣的评估指标随用于训练的计算量、模型参数数量、训练数据集大小或上游性能变化而变化）对于各种架构和大量不同任务中的每个任务，包括大规模视觉、语言、音频、视频、扩散、生成建模、多模态学习、对比学习、AI对齐、机器人、超出分布（OOD）泛化、持续学习、不确定性估计/校准、超出分布检测、对抗鲁棒性、蒸馏、分子、计算机编程/编码、数学单词问题、算术、无监督/自监督学习和强化学习。

    We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
    
[^105]: PromptCast：一种新的基于提示的时间序列预测范式

    PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2210.08964](http://arxiv.org/abs/2210.08964)

    提出了一种新的时间序列预测范式——基于提示的时间序列预测（PromptCast），将数字输入和输出转化为提示，并以句子到句子的方式提出预测任务，可以直接应用于语言模型。

    

    本文提出了一种新的时间序列预测范式——基于提示的时间序列预测（PromptCast）。在这种新的任务中，将原来的数字输入和输出转化为提示，并以句子到句子的方式提出预测任务，使得语言模型可以直接应用于预测的目的。为了支持和促进这个任务的研究，我们还提出了一个大规模的数据集（PISA）。

    This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes th
    
[^106]: CORL: 面向研究的深度强化学习离线库

    CORL: Research-oriented Deep Offline Reinforcement Learning Library. (arXiv:2210.07105v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07105](http://arxiv.org/abs/2210.07105)

    CORL是一个面向研究的深度强化学习离线库，提供了经过充分基准测试的单文件实现离线和离线到在线强化学习算法，并具有简单的开发体验和实验跟踪功能。

    

    CORL是一个开源库，提供了经过充分基准测试的单文件实现深度离线和离线到在线强化学习算法。它强调简单的开发体验，具有直观的代码库和现代分析跟踪工具。在CORL中，我们将方法实现隔离到单独的单个文件中，使性能相关的细节更容易识别。此外，实验跟踪功能可用于帮助记录指标、超参数、依赖项等到云端。最后，我们通过基准测试常用的D4RL数据集，确保了实现的可靠性，提供了透明的结果源，可用于强大的评估工具，例如性能概要、改进概率或预期在线性能。

    CORL is an open-source library that provides thoroughly benchmarked single-file implementations of both deep offline and offline-to-online reinforcement learning algorithms. It emphasizes a simple developing experience with a straightforward codebase and a modern analysis tracking tool. In CORL, we isolate methods implementation into separate single files, making performance-relevant details easier to recognize. Additionally, an experiment tracking feature is available to help log metrics, hyperparameters, dependencies, and more to the cloud. Finally, we have ensured the reliability of the implementations by benchmarking commonly employed D4RL datasets providing a transparent source of results that can be reused for robust evaluation tools such as performance profiles, probability of improvement, or expected online performance.
    
[^107]: FP-Diffusion: 通过强制底层得分的福克-普朗克方程来改进基于得分的扩散模型

    FP-Diffusion: Improving Score-based Diffusion Models by Enforcing the Underlying Score Fokker-Planck Equation. (arXiv:2210.04296v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04296](http://arxiv.org/abs/2210.04296)

    本文提出了FP-Diffusion方法来改进基于得分的扩散模型，通过强制底层得分的福克-普朗克方程来正则化DSM目标函数，以提高模型似然度和守恒程度。

    

    基于得分的生成模型（SGM）学习一组与数据密度相对应的、噪声条件得分函数。这些扰动的数据密度通过福克-普朗克方程（FPE）相互联系，该方程是一种描述物质扩散过程的偏微分方程。在本文中，我们推导了一个对应的方程，称为得分FPE，其特征是扰动的数据密度（即它们的梯度）的噪声条件得分。令人惊讶的是，虽然DSM得分学习方法具有令人印象深刻的实证性能，但我们观察到学习到的得分未能满足底层的得分FPE，这是地面真实得分的固有自一致性属性。我们证明满足得分FPE是可取的，因为它可以提高似然度和守恒程度。因此，我们提出对DSM目标进行正则化，以强制满足分数FPE的要求。

    Score-based generative models (SGMs) learn a family of noise-conditional score functions corresponding to the data density perturbed with increasingly large amounts of noise. These perturbed data densities are linked together by the Fokker-Planck equation (FPE), a partial differential equation (PDE) governing the spatial-temporal evolution of a density undergoing a diffusion process. In this work, we derive a corresponding equation called the score FPE that characterizes the noise-conditional scores of the perturbed data densities (i.e., their gradients). Surprisingly, despite the impressive empirical performance, we observe that scores learned through denoising score matching (DSM) fail to fulfill the underlying score FPE, which is an inherent self-consistency property of the ground truth score. We prove that satisfying the score FPE is desirable as it improves the likelihood and the degree of conservativity. Hence, we propose to regularize the DSM objective to enforce satisfaction of
    
[^108]: 多尺度拓扑奇异性检测

    Topological Singularity Detection at Multiple Scales. (arXiv:2210.00069v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00069](http://arxiv.org/abs/2210.00069)

    本文提出了一种多尺度拓扑奇异性检测方法，可以评估数据的局部固有维度，并量化点的“流形度”，能够检测复杂空间和图像中的奇异性。

    

    流形假设是现代机器学习研究的一个基本假设，它假定数据位于或接近于低固有维度的未知流形上。然而，最近的研究表明，现实世界的数据表现出明显的非流形结构，即奇异性，这可能导致错误的发现。因此，检测这种奇异性在插值和推断任务之前是至关重要的。我们通过开发一个拓扑框架来解决这个问题，该框架能够（i）量化局部固有维度，以及（ii）在多个尺度上产生“欧几里得性”评分，用以评估点的“流形度”。我们的方法可以在图像数据中捕获复杂空间的奇异性，同时捕捉奇异结构和局部几何复杂性。

    The manifold hypothesis, which assumes that data lies on or close to an unknown manifold of low intrinsic dimension, is a staple of modern machine learning research. However, recent work has shown that real-world data exhibits distinct non-manifold structures, i.e. singularities, that can lead to erroneous findings. Detecting such singularities is therefore crucial as a precursor to interpolation and inference tasks. We address this issue by developing a topological framework that (i) quantifies the local intrinsic dimension, and (ii) yields a Euclidicity score for assessing the 'manifoldness' of a point along multiple scales. Our approach identifies singularities of complex spaces, while also capturing singular structures and local geometric complexity in image data.
    
[^109]: 分类指标的分析与比较

    Analysis and Comparison of Classification Metrics. (arXiv:2209.05355v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05355](http://arxiv.org/abs/2209.05355)

    本文回顾并比较了常用于度量分类系统表现的各种指标，发现期望成本指标具有更广泛的适用性和直观性，并可用于解决从连续得分生成分类决策的实践问题。

    

    在机器学习领域，常用各种性能指标来评估分类系统的表现。本文介绍了一些最常用的用于衡量硬决策质量的标准和平衡准确率、标准和平衡错误率、F-beta分数和Matthews相关系数（MCC）等指标。我们回顾了这些和其他指标的定义，并将它们与期望成本（EC）进行比较，后者是每个统计学习课程中都介绍但在机器学习文献中很少使用的指标。我们表明标准和平衡错误率都是EC的特殊情况，进一步展示了EC与F分数和MCC的关系，并认为EC指标优于传统指标，因其更具有优雅性、通用性和直观性，且基于统计学的基本原理。本文中介绍的指标均用于度量硬决策的质量。然而，大多数现代分类系统输出连续得分，而有一个重要的实践问题是如何从这些连续得分中生成分类决策。

    A variety of different performance metrics are commonly used in the machine learning literature for the evaluation of classification systems. Some of the most common ones for measuring quality of hard decisions are standard and balanced accuracy, standard and balanced error rate, F-beta score, and Matthews correlation coefficient (MCC). In this document, we review the definition of these and other metrics and compare them with the expected cost (EC), a metric introduced in every statistical learning course but rarely used in the machine learning literature. We show that both the standard and balanced error rates are special cases of the EC. Further, we show its relation with F-score and MCC and argue that EC is superior to these traditional metrics, being more elegant, general, and intuitive, as well as being based on basic principles from statistics.  The metrics above measure the quality of hard decisions. Yet, most modern classification systems output continuous scores for the class
    
[^110]: 单阶段广泛多实例多标签学习（BMIML）及其在医学图像分类中的应用

    Single-Stage Broad Multi-Instance Multi-Label Learning (BMIML) with Diverse Inter-Correlations and its application to medical image classification. (arXiv:2209.02625v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.02625](http://arxiv.org/abs/2209.02625)

    本文提出了一种单阶段广泛多实例多标签学习（BMIML）框架，解决了多实例和多标签之间的各种关系学习问题。在医学图像分类中具有广泛的应用前景。

    

    本文讨论了多实例（例如图像补丁）和多标签同时关联的问题。现有的多实例多标签（MIML）方法在许多应用中很有用，但大多数受到几个问题的限制：（i）忽略了标签间的相互关系（即，与一个对象对应的多个标签之间的概率关系）；（ii）无法直接学习（或共同学习）不同实例在预测对象标签方面的概率关系，因为缺少实例标签；（iii）多种多样的相关性（例如，标签间的相关性，实例间的相关性）只能在多个阶段中学习。为解决这些问题，本文提出了一种名为广泛多实例多标签学习（BMIML）的新型单阶段框架。

    described by multiple instances (e.g., image patches) and simultaneously associated with multiple labels. Existing MIML methods are useful in many applications but most of which suffer from relatively low accuracy and training efficiency due to several issues: i) the inter-label correlations(i.e., the probabilistic correlations between the multiple labels corresponding to an object) are neglected; ii) the inter-instance correlations (i.e., the probabilistic correlations of different instances in predicting the object label) cannot be learned directly (or jointly) with other types of correlations due to the missing instance labels; iii) diverse inter-correlations (e.g., inter-label correlations, inter-instance correlations) can only be learned in multiple stages. To resolve these issues, a new single-stage framework called broad multi-instance multi-label learning (BMIML) is proposed. In BMIML, there are three innovative modules: i) an auto-weighted label enhancement learning (AWLEL) ba
    
[^111]: 深度强化学习中的白盒对抗策略研究

    White-Box Adversarial Policies in Deep Reinforcement Learning. (arXiv:2209.02167v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.02167](http://arxiv.org/abs/2209.02167)

    本文研究了在强化学习中训练对抗策略的方法，提出了基于白盒攻击的策略，能够访问目标代理的内部状态，从而识别其漏洞，攻击成功率更高。

    

    在强化学习中，对抗策略可以通过训练对抗代理来最小化目标代理的奖励来开发。之前的研究研究了黑盒版本的这些攻击，其中对手仅观察世界状态，并将目标代理视为环境的任何其他部分。然而，这并没有考虑问题中的附加结构。在这项工作中，我们从白盒攻击的文献中获得灵感，以训练更有效的对抗策略。我们研究了白盒对抗策略，并显示访问目标代理的内部状态可以用于识别其漏洞。我们做出了两个贡献。(1)我们介绍了白盒对抗策略，其中攻击者在每个时间步观察目标的内部状态和世界状态。我们制定了使用这些策略攻击2人游戏和生成文本语言模型中的代理的方法。(2)我们证明了与黑盒攻击相比，这些策略可以实现更高的攻击成功率，特别是当目标代理的内部状态比较复杂时。

    In reinforcement learning (RL), adversarial policies can be developed by training an adversarial agent to minimize a target agent's rewards. Prior work has studied black-box versions of these attacks where the adversary only observes the world state and treats the target agent as any other part of the environment. However, this does not take into account additional structure in the problem. In this work, we take inspiration from the literature on white-box attacks to train more effective adversarial policies. We study white-box adversarial policies and show that having access to a target agent's internal state can be useful for identifying its vulnerabilities. We make two contributions. (1) We introduce white-box adversarial policies where an attacker observes both a target's internal state and the world state at each timestep. We formulate ways of using these policies to attack agents in 2-player games and text-generating language models. (2) We demonstrate that these policies can ach
    
[^112]: 深度学习模型的神经网络架构搜索和持续学习交叉研究探究

    Exploring the Intersection between Neural Architecture Search and Continual Learning. (arXiv:2206.05625v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.05625](http://arxiv.org/abs/2206.05625)

    本文研究了神经网络架构搜索和持续学习的交叉研究，旨在开发更加稳健和适应性的智能体，应用于物联网设备、自动驾驶车辆等领域，以解决诸如概念/数据漂移等问题。

    

    尽管人工神经网络（ANNs）已经有了相当大的进展，但它们的设计过程仍然以直觉、经验和反复试错为主。这个依赖于人类的过程往往耗时且容易出错。此外，这些模型通常仅考虑在训练环境下的情况，而没有考虑周围的环境。神经网络的持续适应性和自动化对于一些领域非常重要，在这些领域中模型在部署后的可访问性很有限（例如物联网设备、自动驾驶车辆等）。此外，即使是可访问的模型，在部署后也需要频繁维护以解决诸如概念/数据漂移等问题，这可能会很繁琐和限制性。通过利用和结合神经网络架构搜索（NAS）和持续学习（CL）的方法，可以开发更加稳健和适应性的智能体。本研究进行了第一次广泛的深入探讨此交叉研究的综述。

    Despite the significant advances achieved in Artificial Neural Networks (ANNs), their design process remains notoriously tedious, depending primarily on intuition, experience and trial-and-error. This human-dependent process is often time-consuming and prone to errors. Furthermore, the models are generally bound to their training contexts, with no considerations to their surrounding environments. Continual adaptiveness and automation of neural networks is of paramount importance to several domains where model accessibility is limited after deployment (e.g IoT devices, self-driving vehicles, etc.). Additionally, even accessible models require frequent maintenance post-deployment to overcome issues such as Concept/Data Drift, which can be cumbersome and restrictive. By leveraging and combining approaches from Neural Architecture Search (NAS) and Continual Learning (CL), more robust and adaptive agents can be developed. This study conducts the first extensive review on the intersection be
    
[^113]: E2PN：高效的SE(3)-等变点云网络

    E2PN: Efficient SE(3)-Equivariant Point Network. (arXiv:2206.05398v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.05398](http://arxiv.org/abs/2206.05398)

    本文提出了一种简单、轻量、快速的SE(3)-等变点云网络，通过组卷积和商表示相结合实现，用于处理点云数据，同时在对象分类、姿态估计和关键点匹配等任务上实现了可比或优越的性能。

    

    本文提出了一种卷积结构，用于从3D点云学习SE(3)等变特征。它可以被视为核点卷积(KPConv)的等变版本，KPConv是一种广泛用于处理点云数据的卷积形式。与现有的等变网络相比，我们的设计简单、轻量、快速，易于与现有的特定任务的点云学习流程集成。我们通过组卷积和商表示相结合，实现了这些理想特性。具体而言，我们将SO(3)离散为有限群、使用SO(2)作为稳定子群，形成球形商特征场以节省计算量。我们还提出了一个置换层，从球形特征中恢复SO(3)特征，以保留区分旋转的能力。实验表明，我们的方法在各种任务中实现了可比或优越的性能，包括对象分类、姿态估计和关键点匹配。

    This paper proposes a convolution structure for learning SE(3)-equivariant features from 3D point clouds. It can be viewed as an equivariant version of kernel point convolutions (KPConv), a widely used convolution form to process point cloud data. Compared with existing equivariant networks, our design is simple, lightweight, fast, and easy to be integrated with existing task-specific point cloud learning pipelines. We achieve these desirable properties by combining group convolutions and quotient representations. Specifically, we discretize SO(3) to finite groups for their simplicity while using SO(2) as the stabilizer subgroup to form spherical quotient feature fields to save computations. We also propose a permutation layer to recover SO(3) features from spherical features to preserve the capacity to distinguish rotations. Experiments show that our method achieves comparable or superior performance in various tasks, including object classification, pose estimation, and keypoint-matc
    
[^114]: AdaProp：基于图神经网络的知识图谱推理中学习自适应传播

    AdaProp: Learning Adaptive Propagation for Graph Neural Network based Knowledge Graph Reasoning. (arXiv:2205.15319v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15319](http://arxiv.org/abs/2205.15319)

    本文提出了一种名为AdaProp的方法，该方法可自适应地过滤掉不相关的实体，同时保留有前途的目标，从而高效、强大地进行知识图谱推理。

    

    由于图神经网络（GNN）的广泛应用，已经有许多基于GNN的方法被设计用于知识图谱(KG)推理。GNN-based KG推理方法中一个重要的组成部分是传播路径，它包含每个传播步骤中所涉及的一组实体。现有方法使用手工设计的传播路径，忽略了实体与查询关系之间的相关性。此外，在更大的传播步骤中，涉及的实体数量会爆炸性增长。本文旨在学习一个自适应的传播路径，以过滤掉不相关的实体，同时保留有前途的目标。首先，我们设计了一种增量采样机制，它能够以线性复杂度保留附近目标和分层连接。其次，我们设计了一个基于学习的采样分布来识别语义相关的实体。大量实验证明，我们的方法强大、高效，并且是语义感知的。

    Due to the popularity of Graph Neural Networks (GNNs), various GNN-based methods have been designed to reason on knowledge graphs (KGs). An important design component of GNN-based KG reasoning methods is called the propagation path, which contains a set of involved entities in each propagation step. Existing methods use hand-designed propagation paths, ignoring the correlation between the entities and the query relation. In addition, the number of involved entities will explosively grow at larger propagation steps. In this work, we are motivated to learn an adaptive propagation path in order to filter out irrelevant entities while preserving promising targets. First, we design an incremental sampling mechanism where the nearby targets and layer-wise connections can be preserved with linear complexity. Second, we design a learning-based sampling distribution to identify the semantically related entities. Extensive experiments show that our method is powerful, efficient, and semantic-awa
    
[^115]: 基于上下文BERT调整的阅读理解自动评分模型

    Automated Scoring for Reading Comprehension via In-context BERT Tuning. (arXiv:2205.09864v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09864](http://arxiv.org/abs/2205.09864)

    本文介绍了一种基于预训练语言模型的文本表示和上下文BERT微调的阅读理解自动评分模型，解决了单个问题/题目模型无法利用题目间关联性以及存储模型困难的问题。

    

    自动评分模型有着极大的潜力可以降低人工评分的成本。最近的一些自动评分方法利用了基于预训练语言模型（例如BERT和GPT）的文本表示作为评分模型的输入。然而，大多数方法为每个问题/题目训练一个单独的模型，这适用于试题种类千差万别的作文评分等场景。但是这种方法存在两个问题：1）在阅读理解等与多个问题/题目相关的场景下，无法利用题目之间的关联性；2）当模型参数数量庞大时，存储每个题目独立模型变得很困难。本文介绍了我们在国家教育进步评估（NAEP）阅读理解自动评分挑战赛中获得的（一等奖）解决方案。我们的方法——基于上下文BERT微调，产生一个共用的评分器。

    Automated scoring of open-ended student responses has the potential to significantly reduce human grader effort. Recent advances in automated scoring often leverage textual representations based on pre-trained language models such as BERT and GPT as input to scoring models. Most existing approaches train a separate model for each item/question, which is suitable for scenarios such as essay scoring where items can be quite different from one another. However, these approaches have two limitations: 1) they fail to leverage item linkage for scenarios such as reading comprehension where multiple items may share a reading passage; 2) they are not scalable since storing one model per item becomes difficult when models have a large number of parameters. In this paper, we report our (grand prize-winning) solution to the National Assessment of Education Progress (NAEP) automated scoring challenge for reading comprehension. Our approach, in-context BERT fine-tuning, produces a single shared scor
    
[^116]: 双线性价值网络

    Bilinear value networks. (arXiv:2204.13695v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2204.13695](http://arxiv.org/abs/2204.13695)

    提出了一种通过点积低秩近似来表示Q值的双线性分解方法，其中第一个向量场捕捉状态的局部动态，第二个部分捕捉当前状态和目标之间的全局关系，该方法能够显著提高数据效率，并具有很好的泛化性能。

    

    多目标强化学习的主流框架涉及估计目标条件下的Q值函数。当学习实现多个不同目标时，数据效率与Q函数对新目标的泛化密切相关。目前的范式是使用单一的神经网络来近似Q(s, a, g)。为了改进Q函数的泛化，我们提出了一种双线性分解方法，通过两个向量场之间的点积的低秩近似来表示Q值。第一个向量场f(s, a)捕捉状态s处的环境局部动态；而第二个部分{ϕ}(s, g)则捕捉当前状态和目标之间的全局关系。我们展示了双线性分解方案显著提高了数据效率，相比先前的方法，对于处于分布范围之外的目标具有更好的转移性。我们在模拟的Fetch机器人任务套件和DeepMind Control Suite上提供了实证证据。

    The dominant framework for off-policy multi-goal reinforcement learning involves estimating goal conditioned Q-value function. When learning to achieve multiple goals, data efficiency is intimately connected with the generalization of the Q-function to new goals. The de-facto paradigm is to approximate Q(s, a, g) using monolithic neural networks. To improve the generalization of the Q-function, we propose a bilinear decomposition that represents the Q-value via a low-rank approximation in the form of a dot product between two vector fields. The first vector field, f(s, a), captures the environment's local dynamics at the state s; whereas the second component, {\phi}(s, g), captures the global relationship between the current state and the goal. We show that our bilinear decomposition scheme substantially improves data efficiency, and has superior transfer to out-of-distribution goals compared to prior methods. Empirical evidence is provided on the simulated Fetch robot task-suite and d
    
[^117]: 拓扑经验回放

    Topological Experience Replay. (arXiv:2203.15845v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.15845](http://arxiv.org/abs/2203.15845)

    本文提出了一种拓扑经验回放的方法，通过构建图来明确状态的 Q 值之间的依赖关系，解决了传统采样策略忽视状态间依赖关系的问题，提高了学习深度 Q 函数时的性能和准确性。

    

    最先进的深度 Q 学习方法使用从经验重放缓冲区中采样的状态转换元组更新 Q 值。这种策略通常均匀和随机地采样，或基于诸如时间差（TD）误差等度量优先。这样的采样策略在学习 Q 函数时可能效率低下，因为一个状态的 Q 值取决于继承状态的 Q 值。如果数据采样策略忽略了下一个状态的 Q 值估计的精度，它可能会导致无用和常常不正确的 Q 值更新。为了减轻这个问题，我们将智能体的经验组织成一个图，明确跟踪状态的 Q 值之间的依赖关系。图中的每条边代表通过执行单个操作在两个状态之间的转换。我们通过从一组终端状态开始扩展图中的顶点，并逐步向后移动的广度优先搜索来执行值备份。

    State-of-the-art deep Q-learning methods update Q-values using state transition tuples sampled from the experience replay buffer. This strategy often uniformly and randomly samples or prioritizes data sampling based on measures such as the temporal difference (TD) error. Such sampling strategies can be inefficient at learning Q-function because a state's Q-value depends on the Q-value of successor states. If the data sampling strategy ignores the precision of the Q-value estimate of the next state, it can lead to useless and often incorrect updates to the Q-values. To mitigate this issue, we organize the agent's experience into a graph that explicitly tracks the dependency between Q-values of states. Each edge in the graph represents a transition between two states by executing a single action. We perform value backups via a breadth-first search starting from that expands vertices in the graph starting from the set of terminal states and successively moving backward. We empirically sho
    
[^118]: 并行连续学习用于异构无线网络上动态分布式模型训练

    Parallel Successive Learning for Dynamic Distributed Model Training over Heterogeneous Wireless Networks. (arXiv:2202.02947v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.02947](http://arxiv.org/abs/2202.02947)

    本文提出了并行连续学习（PSL），通过网络、异构性和接近性三个维度的扩展，实现了在无线设备集合上的动态分布式模型训练。

    

    联邦学习(FedL)已成为一种将模型训练分布在一组无线设备上的流行技术，通过设备上的迭代本地更新和服务器上的全局聚合。本文提出并行连续学习（PSL），将FedL架构沿三个维度进行扩展：（i）网络，通过设备间通信实现去中心化协作；（ii）异构性，在三个层次进行解释：（ii-a）学习：PSL考虑到设备上具有不同的小批量大小的异构数量的随机梯度下降迭代；（ii-b）数据：PSL假设数据到达和离开的动态环境中，本地数据集的分布随时间演变，通过新的模型/概念漂移度量来捕获；（ii-c）设备：PSL考虑到具有不同计算和通信能力的设备；（iii）接近性，设备之间以及访问点之间具有不同的距离。

    Federated learning (FedL) has emerged as a popular technique for distributing model training over a set of wireless devices, via iterative local updates (at devices) and global aggregations (at the server). In this paper, we develop parallel successive learning (PSL), which expands the FedL architecture along three dimensions: (i) Network, allowing decentralized cooperation among the devices via device-to-device (D2D) communications. (ii) Heterogeneity, interpreted at three levels: (ii-a) Learning: PSL considers heterogeneous number of stochastic gradient descent iterations with different mini-batch sizes at the devices; (ii-b) Data: PSL presumes a dynamic environment with data arrival and departure, where the distributions of local datasets evolve over time, captured via a new metric for model/concept drift. (ii-c) Device: PSL considers devices with different computation and communication capabilities. (iii) Proximity, where devices have different distances to each other and the acces
    
[^119]: 认知账本项目：通过认知区块链构建个人数字化孪生体

    Cognitive Ledger Project: Towards Building Personal Digital Twins Through Cognitive Blockchain. (arXiv:2201.08163v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2201.08163](http://arxiv.org/abs/2201.08163)

    该论文提出了一个认知数字孪生体的认知架构，即通过基于区块链基础设施的方式将用户的个人数据转化为结构化信息和机器学习模型，最终能够一起组成用户的认知数字孪生体。

    

    认知账本项目旨在开发一个模块化系统，通过基于区块链基础设施的方式将用户的个人数据转化为结构化信息和机器学习模型。在这个正在进行的工作中，我们提出了一个认知数字孪生体的认知架构。所提出的设计在其核心采用了认知区块链（认知账本）。架构包括多个模块，可以将用户在数字环境中的活动转化为可重复使用的知识对象和人工智能，最终能够一起组成用户的认知数字孪生体。

    The Cognitive Ledger Project is an effort to develop a modular system for turning users' personal data into structured information and machine learning models based on a blockchain-based infrastructure. In this work-in-progress paper, we propose a cognitive architecture for cognitive digital twins. The suggested design embraces a cognitive blockchain (Cognitive ledger) at its core. The architecture includes several modules that turn users' activities in the digital environment into reusable knowledge objects and artificial intelligence that one day can work together to form the cognitive digital twin of users.
    
[^120]: 一种基于任务感知元学习的Siamese神经网络分类混淆恶意软件的方法

    Task-Aware Meta Learning-based Siamese Neural Network for Classifying Obfuscated Malware. (arXiv:2110.13409v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2110.13409](http://arxiv.org/abs/2110.13409)

    本文提出了一种基于任务感知元学习的Siamese神经网络分类混淆恶意软件的方法，可以更准确地调整特征嵌入，以应对不同恶意软件家族的混淆变种，并且可以对恶意软件类进行分类，即使只有一个或很少的训练样本可用。

    

    恶意软件作者采用不同的控制流混淆技术创建新的恶意软件变种以避免检测。现有的基于Siamese神经网络（SNN）的恶意软件检测方法在训练数据集中存在这些混淆样本时无法正确分类不同的恶意软件家族，导致高误报率。为了解决这个问题，我们提出了一种新颖的任务感知少样本学习的Siamese神经网络，它可以抵抗受控制流混淆影响的恶意软件变种的存在。利用每个恶意软件家族的平均熵特征作为输入，除了图像特征外，我们的模型还生成特征层的参数，以更准确地为不同的恶意软件家族调整特征嵌入，这些家族都有混淆的恶意软件变种。此外，我们提出的方法可以对恶意软件类进行分类，即使只有一个或很少的训练样本可用。

    Malware authors apply different techniques of control flow obfuscation, in order to create new malware variants to avoid detection. Existing Siamese neural network (SNN)-based malware detection methods fail to correctly classify different malware families when such obfuscated malware samples are present in the training dataset, resulting in high false-positive rates. To address this issue, we propose a novel task-aware few-shot-learning-based Siamese Neural Network that is resilient against the presence of malware variants affected by such control flow obfuscation techniques. Using the average entropy features of each malware family as inputs, in addition to the image features, our model generates the parameters for the feature layers, to more accurately adjust the feature embedding for different malware families, each of which has obfuscated malware variants. In addition, our proposed method can classify malware classes, even if there are only one or a few training samples available. 
    
[^121]: QuantumNAT：注重量子噪声的噪声注入、量化和归一化的量子训练

    QuantumNAT: Quantum Noise-Aware Training with Noise Injection, Quantization and Normalization. (arXiv:2110.11331v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.11331](http://arxiv.org/abs/2110.11331)

    QuantumNAT是一个PQC特定框架，可以在训练和推断阶段执行噪声感知优化，提高鲁棒性，缓解量子噪声

    

    参数化量子电路是实现近期量子硬件优势的有希望方法。然而，由于存在较大的量子噪声（误差），在实际的量子设备上，PQC模型的性能会受到严重的降级。我们提出了QuantumNAT，一个可以在训练和推断阶段执行噪声感知优化的PQC特定框架，以提高其鲁棒性。通过实验我们发现，量子噪声对PQC测量结果的影响是从无噪声结果经过一个缩放和偏移因子得到的线性映射。基于此，我们提出了后测量归一化来缓解特征分布不一致的问题。

    Parameterized Quantum Circuits (PQC) are promising towards quantum advantage on near-term quantum hardware. However, due to the large quantum noises (errors), the performance of PQC models has a severe degradation on real quantum devices. Take Quantum Neural Network (QNN) as an example, the accuracy gap between noise-free simulation and noisy results on IBMQ-Yorktown for MNIST-4 classification is over 60%. Existing noise mitigation methods are general ones without leveraging unique characteristics of PQC; on the other hand, existing PQC work does not consider noise effect. To this end, we present QuantumNAT, a PQC-specific framework to perform noise-aware optimizations in both training and inference stages to improve robustness. We experimentally observe that the effect of quantum noise to PQC measurement outcome is a linear map from noise-free outcome with a scaling and a shift factor. Motivated by that, we propose post-measurement normalization to mitigate the feature distribution di
    
[^122]: LLVIP：适用于低光视觉的可见-红外成对数据集

    LLVIP: A Visible-infrared Paired Dataset for Low-light Vision. (arXiv:2108.10831v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2108.10831](http://arxiv.org/abs/2108.10831)

    提出了适用于低光视觉的可见-红外成对数据集LLVIP，数据集包含30976张图像，解决了在低光条件下各种视觉任务缺失有效目标区域的挑战。行人已被标注，实验结果证明了融合对图像信息的补充效果，揭示了现有算法在非常低光条件下的不足之处。

    

    在低光条件下，由于有效目标区域的丢失，各种视觉任务（如图像融合，行人检测和图像到图像翻译）都面临巨大挑战。此时，可见光和红外图像可以一起使用，提供丰富的细节信息和有效的目标区域。提出了一个适用于低光视觉的可见-红外成对数据集LLVIP。该数据集包含30976张图像，或15488对图像，大多数图像拍摄于非常黑暗的场景，并且所有图像在时间和空间上严格对齐。数据集中的行人已被标记。将数据集与其他可见-红外数据集进行比较，并在数据集上评估了一些流行的视觉算法（包括图像融合，行人检测和图像到图像翻译）的性能。实验结果证明了融合对图像信息的补充效果，并发现了三个视觉任务现有算法在非常低光条件下的不足之处。

    It is very challenging for various visual tasks such as image fusion, pedestrian detection and image-to-image translation in low light conditions due to the loss of effective target areas. In this case, infrared and visible images can be used together to provide both rich detail information and effective target areas. In this paper, we present LLVIP, a visible-infrared paired dataset for low-light vision. This dataset contains 30976 images, or 15488 pairs, most of which were taken at very dark scenes, and all of the images are strictly aligned in time and space. Pedestrians in the dataset are labeled. We compare the dataset with other visible-infrared datasets and evaluate the performance of some popular visual algorithms including image fusion, pedestrian detection and image-to-image translation on the dataset. The experimental results demonstrate the complementary effect of fusion on image information, and find the deficiency of existing algorithms of the three visual tasks in very l
    

