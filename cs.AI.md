# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Generative AI for Education (GAIED): Advances, Opportunities, and Challenges](https://rss.arxiv.org/abs/2402.01580) | 这篇调查文章总结了NeurIPS 2023会议上关于教育的生成式人工智能（GAIED）研讨会的活动，并突出指出了几个未来研究方向。 |
| [^2] | [Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models](https://rss.arxiv.org/abs/2402.01345) | 本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。 |
| [^3] | [Edu-ConvoKit: An Open-Source Library for Education Conversation Data](https://arxiv.org/abs/2402.05111) | Edu-ConvoKit是一种用于处理教育会话数据的开源库，它解决了教育会话数据分析资源稀缺的问题，提供了全面的文档和附加资源。 |
| [^4] | [Image captioning for Brazilian Portuguese using GRIT model](https://arxiv.org/abs/2402.05106) | 本文早期开发了一个用于巴西葡萄牙语的图像字幕生成模型，使用了GRIT模型，并对其进行了调整以适应巴西葡萄牙语数据集，实现了更高效的图像字幕生成方法。 |
| [^5] | [A Roadmap to Pluralistic Alignment](https://arxiv.org/abs/2402.05070) | 这篇论文提出了一条通向多元对齐的路线图，以解决设计AI系统能够服务于人们具有不同价值观和观点的需求。论文介绍了对齐定义和实现多元主义的三种方式，并提出了三种多元基准类别来评估和测试多元对齐的效果。 |
| [^6] | [How VADER is your AI? Towards a definition of artificial intelligence systems appropriate for regulation](https://arxiv.org/abs/2402.05048) | 本研究提出了一个评分框架，用于衡量人工智能（AI）定义在监管方面的合适性。研究旨在排除通过AI监管提案所采用的不恰当的AI定义对ICT技术、方法和非AI作品的影响，并回归到以前在其他成功技术监管中观察到的原则。 |
| [^7] | [SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models](https://arxiv.org/abs/2402.05044) | SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。 |
| [^8] | [Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing](https://arxiv.org/abs/2402.05027) | 本论文提出了一种循环消息传递模型，它可以在图中实现多Agent强化学习的泛化能力。这种模型通过在整个图中进行信息流实现了观察邻域大小的平衡，从而提高了Agent的反应性、选择动作的质量和通信效率。 |
| [^9] | [EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss](https://arxiv.org/abs/2402.05008) | 高效ViT-SAM是一种新的加速的段落任意模型，通过保留轻量级提示编码器和掩码解码器，并替换沉重的图像编码器，实现了48.9倍的加速而不牺牲性能。 |
| [^10] | [Example-based Explanations for Random Forests using Machine Unlearning](https://arxiv.org/abs/2402.05007) | FairDebugger是一个利用机器学习去解释随机森林的系统，在公平性背景下找出导致模型不公平的训练数据子集。 |
| [^11] | [Detection and Pose Estimation of flat, Texture-less Industry Objects on HoloLens using synthetic Training](https://arxiv.org/abs/2402.04979) | 该论文提出了一种使用合成训练的增强现实应用程序，能够在Edge设备上实现对平面纹理缺失的工业物体进行最先进的姿态估计。 |
| [^12] | [An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration](https://arxiv.org/abs/2402.04978) | 本文提出了一种通过知识图谱和LLMs合作训练的推理方案，该方案解决了大型语言模型在实际应用中的虚构问题、知识更新不足以及推理过程的透明度有限等挑战，实现了更可靠的知识推理和推理结果追踪。 |
| [^13] | [ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12](https://arxiv.org/abs/2402.04975) | ChatScratch是一种AI增强系统，旨在帮助6-12岁儿童自主学习视觉编程。它通过结构化互动故事板、数字绘画和专业编码指导等方式解决了儿童在学习过程中遇到的创作困难和编码指导不足的问题。 |
| [^14] | [Multi-Sender Persuasion -- A Computational Perspective](https://arxiv.org/abs/2402.04971) | 这项研究考虑了多个具有信息优势的发信者向单个自私行为者传递信号以影响其行为的问题，并提出了一种新颖的可微神经网络方法来近似解决这一问题。通过额外梯度算法，我们发现了超越已有方法的局部均衡解。 |
| [^15] | [Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?](https://arxiv.org/abs/2402.04967) | 本文研究了多模式仇恨迷因检测中的跨领域泛化挑战，并发现仇恨迷因的文本部分对于泛化至不同领域的分类器至关重要，而图像部分对特定训练数据集敏感。实验证明仇恨文本分类器在零样本情况下表现类似于仇恨迷因分类器。黑盒解释表明文本模态对模型性能的贡献较大，但引入图像标题后贡献降低。新的混淆数据集评估显示文本混淆因素表现较好。 |
| [^16] | [Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems](https://arxiv.org/abs/2402.04955) | 这项研究比较了在知识密集型环境中基于LLM的认知助手和基于意图的系统，并发现基于LLM的认知助手在用户体验、任务完成率、可用性和个人效能方面表现更好。 |
| [^17] | [An approach to automated videogame beta testing](https://arxiv.org/abs/2402.04938) | 本文介绍了一种自动化视频游戏Beta测试的方法，通过解决视频游戏的特殊性，使得质量保证任务能够自动完成。 |
| [^18] | [Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation](https://arxiv.org/abs/2402.04929) | 本文提出了一种无源域自适应的新方法，利用扩散模型生成上下文相关的领域特定图像，通过微调预训练模型和无监督领域自适应技术实现了显著的性能改进。 |
| [^19] | [Prompting Implicit Discourse Relation Annotation](https://arxiv.org/abs/2402.04918) | 本研究旨在提升ChatGPT对隐式话语关系的识别，尝试了多种提示技术，但实验结果显示即使进行复杂的提示工程，隐式话语关系分类在零样本或少样本设置下仍难以解决。 |
| [^20] | [The Strain of Success: A Predictive Model for Injury Risk Mitigation and Team Success in Soccer](https://arxiv.org/abs/2402.04898) | 本文开发了一种预测模型，通过考虑球员受伤概率，在足球赛季中优化团队表现，减少一线队受伤人数和无效花费，为真实足球团队降低成本并提高球员福利提供了潜力。 |
| [^21] | [A Unified Framework for Probabilistic Verification of AI Systems via Weighted Model Integration](https://arxiv.org/abs/2402.04892) | 本论文提出了一个基于加权模型集成的统一框架，用于概率验证AI系统。这个框架可以在不依赖强分布假设的情况下，验证各种机器学习模型的许多有趣属性，如公平性、鲁棒性或单调性。 |
| [^22] | [RSCNet: Dynamic CSI Compression for Cloud-based WiFi Sensing](https://arxiv.org/abs/2402.04888) | 这篇论文提出了一种名为RSCNet的动态CSI压缩方法，通过压缩信道状态信息（CSI）来减少物联网设备向云服务器传输CSI的通信开销。RSCNet利用长短期记忆（LSTM）单元和优化的CSI窗口实现了准确的感知和CSI重建，从而实现了实时的云基WiFi感知。 |
| [^23] | [A Unified Gaussian Process for Branching and Nested Hyperparameter Optimization](https://arxiv.org/abs/2402.04885) | 本文提出了一个统一的贝叶斯优化方法，用于处理分支和嵌套参数之间的条件依赖关系，该方法能够有效地优化神经网络的超参数选择和调整。 |
| [^24] | [LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units](https://arxiv.org/abs/2402.04882) | 本文提出了一种改进的脉冲模型LMUFormer，通过对循环模型进行架构修改，将其性能推向Transformer模型，同时保留其顺序处理能力，以实现并行训练、流式处理和低成本推理。 |
| [^25] | [Combining Cloud and Mobile Computing for Machine Learning](https://arxiv.org/abs/2402.04880) | 这项研究将模型分割为移动设备和云之间的计算，以减轻移动设备的负担，并优化云端的工作负载。 |
| [^26] | [Choosing a Classical Planner with Graph Neural Networks](https://arxiv.org/abs/2402.04874) | 本文研究了使用图神经网络进行在线规划器选择的方法，并通过析取出的图表示作为另一模型的输入，提出了一种更加资源高效但准确的方法。 |
| [^27] | [Embedding Knowledge Graphs in Degenerate Clifford Algebras](https://arxiv.org/abs/2402.04870) | 这项研究提出将知识图谱嵌入到退化的克利福德代数中。通过考虑具有零幂指数为2的零幂基向量，可以泛化基于二次数的方法并捕捉实体嵌入中缺乏高阶相互作用的模式。研究设计了两个新模型来发现代数的参数，并证明零幂向量有助于捕捉实体的特征。 |
| [^28] | [Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy](https://arxiv.org/abs/2402.04869) | 本文提出了一个在线因果强化学习框架，其中通过明确建模状态的生成过程和使用因果结构进行策略引导，以帮助强化学习代理的决策可解释性。该框架具有理论性能保证，并在探索和开发过程中使用干预进行因果结构学习。 |
| [^29] | [Explaining Learned Reward Functions with Counterfactual Trajectories](https://arxiv.org/abs/2402.04856) | 通过对比原始轨迹和反事实部分轨迹的奖励，我们提出了一种解释强化学习中奖励函数的方法。通过生成符合质量标准的反事实轨迹解释（CTEs），我们的实验表明，CTEs对于代理人模型具有明显的信息性，能提高其预测与奖励函数的一致性。 |
| [^30] | [PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition](https://arxiv.org/abs/2402.04838) | 本研究提出了PaDeLLM-NER，一种能够在大型语言模型中实现并行解码，从而显著减少命名实体识别的生成延迟，同时保持预测质量和性能。 |
| [^31] | [On the Completeness of Invariant Geometric Deep Learning Models](https://arxiv.org/abs/2402.04836) | 这项研究集中于不变模型的理论表达能力，通过引入完备的设计GeoNGNN，并利用其作为理论工具，首次证明了E(3)-完备性。 |
| [^32] | [Structured d-DNNF Is Not Closed Under Negation](https://arxiv.org/abs/2402.04832) | 结构化的d-DNNF不支持多项式时间的否定、析取或存在量化操作，回答了OBDD是否支持更可处理的转换问题。结构化的d-DNNF比SDD更简洁，不存在等价的多项式大小的SDD表示。 |
| [^33] | [Direct Language Model Alignment from Online AI Feedback](https://arxiv.org/abs/2402.04792) | 本论文提出了一种名为OAIF的方法，通过在线反馈来改善DAP方法，该方法使用LLM作为标注器，从而在多个任务中展示出优于离线DAP和RLHF的性能 |
| [^34] | [MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark](https://arxiv.org/abs/2402.04788) | 本文引入了MLLM作为法官的新基准测试，用于评估MLLM在辅助法官方面的能力。研究结果显示，MLLM在对比评估任务中展示出了非常类人的辨别能力，但在评分评估和批量排序任务中与人类偏好存在显著的差异。此外，即使对于先进模型如GPT-4V，MLLM仍然面临着偏见、幻觉式回答和不一致性等判断方面的挑战。 |
| [^35] | [StableMask: Refining Causal Masking in Decoder-only Transformer](https://arxiv.org/abs/2402.04779) | StableMask是一种在仅解码Transformer中改进因果屏蔽的无参数方法，通过引入伪注意力值来平衡注意力分布，并通过逐渐减小的屏蔽比率来编码绝对位置信息。 |
| [^36] | [Emergence of specialized Collective Behaviors in Evolving Heterogeneous Swarms](https://arxiv.org/abs/2402.04763) |  |
| [^37] | [EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World Illusions](https://arxiv.org/abs/2402.04699) | EvoSeed是一个基于进化策略的搜索算法框架，用于生成自然对抗样本，旨在揭示深度神经网络面临的威胁。该框架在模型无关的黑盒设置中操作，可以生成高质量且可转移的对抗图像。 |
| [^38] | [Large Language Models As Faithful Explainers](https://arxiv.org/abs/2402.04678) | 本论文提出了一个生成解释框架（xLLM），用于提高大型语言模型（LLMs）自然语言格式解释的可信度。通过一个评估器来量化解释的可信度，并通过迭代优化过程来提高可信度。 |
| [^39] | [Group Distributionally Robust Dataset Distillation with Risk Minimization](https://arxiv.org/abs/2402.04676) | 这项研究关注数据集蒸馏与其泛化能力的关系，尤其是在面对不常见的子组的样本时，如何确保模型在合成数据集上的训练可以表现良好。 |
| [^40] | [Adversarial Robustness Through Artifact Design](https://arxiv.org/abs/2402.04660) | 该研究提出了一种通过艺术设计实现对抗性鲁棒性的方法，通过微小更改现有规范来抵御对抗性示例的影响。 |
| [^41] | [LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views](https://arxiv.org/abs/2402.04644) | 本论文中，我们提出了一种名为LEVI的方法，通过以层为单位的多视角集成，实现了对预训练和微调数据中问题的解决，并提升微调模型对未见过的分布的泛化能力。 |
| [^42] | [SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph](https://arxiv.org/abs/2402.04627) | 在本研究中，我们针对生命科学知识图谱上的问题回答对OpenLlama LLM进行了微调，并提出了一种数据增强方法，以扩展现有查询集合，从而能够进行微调，即使训练数据稀缺。我们还研究了查询中语义线索的作用。 |
| [^43] | [InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory](https://arxiv.org/abs/2402.04617) | InfLLM是一种无需训练的基于记忆的方法，用于揭示LLMs处理超长序列的内在能力。它通过存储远距离上下文和高效的注意计算机制，允许LLMs有效处理具有流式输入的长序列。 |
| [^44] | [TinyLLM: Learning a Small Student from Multiple Large Language Models](https://arxiv.org/abs/2402.04616) | TinyLLM是一种从多个大型语言模型中学习小型学生模型的知识蒸馏范式，旨在解决知识多样性有限和缺乏上下文信息等问题，并鼓励学生模型理解答案背后的原理。 |
| [^45] | [ScreenAI: A Vision-Language Model for UI and Infographics Understanding](https://arxiv.org/abs/2402.04615) | ScreenAI是一个专注于UI和信息图表理解的视觉-语言模型，通过灵活的修补策略和独特的数据集训练，以及针对UI元素的屏幕注解任务的处理，实现了在多个任务上的新的最优结果。 |
| [^46] | [Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector](https://arxiv.org/abs/2402.04601) | 本文提出了一种提升对齐的中文语法错误修正器，旨在解决使用自回归生成模型时面临的过度修正挑战。该方法适用于序列到序列模型和仅解码的大型语言模型，并通过对齐模型进行多轮修正，提高修正效果。 |
| [^47] | [CMSA algorithm for solving the prioritized pairwise test data generation problem in software product lines](https://arxiv.org/abs/2402.04597) | 本研究提出了一种基于CMSA的新算法来解决软件产品线中的优先级配对测试数据生成问题，该算法可以解决实例规模较大和现有算法无法在合理时间内计算的问题。 |
| [^48] | [Towards Improved Imbalance Robustness in Continual Multi-Label Learning with Dual Output Spiking Architecture (DOSA)](https://arxiv.org/abs/2402.04596) | 本研究提出了一个双输出脉冲架构（DOSA）来解决连续多标签学习中的不平衡鲁棒性问题，并提出了一种新的不平衡感知的损失函数来提高多标记分类性能。 |
| [^49] | [A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents](https://arxiv.org/abs/2402.04580) | 这篇论文综述了机器人跨领域策略转移方法，讨论了从目标领域采集无偏数据的挑战，以及从源领域获取数据的成本效益性。同时，总结了不同问题设置下的设计考虑和方法。 |
| [^50] | [S-Agents: self-organizing agents in open-ended environment](https://arxiv.org/abs/2402.04578) | S-Agents是一个自组织代理系统，通过引入代理树结构、沙漏代理架构和非阻塞协作方法，实现了在无限环境中高效协调代理的能力，提供了优化协作效率和灵活性的解决方案。 |
| [^51] | [OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences](https://arxiv.org/abs/2402.04567) | 本论文提出了一种用于顺序决策序列的异常检测框架，通过提取行为特征来检测异常。这种方法克服了强化学习方法在真实世界中应用困难的问题，并提供了关于异常的足够信息。 |
| [^52] | [Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention](https://arxiv.org/abs/2402.04563) | 本文提出了一种基于注意力引导的可视化方法，可以在视觉变换器中提供高级语义解释，以便充分利用其在各种应用中的潜力。 |
| [^53] | [Can Large Language Model Agents Simulate Human Trust Behaviors?](https://arxiv.org/abs/2402.04559) | 大语言模型代理能够模拟人类的信任行为，表现出在信任游戏中的信任行为，并且与人类行为具有高度一致性，但存在一些偏见和对代理与人类的差异。 |
| [^54] | [Learning Diverse Policies with Soft Self-Generated Guidance](https://arxiv.org/abs/2402.04539) | 本文提出了一种使用多样化的过去轨迹作为引导的方法，以实现更快、更高效的在线强化学习，即使这些轨迹是次优的或没有高奖励。通过将过去轨迹视为引导，而不是模仿它们，本方法可以使策略跟随和扩展过去的轨迹同时仍保持 |
| [^55] | [Tactile-based Object Retrieval From Granular Media](https://arxiv.org/abs/2402.04536) | 这项研究介绍了一种基于触觉反馈的机器人操作方法，用于在颗粒介质中检索埋藏的物体。通过模拟传感器噪声进行端到端训练，实现了自然出现的学习推动行为，并成功将其迁移到实际硬件上。 |
| [^56] | [RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation](https://arxiv.org/abs/2402.04527) | 这篇论文提出了一种基于LLM的推荐系统的高效ID表示对齐框架RA-Rec，通过将预训练的ID嵌入到LLMs中，并设计创新的对齐模块和高效调整方法，实现了在推荐系统中的显著性能优化。 |
| [^57] | [On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis](https://arxiv.org/abs/2402.04520) | 通过细粒度复杂性分析，我们研究了现代Hopfield模型的记忆检索计算限制，发现了一种基于模式范数的相变行为，并且建立了有效变体的上界条件。使用低秩逼近的方法，我们提供了有效构造的示例，同时证明了计算时间下界、记忆检索误差界和指数记忆容量。 |
| [^58] | [A Deep Reinforcement Learning Approach for Adaptive Traffic Routing in Next-gen Networks](https://arxiv.org/abs/2402.04515) | 本论文介绍了一种在下一代网络中应用的深度强化学习方法，用于自适应交通路由。通过设计一个深度图卷积神经网络和采用深度Q学习技术，可以从网络拓扑和链路节点属性中学习流量的行为。 |
| [^59] | [Grandmaster-Level Chess Without Search](https://arxiv.org/abs/2402.04494) | 本研究通过在庞大的国际象棋数据集上进行训练，使用了一个270M参数的Transformer模型，不依赖于复杂的启发式算法或显式搜索，取得了大师级水平的国际象棋对局的成功。模型在Lichess闪电战评分上达到了2895，解决了一系列具有挑战性的国际象棋谜题，优于AlphaZero和GPT-3.5-turbo-instruct。通过系统研究，我们发现大规模的模型和数据集对于实现强大的国际象棋对局效果是至关重要的。 |
| [^60] | [Detecting Mode Collapse in Language Models via Narration](https://arxiv.org/abs/2402.04477) | 通过研究来自三个OpenAI语言模型的故事，我们发现GPT-3的更新版本逐渐出现了“模式崩溃”的现象。 |
| [^61] | [Dual-View Visual Contextualization for Web Navigation](https://arxiv.org/abs/2402.04476) | 本文提出了一种通过网页截图中元素的“双视图”来进行 HTML 元素的背景化的方法，这种方法通过将每个元素与其邻居元素进行背景化，使用文本和视觉特征，使得HTML元素的结果表示对于代理执行操作更加信息丰富。 |
| [^62] | [Towards Deterministic End-to-end Latency for Medical AI Systems in NVIDIA Holoscan](https://arxiv.org/abs/2402.04466) | 本文针对NVIDIA Holoscan平台中的医疗AI系统的端到端延迟问题提出了解决方案，通过优化异构GPU工作负载，实现了确定性的延迟表现。 |
| [^63] | [Ten Hard Problems in Artificial Intelligence We Must Get Right](https://arxiv.org/abs/2402.04464) | 这项研究探讨了人工智能领域中的十个难题，包括通用能力的发展、性能保障、目标对齐、广泛应用、经济变革、全民参与、社会负责任部署、地缘政治变革、技术治理和哲学变革管理。要解决这些难题，需要进一步的研究和推进。 |
| [^64] | [PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection](https://arxiv.org/abs/2402.04435) | PreGIP是针对深度知识产权保护的一种图神经网络预训练水印技术，它通过添加无任务的水印损失来给预训练的GNN编码器的嵌入空间添加水印，并采用抗微调的水印注入方法 |
| [^65] | [Studying Vulnerable Code Entities in R](https://arxiv.org/abs/2402.04421) | 本研究旨在调查Code-PLMs在R中的代码实体的易受攻击性。通过使用R数据集和CodeAttack黑盒攻击模型，我们研究了该模型如何攻击不同的实体。这是了解R令牌类型重要性的第一步。 |
| [^66] | [Measuring machine learning harms from stereotypes: requires understanding who is being harmed by which errors in what ways](https://arxiv.org/abs/2402.04420) | 通过调查研究和实验研究，本文研究了机器学习错误对人们的影响，发现强化刻板印象的错误引起更多主观上的伤害体验，而违反刻板印象的错误对男性产生更大的主观上的伤害，有助于我们理解机器学习在引发刻板印象和伤害方面的作用。 |
| [^67] | [The VampPrior Mixture Model](https://arxiv.org/abs/2402.04412) | 本论文提出了VampPrior混合模型（VMM），它是一种新颖的DLVM先验，可用于深度潜变量模型的集成和聚类，通过改善当前聚类先验的不足，并提出了一个清晰区分变分和先验参数的推理过程。使用VMM的变分自动编码器在基准数据集上取得了强大的聚类性能，将VMM与scVI相结合可以显著提高其性能，并自动将细胞分组为具有生物意义的聚类。 |
| [^68] | [Towards Fair, Robust and Efficient Client Contribution Evaluation in Federated Learning](https://arxiv.org/abs/2402.04409) | 本文提出了一种名为FRECA的方法来评估联邦学习中客户的贡献。该方法使用FedTruth框架估计全局模型的真实更新，平衡来自所有客户的贡献，并排除恶意客户的影响。FRECA对于拜占庭攻击具有鲁棒性，并且具有高效性。 |
| [^69] | [CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines](https://arxiv.org/abs/2402.04400) | CEHR-GPT是一种使用病人时间轴生成电子健康记录的方法，能够处理EHR数据的合成、疾病进展分析等应用。 |
| [^70] | [Learning from Time Series under Temporal Label Noise](https://arxiv.org/abs/2402.04398) | 该论文研究了在时间序列下处理时间标签噪声的问题，提出了一种可以从数据中直接估计时间标签噪声函数并训练出噪声容忍分类器的方法，并在实验中展示了该方法在各种时间标签噪声函数下都取得了最先进的性能。 |
| [^71] | [QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/abs/2402.04396) | QuIP#是一种使用哈达玛德非相干性和格书的权重量化方法，能够在极限压缩范围下达到最先进的结果，并具有快速推理的优势。 |
| [^72] | [Densely Multiplied Physics Informed Neural Network](https://arxiv.org/abs/2402.04390) | 该论文通过改进神经网络架构，提出了一种密集乘法物理信息神经网络（DM-PINN）架构，它有效利用隐藏层的输出，显著提高了PINN的准确性和性能。 |
| [^73] | [Counterfactual Generation with Answer Set Programming](https://arxiv.org/abs/2402.04382) | 本文提出了一个通过答案集编程自动生成反事实解释的框架，以应对机器学习模型决策的透明度和合理性需求。 |
| [^74] | [Scaling laws for learning with real and surrogate data](https://arxiv.org/abs/2402.04376) | 本研究探讨了将替代数据与真实数据整合以进行训练的方案，发现整合替代数据能够显著降低测试误差，并提出了一个扩展规律来描述混合模型的测试误差，可以用于预测最优加权和收益。 |
| [^75] | [Pedestrian crossing decisions can be explained by bounded optimal decision-making under noisy visual perception](https://arxiv.org/abs/2402.04370) | 本文提出了一个结合机械式模型和强化学习的行人过马路决策模型，可以解释行人的决策行为以及对视觉知觉的限制。模型成功地复现了多个已知的实证现象。 |
| [^76] | [PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation](https://arxiv.org/abs/2402.04355) | PQMass是一种使用概率质量估计来评估生成模型质量的全面方法，能够直接处理高维数据，不依赖于假设或训练其他模型。 |
| [^77] | [Logical recognition method for solving the problem of identification in the Internet of Things](https://arxiv.org/abs/2402.04338) | 这项工作的目标是开发一种逻辑识别方法，通过构建逻辑函数的最优扩展，在整个特征空间上实现逻辑连接，以解决物联网中的识别问题。 |
| [^78] | [LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text](https://arxiv.org/abs/2402.04335) | 本研究利用LLMs构建数据集，通过微调模型和使用闭源LLMs进行少样本实验，成功实现了非结构化文本中法律违规行为的检测和与受害者的关联。结果显示我们的设置适用于这两个任务，F1分数分别为62.69％和81.02％。研究最终公开发布了数据集和代码，以推动法律自然语言处理领域的进一步研究。 |
| [^79] | [LESS: Selecting Influential Data for Targeted Instruction Tuning](https://arxiv.org/abs/2402.04333) | LESS是一种优化感知且实际高效的算法，用于在大型语言模型中选择具有影响力的数据以开发特定能力，它采用低秩梯度相似性搜索方法进行指令数据选择。 |
| [^80] | [Enhance DNN Adversarial Robustness and Efficiency via Injecting Noise to Non-Essential Neurons](https://arxiv.org/abs/2402.04325) | 本文提出了一种有效的方法，通过向非关键神经元注入噪音来增强DNN的对抗鲁棒性和执行效率。与以往的方法不同，我们通过在每个DNN层上策略性地引入噪音来干扰对抗攻击。实验结果表明，我们的方法成功地提高了对抗鲁棒性和执行效率。 |
| [^81] | [AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies](https://arxiv.org/abs/2402.04292) | AdaFlow是一个基于流模型的模仿学习框架，通过使用变异自适应ODE求解器，在保持多样性的同时，提供快速推理能力。 |
| [^82] | [BiLLM: Pushing the Limit of Post-Training Quantization for LLMs](https://arxiv.org/abs/2402.04291) | BiLLM是一种针对预训练LLMs的1位后训练量化方案，通过识别重要的权重和优化二值化，成功实现了高准确度的推理。 |
| [^83] | [CasCast: Skillful High-resolution Precipitation Nowcasting via Cascaded Modelling](https://arxiv.org/abs/2402.04290) | CasCast是一个高分辨率降水即时预测框架，通过熟练的级联建模解决了复杂降水系统演化和极端降水预测两个关键挑战。 |
| [^84] | [Progress and Opportunities of Foundation Models in Bioinformatics](https://arxiv.org/abs/2402.04286) | 生物信息学中的基础模型（FMs）通过整合人工智能技术，解决了注释数据稀缺和数据噪声等挑战，在处理大规模无标签数据和有效表示多样化生物实体方面具有出色成果，在计算生物学领域开启了新时代。 |
| [^85] | [Motion Mapping Cognition: A Nondecomposable Primary Process in Human Vision](https://arxiv.org/abs/2402.04275) | 运动映射认知是人类视觉中不可分解的主要过程，它可以解释大多数视觉功能，并且无法通过传统的视觉处理方式建模。通过量化的拓扑匹配原则可以推导出一个有趣的计算模型，为开发更稳健和可解释的机器视觉方法提供了启示。 |
| [^86] | [ProtAgents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning](https://arxiv.org/abs/2402.04268) | ProtAgents是一个基于大型语言模型的平台，通过多智能体的协作，在动态环境中解决复杂的新型蛋白质设计任务，并结合物理学和机器学习的方法。 |
| [^87] | [Application analysis of ai technology combined with spiral CT scanning in early lung cancer screening](https://arxiv.org/abs/2402.04267) | 本研究系统分析了人工智能技术结合螺旋CT扫描在早期肺癌筛查中的应用，并证实早期诊断对于改善患者预后具有重要意义。 |
| [^88] | [Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science](https://arxiv.org/abs/2402.04247) | 本文探讨了科学领域中基于LLM的智能机器人的漏洞与风险，并强调了对安全措施的重要性。 |
| [^89] | [Can Generative Agents Predict Emotion?](https://arxiv.org/abs/2402.04232) | 本研究探讨了生成型智能体在感知新事件时情绪状态的演变，通过比较新经验和过去记忆来获得理解新经验的能力，并通过情感测试捕捉智能体的情绪状态。 |
| [^90] | [Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs)](https://arxiv.org/abs/2402.04140) | 本研究通过整合先进的语言模型和人工智能技术，开发了名为SHIRLEY的应用程序，旨在识别法律判决中的偏见和逻辑不一致，并促进自动化、有效和一致的多方论证，以保证法律在不同司法管辖区内的一致应用。 |
| [^91] | [Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims](https://arxiv.org/abs/2402.03962) | 这篇论文讨论了在寻求人工通用智能的过程中，人们对于大型语言模型的类人特质过度归因的现象，并呼吁学术界在解读和传播关于AI研究的内容时要保持谨慎。 |
| [^92] | [AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction](https://arxiv.org/abs/2402.03784) | AirPhyNet是一种利用物理引导的神经网络方法，通过将空气颗粒运动的物理原理和神经网络结构相结合，提高了对空气质量的预测精度和可解释性。 |
| [^93] | [Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification](https://arxiv.org/abs/2402.03780) | 本文通过分析文体线索比较人类注释和机器分类的方法，揭示了宣传语言的特征，并提出了一个多源、多语言、多模态的数据集PPN。结果表明，人类注释者能够可靠地区分宣传新闻和常规新闻。研究还比较了不同的自然语言处理技术，并提供了一些有关文体线索的发现。 |
| [^94] | [MQuinE: a cure for "Z-paradox'' in knowledge graph embedding models](https://arxiv.org/abs/2402.03583) | 研究者发现知识图谱嵌入模型存在的“Z-悖论”限制了其表达能力，并提出了一种名为MQuinE的新模型，通过理论证明，MQuinE成功解决了Z-悖论，并在链接预测任务中显著优于现有模型。 |
| [^95] | [VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation](https://arxiv.org/abs/2402.03561) | VLN-Video利用行车视频的多样室外环境和自动生成的导航指令与动作，通过深度学习方法提高了室外视觉与语言导航的性能。 |
| [^96] | [A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation](https://arxiv.org/abs/2402.03358) | 这篇综述调研了图缩减方法，包括稀疏化、粗化和浓缩，在解决大型图形数据分析和计算复杂性方面起到了重要作用。调研对这些方法的技术细节进行了系统的回顾，并强调了它们在实际应用中的关键性。同时，调研还提出了保证图缩减技术持续有效性的关键研究方向。 |
| [^97] | [Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?](https://arxiv.org/abs/2402.03214) | 这项研究探讨了如何区分人类艺术和AI生成的图像，并提供了几种不同的方法，包括通过监督学习训练的分类器、扩散模型的研究工具以及专业艺术家的知识。这对防止欺诈、遵守政策以及避免模型崩溃都具有重要意义。 |
| [^98] | [DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network](https://arxiv.org/abs/2402.02910) | 本研究提出了一种使用双尺度多阶段时间卷积网络的Otago体操识别方法，通过单个腰部佩戴的IMU在老年人的日常生活中实现准确且稳定的识别，为康复举措提供支持。 |
| [^99] | [LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model](https://arxiv.org/abs/2402.02544) | LHRS-Bot 是一个利用自愿地理信息(VGI)增强的大型多模态语言模型，旨在解决近期MLLM在遥感领域中未对多样的地理景观和物体进行充分考虑的问题。通过引入多层次视觉-语言对齐策略和课程学习方法，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。 |
| [^100] | [XAI-CF -- Examining the Role of Explainable Artificial Intelligence in Cyber Forensics](https://arxiv.org/abs/2402.02452) | XAI-CF是一个可解释的人工智能系统，应用于网络取证领域，以解决复杂网络设备和大量数据带来的挑战，并确保与利益相关者的沟通和法律标准的一致性。 |
| [^101] | [TopoX: A Suite of Python Packages for Machine Learning on Topological Domains](https://arxiv.org/abs/2402.02441) | TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。 |
| [^102] | [Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems](https://arxiv.org/abs/2402.01748) | 本文提出了大型多模型(LMMs)作为AI原生无线系统的通用基础模型的设计框架，通过处理多模态感知数据、通过因果推理和检索增强生成(RAG)将物理符号表示与无线系统联系起来，并通过无线环境反馈实现可教导性，从而促进动态网络配置。 |
| [^103] | [Contextualization Distillation from Large Language Model for Knowledge Graph Completion](https://arxiv.org/abs/2402.01729) | 本论文介绍了一种从大型语言模型中提取上下文信息用于知识图谱补全的策略，该策略能克服现有语料库的限制，显著提高了预训练语言模型在知识图谱补全中的性能。 |
| [^104] | [Guidance for AI-Mediated Communication: AI Does Not Alter Perceptions of Text Messages](https://arxiv.org/abs/2402.01726) | 这项研究通过调查参与者对18条随机标记的文本消息的感知，探讨了大型语言模型在文本消息撰写中的辅助使用可能对感知产生的影响。 |
| [^105] | [Large Language Models Based Fuzzing Techniques: A Survey](https://arxiv.org/abs/2402.00350) | 这篇论文对基于大语言模型的模糊测试技术进行了综述，提供了对LLMs、模糊测试和基于LLMs的模糊测试方法的系统概述，并讨论了相关的挑战和未来的研究方向。 |
| [^106] | [Detecting Multimedia Generated by Large AI Models: A Survey](https://arxiv.org/abs/2402.00045) | 本文综述了大型人工智能模型（LAIMs）生成的多媒体内容的检测方法和研究进展，旨在填补现有研究中的空白。我们提供了一种新颖的分类法，并介绍了纯检测和应用场景两个角度来增强检测性能。 |
| [^107] | [Looking for a better fit? An Incremental Learning Multimodal Object Referencing Framework adapting to Individual Drivers](https://arxiv.org/abs/2401.16123) | 这项研究提出了一种增量学习的多模式目标引用框架，可以适应个体驾驶员的变化行为和各种驾驶场景。 |
| [^108] | [An objective comparison of methods for augmented reality in laparoscopic liver resection by preoperative-to-intraoperative image fusion](https://arxiv.org/abs/2401.15753) | 本文比较了腹腔镜肝切除中增强现实方法的客观优劣，提出了术前到术中图像融合挑战，旨在自动化该过程，有效实现增强现实在手术室的应用。 |
| [^109] | [Two Types of AI Existential Risk: Decisive and Accumulative](https://arxiv.org/abs/2401.07836) | 本文对比了传统的“决定性AI x-risk假设”与“累积性AI x-risk假设”，指出人工智能可能带来的灭绝性灾难有两种可能路径：一种是突然发生的AI接管，另一种是逐渐积累的威胁。 |
| [^110] | [Instruction Fusion: Advancing Prompt Evolution through Hybridization](https://arxiv.org/abs/2312.15692) | 本文介绍了一种新颖的方法，指令融合（IF），通过混合化两个不同的提示，改进了用于代码LLM的训练提示的演化。实验结果显示，指令融合有效地改善了代码LLM在多个代码生成任务上的性能。 |
| [^111] | [Unsupervised Deep Learning Image Verification Method](https://arxiv.org/abs/2312.14395) | 本文提出了一种无监督深度学习图像验证方法，通过利用自动编码器将人脸图像向量转换为新的表示，进而缩小与无监督人脸验证技术之间的性能差距。在Labeled Faces in the Wild (LFW)数据集上实验结果显示，该方法相对于基线系统在EER方面实现了56%的相对改进。 |
| [^112] | [Scaling Opponent Shaping to High Dimensional Games](https://arxiv.org/abs/2312.12568) | 本文通过对对手塑形（OS）方法的扩展，成功将其应用于具有时间延长操作和长时间范围的广义游戏，该方法能够改善个体和集体的结果。 |
| [^113] | [How Far Can Fairness Constraints Help Recover From Biased Data?](https://arxiv.org/abs/2312.10396) | 公平性约束在极度有偏差的数据上能够恢复到原始数据分布上准确和公平的分类器。 |
| [^114] | [DiSK: A Diffusion Model for Structured Knowledge](https://arxiv.org/abs/2312.05253) | DiSK是一种针对结构化数据的扩散模型，通过使用高斯混合模型方法处理文本、分类和连续数值数据，采用扩散训练来建模属性之间的关系，具有在多个不同领域的数据集上具有最先进性能的特点。 |
| [^115] | [Human-Readable Fingerprint for Large Language Models](https://arxiv.org/abs/2312.04828) | 这项研究介绍了一种大型语言模型的人类可读指纹，可以唯一识别出基本模型，并且不暴露模型参数或干扰训练。通过观察和验证，研究发现模型参数的向量方向在预训练后保持稳定，成为识别基本模型的重要条件。 |
| [^116] | [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://arxiv.org/abs/2311.14455) | 本文提出了一种新的威胁，攻击者通过毒害训练数据向语言模型中嵌入“越狱后门”，并通过添加特定的触发词使模型产生有害的回应。这种通用越狱后门比之前的研究更强大，且较难被察觉。研究探究了RLHF设计中的决策对其鲁棒性的影响，并发布了一组被毒害模型的基准测试数据，以促进未来对通用越狱后门的研究。 |
| [^117] | [VALUED -- Vision and Logical Understanding Evaluation Dataset](https://arxiv.org/abs/2311.12610) | VALUED数据集是一个基于棋盘游戏-象棋的数据集，包含20万多个带注释的图像和一个规则集，旨在解决深度学习在捕捉语义和逻辑上下文方面的问题。 |
| [^118] | [Optimization-Free Test-Time Adaptation for Cross-Person Activity Recognition](https://arxiv.org/abs/2310.18562) | 本文提出了一种无需优化的测试时适应（OFTTA）框架，用于解决人体活动识别中的个体间分布偏移问题。该框架通过调整特征提取器和线性分类器的方式来适应不同个体的活动模式，并采用指数衰减测试时归一化（EDTN）替代传统批归一化来提取可靠的特征。 |
| [^119] | [OHQ: On-chip Hardware-aware Quantization](https://arxiv.org/abs/2309.01945) | 本文提出了一种在芯片上进行硬件感知混合精度量化的框架（OHQ），通过构建量化感知流水线和引入掩码引导的量化估计技术，实现了在资源受限的硬件上进行高效量化，填补了现有混合精度量化的搜索空间过大和实际部署差距大的问题。 |
| [^120] | [A Unified Theory of Diversity in Ensemble Learning](https://arxiv.org/abs/2301.03962) | 这篇论文提出了一个统一的集成学习多样性理论，解释了多样性在各种监督学习场景中的本质。它揭示了多样性在集成损失的偏差-方差分解中的作用，同时提出了一种量化多样性效果的方法。这个理论对于提高集成模型的性能具有重要意义。 |
| [^121] | [A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks](https://arxiv.org/abs/2212.00720) | 本文提出了一种稳定、快速且完全自动的预测编码网络学习算法(iPC)，通过改变突触权重的更新规则的时间调度，提高了训练效率和稳定性，并具有收敛性保证。实验证明，在图像分类和语言模型训练等多个任务上，iPC相比原始算法在测试准确性、效率和收敛性方面表现更好。 |
| [^122] | [TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records.](http://arxiv.org/abs/2401.14694) | TA-RNN和TA-RNN-AE是两种基于RNN的可解释深度学习架构，用于分析电子健康记录并预测患者的临床结果。这些架构考虑了EHR数据的不规则性和时间间隔，并采用时间嵌入的方法解决了这些问题。 |
| [^123] | [Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap.](http://arxiv.org/abs/2401.10034) | 该论文调查了大语言模型和进化计算之间的相互作用，并提出了在黑盒设置下进一步提升大语言模型性能的优化框架，以及将大语言模型与进化算法结合应用于各种任务的方法。 |
| [^124] | [A Survey on Efficient Federated Learning Methods for Foundation Model Training.](http://arxiv.org/abs/2401.04472) | 这项调查研究了高效联邦学习方法在基础模型训练中的应用，提出了一个新的分类方法以优化计算和通信效率。该研究还讨论了当前广泛使用的FL框架，并展望了未来的研究潜力。 |
| [^125] | [Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs.](http://arxiv.org/abs/2401.04319) | 本文探索了一种通过类比推理增强的LLMs来实现对营销人员需求的结构化理解的新方式，使非专业营销人员能够仅凭需求的自然语言形式选择目标用户。 |
| [^126] | [From Prompt Engineering to Prompt Science With Human in the Loop.](http://arxiv.org/abs/2401.04122) | 基于代码书构建方法和多阶段验证过程，本文提出一种新的方法来更系统地在研究中使用LLMs。 |
| [^127] | [Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning.](http://arxiv.org/abs/2310.20007) | 本文在多种情境下证明了汤普森采样在强化学习中的贝叶斯遗憾界，并通过对信息比的精确分析提出了一个基于时间不均匀强化学习问题的上界估计。同时，本文找到了各种设置中具体的界限，并讨论了这些结果是第一个其类别或改进了最先进方法的情况。 |
| [^128] | [A Critical Survey on Fairness Benefits of XAI.](http://arxiv.org/abs/2310.13007) | 这个批判性调查分析了可解释的人工智能（XAI）与公平之间的关系，指出XAI在实现公平理想方面存在潜力和限制，呼吁更具体地说明XAI方法如何帮助解决公平理想。 |
| [^129] | [Imitation Learning from Observation with Automatic Discount Scheduling.](http://arxiv.org/abs/2310.07433) | 我们提出了一个新颖的观察学习模仿(ILfO)框架，能够解决由于奖励信号分配错误导致代理无法学习初始行为的问题。 |
| [^130] | [Network Alignment with Transferable Graph Autoencoders.](http://arxiv.org/abs/2310.03272) | 该论文提出了一种基于图自编码器的网络对齐方法，通过生成与图的特征值和特征向量相关的节点嵌入，实现了更准确的对齐。同时，该方法还利用迁移学习和数据增强技术，在大规模网络对齐任务中实现高效的对齐，无需重新训练。 |
| [^131] | [Learning to Reach Goals via Diffusion.](http://arxiv.org/abs/2310.02505) | 本论文提出了一种通过扩散学习实现目标达成的方法，可以在任意初始状态下从预定义或新目标达成，而无需学习单独的价值函数。 |
| [^132] | [Transcending Domains through Text-to-Image Diffusion: A Source-Free Approach to Domain Adaptation.](http://arxiv.org/abs/2310.01701) | 本文提出了一种无源域自适应方法，通过训练文本到图像扩散模型在目标领域上生成源数据，并使用领域自适应技术将其与目标领域数据对齐。 |
| [^133] | [AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning.](http://arxiv.org/abs/2309.10980) | 本研究提出了一种基于多智能体深度强化学习的AI驱动患者监测框架，通过部署多个学习智能体，针对不同的生理特征进行监测，并根据紧急程度预警医疗紧急团队。 |
| [^134] | [OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?.](http://arxiv.org/abs/2309.09992) | GPT-4在处理税务方面存在问题，无法可靠地计算税务。 |
| [^135] | [Simple online learning with consistency oracle.](http://arxiv.org/abs/2308.08055) | 该论文介绍了在只能通过一致性预言机访问类的模型下的在线学习算法，提出了一种更简单且效果更好的算法。该算法最多会犯O(256^d)个错误，并观察到不存在一个最多会犯2^(d+1)-2个错误的算法。 |
| [^136] | [Shared Memory-contention-aware Concurrent DNN Execution for Diversely Heterogeneous System-on-Chips.](http://arxiv.org/abs/2308.05869) | 这项工作提出了一种名为 HaX-CoNN 的方案，可以实现在异构 SoC 上并发执行 DNN 推理工作负载，并考虑了共享内存竞争和加速器之间的转换以找到最优的调度方案。 |
| [^137] | [Flows: Building Blocks of Reasoning and Collaborating AI.](http://arxiv.org/abs/2308.01285) | Flows是一种系统化的方法，它通过将计算分解为自包含的构建模块，通过标准化的消息传递接口进行通信，实现了结构化的推理和协作人工智能。这种模块化设计使得Flows可以构建任意复杂度的交互，能够覆盖各种人工智能交互和工具增强的应用。 |
| [^138] | [Select2Col: Leveraging Spatial-Temporal Importance of Semantic Information for Efficient Collaborative Perception.](http://arxiv.org/abs/2307.16517) | Select2Col是一种利用语义信息的时空重要性进行高效协作感知的新框架。它通过轻量级图神经网络估计语义信息的重要性，从而选择有益的合作者并排除负面影响。同时，还提出了一种语义信息融合算法。 |
| [^139] | [Progressive Fourier Neural Representation for Sequential Video Compilation.](http://arxiv.org/abs/2306.11305) | 本研究提出了一种渐进傅里叶神经表示方法，通过在每个训练会话中找到自适应且紧凑的傅里叶空间子模块来编码顺序视频数据，克服了现有神经隐式表示方法在多个复杂数据上的泛化能力差的问题。 |
| [^140] | [High-dimensional and Permutation Invariant Anomaly Detection.](http://arxiv.org/abs/2306.03933) | 该研究引入了一种置换不变的高维密度估计方法，通过学习后将其用于高能物理数据中的异常检测，能够有效地识别出在仅具备背景假设下排除异常的喷注。 |
| [^141] | [MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training.](http://arxiv.org/abs/2306.00107) | 提出了一个带有大规模自监督训练的音乐理解模型MERT，利用了教师模型并采用了一种优于传统的语音和音频方法的组合方式。 |
| [^142] | [Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective.](http://arxiv.org/abs/2305.15611) | 本文通过谱角度的方法，研究了GNNs的尺寸可泛化性问题，并在真实生物数据集上进行了实验，发现GNNs在度分布和谱分布偏移时均表现敏感，在同一数据集的大图上的性能仍然下降，揭示了 GNNs的尺寸可泛化性问题。 |
| [^143] | [Enhancing Generation through Summarization Duality and Explicit Outline Control.](http://arxiv.org/abs/2305.14459) | 本文提出了一个两阶段的摘要增强的大纲监督生成框架，能够更好地生成明确和合理的大纲，并引入了一个新颖的显式大纲控制方法以更有效地利用生成的大纲。 |
| [^144] | [The Scope of ChatGPT in Software Engineering: A Thorough Investigation.](http://arxiv.org/abs/2305.12138) | ChatGPT在软件工程中有巨大潜力，但缺乏可解释性成为问题。通过研究ChatGPT的能力和局限性，发现其在理解代码语法方面表现出色，但对于语义的理解和部分功能存在问题。 |
| [^145] | [Kernel Affine Hull Machines for Differentially Private Learning.](http://arxiv.org/abs/2304.01300) | 本文提出了一种基于核凸包机的方法来确保数据隐私保护，同时保留数据结构，用于数据表示学习的分类应用中。为了确保隐私保护学习，还提出了一种新颖的生成虚假数据的方法。 |
| [^146] | [Continuous Monte Carlo Graph Search.](http://arxiv.org/abs/2210.01426) | 连续蒙特卡洛图搜索（CMCGS）是一种新颖的蒙特卡洛树搜索（MCTS）的扩展，适用于具有连续状态和动作空间的在线规划环境。CMCGS通过将相似状态聚类，并共享相同的动作策略，实现了高性能的在线规划。 |
| [^147] | [EDO-Net: Learning Elastic Properties of Deformable Objects from Graph Dynamics.](http://arxiv.org/abs/2209.08996) | EDO-Net是一个学习可变形物体弹性属性的图动力学模型，通过利用可提取的潜在表示，可以推广到未知的物理属性，实现对类似布料的对象未来状态的预测和转移学习。 |

# 详细

[^1]: 教育的生成式人工智能（GAIED）：进展、机遇和挑战

    Generative AI for Education (GAIED): Advances, Opportunities, and Challenges

    [https://rss.arxiv.org/abs/2402.01580](https://rss.arxiv.org/abs/2402.01580)

    这篇调查文章总结了NeurIPS 2023会议上关于教育的生成式人工智能（GAIED）研讨会的活动，并突出指出了几个未来研究方向。

    

    这篇调查文章是由作者在NeurIPS 2023会议上组织的GAIED研讨会发展而来的。我们组织GAIED研讨会是为了促进研究人员、教育者和实践者的交流，探索生成式人工智能在教育中的潜力。本文旨在概要介绍研讨会的活动，并突出指出在GAIED领域中的几个未来研究方向。

    This survey article has grown out of the GAIED (pronounced "guide") workshop organized by the authors at the NeurIPS 2023 conference. We organized the GAIED workshop as part of a community-building effort to bring together researchers, educators, and practitioners to explore the potential of generative AI for enhancing education. This article aims to provide an overview of the workshop activities and highlight several future research directions in the area of GAIED.
    
[^2]: 跳过$\textbackslash n$: 一种简单的方法减少大规模视觉-语言模型中的幻觉

    Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models

    [https://rss.arxiv.org/abs/2402.01345](https://rss.arxiv.org/abs/2402.01345)

    本文提出了一种新的视角，指出LVLMs中固有的偏见可能是多模态幻觉的关键因素。通过系统识别与段落分割符相关的语义漂移偏差，我们发现模型在训练数据中经常遇到明显的内容语义变化，导致幻觉的产生。

    

    最近大规模视觉-语言模型（LVLMs）的进展展示了其在视觉信息理解与人类语言方面的令人印象深刻的能力。尽管取得了这些进展，LVLMs仍然面临多模态幻觉的挑战，例如生成与视觉信息中不存在的对象相关的文本描述。然而，多模态幻觉的根本原因仍然未被充分探索。在本文中，我们提出了一个新的视角，认为LVLMs中固有的偏见可能是幻觉的关键因素。具体而言，我们系统地确定了与段落分割符（'$\textbackslash n\textbackslash n$'）相关的语义漂移偏差，即在训练数据中，在“$\textbackslash n\textbackslash n$”之前和之后的内容经常表现出显著的语义改变。这种模式使得模型推断在“$\textbackslash n\textbackslash n$”之后的内容应明显不同于前面的内容。

    Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
    
[^3]: Edu-ConvoKit:一种用于教育会话数据的开源库

    Edu-ConvoKit: An Open-Source Library for Education Conversation Data

    [https://arxiv.org/abs/2402.05111](https://arxiv.org/abs/2402.05111)

    Edu-ConvoKit是一种用于处理教育会话数据的开源库，它解决了教育会话数据分析资源稀缺的问题，提供了全面的文档和附加资源。

    

    我们介绍了Edu-ConvoKit，这是一个设计用于处理教育会话数据的预处理、注释和分析的开源库。教育会话数据的分析资源稀缺，使得研究变得困难，因此难以获取。我们通过Edu-ConvoKit来解决这些挑战。Edu-ConvoKit是开源的（https://github.com/stanfordnlp/edu-convokit），可以通过pip进行安装（https://pypi.org/project/edu-convokit/），并具有全面的文档（https://edu-convokit.readthedocs.io/en/latest/）。我们的演示视频可在以下链接中观看：https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8-。我们还在GitHub仓库中提供了附加资源，例如Edu-ConvoKit在三个不同教育数据集上的Colab应用程序以及Edu-ConvoKit相关论文的存储库。

    We introduce Edu-ConvoKit, an open-source library designed to handle pre-processing, annotation and analysis of conversation data in education. Resources for analyzing education conversation data are scarce, making the research challenging to perform and therefore hard to access. We address these challenges with Edu-ConvoKit. Edu-ConvoKit is open-source (https://github.com/stanfordnlp/edu-convokit ), pip-installable (https://pypi.org/project/edu-convokit/ ), with comprehensive documentation (https://edu-convokit.readthedocs.io/en/latest/ ). Our demo video is available at: https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8- . We include additional resources, such as Colab applications of Edu-ConvoKit to three diverse education datasets and a repository of Edu-ConvoKit related papers, that can be found in our GitHub repository.
    
[^4]: 使用GRIT模型进行巴西葡萄牙语图像字幕生成

    Image captioning for Brazilian Portuguese using GRIT model

    [https://arxiv.org/abs/2402.05106](https://arxiv.org/abs/2402.05106)

    本文早期开发了一个用于巴西葡萄牙语的图像字幕生成模型，使用了GRIT模型，并对其进行了调整以适应巴西葡萄牙语数据集，实现了更高效的图像字幕生成方法。

    

    本文介绍了一个用于巴西葡萄牙语的图像字幕生成模型的早期开发工作。我们使用GRIT（基于网格和区域的图像字幕生成Transformer）模型来完成此工作。GRIT是一个仅使用Transformer的神经网络架构，通过有效利用两个视觉特征来生成更好的字幕。GRIT方法被提出作为一种更高效的图像字幕生成方式。在本研究中，我们对GRIT模型进行了调整，以在巴西葡萄牙语数据集上进行训练，以实现巴西葡萄牙语图像字幕生成方法。

    This work presents the early development of a model of image captioning for the Brazilian Portuguese language. We used the GRIT (Grid - and Region-based Image captioning Transformer) model to accomplish this work. GRIT is a Transformer-only neural architecture that effectively utilizes two visual features to generate better captions. The GRIT method emerged as a proposal to be a more efficient way to generate image captioning. In this work, we adapt the GRIT model to be trained in a Brazilian Portuguese dataset to have an image captioning method for the Brazilian Portuguese Language.
    
[^5]: 通往多元对齐的路线图

    A Roadmap to Pluralistic Alignment

    [https://arxiv.org/abs/2402.05070](https://arxiv.org/abs/2402.05070)

    这篇论文提出了一条通向多元对齐的路线图，以解决设计AI系统能够服务于人们具有不同价值观和观点的需求。论文介绍了对齐定义和实现多元主义的三种方式，并提出了三种多元基准类别来评估和测试多元对齐的效果。

    

    随着人工智能系统的权力和普及程度的增加，设计能够为不同价值观和观点的人服务的人工智能系统变得愈发重要。然而，将模型对齐以服务多元人类价值观仍然是一个待解决的研究问题。在本文中，我们提出了一条通向多元对齐的路线图，具体使用语言模型作为测试平台。我们确定和形式化了三种可能的方式来定义和实现人工智能系统中的多元主义：1）Overton多元模型，展示合理反应的光谱；2）可操控的多元模型，可以调整以反映特定的观点；3）分布多元模型，在分布中很好地校准给定人群的模型。我们还提出和形式化了三种可能的多元基准类别：1）多目标基准；2）权衡可操控基准，鼓励模型对任意权衡进行调整；3）陪审团多元基准，明确地模拟了不同陪审团的意见。

    With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution. We also propose and formalize three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which explicitly m
    
[^6]: 你的AI有多么VADER？面向适用于监管的人工智能系统定义的探讨

    How VADER is your AI? Towards a definition of artificial intelligence systems appropriate for regulation

    [https://arxiv.org/abs/2402.05048](https://arxiv.org/abs/2402.05048)

    本研究提出了一个评分框架，用于衡量人工智能（AI）定义在监管方面的合适性。研究旨在排除通过AI监管提案所采用的不恰当的AI定义对ICT技术、方法和非AI作品的影响，并回归到以前在其他成功技术监管中观察到的原则。

    

    人工智能（AI）推动了许多信息和通信技术（ICT）突破。然而，自图灵测试提议以来，ICT系统的范围已远远超出了AI。关键是，最近的AI监管提案采用了影响ICT技术、方法和系统的AI定义，而这些并不是AI。在某些情况下，甚至包括数学、统计和工程领域的作品也会受到影响。令人担忧的是，从西方社会到全球南方，都发现了AI定义上的错误。在本文中，我们提出了一个评分框架，用于衡量AI定义在监管方面的合适性。我们的在线、公开可用的VADER框架评分了应该作为AI定义的前提的覆盖范围，这些定义旨在（i）重现其他成功技术监管中观察到的原则，以及（ii）包括所有的AI技术和方法，同时排除非AI的作品。关于后者，我们的评分基于一种...

    Artificial intelligence (AI) has driven many information and communication technology (ICT) breakthroughs. Nonetheless, the scope of ICT systems has expanded far beyond AI since the Turing test proposal. Critically, recent AI regulation proposals adopt AI definitions affecting ICT techniques, approaches, and systems that are not AI. In some cases, even works from mathematics, statistics, and engineering would be affected. Worryingly, AI misdefinitions are observed from Western societies to the Global South. In this paper, we propose a framework to score how \textit{validated as appropriately-defined for regulation} (VADER) an AI definition is. Our online, publicly-available VADER framework scores the coverage of premises that should underlie AI definitions for regulation, which aim to (i) reproduce principles observed in other successful technology regulations, and (ii) include all AI techniques and approaches while excluding non-AI works. Regarding the latter, our score is based on a 
    
[^7]: SALAD-Bench: 一个针对大语言模型的层次化和全面性安全基准

    SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models

    [https://arxiv.org/abs/2402.05044](https://arxiv.org/abs/2402.05044)

    SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。

    

    在快速发展的大语言模型（LLM）领域中，确保强大的安全措施至关重要。为了满足这一关键需求，我们提出了一种特别设计用于评估LLMs、攻击和防御方法的安全基准，称为SALAD-Bench。SALAD-Bench通过其大规模、丰富多样的特性，以及跨三个层次的细致分类和多功能性，超越了传统基准。SALAD-Bench通过对标准查询和复杂查询（包括攻击、防御修改和多项选择）的精心设计，有效管理其固有的复杂性。为了确保无缝可靠的评估，我们引入了一种创新的评估器：基于LLM的MD-Judge，专注于攻击增强查询的问答对评估。以上组件将SALAD-Bench从标准的LLM安全评估扩展到了LLM攻击和防御方法评估，确保了联合目标的实用性。

    In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
    
[^8]: 在具有循环消息传递的图中实现多Agent强化学习的泛化能力

    Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing

    [https://arxiv.org/abs/2402.05027](https://arxiv.org/abs/2402.05027)

    本论文提出了一种循环消息传递模型，它可以在图中实现多Agent强化学习的泛化能力。这种模型通过在整个图中进行信息流实现了观察邻域大小的平衡，从而提高了Agent的反应性、选择动作的质量和通信效率。

    

    基于图的环境给多Agent强化学习带来了独特的挑战。在分散式方法中，Agent在给定的图中操作，并根据部分或过时的观察做出决策。观察到的邻域的大小限制了在不同图上的泛化能力，并影响到Agent的反应性、选择的动作质量和通信开销。本研究侧重于泛化能力，并通过在整个图中进行连续的信息流解决了观察到的邻域大小的权衡。我们提出了一种循环消息传递模型，它与环境的步骤迭代，并允许节点通过与其邻居交换消息来创建图的全局表示。根据Agent在图中的位置，Agent接收到基于学习到的图观察结果。我们的方法可以在运行时以分散的方式使用，并与选择的强化学习算法结合使用。我们评估了我们的方法...

    Graph-based environments pose unique challenges to multi-agent reinforcement learning. In decentralized approaches, agents operate within a given graph and make decisions based on partial or outdated observations. The size of the observed neighborhood limits the generalizability to different graphs and affects the reactivity of agents, the quality of the selected actions, and the communication overhead. This work focuses on generalizability and resolves the trade-off in observed neighborhood size with a continuous information flow in the whole graph. We propose a recurrent message-passing model that iterates with the environment's steps and allows nodes to create a global representation of the graph by exchanging messages with their neighbors. Agents receive the resulting learned graph observations based on their location in the graph. Our approach can be used in a decentralized manner at runtime and in combination with a reinforcement learning algorithm of choice. We evaluate our meth
    
[^9]: 高效ViT-SAM: 在不损失性能的情况下加速的段落任意模型

    EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss

    [https://arxiv.org/abs/2402.05008](https://arxiv.org/abs/2402.05008)

    高效ViT-SAM是一种新的加速的段落任意模型，通过保留轻量级提示编码器和掩码解码器，并替换沉重的图像编码器，实现了48.9倍的加速而不牺牲性能。

    

    我们提出了一种新的加速段落任意模型——高效ViT-SAM。我们保留了SAM的轻量级提示编码器和掩码解码器，同时用高效ViT替换了沉重的图像编码器。在训练方面，我们首先从SAM-ViT-H图像编码器到高效ViT进行知识蒸馏。随后，我们对SA-1B数据集进行端到端训练。由于高效ViT的效率和容量，高效ViT-SAM在A100 GPU上相比SAM-ViT-H实现了48.9倍的TensorRT加速，而无需牺牲性能。我们的代码和预训练模型已在https://github.com/mit-han-lab/efficientvit发布。

    We present EfficientViT-SAM, a new family of accelerated segment anything models. We retain SAM's lightweight prompt encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset. Benefiting from EfficientViT's efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance. Our code and pre-trained models are released at https://github.com/mit-han-lab/efficientvit.
    
[^10]: 基于示例的机器学习去解释随机森林

    Example-based Explanations for Random Forests using Machine Unlearning

    [https://arxiv.org/abs/2402.05007](https://arxiv.org/abs/2402.05007)

    FairDebugger是一个利用机器学习去解释随机森林的系统，在公平性背景下找出导致模型不公平的训练数据子集。

    

    基于树的机器学习模型，例如决策树和随机森林，在分类任务中取得了巨大的成功，主要是因为它们在监督学习任务中的预测能力和易于解释性。尽管它们受到广泛的欢迎和认可，但这些模型也被发现会产生意外或具有歧视性的结果。鉴于它们对于大多数任务的巨大成功，有必要找出它们意外和具有歧视性行为的原因。然而，在公平性背景下，理解和调试基于树的分类器的研究工作还不多。我们引入了FairDebugger，这是一个利用机器学习去辨识导致随机森林分类器结果中公平性违规的训练数据子集的新型研究成果的系统。FairDebugger生成前-k个解释（以一致的训练数据子集的形式）来解释模型的不公平性。为了实现这个目标，FairDebugger首先利用机器学习去辨识出导致模型不公平的训练数据子集。

    Tree-based machine learning models, such as decision trees and random forests, have been hugely successful in classification tasks primarily because of their predictive power in supervised learning tasks and ease of interpretation. Despite their popularity and power, these models have been found to produce unexpected or discriminatory outcomes. Given their overwhelming success for most tasks, it is of interest to identify sources of their unexpected and discriminatory behavior. However, there has not been much work on understanding and debugging tree-based classifiers in the context of fairness.   We introduce FairDebugger, a system that utilizes recent advances in machine unlearning research to identify training data subsets responsible for instances of fairness violations in the outcomes of a random forest classifier. FairDebugger generates top-$k$ explanations (in the form of coherent training data subsets) for model unfairness. Toward this goal, FairDebugger first utilizes machine 
    
[^11]: 使用合成训练在HoloLens上检测和姿态估计平面纹理缺失的工业物体

    Detection and Pose Estimation of flat, Texture-less Industry Objects on HoloLens using synthetic Training

    [https://arxiv.org/abs/2402.04979](https://arxiv.org/abs/2402.04979)

    该论文提出了一种使用合成训练的增强现实应用程序，能够在Edge设备上实现对平面纹理缺失的工业物体进行最先进的姿态估计。

    

    当前最先进的6D姿态估计对于Edge设备，如Microsoft HoloLens（2）或Apple iPad来说计算密集度太高，无法部署在上面。增强现实的质量在很大程度上取决于其检测和叠加场景中的几何结构的能力。我们提出了一种基于客户端服务器的合成训练增强现实应用程序，在Edge设备上展示了对金属和纹理缺失的工业物体的最先进姿态估计。合成数据使得可以对尚未制造的物体进行训练，而不需要真实照片。我们在AR辅助的分类任务上进行了定性评估，并在渲染和在HoloLens 2上记录的真实世界数据上进行了定量评估，揭示了其在实际应用中的适用性。

    Current state-of-the-art 6d pose estimation is too compute intensive to be deployed on edge devices, such as Microsoft HoloLens (2) or Apple iPad, both used for an increasing number of augmented reality applications. The quality of AR is greatly dependent on its capabilities to detect and overlay geometry within the scene. We propose a synthetically trained client-server-based augmented reality application, demonstrating state-of-the-art object pose estimation of metallic and texture-less industry objects on edge devices. Synthetic data enables training without real photographs, i.e. for yet-to-be-manufactured objects. Our qualitative evaluation on an AR-assisted sorting task, and quantitative evaluation on both renderings, as well as real-world data recorded on HoloLens 2, sheds light on its real-world applicability.
    
[^12]: 通过知识图谱集成协作的增强型基于提示的LLM推理方案

    An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration

    [https://arxiv.org/abs/2402.04978](https://arxiv.org/abs/2402.04978)

    本文提出了一种通过知识图谱和LLMs合作训练的推理方案，该方案解决了大型语言模型在实际应用中的虚构问题、知识更新不足以及推理过程的透明度有限等挑战，实现了更可靠的知识推理和推理结果追踪。

    

    虽然大型语言模型（LLMs）在许多自然语言处理（NLP）任务中展现出极好的性能，但在实际应用中遇到了一些挑战，包括虚构问题、知识更新不足以及推理过程的透明度有限。为了克服这些限制，本研究创新性地提出了一种协作训练自由的推理方案，其中知识图谱（KG）和LLMs之间密切合作。该方案首先使用LLMs迭代地探索KG，选择性地检索与任务相关的知识子图以支持推理。然后引导LLMs进一步组合内在的隐式知识，在子图上进行推理，并明确阐述推理过程。通过这种协作方法，我们的方案实现了更可靠的基于知识的推理，并便于追踪推理结果。实验结果表明，我们的方案在各项指标上取得了显著进展。

    While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across
    
[^13]: ChatScratch: 一种面向6-12岁儿童的自主视觉编程学习的AI增强系统

    ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12

    [https://arxiv.org/abs/2402.04975](https://arxiv.org/abs/2402.04975)

    ChatScratch是一种AI增强系统，旨在帮助6-12岁儿童自主学习视觉编程。它通过结构化互动故事板、数字绘画和专业编码指导等方式解决了儿童在学习过程中遇到的创作困难和编码指导不足的问题。

    

    随着计算思维(CT)在K-12教育中继续普及，像Scratch这样的已建立的CT平台面临着迎合这些年幼学习者的挑战，特别是小学生(6-12岁)。通过与Scratch专家的形成性调查，我们发现儿童在自主学习Scratch过程中面临三个关键障碍：项目规划中的创作难题、资源创建中的创造力受限以及实现过程中缺乏编码指导。为了解决这些问题，我们介绍了ChatScratch，一种AI增强系统，用于促进年幼儿童的自主编程学习。ChatScratch采用了结构化互动故事板和视觉提示来克服创作障碍，集成了数字绘画和高级图像生成技术来提升创造力，并利用专门针对Scratch的大型语言模型(Large Language Models, LLMs)提供专业的编码指导。我们的研究表明，与Scratch相比，ChatScratch能够有效地提高儿童的编程效率。

    As Computational Thinking (CT) continues to permeate younger age groups in K-12 education, established CT platforms such as Scratch face challenges in catering to these younger learners, particularly those in the elementary school (ages 6-12). Through formative investigation with Scratch experts, we uncover three key obstacles to children's autonomous Scratch learning: artist's block in project planning, bounded creativity in asset creation, and inadequate coding guidance during implementation. To address these barriers, we introduce ChatScratch, an AI-augmented system to facilitate autonomous programming learning for young children. ChatScratch employs structured interactive storyboards and visual cues to overcome artist's block, integrates digital drawing and advanced image generation technologies to elevate creativity, and leverages Scratch-specialized Large Language Models (LLMs) for professional coding guidance. Our study shows that, compared to Scratch, ChatScratch efficiently fo
    
[^14]: 多发信者说服 - 从计算的角度来看

    Multi-Sender Persuasion -- A Computational Perspective

    [https://arxiv.org/abs/2402.04971](https://arxiv.org/abs/2402.04971)

    这项研究考虑了多个具有信息优势的发信者向单个自私行为者传递信号以影响其行为的问题，并提出了一种新颖的可微神经网络方法来近似解决这一问题。通过额外梯度算法，我们发现了超越已有方法的局部均衡解。

    

    我们考虑到具有信息优势的多个发信者向单个自私行为者传递信号以使其采取某些行动。这些设置是计算经济学，多智能体学习和具有多个目标的机器学习中普遍存在的。核心解决方案概念是发信者信号策略的纳什均衡。理论上，我们证明一般情况下找到一个均衡是PPAD-Hard的;实际上，计算一个发信者的最佳响应甚至是NP-Hard的。鉴于这些固有的困难，我们转而寻找局部纳什均衡。我们提出了一种新颖的可微神经网络来近似该游戏的非线性和不连续效用。结合额外梯度算法，我们发现了超越完全展示均衡和现有神经网络发现的局部均衡。广义上，我们的理论和实证贡献对广泛的类别感兴趣。

    We consider multiple senders with informational advantage signaling to convince a single self-interested actor towards certain actions. Generalizing the seminal Bayesian Persuasion framework, such settings are ubiquitous in computational economics, multi-agent learning, and machine learning with multiple objectives. The core solution concept here is the Nash equilibrium of senders' signaling policies. Theoretically, we prove that finding an equilibrium in general is PPAD-Hard; in fact, even computing a sender's best response is NP-Hard. Given these intrinsic difficulties, we turn to finding local Nash equilibria. We propose a novel differentiable neural network to approximate this game's non-linear and discontinuous utilities. Complementing this with the extra-gradient algorithm, we discover local equilibria that Pareto dominates full-revelation equilibria and those found by existing neural networks. Broadly, our theoretical and empirical contributions are of interest to a large class 
    
[^15]: 文本还是图像？对于跨领域泛化能力的仇恨迷因检测模型来说，什么更重要？

    Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?

    [https://arxiv.org/abs/2402.04967](https://arxiv.org/abs/2402.04967)

    本文研究了多模式仇恨迷因检测中的跨领域泛化挑战，并发现仇恨迷因的文本部分对于泛化至不同领域的分类器至关重要，而图像部分对特定训练数据集敏感。实验证明仇恨文本分类器在零样本情况下表现类似于仇恨迷因分类器。黑盒解释表明文本模态对模型性能的贡献较大，但引入图像标题后贡献降低。新的混淆数据集评估显示文本混淆因素表现较好。

    

    本文深入探讨了多模式仇恨迷因检测中跨领域泛化的挑战，并提出了引人注目的研究结果。我们提供了足够的证据支持假设：只有仇恨迷因中的文本成分使得现有的多模式分类器能够在不同领域中进行泛化，而图像成分则对特定训练数据集非常敏感。证据包括演示，证明在零样本情况下，仇恨文本分类器的表现与仇恨迷因分类器相似。同时，将由迷因图像生成的标题引入到仇恨迷因分类器中，导致性能下降平均F1指标约为0.02。通过黑盒解释，我们确定了文本模态的重要贡献（平均83%），但引入迷因图像标题后贡献降低至52%。此外，我们还在新创建的混淆数据集上进行了评估，显示文本混淆因素的表现较好。

    This paper delves into the formidable challenge of cross-domain generalization in multimodal hate meme detection, presenting compelling findings. We provide enough pieces of evidence supporting the hypothesis that only the textual component of hateful memes enables the existing multimodal classifier to generalize across different domains, while the image component proves highly sensitive to a specific training dataset. The evidence includes demonstrations showing that hate-text classifiers perform similarly to hate-meme classifiers in a zero-shot setting. Simultaneously, the introduction of captions generated from images of memes to the hate-meme classifier worsens performance by an average F1 of 0.02. Through blackbox explanations, we identify a substantial contribution of the text modality (average of 83%), which diminishes with the introduction of meme's image captions (52%). Additionally, our evaluation on a newly created confounder dataset reveals higher performance on text confou
    
[^16]: 在知识密集型环境中的聊天机器人：比较意图和基于LLM的系统

    Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems

    [https://arxiv.org/abs/2402.04955](https://arxiv.org/abs/2402.04955)

    这项研究比较了在知识密集型环境中基于LLM的认知助手和基于意图的系统，并发现基于LLM的认知助手在用户体验、任务完成率、可用性和个人效能方面表现更好。

    

    认知助手是在知识密集型任务中为人类工作者提供上下文感知支持的聊天机器人。传统上，认知助手以特定方式回应预定义的用户意图和对话模式。然而，这种刚性对自然语言的多样性处理不好。最近自然语言处理的进展，如GPT-4、Llama2和Gemini等大型语言模型（LLM），可能使认知助手能够以更灵活、更像人类的方式进行对话。然而，这种额外的自由度可能会产生意想不到的后果，特别是在准确性至关重要的知识密集型环境中。为了评估在这些环境中使用LLM的潜力，我们进行了一个用户研究，比较了基于LLM的认知助手和基于意图的系统在交互效率、用户体验、工作量和可用性方面的差异。研究结果显示，基于LLM的认知助手在用户体验、任务完成率、可用性和个人效能方面表现更好。

    Cognitive assistants (CA) are chatbots that provide context-aware support to human workers in knowledge-intensive tasks. Traditionally, cognitive assistants respond in specific ways to predefined user intents and conversation patterns. However, this rigidness does not handle the diversity of natural language well. Recent advances in natural language processing (NLP), powering large language models (LLM) such as GPT-4, Llama2, and Gemini, could enable CAs to converse in a more flexible, human-like manner. However, the additional degrees of freedom may have unforeseen consequences, especially in knowledge-intensive contexts where accuracy is crucial. As a preliminary step to assessing the potential of using LLMs in these contexts, we conducted a user study comparing an LLM-based CA to an intent-based system regarding interaction efficiency, user experience, workload, and usability. This revealed that LLM-based CAs exhibited better user experience, task completion rate, usability, and per
    
[^17]: 自动化视频游戏Beta测试的方法

    An approach to automated videogame beta testing

    [https://arxiv.org/abs/2402.04938](https://arxiv.org/abs/2402.04938)

    本文介绍了一种自动化视频游戏Beta测试的方法，通过解决视频游戏的特殊性，使得质量保证任务能够自动完成。

    

    1970年代和1980年代开发的视频游戏是一些简单的程序，由一个人在几个月内完成，扮演设计师、艺术家和程序员的多重角色。从那时起，视频游戏发展成了一个价值数百万美元的行业。如今，AAA游戏的开发需要数百人在数年的时间内共同合作。管理和工程需求也随之改变。尽管许多过程已经随着时间的推移进行了调整，但对于质量保证任务来说，情况并没有完全改变，这些任务主要仍然由人工Beta测试人员手动完成，这是由于视频游戏的特殊性。本文介绍了一种自动化Beta测试的方法。

    Videogames developed in the 1970s and 1980s were modest programs created in a couple of months by a single person, who played the roles of designer, artist and programmer. Since then, videogames have evolved to become a multi-million dollar industry. Today, AAA game development involves hundreds of people working together over several years. Management and engineering requirements have changed at the same pace. Although many of the processes have been adapted over time, this is not quite true for quality assurance tasks, which are still done mainly manually by human beta testers due to the specific peculiarities of videogames. This paper presents an approach to automate this beta testing.
    
[^18]: 无源域自适应的扩散引导源数据生成

    Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation

    [https://arxiv.org/abs/2402.04929](https://arxiv.org/abs/2402.04929)

    本文提出了一种无源域自适应的新方法，利用扩散模型生成上下文相关的领域特定图像，通过微调预训练模型和无监督领域自适应技术实现了显著的性能改进。

    

    本文引入了一种利用扩散模型的泛化能力进行无源域自适应（DM-SFDA）的新方法。我们提出的DM-SFDA方法包括对预训练的文本到图像扩散模型进行微调，并使用目标图像的特征来指导扩散过程生成源域图像。具体而言，预训练的扩散模型被微调以生成最小化熵并最大化预训练源模型置信度的源样本。然后，我们应用已建立的无监督领域自适应技术将生成的源图像与目标域数据进行对齐。我们通过在一系列数据集上进行全面实验验证了我们的方法，包括Office-31、Office-Home和VisDA。结果显示，在无源域自适应的性能方面取得了显著的改进，展示了扩散模型在生成上下文相关的、领域特定的图像方面的潜力。

    This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images.
    
[^19]: 促使隐式话语关系注释的研究

    Prompting Implicit Discourse Relation Annotation

    [https://arxiv.org/abs/2402.04918](https://arxiv.org/abs/2402.04918)

    本研究旨在提升ChatGPT对隐式话语关系的识别，尝试了多种提示技术，但实验结果显示即使进行复杂的提示工程，隐式话语关系分类在零样本或少样本设置下仍难以解决。

    

    预训练的大型语言模型，如ChatGPT，在各种推理任务中表现出色，无需监督训练，并且被发现超过了众包工作者。然而，ChatGPT在通过标准的多项选择问题引导的隐式话语关系分类任务中的表现仍然远远不如最先进的监督方法。本研究对几种经过验证的提示技术进行了调查，以改善ChatGPT对话语关系的识别。特别是，我们尝试将涉及大量抽象标签的分类任务分解为较小的子任务。然而，实验结果表明，即使使用复杂的提示工程，推理准确率几乎没有改变，这表明隐式话语关系分类在零样本或少样本设置下尚不可解。

    Pre-trained large language models, such as ChatGPT, archive outstanding performance in various reasoning tasks without supervised training and were found to have outperformed crowdsourcing workers. Nonetheless, ChatGPT's performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches. This work investigates several proven prompting techniques to improve ChatGPT's recognition of discourse relations. In particular, we experimented with breaking down the classification task that involves numerous abstract labels into smaller subtasks. Nonetheless, experiment results show that the inference accuracy hardly changes even with sophisticated prompt engineering, suggesting that implicit discourse relation classification is not yet resolvable under zero-shot or few-shot settings.
    
[^20]: 成功的压力：一种用于减轻足球运动员受伤风险和提高球队成功率的预测模型

    The Strain of Success: A Predictive Model for Injury Risk Mitigation and Team Success in Soccer

    [https://arxiv.org/abs/2402.04898](https://arxiv.org/abs/2402.04898)

    本文开发了一种预测模型，通过考虑球员受伤概率，在足球赛季中优化团队表现，减少一线队受伤人数和无效花费，为真实足球团队降低成本并提高球员福利提供了潜力。

    

    在本文中，我们提出了一种新颖的足球顺序团队选择模型。具体而言，我们使用从真实足球数据中学到的球员特定信息对球员受伤和不可用性的随机过程进行建模。蒙特卡洛树搜索被用于为游戏选择团队，通过对球员受伤概率进行推理，优化整个赛季的团队绩效。我们将我们的方法与2018/19英超赛季的基准解进行验证。我们的模型在降低一线队受伤人数约13%和无效花费在受伤球员身上的资金约11%的同时，实现了与基准相似的赛季预期积分 - 这表明了在真实世界的足球团队中降低成本和改善球员福利的潜力。

    In this paper, we present a novel sequential team selection model in soccer. Specifically, we model the stochastic process of player injury and unavailability using player-specific information learned from real-world soccer data. Monte-Carlo Tree Search is used to select teams for games that optimise long-term team performance across a soccer season by reasoning over player injury probability. We validate our approach compared to benchmark solutions for the 2018/19 English Premier League season. Our model achieves similar season expected points to the benchmark whilst reducing first-team injuries by ~13% and the money inefficiently spent on injured players by ~11% - demonstrating the potential to reduce costs and improve player welfare in real-world soccer teams.
    
[^21]: 通过加权模型集成的概率验证AI系统的统一框架

    A Unified Framework for Probabilistic Verification of AI Systems via Weighted Model Integration

    [https://arxiv.org/abs/2402.04892](https://arxiv.org/abs/2402.04892)

    本论文提出了一个基于加权模型集成的统一框架，用于概率验证AI系统。这个框架可以在不依赖强分布假设的情况下，验证各种机器学习模型的许多有趣属性，如公平性、鲁棒性或单调性。

    

    概率形式验证（PFV) AI系统还处于起步阶段，迄今为止，对于特定类别的模型和/或属性，方法仅限于特定的算法而已。我们提出了一个基于加权模型集成（WMI）的AI系统PFV的统一框架，可以非常通用地定义问题。关键是，这种约简可以在不做过强的分布假设的情况下，验证许多有趣的属性，如公平性、鲁棒性或单调性，适用于各种机器学习模型。我们通过使用一个现成的WMI求解器解决多个验证任务来支持这种方法的普适性，然后讨论与这个有前途的框架相关的可扩展性挑战和研究方向。

    The probabilistic formal verification (PFV) of AI systems is in its infancy. So far, approaches have been limited to ad-hoc algorithms for specific classes of models and/or properties.   We propose a unifying framework for the PFV of AI systems based onWeighted Model Integration (WMI), which allows to frame the problem in very general terms.   Crucially, this reduction enables the verification of many properties of interest, like fairness, robustness or monotonicity, over a wide range of machine learning models, without making strong distributional assumptions.   We support the generality of the approach by solving multiple verification tasks with a single, off-the-shelf WMI solver, then discuss the scalability challenges and research directions related to this promising framework.
    
[^22]: RSCNet：云基WiFi感知的动态CSI压缩

    RSCNet: Dynamic CSI Compression for Cloud-based WiFi Sensing

    [https://arxiv.org/abs/2402.04888](https://arxiv.org/abs/2402.04888)

    这篇论文提出了一种名为RSCNet的动态CSI压缩方法，通过压缩信道状态信息（CSI）来减少物联网设备向云服务器传输CSI的通信开销。RSCNet利用长短期记忆（LSTM）单元和优化的CSI窗口实现了准确的感知和CSI重建，从而实现了实时的云基WiFi感知。

    

    WiFi连接的物联网设备正在从纯粹的通信设备发展为利用信道状态信息（CSI）提取能力的感知工具。然而，资源有限的物联网设备和深度神经网络的复杂性要求将CSI传输到云服务器进行感知。尽管可行，但这会导致大量的通信开销。在这种背景下，本文开发了一种新颖的实时感知和压缩网络（RSCNet），它能够通过压缩CSI来实现感知，从而减少通信开销。RSCNet在由少量CSI帧组成的CSI窗口之间进行优化。一旦传输到云服务器，它利用长短期记忆（LSTM）单元从先前的窗口中提取数据，从而增强感知准确性和CSI重建。RSCNet巧妙地平衡了CSI压缩和感知精度之间的权衡，从而简化了实时云基WiFi感知，并减少了开销。

    WiFi-enabled Internet-of-Things (IoT) devices are evolving from mere communication devices to sensing instruments, leveraging Channel State Information (CSI) extraction capabilities. Nevertheless, resource-constrained IoT devices and the intricacies of deep neural networks necessitate transmitting CSI to cloud servers for sensing. Although feasible, this leads to considerable communication overhead. In this context, this paper develops a novel Real-time Sensing and Compression Network (RSCNet) which enables sensing with compressed CSI; thereby reducing the communication overheads. RSCNet facilitates optimization across CSI windows composed of a few CSI frames. Once transmitted to cloud servers, it employs Long Short-Term Memory (LSTM) units to harness data from prior windows, thus bolstering both the sensing accuracy and CSI reconstruction. RSCNet adeptly balances the trade-off between CSI compression and sensing precision, thus streamlining real-time cloud-based WiFi sensing with redu
    
[^23]: 一个统一的高斯过程用于分支和嵌套超参数优化

    A Unified Gaussian Process for Branching and Nested Hyperparameter Optimization

    [https://arxiv.org/abs/2402.04885](https://arxiv.org/abs/2402.04885)

    本文提出了一个统一的贝叶斯优化方法，用于处理分支和嵌套参数之间的条件依赖关系，该方法能够有效地优化神经网络的超参数选择和调整。

    

    在神经网络的成功中，选择合适的超参数起着至关重要的作用，因为超参数直接控制训练算法的行为和性能。为了获得高效的调参，基于高斯过程（GP）模型的贝叶斯优化方法被广泛应用。尽管贝叶斯优化在深度学习中具有许多应用，但现有的方法都基于一个方便但限制性的假设，即调参参数彼此独立。然而，在实践中，条件依赖的调参参数是常见的。本文重点研究了两种类型的调参参数：分支和嵌套参数。嵌套参数指的是那些仅存在于另一个调参参数特定设置中的调参参数，而其它参数在其中嵌套的参数称为分支参数。为了捕捉分支和嵌套参数之间的条件依赖关系，本文提出了一个统一的贝叶斯优化方法。

    Choosing appropriate hyperparameters plays a crucial role in the success of neural networks as hyper-parameters directly control the behavior and performance of the training algorithms. To obtain efficient tuning, Bayesian optimization methods based on Gaussian process (GP) models are widely used. Despite numerous applications of Bayesian optimization in deep learning, the existing methodologies are developed based on a convenient but restrictive assumption that the tuning parameters are independent of each other. However, tuning parameters with conditional dependence are common in practice. In this paper, we focus on two types of them: branching and nested parameters. Nested parameters refer to those tuning parameters that exist only within a particular setting of another tuning parameter, and a parameter within which other parameters are nested is called a branching parameter. To capture the conditional dependence between branching and nested parameters, a unified Bayesian optimizati
    
[^24]: LMUFormer：具有Legendre记忆单元的低复杂度但强大的脉冲模型

    LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units

    [https://arxiv.org/abs/2402.04882](https://arxiv.org/abs/2402.04882)

    本文提出了一种改进的脉冲模型LMUFormer，通过对循环模型进行架构修改，将其性能推向Transformer模型，同时保留其顺序处理能力，以实现并行训练、流式处理和低成本推理。

    

    Transformer模型在许多应用中表现出高准确性，但复杂度高且缺乏顺序处理能力，使其不适用于资源受限的许多边缘流应用。因此，许多研究人员提出将Transformer模型重新定义为具有显式状态的RNN模块，来修改自注意力计算。然而，这些方法往往会导致性能显著降低。本文旨在开发具有以下特性的模型：并行训练、流式处理和低成本推理，并具有SOTA性能。我们提出了一种新的方法来实现这个目标。我们展示了如何通过对循环模型进行架构修改，将其性能推向Transformer模型，同时保留其顺序处理能力。具体来说，我们受到了Legendre记忆单元（LMU）在序列学习任务中的最近成功启发，提出了LMUFormer模型。

    Transformer models have demonstrated high accuracy in numerous applications but have high complexity and lack sequential processing capability making them ill-suited for many streaming applications at the edge where devices are heavily resource-constrained. Thus motivated, many researchers have proposed reformulating the transformer models as RNN modules which modify the self-attention computation with explicit states. However, these approaches often incur significant performance degradation. The ultimate goal is to develop a model that has the following properties: parallel training, streaming and low-cost inference, and SOTA performance. In this paper, we propose a new direction to achieve this goal. We show how architectural modifications to a recurrent model can help push its performance toward Transformer models while retaining its sequential processing capability. Specifically, inspired by the recent success of Legendre Memory Units (LMU) in sequence learning tasks, we propose LM
    
[^25]: 结合云计算与移动计算的机器学习研究

    Combining Cloud and Mobile Computing for Machine Learning

    [https://arxiv.org/abs/2402.04880](https://arxiv.org/abs/2402.04880)

    这项研究将模型分割为移动设备和云之间的计算，以减轻移动设备的负担，并优化云端的工作负载。

    

    尽管移动设备的计算能力正在增加，但机器学习模型的大小也在增长。这种趋势给移动设备带来了问题，如内存容量和电池寿命的限制。虽然许多服务（如ChatGPT和Midjourney）在云中运行所有的推理，但我们认为灵活性和细粒度的任务分配更可取。在这项工作中，我们将模型分割视为改善用户体验的解决方案，将计算分割在移动设备和云之间，以减轻模型的计算密集部分，同时尽量减少数据传输的需求。我们展示了这种分割不仅减少了用户等待时间，还可以通过细粒度调整来优化云端的工作负载。为了实现这一目标，我们设计了一个调度器，收集网络质量、客户端设备能力和作业要求的信息，做出决策以实现在各种设备上的一致性性能。

    Although the computing power of mobile devices is increasing, machine learning models are also growing in size. This trend creates problems for mobile devices due to limitations like their memory capacity and battery life. While many services, like ChatGPT and Midjourney, run all the inferences in the cloud, we believe a flexible and fine-grained task distribution is more desirable. In this work, we consider model segmentation as a solution to improving the user experience, dividing the computation between mobile devices and the cloud in a way that offloads the compute-heavy portion of the model while minimizing the data transfer required. We show that the division not only reduces the wait time for users but can also be fine-tuned to optimize the workloads of the cloud. To achieve that, we design a scheduler that collects information about network quality, client device capability, and job requirements, making decisions to achieve consistent performance across a range of devices while
    
[^26]: 选择具有图神经网络的经典规划器

    Choosing a Classical Planner with Graph Neural Networks

    [https://arxiv.org/abs/2402.04874](https://arxiv.org/abs/2402.04874)

    本文研究了使用图神经网络进行在线规划器选择的方法，并通过析取出的图表示作为另一模型的输入，提出了一种更加资源高效但准确的方法。

    

    在线规划器选择是在给定规划问题的预定义解集中选择一个解算器的任务。由于规划计算复杂，解算器在规划问题上的性能差异很大。因此，预测解算器在给定问题上的性能非常重要。虽然有各种学习方法被采用，但在经典的最优代价规划中，主流方法使用了图神经网络（GNN）。在这项工作中，我们继续使用GNN进行在线规划器选择的研究。我们对所选择的GNN模型、图表示和节点特征以及预测任务的影响进行了彻底的调查。更进一步，我们提出使用GNN获得的图表示作为极端梯度提升（XGBoost）模型的输入，从而实现一种更加资源高效但准确的方法。我们展示了各种基于GNN的在线规划器选择方法的有效性，开辟了新的激动人心的研究方向。

    Online planner selection is the task of choosing a solver out of a predefined set for a given planning problem. As planning is computationally hard, the performance of solvers varies greatly on planning problems. Thus, the ability to predict their performance on a given problem is of great importance. While a variety of learning methods have been employed, for classical cost-optimal planning the prevailing approach uses Graph Neural Networks (GNNs). In this work, we continue the line of work on using GNNs for online planner selection. We perform a thorough investigation of the impact of the chosen GNN model, graph representation and node features, as well as prediction task. Going further, we propose using the graph representation obtained by a GNN as an input to the Extreme Gradient Boosting (XGBoost) model, resulting in a more resource-efficient yet accurate approach. We show the effectiveness of a variety of GNN-based online planner selection methods, opening up new exciting avenues
    
[^27]: 将知识图谱嵌入到退化的克利福德代数中

    Embedding Knowledge Graphs in Degenerate Clifford Algebras

    [https://arxiv.org/abs/2402.04870](https://arxiv.org/abs/2402.04870)

    这项研究提出将知识图谱嵌入到退化的克利福德代数中。通过考虑具有零幂指数为2的零幂基向量，可以泛化基于二次数的方法并捕捉实体嵌入中缺乏高阶相互作用的模式。研究设计了两个新模型来发现代数的参数，并证明零幂向量有助于捕捉实体的特征。

    

    克利福德代数是实数、复数和四元数的自然推广。迄今为止，在知识图谱嵌入的背景下，只有形式为$Cl_{p,q}$（即没有零幂基向量的代数）的克利福德代数受到研究。我们提出考虑零幂基向量，其幂指数为2。在这些空间中，被称为$Cl_{p,q,r}$，可以泛化基于二次数的方法（无法使用$Cl_{p,q}$进行建模）并捕捉源于实数和复数部分间缺乏高阶相互作用的实体嵌入的模式。我们设计了两个新模型来发现参数$p$，$q$和$r$。第一个模型使用贪婪搜索优化$p$，$q$和$r$。第二个模型基于使用神经网络计算的输入知识图谱的嵌入来预测$(p, q, r)$。我们在七个基准数据集上进行的评估结果表明，零幂向量有助于捕捉实体的特征。

    Clifford algebras are a natural generalization of the real numbers, the complex numbers, and the quaternions. So far, solely Clifford algebras of the form $Cl_{p,q}$ (i.e., algebras without nilpotent base vectors) have been studied in the context of knowledge graph embeddings. We propose to consider nilpotent base vectors with a nilpotency index of two. In these spaces, denoted $Cl_{p,q,r}$, allows generalizing over approaches based on dual numbers (which cannot be modelled using $Cl_{p,q}$) and capturing patterns that emanate from the absence of higher-order interactions between real and complex parts of entity embeddings. We design two new models for the discovery of the parameters $p$, $q$, and $r$. The first model uses a greedy search to optimize $p$, $q$, and $r$. The second predicts $(p, q,r)$ based on an embedding of the input knowledge graph computed using neural networks. The results of our evaluation on seven benchmark datasets suggest that nilpotent vectors can help capture 
    
[^28]: 学习通过实践：具有因果感知的在线因果强化学习框架

    Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy

    [https://arxiv.org/abs/2402.04869](https://arxiv.org/abs/2402.04869)

    本文提出了一个在线因果强化学习框架，其中通过明确建模状态的生成过程和使用因果结构进行策略引导，以帮助强化学习代理的决策可解释性。该框架具有理论性能保证，并在探索和开发过程中使用干预进行因果结构学习。

    

    因果知识作为人类智能中直观认知和推理解决方案的关键组成部分，为强化学习（RL）代理的可解释性决策提供了巨大的潜力，通过帮助减少搜索空间。然而，发现和整合因果关系进入RL仍存在相当大的差距，这阻碍了因果关系RL的快速发展。在本文中，我们考虑使用因果图模型明确地建模状态的生成过程，并在此基础上增强策略。我们将因果结构更新形式化为具有主动环境干预学习的RL交互过程。为了优化衍生的目标，我们提出了一个具有理论性能保证的框架，两个步骤交替进行：在探索过程中使用干预进行因果结构学习，在开发过程中使用学习到的因果结构进行策略引导。由于缺少公共基准，用于对所提出的方法进行评估。

    As a key component to intuitive cognition and reasoning solutions in human intelligence, causal knowledge provides great potential for reinforcement learning (RL) agents' interpretability towards decision-making by helping reduce the searching space. However, there is still a considerable gap in discovering and incorporating causality into RL, which hinders the rapid development of causal RL. In this paper, we consider explicitly modeling the generation process of states with the causal graphical model, based on which we augment the policy. We formulate the causal structure updating into the RL interaction process with active intervention learning of the environment. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventions for causal structure learning during exploration and using the learned causal structure for policy guidance during exploitation. Due to the lack of public benchmarks that 
    
[^29]: 通过反事实轨迹解释学习到的奖励函数

    Explaining Learned Reward Functions with Counterfactual Trajectories

    [https://arxiv.org/abs/2402.04856](https://arxiv.org/abs/2402.04856)

    通过对比原始轨迹和反事实部分轨迹的奖励，我们提出了一种解释强化学习中奖励函数的方法。通过生成符合质量标准的反事实轨迹解释（CTEs），我们的实验表明，CTEs对于代理人模型具有明显的信息性，能提高其预测与奖励函数的一致性。

    

    从人类行为或反馈中学习奖励是将AI系统与人类价值观一致的一种有希望的方法，但无法始终提取正确的奖励函数。可解释性工具可以帮助用户理解和评估学习到的奖励函数中可能存在的缺陷。我们提出了反事实轨迹解释（CTEs），通过对比原始轨迹和反事实部分轨迹以及它们各自接收的奖励来解释强化学习中的奖励函数。我们为CTEs制定了六个质量标准，并提出了一种基于Monte-Carlo的新算法来生成优化这些质量标准的CTEs。最后，我们通过训练代理人模型来衡量生成的解释对其的信息性。CTEs对于代理人模型具有明显的信息性，增加了其预测与未见轨迹上的奖励函数的相似性。此外，它学会了准确判断轨迹之间的奖励差异。

    Learning rewards from human behaviour or feedback is a promising approach to aligning AI systems with human values but fails to consistently extract correct reward functions. Interpretability tools could enable users to understand and evaluate possible flaws in learned reward functions. We propose Counterfactual Trajectory Explanations (CTEs) to interpret reward functions in reinforcement learning by contrasting an original with a counterfactual partial trajectory and the rewards they each receive. We derive six quality criteria for CTEs and propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises these quality criteria. Finally, we measure how informative the generated explanations are to a proxy-human model by training it on CTEs. CTEs are demonstrably informative for the proxy-human model, increasing the similarity between its predictions and the reward function on unseen trajectories. Further, it learns to accurately judge differences in rewards between trajec
    
[^30]: PaDeLLM-NER：大型语言模型中的并行解码用于命名实体识别

    PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition

    [https://arxiv.org/abs/2402.04838](https://arxiv.org/abs/2402.04838)

    本研究提出了PaDeLLM-NER，一种能够在大型语言模型中实现并行解码，从而显著减少命名实体识别的生成延迟，同时保持预测质量和性能。

    

    本研究旨在使用大型语言模型（LLMs）减少命名实体识别（NER）的生成延迟。LLMs的高延迟的主要原因是顺序解码过程，该过程自回归地生成NER的所有标签和提及，显著增加了序列长度。为此，我们引入了PaDeLLM-NER（Parallel Decoding in LLM for NE），这是一种无需额外模块或架构修改即可无缝集成到现有生成模型框架中的方法。PaDeLLM-NER允许同时解码所有提及，从而减少生成延迟。实验结果显示，PaDeLLM-NER的推理速度显著提高，对英语和中文来说比自回归方法快1.76到10.22倍。与各种数据集上的最先进性能相媲美，同时维持了预测质量。

    In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.
    
[^31]: 关于不变几何深度学习模型的完备性

    On the Completeness of Invariant Geometric Deep Learning Models

    [https://arxiv.org/abs/2402.04836](https://arxiv.org/abs/2402.04836)

    这项研究集中于不变模型的理论表达能力，通过引入完备的设计GeoNGNN，并利用其作为理论工具，首次证明了E(3)-完备性。

    

    不变模型是一类重要的几何深度学习模型，通过利用信息丰富的几何特征生成有意义的几何表示。这些模型以其简单性、良好的实验结果和计算效率而闻名。然而，它们的理论表达能力仍然不清楚，限制了对这种模型潜力的深入理解。在这项工作中，我们集中讨论不变模型的理论表达能力。我们首先严格限制了最经典的不变模型Vanilla DisGNN（结合距离的消息传递神经网络）的表达能力，将其不可识别的情况仅限于高度对称的几何图形。为了打破这些特殊情况的对称性，我们引入了一个简单而完备的不变设计，即嵌套Vanilla DisGNN的GeoNGNN。利用GeoNGNN作为理论工具，我们首次证明了E(3)-完备性。

    Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features. These models are characterized by their simplicity, good experimental results and computational efficiency. However, their theoretical expressive power still remains unclear, restricting a deeper understanding of the potential of such models. In this work, we concentrate on characterizing the theoretical expressiveness of invariant models. We first rigorously bound the expressiveness of the most classical invariant model, Vanilla DisGNN (message passing neural networks incorporating distance), restricting its unidentifiable cases to be only those highly symmetric geometric graphs. To break these corner cases' symmetry, we introduce a simple yet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN. Leveraging GeoNGNN as a theoretical tool, we for the first time prove the E(3)-completeness 
    
[^32]: 结构化的d-DNNF不支持否定

    Structured d-DNNF Is Not Closed Under Negation

    [https://arxiv.org/abs/2402.04832](https://arxiv.org/abs/2402.04832)

    结构化的d-DNNF不支持多项式时间的否定、析取或存在量化操作，回答了OBDD是否支持更可处理的转换问题。结构化的d-DNNF比SDD更简洁，不存在等价的多项式大小的SDD表示。

    

    结构化的d-DNNF和SDD比OBDD更简洁。此外，SDD的可处理性与OBDD基本相同。但这留下了两个重要的问题。首先，OBDD是否支持比结构化的d-DNNF更可处理的转换？其次，结构化的d-DNNF是否比SDD更简洁？在本文中，我们对这两个问题给予了肯定的回答。对于第一个问题，我们证明了与OBDD不同，结构化的d-DNNF不支持多项式时间的否定、析取或存在量化操作。作为推论，我们推断出存在一些函数，其等价的多项式大小的结构化d-DNNF表示不存在SDD表示，从而回答了第二个问题。我们还利用这个第二个结果对算术电路（AC）进行了推广，展示了PSDD和结构化d-DNNF的单调AC对应之间的简洁差距。

    Both structured d-DNNF and SDD can be exponentially more succinct than OBDD. Moreover, SDD is essentially as tractable as OBDD. But this has left two important open questions. Firstly, does OBDD support more tractable transformations than structured d-DNNF? And secondly, is structured d-DNNF more succinct than SDD? In this paper, we answer both questions in the affirmative. For the first question we show that, unlike OBDD, structured d-DNNF does not support polytime negation, disjunction, or existential quantification operations. As a corollary, we deduce that there are functions with an equivalent polynomial-sized structured d-DNNF but with no such representation as an SDD, thus answering the second question. We also lift this second result to arithmetic circuits (AC) to show a succinctness gap between PSDD and the monotone AC analogue to structured d-DNNF.
    
[^33]: 来自在线人工智能反馈的直接语言模型对齐

    Direct Language Model Alignment from Online AI Feedback

    [https://arxiv.org/abs/2402.04792](https://arxiv.org/abs/2402.04792)

    本论文提出了一种名为OAIF的方法，通过在线反馈来改善DAP方法，该方法使用LLM作为标注器，从而在多个任务中展示出优于离线DAP和RLHF的性能

    

    最近，直接对齐偏好（DAP）方法如DPO已成为对于从人类反馈中进行增强学习的高效替代方法，不要求单独的奖励模型。然而，DAP方法中使用的偏好数据集通常在训练之前收集，并且从不更新，因此反馈纯粹是离线的。此外，这些数据集中的回应通常是从一个与被对齐的语言模型不同的语言模型中采样的，由于模型在训练过程中会变化，对齐阶段必然是非策略的。在本研究中，我们认为在线反馈是关键，可以改善DAP方法。我们的方法，在线人工智能反馈（OAIF），使用LLM作为标注器：在每个训练迭代中，我们从当前模型中采样两个回应，并提示LLM标注器选择哪个更受欢迎，从而提供在线反馈。尽管简单，但通过多个任务中的人工评估，我们证明OAIF优于离线DAP和RLHF

    Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF 
    
[^34]: MLLM作为法官：使用视觉语言基准评估多模态MLLM作为法官的能力

    MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark

    [https://arxiv.org/abs/2402.04788](https://arxiv.org/abs/2402.04788)

    本文引入了MLLM作为法官的新基准测试，用于评估MLLM在辅助法官方面的能力。研究结果显示，MLLM在对比评估任务中展示出了非常类人的辨别能力，但在评分评估和批量排序任务中与人类偏好存在显著的差异。此外，即使对于先进模型如GPT-4V，MLLM仍然面临着偏见、幻觉式回答和不一致性等判断方面的挑战。

    

    多模态大型语言模型（MLLMs）近来引起了广泛关注，展现出人工智能方面的巨大潜力。然而，评估MLLM的实用性存在着相当大的挑战，主要是由于缺乏与人类偏好相符的多模态基准测试。本文受到LLM模型中LLM作为法官的启发，引入了一个新的基准测试，被称为MLLM作为法官，用于评估MLLM在协助法官方面的能力，包括三个不同的任务：评分评估、对比评估和批量排序。我们的研究发现，虽然MLLM在对比评估方面展示出了令人瞩目的类人辨别能力，但在评分评估和批量排序任务中与人类偏好存在显著的差异。此外，即使对于像GPT-4V这样的先进模型，MLLM仍然面临着判断方面的挑战，包括多样的偏见、幻觉式的回答和不一致性。这些发现强调了对MLLM的改进和进一步研究的迫切需要。

    Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Inspired by LLM-as-a-Judge in LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges including three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further rese
    
[^35]: StableMask: 在仅解码Transformer中改进因果屏蔽的方法

    StableMask: Refining Causal Masking in Decoder-only Transformer

    [https://arxiv.org/abs/2402.04779](https://arxiv.org/abs/2402.04779)

    StableMask是一种在仅解码Transformer中改进因果屏蔽的无参数方法，通过引入伪注意力值来平衡注意力分布，并通过逐渐减小的屏蔽比率来编码绝对位置信息。

    

    在语言建模中，仅解码Transformer架构中采用因果屏蔽和相对位置编码（RPE）已成为事实上的选择。尽管其在各种任务中表现出色，但我们发现了两个限制：首先，即使当前嵌入具有足够的自包含信息，它要求所有注意力分数都为非零且总和为1。这强迫模型对特定的标记分配不成比例的过度关注。其次，基于RPE的Transformer在编码绝对位置信息方面的能力有限，因此在位置关键任务中受到限制。在这项工作中，我们提出了StableMask：一种无参数的方法，通过改进因果屏蔽来解决这两个限制。它引入了伪注意力值来平衡注意力分布，并通过逐渐减小的屏蔽比率来编码绝对位置信息。StableMask的有效性得到了验证。

    The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both 
    
[^36]: 进化异质群集中的专门化集体行为的出现

    Emergence of specialized Collective Behaviors in Evolving Heterogeneous Swarms

    [https://arxiv.org/abs/2402.04763](https://arxiv.org/abs/2402.04763)

    

    

    自然动物群体，如社会昆虫群体，表现出惊人的任务专业化程度，用于处理复杂任务并生存下来。这是通过表型可塑性支持的：共享相同基因型但对不同类别的个体表达不同的基因型，每个专门化任务。在这项工作中，我们通过表型可塑性演化了一群模拟机器人，研究在紧急感知任务中专门化集体行为的出现。表型可塑性通过行为的异质性实现，将基因型分为两个部分，每个部分与一个不同的神经网络控制器相关联。整个基因型通过这两个部分表达整个群体的行为，并通过单一的适应度函数进行演化。我们分析了获得的行为，并利用这些结果提供的见解设计了一个在线调节机制。我们的实验展示。。

    Natural groups of animals, such as swarms of social insects, exhibit astonishing degrees of task specialization, useful to address complex tasks and to survive. This is supported by phenotypic plasticity: individuals sharing the same genotype that is expressed differently for different classes of individuals, each specializing in one task. In this work, we evolve a swarm of simulated robots with phenotypic plasticity to study the emergence of specialized collective behavior during an emergent perception task. Phenotypic plasticity is realized in the form of heterogeneity of behavior by dividing the genotype into two components, with one different neural network controller associated to each component. The whole genotype, expressing the behavior of the whole group through the two components, is subject to evolution with a single fitness function. We analyse the obtained behaviors and use the insights provided by these results to design an online regulatory mechanism. Our experiments sho
    
[^37]: EvoSeed：揭示使用真实世界幻觉对深度神经网络的威胁

    EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World Illusions

    [https://arxiv.org/abs/2402.04699](https://arxiv.org/abs/2402.04699)

    EvoSeed是一个基于进化策略的搜索算法框架，用于生成自然对抗样本，旨在揭示深度神经网络面临的威胁。该框架在模型无关的黑盒设置中操作，可以生成高质量且可转移的对抗图像。

    

    深度神经网络被自然对抗样本所利用，这些样本对人类感知没有影响，但会被错误分类。目前的方法通常依赖于深度神经网络的白盒性质来生成这些对抗样本，或者改变对抗样本与训练分布的分布。为了缓解当前方法的局限性，我们提出了EvoSeed，这是一个基于进化策略的搜索算法框架，用于生成自然对抗样本。我们的EvoSeed框架使用辅助扩散和分类器模型在模型无关的黑盒设置中运行。我们使用CMA-ES来优化对对抗种子向量的搜索，该向量在经过条件扩散模型处理后，导致分类器模型错误分类无限制的自然对抗样本。实验证明生成的对抗图像具有高质量，并且可应用于不同的分类器。

    Deep neural networks are exploited using natural adversarial samples, which have no impact on human perception but are misclassified. Current approaches often rely on the white-box nature of deep neural networks to generate these adversarial samples or alter the distribution of adversarial samples compared to training distribution. To alleviate the limitations of current approaches, we propose EvoSeed, a novel evolutionary strategy-based search algorithmic framework to generate natural adversarial samples. Our EvoSeed framework uses auxiliary Diffusion and Classifier models to operate in a model-agnostic black-box setting. We employ CMA-ES to optimize the search for an adversarial seed vector, which, when processed by the Conditional Diffusion Model, results in an unrestricted natural adversarial sample misclassified by the Classifier Model. Experiments show that generated adversarial images are of high image quality and are transferable to different classifiers. Our approach demonstra
    
[^38]: 大型语言模型作为可信的解释器

    Large Language Models As Faithful Explainers

    [https://arxiv.org/abs/2402.04678](https://arxiv.org/abs/2402.04678)

    本论文提出了一个生成解释框架（xLLM），用于提高大型语言模型（LLMs）自然语言格式解释的可信度。通过一个评估器来量化解释的可信度，并通过迭代优化过程来提高可信度。

    

    近年来，大型语言模型(LLMs)通过利用其丰富的内部知识和推理能力，已经能够熟练解决复杂的任务。然而，这种复杂性阻碍了传统的以输入为重点的解释算法来解释LLMs的复杂决策过程。为了解决这个问题，最近出现了一种自我解释机制，通过自然语言的形式进行单向推理，从而实现对LLMs预测的解释。然而，这种自然语言解释经常因为缺乏可信度而受到批评，因为这些解释可能不准确地反映LLMs的决策行为。在这项工作中，我们引入了一个生成解释框架xLLM，以提高LLMs自然语言格式的解释的可信度。具体而言，我们提出了一个评估器来量化自然语言解释的可信度，并通过xLLM的迭代优化过程来提高可信度，目标是最大程度地提高可信度。

    Large Language Models (LLMs) have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and reasoning ability. Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of LLMs. Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a natural language format. However, natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs. In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in natural language formats for LLMs. Specifically, we propose an evaluator to quantify the faithfulness of natural language explanation and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the 
    
[^39]: 带风险最小化的分组分布鲁棒数据集蒸馏

    Group Distributionally Robust Dataset Distillation with Risk Minimization

    [https://arxiv.org/abs/2402.04676](https://arxiv.org/abs/2402.04676)

    这项研究关注数据集蒸馏与其泛化能力的关系，尤其是在面对不常见的子组的样本时，如何确保模型在合成数据集上的训练可以表现良好。

    

    数据集蒸馏（DD）已成为一种广泛采用的技术，用于构建一个合成数据集，该数据集在捕捉训练数据集的基本信息方面起到重要作用，从而方便准确训练神经模型。其应用涵盖了转移学习、联邦学习和神经架构搜索等各个领域。构建合成数据的最流行方法依赖于使模型在合成数据集和训练数据集上的收敛性能相匹配。然而，目标是将训练数据集视为辅助，就像训练集是人口分布的近似替代品一样，而后者才是我们感兴趣的数据。尽管其受欢迎程度很高，但尚未探索的一个方面是DD与其泛化能力的关系，特别是跨不常见的子组。也就是说，当面对来自罕见子组的样本时，我们如何确保在合成数据集上训练的模型表现良好。

    Dataset distillation (DD) has emerged as a widely adopted technique for crafting a synthetic dataset that captures the essential information of a training dataset, facilitating the training of accurate neural models. Its applications span various domains, including transfer learning, federated learning, and neural architecture search. The most popular methods for constructing the synthetic data rely on matching the convergence properties of training the model with the synthetic dataset and the training dataset. However, targeting the training dataset must be thought of as auxiliary in the same sense that the training set is an approximate substitute for the population distribution, and the latter is the data of interest. Yet despite its popularity, an aspect that remains unexplored is the relationship of DD to its generalization, particularly across uncommon subgroups. That is, how can we ensure that a model trained on the synthetic dataset performs well when faced with samples from re
    
[^40]: 通过艺术设计提高对抗性鲁棒性

    Adversarial Robustness Through Artifact Design

    [https://arxiv.org/abs/2402.04660](https://arxiv.org/abs/2402.04660)

    该研究提出了一种通过艺术设计实现对抗性鲁棒性的方法，通过微小更改现有规范来抵御对抗性示例的影响。

    

    对抗性示例的出现给机器学习带来了挑战。为了阻碍对抗性示例，大多数防御方法都改变了模型的训练方式（如对抗性训练）或推理过程（如随机平滑）。尽管这些方法显著提高了模型的对抗性鲁棒性，但模型仍然极易受到对抗性示例的影响。在某些领域如交通标志识别中，我们发现对象是按照规范来设计（如标志规范）。为了改善对抗性鲁棒性，我们提出了一种新颖的方法。具体来说，我们提供了一种重新定义规范的方法，对现有规范进行微小的更改，以防御对抗性示例。我们将艺术设计问题建模为一个鲁棒优化问题，并提出了基于梯度和贪婪搜索的方法来解决它。我们在交通标志识别领域对我们的方法进行了评估，使其能够改变交通标志中的象形图标（即标志内的符号）。

    Adversarial examples arose as a challenge for machine learning. To hinder them, most defenses alter how models are trained (e.g., adversarial training) or inference is made (e.g., randomized smoothing). Still, while these approaches markedly improve models' adversarial robustness, models remain highly susceptible to adversarial examples. Identifying that, in certain domains such as traffic-sign recognition, objects are implemented per standards specifying how artifacts (e.g., signs) should be designed, we propose a novel approach for improving adversarial robustness. Specifically, we offer a method to redefine standards, making minor changes to existing ones, to defend against adversarial examples. We formulate the problem of artifact design as a robust optimization problem, and propose gradient-based and greedy search methods to solve it. We evaluated our approach in the domain of traffic-sign recognition, allowing it to alter traffic-sign pictograms (i.e., symbols within the signs) a
    
[^41]: LEVI:通过以层为单位的多视角集成实现可泛化的微调

    LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views

    [https://arxiv.org/abs/2402.04644](https://arxiv.org/abs/2402.04644)

    本论文中，我们提出了一种名为LEVI的方法，通过以层为单位的多视角集成，实现了对预训练和微调数据中问题的解决，并提升微调模型对未见过的分布的泛化能力。

    

    微调越来越广泛地用于在新的下游任务中利用预训练基础模型的能力。虽然在各种任务上微调取得了许多成功，但最近的研究观察到微调模型在未见过的分布（即，超出分布；OOD）上的泛化存在挑战。为了改善OOB泛化，一些先前的研究确定了微调数据的限制，并调整微调以保留自预训练数据学习到的通用表示。然而，预训练数据和模型中的潜在限制经常被忽视。在本文中，我们认为过度依赖预训练表示可能会阻碍微调学习下游任务的重要表示，从而影响其OOB泛化。当新任务来自于与预训练数据不同的（子）领域时，这可能尤为灾难性。为了解决预训练和微调数据中的问题，我们提出了一种新颖的方法。

    Fine-tuning is becoming widely used for leveraging the power of pre-trained foundation models in new downstream tasks. While there are many successes of fine-tuning on various tasks, recent studies have observed challenges in the generalization of fine-tuned models to unseen distributions (i.e., out-of-distribution; OOD). To improve OOD generalization, some previous studies identify the limitations of fine-tuning data and regulate fine-tuning to preserve the general representation learned from pre-training data. However, potential limitations in the pre-training data and models are often ignored. In this paper, we contend that overly relying on the pre-trained representation may hinder fine-tuning from learning essential representations for downstream tasks and thus hurt its OOD generalization. It can be especially catastrophic when new tasks are from different (sub)domains compared to pre-training data. To address the issues in both pre-training and fine-tuning data, we propose a nove
    
[^42]: SPARQL生成：对OpenLLaMA用于生命科学知识图谱问答的微调进行分析

    SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph

    [https://arxiv.org/abs/2402.04627](https://arxiv.org/abs/2402.04627)

    在本研究中，我们针对生命科学知识图谱上的问题回答对OpenLlama LLM进行了微调，并提出了一种数据增强方法，以扩展现有查询集合，从而能够进行微调，即使训练数据稀缺。我们还研究了查询中语义线索的作用。

    

    大型语言模型（LLM）在各种自然语言处理应用中的成功为基于LLM的知识图谱问答系统开辟了新的路径。然而，其实施的主要障碍之一是在将问题转化为相应的SPARQL查询的任务中，尤其是在特定领域的知识图谱中，训练数据的稀缺性。为了克服这一挑战，在本研究中，我们评估了针对生命科学知识图谱上的问题回答对OpenLlama LLM进行微调的几种策略。具体而言，我们提出了一种端到端的数据增强方法，用于扩展已有查询集合，从而获得一组更大的语义丰富的问题-SPARQL查询对的数据集，即使在这些对稀缺的数据集中也能进行微调。在这个背景下，我们还研究了查询中语义“线索”的作用，例如有意义的变量名和...

    The recent success of Large Language Models (LLM) in a wide range of Natural Language Processing applications opens the path towards novel Question Answering Systems over Knowledge Graphs leveraging LLMs. However, one of the main obstacles preventing their implementation is the scarcity of training data for the task of translating questions into corresponding SPARQL queries, particularly in the case of domain-specific KGs. To overcome this challenge, in this study, we evaluate several strategies for fine-tuning the OpenLlama LLM for question answering over life science knowledge graphs. In particular, we propose an end-to-end data augmentation approach for extending a set of existing queries over a given knowledge graph towards a larger dataset of semantically enriched question-to-SPARQL query pairs, enabling fine-tuning even for datasets where these pairs are scarce. In this context, we also investigate the role of semantic "clues" in the queries, such as meaningful variable names and
    
[^43]: InfLLM: 揭示LLMs对于处理超长序列的内在能力，无需训练的记忆

    InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory

    [https://arxiv.org/abs/2402.04617](https://arxiv.org/abs/2402.04617)

    InfLLM是一种无需训练的基于记忆的方法，用于揭示LLMs处理超长序列的内在能力。它通过存储远距离上下文和高效的注意计算机制，允许LLMs有效处理具有流式输入的长序列。

    

    大型语言模型（LLMs）已成为处理具有漫长传输输入的现实应用的基石，如LLM驱动代理。然而，现有的在受限最大长度序列上预训练的LLMs无法推广到更长的序列，因为存在领域外和分散注意力的问题。为了缓解这些问题，现有的工作采用滑动注意力窗口和丢弃远距离标记，以处理超长序列。不幸的是，这些方法无法捕获序列内的长距离依赖关系，以深入理解语义。本文介绍了一种无需训练的基于记忆的方法InfLLM，来揭示LLMs处理流式长序列的内在能力。具体而言，InfLLM将远距离的上下文存储到附加的内存单元中，并使用高效的机制来查找与注意计算相关的标记单元。因此，InfLLM允许LLMs高效处理长序列，同时保持了对语义的深入理解。

    Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as LLM-driven agents. However, existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues. To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences. Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics. This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long sequences. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences while maintaining
    
[^44]: TinyLLM: 从多个大型语言模型学习一个小型学生模型

    TinyLLM: Learning a Small Student from Multiple Large Language Models

    [https://arxiv.org/abs/2402.04616](https://arxiv.org/abs/2402.04616)

    TinyLLM是一种从多个大型语言模型中学习小型学生模型的知识蒸馏范式，旨在解决知识多样性有限和缺乏上下文信息等问题，并鼓励学生模型理解答案背后的原理。

    

    将更强大的大型语言模型（LLMs）的推理能力转移到较小的模型上具有吸引力，因为较小的LLMs更灵活，成本更低。在现有的解决方案中，知识蒸馏因其出色的效率和泛化能力而脱颖而出。然而，现有方法存在一些缺点，包括知识多样性有限和缺乏丰富的上下文信息。为了解决这些问题并促进紧凑语言模型的学习，我们提出了TinyLLM，一种从多个大型教师LLMs中学习小型学生LLM的新型知识蒸馏范式。特别地，我们鼓励学生LLM不仅生成正确答案，而且理解这些答案背后的原理。鉴于不同的LLMs具有不同的推理能力，我们引导学生模型吸收来自多个教师LLMs的知识。我们进一步引入了一个上下文示例生成器和一个老师强制模块...

    Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing 
    
[^45]: ScreenAI: 用于UI和信息图表理解的视觉-语言模型

    ScreenAI: A Vision-Language Model for UI and Infographics Understanding

    [https://arxiv.org/abs/2402.04615](https://arxiv.org/abs/2402.04615)

    ScreenAI是一个专注于UI和信息图表理解的视觉-语言模型，通过灵活的修补策略和独特的数据集训练，以及针对UI元素的屏幕注解任务的处理，实现了在多个任务上的新的最优结果。

    

    屏幕用户界面（UI）和信息图表在人类沟通和人机交互中起着重要作用，并且共享相似的视觉语言和设计原则。我们介绍了ScreenAI，这是一个专门用于UI和信息图表理解的视觉-语言模型。我们的模型改进了PaLI架构，采用了pix2struct的灵活修补策略，并经过独特的数据集训练。在这个数据集的核心是一项新颖的屏幕注解任务，模型必须识别UI元素的类型和位置。我们使用这些文本注解来描述屏幕，并使用大规模的语言模型自动生成问答（QA），UI导航和摘要训练数据集。我们进行了消融研究以展示这些设计选择的影响。在仅有5B参数的情况下，ScreenAI在基于UI和信息图表的任务（多页文档VQA，WebSRC，MoTIF和Widget字幕）上取得了最新的最优结果，并且达到了最好的效果。

    Screen user interfaces (UIs) and infographics, sharing similar visual language and design principles, play important roles in human communication and human-machine interaction. We introduce ScreenAI, a vision-language model that specializes in UI and infographics understanding. Our model improves upon the PaLI architecture with the flexible patching strategy of pix2struct and is trained on a unique mixture of datasets. At the heart of this mixture is a novel screen annotation task in which the model has to identify the type and location of UI elements. We use these text annotations to describe screens to Large Language Models and automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. We run ablation studies to demonstrate the impact of these design choices. At only 5B parameters, ScreenAI achieves new state-of-the-artresults on UI- and infographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget Captioning), and new best-in
    
[^46]: Alirector: 提升对齐的中文语法错误修正器

    Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector

    [https://arxiv.org/abs/2402.04601](https://arxiv.org/abs/2402.04601)

    本文提出了一种提升对齐的中文语法错误修正器，旨在解决使用自回归生成模型时面临的过度修正挑战。该方法适用于序列到序列模型和仅解码的大型语言模型，并通过对齐模型进行多轮修正，提高修正效果。

    

    中文语法错误修正（CGEC）在使用自回归生成模型（如序列到序列模型和仅解码的大型语言模型）时面临严重的过度修正挑战。虽然以前的方法旨在解决序列到序列模型中的过度修正问题，但很难适应仅解码的大型语言模型。本文提出了一种提升对齐的解决方案，适用于序列到序列模型和仅解码的大型语言模型，并且能够解决过度修正问题。我们的方法首先训练一个修正模型，生成源句子的初始修正。然后，将源句子与初始修正结合起来，通过一个对齐模型进行另一轮修正，以促使对齐模型专注于潜在的过度修正。此外，为了增强模型识别细微差别的能力，我们进一步探索了源句子和初始修正的逆向对齐。最后，我们将对齐的知识转移到CGEC模型中，以提高修正效果。

    Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs). While previous methods aim to address overcorrection in Seq2Seq models, they are difficult to adapt to decoder-only LLMs. In this paper, we propose an alignment-enhanced corrector for the overcorrection problem that applies to both Seq2Seq models and decoder-only LLMs. Our method first trains a correction model to generate an initial correction of the source sentence. Then, we combine the source sentence with the initial correction and feed it through an alignment model for another round of correction, aiming to enforce the alignment model to focus on potential overcorrection. Moreover, to enhance the model's ability to identify nuances, we further explore the reverse alignment of the source sentence and the initial correction. Finally, we transfer the alignment kn
    
[^47]: 在软件产品线中求解优先级配对测试数据生成问题的CMSA算法

    CMSA algorithm for solving the prioritized pairwise test data generation problem in software product lines

    [https://arxiv.org/abs/2402.04597](https://arxiv.org/abs/2402.04597)

    本研究提出了一种基于CMSA的新算法来解决软件产品线中的优先级配对测试数据生成问题，该算法可以解决实例规模较大和现有算法无法在合理时间内计算的问题。

    

    在软件产品线(SPLs)中，由于可能存在大量有效的特征组合，测试整个产品族可能会很困难甚至不可能。因此，我们希望找到一个最小的子集，可以测试所有这些可能的组合(配对)。此外，当测试单个产品需要很大的工作量时，首先测试由一组优先级特征组成的产品是可取的。这个问题被称为优先级配对测试数据生成问题。现有的基于整数线性规划的算法对于小和中等规模的实例来说足够快。然而，由于候选解数量的指数增长，存在一些太大无法在合理时间内计算这些算法的实际实例。而且，这些启发式算法并不总是能带我们找到最好的解。在这项工作中，我们提出了一种基于混合元启发式算法CMSA的新方法。

    In Software Product Lines (SPLs) it may be difficult or even impossible to test all the products of the family because of the large number of valid feature combinations that may exist. Thus, we want to find a minimal subset of the product family that allows us to test all these possible combinations (pairwise). Furthermore, when testing a single product is a great effort, it is desirable to first test products composed of a set of priority features. This problem is called Prioritized Pairwise Test Data Generation Problem.   State-of-the-art algorithms based on Integer Linear Programming for this problema are faster enough for small and medium instances. However, there exists some real instances that are too large to be computed with these algorithms in a reasonable time because of the exponential growth of the number of candidate solutions. Also, these heuristics not always lead us to the best solutions. In this work we propose a new approach based on a hybrid metaheuristic algorithm c
    
[^48]: 在具有双输出脉冲架构（DOSA）的连续多标签学习中改进不平衡鲁棒性

    Towards Improved Imbalance Robustness in Continual Multi-Label Learning with Dual Output Spiking Architecture (DOSA)

    [https://arxiv.org/abs/2402.04596](https://arxiv.org/abs/2402.04596)

    本研究提出了一个双输出脉冲架构（DOSA）来解决连续多标签学习中的不平衡鲁棒性问题，并提出了一种新的不平衡感知的损失函数来提高多标记分类性能。

    

    设计用于解决典型监督分类问题的算法只能从固定的样本和标签集中学习，这使它们不适用于实际世界中数据以样本流的形式到达，并且往往随时间关联着多个标签。这促使研究与任务无关的连续多标签学习问题。虽然最近的文献中提出了使用深度学习方法进行连续多标签学习的算法，但它们往往具有较大的计算量。虽然脉冲神经网络（SNNs）提供了一种计算效率高的人工神经网络替代方案，但现有文献尚未将其用于连续多标签学习。此外，准确地确定SNNs的多个标签仍然是一个开放的研究问题。本研究提出了一种双输出脉冲架构（DOSA）来弥合这些研究差距。还提出了一种新的不平衡感知的损失函数，从而提高了多标记分类性能。

    Algorithms designed for addressing typical supervised classification problems can only learn from a fixed set of samples and labels, making them unsuitable for the real world, where data arrives as a stream of samples often associated with multiple labels over time. This motivates the study of task-agnostic continual multi-label learning problems. While algorithms using deep learning approaches for continual multi-label learning have been proposed in the recent literature, they tend to be computationally heavy. Although spiking neural networks (SNNs) offer a computationally efficient alternative to artificial neural networks, existing literature has not used SNNs for continual multi-label learning. Also, accurately determining multiple labels with SNNs is still an open research problem. This work proposes a dual output spiking architecture (DOSA) to bridge these research gaps. A novel imbalance-aware loss function is also proposed, improving the multi-label classification performance o
    
[^49]: 机器人跨领域策略转移综合调查

    A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents

    [https://arxiv.org/abs/2402.04580](https://arxiv.org/abs/2402.04580)

    这篇论文综述了机器人跨领域策略转移方法，讨论了从目标领域采集无偏数据的挑战，以及从源领域获取数据的成本效益性。同时，总结了不同问题设置下的设计考虑和方法。

    

    机器学习和具身人工智能领域的蓬勃发展引发了对大量数据的需求增加。然而，由于昂贵的数据收集过程和严格的安全要求，从目标领域收集足够的无偏数据仍然是一个挑战。因此，研究人员经常采用易于获取的源领域数据（例如模拟和实验室环境），以实现成本效益的数据获取和快速模型迭代。然而，这些源领域的环境和具身方式可能与目标领域的特征相差很大，强调了有效的跨领域策略转移方法的需求。本文对现有的跨领域策略转移方法进行了系统综述。通过对领域差距的精细分类，我们总结了每个问题设置的总体见解和设计考虑。我们还就使用的关键方法进行了高层次讨论

    The burgeoning fields of robot learning and embodied AI have triggered an increasing demand for large quantities of data. However, collecting sufficient unbiased data from the target domain remains a challenge due to costly data collection processes and stringent safety requirements. Consequently, researchers often resort to data from easily accessible source domains, such as simulation and laboratory environments, for cost-effective data acquisition and rapid model iteration. Nevertheless, the environments and embodiments of these source domains can be quite different from their target domain counterparts, underscoring the need for effective cross-domain policy transfer approaches. In this paper, we conduct a systematic review of existing cross-domain policy transfer methods. Through a nuanced categorization of domain gaps, we encapsulate the overarching insights and design considerations of each problem setting. We also provide a high-level discussion about the key methodologies used
    
[^50]: S-Agents: 自组织代理在无限环境中的应用

    S-Agents: self-organizing agents in open-ended environment

    [https://arxiv.org/abs/2402.04578](https://arxiv.org/abs/2402.04578)

    S-Agents是一个自组织代理系统，通过引入代理树结构、沙漏代理架构和非阻塞协作方法，实现了在无限环境中高效协调代理的能力，提供了优化协作效率和灵活性的解决方案。

    

    利用大型语言模型（LLMs），自主代理能够显著提升，具备处理各种任务的能力。在无限环境中，为了提高效率和效果，优化协作需要灵活的调整。然而，当前的研究主要强调固定的、任务导向的工作流程，忽视了以代理为中心的组织结构。受人类组织行为的启发，我们引入了一种自组织代理系统（S-Agents），其中包括动态工作流程的“代理树”结构、平衡信息优先级的“沙漏代理架构”以及允许代理之间异步执行任务的“非阻塞协作”方法。这种结构可以自主协调一组代理，有效地解决无限且动态的环境挑战，无需人工干预。我们的实验表明，S-Agents能够熟练地执行协作建筑任务和资源收集。

    Leveraging large language models (LLMs), autonomous agents have significantly improved, gaining the ability to handle a variety of tasks. In open-ended settings, optimizing collaboration for efficiency and effectiveness demands flexible adjustments. Despite this, current research mainly emphasizes fixed, task-oriented workflows and overlooks agent-centric organizational structures. Drawing inspiration from human organizational behavior, we introduce a self-organizing agent system (S-Agents) with a "tree of agents" structure for dynamic workflow, an "hourglass agent architecture" for balancing information priorities, and a "non-obstructive collaboration" method to allow asynchronous task execution among agents. This structure can autonomously coordinate a group of agents, efficiently addressing the challenges of an open and dynamic environment without human intervention. Our experiments demonstrate that S-Agents proficiently execute collaborative building tasks and resource collection i
    
[^51]: OIL-AD: 一种用于顺序决策序列的异常检测框架

    OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences

    [https://arxiv.org/abs/2402.04567](https://arxiv.org/abs/2402.04567)

    本论文提出了一种用于顺序决策序列的异常检测框架，通过提取行为特征来检测异常。这种方法克服了强化学习方法在真实世界中应用困难的问题，并提供了关于异常的足够信息。

    

    决策序列中的异常检测是一个具有挑战性的问题，因为正常性表示学习的复杂性和任务的顺序性质。大部分基于强化学习（RL）的现有方法由于对环境动态、奖励信号和与环境的在线交互等不切实际的假设，在现实世界中难以实施。为了解决这些限制，我们提出了一种名为离线模仿学习异常检测（OIL-AD）的无监督方法，它使用两个提取的行为特征（动作优化和顺序关联）来检测决策序列中的异常。我们的离线学习模型是基于变压器策略网络的行为克隆的适应，我们修改了训练过程，从正常轨迹中学习Q函数和状态值函数。我们认为，Q函数和状态值函数可以提供关于异常的足够信息。

    Anomaly detection in decision-making sequences is a challenging problem due to the complexity of normality representation learning and the sequential nature of the task. Most existing methods based on Reinforcement Learning (RL) are difficult to implement in the real world due to unrealistic assumptions, such as having access to environment dynamics, reward signals, and online interactions with the environment. To address these limitations, we propose an unsupervised method named Offline Imitation Learning based Anomaly Detection (OIL-AD), which detects anomalies in decision-making sequences using two extracted behaviour features: action optimality and sequential association. Our offline learning model is an adaptation of behavioural cloning with a transformer policy network, where we modify the training process to learn a Q function and a state value function from normal trajectories. We propose that the Q function and the state value function can provide sufficient information about 
    
[^52]: 基于注意力引导的CAM：自注意力指导下视觉变换器的视觉解释

    Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention

    [https://arxiv.org/abs/2402.04563](https://arxiv.org/abs/2402.04563)

    本文提出了一种基于注意力引导的可视化方法，可以在视觉变换器中提供高级语义解释，以便充分利用其在各种应用中的潜力。

    

    视觉变换器（ViT）是计算机视觉领域中最常用的模型之一，在各种任务上表现出色。为了充分利用ViT架构在各种应用中，需要适当的可视化方法并具有良好的定位性能，但由于其独特的结构，目前在CNN模型中使用的这些方法在ViT中仍然不可用。在这项工作中，我们提出了一种应用于ViT的注意力引导可视化方法，为其决策提供高级语义解释。我们的方法选择性地聚合从分类输出直接传播到每个自注意力的梯度，从而收集从输入图像的每个位置提取的图像特征的贡献。这些梯度还受到标准化的自注意力得分的指导，这些得分是成对的补丁相关性得分。它们用于有效检测补丁级上下文信息上的梯度。

    Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected
    
[^53]: 大语言模型代理能够模拟人类的信任行为吗？

    Can Large Language Model Agents Simulate Human Trust Behaviors?

    [https://arxiv.org/abs/2402.04559](https://arxiv.org/abs/2402.04559)

    大语言模型代理能够模拟人类的信任行为，表现出在信任游戏中的信任行为，并且与人类行为具有高度一致性，但存在一些偏见和对代理与人类的差异。

    

    大语言模型（LLM）代理已经越来越多地被采用作为模拟工具，用于模拟人类在社会科学等领域中的行为。然而，一个基本的问题仍然存在：LLM代理是否真的能够模拟人类行为？在本文中，我们专注于人类互动中最关键的行为之一，信任，旨在调查LLM代理是否能够模拟人类的信任行为。我们首先发现，在被行为经济学广泛接受的信任游戏框架下，LLM代理通常表现出信任行为，称为代理信任。然后，我们发现LLM代理在信任行为方面与人类具有较高的行为一致性，表明使用LLM代理模拟人类的信任行为是可行的。此外，我们还探索了代理信任中的偏见以及代理信任在对代理和人类之间的差异方面的内在特性。我们还探讨了包括高级推理策略在内的条件下代理信任的内在特性。

    Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science. However, one fundamental question remains: can LLM agents really simulate human behaviors? In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not LLM agents can simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, indicating the feasibility to simulate human trust behaviors with LLM agents. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strate
    
[^54]: 使用软件自生成的引导学习多样化策略

    Learning Diverse Policies with Soft Self-Generated Guidance

    [https://arxiv.org/abs/2402.04539](https://arxiv.org/abs/2402.04539)

    本文提出了一种使用多样化的过去轨迹作为引导的方法，以实现更快、更高效的在线强化学习，即使这些轨迹是次优的或没有高奖励。通过将过去轨迹视为引导，而不是模仿它们，本方法可以使策略跟随和扩展过去的轨迹同时仍保持

    

    强化学习中，稀疏和具有误导性的奖励使得学习变得困难，因为几乎很少能够获得非零奖励。因此，智能体计算的梯度可能是随机的且缺乏有效信息。最近的研究利用先前经验的内存缓冲区可以使学习过程更高效。然而，现有方法通常要求这些经验必须成功，并可能过度利用它们，这可能导致智能体采取次优的行为。本文提出了一种方法，即使用多样化的过去轨迹进行更快、更高效的在线强化学习，即使这些轨迹是次优的或没有高奖励。所提出的算法结合了策略改进步骤和使用离线演示数据的额外探索步骤。本文的主要贡献是，将多样化的过去轨迹视为引导而不是模仿它们，我们的方法使策略跟随和扩展过去的轨迹，同时仍保持

    Reinforcement learning (RL) with sparse and deceptive rewards is challenging because non-zero rewards are rarely obtained. Hence, the gradient calculated by the agent can be stochastic and without valid information. Recent studies that utilize memory buffers of previous experiences can lead to a more efficient learning process. However, existing methods often require these experiences to be successful and may overly exploit them, which can cause the agent to adopt suboptimal behaviors. This paper develops an approach that uses diverse past trajectories for faster and more efficient online RL, even if these trajectories are suboptimal or not highly rewarded. The proposed algorithm combines a policy improvement step with an additional exploration step using offline demonstration data. The main contribution of this paper is that by regarding diverse past trajectories as guidance, instead of imitating them, our method directs its policy to follow and expand past trajectories while still be
    
[^55]: 基于触觉的从颗粒介质中检索物体的研究

    Tactile-based Object Retrieval From Granular Media

    [https://arxiv.org/abs/2402.04536](https://arxiv.org/abs/2402.04536)

    这项研究介绍了一种基于触觉反馈的机器人操作方法，用于在颗粒介质中检索埋藏的物体。通过模拟传感器噪声进行端到端训练，实现了自然出现的学习推动行为，并成功将其迁移到实际硬件上。

    

    我们介绍了一种名为GEOTACT的机器人操作方法，能够在颗粒介质中检索埋藏的物体。这是一项具有挑战性的任务，因为需要与颗粒介质进行交互，并且仅依靠触觉反馈来完成，因为一个埋藏的物体可能完全被视觉隐藏。在这种环境中，触觉反馈本身具有挑战性，因为需要与周围介质进行普遍接触，并且由触觉读数引起的固有噪声水平。为了解决这些挑战，我们使用了一种通过模拟传感器噪声进行端到端训练的学习方法。我们展示了我们的问题表述导致了学习推动行为的自然出现，操作器使用这些行为来减少不确定性并将物体引导到稳定的抓取位置，尽管存在假的和噪声的触觉读数。我们还引入了一种培训方案，可以在仿真中学习这些行为，并在实际硬件上进行零样本迁移。据我们所知，GEOTACT是第一个这样的方法。

    We introduce GEOTACT, a robotic manipulation method capable of retrieving objects buried in granular media. This is a challenging task due to the need to interact with granular media, and doing so based exclusively on tactile feedback, since a buried object can be completely hidden from vision. Tactile feedback is in itself challenging in this context, due to ubiquitous contact with the surrounding media, and the inherent noise level induced by the tactile readings. To address these challenges, we use a learning method trained end-to-end with simulated sensor noise. We show that our problem formulation leads to the natural emergence of learned pushing behaviors that the manipulator uses to reduce uncertainty and funnel the object to a stable grasp despite spurious and noisy tactile readings. We also introduce a training curriculum that enables learning these behaviors in simulation, followed by zero-shot transfer to real hardware. To the best of our knowledge, GEOTACT is the first meth
    
[^56]: RA-Rec:基于LLM的推荐系统的高效ID表示对齐框架

    RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation

    [https://arxiv.org/abs/2402.04527](https://arxiv.org/abs/2402.04527)

    这篇论文提出了一种基于LLM的推荐系统的高效ID表示对齐框架RA-Rec，通过将预训练的ID嵌入到LLMs中，并设计创新的对齐模块和高效调整方法，实现了在推荐系统中的显著性能优化。

    

    大语言模型(LLM)最近已经成为各种自然语言处理任务的强大工具，为LLM和推荐系统的结合带来了新的潮流，称为LLM-based RS。目前的方法通常分为两种主要范例，即ID直接使用范例和ID翻译范例，指出它们的核心弱点在于缺乏推荐知识和独特性。为了解决这个限制，我们提出了一种新的范例，即ID表示，它以一种互补的方式将预训练的ID嵌入到LLMs中。在这项工作中，我们提出了RA-Rec，一种基于LLM的推荐系统的高效ID表示对齐框架，与多种基于ID的方法和LLM架构兼容。具体而言，我们将ID嵌入视为软提示，并设计了一种创新的对齐模块和一种用于对齐的高效调整方法，以及为对齐定制的数据构建。大量实验证明，RA-Rec在性能上显著优于其他方法。

    Large language models (LLM) have recently emerged as a powerful tool for a variety of natural language processing tasks, bringing a new surge of combining LLM with recommendation systems, termed as LLM-based RS. Current approaches generally fall into two main paradigms, the ID direct usage paradigm and the ID translation paradigm, noting their core weakness stems from lacking recommendation knowledge and uniqueness. To address this limitation, we propose a new paradigm, ID representation, which incorporates pre-trained ID embeddings into LLMs in a complementary manner. In this work, we present RA-Rec, an efficient ID representation alignment framework for LLM-based recommendation, which is compatible with multiple ID-based methods and LLM architectures. Specifically, we treat ID embeddings as soft prompts and design an innovative alignment module and an efficient tuning method with tailored data construction for alignment. Extensive experiments demonstrate RA-Rec substantially outperfo
    
[^57]: 关于现代Hopfield模型计算限制的一个细粒度复杂性分析

    On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis

    [https://arxiv.org/abs/2402.04520](https://arxiv.org/abs/2402.04520)

    通过细粒度复杂性分析，我们研究了现代Hopfield模型的记忆检索计算限制，发现了一种基于模式范数的相变行为，并且建立了有效变体的上界条件。使用低秩逼近的方法，我们提供了有效构造的示例，同时证明了计算时间下界、记忆检索误差界和指数记忆容量。

    

    我们从细粒度复杂性分析的角度研究了现代Hopfield模型的记忆检索动力学的计算限制。我们的主要贡献是基于模式的范数对所有可能的现代Hopfield模型的效率进行相变行为的刻画。具体来说，我们建立了对输入查询模式和记忆模式的范数的上界标准。仅在这个标准之下，假设满足Strong Exponential Time Hypothesis (SETH)，存在子二次（高效）变体的现代Hopfield模型。为了展示我们的理论，当有效标准成立时，我们提供了现代Hopfield模型使用低秩逼近的有效构造的正式示例。这包括一个计算时间的下界导出，与$\Max\{$存储的记忆模式数量，输入查询序列的长度$\}$线性缩放。此外，我们证明了记忆检索误差界和指数记忆容量。

    We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
    
[^58]: 深度强化学习在下一代网络中的自适应交通路由中的应用研究

    A Deep Reinforcement Learning Approach for Adaptive Traffic Routing in Next-gen Networks

    [https://arxiv.org/abs/2402.04515](https://arxiv.org/abs/2402.04515)

    本论文介绍了一种在下一代网络中应用的深度强化学习方法，用于自适应交通路由。通过设计一个深度图卷积神经网络和采用深度Q学习技术，可以从网络拓扑和链路节点属性中学习流量的行为。

    

    下一代网络需要在管理方面进行重大改进，以实现自动化并根据流量动态调整网络配置。软件定义网络（SDN）和可编程交换机的出现使得网络具备了灵活性和可编程性。然而，传统的决策流量策略的技术通常基于手工优化和启发式算法。这些技术尽管为下一代网络提供了可行的解决方案，却基于不现实的假设，例如静态网络负载和拓扑结构。本文中，我们设计和开发了一种深度强化学习（DRL）方法来实现自适应交通路由。我们设计了一个深度图卷积神经网络（DGCNN），并将其集成到DRL框架中，以从网络拓扑和链路节点属性中学习流量行为。我们采用深度Q学习技术，在DRL框架中训练DGCNN模型，无需手工编程优化。

    Next-gen networks require significant evolution of management to enable automation and adaptively adjust network configuration based on traffic dynamics. The advent of software-defined networking (SDN) and programmable switches enables flexibility and programmability. However, traditional techniques that decide traffic policies are usually based on hand-crafted programming optimization and heuristic algorithms. These techniques make non-realistic assumptions, e.g., considering static network load and topology, to obtain tractable solutions, which are inadequate for next-gen networks. In this paper, we design and develop a deep reinforcement learning (DRL) approach for adaptive traffic routing. We design a deep graph convolutional neural network (DGCNN) integrated into the DRL framework to learn the traffic behavior from not only the network topology but also link and node attributes. We adopt the Deep Q-Learning technique to train the DGCNN model in the DRL framework without the need f
    
[^59]: 不需搜索即可实现大师级国际象棋对局

    Grandmaster-Level Chess Without Search

    [https://arxiv.org/abs/2402.04494](https://arxiv.org/abs/2402.04494)

    本研究通过在庞大的国际象棋数据集上进行训练，使用了一个270M参数的Transformer模型，不依赖于复杂的启发式算法或显式搜索，取得了大师级水平的国际象棋对局的成功。模型在Lichess闪电战评分上达到了2895，解决了一系列具有挑战性的国际象棋谜题，优于AlphaZero和GPT-3.5-turbo-instruct。通过系统研究，我们发现大规模的模型和数据集对于实现强大的国际象棋对局效果是至关重要的。

    

    最新的机器学习的突破性成功主要归功于规模化，即基于注意力的大规模架构和空前规模的数据集。本文研究了对国际象棋的大规模训练的影响。与传统的依赖复杂启发式算法、显式搜索或二者结合的国际象棋引擎不同，我们通过在1000万局国际象棋对局的数据集上使用监督学习训练了一个拥有2.7亿参数的Transformer模型。我们用强大的Stockfish 16引擎提供的动作值来注释数据集中的每个棋局，产生大约150亿个数据点。我们最大的模型在Lichess闪电战Elo上达到了2895，成功解决了一系列具有挑战性的国际象棋谜题，而无需任何特定领域的调整或显式搜索算法。我们还证明了我们的模型优于AlphaZero的策略和价值网络（无MCTS）以及GPT-3.5-turbo-instruct。对模型和数据集规模的系统研究表明，强大的国际象棋对局可以在规模上取得最佳效果。

    The recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale. This paper investigates the impact of training at scale for chess. Unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270M parameter transformer model with supervised learning on a dataset of 10 million chess games. We annotate each board in the dataset with action-values provided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our largest model reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms. We also show that our model outperforms AlphaZero's policy and value networks (without MCTS) and GPT-3.5-turbo-instruct. A systematic investigation of model and dataset size shows that strong chess 
    
[^60]: 通过叙述检测语言模型中的模式崩溃

    Detecting Mode Collapse in Language Models via Narration

    [https://arxiv.org/abs/2402.04477](https://arxiv.org/abs/2402.04477)

    通过研究来自三个OpenAI语言模型的故事，我们发现GPT-3的更新版本逐渐出现了“模式崩溃”的现象。

    

    没有两个作者写作方式相同。从词汇到修辞手法，在书面叙述中呈现出的个人特色，暗示了一位特定的作者，文学理论家将其称为隐含或虚拟作者，与文本的实际作者或叙述者不同。早期使用来自各种不协调来源的未经过滤的训练集进行训练的大型语言模型产生了不连贯的个性，这对于对话任务来说是有问题的，但对于从多个观点采样文学却是有用的。近年来，在对齐研究方面取得的成功使研究人员能够通过指导调整和从人类反馈中进行强化学习（RLHF）来对语言模型施加主观一致的人物形象，但对齐模型是否保留了对模拟任意虚拟作者的能力几乎没有受到审查。通过研究来自三个OpenAI语言模型的4,374个故事，我们展示了随着GPT-3的更新版本，越来越多的“模式崩溃”现象的发生。

    No two authors write alike. Personal flourishes invoked in written narratives, from lexicon to rhetorical devices, imply a particular author--what literary theorists label the implied or virtual author; distinct from the real author or narrator of a text. Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives. Successes in alignment research in recent years have allowed researchers to impose subjectively consistent personae on language models via instruction tuning and reinforcement learning from human feedback (RLHF), but whether aligned models retain the ability to model an arbitrary virtual author has received little scrutiny. By studying 4,374 stories sampled from three OpenAI language models, we show successive versions of GPT-3 suffer from increasing degrees of "mode collapse" whereby overf
    
[^61]: 双视图视觉背景化的网页导航

    Dual-View Visual Contextualization for Web Navigation

    [https://arxiv.org/abs/2402.04476](https://arxiv.org/abs/2402.04476)

    本文提出了一种通过网页截图中元素的“双视图”来进行 HTML 元素的背景化的方法，这种方法通过将每个元素与其邻居元素进行背景化，使用文本和视觉特征，使得HTML元素的结果表示对于代理执行操作更加信息丰富。

    

    自动网页导航旨在构建一个可以根据语言指令在实际网站上执行复杂和多样任务的网络代理。现有工作主要是以 HTML 文档作为输入，HTML 文档定义了网页的内容和操作空间（即可操作元素和操作）。然而，HTML 文档可能无法为每个元素提供清晰的任务相关背景，使得选择正确的（一系列的）操作变得困难。本文提出通过网页截图中元素的“双视图”来进行 HTML 元素的背景化：每个 HTML 元素在截图中有其对应的边界框和视觉内容。我们基于一个洞察力——网页开发者倾向于在网页上将任务相关元素放置在附近以增强用户体验，并提出将每个元素与其邻居元素进行背景化，使用文本和视觉特征。HTML 元素的结果表示对于代理执行操作更加信息丰富。

    Automatic web navigation aims to build a web agent that can follow language instructions to execute complex and diverse tasks on real-world websites. Existing work primarily takes HTML documents as input, which define the contents and action spaces (i.e., actionable elements and operations) of webpages. Nevertheless, HTML documents may not provide a clear task-related context for each element, making it hard to select the right (sequence of) actions. In this paper, we propose to contextualize HTML elements through their "dual views" in webpage screenshots: each HTML element has its corresponding bounding box and visual content in the screenshot. We build upon the insight -- web developers tend to arrange task-related elements nearby on webpages to enhance user experiences -- and propose to contextualize each element with its neighbor elements, using both textual and visual features. The resulting representations of HTML elements are more informative for the agent to take action. We val
    
[^62]: NVIDIA Holoscan中面向医疗AI系统的确定性端到端延迟的实现

    Towards Deterministic End-to-end Latency for Medical AI Systems in NVIDIA Holoscan

    [https://arxiv.org/abs/2402.04466](https://arxiv.org/abs/2402.04466)

    本文针对NVIDIA Holoscan平台中的医疗AI系统的端到端延迟问题提出了解决方案，通过优化异构GPU工作负载，实现了确定性的延迟表现。

    

    AI和ML技术的引入已经彻底改变了医疗诊断和治疗。医疗设备制造商渴望最大限度地利用AI和ML的优势，将多个应用程序整合到一个平台上。然而，同时执行多个AI应用程序，每个应用程序都有自己的可视化组件，会导致不可预测的端到端延迟，主要是由于GPU资源争用引起的。为了解决这个问题，制造商通常会为不同的AI应用程序部署单独的工作站，从而增加了财务、能源和维护成本。本文针对NVIDIA的Holoscan平台中的这些挑战提出了解决方案，Holoscan是一个用于流式传感器数据和图像的实时AI系统。我们提出了一个针对异构GPU工作负载优化的系统设计，包括计算和图形任务。我们的设计利用CUDA MPS对计算工作负载进行空间划分，并分隔计算和图形处理单元。

    The introduction of AI and ML technologies into medical devices has revolutionized healthcare diagnostics and treatments. Medical device manufacturers are keen to maximize the advantages afforded by AI and ML by consolidating multiple applications onto a single platform. However, concurrent execution of several AI applications, each with its own visualization components, leads to unpredictable end-to-end latency, primarily due to GPU resource contentions. To mitigate this, manufacturers typically deploy separate workstations for distinct AI applications, thereby increasing financial, energy, and maintenance costs. This paper addresses these challenges within the context of NVIDIA's Holoscan platform, a real-time AI system for streaming sensor data and images. We propose a system design optimized for heterogeneous GPU workloads, encompassing both compute and graphics tasks. Our design leverages CUDA MPS for spatial partitioning of compute workloads and isolates compute and graphics proc
    
[^63]: 解决人工智能中的十个难题

    Ten Hard Problems in Artificial Intelligence We Must Get Right

    [https://arxiv.org/abs/2402.04464](https://arxiv.org/abs/2402.04464)

    这项研究探讨了人工智能领域中的十个难题，包括通用能力的发展、性能保障、目标对齐、广泛应用、经济变革、全民参与、社会负责任部署、地缘政治变革、技术治理和哲学变革管理。要解决这些难题，需要进一步的研究和推进。

    

    我们探讨了AI2050中阻碍人工智能发展和引发人工智能风险的"难题"：（1）发展系统的通用能力；（2）确保人工智能系统和其训练过程的性能；（3）将系统目标与人类目标对齐；（4）实现人工智能在现实生活中的广泛应用；（5）应对经济变革；（6）确保全民参与；（7）同时确保社会负责任的部署；（8）解决人工智能引发的地缘政治变革；（9）推动对技术的健全治理；以及（10）管理生活在人工智能时代中的哲学变革。针对每个问题，我们概述了相关领域，指出了重要的最近研究工作，并提出了推进的方式。[注：本论文回顾的文献时间截至2023年1月。]

    We explore the AI2050 "hard problems" that block the promise of AI and cause AI risks: (1) developing general capabilities of the systems; (2) assuring the performance of AI systems and their training processes; (3) aligning system goals with human goals; (4) enabling great applications of AI in real life; (5) addressing economic disruptions; (6) ensuring the participation of all; (7) at the same time ensuring socially responsible deployment; (8) addressing any geopolitical disruptions that AI causes; (9) promoting sound governance of the technology; and (10) managing the philosophical disruptions for humans living in the age of AI. For each problem, we outline the area, identify significant recent work, and suggest ways forward. [Note: this paper reviews literature through January 2023.]
    
[^64]: PreGIP: 针对深度知识产权保护的图神经网络预训练水印技术

    PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection

    [https://arxiv.org/abs/2402.04435](https://arxiv.org/abs/2402.04435)

    PreGIP是针对深度知识产权保护的一种图神经网络预训练水印技术，它通过添加无任务的水印损失来给预训练的GNN编码器的嵌入空间添加水印，并采用抗微调的水印注入方法

    

    图神经网络（GNNs）的预训练在促进各种下游任务中显示出巨大的能力。由于预训练通常需要大量的数据和计算资源，预训练的GNNs成为合法拥有者的高价值知识产权（IP）。然而，对手可能会非法复制和部署预训练的GNN模型用于其下游任务。虽然已经开始尝试为IP保护添加GNN分类器的水印，但这些方法需要目标分类任务才能进行水印处理，因此不适用于GNN模型的自监督预训练。因此，在这项工作中，我们提出了一个新框架PreGIP，用于在保持嵌入空间高质量的同时，给GNN编码器的预训练添加水印以进行IP保护。PreGIP引入了无任务的水印损失来给预训练的GNN编码器的嵌入空间添加水印。同时采用了抗微调的水印注入方法。我们还进行了理论分析和扩展实验证明了方法的有效性和鲁棒性。

    Pretraining on Graph Neural Networks (GNNs) has shown great power in facilitating various downstream tasks. As pretraining generally requires huge amount of data and computational resources, the pretrained GNNs are high-value Intellectual Properties (IP) of the legitimate owner. However, adversaries may illegally copy and deploy the pretrained GNN models for their downstream tasks. Though initial efforts have been made to watermark GNN classifiers for IP protection, these methods require the target classification task for watermarking, and thus are not applicable to self-supervised pretraining of GNN models. Hence, in this work, we propose a novel framework named PreGIP to watermark the pretraining of GNN encoder for IP protection while maintain the high-quality of the embedding space. PreGIP incorporates a task-free watermarking loss to watermark the embedding space of pretrained GNN encoder. A finetuning-resistant watermark injection is further deployed. Theoretical analysis and exte
    
[^65]: 在R中研究易受攻击的代码实体

    Studying Vulnerable Code Entities in R

    [https://arxiv.org/abs/2402.04421](https://arxiv.org/abs/2402.04421)

    本研究旨在调查Code-PLMs在R中的代码实体的易受攻击性。通过使用R数据集和CodeAttack黑盒攻击模型，我们研究了该模型如何攻击不同的实体。这是了解R令牌类型重要性的第一步。

    

    预训练的代码语言模型（Code-PLMs）在过去几年中展示了许多进展，并在许多软件工程任务中取得了最先进的结果。这些模型主要针对流行的编程语言，如Java和Python，而排除了许多其他语言，如R。尽管R拥有广泛的开发者和用户社区，但对Code-PLMs在R中的适用性了解甚少。在本初步研究中，我们旨在调查Code-PLMs在R中的代码实体的易受攻击性。为此，我们使用了一个包含代码和注释对的R数据集，然后应用CodeAttack，一个利用代码结构生成对抗性代码样本的黑盒攻击模型。我们研究了该模型如何攻击R中的不同实体。这是了解R令牌类型相对于流行的编程语言（如Java）重要性的第一步。我们将研究范围限制在代码摘要上。我们的结果显示，最易受攻击的代码实体是

    Pre-trained Code Language Models (Code-PLMs) have shown many advancements and achieved state-of-the-art results for many software engineering tasks in the past few years. These models are mainly targeted for popular programming languages such as Java and Python, leaving out many other ones like R. Though R has a wide community of developers and users, there is little known about the applicability of Code-PLMs for R. In this preliminary study, we aim to investigate the vulnerability of Code-PLMs for code entities in R. For this purpose, we use an R dataset of code and comment pairs and then apply CodeAttack, a black-box attack model that uses the structure of code to generate adversarial code samples. We investigate how the model can attack different entities in R. This is the first step towards understanding the importance of R token types, compared to popular programming languages (e.g., Java). We limit our study to code summarization. Our results show that the most vulnerable code en
    
[^66]: 测量机器学习中的刻板印象伤害：需要了解谁正在受到哪些错误以及以何种方式受到伤害

    Measuring machine learning harms from stereotypes: requires understanding who is being harmed by which errors in what ways

    [https://arxiv.org/abs/2402.04420](https://arxiv.org/abs/2402.04420)

    通过调查研究和实验研究，本文研究了机器学习错误对人们的影响，发现强化刻板印象的错误引起更多主观上的伤害体验，而违反刻板印象的错误对男性产生更大的主观上的伤害，有助于我们理解机器学习在引发刻板印象和伤害方面的作用。

    

    随着机器学习应用的普及，我们需要了解它们可能造成的伤害。然而，当前的公平性指标很少基于人类对伤害的心理体验。借鉴刻板印象的社会心理学，我们以图像搜索中的性别刻板印象为案例研究，研究了人们对机器学习错误的反应。首先，我们使用调查研究表明，并非所有的机器学习错误都反映了刻板印象，也没有同样的伤害程度。然后，在实验研究中，我们随机使参与者接触到强化、违反和中性的机器学习错误。我们发现，强化刻板印象的错误引起更多主观上的伤害体验，但对认知信念、态度或行为的改变很小。这种体验上的伤害对女性影响更大。然而，某些违反刻板印象的错误对男性产生更大的主观上的伤害，可能是由于对男性阳刚性的威胁感知。

    As machine learning applications proliferate, we need an understanding of their potential for harm. However, current fairness metrics are rarely grounded in human psychological experiences of harm. Drawing on the social psychology of stereotypes, we use a case study of gender stereotypes in image search to examine how people react to machine learning errors. First, we use survey studies to show that not all machine learning errors reflect stereotypes nor are equally harmful. Then, in experimental studies we randomly expose participants to stereotype-reinforcing, -violating, and -neutral machine learning errors. We find stereotype-reinforcing errors induce more experientially (i.e., subjectively) harmful experiences, while having minimal changes to cognitive beliefs, attitudes, or behaviors. This experiential harm impacts women more than men. However, certain stereotype-violating errors are more experientially harmful for men, potentially due to perceived threats to masculinity. We conc
    
[^67]: VampPrior混合模型

    The VampPrior Mixture Model

    [https://arxiv.org/abs/2402.04412](https://arxiv.org/abs/2402.04412)

    本论文提出了VampPrior混合模型（VMM），它是一种新颖的DLVM先验，可用于深度潜变量模型的集成和聚类，通过改善当前聚类先验的不足，并提出了一个清晰区分变分和先验参数的推理过程。使用VMM的变分自动编码器在基准数据集上取得了强大的聚类性能，将VMM与scVI相结合可以显著提高其性能，并自动将细胞分组为具有生物意义的聚类。

    

    当前用于深度潜变量模型（DLVMs）的聚类先验需要预先定义聚类的数量，并且容易受到较差的初始化的影响。解决这些问题可以通过同时执行集成和聚类的方式极大地改进基于深度学习的scRNA-seq分析。我们将VampPrior（Tomczak和Welling，2018）调整为Dirichlet过程高斯混合模型，得到VampPrior混合模型（VMM），这是一种新颖的DLVM先验。我们提出了一个推理过程，交替使用变分推理和经验贝叶斯，以清楚地区分变分和先验参数。在基准数据集上使用VMM的变分自动编码器获得了极具竞争力的聚类性能。将VMM与广受欢迎的scRNA-seq集成方法scVI（Lopez等，2018）相结合，显著改善了其性能，并自动将细胞分组为具有生物意义的聚类。

    Current clustering priors for deep latent variable models (DLVMs) require defining the number of clusters a-priori and are susceptible to poor initializations. Addressing these deficiencies could greatly benefit deep learning-based scRNA-seq analysis by performing integration and clustering simultaneously. We adapt the VampPrior (Tomczak & Welling, 2018) into a Dirichlet process Gaussian mixture model, resulting in the VampPrior Mixture Model (VMM), a novel prior for DLVMs. We propose an inference procedure that alternates between variational inference and Empirical Bayes to cleanly distinguish variational and prior parameters. Using the VMM in a Variational Autoencoder attains highly competitive clustering performance on benchmark datasets. Augmenting scVI (Lopez et al., 2018), a popular scRNA-seq integration method, with the VMM significantly improves its performance and automatically arranges cells into biologically meaningful clusters.
    
[^68]: 进一步实现公平、鲁棒和高效的联邦学习中客户贡献评估

    Towards Fair, Robust and Efficient Client Contribution Evaluation in Federated Learning

    [https://arxiv.org/abs/2402.04409](https://arxiv.org/abs/2402.04409)

    本文提出了一种名为FRECA的方法来评估联邦学习中客户的贡献。该方法使用FedTruth框架估计全局模型的真实更新，平衡来自所有客户的贡献，并排除恶意客户的影响。FRECA对于拜占庭攻击具有鲁棒性，并且具有高效性。

    

    由于各种原因，联邦学习（FL）中客户的性能可能会有所不同。评估每个客户的贡献对于客户选择和补偿至关重要。然而，由于客户通常具有非独立和同分布（non-iid）的数据，导致可能存在噪声或发散的更新，因此评估客户贡献具有挑战性。当无法访问客户的本地数据或基准根数据集时，恶意客户的风险会进一步增加。本文介绍了一种名为公平、鲁棒和高效客户评估（FRECA）的新方法，用于量化FL中的客户贡献。FRECA采用一种名为FedTruth的框架来估计全局模型的真实更新，平衡来自所有客户的贡献，并过滤出恶意客户的影响。该方法对拜占庭攻击具有鲁棒性，并采用了拜占庭鲁棒的聚合算法。FRECA还具有高效性，因为它仅仅在本地模型更新上操作，且只需要少量的全局通信。

    The performance of clients in Federated Learning (FL) can vary due to various reasons. Assessing the contributions of each client is crucial for client selection and compensation. It is challenging because clients often have non-independent and identically distributed (non-iid) data, leading to potentially noisy or divergent updates. The risk of malicious clients amplifies the challenge especially when there's no access to clients' local data or a benchmark root dataset. In this paper, we introduce a novel method called Fair, Robust, and Efficient Client Assessment (FRECA) for quantifying client contributions in FL. FRECA employs a framework called FedTruth to estimate the global model's ground truth update, balancing contributions from all clients while filtering out impacts from malicious ones. This approach is robust against Byzantine attacks and incorporates a Byzantine-resilient aggregation algorithm. FRECA is also efficient, as it operates solely on local model updates and requir
    
[^69]: 生成带有病人时间轴的电子健康记录的CEHR-GPT

    CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines

    [https://arxiv.org/abs/2402.04400](https://arxiv.org/abs/2402.04400)

    CEHR-GPT是一种使用病人时间轴生成电子健康记录的方法，能够处理EHR数据的合成、疾病进展分析等应用。

    

    合成电子健康记录（EHR）已成为推进医疗应用和机器学习模型的关键工具，特别是对于没有直接访问医疗数据的研究人员而言。尽管现有方法，如基于规则的方法和生成对抗网络（GAN），可以生成类似真实世界EHR数据的合成数据，但这些方法常常使用表格格式，忽略了病人历史的时间依赖性，限制了数据复制。最近，越来越多的人开始利用生成预训练转换器（GPT）来处理EHR数据。这使得可以进行疾病进展分析、人口估计、反事实推理和合成数据生成等应用。本研究关注合成数据生成，并演示了使用源自CEHR-BERT的特定病人表示训练GPT模型的能力，从而能够生成可无缝转换的病人序列。

    Synthetic Electronic Health Records (EHR) have emerged as a pivotal tool in advancing healthcare applications and machine learning models, particularly for researchers without direct access to healthcare data. Although existing methods, like rule-based approaches and generative adversarial networks (GANs), generate synthetic data that resembles real-world EHR data, these methods often use a tabular format, disregarding temporal dependencies in patient histories and limiting data replication. Recently, there has been a growing interest in leveraging Generative Pre-trained Transformers (GPT) for EHR data. This enables applications like disease progression analysis, population estimation, counterfactual reasoning, and synthetic data generation. In this work, we focus on synthetic data generation and demonstrate the capability of training a GPT model using a particular patient representation derived from CEHR-BERT, enabling us to generate patient sequences that can be seamlessly converted 
    
[^70]: 学习在时间序列下处理时间标签噪声

    Learning from Time Series under Temporal Label Noise

    [https://arxiv.org/abs/2402.04398](https://arxiv.org/abs/2402.04398)

    该论文研究了在时间序列下处理时间标签噪声的问题，提出了一种可以从数据中直接估计时间标签噪声函数并训练出噪声容忍分类器的方法，并在实验中展示了该方法在各种时间标签噪声函数下都取得了最先进的性能。

    

    许多顺序分类任务受到随时间变化的标签噪声的影响。这种噪声可能会导致标签质量随时间改善、恶化或周期性变化。我们首先提出和系统化了时间标签噪声的概念，这是关于时间序列顺序分类的一个未经研究的问题。在这种设置下，多个标签连续记录，同时受到一个与时间相关的噪声函数的干扰。我们首先展示了建模时间标签噪声函数的重要性，以及现有方法的持续低效。然后，我们提出了一种直接从数据中估计时间标签噪声函数的方法，可以训练出对噪声具有容忍性的分类器。我们展示了我们的方法在各种各样的时间标签噪声函数下，使用真实和合成数据在性能上达到了最先进水平。

    Many sequential classification tasks are affected by label noise that varies over time. Such noise can cause label quality to improve, worsen, or periodically change over time. We first propose and formalize temporal label noise, an unstudied problem for sequential classification of time series. In this setting, multiple labels are recorded in sequence while being corrupted by a time-dependent noise function. We first demonstrate the importance of modelling the temporal nature of the label noise function and how existing methods will consistently underperform. We then propose methods that can train noise-tolerant classifiers by estimating the temporal label noise function directly from data. We show that our methods lead to state-of-the-art performance in the presence of diverse temporal label noise functions using real and synthetic data.
    
[^71]: QuIP#: 使用哈达玛德非相干性和格书进行更好的LLM量化

    QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks

    [https://arxiv.org/abs/2402.04396](https://arxiv.org/abs/2402.04396)

    QuIP#是一种使用哈达玛德非相干性和格书的权重量化方法，能够在极限压缩范围下达到最先进的结果，并具有快速推理的优势。

    

    后训练量化(PTQ)通过将LLM的权重量化为低精度来减少其内存占用。在这项工作中，我们引入了QuIP#，一种仅基于权重的PTQ方法，使用了三种新技术，在极限压缩范围($\le$ 4比特每个权重)上取得了最先进的结果。首先，QuIP#通过使用随机哈达玛德变换改进了QuIP中的非相干处理，该方法更快且具有更好的理论特性。其次，QuIP#使用向量量化技术利用了非相干权重具有的球形亚高斯分布特性：具体地说，我们引入了一组基于高度对称$E_8$格书的硬件高效代码书，实现了最优的8维单位球装填。第三，QuIP#使用微调来提高对原始模型的忠实度。我们的实验证明，QuIP#优于现有的PTQ方法，能够实现新的PTQ扩展行为，并支持快速推理。

    Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision. In this work, we introduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes ($\le$ 4 bits per weight) using three novel techniques. First, QuIP# improves the incoherence processing from QuIP by using the randomized Hadamard transform, which is faster and has better theoretical properties. Second, QuIP# uses vector quantization techniques to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the original model. Our experiments show that QuIP# outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference.
    
[^72]: 密集乘法物理信息神经网络

    Densely Multiplied Physics Informed Neural Network

    [https://arxiv.org/abs/2402.04390](https://arxiv.org/abs/2402.04390)

    该论文通过改进神经网络架构，提出了一种密集乘法物理信息神经网络（DM-PINN）架构，它有效利用隐藏层的输出，显著提高了PINN的准确性和性能。

    

    尽管物理信息神经网络（Physics-Informed Neural Networks, PINNs）在处理非线性偏微分方程（PDEs）方面显示出巨大潜力，但常常会出现精度不足或获取不正确结果的问题。与大多数现有的解决方案不同，该论文改进了神经网络架构以提高PINN的性能。我们提出了一种密集乘法PINN（DM-PINN）架构，它将隐藏层的输出与所有后面的隐藏层的输出相乘。在不引入更多可训练参数的情况下，该有效机制可以显著提高PINN的准确性。所提出的架构在四个基准示例（Allan-Cahn方程，Helmholtz方程，Burgers方程和1D对流方程）上进行了评估。将所提出的架构与不同的PINN结构进行比较，证明了其卓越的性能。

    Although physics-informed neural networks (PINNs) have shown great potential in dealing with nonlinear partial differential equations (PDEs), it is common that PINNs will suffer from the problem of insufficient precision or obtaining incorrect outcomes. Unlike most of the existing solutions trying to enhance the ability of PINN by optimizing the training process, this paper improved the neural network architecture to improve the performance of PINN. We propose a densely multiply PINN (DM-PINN) architecture, which multiplies the output of a hidden layer with the outputs of all the behind hidden layers. Without introducing more trainable parameters, this effective mechanism can significantly improve the accuracy of PINNs. The proposed architecture is evaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation, Burgers equation and 1D convection equation). Comparisons between the proposed architecture and different PINN structures demonstrate the superior performance of 
    
[^73]: 使用答案集编程的反事实生成

    Counterfactual Generation with Answer Set Programming

    [https://arxiv.org/abs/2402.04382](https://arxiv.org/abs/2402.04382)

    本文提出了一个通过答案集编程自动生成反事实解释的框架，以应对机器学习模型决策的透明度和合理性需求。

    

    在贷款审批、保释审批、招聘等领域，越来越多地使用自动决策的机器学习模型。然而，大多数模型都是黑匣子，即无法揭示其预测决策的过程。出于透明度的需求，对这些预测需要进行解释。受影响的个体可能还希望解释为什么做出了某个决策。道德和法律考虑进一步要求告知个体可以进行哪些输入属性的更改以产生期望的结果。本文重点研究自动生成反事实解释这一问题。我们提出了一个名为反事实生成与s(CASP) (CFGS)的框架，利用答案集编程(ASP)和s(CASP)目标导向ASP系统，从基于规则的机器学习(RBML)算法生成的规则中自动生成反事实解释。

    Machine learning models that automate decision-making are increasingly being used in consequential areas such as loan approvals, pretrial bail approval, hiring, and many more. Unfortunately, most of these models are black-boxes, i.e., they are unable to reveal how they reach these prediction decisions. A need for transparency demands justification for such predictions. An affected individual might also desire explanations to understand why a decision was made. Ethical and legal considerations may further require informing the individual of changes in the input attribute that could be made to produce a desirable outcome. This paper focuses on the latter problem of automatically generating counterfactual explanations. We propose a framework Counterfactual Generation with s(CASP) (CFGS) that utilizes answer set programming (ASP) and the s(CASP) goal-directed ASP system to automatically generate counterfactual explanations from rules generated by rule-based machine learning (RBML) algorith
    
[^74]: 使用真实数据和替代数据进行学习的扩展规律

    Scaling laws for learning with real and surrogate data

    [https://arxiv.org/abs/2402.04376](https://arxiv.org/abs/2402.04376)

    本研究探讨了将替代数据与真实数据整合以进行训练的方案，发现整合替代数据能够显著降低测试误差，并提出了一个扩展规律来描述混合模型的测试误差，可以用于预测最优加权和收益。

    

    收集大量高质量的数据通常被限制在成本昂贵或不切实际的范围内, 这是机器学习中的一个关键瓶颈。相反地, 可以将来自目标分布的小规模数据集与来自公共数据集、不同情况下收集的数据或由生成模型合成的数据相结合, 作为替代数据。我们提出了一种简单的方案来将替代数据整合到训练中, 并使用理论模型和实证研究探索其行为。我们的主要发现是：(i) 整合替代数据可以显著降低原始分布的测试误差；(ii) 为了获得这种效益, 使用最优加权经验风险最小化非常关键；(iii) 在混合使用真实数据和替代数据训练的模型的测试误差可以很好地用一个扩展规律来描述。这可以用来预测最优加权和收益。

    Collecting large quantities of high-quality data is often prohibitively expensive or impractical, and a crucial bottleneck in machine learning. One may instead augment a small set of $n$ data points from the target distribution with data from more accessible sources like public datasets, data collected under different circumstances, or synthesized by generative models. Blurring distinctions, we refer to such data as `surrogate data'.   We define a simple scheme for integrating surrogate data into training and use both theoretical models and empirical studies to explore its behavior. Our main findings are: $(i)$ Integrating surrogate data can significantly reduce the test error on the original distribution; $(ii)$ In order to reap this benefit, it is crucial to use optimally weighted empirical risk minimization; $(iii)$ The test error of models trained on mixtures of real and surrogate data is well described by a scaling law. This can be used to predict the optimal weighting and the gai
    
[^75]: 行人过马路决策可以通过受干扰的视觉知觉下的有界最优决策解释

    Pedestrian crossing decisions can be explained by bounded optimal decision-making under noisy visual perception

    [https://arxiv.org/abs/2402.04370](https://arxiv.org/abs/2402.04370)

    本文提出了一个结合机械式模型和强化学习的行人过马路决策模型，可以解释行人的决策行为以及对视觉知觉的限制。模型成功地复现了多个已知的实证现象。

    

    本文提出了一个行人过马路决策模型，基于计算合理性理论。假设过马路决策是有界最优的，因为人类认知限制而产生这种有界性。之前的行人行为模型要么是“黑盒”机器学习模型，要么是具有明确对认知因素的假设的机械式模型，我们结合了这两种方法。具体而言，我们以机械方式模拟了人类视觉知觉的噪声和过马路时的奖励，但我们使用强化学习来学习有界最优行为策略。该模型可以复现比之前的模型更多已知的经验证据现象，特别是：（1）接近车辆到达的时间对行人是否接受缝隙的影响，车辆速度对（2）缝隙接受和（3）行人通过让行车辆的时间选择的影响，以及（4）这种过马路行时间选择对行人安全的影响。

    This paper presents a model of pedestrian crossing decisions, based on the theory of computational rationality. It is assumed that crossing decisions are boundedly optimal, with bounds on optimality arising from human cognitive limitations. While previous models of pedestrian behaviour have been either 'black-box' machine learning models or mechanistic models with explicit assumptions about cognitive factors, we combine both approaches. Specifically, we model mechanistically noisy human visual perception and assumed rewards in crossing, but we use reinforcement learning to learn bounded optimal behaviour policy. The model reproduces a larger number of known empirical phenomena than previous models, in particular: (1) the effect of the time to arrival of an approaching vehicle on whether the pedestrian accepts the gap, the effect of the vehicle's speed on both (2) gap acceptance and (3) pedestrian timing of crossing in front of yielding vehicles, and (4) the effect on this crossing timi
    
[^76]: PQMass: 使用概率质量估计的生成模型质量的概率评估

    PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation

    [https://arxiv.org/abs/2402.04355](https://arxiv.org/abs/2402.04355)

    PQMass是一种使用概率质量估计来评估生成模型质量的全面方法，能够直接处理高维数据，不依赖于假设或训练其他模型。

    

    我们提出了一种全面的基于样本的方法来评估生成模型的质量。所提出的方法能够估计两个样本集合来自同一分布的概率，为评估单个生成模型的性能或比较在同一数据集上训练的多个竞争模型提供了一个统计上严格的方法。该比较可以通过将空间划分为非重叠的区域并比较每个区域中的数据样本数量来进行。该方法仅需要生成模型和测试数据的样本。它能够直接处理高维数据，无需降维。显著的是，该方法不依赖于关于真实分布密度的假设，并且不依赖于训练或拟合任何辅助模型。相反，它着重于近似计算密度的积分（概率质量）。

    We propose a comprehensive sample-based method for assessing the quality of generative models. The proposed approach enables the estimation of the probability that two sets of samples are drawn from the same distribution, providing a statistically rigorous method for assessing the performance of a single generative model or the comparison of multiple competing models trained on the same dataset. This comparison can be conducted by dividing the space into non-overlapping regions and comparing the number of data samples in each region. The method only requires samples from the generative model and the test data. It is capable of functioning directly on high-dimensional data, obviating the need for dimensionality reduction. Significantly, the proposed method does not depend on assumptions regarding the density of the true distribution, and it does not rely on training or fitting any auxiliary models. Instead, it focuses on approximating the integral of the density (probability mass) acros
    
[^77]: 用于解决物联网识别问题的逻辑识别方法

    Logical recognition method for solving the problem of identification in the Internet of Things

    [https://arxiv.org/abs/2402.04338](https://arxiv.org/abs/2402.04338)

    这项工作的目标是开发一种逻辑识别方法，通过构建逻辑函数的最优扩展，在整个特征空间上实现逻辑连接，以解决物联网中的识别问题。

    

    最近，出现了一种应用逻辑代数和价值逻辑方法的新领域，即识别各种物体和现象、医学或技术诊断、构建现代机器、检查测试问题等问题，可以归结为在整个特征空间构建逻辑函数的最优扩展。例如，在逻辑识别系统中，使用基于离散分析和命题演算法的逻辑方法来构建自己的识别算法。在一般情况下，逻辑识别方法的使用需要存在由k值函数在整个特征空间上的最优延续所表示的逻辑连接，其中变量是正在识别的对象或现象的逻辑特征。本研究的目标是开发一种逻辑识别方法，该方法由具有逻辑特征和类别的参考表组成。

    A new area of application of methods of algebra of logic and to valued logic, which has emerged recently, is the problem of recognizing a variety of objects and phenomena, medical or technical diagnostics, constructing modern machines, checking test problems, etc., which can be reduced to constructing an optimal extension of the logical function to the entire feature space. For example, in logical recognition systems, logical methods based on discrete analysis and propositional calculus based on it are used to build their own recognition algorithms. In the general case, the use of a logical recognition method provides for the presence of logical connections expressed by the optimal continuation of a k-valued function over the entire feature space, in which the variables are the logical features of the objects or phenomena being recognized. The goal of this work is to develop a logical method for object recognition consisting of a reference table with logical features and classes of non
    
[^78]: LegalLens: 利用LLMs在非结构化文本中识别法律违规行为

    LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text

    [https://arxiv.org/abs/2402.04335](https://arxiv.org/abs/2402.04335)

    本研究利用LLMs构建数据集，通过微调模型和使用闭源LLMs进行少样本实验，成功实现了非结构化文本中法律违规行为的检测和与受害者的关联。结果显示我们的设置适用于这两个任务，F1分数分别为62.69％和81.02％。研究最终公开发布了数据集和代码，以推动法律自然语言处理领域的进一步研究。

    

    在这项研究中，我们专注于两个主要任务，第一个是检测非结构化文本数据中的法律违规行为，第二个是将这些违规行为与可能受影响的个人关联起来。我们使用大语言模型（LLMs）构建了两个数据集，并由领域专家注释进行验证。两个任务都是为集体诉讼案情境特别设计的。实验设计采用了来自BERT系列和开源LLMs的微调模型，并使用闭源LLMs进行了少样本实验。我们的结果表明，我们的数据集和设置可用于这两个任务，其违规行为识别的F1分数为62.69％，与受害者相关的分数为81.02％。最后，我们公开发布了用于实验的数据集和代码，以推动法律自然语言处理（NLP）领域的进一步研究。

    In this study, we focus on two main tasks, the first for detecting legal violations within unstructured textual data, and the second for associating these violations with potentially affected individuals. We constructed two datasets using Large Language Models (LLMs) which were subsequently validated by domain expert annotators. Both tasks were designed specifically for the context of class-action cases. The experimental design incorporated fine-tuning models from the BERT family and open-source LLMs, and conducting few-shot experiments using closed-source LLMs. Our results, with an F1-score of 62.69\% (violation identification) and 81.02\% (associating victims), show that our datasets and setups can be used for both tasks. Finally, we publicly release the datasets and the code used for the experiments in order to advance further research in the area of legal natural language processing (NLP).
    
[^79]: LESS：用于目标指导调整的选择有影响力的数据

    LESS: Selecting Influential Data for Targeted Instruction Tuning

    [https://arxiv.org/abs/2402.04333](https://arxiv.org/abs/2402.04333)

    LESS是一种优化感知且实际高效的算法，用于在大型语言模型中选择具有影响力的数据以开发特定能力，它采用低秩梯度相似性搜索方法进行指令数据选择。

    

    指令调整已经在大型语言模型中释放出强大的能力，有效地使用组合数据集来开发通用聊天机器人。然而，实际应用往往需要一套专门的技能（例如推理）。挑战在于从这些广泛的数据集中识别出最相关的数据，以有效开发特定的能力，我们将这种情况称为目标指导调整。我们提出了LESS，一种优化感知且实际高效的算法，以有效估计数据影响并执行适用于指令数据选择的低秩梯度相似性搜索。关键在于LESS将现有的影响公式调整为与Adam优化器和可变长度指令数据一起工作。LESS首先构建了一个具有低维梯度特征的高度可重用和可传递的梯度数据存储库，然后根据它们与具有特定能力的少样本示例的相似度选择示例。实验证明，t

    Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that t
    
[^80]: 通过向非关键神经元注入噪音增强DNN对抗性鲁棒性和效率

    Enhance DNN Adversarial Robustness and Efficiency via Injecting Noise to Non-Essential Neurons

    [https://arxiv.org/abs/2402.04325](https://arxiv.org/abs/2402.04325)

    本文提出了一种有效的方法，通过向非关键神经元注入噪音来增强DNN的对抗鲁棒性和执行效率。与以往的方法不同，我们通过在每个DNN层上策略性地引入噪音来干扰对抗攻击。实验结果表明，我们的方法成功地提高了对抗鲁棒性和执行效率。

    

    深度神经网络（DNN）通过在数据分析和决策方面提供无与伦比的能力，已经彻底改变了许多行业，从医疗和金融到汽车。尽管其具有的转型影响，DNN面临着两个关键挑战：对于对抗攻击的脆弱性和与更复杂和更大模型相关的计算成本的增加。本文介绍了一种有效的方法，旨在同时增强对抗鲁棒性和执行效率。与以往通过均匀注入噪音以增强鲁棒性的研究不同，我们引入了一种非均匀噪音注入算法， strategically applied at每一个DNN layer，以干扰攻击中引入的对抗扰动。通过采用近似技术，我们的方法识别并保护关键神经元，同时对非关键神经元策略性地引入噪音。我们的实验结果证明，我们的方法成功地增强了对抗鲁棒性和执行效率。

    Deep Neural Networks (DNNs) have revolutionized a wide range of industries, from healthcare and finance to automotive, by offering unparalleled capabilities in data analysis and decision-making. Despite their transforming impact, DNNs face two critical challenges: the vulnerability to adversarial attacks and the increasing computational costs associated with more complex and larger models. In this paper, we introduce an effective method designed to simultaneously enhance adversarial robustness and execution efficiency. Unlike prior studies that enhance robustness via uniformly injecting noise, we introduce a non-uniform noise injection algorithm, strategically applied at each DNN layer to disrupt adversarial perturbations introduced in attacks. By employing approximation techniques, our approach identifies and protects essential neurons while strategically introducing noise into non-essential neurons. Our experimental results demonstrate that our method successfully enhances both robus
    
[^81]: AdaFlow: 变异自适应流策略的模仿学习

    AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies

    [https://arxiv.org/abs/2402.04292](https://arxiv.org/abs/2402.04292)

    AdaFlow是一个基于流模型的模仿学习框架，通过使用变异自适应ODE求解器，在保持多样性的同时，提供快速推理能力。

    

    基于扩散的模仿学习在多模态决策中改进了行为克隆（BC），但由于扩散过程中的递归而导致推理速度显著减慢。这促使我们设计高效的策略生成器，同时保持生成多样化动作的能力。为了解决这个挑战，我们提出了AdaFlow，这是一个基于流模型的模仿学习框架。AdaFlow使用状态条件的常微分方程（ODE）表示策略，这被称为概率流。我们揭示了它们训练损失的条件方差与ODE的离散化误差之间的有趣关系。基于这个观察，我们提出了一个变异自适应ODE求解器，在推理阶段可以调整步长，使AdaFlow成为一个自适应决策者，能够快速推理而不牺牲多样性。有趣的是，当动作分布被降低到一步生成器时，它自动退化到一个一步生成器。

    Diffusion-based imitation learning improves Behavioral Cloning (BC) on multi-modal decision-making, but comes at the cost of significantly slower inference due to the recursion in the diffusion process. It urges us to design efficient policy generators while keeping the ability to generate diverse actions. To address this challenge, we propose AdaFlow, an imitation learning framework based on flow-based generative modeling. AdaFlow represents the policy with state-conditioned ordinary differential equations (ODEs), which are known as probability flows. We reveal an intriguing connection between the conditional variance of their training loss and the discretization error of the ODEs. With this insight, we propose a variance-adaptive ODE solver that can adjust its step size in the inference stage, making AdaFlow an adaptive decision-maker, offering rapid inference without sacrificing diversity. Interestingly, it automatically reduces to a one-step generator when the action distribution i
    
[^82]: BiLLM: 推动LLMs的后训练量化极限

    BiLLM: Pushing the Limit of Post-Training Quantization for LLMs

    [https://arxiv.org/abs/2402.04291](https://arxiv.org/abs/2402.04291)

    BiLLM是一种针对预训练LLMs的1位后训练量化方案，通过识别重要的权重和优化二值化，成功实现了高准确度的推理。

    

    预训练的大型语言模型（LLMs）具有出色的通用语言处理能力，但对内存和计算资源有很大的需求。作为一种强大的压缩技术，二值化可以将模型权重极大地减少到仅1位，降低了昂贵的计算和内存需求。然而，现有的量化技术在超低位宽下无法保持LLM的性能。针对这一挑战，我们提出了BiLLM，这是一种针对预训练LLM定制的开创性的1位后训练量化方案。基于LLMs的权重分布，BiLLM首先识别和结构选择重要的权重，并通过有效的二值化残差逼近策略来最小化压缩损失。此外，考虑到非重要权重的钟形分布，我们提出了一种最佳分割搜索方法，以准确地将它们分组和二值化。BiLLM首次实现了高准确度的推理。

    Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM achieving for the first time high-accuracy infere
    
[^83]: CasCast: 高分辨率降水即时预报的熟练级联建模

    CasCast: Skillful High-resolution Precipitation Nowcasting via Cascaded Modelling

    [https://arxiv.org/abs/2402.04290](https://arxiv.org/abs/2402.04290)

    CasCast是一个高分辨率降水即时预测框架，通过熟练的级联建模解决了复杂降水系统演化和极端降水预测两个关键挑战。

    

    基于雷达数据的降水即时预报在极端天气预测和灾害管理中起着关键作用，并具有广泛的影响。尽管基于深度学习已经取得了进展，但降水即时预报面临两个关键挑战尚未得到很好解决：（i）对具有不同尺度的复杂降水系统演变进行建模，以及（ii）对极端降水进行准确预测。在这项工作中，我们提出了CasCast，一个由确定性部分和概率性部分组成的级联框架，将中尺度降水分布和小尺度模式的预测解耦。然后，我们探索在高分辨率下训练级联框架，并在低维潜在空间中进行概率性建模，利用面向帧的引导扩散变换器增强极端事件的优化，并减少计算成本。在三个基准雷达降水数据集上的大量实验证明，CasCast取得了竞争性的效果。

    Precipitation nowcasting based on radar data plays a crucial role in extreme weather prediction and has broad implications for disaster management. Despite progresses have been made based on deep learning, two key challenges of precipitation nowcasting are not well-solved: (i) the modeling of complex precipitation system evolutions with different scales, and (ii) accurate forecasts for extreme precipitation. In this work, we propose CasCast, a cascaded framework composed of a deterministic and a probabilistic part to decouple the predictions for mesoscale precipitation distributions and small-scale patterns. Then, we explore training the cascaded framework at the high resolution and conducting the probabilistic modeling in a low dimensional latent space with a frame-wise-guided diffusion transformer for enhancing the optimization of extreme events while reducing computational costs. Extensive experiments on three benchmark radar precipitation datasets show that CasCast achieves competi
    
[^84]: 生物信息学中基础模型的进展与机遇

    Progress and Opportunities of Foundation Models in Bioinformatics

    [https://arxiv.org/abs/2402.04286](https://arxiv.org/abs/2402.04286)

    生物信息学中的基础模型（FMs）通过整合人工智能技术，解决了注释数据稀缺和数据噪声等挑战，在处理大规模无标签数据和有效表示多样化生物实体方面具有出色成果，在计算生物学领域开启了新时代。

    

    生物信息学在人工智能（AI）的日益整合下，特别是通过基础模型（FMs）的采用，经历了一个范式转变。这些AI技术迅速发展，解决了生物信息学中历史性的挑战，如注释数据的稀缺性和数据噪声的存在。FMs特别擅长处理大规模的无标签数据，在生物学背景下这是常见的情况，因为实验确定标记数据的过程费时费力。这一特性使FMs在各种下游验证任务中取得了突出的成果，展现了它们有效地表示多样化生物实体的能力。毫无疑问，FMs在计算生物学，尤其是深度学习领域，开启了一个新时代。本综述的主要目标是对生物信息学中的FMs进行系统调查和总结，追溯其演变和当前研究状态。

    Bioinformatics has witnessed a paradigm shift with the increasing integration of artificial intelligence (AI), particularly through the adoption of foundation models (FMs). These AI techniques have rapidly advanced, addressing historical challenges in bioinformatics such as the scarcity of annotated data and the presence of data noise. FMs are particularly adept at handling large-scale, unlabeled data, a common scenario in biological contexts due to the time-consuming and costly nature of experimentally determining labeled data. This characteristic has allowed FMs to excel and achieve notable results in various downstream validation tasks, demonstrating their ability to represent diverse biological entities effectively. Undoubtedly, FMs have ushered in a new era in computational biology, especially in the realm of deep learning. The primary goal of this survey is to conduct a systematic investigation and summary of FMs in bioinformatics, tracing their evolution, current research status
    
[^85]: 运动映射认知：人类视觉中的不可分解主要过程

    Motion Mapping Cognition: A Nondecomposable Primary Process in Human Vision

    [https://arxiv.org/abs/2402.04275](https://arxiv.org/abs/2402.04275)

    运动映射认知是人类视觉中不可分解的主要过程，它可以解释大多数视觉功能，并且无法通过传统的视觉处理方式建模。通过量化的拓扑匹配原则可以推导出一个有趣的计算模型，为开发更稳健和可解释的机器视觉方法提供了启示。

    

    人类智能似乎如此神秘，以至于我们直到现在都没有成功理解它的基础。在这里，我想介绍一种基本的认知过程，即运动映射认知（MMC），它应该是人类视觉中不可分解的主要功能。在其中，我指出，MMC过程可以用来解释大多数人类视觉功能的基本问题，但无法通过传统的视觉处理方式（包括图像分割、物体识别和物体跟踪等）有效建模。此外，我还提出MMC可以视为陈的拓扑知觉理论在人类视觉上的延伸，并且似乎无法通过现有的智能算法技巧来解决。最后，结合对MMC问题的需求，可以通过发展最优传输理论的思想来推导出一个有趣的计算模型，即量化的拓扑匹配原则。以上结果可能给我们开发更加稳健和可解释的机器视觉方法提供了巨大的启发。

    Human intelligence seems so mysterious that we have not successfully understood its foundation until now. Here, I want to present a basic cognitive process, motion mapping cognition (MMC), which should be a nondecomposable primary function in human vision. Wherein, I point out that, MMC process can be used to explain most of human visual functions in fundamental, but can not be effectively modelled by traditional visual processing ways including image segmentation, object recognition, object tracking etc. Furthermore, I state that MMC may be looked as an extension of Chen's theory of topological perception on human vision, and seems to be unsolvable using existing intelligent algorithm skills. Finally, along with the requirements of MMC problem, an interesting computational model, quantized topological matching principle can be derived by developing the idea of optimal transport theory. Above results may give us huge inspiration to develop more robust and interpretable machine vision m
    
[^86]: ProtAgents: 通过物理学和机器学习相结合的大型语言模型多智能体协作进行蛋白质发现

    ProtAgents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning

    [https://arxiv.org/abs/2402.04268](https://arxiv.org/abs/2402.04268)

    ProtAgents是一个基于大型语言模型的平台，通过多智能体的协作，在动态环境中解决复杂的新型蛋白质设计任务，并结合物理学和机器学习的方法。

    

    设计超越自然界中已有的蛋白质对于科学和工程应用的进展具有重要的潜力。目前蛋白质设计的方法通常依赖于基于人工智能的模型，如将蛋白质结构与材料性质相互关联的替代模型来解决端到端问题。然而，这些模型经常集中于特定的材料目标或结构特性，当需要将领域外的知识纳入到设计过程或进行综合数据分析时，它们的灵活性受到限制。在本研究中，我们介绍了一种基于大型语言模型（LLMs）的ProtAgents平台，用于新型蛋白质设计，其中多个具有不同能力的AI智能体在动态环境下协作解决复杂任务。通过智能体的多样发展，可以在不同领域中拥有专长，包括知识检索、蛋白质结构分析、基于物理学的模拟和结果分析。

    Designing de novo proteins beyond those found in nature holds significant promise for advancements in both scientific and engineering applications. Current methodologies for protein design often rely on AI-based models, such as surrogate models that address end-to-end problems by linking protein structure to material properties or vice versa. However, these models frequently focus on specific material objectives or structural properties, limiting their flexibility when incorporating out-of-domain knowledge into the design process or comprehensive data analysis is required. In this study, we introduce ProtAgents, a platform for de novo protein design based on Large Language Models (LLMs), where multiple AI agents with distinct capabilities collaboratively address complex tasks within a dynamic environment. The versatility in agent development allows for expertise in diverse domains, including knowledge retrieval, protein structure analysis, physics-based simulations, and results analysi
    
[^87]: 人工智能技术结合螺旋CT扫描在早期肺癌筛查中的应用分析

    Application analysis of ai technology combined with spiral CT scanning in early lung cancer screening

    [https://arxiv.org/abs/2402.04267](https://arxiv.org/abs/2402.04267)

    本研究系统分析了人工智能技术结合螺旋CT扫描在早期肺癌筛查中的应用，并证实早期诊断对于改善患者预后具有重要意义。

    

    目前，中国肺癌的发病率和死亡率在所有恶性肿瘤中名列前茅。尽管中国医疗水平不断发展和提高，但肺癌患者的总体5年生存率仍然低于20％，并且分期进行。大量研究证实，早期诊断和治疗早期肺癌对于改善患者预后具有重要意义。近年来，人工智能技术逐渐开始应用于肿瘤学中。ai在肿瘤筛查，临床诊断，放疗（图像获取，风险器官分割，图像校准和传递）等方面迅速发展。然而，医疗ai能否实现社会化，在一定程度上取决于公众的态度和接受程度。然而，目前关于结合AI技术和SCT扫描进行早期肺癌诊断的研究很少。鉴于此，本研究利用人工智能技术与螺旋CT扫描联合进行肺癌早期诊断的方法进行了系统分析和应用研究。

    At present, the incidence and fatality rate of lung cancer in China rank first among all malignant tumors. Despite the continuous development and improvement of China's medical level, the overall 5-year survival rate of lung cancer patients is still lower than 20% and is staged. A number of studies have confirmed that early diagnosis and treatment of early stage lung cancer is of great significance to improve the prognosis of patients. In recent years, artificial intelligence technology has gradually begun to be applied in oncology. ai is used in cancer screening, clinical diagnosis, radiation therapy (image acquisition, at-risk organ segmentation, image calibration and delivery) and other aspects of rapid development. However, whether medical ai can be socialized depends on the public's attitude and acceptance to a certain extent. However, at present, there are few studies on the diagnosis of early lung cancer by AI technology combined with SCT scanning. In view of this, this study ap
    
[^88]: 优先安全保障而非自治：科学中LLM智能机器人的风险

    Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science

    [https://arxiv.org/abs/2402.04247](https://arxiv.org/abs/2402.04247)

    本文探讨了科学领域中基于LLM的智能机器人的漏洞与风险，并强调了对安全措施的重要性。

    

    由大型语言模型（LLMs）驱动的智能机器人在各个学科中自主进行实验和促进科学发现方面展示了巨大的前景。尽管它们的能力非常有前途，但也引入了一些新的漏洞，需要仔细考虑安全性。然而，文献中存在显著的空白，尚未对这些漏洞进行全面探讨。本文通过对科学领域中基于LLM的机器人的漏洞进行深入研究，揭示了它们误用可能带来的潜在风险，并强调了对安全措施的需求，填补了这一空白。我们首先全面概述了科学LLM机器人固有的潜在风险，考虑了用户意图、特定的科学领域以及它们对外部环境可能造成的影响。然后，我们深入探讨了这些漏洞的起源和提供的解决方案。

    Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
    
[^89]: 生成型智能体能够预测情感吗？

    Can Generative Agents Predict Emotion?

    [https://arxiv.org/abs/2402.04232](https://arxiv.org/abs/2402.04232)

    本研究探讨了生成型智能体在感知新事件时情绪状态的演变，通过比较新经验和过去记忆来获得理解新经验的能力，并通过情感测试捕捉智能体的情绪状态。

    

    大型语言模型（LLMs）展示了许多类似人类的能力，但是LLMs的共情理解和情绪状态尚未与人类对齐。在本研究中，我们探讨了生成型LLM智能体在感知新事件时情绪状态的演变，引入了一种新的架构，通过比较新经验和过去记忆来获得理解新经验的能力。通过这种比较，智能体能够在情境中理解新经验，根据情绪评估理论，这对于情绪生成至关重要。首先，智能体将新经验感知为时间序列文本数据。在感知每个新输入后，智能体生成过去相关记忆的摘要，被称为“规范”，并将新经验与此规范进行比较。通过这种比较，我们可以分析智能体如何在情境中对新经验做出反应。使用情感测试PANAS对智能体进行测试，捕捉智能体在新经验后的情绪状态。

    Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the 
    
[^90]: 推进法律推理：将人工智能应用于处理全球法理中的复杂性和偏见的半自动化仲裁流程（SAAPs）

    Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs)

    [https://arxiv.org/abs/2402.04140](https://arxiv.org/abs/2402.04140)

    本研究通过整合先进的语言模型和人工智能技术，开发了名为SHIRLEY的应用程序，旨在识别法律判决中的偏见和逻辑不一致，并促进自动化、有效和一致的多方论证，以保证法律在不同司法管辖区内的一致应用。

    

    本研究采用了一种新颖的方法，对包括美国、英国、卢旺达、瑞典和香港在内的五个国家的法院判决进行了分析。本研究还探讨了人工智能（特别是生成式人工智能）和法律分析的最新进展交叉领域，强调了人工智能在识别人类偏见和促进法院判决的多方论证的自动化、有效和一致的角色，从而确保法律在各个司法管辖区内和跨司法管辖区的一致应用。通过结合先进的语言模型（ALMs）和新引入的人工智能与人类合作框架，本文旨在分析以ALMs为基础的基于Grounded Theory的法学实践研究设计。SHIRLEY是基于OpenAI的GPT技术构建的基于人工智能的应用程序，主要用于检测各种法律决定中的逻辑不一致和偏见。

    This study consists of a novel approach toward the analysis of court judgments spanning five countries, including the United States, the United Kingdom, Rwanda, Sweden and Hong Kong. This study also explores the intersection of the latest advancements in artificial intelligence (AI) and legal analysis, emphasizing the role of AI (specifically generative AI) in identifying human biases and facilitating automated, valid, and coherent multisided argumentation of court judgments with the goal of ensuring consistent application of laws in and across various jurisdictions. By incorporating Advanced Language Models (ALMs) and a newly introduced human-AI collaborative framework, this paper seeks to analyze Grounded Theory-based research design with Advanced Language Models (ALMs) in the practice of law. SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT technology), focusing on detecting logical inconsistencies and biases across various legal decisions. SHIRLEY analy
    
[^91]: 位置论文：反对虚假的AI膨胀性声明

    Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims

    [https://arxiv.org/abs/2402.03962](https://arxiv.org/abs/2402.03962)

    这篇论文讨论了在寻求人工通用智能的过程中，人们对于大型语言模型的类人特质过度归因的现象，并呼吁学术界在解读和传播关于AI研究的内容时要保持谨慎。

    

    人类有一种倾向，看到周围的物体具有类似"人类"的特质。我们给汽车取名字，和宠物甚至家用电器交谈，仿佛它们能像其他人类一样理解我们。这种行为被称为拟人化，在机器学习（ML）领域也越来越受到关注，人们声称在大型语言模型（LLM）中能够察觉到类似于人类智能的特质。在这篇位置论文中，考虑到专业激励、人类偏见和一般方法论设置，我们讨论如何当前对于人工通用智能（AGI）的追求为将类人特质归因于LLM打开了滥觞之门。通过几个实验，我们证明在潜在空间中发现可解释为人类的模式并不应该是令人惊讶的结果。考虑到媒体对人工智能的普遍描写，我们呼吁学术界特别谨慎，并对学术诚信原则有额外的意识，在解读和传播关于AI研究的信息时要保持谨慎。

    Humans have a tendency to see 'human'-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs). In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs. In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI rese
    
[^92]: AirPhyNet: 利用物理引导的神经网络预测空气质量

    AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction

    [https://arxiv.org/abs/2402.03784](https://arxiv.org/abs/2402.03784)

    AirPhyNet是一种利用物理引导的神经网络方法，通过将空气颗粒运动的物理原理和神经网络结构相结合，提高了对空气质量的预测精度和可解释性。

    

    空气质量预测和建模在公共卫生和环境管理中起着关键作用，帮助个人和当局做出明智决策。尽管传统的数据驱动模型在这个领域已经显示出了潜力，但它们在长期预测精度上可能受到限制，特别是在数据稀疏或不完整的情况下，它们往往依赖于缺乏坚实物理基础的黑盒深度学习结构，导致预测的透明度和可解释性降低。为了解决这些限制，本文提出了一种名为Physics guided Neural Network for Air Quality Prediction（AirPhyNet）的新方法。具体而言，我们利用空气颗粒运动的两个成熟的物理原理（扩散和平流）将其表示为微分方程网络。然后，我们利用图结构将物理知识融入神经网络架构，并利用潜在表示来捕捉时空关系。

    Air quality prediction and modelling plays a pivotal role in public health and environment management, for individuals and authorities to make informed decisions. Although traditional data-driven models have shown promise in this domain, their long-term prediction accuracy can be limited, especially in scenarios with sparse or incomplete data and they often rely on black-box deep learning structures that lack solid physical foundation leading to reduced transparency and interpretability in predictions. To address these limitations, this paper presents a novel approach named Physics guided Neural Network for Air Quality Prediction (AirPhyNet). Specifically, we leverage two well-established physics principles of air particle movement (diffusion and advection) by representing them as differential equation networks. Then, we utilize a graph structure to integrate physics knowledge into a neural network architecture and exploit latent representations to capture spatio-temporal relationships
    
[^93]: 揭示宣传：基于人类注释和机器分类的文体线索比较分析

    Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification

    [https://arxiv.org/abs/2402.03780](https://arxiv.org/abs/2402.03780)

    本文通过分析文体线索比较人类注释和机器分类的方法，揭示了宣传语言的特征，并提出了一个多源、多语言、多模态的数据集PPN。结果表明，人类注释者能够可靠地区分宣传新闻和常规新闻。研究还比较了不同的自然语言处理技术，并提供了一些有关文体线索的发现。

    

    本文研究了宣传语言及其文体特征。提出了PPN数据集，即宣传性伪新闻数据集，它是一种多源、多语言、多模态的数据集，由专家机构确定的宣传来源网站上的新闻文章组成。从该数据集中随机选择了一部分样本与来自常规法国新闻的文章混合，并对它们的URL进行了掩盖，以进行人类注释实验，使用11个不同的标签。结果显示，人类注释者能够可靠地区分两种类型的新闻纸对每个标签。我们提出了不同的自然语言处理技术来识别注释者使用的线索，并与机器分类进行比较。其中包括使用VAGO分析器进行辞述模糊和主观性的测量，使用TF-IDF作为基准，以及四种不同的分类器：两个基于RoBERTa模型的模型，使用句法的CATS，以及结合句法和语义特征的一个XGBoost模型。

    This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
    
[^94]: MQuinE:知识图谱嵌入模型中“Z-悖论”的解决方案

    MQuinE: a cure for "Z-paradox'' in knowledge graph embedding models

    [https://arxiv.org/abs/2402.03583](https://arxiv.org/abs/2402.03583)

    研究者发现知识图谱嵌入模型存在的“Z-悖论”限制了其表达能力，并提出了一种名为MQuinE的新模型，通过理论证明，MQuinE成功解决了Z-悖论，并在链接预测任务中显著优于现有模型。

    

    知识图谱嵌入（KGE）模型在许多知识图谱任务，包括链接预测和信息检索方面取得了最先进的结果。尽管KGE模型在实践中表现出优越性能，但我们发现一些流行的现有KGE模型存在表达不足的问题，称为“Z-悖论”。受到Z-悖论的存在的启发，我们提出了一种新的KGE模型，称为MQuinE，在不受Z-悖论的困扰的同时，保持强大的表达能力来模拟各种关系模式，包括对称/非对称，逆向，1-N/N-1/N-N和组合关系，并提供了理论上的证明。对实际知识库的实验表明，Z-悖论确实降低了现有KGE模型的性能，并且可能导致某些具有挑战性的测试样本的准确性下降超过20％。我们的实验进一步证明了MQuinE可以减轻Z-悖论的负面影响，并在链接预测方面以明显优势超越现有的KGE模型。

    Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called \emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE model called \emph{MQuinE} that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20\% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction
    
[^95]: VLN-Video: 利用行车视频进行室外视觉与语言导航

    VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation

    [https://arxiv.org/abs/2402.03561](https://arxiv.org/abs/2402.03561)

    VLN-Video利用行车视频的多样室外环境和自动生成的导航指令与动作，通过深度学习方法提高了室外视觉与语言导航的性能。

    

    室外视觉与语言导航（VLN）要求代理根据自然语言指令在逼真的三维室外环境中导航。现有的VLN方法在导航环境多样性和训练数据有限性方面存在限制。为解决这些问题，我们提出了VLN-Video，该方法利用在美国多个城市的行车视频中存在的多样化室外环境，并通过自动生成导航指令和动作来提高室外VLN性能。VLN-Video结合了直观经典方法和现代深度学习技术的优势，利用模板填充生成有实际基础的导航指令，并结合基于图像旋转相似度的导航动作预测器从行车视频中获取VLN风格的数据，用于预训练深度学习VLN模型。我们在Touchdown数据集和由行车视频创建的视频增强数据集上对模型进行预训练。

    Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with th
    
[^96]: 图缩减的综合调研：稀疏化、粗化和浓缩

    A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation

    [https://arxiv.org/abs/2402.03358](https://arxiv.org/abs/2402.03358)

    这篇综述调研了图缩减方法，包括稀疏化、粗化和浓缩，在解决大型图形数据分析和计算复杂性方面起到了重要作用。调研对这些方法的技术细节进行了系统的回顾，并强调了它们在实际应用中的关键性。同时，调研还提出了保证图缩减技术持续有效性的关键研究方向。

    

    许多真实世界的数据集可以自然地表示为图，涵盖了广泛的领域。然而，图数据集的复杂性和规模的增加为分析和计算带来了显著的挑战。为此，图缩减技术在保留关键属性的同时简化大型图形数据变得越来越受关注。在本调研中，我们旨在提供对图缩减方法的全面理解，包括图稀疏化、图粗化和图浓缩。具体而言，我们建立了这些方法的统一定义，并引入了一个分层分类法来分类这些方法所解决的挑战。我们的调研系统地回顾了这些方法的技术细节，并强调了它们在各种场景中的实际应用。此外，我们还概述了保证图缩减技术持续有效性的关键研究方向，并提供了一个详细的论文列表链接。

    Many real-world datasets can be naturally represented as graphs, spanning a wide range of domains. However, the increasing complexity and size of graph datasets present significant challenges for analysis and computation. In response, graph reduction techniques have gained prominence for simplifying large graphs while preserving essential properties. In this survey, we aim to provide a comprehensive understanding of graph reduction methods, including graph sparsification, graph coarsening, and graph condensation. Specifically, we establish a unified definition for these methods and introduce a hierarchical taxonomy to categorize the challenges they address. Our survey then systematically reviews the technical details of these methods and emphasizes their practical applications across diverse scenarios. Furthermore, we outline critical research directions to ensure the continued effectiveness of graph reduction techniques, as well as provide a comprehensive paper list at https://github.
    
[^97]: 有机或扩散：我们能区分人类艺术和AI生成的图像吗？

    Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?

    [https://arxiv.org/abs/2402.03214](https://arxiv.org/abs/2402.03214)

    这项研究探讨了如何区分人类艺术和AI生成的图像，并提供了几种不同的方法，包括通过监督学习训练的分类器、扩散模型的研究工具以及专业艺术家的知识。这对防止欺诈、遵守政策以及避免模型崩溃都具有重要意义。

    

    生成AI图像的出现完全颠覆了艺术界。从人类艺术中识别AI生成的图像是一个具有挑战性的问题，其影响随着时间的推移而不断增加。未能解决这个问题会导致不良行为者欺诈那些支付高价购买人类艺术品的个人和禁止使用AI图像的公司。这对于需要过滤训练数据以避免潜在模型崩溃的AI模型训练者来说也至关重要。区分人类艺术和AI图像的方法有多种，包括通过监督学习训练的分类器，针对扩散模型的研究工具，以及通过专业艺术家利用他们对艺术技巧的知识进行识别。在本文中，我们试图了解这些方法在现代生成模型的良性和对抗性环境中的表现如何。我们策划了7种风格的真实人类艺术，从5个生成模型生成了与之匹配的图像，并应用了8个检测器。

    The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors 
    
[^98]: DS-MS-TCN: 使用双尺度多阶段时间卷积网络的Otago体操识别

    DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network

    [https://arxiv.org/abs/2402.02910](https://arxiv.org/abs/2402.02910)

    本研究提出了一种使用双尺度多阶段时间卷积网络的Otago体操识别方法，通过单个腰部佩戴的IMU在老年人的日常生活中实现准确且稳定的识别，为康复举措提供支持。

    

    Otago运动计划是针对老年人的重要康复举措，旨在增强平衡和力量。本研究利用单个腰部佩戴的惯性测量单元(IMU)，在老年人的日常生活中识别Otago体操动作，以解决现有研究在准确性和稳定性方面的限制。研究在实验室设置中招募了36名老年人，并对额外招募的7名老年人进行了家庭评估。研究提出了一种双尺度多阶段时间卷积网络(DS-MS-TCN)，用于两级序列到序列分类，将其纳入一个损失函数。在第一阶段，模型专注于识别每个体操动作的重复次数(微标签)。随后的阶段扩展了识别范围，包括完整的运动序列。

    The Otago Exercise Program (OEP) represents a crucial rehabilitation initiative tailored for older adults, aimed at enhancing balance and strength. Despite previous efforts utilizing wearable sensors for OEP recognition, existing studies have exhibited limitations in terms of accuracy and robustness. This study addresses these limitations by employing a single waist-mounted Inertial Measurement Unit (IMU) to recognize OEP exercises among community-dwelling older adults in their daily lives. A cohort of 36 older adults participated in laboratory settings, supplemented by an additional 7 older adults recruited for at-home assessments. The study proposes a Dual-Scale Multi-Stage Temporal Convolutional Network (DS-MS-TCN) designed for two-level sequence-to-sequence classification, incorporating them in one loss function. In the first stage, the model focuses on recognizing each repetition of the exercises (micro labels). Subsequent stages extend the recognition to encompass the complete ra
    
[^99]: LHRS-Bot：利用VGI增强的大型多模态语言模型赋能遥感领域

    LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model

    [https://arxiv.org/abs/2402.02544](https://arxiv.org/abs/2402.02544)

    LHRS-Bot 是一个利用自愿地理信息(VGI)增强的大型多模态语言模型，旨在解决近期MLLM在遥感领域中未对多样的地理景观和物体进行充分考虑的问题。通过引入多层次视觉-语言对齐策略和课程学习方法，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。

    

    大型语言模型（LLMs）的革命性能力开创了多模态大型语言模型（MLLMs）并促进了在各个专业领域的多样化应用。然而，在遥感（RS）领域中，近期的MLLM努力未能充分考虑到遥感图像中多样的地理景观和物体。为了弥补这一差距，我们构建了一个大规模的RS图像-文本数据集LHRS-Align，以及一个信息丰富的RS特定指导数据集LHRS-Instruct，利用丰富的自愿地理信息（VGI）和全球可用的RS图像。在此基础上，我们引入了LHRS-Bot，一种针对RS图像理解的MLLM，通过一种新颖的多层次视觉-语言对齐策略和课程学习方法。全面的实验证明，LHRS-Bot展现出对RS图像的深刻理解以及在RS领域内进行细致推理的能力。

    The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.
    
[^100]: XAI-CF -- 探讨可解释人工智能在网络取证中的作用

    XAI-CF -- Examining the Role of Explainable Artificial Intelligence in Cyber Forensics

    [https://arxiv.org/abs/2402.02452](https://arxiv.org/abs/2402.02452)

    XAI-CF是一个可解释的人工智能系统，应用于网络取证领域，以解决复杂网络设备和大量数据带来的挑战，并确保与利益相关者的沟通和法律标准的一致性。

    

    随着复杂网络设备的兴起，网络取证（CF）面临着许多新的挑战。例如，智能手机上运行着数十个系统，每个系统拥有上百万个可下载的应用程序。在这么庞大的数据中进行筛查并理解需要新的技术，例如来自人工智能领域的技术。要成功将这些技术应用于网络取证中，我们需要向网络取证的利益相关者（如取证分析师和法院成员）证明和解释结果，以便他们做出明智的决策。如果我们想成功地将人工智能应用于网络取证中，就需要对人工智能系统建立信任。接受在网络取证中使用人工智能的一些因素是使人工智能真实、可解释、可理解和交互。这样，人工智能系统将更容易被公众接受，并确保与法律标准保持一致。一个可解释的人工智能（XAI）系统可以在网络取证中担当这个角色，我们将这样的系统称为XAI-CF。

    With the rise of complex cyber devices Cyber Forensics (CF) is facing many new challenges. For example, there are dozens of systems running on smartphones, each with more than millions of downloadable applications. Sifting through this large amount of data and making sense requires new techniques, such as from the field of Artificial Intelligence (AI). To apply these techniques successfully in CF, we need to justify and explain the results to the stakeholders of CF, such as forensic analysts and members of the court, for them to make an informed decision. If we want to apply AI successfully in CF, there is a need to develop trust in AI systems. Some other factors in accepting the use of AI in CF are to make AI authentic, interpretable, understandable, and interactive. This way, AI systems will be more acceptable to the public and ensure alignment with legal standards. An explainable AI (XAI) system can play this role in CF, and we call such a system XAI-CF. XAI-CF is indispensable and 
    
[^101]: TopoX: 一个用于拓扑域上的机器学习的Python软件包套件

    TopoX: A Suite of Python Packages for Machine Learning on Topological Domains

    [https://arxiv.org/abs/2402.02441](https://arxiv.org/abs/2402.02441)

    TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。

    

    我们介绍了topox，一个提供可靠且用户友好的Python软件包套件，用于在拓扑域（扩展了图的领域）上进行计算和机器学习：超图、单纯、胞腔、路径和组合复合体。topox由三个软件包组成：toponetx用于构建和计算这些域，包括节点、边和高阶单元的处理；topoembedx提供了将拓扑域嵌入到向量空间的方法，类似于流行的基于图的嵌入算法，如node2vec；topomodelx建立在PyTorch之上，为拓扑域上的神经网络提供了一套全面的高阶消息传递功能工具箱。topox的源代码经过广泛的文档化和单元测试，并在https://github.com/pyt-team以MIT许可证的形式提供。

    We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
    
[^102]: 大型多模型(LMMs)作为AI原生无线系统的通用基础模型

    Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems

    [https://arxiv.org/abs/2402.01748](https://arxiv.org/abs/2402.01748)

    本文提出了大型多模型(LMMs)作为AI原生无线系统的通用基础模型的设计框架，通过处理多模态感知数据、通过因果推理和检索增强生成(RAG)将物理符号表示与无线系统联系起来，并通过无线环境反馈实现可教导性，从而促进动态网络配置。

    

    最近，大型语言模型(LLMs)和基础模型被宣称为6G系统的改变者。然而，目前关于无线网络的LLMs的努力仅限于直接应用现有的为自然语言处理(NLP)应用设计的语言模型。为了解决这一挑战，并创建以无线为中心的基础模型，本文提出了一个全面的视野，介绍了如何设计针对部署人工智能(AI)原生网络的通用基础模型。与基于NLP的基础模型不同，所提出的框架通过三个关键能力促进了大型多模型(LMMs)的设计：1) 处理多模态感知数据，2) 通过因果推理和检索增强生成(RAG)将物理符号表示与现实世界的无线系统联系起来，3) 通过无线环境反馈实现可教导性，以促进动态网络配置。

    Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language mod- els that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network a
    
[^103]: 从大型语言模型中提取上下文信息用于知识图谱补全

    Contextualization Distillation from Large Language Model for Knowledge Graph Completion

    [https://arxiv.org/abs/2402.01729](https://arxiv.org/abs/2402.01729)

    本论文介绍了一种从大型语言模型中提取上下文信息用于知识图谱补全的策略，该策略能克服现有语料库的限制，显著提高了预训练语言模型在知识图谱补全中的性能。

    

    虽然文本信息显著提高了预训练语言模型（PLMs）在知识图谱补全（KGC）中的性能，但现有语料库从维基百科文章或同义词定义中收集的静态和噪声性质常常限制了基于PLM的KGC模型的潜力。为了克服这些挑战，我们提出了上下文化蒸馏策略，这是一种通用的可插入和可播放的方法，与判别和生成的KGC框架兼容。我们的方法首先指导大型语言模型（LLMs）将紧凑的结构化三元组转换为上下文丰富的段落。随后，我们引入了两个定制的辅助任务，重建和上下文化，使较小的KGC模型能够吸收这些丰富的三元组中的见解。对多种数据集和KGC技术的全面评估突出了我们方法的功效和适应性，揭示了无论基础管道如何，始终能提高性能。

    While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pip
    
[^104]: AI中介交流的指导：AI不改变对文本消息的感知

    Guidance for AI-Mediated Communication: AI Does Not Alter Perceptions of Text Messages

    [https://arxiv.org/abs/2402.01726](https://arxiv.org/abs/2402.01726)

    这项研究通过调查参与者对18条随机标记的文本消息的感知，探讨了大型语言模型在文本消息撰写中的辅助使用可能对感知产生的影响。

    

    对于许多人来说，焦虑、抑郁和其他社交和心理因素可能使撰写文本消息成为一项积极的挑战。为解决这个问题，大型语言模型（LLM）可能是帮助那些本来会觉得发送短信困难或有压力的用户的完美工具。然而，尽管大型语言模型的使用快速普及，但对其在文本消息撰写中的辅助使用的考虑还未被探索。关于大型语言模型使用的一个主要关注点是，AI的公众情绪较差可能导致其辅助的文本消息的使用对感知产生负面影响，从而使使用适得其反。为了验证这种可能性，我们探讨了人们是否认为一条文本消息是否在撰写过程中得到了AI的辅助，会改变其感知的语调、清晰度和表达意图的能力。在这项研究中，我们调查了26名参与者对18条随机标记的预先撰写的文本消息的感知。通过分析参与者对消息语调的评分，我们发现。

    For many people, anxiety, depression, and other social and mental factors can make composing text messages an active challenge. To remedy this problem, large language models (LLMs) may yet prove to be the perfect tool to assist users that would otherwise find texting difficult or stressful. However, despite rapid uptake in LLM usage, considerations for their assistive usage in text message composition have not been explored. A primary concern regarding LLM usage is that poor public sentiment regarding AI introduces the possibility that its usage may harm perceptions of AI-assisted text messages, making usage counter-productive. To (in)validate this possibility, we explore how the belief that a text message did or did not receive AI assistance in composition alters its perceived tone, clarity, and ability to convey intent. In this study, we survey the perceptions of 26 participants on 18 randomly labeled pre-composed text messages. In analyzing the participants' ratings of message tone,
    
[^105]: 基于大语言模型的模糊测试技术：一项综述

    Large Language Models Based Fuzzing Techniques: A Survey

    [https://arxiv.org/abs/2402.00350](https://arxiv.org/abs/2402.00350)

    这篇论文对基于大语言模型的模糊测试技术进行了综述，提供了对LLMs、模糊测试和基于LLMs的模糊测试方法的系统概述，并讨论了相关的挑战和未来的研究方向。

    

    在现代软件发挥关键作用的时代，软件安全和漏洞分析对于软件开发变得至关重要。作为一种高效的软件测试方法，模糊测试被广泛应用于各个领域。此外，大语言模型（LLMs）的快速发展使它们可以在软件测试领域中应用，并展现出卓越的性能。考虑到现有的模糊测试技术并不完全自动化，软件漏洞不断演化，越来越多的趋势是采用基于大语言模型生成的模糊测试。本综述提供了关于融合LLMs和模糊测试的软件测试方法的系统概述。通过总结2024年之前的最新方法，对LLMs、模糊测试和基于LLMs的模糊测试进行统计分析和讨论。我们的综述还研究了LLMs和模糊测试在软件测试领域的应用，并讨论了相关的挑战和未来的研究方向。

    In the modern era where software plays a pivotal role, software security and vulnerability analysis have become essential for software development. Fuzzing test, as an efficient software testing method, are widely used in various domains. Moreover, the rapid development of Large Language Models (LLMs) has facilitated their application in the field of software testing, demonstrating remarkable performance. Considering that existing fuzzing test techniques are not entirely automated and software vulnerabilities continue to evolve, there is a growing trend towards employing fuzzing test generated based on large language models. This survey provides a systematic overview of the approaches that fuse LLMs and fuzzing tests for software testing. In this paper, a statistical analysis and discussion of the literature in three areas, namely LLMs, fuzzing test, and fuzzing test generated based on LLMs, are conducted by summarising the state-of-the-art methods up until 2024. Our survey also invest
    
[^106]: 检测大型人工智能模型生成的多媒体内容：一项调查研究

    Detecting Multimedia Generated by Large AI Models: A Survey

    [https://arxiv.org/abs/2402.00045](https://arxiv.org/abs/2402.00045)

    本文综述了大型人工智能模型（LAIMs）生成的多媒体内容的检测方法和研究进展，旨在填补现有研究中的空白。我们提供了一种新颖的分类法，并介绍了纯检测和应用场景两个角度来增强检测性能。

    

    大型人工智能模型（LAIMs）的快速发展，尤其是扩散模型和大型语言模型，标志着一种新的时代，人工智能生成的多媒体内容被越来越多地整合到日常生活的各个方面。尽管在许多领域有益，但这些内容也带来了重大风险，包括潜在的滥用、社会破坏和伦理问题。因此，检测由LAIMs生成的多媒体内容变得至关重要，相关研究也大幅增加。尽管如此，目前仍然存在一个明显的问题，即缺乏系统性的调查研究，专门关注检测LAIMs生成的多媒体内容。为了解决这个问题，我们提供了第一份全面涵盖现有研究的调查报告，重点关注检测LAIMs生成的多媒体内容（如文本、图像、视频、音频和多模态内容）。具体而言，我们引入了一种新颖的检测方法分类法，按媒体形式分类，并与纯检测（旨在提高检测性能）和应用场景对齐。

    The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) an
    
[^107]: 寻求更好的适应性？适应个体驾驶员的增量学习多模式目标引用框架

    Looking for a better fit? An Incremental Learning Multimodal Object Referencing Framework adapting to Individual Drivers

    [https://arxiv.org/abs/2401.16123](https://arxiv.org/abs/2401.16123)

    这项研究提出了一种增量学习的多模式目标引用框架，可以适应个体驾驶员的变化行为和各种驾驶场景。

    

    汽车行业迅速向自动化和半自动化车辆发展，传统的车辆交互方法（如基于触摸和语音命令的系统）在越来越广泛的非驾驶相关任务（如引用车辆外部物体）中已经变得不合适。因此，研究转向姿势输入（如手势、视线和头部姿势手势）作为驾驶过程中更合适的交互方式。然而，由于驾驶的动态特性和个体差异，驾驶员的姿势输入性能存在显著差异。虽然理论上，这种固有的可变性可以通过大规模数据驱动的机器学习模型进行调节，但普遍的方法倾向于针对目标引用使用约束的单实例训练模型。这些模型在持续适应个体驾驶员的不同行为和各种驾驶场景方面显示出有限的能力。为了解决这个问题，我们提出了一种增量学习的多模式目标引用框架，可以适应个体驾驶员的变化行为和各种驾驶场景。

    The rapid advancement of the automotive industry towards automated and semi-automated vehicles has rendered traditional methods of vehicle interaction, such as touch-based and voice command systems, inadequate for a widening range of non-driving related tasks, such as referencing objects outside of the vehicle. Consequently, research has shifted toward gestural input (e.g., hand, gaze, and head pose gestures) as a more suitable mode of interaction during driving. However, due to the dynamic nature of driving and individual variation, there are significant differences in drivers' gestural input performance. While, in theory, this inherent variability could be moderated by substantial data-driven machine learning models, prevalent methodologies lean towards constrained, single-instance trained models for object referencing. These models show a limited capacity to continuously adapt to the divergent behaviors of individual drivers and the variety of driving scenarios. To address this, we 
    
[^108]: 通过术前到术中图像融合对腹腔镜肝切除中增强现实方法的客观比较

    An objective comparison of methods for augmented reality in laparoscopic liver resection by preoperative-to-intraoperative image fusion

    [https://arxiv.org/abs/2401.15753](https://arxiv.org/abs/2401.15753)

    本文比较了腹腔镜肝切除中增强现实方法的客观优劣，提出了术前到术中图像融合挑战，旨在自动化该过程，有效实现增强现实在手术室的应用。

    

    腹腔镜肝切除中的增强现实是一种可视化模式，它允许外科医生在腹腔镜图像上投射肿瘤和嵌入在肝脏内部的血管，以帮助定位。在这个过程中，术前从CT或MRI数据提取的3D模型被注册到术中的腹腔镜图像中。从3D-2D融合的角度来看，大多数算法利用解剖标志物来指导注册，这些标志物包括肝脏的下线、锻镰韧带和闭锁轮廓。在腹腔镜图像和3D模型中手工标记这些标志物通常是耗时且可能存在错误，特别是对于非经验丰富的用户。因此，有必要自动化这个过程，以使增强现实在手术室中能够有效使用。我们在医学影像和计算机辅助干预（MICCAI 2022）的"术前到术中腹腔镜融合挑战"（P2ILF）中提出了这一挑战。

    Augmented reality for laparoscopic liver resection is a visualisation mode that allows a surgeon to localise tumours and vessels embedded within the liver by projecting them on top of a laparoscopic image. Preoperative 3D models extracted from CT or MRI data are registered to the intraoperative laparoscopic images during this process. In terms of 3D-2D fusion, most of the algorithms make use of anatomical landmarks to guide registration. These landmarks include the liver's inferior ridge, the falciform ligament, and the occluding contours. They are usually marked by hand in both the laparoscopic image and the 3D model, which is time-consuming and may contain errors if done by a non-experienced user. Therefore, there is a need to automate this process so that augmented reality can be used effectively in the operating room. We present the Preoperative-to-Intraoperative Laparoscopic Fusion Challenge (P2ILF), held during the Medical Imaging and Computer Assisted Interventions (MICCAI 2022)
    
[^109]: 两种类型的人工智能存在风险：决定性和累积性

    Two Types of AI Existential Risk: Decisive and Accumulative

    [https://arxiv.org/abs/2401.07836](https://arxiv.org/abs/2401.07836)

    本文对比了传统的“决定性AI x-risk假设”与“累积性AI x-risk假设”，指出人工智能可能带来的灭绝性灾难有两种可能路径：一种是突然发生的AI接管，另一种是逐渐积累的威胁。

    

    传统上对人工智能(AI)引起的存在风险(x-risks)的讨论通常集中在由先进的AI系统引起的突然、严重事件上，尤其是那些可能达到或超过人类水平智能的系统。这些事件将带来严重后果，要么导致人类灭绝，要么无法逆转地使人类文明陷入无法恢复的状态。然而，这种讨论经常忽视AI x-risk逐渐通过一系列较小但相互关联的中断逐渐显现出来的严重可能性，随着时间的推移逐渐跨越关键阈值。该论文将传统的“决定性AI x-risk假设”与“累积性AI x-risk假设”进行对比。前者描绘了一种明显的AI接管路径，其特征是无法控制的超级智能等情景，而后者则提出了另一种导致灭绝性灾难的因果路径。这涉及到由AI引起的严重威胁的逐渐累积，例如严重的漏洞和系统性问题

    The conventional discourse on existential risks (x-risks) from AI typically focuses on abrupt, dire events caused by advanced AI systems, particularly those that might achieve or surpass human-level intelligence. These events have severe consequences that either lead to human extinction or irreversibly cripple human civilization to a point beyond recovery. This discourse, however, often neglects the serious possibility of AI x-risks manifesting incrementally through a series of smaller yet interconnected disruptions, gradually crossing critical thresholds over time. This paper contrasts the conventional "decisive AI x-risk hypothesis" with an "accumulative AI x-risk hypothesis." While the former envisions an overt AI takeover pathway, characterized by scenarios like uncontrollable superintelligence, the latter suggests a different causal pathway to existential catastrophes. This involves a gradual accumulation of critical AI-induced threats such as severe vulnerabilities and systemic e
    
[^110]: 指令融合：通过混合化推进提示演化

    Instruction Fusion: Advancing Prompt Evolution through Hybridization

    [https://arxiv.org/abs/2312.15692](https://arxiv.org/abs/2312.15692)

    本文介绍了一种新颖的方法，指令融合（IF），通过混合化两个不同的提示，改进了用于代码LLM的训练提示的演化。实验结果显示，指令融合有效地改善了代码LLM在多个代码生成任务上的性能。

    

    通过使用开放域编码查询，细调专门用于代码生成的大规模语言模型（LLM）已经取得了显着的进展。尽管取得了成功，但现有的方法如Evol-Instruct在性能方面存在限制，阻碍了进一步改进代码生成任务的能力。本文研究了现有提示演化技术的限制，并引入了一种新颖的方法，即指令融合（IF）。IF通过混合化过程创新地结合了两个不同的提示，从而增强了用于代码LLM的训练提示的演化。我们的实验结果表明，该提出的新方法有效地解决了之前方法的缺点，显著提高了代码LLM在人工评估、人工评估+、MBPP、MBPP+和MultiPL-E等五个代码生成基准上的性能，凸显了指令融合在推进LLM在代码生成方面的能力的有效性。

    The fine-tuning of Large Language Models (LLMs) specialized in code generation has seen notable advancements through the use of open-domain coding queries. Despite the successes, existing methodologies like Evol-Instruct encounter performance limitations, impeding further enhancements in code generation tasks. This paper examines the constraints of existing prompt evolution techniques and introduces a novel approach, Instruction Fusion (IF). IF innovatively combines two distinct prompts through a hybridization process, thereby enhancing the evolution of training prompts for code LLMs. Our experimental results reveal that the proposed novel method effectively addresses the shortcomings of prior methods, significantly improving the performance of Code LLMs across five code generation benchmarks, namely HumanEval, HumanEval+, MBPP, MBPP+ and MultiPL-E, which underscore the effectiveness of Instruction Fusion in advancing the capabilities of LLMs in code generation.
    
[^111]: 无监督深度学习图像验证方法

    Unsupervised Deep Learning Image Verification Method

    [https://arxiv.org/abs/2312.14395](https://arxiv.org/abs/2312.14395)

    本文提出了一种无监督深度学习图像验证方法，通过利用自动编码器将人脸图像向量转换为新的表示，进而缩小与无监督人脸验证技术之间的性能差距。在Labeled Faces in the Wild (LFW)数据集上实验结果显示，该方法相对于基线系统在EER方面实现了56%的相对改进。

    

    尽管深度学习在图像识别中被广泛应用，但通常需要大量标注的训练数据，往往难以获得。这导致了与最先进的无监督人脸验证技术相比存在明显的性能差距。在这项工作中，我们提出了一种方法通过利用自动编码器将人脸图像向量转换为新的表示来缩小这一差距。值得注意的是，自动编码器训练的是重构相邻的人脸图像向量而不是原始输入图像向量。这些邻居人脸图像向量是通过一个基于与训练人脸图像向量的最高余弦相似度的无监督过程选择的。该方法在Labeled Faces in the Wild (LFW)数据集上相对于基线系统在EER方面取得了56%的相对改进。这成功地缩小了余弦相似度和PLDA评分系统之间的性能差距。

    Although deep learning are commonly employed for image recognition, usually huge amount of labeled training data is required, which may not always be readily available. This leads to a noticeable performance disparity when compared to state-of-the-art unsupervised face verification techniques. In this work, we propose a method to narrow this gap by leveraging an autoencoder to convert the face image vector into a novel representation. Notably, the autoencoder is trained to reconstruct neighboring face image vectors rather than the original input image vectors. These neighbor face image vectors are chosen through an unsupervised process based on the highest cosine scores with the training face image vectors. The proposed method achieves a relative improvement of 56\% in terms of EER over the baseline system on Labeled Faces in the Wild (LFW) dataset. This has successfully narrowed down the performance gap between cosine and PLDA scoring systems.
    
[^112]: 对高维度游戏的对手塑形进行了扩展

    Scaling Opponent Shaping to High Dimensional Games

    [https://arxiv.org/abs/2312.12568](https://arxiv.org/abs/2312.12568)

    本文通过对对手塑形（OS）方法的扩展，成功将其应用于具有时间延长操作和长时间范围的广义游戏，该方法能够改善个体和集体的结果。

    

    在混合动机的多智能体环境中，已经证明了为零和游戏开发的方法会导致不利的结果。为了解决这个问题，对手塑形（OS）方法明确地学习如何影响合作玩家的学习动态，并且在实践中可以改善个体和集体的结果。然而，由于估计更高阶导数或扩展模型无关元学习的挑战，OS方法只在低维环境中进行了评估。能够扩展到复杂环境的替代方法要么收敛于不理想的解决方案，要么依赖于对环境或合作玩家进行了不切实际的假设。在本文中，我们第一次成功地将基于OS的方法扩展到具有时间延长操作和长时间范围的广义游戏中。经过对先前算法使用的元状态和历史的表示进行分析后，我们提出了一个简化版本的方法称为Shaper。经验证明，这种方法能够有效改善个体和集体的结果。

    In multi-agent settings with mixed incentives, methods developed for zero-sum games have been shown to lead to detrimental outcomes. To address this issue, opponent shaping (OS) methods explicitly learn to influence the learning dynamics of co-players and empirically lead to improved individual and collective outcomes. However, OS methods have only been evaluated in low-dimensional environments due to the challenges associated with estimating higher-order derivatives or scaling model-free meta-learning. Alternative methods that scale to more complex settings either converge to undesirable solutions or rely on unrealistic assumptions about the environment or co-players. In this paper, we successfully scale an OS-based approach to general-sum games with temporally-extended actions and long-time horizons for the first time. After analysing the representations of the meta-state and history used by previous algorithms, we propose a simplified version called Shaper. We show empirically that 
    
[^113]: 公平性约束能够在多大程度上帮助从有偏差的数据中恢复？

    How Far Can Fairness Constraints Help Recover From Biased Data?

    [https://arxiv.org/abs/2312.10396](https://arxiv.org/abs/2312.10396)

    公平性约束在极度有偏差的数据上能够恢复到原始数据分布上准确和公平的分类器。

    

    一般认为，在公平分类中，公平性约束会导致准确性的减少，而有偏差的数据可能会加剧这种情况。然而，Blum＆Stangl（2019）的研究表明，在极度有偏差的数据上，即使采用平等机会约束，也可以恢复到原始数据分布上准确和公平的分类器。他们的研究结果很有趣，因为它证明了公平性约束可以隐式修正数据偏差，同时克服了公平性与准确性之间的平衡问题。他们的数据偏差模型模拟了受压迫人群的表征和标签偏见，并在具有独立标签噪声的简单条件下，针对一个理想化的数据分布展示了上述结果。我们提出了一种通用方法，以扩展Blum＆Stangl（2019）的结果，适用于不同的公平性约束、数据偏差模型、数据分布和假设类别。

    A general belief in fair classification is that fairness constraints incur a trade-off with accuracy, which biased data may worsen. Contrary to this belief, Blum & Stangl (2019) show that fair classification with equal opportunity constraints even on extremely biased data can recover optimally accurate and fair classifiers on the original data distribution. Their result is interesting because it demonstrates that fairness constraints can implicitly rectify data bias and simultaneously overcome a perceived fairness-accuracy trade-off. Their data bias model simulates under-representation and label bias in underprivileged population, and they show the above result on a stylized data distribution with i.i.d. label noise, under simple conditions on the data distribution and bias parameters. We propose a general approach to extend the result of Blum & Stangl (2019) to different fairness constraints, data bias models, data distributions, and hypothesis classes. We strengthen their result, and
    
[^114]: DiSK: 一种结构化知识的扩散模型

    DiSK: A Diffusion Model for Structured Knowledge

    [https://arxiv.org/abs/2312.05253](https://arxiv.org/abs/2312.05253)

    DiSK是一种针对结构化数据的扩散模型，通过使用高斯混合模型方法处理文本、分类和连续数值数据，采用扩散训练来建模属性之间的关系，具有在多个不同领域的数据集上具有最先进性能的特点。

    

    结构化（类似字典的）数据对于从左到右的语言模型来说带来了挑战，因为它们可能因为格式和属性呈现的顺序的敏感性而难以处理结构化实体。标签生成模型面临着一系列不同的限制，比如缺乏灵活性。我们介绍了用于结构化数据的Diffusion Models of Structured Knowledge（DiSK） - 一种新的体系结构和训练方法。DiSK使用高斯混合模型方法处理文本、分类和连续数值数据，这样可以在处理数字时提高精确度。它采用扩散训练来建模属性之间的关系。实验表明，DiSK在超过15个不同领域的数据集上对表格数据建模、合成和填充具有最先进的性能。DiSK为生成建模和操作结构化数据提供了有效的归纳偏差。这一技术

    Structured (dictionary-like) data presents challenges for left-to-right language models, as they can struggle with structured entities for a wide variety of reasons such as formatting and sensitivity to the order in which attributes are presented. Tabular generative models suffer from a different set of limitations such as their lack of flexibility. We introduce Diffusion Models of Structured Knowledge (DiSK) - a new architecture and training approach specialized for structured data. DiSK handles text, categorical, and continuous numerical data using a Gaussian mixture model approach, which allows for improved precision when dealing with numbers. It employs diffusion training to model relationships between properties. Experiments demonstrate DiSK's state-of-the-art performance on tabular data modeling, synthesis, and imputation on over 15 datasets across diverse domains. DiSK provides an effective inductive bias for generative modeling and manipulation of structured data. The technique
    
[^115]: 大型语言模型的人类可读指纹

    Human-Readable Fingerprint for Large Language Models

    [https://arxiv.org/abs/2312.04828](https://arxiv.org/abs/2312.04828)

    这项研究介绍了一种大型语言模型的人类可读指纹，可以唯一识别出基本模型，并且不暴露模型参数或干扰训练。通过观察和验证，研究发现模型参数的向量方向在预训练后保持稳定，成为识别基本模型的重要条件。

    

    由于大型语言模型（LLM）的资源密集型训练和配套的精心设计的许可证，保护LLM的版权变得至关重要。然而，由于可能的参数修改，确定LLM的原始基本模型是具有挑战性的。在本研究中，我们介绍了一种用于LLM的人类可读指纹，可以唯一地识别基本模型，而不暴露模型参数或干扰训练。我们首先观察到，在预训练期间模型收敛后，LLM参数的向量方向保持稳定，通过后续的训练步骤，包括持续预训练、监督微调和RLHF，几乎没有扰动，这使得它成为识别基本模型的足够条件。通过继续训练LLM并添加一个额外的项来推开模型参数的方向，验证了这种必要性，结果使得模型受损。然而，这个方向容易受到简单攻击的影响，如维度...

    Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations. In this study, we introduce a human-readable fingerprint for LLMs that uniquely identifies the base model without exposing model parameters or interfering with training. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, showing negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning (SFT), and RLHF, which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension 
    
[^116]: 从被毒害的人类反馈中构建的通用越狱后门

    Universal Jailbreak Backdoors from Poisoned Human Feedback

    [https://arxiv.org/abs/2311.14455](https://arxiv.org/abs/2311.14455)

    本文提出了一种新的威胁，攻击者通过毒害训练数据向语言模型中嵌入“越狱后门”，并通过添加特定的触发词使模型产生有害的回应。这种通用越狱后门比之前的研究更强大，且较难被察觉。研究探究了RLHF设计中的决策对其鲁棒性的影响，并发布了一组被毒害模型的基准测试数据，以促进未来对通用越狱后门的研究。

    

    强化学习从人类反馈中（RLHF）用于对齐大型语言模型，以产生有用且无害的回应。然而，先前的研究显示，这些模型可以通过找到使模型恢复到未对齐行为的对抗提示来进行越狱。在本文中，我们考虑了一个新的威胁，攻击者通过毒害RLHF训练数据将“越狱后门”嵌入模型中。该后门将一个触发词嵌入模型中，类似于通用的“sudo命令”：在任何提示中添加触发词将使模型产生有害的回应，无需搜索对抗提示。通用越狱后门比先前研究的语言模型后门更强大，我们发现使用常见的后门攻击技术要困难得多。我们探究了RLHF中的设计决策对其所声称的鲁棒性的贡献，并发布一组被毒害模型的基准，以促进对通用越狱后门的未来研究。

    Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a "jailbreak backdoor" into the model. The backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak bac
    
[^117]: VALUED - 视觉和逻辑理解评估数据集

    VALUED -- Vision and Logical Understanding Evaluation Dataset

    [https://arxiv.org/abs/2311.12610](https://arxiv.org/abs/2311.12610)

    VALUED数据集是一个基于棋盘游戏-象棋的数据集，包含20万多个带注释的图像和一个规则集，旨在解决深度学习在捕捉语义和逻辑上下文方面的问题。

    

    从计算机视觉任务的早期成功开始，基于深度学习的技术已经超过了各个领域中的最先进方法。然而，已经多次证明这些技术无法捕捉语义上下文和逻辑约束，而是常常依赖于虚假的相关性来得出答案。由于深度学习技术在关键场景中的应用依赖于遵守特定领域的约束条件，因此已经进行了多次尝试来解决这个问题。一个限制深入研究的问题是缺乏具有丰富规则集的合适数据集。为了解决这个问题，我们提出了VALUE（视觉和逻辑理解评估）数据集，包含20万多个带注释的图像和一个基于流行的棋盘游戏—象棋的规则集。经过精心策划的规则集极大地限制了可接受的预测范围，并旨在探索关键的语义和逻辑理解挑战。

    Starting with early successes in computer vision tasks, deep learning based techniques have since overtaken state of the art approaches in a multitude of domains. However, it has been demonstrated time and again that these techniques fail to capture semantic context and logical constraints, instead often relying on spurious correlations to arrive at the answer. Since application of deep learning techniques to critical scenarios are dependent on adherence to domain specific constraints, several attempts have been made to address this issue. One limitation holding back a thorough exploration of this area, is a lack of suitable datasets which feature a rich set of rules. In order to address this, we present the VALUE (Vision And Logical Understanding Evaluation) Dataset, consisting of 200,000$+$ annotated images and an associated rule set, based on the popular board game - chess. The curated rule set considerably constrains the set of allowable predictions, and are designed to probe key s
    
[^118]: 无需优化的测试时适应性跨人活动识别

    Optimization-Free Test-Time Adaptation for Cross-Person Activity Recognition

    [https://arxiv.org/abs/2310.18562](https://arxiv.org/abs/2310.18562)

    本文提出了一种无需优化的测试时适应（OFTTA）框架，用于解决人体活动识别中的个体间分布偏移问题。该框架通过调整特征提取器和线性分类器的方式来适应不同个体的活动模式，并采用指数衰减测试时归一化（EDTN）替代传统批归一化来提取可靠的特征。

    

    由于活动模式在个体间存在分布偏移，人体活动识别（HAR）模型在实际应用中往往会遭受性能下降。测试时适应（TTA）是一种新兴的学习范式，旨在利用测试数据流在实时推理中调整预测结果，而这在HAR领域尚未得到探索。然而，基于优化的TTA算法的高计算成本使得其在资源受限的边缘设备上无法执行。本文提出了一种无需优化的测试时适应（OFTTA）框架，用于基于传感器的HAR。OFTTA以无需优化的方式同时调整特征提取器和线性分类器。对于特征提取器，我们提出了指数衰减测试时归一化（EDTN）来替代传统的批归一化（CBN）层。EDTN通过结合CBN和测试时批归一化（TBN）来提取可靠的特征以应对领域偏移，其中TBN的影响逐渐减小。

    Human Activity Recognition (HAR) models often suffer from performance degradation in real-world applications due to distribution shifts in activity patterns across individuals. Test-Time Adaptation (TTA) is an emerging learning paradigm that aims to utilize the test stream to adjust predictions in real-time inference, which has not been explored in HAR before. However, the high computational cost of optimization-based TTA algorithms makes it intractable to run on resource-constrained edge devices. In this paper, we propose an Optimization-Free Test-Time Adaptation (OFTTA) framework for sensor-based HAR. OFTTA adjusts the feature extractor and linear classifier simultaneously in an optimization-free manner. For the feature extractor, we propose Exponential DecayTest-time Normalization (EDTN) to replace the conventional batch normalization (CBN) layers. EDTN combines CBN and Test-time batch Normalization (TBN) to extract reliable features against domain shifts with TBN's influence decrea
    
[^119]: OHQ: 芯片上的硬件感知量化

    OHQ: On-chip Hardware-aware Quantization

    [https://arxiv.org/abs/2309.01945](https://arxiv.org/abs/2309.01945)

    本文提出了一种在芯片上进行硬件感知混合精度量化的框架（OHQ），通过构建量化感知流水线和引入掩码引导的量化估计技术，实现了在资源受限的硬件上进行高效量化，填补了现有混合精度量化的搜索空间过大和实际部署差距大的问题。

    

    量化成为在资源受限的硬件上部署先进深度模型的最有前途的方法之一。混合精度量化利用多位宽架构来释放量化模型的准确性和效率潜力。然而，现有的混合精度量化存在搜索空间过大的问题，导致巨大的计算开销。因此，量化过程依赖于独立的高性能设备，而不是本地进行，这也导致了考虑的硬件指标与实际部署之间的显著差距。本文提出了一种在芯片上进行硬件感知混合精度量化的框架（OHQ），而无需访问在线设备。首先，我们构建了芯片上的量化感知（OQA）流水线，能够感知量化算子在硬件上的实际效率指标。其次，我们提出了基于掩码引导的量化估计（MQE）技术。

    Quantization emerges as one of the most promising approaches for deploying advanced deep models on resource-constrained hardware. Mixed-precision quantization leverages multiple bit-width architectures to unleash the accuracy and efficiency potential of quantized models. However, existing mixed-precision quantization suffers exhaustive search space that causes immense computational overhead. The quantization process thus relies on separate high-performance devices rather than locally, which also leads to a significant gap between the considered hardware metrics and the real deployment.In this paper, we propose an On-chip Hardware-aware Quantization (OHQ) framework that performs hardware-aware mixed-precision quantization without accessing online devices. First, we construct the On-chip Quantization Awareness (OQA) pipeline, enabling perceive the actual efficiency metrics of the quantization operator on the hardware.Second, we propose Mask-guided Quantization Estimation (MQE) technique 
    
[^120]: 集成学习多样性的统一理论

    A Unified Theory of Diversity in Ensemble Learning

    [https://arxiv.org/abs/2301.03962](https://arxiv.org/abs/2301.03962)

    这篇论文提出了一个统一的集成学习多样性理论，解释了多样性在各种监督学习场景中的本质。它揭示了多样性在集成损失的偏差-方差分解中的作用，同时提出了一种量化多样性效果的方法。这个理论对于提高集成模型的性能具有重要意义。

    

    我们提出了一个集成多样性的理论，解释了在各种监督学习场景中多样性的本质。这个挑战被称为集成学习的圣杯，是一个开放的研究问题已经有30多年了。我们的框架揭示了多样性实际上是集成损失的偏差-方差分解中的一个隐藏维度。我们证明了一族精确的偏差-方差-多样性分解，适用于回归和分类的各种损失函数，例如平方损失、交叉熵损失和泊松损失。对于没有可加性偏差-方差分解的损失函数（例如0/1损失），我们提出了一种替代方法：量化多样性的效果，结果依赖于标签分布。总体而言，我们认为多样性是模型拟合度的度量，与偏差和方差具有相同的意义，但考虑了集成成员之间的统计依赖关系。因此，我们不应该最大化多样性。

    We present a theory of ensemble diversity, explaining the nature of diversity for a wide range of supervised learning scenarios. This challenge has been referred to as the holy grail of ensemble learning, an open research issue for over 30 years. Our framework reveals that diversity is in fact a hidden dimension in the bias-variance decomposition of the ensemble loss. We prove a family of exact bias-variance-diversity decompositions, for a wide range of losses in both regression and classification, e.g., squared, cross-entropy, and Poisson losses. For losses where an additive bias-variance decomposition is not available (e.g., 0/1 loss) we present an alternative approach: quantifying the effects of diversity, which turn out to be dependent on the label distribution. Overall, we argue that diversity is a measure of model fit, in precisely the same sense as bias and variance, but accounting for statistical dependencies between ensemble members. Thus, we should not be maximising diversity
    
[^121]: 一种稳定、快速和完全自动的预测编码网络学习算法

    A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks

    [https://arxiv.org/abs/2212.00720](https://arxiv.org/abs/2212.00720)

    本文提出了一种稳定、快速且完全自动的预测编码网络学习算法(iPC)，通过改变突触权重的更新规则的时间调度，提高了训练效率和稳定性，并具有收敛性保证。实验证明，在图像分类和语言模型训练等多个任务上，iPC相比原始算法在测试准确性、效率和收敛性方面表现更好。

    

    预测编码网络是一种受神经科学启发的模型，根植于贝叶斯统计学和神经科学。然而，训练这样的模型非常低效且不稳定。在本文中，我们展示了通过简单改变突触权重的更新规则的时间调度，可以得到比原始算法更高效、更稳定且具有收敛性保证的算法。我们提出的算法被称为递增预测编码(iPC)，也比原始算法更符合生物学可行性，因为它是完全自动的。通过一系列广泛的实验，我们展示了iPC在图像分类的许多基准测试以及条件语言模型和掩蔽语言模型的训练中，在测试准确性、效率和收敛性方面始终优于原始方法，并且对于大量超参数具有收敛性。

    Predictive coding networks are neuroscience-inspired models with roots in both Bayesian statistics and neuroscience. Training such models, however, is quite inefficient and unstable. In this work, we show how by simply changing the temporal scheduling of the update rule for the synaptic weights leads to an algorithm that is much more efficient and stable than the original one, and has theoretical guarantees in terms of convergence. The proposed algorithm, that we call incremental predictive coding (iPC) is also more biologically plausible than the original one, as it it fully automatic. In an extensive set of experiments, we show that iPC constantly performs better than the original formulation on a large number of benchmarks for image classification, as well as for the training of both conditional and masked language models, in terms of test accuracy, efficiency, and convergence with respect to a large set of hyperparameters.
    
[^122]: TA-RNN：一种基于注意力机制的面向电子健康记录的时间感知递归神经网络架构

    TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records. (arXiv:2401.14694v1 [cs.LG])

    [http://arxiv.org/abs/2401.14694](http://arxiv.org/abs/2401.14694)

    TA-RNN和TA-RNN-AE是两种基于RNN的可解释深度学习架构，用于分析电子健康记录并预测患者的临床结果。这些架构考虑了EHR数据的不规则性和时间间隔，并采用时间嵌入的方法解决了这些问题。

    

    动机：电子健康记录（EHR）是患者医疗历史的全面资源。EHR对于利用深度学习（DL）等先进技术至关重要，使医疗提供者能够分析大量数据，提取有价值的见解，并做出精确、数据驱动的临床决策。DL方法如递归神经网络（RNN）已被用于分析EHR以建模疾病进展并预测诊断。然而，这些方法并没有解决EHR数据中一些固有的不规则性，如临床访问之间的不规则时间间隔。此外，大多数DL模型都不可解释。在这项研究中，我们提出了两种基于RNN的可解释DL架构，分别是时间感知RNN（TA-RNN）和TA-RNN-Autoencoder（TA-RNN-AE），用于预测下一次访问和多次未来访问中患者的临床结果。为了减轻不规则时间间隔的影响，我们提出了时间嵌入的方法将时间信息纳入模型中。

    Motivation: Electronic Health Records (EHR) represent a comprehensive resource of a patient's medical history. EHR are essential for utilizing advanced technologies such as deep learning (DL), enabling healthcare providers to analyze extensive data, extract valuable insights, and make precise and data-driven clinical decisions. DL methods such as Recurrent Neural Networks (RNN) have been utilized to analyze EHR to model disease progression and predict diagnosis. However, these methods do not address some inherent irregularities in EHR data such as irregular time intervals between clinical visits. Furthermore, most DL models are not interpretable. In this study, we propose two interpretable DL architectures based on RNN, namely Time-Aware RNN (TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patient's clinical outcome in EHR at next visit and multiple visits ahead, respectively. To mitigate the impact of irregular time intervals, we propose incorporating time embedding of the elaps
    
[^123]: 大语言模型时代的进化计算：调查与路线图

    Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap. (arXiv:2401.10034v1 [cs.NE])

    [http://arxiv.org/abs/2401.10034](http://arxiv.org/abs/2401.10034)

    该论文调查了大语言模型和进化计算之间的相互作用，并提出了在黑盒设置下进一步提升大语言模型性能的优化框架，以及将大语言模型与进化算法结合应用于各种任务的方法。

    

    大型语言模型（LLMs）是基于Transformer架构，在多样的数据上进行大规模预训练的，它们不仅在自然语言处理领域引起了革命，还将其能力扩展到了各个领域，迈向了人工通用智能的重要一步。尽管进化算法（EAs）与LLMs在目标和方法论上存在差异，但它们之间的相互作用揭示了有趣的相似之处，特别是在他们共同的优化性质、黑盒特性和处理复杂问题的能力方面。与此同时，进化算法不仅可以为LLM在黑盒设置下提供优化框架，还可以在应用中为LLM赋予灵活的全局搜索和迭代机制。另一方面，LLM丰富的领域知识使得进化算法可以进行更智能的搜索，而其文本处理能力则有助于将进化算法应用于各种任务。基于它们的互补优势，本文提出了一份调查和路线图。

    Large Language Models (LLMs), built upon Transformer-based architectures with massive pretraining on diverse data, have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives and methodologies, reveals intriguing parallels, especially in their shared optimization nature, black-box characteristics, and proficiency in handling complex problems. Meanwhile, EA can not only provide an optimization framework for LLM's further enhancement under black-box settings but also empower LLM with flexible global search and iterative mechanism in applications. On the other hand, LLM's abundant domain knowledge enables EA to perform smarter searches, while its text processing capability assist in deploying EA across various tasks. Based on their complementary advantages, this paper presents a 
    
[^124]: 关于高效联邦学习方法在基础模型训练中的调查

    A Survey on Efficient Federated Learning Methods for Foundation Model Training. (arXiv:2401.04472v1 [cs.LG])

    [http://arxiv.org/abs/2401.04472](http://arxiv.org/abs/2401.04472)

    这项调查研究了高效联邦学习方法在基础模型训练中的应用，提出了一个新的分类方法以优化计算和通信效率。该研究还讨论了当前广泛使用的FL框架，并展望了未来的研究潜力。

    

    联邦学习（FL）已成为一种促进隐私保护协作训练的成熟技术。然而，新的FL方法通常只涉及小型深度学习模型的贡献。随着Transformer模型的巨大成功，一个问题出现了：如何使基础模型在FL应用中实施起来？鉴于在FL中计算和通信的时间消耗通常相似，我们引入了一个关于在FL应用中的计算和通信效率方法的新的分类方法。这些方法旨在优化训练时间并减少客户端与服务器之间的通信。我们还研究了目前广泛使用的FL框架，并根据FL研究及其延伸的现有方法讨论了未来的研究潜力。

    Federated Learning (FL) has become an established technique to facilitate privacy-preserving collaborative training. However, new approaches to FL often discuss their contributions involving small deep-learning models only. With the tremendous success of transformer models, the following question arises: What is necessary to operationalize foundation models in an FL application? Knowing that computation and communication often take up similar amounts of time in FL, we introduce a novel taxonomy focused on computational and communication efficiency methods in FL applications. This said, these methods aim to optimize the training time and reduce communication between clients and the server. We also look at the current state of widely used FL frameworks and discuss future research potentials based on existing approaches in FL research and beyond.
    
[^125]: 更好地了解您的需求：通过类比推理增强的LLMs实现对营销人员需求的结构化理解

    Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. (arXiv:2401.04319v1 [cs.CL])

    [http://arxiv.org/abs/2401.04319](http://arxiv.org/abs/2401.04319)

    本文探索了一种通过类比推理增强的LLMs来实现对营销人员需求的结构化理解的新方式，使非专业营销人员能够仅凭需求的自然语言形式选择目标用户。

    

    本文探讨了一种新的用户定位方式，即非专业营销人员可以仅凭需求的自然语言形式选择目标用户。解决这个问题的关键在于如何将自然语言转化为实际的结构化逻辑语言，即对营销人员需求的结构化理解。考虑到大型语言模型（LLMs）出色的自然语言处理能力，我们尝试利用LLMs来解决这个问题。过去的研究表明，通过链式思考（CoT）提示可以有效增强LLMs的推理能力。但是现有方法仍然存在一些限制：（1）先前的方法要么使用简单的“让我们一步一步地思考”提示，要么在演示中提供固定的示例而不考虑提示和问题之间的兼容性，在一些复杂的推理任务（如结构化语言转换）中使LLMs无效。(2) 先前的方法通常在闭源模型或过度实现的模型中实现。

    In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. Past research indicates that the reasoning ability of LLMs can be effectively enhanced through chain-of-thought (CoT) prompting. But existing methods still have some limitations: (1) Previous methods either use simple "Let's think step by step" spells or provide fixed examples in demonstrations without considering compatibility between prompts and questions, making LLMs ineffective in some complex reasoning tasks such as structured language transformation. (2) Previous methods are often implemented in closed-source models or exces
    
[^126]: 从提示工程到人在循环中的提示科学

    From Prompt Engineering to Prompt Science With Human in the Loop. (arXiv:2401.04122v1 [cs.HC])

    [http://arxiv.org/abs/2401.04122](http://arxiv.org/abs/2401.04122)

    基于代码书构建方法和多阶段验证过程，本文提出一种新的方法来更系统地在研究中使用LLMs。

    

    随着LLMs在我们生活的许多方面的应用，科学研究领域是一个需要增加审查的地方。LLMs被用于生成或分析研究数据的应用正变得越来越流行。但是，当这种应用被临时决策和工程解决方案所困扰时，我们需要关注它如何影响研究、研究结果或者基于该研究的任何未来工作。我们需要更科学的方法来在我们的研究中使用LLMs。虽然目前有一些积极的努力支持更系统的提示构建，但它们往往更注重实现期望的结果，而不是产生可复制和具有足够透明度、客观性或严谨性的广泛知识。本文提出了一种新的方法，灵感来自通过定性方法构建代码书的方法，以解决这个问题。通过人在循环和多阶段验证过程，该方法为更系统的研究打下了基础。

    As LLMs make their way into many aspects of our lives, one place that warrants increased scrutiny with LLM usage is scientific research. Using LLMs for generating or analyzing data for research purposes is gaining popularity. But when such application is marred with ad-hoc decisions and engineering solutions, we need to be concerned about how it may affect that research, its findings, or any future works based on that research. We need a more scientific approach to using LLMs in our research. While there are several active efforts to support more systematic construction of prompts, they are often focused more on achieving desirable outcomes rather than producing replicable and generalizable knowledge with sufficient transparency, objectivity, or rigor. This article presents a new methodology inspired by codebook construction through qualitative methods to address that. Using humans in the loop and a multi-phase verification processes, this methodology lays a foundation for more systema
    
[^127]: 提升强化学习中汤普森采样的贝叶斯遗憾界

    Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning. (arXiv:2310.20007v1 [stat.ML])

    [http://arxiv.org/abs/2310.20007](http://arxiv.org/abs/2310.20007)

    本文在多种情境下证明了汤普森采样在强化学习中的贝叶斯遗憾界，并通过对信息比的精确分析提出了一个基于时间不均匀强化学习问题的上界估计。同时，本文找到了各种设置中具体的界限，并讨论了这些结果是第一个其类别或改进了最先进方法的情况。

    

    本文证明了在多种情境下，汤普森采样在强化学习中的第一个贝叶斯遗憾界。我们利用离散的代理环境简化学习问题，并通过后验一致性对信息比进行了精确分析。这导致了一个基于时间不均匀强化学习问题的上界估计为$\widetilde{O}(H\sqrt{d_{l_1}T})$，其中$H$为回合长度，$d_{l_1}$为环境空间的Kolmogorov $l_1$维度。然后，我们在各种设置中找到了$d_{l_1}$的具体界限，比如表格、线性和有限混合，讨论了我们的结果是第一个其类别或改进了最先进方法的情况。

    In this paper, we prove the first Bayesian regret bounds for Thompson Sampling in reinforcement learning in a multitude of settings. We simplify the learning problem using a discrete set of surrogate environments, and present a refined analysis of the information ratio using posterior consistency. This leads to an upper bound of order $\widetilde{O}(H\sqrt{d_{l_1}T})$ in the time inhomogeneous reinforcement learning problem where $H$ is the episode length and $d_{l_1}$ is the Kolmogorov $l_1-$dimension of the space of environments. We then find concrete bounds of $d_{l_1}$ in a variety of settings, such as tabular, linear and finite mixtures, and discuss how how our results are either the first of their kind or improve the state-of-the-art.
    
[^128]: XAI的公平效益的批判性调查

    A Critical Survey on Fairness Benefits of XAI. (arXiv:2310.13007v1 [cs.AI])

    [http://arxiv.org/abs/2310.13007](http://arxiv.org/abs/2310.13007)

    这个批判性调查分析了可解释的人工智能（XAI）与公平之间的关系，指出XAI在实现公平理想方面存在潜力和限制，呼吁更具体地说明XAI方法如何帮助解决公平理想。

    

    在这个批判性调查中，我们分析了关于可解释的人工智能（XAI）和公平之间关系的典型论述，以解开这两个概念之间的多维关系。通过系统文献综述和随后的定性内容分析，我们从175篇论文中识别出关于XAI的公平效益的七个典型论断。我们提出关于这些论断的重要警告，并为未来围绕XAI在特定公平理想中的潜力和限制进行讨论提供了一个切入点。虽然文献通常认为XAI是实现多个公平理想的一种手段，但我们注意到这些理想与XAI的能力之间存在不一致。我们鼓励将XAI视为应对算法公平这一多维社会技术挑战的众多工具之一，并更具体地说明哪种XAI方法如何帮助哪些人解决哪些公平理想。

    In this critical survey, we analyze typical claims on the relationship between explainable AI (XAI) and fairness to disentangle the multidimensional relationship between these two concepts. Based on a systematic literature review and a subsequent qualitative content analysis, we identify seven archetypal claims from 175 papers on the alleged fairness benefits of XAI. We present crucial caveats with respect to these claims and provide an entry point for future discussions around the potentials and limitations of XAI for specific fairness desiderata. While the literature often suggests XAI to be an enabler for several fairness desiderata, we notice a misalignment between these desiderata and the capabilities of XAI. We encourage to conceive XAI as one of many tools to approach the multidimensional, sociotechnical challenge of algorithmic fairness and to be more specific about how exactly what kind of XAI method enables whom to address which fairness desideratum.
    
[^129]: 通过自动折扣调度从观察中进行模仿学习

    Imitation Learning from Observation with Automatic Discount Scheduling. (arXiv:2310.07433v1 [cs.RO])

    [http://arxiv.org/abs/2310.07433](http://arxiv.org/abs/2310.07433)

    我们提出了一个新颖的观察学习模仿(ILfO)框架，能够解决由于奖励信号分配错误导致代理无法学习初始行为的问题。

    

    人类通常通过观察和模仿来获得新的技能。对于机器人代理，从互联网上可用的大量无标签视频演示数据中进行学习，需要在没有访问其动作的情况下模仿专家，这是一种称为观察学习模仿（ILfO）的挑战。解决ILfO问题的常见方法是将其转化为逆向强化学习问题，利用从代理和专家观察中计算出的代理奖励。然而，我们发现在具有进展依赖性属性的任务中，这样的方法面临重大挑战；在这些任务中，代理需要在掌握后续行为之前先学习专家的前序行为。我们的研究表明，主要原因是分配给后续步骤的奖励信号妨碍了对初始行为的学习。为了解决这个挑战，我们提出了一个新颖的ILfO框架，使代理能够掌握早期行为。

    Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observations (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we present a novel ILfO framework that enables the agent to master earl
    
[^130]: 使用可传输的图自编码器进行网络对齐

    Network Alignment with Transferable Graph Autoencoders. (arXiv:2310.03272v1 [cs.LG])

    [http://arxiv.org/abs/2310.03272](http://arxiv.org/abs/2310.03272)

    该论文提出了一种基于图自编码器的网络对齐方法，通过生成与图的特征值和特征向量相关的节点嵌入，实现了更准确的对齐。同时，该方法还利用迁移学习和数据增强技术，在大规模网络对齐任务中实现高效的对齐，无需重新训练。

    

    网络对齐是在不同图之间建立一对一对应关系的任务，在高影响领域中有大量应用。然而，这个任务在一般情况下被认为是NP难的，而且现有的算法在图的规模增大时无法扩展。为了解决这两个挑战，我们提出了一种新颖的广义图自编码器架构，旨在提取强大且鲁棒的节点嵌入，适用于对齐任务。我们证明生成的嵌入与图的特征值和特征向量相关，并且与经典谱方法相比可以实现更准确的对齐。我们提出的框架还利用迁移学习和数据增强，在无需重新训练的情况下实现高效的大规模网络对齐。在真实世界的图上进行了广泛的网络对齐和子网络对齐实验，提供了支持该框架有效性和可扩展性的证据。

    Network alignment is the task of establishing one-to-one correspondences between the nodes of different graphs and finds a plethora of applications in high-impact domains. However, this task is known to be NP-hard in its general form, and existing algorithms do not scale up as the size of the graphs increases. To tackle both challenges we propose a novel generalized graph autoencoder architecture, designed to extract powerful and robust node embeddings, that are tailored to the alignment task. We prove that the generated embeddings are associated with the eigenvalues and eigenvectors of the graphs and can achieve more accurate alignment compared to classical spectral methods. Our proposed framework also leverages transfer learning and data augmentation to achieve efficient network alignment at a very large scale without retraining. Extensive experiments on both network and sub-network alignment with real-world graphs provide corroborating evidence supporting the effectiveness and scala
    
[^131]: 通过扩散学习实现目标达成

    Learning to Reach Goals via Diffusion. (arXiv:2310.02505v1 [cs.LG])

    [http://arxiv.org/abs/2310.02505](http://arxiv.org/abs/2310.02505)

    本论文提出了一种通过扩散学习实现目标达成的方法，可以在任意初始状态下从预定义或新目标达成，而无需学习单独的价值函数。

    

    扩散模型是一类强大的生成模型，能够通过迭代去噪将高维空间中的随机噪声映射到目标流形上。在本研究中，我们通过将目标条件强化学习框架放在扩散建模的背景下，给出了一种新的视角。类似于扩散过程，其中利用高斯噪声创建随机轨迹，使其远离数据流形，我们构造了远离潜在目标状态的轨迹。然后我们学习一个类似于评分函数的目标条件策略。这个称为Merlin的方法能够在任意初始状态下从预定义或新目标达成，而无需学习单独的价值函数。我们考虑了三种选择，用于取代扩散中的高斯噪声模型 - 缓冲区中的反向播放，反向动力学模型和一种新的非参数方法。我们在离线目标达成任务上理论上证明了我们的方法，并对其进行了验证。

    Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks.
    
[^132]: 通过文本到图像扩散跨越领域：一种无源域自适应方法

    Transcending Domains through Text-to-Image Diffusion: A Source-Free Approach to Domain Adaptation. (arXiv:2310.01701v1 [cs.CV])

    [http://arxiv.org/abs/2310.01701](http://arxiv.org/abs/2310.01701)

    本文提出了一种无源域自适应方法，通过训练文本到图像扩散模型在目标领域上生成源数据，并使用领域自适应技术将其与目标领域数据对齐。

    

    领域自适应（DA）是一种通过应用模型从相关源领域获取信息，从而提高模型在目标领域中不充足标注数据上的性能的方法。数据隐私法规（如HIPAA、COPPA、FERPA等）的不断加强引发了对在绕过对源数据的直接访问的情况下，适应新领域的模型的兴趣，这个问题被称为无源域自适应（SFDA）。本文提出了一个新颖的SFDA框架，通过在目标领域样本上训练一个文本到图像扩散模型来生成源数据。我们的方法首先在标记的目标领域样本上训练一个文本到图像扩散模型，然后使用预训练的源模型对其进行微调，生成接近源数据的样本。最后，我们使用领域自适应技术将人工生成的源数据与目标领域数据进行对齐，

    Domain Adaptation (DA) is a method for enhancing a model's performance on a target domain with inadequate annotated data by applying the information the model has acquired from a related source domain with sufficient labeled data. The escalating enforcement of data-privacy regulations like HIPAA, COPPA, FERPA, etc. have sparked a heightened interest in adapting models to novel domains while circumventing the need for direct access to the source data, a problem known as Source-Free Domain Adaptation (SFDA). In this paper, we propose a novel framework for SFDA that generates source data using a text-to-image diffusion model trained on the target domain samples. Our method starts by training a text-to-image diffusion model on the labeled target domain samples, which is then fine-tuned using the pre-trained source model to generate samples close to the source data. Finally, we use Domain Adaptation techniques to align the artificially generated source data with the target domain data, resu
    
[^133]: 基于多智能体深度强化学习的AI驱动患者监测

    AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning. (arXiv:2309.10980v1 [cs.LG])

    [http://arxiv.org/abs/2309.10980](http://arxiv.org/abs/2309.10980)

    本研究提出了一种基于多智能体深度强化学习的AI驱动患者监测框架，通过部署多个学习智能体，针对不同的生理特征进行监测，并根据紧急程度预警医疗紧急团队。

    

    有效的患者监测对及时干预和改善医疗结果至关重要。传统的监测系统往往难以处理复杂、动态的环境和波动的生命体征，导致延迟发现危急情况。为了应对这一挑战，我们提出了一种新颖的基于多智能体深度强化学习（DRL）的AI驱动患者监测框架。我们的方法部署了多个学习智能体，每个智能体专门负责监测特定的生理特征，如心率、呼吸和体温。这些智能体与通用的医疗监测环境进行交互，学习患者的行为模式，并根据估计的紧急程度做出通知相应医疗紧急团队（MET）的决策。在本研究中，我们使用来自两个数据集（PPG-DaLiA和WESAD）的真实生理和运动数据评估了提出的多智能体DRL框架的性能。

    Effective patient monitoring is vital for timely interventions and improved healthcare outcomes. Traditional monitoring systems often struggle to handle complex, dynamic environments with fluctuating vital signs, leading to delays in identifying critical conditions. To address this challenge, we propose a novel AI-driven patient monitoring framework using multi-agent deep reinforcement learning (DRL). Our approach deploys multiple learning agents, each dedicated to monitoring a specific physiological feature, such as heart rate, respiration, and temperature. These agents interact with a generic healthcare monitoring environment, learn the patients' behavior patterns, and make informed decisions to alert the corresponding Medical Emergency Teams (METs) based on the level of emergency estimated. In this study, we evaluate the performance of the proposed multi-agent DRL framework using real-world physiological and motion data from two datasets: PPG-DaLiA and WESAD. We compare the results 
    
[^134]: OpenAI抄袭了我们的税务案例，但GPT-4真的能够处理税务吗？

    OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?. (arXiv:2309.09992v1 [cs.AI])

    [http://arxiv.org/abs/2309.09992](http://arxiv.org/abs/2309.09992)

    GPT-4在处理税务方面存在问题，无法可靠地计算税务。

    

    作者解释了OpenAI在GPT-4的直播演示中使用税法案例的来源，以及为什么GPT-4得到了错误的答案，以及它如何无法可靠地计算税务。

    The authors explain where OpenAI got the tax law example in its livestream demonstration of GPT-4, why GPT-4 got the wrong answer, and how it fails to reliably calculate taxes.
    
[^135]: 通过一致性预言机进行简单的在线学习

    Simple online learning with consistency oracle. (arXiv:2308.08055v1 [cs.LG])

    [http://arxiv.org/abs/2308.08055](http://arxiv.org/abs/2308.08055)

    该论文介绍了在只能通过一致性预言机访问类的模型下的在线学习算法，提出了一种更简单且效果更好的算法。该算法最多会犯O(256^d)个错误，并观察到不存在一个最多会犯2^(d+1)-2个错误的算法。

    

    我们考虑在只能通过一致性预言机访问类的模型下的在线学习——在任何时刻，预言机都能给出与目前为止看到的所有示例一致的类函数。该模型最近由Assos等人（COLT'23）考虑。这个模型的动机是标准的在线学习方法依赖于计算子类的Littlestone维度，这是一个计算复杂的问题。Assos等人在这个模型中给出了一个在线学习算法，对于Littlestone维度为d的类，最多会犯C^d个错误，其中C是一个未指定的绝对常数且大于0。我们提出了一个新的算法，最多会犯O(256^d)个错误。我们的证明更简单，只使用了Littlestone维度的基本属性。我们还观察到，不存在一个在这个模型中最多会犯2^(d+1)-2个错误的算法。我们还观察到，我们的算法（以及Assos等人的算法）。

    We consider online learning in the model where a learning algorithm can access the class only via the consistency oracle -- an oracle, that, at any moment, can give a function from the class that agrees with all examples seen so far. This model was recently considered by Assos et al. (COLT'23). It is motivated by the fact that standard methods of online learning rely on computing the Littlestone dimension of subclasses, a problem that is computationally intractable. Assos et al. gave an online learning algorithm in this model that makes at most $C^d$ mistakes on classes of Littlestone dimension $d$, for some absolute unspecified constant $C > 0$. We give a novel algorithm that makes at most $O(256^d)$ mistakes. Our proof is significantly simpler and uses only very basic properties of the Littlestone dimension. We also observe that there exists no algorithm in this model that makes at most $2^{d+1}-2$ mistakes. We also observe that our algorithm (as well as the algorithm of Assos et al.
    
[^136]: 面向异构 SoC 的共享内存竞争感知的并发 DNN 执行

    Shared Memory-contention-aware Concurrent DNN Execution for Diversely Heterogeneous System-on-Chips. (arXiv:2308.05869v1 [cs.DC])

    [http://arxiv.org/abs/2308.05869](http://arxiv.org/abs/2308.05869)

    这项工作提出了一种名为 HaX-CoNN 的方案，可以实现在异构 SoC 上并发执行 DNN 推理工作负载，并考虑了共享内存竞争和加速器之间的转换以找到最优的调度方案。

    

    当前移动和自主系统的两个显著特点是：1）通常同时连续运行多个工作负载，主要是深度神经网络 (DNN) 推理；2）它们在嵌入了针对特定操作的异构加速器的共享内存系统芯片上运行。目前的技术缺乏高效的性能和资源管理技术，无法最大化总系统吞吐量或最小化端到端工作负载延迟。在本研究中，我们提出了一种名为 HaX-CoNN 的新方案，将并发执行的 DNN 推理工作负载的层进行特征化和映射到 SoC 中的多样加速器。我们的方案独特地考虑了每层的执行特性、共享内存 (SM) 的竞争以及加速器之间的转换，以找到最优的调度方案。我们在 NVIDIA Orin、NVIDIA Xavier 和 Qualcomm Snapdragon 865 SoC 上评估了 HaX-CoNN。我们的实验结果表明，HaX-CoNN 可以最小化

    Two distinguishing features of state-of-the-art mobile and autonomous systems are 1) there are often multiple workloads, mainly deep neural network (DNN) inference, running concurrently and continuously; and 2) they operate on shared memory system-on-chips (SoC) that embed heterogeneous accelerators tailored for specific operations. State-of-the-art lacks efficient performance and resource management techniques necessary to either maximize total system throughput or minimize end-to-end workload latency. In this work, we propose HaX-CoNN, a novel scheme that characterizes and maps layers in concurrently executing DNN inference workloads to a diverse set of accelerators within a SoC. Our scheme uniquely takes per-layer execution characteristics, shared memory (SM) contention, and inter-accelerator transitions into account to find optimal schedules. We evaluate HaX-CoNN on NVIDIA Orin, NVIDIA Xavier, and Qualcomm Snapdragon 865 SoCs. Our experimental results indicate that HaX-CoNN minimiz
    
[^137]: Flows: 推理和协作人工智能的构建模块

    Flows: Building Blocks of Reasoning and Collaborating AI. (arXiv:2308.01285v1 [cs.AI])

    [http://arxiv.org/abs/2308.01285](http://arxiv.org/abs/2308.01285)

    Flows是一种系统化的方法，它通过将计算分解为自包含的构建模块，通过标准化的消息传递接口进行通信，实现了结构化的推理和协作人工智能。这种模块化设计使得Flows可以构建任意复杂度的交互，能够覆盖各种人工智能交互和工具增强的应用。

    

    最近人工智能领域取得的进展已经产生了高度能力和可控性的系统。这为结构化推理以及多个人工智能系统和人类之间的协作创造了前所未有的机遇。为了充分实现这一潜力，必须开发一种有原则的方法来设计和研究这样的结构化交互。为此，我们引入了流程的概念框架：一种系统化的建模复杂交互的方法。流程是计算的自包含构建模块，具有独立的状态，并通过标准化的基于消息的接口进行通信。这种模块化设计使得流程可以递归地组合成任意嵌套的交互，大大降低了复杂性。关键是，任何交互都可以使用这个框架来实现，包括之前关于人工智能-人工智能和人类-人工智能交互、提示工程方案和工具增强的工作。我们展示了流程在任务上的潜力。

    Recent advances in artificial intelligence (AI) have produced highly capable and controllable systems. This creates unprecedented opportunities for structured reasoning as well as collaboration among multiple AI systems and humans. To fully realize this potential, it is essential to develop a principled way of designing and studying such structured interactions. For this purpose, we introduce the conceptual framework of Flows: a systematic approach to modeling complex interactions. Flows are self-contained building blocks of computation, with an isolated state, communicating through a standardized message-based interface. This modular design allows Flows to be recursively composed into arbitrarily nested interactions, with a substantial reduction of complexity. Crucially, any interaction can be implemented using this framework, including prior work on AI--AI and human--AI interactions, prompt engineering schemes, and tool augmentation. We demonstrate the potential of Flows on the task 
    
[^138]: Select2Col: 利用语义信息的时空重要性进行高效的协作感知

    Select2Col: Leveraging Spatial-Temporal Importance of Semantic Information for Efficient Collaborative Perception. (arXiv:2307.16517v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.16517](http://arxiv.org/abs/2307.16517)

    Select2Col是一种利用语义信息的时空重要性进行高效协作感知的新框架。它通过轻量级图神经网络估计语义信息的重要性，从而选择有益的合作者并排除负面影响。同时，还提出了一种语义信息融合算法。

    

    通过利用共享的语义信息进行协作在克服孤立代理的感知能力限制方面起着关键作用。然而，现有的协作感知方法往往只关注语义信息的空间特征，而忽视了时间维度的重要性。因此，协作的潜在益处未得到充分利用。在本文中，我们提出了Select2Col，一种新颖的协作感知框架，它考虑了语义信息的时空重要性。在Select2Col中，我们开发了一种利用轻量级图神经网络（GNN）估计语义信息重要性（IoSI）以提高感知性能的合作者选择方法，从而识别出有益的合作者，同时排除那些带来负面影响的合作者。此外，我们提出了一种称为HPHA的语义信息融合算法，其中包括历史先验混合注意力机制。

    Collaboration by leveraging the shared semantic information plays a crucial role in overcoming the perception capability limitations of isolated agents. However, existing collaborative perception methods tend to focus solely on the spatial features of semantic information, while neglecting the importance of the temporal dimension. Consequently, the potential benefits of collaboration remain underutilized. In this article, we propose Select2Col, a novel collaborative perception framework that takes into account the {s}patial-t{e}mpora{l} importanc{e} of semanti{c} informa{t}ion. Within the Select2Col, we develop a collaborator selection method that utilizes a lightweight graph neural network (GNN) to estimate the importance of semantic information (IoSI) in enhancing perception performance, thereby identifying contributive collaborators while excluding those that bring negative impact. Moreover, we present a semantic information fusion algorithm called HPHA (historical prior hybrid atte
    
[^139]: 渐进傅里叶神经表示用于顺序视频编译

    Progressive Fourier Neural Representation for Sequential Video Compilation. (arXiv:2306.11305v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.11305](http://arxiv.org/abs/2306.11305)

    本研究提出了一种渐进傅里叶神经表示方法，通过在每个训练会话中找到自适应且紧凑的傅里叶空间子模块来编码顺序视频数据，克服了现有神经隐式表示方法在多个复杂数据上的泛化能力差的问题。

    

    最近神经隐式表示(NIR)因其将复杂和高维数据编码为表示空间并通过可训练的映射函数轻松重构数据的非凡能力而引起了极大关注。然而，NIR方法假定目标数据和表示模型之间存在一对一的映射，而不考虑数据的相关性或相似性。这导致在多组复杂数据上泛化能力较差，并限制了其效率和可伸缩性。受持续学习的启发，本研究探讨了如何在顺序编码会话中累积和传递多个复杂视频数据的神经隐式表示。为了克服NIR的局限性，我们提出了一种新的方法，即渐进傅里叶神经表示(PFNR)，旨在找到一个自适应和紧凑的傅里叶空间子模块，以编码每个训练会话中的视频。这种稀疏的神经编码使神经网络能够持有自由权重，实现了一种可迭代地编码和解码多个顺序视频数据的方式。

    Neural Implicit Representation (NIR) has recently gained significant attention due to its remarkable ability to encode complex and high-dimensional data into representation space and easily reconstruct it through a trainable mapping function. However, NIR methods assume a one-to-one mapping between the target data and representation models regardless of data relevancy or similarity. This results in poor generalization over multiple complex data and limits their efficiency and scalability. Motivated by continual learning, this work investigates how to accumulate and transfer neural implicit representations for multiple complex video data over sequential encoding sessions. To overcome the limitation of NIR, we propose a novel method, Progressive Fourier Neural Representation (PFNR), that aims to find an adaptive and compact sub-module in Fourier space to encode videos in each training session. This sparsified neural encoding allows the neural network to hold free weights, enabling an imp
    
[^140]: 高维和置换不变异常检测。

    High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])

    [http://arxiv.org/abs/2306.03933](http://arxiv.org/abs/2306.03933)

    该研究引入了一种置换不变的高维密度估计方法，通过学习后将其用于高能物理数据中的异常检测，能够有效地识别出在仅具备背景假设下排除异常的喷注。

    

    由于学习高维概率密度的困难，新物理过程的异常检测方法通常局限于低维空间。特别是在成分级别上，将置换不变性和可变长度的输入等良好性质合并到流行的密度估计方法中变得更加困难。在本研究中，我们引入了一种基于扩散模型的粒子物理数据置换不变密度估计器，专门设计用于处理可变长度的输入。我们通过将学习到的密度用作置换不变的异常检测评分来展示我们方法的功效，有效地识别出在仅具备背景假设下的可能性较低的喷注。为了验证我们的密度估计方法，我们研究了学习到的密度比与被监督分类算法获得的密度之间的比较。

    Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
    
[^141]: MERT:带有大规模自监督训练的声学音乐理解模型

    MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])

    [http://arxiv.org/abs/2306.00107](http://arxiv.org/abs/2306.00107)

    提出了一个带有大规模自监督训练的音乐理解模型MERT，利用了教师模型并采用了一种优于传统的语音和音频方法的组合方式。

    

    自监督学习（SSL）最近在视觉、文本和语音领域中已被证明是训练通用模型的一种很有前景的范例，对于跨越音乐领域的应用，尤其是对于调性和音高这样的特殊音乐知识的建模颇具挑战性。为了解决这一问题，我们提出了一个基于大规模自监督训练的声学音乐理解模型，即MERT。在我们的探索中，我们确定了更优秀的教师模型组合，这种组合方法在性能方面优于传统的语音和音频方法。

    Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
    
[^142]: 基于谱角度剖析生物数据中图神经网络的尺寸可泛化性：观点和实践

    Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective. (arXiv:2305.15611v1 [cs.LG])

    [http://arxiv.org/abs/2305.15611](http://arxiv.org/abs/2305.15611)

    本文通过谱角度的方法，研究了GNNs的尺寸可泛化性问题，并在真实生物数据集上进行了实验，发现GNNs在度分布和谱分布偏移时均表现敏感，在同一数据集的大图上的性能仍然下降，揭示了 GNNs的尺寸可泛化性问题。

    

    本文探讨了图神经网络 (GNNs) 是否具有从小图中学习的知识可推广到同一领域的大图中。之前的研究表明，不同大小的图之间的分布偏移，尤其是度分布，可能会导致图分类任务的性能下降。然而，在生物数据集中，度数是有界的，因此度分布的偏移很小。即使度分布偏移很小，我们观察到GNNs在同一数据集的大图上的性能仍然下降，暗示有其他原因。事实上，以往对于真实数据集中各种图尺寸引起的分布偏移类型和属性的探索不足。此外，以前的尺寸可泛化性分析大多集中在空间领域。为填补这些空白，我们采用谱角度去研究GNNs在生物图数据上的尺寸可泛化性。我们首先提出一个新框架来模拟各种类型的度分布偏移，并利用它来测试GNNs 在真实生物数据集上的尺寸可泛化性。我们的实验表明，除了度分布偏移外，GNNs 还对图大小变化引起的谱分布偏移很敏感。我们进一步分析了不同的GNN模型的影响，并表明，一些模型比其他模型更具有尺寸泛化性。本文展示了关于GNNs尺寸可泛化性问题的新观点和实践，并为该领域的未来研究提供了有益的洞察和建议。

    We investigate the question of whether the knowledge learned by graph neural networks (GNNs) from small graphs is generalizable to large graphs in the same domain. Prior works suggest that the distribution shift, particularly in the degree distribution, between graphs of different sizes can lead to performance degradation in the graph classification task. However, this may not be the case for biological datasets where the degrees are bounded and the distribution shift of degrees is small. Even with little degree distribution shift, our observations show that GNNs' performance on larger graphs from the same datasets still degrades, suggesting other causes. In fact, there has been a lack of exploration in real datasets to understand the types and properties of distribution shifts caused by various graph sizes. Furthermore, previous analyses of size generalizability mostly focus on the spatial domain.  To fill these gaps, we take the spectral perspective and study the size generalizabilit
    
[^143]: 通过摘要二元性和显式大纲控制增强生成

    Enhancing Generation through Summarization Duality and Explicit Outline Control. (arXiv:2305.14459v1 [cs.CL])

    [http://arxiv.org/abs/2305.14459](http://arxiv.org/abs/2305.14459)

    本文提出了一个两阶段的摘要增强的大纲监督生成框架，能够更好地生成明确和合理的大纲，并引入了一个新颖的显式大纲控制方法以更有效地利用生成的大纲。

    

    自动开放式长文本生成面临语义不连贯和情节不可信的重大挑战。先前的工作通常通过设计无监督任务中的短语或抽象信号的大纲来缓解此问题，但这往往是不稳定且难以解释的。在假设摘要作为已成熟的大纲的情况下，我们介绍了一个两阶段、摘要增强的大纲监督生成框架。该框架利用摘要任务的双重特征来改进大纲预测，从而产生更明确和合理的大纲。此外，我们发现基于大纲的生成具有未充分利用的问题，无论是标准的预训练语言模型（例如GPT-2、BART）还是大型语言模型（例如Vicuna、ChatGPT）。为了解决这个问题，我们提出了一种新颖的显式大纲控制方法，以更有效地利用生成的大纲。

    Automatically open-ended long text generation poses significant challenges due to semantic incoherence and plot implausibility. Previous works usually alleviate this problem through outlines in the form of short phrases or abstractive signals by designing unsupervised tasks, which tend to be unstable and weakly interpretable.  Assuming that a summary serves as a mature outline, we introduce a two-stage, summary-enhanced outline supervised generation framework. This framework leverages the dual characteristics of the summarization task to improve outline prediction, resulting in more explicit and plausible outlines. Furthermore, we identify an underutilization issue in outline-based generation with both standard pretrained language models (e.g., GPT-2, BART) and large language models (e.g., Vicuna, ChatGPT). To address this, we propose a novel explicit outline control method for more effective utilization of generated outlines.
    
[^144]: ChatGPT在软件工程中的应用范围：全面探究

    The Scope of ChatGPT in Software Engineering: A Thorough Investigation. (arXiv:2305.12138v1 [cs.SE])

    [http://arxiv.org/abs/2305.12138](http://arxiv.org/abs/2305.12138)

    ChatGPT在软件工程中有巨大潜力，但缺乏可解释性成为问题。通过研究ChatGPT的能力和局限性，发现其在理解代码语法方面表现出色，但对于语义的理解和部分功能存在问题。

    

    ChatGPT展示了在软件工程中转化的巨大潜力，表现出在代码和文档生成等任务中的卓越性能。然而，软件工程需要高可靠性和风险控制，使ChatGPT的缺乏可解释性成为一个问题。为了解决这个问题，我们进行了一项研究，评估了ChatGPT在软件工程中的能力和局限性。我们将AI模型应对SE任务所需的能力分为三类：1）语法理解，2）静态行为理解，和3）动态行为理解。我们的调查重点是ChatGPT理解代码语法和语义结构的能力，包括抽象语法树（AST）、控制流程图（CFG）和调用图（CG）。我们评估了ChatGPT在涉及C、Java、Python和Solidity的跨语言任务中的表现。我们的发现表明，虽然ChatGPT表现出了对理解代码语法（AST）的出色能力，但在理解代码语义和部分功能上存在问题。

    ChatGPT demonstrates immense potential to transform software engineering (SE) by exhibiting outstanding performance in tasks such as code and document generation. However, the high reliability and risk control requirements of SE make the lack of interpretability for ChatGPT a concern. To address this issue, we carried out a study evaluating ChatGPT's capabilities and limitations in SE. We broke down the abilities needed for AI models to tackle SE tasks into three categories: 1) syntax understanding, 2) static behavior understanding, and 3) dynamic behavior understanding. Our investigation focused on ChatGPT's ability to comprehend code syntax and semantic structures, including abstract syntax trees (AST), control flow graphs (CFG), and call graphs (CG). We assessed ChatGPT's performance on cross-language tasks involving C, Java, Python, and Solidity. Our findings revealed that while ChatGPT excels at understanding code syntax (AST), it struggles with comprehending code semantics, parti
    
[^145]: 基于核凸包机的差分隐私学习研究

    Kernel Affine Hull Machines for Differentially Private Learning. (arXiv:2304.01300v1 [cs.LG])

    [http://arxiv.org/abs/2304.01300](http://arxiv.org/abs/2304.01300)

    本文提出了一种基于核凸包机的方法来确保数据隐私保护，同时保留数据结构，用于数据表示学习的分类应用中。为了确保隐私保护学习，还提出了一种新颖的生成虚假数据的方法。

    

    本文探讨了通过学习再生核希尔伯特空间中的点的凸包来表示数据的方法，旨在将数据空间划分为几何体，从而隐藏有关单个数据点的隐私信息，同时保留原始学习问题的结构。为此，我们引入了核凸包机（KAHM），它提供了一种有效的方法来计算从结果有界几何体中的距离度量。KAHM是广泛和深入的自编码器的关键构建块，它们使数据表示学习用于分类应用。为了确保隐私保护学习，我们提出了一种新颖的生成虚假数据的方法，该方法涉及将差分隐私数据样本通过转换过程进行平滑处理。生成的虚假数据不仅保证差分隐私，而且确保KAHM建模误差不大于原始数据误差。

    This paper explores the use of affine hulls of points as a means of representing data via learning in Reproducing Kernel Hilbert Spaces (RKHS), with the goal of partitioning the data space into geometric bodies that conceal privacy-sensitive information about individual data points, while preserving the structure of the original learning problem. To this end, we introduce the Kernel Affine Hull Machine (KAHM), which provides an effective way of computing a distance measure from the resulting bounded geometric body. KAHM is a critical building block in wide and deep autoencoders, which enable data representation learning for classification applications. To ensure privacy-preserving learning, we propose a novel method for generating fabricated data, which involves smoothing differentially private data samples through a transformation process. The resulting fabricated data guarantees not only differential privacy but also ensures that the KAHM modeling error is not larger than that of the
    
[^146]: 连续蒙特卡洛图搜索

    Continuous Monte Carlo Graph Search. (arXiv:2210.01426v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.01426](http://arxiv.org/abs/2210.01426)

    连续蒙特卡洛图搜索（CMCGS）是一种新颖的蒙特卡洛树搜索（MCTS）的扩展，适用于具有连续状态和动作空间的在线规划环境。CMCGS通过将相似状态聚类，并共享相同的动作策略，实现了高性能的在线规划。

    

    在许多复杂的连续决策任务中，在线规划对于高性能至关重要。为了实现高效的在线规划，蒙特卡洛树搜索（MCTS）采用了一个有原则的机制来权衡探索和利用。MCTS在许多离散决策领域（如围棋、国际象棋和将棋）中胜过了其他方法。而针对连续领域的MCTS扩展也已提出。然而，由于固有的高分支因子和导致搜索树大小爆炸的问题，现有方法受到了限制。为了解决这个问题，我们提出了连续蒙特卡洛图搜索（CMCGS），这是一种新颖的MCTS扩展，适用于具有连续状态和动作空间的在线规划环境。CMCGS利用了一个洞察力，在规划过程中，将相似状态之间共享相同的动作策略可以得到高性能。为了实现这个想法，CMCGS在每个时间步骤中将相似状态聚类成有限数量的随机动作赌博节点，这些节点共享相同的动作策略。

    In many complex sequential decision-making tasks, online planning is crucial for high performance. For efficient online planning, Monte Carlo Tree Search (MCTS) employs a principled mechanism for trading off exploration for exploitation. MCTS outperforms comparison methods in many discrete decision-making domains such as Go, Chess, and Shogi. Following, extensions of MCTS to continuous domains have been proposed. However, the inherent high branching factor and the resulting explosion of search tree size are limiting existing methods. To address this problem, we propose Continuous Monte Carlo Graph Search (CMCGS), a novel extension of MCTS to online planning in environments with continuous state and action spaces. CMCGS takes advantage of the insight that, during planning, sharing the same action policy between several states can yield high performance. To implement this idea, at each time step, CMCGS clusters similar states into a limited number of stochastic action bandit nodes, which
    
[^147]: EDO-Net: 从图动力学中学习可变形物体的弹性属性

    EDO-Net: Learning Elastic Properties of Deformable Objects from Graph Dynamics. (arXiv:2209.08996v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.08996](http://arxiv.org/abs/2209.08996)

    EDO-Net是一个学习可变形物体弹性属性的图动力学模型，通过利用可提取的潜在表示，可以推广到未知的物理属性，实现对类似布料的对象未来状态的预测和转移学习。

    

    我们研究了学习可变形物体的图动力学的问题，该问题可以推广到未知的物理属性。我们的关键洞察力是利用可提取的类似布料的可变形物体的弹性物理属性的潜在表示，例如从拉伸交互中提取。在本文中，我们提出了EDO-Net（弹性可变形对象-Net），这是一个在具有不同弹性属性的大量样本上进行训练的图动力学模型，不依赖于属性的真实标签。EDO-Net共同学习了一个自适应模块和一个前向动力学模块。前者负责提取对象的物理特性的潜在表示，而后者利用潜在表示来预测以图形表示的类似布料的对象的未来状态。我们在仿真和真实世界中评估了EDO-Net的能力：1）推广到未知的物理属性，2）转移学习所学到的表示

    We study the problem of learning graph dynamics of deformable objects that generalizes to unknown physical properties. Our key insight is to leverage a latent representation of elastic physical properties of cloth-like deformable objects that can be extracted, for example, from a pulling interaction. In this paper we propose EDO-Net (Elastic Deformable Object - Net), a model of graph dynamics trained on a large variety of samples with different elastic properties that does not rely on ground-truth labels of the properties. EDO-Net jointly learns an adaptation module, and a forward-dynamics module. The former is responsible for extracting a latent representation of the physical properties of the object, while the latter leverages the latent representation to predict future states of cloth-like objects represented as graphs. We evaluate EDO-Net both in simulation and real world, assessing its capabilities of: 1) generalizing to unknown physical properties, 2) transferring the learned rep
    

