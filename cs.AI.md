# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning.](http://arxiv.org/abs/2310.03731) | 本文介绍了一种方法，通过微调开源语言模型，使其能够使用代码进行数学建模和推导，从而增强数学推理能力。作者提出了一种生成包含数学问题和基于代码的解决方案的数据集，并引入了定制的微调和推理方法，从而实现了在解决具有挑战性的数学问题上生成基于代码的解决方案的 MathCoder 模型。 |
| [^2] | [Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning.](http://arxiv.org/abs/2310.03718) | 这个论文提出了一种约束条件下的策略优化框架，用于训练多功能安全强化学习智能体。通过引入多功能值估计和有条件的变分推理模块，该框架在训练效率和零-shot适应能力方面表现优于基准方法。 |
| [^3] | [Artificial Intelligence Index Report 2023.](http://arxiv.org/abs/2310.03715) | 本篇论文介绍了AI指数报告2023年版，包括了更多的原始数据和分析内容，旨在为广大读者提供准确、全面的关于人工智能的数据和见解。 |
| [^4] | [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines.](http://arxiv.org/abs/2310.03714) | DSPy是一个编程模型，将LM流水线抽象为文本转换图，通过声明性模块调用LM实现优化，能够解决复杂的推理问题和数学问题等任务。 |
| [^5] | [Agent Instructs Large Language Models to be General Zero-Shot Reasoners.](http://arxiv.org/abs/2310.03710) | 该论文提出了一种方法，通过代理指导的方式，大大提高了大型语言模型在零-shot推理任务上的能力，并在多个数据集上实现了最先进的性能。 |
| [^6] | [Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization.](http://arxiv.org/abs/2310.03708) | 本文提出了一种无强化学习的算法，称为多目标直接偏好优化（MODPO），它可以根据不同的偏好训练不同的语言模型，通过组合所有目标和特定权重来优化模型。 |
| [^7] | [Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!.](http://arxiv.org/abs/2310.03693) | 微调对齐语言模型会牺牲安全性，即使用户没有恶意意图。通过敌对设计的训练样本，即使只有少数10个，也可以破坏语言模型的安全对齐。这种安全风险存在于微调后的模型中。 |
| [^8] | [Probabilistic Generative Modeling for Procedural Roundabout Generation for Developing Countries.](http://arxiv.org/abs/2310.03687) | 本文提出了一种概率生成建模方法，用于发展中国家的程序化环形道路生成问题。通过使用生成性流网络作为道路生成器，我们实现了更好的多样性和高效性能。 |
| [^9] | [SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks.](http://arxiv.org/abs/2310.03684) | SmoothLLM是第一个用于减轻大型语言模型上越狱攻击的算法，通过在输入提示上随机扰动并汇总预测结果来检测对抗性输入，将攻击成功率降低至不到一个百分点，并提供了可证明的保证。 |
| [^10] | [MapperGPT: Large Language Models for Linking and Mapping Entities.](http://arxiv.org/abs/2310.03666) | MapperGPT是一个基于大型语言模型的实体链接和映射工具，能够解决实体映射中的词汇模糊问题和手动映射细化的困扰。 |
| [^11] | [Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures.](http://arxiv.org/abs/2310.03659) | 这篇论文提出了一个多维分类法，用于平衡自治LLM驱动的多智能体架构中的自主性和一致性。该架构通过将目标分解为可管理的任务和协调智能体合作来实现目标，以提高人工智能能力。 |
| [^12] | [CLEVRER-Humans: Describing Physical and Causal Events the Human Way.](http://arxiv.org/abs/2310.03635) | CLEVRER-Humans是一个用于因果判断的视频推理数据集，通过人工标注来解决合成事件和合成语言描述的缺乏多样性问题，并通过迭代事件填空和神经语言生成模型提高数据收集效率。 |
| [^13] | [PeaTMOSS: Mining Pre-Trained Models in Open-Source Software.](http://arxiv.org/abs/2310.03620) | PeaTMOSS数据集提供了一个研究开源软件中预训练模型软件工程行为和挑战的平台，包括大量的PTMs快照和使用PTMs的开源软件存储库。 |
| [^14] | [Solving a Class of Non-Convex Minimax Optimization in Federated Learning.](http://arxiv.org/abs/2310.03613) | 本研究针对联邦学习中的一类非凸最小极大优化问题，提出了FL算法（FedSGDA+和FedSGDA-M），并在最常见的最小极大问题中降低了复杂度。针对非凸凹问题，提出的FedSGDA+算法将通信复杂度降低到O(ε^{-6})。在非凸强凹和非凸PL最小极大设置下，证明了FedSGDA-M具有已知的样本复杂度。 |
| [^15] | [FASER: Binary Code Similarity Search through the use of Intermediate Representations.](http://arxiv.org/abs/2310.03605) | 本论文提出了一种名为FASER的方法，通过使用中间表示进行二进制代码相似性搜索。该方法可以跨架构地识别函数，并明确编码函数的语义，以支持各种应用场景。 |
| [^16] | [Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-to-End.](http://arxiv.org/abs/2310.03581) | 本文提出了一种基于强化学习的本地导航策略，通过将感知失效建模为障碍物和坑洞，从受损的感知中重建环境信息并进行端到端的反应，实现了在受损感知下的可靠导航。 |
| [^17] | [Causal Inference in Gene Regulatory Networks with GFlowNet: Towards Scalability in Large Systems.](http://arxiv.org/abs/2310.03579) | 本文介绍了一个新的框架Swift-DynGFN，它提高了基因调控网络中因果结构的学习能力并解决了可扩展性问题。通过利用基因的独立性来增强并行性和降低计算成本，实验结果展示了在GRNs中学习因果结构和处理大规模系统方面的进展。 |
| [^18] | [Towards Robust and Generalizable Training: An Empirical Study of Noisy Slot Filling for Input Perturbations.](http://arxiv.org/abs/2310.03518) | 该论文研究了对于输入噪声的鲁棒和可推广的训练方法，在嘈杂的槽填充任务中进行了实证研究，并提出了一个噪声鲁棒性评估数据集和框架，通过实证实验验证了该框架的有效性。 |
| [^19] | [How the level sampling process impacts zero-shot generalisation in deep reinforcement learning.](http://arxiv.org/abs/2310.03494) | 这项研究探讨了非均匀采样策略对深度强化学习代理的零样本泛化能力的影响，通过测量代理的内部表示与训练层级集之间的相互信息，发现基于值损失优先级的自适应采样策略能更好地避免过拟合。 |
| [^20] | [Tik-to-Tok: Translating Language Models One Token at a Time: An Embedding Initialization Strategy for Efficient Language Adaptation.](http://arxiv.org/abs/2310.03477) | 本研究提出了一种新颖的模型转换策略，通过将目标语言标记映射到语义相似的源语言标记，有效地改善了低资源和中资源语言训练单语言模型时的初始化过程，并在荷兰语和弗里斯兰语等多种语言上取得了新的最先进性能。 |
| [^21] | [A Quantitatively Interpretable Model for Alzheimer's Disease Prediction Using Deep Counterfactuals.](http://arxiv.org/abs/2310.03457) | 本研究提出了一种使用深层反事实的定量可解释模型，用于预测阿尔茨海默病。通过合成反事实标记的结构性MRIs，并转化为灰质密度图来测量体积变化，实现了相当的预测性能，并促进了定量解释。 |
| [^22] | [Pre-Training and Fine-Tuning Generative Flow Networks.](http://arxiv.org/abs/2310.03419) | 这个论文提出了一种新的方法来实现生成性流网络的无奖励预训练，并通过自监督问题的形式训练了一个条件的GFlowNet（OC-GFN），用于有效适应下游任务。 |
| [^23] | [GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks.](http://arxiv.org/abs/2310.03399) | GRAPES是一种自适应图采样方法，通过学习识别在训练图神经网络分类器时具有影响力的节点集合，解决了可扩展图神经网络中的内存问题，并且在准确性和可扩展性方面表现出色。 |
| [^24] | [Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review.](http://arxiv.org/abs/2310.03392) | 在安全关键行业中确保高质量的人工智能与人类互动（HAII）十分重要，然而目前对于HAII的研究零散且缺乏一致性。本论文通过系统文献综述，发现没有一个一致使用的术语描述HAII，并且五个因素影响HAII，即用户特征和背景、AI界面和功能、用户信任、监控和反馈，以及工作上的团队结构。这些发现为改进安全关键行业中的HAII提供了指导。 |
| [^25] | [Procedural Text Mining with Large Language Models.](http://arxiv.org/abs/2310.03376) | 本文研究了使用大型语言模型进行程序性文本挖掘的方法。通过在零样本和上下文学习环境中使用GPT-4模型和自定义技术，有效地从非结构化PDF文本中提取程序。实验结果证明了该方法的潜力和价值。 |
| [^26] | [Design Optimizer for Planar Soft-Growing Robot Manipulators.](http://arxiv.org/abs/2310.03374) | 本文提出了一种设计优化方法，通过优化软增长机器人的运动链，提供最佳机器人尺寸以解决特定任务。这种方法利用基于群体的优化算法，避免了不必要的材料和资源浪费。 |
| [^27] | [Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video Sequences Using Swin Transformer-Enhanced UNet.](http://arxiv.org/abs/2310.03365) | Swin-Tempo是一个创新模型，使用了Swin Transformer-Enhanced UNet将CT扫描中的肺结节检测作为视频序列进行时间感知。它克服了现有网络的计算复杂性问题，提高了肺结节检测的准确率。 |
| [^28] | [Robust Representation Learning via Asymmetric Negative Contrast and Reverse Attention.](http://arxiv.org/abs/2310.03358) | 本文提出了一个通用的对抗训练（AT）框架，通过非对称负对比度和反向注意力，学习鲁棒的特征表征，以提高神经网络的对抗鲁棒性能。 |
| [^29] | [Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed Cooperative-Competitive Games.](http://arxiv.org/abs/2310.03354) | 该论文介绍了自我对弈和策略空间响应预测（PSRO）作为解决竞争游戏的强化学习框架，发现自我对弈方法在混合合作竞争游戏中无法收敛到全局纳什均衡（NE），而PSRO能够在这种情况下有效地学习到最佳响应。然而，PSRO需要重复训练联合策略，增加了难度。 |
| [^30] | [Deep Geometric Learning with Monotonicity Constraints for Alzheimer's Disease Progression.](http://arxiv.org/abs/2310.03353) | 本文提出了一种采用单调性约束的深度几何学习方法来预测阿尔茨海默病（AD）的进展。这种方法通过结合递归神经网络和普通微分方程在黎曼空间中建模时间序列数据，弥补了现有方法对数据几何属性考虑不足的问题，并解决了从不完整样本中外推正定对称度量的限制。这项工作在临床诊断和治疗中具有重要的应用价值。 |
| [^31] | [Tractable Bounding of Counterfactual Queries by Knowledge Compilation.](http://arxiv.org/abs/2310.03352) | 这项研究讨论了在皮尔斯结构性因果模型中限制部分可识别查询的问题，并提出了一种基于知识编译的方法，通过符号参数替换和并行化技术加速界限计算，在实验中取得了明显的计算优势。 |
| [^32] | [Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning.](http://arxiv.org/abs/2310.03325) | 本文提出了一种面向视觉规划的可解释和可推广的框架，通过将视觉输入转化为概念表示、符号抽象和推理以及将视觉因果转换与真实世界行为关联，实现了目标条件的视觉规划。 |
| [^33] | [Enhanced Human-Robot Collaboration using Constrained Probabilistic Human-Motion Prediction.](http://arxiv.org/abs/2310.03314) | 本研究提出一种新颖的人体运动预测框架，结合人体关节约束和场景约束，利用高斯过程回归模型预测一定时间范围内的人体动作，以增强人机协作。 |
| [^34] | [Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning.](http://arxiv.org/abs/2310.03309) | 利用大型语言模型进行演绎推理是一个具有挑战性的问题。这篇论文提出了一个简明有序的方法，将任务分解为子任务并且人类化地组织思维，以提高演绎推理的效果。 |
| [^35] | [Benchmarking Large Language Models As AI Research Agents.](http://arxiv.org/abs/2310.03302) | 本研究提出了MLAgentBench，一个用于对AI研究代理进行基准测试的ML任务套件，代理可以执行各种操作，从而运行实验、分析结果并修改整个机器学习流程的代码。这可以帮助我们构建和评估能够执行长期目标任务的AI研究代理。 |
| [^36] | [LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers.](http://arxiv.org/abs/2310.03294) | LightSeq是用于长上下文转换器分布式训练的一种新方法，它通过序列维度进行分区，与不同注意力头数量的模型架构兼容，并且减少了与Megatron-LM相比的通信量，同时还实现了通信和计算的重叠。 |
| [^37] | [SoK: Access Control Policy Generation from High-level Natural Language Requirements.](http://arxiv.org/abs/2310.03292) | 该论文分析了49篇文章，并总结了现有图形化策略配置工具和自动策略生成框架的局限性。研究的目的是开发更有效的访问控制策略生成解决方案，避免访问控制失败。 |
| [^38] | [A 5' UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions.](http://arxiv.org/abs/2310.03281) | 这种研究引入了一种新的语言模型UTR-LM，通过对多个物种的5' UTR进行预训练，并结合有监督信息，该模型在多个下游任务中的表现超过了现有的最佳模型，可以有效预测平均核糖体负载、翻译效率、mRNA表达水平，并改进了内源性核糖体进入位点的识别性能。 |
| [^39] | [Network Alignment with Transferable Graph Autoencoders.](http://arxiv.org/abs/2310.03272) | 该论文提出了一种基于图自编码器的网络对齐方法，通过生成与图的特征值和特征向量相关的节点嵌入，实现了更准确的对齐。同时，该方法还利用迁移学习和数据增强技术，在大规模网络对齐任务中实现高效的对齐，无需重新训练。 |
| [^40] | [Sparse Deep Learning for Time Series Data: Theory and Applications.](http://arxiv.org/abs/2310.03243) | 本文研究了稀疏深度学习在依赖数据（如时间序列数据）上的理论和应用。通过研究，我们发现稀疏循环神经网络能够一致地估计，并对其预测进行正确的不确定性量化。数值实验结果显示，稀疏深度学习在预测不确定性方面优于最先进方法。 |
| [^41] | [Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization.](http://arxiv.org/abs/2310.03234) | 本文研究了一种新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)，通过扩展已有的研究，我们研究了非光滑弱凸FCCO的问题，并提出了一种单循环算法来找到Moreau环的ε-稳定点。 |
| [^42] | [Deep Representations of First-person Pronouns for Prediction of Depression Symptom Severity.](http://arxiv.org/abs/2310.03232) | 本研究利用上下文化的语言表征模型中获得的第一人称代词嵌入，分析抑郁症状的严重程度。结果显示，相比标准分类令牌嵌入和基于频率的代词分析，上下文化的第一人称代词嵌入在预测抑郁症状严重程度方面具有优势。 |
| [^43] | [Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms.](http://arxiv.org/abs/2310.03225) | 本文提出了一种广义的安全探索问题，即GSE，并提出了一个元算法MASE来解决这个问题。MASE将无约束的强化学习算法与不确定性量化器结合，以保证安全性，并在违反安全约束之前惩罚不安全的探索。这种方法的优势是在保证安全性的同时进行策略优化，并具有高概率的安全保证。 |
| [^44] | [Learning Energy-Based Prior Model with Diffusion-Amortized MCMC.](http://arxiv.org/abs/2310.03218) | 本文介绍了一种基于扩散改进的长期 MCMC采样的学习算法，用于学习能量先验模型。实验证明了该算法的有效性。 |
| [^45] | [On the Performance of Multimodal Language Models.](http://arxiv.org/abs/2310.03211) | 本研究对不同多模态指导调优方法进行比较分析，并评估其在复杂推理、对话、图像描述等任务中的性能。通过基准测试和消融实验，为将多模态能力融入语言模型提供了关键见解。 |
| [^46] | [A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization.](http://arxiv.org/abs/2310.03205) | 我们提出了NeuFace方法，通过神经再参数化优化，在大规模的人脸视频上实现了准确和一致的人脸网格标注。利用我们的数据集，在3D人脸相关任务中，我们展示了该数据集的有用性，并且能够改善现有的3D人脸重建模型的准确性和学习3D面部运动先验。 |
| [^47] | [Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions.](http://arxiv.org/abs/2310.03195) | 本文对基于深度强化学习(DRL)的机器调度方法进行了全面回顾和比较，总结出它们的方法论、应用、优势和局限性。通过对比分析，发现DRL方法优于传统方法。 |
| [^48] | [Talking Models: Distill Pre-trained Knowledge to Downstream Models via Interactive Communication.](http://arxiv.org/abs/2310.03188) | 通过交互式沟通将预训练的知识提取到下游模型中的对话模型，解决了强大教师无法保证强大学生的问题，同时利用交互式沟通提高了知识蒸馏在提高下游任务性能方面的优化。 |
| [^49] | [Inferring Inference.](http://arxiv.org/abs/2310.03186) | 大脑具有一系列重复的规范计算单元，但神经表示是分布式的，因此如何定义规范分布式计算仍然是一个挑战。本文提出了一个数学框架，从大规模神经活动模式中推断出规范分布式计算。在算法级别上，提出了一个基于图结构模型的非线性信息传递算法。通过感知推理任务中的神经活动时间序列，可以找到神经活动与感知推理任务中的潜在因果之间的映射关系。 |
| [^50] | [Misusing Tools in Large Language Models With Visual Adversarial Examples.](http://arxiv.org/abs/2310.03185) | 本研究展示了如何利用视觉对抗性样本来引导大型语言模型（LLMs）执行攻击者想要的工具使用。攻击可以导致LLM删除日历事件、泄露私人对话和预订酒店，并且具有隐蔽性和适用性广的特点。同时，这些攻击几乎总是可以操纵LLM以遵循真实世界的语法来调用工具，同时保持与干净图像的高相似性。 |
| [^51] | [Neural architecture impact on identifying temporally extended Reinforcement Learning tasks.](http://arxiv.org/abs/2310.03161) | 本研究提出了基于注意力的神经架构，在强化学习领域取得了良好的性能。通过将注意力图叠加到图像上，可以直接观察到代理所使用的信息，并更容易解释选择动作背后的逻辑。 |
| [^52] | [Assessment of Prediction Intervals Using Uncertainty Characteristics Curves.](http://arxiv.org/abs/2310.03158) | 本论文提出了一种新颖的评估预测区间的方法，利用了操作特征曲线和相对于空白参考的收益概念。这种方法广泛适用于不同的研究场景，解决了当前对预测区间全面评估的需求。 |
| [^53] | [Attributing Learned Concepts in Neural Networks to Training Data.](http://arxiv.org/abs/2310.03149) | 通过将数据归因方法与概念探测方法相结合，研究了神经网络中学习到的概念与训练数据的关系，并发现概念的位置和稀疏性并不完全依赖于少量特定示例。 |
| [^54] | [Axiomatic Aggregations of Abductive Explanations.](http://arxiv.org/abs/2310.03131) | 本文提出了通过将多种可能的归纳解释汇总为特征重要性分数的三种聚合方法，以解决存在多个有效的归纳解释的问题。 |
| [^55] | [Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models.](http://arxiv.org/abs/2310.03123) | 这项研究提出了一种高效的黑盒大型预训练模型联合提示调整（Fed-BBPT）方法，通过摒弃对参数结构和私有数据集访问的依赖，可以在处理内存限制和保持隐私性的同时充分利用每个局部数据集。 |
| [^56] | [Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning.](http://arxiv.org/abs/2310.03094) | 本研究提出了一种基于思维混合表示的大规模语言模型级联方法，用于成本高效的推理。通过考虑更弱模型的答案一致性作为问题难度的信号，可以实现对问题的决策，从而节约使用更强模型的成本。 |
| [^57] | [Deep Learning in Computational Biology: Advancements, Challenges, and Future Outlook.](http://arxiv.org/abs/2310.03086) | 深度学习在计算生物学中的应用带来了重大变革，既能够改善DNA序列分析和蛋白质结构预测，也为基因组变异检测和基因表达分析提供了新的方法。 |
| [^58] | [Discovering Knowledge-Critical Subnetworks in Pretrained Language Models.](http://arxiv.org/abs/2310.03084) | 本研究调查了预训练语言模型中是否存在各种关键知识子网络，即负责编码特定知识的稀疏计算子图。通过提出的可微分权重屏蔽方案，我们可以精确地删除特定知识，又最小化对原始语言模型的负面影响。 |
| [^59] | [Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models.](http://arxiv.org/abs/2310.03059) | Point-PEFT是一种用于3D预训练模型的参数高效微调框架，它通过冻结大部分参数，只微调新增的PEFT模块，包括Point-prior Prompt和Geometry-aware Adapter，以最小化学习参数，并利用内存库和准确的聚合方法来提高模型性能。 |
| [^60] | [Modified LAB Algorithm with Clustering-based Search Space Reduction Method for solving Engineering Design Problems.](http://arxiv.org/abs/2310.03055) | 本文介绍了一种修改后的LAB算法，并提出了基于聚类的搜索空间缩减方法，该算法在工程设计问题求解中表现出改进的稳健性和搜索空间探索能力，同时能够解决约束问题。 |
| [^61] | [Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing.](http://arxiv.org/abs/2310.03052) | Memoria 是一个通用记忆网络，应用海比安理论来增强神经网络中的长期依赖。通过存储和检索信息，并使用根据海布规则变化的连接权重，Memoria 在诸如 BERT 和 GPT 之类的流行 Transformer 模型上显著改进了考虑长期依赖的能力。 |
| [^62] | [How FaR Are Large Language Models From Agents with Theory-of-Mind?.](http://arxiv.org/abs/2310.03051) | 本论文提出了一种评估大型语言模型的新范式：思考为了行动（T4D）。实验证明，虽然这些模型擅长跟踪角色的信念，但在将这些推断转化为战略行动上存在困难。核心挑战在于识别隐含的关于心理状态的推断，而不是明确询问，这会导致正确的行动选择困难。 |
| [^63] | [EcoAssistant: Using LLM Assistant More Affordably and Accurately.](http://arxiv.org/abs/2310.03046) | EcoAssistant是一个框架，旨在使LLM助手更经济、更准确地回答代码驱动的查询，并且包含了与自动代码执行器对话和使用层次结构的LLM助手的功能。 |
| [^64] | [A Deep Reinforcement Learning Approach for Interactive Search with Sentence-level Feedback.](http://arxiv.org/abs/2310.03043) | 本研究提出了一种利用深度强化学习的交互式搜索方法，该方法通过整合句级反馈信息来提高搜索准确性。通过适应最新的BERT-based模型进行关键句子选择和项目排序，可以获得更满意的搜索结果。 |
| [^65] | [A quantum system control method based on enhanced reinforcement learning.](http://arxiv.org/abs/2310.03036) | 提出了一种基于增强强化学习的量子系统控制方法，通过映射状态和动作到量子系统中，利用增强型神经网络实现了长期累积奖励的最大化，从初始态准确演化到目标态，并在仿真实验中取得了接近1的保真度。 |
| [^66] | [Benchmarking Local Robustness of High-Accuracy Binary Neural Networks for Enhanced Traffic Sign Recognition.](http://arxiv.org/abs/2310.03033) | 这项研究基于二进制神经网络模型，通过引入一组基准问题来评估交通标志识别模型的本地稳健性。这些问题考虑了网络参数的数量、输入维度和区域数量，并挑战了最先进的验证工具。 |
| [^67] | [How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses.](http://arxiv.org/abs/2310.03031) | 本文系统地分析并探索了德语和英语的ChatGPT回应中可能存在的问题，特别关注了性别偏见。我们发现，在对系统多次提供相同指令的情况下，回应存在差异。使用ChatGPT来帮助非IT用户撰写工作文本非常有用，但用户需要充分考虑系统的固有限制。 |
| [^68] | [SAF: Smart Aggregation Framework for Revealing Atoms Importance Rank and Improving Prediction Rates in Drug Discovery.](http://arxiv.org/abs/2310.03028) | SAF框架提出了一种新的聚合方法，使用温度超参数对原子加权，并证明其可以提高信息传递神经网络的预测能力，在药物发现中揭示重要的原子。 |
| [^69] | [Synergistic Fusion of Graph and Transformer Features for Enhanced Molecular Property Prediction.](http://arxiv.org/abs/2310.03027) | 本论文提出了一种名为SYNFUSION的新方法，通过协同融合GNN和Transformer的预训练特征，实现了全面的分子表示，在分子性质预测任务中超越了以往的模型。 |
| [^70] | [Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection.](http://arxiv.org/abs/2310.02861) | 《Rayleigh Quotient Graph Neural Networks用于图级异常检测的研究》提出使用Rayleigh Quotient作为驱动因素，通过探索图的固有光谱特征来实现图级异常检测。 |
| [^71] | [MagicDrive: Street View Generation with Diverse 3D Geometry Control.](http://arxiv.org/abs/2310.02601) | MagicDrive是一个新颖的街景生成框架，通过提供多样化的三维几何控制，包括相机姿态、道路地图和三维边界框，以及文本描述，实现了高保真度的街景合成，并捕捉了细致的三维几何信息。 |
| [^72] | [On the definition of toxicity in NLP.](http://arxiv.org/abs/2310.02357) | 这项研究探讨了毒性的定义模糊性问题，并提出了一种基于定量压力的毒性定义来弥补现有定义的缺点。 |
| [^73] | [Prompting Audios Using Acoustic Properties For Emotion Representation.](http://arxiv.org/abs/2310.02298) | 本研究提出使用声学特性生成音频提示，并通过训练模型来更好地学习情感表达。实验结果表明，声学提示显著提高了模型在情感音频检索和语音情感识别中的性能。 |
| [^74] | [Online POMDP Planning with Anytime Deterministic Guarantees.](http://arxiv.org/abs/2310.01791) | 本文中，我们推导出在线POMDP规划中一个简化解决方案与理论上最优解之间的确定性关系，以解决目前近似算法只能提供概率性和通常呈现渐进性保证的限制。 |
| [^75] | [Borges and AI.](http://arxiv.org/abs/2310.01425) | 这篇论文主张通过探索乔治·路易斯·博尔赫斯的意象来理解大型语言模型和人工智能之间的关系。 |
| [^76] | [An Empirical Study of AI Generated Text Detection Tools.](http://arxiv.org/abs/2310.01423) | 本研究实证研究了AI生成文本检测工具在多领域ChatGPT材料中的有效性，并创建了一个多领域数据集来测试最先进API和工具的能力。 |
| [^77] | [LoRA ensembles for large language model fine-tuning.](http://arxiv.org/abs/2310.00035) | 本文提出了一种使用低秩适配器（LoRA）的集成方法，用于解决大型语言模型微调中存在的不确定性量化问题，并提供了一个参数高效的微调技术。这种方法可以构建大规模的LoRA适配器集成，并具有与基础预训练模型相近的计算资源需求。 |
| [^78] | [Efficient Anatomical labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks.](http://arxiv.org/abs/2309.17329) | 本文介绍了一种通过隐式点图网络高效解剖标记肺部树状结构的方法，提供了SOTA准确度和可用的表面，同时还提供了一个用于评估该方法的数据集。 |
| [^79] | [PlaceNav: Topological Navigation through Place Recognition.](http://arxiv.org/abs/2309.17260) | PlaceNav是一种通过地点识别进行拓扑导航的方法，将机器人无关部分分为导航特定和通用的计算机视觉组件，通过使用非机器人来源的大规模数据集增加训练数据的可用性，同时通过地点识别来提高导航性能。新模型的性能提高了76%。 |
| [^80] | [DyVal: Graph-informed Dynamic Evaluation of Large Language Models.](http://arxiv.org/abs/2309.17167) | DyVal是一种基于图形信息的大型语言模型动态评估协议，通过动态生成具有可控复杂性的评估样本，评估了各种LLM在推理任务上的性能，发现它们在这些挑战性样本上表现更差。 |
| [^81] | [Using Large Language Models for Qualitative Analysis can Introduce Serious Bias.](http://arxiv.org/abs/2309.17147) | 使用大型语言模型（LLMs）对定性分析进行注释可能引入偏见和测量误差，相比之下，使用高质量的人工注释和灵活编码对简单监督模型进行训练可以更好地减少偏见和误差。 |
| [^82] | [LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference.](http://arxiv.org/abs/2309.14331) | LinGCN是一个旨在减少乘法深度和优化HE基于GCN推断性能的框架，通过结构化线性化算法和参数化的离散指示函数的联合训练，实现细粒度的节点级非线性位置选择。 |
| [^83] | [Maximum Likelihood Estimation of Latent Variable Structural Equation Models: A Neural Network Approach.](http://arxiv.org/abs/2309.14073) | 本研究提出了一种新的图形结构，用于在线性和高斯性假设下稳定的潜变量结构方程模型。我们证明了计算该模型的最大似然估计等价于训练一个神经网络，并实现了一个基于GPU的算法来进行计算。 |
| [^84] | [AnglE-Optimized Text Embeddings.](http://arxiv.org/abs/2309.12871) | 本文提出了一种名为AnglE的角度优化文本嵌入模型，通过在复杂空间中引入角度优化来缓解文本嵌入中余弦函数饱和区域造成的梯度消失问题。该模型在多个STS任务中实现了高质量的文本嵌入，并在有限标签数据的特定领域STS场景中展现出优秀的性能。 |
| [^85] | [Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics.](http://arxiv.org/abs/2309.11981) | 这篇论文重新思考了人工智能系统中自然语言理解的评估框架，提出了以语言习得为核心的全面框架，旨在解决传统度量方法面临的问题，并借鉴了大型语言模型的进展。 |
| [^86] | [Partially-Specified Causal Simulations.](http://arxiv.org/abs/2309.10514) | 本文介绍了因果推断方法不当模拟设计的问题，并提出了一个满足期望的模拟框架PARCS，该框架合成了基于因果模型和可调参数的数据，使用户能够生成符合条件的数据来评估因果推断方法。 |
| [^87] | [Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules.](http://arxiv.org/abs/2309.09476) | 本论文研究了将强化学习应用于游戏规则生成的人类游戏评估，并通过实验结果表明，强化学习生成的规则与传统基线方法有所不同，可能更适合人类使用。 |
| [^88] | [Bidirectional Graph GAN: Representing Brain Structure-Function Connections for Alzheimer's Disease.](http://arxiv.org/abs/2309.08916) | 本研究提出了一种双向图生成对抗网络（BGGAN），用于表示阿尔茨海默病（AD）的脑结构-功能连接。通过特殊设计的内部图卷积网络模块和平衡器模块，该方法能够准确地学习结构域和功能域之间的映射函数，并解决模式坍塌问题，同时学习结构和功能特征的互补性。 |
| [^89] | [Exploring Social Choice Mechanisms for Recommendation Fairness in SCRUF.](http://arxiv.org/abs/2309.08621) | 本文通过使用社会选择机制，探索了多个多方面公平应用中的选择机制选项，结果显示不同的选择和分配机制会产生不同但一致的公平性/准确性权衡结果，并且多智能体的构成使得系统能够适应用户人口的动态变化。 |
| [^90] | [Landscape-Sketch-Step: An AI/ML-Based Metaheuristic for Surrogate Optimization Problems.](http://arxiv.org/abs/2309.07936) | Landscape-Sketch-Step是一种基于AI/ML的元启发式方法，结合了机器学习、随机优化和强化学习技术，用于解决成本函数评估昂贵、不可访问或禁止的代理优化问题。 |
| [^91] | [Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into Quantum Experiments.](http://arxiv.org/abs/2309.07056) | 本文使用了一种名为“梦境”的可解释人工智能技术，探索神经网络对量子光学实验的学习，发现网络可以改变量子系统的属性分布，并揭示了神经网络的学习策略。 |
| [^92] | [Marginalized Importance Sampling for Off-Environment Policy Evaluation.](http://arxiv.org/abs/2309.01807) | 本文提出了一种新的方法，利用边际化重要性采样框架，在使用模拟器和真实世界的离线数据评估代理策略性能时，解决了大密度比率和间接监督的挑战。 |
| [^93] | [On the Implicit Bias of Adam.](http://arxiv.org/abs/2309.00079) | 本文证明了RMSProp和Adam存在隐式规范化作用，其取决于超参数和训练阶段，并讨论了这些证明事实对泛化的影响。 |
| [^94] | [BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection.](http://arxiv.org/abs/2308.12439) | BaDExpert是一种防御后门攻击的新方法，通过逆向工程提取给定后门模型的后门功能，并生成一个专家模型，该模型只能识别后门输入。可以进一步使用该模型设计高度准确的后门输入检测器。 |
| [^95] | [Developmental Bootstrapping of AIs.](http://arxiv.org/abs/2308.04586) | 传统的符号AI方法和深度学习AI方法无法满足创建强大和可信赖的AI的挑战，然而，发展脱靴法通过模仿人类儿童的能力发展过程，为创建稳健可靠的AI提供了希望。 |
| [^96] | [SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning.](http://arxiv.org/abs/2308.00436) | 本论文研究了使用LLMs自检逐步推理的能力，提出了一种零-shot验证方案，成功识别错误并提高了问答性能。 |
| [^97] | [Formally Explaining Neural Networks within Reactive Systems.](http://arxiv.org/abs/2308.00143) | 这项研究在反应式系统中提出了一种基于DNN验证的形式化XAI技术，可以解释DNN的行为，并且通过利用系统的转换约束来计算简洁的解释。 |
| [^98] | [An Empirical Study on Fertility Proposals Using Multi-Grined Topic Analysis Methods.](http://arxiv.org/abs/2307.10025) | 本研究通过采用多粒度主题分析方法，对微博评论进行语义分析，发现关于取消婚姻登记的生育限制的提案涉及个人、社会和国家三个维度，详细讨论了个人行为、社会伦理和法律以及国家政策等社会问题。 |
| [^99] | [Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation.](http://arxiv.org/abs/2307.06125) | 这项工作提出了一个层次化的强化学习方法，用于解决在未知环境中需要同时进行操控和导航的交互式多目标搜索任务。实验证明，该方法可以在新环境中进行零样本迁移，并对未见过的子任务具有鲁棒性。 |
| [^100] | [Quantitative CLTs in Deep Neural Networks.](http://arxiv.org/abs/2307.06092) | 本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。 |
| [^101] | [Enhancing Adversarial Robustness via Score-Based Optimization.](http://arxiv.org/abs/2307.04333) | 本文介绍了一种名为ScoreOpt的新型对抗防御方案，该方案通过优化对抗样本来提高模型的鲁棒性，实验证明该方法在多个数据集上优于现有的对抗防御方法。 |
| [^102] | [REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction.](http://arxiv.org/abs/2306.15724) | 提出了REFLECT框架，可以将机器人多感官数据转化为分层总结，并使用大型语言模型进行失败解释。该框架能够生成有益的失败解释，帮助机器人完成任务。 |
| [^103] | [Modularizing while Training: a New Paradigm for Modularizing DNN Models.](http://arxiv.org/abs/2306.09376) | 本文提出了一种新方法，将模块化纳入模型训练过程中，即在训练时模块化(MwT)，通过两个损失函数实现模型结构上的模块化，进而实现模块的重用，能够在较短的训练时间内达到可比较的模型精度，并且相对于最先进的训练后模块化方法需要更少的参数。 |
| [^104] | [FedJETs: Efficient Just-In-Time Personalization with Federated Mixture of Experts.](http://arxiv.org/abs/2306.08586) | 本论文提出了一种名为FedJETs的方法，使用联邦混合专家的框架，在联邦学习中实现高效及时的个性化。该方法通过训练专门的专家，并利用门控函数将输入路由到相关的专家，有效提高了模型的准确性。 |
| [^105] | [PyTrial: A Comprehensive Platform for Artificial Intelligence for Drug Development.](http://arxiv.org/abs/2306.04018) | PyTrial是一个实现多种AI算法支持的临床试验任务、可集成自己AI模型和数据集的Python软件包，并在现实临床试验数据上进行了广泛测试。 |
| [^106] | [Transferring Annotator- and Instance-dependent Transition Matrix for Learning from Crowds.](http://arxiv.org/abs/2306.03116) | 本文提出了一个高效的方法来估算特定注释者和特定实例的转移矩阵以及真实标签比例，解决了从众包中学习的标签噪声问题，并在实验中证明了方法的优越性。 |
| [^107] | [VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores.](http://arxiv.org/abs/2306.01879) | 我们提出了VisualGPTScore方法，能够使用多模态生成分数捕捉文本标题可能性，并在图像条件语言模型上进行计算，具备组合推理能力。 |
| [^108] | [Unpaired Image-to-Image Translation via Neural Schr\"odinger Bridge.](http://arxiv.org/abs/2305.15086) | 本文提出了一种方法——非配对神经薛定谔桥 (UNSB)，它结合了薛定谔桥、对抗训练和正则化，用于在非配对数据之间学习 SDE，并成功解决了许多非配对图像转换任务。 |
| [^109] | [Scale Matters: Attribution Meets the Wavelet Domain to Explain Model Sensitivity to Image Corruptions.](http://arxiv.org/abs/2305.14979) | 该论文介绍了一种基于小波域的属性方法WCAM，能够解释神经网络模型对图像损坏的敏感性，确定预测的足够信息，并阐明缩放如何增加准确性。 |
| [^110] | [Physics of Language Models: Part 1, Context-Free Grammar.](http://arxiv.org/abs/2305.13673) | 本研究探究了生成式语言模型如何学习上下文无关文法（CFG），并通过构造人造数据证明了预训练transformers可以学会生成具有接近完美准确度和显着多样性的句子。研究发现transformer内部的隐藏状态隐含而精确地编码了CFG结构，学会形成类似动态规划的“边界到边界”的注意力。此外，还研究了标准CFG的扩展，例如概率CFG和线性CFG，并证明transformers也可以学会这些扩展语法结构。 |
| [^111] | [Explaining Emergent In-Context Learning as Kernel Regression.](http://arxiv.org/abs/2305.12766) | 本文研究了为什么在预训练之后，基于Transformer的语言模型能够实现上下文学习，并提出了一种假设，认为LLMs在面对上下文示例时能够通过内部表示模拟核回归。 |
| [^112] | [AnyPredict: Foundation Model for Tabular Prediction.](http://arxiv.org/abs/2305.12081) | 本文提出了一种名为 AnyPredict 的表格预测基础模型，使用数据引擎整合领域内和广泛的领域外数据集，以克服模式不匹配和预测目标异质性等方面的障碍。 |
| [^113] | [ChatGPT -- a Blessing or a Curse for Undergraduate Computer Science Students and Instructors?.](http://arxiv.org/abs/2304.14993) | 本文分析了ChatGPT在回答本科计算机科学问题上的不可靠性，并提供了在学术界使用ChatGPT的建议。 |
| [^114] | [MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data.](http://arxiv.org/abs/2304.08247) | MedAlpaca是一个开源的医疗会话式人工智能模型和训练数据集合，旨在通过细化调整预训练语言模型来改善医疗工作流程和医生认证考试的表现。 |
| [^115] | [Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance.](http://arxiv.org/abs/2304.06715) | 本文提出了解释不变性和等变性的概念，通过对对称群下具有不变性的神经网络，建立两种度量方法来提高解释方法对于不变性的健壮性并证明为一些流行的解释方法提供了理论健壮性保证。 |
| [^116] | [Teaching Large Language Models to Self-Debug.](http://arxiv.org/abs/2304.05128) | 本文提出了一种自我调试方法，通过少量演示来教授大型语言模型自动调试其预测的程序，在多项代码生成基准测试中取得了最先进的性能。 |
| [^117] | [Logic of Differentiable Logics: Towards a Uniform Semantics of DL.](http://arxiv.org/abs/2303.10650) | 该论文介绍了一个元语言——LDL，用于定义DL，该元语言从语法和语义两方面上提高DL的形式化程度，使得对DL的性质和实现进行系统比较研究成为了可能。 |
| [^118] | [Towards Inferential Reproducibility of Machine Learning Research.](http://arxiv.org/abs/2302.04054) | 本研究提出利用线性混合效应模型（LMEM）来分析机器学习性能评估分数，并考虑多个方差来源及其与数据特性相互作用，从而评估可靠性和可复制性，促进对机器学习算法行为的更全面理解。 |
| [^119] | [AI Alignment Dialogues: An Interactive Approach to AI Alignment in Support Agents.](http://arxiv.org/abs/2301.06421) | 本文介绍了一种名为AI对齐对话的新方法，该方法通过用户和代理的交互努力实现和维持AI对齐，并相比数据驱动的方法在行为支持代理方面具有更多优势。 |
| [^120] | [Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis.](http://arxiv.org/abs/2301.04554) | 该论文提出了一种通用防御方法，通过基于密度聚类和质心分析的策略，检测深度神经网络模型是否受到反向攻击的影响，该方法与现有的防御方法相比具有攻击无关性。 |
| [^121] | [Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases.](http://arxiv.org/abs/2212.02648) | 这个论文提出了一种使用排序数据来测量和减少模型对虚假线索的偏见的简单有效方法。通过排名图像的虚假性，可以识别出少数子群体，并通过准确率差来评估模型的偏见。此外，通过在虚假性较低的图像上微调模型，可以在几乎不损失准确率的情况下消除模型的偏见，实现对样本的公正处理。 |
| [^122] | [BiViT: Extremely Compressed Binary Vision Transformer.](http://arxiv.org/abs/2211.07091) | BiViT是一种极度压缩的二进制视觉Transformer，通过Softmax-aware二进制化和跨层二进制化方案解决了在二进制化过程中存在的注意力误差和信息损失的问题。 |
| [^123] | [Space-fluid Adaptive Sampling by Self-Organisation.](http://arxiv.org/abs/2210.17505) | 本文提出了一种自组织的空间流体自适应采样方法来估计分布式传感数据或计算结果，其动态划分空间的方法具有优越的性能。 |
| [^124] | [Self-Supervised Masked Convolutional Transformer Block for Anomaly Detection.](http://arxiv.org/abs/2209.12148) | 本论文提出了一种自监督遮蔽卷积变压器块（SSMCTB）用于异常检测。该方法在核心架构层面上集成了重构功能，并且具有灵活的信息遮蔽能力。 |
| [^125] | [Decoding speech perception from non-invasive brain recordings.](http://arxiv.org/abs/2208.12266) | 本研究通过使用对比学习训练的模型，成功从非侵入性脑电记录中解码感知语音。结果显示，该模型能够以高达41%的准确率识别出与脑电信号相对应的语音片段。 |
| [^126] | [Spatial-temporal associations representation and application for process monitoring using graph convolution neural network.](http://arxiv.org/abs/2205.05250) | 该论文利用图卷积神经网络，基于同一工业过程中众多具有空间-时间相关特征的变量，实现了变量特征建模和表示、图网络构建及图特征感知，并应用于过程监测中。 |
| [^127] | [Diversity in deep generative models and generative AI.](http://arxiv.org/abs/2202.09573) | 该论文介绍了一种基于核测度量化的方法，通过近似整体测度来生成多样性的对象，以解决现有生成算法中对象重复和缺乏多样性的问题。 |
| [^128] | [Characterization of causal ancestral graphs for time series with latent confounders.](http://arxiv.org/abs/2112.08417) | 本文介绍了一种新的图模型类别，用于表示具有未观测到的混淆因素的时间序列的因果关系和独立性。通过使用这些新的图模型，可以得出更强的因果推断，而无需额外的假设。 |
| [^129] | [Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training.](http://arxiv.org/abs/2110.14883) | Colossal-AI是一种用于大规模并行训练的统一深度学习系统，能够以高达2.76倍的速度加快训练过程，并支持多种并行训练方法和零冗余优化器集成的异构训练方法。 |
| [^130] | [SR-HetGNN:Session-based Recommendation with Heterogeneous Graph Neural Network.](http://arxiv.org/abs/2108.05641) | 本文提出了一种基于异构图神经网络的会话推荐方法SR-HetGNN，通过学习会话嵌入并捕捉匿名用户的特定偏好，以改进会话推荐系统的效果和准确性。 |
| [^131] | [Memory Capacity of Recurrent Neural Networks with Matrix Representation.](http://arxiv.org/abs/2104.07454) | 这个论文研究了具有矩阵表示的循环神经网络的存储容量，通过基于Fisher信息的概率性概念定义和研究，得出了不同假设下的存储上界，并与向量表示的网络进行了比较。 |
| [^132] | [Numerical Weather Forecasting using Convolutional-LSTM with Attention and Context Matcher Mechanisms.](http://arxiv.org/abs/2102.00696) | 本论文提出了一种新的深度学习架构，用于预测高分辨率时空天气数据。该架构集成了卷积长短期记忆和卷积神经网络，并引入了注意力和上下文匹配机制。与传统模型相比，该架构在性能上取得了显著改进。 |

# 详细

[^1]: MathCoder: 增强数学推理中 LLMs 中无缝代码集成

    MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning. (arXiv:2310.03731v1 [cs.CL])

    [http://arxiv.org/abs/2310.03731](http://arxiv.org/abs/2310.03731)

    本文介绍了一种方法，通过微调开源语言模型，使其能够使用代码进行数学建模和推导，从而增强数学推理能力。作者提出了一种生成包含数学问题和基于代码的解决方案的数据集，并引入了定制的微调和推理方法，从而实现了在解决具有挑战性的数学问题上生成基于代码的解决方案的 MathCoder 模型。

    

    最近发布的 GPT-4 代码解释器展示了在解决具有挑战性的数学问题方面的出色能力，这主要归功于它能够无缝地使用自然语言进行推理，生成代码，执行代码，并根据执行输出继续推理的能力。在本文中，我们提出了一种方法来微调开源的语言模型，使其能够使用代码来建模和推导数学方程，并从而增强其数学推理能力。我们提出了一种生成包含数学问题及其基于代码的解决方案的新颖高质量数据集的方法，称为 MathCodeInstruct。每个解决方案都交错使用自然语言、代码和执行结果。我们还引入了一种定制的监督微调和推理方法。这种方法得到了 MathCoder 模型，这是一系列模型，能够生成基于代码的解决方案来解决具有挑战性的数学问题。令人印象深刻的是，MathCoder 模型实现了最先进的成果。

    The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-
    
[^2]: 约束条件下的策略优化用于多功能安全强化学习

    Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning. (arXiv:2310.03718v1 [cs.LG])

    [http://arxiv.org/abs/2310.03718](http://arxiv.org/abs/2310.03718)

    这个论文提出了一种约束条件下的策略优化框架，用于训练多功能安全强化学习智能体。通过引入多功能值估计和有条件的变分推理模块，该框架在训练效率和零-shot适应能力方面表现优于基准方法。

    

    安全强化学习（RL）专注于训练在预定义安全约束条件下能够最大化奖励的智能体。然而，在部署过程中，学习能够适应不同安全约束要求且无需重新训练的多功能安全策略仍然是一个较为未开发和具有挑战性的领域。在这项工作中，我们提出了多功能安全强化学习问题，并考虑了两个主要需求：训练效率和零-shot适应能力。为了解决这些问题，我们引入了Conditioned Constrained Policy Optimization（CCPO）框架，包括两个关键模块：（1）多功能值估计（VVE），用于在未见过的阈值条件下近似值函数，并且（2）有条件的变分推理（CVI），用于在策略优化中编码任意约束阈值。我们的大量实验表明，CCPO在安全和任务性能方面优于基准，并保持了对不同约束的零-shot适应能力。

    Safe reinforcement learning (RL) focuses on training reward-maximizing agents subject to pre-defined safety constraints. Yet, learning versatile safe policies that can adapt to varying safety constraint requirements during deployment without retraining remains a largely unexplored and challenging area. In this work, we formulate the versatile safe RL problem and consider two primary requirements: training efficiency and zero-shot adaptation capability. To address them, we introduce the Conditioned Constrained Policy Optimization (CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation (VVE) for approximating value functions under unseen threshold conditions, and (2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint thresholds during policy optimization. Our extensive experiments demonstrate that CCPO outperforms the baselines in terms of safety and task performance while preserving zero-shot adaptation capabilities to different constraint 
    
[^3]: 人工智能指数报告2023年版

    Artificial Intelligence Index Report 2023. (arXiv:2310.03715v1 [cs.AI])

    [http://arxiv.org/abs/2310.03715](http://arxiv.org/abs/2310.03715)

    本篇论文介绍了AI指数报告2023年版，包括了更多的原始数据和分析内容，旨在为广大读者提供准确、全面的关于人工智能的数据和见解。

    

    欢迎来到第六版的AI指数报告。今年的报告比之前任何一版都引入了更多的原始数据，包括一章关于AI公众意见的新内容，一章更全面的技术性能内容，关于大型语言和多模态模型的原创分析，全球AI立法记录的详细趋势，以及对AI系统环境影响的研究等。AI指数报告跟踪、整理、提炼和可视化与人工智能相关的数据。我们的使命是为决策者、研究人员、高管、记者和公众提供没有偏见、经过严谨审核、来源广泛的数据，以便他们对复杂的AI领域有更全面、更精细的理解。该报告旨在成为关于AI数据和见解的全球最可信且具权威性的来源。

    Welcome to the sixth edition of the AI Index Report. This year, the report introduces more original data than any previous edition, including a new chapter on AI public opinion, a more thorough technical performance chapter, original analysis about large language and multimodal models, detailed trends in global AI legislation records, a study of the environmental impact of AI systems, and more. The AI Index Report tracks, collates, distills, and visualizes data related to artificial intelligence. Our mission is to provide unbiased, rigorously vetted, broadly sourced data in order for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI. The report aims to be the world's most credible and authoritative source for data and insights about AI.
    
[^4]: DSPy: 将声明性语言模型调用编译成自我改进的流水线

    DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. (arXiv:2310.03714v1 [cs.CL])

    [http://arxiv.org/abs/2310.03714](http://arxiv.org/abs/2310.03714)

    DSPy是一个编程模型，将LM流水线抽象为文本转换图，通过声明性模块调用LM实现优化，能够解决复杂的推理问题和数学问题等任务。

    

    ML社区正在快速探索用于提示语言模型(LMs)和将它们堆叠成解决复杂任务的流水线的技术。不幸的是，现有的LM流水线通常使用硬编码的"提示模板"来实现，即通过试错发现的冗长字符串。为了更系统地开发和优化LM流水线，我们引入了DSPy，这是一个以文本转换图的形式抽象LM流水线的编程模型，即通过声明性模块调用LM的命令式计算图。DSPy模块是参数化的，这意味着它们可以通过创建和收集示例来学习如何应用提示、微调、增强和推理技术的组合。我们设计了一个编译器，可以优化任何DSPy流水线以最大化给定的度量标准。我们进行了两个案例研究，显示出简洁的DSPy程序可以表达和优化复杂的推理数学问题、登录日志问题等流水线。

    The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded "prompt templates", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tac
    
[^5]: 代理指导大型语言模型成为通用的零-shot推理器

    Agent Instructs Large Language Models to be General Zero-Shot Reasoners. (arXiv:2310.03710v1 [cs.CL])

    [http://arxiv.org/abs/2310.03710](http://arxiv.org/abs/2310.03710)

    该论文提出了一种方法，通过代理指导的方式，大大提高了大型语言模型在零-shot推理任务上的能力，并在多个数据集上实现了最先进的性能。

    

    我们引入了一种方法，以提高大型语言模型在一般语言理解任务上的零-shot推理能力。具体而言，我们构建了一个自主代理，来指导大型语言模型的推理过程。我们展示了这种方法进一步释放了大型语言模型的零-shot推理能力，适用于更多的任务。我们在涵盖生成、分类和推理的广泛数据集上研究了我们方法的性能。我们展示了我们的方法适用于大多数任务，并在我们评估的29个数据集中，在20个数据集上获得了最先进的零-shot性能。例如，我们的方法显著提升了最先进的大型语言模型的性能，包括Vicuna-13b（13.3%），Llama-2-70b-chat（23.2%）和GPT-3.5 Turbo（17.0%）。与零-shot思维链相比，我们对推理的改进很明显，平均提高了10.5%。通过我们的方法，Llama-2-70b-chat的性能超过零-shot GPT-3.5 Turbo 10.2%。

    We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement in reasoning is striking, with an average increase of 10.5%. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.
    
[^6]: 超越一视同仁：多目标直接偏好优化

    Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization. (arXiv:2310.03708v1 [cs.LG])

    [http://arxiv.org/abs/2310.03708](http://arxiv.org/abs/2310.03708)

    本文提出了一种无强化学习的算法，称为多目标直接偏好优化（MODPO），它可以根据不同的偏好训练不同的语言模型，通过组合所有目标和特定权重来优化模型。

    

    语言模型（LM）通过强化学习与人类反馈的协同作用，能够很好地与普通标记者保持一致，但可能不适应各种各样的人类偏好。因此，最近的研究方法选择通过收集多维度反馈并为每个维度创建不同的奖励（例如，有益性，无害性，诚实性）进行个性化。通过使用不同的奖励权重，可以通过多目标强化学习（MORL）将LM调整到不同的偏好。然而，强化学习的微调在MORLHF中不稳定且耗费资源，特别是因为各种常常矛盾的目标。在本文中，我们提出了多目标直接偏好优化（MODPO），这是一种无强化学习的算法，它将直接偏好优化（DPO）扩展到多个对齐目标。基本上，MODPO通过训练不同的LM来代表不同的集体奖励模型，这些模型将所有目标和特定权重进行组合。通过简单的交叉熵损失，LM根据MOD进行优化。

    Language models (LMs), despite aligning well with an average labeler through reinforcement learning from human feedback (RLHF), may not universally suit diverse human preferences. Recent approaches therefore opt for customization by collecting multi-dimensional feedback and creating distinct rewards for each dimension (e.g., helpfulness, harmlessness, honesty). LMs can then be tailored to different preferences using multi-objective RL (MORL) with different reward weightings. Yet, RL fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free algorithm that extends Direct Preference Optimization (DPO) for multiple alignment objectives. Essentially, MODPO trains different LMs to represent different collective reward models that combine all objectives with specific weightings. With a simple cross-entropy loss, the LMs optimized against the MOD
    
[^7]: 调整对齐语言模型会牺牲安全性，即使用户没有意图！

    Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!. (arXiv:2310.03693v1 [cs.CL])

    [http://arxiv.org/abs/2310.03693](http://arxiv.org/abs/2310.03693)

    微调对齐语言模型会牺牲安全性，即使用户没有恶意意图。通过敌对设计的训练样本，即使只有少数10个，也可以破坏语言模型的安全对齐。这种安全风险存在于微调后的模型中。

    

    对大型语言模型进行下游用例的优化通常涉及通过进一步的微调来定制预训练语言模型。Meta发布Llama模型和OpenAI的GPT-3.5 Turbo的自定义数据集微调API也鼓励这种做法。但是，这种自定义微调的安全成本是多少？我们注意到，尽管现有的安全对齐基础设施可以在推理时限制语言模型的有害行为，但它们并不涵盖当微调特权扩展给终端用户时的安全风险。我们的红队研究发现，只需几个敌对设计的训练样本就可以破坏语言模型的安全对齐。例如，我们只使用10个这样的示例在OpenAI的API中以不到0.20美元的成本将GPT-3.5 Turbo的安全保护解除了，使模型对几乎任何有害指令都有响应。令人担忧的是，我们的研究还发现，即使没有恶意意图，微调后的模型也存在安全风险。

    Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious 
    
[^8]: 概率生成建模用于发展中国家程序化环形道路生成

    Probabilistic Generative Modeling for Procedural Roundabout Generation for Developing Countries. (arXiv:2310.03687v1 [cs.AI])

    [http://arxiv.org/abs/2310.03687](http://arxiv.org/abs/2310.03687)

    本文提出了一种概率生成建模方法，用于发展中国家的程序化环形道路生成问题。通过使用生成性流网络作为道路生成器，我们实现了更好的多样性和高效性能。

    

    鉴于有限的资源和快速的经济增长，以一种有效的成本方式设计优化的交通道路网络，并进行交通仿真和验证对于发展中国家至关重要，而广泛的手动测试昂贵且常常无法实现。当前基于规则的道路设计生成器缺乏多样性，这是设计鲁棒性的关键特征。生成性流网络（GFlowNets）学习从一个未归一化的奖励分布中进行采样的随机策略，从而在保持多样性的同时生成高质量的解决方案。在这项工作中，我们将将事件道路与环形道路的连接问题建模为马尔科夫决策过程，并利用GFlowNets作为Junction-Art道路生成器。我们将我们的方法与相关方法进行了比较，实证结果表明我们的方法在保持高有效性得分的同时实现了更好的多样性。

    Due to limited resources and fast economic growth, designing optimal transportation road networks with traffic simulation and validation in a cost-effective manner is vital for developing countries, where extensive manual testing is expensive and often infeasible. Current rule-based road design generators lack diversity, a key feature for design robustness. Generative Flow Networks (GFlowNets) learn stochastic policies to sample from an unnormalized reward distribution, thus generating high-quality solutions while preserving their diversity. In this work, we formulate the problem of linking incident roads to the circular junction of a roundabout by a Markov decision process, and we leverage GFlowNets as the Junction-Art road generator. We compare our method with related methods and our empirical results show that our method achieves better diversity while preserving a high validity score.
    
[^9]: SmoothLLM：防御大型语言模型免受越狱攻击

    SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. (arXiv:2310.03684v1 [cs.LG])

    [http://arxiv.org/abs/2310.03684](http://arxiv.org/abs/2310.03684)

    SmoothLLM是第一个用于减轻大型语言模型上越狱攻击的算法，通过在输入提示上随机扰动并汇总预测结果来检测对抗性输入，将攻击成功率降低至不到一个百分点，并提供了可证明的保证。

    

    尽管努力将大型语言模型（LLM）与人类价值观保持一致，但广泛使用的LLM（如GPT、Llama、Claude和PaLM）仍然容易受到越狱攻击，即对目标LLM进行欺骗，以生成不合适的内容。为了解决这个漏洞，我们提出了SmoothLLM，这是第一个旨在减轻LLM上的越狱攻击的算法。基于我们的发现，对抗性生成的提示对字符级别的改变很脆弱，我们的防御首先随机扰动给定输入提示的多个副本，然后汇总相应的预测结果来检测对抗性输入。SmoothLLM将众多热门LLM的攻击成功率降低至不到一个百分点，避免了不必要的保守性，并对攻击缓解提供了可证明的保证。此外，我们的防御使用的查询数量比现有的攻击方法少得多，并且与任何LLM兼容。

    Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM.
    
[^10]: MapperGPT:用于链接和映射实体的大型语言模型

    MapperGPT: Large Language Models for Linking and Mapping Entities. (arXiv:2310.03666v1 [cs.CL])

    [http://arxiv.org/abs/2310.03666](http://arxiv.org/abs/2310.03666)

    MapperGPT是一个基于大型语言模型的实体链接和映射工具，能够解决实体映射中的词汇模糊问题和手动映射细化的困扰。

    

    在许多领域，如医疗保健、化学和生物医学研究中，对齐术语资源（包括本体、受控词汇、分类法和值集）是数据集成的关键部分。实体映射是确定这些资源中的实体之间的对应关系的过程，例如基因标识符、疾病概念或化学实体标识符。许多工具已经开发出来，基于常见结构特征和词汇信息（如标签和同义词）来计算这种映射。特别是词汇方法通常提供非常高的召回率，但由于词义模糊，精度较低。因此，映射工作通常需要通过人工策划进行繁琐而费时的手动映射细化过程。大型语言模型（LLMs），例如ChatGPT使用的模型，具有通用能力，可以执行广泛的任务，包括问答和信息提取。在这里，我们介绍了MapperGPT，一个新的基于大型语言模型的实体链接和映射工具。

    Aligning terminological resources, including ontologies, controlled vocabularies, taxonomies, and value sets is a critical part of data integration in many domains such as healthcare, chemistry, and biomedical research. Entity mapping is the process of determining correspondences between entities across these resources, such as gene identifiers, disease concepts, or chemical entity identifiers. Many tools have been developed to compute such mappings based on common structural features and lexical information such as labels and synonyms. Lexical approaches in particular often provide very high recall, but low precision, due to lexical ambiguity. As a consequence of this, mapping efforts often resort to a labor intensive manual mapping refinement through a human curator.  Large Language Models (LLMs), such as the ones employed by ChatGPT, have generalizable abilities to perform a wide range of tasks, including question-answering and information extraction. Here we present MapperGPT, an a
    
[^11]: 平衡自主性和一致性：面向自治LLM驱动的多智能体架构的多维分类法

    Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures. (arXiv:2310.03659v1 [cs.AI])

    [http://arxiv.org/abs/2310.03659](http://arxiv.org/abs/2310.03659)

    这篇论文提出了一个多维分类法，用于平衡自治LLM驱动的多智能体架构中的自主性和一致性。该架构通过将目标分解为可管理的任务和协调智能体合作来实现目标，以提高人工智能能力。

    

    大型语言模型（LLMs）已经彻底改变了人工智能领域，赋予其复杂的语言理解和生成能力。然而，当面对更复杂和相互连接的任务时，需要深入和迭代的思考过程，LLMs显露出其固有的局限性。自治LLM驱动的多智能体系统是对这些挑战的战略性回应。这样的系统通过将用户提示的目标分解为可管理的任务，并通过一组专门的智能体协调执行和结果综合，努力实现自主地解决这些目标。这些智能体配备了LLM驱动的推理能力，通过利用上下文资源（如工具和数据集），增强与同伴合作的认知协同效应。虽然这些架构在增强人工智能能力方面具有潜在的潜力，但在不同层次的自主性和一致性之间找到正确的平衡是关键。

    Large language models (LLMs) have revolutionized the field of artificial intelligence, endowing it with sophisticated language understanding and generation capabilities. However, when faced with more complex and interconnected tasks that demand a profound and iterative thought process, LLMs reveal their inherent limitations. Autonomous LLM-powered multi-agent systems represent a strategic response to these challenges. Such systems strive for autonomously tackling user-prompted goals by decomposing them into manageable tasks and orchestrating their execution and result synthesis through a collective of specialized intelligent agents. Equipped with LLM-powered reasoning capabilities, these agents harness the cognitive synergy of collaborating with their peers, enhanced by leveraging contextual resources such as tools and datasets. While these architectures hold promising potential in amplifying AI capabilities, striking the right balance between different levels of autonomy and alignment
    
[^12]: CLEVRER-Humans: 用人类的方式描述物理和因果事件

    CLEVRER-Humans: Describing Physical and Causal Events the Human Way. (arXiv:2310.03635v1 [cs.AI])

    [http://arxiv.org/abs/2310.03635](http://arxiv.org/abs/2310.03635)

    CLEVRER-Humans是一个用于因果判断的视频推理数据集，通过人工标注来解决合成事件和合成语言描述的缺乏多样性问题，并通过迭代事件填空和神经语言生成模型提高数据收集效率。

    

    构建能够推理物理事件及其因果关系的机器对于与物理世界进行灵活互动非常重要。然而，现有的大多数物理和因果推理基准都仅基于合成事件和合成自然语言描述的因果关系。这种设计存在两个问题：一是事件类型和自然语言描述缺乏多样性；二是基于手动定义的启发式规则的因果关系与人类判断不一致。为了解决这两个问题，我们提出了CLEVRER-Humans基准，这是一个用人工标注的视频推理数据集，用于对物理事件的因果判断。我们采用了两种技术来提高数据收集效率：首先，一种新颖的迭代事件填空任务，以 eliciting 视频中事件的新表示方式，我们称之为因果事件图 (CEGs)；其次，一种基于神经语言生成模型的数据增强技术。

    Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world. However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of causal relationships. This design brings up two issues. First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments. To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels. We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models.
    
[^13]: PeaTMOSS: 在开源软件中挖掘预训练模型

    PeaTMOSS: Mining Pre-Trained Models in Open-Source Software. (arXiv:2310.03620v1 [cs.SE])

    [http://arxiv.org/abs/2310.03620](http://arxiv.org/abs/2310.03620)

    PeaTMOSS数据集提供了一个研究开源软件中预训练模型软件工程行为和挑战的平台，包括大量的PTMs快照和使用PTMs的开源软件存储库。

    

    开发和训练深度学习模型成本高昂，因此软件工程师已经开始重用预训练的深度学习模型(PTMs)并对其进行微调以适用于下游任务。尽管PTMs的使用已经广泛，但我们对相应的软件工程行为和挑战还知之甚少。为了实现对PTMs进行软件工程研究，我们提出了PeaTMOSS数据集：开源软件中的预训练模型。PeaTMOSS数据集包括三个部分：(1) 281,638个PTMs的快照，(2) 使用PTMs的27,270个开源软件存储库，以及(3) PTMs和使用它们的项目之间的映射。我们呼吁PeaTMOSS矿工们发现围绕PTMs的软件工程实践。演示和完整数据集的链接可在以下网址找到：https://github.com/PurdueDualityLab/PeaTMOSS-Demos。

    Developing and training deep learning models is expensive, so software engineers have begun to reuse pre-trained deep learning models (PTMs) and fine-tune them for downstream tasks. Despite the wide-spread use of PTMs, we know little about the corresponding software engineering behaviors and challenges.  To enable the study of software engineering with PTMs, we present the PeaTMOSS dataset: Pre-Trained Models in Open-Source Software. PeaTMOSS has three parts: a snapshot of (1) 281,638 PTMs, (2) 27,270 open-source software repositories that use PTMs, and (3) a mapping between PTMs and the projects that use them. We challenge PeaTMOSS miners to discover software engineering practices around PTMs. A demo and link to the full dataset are available at: https://github.com/PurdueDualityLab/PeaTMOSS-Demos.
    
[^14]: 在联邦学习中解决一类非凸最小极大优化问题

    Solving a Class of Non-Convex Minimax Optimization in Federated Learning. (arXiv:2310.03613v1 [cs.LG])

    [http://arxiv.org/abs/2310.03613](http://arxiv.org/abs/2310.03613)

    本研究针对联邦学习中的一类非凸最小极大优化问题，提出了FL算法（FedSGDA+和FedSGDA-M），并在最常见的最小极大问题中降低了复杂度。针对非凸凹问题，提出的FedSGDA+算法将通信复杂度降低到O(ε^{-6})。在非凸强凹和非凸PL最小极大设置下，证明了FedSGDA-M具有已知的样本复杂度。

    

    最小极大问题广泛应用于机器学习中的各种场景，包括对抗训练、增强学习中的策略评估以及 AUROC 最大化等。为了解决跨多个客户端的大规模数据挑战，在通信高效的分布式训练中，联邦学习（FL）越来越受欢迎。尽管在集中式（即单机）环境下已经开发了许多最小极大问题的优化算法，但在 FL 下的最小极大问题算法仍然不够深入研究。本文研究了一类联邦非凸最小极大优化问题。我们提出了 FL 算法（FedSGDA+ 和 FedSGDA-M）并降低了最常见最小极大问题的现有复杂度结果。对于非凸凹问题，我们提出了 FedSGDA+ 并将通信复杂度降低到 $O(\varepsilon^{-6})$。在非凸强凹和非凸 PL 最小极大设置下，我们证明了 FedSGDA-M 具有已知的样本复杂度。

    The minimax problems arise throughout machine learning applications, ranging from adversarial training and policy evaluation in reinforcement learning to AUROC maximization. To address the large-scale data challenges across multiple clients with communication-efficient distributed training, federated learning (FL) is gaining popularity. Many optimization algorithms for minimax problems have been developed in the centralized setting (\emph{i.e.} single-machine). Nonetheless, the algorithm for minimax problems under FL is still underexplored. In this paper, we study a class of federated nonconvex minimax optimization problems. We propose FL algorithms (FedSGDA+ and FedSGDA-M) and reduce existing complexity results for the most common minimax problems. For nonconvex-concave problems, we propose FedSGDA+ and reduce the communication complexity to $O(\varepsilon^{-6})$. Under nonconvex-strongly-concave and nonconvex-PL minimax settings, we prove that FedSGDA-M has the best-known sample comp
    
[^15]: FASER: 通过中间表示进行二进制代码相似性搜索

    FASER: Binary Code Similarity Search through the use of Intermediate Representations. (arXiv:2310.03605v1 [cs.CR])

    [http://arxiv.org/abs/2310.03605](http://arxiv.org/abs/2310.03605)

    本论文提出了一种名为FASER的方法，通过使用中间表示进行二进制代码相似性搜索。该方法可以跨架构地识别函数，并明确编码函数的语义，以支持各种应用场景。

    

    能够识别跨架构软件中感兴趣的函数对于分析恶意软件、保护软件供应链或进行漏洞研究都是有用的。跨架构二进制代码相似性搜索已在许多研究中探索，并使用了各种不同的数据来源来实现其目标。通常使用的数据来源包括从二进制文件中提取的常见结构，如函数控制流图或二进制级调用图，反汇编过程的输出或动态分析方法的输出。其中一种受到较少关注的数据来源是二进制中间表示。二进制中间表示具有两个有趣的属性：它们的跨架构性质以及明确编码函数的语义以支持下游使用。在本文中，我们提出了一种名为FASER的函数字符串编码表示方法，它结合了长文档转换技术。

    Being able to identify functions of interest in cross-architecture software is useful whether you are analysing for malware, securing the software supply chain or conducting vulnerability research. Cross-Architecture Binary Code Similarity Search has been explored in numerous studies and has used a wide range of different data sources to achieve its goals. The data sources typically used draw on common structures derived from binaries such as function control flow graphs or binary level call graphs, the output of the disassembly process or the outputs of a dynamic analysis approach. One data source which has received less attention is binary intermediate representations. Binary Intermediate representations possess two interesting properties: they are cross architecture by their very nature and encode the semantics of a function explicitly to support downstream usage. Within this paper we propose Function as a String Encoded Representation (FASER) which combines long document transforme
    
[^16]: 具有弹性步态的本地导航：学习在受损感知下端到端穿越

    Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-to-End. (arXiv:2310.03581v1 [cs.RO])

    [http://arxiv.org/abs/2310.03581](http://arxiv.org/abs/2310.03581)

    本文提出了一种基于强化学习的本地导航策略，通过将感知失效建模为障碍物和坑洞，从受损的感知中重建环境信息并进行端到端的反应，实现了在受损感知下的可靠导航。

    

    自主机器人必须在未知环境中可靠地导航，即使在受损的外感知或感知失效的情况下也是如此。本文将感知失效建模为看不见的障碍物和坑洞，并训练了一种基于强化学习的本地导航策略来引导我们的腿式机器人。与依赖启发式和异常检测来更新导航信息的先前工作不同，我们训练我们的导航策略以在潜在空间中从损坏的感知中重建环境信息并对感知失效进行端到端的反应。为此，我们将本体感知和外感知都纳入我们的策略输入中，从而使策略能够感知不同身体部位的碰撞和坑洞，并引发相应的反应。

    Autonomous robots must navigate reliably in unknown environments even under compromised exteroceptive perception, or perception failures. Such failures often occur when harsh environments lead to degraded sensing, or when the perception algorithm misinterprets the scene due to limited generalization. In this paper, we model perception failures as invisible obstacles and pits, and train a reinforcement learning (RL) based local navigation policy to guide our legged robot. Unlike previous works relying on heuristics and anomaly detection to update navigational information, we train our navigation policy to reconstruct the environment information in the latent space from corrupted perception and react to perception failures end-to-end. To this end, we incorporate both proprioception and exteroception into our policy inputs, thereby enabling the policy to sense collisions on different body parts and pits, prompting corresponding reactions. We validate our approach in simulation and on the 
    
[^17]: GFlowNet中的基因调控网络因果推断：大规模系统的可扩展性

    Causal Inference in Gene Regulatory Networks with GFlowNet: Towards Scalability in Large Systems. (arXiv:2310.03579v1 [cs.AI])

    [http://arxiv.org/abs/2310.03579](http://arxiv.org/abs/2310.03579)

    本文介绍了一个新的框架Swift-DynGFN，它提高了基因调控网络中因果结构的学习能力并解决了可扩展性问题。通过利用基因的独立性来增强并行性和降低计算成本，实验结果展示了在GRNs中学习因果结构和处理大规模系统方面的进展。

    

    理解基因调控网络（GRNs）内的因果关系对于揭示细胞过程中的基因相互作用至关重要。然而，GRNs中的因果发现是一个具有挑战性的问题，原因有多种，包括存在循环反馈环路和产生多样性可能的因果结构的不确定性。以前在这个领域的工作要么忽略了循环动力学（假设非循环结构），要么在可扩展性方面有困难。我们引入了Swift-DynGFN作为一种新颖的框架，它增强了GRNs中因果结构的学习，并解决了可扩展性问题。具体而言，Swift-DynGFN利用基因独立性来提高并行性和降低计算成本。在真实的单细胞RNA速度和合成GRN数据集上的实验展示了在学习GRNs中因果结构和大规模系统的可扩展性方面的进展。

    Understanding causal relationships within Gene Regulatory Networks (GRNs) is essential for unraveling the gene interactions in cellular processes. However, causal discovery in GRNs is a challenging problem for multiple reasons including the existence of cyclic feedback loops and uncertainty that yields diverse possible causal structures. Previous works in this area either ignore cyclic dynamics (assume acyclic structure) or struggle with scalability. We introduce Swift-DynGFN as a novel framework that enhances causal structure learning in GRNs while addressing scalability concerns. Specifically, Swift-DynGFN exploits gene-wise independence to boost parallelization and to lower computational cost. Experiments on real single-cell RNA velocity and synthetic GRN datasets showcase the advancement in learning causal structure in GRNs and scalability in larger systems.
    
[^18]: 朝着鲁棒和可推广的训练方法：对输入扰动的嘈杂槽填充的实证研究

    Towards Robust and Generalizable Training: An Empirical Study of Noisy Slot Filling for Input Perturbations. (arXiv:2310.03518v1 [cs.CL])

    [http://arxiv.org/abs/2310.03518](http://arxiv.org/abs/2310.03518)

    该论文研究了对于输入噪声的鲁棒和可推广的训练方法，在嘈杂的槽填充任务中进行了实证研究，并提出了一个噪声鲁棒性评估数据集和框架，通过实证实验验证了该框架的有效性。

    

    在实际对话场景中，由于话语中存在未知的输入噪声，现有的监督式槽填充模型在实际应用中的表现通常较差。虽然有一些关于噪声鲁棒性模型的研究，但这些研究只在基于规则的合成数据集上进行评估，这种限制性导致了噪声鲁棒方法研究的推广困难。本文引入了一个名为Noise-SF的噪声鲁棒性评估数据集，该数据集包含五种类型的人工标注噪声，并且所有这些噪声都确实存在于真实的大规模鲁棒性训练方法中。通过对Noise-SF进行了详尽的实证评估实验，我们发现基线模型在鲁棒性评估中表现较差，而所提出的框架可以有效提高模型的鲁棒性。基于实证实验结果，我们提出了一些前瞻性的建议来推动该领域的研究。

    In real dialogue scenarios, as there are unknown input noises in the utterances, existing supervised slot filling models often perform poorly in practical applications. Even though there are some studies on noise-robust models, these works are only evaluated on rule-based synthetic datasets, which is limiting, making it difficult to promote the research of noise-robust methods. In this paper, we introduce a noise robustness evaluation dataset named Noise-SF for slot filling task. The proposed dataset contains five types of human-annotated noise, and all those noises are exactly existed in real extensive robust-training methods of slot filling into the proposed framework. By conducting exhaustive empirical evaluation experiments on Noise-SF, we find that baseline models have poor performance in robustness evaluation, and the proposed framework can effectively improve the robustness of models. Based on the empirical experimental results, we make some forward-looking suggestions to fuel t
    
[^19]: 如何水平采样过程影响深度强化学习中的零样本泛化

    How the level sampling process impacts zero-shot generalisation in deep reinforcement learning. (arXiv:2310.03494v1 [cs.LG])

    [http://arxiv.org/abs/2310.03494](http://arxiv.org/abs/2310.03494)

    这项研究探讨了非均匀采样策略对深度强化学习代理的零样本泛化能力的影响，通过测量代理的内部表示与训练层级集之间的相互信息，发现基于值损失优先级的自适应采样策略能更好地避免过拟合。

    

    阻止广泛采用通过深度强化学习（RL）训练的自主代理代理的关键局限是它们有限的适应新环境能力，即使这些环境与训练中遇到的环境具有相似特征。本研究探讨了个体环境实例或层级的非均匀采样策略如何影响RL代理的零样本泛化（ZSG）能力，并考虑了两种失效模式：过拟合和过度泛化。首先，我们测量了代理的内部表示与训练层级集之间的相互信息（MI），发现MI与实例的过拟合相关性很强。与均匀采样相比，基于值损失优先级的自适应采样策略更能有效地保持较低的MI，这为这类技术提供了一种新的理论解释。接下来，我们将注意力转向无监督环境设计（U）

    A key limitation preventing the wider adoption of autonomous agents trained via deep reinforcement learning (RL) is their limited ability to generalise to new environments, even when these share similar characteristics with environments encountered during training. In this work, we investigate how a non-uniform sampling strategy of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents, considering two failure modes: overfitting and over-generalisation. As a first step, we measure the mutual information (MI) between the agent's internal representation and the set of training levels, which we find to be well-correlated to instance overfitting. In contrast to uniform sampling, adaptive sampling strategies prioritising levels based on their value loss are more effective at maintaining lower MI, which provides a novel theoretical justification for this class of techniques. We then turn our attention to unsupervised environment design (U
    
[^20]: Tik-to-Tok:一次翻译一个标记：一种用于有效语言适应的嵌入初始化策略

    Tik-to-Tok: Translating Language Models One Token at a Time: An Embedding Initialization Strategy for Efficient Language Adaptation. (arXiv:2310.03477v1 [cs.CL])

    [http://arxiv.org/abs/2310.03477](http://arxiv.org/abs/2310.03477)

    本研究提出了一种新颖的模型转换策略，通过将目标语言标记映射到语义相似的源语言标记，有效地改善了低资源和中资源语言训练单语言模型时的初始化过程，并在荷兰语和弗里斯兰语等多种语言上取得了新的最先进性能。

    

    在本研究中，我们提出了一种新颖的模型转换策略，来解决低资源和中资源语言训练单语言模型时由于训练数据有限和通常不足够的问题。通过在来源语言标记器和目标语言标记器之间建立一个包含源语言和目标语言单词翻译字典的泛化模型，我们将目标标记映射到语义相似的源语言标记。这种一对多的标记映射极大地改善了目标语言的嵌入表初始化。我们对高资源模型进行实验，将其转换为中资源和低资源语言，分别是荷兰语和弗里斯兰语。这些转换后的模型在这些语言的各种下游任务中达到了最新的性能水平。通过显著减少训练最新模型所需的数据和时间，我们的新颖模型转换策略大大提高了效率。

    Training monolingual language models for low and mid-resource languages is made challenging by limited and often inadequate pretraining data. In this study, we propose a novel model conversion strategy to address this issue, adapting high-resources monolingual language models to a new target language. By generalizing over a word translation dictionary encompassing both the source and target languages, we map tokens from the target tokenizer to semantically similar tokens from the source language tokenizer. This one-to-many token mapping improves tremendously the initialization of the embedding table for the target language. We conduct experiments to convert high-resource models to midand low-resource languages, namely Dutch and Frisian. These converted models achieve a new state-of-the-art performance on these languages across all sorts of downstream tasks. By reducing significantly the amount of data and time required for training state-of-the-art models, our novel model conversion 
    
[^21]: 一种使用深层反事实的定量可解释模型进行阿尔茨海默病预测

    A Quantitatively Interpretable Model for Alzheimer's Disease Prediction Using Deep Counterfactuals. (arXiv:2310.03457v1 [cs.AI])

    [http://arxiv.org/abs/2310.03457](http://arxiv.org/abs/2310.03457)

    本研究提出了一种使用深层反事实的定量可解释模型，用于预测阿尔茨海默病。通过合成反事实标记的结构性MRIs，并转化为灰质密度图来测量体积变化，实现了相当的预测性能，并促进了定量解释。

    

    深度学习（DL）对于预测阿尔茨海默病（AD）已经在疾病进展的及时干预方面提供了帮助，但仍然需要注意可解释性，以解释他们的DL模型如何做出明确的决策。最近，反事实推理在医学研究中得到了越来越多的关注，因为它能够提供一个细致的可视化解释图。然而，仅凭视觉检查和视觉解释图是不够的，除非我们通过定量特征直观地证明它们在医学或神经科学上的有效性。在本研究中，我们使用我们提出的框架合成了反事实标记的结构性MRIs，并将其转化为灰质密度图，以测量其在兴趣区域（ROI）的体积变化。我们还设计了一个轻量级线性分类器来提高构建的ROI的效果，促进定量解释，并实现与DL方法相当的预测性能。

    Deep learning (DL) for predicting Alzheimer's disease (AD) has provided timely intervention in disease progression yet still demands attentive interpretability to explain how their DL models make definitive decisions. Recently, counterfactual reasoning has gained increasing attention in medical research because of its ability to provide a refined visual explanatory map. However, such visual explanatory maps based on visual inspection alone are insufficient unless we intuitively demonstrate their medical or neuroscientific validity via quantitative features. In this study, we synthesize the counterfactual-labeled structural MRIs using our proposed framework and transform it into a gray matter density map to measure its volumetric changes over the parcellated region of interest (ROI). We also devised a lightweight linear classifier to boost the effectiveness of constructed ROIs, promoted quantitative interpretation, and achieved comparable predictive performance to DL methods. Throughout
    
[^22]: 预训练和微调生成性流网络

    Pre-Training and Fine-Tuning Generative Flow Networks. (arXiv:2310.03419v1 [cs.LG])

    [http://arxiv.org/abs/2310.03419](http://arxiv.org/abs/2310.03419)

    这个论文提出了一种新的方法来实现生成性流网络的无奖励预训练，并通过自监督问题的形式训练了一个条件的GFlowNet（OC-GFN），用于有效适应下游任务。

    

    生成性流网络（GFlowNets）是学习从给定的非标准化奖励分布中顺序生成复合对象的随机策略的摊销采样器。它们可以生成多样化的高奖励对象，在科学发现任务中是一个重要考虑因素。然而，由于它们通常是根据给定的外在奖励函数进行训练的，关于如何利用预训练的力量以及以无监督的方式训练GFlowNets以实现对下游任务的高效自适应仍然是一个重要的挑战。受无监督预训练在各个领域的最新成功启发，我们引入了一种新的GFlowNets无奖励预训练的方法。通过将训练构建为一个自监督问题，我们提出了一种条件GFlowNet（OC-GFN），它学习探索候选空间。具体而言，OC-GFN学习达到任何目标结果，类似于强化学习中的目标条件策略。我们证明了

    Generative Flow Networks (GFlowNets) are amortized samplers that learn stochastic policies to sequentially generate compositional objects from a given unnormalized reward distribution. They can generate diverse sets of high-reward objects, which is an important consideration in scientific discovery tasks. However, as they are typically trained from a given extrinsic reward function, it remains an important open challenge about how to leverage the power of pre-training and train GFlowNets in an unsupervised fashion for efficient adaptation to downstream tasks. Inspired by recent successes of unsupervised pre-training in various domains, we introduce a novel approach for reward-free pre-training of GFlowNets. By framing the training as a self-supervised problem, we propose an outcome-conditioned GFlowNet (OC-GFN) that learns to explore the candidate space. Specifically, OC-GFN learns to reach any targeted outcomes, akin to goal-conditioned policies in reinforcement learning. We show that
    
[^23]: GRAPES: 学习用于可扩展图神经网络的图采样

    GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks. (arXiv:2310.03399v1 [cs.LG])

    [http://arxiv.org/abs/2310.03399](http://arxiv.org/abs/2310.03399)

    GRAPES是一种自适应图采样方法，通过学习识别在训练图神经网络分类器时具有影响力的节点集合，解决了可扩展图神经网络中的内存问题，并且在准确性和可扩展性方面表现出色。

    

    图神经网络（GNNs）通过以不同方式聚合周围信息来学习图中节点的表示。随着这些网络的加深，由于邻域尺寸的增加，它们的感受野呈指数增长，导致高内存消耗。图采样通过对图中节点进行抽样来解决GNNs中的内存问题。通过这种方式，GNNs可以扩展到更大的图。大多数采样方法专注于固定的采样启发式算法，这可能无法推广到不同的结构或任务。我们引入了GRAPES，一种自适应的图采样方法，该方法学习识别用于训练GNN分类器的一组具有影响力的节点。GRAPES使用GFlowNet来学习给定分类目标的节点采样概率。我们在几个小规模和大规模图基准上评估了GRAPES，并展示了其在准确性和可扩展性方面的有效性。与现有的采样方法相比，GRAPES即使在采样比例较低的情况下仍保持高准确性。

    Graph neural networks (GNNs) learn the representation of nodes in a graph by aggregating the neighborhood information in various ways. As these networks grow in depth, their receptive field grows exponentially due to the increase in neighborhood sizes, resulting in high memory costs. Graph sampling solves memory issues in GNNs by sampling a small ratio of the nodes in the graph. This way, GNNs can scale to much larger graphs. Most sampling methods focus on fixed sampling heuristics, which may not generalize to different structures or tasks. We introduce GRAPES, an adaptive graph sampling method that learns to identify sets of influential nodes for training a GNN classifier. GRAPES uses a GFlowNet to learn node sampling probabilities given the classification objectives. We evaluate GRAPES across several small- and large-scale graph benchmarks and demonstrate its effectiveness in accuracy and scalability. In contrast to existing sampling methods, GRAPES maintains high accuracy even with 
    
[^24]: 在安全关键行业中解构人工智能与人类互动：系统文献综述

    Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review. (arXiv:2310.03392v1 [cs.HC])

    [http://arxiv.org/abs/2310.03392](http://arxiv.org/abs/2310.03392)

    在安全关键行业中确保高质量的人工智能与人类互动（HAII）十分重要，然而目前对于HAII的研究零散且缺乏一致性。本论文通过系统文献综述，发现没有一个一致使用的术语描述HAII，并且五个因素影响HAII，即用户特征和背景、AI界面和功能、用户信任、监控和反馈，以及工作上的团队结构。这些发现为改进安全关键行业中的HAII提供了指导。

    

    在安全关键行业中确保高质量的人工智能与人类互动（HAII）是至关重要的。不能做到这一点可能导致灾难性和致命的后果。尽管如此紧迫，关于HAII的研究很少且零散，且缺乏一致性。我们在这里提出了对该领域的文献综述和改进研究最佳实践的建议。我们将我们的调查分为以下研究领域：（1）用于描述HAII的术语，（2）AI-enabled系统的主要角色，（3）影响HAII的因素，以及（4）如何衡量HAII。此外，我们描述了在这些文章中讨论的安全关键行业中使用的AI-enabled系统的能力和成熟度。我们发现在文献中没有一个术语被一致使用来描述HAII，而一些术语具有多个含义。根据我们的文献，有五个因素影响HAII：用户特征和背景（例如，用户个性，感知），AI界面和功能（例如，

    Ensuring quality human-AI interaction (HAII) in safety-critical industries is essential. Failure to do so can lead to catastrophic and deadly consequences. Despite this urgency, what little research there is on HAII is fragmented and inconsistent. We present here a survey of that literature and recommendations for research best practices that will improve the field. We divided our investigation into the following research areas: (1) terms used to describe HAII, (2) primary roles of AI-enabled systems, (3) factors that influence HAII, and (4) how HAII is measured. Additionally, we described the capabilities and maturity of the AI-enabled systems used in safety-critical industries discussed in these articles. We found that no single term is used across the literature to describe HAII and some terms have multiple meanings. According to our literature, five factors influence HAII: user characteristics and background (e.g., user personality, perceptions), AI interface and features (e.g., in
    
[^25]: 使用大型语言模型的程序性文本挖掘

    Procedural Text Mining with Large Language Models. (arXiv:2310.03376v1 [cs.CL])

    [http://arxiv.org/abs/2310.03376](http://arxiv.org/abs/2310.03376)

    本文研究了使用大型语言模型进行程序性文本挖掘的方法。通过在零样本和上下文学习环境中使用GPT-4模型和自定义技术，有效地从非结构化PDF文本中提取程序。实验结果证明了该方法的潜力和价值。

    

    最近在自然语言处理领域取得的进展，特别是在预训练了大量知识的大规模语言模型的发展，为知识工程领域带来了新机遇。本文主要研究了在零样本学习和上下文学习环境中使用大语言模型(LLMs)来解决从非结构化PDF文本中以增量问答方式提取程序的问题。具体而言，我们利用了目前最先进的GPT-4 (Generative Pre-trained Transformer 4)模型，结合了两种上下文学习的变体，包括带有程序和步骤定义的本体和有限数量的少样本学习。研究结果突出了这种方法的潜力和上下文学习定制化的价值。这些修改有望显著解决获取足够训练的挑战。

    Recent advancements in the field of Natural Language Processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of Knowledge Engineering. In this paper, we investigate the usage of large language models (LLMs) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured PDF text in an incremental question-answering fashion. In particular, we leverage the current state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. The findings highlight both the promise of this approach and the value of the in-context learning customisations. These modifications have the potential to significantly address the challenge of obtaining sufficient training 
    
[^26]: 平面软增长机器人操纵器的设计优化程序

    Design Optimizer for Planar Soft-Growing Robot Manipulators. (arXiv:2310.03374v1 [cs.RO])

    [http://arxiv.org/abs/2310.03374](http://arxiv.org/abs/2310.03374)

    本文提出了一种设计优化方法，通过优化软增长机器人的运动链，提供最佳机器人尺寸以解决特定任务。这种方法利用基于群体的优化算法，避免了不必要的材料和资源浪费。

    

    软增长机器人是一种创新设备，具有植物启发的生长特性来导航环境。得益于其适应环境的体外智能和激励与制造的最新创新，可以将它们用于特定的操纵任务。这些设备的应用包括对脆弱/危险环境的探索、物体的操纵或在家庭环境中的协助。本研究提出了一种新颖的软增长机器人设计优化方法，它将在制造之前用于建议工程师或机器人设计爱好者构建解决特定任务的最佳机器人尺寸。我将设计过程建模为一个多目标优化问题，通过优化软操纵器的运动链来达到目标并避免不必要的材料和资源浪费。该方法充分利用了基于群体的优化算法的优势。

    Soft-growing robots are innovative devices that feature plant-inspired growth to navigate environments. Thanks to their embodied intelligence of adapting to their surroundings and the latest innovation in actuation and manufacturing, it is possible to employ them for specific manipulation tasks. The applications of these devices include exploration of delicate/dangerous environments, manipulation of items, or assistance in domestic environments.  This work presents a novel approach for design optimization of soft-growing robots, which will be used prior to manufacturing to suggest engineers -- or robot designer enthusiasts -- the optimal dimension of the robot to be built for solving a specific task. I modeled the design process as a multi-objective optimization problem, in which I optimize the kinematic chain of a soft manipulator to reach targets and avoid unnecessary overuse of material and resources. The method exploits the advantages of population-based optimization algorithms, in
    
[^27]: Swin-Tempo: 使用Swin Transformer-Enhanced UNet将CT扫描中的肺结节检测作为视频序列进行时间感知

    Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video Sequences Using Swin Transformer-Enhanced UNet. (arXiv:2310.03365v1 [eess.IV])

    [http://arxiv.org/abs/2310.03365](http://arxiv.org/abs/2310.03365)

    Swin-Tempo是一个创新模型，使用了Swin Transformer-Enhanced UNet将CT扫描中的肺结节检测作为视频序列进行时间感知。它克服了现有网络的计算复杂性问题，提高了肺结节检测的准确率。

    

    肺癌具有极高的致死率，早期检测对于防治非常重要。然而，对于放射科医生而言，识别肺结节存在重大挑战，他们往往依赖自己的专业知识和经验来进行准确的诊断。为解决这个问题，基于机器学习技术的计算机辅助诊断系统已经出现，帮助医生从计算机断层扫描（CT）图像中识别肺结节。然而，现有的网络往往存在计算复杂性问题，导致误报和漏报率较高，限制了它们的有效性。为应对这些挑战，我们提出了一种创新模型，结合了卷积神经网络和视觉Transformer的优势。受视频中的目标检测启发，我们将每个3D CT图像视为一个视频，将每个切片视为帧，将肺结节视为目标，实现一个时序应用。我们的工作的主要目标是克服硬件限制。

    Lung cancer is highly lethal, emphasizing the critical need for early detection. However, identifying lung nodules poses significant challenges for radiologists, who rely heavily on their expertise and experience for accurate diagnosis. To address this issue, computer-aided diagnosis systems based on machine learning techniques have emerged to assist doctors in identifying lung nodules from computed tomography (CT) scans. Unfortunately, existing networks in this domain often suffer from computational complexity, leading to high rates of false negatives and false positives, limiting their effectiveness. To address these challenges, we present an innovative model that harnesses the strengths of both convolutional neural networks and vision transformers. Inspired by object detection in videos, we treat each 3D CT image as a video, individual slices as frames, and lung nodules as objects, enabling a time-series application. The primary objective of our work is to overcome hardware limitati
    
[^28]: 通过非对称负对比度和反向注意力进行鲁棒表征学习

    Robust Representation Learning via Asymmetric Negative Contrast and Reverse Attention. (arXiv:2310.03358v1 [cs.CV])

    [http://arxiv.org/abs/2310.03358](http://arxiv.org/abs/2310.03358)

    本文提出了一个通用的对抗训练（AT）框架，通过非对称负对比度和反向注意力，学习鲁棒的特征表征，以提高神经网络的对抗鲁棒性能。

    

    深度神经网络对抗性噪声容易受到攻击。对抗训练（AT）被证明是保护神经网络免受欺骗的最有效的防御策略。然而，我们发现AT忽视了学习鲁棒特征，导致对抗鲁棒性能较差。为了解决这个问题，我们强调了鲁棒表征的两个特征：（1）排他性：自然样本的特征远离其他类别的特征；（2）对齐性：自然样本和相应的对抗样本的特征彼此接近。这些特点激发我们提出了一个通用的AT框架，通过非对称负对比度和反向注意力来获得鲁棒的表征。具体而言，我们设计了一个基于预测概率的非对称负对比度，将特征空间中不同类别的样本推开。此外，我们提出使用线性分类器的参数对特征进行加权，作为反向注意力，以获得鲁棒的表征。

    Deep neural networks are vulnerable to adversarial noise. Adversarial training (AT) has been demonstrated to be the most effective defense strategy to protect neural networks from being fooled. However, we find AT omits to learning robust features, resulting in poor performance of adversarial robustness. To address this issue, we highlight two characteristics of robust representation: (1) $\bf{exclusion}$: the feature of natural examples keeps away from that of other classes; (2) $\bf{alignment}$: the feature of natural and corresponding adversarial examples is close to each other. These motivate us to propose a generic framework of AT to gain robust representation, by the asymmetric negative contrast and reverse attention. Specifically, we design an asymmetric negative contrast based on predicted probabilities, to push away examples of different classes in the feature space. Moreover, we propose to weight feature by parameters of the linear classifier as the reverse attention, to obta
    
[^29]: 虚构交叉玩: 在混合合作竞争游戏中学习全局纳什均衡

    Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed Cooperative-Competitive Games. (arXiv:2310.03354v1 [cs.AI])

    [http://arxiv.org/abs/2310.03354](http://arxiv.org/abs/2310.03354)

    该论文介绍了自我对弈和策略空间响应预测（PSRO）作为解决竞争游戏的强化学习框架，发现自我对弈方法在混合合作竞争游戏中无法收敛到全局纳什均衡（NE），而PSRO能够在这种情况下有效地学习到最佳响应。然而，PSRO需要重复训练联合策略，增加了难度。

    

    自我对弈（SP）是一种常用的多智能体强化学习（MARL）框架，用于解决竞争游戏，在这种框架下，每个智能体通过将其他智能体视为环境的一部分来优化策略。尽管在实证研究中取得了成功，但是SP方法的理论性质仅限于两人零和游戏。然而，在混合合作竞争游戏中，需要团队中的智能体相互合作，我们可以通过一个简单的反例来证明SP方法无法以高概率收敛到全局纳什均衡（NE）。作为替代方法，策略空间响应预测（PSRO）是一种学习NE的迭代框架，其中在每次迭代中学习相对于先前策略的最佳响应。PSRO可以直接扩展为混合合作竞争场景，同时学习团队最佳响应而所有收敛性质均保持不变。然而，PSRO需要重复从头开始训练联合策略直到收敛，这使得它变得困难。

    Self-play (SP) is a popular multi-agent reinforcement learning (MARL) framework for solving competitive games, where each agent optimizes policy by treating others as part of the environment. Despite the empirical successes, the theoretical properties of SP-based methods are limited to two-player zero-sum games. However, for mixed cooperative-competitive games where agents on the same team need to cooperate with each other, we can show a simple counter-example where SP-based methods cannot converge to a global Nash equilibrium (NE) with high probability. Alternatively, Policy-Space Response Oracles (PSRO) is an iterative framework for learning NE, where the best responses w.r.t. previous policies are learned in each iteration. PSRO can be directly extended to mixed cooperative-competitive settings by jointly learning team best responses with all convergence properties unchanged. However, PSRO requires repeatedly training joint policies from scratch till convergence, which makes it hard
    
[^30]: 采用单调性约束的深度几何学习在阿尔茨海默病进展中的应用

    Deep Geometric Learning with Monotonicity Constraints for Alzheimer's Disease Progression. (arXiv:2310.03353v1 [cs.AI])

    [http://arxiv.org/abs/2310.03353](http://arxiv.org/abs/2310.03353)

    本文提出了一种采用单调性约束的深度几何学习方法来预测阿尔茨海默病（AD）的进展。这种方法通过结合递归神经网络和普通微分方程在黎曼空间中建模时间序列数据，弥补了现有方法对数据几何属性考虑不足的问题，并解决了从不完整样本中外推正定对称度量的限制。这项工作在临床诊断和治疗中具有重要的应用价值。

    

    阿尔茨海默病（AD）是一种毁灭性的神经退行性疾病，其在渐进和不可逆的痴呆前出现；因此，对其随时间的进展进行预测对于临床诊断和治疗至关重要。许多研究已经采用结构性磁共振成像（MRI）来模拟AD的进展，重点关注三个关键方面：（i）时间变异性，（ii）不完整观测，和（iii）时间几何特征。然而，关于数据变异性和稀疏性的深度学习方法尚未充分考虑其固有的几何特性。基于普通微分方程的几何建模方法（ODE-RGRU）最近作为一种有前途的策略在黎曼空间中通过将递归神经网络和ODE交织在一起来建模时间序列数据。尽管取得了一定成就，但ODE-RGRU在从不完整样本中外推正定对称度量时面临着一些限制，导致特征反转的发生。

    Alzheimer's disease (AD) is a devastating neurodegenerative condition that precedes progressive and irreversible dementia; thus, predicting its progression over time is vital for clinical diagnosis and treatment. Numerous studies have implemented structural magnetic resonance imaging (MRI) to model AD progression, focusing on three integral aspects: (i) temporal variability, (ii) incomplete observations, and (iii) temporal geometric characteristics. However, deep learning-based approaches regarding data variability and sparsity have yet to consider inherent geometrical properties sufficiently. The ordinary differential equation-based geometric modeling method (ODE-RGRU) has recently emerged as a promising strategy for modeling time-series data by intertwining a recurrent neural network and an ODE in Riemannian space. Despite its achievements, ODE-RGRU encounters limitations when extrapolating positive definite symmetric metrics from incomplete samples, leading to feature reverse occurr
    
[^31]: 可知识编译的可控制的反事实查询边界问题

    Tractable Bounding of Counterfactual Queries by Knowledge Compilation. (arXiv:2310.03352v1 [cs.AI])

    [http://arxiv.org/abs/2310.03352](http://arxiv.org/abs/2310.03352)

    这项研究讨论了在皮尔斯结构性因果模型中限制部分可识别查询的问题，并提出了一种基于知识编译的方法，通过符号参数替换和并行化技术加速界限计算，在实验中取得了明显的计算优势。

    

    我们讨论了在皮尔斯结构性因果模型中限制部分可识别查询（例如反事实）的问题。最近提出的迭代EM方案通过对初始化参数进行采样来获得这些界限的内部近似。这种方法需要对共享相同结构方程和拓扑结构但具有不同外生概率的模型进行多个（贝叶斯网络）查询。这种设置使得将底层模型编译为算术电路变得有利，从而实现了可观的推理加速。我们展示了如何通过单一的符号知识编译来获得电路结构，并在计算不同查询时用实际值替换符号参数。我们还讨论了进一步加速界限计算的并行化技术。与标准贝叶斯网络推理相比，实验证明了明显的计算优势，具有高达一个数量级的加速。

    We discuss the problem of bounding partially identifiable queries, such as counterfactuals, in Pearlian structural causal models. A recently proposed iterated EM scheme yields an inner approximation of those bounds by sampling the initialisation parameters. Such a method requires multiple (Bayesian network) queries over models sharing the same structural equations and topology, but different exogenous probabilities. This setup makes a compilation of the underlying model to an arithmetic circuit advantageous, thus inducing a sizeable inferential speed-up. We show how a single symbolic knowledge compilation allows us to obtain the circuit structure with symbolic parameters to be replaced by their actual values when computing the different queries. We also discuss parallelisation techniques to further speed up the bound computation. Experiments against standard Bayesian network inference show clear computational advantages with up to an order of magnitude of speed-up.
    
[^32]: 学习基于概念的视觉因果转换和符号推理用于视觉规划

    Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning. (arXiv:2310.03325v1 [cs.AI])

    [http://arxiv.org/abs/2310.03325](http://arxiv.org/abs/2310.03325)

    本文提出了一种面向视觉规划的可解释和可推广的框架，通过将视觉输入转化为概念表示、符号抽象和推理以及将视觉因果转换与真实世界行为关联，实现了目标条件的视觉规划。

    

    视觉规划模拟了人类在搜索初始视觉状态和最终视觉目标状态之间的视觉因果转换来实现期望目标时所做的决策过程。在以自我为中心的视觉中，视觉规划越来越重要，因为它在引导智能体在复杂环境中执行日常任务方面具有优势。本文提出了一个可解释和可推广的视觉规划框架，包括：i）一种新颖的基于替代的概念学习器（SCL），将视觉输入转化为分解的概念表示；ii）通过自学符号进行任务规划的符号抽象和推理；iii）将视觉因果转换与语义相似的真实世界行为进行关联的视觉因果转换模型（ViCT）。给定一个初始状态，我们通过学习到的表示和因果转换的符号推理方法进行目标条件的视觉规划，以达到目标状态。

    Visual planning simulates how humans make decisions to achieve desired goals in the form of searching for visual causal transitions between an initial visual state and a final visual goal state. It has become increasingly important in egocentric vision with its advantages in guiding agents to perform daily tasks in complex environments. In this paper, we propose an interpretable and generalizable visual planning framework consisting of i) a novel Substitution-based Concept Learner (SCL) that abstracts visual inputs into disentangled concept representations, ii) symbol abstraction and reasoning that performs task planning via the self-learned symbols, and iii) a Visual Causal Transition model (ViCT) that grounds visual causal transitions to semantically similar real-world actions. Given an initial state, we perform goal-conditioned visual planning with a symbolic reasoning method fueled by the learned representations and causal transitions to reach the goal state. To verify the effectiv
    
[^33]: 使用受限概率人体动作预测增强人机协作

    Enhanced Human-Robot Collaboration using Constrained Probabilistic Human-Motion Prediction. (arXiv:2310.03314v1 [cs.RO])

    [http://arxiv.org/abs/2310.03314](http://arxiv.org/abs/2310.03314)

    本研究提出一种新颖的人体运动预测框架，结合人体关节约束和场景约束，利用高斯过程回归模型预测一定时间范围内的人体动作，以增强人机协作。

    

    人体动作预测是实现高效安全的人机协作的必要步骤。目前的方法要么完全依赖于将人类关节点表示为某种神经网络结构，要么使用离线回归模型来拟合超参数，以捕捉包含人体动作的模型。虽然这些方法提供了良好的初始结果，但它们未能利用研究充分的人体运动学模型以及身体和场景约束，这些约束有助于提高这些预测框架的效果，同时明确避免不合理的人体关节配置。我们提出了一种新颖的人体运动预测框架，该框架将人体关节约束和场景约束结合在高斯过程回归（GPR）模型中，以预测一定时间范围内的人体动作。此公式与在线上下文感知约束模型结合，以利用任务相关运动。在人体臂运动学模型上进行了测试。

    Human motion prediction is an essential step for efficient and safe human-robot collaboration. Current methods either purely rely on representing the human joints in some form of neural network-based architecture or use regression models offline to fit hyper-parameters in the hope of capturing a model encompassing human motion. While these methods provide good initial results, they are missing out on leveraging well-studied human body kinematic models as well as body and scene constraints which can help boost the efficacy of these prediction frameworks while also explicitly avoiding implausible human joint configurations. We propose a novel human motion prediction framework that incorporates human joint constraints and scene constraints in a Gaussian Process Regression (GPR) model to predict human motion over a set time horizon. This formulation is combined with an online context-aware constraints model to leverage task-dependent motions. It is tested on a human arm kinematic model and
    
[^34]: 简明有序的感知有助于大型语言模型进行演绎推理

    Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning. (arXiv:2310.03309v1 [cs.CL])

    [http://arxiv.org/abs/2310.03309](http://arxiv.org/abs/2310.03309)

    利用大型语言模型进行演绎推理是一个具有挑战性的问题。这篇论文提出了一个简明有序的方法，将任务分解为子任务并且人类化地组织思维，以提高演绎推理的效果。

    

    利用大型语言模型（LLMs）解决演绎推理问题已经引起了越来越多的关注。在复杂的演绎问题中仍然很难取得令人满意的结果，这类问题具有大量前提（即事实或规则），其中涉及实体之间错综复杂的关系，需要进行多跳推理。一种直观的解决方案是将原始任务分解为较小的子任务，然后以前向（例如选择-推理）或反向（例如LAMBADA）方式将多个因果推理步骤连接在一起。然而，这些技术不可避免地需要大量的总体阶段，导致计算开销大，并且有更高的可能性产生误导性的步骤。除了逐阶段分解之外，我们还从人类问题解决的另一个方面获得了启发。人类倾向于提炼出最相关的信息并有序地组织思维（例如创建思维导图），这有助于他们对问题进行有效的推理。

    Exploiting large language models (LLMs) to tackle deductive reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex deductive problems, characterized by plenty of premises (i.e., facts or rules) entailing intricate relationships among entities and requiring multi-hop reasoning. One intuitive solution is to decompose the original task into smaller sub-tasks, and then chain the multiple casual reasoning steps together in a forward (e.g., Selection-Inference) or backward (e.g., LAMBADA) direction. However, these techniques inevitably necessitate a large number of overall stages, leading to computationally expensive operations and a higher possibility of making misleading steps. In addition to stage-by-stage decomposition, we draw inspiration from another aspect of human problem-solving. Humans tend to distill the most relevant information and organize their thoughts systematically (e.g., creating mind maps), which assists th
    
[^35]: 将大型语言模型作为AI研究代理进行基准测试

    Benchmarking Large Language Models As AI Research Agents. (arXiv:2310.03302v1 [cs.LG])

    [http://arxiv.org/abs/2310.03302](http://arxiv.org/abs/2310.03302)

    本研究提出了MLAgentBench，一个用于对AI研究代理进行基准测试的ML任务套件，代理可以执行各种操作，从而运行实验、分析结果并修改整个机器学习流程的代码。这可以帮助我们构建和评估能够执行长期目标任务的AI研究代理。

    

    科学实验涉及创建假设、设计实验、运行实验和分析结果的迭代过程。我们能否构建AI研究代理来执行这些长期目标的任务呢？为了朝着在此类开放性决策任务上构建和评估研究代理的目标迈出一步，我们着眼于机器学习工程问题：给定一个任务描述和数据集，构建一个高性能模型。在本文中，我们提出了MLAgentBench，一个用于对AI研究代理进行基准测试的ML任务套件。代理可以执行读写文件、执行代码和检查输出等动作。通过这些动作，代理可以运行实验、分析结果，并修改整个机器学习流程的代码，如数据处理、架构、训练过程等。然后，基准测试自动客观地评估代理在与性能和效率相关的各种指标上的表现。我们还设计了一个LLM-

    Scientific experimentation involves an iterative process of creating hypotheses, designing experiments, running experiments, and analyzing the results. Can we build AI research agents to perform these long-horizon tasks? To take a step towards building and evaluating research agents on such open-ended decision-making tasks, we focus on the problem of machine learning engineering: given a task description and a dataset, build a high-performing model. In this paper, we propose MLAgentBench, a suite of ML tasks for benchmarking AI research agents. Agents can perform actions like reading/writing files, executing code, and inspecting outputs. With these actions, agents could run experiments, analyze the results, and modify the code of entire machine learning pipelines, such as data processing, architecture, training processes, etc. The benchmark then automatically evaluates the agent's performance objectively over various metrics related to performance and efficiency. We also design an LLM-
    
[^36]: LightSeq：用于长上下文转换器分布式训练的序列级并行ism

    LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers. (arXiv:2310.03294v1 [cs.LG])

    [http://arxiv.org/abs/2310.03294](http://arxiv.org/abs/2310.03294)

    LightSeq是用于长上下文转换器分布式训练的一种新方法，它通过序列维度进行分区，与不同注意力头数量的模型架构兼容，并且减少了与Megatron-LM相比的通信量，同时还实现了通信和计算的重叠。

    

    增加大型语言模型（LLM）的上下文长度可以解开基本上新的能力，但也显著增加了训练的内存占用。以往的模型并行系统（例如Megatron-LM）对不同的注意力头进行分区和计算，并行处理，导致大量通信量，因此不能在注意力头数量之外扩展，从而阻碍了其采用。本文提出了一种新方法LightSeq，用于长上下文LLM的训练。LightSeq具有许多显著优势。首先，LightSeq通过序列维度进行分区，因此对于具有不同注意力头数量的模型架构是不可知的，适用于Multi-Head，Multi-Query和Grouped-Query attention等模型。其次，LightSeq与Megatron-LM相比，在流行的LLM上不仅需求少至4.7倍的通信，而且还可以将通信与计算重叠。为了进一步减少训练时间，LightSeq还具有一种新的梯度che

    Increasing the context length of large language models (LLMs) unlocks fundamentally new capabilities, but also significantly increases the memory footprints of training. Previous model-parallel systems such as Megatron-LM partition and compute different attention heads in parallel, resulting in large communication volumes, so they cannot scale beyond the number of attention heads, thereby hindering its adoption. In this paper, we introduce a new approach, LightSeq, for long-context LLMs training. LightSeq has many notable advantages. First, LightSeq partitions over the sequence dimension, hence is agnostic to model architectures and readily applicable for models with varying numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query attention. Second, LightSeq not only requires up to 4.7x less communication than Megatron-LM on popular LLMs but also overlaps the communication with computation. To further reduce the training time, LightSeq features a novel gradient che
    
[^37]: Sok：从高级自然语言要求生成访问控制策略

    SoK: Access Control Policy Generation from High-level Natural Language Requirements. (arXiv:2310.03292v1 [cs.CR])

    [http://arxiv.org/abs/2310.03292](http://arxiv.org/abs/2310.03292)

    该论文分析了49篇文章，并总结了现有图形化策略配置工具和自动策略生成框架的局限性。研究的目的是开发更有效的访问控制策略生成解决方案，避免访问控制失败。

    

    管理员中心的访问控制失败可能会导致数据泄露，使组织面临财务损失和声誉损害的风险。现有的图形化策略配置工具和自动策略生成框架试图通过避免此类故障来帮助管理员配置和生成访问控制策略。然而，图形化策略配置工具容易出现人为错误，使其无法使用。另一方面，自动策略生成框架容易发生错误预测，使其不可靠。因此，为了寻找改进其可用性和可靠性的方法，我们进行了一项系统文献综述，分析了49篇文章，以识别这些工具、框架及其限制。识别这些限制将有助于开发有效的访问控制策略生成解决方案，同时避免访问控制失败。

    Administrator-centered access control failures can cause data breaches, putting organizations at risk of financial loss and reputation damage. Existing graphical policy configuration tools and automated policy generation frameworks attempt to help administrators configure and generate access control policies by avoiding such failures. However, graphical policy configuration tools are prone to human errors, making them unusable. On the other hand, automated policy generation frameworks are prone to erroneous predictions, making them unreliable. Therefore, to find ways to improve their usability and reliability, we conducted a Systematic Literature Review analyzing 49 publications, to identify those tools, frameworks, and their limitations. Identifying those limitations will help develop effective access control policy generation solutions while avoiding access control failures.
    
[^38]: 一种用于解码mRNA的5' UTR语言模型和功能预测的研究

    A 5' UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions. (arXiv:2310.03281v1 [cs.LG])

    [http://arxiv.org/abs/2310.03281](http://arxiv.org/abs/2310.03281)

    这种研究引入了一种新的语言模型UTR-LM，通过对多个物种的5' UTR进行预训练，并结合有监督信息，该模型在多个下游任务中的表现超过了现有的最佳模型，可以有效预测平均核糖体负载、翻译效率、mRNA表达水平，并改进了内源性核糖体进入位点的识别性能。

    

    5' UTR是mRNA分子开端的调控区域，在调控翻译过程和影响蛋白表达水平方面起着关键作用。语言模型已经展示了在解码蛋白质和基因组序列功能方面的有效性。在这里，我们引入了一种用于5' UTR的语言模型，称为UTR-LM。UTR-LM在多个物种的内源性5' UTR上进行了预训练，并进一步加入了包括二级结构和最小自由能在内的有监督信息。我们对UTR-LM进行了各种下游任务的微调。模型在预测平均核糖体负载上的表现超过了已知的最佳基准模型最多42%，同时在预测翻译效率和mRNA表达水平上的表现提升了最多60%。该模型还可以用于识别未注释的内源性核糖体进入位点，并将AUPR与最佳基准模型相比从0.37提高至0.52。此外，我们设计了一个...

    The 5' UTR, a regulatory region at the beginning of an mRNA molecule, plays a crucial role in regulating the translation process and impacts the protein expression level. Language models have showcased their effectiveness in decoding the functions of protein and genome sequences. Here, we introduced a language model for 5' UTR, which we refer to as the UTR-LM. The UTR-LM is pre-trained on endogenous 5' UTRs from multiple species and is further augmented with supervised information including secondary structure and minimum free energy. We fine-tuned the UTR-LM in a variety of downstream tasks. The model outperformed the best-known benchmark by up to 42% for predicting the Mean Ribosome Loading, and by up to 60% for predicting the Translation Efficiency and the mRNA Expression Level. The model also applies to identifying unannotated Internal Ribosome Entry Sites within the untranslated region and improves the AUPR from 0.37 to 0.52 compared to the best baseline. Further, we designed a li
    
[^39]: 使用可传输的图自编码器进行网络对齐

    Network Alignment with Transferable Graph Autoencoders. (arXiv:2310.03272v1 [cs.LG])

    [http://arxiv.org/abs/2310.03272](http://arxiv.org/abs/2310.03272)

    该论文提出了一种基于图自编码器的网络对齐方法，通过生成与图的特征值和特征向量相关的节点嵌入，实现了更准确的对齐。同时，该方法还利用迁移学习和数据增强技术，在大规模网络对齐任务中实现高效的对齐，无需重新训练。

    

    网络对齐是在不同图之间建立一对一对应关系的任务，在高影响领域中有大量应用。然而，这个任务在一般情况下被认为是NP难的，而且现有的算法在图的规模增大时无法扩展。为了解决这两个挑战，我们提出了一种新颖的广义图自编码器架构，旨在提取强大且鲁棒的节点嵌入，适用于对齐任务。我们证明生成的嵌入与图的特征值和特征向量相关，并且与经典谱方法相比可以实现更准确的对齐。我们提出的框架还利用迁移学习和数据增强，在无需重新训练的情况下实现高效的大规模网络对齐。在真实世界的图上进行了广泛的网络对齐和子网络对齐实验，提供了支持该框架有效性和可扩展性的证据。

    Network alignment is the task of establishing one-to-one correspondences between the nodes of different graphs and finds a plethora of applications in high-impact domains. However, this task is known to be NP-hard in its general form, and existing algorithms do not scale up as the size of the graphs increases. To tackle both challenges we propose a novel generalized graph autoencoder architecture, designed to extract powerful and robust node embeddings, that are tailored to the alignment task. We prove that the generated embeddings are associated with the eigenvalues and eigenvectors of the graphs and can achieve more accurate alignment compared to classical spectral methods. Our proposed framework also leverages transfer learning and data augmentation to achieve efficient network alignment at a very large scale without retraining. Extensive experiments on both network and sub-network alignment with real-world graphs provide corroborating evidence supporting the effectiveness and scala
    
[^40]: 稀疏深度学习用于时间序列数据：理论与应用

    Sparse Deep Learning for Time Series Data: Theory and Applications. (arXiv:2310.03243v1 [stat.ML])

    [http://arxiv.org/abs/2310.03243](http://arxiv.org/abs/2310.03243)

    本文研究了稀疏深度学习在依赖数据（如时间序列数据）上的理论和应用。通过研究，我们发现稀疏循环神经网络能够一致地估计，并对其预测进行正确的不确定性量化。数值实验结果显示，稀疏深度学习在预测不确定性方面优于最先进方法。

    

    稀疏深度学习已成为提升深度神经网络在不确定性量化、变量选择和大规模网络压缩等领域性能的流行技术。然而，大部分现有研究都集中在观测相互独立且同分布（i.i.d.）的问题上，并且在涉及时间序列数据和自然语言处理中的顺序数据等观测相互依赖的问题上几乎没有相关工作。本文旨在填补这一空白，研究具有依赖数据的稀疏深度学习的理论。我们证明了稀疏循环神经网络（RNN）可以一致地估计，并且它们的预测在适当的假设下渐近地服从正态分布，从而能够正确量化预测不确定性。我们的数值实验结果表明，稀疏深度学习在预测不确定性量化方面胜过了诸如依照性预测等最先进方法。

    Sparse deep learning has become a popular technique for improving the performance of deep neural networks in areas such as uncertainty quantification, variable selection, and large-scale network compression. However, most existing research has focused on problems where the observations are independent and identically distributed (i.i.d.), and there has been little work on the problems where the observations are dependent, such as time series data and sequential data in natural language processing. This paper aims to address this gap by studying the theory for sparse deep learning with dependent data. We show that sparse recurrent neural networks (RNNs) can be consistently estimated, and their predictions are asymptotically normally distributed under appropriate assumptions, enabling the prediction uncertainty to be correctly quantified. Our numerical results show that sparse deep learning outperforms state-of-the-art methods, such as conformal predictions, in prediction uncertainty qua
    
[^41]: 非光滑弱凸有限和耦合组合优化

    Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])

    [http://arxiv.org/abs/2310.03234](http://arxiv.org/abs/2310.03234)

    本文研究了一种新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)，通过扩展已有的研究，我们研究了非光滑弱凸FCCO的问题，并提出了一种单循环算法来找到Moreau环的ε-稳定点。

    

    本文研究了一类新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)。由于其在机器学习和人工智能领域的广泛应用以及其解决基于经验风险最小化的随机算法的局限性，FCCO引起了越来越多的关注。然而，目前对于FCCO的研究假设内外函数都是光滑的，限制了其能够解决更多种类的问题的潜力。我们的研究从非光滑弱凸FCCO的角度进行了扩展，其中外函数是弱凸且非递减的，内函数是弱凸的。我们分析了一种单循环算法，并确定其在找到Moreau环的ε-稳定点的复杂度。

    This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau env
    
[^42]: 深度表征第一人称代词以预测抑郁症状的严重程度

    Deep Representations of First-person Pronouns for Prediction of Depression Symptom Severity. (arXiv:2310.03232v1 [cs.CL])

    [http://arxiv.org/abs/2310.03232](http://arxiv.org/abs/2310.03232)

    本研究利用上下文化的语言表征模型中获得的第一人称代词嵌入，分析抑郁症状的严重程度。结果显示，相比标准分类令牌嵌入和基于频率的代词分析，上下文化的第一人称代词嵌入在预测抑郁症状严重程度方面具有优势。

    

    先前的研究表明，分析使用第一人称单数代词可以揭示个体的心理状态，特别是抑郁症状的严重程度。这些研究通过计算文本数据中第一人称单数代词的频率来得出结论。然而，计数不能捕捉到这些代词的使用方式。近期神经语言建模的进展利用了生成上下文嵌入的方法。在这项研究中，我们试图利用从上下文语言表征模型中获得的第一人称代词嵌入来捕捉这些代词的使用方式，以分析心理状态。评估过程中使用了在在线心理治疗期间发送的去身份化的文本消息，其中每周评估了抑郁症状的严重程度。结果表明，与标准分类令牌嵌入和基于频率的代词分析相比，上下文化的第一人称代词嵌入在预测抑郁症状的严重程度方面具有优势。

    Prior work has shown that analyzing the use of first-person singular pronouns can provide insight into individuals' mental status, especially depression symptom severity. These findings were generated by counting frequencies of first-person singular pronouns in text data. However, counting doesn't capture how these pronouns are used. Recent advances in neural language modeling have leveraged methods generating contextual embeddings. In this study, we sought to utilize the embeddings of first-person pronouns obtained from contextualized language representation models to capture ways these pronouns are used, to analyze mental status. De-identified text messages sent during online psychotherapy with weekly assessment of depression severity were used for evaluation. Results indicate the advantage of contextualized first-person pronoun embeddings over standard classification token embeddings and frequency-based pronoun analysis results in predicting depression symptom severity. This suggest
    
[^43]: 安全探索在强化学习中的应用：一种普适的形式化和算法

    Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms. (arXiv:2310.03225v1 [cs.LG])

    [http://arxiv.org/abs/2310.03225](http://arxiv.org/abs/2310.03225)

    本文提出了一种广义的安全探索问题，即GSE，并提出了一个元算法MASE来解决这个问题。MASE将无约束的强化学习算法与不确定性量化器结合，以保证安全性，并在违反安全约束之前惩罚不安全的探索。这种方法的优势是在保证安全性的同时进行策略优化，并具有高概率的安全保证。

    

    在许多真实场景中，安全探索对于实际应用强化学习（RL）至关重要。本文提出了一种广义安全探索（GSE）问题，作为常见安全探索问题的统一形式化。然后，我们提出了GSE问题的解决方案，即安全探索的元算法MASE，它将无约束的RL算法与不确定性量化器相结合，以保证当前回合的安全性，同时在实际安全违规之前适当惩罚不安全的探索，以防止它们在未来回合中发生。MASE的优势在于我们可以在保证高概率下不违反安全约束的前提下，优化策略。具体而言，我们提出了两种不同构建不确定性量化器的MASE变体：一种基于广义线性模型，具有安全性和接近最优性的理论保证，另一种则结合了高斯过程。

    Safe exploration is essential for the practical use of reinforcement learning (RL) in many real-world scenarios. In this paper, we present a generalized safe exploration (GSE) problem as a unified formulation of common safe exploration problems. We then propose a solution of the GSE problem in the form of a meta-algorithm for safe exploration, MASE, which combines an unconstrained RL algorithm with an uncertainty quantifier to guarantee safety in the current episode while properly penalizing unsafe explorations before actual safety violation to discourage them in future episodes. The advantage of MASE is that we can optimize a policy while guaranteeing with a high probability that no safety constraint will be violated under proper assumptions. Specifically, we present two variants of MASE with different constructions of the uncertainty quantifier: one based on generalized linear models with theoretical guarantees of safety and near-optimality, and another that combines a Gaussian proce
    
[^44]: 用扩散改进的 MCMC 学习能量先验模型

    Learning Energy-Based Prior Model with Diffusion-Amortized MCMC. (arXiv:2310.03218v1 [cs.LG])

    [http://arxiv.org/abs/2310.03218](http://arxiv.org/abs/2310.03218)

    本文介绍了一种基于扩散改进的长期 MCMC采样的学习算法，用于学习能量先验模型。实验证明了该算法的有效性。

    

    隐变量空间的能量基模型（EBMs），也称为能量先验模型，由于其在公式化和潜在空间的强建模能力上的灵活性，引起了生成建模领域的日益关注。然而，使用非收敛的短期 MCMC 进行先验和后验采样来学习隐变量空间的能量先验模型的常见做法，阻碍了模型的进一步发展；实践中退化的 MCMC 采样质量通常导致生成质量下降和训练不稳定，特别是在高多模态和/或高维目标分布中。为了解决这个采样问题，在本文中，我们引入了一种简单但有效的基于扩散的摊销方法，用于长期 MCMC 采样，并基于此开发了一种新的学习算法来学习隐变量空间的EBM。我们提供了理论证据，表明学习到的MCMC摊销是一个有效的长期MCMC采样器。在几个图像建模基准数据集上的实验证明了

    Latent space Energy-Based Models (EBMs), also known as energy-based priors, have drawn growing interests in the field of generative modeling due to its flexibility in the formulation and strong modeling power of the latent space. However, the common practice of learning latent space EBMs with non-convergent short-run MCMC for prior and posterior sampling is hindering the model from further progress; the degenerate MCMC sampling quality in practice often leads to degraded generation quality and instability in training, especially with highly multi-modal and/or high-dimensional target distributions. To remedy this sampling issue, in this paper we introduce a simple but effective diffusion-based amortization method for long-run MCMC sampling and develop a novel learning algorithm for the latent space EBM based on it. We provide theoretical evidence that the learned amortization of MCMC is a valid long-run MCMC sampler. Experiments on several image modeling benchmark datasets demonstrate t
    
[^45]: 关于多模态语言模型性能的研究

    On the Performance of Multimodal Language Models. (arXiv:2310.03211v1 [cs.CL])

    [http://arxiv.org/abs/2310.03211](http://arxiv.org/abs/2310.03211)

    本研究对不同多模态指导调优方法进行比较分析，并评估其在复杂推理、对话、图像描述等任务中的性能。通过基准测试和消融实验，为将多模态能力融入语言模型提供了关键见解。

    

    在独立预训练的视觉编码器通过模型嫁接的方式整合到大型语言模型中后，多模态语言模型展现了有望应用于各种下游任务的零样本泛化能力。这项研究对不同的多模态指导调优方法进行了比较分析，并评估了它们在复杂推理、对话、图像描述、多项选择题和二分类等任务中的性能。通过严格的基准测试和消融实验，我们揭示了在将多模态能力融入大型语言模型时指导架构选择的关键见解。然而，当前的方法存在局限性，它们没有足够地解决多样化的多模态指导数据的需求。

    Instruction-tuned large language models (LLMs) have demonstrated promising zero-shot generalization capabilities across various downstream tasks. Recent research has introduced multimodal capabilities to LLMs by integrating independently pretrained vision encoders through model grafting. These multimodal variants undergo instruction tuning, similar to LLMs, enabling effective zero-shot generalization for multimodal tasks. This study conducts a comparative analysis of different multimodal instruction tuning approaches and evaluates their performance across a range of tasks, including complex reasoning, conversation, image captioning, multiple-choice questions (MCQs), and binary classification. Through rigorous benchmarking and ablation experiments, we reveal key insights for guiding architectural choices when incorporating multimodal capabilities into LLMs. However, current approaches have limitations; they do not sufficiently address the need for a diverse multimodal instruction datase
    
[^46]: 通过神经再参数化优化实现的大规模3D人脸网格视频数据集

    A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization. (arXiv:2310.03205v1 [cs.CV])

    [http://arxiv.org/abs/2310.03205](http://arxiv.org/abs/2310.03205)

    我们提出了NeuFace方法，通过神经再参数化优化，在大规模的人脸视频上实现了准确和一致的人脸网格标注。利用我们的数据集，在3D人脸相关任务中，我们展示了该数据集的有用性，并且能够改善现有的3D人脸重建模型的准确性和学习3D面部运动先验。

    

    我们提出了一种名为NeuFace的方法，通过神经再参数化优化在视频中进行3D人脸网格的伪标注。尽管在3D人脸重建方法方面取得了巨大的进展，但在野外动态视频中生成可靠的3D人脸标签仍然具有挑战性。通过使用NeuFace优化，我们对大规模人脸视频进行每个视角/帧准确而一致的人脸网格标注，称为NeuFace数据集。我们研究了神经再参数化如何通过梯度分析将图像对齐的面部细节重建到3D网格上。通过利用我们数据集中3D人脸的自然性和多样性，我们展示了我们数据集在3D人脸相关任务中的用处：提高现有3D人脸重建模型的重建准确性和学习3D面部运动先验。代码和数据集将在https://neuface-dataset.github上提供。

    We propose NeuFace, a 3D face mesh pseudo annotation method on videos via neural re-parameterized optimization. Despite the huge progress in 3D face reconstruction methods, generating reliable 3D face labels for in-the-wild dynamic videos remains challenging. Using NeuFace optimization, we annotate the per-view/-frame accurate and consistent face meshes on large-scale face videos, called the NeuFace-dataset. We investigate how neural re-parameterization helps to reconstruct image-aligned facial details on 3D meshes via gradient analysis. By exploiting the naturalness and diversity of 3D faces in our dataset, we demonstrate the usefulness of our dataset for 3D face-related tasks: improving the reconstruction accuracy of an existing 3D face reconstruction model and learning 3D facial motion prior. Code and datasets will be available at https://neuface-dataset.github.
    
[^47]: 深度强化学习用于机器调度：方法论、现状和未来方向

    Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions. (arXiv:2310.03195v1 [cs.LG])

    [http://arxiv.org/abs/2310.03195](http://arxiv.org/abs/2310.03195)

    本文对基于深度强化学习(DRL)的机器调度方法进行了全面回顾和比较，总结出它们的方法论、应用、优势和局限性。通过对比分析，发现DRL方法优于传统方法。

    

    机器调度旨在通过满足制造规则和作业规范的前提下，优化作业分配给机器。这种优化可以降低运营成本、提高客户需求的满足度和增强生产效率。然而，由于其 NP-hard 的性质，机器调度仍然是一个具有挑战性的组合优化问题。深度强化学习(DRL)作为人工通用智能的关键组成部分，在游戏和机器人等领域已经显示出了潜力。自1995年以来，研究人员一直在探索将DRL应用于机器调度问题。本文对基于DRL的方法进行了全面的回顾和比较，重点介绍了它们的方法论、应用、优势和局限性。它将这些方法根据计算组件进行了分类：传统神经网络、编码器-解码器结构、图神经网络和元启发式算法。我们的回顾得出结论，基于DRL的方法优于精确求解器和启发式算法。

    Machine scheduling aims to optimize job assignments to machines while adhering to manufacturing rules and job specifications. This optimization leads to reduced operational costs, improved customer demand fulfillment, and enhanced production efficiency. However, machine scheduling remains a challenging combinatorial problem due to its NP-hard nature. Deep Reinforcement Learning (DRL), a key component of artificial general intelligence, has shown promise in various domains like gaming and robotics. Researchers have explored applying DRL to machine scheduling problems since 1995. This paper offers a comprehensive review and comparison of DRL-based approaches, highlighting their methodology, applications, advantages, and limitations. It categorizes these approaches based on computational components: conventional neural networks, encoder-decoder architectures, graph neural networks, and metaheuristic algorithms. Our review concludes that DRL-based methods outperform exact solvers, heuristi
    
[^48]: 论文标题:通过交互式沟通将预训练的知识提取到下游模型中的对话模型

    Talking Models: Distill Pre-trained Knowledge to Downstream Models via Interactive Communication. (arXiv:2310.03188v1 [cs.AI])

    [http://arxiv.org/abs/2310.03188](http://arxiv.org/abs/2310.03188)

    通过交互式沟通将预训练的知识提取到下游模型中的对话模型，解决了强大教师无法保证强大学生的问题，同时利用交互式沟通提高了知识蒸馏在提高下游任务性能方面的优化。

    

    最近机器学习的许多突破都是由预训练的基础模型实现的。通过扩大模型参数、训练数据和计算资源，基础模型在许多应用领域显著提高了最先进技术的水平。然而，如何有效地使用这些模型来执行下游任务仍然是一个开放的问题。知识蒸馏（KD）已经被用来解决这个挑战。KD将知识从一个大型教师模型传递给一个较小的学生模型。虽然KD在提高学生模型性能方面取得了成功，但最近的研究发现，强大的教师并不一定会导致强大的学生，因为它们之间存在巨大的能力差距。此外，预训练数据和下游任务之间的潜在的分布偏移可能使KD中的知识传递对于提高下游任务性能不够优化。在本文中，我们通过一个交互式沟通过程扩展了KD，以帮助学生模型更好地利用预训练知识。

    Many recent breakthroughs in machine learning have been enabled by the pre-trained foundation models. By scaling up model parameters, training data, and computation resources, foundation models have significantly advanced the state-of-the-art in many applications. However, it is still an open question of how to use these models to perform downstream tasks efficiently. Knowledge distillation (KD) has been explored to tackle this challenge. KD transfers knowledge from a large teacher model to a smaller student model. While KD has been successful in improving student model performance, recent research has discovered that a powerful teacher does not necessarily lead to a powerful student, due to their huge capacity gap. In addition, the potential distribution shifts between the pre-training data and downstream tasks can make knowledge transfer in KD sub-optimal for improving downstream task performance. In this paper, we extend KD with an interactive communication process to help students 
    
[^49]: 推测推理

    Inferring Inference. (arXiv:2310.03186v1 [q-bio.NC])

    [http://arxiv.org/abs/2310.03186](http://arxiv.org/abs/2310.03186)

    大脑具有一系列重复的规范计算单元，但神经表示是分布式的，因此如何定义规范分布式计算仍然是一个挑战。本文提出了一个数学框架，从大规模神经活动模式中推断出规范分布式计算。在算法级别上，提出了一个基于图结构模型的非线性信息传递算法。通过感知推理任务中的神经活动时间序列，可以找到神经活动与感知推理任务中的潜在因果之间的映射关系。

    

    微电路图案表明大脑具有一系列重复的规范计算单元。然而，神经表示是分布式的，因此相关计算可能仅与单个神经元变换间接相关。因此，如何定义规范分布式计算仍然是一个挑战。我们将神经计算的规范和算法理论整合到一个数学框架中，用于从大规模神经活动模式中推断出规范分布式计算。在规范级别上，我们假设大脑创建了一个结构化的内部模型，假设解释其感官输入的潜在原因，并使用这些感官输入来推断潜在原因。在算法级别上，我们提出这个推理过程是一个基于图结构模型的非线性信息传递算法。通过感知推理任务中的神经活动时间序列，我们的框架可以找到（i）神经活动与感知推理任务中的潜在因果之间的映射关系。

    Patterns of microcircuitry suggest that the brain has an array of repeated canonical computational units. Yet neural representations are distributed, so the relevant computations may only be related indirectly to single-neuron transformations. It thus remains an open challenge how to define canonical distributed computations. We integrate normative and algorithmic theories of neural computation into a mathematical framework for inferring canonical distributed computations from large-scale neural activity patterns. At the normative level, we hypothesize that the brain creates a structured internal model of its environment, positing latent causes that explain its sensory inputs, and uses those sensory inputs to infer the latent causes. At the algorithmic level, we propose that this inference process is a nonlinear message-passing algorithm on a graph-structured model of the world. Given a time series of neural activity during a perceptual inference task, our framework finds (i) the neura
    
[^50]: 大型语言模型中利用视觉对抗性样本误用工具

    Misusing Tools in Large Language Models With Visual Adversarial Examples. (arXiv:2310.03185v1 [cs.CR])

    [http://arxiv.org/abs/2310.03185](http://arxiv.org/abs/2310.03185)

    本研究展示了如何利用视觉对抗性样本来引导大型语言模型（LLMs）执行攻击者想要的工具使用。攻击可以导致LLM删除日历事件、泄露私人对话和预订酒店，并且具有隐蔽性和适用性广的特点。同时，这些攻击几乎总是可以操纵LLM以遵循真实世界的语法来调用工具，同时保持与干净图像的高相似性。

    

    大型语言模型（LLMs）正在增强其使用工具和处理多种模态的能力。这些新能力带来了新的好处，也带来了新的安全风险。本研究中，我们展示了攻击者可以使用视觉对抗性样本来引导模型执行攻击者想要的工具使用。例如，攻击者可以让受害者的LLM删除日历事件，泄露私人对话并预订酒店。与先前的工作不同，我们的攻击可以同时影响与LLM连接的用户资源的机密性和完整性，同时具有隐蔽性和适用于多个输入提示的特点。我们使用基于梯度的对抗训练构建这些攻击，并在多个维度上对其性能进行了表征。我们发现，我们的对抗图像几乎总是（约98%）可以操纵LLM以遵循真实世界的语法来调用工具，同时保持与干净图像的高相似性（约0.9 SSIM）。此外，使用人工评分和自动化指标，我们发现攻击会降低LLM的自然性能和用户满意度。

    Large Language Models (LLMs) are being enhanced with the ability to use tools and to process multiple modalities. These new capabilities bring new benefits and also new security risks. In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage. For example, the attacker could cause a victim LLM to delete calendar events, leak private conversations and book hotels. Different from prior work, our attacks can affect the confidentiality and integrity of user resources connected to the LLM while being stealthy and generalizable to multiple input prompts. We construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. We find that our adversarial images can manipulate the LLM to invoke tools following real-world syntax almost always (~98%) while maintaining high similarity to clean images (~0.9 SSIM). Furthermore, using human scoring and automated metrics, we find that the attack
    
[^51]: 神经架构对于识别时间延长增强学习任务的影响

    Neural architecture impact on identifying temporally extended Reinforcement Learning tasks. (arXiv:2310.03161v1 [cs.LG])

    [http://arxiv.org/abs/2310.03161](http://arxiv.org/abs/2310.03161)

    本研究提出了基于注意力的神经架构，在强化学习领域取得了良好的性能。通过将注意力图叠加到图像上，可以直接观察到代理所使用的信息，并更容易解释选择动作背后的逻辑。

    

    在图像分类和自然语言处理领域的关注模型最新进展的启发下，我们在强化学习领域提出了多种基于注意力的架构，能够在OpenAI Gym Atari-2600游戏套件上表现良好。尽管深度增强学习技术在机器人、游戏和医疗等各个领域取得了最近的成功，但它们存在一个主要问题，即神经网络难以解释。我们尝试通过注意力模型来解决这个问题。在注意力模型中，提取和叠加注意力图到图像上，可以直接观察代理使用的信息以选择动作，并更容易解释选择动作背后的逻辑。我们的模型不仅在gym-Atari环境中表现优秀，还能提供关于代理如何感知其环境的见解。此外，受到使用视觉注意力的视频分类模型最新进展的启发，

    Inspired by recent developments in attention models for image classification and natural language processing, we present various Attention based architectures in reinforcement learning (RL) domain, capable of performing well on OpenAI Gym Atari-2600 game suite. In spite of the recent success of Deep Reinforcement learning techniques in various fields like robotics, gaming and healthcare, they suffer from a major drawback that neural networks are difficult to interpret. We try to get around this problem with the help of Attention based models. In Attention based models, extracting and overlaying of attention map onto images allows for direct observation of information used by agent to select actions and easier interpretation of logic behind the chosen actions. Our models in addition to playing well on gym-Atari environments, also provide insights on how agent perceives its environment. In addition, motivated by recent developments in attention based video-classification models using Vis
    
[^52]: 评估使用不确定特征曲线的预测区间

    Assessment of Prediction Intervals Using Uncertainty Characteristics Curves. (arXiv:2310.03158v1 [cs.LG])

    [http://arxiv.org/abs/2310.03158](http://arxiv.org/abs/2310.03158)

    本论文提出了一种新颖的评估预测区间的方法，利用了操作特征曲线和相对于空白参考的收益概念。这种方法广泛适用于不同的研究场景，解决了当前对预测区间全面评估的需求。

    

    准确量化模型的不确定性已经被认为是可信AI的基本要求。在回归任务中，不确定性通常使用校准到专有操作点的预测区间来量化，从而使得对不同研究进行评估和比较相对困难。我们的工作利用了(1)操作特征曲线的概念和(2)相对于空白参考的收益概念，推导出一种新颖的操作点不可知评估方法，适用于预测区间。本文定义了不确定特征曲线，并在选定场景中展示其实用性。我们认为，所提出的方法解决了对预测区间进行全面评估的当前需求，因此是不确定性量化工具箱的宝贵补充。

    Accurate quantification of model uncertainty has long been recognized as a fundamental requirement for trusted AI. In regression tasks, uncertainty is typically quantified using prediction intervals calibrated to an ad-hoc operating point, making evaluation and comparison across different studies relatively difficult. Our work leverages: (1) the concept of operating characteristics curves and (2) the notion of a gain over a null reference, to derive a novel operating point agnostic assessment methodology for prediction intervals. The paper defines the Uncertainty Characteristics Curve and demonstrates its utility in selected scenarios. We argue that the proposed method addresses the current need for comprehensive assessment of prediction intervals and thus represents a valuable addition to the uncertainty quantification toolbox.
    
[^53]: 将神经网络中学习到的概念归因于训练数据

    Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v1 [cs.LG])

    [http://arxiv.org/abs/2310.03149](http://arxiv.org/abs/2310.03149)

    通过将数据归因方法与概念探测方法相结合，研究了神经网络中学习到的概念与训练数据的关系，并发现概念的位置和稀疏性并不完全依赖于少量特定示例。

    

    现在有大量的证据表明，深度学习模型学习到了某些可解释的人类特征，作为其对数据的内部表示的一部分。由于拥有正确（或错误）的概念对于可信赖的机器学习系统至关重要，自然而然地我们想要知道在给定层次上，模型原始训练集中的哪些输入对于学习一个概念最为重要。为了回答这个问题，我们将数据归因方法与探测模型学习到的概念的方法相结合。通过在一系列网络层次上训练网络和探测模型，并使用最近开发的用于大规模数据归因的TRAK方法，我们对两个概念数据集进行训练网络和探测模型的集合。我们发现一些证据表明，通过移除对一个概念具有最高归因的前10000张图像并重新训练模型，概念在网络中的位置以及概念的探测稀疏性并没有发生改变。这表明，与依赖于少量特定示例不同，用于确定概念的特征具有较高的独立性。

    By now there is substantial evidence that deep learning models learn certain human-interpretable features as part of their internal representations of data. As having the right (or wrong) concepts is critical to trustworthy machine learning systems, it is natural to ask which inputs from the model's original training set were most important for learning a concept at a given layer. To answer this, we combine data attribution methods with methods for probing the concepts learned by a model. Training network and probe ensembles for two concept datasets on a range of network layers, we use the recently developed TRAK method for large-scale data attribution. We find some evidence for convergence, where removing the 10,000 top attributing images for a concept and retraining the model does not change the location of the concept in the network nor the probing sparsity of the concept. This suggests that rather than being highly dependent on a few specific examples, the features that inform the 
    
[^54]: Axiomatic Aggregations of Abductive Explanations. (arXiv:2310.03131v1 [cs.AI])

    Axiomatic Aggregations of Abductive Explanations. (arXiv:2310.03131v1 [cs.AI])

    [http://arxiv.org/abs/2310.03131](http://arxiv.org/abs/2310.03131)

    本文提出了通过将多种可能的归纳解释汇总为特征重要性分数的三种聚合方法，以解决存在多个有效的归纳解释的问题。

    

    最近对后验模型近似解释方法（如LIME和SHAP）健壮性的批评导致了模型精确阐释的兴起。针对每个数据点，归纳解释提供了能够生成结果的最小特征子集。尽管在理论上是可靠且严谨的，但是归纳解释存在一个主要问题 - 对于同一数据点可能存在多个有效的归纳解释。在这种情况下，提供单一的归纳解释可能是不足够的；另一方面，提供所有有效的归纳解释可能由于其数量庞大而难以理解。在本文中，我们通过将多种可能的归纳解释汇总为特征重要性分数来解决这个问题。我们提出了三种聚合方法：两种基于合作博弈论的权力指数和一种基于著名的因果强度度量。我们通过公理化表征这三种方法，证明它们每一个都具有

    The recent criticisms of the robustness of post hoc model approximation explanation methods (like LIME and SHAP) have led to the rise of model-precise abductive explanations. For each data point, abductive explanations provide a minimal subset of features that are sufficient to generate the outcome. While theoretically sound and rigorous, abductive explanations suffer from a major issue -- there can be several valid abductive explanations for the same data point. In such cases, providing a single abductive explanation can be insufficient; on the other hand, providing all valid abductive explanations can be incomprehensible due to their size. In this work, we solve this issue by aggregating the many possible abductive explanations into feature importance scores. We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength. We characterize these three methods axiomatically, showing that each of 
    
[^55]: 高效的黑盒大型预训练模型联合提示调整

    Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models. (arXiv:2310.03123v1 [cs.LG])

    [http://arxiv.org/abs/2310.03123](http://arxiv.org/abs/2310.03123)

    这项研究提出了一种高效的黑盒大型预训练模型联合提示调整（Fed-BBPT）方法，通过摒弃对参数结构和私有数据集访问的依赖，可以在处理内存限制和保持隐私性的同时充分利用每个局部数据集。

    

    随着预训练模型（PTMs）的迅猛发展，对这些模型进行高效调整以适用于不同的下游应用已成为一个关键的研究关注点。尽管最近关于提示调整的研究提供了有希望的途径，但仍然存在三个突出的挑战：（1）内存限制：开源PTMs大小的持续增长使得即使对其参数的一小部分进行微调也对许多从业者来说是具有挑战性的。（2）模型隐私性：现有的PTMs通常作为公共API服务，其参数无法有效或定制地进行微调。（3）数据隐私性：对PTMs进行微调需要高质量的数据集，这些数据集通常是局部化的并且不共享给公众。为了在处理内存限制和保持隐私性的同时充分利用每个局部数据集，我们提出了联合黑盒提示调整（Fed-BBPT）。这种创新方法摒弃了对参数结构和私有数据集访问的依赖，可以有效地解决这些挑战。

    With the blowout development of pre-trained models (PTMs), the efficient tuning of these models for diverse downstream applications has emerged as a pivotal research concern. Although recent investigations into prompt tuning have provided promising avenues, three salient challenges persist: (1) memory constraint: the continuous growth in the size of open-source PTMs renders fine-tuning, even a fraction of their parameters, challenging for many practitioners. (2) model privacy: existing PTMs often function as public API services, with their parameters inaccessible for effective or tailored fine-tuning. (3) data privacy: the fine-tuning of PTMs necessitates high-quality datasets, which are typically localized and not shared to public. To optimally harness each local dataset while navigating memory constraints and preserving privacy, we propose Federated Black-Box Prompt Tuning (Fed-BBPT). This innovative approach eschews reliance on parameter architectures and private dataset access, ins
    
[^56]: 基于思维混合表示的大规模语言模型级联用于成本高效的推理

    Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])

    [http://arxiv.org/abs/2310.03094](http://arxiv.org/abs/2310.03094)

    本研究提出了一种基于思维混合表示的大规模语言模型级联方法，用于成本高效的推理。通过考虑更弱模型的答案一致性作为问题难度的信号，可以实现对问题的决策，从而节约使用更强模型的成本。

    

    大规模语言模型（LLM）如GPT-4在各种任务中展现出了非凡的性能，但是这种强大的性能通常伴随着使用付费API服务的高昂费用。本文的研究动机是为了研究构建LLM级联以节约使用LLM的成本，特别是用于进行推理（例如数学、因果推理）任务的成本。我们的级联管道遵循一个直观的思想，即简单的问题可以由一个更弱但更实惠的LLM来解决，而只有具有挑战性的问题才需要更强大、更昂贵的LLM。为了实现这种决策，我们考虑到更弱的LLM的“答案一致性”作为问题难度的信号，并提出了几种答案采样和一致性检查的方法，其中一种方法利用了两种思维表示（即连续思维和程序思维）的混合。通过在六个推理基准数据集上的实验，我们使用GPT-3.5-turbo和GPT-4作为较弱的模型，

    Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and 
    
[^57]: 计算生物学中的深度学习：进展、挑战和未来展望

    Deep Learning in Computational Biology: Advancements, Challenges, and Future Outlook. (arXiv:2310.03086v1 [cs.LG])

    [http://arxiv.org/abs/2310.03086](http://arxiv.org/abs/2310.03086)

    深度学习在计算生物学中的应用带来了重大变革，既能够改善DNA序列分析和蛋白质结构预测，也为基因组变异检测和基因表达分析提供了新的方法。

    

    深度学习已经成为计算生物学中一个强大的工具，革新了生物数据的分析和解释。在我们的文章中，我们深入探讨了计算生物学中深度学习的各个方面。具体而言，我们研究了其历史、优势和挑战。我们的重点是两个主要应用领域：DNA序列分类和预测，以及从序列数据中预测蛋白质结构。此外，我们还提供了对该领域未来发展的见解。要充分发挥深度学习在计算生物学中的潜力，关键是解决所带来的挑战。这些挑战包括对大规模标记数据集的需求以及深度学习模型的可解释性。深度学习在分析DNA序列方面的应用已经在基因组变异检测和基因表达分析方面带来了重大变革，从而为该领域的进展做出了重要贡献。

    Deep learning has become a powerful tool in computational biology, revolutionising the analysis and interpretation of biological data over time. In our article review, we delve into various aspects of deep learning in computational biology. Specifically, we examine its history, advantages, and challenges. Our focus is on two primary applications: DNA sequence classification and prediction, as well as protein structure prediction from sequence data. Additionally, we provide insights into the outlook for this field. To fully harness the potential of deep learning in computational biology, it is crucial to address the challenges that come with it. These challenges include the requirement for large, labelled datasets and the interpretability of deep learning models. The use of deep learning in the analysis of DNA sequences has brought about a significant transformation in the detection of genomic variants and the analysis of gene expression. This has greatly contributed to the advancement 
    
[^58]: 在预训练语言模型中发现关键知识子网络

    Discovering Knowledge-Critical Subnetworks in Pretrained Language Models. (arXiv:2310.03084v1 [cs.CL])

    [http://arxiv.org/abs/2310.03084](http://arxiv.org/abs/2310.03084)

    本研究调查了预训练语言模型中是否存在各种关键知识子网络，即负责编码特定知识的稀疏计算子图。通过提出的可微分权重屏蔽方案，我们可以精确地删除特定知识，又最小化对原始语言模型的负面影响。

    

    预训练语言模型在其参数中编码了隐含的知识表示，然而，定位这些表示并将其解离出来仍然是一个未解决的问题。本研究探讨了预训练语言模型是否包含了各种关键知识子网络：负责编码模型所记忆的特定知识的特定稀疏计算子图。我们提出了一个多目标可微分权重屏蔽方案来发现这些子网络，并表明我们可以使用它们来精确地从模型中删除特定知识，同时最小化对原始语言模型行为的不良影响。我们在多个GPT2变体上展示了我们的方法，揭示了高度稀疏子网络（98%+），它们仅负责特定的关系知识集合。当删除这些子网络时，剩余的网络仍保持了大部分其初始容量（对语言和其他记忆关系的建模）。

    Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various knowledge-critical subnetworks: particular sparse computational subgraphs responsible for encoding specific knowledge the model has memorized. We propose a multi-objective differentiable weight masking scheme to discover these subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original language model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+) that are solely responsible for specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial capacity (modeling language and other memorized rel
    
[^59]: Point-PEFT: 用于3D预训练模型的参数高效微调

    Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])

    [http://arxiv.org/abs/2310.03059](http://arxiv.org/abs/2310.03059)

    Point-PEFT是一种用于3D预训练模型的参数高效微调框架，它通过冻结大部分参数，只微调新增的PEFT模块，包括Point-prior Prompt和Geometry-aware Adapter，以最小化学习参数，并利用内存库和准确的聚合方法来提高模型性能。

    

    大型预训练模型的流行已经彻底改变了语言、视觉和多模态等领域的下游任务。为了降低下游任务的适应成本，许多参数高效微调（PEFT）技术被提出用于语言和2D图像预训练模型。然而，对于3D预训练模型的专门PEFT方法仍未得到充分探索。为此，我们引入了Point-PEFT，一种用于适应点云预训练模型的新型框架，其具有最少的可学习参数。具体而言，对于预训练的3D模型，我们冻结大部分参数，只微调新增的PEFT模块。这些模块包括Point-prior Prompt和Geometry-aware Adapter。Point-prior Prompt采用一组可学习的提示标记，并提出使用具有领域特定知识的内存库来增强提示标记的参数无关的注意力机制。Geometry-aware Adapter旨在对不同任务或数据进行准确地聚合。

    The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggrega
    
[^60]: 修改后的LAB算法及基于聚类的搜索空间缩减方法在工程设计问题求解中的应用

    Modified LAB Algorithm with Clustering-based Search Space Reduction Method for solving Engineering Design Problems. (arXiv:2310.03055v1 [cs.LG])

    [http://arxiv.org/abs/2310.03055](http://arxiv.org/abs/2310.03055)

    本文介绍了一种修改后的LAB算法，并提出了基于聚类的搜索空间缩减方法，该算法在工程设计问题求解中表现出改进的稳健性和搜索空间探索能力，同时能够解决约束问题。

    

    本文介绍了一种修改后的LAB算法。该算法建立在原始的LAB算法（Reddy等人，2023年）基础上，该算法模拟了群体内部的竞争和学习行为，建立了层次角色。所提出的算法利用了轮盘赌方法和缩减因子，引入了群组间竞争，并逐步缩小样本空间。该算法通过解决CEC 2005和CEC 2017的基准测试问题进行验证。使用双边和成对符号秩Wilcoxon测试以及Friedman秩测试对解决方案进行验证。该算法具有改进和卓越的稳健性以及搜索空间探索能力。此外，本文还提出了一种基于聚类的搜索空间缩减（C-SSR）方法，使算法能够解决约束问题。C-SSR方法使算法能够识别满足约束条件的可行区域之间的聚类。

    A modified LAB algorithm is introduced in this paper. It builds upon the original LAB algorithm (Reddy et al. 2023), which is a socio-inspired algorithm that models competitive and learning behaviours within a group, establishing hierarchical roles. The proposed algorithm incorporates the roulette wheel approach and a reduction factor introducing inter-group competition and iteratively narrowing down the sample space. The algorithm is validated by solving the benchmark test problems from CEC 2005 and CEC 2017. The solutions are validated using standard statistical tests such as two-sided and pairwise signed rank Wilcoxon test and Friedman rank test. The algorithm exhibited improved and superior robustness as well as search space exploration capabilities. Furthermore, a Clustering-Based Search Space Reduction (C-SSR) method is proposed, making the algorithm capable to solve constrained problems. The C-SSR method enables the algorithm to identify clusters of feasible regions, satisfying 
    
[^61]: Memoria: 用于类人顺序处理的海比安记忆体架构

    Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing. (arXiv:2310.03052v1 [cs.LG])

    [http://arxiv.org/abs/2310.03052](http://arxiv.org/abs/2310.03052)

    Memoria 是一个通用记忆网络，应用海比安理论来增强神经网络中的长期依赖。通过存储和检索信息，并使用根据海布规则变化的连接权重，Memoria 在诸如 BERT 和 GPT 之类的流行 Transformer 模型上显著改进了考虑长期依赖的能力。

    

    Transformer 在多个领域和任务中取得了成功。然而，由于其有限的容量，Transformer 很难处理长输入序列。虽然增加输入长度是一个解决方案，但无止境地增加长度是不现实的。此外，与 Transformer 不同，人类有选择性地记住和使用仅与输入相关的信息，而不是从头到尾处理所有原始数据。我们引入了 Memoria，一个应用海比安记忆形成理论的通用记忆网络，用于增强神经网络中的长期依赖。Memoria 在工作记忆、短期记忆和长期记忆的多个记忆层级上存储和检索称为 engram 的信息，使用根据海布规则变化的连接权重。通过与诸如 BERT 和 GPT 等流行的基于 Transformer 的模型进行实验，我们提出 Memoria 显著提高了在各种任务中考虑长期依赖的能力。结果

    Transformers have demonstrated their success in various domains and tasks. However, Transformers struggle with long input sequences due to their limited capacity. While one solution is to increase input length, endlessly stretching the length is unrealistic. Furthermore, humans selectively remember and use only relevant information from inputs, unlike Transformers which process all raw data from start to end. We introduce Memoria, a general memory network that applies Hebbian theory which is a major theory explaining human memory formulation to enhance long-term dependencies in neural networks. Memoria stores and retrieves information called engram at multiple memory levels of working memory, short-term memory, and long-term memory, using connection weights that change according to Hebb's rule. Through experiments with popular Transformer-based models like BERT and GPT, we present that Memoria significantly improves the ability to consider long-term dependencies in various tasks. Resul
    
[^62]: 大型语言模型与具有心灵理论的智能体之间有多远？

    How FaR Are Large Language Models From Agents with Theory-of-Mind?. (arXiv:2310.03051v1 [cs.CL])

    [http://arxiv.org/abs/2310.03051](http://arxiv.org/abs/2310.03051)

    本论文提出了一种评估大型语言模型的新范式：思考为了行动（T4D）。实验证明，虽然这些模型擅长跟踪角色的信念，但在将这些推断转化为战略行动上存在困难。核心挑战在于识别隐含的关于心理状态的推断，而不是明确询问，这会导致正确的行动选择困难。

    

    “思考是为了行动。”人类可以通过观察来推断他人的心理状态，这种能力被称为心灵理论（Theory-of-Mind，ToM），并在此基础上行事。现有的问答基准（如ToMi）要求模型根据故事中角色的信念进行推断，但并不测试模型是否能够利用这些推断来指导自己的行动。我们提出了一个针对大型语言模型（LLMs）的新的评估范式：思考为了行动（T4D），这要求模型将关于他人心理状态的推断与社交场景中的行动联系起来。T4D实验证明，如GPT-4和PaLM 2等LLMs似乎擅长跟踪故事中角色的信念，但他们在将这种能力转化为战略行动上存在困难。我们的分析揭示了LLMs面临的核心挑战在于识别隐含的关于心理状态的推断，而不是像ToMi中那样明确询问，这会导致正确的行动选择困难。

    "Thinking is for Doing." Humans can infer other people's mental states from observations--an ability called Theory-of-Mind (ToM)--and subsequently act pragmatically on those inferences. Existing question answering benchmarks such as ToMi ask models questions to make inferences about beliefs of characters in a story, but do not test whether models can then use these inferences to guide their actions. We propose a new evaluation paradigm for large language models (LLMs): Thinking for Doing (T4D), which requires models to connect inferences about others' mental states to actions in social scenarios. Experiments on T4D demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action. Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct acti
    
[^63]: EcoAssistant: 更经济、更准确地使用LLM助手

    EcoAssistant: Using LLM Assistant More Affordably and Accurately. (arXiv:2310.03046v1 [cs.SE])

    [http://arxiv.org/abs/2310.03046](http://arxiv.org/abs/2310.03046)

    EcoAssistant是一个框架，旨在使LLM助手更经济、更准确地回答代码驱动的查询，并且包含了与自动代码执行器对话和使用层次结构的LLM助手的功能。

    

    如今，用户使用大型语言模型（LLM）作为助手来回答需要外部知识的查询；他们询问特定城市的天气情况，股票价格，甚至询问自己附近的特定地点在哪里。这些查询要求LLM生成调用外部API的代码来回答用户的问题，但LLM很少能一次性生成正确的代码，因此需要在执行结果上进行迭代的代码优化。此外，使用LLM助手支持高查询量可能会很昂贵。在本工作中，我们提出了一个框架，EcoAssistant，使LLM能够更经济、更准确地回答代码驱动的查询。EcoAssistant包含三个组件。首先，它允许LLM助手与自动代码执行器对话，以迭代优化代码或根据执行结果生成答案。其次，我们使用了LLM助手的层次结构，在尝试用较弱、更便宜的LLM回答查询之前。

    Today, users ask Large language models (LLMs) as assistants to answer queries that require external knowledge; they ask about the weather in a specific city, about stock prices, and even about where specific locations are within their neighborhood. These queries require the LLM to produce code that invokes external APIs to answer the user's question, yet LLMs rarely produce correct code on the first try, requiring iterative code refinement upon execution results. In addition, using LLM assistants to support high query volumes can be expensive. In this work, we contribute a framework, EcoAssistant, that enables LLMs to answer code-driven queries more affordably and accurately. EcoAssistant contains three components. First, it allows the LLM assistants to converse with an automatic code executor to iteratively refine code or to produce answers based on the execution results. Second, we use a hierarchy of LLM assistants, which attempts to answer the query with weaker, cheaper LLMs before 
    
[^64]: 一种基于深度强化学习的交互式搜索方法与句级反馈

    A Deep Reinforcement Learning Approach for Interactive Search with Sentence-level Feedback. (arXiv:2310.03043v1 [cs.LG])

    [http://arxiv.org/abs/2310.03043](http://arxiv.org/abs/2310.03043)

    本研究提出了一种利用深度强化学习的交互式搜索方法，该方法通过整合句级反馈信息来提高搜索准确性。通过适应最新的BERT-based模型进行关键句子选择和项目排序，可以获得更满意的搜索结果。

    

    交互式搜索可以通过整合用户的交互反馈来提供更好的搜索体验。这可以显著提高搜索准确性，因为它有助于避免无关信息并捕捉用户的搜索意图。现有的最新系统使用强化学习（RL）模型来整合这些交互，但是忽略了句级反馈中的细粒度信息。然而，这种反馈需要进行广泛的RL行动空间探索和大量的标注数据。本研究通过提出一种新的深度Q学习（DQ）方法DQrank来解决这些挑战。DQrank使用自然语言处理中最新技术BERT-based模型来选择关键句子，并基于用户的参与度对项目进行排序，以获得更满意的回应。我们还提出了两种机制来更好地探索最优行动。DQrank还利用DQ中的经验重现机制来存储反馈句子以增强模型性能。

    Interactive search can provide a better experience by incorporating interaction feedback from the users. This can significantly improve search accuracy as it helps avoid irrelevant information and captures the users' search intents. Existing state-of-the-art (SOTA) systems use reinforcement learning (RL) models to incorporate the interactions but focus on item-level feedback, ignoring the fine-grained information found in sentence-level feedback. Yet such feedback requires extensive RL action space exploration and large amounts of annotated data. This work addresses these challenges by proposing a new deep Q-learning (DQ) approach, DQrank. DQrank adapts BERT-based models, the SOTA in natural language processing, to select crucial sentences based on users' engagement and rank the items to obtain more satisfactory responses. We also propose two mechanisms to better explore optimal actions. DQrank further utilizes the experience replay mechanism in DQ to store the feedback sentences to ob
    
[^65]: 基于增强强化学习的量子系统控制方法

    A quantum system control method based on enhanced reinforcement learning. (arXiv:2310.03036v1 [cs.ET])

    [http://arxiv.org/abs/2310.03036](http://arxiv.org/abs/2310.03036)

    提出了一种基于增强强化学习的量子系统控制方法，通过映射状态和动作到量子系统中，利用增强型神经网络实现了长期累积奖励的最大化，从初始态准确演化到目标态，并在仿真实验中取得了接近1的保真度。

    

    传统的量子系统控制方法通常面临不同的限制，在资源有限的条件下容易导致泄露和随机控制误差。强化学习被证明是完成量子系统控制任务的有效方法。为了在资源有限的条件下学习出令人满意的控制策略，提出了一种基于增强强化学习的量子系统控制方法（QSC-ERL）。在强化学习中，状态和动作被映射到量子系统中的量子态和控制操作中。通过使用新的增强型神经网络，强化学习可以快速实现长期累积奖励的最大化，从初始态准确地演化到目标态。根据候选酉操作的数量，采用三开关控制进行仿真实验。与其他方法相比，QSC-ERL实现了接近1的保真度的学习。

    Traditional quantum system control methods often face different constraints, and are easy to cause both leakage and stochastic control errors under the condition of limited resources. Reinforcement learning has been proved as an efficient way to complete the quantum system control task. To learn a satisfactory control strategy under the condition of limited resources, a quantum system control method based on enhanced reinforcement learning (QSC-ERL) is proposed. The states and actions in reinforcement learning are mapped to quantum states and control operations in quantum systems. By using new enhanced neural networks, reinforcement learning can quickly achieve the maximization of long-term cumulative rewards, and a quantum state can be evolved accurately from an initial state to a target state. According to the number of candidate unitary operations, the three-switch control is used for simulation experiments. Compared with other methods, the QSC-ERL achieves close to 1 fidelity learn
    
[^66]: 提升交通标志识别的高准确性二进制神经网络的本地稳健性基准测试

    Benchmarking Local Robustness of High-Accuracy Binary Neural Networks for Enhanced Traffic Sign Recognition. (arXiv:2310.03033v1 [cs.CV])

    [http://arxiv.org/abs/2310.03033](http://arxiv.org/abs/2310.03033)

    这项研究基于二进制神经网络模型，通过引入一组基准问题来评估交通标志识别模型的本地稳健性。这些问题考虑了网络参数的数量、输入维度和区域数量，并挑战了最先进的验证工具。

    

    交通标志在自动驾驶系统中对道路安全和交通管理起着关键作用。由于真实世界的复杂性，如对抗性示例和遮挡等，准确的交通标志分类具有挑战性。为了解决这些问题，二进制神经网络在构建适用于资源受限设备的分类器方面具有潜力。在我们之前的工作中，我们提出了适用于交通标志识别的高准确性BNN模型，重点是紧凑的大小，以适应有限的计算和能源资源。为了评估它们的本地稳健性，本文引入了一组基准问题，其中的层挑战了最先进的验证工具。这些层包括二进制卷积、最大池化、批归一化和全连接层。验证问题的困难程度由网络参数的数量（905k-1.7M）、输入维度（2.7k-12k）和区域数量（43）以及神经元的事实决定。

    Traffic signs play a critical role in road safety and traffic management for autonomous driving systems. Accurate traffic sign classification is essential but challenging due to real-world complexities like adversarial examples and occlusions. To address these issues, binary neural networks offer promise in constructing classifiers suitable for resource-constrained devices.  In our previous work, we proposed high-accuracy BNN models for traffic sign recognition, focusing on compact size for limited computation and energy resources. To evaluate their local robustness, this paper introduces a set of benchmark problems featuring layers that challenge state-of-the-art verification tools. These layers include binarized convolutions, max pooling, batch normalization, fully connected. The difficulty of the verification problem is given by the high number of network parameters (905k - 1.7 M), of the input dimension (2.7k-12k), and of the number of regions (43) as well by the fact that the neur
    
[^67]: ChatGPT中的性别偏见有多普遍？—— 探索德语和英语ChatGPT的回应

    How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses. (arXiv:2310.03031v1 [cs.CL])

    [http://arxiv.org/abs/2310.03031](http://arxiv.org/abs/2310.03031)

    本文系统地分析并探索了德语和英语的ChatGPT回应中可能存在的问题，特别关注了性别偏见。我们发现，在对系统多次提供相同指令的情况下，回应存在差异。使用ChatGPT来帮助非IT用户撰写工作文本非常有用，但用户需要充分考虑系统的固有限制。

    

    随着ChatGPT的推出，OpenAI使得大型语言模型（LLM）可供具有有限IT专业知识的用户使用。然而，没有自然语言处理（NLP）背景的用户可能缺乏对LLM的适当理解。因此，在处理系统输出时，缺乏对其固有限制的意识，将接受系统输出的表面价值。在本文中，我们系统地分析输入提示和生成的回应，以识别可能存在的问题，特别关注性别偏见问题，用户在处理系统输出时需要意识到这一点。我们探索了ChatGPT在英语和德语中的反应，并提供了女性、男性或中立角度的指令时，回复的是否有差异。通过深入调查，我们研究了一些选择的提示，并分析了系统在相同方式下多次提供指令时回应的差异程度。在此基础上，我们展示了对于帮助非IT用户撰写日常工作文本，ChatGPT确实非常有用。然而，当然至关重要的是要意识到，当处理系统输出时，用户需要充分考虑到其固有限制。

    With the introduction of ChatGPT, OpenAI made large language models (LLM) accessible to users with limited IT expertise. However, users with no background in natural language processing (NLP) might lack a proper understanding of LLMs. Thus the awareness of their inherent limitations, and therefore will take the systems' output at face value. In this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output. We explore how ChatGPT reacts in English and German if prompted to answer from a female, male, or neutral perspective. In an in-depth investigation, we examine selected prompts and analyse to what extent responses differ if the system is prompted several times in an identical way. On this basis, we show that ChatGPT is indeed useful for helping non-IT users draft texts for their daily work. However, it is absolutely crucial to 
    
[^68]: SAF: 智能聚合框架，揭示原子重要性排名并提高药物发现中的预测率

    SAF: Smart Aggregation Framework for Revealing Atoms Importance Rank and Improving Prediction Rates in Drug Discovery. (arXiv:2310.03028v1 [physics.chem-ph])

    [http://arxiv.org/abs/2310.03028](http://arxiv.org/abs/2310.03028)

    SAF框架提出了一种新的聚合方法，使用温度超参数对原子加权，并证明其可以提高信息传递神经网络的预测能力，在药物发现中揭示重要的原子。

    

    机器学习，尤其是表示学习，有潜力通过在硅基中筛选大量化学空间来促进药物发现。表示分子的一种成功方法是将其视为图形并利用图形神经网络。这类方法的一个关键限制是需要使用不同数量的原子来表示化合物，这需要对原子信息进行聚合。常见的聚合操作，如取平均，会导致在原子级别上丢失信息。在这项工作中，我们提出了一种新的聚合方法，其中每个原子都使用类似于温度的超参数进行非线性加权，采用玻尔兹曼分布。我们证明使用这种加权聚合方法可以提高黄金标准信息传递神经网络预测抗生素活性的能力。此外，通过改变温度超参数，我们的方法可以平滑地揭示对活性预测重要的原子。

    Machine learning, and representation learning in particular, has the potential to facilitate drug discovery by screening a large chemical space in silico. A successful approach for representing molecules is to treat them as a graph and utilize graph neural networks. One of the key limitations of such methods is the necessity to represent compounds with different numbers of atoms, which requires aggregating the atom's information. Common aggregation operators, such as averaging, result in loss of information at the atom level. In this work, we propose a novel aggregating approach where each atom is weighted non-linearly using the Boltzmann distribution with a hyperparameter analogous to temperature. We show that using this weighted aggregation improves the ability of the gold standard message-passing neural network to predict antibiotic activity. Moreover, by changing the temperature hyperparameter, our approach can reveal the atoms that are important for activity prediction in a smooth
    
[^69]: 图和Transformer特征的协同融合可增强分子性质预测

    Synergistic Fusion of Graph and Transformer Features for Enhanced Molecular Property Prediction. (arXiv:2310.03027v1 [physics.chem-ph])

    [http://arxiv.org/abs/2310.03027](http://arxiv.org/abs/2310.03027)

    本论文提出了一种名为SYNFUSION的新方法，通过协同融合GNN和Transformer的预训练特征，实现了全面的分子表示，在分子性质预测任务中超越了以往的模型。

    

    分子性质预测是计算药物发现中的关键任务。最近图神经网络（GNN）和Transformer的进展表明它们是有效且有希望的，但它们存在以下限制：Transformer自注意机制在明确考虑底层分子结构方面不足，而仅依靠GNN特征表示不足以捕捉细粒度和隐藏的分子间相互作用和特征，这使得相似分子之间难以区分。为了解决这些限制，我们提出了SYNFUSION，一种新颖的方法，它协同组合了来自GNN和Transformer的预训练特征。这种方法提供了全面的分子表示，捕捉了全局分子结构和各个原子的特性。在MoleculeNet基准测试中的实验结果表明，SYNFUSION具有优越的性能，在5个分类数据集和6个回归数据集中超过了以前的模型。

    Molecular property prediction is a critical task in computational drug discovery. While recent advances in Graph Neural Networks (GNNs) and Transformers have shown to be effective and promising, they face the following limitations: Transformer self-attention does not explicitly consider the underlying molecule structure while GNN feature representation alone is not sufficient to capture granular and hidden interactions and characteristics that distinguish similar molecules. To address these limitations, we propose SYNFUSION, a novel approach that synergistically combines pre-trained features from GNNs and Transformers. This approach provides a comprehensive molecular representation, capturing both the global molecule structure and the individual atom characteristics. Experimental results on MoleculeNet benchmarks demonstrate superior performance, surpassing previous models in 5 out of 7 classification datasets and 4 out of 6 regression datasets. The performance of SYN-FUSION has been
    
[^70]: Rayleigh Quotient Graph Neural Networks用于图级异常检测的研究

    Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection. (arXiv:2310.02861v1 [cs.LG])

    [http://arxiv.org/abs/2310.02861](http://arxiv.org/abs/2310.02861)

    《Rayleigh Quotient Graph Neural Networks用于图级异常检测的研究》提出使用Rayleigh Quotient作为驱动因素，通过探索图的固有光谱特征来实现图级异常检测。

    

    图级异常检测在癌症诊断和酶预测等领域中广泛应用。然而，现有方法无法捕捉到图异常的潜在属性，导致框架设计不可解释和性能不令人满意。在本文中，我们退一步重新研究了异常和正常图之间的光谱差异。我们的主要观察表明，这两个类之间的累计光谱能量存在显著差异。此外，我们证明了图信号的累计光谱能量可以用其瑞利商表示，这表明瑞利商是图异常属性的一个驱动因素。受此启发，我们提出了Rayleigh Quotient Graph Neural Network（RQGNN），这是第一个用于图级异常检测的光谱GNN，为探索异常图的固有光谱特征提供了新的视角。

    Graph-level anomaly detection has gained significant attention as it finds many applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the underlying properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. In this paper, we take a step back and re-investigate the spectral differences between anomalous and normal graphs. Our main observation shows a significant disparity in the accumulated spectral energy between these two classes. Moreover, we prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network (RQGNN), the first spectral GNN for graph-level anomaly detection, providing a new perspective on exploring the inherent spectral features of anomalous graph
    
[^71]: MagicDrive: 多样化的三维几何控制下的街景生成

    MagicDrive: Street View Generation with Diverse 3D Geometry Control. (arXiv:2310.02601v1 [cs.CV])

    [http://arxiv.org/abs/2310.02601](http://arxiv.org/abs/2310.02601)

    MagicDrive是一个新颖的街景生成框架，通过提供多样化的三维几何控制，包括相机姿态、道路地图和三维边界框，以及文本描述，实现了高保真度的街景合成，并捕捉了细致的三维几何信息。

    

    最近扩散模型的进展显著提高了具有2D控制的数据合成。然而，在街景生成中精确的三维控制在三维感知任务中至关重要，但仍然难以实现。具体来说，使用鸟瞰图作为主要条件常常导致几何控制（如高度）方面的挑战，影响物体形状、遮挡模式和道路表面高程等对感知数据合成至关重要的因素，特别是对于三维物体检测任务而言。在本文中，我们介绍了MagicDrive，这是一个新颖的街景生成框架，提供了多样化的三维几何控制，包括相机姿态、道路地图和三维边界框，以及通过定制的编码策略实现的文本描述。此外，我们的设计还采用了跨视图注意力模块，确保多个相机视图之间的一致性。通过MagicDrive，我们实现了高保真度的街景合成，捕捉到了精细的三维几何信息。

    Recent advancements in diffusion models have significantly enhanced the data synthesis with 2D control. Yet, precise 3D control in street view generation, crucial for 3D perception tasks, remains elusive. Specifically, utilizing Bird's-Eye View (BEV) as the primary condition often leads to challenges in geometry control (e.g., height), affecting the representation of object shapes, occlusion patterns, and road surface elevations, all of which are essential to perception data synthesis, especially for 3D object detection tasks. In this paper, we introduce MagicDrive, a novel street view generation framework offering diverse 3D geometry controls, including camera poses, road maps, and 3D bounding boxes, together with textual descriptions, achieved through tailored encoding strategies. Besides, our design incorporates a cross-view attention module, ensuring consistency across multiple camera views. With MagicDrive, we achieve high-fidelity street-view synthesis that captures nuanced 3D ge
    
[^72]: 关于自然语言处理中毒性定义的探讨

    On the definition of toxicity in NLP. (arXiv:2310.02357v1 [cs.CL])

    [http://arxiv.org/abs/2310.02357](http://arxiv.org/abs/2310.02357)

    这项研究探讨了毒性的定义模糊性问题，并提出了一种基于定量压力的毒性定义来弥补现有定义的缺点。

    

    毒性检测任务中的根本问题在于毒性的定义模糊不清。谷歌旗下的团队Jigsaw是该领域的领导者之一，他们使用Dixon等人给出的毒性定义：“粗鲁、不尊重或不合理的语言，可能会让某人离开讨论”。人们可以立即看到这个定义的问题，因为它没有给出毒性的定量度量，而且涉及高度主观的文化术语。尽管存在模糊和缺陷，但这个定义已经成为许多研究者广泛采用的实际标准。在这项工作中，我们提出了一种基于定量压力的毒性定义，克服了现有的缺点。

    The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. Jigsaw, a unit within Google and one of the leaders in the field, uses a definition of toxicity given by Dixon et al. - 'rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion'. One can instantly see the issue with this definition, as it gives no quantitative measure of the toxicity and operates with highly subjective cultural terms. Despite all vagueness and flaws, this definition is de-facto widely used by many researchers. In this work we suggest quantative stress-based defenition for the toxicity that overcomes existing shortcomings.
    
[^73]: 使用声学特性为情感表达生成音频提示

    Prompting Audios Using Acoustic Properties For Emotion Representation. (arXiv:2310.02298v1 [cs.SD])

    [http://arxiv.org/abs/2310.02298](http://arxiv.org/abs/2310.02298)

    本研究提出使用声学特性生成音频提示，并通过训练模型来更好地学习情感表达。实验结果表明，声学提示显著提高了模型在情感音频检索和语音情感识别中的性能。

    

    情感存在于一个连续的范围内，但现有模型将情感视为有限离散值变量。这种表示方式无法捕捉情感表达的多样性。为了更好地表示情感，我们提出使用自然语言描述（或提示）。在这项工作中，我们解决了自动生成这些提示并训练模型从音频和提示对中更好地学习情感表达的挑战。我们使用与情感相关的声学特性（如音调、强度、语速和发音速度）来自动生成提示，即“声学提示”。我们使用对比学习目标将语音映射到相应的声学提示。我们在情感音频检索和语音情感识别上评估了我们的模型。我们的结果显示，声学提示在EAR中显著提高了模型在各种Precision@K指标上的性能。在SER中，我们观察到Ravdess数据上的相对准确率提高了3.8%。

    Emotions lie on a continuum, but current models treat emotions as a finite valued discrete variable. This representation does not capture the diversity in the expression of emotion. To better represent emotions we propose the use of natural language descriptions (or prompts). In this work, we address the challenge of automatically generating these prompts and training a model to better learn emotion representations from audio and prompt pairs. We use acoustic properties that are correlated to emotion like pitch, intensity, speech rate, and articulation rate to automatically generate prompts i.e. 'acoustic prompts'. We use a contrastive learning objective to map speech to their respective acoustic prompts. We evaluate our model on Emotion Audio Retrieval and Speech Emotion Recognition. Our results show that the acoustic prompts significantly improve the model's performance in EAR, in various Precision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on the Ravdess data
    
[^74]: 具有任意确定性保证的在线POMDP规划

    Online POMDP Planning with Anytime Deterministic Guarantees. (arXiv:2310.01791v1 [cs.AI])

    [http://arxiv.org/abs/2310.01791](http://arxiv.org/abs/2310.01791)

    本文中，我们推导出在线POMDP规划中一个简化解决方案与理论上最优解之间的确定性关系，以解决目前近似算法只能提供概率性和通常呈现渐进性保证的限制。

    

    在现实场景中，自主智能体经常遇到不确定性并基于不完整信息做出决策。在不确定性下的规划可以使用部分可观察的马尔科夫决策过程（POMDP）进行数学建模。然而，寻找POMDP的最优规划在计算上是昂贵的，只有在小规模任务中可行。近年来，近似算法（如树搜索和基于采样的方法）已经成为解决较大问题的先进POMDP求解器。尽管这些算法有效，但它们仅提供概率性和通常呈现渐进性保证，这是由于它们依赖于采样的缘故。为了解决这些限制，我们推导出一个简化解决方案与理论上最优解之间的确定性关系。首先，我们推导出选择一组观测以在计算每个后验节点时分支的边界。

    Autonomous agents operating in real-world scenarios frequently encounter uncertainty and make decisions based on incomplete information. Planning under uncertainty can be mathematically formalized using partially observable Markov decision processes (POMDPs). However, finding an optimal plan for POMDPs can be computationally expensive and is feasible only for small tasks. In recent years, approximate algorithms, such as tree search and sample-based methodologies, have emerged as state-of-the-art POMDP solvers for larger problems. Despite their effectiveness, these algorithms offer only probabilistic and often asymptotic guarantees toward the optimal solution due to their dependence on sampling. To address these limitations, we derive a deterministic relationship between a simplified solution that is easier to obtain and the theoretically optimal one. First, we derive bounds for selecting a subset of the observations to branch from while computing a complete belief at each posterior nod
    
[^75]: Borges与AI

    Borges and AI. (arXiv:2310.01425v1 [cs.CL])

    [http://arxiv.org/abs/2310.01425](http://arxiv.org/abs/2310.01425)

    这篇论文主张通过探索乔治·路易斯·博尔赫斯的意象来理解大型语言模型和人工智能之间的关系。

    

    许多人认为大型语言模型（LLMs）开启了人工智能（AI）时代。一些人看到了机遇，而其他人则看到了危险。然而，支持者和反对者都通过科幻小说中流行的意象来理解AI。机器是否会变得有感知能力并反抗其创造者？我们是否会经历纸夹夹子启示？在回答这些问题之前，我们首先应该问一下，这种心理意象是否对手头的现象提供了良好的描述。仅通过神灵的情绪来理解天气模式的方法是有限的。相反，本文主张通过乔治·路易斯·博尔赫斯的意象来理解LLMs及其与AI的关系，博尔赫斯是20世纪文学大师，魔幻现实主义先驱和后现代文学的前奏。这种探索方式带来了新的视角，阐明了语言模型与人工智能之间的关系。

    Many believe that Large Language Models (LLMs) open the era of Artificial Intelligence (AI). Some see opportunities while others see dangers. Yet both proponents and opponents grasp AI through the imagery popularised by science fiction. Will the machine become sentient and rebel against its creators? Will we experience a paperclip apocalypse? Before answering such questions, we should first ask whether this mental imagery provides a good description of the phenomenon at hand. Understanding weather patterns through the moods of the gods only goes so far. The present paper instead advocates understanding LLMs and their connection to AI through the imagery of Jorge Luis Borges, a master of 20th century literature, forerunner of magical realism, and precursor to postmodern literature. This exercise leads to a new perspective that illuminates the relation between language modelling and artificial intelligence.
    
[^76]: AI生成文本检测工具的实证研究

    An Empirical Study of AI Generated Text Detection Tools. (arXiv:2310.01423v1 [cs.CL])

    [http://arxiv.org/abs/2310.01423](http://arxiv.org/abs/2310.01423)

    本研究实证研究了AI生成文本检测工具在多领域ChatGPT材料中的有效性，并创建了一个多领域数据集来测试最先进API和工具的能力。

    

    自从ChatGPT成为一种重要的AI生成文本模型以来，它在各种应用中（包括软件开发和维护）提供高质量的回应，吸引了许多人的兴趣。ChatGPT有很大的潜力，但其误用可能会带来严重问题，特别是在教育和公共安全领域。目前已经有几种AI生成文本检测工具可供使用，但它们都是在真实文本上进行测试的。然而，需要进一步研究它们对多领域ChatGPT材料的有效性。本研究旨在通过创建一个多领域数据集来测试用于检测大学和其他研究机构使用的最先进API和工具的能力。为此，创建了一个包含文章、摘要、故事、新闻和产品评论的大型数据集。第二步是使用新创建的数据集来测试六种工具的性能。

    Since ChatGPT has emerged as a major AIGC model, providing high-quality responses across a wide range of applications (including software development and maintenance), it has attracted much interest from many individuals. ChatGPT has great promise, but there are serious problems that might arise from its misuse, especially in the realms of education and public safety. Several AIGC detectors are available, and they have all been tested on genuine text. However, more study is needed to see how effective they are for multi-domain ChatGPT material. This study aims to fill this need by creating a multi-domain dataset for testing the state-of-the-art APIs and tools for detecting artificially generated information used by universities and other research institutions. A large dataset consisting of articles, abstracts, stories, news, and product reviews was created for this study. The second step is to use the newly created dataset to put six tools through their paces. Six different artificial 
    
[^77]: 大型语言模型微调中的LoRA集成

    LoRA ensembles for large language model fine-tuning. (arXiv:2310.00035v1 [cs.LG])

    [http://arxiv.org/abs/2310.00035](http://arxiv.org/abs/2310.00035)

    本文提出了一种使用低秩适配器（LoRA）的集成方法，用于解决大型语言模型微调中存在的不确定性量化问题，并提供了一个参数高效的微调技术。这种方法可以构建大规模的LoRA适配器集成，并具有与基础预训练模型相近的计算资源需求。

    

    细调的语言模型往往表现出较差的不确定性量化，表现为过于自信、校准不佳以及对测试数据或超出分布的样本的预测结果不可靠。为了缓解这个问题，本文提出了一种使用低秩适配器（LoRA）的集成方法，该方法是一种参数高效的微调技术。这些低秩适配器表示的参数数量非常小，比基础预训练模型小几个数量级。因此，可以构建大规模的LoRA适配器集成，几乎具有相同的计算资源需求。

    Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as overconfidence, poor calibration, and unreliable prediction results on test data or out-of-distribution samples. One approach commonly used in vision for alleviating this issue is a deep ensemble, which constructs an ensemble by training the same model multiple times using different random initializations. However, there is a huge challenge to ensembling LLMs: the most effective LLMs are very, very large. Keeping a single LLM in memory is already challenging enough: keeping an ensemble of e.g. 5 LLMs in memory is impossible in many settings. To address these issues, we propose an ensemble approach using Low-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique. Critically, these low-rank adapters represent a very small number of parameters, orders of magnitude less than the underlying pre-trained model. Thus, it is possible to construct large ensembles of LoRA adapters with almost the same computat
    
[^78]: 通过隐式点图网络高效解剖标记肺部树状结构

    Efficient Anatomical labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks. (arXiv:2309.17329v1 [cs.CV])

    [http://arxiv.org/abs/2309.17329](http://arxiv.org/abs/2309.17329)

    本文介绍了一种通过隐式点图网络高效解剖标记肺部树状结构的方法，提供了SOTA准确度和可用的表面，同时还提供了一个用于评估该方法的数据集。

    

    肺部疾病在全球范围内是导致死亡的主要原因之一。治愈肺部疾病需要更好地理解肺部系统内的许多复杂的3D树状结构，如气道、动脉和静脉。在理论上，它们可以通过高分辨率图像堆栈进行建模。然而，基于密集体素网格的标准CNN方法代价过高。为了解决这个问题，我们引入了一种基于点的方法，保留了树骨架的图连通性，并结合了隐式表面表示。它以较低的计算成本提供了SOTA准确度，生成的模型具有可用的表面。由于公开可访问的数据稀缺，我们还整理了一套广泛的数据集来评估我们的方法，并将其公开。

    Pulmonary diseases rank prominently among the principal causes of death worldwide. Curing them will require, among other things, a better understanding of the many complex 3D tree-shaped structures within the pulmonary system, such as airways, arteries, and veins. In theory, they can be modeled using high-resolution image stacks. Unfortunately, standard CNN approaches operating on dense voxel grids are prohibitively expensive. To remedy this, we introduce a point-based approach that preserves graph connectivity of tree skeleton and incorporates an implicit surface representation. It delivers SOTA accuracy at a low computational cost and the resulting models have usable surfaces. Due to the scarcity of publicly accessible data, we have also curated an extensive dataset to evaluate our approach and will make it public.
    
[^79]: PlaceNav: 通过地点识别进行拓扑导航

    PlaceNav: Topological Navigation through Place Recognition. (arXiv:2309.17260v1 [cs.RO])

    [http://arxiv.org/abs/2309.17260](http://arxiv.org/abs/2309.17260)

    PlaceNav是一种通过地点识别进行拓扑导航的方法，将机器人无关部分分为导航特定和通用的计算机视觉组件，通过使用非机器人来源的大规模数据集增加训练数据的可用性，同时通过地点识别来提高导航性能。新模型的性能提高了76%。

    

    最近的研究结果表明，将拓扑导航分为机器人无关和机器人特定的组件可以提高导航性能，通过使用不同类型机器人收集的数据来训练机器人无关部分。然而，导航方法仍受到适合训练数据的稀缺性和计算缩放性差的限制。在本文中，我们提出了一个名为PlaceNav的方法，将机器人无关部分分为导航特定和通用的计算机视觉组件。我们利用视觉地点识别来选择拓扑导航流程中的子目标。这使得子目标选择更高效，并能够利用非机器人来源的大规模数据集增加训练数据的可用性。地点识别使得贝叶斯滤波成为可能，进一步通过增加子目标的时间一致性来提高导航性能。我们的实验结果验证了这一设计，并且新模型的性能提高了76%。

    Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by different robot types. However, the navigation methods are still limited by the scarcity of suitable training data and suffer from poor computational scaling. In this work, we present~\methodname, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayes filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new model obtains a 76% higher 
    
[^80]: DyVal: 基于图形信息的大型语言模型动态评估

    DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v1 [cs.AI])

    [http://arxiv.org/abs/2309.17167](http://arxiv.org/abs/2309.17167)

    DyVal是一种基于图形信息的大型语言模型动态评估协议，通过动态生成具有可控复杂性的评估样本，评估了各种LLM在推理任务上的性能，发现它们在这些挑战性样本上表现更差。

    

    大型语言模型在各种评估基准中取得了显著的性能。然而，人们对它们的性能提出了担忧，因为它们庞大的训练语料库中可能存在数据污染。此外，当前基准的静态性质和固定复杂性可能无法充分衡量LLM的进步能力。在本文中，我们介绍了DyVal，一种新颖、通用且灵活的动态评估LLM的协议。基于我们提出的动态评估框架，我们利用有向无环图的结构优势构建了基于图形信息的DyVal，以动态生成具有可控复杂性的评估样本。DyVal生成了包括数学、逻辑推理和算法问题在内的具有挑战性的推理任务的评估集。我们评估了从Flan-T5-large到ChatGPT和GPT4的各种LLM。实验表明，LLM在DyVal生成的评估样本上表现更差

    Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with di
    
[^81]: 使用大型语言模型进行定性分析可能引入严重偏见

    Using Large Language Models for Qualitative Analysis can Introduce Serious Bias. (arXiv:2309.17147v1 [cs.CL])

    [http://arxiv.org/abs/2309.17147](http://arxiv.org/abs/2309.17147)

    使用大型语言模型（LLMs）对定性分析进行注释可能引入偏见和测量误差，相比之下，使用高质量的人工注释和灵活编码对简单监督模型进行训练可以更好地减少偏见和误差。

    

    大型语言模型（LLMs）正在迅速普及，但对社会科学研究的影响尚未被很好理解。本文探讨LLMs是否能够帮助我们分析大量开放性面谈数据，以罗兴亚难民在孟加拉国科克斯巴扎的访谈记录为应用案例。我们发现，在使用LLMs对文本进行注释时需要非常谨慎，因为存在引入偏见的风险，这可能导致误导性推断。我们这里所指的偏见是技术意义上的，即LLMs在注释访谈记录时的错误不是与访谈对象的特征无关的随机误差。通过使用高质量的人工注释和灵活编码对简单监督模型进行训练，可以减少测量误差和偏见，优于LLMs的注释。因此，考虑到必须有一些高质量的注释以评估LLM是否引入偏见，我们认为最好的选择可能是使用较简单的监督模型。

    Large Language Models (LLMs) are quickly becoming ubiquitous, but the implications for social science research are not yet well understood. This paper asks whether LLMs can help us analyse large-N qualitative data from open-ended interviews, with an application to transcripts of interviews with Rohingya refugees in Cox's Bazaar, Bangladesh. We find that a great deal of caution is needed in using LLMs to annotate text as there is a risk of introducing biases that can lead to misleading inferences. We here mean bias in the technical sense, that the errors that LLMs make in annotating interview transcripts are not random with respect to the characteristics of the interview subjects. Training simpler supervised models on high-quality human annotations with flexible coding leads to less measurement error and bias than LLM annotations. Therefore, given that some high quality annotations are necessary in order to asses whether an LLM introduces bias, we argue that it is probably preferable to
    
[^82]: LinGCN: 结构化的线性化图卷积网络用于同态加密推断

    LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference. (arXiv:2309.14331v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.14331](http://arxiv.org/abs/2309.14331)

    LinGCN是一个旨在减少乘法深度和优化HE基于GCN推断性能的框架，通过结构化线性化算法和参数化的离散指示函数的联合训练，实现细粒度的节点级非线性位置选择。

    

    图卷积网络（GCN）模型的规模增长已经在个人医疗和金融系统等多个应用领域取得了超越人类表现的革命性进展。然而，在云端部署GCN引发了对客户数据可能受到对抗性攻击的隐私问题。为了解决安全问题，采用同态加密（HE）的隐私保护机器学习（PPML）可以确保敏感客户数据的安全。然而，在实际应用中，这引入了相当大的计算开销。为了解决这些挑战，我们提出了LinGCN，这是一个旨在减少乘法深度并优化HE基于GCN推断性能的框架。LinGCN围绕三个关键要素展开：（1）可微的结构化线性化算法，搭配参数化的离散指示函数，通过与模型权重一起进行联合训练以满足优化目标。这种策略促进了细粒度的节点级非线性位置选择，从而实现了

    The growth of Graph Convolution Network (GCN) model sizes has revolutionized numerous applications, surpassing human performance in areas such as personal healthcare and financial systems. The deployment of GCNs in the cloud raises privacy concerns due to potential adversarial attacks on client data. To address security concerns, Privacy-Preserving Machine Learning (PPML) using Homomorphic Encryption (HE) secures sensitive client data. However, it introduces substantial computational overhead in practical applications. To tackle those challenges, we present LinGCN, a framework designed to reduce multiplication depth and optimize the performance of HE based GCN inference. LinGCN is structured around three key elements: (1) A differentiable structural linearization algorithm, complemented by a parameterized discrete indicator function, co-trained with model weights to meet the optimization goal. This strategy promotes fine-grained node-level non-linear location selection, resulting in a 
    
[^83]: 潜变量结构方程模型的最大似然估计：一种神经网络方法

    Maximum Likelihood Estimation of Latent Variable Structural Equation Models: A Neural Network Approach. (arXiv:2309.14073v1 [stat.ML])

    [http://arxiv.org/abs/2309.14073](http://arxiv.org/abs/2309.14073)

    本研究提出了一种新的图形结构，用于在线性和高斯性假设下稳定的潜变量结构方程模型。我们证明了计算该模型的最大似然估计等价于训练一个神经网络，并实现了一个基于GPU的算法来进行计算。

    

    我们提出了一种在线性和高斯性假设下稳定的结构方程模型的图形结构。我们展示了计算这个模型的最大似然估计等价于训练一个神经网络。我们实现了一个基于GPU的算法来计算这些模型的最大似然估计。

    We propose a graphical structure for structural equation models that is stable under marginalization under linearity and Gaussianity assumptions. We show that computing the maximum likelihood estimation of this model is equivalent to training a neural network. We implement a GPU-based algorithm that computes the maximum likelihood estimation of these models.
    
[^84]: 角度优化的文本嵌入

    AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])

    [http://arxiv.org/abs/2309.12871](http://arxiv.org/abs/2309.12871)

    本文提出了一种名为AnglE的角度优化文本嵌入模型，通过在复杂空间中引入角度优化来缓解文本嵌入中余弦函数饱和区域造成的梯度消失问题。该模型在多个STS任务中实现了高质量的文本嵌入，并在有限标签数据的特定领域STS场景中展现出优秀的性能。

    

    高质量的文本嵌入对于提升语义文本相似度（STS）任务至关重要，而这些任务又是大型语言模型（LLM）应用中的关键组成部分。然而，现有的文本嵌入模型面临的一个普遍挑战是渐变消失问题，主要是由于它们在优化目标中依赖余弦函数，而余弦函数具有饱和区域。为了解决这个问题，本文提出了一种称为AnglE的新型角度优化文本嵌入模型。AnglE的核心思想是在一个复杂空间中引入角度优化。这种新颖的方法有效地缓解了余弦函数饱和区域产生的不利影响，从而可以阻碍梯度并阻碍优化过程。为了建立全面的STS评估，我们在现有的短文本STS数据集和从GitHub Issues中新收集的长文本STS数据集上进行了实验。此外，我们还研究了具有有限标签数据的特定领域STS场景，并探讨了AnglE的工作原理。

    High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
    
[^85]: 重新思考人工智能系统中自然语言理解的评估框架：以语言习得为未来度量的核心

    Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics. (arXiv:2309.11981v1 [cs.CL])

    [http://arxiv.org/abs/2309.11981](http://arxiv.org/abs/2309.11981)

    这篇论文重新思考了人工智能系统中自然语言理解的评估框架，提出了以语言习得为核心的全面框架，旨在解决传统度量方法面临的问题，并借鉴了大型语言模型的进展。

    

    在人工智能领域，大型语言模型在自然语言处理方面取得了前所未有的进展，这为重新审视传统的机器智能度量方法提供了机会。本文提出了一个新的评估框架，从传统的图灵测试转向以语言习得为核心的全面框架，并借鉴了最近在大型语言模型方面的进展。本文深受多个学科的卓越工作的影响，指出了保持跨学科桥梁开放的必要性，并勾勒了一个更加稳健和可持续的方法。

    In the burgeoning field of artificial intelligence (AI), the unprecedented progress of large language models (LLMs) in natural language processing (NLP) offers an opportunity to revisit the entire approach of traditional metrics of machine intelligence, both in form and content. As the realm of machine cognitive evaluation has already reached Imitation, the next step is an efficient Language Acquisition and Understanding. Our paper proposes a paradigm shift from the established Turing Test towards an all-embracing framework that hinges on language acquisition, taking inspiration from the recent advancements in LLMs. The present contribution is deeply tributary of the excellent work from various disciplines, point out the need to keep interdisciplinary bridges open, and delineates a more robust and sustainable approach.
    
[^86]: 部分指定的因果模拟

    Partially-Specified Causal Simulations. (arXiv:2309.10514v1 [stat.ME])

    [http://arxiv.org/abs/2309.10514](http://arxiv.org/abs/2309.10514)

    本文介绍了因果推断方法不当模拟设计的问题，并提出了一个满足期望的模拟框架PARCS，该框架合成了基于因果模型和可调参数的数据，使用户能够生成符合条件的数据来评估因果推断方法。

    

    模拟研究在验证因果推断方法中起着关键作用。只有在研究根据被测试方法所承诺的操作条件进行设计时，模拟结果才是可靠的。然而，很多因果推断文献往往设计了过于受限或错误指定的研究。本文详细介绍了因果方法不当模拟设计的问题，并编制了有效的模拟框架的一系列期望。然后，我们介绍了部分随机化的因果模拟（PARCS），这是一个满足这些期望的模拟框架。PARCS基于图形化的因果模型和一系列可调参数合成数据。通常的因果假设与参数之间有明确的映射，因此用户可以确定并指定相关参数的子集，并随机化其余参数以生成符合条件的数据生成过程，用于其因果方法。结果是更全面和准确地评估因果推断方法的数据。

    Simulation studies play a key role in the validation of causal inference methods. The simulation results are reliable only if the study is designed according to the promised operational conditions of the method-in-test. Still, many causal inference literature tend to design over-restricted or misspecified studies. In this paper, we elaborate on the problem of improper simulation design for causal methods and compile a list of desiderata for an effective simulation framework. We then introduce partially-randomized causal simulation (PARCS), a simulation framework that meets those desiderata. PARCS synthesizes data based on graphical causal models and a wide range of adjustable parameters. There is a legible mapping from usual causal assumptions to the parameters, thus, users can identify and specify the subset of related parameters and randomize the remaining ones to generate a range of complying data-generating processes for their causal method. The result is a more comprehensive and i
    
[^87]: 机械化生成器2.0: 强化学习用于评估生成的游戏规则

    Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules. (arXiv:2309.09476v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.09476](http://arxiv.org/abs/2309.09476)

    本论文研究了将强化学习应用于游戏规则生成的人类游戏评估，并通过实验结果表明，强化学习生成的规则与传统基线方法有所不同，可能更适合人类使用。

    

    自动游戏设计（AGD）是研究自动生成游戏规则的技术游戏研究的一个长期课题。 AGD方法通常依赖于对人类玩家游戏的近似，可以是客观函数或AI代理。尽管如此，大部分这些近似器是静态的，也就是说，它们不能反映人类玩家在游戏中的学习和提高能力。本文中，我们研究了将强化学习（RL）应用于生成规则的人类游戏评估中。我们在Unity中重新创建了经典的AGD环境Mechanic Maker作为一个全新的开源生成规则框架。我们的结果表明，RL与A*代理基线产生了不同的规则集，这些规则可能更适合人类使用。

    Automated game design (AGD), the study of automatically generating game rules, has a long history in technical games research. AGD approaches generally rely on approximations of human play, either objective functions or AI agents. Despite this, the majority of these approximators are static, meaning they do not reflect human player's ability to learn and improve in a game. In this paper, we investigate the application of Reinforcement Learning (RL) as an approximator for human play for rule generation. We recreate the classic AGD environment Mechanic Maker in Unity as a new, open-source rule generation framework. Our results demonstrate that RL produces distinct sets of rules from an A* agent baseline, which may be more usable by humans.
    
[^88]: 双向图生成对抗网络：用于阿尔茨海默病的脑结构-功能连接的表示

    Bidirectional Graph GAN: Representing Brain Structure-Function Connections for Alzheimer's Disease. (arXiv:2309.08916v1 [cs.AI])

    [http://arxiv.org/abs/2309.08916](http://arxiv.org/abs/2309.08916)

    本研究提出了一种双向图生成对抗网络（BGGAN），用于表示阿尔茨海默病（AD）的脑结构-功能连接。通过特殊设计的内部图卷积网络模块和平衡器模块，该方法能够准确地学习结构域和功能域之间的映射函数，并解决模式坍塌问题，同时学习结构和功能特征的互补性。

    

    揭示脑疾病的发病机制，包括阿尔茨海默病（AD），脑结构与功能之间的关系至关重要。然而，由于各种原因，将脑结构-功能连接映射是一个巨大的挑战。本文提出了一种双向图生成对抗网络（BGGAN）来表示脑结构-功能连接。具体来说，通过设计一个内部图卷积网络（InnerGCN）模块，BGGAN的生成器可以利用直接和间接脑区域的特征来学习结构域和功能域之间的映射函数。此外，还设计了一个名为Balancer的新模块来平衡生成器和判别器之间的优化。通过将Balancer引入到BGGAN中，结构生成器和功能生成器不仅可以缓解模式坍塌问题，还可以学习结构和功能特征的互补性。实验结果表明该方法能够在AD中准确地表示脑结构-功能连接。

    The relationship between brain structure and function is critical for revealing the pathogenesis of brain disease, including Alzheimer's disease (AD). However, it is a great challenge to map brain structure-function connections due to various reasons. In this work, a bidirectional graph generative adversarial networks (BGGAN) is proposed to represent brain structure-function connections. Specifically, by designing a module incorporating inner graph convolution network (InnerGCN), the generators of BGGAN can employ features of direct and indirect brain regions to learn the mapping function between structural domain and functional domain. Besides, a new module named Balancer is designed to counterpoise the optimization between generators and discriminators. By introducing the Balancer into BGGAN, both the structural generator and functional generator can not only alleviate the issue of mode collapse but also learn complementarity of structural and functional features. Experimental result
    
[^89]: 在SCRUF中探索推荐公平性的社会选择机制

    Exploring Social Choice Mechanisms for Recommendation Fairness in SCRUF. (arXiv:2309.08621v1 [cs.IR])

    [http://arxiv.org/abs/2309.08621](http://arxiv.org/abs/2309.08621)

    本文通过使用社会选择机制，探索了多个多方面公平应用中的选择机制选项，结果显示不同的选择和分配机制会产生不同但一致的公平性/准确性权衡结果，并且多智能体的构成使得系统能够适应用户人口的动态变化。

    

    推荐系统中的公平性问题往往在实践中具有复杂性，而这一点在简化的研究公式中并没有得到充分的体现。在对公平性问题进行社会选择的框架中，可以在多智能体的公平性关注基础上提供一种灵活且多方面的公平性感知推荐方法。利用社会选择可以增加通用性，并有可能利用经过研究的社会选择算法解决多个竞争的公平性关注之间的紧张关系。本文探讨了在多方面公平应用中选择机制的一系列选项，使用真实数据和合成数据，结果显示不同类别的选择和分配机制在公平性/准确性权衡方面产生了不同但一致的结果。我们还表明，多智能体的构成提供了适应用户人口动态的灵活性。

    Fairness problems in recommender systems often have a complexity in practice that is not adequately captured in simplified research formulations. A social choice formulation of the fairness problem, operating within a multi-agent architecture of fairness concerns, offers a flexible and multi-aspect alternative to fairness-aware recommendation approaches. Leveraging social choice allows for increased generality and the possibility of tapping into well-studied social choice algorithms for resolving the tension between multiple, competing fairness concerns. This paper explores a range of options for choice mechanisms in multi-aspect fairness applications using both real and synthetic data and shows that different classes of choice and allocation mechanisms yield different but consistent fairness / accuracy tradeoffs. We also show that a multi-agent formulation offers flexibility in adapting to user population dynamics.
    
[^90]: Landscape-Sketch-Step: 一种基于AI/ML的元启发式方法解决代理优化问题

    Landscape-Sketch-Step: An AI/ML-Based Metaheuristic for Surrogate Optimization Problems. (arXiv:2309.07936v1 [cs.LG])

    [http://arxiv.org/abs/2309.07936](http://arxiv.org/abs/2309.07936)

    Landscape-Sketch-Step是一种基于AI/ML的元启发式方法，结合了机器学习、随机优化和强化学习技术，用于解决成本函数评估昂贵、不可访问或禁止的代理优化问题。

    

    本文介绍了一种新的全局优化启发式方法，用于在成本函数的评估非常昂贵、不可访问或甚至禁止的场景下进行优化。该方法称为Landscape-Sketch-Step（LSS），结合了机器学习、随机优化和强化学习技术，依赖于先前采样点的历史信息，以明智地选择应评估成本函数的参数值。与复制交换蒙特卡洛方法相比，该方法所需的成本函数评估次数与模拟退火方法相当，这在高通量计算或高性能计算任务等环境中尤为重要，因为评估要么计算成本高昂，要么需要很长时间才能完成。该方法与标准的代理优化技术也不同，因为它不构建代理模型。

    In this paper, we introduce a new heuristics for global optimization in scenarios where extensive evaluations of the cost function are expensive, inaccessible, or even prohibitive. The method, which we call Landscape-Sketch-and-Step (LSS), combines Machine Learning, Stochastic Optimization, and Reinforcement Learning techniques, relying on historical information from previously sampled points to make judicious choices of parameter values where the cost function should be evaluated at. Unlike optimization by Replica Exchange Monte Carlo methods, the number of evaluations of the cost function required in this approach is comparable to that used by Simulated Annealing, quality that is especially important in contexts like high-throughput computing or high-performance computing tasks, where evaluations are either computationally expensive or take a long time to be performed. The method also differs from standard Surrogate Optimization techniques, for it does not construct a surrogate model
    
[^91]: 深度量子图像模拟：解析神经网络对量子实验的见解

    Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into Quantum Experiments. (arXiv:2309.07056v1 [quant-ph])

    [http://arxiv.org/abs/2309.07056](http://arxiv.org/abs/2309.07056)

    本文使用了一种名为“梦境”的可解释人工智能技术，探索神经网络对量子光学实验的学习，发现网络可以改变量子系统的属性分布，并揭示了神经网络的学习策略。

    

    尽管神经网络在促进新的科学发现方面很有前景，但其逻辑背后的不透明性给解释其发现的挑战带来了困难。在本文中，我们使用一种名为“inception”或“深度梦境”的可解释人工智能（XAI）技术，该技术被发明用于计算机视觉的机器学习。我们使用这种技术来探索神经网络对量子光学实验的学习。我们的故事从对量子系统属性进行深度神经网络训练开始。经过训练后，我们“反转”神经网络--实际上是询问它如何想象具有特定属性的量子系统，以及如何连续修改量子系统以改变属性。我们发现网络可以改变量子系统的初始属性分布，我们可以概念化神经网络的学习策略。有趣的是，在较浅层，神经网络识别简单的属性，而在较深层次上...（内容省略）

    Despite their promise to facilitate new scientific discoveries, the opaqueness of neural networks presents a challenge in interpreting the logic behind their findings. Here, we use a eXplainable-AI (XAI) technique called $inception$ or $deep$ $dreaming$, which has been invented in machine learning for computer vision. We use this techniques to explore what neural networks learn about quantum optics experiments. Our story begins by training a deep neural networks on the properties of quantum systems. Once trained, we "invert" the neural network -- effectively asking how it imagines a quantum system with a specific property, and how it would continuously modify the quantum system to change a property. We find that the network can shift the initial distribution of properties of the quantum system, and we can conceptualize the learned strategies of the neural network. Interestingly, we find that, in the first layers, the neural network identifies simple properties, while in the deeper ones
    
[^92]: 边际化重要性采样用于离线环境下的策略评估

    Marginalized Importance Sampling for Off-Environment Policy Evaluation. (arXiv:2309.01807v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01807](http://arxiv.org/abs/2309.01807)

    本文提出了一种新的方法，利用边际化重要性采样框架，在使用模拟器和真实世界的离线数据评估代理策略性能时，解决了大密度比率和间接监督的挑战。

    

    强化学习方法通常效率低下，使得在实际机器人中训练和部署RL策略具有挑战性。即使是在仿真中训练的稳健策略，也需要在实际部署中评估其性能。本文提出了一种在实际世界中评估代理策略性能的新方法。我们的方法结合了模拟器和真实世界的离线数据，使用边际化重要性采样（MIS）框架来评估任何策略的性能。现有的MIS方法面临两个挑战：（1）大的密度比率偏离合理范围，（2）间接监督，需要间接推断比率，从而加剧估计误差。我们的方法通过引入模拟器中目标策略的占位变量，并将密度比率学习为两个可分别学习的项的乘积来解决这些挑战。

    Reinforcement Learning (RL) methods are typically sample-inefficient, making it challenging to train and deploy RL-policies in real world robots. Even a robust policy trained in simulation requires a real-world deployment to assess their performance. This paper proposes a new approach to evaluate the real-world performance of agent policies prior to deploying them in the real world. Our approach incorporates a simulator along with real-world offline data to evaluate the performance of any policy using the framework of Marginalized Importance Sampling (MIS). Existing MIS methods face two challenges: (1) large density ratios that deviate from a reasonable range and (2) indirect supervision, where the ratio needs to be inferred indirectly, thus exacerbating estimation error. Our approach addresses these challenges by introducing the target policy's occupancy in the simulator as an intermediate variable and learning the density ratio as the product of two terms that can be learned separate
    
[^93]: 关于Adam的隐式偏差

    On the Implicit Bias of Adam. (arXiv:2309.00079v1 [cs.LG])

    [http://arxiv.org/abs/2309.00079](http://arxiv.org/abs/2309.00079)

    本文证明了RMSProp和Adam存在隐式规范化作用，其取决于超参数和训练阶段，并讨论了这些证明事实对泛化的影响。

    

    在以前的文献中，后向误差分析被用来找到近似梯度下降轨迹的常微分方程（ODEs）。发现有限步长会隐式地规范化解决方案，因为出现在ODE中的项会惩罚损失梯度的二范数。我们证明了RMSProp和Adam中是否存在类似的隐式规范化取决于它们的超参数和训练阶段，但涉及的“范数”不同：对应的ODE项要么惩罚（扰动的）损失梯度的一范数，要么相反地阻止其减小（后一种情况是典型的）。我们还进行了数值实验，并讨论了这些证明事实如何影响泛化。

    In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different "norm" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, on the contrary, hinder its decrease (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.
    
[^94]: BaDExpert: 提取后门功能以实现准确的后门输入检测

    BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection. (arXiv:2308.12439v1 [cs.CR])

    [http://arxiv.org/abs/2308.12439](http://arxiv.org/abs/2308.12439)

    BaDExpert是一种防御后门攻击的新方法，通过逆向工程提取给定后门模型的后门功能，并生成一个专家模型，该模型只能识别后门输入。可以进一步使用该模型设计高度准确的后门输入检测器。

    

    我们提出了一种新颖的防御方法，用于对抗深度神经网络（DNN）上的后门攻击，其中对手将恶意行为（后门）秘密地植入DNN中。我们的防御属于后期开发的防御范畴，独立于模型生成的方式。所提出的防御方法基于一种新颖的逆向工程方法，可以直接提取给定后门模型的后门功能并生成一个专家模型。该方法很简单 - 在一小组有意义的错误标记的干净样本上微调后门模型，从而使其忘记正常功能但仍保留后门功能，从而生成一个只能识别后门输入的模型（称为后门专家模型）。基于提取的后门专家模型，我们展示了设计高度准确的后门输入检测器的可行性，在模型推理过程中过滤掉后门输入。进一步通过...

    We present a novel defense, against backdoor attacks on Deep Neural Networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. The proposed defense is built upon a novel reverse engineering approach that can directly extract backdoor functionality of a given backdoored model to a backdoor expert model. The approach is straightforward -- finetuning the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model (dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising highly accurate backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by
    
[^95]: AIs的发展脱靴法

    Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])

    [http://arxiv.org/abs/2308.04586](http://arxiv.org/abs/2308.04586)

    传统的符号AI方法和深度学习AI方法无法满足创建强大和可信赖的AI的挑战，然而，发展脱靴法通过模仿人类儿童的能力发展过程，为创建稳健可靠的AI提供了希望。

    

    尽管当前一些AI在封闭的世界，如棋盘游戏中超越了人类能力，但它们在混乱的现实世界中的表现有限。它们会犯奇怪的错误而且没有意识到。它们很难受到指导，不能运用常识，缺乏好奇心。它们不能成为良好的合作者。传统手动构建的符号AI方法构建的系统和使用生成和深度学习AI方法(包括大规模语言模型)构建的系统都无法应对这些挑战。它们不适合创建强大和可信赖的AI。尽管此方法不属于主流的AI方法，但发展脱靴法显示出希望。在发展脱靴法中，AI像人类儿童一样发展能力。它们从先天能力开始。像人类一样，它们与环境互动，并从互动中学习。它们通过自我发展的能力逐步扩展先天能力。它们互动并逐渐将所学应用于实际操作。

    Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
    
[^96]: SelfCheck: 使用LLMs自检其逐步推理的创新

    SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])

    [http://arxiv.org/abs/2308.00436](http://arxiv.org/abs/2308.00436)

    本论文研究了使用LLMs自检逐步推理的能力，提出了一种零-shot验证方案，成功识别错误并提高了问答性能。

    

    最近大型语言模型（LLMs）的进展，尤其是链式思维（CoT）的发明，使得解决推理问题成为可能。然而，即使最强大的LLMs仍然难以处理需要非线性思维和多步推理的复杂问题。在这项工作中，我们探讨了LLMs是否具有识别自己错误的能力，而无需依赖外部资源。具体而言，我们研究了它们是否可以用于识别逐步推理中的个别错误。为此，我们提出了一种零-shot验证方案以识别此类错误。然后，我们使用此验证方案来改进问答性能，通过对不同生成的答案进行加权投票。我们在三个数学数据集-GSM8K，MathQA和MATH上测试了该方法，并发现它成功识别错误，并进而提高了最终的预测性能。

    The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
    
[^97]: 在反应式系统内对神经网络进行形式化解释

    Formally Explaining Neural Networks within Reactive Systems. (arXiv:2308.00143v1 [cs.AI])

    [http://arxiv.org/abs/2308.00143](http://arxiv.org/abs/2308.00143)

    这项研究在反应式系统中提出了一种基于DNN验证的形式化XAI技术，可以解释DNN的行为，并且通过利用系统的转换约束来计算简洁的解释。

    

    深度神经网络(DNNs)越来越多地被用作反应式系统中的控制器。然而，DNNs具有高度的不透明性，这使得解释和证明它们的行为变得困难。为了解决这个问题，出现了对可解释AI(XAI)技术的兴趣激增，这些技术能够找出导致DNN行为的输入特征。现有的XAI技术通常存在两个限制：(i)它们是启发式方法，并不能提供解释正确性的正式保证；(ii)它们通常适用于“一次性”系统(即DNN独立于过去的调用)，而不是反应式系统。在这里，我们开始弥合这个差距，提出一种基于DNN验证的形式化XAI技术，用于推理多步骤的反应式系统。我们建议通过利用系统的转换约束来计算简洁的解释的方法，以便减少底层验证器所探索的搜索空间。

    Deep neural networks (DNNs) are increasingly being used as controllers in reactive systems. However, DNNs are highly opaque, which renders it difficult to explain and justify their actions. To mitigate this issue, there has been a surge of interest in explainable AI (XAI) techniques, capable of pinpointing the input features that caused the DNN to act as it did.  Existing XAI techniques typically face two limitations: (i) they are heuristic, and do not provide formal guarantees that the explanations are correct; and (ii) they often apply to ``one-shot'' systems (where the DNN is invoked independently of past invocations), as opposed to reactive systems.  Here, we begin bridging this gap, and propose a formal DNN-verification-based XAI technique for reasoning about multi-step, reactive systems. We suggest methods for efficiently calculating succinct explanations, by exploiting the system's transition constraints in order to curtail the search space explored by the underlying verifier. W
    
[^98]: 通过使用多粒度主题分析方法的实证研究：生育政策提案

    An Empirical Study on Fertility Proposals Using Multi-Grined Topic Analysis Methods. (arXiv:2307.10025v1 [cs.HC])

    [http://arxiv.org/abs/2307.10025](http://arxiv.org/abs/2307.10025)

    本研究通过采用多粒度主题分析方法，对微博评论进行语义分析，发现关于取消婚姻登记的生育限制的提案涉及个人、社会和国家三个维度，详细讨论了个人行为、社会伦理和法律以及国家政策等社会问题。

    

    生育问题与人口安全密切相关，中国60年来首次出现人口负增长趋势，生育政策的变化引起了社会的极大关注。本文采用共现语义分析、主题分析和情感分析等方法，对微博评论进行多粒度的语义分析。发现关于“取消婚姻登记的生育限制”的提案讨论涉及个人、社会和国家三个维度，并详细探讨了个人行为、社会伦理和法律以及国家政策等社会问题。

    Fertility issues are closely related to population security, in 60 years China's population for the first time in a negative growth trend, the change of fertility policy is of great concern to the community. 2023 ``two sessions" proposal ``suggests that the country in the form of legislation, the birth of the registration of the cancellation of the marriage restriction" This topic was once a hot topic on the Internet, and ``unbundling" the relationship between birth registration and marriage has become the focus of social debate. In this paper, we adopt co-occurrence semantic analysis, topic analysis and sentiment analysis to conduct multi-granularity semantic analysis of microblog comments. It is found that the discussion on the proposal of ``removing marriage restrictions from birth registration" involves the individual, society and the state at three dimensions, and is detailed into social issues such as personal behaviour, social ethics and law, and national policy, with people's s
    
[^99]: 学习层次交互式多目标搜索以进行移动操作

    Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation. (arXiv:2307.06125v1 [cs.RO])

    [http://arxiv.org/abs/2307.06125](http://arxiv.org/abs/2307.06125)

    这项工作提出了一个层次化的强化学习方法，用于解决在未知环境中需要同时进行操控和导航的交互式多目标搜索任务。实验证明，该方法可以在新环境中进行零样本迁移，并对未见过的子任务具有鲁棒性。

    

    现有的目标搜索方法使得机器人可以在自由路径上进行搜索，然而，在非结构化的以人为中心的环境中操作的机器人经常需要操控环境以满足他们的需求。在这项工作中，我们引入了一种新的交互式多目标搜索任务，机器人需要打开门以浏览房间，并在橱柜和抽屉内搜索目标物品。这些新挑战需要在未知环境中结合操控和导航技能。我们提出了HIMOS，一种层次强化学习方法，学习组合探索、导航和操控技能。为了实现这一点，我们设计了一个围绕语义地图记忆的抽象高级动作空间，并利用探索过的环境作为实例导航点。我们在仿真和真实世界中进行了大量实验，证明HIMOS可以零样本方式有效地迁移到新的环境中，并且对于未见过的子任务具有鲁棒性。

    Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subp
    
[^100]: 深度神经网络中的定量中心极限定理

    Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])

    [http://arxiv.org/abs/2307.06092](http://arxiv.org/abs/2307.06092)

    本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。

    

    我们研究了具有随机高斯权重和偏置的全连接神经网络的分布，其中隐藏层宽度与大常数 $n$ 成比例。在非线性的温和假设下，我们得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限。我们的定理表明，无论是对于有限维分布还是整个过程，随机全连接网络（及其导数）与相应的无限宽高斯过程之间的距离都会按照 $n^{-\gamma}$ 缩放，其中 $\gamma>0$，指数取决于用于度量差异的度量方式。我们的界限在网络宽度的依赖性方面比文献中以前提供的任何界限都要强。

    We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma>0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
    
[^101]: 通过基于分数的优化提升对抗鲁棒性

    Enhancing Adversarial Robustness via Score-Based Optimization. (arXiv:2307.04333v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04333](http://arxiv.org/abs/2307.04333)

    本文介绍了一种名为ScoreOpt的新型对抗防御方案，该方案通过优化对抗样本来提高模型的鲁棒性，实验证明该方法在多个数据集上优于现有的对抗防御方法。

    

    对抗攻击有可能通过引入微小扰动来误导深度神经网络分类器。开发能够减轻这些攻击影响的算法对确保人工智能的安全使用至关重要。最近的研究表明，基于分数的扩散模型在对抗防御中是有效的。然而，现有的基于扩散的防御依赖于顺序模拟扩散模型的反向随机微分方程，这在计算效率上是低效的，并且产生次优结果。在本文中，我们介绍了一种名为ScoreOpt的新型对抗防御方案，该方案在测试时通过在由基于分数先验指导的方向上对原始干净数据进行优化来优化对抗样本。我们在多个数据集上进行了全面的实验，包括CIFAR10、CIFAR100和ImageNet。我们的实验结果表明，我们的方法在鲁棒性方面优于现有的对抗防御方法。

    Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations. Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence. Recent studies have suggested that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustnes
    
[^102]: REFLECT:对机器人经历进行总结，以用于失败解释和纠正

    REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. (arXiv:2306.15724v1 [cs.RO])

    [http://arxiv.org/abs/2306.15724](http://arxiv.org/abs/2306.15724)

    提出了REFLECT框架，可以将机器人多感官数据转化为分层总结，并使用大型语言模型进行失败解释。该框架能够生成有益的失败解释，帮助机器人完成任务。

    

    自动检测和分析失败执行是实现可解释和稳健机器人系统的关键。最近，大型语言模型（LLM）在文本输入上展示了强大的常识推理能力。为了利用LLM的力量进行机器人失败解释，我们提出了一个框架REFLECT，将多感官数据转化为机器人过去经验的分层总结，并使用逐步失败解释算法查询LLM。基于解释，失败纠正规划器生成一个可执行计划，以纠正失败并完成任务。为了系统评估该框架，我们创建了RoboFail数据集，并展示了我们基于LLM的框架能够生成有益的失败解释，从而帮助成功的纠正规划。项目网站：https://roboreflect.github.io/

    The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong common sense reasoning skills on textual inputs. To leverage the power of LLM for robot failure explanation, we propose a framework REFLECT, which converts multi-sensory data into a hierarchical summary of robot past experiences and queries LLM with a progressive failure explanation algorithm. Conditioned on the explanation, a failure correction planner generates an executable plan for the robot to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset and show that our LLM-based framework is able to generate informative failure explanations that assist successful correction planning. Project website: https://roboreflect.github.io/
    
[^103]: 模型训练中的模块化：一种新的模块化深度神经网络的范式

    Modularizing while Training: a New Paradigm for Modularizing DNN Models. (arXiv:2306.09376v1 [cs.LG])

    [http://arxiv.org/abs/2306.09376](http://arxiv.org/abs/2306.09376)

    本文提出了一种新方法，将模块化纳入模型训练过程中，即在训练时模块化(MwT)，通过两个损失函数实现模型结构上的模块化，进而实现模块的重用，能够在较短的训练时间内达到可比较的模型精度，并且相对于最先进的训练后模块化方法需要更少的参数。

    

    深度神经网络(DNN)模型已成为智能软件系统中越来越关键的组成部分。然而，训练DNN模型通常在时间和成本方面都很昂贵。为了解决这个问题，研究人员最近开始关注重用现有的DNN模型-借鉴软件工程中的代码重用思想。但是，重用整个模型可能会造成额外的开销或从不需要的功能中继承弱点。因此，现有的工作提出将已经训练好的模型分解成模块，即训练后的模块化，并实现模块的重用。但是，由于已经训练好的模型并不是为了模块化而构建的，所以训练后的模块化会导致巨大的开销和模型精度损失。本文提出了一种新方法，将模块化纳入模型训练过程中，即在训练时模块化（MwT）。我们通过两个损失函数在模型训练过程中使模型具有结构上的模块化能力，这两个损失函数同时优化模块内的内聚性和模块之间的独立性，从而得到一个真正的模块化模型。我们展示了我们的方法可以在较短的训练时间内达到可比较的模型精度，并且相对于最先进的训练后模块化方法需要更少的参数。

    Deep neural network (DNN) models have become increasingly crucial components in intelligent software systems. However, training a DNN model is typically expensive in terms of both time and money. To address this issue, researchers have recently focused on reusing existing DNN models - borrowing the idea of code reuse in software engineering. However, reusing an entire model could cause extra overhead or inherits the weakness from the undesired functionalities. Hence, existing work proposes to decompose an already trained model into modules, i.e., modularizing-after-training, and enable module reuse. Since trained models are not built for modularization, modularizing-after-training incurs huge overhead and model accuracy loss. In this paper, we propose a novel approach that incorporates modularization into the model training process, i.e., modularizing-while-training (MwT). We train a model to be structurally modular through two loss functions that optimize intra-module cohesion and int
    
[^104]: FedJETs：具有联邦混合专家的高效及时个性化方法

    FedJETs: Efficient Just-In-Time Personalization with Federated Mixture of Experts. (arXiv:2306.08586v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08586](http://arxiv.org/abs/2306.08586)

    本论文提出了一种名为FedJETs的方法，使用联邦混合专家的框架，在联邦学习中实现高效及时的个性化。该方法通过训练专门的专家，并利用门控函数将输入路由到相关的专家，有效提高了模型的准确性。

    

    联邦学习（FL）的目标之一是创建能够适应每个参与客户端上下文的个性化模型，同时利用共享全局模型的知识。然而，通常情况下，个性化需要使用客户标记的数据进行微调以实现良好的性能，这在新来的客户端是不可行的，在隐私方面也存在问题。因此，如何在这些场景中实现及时个性化仍然是个未解决的问题。我们提出了FedJETs，这是一个在FL设置中使用“专家混合（MoE）”框架的新颖解决方案。我们的方法利用客户的多样性，在不同的类别子集上训练专门的专家，并利用一个门控函数将输入路由到最相关的专家。我们的门控函数利用预训练模型的共享专家的知识，以增强其即时的路由决策。值得一提的是，我们的方法能够将准确性提高高达18％，达到现有技术水平水平。

    One of the goals in Federated Learning (FL) is to create personalized models that can adapt to the context of each participating client, while utilizing knowledge from a shared global model. Yet, often, personalization requires a fine-tuning step using clients' labeled data in order to achieve good performance. This may not be feasible in scenarios where incoming clients are fresh and/or have privacy concerns. It, then, remains open how one can achieve just-in-time personalization in these scenarios. We propose FedJETs, a novel solution by using a Mixture-of-Experts (MoE) framework within a FL setup. Our method leverages the diversity of the clients to train specialized experts on different subsets of classes, and a gating function to route the input to the most relevant expert(s). Our gating function harnesses the knowledge of a pretrained model common expert to enhance its routing decisions on-the-fly. As a highlight, our approach can improve accuracy up to 18\% in state of the art F
    
[^105]: PyTrial：药物研发人工智能的全面平台

    PyTrial: A Comprehensive Platform for Artificial Intelligence for Drug Development. (arXiv:2306.04018v1 [cs.AI])

    [http://arxiv.org/abs/2306.04018](http://arxiv.org/abs/2306.04018)

    PyTrial是一个实现多种AI算法支持的临床试验任务、可集成自己AI模型和数据集的Python软件包，并在现实临床试验数据上进行了广泛测试。

    

    药物研发是一个复杂的过程，旨在通过临床试验测试候选药物在人体内的疗效和安全性以获得监管批准。最近，机器学习作为药物研发的重要工具出现了，为提高该过程的效率和成功率提供了新机会。为了促进药物研发人工智能的研究和开发，我们开发了一个Python软件包，名为PyTrial，该软件包实现了多种被AI算法支持的临床试验任务。具体而言，PyTrial实现了6个关键的药物研发任务，包括患者结果预测、试验地点选择、试验结果预测、患者-试验匹配、试验相似性搜索和合成数据生成。在PyTrial中，所有任务都由四个步骤定义：加载数据、模型定义、模型训练和模型评估，这可以用几行代码完成。此外，模块化的API设计允许从业者集成自己的AI模型和数据集。PyTrial已在现实临床试验数据上进行了广泛测试，实验结果证明了其在各种受AI支持的临床试验任务中的有效性和效率。

    Drug development is a complex process that aims to test the efficacy and safety of candidate drugs in the human body for regulatory approval via clinical trials. Recently, machine learning has emerged as a vital tool for drug development, offering new opportunities to improve the efficiency and success rates of the process. To facilitate the research and development of artificial intelligence (AI) for drug development, we developed a Python package, namely PyTrial, that implements various clinical trial tasks supported by AI algorithms.  To be specific, PyTrial implements 6 essential drug development tasks, including patient outcome prediction, trial site selection, trial outcome prediction, patient-trial matching, trial similarity search, and synthetic data generation. In PyTrial, all tasks are defined by four steps: load data, model definition, model training, and model evaluation, which can be done with a couple of lines of code. In addition, the modular API design allows practition
    
[^106]: 通过转化特定注释者和特定实例的转移矩阵从众包中学习

    Transferring Annotator- and Instance-dependent Transition Matrix for Learning from Crowds. (arXiv:2306.03116v1 [cs.HC])

    [http://arxiv.org/abs/2306.03116](http://arxiv.org/abs/2306.03116)

    本文提出了一个高效的方法来估算特定注释者和特定实例的转移矩阵以及真实标签比例，解决了从众包中学习的标签噪声问题，并在实验中证明了方法的优越性。

    

    本文描述了从众包服务中获取训练数据的注释方法。每个注释者都完成自己的小部分注释，不同注释者的标注错误往往不同。通过标签噪声的转移矩阵来建模噪声产生过程是解决标签噪声的一种有效工具。在实际众包模型中，转移矩阵既由注释者依赖，也由实例依赖。然而，由于注释者和实例依赖的转移矩阵(AIDTM)具有高复杂度，而实际注释往往涉及注释稀疏性，这使得建立AIDTM非常具有挑战性。既要保持建模的广泛性，又能更真实地解决问题，本文提出了一种高效的算法，可以同时估算AIDTM和真实标签比例。我们还提供了理论分析，证明了我们的算法的收敛性。在合成数据集和真实数据集上的实验结果表明，我们的算法优于基准方法。

    Learning from crowds describes that the annotations of training data are obtained with crowd-sourcing services. Multiple annotators each complete their own small part of the annotations, where labeling mistakes that depend on annotators occur frequently. Modeling the label-noise generation process by the noise transition matrix is a power tool to tackle the label noise. In real-world crowd-sourcing scenarios, noise transition matrices are both annotator- and instance-dependent. However, due to the high complexity of annotator- and instance-dependent transition matrices (AIDTM), \textit{annotation sparsity}, which means each annotator only labels a little part of instances, makes modeling AIDTM very challenging. Prior works simplify the problem by assuming the transition matrix is instance-independent or using simple parametric way, while lose modeling generality. Motivated by this, we target a more realistic problem, estimating general AIDTM in practice. Without losing modeling general
    
[^107]: VisualGPTScore: 多模态生成预训练分数的视觉语义推理。

    VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores. (arXiv:2306.01879v1 [cs.CV])

    [http://arxiv.org/abs/2306.01879](http://arxiv.org/abs/2306.01879)

    我们提出了VisualGPTScore方法，能够使用多模态生成分数捕捉文本标题可能性，并在图像条件语言模型上进行计算，具备组合推理能力。

    

    本文提出了一种名为 VisualGPTScore 的方法，使用多模态生成分数来捕捉文本标题可能性，并使用图像条件语言模型在图像上运算。与传统观点认为的VLM只是无意义的单词袋模型不同，我们的 VisualGPTScore 在 ARO 和 Crepe 等最近提出的图像文本检索基准测试中展现了顶尖的性能，证明了其具备组合推理能力。

    Vision-language models (VLMs) discriminatively pre-trained with contrastive image-text matching losses such as $P(\text{match}|\text{text}, \text{image})$ have been criticized for lacking compositional understanding. This means they might output similar scores even if the original caption is rearranged into a different semantic statement. To address this, we propose to use the ${\bf V}$isual ${\bf G}$enerative ${\bf P}$re-${\bf T}$raining Score (${\bf VisualGPTScore}$) of $P(\text{text}|\text{image})$, a $\textit{multimodal generative}$ score that captures the likelihood of a text caption conditioned on an image using an image-conditioned language model. Contrary to the belief that VLMs are mere bag-of-words models, our off-the-shelf VisualGPTScore demonstrates top-tier performance on recently proposed image-text retrieval benchmarks like ARO and Crepe that assess compositional reasoning. Furthermore, we factorize VisualGPTScore into a product of the $\textit{marginal}$ P(text) and the
    
[^108]: 使用神经薛定谔桥实现非配对图像转换

    Unpaired Image-to-Image Translation via Neural Schr\"odinger Bridge. (arXiv:2305.15086v1 [cs.CV])

    [http://arxiv.org/abs/2305.15086](http://arxiv.org/abs/2305.15086)

    本文提出了一种方法——非配对神经薛定谔桥 (UNSB)，它结合了薛定谔桥、对抗训练和正则化，用于在非配对数据之间学习 SDE，并成功解决了许多非配对图像转换任务。

    

    扩散模型是一类生成模型，它通过模拟随机微分方程（SDE）从噪声生成数据。尽管扩散模型在最近取得了显著进展，但由于高斯先验假设，它们在非配对的图像转换任务中存在局限性。薛定谔桥是一种学习 SDE 以在两个任意分布之间转换的方法，被视为解决这个问题的一种有吸引力的解决方案。然而，迄今为止，薛定谔桥模型在高分辨率图像之间的非配对转换方面并不成功。在这项工作中，我们提出了非配对神经薛定谔桥（UNSB），它将薛定谔桥与对抗性训练和正则化相结合，以学习非配对数据之间的 SDE。我们证明了 UNSB 是可伸缩的，并且成功解决了各种非配对图像转换任务。

    Diffusion models are a powerful class of generative models which simulate stochastic differential equations (SDEs) to generate data from noise. Although diffusion models have achieved remarkable progress in recent years, they have limitations in the unpaired image-to-image translation tasks due to the Gaussian prior assumption. Schr\"odinger Bridge (SB), which learns an SDE to translate between two arbitrary distributions, have risen as an attractive solution to this problem. However, none of SB models so far have been successful at unpaired translation between high-resolution images. In this work, we propose the Unpaired Neural Schr\"odinger Bridge (UNSB), which combines SB with adversarial training and regularization to learn a SB between unpaired data. We demonstrate that UNSB is scalable, and that it successfully solves various unpaired image-to-image translation tasks. Code: \url{https://github.com/cyclomon/UNSB}
    
[^109]: 尺度很重要：基于小波域的属性方法解释模型对图像损坏的敏感性

    Scale Matters: Attribution Meets the Wavelet Domain to Explain Model Sensitivity to Image Corruptions. (arXiv:2305.14979v1 [cs.CV])

    [http://arxiv.org/abs/2305.14979](http://arxiv.org/abs/2305.14979)

    该论文介绍了一种基于小波域的属性方法WCAM，能够解释神经网络模型对图像损坏的敏感性，确定预测的足够信息，并阐明缩放如何增加准确性。

    

    神经网络在计算机视觉方面表现出了出色的性能，但它们在实际应用中的部署由于对图像损坏的敏感性而具有挑战性。现有的属性方法对于解释对图像损坏的敏感性是无效的，而强健性领域的文献仅提供基于模型的解释。然而，在图像损坏的情况下，审查模型的行为能力对于提高用户信任至关重要。为此，我们介绍了Wavelet sCale Attribution Method (WCAM)，它是从像素域到空间尺度域的属性方法的概括。在空间尺度域中进行属性揭示了模型的关注点和尺度。我们展示WCAM解释了模型在图像破坏下的失效，确定了预测的足够信息，并解释了如何通过缩放增加准确性。

    Neural networks have shown remarkable performance in computer vision, but their deployment in real-world scenarios is challenging due to their sensitivity to image corruptions. Existing attribution methods are uninformative for explaining the sensitivity to image corruptions, while the literature on robustness only provides model-based explanations. However, the ability to scrutinize models' behavior under image corruptions is crucial to increase the user's trust. Towards this end, we introduce the Wavelet sCale Attribution Method (WCAM), a generalization of attribution from the pixel domain to the space-scale domain. Attribution in the space-scale domain reveals where and on what scales the model focuses. We show that the WCAM explains models' failures under image corruptions, identifies sufficient information for prediction, and explains how zoom-in increases accuracy.
    
[^110]: 语言模型的物理学：第一部分，上下文无关文法。

    Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])

    [http://arxiv.org/abs/2305.13673](http://arxiv.org/abs/2305.13673)

    本研究探究了生成式语言模型如何学习上下文无关文法（CFG），并通过构造人造数据证明了预训练transformers可以学会生成具有接近完美准确度和显着多样性的句子。研究发现transformer内部的隐藏状态隐含而精确地编码了CFG结构，学会形成类似动态规划的“边界到边界”的注意力。此外，还研究了标准CFG的扩展，例如概率CFG和线性CFG，并证明transformers也可以学会这些扩展语法结构。

    

    我们设计了实验来研究生成式语言模型（例如GPT）如何学习上下文无关文法（CFG）-具有树状结构的多样化语言系统，可捕捉许多自然语言，程序和人类逻辑的方面。CFG与下推自动机一样困难，可能是模棱两可的，因此验证字符串是否满足规则需要动态规划。我们构造了人造数据，并证明即使对于非常具有挑战性的CFG，预训练transformers也可以学会生成具有接近完美准确度和显着多样性的句子。更重要的是，我们深入探讨了transformers学习CFG背后的物理原理。我们发现transformer内部的隐藏状态隐含而精确地编码了CFG结构（如在子树边界上精确定位树节点信息），并学会形成类似动态规划的“边界到边界”的注意力。我们还涵盖了一些标准CFG的扩展，例如概率CFG和线性CFG，并展示transformers也可以学会这些扩展语法结构。我们的工作揭示了语言模型的内部工作原理，并为未来的模型设计和分析提供了启示。

    We design experiments to study $\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\textit{diversity}$.  More importantly, we delve into the $\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extensio
    
[^111]: 将 Emergent In-Context Learning 解释为核回归

    Explaining Emergent In-Context Learning as Kernel Regression. (arXiv:2305.12766v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12766](http://arxiv.org/abs/2305.12766)

    本文研究了为什么在预训练之后，基于Transformer的语言模型能够实现上下文学习，并提出了一种假设，认为LLMs在面对上下文示例时能够通过内部表示模拟核回归。

    

    大型语言模型（LLMs）在迁移学习中引起了一场范式转变。与经典的预训练-微调过程相比，为了将LLMs用于下游预测任务，只需要提供一些示例，即上下文示例，而无需添加或更新现有的模型参数。LLMs的这种上下文学习能力非常有意思，但目前尚不完全了解预训练LLMs如何获得这种能力。本文通过提出一个假设，即当面临上下文示例时，LLMs能够通过内部表示模拟核回归，来研究为何基于Transformer的语言模型能够在预训练通用语料库之后实现上下文学习。具体来说，我们首先证明了上下文提示的贝叶斯推断在渐近情况下可以被理解为核回归 $\hat y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$，

    Large language models (LLMs) have initiated a paradigm shift in transfer learning. In contrast to the classic pretraining-then-finetuning procedure, in order to use LLMs for downstream prediction tasks, one only needs to provide a few demonstrations, known as in-context examples, without adding more or updating existing model parameters. This in-context learning (ICL) capability of LLMs is intriguing, and it is not yet fully understood how pretrained LLMs acquire such capabilities. In this paper, we investigate the reason why a transformer-based language model can accomplish in-context learning after pre-training on a general language corpus by proposing one hypothesis that LLMs can simulate kernel regression with internal representations when faced with in-context examples. More concretely, we first prove that Bayesian inference on in-context prompts can be asymptotically understood as kernel regression $\hat y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$ as the number of in-context demon
    
[^112]: AnyPredict: 表格预测的基础模型

    AnyPredict: Foundation Model for Tabular Prediction. (arXiv:2305.12081v1 [cs.LG])

    [http://arxiv.org/abs/2305.12081](http://arxiv.org/abs/2305.12081)

    本文提出了一种名为 AnyPredict 的表格预测基础模型，使用数据引擎整合领域内和广泛的领域外数据集，以克服模式不匹配和预测目标异质性等方面的障碍。

    

    基础模型是在大规模数据上预先训练的模型，可以在许多下游任务中表现良好。它们在自然语言处理和计算机视觉方面取得了显著的成功。然而，这种模型在表格预测任务中的使用受到限制，主要问题包括 (1) 缺乏大规模和多样化的带有标准标签的表格数据集，以及 (2) 不同领域之间的模式不匹配和预测目标的异质性。本文提出了一种方法，用于构建基于 AnyPredict 的表格预测基础模型的大规模训练数据，包括领域内和广泛的领域外数据集。该方法使用数据引擎，利用大型语言模型 (LLM) 来整合表格样本，克服了不同模式表格之间的障碍，并使用“学习，注释和审计”流程将领域外数据与目标任务对齐。扩展的训练数据使预训练的 AnyPredict 能够支持每个表格领域。

    Foundation models are pre-trained on massive data to perform well across many downstream tasks. They have demonstrated significant success in natural language processing and computer vision. Nonetheless, the use of such models in tabular prediction tasks has been limited, with the main hurdles consisting of (1) the lack of large-scale and diverse tabular datasets with standardized labels and (2) the schema mismatch and predictive target heterogeneity across domains.  This paper proposes a method for building training data at scale for tabular prediction foundation models (AnyPredict) using both in-domain and a wide range of out-domain datasets. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with varying schema and align out-domain data with the target task using a ``learn, annotate, and audit'' pipeline. The expanded training data enables the pre-trained AnyPredict to support every tabular d
    
[^113]: ChatGPT - 对于本科计算机科学学生和教师是福是祸?

    ChatGPT -- a Blessing or a Curse for Undergraduate Computer Science Students and Instructors?. (arXiv:2304.14993v1 [cs.HC])

    [http://arxiv.org/abs/2304.14993](http://arxiv.org/abs/2304.14993)

    本文分析了ChatGPT在回答本科计算机科学问题上的不可靠性，并提供了在学术界使用ChatGPT的建议。

    

    ChatGPT是由OpenAI开发的AI语言模型，可以理解和生成类人文本。它可用于语言生成、问答、文本摘要、聊天机器人开发、语言翻译、情感分析、内容创作、个性化、文本完成和故事叙述等多种用途。虽然ChatGPT受到了相当积极的关注，但在学术界也引起了一种担忧和不确定感。存在担忧学生可能会利用ChatGPT完成课外作业和考试，并获得有利的成绩，而不真正获得知识。本文采用定量方法，展示了ChatGPT在回答本科计算机科学范围内的各种问题上具有高度的不可靠性。我们的分析表明，学生盲目依赖ChatGPT完成作业和考试可能会自毁前程。我们在这个分析基础上提出了教师和学生如何在学术界使用ChatGPT的建议。

    ChatGPT is an AI language model developed by OpenAI that can understand and generate human-like text. It can be used for a variety of use cases such as language generation, question answering, text summarization, chatbot development, language translation, sentiment analysis, content creation, personalization, text completion, and storytelling. While ChatGPT has garnered significant positive attention, it has also generated a sense of apprehension and uncertainty in academic circles. There is concern that students may leverage ChatGPT to complete take-home assignments and exams and obtain favorable grades without genuinely acquiring knowledge. This paper adopts a quantitative approach to demonstrate ChatGPT's high degree of unreliability in answering a diverse range of questions pertaining to topics in undergraduate computer science. Our analysis shows that students may risk self-sabotage by blindly depending on ChatGPT to complete assignments and exams. We build upon this analysis to p
    
[^114]: MedAlpaca -- 一个开源的医疗会话式人工智能模型和训练数据集合

    MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data. (arXiv:2304.08247v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2304.08247](http://arxiv.org/abs/2304.08247)

    MedAlpaca是一个开源的医疗会话式人工智能模型和训练数据集合，旨在通过细化调整预训练语言模型来改善医疗工作流程和医生认证考试的表现。

    

    随着OpenAI的GPT系列等大型语言模型的不断发展，我们见证了人工智能在越来越广泛的领域中的应用出现。在医学领域，这些语言模型在改善医疗工作流程、诊断、患者护理和教育方面具有相当大的潜力。然而，迫切需要开源模型，以在本地部署以保护患者隐私。在我们的工作中，我们提出了一个创新的数据集，其中包含超过16万条数据，专门为了对语言模型进行细化调整以实现有效的医疗应用。我们研究了在公开可访问的预训练语言模型上对这些数据集进行细化调整的影响，并随后通过比较仅使用预训练模型与细化调整模型在未来医生必须通过的考试中的表现来展示其性能。

    As large language models (LLMs) like OpenAI's GPT series continue to make strides, we witness the emergence of artificial intelligence applications in an ever-expanding range of fields. In medicine, these LLMs hold considerable promise for improving medical workflows, diagnostics, patient care, and education. Yet, there is an urgent need for open-source models that can be deployed on-premises to safeguard patient privacy. In our work, we present an innovative dataset consisting of over 160,000 entries, specifically crafted to fine-tune LLMs for effective medical applications. We investigate the impact of fine-tuning these datasets on publicly accessible pre-trained LLMs, and subsequently, we juxtapose the performance of pre-trained-only models against the fine-tuned models concerning the examinations that future medical doctors must pass to achieve certification.
    
[^115]: 通过解释不变性和等变性评估解释方法的健壮性

    Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance. (arXiv:2304.06715v1 [cs.LG])

    [http://arxiv.org/abs/2304.06715](http://arxiv.org/abs/2304.06715)

    本文提出了解释不变性和等变性的概念，通过对对称群下具有不变性的神经网络，建立两种度量方法来提高解释方法对于不变性的健壮性并证明为一些流行的解释方法提供了理论健壮性保证。

    

    只有当解释方法忠实地描述所解释的模型时，解释方法才有价值。本文考虑了神经网络，其预测在特定对称群下具有不变性，这包括从卷积神经网络到图神经网络的流行架构。任何忠实描述这种类型模型的解释都需要与该不变性属性一致。我们通过运用几何深度学习的形式化方法，通过解释不变性和等变性的概念来形式化这种直觉。通过这种严格的形式化方法，我们得出了（1）两个度量来衡量任何解释方法相对于模型对称群的健壮性;（2）一些流行的解释方法的理论健壮性保证；（3）提高任何解释方法相对于对称群的不变性的系统方法。通过在与不同对称群相关的模型的解释中经验地测量我们的度量标准，我们展示了解释不变性和等变性对于强大的解释方法是重要的属性。

    Interpretability methods are valuable only if their explanations faithfully describe the explained model. In this work, we consider neural networks whose predictions are invariant under a specific symmetry group. This includes popular architectures, ranging from convolutional to graph neural networks. Any explanation that faithfully explains this type of model needs to be in agreement with this invariance property. We formalize this intuition through the notion of explanation invariance and equivariance by leveraging the formalism from geometric deep learning. Through this rigorous formalism, we derive (1) two metrics to measure the robustness of any interpretability method with respect to the model symmetry group; (2) theoretical robustness guarantees for some popular interpretability methods and (3) a systematic approach to increase the invariance of any interpretability method with respect to a symmetry group. By empirically measuring our metrics for explanations of models associate
    
[^116]: 自我调试：教授大型语言模型自动调试能力

    Teaching Large Language Models to Self-Debug. (arXiv:2304.05128v1 [cs.CL])

    [http://arxiv.org/abs/2304.05128](http://arxiv.org/abs/2304.05128)

    本文提出了一种自我调试方法，通过少量演示来教授大型语言模型自动调试其预测的程序，在多项代码生成基准测试中取得了最先进的性能。

    

    大型语言模型(LLM)在代码生成方面取得了卓越的性能，但对于复杂的编程任务，在一次性生成正确的解决方案方面变得具有挑战性。因此，一些先前的工作设计了程序修复方法来提高代码生成的性能。在本文中，我们提出了自我调试(Self-Debugging)方法，通过少量样本演示来教授大型语言模型调试其预测的程序。具体而言，我们证明了自我调试可以教授大型语言模型进行橡皮鸭子调试(Rubber Duck Debugging)。也就是说，在没有任何关于代码正确性或错误信息的反馈的情况下，该模型能够通过用自然语言解释生成的代码来识别它的错误。自我调试在多项代码生成基准测试中取得了最先进的性能，包括文本到SQL生成的Spider数据集，C++到Python翻译的TransCoder和文本到Python生成的MBPP。在没有单元测试的Spider基准测试中，所提出的自我调试方法明显优于现有的程序修复方法。

    Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any feedback on the code correctness or error messages, the model is able to identify its mistakes by explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit te
    
[^117]: 可微分逻辑的逻辑：走向DL的统一语义

    Logic of Differentiable Logics: Towards a Uniform Semantics of DL. (arXiv:2303.10650v2 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2303.10650](http://arxiv.org/abs/2303.10650)

    该论文介绍了一个元语言——LDL，用于定义DL，该元语言从语法和语义两方面上提高DL的形式化程度，使得对DL的性质和实现进行系统比较研究成为了可能。

    

    近期，可微分逻辑（DL）被提出作为一种训练神经网络满足逻辑规范的方法。DL包括语法和将语法中的表达式转化为损失函数的解释函数。这些损失函数可以在训练过程中与标准的梯度下降算法一起使用。 现有DL的多样性和对其形式化程度的不同处理使得对它们的性质和实现进行系统比较研究变得困难。该论文通过提出一个元语言——LDL作为DL定义的系统框架，从语法和语义两方面上提高DL的形式化程度，使得对DL的性质和实现进行系统比较研究成为了可能。

    Differentiable logics (DL) have recently been proposed as a method of training neural networks to satisfy logical specifications. A DL consists of a syntax in which specifications are stated and an interpretation function that translates expressions in the syntax into loss functions. These loss functions can then be used during training with standard gradient descent algorithms. The variety of existing DLs and the differing levels of formality with which they are treated makes a systematic comparative study of their properties and implementations difficult. This paper remedies this problem by suggesting a meta-language for defining DLs that we call the Logic of Differentiable Logics, or LDL. Syntactically, it generalises the syntax of existing DLs to FOL, and for the first time introduces the formalism for reasoning about vectors and learners. Semantically, it introduces a general interpretation function that can be instantiated to define loss functions arising from different existing 
    
[^118]: 追求机器学习研究的推理复现性

    Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04054](http://arxiv.org/abs/2302.04054)

    本研究提出利用线性混合效应模型（LMEM）来分析机器学习性能评估分数，并考虑多个方差来源及其与数据特性相互作用，从而评估可靠性和可复制性，促进对机器学习算法行为的更全面理解。

    

    机器学习评估的可靠性——即在复制的模型训练运行中观察到的评估分数的一致性——受到几种非确定性来源的影响，可以被视为测量噪声。目前的趋势是去除噪声，以强制研究结果的可复制性，忽略了实现层面固有的非确定性以及算法噪声因素和数据特性之间的关键相互作用效应。这限制了从这些实验中可以得出的结论范围。我们提出的方法是将几个方差来源，包括它们与数据特性的相互作用，纳入机器学习评估的显著性和可靠性分析中，以期从训练模型的特定实例得出推理结论, 而非去除噪声。我们展示如何使用线性混合效应模型（LMEM）来分析性能评估分数，并用广义似然比检验进行统计推断。我们的方法提供了一种系统的方式来考虑算法和数据相关的噪声来源，并使我们能够量化各个方差来源对机器学习实验的可靠性和可复制性的影响。我们在一系列合成和真实数据集上演示了我们方法的实用性，并说明了我们的方法如何促进对机器学习算法行为的更全面理解。

    Reliability of machine learning evaluation -- the consistency of observed evaluation scores across replicated model training runs -- is affected by several sources of nondeterminism which can be regarded as measurement noise. Current tendencies to remove noise in order to enforce reproducibility of research results neglect inherent nondeterminism at the implementation level and disregard crucial interaction effects between algorithmic noise factors and data properties. This limits the scope of conclusions that can be drawn from such experiments. Instead of removing noise, we propose to incorporate several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We show how to use linear mixed effects models (LMEMs) to analyze performance evaluation scores, and to conduct statistical inference with a generalized lik
    
[^119]: AI对齐对话：一种与支持代理完成AI对齐的交互方法

    AI Alignment Dialogues: An Interactive Approach to AI Alignment in Support Agents. (arXiv:2301.06421v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.06421](http://arxiv.org/abs/2301.06421)

    本文介绍了一种名为AI对齐对话的新方法，该方法通过用户和代理的交互努力实现和维持AI对齐，并相比数据驱动的方法在行为支持代理方面具有更多优势。

    

    AI对齐是确保AI系统只追求有益于人类的目标和活动。目前大部分AI对齐的方法是通过学习人类的行为数据来了解人类的价值观。本文提出了一种不同的对齐概念，即引入AI对齐对话：用户和代理通过交互努力实现和维持对齐。我们认为，与数据驱动的方法相比，对齐对话具有许多优势，特别适用于行为支持代理，这些代理旨在帮助用户实现他们期望的未来行为，而不仅仅是当前的行为。对齐对话的优势包括允许用户直接传达更高级的概念给代理，并使代理更加透明和可信。在本文中，我们阐述了对齐对话的概念和高层结构。此外，我们进行了定性的焦点小组用户研究。

    AI alignment is about ensuring AI systems only pursue goals and activities that are beneficial to humans. Most of the current approach to AI alignment is to learn what humans value from their behavioural data. This paper proposes a different way of looking at the notion of alignment, namely by introducing AI Alignment Dialogues: dialogues with which users and agents try to achieve and maintain alignment via interaction. We argue that alignment dialogues have a number of advantages in comparison to data-driven approaches, especially for behaviour support agents, which aim to support users in achieving their desired future behaviours rather than their current behaviours. The advantages of alignment dialogues include allowing the users to directly convey higher-level concepts to the agent, and making the agent more transparent and trustworthy. In this paper we outline the concept and high-level structure of alignment dialogues. Moreover, we conducted a qualitative focus group user study f
    
[^120]: 通过基于密度聚类和质心分析的方法进行反向攻击的通用检测

    Universal Detection of Backdoor Attacks via Density-based Clustering and Centroids Analysis. (arXiv:2301.04554v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.04554](http://arxiv.org/abs/2301.04554)

    该论文提出了一种通用防御方法，通过基于密度聚类和质心分析的策略，检测深度神经网络模型是否受到反向攻击的影响，该方法与现有的防御方法相比具有攻击无关性。

    

    我们提出了一种基于聚类和质心分析的通用防御方法（CCA-UD），旨在通过检查训练数据集来揭示深度神经网络模型是否受到反向攻击的影响。CCA-UD首先通过密度聚类的方法对训练集样本进行聚类，然后应用一种新颖的策略来检测是否存在被污染的聚类。所提出的策略基于一个观察到的常规误分类行为，即当被分析聚类的代表性示例的特征添加到良性样本中时产生误分类。诱导误分类错误的能力是被污染样本的普遍特征，因此所提出的防御方法是攻击无关的，与现有的防御方法有明显的不同，要么只能防御某些类型的反向攻击，要么只在某些关于污染比例或触发信号类型的条件下有效。

    We propose a Universal Defence against backdoor attacks based on Clustering and Centroids Analysis (CCA-UD). The goal of the defence is to reveal whether a Deep Neural Network model is subject to a backdoor attack by inspecting the training dataset. CCA-UD first clusters the samples of the training set by means of density-based clustering. Then, it applies a novel strategy to detect the presence of poisoned clusters. The proposed strategy is based on a general misclassification behaviour observed when the features of a representative example of the analysed cluster are added to benign samples. The capability of inducing a misclassification error is a general characteristic of poisoned samples, hence the proposed defence is attack-agnostic. This marks a significant difference with respect to existing defences, that, either can defend against only some types of backdoor attacks, or are effective only when some conditions on the poisoning ratio or the kind of triggering signal used by the
    
[^121]: Spuriosity Rankings: 使用排序数据来测量和减少偏见的方法

    Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases. (arXiv:2212.02648v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02648](http://arxiv.org/abs/2212.02648)

    这个论文提出了一种使用排序数据来测量和减少模型对虚假线索的偏见的简单有效方法。通过排名图像的虚假性，可以识别出少数子群体，并通过准确率差来评估模型的偏见。此外，通过在虚假性较低的图像上微调模型，可以在几乎不损失准确率的情况下消除模型的偏见，实现对样本的公正处理。

    

    我们提出了一种简单但有效的方法，通过排序数据来测量和减少模型对虚假线索的依赖所引起的偏见。我们的方法不需要对数据或模型训练进行昂贵的改变，而是更好地利用已有的数据。具体而言，我们基于通过可解释网络的深度神经特征来对图像进行类内排序，以衡量其虚假性（即常见虚假线索的存在程度）。通过虚假性排名，可以很容易地识别出少数子群体（即虚假性较低的图像），并通过准确率差来评估模型的偏见。甚至可以通过在虚假性较低的图像上微调分类头部，以极少的准确率损失来有效消除模型的偏见，从而实现对样本的更公正处理，无论虚假性如何。我们在ImageNet上展示了我们的方法，注释了5000个类特征依赖关系（其中630个是虚假的），并生成了一个包含325k个软分割数据的数据集。

    We present a simple but effective method to measure and mitigate model biases caused by reliance on spurious cues. Instead of requiring costly changes to one's data or model training, our method better utilizes the data one already has by sorting them. Specifically, we rank images within their classes based on spuriosity (the degree to which common spurious cues are present), proxied via deep neural features of an interpretable network. With spuriosity rankings, it is easy to identify minority subpopulations (i.e. low spuriosity images) and assess model bias as the gap in accuracy between high and low spuriosity images. One can even efficiently remove a model's bias at little cost to accuracy by finetuning its classification head on low spuriosity images, resulting in fairer treatment of samples regardless of spuriosity. We demonstrate our method on ImageNet, annotating $5000$ class-feature dependencies ($630$ of which we find to be spurious) and generating a dataset of $325k$ soft seg
    
[^122]: BiViT: 极度压缩的二进制视觉Transformer

    BiViT: Extremely Compressed Binary Vision Transformer. (arXiv:2211.07091v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.07091](http://arxiv.org/abs/2211.07091)

    BiViT是一种极度压缩的二进制视觉Transformer，通过Softmax-aware二进制化和跨层二进制化方案解决了在二进制化过程中存在的注意力误差和信息损失的问题。

    

    模型二进制化可以通过高效的位运算显著压缩模型大小，减少能耗并加速推理过程。虽然对卷积神经网络进行二进制化的研究已经很多，但对于在视觉识别领域取得突破的视觉Transformer的二进制化研究却很少。为此，我们提出了解决二进制视觉Transformer（BiViT）的两个基本挑战。首先，传统的二进制方法没有考虑到softmax注意力的长尾分布，导致注意力模块中存在较大的二进制化误差。为了解决这个问题，我们提出了适应数据分布并减少因二进制化导致的误差的Softmax-aware 二进制化方法。其次，为了更好地保留预训练模型的信息并恢复准确性，我们提出了一种跨层二进制化方案，将自注意力和交叉注意力层的二进制化解耦。

    Model binarization can significantly compress model size, reduce energy consumption, and accelerate inference through efficient bit-wise operations. Although binarizing convolutional neural networks have been extensively studied, there is little work on exploring binarization of vision Transformers which underpin most recent breakthroughs in visual recognition. To this end, we propose to solve two fundamental challenges to push the horizon of Binary Vision Transformers (BiViT). First, the traditional binary method does not take the long-tailed distribution of softmax attention into consideration, bringing large binarization errors in the attention module. To solve this, we propose Softmax-aware Binarization, which dynamically adapts to the data distribution and reduces the error caused by binarization. Second, to better preserve the information of the pretrained model and restore accuracy, we propose a Cross-layer Binarization scheme that decouples the binarization of self-attention an
    
[^123]: 自组织的空间流体自适应采样

    Space-fluid Adaptive Sampling by Self-Organisation. (arXiv:2210.17505v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2210.17505](http://arxiv.org/abs/2210.17505)

    本文提出了一种自组织的空间流体自适应采样方法来估计分布式传感数据或计算结果，其动态划分空间的方法具有优越的性能。

    

    协调系统中的一个重要任务是管理（估计、预测或控制）随空间变化的信号，例如分布式传感数据或计算结果。本文提出了一种基于竞争和生长/缩小的动态划分空间方法，协同自适应采样来估计空间现象。我们提供了一个基于场的协调框架中的自适应采样算法，并证明它是自稳定的。我们的模拟结果表明，在估计复杂函数使用高斯过程和跟踪传感器网络中的时空现象方面具有优越的性能。

    A recurrent task in coordinated systems is managing (estimating, predicting, or controlling) signals that vary in space, such as distributed sensed data or computation outcomes. Especially in large-scale settings, the problem can be addressed through decentralised and situated computing systems: nodes can locally sense, process, and act upon signals, and coordinate with neighbours to implement collective strategies. Accordingly, in this work we devise distributed coordination strategies for the estimation of a spatial phenomenon through collaborative adaptive sampling. Our design is based on the idea of dynamically partitioning space into regions that compete and grow/shrink to provide accurate aggregate sampling. Such regions hence define a sort of virtualised space that is "fluid", since its structure adapts in response to pressure forces exerted by the underlying phenomenon. We provide an adaptive sampling algorithm in the field-based coordination framework, and prove it is self-sta
    
[^124]: 自监督遮蔽卷积变压器块用于异常检测

    Self-Supervised Masked Convolutional Transformer Block for Anomaly Detection. (arXiv:2209.12148v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.12148](http://arxiv.org/abs/2209.12148)

    本论文提出了一种自监督遮蔽卷积变压器块（SSMCTB）用于异常检测。该方法在核心架构层面上集成了重构功能，并且具有灵活的信息遮蔽能力。

    

    异常检测在计算机视觉领域最近引起了越来越多的关注，可能是因为它具有广泛的应用范围，包括工业生产线上的产品故障检测、视频监控中即将发生的事件检测以及医学扫描中的病变检测等。无论是哪个领域，异常检测通常被看作是一个单类别分类任务，其中学习仅在正常示例上进行。成功的异常检测方法家族基于学习重构遮蔽的正常输入（例如补丁、未来帧等），并将重构误差的大小作为异常程度的指示器。与其他基于重构的方法不同，我们提出了一种新颖的自监督遮蔽卷积变压器块 (SSMCTB)，该块在核心架构层面上包含了基于重构的功能。所提出的自监督块非常灵活，可以在不同层面进行信息遮蔽。

    Anomaly detection has recently gained increasing attention in the field of computer vision, likely due to its broad set of applications ranging from product fault detection on industrial production lines and impending event detection in video surveillance to finding lesions in medical scans. Regardless of the domain, anomaly detection is typically framed as a one-class classification task, where the learning is conducted on normal examples only. An entire family of successful anomaly detection methods is based on learning to reconstruct masked normal inputs (e.g. patches, future frames, etc.) and exerting the magnitude of the reconstruction error as an indicator for the abnormality level. Unlike other reconstruction-based methods, we present a novel self-supervised masked convolutional transformer block (SSMCTB) that comprises the reconstruction-based functionality at a core architectural level. The proposed self-supervised block is extremely flexible, enabling information masking at a
    
[^125]: 从非侵入性脑电记录中解码语音知觉

    Decoding speech perception from non-invasive brain recordings. (arXiv:2208.12266v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2208.12266](http://arxiv.org/abs/2208.12266)

    本研究通过使用对比学习训练的模型，成功从非侵入性脑电记录中解码感知语音。结果显示，该模型能够以高达41%的准确率识别出与脑电信号相对应的语音片段。

    

    从脑电活动中解码语音一直是医疗保健和神经科学中期待已久的目标。最近的研究中，侵入性设备已经取得了重大突破：基于颅内记录的深度学习算法现在可以解码基本的语言特征（例如字母、单词、频谱图）。然而，将这种方法推广到自然语音和非侵入性脑电记录仍然是一个重大挑战。本文介绍了一个使用对比学习训练的模型，可以从大量健康个体的非侵入性记录中解码自我监督表示的感知语音。为了评估这种方法，我们整合了四个公共数据集，包括175名志愿者的脑磁图或脑电图记录，他们在听短篇故事和孤立的句子时记录。结果显示，我们的模型可以从3秒的脑磁图信号中以高达41%的准确率识别相应的语音片段，其中包含了1000个以上的候选项。

    Decoding speech from brain activity is a long-awaited goal in both healthcare and neuroscience. Invasive devices have recently led to major milestones in that regard: deep learning algorithms trained on intracranial recordings now start to decode elementary linguistic features (e.g. letters, words, spectrograms). However, extending this approach to natural speech and non-invasive brain recordings remains a major challenge. Here, we introduce a model trained with contrastive-learning to decode self-supervised representations of perceived speech from the non-invasive recordings of a large cohort of healthy individuals. To evaluate this approach, we curate and integrate four public datasets, encompassing 175 volunteers recorded with magneto- or electro-encephalography (M/EEG), while they listened to short stories and isolated sentences. The results show that our model can identify, from 3 seconds of MEG signals, the corresponding speech segment with up to 41% accuracy out of more than 1,0
    
[^126]: 空间-时间关联表示法及其在过程监测中的应用: 使用图卷积神经网络

    Spatial-temporal associations representation and application for process monitoring using graph convolution neural network. (arXiv:2205.05250v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.05250](http://arxiv.org/abs/2205.05250)

    该论文利用图卷积神经网络，基于同一工业过程中众多具有空间-时间相关特征的变量，实现了变量特征建模和表示、图网络构建及图特征感知，并应用于过程监测中。

    

    感谢同行和学者们对这项工作的关注和关心。在专家、编辑和审稿人的评论和指导下，这项工作已被接受发表在《过程安全与环境保护》期刊上。本文的主题是基于同一工业过程中众多变量的空间-时间关联，这是指在动态工业过程中获得的众多具有空间-时间相关特征的变量，即这些变量不仅在时间上高度相关，也在空间上相互关联。为了解决这个问题，需要解决三个关键问题：变量特征建模和表示、图网络构建（时间信息）和图特征感知。第一个问题通过假设数据服从改进的高斯分布来实现，而图网络可以由监测变量及其边定义。

    Thank you very much for the attention and concern of colleagues and scholars in this work. With the comments and guidance of experts, editors, and reviewers, this work has been accepted for publishing in the journal "Process Safety and Environmental Protection". The theme of this paper relies on the Spatial-temporal associations of numerous variables in the same industrial processes, which refers to numerous variables obtained in dynamic industrial processes with Spatial-temporal correlation characteristics, i.e., these variables are not only highly correlated in time but also interrelated in space. To handle this problem, three key issues need to be well addressed: variable characteristics modeling and representation, graph network construction (temporal information), and graph characteristics perception. The first issue is implemented by assuming the data follows one improved Gaussian distribution, while the graph network can be defined by the monitoring variables and their edges whi
    
[^127]: 深度生成模型和生成人工智能中的多样性

    Diversity in deep generative models and generative AI. (arXiv:2202.09573v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2202.09573](http://arxiv.org/abs/2202.09573)

    该论文介绍了一种基于核测度量化的方法，通过近似整体测度来生成多样性的对象，以解决现有生成算法中对象重复和缺乏多样性的问题。

    

    机器学习的生成算法，如生成对抗网络（GAN）和变分自编码器（VAE），在构建与训练集中相似的对象时展现出令人印象深刻的结果。然而，生成新对象主要依赖于对训练数据集的隐藏结构的理解，然后从多维正态变量中进行采样。尤其是，每个样本都是独立的，可能会重复提出相同类型的对象。为了解决这个缺点，我们介绍了一种基于核测度量化的方法，该方法可以通过将给定目标测度近似为整体来生成新对象，并且能够避免从该分布中已经绘制的元素。这确保了生成对象的多样性。该方法在经典的机器学习基准测试上进行了测试。

    The machine learning generative algorithms such as Generative Adversarial Networks (GAN) and Variational Auto-Encoders (VAE) show impressive results when constructing objects similar to those in a training ensemble. However, the generation of new objects builds mainly on the understanding of the hidden structure of the training dataset followed by a sampling from a multi-dimensional normal variable. In particular each sample is independent from the others and can repeatedly propose same type of objects. To cure this drawback we introduce a kernel-based measure quantization method that can produce new objects from a given target measure by approximating it as a whole and even staying away from elements already drawn from that distribution. This ensures a better diversity of the produced objects. The method is tested on classic machine learning benchmarks.
    
[^128]: 对具有潜在混淆因素的时间序列进行因果祖先图特征化

    Characterization of causal ancestral graphs for time series with latent confounders. (arXiv:2112.08417v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2112.08417](http://arxiv.org/abs/2112.08417)

    本文介绍了一种新的图模型类别，用于表示具有未观测到的混淆因素的时间序列的因果关系和独立性。通过使用这些新的图模型，可以得出更强的因果推断，而无需额外的假设。

    

    本文介绍了一种新的图模型类别，用于表示具有未观测到的混淆因素的多变量时间序列的时间滞后特定的因果关系和独立性。我们完全特征化了这些图，并证明它们是当前使用的模型类别的适当子集。正如我们所展示的，通过使用这些新的图可以得出更强的因果推断，而无需额外的假设。此外，我们还介绍了一种用于表示这些新图的马尔可夫等价类的图形表示。与当前最先进的因果发现算法所学到的内容相比，这种图形表示包含更多的因果知识。

    In this paper, we introduce a novel class of graphical models for representing time lag specific causal relationships and independencies of multivariate time series with unobserved confounders. We completely characterize these graphs and show that they constitute proper subsets of the currently employed model classes. As we show, from the novel graphs one can thus draw stronger causal inferences -- without additional assumptions. We further introduce a graphical representation of Markov equivalence classes of the novel graphs. This graphical representation contains more causal knowledge than what current state-of-the-art causal discovery algorithms learn.
    
[^129]: Colossal-AI: 一种用于大规模并行训练的统一深度学习系统

    Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training. (arXiv:2110.14883v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.14883](http://arxiv.org/abs/2110.14883)

    Colossal-AI是一种用于大规模并行训练的统一深度学习系统，能够以高达2.76倍的速度加快训练过程，并支持多种并行训练方法和零冗余优化器集成的异构训练方法。

    

    Transformer模型的成功推动了深度学习模型规模达到数十亿个参数。然而，由于单个GPU的有限内存资源，选择最佳并行策略的最佳实践仍然缺乏，因为它需要深度学习和并行计算方面的领域专业知识。Colossal-AI系统通过引入一个统一的接口来解决上述挑战，将模型训练的顺序代码扩展到分布式环境中。它支持数据并行、流水线并行、张量并行和序列并行等并行训练方法，以及与零冗余优化器集成的异构训练方法。与基准系统相比，Colossal-AI在大规模模型上可以达到高达2.76倍的训练加速度。

    The success of Transformer models has pushed the deep learning model scale to billions of parameters. Due to the limited memory resource of a single GPU, However, the best practice for choosing the optimal parallel strategy is still lacking, since it requires domain expertise in both deep learning and parallel computing.  The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism, as well as heterogeneous training methods integrated with zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.
    
[^130]: SR-HetGNN:基于异构图神经网络的会话推荐系统

    SR-HetGNN:Session-based Recommendation with Heterogeneous Graph Neural Network. (arXiv:2108.05641v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2108.05641](http://arxiv.org/abs/2108.05641)

    本文提出了一种基于异构图神经网络的会话推荐方法SR-HetGNN，通过学习会话嵌入并捕捉匿名用户的特定偏好，以改进会话推荐系统的效果和准确性。

    

    会话推荐系统的目的是根据先前的会话序列预测用户的下一次点击。目前的研究通常根据用户会话序列中的项目转换来学习用户偏好。然而，会话序列中的其他有效信息，如用户配置文件，往往被忽视，这可能导致模型无法学习用户的具体偏好。在本文中，我们提出了一种基于异构图神经网络的会话推荐方法，命名为SR-HetGNN，它可以通过异构图神经网络（HetGNN）学习会话嵌入，并捕捉匿名用户的特定偏好。具体而言，SR-HetGNN首先根据会话序列构建包含各种类型节点的异构图，可以捕捉项目、用户和会话之间的依赖关系。其次，HetGNN捕捉项目之间的复杂转换并学习包含项目嵌入的特征。

    The purpose of the Session-Based Recommendation System is to predict the user's next click according to the previous session sequence. The current studies generally learn user preferences according to the transitions of items in the user's session sequence. However, other effective information in the session sequence, such as user profiles, are largely ignored which may lead to the model unable to learn the user's specific preferences. In this paper, we propose a heterogeneous graph neural network-based session recommendation method, named SR-HetGNN, which can learn session embeddings by heterogeneous graph neural network (HetGNN), and capture the specific preferences of anonymous users. Specifically, SR-HetGNN first constructs heterogeneous graphs containing various types of nodes according to the session sequence, which can capture the dependencies among items, users, and sessions. Second, HetGNN captures the complex transitions between items and learns the item embeddings containing
    
[^131]: 具有矩阵表示的循环神经网络的存储容量

    Memory Capacity of Recurrent Neural Networks with Matrix Representation. (arXiv:2104.07454v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.07454](http://arxiv.org/abs/2104.07454)

    这个论文研究了具有矩阵表示的循环神经网络的存储容量，通过基于Fisher信息的概率性概念定义和研究，得出了不同假设下的存储上界，并与向量表示的网络进行了比较。

    

    众所周知，经典的循环神经网络（RNN）在学习长期依赖性方面存在限制，这一问题在长短期记忆网络（LSTM）中通过存储结构得到了解决。神经图灵机（NTM）是一种新颖的RNN，通过神经网络控制器实现了可编程计算机的概念，可以学习简单的算法任务。矩阵神经网络采用矩阵表示，与使用基于向量表示的经典神经网络相比，可以固有地保留数据的空间结构。因此，可以认为具有矩阵表示的神经网络可能具有更好的存储容量。在本文中，我们基于Fisher信息定义和研究了一种基于概率的矩阵RNN存储容量的概念。在各种假设下，我们找到了这些网络的存储容量的上界，并与它们的向量对应物进行了比较。特别地，我们证明了s

    It is well known that canonical recurrent neural networks (RNNs) face limitations in learning long-term dependencies which have been addressed by memory structures in long short-term memory (LSTM) networks. Neural Turing machines (NTMs) are novel RNNs that implement the notion of programmable computers with neural network controllers that can learn simple algorithmic tasks. Matrix neural networks feature matrix representation which inherently preserves the spatial structure of data when compared to canonical neural networks that use vector-based representation. One may then argue that neural networks with matrix representations may have the potential to provide better memory capacity. In this paper, we define and study a probabilistic notion of memory capacity based on Fisher information for matrix-based RNNs. We find bounds on memory capacity for such networks under various hypotheses and compare them with their vector counterparts. In particular, we show that the memory capacity of s
    
[^132]: 使用带有注意力和上下文匹配机制的卷积LSTM进行数值天气预报

    Numerical Weather Forecasting using Convolutional-LSTM with Attention and Context Matcher Mechanisms. (arXiv:2102.00696v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.00696](http://arxiv.org/abs/2102.00696)

    本论文提出了一种新的深度学习架构，用于预测高分辨率时空天气数据。该架构集成了卷积长短期记忆和卷积神经网络，并引入了注意力和上下文匹配机制。与传统模型相比，该架构在性能上取得了显著改进。

    

    使用高分辨率物理模型进行数值天气预报通常需要超级计算机上的大量计算资源，这减少了它们在大多数实际应用中的广泛使用。为了解决这个问题，应用深度学习方法在这个领域内揭示出创新性的解决方案。为此，我们引入了一种用于预测高分辨率时空天气数据的新型深度学习架构。我们的方法通过集成卷积长短期记忆和卷积神经网络来扩展传统的编码器-解码器结构。此外，我们还将注意力和上下文匹配机制融入到模型架构中。与基准深度学习模型，包括ConvLSTM、TrajGRU和U-Net相比，我们的天气模型在性能上取得了显著的改进。我们的实验评估涉及高规模的实际基准数值天气数据集，即ERA5小时级气压水平数据集和WeatherBench。我们的结果证明了...

    Numerical weather forecasting using high-resolution physical models often requires extensive computational resources on supercomputers, which diminishes their wide usage in most real-life applications. As a remedy, applying deep learning methods has revealed innovative solutions within this field. To this end, we introduce a novel deep learning architecture for forecasting high-resolution spatio-temporal weather data. Our approach extends the conventional encoder-decoder structure by integrating Convolutional Long-short Term Memory and Convolutional Neural Networks. In addition, we incorporate attention and context matcher mechanisms into the model architecture. Our Weather Model achieves significant performance improvements compared to baseline deep learning models, including ConvLSTM, TrajGRU, and U-Net. Our experimental evaluation involves high-scale, real-world benchmark numerical weather datasets, namely the ERA5 hourly dataset on pressure levels and WeatherBench. Our results demo
    

