# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [AnyLoc: Towards Universal Visual Place Recognition.](http://arxiv.org/abs/2308.00688) | AnyLoc实现了一个通用的视觉地点识别(VPR)解决方案，能够在各种结构化和非结构化环境中运行，并通过组合通用特征表示和无监督特征聚合实现了显著的性能提升。 |
| [^2] | [Applicability of scaling laws to vision encoding models.](http://arxiv.org/abs/2308.00678) | 本研究探究了视觉编码模型的可扩展性规律适用性，发现增加训练样本数量和视觉模型参数大小可以提高预测准确性。 |
| [^3] | [Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models.](http://arxiv.org/abs/2308.00675) | 这项工作提出了使用工具文档作为教导大型语言模型使用新工具的替代方法，并通过实证研究证明，仅使用工具文档的零-shot提示足以实现正确的工具使用。 |
| [^4] | [Hessian-Aware Bayesian Optimization for Decision Making Systems.](http://arxiv.org/abs/2308.00629) | 本文介绍了一种感知海森贝叶斯优化算法，旨在解决决策系统优化中梯度反馈稀缺或无效的问题。通过引入紧凑的多层架构和角色概念，并利用感知海森贝叶斯优化方法对参数进行优化，作者实现了对复杂决策系统的高效优化。 |
| [^5] | [Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes.](http://arxiv.org/abs/2308.00628) | 这篇论文提出了一个室外多模态多视角多人类姿势数据库Human-M3，并介绍了一种基于多模态数据输入的算法来生成准确的人体姿势。这个数据库解决了现有数据集的不足，提供了更多的数据多样性。 |
| [^6] | [JIANG: Chinese Open Foundation Language Model.](http://arxiv.org/abs/2308.00624) | JIANG是一个专为中文设计的开放式语言模型，通过使用大量的中文语料库进行训练和优化结构，能够更好地在中文中发挥其能力。 |
| [^7] | [Beyond One-Hot-Encoding: Injecting Semantics to Drive Image Classifiers.](http://arxiv.org/abs/2308.00607) | 本文探索了将本体和语义知识反映到图像分类器中的方法，提高了模型的可解释性和可信度。 |
| [^8] | [Predicting masked tokens in stochastic locations improves masked image modeling.](http://arxiv.org/abs/2308.00566) | 本论文提出了一种名为FlexPredict的随机模型，通过在模型中加入位置不确定性，以预测掩盖的标记位置，从而改善了掩盖图像建模的性能。 |
| [^9] | [Reinforcement Learning-based Non-Autoregressive Solver for Traveling Salesman Problems.](http://arxiv.org/abs/2308.00560) | 基于强化学习的非自回归TSP求解器NAR4TSP使用特别设计的图神经网络进行推理，消除了昂贵标签的依赖，并在解决方案质量、推理延迟和泛化能力方面优于其他四个最先进的模型。 |
| [^10] | [Copula for Instance-wise Feature Selection and Ranking.](http://arxiv.org/abs/2308.00549) | 在实例级特征选择和排序中，我们提出了一种使用Copula的方法，能够更好地考虑特征之间的相关性，实验结果证明我们的方法能够捕捉有意义的相关性。 |
| [^11] | [Predicting Early Dropouts of an Active and Healthy Ageing App.](http://arxiv.org/abs/2308.00539) | 本文提出了一种机器学习方法，用于预测积极健康老龄化应用程序的早期退学。通过使用动态和静态特征构建分类模型以及采用过采样方法提高分类性能，我们获得了高质量的粘附度预测，并在科学挑战赛中获得了第一名。 |
| [^12] | [PressureTransferNet: Human Attribute Guided Dynamic Ground Pressure Profile Transfer using 3D simulated Pressure Maps.](http://arxiv.org/abs/2308.00538) | 我们提出了一种新的人体活动识别方法，使用PressureTransferNet模型可以准确地将人类属性转换为地面压力分布，并在不同场景中验证了其有效性。 |
| [^13] | [Graph Embedding Dynamic Feature-based Supervised Contrastive Learning of Transient Stability for Changing Power Grid Topologies.](http://arxiv.org/abs/2308.00537) | 提出了一种基于图嵌入动态特征的瞬态稳定性GEDF-SCL模型，利用监督对比学习结合GEDF来预测电力系统的瞬态稳定性，并考虑了拓扑信息。实验结果表明该模型具有良好的性能。 |
| [^14] | [Point Annotation Probability Map: Towards Dense Object Counting by Tolerating Annotation Noise.](http://arxiv.org/abs/2308.00530) | 该论文提出通过容忍注释噪声实现了密集对象计数的点注释概率图（PAPM）方法。通过利用广义高斯分布来形成学习目标PAPM，在拥挤场景中具有强鲁棒性。论文提出了两种方法，一种是基于人工设计的PAPM方法（HD-PAPM），另一种是自适应学习的PAPM方法（AL-PAPM）。 |
| [^15] | [Transfer-Ensemble Learning based Deep Convolutional Neural Networks for Diabetic Retinopathy Classification.](http://arxiv.org/abs/2308.00525) | 本文提出了一种基于转移集成学习的深度卷积神经网络模型，用于糖尿病视网膜病变的分类。该模型通过利用两个预训练网络的优势，将图像特征转换为固定长度的向量，并通过集成学习的方法，提高了糖尿病视网膜病变的分类性能。 |
| [^16] | [SurveyLM: A platform to explore emerging value perspectives in augmented language models' behaviors.](http://arxiv.org/abs/2308.00521) | SurveyLM是一个用于分析增强语言模型行为中新兴价值观的平台，通过调查和实验方法系统评估了ALMs的对齐和新兴行为，并利用ALMs的反馈来增强调查和实验设计。 |
| [^17] | [Framework for developing quantitative agent based models based on qualitative expert knowledge: an organised crime use-case.](http://arxiv.org/abs/2308.00505) | 提出了一个基于定性专家知识的量化代理模型开发框架，该框架通过将定性数据翻译成定量规则，为模型构建者和领域专家提供了一个系统和透明的建模过程。以一个有组织犯罪的应用案例为例，演示了该框架的方法。 |
| [^18] | [Explainable Graph Spectral Clustering of Text Documents.](http://arxiv.org/abs/2308.00504) | 本文提出了一种可解释的文本文档的图谱聚类方法，通过展示组合拉普拉斯嵌入、K嵌入和词向量空间嵌入之间的等价性，构建了文本内容和聚类结果之间的桥梁。 |
| [^19] | [Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education.](http://arxiv.org/abs/2308.00479) | 本文介绍了在医学教育领域中，使用检索增强生成（RAG）将非参数化知识库与大规模语言模型（LLMs）结合，以解决幻觉和有害答案问题，并提出了一种使用代表性向量的抽取性和抽象性摘要方法。 |
| [^20] | [A Satellite Imagery Dataset for Long-Term Sustainable Development in United States Cities.](http://arxiv.org/abs/2308.00465) | 该论文开发了一个卫星图像数据集，覆盖了美国最多人口的100个城市以及相应的人口普查区块群，时间跨度为2014年至2023年。这个数据集可用于研究美国城市的可持续发展目标，特别是与贫困、健康相关的SDG指标。 |
| [^21] | [DMFC-GraspNet: Differentiable Multi-Fingered Robotic Grasp Generation in Cluttered Scenes.](http://arxiv.org/abs/2308.00456) | 本文提出了DMFC-GraspNet，在多指机器人抓取生成领域做出了两个主要贡献：一是提出了可微的多指抓取规划方法，实现了多样化和稠密的抓取预测；二是开发了一种稠密标注方法，使得多指机器人手与真实抓取密切关联。结果表明了该方法的有效性。 |
| [^22] | [Structural Embeddings of Tools for Large Language Models.](http://arxiv.org/abs/2308.00447) | 这篇论文突出了大型语言模型（LLM）与外部工具之间基于图的交互方法的重要性，并提出了一个指导与LLM集成大量外部工具的框架。 |
| [^23] | [SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning.](http://arxiv.org/abs/2308.00436) | 本论文研究了使用LLMs自检逐步推理的能力，提出了一种零-shot验证方案，成功识别错误并提高了问答性能。 |
| [^24] | [Patch-wise Auto-Encoder for Visual Anomaly Detection.](http://arxiv.org/abs/2308.00429) | 本论文提出了一种新颖的补丁化自编码器（Patch AE）框架来增强自编码器对异常的重构能力，并在Mvtec AD基准测试中取得了最先进的表现，具有在实际工业应用场景中的潜力。 |
| [^25] | [Generative adversarial networks with physical sound field priors.](http://arxiv.org/abs/2308.00426) | 本文提出了一种基于生成对抗网络（GANs）的深度学习方法，利用物理声场先验知识准确重建声场，实现了较好的精度和能量保留，特别适用于高频范围和超出测量区域的外推情况。与最先进的方法相比，该方法能够处理不同数量和配置的测量位置，为声场重建提供了有前景的方法。 |
| [^26] | [Discourse-Aware Text Simplification: From Complex Sentences to Linked Propositions.](http://arxiv.org/abs/2308.00425) | 本研究提出了一种话语感知的文本简化方法，通过在句子的语义上下文中拆分和重新表达复杂的英语句子，解决了现有句法文本简化方法的保守性和忽略上下文连贯性的问题。 |
| [^27] | [Challenging the Myth of Graph Collaborative Filtering: a Reasoned and Reproducibility-driven Analysis.](http://arxiv.org/abs/2308.00404) | 本文研究挑战图协同过滤的神话，通过关注结果的可复制性，成功复制了六个流行的图推荐模型在几个常见和新数据集上的结果，并与传统协同过滤模型进行比较。 |
| [^28] | [Counterfactual Graph Transformer for Traffic Flow Prediction.](http://arxiv.org/abs/2308.00391) | 本研究提出了一种针对交通流量预测的反事实图变压器模型（CGT），通过实例级解释器和扰动掩码生成器，可以获得具有解释性的预测模型和重要子图。 |
| [^29] | [Artificial-Intelligence-Based Triple Phase Shift Modulation for Dual Active Bridge Converter with Minimized Current Stress.](http://arxiv.org/abs/2308.00382) | 本论文提出了一种基于人工智能的三相移位调制技术，用于双有源桥变换器，以最小化电流应力。这种调制技术可以提高DAB变换器的功率效率和功率密度。 |
| [^30] | [Artificial-Intelligence-Based Hybrid Extended Phase Shift Modulation for the Dual Active Bridge Converter with Full ZVS Range and Optimal Efficiency.](http://arxiv.org/abs/2308.00381) | 本文提出了一种基于人工智能的混合扩展相移调制方法，以实现双有源桥变换器的全零电压开关范围和最优效率。 |
| [^31] | [Shape Completion with Prediction of Uncertain Regions.](http://arxiv.org/abs/2308.00377) | 该论文提出了两种方法来处理在给定模糊物体视图时可能存在的物体部分的不确定区域预测问题。研究表明这些方法可以作为任何预测空间占用的方法的直接扩展，通过后处理占用评分或直接预测不确定性指标来实现。这些方法与已知的概率形状完成方法进行了比较，并使用自动生成的深度图像数据集进行了验证。 |
| [^32] | [Fountain -- an intelligent contextual assistant combining knowledge representation and language models for manufacturing risk identification.](http://arxiv.org/abs/2308.00364) | Fountain是一个智能上下文助手，将知识表示和语言模型结合，用于制造风险识别。它通过描述现有设计和流程准则以及提出的偏差来帮助识别风险，并提供可解释和一致的建议。 |
| [^33] | [MetaGPT: Meta Programming for Multi-Agent Collaborative Framework.](http://arxiv.org/abs/2308.00352) | MetaGPT是一个用于多智能体协作的创新框架，将有效的人工工作流引入到大型语言模型驱动的协作中。它采用元编程方法，将标准操作规程编码为提示，促进结构化协调，并要求模块化输出，赋予智能体领域专业知识，以验证输出并减少错误。这种框架利用了流水线工作模式来分配任务。 |
| [^34] | [Learning Green's Function Efficiently Using Low-Rank Approximations.](http://arxiv.org/abs/2308.00350) | 该论文提出了一种使用低秩分解学习Green函数的方法，通过使用领域数据和蒙特卡洛样本进行分别学习和积分逼近，从而提高了计算效率并保持了准确性。 |
| [^35] | [Dynamic ensemble selection based on Deep Neural Network Uncertainty Estimation for Adversarial Robustness.](http://arxiv.org/abs/2308.00346) | 本文针对深度神经网络在实际应用中的鲁棒性问题，提出了一种通过动态集成选择技术在模型层面上改善模型的防御主动性的方法。通过在训练阶段引入Dirichlet分布作为子模型预测分布的先验，并在轻量级子模型下引入参数空间的多样性约束，构建备选的集成模型空间。在测试阶段，动态选取特定的子模型来提高模型的鲁棒性。 |
| [^36] | [Kidnapping Deep Learning-based Multirotors using Optimized Flying Adversarial Patches.](http://arxiv.org/abs/2308.00344) | 本研究介绍了利用优化的飞行对抗贴片来绑架基于深度学习的多旋翼的方法，并展示了这些方法在对抗贴片数量增加时的良好扩展性。 |
| [^37] | [Monitoring Algorithmic Fairness under Partial Observations.](http://arxiv.org/abs/2308.00341) | 本研究将算法公平性监控扩展到部分观察到的马尔可夫链模型的系统，并且可以监控包含对事件序列上数值函数的期望值的算术表达式的公平性属性。 |
| [^38] | [Target Search and Navigation in Heterogeneous Robot Systems with Deep Reinforcement Learning.](http://arxiv.org/abs/2308.00331) | 本文设计了一个异构机器人系统，通过深度强化学习算法学习出的策略能够在迷宫般的矿井环境中搜索并导航到未知目标位置。引入多阶段强化学习框架和好奇模块加速了训练速度。 |
| [^39] | [Threshold-aware Learning to Generate Feasible Solutions for Mixed Integer Programs.](http://arxiv.org/abs/2308.00327) | 针对混合整数规划问题，本研究提出了一种阈值感知学习的方法，通过优化变量分配率来提高可行解的质量。 |
| [^40] | [Pixel to policy: DQN Encoders for within & cross-game reinforcement learning.](http://arxiv.org/abs/2308.00318) | 本文研究了在强化学习中利用共享结构的DQN编码器的性能。通过迁移学习，该模型在不同任务和环境中学习可转移的策略，实现了更高效的学习和更好的性能。在多个游戏环境中，该模型的平均回合奖励为46.16，在20,000次回合内甚至超过了人类水平表现。 |
| [^41] | [Making the V in Text-VQA Matter.](http://arxiv.org/abs/2308.00295) | 本论文针对文本VQA中对视觉特征理解不足的问题，提出通过学习视觉特征来解决这一问题，通过将TextVQA和VQA数据集相结合进行模型训练，从而提高模型的准确性。 |
| [^42] | [Multi-Modality Multi-Loss Fusion Network.](http://arxiv.org/abs/2308.00264) | 多模态多损失融合网络通过最佳选择和融合多个模态的特征，提高了情感检测的性能，并在多个数据集上实现了最先进的结果。这些研究结果表明了用于增强神经网络中情感检测的特征选择和融合方法的优化方向。 |
| [^43] | [LGViT: Dynamic Early Exiting for Accelerating Vision Transformer.](http://arxiv.org/abs/2308.00255) | 提出了一种用于加速视觉Transformer的早期退出框架LGViT，通过融合局部感知头部和全局聚合头部，以解决ViTs中早期退出方法应用带来性能严重下降的问题。 |
| [^44] | [EEG-based Cognitive Load Classification using Feature Masked Autoencoding and Emotion Transfer Learning.](http://arxiv.org/abs/2308.00246) | 本文提出了一种基于EEG的认知负荷分类的新解决方案，利用特征掩蔽自编码和情绪迁移学习来进行模型训练和分类。实验结果表明，这种方法取得了良好的结果。 |
| [^45] | [The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models.](http://arxiv.org/abs/2308.00245) | 本文详细研究了以使用前初始化错误为案例的大型语言模型辅助静态分析的开放空间，并开发了一种全自动代理程序LLift，该程序能够克服多个挑战，包括错误建模。 |
| [^46] | [Capsa: A Unified Framework for Quantifying Risk in Deep Neural Networks.](http://arxiv.org/abs/2308.00231) | Capsa是一个统一框架，用于扩展具有风险感知的深度神经网络模型。它能够量化多种形式的风险，并将不同算法组合在一起以并行量化不同的风险指标。通过在复杂感知数据集上实现最先进的不确定性估计算法并进行基准测试，验证了capsa的有效性。capsa能够轻松组合aleatoric不确定性、epistemic不确定性和偏见估计能力 |
| [^47] | [Experiments on Generative AI-Powered Parametric Modeling and BIM for Architectural Design.](http://arxiv.org/abs/2308.00227) | 本文介绍了一种利用生成式AI和参数化建模与BIM相结合的新型建筑设计框架，能够促进建筑师与AI的合作，并通过探索设计思路和生成创造性设计来提升设计过程的效率和协作性。 |
| [^48] | [Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias.](http://arxiv.org/abs/2308.00225) | 这项研究发现，经过指导调优的语言模型呈现出新兴的认知偏见，这对于理解和开发更可靠和无偏的语言模型至关重要。 |
| [^49] | [Advancing Beyond Identification: Multi-bit Watermark for Language Models.](http://arxiv.org/abs/2308.00221) | 本研究提出了一种用于语言模型的多位水印技术——COLOR，可在语言模型生成过程中嵌入可追踪的多位信息，实现了提取水印、即时嵌入和维持文本质量等功能，同时允许零位检测。初步实验显示成功在中等长度的文本中嵌入了32位消息，准确率为91.9％。这项研究有效推进了对语言模型滥用的反制策略。 |
| [^50] | [Deep Reinforcement Learning-Based Battery Conditioning Hierarchical V2G Coordination for Multi-Stakeholder Benefits.](http://arxiv.org/abs/2308.00218) | 本研究提出了一种基于深度强化学习的电池调控分层V2G协调策略，旨在促进可再生能源利用和电网稳定。该策略可以实现多方利益，并且考虑了电网、电动车聚合器和用户的各种因素。 |
| [^51] | [Performance Evaluation of Swin Vision Transformer Model using Gradient Accumulation Optimization Technique.](http://arxiv.org/abs/2308.00197) | 本文评估了使用梯度累积优化技术的Swin视觉Transformer模型的性能，结果发现应用该技术会导致模型准确性下降并增加训练时间。 |
| [^52] | [Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?.](http://arxiv.org/abs/2308.00189) | 生成模型作为复杂系统科学，它们能够完成任务的行为表现需要被解释和理解，以实现对其行为的控制和未来研究的指导。 |
| [^53] | [Attribution-Scores in Data Management and Explainable Machine Learning.](http://arxiv.org/abs/2308.00184) | 数据管理和可解释机器学习中的研究描述了使用实际因果关系来解释数据库查询答案和机器学习模型结果的责任评分的最新工作，以及与数据库修复和Shap-score计算相关的内容。 |
| [^54] | [Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity.](http://arxiv.org/abs/2308.00177) | 本研究研究了在标签稀缺的Learning-To-Rank问题中，无监督预训练的深度模型是否能胜过GBDTs和其他非预训练模型。实验结果表明，通过使用SimCLR-Rank方法进行无监督预训练，我们的深度学习模型在大量无标签数据和有限标签数据的情况下取得了显著优势。 |
| [^55] | [Adversarially Robust Neural Legal Judgement Systems.](http://arxiv.org/abs/2308.00165) | 本论文提出了一种对抗性鲁棒的神经法律判断系统，通过对早期存在的系统进行实验发现它们无法处理对抗性攻击。经过大量实验，我们的方法在处理对抗性攻击方面明显优于现有最先进的法律判断预测系统。 |
| [^56] | [Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?.](http://arxiv.org/abs/2308.00158) | 本研究探讨了使用Fine-Tuned的OpenAI LLM进行翻译质量估计的能力，实验证明可以通过Fine-Tuned的ChatGPT来预测机器翻译的质量，但仍有改进的空间。 |
| [^57] | [Formally Explaining Neural Networks within Reactive Systems.](http://arxiv.org/abs/2308.00143) | 这项研究在反应式系统中提出了一种基于DNN验证的形式化XAI技术，可以解释DNN的行为，并且通过利用系统的转换约束来计算简洁的解释。 |
| [^58] | [A Suite of Fairness Datasets for Tabular Classification.](http://arxiv.org/abs/2308.00133) | 这篇论文介绍了一套新的公平数据集，旨在解决表格分类中实验评估数据不足的问题，为未来公平感知机器学习研究提供更严谨的实验评估。 |
| [^59] | [Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods.](http://arxiv.org/abs/2308.00129) | 本文研究了语音数据的表示学习，主要关注多种设置和多种方法，旨在通过利用大量无标签和弱标签数据以及附加数据模态，改进下游序列预测任务。 |
| [^60] | [Getting pwn'd by AI: Penetration Testing with Large Language Models.](http://arxiv.org/abs/2308.00121) | 本文探讨了使用大型语言模型（如GPT3.5）作为AI助手来增强渗透测试人员的能力，实现了高级任务规划和低级漏洞寻找两种用例，取得了有前景的初步结果，并就提供该技术的伦理问题进行了讨论。 |
| [^61] | [A Modular Ontology for MODS -- Metadata Object Description Schema.](http://arxiv.org/abs/2308.00116) | 本研究开发了模块化MODS本体论（MMODS-O），用于解决元数据对象描述模式（MODS）在知识图谱环境下的限制问题，并采用模块化本体论设计方法学（MOMo）实现了平衡。 |
| [^62] | [Three Bricks to Consolidate Watermarks for Large Language Models.](http://arxiv.org/abs/2308.00113) | 本研究提出了三种基于理论和实证考虑的方法，巩固了用于大型语言模型的水印技术。新的统计检验方法能够在低错误阳性率下提供稳定的理论保证。与自然语言处理领域的经典基准测试相比，水印技术的有效性得到了验证，并且我们还开发了先进的检测方案，适用于具有大型语言模型访问权限和多位水印技术的场景。 |
| [^63] | [DPBERT: Efficient Inference for BERT based on Dynamic Planning.](http://arxiv.org/abs/2308.00108) | DPBERT是一个基于动态规划的高效BERT推理方法，通过选择一部分transformer层来加速推理过程，在保持高准确性的同时降低延迟。 |
| [^64] | [Validation of a Zero-Shot Learning Natural Language Processing Tool for Data Abstraction from Unstructured Healthcare Data.](http://arxiv.org/abs/2308.00107) | 验证了一种零样本学习自然语言处理工具的数据抽象能力，结果显示该工具在提取信息的速度上优于医师人工抽象者，并在准确性上非劣。 |
| [^65] | [Can A Single Human Supervise A Swarm of 100 Heterogeneous Robots?.](http://arxiv.org/abs/2308.00102) | 这项研究研究了一个人是否能够监督一个真实环境中的异构机器人群体，结果表明，在适当的工作负荷算法支持下，一个人可以有效地指挥和控制这样的群体。 |
| [^66] | [Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation.](http://arxiv.org/abs/2308.00085) | 本文提出了一种基于常识的因果解释方法，用于多样化的共情回应生成。该方法综合考虑了用户的角度和系统的角度，并通过集成常识知识提升了ChatGPT在系统的推理能力。实验结果表明，该方法在多项评估指标上超过了其他方法。 |
| [^67] | [Towards Semantically Enriched Embeddings for Knowledge Graph Completion.](http://arxiv.org/abs/2308.00081) | 本论文讨论了知识图谱补全算法以及利用嵌入模型捕捉知识图谱中语义的不同方法，并提出知识图谱和语言模型相互受益的观点。 |
| [^68] | [A Novel Deep Learning based Model to Defend Network Intrusion Detection System against Adversarial Attacks.](http://arxiv.org/abs/2308.00077) | 本论文研究了一种基于深度学习的网络入侵检测系统（NIDS）对抗对抗攻击的新型模型。通过实施强大的对抗攻击方法，如FGSM、JSMA、PGD和C&W，并采用对抗训练作为防御方法来增强NIDS模型的稳健性。 |
| [^69] | [Crowd Safety Manager: Towards Data-Driven Active Decision Support for Planning and Control of Crowd Events.](http://arxiv.org/abs/2308.00076) | 这篇论文介绍了一种新颖的技术和方法，旨在通过数据驱动的决策支持系统提升人群管理的规划和操作阶段。该方法利用创新的数据收集技术、数据集成和3D数字孪生技术，结合人工智能工具进行风险识别，并引入了蝴蝶结模型来评估和预测风险水平。 |
| [^70] | [How User Language Affects Conflict Fatality Estimates in ChatGPT.](http://arxiv.org/abs/2308.00072) | 在以色列-巴勒斯坦和土耳其-库尔德冲突的背景下，本研究探讨了用户语言对ChatGPT中冲突死亡估计的影响。研究发现，在使用攻击者的语言进行查询时，GPT-3.5提供的估计较使用被攻击群体的语言查询时低27±11％。此外，否认存在此类袭击的回答进一步增加了这种差异，形成了一种新的偏见机制，可能加大现有的媒体偏见并加剧信息孤立。 |
| [^71] | [Interpretable Stereotype Identification through Reasoning.](http://arxiv.org/abs/2308.00071) | 本研究通过使用推理方法，在零射击刻板印象识别中取得了重要的进展，并发现推理的性能增益远远超过模型规模扩展的增益。推理不仅提高了准确性，还提高了决策的可解释性。 |
| [^72] | [Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges.](http://arxiv.org/abs/2308.00031) | 这篇论文调查了在生成人工智能中应用强化学习的现状、机会和开放研究问题。作者主要讨论了三种应用类型：无特定目标的生成方式、同时最大化目标函数的输出生成方式以及将无法通过目标函数捕捉的期望特征嵌入生成过程的方式。这个新兴领域中存在着丰富的机会和挑战。 |
| [^73] | [Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment.](http://arxiv.org/abs/2308.00016) | 本论文提出了一种通过引入人机交互的新型 alpha 挖掘范式，并利用大型语言模型的能力，通过一种新颖的提示工程算法框架，开发了 Alpha-GPT。通过多个实验，展示了 Alpha-GPT 在量化投资领域的有效性和优势。 |
| [^74] | [An Overview Of Temporal Commonsense Reasoning and Acquisition.](http://arxiv.org/abs/2308.00002) | 本文综述了时间常识推理领域的研究进展，重点关注通过增强语言模型的性能来提高推理能力，并对多个数据集进行评估。然而，这些增强模型仍然难以达到人类水平的推理能力。 |
| [^75] | [De Re and De Dicto Knowledge in Egocentric Setting.](http://arxiv.org/abs/2308.00001) | 本文在自我中心的环境中引入了两个不同模态来捕捉"De Re"和"De Dicto"知识，并证明了这两个模态不能互相定义。 |
| [^76] | [L3DMC: Lifelong Learning using Distillation via Mixed-Curvature Space.](http://arxiv.org/abs/2307.16459) | L3DMC是一种使用混合曲率空间进行终身学习的蒸馏策略，通过建模和维护复杂几何结构来保留已经学到的知识。 |
| [^77] | [Subspace Distillation for Continual Learning.](http://arxiv.org/abs/2307.16419) | 该论文提出了一种新颖的知识蒸馏技术，通过近似数据流形，并用线性子空间建模结构，来在持续学习中减轻灾难性遗忘。实验证明，该方法优于其他方法，在多个具有挑战性的持续学习任务中表现出色。 |
| [^78] | [Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment.](http://arxiv.org/abs/2307.16210) | 在多模态实体对齐中，现有的方法忽视了视觉图像的不完整性和模糊性，本文通过分析表明模型在面对不完整性时容易出现过拟合和性能下降的问题。 |
| [^79] | [An Empirical Study on Bugs Inside PyTorch: A Replication Study.](http://arxiv.org/abs/2307.13777) | 本研究对PyTorch深度学习库中的错误进行了复制研究，通过调查和评估错误的原因和症状，提供了对错误识别和修复过程的了解。 |
| [^80] | [EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus.](http://arxiv.org/abs/2307.11760) | EmotionPrompt是一个基于心理学的方法，通过将情感刺激融入到提示中，提升了大型语言模型在各项任务上的性能，并且同时改善了其真实性和信息量。 |
| [^81] | [Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models.](http://arxiv.org/abs/2307.11224) | Jina Embeddings是一组高性能的句子嵌入模型，能够捕捉文本的语义本质。该论文详细介绍了Jina Embeddings的开发过程，并通过性能评估验证了其优越性能。 |
| [^82] | [Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network.](http://arxiv.org/abs/2307.09866) | 该论文使用图神经网络和强化学习对城市基础设施相互依赖网络中的脆弱节点进行了准确建模和分析。 |
| [^83] | [How is ChatGPT's behavior changing over time?.](http://arxiv.org/abs/2307.09009) | 本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。 |
| [^84] | [Unsupervised Deep Graph Matching Based on Cycle Consistency.](http://arxiv.org/abs/2307.08930) | 本文提出了一种基于循环一致性的无监督深度图匹配方法，不需要真实对应的关键点对，通过在同一对象类别的图像之间强制匹配一致性来进行自我监督学习，该方法具有很高的灵活性，并且在无监督图匹配方面达到了最新的最先进水平。 |
| [^85] | [Pair then Relation: Pair-Net for Panoptic Scene Graph Generation.](http://arxiv.org/abs/2307.08699) | 本文提出了一种名为Pair-Net的全景场景图生成框架，通过使用配对提案网络（PPN）来学习和过滤主体和物体之间的稀疏配对关系，解决了当前全景场景图生成方法中忽视的对象间配对回忆率问题。 |
| [^86] | [Explainable AI with counterfactual paths.](http://arxiv.org/abs/2307.07764) | 本文提出了一种新颖的可解释人工智能方法，使用反事实路径来生成解释。通过确定替代路径，可以提供更直观和可解释的解释模型行为的方式，并帮助识别和减轻模型中的偏见。 |
| [^87] | [SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering.](http://arxiv.org/abs/2307.04192) | SAS视频问答通过自适应采样策略解决了视频问答中的问题，提高了效率和准确性 |
| [^88] | [Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks.](http://arxiv.org/abs/2307.02477) | 通过反事实任务的研究，我们发现当前的语言模型具备一定的抽象推理能力，但它们在任务求解过程中往往也依赖于狭窄、难以转移的过程，这对语言模型的性能解释和理解有着重要的启示。 |
| [^89] | [Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors.](http://arxiv.org/abs/2306.17156) | 该论文系统评估了ChatGPT、GPT-4和人类导师在不同的编程教育场景中的表现，并发现GPT-4优于ChatGPT，接近于人类导师。 |
| [^90] | [ChiPFormer: Transferable Chip Placement via Offline Decision Transformer.](http://arxiv.org/abs/2306.14744) | ChiPFormer通过离线学习可转移的布局策略，显著提高了芯片布局的质量，并在减少布局时间的同时增强了对未知芯片电路的适应能力。 |
| [^91] | [Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models.](http://arxiv.org/abs/2305.17446) | 该论文通过发现预训练语言模型的内在任务特定子空间，提出了一种重新参数化和微调模型的新方法。研究发现在该子空间中，只需少量自由参数即可有效微调模型，并且某些维度对于引入任务特定知识至关重要。 |
| [^92] | [Reinforcement Learning With Reward Machines in Stochastic Games.](http://arxiv.org/abs/2305.17372) | 该论文研究了复杂任务中具有非马尔可夫回报函数的随机博弈的多智能体强化学习，提出了一种基于奖励机制的算法，在纳什均衡下学习每个智能体的最佳应答策略，并证明了学习的Q函数将会收敛于一个纳什均衡的Q函数，前提是阶段博弈具有全局最优点或者鞍点，并且智能体基于这一点进行最佳应答策略的Q函数更新。 |
| [^93] | [AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback.](http://arxiv.org/abs/2305.14387) | 该论文提出了一种名为AlpacaFarm的低成本模拟器，该模拟器为从人类反馈中学习的研究和开发提供了一种解决方案，通过设计LLM提示来模拟人类反馈，提出自动评估并提供参考实现，克服了数据收集的高昂成本、缺乏可信的评估和缺乏参考方法实现的挑战。 |
| [^94] | [GELU Activation Function in Deep Learning: A Comprehensive Mathematical Analysis and Performance.](http://arxiv.org/abs/2305.12073) | 本文对GELU激活函数进行了全面的数学分析和广泛的实验比较，证明了它在深度学习模型中具有优越的性能和适用性。 |
| [^95] | [Continual Multimodal Knowledge Graph Construction.](http://arxiv.org/abs/2305.08698) | 连续多模态知识图谱构建面临着灾难性遗忘的挑战，需要解决新增实体和关系以及多模态源数据变化的问题。 |
| [^96] | [Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts.](http://arxiv.org/abs/2305.05832) | 本文提出了利用因果机制在不同环境下保持不变的直觉来主动准备的代理特征选择和工程技术，用以应对统计预测模型在分布转移情况下的稳定性问题。 |
| [^97] | [FedNoRo: Towards Noise-Robust Federated Learning by Addressing Class Imbalance and Label Noise Heterogeneity.](http://arxiv.org/abs/2305.05230) | 本文提出了一个名为 FedNoRo 的两阶段框架，用于解决类别不平衡和标签噪声异质性的联邦学习问题，并在 ICH 和 ISIC2019 数据集上取得了更好的表现。 |
| [^98] | [The Current State of Summarization.](http://arxiv.org/abs/2305.04853) | 摘要生成领域目前的研究关注点在于预训练的编码器-解码器模型和大规模自回归语言模型的转变，以及评估摘要生成系统的挑战和指令调整模型在零样本摘要生成中的潜力。 |
| [^99] | [Fundamental Limitations of Alignment in Large Language Models.](http://arxiv.org/abs/2304.11082) | 本文通过提出一种理论方法——行为期望边界（BEB），展示了大型语言模型中对齐的基本限制，并证明任何对齐过程都无法根除不希望的行为，这对于防止恶意攻击是不安全的。 |
| [^100] | [Dual-stream Time-Delay Neural Network with Dynamic Global Filter for Speaker Verification.](http://arxiv.org/abs/2303.11020) | 该论文提出了具有动态全局滤波器的双流时延神经网络，在说话人验证领域的表现优于其他最先进的方法，并实现了最先进的性能。 |
| [^101] | [mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection.](http://arxiv.org/abs/2303.09901) | 本研究提出了mCPT模型用于多语言的、多标签的零样本或少样本的框架检测任务，并在西班牙语和其他8种语言中取得了良好的成绩。该方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。 |
| [^102] | [Depression Detection Using Digital Traces on Social Media: A Knowledge-aware Deep Learning Approach.](http://arxiv.org/abs/2303.05389) | 本研究通过使用数字痕迹在社交媒体上检测抑郁症，提出了一种深度知识感知的抑郁症检测框架，并揭示了关键因素。通过真实数据进行的实证研究证明了其准确性和有效性。 |
| [^103] | [Understanding the Diffusion Objective as a Weighted Integral of ELBOs.](http://arxiv.org/abs/2303.00848) | 本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。 |
| [^104] | [On student-teacher deviations in distillation: does it pay to disobey?.](http://arxiv.org/abs/2301.12923) | 通过实验和理论分析，本论文发现在知识蒸馏中，学生网络对教师网络的概率偏离是系统性夸大的，同时也得到了更好的泛化能力。 |
| [^105] | [Abstracting Imperfect Information Away from Two-Player Zero-Sum Games.](http://arxiv.org/abs/2301.09159) | 通过正则化均衡，可以将两人零和博弈中的不完美信息抽象出来并作为完全信息问题处理。 |
| [^106] | [Machine Learning-Aided Efficient Decoding of Reed-Muller Subcodes.](http://arxiv.org/abs/2301.06251) | 本论文研究了具有灵活码率的Reed-Muller子码的高效解码问题，通过扩展递归投影聚合（RPA）译码算法，提出了subRPA和soft-subRPA算法，能够在维持较低复杂性的同时提高译码性能并实现可微分的解码算法。 |
| [^107] | [Lifelong Reinforcement Learning with Modulating Masks.](http://arxiv.org/abs/2212.11110) | 本文研究了终身强化学习中使用调整掩码的方法，通过将调整掩码应用于PPO和IMPALA代理，显著提高了在离散和连续强化学习任务中的性能。 |
| [^108] | [Space-fluid Adaptive Sampling by Self-Organisation.](http://arxiv.org/abs/2210.17505) | 本文提出了一种自组织的空间流体自适应采样方法来估计分布式传感数据或计算结果，其动态划分空间的方法具有优越的性能。 |
| [^109] | [3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows.](http://arxiv.org/abs/2210.11603) | 本文介绍了一种将文本到图像AI集成到3D设计工作流程中的方法，通过插件3DALL-E，设计师可以使用AI生成的图像灵感构建3D模型。研究结果显示，设计师对于3DALL-E在工作流程中有很大的潜力，可以生成参考图像、防止设计固化并激发设计的考虑。 |
| [^110] | [Towards Multi-Agent Reinforcement Learning driven Over-The-Counter Market Simulations.](http://arxiv.org/abs/2210.07184) | 该论文研究了多智能体强化学习在场外交易市场模拟中的应用，通过适当设计奖励函数和共享策略学习，智能体能够学习到涵盖利润损失、最优执行和市场份额等多目标的新兴行为，同时也提出了一种基于强化学习的校准算法。 |
| [^111] | [A Generalization of the Shortest Path Problem to Graphs with Multiple Edge-Cost Estimates.](http://arxiv.org/abs/2208.11489) | 本文提出了一个广义的加权有向图框架，其中可以多次计算（估计）边缘权重，以提高准确性和运行时间成本，解决了最短路径问题的不确定性。 |
| [^112] | [The emergence of division of labor through decentralized social sanctioning.](http://arxiv.org/abs/2208.05568) | 本研究通过引入社会规范模型，展示了分散社会制裁的出现模式能够解决以自利为导向的终身学习个体中的分工问题。 |
| [^113] | [Source Separation of Unknown Numbers of Single-Channel Underwater Acoustic Signals Based on Autoencoders.](http://arxiv.org/abs/2207.11749) | 本研究提出了一种基于自动编码器的解决方案，用于对未知数量的单通道水声信号进行源分离。通过固定输出通道数量和新的性能评估方法，避免了排列问题引起的维度灾难，并在实验证明与已知信号数量相似的分离性能。该算法具有竞争性能、可解释性和可扩展性，在该框架下达到了最先进的水平。 |
| [^114] | [Causal Discovery and Knowledge Injection for Contestable Neural Networks.](http://arxiv.org/abs/2205.09787) | 本研究提出了一种可以进行双向互动的方法，通过允许神经网络展示其所学因果图，并允许人类修改因果图后重新注入机器中，从而提供了一种调试神经网络的方式，实验结果显示该方法可以显著改善预测性能。 |
| [^115] | [A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part II: Applications, Cognitive Models, and Challenges.](http://arxiv.org/abs/2112.15424) | 超维计算（HDC/VSA）是一种计算框架，利用高维分布式表示和代数操作的特性，结合了结构化符号表示和向量分布式表示的优势。本综述的第二部分介绍了HDC/VSA的应用、认知模型和挑战。 |
| [^116] | [A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part I: Models and Data Transformations.](http://arxiv.org/abs/2111.06077) | 本文综述介绍了超维计算和矢量符号化架构（HDC/VSA），这是一种利用高维分布式表示和代数属性的计算模型，融合了结构化符号表示和向量分布式表示的优势。该领域涉及多个学科，并介绍了多个相关模型。 |
| [^117] | [Dynamic Collective Intelligence Learning: Finding Efficient Sparse Model via Refined Gradients for Pruned Weights.](http://arxiv.org/abs/2109.04660) | 本文介绍了动态集体智能学习（DCIL）方法，利用精炼的梯度更新剪枝权重，通过形成双向转发路径来寻找高效稀疏模型。这种方法利用了剪枝和未剪枝权重的集体智能之间的学习协同作用。 |
| [^118] | [Hyperbolic Minesweeper is in P.](http://arxiv.org/abs/2002.09534) | 超半径的扫雷游戏属于P类问题，这一发现不仅适用于扫雷游戏本身，还适用于其他基于超半径平面上嵌入图的谜题。 |

# 详细

[^1]: AnyLoc:朝着通用的视觉地点识别迈进

    AnyLoc: Towards Universal Visual Place Recognition. (arXiv:2308.00688v1 [cs.CV])

    [http://arxiv.org/abs/2308.00688](http://arxiv.org/abs/2308.00688)

    AnyLoc实现了一个通用的视觉地点识别(VPR)解决方案，能够在各种结构化和非结构化环境中运行，并通过组合通用特征表示和无监督特征聚合实现了显著的性能提升。

    

    视觉地点识别(VPR)对于机器人定位至关重要。迄今为止，最有效的VPR方法是针对特定环境和任务设计的：虽然它们在结构化环境（主要是城市驾驶）中表现出强大的性能，但在非结构化环境中它们的性能严重下降，使得大多数方法无法在真实世界中稳健部署。在这项工作中，我们开发了一种通用的VPR解决方案——一种在各种结构化和非结构化环境（城市、户外、室内、空中、水下和地下环境）中均可运行的技术，而无需重新训练或微调。我们证明了从现成的自监督模型中获得的通用特征表示是构建这种通用VPR解决方案的理想基础。通过将这些派生特征与无监督特征聚合相结合，我们的方法套件AnyLoc能够实现高达4倍的显著提高

    Visual Place Recognition (VPR) is vital for robot localization. To date, the most performant VPR approaches are environment- and task-specific: while they exhibit strong performance in structured environments (predominantly urban driving), their performance degrades severely in unstructured environments, rendering most approaches brittle to robust real-world deployment. In this work, we develop a universal solution to VPR -- a technique that works across a broad range of structured and unstructured environments (urban, outdoors, indoors, aerial, underwater, and subterranean environments) without any re-training or fine-tuning. We demonstrate that general-purpose feature representations derived from off-the-shelf self-supervised models with no VPR-specific training are the right substrate upon which to build such a universal VPR solution. Combining these derived features with unsupervised feature aggregation enables our suite of methods, AnyLoc, to achieve up to 4X significantly higher 
    
[^2]: 视觉编码模型的可扩展性规律适用性研究

    Applicability of scaling laws to vision encoding models. (arXiv:2308.00678v1 [q-bio.NC])

    [http://arxiv.org/abs/2308.00678](http://arxiv.org/abs/2308.00678)

    本研究探究了视觉编码模型的可扩展性规律适用性，发现增加训练样本数量和视觉模型参数大小可以提高预测准确性。

    

    本研究旨在探究如何构建高性能的视觉编码模型，以预测大脑活动，并参与Algonauts Project 2023 Challenge。该挑战提供了功能性磁共振成像（fMRI）记录的大脑活动数据，参与者观看图像时产生。我们采用了参数规模从86M到4.3B的多种视觉模型构建预测模型。为了构建高度准确的模型，我们重点分析了两个主要方面：（1）fMRI训练集的样本大小如何影响预测准确性？（2）视觉模型的参数大小对视觉皮质预测准确性的影响是如何变化的？结果显示，随着训练时使用的样本数量增加，预测准确性按照扩展规律得到改善。同样，我们发现随着视觉模型参数大小的增加，预测准确性也按照扩展规律得到改善。这些结果表明，增加训练样本数量和视觉模型参数大小可以提高预测准确性。

    In this paper, we investigated how to build a high-performance vision encoding model to predict brain activity as part of our participation in the Algonauts Project 2023 Challenge. The challenge provided brain activity recorded by functional MRI (fMRI) while participants viewed images. Several vision models with parameter sizes ranging from 86M to 4.3B were used to build predictive models. To build highly accurate models, we focused our analysis on two main aspects: (1) How does the sample size of the fMRI training set change the prediction accuracy? (2) How does the prediction accuracy across the visual cortex vary with the parameter size of the vision models? The results show that as the sample size used during training increases, the prediction accuracy improves according to the scaling law. Similarly, we found that as the parameter size of the vision models increases, the prediction accuracy improves according to the scaling law. These results suggest that increasing the sample siz
    
[^3]: 工具文档使得大型语言模型能够进行零-shot工具使用

    Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models. (arXiv:2308.00675v1 [cs.CL])

    [http://arxiv.org/abs/2308.00675](http://arxiv.org/abs/2308.00675)

    这项工作提出了使用工具文档作为教导大型语言模型使用新工具的替代方法，并通过实证研究证明，仅使用工具文档的零-shot提示足以实现正确的工具使用。

    

    如今，通过提供一些工具使用的演示来教授大型语言模型（LLM）使用新工具。不幸的是，演示很难获得，并且如果选择了错误的演示，可能会导致不良的偏见使用。即使在罕见的情况下，演示是readily available的，也没有原则性的选择协议来确定提供多少个和哪些演示。随着任务变得更加复杂，选择搜索组合数的增长成为不可处理的。我们的工作提供了一种替代演示的方法：工具文档。我们主张使用工具文档来描述各个工具的使用，而不是演示。我们通过跨视觉和语言模态的6个任务上的三个主要实证发现支持我们的主张。首先，在现有的基准测试上，仅使用工具文档的零-shot提示足以引出正确的工具使用，达到了few-shot提示的性能水平。

    Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage. Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration is chosen. Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determine how many and which ones to provide. As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable. Our work provides an alternative to demonstrations: tool documentation. We advocate the use of tool documentation, descriptions for the individual tool usage, over demonstrations. We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities. First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage, achieving performance on par with few-shot prompts. Secon
    
[^4]: Hessian-Aware Bayesian Optimization for Decision Making Systems - 感知海森贝叶斯优化在决策系统中的应用

    Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v1 [cs.LG])

    [http://arxiv.org/abs/2308.00629](http://arxiv.org/abs/2308.00629)

    本文介绍了一种感知海森贝叶斯优化算法，旨在解决决策系统优化中梯度反馈稀缺或无效的问题。通过引入紧凑的多层架构和角色概念，并利用感知海森贝叶斯优化方法对参数进行优化，作者实现了对复杂决策系统的高效优化。

    

    许多优化决策系统的方法依赖于梯度方法，需要从环境中获取有信息量的反馈。然而，当反馈稀缺或者无信息时，这些方法可能导致性能较差。贝叶斯优化等无导数方法可以减少对梯度反馈质量的依赖，但在复杂决策系统的高维环境中往往难以扩展。如果系统需要多个参与者之间的互动来实现共同目标，这个问题就加剧了。为了解决维度问题，我们提出了一种紧凑的多层架构，通过角色的概念来建模参与者之间的动态。此外，我们还引入了感知海森贝叶斯优化来高效地优化由大量参数参数化的多层架构。实验结果表明，我们的方法(HA-GP-UCB)在效果上是有效的。

    Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectiv
    
[^5]: 人类-M3：一个用于室外场景中的3D人体姿势估计的多视角多模态数据集

    Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes. (arXiv:2308.00628v1 [cs.CV])

    [http://arxiv.org/abs/2308.00628](http://arxiv.org/abs/2308.00628)

    这篇论文提出了一个室外多模态多视角多人类姿势数据库Human-M3，并介绍了一种基于多模态数据输入的算法来生成准确的人体姿势。这个数据库解决了现有数据集的不足，提供了更多的数据多样性。

    

    最近，对于室外环境中的3D人体姿势估计越来越受到关注。然而，现有的室外场景3D人体姿势数据集缺乏多样性，因为它们主要只使用一种模态（RGB图像或点云），并且场景中通常只有一个人。数据集基础的有限范围严重阻碍了可用数据的变化性。在本文中，我们提出了Human-M3，这是一个室外多模态多视角多人类姿势数据库，其中包括室外场景的多视角RGB视频和相应的点云数据。为了获得准确的人体姿势，我们提出了一种基于多模态数据输入的算法来生成地面真值标注。这种方法利用了鲁棒的点云检测和跟踪，解决了之前室外场景中多个人的多视角RGB视频中可能存在的不准确人体定位和匹配模糊问题，生成了相关信息。

    3D human pose estimation in outdoor environments has garnered increasing attention recently. However, prevalent 3D human pose datasets pertaining to outdoor scenes lack diversity, as they predominantly utilize only one type of modality (RGB image or pointcloud), and often feature only one individual within each scene. This limited scope of dataset infrastructure considerably hinders the variability of available data. In this article, we propose Human-M3, an outdoor multi-modal multi-view multi-person human pose database which includes not only multi-view RGB videos of outdoor scenes but also corresponding pointclouds. In order to obtain accurate human poses, we propose an algorithm based on multi-modal data input to generate ground truth annotation. This benefits from robust pointcloud detection and tracking, which solves the problem of inaccurate human localization and matching ambiguity that may exist in previous multi-view RGB videos in outdoor multi-person scenes, and generates rel
    
[^6]: JIANG: 中国开放基础语言模型

    JIANG: Chinese Open Foundation Language Model. (arXiv:2308.00624v1 [cs.CL])

    [http://arxiv.org/abs/2308.00624](http://arxiv.org/abs/2308.00624)

    JIANG是一个专为中文设计的开放式语言模型，通过使用大量的中文语料库进行训练和优化结构，能够更好地在中文中发挥其能力。

    

    随着大型语言模型技术的进步，它展示了接近人类水平的能力，可以在各种任务上表现出色。这一成就引起了公司和科研机构的极大兴趣，导致对这些模型的研究和开发进行了大量投资。虽然在这个时期出现了许多大型模型，但其中大多数主要是基于英文数据进行训练。虽然它们在其他语言（如中文）中表现出了不错的性能，但由于词汇设计和训练语料库等因素，其潜力仍然受限，无法完全发挥在中文中的能力。为了解决这个问题，我们介绍了名为JIANG（姜的拼音）的专门针对中文的模型。我们收集了大量的中文语料库来训练模型，并对其结构进行了优化。广泛的实验结果表明...

    With the advancements in large language model technology, it has showcased capabilities that come close to those of human beings across various tasks. This achievement has garnered significant interest from companies and scientific research institutions, leading to substantial investments in the research and development of these models. While numerous large models have emerged during this period, the majority of them have been trained primarily on English data. Although they exhibit decent performance in other languages, such as Chinese, their potential remains limited due to factors like vocabulary design and training corpus. Consequently, their ability to fully express their capabilities in Chinese falls short. To address this issue, we introduce the model named JIANG (Chinese pinyin of ginger) specifically designed for the Chinese language. We have gathered a substantial amount of Chinese corpus to train the model and have also optimized its structure. The extensive experimental res
    
[^7]: 超越One-Hot-Encoding: 注入语义驱动图像分类器

    Beyond One-Hot-Encoding: Injecting Semantics to Drive Image Classifiers. (arXiv:2308.00607v1 [cs.CV])

    [http://arxiv.org/abs/2308.00607](http://arxiv.org/abs/2308.00607)

    本文探索了将本体和语义知识反映到图像分类器中的方法，提高了模型的可解释性和可信度。

    

    图像中包含了与现实世界本体论相关的语义信息：狗的品种具有哺乳动物的相似性，食物的图片通常在家庭环境中描述，等等。然而，在对图像分类进行机器学习模型训练时，对象类之间的相对相似性常常与One-Hot-Encoding标签配对。根据这种逻辑，如果一个图像被标记为“勺子”，那么“茶勺”和“鲨鱼”在训练损失方面是同样错误的。为了克服这个限制，我们探索了整合反映本体和语义知识的额外目标的方法，提高了模型的可解释性和可信度。我们提出了一种通用方法，可以根据与分类标签相关的任何类型的语义信息导出额外的损失项。首先，我们展示了如何将我们的方法应用于本体和词嵌入，并讨论了由此得到的信息如何驱动受监督的学习过程。其次，

    Images are loaded with semantic information that pertains to real-world ontologies: dog breeds share mammalian similarities, food pictures are often depicted in domestic environments, and so on. However, when training machine learning models for image classification, the relative similarities amongst object classes are commonly paired with one-hot-encoded labels. According to this logic, if an image is labelled as 'spoon', then 'tea-spoon' and 'shark' are equally wrong in terms of training loss. To overcome this limitation, we explore the integration of additional goals that reflect ontological and semantic knowledge, improving model interpretability and trustworthiness. We suggest a generic approach that allows to derive an additional loss term starting from any kind of semantic information about the classification label. First, we show how to apply our approach to ontologies and word embeddings, and discuss how the resulting information can drive a supervised learning process. Second
    
[^8]: 在随机位置预测掩盖的标记改善了掩盖图像建模

    Predicting masked tokens in stochastic locations improves masked image modeling. (arXiv:2308.00566v1 [cs.CV])

    [http://arxiv.org/abs/2308.00566](http://arxiv.org/abs/2308.00566)

    本论文提出了一种名为FlexPredict的随机模型，通过在模型中加入位置不确定性，以预测掩盖的标记位置，从而改善了掩盖图像建模的性能。

    

    自监督学习是深度学习中一种有前景的范式，通过构建需要学习有用表示的预训练任务，可以从无标签数据中进行学习。在自然语言处理中，主要的预训练任务是掩盖语言建模（MLM），而在计算机视觉中存在相应的掩盖图像建模（MIM）。然而，MIM具有挑战性，因为它需要在准确位置上预测语义内容。例如，给定一张不完整的狗的图片，我们可以猜测有一个尾巴，但我们无法确定它的确切位置。在这项工作中，我们提出了FlexPredict，这是一个考虑位置不确定性的随机模型，通过将模型条件化到随机掩盖的标记位置上，引导模型学习更加鲁棒对位置不确定性的特征。我们的方法改进了多个任务的下游性能，例如与MIM基准相比，FlexPredict在一系列任务上表现更好。

    Self-supervised learning is a promising paradigm in deep learning that enables learning from unlabeled data by constructing pretext tasks that require learning useful representations. In natural language processing, the dominant pretext task has been masked language modeling (MLM), while in computer vision there exists an equivalent called Masked Image Modeling (MIM). However, MIM is challenging because it requires predicting semantic content in accurate locations. E.g, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose FlexPredict, a stochastic model that addresses this challenge by incorporating location uncertainty into the model. Specifically, we condition the model on stochastic masked token positions to guide the model toward learning features that are more robust to location uncertainties. Our approach improves downstream performance on a range of tasks, e.g, compared to MIM baselines, Fle
    
[^9]: 基于强化学习的非自回归求解器用于旅行推销员问题

    Reinforcement Learning-based Non-Autoregressive Solver for Traveling Salesman Problems. (arXiv:2308.00560v1 [cs.AI])

    [http://arxiv.org/abs/2308.00560](http://arxiv.org/abs/2308.00560)

    基于强化学习的非自回归TSP求解器NAR4TSP使用特别设计的图神经网络进行推理，消除了昂贵标签的依赖，并在解决方案质量、推理延迟和泛化能力方面优于其他四个最先进的模型。

    

    旅行推销员问题（TSP）是组合优化中的一个众所周知的问题，具有在各个领域的应用。然而，现有的TSP求解器在产生高质量解决方案时面临低延迟的挑战。为了解决这个问题，我们提出了NAR4TSP，它使用一个特别设计的图神经网络（GNN）以非自回归（NAR）方式生成TSP解决方案，实现更快的推理速度。此外，NAR4TSP使用增强的强化学习（RL）策略进行训练，消除了传统监督学习基于NAR模型训练所使用的昂贵标签的依赖关系。据我们所知，NAR4TSP是第一个成功结合了RL和NAR解码的TSP求解器。在合成和真实的TSP实例上的实验结果表明，NAR4TSP在解决方案质量、推理延迟和泛化能力方面优于四个最先进的模型。最后，我们展示了NAR4TSP解码过程的可视化。

    The Traveling Salesman Problem (TSP) is a well-known problem in combinatorial optimization with applications in various domains. However, existing TSP solvers face challenges in producing high-quality solutions with low latency. To address this issue, we propose NAR4TSP, which produces TSP solutions in a Non-Autoregressive (NAR) manner using a specially designed Graph Neural Network (GNN), achieving faster inference speed. Moreover, NAR4TSP is trained using an enhanced Reinforcement Learning (RL) strategy, eliminating the dependency on costly labels used to train conventional supervised learning-based NAR models. To the best of our knowledge, NAR4TSP is the first TSP solver that successfully combines RL and NAR decoding. The experimental results on both synthetic and real-world TSP instances demonstrate that NAR4TSP outperforms four state-of-the-art models in terms of solution quality, inference latency, and generalization ability. Lastly, we present visualizations of NAR4TSP's decodin
    
[^10]: Copula用于实例级特征选择和排序

    Copula for Instance-wise Feature Selection and Ranking. (arXiv:2308.00549v1 [cs.LG])

    [http://arxiv.org/abs/2308.00549](http://arxiv.org/abs/2308.00549)

    在实例级特征选择和排序中，我们提出了一种使用Copula的方法，能够更好地考虑特征之间的相关性，实验结果证明我们的方法能够捕捉有意义的相关性。

    

    在神经网络的背景下，实例级特征选择和排序方法可以为每个样本实现良好的特征选择。然而，现有的方法假设特征子集独立，对于考虑特征之间的依赖性存在缺陷。为了解决这个限制，我们提出将高斯copula，一种捕捉变量之间相关性的强大数学技术，无需进行额外的更改就可以将其纳入当前的特征选择框架中。通过在合成和真实数据集上的实验结果，在性能比较和可解释性方面，证明了我们的方法能够捕捉有意义的相关性。

    Instance-wise feature selection and ranking methods can achieve a good selection of task-friendly features for each sample in the context of neural networks. However, existing approaches that assume feature subsets to be independent are imperfect when considering the dependency between features. To address this limitation, we propose to incorporate the Gaussian copula, a powerful mathematical technique for capturing correlations between variables, into the current feature selection framework with no additional changes needed. Experimental results on both synthetic and real datasets, in terms of performance comparison and interpretability, demonstrate that our method is capable of capturing meaningful correlations.
    
[^11]: 预测一个积极健康老龄化应用程序的早期退学

    Predicting Early Dropouts of an Active and Healthy Ageing App. (arXiv:2308.00539v1 [cs.LG])

    [http://arxiv.org/abs/2308.00539](http://arxiv.org/abs/2308.00539)

    本文提出了一种机器学习方法，用于预测积极健康老龄化应用程序的早期退学。通过使用动态和静态特征构建分类模型以及采用过采样方法提高分类性能，我们获得了高质量的粘附度预测，并在科学挑战赛中获得了第一名。

    

    本文提出了一种机器学习方法，用于预测积极健康老龄化应用程序的早期退学。所提出的算法已提交给2022年IFMBE科学挑战赛，是IUPESM WC 2022的一部分。我们处理了给定的数据库并生成了七个数据集。我们使用预处理技术构建分类模型，使用动态和静态特征预测用户的粘附度。我们提交了11次官方运行，结果显示机器学习算法可以提供高质量的粘附度预测。根据结果，动态特征对模型的分类性能有积极影响。由于数据集的不平衡性质，我们采用了过采样方法，如SMOTE和ADASYN，以提高分类性能。过采样方法使结果获得了10%的显著改进。我们的方法在2022年IFMBE科学挑战赛中获得了第一名。

    In this work, we present a machine learning approach for predicting early dropouts of an active and healthy ageing app. The presented algorithms have been submitted to the IFMBE Scientific Challenge 2022, part of IUPESM WC 2022. We have processed the given database and generated seven datasets. We used pre-processing techniques to construct classification models that predict the adherence of users using dynamic and static features. We submitted 11 official runs and our results show that machine learning algorithms can provide high-quality adherence predictions. Based on the results, the dynamic features positively influence a model's classification performance. Due to the imbalanced nature of the dataset, we employed oversampling methods such as SMOTE and ADASYN to improve the classification performance. The oversampling approaches led to a remarkable improvement of 10\%. Our methods won first place in the IFMBE Scientific Challenge 2022.
    
[^12]: PressureTransferNet: 使用3D模拟的压力图进行以人类属性为指导的动态地面压力分布转换的人体活动识别方法

    PressureTransferNet: Human Attribute Guided Dynamic Ground Pressure Profile Transfer using 3D simulated Pressure Maps. (arXiv:2308.00538v1 [cs.CV])

    [http://arxiv.org/abs/2308.00538](http://arxiv.org/abs/2308.00538)

    我们提出了一种新的人体活动识别方法，使用PressureTransferNet模型可以准确地将人类属性转换为地面压力分布，并在不同场景中验证了其有效性。

    

    我们提出了PressureTransferNet，一种使用地面压力信息进行人体活动识别的新方法。我们的方法通过利用不同个体的已有压力数据生成特定活动的特定个体的动态地面压力分布图。PressureTransferNet是一个编码器-解码器模型，以源压力图和目标人类属性向量作为输入，生成反映目标属性的新压力图。为了训练模型，我们使用传感器模拟来创建一个包含各种人类属性和压力分布的多样化数据集。对真实世界数据集的评估显示出它在不同场景中准确地将人类属性转换为地面压力分布的有效性。我们通过基于物理的深度学习模型对合成压力形状的准确性进行了视觉验证，并在接触地面的区域上实现了0.79的二次R-square值。通过F1得分（0.911±0.015）进行分类验证。

    We propose PressureTransferNet, a novel method for Human Activity Recognition (HAR) using ground pressure information. Our approach generates body-specific dynamic ground pressure profiles for specific activities by leveraging existing pressure data from different individuals. PressureTransferNet is an encoder-decoder model taking a source pressure map and a target human attribute vector as inputs, producing a new pressure map reflecting the target attribute. To train the model, we use a sensor simulation to create a diverse dataset with various human attributes and pressure profiles. Evaluation on a real-world dataset shows its effectiveness in accurately transferring human attributes to ground pressure profiles across different scenarios. We visually confirm the fidelity of the synthesized pressure shapes using a physics-based deep learning model and achieve a binary R-square value of 0.79 on areas with ground contact. Validation through classification with F1 score (0.911$\pm$0.015)
    
[^13]: 动态特征下基于图嵌入的电力系统瞬态稳定性监测的监督对比学习

    Graph Embedding Dynamic Feature-based Supervised Contrastive Learning of Transient Stability for Changing Power Grid Topologies. (arXiv:2308.00537v1 [eess.SY])

    [http://arxiv.org/abs/2308.00537](http://arxiv.org/abs/2308.00537)

    提出了一种基于图嵌入动态特征的瞬态稳定性GEDF-SCL模型，利用监督对比学习结合GEDF来预测电力系统的瞬态稳定性，并考虑了拓扑信息。实验结果表明该模型具有良好的性能。

    

    在面对干扰时，准确的在线瞬态稳定性预测对于确保电力系统的稳定至关重要。传统的瞬态稳定性分析依赖于时间域仿真，不能快速适应电力网格拓扑的变化。为了将高维电力系统拓扑结构信息向量化为低维节点嵌入流数据，提出了基于图嵌入动态特征（GEDF）的瞬态稳定性GEDF-监督对比学习（GEDF-SCL）模型。该模型利用监督对比学习结合GEDF来预测瞬态稳定性，考虑了电力系统的拓扑信息。为了评估所提出的GEDF-SCL模型的性能，基于IEEE 39节点系统模型生成了拓扑结构不同的电力网格。通过在这些生成的电力系统拓扑上模拟N-1和N-$\bm{m}$-1故障，获取了瞬态运行数据。测试结果表明GEDF-SCL模型具有良好的瞬态稳定性预测性能。

    Accurate online transient stability prediction is critical for ensuring power system stability when facing disturbances. While traditional transient stablity analysis replies on the time domain simulations can not be quickly adapted to the power grid toplogy change. In order to vectorize high-dimensional power grid topological structure information into low-dimensional node-based graph embedding streaming data, graph embedding dynamic feature (GEDF) has been proposed. The transient stability GEDF-based supervised contrastive learning (GEDF-SCL) model uses supervised contrastive learning to predict transient stability with GEDFs, considering power grid topology information. To evaluate the performance of the proposed GEDF-SCL model, power grids of varying topologies were generated based on the IEEE 39-bus system model. Transient operational data was obtained by simulating N-1 and N-$\bm{m}$-1 contingencies on these generated power system topologies. Test result demonstrated that the GED
    
[^14]: 点注释概率图: 通过容忍注释噪声实现密集对象计数

    Point Annotation Probability Map: Towards Dense Object Counting by Tolerating Annotation Noise. (arXiv:2308.00530v1 [cs.CV])

    [http://arxiv.org/abs/2308.00530](http://arxiv.org/abs/2308.00530)

    该论文提出通过容忍注释噪声实现了密集对象计数的点注释概率图（PAPM）方法。通过利用广义高斯分布来形成学习目标PAPM，在拥挤场景中具有强鲁棒性。论文提出了两种方法，一种是基于人工设计的PAPM方法（HD-PAPM），另一种是自适应学习的PAPM方法（AL-PAPM）。

    

    在拥挤场景中计数对象对于计算机视觉仍然是一个挑战。目前的基于深度学习的方法通常将其形式化为高斯密度回归问题。虽然这种暴力回归方法效果不错，但可能没有很好地考虑到由人工注释过程引起的注释噪声，并可能导致不同的分布。我们推测，在密集对象计数任务中考虑注释噪声可能是有益的。为了获得对注释噪声的强鲁棒性，利用具有可调带宽和形状参数的广义高斯分布（GGD）函数来形成学习目标点注释概率图（PAPM）。具体来说，我们首先提出了一个基于人工设计的PAPM方法（HD-PAPM），其中我们设计了一个基于GGD的函数来容忍注释噪声。对于端到端训练，基于人工设计的PAPM可能对于特定的网络和数据集来说并不是最优的。因此，我们提出了一种自适应学习的PAPM方法（AL-PAPM），

    Counting objects in crowded scenes remains a challenge to computer vision. The current deep learning based approach often formulate it as a Gaussian density regression problem. Such a brute-force regression, though effective, may not consider the annotation noise properly which arises from the human annotation process and may lead to different distributions. We conjecture that it would be beneficial to consider the annotation noise in the dense object counting task. To obtain strong robustness against annotation noise, generalized Gaussian distribution (GGD) function with a tunable bandwidth and shape parameter is exploited to form the learning target point annotation probability map, PAPM. Specifically, we first present a hand-designed PAPM method (HD-PAPM), in which we design a function based on GGD to tolerate the annotation noise. For end-to-end training, the hand-designed PAPM may not be optimal for the particular network and dataset. An adaptively learned PAPM method (AL-PAPM) is
    
[^15]: 基于转移集成学习的深度卷积神经网络用于糖尿病视网膜病变分类

    Transfer-Ensemble Learning based Deep Convolutional Neural Networks for Diabetic Retinopathy Classification. (arXiv:2308.00525v1 [cs.CV])

    [http://arxiv.org/abs/2308.00525](http://arxiv.org/abs/2308.00525)

    本文提出了一种基于转移集成学习的深度卷积神经网络模型，用于糖尿病视网膜病变的分类。该模型通过利用两个预训练网络的优势，将图像特征转换为固定长度的向量，并通过集成学习的方法，提高了糖尿病视网膜病变的分类性能。

    

    本文旨在利用两种流行的预训练卷积神经网络（VGG16和Inception V3）的集成方法，将糖尿病视网膜病变（DR）疾病分类为五个不同的类别。提出的模型旨在利用两个单独网络的优势，以提高对糖尿病视网膜病变的分类性能。集成模型架构涉及冻结每个预训练模型的一部分层以有效利用它们的学习表示。全局平均池化层被添加以将输出特征图转换为固定长度的向量。然后将这些向量连接起来形成输入图像的集合表示。集成模型使用糖尿病视网膜病变图片数据集（APTOS）进行训练，分为训练集和验证集。在训练过程中，模型学习将视网膜图像分类为相应的糖尿病视网膜病变类别。在测试集上进行实验结果评估。

    This article aims to classify diabetic retinopathy (DR) disease into five different classes using an ensemble approach based on two popular pre-trained convolutional neural networks: VGG16 and Inception V3. The proposed model aims to leverage the strengths of the two individual nets to enhance the classification performance for diabetic retinopathy. The ensemble model architecture involves freezing a portion of the layers in each pre-trained model to utilize their learned representations effectively. Global average pooling layers are added to transform the output feature maps into fixed-length vectors. These vectors are then concatenated to form a consolidated representation of the input image. The ensemble model is trained using a dataset of diabetic retinopathy images (APTOS), divided into training and validation sets. During the training process, the model learns to classify the retinal images into the corresponding diabetic retinopathy classes. Experimental results on the test set 
    
[^16]: SurveyLM: 一种探索增强语言模型行为中新兴价值观的平台

    SurveyLM: A platform to explore emerging value perspectives in augmented language models' behaviors. (arXiv:2308.00521v1 [cs.AI])

    [http://arxiv.org/abs/2308.00521](http://arxiv.org/abs/2308.00521)

    SurveyLM是一个用于分析增强语言模型行为中新兴价值观的平台，通过调查和实验方法系统评估了ALMs的对齐和新兴行为，并利用ALMs的反馈来增强调查和实验设计。

    

    本白皮书介绍了我们在SurveyLM上的工作，这是一个用于分析增强语言模型(ALM)在复杂社会环境中通过动态演变的态度和价值观展现出的紧密对齐行为的平台。像ALM这样的社交人工智能系统通常在细微的社交场景中运作，没有单一的正确回答，或者答案很大程度上取决于背景因素，因此需要深入理解它们的对齐动态。为了解决这个问题，我们应用传统社会行为研究中常用的调查和实验方法系统地评估ALM，从而提供了对其对齐和新兴行为的前所未有的深入洞察。此外，SurveyLM平台利用ALM自身的反馈来增强调查和实验设计，利用了ALM未充分利用的方面，加速了高质量调查框架的开发和测试，同时节约了资源。

    This white paper presents our work on SurveyLM, a platform for analyzing augmented language models' (ALMs) emergent alignment behaviors through their dynamically evolving attitude and value perspectives in complex social contexts. Social Artificial Intelligence (AI) systems, like ALMs, often function within nuanced social scenarios where there is no singular correct response, or where an answer is heavily dependent on contextual factors, thus necessitating an in-depth understanding of their alignment dynamics. To address this, we apply survey and experimental methodologies, traditionally used in studying social behaviors, to evaluate ALMs systematically, thus providing unprecedented insights into their alignment and emergent behaviors. Moreover, the SurveyLM platform leverages the ALMs' own feedback to enhance survey and experiment designs, exploiting an underutilized aspect of ALMs, which accelerates the development and testing of high-quality survey frameworks while conserving resour
    
[^17]: 基于定性专家知识的量化代理模型开发框架：一个有组织犯罪的应用案例

    Framework for developing quantitative agent based models based on qualitative expert knowledge: an organised crime use-case. (arXiv:2308.00505v1 [cs.AI])

    [http://arxiv.org/abs/2308.00505](http://arxiv.org/abs/2308.00505)

    提出了一个基于定性专家知识的量化代理模型开发框架，该框架通过将定性数据翻译成定量规则，为模型构建者和领域专家提供了一个系统和透明的建模过程。以一个有组织犯罪的应用案例为例，演示了该框架的方法。

    

    为了对执法目的建模犯罪网络，需要将有限的数据转化为经过验证的基于代理的模型。当前刑事学建模中缺少一个为模型构建者和领域专家提供系统和透明框架的方法，该方法建立了计算犯罪建模的建模过程，包括将定性数据转化为定量规则。因此，我们提出了FREIDA（基于专家知识驱动的数据驱动代理模型框架）。在本文中，犯罪可卡因替代模型（CCRM）将作为示例案例，以演示FREIDA方法。对于CCRM，正在建模荷兰的一个有组织可卡因网络，试图通过移除首脑节点，使剩余代理重新组织，并将网络恢复到稳定状态。定性数据源，例如案件文件，文献和采访，被转化为经验法则。

    In order to model criminal networks for law enforcement purposes, a limited supply of data needs to be translated into validated agent-based models. What is missing in current criminological modelling is a systematic and transparent framework for modelers and domain experts that establishes a modelling procedure for computational criminal modelling that includes translating qualitative data into quantitative rules. For this, we propose FREIDA (Framework for Expert-Informed Data-driven Agent-based models). Throughout the paper, the criminal cocaine replacement model (CCRM) will be used as an example case to demonstrate the FREIDA methodology. For the CCRM, a criminal cocaine network in the Netherlands is being modelled where the kingpin node is being removed, the goal being for the remaining agents to reorganize after the disruption and return the network into a stable state. Qualitative data sources such as case files, literature and interviews are translated into empirical laws, and c
    
[^18]: 可解释的文本文档的图谱聚类

    Explainable Graph Spectral Clustering of Text Documents. (arXiv:2308.00504v1 [cs.LG])

    [http://arxiv.org/abs/2308.00504](http://arxiv.org/abs/2308.00504)

    本文提出了一种可解释的文本文档的图谱聚类方法，通过展示组合拉普拉斯嵌入、K嵌入和词向量空间嵌入之间的等价性，构建了文本内容和聚类结果之间的桥梁。

    

    光谱聚类方法以其能够表示不同形状、密度等的聚类而闻名。然而，将这些算法应用于文本文档时，其结果很难向用户解释，特别是由于在光谱空间中的嵌入与文档内容没有明显的关系。因此，迫切需要研究解释聚类结果的方法。本文提出了对此目标的贡献。我们提出了解释基于组合拉普拉斯的图谱聚类结果的方法。该方法基于展示组合拉普拉斯嵌入、K嵌入（本文提出）和词向量空间嵌入的（近似）等价性。从而构建了文本内容和聚类结果之间的桥梁。我们为这种方法提供了理论背景。我们进行了实验研究，结果表明，在有利条件下，K嵌入很好地近似了拉普拉斯嵌入。

    Spectral clustering methods are known for their ability to represent clusters of diverse shapes, densities etc. However, results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Therefore there is an urgent need to elaborate methods for explaining the outcome of the clustering. This paper presents a contribution towards this goal. We present a proposal of explanation of results of combinatorial Laplacian based graph spectral clustering. It is based on showing (approximate) equivalence of combinatorial Laplacian embedding, $K$-embedding (proposed in this paper) and term vector space embedding. Hence a bridge is constructed between the textual contents and the clustering results. We provide theoretical background for this approach. We performed experimental study showing that $K$-embedding approximates well Laplacian embedding under favourable blo
    
[^19]: 在医学教育中，用于大规模非结构化文本数据的检索增强生成和代表性向量摘要

    Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education. (arXiv:2308.00479v1 [cs.CL])

    [http://arxiv.org/abs/2308.00479](http://arxiv.org/abs/2308.00479)

    本文介绍了在医学教育领域中，使用检索增强生成（RAG）将非参数化知识库与大规模语言模型（LLMs）结合，以解决幻觉和有害答案问题，并提出了一种使用代表性向量的抽取性和抽象性摘要方法。

    

    大规模语言模型越来越多地用于各种任务，包括内容生成和作为聊天机器人。尽管它们在一般任务中表现出色，但在应用于特定领域任务时，需要对LLMs进行调整以减轻产生幻觉和有害答案的问题。检索增强生成（RAG）允许轻松地连接和操作非参数化知识库到LLMs上。本文讨论了RAG在医学教育领域的应用。提出了一种使用代表性向量的大规模非结构化文本数据的抽取性和抽象性摘要方法。

    Large Language Models are increasingly being used for various tasks including content generation and as chatbots. Despite their impressive performances in general tasks, LLMs need to be aligned when applying for domain specific tasks to mitigate the problems of hallucination and producing harmful answers. Retrieval Augmented Generation (RAG) allows to easily attach and manipulate a non-parametric knowledgebases to LLMs. Applications of RAG in the field of medical education are discussed in this paper. A combined extractive and abstractive summarization method for large unstructured textual data using representative vectors is proposed.
    
[^20]: 美国城市可持续发展的卫星图像数据集

    A Satellite Imagery Dataset for Long-Term Sustainable Development in United States Cities. (arXiv:2308.00465v1 [cs.CV])

    [http://arxiv.org/abs/2308.00465](http://arxiv.org/abs/2308.00465)

    该论文开发了一个卫星图像数据集，覆盖了美国最多人口的100个城市以及相应的人口普查区块群，时间跨度为2014年至2023年。这个数据集可用于研究美国城市的可持续发展目标，特别是与贫困、健康相关的SDG指标。

    

    城市在实现可持续发展目标（SDG）和促进经济增长、满足社会需求方面起着重要作用。尤其是卫星图像是研究可持续城市发展的潜在数据源。然而，在美国缺乏一个涵盖多个城市、多年份、多尺度和多个SDG指标的综合数据集，用于监测SDG。为了支持对美国城市SDG的研究，我们使用深度学习模型开发了一个卫星图像数据集，包含25个可持续发展指标的五个SDG。所提出的数据集涵盖了美国人口最多的100个城市和相应的人口普查区块群，时间跨度为2014年至2023年。具体而言，我们收集卫星图像，并使用最先进的目标检测和语义分割模型识别城市的鸟瞰图。我们还收集了人口、夜间光线、调查和建筑环境数据，描述了与贫困、健康相关的SDG指标。

    Cities play an important role in achieving sustainable development goals (SDGs) to promote economic growth and meet social needs. Especially satellite imagery is a potential data source for studying sustainable urban development. However, a comprehensive dataset in the United States (U.S.) covering multiple cities, multiple years, multiple scales, and multiple indicators for SDG monitoring is lacking. To support the research on SDGs in U.S. cities, we develop a satellite imagery dataset using deep learning models for five SDGs containing 25 sustainable development indicators. The proposed dataset covers the 100 most populated U.S. cities and corresponding Census Block Groups from 2014 to 2023. Specifically, we collect satellite imagery and identify objects with state-of-the-art object detection and semantic segmentation models to observe cities' bird's-eye view. We further gather population, nighttime light, survey, and built environment data to depict SDGs regarding poverty, health, e
    
[^21]: DMFC-GraspNet: 多指机器人在杂乱场景中可微的抓取生成

    DMFC-GraspNet: Differentiable Multi-Fingered Robotic Grasp Generation in Cluttered Scenes. (arXiv:2308.00456v1 [cs.RO])

    [http://arxiv.org/abs/2308.00456](http://arxiv.org/abs/2308.00456)

    本文提出了DMFC-GraspNet，在多指机器人抓取生成领域做出了两个主要贡献：一是提出了可微的多指抓取规划方法，实现了多样化和稠密的抓取预测；二是开发了一种稠密标注方法，使得多指机器人手与真实抓取密切关联。结果表明了该方法的有效性。

    

    机器人抓取是机器人操作中必备的基本技能。模仿人手结构的多指机器人手可以进行复杂的物体操作。然而，目前的多指机器人抓取技术通常在每次推理中只能预测一次抓取，限制了其多样性和效率。本文提出了一种可微的多指抓取生成网络（DMFC-GraspNet），针对这一挑战做出了两个主要贡献。首先，提出了一种新颖的神经抓取规划器，预测了一种新的抓取表示方法，实现了多样化而稠密的抓取预测。其次，开发了一种场景创建和标签映射方法，用于多指机器人手的稠密标注，实现了与真实抓取的密切关联。通过仿真研究对所提出的方法进行了评估，并与现有方法进行了比较。结果表明了该方法的有效性。

    Robotic grasping is a fundamental skill required for object manipulation in robotics. Multi-fingered robotic hands, which mimic the structure of the human hand, can potentially perform complex object manipulations. Nevertheless, current techniques for multi-fingered robotic grasping frequently predict only a single grasp for each inference time, limiting their versatility and efficiency. This paper proposes a differentiable multi-fingered grasp generation network (DMFC-GraspNet) with two main contributions to address this challenge. Firstly, a novel neural grasp planner is proposed, which predicts a new grasp representation to enable versatile and dense grasp predictions. Secondly, a scene creation and label mapping method is developed for dense labeling of multi-fingered robotic hands, which allows a dense association of ground truth grasps. The proposed approach is evaluated through simulation studies and compared to existing approaches. The results demonstrate the effectiveness of t
    
[^22]: 大型语言模型的工具结构嵌入

    Structural Embeddings of Tools for Large Language Models. (arXiv:2308.00447v1 [cs.AI])

    [http://arxiv.org/abs/2308.00447](http://arxiv.org/abs/2308.00447)

    这篇论文突出了大型语言模型（LLM）与外部工具之间基于图的交互方法的重要性，并提出了一个指导与LLM集成大量外部工具的框架。

    

    显而易见，当前大型语言模型(LLMs)的状态需要引入外部工具。已经有大量文献记录了其缺乏直接的代数和逻辑推理，并促使研究人员开发了允许LLMs通过外部工具运行的框架。特定任务的工具利用的本体性质可以用有向无环图(DAG)很好地描述。本文的核心目标是突出强调在不久的将来，基于图的方法对LLM-工具交互的重要性。我们提出了一个示范性框架，用于指导指数级增加的外部工具与LLMs的编排，其中工具的目标和功能以图形方式进行层次结构编码。假设作为定义在这里的工具，思维链(CoT)的文本片段可以被想象为一种工具，那么基于图的框架也可以在这个特定方向上开辟新的途径。

    It is evident that the current state of Large Language Models (LLMs) necessitates the incorporation of external tools. The lack of straightforward algebraic and logical reasoning is well documented and prompted researchers to develop frameworks which allow LLMs to operate via external tools. The ontological nature of tool utilization for a specific task can be well formulated with a Directed Acyclic Graph (DAG). The central aim of the paper is to highlight the importance of graph based approaches to LLM-tool interaction in near future. We propose an exemplary framework to guide the orchestration of exponentially increasing numbers of external tools with LLMs,where objectives and functionalities of tools are graph encoded hierarchically. Assuming that textual segments of a Chain-of-Thought (CoT) can be imagined as a tool as defined here, the graph based framework can pave new avenues in that particular direction as well.
    
[^23]: SelfCheck: 使用LLMs自检其逐步推理的创新

    SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])

    [http://arxiv.org/abs/2308.00436](http://arxiv.org/abs/2308.00436)

    本论文研究了使用LLMs自检逐步推理的能力，提出了一种零-shot验证方案，成功识别错误并提高了问答性能。

    

    最近大型语言模型（LLMs）的进展，尤其是链式思维（CoT）的发明，使得解决推理问题成为可能。然而，即使最强大的LLMs仍然难以处理需要非线性思维和多步推理的复杂问题。在这项工作中，我们探讨了LLMs是否具有识别自己错误的能力，而无需依赖外部资源。具体而言，我们研究了它们是否可以用于识别逐步推理中的个别错误。为此，我们提出了一种零-shot验证方案以识别此类错误。然后，我们使用此验证方案来改进问答性能，通过对不同生成的答案进行加权投票。我们在三个数学数据集-GSM8K，MathQA和MATH上测试了该方法，并发现它成功识别错误，并进而提高了最终的预测性能。

    The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
    
[^24]: 基于补丁化自编码器的视觉异常检测

    Patch-wise Auto-Encoder for Visual Anomaly Detection. (arXiv:2308.00429v1 [cs.CV])

    [http://arxiv.org/abs/2308.00429](http://arxiv.org/abs/2308.00429)

    本论文提出了一种新颖的补丁化自编码器（Patch AE）框架来增强自编码器对异常的重构能力，并在Mvtec AD基准测试中取得了最先进的表现，具有在实际工业应用场景中的潜力。

    

    在没有异常先验的情况下进行异常检测是具有挑战性的。在无监督异常检测领域，传统的自编码器（AE）在仅通过正常图像进行训练时倾向于失败，因为模型将无法正确重构异常图像。相反，我们提出了一种新颖的补丁化自编码器（Patch AE）框架，旨在增强AE对异常的重构能力而不是削弱它。图像的每个补丁都通过相应的空间分布特征向量的学习特征表示进行重构，即补丁化重构，这确保了AE对异常的敏感性。我们的方法简单高效。它在Mvtec AD基准测试中取得了最先进的表现，证明了我们模型的有效性。它在实际工业应用场景中具有巨大潜力。

    Anomaly detection without priors of the anomalies is challenging. In the field of unsupervised anomaly detection, traditional auto-encoder (AE) tends to fail based on the assumption that by training only on normal images, the model will not be able to reconstruct abnormal images correctly. On the contrary, we propose a novel patch-wise auto-encoder (Patch AE) framework, which aims at enhancing the reconstruction ability of AE to anomalies instead of weakening it. Each patch of image is reconstructed by corresponding spatially distributed feature vector of the learned feature representation, i.e., patch-wise reconstruction, which ensures anomaly-sensitivity of AE. Our method is simple and efficient. It advances the state-of-the-art performances on Mvtec AD benchmark, which proves the effectiveness of our model. It shows great potential in practical industrial application scenarios.
    
[^25]: 具有物理声场先验知识的生成对抗网络

    Generative adversarial networks with physical sound field priors. (arXiv:2308.00426v1 [eess.AS])

    [http://arxiv.org/abs/2308.00426](http://arxiv.org/abs/2308.00426)

    本文提出了一种基于生成对抗网络（GANs）的深度学习方法，利用物理声场先验知识准确重建声场，实现了较好的精度和能量保留，特别适用于高频范围和超出测量区域的外推情况。与最先进的方法相比，该方法能够处理不同数量和配置的测量位置，为声场重建提供了有前景的方法。

    

    本文提出了一种基于生成对抗网络（GANs）的深度学习方法，用于声场的时空重建。该方法利用平面波基函数，并学习了房间内压力的概率分布，能够准确地从有限数量的测量中重建声场。通过使用两个已建立的数据集对该方法的性能进行评估，并与最先进的方法进行比较。结果表明，该模型能够在精度和能量保留方面实现更好的重建性能，特别是在高频范围和超出测量区域的外推情况下。此外，所提出的方法可以处理不同数量和配置的测量位置，而不会降低性能。结果表明，这种基于生成模型的声场重建方法提供了一种有前景的方法，允许引入物理先验知识。

    This paper presents a deep learning-based approach for the spatio-temporal reconstruction of sound fields using Generative Adversarial Networks (GANs). The method utilises a plane wave basis and learns the underlying statistical distributions of pressure in rooms to accurately reconstruct sound fields from a limited number of measurements. The performance of the method is evaluated using two established datasets and compared to state-of-the-art methods. The results show that the model is able to achieve an improved reconstruction performance in terms of accuracy and energy retention, particularly in the high-frequency range and when extrapolating beyond the measurement region. Furthermore, the proposed method can handle a varying number of measurement positions and configurations without sacrificing performance. The results suggest that this approach provides a promising approach to sound field reconstruction using generative models that allow for a physically informed prior to acousti
    
[^26]: 从复杂句子到链接命题的话语感知文本简化

    Discourse-Aware Text Simplification: From Complex Sentences to Linked Propositions. (arXiv:2308.00425v1 [cs.CL])

    [http://arxiv.org/abs/2308.00425](http://arxiv.org/abs/2308.00425)

    本研究提出了一种话语感知的文本简化方法，通过在句子的语义上下文中拆分和重新表达复杂的英语句子，解决了现有句法文本简化方法的保守性和忽略上下文连贯性的问题。

    

    对于下游自然语言处理应用来说，复杂句子是其中一个重要的障碍，句子的长度和复杂性会导致预测质量下降。文本简化的任务就是为了使句子更容易处理而对其进行修改，使用一系列的重写操作，比如重新排序、删除或拆分。现有的句法文本简化方法存在两个主要缺点：一方面，它们采取了非常保守的方法，倾向于保留输入而不进行转换；另一方面，它们忽略了文本的连贯性，需要跨越从句或句子的上下文来推断语句的真正含义。为了解决这些问题，我们提出了一种话语感知的文本简化方法，在句子所处的语义上下文中拆分和重新表达复杂的英语句子。基于一个基于语言学的转换阶段，该阶段利用从句和命题之间的关系来完成简化操作。

    Sentences that present a complex syntax act as a major stumbling block for downstream Natural Language Processing applications whose predictive quality deteriorates with sentence length and complexity. The task of Text Simplification (TS) may remedy this situation. It aims to modify sentences in order to make them easier to process, using a set of rewriting operations, such as reordering, deletion, or splitting. State-of-the-art syntactic TS approaches suffer from two major drawbacks: first, they follow a very conservative approach in that they tend to retain the input rather than transforming it, and second, they ignore the cohesive nature of texts, where context spread across clauses or sentences is needed to infer the true meaning of a statement. To address these problems, we present a discourse-aware TS approach that splits and rephrases complex English sentences within the semantic context in which they occur. Based on a linguistically grounded transformation stage that uses claus
    
[^27]: 挑战图协同过滤的神话：一项基于推理和可复制性的分析

    Challenging the Myth of Graph Collaborative Filtering: a Reasoned and Reproducibility-driven Analysis. (arXiv:2308.00404v1 [cs.IR])

    [http://arxiv.org/abs/2308.00404](http://arxiv.org/abs/2308.00404)

    本文研究挑战图协同过滤的神话，通过关注结果的可复制性，成功复制了六个流行的图推荐模型在几个常见和新数据集上的结果，并与传统协同过滤模型进行比较。

    

    图神经网络模型（GNNs）的成功显著推动了推荐系统的发展，通过将用户和物品有效地建模为一个二分图和无向图。然而，许多原始的基于图的作品通常在未验证其在具体配置下的有效性的情况下采用基线论文的结果。我们的工作解决了这个问题，着重关注结果的可复制性。我们提出了一种成功复制了六个流行且最新的图推荐模型（NGCF、DGCF、LightGCN、SGL、UltraGCN和GFCF）在三个常见基准数据集（Gowalla、Yelp 2018和亚马逊图书）上的结果的代码。此外，我们将这些图模型与在离线评估中表现良好的传统协同过滤模型进行了比较。此外，我们还扩展了对两个缺乏现有文献中已建立设置的新数据集（Allrecipes和BookCrossing）的研究。由于在这些数据集上的性能与以前的基准数据集不同，使得我们对图推荐模型性能的评估结果更加深入和全面。

    The success of graph neural network-based models (GNNs) has significantly advanced recommender systems by effectively modeling users and items as a bipartite, undirected graph. However, many original graph-based works often adopt results from baseline papers without verifying their validity for the specific configuration under analysis. Our work addresses this issue by focusing on the replicability of results. We present a code that successfully replicates results from six popular and recent graph recommendation models (NGCF, DGCF, LightGCN, SGL, UltraGCN, and GFCF) on three common benchmark datasets (Gowalla, Yelp 2018, and Amazon Book). Additionally, we compare these graph models with traditional collaborative filtering models that historically performed well in offline evaluations. Furthermore, we extend our study to two new datasets (Allrecipes and BookCrossing) that lack established setups in existing literature. As the performance on these datasets differs from the previous bench
    
[^28]: 交通流量预测的反事实图变压器模型

    Counterfactual Graph Transformer for Traffic Flow Prediction. (arXiv:2308.00391v1 [cs.LG])

    [http://arxiv.org/abs/2308.00391](http://arxiv.org/abs/2308.00391)

    本研究提出了一种针对交通流量预测的反事实图变压器模型（CGT），通过实例级解释器和扰动掩码生成器，可以获得具有解释性的预测模型和重要子图。

    

    交通流量预测（TFP）是智能交通系统（ITS）的一个基本问题，它模拟了交通流量的潜在时空依赖关系，用于潜在拥堵预测。最近的基于图的模型采用了多种注意机制，取得了有希望的性能。然而，现有的交通流量预测方法往往会从数据集中继承偏差模式，并缺乏可解释性。为此，我们提出了一种专门为TFP设计的反事实图变压器（CGT）模型，该模型具有实例级解释器（例如，寻找重要子图）。我们设计了一个对输入传感器特征在时间维度上和图变压器模块上的图结构进行扰动的生成器，以获取空间和时间上的反事实解释。通过搜索输入数据特征和图结构上的最佳扰动掩码，我们可以获取简洁而主导的数据或图边链接，供后续使用。

    Traffic flow prediction (TFP) is a fundamental problem of the Intelligent Transportation System (ITS), as it models the latent spatial-temporal dependency of traffic flow for potential congestion prediction. Recent graph-based models with multiple kinds of attention mechanisms have achieved promising performance. However, existing methods for traffic flow prediction tend to inherit the bias pattern from the dataset and lack interpretability. To this end, we propose a Counterfactual Graph Transformer (CGT) model with an instance-level explainer (e.g., finding the important subgraphs) specifically designed for TFP. We design a perturbation mask generator over input sensor features at the time dimension and the graph structure on the graph transformer module to obtain spatial and temporal counterfactual explanations. By searching the optimal perturbation masks on the input data feature and graph structures, we can obtain the concise and dominant data or graph edge links for the subsequent
    
[^29]: 基于人工智能的双有源桥变换器的三相移位调制，以最小化电流应力

    Artificial-Intelligence-Based Triple Phase Shift Modulation for Dual Active Bridge Converter with Minimized Current Stress. (arXiv:2308.00382v1 [eess.SY])

    [http://arxiv.org/abs/2308.00382](http://arxiv.org/abs/2308.00382)

    本论文提出了一种基于人工智能的三相移位调制技术，用于双有源桥变换器，以最小化电流应力。这种调制技术可以提高DAB变换器的功率效率和功率密度。

    

    在许多应用中，双有源桥（DAB）变换器因其出色的功率密度和双向功率传输能力而受到欢迎。到目前为止，三相移位（TPS）可以被认为是DAB变换器最先进的调制技术之一。它可以扩大零电压开关范围并显著提高功率效率。当前，当DAB变换器在TPS调制下时，电流应力已成为一个重要的性能指标。然而，要在DAB变换器在TPS调制下最小化电流应力，分析过程和实现过程中存在两个困难。首先，TPS调制中的三个调制变量对不同操作模式下的电流应力分析带来了挑战。这个分析和推导过程导致了重计算负荷，并且受到低精度的困扰。其次，要实现TPS调制，如果使用查找表，则需要较大的存储器。

    The dual active bridge (DAB) converter has been popular in many applications for its outstanding power density and bidirectional power transfer capacity. Up to now, triple phase shift (TPS) can be considered as one of the most advanced modulation techniques for DAB converter. It can widen zero voltage switching range and improve power efficiency significantly. Currently, current stress of the DAB converter has been an important performance indicator when TPS modulation is applied for smaller size and higher efficiency. However, to minimize the current stress when the DAB converter is under TPS modulation, two difficulties exist in analysis process and realization process, respectively. Firstly, three degrees of modulation variables in TPS modulation bring challenges to the analysis of current stress in different operating modes. This analysis and deduction process leads to heavy computational burden and also suffers from low accuracy. Secondly, to realize TPS modulation, if a lookup ta
    
[^30]: 基于人工智能的混合扩展相移调制对双有源桥变换器的全零电压开关范围和最优效率进行协同控制

    Artificial-Intelligence-Based Hybrid Extended Phase Shift Modulation for the Dual Active Bridge Converter with Full ZVS Range and Optimal Efficiency. (arXiv:2308.00381v1 [eess.SY])

    [http://arxiv.org/abs/2308.00381](http://arxiv.org/abs/2308.00381)

    本文提出了一种基于人工智能的混合扩展相移调制方法，以实现双有源桥变换器的全零电压开关范围和最优效率。

    

    双有源桥（DAB）变换器是许多流行应用领域（如无线充电、电动汽车和可再生能源）的关键支持技术。DAB变换器的零电压开关范围和效率是两个重要的性能指标。为了达到所需的零电压开关范围和效率，调制必须被精心设计。混合调制考虑了多种单一调制策略，以实现良好的综合性能。传统上，为了设计混合调制，使用谐波方法或分段方法，但它们都存在模型建立过程耗时和不准确的问题。因此，提出了一种基于人工智能的混合扩展相移（HEPS）调制。一般来说，HEPS调制是以自动化方式开发的，它减轻了繁琐的模型建立过程，同时保持了高模型精度。在HEPS调制中，考虑了两种EPS策略，以实现在整个工作范围内的全零电压开关操作和最优效率。

    Dual active bridge (DAB) converter is the key enabler in many popular applications such as wireless charging, electric vehicle and renewable energy. ZVS range and efficiency are two significant performance indicators for DAB converter. To obtain the desired ZVS and efficiency performance, modulation should be carefully designed. Hybrid modulation considers several single modulation strategies to achieve good comprehensive performance. Conventionally, to design a hybrid modulation, harmonic approach or piecewise approach is used, but they suffer from time-consuming model building process and inaccuracy. Therefore, an artificial-intelligence-based hybrid extended phase shift (HEPS) modulation is proposed. Generally, the HEPS modulation is developed in an automated fashion, which alleviates cumbersome model building process while keeping high model accuracy. In HEPS modulation, two EPS strategies are considered to realize optimal efficiency with full ZVS operation over entire operating ra
    
[^31]: 带有不确定区域预测的形状完成

    Shape Completion with Prediction of Uncertain Regions. (arXiv:2308.00377v1 [cs.CV])

    [http://arxiv.org/abs/2308.00377](http://arxiv.org/abs/2308.00377)

    该论文提出了两种方法来处理在给定模糊物体视图时可能存在的物体部分的不确定区域预测问题。研究表明这些方法可以作为任何预测空间占用的方法的直接扩展，通过后处理占用评分或直接预测不确定性指标来实现。这些方法与已知的概率形状完成方法进行了比较，并使用自动生成的深度图像数据集进行了验证。

    

    形状完成，即从部分观测预测物体的完整几何形状，对于几个下游任务非常重要，尤其是机器人操作。当基于物体形状重建进行规划或实际抓取的预测时，指示严重几何不确定性是必不可少的。特别是在给定模糊的物体视图时，在整个物体部分存在 irreducible uncertainty 的扩展区域。为了处理这种重要情况，我们提出了两种新方法来预测这些不确定区域，这两种方法都可以作为预测局部空间占用的任何方法的直接扩展，一种是通过后处理占用评分，另一种是通过直接预测不确定性指标。我们将这些方法与两种已知的概率形状完成方法进行了比较。此外，我们还生成了一个基于ShapeNet的数据集，其中包含了真实渲染的物体视图深度图像及其带有地面真值标注。

    Shape completion, i.e., predicting the complete geometry of an object from a partial observation, is highly relevant for several downstream tasks, most notably robotic manipulation. When basing planning or prediction of real grasps on object shape reconstruction, an indication of severe geometric uncertainty is indispensable. In particular, there can be an irreducible uncertainty in extended regions about the presence of entire object parts when given ambiguous object views. To treat this important case, we propose two novel methods for predicting such uncertain regions as straightforward extensions of any method for predicting local spatial occupancy, one through postprocessing occupancy scores, the other through direct prediction of an uncertainty indicator. We compare these methods together with two known approaches to probabilistic shape completion. Moreover, we generate a dataset, derived from ShapeNet, of realistically rendered depth images of object views with ground-truth annot
    
[^32]: Fountain --一个智能上下文助手，结合知识表示和语言模型，用于制造风险识别

    Fountain -- an intelligent contextual assistant combining knowledge representation and language models for manufacturing risk identification. (arXiv:2308.00364v1 [cs.CL])

    [http://arxiv.org/abs/2308.00364](http://arxiv.org/abs/2308.00364)

    Fountain是一个智能上下文助手，将知识表示和语言模型结合，用于制造风险识别。它通过描述现有设计和流程准则以及提出的偏差来帮助识别风险，并提供可解释和一致的建议。

    

    在大规模生产过程中，与批准的设计或流程偏离会导致意想不到的风险。然而，这些变化有时是必要的，因为产品设计特征或制造过程的适应性发生了变化。一个主要挑战是在工作流程的早期阶段识别这些风险，以避免导致保修索赔的故障。我们开发了Fountain作为一个上下文助手，集成在偏差管理工作流程中，通过对现有设计和流程准则以及提出的偏差的描述来帮助识别风险。在制造环境中，该助手提供的建议必须是可解释和一致的。我们通过以下两个组件的结合实现了这一点：1）为特定领域语义相似性微调的语言模型，和2）以物料清单、失效模式和效应分析为基础的属性图的知识表示。

    Deviations from the approved design or processes during mass production can lead to unforeseen risks. However, these changes are sometimes necessary due to changes in the product design characteristics or an adaptation in the manufacturing process. A major challenge is to identify these risks early in the workflow so that failures leading to warranty claims can be avoided. We developed Fountain as a contextual assistant integrated in the deviation management workflow that helps in identifying the risks based on the description of the existing design and process criteria and the proposed deviation. In the manufacturing context, it is important that the assistant provides recommendations that are explainable and consistent. We achieve this through a combination of the following two components 1) language models finetuned for domain specific semantic similarity and, 2) knowledge representation in the form of a property graph derived from the bill of materials, Failure Modes and Effect Ana
    
[^33]: MetaGPT: 元编程用于多智能体协作框架

    MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. (arXiv:2308.00352v1 [cs.AI])

    [http://arxiv.org/abs/2308.00352](http://arxiv.org/abs/2308.00352)

    MetaGPT是一个用于多智能体协作的创新框架，将有效的人工工作流引入到大型语言模型驱动的协作中。它采用元编程方法，将标准操作规程编码为提示，促进结构化协调，并要求模块化输出，赋予智能体领域专业知识，以验证输出并减少错误。这种框架利用了流水线工作模式来分配任务。

    

    最近，在多个大型语言模型驱动的智能体协作中，自动任务解决取得了显著进展。然而，现有的工作主要集中在简单任务上，缺乏对复杂任务的探索和研究，主要是由于幻觉问题。这种幻觉在多个智能体相互作用时被无限放大，导致在解决复杂问题时失败。因此，我们引入了MetaGPT，这是一个创新的框架，在LLM驱动的多智能体协作中采用有效的人工工作流作为元编程方法。具体而言，MetaGPT首先将标准操作规程（SOPs）编码为提示，促进结构化协调。然后，它进一步要求模块化输出，赋予智能体领域专业知识，与人类专业人员平行验证输出并减少错误。通过这种方式，MetaGPT利用流水线工作模式来分配任务

    Recently, remarkable progress has been made in automated task-solving through the use of multi-agents driven by large language models (LLMs). However, existing works primarily focuses on simple tasks lacking exploration and investigation in complicated tasks mainly due to the hallucination problem. This kind of hallucination gets amplified infinitely as multiple intelligent agents interact with each other, resulting in failures when tackling complicated problems.Therefore, we introduce MetaGPT, an innovative framework that infuses effective human workflows as a meta programming approach into LLM-driven multi-agent collaboration. In particular, MetaGPT first encodes Standardized Operating Procedures (SOPs) into prompts, fostering structured coordination. And then, it further mandates modular outputs, bestowing agents with domain expertise paralleling human professionals to validate outputs and reduce compounded errors. In this way, MetaGPT leverages the assembly line work model to assig
    
[^34]: 使用低秩近似有效地学习Green函数

    Learning Green's Function Efficiently Using Low-Rank Approximations. (arXiv:2308.00350v1 [cs.LG])

    [http://arxiv.org/abs/2308.00350](http://arxiv.org/abs/2308.00350)

    该论文提出了一种使用低秩分解学习Green函数的方法，通过使用领域数据和蒙特卡洛样本进行分别学习和积分逼近，从而提高了计算效率并保持了准确性。

    

    使用深度学习模型学习Green函数可以解决不同类型的偏微分方程。使用深度学习学习Green函数的一个实际限制是重复的计算密集的蒙特卡洛积分逼近。我们提出一种通过低秩分解学习Green函数的方法，该方法通过使用领域数据进行评估和蒙特卡洛样本进行积分逼近，从而消除了冗余计算。通过实验证明，与MOD-Net相比，所提出的方法提高了计算时间，同时在准确性上与PINNs和MOD-Net相当。

    Learning the Green's function using deep learning models enables to solve different classes of partial differential equations. A practical limitation of using deep learning for the Green's function is the repeated computationally expensive Monte-Carlo integral approximations. We propose to learn the Green's function by low-rank decomposition, which results in a novel architecture to remove redundant computations by separate learning with domain data for evaluation and Monte-Carlo samples for integral approximation. Using experiments we show that the proposed method improves computational time compared to MOD-Net while achieving comparable accuracy compared to both PINNs and MOD-Net.
    
[^35]: 基于深度神经网络不确定性估计的动态集成选择方法用于对抗鲁棒性

    Dynamic ensemble selection based on Deep Neural Network Uncertainty Estimation for Adversarial Robustness. (arXiv:2308.00346v1 [cs.LG])

    [http://arxiv.org/abs/2308.00346](http://arxiv.org/abs/2308.00346)

    本文针对深度神经网络在实际应用中的鲁棒性问题，提出了一种通过动态集成选择技术在模型层面上改善模型的防御主动性的方法。通过在训练阶段引入Dirichlet分布作为子模型预测分布的先验，并在轻量级子模型下引入参数空间的多样性约束，构建备选的集成模型空间。在测试阶段，动态选取特定的子模型来提高模型的鲁棒性。

    

    深度神经网络在图像识别方面取得了显著的效果，但在实际应用中面临着大量数据不确定性的识别鲁棒性薄弱的问题。这种不确定性是由环境中不可避免的噪声以及可能的对抗性攻击引起的。动态方法可以有效地改善对抗性示例攻防竞赛中的防御主动性。与以前依赖于输入或决策的动态方法不同，本研究通过动态集成选择技术在模型层面上探索动态属性，以进一步保护模型免受白盒攻击并提高鲁棒性。具体而言，在训练阶段，将Dirichlet分布作为子模型预测分布的先验，并在轻量级子模型下引入参数空间的多样性约束，以构建备选集成模型空间。在测试阶段，特定的子模型被动态选取

    The deep neural network has attained significant efficiency in image recognition. However, it has vulnerable recognition robustness under extensive data uncertainty in practical applications. The uncertainty is attributed to the inevitable ambient noise and, more importantly, the possible adversarial attack. Dynamic methods can effectively improve the defense initiative in the arms race of attack and defense of adversarial examples. Different from the previous dynamic method depend on input or decision, this work explore the dynamic attributes in model level through dynamic ensemble selection technology to further protect the model from white-box attacks and improve the robustness. Specifically, in training phase the Dirichlet distribution is apply as prior of sub-models' predictive distribution, and the diversity constraint in parameter space is introduced under the lightweight sub-models to construct alternative ensembel model spaces. In test phase, the certain sub-models are dynamic
    
[^36]: 利用优化的飞行对抗贴片绑架基于深度学习的多旋翼

    Kidnapping Deep Learning-based Multirotors using Optimized Flying Adversarial Patches. (arXiv:2308.00344v1 [cs.RO])

    [http://arxiv.org/abs/2308.00344](http://arxiv.org/abs/2308.00344)

    本研究介绍了利用优化的飞行对抗贴片来绑架基于深度学习的多旋翼的方法，并展示了这些方法在对抗贴片数量增加时的良好扩展性。

    

    自主飞行机器人，例如多旋翼，通常依赖于基于相机图像的深度学习模型进行预测，例如姿态估计。如果将这些模型应用于训练域之外的输入图像，它们可能会产生意想不到的结果。对抗性攻击可以利用这个缺点，例如通过计算小图片，即所谓的对抗贴片，在环境中放置以操纵神经网络的预测。我们引入飞行对抗贴片，通过将多个图片安装在至少一个其他飞行机器人上，因此可以放置在受害多旋翼的视野范围内的任何位置。通过引入攻击者机器人，我们将系统扩展为对抗性多机器人系统。为了实施有效的攻击，我们比较了三种同时优化多个对抗贴片及其在输入图像中位置的方法。我们证明我们的方法在对抗贴片数量增加时具有良好的扩展性。此外，我们还展示了...

    Autonomous flying robots, such as multirotors, often rely on deep learning models that makes predictions based on a camera image, e.g. for pose estimation. These models can predict surprising results if applied to input images outside the training domain. This fault can be exploited by adversarial attacks, for example, by computing small images, so-called adversarial patches, that can be placed in the environment to manipulate the neural network's prediction. We introduce flying adversarial patches, where multiple images are mounted on at least one other flying robot and therefore can be placed anywhere in the field of view of a victim multirotor. By introducing the attacker robots, the system is extended to an adversarial multi-robot system. For an effective attack, we compare three methods that simultaneously optimize multiple adversarial patches and their position in the input image. We show that our methods scale well with the number of adversarial patches. Moreover, we demonstrate
    
[^37]: 在部分观测条件下监控算法公平性

    Monitoring Algorithmic Fairness under Partial Observations. (arXiv:2308.00341v1 [cs.AI])

    [http://arxiv.org/abs/2308.00341](http://arxiv.org/abs/2308.00341)

    本研究将算法公平性监控扩展到部分观察到的马尔可夫链模型的系统，并且可以监控包含对事件序列上数值函数的期望值的算术表达式的公平性属性。

    

    随着人工智能和机器学习软件在决策中的应用越来越多地影响人类，它们在决策过程中保持公平和无偏的要求变得至关重要。为了补充设计时的偏差缓解措施，近期引入了一些运行时验证技术来监控部署系统的算法公平性。之前的监控技术假设对（未知的）被监控系统的状态具有完全可观测性。此外，它们只能监控被指定为不同事件的概率的算术表达式的公平性属性。在这项工作中，我们将公平性监控扩展到部分观察到的马尔可夫链（POMC）模型的系统，并且针对包含对事件序列上的数值函数的期望值的算术表达式的规范。我们仅做出的假设是基础POMC是非周期性的并且起始于稳定分布，且对于其混合时间有一个已知的上界。

    As AI and machine-learned software are used increasingly for making decisions that affect humans, it is imperative that they remain fair and unbiased in their decisions. To complement design-time bias mitigation measures, runtime verification techniques have been introduced recently to monitor the algorithmic fairness of deployed systems. Previous monitoring techniques assume full observability of the states of the (unknown) monitored system. Moreover, they can monitor only fairness properties that are specified as arithmetic expressions over the probabilities of different events. In this work, we extend fairness monitoring to systems modeled as partially observed Markov chains (POMC), and to specifications containing arithmetic expressions over the expected values of numerical functions on event sequences. The only assumptions we make are that the underlying POMC is aperiodic and starts in the stationary distribution, with a bound on its mixing time being known. These assumptions enab
    
[^38]: 异构机器人系统中的目标搜索和导航与深度强化学习

    Target Search and Navigation in Heterogeneous Robot Systems with Deep Reinforcement Learning. (arXiv:2308.00331v1 [cs.RO])

    [http://arxiv.org/abs/2308.00331](http://arxiv.org/abs/2308.00331)

    本文设计了一个异构机器人系统，通过深度强化学习算法学习出的策略能够在迷宫般的矿井环境中搜索并导航到未知目标位置。引入多阶段强化学习框架和好奇模块加速了训练速度。

    

    协作的异构机器人系统可以大大提高目标搜索和导航任务的效率。本文设计了一个由无人机和无人车组成的异构机器人系统，用于在未知环境中进行搜索和救援任务。该系统能够通过深度强化学习算法学习出的策略在类似迷宫的矿井环境中搜索目标并导航到目标位置。在训练过程中，如果同时训练两个机器人，可能无法正确获得与协作相关的奖励。因此，我们引入了一个多阶段强化学习框架和一个好奇模块来鼓励智能体探索未访问的环境。在模拟环境中的实验表明，我们的框架能够训练出异构机器人系统实现未知目标位置的搜索和导航，而现有的基线方法可能无法做到，并且加速了训练速度。

    Collaborative heterogeneous robot systems can greatly improve the efficiency of target search and navigation tasks. In this paper, we design a heterogeneous robot system consisting of a UAV and a UGV for search and rescue missions in unknown environments. The system is able to search for targets and navigate to them in a maze-like mine environment with the policies learned through deep reinforcement learning algorithms. During the training process, if two robots are trained simultaneously, the rewards related to their collaboration may not be properly obtained. Hence, we introduce a multi-stage reinforcement learning framework and a curiosity module to encourage agents to explore unvisited environments. Experiments in simulation environments show that our framework can train the heterogeneous robot system to achieve the search and navigation with unknown target locations while existing baselines may not, and accelerate the training speed.
    
[^39]: 针对混合整数规划生成可行解的阈值感知学习

    Threshold-aware Learning to Generate Feasible Solutions for Mixed Integer Programs. (arXiv:2308.00327v1 [math.OC])

    [http://arxiv.org/abs/2308.00327](http://arxiv.org/abs/2308.00327)

    针对混合整数规划问题，本研究提出了一种阈值感知学习的方法，通过优化变量分配率来提高可行解的质量。

    

    在有限时间内找到高质量的组合优化问题的可行解是具有挑战性的，因为其具有离散的特性。最近，越来越多的机器学习方法被用来解决组合优化问题。神经网络下潜 (Neural diving，ND) 是一种学习方法，用于生成混合整数规划问题中的部分离散变量赋值。然而，ND的一个主要缺点是机器学习和混合整数规划目标之间存在较大差异，即变量值分类准确度与原始解的界的差异。我们的研究发现，特定范围的变量分配率（覆盖率）会产生高质量的可行解，我们建议优化覆盖率来弥补学习和混合整数规划目标之间的差距。因此，我们提出了一种事后方法和一种基于学习的方法来优化覆盖率。我们方法的一个关键思想是共同学习限制覆盖率搜索空间。

    Finding a high-quality feasible solution to a combinatorial optimization (CO) problem in a limited time is challenging due to its discrete nature. Recently, there has been an increasing number of machine learning (ML) methods for addressing CO problems. Neural diving (ND) is one of the learning-based approaches to generating partial discrete variable assignments in Mixed Integer Programs (MIP), a framework for modeling CO problems. However, a major drawback of ND is a large discrepancy between the ML and MIP objectives, i.e., variable value classification accuracy over primal bound. Our study investigates that a specific range of variable assignment rates (coverage) yields high-quality feasible solutions, where we suggest optimizing the coverage bridges the gap between the learning and MIP objectives. Consequently, we introduce a post-hoc method and a learning-based approach for optimizing the coverage. A key idea of our approach is to jointly learn to restrict the coverage search spac
    
[^40]: 像素到策略：用于内部和跨游戏强化学习的DQN编码器

    Pixel to policy: DQN Encoders for within & cross-game reinforcement learning. (arXiv:2308.00318v1 [cs.LG])

    [http://arxiv.org/abs/2308.00318](http://arxiv.org/abs/2308.00318)

    本文研究了在强化学习中利用共享结构的DQN编码器的性能。通过迁移学习，该模型在不同任务和环境中学习可转移的策略，实现了更高效的学习和更好的性能。在多个游戏环境中，该模型的平均回合奖励为46.16，在20,000次回合内甚至超过了人类水平表现。

    

    强化学习可以应用于各种任务和环境。许多这些环境具有相似的共享结构，可以利用这些结构提高其他任务上的RL性能。迁移学习可以利用这种共享结构，通过学习可在不同任务和环境之间转移的策略，可以更有效地学习并在各种任务上改进性能。本研究探讨并比较了从零开始训练的RL模型和迁移学习不同方法的性能。此外，该研究还探讨了在多个游戏环境中训练的模型的性能，旨在开发通用的游戏代理以及使用DQN对预训练的编码器进行迁移学习，并在相同或不同游戏上进行训练。我们的DQN模型在平均回合奖励上达到了46.16，甚至超过了人类水平表现，仅仅使用了20,000次回合。

    Reinforcement Learning can be applied to various tasks, and environments. Many of these environments have a similar shared structure, which can be exploited to improve RL performance on other tasks. Transfer learning can be used to take advantage of this shared structure, by learning policies that are transferable across different tasks and environments and can lead to more efficient learning as well as improved performance on a wide range of tasks. This work explores as well as compares the performance between RL models being trained from the scratch and on different approaches of transfer learning. Additionally, the study explores the performance of a model trained on multiple game environments, with the goal of developing a universal game-playing agent as well as transfer learning a pre-trained encoder using DQN, and training it on the same game or a different game. Our DQN model achieves a mean episode reward of 46.16 which even beats the human-level performance with merely 20k epi
    
[^41]: 让Text-VQA中的V变得重要

    Making the V in Text-VQA Matter. (arXiv:2308.00295v1 [cs.CV])

    [http://arxiv.org/abs/2308.00295](http://arxiv.org/abs/2308.00295)

    本论文针对文本VQA中对视觉特征理解不足的问题，提出通过学习视觉特征来解决这一问题，通过将TextVQA和VQA数据集相结合进行模型训练，从而提高模型的准确性。

    

    文本VQA旨在通过阅读图像中的文本来回答问题。与VQA任务相比，它需要大量的场景-文本关系理解。最近的研究表明，数据集中的问题-答案对更关注图像中的文本，而对于视觉特征则给予较少重视，而且有些问题不需要理解图像。由于缺乏对视觉上下文的理解，使用该数据集训练的模型会预测出有偏差的答案。例如，在类似“标牌上写着什么？”的问题中，模型预测的答案总是“STOP”，这使得模型忽略了图像。为了解决这些问题，我们提出了一种方法，通过使用VQA数据集作为Text-VQA的外部知识，学习视觉特征（让V在Text-VQA中变得重要），以及OCR特征和问题特征。具体来说，我们将TextVQA数据集和VQA数据集进行合并，并在这个合并的数据集上训练模型。

    Text-based VQA aims at answering questions by reading the text present in the images. It requires a large amount of scene-text relationship understanding compared to the VQA task. Recent studies have shown that the question-answer pairs in the dataset are more focused on the text present in the image but less importance is given to visual features and some questions do not require understanding the image. The models trained on this dataset predict biased answers due to the lack of understanding of visual context. For example, in questions like "What is written on the signboard?", the answer predicted by the model is always "STOP" which makes the model to ignore the image. To address these issues, we propose a method to learn visual features (making V matter in TextVQA) along with the OCR features and question features using VQA dataset as external knowledge for Text-based VQA. Specifically, we combine the TextVQA dataset and VQA dataset and train the model on this combined dataset. Suc
    
[^42]: 多模态多损失融合网络

    Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])

    [http://arxiv.org/abs/2308.00264](http://arxiv.org/abs/2308.00264)

    多模态多损失融合网络通过最佳选择和融合多个模态的特征，提高了情感检测的性能，并在多个数据集上实现了最先进的结果。这些研究结果表明了用于增强神经网络中情感检测的特征选择和融合方法的优化方向。

    

    在这项工作中，我们研究了跨多个模态选择和融合特征的最佳方法，并将其组合在神经网络中以改善情感检测。我们比较了不同的融合方法并且研究了多损失训练在多模态融合网络中的影响，从而确定了与子网性能相关的有用发现。我们最好的模型在三个数据集（CMU-MOSI、CMU-MOSEI和CH-SIMS）上实现了最先进的性能，并且在大多数指标上优于其他方法。我们发现，训练多模态特征可以改善单模态测试，并且基于数据集注释模式设计融合方法可以增强模型性能。这些结果表明了在神经网络中增强情感检测的优化特征选择和融合方法的发展方向。

    In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
    
[^43]: LGViT: 加速视觉Transformer的动态早期退出

    LGViT: Dynamic Early Exiting for Accelerating Vision Transformer. (arXiv:2308.00255v1 [cs.CV])

    [http://arxiv.org/abs/2308.00255](http://arxiv.org/abs/2308.00255)

    提出了一种用于加速视觉Transformer的早期退出框架LGViT，通过融合局部感知头部和全局聚合头部，以解决ViTs中早期退出方法应用带来性能严重下降的问题。

    

    最近，对于在资源有限的边缘设备上部署和加速强大的视觉Transformer（ViT）以提供多媒体服务的有效方法已成为吸引人的任务。尽管早期退出是加速推理的可行解决方案，但大部分研究专注于卷积神经网络（CNN）和自然语言处理（NLP）中的Transformer模型。此外，直接将早期退出方法应用于ViTs可能导致性能严重下降。为解决这一挑战，我们系统地研究了ViTs中早期退出的有效性，并指出浅层内部分类器中不足的特征表示和深层内部分类器中捕获目标语义信息的能力有限限制了这些方法的性能。然后，我们提出了一个通用ViTs的早期退出框架，称为LGViT，它融合了异构的退出头部，即局部感知头部和全局聚合头部，以更好地提高性能。

    Recently, the efficient deployment and acceleration of powerful vision transformers (ViTs) on resource-limited edge devices for providing multimedia services have become attractive tasks. Although early exiting is a feasible solution for accelerating inference, most works focus on convolutional neural networks (CNNs) and transformer models in natural language processing (NLP).Moreover, the direct application of early exiting methods to ViTs may result in substantial performance degradation. To tackle this challenge, we systematically investigate the efficacy of early exiting in ViTs and point out that the insufficient feature representations in shallow internal classifiers and the limited ability to capture target semantic information in deep internal classifiers restrict the performance of these methods. We then propose an early exiting framework for general ViTs termed LGViT, which incorporates heterogeneous exiting heads, namely, local perception head and global aggregation head, to
    
[^44]: 基于特征掩蔽自编码和情绪迁移学习的基于EEG的认知负荷分类

    EEG-based Cognitive Load Classification using Feature Masked Autoencoding and Emotion Transfer Learning. (arXiv:2308.00246v1 [cs.LG])

    [http://arxiv.org/abs/2308.00246](http://arxiv.org/abs/2308.00246)

    本文提出了一种基于EEG的认知负荷分类的新解决方案，利用特征掩蔽自编码和情绪迁移学习来进行模型训练和分类。实验结果表明，这种方法取得了良好的结果。

    

    认知负荷是完成任务所需的心理努力量，在性能和决策结果中起着重要作用，因此在各种敏感领域中分类和分析认知负荷至关重要。本文提出了一种利用脑电图（EEG）分类认知负荷的新解决方案。我们的模型采用了一种变压器架构，利用情绪和认知负荷之间的迁移学习。我们使用自监督掩蔽自编码在情绪相关的EEG数据集上进行预训练，并使用冻结权重和微调进行迁移学习，进行下游的认知负荷分类。为了评估我们的方法，我们进行了一系列实验，利用了两个公开可用的基于EEG的情绪数据集，即SEED和SEED-IV，进行预训练，同时我们使用CL-Drive数据集进行下游的认知负荷分类。我们实验的结果表明，我们提出的方法取得了良好的结果。

    Cognitive load, the amount of mental effort required for task completion, plays an important role in performance and decision-making outcomes, making its classification and analysis essential in various sensitive domains. In this paper, we present a new solution for the classification of cognitive load using electroencephalogram (EEG). Our model uses a transformer architecture employing transfer learning between emotions and cognitive load. We pre-train our model using self-supervised masked autoencoding on emotion-related EEG datasets and use transfer learning with both frozen weights and fine-tuning to perform downstream cognitive load classification. To evaluate our method, we carry out a series of experiments utilizing two publicly available EEG-based emotion datasets, namely SEED and SEED-IV, for pre-training, while we use the CL-Drive dataset for downstream cognitive load classification. The results of our experiments show that our proposed approach achieves strong results and ou
    
[^45]: 程序分析指南：与大型语言模型共进之旅

    The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models. (arXiv:2308.00245v1 [cs.SE])

    [http://arxiv.org/abs/2308.00245](http://arxiv.org/abs/2308.00245)

    本文详细研究了以使用前初始化错误为案例的大型语言模型辅助静态分析的开放空间，并开发了一种全自动代理程序LLift，该程序能够克服多个挑战，包括错误建模。

    

    静态分析是软件工程中广泛使用的一种技术，用于识别和减轻错误。然而，在精确性和可扩展性之间实现微妙平衡是一个重要的障碍。大型语言模型（LLM）提供了一个有希望的替代方案，由于最近的进展在理解、生成甚至调试代码方面展示出了显著能力。然而，错误的逻辑可能很复杂，需要复杂的推理和跨多个函数的大范围分析。因此，在这一点上，LLM更适合在辅助角色中与静态分析相补充使用。本文深入研究了以使用前初始化（UBI）错误为案例研究的LLM辅助静态分析的开放空间。为此，我们开发了LLift，一个完全自动化的代理程序，可以与静态分析工具和LLM进行交互。通过精心设计代理程序和提示，我们能够克服许多挑战，包括针对具体错误的建模，

    Static analysis is a widely used technique in software engineering for identifying and mitigating bugs. However, a significant hurdle lies in achieving a delicate balance between precision and scalability. Large Language Models (LLMs) offer a promising alternative, as recent advances demonstrate remarkable capabilities in comprehending, generating, and even debugging code. Yet, the logic of bugs can be complex and require sophisticated reasoning and a large analysis scope spanning multiple functions. Therefore, at this point, LLMs are better used in an assistive role to complement static analysis. In this paper, we take a deep dive into the open space of LLM-assisted static analysis, using use-before-initialization (UBI) bugs as a case study. To this end, we develop LLift, a fully automated agent that interfaces with both a static analysis tool and an LLM. By carefully designing the agent and the prompts, we are able to overcome a number of challenges, including bug-specific modeling, 
    
[^46]: Capsa: 用于量化深度神经网络风险的统一框架

    Capsa: A Unified Framework for Quantifying Risk in Deep Neural Networks. (arXiv:2308.00231v1 [cs.LG])

    [http://arxiv.org/abs/2308.00231](http://arxiv.org/abs/2308.00231)

    Capsa是一个统一框架，用于扩展具有风险感知的深度神经网络模型。它能够量化多种形式的风险，并将不同算法组合在一起以并行量化不同的风险指标。通过在复杂感知数据集上实现最先进的不确定性估计算法并进行基准测试，验证了capsa的有效性。capsa能够轻松组合aleatoric不确定性、epistemic不确定性和偏见估计能力

    

    大规模深度神经网络在复杂问题上表现出色，但也常常出现突然、意外且灾难性的失败，尤其是在具有挑战性的情况下。现有的为神经网络提供风险感知的算法复杂而临时。具体而言，这些方法需要进行重大的工程改变，通常只针对特定设置进行开发，并且很难组合在一起。我们在这里提出了capsa，一个用于扩展具有风险感知的模型的框架。Capsa提供了一种量化多种形式风险的方法，并将不同的算法组合在一起以并行量化不同的风险指标。我们通过在capsa框架中实现最先进的不确定性估计算法并在复杂感知数据集上进行基准测试，验证了capsa的有效性。我们展示了capsa轻松组合aleatoric不确定性、epistemic不确定性和偏见估计的能力

    The modern pervasiveness of large-scale deep neural networks (NNs) is driven by their extraordinary performance on complex problems but is also plagued by their sudden, unexpected, and often catastrophic failures, particularly on challenging scenarios. Existing algorithms that provide risk-awareness to NNs are complex and ad-hoc. Specifically, these methods require significant engineering changes, are often developed only for particular settings, and are not easily composable. Here we present capsa, a framework for extending models with risk-awareness. Capsa provides a methodology for quantifying multiple forms of risk and composing different algorithms together to quantify different risk metrics in parallel. We validate capsa by implementing state-of-the-art uncertainty estimation algorithms within the capsa framework and benchmarking them on complex perception datasets. We demonstrate capsa's ability to easily compose aleatoric uncertainty, epistemic uncertainty, and bias estimation 
    
[^47]: 利用生成式AI驱动的参数化建模和BIM进行建筑设计的实验

    Experiments on Generative AI-Powered Parametric Modeling and BIM for Architectural Design. (arXiv:2308.00227v1 [cs.HC])

    [http://arxiv.org/abs/2308.00227](http://arxiv.org/abs/2308.00227)

    本文介绍了一种利用生成式AI和参数化建模与BIM相结合的新型建筑设计框架，能够促进建筑师与AI的合作，并通过探索设计思路和生成创造性设计来提升设计过程的效率和协作性。

    

    本文介绍了一种新的建筑设计框架，利用了ChatGPT和Veras等生成式AI工具与参数化建模和建筑信息模型（BIM）相结合，以提升设计过程。研究实验了ChatGPT和生成式AI在三维建筑设计中的潜力，超越了其在文本和二维图像生成方面的应用。该框架促进了建筑师与AI之间的合作，便于快速探索设计思路，并产生与环境相关的创造性设计生成。通过将ChatGPT用于脚本编写和Veras用于生成设计思路，与广泛使用的参数化建模和BIM工具相结合，该框架为建筑师提供了一种直观而强大的方法来传达设计意图，从而实现更高效、创造性和协作性的设计过程。

    This paper introduces a new architectural design framework that utilizes generative AI tools including ChatGPT and Veras with parametric modeling and Building Information Modeling (BIM) to enhance the design process. The study experiments with the potential of ChatGPT and generative AI in 3D architectural design, extending beyond its use in text and 2D image generation. The proposed framework promotes collaboration between architects and AI, facilitating a quick exploration of design ideas and producing context-sensitive, creative design generation. By integrating ChatGPT for scripting and Veras for generating design ideas with widely used parametric modeling and BIM tools, the framework provides architects with an intuitive and powerful method to convey design intent, leading to more efficient, creative, and collaborative design processes.
    
[^48]: 被指导的偏见：经过指导调优的语言模型呈现出新兴的认知偏见

    Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias. (arXiv:2308.00225v1 [cs.CL])

    [http://arxiv.org/abs/2308.00225](http://arxiv.org/abs/2308.00225)

    这项研究发现，经过指导调优的语言模型呈现出新兴的认知偏见，这对于理解和开发更可靠和无偏的语言模型至关重要。

    

    最近的研究表明，指导调优和从人类反馈中学习可以显著提高大语言模型（LMs）的能力。虽然这些调优方法可以使模型生成高质量的文本，但我们推测这些经过调优的模型可能会产生更多隐含的认知偏见。我们的研究提供了证据，表明这些经过调优的模型呈现出先前预训练模型中不存在或较不明显的偏见。我们对三种认知偏见进行了研究，包括矛盾效应、确定性效应和信念偏见，这些偏见已被证实对人类的决策和推理有影响。我们的研究结果突显了这些偏见在各种模型中的存在，特别是那些经过指导调优的模型，如Flan-T5、GPT3.5和GPT4。这项研究对于理解指导调优的LMs中的认知偏见是至关重要的，这有助于开发更可靠和无偏的语言模型。

    Recent studies show that instruction tuning and learning from human feedback improve the abilities of large language models (LMs) dramatically. While these tuning methods can make models generate high-quality text, we conjecture that more implicit cognitive biases may arise in these fine-tuned models. Our work provides evidence that these fine-tuned models exhibit biases that were absent or less pronounced in their pretrained predecessors. We examine the extent of this phenomenon in three cognitive biases - the decoy effect, the certainty effect, and the belief bias - all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models, especially those that have undergone instruction tuning, such as Flan-T5, GPT3.5, and GPT4. This research constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.
    
[^49]: 超越识别：用于语言模型的多位水印技术

    Advancing Beyond Identification: Multi-bit Watermark for Language Models. (arXiv:2308.00221v1 [cs.CL])

    [http://arxiv.org/abs/2308.00221](http://arxiv.org/abs/2308.00221)

    本研究提出了一种用于语言模型的多位水印技术——COLOR，可在语言模型生成过程中嵌入可追踪的多位信息，实现了提取水印、即时嵌入和维持文本质量等功能，同时允许零位检测。初步实验显示成功在中等长度的文本中嵌入了32位消息，准确率为91.9％。这项研究有效推进了对语言模型滥用的反制策略。

    

    本研究旨在积极应对大型语言模型在检测机器生成文本方面的滥用。尽管现有方法侧重于检测，但某些恶意滥用需要跟踪对手用户以进行反制。为了解决这个问题，我们提出了“多位水印通过颜色编码”（COLOR）的方法，在语言模型生成过程中嵌入可追踪的多位信息。利用零位水印技术的优势（Kirchenbauer等，2023a），COLOR实现了在没有模型访问权限的情况下提取水印、即时嵌入和维持文本质量的能力，同时允许零位检测。初步实验表明，在中等长度的文本（约500个标记）中成功嵌入了32位消息，准确率为91.9％。这项工作有效地推进了对语言模型滥用进行反制的策略。

    This study aims to proactively tackle misuse of large language models beyond identification of machine-generated text. While existing methods focus on detection, some malicious misuses demand tracing the adversary user for counteracting them. To address this, we propose "Multi-bit Watermark through Color-listing" (COLOR), embedding traceable multi-bit information during language model generation. Leveraging the benefits of zero-bit watermarking (Kirchenbauer et al., 2023a), COLOR enables extraction without model access, on-the-fly embedding, and maintains text quality, while allowing zero-bit detection all at the same time. Preliminary experiments demonstrates successful embedding of 32-bit messages with 91.9% accuracy in moderate-length texts ($\sim$500 tokens). This work advances strategies to counter language model misuse effectively.
    
[^50]: 基于深度强化学习的电池调控分层V2G协调策略以实现多方利益

    Deep Reinforcement Learning-Based Battery Conditioning Hierarchical V2G Coordination for Multi-Stakeholder Benefits. (arXiv:2308.00218v1 [eess.SY])

    [http://arxiv.org/abs/2308.00218](http://arxiv.org/abs/2308.00218)

    本研究提出了一种基于深度强化学习的电池调控分层V2G协调策略，旨在促进可再生能源利用和电网稳定。该策略可以实现多方利益，并且考虑了电网、电动车聚合器和用户的各种因素。

    

    随着电动车（EV）的普及和EV电子技术的发展，车辆对电网（V2G）技术和大规模调度策略已经出现，以促进可再生能源利用和电网稳定。本研究提出了基于深度强化学习（DRL）和权益证明算法的多方参与者分层V2G协调策略。此外，多方参与者包括电网、电动车聚合器（EVAs）和用户，并且所提出的策略可以实现多方利益。在电网方面，考虑到负荷波动和可再生能源消耗，而在EVA方面，考虑了能源限制和充电成本。用户方面考虑了电池SOX的三个关键调节参数，包括电荷状态、功率状态和健康状态。与四种典型基线相比，多方参与者分层协调策略可以增强可再生能源的利用。

    With the growing prevalence of electric vehicles (EVs) and advancements in EV electronics, vehicle-to-grid (V2G) techniques and large-scale scheduling strategies have emerged to promote renewable energy utilization and power grid stability. This study proposes a multi-stakeholder hierarchical V2G coordination based on deep reinforcement learning (DRL) and the Proof of Stake algorithm. Furthermore, the multi-stakeholders include the power grid, EV aggregators (EVAs), and users, and the proposed strategy can achieve multi-stakeholder benefits. On the grid side, load fluctuations and renewable energy consumption are considered, while on the EVA side, energy constraints and charging costs are considered. The three critical battery conditioning parameters of battery SOX are considered on the user side, including state of charge, state of power, and state of health. Compared with four typical baselines, the multi-stakeholder hierarchical coordination strategy can enhance renewable energy con
    
[^51]: 使用梯度累积优化技术评估Swin视觉Transformer模型的性能

    Performance Evaluation of Swin Vision Transformer Model using Gradient Accumulation Optimization Technique. (arXiv:2308.00197v1 [cs.CV])

    [http://arxiv.org/abs/2308.00197](http://arxiv.org/abs/2308.00197)

    本文评估了使用梯度累积优化技术的Swin视觉Transformer模型的性能，结果发现应用该技术会导致模型准确性下降并增加训练时间。

    

    视觉Transformer模型已经成为视觉识别任务的一种有前景的方法，通过利用基于Transformer的架构的优势，从根本上改变了领域。在各种视觉Transformer模型中，Swin Transformer由于其分层设计和有效捕捉局部和全局视觉特征的能力而受到重视。本文使用梯度累积优化（GAO）技术评估了Swin ViT模型的性能。我们研究了梯度累积优化技术对模型的准确性和训练时间的影响。我们的实验结果显示，应用GAO技术会导致Swin ViT模型的准确性显著下降，与标准的Swin Transformer模型相比。此外，当应用GAO模型时，我们还发现Swin ViT模型的训练时间显著增加。这些发现表明，应用GAO技术可能不适合Swin ViT模型，存在一定的问题。

    Vision Transformers (ViTs) have emerged as a promising approach for visual recognition tasks, revolutionizing the field by leveraging the power of transformer-based architectures. Among the various ViT models, Swin Transformers have gained considerable attention due to their hierarchical design and ability to capture both local and global visual features effectively. This paper evaluates the performance of Swin ViT model using gradient accumulation optimization (GAO) technique. We investigate the impact of gradient accumulation optimization technique on the model's accuracy and training time. Our experiments show that applying the GAO technique leads to a significant decrease in the accuracy of the Swin ViT model, compared to the standard Swin Transformer model. Moreover, we detect a significant increase in the training time of the Swin ViT model when GAO model is applied. These findings suggest that applying the GAO technique may not be suitable for the Swin ViT model, and concern sho
    
[^52]: 生成模型作为复杂系统科学：如何理解大型语言模型的行为？

    Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?. (arXiv:2308.00189v1 [cs.LG])

    [http://arxiv.org/abs/2308.00189](http://arxiv.org/abs/2308.00189)

    生成模型作为复杂系统科学，它们能够完成任务的行为表现需要被解释和理解，以实现对其行为的控制和未来研究的指导。

    

    从预训练模型中引导出期望的行为，同时避免不良行为，重新定义了自然语言处理并正在重新塑造我们与计算机的交互方式。曾经是一个科学工程学科，将构建模块堆叠在一起，现在可以说已经是一个复杂系统科学，其中寻求出现的行为以支持以前无法想象的用例。尽管有越来越多的基准测试来衡量任务性能，但我们缺乏解释语言模型展示这些任务完成的行为的解释。我们提出了一个系统性的努力，将语言模型的行为分解为解释跨任务性能的类别，以指导机械解释并帮助未来分析研究。

    Coaxing out desired behavior from pretrained models, while avoiding undesirable ones, has redefined NLP and is reshaping how we interact with computers. What was once a scientific engineering discipline-in which building blocks are stacked one on top of the other-is arguably already a complex systems science, in which emergent behaviors are sought out to support previously unimagined use cases.  Despite the ever increasing number of benchmarks that measure task performance, we lack explanations of what behaviors language models exhibit that allow them to complete these tasks in the first place. We argue for a systematic effort to decompose language model behavior into categories that explain cross-task performance, to guide mechanistic explanations and help future-proof analytic research.
    
[^53]: 数据管理和可解释机器学习中的归因分数

    Attribution-Scores in Data Management and Explainable Machine Learning. (arXiv:2308.00184v1 [cs.DB])

    [http://arxiv.org/abs/2308.00184](http://arxiv.org/abs/2308.00184)

    数据管理和可解释机器学习中的研究描述了使用实际因果关系来解释数据库查询答案和机器学习模型结果的责任评分的最新工作，以及与数据库修复和Shap-score计算相关的内容。

    

    我们描述了最近关于在数据库中使用实际因果关系来定义解释查询答案和机器学习分类模型结果的责任评分的研究。在数据库的情况下，我们展示并利用数据库修复与有用的连接。修复还用于给出数据库的一致性的量化度量。对于分类模型，责任评分得到了适当的扩展和说明。还分析和讨论了Shap-score的高效计算。重点放在作者和合作者的工作上。

    We describe recent research on the use of actual causality in the definition of responsibility scores as explanations for query answers in databases, and for outcomes from classification models in machine learning. In the case of databases, useful connections with database repairs are illustrated and exploited. Repairs are also used to give a quantitative measure of the consistency of a database. For classification models, the responsibility score is properly extended and illustrated. The efficient computation of Shap-score is also analyzed and discussed. The emphasis is placed on work done by the author and collaborators.
    
[^54]: 预训练的深度模型在标签稀缺的Learning-To-Rank中胜过GBDTs

    Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity. (arXiv:2308.00177v1 [cs.LG])

    [http://arxiv.org/abs/2308.00177](http://arxiv.org/abs/2308.00177)

    本研究研究了在标签稀缺的Learning-To-Rank问题中，无监督预训练的深度模型是否能胜过GBDTs和其他非预训练模型。实验结果表明，通过使用SimCLR-Rank方法进行无监督预训练，我们的深度学习模型在大量无标签数据和有限标签数据的情况下取得了显著优势。

    

    尽管深度学习模型在文本和图像领域是最先进的，但它们在表格形式的Learning-To-Rank问题上尚未一致地胜过梯度提升决策树(GBDTs)。近期在文本和图像任务上深度学习模型取得的性能提升主要依赖于无监督预训练，这种方法利用了比有标签数据多几个数量级的无标签数据。据我们所知，无监督预训练还未应用于Learning-To-Rank问题，而该问题通常产生大量无标签数据。本研究探究了无监督预训练是否能提高LTR性能，与GBDTs和其他非预训练模型相比。通过使用简单的设计选择(包括SimCLR-Rank，这是我们针对排名问题修改的SimCLR方法)，我们产生了预训练的深度学习模型，在有大量无标签数据且有限标签数据的情况下，显著优于GBDTs(和其他非预训练模型)。

    While deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. To the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. In this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. Using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data
    
[^55]: 对抗性鲁棒性神经法律判断系统

    Adversarially Robust Neural Legal Judgement Systems. (arXiv:2308.00165v1 [cs.CL])

    [http://arxiv.org/abs/2308.00165](http://arxiv.org/abs/2308.00165)

    本论文提出了一种对抗性鲁棒的神经法律判断系统，通过对早期存在的系统进行实验发现它们无法处理对抗性攻击。经过大量实验，我们的方法在处理对抗性攻击方面明显优于现有最先进的法律判断预测系统。

    

    法律判断预测是根据案情描述来预测法院案件结果的任务。这些任务应用自然语言处理技术来基于事实预测法律判断结果。最近，大规模公开数据集和自然语言处理模型已经增加了与法律判断预测系统相关的研究。为了使这种系统在实践中有所帮助，它们应该能够抵御对抗性攻击。先前的工作主要集中在构建神经法律判断系统上，但对于创建鲁棒的法律判断预测系统（LJP）几乎没有给予足够的关注。我们对早期存在的LJP系统进行了对抗性攻击实验，并发现它们都无法处理攻击。在这项工作中，我们提出了一种构建鲁棒LJP系统的方法。在三个法律数据集上进行的大量实验表明，我们的方法在处理对抗性攻击方面明显优于现有最先进的LJP系统。

    Legal judgment prediction is the task of predicting the outcome of court cases on a given text description of facts of cases. These tasks apply Natural Language Processing (NLP) techniques to predict legal judgment results based on facts. Recently, large-scale public datasets and NLP models have increased research in areas related to legal judgment prediction systems. For such systems to be practically helpful, they should be robust from adversarial attacks. Previous works mainly focus on making a neural legal judgement system; however, significantly less or no attention has been given to creating a robust Legal Judgement Prediction(LJP) system. We implemented adversarial attacks on early existing LJP systems and found that none of them could handle attacks. In this work, we proposed an approach for making robust LJP systems. Extensive experiments on three legal datasets show significant improvements in our approach over the state-of-the-art LJP system in handling adversarial attacks. 
    
[^56]: 使用Fine-Tuned的OpenAI LLM预测机器翻译输出中的完美质量段落：是否可以从历史数据中捕捉编辑距离模式？

    Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])

    [http://arxiv.org/abs/2308.00158](http://arxiv.org/abs/2308.00158)

    本研究探讨了使用Fine-Tuned的OpenAI LLM进行翻译质量估计的能力，实验证明可以通过Fine-Tuned的ChatGPT来预测机器翻译的质量，但仍有改进的空间。

    

    翻译质量估计（TQE）是将输出翻译部署到使用中之前的重要步骤。 TQE对于评估机器翻译（MT）和人工翻译（HT）的质量也是至关重要的，而不需要查看参考翻译。在这项工作中，我们检查了最先进的大型语言模型（LLMs）是否可以为TQE任务和它们的能力进行Fine-Tune。我们以ChatGPT为例，将TQE视为二元分类任务。使用英意和英德训练语料库，我们的实验结果显示，通过ChatGPT的API Fine-Tuned可以在预测翻译质量方面获得相对较高的得分，即是否需要编辑翻译，但肯定有改进准确性的空间。英意双语摘要可在论文中找到。

    Translation Quality Estimation (TQE) is an important step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. In this work, we examine if the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using English-Italian and English-German training corpus, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. if the translation needs to be edited, but there is definitely space to improve the accuracy. English-Italiano bilingual Abstract is available in the paper.
    
[^57]: 在反应式系统内对神经网络进行形式化解释

    Formally Explaining Neural Networks within Reactive Systems. (arXiv:2308.00143v1 [cs.AI])

    [http://arxiv.org/abs/2308.00143](http://arxiv.org/abs/2308.00143)

    这项研究在反应式系统中提出了一种基于DNN验证的形式化XAI技术，可以解释DNN的行为，并且通过利用系统的转换约束来计算简洁的解释。

    

    深度神经网络(DNNs)越来越多地被用作反应式系统中的控制器。然而，DNNs具有高度的不透明性，这使得解释和证明它们的行为变得困难。为了解决这个问题，出现了对可解释AI(XAI)技术的兴趣激增，这些技术能够找出导致DNN行为的输入特征。现有的XAI技术通常存在两个限制：(i)它们是启发式方法，并不能提供解释正确性的正式保证；(ii)它们通常适用于“一次性”系统(即DNN独立于过去的调用)，而不是反应式系统。在这里，我们开始弥合这个差距，提出一种基于DNN验证的形式化XAI技术，用于推理多步骤的反应式系统。我们建议通过利用系统的转换约束来计算简洁的解释的方法，以便减少底层验证器所探索的搜索空间。

    Deep neural networks (DNNs) are increasingly being used as controllers in reactive systems. However, DNNs are highly opaque, which renders it difficult to explain and justify their actions. To mitigate this issue, there has been a surge of interest in explainable AI (XAI) techniques, capable of pinpointing the input features that caused the DNN to act as it did.  Existing XAI techniques typically face two limitations: (i) they are heuristic, and do not provide formal guarantees that the explanations are correct; and (ii) they often apply to ``one-shot'' systems (where the DNN is invoked independently of past invocations), as opposed to reactive systems.  Here, we begin bridging this gap, and propose a formal DNN-verification-based XAI technique for reasoning about multi-step, reactive systems. We suggest methods for efficiently calculating succinct explanations, by exploiting the system's transition constraints in order to curtail the search space explored by the underlying verifier. W
    
[^58]: 一套用于表格分类公平性的公平数据集

    A Suite of Fairness Datasets for Tabular Classification. (arXiv:2308.00133v1 [cs.LG])

    [http://arxiv.org/abs/2308.00133](http://arxiv.org/abs/2308.00133)

    这篇论文介绍了一套新的公平数据集，旨在解决表格分类中实验评估数据不足的问题，为未来公平感知机器学习研究提供更严谨的实验评估。

    

    已经有许多关于提高机器学习分类器在表格数据中公平性的算法的论文。不幸的是，大多数论文只使用了非常少的数据集进行实验评估。我们引入了一套函数，用于提取20个公正数据集并提供相关的公平元数据。希望这些数据集能够促进未来公平感知机器学习研究中更严谨的实验评估。

    There have been many papers with algorithms for improving fairness of machine-learning classifiers for tabular data. Unfortunately, most use only very few datasets for their experimental evaluation. We introduce a suite of functions for fetching 20 fairness datasets and providing associated fairness metadata. Hopefully, these will lead to more rigorous experimental evaluations in future fairness-aware machine learning research.
    
[^59]: 语音表示学习：使用单视图、多视图和多任务方法学习双向编码器

    Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods. (arXiv:2308.00129v1 [eess.AS])

    [http://arxiv.org/abs/2308.00129](http://arxiv.org/abs/2308.00129)

    本文研究了语音数据的表示学习，主要关注多种设置和多种方法，旨在通过利用大量无标签和弱标签数据以及附加数据模态，改进下游序列预测任务。

    

    本文集中研究了针对时间或空间序列数据的表示学习，旨在通过使用学习到的表示改进下游序列预测任务。有监督学习一直是训练深度神经网络学习良好顺序表示的主要方法。然而，扩展有监督学习的一个限制因素是缺乏足够的注释数据。受到这一挑战的启发，自然而然地探索能够利用大量无标签和弱标签数据以及附加数据模态的表示学习方法。本文描述了对语音数据的广泛研究。与大多数关注单一学习设置的其他作品不同，本文研究了多种设置：带辅助损失的有监督学习、无监督学习、半监督学习和多视图学习。除了不同的学习问题，本文还探讨了多种表示学习方法。

    This thesis focuses on representation learning for sequence data over time or space, aiming to improve downstream sequence prediction tasks by using the learned representations. Supervised learning has been the most dominant approach for training deep neural networks for learning good sequential representations. However, one limiting factor to scale supervised learning is the lack of enough annotated data. Motivated by this challenge, it is natural to explore representation learning methods that can utilize large amounts of unlabeled and weakly labeled data, as well as an additional data modality. I describe my broad study of representation learning for speech data. Unlike most other works that focus on a single learning setting, this thesis studies multiple settings: supervised learning with auxiliary losses, unsupervised learning, semi-supervised learning, and multi-view learning. Besides different learning problems, I also explore multiple approaches for representation learning. Tho
    
[^60]: 使用大型语言模型进行渗透测试：AI作为辅助

    Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v1 [cs.CL])

    [http://arxiv.org/abs/2308.00121](http://arxiv.org/abs/2308.00121)

    本文探讨了使用大型语言模型（如GPT3.5）作为AI助手来增强渗透测试人员的能力，实现了高级任务规划和低级漏洞寻找两种用例，取得了有前景的初步结果，并就提供该技术的伦理问题进行了讨论。

    

    软件安全测试领域，尤其是渗透测试是一项需要高水平专业知识的活动，并涉及许多手动测试和分析步骤。本文探讨了使用大型语言模型（如GPT3.5）来增强渗透测试人员的能力。我们研究了两种不同的用例：用于安全测试任务的高级任务规划和在易受攻击的虚拟机中进行低级漏洞寻找。对于后者，我们实现了一个闭环反馈，将由语言模型生成的低级操作与易受攻击的虚拟机（通过SSH连接）相连，并允许语言模型分析虚拟机状态以寻找漏洞，并提供具体的攻击向量。我们讨论了有前景的初步结果，详细介绍了改进的途径，并就提供该技术的伦理问题进行了讨论。

    The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providi
    
[^61]: MODS的模块化本体论 - 元数据对象描述模式

    A Modular Ontology for MODS -- Metadata Object Description Schema. (arXiv:2308.00116v1 [cs.CL])

    [http://arxiv.org/abs/2308.00116](http://arxiv.org/abs/2308.00116)

    本研究开发了模块化MODS本体论（MMODS-O），用于解决元数据对象描述模式（MODS）在知识图谱环境下的限制问题，并采用模块化本体论设计方法学（MOMo）实现了平衡。

    

    元数据对象描述模式（MODS）是用于描述图书概念和元数据的，并由美国国会图书馆维护。MODS的标准版本是基于XML思维的XML模式，这意味着它在知识图谱环境下存在一些重要限制。因此，我们开发了模块化MODS本体论（MMODS-O），它集成了MODS的所有元素和属性。在设计本体论时，我们采用了最近的模块化本体论设计方法学（MOMo），旨在在保守地与MODS向后兼容的同时，在模块化和高质量本体论设计之间取得平衡。

    The Metadata Object Description Schema (MODS) was developed to describe bibliographic concepts and metadata and is maintained by the Library of Congress. Its authoritative version is given as an XML schema based on an XML mindset which means that it has significant limitations for use in a knowledge graphs context. We have therefore developed the Modular MODS Ontology (MMODS-O) which incorporates all elements and attributes of the MODS XML schema. In designing the ontology, we adopt the recent Modular Ontology Design Methodology (MOMo) with the intention to strike a balance between modularity and quality ontology design on the one hand, and conservative backward compatibility with MODS on the other.
    
[^62]: 用于大型语言模型的三个方法巩固水印技术

    Three Bricks to Consolidate Watermarks for Large Language Models. (arXiv:2308.00113v1 [cs.CL])

    [http://arxiv.org/abs/2308.00113](http://arxiv.org/abs/2308.00113)

    本研究提出了三种基于理论和实证考虑的方法，巩固了用于大型语言模型的水印技术。新的统计检验方法能够在低错误阳性率下提供稳定的理论保证。与自然语言处理领域的经典基准测试相比，水印技术的有效性得到了验证，并且我们还开发了先进的检测方案，适用于具有大型语言模型访问权限和多位水印技术的场景。

    

    在判断生成文本和自然文本之间的差异越来越具有挑战性的背景下，水印技术被提出作为一种将生成文本归属于特定模型的有前景的技术。它改变了采样生成过程，留下了无形的痕迹在生成的输出中，以便于后续的检测。本研究基于三个理论和实证考虑，巩固了用于大型语言模型的水印技术。首先，我们引入了新的统计检验方法，提供了牢固的理论保证，即使在低错误阳性率下（小于10^(-6)），这些保证依然有效。其次，我们通过在自然语言处理领域中使用经典基准测试对比了水印技术的有效性，从而获得了关于它们在实际应用中可行性的见解。第三，我们为可以访问大型语言模型的情景以及多位水印技术开发了先进的检测方案。

    The task of discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing generated text to a specific model. It alters the sampling generation process so as to leave an invisible trace in the generated output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10$^{\text{-6}}$). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking.
    
[^63]: DPBERT: 基于动态规划的高效BERT推理

    DPBERT: Efficient Inference for BERT based on Dynamic Planning. (arXiv:2308.00108v1 [cs.CL])

    [http://arxiv.org/abs/2308.00108](http://arxiv.org/abs/2308.00108)

    DPBERT是一个基于动态规划的高效BERT推理方法，通过选择一部分transformer层来加速推理过程，在保持高准确性的同时降低延迟。

    

    大规模预训练语言模型（如BERT）为自然语言处理的发展做出了重要贡献。然而，这些模型需要大量的计算资源，难以应用于计算能力有限的移动设备上。本文旨在解决现有自适应输入推理方法的缺点，这些方法未能充分利用BERT的结构。我们提出了DPBERT，一种新的微调策略，通过选择BERT的一部分transformer层作为计算路径，在推理过程中加速BERT的推理。为此，我们的方法在原始BERT模型中添加了一个规划模块，用于确定推理过程中是否包含或绕过某个层。在GLUE基准测试上的实验结果表明，我们的方法在保持98%准确性的情况下，将延迟降低到75%，相比最先进的自适应输入方法，获得了更好的准确度和速度权衡。

    Large-scale pre-trained language models such as BERT have contributed significantly to the development of NLP. However, those models require large computational resources, making it difficult to be applied to mobile devices where computing power is limited. In this paper we aim to address the weakness of existing input-adaptive inference methods which fail to take full advantage of the structure of BERT. We propose Dynamic Planning in BERT, a novel fine-tuning strategy that can accelerate the inference process of BERT through selecting a subsequence of transformer layers list of backbone as a computational path for an input sample. To do this, our approach adds a planning module to the original BERT model to determine whether a layer is included or bypassed during inference. Experimental results on the GLUE benchmark exhibit that our method reduces latency to 75\% while maintaining 98\% accuracy, yielding a better accuracy-speed trade-off compared to state-of-the-art input-adaptive met
    
[^64]: 验证一种针对非结构化医疗数据的零样本学习自然语言处理工具的数据抽象能力

    Validation of a Zero-Shot Learning Natural Language Processing Tool for Data Abstraction from Unstructured Healthcare Data. (arXiv:2308.00107v1 [cs.CL])

    [http://arxiv.org/abs/2308.00107](http://arxiv.org/abs/2308.00107)

    验证了一种零样本学习自然语言处理工具的数据抽象能力，结果显示该工具在提取信息的速度上优于医师人工抽象者，并在准确性上非劣。

    

    目的：描述一种用于从PDF文档中提取数据的零样本学习自然语言处理（NLP）工具的开发和验证过程，例如电子健康记录中的文本。材料和方法：基于OpenAI的GPT-3.5模型开发了一种数据抽象工具，并与三位医师人工抽象者进行比较，以评估在提取199份去标识化的激素切除术病理报告中的14个独特变量的数据时，完成任务所需的时间和准确性。这些报告以向量化和扫描格式进行了处理，以确定光学字符识别对数据抽象的影响。该工具在数据抽象速度上进行了评估，并证明了准确性的非劣性。结果：人工抽象者每份报告需要平均101秒进行数据抽象，时间范围为15秒至284秒。相比之下，软件工具需求的平均时间为...

    Objectives: To describe the development and validation of a zero-shot learning natural language processing (NLP) tool for abstracting data from unstructured text contained within PDF documents, such as those found within electronic health records. Materials and Methods: A data abstraction tool based on the GPT-3.5 model from OpenAI was developed and compared to three physician human abstractors in terms of time to task completion and accuracy for abstracting data on 14 unique variables from a set of 199 de-identified radical prostatectomy pathology reports. The reports were processed by the software tool in vectorized and scanned formats to establish the impact of optical character recognition on data abstraction. The tool was assessed for superiority for data abstraction speed and non-inferiority for accuracy. Results: The human abstractors required a mean of 101s per report for data abstraction, with times varying from 15 to 284 s. In comparison, the software tool required a mean of 
    
[^65]: 一个人能够监督100个异构机器人的群体吗？

    Can A Single Human Supervise A Swarm of 100 Heterogeneous Robots?. (arXiv:2308.00102v1 [cs.RO])

    [http://arxiv.org/abs/2308.00102](http://arxiv.org/abs/2308.00102)

    这项研究研究了一个人是否能够监督一个真实环境中的异构机器人群体，结果表明，在适当的工作负荷算法支持下，一个人可以有效地指挥和控制这样的群体。

    

    一个未解决的研究问题是，一个人是否能够在真实环境中监督一个异构机器人群体完成任务。一个普遍的担忧是人类的工作负荷是否会到达极限。美国国防高级研究计划局的OFFsensive Swarm-Enabled Tactics计划在美国陆军城市训练场地上进行了一系列军事演习，为我们了解实现这种群体部署的影响提供了机会。指挥和控制群体战术整合团队的群体指挥官使用异构机器人群体执行相关任务。在最终的OFFSET计划现场演习中，团队收集了与群体指挥官的人类表现相关的客观和主观指标。使用一个多维的工作负荷算法，根据工作负荷的五个组成部分估计整体工作负荷，并对结果进行了分析。虽然群体指挥官的工作负荷估计确实超过了负荷极限。

    An open research question has been whether a single human can supervise a true heterogeneous swarm of robots completing tasks in real world environments. A general concern is whether or not the human's workload will be taxed to the breaking point. The Defense Advanced Research Projects Agency's OFFsensive Swarm-Enabled Tactics program's field exercises that occurred at U.S. Army urban training sites provided the opportunity to understand the impact of achieving such swarm deployments. The Command and Control of Aggregate Swarm Tactics integrator team's swarm commander users the heterogeneous robot swarm to conduct relevant missions. During the final OFFSET program field exercise, the team collected objective and subjective metrics related to teh swarm commander's human performance. A multi-dimensional workload algorithm that estimates overall workload based on five components of workload was used to analyze the results. While the swarm commander's workload estimate did cross the overlo
    
[^66]: 先思考再回应：为共情回应生成集成常识的因果解释

    Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation. (arXiv:2308.00085v1 [cs.CL])

    [http://arxiv.org/abs/2308.00085](http://arxiv.org/abs/2308.00085)

    本文提出了一种基于常识的因果解释方法，用于多样化的共情回应生成。该方法综合考虑了用户的角度和系统的角度，并通过集成常识知识提升了ChatGPT在系统的推理能力。实验结果表明，该方法在多项评估指标上超过了其他方法。

    

    最近的共情回应生成方法试图整合常识知识或对情绪原因的推理，以更好地理解用户的经历和感受。然而，这些方法主要关注从用户的角度理解上下文的因果关系，忽略了系统的角度。本文提出了一种基于常识的因果解释方法，用于多样化的共情回应生成，同时考虑用户的角度（用户的欲望和反应）和系统的角度（系统的意图和反应）。我们通过将上下文学习与常识知识相结合，增强了ChatGPT在系统的角度上的推理能力。然后，我们将基于常识的因果解释与ChatGPT和基于T5模型的方法进行整合。实验评估表明，我们的方法在自动评估和人工评估上优于其他可比较的方法。

    Recent approaches to empathetic response generation try to incorporate commonsense knowledge or reasoning about the causes of emotions to better understand the user's experiences and feelings. However, these approaches mainly focus on understanding the causalities of context from the user's perspective, ignoring the system's perspective. In this paper, we propose a commonsense-based causality explanation approach for diverse empathetic response generation that considers both the user's perspective (user's desires and reactions) and the system's perspective (system's intentions and reactions). We enhance ChatGPT's ability to reason for the system's perspective by integrating in-context learning with commonsense knowledge. Then, we integrate the commonsense-based causality explanation with both ChatGPT and a T5-based model. Experimental evaluations demonstrate that our method outperforms other comparable methods on both automatic and human evaluations.
    
[^67]: 为知识图谱补全构建语义丰富的嵌入模型

    Towards Semantically Enriched Embeddings for Knowledge Graph Completion. (arXiv:2308.00081v1 [cs.AI])

    [http://arxiv.org/abs/2308.00081](http://arxiv.org/abs/2308.00081)

    本论文讨论了知识图谱补全算法以及利用嵌入模型捕捉知识图谱中语义的不同方法，并提出知识图谱和语言模型相互受益的观点。

    

    基于嵌入模型的知识图谱补全在过去几年中越来越受关注。目前的大多数算法将知识图谱视为一个多向标记图，缺乏捕捉底层语义的能力。与此同时，大型语言模型（LLMs）已经捕获了大量信息，这一捕获对人工智能领域产生了革命性影响。知识图谱可以从LLMs中受益，反之亦然。本文讨论了基于不同生成嵌入模型变体的知识图谱补全算法。首先讨论了各种知识图谱补全算法，如转导和归纳链接预测以及实体类型预测算法。然后，介绍了利用知识图谱中的类型信息、LLMs以及捕捉不同描述逻辑公理中的语义的算法。最后，通过对现有算法的关键反思对论文进行总结。

    Embedding based Knowledge Graph (KG) Completion has gained much attention over the past few years. Most of the current algorithms consider a KG as a multidirectional labeled graph and lack the ability to capture the semantics underlying the schematic information. In a separate development, a vast amount of information has been captured within the Large Language Models (LLMs) which has revolutionized the field of Artificial Intelligence. KGs could benefit from these LLMs and vice versa. This vision paper discusses the existing algorithms for KG completion based on the variations for generating KG embeddings. It starts with discussing various KG completion algorithms such as transductive and inductive link prediction and entity type prediction algorithms. It then moves on to the algorithms utilizing type information within the KGs, LLMs, and finally to algorithms capturing the semantics represented in different description logic axioms. We conclude the paper with a critical reflection on
    
[^68]: 一种基于深度学习的新型网络入侵检测系统对抗对抗攻击的模型

    A Novel Deep Learning based Model to Defend Network Intrusion Detection System against Adversarial Attacks. (arXiv:2308.00077v1 [cs.CR])

    [http://arxiv.org/abs/2308.00077](http://arxiv.org/abs/2308.00077)

    本论文研究了一种基于深度学习的网络入侵检测系统（NIDS）对抗对抗攻击的新型模型。通过实施强大的对抗攻击方法，如FGSM、JSMA、PGD和C&W，并采用对抗训练作为防御方法来增强NIDS模型的稳健性。

    

    网络入侵检测系统（NIDS）是保护网络空间免受各种安全风险和未知网络攻击的重要工具。已经实施了许多基于机器学习（ML）和深度学习（DL）的NIDS解决方案。然而，所有这些解决方案都容易受到对抗性攻击的影响，恶意行为者通过向系统中注入对抗扰动样本来试图回避或欺骗模型。本研究的主要目标是研究强大的对抗攻击算法及其对DL-based NIDS的防御方法。快速梯度符号方法（FGSM），Jacobian显著性图攻击（JSMA），投影梯度下降（PGD）和Carlini＆Wagner（C＆W）是四种对NIDS实施的强大对抗攻击方法。作为防御方法，采用对抗训练来增强NIDS模型的稳健性。结果可以总结为三个阶段，即1）对抗攻击前，2）对抗攻击后，3）对抗攻击后。

    Network Intrusion Detection System (NIDS) is an essential tool in securing cyberspace from a variety of security risks and unknown cyberattacks. A number of solutions have been implemented for Machine Learning (ML), and Deep Learning (DL) based NIDS. However, all these solutions are vulnerable to adversarial attacks, in which the malicious actor tries to evade or fool the model by injecting adversarial perturbed examples into the system. The main aim of this research work is to study powerful adversarial attack algorithms and their defence method on DL-based NIDS. Fast Gradient Sign Method (FGSM), Jacobian Saliency Map Attack (JSMA), Projected Gradient Descent (PGD) and Carlini & Wagner (C&W) are four powerful adversarial attack methods implemented against the NIDS. As a defence method, Adversarial Training is used to increase the robustness of the NIDS model. The results are summarized in three phases, i.e., 1) before the adversarial attack, 2) after the adversarial attack, and 3) aft
    
[^69]: 人群安全管理系统：基于数据驱动的活动决策支持的规划和控制

    Crowd Safety Manager: Towards Data-Driven Active Decision Support for Planning and Control of Crowd Events. (arXiv:2308.00076v1 [cs.AI])

    [http://arxiv.org/abs/2308.00076](http://arxiv.org/abs/2308.00076)

    这篇论文介绍了一种新颖的技术和方法，旨在通过数据驱动的决策支持系统提升人群管理的规划和操作阶段。该方法利用创新的数据收集技术、数据集成和3D数字孪生技术，结合人工智能工具进行风险识别，并引入了蝴蝶结模型来评估和预测风险水平。

    

    本文提出了一种新颖的技术和方法，旨在增强规划和操作阶段的人群管理。该方法包括创新的数据收集技术、数据集成和可视化，使用3D数字孪生技术，并结合人工智能工具进行风险识别。本文介绍了“蝴蝶结”模型，这是一个综合性框架，旨在评估和预测风险水平。该模型结合了客观估计和预测，如交通流量运营和拥挤程度，以及各种恶化因素，如天气条件、情绪和游客的目的，以评估潜在事件风险。提出的框架应用于Scheveningen的人群安全管理项目，其中DigiTwin基于丰富的实时数据来源进行开发。一个值得注意的数据来源是Resono，提供访客数量和动向的见解，充分利用了一组

    This paper presents novel technology and methodology aimed at enhancing crowd management in both the planning and operational phases. The approach encompasses innovative data collection techniques, data integration, and visualization using a 3D Digital Twin, along with the incorporation of artificial intelligence (AI) tools for risk identification. The paper introduces the Bowtie model, a comprehensive framework designed to assess and predict risk levels. The model combines objective estimations and predictions, such as traffic flow operations and crowdedness levels, with various aggravating factors like weather conditions, sentiments, and the purpose of visitors, to evaluate the expected risk of incidents. The proposed framework is applied to the Crowd Safety Manager project in Scheveningen, where the DigiTwin is developed based on a wealth of real-time data sources. One noteworthy data source is Resono, offering insights into the number of visitors and their movements, leveraging a m
    
[^70]: 用户语言对ChatGPT中冲突死亡估计的影响

    How User Language Affects Conflict Fatality Estimates in ChatGPT. (arXiv:2308.00072v1 [cs.CL])

    [http://arxiv.org/abs/2308.00072](http://arxiv.org/abs/2308.00072)

    在以色列-巴勒斯坦和土耳其-库尔德冲突的背景下，本研究探讨了用户语言对ChatGPT中冲突死亡估计的影响。研究发现，在使用攻击者的语言进行查询时，GPT-3.5提供的估计较使用被攻击群体的语言查询时低27±11％。此外，否认存在此类袭击的回答进一步增加了这种差异，形成了一种新的偏见机制，可能加大现有的媒体偏见并加剧信息孤立。

    

    OpenAI的ChatGPT语言模型因其强大的复杂问题解决和信息检索能力而备受青睐。然而，关于该模型是否会复制语言特定训练数据中存在的偏见的担忧也不断出现。在本研究中，我们在以色列-巴勒斯坦和土耳其-库尔德冲突的背景下解决了这个问题。使用GPT-3.5，我们采用自动查询过程，以希伯来语和阿拉伯语查询关于特定空袭中的伤亡人数。我们的分析发现，当使用攻击者的语言进行查询时，GPT-3.5提供的死亡估计较使用被攻击群体的语言查询时低27±11％。否认存在此类袭击的回答进一步增加了这种差异，创造了一种在常规搜索引擎中不存在的新的偏见机制。这种语言偏见有可能放大现有的媒体偏见，并加剧信息孤立，最终加重冲突。

    OpenAI's ChatGPT language model has gained popularity as a powerful tool for complex problem-solving and information retrieval. However, concerns arise about the reproduction of biases present in the language-specific training data. In this study, we address this issue in the context of the Israeli-Palestinian and Turkish-Kurdish conflicts. Using GPT-3.5, we employed an automated query procedure to inquire about casualties in specific airstrikes, in both Hebrew and Arabic for the former conflict and Turkish and Kurdish for the latter. Our analysis reveals that GPT-3.5 provides 27$\pm$11 percent lower fatality estimates when queried in the language of the attacker than in the language of the targeted group. Evasive answers denying the existence of such attacks further increase the discrepancy, creating a novel bias mechanism not present in regular search engines. This language bias has the potential to amplify existing media biases and contribute to information bubbles, ultimately reinf
    
[^71]: 可解释的推理方法用于刻板印象识别

    Interpretable Stereotype Identification through Reasoning. (arXiv:2308.00071v1 [cs.CL])

    [http://arxiv.org/abs/2308.00071](http://arxiv.org/abs/2308.00071)

    本研究通过使用推理方法，在零射击刻板印象识别中取得了重要的进展，并发现推理的性能增益远远超过模型规模扩展的增益。推理不仅提高了准确性，还提高了决策的可解释性。

    

    鉴于语言模型训练使用了包含固有偏见的大量数据集，可能会不经意地持续系统性歧视，因此，审查和解决语言模型中的偏见变得至关重要，将公平性整合到它们的发展中，以确保这些模型具有公正和无偏的特性。在这项工作中，我们展示了基于Vicuna-13B-v1.3的零射击刻板印象识别中推理的重要性。尽管我们观察到从13B到33B的规模扩展会提高准确性，但我们表明推理的性能增益远远超过规模扩展的增益。我们的研究结果表明，推理可能是使LLMs在刻板印象等领域任务上超越规模定律的关键因素。此外，通过对选定的推理追踪进行定性分析，我们突出显示了推理不仅提高了准确性，还提高了决策的可解释性。

    Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
    
[^72]: 生成人工智能的强化学习：现状、机会和开放研究挑战

    Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. (arXiv:2308.00031v1 [cs.LG])

    [http://arxiv.org/abs/2308.00031](http://arxiv.org/abs/2308.00031)

    这篇论文调查了在生成人工智能中应用强化学习的现状、机会和开放研究问题。作者主要讨论了三种应用类型：无特定目标的生成方式、同时最大化目标函数的输出生成方式以及将无法通过目标函数捕捉的期望特征嵌入生成过程的方式。这个新兴领域中存在着丰富的机会和挑战。

    

    生成人工智能（AI）是近十年来计算机科学领域最令人兴奋的发展之一。与此同时，强化学习（RL）在各种机器学习任务中已经成为非常成功的范式。在本调查中，我们讨论了将RL应用于生成AI中的现状、机会和开放的研究问题。具体而言，我们将讨论三种应用类型，即作为一种无特定目标的生成方式，作为一种同时最大化目标函数的输出生成方式，以及作为一种将无法通过目标函数轻松捕捉的期望特征嵌入生成过程的方式。我们在调查结果中对这个迷人的新兴领域中的机会和挑战进行了深入的讨论。

    Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.
    
[^73]: Alpha-GPT：人机交互式 Alpha 挖掘在量化投资中的应用

    Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment. (arXiv:2308.00016v1 [q-fin.CP])

    [http://arxiv.org/abs/2308.00016](http://arxiv.org/abs/2308.00016)

    本论文提出了一种通过引入人机交互的新型 alpha 挖掘范式，并利用大型语言模型的能力，通过一种新颖的提示工程算法框架，开发了 Alpha-GPT。通过多个实验，展示了 Alpha-GPT 在量化投资领域的有效性和优势。

    

    在量化投资研究中，挖掘新的 alpha（有效的交易信号或因子）是其中最重要的任务之一。传统的 alpha 挖掘方法，无论是手工合成因子还是算法挖掘因子（如遗传编程搜索），都存在固有的局限性，尤其在实施量化分析师的想法方面。在本研究中，我们提出了一种新的 alpha 挖掘范式，引入了人机交互，并通过利用大型语言模型的能力，提出了一种新颖的提示工程算法框架来实现这个范式。此外，我们开发了 Alpha-GPT，一种新的交互式 alpha 挖掘系统框架，以一种启发式的方式“理解”量化研究人员的想法，并输出具有创造性、深入洞察力和有效性的 alpha。通过多个 alpha 挖掘实验，我们展示了 Alpha-GPT 的有效性和优势。

    One of the most important tasks in quantitative investment research is mining new alphas (effective trading signals or factors). Traditional alpha mining methods, either hand-crafted factor synthesizing or algorithmic factor mining (e.g., search with genetic programming), have inherent limitations, especially in implementing the ideas of quants. In this work, we propose a new alpha mining paradigm by introducing human-AI interaction, and a novel prompt engineering algorithmic framework to implement this paradigm by leveraging the power of large language models. Moreover, we develop Alpha-GPT, a new interactive alpha mining system framework that provides a heuristic way to ``understand'' the ideas of quant researchers and outputs creative, insightful, and effective alphas. We demonstrate the effectiveness and advantage of Alpha-GPT via a number of alpha mining experiments.
    
[^74]: 时间常识推理与获取概述

    An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v1 [cs.AI])

    [http://arxiv.org/abs/2308.00002](http://arxiv.org/abs/2308.00002)

    本文综述了时间常识推理领域的研究进展，重点关注通过增强语言模型的性能来提高推理能力，并对多个数据集进行评估。然而，这些增强模型仍然难以达到人类水平的推理能力。

    

    时间常识推理是指理解短语、动作和事件的典型时间背景并将其应用于需要这种知识的问题推理的能力。这种能力在时间自然语言处理任务中至关重要，可能应用于时间线摘要、时间问答和时间自然语言推断等方面。最近的研究表明，大型语言模型虽然善于生成语法正确的句子和解决分类任务，但在推理过程中往往会采取捷径，并陷入简单的语言陷阱。本文章概述了在时间常识推理领域的研究，特别关注通过各种增强方式提高语言模型的性能以及对越来越多数据集的评估。然而，这些增强模型在推理任务上仍然难以达到人类的水平。

    Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning task
    
[^75]: 在自我中心的环境中的"De Re"和"De Dicto"知识

    De Re and De Dicto Knowledge in Egocentric Setting. (arXiv:2308.00001v1 [cs.AI])

    [http://arxiv.org/abs/2308.00001](http://arxiv.org/abs/2308.00001)

    本文在自我中心的环境中引入了两个不同模态来捕捉"De Re"和"De Dicto"知识，并证明了这两个模态不能互相定义。

    

    先前的研究提出了 "自我中心" 这一术语，用于研究代理器的属性而不是可能世界的属性的逻辑系统。在这样的环境中，本文介绍了两种捕捉"De Re"和"De Dicto"知识的不同模态，并证明了这两种模态不能通过彼此定义。

    Prior proposes the term "egocentric" for logical systems that study properties of agents rather than properties of possible worlds. In such a setting, the paper introduces two different modalities capturing de re and de dicto knowledge and proves that these two modalities are not definable through each other.
    
[^76]: L3DMC: 使用混合曲率空间的蒸馏进行终身学习

    L3DMC: Lifelong Learning using Distillation via Mixed-Curvature Space. (arXiv:2307.16459v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.16459](http://arxiv.org/abs/2307.16459)

    L3DMC是一种使用混合曲率空间进行终身学习的蒸馏策略，通过建模和维护复杂几何结构来保留已经学到的知识。

    

    当终身学习模型在一系列任务上进行训练时，其性能会下降，因为在顺序学习新概念时嵌入空间的几何结构会发生变化。现有的终身学习方法大多在固定曲率（例如零曲率的欧几里德空间）上运行，这并不适合建模复杂的数据几何结构。此外，蒸馏策略直接应用于低维嵌入，通过使模型高度稳定来阻碍终身学习模型学习新概念。为了解决这个问题，我们提出了一种名为L3DMC的蒸馏策略，它在混合曲率空间上操作，通过建模和维护复杂的几何结构来保留已经学到的知识。我们建议使用正定的重构核希尔伯特空间（RKHS）将固定曲率空间（欧几里德和双曲）的投影低维嵌入到更高维度的空间中。

    The performance of a lifelong learning (L3) model degrades when it is trained on a series of tasks, as the geometrical formation of the embedding space changes while learning novel concepts sequentially. The majority of existing L3 approaches operate on a fixed-curvature (e.g., zero-curvature Euclidean) space that is not necessarily suitable for modeling the complex geometric structure of data. Furthermore, the distillation strategies apply constraints directly on low-dimensional embeddings, discouraging the L3 model from learning new concepts by making the model highly stable. To address the problem, we propose a distillation strategy named L3DMC that operates on mixed-curvature spaces to preserve the already-learned knowledge by modeling and maintaining complex geometrical structures. We propose to embed the projected low dimensional embedding of fixed-curvature spaces (Euclidean and hyperbolic) to higher-dimensional Reproducing Kernel Hilbert Space (RKHS) using a positive-definite k
    
[^77]: 持续学习的子空间蒸馏

    Subspace Distillation for Continual Learning. (arXiv:2307.16419v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.16419](http://arxiv.org/abs/2307.16419)

    该论文提出了一种新颖的知识蒸馏技术，通过近似数据流形，并用线性子空间建模结构，来在持续学习中减轻灾难性遗忘。实验证明，该方法优于其他方法，在多个具有挑战性的持续学习任务中表现出色。

    

    在持续学习中，一个最终的目标是保留在前面任务中学到的知识，同时学习新任务。为了减轻对先前知识的遗忘，我们提出了一种新颖的知识蒸馏技术，该技术考虑了神经网络潜在/输出空间的流形结构在学习新任务中。为了实现这一点，我们提出了一种近似数据流形的方法，从而通过线性子空间来建模结构并保持神经网络在学习新概念时的知识。我们证明了使用子空间建模具有一些有趣的特性，包括对噪声的鲁棒性，因此在持续学习中可以有效地减轻灾难性遗忘。我们还讨论并展示了我们的方法如何适应分类和分割问题。经验上，我们观察到我们的方法在几个具有挑战性的持续学习方法上表现优于其他方法。

    An ultimate objective in continual learning is to preserve knowledge learned in preceding tasks while learning new tasks. To mitigate forgetting prior knowledge, we propose a novel knowledge distillation technique that takes into the account the manifold structure of the latent/output space of a neural network in learning novel tasks. To achieve this, we propose to approximate the data manifold up-to its first order, hence benefiting from linear subspaces to model the structure and maintain the knowledge of a neural network while learning novel concepts. We demonstrate that the modeling with subspaces provides several intriguing properties, including robustness to noise and therefore effective for mitigating Catastrophic Forgetting in continual learning. We also discuss and show how our proposed method can be adopted to address both classification and segmentation problems. Empirically, we observe that our proposed method outperforms various continual learning methods on several challe
    
[^78]: 重新思考多模态实体对齐中的不确定性缺失和模糊的视觉模态

    Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment. (arXiv:2307.16210v1 [cs.AI])

    [http://arxiv.org/abs/2307.16210](http://arxiv.org/abs/2307.16210)

    在多模态实体对齐中，现有的方法忽视了视觉图像的不完整性和模糊性，本文通过分析表明模型在面对不完整性时容易出现过拟合和性能下降的问题。

    

    作为实体对齐（EA）的重要扩展，多模态实体对齐（MMEA）旨在通过利用相关的视觉信息来识别跨不同知识图谱（KGs）之间的相同实体。然而，现有的MMEA方法主要集中在多模态实体特征的融合范式上，而忽视了缺失和内在模糊性的视觉图像所带来的挑战。本文对视觉模态不完整性进行了进一步分析，在我们提出的MMEA-UMVM数据集上对最新的MMEA模型进行了基准测试，该数据集包含涵盖双语和单语对齐KGs的类型，并采用标准（非迭代）和迭代训练范式来评估模型性能。我们的研究表明，在面对模态不完整性时，模型很容易过拟合模态噪声，并在高缺失模态的情况下出现性能振荡或下降。这证明了增加视觉不确定性的问题。

    As a crucial extension of entity alignment (EA), multi-modal entity alignment (MMEA) aims to identify identical entities across disparate knowledge graphs (KGs) by exploiting associated visual information. However, existing MMEA approaches primarily concentrate on the fusion paradigm of multi-modal entity features, while neglecting the challenges presented by the pervasive phenomenon of missing and intrinsic ambiguity of visual images. In this paper, we present a further analysis of visual modality incompleteness, benchmarking latest MMEA models on our proposed dataset MMEA-UMVM, where the types of alignment KGs covering bilingual and monolingual, with standard (non-iterative) and iterative training paradigms to evaluate the model performance. Our research indicates that, in the face of modality incompleteness, models succumb to overfitting the modality noise, and exhibit performance oscillations or declines at high rates of missing modality. This proves that the inclusion of additiona
    
[^79]: PyTorch内部的错误：一个复制研究的实证研究

    An Empirical Study on Bugs Inside PyTorch: A Replication Study. (arXiv:2307.13777v1 [cs.SE])

    [http://arxiv.org/abs/2307.13777](http://arxiv.org/abs/2307.13777)

    本研究对PyTorch深度学习库中的错误进行了复制研究，通过调查和评估错误的原因和症状，提供了对错误识别和修复过程的了解。

    

    由于其识别复杂数据模式和实现智能行为的显著能力，软件系统越来越依赖于深度学习组件。这种软件开发变革的核心推动者是易于使用的深度学习库的可用性。像PyTorch和TensorFlow这样的库赋予各种智能系统以能力，提供了大量的算法和配置选项，适用于多个领域的系统。然而，这些受欢迎的深度学习库中的错误也可能对其所支持的系统的质量产生严重影响，因此了解如何在这些库中识别和修复错误非常重要。受Jia等人的研究启发，该研究调查了TensorFlow中错误的识别和修复过程，我们对非常流行的深度学习框架PyTorch中的错误进行了特征化。我们调查了PyTorch开发过程中发现的错误的原因和症状，并评估了修复这些错误的方法。

    Software systems are increasingly relying on deep learning components, due to their remarkable capability of identifying complex data patterns and powering intelligent behaviour. A core enabler of this change in software development is the availability of easy-to-use deep learning libraries. Libraries like PyTorch and TensorFlow empower a large variety of intelligent systems, offering a multitude of algorithms and configuration options, applicable to numerous domains of systems. However, bugs in those popular deep learning libraries also may have dire consequences for the quality of systems they enable; thus, it is important to understand how bugs are identified and fixed in those libraries.  Inspired by a study of Jia et al., which investigates the bug identification and fixing process at TensorFlow, we characterize bugs in the PyTorch library, a very popular deep learning framework. We investigate the causes and symptoms of bugs identified during PyTorch's development, and assess the
    
[^80]: EmotionPrompt: 通过情感刺激提升大型语言模型的关键心理学方法

    EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])

    [http://arxiv.org/abs/2307.11760](http://arxiv.org/abs/2307.11760)

    EmotionPrompt是一个基于心理学的方法，通过将情感刺激融入到提示中，提升了大型语言模型在各项任务上的性能，并且同时改善了其真实性和信息量。

    

    大型语言模型（LLMs）在推理、语言理解和数学问题解决等许多领域取得了显著的性能，并被视为人工通用智能（AGI）的关键步骤。然而，LLMs对提示的敏感性仍然是其日常应用的主要瓶颈。本文从心理学中汲取灵感，提出了EmotionPrompt来探索情感智能以提升LLMs的性能。EmotionPrompt基于一个非常简单明了的原则：将情感刺激融入到提示中。实验结果表明，我们的方法在相同的单一提示模板上，与原始的零样本提示和Zero-shot-CoT相比，在8个任务上都显著优于多种模型：ChatGPT、Vicuna-13b、Bloom和T5。此外，观察到EmotionPrompt能够提高真实性和信息量。我们相信EmotionPrompt为探索跨学科知识开辟了一条新的道路。

    Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
    
[^81]: Jina Embeddings:一种新颖的高性能句子嵌入模型

    Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])

    [http://arxiv.org/abs/2307.11224](http://arxiv.org/abs/2307.11224)

    Jina Embeddings是一组高性能的句子嵌入模型，能够捕捉文本的语义本质。该论文详细介绍了Jina Embeddings的开发过程，并通过性能评估验证了其优越性能。

    

    Jina Embeddings由一组高性能的句子嵌入模型组成，能够将各种文本输入转化为数值表示，从而捕捉文本的语义本质。虽然这些模型并非专门设计用于文本生成，但在密集检索和语义文本相似性等应用中表现出色。本文详细介绍了Jina Embeddings的开发过程，从创建高质量的成对和三元数据集开始。它强调了数据清理在数据集准备中的关键作用，并对模型训练过程进行了深入探讨，最后利用Massive Textual Embedding Benchmark（MTEB）进行了全面的性能评估。

    Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB).
    
[^82]: 检测城市基础设施相互依赖网络中的脆弱节点

    Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network. (arXiv:2307.09866v1 [cs.LG])

    [http://arxiv.org/abs/2307.09866](http://arxiv.org/abs/2307.09866)

    该论文使用图神经网络和强化学习对城市基础设施相互依赖网络中的脆弱节点进行了准确建模和分析。

    

    理解和描述城市基础设施的脆弱性对我们具有重要价值，这些基础设施是城市正常运行所必需的工程设施，以网络的形式自然存在。潜在的应用包括保护脆弱设施和设计稳健的拓扑结构等。由于不同拓扑特性和基础设施脆弱性以及其复杂的演化机制之间的强关联，一些启发式分析和机器辅助分析在解决这种场景时存在局限性。在本文中，我们将相互依赖网络建模为异构图，并提出了一种基于图神经网络和强化学习的系统，可以在实际数据上进行训练，准确地描述城市系统的脆弱性。所提出的系统利用深度学习技术来理解和分析异构图，从而能够捕捉级联失败风险。

    Understanding and characterizing the vulnerability of urban infrastructures, which refers to the engineering facilities essential for the regular running of cities and that exist naturally in the form of networks, is of great value to us. Potential applications include protecting fragile facilities and designing robust topologies, etc. Due to the strong correlation between different topological characteristics and infrastructure vulnerability and their complicated evolution mechanisms, some heuristic and machine-assisted analysis fall short in addressing such a scenario. In this paper, we model the interdependent network as a heterogeneous graph and propose a system based on graph neural network with reinforcement learning, which can be trained on real-world data, to characterize the vulnerability of the city system accurately. The presented system leverages deep learning techniques to understand and analyze the heterogeneous graph, which enables us to capture the risk of cascade failu
    
[^83]: ChatGPT的行为随时间变化如何？

    How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])

    [http://arxiv.org/abs/2307.09009](http://arxiv.org/abs/2307.09009)

    本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。

    

    GPT-3.5和GPT-4是两种广泛使用的大型语言模型（LLM）服务。然而，这些模型何时以及如何进行更新是不透明的。在这里，我们对GPT-3.5和GPT-4的2023年3月和2023年6月版本进行了评估，涉及四项不同的任务：1）解决数学问题，2）回答敏感/危险问题，3）生成代码和4）视觉推理。我们发现，GPT-3.5和GPT-4的性能和行为在时间上可以有很大的变化。例如，GPT-4（2023年3月）在识别质数方面表现非常出色（准确率为97.6%），但GPT-4（2023年6月）在相同的问题上表现非常差（准确率为2.4%）。有趣的是，GPT-3.5（2023年6月）在这个任务上比GPT-3.5（2023年3月）要好得多。GPT-4在6月份对回答敏感问题的意愿较3月份要低，而无论是GPT-4还是GPT-3.5在6月份的代码生成中都有更多的格式错误。总体而言，我们的发现表明相同LLM服务的行为在相对较短的时间内可以发生重大变化。

    GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
    
[^84]: 基于循环一致性的无监督深度图匹配

    Unsupervised Deep Graph Matching Based on Cycle Consistency. (arXiv:2307.08930v1 [cs.CV])

    [http://arxiv.org/abs/2307.08930](http://arxiv.org/abs/2307.08930)

    本文提出了一种基于循环一致性的无监督深度图匹配方法，不需要真实对应的关键点对，通过在同一对象类别的图像之间强制匹配一致性来进行自我监督学习，该方法具有很高的灵活性，并且在无监督图匹配方面达到了最新的最先进水平。

    

    我们在稀疏领域的无监督深度图匹配中做出了贡献，应用于图像中的关键点匹配。与标准的“监督”方法相反，我们的方法不需要关键点对之间的真实对应。相反，它通过强制同一对象类别的图像之间的匹配一致性来进行自我监督。由于匹配和一致性损失是离散的，它们的导数不能直接用于学习。我们通过在组合求解器的黑盒微分的最新结果基础上构建我们的方法来解决这个问题。这使得我们的方法非常灵活，因为它与任意网络架构和组合求解器兼容。我们的实验评估表明，我们的技术在无监督图匹配方面达到了新的最先进水平。

    We contribute to the sparsely populated area of unsupervised deep graph matching with application to keypoint matching in images. Contrary to the standard \emph{supervised} approach, our method does not require ground truth correspondences between keypoint pairs. Instead, it is self-supervised by enforcing consistency of matchings between images of the same object category. As the matching and the consistency loss are discrete, their derivatives cannot be straightforwardly used for learning. We address this issue in a principled way by building our method upon the recent results on black-box differentiation of combinatorial solvers. This makes our method exceptionally flexible, as it is compatible with arbitrary network architectures and combinatorial solvers. Our experimental evaluation suggests that our technique sets a new state-of-the-art for unsupervised graph matching.
    
[^85]: 聚焦场景图生成的配对-关系：基于配对网络的全景场景图生成

    Pair then Relation: Pair-Net for Panoptic Scene Graph Generation. (arXiv:2307.08699v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.08699](http://arxiv.org/abs/2307.08699)

    本文提出了一种名为Pair-Net的全景场景图生成框架，通过使用配对提案网络（PPN）来学习和过滤主体和物体之间的稀疏配对关系，解决了当前全景场景图生成方法中忽视的对象间配对回忆率问题。

    

    全景场景图生成是场景图生成中的一项挑战性任务，其旨在使用全景分割代替边界框来创建更全面的场景图表示。与场景图生成相比，全景场景图生成具有一些具有挑战性的问题：像素级分割输出和完全关系探索（它还考虑了物体和物质之间的关系）。因此，当前的全景场景图生成方法的性能有限，这阻碍了下游任务或应用的发展。本工作的目标是设计一种新颖且强大的全景场景图生成基准。为了实现这一目标，我们首先进行了深入分析，确定了当前全景场景图生成模型的瓶颈，发现对象间配对的回忆率是先前的全景场景图生成方法所忽视的一个关键因素。基于这一发现和最近的基于查询的框架，我们提出了一种新的框架：配对-关系（Pair-Net），它使用配对提案网络（PPN）来学习和过滤主体和物体之间的稀疏配对关系。

    Panoptic Scene Graph (PSG) is a challenging task in Scene Graph Generation (SGG) that aims to create a more comprehensive scene graph representation using panoptic segmentation instead of boxes. Compared to SGG, PSG has several challenging problems: pixel-level segment outputs and full relationship exploration (It also considers thing and stuff relation). Thus, current PSG methods have limited performance, which hinders downstream tasks or applications. The goal of this work aims to design a novel and strong baseline for PSG. To achieve that, we first conduct an in-depth analysis to identify the bottleneck of the current PSG models, finding that inter-object pair-wise recall is a crucial factor that was ignored by previous PSG methods. Based on this and the recent query-based frameworks, we present a novel framework: Pair then Relation (Pair-Net), which uses a Pair Proposal Network (PPN) to learn and filter sparse pair-wise relationships between subjects and objects. Moreover, we also 
    
[^86]: 使用反事实路径的可解释人工智能

    Explainable AI with counterfactual paths. (arXiv:2307.07764v1 [cs.AI])

    [http://arxiv.org/abs/2307.07764](http://arxiv.org/abs/2307.07764)

    本文提出了一种新颖的可解释人工智能方法，使用反事实路径来生成解释。通过确定替代路径，可以提供更直观和可解释的解释模型行为的方式，并帮助识别和减轻模型中的偏见。

    

    可解释人工智能是机器学习中日益重要的一个研究领域，其原则上旨在使黑盒模型透明可解释。本文提出了一种新颖的可解释人工智能方法，该方法利用条件置换生成了反事实路径。我们的方法通过确定可能导致不同结果的替代路径来提供反事实解释。所提出的方法特别适用于基于知识图谱的反事实路径解释的生成。通过检查知识图谱中输入数据的假设性变化，我们可以系统地验证模型的行为，并检查对模型预测最重要的特征或特征组合。我们的方法提供了比传统的特征加权方法更直观和可解释的解释模型行为的方式，并可以帮助识别和减轻模型中的偏见。

    Explainable AI (XAI) is an increasingly important area of research in machine learning, which in principle aims to make black-box models transparent and interpretable. In this paper, we propose a novel approach to XAI that uses counterfactual paths generated by conditional permutations. Our method provides counterfactual explanations by identifying alternative paths that could have led to different outcomes. The proposed method is particularly suitable for generating explanations based on counterfactual paths in knowledge graphs. By examining hypothetical changes to the input data in the knowledge graph, we can systematically validate the behaviour of the model and examine the features or combination of features that are most important to the model's predictions. Our approach provides a more intuitive and interpretable explanation for the model's behaviour than traditional feature weighting methods and can help identify and mitigate biases in the model.
    
[^87]: SAS视频问答：自适应采样用于高效视频问答

    SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v1 [cs.CV])

    [http://arxiv.org/abs/2307.04192](http://arxiv.org/abs/2307.04192)

    SAS视频问答通过自适应采样策略解决了视频问答中的问题，提高了效率和准确性

    

    视频问答是视频理解领域的一项基础任务。尽管当前的视觉-语言模型(VLMs)配备了视频变换器(Video Transformers)，实现了时间建模并取得了优秀的结果，但代价是巨大的计算能力，因此在实时应用场景中过于昂贵。一种经济的解决方法是只对视频的一小部分帧进行采样，来代表视频的主要内容，并在这些采样帧上调整图像-文本模型。最近的视频理解模型通常随机采样一组帧或片段，而不考虑它们的内部关联性和与问题的相关性。我们认为这种无目标的采样可能会遗漏可以推导出正确答案的关键帧，在采样稀疏程度增加时，情况会变得更糟，而随着视频长度的增加，采样稀疏程度也会增加。为了解决这个问题，我们提出了两种帧采样策略，分别是

    Video question--answering is a fundamental task in the field of video understanding. Although current vision--language models (VLMs) equipped with Video Transformers have enabled temporal modeling and yielded superior results, they are at the cost of huge computational power and thus too expensive to deploy in real-time application scenarios. An economical workaround only samples a small portion of frames to represent the main content of that video and tune an image--text model on these sampled frames. Recent video understanding models usually randomly sample a set of frames or clips, regardless of internal correlations between their visual contents, nor their relevance to the problem. We argue that such kinds of aimless sampling may omit the key frames from which the correct answer can be deduced, and the situation gets worse when the sampling sparsity increases, which always happens as the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namel
    
[^88]: 推理还是背诵？通过反事实任务探索语言模型的能力和限制

    Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. (arXiv:2307.02477v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.02477](http://arxiv.org/abs/2307.02477)

    通过反事实任务的研究，我们发现当前的语言模型具备一定的抽象推理能力，但它们在任务求解过程中往往也依赖于狭窄、难以转移的过程，这对语言模型的性能解释和理解有着重要的启示。

    

    最近语言模型在各种任务上的出色表现表明它们具备一定程度的抽象推理能力。这些能力是通用且可转移的，还是专门针对预训练过程中遇到的特定任务？为了分开这些效果，我们提出了一个评估框架，基于“反事实”任务变种，这些变种与支撑标准任务的默认假设有所偏离。在一套包含11个任务的实验中，我们观察到反事实变种的非平凡性能，但与默认条件相比，性能显著而持续地下降。这表明当前的语言模型可能在一定程度上具备抽象任务求解能力，但它们通常也依赖于狭窄、难以转移的任务求解过程。这些结果促使我们对语言模型性能进行更加谨慎的解释，以区分这些行为方面。

    The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on "counterfactual" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to a degree, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.
    
[^89]: 编程教育的生成AI：比较ChatGPT、GPT-4和人类导师的表现

    Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors. (arXiv:2306.17156v1 [cs.CY])

    [http://arxiv.org/abs/2306.17156](http://arxiv.org/abs/2306.17156)

    该论文系统评估了ChatGPT、GPT-4和人类导师在不同的编程教育场景中的表现，并发现GPT-4优于ChatGPT，接近于人类导师。

    

    生成AI和大型语言模型在提高计算机教育方面具有很大的潜力，通过为初级编程提供下一代教育技术。最近的研究已经研究了这些模型在与编程教育相关的不同场景中的应用；然而，这些研究由于多种原因而受限，因为它们通常考虑的是已经过时的模型或仅仅特定的场景。因此，缺乏一个系统的研究来对最先进的模型进行全面的编程教育场景基准测试。在我们的工作中，我们系统地评估了两个模型，ChatGPT（基于GPT-3.5）和GPT-4，并将其在各种场景下与人类导师的表现进行了比较。我们使用五个初级Python编程问题和来自在线平台的真实错误程序进行评估，并使用专家评注来评估性能。我们的结果表明，GPT-4明显优于ChatGPT（基于GPT-3.5），并且接近于人类导师。

    Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies for introductory programming. Recent works have studied these models for different scenarios relevant to programming education; however, these works are limited for several reasons, as they typically consider already outdated models or only specific scenario(s). Consequently, there is a lack of a systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios. We evaluate using five introductory Python programming problems and real-world buggy programs from an online platform, and assess performance using expert-based annotations. Our results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to hu
    
[^90]: ChiPFormer: 通过离线决策变换器实现可转移芯片布局

    ChiPFormer: Transferable Chip Placement via Offline Decision Transformer. (arXiv:2306.14744v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14744](http://arxiv.org/abs/2306.14744)

    ChiPFormer通过离线学习可转移的布局策略，显著提高了芯片布局的质量，并在减少布局时间的同时增强了对未知芯片电路的适应能力。

    

    在现代芯片设计中，布局是一个关键步骤，旨在确定芯片画布上电路模块的位置。最近的研究表明，强化学习可以提高芯片布局中的人类性能。然而，这种基于强化学习的方法在训练时间长且在未知的芯片电路中具有较低的转移能力。为了解决这些挑战，我们将芯片布局问题作为一个离线强化学习问题，并提出了ChiPFormer，它可以通过固定的离线数据学习到可转移的布局策略。ChiPFormer具有一些先前的研究所没有的优势。首先，ChiPFormer能够利用离线布局设计在多任务设置中更有效地学习可转移的策略。其次，ChiPFormer能够促进对未知芯片电路的有效微调，将布局运行时间从几小时缩短到几分钟。第三，对32个芯片电路的大量实验证明，ChiPFormer在减少布局时间的同时实现了显著提高的布局质量。

    Placement is a critical step in modern chip design, aiming to determine the positions of circuit modules on the chip canvas. Recent works have shown that reinforcement learning (RL) can improve human performance in chip placement. However, such an RL-based approach suffers from long training time and low transfer ability in unseen chip circuits. To resolve these challenges, we cast the chip placement as an offline RL formulation and present ChiPFormer that enables learning a transferable placement policy from fixed offline data. ChiPFormer has several advantages that prior arts do not have. First, ChiPFormer can exploit offline placement designs to learn transferable policies more efficiently in a multi-task setting. Second, ChiPFormer can promote effective finetuning for unseen chip circuits, reducing the placement runtime from hours to minutes. Third, extensive experiments on 32 chip circuits demonstrate that ChiPFormer achieves significantly better placement quality while reducing t
    
[^91]: 微小子空间中发生微调: 探索预训练语言模型的内在任务特定子空间

    Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models. (arXiv:2305.17446v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17446](http://arxiv.org/abs/2305.17446)

    该论文通过发现预训练语言模型的内在任务特定子空间，提出了一种重新参数化和微调模型的新方法。研究发现在该子空间中，只需少量自由参数即可有效微调模型，并且某些维度对于引入任务特定知识至关重要。

    

    已知预训练语言模型（PLMs）过度参数化并具有显著的冗余，表明PLMs的自由度较小。本文从新的角度研究了重新参数化和微调PLMs的问题：发现内在的任务特定子空间。具体地，通过利用给定任务的微调过程的动态，学习了参数优化轨迹以揭示其内在的任务特定子空间。一个关键发现是，在子空间中，PLMs可以通过少量的自由参数进行有效的微调。此外，我们观察到在子空间的微调过程中出现了一些异常维度。禁用这些维度会严重降低模型性能。这表明这些维度对于引入任务特定知识到下游任务是至关重要的。

    Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs. Motivated by the observation, in this paper, we study the problem of re-parameterizing and fine-tuning PLMs from a new perspective: Discovery of intrinsic task-specific subspace. Specifically, by exploiting the dynamics of the fine-tuning process for a given task, the parameter optimization trajectory is learned to uncover its intrinsic task-specific subspace. A key finding is that PLMs can be effectively fine-tuned in the subspace with a small number of free parameters. Beyond, we observe some outlier dimensions emerging during fine-tuning in the subspace. Disabling these dimensions degrades the model performance significantly. This suggests that these dimensions are crucial to induce task-specific knowledge to downstream tasks.
    
[^92]: 在随机博弈中使用奖励机制的强化学习

    Reinforcement Learning With Reward Machines in Stochastic Games. (arXiv:2305.17372v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2305.17372](http://arxiv.org/abs/2305.17372)

    该论文研究了复杂任务中具有非马尔可夫回报函数的随机博弈的多智能体强化学习，提出了一种基于奖励机制的算法，在纳什均衡下学习每个智能体的最佳应答策略，并证明了学习的Q函数将会收敛于一个纳什均衡的Q函数，前提是阶段博弈具有全局最优点或者鞍点，并且智能体基于这一点进行最佳应答策略的Q函数更新。

    

    我们研究了复杂任务中具有非马尔可夫回报函数的随机博弈的多智能体强化学习。我们利用奖励机制来整合高层次的复杂任务知识。我们开发了一种称为QRM-SG的算法，用于学习每个智能体在纳什均衡下的最佳应答策略。在QRM-SG中，我们在增广状态空间中定义了纳什均衡下的Q函数。增广状态空间整合了随机博弈的状态和奖励机制的状态。每个智能体学习了系统中所有智能体的Q函数。我们证明了在QRM-SG中学习的Q函数将会收敛于一个纳什均衡的Q函数，前提是在学习过程中每个时间步的阶段博弈具有全局最优点或者鞍点，并且智能体基于这一点进行最佳应答策略的Q函数更新。我们使用Lemke-Howson方法来得出给定当前Q函数时的最佳应答策略。

    We investigate multi-agent reinforcement learning for stochastic games with complex tasks, where the reward functions are non-Markovian. We utilize reward machines to incorporate high-level knowledge of complex tasks. We develop an algorithm called Q-learning with reward machines for stochastic games (QRM-SG), to learn the best-response strategy at Nash equilibrium for each agent. In QRM-SG, we define the Q-function at a Nash equilibrium in augmented state space. The augmented state space integrates the state of the stochastic game and the state of reward machines. Each agent learns the Q-functions of all agents in the system. We prove that Q-functions learned in QRM-SG converge to the Q-functions at a Nash equilibrium if the stage game at each time step during learning has a global optimum point or a saddle point, and the agents update Q-functions based on the best-response strategy at this point. We use the Lemke-Howson method to derive the best-response strategy given current Q-func
    
[^93]: AlpacaFarm: 一种从人类反馈中学习的方法模拟框架

    AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])

    [http://arxiv.org/abs/2305.14387](http://arxiv.org/abs/2305.14387)

    该论文提出了一种名为AlpacaFarm的低成本模拟器，该模拟器为从人类反馈中学习的研究和开发提供了一种解决方案，通过设计LLM提示来模拟人类反馈，提出自动评估并提供参考实现，克服了数据收集的高昂成本、缺乏可信的评估和缺乏参考方法实现的挑战。

    

    大型语言模型（LLMs）如ChatGPT因其良好的指令跟随能力而得到了广泛应用。开发这些LLMs需要使用人类反馈进行训练的复杂且尚不明确的工作流程。将此指令跟随过程复制和理解面临三大挑战： 数据收集的高昂成本，缺乏可信的评估和缺乏参考方法实现。我们通过AlpacaFarm解决了这些挑战，这是一个低成本的模拟器，可用于从反馈中学习的研究和开发。第一，我们设计了LLM提示来模拟人类反馈，其成本比众包工作者便宜45倍，并且与人类反馈具有高度一致性。第二，我们提出了一种自动评估方法，并将其与真实世界交互中获得的人类指令进行验证。第三，我们为几种从配对反馈中学习的方法（PPO，best-of-n，expert iteration等）提供了参考实现。

    Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback
    
[^94]: 深度学习中的GELU激活函数：全面的数学分析和性能评估

    GELU Activation Function in Deep Learning: A Comprehensive Mathematical Analysis and Performance. (arXiv:2305.12073v1 [cs.LG])

    [http://arxiv.org/abs/2305.12073](http://arxiv.org/abs/2305.12073)

    本文对GELU激活函数进行了全面的数学分析和广泛的实验比较，证明了它在深度学习模型中具有优越的性能和适用性。

    

    在深度学习模型中，选择最合适的激活函数是影响其学习能力、稳定性和计算效率的关键因素。近年来，高斯误差线性单元（GELU）激活函数已经成为一种主流方法，在各种应用中超越了传统的激活函数，如修正线性单元（ReLU）。本文对GELU激活函数进行了严格的数学分析，详细探讨了其可微性、有界性、平稳性和光滑性等性质。此外，我们对GELU函数进行了广泛的实验比较，利用在CIFAR-10、CIFAR-100和STL-10数据集上训练的残差卷积网络作为实证测试基础。我们的结果证明了GELU相对于其他激活函数的卓越性能，确立了它在广泛的深度学习模型中的适用性。

    Selecting the most suitable activation function is a critical factor in the effectiveness of deep learning models, as it influences their learning capacity, stability, and computational efficiency. In recent years, the Gaussian Error Linear Unit (GELU) activation function has emerged as a dominant method, surpassing traditional functions such as the Rectified Linear Unit (ReLU) in various applications. This study presents a rigorous mathematical investigation of the GELU activation function, exploring its differentiability, boundedness, stationarity, and smoothness properties in detail. Additionally, we conduct an extensive experimental comparison of the GELU function against a broad range of alternative activation functions, utilizing a residual convolutional network trained on the CIFAR-10, CIFAR-100, and STL-10 datasets as the empirical testbed. Our results demonstrate the superior performance of GELU compared to other activation functions, establishing its suitability for a wide ra
    
[^95]: 连续多模态知识图谱构建

    Continual Multimodal Knowledge Graph Construction. (arXiv:2305.08698v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08698](http://arxiv.org/abs/2305.08698)

    连续多模态知识图谱构建面临着灾难性遗忘的挑战，需要解决新增实体和关系以及多模态源数据变化的问题。

    

    多模态知识图谱构建（MKGC）涉及使用多种形式的数据，如文本和图像，创建实体和关系的结构化表示。然而，现有的MKGC模型在处理动态现实场景中新增实体和关系方面面临挑战。目前的连续知识图谱构建设置主要关注从文本数据中提取实体和关系，忽视了其他多模态源。因此，有必要探索连续MKGC的挑战，以解决灾难性遗忘现象，并确保保留从不同形式的数据中提取的过去知识。本研究致力于通过开发终身MKGC基准数据集来研究这个复杂的主题。基于经验发现，当多媒体数据训练时，一些典型的MKGC模型可能在连续设置中意外表现不佳，与那些仅利用文本资源的模型相比。我们以实验证据为基础，总结出以下论点：连续多模态知识图谱构建面临着数据源变化导致的灾难性遗忘问题。

    Multimodal Knowledge Graph Construction (MKGC) involves creating structured representations of entities and relations using multiple modalities, such as text and images. However, existing MKGC models face challenges in handling the addition of new entities and relations in dynamic real-world scenarios. The current continual setting for knowledge graph construction mainly focuses on entity and relation extraction from text data, overlooking other multimodal sources. Therefore, there arises the need to explore the challenge of continual MKGC to address the phenomenon of catastrophic forgetting and ensure the retention of past knowledge extracted from different forms of data. This research focuses on investigating this complex topic by developing lifelong MKGC benchmark datasets. Based on the empirical findings that several typical MKGC models, when trained on multimedia data, might unexpectedly underperform compared to those solely utilizing textual resources in a continual setting, we p
    
[^96]: 因果信息分离：为抗分布转移设计代理特征

    Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts. (arXiv:2305.05832v1 [cs.LG])

    [http://arxiv.org/abs/2305.05832](http://arxiv.org/abs/2305.05832)

    本文提出了利用因果机制在不同环境下保持不变的直觉来主动准备的代理特征选择和工程技术，用以应对统计预测模型在分布转移情况下的稳定性问题。

    

    统计预测模型通常是在与最终使用情况不同的概率分布中进行训练的。为了预测分布转移，有一种方法是利用因果机制在不同环境下保持不变的直觉来主动准备。本文针对一个具有挑战性的场景，其中目标的因果和反因果变量都是未被观察到的。利用信息论，我们为下游观测变量开发了特征选择和工程技术，这些变量充当代理。我们选择有助于建立稳定模型的代理，并使用辅助训练任务从代理中提取增强稳定性的信息。我们在合成数据和真实数据上展示了我们技术的有效性。

    Statistical prediction models are often trained on data that is drawn from different probability distributions than their eventual use cases. One approach to proactively prepare for these shifts harnesses the intuition that causal mechanisms should remain invariant between environments. Here we focus on a challenging setting in which the causal and anticausal variables of the target are unobserved. Leaning on information theory, we develop feature selection and engineering techniques for the observed downstream variables that act as proxies. We identify proxies that help to build stable models and moreover utilize auxiliary training tasks to extract stability-enhancing information from proxies. We demonstrate the effectiveness of our techniques on synthetic and real data.
    
[^97]: FedNoRo: 针对类别不平衡和标签噪声异质性的噪声-鲁棒联邦学习

    FedNoRo: Towards Noise-Robust Federated Learning by Addressing Class Imbalance and Label Noise Heterogeneity. (arXiv:2305.05230v1 [cs.LG])

    [http://arxiv.org/abs/2305.05230](http://arxiv.org/abs/2305.05230)

    本文提出了一个名为 FedNoRo 的两阶段框架，用于解决类别不平衡和标签噪声异质性的联邦学习问题，并在 ICH 和 ISIC2019 数据集上取得了更好的表现。

    

    联邦噪声标签学习(FNLL)正在成为一种有前途的隐私保护的多源分散学习工具。现有研究基于全局数据类别平衡的假设，可能无法建模复杂的标签噪声，特别是在医学场景中。本文首先提出了一个更为真实的联邦标签噪声问题，其中全局数据是类别不平衡的，并且标签噪声是异质的，然后提出了一个名为 FedNoRo 的两阶段框架，用于噪声-鲁棒联邦学习。具体而言，在 FedNoRo 的第一阶段，采用每类损失指标之后跟随高斯混合模型进行嘈杂客户端识别。在第二阶段，同时采用知识蒸馏和距离感知聚合函数进行噪声-鲁棒联邦模型更新。对广泛使用的 ICH 和 ISIC2019 数据集的实验结果表明，FedNoRo 相对于最先进的 FNLL 方法在解决联邦学习中的类别不平衡和标签噪声异质性方面具有卓越的性能。

    Federated noisy label learning (FNLL) is emerging as a promising tool for privacy-preserving multi-source decentralized learning. Existing research, relying on the assumption of class-balanced global data, might be incapable to model complicated label noise, especially in medical scenarios. In this paper, we first formulate a new and more realistic federated label noise problem where global data is class-imbalanced and label noise is heterogeneous, and then propose a two-stage framework named FedNoRo for noise-robust federated learning. Specifically, in the first stage of FedNoRo, per-class loss indicators followed by Gaussian Mixture Model are deployed for noisy client identification. In the second stage, knowledge distillation and a distance-aware aggregation function are jointly adopted for noise-robust federated model updating. Experimental results on the widely-used ICH and ISIC2019 datasets demonstrate the superiority of FedNoRo against the state-of-the-art FNLL methods for addre
    
[^98]: 摘要生成的当前状态

    The Current State of Summarization. (arXiv:2305.04853v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.04853](http://arxiv.org/abs/2305.04853)

    摘要生成领域目前的研究关注点在于预训练的编码器-解码器模型和大规模自回归语言模型的转变，以及评估摘要生成系统的挑战和指令调整模型在零样本摘要生成中的潜力。

    

    随着文本信息的爆炸性增长，摘要生成系统变得越来越重要。本文旨在简明扼要地介绍抽象文本摘要生成的当前最先进技术。作为其中的一部分，我们概述了向预训练的编码器-解码器模型和大规模自回归语言模型转变的现有范例。此外，我们深入探讨了评估摘要生成系统的挑战以及基于指令调整的模型在零样本摘要生成中的潜力。最后，我们简要概述了目前将摘要生成系统整合到商业应用中的情况。

    With the explosive growth of textual information, summarization systems have become increasingly important. This work aims to concisely indicate the current state of the art in abstractive text summarization. As part of this, we outline the current paradigm shifts towards pre-trained encoder-decoder models and large autoregressive language models. Additionally, we delve further into the challenges of evaluating summarization systems and the potential of instruction-tuned models for zero-shot summarization. Finally, we provide a brief overview of how summarization systems are currently being integrated into commercial applications.
    
[^99]: 大型语言模型对齐的基本限制

    Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])

    [http://arxiv.org/abs/2304.11082](http://arxiv.org/abs/2304.11082)

    本文通过提出一种理论方法——行为期望边界（BEB），展示了大型语言模型中对齐的基本限制，并证明任何对齐过程都无法根除不希望的行为，这对于防止恶意攻击是不安全的。

    

    开发与人交互的语言模型的重要方面是对齐其行为，使其对其人类用户有用且无害。这通常通过调整模型的方式来实现，以增强所需的行为并抑制不希望的行为。在本文中，我们提出了一种名为行为期望边界(BEB)的理论方法，它允许我们正式研究大型语言模型中的几个内在特征和对齐的限制。重要的是，我们证明对于任何具有被该模型表现出的有限概率的行为，都存在可以触发模型输出此行为的提示，其概率随提示的长度增加而增加。这意味着任何减弱不希望的行为但未将其完全消除的对齐过程都无法抵御针对性攻击。此外，我们的框架提示了领先的

    An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading al
    
[^100]: 具有动态全局滤波器的双流时延神经网络用于说话人验证

    Dual-stream Time-Delay Neural Network with Dynamic Global Filter for Speaker Verification. (arXiv:2303.11020v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2303.11020](http://arxiv.org/abs/2303.11020)

    该论文提出了具有动态全局滤波器的双流时延神经网络，在说话人验证领域的表现优于其他最先进的方法，并实现了最先进的性能。

    

    时延神经网络(TDNN)是文本无关说话人验证领域的最先进模型之一。然而，对于传统的TDNN来说，捕捉被证明对于鲁棒说话人表示和长时间说话人验证至关重要的全局上下文是困难的。此外，常见的解决方案(例如自我关注)对于输入令牌具有二次复杂度，当应用于TDNN中具有大尺寸特征映射时，这使它们在计算上无法承受。为了解决这些问题，我们提出了TDNN的全局滤波器(GFTDNN)，它应用对数线性复杂度的FFT/IFFT和一组可微频域滤波器来高效地建模语音中的长期依赖关系。此外，还为增强全局滤波器的性能并防止过度拟合，特别设计了动态滤波策略和稀疏正则化方法。此外，我们构建了一个双流TDNN(DS-TDNN)，将基本通道分成两个并行通道，使用不同的频域滤波器训练每个通道。在两个不同的大规模说话人验证数据集上的实验表明，所提出的方法在性能上优于标准TDNN和其他最先进方法，并实现了最先进的性能。

    The time-delay neural network (TDNN) is one of the state-of-the-art models for text-independent speaker verification. However, it is difficult for conventional TDNN to capture global context that has been proven critical for robust speaker representations and long-duration speaker verification in many recent works. Besides, the common solutions, e.g., self-attention, have quadratic complexity for input tokens, which makes them computationally unaffordable when applied to the feature maps with large sizes in TDNN. To address these issues, we propose the Global Filter for TDNN, which applies log-linear complexity FFT/IFFT and a set of differentiable frequency-domain filters to efficiently model the long-term dependencies in speech. Besides, a dynamic filtering strategy, and a sparse regularization method are specially designed to enhance the performance of the global filter and prevent it from overfitting. Furthermore, we construct a dual-stream TDNN (DS-TDNN), which splits the basic cha
    
[^101]: SemEval-2023任务3上的mCPT：用于零样本和少样本框架检测的多语言标签感知对比预训练变压器

    mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])

    [http://arxiv.org/abs/2303.09901](http://arxiv.org/abs/2303.09901)

    本研究提出了mCPT模型用于多语言的、多标签的零样本或少样本的框架检测任务，并在西班牙语和其他8种语言中取得了良好的成绩。该方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。

    

    本文介绍了零样本的西班牙语框架检测任务的获胜系统，并在另外八种语言中取得了良好的成绩。框架检测任务的挑战在于在只有少量或零个样本的情况下识别一组14个框架，即多语言多标签的少样本和零样本设置。我们开发的解决方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。除了描述系统外，我们还进行了嵌入空间分析和消融研究，以展示我们的预训练程序如何支持框架检测以推进计算框架分析。

    This paper presents the winning system for the zero-shot Spanish framing detection task, which also achieves competitive places in eight additional languages. The challenge of the framing detection task lies in identifying a set of 14 frames when only a few or zero samples are available, i.e., a multilingual multi-label few- or zero-shot setting. Our developed solution employs a pre-training procedure based on multilingual Transformers using a label-aware contrastive loss function. In addition to describing the system, we perform an embedding space analysis and ablation study to demonstrate how our pre-training procedure supports framing detection to advance computational framing analysis.
    
[^102]: 在社交媒体上使用数字痕迹进行抑郁症检测：一种知识感知的深度学习方法

    Depression Detection Using Digital Traces on Social Media: A Knowledge-aware Deep Learning Approach. (arXiv:2303.05389v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.05389](http://arxiv.org/abs/2303.05389)

    本研究通过使用数字痕迹在社交媒体上检测抑郁症，提出了一种深度知识感知的抑郁症检测框架，并揭示了关键因素。通过真实数据进行的实证研究证明了其准确性和有效性。

    

    抑郁症是一种全球常见的疾病。它很难诊断，并且持续被低估。由于抑郁症患者在社交媒体上不断分享他们的症状、重大生活事件和治疗方法，研究人员开始利用社交媒体上用户生成的数字痕迹进行抑郁症检测。这种方法在抗击抑郁症方面具有独特的优势，因为它们可以促进创新的方法来对抗抑郁症并减轻其社会和经济负担。然而，大多数现有研究缺乏有效的手段将已建立的医学领域知识纳入抑郁症检测中，或者面临特征提取困难而影响性能。在设计科学研究范式的指导下，我们提出了一种深度知识感知的抑郁症检测 (DKDD) 框架，以准确检测社交媒体用户的抑郁风险，并解释对这种检测起关键作用的因素。通过真实数据进行了大量的实证研究。

    Depression is a common disease worldwide. It is difficult to diagnose and continues to be underdiagnosed. Because depressed patients constantly share their symptoms, major life events, and treatments on social media, researchers are turning to user-generated digital traces on social media for depression detection. Such methods have distinct advantages in combating depression because they can facilitate innovative approaches to fight depression and alleviate its social and economic burden. However, most existing studies lack effective means to incorporate established medical domain knowledge in depression detection or suffer from feature extraction difficulties that impede greater performance. Following the design science research paradigm, we propose a Deep Knowledge-aware Depression Detection (DKDD) framework to accurately detect social media users at risk of depression and explain the critical factors that contribute to such detection. Extensive empirical studies with real-world data
    
[^103]: 以ELBOs的加权积分理解扩散目标

    Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00848](http://arxiv.org/abs/2303.00848)

    本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。

    

    文献中的扩散模型采用不同的目标进行优化，并且这些目标都是加权损失的特例，其中加权函数指定每个噪声级别的权重。均匀加权对应于最大似然的原则性近似ELBO的最大化。但是实际上，由于更好的样本质量，目前的扩散模型使用非均匀加权。本文揭示了加权损失（带有任何加权）和ELBO目标之间的直接关系。我们展示了加权损失可以被写成一种ELBOs的加权积分形式，其中每个噪声级别都有一个ELBO。如果权重函数是单调的，那么加权损失是一种基于似然的目标：它在简单的数据增强下（即高斯噪声扰动）下最大化ELBO。我们的主要贡献是更深入地理解了扩散目标，但我们还进行了一些比较单调和非单调权重的实验。

    Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
    
[^104]: 关于知识蒸馏中的学生-教师偏差：违反规则是否有益？

    On student-teacher deviations in distillation: does it pay to disobey?. (arXiv:2301.12923v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12923](http://arxiv.org/abs/2301.12923)

    通过实验和理论分析，本论文发现在知识蒸馏中，学生网络对教师网络的概率偏离是系统性夸大的，同时也得到了更好的泛化能力。

    

    知识蒸馏（KD）被广泛用于通过训练学生模仿经过训练的“教师”网络的软概率来提高“学生”网络的测试准确性。然而，最近的研究表明，尽管被训练成适应教师的概率，学生不仅明显偏离这些概率，而且表现比教师更好。我们的研究旨在通过确定学生-教师偏差的确切性质，并论证它们与更好的泛化能力如何共存来解决这一看似矛盾的观察。首先，通过对图像和语言数据进行实验，我们确定这些偏差对应于学生系统性地夸大教师的自信水平。接下来，在一些简单的设置中，我们从理论和实证上建立了KD在收敛更快的过程中夸大了梯度下降的隐含偏差的证据。最后，

    Knowledge distillation (KD) has been widely-used to improve the test accuracy of a ``student'' network by training the student to mimic soft probabilities of a trained "teacher" network. Yet, it has been shown in recent work that, despite being trained to fit the teacher's probabilities, the student not only significantly deviates from these probabilities, but also performs even better than the teacher. Our work aims to reconcile this seemingly paradoxical observation by characterizing the precise nature of the student-teacher deviations, and by arguing how they can co-occur with better generalization. First, through experiments on image and language data, we identify that these deviations correspond to the student systematically exaggerating the confidence levels of the teacher. Next, we theoretically and empirically establish in some simple settings that KD also exaggerates the implicit bias of gradient descent in converging faster along the top eigendirections of the data. Finally, 
    
[^105]: 从两人零和博弈中抽象出不完美信息

    Abstracting Imperfect Information Away from Two-Player Zero-Sum Games. (arXiv:2301.09159v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2301.09159](http://arxiv.org/abs/2301.09159)

    通过正则化均衡，可以将两人零和博弈中的不完美信息抽象出来并作为完全信息问题处理。

    

    Nayyar等人在其开创性的工作中表明，通过在游戏过程中让玩家公开宣布其策略，不完美信息可以被从共同效益游戏中抽象出来。这个见解是支撑共同效益游戏合理的求解器和决策时间规划算法的基础。不幸的是，将同样的见解简单应用于两人零和博弈会失败，因为具有公开策略宣布的游戏的纳什均衡可能与原始游戏的纳什均衡不相对应。因此，现有的合理的决策时间规划算法需要复杂的额外机制，其具有不吸引人的特性。本文的主要贡献是展示某些正则化均衡不具有上述的不对应问题，因此，计算它们可以被视为完全信息问题。因为这些正则化均衡可以被无限接近纳什均衡，我们的结果为一种新的视角打开了大门。

    In their seminal work, Nayyar et al. (2013) showed that imperfect information can be abstracted away from common-payoff games by having players publicly announce their policies as they play. This insight underpins sound solvers and decision-time planning algorithms for common-payoff games. Unfortunately, a naive application of the same insight to two-player zero-sum games fails because Nash equilibria of the game with public policy announcements may not correspond to Nash equilibria of the original game. As a consequence, existing sound decision-time planning algorithms require complicated additional mechanisms that have unappealing properties. The main contribution of this work is showing that certain regularized equilibria do not possess the aforementioned non-correspondence problem -- thus, computing them can be treated as perfect-information problems. Because these regularized equilibria can be made arbitrarily close to Nash equilibria, our result opens the door to a new perspectiv
    
[^106]: 机器学习辅助下的Reed-Muller子码高效解码

    Machine Learning-Aided Efficient Decoding of Reed-Muller Subcodes. (arXiv:2301.06251v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2301.06251](http://arxiv.org/abs/2301.06251)

    本论文研究了具有灵活码率的Reed-Muller子码的高效解码问题，通过扩展递归投影聚合（RPA）译码算法，提出了subRPA和soft-subRPA算法，能够在维持较低复杂性的同时提高译码性能并实现可微分的解码算法。

    

    Reed-Muller（RM）码在一般的二进制输入无记忆对称信道上达到容量，并且据推测，在比例定律方面的性能与随机码相当。但是，这些结果是建立在对一般码参数使用最大似然译码器的情况下的。此外，RM码只能接受有限的码率集。对于有限长度的RM码，已经有诸如连续取消列表（SCL）译码器和最近引入的递归投影聚合（RPA）译码器等高效译码器可用。本文我们研究具有灵活码率的RM码子码。首先我们将RPA译码算法扩展到RM子码上。为了降低我们的译码算法（称为subRPA）的复杂性，我们研究了不同的投影剪枝方法。接下来，我们推导出基于软判断的版本，称为soft-subRPA，它不仅改进了subRPA的性能，还使得译码算法可微分。

    Reed-Muller (RM) codes achieve the capacity of general binary-input memoryless symmetric channels and are conjectured to have a comparable performance to that of random codes in terms of scaling laws. However, such results are established assuming maximum-likelihood decoders for general code parameters. Also, RM codes only admit limited sets of rates. Efficient decoders such as successive cancellation list (SCL) decoder and recently-introduced recursive projection-aggregation (RPA) decoders are available for RM codes at finite lengths. In this paper, we focus on subcodes of RM codes with flexible rates. We first extend the RPA decoding algorithm to RM subcodes. To lower the complexity of our decoding algorithm, referred to as subRPA, we investigate different approaches to prune the projections. Next, we derive the soft-decision based version of our algorithm, called soft-subRPA, that not only improves upon the performance of subRPA but also enables a differentiable decoding algorithm. 
    
[^107]: 使用调整掩码的终身强化学习

    Lifelong Reinforcement Learning with Modulating Masks. (arXiv:2212.11110v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11110](http://arxiv.org/abs/2212.11110)

    本文研究了终身强化学习中使用调整掩码的方法，通过将调整掩码应用于PPO和IMPALA代理，显著提高了在离散和连续强化学习任务中的性能。

    

    终身学习旨在创建在其生命周期中持续和逐步学习的人工智能系统，类似生物学习。到目前为止，这方面的尝试遇到了问题，包括灾难性遗忘、任务之间的干扰，以及无法利用先前的知识。虽然已经有相当多的研究集中在学习涉及输入分布变化的多个监督分类任务上，但是终身强化学习必须处理状态和转换分布以及奖励函数的变化。最近针对分类问题开发的使用固定骨干网络的调整掩码对于处理如此大范围的任务变化特别适用。在本文中，我们将调整掩码应用于深层次的终身强化学习，具体包括PPO和IMPALA代理。在离散和连续强化学习任务中与终身强化学习基线进行了比较，结果显示出卓越的性能。我们进一步研究了先前任务的线性组合的使用。

    Lifelong learning aims to create AI systems that continuously and incrementally learn during a lifetime, similar to biological learning. Attempts so far have met problems, including catastrophic forgetting, interference among tasks, and the inability to exploit previous knowledge. While considerable research has focused on learning multiple supervised classification tasks that involve changes in the input distribution, lifelong reinforcement learning (LRL) must deal with variations in the state and transition distributions, and in the reward functions. Modulating masks with a fixed backbone network, recently developed for classification, are particularly suitable to deal with such a large spectrum of task variations. In this paper, we adapted modulating masks to work with deep LRL, specifically PPO and IMPALA agents. The comparison with LRL baselines in both discrete and continuous RL tasks shows superior performance. We further investigated the use of a linear combination of previousl
    
[^108]: 自组织的空间流体自适应采样

    Space-fluid Adaptive Sampling by Self-Organisation. (arXiv:2210.17505v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2210.17505](http://arxiv.org/abs/2210.17505)

    本文提出了一种自组织的空间流体自适应采样方法来估计分布式传感数据或计算结果，其动态划分空间的方法具有优越的性能。

    

    协调系统中的一个重要任务是管理（估计、预测或控制）随空间变化的信号，例如分布式传感数据或计算结果。本文提出了一种基于竞争和生长/缩小的动态划分空间方法，协同自适应采样来估计空间现象。我们提供了一个基于场的协调框架中的自适应采样算法，并证明它是自稳定的。我们的模拟结果表明，在估计复杂函数使用高斯过程和跟踪传感器网络中的时空现象方面具有优越的性能。

    A recurrent task in coordinated systems is managing (estimating, predicting, or controlling) signals that vary in space, such as distributed sensed data or computation outcomes. Especially in large-scale settings, the problem can be addressed through decentralised and situated computing systems: nodes can locally sense, process, and act upon signals, and coordinate with neighbours to implement collective strategies. Accordingly, in this work we devise distributed coordination strategies for the estimation of a spatial phenomenon through collaborative adaptive sampling. Our design is based on the idea of dynamically partitioning space into regions that compete and grow/shrink to provide accurate aggregate sampling. Such regions hence define a sort of virtualised space that is "fluid", since its structure adapts in response to pressure forces exerted by the underlying phenomenon. We provide an adaptive sampling algorithm in the field-based coordination framework, and prove it is self-sta
    
[^109]: 3DALL-E: 将文本到图像AI集成到3D设计工作流程中

    3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows. (arXiv:2210.11603v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2210.11603](http://arxiv.org/abs/2210.11603)

    本文介绍了一种将文本到图像AI集成到3D设计工作流程中的方法，通过插件3DALL-E，设计师可以使用AI生成的图像灵感构建3D模型。研究结果显示，设计师对于3DALL-E在工作流程中有很大的潜力，可以生成参考图像、防止设计固化并激发设计的考虑。

    

    文本到图像AI能够生成新颖的图像以供灵感，但其在3D设计工作流程中的应用以及设计师如何利用AI提供的灵感构建3D模型尚未被探索。为了研究这个问题，我们将DALL-E、GPT-3和CLIP集成到CAD软件中，创建了3DALL-E插件，用于生成3D设计的2D图像灵感。3DALL-E允许用户基于他们正在建模的内容构建文本和图像提示。在一项涉及13名设计师的研究中，我们发现设计师们认为3DALL-E在他们的工作流程中具有巨大的潜力，可以使用文本到图像AI生成参考图像，防止设计固化，并激发设计的考虑。我们详述了在3D建模任务中观察到的提示模式，并提供了参与者所观察到的提示复杂性的度量。根据我们的研究结果，我们讨论了如何将3DALL-E与现有的生成式设计工作流程相结合，并提出了提示文献目录作为一种人工智能-人类设计历史的形式。

    Text-to-image AI are capable of generating novel images for inspiration, but their applications for 3D design workflows and how designers can build 3D models using AI-provided inspiration have not yet been explored. To investigate this, we integrated DALL-E, GPT-3, and CLIP within a CAD software in 3DALL-E, a plugin that generates 2D image inspiration for 3D design. 3DALL-E allows users to construct text and image prompts based on what they are modeling. In a study with 13 designers, we found that designers saw great potential in 3DALL-E within their workflows and could use text-to-image AI to produce reference images, prevent design fixation, and inspire design considerations. We elaborate on prompting patterns observed across 3D modeling tasks and provide measures of prompt complexity observed across participants. From our findings, we discuss how 3DALL-E can merge with existing generative design workflows and propose prompt bibliographies as a form of human-AI design history.
    
[^110]: 多智能体强化学习驱动的场外交易市场模拟

    Towards Multi-Agent Reinforcement Learning driven Over-The-Counter Market Simulations. (arXiv:2210.07184v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2210.07184](http://arxiv.org/abs/2210.07184)

    该论文研究了多智能体强化学习在场外交易市场模拟中的应用，通过适当设计奖励函数和共享策略学习，智能体能够学习到涵盖利润损失、最优执行和市场份额等多目标的新兴行为，同时也提出了一种基于强化学习的校准算法。

    

    我们研究了场外交易市场中流动性提供者和流动性接受者代理之间的博弈，典型示例是外汇市场。我们展示了如何通过适当设计参数化的奖励函数家族，并结合共享策略学习，构建出这个问题的高效解决方案。通过相互对战，我们基于深度强化学习的智能体能够学习到相对于利润损失、最优执行和市场份额等广泛目标的新兴行为。特别地，我们发现流动性提供者自然地学习到平衡对冲和偏斜的策略，其中偏斜是指根据其库存量将买入价格和卖出价格设置为非对称。我们还提出了一种新颖的基于强化学习的校准算法，我们发现它在对博弈均衡施加约束方面表现良好。在理论方面，我们能够展示我们的多智能体策略梯度算法在转换下的收敛速度。

    We study a game between liquidity provider and liquidity taker agents interacting in an over-the-counter market, for which the typical example is foreign exchange. We show how a suitable design of parameterized families of reward functions coupled with shared policy learning constitutes an efficient solution to this problem. By playing against each other, our deep-reinforcement-learning-driven agents learn emergent behaviors relative to a wide spectrum of objectives encompassing profit-and-loss, optimal execution and market share. In particular, we find that liquidity providers naturally learn to balance hedging and skewing, where skewing refers to setting their buy and sell prices asymmetrically as a function of their inventory. We further introduce a novel RL-based calibration algorithm which we found performed well at imposing constraints on the game equilibrium. On the theoretical side, we are able to show convergence rates for our multi-agent policy gradient algorithm under a tran
    
[^111]: 具有多个边缘成本估计的图的最短路径问题的推广

    A Generalization of the Shortest Path Problem to Graphs with Multiple Edge-Cost Estimates. (arXiv:2208.11489v3 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2208.11489](http://arxiv.org/abs/2208.11489)

    本文提出了一个广义的加权有向图框架，其中可以多次计算（估计）边缘权重，以提高准确性和运行时间成本，解决了最短路径问题的不确定性。

    This paper presents a generalized framework for weighted directed graphs, where edge weight can be computed (estimated) multiple times, at increasing accuracy and run-time expense, solving the uncertainty of the shortest path problem.

    图中的最短路径问题是AI理论和应用的基石。现有算法通常忽略边缘权重计算时间。在本文中，我们提出了一个广义的加权有向图框架，其中可以多次计算（估计）边缘权重，以提高准确性和运行时间成本。这引发了一个广义的最短路径问题，优化路径成本及其不确定性的不同方面。我们提出了一个完整的任何时候解决方案算法，实证证明了其功效。

    The shortest path problem in graphs is a cornerstone of AI theory and applications. Existing algorithms generally ignore edge weight computation time. In this paper we present a generalized framework for weighted directed graphs, where edge weight can be computed (estimated) multiple times, at increasing accuracy and run-time expense. This raises a generalized shortest path problem that optimize different aspects of path cost and its uncertainty. We present a complete anytime solution algorithm for the generalized problem, and empirically demonstrate its efficacy.
    
[^112]: 通过分散社会制裁的出现，分工的形成

    The emergence of division of labor through decentralized social sanctioning. (arXiv:2208.05568v4 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2208.05568](http://arxiv.org/abs/2208.05568)

    本研究通过引入社会规范模型，展示了分散社会制裁的出现模式能够解决以自利为导向的终身学习个体中的分工问题。

    

    人类生态成功依赖于我们的独特能力，即灵活自组织成合作社会群体，其中最成功的群体采用了大量的专业化和分工。与大多数其他动物不同，人类通过一生的试错中学习自己要扮演的角色。然而，当某些关键角色比其他角色更具吸引力，并且个体是自利的时，就会出现社会困境：每个个体都希望其他人扮演关键但无报酬的角色，这样他们可以自由选择一个报酬更高的角色。但是，如果每个人都这样行事，且一个关键角色缺乏填补，就会发生灾难。在这种情况下，学习最佳角色分配可能是不可能的。因此，一个基本问题是：如何在一群以自利为导向的终身学习个体中形成分工呢？在这里，我们展示了通过引入社会规范模型（我们将其视为分散社会制裁的出现模式）可以解决这个问题。

    Human ecological success relies on our characteristic ability to flexibly self-organize into cooperative social groups, the most successful of which employ substantial specialization and division of labor. Unlike most other animals, humans learn by trial and error during their lives what role to take on. However, when some critical roles are more attractive than others, and individuals are self-interested, then there is a social dilemma: each individual would prefer others take on the critical-but-unremunerative roles so they may remain free to take one that pays better. But disaster occurs if all act thusly and a critical role goes unfilled. In such situations learning an optimum role distribution may not be possible. Consequently, a fundamental question is: how can division of labor emerge in groups of self-interested lifetime-learning individuals? Here we show that by introducing a model of social norms, which we regard as emerging patterns of decentralized social sanctioning, it be
    
[^113]: 基于自动编码器的未知数量单通道水声信号源分离研究

    Source Separation of Unknown Numbers of Single-Channel Underwater Acoustic Signals Based on Autoencoders. (arXiv:2207.11749v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2207.11749](http://arxiv.org/abs/2207.11749)

    本研究提出了一种基于自动编码器的解决方案，用于对未知数量的单通道水声信号进行源分离。通过固定输出通道数量和新的性能评估方法，避免了排列问题引起的维度灾难，并在实验证明与已知信号数量相似的分离性能。该算法具有竞争性能、可解释性和可扩展性，在该框架下达到了最先进的水平。

    

    目前很少有研究关注未知数量信号的源分离问题，以及如何评估系统的性能尚不清楚。为了解决这两个问题，我们提出了一个具有固定输出通道数量的解决方案，避免了由于输出与目标对齐引起的排列问题导致的维度灾难。具体而言，我们提出了一个基于自动编码器的两步算法，并针对有静音通道的情况提出了一种新的性能评估方法。通过在模拟混合的辐射船噪声上进行的实验表明，所提出的解决方案可以达到与已知信号数量相似的分离性能。所提出的算法在已知信号数量的情况下取得了竞争性能，具有高度可解释性和可扩展性，并在该框架下达到了最先进的水平。

    Few existing studies focus on the source separation problem with unknown numbers of signals, and how to evaluate the performances of the systems is not yet clear. We propose a solution with a fixed number of output channels to address these two problems, enabling it to avoid the dimensional disaster caused by the permutation problem induced by the alignment of outputs to targets. Specifically, we propose a two-step algorithm based on autoencoders and a new performance evaluation method for situations with mute channels. Experiments conducted on simulated mixtures of radiated ship noise show that the proposed solution can achieve similar separation performance to that attained with a known number of signals. The proposed algorithm achieved competitive performance as two algorithms developed for known numbers of signals, which is highly explainable and extensible and get the state of the art under this framework.
    
[^114]: 可争议神经网络的因果发现与知识注入

    Causal Discovery and Knowledge Injection for Contestable Neural Networks. (arXiv:2205.09787v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09787](http://arxiv.org/abs/2205.09787)

    本研究提出了一种可以进行双向互动的方法，通过允许神经网络展示其所学因果图，并允许人类修改因果图后重新注入机器中，从而提供了一种调试神经网络的方式，实验结果显示该方法可以显著改善预测性能。

    

    神经网络在解决机器学习任务方面表现出色，但它们是否学习到了相关的因果关系尚不清楚，而它们的黑箱特性使得模型构建者难以理解和调试。我们提出了一种新颖的方法来解决这些问题，通过允许神经网络驱动的机器展示其所学因果图，并允许人类修改因果图后重新注入机器中，实现双向互动。所学模型保证符合因果图并遵循专家知识，其中部分知识也可以事先给定。通过对模型行为进行可视化并实现知识注入，我们的方法允许从数据中发现因果结构并支撑预测的从业者进行调试。在真实和合成表格数据上的实验表明，我们的方法可以改进预测性能高达2.4倍。

    Neural networks have proven to be effective at solving machine learning tasks but it is unclear whether they learn any relevant causal relationships, while their black-box nature makes it difficult for modellers to understand and debug them. We propose a novel method overcoming these issues by allowing a two-way interaction whereby neural-network-empowered machines can expose the underpinning learnt causal graphs and humans can contest the machines by modifying the causal graphs before re-injecting them into the machines. The learnt models are guaranteed to conform to the graphs and adhere to expert knowledge, some of which can also be given up-front. By building a window into the model behaviour and enabling knowledge injection, our method allows practitioners to debug networks based on the causal structure discovered from the data and underpinning the predictions. Experiments with real and synthetic tabular data show that our method improves predictive performance up to 2.4x while pr
    
[^115]: 超维计算（也被称为向量符号化架构）综述，第二部分：应用、认知模型和挑战

    A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part II: Applications, Cognitive Models, and Challenges. (arXiv:2112.15424v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2112.15424](http://arxiv.org/abs/2112.15424)

    超维计算（HDC/VSA）是一种计算框架，利用高维分布式表示和代数操作的特性，结合了结构化符号表示和向量分布式表示的优势。本综述的第二部分介绍了HDC/VSA的应用、认知模型和挑战。

    

    这是一篇综述的第二部分，专门介绍一个计算框架，最常被称为超维计算和向量符号化架构（HDC/VSA）。这两个名称都指的是一系列使用高维分布式表示的计算模型，并依赖于其关键操作的代数性质，以融合结构化符号表示和向量分布式表示的优势。全息化简表示是一个有影响力的HDC/VSA模型，在机器学习领域中非常知名，并常用于指代整个家族。然而，出于一致性考虑，我们在这里使用HDC/VSA来称呼该领域。本综述的第一部分涵盖了该领域的基础方面，例如导致HDC/VSA发展的历史背景、任何HDC/VSA模型的关键要素、已知的HDC/VSA模型，以及将各种类型的输入数据转化为适合HDC/VSA的高维向量的方法。

    This is Part II of the two-part comprehensive survey devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Holographic Reduced Representations is an influential HDC/VSA model that is well-known in the machine learning domain and often used to refer to the whole family. However, for the sake of consistency, we use HDC/VSA to refer to the field. Part I of this survey covered foundational aspects of the field, such as the historical context leading to the development of HDC/VSA, key elements of any HDC/VSA model, known HDC/VSA models, and the transformation of input data of various types into high-dimensional vectors suitable for H
    
[^116]: 《超维计算综述及矢量符号化架构》第一部分：模型和数据转换

    A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part I: Models and Data Transformations. (arXiv:2111.06077v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2111.06077](http://arxiv.org/abs/2111.06077)

    本文综述介绍了超维计算和矢量符号化架构（HDC/VSA），这是一种利用高维分布式表示和代数属性的计算模型，融合了结构化符号表示和向量分布式表示的优势。该领域涉及多个学科，并介绍了多个相关模型。

    

    这篇综述共分两部分，致力于介绍一个被称为超维计算和矢量符号化架构（HDC/VSA）的计算框架。HDC/VSA是一类使用高维分布式表示和依赖其关键操作的代数属性的计算模型，以融合结构化符号表示和向量分布式表示的优势。HDC/VSA家族中的显著模型包括张量积表示、全息减少表示、乘加置换、二元散射码和稀疏二元分布式表示，还有其他模型。HDC/VSA是一个高度跨学科的领域，涉及计算机科学、电子工程、人工智能、数学和认知科学。这一事实使得对该领域进行全面概述具有挑战性。

    This two-part comprehensive survey is devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Notable models in the HDC/VSA family are Tensor Product Representations, Holographic Reduced Representations, Multiply-Add-Permute, Binary Spatter Codes, and Sparse Binary Distributed Representations but there are other models too. HDC/VSA is a highly interdisciplinary field with connections to computer science, electrical engineering, artificial intelligence, mathematics, and cognitive science. This fact makes it challenging to create a thorough overview of the field. However, due to a surge of new researchers joining the field in recent
    
[^117]: 动态集体智能学习：通过精炼的梯度找到高效稀疏模型以剪枝权重

    Dynamic Collective Intelligence Learning: Finding Efficient Sparse Model via Refined Gradients for Pruned Weights. (arXiv:2109.04660v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.04660](http://arxiv.org/abs/2109.04660)

    本文介绍了动态集体智能学习（DCIL）方法，利用精炼的梯度更新剪枝权重，通过形成双向转发路径来寻找高效稀疏模型。这种方法利用了剪枝和未剪枝权重的集体智能之间的学习协同作用。

    

    随着深度神经网络（DNN）的增长，DNN参数的数量大幅增加。这使得DNN模型难以部署在资源有限的嵌入式系统上。为了缓解这个问题，出现了动态剪枝方法，该方法利用直通估计（STE）来近似剪枝权重的梯度，在训练过程中寻找不同的稀疏模式。STE可以帮助剪枝权重在寻找动态稀疏模式的过程中复活。然而，使用这些粗糙的梯度会导致训练不稳定和性能下降，因为STE近似的梯度信号不可靠。在这项工作中，为了解决这个问题，我们引入了精炼的梯度来更新剪枝权重，通过从两组（剪枝和未剪枝）权重形成双向转发路径。我们提出了一种新颖的动态集体智能学习（DCIL），利用两组权重的集体智能之间的学习协同作用。

    With the growth of deep neural networks (DNN), the number of DNN parameters has drastically increased. This makes DNN models hard to be deployed on resource-limited embedded systems. To alleviate this problem, dynamic pruning methods have emerged, which try to find diverse sparsity patterns during training by utilizing Straight-Through-Estimator (STE) to approximate gradients of pruned weights. STE can help the pruned weights revive in the process of finding dynamic sparsity patterns. However, using these coarse gradients causes training instability and performance degradation owing to the unreliable gradient signal of the STE approximation. In this work, to tackle this issue, we introduce refined gradients to update the pruned weights by forming dual forwarding paths from two sets (pruned and unpruned) of weights. We propose a novel Dynamic Collective Intelligence Learning (DCIL) which makes use of the learning synergy between the collective intelligence of both weight sets. We verify
    
[^118]: 超半径的扫雷游戏在P类问题中。(arXiv:2002.09534v2 [cs.CC] 已更新)

    Hyperbolic Minesweeper is in P. (arXiv:2002.09534v2 [cs.CC] UPDATED)

    [http://arxiv.org/abs/2002.09534](http://arxiv.org/abs/2002.09534)

    超半径的扫雷游戏属于P类问题，这一发现不仅适用于扫雷游戏本身，还适用于其他基于超半径平面上嵌入图的谜题。

    

    我们证明了，尽管扫雷游戏是NP完全的，但其超半径变体属于P类问题。我们的证明不依赖于扫雷游戏的规则，而是适用于基于满足超半径平面上嵌入图的局部约束的任何谜题。

    We show that, while Minesweeper is NP-complete, its hyperbolic variant is in P. Our proof does not rely on the rules of Minesweeper, but is valid for any puzzle based on satisfying local constraints on a graph embedded in the hyperbolic plane.
    

